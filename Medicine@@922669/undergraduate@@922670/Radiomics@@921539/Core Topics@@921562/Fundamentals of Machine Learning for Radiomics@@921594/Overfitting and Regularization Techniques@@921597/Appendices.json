{"hands_on_practices": [{"introduction": "We often talk abstractly about how the LASSO ($L_1$) penalty can shrink coefficients to exactly zero, effectively performing feature selection. This exercise takes you \"under the hood\" to see precisely how this happens for a hypothetical radiomics model [@problem_id:4553894]. By applying the Karush-Kuhn-Tucker (KKT) optimality conditions, you will calculate the exact regularization strength $\\lambda$ at which all feature weights vanish, providing a tangible and rigorous understanding of the \"selection\" mechanism in LASSO.", "problem": "A radiomics researcher is modeling a continuous imaging biomarker as a linear function of three standardized features derived from a computed tomography scan: shape elongation, texture contrast, and intensity skewness. The design matrix with $n=6$ patients and $p=3$ features is\n$$\nX=\\begin{pmatrix}\n-2 & 0 & 1 \\\\\n1 & -2 & 0 \\\\\n1 & 0 & -2 \\\\\n0 & 1 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nwhere each column of $X$ has zero mean and variance $1$ across the $6$ patients. The observed biomarker values (response vector) are\n$$\ny=\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{pmatrix}.\n$$\nConsider the least-squares model without an intercept, regularized by the Least Absolute Shrinkage and Selection Operator (LASSO), that is, minimizing the convex objective\n$$\n\\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{1},\n$$\nwhere $\\beta\\in\\mathbb{R}^{3}$ and $\\lambda\\ge 0$ is the regularization parameter. Using the Karush–Kuhn–Tucker (KKT) optimality conditions, determine the smallest penalty value $\\lambda$ for which the zero vector $\\,\\beta=\\mathbf{0}\\,$ is an optimal solution. Then, compute its exact numerical value for the given $X$ and $y$. Provide your final answer as a single real number. No rounding is required.", "solution": "The problem asks for the smallest regularization parameter $\\lambda$ such that the coefficient vector $\\beta = \\mathbf{0}$ is an optimal solution to the LASSO objective function.\n\nThe problem statement is first validated for soundness and completeness.\n\n**Step 1: Extract Givens**\n- The design matrix for $n=6$ patients and $p=3$ features is:\n$$\nX=\\begin{pmatrix}\n-2 & 0 & 1 \\\\\n1 & -2 & 0 \\\\\n1 & 0 & -2 \\\\\n0 & 1 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\n- The response vector is:\n$$\ny=\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{pmatrix}\n$$\n- The objective function to minimize is the LASSO functional:\n$$\nJ(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{1}\n$$\nwhere $\\beta \\in \\mathbb{R}^{3}$ and $\\lambda \\ge 0$.\n- The condition to be analyzed is finding the smallest $\\lambda$ for which $\\beta = \\mathbf{0}$ is an optimal solution.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard application of LASSO regression, a fundamental technique in statistical learning and signal processing. Its use in radiomics is well-established. The mathematical formulation is correct.\n- **Well-Posed:** The problem is clearly stated, asking for a specific, computable quantity ($\\lambda$) under a precise condition (optimality of $\\beta = \\mathbf{0}$). The existence of a unique, smallest such $\\lambda$ is a known property of the LASSO problem.\n- **Objective:** The problem is stated using precise mathematical language and notation, free from any subjective or ambiguous terms.\n- **Completeness and Consistency:** All necessary data ($X$ and $y$) and definitions are provided. The statement that the columns of $X$ have zero mean and unit variance is verifiable and correct. For example, for the first column $x_1$, the mean is $\\frac{1}{6}(-2+1+1+0+0+0)=0$, and the variance is $\\frac{1}{6}((-2)^2+1^2+1^2+0^2+0^2+0^2) = \\frac{6}{6} = 1$. Similar checks confirm the properties for the other columns. The problem is self-contained and internally consistent.\n\n**Verdict:** The problem is valid.\n\n**Step 3: Proceed to Solution**\nThe objective function to be minimized is:\n$$\nJ(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{1}\n$$\nThis function is a sum of two convex functions: a differentiable quadratic term, $f(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}$, and a non-differentiable but convex regularization term, $g(\\beta) = \\lambda\\|\\beta\\|_{1}$. Because the overall objective function $J(\\beta)$ is convex, the Karush-Kuhn-Tucker (KKT) conditions are both necessary and sufficient for optimality.\n\nThe optimality condition states that the zero vector must be an element of the subgradient of $J(\\beta)$ at the optimal point $\\beta^*$:\n$$\n\\mathbf{0} \\in \\partial J(\\beta^*) = \\nabla f(\\beta^*) + \\partial g(\\beta^*)\n$$\nThe gradient of the least-squares term $f(\\beta)$ is:\n$$\n\\nabla f(\\beta) = \\frac{d}{d\\beta} \\left( \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) \\right) = -X^T(y-X\\beta)\n$$\nThe subgradient of the $L_1$-norm term $g(\\beta)$ is $\\partial g(\\beta) = \\lambda \\partial\\|\\beta\\|_{1}$. The subgradient of the $L_1$-norm, $\\partial\\|\\beta\\|_{1}$, is the set of all vectors $s \\in \\mathbb{R}^p$ such that for each component $j \\in \\{1, 2, \\dots, p\\}$:\n$$\ns_j = \n\\begin{cases} \n\\text{sign}(\\beta_j) & \\text{if } \\beta_j \\neq 0 \\\\\n\\in [-1, 1] & \\text{if } \\beta_j = 0 \n\\end{cases}\n$$\nCombining these, the KKT condition for an optimal solution $\\beta^*$ is:\n$$\n-X^T(y-X\\beta^*) + \\lambda s = \\mathbf{0} \\quad \\text{for some } s \\in \\partial\\|\\beta^*\\|_{1}\n$$\nThis can be rewritten as:\n$$\nX^T(y-X\\beta^*) = \\lambda s\n$$\nWe are interested in the case where the optimal solution is the zero vector, $\\beta^* = \\mathbf{0}$. For this to be true, the KKT conditions must hold at $\\beta = \\mathbf{0}$. Substituting $\\beta^* = \\mathbf{0}$ into the condition gives:\n$$\nX^T(y-X\\mathbf{0}) = \\lambda s\n$$\n$$\nX^T y = \\lambda s\n$$\nIn this case, since $\\beta^* = \\mathbf{0}$, every component $\\beta_j^*$ is $0$. Therefore, according to the definition of the subgradient, each component $s_j$ of the vector $s$ must satisfy $s_j \\in [-1, 1]$.\n\nThe condition $X^T y = \\lambda s$ is a system of equations, one for each feature $j \\in \\{1, 2, 3\\}$:\n$$\n(X^T y)_j = \\lambda s_j\n$$\nFor a solution to exist for $s_j \\in [-1, 1]$, we must have:\n$$\n|(X^T y)_j| = \\lambda |s_j| \\le \\lambda \\cdot 1 = \\lambda\n$$\nThis inequality, $|(X^T y)_j| \\le \\lambda$, must hold for all $j \\in \\{1, 2, 3\\}$. To ensure this is true for all components, $\\lambda$ must be greater than or equal to the maximum of these absolute values:\n$$\n\\lambda \\ge \\max_{j} |(X^T y)_j|\n$$\nThis expression is equivalent to $\\lambda \\ge \\|X^T y\\|_{\\infty}$, where $\\|\\cdot\\|_{\\infty}$ is the maximum absolute component norm (infinity norm).\n\nThe problem asks for the *smallest* value of $\\lambda \\ge 0$ for which $\\beta = \\mathbf{0}$ is an optimal solution. This corresponds to the minimum value of $\\lambda$ that satisfies the derived condition. Thus, the smallest such $\\lambda$ is:\n$$\n\\lambda_{\\text{min}} = \\max_{j} |(X^T y)_j| = \\|X^T y\\|_{\\infty}\n$$\nNow we compute the numerical value. First, we calculate the vector $X^T y$:\n$$\nX^T = \\begin{pmatrix}\n-2 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & -2 & 0 & 1 & 1 & 0 \\\\\n1 & 0 & -2 & 1 & 0 & 0\n\\end{pmatrix}\n$$\n$$\ny = \\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{pmatrix}\n$$\n$$\nX^T y = \\begin{pmatrix}\n(-2)(2) + (1)(-1) + (1)(3) + (0)(0) + (0)(-2) + (0)(1) \\\\\n(0)(2) + (-2)(-1) + (0)(3) + (1)(0) + (1)(-2) + (0)(1) \\\\\n(1)(2) + (0)(-1) + (-2)(3) + (1)(0) + (0)(-2) + (0)(1)\n\\end{pmatrix}\n$$\n$$\nX^T y = \\begin{pmatrix}\n-4 - 1 + 3 \\\\\n2 - 2 \\\\\n2 - 6\n\\end{pmatrix} = \\begin{pmatrix}\n-2 \\\\\n0 \\\\\n-4\n\\end{pmatrix}\n$$\nFinally, we compute the infinity norm of this resulting vector to find the smallest required $\\lambda$:\n$$\n\\lambda = \\|X^T y\\|_{\\infty} = \\max(|-2|, |0|, |-4|) = \\max(2, 0, 4)\n$$\n$$\n\\lambda = 4\n$$\nThus, the smallest penalty value for which the zero vector is an optimal solution is $4$. For any $\\lambda \\ge 4$, the solution remains $\\beta = \\mathbf{0}$. For any $\\lambda < 4$, at least one coefficient will become non-zero.", "answer": "$$\n\\boxed{4}\n$$", "id": "4553894"}, {"introduction": "A powerful model is of little use if its performance is incorrectly estimated, a common pitfall that can lead to misleading conclusions. This is especially true in high-dimensional fields like radiomics, where \"data leakage\" can occur if information from the test set inadvertently influences model training, including the feature selection step [@problem_id:4553958]. This practice challenges you to diagnose this critical methodological flaw and to outline a robust, nested cross-validation pipeline that yields a trustworthy estimate of a model's true generalization ability.", "problem": "A radiomics researcher is working with a high-dimensional magnetic resonance imaging feature matrix and binary labels for tumor grading. Let the dataset be $\\{(x_i,y_i)\\}_{i=1}^n$, with $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\{0,1\\}$, where $p \\gg n$. The researcher computes univariate filter scores (for example, correlation or mutual information) for all $p$ features using all $n$ samples, selects the top $m$ features once, and then reports performance using $K$-fold cross-validation (CV). The researcher is aware of $L_1$ and $L_2$ regularization, but applies no penalty during feature screening. Which option best explains, from first principles, why this practice induces optimistic bias in radiomics and correctly sketches a leakage-free pipeline that nests selection and regularization within folds?\n\nA. Using all $n$ samples to select features makes the validation partitions influence the selection map, so the validation risk is no longer an unbiased proxy of the generalization risk; spurious associations are especially likely when $p \\gg n$. A leakage-free pipeline uses outer $K$-fold cross-validation to estimate performance, and, within each training fold, computes all preprocessing (including scaling), performs feature selection and model fitting using only that fold’s training data, and tunes the penalty parameter for $L_1$ (lasso) or $L_2$ (ridge) via an inner CV on the training fold. The held-out validation fold is transformed using statistics learned on the training fold and is never used in selection or tuning.\n\nB. As long as features are standardized once using all $n$ samples, selecting the top $m$ features on the full dataset does not bias cross-validation because scaling removes dependence; a correct pipeline is to standardize and preselect on all $n$ samples, then run $K$-fold CV without nesting.\n\nC. Optimistic bias occurs only when using $L_1$ regularization because it performs embedded selection; $L_2$ regularization prevents leakage, so it is acceptable to preselect features on all $n$ samples and then perform cross-validated ridge regression without nesting.\n\nD. Optimistic bias can be fully corrected by stratification and permutation testing in the outer CV, so it is acceptable to select features on all $n$ samples prior to CV; a correct pipeline is to preselect once, then perform stratified $K$-fold CV with permutation-based $p$-values to debias estimates.", "solution": "The problem statement describes a common but critical methodological flaw in machine learning pipelines, particularly in high-dimensional domains like radiomics. The core issue is a form of data leakage, which leads to an overestimation of model performance.\n\n**Problem Statement Validation**\n\n*   **Step 1: Extract Givens**\n    *   Dataset: $\\{(x_i,y_i)\\}_{i=1}^n$\n    *   Feature vectors: $x_i \\in \\mathbb{R}^p$\n    *   Labels: $y_i \\in \\{0,1\\}$ (binary)\n    *   Dimensionality relation: $p \\gg n$ (high-dimensional, few samples)\n    *   Researcher's procedure:\n        1.  Compute univariate filter scores for all $p$ features using the entire dataset of $n$ samples.\n        2.  Select the top $m$ features based on these scores. This is performed once on the full dataset.\n        3.  Report performance using $K$-fold cross-validation (CV) on the reduced dataset containing only the $m$ selected features.\n    *   Contextual information: The researcher is aware of $L_1$ and $L_2$ regularization. No regularization is applied during the feature screening step.\n    *   Question: Explain why this procedure induces optimistic bias and describe a correct, leakage-free pipeline.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   The problem statement is scientifically grounded. The scenario described is a classic example of selection bias, a well-documented phenomenon in statistical learning theory. The condition $p \\gg n$ is known to exacerbate this bias.\n    *   The problem is well-posed. It asks for an explanation of a known error and a description of the correct methodology, for which a standard, accepted answer exists.\n    *   The problem is objective, using precise, technical language to describe a specific data analysis workflow.\n    *   The problem setup is complete and internally consistent. It provides all necessary details to identify the methodological flaw.\n\n*   **Step 3: Verdict and Action**\n    *   The problem statement is valid. I will proceed with a full derivation and analysis.\n\n**Derivation of Principle and Correct Pipeline**\n\nThe fundamental principle a cross-validation procedure seeks to uphold is the estimation of a model's generalization error, which is its expected error on new, unseen data. To achieve an unbiased estimate, the test (or validation) set in each fold must be held out and treated as truly unseen data. This means it must not be used in any part of the model training process. The \"model training process\" encompasses not just fitting the final model parameters, but also all preceding steps that use the data to make decisions, such as feature scaling, feature selection, and hyperparameter tuning.\n\nThe researcher's described procedure violates this principle. The procedure is:\n1.  Feature Selection: Use the full dataset $\\{(x_i, y_i)\\}_{i=1}^n$ to select the top $m$ features.\n2.  Cross-Validation: Split the data (now with only $m$ features) into $K$ folds and perform CV.\n\nThe error occurs in step 1. By using the labels $y_i$ from all $n$ samples to score and select features, information from the entire dataset—including data that will later be used for validation—has \"leaked\" into the feature selection step.\n\nConsider a single fold of the $K$-fold CV. The data is partitioned into a training set, $D_{train}$, and a validation set, $D_{val}$. The model is trained on $D_{train}$. However, the very features constituting the data space were chosen because they had a high correlation with the labels in the *full dataset*, which includes $D_{val}$. Therefore, $D_{val}$ is not independent of the feature selection process.\n\nIn the high-dimensional setting where the number of features $p$ is much larger than the number of samples $n$ ($p \\gg n$), it is highly probable that some features will exhibit a strong spurious correlation with the outcome labels purely by random chance. The feature selection step, performed on the full dataset, will preferentially select these spuriously correlated features. When the model is subsequently evaluated on the validation folds, its performance will be artificially inflated because the validation data itself contributed to selecting features that were, by chance, highly predictive of it. The resulting CV score is, therefore, an optimistically biased estimate of the true generalization performance.\n\nA correct, leakage-free pipeline ensures that the validation data for each fold is never seen during any phase of model development. This is achieved through a nested cross-validation scheme:\n\n1.  **Outer Loop (Performance Estimation):** The dataset is split into $K$ folds. For each fold $k \\in \\{1, ..., K\\}$:\n    a. Designate fold $k$ as the hold-out validation set, $D_{val}^{(k)}$. The remaining $K-1$ folds constitute the training set, $D_{train}^{(k)}$.\n    b. **Using only $D_{train}^{(k)}$**, perform the entire model-building pipeline:\n        i. **Preprocessing:** Calculate any necessary statistics (e.g., mean and standard deviation for standardization) only from $D_{train}^{(k)}$. Apply the learned transformation to both $D_{train}^{(k)}$ and $D_{val}^{(k)}$.\n        ii. **Feature Selection:** Perform the feature selection routine (e.g., univariate filtering) using only $D_{train}^{(k)}$ to identify a set of features.\n        iii. **Hyperparameter Tuning:** If the chosen model has hyperparameters, such as the regularization penalty $\\lambda$ for $L_1$ (Lasso) or $L_2$ (Ridge) regression, they must be tuned. This is typically done via an **inner cross-validation loop** performed exclusively on $D_{train}^{(k)}$.\n        iv. **Model Fitting:** Train the final model (with tuned hyperparameters) on the entirety of $D_{train}^{(k)}$ using the features selected in step (ii).\n    c. **Evaluation:** Use the trained model to make predictions on the held-out validation set, $D_{val}^{(k)}$, and compute a performance metric (e.g., accuracy, AUC).\n2.  **Final Performance Estimate:** The performance metrics from all $K$ outer folds are aggregated (e.g., averaged) to provide a single, robust, and unbiased estimate of the model's generalization performance.\n\n**Evaluation of Options**\n\n*   **A. Using all $n$ samples to select features makes the validation partitions influence the selection map, so the validation risk is no longer an unbiased proxy of the generalization risk; spurious associations are especially likely when $p \\gg n$. A leakage-free pipeline uses outer $K$-fold cross-validation to estimate performance, and, within each training fold, computes all preprocessing (including scaling), performs feature selection and model fitting using only that fold’s training data, and tunes the penalty parameter for $L_1$ (lasso) or $L_2$ (ridge) via an inner CV on the training fold. The held-out validation fold is transformed using statistics learned on the training fold and is never used in selection or tuning.**\n    This option provides a complete and precise explanation. It correctly identifies the data leakage from selection, notes the exacerbating effect of the $p \\gg n$ setting, and accurately describes the correct nested CV pipeline where all model-building steps (preprocessing, selection, tuning, fitting) are confined to the training portion of each fold.\n    **Verdict: Correct.**\n\n*   **B. As long as features are standardized once using all $n$ samples, selecting the top $m$ features on the full dataset does not bias cross-validation because scaling removes dependence; a correct pipeline is to standardize and preselect on all $n$ samples, then run $K$-fold CV without nesting.**\n    This option is fundamentally flawed. First, standardizing on all $n$ samples is itself a form of data leakage. Second, and more critically, the claim that scaling \"removes dependence\" and therefore justifies feature pre-selection is false. Scaling does not alter the correlation between features and the outcome variable, which is what the filter-based selection method utilizes. This option prescribes the exact incorrect procedure that leads to bias.\n    **Verdict: Incorrect.**\n\n*   **C. Optimistic bias occurs only when using $L_1$ regularization because it performs embedded selection; $L_2$ regularization prevents leakage, so it is acceptable to preselect features on all $n$ samples and then perform cross-validated ridge regression without nesting.**\n    This option misattributes the source of the bias. The bias described in the problem arises from the *a priori* feature selection step, which is independent of the subsequent modeling choice ($L_1$, $L_2$, etc.). The bias would exist even if a simple logistic regression with no regularization were used. Furthermore, $L_2$ regularization does not \"prevent\" leakage that has already occurred in a preceding, separate step. The recommendation to preselect on all $n$ samples is incorrect.\n    **Verdict: Incorrect.**\n\n*   **D. Optimistic bias can be fully corrected by stratification and permutation testing in the outer CV, so it is acceptable to select features on all $n$ samples prior to CV; a correct pipeline is to preselect once, then perform stratified $K$-fold CV with permutation-based $p$-values to debias estimates.**\n    This option misunderstands the roles of stratification and permutation testing. Stratification improves the quality of the CV estimate by reducing its variance, but it does not correct for bias caused by data leakage. Permutation testing is a tool for hypothesis testing (i.e., generating a $p$-value for the observed performance), not for correcting a biased performance estimate. The performance estimate obtained from the leaky pipeline remains inflated; a permutation test would simply assess if this inflated value is statistically significant, it would not \"debias\" it. The root cause of the bias—pre-selection on the full dataset—is not addressed.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "4553958"}, {"introduction": "Why exactly is regularization a cornerstone of modern statistical modeling? This final, comprehensive exercise demonstrates the answer by exploring the bias-variance trade-off in a simulated radiomics setting with correlated features [@problem_id:4553887]. You will first derive the theoretical variance for Ordinary Least Squares and Ridge estimators and then implement a simulation to visualize how multicollinearity inflates estimation variance. By comparing this with the results from Ridge and LASSO regression, you will gain a concrete understanding of how regularization stabilizes models and leads to more reliable predictions.", "problem": "Consider a simulated linear model for radiomics features extracted from wavelet decompositions. Let $X \\in \\mathbb{R}^{n \\times p}$ denote a feature matrix whose columns represent standardized wavelet-based radiomics features and are mutually correlated according to a Toeplitz structure with parameter $\\rho \\in (0,1)$, that is, the population feature covariance satisfies $\\Sigma_{ij} = \\rho^{|i-j|}$. Suppose the data are generated by the model $y = X \\beta^{\\star} + \\varepsilon$, where $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is a fixed but unknown vector with sparse nonzero entries and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ is zero-mean Gaussian noise with variance $\\sigma^{2}$. All columns of $X$ are centered to zero mean and scaled to unit $\\ell_{2}$ norm, and the response $y$ is centered.\n\nThe goal is to characterize variance inflation of the unregularized ordinary least squares estimator when features are highly correlated and to demonstrate how setting a regularization parameter $\\lambda > 0$ in ridge regression stabilizes the covariance of the estimator. Use only the following fundamental base: the normal linear model $y = X \\beta^{\\star} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, the definition of the ordinary least squares estimator as the minimizer of the squared error, the definition of ridge regression (also known as Tikhonov regularization) as the minimizer of the squared error with an $\\ell_{2}$ penalty, and the definition of the Least Absolute Shrinkage and Selection Operator (LASSO) as the minimizer of the squared error with an $\\ell_{1}$ penalty. Derive the covariance of the estimator starting from these definitions and the Gaussian noise model, and express the covariance in terms of spectral quantities of the feature Gram matrix $X^{\\top} X$ and the noise variance $\\sigma^{2}$. Do not assume or use any shortcut formulas not derivable from these bases.\n\nImplement a program that:\n- Simulates $X$ with the specified Toeplitz correlation structure for the features by drawing $n$ samples from a $p$-dimensional zero-mean multivariate normal distribution with covariance matrix $\\Sigma$ where $\\Sigma_{ij} = \\rho^{|i-j|}$, then column-centers and scales each feature to unit $\\ell_{2}$ norm.\n- Sets a fixed sparse ground-truth vector $\\beta^{\\star}$ with nonzero entries at indices $\\{1,4,6,11,16\\}$ equal to $1$, and zeros elsewhere (using one-based indexing for description; in implementation, use zero-based indexing).\n- Generates $y$ using the model with noise variance $\\sigma^{2}$.\n- Computes the exact covariance of the ordinary least squares estimator using the derived spectral expression based on $X^{\\top} X$ and $\\sigma^{2}$.\n- Computes the exact covariance of the ridge estimator for a given $\\lambda \\ge 0$ using the derived spectral expression based on $X^{\\top} X$, $\\sigma^{2}$, and $\\lambda$.\n- Reports two scalar metrics for each covariance: the trace of the covariance and its largest eigenvalue.\n- For the LASSO estimator, approximates the variability of the estimator by Monte Carlo simulation: generate $B$ independent replicates of the noise term, form $B$ independent responses $y^{(b)} = X \\beta^{\\star} + \\varepsilon^{(b)}$, fit the LASSO for each replicate via coordinate descent to obtain $\\hat{\\beta}_{\\ell_{1}}^{(b)}$, and estimate the average coefficient-wise variance by averaging the sample variances of the coefficients across the $p$ coordinates.\n\nYour program must evaluate the following test suite of parameter configurations:\n- Test $A$ (happy path, highly correlated features): $n = 120$, $p = 20$, $\\rho = 0.95$, $\\sigma^{2} = 0.25$, $\\lambda = 0.1$, LASSO penalty level $\\lambda_{L1} = 0.5$, number of Monte Carlo replicates $B = 300$.\n- Test $B$ (boundary condition for ridge regularization): same $n$, $p$, $\\rho$, and $\\sigma^{2}$ as Test $A$, but with $\\lambda = 0$.\n- Test $C$ (edge case, near-singular Gram matrix): $n = 60$, $p = 20$, $\\rho = 0.999$, $\\sigma^{2} = 0.25$, $\\lambda = 0.1$.\n- Test $D$ (moderate correlation): $n = 120$, $p = 20$, $\\rho = 0.5$, $\\sigma^{2} = 0.25$, $\\lambda = 0.1$.\n\nFor each test, compute the following boolean outputs:\n- For Tests $A$, $C$, and $D$: output two booleans that are $\\text{True}$ if and only if (i) the trace of the ridge covariance is strictly less than the trace of the ordinary least squares covariance, and (ii) the largest eigenvalue of the ridge covariance is strictly less than the largest eigenvalue of the ordinary least squares covariance.\n- For Test $B$: output two booleans that are $\\text{True}$ if and only if (i) the trace of the ridge covariance equals the trace of the ordinary least squares covariance, and (ii) the largest eigenvalue of the ridge covariance equals the largest eigenvalue of the ordinary least squares covariance. Equality should be evaluated up to a numerical tolerance of $10^{-10}$.\n- Additionally for Test $A$: output a third boolean that is $\\text{True}$ if and only if the Monte Carlo estimated average coefficient-wise variance of the LASSO estimator (with $\\lambda_{L1}$ as specified) is strictly less than the ordinary least squares average variance computed as the trace of the ordinary least squares covariance divided by $p$.\n\nUse a fixed random seed $s = 123$ for all randomness to ensure reproducibility. Your program should produce a single line of output containing the nine boolean results in the order: Test $A$ ridge trace comparison, Test $A$ ridge largest eigenvalue comparison, Test $A$ LASSO average variance comparison, Test $B$ ridge trace equality, Test $B$ ridge largest eigenvalue equality, Test $C$ ridge trace comparison, Test $C$ ridge largest eigenvalue comparison, Test $D$ ridge trace comparison, Test $D$ ridge largest eigenvalue comparison. The final output must be a comma-separated list enclosed in square brackets, for example, $\\left[\\text{True},\\text{False},\\ldots\\right]$. Angles are not involved, and no physical units are required; all outputs are booleans.", "solution": "The problem requires the derivation of the covariance matrices for the Ordinary Least Squares (OLS) and Ridge regression estimators, and their subsequent use in a computational comparison under different data generation parameters. The variability of the LASSO estimator is also to be assessed via Monte Carlo simulation.\n\nThe statistical model is the normal linear model, given by $y = X \\beta^{\\star} + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$ is the feature matrix, $y \\in \\mathbb{R}^{n}$ is the response vector, $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is the true but unknown sparse coefficient vector, and $\\varepsilon$ is a vector of i.i.d. Gaussian noise with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. The columns of $X$ are centered and scaled to have unit $\\ell_{2}$ norm.\n\n### Covariance of the Ordinary Least Squares (OLS) Estimator\n\nThe OLS estimator, denoted $\\hat{\\beta}_{\\text{OLS}}$, is defined as the vector $\\beta$ that minimizes the Residual Sum of Squares (RSS):\n$$ \\hat{\\beta}_{\\text{OLS}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\| y - X \\beta \\|_{2}^{2} $$\nThe objective function is $L(\\beta) = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta$. To find the minimum, we take the gradient with respect to $\\beta$ and set it to zero:\n$$ \\nabla_{\\beta} L(\\beta) = -2 X^{\\top}y + 2 X^{\\top}X\\beta = 0 $$\nThis yields the normal equations: $X^{\\top}X\\hat{\\beta}_{\\text{OLS}} = X^{\\top}y$. Assuming the Gram matrix $G = X^{\\top}X$ is invertible (which holds if $X$ has full column rank), the unique solution is:\n$$ \\hat{\\beta}_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y $$\nTo find the covariance of $\\hat{\\beta}_{\\text{OLS}}$, we first express the estimator in terms of the true parameters and the noise. Substituting $y = X \\beta^{\\star} + \\varepsilon$:\n$$ \\hat{\\beta}_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}(X \\beta^{\\star} + \\varepsilon) = (X^{\\top}X)^{-1}(X^{\\top}X)\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\nThe expectation of the estimator is $\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = \\mathbb{E}[\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon] = \\beta^{\\star}$, since $\\mathbb{E}[\\varepsilon] = 0$. This confirms that the OLS estimator is unbiased. The covariance matrix is defined as $\\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\mathbb{E}[(\\hat{\\beta}_{\\text{OLS}} - \\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}])(\\hat{\\beta}_{\\text{OLS}} - \\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}])^{\\top}]$.\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\mathbb{E}[((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)^{\\top}] $$\n$$ = \\mathbb{E}[(X^{\\top}X)^{-1}X^{\\top}\\varepsilon\\varepsilon^{\\top}X(X^{\\top}X)^{-1}] $$\nSince $X$ is considered fixed (conditioned upon), we can move it outside the expectation:\n$$ = (X^{\\top}X)^{-1}X^{\\top}\\mathbb{E}[\\varepsilon\\varepsilon^{\\top}]X(X^{\\top}X)^{-1} $$\nGiven that $\\mathbb{E}[\\varepsilon\\varepsilon^{\\top}] = \\text{Cov}(\\varepsilon) = \\sigma^2 I_n$:\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1}X^{\\top}X(X^{\\top}X)^{-1} $$\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\sigma^2 (X^{\\top}X)^{-1} $$\nLet the spectral decomposition of the symmetric Gram matrix $G = X^{\\top}X$ be $G = V S V^{\\top}$, where $S = \\text{diag}(s_1, \\dots, s_p)$ is the diagonal matrix of eigenvalues $s_i \\ge 0$, and $V$ is the orthogonal matrix of corresponding eigenvectors. Then $G^{-1} = V S^{-1} V^{\\top}$. The covariance is:\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\sigma^2 V S^{-1} V^{\\top} $$\nThe eigenvalues of this covariance matrix are $\\sigma^2/s_i$. The trace is $\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}})) = \\sigma^2 \\sum_{i=1}^{p} \\frac{1}{s_i}$. The largest eigenvalue is $\\lambda_{\\max}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}})) = \\sigma^2 / s_{\\min}$, where $s_{\\min}$ is the smallest eigenvalue of $X^{\\top}X$. High feature correlation (as induced by large $\\rho$) leads to a near-singular $X^{\\top}X$, making $s_{\\min}$ very small and thus inflating the variance of the estimator.\n\n### Covariance of the Ridge Regression Estimator\n\nThe ridge regression estimator, $\\hat{\\beta}_{\\text{ridge}}$, minimizes the RSS penalized by the squared $\\ell_2$-norm of the coefficients, with a regularization parameter $\\lambda > 0$:\n$$ \\hat{\\beta}_{\\text{ridge}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{2}^{2} \\right) $$\nSetting the gradient of the objective function to zero gives:\n$$ -2 X^{\\top}y + 2 X^{\\top}X\\beta + 2 \\lambda \\beta = 0 \\implies (X^{\\top}X + \\lambda I_p)\\hat{\\beta}_{\\text{ridge}} = X^{\\top}y $$\nFor $\\lambda > 0$, the matrix $(X^{\\top}X + \\lambda I_p)$ is always invertible. The solution is:\n$$ \\hat{\\beta}_{\\text{ridge}} = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}y $$\nSubstituting $y = X \\beta^{\\star} + \\varepsilon$:\n$$ \\hat{\\beta}_{\\text{ridge}} = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}(X\\beta^{\\star} + \\varepsilon) = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}X\\beta^{\\star} + (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}\\varepsilon $$\nThe expectation is $\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}] = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}X\\beta^{\\star}$. This is not equal to $\\beta^{\\star}$, so the ridge estimator is biased. Its covariance is calculated with respect to this mean:\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{ridge}}) = \\mathbb{E}[(\\hat{\\beta}_{\\text{ridge}} - \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}])(\\hat{\\beta}_{\\text{ridge}} - \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}])^{\\top}] $$\nThe random part of the estimator is $(X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}\\varepsilon$. Following a similar derivation as for OLS:\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{ridge}}) = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}\\mathbb{E}[\\varepsilon\\varepsilon^{\\top}]X(X^{\\top}X + \\lambda I_p)^{-1} $$\n$$ = \\sigma^2 (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}X(X^{\\top}X + \\lambda I_p)^{-1} $$\nUsing the spectral decomposition $X^{\\top}X = V S V^{\\top}$, we have $(X^{\\top}X + \\lambda I_p)^{-1} = V(S + \\lambda I_p)^{-1}V^{\\top}$. Substituting this into the covariance expression:\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{ridge}}) = \\sigma^2 [V(S+\\lambda I_p)^{-1}V^{\\top}] [VSV^{\\top}] [V(S+\\lambda I_p)^{-1}V^{\\top}] $$\n$$ = \\sigma^2 V (S+\\lambda I_p)^{-1} S (S+\\lambda I_p)^{-1} V^{\\top} $$\nThe eigenvalues of the ridge covariance matrix are $\\nu_i = \\sigma^2 \\frac{s_i}{(s_i+\\lambda)^2}$. The trace is $\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{ridge}})) = \\sum_{i=1}^p \\nu_i = \\sigma^2 \\sum_{i=1}^p \\frac{s_i}{(s_i+\\lambda)^2}$.\nFor any $s_i > 0$ and $\\lambda > 0$, we have $\\frac{s_i}{(s_i+\\lambda)^2} < \\frac{1}{s_i}$. Summing over all $i$ shows that $\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{ridge}})) < \\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}}))$. Also, the largest eigenvalue of the ridge covariance is strictly smaller than that of the OLS covariance. The term $\\lambda$ stabilizes the matrix inversion by shifting the eigenvalues of $X^{\\top}X$ away from zero, thereby reducing the estimator's variance.\n\n### Variability of the LASSO Estimator\n\nThe LASSO estimator, $\\hat{\\beta}_{\\ell_1}$, minimizes the RSS with an $\\ell_1$-norm penalty:\n$$ \\hat{\\beta}_{\\ell_1} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\| y - X \\beta \\|_{2}^{2} + \\lambda_{L1} \\| \\beta \\|_{1} \\right) $$\nwhere $\\| \\beta \\|_{1} = \\sum_{j=1}^p |\\beta_j|$. Unlike OLS and Ridge, the LASSO estimator does not have a closed-form solution. Its statistical properties, including its variance, are more complex to analyze. The problem prescribes a Monte Carlo simulation approach to estimate its average variability. This involves generating $B$ independent noise replicates $\\varepsilon^{(b)}$, forming responses $y^{(b)} = X \\beta^{\\star} + \\varepsilon^{(b)}$, and for each, computing the LASSO estimate $\\hat{\\beta}_{\\ell_1}^{(b)}$ using coordinate descent. The coordinate-wise update rule for the $j$-th coefficient, given that columns of $X$ have unit norm, is given by the soft-thresholding operator $S_{\\lambda_{L1}}(\\cdot)$:\n$$ \\beta_j \\leftarrow S_{\\lambda_{L1}}(X_j^{\\top}(y - \\sum_{k \\neq j} X_k \\beta_k)) = \\text{sgn}(\\rho_j) \\max(|\\rho_j| - \\lambda_{L1}, 0) $$\nwhere $\\rho_j = X_j^{\\top}(y - \\sum_{k \\neq j} X_k \\beta_k)$. After obtaining the set of $B$ estimated vectors $\\{\\hat{\\beta}_{\\ell_1}^{(b)}\\}_{b=1}^B$, the sample variance for each coefficient, $\\widehat{\\text{Var}}(\\hat{\\beta}_{\\ell_1,j})$, is computed. The average coefficient-wise variance is then $\\frac{1}{p}\\sum_{j=1}^p \\widehat{\\text{Var}}(\\hat{\\beta}_{\\ell_1,j})$. This is compared against the average OLS variance, which is $\\frac{1}{p}\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}}))$.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    RNG = np.random.default_rng(123)\n\n    test_cases = [\n        # Test A: Happy path, high correlation\n        {'n': 120, 'p': 20, 'rho': 0.95, 'sigma2': 0.25, 'lambda_': 0.1, 'lambda_lasso': 0.5, 'B': 300, 'test_id': 'A'},\n        # Test B: Boundary condition for Ridge (lambda=0)\n        {'n': 120, 'p': 20, 'rho': 0.95, 'sigma2': 0.25, 'lambda_': 0.0, 'lambda_lasso': 0.5, 'B': 300, 'test_id': 'B'},\n        # Test C: Edge case, near-singular Gram matrix\n        {'n': 60, 'p': 20, 'rho': 0.999, 'sigma2': 0.25, 'lambda_': 0.1, 'lambda_lasso': None, 'B': None, 'test_id': 'C'},\n        # Test D: Moderate correlation\n        {'n': 120, 'p': 20, 'rho': 0.5, 'sigma2': 0.25, 'lambda_': 0.1, 'lambda_lasso': None, 'B': None, 'test_id': 'D'}\n    ]\n\n    all_results = []\n    for params in test_cases:\n        results = run_test(params, RNG)\n        all_results.extend(results)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef run_test(params, rng):\n    \"\"\"\n    Executes a single test case based on the provided parameters.\n    \"\"\"\n    n, p, rho, sigma2, lambda_, lambda_lasso, B = params['n'], params['p'], params['rho'], params['sigma2'], params['lambda_'], params['lambda_lasso'], params['B']\n    test_id = params['test_id']\n\n    # 1. Simulate data.\n    # Construct the Toeplitz population covariance matrix.\n    cov_matrix = toeplitz(rho ** np.arange(p))\n    \n    # Generate feature matrix X from a multivariate normal distribution.\n    X_raw = rng.multivariate_normal(np.zeros(p), cov_matrix, n)\n    \n    # Center and scale columns of X to unit l2-norm.\n    X_centered = X_raw - np.mean(X_raw, axis=0)\n    col_norms = np.linalg.norm(X_centered, axis=0)\n    # Avoid division by zero for columns with zero norm (highly unlikely).\n    col_norms[col_norms == 0] = 1.0\n    X = X_centered / col_norms\n    \n    # Define the sparse ground-truth coefficient vector beta_star.\n    beta_star = np.zeros(p)\n    beta_star_indices = [0, 3, 5, 10, 15]  # 0-indexed for {1, 4, 6, 11, 16}.\n    beta_star[beta_star_indices] = 1.0\n    \n    # 2. Compute covariance metrics for OLS and Ridge.\n    # Compute the Gram matrix and its spectral decomposition.\n    gram_matrix = X.T @ X\n    s, V = np.linalg.eigh(gram_matrix)  # s: eigenvalues (ascending), V: eigenvectors.\n\n    # OLS covariance metrics.\n    # Eigenvalues of Cov(OLS) are sigma^2 / s_i.\n    s_inv = np.array([1.0/val if val > 1e-12 else 0 for val in s])\n    trace_ols = sigma2 * np.sum(s_inv)\n    lambda_max_ols = sigma2 * np.max(s_inv)\n\n    # Ridge covariance metrics.\n    # Eigenvalues of Cov(Ridge) are sigma^2 * s_i / (s_i + lambda)^2.\n    ridge_eigvals = sigma2 * s / ((s + lambda_)**2)\n    trace_ridge = np.sum(ridge_eigvals)\n    lambda_max_ridge = np.max(ridge_eigvals)\n\n    # 3. Perform comparisons and store boolean results.\n    results_bool = []\n    if test_id in ['A', 'C', 'D']:\n        # For lambda > 0, Ridge variance should be smaller.\n        bool1 = trace_ridge < trace_ols\n        bool2 = lambda_max_ridge < lambda_max_ols\n        results_bool.extend([bool1, bool2])\n    elif test_id == 'B':\n        # For lambda = 0, Ridge is identical to OLS.\n        tol = 1e-10\n        bool1 = abs(trace_ridge - trace_ols) < tol\n        bool2 = abs(lambda_max_ridge - lambda_max_ols) < tol\n        results_bool.extend([bool1, bool2])\n        \n    # 4. LASSO Monte Carlo simulation (for Test A only).\n    if test_id == 'A':\n        # Average variance for OLS estimator.\n        avg_var_ols = trace_ols / p\n        \n        lasso_betas = np.zeros((B, p))\n        \n        # Precompute for efficiency in coordinate descent.\n        XtX = X.T @ X\n        \n        for i in range(B):\n            # Generate new noise and response for each replicate.\n            epsilon = rng.normal(0, np.sqrt(sigma2), n)\n            y = X @ beta_star + epsilon\n            \n            # Precompute X.T @ y for the current replicate.\n            XTy = X.T @ y\n            \n            # Solve LASSO via coordinate descent.\n            beta_lasso = np.zeros(p)\n            for _ in range(100):  # Number of full cycles over coordinates.\n                max_change = 0\n                for j in range(p):\n                    beta_old_j = beta_lasso[j]\n                    # Compute argument for soft-thresholding.\n                    rho_j = XTy[j] - (XtX[j, :] @ beta_lasso - XtX[j, j] * beta_old_j)\n                    \n                    # Apply soft-thresholding.\n                    new_beta_j = np.sign(rho_j) * max(abs(rho_j) - lambda_lasso, 0)\n                    beta_lasso[j] = new_beta_j\n                    max_change = max(max_change, abs(new_beta_j - beta_old_j))\n                \n                # Check for convergence.\n                if max_change < 1e-7:\n                    break\n\n            lasso_betas[i, :] = beta_lasso\n            \n        # Estimate average coefficient-wise variance from Monte Carlo samples.\n        sample_variances = np.var(lasso_betas, axis=0, ddof=1)\n        avg_var_lasso = np.mean(sample_variances)\n        \n        bool3 = avg_var_lasso < avg_var_ols\n        # Insert the LASSO comparison as the third result for Test A.\n        results_bool.insert(2, bool3)\n\n    return results_bool\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4553887"}]}