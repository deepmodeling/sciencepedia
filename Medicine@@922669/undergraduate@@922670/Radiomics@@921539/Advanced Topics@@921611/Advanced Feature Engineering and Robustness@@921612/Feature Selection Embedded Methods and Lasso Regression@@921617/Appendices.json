{"hands_on_practices": [{"introduction": "LASSO's ability to select features seems complex, but it is often achieved through a surprisingly simple and efficient algorithm called coordinate descent. This practice takes you under the hood to derive and apply the core update rule, which reveals how individual coefficients are systematically evaluated and shrunk. By understanding this soft-thresholding mechanism [@problem_id:4538688], you gain a concrete intuition for how LASSO achieves sparsity.", "problem": "A radiomics pipeline is used to predict a continuous marker of tumor aggressiveness from computed tomography texture features. To perform embedded feature selection, the model employs the Least Absolute Shrinkage and Selection Operator (LASSO) regression, which minimizes the penalized empirical risk $$\\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$ where $y \\in \\mathbb{R}^{n}$ is the response, $X \\in \\mathbb{R}^{n \\times p}$ is the feature matrix, $\\beta \\in \\mathbb{R}^{p}$ are the coefficients, $n$ is the sample size, and $\\lambda > 0$ is the regularization parameter. Features have been centered so that the intercept is unpenalized and can be ignored for this update.\n\nYou are implementing coordinate descent at the current iterate, holding all coefficients except $\\beta_{j}$ fixed. Let the partial residual with respect to feature $j$ be defined as $$r^{(j)} = y - \\sum_{k \\neq j} x_{k}\\beta_{k},$$ where $x_{k} \\in \\mathbb{R}^{n}$ denotes the $k$-th column of $X$. The one-dimensional subproblem for $\\beta_{j}$ is $$\\min_{\\beta_{j} \\in \\mathbb{R}} \\;\\; \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} + \\lambda |\\beta_{j}|.$$\n\nStarting from the stated objective and definitions, derive the coordinate descent update for $\\beta_{j}$ by solving this one-dimensional convex problem and interpreting the solution in terms of soft-thresholding of the partial residual. Then, compute the numerical update using the following data for a single feature in a cohort of $n = 8$ patients:\n- Feature column $x_{j} = [\\,0.5,\\,-1.1,\\,0.7,\\,0.0,\\,1.4,\\,-0.6,\\,0.8,\\,-1.7\\,]$,\n- Partial residual $r^{(j)} = [\\,2.3,\\,-0.9,\\,1.2,\\,0.0,\\,3.1,\\,-1.5,\\,1.0,\\,-2.8\\,]$,\n- Regularization parameter $\\lambda = 0.3$.\n\nProvide the updated value of $\\beta_{j}$ for this coordinate descent step. Round your answer to four significant figures. Finally, explain why this update mechanism is computationally efficient in high-dimensional radiomics settings where $p \\gg n$.", "solution": "The problem asks for three things: first, the derivation of the coordinate descent update rule for a LASSO regression coefficient $\\beta_j$; second, the numerical computation of this update for a given feature; and third, an explanation of the computational efficiency of this method in high-dimensional settings.\n\nThe problem is validated as scientifically grounded, well-posed, and objective. All definitions and premises are standard in statistical learning theory and the context of LASSO regression. The provided data is self-contained and consistent. Therefore, a solution can be derived.\n\n**Part 1: Derivation of the Coordinate Descent Update**\n\nThe objective is to solve the one-dimensional subproblem for the coefficient $\\beta_j$, holding all other coefficients $\\beta_k$ for $k \\neq j$ fixed. The objective function to minimize with respect to $\\beta_j$ is given by:\n$$L(\\beta_j) = \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} + \\lambda |\\beta_{j}|$$\nwhere the partial residual is $r^{(j)} = y - \\sum_{k \\neq j} x_{k}\\beta_{k}$.\nThis objective function is convex, as it is the sum of a convex quadratic function and a convex absolute value function. However, the absolute value term $|\\beta_j|$ makes the function non-differentiable at $\\beta_j = 0$. We can find the minimum by using subgradient calculus. The condition for $\\beta_j$ to be a minimizer is that $0$ must be contained in the subdifferential of $L(\\beta_j)$, denoted $\\partial L(\\beta_j)$.\n\nThe subdifferential is computed as:\n$$ \\partial L(\\beta_j) = \\frac{d}{d\\beta_j} \\left( \\frac{1}{2n}\\sum_{i=1}^{n}\\left(r^{(j)}_{i} - x_{ij}\\beta_{j}\\right)^{2} \\right) + \\lambda \\cdot \\partial(|\\beta_j|) $$\nThe derivative of the quadratic part is:\n$$ \\frac{1}{2n}\\sum_{i=1}^{n} 2(r^{(j)}_{i} - x_{ij}\\beta_{j})(-x_{ij}) = -\\frac{1}{n}\\sum_{i=1}^{n} (x_{ij}r^{(j)}_{i} - x_{ij}^2\\beta_j) = \\frac{1}{n} \\left( \\beta_j \\sum_{i=1}^{n}x_{ij}^2 - \\sum_{i=1}^{n}x_{ij}r^{(j)}_{i} \\right) $$\nLet's define $c_j = \\sum_{i=1}^{n}x_{ij}r^{(j)}_{i} = x_j^T r^{(j)}$ and $S_j = \\sum_{i=1}^{n}x_{ij}^2 = \\|x_j\\|_2^2$. The derivative becomes $\\frac{1}{n} (S_j \\beta_j - c_j)$.\n\nThe subdifferential of the absolute value function is:\n$$ \\partial(|\\beta_j|) = \\begin{cases} \\{1\\} & \\text{if } \\beta_j > 0 \\\\ \\{-1\\} & \\text{if } \\beta_j < 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases} $$\n\nNow, we set $0 \\in \\partial L(\\beta_j)$:\n$$ 0 \\in \\frac{1}{n} (S_j \\beta_j - c_j) + \\lambda \\cdot \\partial(|\\beta_j|) $$\n\nWe analyze three cases for the value of $\\beta_j$:\n\n**Case 1: $\\beta_j > 0$**\nThe subgradient is a single value, $\\partial(|\\beta_j|) = \\{1\\}$. The condition becomes an equation:\n$$ 0 = \\frac{1}{n} (S_j \\beta_j - c_j) + \\lambda $$\n$$ S_j \\beta_j - c_j = -n\\lambda $$\n$$ \\beta_j = \\frac{c_j - n\\lambda}{S_j} $$\nFor this solution to be consistent with the assumption $\\beta_j > 0$, we must have $c_j - n\\lambda > 0$, which implies $c_j > n\\lambda$.\n\n**Case 2: $\\beta_j < 0$**\nThe subgradient is $\\partial(|\\beta_j|) = \\{-1\\}$. The condition becomes:\n$$ 0 = \\frac{1}{n} (S_j \\beta_j - c_j) - \\lambda $$\n$$ S_j \\beta_j - c_j = n\\lambda $$\n$$ \\beta_j = \\frac{c_j + n\\lambda}{S_j} $$\nFor this solution to be consistent with the assumption $\\beta_j < 0$, we must have $c_j + n\\lambda < 0$, which implies $c_j < -n\\lambda$.\n\n**Case 3: $\\beta_j = 0$**\nThe subgradient is an interval, $\\partial(|\\beta_j|) = [-1, 1]$. The condition becomes an inclusion:\n$$ 0 \\in \\frac{1}{n} (S_j \\cdot 0 - c_j) + \\lambda [-1, 1] $$\n$$ 0 \\in -\\frac{c_j}{n} + [-\\lambda, \\lambda] $$\nThis means there exists some value $g \\in [-\\lambda, \\lambda]$ such that $0 = -c_j/n + g$, or $g = c_j/n$. The condition for this case to hold is that this value of $g$ must be in its allowed range:\n$$ -\\lambda \\le \\frac{c_j}{n} \\le \\lambda \\iff |c_j| \\le n\\lambda $$\n\nCombining these three cases yields the complete solution for $\\beta_j$:\n$$ \\beta_j = \\begin{cases} \\frac{c_j - n\\lambda}{S_j} & \\text{if } c_j > n\\lambda \\\\ \\frac{c_j + n\\lambda}{S_j} & \\text{if } c_j < -n\\lambda \\\\ 0 & \\text{if } |c_j| \\le n\\lambda \\end{cases} $$\nThis can be written compactly using the soft-thresholding operator, $S_{\\alpha}(z) = \\text{sign}(z) \\max(0, |z|-\\alpha)$. The solution is:\n$$ \\beta_j = \\frac{S_{n\\lambda}(c_j)}{S_j} = \\frac{S_{n\\lambda}(x_j^T r^{(j)})}{\\|x_j\\|_2^2} $$\nThis expression represents the coordinate-wise update rule. It shows that the update is found by computing the dot product of the feature vector $x_j$ with its partial residual $r^{(j)}$, soft-thresholding this value, and then normalizing by the squared Euclidean norm of the feature vector.\n\n**Part 2: Numerical Calculation**\n\nGiven the data:\n- Sample size $n = 8$\n- Feature vector $x_{j} = [\\,0.5,\\,-1.1,\\,0.7,\\,0.0,\\,1.4,\\,-0.6,\\,0.8,\\,-1.7\\,]$\n- Partial residual $r^{(j)} = [\\,2.3,\\,-0.9,\\,1.2,\\,0.0,\\,3.1,\\,-1.5,\\,1.0,\\,-2.8\\,]$\n- Regularization parameter $\\lambda = 0.3$\n\nFirst, we compute $c_j = x_j^T r^{(j)}$:\n$$ c_j = (0.5)(2.3) + (-1.1)(-0.9) + (0.7)(1.2) + (0.0)(0.0) + (1.4)(3.1) + (-0.6)(-1.5) + (0.8)(1.0) + (-1.7)(-2.8) $$\n$$ c_j = 1.15 + 0.99 + 0.84 + 0.0 + 4.34 + 0.90 + 0.80 + 4.76 = 13.78 $$\n\nNext, we compute $S_j = \\|x_j\\|_2^2$:\n$$ S_j = (0.5)^2 + (-1.1)^2 + (0.7)^2 + (0.0)^2 + (1.4)^2 + (-0.6)^2 + (0.8)^2 + (-1.7)^2 $$\n$$ S_j = 0.25 + 1.21 + 0.49 + 0.0 + 1.96 + 0.36 + 0.64 + 2.89 = 7.80 $$\n\nThen, we calculate the threshold for the soft-thresholding operator:\n$$ n\\lambda = 8 \\times 0.3 = 2.4 $$\n\nNow we apply the update rule. We compare $|c_j|$ with $n\\lambda$:\n$$ c_j = 13.78 > n\\lambda = 2.4 $$\nThis corresponds to the first case of our derived solution.\n$$ \\beta_j = \\frac{c_j - n\\lambda}{S_j} = \\frac{13.78 - 2.4}{7.80} = \\frac{11.38}{7.80} \\approx 1.458974... $$\nRounding to four significant figures, the updated value for $\\beta_j$ is $1.459$.\n\n**Part 3: Computational Efficiency in High-Dimensional Settings ($p \\gg n$)**\n\nThe coordinate descent algorithm is highly efficient for high-dimensional problems, particularly where the number of features $p$ is much larger than the number of samples $n$, for several key reasons:\n\n1.  **Avoidance of Large Matrix Operations**: The most significant advantage is that it avoids the need to compute, store, or invert the $p \\times p$ Gram matrix $X^T X$. In a $p \\gg n$ scenario, this matrix would be enormous ($p \\times p$), and operations like inversion (costing $O(p^3)$) or even its formation (costing $O(p^2 n)$) are computationally prohibitive. Coordinate descent circumvents this by breaking the problem down into $p$ one-dimensional optimizations.\n\n2.  **Low Cost per Update**: Each coordinate update only involves operations on vectors of length $n$. The main computations for updating $\\beta_j$ are the dot product $x_j^T r$ and a vector update for the residual, each costing $O(n)$ operations. A full cycle through all $p$ features thus has a total complexity of $O(np)$. This linear scaling with $p$ is far more tractable than polynomial scaling ($O(p^2)$ or $O(p^3)$). A single step of proximal gradient descent also costs $O(np)$, but coordinate descent often converges in fewer effective passes, especially when combined with active-set strategies.\n\n3.  **Efficient Residual Updates**: The partial residual $r^{(j)}$ does not need to be recomputed from scratch for each $j$. Instead, a global residual $r = y - X\\beta$ is maintained. The term required for the update, $x_j^T r^{(j)}$, can be efficiently calculated as $x_j^T r^{(j)} = x_j^T(r + x_j \\beta_j)$, where $r$ and $\\beta_j$ are the values before the update. After updating $\\beta_j$ to $\\beta_j^{\\text{new}}$, the global residual is updated via a simple and fast rank-$1$ update: $r^{\\text{new}} = r - x_j(\\beta_j^{\\text{new}} - \\beta_j)$. This costs only $O(n)$ work.\n\n4.  **Exploitation of Sparsity**: LASSO is designed to produce sparse solutions (many $\\beta_j = 0$). Coordinate descent is particularly well-suited to this. As the algorithm proceeds, many coefficients are set to zero and may remain zero over subsequent iterations (if $|x_j^T r| \\le n\\lambda$). This allows for the use of \"active set\" methods, where the algorithm cycles only over the subset of features with non-zero coefficients, drastically reducing the effective value of $p$ and accelerating convergence.\n\nIn radiomics, where it is common to extract thousands of features ($p \\gg 1000$) from a cohort of a few hundred patients ($n \\sim 100-500$), these computational advantages make coordinate descent the workhorse algorithm for fitting LASSO models.", "answer": "$$\\boxed{1.459}$$", "id": "4538688"}, {"introduction": "The coordinate descent algorithm treats each feature's coefficient individually, but does it treat them all fairly? This exercise explores the critical concept that LASSO is not scale-invariant, meaning features with larger variances are penalized differently than those with smaller variances. Through a direct calculation [@problem_id:4538694], you will see why standardizing features to a common scale is a non-negotiable step to ensure that the LASSO penalty is applied equitably.", "problem": "A radiomics analyst is fitting a linear model for a quantitative imaging biomarker, using two features measured from computed tomography scans: $x_{1}$ is the mean lesion intensity in Hounsfield Units, and $x_{2}$ is a dimensionless gray-level co-occurrence matrix energy feature. The response $y$ and each feature column are centered so that $y$ and $x_{j}$ have mean zero. The analyst uses the Least Absolute Shrinkage and Selection Operator (LASSO) penalty, whose objective for parameter vector $\\beta$ is\n$$\n\\frac{1}{2n}\\|y - X\\beta\\|^{2} + \\lambda \\sum_{j=1}^{p} |\\beta_{j}|,\n$$\nwhere $n$ is the number of cases, $X$ is the design matrix with columns $x_{1}$ and $x_{2}$, and $\\lambda \\ge 0$ is the regularization parameter. Assume that the two feature columns are orthogonal in the sense that $\\frac{1}{n} x_{1}^{\\top}x_{2} = 0$.\n\nSuppose the following empirical quantities are measured on $n$ lesions:\n$$\n\\frac{1}{n} x_{1}^{\\top} x_{1} = 10{,}000, \\quad \\frac{1}{n} x_{2}^{\\top} x_{2} = 1, \\quad \\frac{1}{n} x_{1}^{\\top} y = 500, \\quad \\frac{1}{n} x_{2}^{\\top} y = 4.\n$$\nThese values reflect that $x_{1}$ has units of Hounsfield Units and much larger variance than the dimensionless $x_{2}$.\n\nStarting from the LASSO objective definition and the given orthogonality, analyze how scaling each feature to unit variance changes both the magnitude of the LASSO coefficients and the per-feature selection thresholds in terms of $\\lambda$. Let $r_{\\text{unstd}}$ denote the ratio of the minimal penalty parameters required to set the coefficient of $x_{1}$ to zero versus that for $x_{2}$ when the original (unstandardized) features are used, and let $r_{\\text{std}}$ denote the same ratio after standardizing each feature column to have unit variance. Compute the single number $r_{\\text{unstd}} / r_{\\text{std}}$. No rounding is necessary; provide the exact value with no units.", "solution": "The problem requires an analysis of the effect of feature standardization on the LASSO regression penalty. We are asked to find the ratio of two quantities, $r_{\\text{unstd}}$ and $r_{\\text{std}}$, which themselves are ratios of regularization parameter thresholds.\n\nThe LASSO objective function to be minimized is given by:\n$$L(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|^{2} + \\lambda \\sum_{j=1}^{p} |\\beta_{j}|$$\nwhere $\\beta$ is the vector of coefficients, $X$ is the design matrix with columns representing features, $y$ is the response vector, $n$ is the number of samples, $p$ is the number of features (here $p=2$), and $\\lambda \\ge 0$ is the regularization parameter.\n\nThe solution to the LASSO problem, $\\hat{\\beta}$, must satisfy the Karush-Kuhn-Tucker (KKT) conditions. The subgradient of the objective function with respect to each coefficient $\\beta_j$ must contain zero at the solution. The subgradient of $L(\\beta)$ with respect to $\\beta_j$ is:\n$$\\frac{\\partial L}{\\partial \\beta_j} = \\frac{1}{n} (-x_j^\\top(y-X\\beta)) + \\lambda \\cdot s_j$$\nwhere $s_j$ is an element of the subdifferential of the absolute value function, i.e., $s_j = \\text{sgn}(\\beta_j)$ if $\\beta_j \\neq 0$, and $s_j \\in [-1, 1]$ if $\\beta_j = 0$.\nSetting the subgradient to zero gives the KKT condition for $\\beta_j$:\n$$\\frac{1}{n} x_j^\\top(y-X\\hat{\\beta}) = \\lambda s_j$$\nExpanding the term $X\\hat{\\beta} = \\sum_{k=1}^p x_k \\hat{\\beta}_k$:\n$$\\frac{1}{n} x_j^\\top y - \\sum_{k=1}^p \\hat{\\beta}_k \\left(\\frac{1}{n} x_j^\\top x_k\\right) = \\lambda s_j$$\nThe problem states that the features are orthogonal, which means $\\frac{1}{n} x_j^\\top x_k = 0$ for $j \\neq k$. This greatly simplifies the condition, as the sum collapses to a single term where $k=j$:\n$$\\frac{1}{n} x_j^\\top y - \\hat{\\beta}_j \\left(\\frac{1}{n} x_j^\\top x_j\\right) = \\lambda s_j$$\nLet's define the empirical covariance of feature $j$ with the response $y$ as $c_j = \\frac{1}{n} x_j^\\top y$, and the empirical variance of feature $j$ as $v_j = \\frac{1}{n} x_j^\\top x_j$. The condition becomes:\n$$c_j - v_j \\hat{\\beta}_j = \\lambda s_j$$\nThe problem asks for the minimal penalty parameter $\\lambda$ required to set a coefficient to zero. A coefficient $\\hat{\\beta}_j$ is zero if and only if $s_j \\in [-1, 1]$. In this case, the condition becomes $c_j = \\lambda s_j$, which is equivalent to $|c_j| \\le \\lambda$.\nThe minimal non-negative value of $\\lambda$ that ensures $\\hat{\\beta}_j = 0$ is therefore $\\lambda_j = |c_j|$. This value represents the threshold at which the coefficient $\\beta_j$ is shrunk to exactly zero.\n\nWith this general result, we can now analyze the two cases.\n\n**Case 1: Unstandardized Features**\nWe use the features $x_1$ and $x_2$ as given. The relevant empirical quantities are:\n$$c_1 = \\frac{1}{n} x_1^\\top y = 500$$\n$$c_2 = \\frac{1}{n} x_2^\\top y = 4$$\nThe minimal penalty parameter required to set $\\beta_1$ to zero is:\n$$\\lambda_{1, \\text{unstd}} = |c_1| = |500| = 500$$\nThe minimal penalty parameter required to set $\\beta_2$ to zero is:\n$$\\lambda_{2, \\text{unstd}} = |c_2| = |4| = 4$$\nThe ratio $r_{\\text{unstd}}$ is defined as the ratio of these thresholds:\n$$r_{\\text{unstd}} = \\frac{\\lambda_{1, \\text{unstd}}}{\\lambda_{2, \\text{unstd}}} = \\frac{500}{4} = 125$$\n\n**Case 2: Standardized Features**\nStandardizing a feature involves scaling it to have a unit variance. The problem states the features are already centered (mean zero). The sample variance of feature $x_j$ is $v_j = \\frac{1}{n} x_j^\\top x_j$. The scaling factor for each feature is its sample standard deviation, $s_j = \\sqrt{v_j}$.\nFrom the givens:\n$$v_1 = \\frac{1}{n} x_1^\\top x_1 = 10{,}000 \\implies s_1 = \\sqrt{10{,}000} = 100$$\n$$v_2 = \\frac{1}{n} x_2^\\top x_2 = 1 \\implies s_2 = \\sqrt{1} = 1$$\nThe standardized features, which we denote $x'_1$ and $x'_2$, are:\n$$x'_1 = \\frac{x_1}{s_1} = \\frac{x_1}{100}$$\n$$x'_2 = \\frac{x_2}{s_2} = \\frac{x_2}{1} = x_2$$\nWe now consider a new LASSO problem with these standardized features. The orthogonality condition is preserved:\n$$\\frac{1}{n} (x'_1)^\\top x'_2 = \\frac{1}{n} \\left(\\frac{x_1}{100}\\right)^\\top x_2 = \\frac{1}{100} \\left(\\frac{1}{n} x_1^\\top x_2\\right) = \\frac{1}{100} \\cdot 0 = 0$$\nThe minimal $\\lambda$ thresholds for the new coefficients $\\beta'_1$ and $\\beta'_2$ are determined by the new empirical covariances with the response, $c'_1$ and $c'_2$:\n$$c'_1 = \\frac{1}{n} (x'_1)^\\top y = \\frac{1}{n} \\left(\\frac{x_1}{100}\\right)^\\top y = \\frac{1}{100} \\left(\\frac{1}{n} x_1^\\top y\\right) = \\frac{500}{100} = 5$$\n$$c'_2 = \\frac{1}{n} (x'_2)^\\top y = \\frac{1}{n} x_2^\\top y = 4$$\nThe minimal penalty parameter to set $\\beta'_1$ to zero is:\n$$\\lambda_{1, \\text{std}} = |c'_1| = |5| = 5$$\nThe minimal penalty parameter to set $\\beta'_2$ to zero is:\n$$\\lambda_{2, \\text{std}} = |c'_2| = |4| = 4$$\nThe ratio $r_{\\text{std}}$ is:\n$$r_{\\text{std}} = \\frac{\\lambda_{1, \\text{std}}}{\\lambda_{2, \\text{std}}} = \\frac{5}{4}$$\n\n**Final Calculation**\nThe problem asks for the single number representing the ratio $r_{\\text{unstd}} / r_{\\text{std}}$.\n$$\\frac{r_{\\text{unstd}}}{r_{\\text{std}}} = \\frac{125}{\\frac{5}{4}} = 125 \\times \\frac{4}{5} = \\frac{500}{5} = 100$$\nThis result highlights how feature scaling dramatically affects the behavior of LASSO, which is sensitive to the scale of predictors. The unstandardized features suggest that $x_1$ is far more important than $x_2$ (by a factor of $125$), while standardizing reveals their importance to be much more comparable (a factor of $1.25$). The ratio of these ratios is $100$.", "answer": "$$\\boxed{100}$$", "id": "4538694"}, {"introduction": "Once you can fit a LASSO model on standardized data, the final question is: how much regularization should you apply? This practice moves from model fitting to model selection, introducing a powerful heuristic called the one-standard-error rule for choosing the hyperparameter $\\lambda$. This skill is vital for navigating the bias-variance trade-off to select a model that is not only accurate but also simple and robust, a crucial goal in high-dimensional fields like radiomics [@problem_id:4538731].", "problem": "A radiomics team is building a binary classifier to predict therapy response from high-dimensional image-derived features. They fit a Least Absolute Shrinkage and Selection Operator (LASSO) logistic regression model, which embeds feature selection by shrinking some coefficients to exactly zero via an $\\ell_1$-penalty controlled by a regularization parameter $\\lambda$. To estimate generalization performance, they perform $K$-fold Cross-Validation (CV), where $K$ denotes the number of partitions of the data; in each fold, the model is trained on $K-1$ parts and evaluated on the held-out part, and the fold-wise losses are averaged to approximate expected out-of-sample risk. The team considers the mean cross-validated negative log-likelihood (cross-entropy) at candidate values of $\\lambda$ and the corresponding across-fold sample standard deviation. The results (mean across $K=5$ folds, with across-fold sample standard deviation) and the number of non-zero coefficients in the fitted LASSO models are:\n\n- $\\lambda = 0.001$: mean loss $0.512$, standard deviation $0.020$, non-zero coefficients $90$.\n- $\\lambda = 0.01$: mean loss $0.505$, standard deviation $0.024$, non-zero coefficients $45$.\n- $\\lambda = 0.1$: mean loss $0.508$, standard deviation $0.022$, non-zero coefficients $20$.\n- $\\lambda = 0.2$: mean loss $0.511$, standard deviation $0.021$, non-zero coefficients $12$.\n- $\\lambda = 0.5$: mean loss $0.519$, standard deviation $0.018$, non-zero coefficients $6$.\n\nAdvanced undergraduate students are asked to reason from first principles using the following base: the empirical risk from $K$-fold Cross-Validation approximates expected generalization risk; averaging over folds produces a mean whose uncertainty can be quantified by the standard error of the mean, and selecting simpler models (fewer non-zero coefficients) typically reduces variance and improves stability when $p \\gg n$ and features are highly correlated.\n\nWhich option best describes and applies the one-standard-error rule in Cross-Validation and justifies its use in radiomics to favor simpler, more stable models over the minimum-CV-error model?\n\nA. Select $\\lambda = 0.01$ because it has the smallest mean CV loss $0.505$; in radiomics, minimizing the mean CV loss is always optimal and model complexity is irrelevant when using Cross-Validation.\n\nB. Select $\\lambda = 0.2$ because its mean CV loss $0.511$ is within one standard error of the minimum-CV-loss model and it yields fewer non-zero coefficients ($12$), which promotes stability and reproducibility in high-dimensional radiomics where small samples and correlated features amplify variance.\n\nC. Select $\\lambda = 0.5$ because choosing the largest $\\lambda$ always guarantees the best generalization due to strong regularization; Cross-Validation should only be used to check training loss, not to guide model selection.\n\nD. Select $\\lambda = 0.1$ because its mean CV loss is within one standard deviation of the minimum; in radiomics, balancing bias and variance requires using the one-standard-deviation rule instead of one-standard-error.", "solution": "The user has provided a problem concerning model selection in the context of LASSO logistic regression for a radiomics application. The task is to apply the one-standard-error rule for hyperparameter tuning using cross-validation data.\n\n### Problem Validation\n\nFirst, a rigorous validation of the problem statement is necessary.\n\n**Step 1: Extract Givens**\n- **Model**: Least Absolute Shrinkage and Selection Operator (LASSO) logistic regression.\n- **Penalty**: $\\ell_1$-penalty controlled by a regularization parameter $\\lambda$.\n- **Performance Estimation**: $K$-fold Cross-Validation (CV) with $K=5$.\n- **Performance Metric**: Mean cross-validated negative log-likelihood (cross-entropy loss), denoted as \"mean loss\".\n- **Uncertainty Metric**: Across-fold sample standard deviation of the loss.\n- **Data**: A set of candidate $\\lambda$ values and their corresponding performance metrics:\n    - For $\\lambda = 0.001$: mean loss = $0.512$, standard deviation = $0.020$, non-zero coefficients = $90$.\n    - For $\\lambda = 0.01$: mean loss = $0.505$, standard deviation = $0.024$, non-zero coefficients = $45$.\n    - For $\\lambda = 0.1$: mean loss = $0.508$, standard deviation = $0.022$, non-zero coefficients = $20$.\n    - For $\\lambda = 0.2$: mean loss = $0.511$, standard deviation = $0.021$, non-zero coefficients = $12$.\n    - For $\\lambda = 0.5$: mean loss = $0.519$, standard deviation = $0.018$, non-zero coefficients = $6$.\n- **Guiding Principles**:\n    - $K$-fold CV approximates expected generalization risk.\n    - The uncertainty of the mean CV loss is quantified by the standard error of the mean.\n    - Simpler models (fewer non-zero coefficients) are preferred to reduce variance and improve stability, especially in high-dimensional settings ($p \\gg n$) with correlated features, as is common in radiomics.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically sound. It describes a standard and widely accepted methodology in statistical learning and machine learning, namely the use of LASSO for feature selection and regularization, cross-validation for hyperparameter tuning, and the one-standard-error rule for model selection. These techniques are particularly relevant and commonly applied in fields like radiomics, which are characterized by high-dimensional feature spaces.\n- **Well-Posedness**: The problem is well-posed. It provides all the necessary numerical data (mean losses, standard deviations, number of folds, number of coefficients for various $\\lambda$) and a clear objective: apply the one-standard-error rule to this data to select the optimal model. A unique and stable solution can be derived from the given information.\n- **Objectivity**: The problem is stated in objective, precise, and standard technical language. It is free from ambiguity, subjectivity, or opinion.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the derivation of the solution.\n\n### Derivation and Solution\n\nThe goal is to select the optimal regularization parameter $\\lambda$ using the one-standard-error rule. This rule is a heuristic designed to select a more parsimonious model whose generalization performance is statistically indistinguishable from the model with the minimum cross-validated error.\n\nThe steps to apply the rule are as follows:\n1.  Identify the model with the minimum mean cross-validated loss, $L_{min}$. Let the corresponding regularization parameter be $\\lambda_{min}$.\n2.  Calculate the standard error of the mean loss for this model. The standard error ($SE$) is the standard deviation of the losses across the folds, $S$, divided by the square root of the number of folds, $K$. So, $SE_{min} = S(\\lambda_{min}) / \\sqrt{K}$.\n3.  Establish an error threshold: $L_{threshold} = L_{min} + SE_{min}$.\n4.  From the set of all models whose mean loss $L(\\lambda)$ is less than or equal to $L_{threshold}$, select the most parsimonious one. In the context of LASSO, the most parsimonious model is the one with the largest value of $\\lambda$, as this corresponds to stronger regularization and, therefore, the fewest non-zero coefficients.\n\nLet's apply these steps to the provided data.\n\n1.  **Find the minimum mean loss**:\n    By inspecting the data, the minimum mean loss is $L_{min} = 0.505$, which occurs at $\\lambda_{min} = 0.01$.\n\n2.  **Calculate the standard error**:\n    The standard deviation for the model at $\\lambda_{min} = 0.01$ is given as $S(\\lambda_{min}) = 0.024$. The number of folds is $K=5$.\n    The standard error is:\n    $$SE_{min} = \\frac{S(\\lambda_{min})}{\\sqrt{K}} = \\frac{0.024}{\\sqrt{5}}$$\n    Numerically, $\\sqrt{5} \\approx 2.236$.\n    $$SE_{min} \\approx \\frac{0.024}{2.236} \\approx 0.01073$$\n\n3.  **Establish the threshold**:\n    The threshold is the minimum loss plus one standard error:\n    $$L_{threshold} = L_{min} + SE_{min} \\approx 0.505 + 0.01073 = 0.51573$$\n\n4.  **Select the most parsimonious model within the threshold**:\n    We now identify all models with a mean loss less than or equal to $L_{threshold} \\approx 0.51573$ and then choose the one with the largest $\\lambda$ (and thus the fewest non-zero coefficients).\n\n    - $\\lambda = 0.001$: mean loss $0.512 \\le 0.51573$. Coefficients: $90$.\n    - $\\lambda = 0.01$: mean loss $0.505 \\le 0.51573$. Coefficients: $45$.\n    - $\\lambda = 0.1$: mean loss $0.508 \\le 0.51573$. Coefficients: $20$.\n    - $\\lambda = 0.2$: mean loss $0.511 \\le 0.51573$. Coefficients: $12$.\n    - $\\lambda = 0.5$: mean loss $0.519 > 0.51573$. This model is not a candidate.\n\n    The candidate models correspond to $\\lambda \\in \\{0.001, 0.01, 0.1, 0.2\\}$. We must now select the most parsimonious model from this set. Parsimony corresponds to simplicity, which in LASSO means fewer non-zero coefficients, achieved by the largest $\\lambda$. The largest $\\lambda$ in this set is $\\lambda = 0.2$. This model has only $12$ non-zero coefficients, making it much simpler than the minimum-loss model (which has $45$ coefficients).\n\nThe rationale for this choice, particularly in radiomics, is that high-dimensional data is prone to overfitting and instability in feature selection. The one-standard-error rule provides a data-driven justification for sacrificing a statistically insignificant amount of predictive accuracy for a much simpler, more stable, and potentially more generalizable and reproducible model.\n\n### Option-by-Option Analysis\n\n**A. Select $\\lambda = 0.01$ because it has the smallest mean CV loss $0.505$; in radiomics, minimizing the mean CV loss is always optimal and model complexity is irrelevant when using Cross-Validation.**\nThis option simply selects the model with the minimum CV error, ignoring the one-standard-error rule and the problem's emphasis on model simplicity. The claim that model complexity is irrelevant is fundamentally incorrect, especially in $p \\gg n$ scenarios typical of radiomics. This approach risks selecting an overfit and unstable model.\n**Verdict: Incorrect.**\n\n**B. Select $\\lambda = 0.2$ because its mean CV loss $0.511$ is within one standard error of the minimum-CV-loss model and it yields fewer non-zero coefficients ($12$), which promotes stability and reproducibility in high-dimensional radiomics where small samples and correlated features amplify variance.**\nThis option correctly implements the one-standard-error rule. As calculated above, the model with $\\lambda = 0.2$ has a mean loss of $0.511$, which is within the threshold of $0.51573$. It is the most parsimonious model (largest $\\lambda$, fewest non-zero coefficients) that meets this criterion. The justification provided correctly articulates the benefits of this approachâ€”promoting stability and reproducibility by reducing variance, which is a critical concern in high-dimensional fields like radiomics.\n**Verdict: Correct.**\n\n**C. Select $\\lambda = 0.5$ because choosing the largest $\\lambda$ always guarantees the best generalization due to strong regularization; Cross-Validation should only be used to check training loss, not to guide model selection.**\nThis option is based on multiple false premises. First, choosing the largest $\\lambda$ does not guarantee the best generalization; excessive regularization leads to underfitting (high bias) and poor performance, as evidenced by the higher loss ($0.519$) for this model. Second, the assertion that CV is only for checking training loss is a gross misunderstanding of its purpose; CV is a cornerstone technique for estimating out-of-sample error and guiding model selection (hyperparameter tuning). Third, the model at $\\lambda = 0.5$ does not satisfy the one-standard-error rule criterion.\n**Verdict: Incorrect.**\n\n**D. Select $\\lambda = 0.1$ because its mean CV loss is within one standard deviation of the minimum; in radiomics, balancing bias and variance requires using the one-standard-deviation rule instead of one-standard-error.**\nThis option confuses the \"one-standard-error\" rule with a non-standard \"one-standard-deviation\" rule. The correct heuristic uses the standard error of the mean ($SE = S/\\sqrt{K}$), not the standard deviation of the fold losses ($S$). Using the standard deviation ($0.024$ in this case) would create an overly permissive threshold and is not the conventional method. Furthermore, even under the correct one-standard-error rule, $\\lambda=0.1$ is not the final choice because $\\lambda=0.2$ also satisfies the criterion and is more parsimonious.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "4538731"}]}