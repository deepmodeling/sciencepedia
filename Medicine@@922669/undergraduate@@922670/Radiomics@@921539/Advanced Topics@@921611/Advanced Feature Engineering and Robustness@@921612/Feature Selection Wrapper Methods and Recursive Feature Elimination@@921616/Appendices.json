{"hands_on_practices": [{"introduction": "Recursive Feature Elimination (RFE) operates on a simple, powerful principle: iteratively remove the least important feature until a target subset size is reached. But how do we define \"importance\"? This exercise goes to the heart of the matter for one of the most common pairings in machine learning: RFE with a linear Support Vector Machine (SVM). By deriving the feature importance score from the SVM's own optimization objective, you will see how the magnitude of a feature's learned weight, specifically $w_j^2$, becomes a principled and intuitive measure of its contribution [@problem_id:4539669].", "problem": "A radiomics classification task produces a dataset of $n$ labeled samples $\\{(x_i, y_i)\\}_{i=1}^{n}$, where each feature vector $x_i \\in \\mathbb{R}^{p}$ is standardized feature-wise to have zero mean and unit variance across the training set, and each label $y_i \\in \\{-1, +1\\}$. A linear classifier is trained using the maximum-margin principle with hinge loss and an $\\ell_{2}$ penalty, that is, the soft-margin Support Vector Machine (SVM). Starting from the geometric definition of a separating hyperplane and the soft-margin SVM primal optimization formulation with hinge loss and $\\ell_{2}$ regularization, derive the linear decision function of the trained classifier. Then, define a wrapper method for Recursive Feature Elimination (RFE) that ranks features by the first-order change in the regularization term when the coefficient of a single feature $j$ is forced to zero, holding the current solution fixed as a local approximation. Under the given standardization of the radiomics features, provide the closed-form analytic expression, in terms of the trained coefficients, for this feature importance score for feature $j$. Your final answer must be a single analytic expression with no units.", "solution": "The problem requires us to derive a feature importance score for use in Recursive Feature Elimination (RFE) with a linear Support Vector Machine (SVM). The score is defined as the first-order change in the $\\ell_2$ regularization term of the SVM objective when a single feature's coefficient is forced to zero.\n\nFirst, let's establish the context. A linear soft-margin SVM classifier is trained by solving the following primal optimization problem:\n$$ \\min_{w, b, \\xi} \\left( \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i \\right) $$\nsubject to $y_i(w^T x_i + b) \\ge 1 - \\xi_i$ and $\\xi_i \\ge 0$ for all samples $i$. The term we are interested in is the regularization term, $R(w) = \\frac{1}{2} \\|w\\|^2$.\n\nThe RFE ranking criterion is the change in $R(w)$ when a single coefficient, $w_j$, is set to 0. Let the original optimal weight vector be $w$. If we force $w_j$ to zero, the new weight vector $w'$ has components $w'_k = w_k$ for $k \\ne j$ and $w'_j = 0$. The change in the weight vector is a perturbation $\\Delta w = w' - w$, which is a vector with $-w_j$ in the $j$-th position and zeros elsewhere.\n\nThe problem asks for the \"first-order change\" in $R(w)$. This corresponds to the first term in the multivariate Taylor series expansion of $R(w)$ around the point $w$:\n$$ \\Delta R \\approx \\nabla R(w)^T \\Delta w $$\nHere, $\\nabla R(w)$ is the gradient of the regularization term with respect to the weight vector $w$. The regularization term is:\n$$ R(w) = \\frac{1}{2} \\|w\\|^2 = \\frac{1}{2} \\sum_{k=1}^p w_k^2 $$\nIts gradient is straightforward to compute:\n$$ \\nabla R(w) = w $$\nNow, we can compute the first-order change by taking the dot product of the gradient and the perturbation vector:\n$$ \\Delta R_j \\approx w^T \\Delta w = \\sum_{k=1}^p w_k \\Delta w_k $$\nSince the perturbation vector $\\Delta w$ is non-zero only at the $j$-th component (where it is $-w_j$), the sum collapses to a single term:\n$$ \\Delta R_j \\approx w_j (-w_j) = -w_j^2 $$\nThis negative value represents the decrease in the regularization penalty when feature $j$ is removed. Feature importance scores are conventionally non-negative, where a larger value signifies greater importance. The magnitude of this change, $|-w_j^2|$, serves as a natural importance score.\n\nTherefore, the importance score $S_j$ for feature $j$ is:\n$$ S_j = w_j^2 $$\nIn each step of RFE, the feature with the smallest $w_j^2$ is considered the least important and is eliminated. The problem states that features are standardized, which is crucial as it ensures the magnitudes of the coefficients $w_j$ are comparable and reflect feature importance rather than feature scale.", "answer": "$$\\boxed{w_j^2}$$", "id": "4539669"}, {"introduction": "Wrapper methods are powerful but can be deceptively greedy. A simple forward selection algorithm, for instance, adds the single best feature at each step, but what if the best features only reveal their power when combined? This is the classic problem of feature synergy, where a model's performance is much greater than the sum of its parts. This practice presents a thought experiment where a simple greedy search fails to find a synergistic pair of features, demonstrating a critical limitation known as the \"nesting effect,\" and shows how more sophisticated algorithms can overcome it [@problem_id:4539705].", "problem": "In radiomics-based binary classification, suppose a practitioner uses a wrapper approach that evaluates feature subsets by cross-validated accuracy of a fixed classifier. Let the target be malignant versus benign lesions, encoded as a binary label $Y \\in \\{0,1\\}$ with class prior $P(Y=1)=P(Y=0)=0.5$. Consider three handcrafted features derived from texture and shape: $f_1$, $f_2$, and $f_3$.\n\nBase definitions and facts:\n- In a wrapper method, a search procedure proposes feature subsets $S \\subseteq \\{f_1,f_2,f_3,\\dots\\}$, and each subset is scored by a generalization criterion such as cross-validated accuracy of a fixed learner $\\mathcal{L}$ trained only on features in $S$.\n- Greedy sequential forward selection starts from the empty set and iteratively adds the feature that yields the largest increase in the criterion, stopping when a size constraint or a stopping rule is met.\n- Sequential forward floating selection augments forward selection with conditional deletion: after each addition, it repeatedly removes the feature whose removal most improves the criterion, as long as improvement is obtained, thereby allowing backtracking over earlier choices.\n\nConstruct a counterexample, grounded in a plausible radiomics setting, where sequential forward selection fails to include a synergistic pair of features due to individually weak marginal effects, but sequential forward floating selection can recover the pair. Your construction must satisfy all of the following:\n- Specify a concrete data-generating mechanism for $(f_1,f_2,f_3) \\mid Y$ that is scientifically plausible for radiomics and makes $f_1$ and $f_2$ individually weak but jointly strong under a fixed nonlinear learner $\\mathcal{L}$ (e.g., a Support Vector Machine with a radial basis function kernel).\n- Argue from the definitions above why forward selection, with a size budget of $k=2$ features, will fail to include the synergistic pair.\n- Explain, using the mechanics of floating search, how it can nevertheless reach the synergistic pair despite the $k=2$ budget.\n- Use explicit, self-consistent numerical performance levels (e.g., cross-validated accuracies) that are consistent with your data-generating mechanism, and ensure that every claim you make follows from the mechanism and the wrapper definitions rather than from unmotivated heuristics.\n\nWhich option below presents a valid minimal counterexample that meets all the requirements and correctly explains the failure and remedy?\n\nA. Let $Y \\in \\{0,1\\}$ with $P(Y=1)=P(Y=0)=0.5$. Conditioned on $Y$, draw $f_3 \\mid Y \\sim \\mathcal{N}(\\mu_Y,1)$ with $\\mu_1=0.4$ and $\\mu_0=0$, yielding a weak marginally predictive feature. Let $(f_1,f_2)\\mid Y$ be generated by a latent common texture factor $U \\sim \\mathcal{N}(0,1)$ and small independent noise $\\varepsilon_1,\\varepsilon_2 \\sim \\mathcal{N}(0,0.2^2)$ as follows:\n- If $Y=1$, set $f_1 = U + \\varepsilon_1$, $f_2 = U + \\varepsilon_2$ (strong positive correlation).\n- If $Y=0$, set $f_1 = U + \\varepsilon_1$, $f_2 = -U + \\varepsilon_2$ (strong negative correlation).\nThen for each $j \\in \\{1,2\\}$, the marginal $f_j \\mid Y$ has identical distribution across classes (mean $0$ and variance approximately $1.04$), so individually $f_1$ and $f_2$ are non-informative, but jointly they define two nearly orthogonal manifolds in the $(f_1,f_2)$-plane that a Support Vector Machine with radial basis function kernel can separate with high accuracy. In $10$-fold cross-validation with fixed $\\mathcal{L}$:\n- Using $\\{f_1\\}$: accuracy $\\approx 0.50$,\n- Using $\\{f_2\\}$: accuracy $\\approx 0.50$,\n- Using $\\{f_3\\}$: accuracy $\\approx 0.58$,\n- Using $\\{f_1,f_2\\}$: accuracy $\\approx 0.90$,\n- Using $\\{f_3,f_1\\}$ or $\\{f_3,f_2\\}$: accuracy $\\approx 0.59$.\nWith a budget $k=2$, sequential forward selection picks $\\{f_3\\}$ at the first step (best single feature), and then at the second step adds either $f_1$ or $f_2$ for a negligible gain (to about $0.59$), exhausting the budget and missing the synergistic pair $\\{f_1,f_2\\}$. Sequential forward floating selection, however, can add $f_3$, then add $f_1$ (reaching about $0.59$), then perform a conditional addition to include $f_2$ temporarily (forming $\\{f_1,f_2,f_3\\}$ with accuracy near $0.90$), and finally remove $f_3$ in the conditional deletion step because its removal maintains or increases the score (yielding $\\{f_1,f_2\\}$ with accuracy $\\approx 0.90$), thus honoring the $k=2$ budget at the end.\n\nB. Let $Y \\in \\{0,1\\}$ with $P(Y=1)=P(Y=0)=0.5$. Define $f_1$ and $f_2$ as thresholded texture indicators such that $Y=1$ if and only if both $f_11$ and $f_21$, otherwise $Y=0$, and $f_3$ as pure noise independent of $Y$. A linear logistic regression as $\\mathcal{L}$ will immediately capture the joint rule when both $f_1$ and $f_2$ are present; sequential forward selection will therefore always pick both $\\{f_1,f_2\\}$ within two steps because their individual p-values are small, whereas floating search is unnecessary.\n\nC. Let $Y \\in \\{0,1\\}$, take $(f_1,f_2)$ as two nearly independent weak predictors with individual accuracies about $0.55$, and $f_3$ as a strong predictor with accuracy about $0.85$. Use a linear Support Vector Machine as $\\mathcal{L}$ and recursive feature elimination (RFE). Since RFE ranks by weight magnitude, it will always retain the best pair $\\{f_1,f_2\\}$ even if they are weak alone, and forward selection’s failure cannot occur in this setup; floating search offers no advantage.\n\nD. Let $Y \\in \\{0,1\\}$ with $P(Y=1)=P(Y=0)=0.5$. Define $(f_1,f_2)\\mid Y$ exactly as in option A, and $f_3$ as pure noise with accuracy about $0.50$. Use a linear discriminant classifier as $\\mathcal{L}$. Sequential forward selection will fail to include $\\{f_1,f_2\\}$ because linear discriminants cannot exploit correlation differences, but backward elimination from the full set will remove $f_3$ first and keep $\\{f_1,f_2\\}$, so floating search is not needed and provides no remedy under any budget constraint.\n\nSelect the single best option.", "solution": "The problem statement is a valid exercise in understanding the mechanics and contrasting the performance of two prominent wrapper-based feature selection algorithms: Sequential Forward Selection (SFS) and Sequential Forward Floating Selection (SFFS). The task is to identify a valid counterexample where the simpler, greedy SFS fails to find an optimal feature subset due to the nesting effect, while the more complex SFFS succeeds due to its backtracking capability. The problem is well-posed, scientifically grounded in machine learning principles, and uses terminology appropriate to the field of radiomics.\n\nThe core of the problem rests on constructing a scenario with three features, $\\{f_1, f_2, f_3\\}$, and a binary target $Y$ where:\n1.  Features $f_1$ and $f_2$ are individually uninformative but highly informative when used together (synergy). This is often called an XOR-like or interaction problem. Their performance is denoted by $\\text{Acc}(\\{f_1\\}) \\approx \\text{Acc}(\\{f_2\\}) \\approx 0.5$ and $\\text{Acc}(\\{f_1, f_2\\}) \\gg 0.5$.\n2.  Feature $f_3$ is individually more informative than either $f_1$ or $f_2$ alone: $\\text{Acc}(\\{f_3\\})  \\text{Acc}(\\{f_1\\})$ and $\\text{Acc}(\\{f_3\\})  \\text{Acc}(\\{f_2\\})$.\n3.  The optimal two-feature set is $\\{f_1, f_2\\}$, meaning $\\text{Acc}(\\{f_1, f_2\\})$ is the highest among all two-feature sets.\n4.  Adding $f_1$ or $f_2$ to $f_3$ yields little to no improvement over $f_3$ alone, and is significantly worse than the synergistic pair: $\\text{Acc}(\\{f_1, f_2\\})  \\text{Acc}(\\{f_3, f_1\\})$ and $\\text{Acc}(\\{f_1, f_2\\})  \\text{Acc}(\\{f_3, f_2\\})$.\n\nUnder these conditions, SFS, when searching for a $k=2$ feature set, will proceed as follows:\n-   **Step 1:** It evaluates all single-feature subsets. By condition (2), it will select $\\{f_3\\}$ as the best one-feature set.\n-   **Step 2:** It adds the feature that, combined with $\\{f_3\\}$, yields the greatest accuracy. It will evaluate $\\text{Acc}(\\{f_3, f_1\\})$ and $\\text{Acc}(\\{f_3, f_2\\})$. It will select one of these sets.\nBecause SFS can never remove $f_3$, it is trapped by its initial choice and will fail to discover the superior subset $\\{f_1, f_2\\}$. This is the \"nesting effect\" of greedy search.\n\nSFFS, by contrast, can escape this trap. After a forward step (addition), it performs backward steps (deletions) which allow it to backtrack and effectively perform swaps. A standard SFFS algorithm in search of the best $k=2$ set can proceed by finding the best 1-set ($S_1=\\{f_3\\}$), then exploring additions to find a candidate 2-set ($S'_{2}=\\{f_3,f_1\\}$), and then exploring swaps. A swap (e.g., removing $f_3$ and adding $f_2$) would lead it to evaluate $(\\{f_3,f_1\\} \\setminus \\{f_3\\}) \\cup \\{f_2\\} = \\{f_1,f_2\\}$, discovering its high accuracy. Alternatively, during the search, it may form a set of size $3$, $\\{f_1, f_2, f_3\\}$, and then, through a backward elimination step, discover that removing $f_3$ yields an excellent 2-feature set, $\\{f_1, f_2\\}$.\n\nNow we evaluate the provided options against these principles.\n\n**Option A Analysis**\n\n1.  **Data-Generating Mechanism:** The option proposes a well-defined probabilistic model.\n    -   Feature $f_3$: $f_3 \\mid Y \\sim \\mathcal{N}(\\mu_Y, 1)$ with $\\mu_1 = 0.4, \\mu_0 = 0$. The optimal thresholding classifier would achieve an accuracy of $\\Phi(0.2) \\approx 0.579$, matching the claimed accuracy of $\\approx 0.58$. This makes $f_3$ a weak but non-trivial predictor.\n    -   Features $(f_1, f_2)$: The marginal distributions of $f_1$ and $f_2$ are constructed to be identical for both classes ($Y=0$ and $Y=1$). For example, for $f_1$, $p(f_1|Y=1) = p(f_1|Y=0)$, as both are derived from $U+\\varepsilon_1$ where $U$ and $\\varepsilon_1$ are independent of $Y$. This correctly implies that $f_1$ and $f_2$ are individually non-informative, leading to $\\text{Acc}(\\{f_1\\}) \\approx 0.50$ and $\\text{Acc}(\\{f_2\\}) \\approx 0.50$.\n    -   Synergy of $(f_1, f_2)$: If $Y=1$, $f_1 \\approx f_2$. If $Y=0$, $f_1 \\approx -f_2$. These two conditions describe data lying on two nearly orthogonal lines in the $(f_1, f_2)$ plane. A non-linear classifier, such as an SVM with an RBF kernel, can separate these two patterns with high accuracy. The claim of $\\text{Acc}(\\{f_1, f_2\\}) \\approx 0.90$ is therefore plausible and consistent with the data generation.\n    -   Combination with $f_3$: Adding an uninformative feature ($f_1$ or $f_2$) to the weak predictor $f_3$ should not yield a significant improvement. The claimed accuracy of $\\approx 0.59$ for $\\{f_3, f_1\\}$ and $\\{f_3, f_2\\}$ is a plausible small increase from $0.58$.\n2.  **SFS Failure Argument:** With the numerical accuracies provided, SFS will first choose $\\{f_3\\}$ (accuracy $0.58$), then add either $f_1$ or $f_2$ to reach a final set like $\\{f_3, f_1\\}$ with accuracy $\\approx 0.59$. This correctly demonstrates the failure of SFS to find the optimal pair $\\{f_1, f_2\\}$ with accuracy $\\approx 0.90$.\n3.  **SFFS Success Explanation:** The explanation describes a path where the feature set size grows and then shrinks to find the optimal combination: $\\{f_3\\} \\to \\{f_3, f_1\\} \\to \\{f_1, f_2, f_3\\} \\to \\{f_1, f_2\\}$. This sequence of steps is representative of how floating search algorithms operate. By exploring a 3-feature set ($\\{f_1, f_2, f_3\\}$) and finding its high accuracy ($\\approx 0.90$), a subsequent backward or conditional deletion step leads to the evaluation of its 2-feature subsets. The subset $\\{f_1, f_2\\}$ preserves the high accuracy ($\\approx 0.90$), which is a significant improvement over the best 2-feature set found by the initial forward pass ($\\{f_3, f_1\\}$ with accuracy $\\approx 0.59$). SFFS would thus identify $\\{f_1, f_2\\}$ as the superior 2-feature set. The explanation is conceptually sound.\n\n**Verdict:** This option provides a comprehensive and valid counterexample that satisfies all problem requirements. **Correct**.\n\n**Option B Analysis**\nThe setup is $Y=1 \\iff (f_11 \\land f_21)$. This is an AND-gate problem. The option claims a linear logistic regression model can \"immediately capture\" this rule, which is false; a linear model cannot capture this without an explicit interaction term ($f_1 \\cdot f_2$). More importantly, this is not a scenario where SFS fails. Either $f_1$ or $f_2$ alone is predictive (e.g., if $f_1 \\le 1$, $P(Y=1)=0$). SFS would pick one, say $\\{f_1\\}$, then see a massive accuracy gain by adding $f_2$. SFS would successfully find $\\{f_1, f_2\\}$. Therefore, this is not a valid counterexample for the failure of SFS.\n\n**Verdict:** The scenario does not demonstrate SFS failure, and it makes an incorrect statement about linear models. **Incorrect**.\n\n**Option C Analysis**\nThis option dismisses the core question about SFS and SFFS and instead discusses Recursive Feature Elimination (RFE), a backward elimination method. The problem is explicitly about forward selection methods. Furthermore, its claims about RFE are unsubstantiated and likely incorrect. If $f_3$ is a strong predictor and $f_1, f_2$ are weak, a linear SVM would assign a large weight to $f_3$, making it one of the last features to be eliminated by RFE, not the other way around. The option is irrelevant to the question asked.\n\n**Verdict:** The option discusses the wrong algorithm (RFE) and makes unsubstantiated claims. **Incorrect**.\n\n**Option D Analysis**\nThis option uses the same powerful synergistic data generation for $(f_1,f_2)$ as option A, but pairs it with a Linear Discriminant Classifier (LDA) as the learner $\\mathcal{L}$. LDA is a linear classifier that assumes common covariance matrices across classes. The proposed data generation explicitly violates this assumption: the correlation between $f_1$ and $f_2$ is positive for one class and negative for the other. An LDA would be unable to exploit this non-linear separation boundary and would perform poorly on the $\\{f_1, f_2\\}$ pair. This violates a crucial requirement of the problem: that the synergistic pair be \"jointly strong under a fixed... learner $\\mathcal{L}$\". In this setup, $\\{f_1, f_2\\}$ are jointly weak under $\\mathcal{L}=\\text{LDA}$. Therefore, the scenario is not a valid counterexample as specified.\n\n**Verdict:** The chosen learner $\\mathcal{L}$ is unable to exploit the feature synergy, violating a key premise of the problem. **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4539705"}, {"introduction": "In real-world data like radiomics, features are rarely independent; high correlation, or multicollinearity, is the norm. This poses a significant challenge for models that rely on coefficient magnitudes for interpretation or feature ranking, such as RFE. This exercise demonstrates a subtle but critical failure mode of RFE where, due to the confounding effects of a highly correlated feature and random sampling noise, the algorithm can be misled into eliminating the most causally important feature. Understanding this phenomenon of ranking instability is crucial for critically evaluating the results of any RFE procedure [@problem_id:4539571].", "problem": "A radiomics team is building a binary malignancy classifier, but to reason from first principles they approximate the learning step with linear regression under squared error as the wrapper evaluator. Three standardized radiomic features are considered: $x_1$ (first-order intensity mean), $x_2$ (Gaussian-smoothed intensity mean), and $x_3$ (wavelet energy). All features are standardized to zero mean and unit variance. The Recursive Feature Elimination (RFE) procedure ranks features by absolute model coefficient magnitudes at each step. The model in the wrapper is ridge regression with penalty $\\lambda$ on the coefficients.\n\nGround-truth generative model: the malignancy score $y$ follows $y = x_1 + \\varepsilon$, where $\\varepsilon$ is zero-mean noise independent of all features with variance $\\operatorname{Var}(\\varepsilon) = \\sigma_\\varepsilon^2$. In population, the feature correlation matrix and correlations with $y$ satisfy:\n- $\\operatorname{Var}(x_i) = 1$ for $i \\in \\{1,2,3\\}$.\n- $\\operatorname{Corr}(x_1, x_2) = 0.95$, $\\operatorname{Corr}(x_1, x_3) = 0.70$, $\\operatorname{Corr}(x_2, x_3) = 0.95$.\n- $\\operatorname{Corr}(x_1, y) = 1$, $\\operatorname{Corr}(x_2, y) = 0.95$, $\\operatorname{Corr}(x_3, y) = 0.70$.\n\nOn a particular training fold of size $n$ (small), sampling variability yields the following empirical correlations used by the wrapper:\n- $\\widehat{\\operatorname{Corr}}(x_1, y) = 0.85$, $\\widehat{\\operatorname{Corr}}(x_2, y) = 0.85$, $\\widehat{\\operatorname{Corr}}(x_3, y) = 0.87$.\n- The empirical feature-to-feature correlations remain close to their population counterparts: $\\widehat{\\operatorname{Corr}}(x_1, x_2) = 0.95$, $\\widehat{\\operatorname{Corr}}(x_1, x_3) = 0.70$, $\\widehat{\\operatorname{Corr}}(x_2, x_3) = 0.95$.\n\nAssume ridge regression with penalty $\\lambda = 0.30$ is fit on this fold at each RFE step, and the ranking criterion is the absolute value of the fitted coefficients. Use the normal equation approximation for the ridge estimator in the standardized, large-sample limit, replacing $\\Sigma_{xx}$ and $\\Sigma_{xy}$ by the fold’s empirical moments:\n$$\n\\hat{w} \\approx \\left(\\Sigma_{xx} + \\lambda I\\right)^{-1} \\Sigma_{xy},\n$$\nwhere $\\Sigma_{xx}$ is the $3 \\times 3$ empirical correlation matrix of $(x_1, x_2, x_3)$ and $\\Sigma_{xy}$ is the vector of empirical correlations with $y$. In the second RFE step (after dropping one feature), use the corresponding $2 \\times 2$ submatrix and subvectors.\n\nFinally, to judge suboptimality against the ground truth $y = x_1 + \\varepsilon$, recall that for a single-feature linear predictor $\\hat{y} = \\gamma x$, the mean squared prediction error on new data is\n$$\n\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + \\operatorname{Var}\\left(x_1 - \\gamma^\\star x\\right),\n$$\nwith $\\gamma^\\star = \\operatorname{Cov}(y, x)/\\operatorname{Var}(x)$; under standardization and $\\operatorname{Corr}(x_1, x) = \\rho$, this simplifies to\n$$\n\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + 1 - \\rho^2.\n$$\n\nTake $\\sigma_\\varepsilon^2 = 0.20$.\n\nWhich option best describes the actual RFE elimination path on this fold and its consequence, including the quantitative suboptimality of the final subset under the ground truth?\n\nA. First eliminate $x_2$, then eliminate $x_3$, ending with $\\{x_1\\}$. No ranking inversion occurs; the final subset attains $\\operatorname{MSE} = \\sigma_\\varepsilon^2 = 0.20$.\n\nB. First eliminate $x_2$, then eliminate $x_1$, ending with $\\{x_3\\}$. A downstream ranking inversion occurs after removing a redundant stabilizer ($x_2$), and the final subset’s ground-truth error inflates to $\\operatorname{MSE} = \\sigma_\\varepsilon^2 + 1 - \\rho_{13}^2 = 0.20 + 1 - 0.49 = 0.71$.\n\nC. First eliminate $x_3$, then eliminate $x_2$, ending with $\\{x_1\\}$. The initial ranking places $x_3$ as least important; the final subset attains $\\operatorname{MSE} = 0.20$.\n\nD. Eliminate only $x_2$ and stop, because removal of a redundant feature cannot cause downstream ranking changes under ridge; the final subset is $\\{x_1, x_3\\}$ with no suboptimality relative to $\\{x_1\\}$.", "solution": "This problem requires us to trace the steps of a Recursive Feature Elimination (RFE) procedure and evaluate the quality of its final selection. RFE iteratively removes the feature deemed least important, which, in this case, is the feature with the smallest absolute coefficient in a ridge regression model. We must perform this analysis and then calculate the mean squared error (MSE) of the final selected feature against the ground truth.\n\n**RFE Step 1: Model with features $\\{x_1, x_2, x_3\\}$**\n\nFirst, we fit a ridge regression model using all three features to determine which one to eliminate. The ridge coefficients are given by $\\hat{w} = (\\Sigma_{xx} + \\lambda I)^{-1} \\Sigma_{xy}$.\nThe empirical correlation matrices and penalty are:\n$$ \\Sigma_{xx} = \\begin{pmatrix} 1  0.95  0.70 \\\\ 0.95  1  0.95 \\\\ 0.70  0.95  1 \\end{pmatrix}, \\quad \\Sigma_{xy} = \\begin{pmatrix} 0.85 \\\\ 0.85 \\\\ 0.87 \\end{pmatrix}, \\quad \\lambda = 0.30 $$\nWe compute $\\hat{w} = (\\Sigma_{xx} + 0.3 I)^{-1} \\Sigma_{xy}$.\nThe matrix to invert is $A = \\Sigma_{xx} + 0.3I = \\begin{pmatrix} 1.30  0.95  0.70 \\\\ 0.95  1.30  0.95 \\\\ 0.70  0.95  1.30 \\end{pmatrix}$.\nCalculating the coefficients:\n$$ \\hat{w} = \\begin{pmatrix} 1.30  0.95  0.70 \\\\ 0.95  1.30  0.95 \\\\ 0.70  0.95  1.30 \\end{pmatrix}^{-1} \\begin{pmatrix} 0.85 \\\\ 0.85 \\\\ 0.87 \\end{pmatrix} \\approx \\begin{pmatrix} 0.374 \\\\ 0.083 \\\\ 0.407 \\end{pmatrix} $$\nThe absolute values of the coefficients are $|\\hat{w}_1| \\approx 0.374$, $|\\hat{w}_2| \\approx 0.083$, and $|\\hat{w}_3| \\approx 0.407$.\nThe feature with the smallest absolute coefficient is $x_2$. Therefore, RFE eliminates $x_2$ in the first step.\n\n**RFE Step 2: Model with features $\\{x_1, x_3\\}$**\n\nNext, we refit the ridge model on the remaining features, $x_1$ and $x_3$.\nThe new correlation matrices are:\n$$ \\Sigma_{xx}^{(2)} = \\begin{pmatrix} 1  0.70 \\\\ 0.70  1 \\end{pmatrix}, \\quad \\Sigma_{xy}^{(2)} = \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} $$\nThe new coefficients are $\\hat{w}^{(2)} = (\\Sigma_{xx}^{(2)} + 0.3 I)^{-1} \\Sigma_{xy}^{(2)}$.\nThe matrix to invert is $B = \\Sigma_{xx}^{(2)} + 0.3I = \\begin{pmatrix} 1.30  0.70 \\\\ 0.70  1.30 \\end{pmatrix}$.\nCalculating the coefficients:\n$$ \\hat{w}^{(2)} = \\begin{pmatrix} 1.30  0.70 \\\\ 0.70  1.30 \\end{pmatrix}^{-1} \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} = \\frac{1}{1.3^2 - 0.7^2} \\begin{pmatrix} 1.30  -0.70 \\\\ -0.70  1.30 \\end{pmatrix} \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} $$\n$$ \\hat{w}^{(2)} = \\frac{1}{1.20} \\begin{pmatrix} 1.30(0.85) - 0.70(0.87) \\\\ -0.70(0.85) + 1.30(0.87) \\end{pmatrix} = \\frac{1}{1.20} \\begin{pmatrix} 0.496 \\\\ 0.536 \\end{pmatrix} \\approx \\begin{pmatrix} 0.413 \\\\ 0.447 \\end{pmatrix} $$\nThe absolute values are $|\\hat{w}_1^{(2)}| \\approx 0.413$ and $|\\hat{w}_3^{(2)}| \\approx 0.447$.\nNow, the feature with the smaller coefficient is $x_1$. RFE eliminates $x_1$ in the second step. The final remaining feature is $\\{x_3\\}$.\n\n**Consequence Analysis**\n\nThe RFE procedure has incorrectly selected feature $x_3$, eliminating the true causal feature $x_1$. We now calculate the suboptimality of this choice using the ground-truth MSE formula:\n$$ \\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + 1 - \\rho^2 $$\nHere, $\\rho$ is the **population** correlation between the true causal feature ($x_1$) and the selected feature ($x_3$). From the problem statement, the population correlation is $\\operatorname{Corr}(x_1, x_3) = 0.70$. We are given $\\sigma_\\varepsilon^2 = 0.20$.\n$$ \\operatorname{MSE}(x_3) = 0.20 + 1 - (0.70)^2 = 0.20 + 1 - 0.49 = 0.71 $$\nThe optimal MSE, achieved by correctly selecting $x_1$, would be the irreducible error $\\operatorname{MSE}(x_1) = \\sigma_\\varepsilon^2 = 0.20$. The selection of $x_3$ leads to a significantly higher error.\n\nOur step-by-step analysis shows that RFE first eliminates $x_2$, then $x_1$, leaving $\\{x_3\\}$, and the resulting ground-truth MSE is $0.71$. This matches option B perfectly.", "answer": "$$\\boxed{B}$$", "id": "4539571"}]}