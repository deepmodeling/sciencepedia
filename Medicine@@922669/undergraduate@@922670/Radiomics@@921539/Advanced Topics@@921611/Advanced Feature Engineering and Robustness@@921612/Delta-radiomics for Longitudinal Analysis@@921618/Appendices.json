{"hands_on_practices": [{"introduction": "The simplest way to measure change is to subtract a feature's value at two different times. But how reliable is this \"delta\" value, especially when each measurement has its own inherent noise? This practice explores the fundamental concept of error propagation, showing how measurement noise from individual scans contributes to the overall uncertainty of the calculated change. By working through this problem, you will understand how the variance of a delta-feature depends on the noise of the individual measurements and, crucially, on whether those noise sources are correlated [@problem_id:4536699].", "problem": "In delta-radiomics for longitudinal analysis, a radiomic feature is extracted at two time points, denoted by $t_1$ and $t_2$. Let the observed feature value at time $t$ be $Y_t = f(t) + \\epsilon_t$, where $f(t)$ is the true (noise-free) feature value and $\\epsilon_t$ is measurement noise arising from segmentation and reconstruction variability. Assume that for a given patient, the quantities $f(t_1)$ and $f(t_2)$ are fixed (non-random) and that $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ with the same variance $\\sigma^2$ at both time points. Define the delta-radiomic feature as $\\Delta f = Y_{t_2} - Y_{t_1}$. Using only the axioms of variance and covariance for linear combinations of random variables, first compute $\\mathrm{Var}(\\Delta f)$ under the assumption that $\\epsilon_{t_1}$ and $\\epsilon_{t_2}$ are independent. Then consider a more realistic scenario in which there is shared acquisition bias so that $\\mathrm{Cov}(\\epsilon_{t_1},\\epsilon_{t_2}) = \\rho\\,\\sigma^2$ with $-1 \\le \\rho \\le 1$. Derive $\\mathrm{Var}(\\Delta f)$ in terms of $\\sigma^2$ and $\\rho$ under this correlated-noise model. Provide your final answer as a single closed-form analytic expression in terms of $\\sigma^2$ and $\\rho$. No rounding is required.", "solution": "The problem statement is first validated for scientific soundness, self-consistency, and clarity.\n\n**Step 1: Extract Givens**\n- The observed feature value at time $t$ is $Y_t = f(t) + \\epsilon_t$.\n- $f(t)$ is the true, non-random feature value.\n- $\\epsilon_t$ is measurement noise, with $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$. The variance $\\sigma^2$ is the same at both time points $t_1$ and $t_2$.\n- The delta-radiomic feature is defined as $\\Delta f = Y_{t_2} - Y_{t_1}$.\n- Case 1 (independent noise): $\\epsilon_{t_1}$ and $\\epsilon_{t_2}$ are independent.\n- Case 2 (correlated noise): $\\mathrm{Cov}(\\epsilon_{t_1}, \\epsilon_{t_2}) = \\rho\\,\\sigma^2$, where $-1 \\le \\rho \\le 1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded. The additive noise model $Y_t = f(t) + \\epsilon_t$ is a standard and fundamental model for measurement error in numerous scientific disciplines, including radiomics. The assumptions about the noise distribution being Gaussian and the possibility of correlation between measurements at different time points are realistic and well-established in statistical theory. The problem is well-posed, providing all necessary definitions and constraints to derive a unique analytical solution. The language is objective and unambiguous. The problem does not violate any of the invalidity criteria.\n\n**Verdict:** The problem is valid.\n\n**Solution Derivation**\nThe objective is to compute the variance of the delta-radiomic feature, $\\mathrm{Var}(\\Delta f)$. We begin by expressing $\\Delta f$ in terms of its constituent components.\n\nThe definition of the delta-radiomic feature is:\n$$ \\Delta f = Y_{t_2} - Y_{t_1} $$\n\nSubstitute the model for the observed feature values, $Y_t = f(t) + \\epsilon_t$:\n$$ \\Delta f = (f(t_2) + \\epsilon_{t_2}) - (f(t_1) + \\epsilon_{t_1}) $$\n\nRearranging the terms to separate the non-random and random components gives:\n$$ \\Delta f = (f(t_2) - f(t_1)) + (\\epsilon_{t_2} - \\epsilon_{t_1}) $$\n\nWe now compute the variance of $\\Delta f$. According to the problem statement, the true feature values $f(t_1)$ and $f(t_2)$ are fixed, non-random quantities for a given patient. Therefore, their difference, $f(t_2) - f(t_1)$, is a constant. A fundamental property of variance is that for any random variable $X$ and constant $c$, $\\mathrm{Var}(X+c) = \\mathrm{Var}(X)$. Applying this property, we have:\n$$ \\mathrm{Var}(\\Delta f) = \\mathrm{Var}((f(t_2) - f(t_1)) + (\\epsilon_{t_2} - \\epsilon_{t_1})) = \\mathrm{Var}(\\epsilon_{t_2} - \\epsilon_{t_1}) $$\n\nWe now use the axiom for the variance of a linear combination of two random variables, $A$ and $B$:\n$$ \\mathrm{Var}(aA + bB) = a^2 \\mathrm{Var}(A) + b^2 \\mathrm{Var}(B) + 2ab \\mathrm{Cov}(A, B) $$\n\nIn our case, $A = \\epsilon_{t_2}$, $B = \\epsilon_{t_1}$, $a = 1$, and $b = -1$. Substituting these into the formula yields:\n$$ \\mathrm{Var}(\\epsilon_{t_2} - \\epsilon_{t_1}) = (1)^2 \\mathrm{Var}(\\epsilon_{t_2}) + (-1)^2 \\mathrm{Var}(\\epsilon_{t_1}) + 2(1)(-1) \\mathrm{Cov}(\\epsilon_{t_2}, \\epsilon_{t_1}) $$\n$$ \\mathrm{Var}(\\Delta f) = \\mathrm{Var}(\\epsilon_{t_2}) + \\mathrm{Var}(\\epsilon_{t_1}) - 2 \\mathrm{Cov}(\\epsilon_{t_1}, \\epsilon_{t_2}) $$\nNote that the covariance is symmetric, i.e., $\\mathrm{Cov}(\\epsilon_{t_2}, \\epsilon_{t_1}) = \\mathrm{Cov}(\\epsilon_{t_1}, \\epsilon_{t_2})$.\n\nFrom the problem statement, we are given that $\\mathrm{Var}(\\epsilon_{t_1}) = \\sigma^2$ and $\\mathrm{Var}(\\epsilon_{t_2}) = \\sigma^2$. Substituting these into the expression gives:\n$$ \\mathrm{Var}(\\Delta f) = \\sigma^2 + \\sigma^2 - 2 \\mathrm{Cov}(\\epsilon_{t_1}, \\epsilon_{t_2}) = 2\\sigma^2 - 2 \\mathrm{Cov}(\\epsilon_{t_1}, \\epsilon_{t_2}) $$\n\nNow we evaluate this expression for the two specified scenarios.\n\n**Case 1: Independent Noise**\nIf the noise terms $\\epsilon_{t_1}$ and $\\epsilon_{t_2}$ are independent, their covariance is zero:\n$$ \\mathrm{Cov}(\\epsilon_{t_1}, \\epsilon_{t_2}) = 0 $$\nSubstituting this into our general expression for $\\mathrm{Var}(\\Delta f)$:\n$$ \\mathrm{Var}(\\Delta f) = 2\\sigma^2 - 2(0) = 2\\sigma^2 $$\nThus, under the assumption of independent noise, the variance of the delta feature is twice the variance of the individual measurements.\n\n**Case 2: Correlated Noise**\nIn the more realistic scenario, we are given a non-zero covariance:\n$$ \\mathrm{Cov}(\\epsilon_{t_1}, \\epsilon_{t_2}) = \\rho\\,\\sigma^2 $$\nHere, $\\rho$ is the correlation coefficient between the noise terms. Substituting this into our general expression for $\\mathrm{Var}(\\Delta f)$:\n$$ \\mathrm{Var}(\\Delta f) = 2\\sigma^2 - 2(\\rho\\,\\sigma^2) $$\n\nFactoring out the term $2\\sigma^2$, we arrive at the final expression for the variance of the delta-radiomic feature under the correlated-noise model:\n$$ \\mathrm{Var}(\\Delta f) = 2\\sigma^2(1 - \\rho) $$\n\nThis expression is the required result, representing the variance of the delta feature in terms of the single-measurement noise variance $\\sigma^2$ and the correlation $\\rho$. This result shows that positive correlation ($\\rho > 0$) reduces the variance of the difference compared to the independent case, while negative correlation ($\\rho < 0$) increases it.\n\nThe problem asks for the single closed-form analytic expression from the correlated-noise model.", "answer": "$$ \\boxed{2\\sigma^2(1 - \\rho)} $$", "id": "4536699"}, {"introduction": "While a two-point difference is intuitive, it can be overly sensitive to random noise at the start and end points. A more robust approach to quantifying a trend is to use all available data from a patient's longitudinal scans. This exercise guides you through the derivation of the Ordinary Least Squares (OLS) slope, the statistical workhorse for fitting a linear trend, providing a more stable and representative measure of change over time [@problem_id:4536757].", "problem": "In delta-radiomics for longitudinal analysis, a radiomic feature is modeled as a time-varying quantity to quantify change induced by therapy. Consider a single region-of-interest in a patient where the same radiomic feature is measured across therapy using Magnetic Resonance Imaging (MRI). Let the observed feature values be denoted by $f(t)$ at time $t$ (in weeks), and suppose that the true temporal evolution is approximately linear with additive zero-mean noise, i.e., $f(t) = \\beta_{0} + \\beta_{1} t + \\varepsilon(t)$, where $\\varepsilon(t)$ has mean $0$ and finite variance. The slope $\\beta_{1}$ represents the rate of change and is the quantity of interest in delta-radiomics.\n\nStarting from the definitions of least squares and the assumption above, do the following:\n\n1. Using first principles of ordinary least squares (OLS), derive the slope estimator $\\hat{\\beta}_{1}$ for the special case of exactly $2$ time points. Justify why the minimizer forces the fitted line to pass through both observations in this case, and obtain the slope as a function of the two observed pairs $\\{(t_{1}, f(t_{1})), (t_{2}, f(t_{2}))\\}$.\n\n2. Generalize the derivation to $T > 2$ time points. Starting from the sum of squared residuals $\\sum_{i=1}^{T} \\left(f(t_{i}) - \\beta_{0} - \\beta_{1} t_{i}\\right)^{2}$, derive the closed-form OLS slope estimator $\\hat{\\beta}_{1}$ in terms of $\\{t_{i}\\}$ and $\\{f(t_{i})\\}$ only, without leaving any unknowns in the final expression.\n\n3. Apply your results to the following longitudinal dataset measured at $T = 5$ time points: $t \\in \\{0, 2, 4, 6, 8\\}$ (weeks) with measured feature values $f(0) = 1.30$, $f(2) = 1.18$, $f(4) = 1.02$, $f(6) = 0.87$, and $f(8) = 0.80$ (arbitrary feature units). Compute:\n   - The two-point slope using only the baseline and final follow-up measurements.\n   - The OLS slope using all $5$ time points.\n\nExpress both slopes in units of feature units per week. Round each to four significant figures. Report your final answer as a row matrix in the order $\\left[\\hat{\\beta}_{1,\\text{two-point}} \\ \\hat{\\beta}_{1,\\text{OLS}}\\right]$. Do not include units in your final boxed answer.", "solution": "The problem is evaluated as valid, as it is scientifically grounded in standard statistical theory (Ordinary Least Squares), is well-posed with all necessary information provided, and is expressed in objective, formal language. The solution proceeds as requested.\n\nThe model for the radiomic feature $f(t)$ at time $t$ is given by a simple linear regression model:\n$$f(t) = \\beta_{0} + \\beta_{1} t + \\varepsilon(t)$$\nwhere $\\beta_{0}$ is the intercept, $\\beta_{1}$ is the slope, and $\\varepsilon(t)$ is a zero-mean error term. The Ordinary Least Squares (OLS) method aims to find the estimators $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ that minimize the sum of squared residuals (SSR), defined as the sum of the squared differences between the observed values and the values predicted by the model.\n\n### 1. Derivation for Two Time Points ($T=2$)\n\nFor the special case of exactly two measurement pairs, $(t_{1}, f(t_{1}))$ and $(t_{2}, f(t_{2}))$, the sum of squared residuals is:\n$$SSR(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{2} \\left(f(t_{i}) - (\\beta_{0} + \\beta_{1} t_{i})\\right)^{2} = \\left(f(t_{1}) - \\beta_{0} - \\beta_{1} t_{1}\\right)^{2} + \\left(f(t_{2}) - \\beta_{0} - \\beta_{1} t_{2}\\right)^{2}$$\nTo find the values of $\\beta_{0}$ and $\\beta_{1}$ that minimize this quantity, we can use two approaches: direct minimization or calculus.\n\n**Justification via Minimization Principle:**\nThe quantity $SSR(\\beta_{0}, \\beta_{1})$ is a sum of squared terms, which means its value is always non-negative, i.e., $SSR \\ge 0$. The absolute minimum value possible for $SSR$ is $0$. This minimum is achieved if and only if both squared terms are simultaneously zero:\n\\begin{align*} f(t_{1}) - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} t_{1} &= 0 \\\\ f(t_{2}) - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} t_{2} &= 0 \\end{align*}\nThis is a system of two linear equations in two unknowns, $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$. The geometric interpretation of these equations is that the fitted line $\\hat{f}(t) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} t$ must pass exactly through both observed data points $(t_{1}, f(t_{1}))$ and $(t_{2}, f(t_{2}))$. Since it is possible to find such a line (as long as $t_1 \\neq t_2$), the OLS estimators must describe this line, as it results in the minimum possible SSR of $0$.\n\n**Derivation of the Slope Estimator:**\nWe solve the system of equations for $\\hat{\\beta}_{1}$. Rearranging the equations:\n\\begin{align*} f(t_{1}) &= \\hat{\\beta}_{0} + \\hat{\\beta}_{1} t_{1} \\\\ f(t_{2}) &= \\hat{\\beta}_{0} + \\hat{\\beta}_{1} t_{2} \\end{align*}\nSubtracting the first equation from the second yields:\n$$f(t_{2}) - f(t_{1}) = (\\hat{\\beta}_{0} + \\hat{\\beta}_{1} t_{2}) - (\\hat{\\beta}_{0} + \\hat{\\beta}_{1} t_{1}) = \\hat{\\beta}_{1} (t_{2} - t_{1})$$\nAssuming $t_{1} \\neq t_{2}$, we can isolate $\\hat{\\beta}_{1}$:\n$$\\hat{\\beta}_{1} = \\frac{f(t_{2}) - f(t_{1})}{t_{2} - t_{1}}$$\nThis is the familiar formula for the slope of a line passing through two points.\n\n### 2. Generalization for $T > 2$ Time Points\n\nFor a general set of $T$ observations $\\{(t_{i}, f(t_{i}))\\}_{i=1}^{T}$, the sum of squared residuals is:\n$$SSR(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{T} \\left(f(t_{i}) - \\beta_{0} - \\beta_{1} t_{i}\\right)^{2}$$\nTo find the estimators $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ that minimize $SSR$, we take the partial derivatives with respect to $\\beta_{0}$ and $\\beta_{1}$ and set them to zero.\n\nTaking the partial derivative with respect to $\\beta_{0}$:\n$$\\frac{\\partial SSR}{\\partial \\beta_{0}} = \\sum_{i=1}^{T} 2 \\left(f(t_{i}) - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} t_{i}\\right)(-1) = -2 \\sum_{i=1}^{T} \\left(f(t_{i}) - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} t_{i}\\right)$$\nSetting this to zero:\n$$\\sum_{i=1}^{T} f(t_{i}) - \\sum_{i=1}^{T} \\hat{\\beta}_{0} - \\sum_{i=1}^{T} \\hat{\\beta}_{1} t_{i} = 0 \\implies \\sum f(t_{i}) - T\\hat{\\beta}_{0} - \\hat{\\beta}_{1}\\sum t_{i} = 0$$\nThis is the first normal equation.\n\nTaking the partial derivative with respect to $\\beta_{1}$:\n$$\\frac{\\partial SSR}{\\partial \\beta_{1}} = \\sum_{i=1}^{T} 2 \\left(f(t_{i}) - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} t_{i}\\right)(-t_{i}) = -2 \\sum_{i=1}^{T} t_{i}\\left(f(t_{i}) - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} t_{i}\\right)$$\nSetting this to zero:\n$$\\sum_{i=1}^{T} t_{i}f(t_{i}) - \\sum_{i=1}^{T} \\hat{\\beta}_{0}t_{i} - \\sum_{i=1}^{T} \\hat{\\beta}_{1}t_{i}^2 = 0 \\implies \\hat{\\beta}_{0}\\sum t_{i} + \\hat{\\beta}_{1}\\sum t_{i}^2 = \\sum t_{i}f(t_{i})$$\nThis is the second normal equation.\n\nFrom the first normal equation, we can express $\\hat{\\beta}_{0}$ in terms of $\\hat{\\beta}_{1}$. Let $\\bar{t} = \\frac{1}{T}\\sum t_{i}$ and $\\bar{f} = \\frac{1}{T}\\sum f(t_{i})$ be the sample means.\n$$T\\hat{\\beta}_{0} = \\sum f(t_{i}) - \\hat{\\beta}_{1}\\sum t_{i} \\implies \\hat{\\beta}_{0} = \\frac{1}{T}\\sum f(t_{i}) - \\hat{\\beta}_{1} \\frac{1}{T}\\sum t_{i} = \\bar{f} - \\hat{\\beta}_{1}\\bar{t}$$\nNow, substitute this expression for $\\hat{\\beta}_{0}$ into the second normal equation:\n$$(\\bar{f} - \\hat{\\beta}_{1}\\bar{t})\\sum t_{i} + \\hat{\\beta}_{1}\\sum t_{i}^2 = \\sum t_{i}f(t_{i})$$\nDistribute and group terms with $\\hat{\\beta}_{1}$:\n$$\\bar{f}\\sum t_{i} - \\hat{\\beta}_{1}\\bar{t}\\sum t_{i} + \\hat{\\beta}_{1}\\sum t_{i}^2 = \\sum t_{i}f(t_{i})$$\n$$\\hat{\\beta}_{1}\\left(\\sum t_{i}^2 - \\bar{t}\\sum t_{i}\\right) = \\sum t_{i}f(t_{i}) - \\bar{f}\\sum t_{i}$$\nSolve for $\\hat{\\beta}_{1}$:\n$$\\hat{\\beta}_{1} = \\frac{\\sum t_{i}f(t_{i}) - \\bar{f}\\sum t_{i}}{\\sum t_{i}^2 - \\bar{t}\\sum t_{i}}$$\nThis is a valid closed-form solution. A more common and insightful form is obtained by substituting $\\sum t_{i} = T\\bar{t}$ and $\\sum f(t_{i}) = T\\bar{f}$:\n$$\\hat{\\beta}_{1} = \\frac{\\sum t_{i}f(t_{i}) - T\\bar{f}\\bar{t}}{\\sum t_{i}^2 - T\\bar{t}^2}$$\nThis expression can be further shown to be equivalent to the ratio of the sample covariance to the sample variance:\n$$\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{T} (t_{i} - \\bar{t})(f(t_{i}) - \\bar{f})}{\\sum_{i=1}^{T} (t_{i} - \\bar{t})^2}$$\n\n### 3. Application to Longitudinal Dataset\n\nThe given dataset has $T=5$ time points:\n- Time points $t$: $\\{0, 2, 4, 6, 8\\}$ weeks\n- Feature values $f(t)$: $\\{1.30, 1.18, 1.02, 0.87, 0.80\\}$\n\n**Two-point slope calculation:**\nUsing only the baseline ($t_{1}=0, f(t_{1})=1.30$) and final ($t_{2}=8, f(t_{2})=0.80$) measurements, we apply the formula from Part 1:\n$$\\hat{\\beta}_{1,\\text{two-point}} = \\frac{f(8) - f(0)}{8 - 0} = \\frac{0.80 - 1.30}{8} = \\frac{-0.50}{8} = -0.0625$$\nRounded to four significant figures, this is $-0.06250$ feature units per week.\n\n**OLS slope calculation using all 5 time points:**\nWe use the formula $\\hat{\\beta}_{1} = \\frac{\\sum (t_{i} - \\bar{t})(f(t_{i}) - \\bar{f})}{\\sum (t_{i} - \\bar{t})^2}$.\nFirst, calculate the means:\n$$\\bar{t} = \\frac{0+2+4+6+8}{5} = \\frac{20}{5} = 4$$\n$$\\bar{f} = \\frac{1.30 + 1.18 + 1.02 + 0.87 + 0.80}{5} = \\frac{5.17}{5} = 1.034$$\nNow, we compute the necessary sums.\nSum of squared deviations for $t$:\n$$\\sum_{i=1}^{5} (t_{i} - \\bar{t})^2 = (0-4)^2 + (2-4)^2 + (4-4)^2 + (6-4)^2 + (8-4)^2$$\n$$= (-4)^2 + (-2)^2 + 0^2 + 2^2 + 4^2 = 16 + 4 + 0 + 4 + 16 = 40$$\nSum of the products of deviations:\n$$\\sum_{i=1}^{5} (t_{i} - \\bar{t})(f(t_{i}) - \\bar{f}) = (0-4)(1.30-1.034) + (2-4)(1.18-1.034) + (4-4)(1.02-1.034) + (6-4)(0.87-1.034) + (8-4)(0.80-1.034)$$\n$$= (-4)(0.266) + (-2)(0.146) + (0)(-0.014) + (2)(-0.164) + (4)(-0.234)$$\n$$= -1.064 - 0.292 + 0 - 0.328 - 0.936 = -2.620$$\nFinally, we compute the OLS slope estimator:\n$$\\hat{\\beta}_{1,\\text{OLS}} = \\frac{-2.620}{40} = -0.0655$$\nRounded to four significant figures, this is $-0.06550$ feature units per week.\n\nThe two-point slope is $\\hat{\\beta}_{1,\\text{two-point}} = -0.06250$ and the OLS slope using all data is $\\hat{\\beta}_{1,\\text{OLS}} = -0.06550$.", "answer": "$$\n\\boxed{\n\\begin{bmatrix}\n-0.06250 & -0.06550\n\\end{bmatrix}\n}\n$$", "id": "4536757"}, {"introduction": "Real-world clinical studies are rarely perfect; patients may miss scheduled appointments, leading to missing data points in a longitudinal sequence. A common, though simplistic, solution is \"Last Observation Carried Forward\" (LOCF), but what effect does this have on our results? This hypothetical scenario uses a simplified model to reveal and quantify the systematic error, or bias, introduced by this practice, highlighting the critical importance of principled methods for handling missing data in any longitudinal analysis [@problem_id:4536755].", "problem": "A radiomics cohort is monitored longitudinally at $2$ time points: baseline at time $t=0$ and follow-up at time $t=T$. For each subject $i$, a single radiomic feature $X_i(t)$ evolves according to a simple linear trajectory model\n$$\nX_i(t) = X_{i0} + v_i t + \\varepsilon_i(t),\n$$\nwhere $X_{i0}$ is the subject-specific baseline intercept, $v_i$ is the subject-specific slope, and $\\varepsilon_i(t)$ is a zero-mean measurement error process with $\\mathbb{E}[\\varepsilon_i(t)]=0$ for all $t$, uncorrelated with $v_i$. Assume that the missingness of the follow-up measurement is Missing Completely At Random (MCAR), meaning the indicator of missingness at follow-up, $M_i \\in \\{0,1\\}$ with $M_i=1$ indicating missing, satisfies $\\mathbb{P}(M_i=1)=p$ and is independent of $v_i$ and $\\varepsilon_i(t)$. Baseline measurements are always observed.\n\nIn delta-radiomics, the subject-level delta feature is defined as the change between follow-up and baseline,\n$$\nD_i = X_i(T) - X_i(0).\n$$\nIn practice, when the follow-up measurement is missing, the Last Observation Carried Forward (LOCF) convention sets the recorded follow-up equal to the baseline, so the estimated delta under missing follow-up becomes $\\hat{D}_i=0$ for $M_i=1$, and equals the observed change for $M_i=0$.\n\nStarting from the definitions above and the properties of expectation, derive the expected attenuation factor\n$$\nA = \\frac{\\mathbb{E}[\\hat{D}_i]}{\\mathbb{E}[D_i]},\n$$\nthat quantifies how the practice of carrying forward the baseline when follow-up is missing attenuates the expected delta across the cohort under this linear trajectory model and MCAR missingness. Express your final answer as a closed-form analytic expression in terms of $p$ only. No numeric approximation is required, and no units should be included in your final expression.", "solution": "The problem as stated provides a complete and consistent set of definitions and assumptions to derive the requested quantity. It is scientifically grounded within the standard framework of biostatistical modeling for longitudinal data, well-posed, and expressed in objective, formal language. The model is a valid simplification for studying the impact of a specific data-handling convention (Last Observation Carried Forward) under a specific missing data mechanism (Missing Completely At Random). Therefore, the problem is valid and a solution can be derived.\n\nThe objective is to find the expected attenuation factor, $A$, defined as the ratio of the expected estimated delta feature to the expected true delta feature:\n$$\nA = \\frac{\\mathbb{E}[\\hat{D}_i]}{\\mathbb{E}[D_i]}\n$$\nWe will derive expressions for the numerator and the denominator separately.\n\nFirst, let us determine the expected true delta feature, $\\mathbb{E}[D_i]$. The true delta feature for subject $i$ is defined as the change in the radiomic feature from baseline ($t=0$) to follow-up ($t=T$):\n$$\nD_i = X_i(T) - X_i(0)\n$$\nThe linear trajectory model for the feature is given by $X_i(t) = X_{i0} + v_i t + \\varepsilon_i(t)$.\nAt the two time points, the feature values are:\n$$\nX_i(0) = X_{i0} + v_i(0) + \\varepsilon_i(0) = X_{i0} + \\varepsilon_i(0)\n$$\n$$\nX_i(T) = X_{i0} + v_i T + \\varepsilon_i(T)\n$$\nSubstituting these into the expression for $D_i$:\n$$\nD_i = (X_{i0} + v_i T + \\varepsilon_i(T)) - (X_{i0} + \\varepsilon_i(0))\n$$\n$$\nD_i = v_i T + \\varepsilon_i(T) - \\varepsilon_i(0)\n$$\nNow, we take the expectation of $D_i$. By the linearity of the expectation operator:\n$$\n\\mathbb{E}[D_i] = \\mathbb{E}[v_i T + \\varepsilon_i(T) - \\varepsilon_i(0)] = \\mathbb{E}[v_i T] + \\mathbb{E}[\\varepsilon_i(T)] - \\mathbb{E}[\\varepsilon_i(0)]\n$$\nSince $T$ is a constant, $\\mathbb{E}[v_i T] = T \\mathbb{E}[v_i]$. The problem states that the measurement error process $\\varepsilon_i(t)$ has a mean of zero, so $\\mathbb{E}[\\varepsilon_i(T)] = 0$ and $\\mathbb{E}[\\varepsilon_i(0)] = 0$.\nTherefore, the expected true delta is:\n$$\n\\mathbb{E}[D_i] = T \\mathbb{E}[v_i]\n$$\nFor this quantity to be non-trivial, we assume that on average there is some change over time, i.e., $\\mathbb{E}[v_i] \\neq 0$, and that the follow-up time is non-zero, $T > 0$.\n\nNext, we determine the expected estimated delta feature, $\\mathbb{E}[\\hat{D}_i]$. The estimated delta, $\\hat{D}_i$, is defined based on the missingness status $M_i$ of the follow-up measurement.\n$$\n\\hat{D}_i =\n\\begin{cases}\nD_i & \\text{if } M_i = 0 \\text{ (follow-up observed)} \\\\\n0 & \\text{if } M_i = 1 \\text{ (follow-up missing)}\n\\end{cases}\n$$\nThis can be expressed more compactly as:\n$$\n\\hat{D}_i = (1 - M_i) D_i\n$$\nTo find its expectation, we compute $\\mathbb{E}[\\hat{D}_i] = \\mathbb{E}[(1-M_i) D_i]$.\nThe problem states that the missingness is Missing Completely At Random (MCAR), which means the missingness indicator $M_i$ is independent of other variables, including the subject-specific slope $v_i$ and the measurement error process $\\varepsilon_i(t)$. Since $D_i$ is a function of $v_i$, $\\varepsilon_i(0)$, and $\\varepsilon_i(T)$, $M_i$ is also independent of $D_i$.\nFor two independent random variables, say $Y$ and $Z$, the expectation of their product is the product of their expectations: $\\mathbb{E}[YZ] = \\mathbb{E}[Y]\\mathbb{E}[Z]$.\nApplying this property with $Y = (1-M_i)$ and $Z = D_i$:\n$$\n\\mathbb{E}[\\hat{D}_i] = \\mathbb{E}[1-M_i] \\mathbb{E}[D_i]\n$$\nThe missingness indicator $M_i$ is a Bernoulli random variable where $M_i=1$ with probability $p = \\mathbb{P}(M_i=1)$. The expectation of a Bernoulli variable is its success probability, so $\\mathbb{E}[M_i] = p$.\nThe expectation of $(1-M_i)$ is:\n$$\n\\mathbb{E}[1-M_i] = 1 - \\mathbb{E}[M_i] = 1 - p\n$$\nSubstituting this back into the expression for $\\mathbb{E}[\\hat{D}_i]$:\n$$\n\\mathbb{E}[\\hat{D}_i] = (1 - p) \\mathbb{E}[D_i]\n$$\nNow we can compute the attenuation factor $A$.\n$$\nA = \\frac{\\mathbb{E}[\\hat{D}_i]}{\\mathbb{E}[D_i]} = \\frac{(1 - p) \\mathbb{E}[D_i]}{\\mathbb{E}[D_i]}\n$$\nAssuming that the expected true change is non-zero, i.e., $\\mathbb{E}[D_i] \\neq 0$, we can cancel this term from the numerator and denominator. This assumption is implicit in the phrasing of the problem, as attenuation is only a meaningful concept if there is a non-zero signal to attenuate.\n$$\nA = 1 - p\n$$\nThis result demonstrates that under the specified linear model and MCAR mechanism, the practice of Last Observation Carried Forward introduces a systematic bias, attenuating the expected measured change by a factor directly related to the probability of missing data.", "answer": "$$\n\\boxed{1-p}\n$$", "id": "4536755"}]}