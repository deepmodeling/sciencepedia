{"hands_on_practices": [{"introduction": "The first step in quantifying intra-tumor heterogeneity is to analyze the statistical properties of the voxel intensities within a tumor region. This exercise introduces two fundamental metrics: the coefficient of variation ($CV$), which measures the relative dispersion of intensities, and Shannon entropy ($H$), which quantifies the complexity of the intensity distribution. By calculating these \"first-order\" statistics, you will learn to derive a foundational understanding of heterogeneity from imaging data without considering the spatial arrangement of voxels [@problem_id:4547773].", "problem": "A radiomics workflow is applied to a single-scan, multi-parametric Magnetic Resonance Imaging (MRI) of a solid tumor. Habitat imaging has partitioned the tumor into a hypoxic core and a proliferative rim based on normalized signal intensity. Intensities have been min-max normalized to the unit interval so that each voxel intensity is dimensionless in the range $[0,1]$. You are given the intensities for $20$ voxels within the contiguous tumor region (combining both habitats) as follows: hypoxic core voxels $0.90, 0.85, 0.92, 0.88, 0.81, 0.95, 0.78, 0.83, 0.89, 0.87$ and proliferative rim voxels $0.45, 0.52, 0.40, 0.55, 0.47, 0.58, 0.50, 0.60, 0.43, 0.48$. Using the core definitions in statistics and information theory, proceed as follows:\n\n1. Treat the provided $20$-voxel set as a finite population. Compute the population mean $\\mu$ and the population standard deviation $\\sigma$ defined by $\\sigma=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i-\\mu)^{2}}$ for $N=20$, where $x_i$ are the given voxel intensities. Then compute the coefficient of variation, defined conceptually as the ratio of spread to central tendency, $\\mathrm{CV}=\\sigma/\\mu$. Express $\\mathrm{CV}$ as a dimensionless quantity.\n\n2. To quantify distributional heterogeneity, construct a histogram by binning voxel intensities into four non-overlapping intervals: $\\mathcal{B}_1=[0.35,0.50)$, $\\mathcal{B}_2=[0.50,0.65)$, $\\mathcal{B}_3=[0.65,0.80)$, and $\\mathcal{B}_4=[0.80,1.00]$. Let $p_i$ be the empirical probability of bin $\\mathcal{B}_i$, given by the count in $\\mathcal{B}_i$ divided by $N$. Using the definition from information theory with the natural logarithm, compute the Shannon entropy $H=-\\sum_{i=1}^{4}p_i\\,\\ln(p_i)$. Express $H$ in units of natural information content (nats).\n\n3. Briefly interpret, using first principles about dispersion and uncertainty, what the computed $\\mathrm{CV}$ and $H$ imply about intra-tumor heterogeneity across the two habitats.\n\nRound both numerical results to four significant figures. Report $\\mathrm{CV}$ as dimensionless and $H$ in nats. For the final answer, provide only the pair $(\\mathrm{CV}, H)$.", "solution": "The problem statement has been validated and is determined to be sound. It is scientifically grounded in established principles of radiomics, statistics, and information theory, is well-posed with all necessary data and definitions, and is expressed in objective, formal language. It presents a solvable, non-trivial problem.\n\nThe problem requires a three-part analysis of a given set of $N=20$ voxel intensities from a tumor, partitioned into a hypoxic core and a proliferative rim.\n\nFirst, the complete set of $N=20$ voxel intensities, $x_i$, is:\n$$ \\{0.90, 0.85, 0.92, 0.88, 0.81, 0.95, 0.78, 0.83, 0.89, 0.87, 0.45, 0.52, 0.40, 0.55, 0.47, 0.58, 0.50, 0.60, 0.43, 0.48\\} $$\n\n**Part 1: Computation of $\\mu$, $\\sigma$, and $\\mathrm{CV}$**\n\nThe population mean, $\\mu$, is the arithmetic average of the $N$ intensities.\nThe sum of the intensities is:\n$$ \\sum_{i=1}^{20} x_i = (0.90+0.85+0.92+0.88+0.81+0.95+0.78+0.83+0.89+0.87) + (0.45+0.52+0.40+0.55+0.47+0.58+0.50+0.60+0.43+0.48) $$\n$$ \\sum_{i=1}^{20} x_i = 8.68 + 4.98 = 13.66 $$\nThe population mean is:\n$$ \\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i = \\frac{13.66}{20} = 0.683 $$\nNext, we compute the population standard deviation, $\\sigma$. We first calculate the population variance, $\\sigma^2$, using the formula $\\sigma^2 = \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_i^2\\right) - \\mu^2$.\nThe sum of the squares of the intensities is:\n$$ \\sum_{i=1}^{20} x_i^2 = (0.90^2 + \\dots + 0.87^2) + (0.45^2 + \\dots + 0.48^2) $$\n$$ \\sum_{i=1}^{20} x_i^2 = (0.81 + 0.7225 + 0.8464 + 0.7744 + 0.6561 + 0.9025 + 0.6084 + 0.6889 + 0.7921 + 0.7569) + (0.2025 + 0.2704 + 0.16 + 0.3025 + 0.2209 + 0.3364 + 0.25 + 0.36 + 0.1849 + 0.2304) $$\n$$ \\sum_{i=1}^{20} x_i^2 = 7.5582 + 2.5180 = 10.0762 $$\nThe variance is:\n$$ \\sigma^2 = \\frac{10.0762}{20} - (0.683)^2 = 0.50381 - 0.466489 = 0.037321 $$\nThe standard deviation is the square root of the variance:\n$$ \\sigma = \\sqrt{0.037321} \\approx 0.1931864 $$\nThe coefficient of variation, $\\mathrm{CV}$, is the ratio of the standard deviation to the mean:\n$$ \\mathrm{CV} = \\frac{\\sigma}{\\mu} = \\frac{0.1931864}{0.683} \\approx 0.282850 $$\nRounding to four significant figures, $\\mathrm{CV} \\approx 0.2829$.\n\n**Part 2: Computation of Shannon Entropy $H$**\n\nWe first sort the $N=20$ voxel intensities into the four specified bins and determine the counts.\nThe bins are $\\mathcal{B}_1=[0.35,0.50)$, $\\mathcal{B}_2=[0.50,0.65)$, $\\mathcal{B}_3=[0.65,0.80)$, and $\\mathcal{B}_4=[0.80,1.00]$.\n- Count for $\\mathcal{B}_1$: Voxel intensities $\\{0.45, 0.40, 0.47, 0.43, 0.48\\}$. The count is $c_1=5$.\n- Count for $\\mathcal{B}_2$: Voxel intensities $\\{0.52, 0.55, 0.58, 0.50, 0.60\\}$. The count is $c_2=5$.\n- Count for $\\mathcal{B}_3$: Voxel intensity $\\{0.78\\}$. The count is $c_3=1$.\n- Count for $\\mathcal{B}_4$: Voxel intensities $\\{0.90, 0.85, 0.92, 0.88, 0.81, 0.95, 0.83, 0.89, 0.87\\}$. The count is $c_4=9$.\nThe total count is $c_1+c_2+c_3+c_4 = 5+5+1+9=20$, which matches $N$.\nThe empirical probabilities, $p_i = c_i/N$, are:\n$$ p_1 = \\frac{5}{20} = 0.25 $$\n$$ p_2 = \\frac{5}{20} = 0.25 $$\n$$ p_3 = \\frac{1}{20} = 0.05 $$\n$$ p_4 = \\frac{9}{20} = 0.45 $$\nThe Shannon entropy, $H$, is calculated using the formula $H=-\\sum_{i=1}^{4}p_i\\,\\ln(p_i)$.\n$$ H = -[p_1\\ln(p_1) + p_2\\ln(p_2) + p_3\\ln(p_3) + p_4\\ln(p_4)] $$\n$$ H = -[0.25\\ln(0.25) + 0.25\\ln(0.25) + 0.05\\ln(0.05) + 0.45\\ln(0.45)] $$\nSubstituting the values of the natural logarithms:\n$$ H \\approx -[0.25(-1.38629) + 0.25(-1.38629) + 0.05(-2.99573) + 0.45(-0.79851)] $$\n$$ H \\approx -[-0.34657 - 0.34657 - 0.14979 - 0.35933] $$\n$$ H \\approx -[-1.20226] \\approx 1.20226 \\text{ nats} $$\nRounding to four significant figures, $H \\approx 1.202$ nats.\n\n**Part 3: Interpretation**\n\nThe computed metrics, $\\mathrm{CV}$ and $H$, quantify different aspects of intra-tumor heterogeneity.\n- The coefficient of variation, $\\mathrm{CV} \\approx 0.2829$, measures the relative dispersion of the voxel intensities. From first principles, it is the ratio of the spread ($\\sigma$) to the central tendency ($\\mu$). A value of $28.3\\%$ indicates a moderately high level of variability. This signifies that the voxel intensities are not tightly clustered around the mean value but are significantly spread out. This spread is a direct and expected consequence of combining two distinct voxel populations (hypoxic core and proliferative rim) which have demonstrably different mean intensity levels.\n- The Shannon entropy, $H \\approx 1.202$ nats, measures the uncertainty or disorder in the distribution of voxel intensities across the predefined bins. The maximum possible entropy for four bins is $H_{\\text{max}} = \\ln(4) \\approx 1.386$ nats, which would occur if all bins were equally populated. The calculated value is high, being approximately $87\\%$ of the maximum. From first principles, entropy quantifies the unpredictability of observing a voxel in a particular intensity bin. The high value of $H$ implies that the voxel intensities are distributed across multiple bins rather than being concentrated in one, reflecting a complex intensity profile. This lack of a single dominant intensity state is a hallmark of heterogeneity, caused here by the co-existence of the two distinct tumor habitats.\n\nIn summary, both the high $\\mathrm{CV}$ and high $H$ values indicate significant intra-tumor heterogeneity. The $\\mathrm{CV}$ captures the magnitude of the intensity variation, while $H$ captures the complexity and unpredictability of the intensity distribution's shape.", "answer": "$$ \\boxed{\\begin{pmatrix} 0.2829 & 1.202 \\end{pmatrix}} $$", "id": "4547773"}, {"introduction": "While first-order statistics describe what intensities are present, they do not tell us where they are located. To capture the spatial arrangement of voxels, or texture, we turn to \"second-order\" statistics, with the Gray-Level Co-occurrence Matrix (GLCM) being a cornerstone technique [@problem_id:4547797]. This practice will guide you through constructing a GLCM from a small image patch and calculating key texture features like contrast and entropy, providing insight into local patterns of uniformity and complexity.", "problem": "A tumor region-of-interest (ROI) in a magnetic resonance imaging (MRI) slice has been preprocessed and quantized into $4$ gray levels, labeled $0,1,2,3$, to reflect distinct intra-tumor habitats such as necrotic, hypoxic, and well-perfused subregions. Consider the following $4 \\times 4$ patch (rows listed top to bottom), where each entry is a quantized gray level:\n$$\nI \\;=\\;\n\\begin{bmatrix}\n0 & 0 & 1 & 1 \\\\\n0 & 2 & 2 & 1 \\\\\n3 & 2 & 2 & 1 \\\\\n3 & 3 & 2 & 1\n\\end{bmatrix}.\n$$\nUsing the gray-level co-occurrence matrix (GLCM) framework, proceed as follows:\n- Construct the co-occurrence count matrix $N(i,j)$ using one-pixel horizontal adjacency to the right within the patch (that is, for each row, consider ordered pairs of the form $\\big(I(r,c), I(r,c+1)\\big)$).\n- Form the symmetric count matrix $N_{\\text{sym}}(i,j) = N(i,j) + N(j,i)$ to capture undirected adjacency.\n- Normalize $N_{\\text{sym}}$ to a probability mass function $P(i,j)$ by dividing every entry by the sum of all entries in $N_{\\text{sym}}$.\n\nUsing only foundational definitions from second-order statistics and information theory (no shortcut formulas), compute:\n1. The GLCM contrast for this patch, understood as the expected squared difference of gray levels between horizontally adjacent pixels under $P(i,j)$.\n2. The GLCM entropy of $P(i,j)$, using the natural logarithm and expressing the result in nats.\n\nExplain briefly how the numerical values you obtain reflect local heterogeneity versus uniformity in the context of habitat imaging. Round both the contrast and the entropy to four significant figures. Report your final answer as a row vector $\\big(\\text{contrast},\\ \\text{entropy}\\big)$, with the entropy expressed in nats and no units included in the final boxed values.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Image patch $I$:\n$$ I = \\begin{bmatrix} 0 & 0 & 1 & 1 \\\\ 0 & 2 & 2 & 1 \\\\ 3 & 2 & 2 & 1 \\\\ 3 & 3 & 2 & 1 \\end{bmatrix} $$\n- Gray levels: $\\{0, 1, 2, 3\\}$.\n- Co-occurrence rule: One-pixel horizontal adjacency to the right, i.e., pairs of the form $(I(r,c), I(r,c+1))$.\n- Co-occurrence count matrix: $N(i,j)$.\n- Symmetric count matrix: $N_{\\text{sym}}(i,j) = N(i,j) + N(j,i)$.\n- Normalized probability matrix: $P(i,j)$ is obtained by normalizing $N_{\\text{sym}}$ such that the sum of its entries is $1$.\n- GLCM Contrast: Defined as the expected squared difference of gray levels, $E[(i-j)^2]$, under the probability distribution $P(i,j)$.\n- GLCM Entropy: Defined using the natural logarithm, $H = - \\sum_{i,j} P(i,j) \\ln(P(i,j))$.\n- Output requirement: A row vector $(\\text{contrast}, \\text{entropy})$, with both values rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem uses standard definitions and procedures from the field of radiomics and texture analysis. Gray-Level Co-occurrence Matrix (GLCM), contrast, and entropy are fundamental concepts in this domain. The scenario of quantizing MRI data to represent tumor habitats is a common application.\n- **Well-Posed**: The problem provides all necessary data (the image patch), clear and unambiguous instructions for constructing the matrices ($N$, $N_{\\text{sym}}$, $P$), and precise definitions for the quantities to be computed (contrast, entropy). A unique solution exists.\n- **Objective**: The problem is stated in precise, mathematical language, free of any subjective or ambiguous terms.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now be executed.\n\n### Solution Derivation\n\n**Part 1: Construct the GLCMs**\n\nFirst, we construct the non-symmetric co-occurrence count matrix $N(i,j)$ by tallying the specified horizontal pixel pairs. The image patch is $4 \\times 4$, so there are $4$ rows and $3$ pairs per row, for a total of $4 \\times 3 = 12$ pairs.\nThe pairs $(I(r,c), I(r,c+1))$ are:\n- Row 1: $(0,0), (0,1), (1,1)$\n- Row 2: $(0,2), (2,2), (2,1)$\n- Row 3: $(3,2), (2,2), (2,1)$\n- Row 4: $(3,3), (3,2), (2,1)$\n\nTallying these pairs gives the counts for $N(i,j)$, where $i$ is the first gray level in a pair and $j$ is the second.\n- $N(0,0)=1$\n- $N(0,1)=1$\n- $N(0,2)=1$\n- $N(1,1)=1$\n- $N(2,1)=3$\n- $N(2,2)=2$\n- $N(3,2)=2$\n- $N(3,3)=1$\nAll other entries are $0$. The matrix $N(i,j)$ is:\n$$ N(i,j) = \\begin{bmatrix} 1 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 3 & 2 & 0 \\\\ 0 & 0 & 2 & 1 \\end{bmatrix} $$\nThe sum of all entries in $N$ is $1+1+1+1+3+2+2+1 = 12$, which matches the total number of pairs.\n\nNext, we form the symmetric count matrix $N_{\\text{sym}}(i,j) = N(i,j) + N(j,i) = N + N^T$.\n$$ N_{\\text{sym}}(i,j) = \\begin{bmatrix} 1 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 3 & 2 & 0 \\\\ 0 & 0 & 2 & 1 \\end{bmatrix} + \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 1 & 3 & 0 \\\\ 1 & 0 & 2 & 2 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 & 0 \\\\ 1 & 2 & 3 & 0 \\\\ 1 & 3 & 4 & 2 \\\\ 0 & 0 & 2 & 2 \\end{bmatrix} $$\nThe total number of counts in $N_{\\text{sym}}$ is the sum of all its entries, which is $24$.\n\nFinally, we normalize $N_{\\text{sym}}$ to obtain the probability mass function $P(i,j)$ by dividing each entry by $24$.\n$$ P(i,j) = \\frac{1}{24} N_{\\text{sym}}(i,j) = \\frac{1}{24} \\begin{bmatrix} 2 & 1 & 1 & 0 \\\\ 1 & 2 & 3 & 0 \\\\ 1 & 3 & 4 & 2 \\\\ 0 & 0 & 2 & 2 \\end{bmatrix} = \\begin{bmatrix} 2/24 & 1/24 & 1/24 & 0 \\\\ 1/24 & 2/24 & 3/24 & 0 \\\\ 1/24 & 3/24 & 4/24 & 2/24 \\\\ 0 & 0 & 2/24 & 2/24 \\end{bmatrix} $$\n\n**Part 2: Compute GLCM Contrast**\n\nThe GLCM contrast is defined as the expected value of the squared difference between gray levels:\n$$ C = \\sum_{i=0}^{3} \\sum_{j=0}^{3} (i-j)^2 P(i,j) $$\nWe can compute this sum by weighting the non-zero probabilities by the corresponding $(i-j)^2$ values.\n- For $P(0,0)=\\frac{2}{24}$, $(i-j)^2 = (0-0)^2 = 0$.\n- For $P(0,1)=\\frac{1}{24}$, $(i-j)^2 = (0-1)^2 = 1$.\n- For $P(0,2)=\\frac{1}{24}$, $(i-j)^2 = (0-2)^2 = 4$.\n- For $P(1,0)=\\frac{1}{24}$, $(i-j)^2 = (1-0)^2 = 1$.\n- For $P(1,1)=\\frac{2}{24}$, $(i-j)^2 = (1-1)^2 = 0$.\n- For $P(1,2)=\\frac{3}{24}$, $(i-j)^2 = (1-2)^2 = 1$.\n- For $P(2,0)=\\frac{1}{24}$, $(i-j)^2 = (2-0)^2 = 4$.\n- For $P(2,1)=\\frac{3}{24}$, $(i-j)^2 = (2-1)^2 = 1$.\n- For $P(2,2)=\\frac{4}{24}$, $(i-j)^2 = (2-2)^2 = 0$.\n- For $P(2,3)=\\frac{2}{24}$, $(i-j)^2 = (2-3)^2 = 1$.\n- For $P(3,2)=\\frac{2}{24}$, $(i-j)^2 = (3-2)^2 = 1$.\n- For $P(3,3)=\\frac{2}{24}$, $(i-j)^2 = (3-3)^2 = 0$.\n\nSumming the products:\n$$ C = \\frac{1}{24} \\left[ 2(0) + 1(1) + 1(4) + 1(1) + 2(0) + 3(1) + 1(4) + 3(1) + 4(0) + 2(1) + 2(1) + 2(0) \\right] $$\n$$ C = \\frac{1}{24} \\left[ 0 + 1 + 4 + 1 + 0 + 3 + 4 + 3 + 0 + 2 + 2 + 0 \\right] $$\n$$ C = \\frac{20}{24} = \\frac{5}{6} $$\nNumerically, $C \\approx 0.833333...$. Rounded to four significant figures, the contrast is $0.8333$.\n\n**Part 3: Compute GLCM Entropy**\n\nThe GLCM entropy is defined as:\n$$ H = - \\sum_{i=0}^{3} \\sum_{j=0}^{3} P(i,j) \\ln(P(i,j)) $$\nThe summation is over all non-zero $P(i,j)$. Let's group terms by common probability values. The non-zero entries in $N_{\\text{sym}}$ are $1$, $2$, $3$, and $4$.\n- $N_{\\text{sym}}$ has $4$ entries with value $1$. The probability is $p_1 = \\frac{1}{24}$.\n- $N_{\\text{sym}}$ has $5$ entries with value $2$. The probability is $p_2 = \\frac{2}{24} = \\frac{1}{12}$.\n- $N_{\\text{sym}}$ has $2$ entries with value $3$. The probability is $p_3 = \\frac{3}{24} = \\frac{1}{8}$.\n- $N_{\\text{sym}}$ has $1$ entry with value $4$. The probability is $p_4 = \\frac{4}{24} = \\frac{1}{6}$.\n\nThe entropy calculation is:\n$$ H = - \\left[ 4 \\left( \\frac{1}{24} \\ln\\frac{1}{24} \\right) + 5 \\left( \\frac{2}{24} \\ln\\frac{2}{24} \\right) + 2 \\left( \\frac{3}{24} \\ln\\frac{3}{24} \\right) + 1 \\left( \\frac{4}{24} \\ln\\frac{4}{24} \\right) \\right] $$\n$$ H = - \\frac{1}{24} \\left[ 4(\\ln 1 - \\ln 24) + 10(\\ln 2 - \\ln 24) + 6(\\ln 3 - \\ln 24) + 4(\\ln 4 - \\ln 24) \\right] $$\n$$ H = - \\frac{1}{24} \\left[ 4(0) + 10\\ln 2 + 6\\ln 3 + 4\\ln(2^2) - (4+10+6+4)\\ln 24 \\right] $$\n$$ H = - \\frac{1}{24} \\left[ 10\\ln 2 + 6\\ln 3 + 8\\ln 2 - 24\\ln 24 \\right] $$\n$$ H = - \\frac{1}{24} \\left[ 18\\ln 2 + 6\\ln 3 - 24\\ln 24 \\right] $$\n$$ H = \\ln 24 - \\frac{18}{24}\\ln 2 - \\frac{6}{24}\\ln 3 = \\ln 24 - \\frac{3}{4}\\ln 2 - \\frac{1}{4}\\ln 3 $$\nSince $\\ln 24 = \\ln(2^3 \\cdot 3) = 3\\ln 2 + \\ln 3$, we can substitute this in:\n$$ H = (3\\ln 2 + \\ln 3) - \\frac{3}{4}\\ln 2 - \\frac{1}{4}\\ln 3 $$\n$$ H = \\left(3 - \\frac{3}{4}\\right)\\ln 2 + \\left(1 - \\frac{1}{4}\\right)\\ln 3 = \\frac{9}{4}\\ln 2 + \\frac{3}{4}\\ln 3 $$\n$$ H = \\frac{3}{4}(3\\ln 2 + \\ln 3) = \\frac{3}{4}(\\ln 2^3 + \\ln 3) = \\frac{3}{4}\\ln(8 \\cdot 3) = \\frac{3}{4}\\ln(24) $$\nNumerically, $\\ln(24) \\approx 3.1780538...$.\n$$ H \\approx \\frac{3}{4} \\times 3.1780538 \\approx 2.3835404... $$\nRounded to four significant figures, the entropy is $2.384$ nats.\n\n### Interpretation\n\n- **Contrast ($C \\approx 0.8333$):** This value quantifies the local intensity variations. Since $C>0$, the patch is not uniform. The value is moderate, reflecting the presence of boundaries between different gray levels (habitats) but a lack of extreme changes (e.g., no direct adjacencies between $0$ and $3$). This suggests a heterogeneous texture with discernible but not sharply defined subregions.\n\n- **Entropy ($H \\approx 2.384$ nats):** This value measures the disorder or complexity of the texture. For a system with $12$ possible non-zero states (as in our $P(i,j)$ matrix), the maximum possible entropy would occur if all states were equally likely, giving $H_{\\text{max}} = -\\sum_{k=1}^{12} \\frac{1}{12} \\ln(\\frac{1}{12}) = \\ln(12) \\approx 2.485$ nats. Our calculated entropy of $2.384$ nats is very close to this maximum value. This indicates a highly complex and disorganized spatial arrangement of gray levels. In the context of habitat imaging, this high entropy signifies substantial intra-tumor heterogeneity, where different functional habitats are highly intermixed rather than being segregated into large, uniform zones.", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.8333 & 2.384 \\end{pmatrix} } $$", "id": "4547797"}, {"introduction": "The culmination of heterogeneity analysis is \"habitat imaging\"â€”the partitioning of a tumor into biologically distinct subregions. This is often achieved by applying machine learning algorithms to multiparametric imaging data, where each voxel is described by multiple features (e.g., from MRI and PET) [@problem_id:4547809]. In this advanced, code-based exercise, you will implement the k-means clustering algorithm to group voxels into distinct habitats and interpret them based on their underlying physiological feature profiles, simulating a complete radiomics workflow from raw data to biological insight.", "problem": "You are given multiparametric imaging feature vectors for voxels inside a tumor from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). Each voxel has three features: Apparent Diffusion Coefficient (ADC, in $\\mathrm{mm^2/s}$), normalized T2-weighted intensity (dimensionless), and Standardized Uptake Value (SUV, dimensionless). In radiomics, intra-tumor heterogeneity can be modeled as habitats by clustering such multiparametric feature vectors. Your task is to implement a deterministic $k$-means clustering with $k=3$ in the standardized feature space and then interpret each cluster as a habitat based on its feature profile in the original units.\n\nUse the following foundational base:\n- In $k$-means clustering, the aim is to minimize the within-cluster sum of squares. Given data points $\\{x_i\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^d$ and $k$ clusters with centroids $\\{\\mu_j\\}_{j=1}^k$, the objective is\n$$\n\\min_{\\{\\mu_j\\},\\{c_i\\}} \\sum_{i=1}^n \\left\\| x_i - \\mu_{c_i} \\right\\|_2^2,\n$$\nwhere $c_i \\in \\{0,1,\\dots,k-1\\}$ denotes the cluster assignment of point $i$.\n- Standardization by $z$-score for each feature dimension $f$ transforms raw values $x_{i,f}$ to\n$$\nz_{i,f} = \\frac{x_{i,f} - \\bar{x}_f}{s_f},\n$$\nwhere $\\bar{x}_f$ is the mean and $s_f$ is the sample standard deviation across all points for feature $f$. If $s_f=0$, set $s_f=1$ to avoid division by zero (this yields $z_{i,f}=0$ for all $i$ in that feature, which is consistent with no variation).\n\nAlgorithm requirements:\n- Work in the standardized space for clustering.\n- Initialization of centroids must be deterministic: sort points by increasing SUV; for ties, sort by increasing ADC; for remaining ties, sort by increasing T2. Select the first, the median, and the last point in this sorted order as the initial centroids for $k=3$.\n- Assignment step: assign each point to the nearest centroid by Euclidean distance in the standardized space. In the event of an exact tie in distance to multiple centroids, choose the centroid with the smaller Euclidean norm of its centroid vector in the standardized space; if norms are equal, choose the centroid with the smaller index.\n- Update step: move each centroid to the mean of the points currently assigned to it in the standardized space. If a centroid has zero assigned points at any iteration, leave it unchanged for that iteration.\n- Stopping criterion: stop when assignments do not change between successive iterations or after $100$ iterations, whichever comes first.\n\nPost-clustering computations:\n- Compute cluster centroids in original units (ADC in $\\mathrm{mm^2/s}$, T2 dimensionless, SUV dimensionless) by averaging the original features of the points assigned to each cluster. If a cluster has zero assigned points at convergence, set its centroid equal to the global mean vector of the original features.\n- Habitat interpretation rule using original units and global medians computed over all points:\n    - Compute $\\tilde{m}_{\\mathrm{ADC}}$ as the median ADC and $\\tilde{m}_{\\mathrm{SUV}}$ as the median SUV over all points.\n    - For each cluster centroid with $(\\bar{\\mathrm{ADC}}, \\bar{\\mathrm{T2}}, \\bar{\\mathrm{SUV}})$ in original units, assign a habitat code as follows:\n        - Code $1$ (aggressive/proliferative-like) if $\\bar{\\mathrm{SUV}} > \\tilde{m}_{\\mathrm{SUV}}$ and $\\bar{\\mathrm{ADC}} < \\tilde{m}_{\\mathrm{ADC}}$.\n        - Code $2$ (necrotic/edematous-like) if $\\bar{\\mathrm{SUV}} \\le \\tilde{m}_{\\mathrm{SUV}}$ and $\\bar{\\mathrm{ADC}} \\ge \\tilde{m}_{\\mathrm{ADC}}$.\n        - Code $3$ (intermediate/viable-like) otherwise.\n    - Map each point's cluster assignment to its cluster's habitat code to yield per-point habitat labels.\n\nUnits and output:\n- ADC must be handled in $\\mathrm{mm^2/s}$, expressed as floats in that unit in the output centroids. T2 and SUV are dimensionless and expressed as floats.\n- The final output must be a single line containing a list of results for all test cases, where each test case result is a list with three elements:\n    $[C,A,H]$, with\n    - $C$: a flattened list of centroid values in original units ordered by cluster index $0,1,2$ and within each centroid ordered as $[\\mathrm{ADC}, \\mathrm{T2}, \\mathrm{SUV}]$, each rounded to $6$ decimal places,\n    - $A$: the list of per-point cluster assignments (integers $0,1,2$),\n    - $H$: the list of per-point habitat codes (integers $1,2,3$).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with nested lists similarly formatted (e.g., $[[C_1,A_1,H_1],[C_2,A_2,H_2],\\dots]$).\n\nTest suite:\n- Case $1$ (clear three habitats): $9$ points\n    - $[0.00080, 0.30, 12.0]$, $[0.00085, 0.25, 11.0]$, $[0.00090, 0.35, 13.0]$,\n    - $[0.00170, 0.75, 3.0]$, $[0.00160, 0.70, 2.5]$, $[0.00180, 0.80, 4.0]$,\n    - $[0.00120, 0.50, 7.0]$, $[0.00125, 0.55, 6.5]$, $[0.00110, 0.45, 6.8]$.\n- Case $2$ (degenerate identical points): $6$ points\n    - $[0.00120, 0.50, 6.0]$ repeated $6$ times.\n- Case $3$ (near-uniform cluster tendency): $4$ points\n    - $[0.00100, 0.40, 8.0]$, $[0.00101, 0.41, 8.1]$, $[0.00099, 0.39, 7.9]$, $[0.00100, 0.40, 8.0]$.\n- Case $4$ (tie behavior with symmetric variation): $4$ points\n    - $[0.00120, 0.50, 6.0]$, $[0.00130, 0.50, 6.0]$, $[0.00110, 0.50, 6.0]$, $[0.00120, 0.50, 6.0]$.\n\nImplement the specified deterministic $k$-means, habitat interpretation, and output format exactly as described. No external input is required. The program must run as is and print the required single-line output.", "solution": "The problem requires the implementation of a deterministic $k$-means clustering algorithm for segmenting tumor voxels into habitats based on multiparametric imaging data. The process involves feature standardization, a specified iterative clustering procedure, and a rule-based interpretation of the resulting clusters. The solution is presented as a step-by-step breakdown of the required methodology.\n\nThe input data for each voxel is a $3$-dimensional feature vector consisting of Apparent Diffusion Coefficient (ADC, in $\\mathrm{mm^2/s}$), normalized T2-weighted intensity (dimensionless), and Standardized Uptake Value (SUV, dimensionless). The number of clusters is fixed at $k=3$.\n\nThe first step is to standardize the raw feature vectors. Clustering is performed in this standardized space to ensure that features with different scales and units contribute equally to the distance calculations. For each feature dimension $f$, a raw value $x_{i,f}$ for point $i$ is transformed into a $z$-score $z_{i,f}$ using the formula:\n$$\nz_{i,f} = \\frac{x_{i,f} - \\bar{x}_f}{s_f}\n$$\nwhere $\\bar{x}_f$ is the mean and $s_f$ is the sample standard deviation of feature $f$ across all $n$ data points. A specific rule is provided to handle cases where a feature has no variance: if $s_f=0$, the value of $s_f$ is set to $1$ before the division. This correctly results in $z_{i,f}=0$ for all points in that constant feature dimension.\n\nNext, a deterministic $k$-means algorithm is applied to the standardized data points, $\\{z_i\\}_{i=1}^n$. The algorithm is designed to minimize the within-cluster sum of squared Euclidean distances, $\\sum_{i=1}^n \\left\\| z_i - \\mu_{c_i} \\right\\|_2^2$, where $\\mu_j$ is the centroid of cluster $j$ and $c_i$ is the cluster assignment for point $i$. The algorithm proceeds as follows:\n\n1.  **Centroid Initialization**: The initial centroids are chosen deterministically to ensure reproducibility. First, all data points are sorted. The primary sort key is the SUV in increasing order. Ties are broken by sorting on the ADC in increasing order, and any remaining ties are broken by the T2-weighted intensity in increasing order. For $n$ points, the initial three centroids are set to be the standardized feature vectors of the points at the first (index $0$), median (index $\\lfloor(n-1)/2\\rfloor$), and last (index $n-1$) positions in this sorted list.\n\n2.  **Iterative Refinement**: The algorithm then iterates through two steps, assignment and update, until a stopping condition is met.\n    *   **Assignment Step**: Each data point $z_i$ is assigned to the cluster $j$ whose centroid $\\mu_j$ is closest in terms of Euclidean distance. To handle ties, if a point is equidistant to multiple centroids, it is assigned to the cluster whose centroid vector has the smallest Euclidean norm $\\|\\mu_j\\|_2$. If a tie persists (i.e., centroid norms are also equal), the point is assigned to the cluster with the smallest index $j \\in \\{0, 1, 2\\}$.\n    *   **Update Step**: After all points are assigned, the centroids are updated. For each cluster $j$, the new centroid $\\mu_j$ is calculated as the component-wise mean of all standardized data points currently assigned to it. If a cluster becomes empty (i.e., has zero points assigned to it), its centroid is not updated in that iteration.\n\n3.  **Termination**: The iterative process stops when either the cluster assignments for all points do not change from one iteration to the next, or a maximum of $100$ iterations has been reached.\n\nAfter the clustering process converges, post-processing is performed to interpret the results in the context of tumor biology.\n\n1.  **Centroid Conversion**: The final cluster centroids, which are in the standardized space, are converted back to the original feature space. For each cluster $j$, its centroid in original units, $(\\bar{\\mathrm{ADC}}_j, \\bar{\\mathrm{T2}}_j, \\bar{\\mathrm{SUV}}_j)$, is calculated by taking the mean of the original feature vectors of all points assigned to that cluster. If a cluster is empty at convergence, its centroid is defined as the global mean vector of all original feature vectors.\n\n2.  **Habitat Interpretation**: Each cluster is classified into one of three habitat types based on its properties in the original feature space. This classification uses the global median ADC, $\\tilde{m}_{\\mathrm{ADC}}$, and global median SUV, $\\tilde{m}_{\\mathrm{SUV}}$, calculated over all input data points. The rules are:\n    *   **Code $1$ (aggressive/proliferative-like)**: Assigned if the cluster's mean SUV is above the median SUV and its mean ADC is below the median ADC ($\\bar{\\mathrm{SUV}} > \\tilde{m}_{\\mathrm{SUV}}$ and $\\bar{\\mathrm{ADC}} < \\tilde{m}_{\\mathrm{ADC}}$). This corresponds to high metabolic activity and high cellularity.\n    *   **Code $2$ (necrotic/edematous-like)**: Assigned if the cluster's mean SUV is less than or equal to the median SUV and its mean ADC is greater than or equal to the median ADC ($\\bar{\\mathrm{SUV}} \\le \\tilde{m}_{\\mathrm{SUV}}$ and $\\bar{\\mathrm{ADC}} \\ge \\tilde{m}_{\\mathrm{ADC}}$). This corresponds to low metabolic activity and low cellularity/high water content.\n    *   **Code $3$ (intermediate/viable-like)**: Assigned if neither of the above conditions is met.\n\nFinally, the results for each test case are compiled into a specific output format. For each case, the output is a list $[C, A, H]$, where $C$ is a flattened list of the three original-space centroid vectors (ordered by cluster index $0, 1, 2$ and feature ADC, T2, SUV), with values rounded to $6$ decimal places; $A$ is the list of final cluster assignments ($0, 1, 2$) for each point; and $H$ is the list of corresponding habitat codes ($1, 2, 3$) for each point. The final program output is a single line containing a list of these results for all provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic k-means clustering for radiomics habitat analysis.\n    The function processes a suite of test cases, applying standardization,\n    a custom k-means algorithm, and a rule-based habitat interpretation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (clear three habitats): 9 points\n        [\n            [0.00080, 0.30, 12.0], [0.00085, 0.25, 11.0], [0.00090, 0.35, 13.0],\n            [0.00170, 0.75, 3.0], [0.00160, 0.70, 2.5], [0.00180, 0.80, 4.0],\n            [0.00120, 0.50, 7.0], [0.00125, 0.55, 6.5], [0.00110, 0.45, 6.8]\n        ],\n        # Case 2 (degenerate identical points): 6 points\n        [\n            [0.00120, 0.50, 6.0], [0.00120, 0.50, 6.0], [0.00120, 0.50, 6.0],\n            [0.00120, 0.50, 6.0], [0.00120, 0.50, 6.0], [0.00120, 0.50, 6.0]\n        ],\n        # Case 3 (near-uniform cluster tendency): 4 points\n        [\n            [0.00100, 0.40, 8.0], [0.00101, 0.41, 8.1],\n            [0.00099, 0.39, 7.9], [0.00100, 0.40, 8.0]\n        ],\n        # Case 4 (tie behavior with symmetric variation): 4 points\n        [\n            [0.00120, 0.50, 6.0], [0.00130, 0.50, 6.0],\n            [0.00110, 0.50, 6.0], [0.00120, 0.50, 6.0]\n        ]\n    ]\n\n    all_results = []\n    _K = 3\n    _MAX_ITER = 100\n\n    for data in test_cases:\n        X_orig = np.array(data, dtype=float)\n        n_points, n_features = X_orig.shape\n\n        # Step 1: Standardization\n        mean_orig = np.mean(X_orig, axis=0)\n        std_orig = np.std(X_orig, axis=0, ddof=1)\n        std_orig[std_orig == 0] = 1.0  # Handle zero variance as per problem spec\n        X_std = (X_orig - mean_orig) / std_orig\n\n        # Step 2: Deterministic Centroid Initialization\n        # Sort indices by SUV (col 2), then ADC (col 0), then T2 (col 1)\n        # np.lexsort keys are in reverse order of sorting priority\n        sort_indices = np.lexsort((X_orig[:, 1], X_orig[:, 0], X_orig[:, 2]))\n        \n        median_idx_pos = (n_points - 1) // 2\n        centroid_indices = [\n            sort_indices[0],\n            sort_indices[median_idx_pos],\n            sort_indices[n_points - 1]\n        ]\n        centroids_std = X_std[centroid_indices]\n\n        # Step 3: k-means Iterations\n        assignments = np.full(n_points, -1, dtype=int)\n        for _ in range(_MAX_ITER):\n            old_assignments = np.copy(assignments)\n\n            # Assignment Step\n            sq_dists = np.sum((X_std[:, np.newaxis, :] - centroids_std[np.newaxis, :, :])**2, axis=2)\n\n            for i in range(n_points):\n                min_sq_dist = np.min(sq_dists[i])\n                tied_indices = np.where(sq_dists[i] == min_sq_dist)[0]\n                \n                if len(tied_indices) == 1:\n                    assignments[i] = tied_indices[0]\n                else:\n                    # Tie-breaking 1: Centroid norm\n                    tied_centroids = centroids_std[tied_indices]\n                    sq_norms = np.sum(tied_centroids**2, axis=1)\n                    min_sq_norm = np.min(sq_norms)\n                    \n                    # Indices within tied_indices that have the minimum norm\n                    norm_tied_indices = np.where(sq_norms == min_sq_norm)[0]\n                    \n                    # Tie-breaking 2: Smallest original centroid index\n                    final_candidate_indices = tied_indices[norm_tied_indices]\n                    assignments[i] = np.min(final_candidate_indices)\n\n            # Update Step\n            new_centroids_std = np.copy(centroids_std)\n            for j in range(_K):\n                points_in_cluster = X_std[assignments == j]\n                if len(points_in_cluster) > 0:\n                    new_centroids_std[j] = np.mean(points_in_cluster, axis=0)\n                # Else: centroid remains unchanged if cluster is empty\n            centroids_std = new_centroids_std\n            \n            # Convergence Check\n            if np.array_equal(assignments, old_assignments):\n                break\n\n        # Step 4: Post-processing\n        # C: Centroids in original units\n        centroids_orig = []\n        global_mean_orig = np.mean(X_orig, axis=0)\n        for j in range(_K):\n            cluster_points_orig = X_orig[assignments == j]\n            if len(cluster_points_orig) > 0:\n                centroids_orig.append(np.mean(cluster_points_orig, axis=0))\n            else:\n                centroids_orig.append(global_mean_orig)\n        \n        C_flat = np.array(centroids_orig).flatten().tolist()\n        C = [round(val, 6) for val in C_flat]\n\n        # A: Final assignments\n        A = assignments.tolist()\n\n        # H: Habitat codes\n        median_adc = np.median(X_orig[:, 0])\n        median_suv = np.median(X_orig[:, 2])\n\n        cluster_habitat_codes = {}\n        for j in range(_K):\n            adc_bar, _, suv_bar = centroids_orig[j]\n            if suv_bar > median_suv and adc_bar < median_adc:\n                code = 1\n            elif suv_bar <= median_suv and adc_bar >= median_adc:\n                code = 2\n            else:\n                code = 3\n            cluster_habitat_codes[j] = code\n        \n        H = [cluster_habitat_codes[assign] for assign in A]\n\n        all_results.append([C, A, H])\n\n    # Final print statement in the exact required format.\n    # str().replace() is used to produce the compact, no-space list representation.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "4547809"}]}