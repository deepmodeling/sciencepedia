{"hands_on_practices": [{"introduction": "Removing direct identifiers like names is a necessary first step, but it is often not sufficient to guarantee privacy. Combinations of indirect attributes, or \"quasi-identifiers,\" such as age, sex, and location, can be used to re-identify individuals. The principle of $k$-anonymity addresses this risk by ensuring that every individual in a dataset is indistinguishable from at least $k-1$ others. This exercise [@problem_id:4537603] provides a hands-on opportunity to quantify the level of privacy in a dataset by calculating its achieved $k$-anonymity value.", "problem": "A multi-institutional radiomics study aggregates imaging-derived features from patients while removing direct identifiers (for example, name and medical record number). To assess the risk of re-identification, the data steward treats age binned into five-year intervals, sex, and institution as quasi-identifiers. An equivalence class is defined as the set of records sharing identical quasi-identifier values. The goal is to determine the achieved $k$ under $k$-anonymity after generalizing age to five-year bins.\n\nThe dataset contains $N$ records distributed across three institutions $\\mathcal{I}_{1}$, $\\mathcal{I}_{2}$, and $\\mathcal{I}_{3}$; sex $S \\in \\{M, F\\}$; and age bins $\\{30$-$34, 35$-$39, 40$-$44, 45$-$49, 50$-$54\\}$. For each triple (age bin, sex, institution), the number of records is given below:\n\n- Age $30$-$34$, $S=M$, $\\mathcal{I}_{1}$: $3$; $S=F$, $\\mathcal{I}_{1}$: $2$; $S=M$, $\\mathcal{I}_{2}$: $2$; $S=F$, $\\mathcal{I}_{2}$: $2$; $S=M$, $\\mathcal{I}_{3}$: $2$; $S=F$, $\\mathcal{I}_{3}$: $3$.\n- Age $35$-$39$, $S=M$, $\\mathcal{I}_{1}$: $2$; $S=F$, $\\mathcal{I}_{1}$: $2$; $S=M$, $\\mathcalI}_{2}$: $3$; $S=F$, $\\mathcal{I}_{2}$: $2$; $S=M$, $\\mathcal{I}_{3}$: $2$; $S=F$, $\\mathcal{I}_{3}$: $2$.\n- Age $40$-$44$, $S=M$, $\\mathcal{I}_{1}$: $4$; $S=F$, $\\mathcal{I}_{1}$: $2$; $S=M$, $\\mathcal{I}_{2}$: $2$; $S=F$, $\\mathcal{I}_{2}$: $2$; $S=M$, $\\mathcal{I}_{3}$: $3$; $S=F$, $\\mathcal{I}_{3}$: $2$.\n- Age $45$-$49$, $S=M$, $\\mathcal{I}_{1}$: $2$; $S=F$, $\\mathcal{I}_{1}$: $2$; $S=M$, $\\mathcal{I}_{2}$: $2$; $S=F$, $\\mathcal{I}_{2}$: $2$; $S=M$, $\\mathcal{I}_{3}$: $2$; $S=F$, $\\mathcal{I}_{3}$: $2$.\n- Age $50$-$54$, $S=M$, $\\mathcal{I}_{1}$: $3$; $S=F$, $\\mathcal{I}_{1}$: $2$; $S=M$, $\\mathcal{I}_{2}$: $2$; $S=F$, $\\mathcal{I}_{2}$: $2$; $S=M$, $\\mathcal{I}_{3}$: $2$; $S=F$, $\\mathcal{I}_{3}$: $2$.\n\nUsing the definitions of quasi-identifiers, equivalence classes, and $k$-anonymity, construct the equivalence classes induced by the triple (age bin, sex, institution) and compute the achieved $k$. Express your final answer as an integer. No rounding is required, and no units are to be used.", "solution": "The problem requires the calculation of the achieved level of $k$-anonymity for a given dataset. This is a problem in the field of data privacy and security, which is an essential consideration in radiomics and medical data sharing.\n\nFirst, we must formally define the concepts as they apply to this problem.\nThe problem states that the quasi-identifiers (QIs) are the triple: (age bin, sex, institution). These are attributes that, when combined, could potentially be used to re-identify an individual in a dataset.\n\nAn equivalence class is defined as the set of all records (patients) that have the identical combination of quasi-identifier values. For a dataset to be $k$-anonymous, every record must belong to an equivalence class of size at least $k$. In other words, for any combination of QI values present in the data, there must be at least $k$ records that share that combination. The achieved $k$ for a given dataset is therefore the minimum size observed across all equivalence classes.\n\nThe problem provides the following quasi-identifier attributes and their possible values:\n1.  **Institution ($\\mathcal{I}$):** $\\{\\mathcal{I}_{1}, \\mathcal{I}_{2}, \\mathcal{I}_{3}\\}$ ($3$ values)\n2.  **Sex ($S$):** $\\{M, F\\}$ ($2$ values)\n3.  **Age Bin:** $\\{30\\text{-}34, 35\\text{-}39, 40\\text{-}44, 45\\text{-}49, 50\\text{-}54\\}$ ($5$ values)\n\nThe total number of possible unique equivalence classes is the product of the number of values for each quasi-identifier: $3 \\times 2 \\times 5 = 30$. The problem provides the size (i.e., the number of records) for each of these $30$ equivalence classes. Let $|C|$ denote the size of an equivalence class.\n\nWe can list the sizes of all equivalence classes as given in the problem statement:\n- For age bin $30\\text{-}34$, the sizes are: $3, 2, 2, 2, 2, 3$.\n- For age bin $35\\text{-}39$, the sizes are: $2, 2, 3, 2, 2, 2$.\n- For age bin $40\\text{-}44$, the sizes are: $4, 2, 2, 2, 3, 2$.\n- For age bin $45\\text{-}49$, the sizes are: $2, 2, 2, 2, 2, 2$.\n- For age bin $50\\text{-}54$, the sizes are: $3, 2, 2, 2, 2, 2$.\n\nTo find the achieved $k$ for the entire dataset, we must find the minimum size among all these equivalence classes. Let $\\mathcal{S}$ be the set of sizes of all equivalence classes.\n$$\n\\mathcal{S} = \\{3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 4, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2\\}\n$$\nThe value of $k$ for the dataset is the minimum value in this set.\n$$\nk = \\min(\\mathcal{S})\n$$\nBy inspecting the set $\\mathcal{S}$, we observe that the sizes of the equivalence classes are $2$, $3$, and $4$. The smallest value in the set is $2$.\n\nThe existence of at least one equivalence class of size $2$ means that the dataset is not $3$-anonymous, as it violates the condition that all equivalence classes must have a size of at least $3$. The smallest equivalence class size determines the overall anonymity level of the dataset. Since the minimum size is $2$, any individual in the dataset is indistinguishable from at least one other individual based on the chosen quasi-identifiers.\n\nTherefore, the achieved $k$ under $k$-anonymity is $2$.", "answer": "$$\\boxed{2}$$", "id": "4537603"}, {"introduction": "In radiomics, the DICOM file format is the standard, containing not just pixel data but also a rich header of metadata. This metadata presents a classic dilemma: it contains both sensitive Protected Health Information (PHI) that must be removed and critical technical parameters essential for scientific reproducibility. This practice [@problem_id:4537667] places you in the role of a data steward, challenging you to navigate this trade-off by evaluating different de-identification policies for a realistic CT dataset.", "problem": "You are part of a multi-center thoracic computed tomography (CT) radiomics study preparing a dataset for external sharing. Digital Imaging and Communications in Medicine (DICOM) metadata for an example series includes the following tags and values (drawn from a real-case-like scenario), together with project constraints: avoid Protected Health Information (PHI) disclosure under the Health Insurance Portability and Accountability Act (HIPAA), while preserving downstream feature reproducibility.\n\nTags (with example values):\n- PatientName: \"Smith^John\"\n- PatientID: \"JS-44321\"\n- PatientBirthDate: \"1963-07-14\"\n- PatientAge: \"057Y\"\n- PatientSex: \"M\"\n- StudyDate: \"2021-05-03\"\n- StudyTime: \"14:23:09\"\n- AccessionNumber: \"A998877\"\n- InstitutionName: \"City Hospital Boston\"\n- StationName: \"CT-ROOM-3\"\n- ReferringPhysicianName: \"Doe^Jane\"\n- PerformingPhysicianName: \"Lee^Min\"\n- DeviceSerialNumber: \"SN123456\"\n- Manufacturer: \"Siemens\"\n- ModelName: \"Somatom Definition AS\"\n- ConvolutionKernel: \"B31f\"\n- KVP: \"120\"\n- SliceThickness: \"1.0 mm\"\n- PixelSpacing: \"0.7 mm in-plane\"\n- ImageOrientationPatient: \"orthonormal axes\"\n- ImagePositionPatient: \"(-160,-170,-300) mm\"\n- SeriesDescription: \"Lung cancer baseline - John Smith\"\n- AcquisitionProtocolName: \"LungCA_Baseline_Smith\"\n- StudyInstanceUID: \"1.2.826.0.1.3680043.2.1125.1.1234567890.1\"\n- SeriesInstanceUID: \"1.2.826.0.1.3680043.2.1125.1.1234567890.2\"\n- SOPInstanceUID: \"1.2.826.0.1.3680043.2.1125.1.1234567890.2.1\"\n- FrameOfReferenceUID: \"1.2.826.0.1.3680043.2.1125.1.1234567890.3\"\n- Private tags present from vendor\n- BurnedInAnnotation: \"NO\"\n\nFundamental base you may assume:\n- PHI under HIPAA Safe Harbor includes names, geographic subdivisions smaller than a state, all elements of dates (except year) directly related to an individual, telephone/fax/email, Social Security numbers, medical record numbers, account numbers, certificate/license numbers, vehicle/device identifiers and serial numbers, Universal Resource Locators (URLs), Internet Protocol (IP) addresses, biometric identifiers, full-face photographic images and comparable images, and any other unique identifying number, characteristic, or code. Ages over $89$ must be aggregated to a single category.\n- Radiomics feature reproducibility depends on preserving raw pixel data and geometry-defining metadata such as PixelSpacing, SliceThickness, ImageOrientationPatient, ImagePositionPatient, and acquisition parameters like ConvolutionKernel and KVP.\n- Pseudonymization can replace identifiers with consistent surrogates to maintain referential integrity across images without exposing the original values.\n\nYou must choose the best single policy below that balances privacy protection and radiomics reproducibility. Each policy specifies actions: suppress (remove), generalize (coarsen or transform), retain (keep), or pseudonymize (replace with consistent surrogates).\n\nA. Suppress all direct identifiers (PatientName, PatientID, PatientBirthDate, AccessionNumber, InstitutionName, StationName, ReferringPhysicianName, PerformingPhysicianName, DeviceSerialNumber). Generalize dates by keeping only the year for StudyDate and removing StudyTime. Retain PatientSex and PatientAge as originally encoded. Retain Manufacturer, ModelName, ConvolutionKernel, KVP, SliceThickness, PixelSpacing, ImageOrientationPatient, ImagePositionPatient. Retain SeriesDescription and AcquisitionProtocolName unchanged. Retain StudyInstanceUID, SeriesInstanceUID, SOPInstanceUID, FrameOfReferenceUID unchanged. Suppress all private tags.\n\nB. Suppress all tags except the pixel data and StudyInstanceUID, SeriesInstanceUID, SOPInstanceUID. Remove SliceThickness, PixelSpacing, ImageOrientationPatient, ImagePositionPatient, ConvolutionKernel, and KVP to minimize re-identification risk from technical fingerprints. Generalize dates to year and remove PatientAge entirely. Suppress all private tags. Retain Manufacturer and ModelName.\n\nC. Suppress direct identifiers: PatientName, PatientID, PatientBirthDate, AccessionNumber, InstitutionName, StationName, ReferringPhysicianName, PerformingPhysicianName, DeviceSerialNumber. Generalize dates by applying a per-patient random day offset $d_p \\in \\mathbb{Z}$ (e.g., $-365 \\le d_p \\le 365$) consistently across all date/time fields for that patient so that relative intervals are preserved; retain only the shifted year, month, and day, and remove StudyTime or shift it consistently if required by schema. Generalize age to completed years and top-code to \"$90+$\" if original age $>89$; retain PatientSex. Sanitize textual protocol labels by removing embedded names and site hints, e.g., map SeriesDescription and AcquisitionProtocolName to standardized, site-agnostic codes (e.g., \"LungCA_Baseline\") while preserving sequence-type semantics. Retain and do not alter Manufacturer, ModelName, ConvolutionKernel, KVP, SliceThickness, PixelSpacing, ImageOrientationPatient, ImagePositionPatient. Pseudonymize all UIDs (StudyInstanceUID, SeriesInstanceUID, SOPInstanceUID, FrameOfReferenceUID) with a project root to maintain internal referential integrity without exposing site OIDs. Suppress all private tags unless explicitly whitelisted and verified non-PHI. Confirm BurnedInAnnotation is \"NO\"; if \"YES\", remove overlays or redact pixels.\n\nD. Suppress PatientName, PatientID, PatientBirthDate, and AccessionNumber. Retain InstitutionName and StationName to allow site-effect modeling. Generalize dates by adding independent random noise to each StudyDate and StudyTime per series (different offsets for each series), and leave PatientAge unchanged. Retain SeriesDescription and AcquisitionProtocolName unchanged. Retain all geometry and acquisition parameters. Retain all UIDs unchanged. Retain private tags to preserve vendor-specific reconstruction settings.\n\nWhich policy most appropriately balances privacy protection with downstream radiomics feature reproducibility for external data sharing?", "solution": "The central task is to identify the single best policy for de-identifying a thoracic computed tomography (CT) radiomics dataset for external sharing. The policy must achieve a rigorous balance between two competing objectives:\n1. **Privacy Protection**: The de-identification process must comply with the Health Insurance Portability and Accountability Act (HIPAA) Safe Harbor rules, which mandate the removal or sufficient anonymization of 18 specific types of Protected Health Information (PHI).\n2. **Scientific Utility**: The process must preserve the integrity of the data for its intended purpose, which is radiomics analysis. This requires retaining the raw pixel data and specific Digital Imaging and Communications in Medicine (DICOM) metadata tags that define image geometry and acquisition parameters, ensuring feature reproducibility.\n\nBased on these principles, we will evaluate each proposed policy.\n\n**Evaluation of Option A**\n\nThis policy proposes to:\n- Suppress direct identifiers like `PatientName`, `PatientID`, and `DeviceSerialNumber`, which is a necessary step for HIPAA compliance.\n- Generalize dates to the year only, which is a compliant but potentially suboptimal method as it destroys information about the temporal sequence of studies for a given patient.\n- Retain essential radiomics parameters like `PixelSpacing`, `SliceThickness`, `ConvolutionKernel`, and `KVP`, which is correct for reproducibility.\n\nHowever, this policy contains critical, disqualifying flaws:\n- **Flaw 1**: It mandates to \"Retain `SeriesDescription` and `AcquisitionProtocolName` unchanged.\" The provided examples for these tags are \"Lung cancer baseline - John Smith\" and \"LungCA_Baseline_Smith\". Retaining these tags unchanged directly leaks the patient's name, a clear violation of HIPAA. This action contradicts the policy's initial step of suppressing `PatientName`.\n- **Flaw 2**: It mandates to \"Retain `StudyInstanceUID`, `SeriesInstanceUID`, `SOPInstanceUID`, `FrameOfReferenceUID` unchanged.\" Unique Identifiers (UIDs) are considered PHI under the \"any other unique identifying number, characteristic, or code\" clause of the HIPAA Safe Harbor rules. Furthermore, the root of a UID often contains an Organizationally Unique Identifier (OID) that can be traced back to the originating institution, which is also PHI. These UIDs must be pseudonymized, not retained.\n\nDue to these severe violations of HIPAA privacy rules, this policy is unacceptable.\n\n**Verdict:** Incorrect.\n\n**Evaluation of Option B**\n\nThis policy proposes an extreme approach to de-identification. Its key actions are:\n- Suppressing most tags, but, critically:\n- Removing `SliceThickness`, `PixelSpacing`, `ImageOrientationPatient`, `ImagePositionPatient`, `ConvolutionKernel`, and `KVP`.\n\nThis policy is fundamentally flawed for the following reason:\n- **Flaw 1**: The removal of these specific tags makes the dataset useless for radiomics. `PixelSpacing` and `SliceThickness` are required to understand the physical size of voxels, which is essential for calculating any shape or size-based features. `ImageOrientationPatient` and `ImagePositionPatient` define the image's position and orientation in 3D space, which is necessary for aligning images and correctly interpreting feature directionality. `ConvolutionKernel` and `KVP` (kilovolt peak) are critical acquisition parameters that heavily influence image texture; radiomics features are not comparable if these are unknown. This action directly and catastrophically violates the core requirement of preserving downstream feature reproducibility.\n- **Flaw 2**: The policy is also internally inconsistent. The first point states to \"Suppress all tags except the pixel data and `StudyInstanceUID`, `SeriesInstanceUID`, `SOPInstanceUID`,\" while a later point states to \"Retain `Manufacturer` and `ModelName`.\"\n\nThis policy completely fails to meet the scientific utility objective.\n\n**Verdict:** Incorrect.\n\n**Evaluation of Option C**\n\nThis policy presents a comprehensive and nuanced de-identification strategy. Let's analyze its components:\n- **Identifier Suppression**: It correctly proposes to suppress all direct identifiers (`PatientName`, `PatientID`, `PatientBirthDate`, `AccessionNumber`, `InstitutionName`, `DeviceSerialNumber`, etc.). This is the foundational step for HIPAA compliance.\n- **Date Generalization**: It proposes applying a consistent, per-patient random day offset, e.g., $d_p \\in \\mathbb{Z}$ where $-365 \\le d_p \\le 365$. This is a best-practice technique. It makes the absolute dates non-identifiable, satisfying HIPAA, while perfectly preserving the time intervals between a patient's different scans. This is crucial for longitudinal analysis (e.g., tracking tumor response over time).\n- **Age Generalization**: It correctly specifies to generalize age to completed years and top-code at \"$90+$\" for ages over $89$, which is the exact HIPAA Safe Harbor requirement.\n- **Text Field Sanitization**: It recognizes that free-text fields like `SeriesDescription` and `AcquisitionProtocolName` can contain PHI. The strategy of sanitizing them by mapping to standardized, site-agnostic codes (e.g., \"LungCA_Baseline_Smith\" becomes \"LungCA_Baseline\") is excellent. It removes PHI while preserving the scientifically valuable semantic content.\n- **Retention of Radiomics Parameters**: It correctly mandates the retention of all metadata essential for radiomics reproducibility: `Manufacturer`, `ModelName`, `ConvolutionKernel`, `KVP`, `SliceThickness`, `PixelSpacing`, `ImageOrientationPatient`, and `ImagePositionPatient`.\n- **UID Pseudonymization**: It proposes to pseudonymize all UIDs with a new project-specific root. This is the correct procedure. It severs the link to the original institution's records (protecting privacy) while maintaining the internal referential integrity of the dataset, ensuring that all images for a given study are still linked together correctly.\n- **Handling of Ancillary Data**: It includes sound policies for suppressing private tags (unless verified as non-PHI) and checking for and redacting `BurnedInAnnotation`. These steps address more subtle but common pathways for PHI leakage.\n\nThis policy successfully navigates the trade-off. It is aggressive in protecting privacy by addressing all major and subtle PHI types, and it is meticulous in preserving all data necessary for both radiomics reproducibility and longitudinal analysis.\n\n**Verdict:** Correct.\n\n**Evaluation of Option D**\n\nThis policy contains several significant errors and risky procedures:\n- **Flaw 1**: It proposes to \"Retain `InstitutionName` and `StationName` to allow site-effect modeling.\" `InstitutionName` is considered PHI as a geographic subdivision smaller than a state. While modeling site effects is a valid goal, it must be achieved by using a pseudonymized site identifier, not the real name. This is a direct HIPAA violation.\n- **Flaw 2**: Like Option A, it proposes to \"Retain `SeriesDescription` and `AcquisitionProtocolName` unchanged,\" which, as demonstrated, leaks PHI.\n- **Flaw 3**: Like Option A, it proposes to \"Retain all UIDs unchanged,\" which is a HIPAA violation risk.\n- **Flaw 4**: It proposes to \"Retain private tags to preserve vendor-specific reconstruction settings.\" This is extremely risky. Private tags are not standardized and frequently contain PHI. Retaining them without a rigorous verification and whitelisting process, as suggested in Option C, is a major compliance risk.\n- **Flaw 5**: The method for date generalization—\"adding independent random noise to each `StudyDate` and `StudyTime` per series\"—is inferior to the consistent patient-level shift in Option C. Independent noise destroys the true temporal ordering and intervals between a patient's scans, compromising a key aspect of longitudinal data.\n\nThis policy fails on multiple counts of privacy protection and employs a suboptimal method for data utility.\n\n**Verdict:** Incorrect.\n\n**Conclusion**\n\nPolicy C is the only option that is both fully compliant with HIPAA Safe Harbor principles and correctly preserves the data integrity and metadata required for high-quality, reproducible radiomics research. It demonstrates a sophisticated understanding of the nuances of medical data de-identification, addressing not only obvious identifiers but also subtle sources of PHI in UIDs, free-text fields, and burned-in annotations, all while using best-practice techniques like date-shifting to maximize data utility.", "answer": "$$\\boxed{C}$$", "id": "4537667"}, {"introduction": "While $k$-anonymity provides a useful baseline, more advanced techniques offer provable mathematical guarantees of privacy. $\\epsilon$-Differential Privacy (DP) has emerged as a gold standard, ensuring that the outcome of any analysis is not significantly influenced by any single individual's data. This exercise [@problem_id:4537692] demystifies DP by guiding you through the core calculation of the Laplace mechanism: determining the precise amount of calibrated noise needed to protect individual privacy while releasing a useful aggregate statistic.", "problem": "A research team plans to release the sample mean of a de-identified radiomic texture feature computed from a cohort as part of an ethically compliant, privacy-preserving analysis. To mitigate re-identification risk, the release must satisfy $\\epsilon$-Differential Privacy (Differential Privacy (DP)). Each patient’s feature value is hard-clipped into a known, ethically justified range to bound the influence of any individual record.\n\nAssume the following:\n- The dataset consists of $n=275$ independent patients.\n- After clipping, each patient’s feature value lies in the interval $[0,8]$.\n- The target query is the sample mean of this bounded feature under the change-one-record adjacency notion.\n- The privacy budget is $\\epsilon=0.64$.\n- The noise to be added is drawn from a centered Laplace distribution to implement the Laplace mechanism.\n\nStarting only from the definition of $\\epsilon$-DP and the definition of global sensitivity of a function under change-one adjacency, derive the global sensitivity of the sample mean for this bounded feature, and then determine the scale parameter $b$ of the Laplace noise that ensures $\\epsilon$-DP for this release.\n\nReport the numerical value of $b$ rounded to four significant figures. Express the final answer as a dimensionless number.", "solution": "The problem asks for the scale parameter $b$ of a Laplace distribution used to ensure $\\epsilon$-Differential Privacy for the release of a sample mean. The derivation must start from the definitions of global sensitivity and differential privacy.\n\nLet the dataset be denoted by $D = \\{x_1, x_2, \\ldots, x_n\\}$, where $n$ is the number of patients. The problem states that $n=275$. Each patient's feature value $x_i$ is a scalar quantity that, after clipping, is guaranteed to lie within a known range. Let this range be $[L, U]$. The problem specifies this range as $[0, 8]$, so we have $L=0$ and $U=8$.\n\nThe query function, $f$, is the sample mean of the feature values in the dataset:\n$$f(D) = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n\nThe analysis is conducted under the \"change-one-record\" adjacency notion. This means two datasets, $D$ and $D'$, are considered adjacent if they differ in the value of at most one record. Let $D = \\{x_1, \\ldots, x_k, \\ldots, x_n\\}$ and $D' = \\{x_1, \\ldots, x'_k, \\ldots, x_n\\}$, where both $x_k$ and $x'_k$ are in the range $[L, U]$.\n\nThe first step is to derive the global sensitivity of the query function $f$. The global sensitivity, denoted $\\Delta f$, is the maximum possible change in the output of $f$ over all possible pairs of adjacent datasets. For a scalar-valued function, it is defined as:\n$$\\Delta f = \\max_{D, D' \\text{ adjacent}} |f(D) - f(D')|$$\n\nLet's compute the difference $|f(D) - f(D')|$ for our adjacent datasets $D$ and $D'$:\n$$f(D) - f(D') = \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i\\right) - \\left(\\frac{1}{n} \\left(\\sum_{i \\neq k} x_i + x'_k\\right)\\right)$$\n$$f(D) - f(D') = \\frac{1}{n} \\left( \\left(\\sum_{i \\neq k} x_i + x_k\\right) - \\left(\\sum_{i \\neq k} x_i + x'_k\\right) \\right)$$\n$$f(D) - f(D') = \\frac{1}{n} (x_k - x'_k)$$\nTaking the absolute value, we get:\n$$|f(D) - f(D')| = \\left|\\frac{1}{n} (x_k - x'_k)\\right| = \\frac{1}{n} |x_k - x'_k|$$\nTo find the global sensitivity $\\Delta f$, we must find the maximum possible value of this expression. The term $\\frac{1}{n}$ is a positive constant. The maximum is achieved when $|x_k - x'_k|$ is maximized. Since both $x_k$ and $x'_k$ are constrained to the interval $[L, U]$, the maximum difference between them occurs when one value is at the upper bound $U$ and the other is at the lower bound $L$.\n$$\\max |x_k - x'_k| = U - L$$\nSubstituting this into the expression for sensitivity, we find the global sensitivity of the sample mean:\n$$\\Delta f = \\frac{U - L}{n}$$\n\nThe problem requires achieving $\\epsilon$-Differential Privacy using the Laplace mechanism. A randomized mechanism $\\mathcal{M}$ provides $\\epsilon$-DP if for any adjacent datasets $D, D'$ and any set of possible outputs $S$, the following inequality holds:\n$$ \\Pr[\\mathcal{M}(D) \\in S] \\le \\exp(\\epsilon) \\cdot \\Pr[\\mathcal{M}(D') \\in S] $$\nThe Laplace mechanism achieves this by adding noise to the true output of the query function, $f(D)$. The noise is drawn from a Laplace distribution, $\\text{Lap}(b)$, with probability density function $p(y) = \\frac{1}{2b} \\exp\\left(-\\frac{|y|}{b}\\right)$, where $b$ is the scale parameter.\nFor a mechanism $\\mathcal{M}(D) = f(D) + Y$ with $Y \\sim \\text{Lap}(b)$ to satisfy $\\epsilon$-DP, the scale parameter $b$ must be set in relation to the global sensitivity $\\Delta f$ and the privacy budget $\\epsilon$. The required relationship is:\n$$b = \\frac{\\Delta f}{\\epsilon}$$\n\nCombining our derived global sensitivity with this relationship, we get the formula for the scale parameter for the sample mean query:\n$$b = \\frac{(U - L)/n}{\\epsilon} = \\frac{U - L}{n\\epsilon}$$\n\nNow, we substitute the numerical values provided in the problem statement:\n- Number of patients $n = 275$\n- Feature value range is $[0, 8]$, so $L=0$ and $U=8$.\n- Privacy budget $\\epsilon = 0.64$.\n\n$$b = \\frac{8 - 0}{275 \\times 0.64}$$\nFirst, we compute the product in the denominator:\n$$275 \\times 0.64 = 275 \\times \\frac{64}{100} = \\frac{275}{25} \\times \\frac{64}{4} = 11 \\times 16 = 176$$\nNow, substitute this back into the expression for $b$:\n$$b = \\frac{8}{176}$$\nSimplifying the fraction:\n$$b = \\frac{8 \\div 8}{176 \\div 8} = \\frac{1}{22}$$\nTo provide the numerical value, we perform the division:\n$$b = \\frac{1}{22} \\approx 0.04545454\\ldots$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $1/22 \\approx 0.04545454\\ldots$. The first four significant figures are 4, 5, 4, and 5. The fifth significant figure is 4. Since $4  5$, we do not round up. Therefore, the value of $b$ rounded to four significant figures is 0.04545.", "answer": "$$\\boxed{0.04545}$$", "id": "4537692"}]}