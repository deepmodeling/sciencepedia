{"hands_on_practices": [{"introduction": "Radiomics often involves combining data from multiple hospitals, but differences in equipment can introduce technical variations that masquerade as biological signals. This practice [@problem_id:4544647] demonstrates how scanner-specific intensity scaling can create spurious feature differences. By calculating a simple radiomic feature before and after applying a standard normalization technique, you will see how a hypothesis-aware preprocessing step is essential for preventing a data-driven model from learning meaningless artifacts.", "problem": "A radiomics study aggregates Magnetic Resonance Imaging (MRI) images from two centers to build predictive models of tumor phenotype. In a hypothesis-driven design, you posit that a fixed Region of Interest (ROI) within the same histopathological class should have comparable radiomic features across centers after appropriate standardization. In a purely data-driven exploration, the pipeline might naively treat between-center feature differences as discriminative signals. One known source of spurious between-center differences is intensity non-standardization: scanners and reconstruction pipelines can induce different linear scalings of the underlying tissue signal.\n\nConsider an ROI in which the voxel intensities are measured in arbitrary units (a.u.) for Center A as $[70, 74, 76, 72, 68, 80]$ and for Center B as $[30, 40, 45, 35, 25, 55]$. Assume these intensities arise from the same underlying tissue microstructure, but with center-specific linear scaling, and that there is no biological difference between centers for this ROI.\n\nUsing first principles and standard definitions, do the following:\n1. Compute the first-order radiomic “energy” feature defined by $$E = \\sum_{i=1}^{n} x_i^2$$ for each center before any standardization, and interpret the result in the hypothesis-driven versus data-driven context.\n2. Specify a per-center $z$-scoring transform that restores comparability, using the population mean and the population standard deviation, defined respectively by $$\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i, \\quad \\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2}, \\quad z_i = \\frac{x_i - \\mu}{\\sigma}.$$\n3. Compute the energy of the standardized intensities for each center, $$E^{(z)} = \\sum_{i=1}^{n} z_i^2,$$ and then compute the ratio $$R = \\frac{E^{(z)}_{\\text{A}}}{E^{(z)}_{\\text{B}}}.$$\n\nExpress the final answer for $R$ as an exact value. No units are required for $R$ because it is dimensionless. If any intermediate numerical values are reported, they need not be rounded; if rounding is necessary for any intermediate step, round to four significant figures.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a realistic scenario in the field of radiomics concerning the need for data standardization to correct for inter-center variability in medical imaging data. All necessary definitions and data are provided.\n\nThe problem asks for a three-part analysis of radiomic feature comparability between two centers, A and B. We will address each part systematically.\n\nThe provided intensity data for the Regions of Interest (ROI) are:\n- Center A: $X_A = [70, 74, 76, 72, 68, 80]$\n- Center B: $X_B = [30, 40, 45, 35, 25, 55]$\n\nFor both centers, the number of voxels (samples) is $n=6$.\n\n**1. Computation and Interpretation of Pre-Standardization Energy**\n\nThe first-order radiomic feature \"energy\" is defined as $E = \\sum_{i=1}^{n} x_i^2$. We compute this for each center.\n\nFor Center A:\n$$E_A = \\sum_{i=1}^{6} x_{A,i}^2 = 70^2 + 74^2 + 76^2 + 72^2 + 68^2 + 80^2$$\n$$E_A = 4900 + 5476 + 5776 + 5184 + 4624 + 6400 = 32360$$\n\nFor Center B:\n$$E_B = \\sum_{i=1}^{6} x_{B,i}^2 = 30^2 + 40^2 + 45^2 + 35^2 + 25^2 + 55^2$$\n$$E_B = 900 + 1600 + 2025 + 1225 + 625 + 3025 = 9400$$\n\n*Interpretation*:\nThe energy feature, which reflects the overall magnitude of voxel intensities, is substantially different between the two centers ($E_A = 32360$ vs. $E_B = 9400$).\n- In a purely **data-driven** approach, a machine learning model would likely identify this large difference in energy as a highly discriminative feature. If the task were, for instance, to predict tumor phenotype, the model might erroneously learn to associate scanner-specific intensity scaling with the biological outcome. This is a classic example of a confounding variable, where the model learns an artifact of the data collection process rather than a true biological signal.\n- In a **hypothesis-driven** framework, the researcher anticipates such non-biological variations based on prior knowledge of imaging physics and multi-center data collection. The explicit hypothesis is that the underlying tissue is identical and the observed intensity difference is due to a linear scaling artifact. The large discrepancy in the raw energy feature $E$ would prompt the researcher to perform data harmonization, such as the standardization requested in the next step, to remove this confounder before testing any biological hypotheses.\n\n**2. Specification of the $z$-scoring Transform**\n\nThe $z$-scoring transform, $z_i = \\frac{x_i - \\mu}{\\sigma}$, requires the computation of the population mean ($\\mu$) and population standard deviation ($\\sigma$) for each center.\n\nFor Center A:\nThe mean is:\n$$\\mu_A = \\frac{1}{6}(70 + 74 + 76 + 72 + 68 + 80) = \\frac{440}{6} = \\frac{220}{3}$$\nThe variance is:\n$$\\sigma_A^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_{A,i} - \\mu_A)^2 = \\frac{1}{6}\\left[\\left(70-\\frac{220}{3}\\right)^2 + \\left(74-\\frac{220}{3}\\right)^2 + \\dots + \\left(80-\\frac{220}{3}\\right)^2\\right]$$\n$$\\sigma_A^2 = \\frac{1}{6}\\left[\\left(\\frac{-10}{3}\\right)^2 + \\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{8}{3}\\right)^2 + \\left(\\frac{-4}{3}\\right)^2 + \\left(\\frac{-16}{3}\\right)^2 + \\left(\\frac{20}{3}\\right)^2\\right]$$\n$$\\sigma_A^2 = \\frac{1}{6} \\cdot \\frac{100+4+64+16+256+400}{9} = \\frac{1}{6} \\cdot \\frac{840}{9} = \\frac{140}{9}$$\nThe standard deviation is:\n$$\\sigma_A = \\sqrt{\\frac{140}{9}} = \\frac{\\sqrt{140}}{3} = \\frac{2\\sqrt{35}}{3}$$\nThe transform for Center A is $z_{A,i} = \\frac{x_{A,i} - 220/3}{2\\sqrt{35}/3}$.\n\nFor Center B:\nThe mean is:\n$$\\mu_B = \\frac{1}{6}(30 + 40 + 45 + 35 + 25 + 55) = \\frac{230}{6} = \\frac{115}{3}$$\nThe variance is:\n$$\\sigma_B^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_{B,i} - \\mu_B)^2 = \\frac{1}{6}\\left[\\left(30-\\frac{115}{3}\\right)^2 + \\dots + \\left(55-\\frac{115}{3}\\right)^2\\right]$$\n$$\\sigma_B^2 = \\frac{1}{6}\\left[\\left(\\frac{-25}{3}\\right)^2 + \\left(\\frac{5}{3}\\right)^2 + \\left(\\frac{20}{3}\\right)^2 + \\left(\\frac{-10}{3}\\right)^2 + \\left(\\frac{-40}{3}\\right)^2 + \\left(\\frac{50}{3}\\right)^2\\right]$$\n$$\\sigma_B^2 = \\frac{1}{6} \\cdot \\frac{625+25+400+100+1600+2500}{9} = \\frac{1}{6} \\cdot \\frac{5250}{9} = \\frac{875}{9}$$\nThe standard deviation is:\n$$\\sigma_B = \\sqrt{\\frac{875}{9}} = \\frac{\\sqrt{25 \\cdot 35}}{3} = \\frac{5\\sqrt{35}}{3}$$\nThe transform for Center B is $z_{B,i} = \\frac{x_{B,i} - 115/3}{5\\sqrt{35}/3}$.\n\n**3. Computation of Standardized Energies and their Ratio**\n\nThe energy of the standardized intensities is $E^{(z)} = \\sum_{i=1}^{n} z_i^2$. We can compute this value through a general derivation based on the provided definitions.\n$$E^{(z)} = \\sum_{i=1}^{n} z_i^2 = \\sum_{i=1}^{n} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\nThe definition of the population variance is $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2$. Rearranging this gives $\\sum_{i=1}^{n} (x_i - \\mu)^2 = n\\sigma^2$.\nSubstituting this result into the expression for $E^{(z)}$:\n$$E^{(z)} = \\frac{1}{\\sigma^2}(n\\sigma^2) = n$$\nThis is a fundamental property: for any dataset, the sum of squares of its z-scores (calculated using the population mean and standard deviation) is exactly equal to the number of data points, $n$.\n\nFor both Center A and Center B, the number of samples is $n=6$. Therefore, the standardized energy for each center is:\n$$E^{(z)}_{\\text{A}} = n = 6$$\n$$E^{(z)}_{\\text{B}} = n = 6$$\n\nThis result demonstrates that the $z$-scoring transform has successfully removed the linear scaling differences between the centers, resulting in identical energy values. This harmonized feature now correctly reflects the problem's initial condition that there is no biological difference in the ROI between centers.\n\nFinally, we compute the ratio $R$:\n$$R = \\frac{E^{(z)}_{\\text{A}}}{E^{(z)}_{\\text{B}}} = \\frac{6}{6} = 1$$\n\nThe ratio is exactly $1$, confirming that $z$-scoring has made the energy feature comparable across centers.", "answer": "$$\\boxed{1}$$", "id": "4544647"}, {"introduction": "The power of data-driven research lies in its ability to screen numerous features for associations, but this comes with a statistical cost: the more tests you run, the more likely you are to find a significant result by pure chance. This exercise [@problem_id:4544682] quantifies this risk by calculating the family-wise error rate ($FWER$), the probability of making at least one false discovery. You will then derive the classic Bonferroni correction, a cornerstone of hypothesis-driven research for maintaining statistical rigor when multiple comparisons are made.", "problem": "A hypothesis-driven radiomics study plans to test $m$ predefined feature-outcome associations derived from prior mechanistic rationale in medical imaging. Each hypothesis test is conducted at per-test significance level $\\alpha = 0.05$ without multiple-testing correction. Assume the global null hypothesis is true (all $m$ null hypotheses are true) and that individual tests are independent.\n\nUsing the definition of the Family-Wise Error Rate (FWER) as $P(\\text{at least one false rejection among the } m \\text{ tests})$, derive the expected false positive rate as a function of $m$ under these conditions. Then, determine the per-test significance level required by the Bonferroni correction to ensure that the FWER is controlled at $0.05$ regardless of dependence among tests.\n\nProvide your final answer as two analytic expressions in terms of $m$: first, the expected false positive rate without correction; second, the Bonferroni-corrected per-test significance level. No numerical evaluation is required, and no units are involved.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a solution. The concepts of Family-Wise Error Rate (FWER), multiple hypothesis testing, and the Bonferroni correction are standard in statistics and its applications, such as radiomics. The assumptions of the global null hypothesis being true and the independence of tests for the first part of the problem are explicit and allow for a direct derivation.\n\nThe problem asks for two distinct quantities:\n1.  The expected false positive rate as a function of the number of tests, $m$, under the assumption of independence and no correction for multiple comparisons.\n2.  The per-test significance level required by the Bonferroni correction to control the FWER at $0.05$.\n\nLet us address each part sequentially.\n\n**Part 1: Expected False Positive Rate (FWER) without Correction**\n\nThe Family-Wise Error Rate (FWER) is defined in the problem as the probability of making at least one false rejection among the $m$ tests. This is also referred to as a Type I error on the \"family\" of hypotheses. Let $V$ be the random variable representing the number of false positives (Type I errors). The FWER is then $P(V \\ge 1)$.\n\nThe problem states that the global null hypothesis is true, which means all $m$ individual null hypotheses, $H_{0,1}, H_{0,2}, \\dots, H_{0,m}$, are true. Consequently, any rejection of a null hypothesis is, by definition, a false rejection (a Type I error).\n\nThe per-test significance level, $\\alpha$, is the probability of rejecting a single null hypothesis, given that it is true. We are given $\\alpha = 0.05$.\nFor any single test $i$, the probability of a Type I error is:\n$$ P(\\text{reject } H_{0,i}) = \\alpha = 0.05 $$\nThe probability of correctly not rejecting a true null hypothesis for a single test is therefore:\n$$ P(\\text{do not reject } H_{0,i}) = 1 - \\alpha = 1 - 0.05 = 0.95 $$\nCalculating the FWER directly, $P(V \\ge 1)$, involves a complex sum of probabilities. It is more straightforward to calculate the probability of its complement event, which is making zero false rejections, $P(V=0)$, and then use the relation $P(V \\ge 1) = 1 - P(V=0)$.\n\nThe event $\\{V=0\\}$ corresponds to not rejecting any of the $m$ true null hypotheses. The problem states that the individual tests are independent. Therefore, the probability of not rejecting any of the $m$ hypotheses is the product of the individual probabilities of not rejecting each one:\n$$ P(V=0) = P(\\text{no rejections in } m \\text{ tests}) = \\prod_{i=1}^{m} P(\\text{do not reject } H_{0,i}) $$\nSince each test is conducted at the same significance level $\\alpha$, this simplifies to:\n$$ P(V=0) = (1 - \\alpha)^m $$\nNow, we can find the FWER:\n$$ \\text{FWER} = P(V \\ge 1) = 1 - P(V=0) = 1 - (1 - \\alpha)^m $$\nSubstituting the given value $\\alpha = 0.05$, the expression for the expected false positive rate as a function of $m$ is:\n$$ \\text{FWER}(m) = 1 - (1 - 0.05)^m = 1 - (0.95)^m $$\n\n**Part 2: Bonferroni-Corrected Per-Test Significance Level**\n\nThe second part of the problem asks for the per-test significance level, let's call it $\\alpha'$, required by the Bonferroni correction to control the FWER at or below a specified level, which we denote as $\\alpha_{\\text{FWER}} = 0.05$.\n\nThe Bonferroni correction is a method to control the FWER that does not assume independence among the tests. It is based on Boole's inequality, also known as the union bound. Let $E_i$ be the event of a Type I error for the $i$-th test. The FWER is the probability of the union of these events:\n$$ \\text{FWER} = P\\left(\\bigcup_{i=1}^{m} E_i\\right) $$\nBoole's inequality states that for any set of events $E_1, \\dots, E_m$:\n$$ P\\left(\\bigcup_{i=1}^{m} E_i\\right) \\le \\sum_{i=1}^{m} P(E_i) $$\nApplying this to our context, the FWER is bounded above by the sum of the individual per-test error probabilities. If we use a new, adjusted significance level $\\alpha'$ for each test, then $P(E_i) = \\alpha'$. The inequality becomes:\n$$ \\text{FWER} \\le \\sum_{i=1}^{m} \\alpha' = m \\alpha' $$\nTo guarantee that the FWER is controlled at a level of $0.05$, we must ensure that its upper bound does not exceed this value. That is, we require $m \\alpha' \\le 0.05$. The Bonferroni correction sets the adjusted per-test significance level $\\alpha'$ by solving the equality:\n$$ m \\alpha' = 0.05 $$\nSolving for $\\alpha'$ gives the required per-test significance level:\n$$ \\alpha' = \\frac{0.05}{m} $$\nThis corrected significance level, $\\alpha'$, guarantees that $\\text{FWER} \\le 0.05$ regardless of the dependence structure among the tests, as stipulated in the problem.\n\nThe two final analytic expressions requested are $1 - (0.95)^m$ and $\\frac{0.05}{m}$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 - (0.95)^m & \\frac{0.05}{m} \\end{pmatrix}}$$", "id": "4544682"}, {"introduction": "Building a predictive model is only half the battle; we must also evaluate its performance using metrics that reflect real-world clinical value, especially when predicting rare diseases or outcomes. This practice [@problem_id:4544659] reveals why a commonly used metric, the area under the ROC curve (ROC AUC), can be misleadingly optimistic in such imbalanced scenarios. You will instead learn to compute the area under the Precision-Recall curve, a more robust metric for assessing a classifier's performance on the minority class of interest.", "problem": "A radiomics classifier is trained to predict a rare binary clinical endpoint (event present versus absent) from high-dimensional image-derived features. In hypothesis-driven research, the choice of performance metric should follow from the a priori clinical hypothesis about decision trade-offs and event prevalence, whereas in data-driven research, a single metric optimized post hoc may be misleading if it does not reflect the clinical base rate.\n\nUsing only core definitions of confusion-matrix entries and derived rates, explain why the Area Under the Receiver Operating Characteristic curve (ROC AUC) can be misleading under severe class imbalance typical in radiomics, and justify preferring the Precision–Recall curve (PR curve) and its area (PR AUC) in that setting.\n\nThen, for the following hypothetical dataset of predicted scores and binary ground-truth labels, compute the PR AUC. The dataset has $N=20$ patients with $P=2$ event-positive cases ($1$ denotes event present, $0$ denotes event absent). Each pair is given as $(\\text{score}, \\text{label})$, and higher score means higher confidence of the event. The list is pre-sorted from highest to lowest score:\n$\\{(0.97,1),(0.89,0),(0.83,0),(0.78,0),(0.71,0),(0.68,0),(0.62,0),(0.55,0),(0.49,0),(0.44,0),(0.40,1),(0.38,0),(0.33,0),(0.29,0),(0.27,0),(0.22,0),(0.18,0),(0.14,0),(0.09,0),(0.03,0)\\}$.\n\nInstructions for the PR AUC computation:\n- Use the standard definitions $ \\text{precision} = \\frac{TP}{TP+FP} $ and $ \\text{recall} = \\frac{TP}{P} $, where $TP$ and $FP$ are true positives and false positives at a given threshold that classifies all items with score greater than or equal to that threshold as positive.\n- Sweep thresholds by descending score, updating $(TP,FP)$ cumulatively.\n- Construct the PR curve as precision versus recall in this threshold order, including the conventional anchor point at recall $0$ with precision $1$ and the final state where all $N$ instances are predicted positive.\n- Compute the area under this PR curve as the integral of precision with respect to recall using the trapezoidal rule over successive points where recall changes. Vertical segments where recall does not change contribute zero area.\n\nExpress the final PR AUC as a decimal and round your answer to four significant figures. No unit is required.", "solution": "The problem asks for two parts: first, a theoretical explanation of why the Area Under the Receiver Operating Characteristic curve (ROC AUC) can be misleading in scenarios with severe class imbalance, and why the Precision-Recall (PR) curve and its area (PR AUC) are often preferred. Second, it requires the calculation of the PR AUC for a specific dataset.\n\n**Part 1: Conceptual Justification of PR AUC over ROC AUC in Imbalanced Datasets**\n\nTo address this, we must first define the fundamental quantities derived from a confusion matrix for a binary classification problem. Let $P$ be the number of true positive instances (events) and $N_{neg}$ be the number of true negative instances (non-events) in the dataset. At a given classification threshold, let $TP$ be the number of true positives, $FP$ be the number of false positives, $TN$ be the number of true negatives, and $FN$ be the number of false negatives. We have $P = TP + FN$ and $N_{neg} = FP + TN$.\n\nThe ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\nThe TPR, also known as recall or sensitivity, is defined as:\n$$\n\\text{TPR} = \\text{Recall} = \\frac{TP}{P} = \\frac{TP}{TP+FN}\n$$\nThe FPR is defined as:\n$$\n\\text{FPR} = \\frac{FP}{N_{neg}} = \\frac{FP}{FP+TN}\n$$\nThe ROC AUC is the area under this curve, representing the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance.\n\nIn a scenario with severe class imbalance, as is common in radiomics for predicting rare events, the number of negative instances is vastly greater than the number of positive instances ($N_{neg} \\gg P$). The issue with ROC AUC arises from the definition of FPR. Because the denominator of FPR is the total number of negatives, $N_{neg}$, a large absolute number of false positives ($FP$) can still result in a very small, seemingly favorable, FPR. For example, if $P=10$ and $N_{neg}=10000$, a classifier that produces $100$ false positives ($FP=100$) while correctly identifying all positives ($TP=10$, $FN=0$) would have a perfect TPR of $1.0$, but its FPR would be only $\\frac{100}{10000} = 0.01$. This point $(0.01, 1.0)$ is very close to the ideal top-left corner of the ROC space, suggesting excellent performance. However, out of $110$ predicted positive cases ($TP+FP = 10+100$), only $10$ are correct. The clinical utility of such a classifier is poor, as its positive predictions are unreliable. The ROC curve and its AUC are insensitive to this effect because the large value of $N_{neg}$ in the FPR's denominator masks the performance consequences of a high number of false positives.\n\nThe Precision-Recall (PR) curve addresses this deficiency. It plots precision against recall.\nPrecision, also known as Positive Predictive Value (PPV), is defined as:\n$$\n\\text{Precision} = \\frac{TP}{TP+FP}\n$$\nRecall is the same as TPR. The PR curve hence visualizes the trade-off between a model's ability to identify all positive instances (recall) and the accuracy of its positive predictions (precision).\n\nThe denominator of precision is the total number of instances classified as positive ($TP+FP$). Unlike FPR, precision does not involve $TN$. In the imbalanced scenario ($N_{neg} \\gg P$), any increase in $FP$ has a substantial negative impact on precision, because the number of $TP$ is inherently small. Using the previous example, the precision would be $\\frac{10}{10+100} = \\frac{10}{110} \\approx 0.09$. A point on the PR curve with recall $1.0$ and precision $0.09$ clearly indicates poor performance, which was obscured on the ROC curve. The PR curve thus provides a more informative picture of a classifier's performance on the minority positive class, which is typically the class of interest in rare event prediction. Therefore, for hypothesis-driven research where the clinical utility of a positive finding is paramount, or for data-driven discovery in imbalanced domains, the PR curve and its area (PR AUC) are more appropriate and less misleading metrics than ROC AUC.\n\n**Part 2: Calculation of PR AUC**\n\nWe are given a dataset of $N=20$ patients with $P=2$ positive cases and $N_{neg}=18$ negative cases. The data, pre-sorted by score, is $\\{(0.97,1),(0.89,0), \\ldots, (0.03,0)\\}$. We will calculate the PR AUC using the trapezoidal rule as specified. The PR curve is constructed by generating points $(\\text{Recall}_i, \\text{Precision}_i)$ as we lower the classification threshold, including one patient at a time from $i=1$ to $i=20$.\n\nLet $TP_i$ and $FP_i$ be the cumulative counts of true and false positives after considering the top $i$ instances.\n$\\text{Recall}_i = \\frac{TP_i}{P} = \\frac{TP_i}{2}$.\n$\\text{Precision}_i = \\frac{TP_i}{TP_i+FP_i} = \\frac{TP_i}{i}$.\n\nThe PR AUC is calculated by integrating precision with respect to recall. Using the trapezoidal rule, the area is given by the sum of smaller trapezoidal areas between successive points:\n$$\n\\text{AUC} = \\sum_{i=1}^{N} \\frac{1}{2} (\\text{Precision}_i + \\text{Precision}_{i-1}) (\\text{Recall}_i - \\text{Recall}_{i-1})\n$$\nThe conventional anchor point is $p_0 = (\\text{Recall}_0, \\text{Precision}_0) = (0, 1)$. The terms in the sum are non-zero only when $\\text{Recall}_i \\neq \\text{Recall}_{i-1}$, which occurs when a positive instance is encountered.\n\n1.  **Initial state (anchor point):** $i=0$. $p_0 = (\\text{Recall}_0=0, \\text{Precision}_0=1)$.\n\n2.  **First positive instance at $i=1$:** The pair is $(0.97, 1)$.\n    $TP_1 = 1$, $FP_1 = 0$.\n    $\\text{Recall}_1 = \\frac{1}{2} = 0.5$.\n    $\\text{Precision}_1 = \\frac{1}{1} = 1$.\n    The change in recall is $\\text{Recall}_1 - \\text{Recall}_0 = 0.5 - 0 = 0.5$.\n    The area of the first trapezoid is $A_1 = \\frac{1}{2}(\\text{Precision}_1 + \\text{Precision}_0)(\\text{Recall}_1 - \\text{Recall}_0) = \\frac{1}{2}(1 + 1)(0.5) = 0.5$.\n\n3.  **Instances from $i=2$ to $i=10$ are all negative.**\n    For any $i$ in this range, $TP_i$ remains $1$. Thus, $\\text{Recall}_i = \\frac{1}{2}$, which is the same as $\\text{Recall}_1$. So, for $i \\in \\{2, 3, \\ldots, 10\\}$, $(\\text{Recall}_i - \\text{Recall}_{i-1}) = 0$. These steps contribute zero area, as per the \"vertical segments\" rule. The last point in this sequence is at $i=10$.\n    At $i=10$, we have processed 1 positive and 9 negatives. So $TP_{10}=1$, $FP_{10}=9$.\n    $\\text{Recall}_{10} = \\frac{1}{2} = 0.5$.\n    $\\text{Precision}_{10} = \\frac{1}{1+9} = \\frac{1}{10} = 0.1$.\n    This point is $p_{10} = (0.5, 0.1)$. This is the point immediately preceding the next change in recall.\n\n4.  **Second positive instance at $i=11$:** The pair is $(0.40, 1)$.\n    At this step, we have processed 11 instances. $TP_{11} = 2$, $FP_{11} = 9$.\n    $\\text{Recall}_{11} = \\frac{2}{2} = 1.0$.\n    $\\text{Precision}_{11} = \\frac{2}{2+9} = \\frac{2}{11}$.\n    The change in recall is $\\text{Recall}_{11} - \\text{Recall}_{10} = 1.0 - 0.5 = 0.5$.\n    The area of this second trapezoid is calculated between the point before this change ($p_{10}$) and the current point ($p_{11}$).\n    $A_2 = \\frac{1}{2}(\\text{Precision}_{11} + \\text{Precision}_{10})(\\text{Recall}_{11} - \\text{Recall}_{10})$.\n    $A_2 = \\frac{1}{2}\\left(\\frac{2}{11} + \\frac{1}{10}\\right)(0.5) = \\frac{1}{4}\\left(\\frac{2 \\times 10 + 1 \\times 11}{110}\\right) = \\frac{1}{4}\\left(\\frac{20+11}{110}\\right) = \\frac{1}{4}\\left(\\frac{31}{110}\\right) = \\frac{31}{440}$.\n\n5.  **Instances from $i=12$ to $i=20$ are all negative.**\n    For any $i$ in this range, $TP_i$ remains $2$. Thus, $\\text{Recall}_i = 1.0$, which is the same as $\\text{Recall}_{11}$. All subsequent terms $(\\text{Recall}_i - \\text{Recall}_{i-1})$ are zero, contributing no further area.\n\nThe total PR AUC is the sum of the areas from the non-zero segments.\n$$\n\\text{PR AUC} = A_1 + A_2 = \\frac{1}{2} + \\frac{31}{440}\n$$\nTo sum these, we find a common denominator:\n$$\n\\text{PR AUC} = \\frac{220}{440} + \\frac{31}{440} = \\frac{251}{440}\n$$\nNow, we convert this fraction to a decimal and round to four significant figures.\n$$\n\\text{PR AUC} = \\frac{251}{440} \\approx 0.5704545...\n$$\nRounding to four significant figures gives $0.5705$.", "answer": "$$\\boxed{0.5705}$$", "id": "4544659"}]}