{"hands_on_practices": [{"introduction": "To build a strong foundation in evaluating radiomics research, we begin with a direct application of the Radiomics Quality Score (RQS). This exercise presents a hypothetical, high-quality study and a corresponding scoring rubric. By systematically scoring the study item by item, you will gain hands-on familiarity with the key components that define a methodologically robust radiomics investigation and see how they culminate in a final score, as demonstrated in [@problem_id:4554364].", "problem": "A core goal of the radiomics workflow is to generate robust, generalizable, and clinically meaningful models by systematically proceeding through standardized steps including image acquisition, segmentation, preprocessing, feature extraction, model building, and validation. To appraise whether a radiomics study adheres to best practices across this workflow, the Radiomics Quality Score (RQS) is used. The Radiomics Quality Score (RQS) assigns points to specific, widely accepted study design and reporting components that reflect methodological rigor and translational readiness. In this problem, you will compute the total RQS for a hypothetical study by summing the applicable items and subtracting penalties where they occur.\n\nThe scoring rubric for this problem uses the following items and point values, consistent with common practice and constrained to a maximum of 36 points:\n\n- Imaging protocol standardized and fully documented: $+2$\n- Multiple segmentations or inter-observer variability analysis: $+1$\n- Phantom study assessing feature sensitivity to acquisition parameters: $+1$\n- Test–retest or repeatability analysis on a subset of subjects: $+2$\n- Proper feature reduction and correction for multiple hypothesis testing: $+3$\n- Multivariable analysis including non-radiomics covariates (for example, age or clinical stage): $+2$\n- Biological correlates demonstrated (for example, correlation with histopathology or gene expression): $+4$\n- Discrimination statistics reported (for example, area under the receiver operating characteristic curve): $+2$\n- Calibration statistics reported (for example, calibration curve or Hosmer–Lemeshow test): $+1$\n- External validation on an independent cohort: $+4$\n- Comparison against a non-radiomics standard-of-care baseline (for example, Tumor–Node–Metastasis staging): $+2$\n- Open-science elements (for example, code, features, and a de-identified dataset released): $+3$\n- Prospective design with prior registration (for example, registry entry before data accrual): $+7$\n- Cost-effectiveness analysis performed: $+2$\n- Post hoc dichotomization via an optimal cutoff without adequate justification or validation: $-1$\n\nConsider the following hypothetical study designed to predict 2-year local recurrence in head-and-neck cancer:\n\n- The imaging protocol was harmonized across scanners and fully documented in the methods.\n- Two experienced radiologists independently segmented the gross tumor volume; inter-observer robustness was quantified with the intraclass correlation coefficient, and features with low agreement were excluded.\n- A calibrated computed tomography phantom was scanned to assess how voxel size and reconstruction kernel affect selected texture features.\n- A subset of 20 patients underwent repeat imaging within 48 hours; the repeatability of features was quantified and used to retain only stable features.\n- The feature space was reduced using recursive feature elimination; during univariate screening, false discovery rate correction was applied.\n- The final model included age and Tumor–Node–Metastasis stage alongside radiomics features in a multivariable framework.\n- Selected radiomics signatures were correlated with Ki–$67$ proliferation index from biopsy specimens.\n- Model discrimination was reported using the area under the receiver operating characteristic curve.\n- Model calibration was assessed with calibration plots and the Hosmer–Lemeshow goodness-of-fit test.\n- The model was externally validated on an independent cohort from another institution.\n- The model’s performance was compared against a baseline multivariable model using Tumor–Node–Metastasis staging alone.\n- The code, complete feature definitions, trained model, and a de-identified subset of imaging data with segmentations were publicly released in a repository.\n- The study was prospectively designed and registered prior to patient accrual, with adherence to the registered protocol reported.\n- A cost-effectiveness analysis was conducted to evaluate clinical utility relative to standard-of-care imaging follow-up schedules.\n- In exploratory analysis, a single texture feature was dichotomized using the Youden index on the training set to define “high-risk” versus “low-risk” groups; this cutoff was not pre-specified nor justified beyond the training set.\n\nCompute the total Radiomics Quality Score for this study by summing applicable positive items and subtracting any penalties. Report your final answer as a single integer number of points. No rounding is required and no units should be included in your final answer.", "solution": "The problem is valid as it is scientifically grounded in the field of radiomics, well-posed, objective, and contains a complete and consistent set of information for its resolution. The task is to calculate the total Radiomics Quality Score (RQS) for a hypothetical study by systematically evaluating its methodology against a provided scoring rubric. The total score, $S_{\\text{RQS}}$, is the sum of points awarded for meeting specific criteria minus any penalties incurred.\n\nThe scoring for the hypothetical study is determined as follows, by matching the study's description to each item in the rubric:\n\n1.  **Imaging protocol standardized and fully documented ($+2$ points):** The study description states, \"The imaging protocol was harmonized across scanners and fully documented in the methods.\" This directly satisfies the criterion. Points awarded: $+2$.\n\n2.  **Multiple segmentations or inter-observer variability analysis ($+1$ point):** The study description confirms this: \"Two experienced radiologists independently segmented the gross tumor volume; inter-observer robustness was quantified with the intraclass correlation coefficient, and features with low agreement were excluded.\" This fulfills the requirement. Points awarded: $+1$.\n\n3.  **Phantom study assessing feature sensitivity to acquisition parameters ($+1$ point):** This criterion is met, as stated: \"A calibrated computed tomography phantom was scanned to assess how voxel size and reconstruction kernel affect selected texture features.\" Points awarded: $+1$.\n\n4.  **Test–retest or repeatability analysis on a subset of subjects ($+2$ points):** The study includes this analysis: \"A subset of $20$ patients underwent repeat imaging within $48$ hours; the repeatability of features was quantified and used to retain only stable features.\" Points awarded: $+2$.\n\n5.  **Proper feature reduction and correction for multiple hypothesis testing ($+3$ points):** The methodology described, \"The feature space was reduced using recursive feature elimination; during univariate screening, false discovery rate correction was applied,\" satisfies both components of this criterion. Points awarded: $+3$.\n\n6.  **Multivariable analysis including non-radiomics covariates ($+2$ points):** The model description confirms this: \"The final model included age and Tumor–Node–Metastasis stage alongside radiomics features in a multivariable framework.\" Age and clinical stage are non-radiomics covariates. Points awarded: $+2$.\n\n7.  **Biological correlates demonstrated ($+4$ points):** The study establishes a biological link: \"Selected radiomics signatures were correlated with Ki–$67$ proliferation index from biopsy specimens.\" This is a valid biological correlation. Points awarded: $+4$.\n\n8.  **Discrimination statistics reported ($+2$ points):** Discriminatory performance was reported: \"Model discrimination was reported using the area under the receiver operating characteristic curve.\" This is a standard discrimination metric. Points awarded: $+2$.\n\n9.  **Calibration statistics reported ($+1$ point):** Calibration was assessed: \"Model calibration was assessed with calibration plots and the Hosmer–Lemeshow goodness-of-fit test.\" These are standard calibration metrics. Points awarded: $+1$.\n\n10. **External validation on an independent cohort ($+4$ points):** The study performed external validation: \"The model was externally validated on an independent cohort from another institution.\" This is a critical step for generalizability. Points awarded: $+4$.\n\n11. **Comparison against a non-radiomics standard-of-care baseline ($+2$ points):** The model's utility was benchmarked: \"The model’s performance was compared against a baseline multivariable model using Tumor–Node–Metastasis staging alone.\" Points awarded: $+2$.\n\n12. **Open-science elements ($+3$ points):** The study adhered to open-science principles: \"The code, complete feature definitions, trained model, and a de-identified subset of imaging data with segmentations were publicly released in a repository.\" Points awarded: $+3$.\n\n13. **Prospective design with prior registration ($+7$ points):** The study design was prospective: \"The study was prospectively designed and registered prior to patient accrual, with adherence to the registered protocol reported.\" This represents the highest level of evidence. Points awarded: $+7$.\n\n14. **Cost-effectiveness analysis performed ($+2$ points):** The study evaluated economic implications: \"A cost-effectiveness analysis was conducted to evaluate clinical utility relative to standard-of-care imaging follow-up schedules.\" Points awarded: $+2$.\n\n15. **Post hoc dichotomization via an optimal cutoff without adequate justification or validation ($-1$ point):** The study description includes a methodological flaw that incurs a penalty: \"In exploratory analysis, a single texture feature was dichotomized using the Youden index on the training set to define “high-risk” versus “low-risk” groups; this cutoff was not pre-specified nor justified beyond the training set.\" This is a classic example of post hoc dichotomization, which can introduce bias. Points deducted: $-1$.\n\nThe total Radiomics Quality Score is the algebraic sum of all points awarded and penalties deducted. The sum of all positive points is $2+1+1+2+3+2+4+2+1+4+2+3+7+2 = 36$. This sum matches the maximum possible score specified in the problem statement.\n\nThe final calculation is:\n$$S_{\\text{RQS}} = 2 + 1 + 1 + 2 + 3 + 2 + 4 + 2 + 1 + 4 + 2 + 3 + 7 + 2 - 1$$\n$$S_{\\text{RQS}} = 36 - 1$$\n$$S_{\\text{RQS}} = 35$$\nThe total Radiomics Quality Score for the hypothetical study is $35$.", "answer": "$$\\boxed{35}$$", "id": "4554364"}, {"introduction": "The Radiomics Quality Score is more than a simple checklist; its components are designed to guard against critical statistical pitfalls that can invalidate research findings. This exercise delves into the statistical reasoning behind one of the RQS's most important items: correction for multiple hypothesis testing. You will mathematically demonstrate the danger of uncorrected feature screening by calculating the expected proportion of false discoveries, providing a concrete understanding of why the RQS heavily penalizes this practice [@problem_id:4567811].", "problem": "A radiomics study extracts $m$ handcrafted texture and shape features from medical images and screens them via separate univariate statistical tests against a binary clinical endpoint, selecting all features whose $p$-value is less than a nominal significance level $\\alpha$ without any multiple-hypothesis correction. Assume a scientifically plausible scenario in which $m = 1000$ features are tested, of which $m_{0} = 900$ are truly null (no association with the endpoint) and $m_{1} = 100$ are truly non-null (associated with the endpoint). Suppose the univariate tests for the non-null features have average statistical power $\\pi = 0.60$ at level $\\alpha = 0.05$. Treat feature-wise test outcomes as independent and use a large-sample approximation in which sums of independent indicator variables concentrate around their expectations.\n\nStarting from the fundamental definitions of Type I error ($\\alpha$), statistical power ($\\pi$), and the False Discovery Rate (FDR), defined as the expected proportion of false discoveries among all selected features, derive an analytic expression for the expected FDR under this screening procedure and evaluate it for the given $(m, m_{0}, m_{1}, \\alpha, \\pi)$. Express your final numerical value as an exact fraction.\n\nFinally, explain—qualitatively and without additional calculation—how the Radiomics Quality Score (RQS) guards against the pitfall exemplified by your computed expected FDR through its requirements on study design, analysis, and validation. Your explanation will not affect the numerical answer requirement.", "solution": "The problem requires the derivation and calculation of the expected False Discovery Rate (FDR) for a radiomics screening study under specific conditions, and a subsequent qualitative explanation of how the Radiomics Quality Score (RQS) mitigates the illustrated statistical pitfall.\n\n### Problem Validation\n\nThe provided problem statement is validated as scientifically grounded, well-posed, objective, and complete. All necessary parameters and definitions are provided. The scenario described is a standard and highly relevant problem in high-dimensional data analysis, particularly within bioinformatics and radiomics. The parameters are consistent, with the total number of features $m = 1000$ being the sum of the null features $m_0 = 900$ and non-null features $m_1 = 100$. The problem is formalizable using fundamental statistical principles and does not violate any scientific laws or contain logical contradictions. Therefore, a full solution can be constructed.\n\n### Derivation of the Expected False Discovery Rate (FDR)\n\nThe False Discovery Rate is defined as the expected proportion of false discoveries among all features declared significant. In the context of this problem, which employs a large-sample approximation, we can express the expected FDR as the ratio of the expected number of false discoveries to the expected total number of discoveries.\n\nLet's formally define the quantities involved:\n-   $V$: The number of false discoveries. These are truly null features that are incorrectly declared significant (Type I errors).\n-   $S$: The number of true discoveries. These are truly non-null features that are correctly declared significant.\n-   $R = V + S$: The total number of discoveries (all features selected).\n\nThe expected FDR is then approximated as:\n$$\n\\text{Expected FDR} = \\frac{E[V]}{E[R]} = \\frac{E[V]}{E[V] + E[S]}\n$$\n\nWe first calculate the expected number of false discoveries, $E[V]$.\nThere are $m_0 = 900$ truly null features. Each feature is tested at a nominal significance level of $\\alpha = 0.05$. By definition, the significance level $\\alpha$ is the probability of a Type I error for a single test. Assuming the tests are independent, the number of false discoveries $V$ follows a binomial distribution, $V \\sim \\text{Binomial}(m_0, \\alpha)$. The expectation of a binomial distribution is the product of the number of trials and the probability of success.\nTherefore, the expected number of false discoveries is:\n$$\nE[V] = m_0 \\alpha\n$$\n\nNext, we calculate the expected number of true discoveries, $E[S]$.\nThere are $m_1 = 100$ truly non-null features. The average statistical power, $\\pi = 0.60$, is the probability of correctly detecting a true association (i.e., correctly rejecting a false null hypothesis). Assuming independence, the number of true discoveries $S$ follows a binomial distribution, $S \\sim \\text{Binomial}(m_1, \\pi)$.\nTherefore, the expected number of true discoveries is:\n$$\nE[S] = m_1 \\pi\n$$\n\nNow, we can assemble the analytical expression for the expected FDR. The expected total number of discoveries is $E[R] = E[V] + E[S] = m_0 \\alpha + m_1 \\pi$.\nSubstituting the expressions for $E[V]$ and $E[R]$ gives the final analytical expression:\n$$\n\\text{Expected FDR} = \\frac{m_0 \\alpha}{m_0 \\alpha + m_1 \\pi}\n$$\n\n### Numerical Evaluation\n\nWe are given the following values:\n-   Number of null features, $m_0 = 900$\n-   Number of non-null features, $m_1 = 100$\n-   Significance level, $\\alpha = 0.05$\n-   Average statistical power, $\\pi = 0.60$\n\nFirst, we compute the expected number of false discoveries:\n$$\nE[V] = 900 \\times 0.05 = 45\n$$\nOn average, we expect $45$ features to be declared significant by chance alone.\n\nNext, we compute the expected number of true discoveries:\n$$\nE[S] = 100 \\times 0.60 = 60\n$$\nOn average, we expect to correctly identify $60$ of the truly associated features.\n\nThe expected total number of features selected is:\n$$\nE[R] = E[V] + E[S] = 45 + 60 = 105\n$$\n\nFinally, we calculate the expected FDR by taking the ratio of expected false discoveries to expected total discoveries:\n$$\n\\text{Expected FDR} = \\frac{45}{105}\n$$\nTo express this as an exact fraction in simplest form, we divide the numerator and denominator by their greatest common divisor, which is $15$:\n$$\n\\text{Expected FDR} = \\frac{45 \\div 15}{105 \\div 15} = \\frac{3}{7}\n$$\nThe expected FDR is $\\frac{3}{7}$, which is approximately $0.4286$. This means that under the specified screening procedure, we expect that nearly $43\\%$ of the selected features are, in fact, spurious findings with no true association to the clinical endpoint.\n\n### Role of the Radiomics Quality Score (RQS)\n\nThe high calculated value for the expected FDR ($\\frac{3}{7}$) exemplifies a critical pitfall in high-dimensional data analysis: the problem of multiple comparisons. When many hypotheses are tested simultaneously, the probability of obtaining false positives accumulates rapidly if not properly controlled. The Radiomics Quality Score (RQS) is a framework designed to enhance the methodological rigor and reproducibility of radiomics studies, and it includes specific components that guard against this exact pitfall.\n\nThe RQS would score the study described in the problem poorly for several reasons, primarily:\n1.  **Lack of Multiple Testing Correction**: The RQS framework explicitly requires that studies performing multiple hypothesis tests must apply a statistical correction. The problem states that features were selected if their $p  \\alpha$ \"without any multiple-hypothesis correction.\" An adequate study would apply methods like the Bonferroni correction or, more commonly in discovery-oriented fields, a false discovery rate control procedure (e.g., Benjamini-Hochberg). Failure to do so is a major methodological flaw that the RQS is designed to detect and penalize.\n\n2.  **Emphasis on Validation**: A cornerstone of the RQS is the robust validation of findings. The RQS awards points for using data partitioning strategies (e.g., training/test sets) and, most importantly, for testing the final model or findings on an independent, external validation cohort. A feature set contaminated with a high proportion of false positives (as demonstrated by the high expected FDR) would lead to a model that is severely overfitted to the training data. Such a model would exhibit good performance on the data it was built from but would almost certainly fail to generalize, showing poor performance on any independent validation set. The RQS's stringent validation requirements would expose the non-reproducibility and lack of clinical utility of the spurious findings.\n\n3.  **Promotion of Methodological Transparency**: The RQS requires clear and complete reporting of the entire analysis pipeline. A study following RQS guidelines would have to explicitly state that $1000$ features were tested and that no correction for multiplicity was applied. This transparency alone would allow peer reviewers and readers to immediately identify the high risk of false discoveries and critically question the validity of the study's conclusions.\n\nIn essence, the RQS acts as a quality control checklist that enforces best statistical practices. By mandating multiple testing correction and robust independent validation, it directly and indirectly ensures that the final set of reported radiomic features is not overwhelmed by the spurious associations that inevitably arise from large-scale screening without statistical rigor.", "answer": "$$\\boxed{\\frac{3}{7}}$$", "id": "4567811"}, {"introduction": "Applying the RQS to real-world research often involves more than just checking boxes; it requires scientific judgment and the ability to navigate ambiguity. This final practice challenges you to appraise a realistic, imperfect study where scoring decisions are not always clear-cut. By working through the nuances of issues like data harmonization and partial data sharing, you will develop the critical thinking skills necessary to apply the RQS framework effectively as a tool for robust scientific appraisal [@problem_id:4567841].", "problem": "A research team conducts a retrospective radiomics study to predict $1$-year progression-free survival in glioblastoma from preoperative Magnetic Resonance Imaging (MRI). You are asked to score this study with the Radiomics Quality Score (RQS) item-by-item and justify ambiguous scoring decisions. Use the following standard Radiomics Quality Score (RQS) rubric as the base definitions for scoring. The maximum total is $36$ points.\n\nBase RQS rubric (items and points):\n- Imaging protocol and feature extraction fully described: $+2$ if sequence parameters, preprocessing, Region of Interest (ROI) definition, and feature extraction pipeline are sufficiently detailed to allow reproduction.\n- Multiple segmentations to test inter-observer variability: $+1$ if features are recomputed on multiple independent segmentations and variability is quantified.\n- Technical repeatability:\n  - Phantom study to assess feature repeatability: $+1$.\n  - In vivo test-retest to assess feature repeatability: $+2$.\n  - The combined credit for phantom and test-retest is capped at $+3$.\n- Longitudinal stability across multiple time points: $+2$ if the same subjects are imaged at multiple time points and feature stability is analyzed.\n- Feature reduction and control for multiple testing: $+3$ if explicit dimensionality reduction and multiplicity control (for example, Benjamini–Hochberg false discovery rate) are applied appropriately.\n- Comparison to a non-radiomics reference or clinical gold standard: $+2$ if the radiomics model is compared against an established clinical model or guideline-based benchmark.\n- Biological or clinical correlates: $+2$ if radiomics features are linked to biological markers, histopathology, or molecular data with appropriate analysis.\n- Model calibration: $+1$ if calibration (for example, calibration plots, Brier score, or Hosmer–Lemeshow) is reported and interpreted.\n- Validation:\n  - Internal validation (for example, cross-validation or bootstrapping with a locked modeling workflow): $+2$.\n  - External validation on an independent cohort with no information leakage from training to validation (including no harmonization or preprocessing steps fitted using both cohorts): $+4$.\n- Clinical utility analysis: $+1$ if Decision Curve Analysis (DCA) or equivalent is performed and interpreted.\n- Cost-effectiveness analysis: $+1$ if an economic evaluation is performed.\n- Prospective design with preregistration: $+7$ for a preregistered prospective study.\n- Open science:\n  - Data sharing: $+3$ if raw imaging data and outcomes (de-identified) sufficient to reproduce the analysis are publicly available.\n  - Code sharing: $+2$ if complete, versioned code to reproduce the pipeline is publicly available.\n\nHypothetical study summary:\n- Design and cohorts: Retrospective, single-center development cohort of $n=120$ patients; external validation on $n=60$ patients from a different hospital scanned on different MRI vendors and field strengths.\n- Imaging and features: Detailed MRI sequence parameters and preprocessing steps are fully reported; ROI segmentation protocol is described. Feature extraction library, version, and settings are specified.\n- Segmentation variability: Two independent raters segmented a random subset of $n=30$ development cases; features were recomputed on both segmentations, and Intraclass Correlation Coefficients (ICC) were reported; only features with ICC $\\geq 0.85$ were retained.\n- Technical repeatability: A $3$D-printed brain phantom was scanned on both scanners; a subset of key features was analyzed for repeatability; no in vivo test-retest imaging of patients was performed.\n- Longitudinal data: No multiple time points per patient were analyzed.\n- Feature reduction and multiplicity: Stability selection and Benjamini–Hochberg false discovery rate control were used; the full modeling pipeline was embedded in nested cross-validation to prevent information leakage.\n- Modeling and performance: A penalized Cox model was fit; discrimination was summarized by the concordance index and time-dependent Area Under the Receiver Operating Characteristic Curve (AUC). Calibration curves and Brier scores were reported and interpreted.\n- Benchmarking: The radiomics model was compared against a clinical model using age, extent of resection, and O-6-methylguanine-DNA methyltransferase (MGMT) methylation status; net reclassification improvement was reported.\n- Biological correlates: Associations between selected radiomics features and RNA expression subtypes were tested with appropriate multiple-testing control.\n- Validation and harmonization: Internal validation used nested cross-validation on the development cohort. External validation used the $n=60$ independent cohort. To mitigate scanner effects, ComBat harmonization was fitted using only the development cohort and then applied to the external cohort without refitting.\n- Clinical utility: Decision Curve Analysis (DCA) was performed and interpreted.\n- Cost-effectiveness: No economic evaluation was performed.\n- Open science: The complete, versioned code repository is public. A de-identified feature matrix and outcome labels are public, but raw images are not available due to institutional restrictions. The study was not preregistered and was not prospective.\n\nAmbiguities to consider when scoring:\n- Whether multiple segmentations on a subset of subjects suffice for the “multiple segmentations” item.\n- Whether a phantom-only study (without in vivo test-retest) earns only $+1$ under the technical repeatability item.\n- Whether ComBat harmonization applied as described preserves the independence of the external validation.\n- Whether sharing only derived feature matrices (without raw images) qualifies for “data sharing.”\n- Whether the reported calibration and DCA meet the bar for their respective items.\n\nQuestion. Based on the rubric and the study details, which option best states the item-by-item RQS scoring and resolves each ambiguity in a manner consistent with the stated base definitions?\n\nOptions:\n- A. Protocol/feature extraction $+2$; multiple segmentations $+1$; technical repeatability $+3$ (crediting both phantom and inferred test-retest via repeated scans across vendors); no longitudinal $0$; feature reduction/multiplicity $+3$; comparison to clinical model $+2$; biological correlates $+2$; calibration $+1$; internal validation $+2$; external validation $+4$; clinical utility $+1$; cost-effectiveness $0$; prospective preregistration $0$; open data $+3$ (accepting derived features as data); open code $+2$. Total $=26$. Ambiguities resolved by accepting repeated vendor scanning as equivalent to in vivo test-retest and accepting derived features as sufficient for data sharing.\n- B. Protocol/feature extraction $+2$; multiple segmentations $+1$ (subset acceptable as it quantifies inter-observer effects on features); technical repeatability $+1$ (phantom only; no in vivo test-retest); no longitudinal $0$; feature reduction/multiplicity $+3$; comparison to clinical model $+2$; biological correlates $+2$; calibration $+1$; internal validation $+2$; external validation $+4$ (ComBat fitted on development only avoids leakage); clinical utility $+1$; cost-effectiveness $0$; prospective preregistration $0$; open data $0$ (derived features alone insufficient); open code $+2$. Total $=21$. Ambiguities resolved by requiring in vivo test-retest for the $+2$ repeatability credit, requiring raw images for data sharing, and confirming external independence under one-way ComBat application.\n- C. Protocol/feature extraction $+2$; multiple segmentations $0$ (subset deemed insufficient); technical repeatability $+1$ (phantom only); no longitudinal $0$; feature reduction/multiplicity $+3$; comparison to clinical model $+2$; biological correlates $+2$; calibration $+1$; internal validation $+2$; external validation $0$ (harmonization considered leakage regardless of fitting); clinical utility $+1$; cost-effectiveness $0$; prospective preregistration $0$; open data $0$; open code $0$ (code lacks full environment capture). Total $=14$. Ambiguities resolved by requiring whole-cohort multi-rater segmentations, rejecting any harmonization in external validation, and requiring containerization for code credit.\n- D. Protocol/feature extraction $+2$; multiple segmentations $0$ (subset insufficient); technical repeatability $+1$; no longitudinal $0$; feature reduction/multiplicity $+3$; comparison to clinical model $+2$; biological correlates $+2$; calibration $+2$ (crediting both calibration plot and Brier score separately); internal validation $+2$; external validation $+4$; clinical utility $+2$ (crediting net benefit and risk curves separately); cost-effectiveness $0$; prospective preregistration $0$; open data $+1$ (partial credit for derived features); open code $+2$. Total $=23$. Ambiguities resolved by splitting calibration and clinical utility into subcredits and granting partial data-sharing credit.", "solution": "The problem statement is a valid exercise in applying the Radiomics Quality Score (RQS) framework to a hypothetical, but realistic, radiomics study. The problem is scientifically grounded in the field of radiomics, well-posed with a clear rubric and a detailed scenario, and expressed in objective, technical language. All necessary information is provided to perform the scoring, and there are no internal contradictions or violations of scientific principles. Proceeding to the solution.\n\nThe task is to score the described radiomics study item-by-item using the provided RQS rubric. This requires careful interpretation of the study's methods and resolving the stated ambiguities based on a rigorous application of the rubric's definitions.\n\n**Item-by-Item RQS Scoring:**\n\n1.  **Imaging protocol and feature extraction fully described ($+2$ points):** The study summary states: \"Detailed MRI sequence parameters and preprocessing steps are fully reported; ROI segmentation protocol is described. Feature extraction library, version, and settings are specified.\" This meets the criterion of being \"sufficiently detailed to allow reproduction.\"\n    **Score: $+2$.**\n\n2.  **Multiple segmentations to test inter-observer variability ($+1$ point):** The rubric requires that \"features are recomputed on multiple independent segmentations and variability is quantified.\" The study used two independent raters on a random subset of $n=30$ cases, reported Intraclass Correlation Coefficients (ICCs), and used this information to filter features ($ICC \\geq 0.85$). Quantifying inter-rater reliability on a representative subset is a standard and valid approach in medical imaging research. It demonstrates a proper handling of segmentation uncertainty.\n    **Score: $+1$.**\n\n3.  **Technical repeatability (Phantom: $+1$, In vivo: $+2$):** The rubric provides separate credits for a phantom study ($+1$) and an in vivo test-retest study ($+2$). The summary explicitly states a phantom study was performed, but \"no in vivo test-retest imaging of patients was performed.\" Therefore, the study only earns credit for the phantom component.\n    **Score: $+1$.**\n\n4.  **Longitudinal stability across multiple time points ($+2$ points):** The summary clearly states: \"No multiple time points per patient were analyzed.\"\n    **Score: $0$.**\n\n5.  **Feature reduction and control for multiple testing ($+3$ points):** The rubric requires \"explicit dimensionality reduction and multiplicity control.\" The study employed \"Stability selection and Benjamini–Hochberg false discovery rate control\" and embedded the entire pipeline in nested cross-validation. These are advanced and appropriate methods that satisfy the criterion.\n    **Score: $+3$.**\n\n6.  **Comparison to a non-radiomics reference ($+2$ points):** The study's radiomics model was \"compared against a clinical model using age, extent of resection, and O-6-methylguanine-DNA methyltransferase (MGMT) methylation status.\" This is a strong, established clinical benchmark for glioblastoma.\n    **Score: $+2$.**\n\n7.  **Biological or clinical correlates ($+2$ points):** The rubric asks for links to \"biological markers... with appropriate analysis.\" The study tested \"Associations between selected radiomics features and RNA expression subtypes ... with appropriate multiple-testing control.\" This directly fulfills the requirement.\n    **Score: $+2$.**\n\n8.  **Model calibration ($+1$ point):** The rubric requires calibration to be \"reported and interpreted.\" The study reported and interpreted \"Calibration curves and Brier scores.\" This meets the criterion. The rubric assigns a single point for this category, regardless of the number of methods used.\n    **Score: $+1$.**\n\n9.  **Validation - Internal ($+2$ points):** The rubric specifies \"cross-validation or bootstrapping with a locked modeling workflow.\" The study used \"nested cross-validation,\" which is a rigorous form of internal validation that prevents information leakage between model tuning and performance estimation.\n    **Score: $+2$.**\n\n10. **Validation - External ($+4$ points):** The rubric requires an \"independent cohort with no information leakage.\" The study used a cohort from a different hospital. The key ambiguity is the harmonization step. The description states that ComBat harmonization was \"fitted using only the development cohort and then applied to the external cohort without refitting.\" This is the methodologically correct procedure. The harmonization parameters are learned as part of the model on the training data and then applied as a fixed transformation to the independent test data. This preserves the independence of the external validation set and does not constitute information leakage.\n    **Score: $+4$.**\n\n11. **Clinical utility analysis ($+1$ point):** The rubric asks for \"Decision Curve Analysis (DCA) or equivalent.\" The study \"performed and interpreted\" a DCA.\n    **Score: $+1$.**\n\n12. **Cost-effectiveness analysis ($+1$ point):** The summary states: \"No economic evaluation was performed.\"\n    **Score: $0$.**\n\n13. **Prospective design with preregistration ($+7$ points):** The study was \"retrospective\" and \"not preregistered.\"\n    **Score: $0$.**\n\n14. **Open science - Data sharing ($+3$ points):** The rubric explicitly requires \"raw imaging data and outcomes\" to be public. The study shared the \"feature matrix and outcome labels\" but \"raw images are not available.\" Sharing derived features does not allow for independent reproduction of the image segmentation and feature extraction steps, which are critical parts of the radiomics pipeline. Therefore, this does not meet the rubric's requirement.\n    **Score: $0$.**\n\n15. **Open science - Code sharing ($+2$ points):** The rubric requires \"complete, versioned code to reproduce the pipeline.\" The summary confirms \"The complete, versioned code repository is public.\"\n    **Score: $+2$.**\n\n**Total RQS Calculation:**\nTotal Score = $2 + 1 + 1 + 0 + 3 + 2 + 2 + 1 + 2 + 4 + 1 + 0 + 0 + 0 + 2 = 21$.\n\n**Evaluation of Options:**\n\n- **A.** This option incorrectly awards $+3$ for technical repeatability, based on a scientifically flawed interpretation of \"test-retest\". It also incorrectly awards $+3$ for data sharing, being too lenient by accepting derived features instead of the required raw images. The total score of $26$ is incorrect. **Incorrect.**\n\n- **B.** This option's item-by-item score and total score of $21$ match the derived result precisely. Its justifications for the ambiguous points are all correct: a subset is acceptable for segmentation analysis; only phantom credit is due for repeatability; the one-way ComBat application is valid for external validation; and raw images are required for the data sharing credit. **Correct.**\n\n- **C.** This option is overly punitive and methodologically incorrect. It wrongly penalizes the use of a subset for segmentation analysis ($0$ vs. $+1$). It incorrectly dismisses the external validation by mischaracterizing the correct application of harmonization as information leakage ($0$ vs. $+4$). It also adds an extra-rubric requirement (containerization) to deny points for open code ($0$ vs. $+2$). The total score of $14$ is incorrect. **Incorrect.**\n\n- **D.** This option invents a scoring system not supported by the rubric. It incorrectly awards extra points for calibration ($+2$ instead of $+1$) and clinical utility ($+2$ instead of $+1$). It also invents a \"partial credit\" scheme for data sharing ($+1$) which is not in the rubric. It further errs by denying credit for segmentation analysis on a subset ($0$ vs. $+1$). The total score of $23$ is incorrect. **Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "4567841"}]}