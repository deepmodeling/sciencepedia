{"hands_on_practices": [{"introduction": "To bridge the gap between medical images and genomic data, we must first convert visual patterns into numbers. This exercise focuses on a fundamental step: extracting texture features using the Gray-Level Run Length Matrix (GLRLM). By manually calculating the GLRLM and its derived features from a small, synthetic 3D image region, you will gain a concrete understanding of how image heterogeneity is quantified for downstream analysis [@problem_id:4557656].", "problem": "In a radiogenomics workflow linking image-derived texture to messenger ribonucleic acid (mRNA) expression, a three-dimensional (3D) region of interest (ROI) is extracted from a magnetic resonance imaging (MRI) scan and its intensities are quantized to discrete gray levels. Consider the following synthetic 3D ROI, where every voxel belongs to the ROI. Voxel coordinates are indexed by $(x,y,z)$ with $x \\in \\{1,2,3\\}$, $y \\in \\{1,2\\}$, and $z \\in \\{1,2\\}$. Intensities are already quantized to gray levels $\\{1,2,3\\}$.\n\nSlice $z=1$:\n- Row $y=1$: $[1\\ 1\\ 2]$ for $x=1,2,3$\n- Row $y=2$: $[2\\ 2\\ 2]$ for $x=1,2,3$\n\nSlice $z=2$:\n- Row $y=1$: $[3\\ 1\\ 1]$ for $x=1,2,3$\n- Row $y=2$: $[1\\ 3\\ 3]$ for $x=1,2,3$\n\nUsing the definition of the Gray-Level Run Length Matrix (GLRLM), where a run is a maximal sequence of consecutive, colinear voxels with identical gray level along a specified direction, construct the GLRLM for this ROI in the $+\\hat{x}$ direction only (i.e., runs are traced along increasing $x$ within each fixed $(y,z)$ line; no wrapping or cross-line continuation). Then, from this GLRLM, compute the short-run emphasis (SRE) and long-run emphasis (LRE) features, where SRE gives higher weight to shorter runs and LRE gives higher weight to longer runs, both normalized by the total number of runs.\n\nState any quantities you derive symbolically before substituting numbers, and base your derivation on core definitions of run-length counting and feature construction from the GLRLM. Express the final SRE and LRE as unitless real numbers, rounded to four significant figures. Provide your final answer as two values in the order $\\text{SRE}$, $\\text{LRE}$.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the field of radiomics, well-posed with all necessary data and definitions provided, and objective in its formulation. We can proceed with the solution.\n\nThe problem requires the computation of two texture features, Short-Run Emphasis (SRE) and Long-Run Emphasis (LRE), from a Gray-Level Run Length Matrix (GLRLM). The first step is to construct the GLRLM for the given three-dimensional Region of Interest (ROI) in the specified direction.\n\nThe ROI is defined by voxel intensities $I(x,y,z)$ for coordinates $x \\in \\{1,2,3\\}$, $y \\in \\{1,2\\}$, and $z \\in \\{1,2\\}$. The gray levels are $G \\in \\{1,2,3\\}$. The data is provided as:\nSlice $z=1$:\n$I(x, y=1, z=1) = [1, 1, 2]$\n$I(x, y=2, z=1) = [2, 2, 2]$\nSlice $z=2$:\n$I(x, y=1, z=2) = [3, 1, 1]$\n$I(x, y=2, z=2) = [1, 3, 3]$\n\nA run is a maximal sequence of consecutive, colinear voxels with identical gray level. We are instructed to find runs only in the $+\\hat{x}$ direction. This involves analyzing each line of voxels where $y$ and $z$ are fixed and $x$ varies. There are $2 \\times 2 = 4$ such lines in this ROI.\n\n1.  **Line 1: $(y=1, z=1)$**\n    The intensity sequence is $[1, 1, 2]$. We identify two runs:\n    - A run of gray level $g=1$ with length $l=2$.\n    - A run of gray level $g=2$ with length $l=1$.\n\n2.  **Line 2: $(y=2, z=1)$**\n    The intensity sequence is $[2, 2, 2]$. We identify one run:\n    - A run of gray level $g=2$ with length $l=3$.\n\n3.  **Line 3: $(y=1, z=2)$**\n    The intensity sequence is $[3, 1, 1]$. We identify two runs:\n    - A run of gray level $g=3$ with length $l=1$.\n    - A run of gray level $g=1$ with length $l=2$.\n\n4.  **Line 4: $(y=2, z=2)$**\n    The intensity sequence is $[1, 3, 3]$. We identify two runs:\n    - A run of gray level $g=1$ with length $l=1$.\n    - A run of gray level $g=3$ with length $l=2$.\n\nNext, we construct the GLRLM, denoted by the matrix $P$. The entry $P(i,j)$ represents the number of runs of gray level $i$ with length $j$. The number of gray levels is $N_g=3$ and the maximum run length observed is $N_s=3$.\n\nTallying the runs for each gray level:\n- **Gray level $i=1$**:\n    - One run of length $j=2$ from Line 1.\n    - One run of length $j=2$ from Line 3.\n    - One run of length $j=1$ from Line 4.\n    - Total: $P(1,1)=1$, $P(1,2)=2$, $P(1,3)=0$.\n\n- **Gray level $i=2$**:\n    - One run of length $j=1$ from Line 1.\n    - One run of length $j=3$ from Line 2.\n    - Total: $P(2,1)=1$, $P(2,2)=0$, $P(2,3)=1$.\n\n- **Gray level $i=3$**:\n    - One run of length $j=1$ from Line 3.\n    - One run of length $j=2$ from Line 4.\n    - Total: $P(3,1)=1$, $P(3,2)=1$, $P(3,3)=0$.\n\nThe resulting GLRLM, $P$, is a $3 \\times 3$ matrix:\n$$\nP = \\begin{pmatrix}\nP(1,1)  P(1,2)  P(1,3) \\\\\nP(2,1)  P(2,2)  P(2,3) \\\\\nP(3,1)  P(3,2)  P(3,3)\n\\end{pmatrix} = \\begin{pmatrix}\n1  2  0 \\\\\n1  0  1 \\\\\n1  1  0\n\\end{pmatrix}\n$$\n\nThe total number of runs, $N_R$, is the sum of all elements in $P$:\n$$\nN_R = \\sum_{i=1}^{N_g} \\sum_{j=1}^{N_s} P(i,j) = (1+2+0) + (1+0+1) + (1+1+0) = 3 + 2 + 2 = 7\n$$\n\nNow we can compute the SRE and LRE features.\n\n**Short-Run Emphasis (SRE)**: This feature gives higher weight to shorter runs. It is defined as:\n$$\n\\text{SRE} = \\frac{1}{N_R} \\sum_{i=1}^{N_g} \\sum_{j=1}^{N_s} \\frac{P(i,j)}{j^2}\n$$\nSubstituting the values from our GLRLM $P$:\n$$\n\\text{SRE} = \\frac{1}{7} \\left( \\frac{P(1,1)}{1^2} + \\frac{P(1,2)}{2^2} + \\frac{P(2,1)}{1^2} + \\frac{P(2,3)}{3^2} + \\frac{P(3,1)}{1^2} + \\frac{P(3,2)}{2^2} \\right)\n$$\n$$\n\\text{SRE} = \\frac{1}{7} \\left( \\frac{1}{1} + \\frac{2}{4} + \\frac{1}{1} + \\frac{1}{9} + \\frac{1}{1} + \\frac{1}{4} \\right)\n$$\n$$\n\\text{SRE} = \\frac{1}{7} \\left( 1 + \\frac{1}{2} + 1 + \\frac{1}{9} + 1 + \\frac{1}{4} \\right) = \\frac{1}{7} \\left( 3 + \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{9} \\right)\n$$\nTo sum the fractions, we find a common denominator, which is $36$:\n$$\n\\text{SRE} = \\frac{1}{7} \\left( 3 + \\frac{18}{36} + \\frac{9}{36} + \\frac{4}{36} \\right) = \\frac{1}{7} \\left( 3 + \\frac{31}{36} \\right) = \\frac{1}{7} \\left( \\frac{108+31}{36} \\right) = \\frac{1}{7} \\left( \\frac{139}{36} \\right) = \\frac{139}{252}\n$$\nNumerically, this is:\n$$\n\\text{SRE} \\approx 0.5515873...\n$$\nRounding to four significant figures, we get $\\text{SRE} \\approx 0.5516$.\n\n**Long-Run Emphasis (LRE)**: This feature gives higher weight to longer runs. It is defined as:\n$$\n\\text{LRE} = \\frac{1}{N_R} \\sum_{i=1}^{N_g} \\sum_{j=1}^{N_s} j^2 P(i,j)\n$$\nSubstituting the values from our GLRLM $P$:\n$$\n\\text{LRE} = \\frac{1}{7} \\left( 1^2 P(1,1) + 2^2 P(1,2) + 1^2 P(2,1) + 3^2 P(2,3) + 1^2 P(3,1) + 2^2 P(3,2) \\right)\n$$\n$$\n\\text{LRE} = \\frac{1}{7} \\left( 1(1) + 4(2) + 1(1) + 9(1) + 1(1) + 4(1) \\right)\n$$\n$$\n\\text{LRE} = \\frac{1}{7} (1 + 8 + 1 + 9 + 1 + 4) = \\frac{1}{7} (24) = \\frac{24}{7}\n$$\nNumerically, this is:\n$$\n\\text{LRE} \\approx 3.4285714...\n$$\nRounding to four significant figures, we get $\\text{LRE} \\approx 3.429$.\n\nThe final computed values for SRE and LRE are approximately $0.5516$ and $3.429$, respectively.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5516  3.429\n\\end{pmatrix}\n}\n$$", "id": "4557656"}, {"introduction": "Radiogenomics rarely involves simple one-to-one correlations; instead, it seeks complex relationships between groups of imaging features and gene expression profiles. This practice introduces Canonical Correlation Analysis (CCA), a powerful technique for uncovering these multidimensional links. By implementing the method from its linear algebra foundations, you will learn how to identify and quantify the shared axes of variation between the radiomic and genomic domains [@problem_id:4557615].", "problem": "You are given a toy radiogenomics scenario where two radiomics imaging features are linked to two gene expression levels across a small cohort. The goal is to compute the canonical variates and canonical correlations according to Canonical Correlation Analysis (CCA), a multivariate method intended to find linear combinations of variables in two sets that are maximally correlated. Use the following fundamental definitions as the base: given centered data matrices $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ and $\\mathbf{Y} \\in \\mathbb{R}^{n \\times q}$, their sample covariance matrices are $\\mathbf{S}_{\\mathbf{X}\\mathbf{X}} = \\frac{1}{n-1}\\mathbf{X}^\\top \\mathbf{X}$, $\\mathbf{S}_{\\mathbf{Y}\\mathbf{Y}} = \\frac{1}{n-1}\\mathbf{Y}^\\top \\mathbf{Y}$, and $\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}} = \\frac{1}{n-1}\\mathbf{X}^\\top \\mathbf{Y}$. The canonical correlations are defined as the maximal Pearson correlations between linear combinations $\\mathbf{X}\\mathbf{a}$ and $\\mathbf{Y}\\mathbf{b}$, under the constraints that $\\mathrm{Var}(\\mathbf{X}\\mathbf{a}) = 1$ and $\\mathrm{Var}(\\mathbf{Y}\\mathbf{b}) = 1$. Your task is to compute these quantities from first principles, without using any built-in canonical correlation functions.\n\nConstruct the following fixed, deterministic test suite. All vectors are of length $n = 5$, and all columns are already mean-centered. Define the shared imaging feature matrix $\\mathbf{X}$ from two column vectors $\\mathbf{u}$ and $\\mathbf{v}$ as\n$$\n\\mathbf{u} = \\begin{bmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}, \\quad\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{u}  \\mathbf{v} \\end{bmatrix}.\n$$\nDefine auxiliary vectors orthogonal (in the sample inner-product sense) to both $\\mathbf{u}$ and $\\mathbf{v}$ and with zero mean:\n$$\n\\mathbf{w}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{w}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -2 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n$$\nNow define three gene-expression matrices $\\mathbf{Y}$ to probe different regimes:\n- Test case $1$ (perfect linear linkage across both dimensions): let\n$$\n\\mathbf{A} = \\begin{bmatrix} 2  1 \\\\ -1  3 \\end{bmatrix}, \\quad \\mathbf{Y}_1 = \\mathbf{X}\\mathbf{A}.\n$$\n- Test case $2$ (no linear linkage across either dimension): let\n$$\n\\mathbf{Y}_2 = \\begin{bmatrix} \\mathbf{w}_1  \\mathbf{w}_2 \\end{bmatrix}.\n$$\n- Test case $3$ (one linked and one unlinked dimension): let\n$$\n\\mathbf{Y}_3 = \\begin{bmatrix} \\mathbf{u}  \\mathbf{w}_1 \\end{bmatrix}.\n$$\n\nProgram requirements:\n- For each test case, compute the leading two canonical correlations implied by the CCA optimization problem described above. In each case, also compute the corresponding canonical variates (the linear combinations) so that your program logic is faithful to the principle-based definition, but you do not need to print the canonical variate vectors. Use the sample covariance definitions $\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}$, $\\mathbf{S}_{\\mathbf{Y}\\mathbf{Y}}$, $\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}}$ with the factor $\\frac{1}{n-1}$ where $n = 5$.\n- Handle the computations via well-posed linear algebra on $\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}$, $\\mathbf{S}_{\\mathbf{Y}\\mathbf{Y}}$, and $\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}}$. Do not use any black-box canonical correlation routines.\n- Numerical output specification: For each test case $k \\in \\{1,2,3\\}$, let $c_{k,1}$ and $c_{k,2}$ denote the first and second canonical correlation, respectively. Round each $c_{k,j}$ to $6$ decimal places. Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets in the order $[c_{1,1}, c_{1,2}, c_{2,1}, c_{2,2}, c_{3,1}, c_{3,2}]$.\n- Angles are not involved, and there are no physical units. All numeric outputs must be decimal numbers rounded to exactly $6$ digits after the decimal point.\n\nTest suite coverage rationale:\n- Test case $1$ probes the regime where $\\mathbf{Y}$ is a full-rank linear transformation of $\\mathbf{X}$, which should yield the strongest possible linkages.\n- Test case $2$ probes the regime where the imaging and genomic subspaces are orthogonal with respect to the sample inner product, testing the boundary of no linear association.\n- Test case $3$ probes a mixed regime where only one dimension is shared between imaging and genomic spaces.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[c_{1,1},c_{1,2},c_{2,1},c_{2,2},c_{3,1},c_{3,2}]$), with each entry rounded to exactly $6$ decimal places.", "solution": "The user-provided problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Data matrices**: $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ and $\\mathbf{Y} \\in \\mathbb{R}^{n \\times q}$ are centered data matrices.\n- **Sample size**: $n = 5$.\n- **Imaging feature vectors**:\n$$\n\\mathbf{u} = \\begin{bmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ -2 \\end{bmatrix}\n$$\n- **Imaging feature matrix**: $\\mathbf{X} = \\begin{bmatrix} \\mathbf{u}  \\mathbf{v} \\end{bmatrix}$.\n- **Auxiliary vectors**:\n$$\n\\mathbf{w}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{w}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\n$$\n- **Gene-expression matrices (Test Cases)**:\n  1. $\\mathbf{A} = \\begin{bmatrix} 2  1 \\\\ -1  3 \\end{bmatrix}, \\quad \\mathbf{Y}_1 = \\mathbf{X}\\mathbf{A}$.\n  2. $\\mathbf{Y}_2 = \\begin{bmatrix} \\mathbf{w}_1  \\mathbf{w}_2 \\end{bmatrix}$.\n  3. $\\mathbf{Y}_3 = \\begin{bmatrix} \\mathbf{u}  \\mathbf{w}_1 \\end{bmatrix}$.\n- **Covariance definitions**: $\\mathbf{S}_{\\mathbf{X}\\mathbf{X}} = \\frac{1}{n-1}\\mathbf{X}^\\top \\mathbf{X}$, $\\mathbf{S}_{\\mathbf{Y}\\mathbf{Y}} = \\frac{1}{n-1}\\mathbf{Y}^\\top \\mathbf{Y}$, $\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}} = \\frac{1}{n-1}\\mathbf{X}^\\top \\mathbf{Y}$.\n- **CCA Objective**: Maximize $\\mathrm{Corr}(\\mathbf{X}\\mathbf{a}, \\mathbf{Y}\\mathbf{b})$ subject to $\\mathrm{Var}(\\mathbf{X}\\mathbf{a}) = 1$ and $\\mathrm{Var}(\\mathbf{Y}\\mathbf{b}) = 1$.\n- **Output Requirements**: For each case, compute the two canonical correlations, round to $6$ decimal places, and present them in a single comma-separated list: $[c_{1,1}, c_{1,2}, c_{2,1}, c_{2,2}, c_{3,1}, c_{3,2}]$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is a standard application of Canonical Correlation Analysis (CCA), a fundamental method in multivariate statistics. The specified context, radiogenomics, is a valid scientific field where such methods are employed. The mathematical formulation is correct.\n2.  **Well-Posedness**: The problem provides all necessary data and definitions. The given vectors are confirmed to be mean-centered. The matrices are constructed to be well-behaved (e.g., $\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}$ is invertible), ensuring a unique solution exists. The test cases systematically explore different regimes of linear association, which is indicative of a well-designed problem.\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution based on first principles will be derived and implemented.\n\n### Principle-Based Solution\nThe objective of Canonical Correlation Analysis (CCA) is to find weight vectors $\\mathbf{a}$ and $\\mathbf{b}$ that maximize the correlation $\\rho$ between the canonical variates $\\mathbf{U} = \\mathbf{X}\\mathbf{a}$ and $\\mathbf{V} = \\mathbf{Y}\\mathbf{b}$. The correlation is defined as:\n$$\n\\rho = \\frac{\\mathrm{Cov}(\\mathbf{X}\\mathbf{a}, \\mathbf{Y}\\mathbf{b})}{\\sqrt{\\mathrm{Var}(\\mathbf{X}\\mathbf{a}) \\mathrm{Var}(\\mathbf{Y}\\mathbf{b})}}\n$$\nGiven the data is centered and using the provided covariance definitions, we have:\n-   $\\mathrm{Var}(\\mathbf{X}\\mathbf{a}) = \\mathbf{a}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{X}} \\mathbf{a}$\n-   $\\mathrm{Var}(\\mathbf{Y}\\mathbf{b}) = \\mathbf{b}^\\top \\mathbf{S}_{\\mathbf{Y}\\mathbf{Y}} \\mathbf{b}$\n-   $\\mathrm{Cov}(\\mathbf{X}\\mathbf{a}, \\mathbf{Y}\\mathbf{b}) = \\mathbf{a}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}} \\mathbf{b}$\n\nThe optimization problem is to maximize $\\rho = \\mathbf{a}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}} \\mathbf{b}$ subject to the normalization constraints $\\mathbf{a}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{X}} \\mathbf{a} = 1$ and $\\mathbf{b}^\\top \\mathbf{S}_{\\mathbf{Y}\\mathbf{Y}} \\mathbf{b} = 1$. This problem can be solved using Lagrange multipliers, which transforms it into a generalized eigenvalue problem. The solution leads to two equivalent secular equations. We will use the form that solves for the weight vector $\\mathbf{b}$:\n$$\n(\\mathbf{S}_{\\mathbf{Y}\\mathbf{Y}}^{-1} \\mathbf{S}_{\\mathbf{Y}\\mathbf{X}} \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1} \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}}) \\mathbf{b} = \\rho^2 \\mathbf{b}\n$$\nwhere $\\mathbf{S}_{\\mathbf{Y}\\mathbf{X}} = \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}}^\\top$. This is a standard eigenvalue problem. The eigenvalues $\\lambda_i = \\rho_i^2$ are the squared canonical correlations. The canonical correlations $\\rho_i$ are the positive square roots of these eigenvalues. The corresponding eigenvectors $\\mathbf{b}_i$ are the canonical weight vectors for the $\\mathbf{Y}$ variates. The number of non-zero canonical correlations is at most $\\min(\\dim(\\mathbf{X}), \\dim(\\mathbf{Y}))$. Here, both $\\mathbf{X}$ and $\\mathbf{Y}$ have $p=2$ and $q=2$ columns, respectively, so we will find two canonical correlations for each case.\n\nThe computational procedure is as follows:\n1.  Construct the data matrices $\\mathbf{X}$ and $\\mathbf{Y}_k$ for each test case $k \\in \\{1, 2, 3\\}$.\n2.  Compute the sample covariance matrices $\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}$, $\\mathbf{S}_{\\mathbf{Y}_k\\mathbf{Y}_k}$, and $\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_k}$ using the factor $\\frac{1}{n-1} = \\frac{1}{4}$.\n3.  Compute the matrix $\\mathbf{K}_k = \\mathbf{S}_{\\mathbf{Y}_k\\mathbf{Y}_k}^{-1} \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_k}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1} \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_k}$.\n4.  Calculate the eigenvalues of $\\mathbf{K}_k$.\n5.  The canonical correlations are the square roots of these eigenvalues, sorted in descending order.\n\nFirst, we compute the common matrix $\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}$ and its inverse.\n$$\n\\mathbf{X}^\\top\\mathbf{X} = \\begin{bmatrix} \\mathbf{u}^\\top\\mathbf{u}  \\mathbf{u}^\\top\\mathbf{v} \\\\ \\mathbf{v}^\\top\\mathbf{u}  \\mathbf{v}^\\top\\mathbf{v} \\end{bmatrix} = \\begin{bmatrix} 10  -6 \\\\ -6  10 \\end{bmatrix}\n$$\n$$\n\\mathbf{S}_{\\mathbf{X}\\mathbf{X}} = \\frac{1}{4} \\begin{bmatrix} 10  -6 \\\\ -6  10 \\end{bmatrix} = \\begin{bmatrix} 2.5  -1.5 \\\\ -1.5  2.5 \\end{bmatrix}\n$$\n$$\n\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1} = \\frac{1}{2.5^2 - (-1.5)^2} \\begin{bmatrix} 2.5  1.5 \\\\ 1.5  2.5 \\end{bmatrix} = \\frac{1}{4} \\begin{bmatrix} 2.5  1.5 \\\\ 1.5  2.5 \\end{bmatrix} = \\begin{bmatrix} 0.625  0.375 \\\\ 0.375  0.625 \\end{bmatrix}\n$$\n\n**Test Case 1: $\\mathbf{Y}_1 = \\mathbf{X}\\mathbf{A}$**\nHere, $\\mathbf{Y}_1$ is a full-rank linear transformation of $\\mathbf{X}$. The relationship should be perfectly discoverable.\n$\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_1} = \\frac{1}{n-1}\\mathbf{X}^\\top(\\mathbf{X}\\mathbf{A}) = \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}\\mathbf{A}$.\n$\\mathbf{S}_{\\mathbf{Y}_1\\mathbf{Y}_1} = \\frac{1}{n-1}(\\mathbf{X}\\mathbf{A})^\\top(\\mathbf{X}\\mathbf{A}) = \\mathbf{A}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{X}} \\mathbf{A}$.\nThe matrix $\\mathbf{K}_1$ becomes:\n$$\n\\mathbf{K}_1 = (\\mathbf{A}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{X}} \\mathbf{A})^{-1} (\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}\\mathbf{A})^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1} (\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}\\mathbf{A}) = \\mathbf{A}^{-1}\\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1}(\\mathbf{A}^\\top)^{-1} \\mathbf{A}^\\top\\mathbf{S}_{\\mathbf{X}\\mathbf{X}} \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1} \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}\\mathbf{A} = \\mathbf{I}\n$$\nThe eigenvalues of the $2 \\times 2$ identity matrix $\\mathbf{I}$ are $1$ and $1$. Thus, the squared canonical correlations are $\\rho_1^2 = 1.0$ and $\\rho_2^2 = 1.0$. The canonical correlations are $c_{1,1} = 1.0$ and $c_{1,2} = 1.0$.\n\n**Test Case 2: $\\mathbf{Y}_2 = [\\mathbf{w}_1, \\mathbf{w}_2]$**\nThe vectors $\\mathbf{w}_1$ and $\\mathbf{w}_2$ are constructed to be orthogonal to the column space of $\\mathbf{X}$.\n$\\mathbf{u}^\\top\\mathbf{w}_1 = 0$, $\\mathbf{v}^\\top\\mathbf{w}_1 = 0$, $\\mathbf{u}^\\top\\mathbf{w}_2 = 0$, $\\mathbf{v}^\\top\\mathbf{w}_2 = 0$.\nThis implies $\\mathbf{X}^\\top\\mathbf{w}_1 = \\mathbf{0}$ and $\\mathbf{X}^\\top\\mathbf{w}_2 = \\mathbf{0}$.\nTherefore, the cross-covariance matrix is null:\n$$\n\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_2} = \\frac{1}{n-1}\\mathbf{X}^\\top\\mathbf{Y}_2 = \\frac{1}{4} [\\mathbf{X}^\\top\\mathbf{w}_1, \\mathbf{X}^\\top\\mathbf{w}_2] = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}\n$$\nConsequently, the matrix $\\mathbf{K}_2$ is also the null matrix:\n$$\n\\mathbf{K}_2 = \\mathbf{S}_{\\mathbf{Y}_2\\mathbf{Y}_2}^{-1} \\mathbf{0}^\\top \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1} \\mathbf{0} = \\mathbf{0}\n$$\nThe eigenvalues of the zero matrix are $0$ and $0$. The canonical correlations are $c_{2,1} = 0.0$ and $c_{2,2} = 0.0$.\n\n**Test Case 3: $\\mathbf{Y}_3 = [\\mathbf{u}, \\mathbf{w}_1]$**\nThis is a mixed case. One dimension of $\\mathbf{Y}_3$ is in the space of $\\mathbf{X}$ and the other is orthogonal.\n$$\n\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_3} = \\frac{1}{4} \\mathbf{X}^\\top [\\mathbf{u}, \\mathbf{w}_1] = \\frac{1}{4} \\begin{bmatrix} \\mathbf{u}^\\top\\mathbf{u}  \\mathbf{u}^\\top\\mathbf{w}_1 \\\\ \\mathbf{v}^\\top\\mathbf{u}  \\mathbf{v}^\\top\\mathbf{w}_1 \\end{bmatrix} = \\frac{1}{4} \\begin{bmatrix} 10  0 \\\\ -6  0 \\end{bmatrix} = \\begin{bmatrix} 2.5  0 \\\\ -1.5  0 \\end{bmatrix}\n$$\n$$\n\\mathbf{S}_{\\mathbf{Y}_3\\mathbf{Y}_3} = \\frac{1}{4} \\begin{bmatrix} \\mathbf{u}^\\top\\mathbf{u}  \\mathbf{u}^\\top\\mathbf{w}_1 \\\\ \\mathbf{w}_1^\\top\\mathbf{u}  \\mathbf{w}_1^\\top\\mathbf{w}_1 \\end{bmatrix} = \\frac{1}{4} \\begin{bmatrix} 10  0 \\\\ 0  6 \\end{bmatrix} = \\begin{bmatrix} 2.5  0 \\\\ 0  1.5 \\end{bmatrix}\n$$\n$$\n\\mathbf{S}_{\\mathbf{Y}_3\\mathbf{Y}_3}^{-1} = \\begin{bmatrix} 1/2.5  0 \\\\ 0  1/1.5 \\end{bmatrix} = \\begin{bmatrix} 0.4  0 \\\\ 0  2/3 \\end{bmatrix}\n$$\nWe compute the intermediate product $\\mathbf{M} = \\mathbf{S}_{\\mathbf{X}\\mathbf{X}}^{-1} \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_3}$:\n$$\n\\mathbf{M} = \\begin{bmatrix} 0.625  0.375 \\\\ 0.375  0.625 \\end{bmatrix} \\begin{bmatrix} 2.5  0 \\\\ -1.5  0 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  0 \\end{bmatrix}\n$$\nNow we compute $\\mathbf{K}_3 = \\mathbf{S}_{\\mathbf{Y}_3\\mathbf{Y}_3}^{-1} \\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_3}^\\top \\mathbf{M}$:\n$$\n\\mathbf{S}_{\\mathbf{X}\\mathbf{Y}_3}^\\top \\mathbf{M} = \\begin{bmatrix} 2.5  -1.5 \\\\ 0  0 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 2.5  0 \\\\ 0  0 \\end{bmatrix}\n$$\n$$\n\\mathbf{K}_3 = \\begin{bmatrix} 0.4  0 \\\\ 0  2/3 \\end{bmatrix} \\begin{bmatrix} 2.5  0 \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  0 \\end{bmatrix}\n$$\nThe eigenvalues of $\\mathbf{K}_3$ are $1$ and $0$. The canonical correlations are $c_{3,1} = \\sqrt{1} = 1.0$ and $c_{3,2} = \\sqrt{0} = 0.0$.\n\n**Summary of Results**:\n-   Case 1 correlations: $1.0, 1.0$\n-   Case 2 correlations: $0.0, 0.0$\n-   Case 3 correlations: $1.0, 0.0$\nThe results will be ordered as requested in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes canonical correlations for a toy radiogenomics scenario.\n\n    The function implements Canonical Correlation Analysis (CCA) from first principles\n    to find the linear relationships between two sets of variables, X (imaging features)\n    and Y (gene expression levels), across three distinct test cases.\n    \"\"\"\n\n    # Define shared data matrices and constants as specified in the problem.\n    n = 5.0\n    u = np.array([-2, -1, 0, 1, 2], dtype=float).reshape(-1, 1)\n    v = np.array([2, -1, 0, 1, -2], dtype=float).reshape(-1, 1)\n    X = np.hstack([u, v])\n\n    w1 = np.array([1, 0, -2, 0, 1], dtype=float).reshape(-1, 1)\n    w2 = np.array([0, 1, -2, 1, 0], dtype=float).reshape(-1, 1)\n\n    # Test Case 1: Perfect linear linkage\n    # Y1 is a full-rank linear transformation of X.\n    A = np.array([[2, 1], [-1, 3]], dtype=float)\n    Y1 = X @ A\n\n    # Test Case 2: No linear linkage\n    # The column space of Y2 is orthogonal to the column space of X.\n    Y2 = np.hstack([w1, w2])\n\n    # Test Case 3: One linked and one unlinked dimension\n    # One column of Y3 is a column of X, the other is orthogonal to X's space.\n    Y3 = np.hstack([u, w1])\n\n    # Store test cases in a list for iterative processing.\n    test_cases = [\n        {'X': X, 'Y': Y1},\n        {'X': X, 'Y': Y2},\n        {'X': X, 'Y': Y3},\n    ]\n\n    all_correlations = []\n\n    for case in test_cases:\n        X_case = case['X']\n        Y_case = case['Y']\n        \n        # Step 1: Compute sample covariance matrices (with n-1 denominator).\n        # Data is already centered as per problem statement.\n        S_XX = (X_case.T @ X_case) / (n - 1)\n        S_YY = (Y_case.T @ Y_case) / (n - 1)\n        S_XY = (X_case.T @ Y_case) / (n - 1)\n        \n        # Step 2: Compute matrix inverses required for the eigenvalue problem.\n        # The problem is constructed such that these matrices are invertible.\n        try:\n            S_XX_inv = np.linalg.inv(S_XX)\n            S_YY_inv = np.linalg.inv(S_YY)\n        except np.linalg.LinAlgError:\n            # This path should not be taken in this problem, but is good practice.\n            # Handle singular covariance matrices if they occur, e.g., by using pinv\n            # or by recognizing that correlations may be undefined or trivial.\n            # For this problem, if S_XX or S_YY is singular, there are no non-trivial\n            # correlations to find. Assume correlations are 0.\n            all_correlations.extend([0.0, 0.0])\n            continue\n            \n        # Step 3: Form the matrix K for the eigenvalue problem.\n        # K = inv(S_YY) * S_YX * inv(S_XX) * S_XY, where S_YX = S_XY.T\n        S_YX = S_XY.T\n        K = S_YY_inv @ S_YX @ S_XX_inv @ S_XY\n        \n        # Step 4: Compute the eigenvalues of K.\n        # These are the squared canonical correlations.\n        eigenvalues = np.linalg.eigvals(K)\n        \n        # Step 5: Calculate the canonical correlations.\n        # Take the square root of the eigenvalues.\n        # Use np.maximum with 0 to handle potential small negative values from\n        # floating-point inaccuracies for eigenvalues that should be zero.\n        squared_correlations = np.maximum(eigenvalues.real, 0)\n        correlations = np.sqrt(squared_correlations)\n        \n        # Step 6: Sort correlations in descending order to get the leading ones.\n        correlations.sort()\n        sorted_correlations = correlations[::-1]\n        \n        all_correlations.extend(sorted_correlations)\n        \n    # Format the results to 6 decimal places and print in the specified format.\n    formatted_results = [f\"{c:.6f}\" for c in all_correlations]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4557615"}, {"introduction": "When testing thousands of feature-gene associations, the probability of finding spurious correlations by chance alone becomes a major concern. This practice addresses the critical issue of multiple hypothesis testing by guiding you through an implementation of the Benjamini-Hochberg procedure to control the False Discovery Rate (FDR). Mastering this technique is essential for separating statistically significant findings from the background noise in high-throughput radiogenomic analyses [@problem_id:4557666].", "problem": "Radiogenomics associates quantitative image features with genomic variables by performing many statistical hypothesis tests, one per feature–gene pair. In classical frequentist hypothesis testing, a $p$-value is the probability, under the null hypothesis, of observing a test statistic at least as extreme as the observed one. Under the null hypothesis and appropriate regularity conditions, $p$-values are distributed uniformly on $[0,1]$. When many tests are conducted, controlling the expected proportion of false positives among the declared positives is crucial. The False Discovery Rate (FDR) is defined as the expected value of the ratio of the number of false positives to the total number of positives, with the convention that the ratio is $0$ if there are no positives. The Benjamini–Hochberg (BH) procedure controls the FDR at a target level $ \\alpha $ under independence or certain forms of positive dependence across tests.\n\nYour task is to implement, from first principles, the BH procedure to compute adjusted $q$-values from a given list of $p$-values. The adjusted $q$-value for a test is the minimal FDR level at which that test would be called significant by the BH procedure. You must:\n- Accept a list of $m$ $p$-values (where $m$ is the total number of tests), compute the BH-adjusted $q$-values aligned with the original order of tests.\n- Identify the set of discoveries at a target FDR level $ \\alpha $ by comparing the computed $q$-values to $ \\alpha $. Use the unrounded $q$-values for this decision.\n- Round each reported $q$-value to $6$ decimal places for output. There are no physical units involved in this problem.\n- Return, for each test case, a pair consisting of the list of rounded $q$-values (aligned to the original $p$-value order) and the list of indices (using $0$-based indexing) of discoveries where the unrounded $q$-value is less than or equal to $ \\alpha $. Indices must be sorted in ascending order.\n\nDesign your implementation to start from the fundamental definitions above, and reason about ordering and monotonicity required by the BH algorithm. Avoid using any shortcut formulas not derived from the base principles. Ensure scientific realism by treating all $p$-values as elements of $[0,1]$ and reflecting the independence assumption in your algorithmic design.\n\nTest Suite:\nFor each test case, the inputs are the list of $p$-values and the target FDR level $ \\alpha $. Your program must process the following five test cases:\n\n- Case $1$: $m = 10$, $p$-values $[0.001, 0.04, 0.2, 0.5, 0.03, 0.8, 0.07, 0.002, 0.9, 0.15]$, target $ \\alpha = 0.1 $.\n- Case $2$: $m = 8$, $p$-values $[0.05, 0.05, 0.05, 0.05, 0.2, 0.2, 0.9, 0.9]$, target $ \\alpha = 0.05 $.\n- Case $3$: $m = 6$, $p$-values $[0.0, 0.0, 0.01, 0.05, 0.5, 1.0]$, target $ \\alpha = 0.0 $.\n- Case $4$: $m = 5$, $p$-values $[0.6, 0.7, 0.8, 0.9, 1.0]$, target $ \\alpha = 0.5 $.\n- Case $5$: $m = 12$, $p$-values $[0.12, 0.034, 0.0005, 0.48, 0.051, 0.33, 0.42, 0.0021, 0.77, 0.090, 0.16, 0.25]$, target $ \\alpha = 0.05 $.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list: the first element is the list of rounded $q$-values, and the second element is the list of discovery indices. For example, the format must be\n$[\\,[[q\\_1,q\\_2,\\dots,q\\_m],[i\\_1,i\\_2,\\dots]],\\dots\\,]$,\nwith numeric values and indices as specified above.", "solution": "The user-provided problem has been validated and is determined to be sound. It is scientifically grounded in established statistical theory, well-posed with a clear objective and sufficient data, and free of contradictions or ambiguities. The task is to implement the Benjamini-Hochberg (BH) procedure for controlling the False Discovery Rate (FDR) from first principles.\n\n### Principle-Based Derivation and Algorithmic Design\n\nThe core task is to compute adjusted $p$-values, known as $q$-values, for a list of $m$ hypothesis tests, and then identify which tests are significant at a given FDR control level $\\alpha$.\n\n#### 1. Fundamental Definitions\n\nLet there be $m$ null hypotheses, $H_1, H_2, \\dots, H_m$, with corresponding $p$-values, $P = [p_1, p_2, \\dots, p_m]$.\n\nThe Benjamini-Hochberg (BH) procedure for controlling the FDR at a level $\\alpha \\in [0, 1]$ is as follows:\n1.  Sort the $p$-values in non-decreasing order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. Let $H_{(i)}$ be the null hypothesis corresponding to the $p$-value $p_{(i)}$.\n2.  Find the largest integer rank $k$ (where $k \\in \\{1, 2, \\dots, m\\}$) such that the corresponding sorted $p$-value, $p_{(k)}$, satisfies the condition:\n    $$p_{(k)} \\le \\frac{k}{m}\\alpha$$\n3.  If such a $k$ exists, reject all null hypotheses $H_{(i)}$ for $i = 1, 2, \\dots, k$. If no such $k$ exists, reject no hypotheses.\n\n#### 2. Derivation of the Adjusted $q$-value\n\nThe problem defines the adjusted $q$-value for a specific test as the minimal FDR level $\\alpha$ at which that test would be declared significant. Let us derive the formula for the $q$-value corresponding to the $i$-th sorted $p$-value, $p_{(i)}$.\n\nFor the test corresponding to $p_{(i)}$ to be declared significant, its hypothesis $H_{(i)}$ must be rejected. According to the BH procedure, this occurs if there exists some rank $k \\ge i$ for which the condition $p_{(k)} \\le \\frac{k}{m}\\alpha$ is met. This inequality can be rearranged to solve for $\\alpha$:\n$$\\alpha \\ge \\frac{m \\cdot p_{(k)}}{k}$$\nFor a fixed test $H_{(i)}$, we are looking for the *minimal* $\\alpha$ that will lead to its rejection. This requires finding an $\\alpha$ that satisfies the inequality for at least one $k \\ge i$. The smallest such $\\alpha$ is therefore the minimum of all possible right-hand side values for all ranks from $i$ to $m$.\n\nThus, the adjusted $q$-value for the $i$-th ordered test, denoted $q_{(i)}$, is:\n$$q_{(i)} = \\min_{k=i, \\dots, m} \\left( \\frac{m \\cdot p_{(k)}}{k} \\right)$$\nAs a direct consequence of this definition, the sequence of sorted $q$-values must be non-decreasing, i.e., $q_{(1)} \\le q_{(2)} \\le \\dots \\le q_{(m)}$. This is because the set of terms over which the minimum is taken for $q_{(i)}$ is a superset of the terms for $q_{(i+1)}$, so its minimum must be less than or equal to that of $q_{(i+1)}$. This property can be expressed as a recurrence relation:\n$$q_{(i)} = \\min\\left( \\frac{m \\cdot p_{(i)}}{i}, q_{(i+1)} \\right)$$\n\n#### 3. Algorithmic Implementation\n\nThe recurrence relation provides an efficient method for computation. We can compute the $q$-values by starting with the largest $p$-value and proceeding backwards.\n\nThe complete algorithm is as follows:\n1.  **Preparation**: Given the input list of $m$ $p$-values, we must retain their original indices to reconstruct the final output. We create a list of tuples, `(p_value, original_index)`, for all $m$ tests.\n2.  **Sorting**: Sort this list of tuples in ascending order based on the $p$-values. This yields the sorted $p$-values $p_{(1)}, p_{(2)}, \\dots, p_{(m)}$ and their corresponding original indices. The rank of $p_{(i)}$ is $i$ (using $1$-based indexing for the formula).\n3.  **Backward Calculation of $q$-values**: We compute the sorted $q$-values, $q_{(1)}, \\dots, q_{(m)}$, in reverse order of rank to enforce the monotonicity property efficiently.\n    -   The $q$-value for the largest $p$-value, $p_{(m)}$, is simply $p_{(m)}$ itself, which corresponds to the case $k=m$ in the definition:\n        $$q_{(m)} = \\frac{m \\cdot p_{(m)}}{m} = p_{(m)}$$\n    -   For $i$ from $m-1$ down to $1$, we apply the recurrence:\n        $$q_{(i)} = \\min\\left( \\frac{m \\cdot p_{(i)}}{i}, q_{(i+1)} \\right)$$\n    -   Additionally, since $q$-values represent FDR levels, they should not exceed $1$. While the source $p$-values are in $[0, 1]$, the term $\\frac{m \\cdot p_{(i)}}{i}$ can be greater than $1$. The standard BH adjustment caps the resulting q-values at $1$. Our recurrence relation naturally handles this if $p_{(m)} \\le 1$ and subsequent $\\frac{m \\cdot p_{(i)}}{i}$ values are also capped at $1$ before the `min` operation, or if the final list is capped. The latter is simpler: $q_{(i)} \\leftarrow \\min(1.0, q_{(i)})$. The provided recurrence automatically propagates values less than or equal to $1$ correctly. For example if $q_{(i+1)} \\le 1$ and $\\frac{m \\cdot p_{(i)}}{i}  1$, the minimum will be $q_{(i+1)}$, preserving the property.\n4.  **Unsorting**: A new list is created to hold the final $q$-values. Using the stored original indices from the sorting step, we place each computed $q_{(i)}$ back into its original position.\n5.  **Identifying Discoveries**: With the final (unrounded) $q$-values in their original order, we iterate through them. For each test $j$, if its $q$-value, $q_j$, satisfies $q_j \\le \\alpha$, its original index $j$ is added to a list of discoveries. This list must then be sorted in ascending order as required.\n6.  **Final Formatting**: The computed $q$-values are rounded to $6$ decimal places. The final result for each test case is a pair containing the list of rounded $q$-values and the sorted list of discovery indices.\n\nThis step-by-step process, derived from first principles, ensures a correct and robust implementation of the Benjamini-Hochberg adjusted $q$-value computation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Benjamini-Hochberg q-value problem for the given test suite.\n    \"\"\"\n\n    test_cases = [\n        {'p_values': [0.001, 0.04, 0.2, 0.5, 0.03, 0.8, 0.07, 0.002, 0.9, 0.15], 'alpha': 0.1},\n        {'p_values': [0.05, 0.05, 0.05, 0.05, 0.2, 0.2, 0.9, 0.9], 'alpha': 0.05},\n        {'p_values': [0.0, 0.0, 0.01, 0.05, 0.5, 1.0], 'alpha': 0.0},\n        {'p_values': [0.6, 0.7, 0.8, 0.9, 1.0], 'alpha': 0.5},\n        {'p_values': [0.12, 0.034, 0.0005, 0.48, 0.051, 0.33, 0.42, 0.0021, 0.77, 0.090, 0.16, 0.25], 'alpha': 0.05},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p_values = np.array(case['p_values'])\n        alpha = case['alpha']\n        m = len(p_values)\n\n        # Step 1  2: Get the sorting order of p-values\n        # argsort provides the indices that would sort the array.\n        # This preserves the original index information implicitly.\n        sorted_indices = np.argsort(p_values)\n        \n        # Create a sorted version of p-values\n        p_values_sorted = p_values[sorted_indices]\n\n        # Step 3: Backward calculation of q-values\n        # Create an array to hold the sorted q-values.\n        q_values_sorted = np.zeros(m)\n        \n        # The q-value for the largest p-value is the p-value itself.\n        if m > 0:\n            q_values_sorted[m - 1] = p_values_sorted[m - 1]\n\n        # Iterate backwards from the second to last p-value.\n        # i is the 0-based index in the sorted array.\n        for i in range(m - 2, -1, -1):\n            # Rank is (i + 1) for 1-based indexing.\n            rank = i + 1\n            \n            # Calculate the BH comparison value for the current p-value.\n            bh_p_val = (m / rank) * p_values_sorted[i]\n            \n            # The q-value is the minimum of its BH value and the next q-value in the sorted list.\n            # This enforces the non-decreasing (monotonicity) property of q-values.\n            q_values_sorted[i] = min(bh_p_val, q_values_sorted[i + 1])\n        \n        # Ensure q-values are capped at 1.0.\n        q_values_sorted = np.minimum(q_values_sorted, 1.0)\n        \n        # Step 4: Unsort the q-values to match the original p-value order.\n        # Create an empty array for the final q-values.\n        q_values_unrounded = np.zeros(m)\n        # Place the sorted q-values back into their original positions.\n        # This is a powerful numpy feature for reordering.\n        if m > 0:\n            q_values_unrounded[sorted_indices] = q_values_sorted\n\n        # Step 5: Identify discoveries\n        # Compare the unrounded q-values to the alpha level.\n        discoveries = [i for i, q in enumerate(q_values_unrounded) if q = alpha]\n        # Sort indices as required by the problem statement.\n        discoveries.sort()\n\n        # Step 6: Round q-values for the final report\n        q_values_rounded = [round(q, 6) for q in q_values_unrounded]\n\n        all_results.append([q_values_rounded, discoveries])\n\n    # Format the final output string exactly as specified.\n    case_strings = []\n    for res in all_results:\n        q_vals_str = f\"[{','.join(map(str, res[0]))}]\"\n        indices_str = f\"[{','.join(map(str, res[1]))}]\"\n        case_strings.append(f\"[{q_vals_str},{indices_str}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "4557666"}]}