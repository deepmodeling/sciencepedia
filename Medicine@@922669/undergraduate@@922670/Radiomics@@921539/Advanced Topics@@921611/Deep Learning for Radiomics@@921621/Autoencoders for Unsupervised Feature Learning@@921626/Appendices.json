{"hands_on_practices": [{"introduction": "To truly grasp the power of autoencoders for feature learning, we must first confront their primary challenge: the risk of learning a trivial identity mapping. An autoencoder with sufficient capacity can simply learn to copy its input to its output, perfectly minimizing reconstruction error but failing to extract any meaningful underlying structure. This exercise explores two fundamental regularization strategies, weight decay ($R_{W}=\\lambda\\|W\\|_{2}^{2}$) and latent activity regularization ($R_{z}=\\gamma\\|z\\|_{1}$), that prevent this failure mode. By comparing their mechanisms, you will develop a deeper intuition for how to guide an autoencoder to discover compact and useful radiomic representations. [@problem_id:4530388]", "problem": "A radiomics group trains a three-dimensional ($3$D) convolutional autoencoder on unlabeled Computed Tomography (CT) patches to learn unsupervised features that will later be used for tumor stratification. Let the encoder be $f_{\\theta}:\\mathbb{R}^{H\\times W\\times D}\\rightarrow \\mathbb{R}^{m}$, producing latent code $z=f_{\\theta}(x)$ for input patch $x\\in\\mathbb{R}^{H\\times W\\times D}$, and the decoder be $g_{\\phi}:\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{H\\times W\\times D}$, producing reconstruction $\\hat{x}=g_{\\phi}(z)$. The reconstruction loss is the Mean Squared Error (MSE), $L_{\\text{rec}}(x,\\hat{x})=\\|x-\\hat{x}\\|_{2}^{2}$. The model is intentionally high capacity: it uses many channels per layer and $m$ is not smaller than the number of input voxels in a patch, so a near-identity mapping is representable.\n\nThey compare two regularization strategies applied during training:\n- Weight decay on all convolutional kernels, $R_{W}=\\lambda\\|W\\|_{2}^{2}$, where $W$ stacks all trainable weights and $\\lambda>0$.\n- Activity regularization on the latent code, $R_{z}=\\gamma\\|z\\|_{1}$, where $\\gamma>0$ is applied per-sample to the encoder output.\n\nAssume the data contain scanner and physiological noise that can be idealized as additive $0$-mean fluctuations at the voxel level, and that skip connections are not used. From core definitions, consider how each regularizer interacts with (i) the ease of representing an identity-like mapping under $3$D convolution, and (ii) the effective information flow from $x$ to $z$. Which statement best explains which regularizer more directly prevents the trivial identity solution and encourages compact radiomic representations in this high-capacity setting?\n\nA. Weight decay is more effective, because the identity mapping in a $3$D convolution necessarily uses large-magnitude delta-like kernels whose $\\ell_{2}$ norm grows with depth, so $R_{W}$ strongly penalizes identity and blocks information flow.\n\nB. Activity $\\ell_{1}$ regularization on $z$ is more effective, because it directly imposes sparsity on $z$, reducing the information that can pass from $x$ to $z$ per sample; an exact identity would require transmitting nearly all voxel degrees of freedom, which is incompatible with a strong $\\|z\\|_{1}$ penalty.\n\nC. They are essentially equivalent at preventing identity mappings, since shrinking $W$ or shrinking $z$ both reduce MSE reconstruction capacity by the same mechanism of scaling down activations layerwise.\n\nD. Weight decay is preferable in convolutional autoencoders, because $\\|W\\|_{2}^{2}$ penalizes parameter count and thus enforces a narrow effective bottleneck even when $m$ is large, whereas $\\|z\\|_{1}$ primarily risks dead units without limiting the transmitted information.", "solution": "The user wants me to analyze a problem concerning regularization methods for a $3$D convolutional autoencoder in the context of radiomics. I must first validate the problem statement and then, if it is valid, provide a detailed solution and evaluate the given options.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified procedure.\n\n**Step 1: Extract Givens**\n- **Model:** A three-dimensional ($3$D) convolutional autoencoder.\n- **Application:** Trained on unlabeled Computed Tomography (CT) patches for unsupervised feature learning, to be used for tumor stratification.\n- **Encoder:** $f_{\\theta}:\\mathbb{R}^{H\\times W\\times D}\\rightarrow \\mathbb{R}^{m}$, where $z=f_{\\theta}(x)$ is the latent code for an input patch $x\\in\\mathbb{R}^{H\\times W\\times D}$.\n- **Decoder:** $g_{\\phi}:\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{H\\times W\\times D}$, where $\\hat{x}=g_{\\phi}(z)$ is the reconstruction.\n- **Loss Function:** The reconstruction loss is the Mean Squared Error (MSE), $L_{\\text{rec}}(x,\\hat{x})=\\|x-\\hat{x}\\|_{2}^{2}$.\n- **Model Capacity:** The model is \"high capacity,\" with many channels per layer. The latent dimension $m$ is not smaller than the number of input voxels, i.e., $m \\ge H \\times W \\times D$. This implies a near-identity mapping is representable.\n- **Regularization Strategy 1:** Weight decay on all convolutional kernels, $R_{W}=\\lambda\\|W\\|_{2}^{2}$, with $\\lambda>0$. $W$ represents all trainable weights.\n- **Regularization Strategy 2:** Activity regularization on the latent code, $R_{z}=\\gamma\\|z\\|_{1}$, with $\\gamma>0$, applied per-sample.\n- **Data Characteristics:** The data contains additive $0$-mean fluctuations (noise) at the voxel level.\n- **Architectural Constraint:** No skip connections are used.\n- **Question:** The problem asks which statement best explains which regularizer ($R_{W}$ or $R_{z}$) more directly prevents the trivial identity solution and encourages compact radiomic representations in this specific high-capacity setting.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Groundedness:** The problem is grounded in the established field of deep learning, specifically its application to medical imaging (radiomics). Convolutional autoencoders, overcomplete representations ($m \\ge H \\times W \\times D$), Mean Squared Error loss, weight decay ($\\ell_2$ regularization on parameters), and activity regularization ($\\ell_1$ regularization on activations) are all standard and well-defined concepts. The setup describes a common challenge in unsupervised representation learning. The problem is scientifically sound.\n- **Well-Posedness:** The question asks for a qualitative comparison of the *mechanisms* of two different regularizers in a specific, well-defined scenario (preventing a trivial solution in a high-capacity autoencoder). The terms \"trivial identity solution\" and \"compact representations\" have clear meanings in this context. The problem is structured to elicit an understanding of how these regularizers function, which allows for a unique and meaningful conceptual answer. The problem is well-posed.\n- **Objectivity:** The problem statement is expressed in precise, technical language common to machine learning and computer vision. There are no subjective or ambiguous terms. The premises are objective facts about the model and training setup.\n- **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The setup is a canonical example of an overcomplete autoencoder.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is directly relevant to the topic and the concepts are all formalizable.\n    3.  **Incomplete/Contradictory Setup:** None. All necessary components are defined: the model architecture type, the loss, the capacity condition, and the regularizers. The condition $m \\ge H \\times W \\times D$ is critical and properly stated.\n    4.  **Unrealistic/Infeasible:** None. This setup is frequently used in research to learn features from imaging data.\n    5.  **Ill-Posed/Poorly Structured:** None. The question is clear and directs the analysis towards the directness of the mechanism and the goal of compactness.\n    6.  **Pseudo-Profound/Trivial/Tautological:** None. The distinction between parameter regularization and activity regularization is a fundamental and non-trivial concept in neural networks.\n    7.  **Outside Scientific Verifiability:** None. The effects of these regularizers are empirically and theoretically verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed to derive the solution.\n\n### Solution Derivation\n\nThe core of the problem is to compare two regularization strategies, weight decay ($R_W$) and latent code sparsity ($R_z$), in the context of an overcomplete autoencoder. An overcomplete autoencoder is one where the latent dimension $m$ is greater than or equal to the input dimension ($H \\times W \\times D$). Such models are \"high capacity\" and can easily learn a trivial identity mapping, $g_{\\phi}(f_{\\theta}(x)) \\approx x$, by simply copying the input information into the latent space and then back out. This perfectly minimizes the reconstruction loss $L_{\\text{rec}}$ but results in a useless representation $z$ that has not captured any underlying structure of the data. The role of regularization here is to prevent this trivial solution and force the model to learn a \"compact\" or meaningful representation. The data also contains noise, which a trivial identity map would learn to reconstruct, whereas a good model should learn to denoise the input by capturing the signal and ignoring the noise.\n\nLet's analyze the mechanism of each regularizer.\n\n**1. Activity Regularization ($R_{z}=\\gamma\\|z\\|_{1}$):**\nThis regularizer applies a penalty to the $\\ell_1$-norm of the latent code $z$ for each sample. The $\\ell_1$-norm is defined as $\\|z\\|_{1} = \\sum_{i=1}^{m} |z_i|$. It is well-known from optimization theory (e.g., in LASSO regression) that penalizing the $\\ell_1$-norm of a vector encourages sparsity, meaning many of its components will be driven to be exactly zero.\n\n- **Mechanism:** The total loss function is $L = L_{\\text{rec}} + R_z = \\|x - \\hat{x}\\|_{2}^{2} + \\gamma\\|z\\|_{1}$. The optimizer must now balance two competing objectives: accurate reconstruction (low $L_{\\text{rec}}$) and a sparse latent code (low $R_z$).\n- **Effect on Identity Mapping:** A trivial identity mapping requires the latent code $z$ to carry all the information present in the input patch $x$. In an overcomplete setting, there are many ways to do this, but they generally involve a dense (non-sparse) $z$. For $z$ to encode all the fine details and noise of $x$, many of its $m$ components must be non-zero. A dense $z$ will incur a large $\\|z\\|_{1}$ penalty. Therefore, the $R_z$ term directly counteracts the tendency to form a trivial identity mapping by making it costly to use many latent units.\n- **Effect on Representation:** By forcing $z$ to be sparse, the model is compelled to learn a more efficient code. It must select a small subset of its \"basis functions\" (represented by the decoder) to reconstruct the input. This encourages the latent features to correspond to meaningful, recurring patterns in the data, leading to a \"compact\" and often disentangled representation. This is the core principle of sparse coding.\n\n**2. Weight Decay ($R_{W}=\\lambda\\|W\\|_{2}^{2}$):**\nThis regularizer applies a penalty to the squared $\\ell_2$-norm of the model's trainable weights, $W$. The $\\ell_2$-norm is $\\|W\\|_{2} = (\\sum w_{i}^{2})^{1/2}$. This encourages the learning algorithm to find solutions with smaller weight magnitudes.\n\n- **Mechanism:** The total loss function is $L = L_{\\text{rec}} + R_W = \\|x - \\hat{x}\\|_{2}^{2} + \\lambda\\|W\\|_{2}^{2}$. The regularizer acts on the parameters of the functions $f_\\theta$ and $g_\\phi$, not directly on the representation $z$. Its effect on $z$ is indirect.\n- **Effect on Identity Mapping:** The question is whether learning an identity map requires large weights, which would be penalized by $R_W$. In a deep convolutional network, an identity-like transformation can be approximated. For a single layer, a centered delta kernel (e.g., a $3 \\times 3 \\times 3$ kernel with a $1$ at the center and $0$s elsewhere) performs an identity-like operation for a single channel. The squared $\\ell_2$ norm of such a kernel is $1^2 = 1$, which is a very small value. Even in a deep network, it is not a given that an identity-like mapping requires large weights. The model could learn a complex function that approximates identity with a combination of moderate weights. Therefore, $R_W$ does not necessarily or strongly penalize the identity solution. Its main effect is to promote \"smoother\" mappings from input to output, which can help with generalization and might incidentally discourage the fitting of high-frequency noise.\n- **Effect on Representation:** By encouraging smaller weights, weight decay limits the effective complexity of the learned functions $f_\\theta$ and $g_\\phi$. This can prevent overfitting to the noise in $x$. However, it does not directly impose a structural constraint like sparsity on the latent code $z$. The representation $z$ might become smaller in magnitude as a side effect of smaller weights, but it is not forced to be sparse or compact in the same way as with an $\\ell_1$ penalty. The effect is less direct.\n\n**Comparison and Conclusion:**\nThe activity regularization $R_z = \\gamma\\|z\\|_{1}$ acts *directly* on the information channel, the latent code $z$. It directly enforces a sparsity constraint, which is fundamentally incompatible with a trivial identity mapping that needs to pass all information from $x$. This directly promotes a compact, parts-based representation.\nWeight decay $R_W = \\lambda\\|W\\|_{2}^{2}$ acts *indirectly* by regularizing the function space (the model parameters). While it can help prevent overfitting, it does not provide a strong or direct pressure against the identity mapping, nor does it explicitly enforce a compact structure like sparsity on the latent code.\n\nTherefore, activity $\\ell_1$ regularization is the more direct and effective method for the stated goals in this high-capacity setting.\n\n### Option-by-Option Analysis\n\n**A. Weight decay is more effective, because the identity mapping in a $3$D convolution necessarily uses large-magnitude delta-like kernels whose $\\ell_{2}$ norm grows with depth, so $R_{W}$ strongly penalizes identity and blocks information flow.**\n- **Evaluation:** This statement is based on a false premise. A \"delta-like kernel,\" which is a good way for a convolutional layer to approximate an identity mapping on a channel, has a very *small* $\\ell_{2}$ norm. For a kernel with a single entry of $1$ and all others $0$, its squared $\\ell_2$ norm is $1$. This is not a \"large-magnitude\" kernel. The claim that its norm \"grows with depth\" is also unsubstantiated and not generally true for identity mappings. Thus, the reasoning that $R_W$ strongly penalizes identity is flawed.\n- **Verdict:** **Incorrect**.\n\n**B. Activity $\\ell_{1}$ regularization on $z$ is more effective, because it directly imposes sparsity on $z$, reducing the information that can pass from $x$ to $z$ per sample; an exact identity would require transmitting nearly all voxel degrees of freedom, which is incompatible with a strong $\\|z\\|_{1}$ penalty.**\n- **Evaluation:** This statement accurately describes the mechanism of $\\ell_1$ activity regularization. It correctly identifies that the regularization is applied *directly* to the latent code $z$ to enforce sparsity. It correctly reasons that a trivial identity map requires a dense, information-rich latent code, which would incur a high $\\ell_1$ penalty. This conflict between minimizing reconstruction error and minimizing the $\\ell_1$ penalty on $z$ is what forces the model to learn a compact, efficient representation. The entire statement is logically and technically sound.\n- **Verdict:** **Correct**.\n\n**C. They are essentially equivalent at preventing identity mappings, since shrinking $W$ or shrinking $z$ both reduce MSE reconstruction capacity by the same mechanism of scaling down activations layerwise.**\n- **Evaluation:** This statement incorrectly equates the mechanisms of the two regularizers. While both can lead to smaller activation magnitudes, their structural effects are fundamentally different. Weight decay ($\\ell_2$ on $W$) encourages all weights to be small, leading to a general, diffuse shrinkage and a smoother mapping. Activity regularization ($\\ell_1$ on $z$) encourages many components of $z$ to be exactly zero while allowing a few to be large, leading to a sparse, selective representation. The latter is a much more direct way to constrain the information bottleneck and encourage a \"parts-based\" code. They are not equivalent and do not operate by the same mechanism in terms of the structure they impose on the representation.\n- **Verdict:** **Incorrect**.\n\n**D. Weight decay is preferable in convolutional autoencoders, because $\\|W\\|_{2}^{2}$ penalizes parameter count and thus enforces a narrow effective bottleneck even when $m$ is large, whereas $\\|z\\|_{1}$ primarily risks dead units without limiting the transmitted information.**\n- **Evaluation:** This statement contains two significant errors. First, $\\|W\\|_{2}^{2}$ (weight decay) penalizes the *magnitude* of parameters, not the *count* of parameters. A penalty on parameter count would be related to $\\ell_0$ regularization. Second, the claim that $\\|z\\|_{1}$ risks dead units *without limiting the transmitted information* is a contradiction. The very purpose and effect of an $\\ell_1$ penalty is to limit the information by forcing most channels to be zero (or \"dead\" for a given sample), thereby transmitting information only through the few active channels. It is a direct method of information limitation.\n- **Verdict:** **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "4530388"}, {"introduction": "After understanding the conceptual need for regularization, we turn to the practical architecture of building an autoencoder for medical imaging. For radiomics applications involving 3D data like CT scans, convolutional autoencoders are standard. This practice focuses on a critical component of the decoder: the transposed convolution layer, which upsamples the latent features back to the original image dimensions. This exercise will guide you through deriving the formula for the output size and applying it to ensure perfect spatial reconstruction, a crucial skill for designing and debugging your own deep learning models. [@problem_id:4530275]", "problem": "In radiomics, three-dimensional computed tomography (CT) volumes are often modeled with autoencoders for unsupervised feature learning to capture texture and morphology without labels. Consider a decoder stage that uses a single three-dimensional transposed convolution to reconstruct the spatial dimensions of the original volume. The original volume has spatial dimensions $(N_{h}, N_{w}, N_{d}) = (128, 160, 96)$. The decoder input feature map has spatial dimensions $(M_{h}, M_{w}, M_{d}) = (64, 40, 32)$. The transposed convolution uses stride components $(s_{h}, s_{w}, s_{d}) = (2, 4, 3)$, kernel sizes $(k_{h}, k_{w}, k_{d}) = (3, 3, 3)$, dilation $(d_{h}, d_{w}, d_{d}) = (1, 1, 1)$, and symmetric zero-padding $(p_{h}, p_{w}, p_{d}) = (1, 1, 1)$ along the three axes. There is no cropping. The transposed convolution includes an integer output padding $(o_{h}, o_{w}, o_{d})$ with constraints $0 \\leq o_{i} < s_{i}$ for each axis $i \\in \\{h,w,d\\}$.\n\nStarting from the standard discrete convolution definition and the meaning of stride, padding, dilation, and kernel support in one dimension, derive the expression for the output spatial size produced by a transposed convolution along a single axis as a function of the input size, stride, padding, dilation, kernel size, and output padding. Extend the expression to three dimensions by separability across axes. Then, using the provided numerical values, determine the specific output padding $(o_{h}, o_{w}, o_{d})$ required so that the reconstructed output exactly matches $(N_{h}, N_{w}, N_{d}) = (128, 160, 96)$.\n\nExpress your final answer as a single row matrix containing $(o_{h}, o_{w}, o_{d})$. No units are required, and no rounding is needed.", "solution": "The problem is valid as it is scientifically grounded in the principles of deep learning, is well-posed with sufficient and consistent information, and is expressed in objective, formal language. We will proceed with a solution.\n\nThe task is to determine the required output padding $(o_{h}, o_{w}, o_{d})$ for a three-dimensional transposed convolution to achieve a specific output spatial dimension. To do this, we must first derive the general relationship between the input size, output size, and parameters of a transposed convolution.\n\nLet us begin by considering the standard one-dimensional discrete convolution. The output size, $N_{out}$, is determined by the input size, $M_{in}$, the kernel size, $k$, the stride, $s$, the padding, $p$, and the dilation, $d$, according to the formula:\n$$N_{out} = \\left\\lfloor \\frac{M_{in} + 2p - d(k-1) - 1}{s} \\right\\rfloor + 1$$\nA transposed convolution, sometimes referred to as a fractionally-strided convolution or deconvolution, can be understood as an operation that maps an input tensor to an output tensor with dimensions that are compatible with the inverse of a standard convolution. Specifically, for a given set of convolution parameters $(k, s, p, d)$, the associated transposed convolution maps an input of size $N_{out}$ back to an output of size $M_{in}$.\n\nDue to the floor function $\\lfloor \\cdot \\rfloor$ in the standard convolution formula, a single input size $M_{in}$ can be produced by multiple output sizes $N_{out}$ in the forward pass. This means the inverse mapping is not unique. To formalize this, let's rearrange the relationship. Let the input size to the transposed convolution be $M$ and its output size be $N$. This corresponds to a standard convolution with an input of size $N$ producing an output of size $M$.\n$$M = \\left\\lfloor \\frac{N + 2p - d(k-1) - 1}{s} \\right\\rfloor + 1$$\nThis is equivalent to the inequality:\n$$M - 1 \\leq \\frac{N + 2p - d(k-1) - 1}{s} < M$$\nMultiplying by the stride $s$ yields:\n$$(M - 1)s \\leq N + 2p - d(k-1) - 1 < (M - 1)s + s$$\nTo find the range of possible output sizes $N$ for a given input size $M$, we isolate $N$:\n$$(M - 1)s - 2p + d(k-1) + 1 \\leq N < (M - 1)s - 2p + d(k-1) + 1 + s$$\nThis inequality shows that there are $s$ possible integer values for the output size $N$. To resolve this ambiguity and specify a unique output size, an additional parameter called output padding, $o$, is introduced. The output padding $o$ is an integer constrained by $0 \\leq o < s$. The final output size $N$ is then defined as the lower bound of the derived range plus the output padding value:\n$$N = (M - 1)s - 2p + d(k-1) + 1 + o$$\nThis is the general formula for the output size of a one-dimensional transposed convolution.\n\nThe problem states that the three-dimensional operation is separable across axes. Therefore, we can apply this formula independently to each of the three spatial dimensions: height $(h)$, width $(w)$, and depth $(d)$.\nFor the height axis:\n$$N_{h} = (M_{h} - 1)s_{h} - 2p_{h} + d_{h}(k_{h}-1) + 1 + o_{h}$$\nFor the width axis:\n$$N_{w} = (M_{w} - 1)s_{w} - 2p_{w} + d_{w}(k_{w}-1) + 1 + o_{w}$$\nFor the depth axis:\n$$N_{d} = (M_{d} - 1)s_{d} - 2p_{d} + d_{d}(k_{d}-1) + 1 + o_{d}$$\n\nOur goal is to find the values of $(o_{h}, o_{w}, o_{d})$. We can rearrange the general formula to solve for $o$:\n$$o = N - \\left( (M - 1)s - 2p + d(k-1) + 1 \\right)$$\n\nWe now substitute the given numerical values for each dimension.\n\nFor the height axis ($h$):\n- Target output size: $N_{h} = 128$\n- Input size: $M_{h} = 64$\n- Stride: $s_{h} = 2$\n- Padding: $p_{h} = 1$\n- Kernel size: $k_{h} = 3$\n- Dilation: $d_{h} = 1$\n\n$$o_{h} = 128 - \\left( (64 - 1) \\times 2 - 2 \\times 1 + 1 \\times (3-1) + 1 \\right)$$\n$$o_{h} = 128 - \\left( 63 \\times 2 - 2 + 1 \\times 2 + 1 \\right)$$\n$$o_{h} = 128 - \\left( 126 - 2 + 2 + 1 \\right)$$\n$$o_{h} = 128 - 127 = 1$$\nThe constraint $0 \\leq o_{h} < s_{h}$ is $0 \\leq 1 < 2$, which is satisfied.\n\nFor the width axis ($w$):\n- Target output size: $N_{w} = 160$\n- Input size: $M_{w} = 40$\n- Stride: $s_{w} = 4$\n- Padding: $p_{w} = 1$\n- Kernel size: $k_{w} = 3$\n- Dilation: $d_{w} = 1$\n\n$$o_{w} = 160 - \\left( (40 - 1) \\times 4 - 2 \\times 1 + 1 \\times (3-1) + 1 \\right)$$\n$$o_{w} = 160 - \\left( 39 \\times 4 - 2 + 1 \\times 2 + 1 \\right)$$\n$$o_{w} = 160 - \\left( 156 - 2 + 2 + 1 \\right)$$\n$$o_{w} = 160 - 157 = 3$$\nThe constraint $0 \\leq o_{w} < s_{w}$ is $0 \\leq 3 < 4$, which is satisfied.\n\nFor the depth axis ($d$):\n- Target output size: $N_{d} = 96$\n- Input size: $M_{d} = 32$\n- Stride: $s_{d} = 3$\n- Padding: $p_{d} = 1$\n- Kernel size: $k_{d} = 3$\n- Dilation: $d_{d} = 1$\n\n$$o_{d} = 96 - \\left( (32 - 1) \\times 3 - 2 \\times 1 + 1 \\times (3-1) + 1 \\right)$$\n$$o_{d} = 96 - \\left( 31 \\times 3 - 2 + 1 \\times 2 + 1 \\right)$$\n$$o_{d} = 96 - \\left( 93 - 2 + 2 + 1 \\right)$$\n$$o_{d} = 96 - 94 = 2$$\nThe constraint $0 \\leq o_{d} < s_{d}$ is $0 \\leq 2 < 3$, which is satisfied.\n\nThus, the required output padding vector to achieve the desired output dimensions is $(o_{h}, o_{w}, o_{d}) = (1, 3, 2)$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 3 & 2 \\end{pmatrix}}$$", "id": "4530275"}, {"introduction": "Successfully training an autoencoder yields a new set of learned features, but this is not the end of the process. The next step is to curate these features for use in downstream tasks, such as clinical outcome prediction. This hands-on coding practice tackles the problem of feature redundancy, guiding you to implement a principled pruning strategy based on statistical correlation. By comparing latent features against each other and against existing handcrafted features, you will learn how to build a final feature set that is both parsimonious and rich in novel information, a vital skill in practical machine learning. [@problem_id:4530325]", "problem": "You are given two feature matrices per dataset: a latent representation learned by an autoencoder and a set of handcrafted radiomic features. In the context of radiomics, autoencoders are used for unsupervised feature learning, producing latent dimensions that summarize patterns in image-derived data without labels. To evaluate redundancy and propose a pruning strategy that maintains non-redundant information, use the following fundamental base.\n\nLet $Z \\in \\mathbb{R}^{n \\times d_z}$ denote the latent feature matrix for $n$ samples and $d_z$ latent dimensions, and let $H \\in \\mathbb{R}^{n \\times d_h}$ denote the handcrafted feature matrix with $d_h$ features. For any feature vector $x \\in \\mathbb{R}^{n}$, define the sample mean as\n$$\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i,\n$$\nthe sample variance as\n$$\ns_x^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2,\n$$\nand the sample standard deviation (Standard Deviation (SD)) as $s_x = \\sqrt{s_x^2}$. For two feature vectors $x, y \\in \\mathbb{R}^{n}$, define the sample covariance\n$$\n\\operatorname{cov}(x,y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}),\n$$\nand the Pearson Product-Moment Correlation Coefficient (PPMCC)\n$$\nr_{x,y} = \\frac{\\operatorname{cov}(x,y)}{s_x s_y},\n$$\nwhenever $s_x > 0$ and $s_y > 0$. If either $s_x = 0$ or $s_y = 0$, the correlation is undefined in this formulation.\n\nRedundancy is assessed by the magnitude of the correlation. Given a redundancy threshold $\\tau \\in [0,1]$, implement the following pruning policy on the latent dimensions:\n- Zero-variance pruning: If a latent dimension $z_j$ has $s_{z_j} = 0$, mark index $j$ for pruning.\n- Latent-latent redundancy: For any pair of latent dimensions $z_i$ and $z_j$ with $i \\neq j$, if $|r_{z_i,z_j}| \\ge \\tau$, mark for pruning the index with the smaller sample variance $s_{z}^2$. If $s_{z_i}^2 = s_{z_j}^2$ within a small numerical tolerance, mark for pruning the higher index $\\max(i,j)$.\n- Latent-handcrafted redundancy: For any latent dimension $z_i$ and handcrafted feature $h_k$, if $|r_{z_i,h_k}| \\ge \\tau$, mark index $i$ for pruning. If $s_{h_k} = 0$, treat $r_{z_i,h_k}$ as undefined and do not prune solely on that pair.\n\nUse $0$-based indexing for all feature indices. Your program must apply the above policy to each dataset in the test suite and return, for each dataset, the sorted list of latent indices to prune.\n\nTest suite parameter values:\n- Case $1$ (general case):\n  $$\n  Z^{(1)} = \\begin{bmatrix}\n  1 & 2 & 5 \\\\\n  2 & 4 & 4 \\\\\n  3 & 6 & 3 \\\\\n  4 & 8 & 2 \\\\\n  5 & 10 & 1\n  \\end{bmatrix},\\quad\n  H^{(1)} = \\begin{bmatrix}\n  1.1 & 0 \\\\\n  2.2 & 0 \\\\\n  3.3 & 1 \\\\\n  4.4 & 0 \\\\\n  5.5 & 0\n  \\end{bmatrix},\\quad\n  \\tau^{(1)} = 0.95.\n  $$\n- Case $2$ (boundary condition where $|r|$ equals the threshold):\n  $$\n  Z^{(2)} = \\begin{bmatrix}\n  1 & 4 \\\\\n  2 & 3 \\\\\n  3 & 2 \\\\\n  4 & 1\n  \\end{bmatrix},\\quad\n  H^{(2)} = \\begin{bmatrix}\n  0.5 \\\\\n  0.6 \\\\\n  0.1 \\\\\n  0.2\n  \\end{bmatrix},\\quad\n  \\tau^{(2)} = 1.0.\n  $$\n- Case $3$ (edge case with a zero-variance latent and a zero-variance handcrafted feature):\n  $$\n  Z^{(3)} = \\begin{bmatrix}\n  3 & 1 & 0 \\\\\n  3 & 0 & 1 \\\\\n  3 & 1 & 2 \\\\\n  3 & 0 & 3 \\\\\n  3 & 1 & 4\n  \\end{bmatrix},\\quad\n  H^{(3)} = \\begin{bmatrix}\n  0 & 1 \\\\\n  0 & 2 \\\\\n  0 & 1 \\\\\n  0 & 2 \\\\\n  0 & 1\n  \\end{bmatrix},\\quad\n  \\tau^{(3)} = 0.8.\n  $$\n- Case $4$ (negative cross-group correlation exceeding the threshold and unequal latent variances):\n  $$\n  Z^{(4)} = \\begin{bmatrix}\n  1 & 1 \\\\\n  2 & 1 \\\\\n  3 & 2 \\\\\n  4 & 2 \\\\\n  5 & 3 \\\\\n  6 & 3\n  \\end{bmatrix},\\quad\n  H^{(4)} = \\begin{bmatrix}\n  -1 \\\\\n  -2 \\\\\n  -3 \\\\\n  -4 \\\\\n  -5 \\\\\n  -6\n  \\end{bmatrix},\\quad\n  \\tau^{(4)} = 0.95.\n  $$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of pruned latent indices for a case. For example, output in the form\n$$\n[ [\\text{case 1 indices}], [\\text{case 2 indices}], [\\text{case 3 indices}], [\\text{case 4 indices}] ].\n$$\nThe entries must be lists of integers using $0$-based indexing. No units are required because all quantities are dimensionless. Angles are not involved. Any fractional thresholds must be provided as decimals as shown above (e.g., $\\tau = 0.95$).", "solution": "The problem requires the formulation and implementation of a feature pruning strategy for latent dimensions learned by an autoencoder in the context of radiomics. The goal is to reduce redundancy by comparing latent features against each other and against a set of established handcrafted radiomic features. This process is fundamental to building parsimonious and robust machine learning models by retaining only the most informative and unique features. The pruning policy is based on a rigorous application of fundamental statistical principles.\n\nThe core statistical tools specified are the sample variance and the Pearson Product-Moment Correlation Coefficient (PPMCC). For a feature vector $x \\in \\mathbb{R}^{n}$ with $n$ samples, the sample variance, $s_x^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$, measures the feature's variability or dispersion around its mean $\\bar{x}$. A feature with zero variance is constant and carries no information for distinguishing between samples. The PPMCC, $r_{x,y} = \\frac{\\operatorname{cov}(x,y)}{s_x s_y}$, measures the strength and direction of the linear relationship between two feature vectors $x$ and $y$. Its value ranges from $-1$ (perfect negative linear correlation) to $+1$ (perfect positive linear correlation), with $0$ indicating no linear correlation. Redundancy is identified when the absolute value of the correlation, $|r_{x,y}|$, is high.\n\nThe provided pruning policy is a deterministic algorithm applied to the latent feature matrix $Z \\in \\mathbb{R}^{n \\times d_z}$ and the handcrafted feature matrix $H \\in \\mathbb{R}^{n \\times d_h}$, using a given redundancy threshold $\\tau$. The algorithm consists of three distinct stages, and the set of indices to be pruned is the union of indices identified at each stage.\n\n1.  **Zero-Variance Pruning**: The first step is to identify and mark for pruning any latent dimension $z_j$ that has zero variance ($s_{z_j}^2 = 0$). Such features are uninformative as they have the same value for all samples. This is a standard and essential data pre-processing step.\n\n2.  **Latent-Latent Redundancy Analysis**: This stage addresses multicollinearity within the set of learned latent features. For every unique pair of latent dimensions, $(z_i, z_j)$ with $i \\neq j$, their PPMCC, $r_{z_i,z_j}$, is calculated. If $|r_{z_i,z_j}| \\ge \\tau$, the two features are considered redundant. To decide which one to prune, the policy employs an information-based criterion: the feature with the smaller sample variance ($s_z^2$) is removed, under the principle that it explains less of the data's variability. This preserves the feature that captures more information. In the case of equal variances, a deterministic tie-breaking rule is applied: the feature with the higher index, $\\max(i, j)$, is pruned to ensure a unique outcome. This check is only performed if both features have non-zero variance.\n\n3.  **Latent-Handcrafted Redundancy Analysis**: The final stage assesses the novelty of the learned features. Each latent dimension $z_i$ is compared against every handcrafted feature $h_k$. If the correlation magnitude $|r_{z_i,h_k}|$ meets or exceeds the threshold $\\tau$, it implies that the autoencoder has learned a feature that is already captured by the handcrafted feature set. In this scenario, the latent feature $z_i$ is marked for pruning, as it provides no new information beyond what is already known from established radiomic features. The policy correctly specifies that if a handcrafted feature $h_k$ has zero variance ($s_{h_k}=0$), the correlation is undefined, and this pair cannot be a basis for pruning $z_i$.\n\nThe complete algorithm involves systematically applying these three rules. A set data structure is appropriate for collecting the indices of latent features to be pruned, as it automatically handles cases where an index is marked for pruning multiple times. The final output is the sorted list of unique indices from this set. This procedure ensures a comprehensive and principled reduction of the feature space.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and print the results\n    in the specified format.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Z\": np.array([\n                [1, 2, 5],\n                [2, 4, 4],\n                [3, 6, 3],\n                [4, 8, 2],\n                [5, 10, 1]\n            ], dtype=float),\n            \"H\": np.array([\n                [1.1, 0],\n                [2.2, 0],\n                [3.3, 1],\n                [4.4, 0],\n                [5.5, 0]\n            ], dtype=float),\n            \"tau\": 0.95\n        },\n        {\n            \"Z\": np.array([\n                [1, 4],\n                [2, 3],\n                [3, 2],\n                [4, 1]\n            ], dtype=float),\n            \"H\": np.array([\n                [0.5],\n                [0.6],\n                [0.1],\n                [0.2]\n            ], dtype=float),\n            \"tau\": 1.0\n        },\n        {\n            \"Z\": np.array([\n                [3, 1, 0],\n                [3, 0, 1],\n                [3, 1, 2],\n                [3, 0, 3],\n                [3, 1, 4]\n            ], dtype=float),\n            \"H\": np.array([\n                [0, 1],\n                [0, 2],\n                [0, 1],\n                [0, 2],\n                [0, 1]\n            ], dtype=float),\n            \"tau\": 0.8\n        },\n        {\n            \"Z\": np.array([\n                [1, 1],\n                [2, 1],\n                [3, 2],\n                [4, 2],\n                [5, 3],\n                [6, 3]\n            ], dtype=float),\n            \"H\": np.array([\n                [-1],\n                [-2],\n                [-3],\n                [-4],\n                [-5],\n                [-6]\n            ], dtype=float),\n            \"tau\": 0.95\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        pruned_list = prune_latent_features(case[\"Z\"], case[\"H\"], case[\"tau\"])\n        results.append(pruned_list)\n\n    # Use map(str, ...) and join to format the list of lists correctly.\n    # e.g., for [[1, 2], [3]], this produces '[1, 2],[3]'\n    # The f-string then adds the outer brackets to get '[[1, 2],[3]]'\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef prune_latent_features(Z, H, tau):\n    \"\"\"\n    Applies the pruning policy to latent features based on redundancy.\n\n    Args:\n        Z (np.ndarray): Latent feature matrix (n_samples, d_z).\n        H (np.ndarray): Handcrafted feature matrix (n_samples, d_h).\n        tau (float): Redundancy threshold.\n\n    Returns:\n        list: A sorted list of 0-based indices of latent features to prune.\n    \"\"\"\n    if Z.ndim == 1:\n        Z = Z.reshape(-1, 1)\n    if H.ndim == 1:\n        H = H.reshape(-1, 1)\n\n    n_samples, d_z = Z.shape\n    d_h = H.shape[1] if H.size > 0 else 0\n    \n    # Use a small tolerance for floating-point comparisons\n    TOLERANCE = 1e-9\n    \n    pruned_indices = set()\n\n    # Calculate sample variances (ddof=1 for n-1 denominator)\n    latent_variances = np.var(Z, axis=0, ddof=1)\n    if d_h > 0:\n      handcrafted_variances = np.var(H, axis=0, ddof=1)\n    \n    # 1. Zero-variance pruning\n    for j in range(d_z):\n        if latent_variances[j]  TOLERANCE:\n            pruned_indices.add(j)\n\n    # 2. Latent-latent redundancy\n    for i in range(d_z):\n        for j in range(i + 1, d_z):\n            # Only calculate correlation if variances are non-zero\n            if latent_variances[i] > TOLERANCE and latent_variances[j] > TOLERANCE:\n                # np.corrcoef with two 1D arrays returns a 2x2 matrix\n                r = np.corrcoef(Z[:, i], Z[:, j])[0, 1]\n                \n                if np.abs(r) >= tau:\n                    var_i = latent_variances[i]\n                    var_j = latent_variances[j]\n                    \n                    if np.abs(var_i - var_j)  TOLERANCE:\n                        # Tie-break by pruning the higher index\n                        pruned_indices.add(j)\n                    elif var_i  var_j:\n                        pruned_indices.add(i)\n                    else:\n                        pruned_indices.add(j)\n\n    # 3. Latent-handcrafted redundancy\n    if d_h > 0:\n      for i in range(d_z):\n          for k in range(d_h):\n              # Only calculate correlation if variances are non-zero\n              if latent_variances[i] > TOLERANCE and handcrafted_variances[k] > TOLERANCE:\n                  r = np.corrcoef(Z[:, i], H[:, k])[0, 1]\n                  if np.abs(r) >= tau:\n                      pruned_indices.add(i)\n\n    return sorted(list(pruned_indices))\n\nsolve()\n```", "id": "4530325"}]}