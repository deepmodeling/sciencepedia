{"hands_on_practices": [{"introduction": "Attention mechanisms are powerful because they allow a model to dynamically weigh the importance of different pieces of information. At the core of this capability is the scaled dot-product attention calculation. This first exercise breaks down this fundamental computation into its key components—Query ($Q$), Key ($K$), and Value ($V$)—within a plausible multi-modal radiomics scenario, where information from different imaging types must be fused. By working through this problem [@problem_id:4529597], you will gain a concrete understanding of how attention scores are generated and used to create a weighted summary.", "problem": "In a multi-modality radiomics pipeline for lesion characterization, a single Computed Tomography (CT) radiomic signature is used to query complementary information from two auxiliary modalities: Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). The query, keys, and values are embedded in a common latent space of dimension $d=2$. The query vector is $Q=\\begin{bmatrix}1  1\\end{bmatrix}$, the key matrix is $K=\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$, and the value vector is $V=\\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$, where the first component corresponds to MRI and the second to PET. According to the standard scaled similarity-based weighting used in attention mechanisms, cross-attention converts the dot-product similarities between $Q$ and the rows of $K$ into nonnegative weights that sum to one, and then aggregates $V$ as a weighted average. Using this mechanism with embedding dimension $d=2$, compute the cross-attention output for the CT query. Additionally, identify, based on the computed weights and value contributions, which modality (MRI or PET) contributes more to the final aggregated score. Provide the final aggregated score as an exact real-valued number with no units.", "solution": "The problem asks for the computation of a cross-attention output and an analysis of the contributions from different modalities. The underlying mechanism is the scaled dot-product attention, a standard component in transformer-based deep learning models. The formula for the attention output is given by:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V $$\nwhere $Q$ is the query matrix, $K$ is the key matrix, $V$ is the value matrix, and $d_k$ is the dimension of the key vectors.\n\nFirst, we must validate the problem statement.\nStep 1: Extract Givens.\n- The query vector, representing a CT signature, is $Q = \\begin{bmatrix}1  1\\end{bmatrix}$.\n- The key matrix, with rows corresponding to MRI and PET modalities, is $K = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$.\n- The value vector, with components corresponding to MRI and PET, is $V = \\begin{bmatrix}2 \\\\ 5\\end{bmatrix}$.\n- The dimension of the latent space for keys and queries is $d = 2$.\n- The mechanism is scaled similarity-based weighting (scaled dot-product attention). The output is a weighted average of the components of $V$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, as it uses a standard algorithm (attention mechanism) in a plausible context (multi-modal radiomics). It is well-posed, with all necessary data ($Q, K, V, d$) provided and a clear objective. The dimensions of the vectors and matrices are consistent for the required operations ($Q$ is $1 \\times 2$, $K$ is $2 \\times 2$, $V$ is $2 \\times 1$). The problem is objective, complete, and contains no contradictions or ambiguities. It is a valid problem.\n\nStep 3: Verdict and Action.\nThe problem is valid. We will proceed with the solution.\n\nThe calculation proceeds in several steps:\n\n1.  **Compute the dot-product similarity scores.**\n    The scores are the dot products of the query vector $Q$ with each key vector, which are the rows of the key matrix $K$. This operation is equivalent to the matrix multiplication of the query matrix $Q$ and the transpose of the key matrix $K^T$.\n    The key vectors are $K_1 = \\begin{bmatrix}1  0\\end{bmatrix}$ for MRI and $K_2 = \\begin{bmatrix}0  1\\end{bmatrix}$ for PET.\n    The transpose of the key matrix is:\n    $$ K^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}^T = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} $$\n    The scores vector is then computed as:\n    $$ \\text{scores} = Q K^T = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} = \\begin{bmatrix}(1)(1) + (1)(0)  (1)(0) + (1)(1)\\end{bmatrix} = \\begin{bmatrix}1  1\\end{bmatrix} $$\n    The raw similarity score for MRI is $s_1 = 1$, and for PET is $s_2 = 1$.\n\n2.  **Scale the scores.**\n    The scores are scaled by dividing by the square root of the dimension of the key vectors, $d_k = d = 2$. The scaling factor is $\\frac{1}{\\sqrt{2}}$.\n    The scaled scores vector $z$ is:\n    $$ z = \\begin{bmatrix}z_1  z_2\\end{bmatrix} = \\begin{bmatrix}1 \\cdot \\frac{1}{\\sqrt{2}}  1 \\cdot \\frac{1}{\\sqrt{2}}\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}\\end{bmatrix} $$\n\n3.  **Compute the attention weights using the softmax function.**\n    The attention weights are calculated by applying the softmax function to the scaled scores. The weight $w_i$ for the $i$-th modality is given by $w_i = \\frac{\\exp(z_i)}{\\sum_{j} \\exp(z_j)}$.\n    For the MRI modality (first component):\n    $$ w_{MRI} = w_1 = \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    For the PET modality (second component):\n    $$ w_{PET} = w_2 = \\frac{\\exp(z_2)}{\\exp(z_1) + \\exp(z_2)} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{\\exp(\\frac{1}{\\sqrt{2}}) + \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 \\exp(\\frac{1}{\\sqrt{2}})} = \\frac{1}{2} $$\n    The attention weights are equal, $w_{MRI} = 0.5$ and $w_{PET} = 0.5$. This indicates that the CT query vector $Q$ is equally similar to the key vectors for MRI and PET in this latent space representation.\n\n4.  **Compute the final aggregated score (attention output).**\n    The output is the weighted average of the value vector $V$, where the components of $V$ are $v_{MRI} = 2$ and $v_{PET} = 5$.\n    $$ \\text{Output} = \\sum_{i=1}^2 w_i v_i = w_{MRI} v_{MRI} + w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(2) + \\left(\\frac{1}{2}\\right)(5) = 1 + \\frac{5}{2} = 1 + 2.5 = 3.5 $$\n    The final aggregated score is $3.5$.\n\n5.  **Identify the modality with the greater contribution.**\n    The contribution of each modality to the final score is its value multiplied by its attention weight, i.e., $w_i v_i$.\n    - Contribution from MRI: $w_{MRI} v_{MRI} = \\left(\\frac{1}{2}\\right)(2) = 1$.\n    - Contribution from PET: $w_{PET} v_{PET} = \\left(\\frac{1}{2}\\right)(5) = 2.5$.\n    Since $2.5  1$, the PET modality contributes more to the final aggregated score. This is because, while the attention mechanism found both modalities equally relevant (equal weights), the information content or signal represented by the value for PET ($v_{PET}=5$) is substantially larger than that for MRI ($v_{MRI}=2$).", "answer": "$$\\boxed{3.5}$$", "id": "4529597"}, {"introduction": "While attention can be applied spatially, it is also highly effective when applied to the feature channels of a convolutional neural network. The Squeeze-and-Excitation (SE) block is a landmark example of channel attention, allowing a network to re-calibrate which feature maps are most important. This practice [@problem_id:4529549] delves into the practical design of an SE module, focusing on how a 'bottleneck' design with a reduction ratio $r$ makes it lightweight and parameter-efficient. You will analyze the trade-offs between model complexity and the risk of overfitting, a critical consideration when developing models for small-data domains like radiomics.", "problem": "In a convolutional neural network for radiomics, a channel attention module based on Squeeze-and-Excitation (SE) is inserted after a convolutional block with $C$ output channels to adaptively reweight channel-wise feature responses. The SE module performs global average pooling across spatial dimensions (which introduces no trainable parameters), followed by a two-layer Multilayer Perceptron (MLP) with a reduction ratio $r$, mapping $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$. Each fully connected layer includes both weights and biases. The excitation uses a sigmoid nonlinearity, which introduces no trainable parameters.\n\nUsing the core definition that a fully connected layer mapping $\\mathbb{R}^{n_{\\mathrm{in}}} \\rightarrow \\mathbb{R}^{n_{\\mathrm{out}}}$ has $n_{\\mathrm{in}} n_{\\mathrm{out}}$ weights and $n_{\\mathrm{out}}$ biases, derive the total number of trainable parameters in the SE MLP in terms of $C$ and $r$. Then, for $C = 64$ and $r = 16$, compute the numerical value of this parameter count. Finally, based on your derived expression, reason from first principles how increasing $r$ affects parameter efficiency and overfitting risk in small radiomics cohorts with limited sample size, assuming $C$ is fixed.\n\nReport the numerical parameter count as an exact integer. No rounding is required, and no units are needed for the final numerical answer.", "solution": "The problem requires a three-part answer: first, a derivation of the total number of trainable parameters in a Squeeze-and-Excitation (SE) module's multilayer perceptron (MLP); second, a numerical calculation for specific values of $C$ and $r$; and third, a reasoned analysis of the effect of the reduction ratio $r$ on model characteristics. The problem is well-posed and scientifically sound, allowing for a direct solution.\n\nThe SE module's MLP consists of two fully connected (FC) layers. The total number of trainable parameters, denoted as $P_{\\text{total}}$, is the sum of the parameters from each of these two layers. We use the provided definition that a fully connected layer mapping from an input dimension $n_{\\text{in}}$ to an output dimension $n_{\\text{out}}$ has $n_{\\text{in}} n_{\\text{out}}$ weights and $n_{\\text{out}}$ biases, for a total of $n_{\\text{in}} n_{\\text{out}} + n_{\\text{out}}$ parameters.\n\nLet's analyze each layer of the MLP, which performs the mapping $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C/r} \\rightarrow \\mathbb{R}^{C}$.\n\nFirst FC layer (Reduction stage):\nThis layer maps the input feature vector from a dimension of $C$ to a reduced dimension of $C/r$.\n- Input dimension: $n_{\\mathrm{in}} = C$.\n- Output dimension: $n_{\\mathrm{out}} = \\frac{C}{r}$.\nThe number of trainable weights in this layer is the product of the input and output dimensions:\n$$\nW_1 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = C \\times \\frac{C}{r} = \\frac{C^2}{r}\n$$\nThe number of trainable biases is equal to the output dimension:\n$$\nB_1 = n_{\\mathrm{out}} = \\frac{C}{r}\n$$\nTherefore, the total number of parameters in the first FC layer, $P_1$, is:\n$$\nP_1 = W_1 + B_1 = \\frac{C^2}{r} + \\frac{C}{r}\n$$\n\nSecond FC layer (Expansion stage):\nThis layer maps the feature vector from the reduced dimension $C/r$ back to the original channel dimension $C$.\n- Input dimension: $n_{\\mathrm{in}} = \\frac{C}{r}$.\n- Output dimension: $n_{\\mathrm{out}} = C$.\nThe number of trainable weights in this layer is:\n$$\nW_2 = n_{\\mathrm{in}} \\times n_{\\mathrm{out}} = \\frac{C}{r} \\times C = \\frac{C^2}{r}\n$$\nThe number of trainable biases is:\n$$\nB_2 = n_{\\mathrm{out}} = C\n$$\nTherefore, the total number of parameters in the second FC layer, $P_2$, is:\n$$\nP_2 = W_2 + B_2 = \\frac{C^2}{r} + C\n$$\n\nThe total number of trainable parameters in the entire SE MLP is the sum of the parameters from both layers:\n$$\nP_{\\text{total}} = P_1 + P_2 = \\left(\\frac{C^2}{r} + \\frac{C}{r}\\right) + \\left(\\frac{C^2}{r} + C\\right)\n$$\nCombining terms, we get the final expression for the total number of parameters:\n$$\nP_{\\text{total}} = \\frac{2C^2}{r} + \\frac{C}{r} + C\n$$\nThis expression represents the total count of trainable parameters in the SE module's MLP in terms of the number of channels $C$ and the reduction ratio $r$.\n\nNext, we compute the numerical value for $C = 64$ and $r = 16$.\nSubstituting these values into the derived formula:\n$$\nP_{\\text{total}} = \\frac{2(64)^2}{16} + \\frac{64}{16} + 64\n$$\nFirst, we evaluate the terms:\n$(64)^2 = 4096$.\n$$\n\\frac{2(4096)}{16} = \\frac{8192}{16} = 512\n$$\n$$\n\\frac{64}{16} = 4\n$$\nSumming the components:\n$$\nP_{\\text{total}} = 512 + 4 + 64 = 580\n$$\nThus, for $C=64$ and $r=16$, the SE MLP contains $580$ trainable parameters.\n\nFinally, we reason from first principles how increasing the reduction ratio $r$ affects parameter efficiency and overfitting risk, assuming $C$ is fixed. Our derived expression for the total parameter count is $P_{\\text{total}}(r) = \\frac{2C^2+C}{r} + C$.\n\n1.  Effect on Parameter Efficiency:\n    Parameter efficiency refers to a model's ability to achieve high performance with a minimal number of parameters. In the expression $P_{\\text{total}}(r)$, the term $\\frac{2C^2+C}{r}$ is inversely proportional to $r$, while $C$ is a constant. As $r$ increases, the value of this fractional term decreases, thus reducing the overall parameter count $P_{\\text{total}}$. A model with fewer parameters is considered more parameter-efficient because it requires less memory and computational overhead. Therefore, increasing $r$ improves the parameter efficiency of the SE module.\n\n2.  Effect on Overfitting Risk:\n    Overfitting is a phenomenon where a model with high capacity (typically associated with a large number of parameters) learns the specific details and noise of the training data, leading to poor generalization to new, unseen data. In disciplines like radiomics, which often contend with small patient cohorts and limited sample sizes, overfitting is a significant concern. By increasing $r$, we reduce the total number of trainable parameters $P_{\\text{total}}$ in the SE module. This reduction in parameters lowers the model's overall capacity. A model with lower capacity is less able to memorize the training data and is forced to learn more robust, generalizable patterns. This acts as a form of implicit regularization. Consequently, increasing $r$ helps to mitigate the risk of overfitting, which is highly desirable for building robust models from small datasets.", "answer": "$$\n\\boxed{580}\n$$", "id": "4529549"}, {"introduction": "The ubiquitous softmax function is not the only way to convert raw attention scores into a probability distribution. Depending on the clinical task, we might want our model to be more decisive, focusing all its attention on a single critical region rather than diffusing it across several. This exercise [@problem_id:4529607] explores this concept by comparing the standard softmax with sparse alternatives like sparsemax and entmax. By computing each distribution, you will develop an intuition for how the choice of mapping function controls the sparsity of the final attention weights, enabling you to tailor your model's focus.", "problem": "A radiomics model analyzing Computed Tomography (CT) scans uses an attention module over three candidate patches from a tumor-bearing slice. The module outputs unnormalized attention scores $s = [2, 1, 0]$ corresponding to the three patches, where $s_{i}$ measures the relevance of patch $i$ to malignancy. To compare attention mechanisms that induce different levels of sparsity in the attention distribution, proceed as follows, using foundational definitions of probability simplex projections and entropy-regularized mappings.\n\n1. Using the canonical maximum-entropy mapping used in attention, the softmax distribution is defined by $p_{i} = \\exp(s_{i}) \\big/ \\sum_{j=1}^{3} \\exp(s_{j})$. Compute the softmax distribution for $s$.\n\n2. The sparsemax distribution is defined as the Euclidean projection of $s$ onto the probability simplex $\\Delta^{2} = \\{p \\in \\mathbb{R}^{3} : \\sum_{i=1}^{3} p_{i} = 1, p_{i} \\ge 0\\}$, that is, the solution of $\\min_{p \\in \\Delta^{2}} \\|p - s\\|_{2}^{2}$. Starting from this definition, derive the thresholding form $p_{i} = \\max\\{s_{i} - \\tau, 0\\}$ with $\\tau$ chosen such that $\\sum_{i=1}^{3} p_{i} = 1$, determine the correct active set, and compute the sparsemax distribution for $s$.\n\n3. The $\\alpha$-entmax distribution arises from maximizing the Tsallis $\\alpha$-entropy subject to linear consistency with scores, yielding the closed-form solution $p_{i} = \\big[(\\alpha - 1)(s_{i} - \\tau)\\big]_{+}^{\\frac{1}{\\alpha - 1}}$, where $[x]_{+} = \\max\\{x, 0\\}$ and $\\tau$ is chosen so that $\\sum_{i=1}^{3} p_{i} = 1$. Specialize to $\\alpha = 1.5$ and compute the entmax-$1.5$ distribution for $s$ in exact form by identifying the correct support and solving for the threshold $\\tau$.\n\nBriefly interpret, in the context of attention over CT radiomics patches, which of these three mappings better isolates a single highly suspicious patch versus diffusing attention across multiple patches, and why.\n\nFinally, report the attention weight assigned by the entmax-$1.5$ distribution to the highest-scoring patch ($s_{1} = 2$) in exact closed form. Do not approximate numerically; express your answer as a simplified analytic expression.", "solution": "This problem asks for the computation of three different attention distributions (softmax, sparsemax, and entmax-$1.5$) for a given vector of unnormalized scores, followed by an interpretation of the results and a final specific value.\n\n### Step 1: Extract Givens\n- Unnormalized scores: $s = [2, 1, 0]$ for three patches, so $s_1 = 2$, $s_2 = 1$, $s_3 = 0$.\n- Softmax definition: $p_{i} = \\exp(s_{i}) \\big/ \\sum_{j=1}^{3} \\exp(s_{j})$.\n- Sparsemax definition: $p = \\arg\\min_{p \\in \\Delta^{2}} \\|p - s\\|_{2}^{2}$, where $\\Delta^{2} = \\{p \\in \\mathbb{R}^{3} : \\sum_{i=1}^{3} p_{i} = 1, p_{i} \\ge 0\\}$. The solution has the form $p_{i} = \\max\\{s_{i} - \\tau, 0\\}$ with $\\sum p_i = 1$.\n- $\\alpha$-entmax definition: $p_{i} = \\big[(\\alpha - 1)(s_{i} - \\tau)\\big]_{+}^{\\frac{1}{\\alpha - 1}}$, with $[x]_{+} = \\max\\{x, 0\\}$, $\\sum p_i=1$, and $\\alpha=1.5$.\n- Final answer required: The attention weight for the highest-scoring patch under the entmax-$1.5$ distribution.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the field of deep learning and its application to medical imaging (radiomics). Softmax, sparsemax, and $\\alpha$-entmax are established, mathematically defined functions used as attention mechanisms.\n- **Well-Posed**: The problem is well-posed. The inputs are clearly defined, the mathematical operations are standard, and a unique solution exists for each part of the problem.\n- **Objective**: The problem is stated objectively with precise mathematical definitions and no subjective language.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe following sections address each part of the problem.\n\n**1. Softmax Distribution**\n\nThe softmax function is defined as $p_{i} = \\frac{\\exp(s_{i})}{\\sum_{j=1}^{3} \\exp(s_{j})}$.\nGiven the scores $s_1 = 2$, $s_2 = 1$, and $s_3 = 0$, we first compute the normalization term (the denominator):\n$$\n\\sum_{j=1}^{3} \\exp(s_{j}) = \\exp(2) + \\exp(1) + \\exp(0) = e^2 + e^1 + 1\n$$\nNow, we compute the probability for each patch:\n$$\np_1 = \\frac{\\exp(s_1)}{e^2 + e + 1} = \\frac{e^2}{e^2 + e + 1}\n$$\n$$\np_2 = \\frac{\\exp(s_2)}{e^2 + e + 1} = \\frac{e}{e^2 + e + 1}\n$$\n$$\np_3 = \\frac{\\exp(s_3)}{e^2 + e + 1} = \\frac{1}{e^2 + e + 1}\n$$\nThe softmax distribution is the vector $p_{\\text{softmax}} = \\left[ \\frac{e^2}{e^2 + e + 1}, \\frac{e}{e^2 + e + 1}, \\frac{1}{e^2 + e + 1} \\right]$. All probabilities are positive, demonstrating the \"soft\" nature of this mapping.\n\n**2. Sparsemax Distribution**\n\nThe sparsemax probability is given by $p_i = \\max\\{s_i - \\tau, 0\\}$, where the threshold $\\tau$ is chosen such that $\\sum_{i=1}^3 p_i = 1$. Let $S = \\{i | s_i  \\tau\\}$ be the support (the set of indices with non-zero probability). The condition becomes $\\sum_{i \\in S} (s_i - \\tau) = 1$.\n\nWe find the correct support by testing potential support sets, typically in decreasing order of scores. The scores are $s_1=2, s_2=1, s_3=0$.\n\n- Assume the support is $S = \\{1, 2, 3\\}$. This would require $s_3 = 0  \\tau$.\nThe equation for $\\tau$ is $(s_1 - \\tau) + (s_2 - \\tau) + (s_3 - \\tau) = 1$.\n$(2 - \\tau) + (1 - \\tau) + (0 - \\tau) = 1 \\implies 3 - 3\\tau = 1 \\implies 3\\tau = 2 \\implies \\tau = \\frac{2}{3}$.\nThis contradicts the requirement $0  \\tau$, so the support is not of size $3$.\n\n- Assume the support is $S = \\{1, 2\\}$. This requires $s_2 = 1  \\tau$ and $s_3 = 0 \\le \\tau$.\nThe equation for $\\tau$ is $(s_1 - \\tau) + (s_2 - \\tau) = 1$.\n$(2 - \\tau) + (1 - \\tau) = 1 \\implies 3 - 2\\tau = 1 \\implies 2\\tau = 2 \\implies \\tau = 1$.\nThis contradicts the requirement $1  \\tau$, so the support is not of size $2$.\n\n- Assume the support is $S = \\{1\\}$. This requires $s_1 = 2  \\tau$ and $s_2 = 1 \\le \\tau$.\nThe equation for $\\tau$ is $(s_1 - \\tau) = 1$.\n$2 - \\tau = 1 \\implies \\tau = 1$.\nThe conditions are $2  1$ (true) and $1 \\le 1$ (true). So, this is the correct support and threshold.\n\nWith $\\tau=1$, we compute the sparsemax probabilities:\n$$\np_1 = \\max\\{s_1 - \\tau, 0\\} = \\max\\{2 - 1, 0\\} = 1\n$$\n$$\np_2 = \\max\\{s_2 - \\tau, 0\\} = \\max\\{1 - 1, 0\\} = 0\n$$\n$$\np_3 = \\max\\{s_3 - \\tau, 0\\} = \\max\\{0 - 1, 0\\} = 0\n$$\nThe sparsemax distribution is $p_{\\text{sparsemax}} = [1, 0, 0]$. This is a sparse, \"hard\" distribution.\n\n**3. $\\alpha$-entmax Distribution with $\\alpha = 1.5$**\n\nFor $\\alpha=1.5$, the $\\alpha$-entmax probability is $p_{i} = \\big[(\\alpha - 1)(s_{i} - \\tau)\\big]_{+}^{\\frac{1}{\\alpha - 1}}$.\nSubstituting $\\alpha=1.5$:\n$$\np_i = \\big[(1.5 - 1)(s_i - \\tau)\\big]_{+}^{\\frac{1}{1.5-1}} = \\big[0.5(s_i - \\tau)\\big]_{+}^2\n$$\nThe support is $S = \\{i | s_i  \\tau\\}$, and we must satisfy $\\sum_{i \\in S} \\big[0.5(s_i - \\tau)\\big]^2 = 1$.\nThis simplifies to $\\sum_{i \\in S} (s_i - \\tau)^2 = 4$.\n\nWe again test potential support sets.\n- Assume $S=\\{1, 2, 3\\}$. This requires $s_3=0  \\tau$.\n$(2-\\tau)^2 + (1-\\tau)^2 + (0-\\tau)^2 = 4$.\n$(4-4\\tau+\\tau^2) + (1-2\\tau+\\tau^2) + \\tau^2 = 4$.\n$3\\tau^2 - 6\\tau + 5 = 4 \\implies 3\\tau^2 - 6\\tau + 1 = 0$.\nThe roots are $\\tau = \\frac{6 \\pm \\sqrt{36-12}}{6} = 1 \\pm \\frac{\\sqrt{6}}{3}$. Both roots are positive, contradicting $0  \\tau$.\n\n- Assume $S=\\{1, 2\\}$. This requires $s_2=1  \\tau$ and $s_3=0 \\le \\tau$.\n$(2-\\tau)^2 + (1-\\tau)^2 = 4$.\n$(4-4\\tau+\\tau^2) + (1-2\\tau+\\tau^2) = 4$.\n$2\\tau^2 - 6\\tau + 5 = 4 \\implies 2\\tau^2 - 6\\tau + 1 = 0$.\nThe roots are $\\tau = \\frac{6 \\pm \\sqrt{36-8}}{4} = \\frac{6 \\pm \\sqrt{28}}{4} = \\frac{3 \\pm \\sqrt{7}}{2}$.\nThe root $\\tau_1 = \\frac{3 + \\sqrt{7}}{2}  \\frac{3+2}{2} = 2.5$, which violates $1  \\tau$.\nThe root $\\tau_2 = \\frac{3 - \\sqrt{7}}{2}$. Since $2  \\sqrt{7}  3$, we have $0  3-\\sqrt{7}  1$, so $0  \\tau_2  1/2$. This satisfies the conditions $1  \\tau$ and $0 \\le \\tau$.\nThus, the correct threshold is $\\tau = \\frac{3 - \\sqrt{7}}{2}$, and the support is $S=\\{1, 2\\}$.\n\nNow we compute the entmax-$1.5$ probabilities:\n$p_3 = 0$ since $s_3=0  \\tau$.\nFor $ i \\in \\{1, 2\\}$, $p_i = [0.5(s_i - \\tau)]^2 = 0.25(s_i - \\tau)^2$.\n$$\np_1 = 0.25\\left(2 - \\frac{3 - \\sqrt{7}}{2}\\right)^2 = 0.25\\left(\\frac{4 - 3 + \\sqrt{7}}{2}\\right)^2 = 0.25\\frac{(1 + \\sqrt{7})^2}{4} = \\frac{1 + 2\\sqrt{7} + 7}{16} = \\frac{8 + 2\\sqrt{7}}{16} = \\frac{4 + \\sqrt{7}}{8}\n$$\n$$\np_2 = 0.25\\left(1 - \\frac{3 - \\sqrt{7}}{2}\\right)^2 = 0.25\\left(\\frac{2 - 3 + \\sqrt{7}}{2}\\right)^2 = 0.25\\frac{(-1 + \\sqrt{7})^2}{4} = \\frac{1 - 2\\sqrt{7} + 7}{16} = \\frac{8 - 2\\sqrt{7}}{16} = \\frac{4 - \\sqrt{7}}{8}\n$$\nThe entmax-$1.5$ distribution is $p_{\\text{entmax}} = \\left[\\frac{4 + \\sqrt{7}}{8}, \\frac{4 - \\sqrt{7}}{8}, 0\\right]$.\n\n**4. Interpretation**\n\nIn the context of attention over CT radiomics patches, the three mappings offer different strategies for focusing on suspicious regions:\n- **Softmax** (approx. $[0.665, 0.245, 0.090]$) is a non-sparse mapping that *diffuses* attention. It assigns some relevance to all patches, even the one with zero score. This is useful if malignancy is indicated by a combination of features from multiple patches.\n- **Sparsemax** ($[1, 0, 0]$) is a sparse, \"hard\" mapping that *isolates* a single patch. It puts all attention on the most suspicious patch and prunes all others. This is an aggressive \"winner-takes-all\" strategy, ideal for identifying a single, unambiguous lesion.\n- **Entmax-$1.5$** (approx. $[0.831, 0.169, 0]$) is a sparse mapping that offers a compromise. It prunes the least relevant patch but still distributes attention between the top two. It is better than softmax at focusing attention but less extreme than sparsemax, allowing the model to consider a primary and a secondary suspicious region.\n\nTherefore, sparsemax is the mechanism that best isolates a single suspicious patch, while softmax is the one that best diffuses attention.\n\n**5. Final Answer Calculation**\n\nThe problem asks for the attention weight assigned by the entmax-$1.5$ distribution to the highest-scoring patch ($s_1 = 2$). This is the value of $p_1$ we calculated for the entmax-$1.5$ distribution.\n$$\np_1 = \\frac{4 + \\sqrt{7}}{8}\n$$\nThis is the exact, simplified analytical expression.", "answer": "$$\\boxed{\\frac{4 + \\sqrt{7}}{8}}$$", "id": "4529607"}]}