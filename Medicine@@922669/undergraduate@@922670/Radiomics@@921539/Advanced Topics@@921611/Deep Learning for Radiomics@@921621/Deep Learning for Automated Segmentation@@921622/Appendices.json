{"hands_on_practices": [{"introduction": "To effectively use and design deep learning models like the U-Net, one must first grasp how data flows through its architecture. This exercise provides hands-on practice in calculating the spatial dimensions of feature maps as they pass through the network's encoding and decoding paths. By mastering these fundamental calculations, you will gain a deeper intuition for network design and the importance of maintaining dimensional consistency, especially for the skip connections that are central to the U-Net's success in segmentation tasks [@problem_id:4535986].", "problem": "A radiomics pipeline uses a U-shaped convolutional neural network (U-Net) for automated lesion segmentation on two-dimensional medical images. Consider a network configured as follows on an input of spatial size $256 \\times 256$ (height and width):\n\n- Contracting path (encoder): At each of two levels, apply two two-dimensional convolutions with kernel size $3 \\times 3$, stride $1$, and zero padding $0$ (valid convolution), followed by max-pooling with kernel size $2 \\times 2$ and stride $2$.\n- Bottleneck: Apply two two-dimensional convolutions with kernel size $3 \\times 3$, stride $1$, and zero padding $0$.\n- Expanding path (decoder): At each of two levels, apply a transposed convolution with kernel size $2 \\times 2$, stride $2$, zero padding $0$, and output padding $0$, then concatenate (along the channel dimension) with the encoderâ€™s corresponding pre-pooling feature map after center-cropping that encoder feature map as needed to match spatial dimensions. After each concatenation, apply two two-dimensional convolutions with kernel size $3 \\times 3$, stride $1$, and zero padding $0$.\n- Output head: Apply a single two-dimensional convolution with kernel size $1 \\times 1$ and stride $1$ to produce the segmentation logits.\n\nStarting from the standard definitions for the spatial size of the output of a discrete convolution, discrete max-pooling, and discrete transposed convolution, determine the spatial sizes of the feature maps after every operation along the encoder, bottleneck, and decoder, and verify the dimensional consistency at both skip concatenations by computing the necessary center-crop sizes per spatial dimension. Finally, compute the total number of pixels that are cropped across both skip connections (i.e., the sum of all removed pixels from both encoder feature maps due to center-cropping), and report that total as a single integer with no units.", "solution": "The problem is valid. It is a well-posed, scientifically grounded question about the architecture of a U-Net, which is a standard model in deep learning for image segmentation. All parameters are clearly defined, and the task is to perform a series of calculations based on established formulas for convolutional neural network operations.\n\nThe solution requires tracking the spatial dimensions of the feature maps through the network. We begin by defining the formulas for the output size of each type of layer used. Let the input feature map have a spatial size of $H_{\\text{in}} \\times W_{\\text{in}}$, and the output be $H_{\\text{out}} \\times W_{\\text{out}}$. For simplicity, as the operations are symmetric, we only show the calculation for one dimension, $H$.\n\n1.  **Two-Dimensional Convolution**: For a kernel of size $K \\times K$, stride $S$, and padding $P$, the output dimension is $H_{\\text{out}} = \\lfloor \\frac{H_{\\text{in}} + 2P - K}{S} \\rfloor + 1$. In our case, the convolutions are specified as \"valid,\" meaning zero padding ($P=0$), with a kernel size of $K=3$ and stride $S=1$. The formula simplifies to $H_{\\text{out}} = H_{\\text{in}} - 3 + 1 = H_{\\text{in}} - 2$.\n\n2.  **Max-Pooling**: For a kernel of size $K \\times K$ and stride $S$, the output dimension is $H_{\\text{out}} = \\lfloor \\frac{H_{\\text{in}} - K}{S} \\rfloor + 1$. In our case, the kernel size is $K=2$ and stride $S=2$. Assuming the input dimension is even, this simplifies to $H_{\\text{out}} = \\frac{H_{\\text{in}}}{2}$.\n\n3.  **Transposed Convolution**: For a kernel of size $K \\times K$, stride $S$, padding $P$, and output padding $O_p$, the output dimension is $H_{\\text{out}} = (H_{\\text{in}} - 1)S - 2P + K + O_p$. In our case, $K=2$, $S=2$, $P=0$, and $O_p=0$. The formula simplifies to $H_{\\text{out}} = (H_{\\text{in}} - 1) \\times 2 + 2 = 2H_{\\text{in}}$.\n\nWe now trace the spatial dimensions through the network, starting with the $256 \\times 256$ input.\n\n**Contracting Path (Encoder)**\n\n*   **Encoder Level 1:**\n    *   Input: $256 \\times 256$\n    *   First $3 \\times 3$ convolution: $256 - 2 = 254$. Size is $254 \\times 254$.\n    *   Second $3 \\times 3$ convolution: $254 - 2 = 252$. Size is $252 \\times 252$. This is the feature map for the first skip connection, let's call its size $S_1 = 252 \\times 252$.\n    *   $2 \\times 2$ max-pooling: $252 / 2 = 126$. Size is $126 \\times 126$.\n\n*   **Encoder Level 2:**\n    *   Input: $126 \\times 126$\n    *   First $3 \\times 3$ convolution: $126 - 2 = 124$. Size is $124 \\times 124$.\n    *   Second $3 \\times 3$ convolution: $124 - 2 = 122$. Size is $122 \\times 122$. This is the feature map for the second skip connection, let's call its size $S_2 = 122 \\times 122$.\n    *   $2 \\times 2$ max-pooling: $122 / 2 = 61$. Size is $61 \\times 61$.\n\n**Bottleneck**\n\n*   Input: $61 \\times 61$\n*   First $3 \\times 3$ convolution: $61 - 2 = 59$. Size is $59 \\times 59$.\n*   Second $3 \\times 3$ convolution: $59 - 2 = 57$. Size is $57 \\times 57$.\n\n**Expanding Path (Decoder)**\n\n*   **Decoder Level 1:**\n    *   Input from bottleneck: $57 \\times 57$\n    *   $2 \\times 2$ transposed convolution: $57 \\times 2 = 114$. Size is $114 \\times 114$. This is the upsampled feature map.\n    *   **Skip Connection 1 (from Encoder Level 2):** To concatenate, the feature map from the encoder ($S_2 = 122 \\times 122$) must be cropped to match the decoder's upsampled map ($114 \\times 114$).\n        *   Crop size per dimension: $122 - 114 = 8$ pixels.\n        *   Center-cropping removes $8/2 = 4$ pixels from each of the four sides.\n        *   Number of pixels removed from this feature map: $(122 \\times 122) - (114 \\times 114) = 14884 - 12996 = 1888$.\n    *   After concatenation, the size is $114 \\times 114$.\n    *   First $3 \\times 3$ convolution: $114 - 2 = 112$. Size is $112 \\times 112$.\n    *   Second $3 \\times 3$ convolution: $112 - 2 = 110$. Size is $110 \\times 110$.\n\n*   **Decoder Level 2:**\n    *   Input from Decoder Level 1: $110 \\times 110$\n    *   $2 \\times 2$ transposed convolution: $110 \\times 2 = 220$. Size is $220 \\times 220$. This is the upsampled feature map.\n    *   **Skip Connection 2 (from Encoder Level 1):** To concatenate, the feature map from the encoder ($S_1 = 252 \\times 252$) must be cropped to match the decoder's upsampled map ($220 \\times 220$).\n        *   Crop size per dimension: $252 - 220 = 32$ pixels.\n        *   Center-cropping removes $32/2 = 16$ pixels from each of the four sides.\n        *   Number of pixels removed from this feature map: $(252 \\times 252) - (220 \\times 220) = 63504 - 48400 = 15104$.\n    *   After concatenation, the size is $220 \\times 220$.\n    *   First $3 \\times 3$ convolution: $220 - 2 = 218$. Size is $218 \\times 218$.\n    *   Second $3 \\times 3$ convolution: $218 - 2 = 216$. Size is $216 \\times 216$.\n\n**Output Head**\n\n*   Input: $216 \\times 216$\n*   $1 \\times 1$ convolution ($K=1, S=1, P=0$): $H_{\\text{out}} = (216 + 2 \\times 0 - 1) / 1 + 1 = 216$. The size remains $216 \\times 216$.\n\n**Final Calculation**\n\nThe problem asks for the total number of pixels cropped across both skip connections. We sum the pixels removed at each concatenation step.\n\n*   Pixels cropped at the first skip connection (Decoder Level 1): $1888$.\n*   Pixels cropped at the second skip connection (Decoder Level 2): $15104$.\n\nTotal cropped pixels = $1888 + 15104 = 16992$.", "answer": "$$\n\\boxed{16992}\n$$", "id": "4535986"}, {"introduction": "A segmentation model's output is a map of probabilities, but how do we objectively measure its correctness against a ground-truth reference? This practice introduces the fundamental metrics used to evaluate binary segmentation performance, such as sensitivity and precision, derived from the confusion matrix. Working through this problem will help you quantify model accuracy and understand the critical trade-offs that arise when choosing a probability threshold to create the final binary mask [@problem_id:4535965].", "problem": "A biomedical image segmentation pipeline for Radiomics uses a U-shaped convolutional neural network (U-Net), a type of Convolutional Neural Network (CNN), to produce a per-voxel probability map for a binary task: lesion versus background. After applying a fixed probability threshold $t$ to the network output, a predicted binary mask is obtained and compared to a manual ground-truth mask, yielding a confusion matrix with counts of True Positive ($TP$), False Positive ($FP$), True Negative ($TN$), and False Negative ($FN$) voxels. Assume an axial slice contains $N = 100{,}000$ voxels, with ground-truth lesion prevalence $p = 0.02$, so there are $pN = 2{,}000$ lesion voxels and $(1 - p)N = 98{,}000$ background voxels. For threshold $t = 0.6$, suppose the resulting confusion matrix is $TP = 1{,}620$, $FP = 1{,}800$, $TN = 96{,}200$, and $FN = 380$.\n\nUsing the standard, first-principles definitions of binary classification rates for segmentation derived from the confusion matrix, compute the sensitivity, specificity, and precision for the threshold $t = 0.6$. Then, starting from those foundational definitions and without invoking any shortcut formulas provided to you, explain how changing the threshold $t$ and changing the prevalence $p$ affects these three metrics, emphasizing the role of the denominators in their definitions and conditioning on the true class where appropriate. Finally, report the harmonic mean of sensitivity and precision as your single numeric result, rounded to $4$ significant figures. Express the result as a dimensionless decimal number.", "solution": "The U-shaped convolutional neural network (U-Net) produces a probability for each voxel of belonging to the lesion class. Thresholding at $t = 0.6$ yields a predicted binary mask. The confusion matrix counts $TP$, $FP$, $TN$, and $FN$ summarize the joint outcomes between predictions and ground truth.\n\nFrom first principles of binary classification in segmentation, sensitivity (also called the true positive rate or recall) measures the conditional probability of predicting lesion given that the voxel is truly lesion. Specificity (true negative rate) measures the conditional probability of predicting background given that the voxel is truly background. Precision (positive predictive value) measures the conditional probability that a voxel is truly lesion given that it is predicted as lesion. These are defined using the confusion matrix as follows:\n- Sensitivity is the conditional probability $\\Pr(\\hat{Y} = 1 \\mid Y = 1)$, which is computed empirically as the fraction of true lesion voxels that are correctly predicted: \n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN}.\n$$\n- Specificity is the conditional probability $\\Pr(\\hat{Y} = 0 \\mid Y = 0)$, computed as the fraction of true background voxels that are correctly predicted:\n$$\n\\text{specificity} = \\frac{TN}{TN + FP}.\n$$\n- Precision is the conditional probability $\\Pr(Y = 1 \\mid \\hat{Y} = 1)$, computed as the fraction of predicted lesion voxels that are truly lesion:\n$$\n\\text{precision} = \\frac{TP}{TP + FP}.\n$$\n\nUsing the provided counts $TP = 1{,}620$, $FP = 1{,}800$, $TN = 96{,}200$, and $FN = 380$:\n1. Compute sensitivity:\n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN} = \\frac{1{,}620}{1{,}620 + 380} = \\frac{1{,}620}{2{,}000} = \\frac{81}{100} = 0.81.\n$$\n2. Compute specificity:\n$$\n\\text{specificity} = \\frac{TN}{TN + FP} = \\frac{96{,}200}{96{,}200 + 1{,}800} = \\frac{96{,}200}{98{,}000} = \\frac{481}{490} \\approx 0.981632653\\ldots\n$$\n3. Compute precision:\n$$\n\\text{precision} = \\frac{TP}{TP + FP} = \\frac{1{,}620}{1{,}620 + 1{,}800} = \\frac{1{,}620}{3{,}420} = \\frac{9}{19} \\approx 0.473684210\\ldots\n$$\n\nDependence on threshold $t$:\n- Thresholding maps probabilities to binary predictions by declaring a voxel lesion if its predicted probability exceeds $t$. As $t$ decreases, more voxels are labeled as lesion. From the definitions above:\n  - $TP$ generally increases (more true lesion voxels exceed the lower threshold), and $FN$ generally decreases, so the denominator $TP + FN$ is fixed by the ground truth; thus the sensitivity $\\frac{TP}{TP + FN}$ tends to increase as $t$ decreases.\n  - $FP$ generally increases (more background voxels cross the lower threshold spuriously), and $TN$ generally decreases, so specificity $\\frac{TN}{TN + FP}$ tends to decrease as $t$ decreases.\n  - Precision $\\frac{TP}{TP + FP}$ can either increase or decrease as $t$ changes, depending on the balance between increases in $TP$ versus increases in $FP$. If $FP$ grows faster than $TP$, precision decreases; if $TP$ grows faster than $FP$, precision increases.\nThese monotonicities arise because the denominators $TP + FN$ and $TN + FP$ are fixed by the ground-truth class sizes, while the numerators shift with threshold-induced decision changes.\n\nDependence on prevalence $p$:\n- Let $p = \\Pr(Y = 1)$ denote lesion prevalence and $1 - p = \\Pr(Y = 0)$ denote background prevalence. For a given classifier operating at a fixed threshold $t$, define the true positive rate $\\text{TPR} = \\Pr(\\hat{Y} = 1 \\mid Y = 1)$ and the false positive rate $\\text{FPR} = \\Pr(\\hat{Y} = 1 \\mid Y = 0)$. Then, in expectation over the population,\n$$\nTP \\approx pN \\cdot \\text{TPR}, \\quad FN \\approx pN \\cdot (1 - \\text{TPR}), \\quad FP \\approx (1 - p)N \\cdot \\text{FPR}, \\quad TN \\approx (1 - p)N \\cdot (1 - \\text{FPR}).\n$$\nThus,\n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN} = \\frac{pN \\cdot \\text{TPR}}{pN \\cdot \\text{TPR} + pN \\cdot (1 - \\text{TPR})} = \\text{TPR},\n$$\nwhich is independent of $p$ because the denominator $TP + FN$ conditions on $Y = 1$ (the true lesion class). Similarly,\n$$\n\\text{specificity} = \\frac{TN}{TN + FP} = \\frac{(1 - p)N \\cdot (1 - \\text{FPR})}{(1 - p)N \\cdot (1 - \\text{FPR}) + (1 - p)N \\cdot \\text{FPR}} = 1 - \\text{FPR},\n$$\nwhich is also independent of $p$ because the denominator $TN + FP$ conditions on $Y = 0$ (the true background class). In contrast, precision depends on prevalence:\n$$\n\\text{precision} = \\frac{TP}{TP + FP} = \\frac{pN \\cdot \\text{TPR}}{pN \\cdot \\text{TPR} + (1 - p)N \\cdot \\text{FPR}} = \\frac{p \\cdot \\text{TPR}}{p \\cdot \\text{TPR} + (1 - p) \\cdot \\text{FPR}},\n$$\nwhich increases with $p$ (holding $\\text{TPR}$ and $\\text{FPR}$ fixed). Therefore, precision is sensitive to class prevalence, while sensitivity and specificity are not, due to their denominators conditioning on the true class.\n\nFinally, the requested single numeric result is the harmonic mean of sensitivity and precision. The harmonic mean $H$ of two positive numbers $a$ and $b$ is defined by\n$$\nH = \\frac{2}{\\frac{1}{a} + \\frac{1}{b}} = \\frac{2ab}{a + b}.\n$$\nSetting $a$ to the sensitivity and $b$ to the precision:\n$$\na = \\frac{81}{100}, \\quad b = \\frac{9}{19}.\n$$\nCompute\n$$\nH = \\frac{2ab}{a + b} = \\frac{2 \\cdot \\frac{81}{100} \\cdot \\frac{9}{19}}{\\frac{81}{100} + \\frac{9}{19}} = \\frac{\\frac{1458}{1900}}{\\frac{2439}{1900}} = \\frac{1458}{2439} = \\frac{162}{271} \\approx 0.597786\\ldots\n$$\nRounded to $4$ significant figures, the harmonic mean is $0.5978$.", "answer": "$$\\boxed{0.5978}$$", "id": "4535965"}, {"introduction": "The raw output from a segmentation network is rarely perfect and often contains small, spurious regions that are not clinically relevant. This hands-on practice simulates a critical post-processing step used in radiomics pipelines: filtering the segmented objects based on their physical size. By learning to identify connected components and calculate their volume, you can develop robust workflows that clean segmentation masks and improve the reliability of automated analysis [@problem_id:4535929].", "problem": "You are given a three-dimensional binary mask produced by a segmentation model such as the U-shaped Convolutional Neural Network (U-Net). In Radiology Informatics (radiomics), a common post-processing step is to remove spurious detections by filtering connected components in the mask based on their physical volumes. The binary mask can be modeled as a discrete field $M \\in \\{0,1\\}^{N_x \\times N_y \\times N_z}$ where $N_x$, $N_y$, and $N_z$ are positive integers. The grid has voxel spacing $s = (s_x, s_y, s_z)$ measured in millimeters (mm), where $s_x > 0$, $s_y > 0$, and $s_z > 0$.\n\nFundamental bases and core definitions to use:\n- A connected component is defined with respect to a connectivity parameter $c \\in \\{6,18,26\\}$. For $c=6$, two voxels are connected if and only if their indices differ by exactly $1$ along a single axis and are equal along the other two axes (Manhattan distance of $1$). For $c=26$, two voxels are connected if and only if the Chebyshev distance between their indices is $1$. For $c=18$, connectivity includes face and edge neighbors but not corner neighbors (that is, all neighbors with Chebyshev distance $1$ except those where all three axis indices change simultaneously).\n- The physical volume of a single voxel is $v_{\\text{vox}} = s_x \\cdot s_y \\cdot s_z$ in $\\text{mm}^3$.\n- The physical volume of a connected component containing $n$ voxels is $V_{\\text{mm}^3} = n \\cdot v_{\\text{vox}}$ in $\\text{mm}^3$.\n- Convert $\\text{mm}^3$ to milliliters (mL) using the well-tested fact $1\\,\\text{mL} = 1\\,\\text{cm}^3 = 1000\\,\\text{mm}^3$, hence $V_{\\text{mL}} = V_{\\text{mm}^3}/1000$.\n\nYour task is to:\n- Compute connected components in each mask under the specified connectivity $c$.\n- Compute the physical volume in milliliters for each connected component using the given voxel spacing.\n- Retain only those components whose volume $V_{\\text{mL}}$ satisfies $V_{\\min} \\leq V_{\\text{mL}} \\leq V_{\\max}$, where $V_{\\min}$ and $V_{\\max}$ are given in milliliters (mL), and the comparison is inclusive.\n- For each test case, report the number of retained connected components (an integer) and the total retained volume in milliliters (a float). Express the final total volume in milliliters (mL) rounded to $6$ decimal places.\n\nAngle units are not applicable. All physical units must be treated explicitly: spacings are given in millimeters (mm) and volumes must be reported in milliliters (mL).\n\nTest suite:\nUse the following masks, spacings, thresholds, and connectivities. All tensor indices are zero-based.\n\n- Test case $1$ (happy path):\n  - Mask shape: $(5,5,5)$.\n  - Voxel spacing: $s = (1,1,1)$ mm.\n  - Connectivity: $c=6$.\n  - Components:\n    - Component $\\mathcal{A}$: set $M[i,0,0] = 1$ for $i \\in \\{0,1,2,3,4\\}$ (a line of $5$ voxels).\n    - Component $\\mathcal{B}$: set $M[x,y,z] = 1$ for $x \\in \\{2,3,4\\}$, $y \\in \\{2,3\\}$, $z \\in \\{2,3\\}$ (a $3 \\times 2 \\times 2$ block of $12$ voxels).\n  - Volume thresholds in mL: $V_{\\min} = 0.008$, $V_{\\max} = 0.015$.\n\n- Test case $2$ (anisotropic spacing and inclusive upper bound):\n  - Mask shape: $(8,8,6)$.\n  - Voxel spacing: $s = (0.5,0.5,2.0)$ mm.\n  - Connectivity: $c=26$.\n  - Components:\n    - Component $\\mathcal{C}$: set $M[x,y,z] = 1$ for $x \\in \\{0,1,2,3,4\\}$, $y \\in \\{0,1,2,3,4\\}$, $z \\in \\{0,1,2,3\\}$ (a $5 \\times 5 \\times 4$ block of $100$ voxels).\n    - Component $\\mathcal{D}$: set $M[x,y,z] = 1$ for $x \\in \\{6,7\\}$, $y \\in \\{6,7\\}$, $z \\in \\{1,2,3,4,5\\}$ (a $2 \\times 2 \\times 5$ block of $20$ voxels).\n  - Volume thresholds in mL: $V_{\\min} = 0.025$, $V_{\\max} = 0.050$.\n\n- Test case $3$ (boundary equality on the threshold):\n  - Mask shape: $(5,5,5)$.\n  - Voxel spacing: $s = (1,1,1)$ mm.\n  - Connectivity: $c=6$.\n  - Components:\n    - Component $\\mathcal{E}$: set $M[x,y,z] = 1$ for $x \\in \\{0,1,2\\}$, $y \\in \\{0,1,2\\}$, $z \\in \\{0\\}$ (a $3 \\times 3 \\times 1$ block of $9$ voxels).\n    - Component $\\mathcal{F}$: set $M[x,y,z] = 1$ for $x \\in \\{0,1\\}$, $y \\in \\{0,1,2,3,4\\}$, $z \\in \\{2\\}$ (a $2 \\times 5 \\times 1$ block of $10$ voxels).\n  - Volume thresholds in mL: $V_{\\min} = 0.010$, $V_{\\max} = 0.010$.\n\n- Test case $4$ (empty mask edge case):\n  - Mask shape: $(4,4,4)$.\n  - Voxel spacing: $s = (1,1,1)$ mm.\n  - Connectivity: $c=6$.\n  - Components: none, that is $M[i,j,k] = 0$ for all indices.\n  - Volume thresholds in mL: $V_{\\min} = 0.000$, $V_{\\max} = 100.000$.\n\n- Test case $5$ (diagonal adjacency excluded by $6$-connectivity):\n  - Mask shape: $(3,3,3)$.\n  - Voxel spacing: $s = (1,1,1)$ mm.\n  - Connectivity: $c=6$.\n  - Components: set $M[0,0,0] = 1$ and $M[1,1,1] = 1$ (two voxels diagonally adjacent).\n  - Volume thresholds in mL: $V_{\\min} = 0.001$, $V_{\\max} = 1.000$.\n\n- Test case $6$ (diagonal adjacency included by $26$-connectivity):\n  - Mask shape: $(3,3,3)$.\n  - Voxel spacing: $s = (1,1,1)$ mm.\n  - Connectivity: $c=26$.\n  - Components: the same voxel placements as in test case $5$.\n  - Volume thresholds in mL: $V_{\\min} = 0.001$, $V_{\\max} = 1.000$.\n\nFinal output specification:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is a list of two elements of the form $[N_{\\text{retained}},V_{\\text{total\\_mL}}]$, with $N_{\\text{retained}}$ an integer and $V_{\\text{total\\_mL}}$ a float rounded to $6$ decimal places. For example, the output format is: $[[n_1,v_1],[n_2,v_2],\\dots]$.", "solution": "The problem requires the implementation of a standard radiomics post-processing workflow: filtering connected components in a three-dimensional binary mask based on their physical volume. The solution comprises several distinct logical steps: connected component labeling, volume calculation for each component, filtering based on volume thresholds, and aggregation of results.\n\nFirst, we formalize the input. For each test case, we are given a binary mask $M \\in \\{0,1\\}^{N_x \\times N_y \\times N_z}$, a voxel spacing vector $s = (s_x, s_y, s_z)$ in millimeters, a connectivity parameter $c \\in \\{6, 18, 26\\}$, and minimum and maximum volume thresholds $V_{\\min}$ and $V_{\\max}$ in milliliters.\n\nThe core of the task is to identify the connected components within the mask $M$. A connected component is a set of foreground voxels (value of $1$) where each voxel is connected to at least one other voxel in the set, and no voxel in the set is connected to any voxel outside the set. The definition of \"connected\" depends on the parameter $c$. We utilize a standard algorithm for this task: connected component labeling. The `scipy.ndimage.label` function provides a robust and efficient implementation. The connectivity parameter $c$ is mapped to a structuring element that defines the neighborhood for connectivity checks:\n- For $c=6$ (face connectivity), two voxels are neighbors if they are adjacent along one axis. This corresponds to a `connectivity=1` in `scipy.ndimage.generate_binary_structure(3, 1)`.\n- For $c=18$ (face and edge connectivity), two voxels are neighbors if they are adjacent along faces or edges. This corresponds to `connectivity=2` in `scipy.ndimage.generate_binary_structure(3, 2)`.\n- For $c=26$ (face, edge, and corner connectivity), two voxels are neighbors if they are adjacent along faces, edges, or corners. This corresponds to `connectivity=3` in `scipy.ndimage.generate_binary_structure(3, 3)`.\n\nThe `label` function returns a new grid, `labeled_mask`, where all voxels belonging to the same component are assigned a unique positive integer label, and a count of the total number of components found, `num_features`.\n\nOnce the components are identified and labeled, the next step is to calculate the physical volume of each. The physical volume of a single voxel is given by the product of its dimensions:\n$$v_{\\text{vox}} = s_x \\cdot s_y \\cdot s_z \\quad [\\text{mm}^3]$$\nThe volume of a connected component is the number of voxels, $n$, it contains, multiplied by the volume of a single voxel. The number of voxels for each labeled component can be efficiently found using `scipy.ndimage.sum`. The volume in cubic millimeters is then:\n$$V_{\\text{mm}^3} = n \\cdot v_{\\text{vox}}$$\nThe problem requires the volume in milliliters (mL). Using the given conversion factor $1\\,\\text{mL} = 1000\\,\\text{mm}^3$, the volume is:\n$$V_{\\text{mL}} = \\frac{V_{\\text{mm}^3}}{1000} = \\frac{n \\cdot s_x \\cdot s_y \\cdot s_z}{1000}$$\n\nThe third step is filtering. Each component's calculated volume $V_{\\text{mL}}$ is compared against the given thresholds $V_{\\min}$ and $V_{\\max}$. A component is retained if and only if its volume falls within the inclusive range:\n$$V_{\\min} \\leq V_{\\text{mL}} \\leq V_{\\max}$$\n\nFinally, for each test case, we aggregate the results. We count the number of retained components, $N_{\\text{retained}}$, and compute their total volume, $V_{\\text{total\\_mL}}$, by summing the volumes of all retained components.\n$$V_{\\text{total\\_mL}} = \\sum_{i \\in \\text{retained components}} V_{\\text{mL}, i}$$\nThe final total volume is rounded to $6$ decimal places as specified. The result for each test case is a pair $[N_{\\text{retained}}, V_{\\text{total\\_mL}}]$.", "answer": "```\n[[1, 0.012000], [1, 0.050000], [1, 0.010000], [0, 0.0], [2, 0.002000], [1, 0.002000]]\n```", "id": "4535929"}]}