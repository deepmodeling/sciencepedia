{"hands_on_practices": [{"introduction": "Before training a Generative Adversarial Network, we must configure the data pipeline—a critical step that involves more than just loading images. This exercise [@problem_id:4541955] tackles a core engineering challenge: optimizing patch dimensions and batch size. You will learn to balance the statistical need for sufficient spatial context, essential for capturing complex radiomic textures like those measured by a Gray-Level Co-Occurrence Matrix (GLCM), against the hard physical constraint of limited GPU memory.", "problem": "You are tasked with designing a patch-based training pipeline for Generative Adversarial Networks (GANs) used for data augmentation in radiomics. The key requirement is that the patch dimensions must provide sufficient spatial context for computing texture features derived from the Gray-Level Co-Occurrence Matrix (GLCM), while ensuring that the total Graphics Processing Unit (GPU) memory usage does not exceed a specified budget.\n\nBegin from the following foundational base:\n- The Gray-Level Co-Occurrence Matrix (GLCM) for a given offset vector $\\Delta = (d_x, d_y)$ is defined by counting all ordered pairs of pixel locations $(\\mathbf{p}, \\mathbf{q})$ in a discrete image such that $\\mathbf{q} = \\mathbf{p} + \\Delta$, subject to being within patch bounds. For a rectangular patch of height $H$ and width $W$, the number of valid position pairs for offset $\\Delta$ equals the count of positions $\\mathbf{p}$ whose shifted location $\\mathbf{q}$ remains inside the patch. This yields the count $(H - |d_y|) \\cdot (W - |d_x|)$ when $H \\ge |d_y| + 1$ and $W \\ge |d_x| + 1$, and zero otherwise.\n- The total memory used by arrays stored during training for both the generator and the discriminator can be modeled as a product of the per-sample tensor size and an aggregate multiplicative factor accounting for forward activations, backward gradients, and dual-network storage. Let this factor be $\\gamma$. With batch size $B$, patch dimensions $H \\times W$, number of channels $C$, and bytes per element $b$, the activation-related memory is $\\gamma \\cdot B \\cdot H \\cdot W \\cdot C \\cdot b$. Let the combined parameter memory for both networks be $P$. The total memory is then $P + \\gamma \\cdot B \\cdot H \\cdot W \\cdot C \\cdot b$, which must not exceed the available budget $M$ bytes.\n\nYour program must determine integer patch dimensions $H$ and $W$ and an integer batch size $B$ satisfying:\n- For each offset $\\Delta_i = (d_{x,i}, d_{y,i})$ in a given set $\\mathcal{O}$, the number of valid GLCM pairs $(H - |d_{y,i}|) \\cdot (W - |d_{x,i}|)$ is at least a required minimum $K$.\n- The GPU memory constraint $P + \\gamma \\cdot B \\cdot H \\cdot W \\cdot C \\cdot b \\le M$ holds, with all memory quantities expressed in bytes.\n- If no feasible $(H, W, B)$ exists under the given constraints, you must output the triple $(0, 0, 0)$ for that test case.\n\nTo make the choice unique and deterministic, you must select $(H, W, B)$ according to the following rule:\n- Among all $(H, W)$ that satisfy the GLCM constraints for the given offsets and $K$, choose the pair that minimizes the area $H \\cdot W$.\n- Break ties by choosing the pair with the smallest absolute aspect difference $|H - W|$.\n- Break any remaining ties by choosing the smallest $H$.\n- Given the chosen $(H, W)$, choose the largest integer $B$ permitted by the memory budget. If $B < 1$, declare infeasibility by outputting $(0, 0, 0)$.\n\nAll answers must be integers, and all memory quantities must be handled and expressed in bytes.\n\nUse the following test suite of $4$ test cases, each specified by $(M, P, \\gamma, C, b, \\mathcal{O}, K)$:\n\n- Test case $1$ (general case):\n  - $M = 1{,}000{,}000{,}000$ bytes, $P = 200{,}000{,}000$ bytes, $\\gamma = 16$, $C = 1$, $b = 4$ bytes,\n  - $\\mathcal{O} = \\{(1, 0), (0, 1), (1, 1)\\}$,\n  - $K = 80{,}000$.\n\n- Test case $2$ (boundary memory case):\n  - $M = 192$ bytes, $P = 64$ bytes, $\\gamma = 16$, $C = 1$, $b = 4$ bytes,\n  - $\\mathcal{O} = \\{(1, 0)\\}$,\n  - $K = 1$.\n\n- Test case $3$ (infeasible case due to tight budget and large context requirement):\n  - $M = 60{,}000{,}000$ bytes, $P = 50{,}000{,}000$ bytes, $\\gamma = 16$, $C = 1$, $b = 4$ bytes,\n  - $\\mathcal{O} = \\{(10, 10)\\}$,\n  - $K = 1{,}000{,}000$.\n\n- Test case $4$ (anisotropic offsets with moderate budget):\n  - $M = 300{,}000{,}000$ bytes, $P = 100{,}000{,}000$ bytes, $\\gamma = 16$, $C = 1$, $b = 4$ bytes,\n  - $\\mathcal{O} = \\{(7, 0), (0, 1)\\}$,\n  - $K = 20{,}000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result formatted as a list $[H,W,B]$ and with no spaces, for example, $[[H_1,W_1,B_1],[H_2,W_2,B_2],[H_3,W_3,B_3],[H_4,W_4,B_4]]$.", "solution": "The problem asks for the determination of optimal integer patch dimensions, $H$ and $W$, and an integer batch size, $B$, for training a Generative Adversarial Network (GAN) in a radiomics context. The solution must satisfy constraints on Gray-Level Co-Occurrence Matrix (GLCM) statistical sufficiency and total GPU memory usage, while adhering to a deterministic multi-level optimization criterion.\n\nThe problem can be decomposed into two sequential subproblems:\n1.  First, find the optimal patch dimensions $(H, W)$ that satisfy the GLCM requirements and the specified tie-breaking rules, independent of the memory budget.\n2.  Second, given the optimal $(H, W)$, calculate the maximum possible integer batch size $B$ that adheres to the GPU memory constraint.\n\n### Part 1: Determining Optimal Patch Dimensions $(H, W)$\n\nThe GLCM constraint states that for a patch of size $H \\times W$ and for every offset $\\Delta_i = (d_{x,i}, d_{y,i})$ in a given set $\\mathcal{O}$, the number of valid pixel pairs must be at least $K$. The number of pairs is given by $(H - |d_{y,i}|) \\cdot (W - |d_{x,i}|)$. This leads to a system of inequalities:\n$$\n(H - |d_{y,i}|) \\cdot (W - |d_{x,i}|) \\ge K \\quad \\forall \\Delta_i \\in \\mathcal{O}\n$$\nAdditionally, for these counts to be non-zero, it must hold that $H > |d_{y,i}|$ and $W > |d_{x,i}|$ for all $i$. This is equivalent to $H \\ge d_{y,max} + 1$ and $W \\ge d_{x,max} + 1$, where $d_{y,max} = \\max_{i} |d_{y,i}|$ and $d_{x,max} = \\max_i |d_{x,i}|$.\n\nFor any given width $W > d_{x,max}$, the height $H$ must satisfy:\n$$\nH \\ge |d_{y,i}| + \\frac{K}{W - |d_{x,i}|} \\quad \\forall i\n$$\nSince $H$ must be an integer, the minimum required height for a given $W$, which we denote $H_{cand}(W)$, is:\n$$\nH_{cand}(W) = \\max \\left( \\{ d_{y,max} + 1 \\} \\cup \\left\\{ \\left\\lceil \\frac{K}{W - |d_{x,i}|} \\right\\rceil + |d_{y,i}| \\mid \\Delta_i \\in \\mathcal{O} \\right\\} \\right)\n$$\nA symmetric formula exists for the minimum width $W_{cand}(H)$ for a given height $H > d_{y,max}$.\n\nThe optimization goal is to find an integer pair $(H, W)$ that satisfies these constraints and:\n1.  Minimizes the area $A = H \\cdot W$.\n2.  For pairs with the same minimal area, minimizes the absolute aspect difference $|H - W|$.\n3.  For pairs that are still tied, minimizes the height $H$.\n\nThe optimal pair $(H, W)$ must lie on the boundary of the feasible region, meaning it will satisfy either $H = H_{cand}(W)$ or $W = W_{cand}(H)$. To ensure we find the global minimum, we must search along both \"axes\" of this boundary. The search strategy is as follows:\n\n1.  **Establish Search Bounds**: First, we find an initial feasible solution to bound the search space. A simple, provably feasible pair is $(H_0, W_0) = (d_{y,max} + \\lceil\\sqrt{K}\\rceil, d_{x,max} + \\lceil\\sqrt{K}\\rceil)$. The area of this pair, $A_0 = H_0 \\cdot W_0$, serves as an initial upper bound for the minimal area. Any optimal solution $(H, W)$ must satisfy $H \\cdot W \\le A_0$. This implies we only need to search $W$ up to a limit $W_{limit} = \\lfloor A_0 / (d_{y,max}+1) \\rfloor$ and $H$ up to $H_{limit} = \\lfloor A_0 / (d_{x,max}+1) \\rfloor$, as any larger values would produce an area greater than $A_0$ (since $H \\ge d_{y,max}+1$ and $W \\ge d_{x,max}+1$).\n\n2.  **Iterative Search**: We perform two searches. The first search iterates through integer values of $W$ from $d_{x,max}+1$ up to a dynamically updated search limit. For each $W$, we calculate $H_{cand}(W)$ and the resulting area $A = H_{cand}(W) \\cdot W$. We maintain a list of candidate pairs that achieve the minimum area found so far. If a new, smaller minimum area is found, the list is reset, and the search limit can be tightened, pruning the search space. The second search performs the symmetric operation, iterating through $H$ and calculating $W_{cand}(H)$.\n\n3.  **Tie-Breaking**: After both searches are complete, we have a list of all $(H,W)$ pairs that achieve the same global minimum area. We apply the tie-breaking rules to this list. We sort the candidates first by $|H-W|$ in ascending order, and then by $H$ in ascending order. The first pair in the sorted list is the unique optimal solution.\n\n### Part 2: Determining Batch Size $B$\n\nOnce the optimal patch dimensions $(H_{opt}, W_{opt})$ are determined, we find the largest integer batch size $B$ that satisfies the memory constraint:\n$$\nP + \\gamma \\cdot B \\cdot H_{opt} \\cdot W_{opt} \\cdot C \\cdot b \\le M\n$$\nwhere $M$ is the memory budget, $P$ is the parameter memory, $\\gamma$ is the memory factor, $C$ is the number of channels, and $b$ is the bytes per element.\n\nRearranging the inequality to solve for $B$:\n$$\nB \\le \\frac{M - P}{\\gamma \\cdot H_{opt} \\cdot W_{opt} \\cdot C \\cdot b}\n$$\nLet the available activation memory be $M_{avail} = M - P$ and the memory per sample in a batch be $M_{per\\_sample} = \\gamma \\cdot H_{opt} \\cdot W_{opt} \\cdot C \\cdot b$. The maximum integer batch size is:\n$$\nB = \\left\\lfloor \\frac{M_{avail}}{M_{per\\_sample}} \\right\\rfloor\n$$\nIf $M_{avail} < 0$ or the calculated $B$ is less than $1$, no feasible solution exists for the given parameters, and the output must be $(0, 0, 0)$. Otherwise, the final solution is the triple $(H_{opt}, W_{opt}, B)$.\nThis systematic, two-part approach ensures all constraints and optimization criteria are met, providing a correct and deterministic solution.", "answer": "```python\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates finding the optimal (H, W) and then calculating B.\n    \"\"\"\n    test_cases = [\n        (1_000_000_000, 200_000_000, 16, 1, 4, [(1, 0), (0, 1), (1, 1)], 80_000),\n        (192, 64, 16, 1, 4, [(1, 0)], 1),\n        (60_000_000, 50_000_000, 16, 1, 4, [(10, 10)], 1_000_000),\n        (300_000_000, 100_000_000, 16, 1, 4, [(7, 0), (0, 1)], 20_000),\n    ]\n\n    results = []\n    for case in test_cases:\n        M, P, gamma, C, b, O, K = case\n        \n        optimal_hw = find_optimal_hw(O, K)\n        \n        if optimal_hw is None:\n            results.append(\"[0,0,0]\")\n            continue\n\n        H, W = optimal_hw\n        \n        mem_avail = M - P\n        if mem_avail  0:\n            results.append(\"[0,0,0]\")\n            continue\n\n        mem_per_sample = gamma * H * W * C * b\n        if mem_per_sample = 0: # Should not happen with positive inputs\n            results.append(\"[0,0,0]\")\n            continue\n\n        B = mem_avail // mem_per_sample\n        \n        if B  1:\n            results.append(\"[0,0,0]\")\n        else:\n            results.append(f\"[{H},{W},{B}]\")\n            \n    print(f\"[{','.join(results)}]\")\n\ndef find_optimal_hw(O, K):\n    \"\"\"\n    Finds the optimal (H, W) pair based on GLCM constraints and tie-breaking rules.\n    \"\"\"\n    if not O:\n        dx_max, dy_max = 0, 0\n    else:\n        dx_max = max(abs(dx) for dx, dy in O)\n        dy_max = max(abs(dy) for dx, dy in O)\n\n    if K == 0:\n        return dy_max + 1, dx_max + 1\n\n    def ceil_div(a, b):\n        return (a + b - 1) // b\n\n    def calc_h_cand(w_val, offsets, k_val, dy_max_val):\n        h_cand = dy_max_val + 1\n        for dx, dy in offsets:\n            if w_val - abs(dx) = 0:\n                return float('inf') # Invalid W\n            required_h = ceil_div(k_val, w_val - abs(dx)) + abs(dy)\n            h_cand = max(h_cand, required_h)\n        return h_cand\n\n    def calc_w_cand(h_val, offsets, k_val, dx_max_val):\n        w_cand = dx_max_val + 1\n        for dx, dy in offsets:\n            if h_val - abs(dy) = 0:\n                return float('inf') # Invalid H\n            required_w = ceil_div(k_val, h_val - abs(dy)) + abs(dx)\n            w_cand = max(w_cand, required_w)\n        return w_cand\n\n    # Initial upper bound for area\n    s_k = math.isqrt(K - 1) + 1 if K > 0 else 0\n    h0, w0 = dy_max + s_k, dx_max + s_k\n    min_area = h0 * w0\n    candidates = [(h0, w0)]\n\n    # Search iterating W\n    w_search_limit = min_area // (dy_max + 1) if dy_max + 1 > 0 else min_area\n    for W_try in range(dx_max + 1, w_search_limit + 1):\n        if W_try * (dy_max + 1) > min_area:\n            break\n        H_cand = calc_h_cand(W_try, O, K, dy_max)\n        current_area = H_cand * W_try\n        \n        if current_area  min_area:\n            min_area = current_area\n            candidates = [(H_cand, W_try)]\n            # Prune search space\n            w_search_limit = min(w_search_limit, min_area // (dy_max + 1) if dy_max + 1 > 0 else min_area)\n        elif current_area == min_area:\n            candidates.append((H_cand, W_try))\n\n    # Search iterating H\n    h_search_limit = min_area // (dx_max + 1) if dx_max + 1 > 0 else min_area\n    for H_try in range(dy_max + 1, h_search_limit + 1):\n        if H_try * (dx_max + 1) > min_area:\n            break\n        W_cand = calc_w_cand(H_try, O, K, dx_max)\n        current_area = H_try * W_cand\n\n        if current_area  min_area:\n            min_area = current_area\n            candidates = [(H_try, W_cand)]\n            # Prune search space\n            h_search_limit = min(h_search_limit, min_area // (dx_max + 1) if dx_max + 1 > 0 else min_area)\n        elif current_area == min_area:\n            candidates.append((H_try, W_cand))\n\n    # Apply tie-breaking rules\n    final_candidates = [p for p in candidates if p[0] * p[1] == min_area]\n    \n    if not final_candidates:\n        return None\n\n    final_candidates.sort(key=lambda p: (abs(p[0] - p[1]), p[0]))\n    \n    return final_candidates[0]\n\nsolve()\n```", "id": "4541955"}, {"introduction": "After training a GAN, how can we objectively measure its success? A truly effective generator must not only create realistic-looking images but also replicate the underlying statistical distribution of the real data. This practice [@problem_id:4541982] moves beyond simple visual checks and introduces a powerful tool for this task: the Maximum Mean Discrepancy (MMD). You will implement MMD to quantitatively assess how closely the distribution of GAN-generated radiomic features matches the distribution of real features, providing a rigorous method for model evaluation.", "problem": "You are evaluating whether synthetic radiomics feature vectors, produced by Generative Adversarial Networks (GANs), match the distribution of real radiomics feature vectors. A principled way to quantify distributional similarity is through Maximum Mean Discrepancy (MMD), which compares mean embeddings in a Reproducing Kernel Hilbert Space induced by a positive definite kernel. Use the Radial Basis Function (RBF) kernel to instantiate this comparison. Build your solution from the following fundamental bases: the definition of a positive definite kernel and its induced inner product in a Reproducing Kernel Hilbert Space, the concept of empirical expectation as a sample mean, the Euclidean distance in feature space, and the exponential function’s role in the Radial Basis Function kernel. Your task is to implement a program that computes the squared MMD using the biased empirical estimator for two finite samples of radiomics feature vectors, with the RBF kernel bandwidth selected by the median heuristic.\n\nDefinitions and constraints you must use:\n- Maximum Mean Discrepancy (MMD): the distance between kernel mean embeddings of two distributions in a Reproducing Kernel Hilbert Space.\n- Radial Basis Function (RBF) kernel: given by $k(\\mathbf{x},\\mathbf{y})=\\exp\\!\\left(-\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}/(2\\sigma^{2})\\right)$ for bandwidth $\\sigma0$.\n- Median heuristic for bandwidth selection: pool all samples from the two datasets, compute all unordered pairwise squared Euclidean distances $\\{\\|\\mathbf{z}_{i}-\\mathbf{z}_{j}\\|_{2}^{2}: ij\\}$, and set $\\sigma^{2}$ to the median of these distances. If this median equals $0$, set $\\sigma^{2}$ to a positive floor $\\varepsilon$ to avoid division by zero. Use $\\varepsilon=10^{-12}$.\n- Use the biased empirical estimator of the squared MMD, which combines empirical kernel self-similarities and cross-similarities via sample averages so that it is well-defined for sample sizes as small as $1$.\n\nAlgorithmic requirements you must satisfy:\n- Accept two finite sets of feature vectors, one “real” and one “synthetic,” each represented as a two-dimensional array with shape $(m,d)$ and $(n,d)$ respectively, where $m$ and $n$ are sample sizes and $d$ is feature dimension.\n- Compute $\\sigma^{2}$ by the median heuristic as specified above, using the pooled set of size $m+n$ and all unordered pairs with $ij$.\n- With that $\\sigma^{2}$, compute the RBF kernel and then the biased empirical estimator of squared MMD between the two sets.\n- Return a single real number per test case: the squared MMD, rounded to $6$ decimal places.\n\nTest suite:\n- Case A (general “happy path”): real $X=\\big[[0.0],[2.0]\\big]$, synthetic $Y=\\big[[1.0],[3.0]\\big]$.\n- Case B (identical sets boundary): real $X=\\big[[0.0],[2.0]\\big]$, synthetic $Y=\\big[[0.0],[2.0]\\big]$.\n- Case C (small-sample edge with $m=1$): real $X=\\big[[1.0]\\big]$, synthetic $Y=\\big[[-1.0],[2.0]\\big]$.\n- Case D (degenerate constant vectors causing zero pairwise distances): real $X=\\big[[5.0],[5.0]\\big]$, synthetic $Y=\\big[[5.0],[5.0]\\big]$.\n\nAll features are dimensionless scalars in these tests. Your program should process the above four cases in order and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[result1,result2,result3,result4]”), where each result is the squared MMD rounded to $6$ decimal places. No other output is permitted.", "solution": "The problem requires the implementation of a function to compute the squared Maximum Mean Discrepancy ($\\text{MMD}^2$) between two sets of feature vectors, a \"real\" set $X$ and a \"synthetic\" set $Y$. The computation must use the biased empirical estimator for $\\text{MMD}^2$ and a Radial Basis Function (RBF) kernel, with the kernel's bandwidth parameter selected using the median heuristic.\n\nThe solution is constructed from the following fundamental principles:\n\n1.  **The Biased Empirical Estimator for Squared MMD**: The $\\text{MMD}$ is a metric on probability distributions defined within a Reproducing Kernel Hilbert Space ($\\mathcal{H}$) induced by a kernel $k$. For two distributions $P$ and $Q$ with mean embeddings $\\mu_P$ and $\\mu_Q$ in $\\mathcal{H}$, the squared MMD is the squared norm of their difference, $\\text{MMD}^2(P, Q) = \\|\\mu_P - \\mu_Q\\|_{\\mathcal{H}}^2$. This can be expanded using the reproducing property of the kernel as:\n    $$\n    \\text{MMD}^2(P, Q) = \\mathbb{E}_{\\mathbf{x}, \\mathbf{x}' \\sim P}[k(\\mathbf{x}, \\mathbf{x}')] + \\mathbb{E}_{\\mathbf{y}, \\mathbf{y}' \\sim Q}[k(\\mathbf{y}, \\mathbf{y}')] - 2\\mathbb{E}_{\\mathbf{x} \\sim P, \\mathbf{y} \\sim Q}[k(\\mathbf{x}, \\mathbf{y})]\n    $$\n    Given finite samples $X = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_m\\}$ from $P$ and $Y = \\{\\mathbf{y}_1, \\dots, \\mathbf{y}_n\\}$ from $Q$, the biased empirical estimator for $\\text{MMD}^2$ is obtained by replacing the expectations with sample averages:\n    $$\n    \\text{MMD}_b^2(X, Y) = \\frac{1}{m^2} \\sum_{i=1}^m \\sum_{j=1}^m k(\\mathbf{x}_i, \\mathbf{x}_j) + \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n k(\\mathbf{y}_i, \\mathbf{y}_j) - \\frac{2}{mn} \\sum_{i=1}^m \\sum_{j=1}^n k(\\mathbf{x}_i, \\mathbf{y}_j)\n    $$\n    This can be expressed concisely using kernel Gram matrices. Let $K_{XX}$ be the $m \\times m$ matrix with elements $K_{XX, ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$, $K_{YY}$ be the $n \\times n$ matrix with elements $K_{YY, ij} = k(\\mathbf{y}_i, \\mathbf{y}_j)$, and $K_{XY}$ be the $m \\times n$ matrix with elements $K_{XY, ij} = k(\\mathbf{x}_i, \\mathbf{y}_j)$. The estimator is then the sum of the mean of all elements in $K_{XX}$ and $K_{YY}$, minus twice the mean of all elements in $K_{XY}$.\n\n2.  **The Radial Basis Function (RBF) Kernel**: The problem specifies the use of the RBF kernel, defined as:\n    $$\n    k(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}}{2\\sigma^2}\\right)\n    $$\n    where $\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$ is the squared Euclidean distance between the vectors $\\mathbf{x}$ and $\\mathbf{y}$, and $\\sigma  0$ is the kernel bandwidth parameter that controls the \"width\" of the kernel. This kernel is characteristic, meaning the MMD is zero if and only if the distributions are identical.\n\n3.  **The Median Heuristic for Bandwidth Selection**: The performance of the RBF kernel is sensitive to the choice of $\\sigma$. The median heuristic provides a robust, data-driven method for setting this parameter. The procedure is as follows:\n    a. Pool all samples from both datasets $X$ and $Y$ into a single set $Z = X \\cup Y$ of size $N=m+n$.\n    b. Compute the set of all unique, unordered pairwise squared Euclidean distances: $\\{\\|\\mathbf{z}_i - \\mathbf{z}_j\\|_{2}^{2} : 1 \\le i  j \\le N\\}$.\n    c. Set the parameter $\\sigma^2$ to be the median of this set of distances.\n    d. A special case arises if the median distance is $0$, which occurs when a majority of the data points are identical. To prevent division by zero in the kernel, $\\sigma^2$ is set to a small positive floor value, $\\varepsilon = 10^{-12}$.\n\n**Algorithmic Synthesis**\nThe implementation combines these principles into a concrete algorithm:\n\n1.  **Input**: Receive the real data matrix $X$ of shape $(m, d)$ and the synthetic data matrix $Y$ of shape $(n, d)$.\n\n2.  **Bandwidth Calculation**:\n    a. Concatenate $X$ and $Y$ vertically to form a pooled matrix $Z$ of shape $(m+n, d)$.\n    b. Compute the condensed vector of all unique pairwise squared Euclidean distances between the rows of $Z$.\n    c. Calculate the median of these distances. This value is assigned to $\\sigma^2$.\n    d. If $\\sigma^2 = 0$, re-assign it to the floor value $\\varepsilon = 10^{-12}$.\n\n3.  **Kernel Matrix Computation**:\n    a. Compute the three matrices of pairwise squared Euclidean distances: $D_{XX}$ between samples in $X$, $D_{YY}$ between samples in $Y$, and $D_{XY}$ between samples in $X$ and $Y$.\n    b. Apply the RBF kernel element-wise to these distance matrices using the calculated $\\sigma^2$:\n       - $K_{XX} = \\exp(-D_{XX} / (2\\sigma^2))$\n       - $K_{YY} = \\exp(-D_{YY} / (2\\sigma^2))$\n       - $K_{XY} = \\exp(-D_{XY} / (2\\sigma^2))$\n\n4.  **MMD Estimation**:\n    a. Calculate the mean of the elements within each kernel matrix: $\\bar{K}_{XX}$, $\\bar{K}_{YY}$, and $\\bar{K}_{XY}$.\n    b. Combine these means according to the biased estimator formula: $\\text{MMD}_b^2 = \\bar{K}_{XX} + \\bar{K}_{YY} - 2\\bar{K}_{XY}$.\n\n5.  **Output**: Return the final $\\text{MMD}_b^2$ value, rounded to $6$ decimal places as required. This procedure is applied systematically to each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, cdist\n\ndef compute_squared_mmd(X: np.ndarray, Y: np.ndarray, epsilon: float = 1e-12) -> float:\n    \"\"\"\n    Computes the biased empirical estimator of the squared Maximum Mean Discrepancy (MMD^2)\n    using the Radial Basis Function (RBF) kernel with bandwidth selected by the median heuristic.\n\n    Args:\n        X: A numpy array of shape (m, d) representing the first set of samples.\n        Y: A numpy array of shape (n, d) representing the second set of samples.\n        epsilon: A small positive floor for the kernel bandwidth squared to avoid division by zero.\n\n    Returns:\n        The computed squared MMD value.\n    \"\"\"\n    m, d = X.shape\n    n = Y.shape[0]\n\n    # Step 1: Bandwidth selection via the median heuristic.\n    # Pool the data from both distributions.\n    Z = np.vstack([X, Y])\n\n    # Compute all unique pairwise squared Euclidean distances.\n    # pdist computes a condensed distance matrix for the upper triangle.\n    if Z.shape[0]  2:\n        # If there's only one point or less, there are no pairs. The median is undefined.\n        # This case is not in the test suite, but handled for robustness.\n        sigma_sq = epsilon\n    else:\n        sq_dists = pdist(Z, 'sqeuclidean')\n        sigma_sq = np.median(sq_dists)\n\n    # If the median is 0, use the floor value epsilon.\n    if sigma_sq == 0.0:\n        sigma_sq = epsilon\n\n    # Step 2: Compute the RBF kernel matrices.\n    # gamma = 1 / (2 * sigma^2)\n    gamma = 1.0 / (2.0 * sigma_sq)\n\n    # Compute pairwise squared Euclidean distance matrices.\n    dist_XX = cdist(X, X, 'sqeuclidean')\n    dist_YY = cdist(Y, Y, 'sqeuclidean')\n    dist_XY = cdist(X, Y, 'sqeuclidean')\n\n    # Apply the RBF kernel function.\n    K_XX = np.exp(-gamma * dist_XX)\n    K_YY = np.exp(-gamma * dist_YY)\n    K_XY = np.exp(-gamma * dist_XY)\n\n    # Step 3: Compute the biased MMD^2 estimator.\n    # This is equivalent to mean(K_XX) + mean(K_YY) - 2 * mean(K_XY).\n    term1 = K_XX.sum() / (m * m)\n    term2 = K_YY.sum() / (n * n)\n    term3 = -2.0 * K_XY.sum() / (m * n)\n    \n    mmd2 = term1 + term2 + term3\n    \n    return mmd2\n\ndef solve():\n    \"\"\"\n    Runs the MMD^2 computation on the test cases provided in the problem statement\n    and prints the results in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: general \"happy path\"\n        (np.array([[0.0], [2.0]]), np.array([[1.0], [3.0]])),\n        # Case B: identical sets boundary\n        (np.array([[0.0], [2.0]]), np.array([[0.0], [2.0]])),\n        # Case C: small-sample edge with m=1\n        (np.array([[1.0]]), np.array([[-1.0], [2.0]])),\n        # Case D: degenerate constant vectors causing zero pairwise distances\n        (np.array([[5.0], [5.0]]), np.array([[5.0], [5.0]])),\n    ]\n\n    results = []\n    for X, Y in test_cases:\n        # Calculate the squared MMD for the current case.\n        mmd_squared_value = compute_squared_mmd(X, Y)\n        # Round the result to 6 decimal places.\n        rounded_result = round(mmd_squared_value, 6)\n        results.append(rounded_result)\n\n    # Final print statement in the exact required format.\n    # Ensure no trailing zeros are cut by casting to string and handle negative zero.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    cleaned_results = [r.replace(\"-0.000000\", \"0.000000\") for r in formatted_results]\n    print(f\"[{','.join(cleaned_results)}]\")\n\nsolve()\n```", "id": "4541982"}, {"introduction": "A successful GAN generates data that is, on average, similar to the real data, but this does not guarantee every synthetic sample is a masterpiece. To prevent low-quality or outlier samples from degrading our final model, we must implement a quality control filter. In this exercise [@problem_id:4541943], you will build a post-hoc filter using the Mahalanobis distance, a powerful statistical measure that identifies outliers by considering the covariance structure of the feature space. This practice will teach you how to create a principled \"gatekeeper\" to ensure only the most plausible synthetic samples are used for augmenting your dataset.", "problem": "You are given a post-hoc filtering task for Generative Adversarial Network (GAN) based augmentation in radiomics. Radiomics feature vectors are standardized numeric descriptors extracted from medical images. After generating synthetic feature vectors with a Generative Adversarial Network (GAN), you must filter out synthetic samples that are outliers with respect to the real feature distribution using the Mahalanobis distance. Assume the standardized radiomics feature vectors from real samples are approximately multivariate normal.\n\nStarting from first principles, use the following foundational base: the definition of the sample mean, the unbiased sample covariance, the Mahalanobis distance, and the well-tested fact that under a multivariate normal model the squared Mahalanobis distance of a sample from the true mean and covariance follows a chi-squared distribution with degrees of freedom equal to the feature dimension.\n\nThe task is defined as follows:\n- Let the real feature vectors be denoted by $\\mathbf{r}_i \\in \\mathbb{R}^d$ for $i = 1, \\dots, n$, collected in a matrix $R \\in \\mathbb{R}^{n \\times d}$. \n- Let the synthetic feature vectors be $\\mathbf{s}_j \\in \\mathbb{R}^d$ for $j = 1, \\dots, m$, collected in a matrix $S \\in \\mathbb{R}^{m \\times d}$.\n- Compute the sample mean $\\hat{\\boldsymbol{\\mu}}$ and unbiased sample covariance $\\hat{\\Sigma}$ from $R$.\n- Define a regularized covariance $\\Sigma_\\lambda = \\hat{\\Sigma} + \\lambda I_d$, where $I_d$ is the $d \\times d$ identity matrix and $\\lambda \\ge 0$ is a small non-negative regularization parameter to ensure numerical stability (particularly when $\\hat{\\Sigma}$ is singular or ill-conditioned).\n- For each synthetic $\\mathbf{s}_j$, compute the squared Mahalanobis distance \n$$\nm^2(\\mathbf{s}_j) = (\\mathbf{s}_j - \\hat{\\boldsymbol{\\mu}})^\\top \\Sigma_\\lambda^{-1} (\\mathbf{s}_j - \\hat{\\boldsymbol{\\mu}}).\n$$\n- Let $\\alpha \\in (0,1)$ be a specified confidence level. Under the multivariate normal model, $m^2(\\mathbf{x})$ is approximately distributed as a chi-squared variable with $d$ degrees of freedom. Use this to set the threshold \n$$\n\\tau = F^{-1}_{\\chi^2(d)}(\\alpha),\n$$ \nwhere $F^{-1}_{\\chi^2(d)}$ denotes the inverse cumulative distribution function (quantile function) of the chi-squared distribution with $d$ degrees of freedom at probability $\\alpha$.\n- The post-hoc filter keeps $\\mathbf{s}_j$ if $m^2(\\mathbf{s}_j) \\le \\tau$ and removes $\\mathbf{s}_j$ if $m^2(\\mathbf{s}_j)  \\tau$. Treat the equality case $m^2(\\mathbf{s}_j) = \\tau$ as “keep.”\n\nImplement this filter and apply it to the following test suite. For each test case, compute the number of synthetic samples kept by the filter. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n\nTest Suite:\n- Test Case A (general case, $d = 2$):\n    - Real feature matrix $R_A$ with $n = 7$:\n      $$\n      R_A = \\begin{bmatrix}\n      0.0  0.0 \\\\\n      1.0  0.0 \\\\\n      0.0  1.0 \\\\\n      -1.0  0.0 \\\\\n      0.0  -1.0 \\\\\n      0.5  -0.5 \\\\\n      -0.5  0.5\n      \\end{bmatrix}\n      $$\n    - Synthetic feature matrix $S_A$ with $m = 7$:\n      $$\n      S_A = \\begin{bmatrix}\n      0.2  0.1 \\\\\n      3.5  0.0 \\\\\n      0.0  3.5 \\\\\n      -3.0  -3.0 \\\\\n      0.3  -0.4 \\\\\n      1.5  1.5 \\\\\n      -0.1  0.0\n      \\end{bmatrix}\n      $$\n    - Confidence level $\\alpha_A = 0.95$ and regularization $\\lambda_A = 0.0$.\n- Test Case B (boundary condition in one dimension, $d = 1$):\n    - Real feature matrix $R_B$ with $n = 3$:\n      $$\n      R_B = \\begin{bmatrix}\n      -1.0 \\\\\n      0.0 \\\\\n      1.0\n      \\end{bmatrix}\n      $$\n      This yields sample mean $\\hat{\\mu}_B$ and sample variance $\\hat{\\sigma}^2_B$ computed from $R_B$ using the unbiased estimator.\n    - Confidence level $\\alpha_B = 0.95$ and regularization $\\lambda_B = 0.0$.\n    - Synthetic feature matrix $S_B$ with $m = 3$ constructed as:\n      $$\n      S_B = \\begin{bmatrix}\n      \\hat{\\mu}_B \\\\\n      \\hat{\\mu}_B + \\sqrt{\\tau_B \\cdot \\hat{\\sigma}^2_B} \\\\\n      3.0\n      \\end{bmatrix}, \\quad \\text{where} \\quad \\tau_B = F^{-1}_{\\chi^2(1)}(\\alpha_B).\n      $$\n      The second synthetic sample is exactly at the threshold in squared Mahalanobis distance, and must be kept by the rule $m^2 \\le \\tau$.\n- Test Case C (edge case: near-singular covariance in higher dimension, $d = 4$):\n    - Real feature matrix $R_C$ with $n = 2$:\n      $$\n      R_C = \\begin{bmatrix}\n      1.0  1.0  1.0  1.0 \\\\\n      1.0  1.0  1.0  1.0\n      \\end{bmatrix}\n      $$\n      The sample covariance is singular; use regularization.\n    - Synthetic feature matrix $S_C$ with $m = 3$:\n      $$\n      S_C = \\begin{bmatrix}\n      1.05  0.95  1.0  1.1 \\\\\n      5.0  5.0  5.0  5.0 \\\\\n      0.0  0.0  0.0  0.0\n      \\end{bmatrix}\n      $$\n    - Confidence level $\\alpha_C = 0.975$ and regularization $\\lambda_C = 0.1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $[A, B, C]$. Each result must be the integer count of synthetic samples kept for that test case. For example, your output must be of the form $[\\text{count}_A,\\text{count}_B,\\text{count}_C]$.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded, well-posed, objective, and self-contained, providing a clear algorithmic task with specific data for testing. The methodology, based on Mahalanobis distance and its relation to the chi-squared distribution for outlier detection in multivariate normal data, is a standard and valid statistical technique. The test cases are thoughtfully designed to cover a general scenario, a boundary condition, and an edge case involving a singular covariance matrix.\n\nThe task is to implement a post-hoc filter for synthetic radiomics feature vectors. The filter's design is based on the statistical properties of the real feature vectors. We assume the population of real feature vectors follows a $d$-dimensional multivariate normal distribution, $\\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)$. We estimate the population mean $\\boldsymbol{\\mu}$ and covariance $\\Sigma$ using the sample mean $\\hat{\\boldsymbol{\\mu}}$ and the unbiased sample covariance $\\hat{\\Sigma}$ computed from a set of real samples $R \\in \\mathbb{R}^{n \\times d}$.\n\nThe core formulas are as follows:\nThe sample mean vector is calculated as:\n$$\n\\hat{\\boldsymbol{\\mu}} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{r}_i\n$$\nThe unbiased sample covariance matrix is calculated as:\n$$\n\\hat{\\Sigma} = \\frac{1}{n-1} \\sum_{i=1}^{n} (\\mathbf{r}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{r}_i - \\hat{\\boldsymbol{\\mu}})^\\top\n$$\nTo ensure numerical stability, a regularized covariance matrix $\\Sigma_\\lambda$ is used:\n$$\n\\Sigma_\\lambda = \\hat{\\Sigma} + \\lambda I_d\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $I_d$ is the $d \\times d$ identity matrix.\n\nFor any synthetic feature vector $\\mathbf{s}_j$, we compute its squared Mahalanobis distance from the mean of the real data distribution:\n$$\nm^2(\\mathbf{s}_j) = (\\mathbf{s}_j - \\hat{\\boldsymbol{\\mu}})^\\top \\Sigma_\\lambda^{-1} (\\mathbf{s}_j - \\hat{\\boldsymbol{\\mu}})\n$$\nUnder the multivariate normal assumption, the quantity $m^2(\\mathbf{x})$ for a vector $\\mathbf{x}$ drawn from the distribution follows a chi-squared distribution with $d$ degrees of freedom, denoted $\\chi^2(d)$. We define a threshold $\\tau$ based on a confidence level $\\alpha \\in (0,1)$ such that a fraction $\\alpha$ of the real data is expected to have a squared Mahalanobis distance less than or equal to $\\tau$. This threshold is the value of the quantile function (inverse cumulative distribution function) of the $\\chi^2(d)$ distribution at probability $\\alpha$:\n$$\n\\tau = F^{-1}_{\\chi^2(d)}(\\alpha)\n$$\nA synthetic sample $\\mathbf{s}_j$ is kept if its squared Mahalanobis distance does not exceed this threshold, i.e., $m^2(\\mathbf{s}_j) \\le \\tau$.\n\nWe now apply this procedure to each test case.\n\n### Test Case A\nThe given parameters are:\n- Dimension $d=2$, number of real samples $n=7$.\n- Real data matrix $R_A$.\n- Synthetic data matrix $S_A$.\n- Confidence level $\\alpha_A = 0.95$.\n- Regularization $\\lambda_A = 0.0$.\n\n1.  **Compute Sample Mean $\\hat{\\boldsymbol{\\mu}}_A$**:\n    The sum of the columns of $R_A$ is $[0.0, 0.0]^\\top$. Thus, the mean is $\\hat{\\boldsymbol{\\mu}}_A = \\frac{1}{7} [0.0, 0.0]^\\top = [0.0, 0.0]^\\top$.\n\n2.  **Compute Unbiased Sample Covariance $\\hat{\\Sigma}_A$**:\n    With $n=7$ and $\\hat{\\boldsymbol{\\mu}}_A = \\mathbf{0}$, $\\hat{\\Sigma}_A = \\frac{1}{6} \\sum_{i=1}^{7} \\mathbf{r}_i \\mathbf{r}_i^\\top = \\frac{1}{6} R_A^\\top R_A$.\n    $R_A^\\top R_A = \\begin{bmatrix} 2.5  -0.5 \\\\ -0.5  2.5 \\end{bmatrix}$.\n    $\\hat{\\Sigma}_A = \\frac{1}{6} \\begin{bmatrix} 2.5  -0.5 \\\\ -0.5  2.5 \\end{bmatrix} = \\begin{bmatrix} 5/12  -1/12 \\\\ -1/12  5/12 \\end{bmatrix}$.\n\n3.  **Compute Regularized Covariance $\\Sigma_{\\lambda, A}$ and its Inverse**:\n    Since $\\lambda_A = 0.0$, $\\Sigma_{\\lambda, A} = \\hat{\\Sigma}_A$. The determinant is $\\det(\\hat{\\Sigma}_A) = (5/12)^2 - (-1/12)^2 = (25-1)/144 = 24/144 = 1/6$.\n    The inverse is $\\Sigma_{\\lambda, A}^{-1} = \\frac{1}{1/6} \\begin{bmatrix} 5/12  1/12 \\\\ 1/12  5/12 \\end{bmatrix} = 6 \\begin{bmatrix} 5/12  1/12 \\\\ 1/12  5/12 \\end{bmatrix} = \\begin{bmatrix} 2.5  0.5 \\\\ 0.5  2.5 \\end{bmatrix}$.\n\n4.  **Compute Threshold $\\tau_A$**:\n    For $d=2$ and $\\alpha_A=0.95$, $\\tau_A = F^{-1}_{\\chi^2(2)}(0.95) \\approx 5.9915$.\n\n5.  **Filter Synthetic Samples**:\n    For each $\\mathbf{s}_j \\in S_A$, we calculate $m^2(\\mathbf{s}_j) = \\mathbf{s}_j^\\top \\Sigma_{\\lambda, A}^{-1} \\mathbf{s}_j$ and compare to $\\tau_A$.\n    - $\\mathbf{s}_1 = [0.2, 0.1]^\\top$: $m^2 = 0.145 \\le 5.9915$ (Keep).\n    - $\\mathbf{s}_2 = [3.5, 0.0]^\\top$: $m^2 = 30.625  5.9915$ (Remove).\n    - $\\mathbf{s}_3 = [0.0, 3.5]^\\top$: $m^2 = 30.625  5.9915$ (Remove).\n    - $\\mathbf{s}_4 = [-3.0, -3.0]^\\top$: $m^2 = 54.0  5.9915$ (Remove).\n    - $\\mathbf{s}_5 = [0.3, -0.4]^\\top$: $m^2 = 0.505 \\le 5.9915$ (Keep).\n    - $\\mathbf{s}_6 = [1.5, 1.5]^\\top$: $m^2 = 13.5  5.9915$ (Remove).\n    - $\\mathbf{s}_7 = [-0.1, 0.0]^\\top$: $m^2 = 0.025 \\le 5.9915$ (Keep).\n    The number of kept samples is $3$.\n\n### Test Case B\nThe given parameters are:\n- Dimension $d=1$, number of real samples $n=3$, real data matrix $R_B = [-1.0, 0.0, 1.0]^\\top$.\n- Confidence level $\\alpha_B = 0.95$, regularization $\\lambda_B = 0.0$.\n\n1.  **Compute Sample Mean $\\hat{\\mu}_B$ and Variance $\\hat{\\sigma}^2_B$**:\n    For $d=1$, we compute the scalar mean and variance.\n    $\\hat{\\mu}_B = \\frac{1}{3}(-1.0 + 0.0 + 1.0) = 0.0$.\n    $\\hat{\\sigma}^2_B = \\frac{1}{3-1} [(-1.0-0.0)^2 + (0.0-0.0)^2 + (1.0-0.0)^2] = \\frac{1}{2}(1+0+1) = 1.0$.\n\n2.  **Compute Regularized Variance and its Inverse**:\n    Since $\\lambda_B = 0.0$, the regularized variance is $\\sigma^2_{\\lambda, B} = \\hat{\\sigma}^2_B = 1.0$. Its inverse is $1.0$.\n\n3.  **Compute Threshold $\\tau_B$**:\n    For $d=1$ and $\\alpha_B=0.95$, $\\tau_B = F^{-1}_{\\chi^2(1)}(0.95) \\approx 3.8415$.\n\n4.  **Construct $S_B$ and Filter**:\n    $S_B$ is constructed from these statistics. The squared Mahalanobis distance is $m^2(s_j) = (s_j - \\hat{\\mu}_B)^2 / \\sigma^2_{\\lambda, B} = (s_j - 0)^2 / 1 = s_j^2$.\n    - $\\mathbf{s}_1 = [\\hat{\\mu}_B] = [0.0]^\\top$: $m^2 = 0.0^2 = 0.0 \\le 3.8415$ (Keep).\n    - $\\mathbf{s}_2 = [\\hat{\\mu}_B + \\sqrt{\\tau_B \\cdot \\hat{\\sigma}^2_B}] = [\\sqrt{\\tau_B}]^\\top$: $m^2 = (\\sqrt{\\tau_B})^2 = \\tau_B \\le \\tau_B$ (Keep). This tests the boundary condition.\n    - $\\mathbf{s}_3 = [3.0]^\\top$: $m^2 = 3.0^2 = 9.0  3.8415$ (Remove).\n    The number of kept samples is $2$.\n\n### Test Case C\nThe given parameters are:\n- Dimension $d=4$, number of real samples $n=2$, real data $R_C$ with two identical rows.\n- Synthetic data matrix $S_C$.\n- Confidence level $\\alpha_C = 0.975$.\n- Regularization $\\lambda_C = 0.1$.\n\n1.  **Compute Sample Mean $\\hat{\\boldsymbol{\\mu}}_C$**:\n    Both rows of $R_C$ are $[1, 1, 1, 1]$, so the mean is $\\hat{\\boldsymbol{\\mu}}_C = [1.0, 1.0, 1.0, 1.0]^\\top$.\n\n2.  **Compute Unbiased Sample Covariance $\\hat{\\Sigma}_C$**:\n    The deviations from the mean for both samples are $(\\mathbf{r}_i - \\hat{\\boldsymbol{\\mu}}_C) = \\mathbf{0}$. Therefore, the unbiased sample covariance matrix is the $4 \\times 4$ zero matrix: $\\hat{\\Sigma}_C = \\mathbf{0}_{4 \\times 4}$. This matrix is singular.\n\n3.  **Compute Regularized Covariance $\\Sigma_{\\lambda, C}$ and its Inverse**:\n    Regularization is necessary here. With $\\lambda_C=0.1$,\n    $\\Sigma_{\\lambda, C} = \\hat{\\Sigma}_C + \\lambda_C I_4 = \\mathbf{0}_{4 \\times 4} + 0.1 I_4 = 0.1 I_4$.\n    The inverse is $\\Sigma_{\\lambda, C}^{-1} = (0.1 I_4)^{-1} = 10 I_4$.\n\n4.  **Compute Threshold $\\tau_C$**:\n    For $d=4$ and $\\alpha_C=0.975$, $\\tau_C = F^{-1}_{\\chi^2(4)}(0.975) \\approx 11.1433$.\n\n5.  **Filter Synthetic Samples**:\n    For each $\\mathbf{s}_j \\in S_C$, we calculate $m^2(\\mathbf{s}_j) = (\\mathbf{s}_j - \\hat{\\boldsymbol{\\mu}}_C)^\\top (10 I_4) (\\mathbf{s}_j - \\hat{\\boldsymbol{\\mu}}_C) = 10 ||\\mathbf{s}_j - \\hat{\\boldsymbol{\\mu}}_C||^2_2$.\n    - $\\mathbf{s}_1 = [1.05, 0.95, 1.0, 1.1]^\\top$: $\\mathbf{s}_1 - \\hat{\\boldsymbol{\\mu}}_C = [0.05, -0.05, 0.0, 0.1]^\\top$.\n      $m^2 = 10 (0.05^2 + (-0.05)^2 + 0.0^2 + 0.1^2) = 10(0.0025 + 0.0025 + 0.01) = 10(0.015) = 0.15 \\le 11.1433$ (Keep).\n    - $\\mathbf{s}_2 = [5.0, 5.0, 5.0, 5.0]^\\top$: $\\mathbf{s}_2 - \\hat{\\boldsymbol{\\mu}}_C = [4.0, 4.0, 4.0, 4.0]^\\top$.\n      $m^2 = 10 (4^2 + 4^2 + 4^2 + 4^2) = 10(64) = 640  11.1433$ (Remove).\n    - $\\mathbf{s}_3 = [0.0, 0.0, 0.0, 0.0]^\\top$: $\\mathbf{s}_3 - \\hat{\\boldsymbol{\\mu}}_C = [-1.0, -1.0, -1.0, -1.0]^\\top$.\n      $m^2 = 10 ((-1)^2 \\times 4) = 10(4) = 40  11.1433$ (Remove).\n    The number of kept samples is $1$.\n\nThe final results are counts of $3$, $2$, and $1$ for Test Cases A, B, and C respectively.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Implements and tests a post-hoc filter for synthetic data based on Mahalanobis distance.\n    The function processes three distinct test cases and computes the number of synthetic\n    samples kept by the filter for each case.\n    \"\"\"\n    results = []\n\n    # Test Case A: General case, 2D\n    R_A = np.array([\n        [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [-1.0, 0.0],\n        [0.0, -1.0], [0.5, -0.5], [-0.5, 0.5]\n    ])\n    S_A = np.array([\n        [0.2, 0.1], [3.5, 0.0], [0.0, 3.5], [-3.0, -3.0],\n        [0.3, -0.4], [1.5, 1.5], [-0.1, 0.0]\n    ])\n    alpha_A = 0.95\n    lambda_A = 0.0\n    \n    n_A, d_A = R_A.shape\n    mu_hat_A = np.mean(R_A, axis=0)\n    cov_hat_A = np.cov(R_A, rowvar=False, ddof=1)\n    cov_reg_A = cov_hat_A + lambda_A * np.identity(d_A)\n    inv_cov_reg_A = np.linalg.inv(cov_reg_A)\n    tau_A = chi2.ppf(alpha_A, df=d_A)\n\n    count_A = 0\n    for s_j in S_A:\n        diff = s_j - mu_hat_A\n        # Squared Mahalanobis distance calculation\n        m2 = diff.T @ inv_cov_reg_A @ diff\n        if m2 = tau_A:\n            count_A += 1\n    results.append(count_A)\n\n    # Test Case B: Boundary condition, 1D\n    R_B = np.array([\n        [-1.0], [0.0], [1.0]\n    ])\n    alpha_B = 0.95\n    lambda_B = 0.0\n    \n    n_B, d_B = R_B.shape\n    mu_hat_B = np.mean(R_B, axis=0)\n    # For d=1, np.cov with ddof=1 returns scalar unbiased sample variance\n    sigma2_hat_B = np.cov(R_B, rowvar=False, ddof=1)\n    sigma2_reg_B = sigma2_hat_B + lambda_B\n    tau_B = chi2.ppf(alpha_B, df=d_B)\n\n    # Construct the synthetic data matrix S_B based on calculated statistics\n    s1_B = mu_hat_B\n    s2_B = mu_hat_B + np.sqrt(tau_B * sigma2_hat_B)\n    s3_B = np.array([3.0])\n    S_B = np.vstack([s1_B, s2_B, s3_B])\n\n    count_B = 0\n    for s_j in S_B:\n        diff = s_j - mu_hat_B\n        # Squared Mahalanobis distance for 1D case\n        m2 = (diff**2) / sigma2_reg_B\n        if m2 = tau_B:\n            count_B += 1\n    results.append(count_B)\n\n    # Test Case C: Edge case, singular covariance matrix, 4D\n    R_C = np.array([\n        [1.0, 1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0, 1.0]\n    ])\n    S_C = np.array([\n        [1.05, 0.95, 1.0, 1.1],\n        [5.0, 5.0, 5.0, 5.0],\n        [0.0, 0.0, 0.0, 0.0]\n    ])\n    alpha_C = 0.975\n    lambda_C = 0.1\n\n    n_C, d_C = R_C.shape\n    mu_hat_C = np.mean(R_C, axis=0)\n    # cov_hat_C will be a 4x4 matrix of zeros as both samples are identical\n    cov_hat_C = np.cov(R_C, rowvar=False, ddof=1)\n    # Regularization is essential here to make the covariance matrix invertible\n    cov_reg_C = cov_hat_C + lambda_C * np.identity(d_C)\n    inv_cov_reg_C = np.linalg.inv(cov_reg_C)\n    tau_C = chi2.ppf(alpha_C, df=d_C)\n\n    count_C = 0\n    for s_j in S_C:\n        diff = s_j - mu_hat_C\n        # Squared Mahalanobis distance calculation\n        m2 = diff.T @ inv_cov_reg_C @ diff\n        if m2 = tau_C:\n            count_C += 1\n    results.append(count_C)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4541943"}]}