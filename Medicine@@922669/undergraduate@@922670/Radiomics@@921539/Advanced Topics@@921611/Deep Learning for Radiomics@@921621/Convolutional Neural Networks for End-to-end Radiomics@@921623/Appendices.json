{"hands_on_practices": [{"introduction": "Theoretical models often assume ideal, isotropic data, but real-world medical images like MRI scans are frequently anisotropic, with different resolutions along different axes. This exercise challenges you to think like a medical imaging engineer, adapting a CNN's architecture to respect these physical data properties without resorting to resampling, which can introduce artifacts. By carefully choosing parameters like kernel shapes and strides, you will learn how to build models that are intrinsically aware of the data's geometry, a key skill in developing robust radiomics pipelines [@problem_id:4534271].", "problem": "An end-to-end radiomics pipeline processes three-dimensional Magnetic Resonance Imaging (MRI) volumes with anisotropic voxel spacing $(1.0, 1.0, 5.0)$ millimeters along $(x,y,z)$. Consider a three-layer three-dimensional Convolutional Neural Network (CNN) whose purpose is to extract features for downstream supervised learning. You must design a stride and dilation scheme that respects the acquisition anisotropy without any resampling of the input volume. The design must satisfy all of the following constraints, which reflect standard engineering choices in this setting:\n\n- Use unit stride $(1,1,1)$ in the first layer to preserve high-frequency details at the input.\n- Achieve a total in-plane downsampling factor of exactly $4$ over the three layers, applied only in the $x$ and $y$ axes, with no downsampling along $z$. Concretely, use stride $(2,2,1)$ in exactly two of the three layers, and unit stride $(1,1,1)$ in the remaining layer.\n- To respect through-slice anisotropy, forbid any through-slice mixing until the in-plane receptive field in physical units reaches at least one slice thickness. Concretely, use kernel depth $1$ along $z$ (i.e., $k_{z}=1$) in any layer where the preceding in-plane receptive field in physical units is strictly less than $5$ millimeters; use kernel depth $3$ along $z$ (i.e., $k_{z}=3$) starting at the first layer for which the preceding in-plane receptive field in physical units is at least $5$ millimeters.\n- Use the smallest positive integer dilations along all axes consistent with the above constraints.\n\nAssume all convolutional kernels have spatial size $3$ along $x$ and $y$ (i.e., $k_{x}=k_{y}=3$), zero padding is chosen so that the output is aligned to the input center voxel, and there are no skip connections. Treat each spatial axis independently when reasoning about growth of the receptive field across layers.\n\nUsing only the fundamental definition of receptive field growth under composition of convolutions with stride and dilation, determine the final effective receptive field extent in physical units (millimeters) along each axis after the third layer, under your designed scheme. For clarity, define the effective receptive field extent along a given axis to be the physical length of the minimal contiguous input segment that can influence a single output element along that axis, equal to the number of input voxels in the receptive field along that axis multiplied by the voxel spacing along that axis.\n\nExpress your final answer as a row vector $\\begin{pmatrix} R_{x} & R_{y} & R_{z} \\end{pmatrix}$ in millimeters. No rounding is required.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and internally consistent. All data and constraints required for a unique solution are provided. The problem is a valid exercise in applying fundamental principles of convolutional neural networks to a practical design scenario in medical imaging. Therefore, a full solution is warranted.\n\nThe core of the problem is to determine the design parameters of a three-layer 3D CNN—specifically the strides, kernel sizes, and dilations for each layer—based on a set of constraints, and then to calculate the resulting effective receptive field. The effective receptive field $R_i$ (in voxels) along a single axis after layer $i$ can be calculated recursively. Let $R_{i-1}$ be the receptive field after the previous layer (with $R_0=1$), $k_i$ be the kernel size of layer $i$, $d_i$ be the dilation rate of layer $i$, and $S_{i-1}$ be the cumulative product of strides up to layer $i-1$, defined as $S_{i-1} = \\prod_{j=1}^{i-1} s_j$ (with $S_0 = 1$). The recursive formula is:\n$$R_i = R_{i-1} + (k_i - 1) d_i S_{i-1}$$\nWe will apply this formula independently to each spatial axis $(x, y, z)$. The final receptive field extent in physical units is the receptive field in voxels multiplied by the corresponding voxel spacing.\n\nThe given parameters are:\n- Voxel spacing: $v = (v_x, v_y, v_z) = (1.0, 1.0, 5.0)$ mm.\n- Number of layers: $3$.\n- In-plane kernel size: $k_{ix} = k_{iy} = 3$ for $i \\in \\{1, 2, 3\\}$.\n\nWe proceed layer by layer to determine the parameters and calculate the receptive field growth.\n\n**Step 1: Determine Network Parameters**\n\n**Layer 1:**\n- **Stride ($s_1$):** The problem states to use unit stride in the first layer. Thus, $s_1 = (s_{1x}, s_{1y}, s_{1z}) = (1, 1, 1)$.\n- **Dilation ($d_1$):** The problem requires the smallest positive integer dilations. With no other constraints forcing a larger value, the smallest positive integer is $1$. Thus, $d_1 = (d_{1x}, d_{1y}, d_{1z}) = (1, 1, 1)$.\n- **Kernel Size ($k_1$):** We are given $k_{1x}=3$ and $k_{1y}=3$. For $k_{1z}$, we must evaluate the anisotropy constraint. The \"preceding in-plane receptive field\" for layer $1$ is that of the input, which is a single voxel. Its physical size is $\\max(1 \\cdot v_x, 1 \\cdot v_y) = \\max(1.0, 1.0) = 1.0$ mm. Since $1.0 < 5.0$, the constraint requires no through-slice mixing. Therefore, $k_{1z} = 1$.\n- **Layer 1 Parameters:** $s_1=(1,1,1)$, $d_1=(1,1,1)$, $k_1=(3,3,1)$.\n\n**Layer 2:**\n- **Stride ($s_2$):** The total in-plane downsampling factor must be $4$, achieved using stride $(2,2,1)$ in two of the three layers. Since layer $1$ has unit stride, layers $2$ and $3$ must have stride $(2,2,1)$. Thus, $s_2 = (s_{2x}, s_{2y}, s_{2z}) = (2, 2, 1)$.\n- **Dilation ($d_2$):** Using the smallest positive integer, $d_2 = (d_{2x}, d_{2y}, d_{2z}) = (1, 1, 1)$.\n- **Kernel Size ($k_2$):** We are given $k_{2x}=3$ and $k_{2y}=3$. To find $k_{2z}$, we first need the in-plane receptive field after layer $1$.\n  - Cumulative stride product before layer 2 is $S_1 = s_1 = (1,1,1)$.\n  - Receptive field in voxels after layer 1 ($R_1$):\n    - $R_{1x} = R_0 + (k_{1x}-1)d_{1x}S_0 = 1 + (3-1) \\cdot 1 \\cdot 1 = 3$.\n    - $R_{1y} = R_0 + (k_{1y}-1)d_{1y}S_0 = 1 + (3-1) \\cdot 1 \\cdot 1 = 3$.\n  - The preceding in-plane physical receptive field is $\\max(R_{1x} \\cdot v_x, R_{1y} \\cdot v_y) = \\max(3 \\cdot 1.0, 3 \\cdot 1.0) = 3.0$ mm. Since $3.0 < 5.0$, the constraint requires $k_{2z} = 1$.\n- **Layer 2 Parameters:** $s_2=(2,2,1)$, $d_2=(1,1,1)$, $k_2=(3,3,1)$.\n\n**Layer 3:**\n- **Stride ($s_3$):** As determined above, $s_3 = (s_{3x}, s_{3y}, s_{3z}) = (2, 2, 1)$. The total in-plane stride product is $s_{1x}s_{2x}s_{3x} = 1 \\cdot 2 \\cdot 2 = 4$, satisfying the constraint.\n- **Dilation ($d_3$):** Using the smallest positive integer, $d_3 = (d_{3x}, d_{3y}, d_{3z}) = (1, 1, 1)$.\n- **Kernel Size ($k_3$):** We are given $k_{3x}=3$ and $k_{3y}=3$. To find $k_{3z}$, we first need the in-plane receptive field after layer $2$.\n  - Cumulative stride product before layer 3 is $S_2 = (s_{1x}s_{2x}, s_{1y}s_{2y}, s_{1z}s_{2z}) = (1 \\cdot 2, 1 \\cdot 2, 1 \\cdot 1) = (2, 2, 1)$.\n  - Receptive field in voxels after layer 2 ($R_2$):\n    - $R_{2x} = R_{1x} + (k_{2x}-1)d_{2x}S_{1x} = 3 + (3-1) \\cdot 1 \\cdot 1 = 5$.\n    - $R_{2y} = R_{1y} + (k_{2y}-1)d_{2y}S_{1y} = 3 + (3-1) \\cdot 1 \\cdot 1 = 5$.\n  - The preceding in-plane physical receptive field is $\\max(R_{2x} \\cdot v_x, R_{2y} \\cdot v_y) = \\max(5 \\cdot 1.0, 5 \\cdot 1.0) = 5.0$ mm. The condition \"strictly less than $5$ millimeters\" is now false. The rule states to use $k_z=3$ starting at the first layer where the preceding field is \"at least $5$ millimeters\". This condition is met for the first time at layer $3$. Therefore, $k_{3z} = 3$.\n- **Layer 3 Parameters:** $s_3=(2,2,1)$, $d_3=(1,1,1)$, $k_3=(3,3,3)$.\n\n**Step 2: Calculate Final Receptive Field**\n\nNow we calculate the final receptive field in voxels, $R_3$, using the parameters determined above.\n\n**Along x-axis:**\n- $R_{0x}=1$\n- $R_{1x}=3$\n- $R_{2x}=5$\n- $R_{3x} = R_{2x} + (k_{3x}-1)d_{3x}S_{2x} = 5 + (3-1) \\cdot 1 \\cdot 2 = 5 + 4 = 9$.\n\n**Along y-axis:**\n- $R_{0y}=1$\n- $R_{1y}=3$\n- $R_{2y}=5$\n- By symmetry with the x-axis, $R_{3y} = 9$.\n\n**Along z-axis:**\n- We must calculate the receptive field growth from scratch.\n- $R_{0z}=1$.\n- Layer 1: $R_{1z} = R_{0z} + (k_{1z}-1)d_{1z}S_{0z} = 1 + (1-1) \\cdot 1 \\cdot 1 = 1$.\n- Layer 2: $R_{2z} = R_{1z} + (k_{2z}-1)d_{2z}S_{1z} = 1 + (1-1) \\cdot 1 \\cdot 1 = 1$.\n- Layer 3: $R_{3z} = R_{2z} + (k_{3z}-1)d_{3z}S_{2z} = 1 + (3-1) \\cdot 1 \\cdot 1 = 1 + 2 = 3$.\n\nThe final receptive field in voxels is $R_3 = (R_{3x}, R_{3y}, R_{3z}) = (9, 9, 3)$.\n\n**Step 3: Convert to Physical Units**\n\nThe problem asks for the final effective receptive field extent in physical units (mm). We multiply the voxel counts by the voxel spacing $v = (1.0, 1.0, 5.0)$ mm.\n- Extent along x: $R_x = R_{3x} \\cdot v_x = 9 \\cdot 1.0 = 9.0$ mm.\n- Extent along y: $R_y = R_{3y} \\cdot v_y = 9 \\cdot 1.0 = 9.0$ mm.\n- Extent along z: $R_z = R_{3z} \\cdot v_z = 3 \\cdot 5.0 = 15.0$ mm.\n\nThe final effective receptive field extent is $(9.0, 9.0, 15.0)$ mm.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n9.0 & 9.0 & 15.0\n\\end{pmatrix}\n}\n$$", "id": "4534271"}, {"introduction": "Convolutional Neural Networks are powerful but are often seen as \"black boxes,\" which can limit their trust and adoption in high-stakes clinical settings. This practice demystifies the decision-making process by introducing a cornerstone interpretability method: Gradient-weighted Class Activation Mapping (Grad-CAM) [@problem_id:4534276]. You will derive the Grad-CAM formula from first principles and apply it to a concrete example, gaining a hands-on understanding of how to generate visual explanations that highlight the image regions driving a model's prediction.", "problem": "Consider an end-to-end radiomics system in which a three-dimensional Convolutional Neural Network (CNN) is trained to predict a scalar pre-softmax class score $y$ for a target clinical endpoint from an input volumetric image patch. Let the final convolutional block output $K$ feature maps $\\{A^{k}\\}_{k=1}^{K}$, where each $A^{k}$ is indexed by spatial coordinates $(i,j,l)$ with dimensions $I \\times J \\times L$. Define $Z = I \\cdot J \\cdot L$. The Gradient-weighted Class Activation Mapping (Grad-CAM) method assigns an importance weight to each channel based on partial derivatives of $y$ with respect to the feature maps, and produces a spatial heatmap whose positive values indicate regions contributing positively to the prediction. \n\nStarting from the chain rule of differentiation and a first-order Taylor expansion of $y$ with respect to the activations $\\{A^{k}\\}$, derive a closed-form expression for the Grad-CAM heatmap in terms of the feature maps $\\{A^{k}\\}$ and a channel-wise scalar weight constructed by globally averaging the partial derivatives $\\frac{\\partial y}{\\partial A^{k}_{i,j,l}}$ over the spatial index $(i,j,l)$. Justify the use of a rectifying nonlinearity to retain only positively contributing regions in the final heatmap. Base your derivation on well-tested facts about backpropagation and activation importance in CNNs, without assuming any pre-specified final formula.\n\nThen, for a specific case representative of a volumetric radiomics CNN, let $K = 2$, $I = 2$, $J = 2$, and $L = 1$, so that $Z = 4$. Suppose the feature maps are\n$$\nA^{1} = \\begin{pmatrix}\n1 & -2 \\\\\n0 & 3\n\\end{pmatrix}, \\quad\nA^{2} = \\begin{pmatrix}\n-1 & 2 \\\\\n1 & -2\n\\end{pmatrix},\n$$\nindexed by $(i,j,l)$ with $l=1$ for all entries, and the partial derivatives of the pre-softmax score $y$ with respect to the activations are\n$$\n\\frac{\\partial y}{\\partial A^{1}} = \\begin{pmatrix}\n4 & -1 \\\\\n2 & 0\n\\end{pmatrix}, \\quad\n\\frac{\\partial y}{\\partial A^{2}} = \\begin{pmatrix}\n-2 & 1 \\\\\n-1 & 2\n\\end{pmatrix}.\n$$\nCompute the Grad-CAM heatmap value at the voxel $(i,j,l) = (1,1,1)$ produced by globally averaged channel weights and a rectifying nonlinearity. Express your final numerical answer exactly, with no rounding. \n\nFinally, explain how the choice of the convolutional layer at which the feature maps $\\{A^{k}\\}$ are taken affects the resulting Grad-CAM heatmap in the context of volumetric radiomics, addressing resolution, semantic specificity, and gradient reliability. Your explanation should be scientifically grounded and self-consistent, but the final answer must be the single numerical value requested above, with no units.", "solution": "The problem statement is deemed valid. It is scientifically grounded in the principles of deep learning and model interpretability, specifically the Gradient-weighted Class Activation Mapping (Grad-CAM) method. The problem is well-posed, providing all necessary definitions and numerical data for a unique solution to be calculated. The language is objective and the setup is internally consistent and formalizable.\n\nWe are tasked with deriving the expression for a Grad-CAM heatmap and then computing its value for a specific case. The derivation must originate from fundamental principles of calculus as applied to neural networks.\n\nFirst, we derive the general form of the Grad-CAM heatmap. The pre-softmax class score, $y$, is a scalar function of the activations in the final convolutional layer's feature maps, $\\{A^{k}\\}_{k=1}^{K}$. The influence of a specific activation at voxel $(i,j,l)$ in feature map $k$, denoted $A^{k}_{i,j,l}$, on the score $y$ is captured by the partial derivative $\\frac{\\partial y}{\\partial A^{k}_{i,j,l}}$. This gradient is computed via backpropagation.\n\nA first-order Taylor expansion of $y$ around a baseline (e.g., zero activations) would approximate the score as a linear function of the activations, with the gradients serving as coefficients:\n$$ y \\approx y_0 + \\sum_{k=1}^{K} \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{l=1}^{L} \\frac{\\partial y}{\\partial A^{k}_{i,j,l}} A^{k}_{i,j,l} $$\nGrad-CAM simplifies this high-dimensional relationship by first determining the \"importance\" of an entire feature map (or channel) $k$. This is achieved by collapsing the spatial gradient information into a single scalar weight, $\\alpha_k$. As specified in the problem, this weight is obtained by performing global average pooling on the gradients for that channel:\n$$ \\alpha_k = \\frac{1}{Z} \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{l=1}^{L} \\frac{\\partial y}{\\partial A^{k}_{i,j,l}} $$\nHere, $Z = I \\cdot J \\cdot L$ is the total number of spatial locations (voxels) in a feature map. This weight, $\\alpha_k$, represents the average contribution of channel $k$ to the score $y$ per unit of activation.\n\nThe Grad-CAM heatmap, $L_{\\text{Grad-CAM}}$, is then constructed as a weighted linear combination of the feature maps, where the weights are the $\\alpha_k$ values just defined. The value of this intermediate \"raw\" heatmap at a specific voxel $(i,j,l)$ is:\n$$ L_{\\text{raw}}(i,j,l) = \\sum_{k=1}^{K} \\alpha_k A^{k}_{i,j,l} $$\nThis expression localizes the high-level semantic information captured by the channel importances, $\\alpha_k$, onto the spatial grid defined by the feature maps, $A^k$. A high positive value in $L_{\\text{raw}}$ indicates that the corresponding voxel is strongly activated in feature maps that have a strong positive influence on the class score $y$.\n\nFinally, the problem states that a rectifying nonlinearity is applied to retain only positively contributing regions. This corresponds to the application of a Rectified Linear Unit (ReLU) function, which sets all negative values to zero. The final Grad-CAM heatmap is thus:\n$$ L_{\\text{Grad-CAM}}(i,j,l) = \\text{ReLU}(L_{\\text{raw}}(i,j,l)) = \\max(0, L_{\\text{raw}}(i,j,l)) $$\nThe justification for this step is that we are typically interested in visualizing which parts of the input provide positive evidence for the class of interest. Negative values in $L_{\\text{raw}}$ can be ambiguous, arising from either positive activations in negatively-weighted channels or negative activations in positively-weighted channels. By filtering these out, the resulting heatmap highlights only the regions that support the prediction.\n\nNow, we apply this derivation to the specific case provided.\nThe givens are:\n- Number of channels: $K=2$.\n- Feature map dimensions: $I=2$, $J=2$, $L=1$.\n- Total number of voxels per map: $Z = I \\cdot J \\cdot L = 2 \\cdot 2 \\cdot 1 = 4$.\n- Feature maps:\n$$ A^{1} = \\begin{pmatrix} 1 & -2 \\\\ 0 & 3 \\end{pmatrix}, \\quad A^{2} = \\begin{pmatrix} -1 & 2 \\\\ 1 & -2 \\end{pmatrix} $$\n- Gradients of the score with respect to the feature maps:\n$$ \\frac{\\partial y}{\\partial A^{1}} = \\begin{pmatrix} 4 & -1 \\\\ 2 & 0 \\end{pmatrix}, \\quad \\frac{\\partial y}{\\partial A^{2}} = \\begin{pmatrix} -2 & 1 \\\\ -1 & 2 \\end{pmatrix} $$\n\nFirst, we compute the channel importance weights, $\\alpha_1$ and $\\alpha_2$.\nFor channel $k=1$:\n$$ \\alpha_1 = \\frac{1}{4} \\sum_{i,j,l} \\frac{\\partial y}{\\partial A^{1}_{i,j,l}} = \\frac{1}{4} (4 + (-1) + 2 + 0) = \\frac{5}{4} $$\nFor channel $k=2$:\n$$ \\alpha_2 = \\frac{1}{4} \\sum_{i,j,l} \\frac{\\partial y}{\\partial A^{2}_{i,j,l}} = \\frac{1}{4} ((-2) + 1 + (-1) + 2) = \\frac{0}{4} = 0 $$\n\nNext, we compute the raw heatmap value, $L_{\\text{raw}}$, at the specified voxel $(i,j,l) = (1,1,1)$. This corresponds to the element in the first row and first column of the given matrices (assuming $1$-based indexing for $i$ and $j$, and $l=1$ is implicit).\nFrom the given matrices, the activations at $(1,1,1)$ are:\n- $A^{1}_{1,1,1} = 1$\n- $A^{2}_{1,1,1} = -1$\n\nThe raw heatmap value is:\n$$ L_{\\text{raw}}(1,1,1) = \\alpha_1 A^{1}_{1,1,1} + \\alpha_2 A^{2}_{1,1,1} = \\left(\\frac{5}{4}\\right)(1) + (0)(-1) = \\frac{5}{4} $$\n\nFinally, we apply the ReLU function to get the final Grad-CAM value:\n$$ L_{\\text{Grad-CAM}}(1,1,1) = \\text{ReLU}\\left(\\frac{5}{4}\\right) = \\max\\left(0, \\frac{5}{4}\\right) = \\frac{5}{4} $$\nThe value of the Grad-CAM heatmap at voxel $(1,1,1)$ is $\\frac{5}{4}$.\n\nRegarding the choice of the convolutional layer, there exists a fundamental trade-off between spatial resolution and semantic specificity.\n- **Deeper Layers (near the output):** These layers have feature maps with low spatial resolution due to repeated down-sampling (e.g., pooling, strided convolutions). However, they capture high-level, complex semantic features (e.g., tumor texture, organ morphology) that are highly relevant to the clinical endpoint. A Grad-CAM heatmap generated from a deep layer will be coarse but highly class-discriminative, effectively localizing the abstract concepts the network has learned. Gradients at these layers are often cleaner as they are closer to the loss function.\n- **Shallower Layers (near the input):** These layers have high spatial resolution, preserving fine-grained detail from the input image. However, they capture low-level features like edges, corners, and simple textures, which are less semantically rich. A heatmap from a shallow layer will be detailed and precise in location but may highlight features that are not specific to the class of interest (e.g., highlighting all edges, not just those of a lesion).\nIn the context of volumetric radiomics, heatmaps are most often generated from the final convolutional layer. This is because the primary goal is to understand which high-level radiomic patterns, as encoded by the network, are driving the prediction. The loss of spatial resolution is an accepted trade-off for gaining superior semantic insight into the model's decision-making process.", "answer": "$$\\boxed{\\frac{5}{4}}$$", "id": "4534276"}, {"introduction": "This final practice serves as a capstone, integrating the concepts of network design and training into a complete, functional system for a clinically relevant task. You will construct an entire end-to-end pipeline for survival prediction from 3D CT volumes, implementing every component from scratch using only fundamental numerical libraries [@problem_id:4534311]. This deep dive into the mechanics—including the forward pass, the Cox survival loss, and the manual backpropagation of gradients—will provide an unparalleled understanding of how these sophisticated models learn from complex medical data.", "problem": "You are given a scenario from computational medical imaging in radiomics, where the aim is to design a fully differentiable end-to-end pipeline that maps computed tomography (CT) lung cancer volumetric images to a survival risk score using Convolutional Neural Networks (CNNs). The core of this task balances image processing fundamentals with survival analysis. The pipeline must be constructed entirely from first principles and well-tested formulas, using explicit design choices for preprocessing, architecture, and loss function. The final program must implement the pipeline and compute specified outputs on a deterministic synthetic test suite.\n\nStart from the following fundamental base:\n- The discrete three-dimensional convolution operation is defined by $$z_c(d,h,w) = \\sum_{u=0}^{K_d-1} \\sum_{v=0}^{K_h-1} \\sum_{s=0}^{K_w-1} W_{c}(u,v,s) \\cdot X(d+u, h+v, w+s) + b_c,$$ where $X$ is an input volume with one channel, $W_c$ is the kernel for output channel $c$, $b_c$ is a bias, and $(K_d,K_h,K_w)$ is the kernel size.\n- The Softplus activation (a smooth approximation of the Rectified Linear Unit) is defined by $$\\mathrm{softplus}_s(x) = \\frac{1}{s} \\log\\left(1 + e^{s x}\\right),$$ with derivative $$\\frac{d}{dx}\\mathrm{softplus}_s(x) = \\frac{1}{1 + e^{-s x}}.$$ For this problem, use $s=1$.\n- Global Average Pooling across spatial dimensions transforms an activation tensor for each channel into a single scalar per channel by averaging all spatial positions for that channel. For a channel $c$ with $N$ spatial positions, $$p_c = \\frac{1}{N} \\sum_{n=1}^{N} a_c(n).$$\n- A linear risk layer maps pooled features to a risk score via $$r = \\mathbf{w}^\\top \\mathbf{p} + b,$$ where $\\mathbf{w}$ is a weight vector and $b$ is a scalar bias.\n- The negative log partial likelihood for the Cox proportional hazards model with Breslow approximation for ties is defined by $$\\mathcal{L}(\\mathbf{r}) = - \\sum_{\\tau \\in T^+} \\left(\\sum_{i: t_i = \\tau, e_i = 1} r_i \\right) + \\sum_{\\tau \\in T^+} m_\\tau \\cdot \\log \\left( \\sum_{j: t_j \\ge \\tau} e^{r_j} \\right),$$ where $t_i$ are survival times in $months$, $e_i \\in \\{0,1\\}$ are event indicators, $T^+$ is the set of distinct event times, and $m_\\tau$ is the number of events at time $\\tau$. The gradient with respect to each risk score $r_k$ is given by $$\\frac{\\partial \\mathcal{L}}{\\partial r_k} = - e_k + \\sum_{\\tau \\in T^+:\\, t_k \\ge \\tau} m_\\tau \\cdot \\frac{e^{r_k}}{\\sum_{j: t_j \\ge \\tau} e^{r_j}}.$$\n\nYour task:\n- Implement an end-to-end pipeline from input volume to risk score that is differentiable at every step, using only $Python$ with $NumPy$ (no automatic differentiation libraries). The preprocessing, architecture, and loss must be exactly as specified below.\n- Preprocessing: For each volume $X$, perform per-volume $z$-score normalization: $$\\tilde{X} = \\frac{X - \\mu_X}{\\sigma_X + \\epsilon},$$ where $\\mu_X$ is the mean of all voxel intensities in $X$, $\\sigma_X$ is the standard deviation of voxel intensities in $X$, and $\\epsilon = 10^{-6}$.\n- Architecture: Use a single three-dimensional convolutional layer with $C_{in} = 1$ input channel, $C_{out} = 2$ output channels, kernel size $(3,3,3)$, stride $(1,1,1)$, and $valid$ padding (no padding). Apply the Softplus activation (with $s=1$) to the convolution outputs. Then apply Global Average Pooling over the spatial dimensions to obtain a $2$-dimensional pooled feature vector. Map this to a scalar risk score using a linear layer with weight vector $\\mathbf{w} \\in \\mathbb{R}^2$ and scalar bias $b \\in \\mathbb{R}$.\n- Loss: Use the Cox negative log partial likelihood with Breslow ties as defined above.\n- Training step: Compute the loss and its gradient with respect to all learnable parameters ($W$, $b$ for the convolutional layer; $\\mathbf{w}$, $b$ for the linear layer). Perform a single gradient descent update with learning rate $\\eta = 0.1$ on these parameters. Then recompute the loss with the updated parameters.\n\nInitialization:\n- Initialize all learnable parameters using a fixed random seed $0$, with values drawn independently from the normal distribution with mean $0$ and standard deviation $0.01$. Precisely, initialize convolutional weights $W \\in \\mathbb{R}^{2 \\times 1 \\times 3 \\times 3 \\times 3}$, convolutional biases $b \\in \\mathbb{R}^2$, linear weights $\\mathbf{w} \\in \\mathbb{R}^2$, and linear bias $b \\in \\mathbb{R}$.\n\nAngle units do not apply. Survival time unit is $months$. Risk scores are unitless and must be treated as pure real numbers.\n\nTest Suite:\nConstruct four independent cases, each processed with a fresh parameter initialization (the parameters are reset to their initial values for each case):\n\n- Case $1$ (happy path with tied events): $3$ patients, each with a volume of shape $(4,4,4)$ voxels. Define the voxel intensities for patient index $p \\in \\{0,1,2\\}$ by $$V^{(p)}[i,j,k] = p + i - j + 2k,$$ for $i,j,k \\in \\{0,1,2,3\\}$. Survival times (in $months$): $[6,12,12]$. Event indicators: $[1,1,0]$.\n\n- Case $2$ (all censored boundary): $3$ patients, each with a volume of shape $(4,4,4)$. Define $$V^{(p)}[i,j,k] = (i+1)(j+1) + k - p,$$ for $p \\in \\{0,1,2\\}$ and $i,j,k \\in \\{0,1,2,3\\}$. Survival times (in $months$): $[5,7,9]$. Event indicators: $[0,0,0]$.\n\n- Case $3$ (all events at same time tie): $2$ patients, each with a volume of shape $(3,3,3)$. Define $$V^{(p)}[i,j,k] = (-1)^p \\cdot (i \\cdot j + k),$$ for $p \\in \\{0,1\\}$ and $i,j,k \\in \\{0,1,2\\}$. Survival times (in $months$): $[10,10]$. Event indicators: $[1,1]$.\n\n- Case $4$ (single patient boundary): $1$ patient with volume shape $(3,3,3)$. Define $$V^{(0)}[i,j,k] = i + j + k,$$ for $i,j,k \\in \\{0,1,2\\}$. Survival time (in $months$): $[8]$. Event indicator: $[1]$.\n\nRequired final outputs:\n- For each case, compute the loss before the gradient descent update and after the single update. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sublist of the form $[\\mathcal{L}_{before}, \\mathcal{L}_{after}]$. For example, for $4$ cases the output must be of the exact form $$[[\\ell_1^{(before)},\\ell_1^{(after)}],[\\ell_2^{(before)},\\ell_2^{(after)}],[\\ell_3^{(before)},\\ell_3^{(after)}],[\\ell_4^{(before)},\\ell_4^{(after)}]].$$\n\nNotes:\n- Every operation in the pipeline must be differentiable (Softplus is used instead of Rectified Linear Unit to ensure differentiability).\n- You must compute gradients explicitly by applying the chain rule from the loss back to the convolutional weights and biases, then to the linear weights and bias, accumulating across patients in each case.\n- All calculations must be deterministic and reproducible.", "solution": "The problem presents a task to design, implement, and test a deterministic, end-to-end differentiable pipeline for radiomics-based survival prediction. The pipeline maps three-dimensional CT images to a patient-specific risk score using a convolutional neural network (CNN), with all components—preprocessing, network architecture, and loss function—explicitly defined. The problem requires a manual implementation of a single gradient descent step, including the derivation and application of the backpropagation algorithm.\n\nFirst, a validation of the problem statement is conducted.\n\n**Step 1: Extract Givens**\n\n- **3D Convolution:** $z_c(d,h,w) = \\sum_{u=0}^{K_d-1} \\sum_{v=0}^{K_h-1} \\sum_{s=0}^{K_w-1} W_{c}(u,v,s) \\cdot X(d+u, h+v, w+s) + b_c$, where $X$ is a single-channel input, $W_c$ is the kernel for output channel $c$, $b_c$ is bias, and $(K_d,K_h,K_w)$ is kernel size.\n- **Softplus Activation:** $\\mathrm{softplus}_s(x) = \\frac{1}{s} \\log\\left(1 + e^{s x}\\right)$ with $s=1$. Derivative $\\frac{d}{dx}\\mathrm{softplus}_1(x) = \\frac{1}{1 + e^{-x}}$.\n- **Global Average Pooling (GAP):** $p_c = \\frac{1}{N} \\sum_{n=1}^{N} a_c(n)$ over $N$ spatial positions for channel $c$.\n- **Linear Risk Layer:** $r = \\mathbf{w}^\\top \\mathbf{p} + b$.\n- **Cox Loss (Negative Log Partial Likelihood):** $\\mathcal{L}(\\mathbf{r}) = - \\sum_{\\tau \\in T^+} \\left(\\sum_{i: t_i = \\tau, e_i = 1} r_i \\right) + \\sum_{\\tau \\in T^+} m_\\tau \\cdot \\log \\left( \\sum_{j: t_j \\ge \\tau} e^{r_j} \\right)$, where $t_i$ are survival times, $e_i \\in \\{0,1\\}$ are event indicators, $T^+$ is the set of distinct event times, and $m_\\tau$ is the number of events at time $\\tau$.\n- **Cox Loss Gradient:** $\\frac{\\partial \\mathcal{L}}{\\partial r_k} = - e_k + \\sum_{\\tau \\in T^+:\\, t_k \\ge \\tau} m_\\tau \\cdot \\frac{e^{r_k}}{\\sum_{j: t_j \\ge \\tau} e^{r_j}}$.\n- **Preprocessing:** Per-volume Z-score normalization $\\tilde{X} = \\frac{X - \\mu_X}{\\sigma_X + \\epsilon}$ with $\\epsilon = 10^{-6}$.\n- **Architecture:** 3D Conv ($C_{in}=1, C_{out}=2, K=(3,3,3)$, stride=$(1,1,1)$, 'valid' padding) $\\rightarrow$ Softplus ($s=1$) $\\rightarrow$ GAP $\\rightarrow$ Linear layer.\n- **Training:** Single gradient descent step with learning rate $\\eta = 0.1$.\n- **Initialization:** All parameters from $\\mathcal{N}(0, 0.01^2)$ with a fixed random seed of $0$.\n- **Test Cases:**\n    - Case $1$: $3$ patients, volumes $(4,4,4)$, $V^{(p)}[i,j,k] = p + i - j + 2k$, times $[6,12,12]$, events $[1,1,0]$.\n    - Case $2$: $3$ patients, volumes $(4,4,4)$, $V^{(p)}[i,j,k] = (i+1)(j+1) + k - p$, times $[5,7,9]$, events $[0,0,0]$.\n    - Case $3$: $2$ patients, volumes $(3,3,3)$, $V^{(p)}[i,j,k] = (-1)^p \\cdot (i \\cdot j + k)$, times $[10,10]$, events $[1,1]$.\n    - Case $4$: $1$ patient, volume $(3,3,3)$, $V^{(0)}[i,j,k] = i + j + k$, time $[8]$, event $[1]$.\n- **Output:** A list of lists of the form $[[\\mathcal{L}_{1,before}, \\mathcal{L}_{1,after}], \\dots, [\\mathcal{L}_{4,before}, \\mathcal{L}_{4,after}]]$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, drawing on established principles of deep learning (CNNs) and survival analysis (Cox model). The provided formulas are standard definitions. The problem is well-posed, providing a complete and deterministic set of instructions, including data generation, model architecture, hyperparameters, and evaluation metric, ensuring a unique and verifiable solution. The language is objective and precise. The component dimensions are consistent: a $(4,4,4)$ input with a $(3,3,3)$ kernel and 'valid' padding yields a $(2,2,2)$ output, and a $(3,3,3)$ input yields a $(1,1,1)$ output. The problem is computationally intensive but feasible and not trivial, as it requires a careful manual implementation of backpropagation. There are no contradictions, ambiguities, or factual errors.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A solution will be developed.\n\n**Solution Design**\n\nThe solution requires implementing a full forward and backward pass for a small CNN. We will structure the implementation modularly.\n\n**1. Parameter Initialization**\nAll learnable parameters—convolutional weights $W \\in \\mathbb{R}^{2 \\times 1 \\times 3 \\times 3 \\times 3}$, convolutional biases $b_{conv} \\in \\mathbb{R}^2$, linear layer weights $\\mathbf{w} \\in \\mathbb{R}^2$, and linear layer bias $b_{lin} \\in \\mathbb{R}$—are initialized from a normal distribution $\\mathcal{N}(\\mu=0, \\sigma^2=0.01^2)$ using a random seed of $0$. This process is repeated for each of the four test cases.\n\n**2. Forward Pass**\nFor each patient $k$ with input volume $X^{(k)}$, the following steps are performed to compute the risk score $r_k$:\n\n- **Preprocessing:** The input volume $X^{(k)}$ is normalized to $\\tilde{X}^{(k)}$:\n$$ \\mu_k = \\mathrm{mean}(X^{(k)}), \\quad \\sigma_k = \\mathrm{std}(X^{(k)}) $$\n$$ \\tilde{X}^{(k)} = \\frac{X^{(k)} - \\mu_k}{\\sigma_k + 10^{-6}} $$\n- **Convolution:** The normalized volume $\\tilde{X}^{(k)}$ (with channel dimension added) is passed through the 3D convolutional layer, producing a pre-activation tensor $Z^{(k)} \\in \\mathbb{R}^{2 \\times D_{out} \\times H_{out} \\times W_{out}}$. For an output channel $c \\in \\{0, 1\\}$ and spatial indices $(d,h,w)$:\n$$ Z^{(k)}_c(d,h,w) = \\sum_{u=0}^{2} \\sum_{v=0}^{2} \\sum_{s=0}^{2} W_{c,0}(u,v,s) \\cdot \\tilde{X}^{(k)}(d+u, h+v, w+s) + b_{conv,c} $$\nThe output dimensions are $(4-3+1, 4-3+1, 4-3+1) = (2,2,2)$ for cases $1$ and $2$, and $(3-3+1, 3-3+1, 3-3+1) = (1,1,1)$ for cases $3$ and $4$.\n- **Activation:** The Softplus activation with $s=1$ is applied element-wise to $Z^{(k)}$ to get the activation tensor $A^{(k)}$:\n$$ A^{(k)}_c(d,h,w) = \\log\\left(1 + e^{Z^{(k)}_c(d,h,w)}\\right) $$\n- **Global Average Pooling:** The spatial dimensions of $A^{(k)}$ are averaged for each channel to produce a feature vector $\\mathbf{p}_k \\in \\mathbb{R}^2$:\n$$ p_{k,c} = \\frac{1}{D_{out} H_{out} W_{out}} \\sum_{d=0}^{D_{out}-1} \\sum_{h=0}^{H_{out}-1} \\sum_{w=0}^{W_{out}-1} A^{(k)}_c(d,h,w) $$\n- **Linear Layer:** The feature vector $\\mathbf{p}_k$ is mapped to a scalar risk score $r_k$:\n$$ r_k = \\mathbf{w}^\\top \\mathbf{p}_k + b_{lin} = w_0 p_{k,0} + w_1 p_{k,1} + b_{lin} $$\n\n**3. Loss Calculation**\nAfter computing risk scores $\\mathbf{r} = \\{r_k\\}$ for all patients in a case, the Cox negative log partial likelihood $\\mathcal{L}(\\mathbf{r})$ is calculated using the given formula, survival times $\\mathbf{t}$, and event indicators $\\mathbf{e}$.\n\n**4. Backward Pass (Gradient Calculation)**\nWe use the chain rule to compute gradients of the loss $\\mathcal{L}$ with respect to all parameters, accumulating gradients over all patients in the batch. Let $\\nabla_{param} \\mathcal{L}$ denote the gradient of the loss with respect to a parameter.\n\n- **Gradient of Loss w.r.t. Risk Scores:** This is the initial gradient signal, provided by the problem:\n$$ \\delta_k^{(r)} \\equiv \\frac{\\partial \\mathcal{L}}{\\partial r_k} = - e_k + \\sum_{\\tau \\in T^+:\\, t_k \\ge \\tau} m_\\tau \\cdot \\frac{e^{r_k}}{\\sum_{j: t_j \\ge \\tau} e^{r_j}} $$\n- **Gradients for Linear Layer:**\n$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\sum_k \\frac{\\partial \\mathcal{L}}{\\partial r_k} \\frac{\\partial r_k}{\\partial \\mathbf{w}} = \\sum_k \\delta_k^{(r)} \\mathbf{p}_k^\\top $$\n$$ \\nabla_{b_{lin}} \\mathcal{L} = \\sum_k \\frac{\\partial \\mathcal{L}}{\\partial r_k} \\frac{\\partial r_k}{\\partial b_{lin}} = \\sum_k \\delta_k^{(r)} $$\n- **Gradient Backpropagated to Pooled Features:** The gradient signal propagated to the pooled features $\\mathbf{p}_k$ is:\n$$ \\delta_k^{(p)} \\equiv \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{p}_k} = \\frac{\\partial \\mathcal{L}}{\\partial r_k} \\frac{\\partial r_k}{\\partial \\mathbf{p}_k} = \\delta_k^{(r)} \\mathbf{w}^\\top $$\n- **Gradient Backpropagated to Pre-Pooling Activations:** The GAP operation distributes its input gradient $\\delta_k^{(p)}$ uniformly over the spatial dimensions of the activation tensor $A^{(k)}$. Let $N_k = D_{out}H_{out}W_{out}$ be the number of spatial elements. The gradient w.r.t. $A^{(k)}$ is:\n$$ \\delta_k^{(A)}(c,d,h,w) \\equiv \\frac{\\partial \\mathcal{L}}{\\partial A^{(k)}_c(d,h,w)} = \\frac{\\partial \\mathcal{L}}{\\partial p_{k,c}} \\frac{\\partial p_{k,c}}{\\partial A^{(k)}_c(d,h,w)} = \\delta_{k,c}^{(p)} \\cdot \\frac{1}{N_k} $$\n- **Gradient Backpropagated to Pre-Activation Outputs:** Using the derivative of the Softplus function:\n$$ \\delta_k^{(Z)}(c,d,h,w) \\equiv \\frac{\\partial \\mathcal{L}}{\\partial Z^{(k)}_c(d,h,w)} = \\delta_k^{(A)}(c,d,h,w) \\cdot \\frac{d \\mathrm{softplus}(Z^{(k)}_c(d,h,w))}{d Z^{(k)}_c(d,h,w)} = \\delta_k^{(A)}(c,d,h,w) \\cdot \\frac{1}{1 + e^{-Z^{(k)}_c(d,h,w)}} $$\n- **Gradients for Convolutional Layer:**\nThe gradients for the weights are computed by correlating the input $\\tilde{X}^{(k)}$ with the output gradient $\\delta_k^{(Z)}$. The total gradient is the sum over all patients. For output channel $c$ and kernel indices $(u,v,s)$:\n$$ \\nabla_{W_{c,0}} \\mathcal{L}(u,v,s) = \\sum_k \\sum_{d=0}^{D_{out}-1} \\sum_{h=0}^{H_{out}-1} \\sum_{w=0}^{W_{out}-1} \\frac{\\partial \\mathcal{L}}{\\partial Z^{(k)}_c(d,h,w)} \\frac{\\partial Z^{(k)}_c(d,h,w)}{\\partial W_{c,0}(u,v,s)} $$\n$$ \\nabla_{W_{c,0}} \\mathcal{L}(u,v,s) = \\sum_k \\sum_{d,h,w} \\delta_k^{(Z)}(c,d,h,w) \\cdot \\tilde{X}^{(k)}(d+u,h+v,w+s) $$\nThe gradients for the biases are the sum of the pre-activation gradients over all spatial positions and all patients:\n$$ \\nabla_{b_{conv,c}} \\mathcal{L} = \\sum_k \\sum_{d,h,w} \\delta_k^{(Z)}(c,d,h,w) $$\n\n**5. Parameter Update**\nA single step of gradient descent is performed with learning rate $\\eta=0.1$. For each parameter $\\theta \\in \\{W, b_{conv}, \\mathbf{w}, b_{lin}\\}$:\n$$ \\theta_{new} = \\theta_{old} - \\eta \\cdot \\nabla_\\theta \\mathcal{L} $$\n\n**6. Final Loss Calculation**\nAfter updating all parameters, the forward pass is executed again with the new parameters to compute updated risk scores $\\mathbf{r}_{new}$. The final loss, $\\mathcal{L}_{after}$, is then calculated using these new risk scores. The result for each case is the pair $[\\mathcal{L}_{before}, \\mathcal{L}_{after}]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the radiomics pipeline problem for all test cases.\n    \"\"\"\n\n    def generate_volume(case_idx, patient_idx, shape):\n        \"\"\"Generates a synthetic volume based on the case and patient index.\"\"\"\n        vol = np.zeros(shape)\n        D, H, W = shape\n        if case_idx == 0: # Case 1\n            for i in range(D):\n                for j in range(H):\n                    for k in range(W):\n                        vol[i, j, k] = patient_idx + i - j + 2 * k\n        elif case_idx == 1: # Case 2\n            for i in range(D):\n                for j in range(H):\n                    for k in range(W):\n                        vol[i, j, k] = (i + 1) * (j + 1) + k - patient_idx\n        elif case_idx == 2: # Case 3\n            for i in range(D):\n                for j in range(H):\n                    for k in range(W):\n                        vol[i, j, k] = ((-1)**patient_idx) * (i * j + k)\n        elif case_idx == 3: # Case 4\n            for i in range(D):\n                for j in range(H):\n                    for k in range(W):\n                        vol[i, j, k] = i + j + k\n        return vol\n\n    def initialize_parameters(seed):\n        \"\"\"Initializes all learnable parameters with a given seed.\"\"\"\n        rng = np.random.default_rng(seed)\n        std_dev = 0.01\n        \n        # Conv weights: (C_out, C_in, K_d, K_h, K_w)\n        W_conv = rng.normal(0, std_dev, (2, 1, 3, 3, 3))\n        # Conv biases: (C_out)\n        b_conv = rng.normal(0, std_dev, 2)\n        # Linear weights: (C_out)\n        w_lin = rng.normal(0, std_dev, 2)\n        # Linear bias: (scalar)\n        b_lin = rng.normal(0, std_dev, 1)[0]\n        \n        return {\n            \"W_conv\": W_conv, \"b_conv\": b_conv,\n            \"w_lin\": w_lin, \"b_lin\": b_lin\n        }\n\n    def softplus(x):\n        return np.log(1 + np.exp(x))\n\n    def softplus_grad(x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward_pass(volumes, params):\n        \"\"\"Performs a full forward pass from volumes to risk scores.\"\"\"\n        n_patients = len(volumes)\n        risks = np.zeros(n_patients)\n        cache = {\n            \"normalized_volumes\": [],\n            \"pre_activations\": [],\n            \"activations\": [],\n            \"pooled_features\": []\n        }\n\n        C_out = params[\"W_conv\"].shape[0]\n        K_d, K_h, K_w = params[\"W_conv\"].shape[2:]\n\n        for p_idx in range(n_patients):\n            X = volumes[p_idx]\n            \n            # 1. Preprocessing (Z-score normalization)\n            mu = np.mean(X)\n            sigma = np.std(X)\n            epsilon = 1e-6\n            X_tilde = (X - mu) / (sigma + epsilon)\n            cache[\"normalized_volumes\"].append(X_tilde)\n\n            # 2. Convolution\n            D_in, H_in, W_in = X_tilde.shape\n            D_out, H_out, W_out = D_in - K_d + 1, H_in - K_h + 1, W_in - K_w + 1\n            Z = np.zeros((C_out, D_out, H_out, W_out))\n            for c in range(C_out):\n                for d in range(D_out):\n                    for h in range(H_out):\n                        for w in range(W_out):\n                            vol_slice = X_tilde[d:d+K_d, h:h+K_h, w:w+K_w]\n                            Z[c, d, h, w] = np.sum(vol_slice * params[\"W_conv\"][c, 0]) + params[\"b_conv\"][c]\n            cache[\"pre_activations\"].append(Z)\n\n            # 3. Activation\n            A = softplus(Z)\n            cache[\"activations\"].append(A)\n\n            # 4. Global Average Pooling\n            p_vec = np.mean(A, axis=(1, 2, 3))\n            cache[\"pooled_features\"].append(p_vec)\n            \n            # 5. Linear Layer\n            risk = np.dot(params[\"w_lin\"], p_vec) + params[\"b_lin\"]\n            risks[p_idx] = risk\n\n        return risks, cache\n\n    def compute_cox_loss(risks, times, events):\n        \"\"\"Computes the Cox negative log partial likelihood.\"\"\"\n        unique_event_times = sorted(list(set(t for t, e in zip(times, events) if e == 1)))\n        if not unique_event_times:\n            return 0.0\n\n        loss = 0.0\n        for tau in unique_event_times:\n            risk_set_indices = [i for i, t in enumerate(times) if t >= tau]\n            event_set_indices = [i for i, (t, e) in enumerate(zip(times, events)) if t == tau and e == 1]\n            \n            sum_risk_at_tau = np.sum(risks[event_set_indices])\n            sum_exp_risk_in_risk_set = np.sum(np.exp(risks[risk_set_indices]))\n            \n            m_tau = len(event_set_indices)\n            \n            loss -= sum_risk_at_tau\n            loss += m_tau * np.log(sum_exp_risk_in_risk_set)\n            \n        return loss\n\n    def backward_pass(risks, times, events, cache, params):\n        \"\"\"Performs a full backward pass to compute gradients.\"\"\"\n        n_patients = len(risks)\n        \n        # Initialize gradients\n        grads = {\n            \"W_conv\": np.zeros_like(params[\"W_conv\"]),\n            \"b_conv\": np.zeros_like(params[\"b_conv\"]),\n            \"w_lin\": np.zeros_like(params[\"w_lin\"]),\n            \"b_lin\": 0.0\n        }\n\n        # 1. Gradient of Loss w.r.t. risk scores (delta^r)\n        delta_r = np.zeros(n_patients)\n        unique_event_times = sorted(list(set(t for t, e in zip(times, events) if e == 1)))\n        \n        for k in range(n_patients):\n            delta_r[k] = -events[k]\n            for tau in unique_event_times:\n                if times[k] >= tau:\n                    risk_set_indices = [j for j, t in enumerate(times) if t >= tau]\n                    event_set_indices = [j for j, (t, e) in enumerate(zip(times, events)) if t == tau and e == 1]\n                    m_tau = len(event_set_indices)\n                    sum_exp_risk_in_risk_set = np.sum(np.exp(risks[risk_set_indices]))\n                    \n                    if sum_exp_risk_in_risk_set > 0:\n                        delta_r[k] += m_tau * np.exp(risks[k]) / sum_exp_risk_in_risk_set\n\n        # Accumulate gradients over all patients\n        for p_idx in range(n_patients):\n            p_vec = cache[\"pooled_features\"][p_idx]\n            Z = cache[\"pre_activations\"][p_idx]\n            A = cache[\"activations\"][p_idx]\n            X_tilde = cache[\"normalized_volumes\"][p_idx]\n            \n            # 2. Gradients for Linear Layer\n            grads[\"w_lin\"] += delta_r[p_idx] * p_vec\n            grads[\"b_lin\"] += delta_r[p_idx]\n            \n            # 3. Backprop to pooled features (delta^p)\n            delta_p = delta_r[p_idx] * params[\"w_lin\"]\n            \n            # 4. Backprop to pre-pooling activations (delta^A)\n            _, D_out, H_out, W_out = A.shape\n            N_spatial = D_out * H_out * W_out\n            delta_A = np.zeros_like(A)\n            for c in range(len(delta_p)):\n                delta_A[c, :, :, :] = delta_p[c] / N_spatial\n                \n            # 5. Backprop to pre-activation outputs (delta^Z)\n            delta_Z = delta_A * softplus_grad(Z)\n            \n            # 6. Gradients for Convolutional Layer\n            K_d, K_h, K_w = params[\"W_conv\"].shape[2:]\n            C_out = params[\"W_conv\"].shape[0]\n            for c in range(C_out):\n                grads[\"b_conv\"][c] += np.sum(delta_Z[c])\n                for u in range(K_d):\n                    for v in range(K_h):\n                        for s in range(K_w):\n                            # Correlation\n                            grad_W_component = 0.0\n                            for d in range(D_out):\n                                for h in range(H_out):\n                                    for w in range(W_out):\n                                        grad_W_component += delta_Z[c, d, h, w] * X_tilde[d + u, h + v, w + s]\n                            grads[\"W_conv\"][c, 0, u, v, s] += grad_W_component\n\n        return grads\n        \n    test_cases = [\n        {'shape': (4, 4, 4), 'times': [6, 12, 12], 'events': [1, 1, 0]},\n        {'shape': (4, 4, 4), 'times': [5, 7, 9], 'events': [0, 0, 0]},\n        {'shape': (3, 3, 3), 'times': [10, 10], 'events': [1, 1]},\n        {'shape': (3, 3, 3), 'times': [8], 'events': [1]},\n    ]\n    \n    results = []\n    learning_rate = 0.1\n\n    for i, case in enumerate(test_cases):\n        # Reset parameters for each case\n        params = initialize_parameters(seed=0)\n        \n        n_patients = len(case['times'])\n        volumes = [generate_volume(i, p, case['shape']) for p in range(n_patients)]\n\n        # --- Before Update ---\n        risks_before, cache = forward_pass(volumes, params)\n        loss_before = compute_cox_loss(risks_before, case['times'], case['events'])\n\n        # --- Gradient Calculation and Update ---\n        grads = backward_pass(risks_before, case['times'], case['events'], cache, params)\n        \n        updated_params = {}\n        for key in params:\n            updated_params[key] = params[key] - learning_rate * grads[key]\n\n        # --- After Update ---\n        risks_after, _ = forward_pass(volumes, updated_params)\n        loss_after = compute_cox_loss(risks_after, case['times'], case['events'])\n        \n        results.append([loss_before, loss_after])\n\n    # Format the final output string\n    result_str_parts = []\n    for r in results:\n        result_str_parts.append(f\"[{r[0]:.8f},{r[1]:.8f}]\")\n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4534311"}]}