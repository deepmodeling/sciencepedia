{"hands_on_practices": [{"introduction": "Before applying complex resampling techniques, it's crucial to understand how statistical classifiers can inherently account for class imbalance. This practice explores how the Bayes decision rule, the foundation of many classifiers, incorporates prior class probabilities. By deriving the decision boundary for Linear Discriminant Analysis ($LDA$) from first principles, you will see precisely how the threshold shifts to favor the majority class and quantify this effect in a practical radiomics scenario [@problem_id:4543135].", "problem": "A radiomics classifier is built to discriminate between two tumor phenotypes using two standardized features, denoted by the feature vector $\\mathbf{x} \\in \\mathbb{R}^{2}$. Because aggressive tumors are rarer in the dataset than indolent tumors, class imbalance must be incorporated through prior odds in the classifier. Assume the following foundational model:\n\n- The two class-conditional distributions $p(\\mathbf{x}\\,|\\,y=k)$, for $k \\in \\{0,1\\}$, are multivariate normal with a common covariance matrix, that is, $p(\\mathbf{x}\\,|\\,y=k) = \\mathcal{N}(\\boldsymbol{\\mu}_{k}, \\boldsymbol{\\Sigma})$ with the same $\\boldsymbol{\\Sigma}$ for both classes.\n- The Bayes decision rule under equal misclassification costs selects the class with the larger posterior probability $p(y=k\\,|\\,\\mathbf{x})$, equivalently using a likelihood ratio test that includes the class priors $p(y=1)$ and $p(y=0)$.\n- The multivariate normal density is given by\n$$\np(\\mathbf{x}\\,|\\,y=k) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})\\right),\n$$\nwhere $d=2$.\n\nTask 1 (derivation): Starting only from the Bayes decision rule and the form of $p(\\mathbf{x}\\,|\\,y=k)$ above with a common $\\boldsymbol{\\Sigma}$, derive the Linear Discriminant Analysis (LDA) decision rule that explicitly incorporates the prior odds $p(y=1)/p(y=0)$. Show that the rule can be written in the form “decide $y=1$ if $\\mathbf{w}^{\\top}\\mathbf{x} \\ge t$ and $y=0$ otherwise,” and identify $\\mathbf{w}$ and $t$ in terms of $\\boldsymbol{\\mu}_{0}$, $\\boldsymbol{\\mu}_{1}$, $\\boldsymbol{\\Sigma}$, and the priors $p(y=1)$ and $p(y=0)$. Briefly explain how the prior odds shift the threshold.\n\nTask 2 (calculation): In a specific radiomics study, the estimated parameters from training are\n$$\n\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\\quad\n\\boldsymbol{\\mu}_{0} = \\begin{pmatrix} 0 \\\\ \\tfrac{1}{2} \\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\tfrac{3}{2} & 0 \\\\ 0 & \\tfrac{1}{2} \\end{pmatrix},\n$$\nand the estimated class priors (reflecting class imbalance) are $p(y=1) = 0.15$ and $p(y=0) = 0.85$. Using your derivation from Task 1, compute the scalar threshold $t$ in the one-dimensional discriminant $y = \\mathbf{w}^{\\top}\\mathbf{x}$. Round your final numerical value of $t$ to four significant figures. Do not include any units in your answer.", "solution": "The problem is assessed to be valid. It is scientifically grounded in statistical pattern recognition, well-posed with sufficient and consistent information, and objectively stated. It constitutes a standard derivation and application of Linear Discriminant Analysis (LDA).\n\nTask 1: Derivation of the LDA Decision Rule\n\nThe Bayes decision rule dictates that we should choose the class $k$ that maximizes the posterior probability $p(y=k\\,|\\,\\mathbf{x})$. To discriminate between class $y=1$ and class $y=0$, we decide $y=1$ if its posterior probability is greater than or equal to that of class $y=0$. (The boundary case $p(y=1\\,|\\,\\mathbf{x}) = p(y=0\\,|\\,\\mathbf{x})$ is assigned to class $1$ according to the problem's decision rule format $\\mathbf{w}^{\\top}\\mathbf{x} \\ge t$).\nThe rule is: Decide $y=1$ if $p(y=1\\,|\\,\\mathbf{x}) \\ge p(y=0\\,|\\,\\mathbf{x})$.\n\nUsing Bayes' theorem, $p(y=k\\,|\\,\\mathbf{x}) = \\frac{p(\\mathbf{x}\\,|\\,y=k)p(y=k)}{p(\\mathbf{x})}$, the decision rule becomes:\n$$\n\\frac{p(\\mathbf{x}\\,|\\,y=1)p(y=1)}{p(\\mathbf{x})} \\ge \\frac{p(\\mathbf{x}\\,|\\,y=0)p(y=0)}{p(\\mathbf{x})}\n$$\nThe evidence term $p(\\mathbf{x})$ is a positive common factor and can be canceled, yielding a rule based on the likelihoods and priors:\n$$\np(\\mathbf{x}\\,|\\,y=1)p(y=1) \\ge p(\\mathbf{x}\\,|\\,y=0)p(y=0)\n$$\nTo simplify the expression, we take the natural logarithm of both sides. As the logarithm is a monotonically increasing function, this does not alter the decision boundary.\n$$\n\\ln(p(\\mathbf{x}\\,|\\,y=1)) + \\ln(p(y=1)) \\ge \\ln(p(\\mathbf{x}\\,|\\,y=0)) + \\ln(p(y=0))\n$$\nThe class-conditional probability density for a multivariate normal distribution is given as:\n$$\np(\\mathbf{x}\\,|\\,y=k) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\!\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})\\right)\n$$\nThe natural logarithm of this density is:\n$$\n\\ln(p(\\mathbf{x}\\,|\\,y=k)) = -\\ln\\left((2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}\\right) - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})\n$$\nSubstituting this into the decision inequality, the constant term $-\\ln\\left((2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}\\right)$ is the same for both classes (since $\\boldsymbol{\\Sigma}$ is common) and thus cancels out:\n$$\n-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_{1})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{1}) + \\ln(p(y=1)) \\ge -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_{0})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{0}) + \\ln(p(y=0))\n$$\nNow, we expand the quadratic forms $(\\mathbf{x}-\\boldsymbol{\\mu}_{k})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})$:\n$$\n(\\mathbf{x}-\\boldsymbol{\\mu}_{k})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{k}) = \\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}_{k}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_{k}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{k}\n$$\nSubstituting these expansions into the inequality gives:\n$$\n-\\frac{1}{2}\\left(\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1}\\right) + \\ln(p(y=1)) \\ge -\\frac{1}{2}\\left(\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - 2\\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} + \\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{0}\\right) + \\ln(p(y=0))\n$$\nThe $\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$ terms are present on both sides and cancel. This cancellation is what makes the decision boundary linear. We are left with:\n$$\n\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1} + \\ln(p(y=1)) \\ge \\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2}\\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{0} + \\ln(p(y=0))\n$$\nRearranging to group terms involving $\\mathbf{x}$ on the left-hand side and all other (constant) terms on the right-hand side:\n$$\n(\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1} - \\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1})\\mathbf{x} \\ge \\frac{1}{2}\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1} - \\frac{1}{2}\\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{0} + \\ln(p(y=0)) - \\ln(p(y=1))\n$$\nThis can be rewritten as:\n$$\n(\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0})^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x} \\ge \\frac{1}{2}(\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{0}) + \\ln\\left(\\frac{p(y=0)}{p(y=1)}\\right)\n$$\nThis expression is in the desired form $\\mathbf{w}^{\\top}\\mathbf{x} \\ge t$. We identify the linear discriminant vector $\\mathbf{w}$ and the scalar threshold $t$.\nFrom the expression $(\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0})^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}$, we have $\\mathbf{w}^{\\top} = (\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0})^{\\top}\\boldsymbol{\\Sigma}^{-1}$. Taking the transpose and noting that $\\boldsymbol{\\Sigma}^{-1}$ is symmetric (as $\\boldsymbol{\\Sigma}$ is), we find:\n$$\n\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0})\n$$\nThe threshold $t$ is the entire right-hand side of the inequality:\n$$\nt = \\frac{1}{2}(\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{0}) + \\ln\\left(\\frac{p(y=0)}{p(y=1)}\\right)\n$$\nThe prior odds $\\frac{p(y=1)}{p(y=0)}$ are incorporated through the term $\\ln\\left(\\frac{p(y=0)}{p(y=1)}\\right) = -\\ln\\left(\\frac{p(y=1)}{p(y=0)}\\right)$. This term directly shifts the decision threshold $t$. If class $1$ is the minority class, then $p(y=1) < p(y=0)$, which makes the ratio $\\frac{p(y=0)}{p(y=1)} > 1$ and its logarithm positive. A positive value for this log-odds term increases the threshold $t$. A higher threshold means that the projected value $\\mathbf{w}^{\\top}\\mathbf{x}$ must be larger to classify an observation as belonging to class $1$. This effectively shrinks the decision region for the minority class, requiring stronger evidence from the data (the likelihood term) to overcome the low prior probability.\n\nTask 2: Calculation of the Threshold $t$\n\nThe given parameters are:\n$$\n\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\boldsymbol{\\mu}_{0} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\frac{3}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}\n$$\n$$\np(y=1) = 0.15, \\quad p(y=0) = 0.85\n$$\nWe must compute the threshold $t$ using the derived formula. First, we find the inverse of the covariance matrix $\\boldsymbol{\\Sigma}$. Since $\\boldsymbol{\\Sigma}$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal elements:\n$$\n\\boldsymbol{\\Sigma}^{-1} = \\begin{pmatrix} (\\frac{3}{2})^{-1} & 0 \\\\ 0 & (\\frac{1}{2})^{-1} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\nNext, we compute the two quadratic terms in the expression for $t$:\n$$\n\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1} = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2 \\cdot \\frac{2}{3} \\cdot 2 + 1 \\cdot 2 \\cdot 1 = \\frac{8}{3} + 2 = \\frac{14}{3}\n$$\n$$\n\\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{0} = \\begin{pmatrix} 0 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix} = 0 \\cdot \\frac{2}{3} \\cdot 0 + \\frac{1}{2} \\cdot 2 \\cdot \\frac{1}{2} = \\frac{1}{2}\n$$\nNow, we compute the first part of the threshold $t$:\n$$\n\\frac{1}{2}(\\boldsymbol{\\mu}_{1}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{0}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{0}) = \\frac{1}{2}\\left(\\frac{14}{3} - \\frac{1}{2}\\right) = \\frac{1}{2}\\left(\\frac{28-3}{6}\\right) = \\frac{1}{2}\\left(\\frac{25}{6}\\right) = \\frac{25}{12}\n$$\nNext, we compute the log-prior-odds term:\n$$\n\\ln\\left(\\frac{p(y=0)}{p(y=1)}\\right) = \\ln\\left(\\frac{0.85}{0.15}\\right) = \\ln\\left(\\frac{17}{3}\\right)\n$$\nFinally, we sum these two parts to get the threshold $t$:\n$$\nt = \\frac{25}{12} + \\ln\\left(\\frac{17}{3}\\right)\n$$\nTo obtain the numerical value, we compute the two terms:\n$$\n\\frac{25}{12} \\approx 2.08333...\n$$\n$$\n\\ln\\left(\\frac{17}{3}\\right) \\approx 1.73460...\n$$\n$$\nt \\approx 2.08333 + 1.73460 = 3.81793...\n$$\nRounding the result to four significant figures gives $t = 3.818$.", "answer": "$$\\boxed{3.818}$$", "id": "4543135"}, {"introduction": "Handling class imbalance is not just about choosing the right algorithm, but also about applying it correctly within a validation framework. A common and serious mistake is to perform oversampling on the entire dataset before splitting it for cross-validation, which leads to data leakage and overly optimistic results. This practice provides a quantitative look into this pitfall by asking you to calculate the expected inflation of the Area Under the Receiver Operating Characteristic Curve ($AUC$) when this error occurs, cementing the importance of proper validation hygiene [@problem_id:4543122].", "problem": "In a radiomics study, you are building a binary classifier to distinguish malignant versus benign lesions using quantitative image features. The positive (malignant) class is the minority. You evaluate the model using $k$-fold cross-validation and the Area Under the Receiver Operating Characteristic Curve (AUC), formally the Area Under the Receiver Operating Characteristic Curve (AUC). An implementation error applies Synthetic Minority Oversampling Technique (SMOTE) oversampling before the cross-validation split, so that synthetic samples derived from validation positives enter the training fold. As a result, the trained model partially memorizes patterns of certain validation positives, producing an additive increase in their predicted real-valued margin scores. \n\nAssume the following:\n1. Without any leakage, the validation scores for positive and negative cases are independent and normally distributed with equal variance: the positive scores $S_{\\text{pos}} \\sim \\mathcal{N}(\\mu_{p}, \\sigma^{2})$ and the negative scores $S_{\\text{neg}} \\sim \\mathcal{N}(\\mu_{n}, \\sigma^{2})$.\n2. Due to the incorrect oversampling order, a fraction $\\alpha$ of validation positive cases had synthetic clones in the training fold. For those positives, the trained model’s validation scores are shifted upward by an additive constant $\\delta$, so their scores follow $\\mathcal{N}(\\mu_{p}+\\delta, \\sigma^{2})$. The remaining fraction $1-\\alpha$ of positive scores remain $\\mathcal{N}(\\mu_{p}, \\sigma^{2})$. The negative scores remain $\\mathcal{N}(\\mu_{n}, \\sigma^{2})$ and independent of the positives.\n3. Treat the AUC as the probability that a randomly drawn positive has a higher score than a randomly drawn negative under the corresponding score distributions.\n\nSuppose the parameters are $\\mu_{p}-\\mu_{n}=1$, $\\sigma=1$, $\\delta=1$, and $\\alpha=0.2$. Under these assumptions, derive from first principles the expected AUC without leakage and the expected AUC with leakage, then compute the expected inflation in AUC defined as the difference between the leaky AUC and the clean AUC. Express your final result as a decimal and round your answer to four significant figures.", "solution": "The problem requires the calculation of the expected inflation in the Area Under the Receiver Operating Characteristic Curve (AUC) due to a specific data leakage scenario in a radiomics classification task. The leakage arises from applying the Synthetic Minority Oversampling Technique (SMOTE) before splitting the data for $k$-fold cross-validation. We will first derive the expression for the AUC in the absence of leakage, then the AUC with leakage, and finally compute their difference.\n\nLet $S_{\\text{pos}}$ be the score for a positive case and $S_{\\text{neg}}$ be the score for a negative case. The AUC is defined as the probability that a randomly drawn positive has a higher score than a randomly drawn negative, $AUC = P(S_{\\text{pos}} > S_{\\text{neg}})$. This is equivalent to calculating $P(S_{\\text{pos}} - S_{\\text{neg}} > 0)$.\n\n**1. Expected AUC without Leakage ($AUC_{\\text{clean}}$)**\n\nAccording to the problem statement, in the scenario without leakage, the scores are distributed as:\n$S_{\\text{pos}} \\sim \\mathcal{N}(\\mu_{p}, \\sigma^{2})$\n$S_{\\text{neg}} \\sim \\mathcal{N}(\\mu_{n}, \\sigma^{2})$\n\nLet the difference be $D = S_{\\text{pos}} - S_{\\text{neg}}$. Since $S_{\\text{pos}}$ and $S_{\\text{neg}}$ are independent normal random variables, their difference $D$ is also normally distributed.\nThe mean of the difference is $E[D] = E[S_{\\text{pos}}] - E[S_{\\text{neg}}] = \\mu_{p} - \\mu_{n}$.\nThe variance of the difference is $\\text{Var}(D) = \\text{Var}(S_{\\text{pos}}) + \\text{Var}(S_{\\text{neg}}) = \\sigma^{2} + \\sigma^{2} = 2\\sigma^{2}$.\nSo, $D \\sim \\mathcal{N}(\\mu_{p} - \\mu_{n}, 2\\sigma^{2})$.\n\nThe clean AUC is the probability $P(D > 0)$. To calculate this, we standardize the variable $D$:\n$$\nAUC_{\\text{clean}} = P(D > 0) = P\\left(\\frac{D - (\\mu_{p} - \\mu_{n})}{\\sqrt{2\\sigma^{2}}} > \\frac{0 - (\\mu_{p} - \\mu_{n})}{\\sqrt{2\\sigma^{2}}}\\right)\n$$\nLet $Z = \\frac{D - (\\mu_{p} - \\mu_{n})}{\\sqrt{2}\\sigma}$ be a standard normal variable, $Z \\sim \\mathcal{N}(0, 1)$.\n$$\nAUC_{\\text{clean}} = P\\left(Z > -\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right)\n$$\nUsing the property of the standard normal cumulative distribution function (CDF), $\\Phi(z)$, that $P(Z > -z) = P(Z < z) = \\Phi(z)$, we get:\n$$\nAUC_{\\text{clean}} = \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right)\n$$\n\n**2. Expected AUC with Leakage ($AUC_{\\text{leaky}}$)**\n\nWith data leakage, a fraction $\\alpha$ of the positive samples have their scores drawn from $\\mathcal{N}(\\mu_{p}+\\delta, \\sigma^{2})$, while the remaining fraction $1-\\alpha$ are drawn from $\\mathcal{N}(\\mu_{p}, \\sigma^{2})$. The negative scores remain distributed as $\\mathcal{N}(\\mu_{n}, \\sigma^{2})$.\n\nLet $S'_{\\text{pos}}$ be the score of a randomly selected positive case in the leaky scenario. Its distribution is a two-component mixture. We can compute the leaky AUC, $AUC_{\\text{leaky}} = P(S'_{\\text{pos}} > S_{\\text{neg}})$, using the law of total probability, conditioning on whether the positive sample was from the affected group.\n\n$$\nAUC_{\\text{leaky}} = P(S'_{\\text{pos}} > S_{\\text{neg}} | \\text{affected})P(\\text{affected}) + P(S'_{\\text{pos}} > S_{\\text{neg}} | \\text{unaffected})P(\\text{unaffected})\n$$\n\nHere, $P(\\text{affected}) = \\alpha$ and $P(\\text{unaffected}) = 1-\\alpha$.\n\nThe term for unaffected positives is identical to the clean case:\n$P(S'_{\\text{pos}} > S_{\\text{neg}} | \\text{unaffected}) = \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right)$.\n\nFor the affected positives, their scores are $S_{\\text{pos, aff}} \\sim \\mathcal{N}(\\mu_{p}+\\delta, \\sigma^{2})$. The difference is $D_{\\text{aff}} = S_{\\text{pos, aff}} - S_{\\text{neg}}$.\nThe distribution of this difference is $D_{\\text{aff}} \\sim \\mathcal{N}((\\mu_{p}+\\delta) - \\mu_{n}, 2\\sigma^{2})$.\nThe corresponding AUC component is:\n$P(D_{\\text{aff}} > 0) = \\Phi\\left(\\frac{(\\mu_{p} - \\mu_{n}) + \\delta}{\\sqrt{2}\\sigma}\\right)$.\n\nCombining these, the total leaky AUC is:\n$$\nAUC_{\\text{leaky}} = \\alpha \\cdot \\Phi\\left(\\frac{(\\mu_{p} - \\mu_{n}) + \\delta}{\\sqrt{2}\\sigma}\\right) + (1-\\alpha) \\cdot \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right)\n$$\n\n**3. AUC Inflation ($\\Delta \\text{AUC}$)**\n\nThe inflation in AUC is the difference between the leaky and clean AUCs:\n$\\Delta \\text{AUC} = AUC_{\\text{leaky}} - AUC_{\\text{clean}}$\n$$\n\\Delta \\text{AUC} = \\left[ \\alpha \\Phi\\left(\\frac{(\\mu_{p} - \\mu_{n}) + \\delta}{\\sqrt{2}\\sigma}\\right) + (1-\\alpha) \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right) \\right] - \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right)\n$$\n$$\n\\Delta \\text{AUC} = \\alpha \\Phi\\left(\\frac{(\\mu_{p} - \\mu_{n}) + \\delta}{\\sqrt{2}\\sigma}\\right) + \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right) - \\alpha \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right) - \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right)\n$$\n$$\n\\Delta \\text{AUC} = \\alpha \\left[ \\Phi\\left(\\frac{(\\mu_{p} - \\mu_{n}) + \\delta}{\\sqrt{2}\\sigma}\\right) - \\Phi\\left(\\frac{\\mu_{p} - \\mu_{n}}{\\sqrt{2}\\sigma}\\right) \\right]\n$$\n\n**4. Numerical Calculation**\n\nWe are given the parameter values: $\\mu_{p}-\\mu_{n}=1$, $\\sigma=1$, $\\delta=1$, and $\\alpha=0.2$.\nSubstituting these values into the expression for $\\Delta \\text{AUC}$:\n$$\n\\Delta \\text{AUC} = 0.2 \\left[ \\Phi\\left(\\frac{1 + 1}{\\sqrt{2} \\cdot 1}\\right) - \\Phi\\left(\\frac{1}{\\sqrt{2} \\cdot 1}\\right) \\right]\n$$\n$$\n\\Delta \\text{AUC} = 0.2 \\left[ \\Phi\\left(\\frac{2}{\\sqrt{2}}\\right) - \\Phi\\left(\\frac{1}{\\sqrt{2}}\\right) \\right]\n$$\n$$\n\\Delta \\text{AUC} = 0.2 \\left[ \\Phi(\\sqrt{2}) - \\Phi\\left(\\frac{1}{\\sqrt{2}}\\right) \\right]\n$$\nNow we compute the numerical values of the arguments:\n$\\sqrt{2} \\approx 1.41421$\n$1/\\sqrt{2} \\approx 0.70711$\n\nUsing standard normal CDF values:\n$\\Phi(\\sqrt{2}) \\approx \\Phi(1.41421) \\approx 0.92135$\n$\\Phi(1/\\sqrt{2}) \\approx \\Phi(0.70711) \\approx 0.76025$\n\nSubstituting these into the expression for $\\Delta \\text{AUC}$:\n$$\n\\Delta \\text{AUC} \\approx 0.2 \\times (0.92135 - 0.76025)\n$$\n$$\n\\Delta \\text{AUC} \\approx 0.2 \\times 0.16110\n$$\n$$\n\\Delta \\text{AUC} \\approx 0.03222\n$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.03222$. The first significant figure is $3$, followed by $2$, $2$, and $2$. The fifth significant figure (unwritten) is less than $5$, so the result remains $0.03222$.\n\nFor higher precision:\n$\\Phi(\\sqrt{2}) \\approx 0.921354541$\n$\\Phi(1/\\sqrt{2}) \\approx 0.760250107$\n$\\Delta\\text{AUC} = 0.2 \\times (0.921354541 - 0.760250107) = 0.2 \\times 0.161104434 = 0.0322208868$.\nRounding to four significant figures gives $0.03222$.", "answer": "$$\n\\boxed{0.03222}\n$$", "id": "4543122"}, {"introduction": "While oversampling methods like the Synthetic Minority Over-sampling Technique ($SMOTE$) are powerful, they are not a panacea and can have unintended consequences. These algorithms generate new data points by interpolating between existing minority samples, a process that can fail when the minority class has a complex, non-convex shape. This exercise challenges you to analyze the geometric behavior of $SMOTE$ and its variants in a stylized scenario, calculating the rate at which they generate 'off-manifold' samples that can confuse a classifier and degrade performance [@problem_id:4543115].", "problem": "A radiomics pipeline for tumor phenotype classification uses features derived from texture and intensity statistics and applies Principal Component Analysis (PCA) to reduce dimensionality prior to training. Consider a single principal component coordinate $t \\in \\mathbb{R}$ that captures most of the variance of a minority class (e.g., a rare but clinically important phenotype). Empirical analysis indicates that the minority support in this coordinate is nonconvex, consisting of the union of two disjoint intervals $S = [0, 1] \\cup [3, 4]$, while the majority class occupies the gap $G = (1, 3)$ and regions outside $[0, 4]$. Synthetic oversampling is performed to address class imbalance using three methods: Synthetic Minority Over-sampling Technique (SMOTE), Borderline-SMOTE, and Adaptive Synthetic Sampling (ADASYN). Each method generates a synthetic sample $z$ by picking a minority base point $x \\in S$, a minority neighbor $y \\in S$, and a mixing coefficient $\\lambda \\sim \\mathrm{Uniform}(0, 1)$, and then setting\n$$\nz \\;=\\; x + \\lambda (y - x).\n$$\nAssume the following stylized selection model consistent with these methods in the one-dimensional PCA coordinate $t$:\n- For SMOTE, the neighbor $y$ lies in the opposite interval to $x$ with probability $p_{\\mathrm{SMOTE}} = \\frac{1}{20}$ and lies in the same interval with probability $1 - p_{\\mathrm{SMOTE}}$.\n- For Borderline-SMOTE, the neighbor $y$ lies in the opposite interval to $x$ with probability $p_{\\mathrm{B}} = \\frac{3}{20}$ and lies in the same interval with probability $1 - p_{\\mathrm{B}}$.\n- For Adaptive Synthetic Sampling (ADASYN), the neighbor $y$ lies in the opposite interval to $x$ with probability $p_{\\mathrm{ADASYN}} = \\frac{1}{10}$ and lies in the same interval with probability $1 - p_{\\mathrm{ADASYN}}$.\n\nThese probabilities arise from empirical local-neighborhood diagnostics of the minority $k$-nearest neighbor graph (with small $k$), where Borderline-SMOTE emphasizes boundary points and ADASYN emphasizes locally difficult regions, both increasing cross-interval connections relative to SMOTE. In all three methods, the conditional distribution of $\\lambda$ given $(x, y)$ is uniform on $[0, 1]$.\n\nStarting from the core definitions of these oversampling procedures and standard probability measure facts, do the following:\n1. Derive the conditional density of the synthetic sample $z$ given a fixed pair $(x, y)$ for each method in terms of $|y - x|$ and explain how the unconditional synthetic sample distribution is a mixture over all selected $(x, y)$ pairs.\n2. For cross-interval pairs with $x \\in [0, 1]$ and $y \\in [3, 4]$, derive the fraction of $\\lambda \\in [0, 1]$ for which $z$ lies outside the minority support $S$. Then compute the expected value of this fraction when $x$ is uniformly distributed on $[0, 1]$ and $y$ is uniformly distributed on $[3, 4]$, expressing your result in closed form.\n3. Using the result of Part $2$ and the given cross-interval probabilities $p_{\\mathrm{SMOTE}}$, $p_{\\mathrm{B}}$, and $p_{\\mathrm{ADASYN}}$, compute the expected off-manifold generation rate $R$ (the expected fraction of generated samples that fall in the gap $G$) for each method.\n4. Conclude which method minimizes $R$ in this nonconvex-manifold scenario and return its index $i \\in \\{1, 2, 3\\}$ in the order $(\\text{SMOTE}, \\text{Borderline-SMOTE}, \\text{Adaptive Synthetic Sampling})$.\n\nNo rounding is required. Express the final answer as the single index $i$.", "solution": "The user wants me to solve a multi-part problem concerning synthetic oversampling methods in a stylized radiomics feature space. I will first validate the problem statement and then proceed to a full solution if it is deemed valid.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Domain**: Radiomics, synthetic oversampling for class imbalance.\n-   **Feature Space**: A single principal component coordinate `$t \\in \\mathbb{R}$`.\n-   **Minority Class Support**: `$S = [0, 1] \\cup [3, 4]$`.\n-   **Majority Class Support**: `$G = (1, 3)` (the gap) and regions `$(-\\infty, 0) \\cup (4, \\infty)$`.\n-   **Synthetic Sample Generation**: `$z = x + \\lambda (y - x)$`, where `$x, y \\in S$` are minority points and `$\\lambda \\sim \\mathrm{Uniform}(0, 1)$`.\n-   **Cross-Interval Selection Probabilities**:\n    -   SMOTE: `$p_{\\mathrm{SMOTE}} = \\frac{1}{20}$`.\n    -   Borderline-SMOTE: `$p_{\\mathrm{B}} = \\frac{3}{20}$`.\n    -   ADASYN: `$p_{\\mathrm{ADASYN}} = \\frac{1}{10}$`.\n-   **Conditional Distribution of `$\\lambda$`**: `$\\lambda | (x, y) \\sim \\mathrm{Uniform}(0, 1)$`.\n-   **Assumptions for Part 2**: For cross-interval pairs, `$x$` is uniformly distributed on `$[0, 1]$` and `$y$` is uniformly distributed on `$[3, 4]`.\n-   **Tasks**:\n    1.  Derive the conditional density of `$z$` given `$(x, y)` and describe the unconditional distribution.\n    2.  For cross-interval pairs, derive the fraction of `$\\lambda$` for which `$z \\notin S$`. Then find the expected value of this fraction under the given uniform distributions.\n    3.  Compute the expected off-manifold generation rate `$R$` for each method.\n    4.  Find the index `$i \\in \\{1, 2, 3\\}$` (for SMOTE, Borderline-SMOTE, ADASYN respectively) that minimizes `$R$`.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding**: The problem is well-grounded in the principles of machine learning and data science. SMOTE and its variants are established algorithms. The issue of generating synthetic samples in low-density or incorrect regions (like the gap `$G$`) for non-convex data distributions is a known and studied problem. The model is a simplified but valid mathematical abstraction of this real-world issue.\n-   **Well-Posed**: The problem is structured as a sequence of clear, answerable questions. All necessary parameters (`$S$`, `$G$`, `$p` values, distributions) are explicitly defined, making the problem self-contained and solvable. A unique solution exists.\n-   **Objective**: The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n-   **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, realistic within a modeling context, well-posed, and non-trivial.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with a full solution.\n\n### Solution\n\n**Part 1: Conditional Density and Mixture Model**\n\nLet `$Z$` be the random variable for the synthetic sample, generated as `$Z = x + \\Lambda(y-x)$`, where `$(x, y)` are fixed minority points and `$\\Lambda \\sim \\mathrm{Uniform}(0, 1)$`.\n\nThe variable `$Z$` is a linear transformation of a uniform random variable `$\\Lambda$`. The support of `$Z$` is the closed interval between `$x$` and `$y$`, which is `$[\\min(x,y), \\max(x,y)]$`.\n\nThe cumulative distribution function (CDF) of `$Z$`, for `$z \\in [\\min(x,y), \\max(x,y)]$`, is found by:\n`$F_Z(z) = P(Z \\le z) = P(x + \\Lambda(y-x) \\le z)$`.\n\nCase 1: `$y > x$`.\n`$P(\\Lambda(y-x) \\le z-x) = P(\\Lambda \\le \\frac{z-x}{y-x}) = \\frac{z-x}{y-x}$`, since `$\\Lambda$` is uniform on `$[0, 1]$` and `$0 \\le \\frac{z-x}{y-x} \\le 1$`.\n\nCase 2: `$y < x$`.\n`$P(\\Lambda(y-x) \\le z-x) = P(\\Lambda(-(x-y)) \\le -(x-z)) = P(\\Lambda \\ge \\frac{x-z}{x-y}) = 1 - P(\\Lambda < \\frac{x-z}{x-y}) = 1 - \\frac{x-z}{x-y} = \\frac{x-y-x+z}{x-y} = \\frac{z-y}{x-y}$`.\n\nIn both cases, `$F_Z(z) = \\frac{z - \\min(x,y)}{\\max(x,y) - \\min(x,y)} = \\frac{z - \\min(x,y)}{|y-x|}`.\n\nThe conditional probability density function (PDF), `$f_{Z|x,y}(z)$`, is the derivative of the CDF with respect to `$z$`:\n`$f_{Z|x,y}(z) = \\frac{d}{dz}F_Z(z) = \\frac{1}{|y-x|}` for `$z \\in [\\min(x,y), \\max(x,y)]$` and `$0$` otherwise. This shows that the synthetic sample `$z$`, conditioned on the parent points `$(x, y)$`, is uniformly distributed on the interval connecting them.\n\nThe unconditional distribution of `$Z$` is a mixture model. Its density `$f_Z(z)$` is the expectation of the conditional density over the distribution of the parent pairs `$(X, Y)$`. This can be expressed as:\n`$f_Z(z) = \\mathbb{E}_{(X,Y)}[f_{Z|X,Y}(z)] = \\int_S \\int_S f_{Z|x,y}(z) p(x,y) dx dy$`,\nwhere `$p(x,y)$` is the joint probability density for selecting the pair `$(x,y)`. The overall distribution is thus a superposition of uniform distributions on all possible intervals `$[x,y]$`, weighted by the likelihood of choosing each pair.\n\n**Part 2: Expected Off-Manifold Fraction for Cross-Interval Pairs**\n\nWe are given a cross-interval pair with `$x \\in [0, 1]$` and `$y \\in [3, 4]`. The generated point is `$z = x + \\lambda(y-x)$` with `$\\lambda \\in [0, 1]`. The synthetic point lies on the segment `$[x, y] \\subseteq [0, 4]`. The point `$z$` lies outside the minority support `$S = [0, 1] \\cup [3, 4]` if and only if it falls in the gap `$G = (1, 3)$`.\n\nWe need to find the range of `$\\lambda$` for which `$1 < z < 3$`.\n`$1 < x + \\lambda(y-x) < 3$`\n`$1-x < \\lambda(y-x) < 3-x$`\n\nSince `$x \\in [0, 1]$` and `$y \\in [3, 4]`, the term `$y-x$` is strictly positive, with `$y-x \\in [3-1, 4-0] = [2, 4]`. We can divide by `$y-x$` without changing the inequality direction:\n`$\\frac{1-x}{y-x} < \\lambda < \\frac{3-x}{y-x}$`\n\nThe fraction of `$\\lambda \\in [0, 1]` for which this holds is the length of this interval, which we denote `$\\phi(x, y)$`:\n`$\\phi(x, y) = \\frac{3-x}{y-x} - \\frac{1-x}{y-x} = \\frac{(3-x)-(1-x)}{y-x} = \\frac{2}{y-x}`.\n\nNext, we compute the expected value of this fraction, `$E[\\phi(X, Y)]$`, where `$X \\sim \\mathrm{Uniform}([0, 1])$` and `$Y \\sim \\mathrm{Uniform}([3, 4])$. The PDFs are `$f_X(x) = 1$` for `$x \\in [0, 1]$` and `$f_Y(y) = 1$` for `$y \\in [3, 4]`.\n\n`$E[\\phi(X, Y)] = \\int_0^1 \\int_3^4 \\frac{2}{y-x} f_X(x) f_Y(y) \\, dy \\, dx = \\int_0^1 \\left( \\int_3^4 \\frac{2}{y-x} \\, dy \\right) dx`\n\nFirst, integrate with respect to `$y$`:\n`$\\int_3^4 \\frac{2}{y-x} \\, dy = 2 \\left[ \\ln|y-x| \\right]_3^4 = 2(\\ln(4-x) - \\ln(3-x)) = 2 \\ln\\left(\\frac{4-x}{3-x}\\right)$`.\n\nNow, integrate this result with respect to `$x$`:\n`$E[\\phi(X, Y)] = \\int_0^1 2 \\ln\\left(\\frac{4-x}{3-x}\\right) dx = 2 \\int_0^1 (\\ln(4-x) - \\ln(3-x)) \\, dx`\n\nWe use the integral `$\\int \\ln(a-u) \\, du = (a-u)(1 - \\ln(a-u)) + C$`.\nFor the first term:\n`$\\int_0^1 \\ln(4-x) \\, dx = \\left[ (4-x)(1-\\ln(4-x)) \\right]_0^1`\n`$= (3(1-\\ln(3))) - (4(1-\\ln(4))) = 3 - 3\\ln(3) - 4 + 4\\ln(4) = -1 - 3\\ln(3) + 4\\ln(4)$`.\n\nFor the second term:\n`$\\int_0^1 \\ln(3-x) \\, dx = \\left[ (3-x)(1-\\ln(3-x)) \\right]_0^1`\n`$= (2(1-\\ln(2))) - (3(1-\\ln(3))) = 2 - 2\\ln(2) - 3 + 3\\ln(3) = -1 - 2\\ln(2) + 3\\ln(3)$`.\n\nSubtracting the second result from the first:\n`$= (-1 - 3\\ln(3) + 4\\ln(4)) - (-1 - 2\\ln(2) + 3\\ln(3))`\n`$= -6\\ln(3) + 4\\ln(4) + 2\\ln(2)$`\nUsing `$\\ln(4) = \\ln(2^2) = 2\\ln(2)`:\n`$= -6\\ln(3) + 4(2\\ln(2)) + 2\\ln(2) = -6\\ln(3) + 8\\ln(2) + 2\\ln(2) = 10\\ln(2) - 6\\ln(3)$`.\n\nFinally, we multiply by the leading factor of `$2$` from the integral expression:\n`$E[\\phi(X, Y)] = 2(10\\ln(2) - 6\\ln(3)) = 20\\ln(2) - 12\\ln(3)$`.\nThis can also be written as `$\\ln(2^{20}) - \\ln(3^{12}) = \\ln\\left(\\frac{2^{20}}{3^{12}}\\right)$`.\n\n**Part 3: Expected Off-Manifold Generation Rate R**\n\nThe off-manifold generation rate `$R$` is the probability `$P(Z \\in G)$`. Samples can only fall in the gap `$G=(1,3)$` if they are generated from cross-interval pairs. If both `$x$` and `$y$` are in `$[0, 1]` or both are in `$[3, 4]`, their convex combination `$z$` must also lie within that same interval due to its convexity. Therefore, `$z$` cannot be in `$G$`.\n\nThe rate `$R$` is the product of two probabilities:\n1.  The probability of selecting a cross-interval pair `$(x, y)$`. This is given as `$p_{method}$` for each method.\n2.  The conditional probability of generating a sample in `$G` given that a cross-interval pair was selected. This is the expected fraction derived in Part 2, which we will call `$E_\\phi$`.\n\n`$R_{method} = p_{method} \\times E[\\phi(X, Y)] = p_{method} \\times (20\\ln(2) - 12\\ln(3))$`\n\nLet `$E_\\phi = 20\\ln(2) - 12\\ln(3)$`.\n`$R_{\\mathrm{SMOTE}} = p_{\\mathrm{SMOTE}} \\times E_\\phi = \\frac{1}{20}(20\\ln(2) - 12\\ln(3)) = \\ln(2) - \\frac{3}{5}\\ln(3)$`.\n`$R_{\\mathrm{B}} = p_{\\mathrm{B}} \\times E_\\phi = \\frac{3}{20}(20\\ln(2) - 12\\ln(3)) = 3\\ln(2) - \\frac{9}{5}\\ln(3)$`.\n`$R_{\\mathrm{ADASYN}} = p_{\\mathrm{ADASYN}} \\times E_\\phi = \\frac{1}{10}(20\\ln(2) - 12\\ln(3)) = 2\\ln(2) - \\frac{6}{5}\\ln(3)$`.\n\n**Part 4: Minimizing R and Determining the Index**\n\nTo determine which method minimizes the rate `$R$`, we need to compare `$R_{\\mathrm{SMOTE}}$, `$R_{\\mathrm{B}}$`, and `$R_{\\mathrm{ADASYN}}`$.\nThe quantity `$E_\\phi = 20\\ln(2) - 12\\ln(3) = \\ln(2^{20}/3^{12}) = \\ln(1048576 / 531441) \\approx \\ln(1.97) > 0$`. Since `$E_\\phi$` is a positive constant, minimizing `$R_{method} = p_{method} \\times E_\\phi$` is equivalent to minimizing the cross-interval probability `$p_{method}$`.\n\nThe given probabilities are:\n-   `$p_{\\mathrm{SMOTE}} = \\frac{1}{20}$`\n-   `$p_{\\mathrm{B}} = \\frac{3}{20}$`\n-   `$p_{\\mathrm{ADASYN}} = \\frac{1}{10} = \\frac{2}{20}$`\n\nComparing these values:\n`$\\frac{1}{20} < \\frac{2}{20} < \\frac{3}{20}$`\nThis implies `$p_{\\mathrm{SMOTE}} < p_{\\mathrm{ADASYN}} < p_{\\mathrm{B}}$`.\n\nTherefore, the SMOTE method results in the lowest expected off-manifold generation rate `$R$`. The problem asks for the index `$i \\in \\{1, 2, 3\\}$` corresponding to the method in the order (SMOTE, Borderline-SMOTE, ADASYN). SMOTE is the first method listed.\n\nThe index corresponding to SMOTE is `$1$`.", "answer": "$$\\boxed{1}$$", "id": "4543115"}]}