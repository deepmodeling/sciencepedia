{"hands_on_practices": [{"introduction": "To truly master Gradient Boosting Models, we must first look under the hood at the core mathematical engine that drives the learning process. This practice invites you to do just that by deriving the fundamental components of a single boosting iteration for a binary classification task. By calculating the first and second derivatives (gradient and Hessian) of the logistic loss function, you will see precisely how the model quantifies its error and determines the direction for correction. This exercise [@problem_id:4542167] is foundational, revealing the mechanics behind how a GBM sequentially builds trees to improve its predictions.", "problem": "A radiomics team is training a Gradient Boosting of Decision Trees (GBDT) classifier to predict lung nodule malignancy from Computed Tomography (CT) radiomic features. The binary label is $y_i \\in \\{0,1\\}$, where $y_i = 1$ denotes malignant and $y_i = 0$ denotes benign. The model uses the Bernoulli negative log-likelihood with a logistic link. For each sample $i$, the model maintains a score $f_i$ and a probability $p_i = \\frac{1}{1 + \\exp(-f_i)}$. The per-sample loss is $\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$. At iteration $t = 0$, the model initializes $f_i^{(0)} = 0$ for all $i$.\n\nTasks:\n- Starting only from the definitions above and basic calculus, derive the first derivative $g_i = \\frac{\\partial \\ell_i}{\\partial f_i}$ and the second derivative $h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2}$ as functions of $p_i$ and $y_i$, and then evaluate $g_i$ and $h_i$ at $f_i^{(0)} = 0$ for $y_i \\in \\{0,1\\}$.\n- In the first boosting iteration, suppose the tree has $1$ leaf (no split), learning rate is $1$, and there is no regularization. For first-order (gradient) boosting, the constant leaf value $w_{\\mathrm{FO}}$ is chosen to minimize $\\sum_{i=1}^{n} (-g_i - w)^2$. For second-order (Newton) boosting, the constant leaf value $w_{\\mathrm{SO}}$ is chosen to minimize the second-order Taylor approximation $\\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$. Derive closed-form expressions for $w_{\\mathrm{FO}}$ and $w_{\\mathrm{SO}}$ in terms of $n$ and $k = \\sum_{i=1}^{n} y_i$ under the initialization $f_i^{(0)} = 0$.\n\nIn a specific CT radiomics cohort with $n = 200$ patients and $k = 60$ malignant nodules, compute the ratio $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$ for the very first iteration. Report the final answer as an exact real number with no units.", "solution": "The problem is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n-   Binary label: $y_i \\in \\{0,1\\}$, where $y_i = 1$ is malignant.\n-   Model score: $f_i$.\n-   Predicted probability: $p_i = \\frac{1}{1 + \\exp(-f_i)}$.\n-   Per-sample loss function (Bernoulli negative log-likelihood): $\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$.\n-   Initial model score at iteration $t=0$: $f_i^{(0)} = 0$ for all samples $i$.\n-   First derivative (gradient): $g_i = \\frac{\\partial \\ell_i}{\\partial f_i}$.\n-   Second derivative (Hessian): $h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2}$.\n-   First-order boosting leaf value $w_{\\mathrm{FO}}$ minimizes the objective function $\\sum_{i=1}^{n} (-g_i - w)^2$.\n-   Second-order boosting leaf value $w_{\\mathrm{SO}}$ minimizes the objective function $\\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$.\n-   The tree in the first iteration has $1$ leaf.\n-   The learning rate is $1$. There is no regularization.\n-   Total number of patients: $n = 200$.\n-   Number of malignant nodules (positive labels): $k = \\sum_{i=1}^{n} y_i = 60$.\n-   The goal is to compute the ratio $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The components described are standard in the theory of gradient boosting models. The loss function is the binary cross-entropy, the link function is the logistic (sigmoid) function, and the definitions of the gradient and Hessian are correct. The objective functions for finding the leaf weights correspond to the standard optimization problems for first-order (fitting to residuals) and second-order (Newton's method) updates in GBDTs. The problem is well-posed, with all necessary information provided to derive the requested quantities and compute the final ratio. The language is objective and mathematically precise. There are no contradictions, ambiguities, or unrealistic assumptions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation of the Gradient and Hessian\nThe per-sample loss is given by $\\ell_i(f_i) = - y_i \\ln p_i - (1 - y_i) \\ln(1 - p_i)$, with $p_i = \\frac{1}{1 + \\exp(-f_i)}$.\nTo find the derivatives of $\\ell_i$ with respect to $f_i$, we first find the derivative of $p_i$ with respect to $f_i$:\n$$ \\frac{\\partial p_i}{\\partial f_i} = \\frac{\\partial}{\\partial f_i} (1 + \\exp(-f_i))^{-1} = -1 (1 + \\exp(-f_i))^{-2} \\cdot (\\exp(-f_i) \\cdot -1) = \\frac{\\exp(-f_i)}{(1 + \\exp(-f_i))^2} $$\nThis can be expressed in terms of $p_i$:\n$$ \\frac{\\partial p_i}{\\partial f_i} = \\frac{1}{1 + \\exp(-f_i)} \\cdot \\frac{\\exp(-f_i)}{1 + \\exp(-f_i)} = p_i (1 - p_i) $$\nThis is a well-known property of the logistic function.\n\nNow, we use the chain rule to find the first derivative, $g_i$:\n$$ g_i = \\frac{\\partial \\ell_i}{\\partial f_i} = \\frac{\\partial \\ell_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial f_i} $$\nThe derivative of the loss with respect to $p_i$ is:\n$$ \\frac{\\partial \\ell_i}{\\partial p_i} = -\\frac{y_i}{p_i} - (1 - y_i) \\frac{1}{1 - p_i} (-1) = -\\frac{y_i}{p_i} + \\frac{1 - y_i}{1 - p_i} = \\frac{-y_i(1 - p_i) + p_i(1 - y_i)}{p_i(1 - p_i)} = \\frac{-y_i + y_ip_i + p_i - y_ip_i}{p_i(1 - p_i)} = \\frac{p_i - y_i}{p_i(1 - p_i)} $$\nCombining these results:\n$$ g_i = \\left( \\frac{p_i - y_i}{p_i(1 - p_i)} \\right) \\cdot (p_i(1 - p_i)) = p_i - y_i $$\nTo find the second derivative, $h_i$, we differentiate $g_i$ with respect to $f_i$:\n$$ h_i = \\frac{\\partial^2 \\ell_i}{\\partial f_i^2} = \\frac{\\partial g_i}{\\partial f_i} = \\frac{\\partial}{\\partial f_i} (p_i - y_i) = \\frac{\\partial p_i}{\\partial f_i} $$\nUsing our earlier result for $\\frac{\\partial p_i}{\\partial f_i}$:\n$$ h_i = p_i(1 - p_i) $$\n\n### Evaluation at Initialization\nAt the initial iteration $t=0$, the model scores are initialized to $f_i^{(0)} = 0$ for all $i$.\nThe initial probability for each sample is:\n$$ p_i^{(0)} = \\frac{1}{1 + \\exp(-0)} = \\frac{1}{1 + 1} = \\frac{1}{2} $$\nWe evaluate the gradient $g_i$ and Hessian $h_i$ at this initial point:\n$$ g_i^{(0)} = p_i^{(0)} - y_i = \\frac{1}{2} - y_i $$\n$$ h_i^{(0)} = p_i^{(0)}(1 - p_i^{(0)}) = \\frac{1}{2} \\left(1 - \\frac{1}{2}\\right) = \\frac{1}{4} $$\nThese initial values will be used to find the leaf weights for the first tree.\n\n### Derivation of Leaf Weights\nFor the first boosting iteration, we compute a single leaf weight $w$ for the entire dataset.\n\n**First-Order Leaf Weight ($w_{\\mathrm{FO}}$)**\nThe objective is to find $w_{\\mathrm{FO}}$ that minimizes $L_{\\mathrm{FO}}(w) = \\sum_{i=1}^{n} (-g_i - w)^2$. To find the minimum, we set the derivative with respect to $w$ to zero:\n$$ \\frac{d L_{\\mathrm{FO}}}{d w} = \\sum_{i=1}^{n} 2(-g_i - w)(-1) = 2 \\sum_{i=1}^{n} (g_i + w) = 0 $$\n$$ \\sum_{i=1}^{n} g_i + \\sum_{i=1}^{n} w = 0 \\implies \\left(\\sum_{i=1}^{n} g_i\\right) + n w = 0 $$\n$$ w_{\\mathrm{FO}} = -\\frac{\\sum_{i=1}^{n} g_i}{n} $$\nUsing the initial gradients $g_i^{(0)} = \\frac{1}{2} - y_i$ and the definition $k = \\sum_{i=1}^{n} y_i$:\n$$ \\sum_{i=1}^{n} g_i^{(0)} = \\sum_{i=1}^{n} \\left(\\frac{1}{2} - y_i\\right) = \\sum_{i=1}^{n} \\frac{1}{2} - \\sum_{i=1}^{n} y_i = \\frac{n}{2} - k $$\nSubstituting this into the expression for $w_{\\mathrm{FO}}$:\n$$ w_{\\mathrm{FO}} = -\\frac{\\frac{n}{2} - k}{n} = -\\left(\\frac{1}{2} - \\frac{k}{n}\\right) = \\frac{k}{n} - \\frac{1}{2} $$\n\n**Second-Order Leaf Weight ($w_{\\mathrm{SO}}$)**\nThe objective is to find $w_{\\mathrm{SO}}$ that minimizes the second-order Taylor approximation of the loss, $L_{\\mathrm{SO}}(w) = \\sum_{i=1}^{n} \\left(g_i w + \\frac{1}{2} h_i w^2\\right)$. Setting the derivative with respect to $w$ to zero:\n$$ \\frac{d L_{\\mathrm{SO}}}{d w} = \\sum_{i=1}^{n} (g_i + h_i w) = 0 $$\n$$ \\sum_{i=1}^{n} g_i + w \\sum_{i=1}^{n} h_i = 0 $$\n$$ w_{\\mathrm{SO}} = -\\frac{\\sum_{i=1}^{n} g_i}{\\sum_{i=1}^{n} h_i} $$\nUsing the initial gradients and Hessians, $g_i^{(0)} = \\frac{1}{2} - y_i$ and $h_i^{(0)} = \\frac{1}{4}$:\nThe numerator is the same as before: $\\sum_{i=1}^{n} g_i^{(0)} = \\frac{n}{2} - k$.\nThe denominator is:\n$$ \\sum_{i=1}^{n} h_i^{(0)} = \\sum_{i=1}^{n} \\frac{1}{4} = \\frac{n}{4} $$\nSubstituting these sums into the expression for $w_{\\mathrm{SO}}$:\n$$ w_{\\mathrm{SO}} = -\\frac{\\frac{n}{2} - k}{\\frac{n}{4}} = -4 \\frac{\\frac{n}{2} - k}{n} = -4\\left(\\frac{1}{2} - \\frac{k}{n}\\right) = 4\\left(\\frac{k}{n} - \\frac{1}{2}\\right) $$\n\n### Calculation of the Ratio\nThe problem asks for the ratio $R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}}$. Using the derived expressions:\n$$ R = \\frac{w_{\\mathrm{SO}}}{w_{\\mathrm{FO}}} = \\frac{4\\left(\\frac{k}{n} - \\frac{1}{2}\\right)}{\\frac{k}{n} - \\frac{1}{2}} $$\nThis ratio is valid as long as the denominator is non-zero. The denominator is zero if $\\frac{k}{n} = \\frac{1}{2}$.\nFor the given cohort, $n=200$ and $k=60$.\n$$ \\frac{k}{n} = \\frac{60}{200} = \\frac{3}{10} = 0.3 $$\nSince $0.3 \\neq 0.5$, the denominator is non-zero, and we can cancel the term $\\left(\\frac{k}{n} - \\frac{1}{2}\\right)$:\n$$ R = 4 $$\nThe ratio is a constant value of $4$, independent of the specific values of $n$ and $k$ (as long as $2k \\neq n$).\nThe final answer is an exact real number.", "answer": "$$\n\\boxed{4}\n$$", "id": "4542167"}, {"introduction": "Building on the core mechanics, we now turn to a common and critical challenge in real-world radiomics datasets: missing feature values. Instead of relying on separate, often suboptimal, imputation steps, modern Gradient Boosting Machines can handle missing data natively within the tree-building process. This practice [@problem_id:4542131] demonstrates this elegant, built-in mechanism. You will use the same second-order optimization principles from the previous exercise to see how the algorithm intelligently learns a \"default direction\" for missing values at each split, a powerful feature that enhances model robustness and performance.", "problem": "A hospital builds a Gradient Boosting Machine (GBM) to predict a binary radiomics endpoint from a computed tomography radiomic feature $x$ that often has missing values due to region-of-interest variability. Training proceeds by additive trees fitted to the negative gradients of the loss. At a particular internal node, a candidate split on $x$ at threshold $\\tau$ is evaluated. Let the first and second derivatives of the loss with respect to the current model output for each instance at this node be $g_i$ and $h_i$, respectively. Define $G=\\sum_i g_i$ and $H=\\sum_i h_i$ for any group of instances. The regularization includes an $\\ell_2$ leaf penalty with coefficient $\\lambda$ and a per-split complexity penalty $\\gamma$. The GBM uses the standard second-order Taylor expansion of the loss to determine optimal leaf values and split gains.\n\nFor the candidate threshold $\\tau$, the non-missing instances ($x$ observed) partition into a “known-left” group with aggregated derivatives $G_{\\mathrm{KL}}=-12$, $H_{\\mathrm{KL}}=30$ and a “known-right” group with $G_{\\mathrm{KR}}=5$, $H_{\\mathrm{KR}}=20$. The instances with missing $x$ have aggregated derivatives $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$. Hyperparameters are $\\lambda=1$ and $\\gamma=0.1$. The GBM handles missing values by learning, at each split, a default direction that all missing values will follow at both training and inference time.\n\nWhich option correctly describes how the tree-based GBM handles the missing values at this split and correctly computes the split gain under the two possible default directions, thereby identifying the learned default direction?\n\nA. The GBM tries both default directions by assigning all missing instances to the left child or to the right child, computes the split gain in each case using the second-order approximation with regularization, and chooses the direction with larger gain. With missing sent left, the aggregated derivatives are $G_{\\mathrm{L}}=G_{\\mathrm{KL}}+G_{\\mathrm{M}}=-15$, $H_{\\mathrm{L}}=H_{\\mathrm{KL}}+H_{\\mathrm{M}}=36$ and $G_{\\mathrm{R}}=G_{\\mathrm{KR}}=5$, $H_{\\mathrm{R}}=H_{\\mathrm{KR}}=20$, yielding a gain of approximately $2.66$. With missing sent right, $G_{\\mathrm{L}}=-12$, $H_{\\mathrm{L}}=30$ and $G_{\\mathrm{R}}=2$, $H_{\\mathrm{R}}=26$, yielding a gain of approximately $1.42$. The learned default direction is left.\n\nB. The GBM always sends missing instances to the child with the larger number of non-missing instances. The split gain is computed by ignoring missing instances entirely. The gain here is approximately $1.00$, and the default direction is right because $H_{\\mathrm{KR}}=20$ is larger than $H_{\\mathrm{KL}}=30$.\n\nC. The GBM creates a third “missing” child for the split and computes the gain as the sum of three child scores minus the parent score. Using $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$ as a separate child increases the gain beyond either two-way assignment, so the learned default is “missing branch.”\n\nD. The GBM first imputes missing $x$ to the mean of observed $x$ at this node and then proceeds as if there were no missing values; the gain is computed on the imputed full data without trying alternative default directions. The computed gain is approximately $2.00$, and there is no learned default direction at inference time.", "solution": "The user wants me to solve a problem regarding Gradient Boosting Machines (GBMs), specifically how they handle missing values during the construction of a decision tree.\n\n### Step 1: Extract Givens\n- **Model**: Gradient Boosting Machine (GBM)\n- **Task**: Binary classification of a radiomics endpoint.\n- **Feature**: A single radiomic feature $x$ with missing values.\n- **Algorithm**: Additive trees are fitted to the negative gradients of the loss function. The solution uses a second-order Taylor expansion of the loss.\n- **Derivatives notation**: For an instance $i$ at the current node, $g_i$ is the first derivative and $h_i$ is the second derivative of the loss with respect to the model output. $G = \\sum g_i$ and $H = \\sum h_i$ are the aggregated derivatives for a group of instances.\n- **Regularization**: An $\\ell_2$ leaf penalty with coefficient $\\lambda = 1$ and a per-split complexity penalty $\\gamma = 0.1$.\n- **Data for non-missing instances at the split threshold $\\tau$**:\n    - Known-left group (instances with $x \\le \\tau$): $G_{\\mathrm{KL}} = -12$, $H_{\\mathrm{KL}} = 30$.\n    - Known-right group (instances with $x > \\tau$): $G_{\\mathrm{KR}} = 5$, $H_{\\mathrm{KR}} = 20$.\n- **Data for missing instances**:\n    - Missing group: $G_{\\mathrm{M}} = -3$, $H_{\\mathrm{M}} = 6$.\n- **Missing value handling method**: \"The GBM handles missing values by learning, at each split, a default direction that all missing values will follow at both training and inference time.\"\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes the sparsity-aware split-finding algorithm, which is a key component of modern GBM implementations like XGBoost. The use of first and second derivatives ($g_i, h_i$), $\\ell_2$ regularization ($\\lambda$), and a split complexity penalty ($\\gamma$) are standard in this context. The values provided are numerically consistent; for instance, the second derivative sums ($H$) are all positive, as they must be for common loss functions (e.g., log-loss, squared-error loss). The description of learning a \"default direction\" is a precise, technical characterization of the algorithm. The problem is scientifically grounded, well-posed, and objective. It contains all the necessary information to perform the required calculations and does not suffer from any of the invalidity flaws.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed to derive the solution.\n\n### Derivation\nThe core of this problem lies in calculating the gain of a split in a GBM tree. The gain is derived from the objective function, which is approximated using a second-order Taylor expansion. The optimal weight $w^*$ for a leaf and its corresponding objective function value (score) are given by:\n$$ w^* = -\\frac{G}{H + \\lambda} $$\n$$ \\text{score} = -\\frac{1}{2} \\frac{G^2}{H + \\lambda} $$\nwhere $G$ and $H$ are the sum of the first and second derivatives, respectively, for the instances in that leaf, and $\\lambda$ is the $\\ell_2$ regularization parameter.\n\nThe gain of a split is the reduction in the objective function, which is the sum of the scores of the children nodes minus the score of the parent node, minus a complexity penalty $\\gamma$ for introducing the split.\n$$ \\text{Gain} = \\text{score}_{\\text{Left}} + \\text{score}_{\\text{Right}} - \\text{score}_{\\text{Parent}} - \\gamma $$\nSubstituting the formula for the score, we get:\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_{\\text{Parent}}^2}{H_{\\text{Parent}} + \\lambda} \\right] - \\gamma $$\nwhere the subscripts $L$, $R$, and `Parent` refer to the left child, right child, and parent node, respectively.\n\nThe problem states that the GBM learns a default direction. This is done by evaluating two scenarios:\n1.  All instances with missing values are assigned to the left child.\n2.  All instances with missing values are assigned to the right child.\n\nThe algorithm computes the split gain for both scenarios and chooses the direction that results in a higher gain.\n\nFirst, let's find the aggregated derivatives for the parent node, which contains all instances (known-left, known-right, and missing):\n$G_{\\text{Parent}} = G_{\\mathrm{KL}} + G_{\\mathrm{KR}} + G_{\\mathrm{M}} = -12 + 5 + (-3) = -10$\n$H_{\\text{Parent}} = H_{\\mathrm{KL}} + H_{\\mathrm{KR}} + H_{\\mathrm{M}} = 30 + 20 + 6 = 56$\n\nThe parent score term in the gain formula is:\n$$ \\frac{G_{\\text{Parent}}^2}{H_{\\text{Parent}} + \\lambda} = \\frac{(-10)^2}{56 + 1} = \\frac{100}{57} $$\n\n**Scenario 1: Missing values sent to the LEFT child**\n\nThe left child will contain the known-left instances and the missing-value instances. The right child will contain only the known-right instances.\n- **Left Child:**\n  $G_L = G_{\\mathrm{KL}} + G_{\\mathrm{M}} = -12 + (-3) = -15$\n  $H_L = H_{\\mathrm{KL}} + H_{\\mathrm{M}} = 30 + 6 = 36$\n- **Right Child:**\n  $G_R = G_{\\mathrm{KR}} = 5$\n  $H_R = H_{\\mathrm{KR}} = 20$\n\nThe gain for this scenario ($\\text{Gain}_{\\text{left}})$ is:\n$$ \\text{Gain}_{\\text{left}} = \\frac{1}{2} \\left[ \\frac{(-15)^2}{36 + 1} + \\frac{5^2}{20 + 1} - \\frac{100}{57} \\right] - 0.1 $$\n$$ \\text{Gain}_{\\text{left}} = \\frac{1}{2} \\left[ \\frac{225}{37} + \\frac{25}{21} - \\frac{100}{57} \\right] - 0.1 $$\nNumerically:\n$$ \\frac{225}{37} \\approx 6.08108 $$\n$$ \\frac{25}{21} \\approx 1.19048 $$\n$$ \\frac{100}{57} \\approx 1.75439 $$\n$$ \\text{Gain}_{\\text{left}} \\approx \\frac{1}{2} [6.08108 + 1.19048 - 1.75439] - 0.1 = \\frac{1}{2} [5.51717] - 0.1 \\approx 2.7586 - 0.1 = 2.6586 $$\nSo, the gain is approximately $2.66$.\n\n**Scenario 2: Missing values sent to the RIGHT child**\n\nThe left child will contain only the known-left instances. The right child will contain the known-right instances and the missing-value instances.\n- **Left Child:**\n  $G_L = G_{\\mathrm{KL}} = -12$\n  $H_L = H_{\\mathrm{KL}} = 30$\n- **Right Child:**\n  $G_R = G_{\\mathrm{KR}} + G_{\\mathrm{M}} = 5 + (-3) = 2$\n  $H_R = H_{\\mathrm{KR}} + H_{\\mathrm{M}} = 20 + 6 = 26$\n\nThe gain for this scenario ($\\text{Gain}_{\\text{right}})$ is:\n$$ \\text{Gain}_{\\text{right}} = \\frac{1}{2} \\left[ \\frac{(-12)^2}{30 + 1} + \\frac{2^2}{26 + 1} - \\frac{100}{57} \\right] - 0.1 $$\n$$ \\text{Gain}_{\\text{right}} = \\frac{1}{2} \\left[ \\frac{144}{31} + \\frac{4}{27} - \\frac{100}{57} \\right] - 0.1 $$\nNumerically:\n$$ \\frac{144}{31} \\approx 4.64516 $$\n$$ \\frac{4}{27} \\approx 0.14815 $$\n$$ \\text{Gain}_{\\text{right}} \\approx \\frac{1}{2} [4.64516 + 0.14815 - 1.75439] - 0.1 = \\frac{1}{2} [3.03892] - 0.1 \\approx 1.5195 - 0.1 = 1.4195 $$\nSo, the gain is approximately $1.42$.\n\n**Conclusion**\n\nComparing the two gains:\n$\\text{Gain}_{\\text{left}} \\approx 2.66$\n$\\text{Gain}_{\\text{right}} \\approx 1.42$\n\nSince $\\text{Gain}_{\\text{left}} > \\text{Gain}_{\\text{right}}$, the GBM will choose to send the missing values to the left child. The learned default direction is 'left', and the gain for this split is approximately $2.66$.\n\n### Option-by-Option Analysis\n\n**A. The GBM tries both default directions by assigning all missing instances to the left child or to the right child, computes the split gain in each case using the second-order approximation with regularization, and chooses the direction with larger gain. With missing sent left, the aggregated derivatives are $G_{\\mathrm{L}}=G_{\\mathrm{KL}}+G_{\\mathrm{M}}=-15$, $H_{\\mathrm{L}}=H_{\\mathrm{KL}}+H_{\\mathrm{M}}=36$ and $G_{\\mathrm{R}}=G_{\\mathrm{KR}}=5$, $H_{\\mathrm{R}}=H_{\\mathrm{KR}}=20$, yielding a gain of approximately $2.66$. With missing sent right, $G_{\\mathrm{L}}=-12$, $H_{\\mathrm{L}}=30$ and $G_{\\mathrm{R}}=2$, $H_{\\mathrm{R}}=26$, yielding a gain of approximately $1.42$. The learned default direction is left.**\n- This option perfectly describes the sparsity-aware split finding algorithm.\n- The aggregated derivatives for the \"missing left\" scenario ($G_L=-15, H_L=36, G_R=5, H_R=20$) are correct. The calculated gain of $\\approx 2.66$ matches our derivation.\n- The aggregated derivatives for the \"missing right\" scenario ($G_L=-12, H_L=30, G_R=2, H_R=26$) are correct. The calculated gain of $\\approx 1.42$ matches our derivation.\n- The conclusion that the learned default direction is left, based on the higher gain, is also correct.\n- Verdict: **Correct**.\n\n**B. The GBM always sends missing instances to the child with the larger number of non-missing instances. The split gain is computed by ignoring missing instances entirely. The gain here is approximately $1.00$, and the default direction is right because $H_{\\mathrm{KR}}=20$ is larger than $H_{\\mathrm{KL}}=30$.**\n- The description of the algorithm is incorrect. The decision is based on gain maximization, not instance counts. The problem gives no information about instance counts, only sums of derivatives.\n- The claim that the split gain is computed by ignoring missing instances is incorrect. As shown in the derivation, they are integral to the calculation.\n- The claim that the gain is $\\approx 1.00$ is incorrect. A calculation ignoring missing instances would yield a gain of $\\approx 2.34$.\n- The statement \"the default direction is right because $H_{\\mathrm{KR}}=20$ is larger than $H_{\\mathrm{KL}}=30$\" is mathematically false ($20  30$) and algorithmically incorrect.\n- Verdict: **Incorrect**.\n\n**C. The GBM creates a third “missing” child for the split and computes the gain as the sum of three child scores minus the parent score. Using $G_{\\mathrm{M}}=-3$, $H_{\\mathrm{M}}=6$ as a separate child increases the gain beyond either two-way assignment, so the learned default is “missing branch.”**\n- This describes a different method for handling missing values (e.g., as used in CART or C4.5), not the one standard in popular GBMs and not the one implied by the phrase \"learns... a default direction\". This would create a three-way split, not a binary split with a default path. The problem framing and gain formula are for a binary split.\n- Verdict: **Incorrect**.\n\n**D. The GBM first imputes missing $x$ to the mean of observed $x$ at this node and then proceeds as if there were no missing values; the gain is computed on the imputed full data without trying alternative default directions. The computed gain is approximately $2.00$, and there is no learned default direction at inference time.**\n- This describes mean imputation, which is another possible strategy, but it is not the algorithm described in the problem statement (\"learns... a default direction\"). The described algorithm is an integrated part of split-finding, not a pre-processing step.\n- Without knowing the mean of $x$ and the split threshold $\\tau$, it's impossible to know where the imputed instances would go, so the gain cannot be calculated to verify the claim of $\\approx 2.00$.\n- Verdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4542131"}, {"introduction": "After a model is trained, its predictions are of little use in high-stakes fields like medicine unless we can understand *why* it made them. This final practice tackles the crucial topic of model interpretability. Using a simplified tree from a GBM, you will calculate the Shapley Additive exPlanations (SHAP) values for a single prediction [@problem_id:4542187]. This exercise demystifies how SHAP values work, showing how they fairly distribute the contribution of each radiomic feature to the final risk score, thus providing a transparent and quantitative explanation for the model's decision-making process.", "problem": "A radiomics regression model based on Gradient Boosting (GB) is trained to predict a dimensionless malignancy risk score from two standardized radiomic features extracted from Computed Tomography (CT) imaging: gray-level co-occurrence matrix contrast $x_{1}$ and wavelet high-frequency energy $x_{2}$. Consider a single decision tree (learning rate $= 1$ and base score $= 0$) in the ensemble whose structure is as follows. The root node splits on $x_{1}$ at threshold $1.5$: if $x_{1} \\leq 1.5$ the left child further splits on $x_{2}$ at threshold $0.5$, producing leaf values $0.8$ when $x_{2} \\leq 0.5$ and $1.6$ when $x_{2}  0.5$; if $x_{1}  1.5$ the right child is a leaf with value $2.0$. Assume the training distribution used for marginalization in TreeSHAP treats $x_{1}$ and $x_{2}$ as independent, each with probability $0.5$ of being below its respective threshold and $0.5$ of being above. For the specific instance $x = (x_{1}, x_{2}) = (1.8, 0.4)$, the model prediction from this tree is $f(x)$.\n\nUsing the definition of Shapley Additive exPlanations (SHAP) values $\\phi_{1}$ and $\\phi_{2}$ for the two features under the independence assumption described above, compute $\\phi_{1}$ and $\\phi_{2}$ exactly and verify the local accuracy property $\\sum_{j=1}^{2} \\phi_{j} = f(x) - \\mathbb{E}[f(X)]$ for this instance. Provide exact values. Report the pair $(\\phi_{1}, \\phi_{2})$ as a row matrix.", "solution": "### Step 1: Problem Validation\n\n**1. Extract Givens:**\n- **Model:** A single decision tree from a Gradient Boosting (GB) ensemble.\n- **Features:** $x_{1}$ (gray-level co-occurrence matrix contrast), $x_{2}$ (wavelet high-frequency energy).\n- **Model Parameters:** Learning rate $\\eta = 1$, base score $F_{0} = 0$. The model prediction $f(x)$ is the output of this single tree.\n- **Tree Structure:**\n    - Root node: Splits on $x_{1}$ at threshold $1.5$.\n    - Left child ($x_{1} \\leq 1.5$): Splits on $x_{2}$ at threshold $0.5$.\n        - Leaf 1 ($x_{1} \\leq 1.5, x_{2} \\leq 0.5$): Value $0.8$.\n        - Leaf 2 ($x_{1} \\leq 1.5, x_{2}  0.5$): Value $1.6$.\n    - Right child ($x_{1}  1.5$): Is a leaf (Leaf 3) with value $2.0$.\n- **Background Distribution for TreeSHAP:**\n    - Features $x_{1}$ and $x_{2}$ are treated as independent.\n    - $P(x_{1} \\leq 1.5) = 0.5$, $P(x_{1}  1.5) = 0.5$.\n    - $P(x_{2} \\leq 0.5) = 0.5$, $P(x_{2}  0.5) = 0.5$.\n- **Instance to Explain:** $x = (x_{1}, x_{2}) = (1.8, 0.4)$.\n- **Task:** Compute the SHAP values $\\phi_{1}$ and $\\phi_{2}$ for the features, and verify the local accuracy property: $\\sum_{j=1}^{2} \\phi_{j} = f(x) - \\mathbb{E}[f(X)]$.\n\n**2. Validate Using Extracted Givens:**\n- **Scientifically Grounded:** The problem uses established concepts: Gradient Boosting, decision trees, and SHAP (Shapley Additive exPlanations) values, specifically the TreeSHAP variant with feature independence assumption. The context of radiomics is appropriate. All concepts are standard in machine learning and data science.\n- **Well-Posed:** The problem statement provides a completely specified function (the decision tree), a specific input point, and a well-defined background distribution. This is sufficient to uniquely determine the SHAP values and the expected model output.\n- **Objective:** The problem is stated in precise, mathematical language, free from subjectivity.\n\n**3. Verdict and Action:**\nThe problem is valid. It is a well-defined computational problem in the domain of machine learning explainability. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe prediction function $f(x_{1}, x_{2})$ defined by the decision tree is:\n$$\nf(x_{1}, x_{2}) = \\begin{cases}\n0.8  \\text{if } x_{1} \\leq 1.5 \\text{ and } x_{2} \\leq 0.5 \\\\\n1.6  \\text{if } x_{1} \\leq 1.5 \\text{ and } x_{2}  0.5 \\\\\n2.0  \\text{if } x_{1}  1.5\n\\end{cases}\n$$\n\nFirst, we calculate the model's prediction $f(x)$ for the specific instance $x = (x_{1}, x_{2}) = (1.8, 0.4)$.\nSince $x_{1} = 1.8  1.5$, the instance falls into the right child of the root node, which is a leaf.\n$$\nf(1.8, 0.4) = 2.0\n$$\n\nNext, we calculate the expected prediction over the background distribution, $\\mathbb{E}[f(X)]$. The features $X_{1}$ and $X_{2}$ are independent, and each threshold is crossed with probability $0.5$. The joint probability of falling into any of the regions defined by the thresholds is $0.5 \\times 0.5 = 0.25$.\nThe regions and their associated outputs are:\n1. $X_{1} \\leq 1.5, X_{2} \\leq 0.5$: Output is $0.8$. Probability is $0.25$.\n2. $X_{1} \\leq 1.5, X_{2}  0.5$: Output is $1.6$. Probability is $0.25$.\n3. $X_{1}  1.5$ (covers both $X_{2} \\leq 0.5$ and $X_{2}  0.5$): Output is $2.0$. The probability is $P(X_1  1.5) = 0.5$.\n\nSo, the expectation is:\n$$\n\\mathbb{E}[f(X)] = P(X_{1} \\leq 1.5, X_{2} \\leq 0.5) \\cdot 0.8 + P(X_{1} \\leq 1.5, X_{2}  0.5) \\cdot 1.6 + P(X_{1}  1.5) \\cdot 2.0\n$$\n$$\n\\mathbb{E}[f(X)] = (0.5 \\cdot 0.5) \\cdot 0.8 + (0.5 \\cdot 0.5) \\cdot 1.6 + (0.5) \\cdot 2.0\n$$\n$$\n\\mathbb{E}[f(X)] = 0.25 \\cdot 0.8 + 0.25 \\cdot 1.6 + 0.5 \\cdot 2.0 = 0.2 + 0.4 + 1.0 = 1.6\n$$\nThe expected prediction (which corresponds to the base value $\\phi_0$ in SHAP) is $\\mathbb{E}[f(X)]=1.6$.\n\nNow, we compute the SHAP values $\\phi_1$ and $\\phi_2$ for the instance $x=(1.8, 0.4)$. The Shapley value for a feature $j$ among a set of features $F$ is defined as:\n$$\n\\phi_{j}(f, x) = \\sum_{S \\subseteq F\\setminus\\{j\\}} \\frac{|S|! (|F| - |S| - 1)!}{|F|!} [ \\mathbb{E}[f(X) | X_S=x_S, X_j=x_j] - \\mathbb{E}[f(X) | X_S=x_S] ]\n$$\nHere, $|F|=2$, so the formula simplifies. For a feature $j$, we consider the feature sets $S=\\emptyset$ and $S=F\\setminus\\{j\\}$. The weight is $\\frac{0!(2-1)!}{2!} = \\frac{1}{2}$ for both cases.\n\n**Calculation of $\\phi_1$ (for feature $x_1$):**\nThe two orderings are $(\\{x_1\\}, \\{x_2\\})$ and $(\\{x_2\\}, \\{x_1\\})$.\n1.  Marginal contribution of $x_1$ added first (coalition $S=\\emptyset$):\n    The contribution is $\\mathbb{E}[f(X)|X_1=1.8] - \\mathbb{E}[f(X)]$.\n    $\\mathbb{E}[f(X)|X_1=1.8]$ is the expected output when $x_1$ is fixed to $1.8$ and $x_2$ is marginalized. Since $x_1=1.8  1.5$, the tree always outputs $2.0$ regardless of the value of $x_2$.\n    $$\n    \\mathbb{E}[f(X)|X_1=1.8] = 2.0\n    $$\n    The contribution is $2.0 - \\mathbb{E}[f(X)] = 2.0 - 1.6 = 0.4$.\n\n2.  Marginal contribution of $x_1$ added second (coalition $S=\\{x_2\\}$):\n    The contribution is $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] - \\mathbb{E}[f(X)|X_2=0.4]$.\n    $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] = f(1.8, 0.4) = 2.0$.\n    $\\mathbb{E}[f(X)|X_2=0.4]$ is the expected output when $x_2$ is fixed to $0.4$ and $x_1$ is marginalized. Since $x_2=0.4 \\leq 0.5$:\n    - If $X_1 \\leq 1.5$ (with probability $0.5$), the output is $0.8$.\n    - If $X_1  1.5$ (with probability $0.5$), the output is $2.0$.\n    $$\n    \\mathbb{E}[f(X)|X_2=0.4] = 0.5 \\cdot 0.8 + 0.5 \\cdot 2.0 = 0.4 + 1.0 = 1.4\n    $$\n    The contribution is $2.0 - 1.4 = 0.6$.\n\nAveraging the two contributions:\n$$\n\\phi_{1} = \\frac{1}{2} (0.4) + \\frac{1}{2} (0.6) = 0.2 + 0.3 = 0.5\n$$\n\n**Calculation of $\\phi_2$ (for feature $x_2$):**\n1.  Marginal contribution of $x_2$ added first (coalition $S=\\emptyset$):\n    The contribution is $\\mathbb{E}[f(X)|X_2=0.4] - \\mathbb{E}[f(X)]$.\n    We already calculated $\\mathbb{E}[f(X)|X_2=0.4] = 1.4$.\n    The contribution is $1.4 - 1.6 = -0.2$.\n\n2.  Marginal contribution of $x_2$ added second (coalition $S=\\{x_1\\}$):\n    The contribution is $\\mathbb{E}[f(X)|X_1=1.8, X_2=0.4] - \\mathbb{E}[f(X)|X_1=1.8]$.\n    We already have $f(1.8, 0.4) = 2.0$ and $\\mathbb{E}[f(X)|X_1=1.8] = 2.0$.\n    The contribution is $2.0 - 2.0 = 0$.\n\nAveraging the two contributions:\n$$\n\\phi_{2} = \\frac{1}{2} (-0.2) + \\frac{1}{2} (0) = -0.1 + 0 = -0.1\n$$\n\nSo, the SHAP values are $\\phi_1 = 0.5$ and $\\phi_2 = -0.1$.\n\n**Verification of the Local Accuracy Property:**\nThe property states that $\\phi_1 + \\phi_2 = f(x) - \\mathbb{E}[f(X)]$.\n- Left-hand side: $\\phi_1 + \\phi_2 = 0.5 + (-0.1) = 0.4$.\n- Right-hand side: $f(x) - \\mathbb{E}[f(X)] = 2.0 - 1.6 = 0.4$.\nThe equality $0.4 = 0.4$ holds, verifying the local accuracy property.\n\nThe final answer is the pair $(\\phi_1, \\phi_2)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5  -0.1 \\end{pmatrix}}\n$$", "id": "4542187"}]}