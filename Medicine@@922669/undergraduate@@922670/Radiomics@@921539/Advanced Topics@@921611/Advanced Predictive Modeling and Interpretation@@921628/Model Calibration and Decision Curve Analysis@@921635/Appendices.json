{"hands_on_practices": [{"introduction": "To begin our exploration of clinical utility, we must first master the fundamental calculation at the heart of Decision Curve Analysis (DCA). This exercise requires you to compute the net benefit of a radiomics model directly from its performance metrics—the true positives and false positives. By applying the core formula, you will learn how to quantify the model's value while accounting for the harm of unnecessary interventions, a trade-off defined by the clinical decision threshold $t$ [@problem_id:4551107]. This practice will solidify your understanding of net benefit as a metric and demonstrate how to benchmark a model against default strategies like treating all patients or treating none.", "problem": "A calibrated radiomics classifier is used to predict a binary clinical endpoint (presence or absence of a condition) from imaging features. At a decision threshold probability $t$ that reflects the clinical risk tolerance, the classification produces the following confusion matrix on an external validation set: True Positive (TP) $=84$, False Positive (FP) $=60$, True Negative (TN) $=220$, and False Negative (FN) $=36$, with total sample size $N=400$. The threshold probability is $t=0.15$. Using the foundational decision-theoretic interpretation of threshold probability, where the harm of a False Positive is scaled by the odds corresponding to $t$, compute the net benefit $NB(t)$ of the calibrated radiomics model and the net benefits of the two baseline clinical strategies: treat-all and treat-none. Then, report the net benefit advantage of the radiomics model over the best baseline strategy, defined as $NB(t)-\\max\\{NB_{\\text{all}}(t), NB_{\\text{none}}(t)\\}$. Express the final advantage as a decimal fraction and round your answer to four significant figures.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\nThe following data are provided in the problem statement:\n- True Positives, $TP = 84$\n- False Positives, $FP = 60$\n- True Negatives, $TN = 220$\n- False Negatives, $FN = 36$\n- Total sample size, $N = TP + FP + TN + FN = 84 + 60 + 220 + 36 = 400$\n- Decision threshold probability, $t = 0.15$\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on Decision Curve Analysis (DCA), a standard and well-established methodology in biostatistics and medical informatics for evaluating predictive models. The concepts of net benefit, threshold probability, confusion matrix elements, and baseline strategies (treat-all, treat-none) are fundamental to DCA. The application to a radiomics classifier is a common and appropriate context.\n2.  **Well-Posed**: The problem provides all necessary numerical values and a clear, unambiguous objective. The provided numbers are internally consistent ($TP+FP+TN+FN = N$). The question asks for a specific, computable quantity.\n3.  **Objective**: The problem is stated in precise, quantitative terms, free from subjective language or opinion.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A complete solution will be provided.\n\nThe net benefit ($NB$) of a predictive model is defined based on a specified decision threshold probability, $t$. The threshold $t$ represents the probability of disease at which a patient (or clinician) would opt for treatment. The harm of a false positive (unnecessary treatment) is weighted relative to the benefit of a true positive (correct treatment) by the odds of this threshold, which is $\\frac{t}{1-t}$. The formula for net benefit is:\n$$NB(t) = \\frac{TP}{N} - \\frac{FP}{N} \\left( \\frac{t}{1-t} \\right)$$\nwhere $TP$ is the number of true positives, $FP$ is the number of false positives, and $N$ is the total number of subjects.\n\nFirst, we calculate the odds term for the given threshold $t=0.15$:\n$$ \\frac{t}{1-t} = \\frac{0.15}{1 - 0.15} = \\frac{0.15}{0.85} = \\frac{15}{85} = \\frac{3}{17} $$\n\nNext, we compute the net benefit for the radiomics model, $NB(t)$, using the given confusion matrix values:\n$$ NB(t) = \\frac{84}{400} - \\frac{60}{400} \\left( \\frac{3}{17} \\right) $$\nSimplifying the fractions:\n$$ NB(t) = \\frac{21}{100} - \\frac{15}{100} \\left( \\frac{3}{17} \\right) = \\frac{21}{100} - \\frac{45}{1700} $$\nTo subtract, we find a common denominator, which is $1700$:\n$$ NB(t) = \\frac{21 \\times 17}{1700} - \\frac{45}{1700} = \\frac{357 - 45}{1700} = \\frac{312}{1700} $$\nThis fraction can be simplified:\n$$ NB(t) = \\frac{156}{850} = \\frac{78}{425} $$\n\nNow, we compute the net benefit for the two baseline clinical strategies.\n\n1.  **Treat-None Strategy ($NB_{\\text{none}}(t)$)**: In this strategy, no one is treated. Therefore, there are no true positives and no false positives ($TP=0$, $FP=0$). The net benefit is always zero.\n    $$ NB_{\\text{none}}(t) = \\frac{0}{N} - \\frac{0}{N} \\left( \\frac{t}{1-t} \\right) = 0 $$\n\n2.  **Treat-All Strategy ($NB_{\\text{all}}(t)$)**: In this strategy, everyone is treated. All subjects with the condition are true positives, and all subjects without the condition are false positives.\n    The total number of subjects with the condition (prevalence) is $P = TP + FN = 84 + 36 = 120$.\n    The total number of subjects without the condition is $N_{\\text{neg}} = TN + FP = 220 + 60 = 280$.\n    For the treat-all strategy, the number of \"true positives\" is the total number of diseased patients ($120$), and the number of \"false positives\" is the total number of non-diseased patients ($280$).\n    The net benefit is:\n    $$ NB_{\\text{all}}(t) = \\frac{P}{N} - \\frac{N_{\\text{neg}}}{N} \\left( \\frac{t}{1-t} \\right) = \\frac{120}{400} - \\frac{280}{400} \\left( \\frac{3}{17} \\right) $$\n    Simplifying the fractions:\n    $$ NB_{\\text{all}}(t) = \\frac{3}{10} - \\frac{7}{10} \\left( \\frac{3}{17} \\right) = \\frac{3}{10} - \\frac{21}{170} $$\n    To subtract, we find a common denominator, which is $170$:\n    $$ NB_{\\text{all}}(t) = \\frac{3 \\times 17}{170} - \\frac{21}{170} = \\frac{51 - 21}{170} = \\frac{30}{170} = \\frac{3}{17} $$\n\nThe problem asks for the net benefit advantage of the radiomics model over the best baseline strategy. We must first determine the best baseline strategy by finding $\\max\\{NB_{\\text{all}}(t), NB_{\\text{none}}(t)\\}$.\nWe compare $NB_{\\text{all}}(t) = \\frac{3}{17}$ and $NB_{\\text{none}}(t) = 0$. Clearly, $\\frac{3}{17} > 0$, so the best baseline strategy is \"treat-all\".\n$$ \\max\\{NB_{\\text{all}}(t), NB_{\\text{none}}(t)\\} = \\frac{3}{17} $$\n\nThe net benefit advantage is the difference between the model's net benefit and the best baseline's net benefit:\n$$ \\text{Advantage} = NB(t) - \\max\\{NB_{\\text{all}}(t), NB_{\\text{none}}(t)\\} = \\frac{78}{425} - \\frac{3}{17} $$\nTo find the difference, we use a common denominator of $425$, noting that $425 = 25 \\times 17$.\n$$ \\text{Advantage} = \\frac{78}{425} - \\frac{3 \\times 25}{17 \\times 25} = \\frac{78}{425} - \\frac{75}{425} = \\frac{3}{425} $$\n\nFinally, we express this fraction as a decimal and round to four significant figures.\n$$ \\text{Advantage} = \\frac{3}{425} \\approx 0.0070588235... $$\nRounding to four significant figures, we identify the first non-zero digit ($7$) and the three subsequent digits ($058$). The fifth significant digit is $8$, which is $\\ge 5$, so we round up the last digit ($8$) to $9$.\nThe rounded value is $0.007059$.", "answer": "$$\\boxed{0.007059}$$", "id": "4551107"}, {"introduction": "A predictive model is rarely used in the exact same population in which it was trained. A critical challenge arises when the disease prevalence differs between the training and validation cohorts, an issue known as label shift. This practice guides you through the derivation of a correction factor to recalibrate a logistic regression model for a new target prevalence, ensuring its predictions remain meaningful [@problem_id:4551063]. By connecting the model's intercept term to the log-odds of the disease prevalence, you will gain a profound insight into how statistical models can be adapted for robust real-world deployment.", "problem": "A binary radiomics classifier was trained to predict malignancy from computed tomography features using logistic regression. Let $\\mathbf{x}$ denote the feature vector. The trained model outputs predicted probabilities via the logistic function, with linear predictor $\\eta_{\\mathrm{train}}(\\mathbf{x}) = \\beta_{0} + \\beta^{\\top}\\mathbf{x}$, so that $p_{\\mathrm{train}}(y=1 \\mid \\mathbf{x}) = \\frac{1}{1+\\exp\\!\\left(-\\eta_{\\mathrm{train}}(\\mathbf{x})\\right)}$. In external validation, you wish to assess calibration-in-the-large and conduct Decision Curve Analysis (DCA), but the external cohort has a different disease prevalence. Assume the following:\n\n- The external cohort exhibits label shift: $p_{\\mathrm{target}}(\\mathbf{x} \\mid y) = p_{\\mathrm{train}}(\\mathbf{x} \\mid y)$ for $y \\in \\{0,1\\}$, but $p_{\\mathrm{target}}(y=1) = \\pi \\neq \\pi_{\\mathrm{train}}$.\n- The slope parameters $\\beta$ are stable across cohorts and approximate the log-likelihood ratio $\\ln\\!\\left(\\frac{p(\\mathbf{x}\\mid y=1)}{p(\\mathbf{x}\\mid y=0)}\\right)$ under the training distribution.\n- The calibration-in-the-large is defined as the intercept shift needed so that the average predicted probability equals the observed prevalence.\n\nStarting from Bayes’ theorem in odds form and the definition of the logistic link, derive how a change in class prevalence inflates apparent calibration-in-the-large and obtain the closed-form expression for the intercept correction $\\beta_{0}^{*}$ required to restore calibration-in-the-large to the target prevalence $\\pi$. Then, given a trained intercept $\\beta_{0} = -0.2$, a training prevalence $\\pi_{\\mathrm{train}} = 0.30$, and a target prevalence $\\pi = 0.12$, compute the numerically corrected intercept $\\beta_{0}^{*}$. Round your numerical answer to four significant figures. Express the final value as a dimensionless number.\n\nFinally, briefly justify (without additional computation) how this intercept correction affects expected net benefit in Decision Curve Analysis (DCA) at a fixed threshold probability $p_{t}$, based on the standard definition of net benefit in DCA.", "solution": "The problem is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n-   Classifier: Binary logistic regression.\n-   Feature vector: $\\mathbf{x}$.\n-   Training model linear predictor: $\\eta_{\\mathrm{train}}(\\mathbf{x}) = \\beta_{0} + \\beta^{\\top}\\mathbf{x}$.\n-   Training model predicted probability: $p_{\\mathrm{train}}(y=1 \\mid \\mathbf{x}) = \\frac{1}{1+\\exp(-\\eta_{\\mathrm{train}}(\\mathbf{x}))}$.\n-   Label shift assumption: $p_{\\mathrm{target}}(\\mathbf{x} \\mid y) = p_{\\mathrm{train}}(\\mathbf{x} \\mid y)$ for $y \\in \\{0,1\\}$.\n-   Prevalence shift assumption: $p_{\\mathrm{target}}(y=1) = \\pi \\neq \\pi_{\\mathrm{train}}$.\n-   Slope parameter assumption: $\\beta$ are stable and approximate $\\ln\\left(\\frac{p(\\mathbf{x}\\mid y=1)}{p(\\mathbf{x}\\mid y=0)}\\right)$ under the training distribution.\n-   Definition of calibration-in-the-large: The intercept shift needed so that the average predicted probability equals the observed prevalence.\n-   Task 1: Derive the closed-form expression for the corrected intercept $\\beta_{0}^{*}$.\n-   Task 2: Compute the numerical value of $\\beta_{0}^{*}$ given $\\beta_{0} = -0.2$, $\\pi_{\\mathrm{train}} = 0.30$, and $\\pi = 0.12$.\n-   Task 3: Justify the effect of the correction on Decision Curve Analysis (DCA) net benefit.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is grounded in established principles of statistical modeling, specifically logistic regression, model calibration under dataset shift (label shift), and clinical utility assessment via Decision Curve Analysis. The assumptions are standard in the literature on the transportability of prediction models.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary definitions, assumptions, and data to derive the requested formula, compute the numerical value, and provide a qualitative justification. The objectives are clear and unambiguous.\n-   **Objective**: The problem is stated in precise, objective, and formal mathematical language.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be formulated.\n\n### Derivation of the Corrected Intercept $\\beta_{0}^{*}$\nThe derivation begins with Bayes' theorem in odds form, which relates the posterior odds to the prior odds and the likelihood ratio:\n$$ \\frac{p(y=1 \\mid \\mathbf{x})}{p(y=0 \\mid \\mathbf{x})} = \\frac{p(\\mathbf{x} \\mid y=1)}{p(\\mathbf{x} \\mid y=0)} \\times \\frac{p(y=1)}{p(y=0)} $$\nTaking the natural logarithm of both sides yields the log-odds or logit:\n$$ \\ln\\left(\\frac{p(y=1 \\mid \\mathbf{x})}{1-p(y=1 \\mid \\mathbf{x})}\\right) = \\ln\\left(\\frac{p(\\mathbf{x} \\mid y=1)}{p(\\mathbf{x} \\mid y=0)}\\right) + \\ln\\left(\\frac{p(y=1)}{1-p(y=1)}\\right) $$\nThis expression can be written as $\\mathrm{logit}(p(y=1 \\mid \\mathbf{x})) = \\mathrm{log-LR}(\\mathbf{x}) + \\mathrm{logit_prevalence}$.\n\nFor the training cohort, the prevalence is $\\pi_{\\mathrm{train}}$. The logit of the predicted probability is given by the linear predictor $\\eta_{\\mathrm{train}}(\\mathbf{x})$:\n$$ \\eta_{\\mathrm{train}}(\\mathbf{x}) = \\beta_{0} + \\beta^{\\top}\\mathbf{x} = \\ln\\left(\\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)}\\right) + \\ln\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right) $$\nThe problem states that the slope parameters $\\beta$ approximate the log-likelihood ratio, i.e., $\\beta^{\\top}\\mathbf{x} \\approx \\ln\\left(\\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)}\\right)$. This implies that for a well-calibrated model, the intercept $\\beta_{0}$ captures the log-odds of the training prevalence:\n$$ \\beta_{0} \\approx \\ln\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right) $$\nNow, we consider the external validation (target) cohort, which has a different prevalence $\\pi$. We are given the label shift assumption, $p_{\\mathrm{target}}(\\mathbf{x} \\mid y) = p_{\\mathrm{train}}(\\mathbf{x} \\mid y)$, which means the likelihood ratio is conserved across cohorts.\n$$ \\frac{p_{\\mathrm{target}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{target}}(\\mathbf{x} \\mid y=0)} = \\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)} $$\nThe true logit in the target cohort is therefore:\n$$ \\mathrm{logit}(p_{\\mathrm{target}}(y=1 \\mid \\mathbf{x})) = \\ln\\left(\\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) $$\nTo restore calibration-in-the-large, we need to find a new intercept $\\beta_{0}^{*}$ such that the new linear predictor $\\eta_{\\mathrm{target}}(\\mathbf{x}) = \\beta_{0}^{*} + \\beta^{\\top}\\mathbf{x}$ equals the true logit in the target population.\n$$ \\beta_{0}^{*} + \\beta^{\\top}\\mathbf{x} = \\ln\\left(\\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) $$\nSubstituting $\\beta^{\\top}\\mathbf{x} \\approx \\ln\\left(\\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)}\\right)$ into the equation gives:\n$$ \\beta_{0}^{*} + \\ln\\left(\\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)}\\right) \\approx \\ln\\left(\\frac{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=1)}{p_{\\mathrm{train}}(\\mathbf{x} \\mid y=0)}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) $$\nThis simplifies to $\\beta_{0}^{*} \\approx \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$. To find the updated intercept based on the original intercept $\\beta_{0}$, we can establish a relationship. The original linear predictor is $\\eta_{\\mathrm{train}}(\\mathbf{x})$. The corrected one is $\\eta_{\\mathrm{target}}(\\mathbf{x})$.\n$$ \\eta_{\\mathrm{target}}(\\mathbf{x}) = \\eta_{\\mathrm{train}}(\\mathbf{x}) - \\ln\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) $$\nSubstituting the expressions for the linear predictors:\n$$ \\beta_{0}^{*} + \\beta^{\\top}\\mathbf{x} = (\\beta_{0} + \\beta^{\\top}\\mathbf{x}) - \\ln\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) $$\nThe term $\\beta^{\\top}\\mathbf{x}$ cancels, yielding the closed-form expression for the corrected intercept $\\beta_{0}^{*}$:\n$$ \\beta_{0}^{*} = \\beta_{0} + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) - \\ln\\left(\\frac{\\pi_{\\mathrm{train}}}{1-\\pi_{\\mathrm{train}}}\\right) = \\beta_{0} + \\ln\\left(\\frac{\\pi(1-\\pi_{\\mathrm{train}})}{\\pi_{\\mathrm{train}}(1-\\pi)}\\right) $$\n\n### Numerical Computation\nWe are given the values $\\beta_{0} = -0.2$, $\\pi_{\\mathrm{train}} = 0.30$, and $\\pi = 0.12$. Substitute these into the derived expression:\n$$ \\beta_{0}^{*} = -0.2 + \\ln\\left(\\frac{0.12 \\times (1 - 0.30)}{0.30 \\times (1 - 0.12)}\\right) $$\n$$ \\beta_{0}^{*} = -0.2 + \\ln\\left(\\frac{0.12 \\times 0.70}{0.30 \\times 0.88}\\right) $$\n$$ \\beta_{0}^{*} = -0.2 + \\ln\\left(\\frac{0.084}{0.264}\\right) $$\n$$ \\beta_{0}^{*} = -0.2 + \\ln(0.318181\\overline{81}) $$\n$$ \\beta_{0}^{*} \\approx -0.2 - 1.14513 $$\n$$ \\beta_{0}^{*} \\approx -1.34513 $$\nRounding to four significant figures, we get $\\beta_{0}^{*} = -1.345$.\n\n### Justification of Effect on Decision Curve Analysis (DCA)\nDecision Curve Analysis evaluates the clinical utility of a model by calculating the net benefit (NB) across a range of threshold probabilities $p_t$. The net benefit is defined as $\\mathrm{NB}(p_t) = \\frac{\\mathrm{TP}}{N} - \\frac{\\mathrm{FP}}{N} \\left( \\frac{p_t}{1-p_t} \\right)$, where $\\mathrm{TP}$ is the number of true positives, $\\mathrm{FP}$ is the number of false positives, and $N$ is the total sample size. A patient is classified as positive if their predicted probability exceeds $p_t$.\n\nThe original model was trained on a cohort with prevalence $\\pi_{\\mathrm{train}} = 0.30$. When applied to the target cohort with a lower prevalence $\\pi = 0.12$, the model will systematically overestimate the probability of malignancy for every patient. This poor calibration means that for any given threshold $p_t$, the model will classify an excessive number of patients as positive, leading to a high false positive rate. In the net benefit formula, a high $\\mathrm{FP}$ count, weighted by the harm of a false positive intervention $\\frac{p_t}{1-p_t}$, reduces the overall net benefit.\nThe intercept correction $\\beta_{0}^{*}$ recalibrates the model by lowering the linear predictor $\\eta$ for every patient, which in turn lowers every patient's predicted probability. This adjustment brings the average predicted probability in line with the true target prevalence $\\pi$. As a consequence, for a fixed threshold $p_t$, fewer patients will be classified as positive. This specifically reduces the number of false positives that resulted from the original model's systematic overestimation. By improving the model's calibration to the target population, the intercept correction leads to better decision-making, which is reflected in a higher expected net benefit. The reduction in harm from fewer false positives outweighs the corresponding reduction in true positives, thus increasing the clinical utility of the model.", "answer": "$$\\boxed{-1.345}$$", "id": "4551063"}, {"introduction": "After determining a model has a positive net benefit, the crucial next question is whether this advantage is statistically significant or merely a product of chance. This hands-on coding exercise introduces the nonparametric bootstrap, a robust computational method for estimating the uncertainty surrounding net benefit curves [@problem_id:4551064]. You will learn the correct statistical approach for comparing two models by constructing a confidence interval for the *difference* in their net benefits, a technique that provides a definitive answer on which strategy is superior. This practice provides a complete workflow for moving from a simple comparison to a statistically rigorous assessment of clinical utility.", "problem": "You are given a binary classification setting representative of radiomics where a predictive model assigns a probability of malignancy to each patient. You must formalize and implement Decision Curve Analysis (DCA), construct pointwise confidence intervals via the bootstrap for the net benefit curves at prespecified classification thresholds, and operationalize the interpretation of overlapping curves between clinical decision strategies.\n\nStart from the following fundamental base:\n- In binary classification with true label $y \\in \\{0,1\\}$ and predicted class $\\hat{y} \\in \\{0,1\\}$ at threshold $t \\in (0,1)$, the net benefit at threshold $t$ for a given strategy is defined as\n$$\n\\mathrm{NB}(t) = \\frac{\\mathrm{TP}(t)}{N} - \\frac{\\mathrm{FP}(t)}{N} \\cdot \\frac{t}{1-t},\n$$\nwhere $N$ is the total sample size, $\\mathrm{TP}(t)$ is the number of true positives at threshold $t$, and $\\mathrm{FP}(t)$ is the number of false positives at threshold $t$.\n- For the treat-all strategy (treat everyone as positive), the net benefit at threshold $t$ equals\n$$\n\\mathrm{NB}_{\\mathrm{TA}}(t) = \\hat{\\pi} - \\left(1-\\hat{\\pi}\\right)\\frac{t}{1-t},\n$$\nwhere $\\hat{\\pi}$ is the empirical prevalence $\\hat{\\pi} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$.\n- For the treat-none strategy (treat everyone as negative), the net benefit is identically zero for all $t$, that is $\\mathrm{NB}_{\\mathrm{TN}}(t) = 0$.\n- Nonparametric bootstrap is a resampling method that approximates the sampling distribution of a statistic by resampling with replacement from the observed data. The percentile method constructs a two-sided confidence interval by taking the empirical $\\alpha/2$ and $1-\\alpha/2$ quantiles of the bootstrap replicates.\n\nYour task is to implement the following end-to-end procedure.\n\nData generation:\n1. Use a sample of $N = 500$ patients with binary outcomes $y \\in \\{0,1\\}$ and empirical prevalence approximately $0.35$. Generate $y_i \\sim \\mathrm{Bernoulli}(0.35)$ independently for $i \\in \\{1,\\dots,N\\}$. Use the pseudorandom number generator seed $20240517$ to ensure reproducibility.\n2. Construct Model A predicted probabilities $p^{(A)}_i$ conditional on $y_i$ as follows:\n   - If $y_i = 1$, draw $p^{(A)}_i \\sim \\mathrm{Beta}(\\alpha_1,\\beta_1)$ with $(\\alpha_1,\\beta_1) = (5,2)$.\n   - If $y_i = 0$, draw $p^{(A)}_i \\sim \\mathrm{Beta}(\\alpha_0,\\beta_0)$ with $(\\alpha_0,\\beta_0) = (2,5)$.\n3. Construct Model B as a miscalibrated transformation of Model A with retained discrimination via a logistic link transformation. Define the logit function $\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)$ and its inverse $\\mathrm{logit}^{-1}(z) = \\frac{1}{1+e^{-z}}$. Let\n$$\n\\mathrm{logit}\\left(p^{(B)}_i\\right) = a + b \\cdot \\mathrm{logit}\\left(p^{(A)}_i\\right),\n$$\nwith $a = -0.2$ and $b = 1.5$. Ensure numerical stability by clipping inputs to the logit function away from $0$ and $1$, for example to $[10^{-6}, 1-10^{-6}]$.\n\nStrategies and thresholds:\n4. Consider four strategies: Model A (thresholding $p^{(A)}$), Model B (thresholding $p^{(B)}$), treat-all, and treat-none.\n5. Evaluate decision making at the set of thresholds $T = \\{0.01, 0.10, 0.33, 0.50, 0.90\\}$. For each threshold $t \\in T$, define the predicted class for a model by $\\hat{y}^{(M)}_i(t) = \\mathbb{1}\\{p^{(M)}_i \\ge t\\}$ for $M \\in \\{\\text{A}, \\text{B}\\}$, and compute $\\mathrm{NB}^{(M)}(t)$ using the definition given above. For treat-all and treat-none, use the expressions given above.\n\nBootstrap confidence intervals:\n6. Use the nonparametric bootstrap with $B = 1000$ replicates. For each replicate $b \\in \\{1,\\dots,B\\}$, resample the $N$ patients with replacement to obtain a bootstrap sample. For each $t \\in T$ and each strategy, compute the bootstrap net benefit $\\mathrm{NB}^{*,(M)}_b(t)$.\n7. For each $t \\in T$ and each strategy, compute a two-sided pointwise $95$ percent confidence interval using the percentile method, that is the empirical quantiles at levels $0.025$ and $0.975$ of $\\{\\mathrm{NB}^{*,(M)}_b(t)\\}_{b=1}^{B}$.\n\nInterpreting overlapping curves:\n8. To formalize whether two strategies are distinguishable at threshold $t$, consider the difference in net benefit $\\Delta(t) = \\mathrm{NB}^{(M_1)}(t) - \\mathrm{NB}^{(M_2)}(t)$. Use the bootstrap replicates of the difference $\\Delta^*_b(t) = \\mathrm{NB}^{*,(M_1)}_b(t) - \\mathrm{NB}^{*,(M_2)}_b(t)$ to construct a percentile $95$ percent confidence interval for $\\Delta(t)$. Declare $M_1$ superior to $M_2$ at threshold $t$ if and only if the lower bound of this interval is strictly greater than $0$. This approach directly addresses overlap between curves by assessing uncertainty in the difference rather than visually comparing marginal intervals.\n\nTest suite and required output:\n- Use the fixed thresholds $T = \\{0.01, 0.10, 0.33, 0.50, 0.90\\}$ as the test suite.\n- For each $t \\in T$, evaluate the following three comparisons using the bootstrap confidence interval for the difference as described:\n  1. Model A versus Model B.\n  2. Model A versus treat-all.\n  3. Model B versus treat-all.\n- For each comparison, output a boolean indicating whether the first strategy is superior to the second at threshold $t$ according to the rule above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, for $t$ iterating in the order $0.01$, $0.10$, $0.33$, $0.50$, $0.90$, append booleans in the order: Model A greater than Model B, Model A greater than treat-all, Model B greater than treat-all. For example, the output should look like $[b_1,b_2,\\dots,b_{15}]$, where each $b_k$ is either True or False. No other text should be printed.\n- All quantities are unitless. Express all floating-point numbers internally as real numbers; the final printed values are booleans as specified.\n\nNotes:\n- Angles are not involved; you must not use degrees or radians.\n- The bootstrap confidence intervals are pointwise in $t$. You must not pool thresholds or use any parametric assumptions.", "solution": "The objective is to implement and apply Decision Curve Analysis (DCA), a method for evaluating and comparing prediction models in terms of their net benefit in a clinical context. The analysis involves generating synthetic patient data, building two predictive models (one well-calibrated, one miscalibrated), and comparing them against each other and against two baseline strategies (treat-all and treat-none). The statistical significance of differences in net benefit is assessed using nonparametric bootstrap confidence intervals.\n\nThe procedural steps are as follows: data generation, net benefit calculation, bootstrap resampling for confidence interval estimation, and formal comparison of strategies.\n\nFirst, we generate a synthetic dataset of $N=500$ patients. The true binary outcome $y_i \\in \\{0, 1\\}$ for each patient $i$ is drawn from a Bernoulli distribution with a prevalence of $0.35$. Specifically, $y_i \\sim \\mathrm{Bernoulli}(p)$ with $p=0.35$. A specified pseudorandom number generator seed of $20240517$ is used to ensure reproducibility.\n\nSecond, we construct predicted probabilities for two models, Model A and Model B. The probabilities are generated conditional on the true outcome $y_i$.\nFor Model A, the predicted probabilities $p^{(A)}_i$ are drawn from Beta distributions, a standard choice for modeling probabilities. If the patient has the condition ($y_i=1$), the probability is drawn from a $\\mathrm{Beta}(\\alpha_1, \\beta_1)$ distribution with parameters $(\\alpha_1, \\beta_1) = (5, 2)$. If the patient does not have the condition ($y_i=0$), the probability is drawn from a $\\mathrm{Beta}(\\alpha_0, \\beta_0)$ distribution with parameters $(\\alpha_0, \\beta_0) = (2, 5)$. These parameters are chosen such that the model's predictions are, on average, higher for positive cases than for negative cases, indicating discriminative ability.\n\nFor Model B, the predicted probabilities $p^{(B)}_i$ are created by applying a logistic transformation to the probabilities from Model A. This process is intended to simulate a common issue in predictive modeling: miscalibration, where the model's discrimination is preserved but its probabilities are systematically distorted. The transformation is defined by the equation:\n$$\n\\mathrm{logit}\\left(p^{(B)}_i\\right) = a + b \\cdot \\mathrm{logit}\\left(p^{(A)}_i\\right)\n$$\nwhere $\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)$, and the parameters are given as $a = -0.2$ and $b = 1.5$. Inputs to the logit function are clipped to a small interval away from $0$ and $1$, such as $[10^{-6}, 1-10^{-6}]$, to maintain numerical stability.\n\nThird, we define the net benefit metric. For a given risk threshold $t \\in (0, 1)$, a model-based strategy recommends treatment if $p_i \\ge t$. The net benefit of such a strategy is given by:\n$$\n\\mathrm{NB}(t) = \\frac{\\mathrm{TP}(t)}{N} - \\frac{\\mathrm{FP}(t)}{N} \\cdot \\frac{t}{1-t}\n$$\nwhere $N$ is the total sample size, $\\mathrm{TP}(t)$ is the number of true positives, and $\\mathrm{FP}(t)$ is the number of false positives at threshold $t$. The term $\\frac{t}{1-t}$ represents the odds of the outcome at the threshold and acts as a weight for false positives, penalizing them more heavily at higher thresholds where one requires more certainty to act.\n\nWe also consider two reference strategies:\n1. Treat-all: Everyone is treated, regardless of risk. Its net benefit is $\\mathrm{NB}_{\\mathrm{TA}}(t) = \\hat{\\pi} - (1-\\hat{\\pi})\\frac{t}{1-t}$, where $\\hat{\\pi}$ is the sample prevalence $\\frac{1}{N}\\sum y_i$.\n2. Treat-none: No one is treated. Its net benefit is always zero, $\\mathrm{NB}_{\\mathrm{TN}}(t) = 0$.\nThese strategies are evaluated for a specified set of thresholds $T = \\{0.01, 0.10, 0.33, 0.50, 0.90\\}$.\n\nFourth, to assess the statistical uncertainty of the net benefit estimates, we employ the nonparametric bootstrap. This involves creating $B=1000$ new datasets by resampling with replacement from the original data tuple $(y_i, p^{(A)}_i, p^{(B)}_i)$. For each bootstrap sample and for each threshold $t \\in T$, we compute the net benefit for all four strategies (Model A, Model B, treat-all, treat-none). This process yields $B$ bootstrap replicates of the net benefit for each strategy-threshold pair, e.g., $\\{\\mathrm{NB}^{*,(M)}_b(t)\\}_{b=1}^{B}$.\n\nFinally, we formalize the comparison between two strategies, $M_1$ and $M_2$, at a given threshold $t$. Instead of comparing their individual confidence intervals, which can be misleading if they overlap, we directly analyze the distribution of the difference in their net benefits, $\\Delta(t) = \\mathrm{NB}^{(M_1)}(t) - \\mathrm{NB}^{(M_2)}(t)$. Using the bootstrap replicates, we generate an empirical distribution for this difference: $\\Delta^*_b(t) = \\mathrm{NB}^{*,(M_1)}_b(t) - \\mathrm{NB}^{*,(M_2)}_b(t)$. A two-sided $95\\%$ percentile confidence interval for $\\Delta(t)$ is constructed by finding the $0.025$ and $0.975$ quantiles of the $\\{\\Delta^*_b(t)\\}_{b=1}^{B}$ distribution. The decision rule for superiority is: Strategy $M_1$ is declared superior to strategy $M_2$ at threshold $t$ if and only if the lower bound of the $95\\%$ confidence interval for their net benefit difference is strictly greater than $0$.\n\nThe algorithm proceeds by first generating the single synthetic dataset. Then, it enters a bootstrap loop for $B=1000$ iterations. In each iteration, it resamples the data and calculates the net benefits for all strategies at all thresholds. After the loop, it uses the stored bootstrap results to compute the confidence intervals for the differences between strategies (Model A vs. Model B, Model A vs. treat-all, Model B vs. treat-all) for each threshold. The superiority rule is applied, and the resulting boolean values are collected in a specific order for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\nfrom scipy.special import logit, expit\n\ndef solve():\n    \"\"\"\n    Implements the end-to-end Decision Curve Analysis (DCA) procedure as specified.\n    \"\"\"\n    # ------------------\n    # 1. Define Constants  Setup\n    # ------------------\n    N = 500\n    PREVALENCE = 0.35\n    SEED = 20240517\n    \n    # Model A parameters\n    BETA_PARAMS_POS = {'a': 5, 'b': 2}\n    BETA_PARAMS_NEG = {'a': 2, 'b': 5}\n    \n    # Model B transformation parameters\n    LOGIT_A = -0.2\n    LOGIT_B = 1.5\n    CLIP_EPS = 1e-6\n    \n    # Analysis parameters\n    THRESHOLDS = [0.01, 0.10, 0.33, 0.50, 0.90]\n    B = 1000  # Number of bootstrap replicates\n    CI_ALPHA = 0.05\n\n    rng = np.random.default_rng(SEED)\n\n    # ------------------\n    # 2. Data Generation\n    # ------------------\n    # Generate true outcomes y\n    y = rng.binomial(1, PREVALENCE, size=N)\n    \n    # Generate Model A probabilities p_A conditional on y\n    pos_mask = (y == 1)\n    neg_mask = (y == 0)\n    n_pos = np.sum(pos_mask)\n    n_neg = N - n_pos\n\n    p_A = np.zeros(N, dtype=float)\n    p_A[pos_mask] = beta.rvs(a=BETA_PARAMS_POS['a'], b=BETA_PARAMS_POS['b'], size=n_pos, random_state=rng)\n    p_A[neg_mask] = beta.rvs(a=BETA_PARAMS_NEG['a'], b=BETA_PARAMS_NEG['b'], size=n_neg, random_state=rng)\n    \n    # Generate Model B probabilities p_B via logistic transformation\n    p_A_clipped = np.clip(p_A, CLIP_EPS, 1 - CLIP_EPS)\n    logit_p_A = logit(p_A_clipped)\n    logit_p_B = LOGIT_A + LOGIT_B * logit_p_A\n    p_B = expit(logit_p_B)\n\n    # ------------------\n    # 3. Net Benefit Helper Function\n    # ------------------\n    def calculate_net_benefit(y_true, p_pred, t):\n        \"\"\"Calculates net benefit for a model-based strategy.\"\"\"\n        n_samples = len(y_true)\n        if n_samples == 0:\n            return 0.0\n        \n        # Predictions based on threshold t\n        y_pred = p_pred >= t\n        \n        # TP and FP counts\n        # Cast y_true to boolean for bitwise operations\n        y_true_bool = y_true.astype(bool)\n        tp_count = np.sum(y_pred  y_true_bool)\n        fp_count = np.sum(y_pred  ~y_true_bool)\n\n        # Net Benefit formula\n        return (tp_count / n_samples) - (fp_count / n_samples) * t / (1 - t)\n\n    # ------------------\n    # 4. Bootstrap Procedure\n    # ------------------\n    # Store data for easy resampling\n    all_data = np.column_stack((y, p_A, p_B))\n    \n    # Dictionary to store bootstrap replicates of net benefits\n    # Structure: {threshold: {'model_a': [...], 'model_b': [...], 'treat_all': [...]}}\n    boot_nb_results = {t: {'a': [], 'b': [], 'ta': []} for t in THRESHOLDS}\n\n    for _ in range(B):\n        # Create a bootstrap sample\n        indices = rng.choice(N, size=N, replace=True)\n        boot_sample = all_data[indices]\n        y_boot, p_a_boot, p_b_boot = boot_sample[:, 0], boot_sample[:, 1], boot_sample[:, 2]\n\n        for t in THRESHOLDS:\n            # Net benefit for Model A\n            nb_a = calculate_net_benefit(y_boot, p_a_boot, t)\n            boot_nb_results[t]['a'].append(nb_a)\n\n            # Net benefit for Model B\n            nb_b = calculate_net_benefit(y_boot, p_b_boot, t)\n            boot_nb_results[t]['b'].append(nb_b)\n            \n            # Net benefit for Treat-All strategy\n            pi_hat_boot = np.mean(y_boot)\n            if pi_hat_boot > 0: # Avoid issues if bootstrap sample has no positive cases\n                nb_ta = pi_hat_boot - (1 - pi_hat_boot) * t / (1 - t)\n            else:\n                nb_ta = 0.0 # If prevalence is 0, NB is 0 for t>0.\n            boot_nb_results[t]['ta'].append(nb_ta)\n\n    # ------------------\n    # 5. Interpretation and Final Output\n    # ------------------\n    final_results = []\n    \n    for t in THRESHOLDS:\n        # Convert lists to numpy arrays for vectorized operations\n        nb_a_boots = np.array(boot_nb_results[t]['a'])\n        nb_b_boots = np.array(boot_nb_results[t]['b'])\n        nb_ta_boots = np.array(boot_nb_results[t]['ta'])\n\n        # Comparison 1: Model A vs Model B\n        delta_ab = nb_a_boots - nb_b_boots\n        ci_lower_ab = np.quantile(delta_ab, CI_ALPHA / 2)\n        final_results.append(ci_lower_ab > 0)\n\n        # Comparison 2: Model A vs Treat-All\n        delta_ata = nb_a_boots - nb_ta_boots\n        ci_lower_ata = np.quantile(delta_ata, CI_ALPHA / 2)\n        final_results.append(ci_lower_ata > 0)\n        \n        # Comparison 3: Model B vs Treat-All\n        delta_bta = nb_b_boots - nb_ta_boots\n        ci_lower_bta = np.quantile(delta_bta, CI_ALPHA / 2)\n        final_results.append(ci_lower_bta > 0)\n\n    # Print results in the specified single-line format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "4551064"}]}