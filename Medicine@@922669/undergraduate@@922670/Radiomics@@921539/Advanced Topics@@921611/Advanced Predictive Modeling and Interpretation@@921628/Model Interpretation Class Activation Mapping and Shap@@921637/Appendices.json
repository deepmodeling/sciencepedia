{"hands_on_practices": [{"introduction": "Class Activation Mapping (CAM) is a powerful technique for visualizing which parts of an image a convolutional neural network (CNN) uses to make a classification decision. By understanding how to construct these heatmaps from a model's internal feature maps and weights, we can gain valuable insight into its reasoning process. This first exercise provides a hands-on walkthrough of the core mechanics of CAM, from combining feature maps to upsampling the result for human interpretation, building a foundational understanding of this essential interpretability tool [@problem_id:4551422].", "problem": "In a simplified radiomics setting, consider a convolutional neural network trained to classify medical images, where the last convolutional layer produces two spatial feature maps $f_1$ and $f_2$ of size $2\\times 2$. The classifier for class $c$ is linear in the globally averaged features, with learned weights $w_1^c$ and $w_2^c$. Assume that, consistent with standard use in Class Activation Mapping (CAM), positive class evidence at a spatial location is proportional to the rectified linear combination of the feature maps at that location, using the same class weights, and that negative evidence is suppressed by rectification to emphasize the regions contributing positively to class $c$.\n\nYou are given\n$f_1=\\begin{bmatrix}1&2\\\\0&1\\end{bmatrix}$, $f_2=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}$, $w_1^c=2$, and $w_2^c=-1$.\nProceed as follows:\n\n1. From first principles of linearity and spatial equivariance of convolutional features, construct the $2\\times 2$ class activation map $M_c$ for class $c$ as described above, including rectification of negative values.\n\n2. Upsample $M_c$ to a $4\\times 4$ grid using bilinear interpolation with corner alignment (the $2\\times 2$ corners map to the $4\\times 4$ corners). Define the interpolated value at integer grid coordinates $(i,j)$ with $i,j\\in\\{0,1,2,3\\}$ by applying one-dimensional linear interpolation along one axis followed by the other, assuming uniform spacing.\n\n3. Identify the three grid coordinates in the $4\\times 4$ upsampled map with the largest interpolated values. If there is a tie at the cutoff, break ties by lexicographic order on $(i,j)$, prioritizing smaller $i$ and then smaller $j$.\n\n4. Interpret the per-pixel attribution for SHapley Additive exPlanations (SHAP) at the input resolution as being proportional to the upsampled CAM and normalized to sum to $1$ across the $4\\times 4$ grid. That is, if the upsampled values are $\\{v_{ij}\\}$, define $s_{ij}=v_{ij}/\\sum_{p,q} v_{pq}$.\n\nCompute the exact value of the sum of the SHAP-like attributions over the three coordinates identified in step $3$. Express your final answer as a reduced fraction. Do not include any units.", "solution": "The user wants me to solve a multi-step problem related to Class Activation Mapping (CAM) and SHAP in a simplified radiomics context.\n\nHere's my plan:\n1.  **Validate the problem statement:** I'll go through the validation steps meticulously.\n    *   **Step 1: Extract Givens:** I'll list all the provided matrices, weights, and procedural descriptions.\n    *   **Step 2: Validate:** I'll check for scientific soundness, well-posedness, objectivity, etc. The problem is a well-defined mathematical exercise based on the principles of CAM and SHAP. It does not violate any scientific laws, is self-contained, and uses precise language. The steps are clearly laid out. It is valid.\n    *   **Step 3: Verdict:** The verdict will be \"Valid\".\n\n2.  **Solve the problem (if valid):** I will follow the four steps outlined in the problem statement.\n\n    *   **Part 1: Construct the Class Activation Map (CAM), $M_c$.**\n        *   The problem states that the CAM is a \"rectified linear combination of the feature maps at that location, using the same class weights\".\n        *   The linear combination at each spatial location $(u, v)$ is given by $w_1^c f_1(u,v) + w_2^c f_2(u,v)$.\n        *   The rectification means applying a ReLU function: $\\text{ReLU}(x) = \\max(0, x)$.\n        *   So, $M_c(u,v) = \\text{ReLU}(w_1^c f_1(u,v) + w_2^c f_2(u,v))$.\n        *   Given values:\n            *   $f_1 = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}$\n            *   $f_2 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$\n            *   $w_1^c = 2$\n            *   $w_2^c = -1$\n        *   Let's compute the linear combination first: $L = w_1^c f_1 + w_2^c f_2$.\n            *   $L = 2 \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix} + (-1) \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$\n            *   $L = \\begin{bmatrix} 2 & 4 \\\\ 0 & 2 \\end{bmatrix} + \\begin{bmatrix} 0 & -1 \\\\ -1 & 0 \\end{bmatrix}$\n            *   $L = \\begin{bmatrix} 2+0 & 4-1 \\\\ 0-1 & 2+0 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ -1 & 2 \\end{bmatrix}$\n        *   Now, apply rectification to get $M_c$:\n            *   $M_c = \\text{ReLU}(L) = \\begin{bmatrix} \\max(0, 2) & \\max(0, 3) \\\\ \\max(0, -1) & \\max(0, 2) \\end{bmatrix}$\n            *   $M_c = \\begin{bmatrix} 2 & 3 \\\\ 0 & 2 \\end{bmatrix}$\n\n    *   **Part 2: Upsample $M_c$ to a $4\\times 4$ grid using bilinear interpolation.**\n        *   The source grid is $2 \\times 2$, let's call it $S$. Its values are $S_{00}=2, S_{10}=0, S_{01}=3, S_{11}=2$.\n        *   The destination grid is $4 \\times 4$, let's call it $D$. The indices are $(i,j)$ with $i, j \\in \\{0, 1, 2, 3\\}$.\n        *   The problem specifies \"corner alignment\". This means the corners of the $2 \\times 2$ grid map directly to the corners of the $4 \\times 4$ grid.\n        *   Source coordinates $(u, v)$ can be real numbers in $[0, 1] \\times [0, 1]$.\n        *   The values of $S$ are at integer coordinates $(0,0), (0,1), (1,0), (1,1)$.\n        *   Destination coordinates $(i, j)$ need to be mapped to source coordinates $(u, v)$.\n        *   For a $W \\times H$ source and $W' \\times H'$ destination with corner alignment:\n            *   $u = i \\times \\frac{W-1}{W'-1}$\n            *   $v = j \\times \\frac{H-1}{H'-1}$\n        *   Here, $W=2, H=2$ and $W'=4, H'=4$.\n            *   $u = i \\times \\frac{2-1}{4-1} = i/3$\n            *   $v = j \\times \\frac{2-1}{4-1} = j/3$\n        *   The four known points are $M_c(0,0)=2$, $M_c(1,0)=0$, $M_c(0,1)=3$, $M_c(1,1)=2$.\n        *   The general formula for bilinear interpolation at point $(u,v)$ in the unit square is:\n            *   $V(u,v) = M_c(0,0)(1-u)(1-v) + M_c(1,0)u(1-v) + M_c(0,1)(1-u)v + M_c(1,1)uv$\n        *   Let's substitute our values:\n            *   $V(u,v) = 2(1-u)(1-v) + 0 \\cdot u(1-v) + 3(1-u)v + 2uv$\n            *   $V(u,v) = 2(1-u-v+uv) + 3v - 3uv + 2uv$\n            *   $V(u,v) = 2 - 2u - 2v + 2uv + 3v - 3uv + 2uv$\n            *   $V(u,v) = 2 - 2u + v + uv$\n        *   Now I need to calculate the values $v_{ij} = V(i/3, j/3)$ for $i,j \\in \\{0,1,2,3\\}$.\n        *   $v_{ij} = 2 - 2(i/3) + (j/3) + (i/3)(j/3) = 2 - \\frac{2i}{3} + \\frac{j}{3} + \\frac{ij}{9}$.\n        *   Let's build the $4 \\times 4$ matrix. The values will be fractions with denominator 9.\n            *   $v_{ij} = \\frac{18 - 6i + 3j + ij}{9}$\n        *   $i=0$: $v_{0j} = \\frac{18 + 3j}{9} = 2 + \\frac{j}{3}$.\n            *   $j=0: v_{00} = 2 + 0 = 2 = 18/9$.\n            *   $j=1: v_{01} = 2 + 1/3 = 7/3 = 21/9$.\n            *   $j=2: v_{02} = 2 + 2/3 = 8/3 = 24/9$.\n            *   $j=3: v_{03} = 2 + 3/3 = 3 = 27/9$.\n        *   $i=1$: $v_{1j} = \\frac{18 - 6 + 3j + j}{9} = \\frac{12 + 4j}{9}$.\n            *   $j=0: v_{10} = 12/9 = 4/3$.\n            *   $j=1: v_{11} = 16/9$.\n            *   $j=2: v_{12} = 20/9$.\n            *   $j=3: v_{13} = 24/9 = 8/3$.\n        *   $i=2$: $v_{2j} = \\frac{18 - 12 + 3j + 2j}{9} = \\frac{6 + 5j}{9}$.\n            *   $j=0: v_{20} = 6/9 = 2/3$.\n            *   $j=1: v_{21} = 11/9$.\n            *   $j=2: v_{22} = 16/9$.\n            *   $j=3: v_{23} = 21/9 = 7/3$.\n        *   $i=3$: $v_{3j} = \\frac{18 - 18 + 3j + 3j}{9} = \\frac{6j}{9} = \\frac{2j}{3}$.\n            *   $j=0: v_{30} = 0$.\n            *   $j=1: v_{31} = 2/3 = 6/9$.\n            *   $j=2: v_{32} = 4/3 = 12/9$.\n            *   $j=3: v_{33} = 2 = 18/9$.\n        *   So the upsampled map of values $\\{v_{ij}\\}$ is:\n            $$ V = \\frac{1}{9} \\begin{pmatrix}\n            18 & 21 & 24 & 27 \\\\\n            12 & 16 & 20 & 24 \\\\\n            6 & 11 & 16 & 21 \\\\\n            0 & 6 & 12 & 18\n            \\end{pmatrix} $$\n\n    *   **Part 3: Identify the three largest values and their coordinates.**\n        *   We list pairs of (value, coordinate $(i, j)$) and sort them first by value in descending order, and then by coordinate in lexicographic order for ties.\n        *   The matrix of numerators is $\\begin{pmatrix} 18 & 21 & 24 & 27 \\\\ 12 & 16 & 20 & 24 \\\\ 6 & 11 & 16 & 21 \\\\ 0 & 6 & 12 & 18 \\end{pmatrix}$.\n        *   The largest numerator is $27$ at $(0,3)$. Value: $v_{03} = 27/9 = 3$.\n        *   The next largest numerator is $24$, which occurs at $(0,2)$ and $(1,3)$. Value: $v_{02} = v_{13} = 24/9 = 8/3$.\n        *   The next largest numerator is $21$, occurring at $(0,1)$ and $(2,3)$. Value: $v_{01} = v_{23} = 21/9 = 7/3$.\n        *   Following the sorting rule:\n            1.  Value $27/9$ at $(0,3)$.\n            2.  Value $24/9$. Coordinates are $(0,2)$ and $(1,3)$. Lexicographically, $(0,2) < (1,3)$. So, the second coordinate is $(0,2)$.\n            3.  The third coordinate must be $(1,3)$, also with value $24/9$.\n        *   Thus, the three coordinates with the largest interpolated values are $(0,3)$, $(0,2)$, and $(1,3)$. The corresponding values are $v_{03}=3$, $v_{02}=8/3$, and $v_{13}=8/3$.\n\n    *   **Part 4: Compute the sum of SHAP-like attributions.**\n        *   The SHAP-like attribution for a pixel $(i,j)$ is $s_{ij} = v_{ij} / \\sum_{p,q} v_{pq}$.\n        *   First, we compute the total sum of all upsampled values, $\\sum_{p,q} v_{pq}$. This is the sum of all elements in the matrix $V$.\n        *   Sum of numerators:\n            Row 0 sum: $18+21+24+27 = 90$\n            Row 1 sum: $12+16+20+24 = 72$\n            Row 2 sum: $6+11+16+21 = 54$\n            Row 3 sum: $0+6+12+18 = 36$\n        *   Total sum of numerators = $90 + 72 + 54 + 36 = 252$.\n        *   The total sum of values is $\\sum_{p,q} v_{pq} = \\frac{252}{9} = 28$.\n        *   We need to compute the sum of the attributions for the top three coordinates: $S = s_{03} + s_{02} + s_{13}$.\n        *   $s_{03} = \\frac{v_{03}}{\\sum v} = \\frac{3}{28}$.\n        *   $s_{02} = \\frac{v_{02}}{\\sum v} = \\frac{8/3}{28} = \\frac{8}{3 \\times 28} = \\frac{8}{84} = \\frac{2}{21}$.\n        *   $s_{13} = \\frac{v_{13}}{\\sum v} = \\frac{8/3}{28} = \\frac{8}{3 \\times 28} = \\frac{2}{21}$.\n        *   The sum is $S = \\frac{3}{28} + \\frac{2}{21} + \\frac{2}{21} = \\frac{3}{28} + \\frac{4}{21}$.\n        *   The least common multiple of $28 = 2^2 \\times 7$ and $21 = 3 \\times 7$ is $2^2 \\times 3 \\times 7 = 84$.\n        *   $S = \\frac{3 \\times 3}{28 \\times 3} + \\frac{4 \\times 4}{21 \\times 4} = \\frac{9}{84} + \\frac{16}{84} = \\frac{9+16}{84} = \\frac{25}{84}$.\n        *   The numerator $25 = 5^2$ and the denominator $84 = 2^2 \\times 3 \\times 7$ share no common factors, so the fraction is reduced.", "answer": "$$\n\\boxed{\\frac{25}{84}}\n$$", "id": "4551422"}, {"introduction": "While CAMs excel at spatial attribution in images, SHapley Additive exPlanations (SHAP) offer a rigorously grounded method for attributing a model's prediction to its input features, applicable to a wide range of models and data types. A common misconception is that a feature's importance is solely determined by its coefficient in a model. This practice uses a simple linear model to demonstrate a crucial principle of SHAP: attribution depends not just on the model's parameters but also on the feature's value relative to its background distribution, revealing the difference between global feature importance and local, instance-specific explanations [@problem_id:4551432].", "problem": "In a radiomics study, a linear risk score is built from two quantitative features extracted from computed tomography scans. Denote the features by $X_1$ and $X_2$. The training cohort has the following properties:\n- The features are statistically independent.\n- The means are $\\mu_1 = 0$ and $\\mu_2 = 0$.\n- The standard deviations are $\\sigma_1 = 3$ and $\\sigma_2 = 1$.\n\nThe model is the linear function $f(\\mathbf{x}) = \\beta \\left(x_1 + x_2\\right)$ with $\\beta = 2$ and zero intercept. For model interpretation, consider Shapley Additive exPlanations (SHAP), defined via the cooperative game theoretic Shapley values with the background choice given by the empirical training distribution described above. That is, for any subset $S$ of features, the coalition value is $v(S) = \\mathbb{E}\\!\\left[f(X_1, X_2)\\,\\middle|\\, X_S = x_S\\right]$, where the expectation is taken over the training distribution.\n\nConsider a particular patient whose measured features are one standard deviation above the cohort mean for each feature, so that $x_1 = \\mu_1 + \\sigma_1$ and $x_2 = \\mu_2 + \\sigma_2$.\n\nUsing only the definition of Shapley values as averages of marginal contributions over all coalitions and the stated statistical properties of the background, derive the SHAP attributions $\\phi_1$ and $\\phi_2$ for this patient. Then compute the difference $\\Delta = \\phi_1 - \\phi_2$. Express the final answer as an exact number (no rounding). The answer is unitless.", "solution": "The user wants me to solve the following problem:\nIn a radiomics study, a linear risk score is built from two quantitative features extracted from computed tomography scans. Denote the features by $X_1$ and $X_2$. The training cohort has the following properties:\n- The features are statistically independent.\n- The means are $\\mu_1 = 0$ and $\\mu_2 = 0$.\n- The standard deviations are $\\sigma_1 = 3$ and $\\sigma_2 = 1$.\n\nThe model is the linear function $f(\\mathbf{x}) = \\beta \\left(x_1 + x_2\\right)$ with $\\beta = 2$ and zero intercept. For model interpretation, consider Shapley Additive exPlanations (SHAP), defined via the cooperative game theoretic Shapley values with the background choice given by the empirical training distribution described above. That is, for any subset $S$ of features, the coalition value is $v(S) = \\mathbb{E}\\!\\left[f(X_1, X_2)\\,\\middle|\\, X_S = x_S\\right]$, where the expectation is taken over the training distribution.\n\nConsider a particular patient whose measured features are one standard deviation above the cohort mean for each feature, so that $x_1 = \\mu_1 + \\sigma_1$ and $x_2 = \\mu_2 + \\sigma_2$.\n\nUsing only the definition of Shapley values as averages of marginal contributions over all coalitions and the stated statistical properties of the background, derive the SHAP attributions $\\phi_1$ and $\\phi_2$ for this patient. Then compute the difference $\\Delta = \\phi_1 - \\phi_2$. Express the final answer as an exact number (no rounding). The answer is unitless.\n\n### Step 1: Extract Givens\n- Features: $X_1$, $X_2$.\n- Statistical properties of the training distribution:\n  - $X_1$ and $X_2$ are statistically independent.\n  - Mean of $X_1$: $\\mathbb{E}[X_1] = \\mu_1 = 0$.\n  - Mean of $X_2$: $\\mathbb{E}[X_2] = \\mu_2 = 0$.\n  - Standard deviation of $X_1$: $\\sigma_1 = 3$.\n  - Standard deviation of $X_2$: $\\sigma_2 = 1$.\n- Model function: $f(x_1, x_2) = \\beta(x_1 + x_2)$ with $\\beta = 2$, so $f(x_1, x_2) = 2(x_1 + x_2)$.\n- Definition of coalition value function: $v(S) = \\mathbb{E}[f(X_1, X_2) | X_S = x_S]$.\n- Specific instance (patient) to explain:\n  - $x_1 = \\mu_1 + \\sigma_1 = 0 + 3 = 3$.\n  - $x_2 = \\mu_2 + \\sigma_2 = 0 + 1 = 1$.\n- Goal: Compute $\\phi_1$, $\\phi_2$, and $\\Delta = \\phi_1 - \\phi_2$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is well-grounded in the field of explainable artificial intelligence (XAI), specifically focusing on SHAP (Shapley Additive exPlanations), a widely used and theoretically sound method. The context is radiomics, a valid field of medical image analysis. All concepts (statistical independence, mean, standard deviation, linear models, conditional expectation) are standard in mathematics and statistics.\n- **Well-Posed:** The problem provides all necessary definitions, constants, and data to compute a unique solution. The model, the statistical properties of the background distribution, and the instance to be explained are all clearly defined.\n- **Objective:** The problem is stated in precise, objective, mathematical language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the flaws listed in the problem validation checklist. It is a standard, solvable problem in applied mathematics and machine learning.\n\nThe problem is valid. I will now proceed to the solution.\n\n###\nThe SHAP value $\\phi_i$ for a feature $i$ in a set of $N$ features is the weighted average of its marginal contributions to all possible coalitions $S$ of features. The formula is:\n$$ \\phi_i(f, \\mathbf{x}) = \\sum_{S \\subseteq \\{X_1, \\dots, X_N\\} \\setminus \\{X_i\\}} \\frac{|S|!(N - |S| - 1)!}{N!} \\left[ v(S \\cup \\{X_i\\}) - v(S) \\right] $$\nIn this problem, we have $N=2$ features, $X_1$ and $X_2$. The set of features is $F = \\{X_1, X_2\\}$.\n\nFirst, we must calculate the value function $v(S)$ for all possible coalitions $S \\subseteq F$. The coalitions are $\\emptyset$, $\\{X_1\\}$, $\\{X_2\\}$, and $\\{X_1, X_2\\}$. The value function is defined as $v(S) = \\mathbb{E}[f(X_1, X_2) | X_S = x_S]$, where the expectation is over the background distribution of features not in $S$. The instance to be explained has feature values $x_1 = 3$ and $x_2 = 1$.\n\n1.  Coalition $S = \\emptyset$:\n    The value function is the unconditional expectation of the model output, which is the base value $\\phi_0$.\n    $$ v(\\emptyset) = \\mathbb{E}[f(X_1, X_2)] = \\mathbb{E}[2(X_1 + X_2)] $$\n    By linearity of expectation:\n    $$ v(\\emptyset) = 2(\\mathbb{E}[X_1] + \\mathbb{E}[X_2]) = 2(0 + 0) = 0 $$\n\n2.  Coalition $S = \\{X_1\\}$:\n    The value function is the expected model output, conditioned on $X_1$ being fixed to its value $x_1=3$.\n    $$ v(\\{X_1\\}) = \\mathbb{E}[f(X_1, X_2) | X_1 = x_1] = \\mathbb{E}[2(X_1 + X_2) | X_1 = 3] $$\n    $$ v(\\{X_1\\}) = \\mathbb{E}[2(3 + X_2) | X_1 = 3] $$\n    Since $X_1$ and $X_2$ are statistically independent, the distribution of $X_2$ is not affected by the value of $X_1$.\n    $$ v(\\{X_1\\}) = \\mathbb{E}[6 + 2X_2] = 6 + 2\\mathbb{E}[X_2] = 6 + 2(0) = 6 $$\n\n3.  Coalition $S = \\{X_2\\}$:\n    Similarly, we fix $X_2$ to its value $x_2=1$.\n    $$ v(\\{X_2\\}) = \\mathbb{E}[f(X_1, X_2) | X_2 = x_2] = \\mathbb{E}[2(X_1 + X_2) | X_2 = 1] $$\n    $$ v(\\{X_2\\}) = \\mathbb{E}[2(X_1 + 1) | X_2 = 1] $$\n    Again, due to independence:\n    $$ v(\\{X_2\\}) = \\mathbb{E}[2X_1 + 2] = 2\\mathbb{E}[X_1] + 2 = 2(0) + 2 = 2 $$\n\n4.  Coalition $S = \\{X_1, X_2\\}$:\n    Here, both features are fixed to their values. The expectation of a constant is the constant itself. This is the model output for the specific instance.\n    $$ v(\\{X_1, X_2\\}) = \\mathbb{E}[f(X_1, X_2) | X_1 = 3, X_2 = 1] = f(3, 1) = 2(3 + 1) = 8 $$\n\nNow we can compute the SHAP values $\\phi_1$ and $\\phi_2$.\n\nFor feature $X_1$, the possible coalitions it can be added to are $S=\\emptyset$ and $S=\\{X_2\\}$. The formula for $\\phi_1$ with $N=2$ is:\n$$ \\phi_1 = \\frac{0!(2-0-1)!}{2!} [v(\\{X_1\\}) - v(\\emptyset)] + \\frac{1!(2-1-1)!}{2!} [v(\\{X_1, X_2\\}) - v(\\{X_2\\})] $$\n$$ \\phi_1 = \\frac{1 \\cdot 1}{2} [v(\\{X_1\\}) - v(\\emptyset)] + \\frac{1 \\cdot 1}{2} [v(\\{X_1, X_2\\}) - v(\\{X_2\\})] $$\nSubstituting the calculated values:\n$$ \\phi_1 = \\frac{1}{2} [6 - 0] + \\frac{1}{2} [8 - 2] = \\frac{6}{2} + \\frac{6}{2} = 3 + 3 = 6 $$\n\nFor feature $X_2$, the possible coalitions it can be added to are $S=\\emptyset$ and $S=\\{X_1\\}$. The formula for $\\phi_2$ with $N=2$ is:\n$$ \\phi_2 = \\frac{0!(2-0-1)!}{2!} [v(\\{X_2\\}) - v(\\emptyset)] + \\frac{1!(2-1-1)!}{2!} [v(\\{X_1, X_2\\}) - v(\\{X_1\\})] $$\n$$ \\phi_2 = \\frac{1}{2} [v(\\{X_2\\}) - v(\\emptyset)] + \\frac{1}{2} [v(\\{X_1, X_2\\}) - v(\\{X_1\\})] $$\nSubstituting the calculated values:\n$$ \\phi_2 = \\frac{1}{2} [2 - 0] + \\frac{1}{2} [8 - 6] = \\frac{2}{2} + \\frac{2}{2} = 1 + 1 = 2 $$\n\nThe SHAP values are $\\phi_1 = 6$ and $\\phi_2 = 2$.\nAs a check, the efficiency property of SHAP values states that the sum of the attributions must equal the difference between the prediction for the instance and the base value: $f(\\mathbf{x}) - \\mathbb{E}[f(X)] = \\phi_1 + \\phi_2$.\nHere, $f(3, 1) = 8$ and $\\mathbb{E}[f(X)] = v(\\emptyset) = 0$. So, $8-0=8$.\nThe sum of SHAP values is $\\phi_1 + \\phi_2 = 6 + 2 = 8$. The property holds.\n\nA notable simplification exists for linear models with independent features. For a model $f(\\mathbf{x}) = \\sum_j w_j x_j + b$, the SHAP value for feature $i$ is $\\phi_i = w_i(x_i - \\mathbb{E}[X_i])$. In our case, $f(x_1, x_2) = 2x_1 + 2x_2$, so $w_1=2$, $w_2=2$, $b=0$. The features are independent with $\\mathbb{E}[X_1]=0$ and $\\mathbb{E}[X_2]=0$. The instance is $x_1=3$, $x_2=1$.\nApplying this simplified formula:\n$\\phi_1 = w_1(x_1 - \\mathbb{E}[X_1]) = 2(3 - 0) = 6$.\n$\\phi_2 = w_2(x_2 - \\mathbb{E}[X_2]) = 2(1 - 0) = 2$.\nThis confirms the results obtained from the full Shapley value definition.\n\nThe final step is to compute the difference $\\Delta = \\phi_1 - \\phi_2$.\n$$ \\Delta = 6 - 2 = 4 $$\nThe difference is $4$. The standard deviations $\\sigma_1=3$ and $\\sigma_2=1$ were used to determine the specific patient's feature values but do not directly appear in the final SHAP calculation itself, other than setting $x_1=3$ and $x_2=1$.", "answer": "$$\\boxed{4}$$", "id": "4551432"}, {"introduction": "The power of SHAP comes with subtleties, especially when features are not independent. The presence of correlation between features, common in radiomics data, raises a critical question: how should we account for these dependencies when explaining a prediction? This advanced practice explores the distinction between *interventional* and *observational* expectations in the SHAP framework, quantifying how feature correlation can shift attribution values. By working through this example, you will develop a more sophisticated understanding of the assumptions underlying your interpretation tools and the importance of aligning them with the specific explanatory question you seek to answer [@problem_id:4551492].", "problem": "A radiomics pipeline extracts two standardized quantitative features from computed tomography of lung nodules: texture entropy $X_{1}$ and shape compactness $X_{2}$. A linear predictor of malignancy score is trained as $f(x_{1},x_{2}) = w_{1} x_{1} + w_{2} x_{2} + b$. In clinical model interpretation, Class Activation Mapping (CAM) highlights spatial regions for convolutional neural networks, whereas SHapley Additive exPlanations (SHAP) allocates additive contributions of features to a model output. In tabular-feature radiomics, it is essential to distinguish interventional semantics (missing features are sampled independently from their marginals) from observational semantics (missing features are integrated out under their conditional distribution given the observed features).\n\nConsider a single patient with observed features $x_{1} = 0.65$ and $x_{2} = 0.75$. The trained linear model has parameters $w_{1} = 2.0$, $w_{2} = 1.5$, and $b = -0.2$. The feature vector $(X_{1},X_{2})$ in the training population is well-modeled by a bivariate Gaussian with means $\\mu_{1} = 0.50$, $\\mu_{2} = 0.80$, standard deviations $\\sigma_{1} = 0.10$, $\\sigma_{2} = 0.15$, and correlation $\\rho = 0.60 \\neq 0$. Assume the standard properties of multivariate Gaussian distributions and the cooperative game theory definition of SHAP values, and treat $f$ as the payoff function.\n\nStarting from first principles, compute the SHAP values for $X_{1}$ and $X_{2}$ under both interventional and observational semantics for this patient. Then, quantify the dependence-induced attribution shift for $X_{1}$ as the difference between the observational and interventional SHAP values:\n$$\\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}}.$$\nProvide only $\\Delta_{1}$ as your final answer. Round your final numerical answer to four significant figures. Express the answer as a pure number with no units.", "solution": "### Step 1: Extract Givens\n-   **Model:** Linear predictor of malignancy score, $f(x_{1},x_{2}) = w_{1} x_{1} + w_{2} x_{2} + b$.\n-   **Features:** Texture entropy $X_{1}$ and shape compactness $X_{2}$.\n-   **Specified Instance:** $x_1 = 0.65$, $x_2 = 0.75$.\n-   **Model Parameters:** $w_1 = 2.0$, $w_2 = 1.5$, $b = -0.2$.\n-   **Data Distribution:** $(X_{1},X_{2})$ is a bivariate Gaussian.\n    -   Means: $\\mu_1 = 0.50$, $\\mu_2 = 0.80$.\n    -   Standard Deviations: $\\sigma_1 = 0.10$, $\\sigma_2 = 0.15$.\n    -   Correlation: $\\rho = 0.60$.\n-   **Task:** Compute the dependence-induced attribution shift for $X_{1}$, defined as $\\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}}$, where $\\phi_{1}^{\\mathrm{obs}}$ and $\\phi_{1}^{\\mathrm{int}}$ are the SHAP values for $X_{1}$ under observational and interventional semantics, respectively. The final answer must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly grounded in established concepts from machine learning interpretability (SHAP values) and statistics (multivariate Gaussian distributions). The distinction between interventional and observational conditional expectations is a key topic in causality and model interpretation literature. The scenario of interpreting a linear model on radiomics features is scientifically realistic.\n2.  **Well-Posed:** The problem is well-posed. It provides all necessary parameters and a clear, unambiguous objective. The underlying mathematical framework (SHAP value definition, properties of Gaussian distributions) guarantees the existence of a unique, stable, and meaningful solution.\n3.  **Objective:** The problem is stated using precise, objective language and quantitative data. There are no subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and objective, with a complete and consistent setup. I will proceed with a full solution.\n\n### Solution Derivation\nThe SHAP (SHapley Additive exPlanations) value for a feature quantifies its contribution to a model's prediction for a specific instance, based on principles from cooperative game theory. For a model with two features, $X_1$ and $X_2$, the SHAP value for feature $X_1$ at a specific instance $x = (x_1, x_2)$ is given by the weighted average of its marginal contributions over all possible feature subsets (coalitions):\n$$ \\phi_{1} = \\frac{1}{2} \\left[ (E[f(X) | X_1=x_1] - E[f(X)]) + (f(x_1, x_2) - E[f(X) | X_2=x_2]) \\right] $$\nwhere $f(x_1, x_2)$ is the model prediction for the full instance, and $E[f(X)]$ is the baseline expected prediction over the data distribution. The difference between interventional and observational semantics lies in how the conditional expectations $E[f(X) | X_S = x_S]$ are calculated.\n\nThe model is linear: $f(X_1, X_2) = w_1 X_1 + w_2 X_2 + b$.\nThe baseline expectation is the same for both semantics:\n$$ E[f(X)] = E[w_1 X_1 + w_2 X_2 + b] = w_1 E[X_1] + w_2 E[X_2] + b = w_1 \\mu_1 + w_2 \\mu_2 + b $$\n\n**1. Interventional Semantics ($\\phi_{1}^{\\mathrm{int}}$)**\nInterventional semantics assumes feature independence. When conditioning on a subset of features $S$, the remaining features are integrated out over their marginal distributions, independent of the values in $S$.\n\n-   $E^{\\mathrm{int}}[f(X) | X_1=x_1] = E[w_1 x_1 + w_2 X_2 + b] = w_1 x_1 + w_2 E[X_2] + b = w_1 x_1 + w_2 \\mu_2 + b$.\n-   $E^{\\mathrm{int}}[f(X) | X_2=x_2] = E[w_1 X_1 + w_2 x_2 + b] = w_1 E[X_1] + w_2 x_2 + b = w_1 \\mu_1 + w_2 x_2 + b$.\n\nSubstituting these into the SHAP formula for $\\phi_1$:\n$$ \\phi_{1}^{\\mathrm{int}} = \\frac{1}{2} \\left[ (w_1 x_1 + w_2 \\mu_2 + b) - (w_1 \\mu_1 + w_2 \\mu_2 + b) \\right] + \\frac{1}{2} \\left[ (w_1 x_1 + w_2 x_2 + b) - (w_1 \\mu_1 + w_2 x_2 + b) \\right] $$\n$$ \\phi_{1}^{\\mathrm{int}} = \\frac{1}{2} [w_1 (x_1 - \\mu_1)] + \\frac{1}{2} [w_1 (x_1 - \\mu_1)] $$\n$$ \\phi_{1}^{\\mathrm{int}} = w_1 (x_1 - \\mu_1) $$\n\n**2. Observational Semantics ($\\phi_{1}^{\\mathrm{obs}}$)**\nObservational semantics uses the true conditional probability distribution of the features. For a bivariate Gaussian, the conditional expectation of $X_i$ given $X_j=x_j$ is:\n$$ E[X_i | X_j=x_j] = \\mu_i + \\rho \\frac{\\sigma_i}{\\sigma_j} (x_j - \\mu_j) $$\n\nThe required conditional expectations for the model output are:\n-   $E^{\\mathrm{obs}}[f(X) | X_1=x_1] = E[w_1 x_1 + w_2 X_2 + b | X_1=x_1] = w_1 x_1 + w_2 E[X_2 | X_1=x_1] + b$\n    $$ E^{\\mathrm{obs}}[f(X) | X_1=x_1] = w_1 x_1 + w_2 \\left(\\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1)\\right) + b $$\n-   $E^{\\mathrm{obs}}[f(X) | X_2=x_2] = E[w_1 X_1 + w_2 x_2 + b | X_2=x_2] = w_1 E[X_1 | X_2=x_2] + w_2 x_2 + b$\n    $$ E^{\\mathrm{obs}}[f(X) | X_2=x_2] = w_1 \\left(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2)\\right) + w_2 x_2 + b $$\n\nNow, we compute the two marginal contribution terms for $\\phi_{1}^{\\mathrm{obs}}$:\n-   Term 1: $E^{\\mathrm{obs}}[f(X) | X_1=x_1] - E[f(X)]$\n    $$ = \\left(w_1 x_1 + w_2 \\left(\\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1)\\right) + b\\right) - (w_1 \\mu_1 + w_2 \\mu_2 + b) $$\n    $$ = w_1 (x_1 - \\mu_1) + w_2 \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) $$\n-   Term 2: $f(x_1, x_2) - E^{\\mathrm{obs}}[f(X) | X_2=x_2]$\n    $$ = (w_1 x_1 + w_2 x_2 + b) - \\left(w_1 \\left(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2)\\right) + w_2 x_2 + b\\right) $$\n    $$ = w_1 x_1 - w_1 \\left(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2)\\right) = w_1 (x_1 - \\mu_1) - w_1 \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) $$\n\nCombining these terms to find $\\phi_{1}^{\\mathrm{obs}}$:\n$$ \\phi_{1}^{\\mathrm{obs}} = \\frac{1}{2} \\left[ w_1 (x_1 - \\mu_1) + w_2 \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) \\right] + \\frac{1}{2} \\left[ w_1 (x_1 - \\mu_1) - w_1 \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right] $$\n$$ \\phi_{1}^{\\mathrm{obs}} = w_1 (x_1 - \\mu_1) + \\frac{1}{2} \\rho \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) $$\n\n**3. Dependence-Induced Attribution Shift ($\\Delta_{1}$)**\nThe shift is the difference between the observational and interventional SHAP values.\n$$ \\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}} $$\n$$ \\Delta_{1} = \\left[ w_1 (x_1 - \\mu_1) + \\frac{1}{2} \\rho \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) \\right] - \\left[ w_1 (x_1 - \\mu_1) \\right] $$\n$$ \\Delta_{1} = \\frac{\\rho}{2} \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) $$\n\n**4. Numerical Calculation**\nWe substitute the given values into the expression for $\\Delta_{1}$:\n-   $x_1 - \\mu_1 = 0.65 - 0.50 = 0.15$\n-   $x_2 - \\mu_2 = 0.75 - 0.80 = -0.05$\n-   $\\frac{\\sigma_2}{\\sigma_1} = \\frac{0.15}{0.10} = 1.5$\n-   $\\frac{\\sigma_1}{\\sigma_2} = \\frac{0.10}{0.15} = \\frac{2}{3}$\n\n$$ \\Delta_{1} = \\frac{0.60}{2} \\left( (1.5) (1.5) (0.15) - (2.0) \\left(\\frac{2}{3}\\right) (-0.05) \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( (2.25) (0.15) - \\left(\\frac{4}{3}\\right) (-0.05) \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( 0.3375 + \\frac{0.20}{3} \\right) $$\nTo maintain precision, we use fractions:\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{9}{4} \\cdot \\frac{15}{100} - \\frac{4}{3} \\cdot \\left(-\\frac{5}{100}\\right) \\right) $$\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{135}{400} + \\frac{20}{300} \\right) = \\frac{3}{10} \\left( \\frac{27}{80} + \\frac{1}{15} \\right) $$\nThe least common multiple of $80$ and $15$ is $240$.\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{27 \\times 3}{240} + \\frac{1 \\times 16}{240} \\right) = \\frac{3}{10} \\left( \\frac{81 + 16}{240} \\right) $$\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{97}{240} \\right) = \\frac{97}{800} $$\nConverting to a decimal:\n$$ \\Delta_{1} = 0.12125 $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\Delta_{1} \\approx 0.1213 $$\nThe dependence-induced attribution shift for $X_1$ redistributes part of the attribution. The positive correlation ($\\rho > 0$) causes the presence of a high-value $X_1$ ($x_1 > \\mu_1$) to imply a higher expected value for $X_2$, and the presence of a low-value $X_2$ ($x_2 < \\mu_2$) to imply a lower expected value for $X_1$. $\\Delta_1$ captures this complex interplay.", "answer": "$$\\boxed{0.1213}$$", "id": "4551492"}]}