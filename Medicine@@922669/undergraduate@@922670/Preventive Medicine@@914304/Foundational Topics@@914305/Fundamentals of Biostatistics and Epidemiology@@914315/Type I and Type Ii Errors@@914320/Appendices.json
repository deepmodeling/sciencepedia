{"hands_on_practices": [{"introduction": "The cornerstone of ethical and efficient clinical research is the *a priori* sample size calculation. Before investing resources and enrolling participants, we must determine the minimum number of subjects needed to have a reasonable chance of detecting a clinically meaningful effect. This exercise [@problem_id:4992656] guides you through the fundamental derivation and calculation of sample size for a randomized controlled trial with a continuous outcome, directly connecting the theoretical concepts of Type I error ($\\alpha$), power ($1-\\beta$), effect size ($\\delta$), and data variability ($\\sigma$) to a practical study design decision.", "problem": "A two-arm Randomized Controlled Trial (RCT) is planned to compare an antihypertensive drug against placebo with equal allocation. The primary endpoint is the change in systolic blood pressure, measured in millimeters of mercury. Investigators will use a two-sided hypothesis test of the difference in population means with type I error rate $\\alpha = 0.05$. Assume individual patient outcomes within each arm are independent and identically distributed, approximately normal, with a common and known standard deviation $\\sigma = 10$ millimeters of mercury. The clinically meaningful difference to detect is a true mean reduction in the treatment arm relative to placebo of $\\delta = 4$ millimeters of mercury. The design target is statistical power $1 - \\beta = 0.80$.\n\nStarting from the definitions of type I error, type II error, and statistical power, and the distribution of the standardized difference in sample means under the null and under a fixed alternative, derive an expression for the required per-arm sample size for a two-sided test with equal allocation. Then compute the minimal integer number of participants per arm required to attain power at least $0.80$ for $\\delta = 4$, $\\sigma = 10$, and $\\alpha = 0.05$. Provide the final answer as a single integer giving the required per-arm sample size.", "solution": "Let $\\mu_1$ and $\\mu_2$ be the population mean changes in systolic blood pressure for the placebo and treatment arms, respectively. The sample sizes are equal, $n_1 = n_2 = n$. The outcomes are assumed to be normally distributed with a common known standard deviation $\\sigma$. The hypotheses for a two-sided test are:\n$$ H_0: \\mu_1 - \\mu_2 = 0 $$\n$$ H_a: \\mu_1 - \\mu_2 \\neq 0 $$\n\nThe test statistic is the standardized difference in sample means, $\\bar{X}_1 - \\bar{X}_2$. The sampling distribution of the difference is $\\bar{X}_1 - \\bar{X}_2 \\sim N(\\mu_1 - \\mu_2, \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n}) = N(\\mu_1 - \\mu_2, \\frac{2\\sigma^2}{n})$.\nUnder the null hypothesis $H_0$, the test statistic is:\n$$ Z = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{2\\sigma^2}{n}}} $$\nwhich follows a standard normal distribution, $Z \\sim N(0, 1)$.\n\nA Type I error occurs if we reject $H_0$ when it is true. For a two-sided test at a significance level $\\alpha$, we reject $H_0$ if $|Z|  z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the upper $(1-\\alpha/2)$-quantile of the standard normal distribution. This defines the rejection region in terms of the observed sample difference as $|\\bar{X}_1 - \\bar{X}_2|  z_{1-\\alpha/2}\\sqrt{\\frac{2\\sigma^2}{n}}$.\n\nA Type II error occurs if we fail to reject $H_0$ when it is false. The probability of a Type II error is $\\beta$. Statistical power, $1-\\beta$, is the probability of correctly rejecting $H_0$ when the alternative hypothesis $H_a$ is true. We calculate power for a specific alternative, namely that the true difference is the clinically meaningful difference $\\delta$. So, we assume $\\mu_1 - \\mu_2 = \\delta$.\n\nUnder this alternative, the sampling distribution is $\\bar{X}_1 - \\bar{X}_2 \\sim N(\\delta, \\frac{2\\sigma^2}{n})$. The power is the probability that the observed difference falls in the rejection region, given $\\mu_1 - \\mu_2 = \\delta$:\n$$ 1 - \\beta = P\\left(|\\bar{X}_1 - \\bar{X}_2|  z_{1-\\alpha/2}\\sqrt{\\frac{2\\sigma^2}{n}} \\;\\middle|\\; \\mu_1 - \\mu_2 = \\delta\\right) $$\nAssuming $\\delta  0$, the power is dominated by detections in the upper tail of the rejection region. The power can be expressed as:\n$$ 1 - \\beta = P\\left(\\bar{X}_1 - \\bar{X}_2  z_{1-\\alpha/2}\\sqrt{\\frac{2\\sigma^2}{n}}\\right) + P\\left(\\bar{X}_1 - \\bar{X}_2  -z_{1-\\alpha/2}\\sqrt{\\frac{2\\sigma^2}{n}}\\right) $$\nTo evaluate this probability under $H_a$, we standardize using the mean $\\delta$:\n$$ Z' = \\frac{(\\bar{X}_1 - \\bar{X}_2) - \\delta}{\\sqrt{\\frac{2\\sigma^2}{n}}} \\sim N(0, 1) $$\nRewriting the inequalities in terms of $Z'$:\n$$ 1 - \\beta = P\\left(Z'  z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^2/n}}\\right) + P\\left(Z'  -z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^2/n}}\\right) $$\nFor typical study designs, the second term is negligible because $-z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^2/n}}$ is a large negative number, making the probability extremely small. We thus proceed using the standard approximation:\n$$ 1 - \\beta \\approx P\\left(Z'  z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^2/n}}\\right) $$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function of the standard normal distribution. Then $1-\\beta \\approx 1 - \\Phi\\left(z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^2/n}}\\right)$, which implies $\\beta \\approx \\Phi\\left(z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^2/n}}\\right)$.\nBy definition of normal quantiles, $z_\\beta = \\Phi^{-1}(\\beta)$. Thus:\n$$ z_\\beta = z_{1-\\alpha/2} - \\frac{\\delta}{\\sqrt{2\\sigma^2/n}} $$\nSince the standard normal distribution is symmetric about $0$, $z_\\beta = -z_{1-\\beta}$. Substituting this gives:\n$$ -z_{1-\\beta} = z_{1-\\alpha/2} - \\frac{\\delta\\sqrt{n}}{\\sigma\\sqrt{2}} $$\nRearranging to solve for the sample size $n$:\n$$ \\frac{\\delta\\sqrt{n}}{\\sigma\\sqrt{2}} = z_{1-\\alpha/2} + z_{1-\\beta} $$\n$$ \\sqrt{n} = \\frac{\\sigma\\sqrt{2}(z_{1-\\alpha/2} + z_{1-\\beta})}{\\delta} $$\nSquaring both sides yields the desired expression for the per-arm sample size:\n$$ n = \\frac{2\\sigma^2(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nNow, we compute the minimal integer sample size for the given parameters:\nType I error rate $\\alpha = 0.05$.\nStatistical power $1 - \\beta = 0.80$, so the Type II error rate is $\\beta = 0.20$.\nCommon standard deviation $\\sigma = 10$ mmHg.\nClinically meaningful difference $\\delta = 4$ mmHg.\n\nFirst, we find the required quantiles from the standard normal distribution:\nFor a two-sided test with $\\alpha = 0.05$, we need the upper $1 - \\alpha/2 = 1 - 0.025 = 0.975$ quantile:\n$$ z_{1-\\alpha/2} = z_{0.975} \\approx 1.960 $$\nFor a power of $0.80$, we need the upper $1 - \\beta = 0.80$ quantile:\n$$ z_{1-\\beta} = z_{0.80} \\approx 0.842 $$\nSubstituting these values into the derived formula for $n$:\n$$ n = \\frac{2(10)^2(1.960 + 0.842)^2}{(4)^2} $$\n$$ n = \\frac{2 \\times 100 \\times (2.802)^2}{16} $$\n$$ n = \\frac{200 \\times 7.851204}{16} $$\n$$ n = 12.5 \\times 7.851204 $$\n$$ n \\approx 98.14005 $$\nSince the number of participants must be an integer, and the power must be *at least* $0.80$, we must round the calculated value of $n$ up to the next whole number. Choosing $n=98$ would result in a power slightly below the target of $0.80$. Therefore, the minimal integer number of participants required per arm is $99$.", "answer": "$$ \\boxed{99} $$", "id": "4992656"}, {"introduction": "Effective study design is a balancing act involving statistical power, available resources, and the magnitude of the effect you hope to find. This problem [@problem_id:2438719] moves beyond simple calculation to explore the critical interplay between these factors. By analyzing a common scenario in genomics research, you will develop an intuition for why underpowered studies are prone to Type II errors and how decisions about sample size directly influence the reliability of scientific conclusions.", "problem": "A computational biologist is planning a Ribonucleic Acid sequencing (RNA-seq) study comparing a candidate gene’s expression between a disease group and a control group. On the $\\log_2$ scale, prior data suggest that the gene’s expression within each group is approximately normally distributed with common standard deviation $\\sigma = 1.2$. The true difference in population means (disease minus control) on the $\\log_2$ scale is believed to be $\\delta = 1$ (that is, an expected fold change of $2$ on the original scale). The analyst will use a two-sample, two-sided test at significance level $\\alpha = 0.05$. For planning, assume the sampling distribution of the difference in sample means is well-approximated by a normal distribution and that the test controls the Type I error at level $\\alpha$ under the null hypothesis.\n\nUsing only core definitions of Type I error, Type II error, and power, reason from the sampling distribution of the mean difference to assess the following statements about how an under-powered design (very small $n$ per group) affects error rates and how increasing $n$ changes them. Select all statements that are correct.\n\nA) With $n = 3$ samples per group and the parameters above, the approximate power to detect $\\delta = 1$ at $\\alpha = 0.05$ (two-sided) is about $0.18$, implying a Type II error probability $\\beta \\approx 0.82$.\n\nB) If $\\alpha = 0.05$ is held fixed by design, then reducing $n$ increases the probability of a Type I error.\n\nC) To achieve at least $80\\%$ power for detecting $\\delta = 1$ with $\\sigma = 1.2$ at $\\alpha = 0.05$ (two-sided), a per-group sample size of about $n = 23$ suffices under the normal approximation.\n\nD) For fixed $n$ and $\\alpha$, increasing the true effect size $\\delta$ decreases the Type II error probability $\\beta$.", "solution": "Let $\\mu_D$ and $\\mu_C$ be the true mean gene expression on the $\\log_2$ scale for the disease and control populations, respectively. Let $\\bar{X}_D$ and $\\bar{X}_C$ be the sample means from groups of size $n$. The difference in sample means is $\\bar{D} = \\bar{X}_D - \\bar{X}_C$.\n\nThe null hypothesis is $H_0: \\mu_D - \\mu_C = 0$.\nThe two-sided alternative hypothesis is $H_A: \\mu_D - \\mu_C \\neq 0$.\nThe problem specifies the true difference is $\\delta = \\mu_D - \\mu_C = 1$.\n\nThe standard error of the difference in means, $SE_{\\bar{D}}$, is given by:\n$$ SE_{\\bar{D}} = \\sqrt{\\frac{\\sigma_D^2}{n} + \\frac{\\sigma_C^2}{n}} $$\nGiven a common standard deviation $\\sigma = 1.2$, this simplifies to:\n$$ SE_{\\bar{D}} = \\sqrt{\\frac{2\\sigma^2}{n}} = \\sigma\\sqrt{\\frac{2}{n}} $$\n\nUnder the null hypothesis ($H_0$), the sampling distribution of the difference in means is:\n$$ \\bar{D} | H_0 \\sim N(0, SE_{\\bar{D}}^2) $$\nUnder the alternative hypothesis ($H_A$) with true difference $\\delta$:\n$$ \\bar{D} | H_A \\sim N(\\delta, SE_{\\bar{D}}^2) $$\n\nFor a two-sided test at significance level $\\alpha$, we reject $H_0$ if the observed $|\\bar{D}|$ is greater than a critical value $c$. This critical value is determined from the null distribution:\n$$ P(|\\bar{D}|  c | H_0) = \\alpha $$\nUsing the standard normal distribution $Z \\sim N(0, 1)$, we reject $H_0$ if $|Z|  z_{\\alpha/2}$, where $Z = \\frac{\\bar{D} - 0}{SE_{\\bar{D}}}$.\nThus, the critical values for $\\bar{D}$ are $\\pm c$, where $c = z_{\\alpha/2} \\cdot SE_{\\bar{D}}$.\nFor $\\alpha = 0.05$, $\\alpha/2 = 0.025$, and the critical value from the standard normal distribution is $z_{0.025} \\approx 1.96$.\n\nThe probability of a Type II error, $\\beta$, is the probability of failing to reject $H_0$ when $H_A$ is true.\n$$ \\beta = P(-c \\le \\bar{D} \\le c | \\bar{D} \\sim N(\\delta, SE_{\\bar{D}}^2)) $$\nTo compute this probability, we standardize $\\bar{D}$ using the parameters of the alternative distribution:\n$$ \\beta = P\\left(\\frac{-c - \\delta}{SE_{\\bar{D}}} \\le \\frac{\\bar{D} - \\delta}{SE_{\\bar{D}}} \\le \\frac{c - \\delta}{SE_{\\bar{D}}}\\right) $$\nSubstituting $c = z_{\\alpha/2} \\cdot SE_{\\bar{D}}$:\n$$ \\beta = P\\left(\\frac{-z_{\\alpha/2}SE_{\\bar{D}} - \\delta}{SE_{\\bar{D}}} \\le Z \\le \\frac{z_{\\alpha/2}SE_{\\bar{D}} - \\delta}{SE_{\\bar{D}}}\\right) $$\n$$ \\beta = P\\left(-z_{\\alpha/2} - \\frac{\\delta}{SE_{\\bar{D}}} \\le Z \\le z_{\\alpha/2} - \\frac{\\delta}{SE_{\\bar{D}}}\\right) $$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution. Then:\n$$ \\beta = \\Phi\\left(z_{\\alpha/2} - \\frac{\\delta}{SE_{\\bar{D}}}\\right) - \\Phi\\left(-z_{\\alpha/2} - \\frac{\\delta}{SE_{\\bar{D}}}\\right) $$\nPower is the probability of correctly rejecting $H_0$, which is $1 - \\beta$.\n\n**Option-by-Option Analysis**\n\n**A) With $n = 3$ samples per group and the parameters above, the approximate power to detect $\\delta = 1$ at $\\alpha = 0.05$ (two-sided) is about $0.18$, implying a Type II error probability $\\beta \\approx 0.82$.**\n\nWe are given $n=3$, $\\sigma=1.2$, $\\delta=1$, and $\\alpha=0.05$ (so $z_{\\alpha/2} \\approx 1.96$).\nFirst, calculate the standard error:\n$$ SE_{\\bar{D}} = 1.2 \\sqrt{\\frac{2}{3}} \\approx 1.2 \\times 0.8165 \\approx 0.9798 $$\nNext, calculate the argument $\\delta / SE_{\\bar{D}}$:\n$$ \\frac{\\delta}{SE_{\\bar{D}}} = \\frac{1}{0.9798} \\approx 1.0206 $$\nNow, use the formula for $\\beta$:\n$$ \\beta = \\Phi(1.96 - 1.0206) - \\Phi(-1.96 - 1.0206) $$\n$$ \\beta = \\Phi(0.9394) - \\Phi(-2.9806) $$\nUsing a standard normal table or calculator:\n$$ \\Phi(0.9394) \\approx 0.8262 $$\n$$ \\Phi(-2.9806) \\approx 0.0014 $$\nSo, the probability of a Type II error is:\n$$ \\beta \\approx 0.8262 - 0.0014 = 0.8248 \\approx 0.82 $$\nThe power is $1 - \\beta$:\n$$ \\text{Power} = 1 - 0.8248 = 0.1752 \\approx 0.18 $$\nThe calculations confirm the statement. An under-powered design with $n=3$ has a very high probability ($\\approx 82\\%$) of failing to detect the true effect.\n\nVerdict: **Correct**.\n\n**B) If $\\alpha = 0.05$ is held fixed by design, then reducing $n$ increases the probability of a Type I error.**\n\nThis statement is fundamentally incorrect. The significance level $\\alpha$ *is* the probability of a Type I error (rejecting $H_0$ when it is true), which is fixed by the analyst prior to the experiment. The problem explicitly states that the test \"controls the Type I error at level $\\alpha$\". This means that for any given sample size $n$, the decision rule (i.e., the critical values $\\pm c = \\pm z_{\\alpha/2} \\sigma \\sqrt{2/n}$) is chosen precisely to ensure that the probability of a Type I error is $\\alpha$.\nReducing $n$ increases the standard error, which in turn widens the confidence intervals and decreases the power ($1-\\beta$) of the test. It increases the probability of a Type II error, not a Type I error. The probability of a Type I error remains fixed at $\\alpha=0.05$ by construction.\n\nVerdict: **Incorrect**.\n\n**C) To achieve at least $80\\%$ power for detecting $\\delta = 1$ with $\\sigma = 1.2$ at $\\alpha = 0.05$ (two-sided), a per-group sample size of about $n = 23$ suffices under the normal approximation.**\n\nWe require Power $\\ge 0.80$, which means $\\beta \\le 0.20$. The general formula for sample size for a two-sample Z-test is derived from the power equation. A common approximation for power is Power $\\approx \\Phi(\\frac{\\delta}{SE_{\\bar{D}}} - z_{\\alpha/2})$. For Power $= 0.80$, we need the argument to be $z_{0.80} \\approx 0.8416$.\nThis leads to the condition:\n$$ \\frac{\\delta}{SE_{\\bar{D}}} - z_{\\alpha/2} \\ge z_{1-\\beta} $$\n$$ \\frac{\\delta}{\\sigma\\sqrt{2/n}} \\ge z_{\\alpha/2} + z_{1-\\beta} $$\nSolving for $n$:\n$$ \\sqrt{n} \\ge \\frac{\\sigma\\sqrt{2}(z_{\\alpha/2} + z_{1-\\beta})}{\\delta} $$\n$$ n \\ge \\frac{2\\sigma^2(z_{\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nSubstituting the values $\\sigma=1.2$, $\\delta=1$, $z_{\\alpha/2} \\approx 1.96$, and $z_{0.80} \\approx 0.8416$:\n$$ n \\ge \\frac{2(1.2)^2(1.96 + 0.8416)^2}{1^2} $$\n$$ n \\ge \\frac{2(1.44)(2.8016)^2}{1} $$\n$$ n \\ge 2.88 \\times 7.84896 \\approx 22.605 $$\nSince the sample size $n$ must be an integer, the minimum required sample size per group is $n=23$.\nLet's verify the power for $n=23$.\n$$ SE_{\\bar{D}} = 1.2 \\sqrt{\\frac{2}{23}} \\approx 0.35386 $$\n$$ \\frac{\\delta}{SE_{\\bar{D}}} \\approx \\frac{1}{0.35386} \\approx 2.8259 $$\n$$ \\beta = \\Phi(1.96 - 2.8259) - \\Phi(-1.96 - 2.8259) = \\Phi(-0.8659) - \\Phi(-4.7859) $$\n$$ \\beta \\approx 0.1932 - 0.00000085 \\approx 0.1932 $$\nThe power for $n=23$ is $1 - \\beta \\approx 1 - 0.1932 = 0.8068$, which is greater than $80\\%$.\nFor $n=22$, a similar calculation yields $\\beta \\approx 0.2107$, which corresponds to a power of $\\approx 78.93\\%$, less than $80\\%$.\nThus, $n=23$ is indeed the minimum required sample size.\n\nVerdict: **Correct**.\n\n**D) For fixed $n$ and $\\alpha$, increasing the true effect size $\\delta$ decreases the Type II error probability $\\beta$.**\n\nThis statement addresses the relationship between effect size and statistical power. We hold $n$ and $\\alpha$ constant. This implies that $SE_{\\bar{D}}$ and the rejection region ($\\bar{D}  c$ or $\\bar{D}  -c$) are fixed.\nThe Type II error probability $\\beta$ is the probability that an observation from the alternative distribution, $N(\\delta, SE_{\\bar{D}}^2)$, falls into the non-rejection region $[-c, c]$.\nAs the true effect size $\\delta$ increases, the center of the alternative distribution moves further away from the center of the null distribution (which is $0$). This increased separation means that a smaller portion of the alternative distribution's probability mass will overlap with the non-rejection region defined under the null. Consequently, the probability of failing to reject the null hypothesis when it is false decreases. Thus, $\\beta$ decreases, and power ($1-\\beta$) increases.\nMathematically, in the formula for $\\beta$, as $\\delta  0$ increases, the term $\\delta/SE_{\\bar{D}}$ increases. The interval for the standard normal integral, $[-z_{\\alpha/2} - \\delta/SE_{\\bar{D}}, z_{\\alpha/2} - \\delta/SE_{\\bar{D}}]$, shifts to the left (towards more negative values), and the area under the standard normal curve over this interval decreases.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ACD}$$", "id": "2438719"}, {"introduction": "A practitioner's most vital skill is the critical appraisal of scientific literature. This exercise [@problem_id:4589525] presents a common dilemma in preventive medicine: a small pilot study reports a statistically significant finding ($p \\lt 0.05$). You will learn to look beyond the $p$-value and evaluate the result in the context of the study's low statistical power, confronting issues like effect size inflation (the \"winner's curse\") and the crucial difference between statistical significance and practical importance.", "problem": "A public health department evaluates a smartphone reminder application designed to prevent missed childhood measles vaccinations among postpartum individuals. In a randomized controlled trial (RCT), participants are assigned to a control group without reminders or an intervention group with reminders, with $n_C = 30$ and $n_I = 30$. The primary outcome is the proportion who miss the measles vaccination within $6$ months. Based on prior surveillance, the expected control miss rate is $p_C = 0.30$, and the department deems a reduction to $p_I = 0.15$ (an absolute difference $\\Delta = 0.15$) as clinically meaningful. A two-sided hypothesis test at significance level $\\alpha = 0.05$ is prespecified. In the observed trial, $12$ of $30$ in the control arm missed the vaccination ($\\hat{p}_C = 0.40$) and $5$ of $30$ in the intervention arm missed it ($\\hat{p}_I \\approx 0.167$), yielding a reported small $p$-value ($p \\approx 0.038$) for the difference.\n\nUsing the core definitions of Type I error, Type II error, statistical power, and the meaning of the $p$-value, and without assuming any shortcut formulas beyond widely used approximations for two-sample proportions, which of the following interpretations are valid in preventive medicine when a small $p$-value is obtained from a study that had low power for the prespecified clinically meaningful effect?\n\nA. Because $p \\approx 0.038  \\alpha$, the probability that the null hypothesis is true is $3.8\\%$, regardless of prior evidence.\n\nB. When statistical power is low for the prespecified effect (for example, on the order of $30\\%$), and the pre-study probability of a truly effective intervention is low, a larger fraction of statistically significant findings can be Type I errors; thus, replication and careful pre-registration are essential before scaling the program.\n\nC. Low power mechanically inflates the Type I error rate above $\\alpha$, so the chance of a false positive result is greater than $5\\%$ by design.\n\nD. In low-power settings, statistically significant estimates tend to overstate the true effect size and can be fragile to reasonable analytic variations; therefore, this result should be interpreted cautiously and verified in larger studies.\n\nE. Observing a small $p$-value implies the test had high power because $1 - p$ equals the power of the study.", "solution": "The task is to evaluate the validity of several interpretations of a statistically significant finding ($p  0.05$) from a study that was underpowered to detect a prespecified clinically meaningful effect. First, we must clearly define the core concepts.\n\n-   **Type I Error**: The error of rejecting the null hypothesis ($H_0$) when it is in fact true. The probability of committing a Type I error is denoted by $\\alpha$, the significance level, which was prespecified as $0.05$ in this study.\n-   **Type II Error**: The error of failing to reject the null hypothesis when it is in fact false. The probability of committing a Type II error is denoted by $\\beta$.\n-   **Statistical Power**: The probability of correctly rejecting the null hypothesis when it is false. Power is equal to $1 - \\beta$. It is a function of the significance level ($\\alpha$), sample size ($n$), and the true effect size ($\\Delta$). A \"low-power\" study has a high probability of a Type II error.\n-   **$p$-value**: The probability, under the assumption that the null hypothesis is true, of obtaining a result at least as extreme as the observed result. It is a measure of evidence against $H_0$.\n\nNow we evaluate each option.\n\n**A. Because $p \\approx 0.038  \\alpha$, the probability that the null hypothesis is true is $3.8\\%$, regardless of prior evidence.**\n\nThis statement is a well-known logical fallacy, often called \"transposing the conditional\". The $p$-value is the probability of the observed data (or more extreme data) given that the null hypothesis is true, denoted $P(\\text{Data}|H_0)$. The statement incorrectly claims the $p$-value is the probability of the null hypothesis being true given the data, $P(H_0|\\text{Data})$. These two probabilities are not equal. To calculate $P(H_0|\\text{Data})$, one would need to use Bayes' theorem, which requires a \"prior probability\" of the null hypothesis being true, $P(H_0)$. The $p$-value does not and cannot provide this information.\n\n**Verdict: Incorrect.**\n\n**B. When statistical power is low for the prespecified effect (for example, on the order of $30\\%$), and the pre-study probability of a truly effective intervention is low, a larger fraction of statistically significant findings can be Type I errors; thus, replication and careful pre-registration are essential before scaling the program.**\n\nThis statement correctly describes the concept of the Positive Predictive Value (PPV) of a significant test result. The PPV is the post-study probability that a significant finding represents a true effect. An illustrative formula, derived from Bayesian principles, is:\n$$ \\text{PPV} = \\frac{(\\text{Power}) \\times (\\text{Pre-study Odds})}{(\\text{Power}) \\times (\\text{Pre-study Odds}) + \\alpha} $$\nwhere the Pre-study Odds are $P(H_a \\text{ is true}) / P(H_0 \\text{ is true})$. If power is low (e.g., $1-\\beta = 0.30$) and the pre-study odds of a true effect are low (meaning most new interventions tested are ineffective), the denominator is not much larger than the $\\alpha$ term. This results in a low PPV, meaning a high fraction, $1 - \\text{PPV}$, of \"significant\" findings are actually false positives (Type I errors). For example, if power is $0.30$, $\\alpha = 0.05$, and the pre-study probability of a true effect is $10\\%$, the PPV is approximately $40\\%$. This means $60\\%$ of significant findings in this context would be false positives. The conclusion that replication and pre-registration are essential is a sound scientific response to this statistical reality.\n\n**Verdict: Correct.**\n\n**C. Low power mechanically inflates the Type I error rate above $\\alpha$, so the chance of a false positive result is greater than $5\\%$ by design.**\n\nThis is a fundamental misunderstanding of statistical-test definitions. The Type I error rate, $\\alpha$, is a parameter of the test that is fixed by the researcher *before* the experiment. In this case, it was set to $\\alpha = 0.05$. This rate is the long-run frequency of rejecting a true null hypothesis, and it is not affected by the study's power. Low power means a high Type II error rate, $\\beta = 1 - \\text{Power}$. Low power and a high Type I error rate are not synonymous. The statement confuses the conditional probability $\\alpha = P(\\text{Reject } H_0 | H_0 \\text{ is true})$ with the post-study probability $P(H_0 \\text{ is true } | \\text{Reject } H_0)$, which, as discussed in option B, can be much higher than $\\alpha$ in low-power settings. However, the *rate* $\\alpha$ is fixed by design.\n\n**Verdict: Incorrect.**\n\n**D. In low-power settings, statistically significant estimates tend to overstate the true effect size and can be fragile to reasonable analytic variations; therefore, this result should be interpreted cautiously and verified in larger studies.**\n\nThis statement correctly describes a phenomenon known as the \"winner's curse\" or \"effect size inflation\". For a study to achieve statistical significance, the observed effect must cross a certain threshold. In a low-power study, this threshold is far from the null. If the true effect is modest (like the specified $\\Delta = 0.15$), the only way to obtain a significant result is if random sampling variation happens to produce an observed effect that is much larger than the true effect. In this problem, the prespecified clinically meaningful effect was $\\Delta = 0.15$, but the observed effect was $\\hat{\\Delta} = 0.40 - 0.167 = 0.233$, which is over $50\\%$ larger. This is a classic example of this phenomenon. Consequently, such findings are often \"fragile\" (not robust) and the magnitude of the effect should not be taken at face value. The call for caution and verification in larger, more adequately powered studies is the appropriate scientific conclusion.\n\n**Verdict: Correct.**\n\n**E. Observing a small $p$-value implies the test had high power because $1 - p$ equals the power of the study.**\n\nThis statement is nonsensical. There is no mathematical identity or theoretical link stating that Power $= 1 - p$. Power ($1-\\beta$) is a pre-study characteristic of the test, calculated for a specific effect size, and is the probability of correctly detecting a true effect. The $p$-value is a post-study measure of data compatibility with the null hypothesis. A small $p$-value can arise in a low-power study purely by chance (i.e., if the observed effect is large due to sampling variability), as is the case in this very problem. The statement is a complete misrepresentation of statistical principles.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BD}$$", "id": "4589525"}]}