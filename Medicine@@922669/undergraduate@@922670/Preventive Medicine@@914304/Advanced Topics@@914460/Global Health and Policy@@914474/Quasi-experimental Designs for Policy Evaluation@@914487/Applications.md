## Applications and Interdisciplinary Connections

Having established the theoretical principles and statistical mechanics of quasi-experimental designs in the preceding chapters, we now turn our attention to their application. The true value of these methods lies not in their mathematical elegance, but in their capacity to generate credible causal evidence about the real-world effects of policies and programs, particularly when randomized controlled trials (RCTs) are infeasible, unethical, or impractical. This chapter will explore the diverse applications of quasi-experimental designs across various domains of public health and preventive medicine. We will demonstrate how these methods are used to evaluate complex interventions, analyze heterogeneous effects, and inform economic evaluations. By examining a series of applied problems, we will illustrate how the core principles of Difference-in-Differences (DiD), Interrupted Time Series (ITS), Regression Discontinuity (RD), and Synthetic Control Methods (SCM) are operationalized to answer pressing questions in [policy evaluation](@entry_id:136637).

### Core Applications in Public Health Policy Evaluation

Quasi-experimental designs are the methodological workhorses for evaluating the impact of large-scale health policies. Whether assessing a sudden regulatory change or a phased program rollout, these methods provide a framework for constructing a plausible counterfactual—what would have happened in the absence of the policy.

#### Evaluating Sudden, System-Wide Policy Changes with Interrupted Time Series

Many policies, such as new regulations or mandates, are implemented at a single, clearly defined point in time and apply to an entire population. In such cases, a control group may not be readily available. The Interrupted Time Series (ITS) design is exceptionally well-suited for this scenario. By collecting data on an outcome at multiple, equally spaced time points before and after the intervention, ITS uses the pre-intervention trend to project the counterfactual trajectory into the post-intervention period. The policy's impact is then estimated as the "interruption"—a change in the level and/or slope of the outcome trend—that occurs immediately following the intervention.

A classic application is the evaluation of a state mandate requiring clinicians to consult a Prescription Drug Monitoring Program (PDMP) before prescribing opioids. An analyst with access to monthly data on opioid prescription rates for several years before and after the mandate can use a segmented [regression model](@entry_id:163386) to estimate the immediate drop in prescribing (a level change) and any subsequent change in the long-term trend (a slope change) attributable to the policy. The validity of this approach rests on the crucial assumption that no other concurrent event could explain the observed break in the series. Adjustments for seasonality and autocorrelation are also essential for robust inference [@problem_id:4554045].

The power of ITS is enhanced when a policy is rolled out across different regions at different times. This creates a **multiple-baseline ITS design**, which strengthens causal inference. By observing that the outcome trend changes in each region only after the policy is introduced there, we can be more confident that the effect is due to the policy and not some other confounding event. For example, in evaluating a smoke-free housing policy implemented by housing authorities in multiple regions at staggered times, one can first conduct a separate ITS analysis for each region. These region-specific effect estimates can then be pooled using meta-analytic techniques, such as inverse-variance weighting. A random-effects [meta-analysis](@entry_id:263874) is often preferred as it accounts for the plausible scenario where the policy's impact (treatment effect heterogeneity) varies across regions. This two-stage approach provides a robust summary of the policy's average impact while respecting the unique context of each implementation [@problem_id:4566500].

#### Analyzing Staggered Policies with Difference-in-Differences and Synthetic Controls

Perhaps the most common implementation strategy for new programs is a staggered or phased rollout, where different administrative units (e.g., counties, hospitals, districts) adopt the policy at different times. This design structure is a natural fit for Difference-in-Differences (DiD) and related panel data methods. The core idea is to use not-yet-treated units as a control group for units that have just become treated.

The evaluation of health information technology policies, such as the Promoting Interoperability (PI) program, provides a clear context for comparing the major quasi-experimental designs. A simple DiD compares the change in an outcome (e.g., electronic exchange of care summaries) for a treated group of hospitals against the change for an untreated group. ITS, as noted, is ideal for a system-wide rollout where a control group is absent. The Synthetic Control Method (SCM) offers a powerful alternative for case studies, where a single treated unit (or a small number of units) is compared to a "synthetic" counterfactual constructed from a weighted average of untreated donor units. Each design relies on a different core assumption to ensure its validity: DiD on parallel trends, ITS on the continuity of the pre-intervention trend, and SCM on the ability of the donor pool to reproduce the pre-intervention path of the treated unit [@problem_id:4842159].

A critical challenge in modern econometrics is that the standard Two-Way Fixed Effects (TWFE) DiD estimator, implemented as a single regression with unit and time fixed effects, can produce biased estimates in [staggered adoption](@entry_id:636813) settings if the treatment effects are heterogeneous (i.e., they vary across adoption cohorts or over time since adoption). This bias arises because the TWFE model implicitly uses already-treated units as "controls" for later-adopting units. If the effect of the treatment on these early adopters is itself changing over time, they provide a contaminated counterfactual. This can lead to "negative weighting," where some true causal effects contribute negatively to the overall estimate, potentially leading to a final estimate that is biased, misleading, or even has the wrong sign [@problem_id:4566434].

To address this, a new generation of robust estimators has been developed. These methods, such as those proposed by Callaway and Sant'Anna or Sun and Abraham, explicitly avoid using already-treated units as controls. They work by estimating group-[time average](@entry_id:151381) treatment effects (the effect on a specific cohort at a specific time) against a "clean" control group of not-yet-treated units. These granular effects can then be aggregated into policy-relevant summary measures. When evaluating a staggered rollout of a [colorectal cancer](@entry_id:264919) screening program, these modern DiD estimators or the Synthetic Control Method (which naturally uses only untreated donors) are the most appropriate choices to estimate dynamic treatment effects credibly [@problem_id:4566514] [@problem_id:4566434]. A comprehensive evaluation of a policy like abolishing user fees in a low-income country would likewise rely on these robust methods, supplemented by a battery of diagnostic checks such as testing for pre-trends and conducting placebo tests [@problem_id:4991698].

### Advanced Topics and Extensions in Policy Evaluation

Beyond estimating a single average effect, real-world [policy evaluation](@entry_id:136637) demands a more nuanced understanding of how, for whom, and under what conditions a policy works. This requires moving beyond basic models to explore heterogeneity, implementation fidelity, and dose-response relationships.

#### Analyzing Heterogeneity and Equity

A policy's impact is rarely uniform across the population. **Effect modification** (or heterogeneous treatment effects) occurs when the causal effect of an intervention varies across subgroups defined by characteristics like age, income, or health status. Identifying such heterogeneity is often a primary goal of evaluation, especially when assessing a policy's impact on health equity.

In a DiD framework, effect modification can be assessed by including interaction terms in the regression model. For instance, to evaluate if a workplace vaccination mandate has a different effect on influenza hospitalizations for older individuals or those with comorbidities, one can fit a TWFE model that includes three-way [interaction terms](@entry_id:637283) between the policy indicator, age, and comorbidity status. The coefficients on these [interaction terms](@entry_id:637283) directly estimate how the treatment effect is modified by these characteristics. A statistically significant [interaction term](@entry_id:166280) suggests that the policy's benefit is not uniform and may be larger or smaller for certain vulnerable groups [@problem_id:4566451].

At a more basic level, this involves calculating subgroup-specific DiD estimates. In an evaluation of a front-of-pack nutrition labeling policy, an analyst could compute the DiD estimate for urban households and rural households separately. A finding that the policy increased the purchase of healthy foods by $11$ percentage points in urban areas but only $7$ percentage points in rural areas would reveal important heterogeneity, suggesting the policy's effectiveness may depend on the local context [@problem_id:4566465].

A truly equity-focused evaluation goes even further. Consider an equity-focused zoning ordinance designed to attract grocery stores to underserved food deserts. A comprehensive evaluation would not only use a robust DiD design but would also employ a **triple-differences** estimator to formally compare the DiD effect between low-income and higher-income residents, or across racial groups. Moreover, it would analyze not just changes in the average diet quality but also distributional effects, such as whether the policy disproportionately benefits those with the poorest diets at baseline. This requires a sophisticated approach linking GIS-based access metrics, validated dietary measures (like the Healthy Eating Index), and clinical outcomes from electronic health records, while also addressing complex threats to inference such as gentrification-related displacement [@problem_id:4533741].

#### Incorporating Dose-Response and Implementation Fidelity

Policies are often not simple binary (on/off) interventions. Their impact may depend on the intensity of implementation, or the "dose" delivered. For example, the effectiveness of a national smoke-free law may depend on the rigor of local enforcement. To evaluate this, one can construct a continuous **enforcement intensity index** from multiple process indicators (e.g., inspections per venue, citations issued, enforcement budget). This index can then be incorporated into a continuous dose-response DiD model. Such a model, which includes city and time fixed effects, can estimate how the change in health outcomes (like pediatric asthma visits) from pre- to post-law varies with the level of enforcement intensity across different cities. This approach allows policymakers to understand not just *if* the law worked, but *how much* enforcement is needed to achieve a desired public health benefit [@problem_id:4566450].

This concept is closely related to **implementation fidelity**, which measures the degree to which a program is delivered as intended. Fidelity can vary across implementation sites and over time, influencing the observed outcomes. For a mailed colorectal cancer screening program rolled out in a stepped-wedge design, fidelity might be measured as the proportion of eligible patients who are successfully mailed a test kit and who then complete and return it. When evaluating the policy, it is crucial to model this [dose-response relationship](@entry_id:190870).

Furthermore, variation in fidelity has profound implications for **external validity**, or the generalizability of findings. An effect estimated in a study with high fidelity may not be achievable when the policy is scaled up statewide, where fidelity might be lower on average. Advanced methods can be used to transport the findings by modeling the effect as a function of fidelity and then re-weighting or standardizing this function to the anticipated fidelity distribution in the target population. This allows for a more realistic forecast of a policy's likely impact at scale [@problem_id:4566505].

### Interdisciplinary Connections

Quasi-experimental methods do not exist in a vacuum. They are a critical tool in a larger evidence-to-policy pipeline, forming essential connections with disciplines like health economics and informing the very philosophy of evidence-based practice.

#### Bridging Causal Inference and Health Economics

The outputs of quasi-experimental studies—estimates of a policy's causal effect on health outcomes—are crucial inputs for economic evaluations. Health economics seeks to determine whether the health benefits of an intervention justify its costs, guiding efficient resource allocation.

For example, after using a DiD analysis to estimate that a vaccination mandate averted $200$ hospitalizations in a population of $2.5$ million, a health department can proceed to a cost-effectiveness analysis. By summing the policy's implementation and enforcement costs with the incremental costs of vaccine procurement and administration, one can calculate the total incremental cost. The **Incremental Cost-Effectiveness Ratio (ICER)** is then computed as the incremental cost divided by the health effect (e.g., hospitalizations averted). An ICER of, say, $\$22,500$ per hospitalization averted provides a standardized metric that can be compared against a pre-determined willingness-to-pay threshold to decide if the policy represents good value for money [@problem_id:4566455].

Similarly, the **Net Monetary Benefit (NMB)** framework can be applied. A DiD analysis might find that a smoke-free law improved average health-related quality of life by $0.03$ Quality-Adjusted Life Years (QALYs) per person. This health gain is monetized by multiplying it by a societal willingness-to-pay threshold (e.g., $\$75,000$ per QALY). The total incremental costs of the policy, including both implementation costs and any healthcare cost savings, are then subtracted. A positive NMB indicates that the policy is cost-effective at the given threshold. This direct connection allows the rigorous causal estimates from quasi-experimental designs to be translated into the language of economic efficiency and value, directly informing policy decisions [@problem_id:4566494].

#### Evidence Synthesis and the Philosophy of Causal Inference

While this textbook focuses on quasi-experiments, it is vital to understand their place within the broader ecosystem of evidence. The conventional **hierarchy of evidence** for causal claims typically places systematic reviews and meta-analyses of RCTs at the apex, followed by single RCTs, then well-conducted quasi-experiments, and finally other observational studies. This hierarchy reflects a preference for study designs that are less susceptible to confounding. Explanatory RCTs are optimized for **internal validity** to determine a treatment's **efficacy** under ideal conditions. Pragmatic RCTs and quasi-experiments, on the other hand, are often better suited for assessing a policy's **effectiveness** and **external validity** in real-world settings [@problem_id:4525677]. For many critical public health questions, such as the impact of an excise tax, an RCT is simply not possible, making a well-conducted quasi-experiment the highest level of evidence attainable [@problem_id:4525677].

Ultimately, making a causal claim in the absence of randomization requires a high epistemic standard. It is not enough to find a statistically significant association in a single study. Instead, confidence in a causal claim is built through the **triangulation** of evidence. When multiple strong quasi-experimental designs, resting on different identifying assumptions—such as Regression Discontinuity and Difference-in-Differences—produce concordant results, our confidence grows. This confidence is further bolstered by a portfolio of supporting evidence: the absence of an effect in [falsification](@entry_id:260896) tests and on negative control outcomes, the presence of a plausible mechanism, and evidence of a [dose-response relationship](@entry_id:190870). When this body of evidence converges to suggest a policy is effective, it provides a robust foundation for action. Decision-making under uncertainty can be formalized by integrating this evidence into a decision-analytic framework, weighing the expected benefits (which may be equity-weighted) against the opportunity costs. In this way, a rigorous and multifaceted quasi-experimental evaluation program can provide the justification needed to implement evidence-based policies that improve public health [@problem_id:4575867].