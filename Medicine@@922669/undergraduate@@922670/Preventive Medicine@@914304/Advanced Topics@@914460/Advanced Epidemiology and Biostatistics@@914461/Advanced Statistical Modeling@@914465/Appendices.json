{"hands_on_practices": [{"introduction": "In many preventive medicine studies, data has a natural hierarchical structure, such as patients grouped within clinics. This clustering means that observations are not independent, and this exercise explores the statistical consequences. By deriving the variance of a clinic's mean outcome in a random-intercept model, you will see precisely how the within-clinic sample size and the intraclass correlation coefficient (ICC) combine to determine the precision of our estimates [@problem_id:4502153].", "problem": "A multicenter preventive medicine study evaluates a new hypertension prevention program delivered in primary care clinics. Let $j$ index clinics ($j=1,\\dots,J$) and let $i$ index patients within clinic $j$ ($i=1,\\dots,n_j$). At $6$ months, each patient’s systolic blood pressure is recorded as a continuous outcome $y_{ij}$. Suppose the data are adequately described by a random-intercept model,\n$$\ny_{ij} = \\beta_{0} + u_{j} + \\varepsilon_{ij},\n$$\nwhere $u_{j}$ are clinic-level random intercepts and $\\varepsilon_{ij}$ are patient-level errors. Assume $u_{j} \\sim \\mathcal{N}(0,\\sigma_{u}^{2})$, $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma_{e}^{2})$, and that $\\{u_{j}\\}$ are independent across clinics, $\\{\\varepsilon_{ij}\\}$ are independent across patients, and $\\{u_{j}\\}$ are independent of $\\{\\varepsilon_{ij}\\}$. Let the clinic mean be\n$$\n\\bar{y}_{j} = \\frac{1}{n_{j}} \\sum_{i=1}^{n_{j}} y_{ij}.\n$$\nUsing only the core properties of expectations and variances under independence and the definition of the Intraclass Correlation Coefficient (ICC), derive the unconditional variance $\\operatorname{Var}(\\bar{y}_{j})$ and express it in terms of the total variance $\\sigma_{t}^{2} = \\sigma_{u}^{2} + \\sigma_{e}^{2}$, the Intraclass Correlation Coefficient (ICC) defined as $\\rho = \\frac{\\sigma_{u}^{2}}{\\sigma_{t}^{2}}$, and the within-clinic sample size $n_{j}$. Then, for a clinic with $n_{j} = 25$, total variance $\\sigma_{t}^{2} = 144$, and ICC $\\rho = 0.12$, compute the numerical value of $\\operatorname{Var}(\\bar{y}_{j})$. Express the variance in $\\mathrm{mmHg^{2}}$ and round your final numerical answer to four significant figures.", "solution": "The first step is to derive the analytical expression for the variance of the clinic mean, $\\operatorname{Var}(\\bar{y}_{j})$. The clinic mean $\\bar{y}_{j}$ is defined as:\n$$\n\\bar{y}_{j} = \\frac{1}{n_{j}} \\sum_{i=1}^{n_{j}} y_{ij}\n$$\nWe substitute the model equation $y_{ij} = \\beta_{0} + u_{j} + \\varepsilon_{ij}$ into this definition:\n$$\n\\bar{y}_{j} = \\frac{1}{n_{j}} \\sum_{i=1}^{n_{j}} (\\beta_{0} + u_{j} + \\varepsilon_{ij})\n$$\nThe terms $\\beta_{0}$ and $u_{j}$ are constant with respect to the index $i$. We can therefore simplify the summation:\n$$\n\\bar{y}_{j} = \\frac{1}{n_{j}} \\left( \\sum_{i=1}^{n_{j}} \\beta_{0} + \\sum_{i=1}^{n_{j}} u_{j} + \\sum_{i=1}^{n_{j}} \\varepsilon_{ij} \\right) = \\frac{1}{n_{j}} \\left( n_{j}\\beta_{0} + n_{j}u_{j} + \\sum_{i=1}^{n_{j}} \\varepsilon_{ij} \\right)\n$$\n$$\n\\bar{y}_{j} = \\beta_{0} + u_{j} + \\frac{1}{n_{j}} \\sum_{i=1}^{n_{j}} \\varepsilon_{ij}\n$$\nLet's denote the mean of the patient-level errors within a clinic as $\\bar{\\varepsilon}_{j} = \\frac{1}{n_{j}} \\sum_{i=1}^{n_{j}} \\varepsilon_{ij}$. The expression for the clinic mean becomes:\n$$\n\\bar{y}_{j} = \\beta_{0} + u_{j} + \\bar{\\varepsilon}_{j}\n$$\nWe now compute the variance of $\\bar{y}_{j}$. The term $\\beta_{0}$ is a fixed parameter, a constant, so it does not contribute to the variance.\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = \\operatorname{Var}(\\beta_{0} + u_{j} + \\bar{\\varepsilon}_{j}) = \\operatorname{Var}(u_{j} + \\bar{\\varepsilon}_{j})\n$$\nThe problem states that the random intercepts $\\{u_{j}\\}$ are independent of the patient-level errors $\\{\\varepsilon_{ij}\\}$. It follows that $u_{j}$ is independent of $\\bar{\\varepsilon}_{j}$, which is a function of the $\\{\\varepsilon_{ij}\\}$. For independent random variables, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = \\operatorname{Var}(u_{j}) + \\operatorname{Var}(\\bar{\\varepsilon}_{j})\n$$\nFrom the problem statement, we know $\\operatorname{Var}(u_{j}) = \\sigma_{u}^{2}$. We now need to find $\\operatorname{Var}(\\bar{\\varepsilon}_{j})$:\n$$\n\\operatorname{Var}(\\bar{\\varepsilon}_{j}) = \\operatorname{Var}\\left(\\frac{1}{n_{j}} \\sum_{i=1}^{n_{j}} \\varepsilon_{ij}\\right)\n$$\nUsing the property $\\operatorname{Var}(aX) = a^2\\operatorname{Var}(X)$ with $a = 1/n_j$:\n$$\n\\operatorname{Var}(\\bar{\\varepsilon}_{j}) = \\frac{1}{n_{j}^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n_{j}} \\varepsilon_{ij}\\right)\n$$\nThe problem states that the errors $\\{\\varepsilon_{ij}\\}$ are independent across patients. Therefore, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n_{j}} \\varepsilon_{ij}\\right) = \\sum_{i=1}^{n_{j}} \\operatorname{Var}(\\varepsilon_{ij})\n$$\nWe are given that $\\operatorname{Var}(\\varepsilon_{ij}) = \\sigma_{e}^{2}$ for all $i$. Thus:\n$$\n\\sum_{i=1}^{n_{j}} \\operatorname{Var}(\\varepsilon_{ij}) = \\sum_{i=1}^{n_{j}} \\sigma_{e}^{2} = n_{j}\\sigma_{e}^{2}\n$$\nSubstituting this back, we find the variance of the mean error:\n$$\n\\operatorname{Var}(\\bar{\\varepsilon}_{j}) = \\frac{1}{n_{j}^2} (n_{j}\\sigma_{e}^{2}) = \\frac{\\sigma_{e}^{2}}{n_j}\n$$\nCombining the components, we get the variance of the clinic mean in terms of the variance components:\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = \\sigma_{u}^{2} + \\frac{\\sigma_{e}^{2}}{n_j}\n$$\nThe next step is to express this result in terms of the total variance $\\sigma_{t}^{2}$, the ICC $\\rho$, and the sample size $n_j$. The given definitions are $\\sigma_{t}^{2} = \\sigma_{u}^{2} + \\sigma_{e}^{2}$ and $\\rho = \\frac{\\sigma_{u}^{2}}{\\sigma_{t}^{2}}$.\nFrom these definitions, we can express $\\sigma_{u}^{2}$ and $\\sigma_{e}^{2}$:\n$$\n\\sigma_{u}^{2} = \\rho \\sigma_{t}^{2}\n$$\n$$\n\\sigma_{e}^{2} = \\sigma_{t}^{2} - \\sigma_{u}^{2} = \\sigma_{t}^{2} - \\rho \\sigma_{t}^{2} = \\sigma_{t}^{2}(1-\\rho)\n$$\nNow, substitute these into our expression for $\\operatorname{Var}(\\bar{y}_{j})$:\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = (\\rho \\sigma_{t}^{2}) + \\frac{\\sigma_{t}^{2}(1-\\rho)}{n_j}\n$$\nFactoring out $\\sigma_{t}^{2}$ gives:\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = \\sigma_{t}^{2} \\left( \\rho + \\frac{1-\\rho}{n_j} \\right)\n$$\nThis can be written in a more standard form by finding a common denominator:\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = \\sigma_{t}^{2} \\left( \\frac{n_j\\rho + 1 - \\rho}{n_j} \\right) = \\frac{\\sigma_{t}^{2}}{n_j} [1 + (n_j-1)\\rho]\n$$\nThis is the desired analytical expression for $\\operatorname{Var}(\\bar{y}_{j})$.\n\nFinally, we compute the numerical value using the provided data: $n_{j} = 25$, $\\sigma_{t}^{2} = 144$, and $\\rho = 0.12$.\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = \\frac{144}{25} [1 + (25-1)(0.12)]\n$$\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = 5.76 [1 + (24)(0.12)]\n$$\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = 5.76 [1 + 2.88]\n$$\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = 5.76 [3.88]\n$$\n$$\n\\operatorname{Var}(\\bar{y}_{j}) = 22.3488\n$$\nThe problem specifies the unit of variance as $\\mathrm{mmHg^{2}}$ and asks to round the final numerical answer to four significant figures.\n$$\n\\operatorname{Var}(\\bar{y}_{j}) \\approx 22.35 \\ \\mathrm{mmHg^{2}}\n$$", "answer": "$$\n\\boxed{22.35}\n$$", "id": "4502153"}, {"introduction": "Having established that group-level variation exists, multilevel models allow us to quantify it through random effects. This hands-on coding practice demonstrates how to estimate these clinic-specific effects from the data. You will implement the formula for the Best Linear Unbiased Predictor (BLUP) and, through different test cases, gain an intuitive understanding of \"shrinkage\"—how the model judiciously borrows information across groups to produce more stable and reliable estimates, especially for groups with little data [@problem_id:4502179].", "problem": "Consider a hierarchical random-intercept model appropriate for preventive medicine outcomes measured within clinics. Let $j$ index clinics and $i$ index individuals within clinic $j$. The outcome for individual $i$ in clinic $j$ is modeled as $Y_{ij} = \\beta_0 + u_j + \\epsilon_{ij}$, where $u_j$ is the clinic-specific random intercept and $\\epsilon_{ij}$ is the individual-level error term. Assume $u_j \\sim \\mathcal{N}(0,\\sigma_u^2)$ and $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$, with all random variables mutually independent across clinics and individuals. You are given realizations of $Y_{ij}$ and known variance components $\\sigma_u^2$ and $\\sigma_\\epsilon^2$.\n\nStarting only from the definitions of the Gaussian hierarchical model and the rules of conditional expectations for jointly Gaussian random variables, derive how to compute the estimated clinic random intercepts as conditional expectations given the observed data and the variance components. Then, implement this computation in a program that, for each test case below, uses the grand mean across all individuals to estimate the fixed intercept $\\beta_0$, and produces the estimated clinic random intercepts. Your program must output, for each test case, a list of floats representing the estimated clinic random intercepts for all clinics in the order they are listed. Each float must be rounded to $6$ decimals. The final program output must be a single line containing a list of lists, in the same order as the test cases are presented, enclosed in square brackets. For example, an output of the form $[[a_1,a_2],[b_1,b_2,b_3]]$ where each $a_k$ and $b_\\ell$ are floats rounded to $6$ decimals.\n\nTest Suite:\n- Test Case $1$ (small sample sizes, moderate variances): Clinics have outcomes\n  - Clinic $1$: $[0.9,1.2,1.0]$\n  - Clinic $2$: $[1.5,1.3]$\n  - Clinic $3$: $[0.7]$\n  - Clinic $4$: $[1.1,1.2,0.9,1.0]$\n  with $\\sigma_u^2 = 0.05$ and $\\sigma_\\epsilon^2 = 0.20$.\n- Test Case $2$ (strong shrinkage due to high individual-level variance): Clinics have outcomes\n  - Clinic $1$: $[1.0,1.4]$\n  - Clinic $2$: $[0.5]$\n  - Clinic $3$: $[2.0,1.8,2.2]$\n  with $\\sigma_u^2 = 0.05$ and $\\sigma_\\epsilon^2 = 2.00$.\n- Test Case $3$ (one clinic with a large sample size, smaller individual-level variance): Clinics have outcomes\n  - Clinic $1$: $[1.02,0.98,1.01,0.99,1.00,1.03,0.97,1.05,0.95,1.04,1.02,0.96,1.00,1.01,0.99,1.00,1.03,0.97,1.02,0.98]$\n  - Clinic $2$: $[0.9,1.1]$\n  - Clinic $3$: $[1.3]$\n  with $\\sigma_u^2 = 0.10$ and $\\sigma_\\epsilon^2 = 0.05$.\n- Test Case $4$ (weak shrinkage due to large clinic-level variance): Clinics have outcomes\n  - Clinic $1$: $[1.8,2.0,1.9]$\n  - Clinic $2$: $[0.5,0.6]$\n  - Clinic $3$: $[1.2,1.1,1.3,1.4]$\n  with $\\sigma_u^2 = 1.00$ and $\\sigma_\\epsilon^2 = 0.05$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the estimated clinic random intercepts for that case, each rounded to $6$ decimals. No physical units or angles are involved; report pure numbers as specified. The overarching reasoning must elucidate how shrinkage toward the grand mean depends on $\\sigma_u^2$, $\\sigma_\\epsilon^2$, and clinic sample sizes, but the program’s output must only be the numerical lists as specified.", "solution": "The objective is to compute the estimated clinic random intercepts, defined as the conditional expectation of $u_j$ given the observed data. This quantity, denoted $\\hat{u}_j$, is the Best Linear Unbiased Predictor (BLUP) of the random effect $u_j$.\nThe model is given by $Y_{ij} = \\beta_0 + u_j + \\epsilon_{ij}$, with $u_j \\sim \\mathcal{N}(0, \\sigma_u^2)$ and $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$. All random variables are independent.\n\nThe problem states that we should estimate the fixed intercept $\\beta_0$ using the grand mean of all observations. Let $n_j$ be the number of individuals in clinic $j$, and $N = \\sum_j n_j$ be the total number of individuals. The estimate for $\\beta_0$ is:\n$$ \\hat{\\beta}_0 = \\bar{Y}_{..} = \\frac{1}{N} \\sum_{j} \\sum_{i=1}^{n_j} Y_{ij} $$\nWe are asked to find $\\hat{u}_j = E[u_j | \\{Y_{ik}\\}_{i,k}, \\sigma_u^2, \\sigma_\\epsilon^2]$. Since $u_j$ is independent of data from any other clinic $k \\neq j$, this expectation is equivalent to conditioning only on the data from clinic $j$:\n$$ \\hat{u}_j = E[u_j | \\{Y_{ij}\\}_{i=1}^{n_j}, \\sigma_u^2, \\sigma_\\epsilon^2] $$\nFor this derivation, we will treat $\\beta_0$ as known (using its estimate $\\hat{\\beta}_0$). The derivation proceeds by finding the joint distribution of the random variable we want to predict, $u_j$, and a sufficient statistic from the data, the clinic mean $\\bar{Y}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} Y_{ij}$.\n\nThe model implies that both $u_j$ and $\\bar{Y}_j$ are normally distributed.\n1.  The marginal distribution of $u_j$ is given as $u_j \\sim \\mathcal{N}(0, \\sigma_u^2)$. Thus, $E[u_j] = 0$ and $\\text{Var}(u_j) = \\sigma_u^2$.\n\n2.  The clinic mean $\\bar{Y}_j$ can be expressed as:\n    $$ \\bar{Y}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} (\\beta_0 + u_j + \\epsilon_{ij}) = \\beta_0 + u_j + \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\epsilon_{ij} = \\beta_0 + u_j + \\bar{\\epsilon}_j $$\n    where $\\bar{\\epsilon}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} \\epsilon_{ij}$.\n    The expectation of $\\bar{Y}_j$ is $E[\\bar{Y}_j] = E[\\beta_0 + u_j + \\bar{\\epsilon}_j] = \\beta_0 + E[u_j] + E[\\bar{\\epsilon}_j] = \\beta_0 + 0 + 0 = \\beta_0$.\n    The variance of $\\bar{Y}_j$ is:\n    $$ \\text{Var}(\\bar{Y}_j) = \\text{Var}(\\beta_0 + u_j + \\bar{\\epsilon}_j) = \\text{Var}(u_j + \\bar{\\epsilon}_j) $$\n    Since $u_j$ and all $\\epsilon_{ij}$ are independent, $\\text{Var}(u_j + \\bar{\\epsilon}_j) = \\text{Var}(u_j) + \\text{Var}(\\bar{\\epsilon}_j)$.\n    We have $\\text{Var}(u_j) = \\sigma_u^2$ and $\\text{Var}(\\bar{\\epsilon}_j) = \\text{Var}(\\frac{1}{n_j}\\sum_i \\epsilon_{ij}) = \\frac{1}{n_j^2}}\\sum_i \\text{Var}(\\epsilon_{ij}) = \\frac{n_j \\sigma_\\epsilon^2}{n_j^2} = \\frac{\\sigma_\\epsilon^2}{n_j}$.\n    Therefore, $\\text{Var}(\\bar{Y}_j) = \\sigma_u^2 + \\frac{\\sigma_\\epsilon^2}{n_j}$.\n\n3.  The covariance between $u_j$ and $\\bar{Y}_j$ is:\n    $$ \\text{Cov}(u_j, \\bar{Y}_j) = \\text{Cov}(u_j, \\beta_0 + u_j + \\bar{\\epsilon}_j) = \\text{Cov}(u_j, u_j) + \\text{Cov}(u_j, \\bar{\\epsilon}_j) = \\text{Var}(u_j) + 0 = \\sigma_u^2 $$\n    The covariance is zero because $u_j$ is independent of all $\\epsilon_{ik}$.\n\nSince $u_j$ and $\\bar{Y}_j$ are linear combinations of Gaussian random variables, they are jointly Gaussian. For two jointly Gaussian random variables $X_1$ and $X_2$, the conditional expectation of $X_1$ given $X_2=x_2$ is:\n$$ E[X_1 | X_2=x_2] = E[X_1] + \\frac{\\text{Cov}(X_1, X_2)}{\\text{Var}(X_2)} (x_2 - E[X_2]) $$\nLetting $X_1 = u_j$ and $X_2 = \\bar{Y}_j$, we substitute the moments we derived:\n$$ E[u_j | \\bar{Y}_j=\\bar{y}_j] = E[u_j] + \\frac{\\text{Cov}(u_j, \\bar{Y}_j)}{\\text{Var}(\\bar{Y}_j)} (\\bar{y}_j - E[\\bar{Y}_j]) $$\n$$ E[u_j | \\bar{Y}_j=\\bar{y}_j] = 0 + \\frac{\\sigma_u^2}{\\sigma_u^2 + \\sigma_\\epsilon^2/n_j} (\\bar{y}_j - \\beta_0) $$\nThis is the desired formula. The estimated random intercept $\\hat{u}_j$ is obtained by plugging in the observed clinic mean $\\bar{Y}_j$ and the estimated grand mean $\\hat{\\beta}_0$:\n$$ \\hat{u}_j = \\left( \\frac{\\sigma_u^2}{\\sigma_u^2 + \\sigma_\\epsilon^2/n_j} \\right) (\\bar{Y}_j - \\hat{\\beta}_0) $$\nThis formula shows that the estimated random intercept is a \"shrunken\" version of the raw deviation of the clinic mean from the grand mean, $(\\bar{Y}_j - \\hat{\\beta}_0)$. The shrinkage factor, $S_j = \\frac{\\sigma_u^2}{\\sigma_u^2 + \\sigma_\\epsilon^2/n_j}$, determines the degree of this shrinkage.\n\n- **Effect of $n_j$ (clinic sample size)**: As $n_j \\to \\infty$, the term $\\sigma_\\epsilon^2/n_j \\to 0$, so the shrinkage factor $S_j \\to 1$. We trust the clinic's data more, and $\\hat{u}_j \\approx \\bar{Y}_j - \\hat{\\beta}_0$. For small $n_j$, the term $\\sigma_\\epsilon^2/n_j$ is large, $S_j$ is small, and $\\hat{u}_j$ is shrunken towards $0$.\n- **Effect of $\\sigma_u^2$ (between-clinic variance)**: If $\\sigma_u^2$ is large relative to $\\sigma_\\epsilon^2/n_j$, it suggests that clinics are genuinely different from one another. The shrinkage factor $S_j$ is closer to $1$, resulting in less shrinkage.\n- **Effect of $\\sigma_\\epsilon^2$ (within-clinic variance)**: If $\\sigma_\\epsilon^2$ is large, it indicates that individual measurements are noisy. This makes the clinic mean $\\bar{Y}_j$ a less reliable estimate of the clinic's true mean ($\\beta_0 + u_j$). Consequently, the shrinkage factor $S_j$ is smaller, leading to more shrinkage towards the grand mean.\n\nThe computational procedure is as follows:\n1. For each test case, collect all outcome values $Y_{ij}$ from all clinics.\n2. Compute the grand mean $\\hat{\\beta}_0 = \\bar{Y}_{..}$.\n3. For each clinic $j$, compute its sample size $n_j$ and its local mean $\\bar{Y}_j$.\n4. Using the given $\\sigma_u^2$ and $\\sigma_\\epsilon^2$, calculate the estimated random intercept $\\hat{u}_j$ using the derived formula.\n5. Collect the calculated $\\hat{u}_j$ values for all clinics in the specified order.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the estimated random intercepts in a hierarchical model for multiple test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"clinics\": {\n                1: [0.9, 1.2, 1.0],\n                2: [1.5, 1.3],\n                3: [0.7],\n                4: [1.1, 1.2, 0.9, 1.0]\n            },\n            \"sigma_u_sq\": 0.05,\n            \"sigma_eps_sq\": 0.20\n        },\n        {\n            \"clinics\": {\n                1: [1.0, 1.4],\n                2: [0.5],\n                3: [2.0, 1.8, 2.2]\n            },\n            \"sigma_u_sq\": 0.05,\n            \"sigma_eps_sq\": 2.00\n        },\n        {\n            \"clinics\": {\n                1: [1.02, 0.98, 1.01, 0.99, 1.00, 1.03, 0.97, 1.05, 0.95, 1.04, 1.02, 0.96, 1.00, 1.01, 0.99, 1.00, 1.03, 0.97, 1.02, 0.98],\n                2: [0.9, 1.1],\n                3: [1.3]\n            },\n            \"sigma_u_sq\": 0.10,\n            \"sigma_eps_sq\": 0.05\n        },\n        {\n            \"clinics\": {\n                1: [1.8, 2.0, 1.9],\n                2: [0.5, 0.6],\n                3: [1.2, 1.1, 1.3, 1.4]\n            },\n            \"sigma_u_sq\": 1.00,\n            \"sigma_eps_sq\": 0.05\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        clinic_data = case[\"clinics\"]\n        sigma_u_sq = case[\"sigma_u_sq\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        \n        # Step 1: Collect all data points to calculate the grand mean\n        all_outcomes = [outcome for outcomes_list in clinic_data.values() for outcome in outcomes_list]\n        \n        # Step 2: Calculate the grand mean (estimate for beta_0)\n        beta_0_hat = np.mean(all_outcomes)\n        \n        case_results = []\n        \n        # Iterate through clinics in their specified order (keys 1, 2, 3...)\n        for clinic_id in sorted(clinic_data.keys()):\n            outcomes = clinic_data[clinic_id]\n            \n            # Step 3a: Get clinic sample size n_j\n            n_j = len(outcomes)\n            \n            # Step 3b: Calculate clinic mean Y_bar_j\n            y_bar_j = np.mean(outcomes)\n            \n            # Step 4: Calculate the shrinkage factor and the estimated random intercept\n            shrinkage_factor = sigma_u_sq / (sigma_u_sq + sigma_eps_sq / n_j)\n            u_j_hat = shrinkage_factor * (y_bar_j - beta_0_hat)\n            \n            # Round to 6 decimals as required\n            rounded_u_j_hat = round(u_j_hat, 6)\n            \n            case_results.append(rounded_u_j_hat)\n            \n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    # Example: [[a,b],[c,d,e]] becomes \"[[a, b], [c, d, e]]\" by default.\n    # The requirement is a comma-separated list of values without spaces.\n    # So we format it manually.\n    outer_list_str = []\n    for inner_list in all_results:\n        inner_list_str = f\"[{','.join(f'{x:.6f}' for x in inner_list)}]\"\n        outer_list_str.append(inner_list_str)\n    \n    final_output_str = f\"[{','.join(outer_list_str)}]\"\n    \n    # Correction to match the requested output format which uses Python's default list-to-string conversion\n    # [[-0.02, 0.106667, -0.076, -0.015], ...]\n    final_output_str_repr = repr(all_results).replace(\" \", \"\")\n\n    print(final_output_str_repr)\n\nsolve()\n```", "id": "4502179"}, {"introduction": "When analyzing longitudinal data, where individuals are measured repeatedly, we must account for the correlation between measurements over time. However, the true pattern of this correlation is rarely known in advance. This practical coding exercise simulates the real-world task of a data analyst: you will fit several competing models for the residual covariance structure and use the Akaike Information Criterion (AIC) to select the one that provides the best balance between model fit and complexity [@problem_id:4502188].", "problem": "You are given a repeated-measures linear model for depression scores in preventive medicine, observed for $m$ independent subjects across $T$ equally spaced time points $t_1,\\dots,t_T$, with fixed-effects mean structure $E[y_{ij}] = \\beta_0 + \\beta_1 t_j$ and a subject-level residual covariance matrix $\\Sigma$ that is identical across subjects. Assume multivariate normal residuals and no random effects beyond the residual covariance. You must build a program that, given simulated data under specified conditions, fits the model by maximizing the Gaussian likelihood under competing residual covariance structures and then computes the Akaike Information Criterion (AIC) to select among them, balancing fit and parsimony.\n\nStart from the following fundamental base:\n- The multivariate normal log-likelihood for data vector $y \\in \\mathbb{R}^n$ with mean $X \\beta$ and covariance $V$ is the sum of contributions from independent blocks when $V$ is block-diagonal.\n- The generalized least squares estimator for fixed effects is obtained by maximizing the Gaussian likelihood for given covariance parameters.\n- The Akaike Information Criterion is defined as $\\mathrm{AIC} = 2k - 2 \\ell(\\hat{\\theta}, \\hat{\\beta})$, where $k$ is the total number of estimated parameters and $\\ell(\\hat{\\theta}, \\hat{\\beta})$ is the maximized log-likelihood.\n\nModel and estimation requirements:\n- For each candidate residual covariance structure, you must maximize the (full) Gaussian log-likelihood over both the fixed-effects parameters $\\beta$ and the covariance parameters specific to the structure. Use the profile likelihood with respect to $\\beta$ given the covariance parameters. Use only the repeated-measures residual covariance; do not include random intercepts or slopes.\n- Candidate residual covariance structures to evaluate:\n  1. Independent and identically distributed residuals (IND): $\\Sigma = \\sigma^2 I_T$ ($1$ covariance parameter).\n  2. Compound symmetry (CS): $\\Sigma = \\sigma^2 \\{ (1-\\rho) I_T + \\rho \\mathbf{1}\\mathbf{1}^\\top \\}$ with $-1/(T-1) < \\rho < 1$ ($2$ covariance parameters).\n  3. Autoregressive of order $1$ (AR($1$)): $\\Sigma_{jk} = \\sigma^2 \\rho^{|j-k|}$ with $|\\rho| < 1$ ($2$ covariance parameters).\n  4. Unstructured (UN): $\\Sigma$ is any symmetric positive definite $T \\times T$ covariance matrix, parameterized via its Cholesky factor so it is guaranteed positive definite ($T(T+1)/2$ covariance parameters).\n- The total parameter count $k$ used in $\\mathrm{AIC}$ must include both the fixed-effects parameters ($p=2$ for intercept and slope) and the covariance parameters of the chosen structure.\n\nData generation for the test suite:\n- For each test case, generate simulated data according to $y_{ij} = \\beta_0 + \\beta_1 t_j + \\varepsilon_{ij}$ for $i \\in \\{1,\\dots,m\\}$ and $j \\in \\{1,\\dots,T\\}$, where the subject-level residual vector $\\varepsilon_i = (\\varepsilon_{i1},\\dots,\\varepsilon_{iT})^\\top \\sim \\mathcal{N}(0, \\Sigma_{\\text{true}})$ with the specified true structure and parameters below. Use equally spaced times $t_j = j-1$ for $j = 1,\\dots,T$. Use the provided random seed for reproducibility. All hyperparameters are to be treated as unitless.\n\nTest suite (three cases):\n- Case $1$: $m=50$, $T=4$, $\\beta_0=20.0$, $\\beta_1=-1.5$, true structure AR($1$) with $\\rho=0.7$, $\\sigma^2=4.0$, random seed $12345$.\n- Case $2$: $m=30$, $T=4$, $\\beta_0=15.0$, $\\beta_1=0.5$, true structure CS with $\\rho=0.4$, $\\sigma^2=9.0$, random seed $24680$.\n- Case $3$: $m=20$, $T=4$, $\\beta_0=10.0$, $\\beta_1=0.0$, true structure IND with $\\sigma^2=5.0$, random seed $13579$.\n\nImplementation constraints and details:\n- Treat the design matrix as $X = [\\mathbf{1}, t] \\in \\mathbb{R}^{T \\times 2}$ at the subject level, and the full data as $m$ independent subject blocks with identical $X$.\n- Maximize the full Gaussian log-likelihood (not restricted maximum likelihood). The profile likelihood with respect to $\\beta$ should be used during the covariance parameter optimization.\n- For numerical stability, parameterize constrained covariance parameters via unconstrained reals (for example, log-transform for variances, hyperbolic tangent for correlations, or an explicit Cholesky factor for the unstructured covariance).\n- For each test case, compute the maximized log-likelihood and AIC for each of the four candidate residual covariance structures. Let the total parameter counts be $k_{\\text{IND}} = 3$, $k_{\\text{CS}} = 4$, $k_{\\text{AR1}} = 4$, $k_{\\text{UN}} = T(T+1)/2 + 2$; these include $p=2$ fixed effects in all cases.\n- Select the structure with the smallest AIC. In case of an exact numerical tie, select the smallest index by the ordering below.\n\nOutput specification:\n- Use the following index mapping for the structures: IND $\\to 0$, CS $\\to 1$, AR($1$) $\\to 2$, UN $\\to 3$.\n- Your program should produce a single line of output containing the index of the AIC-selected structure for each of the three test cases, as a comma-separated list enclosed in square brackets (for example, $\"[2,1,0]\"$).\n\nYour final program must be complete, runnable, and must not require any user input. Only the single required output line should be printed. No units are involved; all outputs are unitless integers.", "solution": "The problem requires performing model selection for a repeated-measures linear model applied to simulated data from a preventive medicine context. The selection among four candidate residual covariance structures is to be based on the Akaike Information Criterion (AIC). The process involves parameter estimation via maximization of the full Gaussian log-likelihood.\n\n### 1. General Model and Likelihood Function\n\nThe model for the repeated-measures data of subject $i \\in \\{1,\\dots,m\\}$ is given by a $T$-dimensional multivariate normal distribution:\n$$ y_i \\sim \\mathcal{N}(X\\beta, \\Sigma(\\theta)) $$\nwhere $y_i = (y_{i1}, \\dots, y_{iT})^\\top$ is the vector of depression scores for subject $i$ at $T$ time points, $X$ is the $T \\times p$ design matrix for the fixed effects, $\\beta$ is the $p \\times 1$ vector of fixed-effects parameters, and $\\Sigma(\\theta)$ is the $T \\times T$ within-subject residual covariance matrix, parameterized by a vector $\\theta$. In this problem, $p=2$, with the fixed effects being an intercept $\\beta_0$ and a linear time trend $\\beta_1$. The design matrix for times $t_j = j-1$ is identical for all subjects:\n$$ X = \\begin{pmatrix} 1 & t_1 \\\\ 1 & t_2 \\\\ \\vdots & \\vdots \\\\ 1 & t_T \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ \\vdots & \\vdots \\\\ 1 & T-1 \\end{pmatrix} $$\nSince the $m$ subjects are assumed to be independent, the total log-likelihood of the data $Y = \\{y_1, \\dots, y_m\\}$ is the sum of the individual log-likelihoods:\n$$ \\ell(\\beta, \\theta | Y) = \\sum_{i=1}^{m} \\ell_i(\\beta, \\theta | y_i) $$\nThe log-likelihood for a single subject $i$ is:\n$$ \\ell_i(\\beta, \\theta | y_i) = -\\frac{T}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln |\\det(\\Sigma(\\theta))| - \\frac{1}{2} (y_i - X\\beta)^\\top \\Sigma(\\theta)^{-1} (y_i - X\\beta) $$\nThe total log-likelihood is therefore:\n$$ \\ell(\\beta, \\theta | Y) = -\\frac{mT}{2} \\ln(2\\pi) - \\frac{m}{2} \\ln |\\det(\\Sigma(\\theta))| - \\frac{1}{2} \\sum_{i=1}^{m} (y_i - X\\beta)^\\top \\Sigma(\\theta)^{-1} (y_i - X\\beta) $$\n\n### 2. Estimation via Profile Likelihood\n\nTo find the maximum likelihood estimates ($\\hat{\\beta}, \\hat{\\theta}$), we employ a profile likelihood approach. For a fixed value of the covariance parameters $\\theta$, we first maximize $\\ell(\\beta, \\theta | Y)$ with respect to $\\beta$. The resulting estimator for $\\beta$ is the Generalized Least Squares (GLS) estimator:\n$$ \\hat{\\beta}(\\theta) = \\left( \\sum_{i=1}^m X^\\top \\Sigma(\\theta)^{-1} X \\right)^{-1} \\left( \\sum_{i=1}^m X^\\top \\Sigma(\\theta)^{-1} y_i \\right) $$\nSince $X$ and $\\Sigma$ are common to all subjects, this simplifies to:\n$$ \\hat{\\beta}(\\theta) = (m X^\\top \\Sigma(\\theta)^{-1} X)^{-1} (X^\\top \\Sigma(\\theta)^{-1} \\sum_{i=1}^m y_i) = (X^\\top \\Sigma(\\theta)^{-1} X)^{-1} (X^\\top \\Sigma(\\theta)^{-1} \\bar{y}) $$\nwhere $\\bar{y} = \\frac{1}{m} \\sum_{i=1}^m y_i$ is the vector of mean responses at each time point.\n\nSubstituting $\\hat{\\beta}(\\theta)$ back into the log-likelihood function yields the profile log-likelihood, which is a function of $\\theta$ alone:\n$$ \\ell_p(\\theta) = \\ell(\\hat{\\beta}(\\theta), \\theta | Y) $$\nWe then numerically maximize $\\ell_p(\\theta)$ (or, equivalently, minimize $-\\ell_p(\\theta)$) with respect to $\\theta$ to find its maximum likelihood estimate, $\\hat{\\theta}$. The final estimate for $\\beta$ is $\\hat{\\beta} = \\hat{\\beta}(\\hat{\\theta})$. The maximized log-likelihood is $\\ell_{\\max} = \\ell_p(\\hat{\\theta})$.\n\n### 3. Candidate Covariance Structures and Parameterization\n\nFor numerical stability, the constrained parameters of the covariance matrix (variances, correlations) are transformed into unconstrained real numbers for optimization.\n\n1.  **Independent (IND):** $\\Sigma = \\sigma^2 I_T$.\n    -   Parameters: $1$ ($\\sigma^2 > 0$).\n    -   Unconstrained parameter: $\\log(\\sigma^2)$.\n\n2.  **Compound Symmetry (CS):** $\\Sigma_{jk} = \\sigma^2$ if $j=k$, and $\\Sigma_{jk} = \\sigma^2 \\rho$ if $j \\neq k$.\n    -   Parameters: $2$ ($\\sigma^2 > 0$, and $-1/(T-1) < \\rho < 1$ for positive definiteness).\n    -   Unconstrained parameters: $\\log(\\sigma^2)$ and an unconstrained real $u$ which is mapped to $\\rho$ via a scaled and shifted hyperbolic tangent function, e.g., $\\rho = a + (b-a)(\\tanh(u)+1)/2$ where $[a, b]$ is the valid range for $\\rho$.\n\n3.  **Autoregressive Order 1 (AR(1)):** $\\Sigma_{jk} = \\sigma^2 \\rho^{|j-k|}$.\n    -   Parameters: $2$ ($\\sigma^2 > 0$, $|\\rho| < 1$).\n    -   Unconstrained parameters: $\\log(\\sigma^2)$ and $\\text{atanh}(\\rho)$.\n\n4.  **Unstructured (UN):** $\\Sigma$ is an arbitrary symmetric positive definite matrix.\n    -   Parameters: $T(T+1)/2$.\n    -   Parameterization: To ensure $\\Sigma$ is positive definite, it is parameterized via its Cholesky decomposition $\\Sigma = LL^\\top$, where $L$ is a lower triangular matrix. The diagonal elements $L_{jj}$ must be positive, so we use their logarithms $\\log(L_{jj})$ as unconstrained parameters. The off-diagonal elements $L_{jk}$ for $j > k$ are unconstrained.\n\n### 4. Akaike Information Criterion (AIC)\n\nAIC provides a means to select a model by balancing goodness of fit with model parsimony. It is defined as:\n$$ \\mathrm{AIC} = 2k - 2\\ell_{\\max} $$\nwhere $k$ is the total number of estimated parameters in the model, and $\\ell_{\\max}$ is the value of the log-likelihood function at the maximum likelihood estimates. The total parameter count is $k = p + (\\text{number of covariance parameters})$, where $p=2$ for the fixed effects.\n-   $k_{\\text{IND}} = 2 + 1 = 3$\n-   $k_{\\text{CS}} = 2 + 2 = 4$\n-   $k_{\\text{AR1}} = 2 + 2 = 4$\n-   $k_{\\text{UN}} = 2 + T(T+1)/2$\n\nThe model with the lowest AIC is selected as the best model among the candidates.\n\n### 5. Algorithmic Procedure\n\nFor each of the three test cases, the following procedure is executed:\n1.  **Data Simulation:** Generate $m$ independent samples of residual vectors $\\varepsilon_i \\sim \\mathcal{N}(0, \\Sigma_{\\text{true}})$ using the specified true covariance structure and parameters. The observed data are then computed as $y_i = X\\beta + \\varepsilon_i$.\n2.  **Model Fitting:** For each of the four candidate structures (IND, CS, AR(1), UN):\n    a.  Define an objective function that takes a vector of unconstrained covariance parameters, transforms them to construct $\\Sigma$, computes $\\hat{\\beta}(\\Sigma)$, and returns the value of the negative profile log-likelihood. To simplify the calculation and improve numerical stability, we can work with a quantity proportional to the negative log-likelihood:\n        $$ f(\\theta) = m \\ln |\\det(\\Sigma(\\theta))| + \\sum_{i=1}^{m} (y_i - X\\hat{\\beta}(\\theta))^\\top \\Sigma(\\theta)^{-1} (y_i - X\\hat{\\beta}(\\theta)) $$\n    b.  Use a numerical optimization algorithm (e.g., BFGS) to find the unconstrained parameters $\\hat{\\theta}_{\\text{unc}}$ that minimize $f(\\theta)$.\n    c.  The minimized value $f(\\hat{\\theta})$ is used to compute the maximized log-likelihood $\\ell_{\\max} = -\\frac{mT}{2}\\ln(2\\pi) - \\frac{1}{2} f(\\hat{\\theta})$.\n    d.  Calculate the AIC using $\\ell_{\\max}$ and the corresponding parameter count $k$.\n3.  **Model Selection:** Compare the AIC values for the four fitted models and select the model corresponding to the minimum AIC. The index of this model (IND:0, CS:1, AR(1):2, UN:3) is recorded.\n4.  **Output:** After processing all test cases, the final output is a list of the selected model indices.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import cho_solve, cholesky\n\ndef generate_data(m, T, beta, cov_struct_name, cov_params, seed):\n    \"\"\"Generates simulated repeated-measures data.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Construct true covariance matrix\n    sigma2 = cov_params['sigma2']\n    rho = cov_params.get('rho')\n    \n    if cov_struct_name == 'IND':\n        sigma_true = sigma2 * np.identity(T)\n    elif cov_struct_name == 'CS':\n        sigma_true = sigma2 * ((1 - rho) * np.identity(T) + rho * np.ones((T, T)))\n    elif cov_struct_name == 'AR1':\n        times = np.arange(T)\n        exp_matrix = np.abs(times[:, np.newaxis] - times[np.newaxis, :])\n        sigma_true = sigma2 * (rho ** exp_matrix)\n    else:\n        raise ValueError(\"Unknown true covariance structure\")\n\n    # Design matrix\n    times = np.arange(T)\n    X = np.stack([np.ones(T), times], axis=1)\n    \n    mean_vector = X @ beta\n    \n    # Generate data\n    y_data = np.zeros((m, T))\n    for i in range(m):\n        # Generate residuals and add to mean\n        epsilon = rng.multivariate_normal(np.zeros(T), sigma_true)\n        y_data[i, :] = mean_vector + epsilon\n        \n    return y_data, X\n\ndef get_sigma_from_unc(theta_unc, T, model_type):\n    \"\"\"Constructs covariance matrix from unconstrained parameters.\"\"\"\n    if model_type == 'IND':\n        # theta_unc = [log(sigma^2)]\n        sigma2 = np.exp(theta_unc[0])\n        if sigma2 < 1e-9: return None\n        return sigma2 * np.identity(T)\n\n    elif model_type == 'CS':\n        # theta_unc = [log(sigma^2), atanh_transformed_rho]\n        sigma2 = np.exp(theta_unc[0])\n        rho_range_min = -1.0 / (T - 1)\n        # Transform from R -> (-1, 1) -> (rho_min, 1)\n        rho = rho_range_min + (1.0 - rho_range_min) * (np.tanh(theta_unc[1]) + 1.0) / 2.0\n        if sigma2 < 1e-9: return None\n        return sigma2 * ((1 - rho) * np.identity(T) + rho * np.ones((T, T)))\n\n    elif model_type == 'AR1':\n        # theta_unc = [log(sigma^2), atanh(rho)]\n        sigma2 = np.exp(theta_unc[0])\n        rho = np.tanh(theta_unc[1])\n        if sigma2 < 1e-9: return None\n        times = np.arange(T)\n        exp_matrix = np.abs(times[:, np.newaxis] - times[np.newaxis, :])\n        return sigma2 * (rho ** exp_matrix)\n\n    elif model_type == 'UN':\n        # theta_unc = [log(L_jj),..., L_jk,...]\n        L = np.zeros((T, T))\n        diag_indices = np.diag_indices(T)\n        tril_indices = np.tril_indices(T, k=-1)\n        \n        log_diag_L = theta_unc[:T]\n        off_diag_L = theta_unc[T:]\n\n        L[diag_indices] = np.exp(log_diag_L)\n        if np.any(L[diag_indices] < 1e-9): return None\n        L[tril_indices] = off_diag_L\n        return L @ L.T\n\n    return None\n\ndef neg_profile_log_likelihood(theta_unc, y_data, X, T, m, model_type):\n    \"\"\"Objective function for optimization (proportional to neg-log-likelihood).\"\"\"\n    try:\n        sigma = get_sigma_from_unc(theta_unc, T, model_type)\n        if sigma is None: return np.inf\n\n        # Use Cholesky decomposition for stability and efficiency\n        L = cholesky(sigma, lower=True)\n        log_det_sigma = 2 * np.sum(np.log(np.diag(L)))\n        \n        # Calculate inv_sigma @ X and inv_sigma @ y_bar efficiently\n        inv_sigma_X = cho_solve((L, True), X)\n        \n        y_bar = np.mean(y_data, axis=0)\n        inv_sigma_y_bar = cho_solve((L, True), y_bar)\n\n        # GLS estimate for beta\n        term1 = X.T @ inv_sigma_X\n        term2 = X.T @ inv_sigma_y_bar\n        beta_hat = np.linalg.solve(term1, term2)\n        \n        # Sum of quadratic forms component of the likelihood\n        # Sum( (y_i - Xb)^T Sigma^-1 (y_i - Xb) )\n        # = Tr(Sigma^-1 * Sum( (y_i - Xb)(y_i - Xb)^T ))\n        residuals = y_data - (X @ beta_hat)\n        SS = residuals.T @ residuals\n        quad_form_sum = np.trace(cho_solve((L, True), SS))\n        \n        # This is 2 * neg-profile-log-likelihood (up to a constant)\n        val = m * log_det_sigma + quad_form_sum\n        return val\n\n    except (np.linalg.LinAlgError, ValueError):\n        # Catches non-positive definite matrices or other numerical issues\n        return np.inf\n\ndef solve():\n    test_cases = [\n        {'m': 50, 'T': 4, 'beta': np.array([20.0, -1.5]), 'true_struct': 'AR1', 'cov_params': {'rho': 0.7, 'sigma2': 4.0}, 'seed': 12345},\n        {'m': 30, 'T': 4, 'beta': np.array([15.0, 0.5]), 'true_struct': 'CS', 'cov_params': {'rho': 0.4, 'sigma2': 9.0}, 'seed': 24680},\n        {'m': 20, 'T': 4, 'beta': np.array([10.0, 0.0]), 'true_struct': 'IND', 'cov_params': {'sigma2': 5.0}, 'seed': 13579}\n    ]\n\n    model_specs = [\n        {'name': 'IND', 'k_cov': 1},\n        {'name': 'CS',  'k_cov': 2},\n        {'name': 'AR1', 'k_cov': 2},\n        {'name': 'UN',  'k_cov': 4 * (4 + 1) // 2}\n    ]\n    p = 2 # Number of fixed effects\n\n    best_model_indices = []\n\n    for case in test_cases:\n        m, T, beta, true_struct, cov_params, seed = case.values()\n        y_data, X = generate_data(m, T, beta, true_struct, cov_params, seed)\n\n        aics = []\n        \n        # Initial guess from OLS\n        y_flat = y_data.flatten()\n        X_full = np.tile(X, (m, 1))\n        beta_ols = np.linalg.lstsq(X_full, y_flat, rcond=None)[0]\n        residuals_ols = y_flat - X_full @ beta_ols\n        sigma2_ols = np.mean(residuals_ols**2)\n\n        for spec in model_specs:\n            model_type = spec['name']\n            k = p + spec['k_cov']\n            \n            # Initial guess for unconstrained parameters\n            if model_type == 'IND':\n                x0 = [np.log(sigma2_ols)]\n            elif model_type in ['CS', 'AR1']:\n                x0 = [np.log(sigma2_ols), 0.0]\n            elif model_type == 'UN':\n                x0_diag = [np.log(np.sqrt(sigma2_ols))] * T\n                x0_offdiag = [0.0] * (T*(T-1)//2)\n                x0 = x0_diag + x0_offdiag\n\n            res = minimize(neg_profile_log_likelihood, x0,\n                           args=(y_data, X, T, m, model_type),\n                           method='BFGS', options={'gtol': 1e-4})\n            \n            min_val = res.fun\n            if np.isinf(min_val):\n                aic = np.inf\n            else:\n                max_log_lik = -0.5 * (m * T * np.log(2 * np.pi) + min_val)\n                aic = 2 * k - 2 * max_log_lik\n            \n            aics.append(aic)\n            \n        best_model_idx = np.argmin(aics)\n        best_model_indices.append(best_model_idx)\n\n    print(f\"[{','.join(map(str, best_model_indices))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4502188"}]}