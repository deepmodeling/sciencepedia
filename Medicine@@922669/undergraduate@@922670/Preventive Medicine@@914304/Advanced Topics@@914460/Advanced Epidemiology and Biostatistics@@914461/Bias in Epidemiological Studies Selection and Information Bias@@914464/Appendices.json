{"hands_on_practices": [{"introduction": "In epidemiological research, we often rely on data that may not perfectly capture the true health outcome, a common source of information bias. This exercise simulates a realistic scenario where an administrative algorithm, rather than a perfect clinical assessment, is used to identify infections. You will explore the concept of non-differential information bias, calculating how imperfect test sensitivity ($Se$) and specificity ($Sp$) combine to distort a true association, providing a hands-on look at how observed results can differ from reality [@problem_id:4504855].", "problem": "An investigator in preventive medicine conducts an observational cohort study to evaluate whether a preoperative antiseptic protocol reduces the risk of postoperative wound infection. Let the binary exposure variable be $E$, where $E=1$ denotes protocol use and $E=0$ denotes no protocol. The true binary outcome is $Y$, where $Y=1$ denotes infection within $30$ days and $Y=0$ denotes no infection. Due to limited resources, infections are ascertained using an administrative algorithm that misclassifies the outcome. Let the misclassified outcome be $Y^*$.\n\nAssume outcome misclassification is non-differential with respect to exposure. The sensitivity ($Se$) is the probability that the algorithm classifies an infection as infected, $Se=P(Y^*=1\\mid Y=1)$, and the specificity ($Sp$) is the probability that the algorithm classifies a non-infected as non-infected, $Sp=P(Y^*=0\\mid Y=0)$. Suppose $Se=0.8$ and $Sp=0.95$. Further, suppose the true risks are $P(Y=1\\mid E=1)=0.2$ and $P(Y=1\\mid E=0)=0.1$.\n\nUsing only the core definitions of sensitivity and specificity, basic probability rules (including complements and the law of total probability), and the definition of the risk ratio (RR), derive the observed risks $P(Y^*=1\\mid E=1)$ and $P(Y^*=1\\mid E=0)$ and the observed risk ratio $RR^*=\\frac{P(Y^*=1\\mid E=1)}{P(Y^*=1\\mid E=0)}$. Express your final answer as a row matrix with three entries in the order $\\big(P(Y^*=1\\mid E=1),\\;P(Y^*=1\\mid E=0),\\;RR^*\\big)$. No rounding is required.", "solution": "The problem requires the derivation of the observed risks of infection for the exposed and unexposed groups, $P(Y^*=1\\mid E=1)$ and $P(Y^*=1\\mid E=0)$ respectively, and the resulting observed risk ratio, $RR^* = \\frac{P(Y^*=1\\mid E=1)}{P(Y^*=1\\mid E=0)}$. The derivation proceeds from the fundamental definitions of sensitivity, specificity, and the law of total probability, under the explicit assumption of non-differential outcome misclassification.\n\nLet $E$ be the binary exposure variable ($E=1$ for protocol use, $E=0$ for no protocol) and $Y$ be the true binary outcome variable ($Y=1$ for infection, $Y=0$ for no infection). Let $Y^*$ be the misclassified outcome variable. We are given the following values:\nThe sensitivity of the diagnostic algorithm is $Se = P(Y^*=1\\mid Y=1) = 0.8$.\nThe specificity is $Sp = P(Y^*=0\\mid Y=0) = 0.95$.\nThe true risk of infection in the exposed group is $P(Y=1\\mid E=1) = 0.2$.\nThe true risk of infection in the unexposed group is $P(Y=1\\mid E=0) = 0.1$.\n\nThe observed probability of infection in a given exposure group, $P(Y^*=1\\mid E=e)$, can be found using the law of total probability by conditioning on the true outcome status, $Y$. This partitions the calculation across the two states of true outcome, $Y=1$ and $Y=0$:\n$$P(Y^*=1\\mid E=e) = P(Y^*=1\\mid Y=1, E=e)P(Y=1\\mid E=e) + P(Y^*=1\\mid Y=0, E=e)P(Y=0\\mid E=e)$$\n\nThe problem states that the outcome misclassification is non-differential with respect to exposure. This crucial assumption implies that the accuracy of the classification does not depend on the exposure status. Formally:\n$P(Y^*=y^* \\mid Y=y, E=e) = P(Y^*=y^* \\mid Y=y)$ for any state of $y^*$, $y$, and $e$.\n\nWe can therefore simplify the terms in the expansion:\nThe probability of correct classification of an infected individual is the sensitivity: $P(Y^*=1\\mid Y=1, E=e) = P(Y^*=1\\mid Y=1) = Se$.\nThe probability of incorrect classification of a non-infected individual (a false positive) is derived from the specificity: $P(Y^*=1\\mid Y=0, E=e) = P(Y^*=1\\mid Y=0) = 1 - P(Y^*=0\\mid Y=0) = 1 - Sp$.\n\nSubstituting these into the formula for the observed risk yields a general expression:\n$$P(Y^*=1\\mid E=e) = (Se)P(Y=1\\mid E=e) + (1-Sp)P(Y=0\\mid E=e)$$\nSince $P(Y=0\\mid E=e) = 1 - P(Y=1\\mid E=e)$, we can write the formula entirely in terms of the true risk and the test characteristics:\n$$P(Y^*=1\\mid E=e) = (Se)P(Y=1\\mid E=e) + (1-Sp)(1 - P(Y=1\\mid E=e))$$\n\nWe are given $Se=0.8$ and $Sp=0.95$, so $1-Sp = 1 - 0.95 = 0.05$.\n\nNow, we calculate the observed risk for the exposed group ($E=1$), using the true risk $P(Y=1\\mid E=1) = 0.2$:\n$P(Y^*=1\\mid E=1) = (0.8) \\times P(Y=1\\mid E=1) + (0.05) \\times (1 - P(Y=1\\mid E=1))$\n$P(Y^*=1\\mid E=1) = (0.8)(0.2) + (0.05)(1 - 0.2)$\n$P(Y^*=1\\mid E=1) = (0.8)(0.2) + (0.05)(0.8)$\n$P(Y^*=1\\mid E=1) = 0.16 + 0.04$\n$P(Y^*=1\\mid E=1) = 0.20$\n\nNext, we calculate the observed risk for the unexposed group ($E=0$), using the true risk $P(Y=1\\mid E=0) = 0.1$:\n$P(Y^*=1\\mid E=0) = (0.8) \\times P(Y=1\\mid E=0) + (0.05) \\times (1 - P(Y=1\\mid E=0))$\n$P(Y^*=1\\mid E=0) = (0.8)(0.1) + (0.05)(1 - 0.1)$\n$P(Y^*=1\\mid E=0) = (0.8)(0.1) + (0.05)(0.9)$\n$P(Y^*=1\\mid E=0) = 0.08 + 0.045$\n$P(Y^*=1\\mid E=0) = 0.125$\n\nFinally, we calculate the observed risk ratio, $RR^*$, using the derived observed risks:\n$RR^* = \\frac{P(Y^*=1\\mid E=1)}{P(Y^*=1\\mid E=0)} = \\frac{0.20}{0.125}$\nTo simplify the fraction:\n$RR^* = \\frac{200}{125} = \\frac{8 \\times 25}{5 \\times 25} = \\frac{8}{5} = 1.6$\n\nThe true risk ratio is $RR = \\frac{P(Y=1\\mid E=1)}{P(Y=1\\mid E=0)} = \\frac{0.2}{0.1} = 2.0$. The observed risk ratio $RR^*=1.6$ is attenuated, or biased towards the null value of $1.0$, a characteristic effect of non-differential misclassification of a binary outcome.\n\nThe requested quantities are the observed risk in the exposed group, the observed risk in the unexposed group, and the observed risk ratio.", "answer": "$$\\boxed{\\begin{pmatrix} 0.2 & 0.125 & 1.6 \\end{pmatrix}}$$", "id": "4504855"}, {"introduction": "Selection bias undermines a study's validity when the process of recruiting or retaining participants is connected to both the exposure and the outcome being studied. This exercise provides a powerful quantitative framework for understanding this phenomenon. By deriving the relationship between the true odds ratio ($OR_{DE}$), the observed odds ratio ($OR_{DE | S=1}$), and a formal selection bias factor ($B_s$), you will learn to precisely calculate how participant selection can systematically distort research findings [@problem_id:4504830].", "problem": "Consider a source population in which a binary exposure $E \\in \\{0,1\\}$ may influence a binary disease outcome $D \\in \\{0,1\\}$. Investigators recruit participants into a study by a selection mechanism $S \\in \\{0,1\\}$ (with $S=1$ indicating inclusion). The target parameter is the odds ratio (OR), defined in the source population as $OR_{DE}=\\frac{\\text{odds}(D=1 \\mid E=1)}{\\text{odds}(D=1 \\mid E=0)}$, where $\\text{odds}(D=1 \\mid E=j)=\\frac{P(D=1 \\mid E=j)}{P(D=0 \\mid E=j)}$.\n\nLet $s_{ij}=P(S=1 \\mid D=i, E=j)$ denote the selection probability for each disease-exposure stratum. Define the selection bias factor $B_{s}$ by the cross-product ratio of selection probabilities,\n$$\nB_{s}=\\frac{s_{11}\\,s_{00}}{s_{10}\\,s_{01}},\n$$\nwhich quantifies the differential selection across the four $(D,E)$ strata.\n\nStarting only from the definitions above, derive the observed odds ratio among selected participants, $OR_{DE \\mid S=1}$, in terms of $OR_{DE}$ and $B_{s}$. Then, for a study in which the true source-population odds ratio is $OR_{DE}=2.0$ and the selection bias factor is $B_{s}=0.7$, compute the observed odds ratio among selected participants. Provide the observed odds ratio as an exact value (no rounding). In your reasoning, indicate whether the selection mechanism attenuates or amplifies the association compared with the source population.", "solution": "We begin with the fundamental definitions of the odds ratio (OR) and the effect of selection on the joint distribution of $(D,E)$. The source-population odds ratio is\n$$\nOR_{DE}=\\frac{\\text{odds}(D=1 \\mid E=1)}{\\text{odds}(D=1 \\mid E=0)}=\\frac{\\frac{P(D=1 \\mid E=1)}{P(D=0 \\mid E=1)}}{\\frac{P(D=1 \\mid E=0)}{P(D=0 \\mid E=0)}}.\n$$\nEquivalently, expressing the odds ratio in terms of joint probabilities $p_{ij}=P(D=i,E=j)$,\n$$\nOR_{DE}=\\frac{p_{11}/p_{01}}{p_{10}/p_{00}}=\\frac{p_{11}\\,p_{00}}{p_{10}\\,p_{01}}.\n$$\n\nSelection modifies the joint distribution: among selected participants ($S=1$), the joint probability of $(D=i,E=j)$ is proportional to $s_{ij} \\, p_{ij}$, with $s_{ij}=P(S=1 \\mid D=i,E=j)$. Specifically,\n$$\nP(D=i,E=j \\mid S=1)=\\frac{s_{ij}\\,p_{ij}}{\\sum_{i',j'} s_{i'j'}\\,p_{i'j'}}.\n$$\nTherefore, the odds ratio among selected participants is\n$$\nOR_{DE \\mid S=1}=\\frac{P(D=1,E=1 \\mid S=1)/P(D=0,E=1 \\mid S=1)}{P(D=1,E=0 \\mid S=1)/P(D=0,E=0 \\mid S=1)}.\n$$\nSubstituting $P(D=i,E=j \\mid S=1)\\propto s_{ij}\\,p_{ij}$ (the common normalizing denominator cancels within each ratio), we obtain\n$$\nOR_{DE \\mid S=1}=\\frac{(s_{11}\\,p_{11})/(s_{01}\\,p_{01})}{(s_{10}\\,p_{10})/(s_{00}\\,p_{00})}\n=\\frac{s_{11}\\,p_{11}\\,s_{00}\\,p_{00}}{s_{10}\\,p_{10}\\,s_{01}\\,p_{01}}.\n$$\nRearranging terms,\n$$\nOR_{DE \\mid S=1}=\\left(\\frac{p_{11}\\,p_{00}}{p_{10}\\,p_{01}}\\right)\\left(\\frac{s_{11}\\,s_{00}}{s_{10}\\,s_{01}}\\right)=OR_{DE}\\times B_{s}.\n$$\n\nThis expression shows that the observed odds ratio among the selected sample equals the source-population odds ratio multiplied by the selection bias factor. The direction of bias depends on $B_{s}$: if $B_{s}>1$, the association is amplified; if $B_{s}<1$, it is attenuated.\n\nFor the given values $OR_{DE}=2.0$ and $B_{s}=0.7$, we compute\n$$\nOR_{DE \\mid S=1}=2.0 \\times 0.7=1.4.\n$$\nBecause $B_{s}=0.7<1$, the selection mechanism attenuates the association, reducing the observed odds ratio from $2.0$ in the source population to $1.4$ among selected participants.", "answer": "$$\\boxed{1.4}$$", "id": "4504830"}, {"introduction": "While theoretical calculations reveal the expected magnitude of bias, computational simulations allow us to see how bias behaves in the context of finite samples and random chance. This hands-on coding practice challenges you to model the effects of non-differential outcome misclassification on a cohort study from first principles. By running your simulation, you will generate an entire distribution of biased risk ratios, providing a powerful visual and statistical understanding of how systematic error interacts with sampling variability to shape the results we observe [@problem_id:4504834].", "problem": "You are asked to design and implement a simulation to study information bias via nondifferential outcome misclassification in a simple two-arm cohort study. Begin from the following fundamental base: the risk in the exposed group is defined as $R_{1} = P(Y=1 \\mid A=1)$ and the risk in the unexposed group is $R_{0} = P(Y=1 \\mid A=0)$, and the true risk ratio is defined as $RR_{\\text{true}} = R_{1} / R_{0}$. Sensitivity is defined as $Se = P(Y^*=1 \\mid Y=1)$, specificity is defined as $Sp = P(Y^*=0 \\mid Y=0)$, and nondifferential misclassification means that $Se$ and $Sp$ do not depend on exposure $A$. Assume independent Bernoulli assignment of outcomes within each exposure group given the corresponding true risk.\n\nYour program must implement the following, using only these definitions and basic probability rules:\n\n- For each test case, you are given the number of exposed individuals $N_{1}$, the number of unexposed individuals $N_{0}$, the baseline true risk in the unexposed group $p_{0}$, the true risk ratio $RR_{\\text{true}}$, the misclassification sensitivity $Se$, the misclassification specificity $Sp$, and the number of independent simulation runs $R$. Use a fixed pseudorandom seed equal to $1337$ for reproducibility.\n- Derive the true risk in the exposed group as $p_{1} = RR_{\\text{true}} \\cdot p_{0}$ and ensure that $0 \\le p_{1} \\le 1$ and $0 \\le p_{0} \\le 1$.\n- For each run:\n  1. Generate the true number of outcomes in the exposed group by drawing from a binomial distribution with parameters $(N_{1}, p_{1})$, and in the unexposed group with parameters $(N_{0}, p_{0})$.\n  2. Apply nondifferential misclassification independently within each group: a true case is observed as positive with probability $Se$ and a true non-case is observed as positive with probability $1 - Sp$. Use only the definitions of sensitivity and specificity to determine these probabilities.\n  3. Construct the observed $2 \\times 2$ table counts $(a,b,c,d)$ where $a$ is the observed positives among the exposed, $b$ is the observed negatives among the exposed, $c$ is the observed positives among the unexposed, and $d$ is the observed negatives among the unexposed. If any of $a$, $b$, $c$, or $d$ equals $0$, apply a Haldane–Anscombe continuity correction by adding $0.5$ to each of the four cells before computing risks.\n  4. Compute the observed risk ratio $RR_{\\text{obs}} = \\dfrac{a/(a+b)}{c/(c+d)}$.\n- After $R$ runs, summarize the empirical distribution of $RR_{\\text{obs}}$ by computing the following five quantities: the mean, the standard deviation, the median, the empirical lower quantile at probability $0.025$, and the empirical upper quantile at probability $0.975$. Also compute the absolute bias relative to the true risk ratio, defined as $\\text{bias} = \\text{mean}(RR_{\\text{obs}}) - RR_{\\text{true}}$.\n\nYour program should process the following test suite, where each tuple is $(N_{1}, N_{0}, p_{0}, RR_{\\text{true}}, Se, Sp, R)$:\n\n- Test $1$ (happy path, matches the thumbnail): $(2000, 2000, 0.10, 1.5, 0.85, 0.90, 1000)$.\n- Test $2$ (boundary: no misclassification): $(2000, 2000, 0.10, 1.5, 1.0, 1.0, 1000)$.\n- Test $3$ (small sample size): $(300, 300, 0.10, 1.5, 0.85, 0.90, 1000)$.\n- Test $4$ (rare outcome, stronger effect): $(5000, 5000, 0.02, 2.0, 0.85, 0.90, 1000)$.\n- Test $5$ (severe misclassification): $(5000, 5000, 0.10, 1.5, 0.60, 0.60, 1000)$.\n\nAll probabilities must be provided and interpreted as decimals in $[0,1]$. Angles are not involved. No physical units are involved. For each test, the program must output a list containing six floats in this exact order: $[$mean, standard deviation, median, $0.025$ quantile, $0.975$ quantile, bias$]$. Aggregate the results from all tests into a single line printed exactly as one list of lists, with comma-separated entries and enclosed in square brackets, for example: $[[x_{11},x_{12},\\dots,x_{16}],[x_{21},\\dots,x_{26}],\\dots]$. The program must print only this single line.", "solution": "The problem requires the design and implementation of a Monte Carlo simulation to quantify the impact of nondifferential outcome misclassification on the risk ratio in a cohort study. The solution involves defining the true and observed epidemiological parameters, simulating the data generation process under misclassification, and summarizing the resulting distribution of the observed risk ratio. The entire process is grounded in the fundamental principles of probability theory and statistics.\n\nFirst, we establish the definitions provided. In a two-arm cohort study with exposure status $A$ ($A=1$ for exposed, $A=0$ for unexposed) and true binary outcome $Y$ ($Y=1$ for outcome present, $Y=0$ for outcome absent), the true risks are:\n- Risk in the exposed group: $R_{1} = P(Y=1 \\mid A=1)$\n- Risk in the unexposed group: $R_{0} = P(Y=1 \\mid A=0)$\n\nThe true risk ratio, a measure of effect, is the ratio of these risks:\n$$RR_{\\text{true}} = \\frac{R_{1}}{R_{0}}$$\n\nThe problem provides the baseline risk in the unexposed, $p_{0}$, and the true risk ratio, $RR_{\\text{true}}$. The true risk in the exposed group, $p_{1}$, is derived as $p_{1} = RR_{\\text{true}} \\cdot p_{0}$. A necessary condition is that $p_{1}$ must be a valid probability, i.e., $0 \\le p_{1} \\le 1$.\n\nInformation bias arises from measurement error. Here, the outcome $Y$ is misclassified, leading to an observed outcome $Y^*$. The properties of the measurement tool are given by its sensitivity ($Se$) and specificity ($Sp$):\n- Sensitivity: $Se = P(Y^*=1 \\mid Y=1)$, the probability of correctly identifying a true case.\n- Specificity: $Sp = P(Y^*=0 \\mid Y=0)$, the probability of correctly identifying a true non-case.\n\nFrom these definitions, we derive the probabilities of misclassification:\n- Probability of a false negative: $P(Y^*=0 \\mid Y=1) = 1 - Se$\n- Probability of a false positive: $P(Y^*=1 \\mid Y=0) = 1 - Sp$\n\nThe misclassification is nondifferential, meaning the accuracy of the outcome measurement ($Se$ and $Sp$) does not depend on the exposure status $A$.\n\nThe simulation proceeds algorithmically for each of the $R$ independent runs:\n\nStep 1: Generate True Outcome Counts\nFor a given cohort size of $N_{1}$ exposed and $N_{0}$ unexposed individuals, the number of true outcomes (events) in each group is modeled as a random draw from a binomial distribution.\n- Number of true outcomes in the exposed group: $D_{1, \\text{true}} \\sim \\text{Binomial}(N_{1}, p_{1})$\n- Number of true non-outcomes in the exposed group: $H_{1, \\text{true}} = N_{1} - D_{1, \\text{true}}$\n- Number of true outcomes in the unexposed group: $D_{0, \\text{true}} \\sim \\text{Binomial}(N_{0}, p_{0})$\n- Number of true non-outcomes in the unexposed group: $H_{0, \\text{true}} = N_{0} - D_{0, \\text{true}}$\n\nStep 2: Apply Nondifferential Misclassification\nWe simulate the observation process by applying the classification probabilities to the true counts. The number of observed positives is a sum of correctly identified true positives and incorrectly identified true negatives (false positives).\n- For the exposed group ($A=1$):\n  - Number of true positives observed as positive: $TP_{1} \\sim \\text{Binomial}(D_{1, \\text{true}}, Se)$\n  - Number of true negatives observed as positive: $FP_{1} \\sim \\text{Binomial}(H_{1, \\text{true}}, 1 - Sp)$\n  - Total observed positives in the exposed group: $a = TP_{1} + FP_{1}$\n  - Total observed negatives in the exposed group: $b = N_{1} - a$\n\n- For the unexposed group ($A=0$):\n  - Number of true positives observed as positive: $TP_{0} \\sim \\text{Binomial}(D_{0, \\text{true}}, Se)$\n  - Number of true negatives observed as positive: $FP_{0} \\sim \\text{Binomial}(H_{0, \\text{true}}, 1 - Sp)$\n  - Total observed positives in the unexposed group: $c = TP_{0} + FP_{0}$\n  - Total observed negatives in the unexposed group: $d = N_{0} - c$\n\nStep 3: Compute Observed Risk Ratio ($RR_{\\text{obs}}$)\nThe four values $a, b, c, d$ form the observed $2 \\times 2$ contingency table.\nTo handle potential zero counts in any cell, which would make the risk ratio calculation undefined or unstable, a Haldane–Anscombe continuity correction is applied if any of $a, b, c,$ or $d$ is zero. This involves adding $0.5$ to all four cells. Let the (potentially corrected) cell counts be $a', b', c', d'$.\nThe observed risks are calculated from these counts:\n- Observed risk in the exposed: $R^*_{1} = \\frac{a'}{a'+b'}$\n- Observed risk in the unexposed: $R^*_{0} = \\frac{c'}{c'+d'}$\n\nThe observed risk ratio for the current simulation run is:\n$$RR_{\\text{obs}} = \\frac{R^*_{1}}{R^*_{0}} = \\frac{a'/(a'+b')}{c'/(c'+d')}$$\n\nStep 4: Aggregate Results\nThis process is repeated $R$ times, yielding a distribution of $R$ values for $RR_{\\text{obs}}$. This empirical distribution is then summarized using the following statistics:\n- Mean: $\\text{mean}(RR_{\\text{obs}})$\n- Standard Deviation: $\\text{std}(RR_{\\text{obs}})$\n- Median: $\\text{median}(RR_{\\text{obs}})$\n- Quantiles: The empirical lower ($0.025$) and upper ($0.975$) quantiles, which form an empirical $95\\%$ confidence interval.\n\nFinally, the absolute bias is computed as the difference between the mean of the observed risk ratios and the true risk ratio:\n$$\\text{bias} = \\text{mean}(RR_{\\text{obs}}) - RR_{\\text{true}}$$\nThis entire procedure is implemented for each provided test case, using a fixed pseudorandom seed of $1337$ to ensure reproducibility.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates the effect of nondifferential outcome misclassification on the risk ratio.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    # Each tuple: (N1, N0, p0, RR_true, Se, Sp, R)\n    test_cases = [\n        (2000, 2000, 0.10, 1.5, 0.85, 0.90, 1000),  # Test 1\n        (2000, 2000, 0.10, 1.5, 1.0, 1.0, 1000),    # Test 2\n        (300, 300, 0.10, 1.5, 0.85, 0.90, 1000),    # Test 3\n        (5000, 5000, 0.02, 2.0, 0.85, 0.90, 1000),  # Test 4\n        (5000, 5000, 0.10, 1.5, 0.60, 0.60, 1000),  # Test 5\n    ]\n\n    # Main list to store results for all test cases.\n    all_results = []\n    \n    # Use the new recommended way for random number generation for reproducibility.\n    # Fixed seed as specified in the problem.\n    rng = np.random.default_rng(1337)\n\n    for case in test_cases:\n        N1, N0, p0, RR_true, Se, Sp, R = case\n        \n        # Calculate true risk in the exposed group and validate it.\n        p1 = RR_true * p0\n        if not (0 <= p0 <= 1 and 0 <= p1 <= 1):\n            raise ValueError(f\"Invalid probabilities: p0={p0}, p1={p1}\")\n\n        # List to store observed risk ratios from R runs.\n        observed_rrs = []\n        \n        # Probability of a false positive\n        prob_fp = 1.0 - Sp\n\n        for _ in range(R):\n            # Step 1: Generate true number of outcomes in each group.\n            true_outcomes_exposed = rng.binomial(N1, p1)\n            true_non_outcomes_exposed = N1 - true_outcomes_exposed\n            \n            true_outcomes_unexposed = rng.binomial(N0, p0)\n            true_non_outcomes_unexposed = N0 - true_outcomes_unexposed\n\n            # Step 2: Apply nondifferential misclassification.\n            # Exposed group\n            tp_exposed = rng.binomial(true_outcomes_exposed, Se)\n            fp_exposed = rng.binomial(true_non_outcomes_exposed, prob_fp)\n            a = tp_exposed + fp_exposed\n            b = N1 - a\n            \n            # Unexposed group\n            tp_unexposed = rng.binomial(true_outcomes_unexposed, Se)\n            fp_unexposed = rng.binomial(true_non_outcomes_unexposed, prob_fp)\n            c = tp_unexposed + fp_unexposed\n            d = N0 - c\n\n            # Step 3: Apply Haldane-Anscombe continuity correction if any cell is 0.\n            cell_a, cell_b, cell_c, cell_d = a, b, c, d\n            if a == 0 or b == 0 or c == 0 or d == 0:\n                cell_a += 0.5\n                cell_b += 0.5\n                cell_c += 0.5\n                cell_d += 0.5\n            \n            # Step 4: Compute observed risk ratio.\n            # Check for division by zero in the denominator of risks, though unlikely with correction.\n            risk_exposed_obs = cell_a / (cell_a + cell_b)\n            risk_unexposed_obs = cell_c / (cell_c + cell_d)\n            \n            if risk_unexposed_obs == 0:\n                # If observed risk in unexposed is 0, RR is undefined.\n                # In simulation, this can happen by chance. We store it as NaN and handle it later.\n                # In this specific problem context with continuity correction, this is extremely unlikely.\n                observed_rr = np.nan\n            else:\n                observed_rr = risk_exposed_obs / risk_unexposed_obs\n            \n            observed_rrs.append(observed_rr)\n\n        # Convert to numpy array and remove any NaNs before calculating stats.\n        observed_rrs = np.array(observed_rrs)\n        observed_rrs = observed_rrs[~np.isnan(observed_rrs)]\n\n        # Step 5: Summarize the empirical distribution.\n        mean_rr = np.mean(observed_rrs)\n        std_rr = np.std(observed_rrs) # ddof=0 is the default, population std dev of the empirical dist.\n        median_rr = np.median(observed_rrs)\n        q_025, q_975 = np.quantile(observed_rrs, [0.025, 0.975])\n        \n        # Compute bias.\n        bias = mean_rr - RR_true\n        \n        # Store the 6 required floats for this test case.\n        case_results = [mean_rr, std_rr, median_rr, q_025, q_975, bias]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, so we need to format each inner list of floats.\n    formatted_results = [f\"[{','.join(f'{val:.10f}' for val in res)}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4504834"}]}