## Introduction
Distinguishing causation from mere correlation is a fundamental challenge in preventive medicine and public health. While randomized controlled trials (RCTs) are the gold standard for establishing causality, they are often impractical, unethical, or too costly to implement. Consequently, researchers must rely on observational data to answer critical questions about the effectiveness of interventions and policies. However, drawing valid causal conclusions from such data is fraught with difficulty, primarily due to confounding, where the apparent relationship between a treatment and an outcome is distorted by other factors.

This article addresses this knowledge gap by providing a comprehensive guide to the advanced methods used to estimate causal effects from observational data. It moves beyond basic regression adjustment to equip you with a robust toolkit for navigating the complexities of real-world evidence. Over three chapters, you will gain a principled understanding of how to define, identify, and estimate causal effects with rigor.

The journey begins in "Principles and Mechanisms," where we will lay the theoretical groundwork, introducing the potential outcomes framework, Directed Acyclic Graphs (DAGs), and the core mechanics of propensity score and instrumental variable analyses. Next, "Applications and Interdisciplinary Connections" will demonstrate how these methods are operationalized to solve complex problems in fields ranging from program evaluation and health economics to genetics and AI ethics. Finally, "Hands-On Practices" will provide opportunities to apply these concepts through guided exercises, solidifying your ability to use these powerful tools in your own work.

## Principles and Mechanisms

The estimation of causal effects from data is a central objective in preventive medicine and public health. While randomized controlled trials (RCTs) represent the gold standard for causal inquiry, many critical questions can only be addressed using observational data. This chapter delves into the core principles and mechanisms that underpin advanced causal inference methods, providing a formal framework for defining, identifying, and estimating causal effects in complex, non-ideal settings. We will move from the foundational language of potential outcomes to the graphical models that guide analysis, and then to the specific statistical methods designed to mitigate bias.

### The Potential Outcomes Framework: Defining Causal Effects

The modern conception of causality in statistics is built upon the **potential outcomes framework**. To formalize the effect of a binary treatment or exposure, let $T$ be an [indicator variable](@entry_id:204387) where $T=1$ denotes receiving the treatment and $T=0$ denotes the control condition. For any given individual $i$, we can imagine two potential states of the world: the outcome that *would have been observed* if the individual received the treatment, denoted $Y_i(1)$, and the outcome that *would have been observed* if the individual received the control, denoted $Y_i(0)$. The **individual causal effect** is the difference between these two potential outcomes: $\tau_i = Y_i(1) - Y_i(0)$.

The fundamental challenge, often called the **fundamental problem of causal inference**, is that for any given individual, we can only observe one of these potential outcomes. The observed outcome, $Y_i$, is the potential outcome corresponding to the treatment actually received: $Y_i = T_i Y_i(1) + (1-T_i)Y_i(0)$. The other potential outcome is counterfactual and remains unobserved. Consequently, we cannot directly measure individual causal effects. Instead, our goal is to estimate the average causal effect across a population of interest.

Two of the most important population-level causal estimands are the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT).

-   The **Average Treatment Effect (ATE)** is the expected value of the individual causal effect across the entire population of interest. It addresses the question: "What would be the average effect if everyone in the population received the treatment compared to if no one received it?" Formally, the ATE is defined as:
    $ATE = \mathbb{E}[Y(1) - Y(0)]$

-   The **Average Treatment Effect on the Treated (ATT)** is the average causal effect specifically for the subpopulation of individuals who, in reality, received the treatment. It answers the question: "For those who actually received the treatment, what was its average effect?" The ATT is defined as the expected individual causal effect, conditional on having received the treatment:
    $ATT = \mathbb{E}[Y(1) - Y(0) \mid T=1]$

In a study of a preventive intervention like influenza vaccination, the ATE quantifies the expected benefit if the vaccine were universally administered to an entire eligible population, whereas the ATT quantifies the expected benefit experienced by those who chose to be vaccinated [@problem_id:4501620]. In an ideal randomized trial, ATE and ATT will be equal. However, in observational studies, these estimands often differ because the group that chooses to get treated may be systematically different from the group that does not. For instance, individuals at higher risk might be more likely to seek vaccination, leading to a different ATT than ATE.

This entire framework rests on a critical, often implicit, assumption known as the **Stable Unit Treatment Value Assumption (SUTVA)**. SUTVA is a composite assumption that comprises two conditions necessary for the potential outcomes $Y_i(1)$ and $Y_i(0)$ to be well-defined for each individual [@problem_id:4501717].

1.  **No Interference**: This condition states that the potential outcomes for any individual $i$ depend only on their own treatment assignment and are not affected by the treatment assignments of other individuals. If interference exists, the potential outcome for individual $i$ is not simply $Y_i(a_i)$, but rather a more complex function of the entire vector of treatment assignments across all individuals, $Y_i(a_i, \mathbf{a}_{-i})$, where $\mathbf{a}_{-i}$ represents the treatment assignments of everyone else. In a smoking cessation program, for example, if one participant's success is influenced by whether their peers in the program also quit (e.g., through social support or reduced secondhand smoke exposure), the no-interference assumption is violated. In such cases, the simple potential outcome $Y_i(1)$ is ill-defined because its value depends on which other individuals also received the treatment.

2.  **Consistency**: This condition requires that for any treatment level, there is only one version of that treatment. If an individual $i$ receives treatment $T_i=a$, their observed outcome $Y_i$ is equal to their potential outcome $Y_i(a)$. This assumption is violated if there are hidden or unspecified variations in how the treatment is implemented. For instance, if a smoking cessation counseling "offer" ($T_i=1$) involves high-intensity counseling when few people enroll but low-intensity counseling when many people enroll due to capacity constraints, then $T=1$ does not correspond to a single, well-defined intervention. The potential outcome $Y_i(1)$ becomes ambiguous, as it is unclear to which version of the counseling it corresponds [@problem_id:4501717].

### Identifying Causal Effects: Confounding and Its Control

In observational studies, the primary obstacle to estimating causal effects is **confounding**. Confounding occurs when a variable, or a set of variables, is a common cause of both the treatment and the outcome. This common cause creates a non-causal association between the treatment and outcome that can bias the estimated effect. The simple associational difference, $\mathbb{E}[Y \mid T=1] - \mathbb{E}[Y \mid T=0]$, is generally not equal to the ATE precisely because of confounding.

#### Directed Acyclic Graphs (DAGs)

**Directed Acyclic Graphs (DAGs)** are powerful graphical tools for explicitly representing our assumptions about the causal structure of a problem. In a DAG, nodes represent variables, and directed arrows ($A \to B$) represent a direct causal effect of $A$ on $B$. The absence of an arrow represents the assumption of no direct causal effect.

Confounding is represented in a DAG as a **back-door path**: a path between the treatment $T$ and outcome $Y$ that begins with an arrow pointing into $T$. For example, consider a study on a preventive screening policy ($A$) and its effect on mortality ($Y$). If an individual's underlying health-seeking tendency ($U$) influences both their likelihood of accepting the screening offer ($U \to A$) and their mortality risk through other behaviors ($U \to Y$), the path $A \leftarrow U \to Y$ is a back-door path. This path creates a non-causal association between $A$ and $Y$, confounding the true causal effect represented by the path $A \to Y$ [@problem_id:4501630].

To identify the causal effect, we must block all such back-door paths. The standard way to do this is by **conditioning** on (i.e., adjusting for, stratifying on, or matching on) one or more variables that lie on these paths. However, choosing the correct adjustment set is not trivial and is fraught with pitfalls. A critical error is conditioning on a **[collider](@entry_id:192770)**. A [collider](@entry_id:192770) is a variable on a path that receives arrows from two other variables, such as $A \to C \leftarrow U$. A path containing a [collider](@entry_id:192770) is naturally blocked. However, conditioning on the collider *unblocks* the path, inducing a statistical association between its causes ($A$ and $U$). In our screening example, if the screening offer ($A$) and health-seeking behavior ($U$) both influence whether a person gets a clinic appointment ($C$), then $C$ is a [collider](@entry_id:192770). Adjusting for $C$ would induce a spurious association between $A$ and $U$, creating a new non-causal pathway to the outcome $Y$ ($A \leftrightarrow U \to Y$) and biasing the effect estimate [@problem_id:4501630]. This is known as **[collider](@entry_id:192770)-stratification bias**.

#### Propensity Score Methods

For high-dimensional covariate settings, adjusting for all necessary confounders simultaneously can be difficult. **Propensity score (PS) methods** provide a powerful alternative. The propensity score, $e(X)$, is defined as an individual's probability of receiving the treatment, conditional on their measured baseline covariates $X$: $e(X) = P(T=1 \mid X)$. The central idea is that if we can successfully condition on the propensity score, we can achieve balance in the distribution of all measured covariates $X$ between the treated and untreated groups. In essence, within strata of the propensity score, treatment assignment is "as-if" random, allowing for an unbiased comparison.

The validity of PS methods relies on three key assumptions:

1.  **Conditional Exchangeability**: All confounders of the treatment-outcome relationship are measured and included in the set of covariates $X$ used to model the [propensity score](@entry_id:635864). This is also known as the "no unmeasured confounders" assumption.
2.  **Positivity (or Overlap)**: For every level of the covariates $X$, there is a non-zero probability of being both treated and untreated. That is, $0 \lt P(T=1 \mid X) \lt 1$. This is a fundamental requirement for causal identification. If, for a certain type of patient (e.g., the very old with severe comorbidities), everyone receives the treatment, then we have no data on what would have happened to such patients had they not been treated. Causal comparison is impossible for this subgroup [@problem_id:4501673]. When this assumption is nearly violated (i.e., propensity scores are close to 0 or 1), methods like Inverse Probability of Treatment Weighting (IPTW) become highly unstable, as the weights ($1/e(X)$ or $1/(1-e(X))$) become extremely large, leading to estimators with high variance [@problem_id:4501673].
3.  **SUTVA**, as described previously.

A crucial aspect of applying PS methods is the selection of covariates for the propensity score model. The goal is to satisfy conditional exchangeability. Based on our understanding of causal structures from DAGs, the PS model should include all measured confounders (common causes of $T$ and $Y$). Crucially, it should **not** include variables that are solely instruments for treatment, mediators, or colliders. Including a strong instrument (a variable that strongly predicts $T$ but does not affect $Y$ otherwise) can harm the analysis by creating near-violations of positivity. Including a mediator will block a portion of the causal effect, and including a [collider](@entry_id:192770) will induce bias [@problem_id:4501616].

Finally, a common misconception is that the quality of a [propensity score](@entry_id:635864) model should be judged by its ability to predict the treatment assignment (e.g., using metrics like the Area Under the Curve, or AUC). This is incorrect. The purpose of the PS in causal inference is not prediction, but **covariate balance**. Therefore, the success of a PS model must be judged by its ability to produce treated and control groups that are similar with respect to the measured covariates after matching or weighting. Diagnostics such as the **Standardized Mean Difference (SMD)**, which should ideally be below 0.1 for all covariates, are the proper tools for assessing the model. A model with a lower AUC that achieves excellent balance is far superior for causal inference than a model with a very high AUC that leaves substantial imbalance between the groups [@problem_id:4501605].

### Instrumental Variable Methods: Leveraging "As-If" Randomization

What happens when we cannot measure all confounders? In this scenario, methods based on conditional exchangeability, like [propensity score](@entry_id:635864) analysis, will yield biased results. **Instrumental Variable (IV) analysis** offers an alternative path to causal inference in the presence of unmeasured confounding.

An [instrumental variable](@entry_id:137851), $Z$, is a variable that is related to the treatment $T$ but is not associated with the outcome $Y$ through any confounding pathways. To be a valid instrument, $Z$ must satisfy three core assumptions [@problem_id:4501582]:

1.  **Relevance**: The instrument must be associated with the treatment. Mathematically, $Cov(Z, T) \neq 0$. In a study using physician encouragement ($Z$) as an instrument for vaccination ($T$), this means the encouragement must actually change vaccination rates.
2.  **Independence (or Exogeneity)**: The instrument must be independent of any unmeasured confounders of the treatment-outcome relationship. In essence, the instrument itself is "as-if" randomly assigned with respect to the factors that determine the outcome. Graphically, this means there are no open back-door paths from $Z$ to $Y$. Randomly assigning physicians to receive an encouragement prompt would satisfy this, whereas assigning it based on patient risk would violate it.
3.  **Exclusion Restriction**: The instrument can only affect the outcome *through* its effect on the treatment. There is no direct causal pathway from $Z$ to $Y$. For example, the physician encouragement must not affect a patient's hospitalization risk by any other means, such as by promoting other preventive behaviors like mask use, independent of vaccination status.

Under these conditions, the IV estimand, often called the **Wald estimand**, is calculated as the ratio of the effect of the instrument on the outcome to the effect of the instrument on the treatment:
$$ \text{IV Estimand} = \frac{\mathbb{E}[Y \mid Z=1] - \mathbb{E}[Y \mid Z=0]}{\mathbb{E}[T \mid Z=1] - \mathbb{E}[T \mid Z=0]} $$

The numerator of this ratio is the **Intention-to-Treat (ITT)** effect on the outcome. In a randomized encouragement trial, this is the causal effect of being *assigned* to the encouragement group, regardless of whether the treatment was actually taken [@problem_id:4501603]. The denominator is the effect of the instrument on treatment uptake, which measures the degree of compliance with the encouragement.

The Wald estimand does not, in general, recover the ATE. Instead, under the three IV assumptions plus an additional **Monotonicity** assumption (which states that there are no "defiers"—individuals who would take the treatment only if not encouraged, and not take it if encouraged), the IV estimand identifies the **Local Average Treatment Effect (LATE)** [@problem_id:4501603]. The LATE is the average treatment effect specifically for the subpopulation of **compliers**—those individuals who are induced to take the treatment by the instrument.

For example, in a vaccination encouragement campaign where the encouragement increases vaccination rates from $0.20$ to $0.40$ and reduces influenza incidence from $0.15$ to $0.10$, the ITT effect is $0.10 - 0.15 = -0.05$. The increase in vaccination is $0.40 - 0.20 = 0.20$. The LATE is therefore $(-0.05)/(0.20) = -0.25$, which is the average effect of the vaccine among the $20\%$ of the population who were compliers [@problem_id:4501603].

It is critical to distinguish LATE from ATE. The LATE is an average effect for a specific, "local" subpopulation defined by their response to the instrument. If treatment effects are heterogeneous and compliers are systematically different from the general population, the LATE may differ substantially from the ATE. For instance, if an encouragement program for vaccination primarily motivates younger, healthier individuals (who have a smaller absolute risk reduction from the vaccine) to get vaccinated, the resulting LATE will be smaller in magnitude than the ATE for the entire population, which includes older, higher-risk individuals [@problem_id:4501669]. An IV analysis provides a valid causal effect, but for a potentially unrepresentative subgroup, a nuance that must be carefully considered when translating findings into policy.

### Causal Inference with Time-Varying Treatments

The principles discussed thus far become more complex in longitudinal settings where treatments and confounders are measured repeatedly over time. A common and challenging scenario is the presence of **time-dependent confounding**. This occurs when a time-varying covariate, say $L_t$, is both a confounder for a future treatment decision $T_t$ and is itself affected by a past treatment $T_{t-1}$. For example, a patient's blood pressure at visit $t$ ($L_t$) might influence a physician's decision to provide counseling ($T_t$), while also being an outcome of counseling received at visit $t-1$ ($T_{t-1}$).

In such settings, the assumption of conditional exchangeability must be extended to a sequential context. **Sequential exchangeability** requires that at every time point $t$, the treatment assigned, $T_t$, is independent of the potential outcomes, conditional on the observed past covariate and treatment history ($\bar{L}_t$ and $\bar{T}_{t-1}$) [@problem_id:4501638].

Standard statistical methods, such as a regression model of the final outcome $Y$ on the entire treatment history $\bar{T}_K$, fail in the presence of time-dependent confounding. The reason is a fundamental [structural bias](@entry_id:634128). To estimate the effect of treatment $T_t$, one must adjust for the confounder $L_t$. However, because $L_t$ is also an intermediate on the causal pathway from a prior treatment $T_{t-1}$ to the outcome $Y$ (i.e., $T_{t-1} \to L_t \to Y$), adjusting for $L_t$ blocks this pathway. This biases the estimate of the total effect of $T_{t-1}$ [@problem_id:4501638]. Standard regression models are caught in an impossible bind: they cannot simultaneously adjust for confounding of current treatments while correctly estimating the total effects of past treatments. This challenge necessitates specialized methods, such as Marginal Structural Models or the g-formula, which are designed to correctly handle this treatment-confounder feedback loop.