{"hands_on_practices": [{"introduction": "A common task in preventive medicine is to estimate the average effect of an intervention, such as a change in a clinical measurement. This practice provides a fundamental skill: constructing a confidence interval for a population mean when the population's true variability is unknown, a common scenario in real-world research [@problem_id:4514184]. By using the Student's $t$-distribution, you will learn to accurately quantify the uncertainty around an estimated mean effect and understand how sample size influences the precision of your estimate.", "problem": "A preventive medicine team evaluates the impact of a low-sodium dietary counseling program on systolic blood pressure in adults enrolled from community health screenings. From a simple random sample of $n=16$ participants who completed $8$ weeks of counseling, the team observes a sample mean reduction of $\\bar{x}=5.4$ millimeters of mercury (mmHg) and a sample standard deviation of $s=7.2$ mmHg. The population standard deviation is unknown. Using foundational definitions of sampling distributions and the Student’s $t$ distribution, construct a $95\\%$ confidence interval for the population mean reduction, and explain how degrees of freedom influence the interval’s width through the sampling variability term and the quantile used. Then, compute the width of the $95\\%$ confidence interval for the population mean reduction using these data. Round your numerical result to four significant figures. Express the width in millimeters of mercury (mmHg).", "solution": "The problem requires the construction of a $95\\%$ confidence interval for the population mean reduction in systolic blood pressure, an explanation of the role of degrees of freedom, and the computation of the interval's width.\n\nFirst, the necessary parameters from the problem statement are identified:\nSample size, $n = 16$.\nSample mean reduction, $\\bar{x} = 5.4$ mmHg.\nSample standard deviation, $s = 7.2$ mmHg.\nThe confidence level is specified as $95\\%$.\n\nSince the population standard deviation ($\\sigma$) is unknown and the data are from a simple random sample, the appropriate statistical framework is the Student's $t$-distribution. The formula for a confidence interval for the population mean, $\\mu$, is given by:\n$$ \\text{CI} = \\bar{x} \\pm t_{\\alpha/2, df} \\cdot \\frac{s}{\\sqrt{n}} $$\nwhere $\\bar{x}$ is the sample mean, $s$ is the sample standard deviation, $n$ is the sample size, $df$ represents the degrees of freedom, and $t_{\\alpha/2, df}$ is the critical value from the $t$-distribution for a given confidence level.\n\nThe components of this formula are determined as follows:\nThe degrees of freedom ($df$) are calculated as $df = n - 1$.\n$$ df = 16 - 1 = 15 $$\nFor a $95\\%$ confidence level, the significance level $\\alpha$ is:\n$$ \\alpha = 1 - 0.95 = 0.05 $$\nThe critical value is found for a two-tailed probability of $\\alpha$, so we use $\\alpha/2$:\n$$ \\frac{\\alpha}{2} = \\frac{0.05}{2} = 0.025 $$\nWe need to find the critical $t$-value, $t_{0.025, 15}$, which is the value such that the area in the upper tail of the $t$-distribution with $15$ degrees of freedom is $0.025$. Consulting a standard $t$-distribution table or using statistical software, we find:\n$$ t_{0.025, 15} = 2.131 $$\nThe standard error of the mean (SEM), which quantifies the variability of the sample mean, is calculated as:\n$$ \\text{SEM} = \\frac{s}{\\sqrt{n}} = \\frac{7.2}{\\sqrt{16}} = \\frac{7.2}{4} = 1.8 \\, \\text{mmHg} $$\nThe margin of error (ME) is the product of the critical value and the standard error:\n$$ \\text{ME} = t_{0.025, 15} \\cdot \\text{SEM} = 2.131 \\cdot 1.8 = 3.8358 \\, \\text{mmHg} $$\nWith the margin of error, we can construct the $95\\%$ confidence interval for the population mean reduction $\\mu$:\n$$ \\text{CI} = 5.4 \\pm 3.8358 $$\nThis gives an interval from $5.4 - 3.8358 = 1.5642$ to $5.4 + 3.8358 = 9.2358$. So, the $95\\%$ confidence interval is $(1.5642, 9.2358)$ mmHg.\n\nNext, we address how degrees of freedom influence the interval's width. The width ($W$) of the confidence interval is twice the margin of error:\n$$ W = 2 \\cdot \\text{ME} = 2 \\cdot t_{\\alpha/2, df} \\cdot \\frac{s}{\\sqrt{n}} $$\nThe degrees of freedom, $df = n-1$, play a crucial role by determining the specific shape of the Student's $t$-distribution used to find the critical value $t_{\\alpha/2, df}$. The $t$-distribution is used instead of the normal ($Z$) distribution to account for the additional uncertainty introduced by estimating the unknown population standard deviation $\\sigma$ with the sample standard deviation $s$. This uncertainty, which is a component of the overall sampling variability, is greater for smaller sample sizes.\n\nThe $t$-distribution has heavier tails than the standard normal distribution, and the \"heaviness\" of these tails is inversely related to the degrees of freedom. For a small number of degrees of freedom (i.e., a small sample size), the tails are very heavy, meaning that a larger portion of the probability lies far from the mean. To capture a certain level of confidence (e.g., $95\\%$), one must go further out into the tails. This translates to a larger critical value $t_{\\alpha/2, df}$. For example, $t_{0.025, 5} \\approx 2.571$, whereas $t_{0.025, 15} = 2.131$, and as $df \\to \\infty$, the $t$-distribution converges to the $Z$-distribution, with $t_{0.025, \\infty} = z_{0.025} \\approx 1.96$.\n\nSince the width of the confidence interval, $W$, is directly proportional to the critical value $t_{\\alpha/2, df}$, a smaller number of degrees of freedom results in a larger critical value and, consequently, a wider confidence interval (assuming all other factors are constant). This wider interval correctly reflects the increased uncertainty and greater sampling variability associated with a smaller sample. Conversely, as the degrees of freedom increase with a larger sample size, the estimate $s$ becomes a more reliable approximation of $\\sigma$, the critical value $t_{\\alpha/2, df}$ decreases, and the confidence interval becomes narrower, indicating a more precise estimate of the population mean.\n\nFinally, we compute the width of the $95\\%$ confidence interval for the given data. Using the previously calculated margin of error:\n$$ W = 2 \\cdot \\text{ME} = 2 \\cdot 3.8358 = 7.6716 \\, \\text{mmHg} $$\nThe problem requires this result to be rounded to four significant figures.\n$$ W \\approx 7.672 \\, \\text{mmHg} $$\nThis value is the width of the $95\\%$ confidence interval for the population mean reduction in systolic blood pressure.", "answer": "$$ \\boxed{7.672} $$", "id": "4514184"}, {"introduction": "Public health and preventive medicine are filled with questions about proportions: the prevalence of a disease, the coverage of a vaccine, or the uptake of a new service. This exercise challenges you to move beyond simplistic formulas to derive and apply the Wilson score interval, a more accurate and reliable method for constructing a confidence interval for a proportion [@problem_id:4514249]. Understanding this method is crucial because it provides trustworthy results even in situations where more common methods fail, such as when dealing with rare events.", "problem": "A public health team evaluates uptake of a new smoking cessation referral in a preventive medicine clinic. Among $n = 200$ consecutively approached adult smokers, $x = 24$ accepted referral during the first month. Assume $X \\sim \\mathrm{Binomial}(n, p)$, where $p$ is the true uptake probability. Using only fundamental definitions from the binomial model and the large-sample normal approximation of score-type test statistics, derive from first principles the two-sided $95\\%$ Wilson score confidence interval (confidence interval (CI)) for the binomial proportion $p$ by inverting the appropriate score test. Then, apply your derived expression to these data to compute the lower endpoint of the interval. Finally, explain, in terms of coverage probability and finite-sample behavior, why this interval achieves more reliable coverage than the Wald interval in preventive medicine surveillance applications.\n\nExpress the final numerical answer as the lower endpoint of the two-sided $95\\%$ Wilson CI for $p$ as a decimal. Round your answer to four significant figures.", "solution": "This problem requires three parts: deriving the Wilson score confidence interval from first principles, applying it to the given data to find the lower endpoint, and explaining its advantages over the Wald interval.\n\n**Part 1: Derivation of the Wilson Score Confidence Interval**\n\nA $(1-\\alpha)$ confidence interval for a parameter can be constructed by inverting the corresponding hypothesis test. We find the set of all possible values of the true proportion, $p$, for which we would *not* reject the null hypothesis $H_0: p = p_0$ at significance level $\\alpha$.\n\nThe score test statistic for a binomial proportion is based on the large-sample normal approximation. Under the null hypothesis $H_0: p = p_0$, the statistic is:\n$$ Z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} $$\nwhere $\\hat{p} = x/n$ is the sample proportion. This statistic follows an approximate standard normal distribution, $Z \\sim N(0, 1)$.\n\nTo form the confidence interval, we find the set of all values of $p$ that satisfy $|Z| \\leq z_{\\alpha/2}$, where $z_{\\alpha/2}$ is the critical value from the standard normal distribution (for a 95% CI, $z_{0.025} \\approx 1.96$). Squaring the inequality gives:\n$$ Z^2 = \\frac{(\\hat{p} - p)^2}{p(1-p)/n} \\leq z_{\\alpha/2}^2 $$\nThis must be solved for $p$. Rearranging the inequality:\n$$ n(\\hat{p} - p)^2 \\leq z_{\\alpha/2}^2 p(1-p) $$\n$$ n(\\hat{p}^2 - 2\\hat{p}p + p^2) \\leq z_{\\alpha/2}^2 p - z_{\\alpha/2}^2 p^2 $$\nCollecting terms gives a quadratic inequality in $p$ of the form $Ap^2 + Bp + C \\leq 0$:\n$$ (n + z_{\\alpha/2}^2)p^2 - (2n\\hat{p} + z_{\\alpha/2}^2)p + n\\hat{p}^2 \\leq 0 $$\nThe roots of the corresponding quadratic equation $Ap^2 + Bp + C = 0$ define the endpoints of the confidence interval. Using the quadratic formula, the roots are found to be:\n$$ p = \\frac{(2n\\hat{p} + z_{\\alpha/2}^2) \\pm \\sqrt{(2n\\hat{p} + z_{\\alpha/2}^2)^2 - 4(n + z_{\\alpha/2}^2)(n\\hat{p}^2)}}{2(n + z_{\\alpha/2}^2)} $$\nAfter simplification and dividing the numerator and denominator by $2(n + z_{\\alpha/2}^2)$, we get the standard expression for the Wilson score interval endpoints:\n$$ p = \\frac{\\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{\\alpha/2}^2}{4n^2}}}{1 + \\frac{z_{\\alpha/2}^2}{n}} $$\n\n**Part 2: Application to Data**\n\nWe are given $n=200$ and $x=24$. The sample proportion is $\\hat{p} = \\frac{24}{200} = 0.12$. For a 95% confidence interval, $\\alpha = 0.05$, and the critical value is $z_{0.025} \\approx 1.96$.\n\nWe plug these values into the formula for the lower endpoint:\n$$ p_{\\text{lower}} = \\frac{\\hat{p} + \\frac{z_{0.025}^2}{2n} - z_{0.025}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{0.025}^2}{4n^2}}}{1 + \\frac{z_{0.025}^2}{n}} $$\nLet's compute the components:\n- $z_{0.025}^2 = 1.96^2 \\approx 3.8416$\n- Numerator center adjustment: $\\hat{p} + \\frac{z^2}{2n} = 0.12 + \\frac{3.8416}{400} \\approx 0.129604$\n- Denominator: $1 + \\frac{z^2}{n} = 1 + \\frac{3.8416}{200} \\approx 1.019208$\n- Term under the square root: $\\frac{0.12(0.88)}{200} + \\frac{3.8416}{4(200)^2} \\approx 0.000528 + 0.00002401 = 0.00055201$\n- Margin of error component: $1.96 \\times \\sqrt{0.00055201} \\approx 1.96 \\times 0.023495 \\approx 0.046050$\n\nCombining for the lower endpoint:\n$$ p_{\\text{lower}} = \\frac{0.129604 - 0.046050}{1.019208} = \\frac{0.083554}{1.019208} \\approx 0.081980... $$\nRounding to four significant figures, the lower endpoint is $0.08198$.\n\n**Part 3: Comparison of Wilson and Wald Intervals**\n\nThe Wilson score interval has more reliable coverage properties than the more common Wald interval ($\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\hat{p}(1-\\hat{p})/n}$), particularly in preventive medicine applications.\n\n1.  **Coverage Probability**: The actual proportion of times the calculated interval contains the true parameter $p$ is known as its coverage probability. For the Wald interval, the actual coverage can be erratic and often falls well below the nominal level (e.g., 95%), especially when $p$ is close to 0 or 1, or when $n$ is small. The Wilson interval's coverage probability is much more stable and consistently closer to the nominal level across the full range of $p$.\n\n2.  **Boundary Issues**: In surveillance, one might observe zero events ($x=0$). The Wald interval formula yields a standard error of 0, resulting in an absurd zero-width interval $[0,0]$ that wrongly implies perfect certainty. The Wilson interval does not fail in this way and produces a sensible interval (e.g., $[0, \\frac{z^2}{n+z^2}]$).\n\n3.  **Symmetry**: The Wald interval is always symmetric around $\\hat{p}$. The underlying binomial distribution, however, is skewed unless $p=0.5$. The Wilson interval is not centered on $\\hat{p}$ and is asymmetric, which allows it to better approximate the shape of the true sampling distribution, leading to improved performance.\n\nThese properties make the Wilson score interval a superior choice for professional use, especially in public health surveillance where proportions may be extreme and sample sizes variable.", "answer": "$$\\boxed{0.08198}$$", "id": "4514249"}, {"introduction": "A crucial part of interpreting evidence is comparing outcomes between two different groups, such as in a clinical trial or a comparison of two regions. It is tempting to simply check if the confidence intervals for the two groups overlap to declare a difference \"significant\" or not, but this approach can be misleading. This exercise uses a realistic public health scenario to demonstrate from first principles why this visual check is unreliable and reinforces the correct statistical approach for comparing two independent estimates [@problem_id:4514260].", "problem": "A preventive medicine team evaluates a community vaccination campaign. Two independent cross-sectional surveys are conducted in District $A$ and District $B$ after the campaign. In District $A$, $n_A = 1000$ children are sampled and the observed measles vaccination coverage is $\\hat{p}_A = 0.60$. In District $B$, $n_B = 1000$ children are sampled and the observed coverage is $\\hat{p}_B = 0.65$. Assume the sampling in each district is independent and that the large-sample normal approximation for sample proportions is appropriate.\n\nThe team reports that the two district-specific $95\\%$ confidence intervals overlap when visualized. A debate arises about whether this overlap implies that the difference in coverage between districts is not statistically significant at level $\\alpha = 0.05$.\n\nUsing only fundamental definitions and well-tested facts about sampling distributions (for example, the Central Limit Theorem for proportions and the notion that a confidence interval quantifies uncertainty about a single parameter), reason from first principles to determine which statements are correct. You may assume a two-sided test for the difference in coverage.\n\nWhich of the following statements are correct?\n\nA. Overlapping $95\\%$ confidence intervals for two independent estimates can still be associated with a statistically significant difference at level $\\alpha = 0.05$ because testing a difference is based on the sampling distribution of the difference, not on the overlap of intervals for the individual parameters.\n\nB. Overlapping $95\\%$ confidence intervals always imply $p > 0.05$ for the difference between independent estimates, since each interval already contains values consistent with no difference.\n\nC. If two $95\\%$ confidence intervals overlap, a statistically significant difference at level $\\alpha = 0.05$ can occur only if the samples are dependent; independence rules out significance under overlap.\n\nD. Non-overlap of $95\\%$ confidence intervals is exactly equivalent to a two-sample test for difference at $\\alpha = 0.05$, so overlap and significance cannot co-occur.\n\nE. For independent estimates with similar standard errors, non-overlap of approximately $83\\%$ confidence intervals is a better visual heuristic for detecting $p  0.05$ in the difference; $95\\%$ intervals are too wide to serve that purpose reliably.\n\nSelect all that apply.", "solution": "We begin from first principles. For a sample proportion $\\hat{p}$ from a large sample of size $n$, the Central Limit Theorem (CLT) implies the sampling distribution is approximately normal with mean $p$ and variance $p(1-p)/n$. Thus,\n$$\n\\hat{p} \\approx \\mathcal{N}\\!\\left(p,\\; \\frac{p(1-p)}{n}\\right).\n$$\nA two-sided $95\\%$ confidence interval (CI) for $p$ is constructed as\n$$\n\\hat{p} \\pm z_{0.975}\\,\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\n$$\nwhere $z_{0.975} \\approx 1.96$ is the $97.5$th percentile of the standard normal distribution. This CI quantifies uncertainty about a single parameter $p$; it is not, by construction, a direct test about the difference between two parameters.\n\nCompute the $95\\%$ confidence intervals for the two districts to verify overlap. For District $A$,\n$$\n\\text{SE}_A = \\sqrt{\\frac{\\hat{p}_A\\,(1-\\hat{p}_A)}{n_A}} = \\sqrt{\\frac{0.60\\cdot 0.40}{1000}} = \\sqrt{0.00024} \\approx 0.01549,\n$$\nand\n$$\n\\text{CI}_A: \\quad 0.60 \\pm 1.96 \\times 0.01549 \\approx 0.60 \\pm 0.0304 \\Rightarrow [0.5696,\\; 0.6304].\n$$\nFor District $B$,\n$$\n\\text{SE}_B = \\sqrt{\\frac{\\hat{p}_B\\,(1-\\hat{p}_B)}{n_B}} = \\sqrt{\\frac{0.65\\cdot 0.35}{1000}} = \\sqrt{0.0002275} \\approx 0.01508,\n$$\nand\n$$\n\\text{CI}_B: \\quad 0.65 \\pm 1.96 \\times 0.01508 \\approx 0.65 \\pm 0.0296 \\Rightarrow [0.6204,\\; 0.6796].\n$$\nThese intervals overlap on $[0.6204,\\; 0.6304]$.\n\nTo test whether the coverage differs between districts, we analyze the sampling distribution of the difference $\\hat{p}_B - \\hat{p}_A$ under independence. For independent estimates, the variance of the difference equals the sum of the variances:\n$$\n\\text{Var}(\\hat{p}_B - \\hat{p}_A) = \\text{Var}(\\hat{p}_B) + \\text{Var}(\\hat{p}_A),\n$$\nso the standard error (SE) of the difference is\n$$\n\\text{SE}_{\\text{diff}} = \\sqrt{\\text{SE}_A^2 + \\text{SE}_B^2} = \\sqrt{(0.01549)^2 + (0.01508)^2} = \\sqrt{0.0002399 + 0.0002274} = \\sqrt{0.0004673} \\approx 0.02161.\n$$\nThe observed difference is\n$$\n\\hat{p}_B - \\hat{p}_A = 0.65 - 0.60 = 0.05.\n$$\nA two-sided $z$ test for equality of proportions (using the large-sample normal approximation with separate variances) has test statistic\n$$\nz = \\frac{\\hat{p}_B - \\hat{p}_A}{\\text{SE}_{\\text{diff}}} = \\frac{0.05}{0.02161} \\approx 2.313.\n$$\nThe corresponding two-sided $p$-value is approximately\n$$\np \\approx 2\\left(1 - \\Phi(2.313)\\right) \\approx 2\\left(1 - 0.9896\\right) \\approx 0.0208,\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal. Thus, despite the overlap of the $95\\%$ confidence intervals, the difference is statistically significant at level $\\alpha = 0.05$.\n\nConceptually, why does overlap not decide significance? Each $95\\%$ CI is calibrated to contain its own district’s true coverage $p_A$ or $p_B$ with probability $0.95$ over repeated samples. The test of difference uses the sampling distribution of $\\hat{p}_B - \\hat{p}_A$, which has its own variability governed by the combined standard error $\\sqrt{\\text{SE}_A^2 + \\text{SE}_B^2}$. Visual overlap of separate $95\\%$ CIs is neither necessary nor sufficient for $p \\geq 0.05$ in the difference. In fact, non-overlap of $95\\%$ CIs is a conservative visual rule: when standard errors are similar, “just-touching” $95\\%$ CIs implies a test statistic larger than the $1.96$ threshold.\n\nA useful heuristic can be derived from first principles. Suppose two independent estimates have equal standard error $\\text{SE}$. If their two-sided CIs with half-width $z\\,\\text{SE}$ just touch, the difference between point estimates equals $2z\\,\\text{SE}$. The $z$ test statistic for the difference is\n$$\nz_{\\text{diff}} = \\frac{2z\\,\\text{SE}}{\\sqrt{2}\\,\\text{SE}} = \\sqrt{2}\\,z.\n$$\nSetting $z_{\\text{diff}} = 1.96$ (the two-sided $5\\%$ threshold) yields $z = \\frac{1.96}{\\sqrt{2}} \\approx 1.386$. A two-sided confidence interval with half-width $1.386\\,\\text{SE}$ corresponds to approximately an $83\\%$ confidence level (since $1.386$ is the $91.7$th percentile for a one-sided tail, leading to about $83\\%$ two-sided coverage). Therefore, non-overlap of about $83\\%$ CIs is a better visual proxy for $p  0.05$ in the difference than non-overlap of $95\\%$ CIs.\n\nOption-by-option analysis:\n\n- Option A: Correct. It states the fundamental reason: inference about the difference uses the sampling distribution of $\\hat{p}_B - \\hat{p}_A$ and its combined standard error, not the overlap of individual $95\\%$ CIs. Our computation shows overlap coexists with $p \\approx 0.021$.\n\n- Option B: Incorrect. Overlap of $95\\%$ CIs does not guarantee $p > 0.05$ for the difference; our numerical example yields $p \\approx 0.021$ despite overlap. The claim misunderstands what a CI represents.\n\n- Option C: Incorrect. Independence does not preclude significance under overlap. As demonstrated, with independent samples we obtained a significant difference even though the $95\\%$ CIs overlapped. Dependence is not required.\n\n- Option D: Incorrect. Non-overlap of $95\\%$ CIs is not exactly equivalent to a two-sample test at $\\alpha = 0.05$; it is conservative. For similar standard errors, “just-touching” $95\\%$ CIs imply $z_{\\text{diff}} \\approx 2.77$ and $p \\ll 0.05$, not $p = 0.05$, highlighting non-equivalence.\n\n- Option E: Correct. The derivation above shows that non-overlap of approximately $83\\%$ CIs corresponds to $z_{\\text{diff}} \\approx 1.96$ for independent estimates with similar standard errors, making it a more accurate visual heuristic for $p  0.05$ than non-overlap of $95\\%$ CIs.\n\nTherefore, the correct choices are $A$ and $E$.", "answer": "$$\\boxed{AE}$$", "id": "4514260"}]}