{"hands_on_practices": [{"introduction": "A core function of public health surveillance is to generate timely alerts for potential health threats, but an alert's reliability depends on more than just the system's accuracy. This practice explores the concept of Positive Predictive Value ($PPV$), revealing how the low prevalence of a disease can dramatically reduce the confidence we have in any single positive signal [@problem_id:4565209]. By deriving the $PPV$ from first principles, you will gain a crucial understanding of the base rate fallacy and its profound implications for interpreting surveillance data.", "problem": "A regional health department operates three complementary public health surveillance systems: passive reporting by clinicians, active case-finding by field investigators, and a syndromic surveillance algorithm that flags likely outbreak days based on emergency department chief complaints. Consider the syndromic surveillance algorithm as a binary classifier of the latent state “outbreak present on a given day.” Let prevalence $p$ denote the probability $P(D)$ that an outbreak is truly present on a randomly selected day, sensitivity $\\text{Se}$ denote $P(T^{+}\\,|\\,D)$, and specificity $\\text{Sp}$ denote $P(T^{-}\\,|\\,\\neg D)$, where $T^{+}$ denotes an alert and $D$ denotes the presence of an outbreak. Positive Predictive Value (PPV) is defined as $P(D\\,|\\,T^{+})$.\n\nStarting only from the core definitions of sensitivity, specificity, prevalence, conditional probability, the law of total probability, and Bayes’ theorem, derive an expression for PPV in terms of $p$, $\\text{Se}$, and $\\text{Sp}$. Then, for a monitoring season in which the true daily outbreak prevalence is $p=0.005$, the syndromic algorithm has sensitivity $\\text{Se}=0.9$ and specificity $\\text{Sp}=0.95$, compute the PPV. Provide a brief interpretation of how the base rate $p$ constrains the practical utility of alerts for rare events, and how passive versus active versus syndromic surveillance designs might mitigate or exacerbate this constraint through changes in $p$, $\\text{Se}$, or $\\text{Sp}$.\n\nExpress the final PPV as a decimal fraction and round your answer to $4$ significant figures.", "solution": "The problem requires the derivation of an expression for the Positive Predictive Value (PPV) and its calculation for a specific scenario, followed by an interpretation of the result in the context of public health surveillance.\n\nFirst, we validate the problem.\n**Step 1: Extract Givens**\n- Prevalence of an outbreak: $p = P(D)$\n- Sensitivity of the syndromic algorithm: $\\text{Se} = P(T^{+}\\,|\\,D)$\n- Specificity of the syndromic algorithm: $\\text{Sp} = P(T^{-}\\,|\\,\\neg D)$\n- Positive Predictive Value (PPV): PPV $= P(D\\,|\\,T^{+})$\n- $D$: The event that an outbreak is present.\n- $T^{+}$: The event that the algorithm flags an alert.\n- $T^{-}$: The event that the algorithm does not flag an alert.\n- $\\neg D$: The event that an outbreak is not present.\n- Numerical values for calculation: $p = 0.005$, $\\text{Se} = 0.9$, $\\text{Sp} = 0.95$.\n- The derivation must start from the core definitions of conditional probability, the law of total probability, and Bayes’ theorem.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the principles of epidemiology and Bayesian probability. It is well-posed, with all necessary information provided for a unique solution to the derivation and calculation. The language is objective and the terminology is standard. The provided numerical values are realistic for a syndromic surveillance system monitoring a rare health event. The problem is a standard, non-trivial application of probability theory to a real-world scientific domain.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with the solution.\n\n**Part 1: Derivation of PPV**\n\nThe Positive Predictive Value (PPV) is defined as the probability that an outbreak is truly present given that the surveillance system has flagged an alert. Using the definition of conditional probability, this is written as:\n$$\n\\text{PPV} = P(D\\,|\\,T^{+}) = \\frac{P(D \\cap T^{+})}{P(T^{+})}\n$$\nThis is the fundamental structure of Bayes' theorem. We must now express the numerator and denominator in terms of the given quantities: prevalence ($p$), sensitivity ($\\text{Se}$), and specificity ($\\text{Sp}$).\n\nThe numerator, $P(D \\cap T^{+})$, is the joint probability of an outbreak being present and an alert being flagged. We can re-express this using the definition of conditional probability for sensitivity, $\\text{Se} = P(T^{+}\\,|\\,D) = \\frac{P(D \\cap T^{+})}{P(D)}$. Rearranging this gives:\n$$\nP(D \\cap T^{+}) = P(T^{+}\\,|\\,D) P(D) = \\text{Se} \\cdot p\n$$\nThis term represents the probability of a true positive result.\n\nThe denominator, $P(T^{+})$, is the overall probability of the system flagging an alert on any given day. To find this, we use the Law of Total Probability, partitioning the sample space by the presence ($D$) or absence ($\\neg D$) of an outbreak:\n$$\nP(T^{+}) = P(T^{+}\\,|\\,D)P(D) + P(T^{+}\\,|\\,\\neg D)P(\\neg D)\n$$\nWe have expressions for the first part of this sum from the numerator derivation: $P(T^{+}\\,|\\,D)P(D) = \\text{Se} \\cdot p$.\n\nFor the second part of the sum, we need to find $P(T^{+}\\,|\\,\\neg D)$ and $P(\\neg D)$.\nThe probability of no outbreak being present, $P(\\neg D)$, is the complement of the prevalence $p$:\n$$\nP(\\neg D) = 1 - P(D) = 1 - p\n$$\nThe probability $P(T^{+}\\,|\\,\\neg D)$ is the probability of an alert given no outbreak, which is the false positive rate. We are given the specificity, $\\text{Sp} = P(T^{-}\\,|\\,\\neg D)$, which is the probability of no alert given no outbreak. Since an alert is either flagged ($T^{+}$) or not ($T^{-}$), for the condition $\\neg D$, these are complementary events. Therefore:\n$$\nP(T^{+}\\,|\\,\\neg D) = 1 - P(T^{-}\\,|\\,\\neg D) = 1 - \\text{Sp}\n$$\nSubstituting these back into the law of total probability expression, we get the total probability of an alert:\n$$\nP(T^{+}) = (\\text{Se} \\cdot p) + (1 - \\text{Sp})(1 - p)\n$$\nThe first term corresponds to true positives, and the second term corresponds to false positives.\n\nFinally, we substitute the derived expressions for the numerator and denominator back into the PPV formula:\n$$\n\\text{PPV} = \\frac{P(D \\cap T^{+})}{P(T^{+})} = \\frac{\\text{Se} \\cdot p}{(\\text{Se} \\cdot p) + (1 - \\text{Sp})(1 - p)}\n$$\nThis is the desired expression for PPV in terms of $p$, $\\text{Se}$, and $\\text{Sp}$.\n\n**Part 2: Calculation of PPV**\n\nWe are given the values $p = 0.005$, $\\text{Se} = 0.9$, and $\\text{Sp} = 0.95$.\nWe substitute these values into the derived formula:\n$$\n\\text{PPV} = \\frac{(0.9)(0.005)}{(0.9)(0.005) + (1 - 0.95)(1 - 0.005)}\n$$\nFirst, we calculate the individual components:\n- Numerator (True Positives Rate): $(0.9)(0.005) = 0.0045$\n- False Positive Rate: $1 - \\text{Sp} = 1 - 0.95 = 0.05$\n- Probability of no outbreak: $1 - p = 1 - 0.005 = 0.995$\n- Denominator (False Positives Rate Term): $(0.05)(0.995) = 0.04975$\n\nNow, we compute the full fraction:\n$$\n\\text{PPV} = \\frac{0.0045}{0.0045 + 0.04975} = \\frac{0.0045}{0.05425}\n$$\n$$\n\\text{PPV} \\approx 0.08294930875...\n$$\nRounding to $4$ significant figures, we get $0.08295$.\n\n**Part 3: Interpretation**\n\nThe calculated PPV of approximately $0.083$ means that even for a system with high sensitivity ($90\\%$) and high specificity ($95\\%$), only about $8.3\\%$ of alerts correspond to a true outbreak. The remaining $91.7\\%$ are false alarms. This low PPV is a direct consequence of the low prevalence, or base rate, of the event ($p=0.005$). The denominator of the PPV formula is the sum of true positives ($0.0045$) and false positives ($0.04975$). In this case, the false positives outnumber the true positives by a factor of roughly $11$. This phenomenon, where the large number of non-events generates more false alarms than the small number of events generates true signals, is known as the base rate fallacy. It severely constrains the practical utility of alerts for rare events, as frequent false alarms can lead to \"alert fatigue\" where public health officials might start to ignore them, undermining the system's purpose.\n\nThe different surveillance designs interact with this constraint as follows:\n- **Syndromic surveillance**, as modeled here, is highly susceptible to the low base rate problem. Its strength is maximizing timeliness and sensitivity ($\\text{Se}$) for early detection, but this often comes at the cost of imperfect specificity ($\\text{Sp}$). When $\\text{Sp}$ is applied to the vast number of non-outbreak days (determined by $1-p$), a large absolute number of false positives is generated, depressing the PPV.\n- **Passive surveillance** typically has very low sensitivity ($\\text{Se}$) because it relies on voluntary reporting, but often has high specificity ($\\text{Sp}$) since reports are usually based on clinical suspicion or diagnosis. While it would also be subject to the base rate constraint, its primary limitation is often missing outbreaks entirely (low $\\text{Se}$).\n- **Active surveillance** is the most effective at mitigating the constraint, but is the most resource-intensive. It directly increases sensitivity ($\\text{Se}$) by actively seeking cases. Furthermore, if it is targeted towards high-risk populations, it can effectively increase the prevalence ($p$) within the monitored sample. Both increasing $\\text{Se}$ and increasing $p$ directly improve the PPV, making alerts more reliable.", "answer": "$$\\boxed{0.08295}$$", "id": "4565209"}, {"introduction": "While the previous exercise explored the probability of an individual alert being false, this practice examines the cumulative impact of false positives across a large-scale screening program. Using fundamental probability principles, you will calculate the expected number of false alarms generated over a month from thousands of daily tests [@problem_id:4565250]. This hands-on calculation highlights the significant operational burden that even a highly specific test can create, underscoring the importance of strategic testing and confirmatory protocols in public health.", "problem": "A health department operates a passive laboratory-based surveillance program for an emerging infection. Each day, an average of $n=10{,}000$ individuals are tested. Assume tests are administered independently across individuals and days. The diagnostic assay has specificity $0.99$, where specificity is defined as $\\Pr(\\text{test negative} \\mid \\text{truly uninfected})$. During a period of low incidence, you may treat the fraction of truly infected among those tested as negligible so that nearly all tested individuals are truly uninfected.\n\nUsing only core definitions and probability principles, derive from first principles the expected number of false positive results accumulated over a $30$-day month under these conditions. State any stochastic assumptions you invoke and clearly justify each step using definitions such as the meaning of specificity and the properties of expectations of sums of indicator variables. Then, briefly propose at least two realistic mitigation strategies within public health surveillance design (e.g., changes in passive, active, or syndromic surveillance workflows) that would reduce the expected number of false positives or their programmatic impact, and explain why they are expected to work in terms of the quantities that drive your derivation.\n\nReport the final numerical answer as a count for the expected number of false positives over the $30$-day month. Do not include units. No rounding is required.", "solution": "The problem asks for the derivation of the expected number of false positive results over a $30$-day period under specific surveillance conditions, and for a discussion of mitigation strategies. The problem is deemed valid as it is scientifically grounded in epidemiological and statistical principles, is well-posed with sufficient and consistent data, and is formulated objectively.\n\nThe solution is divided into two parts: the derivation of the expected number of false positives, and the proposal of mitigation strategies.\n\n**Part 1: Derivation of the Expected Number of False Positives**\n\nFirst, we define the variables and probabilities based on the problem statement.\nLet $N$ be the number of individuals tested per day, where $N=10{,}000$.\nLet $D$ be the number of days in the surveillance period, where $D=30$.\nLet $S_p$ be the specificity of the diagnostic assay, where $S_p = 0.99$.\n\nLet $U$ be the event that a tested individual is truly uninfected.\nLet $T^+$ be the event that a test result is positive.\nLet $T^-$ be the event that a test result is negative.\n\nThe specificity is defined as the probability of a negative test result given the individual is truly uninfected:\n$$ S_p = \\Pr(T^- \\mid U) = 0.99 $$\nA false positive occurs when an individual who is truly uninfected tests positive. The probability of this event for a single truly uninfected individual is $\\Pr(T^+ \\mid U)$. Since an individual's test result must be either positive or negative, the probabilities are complementary:\n$$ \\Pr(T^+ \\mid U) + \\Pr(T^- \\mid U) = 1 $$\nTherefore, the probability of a false positive for a single uninfected individual is:\n$$ \\Pr(T^+ \\mid U) = 1 - S_p = 1 - 0.99 = 0.01 $$\nThe problem states that during this period of low incidence, we may treat the fraction of truly infected individuals as negligible. This is a crucial simplifying assumption. It allows us to approximate the probability that any randomly selected individual from the tested population is uninfected as $\\Pr(U) \\approx 1$.\n\nThe probability that a randomly tested individual generates a false positive result is the joint probability of being uninfected and testing positive, $\\Pr(T^+ \\cap U)$. Using the definition of conditional probability, this is:\n$$ \\Pr(T^+ \\cap U) = \\Pr(T^+ \\mid U) \\Pr(U) $$\nGiven our assumption that $\\Pr(U) \\approx 1$, the probability of a randomly selected test being a false positive, which we denote as $p_{fp}$, is:\n$$ p_{fp} = \\Pr(T^+ \\cap U) \\approx \\Pr(T^+ \\mid U) = 1 - S_p = 0.01 $$\n\nNow, we model the number of false positives. Let $X_i$ be an indicator random variable for the $i$-th individual tested on a given day, for $i \\in \\{1, 2, \\dots, N\\}$.\nWe define $X_i = 1$ if the $i$-th test is a false positive, and $X_i = 0$ otherwise.\nThe problem states that tests are administered independently. This is our primary stochastic assumption. Under this assumption, each $X_i$ is an independent Bernoulli trial with a success probability of $p_{fp}$.\n\nThe expected value of a single indicator variable is:\n$$ E[X_i] = 1 \\cdot \\Pr(X_i=1) + 0 \\cdot \\Pr(X_i=0) = \\Pr(X_i=1) = p_{fp} $$\nLet $F_{day}$ be the random variable representing the total number of false positives on a single day. $F_{day}$ is the sum of the individual indicator variables:\n$$ F_{day} = \\sum_{i=1}^{N} X_i $$\nBy the linearity of expectation, the expected number of false positives on a single day is the sum of the expectations of the individual indicator variables:\n$$ E[F_{day}] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i] = \\sum_{i=1}^{N} p_{fp} = N \\cdot p_{fp} $$\nSubstituting the values:\n$$ E[F_{day}] = N (1 - S_p) = 10{,}000 \\times (1 - 0.99) = 10{,}000 \\times 0.01 = 100 $$\nSo, we expect $100$ false positives per day.\n\nThe problem asks for the expected number accumulated over a $D=30$-day month. Let $F_{total}$ be the total number of false positives over this period. Let $F_j$ be the number of false positives on day $j$, for $j \\in \\{1, 2, \\dots, D\\}$.\n$$ F_{total} = \\sum_{j=1}^{D} F_j $$\nThe problem states that tests are independent across days, which is another key stochastic assumption. This implies that the expected number of false positives on any day is the same, i.e., $E[F_j] = E[F_{day}] = 100$ for all $j$.\nAgain, using the linearity of expectation for the sum over the days:\n$$ E[F_{total}] = E\\left[\\sum_{j=1}^{D} F_j\\right] = \\sum_{j=1}^{D} E[F_j] $$\nSince $E[F_j]$ is constant, this simplifies to:\n$$ E[F_{total}] = D \\cdot E[F_{day}] = D \\cdot N \\cdot (1 - S_p) $$\nPlugging in the final values:\n$$ E[F_{total}] = 30 \\times 100 = 3000 $$\n\n**Part 2: Mitigation Strategies**\n\nThe derived expected number of false positives, $E[F_{total}] = D \\cdot N \\cdot (1 - S_p)$, is directly proportional to the number of tests performed on uninfected individuals ($N$) and the test's false positive rate ($1-S_p$). Mitigation strategies must therefore aim to reduce one or both of these quantities.\n\n1.  **Implement Confirmatory Testing to Increase Effective Specificity:**\n    This strategy targets the $(1-S_p)$ term. Instead of relying on a single test, any initial positive result is subjected to a second, different confirmatory assay. A case is reported as positive only if both tests are positive. Let the specificity of the primary assay be $S_{p1} = 0.99$ and the specificity of the confirmatory assay be $S_{p2}$. For a truly uninfected individual to be a final false positive, they must test positive on both tests. Assuming the tests are conditionally independent given the infection status, the probability of this joint event is the product of the individual false positive probabilities: $\\Pr(\\text{FP on test 1} \\cap \\text{FP on test 2}) = (1-S_{p1})(1-S_{p2})$. The new effective specificity, $S_{p, \\text{eff}}$, is $1 - (1-S_{p1})(1-S_{p2})$. If $S_{p2}$ is also high (e.g., $0.995$), the new false positive rate becomes $(1-0.99)(1-0.995) = 0.01 \\times 0.005 = 0.00005$. This would reduce the expected number of false positives from $3000$ to $30 \\times 10000 \\times 0.00005 = 15$. This drastically reduces the downstream burden of investigating false alarms, making it a very effective (though more costly per positive case) strategy.\n\n2.  **Refine Testing Criteria via Syndromic Surveillance or Active Surveillance:**\n    This strategy targets the $N$ term, specifically the number of uninfected people being tested. The current passive system tests $N=10{,}000$ individuals, most of whom are uninfected and asymptomatic, leading to a large number of false positives. By refining the testing criteria, we can reduce the number of tests performed on this low-prevalence population.\n    *   **Integration with Syndromic Surveillance:** A surveillance system could be designed to only test individuals who present to healthcare facilities and are flagged by a syndromic surveillance system (e.g., for having a chief complaint of \"fever and cough\"). This targets the test towards individuals with a higher pre-test probability of being infected. While some of these will still be uninfected, the total number of tests performed on the asymptomatic, uninfected population is greatly reduced, which proportionally reduces $N$ in our formula and thus the total number of false positives.\n    *   **Shift to Active Surveillance:** For an emerging infection, a shift towards active surveillance, such as contact tracing around the few known or suspected cases, would also focus testing resources. Instead of broad screening, tests are used on a much smaller, higher-risk population of contacts. This again reduces the value of $N$ applied to the general uninfected pool, lowering the overall false positive count and improving the efficiency of the surveillance system.\n\nBoth strategies reduce the programmatic impact of false positives. The first strategy does so by making false positives much rarer through improved diagnostic accuracy. The second does so by reducing the pool of individuals to whom the imperfect test is applied, thus generating fewer false alarms that require public health action.", "answer": "$$\n\\boxed{3000}\n$$", "id": "4565250"}, {"introduction": "No single surveillance system can capture every case of a disease. This practice introduces a powerful statistical tool, the capture-recapture method, for estimating the true total number of cases in a population by comparing data from two independent surveillance sources [@problem_id:4565224]. By calculating the estimated population size and its variance, you will learn how to synthesize information from incomplete datasets to produce a more accurate assessment of disease burden, a fundamental task in epidemiology.", "problem": "A metropolitan health department is integrating data from two distinct surveillance systems to estimate the total number of incident cases of a new influenza-like illness over a fixed observation window. Source $A$ is a passive surveillance system (healthcare provider reports), and Source $B$ is an active surveillance system (field investigations). Over the same period, Source $A$ recorded $n_{A}=120$ cases, Source $B$ recorded $n_{B}=150$ cases, and cross-linkage identified that $n_{AB}=60$ cases appeared in both sources.\n\nAssume a closed population for the interval, case status does not change during the interval, and the two sources ascertain cases independently with equal capture probabilities across cases. Starting from the core definitions of probability of ascertainment and independence, derive the two-source capture–recapture estimator for the total number of cases $N$ in the population during the period, and then apply a widely accepted small-sample bias reduction to obtain the final point estimate. Using the corresponding variance expression for the bias-reduced estimator, compute the estimated total number of cases $N$ and its variance.\n\nExpress the estimated total number of cases in units of cases and the variance in units of cases squared. Round both the point estimate and the variance to four significant figures. Provide your final numerical results.", "solution": "The problem is first subjected to validation.\n\n### Step 1: Extract Givens\nThe data and conditions provided in the problem statement are:\n-   Number of cases recorded by Source A: $n_{A}=120$\n-   Number of cases recorded by Source B: $n_{B}=150$\n-   Number of cases recorded by both Source A and Source B: $n_{AB}=60$\n-   Assumption 1: The population is closed (no entries, exits, or changes in case status) during the observation period.\n-   Assumption 2: The two sources, A and B, ascertain cases independently.\n-   Assumption 3: All cases have an equal probability of being captured by a given source.\n-   Task 1: Derive the two-source capture–recapture estimator for the total number of cases, $N$.\n-   Task 2: Apply a small-sample bias reduction to the estimator.\n-   Task 3: Compute the numerical point estimate for $N$ and the variance of this estimate.\n-   Task 4: Round both the point estimate and its variance to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n-   **Scientifically Grounded**: The problem describes a classic application of a capture-recapture model, specifically the Lincoln-Petersen method, which is a well-established statistical technique used in epidemiology and ecology to estimate population size. The request to use a bias-corrected form (the Chapman estimator) and its corresponding variance is standard statistical practice. The underlying assumptions (closed population, independence, equal catchability) are the standard ideal conditions for this model. The problem is firmly rooted in statistical and epidemiological principles.\n-   **Well-Posed**: All necessary numerical data ($n_A$, $n_B$, $n_{AB}$) are provided. The objectives are stated clearly: derive the estimator, apply a specific modification (bias reduction), and calculate the resulting point estimate and its variance. The problem is self-contained and allows for a unique, stable solution.\n-   **Objective**: The problem is stated using precise, quantitative, and unbiased language. There are no subjective or opinion-based elements.\n\nThe problem does not exhibit any of the flaws listed in the validation checklist. It is scientifically sound, formally structured, complete, consistent, and feasible.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation and Calculation\n\nLet $N$ be the true total number of incident cases in the population. Let $p_A$ and $p_B$ be the probabilities that a single case is ascertained by Source A and Source B, respectively.\n\nBased on the assumption of equal capture probability for all cases, we can estimate these probabilities from the observed counts:\n$$ \\hat{p}_A = \\frac{n_A}{N} $$\n$$ \\hat{p}_B = \\frac{n_B}{N} $$\n\nThe problem states that the two sources are independent. Therefore, the probability that a case is captured by both sources, $p_{AB}$, is the product of the individual probabilities:\n$$ p_{AB} = p_A p_B $$\n\nWe can also estimate $p_{AB}$ directly from the data as the proportion of the total cases that were captured by both systems:\n$$ \\hat{p}_{AB} = \\frac{n_{AB}}{N} $$\n\nEquating the expressions for $p_{AB}$ by substituting the estimated probabilities $\\hat{p}_A$ and $\\hat{p}_B$:\n$$ \\hat{p}_{AB} \\approx \\hat{p}_A \\hat{p}_B $$\n$$ \\frac{n_{AB}}{N} \\approx \\left(\\frac{n_A}{N}\\right) \\left(\\fracn_B}{N}\\right) = \\frac{n_A n_B}{N^2} $$\n\nSolving this relation for $N$ gives the basic capture-recapture estimator, also known as the Lincoln-Petersen estimator, which is the maximum likelihood estimate for $N$:\n$$ N \\cdot n_{AB} \\approx n_A n_B $$\n$$ \\hat{N}_{LP} = \\frac{n_A n_B}{n_{AB}} $$\n\nThis estimator is known to be biased, particularly for small sample sizes. The problem requires the application of a small-sample bias reduction. A widely accepted correction is the Chapman estimator, denoted $\\hat{N}_C$, which is nearly unbiased if $(n_A + n_B) \\ge N$. The formula is:\n$$ \\hat{N}_C = \\frac{(n_A+1)(n_B+1)}{(n_{AB}+1)} - 1 $$\n\nWe now substitute the given values, $n_A = 120$, $n_B = 150$, and $n_{AB} = 60$, into the Chapman estimator formula to find the point estimate for the total number of cases, $N$.\n$$ \\hat{N}_C = \\frac{(120+1)(150+1)}{(60+1)} - 1 $$\n$$ \\hat{N}_C = \\frac{(121)(151)}{61} - 1 $$\n$$ \\hat{N}_C = \\frac{18271}{61} - 1 $$\n$$ \\hat{N}_C \\approx 299.52459 - 1 = 298.52459 $$\n\nRounding this value to four significant figures, we get an estimated total of $298.5$ cases.\n\nNext, we compute the variance of this bias-reduced estimator. The variance of the Chapman estimator, $\\text{Var}(\\hat{N}_C)$, is given by the formula developed by Seber:\n$$ \\text{Var}(\\hat{N}_C) \\approx \\frac{(n_A+1)(n_B+1)(n_A-n_{AB})(n_B-n_{AB})}{(n_{AB}+1)^2(n_{AB}+2)} $$\n\nWe substitute the given values into this expression:\n-   $n_A+1 = 121$\n-   $n_B+1 = 151$\n-   $n_A-n_{AB} = 120 - 60 = 60$\n-   $n_B-n_{AB} = 150 - 60 = 90$\n-   $n_{AB}+1 = 61$\n-   $n_{AB}+2 = 62$\n\n$$ \\text{Var}(\\hat{N}_C) \\approx \\frac{(121)(151)(60)(90)}{(61)^2(62)} $$\n$$ \\text{Var}(\\hat{N}_C) \\approx \\frac{(18271)(5400)}{(3721)(62)} $$\n$$ \\text{Var}(\\hat{N}_C) \\approx \\frac{98663400}{230702} $$\n$$ \\text{Var}(\\hat{N}_C) \\approx 427.6655... $$\n\nRounding this value to four significant figures, we get a variance of $427.7$ cases squared.\n\nThe final results are an estimated total of $298.5$ cases and a variance of $427.7$ cases$^2$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n298.5 & 427.7\n\\end{pmatrix}\n}\n$$", "id": "4565224"}]}