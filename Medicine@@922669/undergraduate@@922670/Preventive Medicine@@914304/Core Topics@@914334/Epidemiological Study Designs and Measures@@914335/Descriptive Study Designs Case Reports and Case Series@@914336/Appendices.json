{"hands_on_practices": [{"introduction": "The value of a case series hinges on the quality of its foundation: the sampling strategy. A well-designed study aims to capture a representative snapshot of cases, but this is threatened by selection bias, a systematic error that can skew results. This first practice challenges you to move from theory to application by designing a rigorous protocol for a hospital-based case series, focusing on how to operationalize consecutive sampling to minimize bias and ensure the findings are as credible as possible [@problem_id:4518756].", "problem": "A tertiary-care hospital intends to publish a case series describing clinical characteristics and short-term outcomes of patients with suspected e-cigarette or vaping product use-associated lung injury (EVALI). The study is hospital-based, conducted in the Emergency Department (ED) with follow-up through discharge if admitted, and seeks to minimize selection bias inherent to non-probability sampling. The research team must choose a sampling strategy and define explicit inclusion and exclusion criteria before Institutional Review Board (IRB) approval and data collection.\n\nUsing core definitions from observational epidemiology and preventive medicine—specifically, that a case series is a descriptive study that aims to include all eligible cases in a defined setting and timeframe, and that selection bias is a systematic error arising when the mechanism of selecting cases correlates with patient characteristics—identify the option that most rigorously implements consecutive sampling at the hospital level, supported by explicit inclusion and exclusion criteria. The chosen option should operationalize a sampling frame that makes the probability of inclusion $P(S=1 \\mid \\text{eligible})$ approximately constant and near $1$ across all eligible cases over the study interval, thereby reducing selection bias compared to convenience sampling based on staff availability or patient ease-of-recruitment.\n\nWhich of the following strategies best meets this goal?\n\nA. Prospective, consecutive sampling over a defined $6$-month interval, enrolling every eligible patient who presents to the ED at any hour, every day, with an a priori case definition: inclusion criteria are age $\\geq 18$ years, ED presentation with radiographic pulmonary infiltrates plus recent vaping exposure within the prior $90$ days, first presentation for the current episode, residence within the hospital catchment area, and capacity to provide consent or have a legally authorized representative; exclusion criteria are hospital-acquired lung injury (onset $\\geq 48$ hours after admission), inter-facility transfers with definitive diagnosis already established elsewhere and no ED evaluation at this hospital, incarceration (per IRB constraints), active end-of-life hospice enrollment, and refusal or withdrawal of consent. A multilingual recruitment protocol and $24/7$ screening of ED logs ensure capture across nights, weekends, and holidays; the team records screening logs for all eligible patients, including reasons for non-enrollment.\n\nB. Convenience sampling limited to weekdays between 09:00 and 17:00, recruiting patients when a research coordinator is present. Inclusion requires age $\\geq 18$ years and English fluency; exclusion criteria include uninsured status, arrival by ambulance, and admission to the Intensive Care Unit (ICU). The team asserts this approach improves feasibility and data completeness.\n\nC. Retrospective electronic medical record sampling that extracts the first $50$ ED encounters flagged by International Classification of Diseases (ICD) codes related to vaping, regardless of date or time, excluding cases with missing imaging or incomplete notes, and excluding severe cases requiring ICU admission. The team claims this reduces information bias.\n\nD. Systematic daily sampling that enrolls the first $5$ eligible ED cases each calendar day over $6$ months, with inclusion criteria of age $\\geq 18$ years and recent vaping exposure, and exclusion only for refusal of consent. Due to workload limits, any additional eligible cases after the daily cap are not enrolled. The team argues this is “nearly consecutive” and operationally unbiased.\n\nE. Prospective registry capturing consecutive ED cases across a $6$-month interval but excluding patients who arrive during night shifts and those who require interpreter services, to standardize clinical assessment and minimize miscommunication. Inclusion criteria are age $\\geq 18$ years and ED presentation with suspected EVALI; exclusion criteria add arrival between 20:00 and 08:00 and language needs beyond English.\n\nSelect the single best option and be prepared to justify the choice based on first principles of descriptive study design and selection bias reduction.", "solution": "The core task is to identify the sampling strategy that best implements **consecutive sampling** to minimize selection bias. The goal of consecutive sampling in a case series is to enroll every eligible patient from a defined source over a specified time period. This makes the probability of inclusion, $P(S=1 \\mid \\text{eligible})$, as close to $1$ as possible and, crucially, ensures this probability is constant across all subgroups of eligible patients (e.g., regardless of time of arrival, language, or disease severity). Any deviation that makes this probability systematically different from $1$ for certain subgroups introduces selection bias.\n\n**A. Prospective, consecutive sampling over a defined $6$-month interval, enrolling every eligible patient who presents to the ED at any hour, every day...**\nThis option is the gold standard for a hospital-based case series. It explicitly aims for exhaustive capture:\n*   **Time:** The $24/7$ screening and enrollment across the entire $6$-month interval eliminates temporal selection bias. Patients arriving at night or on weekends are not systematically missed.\n*   **Population:** The multilingual protocol directly mitigates selection bias based on language or ethnicity.\n*   **Criteria:** The inclusion/exclusion criteria are specific, clinically justified, and defined *a priori*. They create a well-defined cohort without arbitrarily excluding patients based on convenience or severity.\n*   **Transparency:** The use of screening logs to document non-enrollment is a hallmark of rigorous methodology, allowing for an assessment of potential non-response bias.\nThis strategy makes the maximum effort to ensure $P(S=1 \\mid \\text{eligible}) \\approx 1$ and is constant for all. **Verdict: Correct.**\n\n**B. Convenience sampling limited to weekdays between 09:00 and 17:00...**\nThis is a classic convenience sample, fraught with bias.\n*   **Temporal Bias:** It systematically excludes patients arriving on evenings, nights, and weekends, who may differ in severity or demographics.\n*   **Exclusion Bias:** Excluding non-English speakers, the uninsured, ambulance arrivals, and ICU admissions systematically removes sicker, poorer, and non-English-speaking patients, creating a highly unrepresentative sample. **Verdict: Incorrect.**\n\n**C. Retrospective electronic medical record sampling that extracts the first $50$ ED encounters...**\nThis is a retrospective quota sample, not a consecutive sample.\n*   **Selection Bias:** The ICD code-based search is prone to misclassification. More importantly, excluding severe cases requiring ICU admission and cases with incomplete data (which may correlate with severity or chaotic presentations) creates a sample biased towards less severe, well-documented cases. **Verdict: Incorrect.**\n\n**D. Systematic daily sampling that enrolls the first $5$ eligible ED cases each calendar day...**\nThis is systematic sampling with a quota, which is not consecutive sampling.\n*   **Temporal Bias:** Once the daily cap of $5$ is met, all subsequent eligible patients are excluded. If patient arrivals peak at certain times of the day (e.g., evenings), those patients will be systematically underrepresented. The probability of inclusion becomes $0$ after the cap is hit, violating the core principle. **Verdict: Incorrect.**\n\n**E. Prospective registry capturing consecutive ED cases... but excluding patients who arrive during night shifts and those who require interpreter services...**\nThis option uses the label \"consecutive\" but implements exclusions that defeat the purpose.\n*   **Selection Bias:** Excluding night shifts and non-English speakers introduces the same severe temporal and demographic biases as in option B. Justifying this for \"standardization\" is a methodologically unsound reason to introduce major bias. **Verdict: Incorrect.**\n\n**Conclusion:** Option A is the only strategy that describes a methodologically rigorous implementation of consecutive sampling designed to minimize selection bias by ensuring a complete and unbiased capture of all eligible cases over the defined study period.", "answer": "$$\\boxed{A}$$", "id": "4518756"}, {"introduction": "Once data is collected, especially in a pre-post intervention format common in case reports, a new challenge emerges: correctly attributing observed changes. It is easy to mistake a statistical artifact for a true treatment effect. This exercise delves into the subtle but powerful phenomenon of regression to the mean, presenting a scenario where an apparent clinical improvement could be entirely illusory [@problem_id:4518817]. Mastering this concept is critical for avoiding one of the most common errors in interpreting descriptive data.", "problem": "A clinician writes a case report describing a single patient with a chronic symptom severity score measured on a $0$–$100$ scale, where higher is worse. The population mean for similar untreated patients is approximately $\\mu=70$ with standard deviation $\\sigma=10$. The instrument’s test–retest reliability (correlation between repeated measurements in stable patients) is $r=0.6$. The patient was enrolled when their baseline score was $x_1=90$. Immediately afterward, the clinician initiated an herbal supplement, and the next three weekly scores were $x_2=82$, $x_3=81$, and $x_4=83$. The clinician attributes the $\\approx 8$-point reduction to the supplement.\n\nUsing only fundamental principles from classical measurement theory (observed value equals true value plus random error) and the definition of conditional expectation under joint normality with correlation $r$ (no shortcut formulas are provided), reason from first principles about how regression to the mean can arise after selecting an extreme baseline and whether the observed changes could plausibly occur in the absence of any treatment effect. Then, select all statements below that are most appropriate for distinguishing regression to the mean from a true intervention effect in single-patient case reports and small case series, and that propose scientifically sound design safeguards to mitigate misinterpretation.\n\nA. The post-treatment values being lower than baseline proves efficacy because natural fluctuations would be symmetric around the baseline and not biased toward the population mean.\n\nB. Some or all of the $\\approx 8$-point improvement could be explained by regression to the mean due to imperfect reliability from selecting an extreme baseline; safeguards include taking multiple pre-intervention baseline measurements and using single-case cross-over designs such as a withdrawal sequence (A–B–A) or a randomized $N$-of-$1$ Randomized Controlled Trial (RCT) to create within-patient counterfactuals.\n\nC. Selecting patients with the most extreme baseline values reduces confounding and increases causal validity in case reports by minimizing random error.\n\nD. Increasing measurement reliability (for example, by averaging repeated readings at each time point and standardizing measurement procedures) reduces the magnitude of apparent regression to the mean.\n\nE. Using historical controls from the published literature is sufficient to eliminate regression-to-the-mean concerns in a case report because any regression would also occur in the historical group.\n\nF. In a multiple-baseline case series with $n$ similar patients who start the intervention at staggered times, observing that each patient’s improvement consistently begins only after their own start time provides internal replication that reduces the likelihood that regression to the mean alone explains the pattern.\n\nSelect all that apply.", "solution": "Start from classical measurement theory and basic properties of conditional expectation for jointly normal variables. Let $X_1$ and $X_2$ denote two measurements of the same construct (for example, weekly scores) on a stable patient, each with mean $\\mu$ and variance $\\sigma^2$, and correlation $r$ reflecting test–retest reliability. The joint distribution of $(X_1,X_2)$ is well approximated by a bivariate normal with covariance $\\mathrm{Cov}(X_1,X_2)=r\\sigma^2$ when $\\mathrm{Var}(X_1)=\\mathrm{Var}(X_2)=\\sigma^2$. The conditional expectation of $X_2$ given $X_1=x_1$ follows the linear projection derived from definitions of covariance and variance.\n\nThe regression of $X_2$ on $X_1$ has slope\n$$\n\\beta=\\frac{\\mathrm{Cov}(X_1,X_2)}{\\mathrm{Var}(X_1)}=\\frac{r\\sigma^2}{\\sigma^2}=r,\n$$\nand intercept chosen so that $E[X_2]=\\mu$:\n$$\n\\alpha=\\mu-\\beta\\mu=\\mu-r\\mu.\n$$\nTherefore,\n$$\nE[X_2\\mid X_1=x_1]=\\alpha+\\beta x_1=\\mu+r(x_1-\\mu).\n$$\n\nThis expression is a direct consequence of the linear least-squares projection using the definitions of $\\mathrm{Cov}(\\cdot,\\cdot)$ and $\\mathrm{Var}(\\cdot)$ under the assumption of equal variances and joint normality, which is a standard approximation for repeated continuous measurements.\n\nApply this to the scenario with $\\mu=70$, $\\sigma=10$, $r=0.6$, and $x_1=90$:\n$$\nE[X_2\\mid X_1=90]=70+0.6\\cdot(90-70)=70+0.6\\cdot 20=70+12=82.\n$$\nThus, even in the absence of any treatment effect, after selecting an extreme baseline value $x_1=90$, the expected subsequent score is $82$, which is exactly the magnitude of the observed reduction (approximately $8$ points). The observed sequence $x_2=82$, $x_3=81$, $x_4=83$ is therefore entirely consistent with regression to the mean and random fluctuation, without requiring any causal effect of the intervention. This illustrates how, in a case report with selection on an extreme baseline and imperfect reliability $r<1$, regression to the mean can be misconstrued as an intervention effect.\n\nEvaluate each option:\n\nA. This asserts that natural fluctuations would be symmetric around the baseline and not biased toward the population mean. This is incorrect. The fundamental mechanism of regression to the mean is that, conditional on an extreme observed value $x_1$, the expected subsequent value $E[X_2\\mid X_1=x_1]$ is closer to $\\mu$ when $r<1$. The asymmetry arises from conditioning on an extreme observation that likely includes a positive error component; subsequent observations are expected to be less extreme. Verdict: Incorrect.\n\nB. This recognizes that some or all of the $\\approx 8$-point improvement can be explained by regression to the mean given $r=0.6$ and selection at $x_1=90$, which our derivation quantified as $E[X_2\\mid X_1=90]=82$. It also proposes safeguards that create within-patient counterfactual evidence: multiple pre-intervention baselines to reduce extremity and better estimate the patient’s mean, and single-case experimental designs such as withdrawal (A–B–A) or randomized $N$-of-$1$ Randomized Controlled Trials (RCTs) to demonstrate reversibility or randomization-based contrasts. These are scientifically sound and directly mitigate misinterpretation. Verdict: Correct.\n\nC. This claims that selecting extreme baseline values reduces confounding and increases causal validity by minimizing error. In fact, selection on extremes increases susceptibility to regression to the mean because extreme observations are more likely to include large error components. It worsens, not improves, causal validity in uncontrolled case reports. Verdict: Incorrect.\n\nD. This states that increasing measurement reliability reduces the magnitude of apparent regression to the mean. From the derivation, the slope toward the mean is governed by $r$: when $r\\to 1$, $E[X_2\\mid X_1=x_1]\\to x_1$ and regression to the mean vanishes; when $r<1$, there is shrinkage toward $\\mu$ of magnitude $r(x_1-\\mu)$. Practical methods to increase effective reliability include averaging repeated readings at each time point and standardizing procedures, which reduce random error variance and increase $r$. This is a valid safeguard. Verdict: Correct.\n\nE. Historical controls do not eliminate regression to the mean because the index patient was selected based on an extreme recent value, while the historical group’s selection and measurement context differ. Any differences in selection mechanisms, timing, measurement error, and secular trends mean that regression effects and other biases need not cancel. Without concurrent randomization or within-patient controls, regression to the mean remains a threat. Verdict: Incorrect.\n\nF. A staggered multiple-baseline across-participants design in a small case series with $n$ similar patients who start the intervention at staggered times, introduces internal replication: if each patient’s outcome improves only after their own intervention start and at different calendar times, the alignment of change with the intervention rather than with time or selection suggests a causal effect less compatible with regression to the mean alone. This is a recognized safeguard in single-case methodology. Verdict: Correct.\n\nTherefore, the correct choices are B, D, and F.", "answer": "$$\\boxed{BDF}$$", "id": "4518817"}, {"introduction": "The final step in research is communicating your findings, including the level of uncertainty. While statistical tools like confidence intervals are powerful, their validity is not universal; it is contingent on the study's design. This practice tackles the difficult but essential question of how to responsibly report uncertainty from a case series built on a non-random, convenience sample [@problem_id:4518792]. It pushes beyond rote calculation to explore the epistemic meaning of our statistics, guiding you toward a more transparent and scientifically defensible approach to data analysis.", "problem": "A clinician compiles a case series of $n=38$ consecutive patients treated at a single outpatient clinic for a new dermatologic procedure. The series was assembled by convenience sampling: every patient treated at that clinic over a $6$-month period was included, but no sampling frame beyond the clinic exists, and referral patterns are known to vary by neighborhood. In the series, $x=14$ patients experienced a post-procedure rash. The clinician proposes reporting the observed proportion $\\hat{p}=x/n$ along with a $95\\%$ Confidence Interval (CI) for a population proportion using a standard binomial method.\n\nFrom the perspective of descriptive study designs in preventive medicine, assess whether such a $95\\%$ CI is epistemically meaningful, and identify the most defensible alternative way to communicate uncertainty for this case series. Choose the single best option.\n\nA. The $95\\%$ CI for a population proportion is epistemically weak here because its frequentist coverage property requires a well-defined random sampling mechanism from a target population, which convenience sampling in a single clinic does not provide. A defensible alternative is to report the exact counts $x$ and $n$, the within-series proportion $\\hat{p}$, and, if desired, a model-conditional interval explicitly labeled as a within-series compatibility interval under a binomial counting model, together with a qualitative description of selection and ascertainment uncertainties and a simple sensitivity range such as $\\left[\\frac{x}{n+k},\\,\\frac{x+k}{n}\\right]$ under plausible missed-case scenarios.\n\nB. The $95\\%$ CI is fully meaningful because it always indicates there is a $95\\%$ chance the true population proportion lies within the interval, regardless of how the sample was collected; no alternative uncertainty description is necessary in a descriptive case series.\n\nC. A Bayesian credible interval with a uniform Beta prior makes the interval meaningful even under convenience sampling, since probability statements in Bayesian analysis hold for any sample; therefore it is preferable to report a $95\\%$ credible interval for the population proportion without additional caveats.\n\nD. Bootstrapping the observed data to obtain a $95\\%$ CI solves the lack of random sampling, because resampling from the convenience sample approximates the population; the bootstrap CI should be reported as the primary uncertainty measure in the case series.", "solution": "### Principle-Based Derivation\n\nThe core of this problem lies in the interpretation and justification of a frequentist confidence interval. A $95\\%$ confidence interval for a population proportion, $p$, is an interval $[L, U]$ calculated from sample data. Its defining property, known as **coverage**, is that if the random sampling procedure were repeated a large number of times from the target population, $95\\%$ of the calculated intervals $[L, U]$ would contain the true, fixed value of $p$. The probability statement, $P(\\text{Interval contains } p) = 0.95$, applies to the procedure of generating intervals, not to any single, realized interval.\n\nCrucially, this entire theoretical framework is predicated on a **well-defined random sampling mechanism** from a specified target population. In this problem, the sample is a **convenience sample**—it includes all patients from a single clinic. This sample is not random and is subject to numerous sources of selection bias (geographic, provider, patient self-selection).\n\nBecause the sample is not random, there is no well-defined \"target population\" from which these $n=38$ patients are a representative sample. Therefore, the frequentist coverage guarantee does not apply to any wider population of interest. Attempting to use a standard CI to make an inference about such a population is statistically and epistemically unjustified. The uncertainty it quantifies (random sampling error) is likely dwarfed by the unquantified systematic error (bias).\n\nA defensible approach must acknowledge these limitations. It should prioritize descriptive transparency and contextualize any statistical measures of uncertainty by:\n1.  **Reporting Raw Data:** The most robust information is the count of events, $x=14$, and the total number of patients, $n=38$.\n2.  **Acknowledging Limitations:** Explicitly state that the data arise from a convenience sample and are subject to selection bias.\n3.  **Reframing the Interval:** If an interval is presented, its meaning must be carefully circumscribed, for instance as a \"compatibility interval\" which describes the range of model parameters compatible with the observed data, conditional on the model.\n4.  **Emphasizing Systematic Uncertainty:** Qualitatively discuss the primary sources of uncertainty from systematic biases.\n5.  **Conducting Sensitivity Analysis:** Explore how results might change under plausible scenarios of bias.\n\n### Option-by-Option Analysis\n\n**A. The $95\\%$ CI for a population proportion is epistemically weak here because its frequentist coverage property requires a well-defined random sampling mechanism... A defensible alternative is to report the exact counts $x$ and $n$, the within-series proportion $\\hat{p}$... labeled as a within-series compatibility interval... together with a qualitative description of... uncertainties and a simple sensitivity range...**\nThis option aligns perfectly with the principles derived above. It correctly identifies the failure of the coverage property, recommends transparent reporting of raw data ($x, n$), proposes the modern, correct re-labeling of the interval as a \"compatibility interval\" to avoid unwarranted population-level claims, and correctly emphasizes discussing non-statistical uncertainties (biases) and performing sensitivity analysis. **Verdict: Correct.**\n\n**B. The $95\\%$ CI is fully meaningful because it always indicates there is a $95\\%$ chance the true population proportion lies within the interval, regardless of how the sample was collected...**\nThis option contains multiple fundamental errors. The \"95% chance\" claim is the \"Bayesian fallacy\" of interpreting a frequentist CI. More importantly, the assertion that the interval is meaningful \"regardless of how the sample was collected\" is false; its validity depends entirely on the assumption of random sampling. **Verdict: Incorrect.**\n\n**C. A Bayesian credible interval... makes the interval meaningful even under convenience sampling... without additional caveats.**\nThis misrepresents Bayesian inference. While a Bayesian calculation can be performed on any dataset, its real-world relevance is not divorced from the data-generating process. A Bayesian analysis of a biased sample produces a posterior distribution that is also biased with respect to the target population. It does not magically correct for selection bias. Reporting such an interval \"without additional caveats\" is just as misleading as the frequentist approach. **Verdict: Incorrect.**\n\n**D. Bootstrapping the observed data to obtain a $95\\%$ CI solves the lack of random sampling...**\nThis is a profound misunderstanding of the bootstrap. The bootstrap simulates sampling from the population by resampling from the original sample. This is only valid if the original sample is a good (i.e., random) representation of the population. Resampling from a biased sample only reproduces and quantifies the sampling variability of that biased sample; it cannot correct the bias. **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "4518792"}]}