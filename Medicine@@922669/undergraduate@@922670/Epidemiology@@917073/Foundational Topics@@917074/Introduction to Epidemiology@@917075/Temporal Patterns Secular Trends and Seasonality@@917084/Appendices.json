{"hands_on_practices": [{"introduction": "To understand temporal patterns, we must first learn to measure them. This exercise focuses on quantifying seasonality by creating a seasonal index, a powerful tool for measuring how disease incidence in a specific month or season compares to the annual average. By calculating a normalized seasonal index from hypothetical surveillance data, you will gain a concrete understanding of how to describe and compare seasonal effects in a standardized way [@problem_id:4642118].", "problem": "A city health department monitors monthly counts of a respiratory infection over $5$ consecutive years. Let the observed count at time $t$ be denoted by $Y_t$, and suppose the time series admits a multiplicative decomposition into secular (long-term) trend, seasonality, and remainder as $Y_t = T_t \\times S_{m(t)} \\times R_t$, where $T_t$ is a slowly varying component, $S_{m(t)}$ is the month-specific seasonal component for month $m(t) \\in \\{1,2,\\dots,12\\}$, and $R_t$ is a remainder term with mean $1$ over time. Assume the secular trend is approximately stable over the $5$-year window so that month-specific averages are meaningful estimators of relative seasonal levels.\n\nYou are provided the arithmetic mean of the monthly counts across the $5$ years for each calendar month:\n- January: $450$\n- February: $400$\n- March: $300$\n- April: $300$\n- May: $250$\n- June: $180$\n- July: $200$\n- August: $225$\n- September: $240$\n- October: $360$\n- November: $375$\n- December: $500$\n\nUsing only the definitions above and the principle that over one full seasonal cycle the seasonal component should be neutral in aggregate, derive a month-specific seasonal index estimator based on dividing each month’s mean by an overall mean across all months and years and then imposing a multiplicative neutrality constraint so that the $12$ monthly indices have a product equal to $1$. Then, compute the normalized seasonal index for February using the data given. Provide your final answer as a unitless decimal rounded to four significant figures.", "solution": "The multiplicative decomposition $Y_t = T_t \\times S_{m(t)} \\times R_t$ posits a month-specific seasonal multiplier $S_{m}$ that scales the secular level $T_t$. A natural plug-in estimator of relative seasonality for month $m$ is to compare the mean for that month to an overall mean across all months and years. Let $\\bar{Y}_m$ denote the arithmetic mean of $Y_t$ restricted to month $m$ over the $5$ years, and let $\\bar{Y}$ denote the overall mean across all months and years. Because each month contributes the same number of observations ($5$), $\\bar{Y}$ equals the arithmetic mean of the $12$ month-specific means:\n$$\n\\bar{Y} \\;=\\; \\frac{1}{12}\\sum_{m=1}^{12} \\bar{Y}_m.\n$$\nAn unnormalized relative seasonal factor for month $m$ is then\n$$\ns_m \\;=\\; \\frac{\\bar{Y}_m}{\\bar{Y}}.\n$$\nUnder the multiplicative framework, a standard neutrality requirement over one full cycle is that the product of the normalized seasonal indices equals $1$:\n$$\n\\prod_{m=1}^{12} I_m \\;=\\; 1.\n$$\nTo meet this constraint, we normalize by the geometric mean of the unnormalized factors. Define\n$$\nG \\;=\\; \\left( \\prod_{m=1}^{12} s_m \\right)^{\\frac{1}{12}},\n$$\nand set\n$$\nI_m \\;=\\; \\frac{s_m}{G}.\n$$\nBy construction,\n$$\n\\prod_{m=1}^{12} I_m \\;=\\; \\frac{\\prod_{m=1}^{12} s_m}{G^{12}} \\;=\\; 1.\n$$\n\nWe now compute the quantities for the provided data. First, compute the overall mean $\\bar{Y}$ as the arithmetic mean of the $12$ monthly means:\n\n$$\n\\sum_{m=1}^{12} \\bar{Y}_m \\;=\\; 450 + 400 + 300 + 300 + 250 + 180 + 200 + 225 + 240 + 360 + 375 + 500 \\;=\\; 3780,\n$$\n\nso\n\n$$\n\\bar{Y} \\;=\\; \\frac{3780}{12} \\;=\\; 315.\n$$\n\nThe unnormalized factor for February is\n\n$$\ns_{\\text{Feb}} \\;=\\; \\frac{\\bar{Y}_{\\text{Feb}}}{\\bar{Y}} \\;=\\; \\frac{400}{315}.\n$$\n\n\nNext, compute the normalizing factor $G$:\n\n$$\nG \\;=\\; \\left( \\prod_{m=1}^{12} \\frac{\\bar{Y}_m}{\\bar{Y}} \\right)^{\\frac{1}{12}}\n\\;=\\; \\left( \\frac{\\prod_{m=1}^{12} \\bar{Y}_m}{\\bar{Y}^{12}} \\right)^{\\frac{1}{12}}\n\\;=\\; \\frac{\\left( \\prod_{m=1}^{12} \\bar{Y}_m \\right)^{\\frac{1}{12}}}{\\bar{Y}}.\n$$\n\nObserve that the monthly means can be paired to reveal their product structure:\n- $(450, 200)$: $450 \\times 200 = 90{,}000 = 300^2$,\n- $(400, 225)$: $400 \\times 225 = 90{,}000 = 300^2$,\n- $(360, 250)$: $360 \\times 250 = 90{,}000 = 300^2$,\n- $(375, 240)$: $375 \\times 240 = 90{,}000 = 300^2$,\n- $(300, 300)$: $300 \\times 300 = 90{,}000 = 300^2$,\n- $(500, 180)$: $500 \\times 180 = 90{,}000 = 300^2$.\nThus,\n\n$$\n\\prod_{m=1}^{12} \\bar{Y}_m \\;=\\; (300^2)^6 \\;=\\; 300^{12}.\n$$\n\nTherefore,\n\n$$\nG \\;=\\; \\frac{(300^{12})^{\\frac{1}{12}}}{315} \\;=\\; \\frac{300}{315} \\;=\\; \\frac{20}{21}.\n$$\n\n\nThe normalized seasonal index for February is\n\n$$\nI_{\\text{Feb}} \\;=\\; \\frac{s_{\\text{Feb}}}{G} \\;=\\; \\frac{\\frac{400}{315}}{\\frac{300}{315}} \\;=\\; \\frac{400}{300} \\;=\\; \\frac{4}{3}.\n$$\n\nExpressed as a decimal and rounded to four significant figures,\n\n$$\nI_{\\text{Feb}} \\;\\approx\\; 1.333.\n$$\n\n\nInterpretation: Under a multiplicative seasonal structure with neutral product over the $12$ months, the normalized seasonal index $I_{\\text{Feb}} \\approx 1.333$ indicates that, after controlling for any stable secular level over the $5$-year window, February’s expected count is about $1.333$ times the neutral monthly level (index equal to $1$), reflecting a relative elevation consistent with winter seasonality in respiratory infections. The index is dimensionless and conveys a multiplicative seasonal effect size.", "answer": "$$\\boxed{1.333}$$", "id": "4642118"}, {"introduction": "Once seasonality is identified, we must decide how it relates to the underlying trend. This practice tackles the crucial choice between an additive model, where the seasonal effect is a fixed amount, and a multiplicative model, where it is a proportion of the trend. You will implement a data-driven diagnostic to determine which model is more appropriate, a fundamental step for correctly decomposing a time series and interpreting its components [@problem_id:4642189].", "problem": "You are given the task of operationally comparing additive versus multiplicative Seasonal-Trend decomposition using Loess (STL) in an epidemiological time series context by using a log transform as a diagnostic. Consider a univariate monthly time series of nonnegative counts representing incident cases, indexed by time $t = 1, 2, \\dots, N$, with a seasonal period $m$ months and $K$ complete years so that $N = mK$. The two canonical models are the additive seasonal-trend-noise model $Y_t = T_t + S_t + R_t$ and the multiplicative seasonal-trend-noise model $Y_t = T_t \\times S_t \\times R_t$, where $T_t$ is the trend component, $S_t$ is the seasonal factor with period $m$ (that is, $S_{t+m} = S_t$), and $R_t$ is the remainder with mean $1$ in the multiplicative case and mean $0$ in the additive case. A widely used fact is that, under a multiplicative structure with heteroscedasticity increasing with the mean (as commonly observed for counts), a logarithmic transform $Y'_t = \\log(Y_t + \\varepsilon)$, for small offset $\\varepsilon > 0$ to handle zeros, converts the multiplicative model into an additive one, $Y'_t \\approx \\log T_t + \\log S_t + \\log R_t$, and approximately stabilizes the variance when $\\operatorname{Var}(Y_t \\mid \\text{season})$ grows proportionally to the square of the seasonal mean. The following principle will be used: if $\\operatorname{Var}(Y \\mid \\text{season } j) \\propto \\mu_j^2$ where $\\mu_j = \\mathbb{E}[Y \\mid \\text{season } j]$, then by the delta method, for small coefficients of variation, $\\operatorname{Var}(\\log Y \\mid \\text{season } j) \\approx \\operatorname{Var}(Y \\mid \\text{season } j)/\\mu_j^2 \\approx \\text{constant}$ across $j$.\n\nYour program must implement the following decision rule to choose between additive and multiplicative STL, using only the within-season mean–standard deviation relationship:\n\n1. Given $m$, $K$, and data $\\{Y_t\\}_{t=1}^N$ with $N = mK$, define season index $j \\in \\{1, 2, \\dots, m\\}$ and year index $k \\in \\{1, 2, \\dots, K\\}$ so that $Y_{j,k}$ denotes the observation at month $j$ of year $k$. For each season $j$, compute the sample mean $\\hat{\\mu}_j$ and sample standard deviation $\\hat{s}_j$ of $\\{Y_{j,k}\\}_{k=1}^K$. Compute the Pearson correlation $r_{\\text{raw}}$ between the vectors $(\\hat{\\mu}_1, \\dots, \\hat{\\mu}_m)$ and $(\\hat{s}_1, \\dots, \\hat{s}_m)$ using the usual sample correlation formula \n$$\nr(x,y) = \\frac{\\sum_{i=1}^m (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^m (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^m (y_i - \\bar{y})^2}}.\n$$\n\n2. Define $Y'_t = \\log(Y_t + \\varepsilon)$ with $\\varepsilon = 0.5$. Repeat the above to obtain $\\hat{\\mu}'_j$, $\\hat{s}'_j$, and $r_{\\log}$ from the log-transformed data.\n\n3. Decide multiplicative STL (i.e., apply STL on the log scale) if and only if $r_{\\text{raw}} - r_{\\log} \\ge \\tau$ and $r_{\\text{raw}} \\ge \\rho_{\\min}$, with thresholds $\\tau = 0.3$ and $\\rho_{\\min} = 0.3$. Otherwise decide additive STL (i.e., apply STL on the original scale).\n\nTo make the task fully testable without external data, generate synthetic epidemiological time series under scientifically plausible data-generating mechanisms, using the following parameterized families. For $t = 1, 2, \\dots, N$ with $N = mK$, let $j = 1 + \\big((t-1) \\bmod m\\big)$ denote the season-of-year (month). Define trends and seasonal factors as follows:\n\n- Additive trend: $T^{\\text{add}}_t = b + \\alpha (t-1)$ with $b > 0$ and slope $\\alpha \\ge 0$.\n- Multiplicative (exponential) trend: $T^{\\text{mult}}_t = b \\times \\exp\\big(\\gamma (t-1)/N\\big)$ with $b > 0$ and growth parameter $\\gamma \\ge 0$.\n- Additive season: $S^{\\text{add}}_j = A \\sin\\left(2\\pi j/m\\right)$ with amplitude $A \\ge 0$.\n- Multiplicative season (positive): $S^{\\text{mult}}_j = 1 + A \\sin\\left(2\\pi j/m\\right)$ with amplitude $A \\in [0,1)$ to keep $S^{\\text{mult}}_j > 0$.\n\nNoise mechanisms:\n\n- Additive Gaussian noise: $E^{\\text{add}}_t \\sim \\mathcal{N}(0, \\sigma^2)$, independent across $t$.\n- Multiplicative lognormal noise: $E^{\\text{mult}}_t \\sim \\operatorname{LogNormal}(0, \\sigma_{\\log}^2)$, independent across $t$.\n- Poisson sampling noise: $Y_t \\sim \\operatorname{Poisson}(\\lambda_t)$ with mean $\\lambda_t$.\n\nConstruct the observation $Y_t$ according to one of:\n- Additive model: $Y_t = T^{\\text{add}}_t + S^{\\text{add}}_j + E^{\\text{add}}_t$, constrained to be nonnegative by construction of parameters.\n- Multiplicative model with lognormal noise: $Y_t = T^{\\text{mult}}_t \\times S^{\\text{mult}}_j \\times E^{\\text{mult}}_t$.\n- Poisson counts: $Y_t \\sim \\operatorname{Poisson}\\left(T^{\\text{mult}}_t \\times S^{\\text{mult}}_j\\right)$.\n\nImplement deterministic generation using fixed random seeds, then apply the decision rule. Your program must solve the following test suite of parameter values, each specified as a tuple and described in words. For each case, produce a boolean indicating whether multiplicative STL (log-scale) is chosen (true) or additive STL (original-scale) is chosen (false):\n\n- Case $1$ (happy path multiplicative): $(m, K, b, \\gamma, A, \\sigma_{\\log}, \\text{noise}, \\text{seed}) = (12, 8, 20.0, 0.5, 0.5, 0.25, \\text{`multiplicative_lognormal`}, 123)$. Use $T^{\\text{mult}}_t$, $S^{\\text{mult}}_j$, and lognormal noise.\n- Case $2$ (happy path additive): $(m, K, b, \\alpha, A, \\sigma, \\text{noise}, \\text{seed}) = (12, 8, 20.0, 0.2, 5.0, 3.0, \\text{`additive_gaussian`}, 456)$. Use $T^{\\text{add}}_t$, $S^{\\text{add}}_j$, and additive Gaussian noise.\n- Case $3$ (boundary near-constant): $(m, K, b, \\alpha, A, \\sigma, \\text{noise}, \\text{seed}) = (12, 6, 15.0, 0.0, 0.0, 0.1, \\text{`additive_gaussian`}, 789)$. Use $T^{\\text{add}}_t$, $S^{\\text{add}}_j$, and additive Gaussian noise, yielding nearly constant series.\n- Case $4$ (edge with zeros via Poisson): $(m, K, b, \\gamma, A, \\text{noise}, \\text{seed}) = (12, 10, 1.2, 0.3, 0.8, \\text{`poisson`}, 321)$. Use $T^{\\text{mult}}_t$, $S^{\\text{mult}}_j$, and Poisson sampling.\n\nThere are no physical units to report because the output is boolean-valued choices.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[true,false,true,true]\" but using Python boolean capitalization), in the order of Cases $1$ through $4$.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of time series analysis, specifically concerning the choice between additive and multiplicative decomposition models. The problem is well-posed, objective, and provides a complete, self-contained specification for a deterministic computational task. All parameters, models, and decision criteria are defined with sufficient precision to permit a unique solution. The synthetic data generation processes are based on standard, plausible models used in epidemiology and related fields.\n\nThe core of the problem is to implement a diagnostic test to decide whether an epidemiological time series is better described by an additive model, $Y_t = T_t + S_t + R_t$, or a multiplicative model, $Y_t = T_t \\times S_t \\times R_t$. Here, $Y_t$ is the observed data at time $t$, $T_t$ is the trend, $S_t$ is the seasonal component, and $R_t$ is the residual or noise term. The choice is fundamental as it dictates the appropriate method for decomposition, such as using Seasonal-Trend decomposition using Loess (STL) on the original data (additive) or on log-transformed data (multiplicative).\n\nThe guiding principle is based on the relationship between the mean and the variance of the series. Many processes that generate count data, such as those modeled by Poisson or Lognormal distributions, exhibit heteroscedasticity, where the variance of the observations increases with the mean level. In a multiplicative model, where the seasonal and noise components act as scaling factors on the trend, the standard deviation of the data within a particular season tends to be proportional to the mean level of that season. That is, if $\\mu_j$ is the expected value for season $j$, then $\\operatorname{StdDev}(Y \\mid \\text{season } j) \\propto \\mu_j$. This implies $\\operatorname{Var}(Y \\mid \\text{season } j) \\propto \\mu_j^2$. In contrast, a simple additive model with homoscedastic noise ($R_t$ having constant variance) would show no systematic relationship between the seasonal mean and the seasonal standard deviation.\n\nA logarithmic transformation, $Y'_t = \\log(Y_t + \\varepsilon)$ for a small offset $\\varepsilon > 0$, is often used to stabilize the variance in multiplicative models. By the delta method, if $\\operatorname{Var}(Y) \\propto (\\mathbb{E}[Y])^2$, then $\\operatorname{Var}(\\log Y)$ will be approximately constant. This transformation converts a multiplicative structure into an additive one: $\\log(T_t \\times S_t \\times R_t) = \\log T_t + \\log S_t + \\log R_t$.\n\nThe specified decision rule operationalizes this principle. It uses the Pearson correlation coefficient between the seasonal means and seasonal standard deviations as a metric for the strength of the mean-variance relationship. The algorithm is as follows:\n\n1.  For a given time series $\\{Y_t\\}_{t=1}^N$ with seasonal period $m$ and $K$ years, the data are grouped by season. For each season $j \\in \\{1, 2, \\dots, m\\}$, the sample mean $\\hat{\\mu}_j$ and sample standard deviation $\\hat{s}_j$ are calculated across the $K$ years.\n2.  The Pearson correlation coefficient, denoted $r_{\\text{raw}}$, is computed for the set of pairs $\\{(\\hat{\\mu}_j, \\hat{s}_j)\\}_{j=1}^m$. A strong positive correlation suggests a multiplicative structure.\n3.  The data are transformed using $Y'_t = \\log(Y_t + \\varepsilon)$, with $\\varepsilon = 0.5$. The process is repeated to calculate a new set of seasonal means $\\hat{\\mu}'_j$ and standard deviations $\\hat{s}'_j$, and their correlation, $r_{\\log}$.\n4.  If the log transform successively stabilizes the variance, $r_{\\log}$ should be substantially smaller than $r_{\\text{raw}}$. The decision to choose the multiplicative model is made if two conditions are met:\n    a. The correlation on the raw scale is sufficiently high, indicating a strong initial mean-variance relationship: $r_{\\text{raw}} \\ge \\rho_{\\min}$, with $\\rho_{\\min} = 0.3$.\n    b. The log transform significantly reduces this correlation, indicating successful variance stabilization: $r_{\\text{raw}} - r_{\\log} \\ge \\tau$, with $\\tau = 0.3$.\n    If both conditions hold, the multiplicative model is chosen. Otherwise, the additive model is chosen by default.\n\nThe implementation will proceed by first generating synthetic time series according to the four specified test cases. Each case uses a different combination of trend, seasonal, and noise models to simulate plausible scenarios. For each generated series, the diagnostic procedure is executed to obtain a boolean decision.\n\nThe function `calculate_correlation` will take the time series data, $m$, and $K$ as input. It will reshape the $N$-point series into a $K \\times m$ matrix, where rows represent years and columns represent seasons. It will then compute the mean and sample standard deviation (using a delta-degrees-of-freedom of $1$) for each column (season). Finally, it will compute the Pearson correlation between the resulting mean and standard deviation vectors using `numpy.corrcoef` and return the value.\n\nThe main function will orchestrate this process for each test case, applying the decision rule with the given threshold parameters $\\tau = 0.3$ and $\\rho_{\\min} = 0.3$ and the log-transform offset $\\varepsilon = 0.5$. The results will be collected and formatted as specified. Random seeds are fixed for reproducibility.", "answer": "```python\nimport numpy as np\n\ndef calculate_correlation(data: np.ndarray, m: int, K: int) -> float:\n    \"\"\"\n    Calculates the Pearson correlation between seasonal means and standard deviations.\n\n    Args:\n        data: A 1D numpy array representing the time series.\n        m: The seasonal period.\n        K: The number of complete years.\n\n    Returns:\n        The Pearson correlation coefficient.\n    \"\"\"\n    if data.ndim != 1 or len(data) != m * K:\n        raise ValueError(\"Data must be a 1D array of length m*K.\")\n\n    # Reshape data into a (K, m) matrix [years x seasons]\n    series_matrix = data.reshape(K, m)\n\n    # Calculate mean and standard deviation for each season (column)\n    # ddof=1 for sample standard deviation\n    seasonal_means = np.mean(series_matrix, axis=0)\n    seasonal_stds = np.std(series_matrix, axis=0, ddof=1)\n    \n    # If all standard deviations are zero, correlation is undefined (NaN).\n    # This can happen if the series is constant within each season.\n    # np.corrcoef handles this by returning NaN.\n    if np.all(seasonal_stds == 0):\n        return 0.0 # Define as 0 to avoid NaN propagation issues in comparisons\n\n    # Calculate Pearson correlation coefficient\n    # np.corrcoef returns a 2x2 matrix, we need the off-diagonal element\n    # It returns nan if one of the inputs is constant, which is desired.\n    corr_matrix = np.corrcoef(seasonal_means, seasonal_stds)\n    \n    correlation = corr_matrix[0, 1]\n    \n    # If correlation is NaN (due to constant stds or means), treat it as 0.\n    if np.isnan(correlation):\n        return 0.0\n        \n    return correlation\n\ndef apply_decision_rule(Y: np.ndarray, m: int, K: int, epsilon: float, tau: float, rho_min: float) -> bool:\n    \"\"\"\n    Applies the decision rule to choose between additive and multiplicative models.\n\n    Args:\n        Y: The time series data.\n        m: The seasonal period.\n        K: The number of years.\n        epsilon: Offset for log transform.\n        tau: Threshold for correlation reduction.\n        rho_min: Threshold for raw correlation.\n\n    Returns:\n        True if multiplicative is chosen, False otherwise.\n    \"\"\"\n    # 1. Compute correlation for raw data\n    r_raw = calculate_correlation(Y, m, K)\n\n    # 2. Compute correlation for log-transformed data\n    Y_prime = np.log(Y + epsilon)\n    r_log = calculate_correlation(Y_prime, m, K)\n\n    # 3. Apply the decision rule\n    is_multiplicative = (r_raw - r_log >= tau) and (r_raw >= rho_min)\n    return is_multiplicative\n\ndef generate_series(params: dict):\n    \"\"\"\n    Generates a synthetic time series based on the provided parameters.\n    \"\"\"\n    np.random.seed(params['seed'])\n    \n    m = params['m']\n    K = params['K']\n    N = m * K\n    t_idx = np.arange(N)\n    j_idx = t_idx % m\n    \n    model_type = params['model']\n    \n    if model_type == 'additive_gaussian':\n        b = params['b']\n        alpha = params['alpha']\n        A = params['A']\n        sigma = params['sigma']\n        \n        trend = b + alpha * t_idx\n        seasonality = A * np.sin(2 * np.pi * (j_idx + 1) / m)\n        noise = np.random.normal(0, sigma, N)\n        \n        Y = trend + seasonality + noise\n        # As per problem, data represent non-negative counts.\n        # Although parameters are chosen to make negative values unlikely,\n        # we enforce this for robustness.\n        Y = np.maximum(0, Y)\n\n    elif model_type == 'multiplicative_lognormal':\n        b = params['b']\n        gamma = params['gamma']\n        A = params['A']\n        sigma_log = params['sigma_log']\n\n        trend = b * np.exp(gamma * t_idx / N)\n        seasonality = 1 + A * np.sin(2 * np.pi * (j_idx + 1) / m)\n        noise = np.random.lognormal(0, sigma_log, N)\n        \n        Y = trend * seasonality * noise\n\n    elif model_type == 'poisson':\n        b = params['b']\n        gamma = params['gamma']\n        A = params['A']\n        \n        trend = b * np.exp(gamma * t_idx / N)\n        seasonality = 1 + A * np.sin(2 * np.pi * (j_idx + 1) / m)\n        \n        lambda_t = trend * seasonality\n        # Ensure lambda is non-negative, although parameters should guarantee this.\n        lambda_t = np.maximum(0, lambda_t)\n        Y = np.random.poisson(lambda_t)\n\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n        \n    return Y, m, K\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Multiplicative model with lognormal noise\n        {'model': 'multiplicative_lognormal', 'm': 12, 'K': 8, 'b': 20.0, 'gamma': 0.5, 'A': 0.5, 'sigma_log': 0.25, 'seed': 123},\n        # Case 2: Additive model with Gaussian noise\n        {'model': 'additive_gaussian', 'm': 12, 'K': 8, 'b': 20.0, 'alpha': 0.2, 'A': 5.0, 'sigma': 3.0, 'seed': 456},\n        # Case 3: Additive model, nearly constant\n        {'model': 'additive_gaussian', 'm': 12, 'K': 6, 'b': 15.0, 'alpha': 0.0, 'A': 0.0, 'sigma': 0.1, 'seed': 789},\n        # Case 4: Multiplicative mean structure with Poisson noise\n        {'model': 'poisson', 'm': 12, 'K': 10, 'b': 1.2, 'gamma': 0.3, 'A': 0.8, 'seed': 321}\n    ]\n\n    # Decision rule parameters\n    epsilon = 0.5\n    tau = 0.3\n    rho_min = 0.3\n\n    results = []\n    for params in test_cases:\n        Y, m, K = generate_series(params)\n        is_multiplicative = apply_decision_rule(Y, m, K, epsilon, tau, rho_min)\n        results.append(is_multiplicative)\n\n    # Format the output as specified: [True,False,...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4642189"}, {"introduction": "Analyzing long-term secular trends requires careful attention to confounding factors, especially demographic changes in the population. This practice introduces direct age standardization, a cornerstone of epidemiology for comparing rates over time or between populations. By implementing this method, you will learn to adjust for changes in age structure, allowing you to isolate the true underlying secular trend in disease risk [@problem_id:4642214].", "problem": "A population-based disease surveillance program seeks to quantify secular trends in incidence over long time spans while removing confounding due to changes in age structure. The program will use direct age standardization across two decades for multiple data sets, each with a fixed standard population. Starting from fundamental definitions in epidemiology and basic properties of weighted averages, derive an algorithm that, for each data set, calculates the age-standardized incidence rate for each of two decades and the secular change between them.\n\nUse the following foundational base without introducing any shortcut formulas in the problem statement. For an age group index $i \\in \\{1,\\dots,k\\}$ and time index $t \\in \\{t_1,t_2\\}$:\n- The age-specific incidence rate is defined by $r_{i,t} = c_{i,t}/N_{i,t}$, where $c_{i,t}$ is the number of incident cases and $N_{i,t}$ is the person-time or population-at-risk for the same age group and decade.\n- A fixed standard population provides weights through normalization: use a nonnegative vector of counts $S = (S_1,\\dots,S_k)$ to construct normalized weights $w_i$ that sum to $1$.\n- Express final age-standardized rates in per-$10^5$ person-years. Let $m = 10^5$ be the multiplier.\n\nYour task is to:\n1. Derive, from these definitions, how to compute the direct age-standardized incidence rate for each of two decades, using the same fixed standard population for both decades to isolate secular trends.\n2. Define the secular change as an absolute difference between the age-standardized incidence rates of the two decades.\n3. Implement a program that computes, for each test case, the triple consisting of the two age-standardized rates and their absolute difference, all expressed in per-$10^5$ person-years and rounded to $2$ decimal places.\n\nScientific realism: The data sets below are internally consistent and plausible for an infectious or chronic disease surveillance program. Seasonality is not considered because the aggregation spans decades; the focus is on secular trends. All age groups within each test case share the same fixed standard population used for both decades in that case.\n\nUnits: All rates must be reported in per-$10^5$ person-years.\n\nTest suite and parameter specification (each vector is ordered as $(G_1,G_2,G_3,G_4,G_5)$ for five age groups):\n- Test case $1$ (happy path; moderate aging and moderate incidence change):\n  - Standard population $S^{(1)} = (300000, 320000, 200000, 140000, 40000)$.\n  - Decade $t_1$: cases $C^{(1)}_{t_1} = (180, 450, 720, 980, 500)$, populations $N^{(1)}_{t_1} = (200000, 180000, 120000, 70000, 20000)$.\n  - Decade $t_2$: cases $C^{(1)}_{t_2} = (160, 400, 800, 1100, 600)$, populations $N^{(1)}_{t_2} = (180000, 160000, 130000, 90000, 30000)$.\n- Test case $2$ (edge case; zero cases in a young age group, substantial changes in older groups):\n  - Standard population $S^{(2)} = (400000, 300000, 180000, 100000, 20000)$.\n  - Decade $t_1$: cases $C^{(2)}_{t_1} = (0, 200, 500, 600, 150)$, populations $N^{(2)}_{t_1} = (250000, 200000, 100000, 40000, 5000)$.\n  - Decade $t_2$: cases $C^{(2)}_{t_2} = (0, 210, 600, 700, 350)$, populations $N^{(2)}_{t_2} = (240000, 220000, 120000, 60000, 10000)$.\n- Test case $3$ (boundary; large age-structure shift across decades but constant age-specific rates):\n  - Standard population $S^{(3)} = (500000, 250000, 150000, 80000, 20000)$.\n  - Decade $t_1$: cases $C^{(3)}_{t_1} = (50, 100, 400, 1000, 2000)$, populations $N^{(3)}_{t_1} = (100000, 100000, 100000, 100000, 100000)$.\n  - Decade $t_2$: cases $C^{(3)}_{t_2} = (200, 200, 480, 600, 400)$, populations $N^{(3)}_{t_2} = (400000, 200000, 120000, 60000, 20000)$.\n- Test case $4$ (small populations; check numerical stability and rounding with rare events):\n  - Standard population $S^{(4)} = (200000, 300000, 250000, 200000, 50000)$.\n  - Decade $t_1$: cases $C^{(4)}_{t_1} = (2, 9, 15, 30, 12)$, populations $N^{(4)}_{t_1} = (20000, 30000, 25000, 20000, 5000)$.\n  - Decade $t_2$: cases $C^{(4)}_{t_2} = (3, 12, 20, 45, 25)$, populations $N^{(4)}_{t_2} = (20000, 30000, 25000, 20000, 5000)$.\n\nProgram output format:\n- For each test case, compute three values in order: the age-standardized incidence rate for decade $t_1$, the age-standardized incidence rate for decade $t_2$, and the absolute secular change $|R_{t_2} - R_{t_1}|$, all in per-$10^5$ person-years and rounded to $2$ decimal places.\n- Aggregate the results of all test cases into a single flat list in the order of the test cases: $[R^{(1)}_{t_1}, R^{(1)}_{t_2}, |R^{(1)}_{t_2} - R^{(1)}_{t_1}|, R^{(2)}_{t_1}, R^{(2)}_{t_2}, |R^{(2)}_{t_2} - R^{(2)}_{t_1}|, R^{(3)}_{t_1}, R^{(3)}_{t_2}, |R^{(3)}_{t_2} - R^{(3)}_{t_1}|, R^{(4)}_{t_1}, R^{(4)}_{t_2}, |R^{(4)}_{t_2} - R^{(4)}_{t_1}|]$.\n- Your program should produce a single line of output containing this list as a comma-separated sequence enclosed in square brackets, for example $[x_1,x_2,\\dots,x_{12}]$.", "solution": "The problem requires the derivation and implementation of an algorithm to calculate direct age-standardized incidence rates for two distinct time periods (decades) and the secular change between them. This method is fundamental in epidemiology for comparing rates across time or populations while controlling for the confounding effect of differing age structures. The derivation will proceed from the foundational definitions provided.\n\nLet the index for age groups be $i \\in \\{1, 2, \\dots, k\\}$, and the index for the two decades be $t \\in \\{t_1, t_2\\}$. The given quantities for each test case are:\n- $C_{t} = (c_{1,t}, c_{2,t}, \\dots, c_{k,t})$: A vector of incident case counts for a given decade $t$.\n- $N_{t} = (N_{1,t}, N_{2,t}, \\dots, N_{k,t})$: A vector of person-time or population-at-risk counts for the corresponding age groups and decade.\n- $S = (S_1, S_2, \\dots, S_k)$: A vector of population counts for a fixed standard population.\n- $m = 10^5$: A multiplier to express rates per $10^5$ person-years.\n\n**1. Derivation of Age-Specific Incidence Rates**\n\nThe foundational measure of disease frequency in a specific subpopulation is the age-specific incidence rate. As defined in the problem, for age group $i$ and decade $t$, this rate, denoted $r_{i,t}$, is the ratio of new cases to the total person-time at risk.\n$$r_{i,t} = \\frac{c_{i,t}}{N_{i,t}}$$\nThis calculation is performed for each age group $i$ and for each of the two decades, $t_1$ and $t_2$.\n\n**2. Derivation of Standardized Weights**\n\nThe core of direct standardization is the application of a common age structure to the populations being compared. This common structure is provided by the standard population $S$. We must derive a set of normalized weights, $w_i$, that represent the proportion of the standard population in each age group $i$.\n\nThe total size of the standard population is the sum of its age-specific counts:\n$$S_{total} = \\sum_{j=1}^{k} S_j$$\nThe weight for age group $i$, $w_i$, is the proportion of this total population that belongs to group $i$:\n$$w_i = \\frac{S_i}{S_{total}} = \\frac{S_i}{\\sum_{j=1}^{k} S_j}$$\nBy this construction, the weights are non-negative (since $S_i \\ge 0$) and sum to unity, a necessary property for a weighted average:\n$$\\sum_{i=1}^{k} w_i = \\sum_{i=1}^{k} \\frac{S_i}{\\sum_{j=1}^{k} S_j} = \\frac{\\sum_{i=1}^{k} S_i}{\\sum_{j=1}^{k} S_j} = 1$$\n\n**3. Derivation of the Age-Standardized Incidence Rate (ASR)**\n\nThe direct age-standardized incidence rate for a given decade $t$, which we denote $R_t$, is a weighted average of the age-specific incidence rates $r_{i,t}$. The weights used are the standard population weights $w_i$ derived above. This procedure yields the overall rate that would be observed in the study population if it had the same age structure as the standard population.\n\nThe raw standardized rate, $R_{t, \\text{raw}}$, is computed as:\n$$R_{t, \\text{raw}} = \\sum_{i=1}^{k} w_i \\cdot r_{i,t}$$\nSubstituting the previously derived expressions for $w_i$ and $r_{i,t}$:\n$$R_{t, \\text{raw}} = \\sum_{i=1}^{k} \\left( \\frac{S_i}{\\sum_{j=1}^{k} S_j} \\right) \\cdot \\left( \\frac{c_{i,t}}{N_{i,t}} \\right)$$\nThe problem requires the final rates to be expressed per $m = 10^5$ person-years. Therefore, we scale the raw rate by this multiplier:\n$$R_t = m \\cdot R_{t, \\text{raw}} = 10^5 \\cdot \\sum_{i=1}^{k} \\left( \\frac{S_i}{\\sum_{j=1}^{k} S_j} \\right) \\left( \\frac{c_{i,t}}{N_{i,t}} \\right)$$\nThis calculation is performed for both decade $t_1$ and decade $t_2$, using the respective case and population counts ($c_{i,t_1}, N_{i,t_1}$ and $c_{i,t_2}, N_{i,t_2}$), but crucially, using the *same* standard population weights $w_i$ for both. This ensures that any observed difference between $R_{t_1}$ and $R_{t_2}$ is not due to changes in the age composition of the study populations over the two decades.\n\n**4. Definition of Secular Change**\n\nSecular change quantifies the trend in incidence over the long term. As specified, we define it as the absolute difference between the age-standardized rates of the two decades:\n$$\\Delta R = |R_{t_2} - R_{t_1}|$$\nThis value represents the magnitude of the change in the age-adjusted incidence rate between decade $t_1$ and $t_2$, expressed per $10^5$ person-years.\n\n**5. Algorithmic Summary**\n\nThe complete algorithm to be implemented is as follows:\nFor each test case provided:\n1.  Receive the input vectors for the standard population $S$, cases $C_{t_1}$ and $C_{t_2}$, and study populations $N_{t_1}$ and $N_{t_2}$.\n2.  Calculate the total standard population $S_{total} = \\sum_{i=1}^{k} S_i$.\n3.  Calculate the vector of standard weights $W = (w_1, \\dots, w_k)$ where $w_i = S_i / S_{total}$.\n4.  For decade $t_1$:\n    a. Calculate the vector of age-specific rates $R'_{t_1} = (r_{1,t_1}, \\dots, r_{k,t_1})$ where $r_{i,t_1} = c_{i,t_1} / N_{i,t_1}$.\n    b. Calculate the standardized rate $R_{t_1} = 10^5 \\cdot \\sum_{i=1}^{k} w_i \\cdot r_{i,t_1}$.\n5.  For decade $t_2$:\n    a. Calculate the vector of age-specific rates $R'_{t_2} = (r_{1,t_2}, \\dots, r_{k,t_2})$ where $r_{i,t_2} = c_{i,t_2} / N_{i,t_2}$.\n    b. Calculate the standardized rate $R_{t_2} = 10^5 \\cdot \\sum_{i=1}^{k} w_i \\cdot r_{i,t_2}$.\n6.  Calculate the secular change $\\Delta R = |R_{t_2} - R_{t_1}|$.\n7.  Round the three computed values, $R_{t_1}$, $R_{t_2}$, and $\\Delta R$, to two decimal places.\n8.  Store the resulting triplet of values.\n\nThis procedure will be executed for all test cases, and the results will be aggregated into a single flat list for output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an algorithm for direct age standardization to compute\n    age-standardized incidence rates and their secular change for multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        (\n            (300000, 320000, 200000, 140000, 40000),  # Standard population S^(1)\n            (180, 450, 720, 980, 500),                # Cases C^(1)_t1\n            (200000, 180000, 120000, 70000, 20000),  # Population N^(1)_t1\n            (160, 400, 800, 1100, 600),                # Cases C^(1)_t2\n            (180000, 160000, 130000, 90000, 30000)   # Population N^(1)_t2\n        ),\n        # Test case 2\n        (\n            (400000, 300000, 180000, 100000, 20000),  # Standard population S^(2)\n            (0, 200, 500, 600, 150),                   # Cases C^(2)_t1\n            (250000, 200000, 100000, 40000, 5000),   # Population N^(2)_t1\n            (0, 210, 600, 700, 350),                   # Cases C^(2)_t2\n            (240000, 220000, 120000, 60000, 10000)   # Population N^(2)_t2\n        ),\n        # Test case 3\n        (\n            (500000, 250000, 150000, 80000, 20000),  # Standard population S^(3)\n            (50, 100, 400, 1000, 2000),                # Cases C^(3)_t1\n            (100000, 100000, 100000, 100000, 100000), # Population N^(3)_t1\n            (200, 200, 480, 600, 400),                 # Cases C^(3)_t2\n            (400000, 200000, 120000, 60000, 20000)    # Population N^(3)_t2\n        ),\n        # Test case 4\n        (\n            (200000, 300000, 250000, 200000, 50000),  # Standard population S^(4)\n            (2, 9, 15, 30, 12),                       # Cases C^(4)_t1\n            (20000, 30000, 25000, 20000, 5000),     # Population N^(4)_t1\n            (3, 12, 20, 45, 25),                      # Cases C^(4)_t2\n            (20000, 30000, 25000, 20000, 5000)      # Population N^(4)_t2\n        )\n    ]\n\n    results = []\n    multiplier = 1e5\n\n    for s_pop, c_t1, n_t1, c_t2, n_t2 in test_cases:\n        # Convert tuples to numpy arrays for vectorized operations, ensuring float division.\n        s_pop_arr = np.array(s_pop, dtype=float)\n        c_t1_arr = np.array(c_t1, dtype=float)\n        n_t1_arr = np.array(n_t1, dtype=float)\n        c_t2_arr = np.array(c_t2, dtype=float)\n        n_t2_arr = np.array(n_t2, dtype=float)\n\n        # Step 1: Calculate standard weights\n        total_s_pop = np.sum(s_pop_arr)\n        weights = s_pop_arr / total_s_pop\n\n        # Step 2: Calculate age-specific rates for each decade\n        age_specific_rates_t1 = c_t1_arr / n_t1_arr\n        age_specific_rates_t2 = c_t2_arr / n_t2_arr\n\n        # Step 3: Calculate age-standardized rates (ASR)\n        # The ASR is the weighted average of age-specific rates, scaled by the multiplier.\n        # np.sum(weights * rates) is equivalent to the dot product.\n        asr_t1 = np.sum(weights * age_specific_rates_t1) * multiplier\n        asr_t2 = np.sum(weights * age_specific_rates_t2) * multiplier\n\n        # Step 4: Calculate secular change\n        secular_change = np.abs(asr_t2 - asr_t1)\n\n        # Step 5: Round results to 2 decimal places\n        asr_t1_rounded = np.round(asr_t1, 2)\n        asr_t2_rounded = np.round(asr_t2, 2)\n        secular_change_rounded = np.round(secular_change, 2)\n\n        results.extend([asr_t1_rounded, asr_t2_rounded, secular_change_rounded])\n\n    # Format results to always show two decimal places and create the final output string\n    formatted_results = [f\"{x:.2f}\" for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4642214"}]}