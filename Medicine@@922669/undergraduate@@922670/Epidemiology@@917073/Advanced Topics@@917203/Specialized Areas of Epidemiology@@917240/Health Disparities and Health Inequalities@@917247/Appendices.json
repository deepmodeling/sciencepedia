{"hands_on_practices": [{"introduction": "A foundational task in epidemiology is to compare health outcomes, such as mortality, between different populations. This exercise [@problem_id:4595794] introduces the Standardized Mortality Ratio ($SMR$), a classic technique used to account for differences in population age structures. By calculating the $SMR$, you will learn to quantify the excess mortality in a study group relative to a standard population, providing a crucial first step in identifying and measuring a health disparity.", "problem": "A public health team is assessing mortality in a disadvantaged urban neighborhood cluster that experiences elevated chronic disease burden and limited access to preventive care. To study health inequalities, they decide to perform indirect age standardization against a national standard. Assume a one-year observation period with complete follow-up and negligible in- and out-migration, so that each resident contributes approximately $1$ person-year. The national standard age-specific mortality rates are:\n\n- Ages $0$–$44$: $1.2$ per $1{,}000$ person-years\n- Ages $45$–$64$: $5.6$ per $1{,}000$ person-years\n- Ages $65$–$74$: $14.0$ per $1{,}000$ person-years\n- Ages $75$–$84$: $40.0$ per $1{,}000$ person-years\n- Ages $\\ge 85$: $110.0$ per $1{,}000$ person-years\n\nThe disadvantaged group's age distribution at baseline is:\n\n- Ages $0$–$44$: $12{,}500$ residents\n- Ages $45$–$64$: $8{,}400$ residents\n- Ages $65$–$74$: $2{,}900$ residents\n- Ages $75$–$84$: $1{,}100$ residents\n- Ages $\\ge 85$: $270$ residents\n\nAcross the year, the disadvantaged group experiences $223$ observed deaths in total (all causes). Using only fundamental definitions of mortality, indirect standardization, and ratio measures, compute the standardized mortality ratio for the disadvantaged group relative to the national standard. Express the final standardized mortality ratio as a unitless decimal and round your answer to four significant figures.\n\nThen, explain the interpretational pitfalls of the standardized mortality ratio when the disadvantaged group's underlying health needs differ systematically from the standard population (for example, due to higher baseline morbidity, environmental exposures, or care access barriers). Your explanation should focus on why the ratio might conflate need with performance or other structural factors, and what assumptions are necessary for fair comparison.\n\nNote: Standardized Mortality Ratio (SMR) should be interpreted as a dimensionless ratio comparing observed deaths to the expected number of deaths under the standard rates.", "solution": "The problem requires the calculation of the Standardized Mortality Ratio (SMR) for a disadvantaged group and an explanation of the interpretational pitfalls of this measure in the context of health disparities. The solution proceeds in two parts: first, the quantitative calculation of the SMR, and second, a qualitative analysis of its assumptions and limitations.\n\n**Part 1: Calculation of the Standardized Mortality Ratio (SMR)**\n\nThe Standardized Mortality Ratio is defined as the ratio of the total number of observed deaths in the study population to the total number of expected deaths. The expected deaths are calculated by applying the age-specific mortality rates of a standard population to the age structure of the study population.\n\nThe formula for the SMR is:\n$$ \\text{SMR} = \\frac{\\text{Observed Deaths}}{\\text{Expected Deaths}} = \\frac{D_O}{D_E} $$\n\nThe total number of observed deaths, $D_O$, is given as $223$.\n\nThe total number of expected deaths, $D_E$, is the sum of the expected deaths in each age stratum $i$. The expected deaths for a given stratum, $d_{E,i}$, are calculated by multiplying the number of people in that stratum in the study population, $N_i$, by the corresponding age-specific mortality rate from the standard population, $M_i$.\n$$ D_E = \\sum_i d_{E,i} = \\sum_i (N_i \\times M_i) $$\n\nWe are given the following data:\n\nStudy Population ($N_i$):\n- Ages $0$–$44$: $N_1 = 12,500$\n- Ages $45$–$64$: $N_2 = 8,400$\n- Ages $65$–$74$: $N_3 = 2,900$\n- Ages $75$–$84$: $N_4 = 1,100$\n- Ages $\\ge 85$: $N_5 = 270$\n\nStandard Mortality Rates ($M_i$), converted from per $1,000$ person-years to a decimal rate:\n- Ages $0$–$44$: $M_1 = 1.2 / 1000 = 0.0012$\n- Ages $45$–$64$: $M_2 = 5.6 / 1000 = 0.0056$\n- Ages $65$–$74$: $M_3 = 14.0 / 1000 = 0.0140$\n- Ages $75–84$: $M_4 = 40.0 / 1000 = 0.0400$\n- Ages $\\ge 85$: $M_5 = 110.0 / 1000 = 0.1100$\n\nNow, we calculate the expected deaths for each age stratum:\n- Stratum $1$ (Ages $0$–$44$): $d_{E,1} = N_1 \\times M_1 = 12,500 \\times 0.0012 = 15.0$\n- Stratum $2$ (Ages $45$–$64$): $d_{E,2} = N_2 \\times M_2 = 8,400 \\times 0.0056 = 47.04$\n- Stratum $3$ (Ages $65$–$74$): $d_{E,3} = N_3 \\times M_3 = 2,900 \\times 0.0140 = 40.6$\n- Stratum $4$ (Ages $75$–$84$): $d_{E,4} = N_4 \\times M_4 = 1,100 \\times 0.0400 = 44.0$\n- Stratum $5$ (Ages $\\ge 85$): $d_{E,5} = N_5 \\times M_5 = 270 \\times 0.1100 = 29.7$\n\nThe total number of expected deaths, $D_E$, is the sum of these values:\n$$ D_E = 15.0 + 47.04 + 40.6 + 44.0 + 29.7 = 176.34 $$\n\nWith $D_O = 223$ and $D_E = 176.34$, we can now compute the SMR:\n$$ \\text{SMR} = \\frac{D_O}{D_E} = \\frac{223}{176.34} \\approx 1.2645967... $$\n\nRounding to four significant figures, we get:\n$$ \\text{SMR} \\approx 1.265 $$\nAn SMR of $1.265$ indicates that the disadvantaged group experienced approximately $26.5\\%$ more deaths than would have been expected if they had the same age-specific mortality rates as the national standard population.\n\n**Part 2: Interpretational Pitfalls of the SMR**\n\nThe calculation of the SMR is predicated on a critical, often unstated, assumption: that the standard population's age-specific mortality rates ($M_i$) are the appropriate rates to expect in the study population, once age has been accounted for. For the comparison to be fair, one must assume that, within each age stratum, the study and standard populations are otherwise comparable in terms of underlying health status and risk. This is known as the assumption of comparability of age-specific rate ratios across the populations.\n\nIn the context of health disparities, this assumption is fundamentally violated. The problem statement itself specifies that the study group is a \"disadvantaged urban neighborhood\" with \"elevated chronic disease burden and limited access to preventive care.\" This leads to significant interpretational pitfalls:\n\n1.  **Conflation of Need, Structural Factors, and Performance:** An elevated SMR does not cleanly isolate a single cause. The excess mortality ($26.5\\%$ in this case) is an aggregate effect of multiple, entangled factors:\n    *   **Higher Health Needs:** The disadvantaged population likely has a higher prevalence and severity of morbidity (illness) at any given age compared to the national average. For example, a $50$-year-old in this group may have a health profile more akin to a $60$-year-old in the standard population. Applying the standard mortality rate for $50$-year-olds to this group underestimates the \"expected\" mortality based on their actual health status.\n    *   **Structural Disadvantages:** Factors such as higher exposure to environmental pollution, chronic stress from socioeconomic hardship, food deserts, and unsafe housing can directly increase mortality risk, independent of healthcare access or quality. These factors are not present to the same degree in the standard population, and the SMR does not adjust for them.\n    *   **Healthcare System Performance:** The \"limited access to preventive care\" and likely lower quality of curative care contribute to a higher case-fatality rate for many conditions. This is a failure of the healthcare system to meet the population's needs.\n\n2.  **The Misleading Nature of \"Expected\" Deaths:** The term \"expected deaths\" is a technical one. It does not represent a realistic expectation for a population with a high burden of disease. It represents a hypothetical scenario: \"What if this population, despite its disadvantages, magically had the mortality risk of the healthier, better-resourced standard population?\" The SMR measures the gap between the observed reality and this counterfactual ideal. Attributing the entire gap to a single cause, such as \"poor hospital quality,\" is a form of ecological fallacy. It ignores the fact that the population was sicker to begin with.\n\nFor the SMR to be a fair comparison of, for instance, the quality of healthcare between two regions, it is necessary to assume that the populations have a similar underlying burden of disease and risk factor profile, differing only in the healthcare they receive. In the study of health inequalities, this assumption is invalid by definition. The SMR is therefore a useful tool for *quantifying* the magnitude of an inequality, but it is not a tool for *explaining* its causes without further information. It demonstrates that a problem exists but does not, by itself, diagnose the etiology of the problem.", "answer": "$$ \\boxed{1.265} $$", "id": "4595794"}, {"introduction": "Once a disparity in crude rates is identified, the next step is to understand its drivers. This practice [@problem_id:4595822] introduces the Kitagawa decomposition, an elegant method for partitioning the total difference in rates between two populations into two distinct components. You will implement this method to separate the effect of differing population compositions from the effect of differing subgroup-specific rates, gaining a deeper insight into the mechanics of health inequalities.", "problem": "You are given two populations, each partitioned into a finite number of mutually exclusive and collectively exhaustive subgroups. For each population, you know the subgroup composition (the vector of subgroup proportions that sum to $1$) and the subgroup-specific rates (interpretable as probabilities between $0$ and $1$ expressed as decimals, not using a percent sign). The overall (crude) rate of a population is defined as the weighted sum of subgroup-specific rates using subgroup composition weights. Starting only from these fundamental definitions, derive and implement an exact, symmetric decomposition of the difference in crude rates between the two populations into two additive components: one attributable to differences in population composition across subgroups, and one attributable to differences in subgroup-specific rates. The decomposition must have zero residual by construction and must be symmetric with respect to the two populations.\n\nFormally, for population $A$ with subgroup proportions $\\{p_{A,i}\\}_{i=1}^{k}$ and subgroup rates $\\{r_{A,i}\\}_{i=1}^{k}$, and for population $B$ with subgroup proportions $\\{p_{B,i}\\}_{i=1}^{k}$ and subgroup rates $\\{r_{B,i}\\}_{i=1}^{k}$, the crude rates are\n$$\nR_A = \\sum_{i=1}^{k} p_{A,i} \\, r_{A,i}, \\quad R_B = \\sum_{i=1}^{k} p_{B,i} \\, r_{B,i}.\n$$\nLet $\\Delta = R_A - R_B$. Decompose $\\Delta$ into the sum of two components, a composition component $C$ and a rate component $S$, such that $\\Delta = C + S$ exactly, where $C$ captures the contribution solely from differences in $\\{p_{A,i}\\}$ versus $\\{p_{B,i}\\}$, and $S$ captures the contribution solely from differences in $\\{r_{A,i}\\}$ versus $\\{r_{B,i}\\}$, in a manner that treats the two populations symmetrically.\n\nYour task is to write a complete program that computes, for each provided test case, the triple $[\\Delta, C, S]$ as real numbers rounded to six decimal places. All inputs are already provided in the program and must not be read from standard input. All subgroup proportions must be nonnegative and sum to $1$ within each population, and all subgroup rates must be in the interval $[0,1]$ (decimals, no percent sign).\n\nTest suite:\n- Case $1$ (general three-subgroup case):\n  - $p_A = [0.2, 0.3, 0.5]$, $r_A = [0.1, 0.15, 0.25]$\n  - $p_B = [0.3, 0.3, 0.4]$, $r_B = [0.05, 0.20, 0.35]$\n- Case $2$ (identical compositions; differences only in subgroup rates):\n  - $p_A = [0.4, 0.6]$, $r_A = [0.1, 0.2]$\n  - $p_B = [0.4, 0.6]$, $r_B = [0.15, 0.25]$\n- Case $3$ (identical subgroup rates; differences only in composition):\n  - $p_A = [0.2, 0.8]$, $r_A = [0.1, 0.3]$\n  - $p_B = [0.8, 0.2]$, $r_B = [0.1, 0.3]$\n- Case $4$ (edge case with zero subgroup rate and a zero subgroup proportion):\n  - $p_A = [0.5, 0.0, 0.5]$, $r_A = [0.0, 0.2, 0.4]$\n  - $p_B = [0.2, 0.3, 0.5]$, $r_B = [0.0, 0.1, 0.5]$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list of three numbers $[\\Delta, C, S]$. There must be no spaces anywhere in the output line. For example: $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\ldots]$.\n- All numbers must be rounded to six decimal places.", "solution": "We begin from the fundamental definitions of crude rates and subgroup stratification. Let there be $k$ disjoint subgroups indexed by $i \\in \\{1,\\ldots,k\\}$. For each population $A$ and $B$, denote subgroup compositions by $\\{p_{A,i}\\}$ and $\\{p_{B,i}\\}$, where $\\sum_{i=1}^{k} p_{A,i} = 1$ and $\\sum_{i=1}^{k} p_{B,i} = 1$, with all $p_{\\cdot,i} \\ge 0$. Denote subgroup-specific rates (for example, risks or incidence rates bounded between $0$ and $1$) by $\\{r_{A,i}\\}$ and $\\{r_{B,i}\\}$, respectively. The crude rates are given by\n$$\nR_A = \\sum_{i=1}^{k} p_{A,i} \\, r_{A,i}, \\quad R_B = \\sum_{i=1}^{k} p_{B,i} \\, r_{B,i}.\n$$\nWe seek an exact, symmetric decomposition of $\\Delta = R_A - R_B$ into a composition component $C$ and a rate component $S$ with no residual.\n\nStarting from\n$$\n\\Delta = \\sum_{i=1}^{k} \\big(p_{A,i} r_{A,i} - p_{B,i} r_{B,i}\\big),\n$$\nwe add and subtract terms that average the cross-population compositions and rates in a symmetric way. Specifically, add and subtract $\\tfrac{1}{2} p_{A,i} r_{B,i}$ and $\\tfrac{1}{2} p_{B,i} r_{A,i}$ inside the sum to create equal-weight bridges between populations. Then regroup to separate composition-only and rate-only contrasts:\n$$\n\\Delta = \\sum_{i=1}^{k} \\left[\\frac{(p_{A,i} - p_{B,i})(r_{A,i} + r_{B,i})}{2}\\right] + \\sum_{i=1}^{k} \\left[\\frac{(r_{A,i} - r_{B,i})(p_{A,i} + p_{B,i})}{2}\\right].\n$$\nDefine the composition component\n$$\nC = \\sum_{i=1}^{k} \\frac{(p_{A,i} - p_{B,i})(r_{A,i} + r_{B,i})}{2},\n$$\nand the rate component\n$$\nS = \\sum_{i=1}^{k} \\frac{(r_{A,i} - r_{B,i})(p_{A,i} + p_{B,i})}{2}.\n$$\nBy construction, $C$ depends only on composition differences modulated by the average of subgroup rates, and $S$ depends only on rate differences modulated by the average of subgroup compositions. This is the classical Kitagawa decomposition, which is exact and symmetric, yielding $\\Delta = C + S$ with zero residual.\n\nAlgorithmic steps for each test case:\n- Input vectors $p_A, r_A, p_B, r_B$ of equal length $k$ with valid compositions and rates.\n- Compute $R_A = \\sum_i p_{A,i} r_{A,i}$ and $R_B = \\sum_i p_{B,i} r_{B,i}$, then $\\Delta = R_A - R_B$.\n- Compute $C = \\sum_i \\big((p_{A,i} - p_{B,i}) \\cdot (r_{A,i} + r_{B,i}) / 2\\big)$.\n- Compute $S = \\sum_i \\big((r_{A,i} - r_{B,i}) \\cdot (p_{A,i} + p_{B,i}) / 2\\big)$.\n- Optionally verify numerically that $|\\Delta - (C + S)|$ is negligible (up to floating-point rounding).\n- Round $\\Delta, C, S$ to six decimal places and output as specified.\n\nNow we compute the requested values for each test case.\n\nCase $1$:\n- $p_A = [0.2, 0.3, 0.5]$, $r_A = [0.1, 0.15, 0.25]$; $p_B = [0.3, 0.3, 0.4]$, $r_B = [0.05, 0.20, 0.35]$.\n- $R_A = 0.2 \\cdot 0.1 + 0.3 \\cdot 0.15 + 0.5 \\cdot 0.25 = 0.02 + 0.045 + 0.125 = 0.19$.\n- $R_B = 0.3 \\cdot 0.05 + 0.3 \\cdot 0.20 + 0.4 \\cdot 0.35 = 0.015 + 0.06 + 0.14 = 0.215$.\n- $\\Delta = 0.19 - 0.215 = -0.025$.\n- $C = \\sum_i (p_{A,i} - p_{B,i}) \\cdot (r_{A,i} + r_{B,i}) / 2 = (-0.1)\\cdot 0.075 + 0.0 \\cdot 0.175 + 0.1 \\cdot 0.30 = -0.0075 + 0 + 0.03 = 0.0225$.\n- $S = \\sum_i (r_{A,i} - r_{B,i}) \\cdot (p_{A,i} + p_{B,i}) / 2 = 0.05 \\cdot 0.25 + (-0.05)\\cdot 0.30 + (-0.10)\\cdot 0.45 = 0.0125 - 0.015 - 0.045 = -0.0475$.\n- Check: $C + S = 0.0225 - 0.0475 = -0.025 = \\Delta$.\n\nCase $2$:\n- $p_A = [0.4, 0.6]$, $r_A = [0.1, 0.2]$; $p_B = [0.4, 0.6]$, $r_B = [0.15, 0.25]$.\n- $R_A = 0.4 \\cdot 0.1 + 0.6 \\cdot 0.2 = 0.04 + 0.12 = 0.16$.\n- $R_B = 0.4 \\cdot 0.15 + 0.6 \\cdot 0.25 = 0.06 + 0.15 = 0.21$.\n- $\\Delta = 0.16 - 0.21 = -0.05$.\n- $C = 0$ because $p_A = p_B$.\n- $S = -0.05$.\n- Check: $C + S = -0.05 = \\Delta$.\n\nCase $3$:\n- $p_A = [0.2, 0.8]$, $r_A = [0.1, 0.3]$; $p_B = [0.8, 0.2]$, $r_B = [0.1, 0.3]$.\n- $R_A = 0.2 \\cdot 0.1 + 0.8 \\cdot 0.3 = 0.02 + 0.24 = 0.26$.\n- $R_B = 0.8 \\cdot 0.1 + 0.2 \\cdot 0.3 = 0.08 + 0.06 = 0.14$.\n- $\\Delta = 0.26 - 0.14 = 0.12$.\n- $C = (-0.6)\\cdot 0.1 + 0.6 \\cdot 0.3 = -0.06 + 0.18 = 0.12$.\n- $S = 0$ because $r_A = r_B$.\n- Check: $C + S = 0.12 = \\Delta$.\n\nCase $4$:\n- $p_A = [0.5, 0.0, 0.5]$, $r_A = [0.0, 0.2, 0.4]$; $p_B = [0.2, 0.3, 0.5]$, $r_B = [0.0, 0.1, 0.5]$.\n- $R_A = 0.5 \\cdot 0.0 + 0.0 \\cdot 0.2 + 0.5 \\cdot 0.4 = 0.2$.\n- $R_B = 0.2 \\cdot 0.0 + 0.3 \\cdot 0.1 + 0.5 \\cdot 0.5 = 0.28$.\n- $\\Delta = 0.2 - 0.28 = -0.08$.\n- $C = (0.3)\\cdot 0 + (-0.3)\\cdot 0.15 + (0.0)\\cdot 0.45 = -0.045$.\n- $S = (0.0)\\cdot 0.35 + (0.1)\\cdot 0.15 + (-0.1)\\cdot 0.5 = -0.035$.\n- Check: $C + S = -0.08 = \\Delta$.\n\nThe program will implement these computations generically for any valid input vectors and will print the results for the provided test suite as a single line containing a bracketed list of triples, with no spaces, and numbers rounded to six decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef kitagawa_decomposition(pA, rA, pB, rB):\n    \"\"\"\n    Compute the Kitagawa decomposition for two populations:\n    - pA, rA: compositions and subgroup rates for population A\n    - pB, rB: compositions and subgroup rates for population B\n\n    Returns:\n        delta, comp_effect, rate_effect as floats.\n    \"\"\"\n    pA = np.asarray(pA, dtype=float)\n    rA = np.asarray(rA, dtype=float)\n    pB = np.asarray(pB, dtype=float)\n    rB = np.asarray(rB, dtype=float)\n\n    if not (pA.shape == rA.shape == pB.shape == rB.shape):\n        raise ValueError(\"All input vectors must have the same shape.\")\n\n    # Crude rates\n    RA = float(np.sum(pA * rA))\n    RB = float(np.sum(pB * rB))\n    delta = RA - RB\n\n    # Kitagawa components (symmetric, exact)\n    comp_effect = float(np.sum((pA - pB) * (rA + rB) / 2.0))\n    rate_effect = float(np.sum((rA - rB) * (pA + pB) / 2.0))\n\n    return delta, comp_effect, rate_effect\n\ndef format_float(x, places=6, zero_eps=5e-13):\n    \"\"\"Format a float to a fixed number of decimal places, avoiding negative zero.\"\"\"\n    if abs(x)  zero_eps:\n        x = 0.0\n    return f\"{x:.{places}f}\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (pA, rA, pB, rB)\n    test_cases = [\n        # Case 1\n        ([0.2, 0.3, 0.5], [0.1, 0.15, 0.25],\n         [0.3, 0.3, 0.4], [0.05, 0.20, 0.35]),\n        # Case 2\n        ([0.4, 0.6], [0.1, 0.2],\n         [0.4, 0.6], [0.15, 0.25]),\n        # Case 3\n        ([0.2, 0.8], [0.1, 0.3],\n            [0.8, 0.2], [0.1, 0.3]),\n        # Case 4\n        ([0.5, 0.0, 0.5], [0.0, 0.2, 0.4],\n         [0.2, 0.3, 0.5], [0.0, 0.1, 0.5]),\n    ]\n\n    results_str_parts = []\n    for pA, rA, pB, rB in test_cases:\n        delta, comp, rate = kitagawa_decomposition(pA, rA, pB, rB)\n\n        # Round and format to six decimal places as required.\n        delta_s = format_float(delta, 6)\n        comp_s = format_float(comp, 6)\n        rate_s = format_float(rate, 6)\n\n        triple_str = f\"[{delta_s},{comp_s},{rate_s}]\"\n        results_str_parts.append(triple_str)\n\n    # Final print statement in the exact required format: no spaces.\n    print(f\"[{','.join(results_str_parts)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4595822"}, {"introduction": "In the era of precision medicine and big data, algorithms are increasingly used to predict health risks and guide care. This exercise [@problem_id:4595755] delves into the critical field of algorithmic fairness, exploring how these tools can inadvertently perpetuate or even amplify existing health disparities. By evaluating a risk model against fairness criteria like Equalized Odds and Predictive Parity, you will confront the complex trade-offs that arise when applying a single algorithm to diverse populations with different underlying base rates of disease.", "problem": "You are evaluating a binary risk stratification algorithm for a chronic disease in two demographic groups, labeled Group A and Group B. For each group, you are given counts from a $2 \\times 2$ contingency table comparing the algorithm’s predicted label $\\hat{Y} \\in \\{0,1\\}$ with the true outcome $Y \\in \\{0,1\\}$. Let $TP$ be the count of true positives, $FP$ the count of false positives, $TN$ the count of true negatives, and $FN$ the count of false negatives.\n\nYour task is to implement a program that, for each test case, computes across the two groups:\n- The positive class prevalence (base rate) $\\pi_g = \\dfrac{TP_g + FN_g}{TP_g + FP_g + TN_g + FN_g}$ for group $g \\in \\{\\text{A},\\text{B}\\}$.\n- The True Positive Rate (TPR) $=\\dfrac{TP_g}{TP_g + FN_g}$, the False Positive Rate (FPR) $=\\dfrac{FP_g}{FP_g + TN_g}$, and the Positive Predictive Value (PPV) $=\\dfrac{TP_g}{TP_g + FP_g}$, computed separately for each group.\n- A boolean indicating whether Equalized Odds (EO) holds between the two groups, defined as both $|\\text{TPR}_{\\text{A}} - \\text{TPR}_{\\text{B}}| \\le \\epsilon$ and $|\\text{FPR}_{\\text{A}} - \\text{FPR}_{\\text{B}}| \\le \\epsilon$, with tolerance $\\epsilon = 10^{-6}$.\n- A boolean indicating whether Predictive Parity (PP) holds between the two groups, defined as $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}| \\le \\epsilon$, with tolerance $\\epsilon = 10^{-6}$.\n\nHandling undefined quantities:\n- If a denominator is zero for a rate in a group, that rate is considered undefined in that group. For the parity checks, treat two undefined rates (one in each group for the same metric) as equal; if one rate is undefined and the other is defined, treat them as not equal. For the summary differences requested below, if both PPVs are undefined, set the absolute PPV difference to $0.0$.\n\nFor each test case, your program must output a list with four entries in the following order:\n- A boolean for EO holding ($\\text{True}$ or $\\text{False}$).\n- A boolean for PP holding ($\\text{True}$ or $\\text{False}$).\n- The absolute difference in base rates $|\\pi_{\\text{A}} - \\pi_{\\text{B}}|$, rounded to six decimal places.\n- The absolute difference in PPVs $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|$, rounded to six decimal places, with the undefined-undefined case set to $0.0$ as specified above.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these per-case lists, enclosed in square brackets, for example: $[[\\text{True},\\text{False},0.125000,0.031250],[\\dots]]$. All fractional results must be expressed as decimals; do not use a percentage sign.\n\nUse the following test suite of four cases. Each case provides the eight nonnegative integers $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}},TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}})$:\n\n- Case 1 (identical error rates but different base rates; expect Equalized Odds to hold and Predictive Parity to fail):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (80, 20, 80, 20)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (40, 30, 120, 10)$\n\n- Case 2 (equal PPVs but different error rates; expect Predictive Parity to hold and Equalized Odds to fail):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (50, 50, 50, 50)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (30, 30, 120, 20)$\n\n- Case 3 (degenerate always-negative classifier in both groups; treat both PPVs as undefined and equal, so both parities hold):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (0, 0, 90, 10)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (0, 0, 80, 20)$\n\n- Case 4 (same base rates and same error rates; expect both parities to hold):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (42, 28, 112, 18)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (21, 14, 56, 9)$\n\nYour program must hard-code the above test cases in this order and the tolerance $\\epsilon = 10^{-6}$. It must output a single line: a list of four sublists (one per case) in the exact order of the cases, where each sublist is $[\\text{EO},\\text{PP},|\\pi_{\\text{A}} - \\pi_{\\text{B}}|,|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|]$ with the two floating-point entries rounded to six decimal places as specified.", "solution": "The problem is well-defined for the provided test cases and is scientifically grounded in the fields of epidemiology and algorithmic fairness. It requires the computation of standard performance and fairness metrics for a binary classifier across two demographic groups.\n\nThe analysis proceeds by first defining the necessary quantities based on the provided counts from a $2 \\times 2$ contingency table for each group $g \\in \\{\\text{A}, \\text{B}\\}$: True Positives ($TP_g$), False Positives ($FP_g$), True Negatives ($TN_g$), and False Negatives ($FN_g$).\n\nThe core quantities to be calculated for each group $g$ are:\n1.  **Total Condition Positive**: The number of individuals who truly have the condition.\n    $$P_g = TP_g + FN_g$$\n2.  **Total Condition Negative**: The number of individuals who truly do not have the condition.\n    $$N'_g = FP_g + TN_g$$\n3.  **Total Predicted Positive**: The number of individuals predicted by the algorithm to have the condition.\n    $$\\hat{P}_g = TP_g + FP_g$$\n4.  **Total Population**: The total number of individuals in the group.\n    $$N_{total, g} = TP_g + FP_g + TN_g + FN_g = P_g + N'_g$$\n\nFrom these, we can define the required rates for each group $g$:\n-   **Positive Class Prevalence (Base Rate)**, $\\pi_g$: The proportion of individuals in the group who have the condition.\n    $$\\pi_g = \\frac{P_g}{N_{total, g}} = \\frac{TP_g + FN_g}{TP_g + FP_g + TN_g + FN_g}$$\n-   **True Positive Rate (TPR)**, also known as sensitivity or recall: The proportion of individuals with the condition who are correctly identified by the algorithm.\n    $$\\text{TPR}_g = \\frac{TP_g}{P_g} = \\frac{TP_g}{TP_g + FN_g}$$\n    This is undefined if the denominator $P_g = 0$.\n-   **False Positive Rate (FPR)**: The proportion of individuals without the condition who are incorrectly identified by the algorithm.\n    $$\\text{FPR}_g = \\frac{FP_g}{N'_g} = \\frac{FP_g}{FP_g + TN_g}$$\n    This is undefined if the denominator $N'_g = 0$.\n-   **Positive Predictive Value (PPV)**, also known as precision: The proportion of individuals predicted to have the condition who truly have it.\n    $$\\text{PPV}_g = \\frac{TP_g}{\\hat{P}_g} = \\frac{TP_g}{TP_g + FP_g}$$\n    This is undefined if the denominator $\\hat{P}_g = 0$.\n\nNext, we evaluate the fairness criteria between Group A and Group B, using a tolerance $\\epsilon = 10^{-6}$.\n\n-   **Equalized Odds (EO)**: This criterion is met if the classifier has equal TPR and FPR across both groups. A special handling for undefined rates is required: two undefined rates (e.g., $\\text{TPR}_\\text{A}$ and $\\text{TPR}_\\text{B}$) are considered equal, while a defined rate and an undefined rate are considered not equal. Formally, EO holds if both of the following conditions are met:\n    1.  The TPRs are considered equal: $|\\text{TPR}_{\\text{A}} - \\text{TPR}_{\\text{B}}| \\le \\epsilon$, or both rates are undefined.\n    2.  The FPRs are considered equal: $|\\text{FPR}_{\\text{A}} - \\text{FPR}_{\\text{B}}| \\le \\epsilon$, or both rates are undefined.\n\n-   **Predictive Parity (PP)**: This criterion is met if the classifier has equal PPV across both groups, subject to the same handling of undefined rates. Formally, PP holds if:\n    1.  The PPVs are considered equal: $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}| \\le \\epsilon$, or both rates are undefined.\n\nFinally, for each test case, we must assemble a list containing four values:\n1.  A boolean value indicating if EO holds ($\\text{True}$ or $\\text{False}$).\n2.  A boolean value indicating if PP holds ($\\text{True}$ or $\\text{False}$).\n3.  The absolute difference in base rates, $|\\pi_{\\text{A}} - \\pi_{\\text{B}}|$.\n4.  The absolute difference in PPVs, $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|$. According to the problem specification, if both $\\text{PPV}_{\\text{A}}$ and $\\text{PPV}_{\\text{B}}$ are undefined, this difference is to be set to $0.0$.\n\nThe overall algorithm is as follows:\n1.  For each test case, retrieve the eight input values: $(TP_{\\text{A}}, FP_{\\text{A}}, TN_{\\text{A}}, FN_{\\text{A}}, TP_{\\text{B}}, FP_{\\text{B}}, TN_{\\text{B}}, FN_{\\text{B}})$.\n2.  Define a procedure that takes $(TP, FP, TN, FN)$ as input and calculates the rates $(\\pi, \\text{TPR}, \\text{FPR}, \\text{PPV})$, returning a special value (e.g., `None` in Python) for any rate where the denominator is zero.\n3.  Apply this procedure to the data for Group A and Group B to obtain their respective rates.\n4.  Implement a comparison function that takes two rates and the tolerance $\\epsilon$ and returns `True` if they are considered equal according to the problem's rules (i.e., absolute difference is within tolerance, or both are undefined).\n5.  Use this comparison function to check for EO (comparing TPRs and FPRs) and PP (comparing PPVs).\n6.  Calculate the absolute difference in base rates, $|\\pi_{\\text{A}} - \\pi_{\\text{B}}|$.\n7.  Calculate the absolute difference in PPVs, $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|$, applying the special rule for the case where both are undefined.\n8.  Format the four resulting values into a list, rounding the floating-point numbers to six decimal places.\n9.  Collect the lists from all test cases into a final list and format it into the required string output.\n\n**Example Walkthrough: Case 1**\n-   Group A: $(TP_{\\text{A}}, FP_{\\text{A}}, TN_{\\text{A}}, FN_{\\text{A}}) = (80, 20, 80, 20)$\n    -   $P_A = 80+20=100$, $N'_A = 20+80=100$, $\\hat{P}_A = 80+20=100$, $N_{total,A} = 200$.\n    -   $\\pi_A = 100/200 = 0.5$, $\\text{TPR}_A = 80/100 = 0.8$, $\\text{FPR}_A = 20/100 = 0.2$, $\\text{PPV}_A = 80/100=0.8$.\n-   Group B: $(TP_{\\text{B}}, FP_{\\text{B}}, TN_{\\text{B}}, FN_{\\text{B}}) = (40, 30, 120, 10)$\n    -   $P_B = 40+10=50$, $N'_B = 30+120=150$, $\\hat{P}_B = 40+30=70$, $N_{total,B} = 200$.\n    -   $\\pi_B = 50/200 = 0.25$, $\\text{TPR}_B = 40/50 = 0.8$, $\\text{FPR}_B = 30/150 = 0.2$, $\\text{PPV}_B = 40/70 \\approx 0.571429$.\n-   **EO Check**: $|\\text{TPR}_A - \\text{TPR}_B| = |0.8 - 0.8| = 0 \\le \\epsilon$. $|\\text{FPR}_A - \\text{FPR}_B| = |0.2 - 0.2| = 0 \\le \\epsilon$. Both pass. EO is $\\text{True}$.\n-   **PP Check**: $|\\text{PPV}_A - \\text{PPV}_B| = |0.8 - 40/70| \\approx 0.228571 > \\epsilon$. Fails. PP is $\\text{False}$.\n-   **Differences**:\n    -   $|\\pi_A - \\pi_B| = |0.5 - 0.25| = 0.25$.\n    -   $|\\text{PPV}_A - \\text{PPV}_B| \\approx 0.228571$.\n-   **Result**: $[\\text{True}, \\text{False}, 0.250000, 0.228571]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted.\n\ndef solve():\n    \"\"\"\n    Computes fairness metrics and differences for a binary classifier\n    across two groups for a given suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (80, 20, 80, 20, 40, 30, 120, 10),\n        # Case 2\n        (50, 50, 50, 50, 30, 30, 120, 20),\n        # Case 3\n        (0, 0, 90, 10, 0, 0, 80, 20),\n        # Case 4\n        (42, 28, 112, 18, 21, 14, 56, 9),\n    ]\n\n    epsilon = 1e-6\n    all_results = []\n\n    def calculate_metrics(tp, fp, tn, fn):\n        \"\"\"\n        Calculates prevalence and performance metrics. Returns None for undefined rates.\n        \"\"\"\n        total_pop = tp + fp + tn + fn\n        \n        # Denominators\n        p_cond = tp + fn      # Condition Positive\n        n_cond = fp + tn      # Condition Negative\n        p_pred = tp + fp      # Predicted Positive\n\n        pi = p_cond / total_pop if total_pop > 0 else 0.0\n        tpr = tp / p_cond if p_cond > 0 else None\n        fpr = fp / n_cond if n_cond > 0 else None\n        ppv = tp / p_pred if p_pred > 0 else None\n\n        return pi, tpr, fpr, ppv\n\n    def are_rates_equal(rate_a, rate_b, tol):\n        \"\"\"\n        Compares two rates according to the problem's rules for undefined values.\n        \"\"\"\n        if rate_a is None and rate_b is None:\n            return True\n        if rate_a is None or rate_b is None:\n            return False\n        return abs(rate_a - rate_b) = tol\n\n    for case in test_cases:\n        tp_a, fp_a, tn_a, fn_a, tp_b, fp_b, tn_b, fn_b = case\n\n        pi_a, tpr_a, fpr_a, ppv_a = calculate_metrics(tp_a, fp_a, tn_a, fn_a)\n        pi_b, tpr_b, fpr_b, ppv_b = calculate_metrics(tp_b, fp_b, tn_b, fn_b)\n\n        # Evaluate Equalized Odds (EO)\n        eo_holds = are_rates_equal(tpr_a, tpr_b, epsilon) and \\\n                   are_rates_equal(fpr_a, fpr_b, epsilon)\n\n        # Evaluate Predictive Parity (PP)\n        pp_holds = are_rates_equal(ppv_a, ppv_b, epsilon)\n\n        # Calculate absolute difference in base rates\n        pi_diff = abs(pi_a - pi_b)\n\n        # Calculate absolute difference in PPVs with special handling\n        if ppv_a is None and ppv_b is None:\n            ppv_diff = 0.0\n        # The case where one is None and the other is not is not in the test data,\n        # so a simple calculation is sufficient for the valid test cases.\n        else:\n            ppv_diff = abs(ppv_a - ppv_b)\n\n        # Append formatted results for the current case\n        all_results.append([eo_holds, pp_holds, pi_diff, ppv_diff])\n\n    # Format the final output string exactly as required, avoiding extra spaces.\n    formatted_results = []\n    for res in all_results:\n        # Format: [bool,bool,float,float] without spaces\n        # Python's str(True) is 'True', which is correct.\n        # f-string formatting handles rounding to 6 decimal places.\n        formatted_str = f\"[{res[0]},{res[1]},{res[2]:.6f},{res[3]:.6f}]\"\n        formatted_results.append(formatted_str)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4595755"}]}