{"hands_on_practices": [{"introduction": "When comparing outcomes like mortality rates across different hospitals, a simple comparison of raw numbers can be deeply misleading. One hospital may appear to have worse outcomes simply because it treats sicker patients. This exercise introduces standardization, a fundamental epidemiological technique used to adjust for these differences in patient case-mix, allowing for a more equitable comparison of performance [@problem_id:4597254]. You will practice deriving and calculating the Standardized Mortality Ratio (SMR) to determine if a hospital's mortality is higher or lower than expected, given its unique patient population.", "problem": "A hospital quality assessment study seeks to compare hospital-level mortality while accounting for differences in patient case-mix. In epidemiology, stratification and standardization are used to control for confounding by variables such as severity. Begin from the fundamental definitions: a stratum-specific mortality rate is defined as $r_{i} = \\frac{d_{i}}{n_{i}}$, where $d_{i}$ is the number of deaths and $n_{i}$ is the number of individuals at risk (for example, admissions) in stratum $i$. In direct standardization, the goal is to estimate a mortality rate that would apply to a common reference population by weighting the hospital’s stratum-specific rates by a fixed set of stratum weights from a standard population. In indirect standardization, the goal is to compare the observed number of deaths in the hospital to the number expected if the hospital had experienced the stratum-specific rates from a standard (reference) population; expected deaths are obtained by applying the standard stratum-specific rates to the hospital’s stratum-specific counts.\n\nConsider Hospital $\\mathcal{H}$ with $3$ severity strata defined by a validated risk score. Over a fixed $30$-day in-hospital follow-up window, the following data are observed:\n\n- Stratum $1$: hospital admissions $n_{1} = 1500$, observed deaths $d_{1} = 12$; standard (reference) mortality rate $r^{\\text{std}}_{1} = 0.010$.\n- Stratum $2$: hospital admissions $n_{2} = 900$, observed deaths $d_{2} = 35$; standard (reference) mortality rate $r^{\\text{std}}_{2} = 0.030$.\n- Stratum $3$: hospital admissions $n_{3} = 150$, observed deaths $d_{3} = 36$; standard (reference) mortality rate $r^{\\text{std}}_{3} = 0.120$.\n\nUsing only the above fundamental definitions, first justify the conceptual distinction between direct and indirect standardization in terms of which rates and which weights are applied. Then, derive from first principles an analytic expression for the standardized mortality ratio (Standardized Mortality Ratio (SMR)) as the ratio of total observed deaths to total expected deaths under indirect standardization. Finally, compute Hospital $\\mathcal{H}$’s SMR using the data provided. Round your answer to four significant figures. Express the final result as a unitless decimal.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It presents a standard scenario in epidemiology and health services research, providing all necessary data and definitions for a complete and unambiguous solution. The data are internally consistent and plausible within the given context. Therefore, the problem is deemed valid.\n\nThe solution proceeds in three parts as requested: first, a conceptual justification of the distinction between direct and indirect standardization; second, a derivation of the standardized mortality ratio (SMR) from first principles; and third, the computation of the SMR for Hospital $\\mathcal{H}$.\n\n### 1. Conceptual Distinction Between Direct and Indirect Standardization\n\nThe core objective of standardization is to enable a fair comparison of rates (e.g., mortality rates) between different populations by adjusting for differences in the distribution of one or more confounding variables, such as age, sex, or, in this case, disease severity. The distinction between direct and indirect standardization lies in the reference point and the components used for adjustment.\n\nLet there be $I$ strata, indexed by $i=1, \\dots, I$. For a study population (e.g., Hospital $\\mathcal{H}$), let $d_{i}$ be the number of events (deaths) and $n_{i}$ be the population size in stratum $i$. The stratum-specific rate is $r_{i} = d_{i} / n_{i}$. For a standard population, let $r^{\\text{std}}_{i}$ be the stratum-specific rate and $N^{\\text{std}}_{i}$ be the population size in stratum $i$.\n\n**Direct Standardization:**\nDirect standardization calculates an overall rate that the study population would have if it had the same population structure as the standard population. It applies the study population's stratum-specific rates ($r_{i}$) to the standard population's stratum-specific weights. The weight for stratum $i$, $w^{\\text{std}}_{i}$, is its proportion in the standard population, $w^{\\text{std}}_{i} = N^{\\text{std}}_{i} / \\sum_{k=1}^{I} N^{\\text{std}}_{k}$. The Directly Standardized Rate (DSR) is a weighted average of the study population's rates:\n$$\n\\text{DSR} = \\sum_{i=1}^{I} w^{\\text{std}}_{i} r_{i}\n$$\nThe DSR is an adjusted rate and has units of rate (e.g., deaths per person-time). It answers the counterfactual question: \"What would the mortality rate of Hospital $\\mathcal{H}$ be if its patient mix were identical to that of the standard population?\" This method requires that the stratum-specific rates ($r_{i}$) in the study population are stable (i.e., not based on very small numbers of deaths or individuals).\n\n**Indirect Standardization:**\nIndirect standardization calculates the ratio of observed events to expected events. It applies the standard population's stratum-specific rates ($r^{\\text{std}}_{i}$) to the study population's stratum-specific sizes ($n_{i}$) to calculate the number of \"expected\" events. The total observed events are $O = \\sum_{i=1}^{I} d_{i}$. The total expected events are $E = \\sum_{i=1}^{I} n_{i} r^{\\text{std}}_{i}$. The resulting measure is the Standardized Mortality Ratio (SMR), a unitless ratio:\n$$\n\\text{SMR} = \\frac{O}{E} = \\frac{\\sum_{i=1}^{I} d_{i}}{\\sum_{i=1}^{I} n_{i} r^{\\text{std}}_{i}}\n$$\nThe SMR answers the counterfactual question: \"How does the number of observed deaths in Hospital $\\mathcal{H}$ compare to the number that would be expected if its patients had experienced the mortality rates of the standard population, given Hospital $\\mathcal{H}$'s actual patient mix?\" An SMR greater than $1$ indicates higher mortality than expected, while an SMR less than $1$ indicates lower mortality than expected. This method is preferred when stratum-specific rates in the study population are unstable or unavailable.\n\n**In summary:** The conceptual distinction is which components are from the study versus the standard population. Direct standardization applies **local rates** to a **standard population structure (weights)**. Indirect standardization applies **standard rates** to a **local population structure (counts)**.\n\n### 2. Derivation of the Standardized Mortality Ratio (SMR)\n\nThe SMR is derived from the principles of indirect standardization. The goal is to compare the observed mortality in a specific population (Hospital $\\mathcal{H}$) to the mortality that would be expected if that population were subject to the mortality rates of a standard reference population.\n\n**Step 1: Define Total Observed Deaths ($O$)**\nThe total number of observed deaths in Hospital $\\mathcal{H}$ is the sum of deaths across all $I$ strata. Let $d_{i}$ be the observed number of deaths in stratum $i$.\n$$\nO = \\sum_{i=1}^{I} d_{i}\n$$\n\n**Step 2: Define Total Expected Deaths ($E$)**\nThe number of expected deaths is a hypothetical quantity. It is the number of deaths that would have occurred in Hospital $\\mathcal{H}$ if its patients in each stratum had died at the same rate as the patients in the corresponding stratum of the standard population.\nLet $n_{i}$ be the number of individuals (admissions) in stratum $i$ in Hospital $\\mathcalH$.\nLet $r^{\\text{std}}_{i}$ be the mortality rate in stratum $i$ of the standard population.\nThe expected number of deaths in stratum $i$, denoted $e_{i}$, is the product of the number of individuals in that stratum and the standard rate for that stratum:\n$$\ne_{i} = n_{i} \\times r^{\\text{std}}_{i}\n$$\nThe total number of expected deaths, $E$, is the sum of the expected deaths across all strata:\n$$\nE = \\sum_{i=1}^{I} e_{i} = \\sum_{i=1}^{I} (n_{i} r^{\\text{std}}_{i})\n$$\n\n**Step 3: Define the Standardized Mortality Ratio (SMR)**\nThe SMR is defined as the ratio of the total number of observed deaths to the total number of expected deaths.\n$$\n\\text{SMR} = \\frac{\\text{Total Observed Deaths}}{\\text{Total Expected Deaths}} = \\frac{O}{E}\n$$\nSubstituting the expressions for $O$ and $E$ gives the final analytic expression for the SMR:\n$$\n\\text{SMR} = \\frac{\\sum_{i=1}^{I} d_{i}}{\\sum_{i=1}^{I} n_{i} r^{\\text{std}}_{i}}\n$$\nThis expression is the required derivation from first principles.\n\n### 3. Computation of SMR for Hospital $\\mathcal{H}$\n\nThe problem provides data for Hospital $\\mathcal{H}$ across $I=3$ severity strata.\n\n**Given Data:**\n- Stratum $1$: $n_{1} = 1500$, $d_{1} = 12$, $r^{\\text{std}}_{1} = 0.010$\n- Stratum $2$: $n_{2} = 900$, $d_{2} = 35$, $r^{\\text{std}}_{2} = 0.030$\n- Stratum $3$: $n_{3} = 150$, $d_{3} = 36$, $r^{\\text{std}}_{3} = 0.120$\n\n**Step A: Calculate Total Observed Deaths ($O$)**\nThe total observed deaths are the sum of deaths in each stratum.\n$$\nO = d_{1} + d_{2} + d_{3} = 12 + 35 + 36 = 83\n$$\n\n**Step B: Calculate Total Expected Deaths ($E$)**\nThe total expected deaths are calculated by applying the standard rates to the hospital's patient counts in each stratum and summing the results.\n- Expected deaths in Stratum $1$: $e_{1} = n_{1} \\times r^{\\text{std}}_{1} = 1500 \\times 0.010 = 15$\n- Expected deaths in Stratum $2$: $e_{2} = n_{2} \\times r^{\\text{std}}_{2} = 900 \\times 0.030 = 27$\n- Expected deaths in Stratum $3$: $e_{3} = n_{3} \\times r^{\\text{std}}_{3} = 150 \\times 0.120 = 18$\n\nThe total expected deaths are:\n$$\nE = e_{1} + e_{2} + e_{3} = 15 + 27 + 18 = 60\n$$\n\n**Step C: Calculate the SMR**\nUsing the derived formula and the calculated values for $O$ and $E$:\n$$\n\\text{SMR} = \\frac{O}{E} = \\frac{83}{60}\n$$\nPerforming the division:\n$$\n\\text{SMR} = 1.383333\\dots\n$$\nThe problem requires rounding the answer to four significant figures. The first four significant figures are $1$, $3$, $8$, and $3$. The fifth digit is $3$, so we do not round up.\n$$\n\\text{SMR} \\approx 1.383\n$$\nThis SMR value indicates that Hospital $\\mathcal{H}$ observed approximately $38.3\\%$ more deaths than would be expected if its patients experienced the mortality rates of the standard reference population, after accounting for its specific case-mix of patient severity.", "answer": "$$\n\\boxed{1.383}\n$$", "id": "4597254"}, {"introduction": "Beyond determining if a health service is effective, decision-makers must also consider if it is efficient and offers good value for money. This practice delves into cost-effectiveness analysis (CEA), the primary tool for comparing both the costs and health benefits of different interventions [@problem_id:4597096]. By calculating and comparing Incremental Cost-Effectiveness Ratios (ICERs), you will learn how to identify strategies that are \"dominated\" or inefficient and make optimal decisions based on a society's willingness-to-pay for health gains.", "problem": "A health system is evaluating three mutually exclusive service delivery strategies for managing a chronic condition. For each strategy, you are given the average total cost per patient in dollars and average effectiveness per patient measured in Quality-Adjusted Life Years (QALY), where Quality-Adjusted Life Year (QALY) is a standard measure of health benefit combining survival and quality of life. Let the strategies be labeled $A$, $B$, and $C$. The cost and effectiveness pairs are: $A$: cost $C_A = 20{,}000$, effect $E_A = 2.0$; $B$: cost $C_B = 34{,}000$, effect $E_B = 2.3$; $C$: cost $C_C = 42{,}000$, effect $E_C = 2.7$. Assume costs are measured in dollars and effects in QALYs for a common time horizon and population, and that uncertainty can be ignored for this exercise.\n\nStarting from fundamental definitions of incremental comparisons in health services and outcomes research (costs and effects for two strategies defined over a common decision context), do the following:\n\n1. Derive the general expression for the incremental cost-effectiveness ratio comparing a more effective strategy to a less effective strategy. Your derivation should begin from the definitions of incremental cost and incremental effect and show why this ratio quantifies the additional cost per unit of additional effectiveness.\n\n2. Order the three strategies by effectiveness and use first principles to assess dominance and extended dominance. Explain the conditions under which a strategy is strictly dominated and under which a strategy is ruled out by extended dominance. Apply these concepts to the given data to identify any strategies that should be excluded from consideration.\n\n3. Define the willingness-to-pay threshold $\\lambda$ (in dollars per QALY) and, starting from the principle that the optimal strategy maximizes expected net value, derive the decision rule for choosing among non-dominated strategies under a given $\\lambda$. Explain how to use this rule incrementally along the cost-effectiveness frontier.\n\n4. After excluding any strategies that are dominated or extendedly dominated, determine the single critical willingness-to-pay threshold value $\\lambda^{*}$ (in dollars per QALY) at which the optimal decision switches between the remaining strategies along the efficient frontier. Report $\\lambda^{*}$ as an exact value; do not round. Express your final answer in dollars per QALY.", "solution": "This is a problem in cost-effectiveness analysis, a core component of health services and outcomes research. The problem is valid as it is scientifically grounded in established health economic principles, is well-posed with sufficient and consistent data, and is expressed objectively.\n\nThe solution proceeds in four parts as requested.\n\n1.  Derivation of the Incremental Cost-Effectiveness Ratio (ICER)\n\nLet there be two mutually exclusive strategies, $S_{1}$ and $S_{2}$, with associated costs $C_{1}$ and $C_{2}$, and health effects $E_{1}$ and $E_{2}$, respectively. Assume, without loss of generality, that strategy $S_{2}$ is more effective than strategy $S_{1}$, i.e., $E_{2} > E_{1}$. For $S_{2}$ to be a considerable alternative to $S_{1}$, it must also be more costly, i.e., $C_{2} > C_{1}$. If it were less costly ($C_{2} < C_{1}$), strategy $S_{1}$ would be strictly dominated and would be discarded.\n\nThe incremental cost of choosing $S_{2}$ over $S_{1}$ is defined as the difference in their costs:\n$$ \\Delta C = C_{2} - C_{1} $$\n\nThe incremental effect of choosing $S_{2}$ over $S_{1}$ is defined as the difference in their effects:\n$$ \\Delta E = E_{2} - E_{1} $$\n\nThe incremental cost-effectiveness ratio (ICER) is defined as the ratio of the incremental cost to the incremental effect:\n$$ ICER_{S_{2} \\text{ vs } S_{1}} = \\frac{\\Delta C}{\\Delta E} = \\frac{C_{2} - C_{1}}{E_{2} - E_{1}} $$\n\nThe units of this ratio are cost per unit of effect (e.g., dollars per QALY). Therefore, the ICER quantifies the additional cost that must be incurred to gain one additional unit of health effectiveness by adopting strategy $S_{2}$ instead of strategy $S_{1}$. It is the marginal cost of producing health with $S_{2}$ relative to $S_{1}$.\n\n2.  Assessment of Dominance and Extended Dominance\n\nThe given strategies are:\n- Strategy A: $C_{A} = 20{,}000$, $E_{A} = 2.0$\n- Strategy B: $C_{B} = 34{,}000$, $E_{B} = 2.3$\n- Strategy C: $C_{C} = 42{,}000$, $E_{C} = 2.7$\n\nFirst, we order the strategies by increasing effectiveness: $A \\rightarrow B \\rightarrow C$. The costs are also increasing along this order: $C_{A} < C_{B} < C_{C}$.\n\nA strategy $S_{i}$ is strictly dominated if there exists another strategy $S_{j}$ that is both more effective ($E_{j} > E_{i}$) and less costly ($C_{j} < C_{i}$). In this dataset, no strategy is more effective and simultaneously less costly than another. Therefore, no strategy is strictly dominated.\n\nA strategy is ruled out by extended dominance if it is part of a sequence of strategies ordered by effectiveness, but its ICER relative to the preceding strategy is greater than the ICER of a subsequent, more effective strategy. This implies that the strategy lies above the cost-effectiveness frontier formed by other strategies. To check this, we calculate the sequential ICERs.\n\nThe ICER for moving from strategy $A$ to strategy $B$ is:\n$$ ICER_{B \\text{ vs } A} = \\frac{C_{B} - C_{A}}{E_{B} - E_{A}} = \\frac{34{,}000 - 20{,}000}{2.3 - 2.0} = \\frac{14{,}000}{0.3} = \\frac{140{,}000}{3} \\approx 46{,}666.67 \\text{ dollars/QALY} $$\n\nThe ICER for moving from strategy $B$ to strategy $C$ is:\n$$ ICER_{C \\text{ vs } B} = \\frac{C_{C} - C_{B}}{E_{C} - E_{B}} = \\frac{42{,}000 - 34{,}000}{2.7 - 2.3} = \\frac{8{,}000}{0.4} = 20{,}000 \\text{ dollars/QALY} $$\n\nWe observe that $ICER_{B \\text{ vs } A} > ICER_{C \\text{ vs } B}$. This condition indicates that strategy $B$ is subject to extended dominance. The cost per additional QALY is higher to switch from $A$ to $B$ than it is to switch from $B$ to $C$. This means that strategy $B$ is inefficient relative to a combination of strategies $A$ and $C$. The cost-effectiveness frontier is the convex hull of the points in the cost-effectiveness plane, and strategy $B$ lies above the line segment connecting strategies $A$ and $C$.\n\nTherefore, strategy $B$ should be excluded from consideration. The efficient set of strategies, which form the cost-effectiveness frontier, consists of only $A$ and $C$.\n\n3.  Derivation of the Decision Rule based on Willingness-to-Pay\n\nThe willingness-to-pay (WTP) threshold, denoted by $\\lambda$, represents the maximum price a decision-maker is willing to pay for an additional unit of health effect (e.g., a QALY).\n\nThe optimal strategy is the one that maximizes the net value. A common metric for this is the Net Monetary Benefit (NMB), which converts health effects into monetary units using the WTP threshold $\\lambda$. For a strategy $S_{i}$ with cost $C_{i}$ and effect $E_{i}$, the NMB is:\n$$ NMB_{i} = (\\lambda \\times E_{i}) - C_{i} $$\nThe term $\\lambda \\times E_{i}$ represents the value of the health outcome in monetary terms.\n\nTo choose between two non-dominated strategies, $S_{i}$ and a more effective and more costly strategy $S_{j}$ ($E_{j} > E_{i}$, $C_{j} > C_{i}$), we should adopt $S_{j}$ if its NMB is greater than the NMB of $S_{i}$:\n$$ NMB_{j} > NMB_{i} $$\n$$ (\\lambda \\times E_{j}) - C_{j} > (\\lambda \\times E_{i}) - C_{i} $$\nRearranging the terms to isolate $\\lambda$:\n$$ \\lambda \\times E_{j} - \\lambda \\times E_{i} > C_{j} - C_{i} $$\n$$ \\lambda (E_{j} - E_{i}) > C_{j} - C_{i} $$\nSince $E_{j} > E_{i}$, the term $(E_{j} - E_{i})$ is positive, so we can divide by it without changing the inequality's direction:\n$$ \\lambda > \\frac{C_{j} - C_{i}}{E_{j} - E_{i}} $$\nThe right-hand side of the inequality is the definition of the ICER of $S_{j}$ versus $S_{i}$. Thus, the decision rule is: Choose the more costly and more effective strategy $S_{j}$ over $S_{i}$ if the willingness-to-pay threshold $\\lambda$ is greater than the incremental cost-effectiveness ratio between them.\n\nThis rule is applied incrementally along the cost-effectiveness frontier. Starting with the least costly option ($A$), one considers moving to the next option on the frontier ($C$). This move is justified if $\\lambda > ICER_{C \\text{ vs } A}$. If $\\lambda < ICER_{C \\text{ vs } A}$, the additional cost is not justified by the willingness-to-pay, and the less costly strategy ($A$) remains the optimal choice.\n\n4.  Calculation of the Critical Willingness-to-Pay Threshold $\\lambda^*$\n\nThe critical willingness-to-pay threshold, $\\lambda^*$, is the specific value of $\\lambda$ at which a decision-maker is indifferent between two adjacent strategies on the efficient frontier. This occurs when their Net Monetary Benefits are equal.\n$$ NMB_{j} = NMB_{i} $$\nBased on the derivation in part $3$, this equality holds when:\n$$ \\lambda = \\frac{C_{j} - C_{i}}{E_{j} - E_{i}} = ICER_{j \\text{ vs } i} $$\n\nAfter eliminating strategy $B$ due to extended dominance, the remaining strategies on the efficient frontier are $A$ and $C$. There is only one incremental step on this frontier: from $A$ to $C$. Therefore, there is a single critical threshold $\\lambda^*$ that determines the switch from preferring $A$ to preferring $C$. This threshold is the ICER of strategy $C$ versus strategy $A$.\n\n$$ \\lambda^* = ICER_{C \\text{ vs } A} = \\frac{C_{C} - C_{A}}{E_{C} - E_{A}} $$\n\nSubstituting the given values:\n$$ \\lambda^* = \\frac{42{,}000 - 20{,}000}{2.7 - 2.0} = \\frac{22{,}000}{0.7} $$\n\nTo express this as an exact value, we write the denominator as a fraction:\n$$ \\lambda^* = \\frac{22{,}000}{\\frac{7}{10}} = 22{,}000 \\times \\frac{10}{7} = \\frac{220{,}000}{7} $$\n\nThis is the exact value of the critical WTP threshold in dollars per QALY. If the societal WTP for a QALY is less than this value, strategy $A$ is optimal. If it is greater than this value, strategy $C$ is optimal.", "answer": "$$\n\\boxed{\\frac{220000}{7}}\n$$", "id": "4597096"}, {"introduction": "The gold standard for determining a treatment's causal effect is a randomized controlled trial, but these are often not feasible or ethical. This exercise introduces the \"target trial\" framework, a powerful conceptual tool for designing an observational study that emulates a hypothetical randomized trial as closely as possible [@problem_id:4597136]. By systematically specifying the key components of this target trial—from eligibility criteria to the analysis plan—you will learn to clearly identify the assumptions needed to draw credible causal conclusions from non-randomized, real-world data.", "problem": "A large hospital system seeks to estimate the causal effect of a new care coordination program initiated shortly after discharge on one-year readmission. You will emulate a hypothetical randomized trial (a “target trial”) using observational data to inform policy. The data include, at discharge, baseline covariates $L$ (age, sex, comorbidity index, prior $1$-year utilization, discharge diagnosis, length of stay, and social risk factors), an indicator $A$ of enrollment in the coordination program within $7$ days after discharge, dates of readmission and death, and indicators of loss to administrative follow-up $C$. Assume no interference across patients and that program availability does not change hospital capacity.\n\nUsing only fundamental definitions from the counterfactual framework in epidemiology—namely, potential outcomes $Y^{a}$ under treatment strategy $a$, the intention-to-treat effect defined as a contrast of $\\mathbb{E}[Y^{a}]$ across strategies, and the core identification conditions of exchangeability, positivity, and consistency—select the option that correctly specifies a target trial to evaluate the effect of the program on one-year readmission risk, and that explains how this framing clarifies the identification assumptions in terms of observables and time ordering.\n\nChoose the single best option.\n\nA. Eligibility: adults discharged alive to the community from an index hospitalization, excluding those discharged to hospice. Time zero is the calendar time of discharge. Treatment strategies: enroll in the program within $7$ days versus not enroll within $7$ days (usual care), both defined at discharge and maintained regardless of subsequent adherence (intention-to-treat strategies). Assignment: hypothetical randomization at discharge to program offer versus usual care. Follow-up: from discharge through $365$ days or end of data, whichever occurs first. Outcome: risk of at least one acute care readmission by $365$ days after discharge, with death treated as a competing event (risk defined via the cumulative incidence of readmission by $365$ days). Causal estimand: the difference in one-year readmission risk under program versus usual care. Analysis plan: emulate randomization by weighting or matching on baseline $L$ measured at discharge (e.g., Inverse Probability of Treatment Weighting), estimate the $365$-day cumulative incidence of readmission under each strategy using methods that account for competing risks (e.g., Aalen–Johansen estimator), and apply Inverse Probability of Censoring Weighting to handle loss to follow-up $C$. Identification assumptions clarified by the target trial framing: exchangeability, formalized as $Y^{a} \\perp A \\mid L$ with $L$ measured at discharge (time zero); positivity, $0 < P(A=a \\mid L) < 1$ for all relevant $L$; consistency, that observed outcomes equal counterfactual outcomes under the received strategy; and independent censoring given $L$.\n\nB. Eligibility: all hospitalized adults at admission. Time zero is hospital admission. Treatment strategies: enroll in the program within $7$ days after discharge versus usual care. Assignment: emulate randomization by adjusting for $L$ measured at discharge and $1$ month after discharge. Follow-up: from admission through $365$ days after discharge. Outcome: Kaplan–Meier estimate of readmission by $365$ days treating death as censoring. Causal estimand: the hazard ratio for readmission by $365$ days. Analysis plan: standard Cox proportional hazards regression adjusting for $L$ measured up to $1$ month after discharge. Identification assumptions: exchangeability given $L$ and non-informative censoring.\n\nC. Eligibility: adults discharged alive from an index hospitalization who have at least $2$ program contacts within $30$ days if enrolled. Time zero is the date of the second contact for the treated and $30$ days after discharge for the untreated. Treatment strategies: per-protocol adherence to at least $2$ contacts within $30$ days versus no contacts. Assignment: emulate randomization by conditioning on both baseline $L$ and post-discharge engagement variables. Follow-up: from defined time zero through $365$ days. Outcome: readmission by $365$ days, ignoring death. Causal estimand: per-protocol risk difference. Analysis plan: regression adjustment.\n\nD. Eligibility: adults discharged alive to the community. Time zero is discharge. Treatment strategies: enroll in the program within $7$ days versus usual care. Assignment: emulate randomization by stratifying on a high-dimensional propensity score including variables measured during follow-up. Follow-up: from discharge through $365$ days. Outcome: risk of readmission by $365$ days, counting deaths as no readmission and not otherwise addressed. Causal estimand: odds ratio of readmission by $365$ days. Analysis plan: logistic regression with the propensity score as a covariate; no special handling for loss to follow-up. Identification assumptions: exchangeability given the propensity score.\n\nWhich option best defines a target trial for the intention-to-treat effect on one-year readmission and correctly links the trial components to the identification assumptions needed for valid estimation from the observational data?", "solution": "The problem asks to select the best specification for a target trial to estimate the intention-to-treat (ITT) causal effect of a post-discharge care coordination program on one-year readmission risk, using observational data. The evaluation must be based on the fundamental principles of the counterfactual framework, including potential outcomes, and the core identification assumptions of exchangeability, positivity, and consistency.\n\nFirst, let us establish the key principles for designing such a target trial and its emulation with observational data.\n\n1.  **Research Question**: The goal is to estimate the intention-to-treat (ITT) effect. In an ITT analysis, we compare the outcomes of groups based on their initial assignment to a treatment strategy, regardless of whether they actually received or adhered to the intervention. This preserves the benefits of randomization (or its emulation) by preventing biases introduced by post-randomization events like non-adherence.\n\n2.  **Eligibility Criteria and Time Zero**: The intervention (enrollment in the program) can only occur after discharge from the hospital. Therefore, the appropriate population for the trial consists of patients who are eligible for the program at the moment it could be initiated. This means eligibility must be determined at the time of discharge. Consequently, the start of follow-up, or **time zero**, must be the date of discharge. All covariates $L$ used to adjust for confounding must be measured at or before this time zero to avoid adjusting for mediators or colliders, which would induce bias.\n\n3.  **Treatment Strategies**: The strategies being compared must be well-defined and assignable at time zero. For an ITT analysis, these are: ($1$) assignment to the care coordination program arm, and ($2$) assignment to the usual care arm. In the observational data, the indicator $A$ (enrollment within $7$ days) is the observed proxy for these assignments.\n\n4.  **Outcome and Competing Risks**: The outcome is one-year readmission risk. A patient can be readmitted, or they can die without being readmitted. Death is a **competing event** for readmission. A patient who dies is no longer at risk for readmission. Treating death as simple censoring (as in a standard Kaplan-Meier or Cox regression analysis) assumes that those who die would have the same future risk of readmission as those who remain alive and under follow-up. This assumption of \"independent censoring\" is violated by death, a competing risk. The correct approach is to estimate the **cumulative incidence function (CIF)** for readmission, which properly accounts for the probability of both readmission and the competing event (death). The one-year readmission risk is the value of the CIF for readmission at $365$ days.\n\n5.  **Causal Estimand and Analysis Plan**: The estimand is a contrast in the potential outcome of one-year readmission risk, e.g., the risk difference $\\mathbb{E}[Y^{a=1}] - \\mathbb{E}[Y^{a=0}]$, where $Y^a$ is an indicator for readmission by one year under strategy $a$. To estimate this from observational data, we must:\n    *   **Emulate Randomization**: Adjust for all baseline confounders $L$ (common causes of treatment $A$ and outcome $Y$). This is done to achieve conditional **exchangeability**, formally stated as $Y^{a} \\perp A \\mid L$. Standard methods include matching, stratification, or weighting based on the propensity score, $P(A=1 \\mid L)$. The **positivity** assumption, $0 < P(A=a \\mid L) < 1$, is required for these methods to be valid.\n    *   **Handle Censoring**: Account for loss to administrative follow-up $C$. This is typically addressed by Inverse Probability of Censoring Weighting (IPCW), assuming censoring is independent of the outcome conditional on measured covariates.\n    *   **Estimate Outcome**: Use a statistical estimator appropriate for competing risks, such as the Aalen-Johansen estimator, to calculate the CIF for readmission in each of the (re-weighted or matched) treatment groups.\n    *   **Consistency**: We must assume that an individual's observed outcome corresponds to their potential outcome under the treatment they were observed to receive ($Y = Y^A$ if $A=a$). This is plausible if the treatment strategies are well-defined.\n\nWith these principles established, we can now evaluate each option.\n\n**Option A Evaluation**\n*   **Eligibility**: \"adults discharged alive to the community... excluding those discharged to hospice.\" This is a correctly specified target population.\n*   **Time zero**: \"calendar time of discharge.\" Correct.\n*   **Treatment strategies**: Correctly defines the ITT strategies (\"enroll... versus not enroll... maintained regardless of subsequent adherence\").\n*   **Outcome**: \"risk of at least one acute care readmission... with death treated as a competing event (risk defined via the cumulative incidence of readmission...).\" This is the methodologically correct way to define the outcome and handle the competing risk of death.\n*   **Causal estimand**: \"difference in one-year readmission risk.\" A valid ITT estimand.\n*   **Analysis plan**: \"weighting or matching on baseline $L$ measured at discharge... estimate the $365$-day cumulative incidence... using methods that account for competing risks (e.g., Aalen–Johansen estimator), and apply Inverse Probability of Censoring Weighting to handle loss to follow-up $C$.\" This plan is comprehensive and methodologically sound, addressing confounding, competing risks, and administrative censoring correctly.\n*   **Identification assumptions**: \"exchangeability, formalized as $Y^{a} \\perp A \\mid L$ with $L$ measured at discharge (time zero)... positivity... consistency... and independent censoring given $L$.\" This correctly formalizes the assumptions and links them to the trial design, especially the temporal ordering of measurements.\n\nThis option aligns perfectly with the established principles for a rigorous target trial emulation.\n**Verdict: Correct**\n\n**Option B Evaluation**\n*   **Eligibility & Time zero**: Setting eligibility and time zero at \"hospital admission\" is incorrect. The intervention occurs post-discharge, so randomization must be emulated at discharge. This design is misaligned with the intervention timing.\n*   **Assignment**: Adjusting for variables measured \"$1$ month after discharge\" is a critical error. These are post-treatment variables, and adjusting for them can induce collider stratification bias. Confounding adjustment must be restricted to pre-treatment (baseline) covariates.\n*   **Outcome**: \"treating death as censoring\" and using a \"Kaplan–Meier estimate\" is incorrect. As established, death is a competing risk, and this approach leads to biased estimates of the readmission risk.\n*   **Causal estimand**: A \"hazard ratio\" is a ratio of instantaneous rates, not a direct measure of the contrast in cumulative risk ($\\mathbb{E}[Y^a]$) over one year. The Cox model also relies on the proportional hazards assumption, which may not be appropriate.\n\nThis option contains multiple severe methodological flaws.\n**Verdict: Incorrect**\n\n**Option C Evaluation**\n*   **Research Question**: This option specifies a \"per-protocol\" analysis by restricting eligibility to those who adhere (\"at least $2$ program contacts within $30$ days\"). This does not answer the question about the **intention-to-treat (ITT)** effect.\n*   **Eligibility**: Defining eligibility based on a post-randomization event (adherence) introduces selection bias.\n*   **Time zero**: Defining different time zeros for the treated (\"date of the second contact\") and untreated (\"$30$ days after discharge\") makes a direct comparison nonsensical due to misaligned follow-up clocks.\n*   **Assignment**: Conditioning on \"post-discharge engagement variables\" is a form of post-treatment adjustment that leads to bias.\n*   **Outcome**: \"ignoring death\" is an invalid approach to handling a competing risk and will produce biased results.\n\nThis option describes a flawed per-protocol analysis, not the requested ITT analysis.\n**Verdict: Incorrect**\n\n**Option D Evaluation**\n*   **Assignment**: \"including variables measured during follow-up\" in the propensity score calculation is a critical error. The propensity score must be based solely on pre-treatment (baseline) covariates. This is another instance of improper adjustment for post-randomization variables.\n*   **Outcome**: \"counting deaths as no readmission\" is an improper way to handle the competing risk. It changes the research question from \"what is the risk of readmission?\" to \"what is the risk of being alive and not readmitted?\". This does not estimate the cumulative incidence of readmission.\n*   **Analysis plan**: Using \"logistic regression\" for time-to-event data with censoring and competing risks is inappropriate. It ignores the timing of events. Furthermore, \"no special handling for loss to follow-up\" will lead to bias if censoring is not completely at random.\n\nThis option employs flawed methods for confounding adjustment and outcome analysis.\n**Verdict: Incorrect**\n\nIn summary, Option A is the only choice that correctly specifies all components of a target trial for the ITT effect, including the proper handling of time zero, baseline confounding, competing risks, and censoring, and accurately links these components to the formal causal identification assumptions.", "answer": "$$\\boxed{A}$$", "id": "4597136"}]}