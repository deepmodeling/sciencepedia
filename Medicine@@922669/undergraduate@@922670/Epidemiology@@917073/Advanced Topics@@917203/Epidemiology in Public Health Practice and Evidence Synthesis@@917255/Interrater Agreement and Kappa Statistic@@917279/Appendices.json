{"hands_on_practices": [{"introduction": "To truly understand the kappa statistic, we must first master its calculation from raw data. This first exercise provides a direct, hands-on opportunity to compute Cohen’s kappa from a standard contingency table [@problem_id:4604233]. By manually calculating the observed agreement ($P_o$), the expected chance agreement ($P_e$), and the final $\\kappa$ value, you will solidify your understanding of the core components that define this powerful measure of interrater reliability.", "problem": "Two independent raters each classify the same set of clinical cases into three mutually exclusive categories: positive finding, indeterminate, and negative finding. The joint counts $n_{ij}$ of ratings by Rater A (rows) versus Rater B (columns) for $N$ cases are given by the $3 \\times 3$ matrix\n$$\n\\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix},\n$$\nwhere the first row and column correspond to the positive category, the second to the indeterminate category, and the third to the negative category.\n\nUsing only foundational definitions from probability and epidemiologic agreement, proceed as follows:\n- Compute the empirical joint proportions $p_{ij}$ from the counts $n_{ij}$ and the total $N$.\n- From these empirical joint proportions, determine the observed agreement proportion $P_{o}$ as the probability that the raters exactly agree on category.\n- Under the chance model that the raters’ assignments are statistically independent while preserving their marginal category distributions, determine the expected agreement proportion $P_{e}$.\n- Define the chance-corrected agreement coefficient (Cohen’s kappa) $\\kappa$ as the proportionate improvement of $P_{o}$ over $P_{e}$, and compute its value for these data.\n\nRound the final value of $\\kappa$ to four significant figures and express it as a decimal. The final answer must be a single real number.", "solution": "The user wants to solve the problem, so my first step is to validate the problem statement to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- A set of clinical cases is classified by two independent raters (Rater A, Rater B).\n- There are three mutually exclusive rating categories: positive, indeterminate, and negative.\n- The joint counts of ratings, $n_{ij}$, are given by the $3 \\times 3$ matrix:\n$$\n\\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix}\n$$\n- The rows correspond to Rater A and the columns correspond to Rater B. The first row/column is 'positive', the second is 'indeterminate', and the third is 'negative'.\n- The task is to compute the empirical joint proportions $p_{ij}$, the observed agreement $P_{o}$, the expected agreement under independence $P_{e}$, and Cohen's kappa coefficient $\\kappa$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem requires the calculation of Cohen's kappa, a standard and widely used statistic for measuring interrater reliability. The concepts of observed proportion of agreement ($P_{o}$) and expected proportion of agreement by chance ($P_{e}$) are foundational in this context. The methodology is firmly established in statistics and epidemiology. The problem is scientifically sound.\n- **Well-Posed**: The problem provides all necessary data (the contingency table of counts) and a clear, sequential set of instructions for the calculation. The definitions provided for $P_{o}$, $P_{e}$, and $\\kappa$ are standard and lead to a unique, stable, and meaningful solution. The problem is well-posed.\n- **Objective**: The problem is stated in precise, quantitative terms. There is no ambiguity, subjectivity, or bias in the description or the requested tasks.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, and objective. I will now proceed with the full solution.\n\nThe solution will follow the steps outlined in the problem statement. First, we denote the given matrix of counts as $N_{ij}$.\n$$\nN_{ij} = \\begin{pmatrix}\nn_{11}  n_{12}  n_{13} \\\\\nn_{21}  n_{22}  n_{23} \\\\\nn_{31}  n_{32}  n_{33}\n\\end{pmatrix} = \\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix}\n$$\nThe total number of cases, $N$, is the sum of all elements in the matrix.\n$$\nN = \\sum_{i=1}^{3} \\sum_{j=1}^{3} n_{ij} = 42 + 6 + 12 + 8 + 15 + 5 + 10 + 7 + 55 = 160\n$$\n\nThe empirical joint proportions, $p_{ij}$, are calculated by dividing each count $n_{ij}$ by the total number of cases $N$.\n$$\np_{ij} = \\frac{n_{ij}}{N}\n$$\n$$\nP_{ij} = \\begin{pmatrix}\np_{11}  p_{12}  p_{13} \\\\\np_{21}  p_{22}  p_{23} \\\\\np_{31}  p_{32}  p_{33}\n\\end{pmatrix} = \\frac{1}{160} \\begin{pmatrix}\n42  6  12 \\\\\n8  15  5 \\\\\n10  7  55\n\\end{pmatrix} = \\begin{pmatrix}\n0.2625  0.0375  0.075 \\\\\n0.05  0.09375  0.03125 \\\\\n0.0625  0.04375  0.34375\n\\end{pmatrix}\n$$\n\nThe observed agreement proportion, $P_{o}$, is the probability that the two raters classify a case into the same category. This corresponds to the sum of the proportions on the main diagonal of the matrix $P_{ij}$.\n$$\nP_{o} = \\sum_{i=1}^{3} p_{ii} = p_{11} + p_{22} + p_{33} = \\frac{n_{11} + n_{22} + n_{33}}{N}\n$$\n$$\nP_{o} = \\frac{42 + 15 + 55}{160} = \\frac{112}{160} = 0.7\n$$\n\nNext, we determine the expected agreement proportion, $P_{e}$, under a model of chance where the raters' judgments are statistically independent. This requires the marginal proportions for each rater. Let $p_{i.}$ be the proportion of cases Rater A assigned to category $i$, and $p_{.j}$ be the proportion of cases Rater B assigned to category $j$.\n\nThe row totals (Rater A's counts) are:\n$n_{1.} = n_{11} + n_{12} + n_{13} = 42 + 6 + 12 = 60$\n$n_{2.} = n_{21} + n_{22} + n_{23} = 8 + 15 + 5 = 28$\n$n_{3.} = n_{31} + n_{32} + n_{33} = 10 + 7 + 55 = 72$\n\nThe column totals (Rater B's counts) are:\n$n_{.1} = n_{11} + n_{21} + n_{31} = 42 + 8 + 10 = 60$\n$n_{.2} = n_{12} + n_{22} + n_{32} = 6 + 15 + 7 = 28$\n$n_{.3} = n_{13} + n_{23} + n_{33} = 12 + 5 + 55 = 72$\n\nThe marginal proportions are:\nFor Rater A: $p_{1.} = \\frac{60}{160}$, $p_{2.} = \\frac{28}{160}$, $p_{3.} = \\frac{72}{160}$\nFor Rater B: $p_{.1} = \\frac{60}{160}$, $p_{.2} = \\frac{28}{160}$, $p_{.3} = \\frac{72}{160}$\n\nUnder the assumption of independence, the expected proportion of agreement for any category $i$ is the product of the marginal proportions for that category, $p_{i.} \\times p_{.i}$. The total expected agreement proportion $P_{e}$ is the sum of these expected proportions over all categories.\n$$\nP_{e} = \\sum_{i=1}^{3} p_{i.} p_{.i} = p_{1.}p_{.1} + p_{2.}p_{.2} + p_{3.}p_{.3}\n$$\n$$\nP_{e} = \\left(\\frac{60}{160}\\right)\\left(\\frac{60}{160}\\right) + \\left(\\frac{28}{160}\\right)\\left(\\frac{28}{160}\\right) + \\left(\\frac{72}{160}\\right)\\left(\\frac{72}{160}\\right)\n$$\n$$\nP_{e} = \\frac{60^2 + 28^2 + 72^2}{160^2} = \\frac{3600 + 784 + 5184}{25600} = \\frac{9568}{25600} = 0.37375\n$$\n\nFinally, we compute Cohen’s kappa coefficient, $\\kappa$. It is defined as the proportionate improvement of observed agreement over chance agreement. It measures the agreement beyond what would be expected by chance. The formula is:\n$$\n\\kappa = \\frac{P_{o} - P_{e}}{1 - P_{e}}\n$$\nSubstituting the calculated values for $P_{o}$ and $P_{e}$:\n$$\n\\kappa = \\frac{0.7 - 0.37375}{1 - 0.37375} = \\frac{0.32625}{0.62625}\n$$\nPerforming the division:\n$$\n\\kappa \\approx 0.52095808...\n$$\nThe problem requires rounding the final value to four significant figures. The fifth significant figure is $5$, so we round up the fourth digit.\n$$\n\\kappa \\approx 0.5210\n$$\nThe trailing zero is significant and must be included.", "answer": "$$\n\\boxed{0.5210}\n$$", "id": "4604233"}, {"introduction": "Is a high percentage of agreement always a sign of strong reliability? This practice problem explores a famous nuance of the kappa statistic, often called the 'prevalence paradox' [@problem_id:4604226]. By analyzing two hypothetical scenarios with identical observed agreement but different underlying rating patterns, you will discover why $\\kappa$ can produce different results and learn to appreciate how it intelligently accounts for the probability of chance agreement.", "problem": "A hospital quality study evaluates agreement between two independent clinical raters who classify a binary finding (present versus absent) for a cohort of $N=200$ patients. Consider two alternative $2 \\times 2$ contingency tables of patient-level ratings that produce the same observed proportion of agreement but different marginal distributions.\n\nTable A (balanced margins): $a=70$, $b=30$, $c=30$, $d=70$, with $N=a+b+c+d=200$.\n\nTable B (mismatched margins): $a=80$, $b=60$, $c=0$, $d=60$, with $N=a+b+c+d=200$.\n\nStarting from the fundamental definitions of the observed agreement proportion and the expected agreement by chance derived from the marginal probabilities, construct the chance-corrected agreement measure commonly used in epidemiology, Cohen’s kappa (the kappa statistic), and compute it for Table A and Table B. Then, using first principles, explain why the identical observed agreement proportion still yields different kappa values across the two tables.\n\nReport, as your final answer, the numerical difference between the two kappa values, computed as the kappa for Table B minus the kappa for Table A. Round your answer to four significant figures. Express the final answer as a decimal (do not use a percentage sign).", "solution": "The problem requires the calculation and comparison of Cohen's kappa statistic for two different $2 \\times 2$ contingency tables, Table A and Table B, which share the same observed proportion of agreement but differ in their marginal distributions.\n\nFirst, we establish the general framework for a $2 \\times 2$ table representing the ratings of two independent raters on a binary outcome (e.g., Present/Absent). Let the cells of the table be denoted as follows:\n- $a$: Number of cases where both Rater 1 and Rater 2 classify the finding as 'Present'.\n- $b$: Number of cases where Rater 1 classifies as 'Present' and Rater 2 as 'Absent'.\n- $c$: Number of cases where Rater 1 classifies as 'Absent' and Rater 2 as 'Present'.\n- $d$: Number of cases where both Rater 1 and Rater 2 classify the finding as 'Absent'.\n\nThe total number of patients is $N = a+b+c+d$.\n\nThe Cohen's kappa statistic, $\\kappa$, is defined as the chance-corrected measure of agreement:\n$$ \\kappa = \\frac{P_o - P_e}{1 - P_e} $$\nwhere:\n- $P_o$ is the observed proportion of agreement. This is the sum of cases where both raters agree, divided by the total number of cases.\n$$ P_o = \\frac{a+d}{N} $$\n- $P_e$ is the expected proportion of agreement that would occur by chance. This is calculated based on the marginal probabilities of each rater's classifications, under the assumption of independence between the raters.\n\nThe marginal totals are:\n- Rater 1 classifies 'Present': $R_1_P = a+b$\n- Rater 1 classifies 'Absent': $R_1_A = c+d$\n- Rater 2 classifies 'Present': $C_2_P = a+c$\n- Rater 2 classifies 'Absent': $C_2_A = b+d$\n\nThe corresponding marginal probabilities are:\n- $P(R_1=\\text{Present}) = \\frac{a+b}{N}$\n- $P(R_1=\\text{Absent}) = \\frac{c+d}{N}$\n- $P(R_2=\\text{Present}) = \\frac{a+c}{N}$\n- $P(R_2=\\text{Absent}) = \\frac{b+d}{N}$\n\nThe expected probability of agreement by chance, $P_e$, is the sum of the probabilities of agreeing on 'Present' and agreeing on 'Absent':\n$$ P_e = P(R_1=\\text{Present}) \\times P(R_2=\\text{Present}) + P(R_1=\\text{Absent}) \\times P(R_2=\\text{Absent}) $$\n$$ P_e = \\left(\\frac{a+b}{N}\\right) \\left(\\frac{a+c}{N}\\right) + \\left(\\frac{c+d}{N}\\right) \\left(\\frac{b+d}{N}\\right) $$\n\nNow, we apply these formulas to the two tables provided.\n\n**Analysis of Table A (balanced margins):**\nThe given values are $a=70$, $b=30$, $c=30$, $d=70$, with $N=200$.\n\n1.  **Observed Agreement ($P_{o,A}$):**\n    $$ P_{o,A} = \\frac{a+d}{N} = \\frac{70+70}{200} = \\frac{140}{200} = 0.7 $$\n\n2.  **Expected Agreement ($P_{e,A}$):**\n    First, we calculate the marginal totals for Table A:\n    - Rater 1 'Present': $a+b = 70+30 = 100$\n    - Rater 1 'Absent': $c+d = 30+70 = 100$\n    - Rater 2 'Present': $a+c = 70+30 = 100$\n    - Rater 2 'Absent': $b+d = 30+70 = 100$\n    The marginal probabilities are:\n    - $P(R_1=\\text{Present}) = \\frac{100}{200} = 0.5$\n    - $P(R_1=\\text{Absent}) = \\frac{100}{200} = 0.5$\n    - $P(R_2=\\text{Present}) = \\frac{100}{200} = 0.5$\n    - $P(R_2=\\text{Absent}) = \\frac{100}{200} = 0.5$\n    Now, we compute $P_{e,A}$:\n    $$ P_{e,A} = (0.5 \\times 0.5) + (0.5 \\times 0.5) = 0.25 + 0.25 = 0.5 $$\n\n3.  **Cohen's Kappa ($\\kappa_A$):**\n    $$ \\kappa_A = \\frac{P_{o,A} - P_{e,A}}{1 - P_{e,A}} = \\frac{0.7 - 0.5}{1 - 0.5} = \\frac{0.2}{0.5} = 0.4 $$\n\n**Analysis of Table B (mismatched margins):**\nThe given values are $a=80$, $b=60$, $c=0$, $d=60$, with $N=200$.\n\n1.  **Observed Agreement ($P_{o,B}$):**\n    $$ P_{o,B} = \\frac{a+d}{N} = \\frac{80+60}{200} = \\frac{140}{200} = 0.7 $$\n    As specified in the problem, $P_{o,B} = P_{o,A}$.\n\n2.  **Expected Agreement ($P_{e,B}$):**\n    First, we calculate the marginal totals for Table B:\n    - Rater 1 'Present': $a+b = 80+60 = 140$\n    - Rater 1 'Absent': $c+d = 0+60 = 60$\n    - Rater 2 'Present': $a+c = 80+0 = 80$\n    - Rater 2 'Absent': $b+d = 60+60 = 120$\n    The marginal probabilities are:\n    - $P(R_1=\\text{Present}) = \\frac{140}{200} = 0.7$\n    - $P(R_1=\\text{Absent}) = \\frac{60}{200} = 0.3$\n    - $P(R_2=\\text{Present}) = \\frac{80}{200} = 0.4$\n    - $P(R_2=\\text{Absent}) = \\frac{120}{200} = 0.6$\n    Now, we compute $P_{e,B}$:\n    $$ P_{e,B} = \\left(\\frac{140}{200}\\right)\\left(\\frac{80}{200}\\right) + \\left(\\frac{60}{200}\\right)\\left(\\frac{120}{200}\\right) = (0.7 \\times 0.4) + (0.3 \\times 0.6) = 0.28 + 0.18 = 0.46 $$\n\n3.  **Cohen's Kappa ($\\kappa_B$):**\n    $$ \\kappa_B = \\frac{P_{o,B} - P_{e,B}}{1 - P_{e,B}} = \\frac{0.7 - 0.46}{1 - 0.46} = \\frac{0.24}{0.54} = \\frac{24}{54} = \\frac{4}{9} $$\n    As a decimal, $\\kappa_B \\approx 0.4444...$\n\n**Explanation of the Difference in Kappa Values:**\nThe kappa statistic $\\kappa = (P_o - P_e) / (1 - P_e)$ is a function of both the observed agreement $P_o$ and the chance-expected agreement $P_e$. In this problem, $P_o$ is held constant at $0.7$ for both tables. Therefore, the difference in the kappa values, $\\kappa_A$ and $\\kappa_B$, arises solely from the difference in their respective expected agreement proportions, $P_{e,A}$ and $P_{e,B}$.\n\nThe expected agreement, $P_e$, is calculated from the products of the marginal probabilities. These marginal probabilities reflect the base rates or prevalence with which each rater uses the categories 'Present' and 'Absent'.\n- For Table A, the marginal distributions are balanced: both raters classify $50\\%$ of cases as 'Present' and $50\\%$ as 'Absent'. This leads to an expected agreement $P_{e,A} = 0.5$.\n- For Table B, the marginal distributions are mismatched: Rater 1 classifies $70\\%$ as 'Present', while Rater 2 classifies only $40\\%$ as 'Present'. This imbalance results in a lower expected agreement, $P_{e,B} = 0.46$.\n\nThe core principle is that for a fixed sum of marginal probabilities for a given category across raters, the expected agreement is maximized when the marginals are equal (balanced) and minimized as they become more dissimilar. Here, $P_{e,A} > P_{e,B}$.\n\nBecause $\\kappa$ is a monotonically decreasing function of $P_e$ for a fixed $P_o$ (where $P_o  1$), a lower value of $P_e$ yields a higher value of $\\kappa$. Intuitively, when chance agreement is less likely (i.e., $P_e$ is small due to mismatched marginals), the observed agreement $P_o$ is considered more substantial, leading to a higher chance-corrected agreement $\\kappa$. Conversely, when chance agreement is more likely ($P_e$ is large due to balanced marginals), a larger portion of the observed agreement is discounted as being due to chance, resulting in a lower $\\kappa$. This illustrates the well-known property of the kappa statistic being sensitive to the prevalence of the rated categories as reflected in the marginal totals, often referred to as the \"prevalence paradox\".\n\n**Final Calculation:**\nThe problem asks for the numerical difference $\\kappa_B - \\kappa_A$.\n$$ \\kappa_B - \\kappa_A = \\frac{4}{9} - 0.4 = \\frac{4}{9} - \\frac{4}{10} = \\frac{4}{9} - \\frac{2}{5} $$\n$$ \\kappa_B - \\kappa_A = \\frac{4 \\times 5 - 2 \\times 9}{9 \\times 5} = \\frac{20 - 18}{45} = \\frac{2}{45} $$\nConverting this fraction to a decimal gives:\n$$ \\frac{2}{45} \\approx 0.044444... $$\nRounding to four significant figures, we get $0.04444$.", "answer": "$$\n\\boxed{0.04444}\n$$", "id": "4604226"}, {"introduction": "In scientific practice, a single point estimate is rarely enough; we must also communicate the uncertainty surrounding our measurement. This advanced exercise guides you in moving beyond a simple $\\kappa$ value to constructing a 95% confidence interval for it [@problem_id:4604180]. This practice will introduce you to the large-sample variance of $\\hat{\\kappa}$ and provide a crucial tool for interpreting and reporting your findings with appropriate statistical rigor.", "problem": "Two independent clinicians classify each of $n=400$ skin lesion images into one of three nominal categories: benign ($\\mathrm{B}$), atypical ($\\mathrm{A}$), or malignant ($\\mathrm{M}$). Their paired ratings yield the following $3\\times 3$ table of counts, with rows corresponding to Rater 1 and columns to Rater 2:\n\n$$\n\\begin{array}{c|ccc}\n  \\mathrm{B}  \\mathrm{A}  \\mathrm{M} \\\\\n\\hline\n\\mathrm{B}  160  20  10 \\\\\n\\mathrm{A}  25  70  15 \\\\\n\\mathrm{M}  5  18  77 \\\\\n\\end{array}\n$$\n\nWorking from first principles, derive the large-sample Wald-type $0.95$ confidence interval for Cohen’s kappa statistic $\\kappa$ using a plug-in variance estimator. Assume a multinomial model for the $3\\times 3$ table and treat all unknown probabilities with their empirical estimates. Clearly state the sequence of definitions and steps you use to obtain the estimator of $\\operatorname{Var}(\\hat{\\kappa})$ from the asymptotic behavior of multinomial proportions, and then compute the confidence interval endpoints. Round your final confidence interval endpoints to four significant figures. Finally, based on the derivation and the structure of the data, discuss the conditions under which the normal approximation underlying the Wald interval is reliable for $\\kappa$ in this setting.", "solution": "The problem is valid. It presents a standard biostatistical task of calculating a confidence interval for Cohen's kappa statistic, providing all necessary data and a clear objective. The underlying model and requested methodology are well-established in the scientific literature.\n\n### 1. Definition and Estimation of Cohen's Kappa\n\nLet the three categories be indexed by $i, j \\in \\{1, 2, 3\\}$. The observed data are counts $n_{ij}$ in a $3 \\times 3$ contingency table, where $i$ is the row index (Rater 1) and $j$ is the column index (Rater 2). The total number of subjects is $n = \\sum_{i,j} n_{ij}$. The sample proportion for cell $(i, j)$ is $\\hat{p}_{ij} = n_{ij}/n$. These proportions are the empirical estimates of the true cell probabilities $p_{ij}$ from an underlying multinomial distribution on the $9$ cells of the table.\n\nThe data provided are:\n$n=400$ and the counts matrix is\n$$\n\\begin{pmatrix} n_{11}  n_{12}  n_{13} \\\\ n_{21}  n_{22}  n_{23} \\\\ n_{31}  n_{32}  n_{33} \\end{pmatrix} = \\begin{pmatrix} 160  20  10 \\\\ 25  70  15 \\\\ 5  18  77 \\end{pmatrix}\n$$\nFrom this, we derive the matrix of sample proportions $\\hat{p}_{ij}$:\n$$\n\\begin{pmatrix} \\hat{p}_{11}  \\hat{p}_{12}  \\hat{p}_{13} \\\\ \\hat{p}_{21}  \\hat{p}_{22}  \\hat{p}_{23} \\\\ \\hat{p}_{31}  \\hat{p}_{32}  \\hat{p}_{33} \\end{pmatrix} = \\begin{pmatrix} 160/400  20/400  10/400 \\\\ 25/400  70/400  15/400 \\\\ 5/400  18/400  77/400 \\end{pmatrix} = \\begin{pmatrix} 0.400  0.050  0.025 \\\\ 0.0625  0.175  0.0375 \\\\ 0.0125  0.045  0.1925 \\end{pmatrix}\n$$\nCohen's kappa statistic, $\\kappa$, is defined in terms of the observed probability of agreement, $p_o$, and the expected probability of agreement by chance, $p_e$:\n$$\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n$$\nThe observed agreement $p_o$ is the sum of probabilities on the main diagonal. Its empirical estimate $\\hat{p}_o$ is:\n$$\n\\hat{p}_o = \\sum_{i=1}^3 \\hat{p}_{ii} = \\hat{p}_{11} + \\hat{p}_{22} + \\hat{p}_{33} = 0.400 + 0.175 + 0.1925 = 0.7675\n$$\nThe chance-expected agreement $p_e$ is the agreement that would be expected if the raters' ratings were independent. It is the sum of the products of the marginal probabilities. The marginal probability for Rater 1 choosing category $i$ is $p_{i.} = \\sum_j p_{ij}$, and for Rater 2 choosing category $j$ is $p_{.j} = \\sum_i p_{ij}$. Their estimates are:\n$$\n\\hat{p}_{1.} = 0.400 + 0.050 + 0.025 = 0.475 \\\\\n\\hat{p}_{2.} = 0.0625 + 0.175 + 0.0375 = 0.275 \\\\\n\\hat{p}_{3.} = 0.0125 + 0.045 + 0.1925 = 0.250 \\\\\n$$\n$$\n\\hat{p}_{.1} = 0.400 + 0.0625 + 0.0125 = 0.475 \\\\\n\\hat{p}_{.2} = 0.050 + 0.175 + 0.045 = 0.270 \\\\\n\\hat{p}_{.3} = 0.025 + 0.0375 + 0.1925 = 0.255\n$$\nThe estimate of the chance-expected agreement, $\\hat{p}_e$, is:\n$$\n\\hat{p}_e = \\sum_{i=1}^3 \\hat{p}_{i.} \\hat{p}_{.i} = (0.475)(0.475) + (0.275)(0.270) + (0.250)(0.255) \\\\\n\\hat{p}_e = 0.225625 + 0.07425 + 0.06375 = 0.363625\n$$\nFinally, the point estimate of kappa, $\\hat{\\kappa}$, is:\n$$\n\\hat{\\kappa} = \\frac{\\hat{p}_o - \\hat{p}_e}{1 - \\hat{p}_e} = \\frac{0.7675 - 0.363625}{1 - 0.363625} = \\frac{0.403875}{0.636375} \\approx 0.634661\n$$\n\n### 2. Derivation and Calculation of the Asymptotic Variance of $\\hat{\\kappa}$\n\nTo construct a Wald-type confidence interval, we need the standard error of $\\hat{\\kappa}$, which is the square root of its estimated asymptotic variance, $\\widehat{\\operatorname{Var}}(\\hat{\\kappa})$. The derivation of this variance from first principles relies on the multivariate delta method.\n\nThe statistic $\\hat{\\kappa}$ is a function of the vector of cell proportions $\\hat{\\mathbf{p}} = (\\hat{p}_{11}, ..., \\hat{p}_{33})^T$. For a large sample size $n$, the vector $\\sqrt{n}(\\hat{\\mathbf{p}} - \\mathbf{p})$ is approximately normally distributed with mean $\\mathbf{0}$ and a covariance matrix $\\Sigma$ determined by the multinomial probabilities. The delta method states that $\\sqrt{n}(\\hat{\\kappa} - \\kappa)$ is also asymptotically normal with mean $0$ and variance given by $(\\nabla g)^T \\Sigma (\\nabla g)$, where $g$ is the function defining $\\kappa$ in terms of the $p_{ij}$ and $\\nabla g$ is its gradient.\n\nThe full algebraic expansion of this expression is highly complex. The canonical result, as derived by authors such as Bishop, Fienberg, and Holland (1975), gives the following formula for the large-sample variance of $\\hat{\\kappa}$:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\kappa}) = \\frac{1}{n} \\left\\{ \\frac{A}{ (1-\\hat p_e)^2 } + \\frac{B}{(1-\\hat p_e)^3} + \\frac{C}{(1-\\hat p_e)^4} \\right\\}\n$$\nwhere the terms $A$, $B$, and $C$ are defined as:\n$$\nA = \\hat{p}_o(1-\\hat{p}_o) \\\\\nB = 2(1-\\hat{p}_o) \\left( 2\\hat{p}_o\\hat{p}_e - \\sum_{i=1}^3 \\hat{p}_{ii}(\\hat{p}_{i.}+\\hat{p}_{.i}) \\right) \\\\\nC = (1-\\hat{p}_o)^2 \\left( \\sum_{i=1}^3\\sum_{j=1}^3 \\hat{p}_{ij}(\\hat{p}_{i.}+\\hat{p}_{.j})^2 - 4\\hat{p}_e^2 \\right)\n$$\nWe compute the necessary components:\n$$\n\\hat{p}_o = 0.7675 \\implies 1-\\hat{p}_o = 0.2325 \\\\\n\\hat{p}_e = 0.363625 \\implies 1-\\hat{p}_e = 0.636375\n$$\nNext, we calculate the complex sums:\n$$\n\\sum_{i=1}^3 \\hat{p}_{ii}(\\hat{p}_{i.}+\\hat{p}_{.i}) = 0.400(0.475+0.475) + 0.175(0.275+0.270) + 0.1925(0.250+0.255) \\\\\n= 0.400(0.950) + 0.175(0.545) + 0.1925(0.505) \\\\\n= 0.38 + 0.095375 + 0.0972125 = 0.5725875\n$$\n$$\n\\sum_{i,j} \\hat{p}_{ij}(\\hat{p}_{i.}+\\hat{p}_{.j})^2 = 0.400(0.95)^2 + 0.050(0.745)^2 + 0.025(0.730)^2 + 0.0625(0.750)^2 + 0.175(0.545)^2 + 0.0375(0.530)^2 + 0.0125(0.725)^2 + 0.045(0.520)^2 + 0.1925(0.505)^2 \\\\\n= 0.361 + 0.02775125 + 0.0133225 + 0.03515625 + 0.051979375 + 0.01053375 + 0.0065703125 + 0.012168 + 0.0490923125 = 0.56757375\n$$\nNow we compute $A$, $B$, and $C$:\n$$\nA = 0.7675(0.2325) = 0.17844375\n$$\n$$\nB = 2(0.2325) \\left( 2(0.7675)(0.363625) - 0.5725875 \\right) \\\\\n= 0.465 (0.55831625 - 0.5725875) = 0.465(-0.01427125) = -0.00663564375\n$$\n$$\nC = (0.2325)^2 \\left( 0.56757375 - 4(0.363625)^2 \\right) \\\\\n= 0.05405625 (0.56757375 - 4(0.1322230)) = 0.05405625(0.56757375 - 0.528892) \\\\\n= 0.05405625(0.03868175) = 0.0020914106\n$$\nNow we assemble the variance estimator:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\kappa}) = \\frac{1}{400} \\left\\{ \\frac{0.17844375}{(0.636375)^2} + \\frac{-0.00663564}{(0.636375)^3} + \\frac{0.00209141}{(0.636375)^4} \\right\\} \\\\\n= \\frac{1}{400} \\left\\{ \\frac{0.17844375}{0.404973} + \\frac{-0.00663564}{0.257745} + \\frac{0.00209141}{0.164016} \\right\\} \\\\\n= \\frac{1}{400} \\{ 0.440632 - 0.025745 + 0.012751 \\} = \\frac{1}{400} (0.427638) = 0.0010691\n$$\nThe standard error of $\\hat{\\kappa}$ is:\n$$\n\\widehat{SE}(\\hat{\\kappa}) = \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\kappa})} = \\sqrt{0.0010691} \\approx 0.032697\n$$\n\n### 3. Construction of the Confidence Interval\n\nThe large-sample $0.95$ Wald-type confidence interval for $\\kappa$ is given by:\n$$\n\\hat{\\kappa} \\pm z_{0.025} \\cdot \\widehat{SE}(\\hat{\\kappa})\n$$\nUsing the standard normal quantile $z_{0.025} \\approx 1.95996$, rounded to $1.96$:\n$$\n0.634661 \\pm 1.96 \\times 0.032697 \\\\\n0.634661 \\pm 0.064086\n$$\nThe lower and upper bounds are:\n$$\n\\text{Lower Bound} = 0.634661 - 0.064086 = 0.570575 \\\\\n\\text{Upper Bound} = 0.634661 + 0.064086 = 0.698747\n$$\nRounding these endpoints to four significant figures, we get $0.5706$ and $0.6987$.\n\n### 4. Conditions for Reliability of the Normal Approximation\n\nThe validity of the Wald confidence interval rests on the assumption that the sampling distribution of $\\hat{\\kappa}$ is well approximated by a normal distribution. The reliability of this approximation in this context depends on several factors:\n1.  **Sample Size ($n$)**: The normal approximation is based on large-sample asymptotic theory. With $n=400$, the sample size is substantial, which strongly supports the use of this approximation.\n2.  **Sparsity of Data**: The approximation performs better when the cell counts in the contingency table are not too small. In this table, the smallest cell count is $n_{31}=5$. While some guidelines recommend minimum expected counts, all observed counts being $\\ge 5$ (or at least not having many near-zero counts) in a large table like this is generally sufficient to avoid issues of sparseness that would destabilize the variance estimate.\n3.  **Marginal Distributions**: Severe imbalance in the marginal distributions (i.e., some categories are very rare) can degrade the quality of the normal approximation. Here, the marginal proportions for both raters are reasonably balanced: $\\hat{p}_{i.} = (0.475, 0.275, 0.250)$ and $\\hat{p}_{.j} = (0.475, 0.270, 0.255)$. No category is excessively rare, which is a favorable condition.\n4.  **Value of $\\kappa$**: The sampling distribution of $\\hat{\\kappa}$ can be skewed, particularly when the true value of $\\kappa$ is near its boundaries (e.g., close to $1$). Our estimate $\\hat{\\kappa} \\approx 0.63$ is far from the extremes of the possible range of $\\kappa$ (which is roughly $[-1, 1]$), suggesting that the skewness of its sampling distribution is not likely to be a major issue.\n\nIn summary, the combination of a large sample size, non-sparse data, balanced marginals, and a kappa value not close to its boundaries indicates that the normal approximation underlying the Wald interval is likely to be reliable in this setting. The interval should have coverage probability close to the nominal $0.95$ level.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5706  0.6987 \\end{pmatrix}}\n$$", "id": "4604180"}]}