{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of Generalized Linear Models, we will start with the simplest possible case: a model with no predictors. This exercise [@problem_id:4595227] demonstrates how a fundamental epidemiological quantity, the prevalence of an infection, is represented within the GLM framework. By deriving the estimator from first principles, you will see how the concept of log-odds arises naturally from applying a logit link function to a simple proportion.", "problem": "A cross-sectional study recruits $n=100$ individuals from a community and records a binary infection outcome for each person, where $Y_{i}=1$ indicates an infected individual and $Y_{i}=0$ otherwise. The outcomes are assumed independent and follow a Bernoulli distribution with common probability of infection $p$. Investigators fit an intercept-only generalized linear model (GLM) using the Bernoulli distribution and the logit link, so that the linear predictor is $\\eta=\\beta_{0}$ and the mean is $p=\\Pr(Y_{i}=1)$ with $g(p)=\\ln\\!\\big(p/(1-p)\\big)$. The data show $y=30$ infections out of $n=100$.\n\nStarting from the independence assumption and the Bernoulli likelihood, derive the maximum likelihood estimator for $p$, and then use the link function to obtain the maximum likelihood estimator $\\hat{\\beta}_{0}$. Provide an epidemiological interpretation of $\\hat{\\beta}_{0}$ in terms of log-odds. Give your final answer for $\\hat{\\beta}_{0}$ as an exact analytic expression using natural logarithms. Do not round.", "solution": "The problem is well-posed and requires the derivation of the maximum likelihood estimate for an intercept-only logistic regression model. This is a fundamental exercise in understanding GLMs.\n\nThe model assumes $n$ independent and identically distributed Bernoulli trials, $Y_i \\sim \\text{Bernoulli}(p)$ for $i=1, \\dots, n$. The likelihood function for the data $\\mathbf{y} = (y_1, \\dots, y_n)$ is:\n$$ L(p; \\mathbf{y}) = \\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i} = p^{\\sum y_i} (1-p)^{n-\\sum y_i} $$\nLet $y = \\sum_{i=1}^n y_i$ be the total number of successes (infections). In this case, $y=30$ and $n=100$. The likelihood becomes:\n$$ L(p) = p^{30} (1-p)^{70} $$\nTo find the maximum likelihood estimator (MLE) for $p$, we maximize the log-likelihood function, $\\ell(p)$:\n$$ \\ell(p) = \\ln(L(p)) = 30\\ln(p) + 70\\ln(1-p) $$\nWe take the derivative with respect to $p$ and set it to zero:\n$$ \\frac{d\\ell}{dp} = \\frac{30}{p} - \\frac{70}{1-p} = 0 $$\nSolving for $p$:\n$$ \\frac{30}{p} = \\frac{70}{1-p} \\implies 30(1-p) = 70p \\implies 30 - 30p = 70p \\implies 30 = 100p $$\nThe MLE for the probability of infection $p$ is therefore:\n$$ \\hat{p} = \\frac{30}{100} = 0.3 $$\nThis is simply the sample proportion of infected individuals.\n\nThe problem specifies an intercept-only GLM with a logit link function:\n$$ g(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 $$\nBy the invariance property of maximum likelihood estimators, the MLE for $\\beta_0$ is the logit function applied to the MLE for $p$:\n$$ \\hat{\\beta}_0 = g(\\hat{p}) = \\ln\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) $$\nSubstituting our value for $\\hat{p}$:\n$$ \\hat{\\beta}_0 = \\ln\\left(\\frac{0.3}{1-0.3}\\right) = \\ln\\left(\\frac{0.3}{0.7}\\right) = \\ln\\left(\\frac{3}{7}\\right) $$\n**Epidemiological Interpretation:** The parameter $\\beta_0$ in a logistic model represents the log-odds of the outcome when all predictors are zero. In this intercept-only model, $\\hat{\\beta}_0 = \\ln(3/7)$ is the estimated log-odds of infection for an individual in this community. The odds of infection are estimated to be $3/7$, meaning that for every 3 infected individuals, we expect to find 7 uninfected individuals. The value $\\ln(3/7)$ is the natural logarithm of these odds.", "answer": "$$\n\\boxed{\\ln\\left(\\frac{3}{7}\\right)}\n$$", "id": "4595227"}, {"introduction": "Building on the intercept-only model, we now introduce an exposure variable to understand its effect on an outcome. This practice [@problem_id:4595204] tackles a critical and often-confused topic in epidemiology: the distinction between the odds ratio and the risk ratio. By calculating both measures from a single logistic regression coefficient, you will gain a concrete understanding of why these values differ and why the odds ratio is only an approximation of the risk ratio.", "problem": "A cohort study investigates the association between a binary exposure $X \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$. The outcome is modeled with a generalized linear model (GLM) with a logit link, so that for each $x \\in \\{0,1\\}$ the risk satisfies $\\operatorname{logit}(p(x))=\\beta_{0}+\\beta_{1} x$, where $p(x)=\\Pr(Y=1 \\mid X=x)$ and $\\operatorname{logit}(p)=\\ln\\!\\big(p/(1-p)\\big)$. Suppose that when unexposed the baseline risk is $p(0)=p_{0}=0.2$, and that the fitted logit coefficient for the exposure is $\\beta_{1}=0.8$. Using only the definitions of odds, odds ratio, and the inverse-logit transformation, compute:\n- the exposure odds ratio comparing $X=1$ to $X=0$ under this model, and\n- the risk ratio comparing $p(1)$ to $p(0)$ under this model.\n\nReport both numerical values rounded to four significant figures. Express your final answer as a row matrix in the order (odds ratio, risk ratio). No units are required.", "solution": "The problem requires the computation of the odds ratio and the risk ratio from a given logistic regression model for a binary exposure and a binary outcome. The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution.\n\nThe model for the risk, $p(x)=\\Pr(Y=1 \\mid X=x)$, is given by a generalized linear model with a logit link function:\n$$ \\operatorname{logit}(p(x)) = \\beta_{0}+\\beta_{1} x $$\nwhere $x \\in \\{0,1\\}$. The logit function is defined as $\\operatorname{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$. The given parameters are the baseline risk $p(0) = 0.2$ and the logit coefficient for the exposure, $\\beta_{1} = 0.8$.\n\n### Part 1: Calculation of the Odds Ratio (OR)\n\nThe odds of an event for a given risk $p$ are defined as the ratio of the probability of the event occurring to the probability of it not occurring. For an exposure level $x$, the odds are:\n$$ O(x) = \\frac{p(x)}{1-p(x)} $$\nThe odds ratio ($OR$) compares the odds of the outcome in the exposed group ($X=1$) to the odds in the unexposed group ($X=0$):\n$$ OR = \\frac{O(1)}{O(0)} = \\frac{p(1)/(1-p(1))}{p(0)/(1-p(0))} $$\nFrom the definition of the logit function, we can see that $\\operatorname{logit}(p(x))$ is the natural logarithm of the odds, $O(x)$:\n$$ \\operatorname{logit}(p(x)) = \\ln\\left(\\frac{p(x)}{1-p(x)}\\right) = \\ln(O(x)) $$\nThe given logistic model can therefore be written in terms of the log-odds:\n$$ \\ln(O(x)) = \\beta_{0}+\\beta_{1} x $$\nFor the exposed group ($x=1$), the log-odds are:\n$$ \\ln(O(1)) = \\beta_{0}+\\beta_{1}(1) = \\beta_{0}+\\beta_{1} $$\nFor the unexposed group ($x=0$), the log-odds are:\n$$ \\ln(O(0)) = \\beta_{0}+\\beta_{1}(0) = \\beta_{0} $$\nThe natural logarithm of the odds ratio can be found by taking the difference between the log-odds of the two groups:\n$$ \\ln(OR) = \\ln\\left(\\frac{O(1)}{O(0)}\\right) = \\ln(O(1)) - \\ln(O(0)) $$\nSubstituting the expressions from the model:\n$$ \\ln(OR) = (\\beta_{0}+\\beta_{1}) - \\beta_{0} = \\beta_{1} $$\nThis demonstrates a fundamental property of logistic regression: the coefficient $\\beta_1$ for a binary exposure variable is the log-odds ratio. To find the odds ratio itself, we exponentiate the coefficient $\\beta_1$:\n$$ OR = \\exp(\\beta_{1}) $$\nGiven that $\\beta_{1} = 0.8$, the odds ratio is:\n$$ OR = \\exp(0.8) \\approx 2.2255409 $$\nRounding to four significant figures, we get:\n$$ OR \\approx 2.226 $$\n\n### Part 2: Calculation of the Risk Ratio (RR)\n\nThe risk ratio ($RR$), also known as the relative risk, is the ratio of the risk of the outcome in the exposed group to the risk in the unexposed group:\n$$ RR = \\frac{p(1)}{p(0)} $$\nWe are given the baseline risk $p(0) = 0.2$. To compute the risk ratio, we must first determine the risk in the exposed group, $p(1)$.\n\nWe can find the value of the intercept parameter $\\beta_0$ using the given baseline risk $p(0)$. As shown before, $\\ln(O(0)) = \\beta_0$.\n$$ \\beta_{0} = \\ln(O(0)) = \\ln\\left(\\frac{p(0)}{1-p(0)}\\right) = \\ln\\left(\\frac{0.2}{1-0.2}\\right) = \\ln\\left(\\frac{0.2}{0.8}\\right) = \\ln(0.25) $$\nNow, we can find the log-odds for the exposed group, $\\ln(O(1))$:\n$$ \\ln(O(1)) = \\beta_{0} + \\beta_{1} = \\ln(0.25) + 0.8 $$\nTo find the risk $p(1)$ from the log-odds $\\ln(O(1))$, we use the inverse-logit transformation, which is requested by the problem statement. If $\\eta = \\operatorname{logit}(p) = \\ln(p/(1-p))$, then solving for $p$ gives $p = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}$.\nLet $\\eta_1 = \\ln(O(1)) = \\ln(0.25) + 0.8$. The risk $p(1)$ is:\n$$ p(1) = \\frac{\\exp(\\eta_1)}{1+\\exp(\\eta_1)} = \\frac{\\exp(\\ln(0.25) + 0.8)}{1+\\exp(\\ln(0.25) + 0.8)} $$\nUsing the property $\\exp(a+b) = \\exp(a)\\exp(b)$, we have:\n$$ p(1) = \\frac{\\exp(\\ln(0.25)) \\cdot \\exp(0.8)}{1 + \\exp(\\ln(0.25)) \\cdot \\exp(0.8)} = \\frac{0.25 \\cdot \\exp(0.8)}{1 + 0.25 \\cdot \\exp(0.8)} $$\nSubstituting the numerical value $\\exp(0.8) \\approx 2.2255409$:\n$$ p(1) \\approx \\frac{0.25 \\times 2.2255409}{1 + 0.25 \\times 2.2255409} = \\frac{0.5563852}{1.5563852} \\approx 0.3574883 $$\nNow we can compute the risk ratio:\n$$ RR = \\frac{p(1)}{p(0)} \\approx \\frac{0.3574883}{0.2} \\approx 1.7874415 $$\nRounding to four significant figures, we get:\n$$ RR \\approx 1.787 $$\nThe two requested values are the odds ratio, approximately $2.226$, and the risk ratio, approximately $1.787$.", "answer": "$$\\boxed{\\begin{pmatrix} 2.226 & 1.787 \\end{pmatrix}}$$", "id": "4595204"}, {"introduction": "Generalized Linear Models are not limited to binary outcomes. This final practice [@problem_id:4595174] extends our analysis to count data, such as the weekly number of influenza cases. We will confront a common real-world modeling problem known as overdispersion, where the data are more variable than a standard Poisson model assumes. This exercise will show you how to estimate this excess variation and, crucially, how to adjust your model's standard errors to ensure your statistical inferences are valid.", "problem": "A study of weekly influenza-like illness counts across $n$ community clinics models the number of cases $Y_i$ in clinic $i$ using a Generalized Linear Model (GLM) with a log link and a Poisson mean structure, allowing for potential overdispersion via quasi-likelihood. Specifically, the model assumes the mean $\\mu_i$ satisfies $\\ln(\\mu_i) = \\beta_0 + \\beta_1 x_i$, where $x_i$ is a clinic-level covariate, and the variance follows $\\operatorname{Var}(Y_i) = \\phi V(\\mu_i)$ with variance function $V(\\mu) = \\mu$ and dispersion parameter $\\phi$. The fitted model yields a residual deviance of $D = 150$ with residual degrees of freedom $\\text{df} = 100$.\n\nStarting from the definition of the deviance as twice the log-likelihood ratio between the fitted model and the saturated model, and using large-sample properties of GLMs under quasi-likelihood, derive an estimator for the dispersion parameter $\\phi$. Then, derive how the presence of $\\phi \\neq 1$ modifies the variance-covariance matrix of the estimated regression coefficients and hence the standard errors relative to the naive Poisson case. Using the provided $D$ and $\\text{df}$ values, compute the multiplicative inflation factor applied to the naive Poisson standard errors due to overdispersion and report this factor as a closed-form expression. Do not approximate numerically; provide an exact expression with no units.", "solution": "The problem is well-posed and scientifically grounded in the theory of Generalized Linear Models (GLMs) and quasi-likelihood. We can proceed with a formal derivation.\n\nThe problem asks for three derivations based on a GLM for count data $Y_i$ with mean $\\mu_i$ and variance $\\operatorname{Var}(Y_i) = \\phi \\mu_i$. The model structure is given by $\\ln(\\mu_i) = \\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$, where $\\mathbf{x}_i$ is the vector of covariates for observation $i$ and $\\boldsymbol{\\beta}$ is the vector of regression coefficients. For this problem, $\\eta_i = \\beta_0 + \\beta_1 x_i$.\n\n**Part 1: Derivation of the Estimator for the Dispersion Parameter $\\phi$**\n\nThe problem states that we should start from the definition of the deviance. For a GLM, the (unscaled) deviance is defined as $D(\\mathbf{y}; \\hat{\\boldsymbol{\\mu}}) = 2 \\phi (l(\\mathbf{y}; \\mathbf{y}) - l(\\hat{\\boldsymbol{\\mu}}; \\mathbf{y}))$, where $l$ is the log-likelihood function of the saturated model and the fitted model, respectively. In the context of quasi-likelihood, where the distributional form is not fully specified and only the mean-variance relationship $\\operatorname{Var}(Y_i) = \\phi V(\\mu_i)$ is assumed, the dispersion parameter $\\phi$ is often treated as a nuisance parameter.\n\nA central result in GLM theory is that for large sample sizes, the scaled deviance, defined as $D^*(\\mathbf{y}; \\hat{\\boldsymbol{\\mu}}) = D(\\mathbf{y}; \\hat{\\boldsymbol{\\mu}}) / \\phi$, follows an approximate chi-squared distribution with degrees of freedom equal to the residual degrees of freedom of the model. The residual degrees of freedom, denoted as $\\text{df}$, is the number of observations $n$ minus the number of estimated parameters $p$.\n$$ \\frac{D}{\\phi} \\sim \\chi^2_{\\text{df}} $$\nThe expected value of a chi-squared distribution with $\\text{df}$ degrees of freedom is simply $\\text{df}$. Therefore, taking the expectation of the above expression, we get:\n$$ E\\left[\\frac{D}{\\phi}\\right] \\approx \\text{df} $$\nSince $\\phi$ is a constant, this implies:\n$$ E[D] \\approx \\phi \\cdot \\text{df} $$\nThis relationship provides a basis for a method-of-moments estimator for $\\phi$. By replacing the expected value of the deviance, $E[D]$, with its observed value, $D$, we can solve for an estimate of $\\phi$, denoted by $\\hat{\\phi}$.\n$$ \\hat{\\phi} = \\frac{D}{\\text{df}} $$\nThis is a commonly used estimator for the dispersion parameter in quasi-likelihood models. It is based on the residual deviance. A similar estimator can be derived using the Pearson chi-squared statistic.\n\n**Part 2: Modification of the Variance-Covariance Matrix and Standard Errors**\n\nThe estimation of the regression coefficients $\\boldsymbol{\\beta}$ in a GLM is typically done via iteratively reweighted least squares (IRLS), which solves the quasi-score equations. The variance-covariance matrix of the estimated coefficients, $\\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}})$, is given by the inverse of the Fisher information matrix, which in the quasi-likelihood framework is:\n$$ \\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} $$\nwhere $\\mathbf{X}$ is the model's design matrix and $\\mathbf{W}$ is a diagonal matrix of weights. Each diagonal element $W_{ii}$ of $\\mathbf{W}$ is given by:\n$$ W_{ii} = \\frac{1}{\\operatorname{Var}(Y_i)} \\left( \\frac{d\\mu_i}{d\\eta_i} \\right)^2 $$\nIn this problem, we are given the variance structure $\\operatorname{Var}(Y_i) = \\phi V(\\mu_i) = \\phi \\mu_i$. The link function is the natural logarithm, $g(\\mu_i) = \\ln(\\mu_i) = \\eta_i$. The inverse link function is $\\mu_i = \\exp(\\eta_i)$. The derivative of the mean with respect to the linear predictor is:\n$$ \\frac{d\\mu_i}{d\\eta_i} = \\frac{d}{d\\eta_i} \\exp(\\eta_i) = \\exp(\\eta_i) = \\mu_i $$\nSubstituting these expressions into the formula for the weights, we obtain:\n$$ W_{ii} = \\frac{1}{\\phi \\mu_i} (\\mu_i)^2 = \\frac{\\mu_i}{\\phi} $$\nThe weight matrix is therefore $\\mathbf{W} = \\frac{1}{\\phi} \\mathbf{W}_{\\text{naive}}$, where $\\mathbf{W}_{\\text{naive}}$ is a diagonal matrix with elements $(W_{\\text{naive}})_{ii} = \\mu_i$. This $\\mathbf{W}_{\\text{naive}}$ is the weight matrix that would be used in a standard Poisson GLM, which implicitly assumes $\\phi=1$.\n\nThe variance-covariance matrix for our quasi-Poisson model is:\n$$ \\operatorname{Cov}_Q(\\hat{\\boldsymbol{\\beta}}) = \\left(\\mathbf{X}^T \\left(\\frac{1}{\\phi} \\mathbf{W}_{\\text{naive}}\\right) \\mathbf{X}\\right)^{-1} = \\left(\\frac{1}{\\phi} \\mathbf{X}^T \\mathbf{W}_{\\text{naive}} \\mathbf{X}\\right)^{-1} $$\nUsing the matrix identity $(cA)^{-1} = c^{-1}A^{-1}$, we get:\n$$ \\operatorname{Cov}_Q(\\hat{\\boldsymbol{\\beta}}) = \\phi \\left(\\mathbf{X}^T \\mathbf{W}_{\\text{naive}} \\mathbf{X}\\right)^{-1} $$\nThe term $(\\mathbf{X}^T \\mathbf{W}_{\\text{naive}} \\mathbf{X})^{-1}$ is precisely the variance-covariance matrix for the naive Poisson model, let's call it $\\operatorname{Cov}_P(\\hat{\\boldsymbol{\\beta}})$.\n$$ \\operatorname{Cov}_Q(\\hat{\\boldsymbol{\\beta}}) = \\phi \\cdot \\operatorname{Cov}_P(\\hat{\\boldsymbol{\\beta}}) $$\nThis shows that the entire variance-covariance matrix of the estimated coefficients is inflated by a factor of $\\phi$.\n\nThe standard error (SE) of a single coefficient estimate, say $\\hat{\\beta}_j$, is the square root of the corresponding diagonal element of the variance-covariance matrix.\n$$ \\text{SE}(\\hat{\\beta}_j) = \\sqrt{[\\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}})]_{jj}} $$\nTherefore, the standard errors for the quasi-Poisson model, $\\text{SE}_Q(\\hat{\\beta}_j)$, are related to the standard errors from the naive Poisson model, $\\text{SE}_P(\\hat{\\beta}_j)$, as follows:\n$$ \\text{SE}_Q(\\hat{\\beta}_j) = \\sqrt{[\\phi \\cdot \\operatorname{Cov}_P(\\hat{\\boldsymbol{\\beta}})]_{jj}} = \\sqrt{\\phi} \\sqrt{[\\operatorname{Cov}_P(\\hat{\\boldsymbol{\\beta}})]_{jj}} = \\sqrt{\\phi} \\cdot \\text{SE}_P(\\hat{\\beta}_j) $$\nThus, the presence of overdispersion ($\\phi > 1$) or underdispersion ($\\phi < 1$) modifies the standard errors of the naive model by a multiplicative factor of $\\sqrt{\\phi}$.\n\n**Part 3: Calculation of the Multiplicative Inflation Factor**\n\nThe problem provides the following values from the fitted model:\n- Residual deviance $D = 150$.\n- Residual degrees of freedom $\\text{df} = 100$.\n\nUsing the estimator for $\\phi$ derived in Part 1, we can compute an estimate $\\hat{\\phi}$:\n$$ \\hat{\\phi} = \\frac{D}{\\text{df}} = \\frac{150}{100} = \\frac{3}{2} $$\nThe multiplicative inflation factor applied to the naive Poisson standard errors is $\\sqrt{\\hat{\\phi}}$. We compute this factor using our estimate:\n$$ \\text{Inflation Factor} = \\sqrt{\\hat{\\phi}} = \\sqrt{\\frac{3}{2}} $$\nThis is the exact, closed-form expression for the factor by which the standard errors are increased to account for the observed overdispersion.", "answer": "$$\\boxed{\\sqrt{\\frac{3}{2}}}$$", "id": "4595174"}]}