{"hands_on_practices": [{"introduction": "Understanding the precise definitions of missing data mechanisms is the first step toward choosing an appropriate analysis method. This exercise provides a hands-on look at the crucial, and often subtle, distinction between Missing At Random (MAR) and Missing Not At Random (MNAR). You will construct a scenario where the missingness mechanism's classification changes depending on which variables are considered, and quantify the bias that arises from a naive complete-case analysis [@problem_id:4928144].", "problem": "Consider a cohort study with a binary outcome $Y$ indicating the presence ($Y=1$) or absence ($Y=0$) of a condition and a continuous covariate $X$ representing a standardized risk score. Let $X \\sim \\mathcal{N}(0,1)$, and suppose the data collection protocol records the outcome $Y$ only for participants whose covariate exceeds a threshold. Define the missingness indicator $R$ for $Y$ by $R=1$ if $Y$ is observed and $R=0$ if $Y$ is missing. Assume the following data-generating process:\n- The outcome is determined by a threshold rule $Y=\\mathbf{1}\\{X0\\}$.\n- The missingness mechanism is $R=\\mathbf{1}\\{X-0.5\\}$.\n\nUsing the standard definitions of Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), treat the mechanism $R$ as MAR when conditioning on $X$ but MNAR when $X$ is omitted from the conditioning set, and show this formally by computing $P(R=1 \\mid Y=1)$ and $P(R=1 \\mid Y=0)$ under the specified joint distribution.\n\nThen, consider the complete-case estimator of the marginal prevalence $p=\\Pr(Y=1)$, defined as $p_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)$. Derive $p$ and $p_{\\mathrm{cc}}$, and compute the bias $b = p_{\\mathrm{cc}} - p$ as a closed-form analytic expression in terms of the standard normal cumulative distribution function $\\Phi$. Finally, briefly explain the implications of this mechanism for complete-case analysis and for the validity of Multiple Imputation (MI).\n\nYour final answer should be a single closed-form analytic expression for $b$ in terms of $\\Phi$.", "solution": "We begin with the fundamental definitions. Let $R$ denote the missingness indicator for $Y$. Missing Completely At Random (MCAR) is defined by $P(R \\mid Y_{\\mathrm{mis}}, Y_{\\mathrm{obs}}, X)=P(R)$, Missing At Random (MAR) is defined by $P(R \\mid Y_{\\mathrm{mis}}, Y_{\\mathrm{obs}}, X)=P(R \\mid Y_{\\mathrm{obs}}, X)$, and Missing Not At Random (MNAR) is the complement where the distribution of $R$ depends on unobserved data even after conditioning on observed quantities.\n\nIn the given setup, $X \\sim \\mathcal{N}(0,1)$, the outcome is $Y=\\mathbf{1}\\{X0\\}$, and missingness is $R=\\mathbf{1}\\{X-0.5\\}$. Because $R$ depends only on $X$, we have\n$$\nP(R=1 \\mid X,Y)=P(R=1 \\mid X)=\\mathbf{1}\\{X-0.5\\},\n$$\nwhich shows that, conditional on $X$, missingness does not depend on unobserved values of $Y$, satisfying Missing At Random (MAR) when conditioning on $X$.\n\nTo show that the mechanism is Missing Not At Random (MNAR) when $X$ is omitted, we compute $P(R=1 \\mid Y)$ marginalizing $X$ but conditioning on $Y$. First observe that $Y=1 \\iff X0$ and $Y=0 \\iff X \\le 0$ due to the threshold rule. Then:\n- For $Y=1$, we have $X0$, which implies $X-0.5$, thus\n$$\nP(R=1 \\mid Y=1)=P(X-0.5 \\mid X0)=1.\n$$\n- For $Y=0$, we have $X \\le 0$, and missingness is $R=1$ if $X-0.5$. Therefore\n$$\nP(R=1 \\mid Y=0)=P(-0.5  X \\le 0 \\mid X \\le 0)\n=\\frac{P(-0.5  X \\le 0)}{P(X \\le 0)}\n=\\frac{\\Phi(0)-\\Phi(-0.5)}{\\Phi(0)},\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. Using the symmetry $\\Phi(-a)=1-\\Phi(a)$ and $\\Phi(0)=\\frac{1}{2}$, this simplifies to\n$$\nP(R=1 \\mid Y=0)=\\frac{\\frac{1}{2}-\\left(1-\\Phi(0.5)\\right)}{\\frac{1}{2}}\n=\\frac{\\Phi(0.5)-\\frac{1}{2}}{\\frac{1}{2}}\n=2\\Phi(0.5)-1.\n$$\nSince $P(R=1 \\mid Y=1)=1$ and $P(R=1 \\mid Y=0)=2\\Phi(0.5)-11$, we have $P(R=1 \\mid Y=1) \\neq P(R=1 \\mid Y=0)$, which demonstrates MNAR when $X$ is omitted.\n\nNext, we derive the true marginal prevalence $p=\\Pr(Y=1)$. Because $Y=\\mathbf{1}\\{X0\\}$ and $X \\sim \\mathcal{N}(0,1)$, we have\n$$\np=\\Pr(Y=1)=\\Pr(X0)=1-\\Phi(0)=\\frac{1}{2}.\n$$\n\nWe now derive the complete-case prevalence $p_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)$. Since $R=1$ if and only if $X-0.5$, and $Y=1$ if and only if $X0$, we have\n$$\np_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)=\\Pr(X0 \\mid X-0.5)\n=\\frac{\\Pr(X0)}{\\Pr(X-0.5)}=\\frac{\\frac{1}{2}}{\\Phi(0.5)}.\n$$\nTherefore, the bias of the complete-case estimator of the marginal prevalence is\n$$\nb = p_{\\mathrm{cc}} - p \n= \\frac{\\frac{1}{2}}{\\Phi(0.5)} - \\frac{1}{2}\n= \\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right).\n$$\n\nImplications for analysis:\n- Complete-case analysis estimates $p_{\\mathrm{cc}}$ instead of $p$ and is biased because the observed data are enriched for larger $X$ values, which deterministically correspond to $Y=1$ under the threshold rule; hence $p_{\\mathrm{cc}}p$ and the bias $b$ is positive.\n- Multiple Imputation (MI) would yield valid inference under the Missing At Random (MAR) assumption if the imputation model conditions on $X$ (the variable driving missingness). Because $R$ depends only on $X$, including $X$ in the imputation model aligns with MAR and allows consistent recovery of the marginal distribution of $Y$. In contrast, omitting $X$ would mis-specify the missingness as Missing Not At Random (MNAR) relative to the imputation model, generally leading to biased imputations and biased estimates of $p$.\n\nThe required closed-form analytic expression for the bias $b$ is\n$$\nb=\\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right).\n$$", "answer": "$$\\boxed{\\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right)}$$", "id": "4928144"}, {"introduction": "Once you have decided to use Multiple Imputation (MI), a key practical question arises: how many imputed datasets should you create? This practice delves into the theory behind this decision, connecting the \"fraction of missing information\" ($\\lambda$) to the required number of imputations, $M$. By working through this problem, you will derive and apply a formula to ensure your analysis is statistically efficient and your inferences are stable [@problem_id:4928119].", "problem": "A prospective cohort study investigates the association between baseline systolic blood pressure and a continuous inflammation biomarker using linear regression. Due to survey logistics, a subset of biomarker measurements is missing. Exploratory analyses and study procedures suggest that the missingness mechanism is Missing At Random (MAR), conditional on observed covariates collected for all participants. The investigators plan to use Multiple Imputation (MI) for handling the missing data and must choose the number of imputations $M$ to ensure that inference is stable relative to the ideal of infinitely many imputations. \n\nThey carry out a pilot MI with a large temporary number of imputations to estimate the fraction of missing information for the regression slope of interest, obtaining an estimate $\\hat{\\lambda} = 0.35$. The team wishes to select the smallest $M$ such that the relative efficiency (defined as the efficiency of MI with finite $M$ relative to MI with infinitely many imputations) is at least $\\tau = 0.99$.\n\nWorking from core definitions of missingness mechanisms (Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR)), the concept of complete case analysis, and the standard large-sample relationship between the fraction of missing information and the relative efficiency under MI, address the following:\n\n1. Explain qualitatively how the overall missingness rate influences the fraction of missing information $\\lambda$ under MCAR and MAR, and how this, in turn, affects the required number of imputations $M$ for stable inference. Contrast this with complete case analysis, focusing on information loss.\n2. Using a well-tested approximation that links relative efficiency under MI to the fraction of missing information, derive an inequality for $M$ in terms of $\\lambda$ and $\\tau$ that guarantees the desired efficiency threshold.\n3. Compute the minimal integer $M$ that satisfies the criterion when $\\hat{\\lambda} = 0.35$ and $\\tau = 0.99$. Report the minimal integer value of $M$. No additional rounding beyond selecting this minimal integer is required.", "solution": "The problem is addressed in three parts as requested.\n\n#### 1. Qualitative Explanation of Missingness, Information, and Analysis Choices\n\nThe missing data mechanisms are defined by the dependence of the probability of missingness on the data. A value is Missing Completely At Random (MCAR) if the probability of it being missing is independent of all variables, both observed and unobserved. A value is Missing At Random (MAR) if, conditional on the observed data, the probability of it being missing is independent of the unobserved values. A value is Missing Not At Random (MNAR) if the probability of missingness depends on the unobserved value itself, even after conditioning on observed data. The problem states a MAR mechanism.\n\nThe fraction of missing information, $\\lambda$, quantifies the amount of statistical information about a specific parameter (here, the regression slope) that is lost due to missing data. It is not simply the percentage of missing data points, but rather a function of it, also influenced by the predictive power of the observed data for the missing values.\n\nUnder both MCAR and MAR, the overall rate of missing data has a direct, positive relationship with $\\lambda$. A higher proportion of missing values in the biomarker variable will generally lead to a larger fraction of missing information, $\\lambda$, for the regression slope. However, under MAR, this relationship is moderated by the strength of the imputation model. If the observed covariates (used for imputation) are highly correlated with the biomarker, they can \"recover\" a substantial amount of information, leading to a lower $\\lambda$ for a given missingness rate than would be seen under a weak imputation model or MCAR.\n\nThe required number of imputations, $M$, is directly driven by $\\lambda$. Multiple Imputation accounts for the uncertainty caused by missing data by generating $M$ plausible versions of the complete dataset and pooling the results. The total variance of an MI estimate is a sum of the average within-imputation variance and the between-imputation variance, the latter capturing the uncertainty from missingness. A larger $\\lambda$ implies greater uncertainty due to missingness, which manifests as larger between-imputation variance. To estimate this variance component reliably and achieve stable, efficient inference, a larger number of imputations, $M$, is required.\n\nIn contrast, complete case analysis (CCA), which analyzes only the subjects with no missing data, handles missingness by deletion. For CCA, the primary form of information loss is the reduction in sample size. This loss of power is often much more severe than the effective information loss in a properly conducted MI. While CCA provides unbiased estimates under the strict MCAR assumption, it can lead to severely biased estimates under MAR, which is the scenario in this problem. For instance, if having a high biomarker value is associated with an observed covariate that also predicts a lower chance of the measurement being taken, CCA would systematically exclude these subjects, biasing the estimated regression slope. MI, by using all available data, avoids this sample-size reduction and, if the imputation model is correctly specified, provides unbiased and more efficient estimates under MAR.\n\n#### 2. Derivation of the Inequality for $M$\n\nLet $\\hat{\\theta}$ be the estimate of the parameter of interest (the regression slope). In Multiple Imputation with $M$ imputed datasets, the total variance of the pooled estimate, denoted $T_M$, is given by Rubin's rules as:\n$$ T_M = W + \\left(1 + \\frac{1}{M}\\right)B $$\nwhere $W$ is the average within-imputation variance and $B$ is the between-imputation variance.\n\nThe ideal variance, which would be obtained with an infinite number of imputations ($M \\to \\infty$), is:\n$$ T_{\\infty} = \\lim_{M\\to\\infty} \\left[W + \\left(1 + \\frac{1}{M}\\right)B\\right] = W + B $$\nThe fraction of missing information, $\\lambda$, is defined as the proportion of the total variance that is attributable to the missing data:\n$$ \\lambda = \\frac{B}{T_{\\infty}} = \\frac{B}{W+B} $$\nRelative efficiency, $\\tau$, is the ratio of the ideal variance to the variance with a finite number of imputations, $M$:\n$$ \\tau = \\frac{T_{\\infty}}{T_M} = \\frac{W+B}{W + \\left(1 + \\frac{1}{M}\\right)B} $$\nTo express this in terms of $\\lambda$, we can divide the numerator and denominator by $T_{\\infty} = W+B$:\n$$ \\tau = \\frac{\\frac{W+B}{W+B}}{\\frac{W}{W+B} + \\left(1 + \\frac{1}{M}\\right)\\frac{B}{W+B}} $$\nFrom the definition of $\\lambda$, we know that $\\frac{B}{W+B} = \\lambda$ and $\\frac{W}{W+B} = 1 - \\lambda$. Substituting these into the equation for $\\tau$ gives:\n$$ \\tau = \\frac{1}{(1-\\lambda) + \\left(1 + \\frac{1}{M}\\right)\\lambda} = \\frac{1}{1-\\lambda + \\lambda + \\frac{\\lambda}{M}} = \\frac{1}{1 + \\frac{\\lambda}{M}} $$\nThis is the \"well-tested approximation\" or standard formula. The problem requires a relative efficiency of at least $\\tau$. We therefore set up the inequality:\n$$ \\frac{1}{1 + \\frac{\\lambda}{M}} \\geq \\tau $$\nSince $1 + \\frac{\\lambda}{M}$ and $\\tau$ are positive, we can manipulate the inequality:\n$$ 1 \\geq \\tau \\left(1 + \\frac{\\lambda}{M}\\right) $$\n$$ \\frac{1}{\\tau} \\geq 1 + \\frac{\\lambda}{M} $$\n$$ \\frac{1}{\\tau} - 1 \\geq \\frac{\\lambda}{M} $$\n$$ \\frac{1-\\tau}{\\tau} \\geq \\frac{\\lambda}{M} $$\nSince $M$ must be a positive integer, we can multiply by $M$ and rearrange to solve for $M$:\n$$ M \\left(\\frac{1-\\tau}{\\tau}\\right) \\geq \\lambda $$\n$$ M \\geq \\lambda \\left(\\frac{\\tau}{1-\\tau}\\right) $$\nThis is the desired inequality for $M$ in terms of $\\lambda$ and $\\tau$.\n\n#### 3. Calculation of the Minimal Integer $M$\n\nWe are given the estimated fraction of missing information $\\hat{\\lambda} = 0.35$ and the target relative efficiency $\\tau = 0.99$. We use the estimated $\\hat{\\lambda}$ as a plug-in for the true $\\lambda$ in the derived inequality.\n$$ M \\geq 0.35 \\left(\\frac{0.99}{1-0.99}\\right) $$\n$$ M \\geq 0.35 \\left(\\frac{0.99}{0.01}\\right) $$\n$$ M \\geq 0.35 \\times 99 $$\n$$ M \\geq 34.65 $$\nSince the number of imputations, $M$, must be an integer, we must select the smallest integer that satisfies this condition. The smallest integer greater than or equal to $34.65$ is $35$.\nTherefore, a minimum of $M=35$ imputations is required to achieve the desired relative efficiency of at least $99\\%$.", "answer": "$$\\boxed{35}$$", "id": "4928119"}, {"introduction": "The effectiveness of Multiple Imputation hinges on the quality of the imputation model itselfâ€”the model used to predict the missing values. This exercise simulates a common scenario in which you must choose the best imputation model from several candidates based on the observed data. You will apply the Bayesian Information Criterion (BIC), a standard, likelihood-based tool, to balance model fit and complexity and make a principled decision [@problem_id:4611910].", "problem": "A cohort study aims to estimate associations between a continuous inflammatory biomarker $Y$ measured at baseline and two covariates: age $X$ (in years) and smoking status $Z$ (binary, $Z \\in \\{0,1\\}$). The biomarker $Y$ is missing for some participants due to nonresponse. Investigators judge, based on study design and auxiliary information, that the missingness is Missing At Random (MAR), meaning $P(R=1 \\mid Y, X, Z) = P(R=1 \\mid X, Z)$, where $R$ indicates response for $Y$. To perform Multiple Imputation (MI), they will impute $Y$ under a homoscedastic normal linear model for the conditional distribution $Y \\mid X, Z$.\n\nThey consider three candidate imputation models for $Y \\mid X, Z$, all with independent, identically distributed normal errors and an unknown residual variance:\n\n- Model $M_{1}$: $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$.\n- Model $M_{2}$: $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z + \\varepsilon$.\n- Model $M_{3}$: $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z + \\beta_{3} XZ + \\beta_{4} X^{2} + \\varepsilon$.\n\nAll three models include an unknown error variance $\\sigma^{2}$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, independent across participants. Using only the observed cases for $Y$, with $n_{\\text{obs}} = 420$, the maximum observed-data log-likelihoods for the three models are:\n$$\\ell_{1} = -605.3,\\quad \\ell_{2} = -598.9,\\quad \\ell_{3} = -597.2.$$\n\nChoose an imputation model by a large-sample, likelihood-based selection principle that is consistent under MAR and suitable for selecting among parametric models for the observed-data distribution of $Y \\mid X, Z$. Report the index $m \\in \\{1,2,3\\}$ of the selected model as a single number. No rounding is required for the final answer.", "solution": "The selection of the imputation model will be based on the Bayesian Information Criterion (BIC), as it satisfies the problem's requirement for a large-sample, likelihood-based selection principle that is consistent. The BIC is defined as:\n$$BIC = k \\ln(n) - 2\\ell$$\nwhere $k$ is the number of estimated parameters in the model, $n$ is the number of observations used to fit the model, and $\\ell$ is the maximized value of the log-likelihood function. The model with the minimum BIC value is to be preferred.\n\nIn this problem, the models are fit using the $n_{\\text{obs}} = 420$ participants with complete data on $Y$. Thus, we set $n = 420$. The log-likelihood values $\\ell_m$ for each model $M_m$ are given. The number of parameters, $k$, for each model must be determined. For a normal linear model, the parameters consist of the regression coefficients ($\\beta$s) and the residual variance ($\\sigma^{2}$).\n\n1.  **Model $M_{1}$: $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$**\n    The parameters are $\\beta_0$, $\\beta_1$, and $\\sigma^2$.\n    The number of parameters is $k_{1} = 3$.\n    The log-likelihood is $\\ell_{1} = -605.3$.\n    The BIC is:\n    $$BIC_{1} = k_{1} \\ln(n_{\\text{obs}}) - 2\\ell_{1} = 3 \\ln(420) - 2(-605.3) = 3 \\ln(420) + 1210.6$$\n\n2.  **Model $M_{2}$: $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z + \\varepsilon$**\n    The parameters are $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\sigma^2$.\n    The number of parameters is $k_{2} = 4$.\n    The log-likelihood is $\\ell_{2} = -598.9$.\n    The BIC is:\n    $$BIC_{2} = k_{2} \\ln(n_{\\text{obs}}) - 2\\ell_{2} = 4 \\ln(420) - 2(-598.9) = 4 \\ln(420) + 1197.8$$\n\n3.  **Model $M_{3}$: $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} Z + \\beta_{3} XZ + \\beta_{4} X^{2} + \\varepsilon$**\n    The parameters are $\\beta_0$, $\\beta_1$, $\\beta_2$, $\\beta_3$, $\\beta_4$, and $\\sigma^2$.\n    The number of parameters is $k_{3} = 6$.\n    The log-likelihood is $\\ell_{3} = -597.2$.\n    The BIC is:\n    $$BIC_{3} = k_{3} \\ln(n_{\\text{obs}}) - 2\\ell_{3} = 6 \\ln(420) - 2(-597.2) = 6 \\ln(420) + 1194.4$$\n\nNow, we must compare the three BIC values to find the minimum. We can use the approximation $\\ln(420) \\approx 6.04025$.\n$$BIC_{1} \\approx 3(6.04025) + 1210.6 = 18.12075 + 1210.6 = 1228.72075$$\n$$BIC_{2} \\approx 4(6.04025) + 1197.8 = 24.161 + 1197.8 = 1221.961$$\n$$BIC_{3} \\approx 6(6.04025) + 1194.4 = 36.2415 + 1194.4 = 1230.6415$$\n\nComparing the calculated values:\n$BIC_{1} \\approx 1228.72$\n$BIC_{2} \\approx 1221.96$\n$BIC_{3} \\approx 1230.64$\n\nThe minimum value is $BIC_{2}$. This suggests that model $M_2$ provides the best trade-off between model fit (as measured by the log-likelihood) and model complexity (as measured by the number of parameters), according to the BIC.\n\nTherefore, the selected model is $M_{2}$. The problem asks for the index $m$ of the selected model, which is $2$.", "answer": "$$\\boxed{2}$$", "id": "4611910"}]}