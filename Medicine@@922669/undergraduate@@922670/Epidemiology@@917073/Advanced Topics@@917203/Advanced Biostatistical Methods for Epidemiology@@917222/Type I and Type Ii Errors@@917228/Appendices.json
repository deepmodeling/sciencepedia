{"hands_on_practices": [{"introduction": "In clinical practice and public health screening, decisions often hinge on where to set a cut-off for a continuous biomarker. Setting a threshold is a classic balancing act between two types of errors. This exercise puts you in the role of an epidemiologist designing a screening program, where you must define a diagnostic threshold that meets a specific Type I error rate ($\\alpha$) and then explore the consequence for the Type II error rate ($\\beta$), directly illustrating their inherent trade-off. [@problem_id:4646871]", "problem": "An epidemiology screening program uses a continuous biomarker $X$ to classify individuals as diseased or non-diseased. Higher values of $X$ indicate greater likelihood of disease. Suppose that, conditional on non-disease (null hypothesis $H_{0}$), the biomarker is normally distributed as $X \\mid H_{0} \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ with $\\mu_{0} = 50$ and $\\sigma_{0} = 10$, and, conditional on disease (alternative hypothesis $H_{1}$), it is distributed as $X \\mid H_{1} \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$ with $\\mu_{1} = 80$ and $\\sigma_{1} = 12$. A threshold rule is used: classify as diseased if $X \\geq \\tau$ and non-diseased otherwise.\n\nStarting from the fundamental definitions of Type I error ($\\alpha$) as the probability of incorrectly classifying a non-diseased individual as diseased, and Type II error ($\\beta$) as the probability of incorrectly classifying a diseased individual as non-diseased, and using only properties of the normal distribution and the Cumulative Distribution Function (CDF) $\\Phi$ of the standard normal, derive the threshold $\\tau$ that achieves $\\alpha = 0.05$ and then compute the corresponding $\\beta$ under $H_{1}$.\n\nRound both $\\tau$ and $\\beta$ to four significant figures. Report $\\beta$ as a decimal. The final answer must consist of the two values $(\\tau, \\beta)$.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It represents a standard application of hypothesis testing in a biomedical context. We may proceed with the solution.\n\nLet $X$ be the continuous biomarker. The null hypothesis, $H_{0}$, corresponds to a non-diseased individual, and the alternative hypothesis, $H_{1}$, corresponds to a diseased individual. The distributions of $X$ under each hypothesis are given as:\n$X \\mid H_{0} \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, with $\\mu_{0} = 50$ and $\\sigma_{0} = 10$.\n$X \\mid H_{1} \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$, with $\\mu_{1} = 80$ and $\\sigma_{1} = 12$.\n\nThe decision rule is to classify an individual as diseased if their biomarker value $X$ is greater than or equal to a threshold $\\tau$, i.e., $X \\geq \\tau$.\n\nThe Type I error rate, $\\alpha$, is the probability of incorrectly classifying a non-diseased individual as diseased. This is the probability of rejecting $H_{0}$ when $H_{0}$ is true.\n$$ \\alpha = P(\\text{Reject } H_{0} \\mid H_{0} \\text{ is true}) = P(X \\geq \\tau \\mid H_{0}) $$\nTo compute this probability, we standardize the random variable $X$ using the parameters of its distribution under $H_{0}$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$.\n$$ \\alpha = P\\left(\\frac{X - \\mu_{0}}{\\sigma_{0}} \\geq \\frac{\\tau - \\mu_{0}}{\\sigma_{0}} \\mid H_{0}\\right) = P\\left(Z \\geq \\frac{\\tau - \\mu_{0}}{\\sigma_{0}}\\right) $$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution, defined as $\\Phi(z) = P(Z \\leq z)$. The probability of $Z$ exceeding a value is $P(Z \\geq z) = 1 - P(Z  z) = 1 - \\Phi(z)$.\nThus, we can write $\\alpha$ as:\n$$ \\alpha = 1 - \\Phi\\left(\\frac{\\tau - \\mu_{0}}{\\sigma_{0}}\\right) $$\nWe are given that $\\alpha = 0.05$. We can now solve for the argument of $\\Phi$:\n$$ 0.05 = 1 - \\Phi\\left(\\frac{\\tau - \\mu_{0}}{\\sigma_{0}}\\right) $$\n$$ \\Phi\\left(\\frac{\\tau - \\mu_{0}}{\\sigma_{0}}\\right) = 1 - 0.05 = 0.95 $$\nThis means that the term $\\frac{\\tau - \\mu_{0}}{\\sigma_{0}}$ is the $95^{th}$ percentile of the standard normal distribution. Let this value be denoted by $z_{0.05}$, where $\\Phi(z_{0.05}) = 0.95$. The value of $z_{0.05}$ is approximately $1.64485$.\n$$ \\frac{\\tau - \\mu_{0}}{\\sigma_{0}} = z_{0.05} \\approx 1.64485 $$\nWe can now solve for the threshold $\\tau$:\n$$ \\tau = \\mu_{0} + z_{0.05} \\cdot \\sigma_{0} $$\nSubstituting the given values $\\mu_{0} = 50$ and $\\sigma_{0} = 10$:\n$$ \\tau = 50 + (1.64485) \\cdot 10 = 50 + 16.4485 = 66.4485 $$\nRounding to four significant figures, the threshold is $\\tau \\approx 66.45$.\n\nNext, we calculate the Type II error rate, $\\beta$, which is the probability of incorrectly classifying a diseased individual as non-diseased. This is the probability of failing to reject $H_{0}$ when $H_{1}$ is true. The classification rule for non-diseased is $X  \\tau$.\n$$ \\beta = P(\\text{Fail to reject } H_{0} \\mid H_{1} \\text{ is true}) = P(X  \\tau \\mid H_{1}) $$\nTo compute this probability, we now use the distribution under the alternative hypothesis, $X \\mid H_{1} \\sim \\mathcal{N}(\\mu_{1}, \\sigma_{1}^{2})$. We standardize $X$ using the parameters $\\mu_{1}=80$ and $\\sigma_{1}=12$.\n$$ \\beta = P\\left(\\frac{X - \\mu_{1}}{\\sigma_{1}}  \\frac{\\tau - \\mu_{1}}{\\sigma_{1}} \\mid H_{1}\\right) = P\\left(Z  \\frac{\\tau - \\mu_{1}}{\\sigma_{1}}\\right) $$\nUsing the definition of the standard normal CDF, this is:\n$$ \\beta = \\Phi\\left(\\frac{\\tau - \\mu_{1}}{\\sigma_{1}}\\right) $$\nWe use the unrounded value of $\\tau = 66.4485$ to maintain precision in the intermediate calculation.\n$$ \\beta = \\Phi\\left(\\frac{66.4485 - 80}{12}\\right) = \\Phi\\left(\\frac{-13.5515}{12}\\right) $$\n$$ \\beta = \\Phi(-1.12929...) $$\nUsing a standard normal CDF calculator, we find the value of $\\beta$:\n$$ \\beta \\approx 0.129386 $$\nRounding to four significant figures, the Type II error rate is $\\beta \\approx 0.1294$.\n\nThe final results are $\\tau \\approx 66.45$ and $\\beta \\approx 0.1294$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 66.45  0.1294 \\end{pmatrix}}\n$$", "id": "4646871"}, {"introduction": "A crucial skill for any epidemiologist is to correctly interpret the results of a diagnostic test within a real-world population. It is a common and dangerous mistake to equate a test's false positive rate (a Type I error, $\\alpha$) with the chance that a person with a positive test is actually disease-free. This practice will guide you through calculating a test's positive predictive value (PPV), demonstrating through Bayes' theorem why it is distinct from $1-\\alpha$ and how it is critically dependent on the disease prevalence in the population being tested. [@problem_id:4646863]", "problem": "A public health laboratory deploys a binary diagnostic assay to screen for a low-prevalence infectious disease in a community. Let $D$ denote the event that an individual truly has the disease, and $T^{+}$ denote the event that the assay returns a positive result. The test has sensitivity $\\Pr(T^{+}\\mid D)=0.88$, specificity $\\Pr(T^{-}\\mid \\neg D)=0.96$, and the disease prevalence in the screened population is $\\Pr(D)=0.03$. Investigators propose to interpret a positive test as “rejecting the null hypothesis $H_{0}$: no disease,” with a nominal significance level $\\alpha$ equal to the test’s false positive rate $\\Pr(T^{+}\\mid \\neg D)$.\n\nStarting from Bayes’ theorem and the core definitions of sensitivity, specificity, prevalence, Type I error, and Type II error, derive an analytic expression for the positive predictive value $\\Pr(D\\mid T^{+})$ in terms of $\\Pr(T^{+}\\mid D)$, $\\Pr(T^{-}\\mid \\neg D)$, and $\\Pr(D)$. Then compute its numerical value for the assay described above. As part of your derivation, explain—using only first principles—why $\\Pr(D\\mid T^{+})$ cannot be interpreted as $1-\\alpha$ in a hypothesis test, even when the decision rule “test positive $\\Rightarrow$ reject $H_{0}$” is used.\n\nProvide the numerical value of $\\Pr(D\\mid T^{+})$ as a decimal, rounded to four significant figures. Do not use a percentage sign.", "solution": "We begin with Bayes’ theorem and the standard definitions used in epidemiologic test evaluation. Sensitivity is defined as $\\Pr(T^{+}\\mid D)$, specificity is defined as $\\Pr(T^{-}\\mid \\neg D)$, and prevalence is defined as $\\Pr(D)$. The positive predictive value (PPV) is $\\Pr(D\\mid T^{+})$, the probability a person has the disease given a positive test.\n\nBy Bayes’ theorem,\n$$\n\\Pr(D\\mid T^{+}) \\;=\\; \\frac{\\Pr(T^{+}\\mid D)\\,\\Pr(D)}{\\Pr(T^{+})}.\n$$\nThe marginal probability of a positive test decomposes over the disease status:\n$$\n\\Pr(T^{+}) \\;=\\; \\Pr(T^{+}\\mid D)\\,\\Pr(D) \\;+\\; \\Pr(T^{+}\\mid \\neg D)\\,\\Pr(\\neg D).\n$$\nNote that $\\Pr(T^{+}\\mid \\neg D)$ is the false positive probability, which equals $1-\\text{specificity}$ because $\\Pr(T^{-}\\mid \\neg D)=\\text{specificity}$ and $\\Pr(T^{+}\\mid \\neg D)=1-\\Pr(T^{-}\\mid \\neg D)$. Also, $\\Pr(\\neg D)=1-\\Pr(D)$ by the law of total probability.\n\nSubstituting these identities, we obtain the analytic expression\n$$\n\\Pr(D\\mid T^{+}) \\;=\\; \\frac{\\Pr(T^{+}\\mid D)\\,\\Pr(D)}{\\Pr(T^{+}\\mid D)\\,\\Pr(D) \\;+\\; \\bigl(1-\\Pr(T^{-}\\mid \\neg D)\\bigr)\\,\\bigl(1-\\Pr(D)\\bigr)}.\n$$\nThis expression shows that positive predictive value depends on three quantities: sensitivity, specificity, and prevalence.\n\nWe now compute the numerical value with the provided parameters. The sensitivity is $\\Pr(T^{+}\\mid D)=0.88$, the specificity is $\\Pr(T^{-}\\mid \\neg D)=0.96$, hence the false positive probability is $\\Pr(T^{+}\\mid \\neg D)=1-0.96=0.04$, and the prevalence is $\\Pr(D)=0.03$ so $\\Pr(\\neg D)=1-0.03=0.97$. Plugging into the expression,\n$$\n\\Pr(D\\mid T^{+})\n\\;=\\;\n\\frac{0.88 \\times 0.03}{0.88 \\times 0.03 \\;+\\; 0.04 \\times 0.97}\n\\;=\\;\n\\frac{0.0264}{0.0264 \\;+\\; 0.0388}\n\\;=\\;\n\\frac{0.0264}{0.0652}.\n$$\nThis ratio simplifies exactly to $\\frac{66}{163}$, and numerically\n$$\n\\Pr(D\\mid T^{+}) \\approx 0.404907954\\ldots\n$$\nRounded to four significant figures, the positive predictive value is $0.4049$.\n\nWe next explain why $\\Pr(D\\mid T^{+})$ cannot be interpreted as $1-\\alpha$ in a hypothesis test. In the decision framework “test positive $\\Rightarrow$ reject $H_{0}$” with $H_{0}$ being “no disease,” the Type I error (false positive) rate is\n$$\n\\alpha \\;=\\; \\Pr(\\text{reject } H_{0}\\mid H_{0} \\text{ true}) \\;=\\; \\Pr(T^{+}\\mid \\neg D) \\;=\\; 1-\\text{specificity}.\n$$\nTherefore, $1-\\alpha$ equals specificity:\n$$\n1-\\alpha \\;=\\; \\Pr(T^{-}\\mid \\neg D) \\;=\\; \\text{specificity}.\n$$\nSpecificity is $\\Pr(T^{-}\\mid \\neg D)$, a conditional probability of a negative result given no disease, which is a property of the test conditional on the absence of disease. In contrast, the positive predictive value is\n$$\n\\Pr(D\\mid T^{+}) \\;=\\; \\frac{\\Pr(T^{+}\\mid D)\\,\\Pr(D)}{\\Pr(T^{+}\\mid D)\\,\\Pr(D)+\\Pr(T^{+}\\mid \\neg D)\\,\\Pr(\\neg D)},\n$$\na posterior probability of disease given a positive test that depends on the prevalence $\\Pr(D)$ as well as sensitivity and specificity. These are probabilities conditioned on different events: $\\Pr(D\\mid T^{+})$ conditions on the test result, while $\\Pr(T^{-}\\mid \\neg D)$ conditions on disease status.\n\nConcretely in this scenario, $1-\\alpha=\\text{specificity}=0.96$, whereas the computed $\\Pr(D\\mid T^{+})\\approx 0.4049$. The large discrepancy arises because the positive predictive value incorporates the low prevalence $\\Pr(D)=0.03$, which shifts the posterior probability downward even when specificity is high. Hence, by first principles, $\\Pr(D\\mid T^{+})$ cannot be interpreted as $1-\\alpha$; they are distinct quantities with different conditionings and dependence on prevalence. Moreover, while Type II error $\\beta=\\Pr(\\text{fail to reject } H_{0}\\mid H_{1} \\text{ true})=\\Pr(T^{-}\\mid D)=1-\\text{sensitivity}$ influences $\\Pr(D\\mid T^{+})$ through sensitivity, $1-\\alpha$ depends only on specificity and not on prevalence or sensitivity, further underscoring that $\\Pr(D\\mid T^{+})\\neq 1-\\alpha$ except in degenerate cases.", "answer": "$$\\boxed{0.4049}$$", "id": "4646863"}, {"introduction": "Moving from evaluating tests to designing new research, one of the most important applications of error control is in planning studies. Before investing significant resources in a clinical trial, researchers must ensure their study has a high probability of detecting an effect if it truly exists—a concept known as statistical power ($1-\\beta$). This problem challenges you to perform a fundamental task in study design: calculating the required sample size to achieve a desired power, given acceptable rates for both Type I and Type II errors. [@problem_id:4646891]", "problem": "A public health team plans a two-arm randomized trial to compare an intervention against control on a binary infection outcome within one season. Let the control arm have baseline risk $p_{0} = 0.1$ and suppose the intervention is expected to increase the risk by a risk ratio $r = 1.5$, so that the intervention arm risk is $p_{1} = r p_{0}$. Outcomes within and across arms are independent Bernoulli trials, and allocation is $1:1$ with $n$ individuals per arm. The team will conduct a two-sided hypothesis test of $H_{0}: \\text{risk ratio} = 1$ at Type I error rate $\\alpha = 0.05$, and they wish to achieve power $1 - \\beta = 0.9$ to detect the alternative $H_{1}: \\text{risk ratio} = 1.5$. Assume a large-sample two-sided Wald test on the log risk ratio and use the standard asymptotic normal approximation for the estimator of the log risk ratio under independent Bernoulli sampling.\n\nStarting from the definitions of Type I and Type II errors and the large-sample behavior of estimators under the Central Limit Theorem and the delta method, derive the required sample size formula and compute the smallest integer $n$ per arm that achieves power at least $0.9$ for the stated design. Report only this minimal integer $n$ per arm. Do not provide any intermediate quantities. Use natural logarithms for any log transformations. Express your final answer as the smallest integer $n$ (no units).", "solution": "The hypotheses are $H_{0}: r = 1$ versus two-sided $H_{1}: r \\neq 1$, with target detection at $r = 1.5$. Type I error is controlled at $\\alpha = 0.05$ two-sided, corresponding to the critical value $z_{1-\\alpha/2}$ of the standard normal distribution. Type II error $\\beta$ corresponds to missing the effect when it is truly present; power is $1-\\beta = 0.9$, with $z_{1-\\beta}$ denoting the $(1-\\beta)$ standard normal quantile.\n\nLet $p_{0} = 0.1$ be the control risk, and under the alternative $r = 1.5$, the intervention risk is $p_{1} = r p_{0} = 0.15$. Consider the estimator of the log risk ratio, $\\widehat{\\theta} = \\ln(\\widehat{p}_{1}/\\widehat{p}_{0})$, where $\\widehat{p}_{a}$ is the sample proportion in arm $a \\in \\{0,1\\}$ with $n$ observations per arm.\n\nBy the Central Limit Theorem, $\\widehat{p}_{a} \\overset{approx}{\\sim} \\mathcal{N}(p_{a}, p_{a}(1-p_{a})/n)$. By the delta method with $g(x) = \\ln(x)$ and $g'(x) = 1/x$, we have\n$$\n\\operatorname{Var}\\!\\left(\\ln(\\widehat{p}_{a})\\right) \\approx \\left(\\frac{1}{p_{a}}\\right)^{2} \\operatorname{Var}(\\widehat{p}_{a}) = \\frac{1-p_{a}}{n\\,p_{a}}.\n$$\nIndependence across arms implies\n$$\n\\operatorname{Var}\\!\\left(\\ln\\!\\left(\\frac{\\widehat{p}_{1}}{\\widehat{p}_{0}}\\right)\\right) \\approx \\frac{1-p_{1}}{n\\,p_{1}} + \\frac{1-p_{0}}{n\\,p_{0}}.\n$$\nLet $\\theta = \\ln(r)$ denote the true log risk ratio. Then, for large $n$, the Wald statistic\n$$\nT = \\frac{\\widehat{\\theta} - 0}{\\sqrt{\\operatorname{Var}(\\widehat{\\theta})}}\n$$\nis approximately standard normal under $H_{0}$ and approximately normal with mean $\\mu = \\theta / \\sigma$ and unit variance under the alternative, where\n$$\n\\sigma = \\sqrt{\\frac{1-p_{1}}{n\\,p_{1}} + \\frac{1-p_{0}}{n\\,p_{0}}} = \\sqrt{\\frac{1}{n}\\left(\\frac{1-p_{1}}{p_{1}} + \\frac{1-p_{0}}{p_{0}}\\right)}.\n$$\nA two-sided level-$\\alpha$ Wald test rejects for $|T|  z_{1-\\alpha/2}$. Under the alternative with $\\mu  0$, the power is\n$$\n\\Pr(|T|  z_{1-\\alpha/2}\\,|\\,\\mu) = \\Pr(T  z_{1-\\alpha/2}\\,|\\,\\mu) + \\Pr(T  -z_{1-\\alpha/2}\\,|\\,\\mu).\n$$\nSince $\\mu  0$, the left-tail term is negligible relative to the right tail, and a standard sample size construction that ensures power at least $1-\\beta$ is obtained by requiring\n$$\n\\mu \\ge z_{1-\\alpha/2} + z_{1-\\beta}.\n$$\nSubstituting $\\mu = \\theta/\\sigma$ and solving for $n$ gives\n$$\n\\frac{\\ln(r)}{\\sqrt{\\frac{1}{n}\\left(\\frac{1-p_{1}}{p_{1}} + \\frac{1-p_{0}}{p_{0}}\\right)}} \\ge z_{1-\\alpha/2} + z_{1-\\beta}\n\\quad\\Longrightarrow\\quad\nn \\ge \\frac{\\left(z_{1-\\alpha/2} + z_{1-\\beta}\\right)^{2}\\left(\\frac{1-p_{1}}{p_{1}} + \\frac{1-p_{0}}{p_{0}}\\right)}{\\left(\\ln(r)\\right)^{2}}.\n$$\nPlugging in $p_{0} = 0.1$, $r = 1.5$ so $p_{1} = 0.15$, $\\alpha = 0.05$ so $z_{1-\\alpha/2} = z_{0.975} \\approx 1.9599639845$, and $1-\\beta = 0.9$ so $z_{1-\\beta} = z_{0.9} \\approx 1.2815515655$, we compute:\n$$\n\\frac{1-p_{1}}{p_{1}} + \\frac{1-p_{0}}{p_{0}} = \\frac{0.85}{0.15} + \\frac{0.9}{0.1} = \\frac{17}{3} + 9 = \\frac{44}{3} \\approx 14.6666666667,\n$$\n$$\n\\ln(r) = \\ln(1.5) \\approx 0.4054651081,\n$$\n$$\nz_{1-\\alpha/2} + z_{1-\\beta} \\approx 1.9599639845 + 1.2815515655 \\approx 3.2415155501,\n$$\n$$\n\\left(z_{1-\\alpha/2} + z_{1-\\beta}\\right)^{2} \\approx 10.507422646,\n\\qquad\n\\left(\\ln(r)\\right)^{2} \\approx 0.1644019539.\n$$\nThus\n$$\nn \\ge \\frac{10.507422646 \\times 14.6666666667}{0.1644019539} \\approx \\frac{154.1088653}{0.1644019539} \\approx 937.39.\n$$\nTo guarantee power at least $0.9$, we take the smallest integer $n$ satisfying the inequality, i.e., the ceiling:\n$$\nn = \\lceil 937.39 \\rceil = 938.\n$$\nTherefore, the required sample size per arm is $n = 938$.", "answer": "$$\\boxed{938}$$", "id": "4646891"}]}