{"hands_on_practices": [{"introduction": "Estimating the rate at which new cases of a disease appear in a population is a cornerstone of epidemiology. This exercise provides a foundational look at how we model this process using the Poisson distribution. You will derive the standard error of a rate estimate from first principles, giving you a concrete understanding of how we quantify the statistical uncertainty, or random error, surrounding this fundamental measure.", "problem": "A cohort study follows a stable population at risk for a rare, acute outcome. Over a fixed observation window that contributes a total of $T=500$ person-years, the study observes $x=10$ incident cases. Investigators posit that, under conditions of rare events, independence across disjoint time intervals, and a constant underlying hazard within the window, the event-count process can be approximated by a Poisson process with constant incidence rate $\\lambda$ (in cases per person-year). \n\nUsing only fundamental probabilistic properties of the Poisson process and standard definitions in epidemiology, do the following:\n- Provide a justification for modeling the count $X$ of incident cases as Poisson with mean $\\lambda T$ under the stated conditions, thereby motivating the estimator $\\hat{\\lambda}=X/T$ of the rate.\n- From first principles, derive $\\operatorname{Var}(\\hat{\\lambda})$ and the corresponding standard error $\\operatorname{SE}(\\hat{\\lambda})$ in terms of $\\lambda$ and $T$.\n- Then, using the plug-in principle that replaces the unknown parameter $\\lambda$ with its maximum likelihood estimate (MLE) $\\hat{\\lambda}$, compute the numerical values of $\\operatorname{Var}(\\hat{\\lambda})$ and $\\operatorname{SE}(\\hat{\\lambda})$ for the given data.\n\nUse cases per person-year as the unit for rates; the variance has the corresponding squared unit. Round your numerical answers for $\\operatorname{Var}(\\hat{\\lambda})$ and $\\operatorname{SE}(\\hat{\\lambda})$ to four significant figures.", "solution": "The problem is divided into three parts: first, a justification of the Poisson model and the rate estimator; second, a first-principles derivation of the estimator's variance and standard error; and third, a numerical calculation based on the provided data.\n\nPart 1: Justification of the Poisson Model and the Estimator $\\hat{\\lambda}$\n\nThe modeling of incident cases using a Poisson process rests on a set of fundamental assumptions about the nature of the event-generating process over the total exposure time, $T$. The total person-time, $T$, can be conceptualized as a long observation interval. We can partition this interval into a very large number, $n$, of small, disjoint subintervals of length $\\Delta t = \\frac{T}{n}$.\n\nThe problem states three key conditions:\n1.  Independence across disjoint time intervals: The occurrence of an event in one subinterval does not affect the probability of an event in any other subinterval.\n2.  Constant underlying hazard: The instantaneous risk of an event, the hazard rate $\\lambda$, is constant throughout the observation window.\n3.  Rare events: The outcome is rare, which implies that the probability of an event in a small subinterval $\\Delta t$ is small.\n\nUnder these conditions, for a sufficiently small $\\Delta t$, the probability of observing exactly one event in that subinterval is approximately proportional to its length, $p = P(\\text{1 event in } \\Delta t) \\approx \\lambda \\Delta t$. The probability of observing more than one event is negligible, i.e., $P(\\text{more than 1 event in } \\Delta t) = o(\\Delta t)$. The probability of observing zero events is therefore $P(\\text{0 events in } \\Delta t) \\approx 1 - \\lambda \\Delta t$.\n\nEach of the $n$ subintervals can be treated as an independent Bernoulli trial, where \"success\" is the occurrence of one case. The total number of cases, $X$, over the entire period $T$ is the sum of the outcomes of these $n$ trials. Therefore, $X$ follows a binomial distribution, $X \\sim \\text{Binomial}(n, p)$, where $n$ is the number of trials and $p \\approx \\lambda \\Delta t$ is the probability of success in each trial.\n\nThe expected number of cases is $E[X] = np = n(\\lambda \\Delta t) = n\\left(\\lambda \\frac{T}{n}\\right) = \\lambda T$.\n\nThe Poisson distribution arises as the limit of the binomial distribution as the number of trials $n$ goes to infinity ($n \\to \\infty$) and the probability of success $p$ goes to zero ($p \\to 0$) in such a way that their product, the mean $np = \\lambda T$, remains constant. In our epidemiological context, this corresponds to making the subintervals $\\Delta t$ infinitesimally small. This limiting process is known as the Poisson limit theorem.\n\nThus, the count of incident cases $X$ over a total person-time $T$ is well-approximated by a Poisson distribution with mean (and parameter) $\\mu = \\lambda T$. The probability mass function for observing $k$ cases is:\n$$\nP(X=k) = \\frac{(\\lambda T)^k e^{-\\lambda T}}{k!}\n$$\nThis justifies modeling the random variable $X$ as $X \\sim \\text{Poisson}(\\lambda T)$.\n\nTo motivate the estimator $\\hat{\\lambda} = \\frac{X}{T}$, we can use the method of maximum likelihood. The likelihood function for observing a particular count $x$ is the probability mass function evaluated at $x$, viewed as a function of the parameter $\\lambda$:\n$$\nL(\\lambda | x) = P(X=x) = \\frac{(\\lambda T)^x e^{-\\lambda T}}{x!}\n$$\nFor maximization, it is more convenient to work with the log-likelihood function, $\\ln L(\\lambda | x)$:\n$$\n\\ln L(\\lambda | x) = \\ln\\left(\\frac{(\\lambda T)^x e^{-\\lambda T}}{x!}\\right) = x \\ln(\\lambda T) - \\lambda T - \\ln(x!) = x \\ln(\\lambda) + x \\ln(T) - \\lambda T - \\ln(x!)\n$$\nTo find the value of $\\lambda$ that maximizes this function, we take the first derivative with respect to $\\lambda$ and set it to zero:\n$$\n\\frac{d}{d\\lambda} \\ln L(\\lambda | x) = \\frac{x}{\\lambda} - T = 0\n$$\nSolving for $\\lambda$ yields the maximum likelihood estimator (MLE), denoted $\\hat{\\lambda}$:\n$$\n\\hat{\\lambda} = \\frac{x}{T}\n$$\nReplacing the observed value $x$ with the random variable $X$ gives the estimator as a function of the data, $\\hat{\\lambda} = \\frac{X}{T}$. This estimator is intuitive, representing the observed frequency of events per unit of exposure time.\n\nPart 2: Derivation of $\\operatorname{Var}(\\hat{\\lambda})$ and $\\operatorname{SE}(\\hat{\\lambda})$\n\nThe estimator for the incidence rate is $\\hat{\\lambda} = \\frac{X}{T}$. To derive its variance, we use the properties of the variance operator. Since $T$ is a fixed, non-random quantity (total person-years), we have:\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\operatorname{Var}\\left(\\frac{X}{T}\\right)\n$$\nUsing the variance property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ for a constant $a$ and random variable $Y$, we get:\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\left(\\frac{1}{T}\\right)^2 \\operatorname{Var}(X) = \\frac{1}{T^2} \\operatorname{Var}(X)\n$$\nA fundamental property of the Poisson distribution is that its variance is equal to its mean. Since $X \\sim \\text{Poisson}(\\lambda T)$, its mean is $E[X] = \\lambda T$ and its variance is $\\operatorname{Var}(X) = \\lambda T$. Substituting this into the equation for $\\operatorname{Var}(\\hat{\\lambda})$:\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\frac{1}{T^2}(\\lambda T) = \\frac{\\lambda}{T}\n$$\nThe standard error of an estimator, $\\operatorname{SE}(\\hat{\\lambda})$, is defined as the square root of its variance:\n$$\n\\operatorname{SE}(\\hat{\\lambda}) = \\sqrt{\\operatorname{Var}(\\hat{\\lambda})} = \\sqrt{\\frac{\\lambda}{T}}\n$$\nThese expressions for the variance and standard error depend on the true, unknown rate $\\lambda$.\n\nPart 3: Numerical Computation\n\nThe problem provides the following data: total person-years $T = 500$ and observed number of cases $x = 10$.\nFirst, we compute the point estimate of the incidence rate $\\lambda$ using the MLE formula:\n$$\n\\hat{\\lambda} = \\frac{x}{T} = \\frac{10}{500} = \\frac{1}{50} = 0.02 \\text{ cases per person-year}\n$$\nTo compute a numerical value for the variance and standard error, we use the \"plug-in principle\" as instructed. This involves substituting the unknown parameter $\\lambda$ with its estimate $\\hat{\\lambda}$ in the derived formulas. This gives us the *estimated* variance and *estimated* standard error.\n\nThe estimated variance, denoted $\\widehat{\\operatorname{Var}}(\\hat{\\lambda})$, is:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\lambda}) = \\frac{\\hat{\\lambda}}{T} = \\frac{x/T}{T} = \\frac{x}{T^2}\n$$\nSubstituting the given numerical values:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\lambda}) = \\frac{10}{500^2} = \\frac{10}{250000} = \\frac{1}{25000} = 0.00004\n$$\nThe units are (cases per person-year)$^2$. Rounding to four significant figures, this is $0.00004000$.\n\nThe estimated standard error, denoted $\\widehat{\\operatorname{SE}}(\\hat{\\lambda})$, is the square root of the estimated variance:\n$$\n\\widehat{\\operatorname{SE}}(\\hat{\\lambda}) = \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\lambda})} = \\sqrt{\\frac{x}{T^2}} = \\frac{\\sqrt{x}}{T}\n$$\nSubstituting the numerical values:\n$$\n\\widehat{\\operatorname{SE}}(\\hat{\\lambda}) = \\frac{\\sqrt{10}}{500} \\approx \\frac{3.16227766}{500} \\approx 0.006324555\n$$\nRounding this result to four significant figures, we get $0.006325$. The units are cases per person-year.\n\nThe final numerical results for the estimated variance and standard error of $\\hat{\\lambda}$ are $0.00004000$ and $0.006325$, respectively.", "answer": "$$\n\\boxed{\\begin{pmatrix} 4.000 \\times 10^{-5}  6.325 \\times 10^{-3} \\end{pmatrix}}\n$$", "id": "4626594"}, {"introduction": "While we have many powerful statistical tools, a skilled epidemiologist knows their limitations. This practice presents a common challenge in research: analyzing data from small studies or involving rare outcomes, which often results in sparse data tables [@problem_id:4626605]. You will explore why standard \"large-sample\" tests can be misleading in these situations and learn to justify the use of an \"exact\" test to ensure your conclusions are statistically valid.", "problem": "A small prospective cohort study recruited two groups with fixed sizes: an exposed group of $n_{E}=5$ individuals and an unexposed group of $n_{U}=5$ individuals, for a total of $n=10$. After follow-up, the observed $2\\times 2$ table of disease status by exposure was:\n- Exposed: cases $0$, non-cases $5$.\n- Unexposed: cases $4$, non-cases $1$.\n\nInvestigators wish to test the null hypothesis of no association between exposure and disease. From first principles, argue whether asymptotic normal approximations are appropriate in this setting and, if not, identify an exact alternative and the rationale for its validity. Choose the single best option.\n\nA. Asymptotic normality of common estimators (for example, the difference in proportions or the log odds ratio) relies on conditions such as $n p (1-p)$ being sufficiently large in each group. Here, cell counts are very small and one cell is $0$, placing the estimator at a boundary where the sampling distribution is highly discrete and skewed, violating normal approximation assumptions. An exact test that conditions on the fixed margins—Fisher’s exact test—is appropriate because under the null and fixed margins the cell count follows a hypergeometric distribution, yielding exact tail probabilities and correct type I error control.\n\nB. Asymptotic normal approximations are acceptable as long as the total sample size satisfies $n\\ge 10$, so a large-sample $z$-test for the difference in proportions is preferred. Exact tests are unnecessarily conservative in this setting.\n\nC. Adding $0.5$ to all cells to avoid zeros, then using a Wald test for the log odds ratio, guarantees nominal coverage and valid type I error even with very small counts, so an exact alternative is not needed.\n\nD. Because the exposed group has $0$ cases, it contributes no information. A one-sample exact binomial test on the unexposed group’s case count alone (ignoring the exposed group) is valid and more powerful than conditioning on margins as in Fisher’s exact test.", "solution": "The correct choice is A. This problem illustrates a classic \"sparse data\" scenario where statistical tests based on large-sample (asymptotic) normal approximations are invalid.\n\n**Rationale for Rejecting Asymptotic Tests:**\nStandard tests like the Pearson chi-squared test or z-tests for proportions rely on the assumption that the sampling distribution of the test statistic is well-approximated by a continuous distribution (chi-squared or normal). This assumption holds only with sufficiently large sample sizes. A common rule of thumb is that all *expected* cell counts in a contingency table should be 5 or greater.\n\nUnder the null hypothesis of no association, the overall proportion of cases is $4/10 = 0.4$. The expected cell counts are:\n- Expected Cases, Exposed: $5 \\times 0.4 = 2$\n- Expected Cases, Unexposed: $5 \\times 0.4 = 2$\n\nSince these expected counts are less than 5, the asymptotic approximation is unreliable. Furthermore, the presence of a zero count in the table means the sample odds ratio is undefined, and the true sampling distribution of any effect measure is highly discrete and skewed, not normal.\n\n**Justification for Fisher's Exact Test (Option A):**\nFisher's exact test is specifically designed for these situations. It does not rely on any large-sample approximations. Instead, it calculates the exact probability of observing the data, and all more extreme arrangements, by conditioning on the row and column totals. Under the null hypothesis, the distribution of the cell counts follows the hypergeometric distribution. This method provides a valid p-value and ensures the Type I error rate is properly controlled, which is the primary concern with sparse data.\n\n**Analysis of Other Options:**\n- **Option B:** Is incorrect. A total sample size of 10 is far too small, and the distribution of counts within the cells is the critical factor, which this option ignores.\n- **Option C:** Is incorrect. Adding 0.5 to cells is a continuity correction that can help in some borderline cases, but it is still an approximation and does not \"guarantee\" validity. It is inferior to the exact test, which is the gold standard here.\n- **Option D:** Is incorrect. The observation of 0 cases in the exposed group is crucial information for comparison. Discarding this data is statistically and scientifically invalid and makes it impossible to test for an association.", "answer": "$$\\boxed{A}$$", "id": "4626605"}, {"introduction": "Random error can affect our interpretations in surprising ways, especially when we select individuals for a study based on extreme measurements. This exercise delves into the fascinating phenomenon of \"regression to the mean,\" a direct consequence of measurement variability that can be mistaken for a real effect [@problem_id:4626616]. By calculating the expected blood pressure at follow-up for a group selected for high initial readings, you will see firsthand why extreme values tend to be less extreme upon re-measurement.", "problem": "A hypertension screening program recruits individuals using a single screening measurement of systolic blood pressure (SBP). Let the observed screening SBP be denoted by $X$, the unobserved stable true SBP by $T$, and the follow-up SBP measured one month later by $Y$. Assume a classical measurement error model with $X = T + E$ and $Y = T + E'$, where $E$ and $E'$ are independent, mean-zero errors that are also independent of $T$. In the target population, the screening measurements $X$ are approximately normally distributed with mean $\\mu_{X} = 130$ mmHg and standard deviation $\\sigma_{X} = 20$ mmHg. The single-occasion reliability of SBP is $r = 0.6$, defined as $r = \\mathrm{Var}(T)/\\mathrm{Var}(X)$. The cohort is selected if the screening measurement satisfies $X \\ge c$ with $c = 160$ mmHg. Assume there is no systematic change in true SBP between screening and follow-up, so that the expected true SBP is stable over time in the absence of intervention.\n\nUsing only the core definitions of reliability and properties of conditional expectations for the normal distribution, derive an expression for $\\mathbb{E}[Y \\mid X \\ge c]$ in terms of $r$, $\\mu_{X}$, $\\sigma_{X}$, and $c$, and then compute its numerical value for the parameters given above. Your final numeric answer must be rounded to $4$ significant figures and expressed in mmHg.", "solution": "We begin from the classical measurement error model and the definition of reliability. Let $T$ denote the stable true systolic blood pressure (SBP) with mean $\\mu_{T}$ and variance $\\sigma_{T}^{2}$, and let $E$ and $E'$ be independent, mean-zero errors with variances $\\sigma_{E}^{2}$ and $\\sigma_{E'}^{2}$, respectively, independent of $T$. The screening and follow-up measurements are $X = T + E$ and $Y = T + E'$. The reliability of a single screening measurement is defined as\n$$\nr \\equiv \\frac{\\mathrm{Var}(T)}{\\mathrm{Var}(X)} = \\frac{\\sigma_{T}^{2}}{\\sigma_{T}^{2} + \\sigma_{E}^{2}}.\n$$\nBecause $E$ has mean zero and is independent of $T$, the screening mean satisfies $\\mu_{X} = \\mathbb{E}[X] = \\mathbb{E}[T] + \\mathbb{E}[E] = \\mu_{T}$, so $\\mu_{T} = \\mu_{X}$.\n\nNext, we derive $\\mathbb{E}[T \\mid X]$. Since $X = T + E$ with $T$ independent of $E$ and $(T, X)$ jointly normal, the conditional expectation is linear with slope equal to the regression coefficient\n$$\n\\beta_{TX} = \\frac{\\mathrm{Cov}(T, X)}{\\mathrm{Var}(X)} = \\frac{\\mathrm{Var}(T)}{\\mathrm{Var}(X)} = r,\n$$\nand intercept $\\mu_{T} - r \\mu_{X} = \\mu_{X} - r \\mu_{X} = (1 - r)\\mu_{X}$. Therefore,\n$$\n\\mathbb{E}[T \\mid X = x] = \\mu_{T} + r (x - \\mu_{X}) = \\mu_{X} + r (x - \\mu_{X}).\n$$\nBecause $Y = T + E'$ with $\\mathbb{E}[E' \\mid X] = 0$ and $E'$ independent of $X$, we have\n$$\n\\mathbb{E}[Y \\mid X] = \\mathbb{E}[T \\mid X] + \\mathbb{E}[E' \\mid X] = \\mu_{X} + r (X - \\mu_{X}).\n$$\nTaking the conditional expectation over the selected set $\\{X \\ge c\\}$ yields\n$$\n\\mathbb{E}[Y \\mid X \\ge c] = \\mathbb{E}\\big[\\mathbb{E}[Y \\mid X] \\,\\big|\\, X \\ge c\\big] = \\mu_{X} + r\\big(\\mathbb{E}[X \\mid X \\ge c] - \\mu_{X}\\big).\n$$\nThus, the problem reduces to computing $\\mathbb{E}[X \\mid X \\ge c]$ for a normally distributed $X \\sim \\mathcal{N}(\\mu_{X}, \\sigma_{X}^{2})$. Let $Z = \\frac{X - \\mu_{X}}{\\sigma_{X}} \\sim \\mathcal{N}(0, 1)$ and $\\alpha = \\frac{c - \\mu_{X}}{\\sigma_{X}}$. Using the definition of conditional expectation for a truncated normal,\n$$\n\\mathbb{E}[Z \\mid Z \\ge \\alpha] = \\frac{\\int_{\\alpha}^{\\infty} z \\,\\varphi(z) \\, dz}{\\int_{\\alpha}^{\\infty} \\varphi(z)\\, dz} = \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)},\n$$\nwhere $\\varphi$ and $\\Phi$ denote the standard normal probability density function (PDF) and cumulative distribution function (CDF), respectively, with $\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right)$. Therefore,\n$$\n\\mathbb{E}[X \\mid X \\ge c] = \\mu_{X} + \\sigma_{X} \\cdot \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)}, \\quad \\alpha = \\frac{c - \\mu_{X}}{\\sigma_{X}}.\n$$\nSubstituting this into the expression for $\\mathbb{E}[Y \\mid X \\ge c]$ gives\n$$\n\\mathbb{E}[Y \\mid X \\ge c] = \\mu_{X} + r\\left(\\mu_{X} + \\sigma_{X} \\cdot \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)} - \\mu_{X}\\right) = \\mu_{X} + r \\sigma_{X} \\cdot \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)}.\n$$\nNow plug in the given values $\\mu_{X} = 130$, $\\sigma_{X} = 20$, $c = 160$, and $r = 0.6$ (all numerical values in mmHg except $r$ which is unitless). First compute\n$$\n\\alpha = \\frac{c - \\mu_{X}}{\\sigma_{X}} = \\frac{160 - 130}{20} = 1.5.\n$$\nEvaluate the standard normal PDF and tail probability at $\\alpha = 1.5$:\n$$\n\\varphi(1.5) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{1.5^{2}}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-1.125) \\approx 0.1295175957,\n$$\n$$\n1 - \\Phi(1.5) \\approx 0.0668072013.\n$$\nHence,\n$$\n\\frac{\\varphi(1.5)}{1 - \\Phi(1.5)} \\approx \\frac{0.1295175957}{0.0668072013} \\approx 1.938537037.\n$$\nThen,\n$$\n\\mathbb{E}[X \\mid X \\ge 160] \\approx 130 + 20 \\times 1.938537037 \\approx 130 + 38.77074074 \\approx 168.7707407,\n$$\nand finally\n$$\n\\mathbb{E}[Y \\mid X \\ge 160] \\approx 130 + 0.6 \\times (168.7707407 - 130) = 130 + 0.6 \\times 38.7707407 \\approx 130 + 23.2624444 \\approx 153.2624444.\n$$\nRounded to $4$ significant figures and expressed in mmHg, the expected follow-up mean is $153.3$ mmHg.", "answer": "$$\\boxed{153.3}$$", "id": "4626616"}]}