{"hands_on_practices": [{"introduction": "Before engaging in complex statistical modeling, the first step in any Regression Discontinuity (RD) analysis is to visualize the data. A well-constructed RD plot serves as a powerful tool to assess the plausibility of a discontinuity at the cutoff and to check for potential violations of the design's core assumptions. This exercise [@problem_id:4629748] challenges you to identify the best practices for creating and interpreting these essential plots, helping you build the foundational skill of \"eyeballing\" the data for evidence of a causal effect and the validity of the RD strategy.", "problem": "An epidemiology team evaluates the causal effect of eligibility for a publicly funded influenza vaccination program on hospitalization within one year. Eligibility is assigned deterministically by an age cutoff at $c = 65$ years: individuals with running variable $X_i$ equal to exact age (in years, measured to the day) receive treatment if and only if $X_i \\ge c$. Let the observed outcome be $Y_i$, the treatment indicator be $D_i = \\mathbf{1}\\{X_i \\ge c\\}$, and the potential outcomes be $Y_i(1)$ and $Y_i(0)$. The regression discontinuity design relies on the continuity assumption that $\\mathbb{E}[Y_i(d)\\mid X_i = x]$ is continuous at $x = c$ for $d \\in \\{0,1\\}$, so that the target parameter is the jump at the cutoff,\n$$\n\\tau = \\lim_{x \\downarrow c} \\mathbb{E}[Y_i \\mid X_i = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y_i \\mid X_i = x].\n$$\nThe team wants to construct an informative regression discontinuity plot with binned scatter points and overlaid local polynomial fits on each side of $c$ to visually assess the design and the plausibility of assumptions.\n\nWhich of the following procedures and interpretations correctly describe how to construct such a plot and what visual features support or undermine the design’s validity?\n\nA. Create bins of the running variable $X_i$ that do not cross $c$; within each bin, compute the mean of $Y_i$ and plot that mean at the bin’s midpoint. Overlay separate local polynomial fits (e.g., local linear) on the left and right of $c$, estimated using only observations with $|X_i - c| \\le h$ on the corresponding side and kernel weights that downweight points farther from $c$, without forcing the curves to meet at $x = c$.\n\nB. Pool all observations on both sides of $c$ and fit a single high-order global polynomial of degree $k \\ge 4$ across the entire support of $X_i$; use the fitted curve to infer the jump at $c$ because higher-degree polynomials flexibly approximate the conditional mean function everywhere.\n\nC. Set bin widths so large that most bins span a wide range of $X_i$ (e.g., bin width much larger than the bandwidth $h$ used for the local polynomial), because larger bins reduce noise in the binned means and make the plotted curve smoother near $x = c$.\n\nD. Visually inspect whether the overlaid local fits are smooth away from $x = c$ and whether a discrete jump at $x = c$ is evident for $Y_i$, while companion plots show no visible discontinuities at $x = c$ for predetermined covariates or for the density of $X_i$; lack of bunching of $X_i$ near $c$ supports validity.\n\nE. To avoid selection bias right at the cutoff in the plot, drop all observations with $|X_i - c| \\le \\delta$ for some small $\\delta > 0$, so that the plotted discontinuity is not driven by idiosyncratic observations close to $c$.\n\nF. It is acceptable to use quantile-spaced bins with approximately equal counts per bin on each side; compute within-bin means of $Y_i$ and plot them against bin means or medians of $X_i$, ensuring bins do not straddle $c$. As a robustness check, repeat the plot for several reasonable bandwidths $h$ and polynomial degrees to assess whether the apparent jump and smoothness away from $c$ are stable.\n\nSelect all that apply.", "solution": "The problem statement describes a sharp regression discontinuity (RD) design, a quasi-experimental method for causal inference. The setup is scientifically valid and well-posed. The core of the RD design is to compare observations with a running variable $X_i$ just below a cutoff $c$ to those just above it. The key identifying assumption is the continuity of the conditional expectation of potential outcomes, $\\mathbb{E}[Y_i(d) \\mid X_i = x]$, at $x = c$. The goal is to evaluate procedures for constructing and interpreting an RD plot, which is a primary tool for visualizing the treatment effect and assessing the design's plausibility.\n\nLet us analyze each option based on established principles of regression discontinuity analysis.\n\n**A. Create bins of the running variable $X_i$ that do not cross $c$; within each bin, compute the mean of $Y_i$ and plot that mean at the bin’s midpoint. Overlay separate local polynomial fits (e.g., local linear) on the left and right of $c$, estimated using only observations with $|X_i - c| \\le h$ on the corresponding side and kernel weights that downweight points farther from $c$, without forcing the curves to meet at $x = c$.**\n\nThis option provides a precise and correct description of modern best practices for creating an RD plot.\n1.  **Binning**: The raw data is often too dense to visualize effectively. Binning the data into discrete intervals of the running variable $X_i$ and plotting the mean outcome $\\bar{Y}$ for each bin is a standard technique. It is critical that no bin straddles the cutoff $c$, as this would mix treated and untreated units and obscure the discontinuity. Plotting the bin mean at the bin's midpoint (or, more accurately, the mean of $X_i$ within the bin) is the correct procedure.\n2.  **Local Polynomial Regression**: The state-of-the-art method for estimating the regression functions on either side of the cutoff is local polynomial regression (most commonly, local linear regression, i.e., polynomial of degree $p=1$). This method is preferred over global polynomials because the RD estimand is a *local* parameter defined at the point $x=c$.\n3.  **Separate Fits**: The regression functions must be estimated separately for the data with $X_i < c$ and $X_i \\ge c$. Forcing the curves to be continuous at $c$ would assume the null hypothesis of no treatment effect ($\\tau=0$) and defeat the purpose of the analysis.\n4.  **Bandwidth and Kernel**: Local polynomial regression is performed on a subset of the data within a bandwidth $h$ of the cutoff ($c-h \\le X_i < c$ and $c \\le X_i \\le c+h$). Kernel weights are typically applied to give more influence to observations closer to the cutoff $c$, which is central to the local nature of the estimator.\n\nThis entire procedure is designed to provide a robust visual and quantitative estimate of the jump at the cutoff.\n\n**Verdict for A: Correct.**\n\n**B. Pool all observations on both sides of $c$ and fit a single high-order global polynomial of degree $k \\ge 4$ across the entire support of $X_i$; use the fitted curve to infer the jump at $c$ because higher-degree polynomials flexibly approximate the conditional mean function everywhere.**\n\nThis describes an outdated and now strongly discouraged method. While it is theoretically possible to model the discontinuity using a global polynomial with a treatment indicator (e.g., $Y_i = P_k(X_i) + \\tau D_i + \\epsilon_i$), this approach has been shown to be highly unreliable.\n1.  **Instability**: High-order polynomials ($k \\ge 2$, and especially $k \\ge 4$) are notoriously prone to producing artificial \"wiggles\" and having poor behavior near the boundaries of the data support. The estimated jump $\\hat{\\tau}$ can be extremely sensitive to the choice of the polynomial degree $k$.\n2.  **Local vs. Global**: The RD effect is fundamentally a local phenomenon at the cutoff $c$. A global polynomial uses distant observations to inform the shape of the function at the cutoff, which can induce significant bias. Local polynomial methods, as described in option A, are much more robust and better aligned with the local nature of the estimand. Leading methodologists in the field have strongly advised against using global high-order polynomials in RD analysis.\n\n**Verdict for B: Incorrect.**\n\n**C. Set bin widths so large that most bins span a wide range of $X_i$ (e.g., bin width much larger than the bandwidth $h$ used for the local polynomial), because larger bins reduce noise in the binned means and make the plotted curve smoother near $x = c$.**\n\nThis procedure is ill-advised. While it is true that larger bins reduce the variance of the binned means (less \"noise\"), they do so at the cost of introducing substantial bias. An RD plot is meant to visualize the local behavior of the conditional mean function around the cutoff. Using very large bin widths averages the outcome $Y_i$ over a wide range of $X_i$ values. If the underlying function $\\mathbb{E}[Y_i \\mid X_i = x]$ is not flat, this averaging will produce a binned mean that does not accurately represent the function's value at the bin's center. This is especially problematic near the cutoff, where large bins can smear out and hide the very discontinuity we wish to observe. The bins are a visual aid; their resolution should be fine enough to track the local polynomial fit, not so coarse as to contradict it. Using bins much wider than the estimation bandwidth $h$ is conceptually inconsistent.\n\n**Verdict for C: Incorrect.**\n\n**D. Visually inspect whether the overlaid local fits are smooth away from $x = c$ and whether a discrete jump at $x = c$ is evident for $Y_i$, while companion plots show no visible discontinuities at $x = c$ for predetermined covariates or for the density of $X_i$; lack of bunching of $X_i$ near $c$ supports validity.**\n\nThis option correctly describes the interpretation of the RD plot and essential companion diagnostic tests.\n1.  **Outcome Plot Inspection**: The primary plot of $Y_i$ versus $X_i$ should show a clear jump at $c$ if there is a treatment effect, and the functions on either side should be reasonably smooth, without other jumps or strange patterns.\n2.  **Covariate Balance Test**: A crucial validity check is to create RD plots for predetermined covariates (variables whose values are set before the treatment decision, e.g., gender, baseline health status). The continuity assumption implies that the distribution of potential outcomes is smooth across the threshold. By extension, the distribution of any predetermined characteristic should also be smooth. A jump in a covariate at $c$ would suggest that the populations just above and below the cutoff are different in ways other than treatment assignment, which undermines the causal interpretation.\n3.  **Density Test (Manipulation Check)**: Another critical test is to examine the density of the running variable $X_i$. If individuals can precisely manipulate their value of $X_i$ to get or avoid treatment, we might see a non-random sorting pattern, such as a drop in density just below $c$ and a spike in density just above $c$. This would violate the assumption that agents cannot perfectly control the running variable. A smooth density through the cutoff supports the validity of the design. \"Lack of bunching\" is synonymous with a smooth density.\n\n**Verdict for D: Correct.**\n\n**E. To avoid selection bias right at the cutoff in the plot, drop all observations with $|X_i - c| \\le \\delta$ for some small $\\delta > 0$, so that the plotted discontinuity is not driven by idiosyncratic observations close to $c$.**\n\nThis describes a \"donut hole\" RD. This is not a standard or recommended primary analysis strategy. The entire logic of RD relies on the comparability of individuals *just* on either side of the cutoff. Discarding these most informative observations throws away the data that is at the heart of the identification strategy. This approach fundamentally alters the estimand and can introduce its own biases if the regression functions are not flat. While it might be used as a supplementary sensitivity analysis in rare cases with specific concerns about data quality precisely at the cutoff, it is incorrect to frame it as a standard method to \"avoid selection bias.\" The standard RD framework already accounts for selection into treatment based on $X_i$ via the continuity assumption.\n\n**Verdict for E: Incorrect.**\n\n**F. It is acceptable to use quantile-spaced bins with approximately equal counts per bin on each side; compute within-bin means of $Y_i$ and plot them against bin means or medians of $X_i$, ensuring bins do not straddle $c$. As a robustness check, repeat the plot for several reasonable bandwidths $h$ and polynomial degrees to assess whether the apparent jump and smoothness away from $c$ are stable.**\n\nThis option describes other valid and important procedures in RD analysis.\n1.  **Quantile-Spaced Bins**: Instead of evenly-spaced bins (which may result in some bins having very few observations if the data is sparse), it is a common and acceptable practice to create bins containing an equal number of observations. This helps to stabilize the variance of the binned means across the support of $X_i$. As with any binning, one must plot the bin's outcome mean against the bin's mean of $X_i$ and ensure no bin crosses the cutoff $c$.\n2.  **Robustness Checks**: A credible RD analysis must demonstrate that the main findings are not overly dependent on arbitrary modeling choices. Therefore, it is essential to check the robustness of the results by re-estimating the effect using different bandwidths ($h$) and different local polynomial degrees (e.g., comparing local linear, $p=1$, to local quadratic, $p=2$). If the estimated jump and the overall shape of the plot remain stable across these variations, it increases confidence in the findings.\n\n**Verdict for F: Correct.**", "answer": "$$\\boxed{ADF}$$", "id": "4629748"}, {"introduction": "The central idea of RDD is to estimate a treatment effect that is *local* to the cutoff. It might be tempting to apply a standard smoothing technique, like LOESS, across the entire dataset to visualize the underlying trend, but doing so violates this local principle and can lead to severely misleading conclusions. This hands-on coding exercise [@problem_id:3168530] provides a powerful demonstration of this pitfall by asking you to build a scenario where a global smoother completely masks a true treatment effect, cementing your understanding of why methods must be tailored to the local nature of the RDD.", "problem": "You are given a running variable $X$, an outcome $Y$, and a cutoff $c$. Consider the Regression Discontinuity (RD) setup in which the treatment indicator is $T = \\mathbf{1}\\{X \\ge c\\}$ and the outcome $Y$ follows a structural model $Y = g(X) + \\tau T + \\varepsilon$, where $g$ is a smooth baseline function, $\\tau$ is the target RD jump, and $\\varepsilon$ is noise. The RD estimand is defined by the difference of conditional expectations at the cutoff,\n$$\n\\tau = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x].\n$$\nYou will construct a synthetic example where applying Locally Weighted Scatterplot Smoothing (LOESS) across $X$ hides the discontinuity at $c$, and you will quantify the danger of global smoothers for RD.\n\nDefine the data generating process as follows:\n- The running variable is $X \\sim \\text{Uniform}([-1, 1])$ independently for each observation.\n- The baseline function is $g(x) = x^2$.\n- The cutoff is $c = 0$.\n- The true RD jump is $\\tau = 1$.\n- The noise is $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ independently.\n\nDefine the LOESS predictor $\\hat{f}(x)$ as the solution of a weighted local linear regression at a target point $x_0$ using a fraction $s$ of nearest neighbors. For each $x_0$:\n1. Compute distances $d_i = |x_i - x_0|$ for all observed $x_i$.\n2. Let $k = \\lceil s n \\rceil$, where $n$ is the sample size, and let $d_{\\max}$ be the $k$-th smallest $d_i$.\n3. Define the tricube weights $w_i = \\left(1 - \\left(\\frac{d_i}{d_{\\max}}\\right)^3\\right)^3$ for $d_i \\le d_{\\max}$ and $w_i = 0$ otherwise.\n4. Fit the weighted local linear model $y_i = \\beta_0 + \\beta_1 x_i$ by minimizing $\\sum_i w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$, and set $\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$.\n\nSince LOESS fits a single smooth curve to data on both sides of the cutoff, the fitted function $\\hat{f}(x)$ is continuous in $x$. Consequently, the “visible” discontinuity measured as\n$$\n\\hat{J}_{\\text{LOESS}} = \\hat{f}(c + \\epsilon) - \\hat{f}(c - \\epsilon)\n$$\nfor a small $\\epsilon > 0$ is expected to be approximately $0$, regardless of the true jump $\\tau$, thereby hiding the discontinuity.\n\nAs a principled RD estimator, define the local linear RD estimator by fitting two separate ordinary least squares (OLS) regressions on either side of the cutoff within bandwidth $h$:\n- Left regression uses data with $x \\in [c - h, c)$ and fits $y = \\alpha_- + \\beta_- x$, yielding $\\hat{m}_-(c) = \\hat{\\alpha}_- + \\hat{\\beta}_- c$.\n- Right regression uses data with $x \\in [c, c + h]$ and fits $y = \\alpha_+ + \\beta_+ x$, yielding $\\hat{m}_+(c) = \\hat{\\alpha}_+ + \\hat{\\beta}_+ c$.\n- The RD estimate is $\\hat{\\tau} = \\hat{m}_+(c) - \\hat{m}_-(c)$.\n\nYour program must:\n1. Generate data for each test case using the specified parameters.\n2. Compute $\\hat{J}_{\\text{LOESS}}$ using the LOESS procedure described above with span $s$ and small offset $\\epsilon$.\n3. Compute the local linear RD estimate $\\hat{\\tau}$ using the bandwidth $h$.\n4. Compute the absolute biases $B_L = |\\hat{J}_{\\text{LOESS}} - \\tau|$ and $B_R = |\\hat{\\tau} - \\tau|$.\n5. For each test case, output the ratio $R = \\frac{B_L}{B_R}$.\n\nInterpretation: When $R$ is larger than $1$, the LOESS-based visible jump is more biased than the local linear RD estimate, illustrating the danger of global smoothers that enforce continuity across the cutoff and can hide true discontinuities.\n\nTest suite (each tuple is $(n, \\sigma, s, h, \\epsilon, \\text{seed})$):\n- Case $1$: $(500, 0.1, 0.9, 0.2, 0.001, 17)$.\n- Case $2$: $(500, 1.0, 0.9, 0.2, 0.001, 23)$.\n- Case $3$: $(300, 0.1, 0.5, 0.15, 0.001, 31)$.\n- Case $4$: $(300, 0.1, 0.2, 0.1, 0.001, 47)$.\n\nFinal output format: Your program should produce a single line of output containing the ratios for all test cases as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$. No other output is permitted.", "solution": "The user-provided problem is a well-defined simulation exercise in the field of statistical learning and econometrics, specifically concerning Regression Discontinuity (RD) designs. The problem is scientifically sound, self-contained, and algorithmically specified. It asks for a comparison between two estimators of a discontinuity: one based on a global smoother (LOESS) that is known to perform poorly in this context, and another based on a principled local linear regression approach tailored for RD analysis.\n\nThe problem statement requires us to validate this theoretical behavior through a simulation. We will follow the specified steps for several test cases: data generation, estimation with both methods, and calculation of the ratio of their absolute biases.\n\n### Step 1: Data Generation\nFor each test case, we are given a sample size $n$, a noise standard deviation $\\sigma$, and a random seed. We generate the data according to the specified Data Generating Process (DGP):\n- The running variable $X$ is drawn from a uniform distribution on $[-1, 1]$.\n- The treatment assignment $T$ is deterministic based on $X$ and the cutoff $c = 0$, with $T = \\mathbf{1}\\{X \\ge c\\}$.\n- The outcome $Y$ is generated by the structural model $Y = g(X) + \\tau T + \\varepsilon$, where the baseline is $g(x) = x^2$, the true treatment effect (jump) is $\\tau = 1$, and the noise term $\\varepsilon$ is drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$.\n\n### Step 2: Calculation of the LOESS-based Visible Jump $\\hat{J}_{\\text{LOESS}}$\nThe problem defines a \"visible\" jump based on Locally Weighted Scatterplot Smoothing (LOESS). LOESS fits a single smooth function $\\hat{f}(x)$ over the entire range of $X$, ignoring the discontinuity at the cutoff $c$. The procedure for calculating the LOESS prediction $\\hat{f}(x_0)$ at a target point $x_0$ is as follows:\n1.  For a given span parameter $s$, determine the number of nearest neighbors to use: $k = \\lceil sn \\rceil$.\n2.  Identify the $k$ data points $(x_i, y_i)$ closest to $x_0$ based on the distance $|x_i - x_0|$. Let the distance to the furthest of these neighbors be $d_{\\max}$.\n3.  Assign a tricube weight to each data point $i$: $w_i = \\left(1 - \\left(\\frac{|x_i - x_0|}{d_{\\max}}\\right)^3\\right)^3$ for points within the neighborhood ($|x_i - x_0| \\le d_{\\max}$) and $w_i = 0$ for all other points.\n4.  Fit a weighted local linear regression model, $y_i = \\beta_0 + \\beta_1 x_i + e_i$, by minimizing the weighted sum of squared residuals $\\sum_i w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$.\n5.  The LOESS prediction at $x_0$ is $\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$.\n\nThe \"visible jump\" is then calculated as the difference in the LOESS predictions at two points straddling the cutoff: $\\hat{J}_{\\text{LOESS}} = \\hat{f}(c + \\epsilon) - \\hat{f}(c - \\epsilon)$ for a small $\\epsilon>0$. Since $\\hat{f}(x)$ is a continuous function, this quantity is expected to be close to $0$.\n\n### Step 3: Calculation of the Local Linear RD Estimate $\\hat{\\tau}$\nThis is the standard, principled approach for RD estimation. It acknowledges the discontinuity by fitting separate models on either side of the cutoff $c$:\n1.  **Left of cutoff:** Using data where $x \\in [c-h, c)$, we fit an Ordinary Least Squares (OLS) regression $y = \\alpha_- + \\beta_- x$. The predicted value at the cutoff from the left is $\\hat{m}_-(c) = \\hat{\\alpha}_- + \\hat{\\beta}_- c$.\n2.  **Right of cutoff:** Using data where $x \\in [c, c+h]$, we fit another OLS regression $y = \\alpha_+ + \\beta_+ x$. The predicted value at the cutoff from the right is $\\hat{m}_+(c) = \\hat{\\alpha}_+ + \\hat{\\beta}_+ c$.\n3.  The RD estimate is the difference between these two predicted values: $\\hat{\\tau} = \\hat{m}_+(c) - \\hat{m}_-(c)$. Since $c=0$, this simplifies to $\\hat{\\tau} = \\hat{\\alpha}_+ - \\hat{\\alpha}_-$, the difference in the intercepts of the two local regressions.\n\n### Step 4: Bias and Ratio Calculation\nWith the true jump $\\tau=1$, we compute the absolute bias for both estimators:\n- LOESS bias: $B_L = |\\hat{J}_{\\text{LOESS}} - \\tau|$.\n- RD estimator bias: $B_R = |\\hat{\\tau} - \\tau|$.\n\nThe final output for each test case is the ratio of these biases, $R = B_L / B_R$. A large value of $R$ demonstrates that the LOESS-based method is significantly more biased and effectively hides the true discontinuity, confirming the \"danger of global smoothers\" in an RD context.\nThe entire process is implemented in the provided Python code, which iterates through the specified test cases and computes the ratio $R$ for each.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Regression Discontinuity problem by comparing a LOESS-based approach\n    with a standard local linear RD estimator.\n    \"\"\"\n\n    def _loess_predict(x0, X, Y, s):\n        \"\"\"\n        Computes the LOESS prediction for a single point x0 based on the provided\n        data (X, Y) and span s.\n        \"\"\"\n        n = len(X)\n        k = int(np.ceil(s * n))\n\n        # Clamp k to be within valid range [1, n] for array indexing.\n        if k > n: k = n\n        if k < 1: k = 1\n\n        # 1. Compute distances and find the k-th neighbor's distance (d_max).\n        distances = np.abs(X - x0)\n        sorted_distances = np.sort(distances)\n        d_max = sorted_distances[k-1]\n        \n        # 2. Define tricube weights.\n        weights = np.zeros(n)\n        if d_max > 0:\n            u = distances / d_max\n            mask = u <= 1.0\n            weights[mask] = (1 - u[mask]**3)**3\n        else:  # d_max is 0, only points at x0 get weight 1.\n            weights = (distances == 0).astype(float)\n\n        # 3. Fit weighted local linear model.\n        valid_mask = weights > 0\n        \n        # Fallback for insufficient data points within the window.\n        if np.sum(valid_mask) == 0:\n            return Y[np.argmin(distances)] # Return value of the single closest point.\n        \n        X_w = X[valid_mask]\n        Y_w = Y[valid_mask]\n        weights_w = weights[valid_mask]\n        \n        # If not enough unique X values, fall back to local constant model (weighted mean).\n        if len(np.unique(X_w)) < 2:\n            return np.average(Y_w, weights=weights_w)\n            \n        X_des = np.vstack([np.ones_like(X_w), X_w]).T\n        \n        # WLS can be solved by transforming to an OLS problem.\n        W_sqrt = np.sqrt(weights_w)\n        X_prime = X_des * W_sqrt[:, np.newaxis]\n        Y_prime = Y_w * W_sqrt\n        \n        # np.linalg.lstsq is robust and handles rank-deficient cases.\n        beta_hat = np.linalg.lstsq(X_prime, Y_prime, rcond=None)[0]\n        prediction = beta_hat[0] + beta_hat[1] * x0\n\n        return prediction\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, sigma, s, h, epsilon, seed)\n        (500, 0.1, 0.9, 0.2, 0.001, 17),\n        (500, 1.0, 0.9, 0.2, 0.001, 23),\n        (300, 0.1, 0.5, 0.15, 0.001, 31),\n        (300, 0.1, 0.2, 0.1, 0.001, 47),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, sigma, s, h, epsilon, seed = case\n        \n        # --- Parameter setup ---\n        c = 0.0\n        tau_true = 1.0\n        rng = np.random.default_rng(seed)\n\n        # --- Data Generation ---\n        X = rng.uniform(-1, 1, n)\n        T = (X >= c).astype(int)\n        g_X = X**2\n        noise = rng.normal(0, sigma, n)\n        Y = g_X + tau_true * T + noise\n        \n        # --- LOESS Calculation ---\n        f_hat_plus = _loess_predict(c + epsilon, X, Y, s)\n        f_hat_minus = _loess_predict(c - epsilon, X, Y, s)\n        J_loess = f_hat_plus - f_hat_minus\n\n        # --- Local Linear RD Estimator ---\n        # Left regression\n        mask_left = (X >= c - h) & (X < c)\n        X_left, Y_left = X[mask_left], Y[mask_left]\n        if len(X_left) < 2: raise ValueError(\"Not enough data points left of cutoff.\")\n        X_des_left = np.vstack([np.ones_like(X_left), X_left]).T\n        alpha_minus, beta_minus = np.linalg.lstsq(X_des_left, Y_left, rcond=None)[0]\n        m_hat_minus = alpha_minus + beta_minus * c\n\n        # Right regression\n        mask_right = (X >= c) & (X <= c + h)\n        X_right, Y_right = X[mask_right], Y[mask_right]\n        if len(X_right) < 2: raise ValueError(\"Not enough data points right of cutoff.\")\n        X_des_right = np.vstack([np.ones_like(X_right), X_right]).T\n        alpha_plus, beta_plus = np.linalg.lstsq(X_des_right, Y_right, rcond=None)[0]\n        m_hat_plus = alpha_plus + beta_plus * c\n        \n        tau_hat = m_hat_plus - m_hat_minus\n        \n        # --- Bias and Ratio Calculation ---\n        B_L = np.abs(J_loess - tau_true)\n        B_R = np.abs(tau_hat - tau_true)\n        \n        if B_R == 0.0:\n            R = 1.0 if B_L == 0.0 else np.inf\n        else:\n            R = B_L / B_R\n            \n        results.append(R)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3168530"}, {"introduction": "After visually inspecting the data and understanding the importance of local estimation, the next step is to implement a robust estimator for the treatment effect. This practice [@problem_id:3168528] guides you through coding the workhorse of modern RDD analysis: the local linear estimator. Furthermore, it introduces a crucial step in applied research—assessing the robustness of your findings by including a baseline covariate in the model, which helps ensure that the estimated effect is not an artifact of confounding.", "problem": "Consider a sharp Regression Discontinuity Design (RDD) in a healthcare triage scenario where intensive care is assigned by a threshold rule. Let the running variable be the clinical risk score $X \\in \\mathbb{R}$, let the cutoff be $c \\in \\mathbb{R}$, and define the treatment indicator by $T = \\mathbb{1}\\{X \\geq c\\}$. The outcome $Y \\in \\{0,1\\}$ denotes mortality (with $Y=1$ indicating death), and the risk adjustment covariate $Z \\in \\mathbb{R}$ summarizes baseline risk. The causal estimand of interest is the local average treatment effect at the cutoff, namely the jump in the conditional expectation of $Y$ at $X=c$. Your task is to implement a local linear estimator using kernel-weighted least squares to estimate the discontinuity at the cutoff, both without and with risk adjustment by $Z$, and then assess robustness of the estimated effect to this adjustment.\n\nFundamental base for the design is as follows:\n- Let $Y(1)$ and $Y(0)$ denote the potential outcomes under treatment and control, respectively, and assume the Stable Unit Treatment Value Assumption (SUTVA). Under the sharp assignment mechanism $T=\\mathbb{1}\\{X \\ge c\\}$ and the continuity assumption that the conditional expectations $\\mathbb{E}[Y(0)\\mid X=x]$ and $\\mathbb{E}[Y(1)\\mid X=x]$ are continuous at $x=c$, the local average treatment effect at the cutoff equals the discontinuity in $\\mathbb{E}[Y\\mid X=x]$ at $x=c$.\n- A local linear estimator approximates the conditional expectations on either side of $c$ by linear functions in the centered running variable $R = X - c$, fitted within a bandwidth $h>0$ using kernel weights that downweight points farther from the cutoff.\n\nImplementation requirements:\n- Within a bandwidth $h$, use the triangular kernel weights $K(u) = (1 - |u|)\\mathbb{1}\\{|u|\\le 1\\}$ with $u = R/h$, so the point-wise weight is $w_i = (1 - |R_i|/h)\\mathbb{1}\\{|R_i|\\le h\\}$.\n- Fit the following local linear specification by weighted least squares on observations with $|R|\\le h$:\n  1. Unadjusted: regress $Y$ on the columns $(1, T, R, T\\cdot R)$ and take the coefficient on $T$ as the estimated discontinuity.\n  2. Risk-adjusted: regress $Y$ on $(1, T, R, T\\cdot R, Z)$ and again take the coefficient on $T$ as the estimated discontinuity.\n- For each test case, report three quantities: the unadjusted estimate (a float), the risk-adjusted estimate (a float), and a robustness indicator (a boolean) that equals true if the absolute difference between the adjusted and unadjusted estimates is less than or equal to a tolerance $\\delta>0$, and false otherwise.\n\nTest suite and inputs:\nFor each test case, you are given arrays for $X$, $Y$, $Z$, the cutoff $c$, the bandwidth $h$, and the robustness tolerance $\\delta$. The arrays are:\n\nTest case $1$ (happy path, moderate bandwidth):\n- $X = [\\,45,\\,47,\\,49,\\,50,\\,51,\\,53,\\,55,\\,46,\\,48,\\,52,\\,54,\\,44,\\,56,\\,57,\\,43,\\,58\\,]$\n- $Y = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- $Z = [\\,0.6,\\,0.5,\\,0.55,\\,0.58,\\,0.62,\\,0.65,\\,0.7,\\,0.52,\\,0.57,\\,0.63,\\,0.66,\\,0.48,\\,0.72,\\,0.74,\\,0.46,\\,0.76\\,]$\n- $c = 50$, $h = 5$, $\\delta = 0.05$.\n\nTest case $2$ (boundary case, small bandwidth):\n- $X = [\\,45,\\,47,\\,49,\\,50,\\,51,\\,53,\\,55,\\,46,\\,48,\\,52,\\,54,\\,44,\\,56,\\,57,\\,43,\\,58\\,]$\n- $Y = [\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- $Z = [\\,0.6,\\,0.5,\\,0.55,\\,0.58,\\,0.62,\\,0.65,\\,0.7,\\,0.52,\\,0.57,\\,0.63,\\,0.66,\\,0.48,\\,0.72,\\,0.74,\\,0.46,\\,0.76\\,]$\n- $c = 50$, $h = 2$, $\\delta = 0.10$.\n\nTest case $3$ (edge case, strong covariate influence):\n- $X = [\\,48,\\,49,\\,50,\\,51,\\,52,\\,53,\\,47,\\,46,\\,54,\\,55,\\,45,\\,56\\,]$\n- $Y = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1,\\,0\\,]$\n- $Z = [\\,0.9,\\,0.85,\\,0.88,\\,0.86,\\,0.83,\\,0.8,\\,0.92,\\,0.95,\\,0.78,\\,0.75,\\,0.97,\\,0.74\\,]$\n- $c = 50$, $h = 6$, $\\delta = 0.08$.\n\nAlgorithmic specifications:\n- Construct the centered running variable $R = X - c$.\n- Compute weights $w_i = (1 - |R_i|/h)\\mathbb{1}\\{|R_i|\\le h\\}$ and restrict to observations with $w_i>0$.\n- Form the design matrix $D$ as described above for the unadjusted and adjusted models.\n- Compute the weighted least squares estimator using the Moore–Penrose pseudoinverse to handle near-singular matrices: estimate $\\hat{\\beta} = (D^{\\top} W D)^{+} D^{\\top} W Y$, where $W$ is the diagonal matrix of weights and $(\\cdot)^{+}$ denotes the pseudoinverse. Extract the coefficient on $T$ as the estimated discontinuity.\n\nFinal output format:\nYour program should produce a single line of output containing the concatenated results for all three test cases as a comma-separated list enclosed in square brackets. The order must be, for test case $1$ then test case $2$ then test case $3$: $[\\text{unadjusted}_1,\\text{adjusted}_1,\\text{robust}_1,\\text{unadjusted}_2,\\text{adjusted}_2,\\text{robust}_2,\\text{unadjusted}_3,\\text{adjusted}_3,\\text{robust}_3]$. The booleans must be printed as either true or false in the programming language’s native boolean representation, and the floats must be printed in decimal form. No physical units or angle units are involved. Percentages, if any, must be expressed as decimals.", "solution": "The problem is assessed to be **valid**. It is scientifically grounded in the principles of causal inference and statistical learning, specifically the Regression Discontinuity Design (RDD). The problem is well-posed, providing all necessary data, parameters, and a clear, mathematically sound algorithm for estimation. The language is objective and precise, and the setup is self-contained and consistent.\n\nThe task is to estimate the Local Average Treatment Effect (LATE) at the cutoff of a sharp RDD using a local linear estimator. This is performed under two specifications: an unadjusted model and a model adjusted for a risk covariate $Z$. The robustness of the estimate to this adjustment is then assessed. The estimation procedure is based on kernel-weighted least squares.\n\nThe step-by-step procedure is as follows:\n\n1.  **Data Filtering and Preparation**: For each observation $i$, we first compute the centered running variable $R_i = X_i - c$, where $X_i$ is the clinical risk score and $c$ is the cutoff. The analysis is restricted to a local neighborhood around the cutoff, defined by a bandwidth $h > 0$. The problem specifies using a triangular kernel, with weights given by $w_i = (1 - |R_i|/h)\\mathbb{1}\\{|R_i|\\le h\\}$. The instruction to \"restrict to observations with $w_i > 0$\" implies that only observations for which $|R_i| < h$ are included in the regression. For these selected observations, we define the treatment indicator $T_i = \\mathbb{1}\\{X_i \\geq c\\}$, which is $1$ if the observation is treated and $0$ otherwise.\n\n2.  **Unadjusted Local Linear RDD Estimation**: The first model estimates the treatment effect without adjusting for the covariate $Z$. We fit a weighted least squares (WLS) regression within the bandwidth. The model specification is:\n    $$\n    Y_i = \\beta_0 + \\tau T_i + \\beta_1 R_i + \\beta_2 (T_i \\cdot R_i) + \\epsilon_i\n    $$\n    This model fits separate linear trends on each side of the cutoff. The parameter $\\tau$ represents the jump in the intercept at the cutoff $c$ (where $R_i=0$), which is the RDD estimate of the LATE. The regressors are an intercept ($1$), the treatment indicator ($T$), the centered running variable ($R$), and an interaction term ($T \\cdot R$). The design matrix for the unadjusted model is $D_{\\text{unadj}} = [1, T, R, T \\cdot R]$.\n\n3.  **Risk-Adjusted Local Linear RDD Estimation**: The second model includes the baseline risk covariate $Z$ as an additional linear control. This is done to assess whether the treatment effect estimate is sensitive to the inclusion of other prognostic factors. The model specification is:\n    $$\n    Y_i = \\beta_0 + \\tau_{adj} T_i + \\beta_1 R_i + \\beta_2 (T_i \\cdot R_i) + \\beta_3 Z_i + \\nu_i\n    $$\n    The parameter of interest is again the coefficient on the treatment indicator, $\\tau_{adj}$. The inclusion of $Z$ can improve the precision of the estimate and test for robustness. The design matrix for the adjusted model is $D_{\\text{adj}} = [1, T, R, T \\cdot R, Z]$.\n\n4.  **Weighted Least Squares (WLS) Computation**: Both models are estimated using WLS. The vector of coefficients, $\\hat{\\beta}$, is calculated using the formula specified in the problem statement, which involves the Moore-Penrose pseudoinverse $(\\cdot)^{+}$ to ensure stability, particularly in cases with collinearity or few observations:\n    $$\n    \\hat{\\beta} = (D^{\\top} W D)^{+} D^{\\top} W Y\n    $$\n    Here, $Y$ is the vector of outcomes for the filtered observations, $D$ is the corresponding design matrix (either $D_{\\text{unadj}}$ or $D_{\\text{adj}}$), and $W$ is a diagonal matrix with the triangular kernel weights $w_i$ on its diagonal. The estimated treatment effect ($\\hat{\\tau}$ or $\\hat{\\tau}_{adj}$) is the second element of the resulting $\\hat{\\beta}$ vector, corresponding to the coefficient of the treatment indicator $T$.\n\n5.  **Robustness Assessment**: The final step is to compare the unadjusted estimate, $\\hat{\\tau}_{\\text{unadj}}$, with the risk-adjusted estimate, $\\hat{\\tau}_{\\text{adj}}$. The estimate is considered robust if the absolute difference between the two is within a given tolerance $\\delta$:\n    $$\n    \\text{robust} = (|\\hat{\\tau}_{\\text{adj}} - \\hat{\\tau}_{\\text{unadj}}| \\le \\delta)\n    $$\n    This provides a boolean indicator of the stability of the finding. For each test case, the final output will be the triplet $(\\hat{\\tau}_{\\text{unadj}}, \\hat{\\tau}_{\\text{adj}}, \\text{robust})$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_rdd_estimate(X: np.ndarray, Y: np.ndarray, Z: np.ndarray, c: float, h: float, adjusted: bool) -> float:\n    \"\"\"\n    Calculates the RDD estimate using local linear regression via WLS.\n    \"\"\"\n    # 1. Center the running variable\n    R = X - c\n    \n    # 2. Filter data to be strictly within the bandwidth (where weight w_i > 0)\n    mask = np.abs(R) < h\n    \n    # If no data points are within the bandwidth, return NaN.\n    if not np.any(mask):\n        return np.nan\n\n    R_filt = R[mask]\n    Y_filt = Y[mask]\n    X_filt = X[mask]\n    Z_filt = Z[mask]\n\n    # 3. Compute triangular kernel weights\n    weights = 1 - np.abs(R_filt) / h\n    W = np.diag(weights)\n\n    # 4. Construct the design matrix D\n    intercept = np.ones_like(R_filt)\n    T_filt = (X_filt >= c).astype(float)\n    TR_interaction = T_filt * R_filt\n\n    if adjusted:\n        # Columns: (1, T, R, T*R, Z)\n        D = np.stack([intercept, T_filt, R_filt, TR_interaction, Z_filt], axis=1)\n    else:\n        # Columns: (1, T, R, T*R)\n        D = np.stack([intercept, T_filt, R_filt, TR_interaction], axis=1)\n\n    # Check for underdetermined system (fewer obs than params)\n    # The use of pinv is specified to handle this case.\n    n_obs, n_params = D.shape\n    if n_obs == 0:\n        return np.nan\n\n    # 5. Compute the WLS estimator using the Moore-Penrose pseudoinverse\n    try:\n        D_T_W = D.T @ W\n        # The core formula: beta = (D'WD)^+ D'WY\n        pinv_term = np.linalg.pinv(D_T_W @ D)\n        beta_hat = pinv_term @ D_T_W @ Y_filt\n    except np.linalg.LinAlgError:\n        # This branch is unlikely with pinv but included for robustness\n        return np.nan\n\n    # 6. The RDD estimate is the coefficient on the treatment indicator T, which is the second regressor (index 1).\n    rdd_estimate = beta_hat[1]\n    \n    return rdd_estimate\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([45, 47, 49, 50, 51, 53, 55, 46, 48, 52, 54, 44, 56, 57, 43, 58]),\n            \"Y\": np.array([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]),\n            \"Z\": np.array([0.6, 0.5, 0.55, 0.58, 0.62, 0.65, 0.7, 0.52, 0.57, 0.63, 0.66, 0.48, 0.72, 0.74, 0.46, 0.76]),\n            \"c\": 50.0,\n            \"h\": 5.0,\n            \"delta\": 0.05\n        },\n        {\n            \"X\": np.array([45, 47, 49, 50, 51, 53, 55, 46, 48, 52, 54, 44, 56, 57, 43, 58]),\n            \"Y\": np.array([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0]),\n            \"Z\": np.array([0.6, 0.5, 0.55, 0.58, 0.62, 0.65, 0.7, 0.52, 0.57, 0.63, 0.66, 0.48, 0.72, 0.74, 0.46, 0.76]),\n            \"c\": 50.0,\n            \"h\": 2.0,\n            \"delta\": 0.10\n        },\n        {\n            \"X\": np.array([48, 49, 50, 51, 52, 53, 47, 46, 54, 55, 45, 56]),\n            \"Y\": np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]),\n            \"Z\": np.array([0.9, 0.85, 0.88, 0.86, 0.83, 0.8, 0.92, 0.95, 0.78, 0.75, 0.97, 0.74]),\n            \"c\": 50.0,\n            \"h\": 6.0,\n            \"delta\": 0.08\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X, Y, Z, c, h, delta = case[\"X\"], case[\"Y\"], case[\"Z\"], case[\"c\"], case[\"h\"], case[\"delta\"]\n\n        # Calculate unadjusted estimate\n        unadjusted_est = _calculate_rdd_estimate(X, Y, Z, c, h, adjusted=False)\n        \n        # Calculate risk-adjusted estimate\n        adjusted_est = _calculate_rdd_estimate(X, Y, Z, c, h, adjusted=True)\n        \n        # Assess robustness\n        is_robust = np.abs(adjusted_est - unadjusted_est) <= delta\n        \n        results.extend([unadjusted_est, adjusted_est, is_robust])\n\n    # Final print statement in the exact required format.\n    # str() converts booleans to 'True'/'False' and floats to decimal strings.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3168528"}]}