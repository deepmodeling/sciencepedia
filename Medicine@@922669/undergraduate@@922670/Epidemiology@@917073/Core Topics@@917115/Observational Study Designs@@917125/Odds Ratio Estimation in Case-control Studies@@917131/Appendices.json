{"hands_on_practices": [{"introduction": "The $2 \\times 2$ contingency table is the foundational tool for analyzing case-control studies. While calculating a point estimate for the odds ratio ($OR$) is a simple first step, a full analysis requires us to quantify the uncertainty around this estimate and formally test for an association. This practice will guide you through the complete workflow, from calculating the $OR$ and its logarithm to constructing a confidence interval and performing a Wald hypothesis test, skills that are essential for interpreting and reporting epidemiological findings [@problem_id:4616344]. You will also learn a standard technique for handling the common practical issue of zero cell counts.", "problem": "A case-control study evaluates the association between a binary exposure and a binary disease outcome. Let a $2 \\times 2$ table of counts be formed as follows: the first row corresponds to individuals with the disease (cases) with $a$ exposed and $b$ unexposed, and the second row corresponds to individuals without the disease (controls) with $c$ exposed and $d$ unexposed. The odds of exposure among cases is defined as $a/b$, and the odds of exposure among controls is defined as $c/d$. The odds ratio is defined as the ratio of these two odds. Assume independent sampling within cases and controls.\n\nStarting from these core definitions, derive a procedure to:\n- Estimate the odds ratio, its natural logarithm, and the standard error of the natural logarithm of the estimated odds ratio.\n- Construct a two-sided $(1-\\alpha)$ Confidence Interval (CI) for the true odds ratio using a large-sample normal approximation, where $\\alpha = 0.05$ is the Type I error rate expressed as a decimal.\n- Perform a two-sided Wald hypothesis test of the null hypothesis that the true odds ratio equals $1$.\n- Apply the Haldane–Anscombe correction (add $0.5$ to all four cell counts) when any of $a$, $b$, $c$, or $d$ equals $0$ to avoid undefined quantities.\n\nYour program must implement the above derivation and compute, for each test case, a list containing the following seven elements in order:\n1. The estimated odds ratio $\\widehat{\\mathrm{OR}}$ rounded to six decimal places.\n2. The estimated natural logarithm $\\log(\\widehat{\\mathrm{OR}})$ rounded to six decimal places.\n3. The estimated standard error of $\\log(\\widehat{\\mathrm{OR}})$ rounded to six decimal places.\n4. The lower bound of the $(1-\\alpha)$ CI for the odds ratio, rounded to six decimal places.\n5. The upper bound of the $(1-\\alpha)$ CI for the odds ratio, rounded to six decimal places.\n6. The two-sided Wald test p-value for testing that the true odds ratio equals $1$, rounded to six decimal places.\n7. A boolean indicating whether to reject the null hypothesis at significance level $\\alpha$ (use $True$ to indicate rejection and $False$ otherwise).\n\nTest Suite:\nUse the following six test cases, each specified by $(a,b,c,d)$ in that order:\n- Case 1 (general moderate counts): $(40,60,20,80)$.\n- Case 2 (boundary condition with a zero cell): $(0,50,30,70)$.\n- Case 3 (near-null association): $(30,70,28,72)$.\n- Case 4 (association below one): $(10,90,30,70)$.\n- Case 5 (boundary condition with a zero control exposure cell): $(5,95,0,100)$.\n- Case 6 (large-sample stability): $(1000,1500,1100,1400)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a comma-separated list enclosed in square brackets in the same order as specified above. Do not include any spaces in the output. For example, the output format should look like:\n$[\\,[x_{11},x_{12},\\dots,x_{17}],[x_{21},x_{22},\\dots,x_{27}],\\dots\\,]$,\nwhere $x_{ij}$ are the computed values for case $i$ and element $j$.", "solution": "The problem is valid as it presents a well-defined, scientifically grounded, and objective task based on standard principles of biostatistics and epidemiology. It is self-contained, with all necessary data and methods specified. No inconsistencies, ambiguities, or unsound premises are present. We may therefore proceed with the derivation and solution.\n\nThe problem requires the analysis of data from a case-control study presented in a $2 \\times 2$ contingency table with the following cell counts:\n- $a$: Exposed cases (disease present, exposure present)\n- $b$: Unexposed cases (disease present, exposure absent)\n- $c$: Exposed controls (disease absent, exposure present)\n- $d$: Unexposed controls (disease absent, exposure absent)\n\nThe total number of cases is $n_1 = a+b$ and the total number of controls is $n_2 = c+d$. The sampling is performed such that a number of cases and a number of controls are selected, and their exposure history is then ascertained.\n\n**1. Estimation of the Odds Ratio ($\\mathrm{OR}$)**\n\nThe odds of exposure among the cases is estimated as the ratio of exposed to unexposed cases, $\\mathrm{Odds}_{\\text{cases}} = a/b$.\nThe odds of exposure among the controls is estimated as the ratio of exposed to unexposed controls, $\\mathrm{Odds}_{\\text{controls}} = c/d$.\n\nThe sample odds ratio, denoted as $\\widehat{\\mathrm{OR}}$, is the ratio of these two odds. It serves as an estimator for the true population odds ratio, $\\mathrm{OR}$.\n\n$$\n\\widehat{\\mathrm{OR}} = \\frac{a/b}{c/d} = \\frac{ad}{bc}\n$$\n\nThis is the maximum likelihood estimator of the odds ratio. A value of $\\widehat{\\mathrm{OR}} > 1$ suggests a positive association between the exposure and the disease, a value of $\\widehat{\\mathrm{OR}} < 1$ suggests a negative (protective) association, and a value near $1$ suggests no association.\n\nA practical issue arises if any of the cell counts $a, b, c, d$ are zero. If $b=0$ or $c=0$, the estimator is undefined. If $a=0$ or $d=0$ (and $b, c \\neq 0$), the estimator becomes $0$ or infinite. To circumvent this, the Haldane–Anscombe correction is applied. As specified, if any cell count is $0$, we add $0.5$ to all four cells. Let the (possibly corrected) counts be $a', b', c', d'$. The corrected estimator is:\n\n$$\n\\widehat{\\mathrm{OR}} = \\frac{a'd'}{b'c'}\n$$\nwhere $a' = a+0.5$, $b' = b+0.5$, $c' = c+0.5$, and $d' = d+0.5$ if any original count is $0$; otherwise $a'=a$, $b'=b$, $c'=c$, $d'=d$.\n\n**2. The Log Odds Ratio and its Standard Error**\n\nFor statistical inference, it is highly advantageous to work with the natural logarithm of the odds ratio, $\\log(\\widehat{\\mathrm{OR}})$. The sampling distribution of $\\log(\\widehat{\\mathrm{OR}})$ is more closely approximated by a normal distribution than that of $\\widehat{\\mathrm{OR}}$, especially in finite samples.\n\nThe estimated log odds ratio is:\n$$\n\\log(\\widehat{\\mathrm{OR}}) = \\log\\left(\\frac{a'd'}{b'c'}\\right) = \\log(a') + \\log(d') - \\log(b') - \\log(c')\n$$\n\nThe variance of the log odds ratio estimator can be approximated using the delta method. The large-sample approximation for the variance of $\\log(\\widehat{\\mathrm{OR}})$ is given by the sum of the reciprocals of the cell counts:\n$$\n\\mathrm{Var}(\\log(\\widehat{\\mathrm{OR}})) \\approx \\frac{1}{a'} + \\frac{1}{b'} + \\frac{1}{c'} + \\frac{1}{d'}\n$$\n\nThe standard error of the log odds ratio, $\\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})]$, is the square root of this estimated variance:\n$$\n\\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})] = \\sqrt{\\frac{1}{a'} + \\frac{1}{b'} + \\frac{1}{c'} + \\frac{1}{d'}}\n$$\n\n**3. Confidence Interval for the Odds Ratio**\n\nLeveraging the normal approximation for the log odds ratio, a two-sided $(1-\\alpha)$ confidence interval (CI) for the true log odds ratio, $\\log(\\mathrm{OR})$, is constructed as:\n$$\n\\log(\\widehat{\\mathrm{OR}}) \\pm z_{1-\\alpha/2} \\cdot \\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})]\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For a significance level of $\\alpha = 0.05$, we require the $1 - 0.05/2 = 0.975$ quantile, for which $z_{0.975} \\approx 1.95996$.\n\nLet the lower and upper bounds of this CI be $L = \\log(\\widehat{\\mathrm{OR}}) - z_{0.975} \\cdot \\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})]$ and $U = \\log(\\widehat{\\mathrm{OR}}) + z_{0.975} \\cdot \\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})]$.\n\nTo obtain the confidence interval for the odds ratio $\\mathrm{OR}$ itself, we exponentiate these bounds:\n$$\n\\text{CI for OR} = [\\exp(L), \\exp(U)]\n$$\n\n**4. Wald Hypothesis Test**\n\nWe wish to test the null hypothesis $H_0$ that there is no association between exposure and disease, which corresponds to a true odds ratio of $1$. The alternative hypothesis $H_1$ is that there is an association.\n- $H_0: \\mathrm{OR} = 1$\n- $H_1: \\mathrm{OR} \\neq 1$\n\nOn the log scale, this is equivalent to:\n- $H_0: \\log(\\mathrm{OR}) = 0$\n- $H_1: \\log(\\mathrm{OR}) \\neq 0$\n\nThe Wald test statistic, $Z$, is constructed by dividing the estimated log odds ratio by its standard error. This measures how many standard errors the estimate is away from the null value of $0$.\n$$\nZ = \\frac{\\log(\\widehat{\\mathrm{OR}}) - 0}{\\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})]} = \\frac{\\log(\\widehat{\\mathrm{OR}})}{\\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})]}\n$$\nUnder $H_0$, this statistic $Z$ follows approximately a standard normal distribution, $\\mathcal{N}(0, 1)$.\n\nThe two-sided p-value is the probability of observing a test statistic at least as extreme as the one observed, assuming $H_0$ is true. It is calculated as:\n$$ p = P(|Z_{obs}| \\ge |z|) = 2 \\times P(Z_{obs} \\ge |z|) = 2 \\times (1 - \\Phi(|z|)) $$\nwhere $z$ is the observed value of the test statistic $Z$, and $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\nThe null hypothesis $H_0$ is rejected at the significance level $\\alpha$ if the calculated p-value is less than or equal to $\\alpha$. For this problem, we reject $H_0$ if $p \\le 0.05$.\n\nThe procedure will be applied to each test case, yielding the seven required outputs:\n1. $\\widehat{\\mathrm{OR}}$\n2. $\\log(\\widehat{\\mathrm{OR}})$\n3. $\\widehat{\\mathrm{SE}}[\\log(\\widehat{\\mathrm{OR}})]$\n4. Lower bound of the $95\\%$ CI for $\\mathrm{OR}$\n5. Upper bound of the $95\\%$ CI for $\\mathrm{OR}$\n6. The two-sided Wald test p-value\n7. A boolean indicating rejection of $H_0$ at $\\alpha=0.05$.\nAll numerical results will be rounded to six decimal places.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output string.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, b, c, d)\n        (40, 60, 20, 80),    # Case 1: general moderate counts\n        (0, 50, 30, 70),     # Case 2: boundary condition with a zero cell\n        (30, 70, 28, 72),    # Case 3: near-null association\n        (10, 90, 30, 70),    # Case 4: association below one\n        (5, 95, 0, 100),     # Case 5: boundary condition with a zero control exposure cell\n        (1000, 1500, 1100, 1400), # Case 6: large-sample stability\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _calculate_or_statistics(*case)\n        results.append(result)\n\n    # Format the final output string as per the problem specification.\n    # e.g., [[val1,val2,...],[val1,val2,...]] with no spaces.\n    # The str() function for a boolean correctly produces 'True' or 'False'.\n    output_str = f\"[{','.join([f'[{\",\".join(map(str, r))}]' for r in results])}]\"\n    print(output_str)\n\ndef _calculate_or_statistics(a, b, c, d):\n    \"\"\"\n    Calculates statistics related to the odds ratio for a 2x2 table.\n\n    Args:\n        a (int): Exposed cases\n        b (int): Unexposed cases\n        c (int): Exposed controls\n        d (int): Unexposed controls\n\n    Returns:\n        list: A list containing 7 elements:\n              1. Estimated odds ratio\n              2. Natural log of the estimated odds ratio\n              3. Standard error of the log odds ratio\n              4. Lower bound of the 95% CI for the odds ratio\n              5. Upper bound of the 95% CI for the odds ratio\n              6. Wald test p-value\n              7. Boolean for rejection of the null hypothesis at alpha=0.05\n    \"\"\"\n    alpha = 0.05\n    a_calc, b_calc, c_calc, d_calc = float(a), float(b), float(c), float(d)\n\n    # Apply Haldane–Anscombe correction if any cell is zero.\n    if a == 0 or b == 0 or c == 0 or d == 0:\n        a_calc += 0.5\n        b_calc += 0.5\n        c_calc += 0.5\n        d_calc += 0.5\n\n    # 1. Estimate the odds ratio (OR_hat)\n    # Check for division by zero in the denominator, even after correction (though unlikely)\n    if b_calc == 0 or c_calc == 0:\n        # This case is highly pathological and shouldn't occur with the correction\n        # in place for non-negative integer inputs. We handle it defensively.\n        return [None] * 7\n        \n    or_hat = (a_calc * d_calc) / (b_calc * c_calc)\n\n    # 2. Estimate the natural logarithm of the odds ratio\n    log_or_hat = np.log(or_hat)\n\n    # 3. Estimate the standard error of the log odds ratio\n    se_log_or_hat = np.sqrt(1/a_calc + 1/b_calc + 1/c_calc + 1/d_calc)\n\n    # 4. & 5. Construct a (1-alpha) CI for the true odds ratio\n    z_alpha_2 = norm.ppf(1 - alpha / 2)\n    log_ci_lower = log_or_hat - z_alpha_2 * se_log_or_hat\n    log_ci_upper = log_or_hat + z_alpha_2 * se_log_or_hat\n    \n    ci_or_lower = np.exp(log_ci_lower)\n    ci_or_upper = np.exp(log_ci_upper)\n\n    # 6. Perform a two-sided Wald hypothesis test (H0: OR = 1, i.e., log(OR) = 0)\n    wald_z_stat = log_or_hat / se_log_or_hat\n    p_value = 2 * norm.sf(np.abs(wald_z_stat)) # sf = 1 - cdf, more accurate for tails\n\n    # 7. Determine whether to reject the null hypothesis\n    reject_h0 = p_value <= alpha\n\n    # Round numerical results to six decimal places\n    return [\n        round(or_hat, 6),\n        round(log_or_hat, 6),\n        round(se_log_or_hat, 6),\n        round(ci_or_lower, 6),\n        round(ci_or_upper, 6),\n        round(p_value, 6),\n        reject_h0\n    ]\n\n# Execute the main function\nsolve()\n```", "id": "4616344"}, {"introduction": "In practice, epidemiological data can be sparse, meaning some categories may have zero participants. When a zero appears in a $2 \\times 2$ table, the standard Maximum Likelihood Estimate (MLE) of the odds ratio can break down, yielding an uninformative result of zero or infinity due to a phenomenon called \"separation.\" This exercise demonstrates this very real-world problem and introduces a powerful solution: Penalized Maximum Likelihood (PML) estimation, which produces a stable and finite estimate even when the MLE fails [@problem_id:4616359]. By contrasting these two methods, you will gain a deeper appreciation for robust statistical techniques.", "problem": "Consider a case-control study with a single binary exposure. Let $a$ denote the number of exposed cases, $b$ the number of exposed controls, $c$ the number of unexposed cases, and $d$ the number of unexposed controls, so that the $2\\times 2$ table is assembled by counts $(a,b,c,d)$ across the two strata (case versus control) and the two exposure levels (exposed versus unexposed). The practical estimation target is the odds ratio, defined as the odds of exposure among cases divided by the odds of exposure among controls.\n\nStarting from core definitions and well-tested facts in epidemiology and statistics:\n- The odds of exposure among cases is the ratio of exposed cases to unexposed cases.\n- The odds of exposure among controls is the ratio of exposed controls to unexposed controls.\n- Maximum Likelihood Estimation (MLE) for logistic regression is obtained by maximizing the likelihood with respect to the regression parameters, and in a case-control design with a single binary exposure the slope parameter corresponds to the logarithm of the odds ratio.\n- In sparse data, especially when any cell count is zero, standard MLE for logistic regression can fail due to separation, meaning that the likelihood is monotone in the slope parameter and the MLE does not attain a finite value.\n- Penalized Maximum Likelihood (PML) with the Jeffreys prior corresponds to adding a penalty term of one half times the logarithm of the determinant of the Fisher Information Matrix (FIM) to the log-likelihood. For the $2\\times 2$ case-control problem with a single binary exposure, this penalty yields finite bias-reduced estimates even under separation.\n\nYour task is to implement a program that, for each test case $(a,b,c,d)$:\n1. Computes the MLE of the odds ratio using only the fundamental definitions of odds among cases and controls described above. If the calculation implies divergence because of separation, represent the MLE odds ratio as $+\\infty$ when the numerator grows without bound relative to the denominator, and as $0$ when the numerator collapses to zero while the denominator remains nonzero. If both numerator and denominator collapse simultaneously, return a not-a-number value.\n2. Detects separation logically from the counts and then computes a finite penalized estimate of the odds ratio under the Jeffreys-prior-based penalized likelihood for the single-predictor logistic model, derived from first principles. The penalized estimate must be expressed as a strictly positive real number and must remain finite even when any of $a,b,c,d$ equals zero.\n3. Returns, for each test case, a list of two real numbers $[\\text{MLE odds ratio}, \\text{penalized odds ratio}]$.\n\nTest Suite:\n- Test Case 1 (general non-sparse case): $(a,b,c,d)=(30,70,20,80)$.\n- Test Case 2 (complete separation, no exposed controls): $(a,b,c,d)=(10,0,25,65)$.\n- Test Case 3 (complete separation, no exposed cases): $(a,b,c,d)=(0,40,15,45)$.\n- Test Case 4 (balanced minimal counts, all cells positive): $(a,b,c,d)=(1,1,1,1)$.\n- Test Case 5 (complete separation, no unexposed cases): $(a,b,c,d)=(12,18,0,20)$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case represented as a sublist $[\\text{MLE},\\text{PML}]$ and no whitespace characters. For example:\n\"[ [mle_1,pml_1],[mle_2,pml_2],...[mle_5,pml_5] ]\" without spaces becomes \"[[mle_1,pml_1],[mle_2,pml_2],...[mle_5,pml_5]]\".\n- Each numeric value must be a real number. Use the special floating representations \"inf\" for $+\\infty$ and \"nan\" for not-a-number if applicable.", "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in the principles of epidemiological statistics, specifically concerning case-control study analysis. The problem is well-posed, with all necessary data and definitions provided for a unique and meaningful solution. The language is objective and formal.\n\nThe task is to compute two estimates for the odds ratio ($OR$) from a $2 \\times 2$ contingency table with counts ($a, b, c, d$): the Maximum Likelihood Estimate (MLE) and a Penalized Maximum Likelihood (PML) estimate based on the Jeffreys prior.\n\nLet the counts be defined as:\n- $a$: Number of exposed cases\n- $b$: Number of exposed controls\n- $c$: Number of unexposed cases\n- $d$: Number of unexposed controls\n\nThe analysis proceeds in two parts.\n\n**1. Maximum Likelihood Estimation of the Odds Ratio ($\\widehat{OR}_{MLE}$)**\n\nThe odds ratio is defined as the ratio of the odds of exposure among cases to the odds of exposure among controls.\n\nThe odds of exposure among cases, $O_{cases}$, is given by the ratio of exposed cases to unexposed cases:\n$$ O_{cases} = \\frac{a}{c} $$\n\nThe odds of exposure among controls, $O_{controls}$, is given by the ratio of exposed controls to unexposed controls:\n$$ O_{controls} = \\frac{b}{d} $$\n\nThe odds ratio, $OR$, is the ratio of these two odds:\n$$ OR = \\frac{O_{cases}}{O_{controls}} = \\frac{a/c}{b/d} $$\n\nThe standard Maximum Likelihood Estimator for the odds ratio, denoted $\\widehat{OR}_{MLE}$, is the sample or empirical odds ratio, which is computed directly from the counts:\n$$ \\widehat{OR}_{MLE} = \\frac{ad}{bc} $$\n\nThis estimator is subject to issues when one or more of the cell counts are zero, a condition known as data separation in the context of the corresponding logistic model. The likelihood function fails to converge to a finite maximum, leading to infinite or undefined estimates for the $OR$. Following the problem's specifications, we handle these cases as follows:\n\n-   **Finite, Non-zero Estimate:** If all counts ($a, b, c, d$) are positive, the estimate is finite and calculated as $\\frac{ad}{bc}$.\n-   **Infinite Estimate ($\\infty$):** If the numerator product $ad > 0$ while the denominator product $bc = 0$ (i.e., either $b=0$ or $c=0$ but not both, and both $a$ and $d$ are non-zero), the ratio diverges to infinity. This situation corresponds to complete separation where the exposure is perfectly associated with the case status in the sample. For example, if $b=0$, no controls are exposed.\n-   **Zero Estimate ($0$):** If the numerator product $ad = 0$ (i.e., either $a=0$ or $d=0$ but not both) while the denominator product $bc > 0$, the estimate is $0$. This also corresponds to complete separation. For example, if $a=0$, no cases are exposed.\n-   **Indeterminate Estimate (Not-a-Number, NaN):** If both the numerator and denominator products are zero ($ad=0$ and $bc=0$), the expression becomes an indeterminate form $\\frac{0}{0}$. This occurs, for instance, if a row and a column are simultaneously zero, such as $a=0$ and $c=0$ (no cases) or $a=0$ and $b=0$ (no exposed subjects). In these scenarios, the odds ratio is not meaningfully estimable from the data.\n\n**2. Penalized Maximum Likelihood Estimation of the Odds Ratio ($\\widetilde{OR}_{PML}$)**\n\nTo address the problem of separation, a penalty term can be added to the log-likelihood function. The problem specifies using a penalty derived from the Jeffreys prior for the parameters of the corresponding logistic model. For a single binary predictor in a logistic regression, the use of a Jeffreys prior penalty (Firth's method) is equivalent to performing MLE after augmenting the data. For the $2 \\times 2$ table generated from a case-control design, this simplifies to a remarkably elegant solution: one adds a value of $0.5$ to each cell count before computing the odds ratio.\n\nThis can be understood by viewing the case-control data as two independent binomial samples:\n-   Among $N_{cases} = a+c$ cases, $a$ are exposed.\n-   Among $N_{controls} = b+d$ controls, $b$ are exposed.\n\nThe Jeffreys prior for a binomial proportion is a $\\text{Beta}(0.5, 0.5)$ distribution. The posterior mean, which serves as a penalized estimate, is obtained using counts of $(\\text{successes} + 0.5)$ and $(\\text{failures} + 0.5)$. Applying this logic to our two groups:\n- The penalized odds for cases become $\\frac{a+0.5}{c+0.5}$.\n- The penalized odds for controls become $\\frac{b+0.5}{d+0.5}$.\n\nThus, the penalized (or bias-corrected) odds ratio estimate, $\\widetilde{OR}_{PML}$, is:\n$$ \\widetilde{OR}_{PML} = \\frac{(a+0.5)/(c+0.5)}{(b+0.5)/(d+0.5)} = \\frac{(a+0.5)(d+0.5)}{(b+0.5)(c+0.5)} $$\n\nThis estimator has the significant advantage of always producing a finite, strictly positive real number provided that the total sample size is non-zero, thereby providing a stable estimate even in cases of complete separation where the MLE fails.\n\n**Implementation Plan:**\n\nA function will be implemented to take a tuple of counts ($a, b, c, d$).\n- It will first compute the numerator $ad$ and denominator $bc$ for the MLE. It will use conditional logic to return `np.inf`, `0.0`, `np.nan`, or the calculated ratio `(a*d)/(b*c)`.\n- Second, it will compute the PML estimate using the formula $\\frac{(a+0.5)(d+0.5)}{(b+0.5)(c+0.5)}$.\n- The function will return a list containing the two computed values. The main program will iterate through the provided test cases and format the results into the required string format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating odds ratios for a series of case-control studies.\n\n    For each test case (a, b, c, d), it calculates:\n    1. The Maximum Likelihood Estimate (MLE) of the odds ratio, handling separation\n       (zero counts) by returning inf, 0, or nan as appropriate.\n    2. The Penalized Maximum Likelihood (PML) estimate using the Jeffreys prior\n       method, which is equivalent to adding 0.5 to each cell count.\n\n    The final output is a string representation of a list of lists,\n    formatted exactly as specified in the problem description.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (30, 70, 20, 80),  # Test Case 1 (general non-sparse case)\n        (10, 0, 25, 65),   # Test Case 2 (complete separation, no exposed controls)\n        (0, 40, 15, 45),   # Test Case 3 (complete separation, no exposed cases)\n        (1, 1, 1, 1),      # Test Case 4 (balanced minimal counts, all cells positive)\n        (12, 18, 0, 20),   # Test Case 5 (complete separation, no unexposed cases)\n    ]\n\n    def calculate_ors(a, b, c, d):\n        \"\"\"\n        Calculates MLE and PML odds ratios for a single 2x2 table.\n\n        Args:\n            a (int): Number of exposed cases.\n            b (int): Number of exposed controls.\n            c (int): Number of unexposed cases.\n            d (int): Number of unexposed controls.\n\n        Returns:\n            list: A list containing two floats [mle_or, pml_or].\n                  mle_or can be inf or nan in cases of separation.\n        \"\"\"\n        # 1. Maximum Likelihood Estimation (MLE)\n        mle_or = 0.0\n        numerator_mle = float(a * d)\n        denominator_mle = float(b * c)\n        \n        if denominator_mle == 0.0:\n            if numerator_mle == 0.0:\n                # Indeterminate form 0/0\n                mle_or = np.nan\n            else:\n                # Diverges to infinity\n                mle_or = np.inf\n        else: # denominator_mle is not zero\n            mle_or = numerator_mle / denominator_mle\n        \n        # 2. Penalized Maximum Likelihood (PML) Estimation\n        # This is equivalent to adding 0.5 to each cell (Gart's adjustment)\n        numerator_pml = (a + 0.5) * (d + 0.5)\n        denominator_pml = (b + 0.5) * (c + 0.5)\n        pml_or = numerator_pml / denominator_pml\n        \n        return [mle_or, pml_or]\n\n    results = []\n    for case in test_cases:\n        a, b, c, d = case\n        result = calculate_ors(a, b, c, d)\n        results.append(result)\n\n    # Format the results into the exact required string format:\n    # \"[[mle_1,pml_1],[mle_2,pml_2],...]\" with no whitespace.\n    result_strings = []\n    for mle, pml in results:\n        # np.nan and np.inf string representations are 'nan' and 'inf'\n        mle_str = str(mle)\n        pml_str = str(pml)\n        result_strings.append(f\"[{mle_str},{pml_str}]\")\n    \n    final_output_string = f\"[{','.join(result_strings)}]\"\n\n    print(final_output_string)\n\nsolve()\n```", "id": "4616359"}, {"introduction": "A primary challenge in epidemiology is disentangling the effect of an exposure from that of other factors, known as confounders. Stratification is a classic and intuitive method for controlling confounding, where the analysis is performed separately within levels (strata) of the confounding variable. This practice introduces the Mantel-Haenszel (MH) method, a cornerstone of epidemiology used to combine stratum-specific results into a single, adjusted odds ratio, providing a more accurate measure of the true association [@problem_id:4808937]. This hands-on calculation will not only build your analytical skills but also reinforce your understanding of why the odds ratio is the measure of choice in case-control studies.", "problem": "A medical researcher conducts a stratified case-control study to evaluate the association between a binary exposure $E$ and a binary disease outcome $D$, controlling for a potential confounder by stratification across $3$ levels (e.g., age groups or hospital sites). Within each stratum $k \\in \\{1,2,3\\}$, the data form a $2 \\times 2$ table of counts, with $a_k$ exposed cases, $b_k$ exposed controls, $c_k$ unexposed cases, and $d_k$ unexposed controls. The observed counts are as follows:\n\n- Stratum $1$: $a_1 = 72$, $b_1 = 96$, $c_1 = 48$, $d_1 = 144$.\n- Stratum $2$: $a_2 = 48$, $b_2 = 64$, $c_2 = 32$, $d_2 = 96$.\n- Stratum $3$: $a_3 = 36$, $b_3 = 48$, $c_3 = 24$, $d_3 = 72$.\n\nAssume a common stratum-specific odds ratio across strata. Using the Mantel-Haenszel (MH) approach for stratified analysis in case-control studies, compute the common odds ratio estimator $\\hat{\\theta}_{MH}$ from these data. Round your final numerical answer to four significant figures.\n\nThen, based on fundamental principles of case-control sampling and stratified analysis, explain why a Mantel-Haenszel risk ratio estimator $\\hat{R}_{MH}$ computed from the same $2 \\times 2$ tables would not generally be interpretable in this case-control design without additional information or assumptions. Your explanation should rely on first principles of what is and is not identified under case-control sampling, and should not invoke any unproven shortcut formulas. The final answer to be reported is only the computed value of $\\hat{\\theta}_{MH}$ (with the specified rounding).", "solution": "The problem requires the calculation of the Mantel-Haenszel (MH) common odds ratio estimator, $\\hat{\\theta}_{MH}$, from a stratified case-control study and an explanation of why the corresponding MH risk ratio estimator is not interpretable in this context.\n\nFirst, we validate the problem statement.\nThe givens are:\n- A stratified case-control study with $3$ strata, indexed by $k \\in \\{1, 2, 3\\}$.\n- For each stratum $k$, the data are given in a $2 \\times 2$ table format: $a_k$ (exposed cases), $b_k$ (exposed controls), $c_k$ (unexposed cases), and $d_k$ (unexposed controls).\n- Data for Stratum $1$: $a_1 = 72$, $b_1 = 96$, $c_1 = 48$, $d_1 = 144$.\n- Data for Stratum $2$: $a_2 = 48$, $b_2 = 64$, $c_2 = 32$, $d_2 = 96$.\n- Data for Stratum $3$: $a_3 = 36$, $b_3 = 48$, $c_3 = 24$, $d_3 = 72$.\n- The assumption of a common odds ratio across strata is provided.\n- The task is to compute $\\hat{\\theta}_{MH}$ and explain the non-interpretability of the MH risk ratio estimator in this experimental design.\n\nThe problem is scientifically grounded, as it concerns a standard and fundamental method in biostatistics and epidemiology. It is well-posed, with all necessary data and definitions provided for a unique solution. The language is objective and precise. The data are internally consistent and realistic for such a study. Therefore, the problem is valid.\n\nWe proceed with the solution.\n\nPart 1: Calculation of the Mantel-Haenszel Common Odds Ratio Estimator $\\hat{\\theta}_{MH}$.\n\nThe Mantel-Haenszel estimator for the common odds ratio, $\\theta$, across $K$ strata is given by the formula:\n$$ \\hat{\\theta}_{MH} = \\frac{\\sum_{k=1}^{K} R_k}{\\sum_{k=1}^{K} S_k} = \\frac{\\sum_{k=1}^{K} \\frac{a_k d_k}{N_k}}{\\sum_{k=1}^{K} \\frac{b_k c_k}{N_k}} $$\nwhere $N_k = a_k + b_k + c_k + d_k$ is the total number of subjects in stratum $k$.\n\nWe calculate the components for each of the $3$ strata.\n\nFor Stratum $1$ ($k=1$):\n- $a_1 = 72$, $b_1 = 96$, $c_1 = 48$, $d_1 = 144$.\n- The total number of subjects is $N_1 = 72 + 96 + 48 + 144 = 360$.\n- The numerator component is $R_1 = \\frac{a_1 d_1}{N_1} = \\frac{72 \\times 144}{360} = \\frac{10368}{360} = 28.8$.\n- The denominator component is $S_1 = \\frac{b_1 c_1}{N_1} = \\frac{96 \\times 48}{360} = \\frac{4608}{360} = 12.8$.\n\nFor Stratum $2$ ($k=2$):\n- $a_2 = 48$, $b_2 = 64$, $c_2 = 32$, $d_2 = 96$.\n- The total number of subjects is $N_2 = 48 + 64 + 32 + 96 = 240$.\n- The numerator component is $R_2 = \\frac{a_2 d_2}{N_2} = \\frac{48 \\times 96}{240} = \\frac{4608}{240} = 19.2$.\n- The denominator component is $S_2 = \\frac{b_2 c_2}{N_2} = \\frac{64 \\times 32}{240} = \\frac{2048}{240} = \\frac{128}{15}$.\n\nFor Stratum $3$ ($k=3$):\n- $a_3 = 36$, $b_3 = 48$, $c_3 = 24$, $d_3 = 72$.\n- The total number of subjects is $N_3 = 36 + 48 + 24 + 72 = 180$.\n- The numerator component is $R_3 = \\frac{a_3 d_3}{N_3} = \\frac{36 \\times 72}{180} = \\frac{2592}{180} = 14.4$.\n- The denominator component is $S_3 = \\frac{b_3 c_3}{N_3} = \\frac{48 \\times 24}{180} = \\frac{1152}{180} = 6.4$.\n\nNow, we sum the components across all strata:\n$$ \\sum_{k=1}^{3} R_k = R_1 + R_2 + R_3 = 28.8 + 19.2 + 14.4 = 62.4 $$\n$$ \\sum_{k=1}^{3} S_k = S_1 + S_2 + S_3 = 12.8 + \\frac{128}{15} + 6.4 = 19.2 + \\frac{128}{15} = \\frac{19.2 \\times 15 + 128}{15} = \\frac{288 + 128}{15} = \\frac{416}{15} $$\n\nFinally, we compute the Mantel-Haenszel estimator:\n$$ \\hat{\\theta}_{MH} = \\frac{\\sum R_k}{\\sum S_k} = \\frac{62.4}{\\frac{416}{15}} = \\frac{62.4 \\times 15}{416} = \\frac{936}{416} $$\nSimplifying the fraction:\n$$ \\hat{\\theta}_{MH} = \\frac{936 \\div 104}{416 \\div 104} = \\frac{9}{4} = 2.25 $$\nThe problem asks for the answer to be rounded to four significant figures. This gives $2.250$.\n\nA noteworthy feature of this dataset is that the stratum-specific odds ratio is constant:\n$\\hat{OR}_1 = \\frac{a_1 d_1}{b_1 c_1} = \\frac{72 \\times 144}{96 \\times 48} = 2.25$.\n$\\hat{OR}_2 = \\frac{a_2 d_2}{b_2 c_2} = \\frac{48 \\times 96}{64 \\times 32} = 2.25$.\n$\\hat{OR}_3 = \\frac{a_3 d_3}{b_3 c_3} = \\frac{36 \\times 72}{48 \\times 24} = 2.25$.\nWhen the stratum-specific odds ratios are identical, the MH estimator equals this common value, which our calculation confirms.\n\nPart 2: Non-Interpretability of the Mantel-Haenszel Risk Ratio ($\\hat{R}_{MH}$) in a Case-Control Study.\n\nThe explanation rests on the fundamental principles of study design and what parameters are identifiable from the resulting data.\n\n$1$. **Case-Control Sampling Principle**: A case-control study samples subjects based on their disease outcome status ($D$). A group of individuals with the disease (cases, $D=1$) and a separate group without the disease (controls, $D=0$) are recruited. The researchers then ascertain the prior exposure status ($E$) for subjects in both groups. The number of cases and controls in the study is fixed by the design and does not typically reflect the prevalence of the disease in the source population.\n\n$2$. **Identifiable vs. Non-Identifiable Parameters**:\n   - Because sampling is conditional on $D$, a case-control study allows for the direct estimation of the probability of exposure given disease status, $P(E|D)$. Within stratum $k$, we can estimate $P(E=1|D=1, k)$ from the proportion $\\frac{a_k}{a_k+c_k}$ and $P(E=1|D=0, k)$ from $\\frac{b_k}{b_k+d_k}$.\n   - This allows for the calculation of the *exposure odds ratio*:\n     $$ OR_{\\text{exposure}, k} = \\frac{P(E=1|D=1, k) / P(E=0|D=1, k)}{P(E=1|D=0, k) / P(E=0|D=0, k)} = \\frac{a_k/c_k}{b_k/d_k} = \\frac{a_k d_k}{b_k c_k} $$\n   - A crucial property of the odds ratio is its invariance to the study design. The exposure odds ratio is mathematically identical to the *disease odds ratio* that one would estimate in a cohort study:\n     $$ OR_{\\text{disease}, k} = \\frac{P(D=1|E=1, k) / P(D=0|E=1, k)}{P(D=1|E=0, k) / P(D=0|E=0, k)} $$\n   - Thus, the odds ratio is an identifiable parameter in a case-control study.\n\n$3$. **The Problem with Risk and Risk Ratio**:\n   - The *risk* of disease is the conditional probability $P(D|E)$. Specifically, the risk in the exposed is $P(D=1|E=1)$ and the risk in the unexposed is $P(D=1|E=0)$.\n   - The *risk ratio* ($RR$) is the ratio of these risks: $RR = \\frac{P(D=1|E=1)}{P(D=1|E=0)}$.\n   - To estimate risk, one needs to sample based on exposure status ($E$), as is done in a cohort study. In a cohort study, the proportions $\\frac{a_k}{a_k+b_k}$ and $\\frac{c_k}{c_k+d_k}$ are valid estimates of the risk in the exposed and unexposed groups within stratum $k$, respectively.\n   - However, in a case-control study, the quantities $a_k+b_k$ (total exposed subjects in the sample) and $c_k+d_k$ (total unexposed subjects in the sample) do not represent a random sample of all exposed and unexposed individuals in the source population. The number of cases ($a_k$, $c_k$) and controls ($b_k$, $d_k$) in these totals is an artifact of the sampling ratios chosen by the investigator.\n   - Consequently, the quantities $\\frac{a_k}{a_k+b_k}$ and $\\frac{c_k}{c_k+d_k}$ computed from case-control data do not estimate the true population risks. They are not interpretable as $P(D=1|E=1, k)$ and $P(D=1|E=0, k)$.\n\n$4$. **Invalidity of the $\\hat{R}_{MH}$ Formula**: The Mantel-Haenszel risk ratio estimator is formulated as:\n   $$ \\hat{R}_{MH} = \\frac{\\sum_{k=1}^{K} \\frac{a_k (c_k+d_k)}{N_k}}{\\sum_{k=1}^{K} \\frac{c_k (a_k+b_k)}{N_k}} $$\n   This formula is a weighted average of stratum-specific risk ratios, constructed from terms that estimate risk in cohort studies. Since the component quantities needed to estimate stratum-specific risks are not identifiable from case-control data, applying this formula to such data produces a numerical value that is devoid of the intended epidemiological interpretation. It does not estimate the true common risk ratio. An estimate of the risk ratio can only be obtained from case-control data under additional assumptions (e.g., the rare disease assumption, where $OR \\approx RR$) or with external information (e.g., population disease prevalence), neither of which are provided in the problem statement.", "answer": "$$\\boxed{2.250}$$", "id": "4808937"}]}