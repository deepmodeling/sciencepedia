{"hands_on_practices": [{"introduction": "The foundation of any follow-up study is accurately measuring how often new outcomes occur over time. This exercise grounds you in calculating the incidence rate, a core metric derived from observed events and person-time. By exploring the assumptions required to equate this empirical rate with the theoretical constant hazard, you will deepen your understanding of the models that underpin survival analysis. [@problem_id:4593995]", "problem": "A prospective cohort study follows individuals at risk for a single, non-recurrent outcome. All participants are enrolled at baseline and are followed under uniform procedures, with complete outcome ascertainment. Over the study period, a total of $50$ first events are observed, and the cohort accrues $1{,}250$ person-years of at-risk follow-up time. Using core epidemiologic definitions for incidence rate and the time-to-event hazard, compute the incidence rate based on the observed data. Round your numerical answer to $3$ significant figures and express it in events per person-year. In addition, articulate the set of assumptions under which this incidence rate can be interpreted as the constant hazard of the underlying time-to-event process, relying on fundamental definitions and well-tested statistical facts rather than any shortcut formulas.", "solution": "The problem is deemed valid as it is scientifically grounded in established epidemiological principles, is well-posed with sufficient and consistent data, and is expressed in objective, unambiguous language.\n\nThe solution consists of two parts: first, the computation of the incidence rate, and second, the articulation of the assumptions under which this rate can be interpreted as a constant hazard.\n\nPart 1: Computation of the Incidence Rate\n\nThe incidence rate, also known as incidence density, is a measure of the rate at which new events occur in a population at risk over a specified period. It is defined as the total number of new events divided by the total person-time at risk.\n\nLet $I$ be the total number of first events observed.\nLet $PT$ be the total person-years of at-risk follow-up time.\n\nThe problem provides the following values:\nNumber of events, $I = 50$.\nTotal person-years, $PT = 1,250$ person-years.\n\nThe formula for the incidence rate ($IR$) is:\n$$ IR = \\frac{I}{PT} $$\nSubstituting the given values into the formula:\n$$ IR = \\frac{50}{1,250} \\, \\text{events/person-year} $$\n$$ IR = \\frac{1}{25} \\, \\text{events/person-year} $$\n$$ IR = 0.04 \\, \\text{events/person-year} $$\nThe problem requires the answer to be rounded to $3$ significant figures. To express $0.04$ with three significant figures, we write it as $0.0400$.\nThus, the incidence rate is $0.0400$ events per person-year.\n\nPart 2: Assumptions for Interpreting Incidence Rate as Constant Hazard\n\nThe hazard function, $\\lambda(t)$, represents the instantaneous potential for an event at time $t$, given survival up to time $t$. It is defined as:\n$$ \\lambda(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\le T  t + \\Delta t \\mid T \\ge t)}{\\Delta t} $$\nwhere $T$ is the random variable for the time-to-event.\n\nThe calculated incidence rate, $IR = \\frac{I}{PT}$, is a summary measure of the average event rate over the entire study population and follow-up period. For this average rate to be a valid estimate of a constant hazard, $\\lambda$, several critical assumptions must hold. The primary assumption is that the hazard rate is, in fact, constant over time.\n\nLet's formalize this. The likelihood function for a cohort study with $N$ individuals, where individual $i$ is followed for time $t_i$ and has an event status $\\delta_i$ ($\\delta_i=1$ for an event, $\\delta_i=0$ for censoring), is given by the product of individual contributions. The contribution for an individual is the probability density function $f(t_i) = \\lambda(t_i) S(t_i)$ if an event occurs, and the survival function $S(t_i)$ if the individual is censored. The survival function is $S(t) = \\exp\\left(-\\int_0^t \\lambda(u) du\\right)$.\n\nThe core assumption is that the hazard rate is constant over time:\n**1. Constant Hazard Rate:** We assume $\\lambda(t) = \\lambda$ for all $t$ in the follow-up period. This implies that the risk of the event does not change with the time an individual has been at risk. This is the defining characteristic of an exponential probability distribution for the time-to-event, where $S(t) = \\exp(-\\lambda t)$ and $f(t) = \\lambda \\exp(-\\lambda t)$.\n\nUnder this assumption, the likelihood function for the observed data simplifies significantly. The likelihood $L$ is proportional to the product of individual likelihood contributions:\n$$ L(\\lambda) \\propto \\prod_{i=1}^{N} [f(t_i)]^{\\delta_i} [S(t_i)]^{1-\\delta_i} $$\n$$ L(\\lambda) \\propto \\prod_{i=1}^{N} [\\lambda \\exp(-\\lambda t_i)]^{\\delta_i} [\\exp(-\\lambda t_i)]^{1-\\delta_i} $$\n$$ L(\\lambda) \\propto \\prod_{i=1}^{N} \\lambda^{\\delta_i} \\exp(-\\lambda t_i) $$\nCombining terms across all individuals:\n$$ L(\\lambda) \\propto \\lambda^{\\sum \\delta_i} \\exp\\left(-\\lambda \\sum t_i\\right) $$\nLet $I = \\sum \\delta_i$ be the total number of events and $PT = \\sum t_i$ be the total person-time. The likelihood function is:\n$$ L(\\lambda) \\propto \\lambda^{I} \\exp(-\\lambda \\cdot PT) $$\nTo find the value of $\\lambda$ that maximizes this likelihood, we work with the log-likelihood, $\\ln L(\\lambda)$:\n$$ \\ln L(\\lambda) = I \\ln(\\lambda) - \\lambda \\cdot PT + C $$\nwhere $C$ is a constant. We take the derivative with respect to $\\lambda$ and set it to zero:\n$$ \\frac{d}{d\\lambda} \\ln L(\\lambda) = \\frac{I}{\\lambda} - PT = 0 $$\nSolving for $\\lambda$ gives the maximum likelihood estimator (MLE), denoted $\\hat{\\lambda}$:\n$$ \\hat{\\lambda} = \\frac{I}{PT} $$\nThis result demonstrates that the calculated incidence rate, $IR$, is precisely the maximum likelihood estimator for the hazard rate $\\lambda$ *under the assumption that the hazard rate is constant*.\n\nIn addition to this fundamental assumption, several other conditions, some of which are implied by the problem statement, are necessary for the validity and interpretation of the result:\n\n**2. Homogeneity of Risk:** For the calculated rate $\\hat{\\lambda}$ to represent the hazard for any individual in the cohort (and not just an average), we must assume that the constant hazard rate $\\lambda$ is the same for all individuals in the study. If different subgroups have different (but constant) hazards, the calculated rate represents a weighted average of these hazards.\n\n**3. Independent Censoring:** The reasons for censoring (i.e., an individual's follow-up ending without an event) must be independent of the risk of the outcome. This means that an individual who is lost to follow-up or whose observation period ends must not be at a systematically higher or lower risk than those who remain under observation. The problem's mention of \"complete outcome ascertainment\" and \"uniform procedures\" helps satisfy this, but it remains a critical assumption in practice.\n\n**4. Independence of Individuals:** The time-to-event for any given individual is independent of the time-to-event for all other individuals in the cohort. The occurrence of an event in one participant does not alter the probability of the event for others.\n\nIn summary, the interpretation of the incidence rate as the constant hazard of the underlying process rests principally on the assumption of a time-constant hazard, which corresponds to an exponential survival model.", "answer": "$$ \\boxed{0.0400} $$", "id": "4593995"}, {"introduction": "In practice, outcomes are rarely ascertained with perfect accuracy, especially when using automated methods like claims-based algorithms. This practice challenges you to assess the performance of such a tool by calculating its Positive and Negative Predictive Values ($PPV$ and $NPV$) from known sensitivity and specificity. Mastering this skill is vital for judging the reliability of outcome data in real-world epidemiological research. [@problem_id:4593917]", "problem": "A health system deploys an administrative claims-based algorithm to ascertain acute myocardial infarction during routine follow-up in a dynamic adult cohort. Outcome ascertainment is performed once per person-year (PY), producing a binary classification for each PY: algorithm-positive or algorithm-negative. The true occurrence of acute myocardial infarction within a PY has prevalence $p = 7 \\times 10^{-3}$. From a validation study, the algorithm’s sensitivity is $Se = 0.84$ and specificity is $Sp = 0.97$. Assume person-year classifications are exchangeable and that the algorithm’s classification performance does not vary over time or across individuals. Positive Predictive Value (PPV) is defined as the probability that an algorithm-positive PY is truly diseased, and Negative Predictive Value (NPV) is defined as the probability that an algorithm-negative PY is truly non-diseased.\n\nUsing only core definitions of sensitivity, specificity, and Bayes’ theorem, derive expressions for $PPV$ and $NPV$ in terms of $p$, $Se$, and $Sp$, appropriate to this PY-level ascertainment. Then, compute the numerical values of $PPV$ and $NPV$ for the given $p$, $Se$, and $Sp$ as decimals. Finally, compute the expected number of false positives per $1{,}000$ person-years, defined as the expected count of algorithm-positive PYs among truly non-diseased PYs per $1{,}000$ PY, under the assumptions stated.\n\nRound all numerical answers to three significant figures. Express $PPV$ and $NPV$ as decimals. For the false positives, report the rate per $1{,}000$ person-years.", "solution": "The problem concerns outcome ascertainment in epidemiology at the person-year level using a claims-based algorithm. We begin from the core definitions:\n- Sensitivity $Se$ is defined as $Se = \\Pr(T^{+} \\mid D^{+})$, the probability the algorithm is positive given true disease in a PY.\n- Specificity $Sp$ is defined as $Sp = \\Pr(T^{-} \\mid D^{-})$, the probability the algorithm is negative given no true disease in a PY.\n- The prevalence at the PY level is $p = \\Pr(D^{+})$. Therefore $\\Pr(D^{-}) = 1 - p$.\n- Positive Predictive Value $PPV$ is $PPV = \\Pr(D^{+} \\mid T^{+})$.\n- Negative Predictive Value $NPV$ is $NPV = \\Pr(D^{-} \\mid T^{-})$.\n\nTo derive $PPV$, use Bayes’ theorem. Let $T^{+}$ denote a positive classification and $D^{+}$ denote true disease in the PY. Then\n$$\nPPV = \\Pr(D^{+} \\mid T^{+}) = \\frac{\\Pr(T^{+} \\mid D^{+}) \\Pr(D^{+})}{\\Pr(T^{+})}.\n$$\nCompute $\\Pr(T^{+})$ by the law of total probability:\n$$\n\\Pr(T^{+}) = \\Pr(T^{+} \\mid D^{+}) \\Pr(D^{+}) + \\Pr(T^{+} \\mid D^{-}) \\Pr(D^{-}).\n$$\nBy definition, $\\Pr(T^{+} \\mid D^{+}) = Se$ and $\\Pr(T^{+} \\mid D^{-}) = 1 - Sp$. Therefore\n$$\nPPV = \\frac{Se \\cdot p}{Se \\cdot p + (1 - Sp) \\cdot (1 - p)}.\n$$\n\nSimilarly, to derive $NPV$, let $T^{-}$ denote a negative classification and $D^{-}$ denote no true disease in the PY. Bayes’ theorem gives\n$$\nNPV = \\Pr(D^{-} \\mid T^{-}) = \\frac{\\Pr(T^{-} \\mid D^{-}) \\Pr(D^{-})}{\\Pr(T^{-})}.\n$$\nCompute $\\Pr(T^{-})$:\n$$\n\\Pr(T^{-}) = \\Pr(T^{-} \\mid D^{-}) \\Pr(D^{-}) + \\Pr(T^{-} \\mid D^{+}) \\Pr(D^{+}).\n$$\nBy definition, $\\Pr(T^{-} \\mid D^{-}) = Sp$ and $\\Pr(T^{-} \\mid D^{+}) = 1 - Se$. Therefore\n$$\nNPV = \\frac{Sp \\cdot (1 - p)}{Sp \\cdot (1 - p) + (1 - Se) \\cdot p}.\n$$\n\nFor false positives per $1{,}000$ person-years, we seek the expected number of algorithm-positive classifications among truly non-diseased PYs per $1{,}000$ PY. The probability a PY is truly non-diseased is $(1 - p)$, and conditional on $D^{-}$ the probability of a positive classification is $(1 - Sp)$. Thus, the expected false positives per PY is\n$$\n\\Pr(T^{+} \\cap D^{-}) = \\Pr(T^{+} \\mid D^{-}) \\Pr(D^{-}) = (1 - Sp) (1 - p),\n$$\nand per $1{,}000$ PY, the expected count is\n$$\n1000 \\times (1 - Sp) (1 - p).\n$$\n\nNow substitute the given values $p = 7 \\times 10^{-3}$, $Se = 0.84$, and $Sp = 0.97$.\n\nCompute $PPV$:\n$$\nPPV = \\frac{0.84 \\times 0.007}{0.84 \\times 0.007 + (1 - 0.97) \\times (1 - 0.007)}.\n$$\nEvaluate components:\n$$\n0.84 \\times 0.007 = 0.00588,\n$$\n$$\n1 - 0.97 = 0.03, \\quad 1 - 0.007 = 0.993,\n$$\n$$\n0.03 \\times 0.993 = 0.02979,\n$$\nso the denominator is\n$$\n0.00588 + 0.02979 = 0.03567,\n$$\nand therefore\n$$\nPPV = \\frac{0.00588}{0.03567} \\approx 0.16484.\n$$\nRounded to three significant figures:\n$$\nPPV \\approx 0.165.\n$$\n\nCompute $NPV$:\n$$\nNPV = \\frac{0.97 \\times (1 - 0.007)}{0.97 \\times (1 - 0.007) + (1 - 0.84) \\times 0.007}.\n$$\nEvaluate components:\n$$\n0.97 \\times 0.993 = 0.96321,\n$$\n$$\n1 - 0.84 = 0.16, \\quad 0.16 \\times 0.007 = 0.00112,\n$$\nDenominator:\n$$\n0.96321 + 0.00112 = 0.96433,\n$$\nThus,\n$$\nNPV = \\frac{0.96321}{0.96433} \\approx 0.99884.\n$$\nRounded to three significant figures:\n$$\nNPV \\approx 0.999.\n$$\n\nCompute expected false positives per $1{,}000$ PY:\n$$\n1000 \\times (1 - Sp) (1 - p) = 1000 \\times 0.03 \\times 0.993 = 1000 \\times 0.02979 = 29.79.\n$$\nRounded to three significant figures:\n$$\n\\text{False positives per }1{,}000\\text{ PY} \\approx 29.8.\n$$\n\nTherefore, the requested values rounded to three significant figures are $PPV \\approx 0.165$, $NPV \\approx 0.999$, and false positives per $1{,}000$ person-years $\\approx 29.8$.", "answer": "$$\\boxed{\\begin{pmatrix}0.165  0.999  29.8\\end{pmatrix}}$$", "id": "4593917"}, {"introduction": "Building on the concept of ascertainment error, this final practice demonstrates how such imperfections can distort a study's conclusions. You will quantify the bias introduced into a risk ratio when the outcome detection method performs differently between exposed and unexposed groups. This exercise provides a crucial lesson in how differential misclassification can threaten the validity of an observed association. [@problem_id:4593973]", "problem": "A prospective cohort study follows individuals for one year to estimate the causal effect of a binary exposure on a binary outcome. Let $R_{1}$ denote the true one-year risk of the outcome among the exposed and $R_{0}$ denote the true one-year risk among the unexposed, and define the true risk ratio (RR) as $RR_{\\text{true}} = R_{1} / R_{0}$. The outcome is ascertained at the end of follow-up using an algorithm whose sensitivity and specificity may differ by exposure status due to differential surveillance. Sensitivity is defined as $Se_{x} = \\Pr(\\text{observed outcome} = 1 \\mid \\text{true outcome} = 1, X=x)$ and specificity as $Sp = \\Pr(\\text{observed outcome} = 0 \\mid \\text{true outcome} = 0)$, where $X \\in \\{0,1\\}$ denotes unexposed ($0$) versus exposed ($1$). \n\nAssume the following are scientifically and operationally accurate for this study:\n- The true risk ratio is $RR_{\\text{true}} = 1.5$.\n- The true one-year risk among the unexposed is $R_{0} = 0.10$, so $R_{1} = RR_{\\text{true}} \\times R_{0}$.\n- The sensitivity among the exposed is $Se_{1} = 0.95$; the sensitivity among the unexposed is $Se_{0} = 0.85$.\n- The specificity is common across exposure groups at $Sp = 0.98$.\n- There is complete follow-up with no loss to follow-up; the only imperfection is outcome misclassification at ascertainment, as characterized above.\n- The cohort is sufficiently large that expected values equal their empirical counterparts.\n\nStarting from the core definitions of risk, sensitivity, and specificity, derive the expected observed one-year risks among exposed and unexposed, and from these derive the expected observed risk ratio, $RR_{\\text{obs}}$. Define the additive bias in the risk ratio due to outcome misclassification as $B = RR_{\\text{obs}} - RR_{\\text{true}}$. Compute $B$. Round your final numerical answer to four significant figures. Provide your final answer as a dimensionless number.", "solution": "The task is to quantify the bias in an observed risk ratio due to differential outcome misclassification. The solution will proceed by first deriving a general expression for observed risk, then applying it to the specific parameters given for the exposed and unexposed groups to compute the observed risk ratio and the resulting bias.\n\nThe problem defines the true one-year risk in the exposed group as $R_{1}$ and in the unexposed group as $R_{0}$. The true risk ratio is $RR_{\\text{true}} = \\frac{R_{1}}{R_{0}}$. We are given $RR_{\\text{true}} = 1.5$ and $R_{0} = 0.10$. From this, we can calculate the true risk in the exposed group:\n$$\nR_{1} = RR_{\\text{true}} \\times R_{0} = 1.5 \\times 0.10 = 0.15\n$$\n\nOutcome misclassification is characterized by sensitivity ($Se_{x}$) and specificity ($Sp_{x}$), where $x \\in \\{0, 1\\}$ denotes the exposure status.\nSensitivity is defined as $Se_{x} = \\Pr(\\text{observed outcome} = 1 \\mid \\text{true outcome} = 1, X=x)$.\nSpecificity is defined as $Sp_{x} = \\Pr(\\text{observed outcome} = 0 \\mid \\text{true outcome} = 0, X=x)$.\nThe problem states that specificity is non-differential, so $Sp_{1} = Sp_{0} = Sp = 0.98$.\nSensitivity is differential: $Se_{1} = 0.95$ for the exposed and $Se_{0} = 0.85$ for the unexposed.\n\nThe observed one-year risk, $R_{\\text{obs}, x}$, for an exposure group $x$ is the probability that an individual in that group is classified as having the outcome. We can express this using the law of total probability, by conditioning on the true outcome status:\n$$\nR_{\\text{obs}, x} = \\Pr(\\text{observed outcome} = 1 \\mid X=x)\n$$\n$$\nR_{\\text{obs}, x} = \\Pr(\\text{obs}=1 \\mid \\text{true}=1, X=x)\\Pr(\\text{true}=1 \\mid X=x) + \\Pr(\\text{obs}=1 \\mid \\text{true}=0, X=x)\\Pr(\\text{true}=0 \\mid X=x)\n$$\nHere, $\\Pr(\\text{true}=1 \\mid X=x)$ is the true risk $R_{x}$, and $\\Pr(\\text{true}=0 \\mid X=x)$ is $1 - R_{x}$.\nThe term $\\Pr(\\text{obs}=1 \\mid \\text{true}=1, X=x)$ is the definition of sensitivity, $Se_{x}$.\nThe term $\\Pr(\\text{obs}=1 \\mid \\text{true}=0, X=x)$ represents a false positive. It is equal to $1 - \\Pr(\\text{obs}=0 \\mid \\text{true}=0, X=x)$, which is $1 - Sp$.\nSubstituting these definitions into the equation for observed risk yields the general formula:\n$$\nR_{\\text{obs}, x} = (Se_{x} \\times R_{x}) + ((1 - Sp) \\times (1 - R_{x}))\n$$\n\nWe now apply this formula to calculate the observed risks for the unexposed ($x=0$) and exposed ($x=1$) groups.\n\nFor the unexposed group ($x=0$):\nThe parameters are $R_{0} = 0.10$, $Se_{0} = 0.85$, and $Sp = 0.98$.\n$$\nR_{\\text{obs}, 0} = (Se_{0} \\times R_{0}) + ((1 - Sp) \\times (1 - R_{0}))\n$$\n$$\nR_{\\text{obs}, 0} = (0.85 \\times 0.10) + ((1 - 0.98) \\times (1 - 0.10))\n$$\n$$\nR_{\\text{obs}, 0} = 0.085 + (0.02 \\times 0.90)\n$$\n$$\nR_{\\text{obs}, 0} = 0.085 + 0.018 = 0.103\n$$\n\nFor the exposed group ($x=1$):\nThe parameters are $R_{1} = 0.15$, $Se_{1} = 0.95$, and $Sp = 0.98$.\n$$\nR_{\\text{obs}, 1} = (Se_{1} \\times R_{1}) + ((1 - Sp) \\times (1 - R_{1}))\n$$\n$$\nR_{\\text{obs}, 1} = (0.95 \\times 0.15) + ((1 - 0.98) \\times (1 - 0.15))\n$$\n$$\nR_{\\text{obs}, 1} = 0.1425 + (0.02 \\times 0.85)\n$$\n$$\nR_{\\text{obs}, 1} = 0.1425 + 0.017 = 0.1595\n$$\n\nThe observed risk ratio, $RR_{\\text{obs}}$, is the ratio of these observed risks:\n$$\nRR_{\\text{obs}} = \\frac{R_{\\text{obs}, 1}}{R_{\\text{obs}, 0}} = \\frac{0.1595}{0.103} \\approx 1.548543689...\n$$\n\nThe problem defines the additive bias, $B$, as the difference between the observed risk ratio and the true risk ratio:\n$$\nB = RR_{\\text{obs}} - RR_{\\text{true}}\n$$\nSubstituting the calculated and given values:\n$$\nB = \\frac{0.1595}{0.103} - 1.5\n$$\n$$\nB \\approx 1.548543689... - 1.5\n$$\n$$\nB \\approx 0.048543689...\n$$\n\nThe problem requires rounding the final answer to four significant figures. The first significant figure is $4$, followed by $8$, $5$, and $4$. The next digit is $3$, which is less than $5$, so we do not round up.\n$$\nB \\approx 0.04854\n$$\nThis represents the additive bias in the risk ratio due to the specified differential outcome misclassification.", "answer": "$$\\boxed{0.04854}$$", "id": "4593973"}]}