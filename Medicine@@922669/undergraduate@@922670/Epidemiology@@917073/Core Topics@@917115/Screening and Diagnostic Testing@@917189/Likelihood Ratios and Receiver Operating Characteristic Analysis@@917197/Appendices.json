{"hands_on_practices": [{"introduction": "The performance of a diagnostic test is often described by its sensitivity and specificity, but these metrics don't tell the whole story. To truly understand the meaning of a positive or negative result, we must consider the context of the population being tested. This exercise [@problem_id:4607798] provides a hands-on opportunity to calculate how a test's Positive and Negative Predictive Values ($PPV$ and $NPV$) change dramatically with disease prevalence, demonstrating a fundamental principle in clinical epidemiology.", "problem": "An investigator evaluates a binary diagnostic test for a disease using a fixed decision threshold, corresponding to a single operating point on the Receiver Operating Characteristic (ROC) curve. At this threshold, the test has sensitivity $ \\text{Se} = 0.80 $ and specificity $ \\text{Sp} = 0.95 $. Consider two populations with disease prevalences $ p = 0.05 $ and $ p = 0.50 $. Starting only from the definitions of sensitivity, specificity, prevalence, likelihood ratio (LR), and Bayes theorem in the odds form, derive expressions for the Positive Predictive Value (PPV) and the Negative Predictive Value (NPV) at each prevalence. Compute $ \\text{PPV} $ and $ \\text{NPV} $ for $ p = 0.05 $ and for $ p = 0.50 $. To summarize the impact of prevalence on post-test probability for positive results, report the ratio $ r = \\dfrac{\\text{PPV at } p = 0.50}{\\text{PPV at } p = 0.05} $. Round your final reported value of $ r $ to four significant figures. Provide your final answer as a single dimensionless number.", "solution": "The problem requires the derivation and calculation of Positive Predictive Value (PPV) and Negative Predictive Value (NPV) for a diagnostic test under two different disease prevalences, based on fundamental definitions and Bayes' theorem. The final goal is to compute the ratio of the PPVs.\n\nFirst, let us formally define the events and probabilities involved.\nLet $D$ be the event that an individual has the disease.\nLet $D^c$ be the event that an individual does not have the disease.\nLet $T^+$ be the event of a positive test result.\nLet $T^-$ be the event of a negative test result.\n\nThe given quantities are:\n- Sensitivity ($\\text{Se}$): The probability of a positive test given the disease is present.\n$$ \\text{Se} = P(T^+ | D) = 0.80 $$\n- Specificity ($\\text{Sp}$): The probability of a negative test given the disease is absent.\n$$ \\text{Sp} = P(T^- | D^c) = 0.95 $$\n- Prevalence ($p$): The prior probability of having the disease.\n$$ p = P(D) $$\nWe are given two prevalences: $p_1 = 0.05$ and $p_2 = 0.50$.\n\nThe quantities to be derived are:\n- Positive Predictive Value ($\\text{PPV}$): The post-test probability of having the disease given a positive test result.\n$$ \\text{PPV} = P(D | T^+) $$\n- Negative Predictive Value ($\\text{NPV}$): The post-test probability of not having the disease given a negative test result.\n$$ \\text{NPV} = P(D^c | T^-) $$\n\nThe problem directs us to use Bayes' theorem in the odds form, which states that the posterior odds are equal to the prior odds multiplied by the likelihood ratio.\n$$ \\text{Posterior Odds} = \\text{Likelihood Ratio} \\times \\text{Prior Odds} $$\n\n**Derivation for PPV**\n\nFor a positive test result ($T^+$), we are interested in the posterior probability of disease, $P(D|T^+)$. The prior odds of disease are the ratio of the probability of having the disease to the probability of not having it:\n$$ \\text{Odds}_{\\text{pre}} = \\frac{P(D)}{P(D^c)} = \\frac{p}{1-p} $$\nThe posterior odds of disease after a positive test are:\n$$ \\text{Odds}_{\\text{post}}(T^+) = \\frac{P(D | T^+)}{P(D^c | T^+)} $$\nThe relevant likelihood ratio is the Positive Likelihood Ratio ($\\text{LR}^+$), defined as the ratio of the probability of a positive test in the diseased group to the probability of a positive test in the non-diseased group:\n$$ \\text{LR}^+ = \\frac{P(T^+ | D)}{P(T^+ | D^c)} $$\nWe know $P(T^+ | D) = \\text{Se}$. The denominator is $P(T^+ | D^c) = 1 - P(T^- | D^c) = 1 - \\text{Sp}$.\nTherefore,\n$$ \\text{LR}^+ = \\frac{\\text{Se}}{1 - \\text{Sp}} $$\nApplying Bayes' theorem in odds form:\n$$ \\text{Odds}_{\\text{post}}(T^+) = \\text{LR}^+ \\times \\text{Odds}_{\\text{pre}} = \\left( \\frac{\\text{Se}}{1 - \\text{Sp}} \\right) \\left( \\frac{p}{1 - p} \\right) $$\nTo find the PPV, we convert the posterior odds back to a probability. For any event $A$ with odds $\\text{Odds}(A) = P(A) / (1-P(A))$, the probability is $P(A) = \\text{Odds}(A) / (1 + \\text{Odds}(A))$.\n$$ \\text{PPV} = \\frac{\\text{Odds}_{\\text{post}}(T^+)}{1 + \\text{Odds}_{\\text{post}}(T^+)} $$\n\n**Derivation for NPV**\n\nFor a negative test result ($T^-$), we are interested in the posterior probability of not having the disease, $P(D^c|T^-)$. We can formulate this using the odds of *not* having the disease. The prior odds of not having the disease are:\n$$ \\text{Odds}_{\\text{pre}}(D^c) = \\frac{P(D^c)}{P(D)} = \\frac{1-p}{p} $$\nThe posterior odds of not having the disease after a negative test are:\n$$ \\text{Odds}_{\\text{post}}(D^c | T^-) = \\frac{P(D^c | T^-)}{P(D | T^-)} $$\nThe relevant likelihood ratio describes the evidence provided by a negative test for the absence of disease. This is $\\frac{P(T^-|D^c)}{P(T^-|D)}$.\nWe know $P(T^-|D^c) = \\text{Sp}$. The denominator is $P(T^-|D) = 1 - P(T^+|D) = 1 - \\text{Se}$.\nThe likelihood ratio for no disease given a negative test is: $\\frac{\\text{Sp}}{1 - \\text{Se}}$. Note that this is the reciprocal of the Negative Likelihood Ratio, $\\text{LR}^- = \\frac{1-\\text{Se}}{\\text{Sp}}$.\nApplying Bayes' theorem for the odds of no disease:\n$$ \\text{Odds}_{\\text{post}}(D^c | T^-) = \\left( \\frac{\\text{Sp}}{1 - \\text{Se}} \\right) \\left( \\frac{1-p}{p} \\right) $$\nConverting these posterior odds to the probability NPV:\n$$ \\text{NPV} = \\frac{\\text{Odds}_{\\text{post}}(D^c | T^-)}{1 + \\text{Odds}_{\\text{post}}(D^c | T^-)} $$\n\n**Calculations**\n\nFirst, compute the likelihood ratios using the given values $\\text{Se} = 0.80$ and $\\text{Sp} = 0.95$.\n$$ \\text{LR}^+ = \\frac{0.80}{1 - 0.95} = \\frac{0.80}{0.05} = 16 $$\nThe likelihood ratio for NPV derivation is:\n$$ \\frac{\\text{Sp}}{1 - \\text{Se}} = \\frac{0.95}{1 - 0.80} = \\frac{0.95}{0.20} = 4.75 = \\frac{19}{4} $$\n\n**Case 1: Prevalence $p = 0.05$**\n\nPrior odds of disease:\n$$ \\text{Odds}_{\\text{pre},1} = \\frac{0.05}{1 - 0.05} = \\frac{0.05}{0.95} = \\frac{5}{95} = \\frac{1}{19} $$\nPosterior odds of disease given a positive test:\n$$ \\text{Odds}_{\\text{post},1}(T^+) = \\text{LR}^+ \\times \\text{Odds}_{\\text{pre},1} = 16 \\times \\frac{1}{19} = \\frac{16}{19} $$\nPPV for $p=0.05$:\n$$ \\text{PPV}_1 = \\frac{16/19}{1 + 16/19} = \\frac{16/19}{35/19} = \\frac{16}{35} $$\nPrior odds of no disease:\n$$ \\text{Odds}_{\\text{pre},1}(D^c) = \\frac{1-0.05}{0.05} = \\frac{0.95}{0.05} = 19 $$\nPosterior odds of no disease given a negative test:\n$$ \\text{Odds}_{\\text{post},1}(D^c | T^-) = \\left( \\frac{\\text{Sp}}{1 - \\text{Se}} \\right) \\times \\text{Odds}_{\\text{pre},1}(D^c) = \\frac{19}{4} \\times 19 = \\frac{361}{4} $$\nNPV for $p=0.05$:\n$$ \\text{NPV}_1 = \\frac{361/4}{1 + 361/4} = \\frac{361/4}{365/4} = \\frac{361}{365} $$\n\n**Case 2: Prevalence $p = 0.50$**\n\nPrior odds of disease:\n$$ \\text{Odds}_{\\text{pre},2} = \\frac{0.50}{1 - 0.50} = \\frac{0.50}{0.50} = 1 $$\nPosterior odds of disease given a positive test:\n$$ \\text{Odds}_{\\text{post},2}(T^+) = \\text{LR}^+ \\times \\text{Odds}_{\\text{pre},2} = 16 \\times 1 = 16 $$\nPPV for $p=0.50$:\n$$ \\text{PPV}_2 = \\frac{16}{1 + 16} = \\frac{16}{17} $$\nPrior odds of no disease:\n$$ \\text{Odds}_{\\text{pre},2}(D^c) = \\frac{1-0.50}{0.50} = 1 $$\nPosterior odds of no disease given a negative test:\n$$ \\text{Odds}_{\\text{post},2}(D^c | T^-) = \\left( \\frac{\\text{Sp}}{1 - \\text{Se}} \\right) \\times \\text{Odds}_{\\text{pre},2}(D^c) = \\frac{19}{4} \\times 1 = \\frac{19}{4} $$\nNPV for $p=0.50$:\n$$ \\text{NPV}_2 = \\frac{19/4}{1 + 19/4} = \\frac{19/4}{23/4} = \\frac{19}{23} $$\n\n**Summary of predictive values:**\nFor $p=0.05$: $\\text{PPV}_1 = \\frac{16}{35} \\approx 0.4571$, $\\text{NPV}_1 = \\frac{361}{365} \\approx 0.9890$\nFor $p=0.50$: $\\text{PPV}_2 = \\frac{16}{17} \\approx 0.9412$, $\\text{NPV}_2 = \\frac{19}{23} \\approx 0.8261$\n\n**Final Calculation: Ratio $r$**\n\nThe problem asks for the ratio $r = \\dfrac{\\text{PPV at } p = 0.50}{\\text{PPV at } p = 0.05}$.\n$$ r = \\frac{\\text{PPV}_2}{\\text{PPV}_1} = \\frac{16/17}{16/35} = \\frac{16}{17} \\times \\frac{35}{16} = \\frac{35}{17} $$\nNow, we compute the numerical value and round to four significant figures.\n$$ r = \\frac{35}{17} \\approx 2.0588235... $$\nRounding to four significant figures gives $2.059$.", "answer": "$$\\boxed{2.059}$$", "id": "4607798"}, {"introduction": "A single diagnostic test with a fixed threshold corresponds to just one point in Receiver Operating Characteristic (ROC) space. This practice [@problem_id:4607874] challenges you to think geometrically about test performance by exploring what happens when we combine a simple binary test with trivial \"always positive\" or \"always negative\" classifiers. Through this exercise, you will derive the piecewise linear ROC curve and discover the profound connection between the slope of the ROC curve and the Likelihood Ratios, providing a deeper understanding of the Area Under the Curve (AUC).", "problem": "Consider a binary diagnostic test applied to a population, where the sensitivity $\\text{Se}$ is defined as the true positive rate $P(\\text{Test} = + \\mid \\text{Disease})$ and the specificity $\\text{Sp}$ is defined as the true negative rate $P(\\text{Test} = - \\mid \\text{No Disease})$. In the Receiver Operating Characteristic (ROC) space with $x$-axis equal to the false positive rate $\\text{FPR} = P(\\text{Test} = + \\mid \\text{No Disease})$ and $y$-axis equal to the true positive rate $\\text{TPR} = P(\\text{Test} = + \\mid \\text{Disease})$, this test corresponds to the single operating point $(\\text{FPR}, \\text{TPR}) = (1 - \\text{Sp}, \\text{Se})$, together with the two trivial classifiers that always predict negative (the point $(0,0)$) or always predict positive (the point $(1,1)$). \n\nSuppose one is allowed to randomize decisions by mixing the binary test with a trivial classifier: with probability $\\lambda \\in [0,1]$ use the binary test, and with probability $1 - \\lambda$ use a trivial classifier (either always negative or always positive). Using only the definitions of $\\text{Se}$, $\\text{Sp}$, $\\text{TPR}$, and $\\text{FPR}$ and basic probability, determine how such randomization maps to ROC points and the geometric structure that results, including the slope of the line segments and the Area Under the Curve (AUC).\n\nWhich option most accurately describes the resulting ROC geometry and associated quantities?\n\nA. Randomizing between the binary test $(1 - \\text{Sp}, \\text{Se})$ and the always-negative classifier $(0,0)$ with probability $\\lambda$ yields ROC points $(\\lambda(1 - \\text{Sp}), \\lambda\\,\\text{Se})$, forming the line segment to $(0,0)$ whose slope equals $\\dfrac{\\text{Se}}{1 - \\text{Sp}} = \\text{LR}^+$, where $\\text{LR}^+$ is the positive Likelihood Ratio. Randomizing between the binary test and the always-positive classifier $(1,1)$ yields ROC points $\\big(1 - \\lambda + \\lambda(1 - \\text{Sp}),\\, 1 - \\lambda + \\lambda\\,\\text{Se}\\big)$, forming the line segment to $(1,1)$ whose slope equals $\\dfrac{1 - \\text{Se}}{\\text{Sp}} = \\text{LR}^-$. The convex hull defined by $(0,0)$, $(1 - \\text{Sp}, \\text{Se})$, and $(1,1)$ has piecewise linear ROC whose Area Under the Curve equals $\\dfrac{\\text{Se} + \\text{Sp}}{2}$.\n\nB. Randomizing between the test and the always-negative classifier yields ROC points $(\\lambda(1 - \\text{Sp}),\\, 1 - \\lambda(1 - \\text{Se}))$, which forms a curve bending above the straight line to $(0,0)$; its slope equals $\\dfrac{\\text{Sp}}{\\text{Se}}$, and the Area Under the Curve equals $\\text{Se}\\times \\text{Sp}$.\n\nC. Randomization with trivial classifiers cannot change the ROC location of a fixed binary test; the ROC remains a single point $(1 - \\text{Sp}, \\text{Se})$ and the Area Under the Curve is $0$. Any mixture with trivial classifiers produces points outside the unit square, violating probability constraints.\n\nD. The line segment from $(0,0)$ to $(1 - \\text{Sp}, \\text{Se})$ has slope $\\dfrac{1 - \\text{Se}}{\\text{Sp}} = \\text{LR}^-$, and the line segment from $(1 - \\text{Sp}, \\text{Se})$ to $(1,1)$ has slope $\\dfrac{\\text{Se}}{1 - \\text{Sp}} = \\text{LR}^+$. The Area Under the Curve of the piecewise linear ROC equals $1 - \\dfrac{\\text{Se} + \\text{Sp}}{2}$.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. All definitions and premises are standard in the field of epidemiology and biostatistics. The problem is solvable via application of basic probability theory.\n\nLet $D$ denote the event that a subject has the disease, and $D^c$ the event that the subject does not. The characteristics of the binary test are given by its sensitivity, $\\text{Se} = P(\\text{Test} = + \\mid D)$, and specificity, $\\text{Sp} = P(\\text{Test} = - \\mid D^c)$.\n\nIn the Receiver Operating Characteristic (ROC) space, the coordinates are $(\\text{FPR}, \\text{TPR})$.\nThe True Positive Rate is $\\text{TPR} = P(\\text{Test} = + \\mid D) = \\text{Se}$.\nThe False Positive Rate is $\\text{FPR} = P(\\text{Test} = + \\mid D^c) = 1 - P(\\text{Test} = - \\mid D^c) = 1 - \\text{Sp}$.\nThus, the binary test is represented by the single point $P_B = (1 - \\text{Sp}, \\text{Se})$ in the ROC space. The trivial classifiers are the origin $P_{neg} = (0,0)$ (always predict negative) and the point $P_{pos} = (1,1)$ (always predict positive).\n\nThe problem asks to analyze the effect of randomizing between the binary test $P_B$ and the trivial classifiers. This forms a composite test.\n\nCase 1: Randomization between the binary test ($P_B$) and the always-negative classifier ($P_{neg}$).\nLet the composite test be $T_1$. With probability $\\lambda \\in [0,1]$, we use the binary test, and with probability $1-\\lambda$, we use the always-negative rule. The choice of test is independent of the disease status.\n\nThe TPR of $T_1$ is $\\text{TPR}_1 = P(T_1 = + \\mid D)$. By the law of total probability:\n$$ \\text{TPR}_1 = P(\\text{Test}=+\\mid D) \\cdot \\lambda + P(\\text{always neg. test}=+\\mid D) \\cdot (1-\\lambda) $$\n$$ \\text{TPR}_1 = \\text{Se} \\cdot \\lambda + 0 \\cdot (1-\\lambda) = \\lambda\\,\\text{Se} $$\nThe FPR of $T_1$ is $\\text{FPR}_1 = P(T_1 = + \\mid D^c)$:\n$$ \\text{FPR}_1 = P(\\text{Test}=+\\mid D^c) \\cdot \\lambda + P(\\text{always neg. test}=+\\mid D^c) \\cdot (1-\\lambda) $$\n$$ \\text{FPR}_1 = (1-\\text{Sp}) \\cdot \\lambda + 0 \\cdot (1-\\lambda) = \\lambda(1-\\text{Sp}) $$\nThe operating points for this composite test are $(\\lambda(1-\\text{Sp}), \\lambda\\,\\text{Se})$ for $\\lambda \\in [0,1]$. This is the parametric equation of a line segment connecting $(0,0)$ (for $\\lambda=0$) to $(1-\\text{Sp}, \\text{Se})$ (for $\\lambda=1$).\n\nThe slope of this segment is $m_1 = \\frac{\\Delta \\text{TPR}}{\\Delta \\text{FPR}} = \\frac{\\text{Se} - 0}{(1-\\text{Sp})-0} = \\frac{\\text{Se}}{1-\\text{Sp}}$.\nThis is the ratio of the true positive rate to the false positive rate, which is the definition of the positive Likelihood Ratio, $\\text{LR}^+$. So, $m_1 = \\text{LR}^+$.\n\nCase 2: Randomization between the binary test ($P_B$) and the always-positive classifier ($P_{pos}$).\nLet the composite test be $T_2$. With probability $\\lambda \\in [0,1]$, we use the binary test, and with probability $1-\\lambda$, we use the always-positive rule.\n\nThe TPR of $T_2$ is $\\text{TPR}_2 = P(T_2 = + \\mid D)$:\n$$ \\text{TPR}_2 = P(\\text{Test}=+\\mid D) \\cdot \\lambda + P(\\text{always pos. test}=+\\mid D) \\cdot (1-\\lambda) $$\n$$ \\text{TPR}_2 = \\text{Se} \\cdot \\lambda + 1 \\cdot (1-\\lambda) = 1 - \\lambda + \\lambda\\,\\text{Se} $$\nThe FPR of $T_2$ is $\\text{FPR}_2 = P(T_2 = + \\mid D^c)$:\n$$ \\text{FPR}_2 = P(\\text{Test}=+\\mid D^c) \\cdot \\lambda + P(\\text{always pos. test}=+\\mid D^c) \\cdot (1-\\lambda) $$\n$$ \\text{FPR}_2 = (1-\\text{Sp}) \\cdot \\lambda + 1 \\cdot (1-\\lambda) = 1 - \\lambda + \\lambda(1-\\text{Sp}) $$\nThe operating points are $(1 - \\lambda + \\lambda(1-\\text{Sp}), 1 - \\lambda + \\lambda\\,\\text{Se})$ for $\\lambda \\in [0,1]$. This is the parametric equation of a line segment connecting $(1 - \\text{Sp}, \\text{Se})$ (for $\\lambda=1$) to $(1,1)$ (for $\\lambda=0$).\n\nThe slope of this segment is $m_2 = \\frac{\\Delta \\text{TPR}}{\\Delta \\text{FPR}} = \\frac{1 - \\text{Se}}{1 - (1-\\text{Sp})} = \\frac{1-\\text{Se}}{\\text{Sp}}$.\nThis is the ratio of the false negative rate ($1-\\text{Se}$) to the true negative rate ($\\text{Sp}$), which is the definition of the negative Likelihood Ratio, $\\text{LR}^-$. So, $m_2 = \\text{LR}^-$.\n\nGeometric Structure and Area Under the Curve (AUC):\nThe set of all achievable operating points by randomizing between the binary test and the trivial classifiers forms the convex hull of the points $(0,0)$, $(1-\\text{Sp}, \\text{Se})$, and $(1,1)$. This is a piecewise linear ROC curve consisting of the two line segments derived above.\n\nThe Area Under this Curve (AUC) can be calculated geometrically. It is the area of a polygon with vertices $(0,0)$, $(1-\\text{Sp}, 0)$, $(1-\\text{Sp}, \\text{Se})$, $(1,1)$, and $(1,0)$. This can be decomposed into a triangle and a trapezoid.\nArea of the first part (triangle): base $1-\\text{Sp}$, height $\\text{Se}$.\n$$ A_1 = \\frac{1}{2} \\cdot \\text{base} \\cdot \\text{height} = \\frac{1}{2}(1-\\text{Sp})\\text{Se} $$\nArea of the second part (trapezoid): parallel sides of lengths $\\text{Se}$ and $1$, width $\\text{Sp}$.\n$$ A_2 = \\frac{1}{2}(\\text{side}_1 + \\text{side}_2) \\cdot \\text{width} = \\frac{1}{2}(\\text{Se} + 1)\\text{Sp} $$\nThe total AUC is:\n$$ \\text{AUC} = A_1 + A_2 = \\frac{1}{2}(1-\\text{Sp})\\text{Se} + \\frac{1}{2}(\\text{Se}+1)\\text{Sp} $$\n$$ \\text{AUC} = \\frac{1}{2}\\text{Se} - \\frac{1}{2}\\text{Se}\\cdot\\text{Sp} + \\frac{1}{2}\\text{Se}\\cdot\\text{Sp} + \\frac{1}{2}\\text{Sp} $$\n$$ \\text{AUC} = \\frac{\\text{Se} + \\text{Sp}}{2} $$\n\nNow, we evaluate each option.\n\nA. This option states that randomizing with the always-negative classifier produces points $(\\lambda(1 - \\text{Sp}), \\lambda\\,\\text{Se})$, forming the line segment to $(0,0)$ with slope $\\frac{\\text{Se}}{1 - \\text{Sp}} = \\text{LR}^+$. It states that randomizing with the always-positive classifier produces points $(1 - \\lambda + \\lambda(1 - \\text{Sp}), 1 - \\lambda + \\lambda\\,\\text{Se})$, forming the line segment to $(1,1)$ with slope $\\frac{1 - \\text{Se}}{\\text{Sp}} = \\text{LR}^-$. Finally, it states that the AUC of the resulting piecewise linear ROC is $\\frac{\\text{Se} + \\text{Sp}}{2}$. All these statements perfectly match our derivation.\n**Verdict: Correct**\n\nB. This option gives an incorrect formula for the ROC points from the first randomization: $(\\lambda(1 - \\text{Sp}), 1 - \\lambda(1 - \\text{Se}))$. The derived TPR is $\\lambda\\text{Se}$, not $1 - \\lambda(1 - \\text{Se})$. It incorrectly claims the result is a curve, when it is a line segment. It gives an incorrect slope of $\\frac{\\text{Sp}}{\\text{Se}}$ (the correct slope is $\\frac{\\text{Se}}{1-\\text{Sp}}$). It gives an incorrect AUC of $\\text{Se}\\times \\text{Sp}$ (the correct AUC is $\\frac{\\text{Se}+\\text{Sp}}{2}$).\n**Verdict: Incorrect**\n\nC. This option claims randomization does not change the ROC location, which is fundamentally false as demonstrated by our derivation. It incorrectly states the AUC is $0$. It also incorrectly claims the randomized points lie outside the unit square, whereas our derivation shows they form line segments entirely within the unit square (as they must, being valid probabilities).\n**Verdict: Incorrect**\n\nD. This option swaps the slopes of the two line segments. It claims the slope of the segment from $(0,0)$ to $(1-\\text{Sp}, \\text{Se})$ is $\\text{LR}^-$, and the slope of the segment from $(1-\\text{Sp}, \\text{Se})$ to $(1,1)$ is $\\text{LR}^+$. Our derivation shows the reverse is true. It also provides an incorrect formula for the AUC, $1 - \\frac{\\text{Se} + \\text{Sp}}{2}$, instead of the correct $\\frac{\\text{Se} + \\text{Sp}}{2}$.\n**Verdict: Incorrect**\n\nBased on the rigorous derivation, only option A is an accurate description of the resulting ROC geometry and associated quantities.", "answer": "$$\\boxed{A}$$", "id": "4607874"}, {"introduction": "While the Area Under the Curve (AUC) is a popular metric for evaluating diagnostic models, it only measures discrimination and can sometimes be misleading. This problem [@problem_id:4607812] presents a scenario where two models with identical AUCs are not equally useful, forcing a deeper look into the concept of calibration. By calculating the decision-analytic net benefit, you will learn how to quantify a model's clinical utility at a specific risk threshold and appreciate why a well-calibrated model is crucial for making better decisions.", "problem": "An epidemiologic study considers two binary-outcome predictive models evaluated on a cohort of $N=20$ individuals indexed by $i=1,\\dots,20$. For Model $A$, you are given the predicted probabilities $p_{A,i}$ and observed outcomes $Y_i \\in \\{0,1\\}$ as follows:\n- $i=1$: $p_{A,1}=0.85$, $Y_1=1$\n- $i=2$: $p_{A,2}=0.80$, $Y_2=1$\n- $i=3$: $p_{A,3}=0.75$, $Y_3=1$\n- $i=4$: $p_{A,4}=0.70$, $Y_4=1$\n- $i=5$: $p_{A,5}=0.60$, $Y_5=1$\n- $i=6$: $p_{A,6}=0.55$, $Y_6=1$\n- $i=7$: $p_{A,7}=0.50$, $Y_7=1$\n- $i=8$: $p_{A,8}=0.45$, $Y_8=0$\n- $i=9$: $p_{A,9}=0.40$, $Y_9=1$\n- $i=10$: $p_{A,10}=0.35$, $Y_{10}=0$\n- $i=11$: $p_{A,11}=0.30$, $Y_{11}=0$\n- $i=12$: $p_{A,12}=0.25$, $Y_{12}=0$\n- $i=13$: $p_{A,13}=0.22$, $Y_{13}=1$\n- $i=14$: $p_{A,14}=0.18$, $Y_{14}=0$\n- $i=15$: $p_{A,15}=0.15$, $Y_{15}=0$\n- $i=16$: $p_{A,16}=0.12$, $Y_{16}=0$\n- $i=17$: $p_{A,17}=0.10$, $Y_{17}=0$\n- $i=18$: $p_{A,18}=0.08$, $Y_{18}=0$\n- $i=19$: $p_{A,19}=0.06$, $Y_{19}=0$\n- $i=20$: $p_{A,20}=0.04$, $Y_{20}=0$\n\nModel $B$ is defined by a monotone transformation of Model $A$’s outputs, $p_{B,i}=(p_{A,i})^2$, so that the ranking of individuals by risk is identical between models, but the numerical calibration differs. Consider classification by a single threshold probability $p_t=0.20$: classify an individual as test-positive if the model’s predicted probability is at least $p_t$, and test-negative otherwise. Assume decisions based on this classification aim to maximize decision-analytic net benefit at threshold $p_t$.\n\nUsing only fundamental definitions of sensitivity and specificity, Receiver Operating Characteristic (ROC) ranking, Area Under the ROC Curve (AUC), likelihood ratios, and threshold-based decision analysis, determine which option correctly states which model yields the greater decision-analytic net benefit at the threshold $p_t=0.20$, and why. Note that Model $A$ is well-calibrated to these data, while Model $B$ is miscalibrated due to its transformation.\n\nChoose one:\n\n- A. Model $A$ yields higher net benefit at $p_t=0.20$ because, despite identical Area Under the Receiver Operating Characteristic Curve (AUC), its calibration places more true cases above the threshold with an acceptable increase in false positives given the $p_t$ trade-off.\n\n- B. Model $B$ yields higher net benefit at $p_t=0.20$ because its higher specificity and larger positive likelihood ratio always imply higher net benefit at any threshold.\n\n- C. Both models yield equal net benefit at $p_t=0.20$ because equal AUC implies equal performance at any threshold.\n\n- D. Model $B$ yields higher net benefit at $p_t=0.20$ because miscalibration that lowers probabilities reduces overtreatment and thus improves net benefit whenever AUCs are equal.", "solution": "The problem asks to determine which of two predictive models, Model $A$ or Model $B$, yields a higher decision-analytic net benefit at a specified probability threshold $p_t = 0.20$. The analysis will proceed by first validating the problem, then deriving the net benefit for each model based on fundamental definitions, and finally evaluating each option.\n\nFirst, we compile the necessary information from the problem statement. The cohort consists of $N=20$ individuals. We must determine the number of individuals with the outcome (cases, $Y=1$) and without the outcome (non-cases, $Y=0$).\n- Cases ($D$): Individuals with $Y_i=1$ are $i \\in \\{1, 2, 3, 4, 5, 6, 7, 9, 13\\}$. The total number of cases is $D = 9$.\n- Non-cases ($H$): Individuals with $Y_i=0$ are $i \\in \\{8, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20\\}$. The total number of non-cases is $H=11$.\nThe total number of subjects is $N = D+H = 9+11=20$, which is consistent with the problem statement.\n\nThe decision threshold is given as $p_t = 0.20$. An individual is classified as test-positive if their predicted probability is greater than or equal to $p_t$, and test-negative otherwise.\n\nThe decision-analytic net benefit (NB) at a threshold $p_t$ is defined as:\n$$NB(p_t) = \\frac{TP}{N} - \\frac{FP}{N} \\cdot \\frac{p_t}{1-p_t}$$\nwhere $TP$ is the number of true positives, $FP$ is the number of false positives, and $N$ is the total number of individuals. The term $\\frac{p_t}{1-p_t}$ represents the odds of the outcome at the threshold, which defines the relative cost of a false positive to a true positive. For $p_t=0.20$, this trade-off is $\\frac{0.20}{1-0.20} = \\frac{0.20}{0.80} = \\frac{1}{4}$.\n\n**Analysis of Model A**\nFor Model $A$, an individual is classified as test-positive if $p_{A,i} \\ge 0.20$. We examine the provided data to find which individuals meet this criterion and determine if they are true or false positives.\n- Individuals with $p_{A,i} \\ge 0.20$: These are individuals $i = 1, \\dots, 13$.\n- True Positives for Model A ($TP_A$): These are cases ($Y_i=1$) with $p_{A,i} \\ge 0.20$.\n  - Individuals $1, 2, 3, 4, 5, 6, 7, 9, 13$ are cases. All have $p_{A,i} \\ge 0.22 > 0.20$.\n  - Thus, $TP_A = 9$.\n- False Positives for Model A ($FP_A$): These are non-cases ($Y_i=0$) with $p_{A,i} \\ge 0.20$.\n  - Individuals $8, 10, 11, 12$ are non-cases. Their probabilities are $0.45, 0.35, 0.30, 0.25$ respectively, all of which are $\\ge 0.20$.\n  - Thus, $FP_A = 4$.\n\nNow, we calculate the net benefit for Model A at $p_t=0.20$:\n$$NB_A(0.20) = \\frac{TP_A}{N} - \\frac{FP_A}{N} \\cdot \\frac{0.20}{1-0.20} = \\frac{9}{20} - \\frac{4}{20} \\cdot \\frac{1}{4}$$\n$$NB_A(0.20) = \\frac{9}{20} - \\frac{1}{20} = \\frac{8}{20} = 0.40$$\n\n**Analysis of Model B**\nFor Model B, the predicted probabilities are $p_{B,i} = (p_{A,i})^2$. An individual is classified as test-positive if $p_{B,i} \\ge 0.20$. This condition is equivalent to $(p_{A,i})^2 \\ge 0.20$, which simplifies to $p_{A,i} \\ge \\sqrt{0.20}$.\n$\\sqrt{0.20} = \\sqrt{1/5} = 1/\\sqrt{5} \\approx 0.4472$.\nSo, for Model B, the classification rule is equivalent to classifying based on Model A's probabilities with a higher effective threshold of approximately $0.4472$.\n\nWe find the individuals who meet the criterion $p_{A,i} \\ge 0.4472$:\n- Individuals with $p_{A,i} \\ge 0.4472$: These are individuals $i = 1, \\dots, 8$.\n- True Positives for Model B ($TP_B$): These are cases ($Y_i=1$) with $p_{A,i} \\ge 0.4472$.\n  - Individuals $1, 2, 3, 4, 5, 6, 7$ are cases with probabilities $\\ge 0.50$.\n  - Thus, $TP_B = 7$.\n- False Positives for Model B ($FP_B$): These are non-cases ($Y_i=0$) with $p_{A,i} \\ge 0.4472$.\n  - Individual $8$ is a non-case with $p_{A,8} = 0.45 \\ge 0.4472$.\n  - Thus, $FP_B = 1$.\n\nNow, we calculate the net benefit for Model B. It is critical to use the decision threshold $p_t=0.20$ in the net benefit formula, as this value reflects the decision-maker's preference, not the model's internal probability scale.\n$$NB_B(0.20) = \\frac{TP_B}{N} - \\frac{FP_B}{N} \\cdot \\frac{0.20}{1-0.20} = \\frac{7}{20} - \\frac{1}{20} \\cdot \\frac{1}{4}$$\n$$NB_B(0.20) = \\frac{7}{20} - \\frac{1}{80} = \\frac{28}{80} - \\frac{1}{80} = \\frac{27}{80} = 0.3375$$\n\n**Comparison of Net Benefits**\nComparing the two models, we find $NB_A(0.20) = 0.40$ and $NB_B(0.20) = 0.3375$.\nClearly, $NB_A(0.20) > NB_B(0.20)$. Model A provides a greater net benefit at the decision threshold $p_t=0.20$.\n\n**Evaluation of Options**\n\n- **A. Model $A$ yields higher net benefit at $p_t=0.20$ because, despite identical Area Under the Receiver Operating Characteristic Curve (AUC), its calibration places more true cases above the threshold with an acceptable increase in false positives given the $p_t$ trade-off.**\n  - The conclusion \"Model $A$ yields higher net benefit at $p_t=0.20$\" is consistent with our calculation ($0.40 > 0.3375$).\n  - The claim of \"identical Area Under the Receiver Operating Characteristic Curve (AUC)\" is correct. Since $p_{B,i}=(p_{A,i})^2$ is a strictly monotone transformation, the rank ordering of individuals by predicted probability is identical for both models. Since the ROC curve is constructed based on this rank ordering, the ROC curves—and thus the AUCs—are identical.\n  - The claim that Model A \"places more true cases above the threshold\" is correct ($TP_A=9$ vs $TP_B=7$).\n  - The claim of an \"acceptable increase in false positives\" is also correct. Model A has $FP_A=4$ versus $FP_B=1$, an increase of $3$. This trade-off (gaining $2$ TPs for an additional $3$ FPs) results in a higher net benefit, making it \"acceptable\" in the context of the $p_t=0.20$ decision threshold. The superior calibration of Model A makes its raw probability output a better guide for action at the given threshold $p_t$. This option is fully consistent with the analysis.\n  - Verdict: **Correct**.\n\n- **B. Model $B$ yields higher net benefit at $p_t=0.20$ because its higher specificity and larger positive likelihood ratio always imply higher net benefit at any threshold.**\n  - The premise \"Model $B$ yields higher net benefit\" is false.\n  - Let's check the ancillary claims for the operating points defined by the threshold $p_t=0.20$. The sensitivity and specificity for Model A are $TPR_A = TP_A/D = 9/9 = 1.0$ and $Spec_A = (H-FP_A)/H = (11-4)/11 = 7/11$. The false positive rate is $FPR_A = 1 - Spec_A = 4/11$. For Model B, $TPR_B = 7/9$ and $Spec_B = (11-1)/11=10/11$. So, $Spec_B > Spec_A$. The positive likelihood ratios are $LR+_A = TPR_A/FPR_A = 1.0 / (4/11) = 2.75$ and $LR+_B = TPR_B/FPR_B = (7/9)/(1/11) = 77/9 \\approx 8.56$. So $LR+_B > LR+_A$. While these two claims are correct for the specific classification points, the general assertion that this \"always imply higher net benefit at any threshold\" is incorrect. Net benefit depends on the specific trade-off between TPR and FPR, which is governed by $p_t$.\n  - Verdict: **Incorrect**.\n\n- **C. Both models yield equal net benefit at $p_t=0.20$ because equal AUC implies equal performance at any threshold.**\n  - The premise \"Both models yield equal net benefit\" is false.\n  - The reasoning \"equal AUC implies equal performance at any threshold\" is a fundamental misunderstanding of these metrics. AUC measures overall discrimination (ranking ability) across all thresholds, not performance at a single, specific threshold. Calibration, which affects performance at a specific threshold, is not captured by AUC.\n  - Verdict: **Incorrect**.\n\n- **D. Model $B$ yields higher net benefit at $p_t=0.20$ because miscalibration that lowers probabilities reduces overtreatment and thus improves net benefit whenever AUCs are equal.**\n  - The premise \"Model $B$ yields higher net benefit\" is false.\n  - The reasoning is flawed. While Model B's miscalibration does lead to fewer false positives ($FP_B=1$ vs $FP_A=4$)—reducing what could be termed \"overtreatment\"—it does so at the cost of missing true positives ($TP_B=7$ vs $TP_A=9$). The net effect on utility, as measured by net benefit, is negative in this case. Reducing FPs does not automatically improve net benefit; the balance with TPs is what matters.\n  - Verdict: **Incorrect**.\n\nIn conclusion, Model A yields a higher net benefit because it is better calibrated. Applying the decision threshold $p_t=0.20$ to the well-calibrated probabilities of Model A leads to a more optimal classification decision compared to applying the same threshold to the miscalibrated probabilities of Model B.", "answer": "$$\\boxed{A}$$", "id": "4607812"}]}