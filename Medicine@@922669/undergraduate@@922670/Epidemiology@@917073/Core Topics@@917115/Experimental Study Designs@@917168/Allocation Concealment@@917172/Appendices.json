{"hands_on_practices": [{"introduction": "To appreciate the importance of allocation concealment, we must first understand how randomization schemes can become predictable. While permuted block randomization is excellent for maintaining balance between treatment arms, using small, fixed block sizes can unintentionally compromise concealment. This first exercise provides a hands-on demonstration of this vulnerability by asking you to calculate the probability of correctly guessing a future assignment within a common block design, revealing its deterministic nature under certain conditions. [@problem_id:4570947]", "problem": "A two-arm randomized controlled trial (RCT) employs permuted block randomization with block size $4$ and equal allocation within each block. Equal allocation enforces that each block contains exactly $2$ assignments to treatment $T$ and $2$ assignments to control $C$. The randomization procedure is defined as a uniform random permutation over all distinct sequences formed from the multiset $\\{T, T, C, C\\}$, so that every admissible block sequence has equal probability.\n\nConsider a recruiter who is aware of the block size $4$, the equal allocation constraint ($2$ of $T$ and $2$ of $C$ per block), and that sequences are uniformly permuted as described, but has no access to the concealed sequence itself. The recruiter observes, in real time, the first $3$ assignments of a given block and aims to predict the fourth assignment before it is revealed.\n\nUsing only fundamental definitions of randomization and conditional probability, and without assuming any additional information beyond the stated design, compute the probability that the recruiter can correctly predict the fourth assignment given the observation of the first $3$ assignments within the block. Express your final answer as a single real number. No rounding is required.", "solution": "The problem asks for the probability that a recruiter can correctly predict the fourth and final assignment in a permuted block of size $4$, given the trial's design parameters.\n\nLet $T$ denote the treatment allocation and $C$ denote the control allocation. The problem states that each block has a fixed size of $4$ and is composed of exactly $2$ assignments to $T$ and $2$ assignments to $C$. The set of elements for any block is thus the multiset $\\{T, T, C, C\\}$.\n\nThe randomization procedure consists of a uniform random permutation of these elements. The total number of distinct sequences (blocks) that can be formed is given by the multinomial coefficient:\n$$N = \\binom{4}{2, 2} = \\frac{4!}{2! \\cdot 2!} = \\frac{24}{4} = 6$$\nSince the permutation is uniform, each of these $6$ distinct sequences has an equal probability of being chosen, which is $P(\\text{sequence}) = \\frac{1}{6}$. The $6$ possible sequences are:\n$1$. $TTCC$\n$2$. $TCTC$\n$3$. $TCCT$\n$4$. $CTTC$\n$5$. $CTCT$\n$6$. $CCTT$\n\nThe recruiter knows the block size ($4$) and the allocation rule ($2$ of $T$ and $2$ of $C$). The recruiter's task is to predict the fourth assignment, denoted $a_4$, after observing the first three assignments, $(a_1, a_2, a_3)$.\n\nLet $E$ be the event that the recruiter's prediction for the fourth assignment is correct. We want to compute the probability $P(E)$. A rational recruiter will use all available information to make the best possible prediction. The information consists of the observed sequence $(a_1, a_2, a_3)$ and the known constraints on the block composition.\n\nLet $N_T(k)$ and $N_C(k)$ be the number of $T$ and $C$ assignments, respectively, among the first $k$ positions in the block. The recruiter observes $(a_1, a_2, a_3)$ and can therefore count $N_T(3)$ and $N_C(3)$. The defining constraint of the block is that $N_T(4) = 2$ and $N_C(4) = 2$.\n\nAfter $3$ assignments have been observed, it must be that $N_T(3) + N_C(3) = 3$. Since the entire block can only contain $2$ of $T$ and $2$ of $C$, it is impossible for the first three assignments to be all $T$ (i.e., $N_T(3)=3$) or all $C$ (i.e., $N_C(3)=3$). Therefore, there are only two possible scenarios for the composition of the first three observed assignments:\n\nCase 1: The first three assignments consist of two $T$'s and one $C$.\nIn this case, $N_T(3) = 2$ and $N_C(3) = 1$. The recruiter knows that for the complete block of four, $N_T(4)$ must be $2$. Since $N_T(4) = N_T(3) + (\\text{count of } T \\text{ in } a_4)$, we have $2 = 2 + (\\text{count of } T \\text{ in } a_4)$, which implies the fourth assignment $a_4$ cannot be $T$.\nSimultaneously, the recruiter knows $N_C(4) = 2$. Since $N_C(4) = N_C(3) + (\\text{count of } C \\text{ in } a_4)$, we have $2 = 1 + (\\text{count of } C \\text{ in } a_4)$, which implies the fourth assignment $a_4$ must be $C$.\nThe fourth assignment is therefore uniquely determined to be $C$. The recruiter, using this logic, will predict $C$. As this conclusion is a logical necessity of the block design, the prediction is guaranteed to be correct.\n\nCase 2: The first three assignments consist of one $T$ and two $C$'s.\nIn this case, $N_T(3) = 1$ and $N_C(3) = 2$. Using the same logic as above, the recruiter can deduce the identity of $a_4$.\nFrom the constraint $N_T(4) = 2$, we have $2 = 1 + (\\text{count of } T \\text{ in } a_4)$, which implies $a_4$ must be $T$.\nFrom the constraint $N_C(4) = 2$, we have $2 = 2 + (\\text{count of } C \\text{ in } a_4)$, which implies $a_4$ cannot be $C$.\nThe fourth assignment is therefore uniquely determined to be $T$. The recruiter will predict $T$, and this prediction is guaranteed to be correct.\n\nIn every possible situation, after observing the first three assignments, the identity of the fourth is no longer probabilistic; it is deterministic. The recruiter's prediction, based on the known rules, will thus be correct with certainty.\n\nTo formalize this using conditional probability, let $O_3$ be the random variable for the observed sequence of the first three assignments, and let $o$ be a specific outcome for $O_3$. The event $E$ is that the prediction is correct. We are calculating $P(E)$. By the Law of Total Probability:\n$$P(E) = \\sum_{o} P(E | O_3 = o) P(O_3 = o)$$\nThe sum is over all possible outcomes $o$ for the first three assignments. As we have shown, for any possible observed sequence $o$ (which will fall into either Case 1 or Case 2), the recruiter can predict the fourth assignment with certainty. This means the conditional probability of being correct, given any specific observation, is $1$:\n$$P(E | O_3 = o) = 1 \\quad \\forall o$$\nTherefore, the equation becomes:\n$$P(E) = \\sum_{o} (1) \\cdot P(O_3 = o) = \\sum_{o} P(O_3 = o)$$\nThe sum of the probabilities of all possible disjoint outcomes for the random variable $O_3$ is, by definition, equal to $1$.\n$$P(E) = 1$$\nThe probability that the recruiter can correctly predict the fourth assignment is $1$.", "answer": "$$\\boxed{1}$$", "id": "4570947"}, {"introduction": "Having established that some randomization schemes can be predictable, we now turn to the critical question: what is the consequence of this predictability? When recruiters anticipate an upcoming treatment allocation, they may consciously or unconsciously enroll patients with different baseline characteristics, leading to selection bias. This exercise will guide you through a fundamental derivation in epidemiology, allowing you to quantify exactly how this selection bias translates into a biased estimate of the true treatment effect. [@problem_id:4571000]", "problem": "Consider a two-arm Randomized Controlled Trial (RCT) evaluating the effect of a treatment on a binary outcome. Due to imperfect allocation concealment, clinicians anticipate treatment assignments and selectively enroll patients, leading to a systematic difference in baseline risk between arms. Let $T \\in \\{0,1\\}$ denote the treatment indicator, $X$ a baseline risk score measured before treatment assignment, and $Y$ the outcome. Assume the following structural expectation model holds for all participants:\n$$\nE[Y \\mid T, X] = \\alpha + \\Delta T + \\beta X,\n$$\nwhere $\\alpha$ is a baseline intercept, $\\Delta$ is the causal effect of treatment on the outcome, and $\\beta$ captures the association between baseline risk and the outcome.\n\nSuppose allocation concealment failure induces a systematic difference in the mean baseline risk, such that\n$$\nE[X \\mid T=1] = E[X \\mid T=0] + \\delta,\n$$\nwith $\\delta$ a known, nonzero constant representing the increase in mean baseline risk in the treated arm due to selection. Assume a large sample so that the difference-in-means estimator equals the difference in arm-specific conditional expectations,\n$$\n\\hat{\\Delta} = E[Y \\mid T=1] - E[Y \\mid T=0].\n$$\n\nStarting only from the laws of conditional expectation and the given outcome model, derive the large-sample expression for $\\hat{\\Delta}$ as a closed-form function of $\\Delta$, $\\beta$, and $\\delta$. Express your final answer as a single analytic expression. No rounding is required.", "solution": "The objective is to derive an expression for the large-sample difference-in-means estimator, $\\hat{\\Delta}$, defined as:\n$$\n\\hat{\\Delta} = E[Y \\mid T=1] - E[Y \\mid T=0]\n$$\nWe will derive expressions for the two terms on the right-hand side, $E[Y \\mid T=1]$ and $E[Y \\mid T=0]$, using the Law of Iterated Expectations. The law states that for random variables $A$ and $B$, $E[A] = E[E[A \\mid B]]$. We apply this principle by conditioning on the treatment assignment $T$.\n\nFirst, let us evaluate the expected outcome in the treatment arm, $E[Y \\mid T=1]$. By the Law of Iterated Expectations, we can express this by conditioning on the baseline risk score $X$:\n$$\nE[Y \\mid T=1] = E[E[Y \\mid T=1, X] \\mid T=1]\n$$\nThe problem provides the inner conditional expectation, $E[Y \\mid T, X] = \\alpha + \\Delta T + \\beta X$. Substituting $T=1$ into this model gives:\n$$\nE[Y \\mid T=1, X] = \\alpha + \\Delta(1) + \\beta X = \\alpha + \\Delta + \\beta X\n$$\nNow, we substitute this back into the iterated expectation expression:\n$$\nE[Y \\mid T=1] = E[\\alpha + \\Delta + \\beta X \\mid T=1]\n$$\nUsing the linearity of the expectation operator, we can distribute the expectation:\n$$\nE[Y \\mid T=1] = E[\\alpha \\mid T=1] + E[\\Delta \\mid T=1] + E[\\beta X \\mid T=1]\n$$\nSince $\\alpha$, $\\Delta$, and $\\beta$ are defined as constants, they are unaffected by the expectation operator. Thus, we have:\n$$\nE[Y \\mid T=1] = \\alpha + \\Delta + \\beta E[X \\mid T=1]\n$$\n\nNext, we follow an analogous procedure to evaluate the expected outcome in the control arm, $E[Y \\mid T=0]$. Again, we apply the Law of Iterated Expectations:\n$$\nE[Y \\mid T=0] = E[E[Y \\mid T=0, X] \\mid T=0]\n$$\nSubstituting $T=0$ into the given structural expectation model:\n$$\nE[Y \\mid T=0, X] = \\alpha + \\Delta(0) + \\beta X = \\alpha + \\beta X\n$$\nSubstituting this into the iterated expectation expression for the control arm:\n$$\nE[Y \\mid T=0] = E[\\alpha + \\beta X \\mid T=0]\n$$\nBy linearity of expectation, and since $\\alpha$ and $\\beta$ are constants:\n$$\nE[Y \\mid T=0] = \\alpha + \\beta E[X \\mid T=0]\n$$\n\nNow we have expressions for the expected outcomes in both arms. We can substitute these into the definition of the estimator $\\hat{\\Delta}$:\n$$\n\\hat{\\Delta} = E[Y \\mid T=1] - E[Y \\mid T=0]\n$$\n$$\n\\hat{\\Delta} = (\\alpha + \\Delta + \\beta E[X \\mid T=1]) - (\\alpha + \\beta E[X \\mid T=0])\n$$\nThe intercept term $\\alpha$ cancels out:\n$$\n\\hat{\\Delta} = \\Delta + \\beta E[X \\mid T=1] - \\beta E[X \\mid T=0]\n$$\nFactoring out the common term $\\beta$:\n$$\n\\hat{\\Delta} = \\Delta + \\beta (E[X \\mid T=1] - E[X \\mid T=0])\n$$\nFinally, we use the given relationship for the systematic difference in mean baseline risk between the arms:\n$$\nE[X \\mid T=1] = E[X \\mid T=0] + \\delta\n$$\nThis implies that the difference is:\n$$\nE[X \\mid T=1] - E[X \\mid T=0] = \\delta\n$$\nSubstituting this difference $\\delta$ into our expression for $\\hat{\\Delta}$ yields the final result:\n$$\n\\hat{\\Delta} = \\Delta + \\beta \\delta\n$$\nThis expression shows that the naive difference-in-means estimator, $\\hat{\\Delta}$, is a biased estimator of the true causal effect $\\Delta$. The bias term is $\\beta \\delta$, which is the product of the association between the baseline risk factor and the outcome ($\\beta$) and the systematic difference in the mean baseline risk factor between the treatment arms ($\\delta$). The bias is nonzero because both $\\beta$ and $\\delta$ are nonzero.", "answer": "$$\n\\boxed{\\Delta + \\beta \\delta}\n$$", "id": "4571000"}, {"introduction": "In practice, the exact magnitude of selection bias is unknown, making it difficult to correct the final results of a trial. Epidemiologists therefore use sensitivity analysis to explore how robust a study's conclusions are to potential biases. This advanced exercise puts you in the role of a researcher, modeling a plausible mechanism for selection bias and deriving a formula for the induced bias as a function of a \"selection strength\" parameter, a powerful technique for critically appraising trial evidence. [@problem_id:4570929]", "problem": "Consider an individually randomized controlled trial (randomization ratio $1:1$) evaluating the effect of a binary treatment $T \\in \\{0,1\\}$ on a continuous outcome $Y$. Let $X \\in \\{0,1\\}$ denote a binary baseline prognostic covariate with population prevalence $\\Pr(X=1)=p$ among all eligible participants. Assume the stable unit treatment value assumption (SUTVA) and consistency hold, and that, under ideal conduct with perfect allocation concealment, treatment assignment $T$ is independent of $X$ within the eligible population.\n\nSuppose the structural outcome model is additive and given by $\\mathbb{E}[Y \\mid T, X] = \\alpha + \\beta X + \\delta T$, where $\\delta$ is the true average treatment effect (ATE) in the eligible population.\n\nNow consider a breach of allocation concealment: recruiters can observe the upcoming assignment and selectively enroll patients based on their covariate $X$ and the predicted assignment. Let $S \\in \\{0,1\\}$ indicate enrollment. Model the enrollment mechanism by the odds-weighting function\n$$w(T,X) = \\gamma^{T X},$$\nso that, relative to other combinations, the odds of enrollment is multiplied by $\\gamma$ only for individuals with $X=1$ when the predicted assignment is $T=1$; otherwise the odds are unchanged. Assume randomization remains $1:1$ among those enrolled.\n\nDefine the naive difference-in-means estimator computed among the enrolled as\n$$\\widehat{\\Delta}_{\\text{naive}} = \\overline{Y}_{T=1,S=1} - \\overline{Y}_{T=0,S=1}.$$\nStarting from the core definitions of randomization and conditional expectation, derive the induced bias\n$$B(\\gamma) = \\mathbb{E}[\\widehat{\\Delta}_{\\text{naive}}] - \\delta$$\nas a closed-form function of $\\gamma$, $p$, and $\\beta$ under the enrollment mechanism $w(T,X)$. Then, for a sensitivity analysis at $\\gamma=3$ with $p=0.3$ and $\\beta=4$, compute the numerical value of $B(3)$. Express the final answer as a real number. If you use approximations, round your answer to four significant figures; otherwise provide the exact value.", "solution": "The objective is to derive the bias of the naive estimator, defined as $B(\\gamma) = \\mathbb{E}[\\widehat{\\Delta}_{\\text{naive}}] - \\delta$.\nThe expectation of the naive estimator, in a large sample context, is the difference in the expected outcomes between the treated and control groups among the enrolled population ($S=1$):\n$$ \\mathbb{E}[\\widehat{\\Delta}_{\\text{naive}}] = \\mathbb{E}[Y \\mid T=1, S=1] - \\mathbb{E}[Y \\mid T=0, S=1] $$\nWe will compute each of these two conditional expectations separately.\n\nThe structural outcome model is given as $\\mathbb{E}[Y \\mid T, X] = \\alpha + \\beta X + \\delta T$. A standard assumption in such sensitivity analyses is that enrollment $S$ is independent of the outcome $Y$ conditional on treatment $T$ and the covariate $X$, i.e., $Y \\perp S \\mid (T, X)$. This implies that $\\mathbb{E}[Y \\mid T, S=1, X] = \\mathbb{E}[Y \\mid T, X]$.\n\nUsing the law of total expectation, we can express the conditional expectations of $Y$ in terms of the conditional expectation of the covariate $X$:\n$$ \\mathbb{E}[Y \\mid T=t, S=1] = \\mathbb{E}[\\mathbb{E}[Y \\mid T=t, S=1, X] \\mid T=t, S=1] $$\n$$ \\mathbb{E}[Y \\mid T=t, S=1] = \\mathbb{E}[\\mathbb{E}[Y \\mid T=t, X] \\mid T=t, S=1] $$\n$$ \\mathbb{E}[Y \\mid T=t, S=1] = \\mathbb{E}[\\alpha + \\beta X + \\delta t \\mid T=t, S=1] $$\n$$ \\mathbb{E}[Y \\mid T=t, S=1] = \\alpha + \\delta t + \\beta \\mathbb{E}[X \\mid T=t, S=1] $$\nThe problem thus reduces to finding the expected value of the covariate $X$ within each treatment arm of the enrolled population, $\\mathbb{E}[X \\mid T=t, S=1]$.\n\n$\\mathbb{E}[X \\mid T=t, S=1]$ is simply the prevalence of the covariate $X=1$ in the specified subgroup, $\\Pr(X=1 \\mid T=t, S=1)$. We derive this probability by modeling the selection process.\n\nThe problem states that recruiters can see the upcoming assignment ($T=0$ or $T=1$) and then selectively enroll patients based on their covariate $X$. The constraint that randomization remains $1:1$ among the enrolled implies that two separate recruitment processes occur to fill the two equally sized treatment arms.\n\nFor the control arm ($T=0$):\nThe odds-weighting function is $w(T,X) = \\gamma^{TX}$. For $T=0$, the weights are $w(0,0)=\\gamma^{0}=1$ and $w(0,1)=\\gamma^{0}=1$. Since the weights are identical for both levels of the covariate $X$, there is no selective enrollment in the control arm. The distribution of $X$ in the control arm mirrors that of the eligible population.\n$$ \\Pr(X=1 \\mid T=0, S=1) = \\Pr(X=1) = p $$\n\nFor the treatment arm ($T=1$):\nThe weights are $w(1,0)=\\gamma^{0}=1$ and $w(1,1)=\\gamma^{1}=\\gamma$. The selection mechanism gives preferential enrollment to individuals with $X=1$ (assuming $\\gamma>1$) when the allocation is for the treatment arm. The probability of an individual having $X=x$ in the enrolled treatment group is proportional to their prevalence in the eligible population, $\\Pr(X=x)$, multiplied by the selection weight $w(1,x)$.\n$$ \\Pr(X=1 \\mid T=1, S=1) = \\frac{\\Pr(X=1) w(1,1)}{\\Pr(X=1) w(1,1) + \\Pr(X=0) w(1,0)} $$\nGiven $\\Pr(X=1)=p$ and $\\Pr(X=0)=1-p$:\n$$ \\Pr(X=1 \\mid T=1, S=1) = \\frac{p \\cdot \\gamma}{p \\cdot \\gamma + (1-p) \\cdot 1} = \\frac{\\gamma p}{1-p+\\gamma p} $$\n\nNow we can calculate the expected outcomes in the enrolled groups.\nFor the control arm:\n$$ \\mathbb{E}[X \\mid T=0, S=1] = \\Pr(X=1 \\mid T=0, S=1) = p $$\n$$ \\mathbb{E}[Y \\mid T=0, S=1] = \\alpha + \\beta \\mathbb{E}[X \\mid T=0, S=1] = \\alpha + \\beta p $$\n\nFor the treatment arm:\n$$ \\mathbb{E}[X \\mid T=1, S=1] = \\Pr(X=1 \\mid T=1, S=1) = \\frac{\\gamma p}{1-p+\\gamma p} $$\n$$ \\mathbb{E}[Y \\mid T=1, S=1] = \\alpha + \\delta + \\beta \\mathbb{E}[X \\mid T=1, S=1] = \\alpha + \\delta + \\beta \\frac{\\gamma p}{1-p+\\gamma p} $$\n\nWe can now find the expectation of the naive estimator:\n$$ \\mathbb{E}[\\widehat{\\Delta}_{\\text{naive}}] = \\mathbb{E}[Y \\mid T=1, S=1] - \\mathbb{E}[Y \\mid T=0, S=1] $$\n$$ \\mathbb{E}[\\widehat{\\Delta}_{\\text{naive}}] = \\left(\\alpha + \\delta + \\beta \\frac{\\gamma p}{1-p+\\gamma p}\\right) - (\\alpha + \\beta p) $$\n$$ \\mathbb{E}[\\widehat{\\Delta}_{\\text{naive}}] = \\delta + \\beta \\left( \\frac{\\gamma p}{1-p+\\gamma p} - p \\right) $$\n\nThe bias $B(\\gamma)$ is the difference between this expectation and the true ATE $\\delta$:\n$$ B(\\gamma) = \\mathbb{E}[\\widehat{\\Delta}_{\\text{naive}}] - \\delta = \\beta \\left( \\frac{\\gamma p}{1-p+\\gamma p} - p \\right) $$\nTo obtain a more interpretable closed form, we combine the terms inside the parenthesis:\n$$ \\frac{\\gamma p}{1-p+\\gamma p} - \\frac{p(1-p+\\gamma p)}{1-p+\\gamma p} = \\frac{\\gamma p - p + p^2 - \\gamma p^2}{1-p+\\gamma p} $$\n$$ = \\frac{p(\\gamma - 1 + p - \\gamma p)}{1-p+\\gamma p} = \\frac{p[(\\gamma - 1) - p(\\gamma - 1)]}{1-p+\\gamma p} = \\frac{p(1-p)(\\gamma-1)}{1-p+\\gamma p} $$\nThus, the closed-form expression for the bias is:\n$$ B(\\gamma) = \\beta \\frac{p(1-p)(\\gamma-1)}{1-p+\\gamma p} $$\n\nFinally, we compute the numerical value of the bias for the sensitivity analysis with parameters $\\gamma=3$, $p=0.3$, and $\\beta=4$.\n$$ B(3) = 4 \\times \\frac{0.3 \\times (1-0.3) \\times (3-1)}{1-0.3 + 3 \\times 0.3} $$\n$$ B(3) = 4 \\times \\frac{0.3 \\times 0.7 \\times 2}{0.7 + 0.9} $$\n$$ B(3) = 4 \\times \\frac{0.42}{1.6} $$\n$$ B(3) = \\frac{1.68}{1.6} $$\n$$ B(3) = \\frac{168}{160} = \\frac{21 \\times 8}{20 \\times 8} = \\frac{21}{20} = 1.05 $$\nThe value is an exact rational number, so no rounding is required.", "answer": "$$\\boxed{1.05}$$", "id": "4570929"}]}