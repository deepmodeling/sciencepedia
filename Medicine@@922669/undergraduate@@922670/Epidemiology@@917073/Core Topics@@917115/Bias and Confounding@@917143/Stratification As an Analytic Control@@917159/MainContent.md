## Introduction
Stratification is a cornerstone of epidemiological analysis, offering a powerful method to approximate the rigor of an experiment within observational data. In settings where randomized controlled trials are not feasible, researchers face the persistent challenge of confounding, where a third variable creates a spurious association between an exposure and an outcome, leading to biased conclusions. This article demystifies stratification as a primary analytic tool to control for such confounding and to uncover crucial differences in effect across populations. Over the following sections, you will build a robust understanding of this technique. The first section, "Principles and Mechanisms," will lay the theoretical foundation, explaining how stratification works from both an intuitive and a formal causal inference perspective. Next, "Applications and Interdisciplinary Connections" will demonstrate its use in real-world scenarios, from classic study designs to modern [genetic epidemiology](@entry_id:171643) and health equity research. Finally, "Hands-On Practices" will provide you with the opportunity to apply these concepts to tangible data problems, solidifying your ability to use stratification effectively.

## Principles and Mechanisms

### The Core Principle: Conditioning to Emulate an Experiment

In an ideal scientific investigation, such as a perfectly executed randomized controlled trial, the groups receiving different treatments or exposures would be, on average, identical in all other respects. This property, known as **exchangeability**, ensures that any observed difference in outcomes can be confidently attributed to the exposure itself. However, in observational epidemiology, we rarely have the luxury of randomization. Instead, individuals self-select or are selected into exposure groups based on a multitude of factors. This creates the persistent challenge of **confounding**.

Confounding occurs when a variable, the **confounder**, is associated with both the exposure and the outcome, creating a non-causal "backdoor" path of association between them. For example, consider a simple causal structure where a variable $C$ is a common cause of exposure $A$ and outcome $Y$. This can be depicted using a **Directed Acyclic Graph (DAG)** as $A \leftarrow C \rightarrow Y$ [@problem_id:4638409]. The observed association between $A$ and $Y$ is a mixture of the true causal effect ($A \rightarrow Y$) and the spurious association generated by the backdoor path through $C$.

**Stratification** is a fundamental analytic technique designed to control for confounding. The core idea is to break the association between the confounder and the exposure by analyzing the data within subgroups, or **strata**, defined by levels of the confounder. Within a specific stratum, say where $C=c$, the confounder is held constant. Consequently, within that subgroup, it can no longer be associated with the exposure $A$. By comparing outcomes between exposed and unexposed individuals *within each stratum*, we obtain a measure of association that is free from the confounding influence of that variable.

Consider a hypothetical study examining the effect of high ambient fine particulate matter ($A$) on asthma exacerbation ($Y$), where neighborhood socioeconomic status ($C$) is a common cause of both. Individuals in lower-SES neighborhoods might be more likely to live near sources of pollution (higher $A$) and also have a higher baseline risk of asthma exacerbation due to other factors like housing quality or healthcare access (higher $Y$) [@problem_id:4638395]. A crude comparison of all individuals with high exposure to all those with low exposure would likely overestimate the effect of pollution, as it would incorrectly attribute the added risk from lower SES to the pollution itself. By stratifying on $C$, we would compare high- versus low-exposure individuals within the lower-SES stratum, and separately make the same comparison within the higher-SES stratum. This process emulates a localized experiment within each level of the confounder, moving us from a state of crude non-exchangeability towards a more refined **conditional exchangeability**.

### Formalizing the Conditions for Causal Identification

To move from the intuitive idea of stratification to a rigorous quantitative method, we rely on the **potential outcomes** framework. For a binary exposure $A \in \{0, 1\}$, let $Y^1$ be the potential outcome an individual would experience if exposed ($A=1$) and $Y^0$ be the potential outcome they would experience if unexposed ($A=0$). The individual causal effect is the difference $Y^1 - Y^0$, which is typically unobservable since we can only observe one of these potential outcomes for any given person. Our goal in a population study is often to estimate the **Average Causal Effect (ACE)** or Average Treatment Effect (ATE), defined as $\mathbb{E}[Y^1 - Y^0]$.

Identifying this causal quantity from observational data via stratification requires three key, untestable assumptions [@problem_id:4638399]:

1.  **Consistency**: This assumption links the potential outcomes to the observed data. It states that an individual's observed outcome is their potential outcome corresponding to the exposure they actually received. Formally, if an individual has exposure status $A=a$, then their observed outcome $Y$ is equal to $Y^a$.

2.  **Conditional Exchangeability**: This is the "no unmeasured confounding" assumption. It posits that, within strata defined by a sufficient set of baseline covariates $C$, the potential outcomes are independent of the actual exposure received. Formally, $Y^a \perp A \mid C$ for all $a \in \{0,1\}$. This implies that within a stratum of $C$, the unexposed group serves as a valid counterfactual for what would have happened to the exposed group had they been unexposed (and vice versa).

3.  **Positivity**: Also known as experimental treatment assignment, this assumption requires that within every stratum of $C$ that exists in the population, there is a non-zero probability of receiving each level of exposure. Formally, for any level $c$ of the covariates where $P(C=c) > 0$, we must have $0  P(A=a \mid C=c)  1$ for all $a$. If this were not true (e.g., if everyone in a certain stratum was unexposed), no comparison could be made in that stratum, and we would have no empirical basis for the effect of exposure.

Under these three assumptions, we can identify the ACE. The derivation proceeds as follows. By [linearity of expectation](@entry_id:273513), $\mathbb{E}[Y^1 - Y^0] = \mathbb{E}[Y^1] - \mathbb{E}[Y^0]$. Let's identify the term $\mathbb{E}[Y^a]$:

$$ \begin{aligned} \mathbb{E}[Y^a]  = \sum_c \mathbb{E}[Y^a \mid C=c] P(C=c)  \quad \text{(by Law of Total Expectation)} \\  = \sum_c \mathbb{E}[Y^a \mid A=a, C=c] P(C=c)  \quad \text{(by Conditional Exchangeability)} \\  = \sum_c \mathbb{E}[Y \mid A=a, C=c] P(C=c)  \quad \text{(by Consistency)} \end{aligned} $$

This final expression, known as the **g-formula** or **standardization formula**, is revolutionary: it expresses the unobservable population average potential outcome $\mathbb{E}[Y^a]$ entirely in terms of observable quantities: the stratum-specific risks $\mathbb{E}[Y \mid A=a, C=c]$ and the prevalence of the strata $P(C=c)$.

### The Mechanics of Stratification and Standardization

The practical application of stratification involves two main steps: calculating stratum-specific effects and, if desired, combining them into a single summary measure through standardization.

First, within each stratum $c$ defined by the confounder(s), we compute a measure of association between the exposure and outcome. For a binary outcome, the most common measures are [@problem_id:4638422]:
-   **Stratum-specific Risk Difference ($RD_c$)**: $RD_c = P(Y=1 \mid A=1, C=c) - P(Y=1 \mid A=0, C=c)$
-   **Stratum-specific Risk Ratio ($RR_c$)**: $RR_c = \frac{P(Y=1 \mid A=1, C=c)}{P(Y=1 \mid A=0, C=c)}$
-   **Stratum-specific Odds Ratio ($OR_c$)**: $OR_c = \frac{P(Y=1 \mid A=1, C=c) / P(Y=0 \mid A=1, C=c)}{P(Y=1 \mid A=0, C=c) / P(Y=0 \mid A=0, C=c)}$

Calculating these measures is the essence of stratification. The failure to do so can lead to severely misleading conclusions, a phenomenon famously known as **Simpson's Paradox**. This paradox occurs when an association observed in an overall population is reversed within all of its subgroups. For instance, a new antiviral prophylaxis ($A$) might appear harmful in a crude analysis, showing a higher risk of infection ($Y$) among the treated. However, stratification by a confounder, such as high-risk work assignment ($Z$), might reveal that the drug is protective in *both* the high-risk and low-risk groups. The paradox arises because high-risk individuals are both more likely to take the drug and more likely to get infected, creating a confounding effect that masks the drug's true benefit when the strata are improperly pooled [@problem_id:4638419].

After computing stratum-specific effects, we often wish to report a single summary effect for the entire population that has been adjusted for the confounder. This is achieved through **standardization**. Using the identification formula derived previously, the causal risk difference is:
$$ \text{RD}_{\text{causal}} = \mathbb{E}[Y^1] - \mathbb{E}[Y^0] = \sum_c P(C=c) \left\{ \mathbb{E}[Y \mid A=1, C=c] - \mathbb{E}[Y \mid A=0, C=c] \right\} $$
$$ \text{RD}_{\text{causal}} = \sum_c P(C=c) \cdot RD_c $$
This shows that the adjusted risk difference is simply a weighted average of the stratum-specific risk differences, where the weights are the proportion of the population in each stratum. This process yields a summary measure of effect that would be observed in the original population, but with the confounding effect of $C$ removed. For example, if we have stratum-specific risk differences of $0.02$ and $0.15$ in two strata that make up $70\%$ and $30\%$ of the population, respectively, the standardized risk difference would be $(0.02)(0.70) + (0.15)(0.30) = 0.059$ [@problem_id:4638422].

### Stratification for Describing Effect Heterogeneity

While often discussed as a method for *controlling* confounding, stratification's other crucial function is to *investigate* **Effect Measure Modification (EMM)**. EMM, also known as interaction or heterogeneity of effect, occurs when the magnitude or direction of an exposure's effect on an outcome differs across levels of a third variable, the **modifier**.

EMM is a biological or social phenomenon, not a bias to be eliminated. It is a feature of the causal relationship that we want to discover and describe. Stratification is the most direct way to assess EMM. By calculating separate effect measures in each stratum, we can simply compare them. If the stratum-specific measures are different, EMM is present.

A critical point is that EMM is distinct from confounding. A variable can be an effect modifier without being a confounder, and vice-versa. Consider a study of an occupational exposure ($E$) and respiratory outcome ($Y$), stratified by age group ($Z$). Suppose the risk ratio is $2.0$ in younger workers and $2.0$ in older workers, but the risk difference is $0.05$ in younger workers and $0.20$ in older workers [@problem_id:4638389]. Here, the effect is homogeneous on the multiplicative scale ($RR_0 = RR_1$) but heterogeneous on the additive scale ($RD_0 \neq RD_1$). This is EMM on the additive scale. If the exposure was equally distributed across age groups, age would not be a confounder, yet it remains an important effect modifier. Failing to stratify would lead to reporting a single, averaged risk difference (e.g., $0.125$), completely obscuring the fact that the absolute impact of the exposure is four times larger in older workers.

This also highlights that EMM is **scale-dependent**. The presence or absence of interaction can depend on whether we measure the effect on an additive scale (like risk difference) or a multiplicative scale (like risk ratio or odds ratio). Stratification allows for a transparent assessment on any scale of interest, a clarity that can be masked by regression models unless [interaction terms](@entry_id:637283) are explicitly specified and tested [@problem_id:4638395].

### Advanced Topics and Practical Challenges

#### Choosing Stratification Variables

The decision of which covariates to stratify upon is critical. Adjusting for the wrong variables can fail to control confounding, or worse, introduce new bias. Modern epidemiology uses **Directed Acyclic Graphs (DAGs)** to formalize causal assumptions and guide the selection of adjustment sets. The **[backdoor criterion](@entry_id:637856)** provides a graphical rule for this selection: to estimate the total causal effect of $A$ on $Y$, we must condition on a set of variables $C$ that blocks all non-causal (backdoor) paths between $A$ and $Y$, without conditioning on any variables that are descendants of the exposure $A$ [@problem_id:4638409].

This rule leads to clear prescriptions [@problem_id:4638382] [@problem_id:4638419]:
-   **Confounders**: Variables that are common causes of exposure and outcome (e.g., $A \leftarrow C \rightarrow Y$) create backdoor paths. They **must** be conditioned on to block these paths.
-   **Mediators**: Variables that lie on the causal pathway from exposure to outcome (e.g., $A \rightarrow M \rightarrow Y$) are descendants of the exposure. Conditioning on a mediator is incorrect when estimating the *total effect*, as it blocks a portion of the very effect we want to measure.
-   **Colliders**: A collider is a variable that is a common effect of two other variables (e.g., $A \rightarrow L \leftarrow Y$). A backdoor path containing a [collider](@entry_id:192770) is already blocked by the [collider](@entry_id:192770). Conditioning on a collider *opens* the path, inducing a spurious association known as [collider](@entry_id:192770)-stratification bias. Therefore, one must **not** condition on colliders or their descendants.
-   **Instrumental Variables**: Variables that cause the exposure but do not affect the outcome except through the exposure ($Z \rightarrow A \rightarrow Y$) are not confounders and should not be included in a standard stratification adjustment set. They do not block backdoor paths and can in some cases amplify bias from unmeasured confounding.
-   **Precision Variables**: Variables that are predictors of the outcome but are not associated with the exposure can be included in an adjustment set. They do not reduce [confounding bias](@entry_id:635723) but can increase the precision (i.e., reduce the variance) of the effect estimate, especially for collapsible measures like the risk difference.

#### The Problem of Sparsity: The Curse of Dimensionality

While stratification is conceptually simple, it faces a severe practical limitation when we need to control for multiple confounders simultaneously. If we stratify on a set of seven covariates with 4, 5, 3, 2, 10, 2, and 3 levels respectively, we create $4 \times 5 \times 3 \times 2 \times 10 \times 2 \times 3 = 7200$ unique strata. In a study with a sample size of $N=2000$, the expected number of individuals per stratum is less than one ($2000/7200 \approx 0.28$) [@problem_id:4638392].

This **[data sparsity](@entry_id:136465)** has devastating consequences. Most strata will be empty. The few that are not empty will likely contain only exposed or only unexposed individuals. This creates empirical or **near-positivity violations**, where the condition $0  P(A=a \mid C=c)  1$ fails in the sample, making within-stratum comparisons impossible. This problem is often called the **[curse of dimensionality](@entry_id:143920)**.

To overcome this, several strategies are employed:
-   **Coarsening Covariates**: Reduce the number of categories for each variable (e.g., group ages into wider bands), guided by subject-matter knowledge.
-   **Restricting Adjustment Set**: Based on a DAG, include only the minimal set of covariates necessary to block backdoor paths.
-   **Propensity Score Methods**: A powerful alternative is to use a **propensity score**, $e(X) = P(A=1 \mid X=x)$, which is the conditional probability of exposure given the full set of covariates $X$. The [propensity score](@entry_id:635864) is a balancing score, meaning that conditional on $e(X)$, the distribution of $X$ is the same between exposed and unexposed groups. By summarizing all confounders into this single scalar variable, one can stratify on the [propensity score](@entry_id:635864) (e.g., by quintiles) to control for confounding by all covariates in $X$ simultaneously, thus reducing a high-dimensional problem to a single dimension and mitigating sparsity [@problem_id:4638392].

#### Structural Violations of Positivity

Sometimes, positivity is violated not due to random sparsity in a finite sample, but for deterministic or structural reasons. For example, in a study of smoking's effect on mortality in a long-term care system, there may be a strict policy prohibiting smoking for residents over age 90. In the stratum "age $\ge 90$," the probability of being a smoker is structurally zero: $P(A=1 \mid \text{age} \ge 90) = 0$ [@problem_id:4638411].

In such cases, it is impossible to estimate the risk among smokers in that stratum from the data. The identification formula for $\mathbb{E}[Y^1]$ breaks down. No amount of statistical manipulation, including Inverse Probability Weighting (IPTW), can create data where none exists. The only principled solution within the observed data is to **change the target estimand**. Instead of trying to estimate the effect for the entire population, one can restrict the analysis and the inference to the subpopulation where positivity holds (e.g., residents aged 50-89). This yields a valid causal effect for the restricted population. Any estimate for the full population would rely on untestable, model-based extrapolation for the missing stratum.

#### Non-Collapsibility of the Odds Ratio

Finally, users of stratification must be aware of a peculiar mathematical property of the odds ratio: it is **non-collapsible**. A measure is collapsible if the crude (marginal) measure is equal to the common stratum-specific measure when there is no confounding. The risk difference is collapsible. The risk ratio is collapsible under the condition that the stratification variable is independent of the exposure. The odds ratio, however, is not.

This means that even when there is no confounding (e.g., the stratification variable $Z$ is independent of exposure $E$), a crude odds ratio will generally not be equal to a constant, stratum-specific odds ratio [@problem_id:4638396]. For instance, if the OR is $2.0$ in stratum $Z=0$ and $2.0$ in stratum $Z=1$, and $Z$ is not a confounder, the crude OR that ignores $Z$ may be something like $1.82$. This difference is not bias; it is an inherent mathematical property arising from the non-linearity of the odds function (Jensen's inequality).

The implication is profound: when using the odds ratio, a change between the crude and adjusted (stratified) estimate cannot, by itself, be taken as evidence of confounding. The adjusted OR has a **conditional** interpretation (the effect within strata), while the crude OR has a **marginal** interpretation (the effect in the total population). Due to non-collapsibility, these two estimands can be genuinely different even in the absence of confounding. This is a critical consideration for interpreting the output of logistic regression models, which estimate conditional odds ratios.