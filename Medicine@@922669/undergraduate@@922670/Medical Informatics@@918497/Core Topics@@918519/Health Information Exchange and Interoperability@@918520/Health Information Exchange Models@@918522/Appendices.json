{"hands_on_practices": [{"introduction": "The cornerstone of any Health Information Exchange (HIE) is its ability to accurately and reliably identify patients across different healthcare systems. This practice delves into the critical task of evaluating patient matching algorithms, which are essential for preventing both dangerous data merges and incomplete patient records. By calculating and interpreting key performance metrics like precision, recall, and the $F_1$ score, you will gain a quantitative understanding of the trade-offs between data completeness and patient safety.[@problem_id:4841823]", "problem": "A regional Health Information Exchange (HIE) operates a probabilistic patient matching algorithm that classifies candidate Electronic Health Record (EHR) pairs as either \"match\" or \"non-match.\" On a validation dataset of $10{,}000$ candidate record pairs with adjudicated ground truth, the algorithm’s outcomes relative to the ground truth are summarized by the following confusion matrix counts: true positives $TP = 850$, false positives $FP = 350$, false negatives $FN = 150$, and true negatives $TN = 8{,}650$. Starting only from the foundational definitions of the confusion matrix and the interpretation of precision and recall in binary classification, derive the expressions for precision, recall, and the $F_1$ score in terms of $TP$, $FP$, and $FN$, and compute these values for the algorithm described. Then, explain the trade-offs between precision and recall in patient matching within Health Information Exchange models, focusing on how adjusting a similarity threshold impacts $TP$, $FP$, and $FN$, and the potential downstream effects on data quality and patient safety in cross-organizational exchange. Report the final numerical answer as the computed $F_1$ score for this dataset, rounded to four significant figures. No units are required.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of classification metrics, well-posed with all necessary data provided, and objective in its formulation. The problem is a standard application of data science principles to the domain of medical informatics.\n\nThe problem requires the derivation of key binary classification metrics from first principles, their calculation for a given dataset, and a qualitative explanation of their implications in the specific context of Health Information Exchange (HIE) patient matching.\n\nFirst, we define the terms based on the confusion matrix components provided:\n-   A \"positive\" instance refers to a candidate record pair that is a true match.\n-   A \"negative\" instance refers to a candidate record pair that is a true non-match.\n-   True Positives ($TP$): The number of true matches correctly classified as a \"match\" by the algorithm. Given as $TP = 850$.\n-   False Positives ($FP$): The number of true non-matches incorrectly classified as a \"match\". This is a Type I error. Given as $FP = 350$.\n-   False Negatives ($FN$): The number of true matches incorrectly classified as a \"non-match\". This is a Type II error. Given as $FN = 150$.\n-   True Negatives ($TN$): The number of true non-matches correctly classified as a \"non-match\". Given as $TN = 8650$.\nThe total number of pairs is $TP+FP+FN+TN = 850 + 350 + 150 + 8650 = 10000$, which matches the provided dataset size.\n\nNow, we derive the expressions for precision, recall, and the $F_1$ score.\n\nPrecision, also known as the Positive Predictive Value ($PPV$), measures the accuracy of the positive predictions. It answers the question: \"Of all the pairs that the algorithm classified as a match, what fraction were actually matches?\"\nThe total number of pairs classified as a match is the sum of true positives and false positives, $TP + FP$. The number of these that are correct is $TP$.\nTherefore, the expression for precision is:\n$$\n\\text{Precision} = \\frac{TP}{TP + FP}\n$$\n\nRecall, also known as sensitivity or the True Positive Rate ($TPR$), measures the completeness of the positive predictions. It answers the question: \"Of all the pairs that were actually matches, what fraction did the algorithm correctly identify?\"\nThe total number of actual matches in the dataset is the sum of the true positives (which were correctly identified) and the false negatives (which were missed), $TP + FN$. The number of these that the algorithm correctly identified is $TP$.\nTherefore, the expression for recall is:\n$$\n\\text{Recall} = \\frac{TP}{TP + FN}\n$$\n\nThe $F_1$ score is defined as the harmonic mean of precision and recall. It serves as a single metric that balances both concerns. The harmonic mean of two numbers, $A$ and $B$, is given by $\\frac{2AB}{A+B}$. Substituting precision and recall for $A$ and $B$:\n$$\nF_1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\nSubstituting the expressions in terms of $TP$, $FP$, and $FN$:\n$$\nF_1 = \\frac{2 \\cdot \\left(\\frac{TP}{TP + FP}\\right) \\cdot \\left(\\frac{TP}{TP + FN}\\right)}{\\left(\\frac{TP}{TP + FP}\\right) + \\left(\\frac{TP}{TP + FN}\\right)}\n$$\nTo simplify, we find a common denominator for the sum in the denominator of the main fraction:\n$$\nF_1 = \\frac{\\frac{2 \\cdot TP^2}{(TP + FP)(TP + FN)}}{\\frac{TP(TP + FN) + TP(TP + FP)}{(TP + FP)(TP + FN)}} = \\frac{2 \\cdot TP^2}{TP(TP + FN) + TP(TP + FP)}\n$$\nFactoring out $TP$ from the denominator:\n$$\nF_1 = \\frac{2 \\cdot TP^2}{TP((TP + FN) + (TP + FP))} = \\frac{2 \\cdot TP}{2TP + FP + FN}\n$$\nThis final expression is a convenient form for calculation.\n\nNext, we compute these values for the given algorithm.\nGiven: $TP = 850$, $FP = 350$, $FN = 150$.\n\nPrecision:\n$$\n\\text{Precision} = \\frac{850}{850 + 350} = \\frac{850}{1200} = \\frac{85}{120} = \\frac{17}{24} \\approx 0.7083\n$$\n\nRecall:\n$$\n\\text{Recall} = \\frac{850}{850 + 150} = \\frac{850}{1000} = 0.85\n$$\n\n$F_1$ Score:\nUsing the simplified formula derived above:\n$$\nF_1 = \\frac{2 \\cdot TP}{2TP + FP + FN} = \\frac{2 \\cdot 850}{2 \\cdot 850 + 350 + 150} = \\frac{1700}{1700 + 500} = \\frac{1700}{2200} = \\frac{17}{22}\n$$\nConverting this to a decimal and rounding to four significant figures:\n$$\nF_1 = \\frac{17}{22} \\approx 0.772727... \\approx 0.7727\n$$\n\nFinally, we explain the trade-offs between precision and recall in the context of HIE patient matching. Probabilistic patient matching algorithms typically calculate a similarity score between record pairs and classify them based on a predefined threshold. The choice of this threshold directly governs the trade-off.\n\n-   **Impact of Adjusting the Similarity Threshold**:\n    -   **Raising the threshold** makes the algorithm more stringent. It requires a higher degree of similarity to declare a \"match\". This leads to a decrease in both $TP$ (as some borderline true matches are missed) and $FP$ (as more non-matches are correctly rejected). Consequently, $FN$ increases and $TN$ increases. The result is typically an **increase in precision** (fewer false links) and a **decrease in recall** (more missed true links).\n    -   **Lowering the threshold** makes the algorithm more lenient. It declares a \"match\" with less evidence. This leads to an increase in both $TP$ (as more true matches are found) and $FP$ (as more non-matches are incorrectly linked). Consequently, $FN$ decreases and $TN$ decreases. The result is a **decrease in precision** (more false links) and an **increase in recall** (fewer missed true links).\n\n-   **Downstream Effects on Data Quality and Patient Safety**:\n    -   **High-Precision, Low-Recall Scenario (High Threshold)**: The primary benefit is high data integrity for linked records. There is a low probability of creating an \"overlay,\" which is the merging of records from two distinct patients. An overlay is a critical patient safety failure, as a clinician might make a life-threatening decision based on another patient's allergies, medications, or lab results. The downside is record fragmentation. With high $FN$, many of a single patient's records remain unlinked across the HIE. This undermines the purpose of the HIE, leading to an incomplete patient view, which can cause redundant testing, missed diagnoses, or failure to identify trends.\n    -   **Low-Precision, High-Recall Scenario (Low Threshold)**: The primary benefit is a more comprehensive, longitudinal patient record, as most true matches are correctly linked (low $FN$). This fulfills the main objective of the HIE. However, the high rate of false positives ($FP$) poses a severe risk to patient safety and data quality. The creation of numerous overlays pollutes the database, erodes physician trust in the system, and can directly lead to medical errors. The operational cost of manually identifying and resolving these incorrect links is also prohibitive.\n\nThe fundamental trade-off in HIE is between patient safety risks. A false positive (overlay) can lead to direct, immediate harm. A false negative (fragmented record) is an error of omission that can also lead to harm, but often less directly. For this reason, patient matching systems in production environments are almost always tuned to favor extremely high precision at the expense of recall. The challenge is to improve the underlying algorithm to increase recall without sacrificing the necessary high level of precision.", "answer": "$$\n\\boxed{0.7727}\n$$", "id": "4841823"}, {"introduction": "Once a patient is correctly identified, the next challenge in HIE is aggregating their information from disparate sources to form a single, comprehensive record. This exercise applies fundamental probability theory to model the completeness of an aggregated medication list, a common and critical task. You will derive how completeness improves as more data sources are added and quantify the marginal gain, illustrating the important real-world concept of diminishing returns in data integration.[@problem_id:4841791]", "problem": "In a query-based Health Information Exchange (HIE) model, a patient's medication list is aggregated from multiple independent sources. For any single true medication on the patient's list, define the completeness of the aggregated list as the probability that the medication is present in the union of all sources. Assume the following fundamental base: (i) event independence across sources for whether a given medication is captured, and (ii) completeness is the fraction of true items captured, which for a single medication equals the probability it is captured by at least one source. Let the per-source capture probabilities be $p_{1}, p_{2}, \\ldots, p_{n}$, where source $i$ captures a given true medication independently with probability $p_{i}$ and misses it with probability $1 - p_{i}$.\n\nStarting from these definitions, derive a closed-form analytic expression for the expected completeness across $n$ sources. Then, for a specific Health Information Exchange (HIE) aggregation that draws medication data from three sources—Electronic Health Record (EHR), Pharmacy Claims (PC), and a regional HIE repository—with capture probabilities $p_{1} = 0.52$, $p_{2} = 0.43$, and $p_{3} = 0.31$, compute the expected completeness for the patient's medication list across these three sources. Next, derive from first principles the marginal expected gain in completeness when adding a fourth independent source with capture probability $p_{4}$, and evaluate this marginal expected gain when the fourth source is an electronic prescribing (eRx) network with $p_{4} = 0.27$.\n\nExpress both the expected completeness across the initial three sources and the marginal expected gain from adding the eRx source as decimal fractions, and round your answers to four significant figures. Do not use a percentage sign.", "solution": "The problem asks for two quantities: the expected completeness for $n=3$ sources, and the marginal gain in completeness when adding a fourth source.\n\nFirst, we derive the general closed-form expression for the expected completeness, which we denote as $C_n$, for $n$ independent sources. The term \"expected completeness\" for a single medication is synonymous with the probability of its capture. A medication is successfully aggregated if it is captured by at least one source. This is the event of the union of individual capture events. Let $A_i$ be the event that source $i$ captures the medication, with probability $P(A_i) = p_i$. The completeness is the probability of the union of these events:\n$$C_n = P(A_1 \\cup A_2 \\cup \\dots \\cup A_n)$$\nCalculating the probability of a union directly is complex. It is more straightforward to use the complement rule. The complement of \"at least one source captures the medication\" is \"all sources miss the medication\". The event that source $i$ misses the medication is $A_i^c$, with probability $P(A_i^c) = 1 - p_i$.\nThe completeness is therefore:\n$$C_n = 1 - P(\\text{all sources miss the medication})$$\n$$C_n = 1 - P(A_1^c \\cap A_2^c \\cap \\dots \\cap A_n^c)$$\nThe problem states that the capture events are independent across sources. This independence extends to their complements. Therefore, the probability of the intersection of these complement events is the product of their individual probabilities:\n$$P(A_1^c \\cap A_2^c \\cap \\dots \\cap A_n^c) = P(A_1^c) P(A_2^c) \\dots P(A_n^c) = \\prod_{i=1}^{n} P(A_i^c)$$\nSubstituting $P(A_i^c) = 1 - p_i$, we obtain the probability that all sources miss the medication:\n$$\\prod_{i=1}^{n} (1 - p_i)$$\nThus, the closed-form analytic expression for the expected completeness across $n$ sources is:\n$$C_n = 1 - \\prod_{i=1}^{n} (1 - p_i)$$\nThis completes the first part of the derivation.\n\nNext, we compute the expected completeness for the three specified sources with capture probabilities $p_{1} = 0.52$, $p_{2} = 0.43$, and $p_{3} = 0.31$.\nUsing the derived formula for $n=3$:\n$$C_3 = 1 - (1 - p_1)(1 - p_2)(1 - p_3)$$\nSubstituting the given values:\n$$C_3 = 1 - (1 - 0.52)(1 - 0.43)(1 - 0.31)$$\n$$C_3 = 1 - (0.48)(0.57)(0.69)$$\nFirst, we compute the product of the miss probabilities:\n$$(0.48)(0.57) = 0.2736$$\n$$(0.2736)(0.69) = 0.188784$$\nNow, substitute this back into the expression for $C_3$:\n$$C_3 = 1 - 0.188784 = 0.811216$$\nRounding to four significant figures gives $0.8112$.\n\nSecond, we derive an expression for the marginal expected gain in completeness when adding a fourth source with capture probability $p_4$. The marginal gain, which we denote as $\\Delta C_4$, is the difference between the completeness with four sources, $C_4$, and the completeness with three sources, $C_3$.\n$$\\Delta C_4 = C_4 - C_3$$\nUsing our general formula:\n$$C_3 = 1 - \\prod_{i=1}^{3} (1 - p_i)$$\n$$C_4 = 1 - \\prod_{i=1}^{4} (1 - p_i) = 1 - (1 - p_4) \\prod_{i=1}^{3} (1 - p_i)$$\nNow we compute the difference:\n$$\\Delta C_4 = \\left(1 - (1 - p_4) \\prod_{i=1}^{3} (1 - p_i)\\right) - \\left(1 - \\prod_{i=1}^{3} (1 - p_i)\\right)$$\n$$\\Delta C_4 = 1 - \\prod_{i=1}^{3} (1 - p_i) + p_4 \\prod_{i=1}^{3} (1 - p_i) - 1 + \\prod_{i=1}^{3} (1 - p_i)$$\nThe terms cancel, leaving:\n$$\\Delta C_4 = p_4 \\prod_{i=1}^{3} (1 - p_i)$$\nThis result is intuitive: the gain from the fourth source occurs only if all of the first three sources miss the medication (an event with probability $\\prod_{i=1}^{3} (1-p_i)$) AND the fourth source captures it (an event with probability $p_4$).\n\nFinally, we evaluate this marginal gain for $p_{4} = 0.27$. We have already calculated the product term:\n$$\\prod_{i=1}^{3} (1 - p_i) = 0.188784$$\nSubstituting the values:\n$$\\Delta C_4 = (0.27) (0.188784)$$\n$$\\Delta C_4 = 0.05097168$$\nRounding to four significant figures gives $0.05097$.\nThe two requested values are the expected completeness for the three sources ($C_3$) and the marginal expected gain from the fourth source ($\\Delta C_4$).", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8112 & 0.05097 \\end{pmatrix}}\n$$", "id": "4841791"}, {"introduction": "The architectural model chosen for an HIE has profound effects on its performance, scalability, and cost. This problem provides a hands-on comparison between two dominant paradigms: the document-centric model (represented by the Continuity of Care Document, or CCD) and the modern, resource-centric API model (represented by FHIR). By calculating the network bandwidth required for each under realistic conditions, you will analyze a key technical trade-off that system architects face when designing interoperability solutions.[@problem_id:4841863]", "problem": "A regional Health Information Exchange (HIE) is evaluating two exchange models for encounter summaries between hospitals and clinics: a document-centric model using the Continuity of Care Document (CCD) and a resource-centric model using Fast Healthcare Interoperability Resources (FHIR). Both models use Hypertext Transfer Protocol (HTTP) with Transport Layer Security (TLS) and gzip compression applied to payloads; assume non-payload protocol overhead is not compressible.\n\nUse the following scientifically realistic assumptions:\n\n- CCD model: Each exchange transmits a single XML CCD. The average uncompressed payload size is $2.4\\,\\mathrm{MB}$. gzip reduces the payload size by $70\\%$ (so the compressed payload is $30\\%$ of the original). There is exactly one HTTP/TLS request per exchange with an average non-compressible overhead of $2\\,\\mathrm{KB}$ per request.\n- FHIR model: Each exchange transmits a set of $20$ JSON FHIR resources. The average uncompressed payload size per resource is $60\\,\\mathrm{KB}$. gzip reduces the payload size per resource by $50\\%$. Each resource is requested in a separate HTTP/TLS call with an average non-compressible overhead of $2\\,\\mathrm{KB}$ per request.\n- Exchange frequency: Exchanges occur uniformly in time at an average rate of $1{,}000$ exchanges per hour.\n\nUse the decimal definitions $1\\,\\mathrm{KB} = 10^{3}\\,\\mathrm{bytes}$, $1\\,\\mathrm{MB} = 10^{6}\\,\\mathrm{bytes}$, and $1\\,\\mathrm{Mb} = 10^{6}\\,\\mathrm{bits}$. Take $1\\,\\mathrm{byte} = 8\\,\\mathrm{bits}$.\n\nStarting from first principles of throughput as data per unit time, compute the difference in the average bandwidth consumption (CCD minus FHIR) in megabits per second, under the stated conditions. Express your answer in $\\mathrm{Mbps}$ and round your result to four significant figures.", "solution": "The problem requires the computation of the difference in average bandwidth consumption between the CCD and FHIR models, $\\Delta B = B_{\\text{CCD}} - B_{\\text{FHIR}}$. Bandwidth is defined as data transferred per unit time. The average bandwidth for a given model can be expressed as the product of the total data transferred per exchange ($D$) and the frequency of exchanges ($R_{\\text{exch}}$).\n\n$$B = D \\times R_{\\text{exch}}$$\n\nWe must first calculate the total data transferred per exchange for each model, $D_{\\text{CCD}}$ and $D_{\\text{FHIR}}$. The total data is the sum of the compressed payload size and the total non-compressible overhead.\n\nThe given unit conversions are:\n$1\\,\\mathrm{KB} = 10^{3}\\,\\mathrm{bytes}$\n$1\\,\\mathrm{MB} = 10^{6}\\,\\mathrm{bytes}$\n$1\\,\\mathrm{byte} = 8\\,\\mathrm{bits}$\n$1\\,\\text{hour} = 3600\\,\\mathrm{s}$\n$1\\,\\mathrm{Mbps} = 10^{6}\\,\\mathrm{bits}/\\mathrm{s}$\n\nFirst, we calculate the total data per exchange for the CCD model, $D_{\\text{CCD}}$.\nThe uncompressed payload size is $S_{\\text{CCD,uncomp}} = 2.4\\,\\mathrm{MB} = 2.4 \\times 10^{6}\\,\\mathrm{bytes}$.\nThe compression reduces the payload size by $70\\%$, meaning the compressed payload is $100\\% - 70\\% = 30\\%$ of the original size. The compression factor is $c_{\\text{CCD}} = 0.30$.\nThe compressed payload size is:\n$$S_{\\text{CCD,comp}} = c_{\\text{CCD}} \\times S_{\\text{CCD,uncomp}} = 0.30 \\times (2.4 \\times 10^{6}\\,\\mathrm{bytes}) = 0.72 \\times 10^{6}\\,\\mathrm{bytes} = 720,000\\,\\mathrm{bytes}$$\nThere is $N_{\\text{CCD,req}} = 1$ request per exchange, with an overhead of $O_{\\text{req}} = 2\\,\\mathrm{KB} = 2 \\times 10^{3}\\,\\mathrm{bytes}$ per request.\nThe total overhead for the CCD model is:\n$$O_{\\text{CCD,total}} = N_{\\text{CCD,req}} \\times O_{\\text{req}} = 1 \\times (2 \\times 10^{3}\\,\\mathrm{bytes}) = 2,000\\,\\mathrm{bytes}$$\nThe total data per CCD exchange is the sum of the compressed payload and the total overhead:\n$$D_{\\text{CCD}} = S_{\\text{CCD,comp}} + O_{\\text{CCD,total}} = 720,000\\,\\mathrm{bytes} + 2,000\\,\\mathrm{bytes} = 722,000\\,\\mathrm{bytes}$$\n\nNext, we calculate the total data per exchange for the FHIR model, $D_{\\text{FHIR}}$.\nEach exchange involves $N_{\\text{FHIR,res}} = 20$ resources. The uncompressed payload size per resource is $S_{\\text{FHIR,res,uncomp}} = 60\\,\\mathrm{KB} = 60 \\times 10^{3}\\,\\mathrm{bytes}$.\nThe compression reduces the payload size by $50\\%$, so the compression factor is $c_{\\text{FHIR}} = 0.50$.\nThe compressed payload size per resource is:\n$$S_{\\text{FHIR,res,comp}} = c_{\\text{FHIR}} \\times S_{\\text{FHIR,res,uncomp}} = 0.50 \\times (60 \\times 10^{3}\\,\\mathrm{bytes}) = 30 \\times 10^{3}\\,\\mathrm{bytes}$$\nThe total compressed payload size for a FHIR exchange is the sum over all resources:\n$$S_{\\text{FHIR,comp,total}} = N_{\\text{FHIR,res}} \\times S_{\\text{FHIR,res,comp}} = 20 \\times (30 \\times 10^{3}\\,\\mathrm{bytes}) = 600,000\\,\\mathrm{bytes}$$\nEach of the $N_{\\text{FHIR,req}} = 20$ resources is requested in a separate call, each with an overhead of $O_{\\text{req}} = 2 \\times 10^{3}\\,\\mathrm{bytes}$.\nThe total overhead for the FHIR model is:\n$$O_{\\text{FHIR,total}} = N_{\\text{FHIR,req}} \\times O_{\\text{req}} = 20 \\times (2 \\times 10^{3}\\,\\mathrm{bytes}) = 40,000\\,\\mathrm{bytes}$$\nThe total data per FHIR exchange is:\n$$D_{\\text{FHIR}} = S_{\\text{FHIR,comp,total}} + O_{\\text{FHIR,total}} = 600,000\\,\\mathrm{bytes} + 40,000\\,\\mathrm{bytes} = 640,000\\,\\mathrm{bytes}$$\n\nNow we can find the difference in data per exchange, $\\Delta D$, in bytes:\n$$\\Delta D = D_{\\text{CCD}} - D_{\\text{FHIR}} = 722,000\\,\\mathrm{bytes} - 640,000\\,\\mathrm{bytes} = 82,000\\,\\mathrm{bytes}$$\nTo calculate bandwidth in bits per second, we must convert this data difference to bits:\n$$\\Delta D_{\\text{bits}} = \\Delta D \\times 8\\,\\frac{\\mathrm{bits}}{\\mathrm{byte}} = 82,000\\,\\mathrm{bytes} \\times 8\\,\\frac{\\mathrm{bits}}{\\mathrm{byte}} = 656,000\\,\\mathrm{bits}$$\nThe exchange frequency is given as $R_{\\text{exch}} = 1,000\\,\\text{exchanges per hour}$. We convert this to exchanges per second:\n$$R_{\\text{exch}} = \\frac{1,000\\,\\text{exchanges}}{1\\,\\text{hour}} \\times \\frac{1\\,\\text{hour}}{3,600\\,\\mathrm{s}} = \\frac{1,000}{3,600}\\,\\mathrm{s}^{-1}$$\nThe difference in bandwidth, $\\Delta B$, in bits per second is:\n$$\\Delta B = \\Delta D_{\\text{bits}} \\times R_{\\text{exch}} = 656,000\\,\\mathrm{bits} \\times \\frac{1,000}{3,600}\\,\\mathrm{s}^{-1} = \\frac{656,000,000}{3,600}\\,\\frac{\\mathrm{bits}}{\\mathrm{s}}$$\nTo express the result in megabits per second (Mbps), we use the conversion $1\\,\\mathrm{Mbps} = 10^{6}\\,\\mathrm{bits}/\\mathrm{s}$:\n$$\\Delta B = \\frac{656,000,000}{3,600}\\,\\frac{\\mathrm{bits}}{\\mathrm{s}} \\times \\frac{1\\,\\mathrm{Mbps}}{10^{6}\\,\\mathrm{bits}/\\mathrm{s}} = \\frac{656 \\times 10^{6}}{3,600 \\times 10^{6}}\\,\\mathrm{Mbps} = \\frac{656}{3,600}\\,\\mathrm{Mbps}$$\nNow we compute the numerical value:\n$$\\Delta B = \\frac{656}{3,600} = \\frac{164}{900} = \\frac{41}{225} \\approx 0.182222... \\,\\mathrm{Mbps}$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $1$, $8$, $2$, and $2$. The fifth digit is $2$, so we round down.\n$$\\Delta B \\approx 0.1822\\,\\mathrm{Mbps}$$\nThis is the final calculated difference in bandwidth consumption.", "answer": "$$\\boxed{0.1822}$$", "id": "4841863"}]}