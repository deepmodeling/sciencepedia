{"hands_on_practices": [{"introduction": "The Federated Averaging (FedAvg) algorithm forms the backbone of many modern federated learning systems. Before delving into privacy enhancements or performance optimizations, it is crucial to understand its fundamental operational structure. This exercise [@problem_id:4840326] challenges you to deconstruct the FedAvg process and derive expressions for its two primary costs: the total amount of local computation and the number of communication rounds, providing a quantitative foundation for analyzing any FL system.", "problem": "A medical informatics consortium of $K$ hospitals aims to train a predictive model on Electronic Health Record (EHR) data without centralizing patient information, using Federated Averaging (FedAvg) for privacy-preserving analysis. Hospital $i$ has $N_{i}$ patient records locally stored. In FedAvg, a central server coordinates $R$ global rounds. In each global round, the server broadcasts the current model to all $K$ hospitals, each hospital trains locally for $E$ epochs using mini-batch stochastic gradient descent with batch size $B$, and then each hospital returns its updated model to the server, which aggregates them.\n\nStarting from the foundational definitions that an epoch at client $i$ is a complete pass over its $N_{i}$ records and that a mini-batch gradient step is taken for each mini-batch formed from the local dataset (with the last mini-batch allowed to be smaller than $B$ and still counted as a single step), derive a closed-form expression for:\n- the total number of local gradient steps performed across all hospitals over all $R$ global rounds, and\n- the total number of communication rounds that occur between the server and the hospitals over the entire training process.\n\nExpress your final answer symbolically in terms of $K$, $E$, $B$, $R$, and $\\{N_{i}\\}_{i=1}^{K}$. No rounding is required. Provide both quantities as a single row matrix using the LaTeX `pmatrix` environment.", "solution": "The problem statement shall first be subjected to a rigorous validation process before a solution is attempted.\n\n### Step 1: Extract Givens\nThe following data and definitions are provided verbatim in the problem statement:\n-   Number of hospitals: $K$\n-   Number of patient records at hospital $i$: $N_{i}$, where $i$ is an index from $1$ to $K$\n-   Total number of global rounds: $R$\n-   Number of local epochs per global round: $E$\n-   Mini-batch size for Stochastic Gradient Descent (SGD): $B$\n-   Definition of an epoch: \"a complete pass over its $N_{i}$ records\"\n-   Condition for a gradient step: \"a mini-batch gradient step is taken for each mini-batch formed from the local dataset\"\n-   Condition for the final mini-batch: \"the last mini-batch allowed to be smaller than $B$ and still counted as a single step\"\n-   Required outputs:\n    1.  A closed-form expression for the total number of local gradient steps.\n    2.  A closed-form expression for the total number of communication rounds.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria:\n-   **Scientifically Grounded:** The problem describes the Federated Averaging (FedAvg) algorithm, a well-established and canonical method in the field of federated learning. Its application to medical informatics for privacy-preserving analysis is a primary and realistic use case. The setup is scientifically sound.\n-   **Well-Posed:** The problem is well-posed. All variables ($K$, $R$, $E$, $B$, $\\{N_{i}\\}_{i=1}^{K}$) are defined. The process for calculating gradient steps is explicitly detailed, including the handling of the final, potentially smaller, mini-batch. This detail unambiguously implies the use of a ceiling function. The term \"communication round\" is understood in the context of federated learning to be synonymous with a \"global round\" or \"aggregation cycle\". The problem is self-contained and provides sufficient information to derive unique and meaningful expressions for the requested quantities.\n-   **Objective:** The problem is stated using clear, precise, and unambiguous technical language. It is free of any subjective or opinion-based content.\n\nThe problem does not exhibit any of the enumerated flaws (e.g., scientific unsoundness, incompleteness, contradiction, ambiguity). It is a formalizable and solvable problem rooted in established computer science and machine learning principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete and reasoned solution will now be provided.\n\n### Solution Derivation\n\nThe problem requires the derivation of two quantities: the total number of local gradient steps and the total number of communication rounds. Each will be derived from the provided definitions.\n\n**1. Total Number of Local Gradient Steps**\n\nLet us first determine the number of gradient steps performed by a single hospital, designated as hospital $i$, within a single local training epoch.\n-   Hospital $i$ possesses a local dataset of size $N_i$.\n-   Local training is performed using mini-batch SGD with a batch size of $B$.\n-   A gradient descent step is executed for each mini-batch.\n-   The problem specifies that the last mini-batch may be smaller than $B$. This means that the total number of records, $N_i$, is partitioned into as many full batches of size $B$ as possible, with the remaining records forming a final batch.\n-   The number of batches required to process $N_i$ records is therefore the total number of records divided by the batch size, rounded up to the nearest integer. This operation is represented by the ceiling function.\n-   Number of gradient steps per epoch for hospital $i$: $\\lceil \\frac{N_i}{B} \\rceil$.\n\nNext, we calculate the total number of gradient steps for hospital $i$ during one global round.\n-   In each global round, each hospital trains for $E$ local epochs.\n-   Since the number of steps per epoch is constant for a given hospital, the total number of steps for hospital $i$ in one global round is $E$ multiplied by the number of steps per epoch.\n-   Number of gradient steps for hospital $i$ per global round: $E \\times \\lceil \\frac{N_i}{B} \\rceil$.\n\nNow, we can calculate the total number of gradient steps across all hospitals during one global round.\n-   There are $K$ hospitals in the consortium.\n-   The total number of steps in one global round is the sum of the steps taken by each hospital.\n-   Total steps per global round: $\\sum_{i=1}^{K} \\left( E \\times \\lceil \\frac{N_i}{B} \\rceil \\right) = E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil$.\n\nFinally, we determine the total number of local gradient steps over the entire training process.\n-   The entire process consists of $R$ global rounds.\n-   The total number of gradient steps is the number of steps per global round multiplied by the total number of global rounds.\n-   Let $S_{total}$ be the total number of local gradient steps.\n$$S_{total} = R \\times \\left( E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil \\right) = R E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil$$\nThis is the first required expression.\n\n**2. Total Number of Communication Rounds**\n\nThe second task is to determine the total number of communication rounds.\n-   The problem describes the sequence of events within a single \"global round\":\n    1.  The central server broadcasts the current global model to all $K$ hospitals.\n    2.  The hospitals perform local training.\n    3.  Each hospital returns its updated model to the server for aggregation.\n-   This complete cycle of broadcast, local computation, and aggregation constitutes the fundamental unit of interaction between the server and the clients in the FedAvg protocol. In the standard nomenclature of federated learning, this cycle is defined as one \"communication round.\"\n-   The problem states that the training process is conducted for a total of $R$ global rounds.\n-   Therefore, by definition, the total number of communication rounds is equal to the total number of global rounds.\n-   Let $C_{total}$ be the total number of communication rounds.\n$$C_{total} = R$$\nThis is the second required expression.\n\nThe final answer combines these two results into a single row matrix as requested.", "answer": "$$ \\boxed{ \\begin{pmatrix} R E \\sum_{i=1}^{K} \\lceil \\frac{N_i}{B} \\rceil & R \\end{pmatrix} } $$", "id": "4840326"}, {"introduction": "While federated learning avoids centralizing raw data, it is not without costs; the repeated transmission of large model updates often creates a significant communication bottleneck. This practice [@problem_id:4840332] makes this challenge concrete by asking you to calculate the per-round communication time for a realistic diagnostic model. By comparing standard floating-point representations with a more compact quantized format, you will gain practical insight into how engineering choices can dramatically impact the feasibility of an FL deployment.", "problem": "A consortium of hospitals is training a shared diagnostic model using Federated Averaging (FedAvg), a standard algorithm in federated learning, to preserve patient privacy by keeping data on-premise. In each communication round, every hospital transmits its local model update to the coordinator and then receives the aggregated global model. Assume the model has $d=10^{7}$ trainable parameters and that the size of a transmitted update equals the size of the parameter vector. Consider two encoding regimes for parameters: single-precision floating point with $32$ bits per parameter and uniform quantization with $8$ bits per parameter. The network link available to each hospital is $100$ megabits per second, where one megabit is $10^{6}$ bits.\n\nStarting from the core definitions of data size and throughput (data size in bits, throughput in bits per second, and time as size divided by rate), derive the total per-round communication time per hospital (upload plus download) under both encoding regimes. Assume no compression, no protocol overhead, and that the coordinator sends back a model of the same size as the update sent by the hospital.\n\nState whether each regime is feasible under a per-round wall-clock budget of $2\\,\\mathrm{s}$, where feasibility is defined as total per-round communication time less than or equal to $2\\,\\mathrm{s}$.\n\nRound your final numerical times to four significant figures, and express the final times in seconds. Provide your final answer as a row matrix, with the first entry equal to the $32$-bit time and the second entry equal to the $8$-bit time.", "solution": "The problem is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Number of model parameters: $d = 10^7$\n-   Encoding regime 1 (single-precision): $b_{32} = 32$ bits per parameter\n-   Encoding regime 2 (quantized): $b_8 = 8$ bits per parameter\n-   Network link speed (throughput): $R = 100$ megabits per second, where $1$ megabit $= 10^6$ bits.\n-   Communication round structure: each hospital performs one upload and one download.\n-   Size of downloaded model: same as the uploaded model update.\n-   Assumptions: no compression, no protocol overhead.\n-   Feasibility budget: total per-round communication time $\\le T_{budget} = 2\\,\\mathrm{s}$.\n-   Calculation basis: time = data size / throughput.\n-   Rounding requirement: final times to be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in principles of computer networking and federated learning. The scenario is a standard application of federated learning in medical informatics. The parameters provided ($10^7$ parameters, $100\\,\\mathrm{Mbps}$ link, $32$-bit and $8$-bit encodings) are realistic values in modern machine learning and network infrastructure. The problem is well-posed, providing all necessary information and clear, objective definitions (e.g., $1$ megabit $= 10^6$ bits) to calculate a unique solution. It is free of scientific flaws, ambiguities, or contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be provided.\n\n### Solution Derivation\n\nThe analysis proceeds by first defining the total data size to be transmitted in a single communication round for each encoding regime. Then, using the given network throughput, the time required for this transmission is calculated. Finally, this time is compared against the specified feasibility budget.\n\nLet $d$ be the number of trainable parameters in the model.\nLet $b$ be the number of bits used to encode each parameter.\nThe size of a model update, $S$, in bits is given by the product of the number of parameters and the number of bits per parameter:\n$$S = d \\times b$$\n\nThe network link speed, $R$, is given as $100$ megabits per second. Using the provided definition, this is:\n$$R = 100 \\times 10^6 \\frac{\\mathrm{bits}}{\\mathrm{second}} = 10^8 \\frac{\\mathrm{bits}}{\\mathrm{second}}$$\n\nA single communication round for one hospital consists of one upload of the model update to the coordinator and one download of the aggregated global model from the coordinator. The problem states that the downloaded model has the same size as the uploaded update. Therefore, the total data transmitted per hospital per round, $S_{total}$, is twice the size of a single update:\n$$S_{total} = S_{upload} + S_{download} = S + S = 2S$$\n\nThe total time for one round, $T_{round}$, is the total data size divided by the network link speed:\n$$T_{round} = \\frac{S_{total}}{R} = \\frac{2S}{R} = \\frac{2db}{R}$$\n\nWe now calculate this time for both encoding regimes.\n\n**Regime 1: Single-Precision Floating Point ($32$ bits)**\n\nIn this case, the number of bits per parameter is $b_{32} = 32$.\nThe number of parameters is $d = 10^7$.\nThe size of a single model update is:\n$$S_{32} = d \\times b_{32} = 10^7 \\times 32 = 3.2 \\times 10^8 \\text{ bits}$$\n\nThe total per-round communication time, $T_{round, 32}$, is:\n$$T_{round, 32} = \\frac{2 S_{32}}{R} = \\frac{2 \\times (3.2 \\times 10^8 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 3.2\\,\\mathrm{s} = 6.4\\,\\mathrm{s}$$\n\nThe problem requires rounding to four significant figures. Thus, $T_{round, 32} = 6.400\\,\\mathrm{s}$.\n\n**Feasibility Check for Regime 1:**\nThe feasibility budget is $T_{budget} = 2\\,\\mathrm{s}$.\nWe compare the calculated time: $6.400\\,\\mathrm{s} > 2\\,\\mathrm{s}$.\nTherefore, the $32$-bit encoding regime is **not feasible** under the given constraints.\n\n**Regime 2: Uniform Quantization ($8$ bits)**\n\nIn this case, the number of bits per parameter is $b_8 = 8$.\nThe number of parameters is $d = 10^7$.\nThe size of a single model update is:\n$$S_8 = d \\times b_8 = 10^7 \\times 8 = 8 \\times 10^7 \\text{ bits}$$\n\nThe total per-round communication time, $T_{round, 8}$, is:\n$$T_{round, 8} = \\frac{2 S_8}{R} = \\frac{2 \\times (8 \\times 10^7 \\text{ bits})}{10^8 \\text{ bits/s}} = 2 \\times 0.8\\,\\mathrm{s} = 1.6\\,\\mathrm{s}$$\n\nThe problem requires rounding to four significant figures. Thus, $T_{round, 8} = 1.600\\,\\mathrm{s}$.\n\n**Feasibility Check for Regime 2:**\nThe feasibility budget is $T_{budget} = 2\\,\\mathrm{s}$.\nWe compare the calculated time: $1.600\\,\\mathrm{s} \\le 2\\,\\mathrm{s}$.\nTherefore, the $8$-bit encoding regime is **feasible** under the given constraints.\n\nThe final answer requires the two calculated times, in seconds and rounded to four significant figures, presented as a row matrix.\nThe time for the $32$-bit regime is $6.400\\,\\mathrm{s}$.\nThe time for the $8$-bit regime is $1.600\\,\\mathrm{s}$.\nThe row matrix is $\\begin{pmatrix} 6.400 & 1.600 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n6.400 & 1.600\n\\end{pmatrix}\n}\n$$", "id": "4840332"}, {"introduction": "Applying a privacy-enhancing technology like Differential Privacy (DP) to federated learning involves a critical design choice: where to introduce the protective noise. This exercise [@problem_id:4840335] contrasts two canonical approaches—Local DP, where clients add noise before transmission, and Central DP, where a trusted aggregator adds noise to the sum. By analyzing the trade-offs between these models, particularly under different trust assumptions, you will understand how the design of a privacy mechanism affects not only model utility but also the robustness of its guarantees.", "problem": "In a hospital consortium applying Federated Learning (FL), each of $n$ sites computes a clipped local gradient $g_i \\in \\mathbb{R}^d$ on its own electronic health record cohort and participates in Secure Aggregation (SA), a cryptographic protocol that reveals to the server only the exact vector sum $G = \\sum_{i=1}^{n} g_i$ and hides all individual $g_i$. Two privacy designs are considered for protecting patient-level information: a Local Differential Privacy (DP) design and a Central Differential Privacy design. The consortium must decide which design is appropriate under different trust assumptions about SA and how the randomization should be placed.\n\nYou may assume the following well-tested foundational facts and definitions:\n\n- Differential Privacy (DP) for a randomized mechanism $M$ on datasets is defined by: for any pair of neighboring datasets $D$ and $D'$ differing in exactly $1$ individual's record and any measurable set $S$, the mechanism satisfies $(\\epsilon,\\delta)$-DP if\n$$\n\\Pr[M(D) \\in S] \\le e^{\\epsilon} \\Pr[M(D') \\in S] + \\delta.\n$$\n- Local Differential Privacy (LDP) is the special case where each client applies a randomized mechanism $M_i$ to its own input $x_i$ before any curator sees it; the neighboring relation is on two possible inputs $x_i$ and $x_i'$ for the same client.\n- Central Differential Privacy (central DP) is the case where the curator holds the aggregate dataset (or statistic) and applies a single randomized mechanism $M$ centrally to protect the contribution of any single individual in the global dataset; the neighboring relation is on datasets $D$ and $D'$ that differ in $1$ record.\n- Secure Aggregation (SA) is a cryptographic protocol ensuring that only $G$ is revealed to the server, and no individual $g_i$ is revealed; SA is not itself a DP mechanism, but it can be a trusted component that constrains what the adversary can observe.\n- Post-processing invariance: if a mechanism $M$ satisfies $(\\epsilon,\\delta)$-DP, then for any data-independent function $f$, the composed output $f(M(\\cdot))$ also satisfies $(\\epsilon,\\delta)$-DP.\n- For independent additive noise vectors, variances add under summation: if $\\tilde{g}_i = g_i + Z_i$ with independent noise $Z_i$ of zero mean and covariance $\\Sigma_i$, then the sum noise $Z = \\sum_{i=1}^{n} Z_i$ has covariance $\\sum_{i=1}^{n} \\Sigma_i$.\n- Sensitivity of the clipped sum: if each $g_i$ is clipped so that $\\|g_i\\|_2 \\le C$, then replacing any single $g_j$ with $g_j'$ changes $G$ by at most $\\|g_j - g_j'\\|_2 \\le 2C$, so the $\\ell_2$-sensitivity of $G$ is bounded.\n\nTwo plans are proposed:\n\n- Plan L (local DP): each client applies an $(\\epsilon_L,\\delta_L)$-LDP mechanism to its gradient, producing $\\tilde{g}_i = M_i(g_i)$ locally, and then SA is used to sum $\\tilde{G} = \\sum_{i=1}^{n} \\tilde{g}_i$.\n- Plan C (central DP): each client inputs its true $g_i$ into SA, which reveals only $G$ to the curator; the curator applies a single $(\\epsilon_C,\\delta_C)$-DP mechanism $M$ to $G$ to produce a released aggregate $\\hat{G} = M(G)$.\n\nAssume an honest-but-curious adversary who can observe whatever the server sees, and consider both the case where SA is trusted (it reveals only $G$ and never individual $g_i$) and the case where SA fails catastrophically (individual $g_i$ are revealed to the server).\n\nWhich statements correctly contrast local DP and central DP in this federated learning setting?\n\nA. If SA fails catastrophically so that the server sees each $\\tilde{g}_i$ in Plan L and each raw $g_i$ in Plan C, then Plan L still provides an $(\\epsilon_L,\\delta_L)$ guarantee for each client's data against the server, whereas Plan C provides no DP guarantee because no randomization has occurred prior to aggregation.\n\nB. Under trusted SA, Plan C can achieve a target $(\\epsilon_C,\\delta_C)$ at the level of the aggregate $G$ while adding random noise only once centrally, which generally yields higher utility than Plan L for the same privacy target because Plan L’s independent client-side noise accumulates across $n$ participants in the aggregate.\n\nC. In Plan L, the neighboring relation used for the $(\\epsilon_L,\\delta_L)$ guarantee is defined on global datasets $D$ and $D'$ differing in $1$ individual across all clients, not on a single client’s two possible inputs.\n\nD. In Plan C with trusted SA, the encryption and masking randomness used inside SA is the randomness that makes the release $(\\epsilon_C,\\delta_C)$-DP; the curator need not add any further randomization to $G$.\n\nE. In Plan L, releasing $\\tilde{g}_1, \\ldots, \\tilde{g}_n$ in a single round causes the per-client privacy parameter to degrade linearly in $n$ due to composition, even if each client’s mechanism individually satisfies $(\\epsilon_L,\\delta_L)$-LDP.\n\nSelect all that apply.", "solution": "The problem statement must first be validated for scientific correctness, logical consistency, and objectivity.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A federated learning consortium of $n$ hospital sites.\n- Each site $i$ computes a clipped local gradient $g_i \\in \\mathbb{R}^d$ such that $\\|g_i\\|_2 \\le C$.\n- Secure Aggregation (SA) is a cryptographic protocol that reveals only the sum $G = \\sum_{i=1}^{n} g_i$ to a server, hiding individual $g_i$.\n- Differential Privacy (DP) definition: For a mechanism $M$, neighboring datasets $D, D'$, and any measurable set $S$, $\\Pr[M(D) \\in S] \\le e^{\\epsilon} \\Pr[M(D') \\in S] + \\delta$.\n- Local Differential Privacy (LDP) is a special case where a client randomizes its own input $x_i$ with a mechanism $M_i$. The neighboring relation is between two possible inputs $x_i, x_i'$ for the same client.\n- Central Differential Privacy (central DP) is a case where a curator applies a mechanism $M$ to an aggregate. The neighboring relation is on global datasets $D, D'$ differing by one record.\n- SA is not a DP mechanism but can be a trusted component.\n- Post-processing invariance: Applying a data-independent function $f$ to a DP-protected output does not degrade privacy.\n- For independent additive noise vectors $Z_i$ with zero mean and covariance $\\Sigma_i$ added to $g_i$, the sum noise $Z = \\sum_{i=1}^{n} Z_i$ has covariance $\\sum_{i=1}^{n} \\Sigma_i$.\n- The $\\ell_2$-sensitivity of the sum $G$ with respect to changing one client's contribution is bounded by $2C$.\n- Plan L (local DP): Each client computes $\\tilde{g}_i = M_i(g_i)$ via an $(\\epsilon_L, \\delta_L)$-LDP mechanism. SA is then used to compute $\\tilde{G} = \\sum_{i=1}^{n} \\tilde{g}_i$.\n- Plan C (central DP): Clients provide their true $g_i$ to SA. SA provides $G = \\sum_{i=1}^{n} g_i$ to the curator. The curator computes a released aggregate $\\hat{G} = M(G)$ using an $(\\epsilon_C, \\delta_C)$-DP mechanism.\n- The adversary is honest-but-curious and observes what the server sees.\n- Two threat models are considered: (1) SA is trusted and works as specified. (2) SA fails catastrophically, revealing all its inputs to the server.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and logically sound.\n- **Scientifically Grounded**: The problem is based on foundational and standard concepts in privacy-preserving machine learning, namely Federated Learning, Differential Privacy (both local and central models), and Secure Aggregation. The definitions and properties provided (DP, LDP, SA, post-processing, noise summation, sensitivity) are all correct and well-established in the field.\n- **Well-Posed**: The problem is well-posed. It presents two distinct and clearly defined plans (Plan L and Plan C) and asks for a comparative analysis of their properties under different trust assumptions. The question is structured to test the understanding of these concepts, and a definitive evaluation of the given statements is possible based on the provided definitions.\n- **Objective**: The language is precise, formal, and free of subjectivity. All key terms are explicitly defined.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, contradiction, infeasibility, or ambiguity. The setup is a canonical scenario for discussing privacy in FL.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the analysis of each option.\n\n### Solution and Option-by-Option Analysis\n\n**A. If SA fails catastrophically so that the server sees each $\\tilde{g}_i$ in Plan L and each raw $g_i$ in Plan C, then Plan L still provides an $(\\epsilon_L,\\delta_L)$ guarantee for each client's data against the server, whereas Plan C provides no DP guarantee because no randomization has occurred prior to aggregation.**\n\n- **Analysis of Plan L under SA failure**: In Plan L, each client $i$ applies a local randomization mechanism $M_i$ to its gradient $g_i$ to produce $\\tilde{g}_i$. This randomization is performed locally, on the client's device, before any data is sent. The mechanism $M_i$ is, by definition, $(\\epsilon_L, \\delta_L)$-LDP. This guarantee holds with respect to the client's own data. If SA fails and the server observes $\\tilde{g}_i$, the server is seeing an already-randomized value. The LDP guarantee means that observing $\\tilde{g}_i$ provides limited information about the original $g_i$. Thus, each client's privacy is protected to the level of $(\\epsilon_L,\\delta_L)$-DP against the server.\n\n- **Analysis of Plan C under SA failure**: In Plan C, clients send their exact, non-randomized gradients $g_i$ to the SA protocol. The entire privacy guarantee hinges on two components: (1) the SA protocol successfully hiding the individual $g_i$ and only revealing the sum $G$, and (2) the curator subsequently adding noise to $G$. If SA fails catastrophically, the server sees the raw gradients $\\{g_1, g_2, \\ldots, g_n\\}$. Since no randomization has occurred at this point, the server has access to the exact, unprotected contributions from each client. This constitutes a complete privacy failure; there is no DP guarantee whatsoever.\n\n- **Verdict**: The statement is a correct and fundamental comparison of the trust models in local versus central DP. LDP is robust to failures of the central server or aggregator, while central DP is critically dependent on them. **Correct**.\n\n**B. Under trusted SA, Plan C can achieve a target $(\\epsilon_C,\\delta_C)$ at the level of the aggregate $G$ while adding random noise only once centrally, which generally yields higher utility than Plan L for the same privacy target because Plan L’s independent client-side noise accumulates across $n$ participants in the aggregate.**\n\n- **Analysis of Plan C noise**: In Plan C, the trusted curator receives the exact sum $G = \\sum_{i=1}^{n} g_i$. To make this release $(\\epsilon_C, \\delta_C)$-DP, the curator adds noise. For example, using the Gaussian mechanism, the noise standard deviation is proportional to the $\\ell_2$-sensitivity of $G$. The problem states this sensitivity is bounded by $2C$. The variance of the added noise is thus proportional to $(2C)^2 = 4C^2$. The noise is added only once.\n\n- **Analysis of Plan L noise**: In Plan L, each of the $n$ clients adds noise to their local gradient $g_i$ to achieve $(\\epsilon_L, \\delta_L)$-LDP. The sensitivity of each client's $g_i$ is $C$ (as $\\|g_i\\|_2 \\le C$). The noise added by each client, say $Z_i$, must be large enough to obscure a change of this magnitude. Let's assume the variance of each $Z_i$ is $\\sigma_L^2$, which is proportional to $C^2$. The server receives the sum $\\tilde{G} = \\sum_{i=1}^{n} \\tilde{g}_i = G + \\sum_{i=1}^{n} Z_i$. Since the clients' noises $Z_i$ are independent, their variances add. The total noise in the final aggregate has a variance of $\\sum_{i=1}^{n} \\text{Var}(Z_i) = n \\sigma_L^2$. This variance is proportional to $nC^2$.\n\n- **Comparison**: For a comparable level of privacy, the noise variance in Plan L scales with $n$ (the number of participants), whereas in Plan C it is constant with respect to $n$. For any $n > 4$, the total noise in Plan L is significantly larger than in Plan C, assuming similar privacy parameters and mechanisms. This substantially larger noise in the aggregate gradient $\\tilde{G}$ degrades its quality, leading to lower model accuracy and/or slower convergence. Therefore, Plan C generally yields much higher utility (accuracy) than Plan L.\n\n- **Verdict**: The statement correctly identifies the central utility-privacy trade-off between the local and central DP models. The accumulation of noise is the primary drawback of the local model. **Correct**.\n\n**C. In Plan L, the neighboring relation used for the $(\\epsilon_L,\\delta_L)$ guarantee is defined on global datasets $D$ and $D'$ differing in $1$ individual across all clients, not on a single client’s two possible inputs.**\n\n- **Analysis**: This statement describes the neighboring relation for central DP, not local DP. The problem correctly defines LDP: \"Local Differential Privacy (LDP) is the special case where each client applies a randomized mechanism $M_i$ to its own input $x_i$... the neighboring relation is on two possible inputs $x_i$ and $x_i'$ for the same client.\" Plan L is explicitly an LDP design. The $(\\epsilon_L, \\delta_L)$ guarantee for client $i$ ensures that an adversary seeing $\\tilde{g}_i$ cannot reliably distinguish if the client's local data was $D_i$ or a neighboring $D'_i$ (which differ by one patient record). The privacy guarantee is local to the client. The concept of a global dataset is irrelevant to the LDP guarantee for a single client.\n\n- **Verdict**: The statement fundamentally misunderstands and mischaracterizes Local Differential Privacy. **Incorrect**.\n\n**D. In Plan C with trusted SA, the encryption and masking randomness used inside SA is the randomness that makes the release $(\\epsilon_C,\\delta_C)$-DP; the curator need not add any further randomization to $G$.**\n\n- **Analysis**: This statement confuses cryptographic security with the statistical privacy of DP. Secure Aggregation uses cryptographic techniques (like secret sharing or homomorphic encryption) to compute the sum $G$ while keeping the addends $g_i$ secret from the server. The randomness in SA (e.g., cryptographic keys, random masks that cancel upon summation) is designed to provide this cryptographic security. However, the output of a perfect SA protocol is the *exact* sum $G = \\sum g_i$. Releasing this exact sum $G$ does not satisfy DP. An adversary with background knowledge (e.g., knowing all other gradients) could perfectly deduce a specific client's contribution. DP requires the injection of calibrated *statistical noise* into the output, a process that is separate from SA. In Plan C, the curator receives $G$ from SA and *then* must apply a DP mechanism $M$ (e.g., add noise) to produce the public, privacy-preserving output $\\hat{G}$.\n\n- **Verdict**: The statement is incorrect. The randomness from SA is for security, not for DP-style privacy. Additional, calibrated noise is required. **Incorrect**.\n\n**E. In Plan L, releasing $\\tilde{g}_1, \\ldots, \\tilde{g}_n$ in a single round causes the per-client privacy parameter to degrade linearly in $n$ due to composition, even if each client’s mechanism individually satisfies $(\\epsilon_L,\\delta_L)$-LDP.**\n\n- **Analysis**: This statement misapplies the concept of privacy composition. Composition theorems in DP describe how privacy degrades when the *same dataset* is queried multiple times. In the described scenario, we are considering the privacy of a single client, say client $j$. The adversary sees the set of outputs $\\{\\tilde{g}_1, \\ldots, \\tilde{g}_n\\}$. The privacy of client $j$'s data is protected by the mechanism $M_j$, which randomizes $g_j$ to produce $\\tilde{g}_j$. The other outputs, $\\tilde{g}_i$ for $i \\neq j$, are generated from the data of *other clients*, which are independent of client $j$'s data. Because $\\tilde{g}_i$ (for $i \\neq j$) is statistically independent of $g_j$, observing $\\tilde{g}_i$ provides no information about $g_j$. Therefore, the privacy guarantee for client $j$ is solely determined by its own LDP mechanism $M_j$ and is not affected by the release of other clients' independently privatized reports. The per-client privacy guarantee remains $(\\epsilon_L, \\delta_L)$.\n\n- **Verdict**: The statement incorrectly applies composition principles. The privacy of one client's data is not degraded by the release of independently-randomized data from other clients. **Incorrect**.", "answer": "$$\\boxed{AB}$$", "id": "4840335"}]}