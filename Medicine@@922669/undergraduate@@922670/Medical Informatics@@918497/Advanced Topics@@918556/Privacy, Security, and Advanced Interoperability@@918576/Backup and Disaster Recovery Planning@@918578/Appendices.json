{"hands_on_practices": [{"introduction": "A cornerstone of any disaster recovery plan is the ability to replicate data to a secure, remote location. This practice addresses a fundamental question: how much network bandwidth is required to transfer nightly data changes within the designated backup window? By working through this calculation, you will learn to account for real-world factors like data compression and protocol overhead, translating a high-level data volume into a concrete network capacity requirement. Mastering this skill is essential for designing and budgeting for a robust disaster recovery infrastructure. [@problem_id:4823533]", "problem": "A regional hospital replicates changed electronic health record data nightly from its on-premises primary data center to a cloud-based disaster recovery site. The storage team reports that the nightly change set is $3\\ \\text{TB}$, measured in decimal terabytes where $1\\ \\text{TB} = 10^{12}\\ \\text{bytes}$. The data reduction applied by the backup software achieves a sustained compression ratio of $2{:}1$ on the wire. The networking team measures that transport security and protocol headers introduce a constant protocol overhead of $10\\%$ on the transmitted payload. Assume that $1\\ \\text{byte} = 8\\ \\text{bits}$, $1\\ \\text{Mb/s} = 10^{6}\\ \\text{bits/s}$, and that the replication runs continuously at a steady rate across the entire window with negligible packet loss and no competing traffic.\n\nUsing the fundamental relation that a constant data rate equals total transmitted volume divided by total transmission time, and the definition that a compression ratio of $a{:}b$ reduces payload size by a factor of $\\frac{b}{a}$ while a protocol overhead of $p$ increases transmitted volume to a factor of $(1+p)$ of the payload, compute the minimum wide area network bandwidth required to guarantee that the entire compressed and protocol-encapsulated nightly change set is transmitted within a $6$-hour window.\n\nExpress your answer in megabits per second $\\left(\\text{Mb/s}\\right)$ and round to three significant figures.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of data communications, is well-posed with sufficient and consistent data, and is expressed in objective, unambiguous language. All required definitions, constants, and constraints are provided. We may therefore proceed with the calculation.\n\nThe objective is to compute the minimum required network bandwidth in megabits per second ($\\text{Mb/s}$) to transfer a given volume of data within a specified time frame, accounting for data compression and protocol overhead. We will follow a systematic process of first determining the total volume of data to be transmitted and then dividing by the total time available for transmission.\n\nLet $V_{raw}$ be the initial size of the nightly change set.\nThe problem states $V_{raw} = 3\\ \\text{TB}$.\nUsing the provided definition, $1\\ \\text{TB} = 10^{12}\\ \\text{bytes}$, we have:\n$$V_{raw} = 3 \\times 10^{12}\\ \\text{bytes}$$\n\nThe data is compressed with a ratio of $2{:}1$. The problem specifies that a compression ratio of $a{:}b$ reduces the payload size by a factor of $\\frac{b}{a}$. Here, $a=2$ and $b=1$. The compression factor is $\\frac{1}{2}$.\nLet $V_{payload}$ be the size of the data after compression.\n$$V_{payload} = V_{raw} \\times \\frac{1}{2} = (3 \\times 10^{12}\\ \\text{bytes}) \\times \\frac{1}{2} = 1.5 \\times 10^{12}\\ \\text{bytes}$$\nThis is the size of the compressed payload that needs to be transmitted.\n\nNext, we account for protocol overhead. The overhead is given as $p = 10\\%$, which is equivalent to a decimal value of $p=0.1$. The problem defines that this overhead increases the transmitted volume by a factor of $(1+p)$.\nLet $V_{tx}$ be the total volume of data that must be transmitted over the network, including the payload and the overhead.\n$$V_{tx} = V_{payload} \\times (1 + p) = (1.5 \\times 10^{12}\\ \\text{bytes}) \\times (1 + 0.1) = (1.5 \\times 10^{12}\\ \\text{bytes}) \\times 1.1$$\n$$V_{tx} = 1.65 \\times 10^{12}\\ \\text{bytes}$$\n\nNetwork bandwidth is typically measured in bits per second. Therefore, we must convert the total transmitted volume from bytes to bits. Using the given conversion $1\\ \\text{byte} = 8\\ \\text{bits}$:\n$$V_{tx, bits} = V_{tx} \\times 8\\ \\frac{\\text{bits}}{\\text{byte}} = (1.65 \\times 10^{12}\\ \\text{bytes}) \\times 8\\ \\frac{\\text{bits}}{\\text{byte}}$$\n$$V_{tx, bits} = 13.2 \\times 10^{12}\\ \\text{bits} = 1.32 \\times 10^{13}\\ \\text{bits}$$\n\nThe transmission must be completed within a $6$-hour time window. Let this time be $T$.\n$$T = 6\\ \\text{hours}$$\nTo calculate the data rate in bits per second, we must convert this time window into seconds.\n$$T_{sec} = 6\\ \\text{hours} \\times \\left(\\frac{60\\ \\text{minutes}}{1\\ \\text{hour}}\\right) \\times \\left(\\frac{60\\ \\text{seconds}}{1\\ \\text{minute}}\\right) = 6 \\times 3600\\ \\text{seconds}$$\n$$T_{sec} = 21600\\ \\text{s} = 2.16 \\times 10^4\\ \\text{s}$$\n\nThe required bandwidth, or data rate $R$, is the total transmitted volume in bits divided by the transmission time in seconds.\n$$R = \\frac{V_{tx, bits}}{T_{sec}} = \\frac{1.32 \\times 10^{13}\\ \\text{bits}}{2.16 \\times 10^4\\ \\text{s}}$$\n$$R = \\left(\\frac{1.32}{2.16}\\right) \\times 10^{13-4}\\ \\frac{\\text{bits}}{\\text{s}} = \\left(\\frac{11}{18}\\right) \\times 10^9\\ \\frac{\\text{bits}}{\\text{s}}$$\nNumerically, this is approximately $0.6111... \\times 10^9\\ \\text{b/s}$, or $611,111,111.11...\\ \\text{b/s}$.\n\nThe problem requires the answer in megabits per second ($\\text{Mb/s}$), where $1\\ \\text{Mb/s} = 10^6\\ \\text{b/s}$.\n$$R_{Mbps} = \\frac{R}{10^6} = \\frac{\\left(\\frac{11}{18}\\right) \\times 10^9\\ \\text{b/s}}{10^6\\ \\text{b/s/Mb/s}} = \\left(\\frac{11}{18}\\right) \\times 10^3\\ \\text{Mb/s}$$\n$$R_{Mbps} \\approx 0.6111... \\times 10^3\\ \\text{Mb/s} = 611.111...\\ \\text{Mb/s}$$\n\nFinally, we round the result to three significant figures. The first three significant figures are $6$, $1$, and $1$. The fourth figure is $1$, which is less than $5$, so we round down.\n$$R_{rounded} = 611\\ \\text{Mb/s}$$\nThis is the minimum wide area network bandwidth required to meet the specified disaster recovery objective.", "answer": "$$\\boxed{611}$$", "id": "4823533"}, {"introduction": "Effective disaster recovery planning extends beyond initial setup; it requires continuous monitoring to ensure that performance meets critical business and clinical requirements. This problem places you in a realistic scenario where an observed replication lag exceeds the defined Recovery Point Objective (RPO), creating a risk of unacceptable data loss. You will analyze the root causes of the lag and evaluate different architectural solutions, learning to weigh the trade-offs between performance, cost, and risk to make a defensible recommendation. This exercise hones the crucial skill of adapting a system's design to meet its evolving operational realities. [@problem_id:4823584]", "problem": "A regional hospital operates an Electronic Health Record (EHR) platform that stores Electronic Protected Health Information (EPHI). To meet the Health Insurance Portability and Accountability Act (HIPAA) Security Rule requirements for availability and integrity, the hospital has established a Recovery Point Objective (RPO) of $60$ seconds for all EPHI writes. The system replicates to a secondary region using asynchronous, batched, cross-region database replication. Current observability shows an average cross-region replication lag of $90$ seconds during peak periods. The measured workload characteristics are: average write throughput $\\lambda = 15$ writes per second, periodic bursts at $\\lambda_b = 50$ writes per second lasting $D = 10$ seconds, a single replication stream with service capacity $\\mu = 20$ writes per second, and a batch flush interval $T_f = 30$ seconds. The network round-trip time between regions is approximately $150$ $\\mathrm{ms}$.\n\nAssume that during a region failover, any committed writes not yet present on the remote replica are lost and must be recovered from logs or re-entered. Using fundamental definitions of Recovery Point Objective (RPO) as the maximum tolerable data age at recovery, and reasoning from first principles about commit semantics and lag accumulation under variable load, select the option(s) that most appropriately close the $30$-second gap between the observed lag and the RPO or, if closing the gap is not feasible, provide a defensible risk acceptance posture with compensating controls that specifically address integrity and availability risks to EPHI.\n\nA. Switch all EPHI writes to synchronous cross-region replication using a commit quorum of $k = 2$ out of $n = 3$ regions, accepting an increase in per-transaction latency of approximately $150$ $\\mathrm{ms}$ to ensure the remote copy is up to date at commit.\n\nB. Increase parallel replication streams from $1$ to $4$, raising effective service capacity to $\\mu' = 80$ writes per second, and reduce the batch flush interval to $T_f' = 10$ seconds.\n\nC. Keep the architecture unchanged but add weekly reconciliation and audit log review processes to detect and manually re-enter missing records after failover; document risk acceptance based on historical incident rates.\n\nD. Implement encryption at rest using Hardware Security Modules (HSMs) and immutable audit logs, while retaining the current asynchronous replication lag of $90$ seconds, to argue that confidentiality and accountability controls justify accepting the RPO gap.\n\nSelect all that apply.", "solution": "The validity of the problem statement must first be assessed.\n\n### Step 1: Extract Givens\n-   **System Requirement:** Recovery Point Objective (RPO) $\\le 60$ seconds.\n-   **Observed System State:** Average cross-region replication lag during peak periods is $90$ seconds.\n-   **Replication Architecture:** Asynchronous, batched, cross-region database replication.\n-   **Workload - Average:** Write throughput $\\lambda = 15$ writes per second.\n-   **Workload - Peak Burst:** Write throughput $\\lambda_b = 50$ writes per second for a duration $D = 10$ seconds.\n-   **Replication System Parameters:**\n    -   Number of replication streams: $1$.\n    -   Service capacity per stream: $\\mu = 20$ writes per second.\n    -   Batch flush interval: $T_f = 30$ seconds.\n-   **Network Parameter:** Round-trip time (RTT) between regions is approximately $150 \\, \\mathrm{ms}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement concerns the field of systems engineering and disaster recovery, applying fundamental concepts to a medical informatics scenario. The core task is to reconcile a required RPO with observed system performance.\n\n1.  **Scientific Groundedness and Consistency:** The problem is grounded in queuing theory and distributed systems principles. We can model the components of replication lag to verify the given numbers. The total replication lag ($L$) is the sum of the batching delay ($L_{batch}$), the queuing delay ($L_{queue}$), and the network transit delay ($L_{network}$).\n    -   **Batching Delay ($L_{batch}$):** A write arriving just after a batch flush must wait for the next one. The maximum batching delay is equal to the flush interval, so $L_{batch, max} = T_f = 30$ seconds.\n    -   **Queuing Delay ($L_{queue}$):** This occurs when the arrival rate exceeds the service rate.\n        -   Under average load, $\\lambda = 15 \\, \\mathrm{s}^{-1}$ and $\\mu = 20 \\, \\mathrm{s}^{-1}$. Since $\\lambda < \\mu$, the queue does not grow indefinitely.\n        -   During a peak burst, $\\lambda_b = 50 \\, \\mathrm{s}^{-1}$, which is greater than $\\mu = 20 \\, \\mathrm{s}^{-1}$. A backlog accumulates.\n        -   Rate of queue growth during burst: $\\lambda_b - \\mu = 50 - 20 = 30$ writes/second.\n        -   Total backlog accumulated during the $D = 10$ second burst: $(30 \\, \\mathrm{s}^{-1}) \\times (10 \\, \\mathrm{s}) = 300$ writes.\n        -   After the burst, assuming the load returns to the average $\\lambda = 15 \\, \\mathrm{s}^{-1}$, the backlog is cleared at a rate of $\\mu - \\lambda = 20 - 15 = 5$ writes/second.\n        -   Time to clear the backlog: $(300 \\, \\mathrm{writes}) / (5 \\, \\mathrm{writes/s}) = 60$ seconds. This is the peak queuing delay, $L_{queue, peak} = 60$ seconds.\n    -   **Network Delay ($L_{network}$):** The one-way transit time is approximately $RTT / 2 = 150 \\, \\mathrm{ms} / 2 = 75 \\, \\mathrm{ms} = 0.075$ seconds, which is negligible compared to the other delays.\n    -   **Total Peak Lag:** $L_{peak} \\approx L_{batch, max} + L_{queue, peak} = 30 \\, \\mathrm{s} + 60 \\, \\mathrm{s} = 90$ seconds.\n    \n    This calculated peak lag precisely matches the \"observed... replication lag of $90$ seconds during peak periods\" given in the problem. The problem statement is internally consistent and factually sound from a systems engineering perspective.\n\n2.  **Well-Posed and Objective:** The problem provides a clear, quantitative gap between a requirement (RPO of $60$ s) and performance ($90$ s lag). The terminology is standard in the field. The objective is clearly stated: close the gap or provide a defensible risk acceptance posture. The problem is well-posed.\n\n### Step 3: Verdict and Action\nThe problem is valid. The premises are scientifically sound, internally consistent, and well-defined. I will now proceed with the solution by evaluating each option.\n\n---\n\nThe objective is to reduce the peak replication lag from $90$ seconds to meet the RPO of $60$ seconds. The current lag is composed of approximately $30$ seconds of batching delay and $60$ seconds of queuing delay. A successful solution must address one or both of these components.\n\n### Option-by-Option Analysis\n\n**A. Switch all EPHI writes to synchronous cross-region replication using a commit quorum of $k = 2$ out of $n = 3$ regions, accepting an increase in per-transaction latency of approximately $150$ $\\mathrm{ms}$ to ensure the remote copy is up to date at commit.**\n\n-   **Principle:** Synchronous replication ensures that a transaction is not confirmed (committed) to the application until it has been durably stored on a required number of replicas. In this case, with a quorum of $k=2$, a write must be present in the primary region and at least one remote region before acknowledgement.\n-   **Impact on RPO:** If the primary region fails, any committed write is guaranteed to exist on at least one other replica. This means the data loss, measured in time for committed transactions, is zero. Therefore, the RPO becomes $0$ seconds. An RPO of $0$ s is well within the required RPO of $60$ s.\n-   **Trade-offs:** The statement correctly identifies the primary trade-off: increased write latency. The application must wait for a network round-trip ($150$ ms) plus processing time on the remote replica before the write is confirmed.\n-   **Verdict:** This is a highly effective, albeit potentially costly in terms of performance, way to solve the problem. It completely eliminates the risk of data loss for committed writes, thereby closing the RPO gap. It is a valid and appropriate technical solution. **Correct**.\n\n**B. Increase parallel replication streams from $1$ to $4$, raising effective service capacity to $\\mu' = 80$ writes per second, and reduce the batch flush interval to $T_f' = 10$ seconds.**\n\n-   **Principle:** This option modifies the parameters of the existing asynchronous replication system. It targets both sources of lag identified in the validation step.\n-   **Impact on Queuing Delay:** The new effective service capacity is $\\mu' = 4 \\times 20 \\, \\mathrm{s}^{-1} = 80$ writes per second. During peak bursts, the arrival rate is $\\lambda_b = 50$ writes per second. Since the new service capacity $\\mu'$ is greater than the peak arrival rate $\\lambda_b$ ($80 > 50$), the replication system can now process writes faster than they arrive, even during bursts. This eliminates the formation of a persistent backlog and reduces the peak queuing delay $L_{queue}$ to effectively $0$ seconds.\n-   **Impact on Batching Delay:** The batch flush interval is reduced from $T_f = 30$ s to $T_f' = 10$ s. This reduces the maximum batching delay, $L_{batch, max}$, to $10$ seconds.\n-   **New Total Lag:** The new total peak lag will be the sum of the new maximum batching delay and the new (negligible) queuing delay: $L'_{peak} \\approx L'_{batch, max} + L'_{queue, peak} \\approx 10 \\, \\mathrm{s} + 0 \\, \\mathrm{s} = 10$ seconds.\n-   **Verdict:** A peak lag of $10$ seconds is well within the required RPO of $60$ seconds. This option addresses the root causes of the lag within the existing asynchronous framework and successfully closes the gap. It is a valid and appropriate technical solution. **Correct**.\n\n**C. Keep the architecture unchanged but add weekly reconciliation and audit log review processes to detect and manually re-enter missing records after failover; document risk acceptance based on historical incident rates.**\n\n-   **Principle:** This is a procedural control and risk acceptance strategy, rather than a technical solution. It accepts that up to $90$ seconds of data will be lost in a failover.\n-   **Feasibility and Risk:** For an EHR system with an average write rate of $\\lambda = 15$ writes/s, a $90$-second data loss window implies $15 \\times 90 = 1350$ lost records. During peak times, this number would be higher. Manually reconciling and re-entering thousands of clinical records (e.g., medication orders, lab results, vital signs) from logs is exceedingly slow, costly, and critically, prone to human error. Such errors could directly impact patient safety.\n-   **HIPAA Compliance:** The HIPAA Security Rule requires ensuring the *integrity* and *availability* of EPHI. A strategy that knowingly accepts routine data loss and relies on a high-risk manual recovery process for a critical system is not a defensible posture, especially when technically and economically feasible solutions (like A and B) exist. Documenting risk acceptance does not absolve a covered entity of its responsibility to implement reasonable and appropriate safeguards.\n-   **Verdict:** This option fails to \"appropriately\" close the gap. Instead, it proposes accepting a significant and avoidable risk to data integrity and patient safety. It is not a defensible strategy in this context. **Incorrect**.\n\n**D. Implement encryption at rest using Hardware Security Modules (HSMs) and immutable audit logs, while retaining the current asynchronous replication lag of $90$ seconds, to argue that confidentiality and accountability controls justify accepting the RPO gap.**\n\n-   **Principle:** This option proposes adding security controls from different domains (confidentiality and accountability) as a compensating control for a deficiency in availability.\n-   **Relevance:** The problem is about data **availability** and **integrity** in the face of a disaster, as quantified by the RPO.\n    -   **Encryption at rest** is a **confidentiality** control. It protects data from being read by unauthorized parties who gain access to the physical storage. It does not prevent the data from being lost in a failover.\n    -   **Immutable audit logs** are an **accountability** control. They provide a secure record of actions performed on the system. While they might help identify what data was lost, they do not prevent the loss of the primary data itself.\n-   **Category Error:** The HIPAA Security Rule mandates safeguards for Confidentiality, Integrity, and Availability as three distinct, co-equal requirements. Strengthening controls in one area (Confidentiality) does not compensate for a failure in another (Availability). The argument presented in this option is a non-sequitur; the proposed controls are irrelevant to the problem of data loss during a failover event.\n-   **Verdict:** This option does not address the RPO gap in any way. The proposed controls, while good practices for other reasons, are fundamentally mismatched to the specified problem. **Incorrect**.", "answer": "$$\\boxed{AB}$$", "id": "4823584"}, {"introduction": "Choosing the right backup strategy involves a complex interplay of trade-offs between restore speed, impact on production systems, and storage repository performance. This advanced practice challenges you to move beyond simple metrics and build a quantitative model to compare two common backup policies: the traditional weekly full with daily incrementals versus a daily synthetic full. You will formalize the constraints imposed by I/O budgets and backup windows to determine the precise conditions under which one strategy outperforms the other. This problem exemplifies how a medical informatics professional can use modeling to make optimal, evidence-based decisions for complex data protection architectures. [@problem_id:4823600]", "problem": "A hospital’s Electronic Health Record (EHR) primary dataset has size $S$ and is protected by nightly backups. Each day, a fraction $c$ of the dataset changes, so the daily incremental volume is $cS$. The hospital is considering switching from a weekly full plus daily incremental backup policy to a daily synthetic full backup policy constructed by merging the previous full with the day’s incremental on the backup repository. The objective is to reduce expected restore time while not extending the production backup window, subject to I/O budget constraints on both the production system and the backup repository.\n\nAssume the following well-tested bases:\n- The time to transfer volume $V$ over a channel of sustained throughput $\\Theta$ is $t = V / \\Theta$.\n- I/O budget constraints impose upper bounds on sustained throughput available to backup processes within specified time windows.\n- A restore that replays a chain of incrementals introduces an average per-incremental latency penalty due to index lookups and random I/O.\n\nParameters:\n- Dataset size: $S = 12{,}000{,}000$ MB.\n- Daily change rate (unknown): $c \\in [0,1]$.\n- Production I/O budget for backup reads: $B_{p} = 650$ MB/s.\n- Network throughput from production to the backup gateway: $T_{n} = 500$ MB/s.\n- Nightly production backup window duration: $W = 4$ hours.\n- Backup repository I/O budget available to the synthetic merge: $U_{b} = 600$ MB/s, available for $M = 12$ hours each day (outside the production window).\n- Restore read throughput from backup repository to the restore target: $R_{b} = 800$ MB/s.\n- Traditional policy: a full backup once per week and daily incrementals thereafter; consider a worst-case restore at day index $i = 6$ since the last full.\n- Average per-incremental restore latency penalty (traditional chain rehydration): $L = 20$ s.\n- Synthetic full read slowdown factor (due to deduplication fragmentation): $\\phi = 1.3$ (unitless).\n- Traditional chain read slowdown factor: $\\psi = 1.05$ (unitless).\n\nUse the bases above to:\n- Derive the condition under which the synthetic full restore time is strictly less than the traditional chain restore time on day index $i$, in terms of $c$ and the given parameters.\n- Formalize the production backup window constraint using the effective production-to-backup throughput $T_{p} = \\min(B_{p}, T_{n})$.\n- Formalize the backup repository I/O budget constraint for the daily synthetic merge by accounting for reads of the previous full and the day’s incremental and the write of the new full.\n\nThen, using the provided parameter values, compute the maximum daily change rate $c_{\\star}$ such that:\n- A daily synthetic full backup strictly reduces restore time relative to the traditional chain on day index $i$, and\n- The production backup window is not extended, and\n- The synthetic merge stays within the backup repository’s daily I/O budget.\n\nExpress $c_{\\star}$ as a unitless decimal. Round your final answer to four significant figures.", "solution": "The problem asks for the maximum daily change rate, denoted as $c_{\\star}$, for an Electronic Health Record (EHR) dataset that satisfies three specific conditions. These conditions relate to restore time, the production backup window, and the I/O budget of the backup repository. We will formulate each condition as an inequality involving the change rate $c$ and then solve for the maximum value of $c$ that satisfies all constraints.\n\nFirst, we establish the time units for consistency. All throughputs are given in MB/s, so we will convert time windows from hours to seconds.\nThe nightly production backup window is $W = 4 \\text{ hours} = 4 \\times 3600 \\text{ s} = 14400 \\text{ s}$.\nThe backup repository's daily processing window is $M = 12 \\text{ hours} = 12 \\times 3600 \\text{ s} = 43200 \\text{ s}$.\n\nThe problem is solved by deriving three inequalities for $c$ based on the given constraints.\n\n1.  **Restore Time Constraint**\n    The first condition is that the restore time for the daily synthetic full backup policy, $T_{\\text{restore,synth}}$, must be strictly less than the worst-case restore time for the traditional policy, $T_{\\text{restore,trad}}$.\n\n    The traditional policy involves restoring one full backup of size $S$ and $i$ incremental backups, each of average size $cS$. The worst case is given for day index $i=6$. The total data volume to be read is $S + i(cS) = S(1+ic)$. This process is subject to a read slowdown factor $\\psi$ and a latency penalty $L$ for each of the $i$ incrementals. The restore is performed at a read throughput of $R_b$.\n    $$T_{\\text{restore,trad}} = \\psi \\frac{S(1+ic)}{R_b} + iL$$\n\n    The synthetic full backup policy involves restoring a single file of size $S$. This process is subject to a read slowdown factor $\\phi$ due to deduplication fragmentation.\n    $$T_{\\text{restore,synth}} = \\phi \\frac{S}{R_b}$$\n\n    The constraint is $T_{\\text{restore,synth}} < T_{\\text{restore,trad}}$:\n    $$\\phi \\frac{S}{R_b} < \\psi \\frac{S(1+ic)}{R_b} + iL$$\n    We can rearrange this inequality to solve for $c$:\n    $$\\phi \\frac{S}{R_b} - \\psi \\frac{S}{R_b} - iL < \\frac{\\psi Sic}{R_b}$$\n    $$\\frac{S}{R_b}(\\phi - \\psi) - iL < c \\left(\\frac{\\psi Si}{R_b}\\right)$$\n    $$c > \\frac{\\frac{S}{R_b}(\\phi - \\psi) - iL}{\\frac{\\psi Si}{R_b}} = \\frac{S(\\phi - \\psi) - iLR_b}{\\psi Si}$$\n    $$c > \\frac{\\phi - \\psi}{\\psi i} - \\frac{L R_b}{\\psi S}$$\n    This inequality provides a lower bound for $c$.\n\n2.  **Production Backup Window Constraint**\n    The second condition is that the daily backup of incremental changes from the production system must not exceed the available backup window $W$. The volume of data to be transferred is the daily change, $cS$. The transfer is limited by the effective production-to-backup throughput, $T_p$, which is the minimum of the production I/O budget for backup reads, $B_p$, and the network throughput, $T_n$.\n    $$T_p = \\min(B_p, T_n)$$\n    The time required for this backup is $t_{\\text{backup}} = \\frac{cS}{T_p}$. This must be less than or equal to $W$:\n    $$\\frac{cS}{T_p} \\le W$$\n    Solving for $c$, we get an upper bound:\n    $$c \\le \\frac{W T_p}{S}$$\n\n3.  **Backup Repository I/O Budget Constraint**\n    The third condition is that the daily synthetic merge process on the backup repository must be completed within its allotted time window $M$. This process involves reading the previous full backup (volume $S$), reading the day's incremental (volume $cS$), and writing the new synthetic full backup (volume $S$). The total I/O volume is the sum of these operations:\n    $$V_{\\text{merge,IO}} = S + cS + S = S(2+c)$$\n    This entire operation is sustained by the backup repository's I/O budget, which provides a throughput of $U_b$. The time taken for the merge is $t_{\\text{merge}} = \\frac{V_{\\text{merge,IO}}}{U_b}$. This must be less than or equal to $M$:\n    $$\\frac{S(2+c)}{U_b} \\le M$$\n    Solving for $c$, we obtain another upper bound:\n    $$S(2+c) \\le M U_b \\implies 2+c \\le \\frac{M U_b}{S} \\implies c \\le \\frac{M U_b}{S} - 2$$\n\n**Calculation of $c_{\\star}$**\nWe are looking for the maximum value $c_{\\star}$ that satisfies all three inequalities. This value will be the minimum of the derived upper bounds, provided this value also satisfies the lower bound inequality.\n$$c_{\\star} = \\min\\left(\\frac{W T_p}{S}, \\frac{M U_b}{S} - 2\\right)$$\n\nLet's now substitute the given parameter values:\n$S = 12,000,000$ MB\n$c \\in [0,1]$\n$B_p = 650$ MB/s\n$T_n = 500$ MB/s\n$W = 14400$ s\n$U_b = 600$ MB/s\n$M = 43200$ s\n$R_b = 800$ MB/s\n$i = 6$\n$L = 20$ s\n$\\phi = 1.3$\n$\\psi = 1.05$\n\nFirst, we calculate the lower bound for $c$ from the restore time constraint:\n$$c > \\frac{1.3 - 1.05}{1.05 \\times 6} - \\frac{20 \\text{ s} \\times 800 \\text{ MB/s}}{1.05 \\times 12,000,000 \\text{ MB}} = \\frac{0.25}{6.3} - \\frac{16000}{12,600,000}$$\n$$c > 0.0396825... - 0.0012698... = 0.0384127...$$\n\nNext, we calculate the upper bounds.\nFor the production window constraint, we first find $T_p$:\n$$T_p = \\min(650 \\text{ MB/s}, 500 \\text{ MB/s}) = 500 \\text{ MB/s}$$\nThe upper bound from this constraint is:\n$$c \\le \\frac{W T_p}{S} = \\frac{14400 \\text{ s} \\times 500 \\text{ MB/s}}{12,000,000 \\text{ MB}} = \\frac{7,200,000}{12,000,000} = 0.6$$\n\nFor the backup repository budget constraint, the upper bound is:\n$$c \\le \\frac{M U_b}{S} - 2 = \\frac{43200 \\text{ s} \\times 600 \\text{ MB/s}}{12,000,000 \\text{ MB}} - 2 = \\frac{25,920,000}{12,000,000} - 2$$\n$$c \\le 2.16 - 2 = 0.16$$\n\nThe set of all valid $c$ must satisfy $c > 0.0384127...$, $c \\le 0.6$, and $c \\le 0.16$.\nCombining these, the feasible range for $c$ is:\n$$0.0384127... < c \\le \\min(0.6, 0.16)$$\n$$0.0384127... < c \\le 0.16$$\nThe maximum daily change rate $c_{\\star}$ is the supremum of this interval, which is the upper bound.\n$$c_{\\star} = 0.16$$\nThe problem requires the answer to be rounded to four significant figures.\n$$c_{\\star} = 0.1600$$", "answer": "$$\\boxed{0.1600}$$", "id": "4823600"}]}