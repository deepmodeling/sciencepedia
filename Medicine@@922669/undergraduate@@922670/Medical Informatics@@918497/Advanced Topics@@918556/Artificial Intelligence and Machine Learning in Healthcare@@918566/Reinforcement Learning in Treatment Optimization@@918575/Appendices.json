{"hands_on_practices": [{"introduction": "To begin, we ground our understanding in the core framework of Reinforcement Learning: the Markov Decision Process (MDP). This first practice challenges you to model a simplified acute care scenario as an MDP, where all the rules—the probabilities of patient state transitions and the associated rewards—are known. By applying the principle of optimality, you will calculate the best possible treatment strategy, providing a foundational benchmark for how RL formulates and solves sequential decision problems. [@problem_id:4855005]", "problem": "A clinician is designing a Reinforcement Learning (RL) strategy for sequential treatment in an acute care setting. Model the problem as a discounted finite Markov Decision Process (MDP) with states, actions, transition probabilities, and a clinically motivated reward function. The state space consists of $3$ health states: $s_{1}$ (Critical), $s_{2}$ (Stable), and $s_{3}$ (Discharged). The action space consists of $2$ treatment choices available in $s_{1}$ and $s_{2}$: $a_{1}$ (Aggressive) and $a_{2}$ (Conservative). State $s_{3}$ is absorbing with a single passive action that leaves the state unchanged.\n\nThe transition dynamics are as follows for $s_{1}$ (Critical):\n- If action $a_{1}$ (Aggressive) is chosen: $\\mathbb{P}(s_{1} \\rightarrow s_{1}) = 0.2$, $\\mathbb{P}(s_{1} \\rightarrow s_{2}) = 0.6$, $\\mathbb{P}(s_{1} \\rightarrow s_{3}) = 0.2$.\n- If action $a_{2}$ (Conservative) is chosen: $\\mathbb{P}(s_{1} \\rightarrow s_{1}) = 0.5$, $\\mathbb{P}(s_{1} \\rightarrow s_{2}) = 0.4$, $\\mathbb{P}(s_{1} \\rightarrow s_{3}) = 0.1$.\n\nFor $s_{2}$ (Stable):\n- If action $a_{1}$ (Aggressive) is chosen: $\\mathbb{P}(s_{2} \\rightarrow s_{1}) = 0.1$, $\\mathbb{P}(s_{2} \\rightarrow s_{2}) = 0.2$, $\\mathbb{P}(s_{2} \\rightarrow s_{3}) = 0.7$.\n- If action $a_{2}$ (Conservative) is chosen: $\\mathbb{P}(s_{2} \\rightarrow s_{1}) = 0.05$, $\\mathbb{P}(s_{2} \\rightarrow s_{2}) = 0.45$, $\\mathbb{P}(s_{2} \\rightarrow s_{3}) = 0.5$.\n\nState $s_{3}$ (Discharged) is absorbing: $\\mathbb{P}(s_{3} \\rightarrow s_{3}) = 1$.\n\nThe one-step reward $r(s,a,s')$ is defined to encode clinical outcome and treatment side effects. Upon transitioning to $s' \\in \\{s_{1}, s_{2}, s_{3}\\}$ from a non-absorbing state $s \\in \\{s_{1}, s_{2}\\}$ under action $a \\in \\{a_{1}, a_{2}\\}$, the reward is\n- Outcome component: $+10$ if $s' = s_{3}$ (Discharged), $+2$ if $s' = s_{2}$ (Stable), $-3$ if $s' = s_{1}$ (Critical).\n- Action cost: $-2$ for $a = a_{1}$ (Aggressive), $-0.5$ for $a = a_{2}$ (Conservative).\n\nFor transitions originating from the absorbing state $s_{3}$, all rewards are $0$. The discount factor is $\\gamma = 0.9$.\n\nStarting from the initial state $s_{1}$ (Critical) at time $t=0$, determine the optimal stationary deterministic policy that maximizes the expected discounted sum of rewards and compute the corresponding optimal state-value at $s_{1}$, denoted $V^{\\ast}(s_{1})$.\n\nBase your derivation on first principles for Markov Decision Processes and reinforcement learning, beginning from the definition of expected discounted return and the principle of optimality. You must explicitly justify the optimal policy you choose before computing $V^{\\ast}(s_{1})$.\n\nAnswer specification:\n- Report only the optimal expected discounted value $V^{\\ast}(s_{1})$ as a single real number.\n- Round your final numerical answer to four significant figures.\n- Do not include units in your final numeric answer.", "solution": "To find the optimal policy $\\pi^*$ and the corresponding optimal state-value $V^*(s_1)$, we use the Bellman optimality equations. The value of the absorbing state $s_3$ is $V^*(s_3) = 0$, as all subsequent rewards are zero. For the non-absorbing states $s_1$ and $s_2$, the optimal values $V^*(s_1)$ and $V^*(s_2)$ must satisfy the following system of equations:\n$V^*(s) = \\max_{a \\in \\{a_1, a_2\\}} Q^*(s, a)$\nwhere the action-value function is given by:\n$Q^*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma V^*(S_{t+1}) | S_t=s, A_t=a]$\n\nFirst, let's calculate the expected one-step reward for each state-action pair, which is the sum of the expected outcome component and the action cost.\nFor state $s_1$:\n- $a_1$ (Aggressive): $\\mathbb{E}[R|s_1,a_1] = 0.2(-3) + 0.6(2) + 0.2(10) - 2 = -0.6 + 1.2 + 2 - 2 = 0.6$.\n- $a_2$ (Conservative): $\\mathbb{E}[R|s_1,a_2] = 0.5(-3) + 0.4(2) + 0.1(10) - 0.5 = -1.5 + 0.8 + 1 - 0.5 = -0.2$.\n\nFor state $s_2$:\n- $a_1$ (Aggressive): $\\mathbb{E}[R|s_2,a_1] = 0.1(-3) + 0.2(2) + 0.7(10) - 2 = -0.3 + 0.4 + 7 - 2 = 5.1$.\n- $a_2$ (Conservative): $\\mathbb{E}[R|s_2,a_2] = 0.05(-3) + 0.45(2) + 0.5(10) - 0.5 = -0.15 + 0.9 + 5 - 0.5 = 5.25$.\n\nNow we can write the Bellman equations for $V_1 = V^*(s_1)$ and $V_2 = V^*(s_2)$, with $\\gamma = 0.9$ and $V_3 = V^*(s_3) = 0$:\n$Q^*(s_1, a_1) = 0.6 + 0.9[0.2V_1 + 0.6V_2 + 0.2V_3] = 0.6 + 0.18V_1 + 0.54V_2$\n$Q^*(s_1, a_2) = -0.2 + 0.9[0.5V_1 + 0.4V_2 + 0.1V_3] = -0.2 + 0.45V_1 + 0.36V_2$\n$V_1 = \\max(0.6 + 0.18V_1 + 0.54V_2, -0.2 + 0.45V_1 + 0.36V_2)$\n\n$Q^*(s_2, a_1) = 5.1 + 0.9[0.1V_1 + 0.2V_2 + 0.7V_3] = 5.1 + 0.09V_1 + 0.18V_2$\n$Q^*(s_2, a_2) = 5.25 + 0.9[0.05V_1 + 0.45V_2 + 0.5V_3] = 5.25 + 0.045V_1 + 0.405V_2$\n$V_2 = \\max(5.1 + 0.09V_1 + 0.18V_2, 5.25 + 0.045V_1 + 0.405V_2)$\n\nWe can solve this system using value iteration, but it's often faster to guess the optimal policy and verify. Let's assume the policy is to choose the action with the highest immediate expected reward: for $s_1$, choose $a_1$ (0.6 > -0.2), and for $s_2$, choose $a_2$ (5.25 > 5.1). We test the policy $\\pi = \\{s_1 \\to a_1, s_2 \\to a_2\\}$.\nUnder this policy, the value functions $V^{\\pi}_1$ and $V^{\\pi}_2$ satisfy a system of linear equations:\n$V_1 = 0.6 + 0.18V_1 + 0.54V_2 \\implies 0.82V_1 - 0.54V_2 = 0.6 \\implies 41V_1 - 27V_2 = 30$\n$V_2 = 5.25 + 0.045V_1 + 0.405V_2 \\implies -0.045V_1 + 0.595V_2 = 5.25 \\implies -9V_1 + 119V_2 = 1050$\n\nFrom the second equation, $V_1 = \\frac{119V_2 - 1050}{9}$. Substituting into the first equation:\n$41\\left(\\frac{119V_2 - 1050}{9}\\right) - 27V_2 = 30$\n$41(119V_2 - 1050) - 243V_2 = 270$\n$4879V_2 - 43050 - 243V_2 = 270$\n$4636V_2 = 43320 \\implies V_2 = \\frac{43320}{4636} = \\frac{10830}{1159} \\approx 9.344$\n\nSubstituting $V_2$ back to find $V_1$:\n$V_1 = \\frac{119(\\frac{10830}{1159}) - 1050}{9} = \\frac{1288770 - 1050 \\cdot 1159}{9 \\cdot 1159} = \\frac{1288770 - 1216950}{10431} = \\frac{71820}{10431} = \\frac{7980}{1159} \\approx 6.8852459$\n\nTo confirm this policy is optimal, we must verify that no single action change improves the value (policy improvement theorem).\n1. For state $s_1$, is $Q^*(s_1, a_1) \\geq Q^*(s_1, a_2)$? With our calculated values $V_1 \\approx 6.885$ and $V_2 \\approx 9.344$:\n$Q^*(s_1, a_1) = V_1 \\approx 6.885$\n$Q^*(s_1, a_2) = -0.2 + 0.45(6.8852) + 0.36(9.3443) \\approx -0.2 + 3.0983 + 3.3639 = 6.2622$.\nSince $6.885 > 6.2622$, choosing $a_1$ is indeed optimal for $s_1$.\n2. For state $s_2$, is $Q^*(s_2, a_2) \\geq Q^*(s_2, a_1)$?\n$Q^*(s_2, a_2) = V_2 \\approx 9.344$\n$Q^*(s_2, a_1) = 5.1 + 0.09(6.8852) + 0.18(9.3443) \\approx 5.1 + 0.6197 + 1.6820 = 7.4017$.\nSince $9.344 > 7.4017$, choosing $a_2$ is indeed optimal for $s_2$.\n\nThe policy $\\pi = \\{s_1 \\to a_1, s_2 \\to a_2\\}$ is optimal. The optimal state-value at $s_1$ is $V^*(s_1) = \\frac{7980}{1159} \\approx 6.8852459$.\nRounding to four significant figures, we get $6.885$.", "answer": "$$\\boxed{6.885}$$", "id": "4855005"}, {"introduction": "In real-world clinical settings, we rarely know the exact outcomes of our treatments beforehand, which introduces the classic exploration-exploitation dilemma: should we stick with a treatment that seems effective, or explore other options that might be even better? This exercise simplifies the problem into a multi-armed bandit scenario, where you will use the Upper Confidence Bound (UCB) algorithm to make a principled choice. This involves quantitatively balancing the potential of each treatment against the uncertainty in our estimates, a core challenge in adaptive clinical trials. [@problem_id:4855011]", "problem": "A hospital formulary committee is piloting Reinforcement Learning (RL) to optimize antihypertensive dosing for a single-day titration protocol. The scenario is modeled as a stochastic multi-armed bandit with $3$ dosing actions (indexed by $i \\in \\{1,2,3\\}$). Each action yields a reward equal to the normalized improvement in systolic blood pressure over a $24$-hour period, scaled to lie in $[0,1]$. At the current decision time, the empirical mean reward estimates and observation counts from prior patients are: for action $1$, $\\hat{\\mu}_{1} = 0.62$ with $n_{1} = 40$; for action $2$, $\\hat{\\mu}_{2} = 0.66$ with $n_{2} = 20$; for action $3$, $\\hat{\\mu}_{3} = 0.58$ with $n_{3} = 10$. Let the current time index be $t = \\sum_{i=1}^{3} n_{i}$.\n\nAssume rewards are independent and identically distributed (i.i.d.) and bounded in $[0,1]$. Using Hoeffding’s inequality as the foundational statistical principle to construct a high-probability upper bound on the unknown true mean reward $\\mu_{i}$ for each action, adopt a confidence schedule $\\delta(t) = t^{-2}$ to control exploration. From these foundations, derive an upper confidence bound for each action and use it to determine which action should be selected next according to the upper confidence bound (UCB) principle. If there is a tie, select the action with the smallest index.\n\nState the final answer as the index of the selected action, expressed as a single number with no units. No rounding is required for the final answer.", "solution": "The problem requires selecting the next action according to the Upper Confidence Bound (UCB) principle. For each action $i$, its UCB is the sum of the empirical mean reward $\\hat{\\mu}_i$ and an exploration bonus that quantifies the uncertainty in this estimate. The action with the highest UCB is chosen.\n\nThe exploration bonus is derived from Hoeffding's inequality, which for rewards bounded in $[0,1]$ states:\n$$ P(\\mu_i > \\hat{\\mu}_i + \\epsilon) \\le \\exp(-2n_i \\epsilon^2) $$\nwhere $\\mu_i$ is the true mean reward, $\\hat{\\mu}_i$ is the empirical mean from $n_i$ observations, and $\\epsilon$ is the bonus term.\n\nThe problem specifies a confidence schedule $\\delta(t) = t^{-2}$, which we set as the upper limit for the probability of error:\n$$ \\exp(-2n_i \\epsilon^2) = \\delta(t) = t^{-2} $$\nSolving for $\\epsilon$, we get the exploration bonus:\n$$ -2n_i \\epsilon^2 = \\ln(t^{-2}) = -2\\ln(t) $$\n$$ \\epsilon^2 = \\frac{\\ln(t)}{n_i} \\implies \\epsilon = \\sqrt{\\frac{\\ln(t)}{n_i}} $$\n\nThe UCB for action $i$ is therefore:\n$$ UCB_i(t) = \\hat{\\mu}_i + \\sqrt{\\frac{\\ln(t)}{n_i}} $$\n\nFirst, we calculate the total number of observations, $t$:\n$$ t = n_1 + n_2 + n_3 = 40 + 20 + 10 = 70 $$\n\nNow, we compute the UCB for each of the three actions using the given data:\n*   Action 1: $\\hat{\\mu}_1 = 0.62, n_1 = 40$\n    $UCB_1 = 0.62 + \\sqrt{\\frac{\\ln(70)}{40}}$\n*   Action 2: $\\hat{\\mu}_2 = 0.66, n_2 = 20$\n    $UCB_2 = 0.66 + \\sqrt{\\frac{\\ln(70)}{20}}$\n*   Action 3: $\\hat{\\mu}_3 = 0.58, n_3 = 10$\n    $UCB_3 = 0.58 + \\sqrt{\\frac{\\ln(70)}{10}}$\n\nTo compare these values, we can calculate them numerically. Using $\\ln(70) \\approx 4.2485$:\n$UCB_1 \\approx 0.62 + \\sqrt{\\frac{4.2485}{40}} \\approx 0.62 + \\sqrt{0.1062} \\approx 0.62 + 0.3259 = 0.9459$\n$UCB_2 \\approx 0.66 + \\sqrt{\\frac{4.2485}{20}} \\approx 0.66 + \\sqrt{0.2124} \\approx 0.66 + 0.4609 = 1.1209$\n$UCB_3 \\approx 0.58 + \\sqrt{\\frac{4.2485}{10}} \\approx 0.58 + \\sqrt{0.4248} \\approx 0.58 + 0.6518 = 1.2318$\n\nComparing the UCB values, we find:\n$ 1.2318 > 1.1209 > 0.9459 \\implies UCB_3 > UCB_2 > UCB_1 $\n\nThe UCB principle dictates selecting the action with the highest UCB value, which is action 3. Action 3 has the lowest empirical mean ($\\hat{\\mu}_3 = 0.58$) but also the fewest observations ($n_3 = 10$), leading to the largest exploration bonus and thus the highest UCB. This reflects the UCB algorithm's strategy of exploring less-sampled arms that have high potential.\n\nThe selected action is 3.", "answer": "$$\\boxed{3}$$", "id": "4855011"}, {"introduction": "Maximizing patient improvement is a primary goal, but medical decision-making must also operate within critical constraints, such as limiting side effects, managing costs, or avoiding risky procedures. This final practice introduces the Constrained Markov Decision Process (CMDP) framework, which extends the standard MDP to formally incorporate such safety or resource limits. You will determine an optimal treatment policy that maximizes the clinical reward while provably adhering to a specified cost constraint, a crucial step towards developing responsible and deployable AI in healthcare. [@problem_id:4855060]", "problem": "A hospital is designing a treatment policy using Reinforcement Learning (RL) for a patient with a chronic condition whose health status is stable across days. Model this as a Constrained Markov Decision Process (CMDP), defined as follows: a CMDP is a Markov Decision Process (MDP) with an additional cost function and a constraint on the expected discounted cumulative cost. An MDP consists of a set of states $S$, a set of actions $A$, transition probabilities $P$, a reward function $r(s,a)$, and a discount factor $\\gamma$; in a CMDP, there is also a cost function $d(s,a)$ and an allowed bound on the expected discounted cost.\n\nAssume a single state $s \\in S$ representing the stable health status and two actions $A = \\{a_{H}, a_{L}\\}$ representing high-intensity and low-intensity treatments, respectively. The per-decision reward and cost are defined by $r(s,a_{H}) = 5$, $r(s,a_{L}) = 2$, $d(s,a_{H}) = 3$, and $d(s,a_{L}) = 1$. Transitions are deterministic and the state remains $s$ after any action, so for all $t \\in \\mathbb{N}$ the state at time $t$ is $s$ with probability $1$. The discount factor is $\\gamma = 0.9$. The CMDP constraint requires that the expected discounted cumulative cost over the infinite horizon is at most $D_{\\max} = 12$.\n\nA stationary randomized policy $\\pi$ selects $a_{H}$ with probability $\\pi(a_{H} \\mid s) = x$ and $a_{L}$ with probability $\\pi(a_{L} \\mid s) = 1 - x$, where $x \\in [0,1]$ is to be determined. The objective is to choose $\\pi$ to maximize the expected discounted cumulative reward subject to the CMDP cost constraint.\n\nStarting from the definitions of expected discounted return in MDPs and the geometric series for bounded rewards and costs, derive the feasible set for $x$ and compute the constrained optimal policy $\\pi^{*}$ that maximizes the expected discounted cumulative reward while satisfying the cost constraint. Express your final policy as a row matrix in the order $\\left(\\pi^{*}(a_{H} \\mid s), \\pi^{*}(a_{L} \\mid s)\\right)$, using exact values. No rounding is required, and no units are needed for probabilities.", "solution": "The problem is to find an optimal stationary randomized policy $\\pi^*$ for a Constrained Markov Decision Process (CMDP) with a single state. The policy is defined by the probability $x \\in [0,1]$ of choosing the high-intensity action $a_H$. The objective is to maximize the expected discounted cumulative reward, subject to a constraint on the expected discounted cumulative cost.\n\nThe given parameters are:\n- State space: $S = \\{s\\}$\n- Action space: $A = \\{a_{H}, a_{L}\\}$\n- Rewards: $r(s,a_{H}) = 5$, $r(s,a_{L}) = 2$\n- Costs: $d(s,a_{H}) = 3$, $d(s,a_{L}) = 1$\n- Discount factor: $\\gamma = 0.9$\n- Cost constraint bound: $D_{\\max} = 12$\n- Policy: $\\pi(a_{H} \\mid s) = x$, $\\pi(a_{L} \\mid s) = 1 - x$\n\nSince the state is always $s$, the expected immediate reward under policy $\\pi$ is constant for each time step. This expected reward, denoted $E_{\\pi}[r]$, is calculated as:\n$$\nE_{\\pi}[r] = \\pi(a_{H} \\mid s) \\cdot r(s,a_{H}) + \\pi(a_{L} \\mid s) \\cdot r(s,a_{L})\n$$\nSubstituting the given values:\n$$\nE_{\\pi}[r] = x \\cdot 5 + (1-x) \\cdot 2 = 5x + 2 - 2x = 3x + 2\n$$\nThe total expected discounted cumulative reward, or value function $V^{\\pi}(s)$, is the sum of discounted expected rewards over an infinite horizon:\n$$\nV^{\\pi}(s) = \\sum_{t=0}^{\\infty} \\gamma^t E_{\\pi}[r] = (3x + 2) \\sum_{t=0}^{\\infty} \\gamma^t\n$$\nThe sum is a geometric series $\\sum_{t=0}^{\\infty} \\gamma^t = \\frac{1}{1-\\gamma}$ for $|\\gamma| < 1$. Given $\\gamma = 0.9$, the sum is $\\frac{1}{1-0.9} = \\frac{1}{0.1} = 10$.\nThus, the value function is:\n$$\nV^{\\pi}(s) = (3x + 2) \\cdot 10 = 30x + 20\n$$\nSimilarly, we calculate the expected immediate cost under policy $\\pi$, denoted $E_{\\pi}[d]$:\n$$\nE_{\\pi}[d] = \\pi(a_{H} \\mid s) \\cdot d(s,a_{H}) + \\pi(a_{L} \\mid s) \\cdot d(s,a_{L})\n$$\nSubstituting the given values:\n$$\nE_{\\pi}[d] = x \\cdot 3 + (1-x) \\cdot 1 = 3x + 1 - x = 2x + 1\n$$\nThe total expected discounted cumulative cost, $D^{\\pi}(s)$, is the sum of discounted expected costs over the infinite horizon:\n$$\nD^{\\pi}(s) = \\sum_{t=0}^{\\infty} \\gamma^t E_{\\pi}[d] = (2x + 1) \\sum_{t=0}^{\\infty} \\gamma^t = (2x + 1) \\frac{1}{1-\\gamma}\n$$\nUsing $\\frac{1}{1-\\gamma} = 10$, the cost function is:\n$$\nD^{\\pi}(s) = (2x + 1) \\cdot 10 = 20x + 10\n$$\nThe problem includes a constraint on the total expected discounted cost: $D^{\\pi}(s) \\le D_{\\max}$.\nSubstituting the expression for $D^{\\pi}(s)$ and the value of $D_{\\max}$:\n$$\n20x + 10 \\le 12\n$$\nWe solve this inequality for $x$:\n$$\n20x \\le 12 - 10\n$$\n$$\n20x \\le 2\n$$\n$$\nx \\le \\frac{2}{20} = \\frac{1}{10}\n$$\nThe variable $x$ represents a probability, so it must also satisfy $0 \\le x \\le 1$. Combining this with the constraint from the cost function, the feasible set for $x$ is:\n$$\nx \\in [0, 1] \\cap (-\\infty, \\frac{1}{10}] = [0, \\frac{1}{10}]\n$$\nThe objective is to maximize the value function $V^{\\pi}(s) = 30x + 20$ subject to $x$ being in the feasible set $[0, \\frac{1}{10}]$.\nThe objective function $V^{\\pi}(s)$ is a linear function of $x$ with a positive slope of $30$. A linear function with a positive slope is monotonically increasing. Therefore, its maximum value on a closed interval occurs at the rightmost point of the interval.\nThe maximum value for $x$ in the feasible set $[0, \\frac{1}{10}]$ is $x = \\frac{1}{10}$.\nThis optimal value of $x$ is denoted $x^*$.\n$$\nx^* = \\frac{1}{10}\n$$\nThe optimal policy $\\pi^*$ is determined by this value of $x^*$:\n$$\n\\pi^*(a_{H} \\mid s) = x^* = \\frac{1}{10}\n$$\n$$\n\\pi^*(a_{L} \\mid s) = 1 - x^* = 1 - \\frac{1}{10} = \\frac{9}{10}\n$$\nThe problem requests the final policy as a row matrix in the order $(\\pi^*(a_{H} \\mid s), \\pi^*(a_{L} \\mid s))$.\nThe optimal policy is therefore $\\left( \\frac{1}{10}, \\frac{9}{10} \\right)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{10} & \\frac{9}{10}\n\\end{pmatrix}\n}\n$$", "id": "4855060"}]}