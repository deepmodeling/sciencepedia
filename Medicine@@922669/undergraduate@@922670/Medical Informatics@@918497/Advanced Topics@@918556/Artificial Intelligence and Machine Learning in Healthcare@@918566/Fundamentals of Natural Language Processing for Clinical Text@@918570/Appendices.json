{"hands_on_practices": [{"introduction": "In any Natural Language Processing (NLP) pipeline, a common first step is text normalization—operations like lowercasing and removing punctuation to reduce textual variability. However, the clinical domain demands extreme precision, as a small change can drastically alter meaning. This exercise challenges you to move beyond generic NLP recipes and critically assess which standard normalization techniques are 'unsafe' when applied to sensitive clinical data, where preserving meaning is a matter of patient safety [@problem_id:4841496].", "problem": "A clinical Natural Language Processing (NLP) pipeline is being designed to preprocess free-text notes before downstream extraction of medication names and physiological units. Let a normalization function be any deterministic mapping $f:\\Sigma^* \\rightarrow \\Sigma^*$ from strings over an alphabet $\\Sigma$ to strings over the same alphabet that is intended to reduce superficial variability while preserving the underlying clinical semantics. A normalization operation is unsafe for clinical units and medication names if applying $f$ indiscriminately can transform a string into a different one that plausibly denotes a different entity, dose, or measurement than originally intended, or makes a true entity indistinguishable from a different concept. Consider the following normalization operations, formalized as deterministic functions:\n- Lowercasing: $f_{\\text{lower}}$ maps uppercase letters to their lowercase forms.\n- Punctuation stripping: $f_{\\text{punct}}$ removes all characters belonging to a designated punctuation set (for example, hyphens, slashes, periods).\n- Unicode normalization: $f_{\\text{unicode}}$ maps strings to a Unicode normalization form such as Normalization Form Canonical Composition (NFC) or Normalization Form Compatibility Composition (NFKC), using the Unicode Standard’s equivalence classes to ensure canonically equivalent strings are identical.\n- De-duplication: $f_{\\text{dedup}}$ collapses any maximal run of consecutive identical characters to a single instance of that character.\n\nThe pipeline must preserve critical distinctions needed to correctly identify entities such as International Unit (IU), milligram (mg), millimeters of mercury (mmHg), and brand or formulation cues (for example, Extended Release (ER) suffixes). Which of the following statements correctly identify normalization operations that are unsafe when applied indiscriminately to clinical units and medication names? Select all that apply.\n\nA. Lowercasing is unsafe because it collapses case-sensitive distinctions such as the analyte “Mg” (magnesium) into the unit “mg” (milligram), and may also blur formulation cues like “ER” in “metoprolol ER” by making them indistinguishable from unrelated lowercase tokens.\n\nB. Unconditional punctuation stripping is unsafe because it removes hyphens, slashes, and decimal points that encode meaning, as in “metoprolol-ER” (formulation), “NovoLog Mix $70/30$” (ratio), and “$0.5$ mg” (dose), thereby changing or obscuring the intended medication or unit.\n\nC. Unicode normalization to NFC or NFKC is universally safe for clinical units and medication names; it cannot change semantics because it only merges canonically or compatibly equivalent code points.\n\nD. Character de-duplication is unsafe because it alters units and abbreviations that rely on repeated letters, such as “mmHg” becoming “mHg” and “SSRI” becoming “SRI,” thereby changing or obscuring the intended unit or class.\n\nE. Unicode normalization that performs compatibility decomposition followed by aggressive American Standard Code for Information Interchange (ASCII) folding is unsafe because symbols such as the micro sign “µ” or Greek small letter mu “μ” in “μg” may be flattened to “ug,” increasing ambiguity and risking misinterpretation of microgram doses.\n\nChoose all that apply.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A normalization function is a deterministic mapping $f:\\Sigma^* \\rightarrow \\Sigma^*$.\n- An operation $f$ is defined as \"unsafe\" if its indiscriminate application can transform a string into one denoting a different entity, dose, or measurement, or make a true entity indistinguishable from a different concept.\n- The context is a clinical Natural Language Processing (NLP) pipeline for preprocessing free-text notes.\n- The pipeline must preserve distinctions for entities like International Unit (IU), milligram (mg), millimeters of mercury (mmHg), and medication formulation cues like Extended Release (ER).\n- The normalization operations to be evaluated are:\n    - Lowercasing: $f_{\\text{lower}}$\n    - Punctuation stripping: $f_{\\text{punct}}$\n    - Unicode normalization: $f_{\\text{unicode}}$ (to NFC or NFKC)\n    - De-duplication: $f_{\\text{dedup}}$ (collapses consecutive identical characters)\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the established fields of natural language processing and medical informatics. The described operations ($f_{\\text{lower}}$, $f_{\\text{punct}}$, etc.) are standard text preprocessing techniques. The clinical examples (Mg vs. mg, mmHg, medication names) are factually correct and represent real-world challenges in clinical text processing. The premise is scientifically sound.\n- **Well-Posed**: The problem provides clear definitions for a \"normalization function\" and an \"unsafe\" operation. It asks for an evaluation of given statements against these definitions. The task is specific and allows for a definite set of correct answers.\n- **Objective**: The problem is stated in precise, formal, and objective language. The functions are described deterministically. The examples are factual and not subject to opinion.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and contains sufficient information to proceed with a solution.\n\n### Analysis of Options\n\nThe task is to identify which statements correctly describe a normalization operation as unsafe according to the provided definition.\n\n**A. Lowercasing: $f_{\\text{lower}}$**\nThis statement claims that lowercasing is unsafe. It provides two examples:\n1. The symbol for the element magnesium is \"Mg\", while the symbol for the unit milligram is \"mg\". Applying the lowercasing function yields $f_{\\text{lower}}(\\text{\"Mg\"}) = \\text{\"mg\"}$. This makes the elemental analyte indistinguishable from the unit of mass, which is a critical semantic distinction in a clinical context (e.g., a lab result for serum magnesium versus a medication dosage).\n2. For a medication like \"metoprolol ER\", where \"ER\" signifies \"Extended Release\", $f_{\\text{lower}}(\\text{\"metoprolol ER\"}) = \\text{\"metoprolol er\"}$. While \"er\" might still be interpretable, the standardized, capitalized form \"ER\" is a stronger, less ambiguous signal for the drug formulation. The loss of case can increase ambiguity.\nThe first example, \"Mg\" vs. \"mg\", is a definitive case where applying $f_{\\text{lower}}$ makes a \"true entity indistinguishable from a different concept,\" perfectly matching the definition of an unsafe operation.\n*Verdict on A*: **Correct**.\n\n**B. Unconditional punctuation stripping: $f_{\\text{punct}}$**\nThis statement claims that unconditional punctuation stripping is unsafe. It provides several examples of meaningful punctuation:\n1. In \"metoprolol-ER\", the hyphen connects the drug name to its formulation. $f_{\\text{punct}}(\\text{\"metoprolol-ER\"}) = \\text{\"metoprololER\"}$. This may complicate tokenization and pattern matching.\n2. In \"NovoLog Mix $70/30$\", the slash separates the components of an insulin mixture ratio. $f_{\\text{punct}}(\\text{\"NovoLog Mix 70/30\"}) = \\text{\"NovoLog Mix 7030\"}$. This transforms a ratio into a four-digit number, completely obscuring the original meaning.\n3. In \"$0.5$ mg\", the period is a decimal point indicating a dose. $f_{\\text{punct}}(\\text{\"0.5 mg\"}) = \\text{\"05 mg\"}$. This changes the dose from one-half of a milligram to five milligrams, a 10-fold increase, which could have catastrophic clinical consequences.\nThese examples, particularly the last two, demonstrate that $f_{\\text{punct}}$ can transform a string into one that \"denotes a different... dose, or measurement than originally intended.\"\n*Verdict on B*: **Correct**.\n\n**C. Unicode normalization: $f_{\\text{unicode}}$**\nThis statement claims that Unicode normalization to NFC or NFKC is \"universally safe\". To be universally safe, it must not cause any loss of semantic distinction in any plausible clinical scenario.\n- NFC (Normalization Form Canonical Composition) combines characters and their diacritics into precomposed forms (e.g., `e` + `´` → `é`). This is generally considered semantically lossless, as it deals with canonical equivalences.\n- NFKC (Normalization Form Compatibility Composition) is more aggressive. It first applies compatibility decompositions. For instance, a ligature like `ﬁ` (U+FB01) would be decomposed to `f` and `i`. A superscript like `²` (U+00B2) would be mapped to the digit `2` (U+0032). The Roman numeral `Ⅳ` (U+2163) may be mapped to `IV`. These transformations do not preserve all semantic nuances. A character and its compatibility equivalent are, by definition of the Unicode standard, not necessarily interchangeable in all contexts. For example, changing a superscript in a chemical formula or a mathematical expression could alter its meaning.\nThe claim of \"universal safety\" for both NFC and NFKC is an exceptionally strong claim. Since NFKC is explicitly designed to break some semantic distinctions for the sake of compatibility (e.g., for searching or collation), it cannot be considered universally safe. The potential for NFKC to alter meaning makes this statement false.\n*Verdict on C*: **Incorrect**.\n\n**D. Character de-duplication: $f_{\\text{dedup}}$**\nThis statement claims that de-duplicating consecutive characters is unsafe. It provides two examples:\n1. The unit for blood pressure is \"mmHg\" (millimeters of mercury). The function $f_{\\text{dedup}}$ would transform this: $f_{\\text{dedup}}(\\text{\"mmHg\"}) = \\text{\"mHg\"}$. The resulting string \"mHg\" is not a standard representation and obscures the intended unit, which relies on the double \"m\".\n2. The acronym \"SSRI\" stands for Selective Serotonin Reuptake Inhibitor. The function $f_{\\text{dedup}}$ would transform this: $f_{\\text{dedup}}(\\text{\"SSRI\"}) = \\text{\"SRI\"}$. While \"SRI\" (Serotonin Reuptake Inhibitor) is a related concept, the transformation loses specificity.\nThe \"mmHg\" example is a clear case where a standard unit representation is corrupted, matching the definition of an unsafe operation.\n*Verdict on D*: **Correct**.\n\n**E. Unicode normalization with ASCII folding**\nThis statement claims that a process of compatibility decomposition followed by ASCII folding is unsafe.\nThe process is described as mapping symbols like the micro sign \"µ\" (U+00B5) or Greek small letter mu \"μ\" (U+03BC) to \"u\", resulting in \"μg\" (microgram) becoming \"ug\".\nThe abbreviation \"ug\" for microgram is on the \"List of Error-Prone Abbreviations, Symbols, and Dose Designations\" by the Institute for Safe Medication Practices (ISMP) precisely because it is easily mistaken for \"mg\" (milligram), which can lead to a 1000-fold dosing error. The recommended, unambiguous abbreviation is \"mcg\".\nAn NLP operation that systematically converts a correct representation (\"μg\") into a known dangerous one (\"ug\") \"increase[s] ambiguity and risk[s] misinterpretation,\" making it unsafe by the problem's definition.\n*Verdict on E*: **Correct**.", "answer": "$$\\boxed{ABDE}$$", "id": "4841496"}, {"introduction": "Extracting clinical concepts is not enough; we must also determine their factuality—is a diagnosis confirmed, denied, or uncertain? This practice explores negation detection, a crucial task that involves identifying negation cues (like 'no' or 'denies') and their scope. You will analyze why simple keyword-based or window-based methods often fail in the face of complex grammatical structures, underscoring the necessity of using syntactic information to correctly interpret clinical statements [@problem_id:4841498].", "problem": "A clinical negation detection system is designed to determine the factuality of clinical concept mentions in patient text. Let a sentence be tokenized into a sequence $t_1, t_2, \\dots, t_n$ with $n \\in \\mathbb{N}$. A negation cue at position $i$ is a lexical item (for example, a determiner such as \"no\" or a verb such as \"denies\") that may invert the factuality of some part of the sentence. The scope of negation is defined as a set $S \\subseteq \\{1,2,\\dots,n\\}$ such that for each $j \\in S$, the clinical concept realized at token $t_j$ has its default affirmative factuality inverted due to the compositional semantics of the cue. The default affirmative factuality is the truth-conditional interpretation that the clinical concept is present unless explicitly modified by negation or uncertainty operators. Consider the following sentence and assumptions:\n\nSentence (with token indices):\n$t_1=$\"The\", $t_2=$\"patient\", $t_3=$\"denies\", $t_4=$\"telling\", $t_5=$\"the\", $t_6=$\"nurse\", $t_7=$\"that\", $t_8=$\"he\", $t_9=$\"has\", $t_{10}=$\"chest\", $t_{11}=$\"pain\", $t_{12}=$\"but\", $t_{13}=$\"shows\", $t_{14}=$\"no\", $t_{15}=$\"evidence\", $t_{16}=$\"of\", $t_{17}=$\"pneumonia\".\n\nClinical concepts of interest: \"chest pain\" whose head is $t_{11}$ and \"pneumonia\" at $t_{17}$.\n\nNegation cues: \"denies\" at $t_3$ (a verb taking clausal or nominal complements) and \"no\" at $t_{14}$ (a determiner modifying a noun).\n\nSimplified dependency structure (heads point to dependents):\n- $t_3$ (\"denies\") is the main predicate with $t_2$ (\"patient\") as its subject; $t_4$ (\"telling\") is an open clausal complement (xcomp) of $t_3$.\n- $t_4$ (\"telling\") takes a clausal complement (ccomp) headed by $t_9$ (\"has\"), marked by $t_7$ (\"that\").\n- $t_9$ (\"has\") takes a direct object headed by $t_{11}$ (\"pain\") with a compound modifier $t_{10}$ (\"chest\").\n- $t_{15}$ (\"evidence\") is a noun modified by the determiner $t_{14}$ (\"no\") and takes a prepositional complement \"of $t_{17}$\" (\"pneumonia\").\n\nTwo algorithms are contrasted:\n1. A compositional scope algorithm that uses the syntactic structure to compute $S$ as the minimal complement domain governed by the negation cue’s head, restricted to clinical concept mentions; it respects the semantic type of the head (for example, \"denies\" is a speech-act predicate whose negation inverts the truth of the speech act, not necessarily the subordinate clinical proposition; \"no\" deterministically negates the NP it modifies, which in clinical narrations conventionally signals the absence of the finding in the complement \"of $X$\").\n2. A linear window algorithm that, for a cue at position $i$, marks all tokens in the window $W_k(i) = \\{i+1, i+2, \\dots, i+k\\}$ as within negation scope, where $k$ is a fixed integer. For the sentence above, suppose $k_{\\text{denies}} = 8$ for \"denies\" and $k_{\\text{no}} = 3$ for \"no\".\n\nQuestion: Using the core definitions above, choose the option that correctly defines the scope of negation for clinical statements and correctly identifies which clinical concepts are negated in the example. The option must also correctly explain why the window-based approach fails in the presence of nested clauses in this sentence.\n\nA. The scope of negation is the minimal syntactic complement of each negation cue constrained by the semantic type of its head: for \"denies\" ($t_3$), which is a speech-act predicate, the scope includes only direct nominal complements that are clinical concepts (for example, \"denies chest pain\"); it does not propagate through an intervening communication predicate (\"telling\") into the subordinate proposition \"he has chest pain\". For \"no\" ($t_{14}$), the scope is the noun phrase headed by \"evidence\" with its \"of $X$\" complement, which conventionally negates \"$X$\" in clinical narratives. Therefore \"pneumonia\" ($t_{17}$) is negated, while \"chest pain\" ($t_{11}$) is not negated by \"denies\" in this sentence. The window method with $W_{8}(3) = \\{4,5,6,7,8,9,10,11\\}$ wrongly marks \"chest pain\" as negated because it ignores the nested clausal boundary introduced by \"telling ... that\", illustrating failure with nested clauses.\n\nB. The scope of negation is all tokens within a fixed window after each cue; hence \"chest pain\" and \"pneumonia\" are both negated, and nested clauses do not affect scope because the window uniformly covers them.\n\nC. The scope of negation is the remainder of the sentence after a cue until a coordinating conjunction; thus \"denies\" negates \"chest pain\" until \"but\", and \"no\" negates \"pneumonia\"; nested clauses are handled implicitly by stopping at \"but\", so both \"chest pain\" and \"pneumonia\" are negated.\n\nD. The scope of negation propagates through all clausal complements of a cue; \"denies\" negates the entire proposition \"he has chest pain\", and \"no\" negates \"pneumonia\", so both concepts are negated; nested clauses are correctly handled because negation always distributes through embeddings.\n\nChoose the correct option. Justify your choice by deriving the scope from the definitions and the given structures, and by analyzing the failure (or success) of the window-based approach on the provided sentence.", "solution": "The problem requires an analysis of negation scope using two algorithms: a compositional, syntax-aware algorithm and a simple linear window algorithm. The goal is to determine the factuality of \"chest pain\" and \"pneumonia\".\n\n### Derivation of Solution\n\n**1. Analysis of \"pneumonia\"**\nThe relevant part of the sentence is `...shows no evidence of pneumonia`.\n- The negation cue is \"no\" at token index $t_{14}$.\n- The clinical concept is \"pneumonia\" at token index $t_{17}$.\n\n*   **Compositional Algorithm (Algorithm 1):** The rule for \"no\" states that it \"deterministically negates the NP it modifies, which in clinical narrations conventionally signals the absence of the finding in the complement 'of $X$'\". In \"no evidence of pneumonia\", the complement \"$X$\" is \"pneumonia\". Therefore, \"pneumonia\" is **negated**.\n*   **Linear Window Algorithm (Algorithm 2):** The cue is at index $i = 14$ with a window size of $k_{\\text{no}} = 3$. The scope is the set of token indices $W_3(14) = \\{15, 16, 17\\}$. Since the token for \"pneumonia\" ($t_{17}$) is in this window, this algorithm also marks \"pneumonia\" as **negated**.\n\nIn this case, both algorithms correctly conclude that \"pneumonia\" is negated.\n\n**2. Analysis of \"chest pain\"**\nThe relevant part of the sentence is `The patient denies telling the nurse that he has chest pain...`.\n- The negation cue is \"denies\" at token index $t_3$.\n- The clinical concept is \"chest pain\" with its head at token index $t_{11}$.\n\n*   **Compositional Algorithm (Algorithm 1):** The rule for \"denies\" is crucial: it is a \"speech-act predicate whose negation inverts the truth of the speech act, not necessarily the subordinate clinical proposition\". The structure is `patient denies [telling the nurse [that he has chest pain]]`. The patient is denying the act of \"telling\". The sentence does not assert that the patient does not have chest pain; it only reports the patient's denial of a prior statement. Therefore, according to the provided semantic rules, \"chest pain\" is **not negated**.\n*   **Linear Window Algorithm (Algorithm 2):** The cue is at index $i=3$ with a window size of $k_{\\text{denies}} = 8$. The scope is the set of token indices $W_8(3) = \\{4, 5, 6, 7, 8, 9, 10, 11\\}$. The head token for \"chest pain\" ($t_{11}$) falls within this window. Therefore, the algorithm incorrectly marks \"chest pain\" as **negated**.\n\n**3. Failure Analysis of the Window-Based Approach**\nThe linear window algorithm fails for \"chest pain\" because it is a simple linear heuristic that is ignorant of syntax and semantics. It cannot recognize the nested clausal structure `denies [telling [... that ...]]`. It simply counts a fixed number of tokens, wrongly crossing the syntactic boundary of the subordinate clause and associating the negation cue \"denies\" with the deeply embedded concept \"chest pain\".\n\n### Option-by-Option Analysis\n\n*   **A**: This option correctly states that the compositional algorithm negates \"pneumonia\" but does not negate \"chest pain\" due to the specific semantics of the speech-act predicate \"denies\". It also correctly identifies that the window method fails by wrongly marking \"chest pain\" as negated because it ignores the clausal boundary. This matches our derivation. **This is the correct option.**\n*   **B**: This option incorrectly presents the flawed window-based method as the correct definition of scope and incorrectly concludes that \"chest pain\" is negated.\n*   **C**: This option introduces a third, undefined algorithm (stopping at a conjunction) and incorrectly concludes that \"chest pain\" is negated.\n*   **D**: This option makes a false linguistic generalization that \"negation always distributes through embeddings,\" which contradicts the provided definition for \"denies,\" and incorrectly concludes that \"chest pain\" is negated.", "answer": "$$\\boxed{A}$$", "id": "4841498"}, {"introduction": "After building a model to extract structured information, such as identifying clinical problems via Named Entity Recognition (NER), how do we know if it is actually working well? This final practice focuses on the critical skill of evaluation. You will calculate and contrast two different types of performance metrics—token-level and span-level scores—to understand why a high 'accuracy' can sometimes be misleading and why choosing the right evaluation metric is essential for building reliable clinical NLP systems [@problem_id:4841429].", "problem": "A clinical named entity recognition system is evaluated on a single sentence tokenized into $10$ tokens: $\\text{Patient}$ $(1)$, $\\text{has}$ $(2)$, $\\text{chest}$ $(3)$, $\\text{pain}$ $(4)$, $\\text{and}$ $(5)$, $\\text{shortness}$ $(6)$, $\\text{of}$ $(7)$, $\\text{breath}$ $(8)$, $\\text{denies}$ $(9)$, $\\text{fever}$ $(10)$. The gold labels use the Begin–Inside–Outside (BIO) scheme for the clinical problem entity type, where $\\text{B-P}$ denotes the beginning of a problem span, $\\text{I-P}$ denotes the inside of a problem span, and $\\text{O}$ denotes a token outside any problem span. The gold BIO tags for the tokens are:\n$(1)\\ \\text{O},\\ (2)\\ \\text{O},\\ (3)\\ \\text{B-P},\\ (4)\\ \\text{I-P},\\ (5)\\ \\text{O},\\ (6)\\ \\text{B-P},\\ (7)\\ \\text{I-P},\\ (8)\\ \\text{I-P},\\ (9)\\ \\text{O},\\ (10)\\ \\text{B-P}$.\nThe system’s predicted BIO tags are:\n$(1)\\ \\text{B-P},\\ (2)\\ \\text{I-P},\\ (3)\\ \\text{B-P},\\ (4)\\ \\text{I-P},\\ (5)\\ \\text{O},\\ (6)\\ \\text{B-P},\\ (7)\\ \\text{I-P},\\ (8)\\ \\text{O},\\ (9)\\ \\text{O},\\ (10)\\ \\text{B-P}$.\n\nAssume the following evaluation conventions grounded in standard information retrieval definitions:\n- Token-level evaluation treats BIO tag assignment as a single-label multi-class decision per token, and the token-level $F_{1}$-score is the micro-averaged harmonic mean of token-level precision and token-level recall across all BIO classes over all $10$ tokens.\n- Span-level evaluation treats each contiguous problem span as a unit, and a predicted span is counted as correct only if its boundaries and entity type exactly match a gold span. The span-level $F_{1}$-score is computed from span-level precision and span-level recall over all spans in this sentence.\n\nIt is known that $7$ tokens have their predicted BIO tag exactly matching the gold tag, while only $2$ predicted spans exactly match gold spans. Using only the definitions above as the fundamental base, compute:\n- the token-level micro-averaged $F_{1}$-score over the $10$ tokens, and\n- the span-level exact-match $F_{1}$-score over predicted and gold spans.\n\nRound both $F_{1}$-scores to four significant figures. Express the final answers as decimals without any units.", "solution": "The problem is evaluated and found to be valid. It is scientifically grounded in the principles of natural language processing evaluation, is well-posed with sufficient and consistent data, and is expressed in objective language. The provided summary statistics (\"$7$ tokens have their predicted BIO tag exactly matching the gold tag\" and \"$2$ predicted spans exactly match gold spans\") are consistent with the raw data, confirming the self-consistency of the problem statement. We may therefore proceed with the solution.\n\nThe problem asks for the computation of two distinct metrics: a token-level micro-averaged $F_{1}$-score and a span-level exact-match $F_{1}$-score. We will address each in turn.\n\n### Part 1: Token-Level Micro-Averaged $F_{1}$-Score\n\nThe evaluation is at the token level, where each of the $N=10$ tokens is assigned a single BIO tag from the set $\\{\\text{B-P, I-P, O}\\}$. This is a single-label multi-class classification task. The token-level $F_{1}$-score is defined as the micro-averaged harmonic mean of precision and recall.\n\nIn micro-averaging, we sum the true positives ($TP$), false positives ($FP$), and false negatives ($FN$) for each class before calculating the metrics. For a single-label classification task over a fixed number of items (tokens), the micro-averaged precision, recall, and $F_{1}$-score are all equal to the overall accuracy.\n\nAccuracy is the ratio of correctly classified items to the total number of items. The problem statement explicitly provides this information: \"$7$ tokens have their predicted BIO tag exactly matching the gold tag\" out of a total of $10$ tokens.\nLet $N_{correct} = 7$ be the number of correctly classified tokens and $N_{total} = 10$ be the total number of tokens.\n\nThe overall accuracy is $\\frac{N_{correct}}{N_{total}} = \\frac{7}{10} = 0.7$.\nTherefore, micro-averaged precision ($P_{micro}$), recall ($R_{micro}$), and $F_{1}$-score ($F_{1,token}$) are all equal to $0.7$.\n$$ F_{1,token} = 0.7 $$\nRounding to four significant figures as required, we get $0.7000$.\n\n### Part 2: Span-Level Exact-Match $F_{1}$-Score\n\nFor the span-level evaluation, we first identify the contiguous problem spans from both the gold and predicted BIO tags. A span is a sequence of tokens starting with a `B-P` tag, followed by zero or more `I-P` tags.\n\n**Gold Spans (Ground Truth):**\nThe gold tags are: O, O, B-P, I-P, O, B-P, I-P, I-P, O, B-P.\n- `chest pain`: Tokens $3$ (`B-P`) and $4$ (`I-P`).\n- `shortness of breath`: Tokens $6$ (`B-P`), $7$ (`I-P`), and $8$ (`I-P`).\n- `fever`: Token $10$ (`B-P`).\nSo, there are $3$ gold spans. Let $N_{gold} = 3$.\n\n**Predicted Spans (System Output):**\nThe predicted tags are: B-P, I-P, B-P, I-P, O, B-P, I-P, O, O, B-P.\n- `Patient has`: Tokens $1$ (`B-P`) and $2$ (`I-P`).\n- `chest pain`: Tokens $3$ (`B-P`) and $4$ (`I-P`).\n- `shortness of`: Tokens $6$ (`B-P`) and $7$ (`I-P`).\n- `fever`: Token $10$ (`B-P`).\nSo, there are $4$ predicted spans. Let $N_{predicted} = 4$.\n\nNow we calculate the number of true positives ($TP$), false positives ($FP$), and false negatives ($FN$) for the spans. A predicted span is a true positive if it matches a gold span exactly in terms of both boundaries and entity type.\n\n- **True Positives ($TP$)**: The number of predicted spans that exactly match a gold span.\n  - Predicted `chest pain` (tokens $3$-$4$) matches gold `chest pain` (tokens $3$-$4$). This is $1$ TP.\n  - Predicted `fever` (token $10$) matches gold `fever` (token $10$). This is a second TP.\n  - The other two predicted spans do not match any gold span exactly.\n  - Thus, $TP = 2$. This is consistent with the problem statement: \"$2$ predicted spans exactly match gold spans\".\n\n- **False Positives ($FP$)**: The number of predicted spans that are not true positives.\n  - $FP = N_{predicted} - TP = 4 - 2 = 2$.\n  - The false positive spans are `Patient has` (tokens $1$-$2$) and `shortness of` (tokens $6$-$7$).\n\n- **False Negatives ($FN$)**: The number of gold spans that are not matched by any predicted span.\n  - $FN = N_{gold} - TP = 3 - 2 = 1$.\n  - The false negative span is `shortness of breath` (tokens $6$-$8$). The system's prediction `shortness of` (tokens 6-7) is a partial match, but under the exact-match criterion, it does not count as a correct prediction, and the gold span is considered missed.\n\nWith these values, we can compute span-level precision ($P_{span}$) and recall ($R_{span}$).\n$$ P_{span} = \\frac{TP}{TP + FP} = \\frac{2}{2 + 2} = \\frac{2}{4} = 0.5 $$\n$$ R_{span} = \\frac{TP}{TP + FN} = \\frac{2}{2 + 1} = \\frac{2}{3} $$\n\nThe span-level $F_{1}$-score ($F_{1,span}$) is the harmonic mean of $P_{span}$ and $R_{span}$:\n$$ F_{1,span} = 2 \\times \\frac{P_{span} \\times R_{span}}{P_{span} + R_{span}} = 2 \\times \\frac{0.5 \\times \\frac{2}{3}}{0.5 + \\frac{2}{3}} $$\n$$ F_{1,span} = 2 \\times \\frac{\\frac{1}{3}}{\\frac{3}{6} + \\frac{4}{6}} = 2 \\times \\frac{\\frac{1}{3}}{\\frac{7}{6}} = 2 \\times \\frac{1}{3} \\times \\frac{6}{7} = \\frac{4}{7} $$\nTo express this as a decimal rounded to four significant figures:\n$$ F_{1,span} = \\frac{4}{7} \\approx 0.57142857... $$\nRounding gives $0.5714$.\n\nThe two computed values are:\n- Token-level micro-averaged $F_{1}$-score: $0.7000$\n- Span-level exact-match $F_{1}$-score: $0.5714$", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.7000 & 0.5714 \\end{pmatrix} } $$", "id": "4841429"}]}