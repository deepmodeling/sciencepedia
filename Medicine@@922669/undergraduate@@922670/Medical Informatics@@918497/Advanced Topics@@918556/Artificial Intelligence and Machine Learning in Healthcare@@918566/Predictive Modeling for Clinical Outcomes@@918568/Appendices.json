{"hands_on_practices": [{"introduction": "Before any predictive model is built, we must ensure the integrity of our data and study design. This exercise tackles a critical and often subtle pitfall in observational research known as immortal time bias. By comparing a naive cohort design with a corrected one, you will gain hands-on experience in identifying and eliminating this bias, a fundamental skill for producing valid and reliable clinical predictions [@problem_id:4853264].", "problem": "You are given a cohort assembled around a laboratory test, initially indexed at laboratory result time, which inadvertently induces immortal time bias. Redesign the cohort to index at laboratory order time and recompute the outcome risk windows, then compare the naive design (indexed at result time) to the corrected design (indexed at order time). The goal is to compute risks for a fixed-length outcome window and quantify the difference attributable to immortal time.\n\nBase from fundamental definitions in time-to-event analysis:\n1. For each individual indexed by $i \\in \\{1,\\dots,n\\}$, you observe laboratory order time $t_i^{\\mathrm{ord}} \\in \\mathbb{R}_{\\ge 0}$, processing delay $d_i \\in \\mathbb{R}_{\\ge 0}$, laboratory result time $t_i^{\\mathrm{res}} = t_i^{\\mathrm{ord}} + d_i$, outcome time $t_i^{\\mathrm{out}} \\in \\mathbb{R}_{\\ge 0} \\cup \\{+\\infty\\}$, and censoring time $t_i^{\\mathrm{cens}} \\in \\mathbb{R}_{\\ge 0} \\cup \\{+\\infty\\}$. All times are measured on a common axis in days, with $+\\infty$ denoting absence of the corresponding event within the observation horizon.\n2. Define an index function $t_i^{\\mathrm{idx},s}$ that depends on a schema $s \\in \\{\\mathrm{naive}, \\mathrm{corr}\\}$ by $t_i^{\\mathrm{idx},\\mathrm{naive}} := t_i^{\\mathrm{res}}$ and $t_i^{\\mathrm{idx},\\mathrm{corr}} := t_i^{\\mathrm{ord}}$.\n3. For a fixed window length $W \\in \\mathbb{R}_{ 0}$ (in days), define the at-risk inclusion rule for schema $s$ as: individual $i$ is included if and only if $t_i^{\\mathrm{cens}} \\ge t_i^{\\mathrm{idx},s}$ and $t_i^{\\mathrm{out}} \\ge t_i^{\\mathrm{idx},s}$. This rule encodes being event-free and uncensored at the index time.\n4. For included individuals, define the outcome indicator $y_i^{(s)}$ by $y_i^{(s)} = 1$ if $t_i^{\\mathrm{out}} \\in [\\,t_i^{\\mathrm{idx},s}, \\min\\{t_i^{\\mathrm{idx},s} + W, t_i^{\\mathrm{cens}}\\}\\,]$, and $y_i^{(s)} = 0$ otherwise. The interval endpoints are inclusive.\n5. Define the risk for schema $s$ as $R^{(s)} = \\left(\\sum_{i \\in \\mathcal{I}_s} y_i^{(s)}\\right) / \\left|\\mathcal{I}_s\\right|$, where $\\mathcal{I}_s$ is the set of included individuals under schema $s$. The quantity of interest for each dataset is $(R^{(\\mathrm{naive})}, R^{(\\mathrm{corr})}, R^{(\\mathrm{corr})} - R^{(\\mathrm{naive})})$.\n\nAll time values are in days; output risks must be reported as decimals (not percentages), rounded to six digits after the decimal point.\n\nImplement a program that, for each test case below, computes the triple $(R^{(\\mathrm{naive})}, R^{(\\mathrm{corr})}, R^{(\\mathrm{corr})} - R^{(\\mathrm{naive})})$ using the definitions above and outputs all results in a single line as a comma-separated list enclosed in square brackets.\n\nTest suite (all times in days; use $+\\infty$ where stated):\n- Test case $1$ with window $W = 7$ and individuals:\n  - $i_1$: $t^{\\mathrm{ord}} = 0$, $d = 2$ so $t^{\\mathrm{res}} = 2$, $t^{\\mathrm{out}} = 5$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_2$: $t^{\\mathrm{ord}} = 1$, $d = 3$ so $t^{\\mathrm{res}} = 4$, $t^{\\mathrm{out}} = 3$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_3$: $t^{\\mathrm{ord}} = 4$, $d = 1$ so $t^{\\mathrm{res}} = 5$, $t^{\\mathrm{out}} = +\\infty$, $t^{\\mathrm{cens}} = 10$.\n  - $i_4$: $t^{\\mathrm{ord}} = 2$, $d = 2$ so $t^{\\mathrm{res}} = 4$, $t^{\\mathrm{out}} = 12$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_5$: $t^{\\mathrm{ord}} = 3$, $d = 7$ so $t^{\\mathrm{res}} = 10$, $t^{\\mathrm{out}} = 8$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_6$: $t^{\\mathrm{ord}} = 6$, $d = 2$ so $t^{\\mathrm{res}} = 8$, $t^{\\mathrm{out}} = +\\infty$, $t^{\\mathrm{cens}} = 7$.\n- Test case $2$ with window $W = 3$ and individuals:\n  - $i_1$: $t^{\\mathrm{ord}} = 0$, $d = 0$ so $t^{\\mathrm{res}} = 0$, $t^{\\mathrm{out}} = 0$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_2$: $t^{\\mathrm{ord}} = 5$, $d = 2$ so $t^{\\mathrm{res}} = 7$, $t^{\\mathrm{out}} = 8$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_3$: $t^{\\mathrm{ord}} = 1$, $d = 3$ so $t^{\\mathrm{res}} = 4$, $t^{\\mathrm{out}} = 2$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_4$: $t^{\\mathrm{ord}} = 3$, $d = 1$ so $t^{\\mathrm{res}} = 4$, $t^{\\mathrm{out}} = +\\infty$, $t^{\\mathrm{cens}} = 4$.\n  - $i_5$: $t^{\\mathrm{ord}} = 2$, $d = 5$ so $t^{\\mathrm{res}} = 7$, $t^{\\mathrm{out}} = 1$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_6$: $t^{\\mathrm{ord}} = 6$, $d = 0$ so $t^{\\mathrm{res}} = 6$, $t^{\\mathrm{out}} = +\\infty$, $t^{\\mathrm{cens}} = 6$.\n- Test case $3$ with window $W = 5$ and individuals:\n  - $i_1$: $t^{\\mathrm{ord}} = 0$, $d = 4$ so $t^{\\mathrm{res}} = 4$, $t^{\\mathrm{out}} = 3$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_2$: $t^{\\mathrm{ord}} = 2$, $d = 1$ so $t^{\\mathrm{res}} = 3$, $t^{\\mathrm{out}} = +\\infty$, $t^{\\mathrm{cens}} = 2$.\n  - $i_3$: $t^{\\mathrm{ord}} = 5$, $d = 10$ so $t^{\\mathrm{res}} = 15$, $t^{\\mathrm{out}} = +\\infty$, $t^{\\mathrm{cens}} = 7$.\n  - $i_4$: $t^{\\mathrm{ord}} = 8$, $d = 0$ so $t^{\\mathrm{res}} = 8$, $t^{\\mathrm{out}} = 12$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_5$: $t^{\\mathrm{ord}} = 1$, $d = 2$ so $t^{\\mathrm{res}} = 3$, $t^{\\mathrm{out}} = 20$, $t^{\\mathrm{cens}} = +\\infty$.\n  - $i_6$: $t^{\\mathrm{ord}} = 9$, $d = 1$ so $t^{\\mathrm{res}} = 10$, $t^{\\mathrm{out}} = +\\infty$, $t^{\\mathrm{cens}} = +\\infty$.\n\nYour program must:\n- Implement the definitions above exactly.\n- For each test case, compute $(R^{(\\mathrm{naive})}, R^{(\\mathrm{corr})}, R^{(\\mathrm{corr})} - R^{(\\mathrm{naive})})$.\n- Produce a single line of output containing all nine numbers for the three test cases in order, rounded to six digits after the decimal point, as a comma-separated list enclosed in square brackets. The format is:\n  $[r_{1,\\mathrm{naive}}, r_{1,\\mathrm{corr}}, r_{1,\\mathrm{diff}}, r_{2,\\mathrm{naive}}, r_{2,\\mathrm{corr}}, r_{2,\\mathrm{diff}}, r_{3,\\mathrm{naive}}, r_{3,\\mathrm{corr}}, r_{3,\\mathrm{diff}}]$ where $r_{k,\\mathrm{diff}} = r_{k,\\mathrm{corr}} - r_{k,\\mathrm{naive}}$ for test case $k \\in \\{1,2,3\\}$.", "solution": "The problem statement is formally well-defined, scientifically grounded in the principles of time-to-event analysis, and computationally tractable. It provides a clear, self-contained set of definitions and data, allowing for a unique and verifiable solution. The problem addresses the concept of immortal time bias, a critical issue in observational studies and clinical predictive modeling, where an inappropriate choice of index time (time zero) can lead to biased estimates of risk. The problem is valid.\n\nThe core task is to compare two cohort construction schemas for estimating the risk of an outcome within a fixed window of time $W$ following a laboratory test. The key difference lies in the definition of the index time, $t^{\\mathrm{idx}}$, from which all follow-up is measured.\n\n1.  **Naive Schema ($s = \\mathrm{naive}$)**: The index time is the laboratory result time, $t_i^{\\mathrm{idx},\\mathrm{naive}} = t_i^{\\mathrm{res}}$. This design is common but flawed. For an individual $i$ to be included in the cohort, they must be alive and uncensored at $t_i^{\\mathrm{res}}$. The time interval between ordering the test ($t_i^{\\mathrm{ord}}$) and receiving the result ($t_i^{\\mathrm{res}}$), which is the processing delay $d_i$, becomes \"immortal time.\" By definition, no adverse outcome can be observed during this period for individuals included in the cohort, as anyone who experiences the outcome or is censored during this time is excluded from the analysis. This systematically removes early events, leading to an underestimation of the true risk.\n\n2.  **Corrected Schema ($s = \\mathrm{corr}$)**: The index time is the laboratory order time, $t_i^{\\mathrm{idx},\\mathrm{corr}} = t_i^{\\mathrm{ord}}$. This is the conceptually correct approach. The moment the test is ordered is when the clinical decision-making process begins and the patient is officially \"at risk\" with respect to the information the test will provide. This design correctly allows for events and censoring to occur at any point after the order time, including during the processing delay, providing an unbiased estimate of risk.\n\nThe risk for each schema $s$, denoted $R^{(s)}$, is calculated as the ratio of the number of individuals who experience the outcome within the specified risk window to the total number of individuals included in the at-risk set for that schema. The calculation proceeds by applying the formal definitions provided. Let's denote $+\\infty$ by $\\infty$.\n\n### Test Case 1: $W=7$\n\n- Individual data:\n  - $i_1$: $t^{\\mathrm{ord}} = 0$, $d = 2$, $t^{\\mathrm{res}} = 2$, $t^{\\mathrm{out}} = 5$, $t^{\\mathrm{cens}} = \\infty$.\n  - $i_2$: $t^{\\mathrm{ord}} = 1$, $d = 3$, $t^{\\mathrm{res}} = 4$, $t^{\\mathrm{out}} = 3$, $t^{\\mathrm{cens}} = \\infty$.\n  - $i_3$: $t^{\\mathrm{ord}} = 4$, $d = 1$, $t^{\\mathrm{res}} = 5$, $t^{\\mathrm{out}} = \\infty$, $t^{\\mathrm{cens}} = 10$.\n  - $i_4$: $t^{\\mathrm{ord}} = 2$, $d = 2$, $t^{\\mathrm{res}} = 4$, $t^{\\mathrm{out}} = 12$, $t^{\\mathrm{cens}} = \\infty$.\n  - $i_5$: $t^{\\mathrm{ord}} = 3$, $d = 7$, $t^{\\mathrm{res}} = 10$, $t^{\\mathrm{out}} = 8$, $t^{\\mathrm{cens}} = \\infty$.\n  - $i_6$: $t^{\\mathrm{ord}} = 6$, $d = 2$, $t^{\\mathrm{res}} = 8$, $t^{\\mathrm{out}} = \\infty$, $t^{\\mathrm{cens}} = 7$.\n\n#### Naive Schema ($t^{\\mathrm{idx}} = t^{\\mathrm{res}}$)\nAn individual $i$ is included if $t_i^{\\mathrm{cens}} \\ge t_i^{\\mathrm{res}}$ and $t_i^{\\mathrm{out}} \\ge t_i^{\\mathrm{res}}$.\n- $i_1$: $t^{\\mathrm{res}}=2$. Included ($\\infty \\ge 2$, $5 \\ge 2$).\n- $i_2$: $t^{\\mathrm{res}}=4$. Excluded ($t^{\\mathrm{out}}=3  4$).\n- $i_3$: $t^{\\mathrm{res}}=5$. Included ($10 \\ge 5$, $\\infty \\ge 5$).\n- $i_4$: $t^{\\mathrm{res}}=4$. Included ($\\infty \\ge 4$, $12 \\ge 4$).\n- $i_5$: $t^{\\mathrm{res}}=10$. Excluded ($t^{\\mathrm{out}}=8  10$).\n- $i_6$: $t^{\\mathrm{res}}=8$. Excluded ($t^{\\mathrm{cens}}=7  8$).\n\nThe set of included individuals is $\\mathcal{I}_{\\mathrm{naive}} = \\{i_1, i_3, i_4\\}$, so $|\\mathcal{I}_{\\mathrm{naive}}| = 3$.\nThe outcome indicator $y_i^{(s)}=1$ if $t_i^{\\mathrm{out}} \\in [t_i^{\\mathrm{idx},s}, \\min\\{t_i^{\\mathrm{idx},s} + W, t_i^{\\mathrm{cens}}\\}]$.\n- $i_1$: $t^{\\mathrm{idx}}=2$. Window $[2, \\min\\{2+7, \\infty\\}]=[2, 9]$. $t^{\\mathrm{out}}=5 \\in [2, 9]$, so $y_1=1$.\n- $i_3$: $t^{\\mathrm{idx}}=5$. Window $[5, \\min\\{5+7, 10\\}]=[5, 10]$. $t^{\\mathrm{out}}=\\infty \\notin [5, 10]$, so $y_3=0$.\n- $i_4$: $t^{\\mathrm{idx}}=4$. Window $[4, \\min\\{4+7, \\infty\\}]=[4, 11]$. $t^{\\mathrm{out}}=12 \\notin [4, 11]$, so $y_4=0$.\n\nThe total number of outcomes is $1$. The risk is $R^{(\\mathrm{naive})} = 1/3 \\approx 0.333333$.\n\n#### Corrected Schema ($t^{\\mathrm{idx}} = t^{\\mathrm{ord}}$)\nAn individual $i$ is included if $t_i^{\\mathrm{cens}} \\ge t_i^{\\mathrm{ord}}$ and $t_i^{\\mathrm{out}} \\ge t_i^{\\mathrm{ord}}$.\n- $i_1$: $t^{\\mathrm{ord}}=0$. Included ($\\infty \\ge 0$, $5 \\ge 0$).\n- $i_2$: $t^{\\mathrm{ord}}=1$. Included ($\\infty \\ge 1$, $3 \\ge 1$).\n- $i_3$: $t^{\\mathrm{ord}}=4$. Included ($10 \\ge 4$, $\\infty \\ge 4$).\n- $i_4$: $t^{\\mathrm{ord}}=2$. Included ($\\infty \\ge 2$, $12 \\ge 2$).\n- $i_5$: $t^{\\mathrm{ord}}=3$. Included ($\\infty \\ge 3$, $8 \\ge 3$).\n- $i_6$: $t^{\\mathrm{ord}}=6$. Included ($7 \\ge 6$, $\\infty \\ge 6$).\n\nAll $6$ individuals are included, $\\mathcal{I}_{\\mathrm{corr}} = \\{i_1, i_2, i_3, i_4, i_5, i_6\\}$, so $|\\mathcal{I}_{\\mathrm{corr}}| = 6$.\n- $i_1$: $t^{\\mathrm{idx}}=0$. Window $[0, 7]$. $t^{\\mathrm{out}}=5 \\in [0, 7]$, $y_1=1$.\n- $i_2$: $t^{\\mathrm{idx}}=1$. Window $[1, 8]$. $t^{\\mathrm{out}}=3 \\in [1, 8]$, $y_2=1$.\n- $i_3$: $t^{\\mathrm{idx}}=4$. Window $[4, 10]$. $t^{\\mathrm{out}}=\\infty \\notin [4, 10]$, $y_3=0$.\n- $i_4$: $t^{\\mathrm{idx}}=2$. Window $[2, 9]$. $t^{\\mathrm{out}}=12 \\notin [2, 9]$, $y_4=0$.\n- $i_5$: $t^{\\mathrm{idx}}=3$. Window $[3, 10]$. $t^{\\mathrm{out}}=8 \\in [3, 10]$, $y_5=1$.\n- $i_6$: $t^{\\mathrm{idx}}=6$. Window $[6, \\min\\{6+7, 7\\}]=[6, 7]$. $t^{\\mathrm{out}}=\\infty \\notin [6, 7]$, $y_6=0$.\n\nThe total number of outcomes is $1+1+0+0+1+0 = 3$. The risk is $R^{(\\mathrm{corr})} = 3/6 = 0.5$.\nThe resulting triple is $(0.333333, 0.500000, 0.166667)$.\n\n### Test Case 2: $W=3$\nFollowing the same procedure:\n- **Naive Schema**: Included set $\\mathcal{I}_{\\mathrm{naive}} = \\{i_1, i_2, i_4, i_6\\}$, $|\\mathcal{I}_{\\mathrm{naive}}| = 4$. Outcomes: $y_1=1, y_2=1, y_4=0, y_6=0$. Risk $R^{(\\mathrm{naive})} = 2/4 = 0.5$.\n- **Corrected Schema**: Included set $\\mathcal{I}_{\\mathrm{corr}} = \\{i_1, i_2, i_3, i_4, i_6\\}$, $|\\mathcal{I}_{\\mathrm{corr}}| = 5$. Outcomes: $y_1=1, y_2=1, y_3=1, y_4=0, y_6=0$. Risk $R^{(\\mathrm{corr})} = 3/5 = 0.6$.\n- The resulting triple is $(0.500000, 0.600000, 0.100000)$.\n\n### Test Case 3: $W=5$\nFollowing the same procedure:\n- **Naive Schema**: Included set $\\mathcal{I}_{\\mathrm{naive}} = \\{i_4, i_5, i_6\\}$, $|\\mathcal{I}_{\\mathrm{naive}}| = 3$. Outcomes: $y_4=1, y_5=0, y_6=0$. Risk $R^{(\\mathrm{naive})} = 1/3 \\approx 0.333333$.\n- **Corrected Schema**: Included set $\\mathcal{I}_{\\mathrm{corr}} = \\{i_1, i_2, i_3, i_4, i_5, i_6\\}$, $|\\mathcal{I}_{\\mathrm{corr}}| = 6$. Outcomes: $y_1=1, y_2=0, y_3=0, y_4=1, y_5=0, y_6=0$. Risk $R^{(\\mathrm{corr})} = 2/6 = 1/3 \\approx 0.333333$.\n- The resulting triple is $(0.333333, 0.333333, 0.000000)$. In this instance, the bias in cohort construction happened to be offset by changes in the denominator and numerator, resulting fortuitously in no net change to the risk estimate.\n\nThe full set of calculations confirms the logic and results that will be implemented in the program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the immortal time bias problem for the given test cases.\n    \"\"\"\n    \n    # Use np.inf to represent infinity for time values.\n    inf = np.inf\n\n    test_cases = [\n        (7, [\n            (0, 2, 5, inf),   # i_1\n            (1, 3, 3, inf),   # i_2\n            (4, 1, inf, 10),  # i_3\n            (2, 2, 12, inf),  # i_4\n            (3, 7, 8, inf),   # i_5\n            (6, 2, inf, 7)    # i_6\n        ]),\n        (3, [\n            (0, 0, 0, inf),   # i_1\n            (5, 2, 8, inf),   # i_2\n            (1, 3, 2, inf),   # i_3\n            (3, 1, inf, 4),   # i_4\n            (2, 5, 1, inf),   # i_5\n            (6, 0, inf, 6)    # i_6\n        ]),\n        (5, [\n            (0, 4, 3, inf),   # i_1\n            (2, 1, inf, 2),   # i_2\n            (5, 10, inf, 7),  # i_3\n            (8, 0, 12, inf),  # i_4\n            (1, 2, 20, inf),  # i_5\n            (9, 1, inf, inf)  # i_6\n        ])\n    ]\n\n    all_results = []\n\n    for W, individuals in test_cases:\n        # --- Naive Schema Calculation ---\n        included_naive_count = 0\n        outcome_naive_count = 0\n        for ind in individuals:\n            t_ord, d, t_out, t_cens = ind\n            t_res = t_ord + d\n            \n            # Inclusion rule: individual is event-free and uncensored at index time\n            if t_cens = t_res and t_out = t_res:\n                included_naive_count += 1\n                \n                # Outcome rule: outcome occurs within the risk window\n                t_idx_naive = t_res\n                risk_window_end = min(t_idx_naive + W, t_cens)\n                if t_idx_naive = t_out = risk_window_end:\n                    outcome_naive_count += 1\n        \n        r_naive = outcome_naive_count / included_naive_count if included_naive_count  0 else 0.0\n\n        # --- Corrected Schema Calculation ---\n        included_corr_count = 0\n        outcome_corr_count = 0\n        for ind in individuals:\n            t_ord, d, t_out, t_cens = ind\n            \n            # Inclusion rule\n            if t_cens = t_ord and t_out = t_ord:\n                included_corr_count += 1\n                \n                # Outcome rule\n                t_idx_corr = t_ord\n                risk_window_end = min(t_idx_corr + W, t_cens)\n                if t_idx_corr = t_out = risk_window_end:\n                    outcome_corr_count += 1\n                    \n        r_corr = outcome_corr_count / included_corr_count if included_corr_count  0 else 0.0\n        \n        r_diff = r_corr - r_naive\n        \n        all_results.extend([r_naive, r_corr, r_diff])\n    \n    # Format results to six decimal places for the final output string\n    formatted_results = [f\"{round(r, 6):.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4853264"}, {"introduction": "With a properly defined cohort, the next step is to construct the predictive model itself. This practice moves beyond pre-built libraries, guiding you to implement a regularized logistic regression model from its mathematical foundations. By deriving the objective function and implementing a Newton-type optimization algorithm, you will develop a deep understanding of how models learn from clinical data and how techniques like `$L_2$` regularization lead to more robust and generalizable predictions [@problem_id:4853338].", "problem": "You will implement a penalized binary classifier for a clinical prediction task in medical informatics, using a model that maps patient features to the probability of $30$-day mortality. The target variable is $30$-day mortality coded as $0$ (survived) or $1$ (died). The feature set includes age, vitals, and comorbidities. You must derive, implement, and apply a regularized logistic regression with an `$L_2$` (Euclidean) penalty, and then interpret the change in odds per unit increase in age. Your implementation must start from the fundamental base that binary outcomes are modeled as independent Bernoulli random variables with probabilities determined by a differentiable link function from a linear predictor, together with penalization that discourages large parameter magnitudes. Do not assume or use any pre-given shortcut formulas; derive what you need from the foundational definitions.\n\nClinical variables and units:\n- Age $a$ in years.\n- Systolic blood pressure $s$ in millimeters of mercury (mmHg).\n- Heart rate $h$ in beats per minute (bpm).\n- Diabetes mellitus indicator $d \\in \\{0,1\\}$.\n- Chronic kidney disease indicator $c \\in \\{0,1\\}$.\n- Mortality outcome $y \\in \\{0,1\\}$.\n\nDataset (each line is one patient with $(a, s, h, d, c, y)$):\n- Patient $1$: $a=72$ years, $s=110$ mmHg, $h=92$ bpm, $d=1$, $c=1$, $y=1$.\n- Patient $2$: $a=55$ years, $s=130$ mmHg, $h=78$ bpm, $d=0$, $c=0$, $y=0$.\n- Patient $3$: $a=80$ years, $s=100$ mmHg, $h=88$ bpm, $d=1$, $c=1$, $y=1$.\n- Patient $4$: $a=45$ years, $s=120$ mmHg, $h=70$ bpm, $d=0$, $c=0$, $y=0$.\n- Patient $5$: $a=67$ years, $s=115$ mmHg, $h=85$ bpm, $d=0$, $c=1$, $y=1$.\n- Patient $6$: $a=60$ years, $s=140$ mmHg, $h=75$ bpm, $d=1$, $c=0$, $y=0$.\n- Patient $7$: $a=50$ years, $s=125$ mmHg, $h=68$ bpm, $d=0$, $c=0$, $y=0$.\n- Patient $8$: $a=76$ years, $s=105$ mmHg, $h=95$ bpm, $d=1$, $c=1$, $y=1$.\n- Patient $9$: $a=65$ years, $s=110$ mmHg, $h=82$ bpm, $d=1$, $c=0$, $y=0$.\n- Patient $10$: $a=58$ years, $s=135$ mmHg, $h=80$ bpm, $d=0$, $c=0$, $y=0$.\n- Patient $11$: $a=85$ years, $s=98$ mmHg, $h=100$ bpm, $d=1$, $c=1$, $y=1$.\n- Patient $12$: $a=70$ years, $s=108$ mmHg, $h=90$ bpm, $d=0$, $c=1$, $y=1$.\n\nModeling and tasks:\n- Use a linear predictor with an explicit intercept (constant term). Do not apply any penalty to the intercept; apply the `$L_2$` penalty to all other coefficients.\n- Starting from the definition of the Bernoulli likelihood and a differentiable link between the linear predictor and event probability, derive the objective function that is the sum of the negative log-likelihood and an `$L_2$` penalty term on the non-intercept coefficients.\n- Derive the gradient and Hessian of the objective with respect to the coefficient vector, and implement a Newton-type method with a backtracking line search to find the coefficient estimates.\n- Compute the coefficient estimates for the specified test suite.\n- Interpret the change in odds per unit increase in age (in years) as the exponential of the age coefficient. Report this interpretation for each test case.\n\nTest suite:\n- Case $1$: Penalty parameter $\\lambda = 0$, use the base features $(a, s, h, d, c)$ on all $12$ patients, no collinearity.\n- Case $2$: Penalty parameter $\\lambda = 0.1$, same features and patients as Case $1$.\n- Case $3$: Penalty parameter $\\lambda = 10$, same features and patients as Case $1$.\n- Case $4$: Penalty parameter $\\lambda = 1$, add a perfectly collinear feature equal to age (i.e., include both $a$ and an exact duplicate $a'$) on all $12$ patients.\n- Case $5$: Penalty parameter $\\lambda = 1$, use the base features $(a, s, h, d, c)$ but only the first $6$ patients.\n\nAnswer specification and units:\n- The coefficient for age must be interpreted per $1$ year increase, so the odds multiplier is $\\exp(\\beta_{\\text{age}})$, a unitless factor. Express the odds multiplier as a decimal.\n- Your program should compute, for each test case, the age coefficient $\\beta_{\\text{age}}$ and the odds multiplier $\\exp(\\beta_{\\text{age}})$.\n- Output format: a single line containing a comma-separated list enclosed in square brackets with the following sequence of floating-point numbers rounded to six decimal places: $[\\beta_{\\text{age}}^{(1)}, \\exp(\\beta_{\\text{age}}^{(1)}), \\beta_{\\text{age}}^{(2)}, \\exp(\\beta_{\\text{age}}^{(2)}), \\beta_{\\text{age}}^{(3)}, \\exp(\\beta_{\\text{age}}^{(3)}), \\beta_{\\text{age}}^{(4)}, \\exp(\\beta_{\\text{age}}^{(4)}), \\beta_{\\text{age}}^{(5)}, \\exp(\\beta_{\\text{age}}^{(5)})]$, where the superscript indicates the case number from $1$ to $5$.\n\nYour program must be self-contained, accept no input, and produce exactly the single-line output specified above.", "solution": "The problem requires the derivation and implementation of a logistic regression model with $L_2$ regularization (also known as ridge regression) to predict 30-day mortality from a set of clinical features. The implementation must be from first principles, beginning with the statistical foundation of the model and culminating in a Newton-Raphson optimization algorithm with backtracking line search.\n\n### I. Model Formulation\n\nWe model the binary outcome $y_i \\in \\{0, 1\\}$ for each patient $i$ as an independent Bernoulli random variable. The probability of a positive outcome ($y_i=1$, mortality) is denoted by $p_i$, which is a function of the patient's feature vector $\\mathbf{x}_i$. The feature vector for patient $i$ is $\\mathbf{x}_i = [1, a_i, s_i, h_i, d_i, c_i]$, where the initial $1$ corresponds to an intercept term. The full coefficient vector is $\\boldsymbol{\\beta} = [\\beta_0, \\beta_a, \\beta_s, \\beta_h, \\beta_d, \\beta_c]^T$, where $\\beta_0$ is the intercept.\n\nA linear predictor, $\\eta_i$, is defined as the inner product of the feature and coefficient vectors:\n$$\n\\eta_i = \\mathbf{x}_i \\boldsymbol{\\beta} = \\beta_0 + \\beta_a a_i + \\beta_s s_i + \\beta_h h_i + \\beta_d d_i + \\beta_c c_i\n$$\n\nThe relationship between the linear predictor $\\eta_i$ and the probability $p_i$ is established by the logistic link function (the inverse of the logit function):\n$$\np_i = P(y_i=1 | \\mathbf{x}_i; \\boldsymbol{\\beta}) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n$$\nwhere $\\sigma(\\cdot)$ is the sigmoid function. Correspondingly, the probability of survival is $P(y_i=0) = 1 - p_i = \\frac{e^{-\\eta_i}}{1 + e^{-\\eta_i}} = \\frac{1}{1 + e^{\\eta_i}}$.\n\n### II. Derivation of the Penalized Objective Function\n\nThe objective is to find the coefficient vector $\\boldsymbol{\\beta}$ that maximizes the likelihood of observing the given data, while penalizing large coefficient values to prevent overfitting. This is equivalent to minimizing the penalized negative log-likelihood.\n\n**1. Likelihood Function:**\nGiven that the $N$ observations are independent, the total likelihood of the data $(\\mathbf{y}, X)$ is the product of the individual Bernoulli probabilities:\n$$\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^{N} p_i^{y_i} (1 - p_i)^{1-y_i}\n$$\n\n**2. Log-Likelihood Function:**\nIt is more convenient to work with the logarithm of the likelihood function:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\nWe can express this in terms of the linear predictor $\\eta_i$. The log-odds (logit) is $\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\eta_i$. Also, $\\log(1-p_i) = -\\log(1+e^{\\eta_i})$. Substituting these into the log-likelihood expression yields:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ y_i \\eta_i - \\log(1+e^{\\eta_i}) \\right] = \\sum_{i=1}^{N} \\left[ y_i (\\mathbf{x}_i \\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i \\boldsymbol{\\beta}}) \\right]\n$$\n\n**3. Penalized Objective Function:**\nWe aim to minimize the negative log-likelihood, augmented with an $L_2$ penalty term. The penalty applies to all coefficients except the intercept $\\beta_0$. Let $p$ be the number of features (excluding the intercept). The penalty term is $\\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2$, where $\\lambda \\ge 0$ is the regularization parameter.\nThe final objective function to be minimized is $J(\\boldsymbol{\\beta})$:\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2 = \\sum_{i=1}^{N} \\left[ \\log(1+e^{\\mathbf{x}_i \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i \\boldsymbol{\\beta}) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^{p} \\beta_j^2\n$$\n\n### III. Optimization using Newton's Method\n\nTo find the optimal $\\boldsymbol{\\beta}$ that minimizes $J(\\boldsymbol{\\beta})$, we use a Newton-Raphson method, which is an iterative second-order optimization algorithm. The update rule is:\n$$\n\\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - \\alpha [H(\\boldsymbol{\\beta}^{(k)})]^{-1} \\mathbf{g}(\\boldsymbol{\\beta}^{(k)})\n$$\nwhere $\\mathbf{g}(\\boldsymbol{\\beta})$ is the gradient of $J(\\boldsymbol{\\beta})$, $H(\\boldsymbol{\\beta})$ is its Hessian matrix, and $\\alpha$ is a step size determined by a line search procedure.\n\n**1. Gradient of the Objective Function:**\nThe gradient $\\mathbf{g}(\\boldsymbol{\\beta}) = \\nabla J(\\boldsymbol{\\beta})$ is a vector of partial derivatives $\\frac{\\partial J}{\\partial \\beta_j}$.\n$$\n\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^{N} \\left[ \\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i \\boldsymbol{\\beta}}) - y_i \\frac{\\partial}{\\partial \\beta_j} (\\mathbf{x}_i \\boldsymbol{\\beta}) \\right] + \\frac{\\partial}{\\partial \\beta_j} \\left( \\frac{\\lambda}{2} \\sum_{k=1}^{p} \\beta_k^2 \\right)\n$$\nUsing the chain rule, $\\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\eta_i}) = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} \\frac{\\partial \\eta_i}{\\partial \\beta_j} = p_i x_{ij}$. Also, $\\frac{\\partial}{\\partial \\beta_j} (\\mathbf{x}_i \\boldsymbol{\\beta}) = x_{ij}$. The derivative of the penalty term is $\\lambda \\beta_j$ for $j \\ge 1$ and $0$ for $j=0$.\n$$\n\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^{N} [p_i x_{ij} - y_i x_{ij}] + \\lambda \\beta_j \\cdot \\mathbb{I}(j \\ge 1) = \\sum_{i=1}^{N} (p_i - y_i) x_{ij} + \\lambda \\beta_j \\cdot \\mathbb{I}(j \\ge 1)\n$$\nIn matrix notation, let $X$ be the $N \\times (p+1)$ design matrix, $\\mathbf{y}$ be the $N \\times 1$ vector of outcomes, and $\\mathbf{p}$ be the $N \\times 1$ vector of probabilities. Let $\\boldsymbol{\\beta}^*$ be the coefficient vector with the intercept zeroed out ($\\beta_0^*=0, \\beta_j^*=\\beta_j$ for $j \\ge 1$). The gradient is:\n$$\n\\mathbf{g}(\\boldsymbol{\\beta}) = \\nabla J(\\boldsymbol{\\beta}) = X^T(\\mathbf{p} - \\mathbf{y}) + \\lambda \\boldsymbol{\\beta}^*\n$$\n\n**2. Hessian of the Objective Function:**\nThe Hessian $H(\\boldsymbol{\\beta}) = \\nabla^2 J(\\boldsymbol{\\beta})$ is a matrix of second partial derivatives $H_{jk} = \\frac{\\partial^2 J}{\\partial \\beta_k \\partial \\beta_j}$.\n$$\nH_{jk} = \\frac{\\partial}{\\partial \\beta_k} \\left[ \\sum_{i=1}^{N} (p_i - y_i) x_{ij} \\right] + \\frac{\\partial}{\\partial \\beta_k} (\\lambda \\beta_j \\cdot \\mathbb{I}(j \\ge 1))\n$$\nThe derivative of $p_i$ with respect to $\\eta_i$ is $\\frac{dp_i}{d\\eta_i} = p_i(1-p_i)$. Using the chain rule, $\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{dp_i}{d\\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i)x_{ik}$.\n$$\nH_{jk} = \\sum_{i=1}^{N} x_{ij} \\frac{\\partial p_i}{\\partial \\beta_k} + \\lambda \\delta_{jk} \\cdot \\mathbb{I}(j \\ge 1) = \\sum_{i=1}^{N} x_{ij} p_i(1-p_i) x_{ik} + \\lambda \\delta_{jk} \\cdot \\mathbb{I}(j \\ge 1)\n$$\nwhere $\\delta_{jk}$ is the Kronecker delta. In matrix form, let $W$ be an $N \\times N$ diagonal matrix with $W_{ii} = p_i(1-p_i)$. Let $\\boldsymbol{\\Lambda}$ be a $(p+1) \\times (p+1)$ diagonal matrix with $\\Lambda_{00}=0$ and $\\Lambda_{jj}=\\lambda$ for $j \\ge 1$. The Hessian is:\n$$\nH(\\boldsymbol{\\beta}) = X^T W X + \\boldsymbol{\\Lambda}\n$$\nThis Hessian is guaranteed to be positive semi-definite. For $\\lambda  0$, it is positive definite, ensuring a unique solution even in the presence of collinearity (as in Case 4) or data separation (as in Case 5).\n\n**3. Backtracking Line Search:**\nTo ensure convergence, the full Newton step may be too large. We introduce a step size $\\alpha \\in (0, 1]$ chosen to satisfy the Armijo-Goldstein sufficient decrease condition. Given a search direction $\\mathbf{d} = -H^{-1}\\mathbf{g}$, we start with $\\alpha=1$ and decrease it by a factor $\\tau$ (e.g., $0.5$) until $J(\\boldsymbol{\\beta} + \\alpha \\mathbf{d}) \\le J(\\boldsymbol{\\beta}) + c_1 \\alpha \\mathbf{g}^T \\mathbf{d}$ for a small constant $c_1$ (e.g., $10^{-4}$).\n\n### IV. Interpretation of Coefficients\nIn logistic regression, the coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor. The odds of mortality are $\\frac{p_i}{1-p_i} = e^{\\eta_i}$. For a one-unit increase in a feature $x_j$, the new log-odds are $\\eta_i + \\beta_j$. The new odds are $e^{\\eta_i + \\beta_j} = e^{\\eta_i}e^{\\beta_j}$. Thus, the odds are multiplied by a factor of $e^{\\beta_j}$, which is the odds ratio. For the age coefficient $\\beta_a$, the odds ratio for a one-year increase in age is $\\exp(\\beta_a)$.\n\n### V. Application to Test Cases\nThe derived algorithm is applied to the five specified test cases. For each case, the appropriate design matrix $X$, outcome vector $\\mathbf{y}$, and penalty parameter $\\lambda$ are constructed. The Newton-Raphson optimizer is then used to find the coefficient vector $\\boldsymbol{\\beta}$. Finally, the age coefficient $\\beta_a$ (which is $\\beta_1$ in our zero-indexed vector) and the corresponding odds ratio $\\exp(\\beta_a)$ are computed and reported.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and applies a regularized logistic regression model\n    from first principles to a clinical prediction task.\n    \"\"\"\n\n    # Dataset: (age, sbp, hr, diabetes, ckd, mortality)\n    data = np.array([\n        [72, 110, 92, 1, 1, 1],\n        [55, 130, 78, 0, 0, 0],\n        [80, 100, 88, 1, 1, 1],\n        [45, 120, 70, 0, 0, 0],\n        [67, 115, 85, 0, 1, 1],\n        [60, 140, 75, 1, 0, 0],\n        [50, 125, 68, 0, 0, 0],\n        [76, 105, 95, 1, 1, 1],\n        [65, 110, 82, 1, 0, 0],\n        [58, 135, 80, 0, 0, 0],\n        [85, 98, 100, 1, 1, 1],\n        [70, 108, 90, 0, 1, 1]\n    ], dtype=float)\n\n    X_raw = data[:, :-1]\n    y_raw = data[:, -1]\n\n    def _logistic_func(eta):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        # p = 1 / (1 + exp(-eta))\n        p = np.empty_like(eta)\n        pos_mask = eta = 0\n        neg_mask = ~pos_mask\n        p[pos_mask] = 1. / (1. + np.exp(-eta[pos_mask]))\n        exp_eta_neg = np.exp(eta[neg_mask])\n        p[neg_mask] = exp_eta_neg / (1. + exp_eta_neg)\n        return p\n\n    def _objective_function(X, y, beta, lambda_val):\n        \"\"\"Calculates the penalized negative log-likelihood.\"\"\"\n        eta = X @ beta\n        beta_penalized = beta.copy()\n        beta_penalized[0] = 0.0\n        \n        # Numerically stable calculation of log(1 + exp(eta))\n        log_likelihood_term = np.empty_like(eta)\n        pos_mask = eta = 0\n        neg_mask = ~pos_mask\n        log_likelihood_term[pos_mask] = eta[pos_mask] + np.log(1. + np.exp(-eta[pos_mask]))\n        log_likelihood_term[neg_mask] = np.log(1. + np.exp(eta[neg_mask]))\n        \n        neg_log_likelihood = np.sum(log_likelihood_term - y * eta)\n        penalty = (lambda_val / 2.0) * np.sum(beta_penalized**2)\n        return neg_log_likelihood + penalty\n\n    def solve_logistic_newton(X, y, lambda_val, max_iter=100, tol=1e-8):\n        \"\"\"\n        Solves regularized logistic regression using Newton-Raphson with backtracking.\n        \"\"\"\n        n_samples, n_features = X.shape\n        beta = np.zeros(n_features)\n        \n        # Backtracking line search parameters\n        alpha_init = 1.0\n        c1 = 1e-4\n        tau = 0.5\n\n        for i in range(max_iter):\n            beta_old = beta.copy()\n\n            # Calculate probabilities, gradient, and Hessian\n            eta = X @ beta\n            p = _logistic_func(eta)\n            \n            # Gradient\n            beta_penalized = beta.copy()\n            beta_penalized[0] = 0.0\n            grad = X.T @ (p - y) + lambda_val * beta_penalized\n\n            # Hessian\n            W = np.diag(p * (1.0 - p))\n            hessian = X.T @ W @ X\n            hessian_penalty = np.diag(np.full(n_features, lambda_val))\n            hessian_penalty[0, 0] = 0.0\n            hessian += hessian_penalty\n\n            # Newton step direction\n            try:\n                step = np.linalg.solve(hessian, -grad)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular\n                step = -grad\n\n            # Backtracking line search\n            alpha = alpha_init\n            obj_val = _objective_function(X, y, beta, lambda_val)\n            grad_dot_step = grad.T @ step\n            \n            while True:\n                beta_new = beta + alpha * step\n                obj_val_new = _objective_function(X, y, beta_new, lambda_val)\n                \n                # Check Armijo-Goldstein condition\n                if obj_val_new = obj_val + c1 * alpha * grad_dot_step or alpha  1e-9:\n                    break\n                alpha *= tau\n            \n            beta = beta_new\n\n            # Check for convergence\n            if np.linalg.norm(beta - beta_old)  tol:\n                break\n        \n        return beta\n\n    test_cases = [\n        {'lambda': 0.0, 'data': (X_raw, y_raw), 'collinear': False},\n        {'lambda': 0.1, 'data': (X_raw, y_raw), 'collinear': False},\n        {'lambda': 10.0, 'data': (X_raw, y_raw), 'collinear': False},\n        {'lambda': 1.0, 'data': (X_raw, y_raw), 'collinear': True},\n        {'lambda': 1.0, 'data': (X_raw[:6], y_raw[:6]), 'collinear': False},\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_val = case['lambda']\n        X_case_raw, y_case = case['data']\n        n_samples = X_case_raw.shape[0]\n\n        if case['collinear']:\n            age_col = X_case_raw[:, 0].reshape(-1, 1)\n            X_case_features = np.hstack([X_case_raw, age_col])\n        else:\n            X_case_features = X_case_raw\n\n        X = np.hstack([np.ones((n_samples, 1)), X_case_features])\n        \n        beta_solution = solve_logistic_newton(X, y_case, lambda_val)\n        \n        beta_age = beta_solution[1]  # Age is the first feature after intercept\n        odds_multiplier = np.exp(beta_age)\n        \n        results.append(round(beta_age, 6))\n        results.append(round(odds_multiplier, 6))\n\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "4853338"}, {"introduction": "A predictive model is only as valuable as our ability to measure its performance. This final practice focuses on evaluation, specifically for models predicting time-to-event outcomes. You will implement Harrell's Concordance Index (C-index), a key metric for assessing a model's ability to correctly rank patients by risk, forcing you to engage with the statistical complexities introduced by right-censored data—a common feature of clinical datasets [@problem_id:4853322].", "problem": "You are given predictions from a time-to-event model in a clinical setting, where each patient has an observed time $T_i$ (in arbitrary time units), an event indicator $\\delta_i$ (with $\\delta_i = 1$ meaning that the clinical event occurred and $\\delta_i = 0$ meaning the observation was right-censored), and a model-produced risk score $r_i$ (larger $r_i$ indicates higher risk of earlier event). The goal is to implement a program that computes Harrell’s Concordance Index (C-index) for survival data, rigorously handling tied and censored pairs. The Concordance Index (C-index) is defined as the fraction of appropriately comparable patient pairs for which the ordering of the predicted risk scores matches the observed ordering of event times, with tied risk scores contributing half credit.\n\nFundamental base to use: survival data produce right-censored observations, which restrict pairwise comparisons to those pairs where the earlier observed time corresponds to an event (and not censoring). Two patients $i$ and $j$ are considered comparable if and only if either $T_i  T_j$ with $\\delta_i = 1$, or $T_j  T_i$ with $\\delta_j = 1$. If $T_i = T_j$ then the pair is not comparable regardless of $\\delta_i$ and $\\delta_j$. For each comparable pair, the pair contributes:\n- $1$ if the patient with the earlier event time has a strictly higher risk score,\n- $0$ if the patient with the earlier event time has a strictly lower risk score,\n- $0.5$ if the two risk scores are exactly equal.\n\nHarrell’s C-index for the dataset is then computed as the sum of pairwise contributions divided by the number of comparable pairs. If there are no comparable pairs, return $0.0$ as a sentinel value indicating that discrimination cannot be assessed from the provided data.\n\nImplement a program that, for each test case below, computes the Harrell’s C-index following the above rules and prints a single line containing all results as decimal values with exactly four digits after the decimal point, aggregated in a comma-separated list enclosed in square brackets.\n\nTest suite (each case is a tuple of $(T, \\delta, r)$):\n- Case $1$ (mixed events and censoring, mostly well-ordered risk):\n  - $T = [10, 8, 12, 7, 14, 9]$\n  - $\\delta = [1, 1, 0, 1, 0, 1]$\n  - $r = [0.7, 0.8, 0.2, 0.9, 0.1, 0.6]$\n- Case $2$ (perfect ordering, all events):\n  - $T = [5, 10, 15, 20, 25]$\n  - $\\delta = [1, 1, 1, 1, 1]$\n  - $r = [0.9, 0.8, 0.7, 0.6, 0.5]$\n- Case $3$ (no comparable pairs, all censored):\n  - $T = [5, 10, 15]$\n  - $\\delta = [0, 0, 0]$\n  - $r = [0.5, 0.6, 0.7]$\n- Case $4$ (ties in event times and risk scores, mixed censoring):\n  - $T = [10, 10, 12, 12, 15]$\n  - $\\delta = [1, 1, 1, 0, 1]$\n  - $r = [0.5, 0.5, 0.7, 0.7, 0.4]$\n- Case $5$ (inverse ordering, all events):\n  - $T = [5, 8, 12, 20]$\n  - $\\delta = [1, 1, 1, 1]$\n  - $r = [0.1, 0.2, 0.3, 0.4]$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and with exactly four digits after the decimal point for each value (e.g., $[0.9286,1.0000,0.0000,0.4286,0.0000]$). No user input is required. No physical units are involved. Express all results as decimals, not percentages.", "solution": "The problem requires the implementation of Harrell's Concordance Index (C-index) for right-censored survival data. This is a standard measure of discrimination for models that predict time-to-event outcomes. Before proceeding to the solution, a formal validation of the problem statement is required.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n-   **Input Data**: For each patient $i$, we are given:\n    -   An observed time $T_i$.\n    -   An event indicator $\\delta_i$, where $\\delta_i = 1$ indicates an event and $\\delta_i = 0$ indicates right-censoring.\n    -   A model-produced risk score $r_i$, where higher values indicate higher risk.\n-   **Definition of Comparable Pairs**: A pair of patients $(i, j)$ is comparable if and only if the patient with the earlier observed time experienced an event. Formally, this means either ($T_i  T_j$ and $\\delta_i = 1$) or ($T_j  T_i$ and $\\delta_j = 1$). Pairs with tied event times ($T_i = T_j$) are not comparable.\n-   **Scoring of Comparable Pairs**: For a comparable pair:\n    -   The score is $1$ if the patient with the earlier event time has a strictly higher risk score. This is a concordant pair.\n    -   The score is $0$ if the patient with the earlier event time has a strictly lower risk score. This is a discordant pair.\n    -   The score is $0.5$ if the risk scores are exactly equal. This is a tied pair.\n-   **C-index Calculation**: The C-index is the sum of scores for all comparable pairs divided by the total number of comparable pairs.\n-   **Edge Case**: If there are no comparable pairs, the C-index is defined as $0.0$.\n-   **Test Suite**: Five test cases are provided, each consisting of arrays for $T$, $\\delta$, and $r$.\n-   **Output Format**: A single line containing a comma-separated list of C-index values for each test case, formatted to four decimal places and enclosed in square brackets.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n-   **Scientifically Grounded**: The problem describes Harrell's C-index, a cornerstone metric in survival analysis for evaluating the discriminative power of prognostic models. The definitions for comparable pairs (accounting for censoring) and the scoring of concordance, discordance, and ties are standard and correct in the field of biostatistics and medical informatics. The problem is scientifically sound.\n-   **Well-Posed**: The rules provided are explicit and unambiguous. They define a clear algorithm that, given any valid input dataset ($T, \\delta, r$), yields a unique numerical result. The explicit handling of the case with zero comparable pairs (division by zero) ensures the problem is well-posed.\n-   **Objective**: The problem is stated using precise mathematical definitions and algorithmic rules, free of any subjective or ambiguous language.\n-   **Completeness and Consistency**: All necessary components (data structure, definitions, calculation rule, edge case handling) are provided. There are no contradictions. For example, the explicit rule that pairs with $T_i = T_j$ are not comparable prevents ambiguity.\n-   **Realism and Feasibility**: The data provided in the test cases are simple numerical arrays and are entirely plausible as outputs from a clinical study and a predictive model. The task is computationally feasible.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is scientifically grounded, well-posed, objective, and complete. I will now proceed with a full solution.\n\nThe solution involves a systematic pairwise comparison of all patients in a given dataset. Let the number of patients be $N$. We must consider every unique pair of patients $(i, j)$ where $0 \\le i  j  N$. For each pair, we determine if they are comparable and, if so, calculate their contribution to the C-index.\n\nLet $C_{total}$ be the count of comparable pairs and $S_{concordant}$ be the sum of scores. Initially, both are set to $0$.\n\nThe algorithm proceeds as follows:\n1.  Initialize $C_{total} = 0$ and $S_{concordant} = 0.0$.\n2.  Iterate through all pairs of patients $(i, j)$ such that $i$ ranges from $0$ to $N-2$ and $j$ ranges from $i+1$ to $N-1$.\n3.  For each pair $(i, j)$, retrieve their observed times ($T_i, T_j$), event indicators ($\\delta_i, \\delta_j$), and risk scores ($r_i, r_j$).\n4.  Apply the comparability rule:\n    a. If $T_i  T_j$ and $\\delta_i = 1$, the pair is comparable. The patient with the earlier event is $i$.\n    b. If $T_j  T_i$ and $\\delta_j = 1$, the pair is comparable. The patient with the earlier event is $j$.\n    c. If $T_i = T_j$, the pair is not comparable.\n    d. If $T_i  T_j$ but $\\delta_i = 0$, the pair is not comparable.\n    e. If $T_j  T_i$ but $\\delta_j = 0$, the pair is not comparable.\n5.  If a pair is determined to be comparable:\n    a. Increment the count of comparable pairs: $C_{total} = C_{total} + 1$.\n    b. Let the patient with the earlier event be $u$ and the other patient be $v$. We compare their risk scores $r_u$ and $r_v$.\n    c. If $r_u  r_v$ (concordant), add $1.0$ to the score sum: $S_{concordant} = S_{concordant} + 1.0$.\n    d. If $r_u  r_v$ (discordant), add $0.0$ to the score sum: $S_{concordant} = S_{concordant} + 0.0$.\n    e. If $r_u = r_v$ (tied risk), add $0.5$ to the score sum: $S_{concordant} = S_{concordant} + 0.5$.\n6.  After iterating through all pairs, the C-index is calculated.\n    a. If $C_{total}  0$, the C-index is $I_C = \\frac{S_{concordant}}{C_{total}}$.\n    b. If $C_{total} = 0$, the C-index is defined as $0.0$.\n\nThis algorithm correctly implements the specified definition of Harrell's C-index by exhaustively checking all pairs, correctly identifying comparable pairs according to the rules of censored data, and scoring them appropriately.", "answer": "```python\nimport numpy as np\n\ndef calculate_c_index(T, delta, r):\n    \"\"\"\n    Computes Harrell's Concordance Index (C-index) for survival data.\n\n    Args:\n        T (list or np.ndarray): Observed times for each patient.\n        delta (list or np.ndarray): Event indicators (1=event, 0=censored).\n        r (list or np.ndarray): Predicted risk scores (higher value means higher risk).\n\n    Returns:\n        float: The calculated C-index.\n    \"\"\"\n    T = np.asarray(T, dtype=np.float64)\n    delta = np.asarray(delta, dtype=np.int32)\n    r = np.asarray(r, dtype=np.float64)\n\n    n_patients = len(T)\n    if n_patients  2:\n        return 0.0\n\n    comparable_pairs_count = 0\n    concordant_sum = 0.0\n\n    for i in range(n_patients):\n        for j in range(i + 1, n_patients):\n            # Extract data for the pair (i, j)\n            T_i, delta_i, r_i = T[i], delta[i], r[i]\n            T_j, delta_j, r_j = T[j], delta[j], r[j]\n\n            # Rule: Pairs with tied event times are not comparable.\n            if T_i == T_j:\n                continue\n\n            # Determine if the pair is comparable based on censoring rules\n            # and identify the patient with the earlier event.\n            is_comparable = False\n            earlier_event_patient_risk = 0\n            other_patient_risk = 0\n\n            if T_i  T_j and delta_i == 1:\n                # Patient i has an event before patient j's observation\n                is_comparable = True\n                earlier_event_patient_risk = r_i\n                other_patient_risk = r_j\n            elif T_j  T_i and delta_j == 1:\n                # Patient j has an event before patient i's observation\n                is_comparable = True\n                earlier_event_patient_risk = r_j\n                other_patient_risk = r_i\n            \n            # If the pair is comparable, score it.\n            if is_comparable:\n                comparable_pairs_count += 1\n                \n                # Concordant: earlier event has higher risk score\n                if earlier_event_patient_risk  other_patient_risk:\n                    concordant_sum += 1.0\n                # Tied risk scores\n                elif earlier_event_patient_risk == other_patient_risk:\n                    concordant_sum += 0.5\n                # Discordant: earlier event has lower risk score (add 0.0)\n\n    # Calculate C-index\n    if comparable_pairs_count == 0:\n        return 0.0\n    else:\n        return concordant_sum / comparable_pairs_count\n\ndef solve():\n    \"\"\"\n    Runs the C-index calculation for all provided test cases and prints the results.\n    \"\"\"\n    # Test cases defined in the problem statement\n    test_cases = [\n        # Case 1\n        (\n            [10, 8, 12, 7, 14, 9],\n            [1, 1, 0, 1, 0, 1],\n            [0.7, 0.8, 0.2, 0.9, 0.1, 0.6]\n        ),\n        # Case 2\n        (\n            [5, 10, 15, 20, 25],\n            [1, 1, 1, 1, 1],\n            [0.9, 0.8, 0.7, 0.6, 0.5]\n        ),\n        # Case 3\n        (\n            [5, 10, 15],\n            [0, 0, 0],\n            [0.5, 0.6, 0.7]\n        ),\n        # Case 4\n        (\n            [10, 10, 12, 12, 15],\n            [1, 1, 1, 0, 1],\n            [0.5, 0.5, 0.7, 0.7, 0.4]\n        ),\n        # Case 5\n        (\n            [5, 8, 12, 20],\n            [1, 1, 1, 1],\n            [0.1, 0.2, 0.3, 0.4]\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        T, delta, r = case\n        c_index = calculate_c_index(T, delta, r)\n        results.append(f\"{c_index:.4f}\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4853322"}]}