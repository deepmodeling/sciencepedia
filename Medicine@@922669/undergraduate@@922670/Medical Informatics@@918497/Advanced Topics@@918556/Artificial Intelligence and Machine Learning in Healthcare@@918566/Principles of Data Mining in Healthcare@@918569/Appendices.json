{"hands_on_practices": [{"introduction": "Raw electronic health record (EHR) data, such as a list of diagnosis codes, must be carefully processed to become useful for data mining. This exercise guides you through a fundamental task in clinical informatics called computational phenotyping, which involves defining a patient's condition from data. By calculating the widely-used Charlson Comorbidity Index (CCI), you will practice applying temporal rules like lookback windows and handling clinical hierarchies, transforming low-level data into a high-value predictive feature. [@problem_id:4854005]", "problem": "A health system seeks to compute a baseline Charlson Comorbidity Index (CCI) at an index encounter for a single patient, using International Classification of Diseases (ICD) diagnosis codes recorded in an Electronic Health Record (EHR). The baseline must follow standard cohort alignment principles from medical informatics and healthcare data mining: comorbidities should be ascertained from a fixed lookback period prior to the index date, and codes recorded after the index date are excluded from the baseline unless explicitly modeled to correct for potential misclassification. \n\nUse the following foundational basis:\n- The Charlson Comorbidity Index (CCI) is defined as the sum of category weights, with each comorbidity category counted at most once. Relevant categories for this patient and their canonical weights are: myocardial infarction ($1$), congestive heart failure ($1$), peripheral vascular disease ($1$), chronic pulmonary disease ($1$), diabetes without complication ($1$), diabetes with end-organ damage ($2$), moderate to severe renal disease ($2$), and acquired immunodeficiency syndrome / human immunodeficiency virus (AIDS/HIV) ($6$). If both diabetes without complication and diabetes with end-organ damage are present, only the higher weight for diabetes with end-organ damage ($2$) is counted.\n- Cohort alignment: A fixed lookback window of $365$ days prior to the index date $I$ is used to define baseline comorbidity status. Codes recorded strictly after $I$ are excluded from baseline ascertainment, but to address potential misclassification for chronic conditions recorded post-index, assume an uncertainty model in which a post-index code contributes its weight multiplied by the probability that it reflects a pre-existing condition rather than a new incident condition associated with the index event. Assume independence of these post-index misclassification probabilities across categories.\n\nIndex date:\n- $I = 2024$-$07$-$01$\n\nICD-10 codes with record dates for the patient:\n- I25.2 (old myocardial infarction) on $2023$-$08$-$15$\n- I50.9 (congestive heart failure) on $2024$-$06$-$10$\n- E11.9 (type $2$ diabetes mellitus without complications) on $2023$-$12$-$05$\n- E11.21 (type $2$ diabetes mellitus with diabetic nephropathy) on $2024$-$06$-$20$\n- J44.9 (chronic obstructive pulmonary disease) on $2024$-$03$-$10$\n- I73.9 (peripheral vascular disease) on $2024$-$05$-$21$\n- N18.5 (chronic kidney disease, stage $5$) on $2024$-$07$-$10$, recorded post-index, with probability $p_{r} = 0.65$ that it reflects a pre-existing condition\n- B20 (human immunodeficiency virus disease) on $2024$-$07$-$03$, recorded post-index, with probability $p_{h} = 0.25$ that it reflects a pre-existing condition\n- C18.9 (malignant neoplasm of colon) on $2023$-$01$-$20$\n- G81.9 (hemiplegia, unspecified) on $2022$-$12$-$31$\n\nTasks grounded in data mining principles:\n- Determine the $365$-day lookback window strictly prior to $I$.\n- Identify which codes fall inside the lookback window and map them to CCI categories using the category rules above, ensuring that within-category de-duplication and severity precedence for diabetes are enforced. Codes outside the lookback window are excluded. For codes recorded after $I$, exclude them from deterministic baseline status but incorporate their contributions in an expected-value sense using the provided misclassification probabilities.\n- Under the independence assumption for post-index misclassification across categories, compute the expected CCI at $I$ as the sum of the deterministic pre-index baseline weights plus the probability-weighted post-index contributions.\n\nExpress the final answer as a unitless decimal, and round your answer to four significant figures. No units are required in the final answer. Provide only the single numerical value as the final answer.", "solution": "The user has requested the calculation of an expected Charlson Comorbidity Index (CCI) for a patient based on diagnosis codes from an Electronic Health Record (EHR). The process requires validating the problem statement, applying principles of medical informatics for cohort construction, and performing the specified calculations.\n\n### Step 1: Problem Validation\n\nFirst, the givens of the problem statement are extracted verbatim.\n- **Index Date**: $I = 2024-07-01$.\n- **Lookback Window**: $365$ days prior to the index date $I$.\n- **CCI Categories and Weights**:\n  - Myocardial infarction: weight $1$.\n  - Congestive heart failure: weight $1$.\n  - Peripheral vascular disease: weight $1$.\n  - Chronic pulmonary disease: weight $1$.\n  - Diabetes without complication: weight $1$.\n  - Diabetes with end-organ damage: weight $2$.\n  - Moderate to severe renal disease: weight $2$.\n  - Acquired immunodeficiency syndrome / human immunodeficiency virus (AIDS/HIV): weight $6$.\n- **CCI Calculation Rules**:\n  - Each category is counted at most once.\n  - If both diabetes categories are present, only the higher weight ($2$) is counted.\n- **Post-Index Uncertainty Model**:\n  - A post-index code contributes its weight multiplied by the probability that it reflects a pre-existing condition.\n  - Independence of these probabilities is assumed.\n- **Patient Data (ICD-10 codes and dates)**:\n  - `I25.2` (old myocardial infarction) on $2023-08-15$\n  - `I50.9` (congestive heart failure) on $2024-06-10$\n  - `E11.9` (type $2$ diabetes mellitus without complications) on $2023-12-05$\n  - `E11.21` (type $2$ diabetes mellitus with diabetic nephropathy) on $2024-06-20$\n  - `J44.9` (chronic obstructive pulmonary disease) on $2024-03-10$\n  - `I73.9` (peripheral vascular disease) on $2024-05-21$\n  - `N18.5` (chronic kidney disease, stage $5$) on $2024-07-10$, with probability $p_{r} = 0.65$ of being pre-existing.\n  - `B20` (human immunodeficiency virus disease) on $2024-07-03$, with probability $p_{h} = 0.25$ of being pre-existing.\n  - `C18.9` (malignant neoplasm of colon) on $2023-01-20$\n  - `G81.9` (hemiplegia, unspecified) on $2022-12-31$\n- **Task**: Compute the expected CCI, rounding the final answer to four significant figures.\n\nThe problem is scientifically grounded, using standard concepts from medical informatics like the CCI, ICD codes, and lookback windows. It is well-posed, with all necessary data and rules provided to compute a unique, meaningful solution. The language is objective and precise. The problem does not violate any of the invalidity criteria. Therefore, the problem is deemed **valid**.\n\n### Step 2: Solution Derivation\n\nThe solution proceeds in four stages: (1) determining the lookback period, (2) identifying and mapping relevant baseline codes, (3) calculating the deterministic component of the CCI, and (4) calculating the expected value from post-index codes to find the total expected CCI.\n\n**1. Determine the Lookback Window**\nThe index date is $I = 2024-07-01$. The lookback window spans the $365$ days strictly prior to this date. The year $2024$ is a leap year, containing February $29$.\nThe period starts on $2023-07-02$ and ends on $2024-06-30$. Any diagnosis code recorded within this inclusive date range $[2023-07-02, 2024-06-30]$ is considered for the baseline comorbidity assessment.\n\n**2. Identify and Map Baseline Codes**\nWe examine each code's record date to determine its inclusion.\n- `I25.2` (Myocardial Infarction) on $2023-08-15$: **IN** the window. Maps to Myocardial Infarction.\n- `I50.9` (Congestive Heart Failure) on $2024-06-10$: **IN** the window. Maps to Congestive Heart Failure.\n- `E11.9` (Diabetes w/o complication) on $2023-12-05$: **IN** the window. Maps to Diabetes without complication.\n- `E11.21` (Diabetes w/ complication) on $2024-06-20$: **IN** the window. Maps to Diabetes with end-organ damage.\n- `J44.9` (Chronic Pulmonary Disease) on $2024-03-10$: **IN** the window. Maps to Chronic Pulmonary Disease.\n- `I73.9` (Peripheral Vascular Disease) on $2024-05-21$: **IN** the window. Maps to Peripheral Vascular Disease.\n- `C18.9` (Malignant neoplasm) on $2023-01-20$: **OUTSIDE** the window (too early).\n- `G81.9` (Hemiplegia) on $2022-12-31$: **OUTSIDE** the window (too early).\n\nThe codes `N18.5` and `B20` are recorded after the index date and will be handled separately.\n\n**3. Calculate the Deterministic Baseline CCI ($C_{det}$)**\nFrom the codes within the lookback window, we sum the weights of the corresponding unique CCI categories, applying the specified rules.\n- Myocardial Infarction: weight $w_{mi} = 1$.\n- Congestive Heart Failure: weight $w_{chf} = 1$.\n- Chronic Pulmonary Disease: weight $w_{cpd} = 1$.\n- Peripheral Vascular Disease: weight $w_{pvd} = 1$.\n- Diabetes: Both `diabetes without complication` (weight $1$) and `diabetes with end-organ damage` (weight $2$) are present. According to the rule, only the higher weight is counted. Thus, the contribution for diabetes is $w_{dm} = 2$.\n\nThe deterministic baseline score, $C_{det}$, is the sum of these weights:\n$$C_{det} = w_{mi} + w_{chf} + w_{cpd} + w_{pvd} + w_{dm}$$\n$$C_{det} = 1 + 1 + 1 + 1 + 2 = 6$$\n\n**4. Calculate the Expected CCI**\nThe total expected CCI is the sum of the deterministic baseline score and the expected contributions from the post-index codes.\nThe expected contribution of a post-index code is its weight multiplied by its probability of reflecting a pre-existing condition.\n\n- For `N18.5` (Moderate to severe renal disease):\n  - Weight: $w_{r} = 2$.\n  - Probability: $p_{r} = 0.65$.\n  - Expected contribution: $E_{r} = w_{r} \\times p_{r} = 2 \\times 0.65 = 1.3$.\n\n- For `B20` (AIDS/HIV):\n  - Weight: $w_{h} = 6$.\n  - Probability: $p_{h} = 0.25$.\n  - Expected contribution: $E_{h} = w_{h} \\times p_{h} = 6 \\times 0.25 = 1.5$.\n\nThe total expected contribution from post-index codes, $E_{post}$, is the sum of the individual expected contributions, based on the independence assumption.\n$$E_{post} = E_{r} + E_{h} = 1.3 + 1.5 = 2.8$$\n\nThe total expected CCI, $E[CCI]$, is:\n$$E[CCI] = C_{det} + E_{post}$$\n$$E[CCI] = 6 + 2.8 = 8.8$$\n\nThe problem requires the answer to be rounded to four significant figures. The value $8.8$ is expressed as $8.800$ to meet this requirement.", "answer": "$$\\boxed{8.800}$$", "id": "4854005"}, {"introduction": "When we create features from healthcare data, such as indicators for thousands of possible medical codes, the resulting patient-feature matrix is typically sparse—meaning it is overwhelmingly filled with zeros. Storing and processing this matrix naively is incredibly inefficient. This practice delves into the crucial trade-off between dense and sparse data representations, allowing you to calculate the tipping point where one becomes more efficient than the other and understand the profound impact of sparsity on algorithmic performance. [@problem_id:4854028]", "problem": "Consider a binary patient-feature matrix $X \\in \\{0,1\\}^{n \\times p}$, where $n$ is the number of patients and $p$ is the number of features. Let the proportion of nonzero entries (sparsity level) be $q \\in (0,1)$, so the number of nonzeros is $k = q \\, n \\, p$. Two storage schemes are used in healthcare data mining systems: a dense bit-level representation that stores each entry of $X$ using $1$ bit, and a sparse index-only representation in which each nonzero is stored by its row and column indices. The latter is closely related to Compressed Sparse Row (CSR) and Coordinate List (COO); for the purposes of this problem, assume each nonzero stores both its row index and its column index in fixed-width binary fields with minimal bit-lengths sufficient to address $n$ rows and $p$ columns, namely $\\lceil \\log_{2} n \\rceil$ bits for a row index and $\\lceil \\log_{2} p \\rceil$ bits for a column index. Because $X$ is binary, no value field is stored for the sparse scheme.\n\nStarting from core definitions of information representation (bit-length needed to index $n$ and $p$ distinct positions) and algorithmic complexity (order-of-growth with respect to problem size), do the following:\n\n(a) Derive the total memory usage in bits, $S_{\\text{dense}}(n,p,q)$ and $S_{\\text{sparse}}(n,p,q)$, for the dense and the sparse index-only representations described above.\n\n(b) Solve for the critical sparsity $q_{\\star}$ at which the two representations use equal memory, expressed as a closed-form analytic expression in terms of $n$ and $p$ only. Interpret how $q$ relative to $q_{\\star}$ determines which representation is more memory-efficient.\n\n(c) Using the fundamental definition of algorithmic complexity, reason about the runtime order for computing the matrix-vector product $X \\boldsymbol{\\beta}$ for $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ under the dense and sparse schemes as a function of $n$, $p$, and $q$. Explain how increasing sparsity (smaller $q$) affects algorithmic complexity for large-scale healthcare datasets with many features but sparse observations. No numerical answer is required for part (c).\n\nYour final answer must be the single closed-form expression for $q_{\\star}$ from part (b). No rounding is required.", "solution": "We begin from core information-theoretic and algorithmic principles. A binary matrix stored densely requires $1$ bit per entry. The minimal number of bits required to encode an integer index over $n$ possible rows is $\\lceil \\log_{2} n \\rceil$, and similarly over $p$ possible columns is $\\lceil \\log_{2} p \\rceil$. These follow from the definition of binary encoding length for addressing a finite set of positions.\n\nPart (a). The dense representation places every entry of $X$ explicitly in memory with $1$ bit per entry, hence\n$$\nS_{\\text{dense}}(n,p,q) = n \\, p.\n$$\nThe sparse index-only representation stores only the nonzeros. The number of nonzeros is $k = q \\, n \\, p$. For each nonzero, the scheme stores its row index using $b_{r} = \\lceil \\log_{2} n \\rceil$ bits and its column index using $b_{c} = \\lceil \\log_{2} p \\rceil$ bits. Because the underlying matrix is binary, no value field needs to be stored. Therefore, the total memory in bits is\n$$\nS_{\\text{sparse}}(n,p,q) = k \\, (b_{r} + b_{c}) = q \\, n \\, p \\, \\big( \\lceil \\log_{2} n \\rceil + \\lceil \\log_{2} p \\rceil \\big).\n$$\n\nPart (b). The critical sparsity $q_{\\star}$ is the value of $q$ at which the two memory usages are equal. Setting $S_{\\text{dense}}(n,p,q_{\\star}) = S_{\\text{sparse}}(n,p,q_{\\star})$ yields\n$$\nn \\, p = q_{\\star} \\, n \\, p \\, \\big( \\lceil \\log_{2} n \\rceil + \\lceil \\log_{2} p \\rceil \\big).\n$$\nAssuming $n \\geq 1$ and $p \\geq 1$, we can cancel $n \\, p$ to obtain\n$$\nq_{\\star} = \\frac{1}{\\lceil \\log_{2} n \\rceil + \\lceil \\log_{2} p \\rceil}.\n$$\nInterpretation: if $q < q_{\\star}$, then $S_{\\text{sparse}}(n,p,q) < S_{\\text{dense}}(n,p,q)$, so the sparse scheme is more memory-efficient; if $q > q_{\\star}$, the dense scheme uses less memory. The threshold depends monotonically on $\\lceil \\log_{2} n \\rceil$ and $\\lceil \\log_{2} p \\rceil$, decreasing as either dimension grows, meaning that for large matrices the sparse scheme requires very small $q$ to be memory-favorable when only indices are stored.\n\nPart (c). We analyze the runtime to compute $X \\boldsymbol{\\beta}$ by counting the number of arithmetic operations that scale with the stored elements. For the dense representation, every entry contributes to the computation. A straightforward matrix-vector multiply performs $n \\, p$ multiply-accumulate operations (specifically, $n \\, p$ multiplications and approximately $n (p - 1)$ additions), which scales as\n$$\nO(n \\, p).\n$$\nFor the sparse index-only representation, only the $k = q \\, n \\, p$ nonzero entries contribute. Each nonzero entry corresponds to adding the feature contribution to the appropriate patient’s running sum. If the matrix is binary, the multiplication $X_{ij} \\beta_{j}$ reduces to either including or skipping $\\beta_{j}$, but the operation count remains proportional to $k$. Thus the runtime scales as\n$$\nO(k) = O(q \\, n \\, p).\n$$\nAs $q$ decreases (greater sparsity), the sparse runtime reduces proportionally, while the dense runtime remains $O(n \\, p)$ regardless of $q$. In healthcare-scale datasets with very large $p$ but relatively few positive findings per patient, adopting sparse computation can yield substantial gains in both memory and runtime, enabling algorithms such as linear models, logistic regression, and iterative solvers to operate in time proportional to the number of clinically present features rather than the full feature space.\n\nThe closed-form expression for the critical sparsity derived in part (b) is the requested final answer.", "answer": "$$\\boxed{\\frac{1}{\\lceil \\log_{2} n \\rceil + \\lceil \\log_{2} p \\rceil}}$$", "id": "4854028"}, {"introduction": "A predictive model's output, a probability, is not a decision in itself; it's an input to a decision-making process where the stakes are high. In healthcare, the cost of a false negative (e.g., a missed diagnosis) is often vastly different from the cost of a false positive (e.g., an unnecessary procedure). This exercise uses the principles of decision theory to determine the optimal probability threshold for action, balancing these asymmetric costs to minimize expected harm and maximize clinical utility. [@problem_id:4853996]", "problem": "A hospital deploys a calibrated probabilistic classifier for early detection of a binary condition, where calibration means that for any patient with features $X=x$, the model score equals the posterior probability $s(x)=\\mathbb{P}(Y=1\\mid X=x)$. The clinical action based on the model is a single-threshold decision rule: predict positive if $s(x)\\ge \\tau$ and negative otherwise. The cost of a false negative is $C_{FN}>0$, the cost of a false positive is $C_{FP}>0$, and the disease prevalence is $\\pi=\\mathbb{P}(Y=1)\\in(0,1)$. Assume there is no cost for correct decisions, and that expected per-patient cost is the quantity to be minimized.\n\nUsing only the core definitions of expected cost, conditional risk, and the definitions of true positive rate $TPR(\\tau)$ and false positive rate $FPR(\\tau)$ as functions of the threshold $\\tau$, derive from first principles:\n\n1. The optimal threshold $\\tau^{*}$ that minimizes the expected cost per patient under the threshold rule applied to the calibrated score $s(x)$.\n2. An explicit expression for the slope of the Receiver Operating Characteristic (ROC) curve, defined as $d\\,TPR/d\\,FPR$ evaluated at the operating point induced by $\\tau^{*}$, expressed in terms of $C_{FN}$, $C_{FP}$, and $\\pi$.\n\nReport your final answer as a single row matrix $\\begin{pmatrix}\\tau^{*} & m^{*}\\end{pmatrix}$, where $m^{*}$ denotes the slope of the ROC curve at the operating point corresponding to $\\tau^{*}$. No numerical substitution is required. If you choose to provide any numerical approximation, round to four significant figures. The answer is dimensionless and should be expressed without any units.", "solution": "The user has provided a problem statement that requires validation before a solution can be attempted.\n\n### Step 1: Extract Givens\n- A calibrated probabilistic classifier is used for a binary condition ($Y \\in \\{0, 1\\}$).\n- The classifier score $s(x)$ equals the posterior probability: $s(x) = \\mathbb{P}(Y=1 \\mid X=x)$.\n- A single-threshold decision rule is applied: predict positive if $s(x) \\ge \\tau$ and negative otherwise.\n- The cost of a false negative is $C_{FN} > 0$.\n- The cost of a false positive is $C_{FP} > 0$.\n- There is no cost for correct decisions (cost is $0$).\n- The disease prevalence is $\\pi = \\mathbb{P}(Y=1)$, with $\\pi \\in (0, 1)$.\n- The objective is to minimize the expected per-patient cost.\n- The task is to derive:\n  1. The optimal threshold $\\tau^{*}$ that minimizes the expected cost.\n  2. The slope of the Receiver Operating Characteristic (ROC) curve, $m^{*} = d\\,TPR/d\\,FPR$, at the operating point corresponding to $\\tau^{*}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is a standard application of Bayesian decision theory to classifier thresholding, a fundamental concept in statistical pattern recognition and medical informatics. It is scientifically sound.\n- **Well-Posed**: The problem is well-defined. It provides all necessary components (costs, probabilities, a decision rule, and a clear objective function—expected cost) to derive a unique solution for the optimal threshold and the corresponding ROC slope.\n- **Objective**: The problem is stated using precise, unambiguous mathematical and statistical language.\n\nThe problem does not exhibit any of the defined flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It is a formal, solvable problem based on established principles.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived from first principles as requested.\n\n### Derivation\n\nThe solution is derived in two parts as requested by the problem statement.\n\n**Part 1: Derivation of the Optimal Threshold $\\tau^{*}$**\n\nThe optimal decision rule is one that minimizes the expected cost for any given patient. We formulate the conditional risk (expected cost) for a patient with features $x$, which results in a score $s(x)$, for each of the two possible actions: predict positive ($\\hat{y}=1$) or predict negative ($\\hat{y}=0$).\n\nLet $s(x)$ be the score for a given patient. Due to the classifier's calibration, we have $s(x) = \\mathbb{P}(Y=1 \\mid X=x)$. Consequently, the probability of the patient being negative is $\\mathbb{P}(Y=0 \\mid X=x) = 1 - s(x)$.\n\nThe conditional risk of predicting positive, $R(\\hat{y}=1 \\mid x)$, is the sum of the costs of the possible outcomes, weighted by their conditional probabilities:\n$$R(\\hat{y}=1 \\mid x) = (\\text{cost if } Y=1, \\hat{y}=1) \\cdot \\mathbb{P}(Y=1 \\mid X=x) + (\\text{cost if } Y=0, \\hat{y}=1) \\cdot \\mathbb{P}(Y=0 \\mid X=x)$$\nThe first term is a true positive, with cost $0$. The second term is a false positive, with cost $C_{FP}$.\n$$R(\\hat{y}=1 \\mid x) = 0 \\cdot s(x) + C_{FP} \\cdot (1 - s(x)) = C_{FP}(1 - s(x))$$\n\nThe conditional risk of predicting negative, $R(\\hat{y}=0 \\mid x)$, is similarly defined:\n$$R(\\hat{y}=0 \\mid x) = (\\text{cost if } Y=1, \\hat{y}=0) \\cdot \\mathbb{P}(Y=1 \\mid X=x) + (\\text{cost if } Y=0, \\hat{y}=0) \\cdot \\mathbb{P}(Y=0 \\mid X=x)$$\nThe first term is a false negative, with cost $C_{FN}$. The second term is a true negative, with cost $0$.\n$$R(\\hat{y}=0 \\mid x) = C_{FN} \\cdot s(x) + 0 \\cdot (1 - s(x)) = C_{FN}s(x)$$\n\nTo minimize the expected cost, for any given $s(x)$, we should choose the action with the lower conditional risk. The optimal decision is to predict positive ($\\hat{y}=1$) if and only if $R(\\hat{y}=1 \\mid x) \\le R(\\hat{y}=0 \\mid x)$. The case of equality is conventionally assigned to the positive class.\n$$C_{FP}(1 - s(x)) \\le C_{FN}s(x)$$\nWe solve for $s(x)$:\n$$C_{FP} - C_{FP}s(x) \\le C_{FN}s(x)$$\n$$C_{FP} \\le (C_{FN} + C_{FP})s(x)$$\n$$s(x) \\ge \\frac{C_{FP}}{C_{FN} + C_{FP}}$$\n\nThe problem states that the decision rule is to predict positive if $s(x) \\ge \\tau$. Comparing this with our derived optimal rule, we identify the optimal threshold $\\tau^{*}$ as the value that satisfies this condition.\n$$\\tau^{*} = \\frac{C_{FP}}{C_{FN} + C_{FP}}$$\n\n**Part 2: Derivation of the ROC Slope $m^{*}$ at $\\tau^{*}$**\n\nThe Receiver Operating Characteristic (ROC) curve plots the True Positive Rate ($TPR$) against the False Positive Rate ($FPR$) for varying threshold values $\\tau$. The slope of the ROC curve at a point is given by $m = d\\,TPR / d\\,FPR$.\n\nLet $f_1(s)$ be the probability density function (PDF) of the score $s(x)$ for the positive class ($Y=1$), and $f_0(s)$ be the PDF of the score for the negative class ($Y=0$). The $TPR$ and $FPR$ as functions of the threshold $\\tau$ are:\n$$TPR(\\tau) = \\mathbb{P}(s(X) \\ge \\tau \\mid Y=1) = \\int_{\\tau}^{1} f_1(s) \\,ds$$\n$$FPR(\\tau) = \\mathbb{P}(s(X) \\ge \\tau \\mid Y=0) = \\int_{\\tau}^{1} f_0(s) \\,ds$$\n\nUsing the chain rule, the slope $m(\\tau)$ is:\n$$m(\\tau) = \\frac{d\\,TPR}{d\\,FPR} = \\frac{d\\,TPR/d\\tau}{d\\,FPR/d\\tau}$$\nApplying the Fundamental Theorem of Calculus:\n$$\\frac{d\\,TPR}{d\\tau} = \\frac{d}{d\\tau} \\int_{\\tau}^{1} f_1(s) \\,ds = -f_1(\\tau)$$\n$$\\frac{d\\,FPR}{d\\tau} = \\frac{d}{d\\tau} \\int_{\\tau}^{1} f_0(s) \\,ds = -f_0(\\tau)$$\nThus, the slope at a point corresponding to threshold $\\tau$ is the ratio of the densities at that score value:\n$$m(\\tau) = \\frac{-f_1(\\tau)}{-f_0(\\tau)} = \\frac{f_1(\\tau)}{f_0(\\tau)}$$\n\nWe now relate this ratio to the score value $\\tau$ using the classifier's calibration property, $s = \\mathbb{P}(Y=1 \\mid \\text{score}=s)$. Applying Bayes' theorem to the posterior probability:\n$$\\mathbb{P}(Y=1 \\mid \\text{score}=s) = \\frac{\\mathbb{P}(\\text{score}=s \\mid Y=1) \\mathbb{P}(Y=1)}{\\mathbb{P}(\\text{score}=s)}$$\nIn terms of PDFs and the prevalence $\\pi$:\n$$s = \\frac{f_1(s) \\pi}{f_1(s) \\pi + f_0(s) (1-\\pi)}$$\nWe rearrange this equation to solve for the ratio $f_1(s)/f_0(s)$:\n$$s(f_1(s) \\pi + f_0(s) (1-\\pi)) = f_1(s) \\pi$$\n$$s f_0(s) (1-\\pi) = f_1(s) \\pi - s f_1(s) \\pi = f_1(s) \\pi (1-s)$$\n$$\\frac{f_1(s)}{f_0(s)} = \\frac{s(1-\\pi)}{\\pi(1-s)}$$\nThe slope of the ROC curve at the operating point defined by threshold $\\tau$ is therefore:\n$$m(\\tau) = \\frac{\\tau(1-\\pi)}{\\pi(1-\\tau)}$$\n\nWe need to evaluate this slope at the optimal threshold $\\tau^{*}$. Let $m^{*} = m(\\tau^{*})$:\n$$m^{*} = \\frac{\\tau^{*}(1-\\pi)}{\\pi(1-\\tau^{*})}$$\nWe substitute the expression for $\\tau^{*}$ from Part 1:\n$$\\tau^{*} = \\frac{C_{FP}}{C_{FN} + C_{FP}}$$\nAnd also substitute the expression for $1-\\tau^{*}$:\n$$1 - \\tau^{*} = 1 - \\frac{C_{FP}}{C_{FN} + C_{FP}} = \\frac{C_{FN} + C_{FP} - C_{FP}}{C_{FN} + C_{FP}} = \\frac{C_{FN}}{C_{FN} + C_{FP}}$$\nNow, substitute these into the equation for $m^{*}$:\n$$m^{*} = \\frac{\\left( \\frac{C_{FP}}{C_{FN} + C_{FP}} \\right) (1-\\pi)}{\\pi \\left( \\frac{C_{FN}}{C_{FN} + C_{FP}} \\right)}$$\nThe term $(C_{FN} + C_{FP})$ in the numerator and denominator cancels out, yielding:\n$$m^{*} = \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}$$\nThis is the required expression for the slope of the ROC curve at the optimal operating point.\n\nThe final answer is the pair $(\\tau^{*}, m^{*})$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{C_{FP}}{C_{FN} + C_{FP}} & \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi} \\end{pmatrix}}$$", "id": "4853996"}]}