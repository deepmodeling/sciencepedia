{"hands_on_practices": [{"introduction": "A foundational challenge in patient safety is that official reports capture only a fraction of all adverse events. To see the full picture, we must estimate what remains unseen. This practice introduces the capture-recapture method, a powerful statistical technique that uses the overlap between two independent reporting systems—such as an automated tool and voluntary reports—to estimate the true total number of events. By applying this method to a hypothetical dataset [@problem_id:4852033], you will learn to quantify the hidden burden of underreporting, a critical first step in gauging the true scale of patient safety issues.", "problem": "A tertiary hospital uses two independent adverse event reporting sources for patient safety monitoring: a rule-based trigger tool within the Electronic Health Record (EHR) and a voluntary incident reporting system. Over one quarter, the trigger tool recorded $n_1 = 120$ adverse events and the incident reporting system recorded $n_2 = 150$ adverse events. Upon de-duplication, there were $m = 60$ events that appeared in both sources.\n\nAssume a closed population of adverse events over the quarter, equal catchability across events, and source independence, meaning the probability that an event is captured by both sources equals the product of the probabilities that it is captured by each source individually. Using these assumptions and starting from the definitions of capture probabilities and counts, derive the two-source capture-recapture estimator for the total number of adverse events in the quarter and compute its numerical value for the given data.\n\nDefine the underreporting fraction as the proportion of adverse events missing from both sources relative to the estimated total, that is, the fraction of the estimated total number of adverse events that were not observed in either source. Compute this underreporting fraction for the given data.\n\nExpress the final underreporting fraction as a decimal number and round your answer to four significant figures.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established statistical method of capture-recapture analysis, a standard technique in fields like epidemiology and ecology, and its application to adverse event reporting is a recognized use case in medical informatics. The problem is well-posed, providing all necessary data ($n_1, n_2, m$) and clear assumptions (closed population, equal catchability, source independence) to derive a unique and meaningful solution. The language is objective and the data are internally consistent.\n\nLet $N$ represent the true total number of adverse events in the closed population during the quarter. This is the unknown quantity to be estimated.\nLet $n_1$ be the number of events captured by the first source (the EHR trigger tool), so $n_1 = 120$.\nLet $n_2$ be the number of events captured by the second source (the voluntary incident reporting system), so $n_2 = 150$.\nLet $m$ be the number of events captured by both sources, so $m = 60$.\n\nLet $p_1$ be the probability that a given adverse event is captured by source $1$.\nLet $p_2$ be the probability that a given adverse event is captured by source $2$.\nThe assumption of equal catchability means $p_1$ and $p_2$ are constant for all events.\n\nThe number of events captured by each source can be related to the total population size $N$ and the capture probabilities. The expected number of events captured are:\n$$E[n_1] = Np_1$$\n$$E[n_2] = Np_2$$\n\nThe assumption of source independence states that the probability of an event being captured by both sources is the product of the individual capture probabilities, $p_1 p_2$. Therefore, the expected number of events captured by both sources is:\n$$E[m] = N p_1 p_2$$\n\nWe can derive an estimator for $N$ using the method of moments. From the equations above, we can express $p_1$ and $p_2$ as:\n$$p_1 = \\frac{E[n_1]}{N} \\quad \\text{and} \\quad p_2 = \\frac{E[n_2]}{N}$$\nSubstituting these into the equation for $E[m]$:\n$$E[m] = N \\left( \\frac{E[n_1]}{N} \\right) \\left( \\frac{E[n_2]}{N} \\right) = \\frac{E[n_1] E[n_2]}{N}$$\nRearranging to solve for $N$, we get:\n$$N = \\frac{E[n_1] E[n_2]}{E[m]}$$\nBy replacing the expected values with the observed sample counts ($n_1$, $n_2$, and $m$), we obtain the two-source capture-recapture estimator for $N$, denoted as $\\hat{N}$:\n$$\\hat{N} = \\frac{n_1 n_2}{m}$$\nThis is also known as the Lincoln-Petersen estimator.\n\nNow, we compute the numerical value of $\\hat{N}$ using the provided data:\n$$\\hat{N} = \\frac{120 \\times 150}{60} = \\frac{18000}{60} = 300$$\nThe estimated total number of adverse events is $300$.\n\nNext, we are asked to compute the underreporting fraction, which is defined as the proportion of adverse events missing from both sources relative to the estimated total. This corresponds to the probability that an event is missed by source $1$ AND missed by source $2$.\nThe probability of an event being missed by source $1$ is $(1 - p_1)$.\nThe probability of an event being missed by source $2$ is $(1 - p_2)$.\nDue to the independence assumption, the probability of being missed by both sources is the product of these individual probabilities:\n$$P(\\text{missed by both}) = (1 - p_1)(1 - p_2)$$\nThis quantity is the underreporting fraction, which we denote $f_U$. We estimate it by substituting estimators for $p_1$ and $p_2$.\n\nWe can estimate $p_1$ and $p_2$ from the data. The proportion of events from the second sample ($n_2$) that were also captured by the first source ($m$) provides an estimate for $p_1$.\n$$\\hat{p}_1 = \\frac{m}{n_2}$$\nSimilarly, the proportion of events from the first sample ($n_1$) that were also captured by the second source ($m$) provides an estimate for $p_2$.\n$$\\hat{p}_2 = \\frac{m}{n_1}$$\nSubstituting these estimators into the expression for the underreporting fraction gives our estimator $\\hat{f}_U$:\n$$\\hat{f}_U = (1 - \\hat{p}_1)(1 - \\hat{p}_2) = \\left(1 - \\frac{m}{n_2}\\right)\\left(1 - \\frac{m}{n_1}\\right)$$\nUsing the given numerical values:\n$$\\hat{p}_1 = \\frac{60}{150} = \\frac{2}{5} = 0.4$$\n$$\\hat{p}_2 = \\frac{60}{120} = \\frac{1}{2} = 0.5$$\nNow we compute the underreporting fraction:\n$$\\hat{f}_U = (1 - 0.4)(1 - 0.5) = (0.6)(0.5) = 0.3$$\n\nAlternatively, we can calculate the number of events missed by both sources. The total number of unique events observed is $n_1 + n_2 - m = 120 + 150 - 60 = 210$.\nThe estimated number of unobserved events, $U$, is the estimated total minus the observed total:\n$$U = \\hat{N} - (n_1 + n_2 - m) = 300 - 210 = 90$$\nThe underreporting fraction is then the ratio of unobserved events to the estimated total:\n$$\\hat{f}_U = \\frac{U}{\\hat{N}} = \\frac{90}{300} = 0.3$$\nBoth methods yield the same result. The problem requires the answer as a decimal number rounded to four significant figures. The value $0.3$ expressed to four significant figures is $0.3000$.", "answer": "$$\\boxed{0.3000}$$", "id": "4852033"}, {"introduction": "Once we recognize the need for better detection, we can build automated tools to sift through vast amounts of clinical data. But how do we know if these tools are any good? This exercise will walk you through the calculation of sensitivity and specificity, the two cornerstone metrics for evaluating any classification system. Using the performance data of a hypothetical keyword-based adverse event classifier [@problem_id:4852083], you will gain hands-on experience in assessing a tool's ability to correctly identify true events while avoiding false alarms.", "problem": "A hospital deploys a simple keyword-based rule system to flag mentions of a specific medication-related adverse event in Electronic Health Record (EHR) notes. In a test set of $500$ notes, independent adjudication (the gold standard) identified $50$ notes that truly contain the adverse event. The classifier flagged $65$ notes as positive; of these flagged notes, $45$ were truly positive, and $20$ were false positives. Using only the standard definitions of sensitivity and specificity relative to the gold standard, compute both metrics for this classifier. Express each metric as a reduced fraction. Provide your final answer as a row matrix $\\begin{pmatrix}\\text{sensitivity} & \\text{specificity}\\end{pmatrix}$. No rounding is required. Then, in one to two sentences, explain how adding clinical negation handling in Natural Language Processing (NLP) would be expected to change these metrics if it primarily corrects phrases such as “no evidence of X” and “denies X” without altering recognition of affirmed events. Your explanation does not affect the numerical answer you submit.", "solution": "The problem is well-posed and valid. It provides a self-contained and consistent set of data for a standard binary classification evaluation task. All necessary data points are provided, and they are internally consistent. The concepts of sensitivity and specificity are fundamental, objective, and scientifically grounded metrics in statistics and medical informatics.\n\nTo solve the problem, we first establish the components of the confusion matrix. Let $N$ be the total number of notes, $P$ be the number of notes with the condition present (true cases), and $N_{neg}$ be the number of notes with the condition absent (true negatives).\n\nFrom the givens:\nTotal notes, $N = 500$.\nTotal notes with the true adverse event, $P = 50$.\nTherefore, the total number of notes without the adverse event is $N_{neg} = N - P = 500 - 50 = 450$.\n\nThe classifier's performance is described by the following:\nTotal notes flagged as positive (Predicted Positives) = $65$.\nOf these, the number of correctly flagged notes, or True Positives ($TP$), is $TP = 45$.\nThe number of incorrectly flagged notes, or False Positives ($FP$), is $FP = 20$.\nWe can verify the consistency of the data: $TP + FP = 45 + 20 = 65$, which matches the total number of notes flagged as positive.\n\nNow we can determine the remaining two components of the confusion matrix:\nFalse Negatives ($FN$) are the notes that truly contain the adverse event but were not flagged by the classifier. This is the difference between the total true cases and the true positives.\n$$FN = P - TP = 50 - 45 = 5$$\n\nTrue Negatives ($TN$) are the notes that do not contain the adverse event and were correctly not flagged by the classifier. This is the difference between the total non-event notes and the false positives.\n$$TN = N_{neg} - FP = 450 - 20 = 430$$\n\nWe can summarize these values in a contingency table:\n\\begin{array}{c|c|c|c}\n & \\text{Condition Positive} & \\text{Condition Negative} & \\text{Total} \\\\\n\\hline\n\\text{Predicted Positive} & TP = 45 & FP = 20 & 65 \\\\\n\\hline\n\\text{Predicted Negative} & FN = 5 & TN = 430 & 435 \\\\\n\\hline\n\\text{Total} & 50 & 450 & 500 \\\\\n\\end{array}\n\nNow, we compute sensitivity and specificity using their standard definitions.\n\nSensitivity, also known as the True Positive Rate ($TPR$), is the proportion of actual positives that are correctly identified. The formula is:\n$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\nSubstituting the values we found:\n$$\\text{Sensitivity} = \\frac{45}{45 + 5} = \\frac{45}{50}$$\nReducing this fraction to its simplest form:\n$$\\text{Sensitivity} = \\frac{9 \\times 5}{10 \\times 5} = \\frac{9}{10}$$\n\nSpecificity, also known as the True Negative Rate ($TNR$), is the proportion of actual negatives that are correctly identified. The formula is:\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\nSubstituting the values we found:\n$$\\text{Specificity} = \\frac{430}{430 + 20} = \\frac{430}{450}$$\nReducing this fraction to its simplest form by dividing the numerator and denominator by $10$:\n$$\\text{Specificity} = \\frac{43}{45}$$\nThe number $43$ is prime, so this fraction cannot be reduced further.\n\nThe final answer is to be presented as a row matrix $\\begin{pmatrix}\\text{sensitivity} & \\text{specificity}\\end{pmatrix}$.\n$$\\begin{pmatrix} \\frac{9}{10} & \\frac{43}{45} \\end{pmatrix}$$\n\nRegarding the final explanatory question: Adding NLP for negation handling would primarily correct false positives by identifying negated mentions of the event, thereby increasing the number of true negatives. This change would be expected to increase the classifier's specificity while leaving its sensitivity unchanged, as the number of true positives and false negatives would not be affected.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{10} & \\frac{43}{45} \\end{pmatrix}}$$", "id": "4852083"}, {"introduction": "A snapshot evaluation of a safety tool is useful, but true safety management requires continuous vigilance. We need methods to monitor performance over time and detect when a process is drifting into a state of higher risk. This practice introduces the Exponentially Weighted Moving Average (EWMA) chart, a key method from Statistical Process Control designed to detect small, persistent shifts in data. By calculating the EWMA statistic for a sequence of hypothetical adverse event rates [@problem_id:4852115], you will understand how this technique provides an early warning signal, allowing for proactive intervention before major problems arise.", "problem": "A tertiary hospital uses Exponentially Weighted Moving Average (EWMA) monitoring within Statistical Process Control (SPC) to track the daily proportion of adverse medication events recorded in the electronic incident reporting system. The observed daily proportions over four consecutive days are $x_{1}=0.02$, $x_{2}=0.03$, $x_{3}=0.05$, and $x_{4}=0.04$ (each expressed as a decimal fraction of events per case). The baseline EWMA is initialized at $\\hat{z}_{0}=0.02$. The smoothing parameter is $\\lambda=0.2$.\n\nStarting from the fundamental definition that an EWMA is a weighted average of past observations with weights that decay geometrically over time, and that the total weight allocated to the past observations and the baseline must sum to $1$, derive the recursive form satisfied by the EWMA statistic and then compute the EWMA value at day $t=4$.\n\nExplain why EWMA monitoring is appropriate for patient safety (small-shift) detection by connecting the geometric weight decay to sensitivity for recent changes, and then use the derived recursion to obtain $\\hat{z}_{4}$ given $\\lambda=0.2$, $\\hat{z}_{0}=0.02$, and $x_{1}=0.02$, $x_{2}=0.03$, $x_{3}=0.05$, $x_{4}=0.04$. Express your final answer for $\\hat{z}_{4}$ as a decimal fraction and do not round.", "solution": "The problem statement is first validated for scientific soundness, self-consistency, and clarity.\n\n**Step 1: Extract Givens**\n- **Process**: Exponentially Weighted Moving Average (EWMA) monitoring of daily proportion of adverse medication events.\n- **Observations**: Daily proportions $x_{1}=0.02$, $x_{2}=0.03$, $x_{3}=0.05$, and $x_{4}=0.04$.\n- **Initial Condition**: Baseline EWMA at day $t=0$ is $\\hat{z}_{0}=0.02$.\n- **Smoothing Parameter**: $\\lambda=0.2$.\n- **Task 1**: Derive the recursive form of the EWMA statistic from its fundamental definition as a geometrically weighted average where total weight sums to $1$.\n- **Task 2**: Explain the appropriateness of EWMA for detecting small shifts in patient safety data.\n- **Task 3**: Compute the EWMA value at day $t=4$, denoted $\\hat{z}_{4}$.\n- **Output Format for $\\hat{z}_{4}$**: Decimal fraction without rounding.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically sound. EWMA is a standard technique in Statistical Process Control (SPC), and its application to monitoring adverse events in healthcare (a subfield of medical informatics) is a well-established and valid use case. The concept of using EWMA for small-shift detection is a core feature of the method.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary data ($\\hat{z}_{0}$, $\\lambda$, and the time series data $x_t$) to uniquely determine the value of $\\hat{z}_{4}$. The tasks—derivation, explanation, and calculation—are clear and lead to a definitive result.\n- **Objectivity**: The problem is stated in objective, quantitative terms, free of subjective or ambiguous language.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, and objective. There are no contradictions, missing information, or violations of scientific principles. The solution process may proceed.\n\n**Part 1: Derivation of the Recursive Form**\n\nThe fundamental definition of the EWMA statistic, $\\hat{z}_t$, at time $t$ is a weighted average of all past observations ($x_1, x_2, \\dots, x_t$) and the initial baseline value $\\hat{z}_0$. The weights for the observations decay geometrically into the past. For an observation $k$ periods in the past, $x_{t-k}$, its weight is proportional to $(1-\\lambda)^k$. The most recent observation, $x_t$, receives a weight of $\\lambda$.\n\nSpecifically, the expanded form of the EWMA statistic at time $t$ is:\n$$ \\hat{z}_{t} = \\lambda x_{t} + \\lambda(1-\\lambda)x_{t-1} + \\lambda(1-\\lambda)^{2}x_{t-2} + \\dots + \\lambda(1-\\lambda)^{t-1}x_{1} + (1-\\lambda)^{t}\\hat{z}_{0} $$\nThis can be written as:\n$$ \\hat{z}_{t} = \\left( \\sum_{k=0}^{t-1} \\lambda(1-\\lambda)^{k}x_{t-k} \\right) + (1-\\lambda)^{t}\\hat{z}_{0} $$\nThe problem states that the total weight must sum to $1$. The sum of the weights on the observations is a finite geometric series:\n$$ \\sum_{k=0}^{t-1} \\lambda(1-\\lambda)^{k} = \\lambda \\sum_{k=0}^{t-1} (1-\\lambda)^{k} = \\lambda \\left( \\frac{1 - (1-\\lambda)^{t}}{1 - (1-\\lambda)} \\right) = \\lambda \\left( \\frac{1 - (1-\\lambda)^{t}}{\\lambda} \\right) = 1 - (1-\\lambda)^{t} $$\nThe total weight is the sum of the weights on the observations plus the weight on the initial value $\\hat{z}_0$, which is $(1-\\lambda)^t$.\nTotal Weight $= [1 - (1-\\lambda)^{t}] + (1-\\lambda)^{t} = 1$. The condition is satisfied.\n\nTo derive the recursive form, we start with the definition of $\\hat{z}_t$:\n$$ \\hat{z}_{t} = \\lambda x_{t} + \\lambda(1-\\lambda)x_{t-1} + \\lambda(1-\\lambda)^{2}x_{t-2} + \\dots + (1-\\lambda)^{t}\\hat{z}_{0} $$\nWe can factor out $(1-\\lambda)$ from all terms except the first:\n$$ \\hat{z}_{t} = \\lambda x_{t} + (1-\\lambda) \\left[ \\lambda x_{t-1} + \\lambda(1-\\lambda)x_{t-2} + \\dots + (1-\\lambda)^{t-1}\\hat{z}_{0} \\right] $$\nThe expression inside the brackets is, by definition, the EWMA statistic at time $t-1$, which is $\\hat{z}_{t-1}$.\n$$ \\hat{z}_{t-1} = \\lambda x_{t-1} + \\lambda(1-\\lambda)x_{t-2} + \\dots + \\lambda(1-\\lambda)^{t-2}x_1 + (1-\\lambda)^{t-1}\\hat{z}_{0} $$\nSubstituting this into the equation for $\\hat{z}_t$ yields the recursive form:\n$$ \\hat{z}_{t} = \\lambda x_{t} + (1-\\lambda)\\hat{z}_{t-1} $$\nThis is the standard recursive formula for the EWMA.\n\n**Part 2: Explanation of Appropriateness for Patient Safety Monitoring**\n\nThe EWMA is particularly appropriate for detecting small, persistent shifts in a process mean, which is often the pattern of interest in patient safety monitoring. A sudden, large spike in adverse events is easily detected by simple charts, but a small, sustained increase—perhaps due to a subtle systemic issue—may go unnoticed. The EWMA's mechanism is well-suited for this scenario for two key reasons:\n\n$1$. **Inertia and Smoothing**: The recursive formula $\\hat{z}_{t} = \\lambda x_{t} + (1-\\lambda)\\hat{z}_{t-1}$ shows that the current EWMA value is a weighted average of the newest data point $x_t$ and the previous EWMA value $\\hat{z}_{t-1}$. The parameter $\\lambda$ ($0 < \\lambda \\le 1$) controls the balance. For small-shift detection, a small $\\lambda$ (e.g., $\\lambda \\in [0.1, 0.3]$) is typically chosen. This gives more weight to the historical component $\\hat{z}_{t-1}$ (weight $1-\\lambda$) and less weight to the current observation $x_t$ (weight $\\lambda$). This property provides inertia, smoothing out random fluctuations or noise in the data. An isolated, random high value for an adverse event proportion will not cause a drastic jump in $\\hat{z}_t$, preventing false alarms.\n\n$2$. **Sensitivity to Sustained Shifts**: The \"memory\" of the process, captured in $\\hat{z}_{t-1}$, is what makes EWMA sensitive to small but persistent shifts. While a single new data point $x_t$ has a limited impact (scaled by $\\lambda$), if the underlying process mean shifts upwards, consecutive $x_t$ values will tend to be higher than the historical average. Each new higher value will pull the EWMA statistic $\\hat{z}_t$ slightly upwards. Over several time periods, this cumulative effect will cause $\\hat{z}_t$ to drift significantly from its baseline, eventually crossing a control limit and signaling a change. This is in contrast to a standard Shewhart chart, which only considers the current data point and has no memory, making it less effective at detecting small shifts that do not push any single point outside its wide control limits.\n\nThe geometric decay of weights ensures that the most recent data have the most influence, allowing the system to adapt to new conditions, while the aggregate influence of past data provides the stability and memory needed to reliably detect subtle but meaningful changes in patient safety metrics.\n\n**Part 3: Calculation of $\\hat{z}_{4}$**\n\nWe use the derived recursive formula $\\hat{z}_{t} = \\lambda x_{t} + (1-\\lambda)\\hat{z}_{t-1}$ with the given values:\n- Initial value: $\\hat{z}_{0}=0.02$\n- Smoothing parameter: $\\lambda=0.2$, which implies $1-\\lambda=0.8$.\n- Data points: $x_{1}=0.02$, $x_{2}=0.03$, $x_{3}=0.05$, $x_{4}=0.04$.\n\n**Step-by-step computation:**\n\n- **At $t=1$:**\n$$ \\hat{z}_{1} = \\lambda x_{1} + (1-\\lambda)\\hat{z}_{0} = (0.2)(0.02) + (0.8)(0.02) = 0.004 + 0.016 = 0.02 $$\n\n- **At $t=2$:**\n$$ \\hat{z}_{2} = \\lambda x_{2} + (1-\\lambda)\\hat{z}_{1} = (0.2)(0.03) + (0.8)(0.02) = 0.006 + 0.016 = 0.022 $$\n\n- **At $t=3$:**\n$$ \\hat{z}_{3} = \\lambda x_{3} + (1-\\lambda)\\hat{z}_{2} = (0.2)(0.05) + (0.8)(0.022) = 0.01 + 0.0176 = 0.0276 $$\n\n- **At $t=4$:**\n$$ \\hat{z}_{4} = \\lambda x_{4} + (1-\\lambda)\\hat{z}_{3} = (0.2)(0.04) + (0.8)(0.0276) = 0.008 + 0.02208 = 0.03008 $$\n\nThe value of the EWMA statistic at day $t=4$ is $0.03008$.", "answer": "$$\\boxed{0.03008}$$", "id": "4852115"}]}