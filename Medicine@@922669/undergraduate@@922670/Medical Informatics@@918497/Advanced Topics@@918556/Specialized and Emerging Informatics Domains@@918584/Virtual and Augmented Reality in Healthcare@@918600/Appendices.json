{"hands_on_practices": [{"introduction": "The effectiveness of any Virtual or Augmented Reality application, especially in medicine where visual precision is critical, begins with the quality of the display itself. We can quantify this quality by the angular resolution, which measures the angle subtended by a single pixel at the user's eye and determines the finest detail a surgeon or trainee can perceive. This exercise provides a first-principles approach to calculating this crucial metric, connecting physical display properties to the fundamental limits of visual perception in a head-mounted display. [@problem_id:4863089]", "problem": "A medical training system uses a head-mounted display (HMD) to deliver Virtual Reality (VR) surgical simulations. To reason about perceptual detail, assume a simple geometric eye model: a flat, rectangular microdisplay of active horizontal width subtends a horizontal field of view (FOV) at the eye, and the eye’s entrance pupil is located at a fixed vertex distance from the display. The horizontal FOV is the visual angle subtended by the active display width about the eye’s optical axis. Pixels are arranged on a uniform grid with constant pixel pitch. Using only trigonometric geometry and the definition of a visual angle, derive from first principles an expression for the average horizontal angular subtense per pixel (that is, the average horizontal angular resolution per pixel across the active width) in arcminutes. Then evaluate it for a pixel pitch of $10\\,\\mu\\text{m}$, a display-to-eye vertex distance of $40\\,\\text{mm}$, and a horizontal FOV of $100^{\\circ}$. \n\nAssumptions: \n- The microdisplay is flat and centered on the optical axis. \n- Pixels are densely packed with constant pitch and fully span the active width that produces the stated FOV. \n- The horizontal FOV equals the visual angle subtended by the active width of the display at the given vertex distance.\n\nCompute the final numerical value of the average horizontal angular resolution per pixel and express it in arcminutes. Round your answer to three significant figures.", "solution": "The user has provided a problem to be solved. As per the instructions, the first step is to validate the problem statement.\n\n### Step 1: Extract Givens\nThe givens extracted verbatim from the problem statement are:\n-   **Model**: A simple geometric eye model with a flat, rectangular microdisplay.\n-   **Pixel Pitch**: $p = 10 \\, \\mu\\text{m}$.\n-   **Vertex Distance**: $d = 40 \\, \\text{mm}$.\n-   **Horizontal Field of View**: $\\theta_{\\text{FOV}} = 100^{\\circ}$.\n-   **Target Quantity**: Average horizontal angular subtense per pixel, in arcminutes.\n-   **Assumptions**:\n    1.  The microdisplay is flat and centered on the optical axis.\n    2.  Pixels are densely packed with constant pitch and fully span the active width.\n    3.  The horizontal FOV equals the visual angle subtended by the active width of the display at the given vertex distance.\n-   **Task Requirements**:\n    1.  Derive an expression for the average horizontal angular subtense per pixel from first principles using trigonometric geometry.\n    2.  Evaluate the expression for the given numerical values.\n    3.  Round the final numerical answer to three significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n-   **Scientifically Grounded**: The problem is grounded in basic geometric optics. The model of a flat screen viewed from a fixed distance is a standard first-order approximation used in analyzing head-mounted displays. The physical quantities (pixel pitch, vertex distance, FOV) are within realistic ranges for current technology. The concepts of angular resolution and field of view are fundamental to display engineering and visual perception.\n-   **Well-Posed**: The problem is well-posed. It provides all the necessary data ($p$, $d$, $\\theta_{\\text{FOV}}$) and a clear set of assumptions to define the geometric relationship between these quantities. The objective is to derive a specific physical quantity and then compute its value, which is a standard structure for a physics or engineering problem. A unique solution exists.\n-   **Objective**: The language is precise, quantitative, and free of subjective or ambiguous terminology. The assumptions are explicitly stated, leaving no room for misinterpretation.\n\nThe problem does not exhibit any of the defined flaws:\n1.  **Scientific or Factual Unsoundness**: No violation of scientific principles.\n2.  **Non-Formalizable or Irrelevant**: The problem is readily formalizable using geometry.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and self-consistent.\n4.  **Unrealistic or Infeasible**: The parameters are physically plausible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-structured and leads to a unique solution.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a non-trivial derivation and calculation.\n7.  **Outside Scientific Verifiability**: The derivation and result are mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned solution will be provided.\n\n### Solution Derivation\nThe problem asks for the average horizontal angular subtense per pixel. This quantity, which we will denote as $\\alpha_{\\text{avg}}$, is defined as the total horizontal field of view divided by the total number of horizontal pixels.\n$$\n\\alpha_{\\text{avg}} = \\frac{\\theta_{\\text{FOV}}}{N}\n$$\nwhere $\\theta_{\\text{FOV}}$ is the horizontal field of view and $N$ is the number of pixels along the horizontal width of the display.\n\nOur first task is to find an expression for $N$. The number of pixels is the total active horizontal width of the display, $W$, divided by the pixel pitch, $p$.\n$$\nN = \\frac{W}{p}\n$$\nThe problem requires us to relate the width $W$ to the field of view $\\theta_{\\text{FOV}}$ and the vertex distance $d$. We use the provided geometric model. The display is flat and centered on the optical axis, with the eye's entrance pupil at a distance $d$ from the display. This forms an isosceles triangle with the eye at the apex, the display width $W$ as the base, and the distance $d$ as the altitude.\n\nThis geometry can be analyzed by considering one of the two right-angled triangles formed by bisecting the isosceles triangle with the altitude $d$. Each right-angled triangle has:\n-   An angle at the eye of $\\frac{\\theta_{\\text{FOV}}}{2}$.\n-   An opposite side of length $\\frac{W}{2}$.\n-   An adjacent side of length $d$.\n\nFrom the definition of the tangent function in a right-angled triangle:\n$$\n\\tan\\left(\\frac{\\theta_{\\text{FOV}}}{2}\\right) = \\frac{\\text{opposite}}{\\text{adjacent}} = \\frac{W/2}{d}\n$$\nWe can solve for the display width $W$:\n$$\nW = 2d \\tan\\left(\\frac{\\theta_{\\text{FOV}}}{2}\\right)\n$$\nNow we can find the total number of horizontal pixels, $N$, by substituting this expression for $W$:\n$$\nN = \\frac{W}{p} = \\frac{2d \\tan\\left(\\frac{\\theta_{\\text{FOV}}}{2}\\right)}{p}\n$$\nFinally, we substitute this expression for $N$ into our initial definition for the average angular subtense per pixel, $\\alpha_{\\text{avg}}$:\n$$\n\\alpha_{\\text{avg}} = \\frac{\\theta_{\\text{FOV}}}{N} = \\frac{\\theta_{\\text{FOV}}}{\\frac{2d \\tan\\left(\\frac{\\theta_{\\text{FOV}}}{2}\\right)}{p}}\n$$\nThis simplifies to the desired expression:\n$$\n\\alpha_{\\text{avg}} = \\frac{p \\cdot \\theta_{\\text{FOV}}}{2d \\tan\\left(\\frac{\\theta_{\\text{FOV}}}{2}\\right)}\n$$\nThis expression gives the average angular size of a pixel. Note that the units of $\\alpha_{\\text{avg}}$ will be the same as the units used for $\\theta_{\\text{FOV}}$ in the numerator, as the ratio $p/d$ is dimensionless and the trigonometric function is also dimensionless.\n\n### Numerical Evaluation\nWe are given the following values:\n-   $p = 10 \\, \\mu\\text{m} = 10 \\times 10^{-6} \\, \\text{m}$\n-   $d = 40 \\, \\text{mm} = 40 \\times 10^{-3} \\, \\text{m}$\n-   $\\theta_{\\text{FOV}} = 100^{\\circ}$\n\nFirst, we calculate the half-angle for the tangent function:\n$$\n\\frac{\\theta_{\\text{FOV}}}{2} = \\frac{100^{\\circ}}{2} = 50^{\\circ}\n$$\nNow, we substitute the values into the derived expression. We will calculate $\\alpha_{\\text{avg}}$ in degrees first.\n$$\n\\alpha_{\\text{avg}}[\\text{degrees}] = \\frac{(10 \\times 10^{-6} \\, \\text{m}) \\cdot 100^{\\circ}}{2 \\cdot (40 \\times 10^{-3} \\, \\text{m}) \\tan(50^{\\circ})}\n$$\n$$\n\\alpha_{\\text{avg}}[\\text{degrees}] = \\frac{10^{-3}}{8 \\times 10^{-2} \\tan(50^{\\circ})} = \\frac{1}{80 \\tan(50^{\\circ})}\n$$\nUsing the value $\\tan(50^{\\circ}) \\approx 1.19175359$:\n$$\n\\alpha_{\\text{avg}}[\\text{degrees}] \\approx \\frac{1}{80 \\cdot 1.19175359} \\approx \\frac{1}{95.340287} \\approx 0.01048873^{\\circ}\n$$\nThe problem requires the answer in arcminutes. We use the conversion factor $1^{\\circ} = 60 \\, \\text{arcmin}$.\n$$\n\\alpha_{\\text{avg}}[\\text{arcmin}] = \\alpha_{\\text{avg}}[\\text{degrees}] \\times 60\n$$\n$$\n\\alpha_{\\text{avg}}[\\text{arcmin}] \\approx 0.01048873 \\times 60 \\approx 0.6293238 \\, \\text{arcmin}\n$$\nFinally, we round the result to three significant figures as requested.\n$$\n\\alpha_{\\text{avg}} \\approx 0.629 \\, \\text{arcmin}\n$$\nThis value represents the average angle that a single pixel subtends at the eye. For reference, the acuity of the human eye is often cited as being around $1$ arcminute, so this resolution is in the ballpark of human perception limits under ideal conditions, although slightly coarser.", "answer": "$$\\boxed{0.629}$$", "id": "4863089"}, {"introduction": "In surgical applications, Augmented Reality enhances the surgeon's view by overlaying a 3D model of a patient's anatomy directly onto the live video feed. The accuracy of this model, which is generated from pre-operative medical scans through a process called segmentation, is paramount for patient safety. This practice introduces key performance metrics—the Dice coefficient, Jaccard index, precision, and recall—that quantitatively evaluate segmentation accuracy against a ground-truth reference. By calculating these metrics, you will learn to interpret the quality of an anatomical model and understand the critical clinical trade-offs between different types of segmentation errors. [@problem_id:4863080]", "problem": "An augmented reality (AR) surgical guidance system overlays a segmented anatomical structure—the left kidney capsule—onto live laparoscopic video to assist surgeons in maintaining safe margins during partial nephrectomy. In the medical informatics pipeline, segmentation masks are represented as binary sets of voxels. Let the predicted AR segmentation mask be the set $P$ and the ground-truth mask (manually curated by expert radiologists from preoperative computed tomography) be the set $G$. For a particular case, the following are observed from volumetric analysis:\n- The predicted mask contains $|P| = 110{,}000$ voxels.\n- The ground-truth mask contains $|G| = 120{,}000$ voxels.\n- The intersection contains $|P \\cap G| = 100{,}000$ voxels.\n\nUsing fundamental set-theoretic definitions of binary classification counts for segmentation, where true positives are overlapping voxels, false positives are predicted voxels not in the ground truth, and false negatives are ground-truth voxels not predicted, compute the following segmentation performance metrics for the overlay:\n- Dice coefficient,\n- Jaccard index,\n- precision,\n- recall.\n\nExpress each metric as a unitless decimal between $0$ and $1$ and round your results to four significant figures. Report your four metrics in the single final answer in the order: Dice, Jaccard, precision, recall. Additionally, in your reasoning, interpret the clinical relevance of these values for AR overlay safety and usability, starting from core definitions rather than shortcut formulas or pre-memorized results.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- The predicted AR segmentation mask is the set $P$.\n- The ground-truth mask is the set $G$.\n- The size of the predicted mask is $|P| = 110,000$ voxels.\n- The size of the ground-truth mask is $|G| = 120,000$ voxels.\n- The size of the intersection of the two masks is $|P \\cap G| = 100,000$ voxels.\n- Definitions for classification counts:\n  - True positives ($TP$): overlapping voxels.\n  - False positives ($FP$): predicted voxels not in the ground truth.\n  - False negatives ($FN$): ground-truth voxels not predicted.\n- Required computations: Dice coefficient, Jaccard index, precision, and recall.\n- Output format: Decimal rounded to four significant figures, reported in the order Dice, Jaccard, precision, recall.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It utilizes standard, formally defined concepts from set theory and applies them to the field of medical image analysis, a core component of medical informatics. The provided data are internally consistent; the size of the intersection, $|P \\cap G| = 100,000$, is correctly less than or equal to the sizes of the individual sets, $|P| = 110,000$ and $|G| = 120,000$. The problem is complete, as it furnishes all necessary data to compute the requested metrics. The voxel counts are realistic for a high-resolution medical scan of an anatomical substructure. There are no ambiguities, contradictions, or violations of scientific principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution Derivation\n\nThe problem requires the calculation of four performance metrics for a segmentation mask based on set-theoretic principles. We begin by formalizing the definitions of true positives ($TP$), false positives ($FP$), and false negatives ($FN$) using the provided sets $P$ (prediction) and $G$ (ground truth).\n\nA **true positive** ($TP$) is a voxel that is correctly identified as part of the kidney capsule. These are the voxels present in both the predicted mask $P$ and the ground-truth mask $G$. Therefore, the number of true positives is the cardinality of the intersection of the two sets.\n$$TP = |P \\cap G|$$\nUsing the given data, we have:\n$$TP = 100,000$$\n\nA **false positive** ($FP$) is a voxel that is incorrectly identified as part of the kidney capsule. These are the voxels present in the predicted mask $P$ but not in the ground-truth mask $G$. This corresponds to the set difference $P \\setminus G$. The number of false positives is the cardinality of this set difference, which can be calculated as:\n$$FP = |P \\setminus G| = |P| - |P \\cap G|$$\nSubstituting the given values:\n$$FP = 110,000 - 100,000 = 10,000$$\n\nA **false negative** ($FN$) is a voxel that belongs to the kidney capsule but was missed by the segmentation algorithm. These are the voxels present in the ground-truth mask $G$ but not in the predicted mask $P$. This corresponds to the set difference $G \\setminus P$. The number of false negatives is the cardinality of this set difference:\n$$FN = |G \\setminus P| = |G| - |P \\cap G|$$\nSubstituting the given values:\n$$FN = 120,000 - 100,000 = 20,000$$\n\nWith these fundamental quantities established, we can now define and compute each of the required metrics.\n\n**1. Dice Coefficient (Sørensen–Dice index)**\nThe Dice coefficient is a measure of spatial overlap between two sets. It is defined as twice the size of the intersection divided by the sum of the sizes of the two sets.\n$$Dice = \\frac{2 |P \\cap G|}{|P| + |G|}$$\nIn terms of $TP$, $FP$, and $FN$, we can express $|P| = TP + FP$ and $|G| = TP + FN$. Substituting these gives:\n$$Dice = \\frac{2 TP}{(TP + FP) + (TP + FN)} = \\frac{2 TP}{2 TP + FP + FN}$$\nUsing the given cardinalities:\n$$Dice = \\frac{2 \\times 100,000}{110,000 + 120,000} = \\frac{200,000}{230,000} = \\frac{20}{23}$$\nAs a decimal rounded to four significant figures:\n$$Dice \\approx 0.869565... \\approx 0.8696$$\n\n**2. Jaccard Index (Intersection over Union, IoU)**\nThe Jaccard index is another measure of set similarity, defined as the size of the intersection divided by the size of the union.\n$$Jaccard = \\frac{|P \\cap G|}{|P \\cup G|}$$\nThe cardinality of the union is given by the principle of inclusion-exclusion: $|P \\cup G| = |P| + |G| - |P \\cap G|$.\nIn terms of $TP$, $FP$, and $FN$, the union $|P \\cup G|$ represents all voxels flagged in at least one of the masks, which is $TP + FP + FN$.\n$$Jaccard = \\frac{TP}{TP + FP + FN}$$\nUsing the calculated values:\n$$Jaccard = \\frac{100,000}{100,000 + 10,000 + 20,000} = \\frac{100,000}{130,000} = \\frac{10}{13}$$\nAs a decimal rounded to four significant figures:\n$$Jaccard \\approx 0.769230... \\approx 0.7692$$\n\n**3. Precision (Positive Predictive Value)**\nPrecision answers the question: \"Of all the voxels a system predicted to be target tissue, what fraction were actually target tissue?\" It is the ratio of true positives to the total number of predicted positives. The total number of predicted positives is the size of the predicted set, $|P|$.\n$$Precision = \\frac{TP}{|P|} = \\frac{TP}{TP + FP}$$\nUsing the calculated values:\n$$Precision = \\frac{100,000}{110,000} = \\frac{10}{11}$$\nAs a decimal rounded to four significant figures:\n$$Precision \\approx 0.909090... \\approx 0.9091$$\n\n**4. Recall (Sensitivity, True Positive Rate)**\nRecall answers the question: \"Of all the voxels that are truly target tissue, what fraction did the system correctly identify?\" It is the ratio of true positives to the total number of actual positives. The total number of actual positives is the size of the ground-truth set, $|G|$.\n$$Recall = \\frac{TP}{|G|} = \\frac{TP}{TP + FN}$$\nUsing the calculated values:\n$$Recall = \\frac{100,000}{120,000} = \\frac{10}{12} = \\frac{5}{6}$$\nAs a decimal rounded to four significant figures:\n$$Recall \\approx 0.833333... \\approx 0.8333$$\n\n### Clinical Relevance of Metrics\n\n- **Dice ($0.8696$) and Jaccard ($0.7692$):** These metrics give a global sense of the overlay's geometric accuracy. A Dice score of approximately $0.87$ indicates a good-to-excellent spatial overlap. For an AR surgical guide, this is crucial for surgeon trust and usability. A low score would imply a poorly shaped or misaligned overlay, rendering it useless or even dangerous.\n- **Precision ($0.9091$):** This is a critical safety metric. High precision means the AR overlay has a low rate of false positives ($FP=10,000$). When the system highlights a region, a surgeon can be confident ($90.91\\%$ in this case) that it is indeed the target tissue. This minimizes the risk of iatrogenic damage to healthy, non-target structures that might have been incorrectly included in the overlay.\n- **Recall ($0.8333$):** This metric relates to the completeness of the overlay. A recall of $0.8333$ means the system successfully identified $83.33\\%$ of the true kidney capsule but missed $16.67\\%$ ($FN=20,000$ voxels). For a partial nephrectomy, where a key goal is to preserve maximal healthy kidney tissue, these false negatives are significant. The surgeon, relying on the overlay, might cut closer to the actual kidney than intended, believing they have a safe margin in an area where the overlay is absent. This could compromise the renopreservation goal of the surgery.\n\nIn summary, the AR system is quite precise, reducing the risk of cutting outside the target, but its recall indicates it is not perfectly complete, creating a risk of excising too close to the true boundary. The high Dice coefficient suggests it is a very useful guide overall, but the surgeon must remain aware of its potential to under-represent the full extent of the anatomical structure.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8696 & 0.7692 & 0.9091 & 0.8333 \\end{pmatrix}}\n$$", "id": "4863080"}, {"introduction": "After creating an accurate 3D model, an AR system must precisely align or \"register\" it with the real world in real-time. The accuracy of this registration is the cornerstone of a trustworthy surgical navigation system. This exercise guides you through quantifying registration accuracy using the root-mean-square (RMS) reprojection error, a standard method for validating tracking algorithms. You will apply the pinhole camera model to project 3D world points into 2D image coordinates and measure the discrepancy from their observed locations, a fundamental skill in computer vision for AR. [@problem_id:4863100]", "problem": "A surgical navigation system for Augmented Reality (AR) overlays relies on a calibrated monocular camera to register a fiducial marker attached to a patient. During a validation step, you are given the extrinsic pose estimate and the camera intrinsics, and you collect detected two-dimensional image coordinates of fiducial points. Assuming the standard pinhole camera model with no lens distortion, compute the root-mean-square (RMS) reprojection error across all correspondences.\n\nUse the following data:\n\n- Camera intrinsics (in pixels): $f_x = 1200$, $f_y = 1200$, $c_x = 640$, $c_y = 360$.\n- Camera pose (rotation and translation from world to camera coordinates):\n$$\nR = \\begin{pmatrix}\n0 & -1 & 0 \\\\\n1 & \\phantom{-}0 & 0 \\\\\n0 & \\phantom{-}0 & 1\n\\end{pmatrix}, \\quad\nt = \\begin{pmatrix}\n0.02 \\\\\n-0.01 \\\\\n0.50\n\\end{pmatrix} \\text{ meters.}\n$$\n- Fiducial three-dimensional world points (in meters):\n$\\mathbf{X}_1 = (0.10, 0.00, 0.00)$,\n$\\mathbf{X}_2 = (0.00, 0.10, 0.00)$,\n$\\mathbf{X}_3 = (-0.10, 0.00, 0.00)$,\n$\\mathbf{X}_4 = (0.00, -0.10, 0.00)$.\n- Detected two-dimensional image coordinates (in pixels):\n$\\mathbf{u}_1^{\\text{meas}} = (690, 575)$,\n$\\mathbf{u}_2^{\\text{meas}} = (447, 338)$,\n$\\mathbf{u}_3^{\\text{meas}} = (685, 100)$,\n$\\mathbf{u}_4^{\\text{meas}} = (931, 334)$.\n\nAdopt the following definitions grounded in standard camera geometry. Transform each three-dimensional world point $\\mathbf{X}_w$ to camera coordinates by $\\mathbf{X}_c = R \\mathbf{X}_w + t$. Then project to the image plane using normalized coordinates $(x_n, y_n) = (X_c/Z_c, Y_c/Z_c)$ and pixel coordinates $(u, v) = (f_x x_n + c_x, f_y y_n + c_y)$. For each correspondence, define the reprojection error as the Euclidean distance in pixels between the projected pixel coordinate and the measured pixel coordinate. The RMS reprojection error across $N$ correspondences is the square root of the mean of the squared Euclidean distances.\n\nCompute the RMS reprojection error using the data above. Express the final answer in pixels and round your answer to four significant figures.", "solution": "The problem is first validated against the established criteria.\n\n### Step 1: Extract Givens\n\n- Camera intrinsics (in pixels): $f_x = 1200$, $f_y = 1200$, $c_x = 640$, $c_y = 360$.\n- Camera pose (rotation and translation from world to camera coordinates):\n$$\nR = \\begin{pmatrix}\n0 & -1 & 0 \\\\\n1 & \\phantom{-}0 & 0 \\\\\n0 & \\phantom{-}0 & 1\n\\end{pmatrix}, \\quad\nt = \\begin{pmatrix}\n0.02 \\\\\n-0.01 \\\\\n0.50\n\\end{pmatrix} \\text{ meters.}\n$$\n- Fiducial three-dimensional world points (in meters):\n$\\mathbf{X}_1 = (0.10, 0.00, 0.00)$,\n$\\mathbf{X}_2 = (0.00, 0.10, 0.00)$,\n$\\mathbf{X}_3 = (-0.10, 0.00, 0.00)$,\n$\\mathbf{X}_4 = (0.00, -0.10, 0.00)$.\n- Detected two-dimensional image coordinates (in pixels):\n$\\mathbf{u}_1^{\\text{meas}} = (690, 575)$,\n$\\mathbf{u}_2^{\\text{meas}} = (447, 338)$,\n$\\mathbf{u}_3^{\\text{meas}} = (685, 100)$,\n$\\mathbf{u}_4^{\\text{meas}} = (931, 334)$.\n- Definitions:\n  - World to camera transformation: $\\mathbf{X}_c = R \\mathbf{X}_w + t$.\n  - Projection to normalized image coordinates: $(x_n, y_n) = (X_c/Z_c, Y_c/Z_c)$.\n  - Projection to pixel coordinates: $(u, v) = (f_x x_n + c_x, f_y y_n + c_y)$.\n  - Reprojection error: Euclidean distance between projected and measured coordinates.\n  - RMS reprojection error: $E_{RMS} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} ||\\mathbf{u}_i^{\\text{proj}} - \\mathbf{u}_i^{\\text{meas}}||^2}$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is based on the standard pinhole camera model, a fundamental and widely-used principle in computer vision and photogrammetry. The transformations and projection equations are correct and standard in the field. The provided values for camera intrinsics, pose, and point coordinates are physically plausible. The context of AR-based surgical navigation is a valid and significant application area in medical informatics.\n- **Well-Posed**: The problem provides a complete set of data and a clear, unambiguous set of instructions to compute a single numerical value. All variables are defined, and the goal is explicit. A unique solution exists and can be determined by following the specified procedure.\n- **Objective**: The problem is stated using precise, quantitative, and unbiased language. There are no subjective or opinion-based statements.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria (e.g., scientific unsoundness, incompleteness, ambiguity).\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. A complete solution will be provided.\n\nThe root-mean-square (RMS) reprojection error is calculated by projecting each three-dimensional world point onto the image plane using the given camera model and extrinsics, and then comparing these projected coordinates to the measured coordinates.\n\nThe projection process involves two steps for each world point $\\mathbf{X}_w = (X_w, Y_w, Z_w)^T$:\n1.  Transform the point from world coordinates to camera coordinates $\\mathbf{X}_c = (X_c, Y_c, Z_c)^T$ using the rotation matrix $R$ and translation vector $t$:\n    $$ \\mathbf{X}_c = R \\mathbf{X}_w + t $$\n2.  Project the point from camera coordinates to pixel coordinates $\\mathbf{u}^{\\text{proj}} = (u, v)^T$ using the camera intrinsic parameters $f_x, f_y, c_x, c_y$:\n    $$ u = f_x \\frac{X_c}{Z_c} + c_x $$\n    $$ v = f_y \\frac{Y_c}{Z_c} + c_y $$\n\nThe squared reprojection error for the $i$-th point is the squared Euclidean distance between the projected point $\\mathbf{u}_i^{\\text{proj}} = (u_i, v_i)^T$ and the measured point $\\mathbf{u}_i^{\\text{meas}} = (u_i^{\\text{meas}}, v_i^{\\text{meas}})^T$:\n$$ e_i^2 = ||\\mathbf{u}_i^{\\text{proj}} - \\mathbf{u}_i^{\\text{meas}}||^2 = (u_i - u_i^{\\text{meas}})^2 + (v_i - v_i^{\\text{meas}})^2 $$\n\nThe RMS reprojection error for $N$ points is:\n$$ E_{\\text{RMS}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} e_i^2} $$\n\nWe now apply this procedure for each of the $N=4$ fiducial points.\n\n**Point 1:** $\\mathbf{X}_1 = (0.10, 0.00, 0.00)^T$\n- Camera coordinates:\n$$ \\mathbf{X}_{c,1} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.10 \\\\ 0.00 \\\\ 0.00 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.10 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.09 \\\\ 0.50 \\end{pmatrix} $$\n- Projected pixel coordinates:\n$u_1 = 1200 \\times \\frac{0.02}{0.50} + 640 = 1200 \\times 0.04 + 640 = 48 + 640 = 688$\n$v_1 = 1200 \\times \\frac{0.09}{0.50} + 360 = 1200 \\times 0.18 + 360 = 216 + 360 = 576$\nSo, $\\mathbf{u}_1^{\\text{proj}} = (688, 576)$.\n- Squared error: $\\mathbf{u}_1^{\\text{meas}} = (690, 575)$\n$e_1^2 = (688 - 690)^2 + (576 - 575)^2 = (-2)^2 + 1^2 = 4 + 1 = 5$\n\n**Point 2:** $\\mathbf{X}_2 = (0.00, 0.10, 0.00)^T$\n- Camera coordinates:\n$$ \\mathbf{X}_{c,2} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.00 \\\\ 0.10 \\\\ 0.00 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} -0.10 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} -0.08 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} $$\n- Projected pixel coordinates:\n$u_2 = 1200 \\times \\frac{-0.08}{0.50} + 640 = 1200 \\times (-0.16) + 640 = -192 + 640 = 448$\n$v_2 = 1200 \\times \\frac{-0.01}{0.50} + 360 = 1200 \\times (-0.02) + 360 = -24 + 360 = 336$\nSo, $\\mathbf{u}_2^{\\text{proj}} = (448, 336)$.\n- Squared error: $\\mathbf{u}_2^{\\text{meas}} = (447, 338)$\n$e_2^2 = (448 - 447)^2 + (336 - 338)^2 = 1^2 + (-2)^2 = 1 + 4 = 5$\n\n**Point 3:** $\\mathbf{X}_3 = (-0.10, 0.00, 0.00)^T$\n- Camera coordinates:\n$$ \\mathbf{X}_{c,3} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} -0.10 \\\\ 0.00 \\\\ 0.00 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -0.10 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ -0.11 \\\\ 0.50 \\end{pmatrix} $$\n- Projected pixel coordinates:\n$u_3 = 1200 \\times \\frac{0.02}{0.50} + 640 = 1200 \\times 0.04 + 640 = 48 + 640 = 688$\n$v_3 = 1200 \\times \\frac{-0.11}{0.50} + 360 = 1200 \\times (-0.22) + 360 = -264 + 360 = 96$\nSo, $\\mathbf{u}_3^{\\text{proj}} = (688, 96)$.\n- Squared error: $\\mathbf{u}_3^{\\text{meas}} = (685, 100)$\n$e_3^2 = (688 - 685)^2 + (96 - 100)^2 = 3^2 + (-4)^2 = 9 + 16 = 25$\n\n**Point 4:** $\\mathbf{X}_4 = (0.00, -0.10, 0.00)^T$\n- Camera coordinates:\n$$ \\mathbf{X}_{c,4} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.00 \\\\ -0.10 \\\\ 0.00 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} 0.10 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.02 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} = \\begin{pmatrix} 0.12 \\\\ -0.01 \\\\ 0.50 \\end{pmatrix} $$\n- Projected pixel coordinates:\n$u_4 = 1200 \\times \\frac{0.12}{0.50} + 640 = 1200 \\times 0.24 + 640 = 288 + 640 = 928$\n$v_4 = 1200 \\times \\frac{-0.01}{0.50} + 360 = 1200 \\times (-0.02) + 360 = -24 + 360 = 336$\nSo, $\\mathbf{u}_4^{\\text{proj}} = (928, 336)$.\n- Squared error: $\\mathbf{u}_4^{\\text{meas}} = (931, 334)$\n$e_4^2 = (928 - 931)^2 + (336 - 334)^2 = (-3)^2 + 2^2 = 9 + 4 = 13$\n\nFinally, we compute the RMS reprojection error:\n- Sum of squared errors:\n$$ \\sum_{i=1}^{4} e_i^2 = e_1^2 + e_2^2 + e_3^2 + e_4^2 = 5 + 5 + 25 + 13 = 48 $$\n- Mean squared error (MSE):\n$$ \\text{MSE} = \\frac{1}{4} \\sum_{i=1}^{4} e_i^2 = \\frac{48}{4} = 12 $$\n- Root-mean-square (RMS) error:\n$$ E_{\\text{RMS}} = \\sqrt{\\text{MSE}} = \\sqrt{12} $$\nTo provide a numerical answer, we compute the value and round to four significant figures.\n$$ E_{\\text{RMS}} = \\sqrt{12} \\approx 3.4641016... $$\nRounding to four significant figures gives $3.464$. The units are pixels.", "answer": "$$\\boxed{3.464}$$", "id": "4863100"}]}