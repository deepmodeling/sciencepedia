## Introduction
The widespread adoption of Electronic Health Records (EHRs) has unlocked an unprecedented opportunity to generate clinical evidence from data collected during routine patient care. This practice, known as generating Real-World Evidence (RWE), promises to complement the findings from traditional Randomized Controlled Trials (RCTs) by providing insights into treatment effectiveness across broader, more diverse patient populations. However, this promise is met with a significant challenge: EHR data is not collected for research. It is a complex byproduct of clinical workflows, administrative processes, and billing requirements, making it inherently messy, incomplete, and susceptible to [systematic errors](@entry_id:755765). Without a deep understanding of these limitations, researchers risk producing biased and unreliable findings.

This article provides a comprehensive guide to navigating the complexities of using EHR data for research. It is designed to equip you with the knowledge and skills to transform raw, observational data into robust, credible evidence. In the following chapters, you will embark on a structured journey from theory to practice. The journey begins with **Principles and Mechanisms**, where we dissect the fundamental nature of EHR data, from assessing its quality to understanding the standardized vocabularies and data models that enable large-scale analysis. This chapter will also arm you with a clear [taxonomy](@entry_id:172984) of the biases—confounding, selection, and information bias—that can threaten the validity of observational research. Next, **Applications and Interdisciplinary Connections** transitions from theory to execution, demonstrating how to design valid observational studies by emulating target trials and using active-comparator designs. We will explore the statistical machinery, including propensity scores and instrumental variables, used to control for bias and make causal claims. Finally, **Hands-On Practices** provides an opportunity to apply these concepts, guiding you through practical challenges in [data transformation](@entry_id:170268), cohort phenotyping, and avoiding critical analytical errors like immortal time bias. By mastering these components, you will be prepared to harness the power of real-world data to contribute to a Learning Health System.

## Principles and Mechanisms

The use of routinely collected data from Electronic Health Records (EHRs) to generate clinical evidence—a practice central to the field of Real-World Evidence (RWE)—presents both profound opportunities and significant methodological challenges. Unlike data from a Randomized Controlled Trial (RCT), which are generated under a controlled experimental protocol, EHR data are an artifact of clinical care, administrative processes, and billing requirements. Understanding the principles and mechanisms that govern the generation and interpretation of this data is paramount for any researcher seeking to produce valid and reliable scientific findings. This chapter elucidates these core principles, from the fundamental nature of EHR data to the analytical and ethical frameworks required for its responsible use.

### The Nature and Quality of EHR Data for Research

At the heart of RWE is the distinction between evidence generated from routine practice and evidence from controlled experiments. While RCTs are the gold standard for establishing causal efficacy due to their use of randomization, their findings may have limited **external validity**, or generalizability. The strict inclusion and exclusion criteria of trials often create a study population that is younger, healthier, and has fewer comorbidities than the heterogeneous patient populations seen in routine clinical care. Conversely, RWE derived from EHRs, disease registries, or administrative claims data often possesses strong external validity precisely because it reflects this real-world complexity. However, this strength comes at a cost to **internal validity**—the degree to which a study can claim to have identified a causal effect. Because treatment assignment is not randomized, RWE studies are susceptible to a host of biases, most notably confounding, which must be addressed through statistical methods [@problem_id:4744803].

This fundamental trade-off is rooted in the distinction between the **primary use** and **secondary use** of health data. EHR data are captured for the primary use of supporting individual clinical care, documentation, and billing. Their secondary use for research, quality measurement, or policy analysis is an opportunistic repurposing of this existing information. This has critical implications for **construct validity**, which is the degree to which a measured variable in a study accurately reflects the theoretical concept it is intended to represent [@problem_id:4853677]. For example, a diagnosis code for diabetes entered for billing purposes may not perfectly capture the clinical construct of having diabetes, as it can be subject to errors, delays, or administrative incentives. When a clinician uses the EHR for primary care, they can integrate multiple data points and use their own judgment to interpret the data for an individual patient, mitigating some of the limitations of any single data element. In secondary analysis, however, the researcher must take the data as recorded, and these imperfections can become sources of systematic error and bias in a population-level study.

#### Dimensions of Data Quality

The fitness of EHR data for research depends on its quality, which can be systematically assessed across several dimensions. Understanding these dimensions allows researchers to anticipate potential sources of error. Key dimensions include [@problem_id:4862778]:

*   **Completeness**: The degree to which required data elements are present. For instance, if smoking status, a critical cardiovascular risk factor, is missing for $25\%$ of patient visits, the dataset has a significant completeness problem for a study on antihypertensive effectiveness.

*   **Accuracy**: The closeness of a recorded value to the true, underlying clinical state. Accuracy can be compromised by **measurement error**, which arises during the process of data generation. A systolic blood pressure reading of $160$ mmHg taken while a patient is speaking is likely less accurate than a subsequent reading of $138$ mmHg taken under proper protocol, as the patient's state violated the measurement procedure.

*   **Consistency**: Adherence to uniform formats, units, and value sets across the dataset. A common issue is recording the same laboratory analyte with different units, such as serum potassium appearing as $5.8$ “mmol/L” in one record and $5.8$ “mEq/L” in another. While numerically equivalent for potassium, this representational inconsistency can break automated data processing pipelines.

*   **Plausibility**: The clinical and biological reasonableness of data values. A record for an adult patient with a height of $180$ cm and a weight of $12$ kg would be flagged as a plausibility violation, as this combination is incompatible with life and almost certainly indicates a data entry error.

*   **Timeliness**: The degree to which data are recorded at or near the time of the event and are available for use in a suitable timeframe. A $7$-hour gap between a medication order and its documented administration may be perfectly timely from a data-entry perspective, but it reveals a significant delay in the care process itself, which is a crucial finding for any study of care delivery.

It is also vital to distinguish **measurement error** from **coding error**. As noted, measurement error occurs at the point of data generation (e.g., from a faulty device or improper patient positioning). Coding error, in contrast, occurs during data abstraction or entry, such as when a clinician's note documenting Type 1 diabetes is incorrectly mapped to an International Classification of Diseases (ICD) code for Type 2 diabetes [@problem_id:4862778]. Both are threats to [data quality](@entry_id:185007), but they originate at different points in the data lifecycle.

### Structuring Data for Analysis: Semantics and Schemas

Raw EHR data, with its local codes and idiosyncratic structures, cannot typically be analyzed across multiple institutions without significant harmonization. This process occurs at two levels: semantic (the meaning of data) and structural (the organization of data).

#### Semantic Interoperability: Vocabularies and Ontologies

To ensure that a "myocardial infarction" at one hospital means the same thing as at another, researchers rely on standard terminologies and classifications. It is essential to distinguish between these:

*   A **classification**, like the **International Classification of Diseases, 10th Revision, Clinical Modification (ICD-10-CM)**, groups related concepts into categories. ICD is optimized for administrative use cases like billing and morbidity statistics. Its structure is typically **monohierarchical**, with each code residing in a single chapter lineage, and it has relatively coarse granularity.

*   A **terminology** or **ontology**, like the **Systematized Nomenclature of Medicine—Clinical Terms (SNOMED CT)**, is a comprehensive collection of clinical concepts with formal, machine-readable definitions and relationships. SNOMED CT is **polyhierarchical**, meaning a concept can have multiple parent concepts (e.g., "viral pneumonia" is-a "pneumonia" and is-a "viral disease"). Its high granularity and logical structure make it ideal for detailed clinical applications like computable phenotyping.

Other key standards address specific data domains [@problem_id:4862773]:
*   **Logical Observation Identifiers Names and Codes (LOINC)** is the standard for identifying laboratory tests and clinical observations. A LOINC code standardizes the "question" (e.g., "what is the concentration of serum glucose?"), not the "answer" (e.g., $120$ mg/dL). It is indispensable for harmonizing lab data across institutions.
*   **RxNorm** is a normalized vocabulary for clinical drugs in the United States. It provides standard names and codes for medications based on their active ingredient, strength, and dose form, linking brand names, generic names, and specific packaged products. It is essential for pharmacoepidemiology and drug safety research.

#### Structural Interoperability: Common Data Models

Beyond standardizing codes, multi-site research requires a common [data structure](@entry_id:634264), or a **Common Data Model (CDM)**. A CDM is a standardized database schema into which local data are transformed, enabling the execution of the same analysis code across different sites. This approach is central to achieving **analytic interoperability**. The most prominent example is the **Observational Medical Outcomes Partnership (OMOP) Common Data Model**, which uses a relational schema of person-centric tables (e.g., `CONDITION_OCCURRENCE`, `DRUG_EXPOSURE`) and mandates the mapping of local codes to the standardized vocabularies described above [@problem_id:4862777].

This approach contrasts with **Fast Healthcare Interoperability Resources (FHIR)**, an HL7 standard designed primarily for **exchange interoperability**. FHIR defines data as discrete "Resources" (e.g., a `Patient` resource, an `Observation` resource) that are exchanged via Application Programming Interfaces (APIs). While FHIR provides the framework for semantic consistency, it does not mandate a single analytic schema. Achieving research-grade [reproducibility](@entry_id:151299) using FHIR typically requires an additional layer of harmonization, where all sites agree to implement a common set of constraints (called "profiles") on the resources. In essence, OMOP prioritizes making the data ready for network analytics, while FHIR prioritizes making the data ready for transactional exchange.

### Core Challenges to Causal Inference with EHR Data

Even with high-quality, harmonized data, the observational nature of EHRs creates significant hurdles for drawing valid causal conclusions. Bias, or [systematic error](@entry_id:142393), can be introduced in numerous ways. The three canonical types of bias in epidemiology are confounding, selection bias, and information bias.

#### A Taxonomy of Bias in EHR Research

Understanding the specific ways these biases manifest in EHR data is a critical skill for any researcher [@problem_id:4862759].

*   **Confounding** occurs when a third variable is a common cause of both the exposure and the outcome, creating a spurious association. The classic example is **confounding by indication**, where the disease that prompts the prescription of a drug is also a risk factor for the outcome of interest. For example, if an unmeasured comorbidity increases both the likelihood of receiving a specific medication and the risk of hospitalization, a naive analysis could wrongly attribute the increased hospitalization risk to the medication.

*   **Selection Bias** arises when the process of selecting subjects into the analysis induces a spurious association. A common source in EHR research is conditioning on healthcare utilization. For instance, if a study is restricted to patients with at least one clinic visit in the past year, and both the exposure (e.g., a medication) and the outcome (e.g., a disease) increase the probability of having a visit, the analysis conditions on a **[collider](@entry_id:192770)**. This can create a distorted association between the exposure and outcome within the selected group of frequent care-seekers.

*   **Information Bias** stems from systematic error in the measurement of exposure, outcome, or other variables. EHR data are particularly prone to this. For example, if documentation improvement initiatives focus on patients receiving a new drug, the data quality for that group may improve, leading to a systematic difference in how outcomes are recorded between the exposed and unexposed groups. This is a form of information bias arising from **coding practices**.

#### Information Bias: The Problem of Imperfect Measurement

Information bias is pervasive because EHR data are rarely perfect representations of clinical truth. A common form is **outcome misclassification**, where the recorded outcome ($Y^{*}$) differs from the true outcome ($Y$). This misclassification can be:

*   **Nondifferential**, meaning the probability of misclassification is the same for both exposed and unexposed individuals. Formally, the sensitivity ($Se = P(Y^{*}=1 \mid Y=1)$) and specificity ($Sp = P(Y^{*}=0 \mid Y=0)$) of the outcome classifier are independent of exposure status. For a binary outcome, nondifferential misclassification (provided $Se+Sp>1$) generally biases the estimated risk ratio or odds ratio **toward the null value of 1**, attenuating the true effect [@problem_id:4862756]. For example, if a true risk ratio is $3.0$, a nondifferential classifier with $Se=0.80$ and $Sp=0.90$ could lead to an observed risk ratio of approximately $1.82$, underestimating the true association.

*   **Differential**, meaning the probability of misclassification *does* depend on exposure status. A classic example is **surveillance bias** or **detection bias**. If patients who initiate a new medication ($X$) are monitored more closely and receive more laboratory tests, they have a higher chance of having an outcome ($Y$) detected, even if the medication has no true effect on the outcome's incidence. In this case, the sensitivity of outcome detection is higher in the exposed group, leading to differential misclassification that can create a spurious association where none exists [@problem_id:4862759] [@problem_id:4862756].

#### Selection Bias: Immortal Time and Other Pitfalls

While many forms of selection bias exist, **immortal time bias** is a particularly common and subtle error in EHR-based cohort studies. It is a form of misclassification and selection bias that arises when the definition of exposure is based on information that occurs after follow-up begins [@problem_id:4862763].

Consider a study where patients are classified as "exposed" if they fill a prescription within 30 days of a baseline clinic visit, and their follow-up is counted from that baseline visit. By this definition, to be in the "exposed" group, a patient must survive without the outcome until they fill the prescription. This pre-initiation period is "immortal" because an event during this time would have a patient from being classified as exposed. Incorrectly classifying this immortal, event-free time as "exposed" artificially dilutes the incidence rate in the exposed group, creating a spurious appearance of a protective effect. For example, in a hypothetical scenario where a drug has no true effect (true hazard ratio of $1.0$), this flawed analysis could yield an apparent [rate ratio](@entry_id:164491) of $0.89$, wrongly suggesting the drug is beneficial. This is distinct from **time-dependent confounding**, where a factor that varies over time is a common cause of subsequent exposure and outcome. Immortal time bias is a structural error in study design and analysis, not a confounding problem.

#### The Pervasive Challenge of Missing Data

Data are rarely complete in EHRs. The reasons why data are missing are crucial, as they determine whether the missingness will introduce bias. The three mechanisms are [@problem_id:4862807]:

*   **Missing Completely At Random (MCAR)**: The probability of a value being missing is independent of any observed or unobserved data. This is rare in EHR data.
*   **Missing At Random (MAR)**: The probability of a value being missing depends only on *observed* data. For example, if blood pressure is systematically measured at in-person visits but not telemedicine visits, and visit type is recorded, the missingness of blood pressure is MAR. The reason for missingness is known and can be accounted for.
*   **Missing Not At Random (MNAR)**: The probability of a value being missing depends on the *unobserved* value itself. This is a common and difficult problem in EHR research. For instance, a clinician may be more likely to order a Hemoglobin A1c test if they suspect a patient has uncontrolled diabetes based on symptoms or other unrecorded factors. This means patients with high, unobserved A1c values are more likely to have the test performed. The missingness is related to the very value that is missing, which can lead to significant bias if not handled properly.

A key takeaway is that clinical workflows create **heterogeneous missingness mechanisms**; within a single EHR dataset, some variables may be MAR while others are MNAR, requiring a nuanced, variable-by-variable approach to analysis.

### Ensuring Rigor and Responsibility in RWE Generation

Generating trustworthy RWE requires more than just advanced statistical techniques; it demands a commitment to transparency, reproducibility, and ethical conduct.

#### Data Provenance and the Foundation of Reproducibility

For science to be verifiable, it must be reproducible. In computational science, this means that another researcher, given the same raw data and code, should be able to produce the same results. This is impossible without meticulous documentation of **[data provenance](@entry_id:175012)**—the history of a data element's origin, context, and transformations—and **data lineage**, the chronological record of processing steps from raw input to analytic output [@problem_id:4862752].

Imagine a multi-site study where, halfway through the study period, one site changes its algorithm for summarizing blood pressure from a 7-day average to a 3-day average. If this change is not documented in the metadata (i.e., the provenance is incomplete), the analyst is unknowingly mixing data from two different measurement processes. This **[non-stationarity](@entry_id:138576)** violates the assumptions of most statistical models and can lead to biased estimates. Furthermore, because the transformation steps are unknown, no independent investigator could ever hope to reconstruct the analytic dataset from the raw data, rendering the findings irreproducible and undermining their scientific credibility.

#### Ethical and Regulatory Imperatives

Finally, research using data derived from individuals carries profound ethical responsibilities. The Belmont Report's principles provide a guiding framework [@problem_id:4862817]:

*   **Respect for Persons** acknowledges individual autonomy and demands informed consent. In large-scale EHR research where contacting tens of thousands of individuals is impracticable, this principle can be upheld through a **waiver of informed consent**, which is permitted by regulations like the U.S. Common Rule. An Institutional Review Board (IRB) may grant such a waiver if the research involves no more than minimal risk, the waiver will not adversely affect the rights and welfare of the subjects, and the research could not practicably be carried out without it. This typically applies to purely retrospective analyses within a secure environment. However, if researchers plan to recontact patients for a new activity, such as a survey, obtaining consent is generally considered practicable and therefore required.

*   **Beneficence** requires maximizing potential benefits while minimizing potential harms. For EHR research, the primary benefit is the generation of new knowledge to improve public health. The primary harm is the risk of a breach of confidentiality. This principle obligates researchers to implement robust technical and administrative safeguards, such as secure computing enclaves, access controls, and data use agreements, to ensure a favorable risk-benefit balance.

*   **Justice** concerns the fair distribution of the burdens and benefits of research. This means that subject selection must be equitable. Excluding a group, such as undocumented immigrants, for reasons of convenience (e.g., anticipated difficulty with follow-up) is a violation of justice. Conversely, deliberately **[oversampling](@entry_id:270705)** underrepresented minority groups to ensure that findings are statistically robust and generalizable to those populations is a positive application of the justice principle, as it helps ensure they share in the benefits of the research.

By integrating these technical, methodological, and ethical principles, researchers can harness the power of EHR data to generate real-world evidence that is not only scientifically valid but also transparent, reproducible, and worthy of public trust.