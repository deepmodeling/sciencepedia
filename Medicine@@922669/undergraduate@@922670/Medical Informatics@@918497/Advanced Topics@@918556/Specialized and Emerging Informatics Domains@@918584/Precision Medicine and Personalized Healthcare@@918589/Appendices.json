{"hands_on_practices": [{"introduction": "Precision medicine is built upon the analysis of high-dimensional data, such as sequencing the expression of thousands of genes to find associations with disease. This practice addresses a fundamental statistical challenge in that process: the problem of multiple testing. By testing many hypotheses at once, you dramatically increase the odds of finding false positives by pure chance. This exercise will guide you through the application of the Benjamini-Hochberg procedure, a powerful method for controlling the False Discovery Rate (FDR), allowing you to sift through a mountain of data to find statistically robust candidate biomarkers. [@problem_id:4852814]", "problem": "A precision oncology study uses Ribonucleic Acid sequencing (RNA-seq) to profile gene expression in patient tumors to identify genes whose expression levels are associated with response to a targeted therapy. Testing each gene independently yields many hypothesis tests. In this high-dimensional setting, explain why performing many hypothesis tests increases the chance of making at least one false positive, and why controlling the False Discovery Rate (FDR) is often preferable to controlling the Family-Wise Error Rate (FWER) when prioritizing candidate biomarkers for follow-up. Then, using a standard step-up approach that controls the FDR at a target level $q$ under independence, determine the data-driven $p$-value cutoff (that is, the largest $p$-value among the rejected hypotheses) for declaring statistical significance.\n\nAssume the study tests $m = 18$ genes and obtains the following set of $p$-values (unsorted): $\\{0.062,\\ 0.021,\\ 0.0087,\\ 0.055,\\ 0.0012,\\ 0.026,\\ 0.0168,\\ 0.047,\\ 0.039,\\ 0.0125,\\ 0.0102,\\ 0.091,\\ 0.0025,\\ 0.0006,\\ 0.006,\\ 0.033,\\ 0.078,\\ 0.0041\\}$. Control the FDR at level $q = 0.05$ using the Benjamini–Hochberg (BH) procedure and report the resulting $p$-value cutoff as a pure number with no units. No rounding is required.", "solution": "The problem is valid. It is scientifically grounded in the fields of biostatistics and medical informatics, presenting a standard and well-posed question about multiple hypothesis testing correction in the context of genomic data analysis. All necessary data and definitions are provided, and the problem is objective and free of ambiguity.\n\nThe problem requires a two-part explanation followed by a calculation.\n\nFirst, an explanation of why performing many hypothesis tests increases the chance of making at least one false positive.\nLet $\\alpha$ be the significance level for a single hypothesis test, which represents the probability of a Type I error (a false positive) when the null hypothesis is true. A typical value is $\\alpha = 0.05$. The probability of correctly not rejecting a true null hypothesis is therefore $1 - \\alpha$. If we conduct $m$ independent hypothesis tests, the probability of making no Type I errors across all of them is $(1 - \\alpha)^m$. The probability of making at least one Type I error, known as the Family-Wise Error Rate (FWER), is the complement of this event:\n$$\n\\text{FWER} = 1 - (1 - \\alpha)^m\n$$\nFor the given problem, with $m = 18$ genes and a per-test significance level of $\\alpha = 0.05$, the FWER without any correction would be:\n$$\n\\text{FWER} = 1 - (1 - 0.05)^{18} = 1 - (0.95)^{18} \\approx 1 - 0.3972 = 0.6028\n$$\nThis means there is over a $60\\%$ chance of observing at least one false positive result, a drastic inflation from the desired $5\\%$ level. As $m$ grows large, as is common in genomics where thousands or tens of thousands of genes are tested, this probability rapidly approaches $1$, making false positives virtually certain.\n\nSecond, an explanation of why controlling the False Discovery Rate (FDR) is often preferable to controlling the FWER in biomarker discovery.\nFWER control aims to make the probability of even a single false positive across all tests very low (e.g., $\\le \\alpha$). This is a very stringent criterion. Methods that control the FWER, such as the Bonferroni correction (which uses a significance threshold of $\\alpha/m$), are highly conservative. While they effectively reduce false positives, they also severely reduce statistical power, meaning they are much more likely to miss true effects (i.e., they increase the rate of false negatives). In an exploratory setting like biomarker discovery, the primary goal is to generate a list of promising candidates for further, more rigorous (and often expensive) validation. The cost of a false negative (missing a potentially life-saving biomarker) can be very high.\n\nFDR control, in contrast, is less stringent. The FDR is defined as the expected proportion of rejected hypotheses that are actually false rejections (false positives).\n$$\n\\text{FDR} = E\\left[\\frac{V}{R} | R > 0\\right]P(R>0)\n$$\nwhere $V$ is the number of false positives (erroneously rejected true nulls) and $R$ is the total number of rejected nulls. Controlling the FDR at a level $q$ (e.g., $q = 0.05$) means that, on average, no more than $5\\%$ of the genes declared significant are expected to be false positives. This approach tolerates a small fraction of false positives within the list of discoveries in exchange for substantially greater power to detect true effects. This trade-off is often desirable in high-dimensional discovery science, as it generates a richer candidate list for follow-up studies, which are designed to then filter out the false positives.\n\nFinally, we apply the Benjamini–Hochberg (BH) procedure to find the data-driven p-value cutoff. The procedure controls the FDR at level $q = 0.05$ for $m = 18$ independent tests.\n\nStep 1: Sort the $m=18$ p-values in ascending order, denoted as $P_{(i)}$ for $i=1, \\dots, 18$.\nThe provided set is $\\{0.062, 0.021, 0.0087, 0.055, 0.0012, 0.026, 0.0168, 0.047, 0.039, 0.0125, 0.0102, 0.091, 0.0025, 0.0006, 0.006, 0.033, 0.078, 0.0041\\}$.\nThe sorted p-values $P_{(i)}$ are:\n$P_{(1)} = 0.0006$\n$P_{(2)} = 0.0012$\n$P_{(3)} = 0.0025$\n$P_{(4)} = 0.0041$\n$P_{(5)} = 0.006$\n$P_{(6)} = 0.0087$\n$P_{(7)} = 0.0102$\n$P_{(8)} = 0.0125$\n$P_{(9)} = 0.0168$\n$P_{(10)} = 0.021$\n$P_{(11)} = 0.026$\n$P_{(12)} = 0.033$\n$P_{(13)} = 0.039$\n$P_{(14)} = 0.047$\n$P_{(15)} = 0.055$\n$P_{(16)} = 0.062$\n$P_{(17)} = 0.078$\n$P_{(18)} = 0.091$\n\nStep 2: Find the largest index $k$ such that $P_{(k)} \\le \\frac{k}{m}q$.\nHere, $m=18$ and $q=0.05$. The condition is $P_{(k)} \\le \\frac{k}{18}(0.05)$. We check this condition for each $i$:\nFor $i=1$: $P_{(1)} = 0.0006 \\le \\frac{1}{18}(0.05) \\approx 0.00278$. The condition holds.\nFor $i=2$: $P_{(2)} = 0.0012 \\le \\frac{2}{18}(0.05) \\approx 0.00556$. The condition holds.\nFor $i=3$: $P_{(3)} = 0.0025 \\le \\frac{3}{18}(0.05) \\approx 0.00833$. The condition holds.\nFor $i=4$: $P_{(4)} = 0.0041 \\le \\frac{4}{18}(0.05) \\approx 0.01111$. The condition holds.\nFor $i=5$: $P_{(5)} = 0.006 \\le \\frac{5}{18}(0.05) \\approx 0.01389$. The condition holds.\nFor $i=6$: $P_{(6)} = 0.0087 \\le \\frac{6}{18}(0.05) \\approx 0.01667$. The condition holds.\nFor $i=7$: $P_{(7)} = 0.0102 \\le \\frac{7}{18}(0.05) \\approx 0.01944$. The condition holds.\nFor $i=8$: $P_{(8)} = 0.0125 \\le \\frac{8}{18}(0.05) \\approx 0.02222$. The condition holds.\nFor $i=9$: $P_{(9)} = 0.0168 \\le \\frac{9}{18}(0.05) = 0.025$. The condition holds.\nFor $i=10$: $P_{(10)} = 0.021 \\le \\frac{10}{18}(0.05) \\approx 0.02778$. The condition holds.\nFor $i=11$: $P_{(11)} = 0.026 \\le \\frac{11}{18}(0.05) \\approx 0.03056$. The condition holds.\nFor $i=12$: $P_{(12)} = 0.033 \\le \\frac{12}{18}(0.05) \\approx 0.03333$. The condition holds.\nFor $i=13$: $P_{(13)} = 0.039 > \\frac{13}{18}(0.05) \\approx 0.03611$. The condition fails.\n\nThe largest index $k$ for which the condition holds is $k = 12$.\n\nStep 3: Determine the cutoff.\nThe BH procedure rejects all null hypotheses $H_{(i)}$ for $i = 1, \\dots, k$. In this case, we reject the null hypotheses for the $12$ smallest p-values. The data-driven p-value cutoff for declaring significance is the largest p-value among those that are rejected, which is $P_{(k)}$.\nTherefore, the cutoff is $P_{(12)} = 0.033$. Any original p-value less than or equal to this value is declared statistically significant.", "answer": "$$\\boxed{0.033}$$", "id": "4852814"}, {"introduction": "After identifying potential biomarkers, the next step in personalized healthcare is often to build a risk prediction model that can forecast outcomes for individual patients. However, a model is only as good as its performance, and assessing this requires a specific set of tools. This practice introduces you to three critical aspects of model evaluation: discrimination (its ability to separate high-risk from low-risk patients, measured by the AUC), calibration (how well its predicted probabilities match real-world frequencies), and overall accuracy (measured by the Brier score). By calculating these metrics for a hypothetical model, you will gain hands-on experience in the rigorous validation required before any predictive tool can be considered for clinical use. [@problem_id:4852815]", "problem": "A clinical decision support system produces individualized risk estimates for a binary outcome (for example, the occurrence of an adverse drug reaction within $30$ days) to inform precision medicine and personalized healthcare. For $n=6$ patients, the model outputs predicted probabilities $\\hat{p}_i$ and the observed outcomes $y_i \\in \\{0,1\\}$ as follows: patient $1$: $\\hat{p}_1=0.12$, $y_1=0$; patient $2$: $\\hat{p}_2=0.18$, $y_2=0$; patient $3$: $\\hat{p}_3=0.33$, $y_3=1$; patient $4$: $\\hat{p}_4=0.52$, $y_4=0$; patient $5$: $\\hat{p}_5=0.61$, $y_5=1$; patient $6$: $\\hat{p}_6=0.85$, $y_6=1$.\n\nUsing foundational definitions appropriate to medical informatics evaluation of probabilistic risk models:\n\n- Discrimination is quantified by the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), defined empirically as the probability that a randomly chosen case (with $y=1$) has a higher predicted risk than a randomly chosen control (with $y=0$), with ties contributing one-half.\n- Calibration is summarized by an intercept and slope obtained from the least-squares linear calibration model under squared-error loss, where the best linear projection of $y$ onto $1$ and $\\hat{p}$ minimizes $\\sum_{i=1}^{n}(y_i - \\alpha - \\beta \\hat{p}_i)^{2}$. The calibration intercept $\\alpha$ and calibration slope $\\beta$ are the coefficients of this projection.\n- Overall performance under squared-error loss is measured by the Brier score, defined as $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{p}_i - y_i)^{2}$.\n\nStarting from these definitions and first principles (probability of concordance for AUC, normal equations for least squares calibration, and mean squared error for the Brier score), compute the AUC, the calibration intercept $\\alpha$, the calibration slope $\\beta$, and the Brier score for the given data. Round each metric to four significant figures. Report your final answer as a single row matrix containing, in order, the AUC, the calibration intercept, the calibration slope, and the Brier score. No units are required.", "solution": "The problem provides data for $n=6$ patients, consisting of predicted probabilities $\\hat{p}_i$ from a risk model and observed binary outcomes $y_i$. The task is to compute three performance metrics: the Area Under the ROC Curve (AUC) for discrimination, the calibration intercept $\\alpha$ and slope $\\beta$ for calibration, and the Brier score for overall performance. The calculations must be based on the provided definitions and first principles.\n\nThe given data are:\n- Patient $1$: $\\hat{p}_1=0.12$, $y_1=0$\n- Patient $2$: $\\hat{p}_2=0.18$, $y_2=0$\n- Patient $3$: $\\hat{p}_3=0.33$, $y_3=1$\n- Patient $4$: $\\hat{p}_4=0.52$, $y_4=0$\n- Patient $5$: $\\hat{p}_5=0.61$, $y_5=1$\n- Patient $6$: $\\hat{p}_6=0.85$, $y_6=1$\n\nFirst, we identify the cases (patients with the event, $y_i=1$) and controls (patients without the event, $y_i=0$).\n- Cases: Patients $3, 5, 6$. Their predicted risks are $\\{\\hat{p}_3, \\hat{p}_5, \\hat{p}_6\\} = \\{0.33, 0.61, 0.85\\}$. The number of cases is $n_1=3$.\n- Controls: Patients $1, 2, 4$. Their predicted risks are $\\{\\hat{p}_1, \\hat{p}_2, \\hat{p}_4\\} = \\{0.12, 0.18, 0.52\\}$. The number of controls is $n_0=3$.\n\n**1. Area Under the ROC Curve (AUC)**\n\nThe AUC is defined as the probability that a randomly chosen case has a higher predicted risk than a randomly chosen control, with ties contributing $0.5$. This is equivalent to calculating the Wilcoxon-Mann-Whitney U statistic. We form all possible pairs of (case, control) and compare their predicted risks. The total number of pairs is $n_1 \\times n_0 = 3 \\times 3 = 9$.\n\nWe tabulate the comparisons:\n1.  Pair $(\\hat{p}_3, \\hat{p}_1) = (0.33, 0.12)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n2.  Pair $(\\hat{p}_3, \\hat{p}_2) = (0.33, 0.18)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n3.  Pair $(\\hat{p}_3, \\hat{p}_4) = (0.33, 0.52)$: $\\hat{p}_{\\text{case}} < \\hat{p}_{\\text{control}}$. Score = $0$.\n4.  Pair $(\\hat{p}_5, \\hat{p}_1) = (0.61, 0.12)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n5.  Pair $(\\hat{p}_5, \\hat{p}_2) = (0.61, 0.18)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n6.  Pair $(\\hat{p}_5, \\hat{p}_4) = (0.61, 0.52)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n7.  Pair $(\\hat{p}_6, \\hat{p}_1) = (0.85, 0.12)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n8.  Pair $(\\hat{p}_6, \\hat{p}_2) = (0.85, 0.18)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n9.  Pair $(\\hat{p}_6, \\hat{p}_4) = (0.85, 0.52)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n\nThere are no ties in predicted risk between cases and controls. The total score is the sum of scores for all pairs:\n$$ \\text{Total Score} = 1+1+0+1+1+1+1+1+1 = 8 $$\nThe AUC is the total score divided by the total number of pairs:\n$$ \\text{AUC} = \\frac{8}{9} \\approx 0.88888... $$\nRounding to four significant figures, we get $\\text{AUC} = 0.8889$.\n\n**2. Calibration Intercept ($\\alpha$) and Slope ($\\beta$)**\n\nThe calibration intercept $\\alpha$ and slope $\\beta$ are determined by a simple linear regression of the observed outcomes $y_i$ on the predicted probabilities $\\hat{p}_i$. We aim to find $\\alpha$ and $\\beta$ that minimize the sum of squared errors $S = \\sum_{i=1}^{n}(y_i - \\alpha - \\beta \\hat{p}_i)^{2}$. The solution is given by the normal equations, leading to the standard formulas for least-squares estimates:\n$$ \\beta = \\frac{\\sum_{i=1}^{n}(\\hat{p}_i - \\bar{p})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(\\hat{p}_i - \\bar{p})^2} = \\frac{n\\sum_{i=1}^{n}\\hat{p}_iy_i - (\\sum_{i=1}^{n}\\hat{p}_i)(\\sum_{i=1}^{n}y_i)}{n\\sum_{i=1}^{n}\\hat{p}_i^2 - (\\sum_{i=1}^{n}\\hat{p}_i)^2} $$\n$$ \\alpha = \\bar{y} - \\beta\\bar{p} $$\nwhere $\\bar{p}$ is the mean of the predicted probabilities and $\\bar{y}$ is the mean of the outcomes.\n\nWe compute the necessary sums with $n=6$:\n- $\\sum y_i = 0+0+1+0+1+1 = 3$\n- $\\sum \\hat{p}_i = 0.12+0.18+0.33+0.52+0.61+0.85 = 2.61$\n- $\\sum \\hat{p}_iy_i = (0.12)(0)+(0.18)(0)+(0.33)(1)+(0.52)(0)+(0.61)(1)+(0.85)(1) = 0.33+0.61+0.85 = 1.79$\n- $\\sum \\hat{p}_i^2 = 0.12^2+0.18^2+0.33^2+0.52^2+0.61^2+0.85^2 = 0.0144+0.0324+0.1089+0.2704+0.3721+0.7225 = 1.5207$\n\nNow we can compute $\\beta$:\n$$ \\beta = \\frac{6(1.79) - (2.61)(3)}{6(1.5207) - (2.61)^2} = \\frac{10.74 - 7.83}{9.1242 - 6.8121} = \\frac{2.91}{2.3121} \\approx 1.258596... $$\nRounding to four significant figures gives $\\beta = 1.259$.\n\nNext, we compute the means $\\bar{y}$ and $\\bar{p}$:\n- $\\bar{y} = \\frac{\\sum y_i}{n} = \\frac{3}{6} = 0.5$\n- $\\bar{p} = \\frac{\\sum \\hat{p}_i}{n} = \\frac{2.61}{6} = 0.435$\n\nThen, we calculate $\\alpha$:\n$$ \\alpha = \\bar{y} - \\beta\\bar{p} \\approx 0.5 - (1.258596...)(0.435) = 0.5 - 0.547489... = -0.047489... $$\nRounding to four significant figures gives $\\alpha = -0.04749$.\n\n**3. Brier Score**\n\nThe Brier score is the mean squared error between the predicted probabilities and the observed outcomes. It is defined as:\n$$ \\text{Brier Score} = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{p}_i - y_i)^2 $$\nWe calculate the squared error for each patient:\n- $(\\hat{p}_1-y_1)^2 = (0.12-0)^2 = 0.0144$\n- $(\\hat{p}_2-y_2)^2 = (0.18-0)^2 = 0.0324$\n- $(\\hat{p}_3-y_3)^2 = (0.33-1)^2 = (-0.67)^2 = 0.4489$\n- $(\\hat{p}_4-y_4)^2 = (0.52-0)^2 = 0.2704$\n- $(\\hat{p}_5-y_5)^2 = (0.61-1)^2 = (-0.39)^2 = 0.1521$\n- $(\\hat{p}_6-y_6)^2 = (0.85-1)^2 = (-0.15)^2 = 0.0225$\n\nThe sum of these squared errors is:\n$$ \\sum_{i=1}^{6}(\\hat{p}_i - y_i)^2 = 0.0144+0.0324+0.4489+0.2704+0.1521+0.0225 = 0.9407 $$\nThe Brier score is the mean of this sum:\n$$ \\text{Brier Score} = \\frac{0.9407}{6} \\approx 0.1567833... $$\nRounding to four significant figures, we get $\\text{Brier Score} = 0.1568$.\n\nIn summary, the computed metrics rounded to four significant figures are:\n- $\\text{AUC} = 0.8889$\n- Calibration Intercept $\\alpha = -0.04749$\n- Calibration Slope $\\beta = 1.259$\n- Brier Score $= 0.1568$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.8889 & -0.04749 & 1.259 & 0.1568\n\\end{pmatrix}\n}\n$$", "id": "4852815"}, {"introduction": "The final and most crucial step in precision medicine is translating validated evidence into actionable guidance at the point of care. This is often achieved through Clinical Decision Support (CDS) systems, which require precise, unambiguous, and executable logic. In this exercise, you will step into the role of a medical informatician and formalize a well-known pharmacogenomic rule for personalizing warfarin dosage based on a patient's *VKORC1* and *CYP2C9* genotypes. This practice will challenge you to convert clinical principles and data into a concrete algorithm that triggers alerts, calculates a personalized dose, and flags high-risk patients, directly bridging the gap between genomic data and improved patient safety. [@problem_id:4852840]", "problem": "You are to implement, as an executable program, the semantics of a pharmacogenomics clinical decision support rule equivalent in intent to what would be expressed in Clinical Quality Language (CQL) or Arden Syntax, for initial warfarin dose personalization based on genotypes for vitamin K epoxide reductase complex subunit 1 (*VKORC1*) and cytochrome P450 family 2 subfamily C member 9 (*CYP2C9*). The goal is to compute an initial daily dose recommendation and to indicate whether the rule is triggered and whether the patient is flagged as high risk for over-anticoagulation on initiation. Your program must produce results for a defined test suite and print them in the exact output format at the end.\n\nFundamental base. Use the following well-tested principles from clinical pharmacology and medical informatics as the derivational basis, not as target formulas: \n- Maintenance dose at steady state is proportional to the drug’s systemic clearance. If the target pharmacodynamic effect (for warfarin, as monitored by the International Normalized Ratio (INR)) is fixed, then for two individuals, the ratio of required maintenance doses is proportional to the ratio of their clearances. \n- Pharmacodynamic sensitivity modifies the dose needed to achieve a given effect; if a patient is more sensitive, a lower dose is needed. For two individuals with respective sensitivities, the ratio of doses required to achieve the same effect is inversely proportional to the ratio of sensitivities. \n- Genotypes can be mapped to relative, dimensionless multipliers of clearance or sensitivity. We assume independence of gene effects across VKORC1 (sensitivity) and CYP2C9 (clearance), a standard simplifying assumption used in many clinical decision support models.\n\nClinical semantics to formalize. The Clinical Quality Language (CQL) or Arden Syntax logic is abstracted as follows in terms of trigger and action:\n- Trigger semantics: The rule triggers if and only if three conditions are simultaneously true: (i) a new warfarin initiation is occurring, (ii) a *VKORC1* genotype value is available, and (iii) a *CYP2C9* genotype value is available. Formally, define a boolean trigger variable $T$ as $T = \\text{is\\_new\\_start} \\land \\text{has\\_VKORC1} \\land \\text{has\\_CYP2C9}$.\n- Action semantics: If $T$ is true, compute a personalized initial daily dose recommendation $D'$ from a clinician-provided baseline empiric dose $D$ by adjusting for relative clearance and sensitivity. If $T$ is false, return the baseline empiric dose $D$ unchanged as the recommendation. In all cases, round the recommendation to the nearest $0.5$ milligrams per day (mg/day) using round-half-up, and clamp the result to the closed interval $[0.5, 10.0]$ mg/day. Additionally, compute a high-risk flag $R$ defined as $R = 1$ if and only if $T$ is true and the overall multiplicative adjustment implies at least a $50$ percent reduction from baseline (that is, the combined adjustment factor is less than or equal to $0.5$); otherwise $R = 0$.\n\nGenotype-to-parameter mappings. Use the following genotype-to-multiplier mappings, where *VKORC1* affects a relative sensitivity multiplier and *CYP2C9* affects a relative clearance multiplier. All multipliers are dimensionless and represent values relative to a typical reference:\n- *VKORC1* to relative sensitivity $s_{\\mathrm{VKORC1}}$: \n  - GG $\\rightarrow$ $1.0$\n  - GA $\\rightarrow$ $1.4$\n  - AA $\\rightarrow$ $1.7$\n- *CYP2C9* to relative clearance $c_{\\mathrm{CYP2C9}}$: \n  - $^\\ast 1/^\\ast 1 \\rightarrow 1.0$\n  - $^\\ast 1/^\\ast 2 \\rightarrow 0.85$\n  - $^\\ast 1/^\\ast 3 \\rightarrow 0.75$\n  - $^\\ast 2/^\\ast 2 \\rightarrow 0.60$\n  - $^\\ast 2/^\\ast 3 \\rightarrow 0.50$\n  - $^\\ast 3/^\\ast 3 \\rightarrow 0.40$\n\nInput model and computation tasks. For each test case, you are given:\n- A clinician-chosen baseline empiric dose $D$ in mg/day, a real number.\n- A *VKORC1* genotype string in the set {\"GG\", \"GA\", \"AA\"} or the literal string \"unknown\".\n- A *CYP2C9* genotype string in the set {\"*1/*1\", \"*1/*2\", \"*1/*3\", \"*2/*2\", \"*2/*3\", \"*3/*3\"} or the literal string \"unknown\".\n- A boolean is\\_new\\_start indicating a new start of warfarin therapy.\n\nYour tasks per test case:\n- Determine $T$ using availability of genotype values and the is\\_new\\_start input.\n- If $T$ is true, compute $D'$ using the scientific base you derived from the principles above, the provided genotype multipliers, rounding to the nearest $0.5$ mg/day by round-half-up, and clamping to $[0.5, 10.0]$ mg/day.\n- If $T$ is false, set $D' = D$ and still apply rounding to the nearest $0.5$ mg/day by round-half-up, followed by clamping to $[0.5, 10.0]$ mg/day.\n- Compute $R$ as defined above.\n\nAngle units are not involved. All physical outputs must be in mg/day. All percentage-like conditions must be implemented as decimal fractions, not with a percentage sign.\n\nTest suite. Implement your program to process exactly the following five test cases in the given order. Each case is a quadruple $(D,\\ \\text{*VKORC1*},\\ \\text{*CYP2C9*},\\ \\text{is\\_new\\_start})$:\n1. $(5.0,\\ \\text{\"GA\"},\\ \\text{\"*1/*2\"},\\ \\text{true})$\n2. $(7.0,\\ \\text{\"AA\"},\\ \\text{\"*3/*3\"},\\ \\text{true})$\n3. $(5.0,\\ \\text{\"unknown\"},\\ \\text{\"*1/*1\"},\\ \\text{true})$\n4. $(4.5,\\ \\text{\"GG\"},\\ \\text{\"*1/*1\"},\\ \\text{false})$\n5. $(12.0,\\ \\text{\"GG\"},\\ \\text{\"*1/*1\"},\\ \\text{true})$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a three-element list in the order $[T,\\ D',\\ R]$, where $T$ is a boolean rendered as lowercase \"true\" or \"false\", $D'$ is a float in mg/day rounded as specified, and $R$ is an integer in $\\{0,1\\}$. For example, a syntactically valid overall output looks like: \n[[\"true\",1.5,1],[\"false\",5.0,0],...]\nbut you must output the actual computed values for the provided test suite, preserving the list-of-lists structure and using lowercase \"true\"/\"false\" literals without quotes for the booleans.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of pharmacogenomics, well-posed with a complete and consistent set of rules, and expressed objectively. A unique solution can be algorithmically determined for each test case.\n\nThe core of the problem is to formalize a clinical decision support rule for personalizing an initial warfarin dose. This requires deriving a dose adjustment formula from fundamental pharmacological principles and then implementing the specified logic for triggering the rule, calculating the dose, and flagging high-risk patients.\n\n**1. Derivation of the Dose Adjustment Formula**\n\nThe problem provides two core principles:\n1.  The required drug dose at steady state is proportional to the drug's systemic clearance. For two individuals, patient ($p$) and reference ($ref$), this implies:\n    $$ \\frac{D_p}{D_{ref}} \\propto \\frac{Cl_p}{Cl_{ref}} $$\n2.  The required dose is inversely proportional to the patient's sensitivity to the drug.\n    $$ \\frac{D_p}{D_{ref}} \\propto \\frac{S_{ref}}{S_p} $$\n\nThe problem provides dimensionless multipliers for relative clearance, $c_{\\mathrm{CYP2C9}} = Cl_p/Cl_{ref}$, and relative sensitivity, $s_{\\mathrm{VKORC1}} = S_p/S_{ref}$. The baseline empiric dose, $D$, provided by the clinician can be treated as the reference dose, $D_{ref}$, corresponding to an individual with reference genotypes (VKORC1 GG and CYP2C9 $*1/*1$), for whom both multipliers are $1.0$.\n\nAssuming the effects of clearance and sensitivity are independent, we can combine the proportionalities:\n$$ \\frac{D_p}{D_{ref}} \\propto \\left( \\frac{Cl_p}{Cl_{ref}} \\right) \\times \\left( \\frac{S_{ref}}{S_p} \\right) $$\n\nSubstituting the given relative multipliers and setting the proportionality constant to $1$, the personalized dose for the patient, $D'$, is:\n$$ D' = D_{ref} \\times c_{\\mathrm{CYP2C9}} \\times \\frac{1}{s_{\\mathrm{VKORC1}}} $$\n\nThe overall multiplicative adjustment factor is therefore $\\frac{c_{\\mathrm{CYP2C9}}}{s_{\\mathrm{VKORC1}}}$. The unadjusted, unrounded personalized dose, which we can call $D''_{\\text{calc}}$, is calculated as:\n$$ D''_{\\text{calc}} = D \\times \\frac{c_{\\mathrm{CYP2C9}}}{s_{\\mathrm{VKORC1}}} $$\nwhere $D$ is the clinician-provided baseline dose.\n\n**2. Algorithmic Implementation of the Clinical Rule**\n\nFor each test case, specified by the tuple $(D, \\text{VKORC1 genotype}, \\text{CYP2C9 genotype}, \\text{is\\_new\\_start})$, the following sequence of operations is performed.\n\n**Step 2.1: Determine the Trigger Status ($T$)**\n\nThe rule triggers if and only if therapy is being newly initiated and both required genotype results are available. Let $V_{\\text{known}}$ be a boolean indicating if the VKORC1 genotype is known (i.e., not `\"unknown\"`), and $C_{\\text{known}}$ be a boolean for the CYP2C9 genotype. The trigger variable $T$ is defined as:\n$$ T = (\\text{is\\_new\\_start}) \\land V_{\\text{known}} \\land C_{\\text{known}} $$\n\n**Step 2.2: Compute the High-Risk Flag ($R$)**\n\nThe high-risk flag $R$ depends on the trigger status $T$.\n- If $T$ is false, then $R=0$.\n- If $T$ is true, we compute the adjustment factor, $A = \\frac{c_{\\mathrm{CYP2C9}}}{s_{\\mathrm{VKORC1}}}$. The flag $R$ is set to $1$ if the patient is at high risk, defined as requiring a dose reduction of at least $50\\%$. This corresponds to an adjustment factor less than or equal to $0.5$.\n$$ R = \\begin{cases} 1 & \\text{if } T \\text{ is true and } A \\le 0.5 \\\\ 0 & \\text{otherwise} \\end{cases} $$\n\n**Step 2.3: Calculate the Recommended Dose ($D'$)**\n\nThe calculation of the intermediate dose, $D''$, depends on the trigger $T$.\n- If $T$ is true, the personalized dose is calculated using the formula derived in section 1:\n  $$ D'' = D \\times \\frac{c_{\\mathrm{CYP2C9}}}{s_{\\mathrm{VKORC1}}} $$\n- If $T$ is false, the personalized dose is simply the baseline dose:\n  $$ D'' = D $$\n\nThis intermediate dose $D''$ must then undergo rounding and clamping, regardless of the value of $T$.\n\n**Step 2.4: Apply Rounding**\n\nThe dose must be rounded to the nearest $0.5$ mg/day using the \"round-half-up\" method. For a given value $x$, this can be implemented by scaling, rounding, and unscaling. The formula is:\n$$ D''' = \\frac{\\lfloor 2x + 0.5 \\rfloor}{2} $$\nLet's apply this to $D''$. Let $D'''$ be the rounded dose:\n$$ D''' = \\frac{\\lfloor 2 \\cdot D'' + 0.5 \\rfloor}{2} $$\n\n**Step 2.5: Apply Clamping**\n\nThe final recommended dose, $D'$, is obtained by clamping the rounded dose $D'''$ to the specified closed interval $[0.5, 10.0]$ mg/day.\n$$ D' = \\max(0.5, \\min(10.0, D''')) $$\n\n**3. Execution on Test Suite**\n\nWe apply this algorithm to the given test suite. The genotype-to-multiplier mappings are:\n- $s_{\\mathrm{VKORC1}}$: `{\"GG\": 1.0, \"GA\": 1.4, \"AA\": 1.7}`\n- $c_{\\mathrm{CYP2C9}}$: `{\"*1/*1\": 1.0, \"*1/*2\": 0.85, \"*1/*3\": 0.75, \"*2/*2\": 0.60, \"*2/*3\": 0.50, \"*3/*3\": 0.40}`\n\n- **Case 1:** $(5.0, \\text{\"GA\"}, \\text{\"*1/*2\"}, \\text{true})$\n  - $T$: true $\\land$ true $\\land$ true $\\implies T=\\text{true}$.\n  - $s_{\\mathrm{VKORC1}} = 1.4$, $c_{\\mathrm{CYP2C9}} = 0.85$. Adjustment factor $A = 0.85/1.4 \\approx 0.607$.\n  - $R$: $A > 0.5 \\implies R=0$.\n  - $D'' = 5.0 \\times 0.607 \\approx 3.036$.\n  - Rounding $3.036$: $\\lfloor 2 \\cdot 3.036 + 0.5 \\rfloor / 2 = \\lfloor 6.072 + 0.5 \\rfloor / 2 = \\lfloor 6.572 \\rfloor / 2 = 6/2 = 3.0$.\n  - Clamping $3.0$: $\\max(0.5, \\min(10.0, 3.0)) = 3.0$.\n  - Result: $[\\text{true}, 3.0, 0]$\n\n- **Case 2:** $(7.0, \\text{\"AA\"}, \\text{\"*3/*3\"}, \\text{true})$\n  - $T$: true $\\land$ true $\\land$ true $\\implies T=\\text{true}$.\n  - $s_{\\mathrm{VKORC1}} = 1.7$, $c_{\\mathrm{CYP2C9}} = 0.40$. Adjustment factor $A = 0.40/1.7 \\approx 0.235$.\n  - $R$: $A \\le 0.5 \\implies R=1$.\n  - $D'' = 7.0 \\times 0.235 \\approx 1.647$.\n  - Rounding $1.647$: $\\lfloor 2 \\cdot 1.647 + 0.5 \\rfloor / 2 = \\lfloor 3.294 + 0.5 \\rfloor / 2 = \\lfloor 3.794 \\rfloor / 2 = 3/2 = 1.5$.\n  - Clamping $1.5$: $\\max(0.5, \\min(10.0, 1.5)) = 1.5$.\n  - Result: $[\\text{true}, 1.5, 1]$\n\n- **Case 3:** $(5.0, \\text{\"unknown\"}, \\text{\"*1/*1\"}, \\text{true})$\n  - $T$: true $\\land$ false $\\land$ true $\\implies T=\\text{false}$.\n  - $R$: $T$ is false $\\implies R=0$.\n  - $D'' = 5.0$.\n  - Rounding $5.0$: $\\lfloor 2 \\cdot 5.0 + 0.5 \\rfloor / 2 = \\lfloor 10.5 \\rfloor / 2 = 10/2 = 5.0$.\n  - Clamping $5.0$: $\\max(0.5, \\min(10.0, 5.0)) = 5.0$.\n  - Result: $[\\text{false}, 5.0, 0]$\n\n- **Case 4:** $(4.5, \\text{\"GG\"}, \\text{\"*1/*1\"}, \\text{false})$\n  - $T$: false $\\land$ true $\\land$ true $\\implies T=\\text{false}$.\n  - $R$: $T$ is false $\\implies R=0$.\n  - $D'' = 4.5$.\n  - Rounding $4.5$: $\\lfloor 2 \\cdot 4.5 + 0.5 \\rfloor / 2 = \\lfloor 9.5 \\rfloor / 2 = 9/2 = 4.5$.\n  - Clamping $4.5$: $\\max(0.5, \\min(10.0, 4.5)) = 4.5$.\n  - Result: $[\\text{false}, 4.5, 0]$\n\n- **Case 5:** $(12.0, \\text{\"GG\"}, \\text{\"*1/*1\"}, \\text{true})$\n  - $T$: true $\\land$ true $\\land$ true $\\implies T=\\text{true}$.\n  - $s_{\\mathrm{VKORC1}} = 1.0$, $c_{\\mathrm{CYP2C9}} = 1.0$. Adjustment factor $A = 1.0/1.0 = 1.0$.\n  - $R$: $A > 0.5 \\implies R=0$.\n  - $D'' = 12.0 \\times 1.0 = 12.0$.\n  - Rounding $12.0$: $\\lfloor 2 \\cdot 12.0 + 0.5 \\rfloor / 2 = \\lfloor 24.5 \\rfloor / 2 = 24/2 = 12.0$.\n  - Clamping $12.0$: $\\max(0.5, \\min(10.0, 12.0)) = 10.0$.\n  - Result: $[\\text{true}, 10.0, 0]$\n\nThese steps and calculations are implemented in the provided program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a pharmacogenomics clinical decision support rule for warfarin dosing.\n    \"\"\"\n\n    # Genotype-to-parameter mappings as specified in the problem statement.\n    S_VKORC1_MAP = {\n        \"GG\": 1.0,\n        \"GA\": 1.4,\n        \"AA\": 1.7,\n    }\n\n    C_CYP2C9_MAP = {\n        \"*1/*1\": 1.0,\n        \"*1/*2\": 0.85,\n        \"*1/*3\": 0.75,\n        \"*2/*2\": 0.60,\n        \"*2/*3\": 0.50,\n        \"*3/*3\": 0.40,\n    }\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (D, VKORC1, CYP2C9, is_new_start)\n        (5.0, \"GA\", \"*1/*2\", True),\n        (7.0, \"AA\", \"*3/*3\", True),\n        (5.0, \"unknown\", \"*1/*1\", True),\n        (4.5, \"GG\", \"*1/*1\", False),\n        (12.0, \"GG\", \"*1/*1\", True),\n    ]\n\n    results = []\n    \n    # Define the rounding function as per \"round-half-up\" to nearest 0.5.\n    def round_half_up_to_0_5(val):\n        \"\"\"Rounds a number to the nearest 0.5 using the round-half-up method.\"\"\"\n        return np.floor(val * 2 + 0.5) / 2.0\n\n    for case in test_cases:\n        D, vkorc1_geno, cyp2c9_geno, is_new_start = case\n\n        # Step 1: Determine Trigger Status (T)\n        has_vkorc1 = vkorc1_geno != \"unknown\"\n        has_cyp2c9 = cyp2c9_geno != \"unknown\"\n        T = is_new_start and has_vkorc1 and has_cyp2c9\n\n        R = 0\n        dose_unrounded = D # Default if T is false\n\n        if T:\n            # Rule is triggered.\n            s_vkorc1 = S_VKORC1_MAP[vkorc1_geno]\n            c_cyp2c9 = C_CYP2C9_MAP[cyp2c9_geno]\n\n            # Calculate adjustment factor\n            adjustment_factor = c_cyp2c9 / s_vkorc1\n\n            # Step 2: Compute High-Risk Flag (R)\n            if adjustment_factor <= 0.5:\n                R = 1\n            \n            # Calculate the unrounded, unadjusted dose\n            dose_unrounded = D * adjustment_factor\n        \n        # Step 3 & 4: Apply rounding then clamping to the dose\n        # This is done for all cases, whether T is true or false.\n        dose_rounded = round_half_up_to_0_5(dose_unrounded)\n        \n        # Clamp the result to the closed interval [0.5, 10.0]\n        D_prime = np.clip(dose_rounded, 0.5, 10.0)\n\n        results.append([T, D_prime, R])\n\n    # Final print statement in the exact required format.\n    # We need to manually format the boolean to lowercase 'true'/'false'.\n    result_strings = []\n    for res in results:\n        # Format: [boolean,float,int] -> \"[true,1.5,1]\"\n        t_str = str(res[0]).lower()\n        d_prime_str = f\"{res[1]:.1f}\" # Ensure at least one decimal place\n        r_str = str(res[2])\n        result_strings.append(f\"[{t_str},{d_prime_str},{r_str}]\")\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "4852840"}]}