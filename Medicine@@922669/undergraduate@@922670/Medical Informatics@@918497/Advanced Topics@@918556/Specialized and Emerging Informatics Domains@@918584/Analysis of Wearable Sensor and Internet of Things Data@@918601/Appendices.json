{"hands_on_practices": [{"introduction": "Before a new wearable sensor can be trusted for clinical or research purposes, it must be validated against an established gold standard. This practice introduces the Bland-Altman analysis, the standard statistical method for assessing the agreement between two quantitative measurement methods. By working through this problem, you will learn to calculate the mean bias and the $95\\%$ limits of agreement, allowing you to quantify how much a new wearable device's measurements differ from a trusted clinical monitor and to assess the consistency of this difference across a range of values [@problem_id:4822398]. This is a foundational skill for anyone involved in the evaluation and deployment of new medical technology.", "problem": "A research team in medical informatics is validating a consumer wearable against a clinical bedside cardiac monitor using paired heart rate measurements collected simultaneously from a cohort of $10$ adult inpatients. The focus is on agreement analysis for Internet of Things (IoT) devices in clinical settings. For each subject $i$, let $C_i$ be the heart rate from the clinical monitor and $W_i$ be the heart rate from the wearable, both in beats per minute (bpm). The paired data are:\n\n- Subject $1$: $C_1 = 72$, $W_1 = 74$\n- Subject $2$: $C_2 = 78$, $W_2 = 80$\n- Subject $3$: $C_3 = 65$, $W_3 = 66$\n- Subject $4$: $C_4 = 88$, $W_4 = 89$\n- Subject $5$: $C_5 = 90$, $W_5 = 93$\n- Subject $6$: $C_6 = 76$, $W_6 = 77$\n- Subject $7$: $C_7 = 84$, $W_7 = 82$\n- Subject $8$: $C_8 = 92$, $W_8 = 95$\n- Subject $9$: $C_9 = 70$, $W_9 = 69$\n- Subject $10$: $C_{10} = 80$, $W_{10} = 79$\n\nAssume the within-subject measurement differences are approximately normally distributed and independent across subjects.\n\nTasks:\n1. Compute the mean bias of the wearable relative to the clinical monitor using the differences $d_i = W_i - C_i$.\n2. Compute the $95\\%$ limits of agreement for the differences.\n3. Assess proportional bias by fitting an ordinary least squares regression of $d_i$ on the within-subject means $m_i = (W_i + C_i)/2$ and performing a two-sided hypothesis test at significance level $\\alpha = 0.05$ for the slope being $0$.\n\nReport your computations and reasoning. Express the final answer as the upper limit of agreement in beats per minute (bpm), rounded to three significant figures. The final reported value must be given in bpm.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- Number of subjects, $n=10$.\n- Paired heart rate measurements $(C_i, W_i)$ in beats per minute (bpm) for subjects $i=1, \\dots, 10$:\n  - Subject $1$: $C_1 = 72$, $W_1 = 74$\n  - Subject $2$: $C_2 = 78$, $W_2 = 80$\n  - Subject $3$: $C_3 = 65$, $W_3 = 66$\n  - Subject $4$: $C_4 = 88$, $W_4 = 89$\n  - Subject $5$: $C_5 = 90$, $W_5 = 93$\n  - Subject $6$: $C_6 = 76$, $W_6 = 77$\n  - Subject $7$: $C_7 = 84$, $W_7 = 82$\n  - Subject $8$: $C_8 = 92$, $W_8 = 95$\n  - Subject $9$: $C_9 = 70$, $W_9 = 69$\n  - Subject $10$: $C_{10} = 80$, $W_{10} = 79$\n- Assumption: The within-subject measurement differences are approximately normally distributed and independent across subjects.\n- Definitions:\n  - Difference: $d_i = W_i - C_i$\n  - Within-subject mean: $m_i = (W_i + C_i)/2$\n- Tasks:\n  1. Compute the mean bias, $\\bar{d}$.\n  2. Compute the $95\\%$ limits of agreement.\n  3. Assess proportional bias by regressing $d_i$ on $m_i$ and testing the slope's significance at $\\alpha = 0.05$.\n- Required Output: The upper limit of agreement in bpm, rounded to three significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem describes a standard Bland-Altman analysis, a widely accepted statistical method for comparing two clinical measurement techniques. The context (validating a wearable sensor against a clinical standard) is a common and important task in medical informatics. The methodology is scientifically sound.\n- **Well-Posed**: All necessary data, assumptions (normality of differences), and methodological instructions (OLS regression, significance level) are provided. The tasks lead to a unique, computable set of results.\n- **Objective**: The problem is stated using precise, quantitative language without subjective or biased phrasing.\n- **Complete and Consistent**: The data set is complete for the specified number of subjects ($n=10$). The tasks are logically sequential and do not contain contradictions.\n- **Realistic and Feasible**: The heart rate values are physiologically realistic for adult inpatients. The small differences between the devices are plausible for a consumer-grade device being validated.\n- **Other Flaws**: The problem is not metaphorical, trivial, ill-posed, or outside the realm of scientific verification.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution is warranted.\n\n### Solution Derivation\n\nThe analysis proceeds by completing the three specified tasks.\n\n**Task 1: Compute the mean bias**\n\nFirst, we calculate the difference $d_i = W_i - C_i$ for each subject $i$.\n- $d_1 = 74 - 72 = 2$\n- $d_2 = 80 - 78 = 2$\n- $d_3 = 66 - 65 = 1$\n- $d_4 = 89 - 88 = 1$\n- $d_5 = 93 - 90 = 3$\n- $d_6 = 77 - 76 = 1$\n- $d_7 = 82 - 84 = -2$\n- $d_8 = 95 - 92 = 3$\n- $d_9 = 69 - 70 = -1$\n- $d_{10} = 79 - 80 = -1$\n\nThe mean bias, $\\bar{d}$, is the sample mean of these differences.\n$$ \\sum_{i=1}^{10} d_i = 2 + 2 + 1 + 1 + 3 + 1 - 2 + 3 - 1 - 1 = 9 $$\n$$ \\bar{d} = \\frac{\\sum_{i=1}^{10} d_i}{n} = \\frac{9}{10} = 0.9 $$\nThe mean bias is $0.9$ bpm, indicating the wearable tends to read slightly higher than the clinical monitor on average.\n\n**Task 2: Compute the 95% limits of agreement**\n\nThe limits of agreement (LoA) are defined as the interval expected to contain approximately $95\\%$ of future differences. This is calculated as $\\bar{d} \\pm 1.96 s_d$, where $s_d$ is the sample standard deviation of the differences.\n\nWe first compute the sample variance, $s_d^2$.\n$$ s_d^2 = \\frac{1}{n-1} \\sum_{i=1}^{10} (d_i - \\bar{d})^2 $$\nThe sum of squared differences is:\n$$ \\sum_{i=1}^{10} d_i^2 = 2^2 + 2^2 + 1^2 + 1^2 + 3^2 + 1^2 + (-2)^2 + 3^2 + (-1)^2 + (-1)^2 $$\n$$ \\sum_{i=1}^{10} d_i^2 = 4 + 4 + 1 + 1 + 9 + 1 + 4 + 9 + 1 + 1 = 35 $$\nUsing the computational formula for variance:\n$$ s_d^2 = \\frac{1}{n-1} \\left( \\sum_{i=1}^{10} d_i^2 - \\frac{(\\sum_{i=1}^{10} d_i)^2}{n} \\right) = \\frac{1}{10-1} \\left( 35 - \\frac{9^2}{10} \\right) = \\frac{1}{9} (35 - 8.1) = \\frac{26.9}{9} $$\nThe sample standard deviation is:\n$$ s_d = \\sqrt{\\frac{26.9}{9}} \\approx 1.7288 \\text{ bpm} $$\nThe $95\\%$ limits of agreement are:\n$$ \\text{LoA} = \\bar{d} \\pm 1.96 s_d = 0.9 \\pm 1.96 \\times 1.7288 $$\n$$ \\text{LoA} = 0.9 \\pm 3.3885 $$\n- Upper Limit of Agreement (ULA): $0.9 + 3.3885 = 4.2885$ bpm.\n- Lower Limit of Agreement (LLA): $0.9 - 3.3885 = -2.4885$ bpm.\n\n**Task 3: Assess proportional bias**\n\nProportional bias is assessed by fitting an ordinary least squares (OLS) regression model $d_i = \\beta_0 + \\beta_1 m_i + \\epsilon_i$, where $m_i = (W_i + C_i)/2$. We test the null hypothesis $H_0: \\beta_1 = 0$ against the alternative $H_1: \\beta_1 \\neq 0$ at a significance level of $\\alpha = 0.05$.\n\nFirst, we calculate the within-subject means $m_i$:\n- $m_1 = (74+72)/2 = 73.0$\n- $m_2 = (80+78)/2 = 79.0$\n- $m_3 = (66+65)/2 = 65.5$\n- $m_4 = (89+88)/2 = 88.5$\n- $m_5 = (93+90)/2 = 91.5$\n- $m_6 = (77+76)/2 = 76.5$\n- $m_7 = (82+84)/2 = 83.0$\n- $m_8 = (95+92)/2 = 93.5$\n- $m_9 = (69+70)/2 = 69.5$\n- $m_{10} = (79+80)/2 = 79.5$\n\nNext, we compute the necessary sums for the OLS estimate of the slope $\\hat{\\beta}_1 = S_{md} / S_{mm}$.\n$$ \\sum m_i = 799.5 \\implies \\bar{m} = \\frac{799.5}{10} = 79.95 $$\n$$ S_{mm} = \\sum (m_i - \\bar{m})^2 = \\sum m_i^2 - n \\bar{m}^2 $$\n$$ \\sum m_i^2 = 73^2 + \\dots + 79.5^2 = 64698.75 $$\n$$ S_{mm} = 64698.75 - 10 \\times (79.95)^2 = 64698.75 - 63920.025 = 778.725 $$\n$$ S_{md} = \\sum (m_i - \\bar{m})(d_i - \\bar{d}) = \\sum m_i d_i - n \\bar{m} \\bar{d} $$\n$$ \\sum m_i d_i = (73.0 \\times 2) + \\dots + (79.5 \\times -1) = 774.5 $$\n$$ S_{md} = 774.5 - 10 \\times 79.95 \\times 0.9 = 774.5 - 719.55 = 54.95 $$\nThe OLS estimate for the slope is:\n$$ \\hat{\\beta}_1 = \\frac{S_{md}}{S_{mm}} = \\frac{54.95}{778.725} \\approx 0.070564 $$\nTo test the hypothesis $H_0: \\beta_1=0$, we compute the t-statistic. First, we need the standard error of the slope, $SE(\\hat{\\beta}_1)$.\nThe sum of squared residuals is $SSE = S_{dd} - \\hat{\\beta}_1 S_{md}$, where $S_{dd} = \\sum(d_i - \\bar{d})^2 = (n-1)s_d^2 = 26.9$.\n$$ SSE = 26.9 - (0.070564) \\times 54.95 \\approx 26.9 - 3.8775 = 23.0225 $$\nThe mean squared error is $MSE = \\frac{SSE}{n-2} = \\frac{23.0225}{8} \\approx 2.8778$.\nThe standard error of the slope is:\n$$ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{MSE}{S_{mm}}} = \\sqrt{\\frac{2.8778}{778.725}} \\approx \\sqrt{0.0036956} \\approx 0.06079 $$\nThe t-statistic is:\n$$ t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} = \\frac{0.070564}{0.06079} \\approx 1.161 $$\nFor a two-sided test with $\\alpha = 0.05$ and degrees of freedom $df = n-2 = 8$, the critical t-value is $t_{crit} = t_{8, 0.975} = 2.306$.\nSince $|t| \\approx 1.161 < 2.306$, we fail to reject the null hypothesis $H_0$. There is no statistically significant evidence of proportional bias at the $\\alpha = 0.05$ level. Therefore, the use of the simple limits of agreement calculated in Task 2 is appropriate.\n\nThe problem asks for the upper limit of agreement, rounded to three significant figures.\nFrom Task 2, ULA = $4.2885$ bpm.\nRounding to three significant figures gives $4.29$ bpm.", "answer": "$$\n\\boxed{4.29}\n$$", "id": "4822398"}, {"introduction": "Once a sensor is validated, its data can be used to power diagnostic algorithms, such as an automated fall detector. The next critical step is to evaluate the performance of this algorithm. This exercise guides you through the calculation of fundamental diagnostic accuracy metrics, including sensitivity, specificity, Positive Predictive Value (PPV), and Negative Predictive Value (NPV). More importantly, it demonstrates the profound impact of event prevalence on an algorithm's real-world utility, a concept rooted in Bayes' theorem [@problem_id:4822391]. Understanding this relationship is essential for interpreting test results correctly and for recognizing why a good algorithm might perform brilliantly in a high-risk rehabilitation ward but generate an overwhelming number of false alarms in a low-risk home setting.", "problem": "A wearable accelerometer fall detector deployed in an Internet of Things (IoT) network is evaluated on a curated dataset of $2000$ annotated $5$-second monitoring windows collected from community-dwelling adults. Ground truth labels are assigned by synchronized camera review. Among these $2000$ windows, $100$ contain a true fall ($Y=1$) and $1900$ contain no fall ($Y=0$). The detector issues an alert ($\\hat{Y}=1$) when it believes a fall occurred. In the validation set, the device correctly alerts on $85$ true falls and misses $15$ true falls. On non-fall windows, it generates $190$ false alerts and correctly suppresses alerts on $1710$ windows. Thus, the confusion matrix counts are: True Positive ($\\mathrm{TP}$) $=85$, False Negative ($\\mathrm{FN}$) $=15$, False Positive ($\\mathrm{FP}$) $=190$, True Negative ($\\mathrm{TN}$) $=1710$.\n\nUsing only core definitions from diagnostic test evaluation and conditional probability, do the following:\n\n1. Compute the empirical sensitivity $\\mathrm{Se}$, specificity $\\mathrm{Sp}$, Positive Predictive Value (PPV), and Negative Predictive Value (NPV) from the given counts. All intermediate quantities must be expressed as decimals or exact fractions.\n\n2. Starting from the definitions of conditional probability and Bayesâ€™ rule, derive an analytic expression for $\\mathrm{PPV}$ as a function of disease (fall) prevalence $p=\\Pr(Y=1)$, sensitivity $\\mathrm{Se}=\\Pr(\\hat{Y}=1\\mid Y=1)$, and specificity $\\mathrm{Sp}=\\Pr(\\hat{Y}=0\\mid Y=0)$, without introducing any shortcut formulas.\n\n3. Using your derived expression and the empirical $\\mathrm{Se}$ and $\\mathrm{Sp}$ from part $1$, compute $\\mathrm{PPV}$ under two realistic deployment scenarios:\n   - Home monitoring with low prevalence $p_{\\mathrm{home}}=0.005$.\n   - Rehabilitation ward monitoring with higher prevalence $p_{\\mathrm{rehab}}=0.05$.\n\n4. Briefly interpret how the change in prevalence affects the clinical usefulness of alerts, focusing on the burden of false alerts versus true alerts across the two scenarios.\n\nFinally, report only the ratio $$R=\\frac{\\mathrm{PPV}(p_{\\mathrm{rehab}})}{\\mathrm{PPV}(p_{\\mathrm{home}})}$$ as your final numeric answer. Round $R$ to three significant figures. Do not include units; express all probabilities as decimals or fractions, not with a percentage sign.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Total monitoring windows: $N=2000$.\n- True fall windows ($Y=1$): $N_{Y=1}=100$.\n- No fall windows ($Y=0$): $N_{Y=0}=1900$.\n- Detector alert: $\\hat{Y}=1$.\n- No alert: $\\hat{Y}=0$.\n- True Positive ($\\mathrm{TP}$): $85$.\n- False Negative ($\\mathrm{FN}$): $15$.\n- False Positive ($\\mathrm{FP}$): $190$.\n- True Negative ($\\mathrm{TN}$): $1710$.\n- Prevalence for home monitoring scenario: $p_{\\mathrm{home}}=0.005$.\n- Prevalence for rehabilitation ward scenario: $p_{\\mathrm{rehab}}=0.05$.\n\n**Step 2: Validate Using Extracted Givens**\nThe provided data are self-consistent. The sum of counts for true conditions matches the totals:\n- Total fall windows: $\\mathrm{TP} + \\mathrm{FN} = 85 + 15 = 100$, which matches the given $N_{Y=1}$.\n- Total non-fall windows: $\\mathrm{FP} + \\mathrm{TN} = 190 + 1710 = 1900$, which matches the given $N_{Y=0}$.\n- Total windows: $100 + 1900 = 2000$, which matches the given $N$.\nThe problem is based on standard, well-defined concepts in diagnostic test evaluation and probability theory. The scenario is a realistic application within medical informatics. The problem is complete, consistent, and well-posed. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution is warranted.\n\n**Part 1: Computation of Empirical Performance Metrics**\n\nWe compute the sensitivity ($\\mathrm{Se}$), specificity ($\\mathrm{Sp}$), Positive Predictive Value ($\\mathrm{PPV}$), and Negative Predictive Value ($\\mathrm{NPV}$) from the given confusion matrix counts.\n\nSensitivity is the proportion of true positives that are correctly identified:\n$$ \\mathrm{Se} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} = \\frac{85}{85 + 15} = \\frac{85}{100} = 0.85 $$\n\nSpecificity is the proportion of true negatives that are correctly identified:\n$$ \\mathrm{Sp} = \\frac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FP}} = \\frac{1710}{1710 + 190} = \\frac{1710}{1900} = \\frac{171}{190} = 0.9 $$\n\nPositive Predictive Value is the proportion of positive test results that are true positives. This is the value for the given dataset, where the prevalence is $p_{\\mathrm{data}} = \\frac{100}{2000} = 0.05$:\n$$ \\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} = \\frac{85}{85 + 190} = \\frac{85}{275} = \\frac{17}{55} \\approx 0.3091 $$\n\nNegative Predictive Value is the proportion of negative test results that are true negatives:\n$$ \\mathrm{NPV} = \\frac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FN}} = \\frac{1710}{1710 + 15} = \\frac{1710}{1725} = \\frac{114}{115} \\approx 0.9913 $$\n\n**Part 2: Derivation of PPV Expression from First Principles**\n\nThe Positive Predictive Value ($\\mathrm{PPV}$) is the conditional probability of having the condition ($Y=1$) given a positive test result ($\\hat{Y}=1$).\n$$ \\mathrm{PPV} = \\Pr(Y=1 \\mid \\hat{Y}=1) $$\nUsing Bayes' rule, which states $\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}$, we set $A$ to be the event $Y=1$ and $B$ to be the event $\\hat{Y}=1$:\n$$ \\mathrm{PPV} = \\frac{\\Pr(\\hat{Y}=1 \\mid Y=1) \\Pr(Y=1)}{\\Pr(\\hat{Y}=1)} $$\nThe terms in this expression are defined as:\n- $\\Pr(Y=1)$ is the prevalence of the condition, denoted as $p$.\n- $\\Pr(\\hat{Y}=1 \\mid Y=1)$ is the probability of a positive test given the condition is present, which is the definition of sensitivity, $\\mathrm{Se}$.\n\nThe denominator, $\\Pr(\\hat{Y}=1)$, is the overall probability of a positive test. We expand this using the law of total probability:\n$$ \\Pr(\\hat{Y}=1) = \\Pr(\\hat{Y}=1 \\mid Y=1)\\Pr(Y=1) + \\Pr(\\hat{Y}=1 \\mid Y=0)\\Pr(Y=0) $$\nWe can identify the components of this expansion:\n- $\\Pr(Y=0) = 1 - \\Pr(Y=1) = 1-p$.\n- $\\Pr(\\hat{Y}=1 \\mid Y=0)$ is the probability of a positive test when the condition is absent (a false positive). This is equal to $1 - \\mathrm{Sp}$, since specificity $\\mathrm{Sp} = \\Pr(\\hat{Y}=0 \\mid Y=0)$.\n\nSubstituting these definitions back into the expansion for $\\Pr(\\hat{Y}=1)$:\n$$ \\Pr(\\hat{Y}=1) = (\\mathrm{Se} \\cdot p) + ((1 - \\mathrm{Sp}) \\cdot (1-p)) $$\nFinally, substituting all components back into the Bayes' rule expression for $\\mathrm{PPV}$, we arrive at the desired analytic expression:\n$$ \\mathrm{PPV}(p, \\mathrm{Se}, \\mathrm{Sp}) = \\frac{\\mathrm{Se} \\cdot p}{(\\mathrm{Se} \\cdot p) + (1 - \\mathrm{Sp})(1 - p)} $$\n\n**Part 3: Calculation of PPV for Deployment Scenarios**\n\nWe use the derived formula and the empirical values $\\mathrm{Se} = 0.85$ and $\\mathrm{Sp} = 0.9$ (so $1-\\mathrm{Sp}=0.1$) to calculate $\\mathrm{PPV}$ for the two specified prevalences.\n\nScenario A: Home monitoring with low prevalence, $p_{\\mathrm{home}} = 0.005$.\n$$ \\mathrm{PPV}(p_{\\mathrm{home}}) = \\frac{0.85 \\cdot 0.005}{(0.85 \\cdot 0.005) + (0.1 \\cdot (1 - 0.005))} $$\n$$ \\mathrm{PPV}(p_{\\mathrm{home}}) = \\frac{0.00425}{0.00425 + (0.1 \\cdot 0.995)} = \\frac{0.00425}{0.00425 + 0.0995} = \\frac{0.00425}{0.10375} $$\nAs an exact fraction, this is $\\frac{425}{103750} = \\frac{17}{415}$.\nAs a decimal, $\\mathrm{PPV}(p_{\\mathrm{home}}) \\approx 0.04096$.\n\nScenario B: Rehabilitation ward with higher prevalence, $p_{\\mathrm{rehab}} = 0.05$.\n$$ \\mathrm{PPV}(p_{\\mathrm{rehab}}) = \\frac{0.85 \\cdot 0.05}{(0.85 \\cdot 0.05) + (0.1 \\cdot (1 - 0.05))} $$\n$$ \\mathrm{PPV}(p_{\\mathrm{rehab}}) = \\frac{0.0425}{0.0425 + (0.1 \\cdot 0.95)} = \\frac{0.0425}{0.0425 + 0.095} = \\frac{0.0425}{0.1375} = \\frac{17}{55} $$\nAs a decimal, $\\mathrm{PPV}(p_{\\mathrm{rehab}}) \\approx 0.30909$. This value correctly matches the empirical PPV calculated in Part 1, as the prevalence in the original dataset was $p_{\\mathrm{data}} = \\frac{100}{2000} = 0.05$.\n\n**Part 4: Interpretation of Prevalence Effect**\n\nThe dramatic difference between $\\mathrm{PPV}(p_{\\mathrm{home}}) \\approx 0.041$ and $\\mathrm{PPV}(p_{\\mathrm{rehab}}) \\approx 0.309$ highlights the profound impact of event prevalence on the clinical utility of a diagnostic test.\n- In the low-prevalence home setting ($p=0.005$), only about $4.1\\%$ of the alerts are true falls. This means that for every $100$ alerts, about $96$ are false alarms. The ratio of false alerts to true alerts is approximately $\\frac{1-0.041}{0.041} \\approx 23.4$. A caregiver would need to respond to over $24$ alerts on average to witness one true fall. This high burden of false positives can lead to \"alert fatigue,\" where users begin to ignore or distrust the system, potentially causing a true fall to be missed.\n- In the higher-prevalence rehabilitation ward ($p=0.05$), about $30.9\\%$ of alerts are true falls. The ratio of false alerts to true alerts is $\\frac{1-0.309}{0.309} \\approx 2.2$. Here, a caregiver responds to approximately $3$ alerts to witness one true fall. While still significant, this false-alert burden is far more manageable and indicates that the alerts are substantially more reliable and clinically useful in this context.\nIn summary, a test with fixed sensitivity and specificity is far more effective and trustworthy when applied to a population with a higher prevalence of the condition.\n\n**Final Calculation: Ratio R**\n\nThe problem asks for the ratio $R=\\frac{\\mathrm{PPV}(p_{\\mathrm{rehab}})}{\\mathrm{PPV}(p_{\\mathrm{home}})}$.\nUsing the exact fractional values calculated in Part 3:\n$$ R = \\frac{17/55}{17/415} = \\frac{17}{55} \\cdot \\frac{415}{17} = \\frac{415}{55} $$\nSimplifying the fraction:\n$$ R = \\frac{83 \\cdot 5}{11 \\cdot 5} = \\frac{83}{11} $$\nAs a decimal, $R = 7.545454...$. Rounding to three significant figures gives $7.55$.", "answer": "$$\\boxed{7.55}$$", "id": "4822391"}, {"introduction": "Data from wearable sensors in real-world IoT environments are rarely perfect; signal dropouts and missing data are common challenges. While imputation methods can fill these gaps, they are not without consequences. This practice explores this critical issue by asking you to use linear interpolation to fill a gap in a heart rate time series and then analyze its effect on a key physiological metric, Heart Rate Variability (HRV). By computing the Root Mean Square of Successive Differences (RMSSD), you will discover firsthand how a seemingly straightforward data processing step can introduce a systematic bias, artificially smoothing the data and masking the very physiological fluctuations you aim to measure [@problem_id:4822393]. This is a crucial lesson in data hygiene and the importance of critically assessing the impact of every stage of your analysis pipeline.", "problem": "A wrist-worn wearable heart rate monitor in an Internet of Things (IoT) setting streams heart rate data at a sampling rate of $1$ Hz, meaning one sample per second at times $t = 0, 1, 2, \\dots$ seconds. Due to a short wireless dropout, there is a $10$-second gap with no data between two valid samples: immediately before the gap, at time $t_0 = 0$ seconds, the heart rate is $70$ beats per minute (bpm), and immediately after the gap, at time $t_1 = 11$ seconds, the heart rate is $80$ bpm. Assume the heart rate varies linearly between these two endpoint samples, and use linear interpolation (the unique affine function passing through the two endpoints) to impute the missing values at times $t = 1, 2, \\dots, 10$ seconds. Using the resulting $12$-sample sequence over $t = 0, 1, \\dots, 11$ seconds, compute the Root Mean Square of Successive Differences (RMSSD) of the heart rate time series for this $12$-second segment. Express the final RMSSD in beats per minute (bpm) and round your answer to four significant figures. In your reasoning, starting from core definitions, explain why linear interpolation can introduce bias in variability metrics of heart rate time series relative to the unknown true physiological dynamics.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The scenario is a realistic application of signal processing and data imputation in medical informatics.\n\nThe problem requires us to perform two main tasks: first, to compute the Root Mean Square of Successive Differences (RMSSD) for a heart rate time series with missing data imputed via linear interpolation, and second, to explain the bias introduced by this imputation method on variability metrics.\n\nFirst, we establish the time series of heart rate values. We are given two data points: at time $t_0 = 0$ seconds, the heart rate is $H(t_0) = 70$ beats per minute (bpm), and at time $t_1 = 11$ seconds, the heart rate is $H(t_1) = 80$ bpm. The data are sampled at $1$ Hz, so we have a sequence of points at integer times $t = 0, 1, 2, \\dots, 11$.\n\nThe problem specifies using a unique affine function, a form of linear interpolation, to determine the heart rate $H(t)$ for the times between $t_0$ and $t_1$. An affine function has the form $H(t) = mt + c$, where $m$ is the slope and $c$ is the y-intercept.\n\nThe slope $m$ is calculated from the two given points $(t_0, H(t_0))$ and $(t_1, H(t_1))$:\n$$ m = \\frac{H(t_1) - H(t_0)}{t_1 - t_0} = \\frac{80 - 70}{11 - 0} = \\frac{10}{11} $$\nThe units of the slope are bpm/second.\n\nThe intercept $c$ is the value of the function at $t=0$, which is given as $H(0) = 70$ bpm.\nThus, the linear function describing the heart rate over this interval is:\n$$ H(t) = \\frac{10}{11}t + 70 $$\n\nWe are asked to consider the $12$-sample sequence from $t=0$ to $t=11$. Let this sequence be denoted by $\\{H_i\\}_{i=0}^{11}$, where $H_i = H(i)$. The values in this sequence are generated by the function above for $i = 0, 1, 2, \\dots, 11$.\n\nNext, we compute the RMSSD of this time series. The RMSSD is a standard measure of heart rate variability, defined as the square root of the mean of the squared successive differences in the time series. For a series of $N$ samples, $\\{H_i\\}_{i=0}^{N-1}$, the formula is:\n$$ \\text{RMSSD} = \\sqrt{\\frac{1}{N-1} \\sum_{i=0}^{N-2} (H_{i+1} - H_i)^2} $$\nIn our case, the number of samples is $N = 12$. The index $i$ runs from $0$ to $10$.\n\nLet's compute the successive difference, $H_{i+1} - H_i$. Since our time series is based on a linear function, this difference will be constant.\n$$ H_{i+1} - H_i = \\left(\\frac{10}{11}(i+1) + 70\\right) - \\left(\\frac{10}{11}i + 70\\right) $$\n$$ H_{i+1} - H_i = \\frac{10}{11}(i+1) - \\frac{10}{11}i = \\frac{10}{11}i + \\frac{10}{11} - \\frac{10}{11}i = \\frac{10}{11} $$\nThe difference between any two consecutive samples is constant and equal to the slope $m$, which is $\\frac{10}{11}$ bpm.\n\nNow we can substitute this into the RMSSD formula. The summation runs from $i=0$ to $N-2 = 12-2 = 10$. There are $11$ terms in the sum.\n$$ \\text{RMSSD} = \\sqrt{\\frac{1}{12-1} \\sum_{i=0}^{10} \\left(\\frac{10}{11}\\right)^2} $$\n$$ \\text{RMSSD} = \\sqrt{\\frac{1}{11} \\sum_{i=0}^{10} \\frac{100}{121}} $$\nSince the term inside the summation is constant, the sum is simply the number of terms ($11$) multiplied by the constant value:\n$$ \\text{RMSSD} = \\sqrt{\\frac{1}{11} \\left(11 \\times \\frac{100}{121}\\right)} $$\n$$ \\text{RMSSD} = \\sqrt{\\frac{100}{121}} = \\frac{\\sqrt{100}}{\\sqrt{121}} = \\frac{10}{11} $$\nThe exact value of the RMSSD is $\\frac{10}{11}$ bpm. To provide the final answer, we must compute the numerical value and round it to four significant figures.\n$$ \\frac{10}{11} \\approx 0.909090... $$\nRounding to four significant figures gives $0.9091$.\n\nFinally, we address the conceptual part of the problem: why linear interpolation can introduce bias in variability metrics.\nRMSSD is a time-domain metric designed to quantify short-term, high-frequency variations in a physiological signal like heart rate. A healthy cardiovascular system exhibits complex, non-linear fluctuations from one moment to the next, driven by the autonomic nervous system (e.g., respiratory sinus arrhythmia). This inherent physiological variability is a key indicator of health.\n\nLinear interpolation, by its definition, replaces the unknown, potentially complex signal segment with a straight line. A straight line is an infinitely smooth function with a constant first derivative. In the context of our discrete time series, this means the difference between successive samples is constant, as we calculated ($H_{i+1} - H_i = \\frac{10}{11}$).\n\nThis imposition of deterministic smoothness has a critical consequence: it artificially suppresses or entirely removes the natural, stochastic, high-frequency fluctuations that would have been present in the true heart rate signal during the $10$-second data gap. The calculated RMSSD of $\\approx 0.9091$ bpm is a very low value, indicating minimal variability. It merely reflects the constant slope of the interpolation line, not any underlying physiological process. The true heart rate during the dropout would almost certainly have exhibited much greater moment-to-moment variation, resulting in a higher true RMSSD.\n\nTherefore, using linear interpolation to impute missing data in a heart rate time series introduces a systematic negative bias in variability metrics like RMSSD. It leads to an underestimation of the true physiological variability because it replaces a segment of a naturally noisy, complex signal with a simple, noise-free, and predictable pattern. This bias can lead to incorrect clinical or physiological interpretations if not properly accounted for. More advanced imputation techniques may attempt to inject realistic noise or follow more complex patterns, but all imputation methods introduce some level of artifact and potential bias.", "answer": "$$\n\\boxed{0.9091}\n$$", "id": "4822393"}]}