{"hands_on_practices": [{"introduction": "A core function of a Clinical Data Warehouse is to provide a longitudinal, historical view of a patient. This practice explores Slowly Changing Dimension (SCD) Type 2, a key data modeling technique for tracking changes in patient attributes like insurance coverage over time. You will apply the principles of \"as-of\" temporal queries to resolve a realistic data conflict, demonstrating how data warehouses can provide a single, authoritative version of the truth at any given point in history. [@problem_id:4826408]", "problem": "A Clinical Data Warehouse (CDW) maintains a Slowly Changing Dimension Type 2 (SCD Type 2) dimension for payer insurance coverage. Each row represents a version of a patient’s insurance attributes, with business-effective validity captured by a half-open interval $\\left[\\text{effective\\_start}, \\text{effective\\_end}\\right)$, where $\\text{effective\\_end}$ may be open-ended. The Clinical Data Warehouse enforces homogeneity of natural keys and aims for non-overlapping business-valid intervals per natural key, but real-world ingestion defects may introduce overlapping intervals that must be reconciled during “as-of” queries.\n\nFundamental facts you may assume:\n- In SCD Type 2, the business-valid interval is modeled as $\\left[\\text{effective\\_start}, \\text{effective\\_end}\\right)$, meaning $\\text{effective\\_start} \\leq t < \\text{effective\\_end}$ indicates validity at time $t$.\n- Open-ended validity is represented by $\\text{effective\\_end} = +\\infty$.\n- The “as-of” operator returns the single row that is business-valid at time $t$.\n\nConsider one patient with natural key $k = 98765$. The dimension has the following rows (each bullet is one row):\n- Surrogate key $s = 101$, version $v = 1$, $\\text{effective\\_start} = 2023\\text{-}01\\text{-}01$, $\\text{effective\\_end} = 2023\\text{-}12\\text{-}31$, plan name “Medicaid”, plan code $3$, $\\text{etl\\_load\\_ts} = 2023\\text{-}01\\text{-}02\\ \\text{T}\\ 04{:}00$.\n- Surrogate key $s = 102$, version $v = 2$, $\\text{effective\\_start} = 2024\\text{-}01\\text{-}01$, $\\text{effective\\_end} = +\\infty$, plan name “Commercial A”, plan code $1$, $\\text{etl\\_load\\_ts} = 2024\\text{-}01\\text{-}02\\ \\text{T}\\ 05{:}00$.\n- Surrogate key $s = 135$, version $v = 3$, $\\text{effective\\_start} = 2024\\text{-}03\\text{-}15$, $\\text{effective\\_end} = +\\infty$, plan name “Commercial A”, plan code $1$, $\\text{etl\\_load\\_ts} = 2024\\text{-}03\\text{-}16\\ \\text{T}\\ 05{:}52$.\n\nLet the as-of time be $t = 2024\\text{-}06\\text{-}15$. Define the requested output as a single integer code $c^{\\ast}$ that equals the plan code of the patient’s insurance in force at time $t$. If no row is valid at time $t$, define $c^{\\ast} = 0$.\n\nTasks:\n- Using only the foundational SCD Type 2 semantics stated above, determine $c^{\\ast}$ at $t = 2024\\text{-}06\\text{-}15$.\n- In your reasoning, explicitly justify how overlapping intervals are handled in a principled, deterministic manner grounded in SCD Type 2 semantics and data governance, and show that your approach yields a unique $c^{\\ast}$ for this data.\n\nProvide the final value of $c^{\\ast}$ as a single integer. No rounding is needed, and no units are required.", "solution": "The problem is deemed valid. It is a well-posed problem within the domain of data warehousing and medical informatics. It is scientifically grounded in the established principles of Slowly Changing Dimensions (SCD), is internally consistent, and contains the necessary information to derive a unique, deterministic solution. The core task involves resolving an ambiguity—overlapping business-valid time intervals—by applying a standard data governance principle, which is a common and practical challenge in this field.\n\nThe problem requires determining the correct insurance plan code, denoted as $c^{\\ast}$, for a patient with natural key $k = 98765$ at the specific \"as-of\" time $t = 2024\\text{-}06\\text{-}15$. The business validity of each record is defined by the half-open interval $[\\text{effective\\_start}, \\text{effective\\_end})$. A record is considered valid at time $t$ if the condition $\\text{effective\\_start} \\leq t < \\text{effective\\_end}$ is met.\n\nWe are given three records for the patient with natural key $k = 98765$:\n\\begin{enumerate}\n    \\item Row $1$: surrogate key $s_1 = 101$, version $v_1 = 1$. Interval $[\\text{start}_1, \\text{end}_1) = [2023\\text{-}01\\text{-}01, 2023\\text{-}12\\text{-}31)$. Plan code $c_1 = 3$. ETL timestamp $\\text{ts}_1 = 2023\\text{-}01\\text{-}02\\ \\text{T}\\ 04{:}00$.\n    \\item Row $2$: surrogate key $s_2 = 102$, version $v_2 = 2$. Interval $[\\text{start}_2, \\text{end}_2) = [2024\\text{-}01\\text{-}01, +\\infty)$. Plan code $c_2 = 1$. ETL timestamp $\\text{ts}_2 = 2024\\text{-}01\\text{-}02\\ \\text{T}\\ 05{:}00$.\n    \\item Row $3$: surrogate key $s_3 = 135$, version $v_3 = 3$. Interval $[\\text{start}_3, \\text{end}_3) = [2024\\text{-}03\\text{-}15, +\\infty)$. Plan code $c_3 = 1$. ETL timestamp $\\text{ts}_3 = 2024\\text{-}03\\text{-}16\\ \\text{T}\\ 05{:}52$.\n\\end{enumerate}\n\nThe first step is to identify the set of rows, $R_{\\text{valid}}$, that are valid at the as-of time $t = 2024\\text{-}06\\text{-}15$. We test the validity condition for each row.\n\nFor Row $1$:\nThe interval is $[2023\\text{-}01\\text{-}01, 2023\\text{-}12\\text{-}31)$. The condition is $2023\\text{-}01\\text{-}01 \\leq 2024\\text{-}06\\text{-}15 < 2023\\text{-}12\\text{-}31$. This is false, because $2024\\text{-}06\\text{-}15$ is not less than $2023\\text{-}12\\text{-}31$. Thus, Row $1$ is not valid at time $t$.\n\nFor Row $2$:\nThe interval is $[2024\\text{-}01\\text{-}01, +\\infty)$. The condition is $2024\\text{-}01\\text{-}01 \\leq 2024\\text{-}06\\text{-}15 < +\\infty$. This is true. Thus, Row $2$ is a candidate for the valid record at time $t$.\n\nFor Row $3$:\nThe interval is $[2024\\text{-}03\\text{-}15, +\\infty)$. The condition is $2024\\text{-}03\\text{-}15 \\leq 2024\\text{-}06\\text{-}15 < +\\infty$. This is true. Thus, Row $3$ is also a candidate for the valid record at time $t$.\n\nWe have identified two valid rows, $R_{\\text{valid}} = \\{\\text{Row } 2, \\text{Row } 3\\}$, for the as-of time $t$. This presents a conflict, as the definition of the \"as-of\" operator states that it returns a *single* row. The problem statement anticipates this, noting that \"ingestion defects may introduce overlapping intervals that must be reconciled\". A principled, deterministic resolution is required.\n\nIn data warehousing and data governance, when multiple records claim validity for the same business time due to data quality issues, the conflict is resolved by selecting the record that represents the most current or authoritative information. The ETL load timestamp, $\\text{etl\\_load\\_ts}$, provides the necessary metadata for this resolution. It records the system time when the information was ingested into the warehouse. A later timestamp signifies more recent information, which is assumed to supersede any earlier, conflicting information. This \"last write wins\" strategy is a standard principle for maintaining data integrity.\n\nTherefore, the deterministic rule for selecting the unique valid row $r^{\\ast}$ from the set of candidates $R_{\\text{valid}}$ is to choose the one with the maximum $\\text{etl\\_load\\_ts}$.\nFormally, $r^{\\ast} = \\arg\\max_{r_i \\in R_{\\text{valid}}}(\\text{ts}_i)$.\n\nLet us apply this rule to the two candidate rows:\n\\begin{itemize}\n    \\item Row $2$: $\\text{ts}_2 = 2024\\text{-}01\\text{-}02\\ \\text{T}\\ 05{:}00$\n    \\item Row $3$: $\\text{ts}_3 = 2024\\text{-}03\\text{-}16\\ \\text{T}\\ 05{:}52$\n\\end{itemize}\nComparing the timestamps, we find that $\\text{ts}_3 > \\text{ts}_2$. This means that the information in Row $3$ was loaded into the Clinical Data Warehouse after the information in Row $2$. Consequently, Row $3$ is the authoritative record for the period where their effective intervals overlap. Other metadata such as the version number ($v_3 > v_2$) and the surrogate key ($s_3 > s_2$) are consistent with this conclusion, but the `etl_load_ts` is the most direct and reliable arbiter.\n\nThe uniquely determined valid row is Row $3$. The problem asks for the plan code, $c^{\\ast}$, from this row. The plan code for Row $3$ is given as $c_3 = 1$.\n\nTherefore, the patient's insurance plan code in force at time $t = 2024\\text{-}06\\text{-}15$ is $1$.\nIf no row had been found valid, the answer would be $c^{\\ast} = 0$ per the problem definition, but this is not the case here.", "answer": "$$\n\\boxed{1}\n$$", "id": "4826408"}, {"introduction": "The adage \"garbage in, garbage out\" is especially critical in healthcare analytics, making data quality assessment a vital CDW function. This exercise introduces a quantitative approach to measuring data quality by combining multiple metrics—completeness, conformance, and plausibility—into a single, interpretable score. By calculating a weighted average, you will practice a fundamental method used by data governance bodies to monitor and maintain the trustworthiness of clinical data. [@problem_id:4826434]", "problem": "A large academic hospital maintains a Clinical Data Warehouse (CDW) that integrates data from multiple clinical domains. The hospital’s data governance committee evaluates data quality monthly using three normalized metrics on each domain: completeness, conformance, and plausibility. Completeness is defined as the fraction of required data elements present, conformance is the fraction of values adhering to schema and coding standards, and plausibility is the fraction of values falling within clinically credible ranges. Each metric is scored on the unit interval $[0,1]$.\n\nSuppose the committee has determined that, for a particular domain in the current month, the metric scores are completeness $C=0.92$, conformance $F=0.97$, and plausibility $P=0.99$. The committee further specifies relative importance weights for these metrics: completeness weight $w_C=0.5$, conformance weight $w_F=0.3$, and plausibility weight $w_P=0.2$, with weights interpreted as nonnegative trade-off coefficients that sum to $1$.\n\nStarting from foundational multi-attribute measurement principles appropriate to medical informatics evaluations—namely that the composite data quality score should satisfy (i) normalization and convexity with respect to metric scores on $[0,1]$, (ii) monotonicity in each metric, and (iii) additive independence across metrics—derive the general functional form of the composite data quality score consistent with these principles, and then compute its value for the domain given above. Express the final result as a decimal fraction. Round your answer to four significant figures.", "solution": "The goal is to derive a composite data quality score for three metrics—completeness, conformance, and plausibility—each normalized to the unit interval $[0,1]$, with relative importance weights $w_C$, $w_F$, and $w_P$ that are nonnegative and sum to $1$.\n\nWe begin from widely accepted foundations in multi-attribute measurement and utility theory as applied in medical informatics quality evaluation:\n\n1. Normalization and convexity with respect to metric scores on $[0,1]$: If each metric $X \\in \\{C,F,P\\}$ is scored in $[0,1]$ and the composite function $Q(C,F,P)$ is intended to produce a normalized score, then $Q$ should map $[0,1]^3$ into $[0,1]$ and respect convex combinations of equally preferred profiles. This motivates a functional form that is a convex combination of the component scores, ensuring the composite remains in $[0,1]$.\n\n2. Monotonicity in each metric: If any single metric improves while others are held constant, the overall data quality should not decrease. Formally, for each $X \\in \\{C,F,P\\}$, $Q$ should be nondecreasing in $X$. A linear form with nonnegative coefficients satisfies this property.\n\n3. Additive independence: The trade-offs among metrics do not depend on the levels of other metrics, so that preferences over mixtures can be represented additively. Under additive independence (and appropriate continuity and solvability conditions), standard multi-attribute utility theory yields an additive form where the composite is the sum of single-attribute contributions weighted by constants that reflect relative importance.\n\nUnder these principles, the unique (up to positive affine scaling and normalization) composite consistent with normalization on $[0,1]$ and weights that sum to $1$ is the weighted arithmetic mean:\n$$\nQ(C,F,P) = w_C\\,C + w_F\\,F + w_P\\,P,\n$$\nwith $w_C \\ge 0$, $w_F \\ge 0$, $w_P \\ge 0$, and $w_C + w_F + w_P = 1$.\n\nWe are given $C=0.92$, $F=0.97$, $P=0.99$, and $w_C=0.5$, $w_F=0.3$, $w_P=0.2$. Substituting into the derived form:\n$$\nQ = (0.5)(0.92) + (0.3)(0.97) + (0.2)(0.99).\n$$\nCompute term-by-term:\n- $(0.5)(0.92) = 0.46$,\n- $(0.3)(0.97) = 0.291$,\n- $(0.2)(0.99) = 0.198$.\n\nSum:\n$$\nQ = 0.46 + 0.291 + 0.198 = 0.949.\n$$\n\nTo satisfy the rounding instruction of four significant figures, we express $0.949$ to four significant figures as $0.9490$. The result is a dimensionless decimal fraction in $[0,1]$.", "answer": "$$\\boxed{0.9490}$$", "id": "4826434"}, {"introduction": "Unlocking the research potential of a CDW requires sharing data responsibly, with patient privacy as the highest priority. This problem delves into $k$-anonymity, a foundational method for de-identifying data by ensuring individuals are \"hidden in a crowd.\" You will step into the role of a data steward to analyze a dataset's equivalence classes and quantify its compliance with a specific privacy standard, a critical skill for the ethical secondary use of health data. [@problem_id:4826387]", "problem": "A Clinical Data Warehouse (CDW) aggregates patient-level data from multiple clinical systems. In a de-identified extract built for secondary use, the data steward treats the quasi-identifier set $\\{\\text{age}, \\text{sex}, \\text{three-digit ZIP code}\\}$ as the attributes used to form equivalence classes. By definition, $k$-anonymity requires that every record’s equivalence class (under these quasi-identifiers) contains at least $k$ records. Consider a CDW extract with $100{,}000$ records partitioned into $500$ equivalence classes under these quasi-identifiers. The class-size histogram is as follows: $320$ classes have size $220$, $150$ classes have size $196$, $20$ classes have size $8$, and $10$ classes have size $4$. Using only the core definitions of $k$-anonymity and equivalence classes, first reason from first principles to characterize which records violate $k$-anonymity for $k=10$ and why. Then, define $R(k)$ as the total number of records in the dataset that belong to equivalence classes that fail to meet $k$-anonymity, and compute $R(10)$ for the given histogram. Express your final answer as a single integer.", "solution": "The problem requires an analysis of a dataset's compliance with the principle of $k$-anonymity. We begin by establishing the fundamental definitions.\n\nAn equivalence class is a set of records that are indistinguishable from one another with respect to a defined set of attributes known as quasi-identifiers. In this problem, the quasi-identifier set is specified as $\\{\\text{age}, \\text{sex}, \\text{three-digit ZIP code}\\}$. All records within a single equivalence class share the same values for these three attributes.\n\nThe principle of $k$-anonymity states that a dataset is $k$-anonymous if every record in the dataset belongs to an equivalence class containing at least $k$ records. Let $s$ denote the size (i.e., the number of records) of an arbitrary equivalence class. For a dataset to satisfy $k$-anonymity, the condition $s \\ge k$ must hold for all equivalence classes in the dataset.\n\nA record violates $k$-anonymity if it is a member of an equivalence class for which the size $s$ is less than $k$. The question specifies a value of $k=10$. Therefore, any record belonging to an equivalence class of size $s < 10$ is considered to violate $10$-anonymity. These records are more susceptible to re-identification because they can only be linked to a small group of individuals.\n\nThe problem provides a histogram of equivalence class sizes for a dataset containing a total of $N=100,000$ records partitioned into $500$ classes:\n1. There are $n_1 = 320$ classes of size $s_1 = 220$.\n2. There are $n_2 = 150$ classes of size $s_2 = 196$.\n3. There are $n_3 = 20$ classes of size $s_3 = 8$.\n4. There are $n_4 = 10$ classes of size $s_4 = 4$.\n\nWe must now characterize which records violate $10$-anonymity by comparing each class size $s_i$ to the required minimum size $k=10$.\n\nFor the first group of classes, the size is $s_1 = 220$. Since $220 \\ge 10$, these $320$ classes satisfy the $10$-anonymity criterion. The records within these classes are not in violation.\n\nFor the second group of classes, the size is $s_2 = 196$. Since $196 \\ge 10$, these $150$ classes also satisfy the $10$-anonymity criterion. The records within these classes are not in violation.\n\nFor the third group of classes, the size is $s_3 = 8$. Since $8 < 10$, these $20$ classes fail to meet the $10$-anonymity criterion. Consequently, all records belonging to these classes are in violation of $10$-anonymity.\n\nFor the fourth group of classes, the size is $s_4 = 4$. Since $4 < 10$, these $10$ classes also fail to meet the $10$-anonymity criterion. All records belonging to these classes are in violation of $10$-anonymity.\n\nThe problem defines $R(k)$ as the total number of records in the dataset that belong to equivalence classes that fail to meet $k$-anonymity. Based on our analysis, the classes that fail to meet $10$-anonymity are those with sizes $s_3=8$ and $s_4=4$. To compute $R(10)$, we must sum the number of records in all such violating classes.\n\nThe total number of records in the $n_3 = 20$ classes of size $s_3=8$ is:\n$$N_{\\text{violating},1} = n_3 \\times s_3 = 20 \\times 8 = 160$$\n\nThe total number of records in the $n_4 = 10$ classes of size $s_4=4$ is:\n$$N_{\\text{violating},2} = n_4 \\times s_4 = 10 \\times 4 = 40$$\n\nThe total number of records $R(10)$ that violate $10$-anonymity is the sum of the records in all violating classes:\n$$R(10) = N_{\\text{violating},1} + N_{\\text{violating},2} = 160 + 40 = 200$$\n\nThus, there are a total of $200$ records in the dataset that belong to equivalence classes whose sizes are smaller than $10$.", "answer": "$$\n\\boxed{200}\n$$", "id": "4826387"}]}