{"hands_on_practices": [{"introduction": "The foundation of a Common Data Model is the use of a shared, standard vocabulary. This exercise guides you through the crucial process of 'vocabulary mapping,' where a diagnosis code from a source system, such as an International Classification of Diseases (ICD-10-CM) code, is translated into its equivalent standard concept in the Systematized Nomenclature of Medicine — Clinical Terms (SNOMED CT) [@problem_id:4829300]. Mastering this procedure is fundamental for ensuring that data from different sources are truly interoperable and semantically aligned.", "problem": "A health system is standardizing diagnosis data from the International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) to the Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) using the Systematized Nomenclature of Medicine — Clinical Terms (SNOMED CT) as the standard vocabulary for conditions. You are asked to resolve the standard SNOMED concept identifiers and describe the correct handling of mappings that yield multiple valid targets.\n\nFundamental base definitions and facts to use:\n- In the OMOP CDM, the `CONCEPT` table stores records with attributes including `concept_id` (OMOP internal identifier), `concept_code` (source vocabulary code), `vocabulary_id` (which vocabulary the concept belongs to), `standard_concept` (with `'S'` indicating standard concepts), `valid_start_date`, `valid_end_date`, and `invalid_reason` (null if valid).\n- The `CONCEPT_RELATIONSHIP` table stores relationships between concepts with attributes including `concept_id_1` (source), `concept_id_2` (target), `relationship_id`, `valid_start_date`, `valid_end_date`, and `invalid_reason` (null if valid).\n- The relationship `'Maps to'` expresses the equivalence mapping from a non-standard source concept to one or more standard target concepts. The direction is from `concept_id_1` (source) to `concept_id_2` (standard target). Standard condition concepts are in `vocabulary_id` = 'SNOMED' with `standard_concept` = 'S'.\n- Validity filtering requires `invalid_reason` = NULL and, for a given current time $T$, $valid_start_date \\le T  valid_end_date$.\n\nConsider the ICD-10-CM code `E11.9` (“Type 2 diabetes mellitus without complications”). You are provided with a consistent sample of concept and relationship records sufficient to determine the mapping:\n- `CONCEPT` entries:\n  - Source: `concept_id` = 1001, `concept_code` = 'E11.9', `vocabulary_id` = 'ICD10CM', `standard_concept` = NULL, `invalid_reason` = NULL, and its validity window covers the current time $T$.\n  - Target candidate 1: `concept_id` = 2001, `concept_code` = '44054006', `vocabulary_id` = 'SNOMED', `standard_concept` = 'S', `invalid_reason` = NULL, and its validity window covers $T$.\n  - Target candidate 2: `concept_id` = 2002, `concept_code` = '999999999', `vocabulary_id` = 'SNOMED', `standard_concept` = 'S', `invalid_reason` = 'D', and its validity window does not cover $T$.\n- `CONCEPT_RELATIONSHIP` entries:\n  - `concept_id_1` = 1001, `concept_id_2` = 2001, `relationship_id` = 'Maps to', `invalid_reason` = NULL, and its validity window covers $T$.\n  - `concept_id_1` = 1001, `concept_id_2` = 2002, `relationship_id` = 'Maps to', `invalid_reason` = 'D', and its validity window does not cover $T$.\n\nTask: Choose the option that correctly specifies the method to resolve the standard SNOMED target `concept_id` set for `E11.9` using `CONCEPT` and `CONCEPT_RELATIONSHIP`, identifies the correct resolved set for the sample data above, and states the correct handling when multiple valid `'Maps to'` targets exist.\n\nA. Filter `CONCEPT` to locate the ICD-10-CM source by matching `vocabulary_id` = 'ICD10CM' and `concept_code` = 'E11.9' to get `concept_id` = 1001. From `CONCEPT_RELATIONSHIP`, select records where `concept_id_1` = 1001, `relationship_id` = 'Maps to', `invalid_reason` = NULL, and the validity window covers $T$. Join those targets to `CONCEPT` and keep only `vocabulary_id` = 'SNOMED' with `standard_concept` = 'S' and valid at $T$. The resolved set for the sample is {2001}; if multiple valid targets exist, retain all and represent them as multiple standardized records (one per target) rather than arbitrarily choosing one.\n\nB. Use the `'Is a'` relationship from `CONCEPT_RELATIONSHIP` on the ICD-10-CM concept to find hierarchical parents, then choose the smallest `concept_id` among targets as the standard. The resolved set for the sample is {2002}; if multiple targets exist, pick the one with the lowest `concept_id` to ensure determinism.\n\nC. Match the ICD-10-CM code `E11.9` directly to the SNOMED `concept_code` `44054006` by string equality and treat that code as the OMOP `concept_id`. The resolved set for the sample is {44054006}; if multiple matches exist, select the most specific by longest string length.\n\nD. Use `'Maps to'` but join on `concept_id_2` back to `concept_id_1` to find targets; ignore `invalid_reason` and validity windows to avoid dropping historical mappings. The resolved set for the sample is {2001, 2002}; if multiple valid targets exist, collapse to a single target by choosing the one with highest prevalence in your data.", "solution": "The task is to determine the correct procedure for standardizing an International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) code to a Systematized Nomenclature of Medicine — Clinical Terms (SNOMED CT) concept within the Observational Medical Outcomes Partnership Common Data Model (OMOP CDM). This involves identifying the correct standard `concept_id`, applying the procedure to sample data for the code `E11.9`, and stating the general rule for handling one-to-many mappings.\n\nFirst, let us formalize the procedure based on the provided definitions and facts. Let $T$ be the current time.\n\n1.  **Identify the Source Concept**: We start with a source code, $c_{source}$, from a source vocabulary, $v_{source}$. In this problem, $c_{source} = 'E11.9'$ and $v_{source} = 'ICD10CM'$. We must find its corresponding `concept_id` in the `CONCEPT` table.\n    - We query the `CONCEPT` table for a record where `concept_code` = $c_{source}$ and `vocabulary_id` = $v_{source}$.\n    - According to the sample data, for `concept_code` = 'E11.9' and `vocabulary_id` = 'ICD10CM', we find the source concept with `concept_id` = 1001.\n    - The problem statement asserts this source concept is valid at time $T$, which aligns with the sample data (`invalid_reason` = NULL and its validity window covers $T$). The source concept is expected to be non-standard, which is consistent with `standard_concept` = NULL.\n\n2.  **Find and Filter 'Maps to' Relationships**: We use the source `concept_id` (which is 1001) to find mappings to standard concepts. The problem specifies that the equivalence mapping is `'Maps to'`.\n    - We query the `CONCEPT_RELATIONSHIP` table for records where `concept_id_1` = 1001 and `relationship_id` = 'Maps to'.\n    - The sample data provides two such relationships, targeting `concept_id_2` = 2001 and `concept_id_2` = 2002.\n    - We must apply the validity filtering rules to these relationships: `invalid_reason` = NULL and $valid\\_start\\_date \\le T  valid\\_end\\_date$.\n    - For the relationship to `concept_id_2` = 2001: `invalid_reason` = NULL and its validity window covers $T$. This relationship is valid.\n    - For the relationship to `concept_id_2` = 2002: `invalid_reason` = 'D' (typically 'Deprecated') and its validity window does not cover $T$. This relationship is invalid.\n    - After this step, the only candidate target concept is identified by `concept_id` = 2001.\n\n3.  **Validate the Target Concept(s)**: The final step is to verify that the target concept(s) obtained from the valid relationships are themselves valid, standard concepts in the target vocabulary. The target vocabulary is SNOMED, and standard concepts are marked with `standard_concept` = 'S'.\n    - We must check the record for `concept_id` = 2001 in the `CONCEPT` table.\n    - Sample data for `concept_id` = 2001: `vocabulary_id` = 'SNOMED', `standard_concept` = 'S', `invalid_reason` = NULL, and its validity window covers $T$. All conditions are met. Thus, `concept_id` = 2001 is a valid standard target.\n    - For completeness, we can check the invalid target `concept_id` = 2002. The sample data shows `invalid_reason` = 'D' and its validity window does not cover $T$. This concept is not valid, which provides a secondary reason for its exclusion.\n\n4.  **Determine the Resolved Set for the Sample**: Based on the step-by-step application of the rules, the only valid standard target for the source code `E11.9` (which is `concept_id` = 1001) is `concept_id` = 2001. Therefore, the resolved set is {2001}.\n\n5.  **General Handling of Multiple Valid Targets**: The `'Maps to'` relationship in the OMOP vocabulary can be one-to-many. If a single source concept maps to multiple target concepts, and all those relationships and target concepts are valid and standard, the correct procedure is to retain all of them. This is because the set of target concepts collectively represents the meaning of the source concept. Arbitrarily selecting one would lead to a loss of clinically relevant information. When loading data into a clinical data table like `CONDITION_OCCURRENCE`, this would result in creating one record for each valid standard target concept.\n\nNow we evaluate each option based on this derived correct procedure.\n\n**A. Filter `CONCEPT` to locate the ICD-10-CM source by matching `vocabulary_id` = 'ICD10CM' and `concept_code` = 'E11.9' to get `concept_id` = 1001. From `CONCEPT_RELATIONSHIP`, select records where `concept_id_1` = 1001, `relationship_id` = 'Maps to', `invalid_reason` = NULL, and the validity window covers $T$. Join those targets to `CONCEPT` and keep only `vocabulary_id` = 'SNOMED' with `standard_concept` = 'S' and valid at $T$. The resolved set for the sample is {2001}; if multiple valid targets exist, retain all and represent them as multiple standardized records (one per target) rather than arbitrarily choosing one.**\n- The described procedure is a perfect match for the correct methodology derived above, including all necessary filtering steps on both the relationship and the target concept.\n- The identified resolved set for the sample, {2001}, is correct.\n- The statement on handling multiple targets (retaining all) is correct and aligns with OMOP best practices.\n- **Verdict**: **Correct**.\n\n**B. Use the `'Is a'` relationship from `CONCEPT_RELATIONSHIP` on the ICD-10-CM concept to find hierarchical parents, then choose the smallest `concept_id` among targets as the standard. The resolved set for the sample is {2002}; if multiple targets exist, pick the one with the lowest `concept_id` to ensure determinism.**\n- This option incorrectly proposes using the `'Is a'` relationship instead of `'Maps to'`. The `'Is a'` relationship defines hierarchical structures, typically within a single vocabulary, not for mapping between different vocabularies.\n- The proposed tie-breaking rule (lowest `concept_id`) is arbitrary and not a valid OMOP convention.\n- The identified resolved set of {2002} is incorrect; this concept is explicitly defined as invalid in the sample data.\n- **Verdict**: **Incorrect**.\n\n**C. Match the ICD-10-CM code `E11.9` directly to the SNOMED `concept_code` `44054006` by string equality and treat that code as the OMOP `concept_id`. The resolved set for the sample is {44054006}; if multiple matches exist, select the most specific by longest string length.**\n- This option makes a fundamental error by attempting to match a `concept_code` from one vocabulary to a `concept_code` in another. The purpose of the OMOP vocabulary tables, specifically `CONCEPT` and `CONCEPT_RELATIONSHIP`, is to provide a structured way to perform these mappings via integer `concept_id`s.\n- It also confuses `concept_code` with `concept_id`. The resolved set {44054006} is a SNOMED code, not an OMOP `concept_id`.\n- The tie-breaking rule (\"longest string length\") is baseless.\n- **Verdict**: **Incorrect**.\n\n**D. Use `'Maps to'` but join on `concept_id_2` back to `concept_id_1` to find targets; ignore `invalid_reason` and validity windows to avoid dropping historical mappings. The resolved set for the sample is {2001, 2002}; if multiple valid targets exist, collapse to a single target by choosing the one with highest prevalence in your data.**\n- This option incorrectly suggests ignoring the validity filters (`invalid_reason` and date windows). These filters are crucial for ensuring that the mappings used are current and correct as defined by the vocabulary maintainers. Using deprecated or invalid mappings leads to incorrect standardization.\n- As a result of ignoring validity, it incorrectly includes the invalid concept 2002 in the resolved set.\n- The rule for handling multiple targets (collapsing based on data prevalence) is a research-specific choice, not a standard ETL procedure, and it leads to information loss. The standard approach requires deterministic mapping based on the vocabularies alone.\n- **Verdict**: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "4829300"}, {"introduction": "Once individual records are standardized, the next step is often to build higher-level features that capture a patient's journey over time. This practice demonstrates a core algorithm within the OMOP CDM: the generation of a `DRUG_ERA` from a series of `DRUG_EXPOSURE` records [@problem_id:4829283]. By applying a 'persistence window' to merge individual prescription fills, you will learn how to transform transactional data into a continuous variable representing an era of treatment, a critical feature for many clinical studies.", "problem": "In the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM), the table `DRUG_ERA` is derived from `DRUG_EXPOSURE`. A `DRUG_EXPOSURE` record with a start date and a days-supply defines an exposure interval from its start date through the inclusive end date computed from the days-supply. A `DRUG_ERA` is formed by merging `DRUG_EXPOSURE` intervals that either overlap or are separated by a gap that does not exceed a fixed persistence window. Consider a single patient with three `DRUG_EXPOSURE` records for the same ingredient, with start-day offsets relative to an index date of $0$, $25$, and $90$, each having $30$ days of supply. The persistence window is $30$ days.\n\nStarting from the definitions above and without invoking any shortcut formulas, derive the resulting `DRUG_ERA` intervals by:\n- constructing each `DRUG_EXPOSURE` interval from its start day and days-supply, and\n- applying the persistence-window rule to determine whether intervals should be merged into a single `DRUG_ERA`.\n\nExpress the final `DRUG_ERA` start and end day offsets as exact integers, ordered chronologically, and provide the four numbers $(s_{1}, e_{1}, s_{2}, e_{2})$ in a single row matrix. No rounding is required. Report day offsets relative to the index date as plain integers without units.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established definitions of the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM), a standard in medical informatics. The problem is well-posed, providing all necessary data and definitions to derive a unique, logical solution. The language is objective and the premises are consistent.\n\nThe task is to determine the `DRUG_ERA` intervals resulting from three `DRUG_EXPOSURE` records for a single patient and ingredient. The derivation must follow the fundamental rules of interval construction and merging as defined by the OMOP CDM.\n\nThe provided data are:\n- Three `DRUG_EXPOSURE` records with start-day offsets $s_1 = 0$, $s_2 = 25$, and $s_3 = 90$, relative to an index date.\n- Each record has a `days_supply` of $d = 30$ days.\n- The `persistence_window` for merging is $P = 30$ days.\n\nFirst, we construct the interval for each `DRUG_EXPOSURE` record. The exposure interval is inclusive of its start and end days. For a record with start day $s$ and a supply of $d$ days, the exposure covers the days from $s$ to $s + d - 1$. The end day, $e$, is therefore $e = s + d - 1$.\n\nLet $E_1$, $E_2$, and $E_3$ be the three `DRUG_EXPOSURE` intervals.\n\nFor the first record, with $s_1 = 0$ and $d = 30$:\nThe start day is $s_1 = 0$.\nThe end day is $e_1 = s_1 + d - 1 = 0 + 30 - 1 = 29$.\nSo, the first exposure interval is $E_1 = [0, 29]$.\n\nFor the second record, with $s_2 = 25$ and $d = 30$:\nThe start day is $s_2 = 25$.\nThe end day is $e_2 = s_2 + d - 1 = 25 + 30 - 1 = 54$.\nSo, the second exposure interval is $E_2 = [25, 54]$.\n\nFor the third record, with $s_3 = 90$ and $d = 30$:\nThe start day is $s_3 = 90$.\nThe end day is $e_3 = s_3 + d - 1 = 90 + 30 - 1 = 119$.\nSo, the third exposure interval is $E_3 = [90, 119]$.\n\nNext, we apply the `DRUG_ERA` merging logic. A `DRUG_ERA` is formed by merging exposure intervals that either overlap or are separated by a gap less than or equal to the persistence window. The process is executed chronologically.\n\nWe begin with the first exposure interval, $E_1 = [0, 29]$, as the initial fragment of our first drug era. Let the current era under construction be $Era_{current}$. Initially, $Era_{current} = [0, 29]$.\n\nWe then consider the next exposure, $E_2 = [25, 54]$. We must determine if $E_2$ should be merged with $Era_{current}$. Two intervals, $[s_A, e_A]$ and $[s_B, e_B]$ with $s_A \\le s_B$, are merged if the gap between them, calculated as $g = s_B - e_A - 1$, satisfies the condition $g \\le P$. A negative or zero gap indicates overlap or contiguity, which always results in merging.\n\nThe start of $E_2$ is $s_2 = 25$, and the end of $Era_{current}$ is $e_{current} = 29$.\nThe gap is $g_{1,2} = s_2 - e_{current} - 1 = 25 - 29 - 1 = -5$.\nThe persistence window is $P=30$. The condition for merging is $g_{1,2} \\le P$, which is $-5 \\le 30$. This is true.\nTherefore, $E_2$ is merged with the current era. The new merged era spans from the start of the first interval to the end of the second interval.\nThe updated $Era_{current}$ becomes $[s_1, e_2] = [0, 54]$.\n\nNow, we consider the next exposure, $E_3 = [90, 119]$. We check if $E_3$ should be merged with the current era, $Era_{current} = [0, 54]$.\nThe start of $E_3$ is $s_3 = 90$, and the end of $Era_{current}$ is $e_{current} = 54$.\nThe gap is $g_{2,3} = s_3 - e_{current} - 1 = 90 - 54 - 1 = 35$.\nWe check the merging condition: $g_{2,3} \\le P$, which is $35 \\le 30$. This is false.\nSince the gap of $35$ days exceeds the persistence window of $30$ days, $E_3$ is not merged with the current era.\n\nThis means the first drug era is finalized. Its interval is $Era_1 = [0, 54]$.\nA new drug era, $Era_2$, is initiated starting with the non-merging exposure, $E_3$. Thus, $Era_2 = E_3 = [90, 119]$.\nSince there are no more exposure records, the second drug era, $Era_2$, is also finalized.\n\nThe resulting `DRUG_ERA` intervals are:\n- $Era_1$: start day $0$, end day $54$.\n- $Era_2$: start day $90$, end day $119$.\n\nThe problem requires the final answer as a single row matrix containing the four numbers $(s_{Era1}, e_{Era1}, s_{Era2}, e_{Era2})$. These numbers are $(0, 54, 90, 119)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  54  90  119\n\\end{pmatrix}\n}\n$$", "id": "4829283"}, {"introduction": "The ultimate power of a Common Data Model is its ability to facilitate large-scale, multi-institutional research. This exercise puts your skills to the test in a scenario that simulates a federated analysis, where standardized data from different sites are aggregated to answer a key epidemiological question [@problem_id:4829313]. By calculating an incidence rate from pooled data, you will see firsthand how the interoperability achieved through a CDM enables researchers to combine statistical power and generate more robust scientific evidence.", "problem": "A multi-institutional clinical research network has standardized its electronic health records into the Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) and uses Informatics for Integrating Biology and the Bedside (i2b2) to define and distribute a cohort specification for incident cases of a condition across sites. Each site contributes its count of incident cases and accumulated person-time, computed by excluding prevalent cases at cohort entry and summing observation time until first incident event, censoring, or end of observation. After quality control and OMOP vocabulary harmonization, the network aggregates the data, yielding a total of 150 incident cases over 12,500 person-years of follow-up.\n\nStarting from the definition of the incidence rate as events per unit person-time and assuming incident events follow a Poisson process with a constant hazard over time, derive the estimator for the incidence rate and its approximate sampling variability using properties of the Poisson distribution. Using these derivations, compute the $95\\%$ normal-approximation margin-of-error for the incidence rate when expressed per 1,000 person-years, based on the aggregated data 150 and 12,500.\n\nRound your final numerical answer to four significant figures. Express the margin-of-error in units of “per 1,000 person-years.” The final answer must be a single number.", "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n- Data Model: Observational Medical Outcomes Partnership Common Data Model (OMOP CDM).\n- Cohort Definition Tool: Informatics for Integrating Biology and the Bedside (i2b2).\n- Study Design Parameter: Incident cases of a condition.\n- Aggregated Data: A total of 150 incident cases.\n- Aggregated Data: A total of 12,500 person-years of follow-up.\n- Assumption: Incident events follow a Poisson process with a constant hazard over time.\n- Task: Derive the estimator for the incidence rate.\n- Task: Derive the approximate sampling variability for the incidence rate estimator.\n- Task: Compute the $95\\%$ normal-approximation margin-of-error for the incidence rate.\n- Unit for final result: Per 1,000 person-years.\n- Numerical Precision: Round the final answer to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly grounded in the fields of epidemiology and biostatistics. The use of OMOP and i2b2 are standard practices in clinical research informatics. The modeling of incident event counts over person-time using a Poisson distribution is a fundamental and widely accepted method in survival analysis and rate estimation.\n- **Well-Posed:** The problem is well-posed. It provides all necessary data (150 cases, 12,500 person-years) and a clear modeling assumption (Poisson process) to derive the requested quantities and compute a unique numerical answer.\n- **Objective:** The problem is stated in precise, objective language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\n- **Verdict:** The problem is valid. It is scientifically sound, self-contained, and well-posed.\n- **Action:** Proceed with the solution.\n\n**Derivation and Calculation**\n\nLet $N$ be the number of observed incident events and $T$ be the total accumulated person-time. The problem states that the events follow a Poisson process with a constant incidence rate (hazard) $\\lambda$. For a given observation period of total person-time $T$, the number of events $N$ follows a Poisson distribution with mean $\\mu = \\lambda T$.\nThe probability mass function for observing $N=n$ events is:\n$$ P(N=n | \\lambda) = \\frac{(\\lambda T)^n e^{-\\lambda T}}{n!} $$\nThis function, viewed as a function of the parameter $\\lambda$ for a fixed observation $n$, is the likelihood function $L(\\lambda; n)$. To find the maximum likelihood estimator (MLE) for $\\lambda$, we maximize the log-likelihood function, $\\ln L(\\lambda)$:\n$$ \\ln L(\\lambda) = \\ln\\left(\\frac{(\\lambda T)^n e^{-\\lambda T}}{n!}\\right) = n \\ln(\\lambda T) - \\lambda T - \\ln(n!) $$\n$$ \\ln L(\\lambda) = n \\ln(\\lambda) + n \\ln(T) - \\lambda T - \\ln(n!) $$\nTo find the maximum, we take the first derivative with respect to $\\lambda$ and set it to zero:\n$$ \\frac{d}{d\\lambda} \\ln L(\\lambda) = \\frac{n}{\\lambda} - T = 0 $$\nSolving for $\\lambda$ yields the MLE, denoted by $\\hat{\\lambda}$:\n$$ \\hat{\\lambda} = \\frac{n}{T} $$\nThis is the estimator for the incidence rate, which is the number of events per unit person-time, consistent with the problem's definition.\n\nNext, we derive the sampling variability of this estimator. The variance of an MLE can be approximated by the inverse of the Fisher information, $I(\\lambda)$. The Fisher information is the negative expected value of the second derivative of the log-likelihood function.\nThe second derivative is:\n$$ \\frac{d^2}{d\\lambda^2} \\ln L(\\lambda) = -\\frac{n}{\\lambda^2} $$\nThe Fisher information is:\n$$ I(\\lambda) = -E\\left[\\frac{d^2}{d\\lambda^2} \\ln L(\\lambda)\\right] = -E\\left[-\\frac{N}{\\lambda^2}\\right] = \\frac{E[N]}{\\lambda^2} $$\nSince $E[N] = \\lambda T$, we have:\n$$ I(\\lambda) = \\frac{\\lambda T}{\\lambda^2} = \\frac{T}{\\lambda} $$\nThe variance of the estimator $\\hat{\\lambda}$ is approximately the inverse of the Fisher information:\n$$ \\text{Var}(\\hat{\\lambda}) \\approx [I(\\lambda)]^{-1} = \\frac{\\lambda}{T} $$\nWe can estimate this variance by substituting the MLE $\\hat{\\lambda}$ for the true parameter $\\lambda$:\n$$ \\widehat{\\text{Var}}(\\hat{\\lambda}) = \\frac{\\hat{\\lambda}}{T} = \\frac{(n/T)}{T} = \\frac{n}{T^2} $$\nThe standard error (SE) of the estimator is the square root of the estimated variance:\n$$ \\text{SE}(\\hat{\\lambda}) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\lambda})} = \\sqrt{\\frac{n}{T^2}} = \\frac{\\sqrt{n}}{T} $$\nThe problem requires the margin of error for the incidence rate expressed per $1,000$ person-years. Let $\\lambda_{1k}$ be the incidence rate per $1,000$ person-years, so $\\lambda_{1k} = 1000 \\times \\lambda$.\nThe estimator for this rescaled rate is $\\hat{\\lambda}_{1k} = 1000 \\times \\hat{\\lambda} = 1000 \\frac{n}{T}$.\nThe standard error of this rescaled estimator is:\n$$ \\text{SE}(\\hat{\\lambda}_{1k}) = \\text{SE}(1000 \\times \\hat{\\lambda}) = 1000 \\times \\text{SE}(\\hat{\\lambda}) = 1000 \\frac{\\sqrt{n}}{T} $$\nThe problem specifies a $95\\%$ normal-approximation confidence interval. The margin of error (ME) for this interval is given by:\n$$ \\text{ME} = z_{\\alpha/2} \\times \\text{SE}(\\hat{\\lambda}_{1k}) $$\nFor a $95\\%$ confidence level, $\\alpha = 0.05$, so $\\alpha/2 = 0.025$. The corresponding critical value from the standard normal distribution is $z_{0.025} \\approx 1.96$.\n\nWe are given the aggregated data: $n = 150$ incident cases and $T = 12,500$ person-years.\nWe can now compute the margin of error:\n$$ \\text{ME} = 1.96 \\times 1000 \\times \\frac{\\sqrt{150}}{12,500} $$\n$$ \\text{ME} = 1.96 \\times \\frac{\\sqrt{150}}{12.5} $$\nTo simplify the calculation: $\\sqrt{150} = \\sqrt{25 \\times 6} = 5\\sqrt{6}$.\n$$ \\text{ME} = 1.96 \\times \\frac{5\\sqrt{6}}{12.5} = 1.96 \\times \\frac{5\\sqrt{6}}{25/2} = 1.96 \\times \\frac{10\\sqrt{6}}{25} = 1.96 \\times \\frac{2\\sqrt{6}}{5} $$\n$$ \\text{ME} = 1.96 \\times 0.4 \\sqrt{6} = 0.784 \\sqrt{6} $$\nNow we compute the numerical value:\n$$ \\text{ME} \\approx 0.784 \\times 2.44948974 \\dots $$\n$$ \\text{ME} \\approx 1.920400... $$\nThe problem requires rounding the final answer to four significant figures.\n$$ \\text{ME} \\approx 1.920 $$\nThis value is the margin of error for the incidence rate, expressed in units of cases per $1,000$ person-years.", "answer": "$$\\boxed{1.920}$$", "id": "4829313"}]}