{"hands_on_practices": [{"introduction": "Modern healthcare systems rely on strong encryption as a fundamental pillar for protecting the confidentiality of patient data. But how can we be sure that this digital lock is strong enough? This first practice explores the concept of computational security by tasking you with estimating the time required to break the widely used Advanced Encryption Standard (AES) through a brute-force search. By performing this calculation [@problem_id:4850559], you will gain a concrete appreciation for why modern cryptographic algorithms are considered secure against even the most powerful adversaries, compelling us to focus on other vectors of attack like key management and human factors.", "problem": "A regional hospital encrypts its electronic health record database using the Advanced Encryption Standard (AES) with a $128$-bit key. Consider an external adversary who attempts a naive exhaustive key search using a large rented compute cluster capable of $10^{12}$ independent key guesses per second. Assume the hospital’s AES keys are generated uniformly at random and that an exhaustive search enumerates candidate keys without repetition, so the position of the correct key in the enumeration is uniformly distributed over the $2^{128}$ possible keys. Starting only from fundamental definitions of search over a finite uniform space, expected value for a uniformly distributed position, and unit conversions, derive an analytical expression for the expected brute-force time and evaluate it numerically in years. Then, briefly justify from first principles how the magnitude of this time should inform realistic adversary models in healthcare (for example, financially motivated criminal groups, nation-state actors, cloud providers), assuming only classical computing hardware.\n\nRound your final numerical answer to three significant figures and express it in years. Your final answer must be a single number.", "solution": "The problem requires the derivation and evaluation of the expected time for a brute-force attack on a database encrypted with AES, using a $128$-bit key. It also asks for a justification of how this result informs realistic adversary models.\n\nFirst, we establish the size of the key space. An AES key of $128$ bits means there are $N = 2^{128}$ possible unique keys.\n\nThe problem states that the search for the correct key is exhaustive and without repetition, and the position of the correct key is uniformly distributed over the set of all possible keys. Let the random variable $X$ represent the number of keys that must be checked to find the correct one. $X$ follows a discrete uniform distribution on the set $\\{1, 2, 3, \\dots, N\\}$. The probability of finding the key at any specific position $i$ is $P(X=i) = \\frac{1}{N}$.\n\nThe expected number of guesses, $E[X]$, is the expected value of this distribution. Starting from the fundamental definition of expected value for a discrete random variable:\n$$E[X] = \\sum_{i=1}^{N} i \\cdot P(X=i)$$\nSubstituting the probability $P(X=i) = \\frac{1}{N}$:\n$$E[X] = \\sum_{i=1}^{N} i \\cdot \\frac{1}{N} = \\frac{1}{N} \\sum_{i=1}^{N} i$$\nThe sum of the first $N$ positive integers is given by the well-known formula $\\sum_{i=1}^{N} i = \\frac{N(N+1)}{2}$. Substituting this into the expression for $E[X]$:\n$$E[X] = \\frac{1}{N} \\left( \\frac{N(N+1)}{2} \\right) = \\frac{N+1}{2}$$\nFor a key space of size $N = 2^{128}$, the expected number of keys to check is $E[X] = \\frac{2^{128}+1}{2}$.\n\nThe adversary is capable of a guessing rate $R = 10^{12}$ keys per second. The expected time to find the key, $T_{\\text{expected}}$, is the expected number of guesses divided by the rate of guessing. The analytical expression for the expected time in seconds is:\n$$T_{\\text{expected}} = \\frac{E[X]}{R} = \\frac{(2^{128}+1)/2}{10^{12}} = \\frac{2^{128}+1}{2 \\times 10^{12}} \\text{ seconds}$$\nSince $2^{128}$ is an extremely large number, the $+1$ term is negligible, and the expression can be accurately approximated by $\\frac{2^{128}}{2 \\times 10^{12}} = \\frac{2^{127}}{10^{12}}$ seconds.\n\nTo evaluate this time in years, we must perform a unit conversion. The number of seconds in one year, $S_{\\text{year}}$, is calculated based on an average year length of $365.25$ days to account for leap years:\n$$S_{\\text{year}} = 365.25 \\frac{\\text{days}}{\\text{year}} \\times 24 \\frac{\\text{hours}}{\\text{day}} \\times 60 \\frac{\\text{minutes}}{\\text{hour}} \\times 60 \\frac{\\text{seconds}}{\\text{minute}} = 31,557,600 \\frac{\\text{seconds}}{\\text{year}}$$\nThis can be written as $S_{\\text{year}} \\approx 3.15576 \\times 10^{7}$ seconds/year.\n\nThe expected time in years, $T_{\\text{years}}$, is thus:\n$$T_{\\text{years}} = \\frac{T_{\\text{expected}}}{S_{\\text{year}}} = \\frac{2^{128}+1}{2 \\times 10^{12} \\times 31,557,600}$$\nNow, we perform the numerical evaluation. We use the approximation $2^{128} \\approx 3.4028 \\times 10^{38}$.\n$$T_{\\text{years}} \\approx \\frac{3.4028 \\times 10^{38}}{2 \\times 10^{12} \\times 3.15576 \\times 10^7} = \\frac{3.4028 \\times 10^{38}}{6.31152 \\times 10^{19}}$$\n$$T_{\\text{years}} \\approx 0.53914 \\times 10^{19} \\approx 5.39 \\times 10^{18} \\text{ years}$$\nRounding to three significant figures, the expected time is $5.39 \\times 10^{18}$ years.\n\nThis astronomically large timescale has direct implications for realistic adversary models in healthcare data security, based on first principles of computational feasibility:\nThe calculated expected time of roughly $5.4 \\times 10^{18}$ years is more than a hundred million times the current estimated age of the universe (approximately $1.38 \\times 10^{10}$ years). From this fundamental result, we can deduce that a brute-force attack against AES-128 is computationally infeasible for any adversary confined to the realm of classical computing, irrespective of their resources.\n\n1.  **Financially Motivated Criminal Groups:** These adversaries operate on short time horizons and are driven by return on investment. An attack that requires more time than the lifespan of the solar system is economically absurd. Therefore, from first principles, their efforts will necessarily be directed at weaker points in the security chain, such as social engineering (e.g., phishing hospital staff to obtain credentials), exploiting software vulnerabilities in the electronic health record system, or physically stealing devices and hoping for weak key management.\n\n2.  **Nation-State Actors:** While possessing vast resources and long-term strategic goals, even nation-states are bound by physical and computational limits. The energy and time required for a brute-force attack of this scale are prohibitive and far exceed any conceivable global capacity. Thus, a realistic threat model for such actors must also exclude brute-force attacks on the algorithm itself. Instead, they would focus on more sophisticated methods: exploiting zero-day vulnerabilities in the software stack, conducting side-channel attacks to extract keys from hardware, compromising the key generation or management infrastructure, or using espionage and coercion to obtain keys.\n\n3.  **Cloud Providers:** As operators of the infrastructure where data may be stored, cloud providers are in a unique position. The infeasibility of breaking the encryption algorithm means that if patient data is properly encrypted with keys that the provider does not have access to (e.g., via client-side encryption), the data's confidentiality is maintained even from the provider. The threat model shifts from the provider attacking the cryptography to the provider's internal security controls, access policies, and potential for compelled legal disclosure of any keys they do manage.\n\nIn conclusion, the fundamental computational security of the AES-128 primitive itself is, for all practical purposes, absolute against brute-force attacks. This forces any rational adversary to attack the implementation, the key management processes, the underlying hardware, and the human operators, rather than the core cryptographic algorithm. Therefore, realistic security models and defensive strategies for protecting health records must prioritize the security of the entire system over the mathematical strength of the encryption standard.", "answer": "$$\\boxed{5.39 \\times 10^{18}}$$", "id": "4850559"}, {"introduction": "Beyond encrypting data, we often need to share it for research or public health purposes in a way that protects individual identities. Methods like $k$-anonymity are designed for this, but selecting the right parameters can seem arbitrary. This exercise demonstrates how to move from abstract rules to principled, risk-based privacy engineering. You will learn to derive the minimum level of anonymization, the integer $k$, required to ensure that the statistical risk of re-identifying a person in the dataset remains below a specific, acceptable threshold [@problem_id:4850554].", "problem": "A regional health authority plans to release a de-identified dataset of patient encounters containing quasi-identifiers. The dataset contains $N = 100{,}000$ records drawn by simple random sampling without replacement from a regional population of size $M = 500{,}000$. The authority will enforce $k$-anonymity, meaning every equivalence class formed by the quasi-identifiers in the released dataset will have at least $k$ records.\n\nAdopt the following risk interpretation consistent with population-based reasoning: an intruder knows that a target individual resides in the regional population but does not know whether the target is included in the released dataset. The intruder performs a single attempt to re-identify the target by exact matching on the quasi-identifiers and, if multiple records match, chooses uniformly among those matches.\n\nDefine the per-record re-identification risk as the probability that the intruder correctly re-identifies the target in this single attempt. The authority requires that the estimated maximum per-record re-identification risk across the released dataset be strictly below $0.01$.\n\nUsing only the fundamental definitions of $k$-anonymity, simple random sampling, and probability (no additional formulas), and assuming that the worst-case risk occurs in the smallest allowed equivalence classes, compute the smallest integer $k$ that ensures the maximum re-identification risk is strictly below $0.01$. Provide the final $k$ as an integer. No rounding rule is needed beyond selecting the smallest integer that satisfies the risk requirement.", "solution": "The problem requires us to determine the smallest integer value of $k$ for $k$-anonymity such that the maximum per-record re-identification risk is strictly below a given threshold. The solution will be derived from fundamental principles of probability as specified.\n\nLet $N$ be the number of records in the released dataset, and $M$ be the size of the total population from which the sample is drawn. We are given $N = 100,000$ and $M = 500,000$. The sampling method is simple random sampling without replacement.\n\nThe per-record re-identification risk is defined as the probability that an intruder can correctly identify a target individual in a single attempt. Let this risk be denoted by $R$. The intruder knows the target is in the population of size $M$ but does not know if the target is in the sample of size $N$.\n\nLet us define the following events for a given target individual:\n$E_{sample}$: The event that the target individual is included in the released dataset.\n$E_{id}$: The event that the intruder correctly identifies the target individual.\n\nThe re-identification risk is the probability of the event $E_{id}$, i.e., $R = P(E_{id})$. Using the law of total probability, we can condition on whether the target is in the sample or not:\n$$R = P(E_{id}) = P(E_{id} | E_{sample}) P(E_{sample}) + P(E_{id} | E_{sample}^c) P(E_{sample}^c)$$\nwhere $E_{sample}^c$ is the complement of $E_{sample}$ (the target is not in the sample).\n\nIf the target is not in the sample ($E_{sample}^c$), the intruder cannot correctly identify them from the dataset. Therefore, the probability of correct identification given the target is not in the sample is zero: $P(E_{id} | E_{sample}^c) = 0$.\nThe formula for the risk simplifies to:\n$$R = P(E_{id} | E_{sample}) P(E_{sample})$$\n\nWe must now determine the two probabilities on the right-hand side.\n\nFirst, let's calculate $P(E_{sample})$. The dataset is a simple random sample of size $N$ drawn without replacement from a population of size $M$. The probability that any specific individual from the population is included in this sample is the ratio of the sample size to the population size.\n$$P(E_{sample}) = \\frac{N}{M}$$\nSubstituting the given values:\n$$P(E_{sample}) = \\frac{100,000}{500,000} = \\frac{1}{5}$$\n\nNext, let's calculate $P(E_{id} | E_{sample})$. This is the probability of correct identification given that the target is in the sample.\nAccording to the problem, the intruder performs an exact match on the quasi-identifiers. This partitions the dataset into equivalence classes. Let us consider the equivalence class to which the target's record belongs. Let the size of this equivalence class in the sample be $s$.\nThe intruder finds these $s$ matching records. The problem states that if multiple records match, the intruder chooses one uniformly at random. Since the target's record is one of these $s$ records, the probability of selecting the correct one is $\\frac{1}{s}$.\n$$P(E_{id} | E_{sample}) = \\frac{1}{s}$$\n\nCombining these parts, the re-identification risk $R$ for a record in an equivalence class of size $s$ is:\n$$R(s) = \\frac{1}{s} \\times \\frac{N}{M}$$\n\nThe problem requires that the *maximum* per-record re-identification risk be strictly below $0.01$. The risk $R(s)$ is a monotonically decreasing function of $s$. Therefore, the maximum risk, $R_{max}$, occurs for the minimum possible value of $s$.\n\nThe dataset is stated to be $k$-anonymous, which means that every equivalence class in the released dataset must have at least $k$ records. Thus, for any equivalence class of size $s$ in the dataset, we have $s \\ge k$. The minimum possible size for $s$ is therefore $k$. The problem explicitly directs us to assume that \"the worst-case risk occurs in the smallest allowed equivalence classes,\" which confirms that we should use $s_{min} = k$ to calculate the maximum risk.\n\n$$R_{max} = R(s=k) = \\frac{1}{k} \\frac{N}{M}$$\n\nThe authority's requirement is $R_{max} < 0.01$. We can now set up the inequality to solve for $k$:\n$$\\frac{1}{k} \\frac{N}{M} < 0.01$$\nSubstituting the numerical values for $N$ and $M$:\n$$\\frac{1}{k} \\left( \\frac{100,000}{500,000} \\right) < 0.01$$\n$$\\frac{1}{k} \\left( \\frac{1}{5} \\right) < 0.01$$\n$$\\frac{1}{5k} < \\frac{1}{100}$$\nSince $k$ is a count of records, it must be a positive integer ($k \\ge 1$). We can multiply both sides by $100 \\times 5k$ (which is positive) without changing the direction of the inequality:\n$$100 < 5k$$\nDividing by $5$:\n$$\\frac{100}{5} < k$$\n$$20 < k$$\n\nThe problem asks for the smallest integer $k$ that satisfies this condition. The smallest integer strictly greater than $20$ is $21$.\n\nTherefore, the smallest integer value for $k$-anonymity that ensures the maximum re-identification risk is strictly below the $0.01$ threshold is $21$.", "answer": "$$\\boxed{21}$$", "id": "4850554"}, {"introduction": "A comprehensive security strategy must account for threats from within an organization, not just from external attackers. Monitoring for unusual access patterns by authorized staff is crucial, but an overly sensitive system can flood security teams with false alarms. This final practice delves into the real-world challenge of operational security by applying statistical decision theory to design an optimal alerting system. You will derive an alert threshold that mathematically minimizes the total expected cost, carefully balancing the cost of investigating false positives against the harm of missing a genuine confidentiality breach [@problem_id:4850557].", "problem": "A hospital’s Electronic Health Record (EHR) system must protect the confidentiality of Very Important Person (VIP) patient records. To monitor potential insider misuse, security analytics compute, for each analyst and each day, the scalar statistic $x$ equal to the count of distinct VIP records accessed that day by that analyst. Historical logs yield a statistical baseline model for benign behavior: on a benign day, $x$ is well-approximated by a normal distribution with mean $\\mu_{b}$ and standard deviation $\\sigma$. On a day when an analyst is engaging in misuse targeting VIP records, $x$ is well-approximated by a normal distribution with mean $\\mu_{a} > \\mu_{b}$ and the same standard deviation $\\sigma$. Each day is benign with prior probability $1 - p$ and malicious with prior probability $p$, where $p$ is small but nonzero due to rare events.\n\nThe monitoring rule is a threshold alert: raise an alert if $x > T$. An alert on a benign day incurs an operational cost $c_{\\mathrm{fp}}$ due to investigating a false positive. Failing to alert on a malicious day incurs a confidentiality harm cost $c_{\\mathrm{miss}}$ due to a missed detection. Assume these costs are additive over days, independent across analysts, and that costs on correctly classified days are zero.\n\nStarting from first principles of expected-cost minimization and the properties of the normal distribution, derive an analytic expression for the threshold $T^{*}$ that minimizes the expected daily cost over the mixture of benign and malicious days. Then evaluate $T^{*}$ for the scientifically plausible parameterization\n$\\mu_{b} = 0.8$, $\\mu_{a} = 3.0$, $\\sigma = 0.6$, $p = 0.004$, $c_{\\mathrm{fp}} = 1$, and $c_{\\mathrm{miss}} = 400$.\nRound your numerical answer for the optimal threshold to four significant figures. Express the threshold as a count per analyst per day (that is, as a pure number).", "solution": "The problem requires the derivation of an optimal decision threshold $T^{*}$ that minimizes the expected daily cost associated with monitoring for insider misuse. This is a classic problem in statistical decision theory, specifically Bayes decision theory for a binary hypothesis test.\n\nLet $x$ be the observed statistic, representing the count of distinct VIP records accessed by an analyst in a day. The problem states that $x$ is well-approximated by a normal distribution. Let $H_b$ denote the hypothesis that a day is benign and $H_a$ denote the hypothesis that a day is malicious.\n\nThe statistical models for the two hypotheses are given by the conditional probability density functions (PDFs) of $x$:\nUnder $H_b$: $x$ is distributed as $\\mathcal{N}(\\mu_{b}, \\sigma^2)$, with PDF $f_b(x)$.\nUnder $H_a$: $x$ is distributed as $\\mathcal{N}(\\mu_{a}, \\sigma^2)$, with PDF $f_a(x)$.\n\nThe prior probabilities for these hypotheses are $P(H_b) = 1 - p$ and $P(H_a) = p$.\nThe decision rule is to raise an alert if the observed value $x$ exceeds a threshold $T$, i.e., if $x > T$.\n\nWe must formulate the expected cost, $E[C(T)]$, as a function of the threshold $T$. The total cost is the sum of the costs of the two possible errors, weighted by their probabilities of occurrence.\n1. A **False Positive** (Type I error) occurs if an alert is raised ($x > T$) on a benign day ($H_b$). This happens with probability $P(x > T | H_b)$ and incurs a cost $c_{\\mathrm{fp}}$.\n2. A **Missed Detection** (False Negative or Type II error) occurs if no alert is raised ($x \\le T$) on a malicious day ($H_a$). This happens with probability $P(x \\le T | H_a)$ and incurs a cost $c_{\\mathrm{miss}}$.\nCorrect decisions (True Negative, True Positive) incur a cost of $0$.\n\nThe total expected daily cost is the sum of the expected costs from each type of error:\n$$E[C(T)] = P(H_b) \\cdot c_{\\mathrm{fp}} \\cdot P(x > T | H_b) + P(H_a) \\cdot c_{\\mathrm{miss}} \\cdot P(x \\le T | H_a)$$\nSubstituting the prior probabilities:\n$$E[C(T)] = (1 - p) c_{\\mathrm{fp}} P(x > T | H_b) + p c_{\\mathrm{miss}} P(x \\le T | H_a)$$\nThe conditional probabilities can be expressed as integrals of the respective PDFs:\n$$P(x > T | H_b) = \\int_{T}^{\\infty} f_b(x) \\,dx$$\n$$P(x \\le T | H_a) = \\int_{-\\infty}^{T} f_a(x) \\,dx$$\nSo, the expected cost function is:\n$$E[C(T)] = (1 - p) c_{\\mathrm{fp}} \\int_{T}^{\\infty} f_b(x) \\,dx + p c_{\\mathrm{miss}} \\int_{-\\infty}^{T} f_a(x) \\,dx$$\nTo find the optimal threshold $T^{*}$ that minimizes this cost, we differentiate $E[C(T)]$ with respect to $T$ and set the derivative to zero. Using the Leibniz integral rule (a consequence of the Fundamental Theorem of Calculus):\n$$\\frac{d}{dT} \\int_{T}^{\\infty} g(x) \\,dx = -g(T) \\quad \\text{and} \\quad \\frac{d}{dT} \\int_{-\\infty}^{T} g(x) \\,dx = g(T)$$\nApplying this to our cost function, we obtain the derivative:\n$$\\frac{dE[C(T)]}{dT} = (1 - p) c_{\\mathrm{fp}} [-f_b(T)] + p c_{\\mathrm{miss}} [f_a(T)]$$\nSetting this derivative to zero to find the optimal threshold $T^{*}$:\n$$p c_{\\mathrm{miss}} f_a(T^{*}) - (1 - p) c_{\\mathrm{fp}} f_b(T^{*}) = 0$$\nThis leads to the condition:\n$$p c_{\\mathrm{miss}} f_a(T^{*}) = (1 - p) c_{\\mathrm{fp}} f_b(T^{*})$$\nThis equation is often expressed in terms of the likelihood ratio, $\\Lambda(T) = f_a(T) / f_b(T)$:\n$$\\Lambda(T^{*}) = \\frac{f_a(T^{*})}{f_b(T^{*})} = \\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}$$\nNow, we substitute the formulas for the normal PDFs, $f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$:\n$$\\frac{\\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(T^{*}-\\mu_a)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(T^{*}-\\mu_b)^2}{2\\sigma^2}\\right)} = \\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}$$\nThe pre-exponential normalization terms cancel, simplifying the equation to:\n$$\\exp\\left(\\frac{(T^{*}-\\mu_b)^2 - (T^{*}-\\mu_a)^2}{2\\sigma^2}\\right) = \\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}$$\nTaking the natural logarithm of both sides gives:\n$$\\frac{(T^{*}-\\mu_b)^2 - (T^{*}-\\mu_a)^2}{2\\sigma^2} = \\ln\\left(\\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}\\right)$$\nThe numerator is a difference of squares: $(T^{*})^2 - 2T^{*}\\mu_b + \\mu_b^2 - ((T^{*})^2 - 2T^{*}\\mu_a + \\mu_a^2) = 2T^{*}(\\mu_a - \\mu_b) - (\\mu_a^2 - \\mu_b^2)$.\n$$2T^{*}(\\mu_a - \\mu_b) - (\\mu_a^2 - \\mu_b^2) = 2\\sigma^2 \\ln\\left(\\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}\\right)$$\nFactoring $(\\mu_a^2 - \\mu_b^2) = (\\mu_a - \\mu_b)(\\mu_a + \\mu_b)$:\n$$2T^{*}(\\mu_a - \\mu_b) - (\\mu_a - \\mu_b)(\\mu_a + \\mu_b) = 2\\sigma^2 \\ln\\left(\\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}\\right)$$\nSince $\\mu_a > \\mu_b$, the term $(\\mu_a - \\mu_b)$ is non-zero, so we can divide by $2(\\mu_a - \\mu_b)$:\n$$T^{*} - \\frac{\\mu_a + \\mu_b}{2} = \\frac{\\sigma^2}{\\mu_a - \\mu_b} \\ln\\left(\\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}\\right)$$\nIsolating $T^{*}$ gives the final analytic expression for the optimal threshold:\n$$T^{*} = \\frac{\\mu_a + \\mu_b}{2} + \\frac{\\sigma^2}{\\mu_a - \\mu_b} \\ln\\left(\\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}}\\right)$$\nThis expression shows that the optimal threshold is the midpoint of the two means, adjusted by a term that depends on the variance, the separation of the means, the prior probabilities, and the costs of errors.\n\nNow, we evaluate $T^{*}$ for the given parameters: $\\mu_{b} = 0.8$, $\\mu_{a} = 3.0$, $\\sigma = 0.6$, $p = 0.004$, $c_{\\mathrm{fp}} = 1$, and $c_{\\mathrm{miss}} = 400$.\nFirst, calculate the components of the formula:\nThe midpoint of the means:\n$$\\frac{\\mu_a + \\mu_b}{2} = \\frac{3.0 + 0.8}{2} = \\frac{3.8}{2} = 1.9$$\nThe coefficient of the logarithmic term:\n$$\\frac{\\sigma^2}{\\mu_a - \\mu_b} = \\frac{(0.6)^2}{3.0 - 0.8} = \\frac{0.36}{2.2}$$\nThe argument of the logarithm, which is the ratio of the expected cost of a false positive to the expected cost of a missed detection at the decision boundary:\n$$\\frac{(1 - p) c_{\\mathrm{fp}}}{p c_{\\mathrm{miss}}} = \\frac{(1 - 0.004) \\times 1}{0.004 \\times 400} = \\frac{0.996}{1.6} = 0.6225$$\nSubstituting these values back into the expression for $T^{*}$:\n$$T^{*} = 1.9 + \\frac{0.36}{2.2} \\ln(0.6225)$$\nWe compute the numerical value:\n$$\\ln(0.6225) \\approx -0.47407212$$\n$$\\frac{0.36}{2.2} \\approx 0.16363636$$\n$$T^{*} \\approx 1.9 + (0.16363636) \\times (-0.47407212)$$\n$$T^{*} \\approx 1.9 - 0.07757675$$\n$$T^{*} \\approx 1.82242325$$\nThe problem requires rounding the numerical answer to four significant figures.\n$$T^{*} \\approx 1.822$$\nThe optimal threshold is a unitless count.", "answer": "$$\\boxed{1.822}$$", "id": "4850557"}]}