{"hands_on_practices": [{"introduction": "Before any analysis can begin, we must assess the quality and availability of our data. This practice demonstrates how to quantify the completeness of information—in this case, smoking status—from both structured fields and unstructured notes. More importantly, it introduces the use of statistical confidence intervals to express our certainty about these completeness estimates, a crucial skill for any data audit [@problem_id:4857102].", "problem": "A health system is auditing the completeness of smoking status documentation among adults in its Electronic Health Record (EHR). Completeness is defined as the proportion of patients for whom smoking status is non-missing. For a cohort of $n=500$ patients, the smoking status structured field is non-missing for $k=420$ patients. In parallel, the health system applies Natural Language Processing (NLP) to unstructured clinical notes. To estimate the completeness achievable by notes, an independent simple random validation sample of $m=300$ patients is manually reviewed, and smoking status is found documented in notes for $c=240$ patients. Assume each patient’s documentation status is a Bernoulli random variable and the samples are independent.\n\nUsing a two-sided Wilson score interval with confidence level $95\\%$ (that is, with $z=1.96$), construct the confidence intervals for the completeness proportions for structured fields and for notes. Then compute the ratio of the lower confidence bound for the structured-field completeness to the lower confidence bound for the notes-based completeness. Round your final answer to four significant figures and express it as a decimal (do not use a percentage sign).", "solution": "The user wants me to solve a statistics problem.\n\n### Step 1: Extract Givens\n- Cohort size for structured field analysis: $n=500$ patients.\n- Number of patients with non-missing structured field data: $k=420$.\n- Validation sample size for unstructured notes analysis: $m=300$ patients.\n- Number of patients with smoking status found in notes: $c=240$.\n- Assumption: Each patient's documentation status is a Bernoulli random variable.\n- Assumption: The two samples are independent.\n- Method: Two-sided Wilson score interval.\n- Confidence level: $95\\%$.\n- z-score associated with $95\\%$ confidence: $z=1.96$.\n- Task: Compute the ratio of the lower confidence bound for structured-field completeness to the lower confidence bound for notes-based completeness.\n- Final answer precision: Round to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective. It provides all necessary data and assumptions to perform a standard statistical calculation (Wilson score interval). The values are plausible within the context of a medical informatics audit. The problem does not violate any of the invalidity criteria.\n\n- **Scientific Soundness**: The problem utilizes a standard, well-established statistical method (Wilson score interval) for estimating a binomial proportion's confidence interval. This is appropriate for the given scenario of Bernoulli trials (documentation is present or not present).\n- **Completeness**: All required numerical values ($n$, $k$, $m$, $c$) and parameters ($z$, confidence level) are provided.\n- **Consistency**: The number of successes ($k$ and $c$) does not exceed the sample sizes ($n$ and $m$, respectively).\n- **Clarity**: The definitions and objectives are stated explicitly and unambiguously.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires the calculation of two separate confidence intervals for binomial proportions using the Wilson score method, followed by a computation of the ratio of their lower bounds.\n\nLet $p$ be the true proportion of successes in a population, estimated by the sample proportion $\\hat{p} = \\frac{x}{N}$, where $x$ is the number of successes in a sample of size $N$. The Wilson score interval for $p$ with a confidence level corresponding to the standard normal quantile $z$ is given by:\n$$ \\left( \\frac{\\hat{p} + \\frac{z^2}{2N}}{1 + \\frac{z^2}{N}} - \\frac{z}{1 + \\frac{z^2}{N}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N} + \\frac{z^2}{4N^2}}, \\quad \\frac{\\hat{p} + \\frac{z^2}{2N}}{1 + \\frac{z^2}{N}} + \\frac{z}{1 + \\frac{z^2}{N}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N} + \\frac{z^2}{4N^2}} \\right) $$\nWe are interested in the lower confidence bound, $L$, for each case. The formula for the lower bound is:\n$$ L = \\frac{1}{1 + \\frac{z^2}{N}} \\left[ \\left(\\hat{p} + \\frac{z^2}{2N}\\right) - z \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N} + \\frac{z^2}{4N^2}} \\right] $$\n\nFirst, we analyze the completeness of the structured field data.\nThe givens are:\n- Sample size: $n = 500$.\n- Number of successes (non-missing status): $k = 420$.\n- z-score: $z = 1.96$.\n\nThe sample proportion for the structured data, $\\hat{p}_{struc}$, is:\n$$ \\hat{p}_{struc} = \\frac{k}{n} = \\frac{420}{500} = 0.84 $$\nWe compute the lower confidence bound, $L_{struc}$, for the structured data proportion. We have $z^2 = 1.96^2 = 3.8416$.\nThe components of the formula are:\n$$ \\frac{z^2}{n} = \\frac{3.8416}{500} = 0.0076832 $$\n$$ \\hat{p}_{struc} + \\frac{z^2}{2n} = 0.84 + \\frac{3.8416}{2 \\times 500} = 0.84 + 0.0038416 = 0.8438416 $$\n$$ \\frac{\\hat{p}_{struc}(1-\\hat{p}_{struc})}{n} + \\frac{z^2}{4n^2} = \\frac{0.84(1-0.84)}{500} + \\frac{3.8416}{4 \\times 500^2} = \\frac{0.1344}{500} + \\frac{3.8416}{1000000} = 0.0002688 + 0.0000038416 = 0.0002726416 $$\nPlugging these into the formula for $L_{struc}$:\n$$ L_{struc} = \\frac{1}{1 + 0.0076832} \\left[ 0.8438416 - 1.96 \\sqrt{0.0002726416} \\right] $$\n$$ L_{struc} = \\frac{1}{1.0076832} \\left[ 0.8438416 - 1.96 \\times 0.01651186 \\right] $$\n$$ L_{struc} = \\frac{1}{1.0076832} \\left[ 0.8438416 - 0.03236325 \\right] $$\n$$ L_{struc} = \\frac{0.81147835}{1.0076832} \\approx 0.805290 $$\n\nNext, we analyze the completeness of the data from unstructured notes.\nThe givens are:\n- Sample size: $m = 300$.\n- Number of successes (status documented): $c = 240$.\n- z-score: $z = 1.96$.\n\nThe sample proportion for the notes data, $\\hat{p}_{notes}$, is:\n$$ \\hat{p}_{notes} = \\frac{c}{m} = \\frac{240}{300} = 0.80 $$\nWe compute the lower confidence bound, $L_{notes}$, for the notes data proportion. We again use $z^2 = 3.8416$.\nThe components of the formula are:\n$$ \\frac{z^2}{m} = \\frac{3.8416}{300} \\approx 0.01280533 $$\n$$ \\hat{p}_{notes} + \\frac{z^2}{2m} = 0.80 + \\frac{3.8416}{2 \\times 300} = 0.80 + \\frac{3.8416}{600} \\approx 0.80 + 0.00640267 = 0.80640267 $$\n$$ \\frac{\\hat{p}_{notes}(1-\\hat{p}_{notes})}{m} + \\frac{z^2}{4m^2} = \\frac{0.80(1-0.80)}{300} + \\frac{3.8416}{4 \\times 300^2} = \\frac{0.16}{300} + \\frac{3.8416}{360000} \\approx 0.00053333 + 0.00001067 = 0.00054400 $$\nPlugging these into the formula for $L_{notes}$:\n$$ L_{notes} = \\frac{1}{1 + 0.01280533} \\left[ 0.80640267 - 1.96 \\sqrt{0.00054400} \\right] $$\n$$ L_{notes} = \\frac{1}{1.01280533} \\left[ 0.80640267 - 1.96 \\times 0.0233238 \\right] $$\n$$ L_{notes} = \\frac{1}{1.01280533} \\left[ 0.80640267 - 0.04571465 \\right] $$\n$$ L_{notes} = \\frac{0.76068802}{1.01280533} \\approx 0.751069 $$\n\nFinally, we compute the ratio, $R$, of the lower confidence bound for the structured-field completeness to the lower confidence bound for the notes-based completeness.\n$$ R = \\frac{L_{struc}}{L_{notes}} $$\n$$ R = \\frac{0.805290}{0.751069} \\approx 1.072189 $$\nThe problem requires the final answer to be rounded to four significant figures.\n$$ R \\approx 1.072 $$", "answer": "$$\\boxed{1.072}$$", "id": "4857102"}, {"introduction": "Once we know data exists, the next challenge is retrieving it efficiently. This exercise provides a conceptual framework for comparing the query complexity of extracting medication information from a highly structured database (the OMOP Common Data Model) versus an unstructured source (discharge summaries). By operationalizing complexity as the number of joins or expected operations, you will gain insight into the fundamental trade-offs between the rigid predictability of structured queries and the probabilistic nature of text processing [@problem_id:4857095].", "problem": "A health system stores its medication data in the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) and also archives unstructured discharge summaries in the electronic health record (EHR). You need to retrieve all medications for a single patient at the ingredient level and compare the relative query complexity between the structured and unstructured approaches using a simple, operational proxy. Use the following definitions and assumptions.\n\nDefinitions and assumptions for the structured (OMOP CDM) retrieval:\n- In OMOP CDM, medication exposures are recorded in table $drug\\_exposure$ with field $drug\\_concept\\_id$.\n- Ingredient roll-ups are represented via table $concept\\_ancestor$ by linking a descendant drug concept to its ancestor ingredient concept via fields $descendant\\_concept\\_id$ and $ancestor\\_concept\\_id$.\n- Human-readable names for concepts are in table $concept$ via field $concept\\_name$.\n- The goal is to retrieve all ingredient-level medication names and exposure dates for a single patient identified by $person\\_id = p$, at the minimal level of standardization necessary to report ingredients and their dates. Assume that $person\\_id$ filtering can be applied directly on $drug\\_exposure$ and that only standard ingredient names are required.\n- Count the number of equi-joins needed between OMOP tables to meet the above goal, minimizing joins while preserving correctness.\n\nDefinitions and assumptions for the unstructured (discharge summary) retrieval:\n- The unstructured pipeline examines $n$ discharge summaries for the same patient.\n- For each discharge summary, with probability $q$ a dedicated “Medications” section is present and the pipeline needs $a$ operations to extract medications; otherwise, with probability $(1-q)$, the section is absent and the pipeline needs $b$ operations to extract medications by scanning the entire document.\n- Assume independence across documents and that each “operation” is counted as one unit of effort.\n\nComplexity index:\n- Define the structured query complexity as the number of joins, $J_{s}$.\n- Define the unstructured expected operation count as the expected number of operations across all $n$ discharge summaries, $O_{u}$, derived from first principles.\n- Define the comparative complexity index as the ratio $R = \\frac{O_{u}}{J_{s}}$.\n\nData:\n- Number of discharge summaries $n = 3$.\n- Probability a dedicated medications section is present $q = 0.8$.\n- Operations if section present $a = 5$.\n- Operations if section absent $b = 20$.\n\nUsing only the above definitions and assumptions, compute the ratio $R$. Report a single real-valued number. No rounding is required.", "solution": "The user has provided a scientifically grounded, well-posed, and objective problem. All necessary data and definitions are supplied, and there are no contradictions or ambiguities. The problem is valid.\n\nThe solution requires calculating a comparative complexity index, $R$, defined as the ratio of the unstructured expected operation count, $O_u$, to the structured query complexity, $J_s$. We must compute $J_s$ and $O_u$ separately and then find their ratio.\n\nFirst, we determine the structured query complexity, $J_s$, which is defined as the minimal number of equi-joins needed to retrieve the specified data from the OMOP Common Data Model. The goal is to obtain all ingredient-level medication names and their corresponding exposure dates for a single patient identified by $person\\_id = p$.\n\n1.  The starting point is the $drug\\_exposure$ table, which contains the patient's medication exposures. We filter this table for the specific patient, $person\\_id = p$. This table provides the $drug\\_concept\\_id$ for each medication event and the associated exposure date.\n\n2.  The $drug\\_concept\\_id$s in the $drug\\_exposure$ table represent the specific drugs administered, which are not necessarily ingredients. To find the active ingredient for each drug, we must use the $concept\\_ancestor$ table. This table maps descendant concepts (like specific drug formulations) to their ancestor concepts (like ingredients). Therefore, the first join is required between the $drug\\_exposure$ table and the $concept\\_ancestor$ table. The join condition is $drug\\_exposure.drug\\_concept\\_id = concept\\_ancestor.descendant\\_concept\\_id$. This join allows us to retrieve the $ancestor\\_concept\\_id$ for each medication exposure, where this ancestor is the ingredient concept.\n\n3.  At this point, we have the ingredient's concept identifier ($ancestor\\_concept\\_id$) and the exposure date. The final requirement is to obtain the human-readable name of the ingredient. The $concept$ table stores names for all concepts. Thus, a second join is necessary. We join the result from the previous step with the $concept$ table on the condition $concept\\_ancestor.ancestor\\_concept\\_id = concept.concept\\_id$. From this, we can select the $concept\\_name$.\n\nThis two-join process ($drug\\_exposure \\rightarrow concept\\_ancestor \\rightarrow concept$) is the minimal sequence to satisfy all requirements. A single join would be insufficient. For example, joining only $drug\\_exposure$ with $concept$ would yield the name of the administered drug, not its ingredient. Therefore, the structured query complexity, or the number of joins, is $J_s = 2$.\n\nNext, we calculate the unstructured expected operation count, $O_u$. This is the expected number of total operations required to extract medication information from $n$ discharge summaries.\n\nLet $X_i$ be the random variable representing the number of operations for the $i$-th summary, for $i = 1, 2, \\ldots, n$. The problem states that for any given summary:\n-   With probability $q$, a \"Medications\" section is present, and the number of operations is $a$.\n-   With probability $(1-q)$, the section is absent, and the number of operations is $b$.\n\nThe expected number of operations for a single discharge summary, $E[X_i]$, is given by the definition of expected value:\n$$E[X_i] = a \\cdot q + b \\cdot (1-q)$$\nThe total number of operations across all $n$ documents is the sum of the operations for each document, $\\sum_{i=1}^{n} X_i$. The total expected number of operations, $O_u$, is the expectation of this sum. By the linearity of expectation, and since the process for each document is independent and identically distributed:\n$$O_u = E\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} E[X_i] = n \\cdot E[X_i]$$\nSubstituting the expression for $E[X_i]$:\n$$O_u = n(aq + b(1-q))$$\nWe are given the following values:\n-   Number of discharge summaries, $n = 3$.\n-   Probability of section presence, $q = 0.8$.\n-   Operations if section is present, $a = 5$.\n-   Operations if section is absent, $b = 20$.\n\nSubstituting these values into the equation for $O_u$:\n$$O_u = 3 \\cdot (5 \\cdot 0.8 + 20 \\cdot (1-0.8))$$\n$$O_u = 3 \\cdot (5 \\cdot 0.8 + 20 \\cdot 0.2)$$\n$$O_u = 3 \\cdot (4 + 4)$$\n$$O_u = 3 \\cdot 8$$\n$$O_u = 24$$\nSo, the unstructured expected operation count is $O_u = 24$.\n\nFinally, we compute the comparative complexity index, $R$, which is the ratio of $O_u$ to $J_s$:\n$$R = \\frac{O_u}{J_s}$$\nUsing the values we calculated:\n$$R = \\frac{24}{2} = 12$$\nThe ratio $R$ is $12$.", "answer": "$$\\boxed{12}$$", "id": "4857095"}, {"introduction": "Retrieving data is only half the battle; we must also evaluate its accuracy. This final practice walks you through calculating and comparing standard performance metrics for identifying a clinical concept from both structured and unstructured sources. By computing precision, recall, and the $F1$ score, you will learn to quantitatively assess the trade-off between the potential richness of clinical notes and the often-assumed reliability of structured data fields [@problem_id:4857088].", "problem": "A clinical natural language processing (NLP) pipeline is evaluated for extracting the concept “documented chronic kidney disease” from unstructured clinical notes, using expert chart review as the reference standard. For the same concept, the Electronic Health Record (EHR) contains a structured problem-list field. The evaluation is run on $N = 500$ patient encounters. The confusion matrix counts for the unstructured concept extraction are $TP = 190$, $FP = 35$, $TN = 245$, and $FN = 30$. Separately, the confusion matrix counts for the structured field are $TP_{s} = 180$, $FP_{s} = 20$, $TN_{s} = 260$, and $FN_{s} = 40$. Using first principles definitions of classification metrics based on these counts, compute the precision, recall, specificity, and $F1$ for the unstructured extraction, and the accuracy for the structured field. Then, define the signed difference $\\Delta$ between the unstructured extraction $F1$ and the structured field accuracy as $\\Delta = F1_{\\text{unstructured}} - \\text{accuracy}_{\\text{structured}}$. Report $\\Delta$ as a decimal and round your final answer to four significant figures.", "solution": "The problem requires the calculation and comparison of performance metrics for two different methods of identifying a clinical concept (\"documented chronic kidney disease\") from patient encounter data. The first method is an unstructured data extraction pipeline using natural language processing (NLP), and the second is a structured data field from the Electronic Health Record (EHR). The evaluation is performed against a reference standard (expert chart review) for a sample of $N=500$ encounters.\n\nTo begin, we must first validate the problem statement.\n\nThe givens are:\nTotal encounters, $N = 500$.\n\nFor unstructured concept extraction:\nTrue Positives, $TP = 190$.\nFalse Positives, $FP = 35$.\nTrue Negatives, $TN = 245$.\nFalse Negatives, $FN = 30$.\n\nFor the structured problem-list field:\nTrue Positives, $TP_s = 180$.\nFalse Positives, $FP_s = 20$.\nTrue Negatives, $TN_s = 260$.\nFalse Negatives, $FN_s = 40$.\n\nThe problem asks for several calculations:\n1.  Precision, recall, specificity, and $F1$ score for the unstructured extraction.\n2.  Accuracy for the structured field.\n3.  The signed difference $\\Delta = F1_{\\text{unstructured}} - \\text{accuracy}_{\\text{structured}}$.\n\nBefore proceeding, we verify the internal consistency of the provided data.\nFor the unstructured data, the sum of the confusion matrix cells is $TP + FP + TN + FN = 190 + 35 + 245 + 30 = 500$, which matches the total number of encounters $N$.\nFor the structured data, the sum is $TP_s + FP_s + TN_s + FN_s = 180 + 20 + 260 + 40 = 500$, which also matches $N$.\nFurthermore, the number of \"true positive\" cases in the reference standard must be consistent between the two evaluations.\nFor the unstructured evaluation, the number of positives in the reference is $P = TP + FN = 190 + 30 = 220$.\nFor the structured evaluation, the number of positives is $P_s = TP_s + FN_s = 180 + 40 = 220$.\nThe number of \"true negative\" cases must also be consistent.\nFor the unstructured evaluation, the number of negatives is $N_{neg} = TN + FP = 245 + 35 = 280$.\nFor the structured evaluation, the number of negatives is $N_{s,neg} = TN_s + FP_s = 260 + 20 = 280$.\nSince $P = P_s$ and $N_{neg} = N_{s,neg}$, and the total encounters $N$ is consistent, the problem is well-defined, self-contained, and scientifically grounded in standard classification performance evaluation. The problem is deemed valid.\n\nWe now proceed with the calculations based on their first-principles definitions.\n\nFor the unstructured data extraction:\nThe total number of predicted positive cases is $TP + FP = 190 + 35 = 225$.\nThe total number of actual positive cases is $TP + FN = 190 + 30 = 220$.\nThe total number of actual negative cases is $TN + FP = 245 + 35 = 280$.\n\nPrecision, which measures the proportion of correctly predicted positives among all predicted positives, is defined as:\n$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\nSubstituting the values for the unstructured extraction:\n$$ \\text{Precision}_{\\text{unstructured}} = \\frac{190}{190 + 35} = \\frac{190}{225} $$\n\nRecall (or Sensitivity), which measures the proportion of correctly identified actual positives, is defined as:\n$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\nSubstituting the values:\n$$ \\text{Recall}_{\\text{unstructured}} = \\frac{190}{190 + 30} = \\frac{190}{220} $$\n\nSpecificity, which measures the proportion of correctly identified actual negatives, is defined as:\n$$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\nSubstituting the values:\n$$ \\text{Specificity}_{\\text{unstructured}} = \\frac{245}{245 + 35} = \\frac{245}{280} $$\n\nThe $F1$ score is the harmonic mean of precision and recall. It is defined as:\n$$ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nA more direct formula using the confusion matrix counts is:\n$$ F1 = \\frac{2TP}{2TP + FP + FN} $$\nSubstituting the values for the unstructured extraction:\n$$ F1_{\\text{unstructured}} = \\frac{2 \\times 190}{2 \\times 190 + 35 + 30} = \\frac{380}{380 + 65} = \\frac{380}{445} $$\n\nNext, we calculate the accuracy for the structured data field.\nAccuracy measures the proportion of all correct predictions (both true positives and true negatives) among the total number of cases. It is defined as:\n$$ \\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN} = \\frac{TP + TN}{N} $$\nUsing the counts for the structured field ($TP_s, TN_s$) and the total number of encounters $N$:\n$$ \\text{accuracy}_{\\text{structured}} = \\frac{TP_s + TN_s}{N} = \\frac{180 + 260}{500} = \\frac{440}{500} $$\n\nFinally, we are asked to compute the signed difference $\\Delta$:\n$$ \\Delta = F1_{\\text{unstructured}} - \\text{accuracy}_{\\text{structured}} $$\nSubstituting the fractional forms we derived:\n$$ \\Delta = \\frac{380}{445} - \\frac{440}{500} $$\nTo compute the final value, we convert these fractions to their decimal representations.\n$$ F1_{\\text{unstructured}} = \\frac{380}{445} \\approx 0.85393258... $$\n$$ \\text{accuracy}_{\\text{structured}} = \\frac{440}{500} = 0.88 $$\nNow, we compute the difference:\n$$ \\Delta \\approx 0.85393258 - 0.88 = -0.02606741... $$\nThe problem requires this result to be rounded to four significant figures. The first non-zero digit is $2$ in the hundredths place. The four significant figures are $2$, $6$, $0$, and $6$. The fifth significant figure is $7$. Since $7 \\geq 5$, we round up the fourth significant figure.\n$$ \\Delta \\approx -0.02607 $$\nThis is the final numerical answer.", "answer": "$$\\boxed{-0.02607}$$", "id": "4857088"}]}