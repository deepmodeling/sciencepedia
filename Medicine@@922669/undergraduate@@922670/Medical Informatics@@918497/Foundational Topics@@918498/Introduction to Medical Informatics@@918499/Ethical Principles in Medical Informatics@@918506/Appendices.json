{"hands_on_practices": [{"introduction": "Releasing medical data for research is vital, but protecting patient identity is paramount. A common approach is to de-identify data by generalizing or suppressing quasi-identifiers—attributes like age, gender, and ZIP code that can collectively identify a person. This exercise demonstrates why naive de-identification is dangerously insufficient, exposing how an adversary can use publicly available data in a \"linkage attack\" to re-identify individuals and their sensitive diagnoses. By working through this scenario, you will learn to assess vulnerabilities and construct a more robust de-identification strategy that satisfies the principles of $k$-anonymity and $l$-diversity [@problem_id:4837957].", "problem": "A hospital plans to release a microdata table with attributes ZIP code (ZIP5), birth year (Year), sex (Sex), and diagnosis (Diagnosis). The quasi-identifiers are ZIP5, Year, and Sex. The table contains $10$ discharge records drawn from a single municipality, with the following quasi-identifier profiles and diagnoses (each bullet represents one record, and all records are distinct persons):\n- $(02138, 1980, F)$ with Diagnosis $=$ HIV.\n- $(02138, 1975, M)$ with Diagnosis $=$ Cancer.\n- $(02139, 1982, F)$ with Diagnosis $=$ Flu.\n- $(02139, 1982, F)$ with Diagnosis $=$ Flu.\n- $(02139, 1990, M)$ with Diagnosis $=$ Depression.\n- $(02139, 1990, M)$ with Diagnosis $=$ HIV.\n- $(02140, 1970, F)$ with Diagnosis $=$ Cancer.\n- $(02140, 1970, F)$ with Diagnosis $=$ Diabetes.\n- $(02140, 1985, M)$ with Diagnosis $=$ HIV.\n- $(02140, 1985, M)$ with Diagnosis $=$ Flu.\n\nA contemporaneous voter registration roll for the same municipality provides an external population file with the count of residents for each $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$ triple. For the triples that appear in the hospital data above, the voter roll shows the following counts of residents:\n- $(02138, 1980, F)$ appears $1$ time.\n- $(02138, 1975, M)$ appears $1$ time.\n- $(02139, 1982, F)$ appears $3$ times.\n- $(02139, 1990, M)$ appears $5$ times.\n- $(02140, 1970, F)$ appears $2$ times.\n- $(02140, 1985, M)$ appears $4$ times.\n\nAssume the hospital cohort is a subset of the voter roll’s municipality population and that adversaries can link on quasi-identifiers. The hospital’s initial, naive generalization scheme is: drop day and month of birth but keep Year as-is, keep ZIP5 unchanged, and keep Sex unchanged.\n\nFrom first principles in data privacy for medical informatics, use the definitions of quasi-identifier, $k$-anonymity, and distinct $l$-diversity to reason about linkage risk and de-identification. Then select the option that both:\n- correctly explains why the naive scheme is vulnerable to linkage using the voter roll, and\n- specifies a concrete, resilient release strategy that uses generalization and suppression to achieve $k$-anonymity and distinct $l$-diversity for Diagnosis with explicit parameters $(k,l)$, while suppressing as little as possible and preserving as much utility as feasible.\n\nWhich option is best?\n\nA. Linking attacks do not succeed once day and month of birth are removed, because Year is already too coarse. Set $k=2$, $l=2$ and keep ZIP5 and Year; suppress the two records from ZIP $02138$ only. No further changes are needed.\n\nB. Linking attacks succeed whenever a quasi-identifier triple is unique in the voter roll; if a $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$ appears $1$ time externally and $1$ time in the release, identity disclosure risk is $1$. To mitigate with minimal utility loss, set $k=2$ and $l=2$ (distinct $l$-diversity), generalize ZIP5 to ZIP3 (retain the first $3$ digits), generalize Year to decade bins (e.g., 1970s, 1980s, 1990s), and apply local suppression of Sex only for the $1970\\text{s}$ group (i.e., set Sex $=*$ for all $1970\\text{s}$ records) to eliminate the remaining singleton. Release the resulting table.\n\nC. Increasing $k$ alone eliminates linkage even if quasi-identifiers remain very specific. Set $k=5$, $l=2$ with no suppression by generalizing ZIP5 to ZIP2 and leaving Year and Sex exact.\n\nD. Linking can be neutralized by coarsening ZIP alone. Set $k=2$, $l=3$; generalize ZIP5 to ZIP3, leave Year and Sex exact, and release without suppression.\n\nE. Suppressing records that are unique in the release automatically enforces $l$-diversity. Set $k=2$, $l=2$; generalize ZIP5 to ZIP3, leave Year exact, and suppress any records that remain unique on $(\\text{ZIP3}, \\text{Year}, \\text{Sex})$; no attention to Diagnosis distribution is needed thereafter.", "solution": "The user wants me to act as a STEM professor. I need to validate the problem, and if it's valid, solve it and evaluate the options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Microdata Table:** A table with $10$ discharge records.\n*   **Attributes:** ZIP code (ZIP5), birth year (Year), sex (Sex), and diagnosis (Diagnosis).\n*   **Quasi-identifiers (QIs):** The set of attributes {ZIP5, Year, Sex}.\n*   **Data Records:**\n    1.  $ (02138, 1980, F)$, Diagnosis $=$ HIV.\n    2.  $ (02138, 1975, M)$, Diagnosis $=$ Cancer.\n    3.  $ (02139, 1982, F)$, Diagnosis $=$ Flu.\n    4.  $ (02139, 1982, F)$, Diagnosis $=$ Flu.\n    5.  $ (02139, 1990, M)$, Diagnosis $=$ Depression.\n    6.  $ (02139, 1990, M)$, Diagnosis $=$ HIV.\n    7.  $ (02140, 1970, F)$, Diagnosis $=$ Cancer.\n    8.  $ (02140, 1970, F)$, Diagnosis $=$ Diabetes.\n    9.  $ (02140, 1985, M)$, Diagnosis $=$ HIV.\n    10. $ (02140, 1985, M)$, Diagnosis $=$ Flu.\n*   **External Population File (Voter Roll):** Provides counts for each $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$ triple in the municipality.\n*   **Population Counts for QIs in the hospital data:**\n    *   $ (02138, 1980, F) $: count $= 1$.\n    *   $ (02138, 1975, M) $: count $= 1$.\n    *   $ (02139, 1982, F) $: count $= 3$.\n    *   $ (02139, 1990, M) $: count $= 5$.\n    *   $ (02140, 1970, F) $: count $= 2$.\n    *   $ (02140, 1985, M) $: count $= 4$.\n*   **Assumptions:**\n    *   The hospital cohort is a subset of the voter roll population.\n    *   Adversaries can link data on the quasi-identifiers.\n*   **Naive Generalization Scheme:** Drop day/month of birth, keep Year, ZIP5, and Sex as-is.\n*   **Task:**\n    1.  Explain the vulnerability of the naive scheme to linkage attacks.\n    2.  Specify a concrete, resilient release strategy using generalization and suppression.\n    3.  The strategy must achieve $k$-anonymity and distinct $l$-diversity for specified parameters $(k,l)$.\n    4.  The strategy should minimize suppression and maximize data utility.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem uses standard, well-defined concepts from the field of data privacy and medical informatics, including quasi-identifiers, $k$-anonymity, distinct $l$-diversity, generalization, suppression, and linkage attacks. These concepts are rigorously defined and form the basis of modern data de-identification practices. The problem is scientifically sound.\n2.  **Well-Posed:** The problem provides a concrete dataset, an external data source, a clear set of quasi-identifiers, and a sensitive attribute. The objective is clearly stated: achieve a specific privacy standard (a combination of $k$-anonymity and distinct $l$-diversity) while preserving utility. The constraints and data are sufficient to allow for a unique \"best\" choice among the given options to be determined through logical derivation and calculation.\n3.  **Objective:** The problem is described using numerical data and formal definitions. The language is precise and unbiased.\n4.  **Completeness and Consistency:** All necessary data to evaluate the initial risk and the proposed solutions in the options are provided. There are no internal contradictions in the data. For example, the number of records in the hospital data for a given QI is less than or equal to the count in the voter roll, consistent with the subset assumption.\n5.  **Realism:** The scenario is a canonical and realistic example of de-identification challenges in healthcare. The use of voter rolls as an external linkage file is a classic threat model. The data values are plausible.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically grounded, well-posed, objective, complete, and realistic. I will now proceed with the solution derivation and evaluation of the options.\n\n### Solution Derivation\n\nThe problem requires an understanding of several first principles in data privacy.\n\n*   A **quasi-identifier (QI)** is a set of attributes that, in combination, can be linked to external data sources to re-identify individuals. In this problem, the QI is the triple $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$.\n*   **$k$-anonymity** requires that for any combination of quasi-identifier values in the released table, there must be at least $k$ records that share that combination. Each such group of records is called an equivalence class.\n*   **Distinct $l$-diversity** is a refinement of $k$-anonymity that addresses attribute disclosure. It requires that every equivalence class must have at least $l$ distinct values for the sensitive attribute (here, Diagnosis). This prevents an adversary from learning the sensitive value even if they cannot identify the specific individual (e.g., if all $k$ people in a group have the same diagnosis).\n*   A **linkage attack** occurs when an adversary uses the QIs from the released data to find a match in an external file (like the voter roll) that contains explicit identifiers (like name or address), thereby re-identifying a subject and learning their sensitive information.\n\n**Vulnerability of the Naive Scheme**\n\nThe naive scheme releases the QI triple $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$ unchanged. An adversary can use this triple to link with the voter registration roll.\n\n*   For the patient with QI $= (02138, 1980, F)$, the voter roll shows that exactly $1$ person in the entire municipality matches this profile. Since the hospital data contains a record with this QI, and the hospital cohort is a subset of the municipality, the adversary knows with $100\\%$ certainty that this specific person (identifiable from the voter roll) is the patient with Diagnosis $=$ HIV. This is a complete identity and attribute disclosure.\n*   Similarly, for the patient with QI $= (02138, 1975, M)$, the voter roll population count is $1$. The adversary can re-identify this person and learn their Diagnosis is Cancer.\n\nThe vulnerability is due to the existence of unique or small-population QI combinations in the public domain that also appear in the sensitive dataset. The statement in Option B correctly identifies this: \"Linking attacks succeed whenever a quasi-identifier triple is unique in the voter roll; if a $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$ appears $1$ time externally and $1$ time in the release, identity disclosure risk is $1$.\"\n\nTo mitigate this, we must modify the QIs through generalization (making them less specific) or suppression (removing information) to ensure that each QI combination in the released data corresponds to a sufficiently large group of individuals, both in the release itself ($k$-anonymity) and in the external population. The solution must also ensure diversity of the sensitive attribute ($l$-diversity).\n\n### Option-by-Option Analysis\n\nThe goal is to find a strategy that achieves the stated $(k,l)$ parameters while \"suppressing as little as possible and preserving as much utility as feasible.\"\n\n**A. Linking attacks do not succeed once day and month of birth are removed, because Year is already too coarse. Set $k=2$, $l=2$ and keep ZIP5 and Year; suppress the two records from ZIP $02138$ only. No further changes are needed.**\n*   **Vulnerability Explanation:** The assertion that linking attacks fail because 'Year is already too coarse' is demonstrably false. Our analysis shows linkage succeeds precisely because the full QI $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$ is too specific, not because Year is coarse.\n*   **Strategy Analysis:** This strategy proposes suppressing the $2$ records that are unique in the voter roll and keeping the remaining $8$ records with their original QIs. The remaining records form the following equivalence classes based on $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$:\n    *   Group 1: $(\\text{02139, 1982, F})$. Records: $2$. Diagnoses: {Flu, Flu}.\n    *   Group 2: $(\\text{02139, 1990, M})$. Records: $2$. Diagnoses: {Depression, HIV}.\n    *   Group 3: $(\\text{02140, 1970, F})$. Records: $2$. Diagnoses: {Cancer, Diabetes}.\n    *   Group 4: $(\\text{02140, 1985, M})$. Records: $2$. Diagnoses: {HIV, Flu}.\n    All groups have a size of $2$, so the dataset achieves $k=2$ anonymity. However, for Group $1$, the number of distinct diagnoses is $1$ (only Flu). This fails to meet the required distinct $l$-diversity of $l=2$.\n*   **Verdict:** **Incorrect**. The vulnerability explanation is wrong, and the proposed strategy fails to meet the $l=2$ requirement.\n\n**B. Linking attacks succeed whenever a quasi-identifier triple is unique in the voter roll; if a $(\\text{ZIP5}, \\text{Year}, \\text{Sex})$ appears $1$ time externally and $1$ time in the release, identity disclosure risk is $1$. To mitigate with minimal utility loss, set $k=2$ and $l=2$ (distinct $l$-diversity), generalize ZIP5 to ZIP3 (retain the first $3$ digits), generalize Year to decade bins (e.g., 1970s, 1980s, 1990s), and apply local suppression of Sex only for the $1970\\text{s}$ group (i.e., set Sex $=*$ for all $1970\\text{s}$ records) to eliminate the remaining singleton. Release the resulting table.**\n*   **Vulnerability Explanation:** This explanation is precise and correct, as established in the derivation above.\n*   **Strategy Analysis:** Let's apply the transformations.\n    1.  **Generalize ZIP5 to ZIP3:** All ZIP codes ($02138$, $02139$, $02140$) become $021$. This makes the ZIP attribute uniform across all records.\n    2.  **Generalize Year to decade:** We use standard decade bins: [1970-1979] ('1970s'), [1980-1989] ('1980s'), [1990-1999] ('1990s').\n    3.  **Form initial groups** based on $(\\text{ZIP3}, \\text{Decade}, \\text{Sex})$:\n        *   $(\\text{021, 1980s, F})$: $3$ records (from $1980, 1982, 1982$). Diagnoses: {HIV, Flu, Flu}.\n        *   $(\\text{021, 1970s, M})$: $1$ record (from $1975$). Diagnosis: {Cancer}. This is a singleton and violates $k=2$.\n        *   $(\\text{021, 1990s, M})$: $2$ records (from $1990, 1990$). Diagnoses: {Depression, HIV}.\n        *   $(\\text{021, 1970s, F})$: $2$ records (from $1970, 1970$). Diagnoses: {Cancer, Diabetes}.\n        *   $(\\text{021, 1980s, M})$: $2$ records (from $1985, 1985$). Diagnoses: {HIV, Flu}.\n    4.  **Apply local suppression:** The singleton group is $(\\text{021, 1970s, M})$. The option suggests suppressing Sex for the '1970s group'. This means modifying both of the 1970s groups to use Sex $= *$. This merges the $(\\text{021, 1970s, M})$ group (size $1$) and the $(\\text{021, 1970s, F})$ group (size $2$) into a new group.\n    5.  **Final Equivalence Classes:**\n        *   Class 1: $(\\text{021, 1980s, F})$. Size $= 3$. $k=3 \\ge 2$. Diagnoses: {HIV, Flu, Flu}. Distinct diagnoses $= 2$. $l=2 \\ge 2$. OK.\n        *   Class 2: $(\\text{021, 1990s, M})$. Size $= 2$. $k=2 \\ge 2$. Diagnoses: {Depression, HIV}. Distinct diagnoses $= 2$. $l=2 \\ge 2$. OK.\n        *   Class 3: $(\\text{021, 1980s, M})$. Size $= 2$. $k=2 \\ge 2$. Diagnoses: {HIV, Flu}. Distinct diagnoses $= 2$. $l=2 \\ge 2$. OK.\n        *   Class 4 (merged): $(\\text{021, 1970s, *})$. Size $= 1+2 = 3$. $k=3 \\ge 2$. Diagnoses: {Cancer, Cancer, Diabetes}. Distinct diagnoses $= 2$. $l=2 \\ge 2$. OK.\n    This strategy successfully transforms the entire dataset to satisfy both $k=2$ and distinct $l=2$ without suppressing any full records. It is a valid and robust solution.\n*   **Verdict:** **Correct**. The explanation is sound, and the step-by-step procedure achieves the specified $(k,l)$ goals for the entire dataset while preserving all records.\n\n**C. Increasing $k$ alone eliminates linkage even if quasi-identifiers remain very specific. Set $k=5$, $l=2$ with no suppression by generalizing ZIP5 to ZIP2 and leaving Year and Sex exact.**\n*   **Vulnerability Explanation:** The statement is vague and misleading. One does not simply \"increase $k$\"; one applies transformations (like generalization) which result in a dataset that satisfies a certain $k$. The claim that this works even with \"very specific\" QIs is contradictory.\n*   **Strategy Analysis:** Generalize ZIP5 to ZIP2 (all become $02$). Leave Year and Sex exact. The QIs are effectively $(\\text{Year}, \\text{Sex})$. The equivalence classes are:\n    *   $(\\text{1980, F})$: size $1$.\n    *   $(\\text{1975, M})$: size $1$.\n    *   $(\\text{1982, F})$: size $2$.\n    *   $(\\text{1990, M})$: size $2$.\n    *   $(\\text{1970, F})$: size $2$.\n    *   $(\\text{1985, M})$: size $2$.\n    No group has a size of $5$. The strategy completely fails to achieve its stated goal of $k=5$.\n*   **Verdict:** **Incorrect**. The explanation is weak and the strategy fails to achieve $k=5$.\n\n**D. Linking can be neutralized by coarsening ZIP alone. Set $k=2$, $l=3$; generalize ZIP5 to ZIP3, leave Year and Sex exact, and release without suppression.**\n*   **Vulnerability Explanation:** \"coarsening ZIP alone\" is an oversimplification. Other attributes may still be too specific in combination.\n*   **Strategy Analysis:** Generalize ZIP5 to ZIP3 (all become $021$). Leave Year and Sex exact. The QIs are effectively $(\\text{Year}, \\text{Sex})$. The equivalence classes are the same as in the analysis for Option C.\n    *   $(\\text{1980, F})$: size $1$. Fails $k=2$.\n    *   $(\\text{1975, M})$: size $1$. Fails $k=2$.\n    The entire strategy fails because it does not produce a $k=2$ anonymous dataset. Furthermore, even for the groups of size $2$, none can satisfy $l=3$, as there are only $2$ records per group. For instance, group $(\\text{1990, M})$ has diagnoses {Depression, HIV}, so its distinct diversity is $l=2$, which fails the goal of $l=3$.\n*   **Verdict:** **Incorrect**. The strategy fails to meet both the $k=2$ requirement and the $l=3$ requirement.\n\n**E. Suppressing records that are unique in the release automatically enforces $l$-diversity. Set $k=2$, $l=2$; generalize ZIP5 to ZIP3, leave Year exact, and suppress any records that remain unique on $(\\text{ZIP3}, \\text{Year}, \\text{Sex})$; no attention to Diagnosis distribution is needed thereafter.**\n*   **Vulnerability Explanation:** The claim that suppressing unique records \"automatically enforces $l$-diversity\" is fundamentally false. $k$-anonymity and $l$-diversity are distinct properties. A $k$-anonymous group can easily fail $l$-diversity if all $k$ members share the same sensitive attribute. This shows a critical misunderstanding of the principles.\n*   **Strategy Analysis:** Generalize ZIP5 to ZIP3. Leave Year and Sex exact. The QIs are $(\\text{Year}, \\text{Sex})$. The records with QIs $(\\text{1980, F})$ and $(\\text{1975, M})$ are unique (size $1$) and would be suppressed. The remaining $8$ records form groups of size $2$. Let's examine the group with QI $(\\text{1982, F})$. It has size $2$, so it meets $k=2$. Its diagnoses are {Flu, Flu}. The number of distinct diagnoses is $1$. This fails to meet the $l=2$ requirement. The premise that \"no attention to Diagnosis distribution is needed\" is the very reason the strategy fails.\n*   **Verdict:** **Incorrect**. The reasoning about privacy principles is flawed, and the resulting strategy does not satisfy $l=2$.", "answer": "$$\\boxed{B}$$", "id": "4837957"}, {"introduction": "While methods like $k$-anonymity offer some protection, they lack a formal, provable guarantee against a determined adversary. We now turn to a more rigorous framework: Differential Privacy (DP), which has become the gold standard for private data analysis. This practice provides a hands-on exploration of the Laplace mechanism, a fundamental tool in DP, by asking you to derive how adding mathematically calibrated noise can protect individual privacy within a dataset [@problem_id:4838057]. You will calculate the precise amount of noise needed to achieve a given privacy level, $\\epsilon$, and evaluate its direct impact on the utility of the data, illustrating the core trade-off between privacy and accuracy.", "problem": "A regional health authority seeks to publish a histogram of diagnosis counts from an electronic health dataset while upholding the ethical principle of patient privacy and confidentiality. To formalize privacy protection, they adopt Differential Privacy (DP). A randomized mechanism $\\mathcal{M}$ satisfies $\\epsilon$-Differential Privacy (DP) if for all neighboring datasets $D$ and $D'$ that differ in one individual's record, and for all measurable sets $S$ of outputs,\n$$\n\\Pr\\big(\\mathcal{M}(D)\\in S\\big) \\leq \\exp(\\epsilon)\\,\\Pr\\big(\\mathcal{M}(D')\\in S\\big).\n$$\nThe mechanism releases a count query $f(D)$ for each histogram bin by adding noise $X$ drawn from a Laplace distribution with density\n$$\np_{X}(x) = \\frac{1}{2b}\\exp\\!\\left(-\\frac{|x|}{b}\\right),\n$$\nwhere $b>0$ is a scale parameter to be determined from the DP requirement and the global sensitivity of the query. For a single-bin count in a diagnosis histogram, the global $\\ell_{1}$ sensitivity is $\\Delta = 1$, because adding or removing one record changes any single count by at most $1$.\n\nStarting from the formal DP definition and the Laplace mechanism density above, derive the necessary relationship between the scale parameter $b$, the sensitivity $\\Delta$, and the privacy parameter $\\epsilon$ to ensure $\\epsilon$-DP. Then, using properties of the Laplace distribution, derive the expected absolute error $\\mathbb{E}[|X|]$ introduced into the released count.\n\nFinally, define the impact on a rare disease count as the expected relative absolute error\n$$\nr \\equiv \\frac{\\mathbb{E}[|X|]}{c},\n$$\nwhere $c$ is the true count in the bin. Compute the scale $b$, the expected absolute error $\\mathbb{E}[|X|]$, and the impact $r$ for $\\epsilon = 0.5$, $\\Delta = 1$, and a rare disease count of $c = 12$. Express your final answers as exact values without rounding, and present them as a single row matrix containing, in order, $b$, $\\mathbb{E}[|X|]$, and $r$.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established theory of Differential Privacy, well-posed with a clear objective, and all necessary information is provided in a consistent and objective manner.\n\nWe proceed with the derivation as requested.\n\nThe problem asks for three main derivations and calculations:\n1.  The relationship between the Laplace scale parameter $b$, the global sensitivity $\\Delta$, and the privacy parameter $\\epsilon$.\n2.  The expected absolute error $\\mathbb{E}[|X|]$ for the Laplace noise.\n3.  The numerical values for $b$, $\\mathbb{E}[|X|]$, and the relative impact $r$ given the specified parameters.\n\n**Part 1: Derivation of the relationship between $b$, $\\Delta$, and $\\epsilon$**\n\nThe randomized mechanism $\\mathcal{M}$ takes a dataset $D$ and outputs a noisy version of the query result $f(D)$. The output is $\\mathcal{M}(D) = f(D) + X$, where $X$ is a random variable drawn from the Laplace distribution with probability density function (PDF):\n$$\np_{X}(x) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)\n$$\nAn arbitrary output value $z$ is produced with a probability density proportional to $p_X(z - f(D))$. Thus, the probability density of observing output $z$ given dataset $D$ is $p(z|D) = p_X(z - f(D))$.\n\nThe definition of $\\epsilon$-Differential Privacy (DP) states that for any two neighboring datasets $D$ and $D'$, which differ by a single individual's record, and for any set of possible outputs $S$, the following inequality must hold:\n$$\n\\Pr\\big(\\mathcal{M}(D)\\in S\\big) \\leq \\exp(\\epsilon)\\,\\Pr\\big(\\mathcal{M}(D')\\in S\\big)\n$$\nThis condition is satisfied if the ratio of the probability densities for any output $z$ is bounded:\n$$\n\\frac{p(z|D)}{p(z|D')} \\leq \\exp(\\epsilon)\n$$\nLet us compute this ratio using the Laplace PDF:\n$$\n\\frac{p(z|D)}{p(z|D')} = \\frac{p_X(z - f(D))}{p_X(z - f(D'))} = \\frac{\\frac{1}{2b}\\exp\\left(-\\frac{|z - f(D)|}{b}\\right)}{\\frac{1}{2b}\\exp\\left(-\\frac{|z - f(D')|}{b}\\right)} = \\exp\\left(\\frac{|z - f(D')| - |z - f(D)|}{b}\\right)\n$$\nTo find the maximum possible value of this ratio, we need to maximize the term in the exponent. By the reverse triangle inequality, $|a| - |b| \\leq |a - b|$. Let $a = z - f(D')$ and $b = z - f(D)$. Then:\n$$\n|z - f(D')| - |z - f(D)| \\leq |(z - f(D')) - (z - f(D))| = |f(D) - f(D')|\n$$\nThe global $\\ell_1$ sensitivity $\\Delta$ of the query $f$ is defined as the maximum possible change in the $\\ell_1$ norm of the output when one record is changed in the input dataset:\n$$\n\\Delta = \\max_{D, D'} \\|f(D) - f(D')\\|_1\n$$\nFor a single-bin count query, the function $f$ returns a scalar, so the $\\ell_1$ norm is simply the absolute value: $\\|f(D) - f(D')\\|_1 = |f(D) - f(D')|$. Thus,\n$$\n|z - f(D')| - |z - f(D)| \\leq |f(D) - f(D')| \\leq \\Delta\n$$\nSubstituting this upper bound into the exponent of the probability density ratio:\n$$\n\\frac{p(z|D)}{p(z|D')} \\leq \\exp\\left(\\frac{\\Delta}{b}\\right)\n$$\nTo satisfy the $\\epsilon$-DP condition, this ratio must be bounded by $\\exp(\\epsilon)$. Therefore, we require:\n$$\n\\exp\\left(\\frac{\\Delta}{b}\\right) \\leq \\exp(\\epsilon)\n$$\nTaking the natural logarithm of both sides gives:\n$$\n\\frac{\\Delta}{b} \\leq \\epsilon\n$$\nTo add the minimum amount of noise that still guarantees $\\epsilon$-DP, we choose the smallest possible value for the scale parameter $b$, which corresponds to making the inequality an equality:\n$$\nb = \\frac{\\Delta}{\\epsilon}\n$$\nThis is the required relationship.\n\n**Part 2: Derivation of the expected absolute error $\\mathbb{E}[|X|]$**\n\nThe expected absolute error introduced by the mechanism is a measure of the noise magnitude. This is given by the expectation of the absolute value of the noise variable $X$, which is $\\mathbb{E}[|X|]$. We can calculate this using the definition of expected value for a continuous random variable:\n$$\n\\mathbb{E}[|X|] = \\int_{-\\infty}^{\\infty} |x| p_X(x) \\, dx = \\int_{-\\infty}^{\\infty} |x| \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right) \\, dx\n$$\nThe integrand $|x| \\exp(-|x|/b)$ is an even function, so we can simplify the integral by integrating over $[0, \\infty)$ and multiplying by $2$:\n$$\n\\mathbb{E}[|X|] = 2 \\int_{0}^{\\infty} x \\frac{1}{2b}\\exp\\left(-\\frac{x}{b}\\right) \\, dx = \\frac{1}{b} \\int_{0}^{\\infty} x \\exp\\left(-\\frac{x}{b}\\right) \\, dx\n$$\nWe solve the integral using integration by parts, $\\int u \\, dv = uv - \\int v \\, du$. Let $u = x$ and $dv = \\exp(-x/b) \\, dx$. Then $du = dx$ and $v = -b \\exp(-x/b)$.\n$$\n\\int_{0}^{\\infty} x \\exp\\left(-\\frac{x}{b}\\right) \\, dx = \\left[ -bx \\exp\\left(-\\frac{x}{b}\\right) \\right]_{0}^{\\infty} - \\int_{0}^{\\infty} (-b) \\exp\\left(-\\frac{x}{b}\\right) \\, dx\n$$\nThe first term evaluates to $0$:\n$$\n\\lim_{x \\to \\infty} \\left(-bx \\exp\\left(-\\frac{x}{b}\\right)\\right) - (-b \\cdot 0 \\cdot \\exp(0)) = -b \\lim_{x \\to \\infty} \\frac{x}{\\exp(x/b)} - 0\n$$\nUsing L'Hôpital's rule on the limit:\n$$\n\\lim_{x \\to \\infty} \\frac{x}{\\exp(x/b)} = \\lim_{x \\to \\infty} \\frac{1}{(1/b)\\exp(x/b)} = 0\n$$\nSo, the first term is $0$. The integral becomes:\n$$\n\\int_{0}^{\\infty} x \\exp\\left(-\\frac{x}{b}\\right) \\, dx = \\int_{0}^{\\infty} b \\exp\\left(-\\frac{x}{b}\\right) \\, dx = b \\left[ -b \\exp\\left(-\\frac{x}{b}\\right) \\right]_{0}^{\\infty}\n$$\n$$\n= b \\left( \\lim_{x \\to \\infty} \\left(-b \\exp\\left(-\\frac{x}{b}\\right)\\right) - \\left(-b \\exp(0)\\right) \\right) = b(0 - (-b)) = b^2\n$$\nSubstituting this result back into the expression for $\\mathbb{E}[|X|]$:\n$$\n\\mathbb{E}[|X|] = \\frac{1}{b} (b^2) = b\n$$\nThus, the expected absolute error is equal to the scale parameter $b$.\n\n**Part 3: Calculation of $b$, $\\mathbb{E}[|X|]$, and $r$**\n\nWe are given the following parameters:\n- Privacy parameter: $\\epsilon = 0.5$\n- Global sensitivity: $\\Delta = 1$\n- True count of the rare disease: $c = 12$\n\nUsing the derived formulas, we can compute the required values.\n\n- **Scale parameter $b$**:\n$$\nb = \\frac{\\Delta}{\\epsilon} = \\frac{1}{0.5} = 2\n$$\n\n- **Expected absolute error $\\mathbb{E}[|X|]$**:\n$$\n\\mathbb{E}[|X|] = b = 2\n$$\n\n- **Impact $r$**:\nThe impact is defined as the expected relative absolute error:\n$$\nr = \\frac{\\mathbb{E}[|X|]}{c}\n$$\nSubstituting the values:\n$$\nr = \\frac{2}{12} = \\frac{1}{6}\n$$\n\nThe final answers are $b=2$, $\\mathbb{E}[|X|]=2$, and $r=\\frac{1}{6}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 2 & \\frac{1}{6} \\end{pmatrix}}\n$$", "id": "4838057"}, {"introduction": "Beyond protecting data privacy, we have an ethical duty to ensure that the algorithms we build are fair and do not perpetuate or amplify existing health disparities. This final practice shifts our focus from data protection to algorithmic justice. You will be presented with the performance of a machine learning screening tool across two different demographic groups and asked to calculate several standard fairness metrics: statistical parity, equal opportunity, and calibration [@problem_id:4838050]. The exercise moves beyond simple calculation, challenging you to reason from the core principles of medical ethics—beneficence, nonmaleficence, and justice—to determine which definition of fairness is most critical in a specific clinical context.", "problem": "A health system deploys a binary machine learning screening tool to flag patients for further diagnostic workup for a serious but treatable condition. Two demographic groups, labeled $A$ and $B$, each consist of $1{,}000$ patients. The observed screening outcomes (predicted positive or predicted negative) and true disease status (diseased or not diseased) yield the following confusion-matrix counts.\n\nFor group $A$:\n- Diseased patients: $320$ predicted positive, $80$ predicted negative.\n- Not diseased patients: $180$ predicted positive, $420$ predicted negative.\n\nFor group $B$:\n- Diseased patients: $210$ predicted positive, $90$ predicted negative.\n- Not diseased patients: $140$ predicted positive, $560$ predicted negative.\n\nStarting from fundamental definitions of rates derived from the confusion matrix in binary classification, compute the following cross-group fairness quantities for the screening tool:\n\n1. The statistical parity difference, defined as the difference in the positive classification rates between groups $A$ and $B$.\n2. The equal opportunity difference, defined as the difference in true positive rates between groups $A$ and $B$.\n3. The calibration error magnitude across groups, operationalized here as the absolute difference in positive predictive values between groups $A$ and $B$.\n\nThen, reason from first principles of medical ethics, including justice, beneficence, and nonmaleficence, to assess which single fairness criterion among statistical parity, equal opportunity, and calibration within groups is most ethically appropriate for a screening context in medical informatics. Explicitly justify your assessment in your derivation.\n\nExpress each computed fairness quantity as a decimal and round to four significant figures. Provide the final answer as a row matrix in the order statistical parity difference, equal opportunity difference, calibration error magnitude. No units are required in the final numeric answer.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n\nThe problem provides the following data derived from a binary machine learning screening tool applied to two demographic groups, $A$ and $B$, each with $1{,}000$ patients.\n\nFor group $A$:\n- Total patients: $N_A = 1{,}000$\n- Diseased patients: $320$ predicted positive, $80$ predicted negative.\n- Not diseased patients: $180$ predicted positive, $420$ predicted negative.\n\nFor group $B$:\n- Total patients: $N_B = 1{,}000$\n- Diseased patients: $210$ predicted positive, $90$ predicted negative.\n- Not diseased patients: $140$ predicted positive, $560$ predicted negative.\n\nThe tasks are:\n1. Compute the statistical parity difference.\n2. Compute the equal opportunity difference.\n3. Compute the calibration error magnitude.\n4. Assess which fairness criterion (statistical parity, equal opportunity, or calibration) is most ethically appropriate for this context, using the principles of justice, beneficence, and nonmaleficence.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, utilizing standard definitions from machine learning fairness (statistical parity, equal opportunity, calibration/positive predictive value) and established principles of medical ethics. The provided data is self-contained and consistent. For group $A$, the total patients are $320 + 80 + 180 + 420 = 1{,}000$. For group $B$, the total is $210 + 90 + 140 + 560 = 1{,}000$. Both match the stated group sizes. The problem is well-posed, as the quantities to be computed are precisely defined and the data allows for their unique calculation. The language is objective. The scenario is a realistic application in medical informatics. No flaws are identified.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A full solution will be provided.\n\n**Derivation**\n\nLet $TP$ denote True Positives, $FN$ denote False Negatives, $FP$ denote False Positives, and $TN$ denote True Negatives. The subscripts $A$ and $B$ will denote the respective groups.\n\nFrom the problem statement, we can populate the confusion matrices.\n\nFor group $A$:\n- $TP_A = 320$\n- $FN_A = 80$\n- $FP_A = 180$\n- $TN_A = 420$\n\nFor group $B$:\n- $TP_B = 210$\n- $FN_B = 90$\n- $FP_B = 140$\n- $TN_B = 560$\n\nFrom these counts, we can compute the total number of patients with the disease ($P$), the total number of patients without the disease ($N$), the total number of patients predicted positive ($PP$), and the total number of patients in each group ($T$).\n\nFor group $A$:\n- Total with disease: $P_A = TP_A + FN_A = 320 + 80 = 400$\n- Total predicted positive: $PP_A = TP_A + FP_A = 320 + 180 = 500$\n- Total patients: $T_A = TP_A + FN_A + FP_A + TN_A = 320 + 80 + 180 + 420 = 1{,}000$\n\nFor group $B$:\n- Total with disease: $P_B = TP_B + FN_B = 210 + 90 = 300$\n- Total predicted positive: $PP_B = TP_B + FP_B = 210 + 140 = 350$\n- Total patients: $T_B = TP_B + FN_B + FP_B + TN_B = 210 + 90 + 140 + 560 = 1{,}000$\n\nThe prevalence of the disease in each group is different:\n- Prevalence in group $A$: $\\frac{P_A}{T_A} = \\frac{400}{1{,}000} = 0.4$\n- Prevalence in group $B$: $\\frac{P_B}{T_B} = \\frac{300}{1{,}000} = 0.3$\n\nNow, we define and compute the required rates for each group.\n\n1.  **Positive Classification Rate ($PCR$)**: The proportion of the group that is predicted positive.\n    $$PCR = \\frac{TP + FP}{T}$$\n    - $PCR_A = \\frac{500}{1{,}000} = 0.5$\n    - $PCR_B = \\frac{350}{1{,}000} = 0.35$\n\n2.  **True Positive Rate ($TPR$)**, or sensitivity: The proportion of diseased individuals who are correctly predicted positive.\n    $$TPR = \\frac{TP}{TP + FN} = \\frac{TP}{P}$$\n    - $TPR_A = \\frac{TP_A}{P_A} = \\frac{320}{400} = 0.8$\n    - $TPR_B = \\frac{TP_B}{P_B} = \\frac{210}{300} = 0.7$\n\n3.  **Positive Predictive Value ($PPV$)**, or precision: The proportion of positive predictions that are correct.\n    $$PPV = \\frac{TP}{TP + FP} = \\frac{TP}{PP}$$\n    - $PPV_A = \\frac{TP_A}{PP_A} = \\frac{320}{500} = 0.64$\n    - $PPV_B = \\frac{TP_B}{PP_B} = \\frac{210}{350} = 0.6$\n\nNext, we compute the specified cross-group fairness quantities.\n\n1.  **Statistical Parity Difference ($\\Delta_{SP}$)**: The difference in positive classification rates.\n    $$\\Delta_{SP} = PCR_A - PCR_B = 0.5 - 0.35 = 0.15$$\n    Rounding to four significant figures, this is $0.1500$.\n\n2.  **Equal Opportunity Difference ($\\Delta_{EO}$)**: The difference in true positive rates.\n    $$\\Delta_{EO} = TPR_A - TPR_B = 0.8 - 0.7 = 0.1$$\n    Rounding to four significant figures, this is $0.1000$.\n\n3.  **Calibration Error Magnitude ($|\\Delta_{Cal}|$)**: The absolute difference in positive predictive values.\n    $$|\\Delta_{Cal}| = |PPV_A - PPV_B| = |0.64 - 0.6| = 0.04$$\n    Rounding to four significant figures, this is $0.04000$.\n\nFinally, we assess which fairness criterion is most ethically appropriate for this context. The problem specifies a screening tool for a \"serious but treatable condition.\" This context is critical.\n\n-   **Beneficence** (to act in the best interest of the patient) and **Nonmaleficence** (to do no harm) are central. For a serious, treatable condition, the greatest benefit comes from correctly identifying sick individuals so they can be treated ($TP$), and the greatest harm comes from failing to identify them ($FN$).\n-   **Justice** relates to the fair distribution of benefits and burdens.\n\nLet us evaluate the three fairness criteria in light of these principles:\n\n-   **Statistical Parity** would require $PCR_A \\approx PCR_B$. This criterion ensures that both groups are subjected to follow-up testing at similar rates. While this equalizes the burden of the screening process itself, it ignores the different underlying prevalences of the disease ($0.4$ in group $A$ vs. $0.3$ in group $B$). Enforcing statistical parity on groups with different prevalences can force a model to be less accurate, potentially increasing harmful false negatives in the higher-prevalence group or burdensome false positives in the lower-prevalence group. This conflicts with the principles of beneficence and nonmaleficence by prioritizing procedural equality over clinical accuracy.\n\n-   **Calibration** would require $PPV_A \\approx PPV_B$. This ensures that a positive test result has the same meaning (i.e., the same probability of being correct) for a patient from either group. This is important for clinical decision-making and nonmaleficence, as it prevents physicians from misinterpreting the significance of a positive screen based on group membership. It minimizes the harm of unnecessary follow-up procedures by ensuring the test's positive signal is consistently reliable. In this case, the calibration is quite good ($|0.64 - 0.6| = 0.04$).\n\n-   **Equal Opportunity** would require $TPR_A \\approx TPR_B$. This criterion demands that individuals who are actually sick have an equal chance of being identified by the screening tool, regardless of their group. In this case, there is a disparity: sick individuals in group $A$ have an $80\\%$ chance of being flagged ($TPR_A=0.8$), while those in group $B$ have only a $70\\%$ chance ($TPR_B=0.7$). Given that the condition is \"serious but treatable,\" a false negative represents a catastrophic failure: a sick person is denied access to treatment. This directly violates the principle of beneficence (failing to provide benefit) and causes significant harm (violating nonmaleficence). From a justice perspective, it means the primary benefit of the screening program—early detection and treatment—is not being distributed fairly. The members of group $B$ are systematically disadvantaged.\n\n**Conclusion of Ethical Assessment:**\nWhile calibration is important for the interpretability and trustworthiness of a positive result, the most critical ethical imperative in screening for a *serious, treatable condition* is to ensure that all individuals who have the disease receive an equal opportunity for detection and subsequent treatment. A failure of equal opportunity means one group is systematically deprived of this life-altering benefit. Therefore, **equal opportunity** is the most ethically appropriate fairness criterion in this specific context, as it directly aligns with the core principles of beneficence, nonmaleficence, and justice regarding the equitable distribution of clinical benefit.\n\nThe final computed values for the three metrics are $0.1500$, $0.1000$, and $0.04000$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1500 & 0.1000 & 0.04000 \\end{pmatrix}}\n$$", "id": "4838050"}]}