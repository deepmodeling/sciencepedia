{"hands_on_practices": [{"introduction": "Sequencing directly from clinical samples is a powerful tool, but it presents the challenge of separating the pathogen's genetic signal from the noise of host DNA. Before diving into complex analyses, a crucial first step is to assess whether we have sequenced enough of the pathogen's genome to draw meaningful conclusions. This exercise simulates a realistic scenario where you will calculate the effective pathogen coverage from a mixed sample and use this to evaluate the feasibility of genome assembly and variant calling, two fundamental tasks in pathogen surveillance. [@problem_id:4688565]", "problem": "A public health laboratory performs whole-genome sequencing (WGS) directly on a nasopharyngeal swab during an outbreak to monitor a suspected bacterial pathogen. Sequencing produced $R$ paired-end read pairs, each read of length $L$ nucleotides, from a mixed sample in which a fraction $h$ of reads are of human origin. Among the non-human reads, a fraction $f$ taxonomically classifies to the pathogen of interest. The pathogen genome size is $G$ nucleotides. Assume all pathogen-assigned reads map uniquely and uniformly across the pathogen genome.\n\nUsing only core definitions and well-tested assumptions:\n- Average depth (also called coverage) is defined as total sequenced bases assigned to the target divided by the target genome size.\n- Under the classical random coverage (Poisson) model of Lander and Waterman, the number of times a given base is covered follows a Poisson distribution with mean equal to the average depth; therefore, the expected breadth of coverage (the fraction of bases covered at least once) can be derived from this model.\n\nGiven the following values:\n- $R = 25{,}000{,}000$ read pairs,\n- $L = 150$ nucleotides per read,\n- $h = 0.98$,\n- $f = 0.70$,\n- $G = 4{,}800{,}000$ nucleotides,\n\ncompute the expected residual pathogen average depth after excluding human reads and accounting for the taxonomic fraction $f$. Then, under the Poisson coverage assumption, assess viability for de novo assembly and for single-nucleotide variant calling using the following criteria:\n- Assembly is viable if average depth is at least $30$ and expected breadth is at least $0.95$.\n- Variant calling is viable if average depth is at least $20$ and expected breadth is at least $0.90$.\n\nReport as your final answer only the computed residual pathogen average depth as a pure number (fold depth, dimensionless), rounded to three significant figures. Use any additional conclusions only in your solution reasoning; do not include them in the final answer.", "solution": "The problem has been validated and is scientifically grounded, well-posed, and objective. It contains sufficient information for a unique solution based on established principles in genomics and bioinformatics.\n\nThe objective is to compute the average sequencing depth for a pathogen genome from a metagenomic sample and subsequently assess the viability for de novo assembly and single-nucleotide variant (SNV) calling based on given criteria.\n\nThe following parameters are provided:\n- Total number of paired-end read pairs: $R = 25,000,000 = 2.5 \\times 10^7$\n- Length of each read: $L = 150$ nucleotides\n- Fraction of human reads (host contamination): $h = 0.98$\n- Fraction of pathogen reads among non-human reads: $f = 0.70$\n- Pathogen genome size: $G = 4,800,000 = 4.8 \\times 10^6$ nucleotides\n\nFirst, we calculate the total number of sequenced bases in the experiment. Since there are $R$ read pairs and each read has a length of $L$, the total number of reads is $2R$, and the total number of sequenced bases, $B_{total}$, is:\n$$B_{total} = 2 \\times R \\times L$$\n\nNext, we determine the number of bases that belong to the pathogen of interest. The fraction of reads originating from human DNA is $h$. Therefore, the fraction of non-human reads is $(1 - h)$.\nFraction of non-human reads $= 1 - 0.98 = 0.02$.\n\nOf these non-human reads, a fraction $f$ is attributed to the pathogen. Thus, the total number of bases sequenced from the pathogen, $B_{pathogen}$, is:\n$$B_{pathogen} = B_{total} \\times (1 - h) \\times f = 2 \\times R \\times L \\times (1 - h) \\times f$$\n\nSubstituting the given values:\n$$B_{pathogen} = 2 \\times (2.5 \\times 10^7) \\times 150 \\times (1 - 0.98) \\times 0.70$$\n$$B_{pathogen} = (5.0 \\times 10^7) \\times 150 \\times 0.02 \\times 0.70$$\n$$B_{pathogen} = (7.5 \\times 10^9) \\times 0.014$$\n$$B_{pathogen} = 1.05 \\times 10^8 \\text{ bases}$$\n\nThe average depth, also known as coverage ($C$), is defined as the total number of bases assigned to the target genome divided by the size of the target genome.\n$$C = \\frac{B_{pathogen}}{G}$$\n\nUsing the calculated value for $B_{pathogen}$ and the given value for $G$:\n$$C = \\frac{1.05 \\times 10^8}{4.8 \\times 10^6}$$\n$$C = \\frac{105}{4.8} = \\frac{1050}{48} = \\frac{525}{24} = \\frac{175}{8}$$\n$$C = 21.875$$\nThe average depth of coverage for the pathogen genome is $21.875\\text{x}$. The problem requests this value rounded to three significant figures, which is $21.9$.\n\nNow, we must assess the viability for de novo assembly and variant calling. This requires calculating the expected breadth of coverage. According to the Lander-Waterman model, the probability that a given base is not covered (i.e., covered $0$ times) follows a Poisson distribution with mean $\\lambda = C$. The probability of zero coverage, $P(k=0)$, is:\n$$P(k=0) = \\frac{e^{-C} C^0}{0!} = e^{-C}$$\nThe expected breadth of coverage, $B$, is the fraction of the genome covered at least once, which is $1 - P(k=0)$.\n$$B = 1 - e^{-C}$$\nFor our calculated average depth $C = 21.875$:\n$$B = 1 - e^{-21.875}$$\nThe value of $e^{-21.875}$ is extremely small, approximately $3.16 \\times 10^{-10}$. Therefore, the breadth of coverage is effectively $1.0$ to a very high precision.\n$$B \\approx 1.0$$\n\nWe can now evaluate the viability based on the provided criteria:\n\n1.  **De novo assembly viability:**\n    - Criterion 1: Average depth $\\ge 30$. Our calculated depth is $C = 21.875$. Since $21.875  30$, this criterion is **not met**.\n    - Criterion 2: Expected breadth $\\ge 0.95$. Our calculated breadth is $B \\approx 1.0$. Since $1.0 \\ge 0.95$, this criterion is met.\n    - Because the average depth criterion is not met, de novo assembly is deemed **not viable**.\n\n2.  **SNV calling viability:**\n    - Criterion 1: Average depth $\\ge 20$. Our calculated depth is $C = 21.875$. Since $21.875 \\ge 20$, this criterion is **met**.\n    - Criterion 2: Expected breadth $\\ge 0.90$. Our calculated breadth is $B \\approx 1.0$. Since $1.0 \\ge 0.90$, this criterion is **met**.\n    - Because both criteria are met, SNV calling is deemed **viable**.\n\nThe problem asks for the computed residual pathogen average depth, rounded to three significant figures.\n\nAverage Depth $C = 21.875$.\nRounding to three significant figures gives $21.9$.", "answer": "$$\\boxed{21.9}$$", "id": "4688565"}, {"introduction": "Identifying the true genetic variants that distinguish pathogen isolates is central to tracking an outbreak, but sequencing instruments are not infallible. This practice moves beyond simple read counting to introduce a more rigorous, probabilistic approach to variant calling rooted in Bayes’ theorem. You will derive an expression for the posterior probability of a true variant, learning how to formally combine prior knowledge about mutation rates with the evidence provided by sequencing reads and their associated quality scores. [@problem_id:4688508]", "problem": "In a pathogen outbreak investigation using Whole-Genome Sequencing (WGS), a site in the genome shows evidence for a non-reference base from sequencing data. Suppose there are $k$ independent reads supporting this same non-reference base at the site, and the average Phred quality score (Phred) of those base calls is $Q$. Assume the following generative model:\n\n- The Phred quality score $Q$ relates to the probability of an incorrect base call $p_{\\mathrm{err}}$ by the definition of Phred quality, namely $Q = -10 \\log_{10}(p_{\\mathrm{err}})$. This implies $p_{\\mathrm{err}} = 10^{-Q/10}$.\n- Under the hypothesis of a true variant at the site ($H_{1}$), each supporting read correctly reports the non-reference base with probability $1 - p_{\\mathrm{err}}$, independently across reads.\n- Under the hypothesis of no true variant ($H_{0}$), each supporting read reports the non-reference base only by sequencing error with probability $p_{\\mathrm{err}}$, independently across reads.\n- The prior probability that any given site harbors a true variant in this pathogen sample is $\\pi$, and the prior probability of no true variant is $1 - \\pi$.\n\nUsing Bayes’ theorem and the assumptions above, derive a closed-form expression for the posterior probability of a true variant at the site, $P(H_{1} \\mid \\text{$k$ supporting reads with average quality $Q$})$, in terms of $k$, $Q$, and $\\pi$. Your final answer must be a single closed-form analytic expression. No numerical approximation is required.", "solution": "The objective is to derive a closed-form expression for the posterior probability of a true variant at a genomic site, given evidence from whole-genome sequencing data. We are asked to find $P(H_{1} \\mid D)$, where $H_{1}$ is the hypothesis that a true variant exists, and $D$ is the observation of $k$ independent reads supporting a non-reference base, with an average Phred quality score of $Q$.\n\nWe begin by applying Bayes’ theorem, which relates the posterior probability to the prior probability and the likelihood of the data:\n$$\nP(H_{1} \\mid D) = \\frac{P(D \\mid H_{1}) P(H_{1})}{P(D)}\n$$\n\nThe denominator, $P(D)$, is the marginal probability of observing the data, which can be computed using the law of total probability by summing over all possible hypotheses. In this case, there are two mutually exclusive hypotheses: $H_{1}$ (a true variant exists) and $H_{0}$ (no true variant exists).\n$$\nP(D) = P(D \\mid H_{1}) P(H_{1}) + P(D \\mid H_{0}) P(H_{0})\n$$\n\nSubstituting this into the Bayes' theorem expression gives:\n$$\nP(H_{1} \\mid D) = \\frac{P(D \\mid H_{1}) P(H_{1})}{P(D \\mid H_{1}) P(H_{1}) + P(D \\mid H_{0}) P(H_{0})}\n$$\n\nNext, we must define the terms in this equation based on the problem statement.\n\nThe prior probabilities are given as:\n-   $P(H_{1}) = \\pi$ (the prior probability of a true variant)\n-   $P(H_{0}) = 1 - \\pi$ (the prior probability of no true variant)\n\nThe Phred quality score $Q$ is related to the probability of an incorrect base call, $p_{\\mathrm{err}}$, by the formula:\n$$\nQ = -10 \\log_{10}(p_{\\mathrm{err}})\n$$\nSolving for $p_{\\mathrm{err}}$, we get:\n$$\np_{\\mathrm{err}} = 10^{-Q/10}\n$$\nThis value of $p_{\\mathrm{err}}$ will be treated as the error probability for each of the $k$ reads. The probability of a *correct* base call is therefore $1 - p_{\\mathrm{err}}$.\n\nNow, we formulate the likelihoods, which are the probabilities of observing the data $D$ under each hypothesis. The data $D$ is the event that all $k$ independent reads support the non-reference base.\n\n1.  Likelihood under hypothesis $H_{1}$ (true variant):\n    If a true variant exists, observing a read that supports this variant is a *correct* call. The probability for a single read to be correct is $1 - p_{\\mathrm{err}}$. Since the $k$ reads are independent, the probability that all $k$ reads correctly report the variant is:\n    $$\n    P(D \\mid H_{1}) = (1 - p_{\\mathrm{err}})^k\n    $$\n\n2.  Likelihood under hypothesis $H_{0}$ (no true variant):\n    If no true variant exists, observing a read that supports the non-reference base must be a sequencing *error*. The probability for a single read to be an error is $p_{\\mathrm{err}}$. Since the $k$ reads are independent, the probability that all $k$ reads erroneously report the non-reference base is:\n    $$\n    P(D \\mid H_{0}) = (p_{\\mathrm{err}})^k\n    $$\n\nNow, we substitute the priors and the derived likelihoods into the full Bayesian formula:\n$$\nP(H_{1} \\mid D) = \\frac{(1 - p_{\\mathrm{err}})^k \\cdot \\pi}{(1 - p_{\\mathrm{err}})^k \\cdot \\pi + (p_{\\mathrm{err}})^k \\cdot (1 - \\pi)}\n$$\n\nThe final step is to express this posterior probability in terms of the given variables $k$, $Q$, and $\\pi$. We substitute $p_{\\mathrm{err}} = 10^{-Q/10}$ into the expression:\n$$\nP(H_{1} \\mid \\text{$k$ reads, quality $Q$}) = \\frac{\\pi (1 - 10^{-Q/10})^k}{\\pi (1 - 10^{-Q/10})^k + (1 - \\pi) (10^{-Q/10})^k}\n$$\n\nThis equation represents the closed-form expression for the posterior probability of a true variant. It can be further simplified to highlight the role of the prior odds and the likelihood ratio. Let's divide the numerator and the denominator by $\\pi (1 - 10^{-Q/10})^k$:\n$$\nP(H_{1} \\mid D) = \\frac{1}{1 + \\frac{(1 - \\pi)}{\\pi} \\frac{(10^{-Q/10})^k}{(1 - 10^{-Q/10})^k}}\n$$\nThis can be written as:\n$$\nP(H_{1} \\mid D) = \\frac{1}{1 + \\frac{1 - \\pi}{\\pi} \\left( \\frac{10^{-Q/10}}{1 - 10^{-Q/10}} \\right)^k}\n$$\nBoth forms are equivalent. The first form derived is more direct. We will present the first form as the final answer.", "answer": "$$\n\\boxed{\\frac{\\pi (1 - 10^{-Q/10})^k}{\\pi (1 - 10^{-Q/10})^k + (1 - \\pi) (10^{-Q/10})^k}}\n$$", "id": "4688508"}, {"introduction": "The ultimate goal of genomic surveillance is to reconstruct outbreak dynamics and inform public health interventions by translating raw SNP distances between isolates into meaningful transmission clusters. In this computational practice, you will use graph theory to model an outbreak, defining clusters based on a genetic relatedness threshold. This exercise challenges you to track how these clusters grow and merge over time as new isolates are sequenced, providing direct insight into the dynamics of disease transmission. [@problem_id:4688569]", "problem": "A set of pathogen isolates has been sequenced using Whole-Genome Sequencing (WGS), producing a pairwise single nucleotide polymorphism (SNP) distance matrix. Under a fixed transmission plausibility threshold, isolates whose pairwise SNP distance is below or equal to the threshold are considered linked. This yields an undirected graph whose connected components operationally define surveillance clusters. Over time, newly sampled isolates expand the graph, potentially changing cluster membership. The task is to compute cluster sizes at specified cumulative time snapshots and a principled stability score for how cluster membership of already observed isolates changes between consecutive snapshots.\n\nGiven the following definitions grounded in medical microbiology and graph theory:\n- Single nucleotide polymorphism (SNP) count approximates genetic divergence consistent with the Central Dogma of Molecular Biology, where differences at the nucleotide level accumulate over replication. A pairwise SNP distance between isolates $i$ and $j$ is denoted $D_{ij}$, where $D_{ij} \\in \\mathbb{N}_0$ and $D_{ij} = D_{ji}$ with $D_{ii} = 0$.\n- For a chosen threshold $\\tau \\in \\mathbb{N}_0$, construct an undirected simple graph $G = (V,E)$ with vertex set $V = \\{0,1,\\dots,n-1\\}$ and edge set $E = \\{ \\{i,j\\} : D_{ij} \\le \\tau, i \\neq j \\}$. A connected component is a maximal subset of $V$ in which any two vertices are connected by a path.\n- Isolates are sampled at integer time points $t_i \\in \\mathbb{Z}$ (e.g., days). For a nondecreasing list of snapshot times $T_1, T_2, \\dots, T_K$ with $T_k \\in \\mathbb{Z}$, define the cumulative active set at snapshot $k$ as $S_k = \\{ i \\in V : t_i \\le T_k \\}$.\n- For each snapshot $k$, compute the connected components of the graph induced by $S_k$, and record the multiset of component sizes as a list of integers sorted in nonincreasing order.\n- Define a stability score between consecutive snapshots $k$ and $k+1$ that quantifies how cluster membership of already seen isolates changes. Let $\\mathcal{P}_k$ be the partition of $S_k$ into connected components at snapshot $k$, and let $\\mathcal{P}_{k+1}$ be the partition of $S_{k+1}$. Consider unordered isolate pairs from the earlier snapshot: $U_k = \\{ \\{i,j\\} : i,j \\in S_k, i  j \\}$. Define the indicator $I_k(i,j) = 1$ if $i$ and $j$ are in the same block of $\\mathcal{P}_k$, and $I_{k+1}(i,j) = 1$ if $i$ and $j$ are in the same block of $\\mathcal{P}_{k+1}$ (restricting to $S_k \\subseteq S_{k+1}$). The stability score is\n$$\ns_k = \\frac{1}{|U_k|} \\sum_{\\{i,j\\} \\in U_k} \\mathbf{1}\\big( I_k(i,j) = I_{k+1}(i,j) \\big),\n$$\nwhich is a decimal in $[0,1]$.\n\nImplement a program that, for each test case, computes:\n- For each snapshot $k \\in \\{1,\\dots,K\\}$, the sorted list of connected component sizes for the induced graph on $S_k$.\n- For each consecutive pair $(k,k+1)$, the stability score $s_k$ rounded to three decimal places.\n\nAll times are integer days; no physical unit conversion is required. Angles are not involved. Express all stability scores as decimals rounded to three decimal places.\n\nUse the following test suite, where each test case specifies $n$, the matrix $D$, the timestamp list $t$, the snapshot list $T$, and the threshold $\\tau$:\n\n- Test case $1$ (bridge-driven merger over time):\n    - $n = 6$,\n    $$\n    D^{(1)} = \\begin{pmatrix}\n    0  1  4  2  6  6 \\\\\n    1  0  4  2  6  6 \\\\\n    4  4  0  1  6  6 \\\\\n    2  2  1  0  6  6 \\\\\n    6  6  6  6  0  1 \\\\\n    6  6  6  6  1  0\n    \\end{pmatrix},\n    $$\n    $t^{(1)} = [1,2,3,4,5,6]$, $T^{(1)} = [3,5,6]$, $\\tau^{(1)} = 2$.\n- Test case $2$ (zero-threshold boundary with all isolates distinct):\n    - $n = 5$,\n    $$\n    D^{(2)} = \\begin{pmatrix}\n    0  3  3  3  3 \\\\\n    3  0  3  3  3 \\\\\n    3  3  0  3  3 \\\\\n    3  3  3  0  3 \\\\\n    3  3  3  3  0\n    \\end{pmatrix},\n    $$\n    $t^{(2)} = [1,2,3,4,5]$, $T^{(2)} = [2,4,5]$, $\\tau^{(2)} = 0$.\n- Test case $3$ (high-threshold fully connected growth):\n    - $n = 4$,\n    $$\n    D^{(3)} = \\begin{pmatrix}\n    0  2  1  2 \\\\\n    2  0  2  1 \\\\\n    1  2  0  2 \\\\\n    2  1  2  0\n    \\end{pmatrix},\n    $$\n    $t^{(3)} = [1,2,3,4]$, $T^{(3)} = [2,3,4]$, $\\tau^{(3)} = 3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list corresponds to one test case and must be a two-item list:\n- The first item is a list of lists of integers, where the $k$-th inner list contains the sorted connected component sizes at snapshot $k$.\n- The second item is a list of floats with three-decimal rounding, containing the stability scores $s_k$ for each consecutive snapshot pair.\n\nFor example, the output format is $[ \\text{result}_1, \\text{result}_2, \\text{result}_3 ]$, where each $\\text{result}_m$ has the structure $[ \\text{SIZES}_m, \\text{STABILITIES}_m ]$.", "solution": "The problem validation process is strict and has defined steps:\n1.  **Extract Givens**: List all provided data verbatim.\n2.  **Validate Using Extracted Givens**: Check for scientific grounding, well-posedness, objectivity, and other potential flaws.\n3.  **Verdict and Action**: If invalid, explain why and output a specific formatted string. If valid, proceed to solve.\n\nThe solution must be presented in a step-by-step, principle-based manner.\nThe final answer must be a self-contained Python script adhering to specific environment constraints (`python 3.12`, `numpy 1.23.5`, `scipy 1.11.4`). The output format is explicitly defined.\n\nLet's start with the validation process.\n\n### Step 1: Extract Givens\n\n- **Topic**: Whole-genome sequencing for pathogen surveillance.\n- **Input Data**: A pairwise single nucleotide polymorphism (SNP) distance matrix, $D$.\n- **Definitions**:\n    - SNP distance $D_{ij} \\in \\mathbb{N}_0$, $D_{ij} = D_{ji}$, $D_{ii} = 0$.\n    - Transmission plausibility threshold $\\tau \\in \\mathbb{N}_0$.\n    - Undirected simple graph $G = (V, E)$ where $V = \\{0, 1, \\dots, n-1\\}$ and $E = \\{ \\{i,j\\} : D_{ij} \\le \\tau, i \\neq j \\}$.\n    - Connected components of this graph are surveillance clusters.\n    - Isolates have integer sampling times $t_i \\in \\mathbb{Z}$.\n    - Nondecreasing list of snapshot times $T_1, T_2, \\dots, T_K$.\n    - Cumulative active set at snapshot $k$: $S_k = \\{ i \\in V : t_i \\le T_k \\}$.\n    - At each snapshot $k$, we compute connected components of the graph induced by $S_k$.\n    - The output for components is a multiset of component sizes, sorted nonincreasingly.\n    - Stability score $s_k$ between snapshots $k$ and $k+1$:\n        - $\\mathcal{P}_k$: partition of $S_k$ into connected components.\n        - $\\mathcal{P}_{k+1}$: partition of $S_{k+1}$ into connected components.\n        - $U_k = \\{ \\{i,j\\} : i,j \\in S_k, i  j \\}$.\n        - $I_k(i,j) = 1$ if $i, j$ are in the same block of $\\mathcal{P}_k$, $0$ otherwise.\n        - $I_{k+1}(i,j) = 1$ if $i, j$ are in the same block of $\\mathcal{P}_{k+1}$, $0$ otherwise.\n        - Formula: $s_k = \\frac{1}{|U_k|} \\sum_{\\{i,j\\} \\in U_k} \\mathbf{1}\\big( I_k(i,j) = I_{k+1}(i,j) \\big)$.\n        - This is a decimal in $[0,1]$.\n- **Task**:\n    1. For each snapshot $k \\in \\{1,\\dots,K\\}$, compute the sorted list of connected component sizes for the graph induced on $S_k$.\n    2. For each consecutive pair $(k,k+1)$, compute the stability score $s_k$ rounded to three decimal places.\n- **Test Cases**:\n    - **Test Case 1**:\n        - $n=6$\n        - $D^{(1)} = \\begin{pmatrix} 0  1  4  2  6  6 \\\\ 1  0  4  2  6  6 \\\\ 4  4  0  1  6  6 \\\\ 2  2  1  0  6  6 \\\\ 6  6  6  6  0  1 \\\\ 6  6  6  6  1  0 \\end{pmatrix}$\n        - $t^{(1)} = [1,2,3,4,5,6]$\n        - $T^{(1)} = [3,5,6]$\n        - $\\tau^{(1)} = 2$\n    - **Test Case 2**:\n        - $n=5$\n        - $D^{(2)} = \\begin{pmatrix} 0  3  3  3  3 \\\\ 3  0  3  3  3 \\\\ 3  3  0  3  3 \\\\ 3  3  3  0  3 \\\\ 3  3  3  3  0 \\end{pmatrix}$\n        - $t^{(2)} = [1,2,3,4,5]$\n        - $T^{(2)} = [2,4,5]$\n        - $\\tau^{(2)} = 0$\n    - **Test Case 3**:\n        - $n=4$\n        - $D^{(3)} = \\begin{pmatrix} 0  2  1  2 \\\\ 2  0  2  1 \\\\ 1  2  0  2 \\\\ 2  1  2  0 \\end{pmatrix}$\n        - $t^{(3)} = [1,2,3,4]$\n        - $T^{(3)} = [2,3,4]$\n        - $\\tau^{(3)} = 3$\n- **Output Format**:\n    - Single line: `[result1, result2, result3]`\n    - `result_m = [SIZES_m, STABILITIES_m]`\n    - `SIZES_m` is a list of lists of integers.\n    - `STABILITIES_m` is a list of floats (rounded to 3 decimal places).\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientific or Factual Unsoundness**: The problem is grounded in microbial genomics and epidemiology, specifically pathogen surveillance using WGS. The use of SNP distances to infer relatedness and define clusters is a standard, widely accepted practice. The concept of a SNP threshold to define a transmission cluster is a common operational definition in public health. The mathematical formulation (graph theory) is appropriate for this application. The physics/chemistry constraints mentioned in the template (unit conversions, angles) are explicitly stated to be irrelevant, which is correct. The problem is scientifically and factually sound.\n\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable. It translates the biological/epidemiological concept of a surveillance cluster into a precise graph-theoretic problem (connected components). The definitions are rigorous. The topic is exactly *whole-genome sequencing for pathogen surveillance* in *medical microbiology*.\n\n3.  **Incomplete or Contradictory Setup**: The problem is well-specified. For each test case, it provides all necessary inputs: the number of isolates ($n$), the complete SNP distance matrix ($D$), the sampling time for each isolate ($t$), the snapshot times ($T$), and the SNP threshold ($\\tau$). The definitions for constructing the graph, defining snapshots, and calculating the outputs (component sizes and stability score) are complete and unambiguous. There are no contradictions. The distance matrix properties ($D_{ij} = D_{ji}$, $D_{ii}=0$) are correctly stated and hold for the test cases.\n\n4.  **Unrealistic or Infeasible**: The SNP distances are non-negative integers, which is realistic. The sampling times are integers, which is a valid discretization. The problem scale is small, making it computationally feasible. The given data is dimensionally consistent (e.g., matrix sizes match $n$). The problem is realistic and feasible.\n\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. For a given graph, the connected components are uniquely defined. The stability score calculation is also uniquely defined. The problem is structured clearly, leading to a unique solution. The terms are defined precisely (e.g., `S_k`, `I_k(i,j)`).\n\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires implementing graph traversal algorithms (like BFS or DFS, or using a Union-Find data structure) and carefully managing subsets of nodes at different time points. The stability score calculation is a non-trivial aggregation that requires careful implementation. It is a genuine computational problem reflecting a real-world analysis task.\n\n7.  **Outside Scientific Verifiability**: The problem is fully verifiable. The calculations are deterministic, and for any given input, the output can be independently computed and verified.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. I will proceed with the solution.\n\n### Algorithmic Solution\n\nThe problem requires analyzing the structure of pathogen clusters at discrete time snapshots and quantifying the stability of these clusters between snapshots. The solution is naturally divided into two main parts: first, determining the cluster structure at each snapshot, and second, calculating the stability score between consecutive snapshots.\n\n**1. Modeling and Cluster Identification**\n\nThe core of the problem lies in translating the biological scenario into a formal mathematical model. This is achieved using graph theory:\n- Each pathogen isolate is represented as a vertex in a graph, $V = \\{0, 1, \\dots, n-1\\}$.\n- A genetic link between two isolates $i$ and $j$ is established if their SNP distance $D_{ij}$ does not exceed a specified threshold $\\tau$. This defines an edge $\\{i,j\\}$.\n- The resulting undirected graph $G = (V, E)$, where $E = \\{ \\{i,j\\} : D_{ij} \\le \\tau, i \\neq j \\}$, encapsulates all potential transmission links.\n- At any given time, only a subset of isolates have been sampled. A snapshot at time $T_k$ considers the cumulative set of isolates $S_k = \\{ i \\in V : t_i \\le T_k \\}$.\n- The surveillance clusters at snapshot $k$ are defined as the connected components of the subgraph of $G$ induced by the vertex set $S_k$.\n\nTo compute these connected components for each snapshot, a Disjoint Set Union (DSU) or Union-Find data structure is an efficient choice. The algorithm for a single snapshot $k$ is as follows:\n1. Identify the set of active isolates $S_k$. If $S_k$ is empty, there are no components.\n2. Initialize a DSU structure with $|S_k|$ elements, one for each isolate in $S_k$. Each isolate starts in its own set.\n3. Iterate through all unique pairs of isolates $\\{i, j\\}$ within $S_k$.\n4. If the SNP distance $D_{ij} \\le \\tau$, merge the sets containing $i$ and $j$ using the `union` operation of the DSU.\n5. After all pairs have been checked, the final sets in the DSU structure represent the connected components (clusters).\n6. The size of each component is the number of isolates it contains. These sizes are collected and sorted in nonincreasing order.\n7. For the stability calculation, the partition $\\mathcal{P}_k$ of $S_k$ is stored. A practical representation is a dictionary mapping each isolate's ID to the representative (root) of its component.\n\n**2. Stability Score Calculation**\n\nThe stability score $s_k$ measures the consistency of cluster assignments for isolates that were already present in the previous snapshot. It is calculated for each consecutive pair of snapshots, $(k, k+1)$.\n- Let $\\mathcal{P}_k$ and $\\mathcal{P}_{k+1}$ be the partitions of $S_k$ and $S_{k+1}$ into clusters, respectively.\n- The set of interest is $S_k$, the isolates present at the earlier snapshot. The set of pairs to evaluate is $U_k = \\{ \\{i,j\\} : i,j \\in S_k, i  j \\}$.\n- For each pair $\\{i,j\\} \\in U_k$, we compare their co-clustering status in both snapshots.\n  - $I_k(i,j)=1$ if $i$ and $j$ are in the same cluster in $\\mathcal{P}_k$; $0$ otherwise.\n  - $I_{k+1}(i,j)=1$ if $i$ and $j$ are in the same cluster in $\\mathcal{P}_{k+1}$; $0$ otherwise. Note that since $S_k \\subseteq S_{k+1}$, isolates $i,j$ are present in the graph for snapshot $k+1$.\n- The score is the fraction of pairs for which the status is unchanged:\n  $$s_k = \\frac{1}{|U_k|} \\sum_{\\{i,j\\} \\in U_k} \\mathbf{1}\\big( I_k(i,j) = I_{k+1}(i,j) \\big)$$\n- The summation counts the number of pairs $\\{i,j\\}$ that are either clustered together in both snapshots or separated in both snapshots.\n- A special case arises if $|S_k|  2$. Here, the set of pairs $U_k$ is empty, and the denominator $|U_k|$ is zero. By convention, in this scenario of vacuous truth (no pairs exist to disagree), the stability is maximal, $s_k = 1.0$.\n\nThe overall implementation will process each test case by first computing the cluster partitions for all its snapshots and then calculating the stability scores between consecutive partitions. The final results are aggregated and formatted as specified.", "answer": "```python\nimport numpy as np\n\nclass UnionFind:\n    \"\"\"\n    A class for the Disjoint Set Union (DSU) or Union-Find data structure.\n    Implements union by size and path compression for efficiency.\n    \"\"\"\n    def __init__(self, size):\n        self.parent = list(range(size))\n        self.set_size = [1] * size\n\n    def find(self, i):\n        \"\"\"Finds the representative of the set containing element i with path compression.\"\"\"\n        if self.parent[i] == i:\n            return i\n        self.parent[i] = self.find(self.parent[i])\n        return self.parent[i]\n\n    def union(self, i, j):\n        \"\"\"Merges the sets containing elements i and j using union by size.\"\"\"\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            # Union by size: attach smaller tree under root of larger tree\n            if self.set_size[root_i]  self.set_size[root_j]:\n                root_i, root_j = root_j, root_i\n            self.parent[root_j] = root_i\n            self.set_size[root_i] += self.set_size[root_j]\n            return True\n        return False\n\ndef process_case(n, D, t, T, tau):\n    \"\"\"\n    Processes a single test case to compute cluster sizes and stability scores.\n    \"\"\"\n    snapshot_partitions = {}\n    snapshot_sizes = []\n\n    for k, T_k in enumerate(T):\n        snapshot_idx = k + 1\n        \n        active_isolates = sorted([i for i, time in enumerate(t) if time = T_k])\n        \n        if not active_isolates:\n            snapshot_sizes.append([])\n            snapshot_partitions[snapshot_idx] = {}\n            continue\n\n        num_active = len(active_isolates)\n        isolate_to_uf_idx = {iso: i for i, iso in enumerate(active_isolates)}\n        \n        uf = UnionFind(num_active)\n        \n        for i in range(num_active):\n            for j in range(i + 1, num_active):\n                iso1 = active_isolates[i]\n                iso2 = active_isolates[j]\n                if D[iso1, iso2] = tau:\n                    uf.union(isolate_to_uf_idx[iso1], isolate_to_uf_idx[iso2])\n\n        # Store partition for stability calculation\n        partition = {iso: uf.find(isolate_to_uf_idx[iso]) for iso in active_isolates}\n        snapshot_partitions[snapshot_idx] = partition\n\n        # Calculate component sizes\n        component_members = {}\n        for iso in active_isolates:\n            root = partition[iso]\n            component_members.setdefault(root, []).append(iso)\n        \n        sizes = sorted([len(v) for v in component_members.values()], reverse=True)\n        snapshot_sizes.append(sizes)\n\n    stability_scores = []\n    for k in range(len(T) - 1):\n        snapshot_idx_k = k + 1\n        snapshot_idx_k1 = k + 2\n        \n        partition_k = snapshot_partitions[snapshot_idx_k]\n        partition_k1 = snapshot_partitions[snapshot_idx_k1]\n        \n        isolates_k = sorted(list(partition_k.keys()))\n        num_isolates_k = len(isolates_k)\n\n        if num_isolates_k  2:\n            score = 1.0\n        else:\n            num_pairs = num_isolates_k * (num_isolates_k - 1) // 2\n            agreement_count = 0\n            for i in range(num_isolates_k):\n                for j in range(i + 1, num_isolates_k):\n                    iso1 = isolates_k[i]\n                    iso2 = isolates_k[j]\n                    \n                    same_k = (partition_k[iso1] == partition_k[iso2])\n                    same_k1 = (partition_k1[iso1] == partition_k1[iso2])\n                    \n                    if same_k == same_k1:\n                        agreement_count += 1\n            \n            score = agreement_count / num_pairs\n\n        stability_scores.append(round(score, 3))\n        \n    return [snapshot_sizes, stability_scores]\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print results.\n    \"\"\"\n    test_cases = [\n        (\n            6,\n            np.array([\n                [0, 1, 4, 2, 6, 6], [1, 0, 4, 2, 6, 6], [4, 4, 0, 1, 6, 6],\n                [2, 2, 1, 0, 6, 6], [6, 6, 6, 6, 0, 1], [6, 6, 6, 6, 1, 0]\n            ]),\n            [1, 2, 3, 4, 5, 6],\n            [3, 5, 6],\n            2\n        ),\n        (\n            5,\n            np.array([\n                [0, 3, 3, 3, 3], [3, 0, 3, 3, 3], [3, 3, 0, 3, 3],\n                [3, 3, 3, 0, 3], [3, 3, 3, 3, 0]\n            ]),\n            [1, 2, 3, 4, 5],\n            [2, 4, 5],\n            0\n        ),\n        (\n            4,\n            np.array([\n                [0, 2, 1, 2], [2, 0, 2, 1], [1, 2, 0, 2], [2, 1, 2, 0]\n            ]),\n            [1, 2, 3, 4],\n            [2, 3, 4],\n            3\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        n, D, t, T, tau = case\n        result = process_case(n, D, t, T, tau)\n        results.append(result)\n\n    # Format the final output string according to the problem specification.\n    # The default str() representation for lists includes spaces, which matches\n    # the example format \"[ result_1, result_2, result_3 ]\".\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "4688569"}]}