{"hands_on_practices": [{"introduction": "To truly understand how a random forest works, we must first deconstruct its fundamental building block: the decision tree. The power of a tree lies in its recursive process of splitting the data to create more homogeneous, or \"pure,\" subgroups. This exercise takes you directly into the core of this mechanism by having you manually calculate the improvement from a single split in a regression setting, using squared error as the measure of impurity. Performing this calculation by hand solidifies the concept of empirical risk reduction, which is the engine that drives the entire tree-building process [@problem_id:4910518].", "problem": "A hospital biostatistics team is building an ensemble of regression trees within a Random Forest (RF) model for predicting adult systolic blood pressure from covariates $(X_{1},\\dots,X_{p})$. For a specific tree at its current root node, the team considers a candidate split on the covariate $X_{2}$ (Body Mass Index). The outcome $Y$ is systolic blood pressure measured in millimeters of mercury (mmHg). The squared loss is used. The dataset at this node consists of $n=10$ patients with observed pairs $(X_{2}, Y)$:\n- Patient $1$: $(X_{2}=22.0,\\;Y=118)$\n- Patient $2$: $(X_{2}=24.5,\\;Y=130)$\n- Patient $3$: $(X_{2}=29.0,\\;Y=142)$\n- Patient $4$: $(X_{2}=26.0,\\;Y=126)$\n- Patient $5$: $(X_{2}=31.0,\\;Y=150)$\n- Patient $6$: $(X_{2}=27.5,\\;Y=134)$\n- Patient $7$: $(X_{2}=28.0,\\;Y=138)$\n- Patient $8$: $(X_{2}=23.0,\\;Y=120)$\n- Patient $9$: $(X_{2}=34.0,\\;Y=160)$\n- Patient $10$: $(X_{2}=25.0,\\;Y=132)$\n\nThe candidate split is at threshold $t=27.0$, sending observations with $X_{2}\\leq t$ to the left child $L$ and those with $X_{2}>t$ to the right child $R$. The empirical risk at any node $N$ under squared loss is defined as\n$$\nR_{N} \\;=\\; \\frac{1}{n_{N}}\\sum_{i\\in N}\\left(Y_{i}-\\bar{Y}_{N}\\right)^{2},\n$$\nwhere $n_{N}$ is the number of observations in node $N$ and $\\bar{Y}_{N}$ is the sample mean of $Y$ in node $N$. The decrease in empirical risk due to the split is\n$$\n\\Delta \\;=\\; R_{\\text{parent}} \\;-\\; \\left(\\frac{n_{L}}{n}R_{L}\\;+\\;\\frac{n_{R}}{n}R_{R}\\right).\n$$\n\nAssume the Random Forest uses a minimum impurity decrease parameter $\\lambda=10$ (in squared mmHg per sample), so the split is accepted if and only if $\\Delta>\\lambda$. Compute the net gain\n$$\nG \\;=\\; \\Delta \\;-\\; \\lambda,\n$$\nfor this candidate split on $X_{2}$ at $t=27.0$. Express your final numeric answer for $G$ in squared millimeters of mercury per sample, rounded to four significant figures.", "solution": "We start from the definition of empirical risk under squared loss. For a node $N$, the empirical risk is\n$$\nR_{N} \\;=\\; \\frac{1}{n_{N}}\\sum_{i\\in N}\\left(Y_{i}-\\bar{Y}_{N}\\right)^{2}.\n$$\nThis is the mean squared error (MSE) within the node. The value $\\bar{Y}_{N}$ that minimizes $\\frac{1}{n_{N}}\\sum_{i\\in N}\\left(Y_{i}-\\mu\\right)^{2}$ over $\\mu$ is the sample mean. This follows by differentiating with respect to $\\mu$:\n$$\n\\frac{\\partial}{\\partial \\mu}\\left[\\frac{1}{n_{N}}\\sum_{i\\in N}\\left(Y_{i}-\\mu\\right)^{2}\\right]\n\\;=\\; \\frac{1}{n_{N}}\\sum_{i\\in N}\\left(-2\\right)\\left(Y_{i}-\\mu\\right)\n\\;=\\; -\\frac{2}{n_{N}}\\left(\\sum_{i\\in N}Y_{i}-n_{N}\\mu\\right),\n$$\nsetting to zero yields $\\mu=\\bar{Y}_{N}$. Therefore, computing $R_{N}$ requires $\\bar{Y}_{N}$ and the sum of squared deviations.\n\nTo compute sums of squared deviations efficiently, we use the identity\n$$\n\\sum_{i\\in N}\\left(Y_{i}-\\bar{Y}_{N}\\right)^{2}\n\\;=\\;\n\\sum_{i\\in N}Y_{i}^{2} \\;-\\; n_{N}\\,\\bar{Y}_{N}^{2},\n$$\nwhich follows from expanding the square and using $\\sum_{i\\in N} (Y_{i}-\\bar{Y}_{N}) = 0$.\n\nWe first partition the data according to the split threshold $t=27.0$ on $X_{2}$:\n- Left child $L$ ($X_{2}\\leq 27.0$): Patients $1,2,4,8,10$ with $Y$ values $\\{118,130,126,120,132\\}$ and $n_{L}=5$.\n- Right child $R$ ($X_{2}>27.0$): Patients $3,5,6,7,9$ with $Y$ values $\\{142,150,134,138,160\\}$ and $n_{R}=5$.\nThe parent node contains all $n=10$ patients.\n\nCompute the parent mean $\\bar{Y}_{\\text{parent}}$:\n$$\n\\sum_{i=1}^{10} Y_{i} \\;=\\; 118+130+142+126+150+134+138+120+160+132 \\;=\\; 1350,\n$$\nso\n$$\n\\bar{Y}_{\\text{parent}} \\;=\\; \\frac{1350}{10} \\;=\\; 135.\n$$\nCompute $\\sum Y_{i}^{2}$ for the parent node:\n$$\n118^{2}=13924,\\;\\;130^{2}=16900,\\;\\;142^{2}=20164,\\;\\;126^{2}=15876,\\;\\;150^{2}=22500,\n$$\n$$\n134^{2}=17956,\\;\\;138^{2}=19044,\\;\\;120^{2}=14400,\\;\\;160^{2}=25600,\\;\\;132^{2}=17424.\n$$\nSumming these,\n$$\n\\sum_{i=1}^{10} Y_{i}^{2} \\;=\\; 183788.\n$$\nTherefore, the parent sum of squared deviations (sum of squared errors, $\\text{SSE}_{\\text{parent}}$) is\n$$\n\\text{SSE}_{\\text{parent}} \\;=\\; \\sum_{i=1}^{10} Y_{i}^{2} - n\\,\\bar{Y}_{\\text{parent}}^{2}\n\\;=\\; 183788 \\;-\\; 10\\times 135^{2}\n\\;=\\; 183788 \\;-\\; 182250\n\\;=\\; 1538.\n$$\nThus the parent risk is\n$$\nR_{\\text{parent}} \\;=\\; \\frac{\\text{SSE}_{\\text{parent}}}{n} \\;=\\; \\frac{1538}{10} \\;=\\; 153.8.\n$$\n\nCompute the left node mean and $\\text{SSE}_{L}$:\n$$\n\\sum_{i\\in L} Y_{i} \\;=\\; 118+130+126+120+132 \\;=\\; 626,\n\\quad\n\\bar{Y}_{L} \\;=\\; \\frac{626}{5} \\;=\\; 125.2,\n$$\n$$\n\\sum_{i\\in L} Y_{i}^{2} \\;=\\; 13924+16900+15876+14400+17424 \\;=\\; 78524,\n$$\nso\n$$\n\\text{SSE}_{L} \\;=\\; 78524 \\;-\\; 5\\times (125.2)^{2}.\n$$\nCompute $(125.2)^{2}$:\n$$\n(125.2)^{2} \\;=\\; 15675.04,\n$$\nhence\n$$\n\\text{SSE}_{L} \\;=\\; 78524 \\;-\\; 5\\times 15675.04 \\;=\\; 78524 \\;-\\; 78375.2 \\;=\\; 148.8,\n$$\nand\n$$\nR_{L} \\;=\\; \\frac{\\text{SSE}_{L}}{n_{L}} \\;=\\; \\frac{148.8}{5} \\;=\\; 29.76.\n$$\n\nCompute the right node mean and $\\text{SSE}_{R}$:\n$$\n\\sum_{i\\in R} Y_{i} \\;=\\; 142+150+134+138+160 \\;=\\; 724,\n\\quad\n\\bar{Y}_{R} \\;=\\; \\frac{724}{5} \\;=\\; 144.8,\n$$\n$$\n\\sum_{i\\in R} Y_{i}^{2} \\;=\\; 20164+22500+17956+19044+25600 \\;=\\; 105264,\n$$\nso\n$$\n\\text{SSE}_{R} \\;=\\; 105264 \\;-\\; 5\\times (144.8)^{2}.\n$$\nCompute $(144.8)^{2}$:\n$$\n(144.8)^{2} \\;=\\; 20967.04,\n$$\nhence\n$$\n\\text{SSE}_{R} \\;=\\; 105264 \\;-\\; 5\\times 20967.04 \\;=\\; 105264 \\;-\\; 104835.2 \\;=\\; 428.8,\n$$\nand\n$$\nR_{R} \\;=\\; \\frac{\\text{SSE}_{R}}{n_{R}} \\;=\\; \\frac{428.8}{5} \\;=\\; 85.76.\n$$\n\nCompute the decrease in empirical risk $\\Delta$:\n$$\n\\Delta \\;=\\; R_{\\text{parent}} \\;-\\; \\left(\\frac{n_{L}}{n}R_{L}\\;+\\;\\frac{n_{R}}{n}R_{R}\\right)\n\\;=\\; 153.8 \\;-\\; \\left(\\frac{5}{10}\\times 29.76 \\;+\\; \\frac{5}{10}\\times 85.76\\right).\n$$\nEvaluate the weighted child risk:\n$$\n\\frac{5}{10}\\times 29.76 \\;=\\; 14.88,\\qquad \\frac{5}{10}\\times 85.76 \\;=\\; 42.88,\n$$\nso\n$$\n\\frac{n_{L}}{n}R_{L}\\;+\\;\\frac{n_{R}}{n}R_{R} \\;=\\; 14.88+42.88 \\;=\\; 57.76,\n$$\nand\n$$\n\\Delta \\;=\\; 153.8 \\;-\\; 57.76 \\;=\\; 96.04.\n$$\n\nWith minimum impurity decrease parameter $\\lambda=10$, the net gain is\n$$\nG \\;=\\; \\Delta \\;-\\; \\lambda \\;=\\; 96.04 \\;-\\; 10 \\;=\\; 86.04.\n$$\nSince $G>0$, the split would be accepted under the Random Forest criterion. Per the problem instructions, we report $G$ rounded to four significant figures. The value $86.04$ already has four significant figures.", "answer": "$$\\boxed{86.04}$$", "id": "4910518"}, {"introduction": "Once an ensemble model like a random forest is trained, evaluating its performance is the next critical step. A key advantage of random forests is their ability to generate Out-Of-Bag (OOB) predictions, which provide an unbiased estimate of performance without needing a separate validation set. This hands-on coding practice leverages OOB predictions to tackle a crucial issue in modern biostatistics: assessing model fairness across different demographic groups. You will translate statistical definitions like the true positive rate ($TPR$), false positive rate ($FPR$), and Brier score into a functional program to quantify performance disparities, bridging the gap between theory and responsible, real-world application [@problem_id:4910502].", "problem": "You are given a binary classification setting arising from an ensemble of decision trees such as a random forest. Each observation has an Out-Of-Bag (OOB) predicted probability of the positive class, a true binary label, and a categorical demographic subgroup label. Your task is to write a complete, runnable program that, from first principles and using the definitions of confusion-matrix-based rates and mean squared loss, computes group-wise performance metrics from the OOB predicted probabilities and then computes absolute disparities between two specified demographic subgroups.\n\nDefinitions to be used in your reasoning and implementation:\n- The Out-Of-Bag (OOB) predicted probability for an observation is the average predicted probability over those trees that were not trained on that observation in the bootstrap sampling procedure, and it is treated as an unbiased estimate of prediction for that observation.\n- Given a classification threshold $\\tau$, convert an OOB predicted probability $p_i$ into a predicted class $\\hat{y}_i$ by the rule $\\hat{y}_i = 1$ if $p_i \\ge \\tau$ and $\\hat{y}_i = 0$ otherwise.\n- For any subgroup, define the confusion-matrix counts from $\\{\\hat{y}_i, y_i\\}$ restricted to that subgroup: true positives, false positives, true negatives, and false negatives. From these, derive the true positive rate, false positive rate, and positive predictive value using their standard definitions in terms of counts.\n- Define the Brier score for any subgroup as the mean squared error between predicted probabilities and true labels within that subgroup.\n\nYour program must implement the following, applied to each test case provided below:\n1. Compute, for subgroup $\\mathrm{A}$ and subgroup $\\mathrm{B}$ separately, the following four metrics:\n   - True positive rate,\n   - False positive rate,\n   - Positive predictive value,\n   - Brier score.\n2. Compute the absolute disparity for each metric between subgroup $\\mathrm{A}$ and subgroup $\\mathrm{B}$ (that is, the absolute value of the difference between the subgroup metrics).\n3. Edge-case conventions to enforce:\n   - If any denominator in a rate definition is zero for a subgroup, define that rate to be $0$ for that subgroup.\n   - All absolute disparities must be nonnegative real numbers.\n\nInput specification is fixed in the program (no user input). Use the following test suite, where arrays are given in the order of OOB predicted probabilities, true labels, subgroup labels, and the threshold $\\tau$:\n- Test case $1$:\n  - OOB predicted probabilities: $[0.9, 0.4, 0.7, 0.2, 0.8, 0.3]$\n  - True labels: $[1, 0, 1, 0, 1, 0]$\n  - Subgroups: $[\\mathrm{A}, \\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$\n  - Threshold: $\\tau = 0.5$\n- Test case $2$:\n  - OOB predicted probabilities: $[0.6, 0.2, 0.7, 0.9, 0.1]$\n  - True labels: $[0, 0, 1, 1, 0]$\n  - Subgroups: $[\\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$\n  - Threshold: $\\tau = 0.5$\n- Test case $3$:\n  - OOB predicted probabilities: $[0.4, 0.6, 0.3, 0.9, 0.2]$\n  - True labels: $[0, 1, 0, 1, 0]$\n  - Subgroups: $[\\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$\n  - Threshold: $\\tau = 0.0$\n- Test case $4$:\n  - OOB predicted probabilities: $[1.0, 0.8, 0.2, 0.7, 0.99, 0.95]$\n  - True labels: $[1, 0, 0, 0, 1, 1]$\n  - Subgroups: $[\\mathrm{A}, \\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$\n  - Threshold: $\\tau = 1.0$\n\nFor each test case, your program must output a list of four floating-point numbers in the following order:\n$[$ absolute disparity in true positive rate between $\\mathrm{A}$ and $\\mathrm{B}$, absolute disparity in false positive rate, absolute disparity in positive predictive value, absolute disparity in Brier score $]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the results for all test cases concatenated in order into a single flat list. Each floating-point result must be rounded to exactly $6$ digits after the decimal point. For example, a valid output with two test cases would look like $[0.125000,0.500000,0.000000,0.031250,0.333333,0.000000,0.200000,0.100000]$.", "solution": "The problem is valid. It presents a well-defined task in biostatistics and machine learning model evaluation, grounded in standard, formalizable definitions. All necessary data and parameters are provided, the constraints are consistent, and there are no scientific or logical flaws.\n\nThis problem requires the computation of several standard performance metrics for a binary classifier, stratified by demographic subgroup, and the subsequent calculation of the absolute disparity in these metrics between two groups, denoted $\\mathrm{A}$ and $\\mathrm{B}$. The metrics are the True Positive Rate ($TPR$), False Positive Rate ($FPR$), Positive Predictive Value ($PPV$), and the Brier Score ($BS$).\n\nFirst, we formalize the definitions. Let the dataset for a subgroup $G$ consist of $N_G$ observations, where each observation $i$ has an Out-Of-Bag (OOB) predicted probability $p_i$, a true label $y_i \\in \\{0, 1\\}$, and a subgroup label. For a given classification threshold $\\tau$, the predicted label $\\hat{y}_i$ is determined by the rule:\n$$\n\\hat{y}_i = \\begin{cases} 1 & \\text{if } p_i \\ge \\tau \\\\ 0 & \\text{if } p_i < \\tau \\end{cases}\n$$\n\nThe performance metrics for subgroup $G$ are derived from the counts of true positives ($TP_G$), false positives ($FP_G$), true negatives ($TN_G$), and false negatives ($FN_G$):\n- $TP_G = \\sum_{i \\in G} \\mathbf{1}(y_i = 1 \\text{ and } \\hat{y}_i = 1)$\n- $FP_G = \\sum_{i \\in G} \\mathbf{1}(y_i = 0 \\text{ and } \\hat{y}_i = 1)$\n- $TN_G = \\sum_{i \\in G} \\mathbf{1}(y_i = 0 \\text{ and } \\hat{y}_i = 0)$\n- $FN_G = \\sum_{i \\in G} \\mathbf{1}(y_i = 1 \\text{ and } \\hat{y}_i = 0)$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function.\n\nThe rates are then defined as:\n- **True Positive Rate (Sensitivity/Recall)**: $TPR_G = \\frac{TP_G}{TP_G + FN_G}$. The denominator is the total number of actual positive cases in subgroup $G$.\n- **False Positive Rate**: $FPR_G = \\frac{FP_G}{FP_G + TN_G}$. The denominator is the total number of actual negative cases in subgroup $G$.\n- **Positive Predictive Value (Precision)**: $PPV_G = \\frac{TP_G}{TP_G + FP_G}$. The denominator is the total number of predicted positive cases in subgroup $G$.\n\nAs per the problem specification, if any of these denominators are zero, the corresponding rate is defined to be $0$.\n\nThe **Brier Score** for subgroup $G$ is the mean squared error between the predicted probabilities and the true labels:\n$$\nBS_G = \\frac{1}{N_G} \\sum_{i \\in G} (p_i - y_i)^2\n$$\nwhere $N_G$ is the count of individuals in subgroup $G$.\n\nFinally, the absolute disparity for each metric $M \\in \\{TPR, FPR, PPV, BS\\}$ is computed as $\\Delta_{M} = |M_A - M_B|$.\n\nWe will now apply these definitions to each test case.\n\n**Test Case 1**:\n- Data: $p = [0.9, 0.4, 0.7, 0.2, 0.8, 0.3]$, $y = [1, 0, 1, 0, 1, 0]$, subgroups = $[\\mathrm{A}, \\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$, $\\tau = 0.5$.\n- Subgroup A data: $p_A = [0.9, 0.4, 0.7]$, $y_A = [1, 0, 1]$. With $\\tau=0.5$, $\\hat{y}_A=[1, 0, 1]$.\n- A counts: $TP_A=2, FP_A=0, TN_A=1, FN_A=0$.\n- A metrics: $TPR_A = \\frac{2}{2+0} = 1$. $FPR_A = \\frac{0}{0+1} = 0$. $PPV_A = \\frac{2}{2+0} = 1$. $BS_A = \\frac{1}{3}((0.9-1)^2 + (0.4-0)^2 + (0.7-1)^2) = \\frac{0.01+0.16+0.09}{3} = \\frac{0.26}{3}$.\n- Subgroup B data: $p_B = [0.2, 0.8, 0.3]$, $y_B = [0, 1, 0]$. With $\\tau=0.5$, $\\hat{y}_B=[0, 1, 0]$.\n- B counts: $TP_B=1, FP_B=0, TN_B=2, FN_B=0$.\n- B metrics: $TPR_B = \\frac{1}{1+0} = 1$. $FPR_B = \\frac{0}{0+2} = 0$. $PPV_B = \\frac{1}{1+0} = 1$. $BS_B = \\frac{1}{3}((0.2-0)^2 + (0.8-1)^2 + (0.3-0)^2) = \\frac{0.04+0.04+0.09}{3} = \\frac{0.17}{3}$.\n- Disparities: $\\Delta_{TPR} = |1-1|=0$. $\\Delta_{FPR} = |0-0|=0$. $\\Delta_{PPV} = |1-1|=0$. $\\Delta_{BS} = |\\frac{0.26}{3} - \\frac{0.17}{3}| = \\frac{0.09}{3} = 0.03$.\n\n**Test Case 2**:\n- Data: $p = [0.6, 0.2, 0.7, 0.9, 0.1]$, $y = [0, 0, 1, 1, 0]$, subgroups = $[\\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$, $\\tau = 0.5$.\n- Subgroup A data: $p_A = [0.6, 0.2]$, $y_A = [0, 0]$. With $\\tau=0.5$, $\\hat{y}_A=[1, 0]$.\n- A counts: $TP_A=0, FP_A=1, TN_A=1, FN_A=0$.\n- A metrics: Actual positives $TP_A+FN_A=0 \\implies TPR_A=0$. $FPR_A=\\frac{1}{1+1}=0.5$. $PPV_A=\\frac{0}{0+1}=0$. $BS_A = \\frac{1}{2}((0.6-0)^2 + (0.2-0)^2) = \\frac{0.36+0.04}{2} = 0.2$.\n- Subgroup B data: $p_B = [0.7, 0.9, 0.1]$, $y_B = [1, 1, 0]$. With $\\tau=0.5$, $\\hat{y}_B=[1, 1, 0]$.\n- B counts: $TP_B=2, FP_B=0, TN_B=1, FN_B=0$.\n- B metrics: $TPR_B = \\frac{2}{2+0}=1$. $FPR_B = \\frac{0}{0+1}=0$. $PPV_B=\\frac{2}{2+0}=1$. $BS_B = \\frac{1}{3}((0.7-1)^2 + (0.9-1)^2 + (0.1-0)^2) = \\frac{0.09+0.01+0.01}{3} = \\frac{0.11}{3}$.\n- Disparities: $\\Delta_{TPR} = |0-1|=1$. $\\Delta_{FPR} = |0.5-0|=0.5$. $\\Delta_{PPV} = |0-1|=1$. $\\Delta_{BS} = |0.2 - \\frac{0.11}{3}| = |\\frac{0.6-0.11}{3}| = \\frac{0.49}{3} \\approx 0.163333$.\n\n**Test Case 3**:\n- Data: $p = [0.4, 0.6, 0.3, 0.9, 0.2]$, $y = [0, 1, 0, 1, 0]$, subgroups = $[\\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$, $\\tau = 0.0$.\n- Since all probabilities are $\\ge 0$, all $\\hat{y_i}=1$.\n- Subgroup A data: $p_A=[0.4, 0.6]$, $y_A=[0, 1]$. $\\hat{y}_A=[1, 1]$.\n- A counts: $TP_A=1, FP_A=1, TN_A=0, FN_A=0$.\n- A metrics: $TPR_A=\\frac{1}{1+0}=1$. $FPR_A=\\frac{1}{1+0}=1$. $PPV_A=\\frac{1}{1+1}=0.5$. $BS_A = \\frac{1}{2}((0.4-0)^2+(0.6-1)^2) = \\frac{0.16+0.16}{2}=0.16$.\n- Subgroup B data: $p_B=[0.3, 0.9, 0.2]$, $y_B=[0, 1, 0]$. $\\hat{y}_B=[1, 1, 1]$.\n- B counts: $TP_B=1, FP_B=2, TN_B=0, FN_B=0$.\n- B metrics: $TPR_B=\\frac{1}{1+0}=1$. $FPR_B=\\frac{2}{2+0}=1$. $PPV_B=\\frac{1}{1+2}=\\frac{1}{3}$. $BS_B = \\frac{1}{3}((0.3-0)^2+(0.9-1)^2+(0.2-0)^2) = \\frac{0.09+0.01+0.04}{3} = \\frac{0.14}{3}$.\n- Disparities: $\\Delta_{TPR} = |1-1|=0$. $\\Delta_{FPR} = |1-1|=0$. $\\Delta_{PPV} = |0.5 - \\frac{1}{3}| = \\frac{1}{6} \\approx 0.166667$. $\\Delta_{BS} = |0.16 - \\frac{0.14}{3}| = |\\frac{0.48-0.14}{3}| = \\frac{0.34}{3} \\approx 0.113333$.\n\n**Test Case 4**:\n- Data: $p = [1.0, 0.8, 0.2, 0.7, 0.99, 0.95]$, $y = [1, 0, 0, 0, 1, 1]$, subgroups = $[\\mathrm{A}, \\mathrm{A}, \\mathrm{A}, \\mathrm{B}, \\mathrm{B}, \\mathrm{B}]$, $\\tau = 1.0$.\n- Predictions are $1$ only if $p \\ge 1.0$.\n- Subgroup A data: $p_A=[1.0, 0.8, 0.2]$, $y_A=[1, 0, 0]$. $\\hat{y}_A=[1, 0, 0]$.\n- A counts: $TP_A=1, FP_A=0, TN_A=2, FN_A=0$.\n- A metrics: $TPR_A=\\frac{1}{1+0}=1$. $FPR_A=\\frac{0}{0+2}=0$. $PPV_A=\\frac{1}{1+0}=1$. $BS_A = \\frac{1}{3}((1.0-1)^2+(0.8-0)^2+(0.2-0)^2) = \\frac{0+0.64+0.04}{3} = \\frac{0.68}{3}$.\n- Subgroup B data: $p_B=[0.7, 0.99, 0.95]$, $y_B=[0, 1, 1]$. $\\hat{y}_B=[0, 0, 0]$.\n- B counts: $TP_B=0, FP_B=0, TN_B=1, FN_B=2$.\n- B metrics: $TPR_B=\\frac{0}{0+2}=0$. $FPR_B=\\frac{0}{0+1}=0$. Predicted positives $TP_B+FP_B=0 \\implies PPV_B=0$. $BS_B = \\frac{1}{3}((0.7-0)^2+(0.99-1)^2+(0.95-1)^2) = \\frac{0.49+0.0001+0.0025}{3} = \\frac{0.4926}{3}$.\n- Disparities: $\\Delta_{TPR} = |1-0|=1$. $\\Delta_{FPR} = |0-0|=0$. $\\Delta_{PPV} = |1-0|=1$. $\\Delta_{BS} = |\\frac{0.68}{3} - \\frac{0.4926}{3}| = \\frac{0.1874}{3} \\approx 0.062467$.\n\nThe following program implements these calculations.", "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(probs, labels, threshold):\n    \"\"\"\n    Computes performance metrics for a single subgroup.\n\n    Args:\n        probs (np.ndarray): Predicted probabilities for the positive class.\n        labels (np.ndarray): True binary labels (0 or 1).\n        threshold (float): Classification threshold.\n\n    Returns:\n        tuple: A tuple containing (tpr, fpr, ppv, brier_score).\n    \"\"\"\n    if len(labels) == 0:\n        return 0.0, 0.0, 0.0, 0.0\n\n    # Brier Score computation\n    brier_score = np.mean((probs - labels) ** 2)\n\n    # Convert probabilities to predicted labels\n    predicted_labels = (probs >= threshold).astype(int)\n\n    # Confusion matrix counts\n    tp = np.sum((predicted_labels == 1) & (labels == 1))\n    fp = np.sum((predicted_labels == 1) & (labels == 0))\n    tn = np.sum((predicted_labels == 0) & (labels == 0))\n    fn = np.sum((predicted_labels == 0) & (labels == 1))\n\n    # Denominators for the rates\n    actual_positives = tp + fn\n    actual_negatives = fp + tn\n    predicted_positives = tp + fp\n\n    # True Positive Rate (TPR)\n    tpr = tp / actual_positives if actual_positives > 0 else 0.0\n\n    # False Positive Rate (FPR)\n    fpr = fp / actual_negatives if actual_negatives > 0 else 0.0\n\n    # Positive Predictive Value (PPV)\n    ppv = tp / predicted_positives if predicted_positives > 0 else 0.0\n\n    return tpr, fpr, ppv, brier_score\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and compute disparities.\n    \"\"\"\n    test_cases = [\n        (np.array([0.9, 0.4, 0.7, 0.2, 0.8, 0.3]),\n         np.array([1, 0, 1, 0, 1, 0]),\n         np.array(['A', 'A', 'A', 'B', 'B', 'B']),\n         0.5),\n        (np.array([0.6, 0.2, 0.7, 0.9, 0.1]),\n         np.array([0, 0, 1, 1, 0]),\n         np.array(['A', 'A', 'B', 'B', 'B']),\n         0.5),\n        (np.array([0.4, 0.6, 0.3, 0.9, 0.2]),\n         np.array([0, 1, 0, 1, 0]),\n         np.array(['A', 'A', 'B', 'B', 'B']),\n         0.0),\n        (np.array([1.0, 0.8, 0.2, 0.7, 0.99, 0.95]),\n         np.array([1, 0, 0, 0, 1, 1]),\n         np.array(['A', 'A', 'A', 'B', 'B', 'B']),\n         1.0)\n    ]\n\n    all_results = []\n    for probs, labels, subgroups, threshold in test_cases:\n        # Filter data for subgroup A\n        mask_a = subgroups == 'A'\n        probs_a = probs[mask_a]\n        labels_a = labels[mask_a]\n\n        # Filter data for subgroup B\n        mask_b = subgroups == 'B'\n        probs_b = probs[mask_b]\n        labels_b = labels[mask_b]\n\n        # Calculate metrics for each subgroup\n        tpr_a, fpr_a, ppv_a, bs_a = calculate_metrics(probs_a, labels_a, threshold)\n        tpr_b, fpr_b, ppv_b, bs_b = calculate_metrics(probs_b, labels_b, threshold)\n\n        # Calculate absolute disparities\n        disp_tpr = abs(tpr_a - tpr_b)\n        disp_fpr = abs(fpr_a - fpr_b)\n        disp_ppv = abs(ppv_a - ppv_b)\n        disp_bs = abs(bs_a - bs_b)\n        \n        case_results = [disp_tpr, disp_fpr, disp_ppv, disp_bs]\n        all_results.extend(case_results)\n\n    # Format the final output string\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4910502"}, {"introduction": "While random forests are exceptionally powerful, being an expert practitioner means understanding not only a model's strengths but also its limitations. A model is only as good as the data it's trained on, and this has profound implications for how we interpret its predictions, especially for data points that look different from what it has seen before. This exercise challenges you to reason about one of the most critical limitations of all tree-based models: their inability to extrapolate trends beyond the range of the training data. By analyzing how this impacts interpretation tools like Partial Dependence Plots, you will develop the critical thinking skills necessary to use these models safely and avoid common pitfalls in their application [@problem_id:4910532].", "problem": "A biostatistics study models a continuous clinical outcome $Y$ (for example, systolic blood pressure) from covariates $X=(X_{1},\\dots,X_{p})$ using a regression random forest. Each tree is constructed by Classification And Regression Trees (CART), where a tree partitions the covariate space by axis-aligned thresholds and the prediction in a terminal node is the sample mean of in-bag training outcomes within that node. The random forest predictor is defined as an average of $T$ trees, so that $\\hat{f}_{\\mathrm{RF}}(x)=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{f}_{t}(x)$, where each $\\hat{f}_{t}$ is a piecewise constant function determined by the treeâ€™s leaves. Consider a primary covariate $X_{1}$ (for example, age) whose empirical training support is the closed interval $[\\min_{i}X_{1}^{(i)},\\max_{i}X_{1}^{(i)}]$, with $\\min_{i}X_{1}^{(i)}=45$ and $\\max_{i}X_{1}^{(i)}=80$ in years. In practice, biostatisticians often visualize the relationship between $Y$ and a covariate via the partial dependence function. For a feature index $j\\in\\{1,\\dots,p\\}$, the partial dependence function is defined as $PD_{j}(x)=\\mathbb{E}_{X_{-j}}[\\hat{f}_{\\mathrm{RF}}(x,X_{-j})]$, and is commonly approximated by $\\frac{1}{n}\\sum_{i=1}^{n}\\hat{f}_{\\mathrm{RF}}(x,X_{-j}^{(i)})$, where $X_{-j}$ denotes all features except $X_{j}$. Using only these fundamental definitions and facts, reason about the behavior of $\\hat{f}_{\\mathrm{RF}}(x)$ and $PD_{1}(x)$ for $x$ outside the empirical support $[45,80]$ and propose scientifically sound safeguards to monitor partial dependence outside training support. Select all statements that are correct.\n\nA. In a regression random forest constructed from CART, for a single continuous feature $X_{1}$ with empirical support $[\\min_{i}X_{1}^{(i)},\\max_{i}X_{1}^{(i)}]$, the prediction $\\hat{f}_{\\mathrm{RF}}(x)$ for any $x>\\max_{i}X_{1}^{(i)}$ equals the average of the extreme-right terminal node means across trees, and remains constant as $x$ increases further.\n\nB. Because random forests average many trees, they can reliably extrapolate linearly beyond the training range whenever the underlying relationship is monotone, so $\\hat{f}_{\\mathrm{RF}}(x)$ will continue the learned trend past $x=80$.\n\nC. The partial dependence $PD_{1}(x)=\\mathbb{E}_{X_{-1}}[\\hat{f}_{\\mathrm{RF}}(x,X_{-1})]$ evaluated at $x$ with zero empirical density of $X_{1}$ (for example, $x<45$ or $x>80$) uses combinations $(x,X_{-1}^{(i)})$ that lie outside observed support; a practical safeguard is to monitor a kernel density estimate $\\hat{f}_{X_{1}}(x)$ and either flag $PD_{1}(x)$ when $\\hat{f}_{X_{1}}(x)$ is near $0$ or restrict visualization to an empirical quantile interval such as $[q_{0.01}(X_{1}),q_{0.99}(X_{1})]$.\n\nD. Increasing the number of trees $T$ removes extrapolation limits, because by the law of large numbers $\\hat{f}_{\\mathrm{RF}}(x)$ converges to a smooth function that extrapolates outside the training support.\n\nE. In multivariate settings, a safeguard is to approximate a conditional partial dependence by weighting training observations according to $\\omega_{i}(x)\\propto\\mathbf{1}\\{|X_{1}^{(i)}-x|\\leq\\epsilon\\}$ for a small $\\epsilon>0$, so that $PD_{1}^{\\mathrm{cond}}(x)\\approx\\frac{\\sum_{i=1}^{n}\\omega_{i}(x)\\hat{f}_{\\mathrm{RF}}(x,X_{-1}^{(i)})}{\\sum_{i=1}^{n}\\omega_{i}(x)}$, and to flag $x$ where $\\sum_{i=1}^{n}\\omega_{i}(x)$ is small, indicating paucity of support.\n\nF. Bootstrap resampling used in random forest training expands the covariate support of $X_{1}$ beyond $[45,80]$, making $PD_{1}(x)$ at $x>80$ trustworthy without additional monitoring.\n\nChoose all correct options.", "solution": "The user has provided a problem statement concerning the extrapolation behavior of regression random forests and the interpretation of partial dependence functions outside the training data support.\n\n### Step 1: Extract Givens\n- **Model:** A regression random forest for a continuous outcome $Y$ based on covariates $X=(X_{1},\\dots,X_{p})$.\n- **Base Learner:** Classification And Regression Trees (CART).\n- **CART Prediction Mechanism:** A tree partitions the covariate space via axis-aligned splits. The prediction in any terminal node (leaf) is the sample mean of the in-bag training outcomes for the data points falling into that node.\n- **Random Forest Predictor:** $\\hat{f}_{\\mathrm{RF}}(x)=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{f}_{t}(x)$, where $\\hat{f}_{t}$ is the prediction from the $t$-th tree, a piecewise constant function.\n- **Covariate of Interest:** $X_{1}$, representing a variable like age.\n- **Empirical Training Support for $X_{1}$:** The closed interval $[\\min_{i}X_{1}^{(i)},\\max_{i}X_{1}^{(i)}]$, which is given as $[45, 80]$ years.\n- **Partial Dependence Function (PDF):** For a feature $j$, $PD_{j}(x)=\\mathbb{E}_{X_{-j}}[\\hat{f}_{\\mathrm{RF}}(x,X_{-j})]$.\n- **Empirical PDF Approximation:** $\\frac{1}{n}\\sum_{i=1}^{n}\\hat{f}_{\\mathrm{RF}}(x,X_{-j}^{(i)})$, where $X_{-j}$ denotes all features except $X_{j}$.\n- **Objective:** To reason about the behavior of $\\hat{f}_{\\mathrm{RF}}(x)$ and $PD_{1}(x)$ for values of the first covariate, let's call its argument $x_1$, such that $x_1 \\notin [45, 80]$, and to evaluate proposed safeguards.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective. It is based on standard, formal definitions of random forests (as ensembles of CART trees) and partial dependence functions, which are foundational concepts in modern statistical learning and biostatistics. The premises are factually sound and internally consistent. The problem is not underspecified, as it provides all necessary definitions to reason about the model's behavior. The scenario is a realistic application in biostatistics. There are no logical fallacies or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed with a detailed derivation and evaluation of each option.\n\n### Derivation and Option Evaluation\n\nThe core of the problem lies in understanding the extrapolation behavior of a CART-based regression tree. A CART tree generates predictions by partitioning the feature space. The splits are of the form $X_j \\leq c$. A crucial, though often implicit, detail is that the split points $c$ are chosen from the values observed in the training data for the feature $X_j$. Consequently, for a feature $X_1$ with training data support $[45, 80]$, all split points for $X_1$ in any tree will lie within this interval.\n\nConsider a single tree $\\hat{f}_t$ and a new data point $(x_1, x_{-1})$ where $x_1 > 80$. As this point is passed down the tree, at any decision node involving a split on $X_1$ (e.g., $X_1 \\leq c_k$ where $c_k \\leq 80$), the condition will always be false. The point will thus be routed to the \"right\" child node. This means that for any $x_1 > 80$, the point will follow the same path through the tree as a point with $x_1 = 80$ (or more precisely, any value just greater than the largest split point for $X_1$ in that tree). It will land in the \"right-most\" terminal node with respect to the $X_1$ axis. The prediction for all such points is the constant value associated with this terminal node (the mean of the in-bag training samples that fell into it). Symmetrically, for any $x_1 < 45$, the point will land in the \"left-most\" terminal node.\n\nThe random forest predictor $\\hat{f}_{\\mathrm{RF}}$ is an average of these tree predictors. Since each $\\hat{f}_t$ is constant for $x_1 > 80$, their average, $\\hat{f}_{\\mathrm{RF}}$, must also be constant for $x_1 > 80$. The random forest model is incapable of extrapolating a learned trend beyond the support of the training data; it can only output a constant value determined by the predictions at the boundary of the support.\n\nThe partial dependence function $PD_1(x_1)$ is an average of predictions $\\hat{f}_{\\mathrm{RF}}(x_1, X_{-1}^{(i)})$ over the empirical distribution of $X_{-1}$. Since $\\hat{f}_{\\mathrm{RF}}$ is constant for $x_1 > 80$ for any fixed $X_{-1}^{(i)}$, the average over $i$ will also be constant for $x_1 > 80$.\n\nWith these principles established, I will evaluate each option.\n\n**A. In a regression random forest constructed from CART, for a single continuous feature $X_{1}$ with empirical support $[\\min_{i}X_{1}^{(i)},\\max_{i}X_{1}^{(i)}]$, the prediction $\\hat{f}_{\\mathrm{RF}}(x)$ for any $x>\\max_{i}X_{1}^{(i)}$ equals the average of the extreme-right terminal node means across trees, and remains constant as $x$ increases further.**\n- As derived above, for a given tree $t$ and any $x_1 > \\max_{i}X_{1}^{(i)}$, the prediction $\\hat{f}_t(x_1, x_{-1})$ is constant. This constant is the mean of the training outcomes in the terminal node that is \"extreme-right\" with respect to the splits on $X_1$.\n- The random forest prediction $\\hat{f}_{\\mathrm{RF}}(x)$ is the average of these individual tree predictions. Therefore, for $x_1 > \\max_{i}X_{1}^{(i)}$, $\\hat{f}_{\\mathrm{RF}}(x)$ is the average of the predictions from these extreme-right nodes. Since each tree's prediction is constant in this region, the average is also constant.\n- The statement is a precise and correct description of the model's behavior.\n- **Verdict: Correct.**\n\n**B. Because random forests average many trees, they can reliably extrapolate linearly beyond the training range whenever the underlying relationship is monotone, so $\\hat{f}_{\\mathrm{RF}}(x)$ will continue the learned trend past $x=80$.**\n- This statement is fundamentally incorrect. The base learners, CART trees, are piecewise constant functions. The average of piecewise constant functions cannot produce a linear extrapolation. As established in the analysis of option A, the prediction function $\\hat{f}_{\\mathrm{RF}}$ becomes flat outside the training data's support. Averaging reduces variance but does not change this inherent structural limitation of the base learners.\n- **Verdict: Incorrect.**\n\n**C. The partial dependence $PD_{1}(x)=\\mathbb{E}_{X_{-1}}[\\hat{f}_{\\mathrm{RF}}(x,X_{-1})]$ evaluated at $x$ with zero empirical density of $X_{1}$ (for example, $x<45$ or $x>80$) uses combinations $(x,X_{-1}^{(i)})$ that lie outside observed support; a practical safeguard is to monitor a kernel density estimate $\\hat{f}_{X_{1}}(x)$ and either flag $PD_{1}(x)$ when $\\hat{f}_{X_{1}}(x)$ is near $0$ or restrict visualization to an empirical quantile interval such as $[q_{0.01}(X_{1}),q_{0.99}(X_{1})]$.**\n- The calculation of $PD_1(x_1)$ involves creating synthetic data points $(x_1, X_{-1}^{(i)})$ for $i=1, \\dots, n$. If $x_1$ is in a region where the marginal density of $X_1$ is zero (e.g., $x_1 > 80$), then these synthetic points lie outside the joint support of the training data. The model's predictions for these unobserved combinations of features may not be meaningful. The premise is correct.\n- Providing visual cues about data density is a crucial aspect of responsible model interpretation. Plotting a rug plot or a kernel density estimate $\\hat{f}_{X_{1}}(x_1)$ on the same axes as the $PD_1(x_1)$ plot is a standard and scientifically sound method to warn the user about regions where the PDP is unreliable due to lack of data.\n- Restricting the plot to a high-density interval, such as between the $1$st and $99$th percentiles, is another common and valid technique to avoid displaying the misleading, flat-lined portions of the PDP. Both proposed safeguards are state-of-the-art best practices.\n- **Verdict: Correct.**\n\n**D. Increasing the number of trees $T$ removes extrapolation limits, because by the law of large numbers $\\hat{f}_{\\mathrm{RF}}(x)$ converges to a smooth function that extrapolates outside the training support.**\n- As $T \\rightarrow \\infty$, the random forest predictor $\\hat{f}_{\\mathrm{RF}}(x)$ converges to its expectation $\\mathbb{E}[\\hat{f}_{t}(x)]$. However, this limiting function inherits the structural properties of its constituent trees. Since each $\\hat{f}_t(x)$ is constant for $x_1 > \\max_i X_1^{(i)}$, their expectation $\\mathbb{E}[\\hat{f}_t(x)]$ will also be constant in this region. The Law of Large Numbers ensures convergence, but it does not alter the nature of the function being converged to. Increasing $T$ does not confer a new ability to extrapolate trends; it only stabilizes the prediction. The claim that the limit is a \"smooth function that extrapolates\" is false.\n- **Verdict: Incorrect.**\n\n**E. In multivariate settings, a safeguard is to approximate a conditional partial dependence by weighting training observations according to $\\omega_{i}(x)\\propto\\mathbf{1}\\{|X_{1}^{(i)}-x|\\leq\\epsilon\\}$ for a small $\\epsilon>0$, so that $PD_{1}^{\\mathrm{cond}}(x)\\approx\\frac{\\sum_{i=1}^{n}\\omega_{i}(x)\\hat{f}_{\\mathrm{RF}}(x,X_{-1}^{(i)})}{\\sum_{i=1}^{n}\\omega_{i}(x)}$, and to flag $x$ where $\\sum_{i=1}^{n}\\omega_{i}(x)$ is small, indicating paucity of support.**\n- This statement describes a method for estimating a localized or conditional partial dependence. It aims to address the issue that the standard PDP averages over all values of $X_{-1}^{(i)}$, even those that are unrealistic given a specific value of $X_1=x_1$. By weighting only observations where $X_1^{(i)}$ is close to $x_1$, the average is computed over a more relevant, local distribution of $X_{-1}$. This is a valid and more nuanced approach to understanding feature effects.\n- The term $\\sum_{i=1}^{n}\\omega_{i}(x)$ is the number of training points in a small neighborhood of $x_1$. A small value for this sum directly indicates a scarcity of data around $x_1$. Flagging or not reporting $PD_{1}^{\\mathrm{cond}}(x_1)$ when this count is low is a direct and sound method for ensuring the estimate is based on sufficient evidence.\n- The proposed procedure is a scientifically sound safeguard.\n- **Verdict: Correct.**\n\n**F. Bootstrap resampling used in random forest training expands the covariate support of $X_{1}$ beyond $[45,80]$, making $PD_{1}(x)$ at $x>80$ trustworthy without additional monitoring.**\n- Bootstrap resampling involves drawing samples from the original training dataset *with replacement*. The set of unique values present in any bootstrap sample is a subset of the unique values in the original dataset. For the covariate $X_1$, the minimum value in any bootstrap bag will be greater than or equal to $45$, and the maximum value will be less than or equal to $80$.\n- The premise that bootstrapping expands the support is factually incorrect. It does the opposite, or at best, maintains it. It cannot create data values that were not in the original training set. Therefore, the conclusion that this makes the PDP trustworthy outside the original support is false.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{ACE}$$", "id": "4910532"}]}