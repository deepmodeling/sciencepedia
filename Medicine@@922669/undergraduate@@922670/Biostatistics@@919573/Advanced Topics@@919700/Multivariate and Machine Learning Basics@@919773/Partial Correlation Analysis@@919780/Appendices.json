{"hands_on_practices": [{"introduction": "The most direct way to understand partial correlation is to build it from the ground up. This practice walks you through the fundamental definition: quantifying the relationship between two variables after removing the linear influence of a third. By calculating the correlation of residuals from simple linear regressions, you will gain a concrete, computational grasp of what it means to 'control for' a variable. [@problem_id:4937002]", "problem": "A biostatistics study investigates whether changes in systolic blood pressure, denoted by $X$ (in arbitrary standardized units), are associated with changes in an inflammatory biomarker, denoted by $Y$, after accounting for participant age, denoted by $Z$ (age centered at the cohort mean). You are given a sample of $n=6$ participants with observed triplets $(x_i,y_i,z_i)$:\n- $i=1$: $(x_1,y_1,z_1)=(-17,\\,12,\\, -5)$\n- $i=2$: $(x_2,y_2,z_2)=(-15,\\,4,\\, -3)$\n- $i=3$: $(x_3,y_3,z_3)=(-3,\\,5,\\, -1)$\n- $i=4$: $(x_4,y_4,z_4)=(3,\\, -5,\\, 1)$\n- $i=5$: $(x_5,y_5,z_5)=(14,\\, -5,\\, 3)$\n- $i=6$: $(x_6,y_6,z_6)=(18,\\, -11,\\, 5)$\n\nStarting from fundamental definitions, estimate the partial correlation of $X$ and $Y$ controlling for $Z$, denoted by $\\hat{\\rho}_{XY\\cdot Z}$, by performing the following steps:\n\n1. Fit the simple linear regression of $X$ on $Z$ using ordinary least squares (OLS), and extract the residuals $\\hat{e}_{Xi}=x_i-(\\hat{\\alpha}_X+\\hat{\\beta}_X z_i)$.\n2. Fit the simple linear regression of $Y$ on $Z$ using OLS, and extract the residuals $\\hat{e}_{Yi}=y_i-(\\hat{\\alpha}_Y+\\hat{\\beta}_Y z_i)$.\n3. Compute the Pearson product-moment correlation coefficient (PCC) between the residuals, $\\hat{\\rho}_{XY\\cdot Z}$, using only the foundational definitions of sample mean, sample covariance, and sample variance.\n\nReport $\\hat{\\rho}_{XY\\cdot Z}$ as a decimal and round your final answer to four significant figures. No units are required for the final answer.", "solution": "The problem requires the estimation of the partial correlation coefficient $\\hat{\\rho}_{XY\\cdot Z}$ from a sample of $n=6$ observations. The procedure outlined is to first regress $X$ on $Z$ and $Y$ on $Z$ separately using ordinary least squares (OLS), and then to compute the Pearson product-moment correlation coefficient of the resulting sets of residuals.\n\nThe provided data points $(x_i, y_i, z_i)$ are:\n$(-17, 12, -5)$, $(-15, 4, -3)$, $(-3, 5, -1)$, $(3, -5, 1)$, $(14, -5, 3)$, $(18, -11, 5)$.\n\nFirst, we calculate the sample means of the variables $X$, $Y$, and $Z$.\n$\\bar{x} = \\frac{1}{6} \\sum_{i=1}^{6} x_i = \\frac{1}{6}(-17 - 15 - 3 + 3 + 14 + 18) = \\frac{0}{6} = 0$.\n$\\bar{y} = \\frac{1}{6} \\sum_{i=1}^{6} y_i = \\frac{1}{6}(12 + 4 + 5 - 5 - 5 - 11) = \\frac{0}{6} = 0$.\n$\\bar{z} = \\frac{1}{6} \\sum_{i=1}^{6} z_i = \\frac{1}{6}(-5 - 3 - 1 + 1 + 3 + 5) = \\frac{0}{6} = 0$.\nThe fact that all variables are mean-centered simplifies the OLS calculations.\n\n### Step 1: Fit the linear regression of $X$ on $Z$ and find the residuals.\nThe simple linear regression model is $X = \\alpha_X + \\beta_X Z + \\epsilon_X$.\nThe OLS estimators for the coefficients are given by:\n$\\hat{\\beta}_X = \\frac{\\sum_{i=1}^n (z_i - \\bar{z})(x_i - \\bar{x})}{\\sum_{i=1}^n (z_i - \\bar{z})^2}$\n$\\hat{\\alpha}_X = \\bar{x} - \\hat{\\beta}_X \\bar{z}$\nSince $\\bar{x} = 0$ and $\\bar{z} = 0$, these formulas simplify to:\n$\\hat{\\beta}_X = \\frac{\\sum_{i=1}^n x_i z_i}{\\sum_{i=1}^n z_i^2}$\n$\\hat{\\alpha}_X = 0$\n\nWe compute the necessary sums:\n$\\sum_{i=1}^6 z_i^2 = (-5)^2 + (-3)^2 + (-1)^2 + 1^2 + 3^2 + 5^2 = 25 + 9 + 1 + 1 + 9 + 25 = 70$.\n$\\sum_{i=1}^6 x_i z_i = (-17)(-5) + (-15)(-3) + (-3)(-1) + (3)(1) + (14)(3) + (18)(5) = 85 + 45 + 3 + 3 + 42 + 90 = 268$.\n\nThe estimated slope is:\n$\\hat{\\beta}_X = \\frac{268}{70} = \\frac{134}{35}$.\nThe fitted regression line is $\\hat{x}_i = \\hat{\\alpha}_X + \\hat{\\beta}_X z_i = \\frac{134}{35} z_i$.\n\nThe residuals $\\hat{e}_{Xi}$ are calculated as $x_i - \\hat{x}_i$:\n$\\hat{e}_{X1} = -17 - \\frac{134}{35}(-5) = -17 + \\frac{134}{7} = \\frac{-119 + 134}{7} = \\frac{15}{7}$\n$\\hat{e}_{X2} = -15 - \\frac{134}{35}(-3) = -15 + \\frac{402}{35} = \\frac{-525 + 402}{35} = -\\frac{123}{35}$\n$\\hat{e}_{X3} = -3 - \\frac{134}{35}(-1) = -3 + \\frac{134}{35} = \\frac{-105 + 134}{35} = \\frac{29}{35}$\n$\\hat{e}_{X4} = 3 - \\frac{134}{35}(1) = \\frac{105 - 134}{35} = -\\frac{29}{35}$\n$\\hat{e}_{X5} = 14 - \\frac{134}{35}(3) = 14 - \\frac{402}{35} = \\frac{490 - 402}{35} = \\frac{88}{35}$\n$\\hat{e}_{X6} = 18 - \\frac{134}{35}(5) = 18 - \\frac{134}{7} = \\frac{126 - 134}{7} = -\\frac{8}{7}$\n\n### Step 2: Fit the linear regression of $Y$ on $Z$ and find the residuals.\nThe model is $Y = \\alpha_Y + \\beta_Y Z + \\epsilon_Y$.\nWith $\\bar{y} = 0$ and $\\bar{z} = 0$, the estimators simplify to:\n$\\hat{\\beta}_Y = \\frac{\\sum_{i=1}^n y_i z_i}{\\sum_{i=1}^n z_i^2}$\n$\\hat{\\alpha}_Y = 0$\n\nWe compute the sum of products for $y_i$ and $z_i$:\n$\\sum_{i=1}^6 y_i z_i = (12)(-5) + (4)(-3) + (5)(-1) + (-5)(1) + (-5)(3) + (-11)(5) = -60 - 12 - 5 - 5 - 15 - 55 = -152$.\n\nThe estimated slope is:\n$\\hat{\\beta}_Y = \\frac{-152}{70} = -\\frac{76}{35}$.\nThe fitted regression line is $\\hat{y}_i = \\hat{\\alpha}_Y + \\hat{\\beta}_Y z_i = -\\frac{76}{35} z_i$.\n\nThe residuals $\\hat{e}_{Yi}$ are calculated as $y_i - \\hat{y}_i$:\n$\\hat{e}_{Y1} = 12 - (-\\frac{76}{35})(-5) = 12 - \\frac{76}{7} = \\frac{84 - 76}{7} = \\frac{8}{7}$\n$\\hat{e}_{Y2} = 4 - (-\\frac{76}{35})(-3) = 4 - \\frac{228}{35} = \\frac{140 - 228}{35} = -\\frac{88}{35}$\n$\\hat{e}_{Y3} = 5 - (-\\frac{76}{35})(-1) = 5 - \\frac{76}{35} = \\frac{175 - 76}{35} = \\frac{99}{35}$\n$\\hat{e}_{Y4} = -5 - (-\\frac{76}{35})(1) = -5 + \\frac{76}{35} = \\frac{-175 + 76}{35} = -\\frac{99}{35}$\n$\\hat{e}_{Y5} = -5 - (-\\frac{76}{35})(3) = -5 + \\frac{228}{35} = \\frac{-175 + 228}{35} = \\frac{53}{35}$\n$\\hat{e}_{Y6} = -11 - (-\\frac{76}{35})(5) = -11 + \\frac{76}{7} = \\frac{-77 + 76}{7} = -\\frac{1}{7}$\n\n### Step 3: Compute the Pearson correlation between the residuals.\nThe Pearson correlation coefficient between two variables $A$ and $B$ is defined using sample covariance and variance. It can be computed as:\n$\\hat{\\rho}_{AB} = \\frac{\\sum_{i=1}^n (a_i - \\bar{a})(b_i - \\bar{b})}{\\sqrt{\\sum_{i=1}^n (a_i - \\bar{a})^2 \\sum_{i=1}^n (b_i - \\bar{b})^2}}$\nFor OLS residuals with an intercept in the model, the sample mean is zero. Let's verify: $\\sum \\hat{e}_{Xi} = \\frac{15}{7}-\\frac{123}{35}+\\frac{29}{35}-\\frac{29}{35}+\\frac{88}{35}-\\frac{8}{7} = \\frac{75-123+29-29+88-40}{35} = \\frac{192-192}{35} = 0$. Similarly, $\\sum \\hat{e}_{Yi} = 0$.\nSo, $\\bar{\\hat{e}}_X = 0$ and $\\bar{\\hat{e}}_Y = 0$. The formula for the correlation of the residuals simplifies to:\n$\\hat{\\rho}_{XY\\cdot Z} = \\hat{\\rho}_{\\hat{e}_X\\hat{e}_Y} = \\frac{\\sum_{i=1}^n \\hat{e}_{Xi} \\hat{e}_{Yi}}{\\sqrt{(\\sum_{i=1}^n \\hat{e}_{Xi}^2) (\\sum_{i=1}^n \\hat{e}_{Yi}^2)}}$\n\nWe compute the required sums:\nSum of cross-products of residuals:\n$\\sum \\hat{e}_{Xi} \\hat{e}_{Yi} = (\\frac{15}{7})(\\frac{8}{7}) + (-\\frac{123}{35})(-\\frac{88}{35}) + (\\frac{29}{35})(\\frac{99}{35}) + (-\\frac{29}{35})(-\\frac{99}{35}) + (\\frac{88}{35})(\\frac{53}{35}) + (-\\frac{8}{7})(-\\frac{1}{7})$\n$= \\frac{120}{49} + \\frac{10824}{1225} + \\frac{2871}{1225} + \\frac{2871}{1225} + \\frac{4664}{1225} + \\frac{8}{49}$\nSince $1225 = 35^2 = (5 \\times 7)^2 = 25 \\times 49$, we can use a common denominator of $1225$:\n$= \\frac{120 \\times 25}{1225} + \\frac{10824}{1225} + \\frac{2871}{1225} + \\frac{2871}{1225} + \\frac{4664}{1225} + \\frac{8 \\times 25}{1225}$\n$= \\frac{3000 + 10824 + 2871 + 2871 + 4664 + 200}{1225} = \\frac{24430}{1225}$\n\nSum of squared residuals for $X$:\n$\\sum \\hat{e}_{Xi}^2 = (\\frac{15}{7})^2 + (-\\frac{123}{35})^2 + (\\frac{29}{35})^2 + (-\\frac{29}{35})^2 + (\\frac{88}{35})^2 + (-\\frac{8}{7})^2$\n$= \\frac{225}{49} + \\frac{15129}{1225} + \\frac{841}{1225} + \\frac{841}{1225} + \\frac{7744}{1225} + \\frac{64}{49}$\n$= \\frac{225 \\times 25}{1225} + \\frac{15129}{1225} + \\frac{841}{1225} + \\frac{841}{1225} + \\frac{7744}{1225} + \\frac{64 \\times 25}{1225}$\n$= \\frac{5625 + 15129 + 841 + 841 + 7744 + 1600}{1225} = \\frac{31780}{1225}$\n\nSum of squared residuals for $Y$:\n$\\sum \\hat{e}_{Yi}^2 = (\\frac{8}{7})^2 + (-\\frac{88}{35})^2 + (\\frac{99}{35})^2 + (-\\frac{99}{35})^2 + (\\frac{53}{35})^2 + (-\\frac{1}{7})^2$\n$= \\frac{64}{49} + \\frac{7744}{1225} + \\frac{9801}{1225} + \\frac{9801}{1225} + \\frac{2809}{1225} + \\frac{1}{49}$\n$= \\frac{64 \\times 25}{1225} + \\frac{7744}{1225} + \\frac{9801}{1225} + \\frac{9801}{1225} + \\frac{2809}{1225} + \\frac{1 \\times 25}{1225}$\n$= \\frac{1600 + 7744 + 9801 + 9801 + 2809 + 25}{1225} = \\frac{31780}{1225}$\n\nNow we compute the correlation:\n$\\hat{\\rho}_{XY\\cdot Z} = \\frac{\\frac{24430}{1225}}{\\sqrt{(\\frac{31780}{1225}) (\\frac{31780}{1225})}} = \\frac{\\frac{24430}{1225}}{\\frac{31780}{1225}} = \\frac{24430}{31780} = \\frac{2443}{3178}$\n\nFinally, we calculate the decimal value and round to four significant figures:\n$\\hat{\\rho}_{XY\\cdot Z} = \\frac{2443}{3178} \\approx 0.76872246...$\nRounding to four significant figures gives $0.7687$.", "answer": "$$\\boxed{0.7687}$$", "id": "4937002"}, {"introduction": "While the residual method is intuitive, a more powerful and scalable approach comes from linear algebra. This exercise introduces the connection between the covariance matrix, its inverse (the precision matrix), and partial correlations. You will see how the off-diagonal elements of a properly scaled precision matrix directly reveal the strength of association between two variables, conditional on all others. [@problem_id:4937043]", "problem": "A biostatistics cohort study records three continuous variables per participant: systolic blood pressure $X$ (in $\\mathrm{mmHg}$), low-density lipoprotein cholesterol $Y$ (in $\\mathrm{mg/dL}$), and age $Z$ (in years). Assume the joint vector $(X,Y,Z)$ is well-approximated by a Multivariate Normal (MVN) distribution with mean zero and estimated covariance matrix\n$$\n\\hat{\\Sigma}\n=\n\\begin{pmatrix}\n100 & 40 & 30 \\\\\n40 & 90 & 20 \\\\\n30 & 20 & 80\n\\end{pmatrix}.\n$$\nStarting from the definition of partial correlation as the correlation between two variables after linearly removing the influence of the remaining variables, and using linear-algebraic identities that link conditional covariances to sub-blocks of the inverse covariance, proceed as follows:\n1. Compute the estimated precision matrix $\\hat{\\Omega} = \\hat{\\Sigma}^{-1}$.\n2. Using the relationship you derive between entries of $\\hat{\\Omega}$ and pairwise partial correlations, compute the three pairwise partial correlations among $(X,Y,Z)$ when controlling for the remaining variable in each case, namely $\\rho_{XY\\cdot Z}$, $\\rho_{XZ\\cdot Y}$, and $\\rho_{YZ\\cdot X}$.\n\nExpress the final three partial correlations as decimals rounded to four significant figures, and report them in the order $(\\rho_{XY\\cdot Z}, \\rho_{XZ\\cdot Y}, \\rho_{YZ\\cdot X})$. The final answer must be a single row matrix.", "solution": "The goal is to compute pairwise partial correlations from an estimated covariance matrix by first obtaining the precision matrix and then relating its entries to conditional correlations. The fundamental base we use is the definition of partial correlation as the correlation of residuals after projecting each variable onto the conditioning set, together with well-tested linear algebra facts about blockwise inversion and conditional covariance in the Multivariate Normal (MVN) family.\n\nLet $(X,Y,Z)^{\\top}$ have covariance matrix $\\Sigma$ and precision matrix $\\Omega = \\Sigma^{-1}$. For a partition of the index set into $S$ and its complement $S^{c}$, the MVN identity gives the conditional covariance\n$$\n\\operatorname{Cov}\\big(V_{S} \\mid V_{S^{c}}\\big) \\;=\\; \\big(\\Omega_{SS}\\big)^{-1},\n$$\nwhere $\\Omega_{SS}$ is the submatrix of $\\Omega$ indexed by $S$. This is a consequence of the Schur complement and block matrix inversion for the MVN distribution. In our setting, to obtain the partial correlation between two variables, say $X$ and $Y$, controlling for $Z$, we let $S=\\{X,Y\\}$ and $S^{c}=\\{Z\\}$, so the conditional covariance of $(X,Y)$ given $Z$ is the inverse of the $2\\times 2$ block $\\Omega_{\\{X,Y\\},\\{X,Y\\}}$.\n\nIf we denote the $2\\times 2$ block by\n$$\n\\Omega_{\\{X,Y\\},\\{X,Y\\}} \\;=\\;\n\\begin{pmatrix}\n\\Omega_{11} & \\Omega_{12} \\\\\n\\Omega_{12} & \\Omega_{22}\n\\end{pmatrix},\n$$\nthen\n$$\n\\big(\\Omega_{\\{X,Y\\},\\{X,Y\\}}\\big)^{-1}\n=\n\\frac{1}{\\Omega_{11}\\Omega_{22}-\\Omega_{12}^{2}}\n\\begin{pmatrix}\n\\Omega_{22} & -\\Omega_{12} \\\\\n-\\Omega_{12} & \\Omega_{11}\n\\end{pmatrix}.\n$$\nBy the definition of correlation,\n$$\n\\rho_{XY\\cdot Z}\n=\n\\frac{\\operatorname{Cov}(X,Y\\mid Z)}{\\sqrt{\\operatorname{Var}(X\\mid Z)\\operatorname{Var}(Y\\mid Z)}}\n=\n\\frac{-\\Omega_{12}/(\\Omega_{11}\\Omega_{22}-\\Omega_{12}^{2})}{\\sqrt{\\big(\\Omega_{22}/(\\Omega_{11}\\Omega_{22}-\\Omega_{12}^{2})\\big)\\big(\\Omega_{11}/(\\Omega_{11}\\Omega_{22}-\\Omega_{12}^{2})\\big)}}\n=\n-\\frac{\\Omega_{12}}{\\sqrt{\\Omega_{11}\\Omega_{22}}}.\n$$\nThe same argument applies for any pair, yielding, for indices $i\\neq j$,\n$$\n\\rho_{ij\\cdot \\text{rest}} \\;=\\; -\\frac{\\Omega_{ij}}{\\sqrt{\\Omega_{ii}\\Omega_{jj}}}.\n$$\n\nWe now compute $\\hat{\\Omega}=\\hat{\\Sigma}^{-1}$ for the given matrix. For\n$$\n\\hat{\\Sigma}\n=\n\\begin{pmatrix}\n100 & 40 & 30 \\\\\n40 & 90 & 20 \\\\\n30 & 20 & 80\n\\end{pmatrix},\n$$\nthe determinant is\n\\begin{align*}\n\\det(\\hat{\\Sigma})\n&= 100(90\\cdot 80 - 20\\cdot 20) - 40(40\\cdot 80 - 20\\cdot 30) + 30(40\\cdot 20 - 90\\cdot 30) \\\\\n&= 100(7200 - 400) - 40(3200 - 600) + 30(800 - 2700) \\\\\n&= 100\\cdot 6800 - 40\\cdot 2600 + 30\\cdot (-1900) \\\\\n&= 680000 - 104000 - 57000 \\\\\n&= 519000.\n\\end{align*}\nThe cofactor matrix $\\mathbf{C}$ has entries\n\\begin{align*}\nC_{11} &= \\det\\begin{pmatrix}90 & 20 \\\\ 20 & 80\\end{pmatrix} = 6800, \\\\\nC_{12} &= -\\det\\begin{pmatrix}40 & 20 \\\\ 30 & 80\\end{pmatrix} = -2600, \\\\\nC_{13} &= \\det\\begin{pmatrix}40 & 90 \\\\ 30 & 20\\end{pmatrix} = -1900, \\\\\nC_{21} &= -\\det\\begin{pmatrix}40 & 30 \\\\ 20 & 80\\end{pmatrix} = -2600, \\\\\nC_{22} &= \\det\\begin{pmatrix}100 & 30 \\\\ 30 & 80\\end{pmatrix} = 7100, \\\\\nC_{23} &= -\\det\\begin{pmatrix}100 & 40 \\\\ 30 & 20\\end{pmatrix} = -800, \\\\\nC_{31} &= \\det\\begin{pmatrix}40 & 30 \\\\ 90 & 20\\end{pmatrix} = -1900, \\\\\nC_{32} &= -\\det\\begin{pmatrix}100 & 30 \\\\ 40 & 20\\end{pmatrix} = -800, \\\\\nC_{33} &= \\det\\begin{pmatrix}100 & 40 \\\\ 40 & 90\\end{pmatrix} = 7400.\n\\end{align*}\nSince the cofactor matrix is symmetric, the adjugate is its transpose, and we have\n$$\n\\hat{\\Omega}\n=\n\\hat{\\Sigma}^{-1}\n=\n\\frac{1}{\\det(\\hat{\\Sigma})}\\,\\mathbf{C}\n=\n\\frac{1}{519000}\n\\begin{pmatrix}\n6800 & -2600 & -1900 \\\\\n-2600 & 7100 & -800 \\\\\n-1900 & -800 & 7400\n\\end{pmatrix}.\n$$\nTo compute partial correlations, it is convenient to note that the common scale $1/\\det(\\hat{\\Sigma})$ cancels in the standardization, so we may use cofactors directly:\n\\begin{align*}\n\\rho_{XY\\cdot Z}\n&= -\\frac{\\hat{\\Omega}_{12}}{\\sqrt{\\hat{\\Omega}_{11}\\hat{\\Omega}_{22}}}\n= -\\frac{C_{12}/\\det(\\hat{\\Sigma})}{\\sqrt{(C_{11}/\\det(\\hat{\\Sigma}))(C_{22}/\\det(\\hat{\\Sigma}))}}\n= -\\frac{C_{12}}{\\sqrt{C_{11}C_{22}}}\n= \\frac{2600}{\\sqrt{6800\\cdot 7100}}, \\\\\n\\rho_{XZ\\cdot Y}\n&= -\\frac{\\hat{\\Omega}_{13}}{\\sqrt{\\hat{\\Omega}_{11}\\hat{\\Omega}_{33}}}\n= -\\frac{C_{13}}{\\sqrt{C_{11}C_{33}}}\n= \\frac{1900}{\\sqrt{6800\\cdot 7400}}, \\\\\n\\rho_{YZ\\cdot X}\n&= -\\frac{\\hat{\\Omega}_{23}}{\\sqrt{\\hat{\\Omega}_{22}\\hat{\\Omega}_{33}}}\n= -\\frac{C_{23}}{\\sqrt{C_{22}C_{33}}}\n= \\frac{800}{\\sqrt{7100\\cdot 7400}}.\n\\end{align*}\nWe now evaluate these numerically and round to four significant figures:\n\\begin{align*}\n\\sqrt{6800\\cdot 7100} &= \\sqrt{48{,}280{,}000} \\approx 6948.302, \\quad \\rho_{XY\\cdot Z} \\approx \\frac{2600}{6948.302} \\approx 0.3742, \\\\\n\\sqrt{6800\\cdot 7400} &= \\sqrt{50{,}320{,}000} \\approx 7093.656, \\quad \\rho_{XZ\\cdot Y} \\approx \\frac{1900}{7093.656} \\approx 0.2678, \\\\\n\\sqrt{7100\\cdot 7400} &= \\sqrt{52{,}540{,}000} \\approx 7248.035, \\quad \\rho_{YZ\\cdot X} \\approx \\frac{800}{7248.035} \\approx 0.1104.\n\\end{align*}\nReporting in the requested order $(\\rho_{XY\\cdot Z}, \\rho_{XZ\\cdot Y}, \\rho_{YZ\\cdot X})$ and rounding to four significant figures gives the final row matrix.", "answer": "$$\\boxed{\\begin{pmatrix}0.3742 & 0.2678 & 0.1104\\end{pmatrix}}$$", "id": "4937043"}, {"introduction": "Partial correlation is a powerful tool, but its misuse can lead to incorrect scientific conclusions. This practice explores a critical pitfall known as 'collider bias,' where statistically adjusting for a common effect of two independent causes can create a spurious correlation between them. Through both analytical derivation and simulation, you will learn to recognize and understand this counterintuitive phenomenon, a vital skill for interpreting data from observational studies. [@problem_id:4937028]", "problem": "Consider the Directed Acyclic Graph (DAG) structure with a collider: $X \\to Z \\leftarrow Y$. Let $X$, $Y$, and $Z$ be real-valued random variables generated according to the following linear-Gaussian structural equations: $X \\sim \\mathcal{N}(0,\\sigma_X^2)$, $Y \\sim \\mathcal{N}(0,\\sigma_Y^2)$, $E \\sim \\mathcal{N}(0,\\sigma_E^2)$ with mutual independence among $X$, $Y$, and $E$, and\n$$\nZ = \\alpha X + \\beta Y + E.\n$$\nIn this model, $X$ and $Y$ are marginally independent because $\\mathrm{Cov}(X,Y) = 0$. However, adjusting for $Z$ (i.e., conditioning on $Z$) can introduce a non-zero partial correlation between $X$ and $Y$ due to the collider at $Z$.\n\nYour tasks are:\n- Using only fundamental definitions of covariance, correlation, and properties of conditioning for the multivariate normal distribution, derive the analytical expression for the partial correlation between $X$ and $Y$ given $Z$, denoted by $\\rho_{XY \\cdot Z}$. Do not assume any shortcut formulas without derivation; begin from the definitions of covariance and the conditional covariance formula for jointly Gaussian variables.\n- From your derivation, determine the sign of $\\rho_{XY \\cdot Z}$ as a function of the signs of $\\alpha$ and $\\beta$, and explain why adjusting for the collider $Z$ can induce a spurious association between $X$ and $Y$ even though they are marginally independent.\n- Implement a program that, for each parameter set in the test suite below, computes:\n  1. The derived theoretical value of $\\rho_{XY \\cdot Z}$ using your analytic expression.\n  2. A simulation-based estimate of $\\rho_{XY \\cdot Z}$ by generating $n$ samples of $(X,Y,Z)$ from the structural model, regressing $X$ and $Y$ on $Z$ (each with an intercept) to obtain residuals, and then computing the Pearson correlation of those residuals.\n  3. The absolute error between the theoretical value and the simulation estimate.\n\nThe test suite consists of the following parameter sets, each specified as a tuple $(\\alpha,\\beta,\\sigma_X,\\sigma_Y,\\sigma_E,n,\\text{seed})$:\n- Case $1$ (happy path, moderate noise): $(1.0, 1.0, 1.0, 1.0, 1.0, 200000, 12345)$.\n- Case $2$ (boundary, collider absent on one parent): $(0.0, 1.0, 1.0, 1.0, 1.0, 200000, 23456)$.\n- Case $3$ (boundary, zero noise, symmetric coefficients): $(1.0, 1.0, 1.0, 1.0, 0.0, 50000, 34567)$.\n- Case $4$ (edge, large noise): $(1.0, 1.0, 1.0, 1.0, 10.0, 200000, 45678)$.\n- Case $5$ (asymmetric coefficients and small noise): $(2.0, 0.5, 1.0, 1.0, 0.2, 200000, 56789)$.\n- Case $6$ (small sample size): $(1.0, 1.0, 1.0, 1.0, 1.0, 500, 67890)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three floating-point numbers $[\\rho_{\\text{theory}}, \\rho_{\\text{sim}}, \\text{abs\\_error}]$ in that order. For example, the output format must be of the form\n$$\n[\\,[\\rho_{\\text{theory,1}},\\rho_{\\text{sim,1}},\\text{abs\\_error}_1],\\ldots,[\\rho_{\\text{theory,6}},\\rho_{\\text{sim,6}},\\text{abs\\_error}_6]\\,].\n$$\nNo physical units are involved in this problem. All outputs must be real numbers. The random number generation must be reproducible by using the provided seeds for each case.", "solution": "The problem is valid as it is scientifically grounded in the theory of multivariate statistics and causal inference, is well-posed with all necessary information provided, and is expressed objectively. We will proceed with a solution.\n\nThe primary task is to derive the partial correlation $\\rho_{XY \\cdot Z}$ between variables $X$ and $Y$ conditional on $Z$, within the specified linear-Gaussian structural causal model. The model is defined by the DAG $X \\to Z \\leftarrow Y$ and the equations:\n$X \\sim \\mathcal{N}(0, \\sigma_X^2)$\n$Y \\sim \\mathcal{N}(0, \\sigma_Y^2)$\n$E \\sim \\mathcal{N}(0, \\sigma_E^2)$\n$Z = \\alpha X + \\beta Y + E$\nThe variables $X$, $Y$, and $E$ are mutually independent.\n\nOur derivation will proceed from first principles, as requested, by first establishing the joint distribution of $(X, Y, Z)$ and then using the formula for conditional covariance for multivariate normal distributions.\n\n**1. Joint Distribution of $(X, Y, Z)$**\n\nSince $X$, $Y$, and $Z$ are linear combinations of independent Gaussian random variables ($X, Y, E$), they are jointly normally distributed. We can characterize this distribution by its mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$.\n\nThe mean vector is given by $\\boldsymbol{\\mu} = (\\mathbb{E}[X], \\mathbb{E}[Y], \\mathbb{E}[Z])^T$.\nGiven $X \\sim \\mathcal{N}(0, \\sigma_X^2)$ and $Y \\sim \\mathcal{N}(0, \\sigma_Y^2)$, we have $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Y] = 0$.\nThe expectation of $Z$ is $\\mathbb{E}[Z] = \\mathbb{E}[\\alpha X + \\beta Y + E] = \\alpha \\mathbb{E}[X] + \\beta \\mathbb{E}[Y] + \\mathbb{E}[E] = \\alpha(0) + \\beta(0) + 0 = 0$.\nThus, the mean vector is $\\boldsymbol{\\mu} = (0, 0, 0)^T$.\n\nThe covariance matrix is $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathrm{Var}(X) & \\mathrm{Cov}(X,Y) & \\mathrm{Cov}(X,Z) \\\\ \\mathrm{Cov}(Y,X) & \\mathrm{Var}(Y) & \\mathrm{Cov}(Y,Z) \\\\ \\mathrm{Cov}(Z,X) & \\mathrm{Cov}(Z,Y) & \\mathrm{Var}(Z) \\end{pmatrix}$. We compute each element:\n- $\\mathrm{Var}(X) = \\sigma_X^2$ (given).\n- $\\mathrm{Var}(Y) = \\sigma_Y^2$ (given).\n- $\\mathrm{Cov}(X,Y) = 0$, since $X$ and $Y$ are independent.\n- $\\mathrm{Cov}(X,Z) = \\mathrm{Cov}(X, \\alpha X + \\beta Y + E) = \\alpha \\mathrm{Cov}(X,X) + \\beta \\mathrm{Cov}(X,Y) + \\mathrm{Cov}(X,E)$. Due to independence, $\\mathrm{Cov}(X,Y)=0$ and $\\mathrm{Cov}(X,E)=0$. So, $\\mathrm{Cov}(X,Z) = \\alpha \\mathrm{Var}(X) = \\alpha \\sigma_X^2$.\n- $\\mathrm{Cov}(Y,Z) = \\mathrm{Cov}(Y, \\alpha X + \\beta Y + E) = \\alpha \\mathrm{Cov}(Y,X) + \\beta \\mathrm{Cov}(Y,Y) + \\mathrm{Cov}(Y,E)$. Due to independence, $\\mathrm{Cov}(Y,X)=0$ and $\\mathrm{Cov(Y,E)}=0$. So, $\\mathrm{Cov}(Y,Z) = \\beta \\mathrm{Var}(Y) = \\beta \\sigma_Y^2$.\n- $\\mathrm{Var}(Z) = \\mathrm{Var}(\\alpha X + \\beta Y + E)$. Since $X$, $Y$, and $E$ are mutually independent, the variance of the sum is the sum of the variances: $\\mathrm{Var}(Z) = \\alpha^2 \\mathrm{Var}(X) + \\beta^2 \\mathrm{Var}(Y) + \\mathrm{Var}(E) = \\alpha^2 \\sigma_X^2 + \\beta^2 \\sigma_Y^2 + \\sigma_E^2$.\n\nThe full covariance matrix for the vector $(X, Y, Z)^T$ is:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_X^2 & 0 & \\alpha \\sigma_X^2 \\\\ 0 & \\sigma_Y^2 & \\beta \\sigma_Y^2 \\\\ \\alpha \\sigma_X^2 & \\beta \\sigma_Y^2 & \\alpha^2 \\sigma_X^2 + \\beta^2 \\sigma_Y^2 + \\sigma_E^2 \\end{pmatrix}\n$$\n\n**2. Conditional Covariance and Partial Correlation**\n\nThe definition of partial correlation $\\rho_{XY \\cdot Z}$ is the Pearson correlation coefficient between $X$ and $Y$ in their conditional joint distribution given $Z=z$.\n$$\n\\rho_{XY \\cdot Z} = \\frac{\\mathrm{Cov}(X, Y | Z)}{\\sqrt{\\mathrm{Var}(X | Z) \\mathrm{Var}(Y | Z)}}\n$$\nFor a multivariate normal distribution, the conditional variances and covariances are constant and do not depend on the specific value $z$ that $Z$ takes. We can compute them using the formula for the conditional covariance matrix. Let the joint vector be partitioned as $\\mathbf{v} = (\\mathbf{v}_1^T, \\mathbf{v}_2^T)^T$ with covariance matrix $\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}$. The conditional covariance matrix of $\\mathbf{v}_1$ given $\\mathbf{v}_2$ is $\\boldsymbol{\\Sigma}_{1|2} = \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} \\boldsymbol{\\Sigma}_{21}$.\n\nIn our case, we partition the vector $(X,Y,Z)^T$ into $\\mathbf{v}_1 = (X, Y)^T$ and $\\mathbf{v}_2 = Z$. The corresponding blocks of the covariance matrix $\\boldsymbol{\\Sigma}$ are:\n- $\\boldsymbol{\\Sigma}_{11} = \\begin{pmatrix} \\mathrm{Var}(X) & \\mathrm{Cov}(X,Y) \\\\ \\mathrm{Cov}(Y,X) & \\mathrm{Var}(Y) \\end{pmatrix} = \\begin{pmatrix} \\sigma_X^2 & 0 \\\\ 0 & \\sigma_Y^2 \\end{pmatrix}$\n- $\\boldsymbol{\\Sigma}_{12} = \\begin{pmatrix} \\mathrm{Cov}(X,Z) \\\\ \\mathrm{Cov}(Y,Z) \\end{pmatrix} = \\begin{pmatrix} \\alpha \\sigma_X^2 \\\\ \\beta \\sigma_Y^2 \\end{pmatrix}$\n- $\\boldsymbol{\\Sigma}_{21} = \\boldsymbol{\\Sigma}_{12}^T = \\begin{pmatrix} \\alpha \\sigma_X^2 & \\beta \\sigma_Y^2 \\end{pmatrix}$\n- $\\boldsymbol{\\Sigma}_{22} = \\mathrm{Var}(Z) = \\alpha^2 \\sigma_X^2 + \\beta^2 \\sigma_Y^2 + \\sigma_E^2$\n\nThe inverse of the scalar matrix $\\boldsymbol{\\Sigma}_{22}$ is simply $\\boldsymbol{\\Sigma}_{22}^{-1} = \\frac{1}{\\mathrm{Var}(Z)}$.\n\nNow, we compute the conditional covariance matrix for $(X,Y)$ given $Z$, which we denote $\\boldsymbol{\\Sigma}_{XY|Z}$:\n$$\n\\boldsymbol{\\Sigma}_{XY|Z} = \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} \\boldsymbol{\\Sigma}_{21}\n= \\begin{pmatrix} \\sigma_X^2 & 0 \\\\ 0 & \\sigma_Y^2 \\end{pmatrix} - \\begin{pmatrix} \\alpha \\sigma_X^2 \\\\ \\beta \\sigma_Y^2 \\end{pmatrix} \\frac{1}{\\mathrm{Var}(Z)} \\begin{pmatrix} \\alpha \\sigma_X^2 & \\beta \\sigma_Y^2 \\end{pmatrix}\n$$\n$$\n\\boldsymbol{\\Sigma}_{XY|Z} = \\begin{pmatrix} \\sigma_X^2 & 0 \\\\ 0 & \\sigma_Y^2 \\end{pmatrix} - \\frac{1}{\\mathrm{Var}(Z)} \\begin{pmatrix} \\alpha^2 \\sigma_X^4 & \\alpha \\beta \\sigma_X^2 \\sigma_Y^2 \\\\ \\alpha \\beta \\sigma_X^2 \\sigma_Y^2 & \\beta^2 \\sigma_Y^4 \\end{pmatrix}\n= \\begin{pmatrix} \\sigma_X^2 - \\frac{\\alpha^2 \\sigma_X^4}{\\mathrm{Var}(Z)} & -\\frac{\\alpha \\beta \\sigma_X^2 \\sigma_Y^2}{\\mathrm{Var}(Z)} \\\\ -\\frac{\\alpha \\beta \\sigma_X^2 \\sigma_Y^2}{\\mathrm{Var}(Z)} & \\sigma_Y^2 - \\frac{\\beta^2 \\sigma_Y^4}{\\mathrm{Var}(Z)} \\end{pmatrix}\n$$\nThe elements of this matrix are the conditional variances and covariance:\n- $\\mathrm{Var}(X|Z) = \\sigma_X^2 \\left(1 - \\frac{\\alpha^2 \\sigma_X^2}{\\mathrm{Var}(Z)}\\right) = \\frac{\\sigma_X^2 (\\mathrm{Var}(Z) - \\alpha^2 \\sigma_X^2)}{\\mathrm{Var}(Z)}$\n- $\\mathrm{Var}(Y|Z) = \\sigma_Y^2 \\left(1 - \\frac{\\beta^2 \\sigma_Y^2}{\\mathrm{Var}(Z)}\\right) = \\frac{\\sigma_Y^2 (\\mathrm{Var}(Z) - \\beta^2 \\sigma_Y^2)}{\\mathrm{Var}(Z)}$\n- $\\mathrm{Cov}(X,Y|Z) = -\\frac{\\alpha \\beta \\sigma_X^2 \\sigma_Y^2}{\\mathrm{Var}(Z)}$\n\nSubstituting these into the partial correlation formula:\n$$\n\\rho_{XY \\cdot Z} = \\frac{-\\frac{\\alpha \\beta \\sigma_X^2 \\sigma_Y^2}{\\mathrm{Var}(Z)}}{\\sqrt{\\frac{\\sigma_X^2 (\\mathrm{Var}(Z) - \\alpha^2 \\sigma_X^2)}{\\mathrm{Var}(Z)} \\frac{\\sigma_Y^2 (\\mathrm{Var}(Z) - \\beta^2 \\sigma_Y^2)}{\\mathrm{Var}(Z)}}}\n= \\frac{-\\alpha \\beta \\sigma_X^2 \\sigma_Y^2}{\\sigma_X \\sigma_Y \\sqrt{(\\mathrm{Var}(Z) - \\alpha^2 \\sigma_X^2)(\\mathrm{Var}(Z) - \\beta^2 \\sigma_Y^2)}}\n$$\n$$\n\\rho_{XY \\cdot Z} = \\frac{-\\alpha \\beta \\sigma_X \\sigma_Y}{\\sqrt{(\\mathrm{Var}(Z) - \\alpha^2 \\sigma_X^2)(\\mathrm{Var}(Z) - \\beta^2 \\sigma_Y^2)}}\n$$\nFinally, we substitute $\\mathrm{Var}(Z) = \\alpha^2 \\sigma_X^2 + \\beta^2 \\sigma_Y^2 + \\sigma_E^2$:\n- $\\mathrm{Var}(Z) - \\alpha^2 \\sigma_X^2 = \\beta^2 \\sigma_Y^2 + \\sigma_E^2$\n- $\\mathrm{Var}(Z) - \\beta^2 \\sigma_Y^2 = \\alpha^2 \\sigma_X^2 + \\sigma_E^2$\n\nThis yields the final analytical expression for the partial correlation:\n$$\n\\rho_{XY \\cdot Z} = \\frac{-\\alpha \\beta \\sigma_X \\sigma_Y}{\\sqrt{(\\alpha^2 \\sigma_X^2 + \\sigma_E^2)(\\beta^2 \\sigma_Y^2 + \\sigma_E^2)}}\n$$\n\n**3. Analysis of the Result and Collider Bias**\n\nThe derived formula reveals the nature of the association induced by conditioning on the collider $Z$.\n- **Sign of $\\rho_{XY \\cdot Z}$**: The standard deviations $\\sigma_X, \\sigma_Y$ are non-negative. The denominator involves a square root of a sum of non-negative terms, so it is also non-negative (and strictly positive unless parameters lead to a degenerate case, e.g., $\\sigma_E=0$ and $\\alpha=0$). The sign of $\\rho_{XY \\cdot Z}$ is therefore determined entirely by the term $-\\alpha \\beta$.\n    - If $\\alpha$ and $\\beta$ have the same sign (both positive or both negative), then $\\alpha\\beta > 0$, which implies $\\rho_{XY \\cdot Z}  0$. A negative correlation is induced.\n    - If $\\alpha$ and $\\beta$ have opposite signs, then $\\alpha\\beta  0$, which implies $\\rho_{XY \\cdot Z} > 0$. A positive correlation is induced.\n    - If either $\\alpha=0$ or $\\beta=0$, then $\\rho_{XY \\cdot Z} = 0$. No correlation is induced, as the collider structure is broken (e.g., if $\\alpha=0$, $X$ is not a cause of $Z$).\n\n- **Collider Bias Explanation**: This phenomenon, where conditioning on a common effect (a \"collider\") of two independent causes induces a spurious statistical association between them, is known as collider bias or the \"explaining away\" effect. Initially, $X$ and $Y$ are independent ($\\rho_{XY}=0$). However, once we observe the value of their common effect $Z$, information about one cause provides information about the other. For instance, if $\\alpha>0$ and $\\beta>0$, a higher-than-average value of $X$ would contribute to a higher value of $Z$. If we hold $Z$ fixed at its average value, this unexpectedly large contribution from $X$ must be \"explained away\" by a lower-than-average contribution from $Y$, inducing a negative correlation. This is precisely what our sign analysis concluded. This induced correlation is non-causal and an artifact of the statistical adjustment.\n\n**4. Simulation Methodology**\n\nThe problem asks for a simulation-based estimate to compare against the theoretical value. The specified procedure involves computing the sample partial correlation by its definition: the correlation of residuals. For each test case, we will:\n1. Generate $n$ independent samples of $X$, $Y$, and $E$ from their respective normal distributions.\n2. Compute the corresponding samples of $Z$ using the structural equation $Z = \\alpha X + \\beta Y + E$.\n3. Perform an ordinary least squares (OLS) regression of $X$ on $Z$ (including an intercept term) to obtain the residuals $R_X$.\n4. Perform an OLS regression of $Y$ on $Z$ (including an intercept term) to obtain the residuals $R_Y$.\n5. Compute the sample Pearson correlation coefficient between the residual vectors $R_X$ and $R_Y$. This value is the simulation-based estimate, $\\rho_{\\text{sim}}$.\nThis procedure will be implemented in the provided Python script.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the partial correlation problem for a collider structure.\n    For each test case, it computes the theoretical partial correlation,\n    a simulation-based estimate, and the absolute error between them.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (alpha, beta, sigma_x, sigma_y, sigma_e, n, seed).\n    test_cases = [\n        (1.0, 1.0, 1.0, 1.0, 1.0, 200000, 12345),\n        (0.0, 1.0, 1.0, 1.0, 1.0, 200000, 23456),\n        (1.0, 1.0, 1.0, 1.0, 0.0, 50000, 34567),\n        (1.0, 1.0, 1.0, 1.0, 10.0, 200000, 45678),\n        (2.0, 0.5, 1.0, 1.0, 0.2, 200000, 56789),\n        (1.0, 1.0, 1.0, 1.0, 1.0, 500, 67890),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        alpha, beta, sigma_x, sigma_y, sigma_e, n, seed = case\n        \n        # 1. Theoretical calculation of rho_xy.z\n        numerator = -alpha * beta * sigma_x * sigma_y\n        \n        den_term1 = alpha**2 * sigma_x**2 + sigma_e**2\n        den_term2 = beta**2 * sigma_y**2 + sigma_e**2\n        denominator = np.sqrt(den_term1 * den_term2)\n        \n        if denominator == 0.0:\n            # This occurs if a term like (alpha**2 * sigma_x**2 + sigma_e**2) is zero.\n            # This requires alpha*sigma_x = 0 and sigma_e = 0.\n            # If so, the numerator will also be zero (if alpha=0), leading to rho=0.\n            # Special case sigma_e=0: rho = -sgn(alpha*beta).\n            # The general formula can result in 0/0, which should be 0 unless\n            # alpha, beta, sigma_x, sigma_y are all non-zero and sigma_e is zero.\n            if numerator == 0:\n                rho_theory = 0.0\n            else: # Denominator is 0 but numerator is not, e.g. sigma_e=0, alpha!=0, beta!=0\n                rho_theory = -np.sign(alpha * beta)\n        else:\n            rho_theory = numerator / denominator\n\n        # 2. Simulation-based estimate of rho_xy.z\n        rng = np.random.default_rng(seed)\n        \n        # Generate samples\n        X_samples = rng.normal(loc=0, scale=sigma_x, size=n)\n        Y_samples = rng.normal(loc=0, scale=sigma_y, size=n)\n        E_samples = rng.normal(loc=0, scale=sigma_e, size=n)\n        \n        Z_samples = alpha * X_samples + beta * Y_samples + E_samples\n        \n        # Regress X on Z and Y on Z to get residuals\n        # Create design matrix for regression with an intercept\n        Z_reg = np.vstack([Z_samples, np.ones(n)]).T\n        \n        # Residuals for X\n        try:\n            coeffs_x, _, _, _ = np.linalg.lstsq(Z_reg, X_samples, rcond=None)\n            X_pred = Z_reg @ coeffs_x\n            R_X = X_samples - X_pred\n        except np.linalg.LinAlgError:\n            # This case is unlikely with the given parameters but included for robustness\n            R_X = np.zeros(n)\n        \n        # Residuals for Y\n        try:\n            coeffs_y, _, _, _ = np.linalg.lstsq(Z_reg, Y_samples, rcond=None)\n            Y_pred = Z_reg @ coeffs_y\n            R_Y = Y_samples - Y_pred\n        except np.linalg.LinAlgError:\n            R_Y = np.zeros(n)\n            \n        # Compute Pearson correlation of the residuals\n        # Handle cases where residuals are constant (zero variance)\n        if np.var(R_X) == 0 or np.var(R_Y) == 0:\n            rho_sim = 0.0\n        else:\n            rho_sim = np.corrcoef(R_X, R_Y)[0, 1]\n\n        # 3. Absolute error\n        abs_error = np.abs(rho_theory - rho_sim)\n        \n        # Append the list of results for this case\n        results.append([rho_theory, rho_sim, abs_error])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list [a, b, c] to its string representation '[a, b, c]'\n    # and join them with commas, all inside a final pair of brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "4937028"}]}