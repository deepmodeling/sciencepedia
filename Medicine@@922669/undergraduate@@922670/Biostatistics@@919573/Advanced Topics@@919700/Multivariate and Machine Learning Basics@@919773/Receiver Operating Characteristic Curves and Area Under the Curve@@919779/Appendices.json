{"hands_on_practices": [{"introduction": "To truly master the concept of the Area Under the Curve (AUC), it is essential to connect the abstract definition to a concrete statistical model. This first practice challenges you to derive the AUC from first principles for a classifier whose scores follow Gaussian distributions [@problem_id:3167105]. By working through this derivation, you will gain a deep, analytical understanding of how the separation between class distributions and their variance directly translates into a single, powerful measure of discriminatory ability.", "problem": "A binary classifier produces a real-valued score $S$ from input features $X$ to discriminate between classes $Y \\in \\{0,1\\}$. Suppose the score distributions conditional on the class label are Gaussian with equal variance: $S \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ and $S \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$, where $\\mu_{1}, \\mu_{0} \\in \\mathbb{R}$ and $\\sigma  0$. Consider the threshold rule that predicts $Y=1$ if and only if $S \\geq t$ for a threshold $t \\in \\mathbb{R}$.\n\nUsing only the base definitions of True Positive Rate (TPR), False Positive Rate (FPR), the Receiver Operating Characteristic (ROC) curve, and the Area Under the Curve (AUC), do the following:\n- Derive the parametric form of the Receiver Operating Characteristic (ROC) curve, that is, express the True Positive Rate as a function of the False Positive Rate induced by the threshold $t$.\n- Then derive a closed-form analytic expression for the Area Under the Curve (AUC) for this model as a function of $\\mu_{1}, \\mu_{0}$, and $\\sigma$.\n\nFor clarity, the Receiver Operating Characteristic (ROC) curve is the plot of True Positive Rate (TPR) against False Positive Rate (FPR) as the threshold $t$ varies, and the Area Under the Curve (AUC) is the area under this curve. Express the final answer as a single closed-form analytic expression for the AUC. No rounding is needed.", "solution": "The user wants a detailed derivation for the problem. I will follow the plan outlined in my thought process.\n\n**1. Problem Validation**\n\nThe problem statement is validated as follows:\n\n*   **Step 1: Extract Givens**\n    *   Classifier: Binary, with classes $Y \\in \\{0, 1\\}$.\n    *   Score: Real-valued score $S$.\n    *   Conditional Distributions: $S \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ and $S \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$.\n    *   Parameters: $\\mu_{1}, \\mu_{0} \\in \\mathbb{R}$ and $\\sigma  0$.\n    *   Decision Rule: Predict $Y=1$ if and only if $S \\geq t$ for a threshold $t \\in \\mathbb{R}$.\n    *   Definitions: True Positive Rate (TPR), False Positive Rate (FPR), Receiver Operating Characteristic (ROC) curve, Area Under the Curve (AUC).\n    *   Tasks: \n        1. Derive the parametric form of the ROC curve (TPR as a function of FPR).\n        2. Derive a closed-form analytic expression for the AUC as a function of $\\mu_{1}$, $\\mu_{0}$, and $\\sigma$.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   The problem is scientifically grounded, rooted in standard statistical decision theory and machine learning principles. The use of Gaussian distributions for class-conditional scores is a common and well-understood model (e.g., basis for Linear Discriminant Analysis).\n    *   The problem is well-posed. All necessary information is provided, the terms are standard and clearly defined, and a unique, meaningful solution exists.\n    *   The problem is objective, using precise mathematical language without subjective or ambiguous statements.\n    *   The problem exhibits none of the invalidity flaws listed in the instructions. It is a standard, solvable problem in its field.\n\n*   **Step 3: Verdict and Action**\n    *   The problem is valid. The solution process will proceed.\n\n**2. Solution Derivation**\n\nThe solution requires two parts: first, deriving the ROC curve equation, and second, calculating the area under it (AUC).\n\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution, $\\mathcal{N}(0, 1)$, defined as $\\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du$.\n\n**Part 1: Derivation of the ROC Curve**\n\nThe ROC curve is a plot of the True Positive Rate (TPR) against the False Positive Rate (FPR) for varying decision thresholds $t$.\n\nThe True Positive Rate, $\\text{TPR}(t)$, is the probability of correctly classifying a positive instance ($Y=1$). The classification rule is $S \\ge t$.\n$$\n\\text{TPR}(t) = P(S \\ge t \\mid Y=1)\n$$\nGiven $S \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$, we can standardize the random variable $S$:\n$$\n\\text{TPR}(t) = P\\left(\\frac{S-\\mu_1}{\\sigma} \\ge \\frac{t-\\mu_1}{\\sigma} \\mid Y=1\\right)\n$$\nThe term $\\frac{S-\\mu_1}{\\sigma}$ is a standard normal random variable. Let's call it $Z$.\n$$\n\\text{TPR}(t) = P\\left(Z \\ge \\frac{t-\\mu_1}{\\sigma}\\right) = 1 - P\\left(Z  \\frac{t-\\mu_1}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{t-\\mu_1}{\\sigma}\\right)\n$$\nUsing the symmetry property of the standard normal distribution, $1 - \\Phi(z) = \\Phi(-z)$, we get:\n$$\n\\text{TPR}(t) = \\Phi\\left(-\\frac{t-\\mu_1}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_1-t}{\\sigma}\\right)\n$$\n\nThe False Positive Rate, $\\text{FPR}(t)$, is the probability of incorrectly classifying a negative instance ($Y=0$) as positive.\n$$\n\\text{FPR}(t) = P(S \\ge t \\mid Y=0)\n$$\nGiven $S \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$, we standardize similarly:\n$$\n\\text{FPR}(t) = P\\left(\\frac{S-\\mu_0}{\\sigma} \\ge \\frac{t-\\mu_0}{\\sigma} \\mid Y=0\\right)\n$$\n$$\n\\text{FPR}(t) = P\\left(Z \\ge \\frac{t-\\mu_0}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{t-\\mu_0}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_0-t}{\\sigma}\\right)\n$$\n\nTo find the ROC curve, we must express TPR as a function of FPR. Let $y = \\text{TPR}$ and $x = \\text{FPR}$.\nWe have the parametric equations:\n$y(t) = \\Phi\\left(\\frac{\\mu_1-t}{\\sigma}\\right)$\n$x(t) = \\Phi\\left(\\frac{\\mu_0-t}{\\sigma}\\right)$\n\nFrom the equation for $x$, we can express $t$ in terms of $x$ using the inverse CDF of the standard normal distribution, $\\Phi^{-1}$ (the probit function).\n$$\n\\Phi^{-1}(x) = \\frac{\\mu_0-t}{\\sigma}\n$$\nSolving for $t$:\n$$\n\\sigma \\Phi^{-1}(x) = \\mu_0-t \\implies t = \\mu_0 - \\sigma \\Phi^{-1}(x)\n$$\nNow, substitute this expression for $t$ into the equation for $y$:\n$$\ny = \\Phi\\left(\\frac{\\mu_1 - (\\mu_0 - \\sigma \\Phi^{-1}(x))}{\\sigma}\\right)\n$$\n$$\ny = \\Phi\\left(\\frac{\\mu_1 - \\mu_0 + \\sigma \\Phi^{-1}(x)}{\\sigma}\\right)\n$$\n$$\ny = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sigma} + \\Phi^{-1}(x)\\right)\n$$\nThus, the parametric form of the ROC curve is:\n$$\n\\text{TPR} = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sigma} + \\Phi^{-1}(\\text{FPR})\\right)\n$$\n\n**Part 2: Derivation of the Area Under the Curve (AUC)**\n\nThe AUC is the integral of the ROC function from $\\text{FPR}=0$ to $\\text{FPR}=1$.\n$$\n\\text{AUC} = \\int_{0}^{1} \\text{TPR}(\\text{FPR}) \\, d(\\text{FPR})\n$$\nWhile this integral can be evaluated directly, a more insightful method is to use the probabilistic interpretation of AUC. The AUC is equivalent to the probability that a randomly chosen positive instance will have a higher score than a randomly chosen negative instance. Let $S_1$ be a random score from a positive case ($Y=1$) and $S_0$ be a random score from a negative case ($Y=0$). Then:\n$$\n\\text{AUC} = P(S_1 > S_0)\n$$\nHere, $S_1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ and $S_0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$, and they are independent.\nWe are interested in the probability $P(S_1 - S_0  0)$.\nLet's define a new random variable $D = S_1 - S_0$. Since $S_1$ and $S_0$ are independent normal random variables, their difference $D$ is also normally distributed.\n\nThe mean of $D$ is:\n$$\nE[D] = E[S_1 - S_0] = E[S_1] - E[S_0] = \\mu_1 - \\mu_0\n$$\nThe variance of $D$ is (due to independence):\n$$\n\\text{Var}(D) = \\text{Var}(S_1 - S_0) = \\text{Var}(S_1) + \\text{Var}(-S_0) = \\text{Var}(S_1) + (-1)^2 \\text{Var}(S_0) = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nSo, the distribution of the difference is $D \\sim \\mathcal{N}(\\mu_1 - \\mu_0, 2\\sigma^2)$.\n\nNow we can compute the probability $P(D  0)$. We standardize the variable $D$:\n$$\nP(D0) = P\\left(\\frac{D - E[D]}{\\sqrt{\\text{Var}(D)}}  \\frac{0 - E[D]}{\\sqrt{\\text{Var}(D)}}\\right)\n$$\n$$\nP(D0) = P\\left(Z  \\frac{0 - (\\mu_1 - \\mu_0)}{\\sqrt{2\\sigma^2}}\\right)\n$$\nwhere $Z \\sim \\mathcal{N}(0, 1)$.\n$$\nP(D0) = P\\left(Z  -\\frac{\\mu_1 - \\mu_0}{\\sigma\\sqrt{2}}\\right)\n$$\nUsing the symmetry property $P(Z  -z) = P(Z  z) = \\Phi(z)$, we find:\n$$\nP(D0) = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sigma\\sqrt{2}}\\right)\n$$\nTherefore, the closed-form analytic expression for the AUC is:\n$$\n\\text{AUC} = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sigma\\sqrt{2}}\\right)\n$$\nThis completes the derivation.", "answer": "$$\\boxed{\\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sigma\\sqrt{2}}\\right)}$$", "id": "3167105"}, {"introduction": "While the AUC is defined geometrically as the area under the ROC curve, its most intuitive and widely cited interpretation is probabilistic. This hands-on coding exercise [@problem_id:3167097] allows you to empirically verify this fundamental equivalence: the AUC is the probability that a randomly chosen positive instance will receive a higher score than a randomly chosen negative instance. By implementing and comparing both the geometric and probabilistic calculations, you will solidify your understanding and gain practical skills for computing AUC from real data.", "problem": "Construct a self-contained program that, for several specified test cases, compares two empirically computed quantities derived from binary classification scores: the geometric Area Under the Curve for the Receiver Operating Characteristic (ROC) and a direct pairwise exceedance probability between positives and negatives. The fundamental base for this task is the definition of the Receiver Operating Characteristic (ROC) curve and its Area Under the Curve (AUC), along with the definition of probability as the limit of relative frequency. You must work strictly from these core definitions.\n\nLet a binary classification problem be represented by a real-valued score function $S:\\mathbb{R}^d\\to\\mathbb{R}$ applied to instances, with true labels $Y\\in\\{0,1\\}$, where $Y=1$ denotes the positive class and $Y=0$ denotes the negative class. For any threshold $t\\in\\mathbb{R}$, define the True Positive Rate (TPR) and False Positive Rate (FPR) as\n$$\n\\mathrm{TPR}(t) = \\frac{\\#\\{i:\\,Y_i=1,\\,S_i\\ge t\\}}{\\#\\{i:\\,Y_i=1\\}},\\quad\n\\mathrm{FPR}(t) = \\frac{\\#\\{i:\\,Y_i=0,\\,S_i\\ge t\\}}{\\#\\{i:\\,Y_i=0\\}}.\n$$\nThe Receiver Operating Characteristic (ROC) curve is the parametric curve $\\big(\\mathrm{FPR}(t),\\mathrm{TPR}(t)\\big)$ traced as $t$ varies over all possible thresholds. The Area Under the Curve (AUC) is defined as the Riemann integral of $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ along this curve:\n$$\n\\mathrm{AUC} = \\int \\mathrm{TPR}\\,d(\\mathrm{FPR}),\n$$\ninterpreted geometrically as the area below the ROC curve in the unit square.\n\nIndependently, consider two random scores $S^+$ and $S^-$ obtained by sampling one positive instance and one negative instance according to the empirical distributions of the positive and negative scores, respectively. Define the direct pairwise exceedance probability as\n$$\np_{} = \\mathbb{P}\\big(S^+  S^-\\big),\n$$\nand the tie-adjusted exceedance probability as\n$$\np_{\\text{half}} = \\mathbb{P}\\big(S^+  S^-\\big) + \\frac{1}{2}\\,\\mathbb{P}\\big(S^+ = S^-\\big).\n$$\nYour program must empirically estimate $\\mathrm{AUC}$ from the ROC definition using the trapezoidal rule over the empirical ROC points, and independently estimate $p_{}$ and $p_{\\text{half}}$ by counting all positive–negative score pairs. Then, for each test case, compute two floats: the absolute difference $|\\mathrm{AUC} - p_{}|$ and the absolute difference $|\\mathrm{AUC} - p_{\\text{half}}|$.\n\nImplementation requirements:\n- For each test case, generate positive scores and negative scores according to the specified distributions and parameters, using a fixed random seed $42$ for reproducibility.\n- Compute $\\mathrm{AUC}$ by:\n  - Sorting all scores in descending order.\n  - Sweeping thresholds at each unique score value to construct empirical ROC points $\\big(\\mathrm{FPR},\\mathrm{TPR}\\big)$.\n  - Integrating $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ via the trapezoidal rule on these points, including the endpoints $(0,0)$ and $(1,1)$.\n- Compute $p_{}$ and $p_{\\text{half}}$ by counting all positive–negative pairs:\n  - For $p_{}$: count strictly greater comparisons.\n  - For $p_{\\text{half}}$: add half credit for ties (equal scores).\n\nTest suite:\n- Case $1$ (continuous, moderate separation): Positive scores $S^+$ sampled from a Normal distribution with mean $\\mu_+=1$ and standard deviation $\\sigma_+=1$; negative scores $S^-$ sampled from a Normal distribution with mean $\\mu_-=0$ and standard deviation $\\sigma_-=1$; sample sizes $n^+=1000$, $n^-=1000$.\n- Case $2$ (continuous, identical distributions): $S^+$ and $S^-$ both from Normal with $\\mu=0$, $\\sigma=1$; $n^+=2000$, $n^-=2000$.\n- Case $3$ (continuous, strong separation): $S^+$ from Normal with $\\mu_+=3$, $\\sigma_+=0.5$; $S^-$ from Normal with $\\mu_-=0$, $\\sigma_-=0.5$; $n^+=1000$, $n^-=1000$.\n- Case $4$ (discrete with ties): $S^+$ from Bernoulli with success probability $p_+=0.6$; $S^-$ from Bernoulli with success probability $p_-=0.4$; $n^+=5000$, $n^-=5000$.\n- Case $5$ (continuous, small sample, boundary variance): $S^+$ from Normal with $\\mu_+=0.2$, $\\sigma_+=1$; $S^-$ from Normal with $\\mu_-=0$, $\\sigma_-=1$; $n^+=10$, $n^-=10$.\n- Case $6$ (continuous, skewed distributions): $S^+$ from Exponential with rate $\\lambda_+=1$ (scale $1/\\lambda_+=1$); $S^-$ from Exponential with rate $\\lambda_-=2$ (scale $1/\\lambda_-=0.5$); $n^+=2000$, $n^-=2000$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$\n[\\Delta_1^{},\\Delta_1^{\\text{half}},\\Delta_2^{},\\Delta_2^{\\text{half}},\\dots,\\Delta_6^{},\\Delta_6^{\\text{half}}],\n$$\nwhere $\\Delta_k^{}=|\\mathrm{AUC}-p_{}|$ and $\\Delta_k^{\\text{half}}=|\\mathrm{AUC}-p_{\\text{half}}|$ for case $k$. Each float should be rounded to $6$ decimal places.", "solution": "The user has provided a scientifically and mathematically sound problem. I will now proceed with the solution.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens**\n\n*   **Problem Domain**: Binary classification, Receiver Operating Characteristic (ROC) curves, Area Under the Curve (AUC).\n*   **Definitions**:\n    *   Positive class label $Y=1$, negative class label $Y=0$.\n    *   Score function $S:\\mathbb{R}^d\\to\\mathbb{R}$.\n    *   True Positive Rate: $\\mathrm{TPR}(t) = \\frac{\\#\\{i:\\,Y_i=1,\\,S_i\\ge t\\}}{\\#\\{i:\\,Y_i=1\\}}$.\n    *   False Positive Rate: $\\mathrm{FPR}(t) = \\frac{\\#\\{i:\\,Y_i=0,\\,S_i\\ge t\\}}{\\#\\{i:\\,Y_i=0\\}}$.\n    *   ROC Curve: Parametric curve $(\\mathrm{FPR}(t),\\mathrm{TPR}(t))$ as $t$ varies.\n    *   AUC: $\\mathrm{AUC} = \\int \\mathrm{TPR}\\,d(\\mathrm{FPR})$, interpreted as the area under the ROC curve.\n    *   Direct Pairwise Exceedance Probability: $p_{} = \\mathbb{P}(S^+  S^-)$, where $S^+$ and $S^-$ are scores from a randomly chosen positive and negative instance, respectively.\n    *   Tie-Adjusted Exceedance Probability: $p_{\\text{half}} = \\mathbb{P}(S^+  S^-) + \\frac{1}{2}\\,\\mathbb{P}(S^+ = S^-)$.\n*   **Computational Requirements**:\n    *   Use a fixed random seed of $42$.\n    *   Estimate $\\mathrm{AUC}$ by constructing empirical ROC points by sweeping thresholds at unique score values and using the trapezoidal rule for integration. The endpoints $(0,0)$ and $(1,1)$ must be included.\n    *   Estimate $p_{}$ and $p_{\\text{half}}$ by enumerating all pairs of positive and negative scores and counting outcomes.\n*   **Output**: For each test case, compute two absolute differences: $\\Delta^{} = |\\mathrm{AUC} - p_{}|$ and $\\Delta^{\\text{half}} = |\\mathrm{AUC} - p_{\\text{half}}|$. The final output must be a single comma-separated list of these differences for all test cases, with each value rounded to $6$ decimal places.\n*   **Test Cases**:\n    1.  $S^+ \\sim N(\\mu=1, \\sigma=1)$, $S^- \\sim N(\\mu=0, \\sigma=1)$; $n^+=1000, n^-=1000$.\n    2.  $S^+ \\sim N(\\mu=0, \\sigma=1)$, $S^- \\sim N(\\mu=0, \\sigma=1)$; $n^+=2000, n^-=2000$.\n    3.  $S^+ \\sim N(\\mu=3, \\sigma=0.5)$, $S^- \\sim N(\\mu=0, \\sigma=0.5)$; $n^+=1000, n^-=1000$.\n    4.  $S^+ \\sim \\text{Bernoulli}(p=0.6)$, $S^- \\sim \\text{Bernoulli}(p=0.4)$; $n^+=5000, n^-=5000$.\n    5.  $S^+ \\sim N(\\mu=0.2, \\sigma=1)$, $S^- \\sim N(\\mu=0, \\sigma=1)$; $n^+=10, n^-=10$.\n    6.  $S^+ \\sim \\text{Exponential}(\\lambda=1)$, $S^- \\sim \\text{Exponential}(\\lambda=2)$; $n^+=2000, n^-=2000$.\n\n**1.2. Validation and Verdict**\n\nThe problem is reviewed against the validation criteria:\n*   **Scientifically Grounded**: The definitions of TPR, FPR, ROC, and AUC are standard in statistics and machine learning. The relationship between AUC and the probability $\\mathbb{P}(S^+  S^-)$ (more precisely, the Mann-Whitney U statistic which is equivalent to $p_{\\text{half}}$) is a fundamental and well-established theorem. The problem asks for an empirical verification of this theorem using specified numerical methods. All concepts are scientifically sound.\n*   **Well-Posed**: The problem is well-posed. The inputs (distributions and sample sizes), computational procedures (trapezoidal rule, pairwise counting), and desired outputs are all specified unambiguously. The use of a fixed random seed ensures a unique and reproducible solution.\n*   **Objective**: The language is formal, precise, and free of any subjectivity or bias.\n*   **Complete and Consistent**: The problem statement is self-contained. It provides all necessary definitions, data generation parameters, and computational instructions. There are no contradictions.\n*   **Feasible**: The specified sample sizes lead to computations that are intensive but perfectly feasible on modern hardware (e.g., $5000 \\times 5000 = 25 \\times 10^6$ pairwise comparisons).\n\n**Verdict**: The problem is **valid**.\n\n### Step 2: Solution Design\n\nThe task requires implementing two distinct methods to quantify the separation between positive and negative score distributions and then comparing their results.\n\n**2.1. Calculation of AUC via the Trapezoidal Rule**\n\nThe empirical ROC curve is a plot of $(\\mathrm{FPR}, \\mathrm{TPR})$ pairs evaluated at different classification thresholds. A key insight is that the ROC curve is piecewise constant and only changes at thresholds corresponding to the observed score values. The most direct way to construct the curve and compute its area is as follows:\n\n1.  Combine all positive scores (labeled $1$) and negative scores (labeled $0$) into a single collection of $(score, label)$ pairs. Let $n^+$ be the total number of positive instances and $n^-$ be the total number of negative instances.\n2.  Sort these pairs in descending order based on their scores. In case of ties in scores, the order of labels does not affect the final AUC calculation as they will be processed as a single block.\n3.  Initialize the ROC curve with the point $(0,0)$, which corresponds to a threshold $t \\to \\infty$ where no instances are classified as positive. Initialize the true positive count $(TP)$ and false positive count $(FP)$ to $0$.\n4.  Iterate through the sorted pairs. For each pair $(s_i, y_i)$:\n    *   Update the counts: if $y_i=1$, increment $TP$; if $y_i=0$, increment $FP$.\n    *   To construct the empirical ROC points, we only add a new point when the threshold effectively changes. This occurs after processing a block of scores with the same value. Thus, we check if the current score $s_i$ is different from the next score $s_{i+1}$ (or if we are at the end of the list).\n    *   When such a change occurs, a new point on the ROC curve is defined by $(\\frac{FP}{n^-}, \\frac{TP}{n^+})$. Add this point to our list of ROC points.\n5.  After iterating through all scores, the final point will be $(1,1)$. The complete list of generated points, starting with $(0,0)$, defines the vertices of the empirical ROC curve.\n6.  The Area Under the Curve (AUC) is then calculated by applying the trapezoidal rule to these vertices. For each consecutive pair of points $(x_i, y_i)$ and $(x_{i+1}, y_{i+1})$, the area of the trapezoid underneath is given by $\\frac{1}{2}(y_i + y_{i+1})(x_{i+1} - x_i)$. Summing these areas over all segments gives the total AUC.\n\n**2.2. Calculation of Pairwise Exceedance Probabilities**\n\nThe probabilities $p_{}$ and $p_{\\text{half}}$ are estimated directly from their definitions by empirical counting.\n\n1.  Given the set of positive scores $\\{S^+_1, \\dots, S^+_{n^+}\\}$ and negative scores $\\{S^-_1, \\dots, S^-_{n^-}\\}$, we form a total of $N = n^+ \\times n^-$ pairs.\n2.  Initialize two counters: `greater_count = 0` and `equal_count = 0`.\n3.  Iterate through every possible pair $(S^+_i, S^-_j)$:\n    *   If $S^+_i  S^-_j$, increment `greater_count`.\n    *   If $S^+_i = S^-_j$, increment `equal_count`.\n4.  The empirical probabilities are then calculated as:\n    *   $p_{} = \\frac{\\text{greater\\_count}}{N}$\n    *   $p_{\\text{half}} = \\frac{\\text{greater\\_count} + 0.5 \\times \\text{equal\\_count}}{N}$\n\nThis method is computationally equivalent to calculating the Mann-Whitney U statistic, which is known to be equal to the AUC. The problem thus sets up a numerical verification of this statistical theorem.\n\n**2.3. Program Implementation**\n\nA Python program will be structured to execute these calculations for each of the six test cases.\n*   A centralized random number generator (`numpy.random.default_rng`) with the specified seed ($42$) will be used for reproducibility.\n*   Helper functions will be defined for generating scores, calculating AUC, and calculating the pairwise probabilities to ensure code clarity and modularity.\n*   The main loop will iterate through the test case parameters, call the helper functions, compute the required absolute differences, and store the rounded results.\n*   Finally, the program will print the collected results in the specified format. For exponential distributions, we note that the `numpy` library's `exponential` function is parametrized by the scale $\\beta$, which is the reciprocal of the rate $\\lambda$ (i.e., $\\beta = 1/\\lambda$).", "answer": "```python\nimport numpy as np\n\ndef calculate_auc(pos_scores, neg_scores):\n    \"\"\"\n    Computes the Area Under the ROC Curve using the trapezoidal rule.\n    \"\"\"\n    n_pos = len(pos_scores)\n    n_neg = len(neg_scores)\n    \n    if n_pos == 0 or n_neg == 0:\n        return 0.5\n\n    # Combine scores and labels, then sort by score descending.\n    labels = [1] * n_pos + [0] * n_neg\n    scores = list(pos_scores) + list(neg_scores)\n    \n    sorted_pairs = sorted(zip(scores, labels), key=lambda x: x[0], reverse=True)\n    \n    # Initialize ROC points starting at (0,0).\n    roc_points = [(0.0, 0.0)]\n    tp_count, fp_count = 0, 0\n    \n    # Iterate through sorted scores to define ROC curve vertices.\n    for i in range(len(sorted_pairs)):\n        score, label = sorted_pairs[i]\n        \n        if label == 1:\n            tp_count += 1\n        else:\n            fp_count += 1\n            \n        # A new vertex is formed when the score value changes, or at the end.\n        is_last_element = (i == len(sorted_pairs) - 1)\n        is_score_change = not is_last_element and (score != sorted_pairs[i+1][0])\n        \n        if is_last_element or is_score_change:\n            tpr = tp_count / n_pos\n            fpr = fp_count / n_neg\n            roc_points.append((fpr, tpr))\n            \n    # Calculate area using the trapezoidal rule.\n    auc = 0.0\n    for i in range(len(roc_points) - 1):\n        x1, y1 = roc_points[i]\n        x2, y2 = roc_points[i+1]\n        auc += (x2 - x1) * (y1 + y2) / 2.0\n        \n    return auc\n\ndef calculate_probabilities(pos_scores, neg_scores):\n    \"\"\"\n    Computes p_ and p_half by direct pairwise comparison.\n    \"\"\"\n    n_pos = len(pos_scores)\n    n_neg = len(neg_scores)\n    total_pairs = n_pos * n_neg\n\n    if total_pairs == 0:\n        return 0.5, 0.5\n\n    gt_count = 0\n    eq_count = 0\n\n    for s_pos in pos_scores:\n        for s_neg in neg_scores:\n            if s_pos  s_neg:\n                gt_count += 1\n            elif s_pos == s_neg:\n                eq_count += 1\n                \n    p_gt = gt_count / total_pairs\n    p_half = (gt_count + 0.5 * eq_count) / total_pairs\n    \n    return p_gt, p_half\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    rng = np.random.default_rng(seed=42)\n    \n    test_cases = [\n        {'type': 'normal', 'params_pos': {'loc': 1, 'scale': 1, 'size': 1000}, 'params_neg': {'loc': 0, 'scale': 1, 'size': 1000}},\n        {'type': 'normal', 'params_pos': {'loc': 0, 'scale': 1, 'size': 2000}, 'params_neg': {'loc': 0, 'scale': 1, 'size': 2000}},\n        {'type': 'normal', 'params_pos': {'loc': 3, 'scale': 0.5, 'size': 1000}, 'params_neg': {'loc': 0, 'scale': 0.5, 'size': 1000}},\n        {'type': 'bernoulli', 'params_pos': {'n': 1, 'p': 0.6, 'size': 5000}, 'params_neg': {'n': 1, 'p': 0.4, 'size': 5000}},\n        {'type': 'normal', 'params_pos': {'loc': 0.2, 'scale': 1, 'size': 10}, 'params_neg': {'loc': 0, 'scale': 1, 'size': 10}},\n        {'type': 'exponential', 'params_pos': {'scale': 1.0, 'size': 2000}, 'params_neg': {'scale': 0.5, 'size': 2000}} # rate=1/scale\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        if case['type'] == 'normal':\n            pos_scores = rng.normal(**case['params_pos'])\n            neg_scores = rng.normal(**case['params_neg'])\n        elif case['type'] == 'bernoulli':\n            pos_scores = rng.binomial(**case['params_pos'])\n            neg_scores = rng.binomial(**case['params_neg'])\n        elif case['type'] == 'exponential':\n            pos_scores = rng.exponential(**case['params_pos'])\n            neg_scores = rng.exponential(**case['params_neg'])\n\n        # 1. Compute AUC using the trapezoidal rule on the empirical ROC curve\n        auc = calculate_auc(pos_scores, neg_scores)\n        \n        # 2. Compute probabilities by pairwise counting\n        p_gt, p_half = calculate_probabilities(pos_scores, neg_scores)\n        \n        # 3. Calculate absolute differences\n        delta_gt = abs(auc - p_gt)\n        delta_half = abs(auc - p_half)\n        \n        results.append(round(delta_gt, 6))\n        results.append(round(delta_half, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3167097"}, {"introduction": "A high AUC indicates strong discriminatory power, but is it always the right metric for the job? This final practice explores a critical nuance in model evaluation by contrasting the ROC curve with the Precision-Recall (PR) curve [@problem_id:4946956]. You will demonstrate mathematically how two models with identical ROC curves can exhibit vastly different practical performance when class prevalence is low, a common scenario in fields like medical diagnostics. This exercise highlights the importance of choosing evaluation metrics that align with the specific goals and data characteristics of your application.", "problem": "A biomedical screening test outputs a continuous risk score, and decisions are made by comparing the score to a threshold. For thresholds spanning the entire score range, the test’s receiver operating characteristic (ROC) curve—true positive rate versus false positive rate—can be parameterized by recall $t \\in [0,1]$ (recall equals true positive rate) with a piecewise-linear false positive rate function $f(t)$:\n$$\nf(t) \\;=\\;\n\\begin{cases}\n0.1\\,t,  0 \\le t \\le 0.4, \\\\\n0.65\\,t - 0.22,  0.4  t \\le 0.8, \\\\\n3.5\\,t - 2.5,  0.8  t \\le 1.\n\\end{cases}\n$$\nTwo classification systems, $\\mathcal{C}_A$ and $\\mathcal{C}_B$, are deployed in two independent clinical cohorts. They share identical discrimination captured by the ROC curve above (hence they have identical ROC curves), but the cohorts differ in disease prevalence: for $\\mathcal{C}_A$, the prevalence is $\\pi_A = 0.1$, and for $\\mathcal{C}_B$, the prevalence is $\\pi_B = 0.4$.\n\nUsing only fundamental definitions—prevalence $\\pi$ as the proportion of truly diseased individuals, true positive rate $TPR = TP/P$, false positive rate $FPR = FP/N$, recall $r = TPR$, and precision $p = TP/(TP+FP)$—construct the precision-recall (PR) curves implied by the ROC function $f(t)$ for each cohort and compute the average precision (area under the precision-recall curve) for each cohort by integrating precision over recall $t$ from $0$ to $1$. Then, compute the difference in average precision between the two cohorts,\n$$\n\\Delta \\;=\\; AP(\\pi_B) \\;-\\; AP(\\pi_A).\n$$\nRound your final answer for $\\Delta$ to four significant figures. Express your answer as a decimal (no percentage sign).", "solution": "We begin from definitions. Let the total population be normalized to $1$. Then the number of truly positive individuals is $P = \\pi$ and the number of truly negative individuals is $N = 1 - \\pi$. At a given recall level $t \\in [0,1]$, the true positives are $TP = P \\cdot TPR = \\pi \\, t$, and the false positives are $FP = N \\cdot FPR = (1-\\pi)\\, f(t)$, where $f(t)$ is the false positive rate as a function of recall from the given ROC parameterization.\n\nPrecision $p$ at recall $t$ is defined as $p = TP/(TP + FP)$, so substituting yields\n$$\np(t;\\pi) \\;=\\; \\frac{\\pi \\, t}{\\pi \\, t + (1-\\pi)\\, f(t)}.\n$$\nAverage precision $AP(\\pi)$ is the area under the precision-recall curve, defined here as\n$$\nAP(\\pi) \\;=\\; \\int_{0}^{1} p(t;\\pi)\\, dt \\;=\\; \\int_{0}^{1} \\frac{\\pi \\, t}{\\pi \\, t + (1-\\pi)\\, f(t)} \\, dt.\n$$\nBecause $f(t)$ is piecewise linear, we evaluate the integral on each segment and sum.\n\nWrite $f(t) = m\\,t + c$ on a segment; then the integrand becomes\n$$\n\\frac{\\pi \\, t}{\\pi \\, t + (1-\\pi)\\,(m\\,t + c)} \\;=\\; \\frac{\\pi \\, t}{\\alpha \\, t + \\beta},\n$$\nwhere $\\alpha = \\pi + (1-\\pi)\\, m$ and $\\beta = (1-\\pi)\\, c$. The antiderivative is obtained by noting\n$$\n\\int \\frac{t}{\\alpha\\, t + \\beta}\\, dt \\;=\\; \\frac{t}{\\alpha} \\;-\\; \\frac{\\beta}{\\alpha^{2}} \\ln(\\alpha\\, t + \\beta) \\;+\\; C.\n$$\nTherefore, on any segment $[a,b]$,\n$$\n\\int_{a}^{b} \\frac{\\pi \\, t}{\\alpha \\, t + \\beta}\\, dt \\;=\\; \\pi \\left[ \\frac{t}{\\alpha} - \\frac{\\beta}{\\alpha^{2}} \\ln(\\alpha \\, t + \\beta) \\right]_{t=a}^{t=b}.\n$$\n\nWe now apply this to each of the three segments for the two prevalences.\n\nSegment 1: $t \\in [0,0.4]$, $f(t) = 0.1\\, t$ so $m = 0.1$, $c = 0$. Then $\\alpha_{1} = \\pi + (1-\\pi)\\, 0.1 = 0.1 + 0.9\\, \\pi$ and $\\beta_{1} = 0$. The integral simplifies to\n$$\nI_{1}(\\pi) \\;=\\; \\int_{0}^{0.4} \\frac{\\pi \\, t}{\\alpha_{1} \\, t}\\, dt \\;=\\; \\int_{0}^{0.4} \\frac{\\pi}{\\alpha_{1}}\\, dt \\;=\\; \\frac{0.4 \\, \\pi}{0.1 + 0.9\\, \\pi}.\n$$\nThus,\n- For $\\pi_A = 0.1$: $I_{1}(\\pi_A) = \\frac{0.4 \\cdot 0.1}{0.1 + 0.9 \\cdot 0.1} = \\frac{0.04}{0.19} = \\frac{4}{19} \\approx 0.210526315789$.\n- For $\\pi_B = 0.4$: $I_{1}(\\pi_B) = \\frac{0.4 \\cdot 0.4}{0.1 + 0.9 \\cdot 0.4} = \\frac{0.16}{0.46} = \\frac{8}{23} \\approx 0.347826086957$.\n\nSegment 2: $t \\in [0.4,0.8]$, $f(t) = 0.65\\, t - 0.22$ so $m = 0.65$, $c = -0.22$. Then\n$$\n\\alpha_{2} = \\pi + (1-\\pi)\\, 0.65 \\;=\\; 0.65 + 0.35\\, \\pi, \\qquad \\beta_{2} = (1-\\pi)\\, (-0.22) \\;=\\; -0.22 + 0.22\\, \\pi.\n$$\nThe segment integral is\n$$\nI_{2}(\\pi) \\;=\\; \\pi \\left[ \\frac{t}{\\alpha_{2}} - \\frac{\\beta_{2}}{\\alpha_{2}^{2}} \\ln(\\alpha_{2} \\, t + \\beta_{2}) \\right]_{t=0.4}^{t=0.8}.\n$$\nEvaluate numerically for each prevalence:\n- For $\\pi_A = 0.1$: $\\alpha_{2} = 0.685$, $\\beta_{2} = -0.198$, so $\\alpha_{2}\\, t + \\beta_{2}$ at $t=0.4$ is $0.076$ and at $t=0.8$ is $0.35$. Then\n$$\nI_{2}(\\pi_A) \\;=\\; 0.1 \\left( \\frac{0.4}{0.685} - \\frac{-0.198}{0.685^{2}} \\ln\\!\\frac{0.35}{0.076} \\right)\n\\;\\approx\\; 0.1 \\left( 0.583941606 + 0.644433120 \\right)\n\\;\\approx\\; 0.122837473.\n$$\n- For $\\pi_B = 0.4$: $\\alpha_{2} = 0.79$, $\\beta_{2} = -0.132$, so $\\alpha_{2}\\, t + \\beta_{2}$ at $t=0.4$ is $0.184$ and at $t=0.8$ is $0.5$. Then\n$$\nI_{2}(\\pi_B) \\;=\\; 0.4 \\left( \\frac{0.4}{0.79} - \\frac{-0.132}{0.79^{2}} \\ln\\!\\frac{0.5}{0.184} \\right)\n\\;\\approx\\; 0.4 \\left( 0.506329114 + 0.211419421 \\right)\n\\;\\approx\\; 0.287099410.\n$$\n\nSegment 3: $t \\in [0.8,1]$, $f(t) = 3.5\\, t - 2.5$ so $m = 3.5$, $c = -2.5$. Then\n$$\n\\alpha_{3} = \\pi + (1-\\pi)\\, 3.5 \\;=\\; 3.5 - 2.5\\, \\pi, \\qquad \\beta_{3} = (1-\\pi)\\, (-2.5) \\;=\\; -2.5 + 2.5\\, \\pi.\n$$\nThe segment integral is\n$$\nI_{3}(\\pi) \\;=\\; \\pi \\left[ \\frac{t}{\\alpha_{3}} - \\frac{\\beta_{3}}{\\alpha_{3}^{2}} \\ln(\\alpha_{3} \\, t + \\beta_{3}) \\right]_{t=0.8}^{t=1}.\n$$\nNote $\\alpha_{3} \\cdot 1 + \\beta_{3} = 1$ for any $\\pi$, and $\\alpha_{3} \\cdot 0.8 + \\beta_{3} = 0.3 + 0.5\\, \\pi$. Thus\n$$\nI_{3}(\\pi) \\;=\\; \\pi \\left( \\frac{0.2}{\\alpha_{3}} - \\frac{\\beta_{3}}{\\alpha_{3}^{2}} \\ln\\!\\frac{1}{0.3 + 0.5\\, \\pi} \\right).\n$$\nEvaluate numerically:\n- For $\\pi_A = 0.1$: $\\alpha_{3} = 3.25$, $\\beta_{3} = -2.25$, $0.3 + 0.5\\, \\pi_A = 0.35$. Then\n$$\nI_{3}(\\pi_A) \\;=\\; 0.1 \\left( \\frac{0.2}{3.25} - \\frac{-2.25}{3.25^{2}} \\ln\\!\\frac{1}{0.35} \\right)\n\\;=\\; 0.1 \\left( 0.061538462 + 0.223630740 \\right)\n\\;\\approx\\; 0.028516920.\n$$\n- For $\\pi_B = 0.4$: $\\alpha_{3} = 2.5$, $\\beta_{3} = -1.5$, $0.3 + 0.5\\, \\pi_B = 0.5$. Then\n$$\nI_{3}(\\pi_B) \\;=\\; 0.4 \\left( \\frac{0.2}{2.5} - \\frac{-1.5}{2.5^{2}} \\ln\\!\\frac{1}{0.5} \\right)\n\\;=\\; 0.4 \\left( 0.08 + 0.24 \\ln 2 \\right)\n\\;\\approx\\; 0.098542129.\n$$\n\nSumming segments yields average precision for each cohort:\n$$\nAP(\\pi_A) \\;=\\; I_{1}(\\pi_A) + I_{2}(\\pi_A) + I_{3}(\\pi_A)\n\\;\\approx\\; 0.210526316 + 0.122837473 + 0.028516920 \\;=\\; 0.361880709,\n$$\n$$\nAP(\\pi_B) \\;=\\; I_{1}(\\pi_B) + I_{2}(\\pi_B) + I_{3}(\\pi_B)\n\\;\\approx\\; 0.347826087 + 0.287099410 + 0.098542129 \\;=\\; 0.733467626.\n$$\nTherefore, the difference in average precision due solely to prevalence is\n$$\n\\Delta \\;=\\; AP(\\pi_B) - AP(\\pi_A) \\;\\approx\\; 0.733467626 - 0.361880709 \\;=\\; 0.371586918.\n$$\nRounded to four significant figures, $\\Delta \\approx 0.3716$.", "answer": "$$\\boxed{0.3716}$$", "id": "4946956"}]}