{"hands_on_practices": [{"introduction": "The true power of MANOVA is not just in comparing groups, but in testing specific, nuanced scientific questions. This practice bridges the gap between a biological inquiry and its statistical formulation. You will learn to encode a hypothesis—comparing a combination of treatments on a custom-weighted biomarker score—into the precise language of linear algebra using a contrast vector $L$ and a response weight matrix $M$, a fundamental skill for any applied researcher [@problem_id:4931274].", "problem": "A randomized biostatistics study compares three groups: control ($C$), Treatment $A$ ($T_A$), and Treatment $B$ ($T_B$). For each participant, three inflammatory biomarkers are measured: interleukin-$6$ ($\\mathrm{IL}\\text{-}6$), C-reactive protein ($\\mathrm{CRP}$), and tumor necrosis factor-$\\alpha$ ($\\mathrm{TNF}\\text{-}\\alpha$). Suppose the design has equal group sizes with $n$ replicates per group, the Multivariate Analysis of Variance (MANOVA) cell-means model holds, and the within-group covariance structure is common across groups.\n\nEmpirically, the group mean vectors (in arbitrary units) are observed as\n$$\n\\bar{\\mathbf{y}}_{C} = \\begin{pmatrix} 2.1 \\\\ 3.5 \\\\ 1.8 \\end{pmatrix}, \\quad\n\\bar{\\mathbf{y}}_{T_A} = \\begin{pmatrix} 2.8 \\\\ 4.0 \\\\ 2.2 \\end{pmatrix}, \\quad\n\\bar{\\mathbf{y}}_{T_B} = \\begin{pmatrix} 2.5 \\\\ 3.8 \\\\ 2.0 \\end{pmatrix},\n$$\nwith $n = 12$ per group. The pooled within-group covariance matrix is\n$$\n\\mathbf{S} = \\begin{pmatrix}\n0.16 & 0.10 & 0.08 \\\\\n0.10 & 0.25 & 0.12 \\\\\n0.08 & 0.12 & 0.20\n\\end{pmatrix}.\n$$\n\nInvestigators pose the scientific question: “Is the average of Treatments $A$ and $B$ higher than Control on a composite inflammation score that upweights $\\mathrm{IL}\\text{-}6$?” Encode this scientific question as a contrast across groups and a response weight matrix, then compute the associated hypothesis Sum of Squares and Cross-Products (SSCP) quantity and the corresponding univariate $F$-type test statistic for that scalar effect.\n\nSpecifically:\n- Encode the group contrast as a row vector $\\mathbf{L} \\in \\mathbb{R}^{1 \\times 3}$ acting on the cell means $\\mathbf{B} \\in \\mathbb{R}^{3 \\times 3}$ so that $\\mathbf{L}\\mathbf{B}$ represents “average of $T_A$ and $T_B$ minus $C$”.\n- Encode the composite inflammation score using a response weight matrix $\\mathbf{M} \\in \\mathbb{R}^{3 \\times 1}$ with weights\n$$\n\\mathbf{M} = \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix}.\n$$\n\nAssume the one-way balanced cell-means design with $\\mathbf{X}^{\\top}\\mathbf{X} = n\\,\\mathbf{I}_{3}$. Using these encodings, compute the scalar hypothesis SSCP $H$ corresponding to $(\\mathbf{L}, \\mathbf{M})$ and then compute the univariate $F$-type test statistic for this effect, formed by dividing $H$ by the appropriate error mean square for the weighted response. Report only the final $F$-type test statistic value. Round your answer to four significant figures.", "solution": "The user wants to validate and solve a biostatistics problem involving a one-way Multivariate Analysis of Variance (MANOVA).\n\n### Step 1: Extract Givens\n-   **Groups**: Control ($C$), Treatment $A$ ($T_A$), Treatment $B$ ($T_B$). Number of groups $k=3$.\n-   **Biomarkers**: Interleukin-$6$ ($\\mathrm{IL}\\text{-}6$), C-reactive protein ($\\mathrm{CRP}$), and tumor necrosis factor-$\\alpha$ ($\\mathrm{TNF}\\text{-}\\alpha$). Number of dependent variables $p=3$.\n-   **Sample Size**: Equal group sizes with $n = 12$ replicates per group. Total sample size $N = nk = 36$.\n-   **Model**: One-way balanced cell-means MANOVA model, common within-group covariance structure.\n-   **Group Mean Vectors**:\n    $$ \\bar{\\mathbf{y}}_{C} = \\begin{pmatrix} 2.1 \\\\ 3.5 \\\\ 1.8 \\end{pmatrix}, \\quad \\bar{\\mathbf{y}}_{T_A} = \\begin{pmatrix} 2.8 \\\\ 4.0 \\\\ 2.2 \\end{pmatrix}, \\quad \\bar{\\mathbf{y}}_{T_B} = \\begin{pmatrix} 2.5 \\\\ 3.8 \\\\ 2.0 \\end{pmatrix} $$\n-   **Pooled Within-Group Covariance Matrix**:\n    $$ \\mathbf{S} = \\begin{pmatrix} 0.16 & 0.10 & 0.08 \\\\ 0.10 & 0.25 & 0.12 \\\\ 0.08 & 0.12 & 0.20 \\end{pmatrix} $$\n-   **Group Contrast Vector**: $\\mathbf{L} \\in \\mathbb{R}^{1 \\times 3}$ representing \"average of $T_A$ and $T_B$ minus $C$\".\n-   **Response Weight Matrix**:\n    $$ \\mathbf{M} = \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} $$\n-   **Design Matrix Property**: $\\mathbf{X}^{\\top}\\mathbf{X} = n\\,\\mathbf{I}_{3} = 12\\,\\mathbf{I}_{3}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard application of MANOVA in biostatistics. The experimental design (randomized study), variables (inflammatory biomarkers), and statistical model are common and scientifically sound.\n-   **Well-Posed**: The problem is well-defined. It provides all necessary numerical values and clear instructions to compute a specific test statistic. The pooled covariance matrix $\\mathbf{S}$ is symmetric and can be verified to be positive definite, making it a valid covariance matrix.\n-   **Objective**: The problem is stated in precise, objective mathematical and statistical terms.\n\nThe problem contains no scientific unsoundness, missing information, contradictions, or ambiguities. It is a formal, solvable problem in biostatistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution\n\nThe problem requires the computation of a univariate $F$-type test statistic for a specific hypothesis within a MANOVA framework. The hypothesis involves a contrast across the groups and a linear combination of the response variables. This is a test of a doubly univariate hypothesis.\n\nThe general linear model for MANOVA is given by $\\mathbf{Y} = \\mathbf{X}\\mathbf{B} + \\mathbf{E}$, where $\\mathbf{B}$ is the matrix of true cell means. The estimate of $\\mathbf{B}$ is $\\hat{\\mathbf{B}}$, which is a $k \\times p$ matrix whose rows are the group mean vectors. With the groups ordered as ($C$, $T_A$, $T_B$), we have:\n$$ \\hat{\\mathbf{B}} = \\begin{pmatrix} \\bar{\\mathbf{y}}_{C}^{\\top} \\\\ \\bar{\\mathbf{y}}_{T_A}^{\\top} \\\\ \\bar{\\mathbf{y}}_{T_B}^{\\top} \\end{pmatrix} = \\begin{pmatrix} 2.1 & 3.5 & 1.8 \\\\ 2.8 & 4.0 & 2.2 \\\\ 2.5 & 3.8 & 2.0 \\end{pmatrix} $$\nThe hypothesis is of the form $\\mathbf{LBM} = \\mathbf{0}$.\n\nThe scientific question \"Is the average of Treatments $A$ and $B$ higher than Control\" corresponds to the contrast $\\frac{1}{2}\\mu_{T_A} + \\frac{1}{2}\\mu_{T_B} - \\mu_{C}$. The contrast vector $\\mathbf{L}$ acts on the rows of $\\hat{\\mathbf{B}}$ and is:\n$$ \\mathbf{L} = \\begin{pmatrix} -1 & 0.5 & 0.5 \\end{pmatrix} $$\nThe composite inflammation score is a linear combination of the biomarkers with weights given by the response weight matrix $\\mathbf{M}$:\n$$ \\mathbf{M} = \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} $$\nThe univariate $F$-type test statistic for this specific effect is the ratio of the hypothesis mean square ($MS_H$) to the error mean square ($MS_E$) for the transformed variable.\n$$ F = \\frac{MS_H}{MS_E} $$\nThe hypothesis has $df_H = \\text{rank}(\\mathbf{L}) = 1$ degree of freedom. Thus, $MS_H = H / df_H = H$, where $H$ is the scalar hypothesis sum of squares.\n\nFirst, we compute the hypothesis sum of squares, $H$. This is given by the formula:\n$$ H = \\left( \\mathbf{L}\\hat{\\mathbf{B}}\\mathbf{M} \\right)^{\\top} \\left( \\mathbf{L}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{L}^{\\top} \\right)^{-1} \\left( \\mathbf{L}\\hat{\\mathbf{B}}\\mathbf{M} \\right) $$\nLet $\\hat{\\psi} = \\mathbf{L}\\hat{\\mathbf{B}}\\mathbf{M}$ be the scalar estimate of the effect.\n$$ \\mathbf{L}\\hat{\\mathbf{B}} = \\begin{pmatrix} -1 & 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} 2.1 & 3.5 & 1.8 \\\\ 2.8 & 4.0 & 2.2 \\\\ 2.5 & 3.8 & 2.0 \\end{pmatrix} $$\n$$ \\mathbf{L}\\hat{\\mathbf{B}} = \\begin{pmatrix} (-1)(2.1) + (0.5)(2.8) + (0.5)(2.5) & (-1)(3.5) + (0.5)(4.0) + (0.5)(3.8) & (-1)(1.8) + (0.5)(2.2) + (0.5)(2.0) \\end{pmatrix} $$\n$$ \\mathbf{L}\\hat{\\mathbf{B}} = \\begin{pmatrix} -2.1 + 1.4 + 1.25 & -3.5 + 2.0 + 1.9 & -1.8 + 1.1 + 1.0 \\end{pmatrix} $$\n$$ \\mathbf{L}\\hat{\\mathbf{B}} = \\begin{pmatrix} 0.55 & 0.4 & 0.3 \\end{pmatrix} $$\nNow, we compute $\\hat{\\psi}$:\n$$ \\hat{\\psi} = (\\mathbf{L}\\hat{\\mathbf{B}})\\mathbf{M} = \\begin{pmatrix} 0.55 & 0.4 & 0.3 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} $$\n$$ \\hat{\\psi} = (0.55)(0.5) + (0.4)(0.3) + (0.3)(0.2) = 0.275 + 0.120 + 0.060 = 0.455 $$\nNext, we compute the scalar term involving the design matrix:\n$$ \\mathbf{L}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{L}^{\\top} = \\mathbf{L} \\left(\\frac{1}{n}\\mathbf{I}_{3}\\right) \\mathbf{L}^{\\top} = \\frac{1}{12} \\mathbf{L}\\mathbf{L}^{\\top} $$\n$$ \\mathbf{L}\\mathbf{L}^{\\top} = \\begin{pmatrix} -1 & 0.5 & 0.5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0.5 \\\\ 0.5 \\end{pmatrix} = (-1)^2 + (0.5)^2 + (0.5)^2 = 1 + 0.25 + 0.25 = 1.5 $$\n$$ \\mathbf{L}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{L}^{\\top} = \\frac{1.5}{12} = \\frac{3/2}{12} = \\frac{3}{24} = \\frac{1}{8} $$\nThe inverse is $(\\frac{1}{8})^{-1} = 8$.\nNow, we can compute $H$:\n$$ H = (\\hat{\\psi})^2 \\cdot 8 = (0.455)^2 \\cdot 8 = 0.207025 \\cdot 8 = 1.6562 $$\nThis is the hypothesis sum of squares. Since $df_H = 1$, $MS_H = 1.6562$.\n\nSecond, we compute the error mean square, $MS_E$, for the composite inflammation score. This is the pooled within-group variance of the transformed variable $U = \\mathbf{Y}^\\top \\mathbf{M}$, and is given by:\n$$ MS_E = \\mathbf{M}^{\\top}\\mathbf{S}\\mathbf{M} $$\nWe have:\n$$ \\mathbf{S} = \\begin{pmatrix} 0.16 & 0.10 & 0.08 \\\\ 0.10 & 0.25 & 0.12 \\\\ 0.08 & 0.12 & 0.20 \\end{pmatrix}, \\quad \\mathbf{M} = \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} $$\nFirst, compute $\\mathbf{S}\\mathbf{M}$:\n$$ \\mathbf{S}\\mathbf{M} = \\begin{pmatrix} (0.16)(0.5) + (0.10)(0.3) + (0.08)(0.2) \\\\ (0.10)(0.5) + (0.25)(0.3) + (0.12)(0.2) \\\\ (0.08)(0.5) + (0.12)(0.3) + (0.20)(0.2) \\end{pmatrix} = \\begin{pmatrix} 0.08 + 0.03 + 0.016 \\\\ 0.05 + 0.075 + 0.024 \\\\ 0.04 + 0.036 + 0.04 \\end{pmatrix} = \\begin{pmatrix} 0.126 \\\\ 0.149 \\\\ 0.116 \\end{pmatrix} $$\nThen, compute $MS_E = \\mathbf{M}^{\\top}(\\mathbf{S}\\mathbf{M})$:\n$$ MS_E = \\begin{pmatrix} 0.5 & 0.3 & 0.2 \\end{pmatrix} \\begin{pmatrix} 0.126 \\\\ 0.149 \\\\ 0.116 \\end{pmatrix} $$\n$$ MS_E = (0.5)(0.126) + (0.3)(0.149) + (0.2)(0.116) = 0.063 + 0.0447 + 0.0232 = 0.1309 $$\nFinally, we compute the $F$-statistic:\n$$ F = \\frac{MS_H}{MS_E} = \\frac{1.6562}{0.1309} \\approx 12.652406417... $$\nRounding the result to four significant figures, we get $12.65$.", "answer": "$$\\boxed{12.65}$$", "id": "4931274"}, {"introduction": "Once a hypothesis is formulated, we need to quantify the evidence from our data. This exercise takes you into the computational core of MANOVA, starting from the hypothesis ($H$) and error ($E$) sum-of-squares-and-products matrices. You will calculate Wilks' Lambda, one of the most common MANOVA test statistics, and then use the Bartlett correction to find an approximate test value, connecting raw variation to a formal hypothesis test [@problem_id:4931290].", "problem": "A biostatistician is comparing the mean vectors of circulating biomarker outcomes across two treatment groups using Multivariate Analysis of Variance (MANOVA). Group $1$ has $n_1 = 12$ participants and Group $2$ has $n_2 = 10$ participants, so the total sample size is $N = 22$, the number of outcomes is $p = 3$, and the number of groups is $g = 2$. The sums of squares and cross-products matrices for the within-group (error) variation and the between-group (hypothesis) variation, computed from centered biomarker vectors, are given by\n$$\nE \\;=\\; \\begin{pmatrix}\n120 & 15 & 10 \\\\\n15 & 100 & 20 \\\\\n10 & 20 & 80\n\\end{pmatrix},\n\\qquad\nH \\;=\\; \\begin{pmatrix}\n30 & 12 & 6 \\\\\n12 & 25 & 8 \\\\\n6 & 8 & 20\n\\end{pmatrix}.\n$$\nUsing core definitions for MANOVA, first compute Wilks' lambda $\\Lambda$ from $E$ and $H$. Then, using the Bartlett correction based on large-sample theory, obtain the corresponding approximately chi-square distributed test statistic for the null hypothesis that the two mean vectors are equal. Express the final answer as the Bartlett-corrected chi-square statistic, rounded to four significant figures. No units are required.", "solution": "The user has provided a problem in the field of biostatistics, specifically concerning a Multivariate Analysis of Variance (MANOVA). The task is to compute a test statistic for the equality of mean vectors between two groups.\n\nThe initial step is to validate the problem statement.\n\n### Step 1: Extract Givens\n- Number of participants in Group $1$: $n_1 = 12$\n- Number of participants in Group $2$: $n_2 = 10$\n- Total sample size: $N = n_1 + n_2 = 12 + 10 = 22$\n- Number of dependent variables (outcomes): $p = 3$\n- Number of groups: $g = 2$\n- Within-group (error) sums of squares and cross-products (SSCP) matrix:\n$$\nE = \\begin{pmatrix}\n120 & 15 & 10 \\\\\n15 & 100 & 20 \\\\\n10 & 20 & 80\n\\end{pmatrix}\n$$\n- Between-group (hypothesis) sums of squares and cross-products (SSCP) matrix:\n$$\nH = \\begin{pmatrix}\n30 & 12 & 6 \\\\\n12 & 25 & 8 \\\\\n6 & 8 & 20\n\\end{pmatrix}\n$$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a standard application of MANOVA, a fundamental technique in multivariate statistics. The use of SSCP matrices $E$ and $H$ to test for differences in mean vectors is a core part of this methodology. The context of comparing biomarker outcomes in biostatistics is appropriate and realistic. The matrices $E$ and $H$ are symmetric, as required for SSCP matrices. They are also positive definite, a condition typically met in practice when the number of subjects exceeds the number of variables within each group. The problem is scientifically sound.\n- **Well-Posed**: The problem asks for the calculation of a specific, well-defined test statistic (Wilks' lambda followed by Bartlett's correction) and provides all the necessary numerical data and definitions to do so. The solution is unique and stable.\n- **Objective**: The problem is stated using precise, standard statistical terminology and is free of any subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard, well-posed problem in applied statistics. I will now proceed with the solution.\n\nThe solution involves two main steps: first, computing Wilks' lambda, $\\Lambda$, and second, using this value to compute the Bartlett-corrected chi-square statistic.\n\n**Step 1: Compute Wilks' Lambda ($\\Lambda$)**\n\nWilks' lambda is defined as the ratio of the determinant of the error SSCP matrix, $E$, to the determinant of the total SSCP matrix, $T$, where $T = E + H$.\n$$\n\\Lambda = \\frac{\\det(E)}{\\det(T)} = \\frac{\\det(E)}{\\det(E+H)}\n$$\n\nFirst, we compute the total SSCP matrix, $T$:\n$$\nT = E + H = \\begin{pmatrix} 120 & 15 & 10 \\\\ 15 & 100 & 20 \\\\ 10 & 20 & 80 \\end{pmatrix} + \\begin{pmatrix} 30 & 12 & 6 \\\\ 12 & 25 & 8 \\\\ 6 & 8 & 20 \\end{pmatrix} = \\begin{pmatrix} 150 & 27 & 16 \\\\ 27 & 125 & 28 \\\\ 16 & 28 & 100 \\end{pmatrix}\n$$\n\nNext, we compute the determinant of the error matrix, $\\det(E)$:\n$$\n\\det(E) = 120 \\begin{vmatrix} 100 & 20 \\\\ 20 & 80 \\end{vmatrix} - 15 \\begin{vmatrix} 15 & 20 \\\\ 10 & 80 \\end{vmatrix} + 10 \\begin{vmatrix} 15 & 100 \\\\ 10 & 20 \\end{vmatrix}\n$$\n$$\n\\det(E) = 120(100 \\cdot 80 - 20 \\cdot 20) - 15(15 \\cdot 80 - 10 \\cdot 20) + 10(15 \\cdot 20 - 10 \\cdot 100)\n$$\n$$\n\\det(E) = 120(8000 - 400) - 15(1200 - 200) + 10(300 - 1000)\n$$\n$$\n\\det(E) = 120(7600) - 15(1000) + 10(-700)\n$$\n$$\n\\det(E) = 912000 - 15000 - 7000 = 890000\n$$\n\nNow, we compute the determinant of the total matrix, $\\det(T)$:\n$$\n\\det(T) = 150 \\begin{vmatrix} 125 & 28 \\\\ 28 & 100 \\end{vmatrix} - 27 \\begin{vmatrix} 27 & 28 \\\\ 16 & 100 \\end{vmatrix} + 16 \\begin{vmatrix} 27 & 125 \\\\ 16 & 28 \\end{vmatrix}\n$$\n$$\n\\det(T) = 150(125 \\cdot 100 - 28 \\cdot 28) - 27(27 \\cdot 100 - 16 \\cdot 28) + 16(27 \\cdot 28 - 16 \\cdot 125)\n$$\n$$\n\\det(T) = 150(12500 - 784) - 27(2700 - 448) + 16(756 - 2000)\n$$\n$$\n\\det(T) = 150(11716) - 27(2252) + 16(-1244)\n$$\n$$\n\\det(T) = 1757400 - 60804 - 19904 = 1757400 - 80708 = 1676692\n$$\n\nFinally, we calculate Wilks' lambda:\n$$\n\\Lambda = \\frac{\\det(E)}{\\det(T)} = \\frac{890000}{1676692} \\approx 0.530804\n$$\n\n**Step 2: Compute the Bartlett-Corrected Chi-Square Statistic**\n\nFor large samples, a transformation of Wilks' lambda can be used to approximate a chi-square distribution. The Bartlett correction provides a more accurate approximation. The test statistic, which we will denote as $\\chi^2_{\\text{Bartlett}}$, is given by:\n$$\n\\chi^2_{\\text{Bartlett}} = -\\left( N - 1 - \\frac{p+g}{2} \\right) \\ln(\\Lambda)\n$$\nThis statistic is approximately distributed as a chi-square variable with $p(g-1)$ degrees of freedom.\n\nLet's compute the multiplier term first. We are given $N=22$, $p=3$, and $g=2$:\n$$\nM = N - 1 - \\frac{p+g}{2} = 22 - 1 - \\frac{3+2}{2} = 21 - \\frac{5}{2} = 21 - 2.5 = 18.5\n$$\n\nNow we can compute the test statistic:\n$$\n\\chi^2_{\\text{Bartlett}} = -M \\ln(\\Lambda) = -18.5 \\times \\ln\\left(\\frac{890000}{1676692}\\right)\n$$\n$$\n\\chi^2_{\\text{Bartlett}} \\approx -18.5 \\times \\ln(0.530804005)\n$$\n$$\n\\chi^2_{\\text{Bartlett}} \\approx -18.5 \\times (-0.6333935)\n$$\n$$\n\\chi^2_{\\text{Bartlett}} \\approx 11.71778\n$$\n\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\chi^2_{\\text{Bartlett}} \\approx 11.72\n$$\nThe degrees of freedom for this test are $df = p(g-1) = 3(2-1) = 3$. This information is not required for the final answer but is relevant for interpreting the statistic.\n\nThe final computed value for the Bartlett-corrected chi-square statistic is $11.72$.", "answer": "$$\n\\boxed{11.72}\n$$", "id": "4931290"}, {"introduction": "Statistical models are only as reliable as the data they are built on, and MANOVA is no exception. This practical exercise equips you with a crucial diagnostic skill: identifying multivariate outliers that could distort your results. You will use the Mahalanobis distance to measure how unusual each data point is and see firsthand how removing outliers can impact the final test statistics, leading to more robust scientific conclusions [@problem_id:4931275].", "problem": "You are given a one-way Multivariate Analysis of Variance (MANOVA) setting with a multivariate response and categorical groups. Starting from the core definitions of linear models and multivariate covariance structure, implement a diagnostic based on Mahalanobis distances of residuals to identify multivariate outliers, and quantify their influence on the error Sum of Squares and Cross Products (SSCP) matrix and standard MANOVA test statistics.\n\nFundamental base and definitions:\n- Under the one-way MANOVA model, each observation vector is denoted by $Y_{ij} \\in \\mathbb{R}^p$, where $j \\in \\{1,\\dots,g\\}$ indexes the group and $i \\in \\{1,\\dots,n_j\\}$ indexes the observation within group $j$. The total sample size is $N = \\sum_{j=1}^g n_j$ and the response dimension is $p$.\n- Let the group mean vectors be $\\mu_j \\in \\mathbb{R}^p$ and the overall mean vector be $\\bar{\\mu} \\in \\mathbb{R}^p$. Residuals are defined as $R_{ij} = Y_{ij} - \\mu_j$.\n- The pooled error SSCP matrix $E \\in \\mathbb{R}^{p \\times p}$ is constructed from residuals, and the hypothesis SSCP matrix $H \\in \\mathbb{R}^{p \\times p}$ is constructed from group mean differences relative to the overall mean.\n- The pooled residual covariance matrix is defined as $S_e = E / (N - g)$.\n- The squared Mahalanobis distance of residual $R_{ij}$ with respect to the pooled residual covariance is $D^2_{ij}$, which is used to detect outliers relative to a threshold derived from the chi-square distribution with $p$ degrees of freedom.\n\nTasks:\n1. For each test case, compute residuals $R_{ij}$ using group sample means, compute the pooled error SSCP matrix $E$, and the hypothesis SSCP matrix $H$.\n2. Using the pooled residual covariance matrix $S_e$, compute the squared Mahalanobis distances $D^2_{ij}$ for all observations. Flag an observation as an outlier if its $D^2_{ij}$ exceeds the chi-square upper quantile at level $(1-\\alpha)$ with $p$ degrees of freedom.\n3. Compute the following standard MANOVA test statistics both before and after removing flagged outliers, using $E$ and $H$ built from the current dataset:\n   - Wilks' statistic (also known as Wilks' Lambda).\n   - Pillai's trace.\n   - Hotelling–Lawley trace.\n   - Roy's largest root.\n4. For each test case, output the number of flagged outliers and the changes in the above four test statistics after removing flagged outliers, reported as the value after removal minus the value before removal.\n\nScientific realism requirements:\n- All computations must be grounded in the linear model structure and multivariate covariance definitions, with no reliance on shortcut formulas in the problem statement.\n\nTest suite:\nUse the following three test cases. Each test case specifies the response matrix $Y$ as an ordered list of vectors and the corresponding group labels as an ordered list of integers. The significance level $\\alpha$ is provided for the outlier thresholding.\n\n- Test Case 1 (happy path, $p=2$, three groups, no outliers expected, $\\alpha=0.01$):\n  - $Y$ consists of $N=15$ observations in $\\mathbb{R}^2$ ordered by group as follows:\n    - Group $0$ ($n_0=5$):\n      - $(0.1, -0.2)$, $(-0.3, 0.2)$, $(0.2, 0.1)$, $(-0.1, -0.1)$, $(0.05, 0.0)$\n    - Group $1$ ($n_1=5$):\n      - $(2.1, 1.0)$, $(1.9, 1.1)$, $(2.2, 0.9)$, $(2.0, 1.2)$, $(2.05, 1.05)$\n    - Group $2$ ($n_2=5$):\n      - $(-1.1, 2.9)$, $(-0.9, 3.1)$, $(-1.2, 3.0)$, $(-0.8, 3.2)$, $(-1.0, 2.8)$\n  - Group labels (ordered to align with $Y$): $[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]$\n  - $\\alpha = 0.01$\n\n- Test Case 2 (outlier present, $p=2$, three groups, $\\alpha=0.01$):\n  - $Y$ consists of $N=16$ observations in $\\mathbb{R}^2$ ordered by group as follows:\n    - Group $0$ ($n_0=5$):\n      - $(0.1, -0.2)$, $(-0.3, 0.2)$, $(0.2, 0.1)$, $(-0.1, -0.1)$, $(0.05, 0.0)$\n    - Group $1$ ($n_1=6$): the first five as in Test Case 1, plus an outlier\n      - $(2.1, 1.0)$, $(1.9, 1.1)$, $(2.2, 0.9)$, $(2.0, 1.2)$, $(2.05, 1.05)$, $(6.0, -3.0)$\n    - Group $2$ ($n_2=5$):\n      - $(-1.1, 2.9)$, $(-0.9, 3.1)$, $(-1.2, 3.0)$, $(-0.8, 3.2)$, $(-1.0, 2.8)$\n  - Group labels (ordered to align with $Y$): $[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]$\n  - $\\alpha = 0.01$\n\n- Test Case 3 (different dimensionality, $p=3$, two groups, no outliers expected, $\\alpha=0.05$):\n  - $Y$ consists of $N=11$ observations in $\\mathbb{R}^3$ ordered by group as follows:\n    - Group $0$ ($n_0=6$):\n      - $(0.2, -0.1, 0.05)$, $(-0.2, 0.1, -0.1)$, $(0.1, 0.0, 0.2)$, $(0.0, -0.2, 0.1)$, $(0.15, 0.05, -0.05)$, $(-0.1, 0.2, 0.0)$\n    - Group $1$ ($n_1=5$):\n      - $(1.1, 0.9, -1.1)$, $(0.9, 1.2, -0.8)$, $(1.2, 1.0, -1.2)$, $(1.0, 1.1, -1.0)$, $(1.05, 0.95, -1.05)$\n  - Group labels (ordered to align with $Y$): $[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]$\n  - $\\alpha = 0.05$\n\nRequired final output format:\n- For each test case, compute:\n  - the integer number of flagged outliers,\n  - four floating-point values equal to the differences in Wilks' statistic, Pillai's trace, Hotelling–Lawley trace, and Roy's largest root, computed as the value after removing flagged outliers minus the value before removal.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list in the order $[ \\text{count}, \\Delta\\text{Wilks}, \\Delta\\text{Pillai}, \\Delta\\text{HL}, \\Delta\\text{Roy} ]$. For example: $[[c_1, d_{1,1}, d_{1,2}, d_{1,3}, d_{1,4}],[c_2, d_{2,1}, d_{2,2}, d_{2,3}, d_{2,4}],[c_3, d_{3,1}, d_{3,2}, d_{3,3}, d_{3,4}]]$.", "solution": "The problem requires the implementation of an outlier detection diagnostic for a one-way Multivariate Analysis of Variance (MANOVA) model. This involves calculating Mahalanobis distances for residuals, identifying outliers based on a chi-square distribution threshold, and quantifying their influence on standard MANOVA test statistics. The solution will be developed from the fundamental definitions of the linear model and covariance structures.\n\nFirst, we establish the mathematical framework for the one-way MANOVA.\nLet $Y_{ij} \\in \\mathbb{R}^p$ be the $p$-dimensional response vector for the $i$-th subject in the $j$-th group, where $i \\in \\{1, \\dots, n_j\\}$ and $j \\in \\{1, \\dots, g\\}$. The total number of observations is $N = \\sum_{j=1}^g n_j$.\n\nThe model assumes that $Y_{ij} = \\mu_j + \\epsilon_{ij}$, where $\\mu_j$ is the mean vector for group $j$ and $\\epsilon_{ij}$ are independent random error vectors with mean $0$ and a common covariance matrix $\\Sigma$. In practice, the true means $\\mu_j$ and the overall mean $\\bar{\\mu}$ are unknown and are estimated from the data.\n\nThe sample estimates are:\n- Group mean vector: $\\bar{Y}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} Y_{ij}$\n- Overall mean vector: $\\bar{Y} = \\frac{1}{N} \\sum_{j=1}^g \\sum_{i=1}^{n_j} Y_{ij} = \\frac{1}{N} \\sum_{j=1}^g n_j \\bar{Y}_j$\n\nFrom these, we construct two fundamental matrices, the Sum of Squares and Cross-Products (SSCP) matrices:\n1.  The **Hypothesis SSCP matrix ($H$)** quantifies the variation between groups:\n    $$H = \\sum_{j=1}^g n_j (\\bar{Y}_j - \\bar{Y})(\\bar{Y}_j - \\bar{Y})^T$$\n2.  The **Error SSCP matrix ($E$)** quantifies the variation within groups:\n    $$E = \\sum_{j=1}^g \\sum_{i=1}^{n_j} (Y_{ij} - \\bar{Y}_j)(Y_{ij} - \\bar{Y}_j)^T$$\nThe matrix $E$ is an estimator for $(N-g)\\Sigma$. The total variation is captured by the Total SSCP matrix, $T = H + E$.\n\nFor outlier detection, we analyze the residuals, $R_{ij} = Y_{ij} - \\bar{Y}_j$. An unbiased estimate of the common error covariance matrix $\\Sigma$ is the pooled residual covariance matrix, $S_e$:\n$$S_e = \\frac{E}{N-g}$$\nThe squared Mahalanobis distance of a residual $R_{ij}$ measures its distance from the center (mean residual, which is $0$) adjusted for the covariance structure:\n$$D^2_{ij} = (Y_{ij} - \\bar{Y}_j)^T S_e^{-1} (Y_{ij} - \\bar{Y}_j)$$\nUnder the assumption of multivariate normality of the errors, $D^2_{ij}$ approximately follows a chi-square distribution with $p$ degrees of freedom. An observation $Y_{ij}$ is flagged as a potential outlier if its corresponding $D^2_{ij}$ is unusually large. We use the upper $(1-\\alpha)$ quantile of the $\\chi^2_p$ distribution as a threshold:\n$$\\text{Outlier if } D^2_{ij} > \\chi^2_{p, 1-\\alpha}$$\n\nThe standard MANOVA test statistics are used to test the null hypothesis $H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_g$. These statistics are functions of the eigenvalues of the matrix $E^{-1}H$. Let the non-zero eigenvalues be $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_s$, where $s = \\min(p, g-1)$. The four statistics are:\n1.  **Wilks' Lambda**: $\\Lambda = \\det(E) / \\det(E+H) = \\prod_{k=1}^s \\frac{1}{1+\\lambda_k}$. It represents the proportion of total variance not explained by group differences. Values closer to $0$ indicate strong group differences.\n2.  **Pillai's Trace**: $V = \\text{tr}((E+H)^{-1}H) = \\sum_{k=1}^s \\frac{\\lambda_k}{1+\\lambda_k}$. It represents the proportion of explained variance. Values closer to $s$ indicate strong group differences.\n3.  **Hotelling-Lawley Trace**: $T_{HL} = \\text{tr}(E^{-1}H) = \\sum_{k=1}^s \\lambda_k$. It is the sum of the eigenvalues.\n4.  **Roy's Largest Root**: $\\Theta = \\lambda_1$. It is the largest eigenvalue, focusing on the maximal group difference in a single dimension.\n\nThe algorithm to solve the problem proceeds in two main stages for each test case:\n\n**Stage 1: Analysis on the Full Dataset**\n1.  Given the data matrix $Y$ and group labels, calculate $p$, $g$, $N$, and $n_j$ for each group.\n2.  Compute the sample group means $\\bar{Y}_j$ and the overall sample mean $\\bar{Y}$.\n3.  Construct the SSCP matrices $H$ and $E$ using their definitions.\n4.  Calculate the eigenvalues of $E^{-1}H$ and compute the four MANOVA test statistics (Wilks, Pillai, Hotelling-Lawley, Roy). These are the \"before\" values.\n5.  Compute the pooled covariance matrix $S_e = E / (N-g)$ and its inverse $S_e^{-1}$.\n6.  For each observation $Y_{ij}$, calculate the squared Mahalanobis distance $D^2_{ij} = (Y_{ij} - \\bar{Y}_j)^T S_e^{-1} (Y_{ij} - \\bar{Y}_j)$.\n7.  Compare each $D^2_{ij}$ to the critical value $\\chi^2_{p, 1-\\alpha}$ to identify and count the outliers.\n\n**Stage 2: Analysis on the Reduced Dataset and Comparison**\n1.  If no outliers are flagged in Stage 1, the change in statistics is $0$.\n2.  If outliers are found, create a new dataset by removing all flagged observations.\n3.  Recalculate all quantities for this reduced dataset: new sample sizes ($N'$, $n_j'$), new means ($\\bar{Y}_j'$, $\\bar{Y}'$), and new SSCP matrices ($H'$, $E'$).\n4.  Compute the four MANOVA test statistics again using the new matrices. These are the \"after\" values.\n5.  For each of the four statistics, calculate the change as (value after removal) - (value before removal).\n\nThis procedure provides the number of outliers and a quantitative measure of their influence on the MANOVA test results, thereby satisfying all requirements of the problem.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to solve the MANOVA outlier diagnostic problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"Y\": np.array([\n                [0.1, -0.2], [-0.3, 0.2], [0.2, 0.1], [-0.1, -0.1], [0.05, 0.0],\n                [2.1, 1.0], [1.9, 1.1], [2.2, 0.9], [2.0, 1.2], [2.05, 1.05],\n                [-1.1, 2.9], [-0.9, 3.1], [-1.2, 3.0], [-0.8, 3.2], [-1.0, 2.8]\n            ]),\n            \"labels\": np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]),\n            \"alpha\": 0.01\n        },\n        {\n            \"Y\": np.array([\n                [0.1, -0.2], [-0.3, 0.2], [0.2, 0.1], [-0.1, -0.1], [0.05, 0.0],\n                [2.1, 1.0], [1.9, 1.1], [2.2, 0.9], [2.0, 1.2], [2.05, 1.05], [6.0, -3.0],\n                [-1.1, 2.9], [-0.9, 3.1], [-1.2, 3.0], [-0.8, 3.2], [-1.0, 2.8]\n            ]),\n            \"labels\": np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]),\n            \"alpha\": 0.01\n        },\n        {\n            \"Y\": np.array([\n                [0.2, -0.1, 0.05], [-0.2, 0.1, -0.1], [0.1, 0.0, 0.2], [0.0, -0.2, 0.1],\n                [0.15, 0.05, -0.05], [-0.1, 0.2, 0.0],\n                [1.1, 0.9, -1.1], [0.9, 1.2, -0.8], [1.2, 1.0, -1.2], [1.0, 1.1, -1.0],\n                [1.05, 0.95, -1.05]\n            ]),\n            \"labels\": np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]),\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Y_data, labels, alpha = case[\"Y\"], case[\"labels\"], case[\"alpha\"]\n        p = Y_data.shape[1]\n\n        # --- Stage 1: Analysis on Full Dataset ---\n        metrics_before = _calculate_manova_metrics(Y_data, labels)\n        stats_before = [\n            metrics_before['wilks'], metrics_before['pillai'],\n            metrics_before['hl'], metrics_before['roy']\n        ]\n        \n        N, g = metrics_before['N'], metrics_before['g']\n        E = metrics_before['E']\n        \n        # Outlier Detection\n        outlier_indices = []\n        if N - g > p:\n            S_e = E / (N - g)\n            try:\n                S_e_inv = np.linalg.inv(S_e)\n                threshold = chi2.ppf(1 - alpha, df=p)\n                \n                for i in range(N):\n                    obs = Y_data[i, :]\n                    group_label = labels[i]\n                    group_mean = metrics_before['group_means'][group_label]\n                    residual = obs - group_mean\n                    d_squared = residual.T @ S_e_inv @ residual\n                    if d_squared > threshold:\n                        outlier_indices.append(i)\n            except np.linalg.LinAlgError:\n                # S_e is singular, cannot perform outlier detection.\n                pass\n\n        num_outliers = len(outlier_indices)\n\n        # --- Stage 2: Analysis on Reduced Dataset ---\n        if num_outliers > 0:\n            Y_after = np.delete(Y_data, outlier_indices, axis=0)\n            labels_after = np.delete(labels, outlier_indices, axis=0)\n            metrics_after = _calculate_manova_metrics(Y_after, labels_after)\n            stats_after = [\n                metrics_after['wilks'], metrics_after['pillai'],\n                metrics_after['hl'], metrics_after['roy']\n            ]\n        else:\n            stats_after = stats_before\n\n        delta_stats = [s_after - s_before for s_before, s_after in zip(stats_before, stats_after)]\n        \n        results.append([num_outliers] + delta_stats)\n\n    # Format the final output string without spaces within lists\n    inner_parts = []\n    for res_list in results:\n        inner_str = '[' + ','.join(f\"{x:.10f}\" if isinstance(x, float) else str(x) for x in res_list) + ']'\n        inner_parts.append(inner_str)\n    final_str = '[' + ','.join(inner_parts) + ']'\n    print(final_str)\n\ndef _calculate_manova_metrics(Y, labels):\n    \"\"\"\n    Helper function to calculate MANOVA metrics for a given dataset.\n    \"\"\"\n    if Y.shape[0] == 0:\n        p = Y.shape[1] if Y.ndim > 1 else 0\n        return {'wilks': 0.0, 'pillai': 0.0, 'hl': 0.0, 'roy': 0.0,\n                'E': np.zeros((p, p)), 'group_means': {}, 'N': 0, 'g': 0}\n\n    p = Y.shape[1]\n    N = Y.shape[0]\n    unique_labels = sorted(list(np.unique(labels)))\n    g = len(unique_labels)\n    \n    group_means = {label: Y[labels == label].mean(axis=0) for label in unique_labels}\n    \n    # Error SSCP matrix E\n    E = np.zeros((p, p))\n    for label in unique_labels:\n        group_data = Y[labels == label]\n        if group_data.shape[0] > 0:\n            residuals = group_data - group_means[label]\n            E += residuals.T @ residuals\n\n    # If N-g  p, E is singular, cannot compute test statistics meaningfully.\n    # Return E for Mahalanobis dist calculation but stats as NaN/default.\n    if N - g  p or g = 1:\n        stats = {'wilks': 1.0, 'pillai': 0.0, 'hl': 0.0, 'roy': 0.0}\n        return {**stats, 'E': E, 'group_means': group_means, 'N': N, 'g': g}\n\n    # Hypothesis SSCP matrix H\n    overall_mean = Y.mean(axis=0)\n    H = np.zeros((p, p))\n    for label in unique_labels:\n        n_j = np.sum(labels == label)\n        mean_diff = group_means[label] - overall_mean\n        H += n_j * np.outer(mean_diff, mean_diff)\n    \n    try:\n        E_inv_H = np.linalg.inv(E) @ H\n        eigenvalues = np.linalg.eigvals(E_inv_H)\n    except np.linalg.LinAlgError:\n        stats = {'wilks': np.nan, 'pillai': np.nan, 'hl': np.nan, 'roy': np.nan}\n        return {**stats, 'E': E, 'group_means': group_means, 'N': N, 'g': g}\n\n    eigenvalues = np.sort(np.real(eigenvalues))[::-1]\n    eigenvalues[eigenvalues  0] = 0.0\n    \n    s = min(p, g - 1)\n    # Use only the s largest eigenvalues\n    eigvals = eigenvalues[:s]\n    \n    stats = {\n        'wilks': np.prod(1 / (1 + eigvals)),\n        'pillai': np.sum(eigvals / (1 + eigvals)),\n        'hl': np.sum(eigvals),\n        'roy': eigvals[0] if len(eigvals) > 0 else 0.0\n    }\n    \n    return {**stats, 'E': E, 'group_means': group_means, 'N': N, 'g': g}\n\n\nsolve()\n```", "id": "4931275"}]}