{"hands_on_practices": [{"introduction": "At the heart of Principal Component Analysis lies the eigen-decomposition of the covariance matrix, which unlocks the variance structure of the data. This first practice provides a direct look at this core mechanism. By starting with a defined covariance matrix representing two correlated measurements, you will use its eigenvalues to quantify the proportion of total variance captured by the first and most significant principal component [@problem_id:1946278]. This exercise solidifies the fundamental link between eigenvalues and the concept of explained variance.", "problem": "An autonomous environmental monitoring drone uses a pair of identical sensors to measure atmospheric pressure. Let the readings of the two sensors, after being centered by subtracting their long-term average, be represented by the random variables $X_1$ and $X_2$.\n\nThe joint behavior of these readings is described by a bivariate random vector $(X_1, X_2)$ with a covariance matrix $\\Sigma$. Because the sensors are of the same type and subject to similar environmental fluctuations, they have the same variance, $\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma^2$, for some constant $\\sigma > 0$. Their readings are also correlated, with a correlation coefficient $\\rho$ such that $0 < \\rho < 1$. The covariance matrix is therefore given by:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma^2 & \\rho\\sigma^2 \\\\ \\rho\\sigma^2 & \\sigma^2 \\end{pmatrix}\n$$\nTo reduce data redundancy and identify the primary axis of variation, the engineering team applies Principal Component Analysis (PCA). PCA transforms the original correlated variables $(X_1, X_2)$ into a new set of uncorrelated variables, known as principal components. The first principal component is defined as the linear combination of $X_1$ and $X_2$ that captures the maximum possible variance.\n\nDetermine the proportion of the total variance in the data that is explained by the first principal component. Express your answer as a symbolic expression in terms of $\\rho$.", "solution": "We are given a centered bivariate random vector with covariance matrix\n$$\n\\Sigma=\\begin{pmatrix}\\sigma^{2} & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}\\end{pmatrix},\n$$\nwhere $0<\\rho<1$ and $\\sigma>0$. In PCA, the variances of the principal components are the eigenvalues of the covariance matrix. The proportion of total variance explained by the first principal component equals its eigenvalue divided by the total variance, which is the trace of $\\Sigma$.\n\nFirst, compute the eigenvalues of $\\Sigma$ by solving the characteristic equation\n$$\n\\det(\\Sigma-\\lambda I)=0.\n$$\nWe have\n$$\n\\det\\begin{pmatrix}\\sigma^{2}-\\lambda & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}-\\lambda\\end{pmatrix}\n=(\\sigma^{2}-\\lambda)^{2}-(\\rho\\sigma^{2})^{2}=0.\n$$\nThus,\n$$\n(\\sigma^{2}-\\lambda)^{2}=\\rho^{2}\\sigma^{4}\n\\quad\\Longrightarrow\\quad\n\\sigma^{2}-\\lambda=\\pm\\rho\\sigma^{2}\n\\quad\\Longrightarrow\\quad\n\\lambda=\\sigma^{2}(1\\pm\\rho).\n$$\nSince $0<\\rho<1$, the largest eigenvalue is\n$$\n\\lambda_{1}=\\sigma^{2}(1+\\rho).\n$$\nThe total variance equals the trace of $\\Sigma$,\n$$\n\\operatorname{tr}(\\Sigma)=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2},\n$$\nwhich also equals the sum of the eigenvalues $\\sigma^{2}(1+\\rho)+\\sigma^{2}(1-\\rho)=2\\sigma^{2}$. Therefore, the proportion of total variance explained by the first principal component is\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(\\Sigma)}=\\frac{\\sigma^{2}(1+\\rho)}{2\\sigma^{2}}=\\frac{1+\\rho}{2}.\n$$", "answer": "$$\\boxed{\\frac{1+\\rho}{2}}$$", "id": "1946278"}, {"introduction": "Principal components are more than just abstract mathematical results; they have a clear geometric interpretation as the axes of greatest variation within a dataset. This exercise is designed to build your intuition by working in reverse. Given the directions of the principal components (eigenvectors) and the relative amount of variance they explain (eigenvalues), you will deduce the most plausible shape and orientation of the data cloud [@problem_id:1383893], connecting the algebra of PCA to its visual meaning.", "problem": "A data scientist is performing Principal Component Analysis (PCA) on a large, two-dimensional dataset where each data point is represented by a vector $[x, y]^T$. After centering the data by subtracting the mean, the $2 \\times 2$ sample covariance matrix is computed. The analysis reveals that the eigenvectors of this covariance matrix are $\\mathbf{v}_1 = [1, 1]^T$ and $\\mathbf{v}_2 = [-1, 1]^T$. Furthermore, it is found that the eigenvalue corresponding to $\\mathbf{v}_1$ is substantially larger than the eigenvalue corresponding to $\\mathbf{v}_2$.\n\nBased on this information, which of the following statements provides the most plausible description of the geometric distribution of the centered data points in the $xy$-plane?\n\nA. The data points are distributed in a roughly circular cloud centered at the origin.\n\nB. The data points form an elongated cloud primarily aligned with the y-axis.\n\nC. The data points form an elongated cloud whose major axis is aligned with the line $y = -x$.\n\nD. The data points form an elongated cloud whose major axis is aligned with the line $y = x$.\n\nE. The data points are concentrated in two distinct clusters, one in the first quadrant and one in the third quadrant.", "solution": "Let the centered data points be $\\{\\mathbf{x}_{i}\\}_{i=1}^{n}$ with $\\mathbf{x}_{i} \\in \\mathbb{R}^{2}$ and sample covariance matrix\n$$\nS=\\frac{1}{n-1}\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{T}.\n$$\nPrincipal Component Analysis diagonalizes $S$ via eigen-decomposition. The eigenvectors $\\{\\mathbf{v}_{k}\\}$ and eigenvalues $\\{\\lambda_{k}\\}$ satisfy\n$$\nS\\mathbf{v}_{k}=\\lambda_{k}\\mathbf{v}_{k},\n$$\nwhere each $\\lambda_{k}$ equals the variance of the data projected onto direction $\\mathbf{v}_{k}$. The geometric shape of the centered point cloud is well approximated by an ellipse whose principal axes are aligned with the eigenvectors of $S$, with axis lengths proportional to $\\sqrt{\\lambda_{k}}$. The largest eigenvalue determines the major axis direction.\n\nWe are given eigenvectors $\\mathbf{v}_{1}=[1,1]^{T}$ and $\\mathbf{v}_{2}=[-1,1]^{T}$ with the eigenvalue corresponding to $\\mathbf{v}_{1}$ substantially larger than that corresponding to $\\mathbf{v}_{2}$. The direction $\\mathbf{v}_{1}=[1,1]^{T}$ corresponds to the line $y=x$ because for a direction vector $[a,b]^{T}$, the aligned line has slope $b/a$. Thus $[1,1]^{T}$ yields $y=(1/1)x=x$. Similarly, $\\mathbf{v}_{2}=[-1,1]^{T}$ corresponds to $y=-x$. Since $\\lambda_{1}\\gg\\lambda_{2}$, the variance is much larger along $\\mathbf{v}_{1}$, so the major axis of the data cloud aligns with $y=x$.\n\nTherefore, the most plausible description is that the centered data form an elongated cloud whose major axis is aligned with the line $y=x$, which corresponds to option D. Options A and B are inconsistent with the eigenstructure (A would require approximately equal eigenvalues; B would require a principal direction aligned with the y-axis), and option C would require the largest eigenvalue to be associated with $\\mathbf{v}_{2}$. Option E cannot be inferred from covariance alone and is not the most direct implication of the given PCA results.", "answer": "$$\\boxed{D}$$", "id": "1383893"}, {"introduction": "To truly master a technique, it's essential to perform it from start to finish. This capstone practice guides you through a complete, manual PCA calculation on a small, hypothetical gene expression dataset. You will apply all the key steps—data centering, computing the sample covariance matrix, and finding the dominant eigenvector—to derive the first principal component loading vector yourself [@problem_id:2416060]. This comprehensive walkthrough consolidates the theoretical steps into a concrete, practical procedure.", "problem": "A gene expression experiment measured log-transformed expression values for $G_1$, $G_2$, and $G_3$ across $S_1$, $S_2$, $S_3$, and $S_4$. The data matrix $X \\in \\mathbb{R}^{4 \\times 3}$ has samples as rows and genes as columns:\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\nTreat samples as independent observations and genes as variables. Using principal component analysis (PCA), compute the first principal component loading vector in gene space by:\n- column-centering $X$,\n- forming the sample covariance matrix across genes with denominator $n-1$ for $n=4$ samples, and\n- taking the unit-norm eigenvector of this covariance matrix corresponding to the largest eigenvalue.\n\nReport the loading vector as a $1 \\times 3$ row matrix ordered as $(G_1, G_2, G_3)$, with the sign chosen so that its first nonzero entry is positive. No rounding is required.", "solution": "We are asked to compute the first principal component loading vector in gene space using the eigen-decomposition of the sample covariance matrix across genes. Let $n=4$ be the number of samples and $p=3$ be the number of genes. The data matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows as samples and columns as genes.\n\nStep $1$: Column-centering. Compute the column means of $X$:\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\nSubtract these means from each column to obtain the centered matrix $Z$:\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\nStep $2$: Sample covariance matrix across genes. Using the denominator $n-1=3$, the sample covariance of genes is\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\nObserve that each row of $Z$ is a scalar multiple of $(1,\\,1,\\,1)$, so all three centered gene columns are identical. Compute $Z^{\\top}Z$ by noting that for any two columns $j$ and $k$, the $(j,k)$ entry equals $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$. Since all three columns are identical, every entry of $Z^{\\top}Z$ equals the sum of squares of a single centered column:\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\nTherefore,\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\nStep $3$: Eigen-decomposition. Let $J \\in \\mathbb{R}^{3 \\times 3}$ denote the all-ones matrix, i.e., $J_{jk}=1$ for all $j,k$. It is known from first principles that $J$ has rank $1$ with eigenvalues $3$ and $0$ (with multiplicity $2$). A corresponding unit-norm eigenvector for the eigenvalue $3$ is proportional to $(1,\\,1,\\,1)^{\\top}$, specifically\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nSince $S = \\frac{5}{3} J$, the eigenvalues of $S$ are $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ and $\\lambda_{2} = 0$, $\\lambda_{3} = 0$, with the same eigenvectors as $J$. Thus, the first principal component loading vector in gene space is the unit-norm eigenvector associated with $\\lambda_{1}=5$, namely $v$ as above. Choosing the sign so that the first nonzero entry is positive yields\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\nTherefore, ordered as $(G_1, G_2, G_3)$ and written as a $1 \\times 3$ row matrix, the first principal component loading vector is\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2416060"}]}