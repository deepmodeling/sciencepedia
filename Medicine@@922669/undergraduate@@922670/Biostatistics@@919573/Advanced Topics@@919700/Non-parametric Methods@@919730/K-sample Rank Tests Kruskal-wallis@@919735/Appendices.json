{"hands_on_practices": [{"introduction": "To truly understand the Kruskal-Wallis test, we must first master its mechanics. This exercise guides you through the fundamental algorithm using a small, hypothetical dataset. By pooling the data, assigning ranks, correctly handling ties with mid-ranks, and computing the final tie-corrected $H$ statistic, you will build a solid foundation for applying the test in more complex scenarios.", "problem": "In a biostatistics study comparing a continuous biomarker across three independent groups, the investigator wishes to use the Kruskal-Wallis k-sample rank test to avoid distributional assumptions. Three groups are observed: $G_{1}=\\{2,5\\}$, $G_{2}=\\{1,3\\}$, and $G_{3}=\\{2,4\\}$. The fundamental basis of the Kruskal-Wallis procedure is to pool all $N$ observations, assign ranks to the pooled values from $1$ (smallest) to $N$ (largest), using midranks when ties occur, then to compute groupwise rank sums and form a test statistic that depends on the rank sums and group sizes. Ties in the pooled data require a standard tie correction to the test statistic.\n\nStarting from these principles, do the following:\n- Pool the data across $G_{1}$, $G_{2}$, and $G_{3}$ and assign midranks to all tied values.\n- Compute the rank sums $R_{1}$, $R_{2}$, and $R_{3}$ for $G_{1}$, $G_{2}$, and $G_{3}$, respectively.\n- Using these rank sums, the group sizes, and the pooled sample size $N$, compute the tie-corrected Kruskal-Wallis statistic for $k=3$ groups.\n\nRound your final numeric answer to four significant figures. No units are required.", "solution": "First, we identify the group sizes and the total sample size.\nFor group $G_1$, the sample size is $n_1=2$.\nFor group $G_2$, the sample size is $n_2=2$.\nFor group $G_3$, the sample size is $n_3=2$.\nThe total number of observations is $N = n_1 + n_2 + n_3 = 2 + 2 + 2 = 6$.\n\nNext, we pool all observations into a single dataset: $\\{2, 5, 1, 3, 2, 4\\}$.\nWe sort the pooled data in ascending order: $\\{1, 2, 2, 3, 4, 5\\}$.\n\nWe then assign ranks to these sorted observations from $1$ to $N=6$. Since there is a tie for the value $2$, we use midranks. The two observations of value $2$ occupy the $2$nd and $3$rd positions in the sorted list. The midrank for these tied values is the average of their positions: $\\frac{2+3}{2} = 2.5$.\nThe ranks for the sorted data are:\n- Value $1$: Rank $1$\n- Value $2$: Rank $2.5$\n- Value $2$: Rank $2.5$\n- Value $3$: Rank $4$\n- Value $4$: Rank $5$\n- Value $5$: Rank $6$\n\nNow, we associate these ranks with their original groups and compute the rank sums $R_i$ for each group $i$.\n- For $G_1 = \\{2, 5\\}$, the corresponding ranks are $\\{2.5, 6\\}$. The rank sum is $R_1 = 2.5 + 6 = 8.5$.\n- For $G_2 = \\{1, 3\\}$, the corresponding ranks are $\\{1, 4\\}$. The rank sum is $R_2 = 1 + 4 = 5$.\n- For $G_3 = \\{2, 4\\}$, the corresponding ranks are $\\{2.5, 5\\}$. The rank sum is $R_3 = 2.5 + 5 = 7.5$.\n\nAs a check, the sum of all rank sums must equal the sum of all ranks from $1$ to $N$: $\\frac{N(N+1)}{2}$.\n$R_1 + R_2 + R_3 = 8.5 + 5 + 7.5 = 21$.\n$\\frac{6(6+1)}{2} = \\frac{42}{2} = 21$. The check is successful.\n\nThe Kruskal-Wallis test statistic, commonly denoted by $H$, is calculated using the formula:\n$$H = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)$$\nSubstituting the known values:\nThe sum of squared rank sums divided by their respective sample sizes is:\n$$\\sum_{i=1}^{3} \\frac{R_i^2}{n_i} = \\frac{(8.5)^2}{2} + \\frac{5^2}{2} + \\frac{(7.5)^2}{2} = \\frac{72.25}{2} + \\frac{25}{2} + \\frac{56.25}{2} = \\frac{72.25 + 25 + 56.25}{2} = \\frac{153.5}{2} = 76.75$$\nNow we compute $H$:\n$$H = \\frac{12}{6(6+1)}(76.75) - 3(6+1) = \\frac{12}{42}(76.75) - 3(7) = \\frac{2}{7}(76.75) - 21$$\n$$H = \\frac{153.5}{7} - 21 = \\frac{153.5 - 147}{7} = \\frac{6.5}{7} = \\frac{13/2}{7} = \\frac{13}{14}$$\n\nBecause there are ties in the data, the statistic must be corrected. The tie-corrected statistic, $H_c$, is given by $H_c = \\frac{H}{C}$, where $C$ is the tie correction factor. The formula for $C$ is:\n$$C = 1 - \\frac{\\sum_{j=1}^{g} (t_j^3 - t_j)}{N^3 - N}$$\nHere, $g$ is the number of groups of tied ranks, and $t_j$ is the number of tied observations in the $j$-th group of ties.\nIn our data, there is one group of ties: the value $2$ appears twice. Thus, $g=1$ and $t_1=2$.\nThe summation term is:\n$$\\sum_{j=1}^{1} (t_j^3 - t_j) = t_1^3 - t_1 = 2^3 - 2 = 8 - 2 = 6$$\nThe denominator term is:\n$$N^3 - N = 6^3 - 6 = 216 - 6 = 210$$\nThe correction factor $C$ is:\n$$C = 1 - \\frac{6}{210} = 1 - \\frac{1}{35} = \\frac{34}{35}$$\nFinally, we compute the tie-corrected Kruskal-Wallis statistic $H_c$:\n$$H_c = \\frac{H}{C} = \\frac{13/14}{34/35} = \\frac{13}{14} \\times \\frac{35}{34} = \\frac{13 \\times (5 \\times 7)}{ (2 \\times 7) \\times 34} = \\frac{13 \\times 5}{2 \\times 34} = \\frac{65}{68}$$\nTo provide the final answer, we convert this fraction to a decimal and round to four significant figures.\n$$H_c = \\frac{65}{68} \\approx 0.95588235...$$\nRounding to four significant figures gives $0.9559$.", "answer": "$$\\boxed{0.9559}$$", "id": "4921335"}, {"introduction": "Beyond pure calculation, it is crucial to grasp the principles that make the Kruskal-Wallis test a powerful non-parametric tool. This thought experiment [@problem_id:1961654] explores how the test statistic is invariant to certain data transformations that preserve order. By analyzing why the test's outcome remains unchanged when the data's scale or sign is altered, you will gain a deeper appreciation for its rank-based nature and its robustness.", "problem": "A team of material scientists is investigating the fracture toughness of three new types of ceramic composites, denoted as Type A, Type B, and Type C. They conduct experiments on several samples of each type, measuring the energy absorbed (in Joules) before fracture. All measurements are distinct positive values.\n\nTwo data analysts, Ana and Ben, are assigned to determine if there is a statistically significant difference in the median fracture toughness among the three composite types. They decide to use the Kruskal-Wallis test, a non-parametric method. The Kruskal-Wallis test statistic, $H$, is calculated using the formula:\n$$H = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)$$\nwhere $k$ is the number of groups (in this case, $k=3$), $n_i$ is the number of samples in group $i$, $N$ is the total number of samples across all groups, and $R_i$ is the sum of the ranks for the measurements in group $i$. The ranks are assigned by combining all $N$ measurements and ordering them from smallest to largest.\n\nAna uses the original experimental data (`Dataset 1`) for her analysis. Ben, following a different data processing protocol, first transforms every measurement by multiplying it by a negative constant, $c = -50$, creating `Dataset 2`.\n\nLet $H_1$ be the value of the Kruskal-Wallis statistic that Ana calculates from `Dataset 1`, and let $H_2$ be the value that Ben calculates from `Dataset 2`. What is the relationship between $H_1$ and $H_2$?\n\nA. $H_2 = H_1$\nB. $H_2 = |c| H_1$\nC. $H_2 = c^2 H_1$\nD. $H_2 = -H_1$\nE. The relationship cannot be determined without knowing the specific data values or sample sizes.\nF. $H_2 = \\frac{1}{c} H_1$", "solution": "The Kruskal-Wallis statistic is\n$$\nH=\\frac{12}{N(N+1)}\\sum_{i=1}^{k}\\frac{R_{i}^{2}}{n_{i}}-3(N+1)\n$$,\nwhere $R_{i}$ is the sum of the ranks for group $i$, $n_{i}$ is the sample size of group $i$, $k=3$, and $N=\\sum_{i=1}^{k}n_{i}$ is the total sample size. Ranks are assigned by ordering all $N$ observations from smallest to largest.\n\nBen transforms every observation by multiplying by a negative constant $c  0$. Because all measurements are distinct, this transformation reverses the order of all observations. Therefore, if an observation has global rank $r$ in Dataset 1, its rank in Dataset 2 becomes\n$$\nr' = N+1 - r.\n$$\n\nFor group $i$, let $R_{i}$ be the sum of its ranks in Dataset 1 and $R_{i}'$ the sum in Dataset 2. If group $i$ has $n_{i}$ observations with ranks $\\{r_{ij}\\}_{j=1}^{n_{i}}$, then\n$$\nR_{i}'=\\sum_{j=1}^{n_{i}}r_{ij}'=\\sum_{j=1}^{n_{i}}\\left(N+1-r_{ij}\\right)=n_{i}(N+1)-R_{i}.\n$$\n\nCompute the term appearing in $H$:\n$$\n\\sum_{i=1}^{k}\\frac{(R_{i}')^{2}}{n_{i}}=\\sum_{i=1}^{k}\\frac{\\left(n_{i}(N+1)-R_{i}\\right)^{2}}{n_{i}}\n=\\sum_{i=1}^{k}\\left(n_{i}(N+1)^{2}-2(N+1)R_{i}+\\frac{R_{i}^{2}}{n_{i}}\\right).\n$$\nUsing $\\sum_{i=1}^{k}n_{i}=N$ and $\\sum_{i=1}^{k}R_{i}=\\sum_{r=1}^{N}r=\\frac{N(N+1)}{2}$, we obtain\n$$\n\\sum_{i=1}^{k}\\frac{(R_{i}')^{2}}{n_{i}}=N(N+1)^{2}-2(N+1)\\cdot\\frac{N(N+1)}{2}+\\sum_{i=1}^{k}\\frac{R_{i}^{2}}{n_{i}}\n=\\sum_{i=1}^{k}\\frac{R_{i}^{2}}{n_{i}}.\n$$\n\nSubstituting into the Kruskal-Wallis formula,\n$$\nH_{2}=\\frac{12}{N(N+1)}\\sum_{i=1}^{k}\\frac{(R_{i}')^{2}}{n_{i}}-3(N+1)\n=\\frac{12}{N(N+1)}\\sum_{i=1}^{k}\\frac{R_{i}^{2}}{n_{i}}-3(N+1)=H_{1}.\n$$\n\nThus, multiplying all observations by a negative constant reverses ranks but leaves the Kruskal-Wallis statistic unchanged. Therefore, $H_{2}=H_{1}$, corresponding to option A.", "answer": "$$\\boxed{A}$$", "id": "1961654"}, {"introduction": "In modern biostatistics, theory is often validated through computational practice. The Kruskal-Wallis test relies on the chi-square distribution as an approximation to determine statistical significance, but how well does this approximation hold for finite samples? This hands-on coding exercise challenges you to use Monte Carlo simulation to generate the test's null distribution and compare it against its theoretical counterpart, providing practical experience in the powerful techniques used to assess the performance of statistical methods.", "problem": "You are asked to assess the finite-sample accuracy of the large-sample chi-square approximation for the Kruskal-Wallis test statistic under the null hypothesis for $k=4$ groups and total sample size $N=24$. Work from fundamental rank-based definitions: the Kruskal-Wallis procedure ranks the pooled data across all groups using average ranks for ties, constructs group-level rank sums, and forms a test statistic that, under the null hypothesis of identically distributed continuous populations, converges in distribution to a chi-square distribution with $k-1$ degrees of freedom. When ties occur, a standard tie-correction factor is applied to the statistic. Your task is to implement this from first principles and evaluate the quality of the chi-square approximation in finite samples via simulation.\n\nWrite a program that:\n- Implements the Kruskal-Wallis $H$-statistic directly from its rank-based definition using average ranks for ties and applies the standard tie-correction factor.\n- Simulates the null distribution of $H$ by repeatedly generating independent and identically distributed samples for the $k$ groups (all groups sampled from the same distribution under the null).\n- Quantifies the deviation between the simulated null distribution of $H$ and the theoretical chi-square distribution with $k-1$ degrees of freedom via three numerical summaries:\n  1. The absolute deviation of the empirical mean of $H$ from $k-1$.\n  2. The absolute deviation of the empirical upper-tail probability at the theoretical $0.95$ chi-square quantile, that is, $\\left| \\mathbb{P}(H \\ge q_{0.95}) - 0.05 \\right|$, where $q_{0.95}$ is the $0.95$ quantile of $\\chi^2_{k-1}$.\n  3. The Kolmogorov distance between the empirical cumulative distribution function of $H$ and the cumulative distribution function of $\\chi^2_{k-1}$, defined as the supremum over $x$ of the absolute difference between the two distribution functions.\n\nUse the following fixed test suite to ensure coverage of different facets of the approximation:\n- Case A (balanced, continuous null): $k=4$, group sizes $[6,6,6,6]$ (so $N=24$), number of simulations $20000$, null sampling distribution standard normal, random seed $12345$.\n- Case B (unbalanced, continuous null): $k=4$, group sizes $[3,5,7,9]$ (so $N=24$), number of simulations $20000$, null sampling distribution standard normal, random seed $24680$.\n- Case C (balanced, discrete null with ties): $k=4$, group sizes $[6,6,6,6]$ (so $N=24$), number of simulations $20000$, null sampling distribution Poisson with mean $2$ (to induce ties), random seed $13579$.\n\nFor each case, your program must:\n- Simulate the Kruskal-Wallis $H$-statistic under the null for the specified number of replications using the specified group sizes and seed.\n- Compute the following three quantities, each rounded to $6$ decimal places:\n  1. $\\left| \\overline{H} - (k-1) \\right|$,\n  2. $\\left| \\widehat{\\mathbb{P}}(H \\ge q_{0.95}) - 0.05 \\right|$ where $q_{0.95}$ is the $0.95$ quantile of $\\chi^2_{k-1}$,\n  3. The Kolmogorov distance between the empirical distribution of $H$ and $\\chi^2_{k-1}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of the three floats in the order specified above. For example, the output must have the form\n- $[\\,[m_A,t_A,d_A],[m_B,t_B,d_B],[m_C,t_C,d_C]\\,]$,\nwhere $m_\\cdot$ denotes the absolute mean deviation, $t_\\cdot$ the absolute tail probability deviation at the $0.95$ quantile, and $d_\\cdot$ the Kolmogorov distance for the corresponding case.\n\nNo physical units and no angle units are involved. All probabilities must be expressed as decimals. Round all reported floats to $6$ decimal places. The program must be fully deterministic and use the provided seeds to initialize the random number generator. Do not require any user input and do not read or write any files.", "solution": "The Kruskal-Wallis test is a non-parametric method for testing whether samples originate from the same distribution. It is used to compare two or more independent samples of equal or different sample sizes.\n\nLet there be $k$ groups of data. Let the sample size of the $i$-th group be $n_i$, for $i \\in \\{1, 2, \\dots, k\\}$. The total number of observations is $N = \\sum_{i=1}^{k} n_i$. The procedure is as follows:\n\n1.  **Pooling and Ranking**: All $N$ observations from the $k$ groups are combined into a single dataset. This pooled dataset is then ranked from $1$ to $N$. In the case of ties (identical values), each tied observation is assigned the average of the ranks they would have occupied. For example, if three observations are tied for the ranks $5$, $6$, and $7$, each is assigned the rank $(5+6+7)/3 = 6$.\n\n2.  **Rank Sums**: The sum of the ranks for the observations within each group $i$ is calculated. This sum is denoted by $R_i$.\n    $$ R_i = \\sum_{j=1}^{n_i} \\text{rank}(x_{ij}) $$\n    where $x_{ij}$ is the $j$-th observation in the $i$-th group.\n\n3.  **H-statistic Calculation**: The Kruskal-Wallis $H$-statistic is computed using the formula:\n    $$ H = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1) $$\n\n4.  **Tie Correction**: When ties are present in the data, the $H$-statistic is adjusted by dividing it by a correction factor, $C$. Let $g$ be the number of groups of tied ranks, and let $t_j$ be the number of tied observations in the $j$-th group of ties. The correction factor is given by:\n    $$ C = 1 - \\frac{\\sum_{j=1}^{g} (t_j^3 - t_j)}{N^3 - N} $$\n    The tie-corrected statistic, which we will also denote by $H$ for simplicity in the following context, is $H_{\\text{corrected}} = H/C$. If no ties exist, then $C=1$ and the statistic remains unchanged. If all observations are identical, $C=0$; in this case, the statistic is conventionally defined as $0$, as there is no evidence to reject the null hypothesis.\n\nUnder the null hypothesis that all groups are sampled from identical populations, the distribution of the $H$-statistic can be approximated by a chi-square distribution with $k-1$ degrees of freedom. This approximation is generally considered reliable when the group sizes $n_i$ are sufficiently large (e.g., $n_i \\ge 5$).\n\nThis problem requires us to assess the quality of this chi-square approximation for a finite sample scenario where $k=4$ and $N=24$. This is achieved through a Monte Carlo simulation.\n\nFor each of the three test cases specified:\n1.  A random number generator is initialized with a specific seed to ensure reproducibility.\n2.  A total of $S=20000$ simulation trials are run. In each trial:\n    a. Samples for the $k=4$ groups are generated according to the specified null distribution (standard normal for Cases A and B, Poisson for Case C) and group sizes.\n    b. The tie-corrected Kruskal-Wallis $H$-statistic is calculated from first principles as described above.\n3.  The result is a set of $S=20000$ values of the $H$-statistic, $\\{H_s\\}_{s=1}^{S}$, which forms an empirical distribution for the statistic under the null hypothesis.\n\nThis empirical distribution is then compared to the theoretical $\\chi^2$ distribution with $df = k-1 = 3$ degrees of freedom using three metrics:\n\n1.  **Absolute Mean Deviation**: The theoretical mean of a $\\chi^2_{df}$ distribution is its degrees of freedom, $df$. We calculate the empirical mean $\\overline{H} = \\frac{1}{S}\\sum_{s=1}^S H_s$ and compute the absolute deviation: $|\\overline{H} - (k-1)|$.\n\n2.  **Absolute Tail Probability Deviation**: We find the theoretical $0.95$ quantile of the $\\chi^2_{3}$ distribution, denoted $q_{0.95}$. We then estimate the empirical probability of exceeding this value, $\\widehat{\\mathbb{P}}(H \\ge q_{0.95}) = \\frac{1}{S} |\\{s: H_s \\ge q_{0.95}\\}|$. The metric is the absolute difference between this empirical probability and the theoretical one, $0.05$: $|\\widehat{\\mathbb{P}}(H \\ge q_{0.95}) - 0.05|$.\n\n3.  **Kolmogorov Distance**: This is the supremum of the absolute difference between the empirical cumulative distribution function (ECDF) of the simulated $H$ values and the theoretical CDF of the $\\chi^2_{3}$ distribution. It is formally defined as $D = \\sup_x |F_{\\text{emp}}(x) - F_{\\chi^2_3}(x)|$. This value is computed efficiently using `scipy.stats.kstest`.\n\nThe implementation follows the logic outlined above. A function is created to calculate the tie-corrected $H$-statistic for a given list of samples. This function pools the data, uses `scipy.stats.rankdata` to handle ranking and ties, calculates group rank sums, and then computes the $H$-statistic and the tie-correction factor. The main simulation loop iterates through each test case, generates the data, calls the statistic function, and collects the results. Finally, the three evaluation metrics are computed from the collection of simulated statistics and formatted as required. All numerical results are rounded to $6$ decimal places.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_kruskal_wallis_h(list_of_samples):\n    \"\"\"\n    Calculates the Kruskal-Wallis H-statistic from first principles,\n    including tie correction.\n\n    Args:\n        list_of_samples (list of np.ndarray): A list where each element\n            is a numpy array representing a group's sample data.\n\n    Returns:\n        float: The tie-corrected Kruskal-Wallis H-statistic.\n    \"\"\"\n    n_i = np.array([len(s) for s in list_of_samples])\n    k = len(list_of_samples)\n    N = int(np.sum(n_i))\n\n    if N = 1:\n        return 0.0\n\n    all_data = np.concatenate(list_of_samples)\n    ranks = stats.rankdata(all_data, method='average')\n\n    # Split ranks back into groups and sum them\n    split_indices = np.cumsum(n_i)[:-1]\n    group_ranks_list = np.split(ranks, split_indices)\n    group_rank_sums = np.array([np.sum(arr) for arr in group_ranks_list])\n\n    # Calculate H-statistic\n    H = (12.0 / (N * (N + 1))) * np.sum(group_rank_sums**2 / n_i) - 3.0 * (N + 1)\n\n    # Calculate tie-correction factor C\n    _, tie_counts = np.unique(all_data, return_counts=True)\n    tied_groups = tie_counts[tie_counts  1]\n\n    tie_correction_numerator = np.sum(tied_groups**3 - tied_groups, dtype=np.float64)\n    tie_correction_denominator = N**3 - N\n\n    if tie_correction_denominator == 0:\n        # This implies all values are identical (N=t_1), so H is 0.\n        return 0.0\n\n    C = 1.0 - tie_correction_numerator / tie_correction_denominator\n\n    if C == 0:\n        # If correction factor is 0, the test is uninformative.\n        return 0.0\n\n    return H / C\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'case_id': 'A', 'k': 4, 'group_sizes': [6, 6, 6, 6], 'n_sims': 20000, \n         'dist': 'normal', 'dist_params': {}, 'seed': 12345},\n        {'case_id': 'B', 'k': 4, 'group_sizes': [3, 5, 7, 9], 'n_sims': 20000, \n         'dist': 'normal', 'dist_params': {}, 'seed': 24680},\n        {'case_id': 'C', 'k': 4, 'group_sizes': [6, 6, 6, 6], 'n_sims': 20000, \n         'dist': 'poisson', 'dist_params': {'mu': 2}, 'seed': 13579}\n    ]\n\n    all_results_lists = []\n\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        k = case['k']\n        df = k - 1\n        group_sizes = case['group_sizes']\n        n_sims = case['n_sims']\n\n        h_values = np.zeros(n_sims)\n\n        for i in range(n_sims):\n            samples_list = []\n            for size in group_sizes:\n                if case['dist'] == 'normal':\n                    s = rng.normal(loc=0, scale=1, size=size)\n                else:  # poisson\n                    s = rng.poisson(lam=case['dist_params']['mu'], size=size)\n                samples_list.append(s)\n            \n            h_values[i] = calculate_kruskal_wallis_h(samples_list)\n        \n        # Metric 1: Absolute mean deviation\n        mean_h = np.mean(h_values)\n        metric1 = abs(mean_h - df)\n        \n        # Metric 2: Absolute tail probability deviation\n        q95 = stats.chi2.ppf(0.95, df=df)\n        empirical_tail_prob = np.mean(h_values = q95)\n        metric2 = abs(empirical_tail_prob - 0.05)\n        \n        # Metric 3: Kolmogorov distance\n        ks_statistic, _ = stats.kstest(h_values, cdf='chi2', args=(df,))\n        metric3 = ks_statistic\n        \n        all_results_lists.append([\n            round(metric1, 6),\n            round(metric2, 6),\n            round(metric3, 6)\n        ])\n\n    # Format output string manually to match requirements (no spaces)\n    case_strs = []\n    for res_list in all_results_lists:\n        case_strs.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    print(f\"[{','.join(case_strs)}]\")\n\nsolve()\n```", "id": "4921359"}]}