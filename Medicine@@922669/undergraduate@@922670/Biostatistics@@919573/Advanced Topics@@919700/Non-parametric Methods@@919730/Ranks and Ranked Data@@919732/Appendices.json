{"hands_on_practices": [{"introduction": "This first exercise lays the groundwork for understanding non-parametric hypothesis testing by focusing on the Wilcoxon rank-sum test. You will implement a recursive algorithm to construct the exact null distribution of the test statistic from combinatorial first principles, demystifying where p-values come from. By also implementing the normal approximation, you will see how exact methods for small samples connect to the large-sample theory that is foundational to much of statistics [@problem_id:4946603].", "problem": "Consider two independent groups of continuous measurements with no tied values. The Wilcoxon rank-sum statistic for group $A$ is defined as the sum of its ranks after pooling all observations from both groups and ranking them in ascending order, with the smallest observation assigned rank $1$. Under the null hypothesis that group labels are exchangeable, the exact sampling distribution of the rank-sum statistic arises from all ways to allocate $n_1$ labels $A$ among the $N=n_1+n_2$ pooled observations, each allocation equally likely.\n\nYour task is to implement, from first principles, a recursion to compute the exact two-sided $p$-value of the Wilcoxon rank-sum statistic for small group sizes, and to compare it to the normal approximation with continuity correction. The derivations should start from the core definitions in ranked data and basic probability, and must not rely on pre-stated closed-form shortcut formulas for the full distribution. You may use well-tested facts such as the behavior of sums of independent and identically distributed ranks under the Central Limit Theorem.\n\nDefinitions and requirements:\n- Let $A=\\{a_1,\\dots,a_{n_1}\\}$ and $B=\\{b_1,\\dots,b_{n_2}\\}$ with $n_1\\geq 1$, $n_2\\geq 1$, and all values distinct. Let $N=n_1+n_2$.\n- Let $W$ denote the observed Wilcoxon rank-sum for group $A$.\n- Under the null hypothesis, the exact distribution of $W$ is induced by uniformly choosing $n_1$ ranks from $\\{1,2,\\dots,N\\}$ and summing them. Use a recursion based on this combinatorial definition to count, for each integer $s$, how many allocations of $n_1$ ranks produce a sum equal to $s$. From these counts, compute the exact lower-tail probability $P(W\\leq w_{\\text{obs}})$ and upper-tail probability $P(W\\geq w_{\\text{obs}})$, and then the exact two-sided $p$-value as $2\\cdot \\min\\{P(W\\leq w_{\\text{obs}}), P(W\\geq w_{\\text{obs}})\\}$, truncated at $1$.\n- For the normal approximation with continuity correction, derive the mean and variance of $W$ under the null from foundational properties of ranks, invoke the Central Limit Theorem to approximate $W$ by a normal distribution, and apply continuity correction by subtracting $0.5$ from the absolute deviation $|w_{\\text{obs}}-\\mu|$ before standardization. Use this to compute the approximate two-sided $p$-value.\n\nTest suite:\nImplement your program to compute the exact two-sided $p$-value and the continuity-corrected normal approximation for the following five test cases. In each case, the arrays contain distinct real numbers:\n- Case $1$: $A=[5.1,4.7,2.3]$, $B=[1.0,3.5,6.0,7.2]$.\n- Case $2$: $A=[0.5]$, $B=[1.2,2.0,3.1,4.4,5.9]$.\n- Case $3$: $A=[9.0,8.0]$, $B=[1.0,2.0,3.0]$.\n- Case $4$: $A=[2.0,4.0,5.0,7.0]$, $B=[1.0,3.0,6.0,8.0]$.\n- Case $5$: $A=[10.0,1.5,3.3]$, $B=[0.7,2.2,4.8,5.0,7.1]$.\n\nOutput specification:\n- For each case, output a list of three decimal numbers: the exact two-sided $p$-value, the continuity-corrected normal approximation two-sided $p$-value, and the absolute difference between the two. Round each decimal to $6$ places.\n- Aggregate the results across the five cases into a single line as a comma-separated list of these three-element lists, enclosed in square brackets, with no spaces.\n- Your program must be self-contained, take no input, and use only the specified runtime environment.\n\nThere are no physical units or angles involved, and no percentages should be printed; decimals are required.", "solution": "The user wants me to solve a problem about the Wilcoxon rank-sum test, which is a common task in biostatistics. The problem is valid, scientifically grounded, and well-posed. All definitions and conditions are standard and self-consistent. The task is to compute exact and approximate p-values for given data sets. I will now proceed with the solution.\n\n### Part 1: Formalization of the Wilcoxon Rank-Sum Test\n\nLet there be two independent samples, group $A$ with $n_1$ observations and group $B$ with $n_2$ observations. We assume the data is continuous, so there are no tied values. The total number of observations is $N = n_1 + n_2$.\n\nThe Wilcoxon rank-sum test is a non-parametric test of the null hypothesis ($H_0$) that the two samples are drawn from the same distribution. Under $H_0$, the labels 'A' and 'B' are exchangeable. To perform the test, we first pool all $N$ observations and rank them from $1$ (smallest) to $N$ (largest).\n\nThe test statistic, $W$, is the sum of the ranks assigned to the observations in one of the groups. The problem specifies that $W$ is the rank-sum for group $A$.\n\nUnder $H_0$, any set of $n_1$ ranks for group $A$ is equally likely. The total number of ways to choose $n_1$ ranks from the set $\\{1, 2, \\dots, N\\}$ is given by the binomial coefficient $\\binom{N}{n_1}$. The distribution of $W$ is the distribution of the sum of $n_1$ integers chosen uniformly at random without replacement from $\\{1, 2, \\dots, N\\}$.\n\n### Part 2: Exact Distribution via Recursion\n\nTo find the exact sampling distribution of $W$, we need to count how many distinct sets of $n_1$ ranks sum to a particular value $s$. This is a classic combinatorial problem that can be solved using a recurrence relation, which lends itself to a dynamic programming implementation.\n\nLet $C(k, i, s)$ be the number of ways to choose $i$ distinct integers from the set $\\{1, 2, \\dots, k\\}$ such that their sum is exactly $s$. We want to find the distribution $C(N, n_1, s)$ for all possible sum values $s$.\n\nThe recurrence relation is derived by considering the largest element, $k$:\n1.  **Case 1: The integer $k$ is not included.** We must choose $i$ integers from $\\{1, 2, \\dots, k-1\\}$ that sum to $s$. The number of ways to do this is $C(k-1, i, s)$.\n2.  **Case 2: The integer $k$ is included.** We must choose $i-1$ integers from $\\{1, 2, \\dots, k-1\\}$ that sum to $s-k$. The number of ways to do this is $C(k-1, i-1, s-k)$.\n\nCombining these cases, the recurrence is:\n$$C(k, i, s) = C(k-1, i, s) + C(k-1, i-1, s-k)$$\nThe base cases are:\n- $C(k, 0, 0) = 1$ for any $k \\ge 0$ (there is one way to choose 0 items, with a sum of 0).\n- $C(k, i, s) = 0$ if $i>k$, $s<0$, or if $i=0$ and $s \\neq 0$. Also, $C(k,i,s)=0$ if $s$ is smaller than the minimum possible sum ($\\sum_{j=1}^i j$) or larger than the maximum possible sum ($\\sum_{j=k-i+1}^k j$).\n\nOnce the counts $C(N, n_1, s)$ are computed for all relevant $s$, the probability of observing a specific rank-sum $s$ is:\n$$P(W=s) = \\frac{C(N, n_1, s)}{\\binom{N}{n_1}}$$\nFor an observed rank-sum $w_{\\text{obs}}$, the lower-tail and upper-tail probabilities are:\n$$P(W \\le w_{\\text{obs}}) = \\sum_{s \\le w_{\\text{obs}}} P(W=s)$$\n$$P(W \\ge w_{\\text{obs}}) = \\sum_{s \\ge w_{\\text{obs}}} P(W=s)$$\nThe exact two-sided p-value is then defined as $2 \\cdot \\min\\{P(W \\le w_{\\text{obs}}), P(W \\ge w_{\\text{obs}})\\}$, truncated at $1$.\n\n### Part 3: Normal Approximation with Continuity Correction\n\nFor larger sample sizes, the Central Limit Theorem allows us to approximate the distribution of $W$ with a normal distribution. To do this, we first need the mean ($\\mu_W$) and variance ($\\sigma_W^2$) of $W$ under the null hypothesis.\n\nThe statistic $W$ is the sum of $n_1$ ranks drawn randomly without replacement from the population of ranks $\\{1, 2, \\dots, N\\}$.\nThe mean of this population of ranks is:\n$$ E[\\text{rank}] = \\frac{1}{N} \\sum_{j=1}^{N} j = \\frac{N(N+1)}{2N} = \\frac{N+1}{2} $$\nThe variance of this population of ranks is:\n$$ \\sigma^2_{\\text{rank}} = E[\\text{rank}^2] - (E[\\text{rank}])^2 = \\left(\\frac{1}{N} \\sum_{j=1}^{N} j^2\\right) - \\left(\\frac{N+1}{2}\\right)^2 $$\nUsing the formula for the sum of squares, $\\sum_{j=1}^N j^2 = \\frac{N(N+1)(2N+1)}{6}$, we get:\n$$ \\sigma^2_{\\text{rank}} = \\frac{(N+1)(2N+1)}{6} - \\frac{(N+1)^2}{4} = \\frac{N+1}{12} [2(2N+1) - 3(N+1)] = \\frac{N^2-1}{12} $$\nThe expectation of $W$ is the sum of the expectations of the $n_1$ ranks drawn:\n$$ \\mu_W = E[W] = E\\left[\\sum_{i=1}^{n_1} R_i\\right] = \\sum_{i=1}^{n_1} E[R_i] = n_1 E[\\text{rank}] = \\frac{n_1(N+1)}{2} $$\nThe variance of a sum of $n_1$ items sampled without replacement from a population of size $N$ is given by $Var(\\sum R_i) = n_1 \\sigma^2_{\\text{rank}} \\frac{N-n_1}{N-1}$. Substituting the population variance:\n$$ \\sigma_W^2 = Var(W) = n_1 \\left(\\frac{N^2-1}{12}\\right) \\frac{N-n_1}{N-1} = n_1 \\frac{(N-1)(N+1)}{12} \\frac{n_2}{N-1} = \\frac{n_1 n_2 (N+1)}{12} $$\nThe null distribution of $W$ is approximated by $\\mathcal{N}(\\mu_W, \\sigma_W^2)$. Since $W$ is a discrete variable, a continuity correction is applied to improve the approximation. The correction involves adjusting the observed value by $0.5$ towards the mean. The standardized test statistic $Z_c$ is:\n$$ Z_c = \\frac{|w_{\\text{obs}} - \\mu_W| - 0.5}{\\sigma_W} $$\nIf $|w_{\\text{obs}} - \\mu_W| \\le 0.5$, the numerator becomes non-positive, yielding $Z_c \\le 0$. The two-sided p-value is calculated as $2 \\cdot P(Z > Z_c)$ for a standard normal variable $Z \\sim \\mathcal{N}(0,1)$. If $Z_c \\le 0$, this p-value is $\\ge 1$, so it is taken to be $1$. In other cases, it is computed from the survival function of the standard normal distribution.\n\n### Part 4: Implementation for Test Cases\n\nThe solution is implemented in Python. A helper function calculates the observed rank-sum $w_{\\text{obs}}$ and sample sizes for each test case. The exact distribution of $W$ is found using the recursive formulation implemented with dynamic programming to avoid redundant calculations. From the distribution, the exact p-value is computed. A separate function calculates the mean and variance of $W$ and uses the normal approximation with continuity correction to find the approximate p-value. The absolute difference is then calculated. The results for all test cases are formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import comb\n\ndef get_wilcoxon_rank_sum(A, B):\n    \"\"\"\n    Calculates the Wilcoxon rank-sum statistic W for group A.\n    \n    Args:\n        A (list): Observations for group A.\n        B (list): Observations for group B.\n    \n    Returns:\n        tuple: (w_obs, n1, n2) where w_obs is the rank-sum for group A,\n               n1 is the size of A, and n2 is the size of B.\n    \"\"\"\n    n1 = len(A)\n    n2 = len(B)\n    \n    # Pool data and assign group labels\n    pooled_data = []\n    for x in A:\n        pooled_data.append((x, 'A'))\n    for x in B:\n        pooled_data.append((x, 'B'))\n    \n    # Sort pooled data by value to determine ranks\n    pooled_data.sort(key=lambda item: item[0])\n    \n    # Calculate rank sum for group A\n    w_obs = 0\n    for i, item in enumerate(pooled_data):\n        rank = i + 1  # Ranks are 1-based\n        if item[1] == 'A':\n            w_obs += rank\n            \n    return w_obs, n1, n2\n\nmemo_dist = {}\ndef compute_rank_sum_dist(n, k):\n    \"\"\"\n    Computes the exact distribution of the sum of n ranks chosen from {1,...,k}.\n    This is based on the recurrence C(k,i,s) = C(k-1,i,s) + C(k-1,i-1,s-k), where C\n    is the number of ways to choose i items from {1..k} to sum to s. This function\n    uses dynamic programming to find the counts for all possible sums.\n\n    Args:\n        n (int): The number of ranks to choose (n1).\n        k (int): The total number of ranks to choose from (N).\n    \n    Returns:\n        np.ndarray: An array where the index represents the sum and the value\n                    is the count of combinations yielding that sum.\n    \"\"\"\n    if (n, k) in memo_dist:\n        return memo_dist[(n, k)]\n\n    # The max possible sum for n items chosen from {1..k}\n    max_s = int(n * k - n * (n - 1) / 2)\n    \n    # DP table: dp[i][s] = # ways to choose i items with sum s from ranks {1..current_k}\n    # We use a space-optimized DP with a single 2D array.\n    dp = np.zeros((n + 1, max_s + 1), dtype=np.uint64)\n    dp[0, 0] = 1\n\n    for current_rank in range(1, k + 1):\n        # Iterate i backwards to use results from the previous 'current_rank' step\n        for i in range(min(current_rank, n), 0, -1):\n            for s in range(1, max_s + 1):\n                if s >= current_rank:\n                    dp[i, s] += dp[i - 1, s - current_rank]\n\n    dist = dp[n, :]\n    memo_dist[(n, k)] = dist\n    return dist\n\ndef get_exact_p_value(w_obs, n1, n2):\n    \"\"\"Calculates the exact two-sided p-value.\"\"\"\n    N = n1 + n2\n    \n    dist_counts = compute_rank_sum_dist(n1, N)\n    total_combinations = comb(N, n1, exact=True)\n    if total_combinations == 0: return 1.0\n    \n    # Calculate lower-tail and upper-tail probabilities\n    p_le = np.sum(dist_counts[:w_obs + 1]) / total_combinations\n    p_ge = np.sum(dist_counts[w_obs:]) / total_combinations\n    \n    p_val = 2 * min(p_le, p_ge)\n    return min(p_val, 1.0)\n\ndef get_normal_approx_p_value(w_obs, n1, n2):\n    \"\"\"Calculates the two-sided p-value using normal approximation with continuity correction.\"\"\"\n    N = n1 + n2\n    mu_W = n1 * (N + 1) / 2.0\n    var_W = n1 * n2 * (N + 1) / 12.0\n    \n    if var_W == 0:\n        return 1.0 if w_obs == mu_W else 0.0\n\n    sigma_W = np.sqrt(var_W)\n    deviation = abs(w_obs - mu_W)\n    \n    # Standardized score with continuity correction\n    z_c = (deviation - 0.5) / sigma_W\n    \n    if z_c < 0:\n        p_val_approx = 1.0\n    else:\n        # Two-sided p-value is 2 * P(Z > z_c)\n        p_val_approx = 2 * norm.sf(z_c)\n        \n    return min(p_val_approx, 1.0)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        ([5.1, 4.7, 2.3], [1.0, 3.5, 6.0, 7.2]),\n        ([0.5], [1.2, 2.0, 3.1, 4.4, 5.9]),\n        ([9.0, 8.0], [1.0, 2.0, 3.0]),\n        ([2.0, 4.0, 5.0, 7.0], [1.0, 3.0, 6.0, 8.0]),\n        ([10.0, 1.5, 3.3], [0.7, 2.2, 4.8, 5.0, 7.1]),\n    ]\n    \n    all_results = []\n\n    for A, B in test_cases:\n        w_obs, n1, n2 = get_wilcoxon_rank_sum(A, B)\n        \n        p_exact = get_exact_p_value(w_obs, n1, n2)\n        p_approx = get_normal_approx_p_value(w_obs, n1, n2)\n        abs_diff = abs(p_exact - p_approx)\n        \n        all_results.append(f\"[{p_exact:.6f},{p_approx:.6f},{abs_diff:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "4946603"}, {"introduction": "Building upon the foundation of exact tests, this practice addresses a common scenario in biostatistics: the presence of tied data, such as from Likert scales or rounded measurements. You will develop an exact enumeration method that correctly handles tied ranks, constructing the true null distribution when the standard no-tie assumption is violated. This exercise provides critical skills for applying rank-based tests correctly and robustly to real-world datasets [@problem_id:4946639].", "problem": "You are asked to implement an exact permutation-based adjustment of null distributions for rank statistics when outcomes are discrete, as in Likert scales. Under the null hypothesis of exchangeability, the labels of group membership are uniformly random over the pooled multiset of responses. With discrete outcomes, tied ranks occur, and midranks must be used. Your task is to derive, implement, and evaluate an exact enumeration strategy that respects the observed pooled category counts of the Likert variable.\n\nFundamental base:\n- Under the null hypothesis of no group effect, group labels are exchangeable across the pooled observations.\n- Ties are handled by assigning midranks: all observations in the same category receive the average of the ranks they would occupy if broken arbitrarily.\n- The Wilcoxon rank-sum statistic $W$ for a group of size $n_1$ is the sum of the midranks of the observations in that group.\n- The total number of pooled observations is $N = n_1 + n_2$, and the mean of the null distribution of $W$ is $\\mu = n_1 (N + 1)/2$.\n\nYour program must:\n1. Compute midranks from the pooled data by sorting unique Likert categories in ascending order and assigning, for each tie block of pooled size $c_k$, the midrank $m_k$ equal to the average of the block’s ranks.\n2. Express the Wilcoxon rank-sum statistic as $W = \\sum_{k} x_k \\, m_k$, where $x_k$ is the number of group-$A$ observations in category $k$. The vector $(x_k)$ ranges over all integer allocations satisfying $0 \\le x_k \\le c_k$ and $\\sum_k x_k = n_1$.\n3. Use exact enumeration weighted by combinatorial counts to construct the exact null distribution of $W$. Each allocation $(x_k)$ corresponds to $\\prod_k \\binom{c_k}{x_k}$ distinct labelings, each equally likely under the null. The total number of labelings is $\\binom{N}{n_1}$. Therefore, the probability of an allocation $(x_k)$ is $\\left(\\prod_k \\binom{c_k}{x_k}\\right) \\big/ \\binom{N}{n_1}$.\n4. Aggregate allocations with the same $W$ to get the exact probability mass function of $W$, and compute:\n   - The observed statistic $w_{\\text{obs}}$ from the actual group assignments.\n   - The left one-sided $p$-value $p_{\\text{left}} = \\mathbb{P}(W \\le w_{\\text{obs}})$.\n   - The right one-sided $p$-value $p_{\\text{right}} = \\mathbb{P}(W \\ge w_{\\text{obs}})$.\n   - The two-sided $p$-value defined by the distance from the mean: $p_{\\text{two}} = \\mathbb{P}(|W - \\mu| \\ge |w_{\\text{obs}} - \\mu|)$.\n5. To avoid fractional midranks in computation, scale all midranks by a factor of $2$ so that $W$ becomes an integer-valued statistic $W^{\\ast} = 2 W$, and similarly $\\mu^{\\ast} = 2 \\mu = n_1 (N + 1)$. Use this scaled form to perform exact enumeration and compute probabilities.\n\nTest suite:\nImplement your program to process the following four test cases. In each case, group $A$ and group $B$ are provided as lists of Likert responses. All Likert values are integers, and midranks must be computed from the pooled data.\n\n- Case $1$ (happy path with multiple ties):\n  - Group $A$: $[$1$,$2$,$4$,$4$,$5$]$\n  - Group $B$: $[$1$,$1$,$3$,$4$,$5$,$5$,$5$]$\n\n- Case $2$ (degenerate all-equal category):\n  - Group $A$: $[$3$,$3$]$\n  - Group $B$: $[$3$,$3$,$3$]$\n\n- Case $3$ (no ties in the pooled data):\n  - Group $A$: $[$1$,$2$]$\n  - Group $B$: $[$3$,$4$,$5$]$\n\n- Case $4$ (balanced sizes with central ties):\n  - Group $A$: $[$2$,$2$,$3$,$4$]$\n  - Group $B$: $[$1$,$2$,$3$,$3$]$\n\nRequired output:\n- For each case, output a list $[p_{\\text{two}}, p_{\\text{left}}, p_{\\text{right}}]$ where each entry is a float rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these per-case lists, enclosed in square brackets. For example: $[[p_{1,\\text{two}},p_{1,\\text{left}},p_{1,\\text{right}}],[p_{2,\\text{two}},p_{2,\\text{left}},p_{2,\\text{right}}],\\dots]$.", "solution": "The user-provided problem is a valid and well-posed question in the domain of biostatistics, specifically concerning non-parametric hypothesis testing with discrete data. All provided information is scientifically sound, self-consistent, and sufficient for deriving a unique solution. The core of the problem is to implement an exact permutation test for the Wilcoxon rank-sum statistic, accounting for tied ranks, a standard procedure in statistical analysis.\n\nThe methodology is outlined as follows:\n\n1.  **Data Pooling and Rank Calculation**: For each test case, the data from group $A$ (size $n_1$) and group $B$ (size $n_2$) are pooled into a single dataset of size $N = n_1 + n_2$. The unique values (Likert categories) are identified and sorted. Let there be $K$ unique categories. For each category $k \\in \\{1, \\dots, K\\}$, we count its total frequency $c_k$ in the pooled data.\n    The ranks of the $c_k$ observations in category $k$ occupy the positions from $\\left(1 + \\sum_{j=1}^{k-1} c_j\\right)$ to $\\left(\\sum_{j=1}^{k} c_j\\right)$. The midrank $m_k$ for category $k$ is the average of these ranks:\n    $$m_k = \\frac{\\left(1 + \\sum_{j=1}^{k-1} c_j\\right) + \\left(\\sum_{j=1}^{k} c_j\\right)}{2} = \\left(\\sum_{j=1}^{k-1} c_j\\right) + \\frac{c_k + 1}{2}$$\n    To avoid floating-point arithmetic during enumeration, we use a scaled midrank $m_k^* = 2m_k$, which is guaranteed to be an integer:\n    $$m_k^* = 2\\left(\\sum_{j=1}^{k-1} c_j\\right) + c_k + 1$$\n\n2.  **Observed Statistic and Null Hypothesis Parameters**: The Wilcoxon rank-sum statistic for group $A$, $W$, is the sum of the ranks of its observations. This can be expressed in terms of category counts. Let $x_k$ be the number of observations from group $A$ that fall into category $k$. The scaled statistic $W^*$ is then:\n    $$W^* = \\sum_{k=1}^{K} x_k m_k^*$$\n    We compute the observed value of this statistic, $w_{\\text{obs}}^*$, using the actual counts $x_{k, \\text{obs}}$ from the provided group $A$ data.\n    Under the null hypothesis of exchangeability, the mean of the distribution of $W$ is $\\mu = \\frac{n_1(N+1)}{2}$. The scaled mean is $\\mu^* = 2\\mu = n_1(N+1)$, which is an integer.\n\n3.  **Exact Enumeration of the Null Distribution**: The crucial step is to construct the exact probability mass function (PMF) of $W^*$ under the null hypothesis. The null hypothesis implies that any subset of $n_1$ observations from the pooled set of $N$ is equally likely to form group $A$. The total number of ways to choose group $A$ is $\\binom{N}{n_1}$.\n    An equivalent perspective, which is computationally more efficient, is to consider all possible allocations of the $n_1$ observations of group $A$ into the $K$ categories. An allocation is a vector of counts $(x_1, x_2, \\dots, x_K)$ satisfying the constraints:\n    $$0 \\le x_k \\le c_k \\quad \\text{for all } k=1, \\dots, K$$\n    $$\\sum_{k=1}^{K} x_k = n_1$$\n    For each such valid allocation, the number of ways to form group $A$ is the product of binomial coefficients $\\prod_{k=1}^{K} \\binom{c_k}{x_k}$. The probability of this allocation is:\n    $$\\mathbb{P}(x_1, \\dots, x_K) = \\frac{\\prod_{k=1}^{K} \\binom{c_k}{x_k}}{\\binom{N}{n_1}}$$\n    We generate all valid allocation vectors $(x_k)$ using a recursive backtracking algorithm. For each allocation, we compute the corresponding statistic $W^* = \\sum_k x_k m_k^*$ and its probability. The full PMF of $W^*$ is constructed by summing the probabilities of all allocations that yield the same value of $W^*$.\n\n4.  **P-value Calculation**: With the complete PMF of $W^*$, we calculate the required p-values based on the observed statistic $w_{\\text{obs}}^*$ and the scaled mean $\\mu^*$:\n    -   **Left-sided p-value**: $p_{\\text{left}} = \\mathbb{P}(W^* \\le w_{\\text{obs}}^*) = \\sum_{w^* \\le w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **Right-sided p-value**: $p_{\\text{right}} = \\mathbb{P}(W^* \\ge w_{\\text{obs}}^*) = \\sum_{w^* \\ge w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **Two-sided p-value**: This is based on the distance from the mean. Let $d_{\\text{obs}} = |w_{\\text{obs}}^* - \\mu^*|$. The p-value is the probability of observing a result at least as extreme:\n        $$p_{\\text{two}} = \\mathbb{P}(|W^* - \\mu^*| \\ge d_{\\text{obs}}) = \\sum_{w^* \\text{ s.t. } |w^*-\\mu^*| \\ge d_{\\text{obs}}} \\mathbb{P}(W^*=w^*)$$\n    Since $W^*$, $\\mu^*$, and $w_{\\text{obs}}^*$ are all integers, these calculations are exact and do not involve floating-point comparison issues.\n\nThe implementation will follow this logic. A recursive function will handle the enumeration of allocations. The `scipy.special.comb` function will be used for calculating binomial coefficients to ensure accuracy and handle potentially large numbers, although the test cases are small enough that this is not a major concern. Finally, the computed p-values are rounded to $6$ decimal places as required.", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom collections import defaultdict\n\ndef calculate_p_values(group_a, group_b):\n    \"\"\"\n    Computes exact permutation p-values for the Wilcoxon rank-sum test with ties.\n    \n    Args:\n        group_a (list): List of observations for group A.\n        group_b (list): List of observations for group B.\n        \n    Returns:\n        list: A list containing [p_two, p_left, p_right] rounded to 6 decimal places.\n    \"\"\"\n    \n    # Step 1: Initialize data and basic parameters\n    n1 = len(group_a)\n    n2 = len(group_b)\n    N = n1 + n2\n    \n    # Handle trivial case where a group is empty\n    if n1 == 0 or n2 == 0:\n        return [1.0, 1.0, 1.0]\n\n    pooled_data = np.concatenate((group_a, group_b))\n    \n    # Step 2: Compute category counts and scaled midranks\n    unique_cats, cat_indices = np.unique(pooled_data, return_inverse=True)\n    num_cats = len(unique_cats)\n    \n    # c_k: counts of each unique category in the pooled data\n    c_k = np.bincount(cat_indices, minlength=num_cats)\n    \n    # x_k_obs: observed counts of each unique category in group A\n    group_a_cats = cat_indices[:n1]\n    x_k_obs = np.bincount(group_a_cats, minlength=num_cats)\n    \n    # m_star_k: scaled midranks for each category\n    cum_c = np.concatenate(([0], np.cumsum(c_k)))\n    m_star_k = 2 * cum_c[:-1] + c_k + 1\n\n    # Step 3: Compute observed statistic and mean\n    w_star_obs = np.sum(x_k_obs * m_star_k)\n    mu_star = n1 * (N + 1)\n\n    # Step 4: Enumerate all possible allocations and build the null distribution\n    distribution = defaultdict(float)\n    total_perms = comb(N, n1, exact=True)\n\n    allocations = []\n    def find_allocations(k_idx, n1_rem, current_alloc):\n        \"\"\"\n        Recursively find all valid allocations (x_k vectors).\n        \"\"\"\n        if k_idx == num_cats:\n            if n1_rem == 0:\n                allocations.append(current_alloc)\n            return\n\n        # Iterate through possible counts x_k for the current category\n        max_count = min(c_k[k_idx], n1_rem)\n        for x_k in range(max_count + 1):\n            find_allocations(k_idx + 1, n1_rem - x_k, current_alloc + [x_k])\n\n    find_allocations(0, n1, [])\n\n    for alloc_vec in allocations:\n        # Calculate the statistic W* for this allocation\n        w_star = np.sum(np.array(alloc_vec) * m_star_k)\n        \n        # Calculate the number of permutations for this allocation\n        prob_numerator = 1\n        for i in range(num_cats):\n            prob_numerator *= comb(c_k[i], alloc_vec[i], exact=True)\n            \n        # Add probability to the distribution map\n        probability = prob_numerator / total_perms\n        distribution[w_star] += probability\n\n    # Step 5: Calculate p-values from the null distribution\n    p_left = 0.0\n    p_right = 0.0\n    p_two = 0.0\n    \n    # d_obs is integer, as are all w_star and mu_star\n    d_obs = abs(w_star_obs - mu_star)\n\n    for w_star, prob in distribution.items():\n        if w_star <= w_star_obs:\n            p_left += prob\n        if w_star >= w_star_obs:\n            p_right += prob\n        if abs(w_star - mu_star) >= d_obs:\n            p_two += prob\n            \n    # Round results to 6 decimal places as required\n    return [round(p_two, 6), round(p_left, 6), round(p_right, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path with multiple ties)\n        {'group_a': [1, 2, 4, 4, 5], 'group_b': [1, 1, 3, 4, 5, 5, 5]},\n        \n        # Case 2 (degenerate all-equal category)\n        {'group_a': [3, 3], 'group_b': [3, 3, 3]},\n        \n        # Case 3 (no ties in the pooled data)\n        {'group_a': [1, 2], 'group_b': [3, 4, 5]},\n        \n        # Case 4 (balanced sizes with central ties)\n        {'group_a': [2, 2, 3, 4], 'group_b': [1, 2, 3, 3]}\n    ]\n    \n    results = []\n    for case in test_cases:\n        p_values = calculate_p_values(case['group_a'], case['group_b'])\n        results.append(p_values)\n        \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4946639"}, {"introduction": "This final practice moves from hypothesis testing to correlation, exploring the fundamental reason *why* rank-based methods are so valuable. Through a simulation study, you will compare the performance of Spearman's rank correlation with the classic Pearson's correlation in the face of non-linear relationships and extreme outliers. This hands-on comparison will provide a clear, empirical demonstration of the concept of robustness, a key strength of non-parametric statistics [@problem_id:4946608].", "problem": "You will implement a simulation study to compare the empirical robustness of Spearman’s rank correlation coefficient (Spearman’s $\\rho_s$) and Pearson’s product-moment correlation coefficient (Pearson’s $\\rho$) under monotone nonlinear relationships with heavy-tailed noise and point-mass contamination. The study must be based on first principles: definitions of sample correlation, rank transformation, and the basic properties of monotone increasing functions and heavy-tailed distributions.\n\nUse the following foundational bases appropriate to ranked data in biostatistics:\n- The sample Pearson correlation between two real-valued vectors $x$ and $y$ of common length $n$ is defined as the standardized sample covariance of $x$ and $y$.\n- The sample Spearman correlation between two real-valued vectors $x$ and $y$ is the sample Pearson correlation computed on the rank-transformed data, where each observation is replaced by its rank among all observations (with average ranks for ties).\n- A function $g:\\mathbb{R}\\to\\mathbb{R}$ is monotone increasing if for all $x_1,x_2\\in\\mathbb{R}$, $x_1\\le x_2$ implies $g(x_1)\\le g(x_2)$.\n- Heavy-tailed noise is represented by a Student’s $t$ distribution with low degrees of freedom.\n\nYour program must perform the following steps exactly:\n1. For each test case, simulate $R$ independent replicates of a dataset $\\{(X_i,Y_i)\\}_{i=1}^n$ as follows:\n   - Generate $X_i \\sim \\mathcal{N}(0,1)$ independently for $i=1,\\dots,n$.\n   - Generate independent noise $E_i \\sim t_{\\nu}$ (Student’s $t$ with $\\nu$ degrees of freedom). Multiply the noise by a fixed scale $s$ to control its magnitude, resulting in $s\\,E_i$.\n   - Choose a monotone increasing function $g$ and set the clean response as $Y_i^{(\\mathrm{clean})}=g(X_i)+s\\,E_i$.\n   - Create a contaminated response $Y_i^{(\\mathrm{cont})}$ by selecting $m=\\lfloor p\\,n\\rfloor$ indices uniformly without replacement and replacing those $m$ values via $Y_i^{(\\mathrm{cont})}=Y_i^{(\\mathrm{clean})}+M\\cdot S_i$, where $S_i$ are independent Rademacher signs taking values in $\\{-1,+1\\}$ with equal probability.\n2. For each replicate, compute:\n   - The sample Pearson correlation on the clean data, $\\widehat{\\rho}_{P}^{(\\mathrm{clean})}$, and on the contaminated data, $\\widehat{\\rho}_{P}^{(\\mathrm{cont})}$.\n   - The sample Spearman correlation on the clean data, $\\widehat{\\rho}_{S}^{(\\mathrm{clean})}$, and on the contaminated data, $\\widehat{\\rho}_{S}^{(\\mathrm{cont})}$.\n3. For each replicate, compute the absolute changes\n   $$\\Delta_P=\\left|\\widehat{\\rho}_{P}^{(\\mathrm{cont})}-\\widehat{\\rho}_{P}^{(\\mathrm{clean})}\\right|,\\qquad \\Delta_S=\\left|\\widehat{\\rho}_{S}^{(\\mathrm{cont})}-\\widehat{\\rho}_{S}^{(\\mathrm{clean})}\\right|.$$\n4. Average these absolute changes across the $R$ replicates to obtain $\\overline{\\Delta}_P$ and $\\overline{\\Delta}_S$ for the test case.\n5. For each test case, return a boolean indicating whether $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$.\n\nScientific rationale that your implementation must adhere to:\n- Monotone increasing transformations preserve ranks. Therefore, in the absence of contamination, Spearman’s correlation is invariant under strictly monotone increasing transformations of the marginals, while Pearson’s correlation is not.\n- Heavy-tailed noise increases the frequency and magnitude of extreme values, inflating sensitivity of estimators relying on finite second moments. Pearson’s correlation is known to be more sensitive to such extremes compared to rank-based measures.\n- Point-mass contamination introduces extreme outliers that primarily stress robustness.\n\nUse the following fixed constants across all test cases:\n- Number of replicates $R = 400$.\n- Noise scale $s = 0.5$.\n- Contamination magnitude $M = 25$.\n- Random number generator seed fixed to an integer of your choice to ensure reproducibility.\n\nUse the following monotone increasing functions $g$:\n- $g_{\\mathrm{lin}}(x)=x$.\n- $g_{\\exp}(x)=\\exp(x/2)$.\n- $g_{\\mathrm{logit}}(x)=\\frac{1}{1+\\exp(-x)}$.\n- $g_{\\mathrm{cub}}(x)=x^3$.\n\nTest Suite (provide results in this exact order):\n- Case $1$: $g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.1$.\n- Case $2$: $g=g_{\\exp}$, $n=200$, $\\nu=3$, $p=0.1$.\n- Case $3$: $g=g_{\\mathrm{logit}}$, $n=200$, $\\nu=1$, $p=0.1$.\n- Case $4$: $g=g_{\\mathrm{cub}}$, $n=80$, $\\nu=3$, $p=0.3$.\n- Case $5$: $g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.0$.\n\nFinal output specification:\n- Your program should produce a single line of output containing a list of $5$ booleans corresponding to the $5$ cases above in order. The $k$-th boolean must be $\\mathrm{True}$ if and only if $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ for case $k$, and $\\mathrm{False}$ otherwise.\n- The line must be formatted as a Python-style list with commas and no spaces, for example: $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False}]$.\n- No additional text should be printed.\n\nNo physical units are involved. Angles are not involved. Any proportions must be returned as decimals, but your final output here consists only of booleans. The program must be fully self-contained and require no user input, external files, or network access.", "solution": "The user has requested the implementation of a simulation study to evaluate the robustness of Pearson's and Spearman's correlation coefficients. The problem is scientifically sound, well-posed, and all parameters and procedures are explicitly defined. Therefore, a solution is warranted.\n\n### Principle-Based Design\n\nThe core of this problem lies in comparing the robustness of two common measures of statistical association: the Pearson product-moment correlation coefficient, $\\widehat{\\rho}_P$, and the Spearman rank correlation coefficient, $\\widehat{\\rho}_S$. Robustness, in this context, refers to the stability of an estimator in the presence of deviations from ideal assumptions, such as outliers or non-normal error distributions.\n\n**1. Theoretical Foundations**\n\n**Pearson Correlation ($\\widehat{\\rho}_P$)**: The sample Pearson correlation coefficient measures the linear association between two continuous variables, $x$ and $y$. For a sample of size $n$, $\\{(x_i, y_i)\\}_{i=1}^n$, it is defined as the sample covariance divided by the product of the sample standard deviations:\n$$ \\widehat{\\rho}_P(x,y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means. This formula reveals that $\\widehat{\\rho}_P$ is computed directly from the raw data values. Consequently, it is highly sensitive to outliers. A single extreme data point can arbitrarily inflate or deflate the sums of squares and cross-products, disproportionately influencing the means, variances, and covariance, thereby distorting the correlation estimate.\n\n**Spearman Correlation ($\\widehat{\\rho}_S$)**: The sample Spearman correlation coefficient assesses the strength of a monotonic relationship between two variables. It is a non-parametric measure. Its key feature is that it operates not on the raw data, but on the ranks of the data. The procedure involves:\n1. Transforming the vector $x$ into a vector of its ranks, $R_x$.\n2. Transforming the vector $y$ into a vector of its ranks, $R_y$. In case of ties, the average rank is assigned.\n3. Calculating the Pearson correlation coefficient of these rank-transformed vectors:\n$$ \\widehat{\\rho}_S(x,y) = \\widehat{\\rho}_P(R_x, R_y) $$\nBy converting data to ranks, the influence of the actual magnitudes is removed. An extreme outlier is simply assigned the highest or lowest rank (e.g., $1$ or $n$), a value on the same integer scale as all other ranks. This transformation bounds the influence of any single observation, making $\\widehat{\\rho}_S$ inherently more robust to outliers than $\\widehat{\\rho}_P$.\n\n**2. Simulation Methodology**\n\nThe simulation is designed to create datasets where these theoretical differences in robustness can be empirically quantified.\n\n**Data Generation**:\n- The predictor variable $X_i$ is drawn from a standard normal distribution, $X_i \\sim \\mathcal{N}(0,1)$, which is a common baseline.\n- A monotone increasing function $g$ is applied, $g(X_i)$, to create a non-linear (or linear, for $g(x)=x$) relationship. Since $\\widehat{\\rho}_S$ measures monotonic association, it is expected to perform well even when the relationship is not linear, unlike $\\widehat{\\rho}_P$.\n- Heavy-tailed noise, $s \\cdot E_i$ where $E_i \\sim t_{\\nu}$, is added. A Student's $t$-distribution with low degrees of freedom $\\nu$ (e.g., $\\nu=1$ for the Cauchy distribution, $\\nu=3$) has heavier tails than a normal distribution, meaning it produces extreme values more frequently. This tests the estimators' sensitivity to non-Gaussian noise.\n- The resulting \"clean\" data is $Y_i^{(\\mathrm{clean})} = g(X_i) + s \\cdot E_i$.\n- Point-mass contamination is introduced to a fraction $p$ of the data. A large magnitude shock, $M \\cdot S_i$, is added to $m = \\lfloor p \\cdot n\\rfloor$ randomly chosen observations to create $Y_i^{(\\mathrm{cont})}$. This models gross measurement errors or data corruption, creating extreme outliers designed to stress-test the estimators.\n\n**Quantifying Robustness**:\nThe simulation measures robustness by quantifying how much each correlation estimate changes when the data is contaminated. For each of the $R=400$ replicates, we compute:\n- The change in Pearson correlation: $\\Delta_P = |\\widehat{\\rho}_{P}^{(\\mathrm{cont})} - \\widehat{\\rho}_{P}^{(\\mathrm{clean})}|$.\n- The change in Spearman correlation: $\\Delta_S = |\\widehat{\\rho}_{S}^{(\\mathrm{cont})} - \\widehat{\\rho}_{S}^{(\\mathrm{clean})}|$.\n\nA smaller $\\Delta$ indicates greater robustness. By averaging these changes over many replicates, we obtain stable estimates, $\\overline{\\Delta}_P$ and $\\overline{\\Delta}_S$, of the expected impact of contamination on each coefficient. The final comparison, $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$, provides a direct verdict on which estimator is more robust under the specified conditions.\n\n**3. Implementation Plan**\n\nThe program will be implemented in Python using the `numpy` and `scipy` libraries.\n- A fixed integer seed for the random number generator (`numpy.random.default_rng`) ensures reproducibility.\n- The data generation process follows the specified steps precisely. `numpy.random.Generator.normal` and `numpy.random.Generator.standard_t` will be used to generate the random variates.\n- The contamination is applied by first creating a copy of the clean response vector, then selecting indices without replacement using `numpy.random.Generator.choice`, and finally adding the scaled Rademacher noise.\n- To compute Pearson correlation, `numpy.corrcoef` is used.\n- To compute Spearman correlation as per its definition, `scipy.stats.rankdata` (with `method='average'` for ties) is first used to obtain the ranks of $X$ and $Y$, and then `numpy.corrcoef` is applied to these rank vectors.\n- The main logic iterates through the five test cases, performing $R$ replicates for each. Within each case, the absolute changes $\\Delta_P$ and $\\Delta_S$ are calculated in each replicate and accumulated.\n- After all replicates for a case are complete, the averages $\\overline{\\Delta}_P$ and $\\overline{\\Delta}_S$ are computed, and the boolean result of the comparison $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ is stored.\n- Finally, the list of booleans for all test cases is printed in the specified format. The special case where $p=0.0$ serves as a control; with no contamination, both $\\Delta_P$ and $\\Delta_S$ should be zero, making the inequality $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ hold true. For all other cases, theory predicts a similar outcome due to the superior robustness of the rank-based Spearman correlation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef solve():\n    \"\"\"\n    Implements a simulation study to compare the robustness of Pearson's and \n    Spearman's correlation coefficients under various conditions.\n    \"\"\"\n    # Define fixed constants for the simulation\n    R = 400  # Number of replicates\n    S_SCALE = 0.5  # Noise scale s\n    M_CONTAMINATION = 25.0  # Contamination magnitude M\n    SEED = 42  # Fixed seed for reproducibility\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(SEED)\n\n    # Define the monotone increasing functions\n    def g_lin(x):\n        return x\n\n    def g_exp(x):\n        return np.exp(x / 2.0)\n\n    def g_logit(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def g_cub(x):\n        return x**3\n\n    # Define the test suite\n    test_cases = [\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_lin'},\n        {'g': g_exp, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_exp'},\n        {'g': g_logit, 'n': 200, 'nu': 1, 'p': 0.1, 'name': 'g_logit'},\n        {'g': g_cub, 'n': 80, 'nu': 3, 'p': 0.3, 'name': 'g_cub'},\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.0, 'name': 'g_lin_no_cont'},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        g, n, nu, p = case['g'], case['n'], case['nu'], case['p']\n        \n        total_delta_p = 0.0\n        total_delta_s = 0.0\n\n        for _ in range(R):\n            # Step 1: Generate data\n            # Generate X_i ~ N(0, 1)\n            x_data = rng.normal(loc=0.0, scale=1.0, size=n)\n            \n            # Generate noise E_i ~ t_nu\n            noise = rng.standard_t(df=nu, size=n)\n            \n            # Generate clean response Y_i^(clean)\n            y_clean = g(x_data) + S_SCALE * noise\n            \n            # Create contaminated response Y_i^(cont)\n            y_cont = np.copy(y_clean)\n            m = int(np.floor(p * n))\n            \n            if m > 0:\n                indices_to_contaminate = rng.choice(n, size=m, replace=False)\n                signs = rng.choice([-1, 1], size=m)\n                y_cont[indices_to_contaminate] += M_CONTAMINATION * signs\n            \n            # Step 2: Compute correlations\n            # Pearson correlations\n            rho_p_clean = np.corrcoef(x_data, y_clean)[0, 1]\n            rho_p_cont = np.corrcoef(x_data, y_cont)[0, 1]\n\n            # Spearman correlations (as Pearson on ranks)\n            rank_x = rankdata(x_data, method='average')\n            rank_y_clean = rankdata(y_clean, method='average')\n            rank_y_cont = rankdata(y_cont, method='average')\n            \n            rho_s_clean = np.corrcoef(rank_x, rank_y_clean)[0, 1]\n            rho_s_cont = np.corrcoef(rank_x, rank_y_cont)[0, 1]\n\n            # Step 3: Compute absolute changes\n            delta_p = np.abs(rho_p_cont - rho_p_clean)\n            delta_s = np.abs(rho_s_cont - rho_s_clean)\n            \n            total_delta_p += delta_p\n            total_delta_s += delta_s\n\n        # Step 4: Average absolute changes\n        avg_delta_p = total_delta_p / R\n        avg_delta_s = total_delta_s / R\n\n        # Step 5: Compare and store boolean result\n        results.append(avg_delta_s <= avg_delta_p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4946608"}]}