{"hands_on_practices": [{"introduction": "A cornerstone of Bayesian analysis is the use of conjugate priors, which provide an elegant and computationally convenient framework for updating beliefs. This exercise focuses on the Poisson-Gamma conjugate pair, a fundamental model in biostatistics for analyzing count data, such as the incidence of a disease. By working through this problem, you will gain direct experience with the mechanics of Bayesian updating, from deriving the posterior distribution for a model parameter to generating predictions for future data. Furthermore, you will develop an intuitive understanding of how prior hyperparameters act as \"pseudo-data,\" representing the strength and substance of our initial beliefs. [@problem_id:4912491]", "problem": "A hospital epidemiology unit monitors incident counts of methicillin-resistant Staphylococcus aureus colonizations across wards. For ward $i$, the observed count is $y_{i}$ over exposure $t_{i}$ person-days. Assume a constant incidence rate per person-day, denoted by $\\lambda$, and that, conditional on $\\lambda$, counts are independent with $y_{i} \\sim \\mathrm{Poisson}(\\lambda t_{i})$ for $i = 1, \\dots, n$. The prior for $\\lambda$ is $\\mathrm{Gamma}(\\alpha_{0}, \\beta_{0})$ with shape $\\alpha_{0}  0$ and rate $\\beta_{0}  0$, meaning its density is $p(\\lambda) = \\frac{\\beta_{0}^{\\alpha_{0}}}{\\Gamma(\\alpha_{0})} \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0} \\lambda)$ for $\\lambda  0$.\n\nStarting from Bayes’ theorem and the definitions of the Poisson and Gamma densities (no other formulas are provided), derive the posterior distribution $p(\\lambda \\mid y_{1:n}, t_{1:n})$ and the posterior predictive distribution $p(y_{\\mathrm{new}} \\mid y_{1:n}, t_{1:n}, t_{\\mathrm{new}})$ for a future count $y_{\\mathrm{new}}$ over a new exposure $t_{\\mathrm{new}}  0$. Then, interpret the roles of $\\alpha_{0}$ and $\\beta_{0}$ as pseudo-counts and pseudo-time in terms of how they combine with the observed data.\n\nExpress your final answer as the closed-form analytical expression of the posterior predictive probability mass function for $y_{\\mathrm{new}}$ in terms of $\\alpha_{0}$, $\\beta_{0}$, $\\sum_{i=1}^{n} y_{i}$, $\\sum_{i=1}^{n} t_{i}$, and $t_{\\mathrm{new}}$, using the Gamma function. No rounding is required.", "solution": "The problem is a standard Bayesian inference exercise involving a Poisson likelihood and a Gamma prior. Before proceeding to the solution, the problem statement must be validated.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Observed count for ward $i$: $y_i$\n-   Exposure for ward $i$: $t_i$ person-days\n-   Model for counts: $y_{i} \\sim \\mathrm{Poisson}(\\lambda t_{i})$, conditional on $\\lambda$, for $i = 1, \\dots, n$.\n-   Counts are independent.\n-   Incidence rate parameter: $\\lambda  0$.\n-   Prior for $\\lambda$: $\\mathrm{Gamma}(\\alpha_{0}, \\beta_{0})$.\n-   Prior shape parameter: $\\alpha_{0}  0$.\n-   Prior rate parameter: $\\beta_{0}  0$.\n-   Prior probability density function (PDF): $p(\\lambda) = \\frac{\\beta_{0}^{\\alpha_{0}}}{\\Gamma(\\alpha_{0})} \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0} \\lambda)$.\n-   Future count: $y_{\\mathrm{new}}$.\n-   New exposure: $t_{\\mathrm{new}}  0$.\n-   Future count model: $y_{\\mathrm{new}} \\sim \\mathrm{Poisson}(\\lambda t_{\\mathrm{new}})$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly grounded in established principles of biostatistics and Bayesian inference. Modeling count data with a Poisson distribution is a standard practice in epidemiology. The use of a Gamma distribution as a prior for the Poisson rate parameter is a canonical example of conjugate priors, a fundamental concept in Bayesian statistics.\n-   **Well-Posed:** The problem is well-posed. It provides a complete specification of the likelihood function and the prior distribution, from which the posterior and posterior predictive distributions can be uniquely derived through standard mathematical procedures.\n-   **Objective:** The problem is stated in precise, objective mathematical language. It contains no subjective claims or ambiguities.\n\nThe problem statement does not violate any of the invalidity criteria. It is a scientifically sound, well-posed, and objective problem formulation.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be derived.\n\n### Derivation of the Posterior and Posterior Predictive Distributions\n\nThe solution involves three main steps:\n1.  Derive the posterior distribution of the incidence rate $\\lambda$.\n2.  Interpret the prior parameters $\\alpha_0$ and $\\beta_0$.\n3.  Derive the posterior predictive distribution for a new observation $y_{\\mathrm{new}}$.\n\n**1. Posterior Distribution of $\\lambda$**\n\nWe start with Bayes' theorem, which states that the posterior distribution is proportional to the product of the likelihood and the prior distribution:\n$$p(\\lambda \\mid y_{1:n}, t_{1:n}) \\propto p(y_{1:n} \\mid \\lambda, t_{1:n}) p(\\lambda)$$\nLet's denote the data by $D = \\{y_{1:n}, t_{1:n}\\}$.\n\n**Likelihood:**\nThe observations $y_i$ are assumed to be independent draws from a Poisson distribution, so the joint likelihood is the product of the individual probability mass functions (PMFs):\n$$p(D \\mid \\lambda) = \\prod_{i=1}^{n} p(y_i \\mid \\lambda, t_i)$$\nThe PMF of a Poisson distribution with mean $\\lambda t_i$ is $p(y_i \\mid \\lambda, t_i) = \\frac{(\\lambda t_i)^{y_i} \\exp(-\\lambda t_i)}{y_i!}$.\nThus, the joint likelihood is:\n$$p(D \\mid \\lambda) = \\prod_{i=1}^{n} \\frac{(\\lambda t_i)^{y_i} \\exp(-\\lambda t_i)}{y_i!} = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\left( \\prod_{i=1}^{n} \\lambda^{y_i} \\right) \\left( \\prod_{i=1}^{n} \\exp(-\\lambda t_i) \\right)$$\nCombining terms, we get:\n$$p(D \\mid \\lambda) = \\left( \\prod_{i=1}^{n} \\frac{t_i^{y_i}}{y_i!} \\right) \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)$$\nAs a function of $\\lambda$, the likelihood is proportional to:\n$$p(D \\mid \\lambda) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)$$\n\n**Prior:**\nThe prior distribution for $\\lambda$ is given as a Gamma distribution $\\mathrm{Gamma}(\\alpha_0, \\beta_0)$:\n$$p(\\lambda) = \\frac{\\beta_{0}^{\\alpha_{0}}}{\\Gamma(\\alpha_{0})} \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0} \\lambda)$$\nAs a function of $\\lambda$, the prior is proportional to:\n$$p(\\lambda) \\propto \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0} \\lambda)$$\n\n**Posterior:**\nNow, we multiply the likelihood and the prior:\n$$p(\\lambda \\mid D) \\propto \\left( \\lambda^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right) \\right) \\left( \\lambda^{\\alpha_{0}-1} \\exp(-\\beta_{0} \\lambda) \\right)$$\n$$p(\\lambda \\mid D) \\propto \\lambda^{\\alpha_{0} + \\sum_{i=1}^{n} y_i - 1} \\exp\\left(-(\\beta_{0} + \\sum_{i=1}^{n} t_i) \\lambda\\right)$$\nThis functional form is the kernel of a Gamma distribution. We can identify the updated parameters of the posterior distribution:\n-   Posterior shape parameter: $\\alpha_n = \\alpha_0 + \\sum_{i=1}^{n} y_i$\n-   Posterior rate parameter: $\\beta_n = \\beta_0 + \\sum_{i=1}^{n} t_i$\n\nTherefore, the posterior distribution of $\\lambda$ is a Gamma distribution:\n$$\\lambda \\mid D \\sim \\mathrm{Gamma}\\left(\\alpha_0 + \\sum_{i=1}^{n} y_i, \\beta_0 + \\sum_{i=1}^{n} t_i\\right)$$\nThe full posterior PDF is:\n$$p(\\lambda \\mid D) = \\frac{(\\beta_0 + \\sum_{i=1}^{n} t_i)^{\\alpha_0 + \\sum_{i=1}^{n} y_i}}{\\Gamma(\\alpha_0 + \\sum_{i=1}^{n} y_i)} \\lambda^{\\alpha_0 + \\sum_{i=1}^{n} y_i - 1} \\exp\\left(-(\\beta_0 + \\sum_{i=1}^{n} t_i) \\lambda\\right)$$\n\n**2. Interpretation of Prior Parameters**\n\nThe forms of the posterior parameters $\\alpha_n$ and $\\beta_n$ provide a clear interpretation for the prior parameters $\\alpha_0$ and $\\beta_0$. The posterior shape $\\alpha_n$ is the sum of the prior shape $\\alpha_0$ and the total number of observed events, $\\sum y_i$. The posterior rate $\\beta_n$ is the sum of the prior rate $\\beta_0$ and the total observed exposure time, $\\sum t_i$.\nThis additive updating structure implies that $\\alpha_0$ can be interpreted as a **pseudo-count** of events from prior knowledge or a previous hypothetical experiment. Similarly, $\\beta_0$ can be interpreted as the corresponding **pseudo-time** (or pseudo-exposure) during which these $\\alpha_0$ events were 'observed'. The strength of the prior belief is determined by the magnitudes of $\\alpha_0$ and $\\beta_0$; larger values represent stronger prior information that requires more data to be overcome.\n\n**3. Posterior Predictive Distribution of $y_{\\mathrm{new}}$**\n\nThe posterior predictive distribution for a new observation $y_{\\mathrm{new}}$ given the observed data $D$ and a new exposure time $t_{\\mathrm{new}}$ is found by integrating the likelihood of $y_{\\mathrm{new}}$ over the posterior distribution of $\\lambda$:\n$$p(y_{\\mathrm{new}} \\mid D, t_{\\mathrm{new}}) = \\int_{0}^{\\infty} p(y_{\\mathrm{new}} \\mid \\lambda, t_{\\mathrm{new}}) p(\\lambda \\mid D) d\\lambda$$\nWe substitute the respective distributions:\n-   $p(y_{\\mathrm{new}} \\mid \\lambda, t_{\\mathrm{new}}) = \\frac{(\\lambda t_{\\mathrm{new}})^{y_{\\mathrm{new}}} \\exp(-\\lambda t_{\\mathrm{new}})}{y_{\\mathrm{new}}!}$\n-   $p(\\lambda \\mid D) = \\frac{\\beta_n^{\\alpha_n}}{\\Gamma(\\alpha_n)} \\lambda^{\\alpha_n - 1} \\exp(-\\beta_n \\lambda)$, where $\\alpha_n = \\alpha_0 + \\sum y_i$ and $\\beta_n = \\beta_0 + \\sum t_i$.\n\nThe integral becomes:\n$$p(y_{\\mathrm{new}} \\mid D, t_{\\mathrm{new}}) = \\int_{0}^{\\infty} \\left( \\frac{(\\lambda t_{\\mathrm{new}})^{y_{\\mathrm{new}}} \\exp(-\\lambda t_{\\mathrm{new}})}{y_{\\mathrm{new}}!} \\right) \\left( \\frac{\\beta_n^{\\alpha_n}}{\\Gamma(\\alpha_n)} \\lambda^{\\alpha_n - 1} \\exp(-\\beta_n \\lambda) \\right) d\\lambda$$\nWe group the terms that do not depend on $\\lambda$ outside the integral. We also use the identity $y_{\\mathrm{new}}! = \\Gamma(y_{\\mathrm{new}}+1)$:\n$$p(y_{\\mathrm{new}} \\mid D, t_{\\mathrm{new}}) = \\frac{t_{\\mathrm{new}}^{y_{\\mathrm{new}}}}{\\Gamma(y_{\\mathrm{new}}+1)} \\frac{\\beta_n^{\\alpha_n}}{\\Gamma(\\alpha_n)} \\int_{0}^{\\infty} \\lambda^{y_{\\mathrm{new}}} \\exp(-\\lambda t_{\\mathrm{new}}) \\lambda^{\\alpha_n - 1} \\exp(-\\beta_n \\lambda) d\\lambda$$\nCombine the terms involving $\\lambda$ inside the integral:\n$$p(y_{\\mathrm{new}} \\mid D, t_{\\mathrm{new}}) = \\frac{t_{\\mathrm{new}}^{y_{\\mathrm{new}}} \\beta_n^{\\alpha_n}}{\\Gamma(y_{\\mathrm{new}}+1) \\Gamma(\\alpha_n)} \\int_{0}^{\\infty} \\lambda^{(y_{\\mathrm{new}} + \\alpha_n) - 1} \\exp(-(\\beta_n + t_{\\mathrm{new}}) \\lambda) d\\lambda$$\nThe integral has the form of an unnormalized Gamma PDF. We use the identity $\\int_0^{\\infty} x^{a-1} \\exp(-bx) dx = \\frac{\\Gamma(a)}{b^a}$.\nHere, $a = y_{\\mathrm{new}} + \\alpha_n$ and $b = \\beta_n + t_{\\mathrm{new}}$. So the integral evaluates to:\n$$\\int_{0}^{\\infty} \\lambda^{(y_{\\mathrm{new}} + \\alpha_n) - 1} \\exp(-(\\beta_n + t_{\\mathrm{new}}) \\lambda) d\\lambda = \\frac{\\Gamma(y_{\\mathrm{new}} + \\alpha_n)}{(\\beta_n + t_{\\mathrm{new}})^{y_{\\mathrm{new}} + \\alpha_n}}$$\nSubstituting this result back into our expression for the predictive probability:\n$$p(y_{\\mathrm{new}} \\mid D, t_{\\mathrm{new}}) = \\frac{t_{\\mathrm{new}}^{y_{\\mathrm{new}}} \\beta_n^{\\alpha_n}}{\\Gamma(y_{\\mathrm{new}}+1) \\Gamma(\\alpha_n)} \\frac{\\Gamma(y_{\\mathrm{new}} + \\alpha_n)}{(\\beta_n + t_{\\mathrm{new}})^{y_{\\mathrm{new}} + \\alpha_n}}$$\nRearranging the terms yields the final expression for the posterior predictive probability mass function:\n$$p(y_{\\mathrm{new}} \\mid D, t_{\\mathrm{new}}) = \\frac{\\Gamma(y_{\\mathrm{new}} + \\alpha_n)}{\\Gamma(y_{\\mathrm{new}}+1) \\Gamma(\\alpha_n)} \\frac{t_{\\mathrm{new}}^{y_{\\mathrm{new}}} \\beta_n^{\\alpha_n}}{(\\beta_n + t_{\\mathrm{new}})^{y_{\\mathrm{new}} + \\alpha_n}}$$\nThis is the PMF of a Negative Binomial distribution. Finally, we substitute back the definitions of $\\alpha_n$ and $\\beta_n$:\n$$p(y_{\\mathrm{new}} \\mid D, t_{\\mathrm{new}}) = \\frac{\\Gamma(y_{\\mathrm{new}} + \\alpha_0 + \\sum_{i=1}^{n} y_i)}{\\Gamma(y_{\\mathrm{new}}+1) \\Gamma(\\alpha_0 + \\sum_{i=1}^{n} y_i)} \\frac{t_{\\mathrm{new}}^{y_{\\mathrm{new}}} (\\beta_0 + \\sum_{i=1}^{n} t_i)^{\\alpha_0 + \\sum_{i=1}^{n} y_i}}{(\\beta_0 + \\sum_{i=1}^{n} t_i + t_{\\mathrm{new}})^{y_{\\mathrm{new}} + \\alpha_0 + \\sum_{i=1}^{n} y_i}}$$\nThis is the closed-form analytical expression for the posterior predictive PMF as requested.", "answer": "$$\n\\boxed{\\frac{\\Gamma(y_{\\mathrm{new}} + \\alpha_{0} + \\sum_{i=1}^{n} y_{i})}{\\Gamma(y_{\\mathrm{new}}+1) \\Gamma(\\alpha_{0} + \\sum_{i=1}^{n} y_{i})} \\frac{t_{\\mathrm{new}}^{y_{\\mathrm{new}}} (\\beta_{0} + \\sum_{i=1}^{n} t_{i})^{\\alpha_{0} + \\sum_{i=1}^{n} y_{i}}}{(\\beta_{0} + \\sum_{i=1}^{n} t_{i} + t_{\\mathrm{new}})^{y_{\\mathrm{new}} + \\alpha_{0} + \\sum_{i=1}^{n} y_{i}}}}\n$$", "id": "4912491"}, {"introduction": "Once the mechanics of Bayesian inference are understood, the pivotal question becomes which prior to select, particularly when aiming for objectivity. This practice explores this question by introducing the Jeffreys prior, a formal rule for generating priors that are invariant to how the model parameter is defined. You will derive this prior for a Bernoulli model, common in clinical trials, and contrast its predictive behavior with that of a simple uniform prior. This comparison reveals that the choice of a \"non-informative\" prior is a nuanced decision that can significantly influence conclusions, especially with the small sample sizes often found in pilot studies. [@problem_id:4912591]", "problem": "A pilot biostatistics study considers a binary clinical endpoint for a novel therapy, modeled as a Bernoulli random variable with parameter $\\theta \\in (0,1)$. The goal is to use a prior that reflects parameterization invariance and to compare its prior predictive behavior to a uniform prior for small sample sizes. Starting from first principles, treat the data $X \\in \\{0,1\\}$ as a single Bernoulli trial with likelihood and derive the Fisher information for $\\theta$. Use this to define the Jeffreys prior for the Bernoulli parameter. Then, consider a sample of size $n=3$ and compute the prior predictive probability of observing $3$ responders (all successes) under the Jeffreys prior. Also compute the prior predictive probability of $3$ responders under the $\\mathrm{Beta}(1,1)$ prior. Finally, report the ratio of these two prior predictive probabilities (Jeffreys prior divided by $\\mathrm{Beta}(1,1)$ prior) as a single exact value. Do not round; provide the exact ratio as a reduced fraction.", "solution": "The user wants to solve a problem in Bayesian biostatistics. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n-   **Model**: The data is from a binary clinical endpoint, modeled as a Bernoulli random variable.\n-   **Parameter**: The Bernoulli parameter is $\\theta \\in (0,1)$.\n-   **Data for Fisher Information**: A single Bernoulli trial $X \\in \\{0,1\\}$ with likelihood $P(X=x|\\theta)$.\n-   **Data for Predictive Probability**: A sample of size $n=3$.\n-   **Event of Interest**: Observing $k=3$ responders (successes) in the sample of size $n=3$.\n-   **Task 1**: Derive the Fisher information $I(\\theta)$ for the Bernoulli parameter.\n-   **Task 2**: Define the Jeffreys prior for $\\theta$ using $I(\\theta)$.\n-   **Task 3**: Compute the prior predictive probability of $k=3$ successes in $n=3$ trials under the Jeffreys prior, let's call this $P_J(Y=3|n=3)$.\n-   **Task 4**: Compute the prior predictive probability of $k=3$ successes in $n=3$ trials under the $\\mathrm{Beta}(1,1)$ prior, let's call this $P_U(Y=3|n=3)$.\n-   **Task 5**: Compute the ratio $\\frac{P_J(Y=3|n=3)}{P_U(Y=3|n=3)}$.\n-   **Required Format**: The final answer must be an exact reduced fraction.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The concepts of Bernoulli trials, Fisher information, Jeffreys priors, Beta priors, and prior predictive distributions are fundamental and standard in Bayesian statistics and its application in biostatistics. The problem is well-posed, with a clear sequence of calculations leading to a unique, deterministic result. The language is objective and precise. The problem is self-contained and does not contain any contradictions or ambiguities. It is a standard, non-trivial exercise in theoretical statistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n### Solution Derivation\n\nThe problem requires a step-by-step derivation, starting from the Fisher information to the final ratio of prior predictive probabilities.\n\n**1. Fisher Information for the Bernoulli Parameter**\n\nA single Bernoulli trial with success parameter $\\theta$ has the probability mass function $P(X=x|\\theta) = \\theta^x (1-\\theta)^{1-x}$ for $x \\in \\{0, 1\\}$. The likelihood function for a single observation $x$ is $L(\\theta|x) = \\theta^x (1-\\theta)^{1-x}$.\n\nThe log-likelihood is $\\ell(\\theta|x) = \\ln L(\\theta|x) = x \\ln(\\theta) + (1-x) \\ln(1-\\theta)$.\n\nTo find the Fisher information, $I(\\theta)$, we compute the negative expectation of the second derivative of the log-likelihood with respect to $\\theta$.\n\nFirst derivative:\n$$ \\frac{\\partial \\ell}{\\partial \\theta} = \\frac{x}{\\theta} - \\frac{1-x}{1-\\theta} $$\n\nSecond derivative:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\theta^2} = -\\frac{x}{\\theta^2} - \\frac{1-x}{(1-\\theta)^2} $$\n\nThe Fisher information for a single observation is $I(\\theta) = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right]$. The expectation is taken over the distribution of $X$, where $E[X] = \\theta$.\n$$ I(\\theta) = -E\\left[ -\\frac{X}{\\theta^2} - \\frac{1-X}{(1-\\theta)^2} \\right] = E\\left[ \\frac{X}{\\theta^2} \\right] + E\\left[ \\frac{1-X}{(1-\\theta)^2} \\right] $$\n$$ I(\\theta) = \\frac{E[X]}{\\theta^2} + \\frac{1-E[X]}{(1-\\theta)^2} = \\frac{\\theta}{\\theta^2} + \\frac{1-\\theta}{(1-\\theta)^2} = \\frac{1}{\\theta} + \\frac{1}{1-\\theta} $$\nSimplifying, we get:\n$$ I(\\theta) = \\frac{1-\\theta+\\theta}{\\theta(1-\\theta)} = \\frac{1}{\\theta(1-\\theta)} $$\n\n**2. Jeffreys Prior for the Bernoulli Parameter**\n\nThe Jeffreys prior, $p_J(\\theta)$, is defined as being proportional to the square root of the Fisher information:\n$$ p_J(\\theta) \\propto \\sqrt{I(\\theta)} = \\sqrt{\\frac{1}{\\theta(1-\\theta)}} = \\theta^{-1/2} (1-\\theta)^{-1/2} $$\nThis is the kernel of a Beta distribution, $p(\\theta|\\alpha, \\beta) \\propto \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$. By comparing the exponents, we have $\\alpha-1 = -1/2$ and $\\beta-1 = -1/2$, which gives $\\alpha = 1/2$ and $\\beta = 1/2$. Thus, the Jeffreys prior for the Bernoulli parameter is a $\\mathrm{Beta}(1/2, 1/2)$ distribution. The normalized probability density function is:\n$$ p_J(\\theta) = \\frac{\\theta^{1/2-1} (1-\\theta)^{1/2-1}}{B(1/2, 1/2)} = \\frac{\\theta^{-1/2} (1-\\theta)^{-1/2}}{\\frac{\\Gamma(1/2)\\Gamma(1/2)}{\\Gamma(1)}} = \\frac{1}{\\pi} \\theta^{-1/2} (1-\\theta)^{-1/2} $$\nwhere we use $\\Gamma(1/2) = \\sqrt{\\pi}$ and $\\Gamma(1) = 1$.\n\n**3. Prior Predictive Probability under the Jeffreys Prior**\n\nWe consider a sample of size $n=3$ and observe $k=3$ successes. Let $Y$ be the number of successes. The likelihood of this observation is given by the binomial probability mass function:\n$$ P(Y=3|\\theta, n=3) = \\binom{3}{3} \\theta^3 (1-\\theta)^{3-3} = \\theta^3 $$\nThe prior predictive probability, $P_J(Y=3|n=3)$, is the marginal probability of the data, obtained by integrating the likelihood against the prior distribution:\n$$ P_J(Y=3|n=3) = \\int_0^1 P(Y=3|\\theta, n=3) p_J(\\theta) d\\theta $$\n$$ P_J(Y=3|n=3) = \\int_0^1 \\theta^3 \\left( \\frac{1}{\\pi} \\theta^{-1/2} (1-\\theta)^{-1/2} \\right) d\\theta = \\frac{1}{\\pi} \\int_0^1 \\theta^{5/2} (1-\\theta)^{-1/2} d\\theta $$\nThe integral is in the form of a Beta function, $B(a, b) = \\int_0^1 t^{a-1} (1-t)^{b-1} dt = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$.\nHere, $a-1 = 5/2 \\Rightarrow a=7/2$ and $b-1 = -1/2 \\Rightarrow b=1/2$.\nThe integral is $B(7/2, 1/2)$. We compute this using properties of the Gamma function, $\\Gamma(z+1)=z\\Gamma(z)$ and $\\Gamma(n)=(n-1)!$ for integer $n$.\n$$ \\Gamma(7/2) = \\frac{5}{2}\\Gamma(5/2) = \\frac{5}{2}\\frac{3}{2}\\Gamma(3/2) = \\frac{5}{2}\\frac{3}{2}\\frac{1}{2}\\Gamma(1/2) = \\frac{15}{8}\\sqrt{\\pi} $$\n$$ \\Gamma(1/2) = \\sqrt{\\pi} $$\n$$ \\Gamma(7/2 + 1/2) = \\Gamma(4) = 3! = 6 $$\nSo, $B(7/2, 1/2) = \\frac{\\Gamma(7/2)\\Gamma(1/2)}{\\Gamma(4)} = \\frac{(\\frac{15}{8}\\sqrt{\\pi})(\\sqrt{\\pi})}{6} = \\frac{15\\pi}{48} = \\frac{5\\pi}{16}$.\nSubstituting this back into the expression for the prior predictive probability:\n$$ P_J(Y=3|n=3) = \\frac{1}{\\pi} B(7/2, 1/2) = \\frac{1}{\\pi} \\left( \\frac{5\\pi}{16} \\right) = \\frac{5}{16} $$\n\n**4. Prior Predictive Probability under the $\\mathrm{Beta}(1,1)$ Prior**\n\nThe $\\mathrm{Beta}(1,1)$ prior is the uniform prior on $(0,1)$, with probability density function $p_U(\\theta) = 1$ for $\\theta \\in (0,1)$.\nThe prior predictive probability, $P_U(Y=3|n=3)$, is:\n$$ P_U(Y=3|n=3) = \\int_0^1 P(Y=3|\\theta, n=3) p_U(\\theta) d\\theta $$\n$$ P_U(Y=3|n=3) = \\int_0^1 \\theta^3 \\cdot 1 d\\theta = \\left[ \\frac{\\theta^4}{4} \\right]_0^1 = \\frac{1}{4} - 0 = \\frac{1}{4} $$\n\n**5. Ratio of the Prior Predictive Probabilities**\n\nFinally, we compute the ratio of the prior predictive probability under the Jeffreys prior to that under the $\\mathrm{Beta}(1,1)$ prior.\n$$ \\text{Ratio} = \\frac{P_J(Y=3|n=3)}{P_U(Y=3|n=3)} = \\frac{5/16}{1/4} $$\n$$ \\text{Ratio} = \\frac{5}{16} \\times \\frac{4}{1} = \\frac{20}{16} = \\frac{5}{4} $$\nThis ratio demonstrates that the Jeffreys prior assigns a higher probability to extreme outcomes (all successes or all failures) compared to the uniform prior.", "answer": "$$\n\\boxed{\\frac{5}{4}}\n$$", "id": "4912591"}, {"introduction": "As statistical models grow in complexity, so do the challenges associated with prior specification. In applied models like logistic regression, improper priors—those that do not integrate to one—are sometimes used as a default, but this practice is not without peril. This exercise serves as a crucial diagnostic test of your understanding, challenging you to identify specific data patterns, such as complete separation and collinearity, that can cause a seemingly harmless flat prior to produce an invalid, improper posterior distribution. Mastering this skill is essential for any practitioner, as it highlights the critical interplay between data, model, and prior choice that underlies valid Bayesian inference. [@problem_id:4912450]", "problem": "Consider a binary outcome study with observations $\\{(y_i, x_i)\\}_{i=1}^n$, where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}$, modeled by a logistic regression with linear predictor $\\eta_i = \\beta_0 + \\beta_1 x_i$ and success probability $p_i = \\operatorname{logit}^{-1}(\\eta_i) = (1 + \\exp(-\\eta_i))^{-1}$. The likelihood under conditional independence is\n$$\nL(\\beta_0,\\beta_1) \\;=\\; \\prod_{i=1}^n p_i^{\\,y_i}\\,\\{1-p_i\\}^{\\,1-y_i}.\n$$\nSuppose a Bayesian analysis uses an improper flat prior on the regression coefficients, that is, $\\pi(\\beta_0,\\beta_1) \\propto 1$ on $\\mathbb{R}^2$. By Bayes’ rule, the posterior density is proportional to $L(\\beta_0,\\beta_1)\\,\\pi(\\beta_0,\\beta_1)$. A posterior is proper if and only if its normalizing constant $\\int_{\\mathbb{R}^2} L(\\beta_0,\\beta_1)\\,\\mathrm{d}\\beta_0\\,\\mathrm{d}\\beta_1$ is finite.\n\nYour task is to identify counterexamples in which this improper prior yields an improper posterior, and to connect each failure to the underlying data pattern. For each option below, a data scenario and fitted model are specified. Assume independence across observations and the stated improper flat prior on all free coefficients. Select all options in which the posterior is improper, and in each selected option identify the precise data pattern that causes the failure.\n\nA. Intercept-only model with all successes. Model: $p(y_i=1) = \\operatorname{logit}^{-1}(\\beta_0)$ with prior $\\pi(\\beta_0)\\propto 1$ on $\\mathbb{R}$. Data: $n=5$ with $(y_1,\\dots,y_5)=(1,1,1,1,1)$.\n\nB. Nonseparable, full-rank single-predictor model. Model: $p(y_i=1) = \\operatorname{logit}^{-1}(\\beta_0+\\beta_1 x_i)$ with prior $\\pi(\\beta_0,\\beta_1)\\propto 1$. Data: $n=4$ with $(x,y)$ pairs $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$.\n\nC. Perfectly collinear predictors without separation. Model: $p(y_i=1) = \\operatorname{logit}^{-1}(\\beta_0+\\beta_x x_i+\\beta_z z_i)$ with prior $\\pi(\\beta_0,\\beta_x,\\beta_z)\\propto 1$. Data: $n=4$ with $(x,z,y)$ pairs $(0,0,0)$, $(0,0,1)$, $(1,1,0)$, $(1,1,1)$.\n\nD. Complete separation by a single predictor. Model: $p(y_i=1) = \\operatorname{logit}^{-1}(\\beta_0+\\beta_1 x_i)$ with prior $\\pi(\\beta_0,\\beta_1)\\propto 1$. Data: $n=4$ with $(x,y)$ pairs $(-1,0)$, $(-1,0)$, $(+1,1)$, $(+1,1)$.\n\nChoose all that apply.", "solution": "The validity of the problem statement has been confirmed. The problem describes standard scenarios in Bayesian logistic regression and asks to identify conditions under which an improper flat prior leads to an improper posterior. This is a well-defined and fundamental question in Bayesian analysis. We will now proceed with the solution.\n\nThe posterior density is proper if and only if the likelihood function is integrable over the entire parameter space, given that the prior is an improper uniform prior, i.e., $\\pi(\\boldsymbol{\\beta}) \\propto 1$. The condition is $\\int L(\\boldsymbol{\\beta}) \\, \\mathrm{d}\\boldsymbol{\\beta}  \\infty$. An improper posterior arises if this integral diverges. This typically occurs under two conditions: (1) perfect collinearity in the design matrix, or (2) complete or quasi-complete separation of the data points. We analyze each option based on these principles.\n\n### Option A: Intercept-only model with all successes.\n**Model**: The success probability is $p_i = \\operatorname{logit}^{-1}(\\beta_0) = (1 + \\exp(-\\beta_0))^{-1}$. The prior is $\\pi(\\beta_0) \\propto 1$.\n**Data**: $n=5$ with all outcomes being successes, i.e., $y_i=1$ for $i=1, \\dots, 5$.\n**Analysis**: The likelihood function for this data is:\n$$\nL(\\beta_0) = \\prod_{i=1}^5 p_i^{y_i} (1-p_i)^{1-y_i} = \\prod_{i=1}^5 p_i^1 (1-p_i)^0 = p_i^5 = \\left(\\frac{1}{1 + \\exp(-\\beta_0)}\\right)^5\n$$\nTo determine if the posterior is proper, we must check if the integral of the likelihood over the parameter space $\\mathbb{R}$ is finite:\n$$\n\\int_{-\\infty}^{\\infty} L(\\beta_0) \\, \\mathrm{d}\\beta_0 = \\int_{-\\infty}^{\\infty} \\left(\\frac{1}{1 + \\exp(-\\beta_0)}\\right)^5 \\, \\mathrm{d}\\beta_0\n$$\nLet's examine the behavior of the integrand as $\\beta_0 \\to \\infty$:\n$$\n\\lim_{\\beta_0 \\to \\infty} L(\\beta_0) = \\lim_{\\beta_0 \\to \\infty} \\left(\\frac{1}{1 + \\exp(-\\beta_0)}\\right)^5 = \\left(\\frac{1}{1 + 0}\\right)^5 = 1\n$$\nSince the integrand approaches a non-zero constant ($1$) as $\\beta_0 \\to \\infty$, the integral over the interval $[M, \\infty)$ for any large $M$ will diverge. Therefore, the total integral is infinite. The posterior is improper.\n**Data Pattern Causing Failure**: The failure is caused by a lack of variation in the outcome variable (all $y_i$ are $1$). This is a form of **complete separation** for an intercept-only model, where the model can achieve a perfect fit by driving the probability $p_i$ to $1$, which corresponds to $\\beta_0 \\to \\infty$. The likelihood function does not decay, leading to a non-integrable posterior density.\n\n**Verdict**: **Correct**. The posterior is improper.\n\n### Option B: Nonseparable, full-rank single-predictor model.\n**Data**: $(x,y)$ pairs are $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$.\n**Analysis**: This data configuration exhibits **overlap**. At $x=0$, both outcomes $y=0$ and $y=1$ are observed. Similarly, at $x=1$, both outcomes are observed. This explicitly prevents complete separation. Complete separation requires the existence of a parameter vector $(\\beta_0, \\beta_1)$ such that $\\beta_0 + \\beta_1 x_i$ has a consistent sign for all $y_i=1$ and the opposite sign for all $y_i=0$. For this dataset, separation would require:\nFor $(0,0)$: $\\beta_0 + \\beta_1(0)  0 \\implies \\beta_0  0$.\nFor $(0,1)$: $\\beta_0 + \\beta_1(0)  0 \\implies \\beta_0  0$.\nThese conditions are contradictory. Thus, separation is impossible.\nThe design matrix is $\\mathbf{X} = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 1  1 \\\\ 1  1 \\end{pmatrix}$. The two columns are linearly independent, so the matrix has full rank.\nIt is a known result in Bayesian logistic regression that if the data are not completely or quasi-completely separable and the design matrix has full rank, the posterior resulting from a flat improper prior is proper.\n**Data Pattern**: The pattern is one of **overlap**, which is the opposite of separation. This ensures a proper posterior.\n\n**Verdict**: **Incorrect**. The posterior is proper.\n\n### Option C: Perfectly collinear predictors without separation.\n**Model**: $p(y_i=1) = \\operatorname{logit}^{-1}(\\beta_0+\\beta_x x_i+\\beta_z z_i)$.\n**Data**: $(x,z,y)$ pairs are $(0,0,0)$, $(0,0,1)$, $(1,1,0)$, $(1,1,1)$.\n**Analysis**: Let's examine the design matrix for the predictors $(\\text{intercept}, x, z)$:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  0  0 \\\\ 1  0  0 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}\n$$\nThe second column (for predictor $x$) is identical to the third column (for predictor $z$). This means there is **perfect collinearity**. The columns are linearly dependent, as $1 \\cdot (\\text{column } x) - 1 \\cdot (\\text{column } z) + 0 \\cdot (\\text{intercept column}) = \\mathbf{0}$. The design matrix does not have full rank.\nThe linear predictor is $\\eta_i = \\beta_0 + \\beta_x x_i + \\beta_z z_i$. Because $x_i = z_i$ for all $i$ in the dataset, this becomes $\\eta_i = \\beta_0 + (\\beta_x + \\beta_z) x_i$. The likelihood function $L(\\beta_0, \\beta_x, \\beta_z)$ depends only on $\\beta_0$ and the sum $\\beta_x+\\beta_z$, but not on $\\beta_x$ and $\\beta_z$ individually.\nFor any constant $c$, the parameter vector $(\\beta_0, \\beta_x+c, \\beta_z-c)$ produces the same linear predictors and thus the same likelihood value as $(\\beta_0, \\beta_x, \\beta_z)$. This means the likelihood function is constant along lines in the $(\\beta_x, \\beta_z)$ plane for a fixed $\\beta_0$ and sum $\\beta_x+\\beta_z$.\nThe integral of the likelihood is $\\int_{\\mathbb{R}^3} L(\\beta_0, \\beta_x, \\beta_z) \\, \\mathrm{d}\\beta_0 \\, \\mathrm{d}\\beta_x \\, \\mathrm{d}\\beta_z$. Because the likelihood is constant along infinite lines in the parameter space, the integral over these lines will diverge. Thus, the total integral over $\\mathbb{R}^3$ is infinite, and the posterior is improper.\n**Data Pattern Causing Failure**: The failure is due to **perfect collinearity** between the predictors $x$ and $z$. This makes the individual coefficients $\\beta_x$ and $\\beta_z$ non-identifiable.\n\n**Verdict**: **Correct**. The posterior is improper.\n\n### Option D: Complete separation by a single predictor.\n**Data**: $(x,y)$ pairs are $(-1,0)$, $(-1,0)$, $(+1,1)$, $(+1,1)$.\n**Analysis**: This is a classic case of **complete separation**. All data points with $y_i=0$ have $x_i=-1$, and all data points with $y_i=1$ have $x_i=+1$. The predictor $x$ perfectly separates the outcomes.\nWe can find a vector $(\\beta_0, \\beta_1)$ that achieves this separation. For example, let $\\beta_0=0$ and $\\beta_1=1$. The linear predictor is $\\eta_i = x_i$.\nFor $y_i=0$, we have $x_i=-1$, so $\\eta_i = -1  0$. The probability of success is pushed towards $0$.\nFor $y_i=1$, we have $x_i=+1$, so $\\eta_i = +1  0$. The probability of success is pushed towards $1$.\nNow, consider scaling these parameters by a factor $k  0$. Let the parameter vector be $(0, k)$. The linear predictors become $\\eta_i = k x_i$.\nThe likelihood function is $L(\\beta_0, \\beta_1) = \\prod_i p_i^{y_i} (1-p_i)^{1-y_i}$. For our data and parameters $(0, k)$:\n$$\nL(0, k) = \\left(1 - p(x=-1)\\right)^2 \\cdot \\left(p(x=+1)\\right)^2\n$$\nwhere $p(x) = (1+\\exp(-(kx)))^{-1}$.\n$$\nL(0, k) = \\left(1 - \\frac{1}{1+\\exp(k)}\\right)^2 \\left(\\frac{1}{1+\\exp(-k)}\\right)^2 = \\left(\\frac{\\exp(k)}{1+\\exp(k)}\\right)^2 \\left(\\frac{1}{1+\\exp(-k)}\\right)^2\n$$\nAs $k \\to \\infty$:\n$\\frac{\\exp(k)}{1+\\exp(k)} \\to 1$ and $\\frac{1}{1+\\exp(-k)} \\to 1$.\nThus, $\\lim_{k \\to \\infty} L(0, k) = 1^2 \\cdot 1^2 = 1$.\nThe likelihood function approaches $1$ along the ray $(\\beta_0, \\beta_1) = (0, k)$ as $k \\to \\infty$. The integral of a function that does not decay to zero over an infinite domain must diverge. Therefore, the posterior is improper.\n**Data Pattern Causing Failure**: The failure is caused by **complete separation**, where a linear combination of the predictors can perfectly classify the binary outcomes.\n\n**Verdict**: **Correct**. The posterior is improper.\n\nFinal conclusion: options A, C, and D describe scenarios that lead to an improper posterior when using a flat prior in logistic regression.", "answer": "$$\\boxed{ACD}$$", "id": "4912450"}]}