{"hands_on_practices": [{"introduction": "This first exercise is a cornerstone of Bayesian hypothesis testing. We will derive the Bayes factor for a fundamental scenario: testing whether a population mean is zero, based on data from a normal distribution. This practice [@problem_id:4896235] is essential for understanding the mechanics of model comparison from first principles, requiring you to compute and compare the marginal likelihood of the data under two competing hypotheses.", "problem": "A clinical laboratory measures a continuous biomarker in $n$ independent and identically distributed (i.i.d.) patients and models the observations as $y_{1},\\dots,y_{n} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ with known variance $\\sigma^{2}$. The scientific question is whether the population mean level is exactly zero. Consider the model comparison between the point null hypothesis $H_{0}:\\mu=0$ and the alternative hypothesis $H_{1}:\\mu \\sim \\mathcal{N}(0,\\tau^{2})$, where $\\tau^{2}$ is a specified prior variance reflecting plausible effect sizes under $H_{1}$. Using the definition of the Bayes factor (BF) as the ratio of marginal likelihoods, derive from first principles the Bayes factor in favor of $H_{1}$ over $H_{0}$, denoted $BF_{10}$. Start from the likelihood for the normal model with known variance and the definition of the marginal likelihood under each hypothesis, and perform the required integration over the parameter under $H_{1}$. Express your final answer as a single closed-form expression simplified in terms of the sample mean $\\bar{y}$, the sample size $n$, the known variance $\\sigma^{2}$, and the prior variance $\\tau^{2}$. Do not introduce any additional statistics beyond these four quantities. The final answer must be a single analytical expression. No numerical evaluation or rounding is required.", "solution": "The problem as stated is formally analyzed and validated against the specified criteria.\n\n**Step 1: Extract Givens**\n- Data: $y_{1}, \\dots, y_{n}$ are independent and identically distributed (i.i.d.) observations.\n- Data Generating Model: $y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$, for $i=1, \\dots, n$.\n- Known Parameter: The variance $\\sigma^2$ is known.\n- Null Hypothesis $H_0$: The parameter $\\mu$ is fixed at a specific value, $\\mu=0$.\n- Alternative Hypothesis $H_1$: The parameter $\\mu$ is a random variable drawn from a prior distribution, $\\mu \\sim \\mathcal{N}(0, \\tau^2)$.\n- Known Hyperparameter: The prior variance $\\tau^2$ is specified.\n- Objective: Derive the Bayes factor in favor of $H_1$ over $H_0$, denoted $BF_{10}$. The derivation must start from first principles.\n- Required a final expression in terms of the sample mean $\\bar{y}$, sample size $n$, known variance $\\sigma^2$, and prior variance $\\tau^2$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed for validity:\n- **Scientifically Grounded:** The problem is a classic and fundamental exercise in Bayesian model comparison. The use of a normal likelihood with a conjugate normal prior for the mean parameter is a standard textbook case (often called the \"Bayesian Z-test\"). This is a well-established method in statistics and its applications, including biostatistics. No scientific or factual unsoundness is present.\n- **Well-Posed:** The problem is mathematically well-defined. All necessary information (the likelihood, the hypotheses including the prior for $H_1$, and the known constants) is provided to derive the Bayes factor. A unique analytical solution exists.\n- **Objective:** The problem is stated using precise, formal, and unbiased mathematical language.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. It is scientifically sound, well-posed, objective, and contains all necessary information for a complete derivation. The solution process will now proceed.\n\n**Derivation of the Bayes Factor**\n\nThe Bayes factor $BF_{10}$ is defined as the ratio of the marginal likelihoods of the data $y = (y_1, \\dots, y_n)$ under hypothesis $H_1$ to that under hypothesis $H_0$:\n$$\nBF_{10} = \\frac{p(y|H_1)}{p(y|H_0)}\n$$\nBy the sufficiency principle, the Bayes factor can be calculated using a sufficient statistic for the parameter $\\mu$. For a normal distribution with known variance $\\sigma^2$, the sample mean $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ is a sufficient statistic for $\\mu$. The sampling distribution of $\\bar{y}$ given $\\mu$ is:\n$$\n\\bar{y} | \\mu, \\sigma^2 \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\nWe will therefore compute the Bayes factor as the ratio of the marginal likelihoods of the sufficient statistic $\\bar{y}$:\n$$\nBF_{10} = \\frac{p(\\bar{y}|H_1)}{p(\\bar{y}|H_0)}\n$$\n\n**1. Marginal Likelihood under $H_0$**\n\nUnder the null hypothesis $H_0$, the mean $\\mu$ is fixed at $0$. The sampling distribution of $\\bar{y}$ is therefore:\n$$\n\\bar{y} | H_0 \\sim \\mathcal{N}\\left(0, \\frac{\\sigma^2}{n}\\right)\n$$\nThe probability density function (PDF) for $\\bar{y}$ under $H_0$, evaluated at the observed sample mean, is:\n$$\np(\\bar{y}|H_0) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2/n)}} \\exp\\left(-\\frac{\\bar{y}^2}{2(\\sigma^2/n)}\\right) = \\sqrt{\\frac{n}{2\\pi\\sigma^2}} \\exp\\left(-\\frac{n\\bar{y}^2}{2\\sigma^2}\\right)\n$$\n\n**2. Marginal Likelihood under $H_1$**\n\nUnder the alternative hypothesis $H_1$, the parameter $\\mu$ is not fixed but is considered a random variable with a specified prior distribution, $\\mu \\sim \\mathcal{N}(0, \\tau^2)$. The model for $\\bar{y}$ is hierarchical:\n$$\n\\text{Level 1 (Sampling Distribution): } \\bar{y}|\\mu \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n$$\n\\text{Level 2 (Prior Distribution): } \\mu \\sim \\mathcal{N}(0, \\tau^2)\n$$\nTo find the marginal likelihood $p(\\bar{y}|H_1)$, we integrate the joint distribution of $\\bar{y}$ and $\\mu$ over all possible values of $\\mu$:\n$$\np(\\bar{y}|H_1) = \\int_{-\\infty}^{\\infty} p(\\bar{y}|\\mu) p(\\mu|H_1) d\\mu\n$$\nThis is equivalent to finding the marginal distribution of $\\bar{y}$. For this hierarchical normal model, the marginal distribution of $\\bar{y}$ is also normal. We can find its parameters using the law of total expectation and law of total variance.\n\nThe marginal mean of $\\bar{y}$ is:\n$$\nE[\\bar{y}|H_1] = E[E[\\bar{y}|\\mu]] = E[\\mu] = 0\n$$\nThe marginal variance of $\\bar{y}$ is:\n$$\n\\text{Var}(\\bar{y}|H_1) = E[\\text{Var}(\\bar{y}|\\mu)] + \\text{Var}(E[\\bar{y}|\\mu]) = E\\left[\\frac{\\sigma^2}{n}\\right] + \\text{Var}(\\mu) = \\frac{\\sigma^2}{n} + \\tau^2\n$$\nThus, the marginal distribution of $\\bar{y}$ under $H_1$ is:\n$$\n\\bar{y}|H_1 \\sim \\mathcal{N}\\left(0, \\frac{\\sigma^2}{n} + \\tau^2\\right)\n$$\nThe PDF for $\\bar{y}$ under $H_1$ is:\n$$\np(\\bar{y}|H_1) = \\frac{1}{\\sqrt{2\\pi\\left(\\frac{\\sigma^2}{n} + \\tau^2\\right)}} \\exp\\left(-\\frac{\\bar{y}^2}{2\\left(\\frac{\\sigma^2}{n} + \\tau^2\\right)}\\right)\n$$\n\n**3. Assembling the Bayes Factor**\n\nNow, we compute the ratio $BF_{10} = p(\\bar{y}|H_1) / p(\\bar{y}|H_0)$:\n$$\nBF_{10} = \\frac{\\frac{1}{\\sqrt{2\\pi\\left(\\frac{\\sigma^2}{n} + \\tau^2\\right)}} \\exp\\left(-\\frac{\\bar{y}^2}{2\\left(\\frac{\\sigma^2}{n} + \\tau^2\\right)}\\right)}{\\frac{1}{\\sqrt{2\\pi\\left(\\frac{\\sigma^2}{n}\\right)}} \\exp\\left(-\\frac{\\bar{y}^2}{2\\left(\\frac{\\sigma^2}{n}\\right)}\\right)}\n$$\nWe can separate the ratio into two parts: the ratio of the normalizing constants and the ratio of the exponential terms.\n\nRatio of constants:\n$$\n\\frac{\\sqrt{2\\pi(\\sigma^2/n)}}{\\sqrt{2\\pi(\\sigma^2/n + \\tau^2)}} = \\sqrt{\\frac{\\sigma^2/n}{\\sigma^2/n + \\tau^2}} = \\sqrt{\\frac{\\sigma^2/n}{(\\sigma^2 + n\\tau^2)/n}} = \\sqrt{\\frac{\\sigma^2}{\\sigma^2 + n\\tau^2}}\n$$\nThis term can also be written as $(\\frac{\\sigma^2}{\\sigma^2 + n\\tau^2})^{1/2}$.\n\nRatio of exponential terms:\n$$\n\\frac{\\exp\\left(-\\frac{\\bar{y}^2}{2(\\sigma^2/n + \\tau^2)}\\right)}{\\exp\\left(-\\frac{\\bar{y}^2}{2(\\sigma^2/n)}\\right)} = \\exp\\left(\\frac{\\bar{y}^2}{2(\\sigma^2/n)} - \\frac{\\bar{y}^2}{2(\\sigma^2/n + \\tau^2)}\\right)\n$$\nLet's simplify the exponent:\n$$\n\\frac{\\bar{y}^2}{2}\\left[\\frac{1}{\\sigma^2/n} - \\frac{1}{\\sigma^2/n + \\tau^2}\\right] = \\frac{\\bar{y}^2}{2}\\left[\\frac{(\\sigma^2/n + \\tau^2) - (\\sigma^2/n)}{(\\sigma^2/n)(\\sigma^2/n + \\tau^2)}\\right]\n$$\n$$\n= \\frac{\\bar{y}^2}{2}\\left[\\frac{\\tau^2}{(\\sigma^2/n)((\\sigma^2 + n\\tau^2)/n)}\\right] = \\frac{\\bar{y}^2}{2}\\left[\\frac{\\tau^2}{\\sigma^2(\\sigma^2 + n\\tau^2)/n^2}\\right] = \\frac{n^2 \\bar{y}^2 \\tau^2}{2\\sigma^2(\\sigma^2 + n\\tau^2)}\n$$\nCombining the two parts gives the final expression for the Bayes factor:\n$$\nBF_{10} = \\left(\\frac{\\sigma^2}{\\sigma^2 + n\\tau^2}\\right)^{1/2} \\exp\\left(\\frac{n^2 \\bar{y}^2 \\tau^2}{2\\sigma^2 (\\sigma^2 + n\\tau^2)}\\right)\n$$\nThis expression is in terms of the required quantities $\\bar{y}$, $n$, $\\sigma^2$, and $\\tau^2$.", "answer": "$$\n\\boxed{\\left(\\frac{\\sigma^2}{\\sigma^2 + n\\tau^2}\\right)^{1/2} \\exp\\left(\\frac{n^2 \\bar{y}^2 \\tau^2}{2\\sigma^2 (\\sigma^2 + n\\tau^2)}\\right)}\n$$", "id": "4896235"}, {"introduction": "Building upon the normal mean test, this exercise [@problem_id:4896239] delves into a famous conceptual puzzle known as Lindley's paradox. We will analyze a thought experiment where, for a fixed level of statistical significance (a constant $p$-value), the Bayesian evidence can point in the opposite direction as the sample size grows. This practice highlights a critical distinction between Bayesian and frequentist inference and deepens your understanding of how sample size and prior specifications influence the Bayes factor.", "problem": "A biostatistics study measures a continuous biomarker in a large cohort. Let $y_{1},\\dots,y_{n}$ be independent and identically distributed observations modeled as $y_{i}\\sim \\mathcal{N}(\\mu,\\sigma^{2})$, where $\\sigma^{2}$ is known from assay validation. Consider testing the point null hypothesis $H_{0}:\\mu=0$ against the composite alternative $H_{1}:\\mu\\neq 0$ with a prior under $H_{1}$ given by $\\mu\\sim \\mathcal{N}(0,\\tau^{2})$, where $\\tau^{2}>0$ is fixed and does not depend on $n$. Define the Bayes factor (BF) in favor of $H_{1}$ over $H_{0}$ as $BF_{10}=\\dfrac{m_{1}(\\bar{y})}{m_{0}(\\bar{y})}$, where $m_{j}(\\bar{y})$ denotes the marginal likelihood of the sample mean $\\bar{y}$ under hypothesis $H_{j}$, and define the two-sided $p$-value of the $z$-test as $p=2\\left(1-\\Phi(|z_{n}|)\\right)$, where $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution and $z_{n}=\\sqrt{n}\\,\\bar{y}/\\sigma$.\n\nAssume a “diffuse prior” regime in which $\\tau^{2}$ is a fixed constant that is large relative to $\\sigma^{2}$, and consider a sequence of studies indexed by $n$ such that the standardized test statistic is held fixed at a prespecified value $z_{n}=z_{0}$ for all $n\\geq 1$ (this implies that $\\bar{y}$ shrinks as $n$ grows). Starting only from the normal sampling model and the definitions of Bayes factor and $z$-test $p$-value given above, derive an exact expression for $BF_{10}$ in terms of $z_{0}$, $n$, $\\sigma$, and $\\tau$, simplify it, and then take the limit as $n\\to\\infty$. Also determine the corresponding two-sided $z$-test $p$-value for this fixed $z_{0}$. Express your final answer as a $1\\times 2$ row vector $\\left[\\lim_{n\\to\\infty} BF_{10},\\; p\\right]$. No numerical approximation is required; write the $p$-value in closed form using $\\Phi(\\cdot)$.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **Data Model**: $y_{1}, \\dots, y_{n}$ are independent and identically distributed (i.i.d.) observations from a normal distribution, $y_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$.\n-   **Known Parameter**: The variance $\\sigma^{2}$ is known.\n-   **Hypotheses**:\n    -   Null Hypothesis $H_{0}: \\mu=0$.\n    -   Alternative Hypothesis $H_{1}: \\mu \\neq 0$.\n-   **Prior Distribution**: Under $H_{1}$, the prior for $\\mu$ is $\\mu \\sim \\mathcal{N}(0, \\tau^{2})$, where $\\tau^{2} > 0$ is a fixed constant that does not depend on $n$.\n-   **Bayes Factor Definition**: The Bayes factor in favor of $H_{1}$ over $H_{0}$ is $BF_{10} = \\dfrac{m_{1}(\\bar{y})}{m_{0}(\\bar{y})}$, where $m_{j}(\\bar{y})$ is the marginal likelihood of the sample mean $\\bar{y}$ under hypothesis $H_{j}$.\n-   **Test Statistic Definition**: The $z$-test statistic is $z_{n} = \\sqrt{n}\\,\\bar{y}/\\sigma$.\n-   **p-value Definition**: The two-sided $p$-value is $p=2\\left(1-\\Phi(|z_{n}|)\\right)$, where $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n-   **Asymptotic Condition**: The analysis considers a sequence of studies where the test statistic is held fixed at a value $z_{n} = z_{0}$ for all $n \\geq 1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, resting on fundamental principles of statistical inference, specifically Bayesian and frequentist hypothesis testing. The model, prior, and test definitions are standard in biostatistics. The problem is well-posed, providing all necessary information to perform the requested derivations. The condition $z_{n}=z_{0}$ is a theoretical construct for examining the asymptotic properties of the Bayes factor and its relationship to the $p$-value, a classic setup known as the Jeffreys-Lindley paradox. This does not represent a physical impossibility but rather a specific mathematical limit to investigate. The problem's language is objective and unambiguous. It is not incomplete, contradictory, or trivial.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Solution\n\nThe first step is to derive the marginal likelihoods of the sample mean $\\bar{y}$ under each hypothesis. Given the data $y_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, the sample mean $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ has a sampling distribution $\\bar{y}|\\mu \\sim \\mathcal{N}(\\mu, \\sigma^{2}/n)$.\n\n**Marginal Likelihood under $H_{0}$**\nUnder the null hypothesis $H_{0}: \\mu=0$, the distribution of $\\bar{y}$ is directly specified by the sampling model with $\\mu$ set to $0$:\n$$\n\\bar{y}|H_{0} \\sim \\mathcal{N}(0, \\sigma^{2}/n)\n$$\nThe marginal likelihood under $H_{0}$, denoted $m_{0}(\\bar{y})$, is the probability density function (PDF) of this distribution evaluated at the observed $\\bar{y}$:\n$$\nm_{0}(\\bar{y}) = \\frac{1}{\\sqrt{2\\pi(\\sigma^{2}/n)}} \\exp\\left(-\\frac{\\bar{y}^{2}}{2(\\sigma^{2}/n)}\\right) = \\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{n\\bar{y}^{2}}{2\\sigma^{2}}\\right)\n$$\n\n**Marginal Likelihood under $H_{1}$**\nUnder the alternative hypothesis $H_{1}$, the parameter $\\mu$ is considered a random variable with prior distribution $\\mu \\sim \\mathcal{N}(0, \\tau^{2})$. The marginal likelihood $m_{1}(\\bar{y})$ is obtained by integrating the product of the likelihood $p(\\bar{y}|\\mu)$ and the prior $p(\\mu|H_{1})$ over all possible values of $\\mu$:\n$$\nm_{1}(\\bar{y}) = \\int_{-\\infty}^{\\infty} p(\\bar{y}|\\mu) p(\\mu|H_{1}) \\, d\\mu\n$$\nThis is a standard convolution of two normal distributions. We have:\n-   Likelihood: $\\bar{y}|\\mu \\sim \\mathcal{N}(\\mu, \\sigma^{2}/n)$\n-   Prior: $\\mu \\sim \\mathcal{N}(0, \\tau^{2})$\nThe resulting marginal distribution for $\\bar{y}$ is also normal. The mean of the marginal distribution is $E[\\bar{y}] = E[E[\\bar{y}|\\mu]] = E[\\mu] = 0$. The variance is $Var(\\bar{y}) = Var(E[\\bar{y}|\\mu]) + E[Var(\\bar{y}|\\mu)] = Var(\\mu) + E[\\sigma^{2}/n] = \\tau^{2} + \\sigma^{2}/n$.\nThus, the marginal distribution of $\\bar{y}$ under $H_{1}$ is:\n$$\n\\bar{y}|H_{1} \\sim \\mathcal{N}(0, \\tau^{2} + \\sigma^{2}/n)\n$$\nThe marginal likelihood under $H_{1}$ is the PDF of this distribution:\n$$\nm_{1}(\\bar{y}) = \\frac{1}{\\sqrt{2\\pi(\\tau^{2} + \\sigma^{2}/n)}} \\exp\\left(-\\frac{\\bar{y}^{2}}{2(\\tau^{2} + \\sigma^{2}/n)}\\right)\n$$\n\n**Derivation of the Bayes Factor $BF_{10}$**\nThe Bayes factor in favor of $H_{1}$ over $H_{0}$ is the ratio of the marginal likelihoods:\n$$\nBF_{10} = \\frac{m_{1}(\\bar{y})}{m_{0}(\\bar{y})} = \\frac{\\frac{1}{\\sqrt{2\\pi(\\tau^{2} + \\sigma^{2}/n)}} \\exp\\left(-\\frac{\\bar{y}^{2}}{2(\\tau^{2} + \\sigma^{2}/n)}\\right)}{\\frac{1}{\\sqrt{2\\pi(\\sigma^{2}/n)}} \\exp\\left(-\\frac{n\\bar{y}^{2}}{2\\sigma^{2}}\\right)}\n$$\nSimplifying the ratio of the normalization constants:\n$$\n\\frac{\\sqrt{2\\pi(\\sigma^{2}/n)}}{\\sqrt{2\\pi(\\tau^{2} + \\sigma^{2}/n)}} = \\sqrt{\\frac{\\sigma^{2}/n}{\\tau^{2} + \\sigma^{2}/n}} = \\sqrt{\\frac{\\sigma^{2}}{n\\tau^{2} + \\sigma^{2}}}\n$$\nNow, substitute the fixed test statistic value $z_{n}=z_{0}$. From the definition $z_{n} = \\sqrt{n}\\bar{y}/\\sigma$, we have $\\bar{y} = z_{0}\\sigma/\\sqrt{n}$. Substituting this into the exponential terms:\nThe exponent of the numerator is:\n$$\n-\\frac{(z_{0}\\sigma/\\sqrt{n})^{2}}{2(\\tau^{2} + \\sigma^{2}/n)} = -\\frac{z_{0}^{2}\\sigma^{2}/n}{2(\\frac{n\\tau^{2}+\\sigma^{2}}{n})} = -\\frac{z_{0}^{2}\\sigma^{2}}{2(n\\tau^{2}+\\sigma^{2})}\n$$\nThe exponent of the denominator is:\n$$\n-\\frac{n(z_{0}\\sigma/\\sqrt{n})^{2}}{2\\sigma^{2}} = -\\frac{n(z_{0}^{2}\\sigma^{2}/n)}{2\\sigma^{2}} = -\\frac{z_{0}^{2}}{2}\n$$\nThe term in the exponent of the final $BF_{10}$ expression is the difference:\n$$\n-\\frac{z_{0}^{2}\\sigma^{2}}{2(n\\tau^{2}+\\sigma^{2})} - \\left(-\\frac{z_{0}^{2}}{2}\\right) = \\frac{z_{0}^{2}}{2} \\left(1 - \\frac{\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}}\\right) = \\frac{z_{0}^{2}}{2} \\left(\\frac{n\\tau^{2}+\\sigma^{2}-\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}}\\right) = \\frac{z_{0}^{2} n\\tau^{2}}{2(n\\tau^{2}+\\sigma^{2})}\n$$\nCombining the pre-factor and the exponential term gives the exact expression for $BF_{10}$:\n$$\nBF_{10} = \\sqrt{\\frac{\\sigma^{2}}{n\\tau^{2} + \\sigma^{2}}} \\exp\\left(\\frac{z_{0}^{2} n\\tau^{2}}{2(n\\tau^{2}+\\sigma^{2})}\\right)\n$$\n\n**Limit of the Bayes Factor as $n \\to \\infty$**\nWe now take the limit of this expression as $n \\to \\infty$. Let's analyze the pre-factor and the exponential term separately.\nLimit of the pre-factor:\n$$\n\\lim_{n\\to\\infty} \\sqrt{\\frac{\\sigma^{2}}{n\\tau^{2} + \\sigma^{2}}} = \\lim_{n\\to\\infty} \\sqrt{\\frac{\\sigma^{2}/n}{\\tau^{2} + \\sigma^{2}/n}} = \\sqrt{\\frac{0}{\\tau^{2} + 0}} = 0\n$$\nsince $\\tau^{2} > 0$.\nLimit of the argument of the exponential function:\n$$\n\\lim_{n\\to\\infty} \\frac{z_{0}^{2} n\\tau^{2}}{2(n\\tau^{2}+\\sigma^{2})} = \\lim_{n\\to\\infty} \\frac{z_{0}^{2} \\tau^{2}}{2(\\tau^{2}+\\sigma^{2}/n)} = \\frac{z_{0}^{2} \\tau^{2}}{2(\\tau^{2}+0)} = \\frac{z_{0}^{2}}{2}\n$$\nThe limit of the exponential term is therefore $\\exp\\left(\\frac{z_{0}^{2}}{2}\\right)$, which is a finite constant.\nThe overall limit of the Bayes factor is the product of these limits:\n$$\n\\lim_{n\\to\\infty} BF_{10} = \\left(\\lim_{n\\to\\infty} \\sqrt{\\frac{\\sigma^{2}}{n\\tau^{2} + \\sigma^{2}}}\\right) \\times \\left(\\lim_{n\\to\\infty} \\exp\\left(\\frac{z_{0}^{2} n\\tau^{2}}{2(n\\tau^{2}+\\sigma^{2})}\\right)\\right) = 0 \\times \\exp\\left(\\frac{z_{0}^{2}}{2}\\right) = 0\n$$\n\n**Determination of the p-value**\nThe two-sided $p$-value is defined as $p=2\\left(1-\\Phi(|z_{n}|)\\right)$. The problem states that for the sequence of studies under consideration, the test statistic is held fixed at a constant value, $z_{n}=z_{0}$. Therefore, the $p$-value is also constant for all $n$:\n$$\np = 2\\left(1 - \\Phi(|z_{0}|)\\right)\n$$\nThis expression does not depend on $n$, so its value remains constant as $n \\to \\infty$.\n\nThe final result is the pair consisting of the limit of the Bayes factor and the corresponding fixed $p$-value.\nThis demonstrates the Jeffreys-Lindley paradox: for a fixed level of evidence against $H_{0}$ according to the frequentist $p$-value (i.e., fixed $z_{0}$), as the sample size $n$ increases, the Bayesian evidence in favor of the alternative hypothesis $H_{1}$ approaches zero. In other words, the Bayes factor overwhelmingly supports the null hypothesis $H_{0}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 2(1 - \\Phi(|z_{0}|)) \\end{pmatrix}}\n$$", "id": "4896239"}, {"introduction": "Moving from analytical derivations to practical computation, this exercise addresses a crucial aspect of biostatistical modeling: robustness to outliers. You will compare a standard regression model assuming normal errors against one assuming a heavy-tailed Student's $t$ distribution. This practice [@problem_id:4896241] will require you to numerically compute the Bayes factor, providing a hands-on method for quantifying evidence in favor of a more robust model when data deviates from normality.", "problem": "You are given a sequence of independent observations modeled by a one-parameter regression with an intercept-only mean. You will compare two error models for the residuals and compute the Bayes factor to assess robustness to outliers using heavy-tailed errors.\n\nFundamental definitions and setup:\n- For observations $y_1,\\dots,y_n$, consider the regression $y_i = \\beta + \\varepsilon_i$ with a proper Gaussian prior $\\beta \\sim \\mathcal{N}(0,\\tau^2)$, where $\\tau^2$ is known and fixed.\n- Model $\\mathcal{M}_0$ (normal errors): $\\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$ with known scale $\\sigma^2$.\n- Model $\\mathcal{M}_1$ (Student $t$ errors): $\\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} t_\\nu(0,s)$ with known degrees of freedom $\\nu$ and known scale $s$.\n- The Bayes factor for $\\mathcal{M}_1$ against $\\mathcal{M}_0$ is defined by $BF_{10} = \\dfrac{p(\\mathbf{y}\\mid \\mathcal{M}_1)}{p(\\mathbf{y}\\mid \\mathcal{M}_0)}$, where the marginal likelihood under a model $\\mathcal{M}$ integrating out $\\beta$ is\n$$\np(\\mathbf{y}\\mid \\mathcal{M}) = \\int_{-\\infty}^{\\infty} \\left[\\prod_{i=1}^n f(y_i\\mid \\beta, \\mathcal{M})\\right] \\, p(\\beta) \\, d\\beta,\n$$\nwith $f(\\cdot\\mid \\beta,\\mathcal{M})$ the model-specific error density shifted by $\\beta$ and $p(\\beta)$ the prior density of $\\beta$.\n\nYour task:\n- For each specified test case, compute the natural logarithm of the Bayes factor $\\log BF_{10}$ using only the above fundamental definitions. All hyperparameters are fixed and known.\n- Use base-$e$ logarithms for all computations of $\\log BF_{10}$ and present the final results rounded to six digits after the decimal point.\n\nTest suite:\nFor each case below, use the provided data $\\mathbf{y}$ and hyperparameters $(\\sigma,s,\\nu,\\tau)$.\n- Case A (no outliers, moderate tails): $\\mathbf{y} = [0.20,-0.11,0.38,-0.27,0.05,0.02,-0.18,0.31,-0.44,0.19]$, $\\sigma=1$, $s=1$, $\\nu=5$, $\\tau=10$.\n- Case B (one large outlier): $\\mathbf{y} = [0.20,-0.11,0.38,-0.27,0.05,0.02,-0.18,0.31,-0.44,0.19,8.00]$, $\\sigma=1$, $s=1$, $\\nu=5$, $\\tau=10$.\n- Case C (two extreme outliers with opposite signs): $\\mathbf{y} = [-0.20,0.10,0.30,-0.10,6.50,-7.00,0.20,0.00]$, $\\sigma=1$, $s=1$, $\\nu=3$, $\\tau=10$.\n- Case D (small sample, Student $t$ close to normal): $\\mathbf{y} = [0.50,-0.40,0.20]$, $\\sigma=1$, $s=1$, $\\nu=100$, $\\tau=10$.\n- Case E (heavy tails, extreme outlier): $\\mathbf{y} = [-0.10,0.00,0.10,10.00]$, $\\sigma=1$, $s=1$, $\\nu=1$, $\\tau=10$.\n\nRequirements and output format:\n- Your program must compute $\\log BF_{10}$ for each case as a real number and aggregate the results into a single line of output containing a comma-separated list enclosed in square brackets.\n- The final printed line must be exactly of the form $[\\ell_A,\\ell_B,\\ell_C,\\ell_D,\\ell_E]$, where each $\\ell_{\\cdot}$ is the value of $\\log BF_{10}$ for the corresponding case, rounded to six digits after the decimal point.\n- No physical units are involved in this problem. Angles are not used. Percentages are not used. All outputs must be real numbers.", "solution": "The objective is to compute the natural logarithm of the Bayes factor, $\\log BF_{10}$, for comparing a model with Student's $t$ errors ($\\mathcal{M}_1$) against a model with Normal errors ($\\mathcal{M}_0$). The Bayes factor is the ratio of their respective marginal likelihoods, so its logarithm is the difference of their log marginal likelihoods:\n$$ \\log BF_{10} = \\log p(\\mathbf{y} | \\mathcal{M}_1) - \\log p(\\mathbf{y} | \\mathcal{M}_0) $$\nWe will derive the expression for each log marginal likelihood.\n\n**Log Marginal Likelihood for Model $\\mathcal{M}_0$ (Normal Errors)**\n\nThe model is defined by $y_i = \\beta + \\varepsilon_i$, where the prior on the intercept is $\\beta \\sim \\mathcal{N}(0, \\tau^2)$ and the errors are $\\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)$. This is a standard hierarchical Gaussian model. Integrating out the latent parameter $\\beta$ yields a marginal distribution for the observation vector $\\mathbf{y} = (y_1, \\dots, y_n)^T$ that is a multivariate normal distribution, $\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_y, \\Sigma_y)$.\n\nThe marginal mean vector is $\\boldsymbol{\\mu}_y = \\mathbf{0}$, since $E[y_i] = E[\\beta] + E[\\varepsilon_i] = 0 + 0 = 0$.\nThe elements of the marginal covariance matrix $\\Sigma_y$ are:\n- Variance: $Var(y_i) = Var(\\beta + \\varepsilon_i) = Var(\\beta) + Var(\\varepsilon_i) = \\tau^2 + \\sigma^2$ (since $\\beta$ and $\\varepsilon_i$ are independent).\n- Covariance ($i \\neq j$): $Cov(y_i, y_j) = Cov(\\beta + \\varepsilon_i, \\beta + \\varepsilon_j) = Var(\\beta) = \\tau^2$ (since $\\varepsilon_i$ and $\\varepsilon_j$ are independent).\n\nThus, the covariance matrix is $\\Sigma_y = \\sigma^2 I_n + \\tau^2 J_n$, where $I_n$ is the $n \\times n$ identity matrix and $J_n$ is the $n \\times n$ matrix of all ones.\n\nThe log marginal likelihood is the log-density of this multivariate normal distribution evaluated at the observed data vector $\\mathbf{y}$:\n$$ \\log p(\\mathbf{y} | \\mathcal{M}_0) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma_y| - \\frac{1}{2}\\mathbf{y}^T \\Sigma_y^{-1} \\mathbf{y} $$\nFor the specific structure of $\\Sigma_y$, the determinant and inverse have well-known forms:\n- Determinant: $|\\Sigma_y| = (\\sigma^2)^{n-1}(\\sigma^2 + n\\tau^2)$.\n- Inverse (via Sherman-Morrison-Woodbury formula): $\\Sigma_y^{-1} = \\frac{1}{\\sigma^2}I_n - \\frac{\\tau^2}{\\sigma^2(\\sigma^2+n\\tau^2)}J_n$.\n\nSubstituting these into the log-likelihood expression, the quadratic form $\\mathbf{y}^T \\Sigma_y^{-1} \\mathbf{y}$ simplifies to:\n$$ \\mathbf{y}^T \\Sigma_y^{-1} \\mathbf{y} = \\frac{1}{\\sigma^2}\\mathbf{y}^T I_n \\mathbf{y} - \\frac{\\tau^2}{\\sigma^2(\\sigma^2+n\\tau^2)}\\mathbf{y}^T J_n \\mathbf{y} = \\frac{1}{\\sigma^2}\\sum_{i=1}^n y_i^2 - \\frac{\\tau^2 (\\sum_{i=1}^n y_i)^2}{\\sigma^2(\\sigma^2+n\\tau^2)} $$\nThis provides a complete analytical formula for $\\log p(\\mathbf{y} | \\mathcal{M}_0)$, which can be computed directly from the data $\\mathbf{y}$ and hyperparameters $\\sigma$ and $\\tau$.\n\n**Log Marginal Likelihood for Model $\\mathcal{M}_1$ (Student's $t$ Errors)**\n\nFor this model, the errors are $\\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} t_\\nu(0, s)$. The PDF for a single observation $y_i$ conditioned on $\\beta$ is given by the location-scale Student's $t$ density:\n$$ f(y_i|\\beta, \\mathcal{M}_1) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{s\\sqrt{\\nu\\pi}\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{(y_i-\\beta)^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}} $$\nThe full likelihood is the product $L_1(\\beta) = \\prod_{i=1}^n f(y_i|\\beta, \\mathcal{M}_1)$, and the prior on $\\beta$ is unchanged: $p(\\beta) = \\mathcal{N}(\\beta|0, \\tau^2)$. The marginal likelihood is the integral of their product over $\\beta$:\n$$ p(\\mathbf{y} | \\mathcal{M}_1) = \\int_{-\\infty}^{\\infty} \\left[\\prod_{i=1}^n f(y_i|\\beta, \\mathcal{M}_1)\\right] \\, p(\\beta) \\, d\\beta $$\nThis integral does not possess a general closed-form analytical solution. It must be computed using numerical methods. We employ one-dimensional numerical quadrature for this task.\n\nTo maintain numerical stability, especially when the integrand becomes very small, we work with its logarithm. Let the log-integrand be $g(\\beta) = \\log(L_1(\\beta)p(\\beta))$.\n$$ g(\\beta) = \\left( \\sum_{i=1}^n \\log f(y_i|\\beta, \\mathcal{M}_1) \\right) + \\log p(\\beta) $$\nThe marginal likelihood is $p(\\mathbf{y} | \\mathcal{M}_1) = \\int_{-\\infty}^{\\infty} \\exp(g(\\beta)) d\\beta$. A robust numerical evaluation proceeds as follows:\n$1$. Find the value $\\hat{\\beta}$ that maximizes $g(\\beta)$. Let this maximum value be $g_{max} = g(\\hat{\\beta})$. This is achieved by numerically minimizing $-g(\\beta)$.\n$2$. Rewrite the integral as:\n$$ p(\\mathbf{y} | \\mathcal{M}_1) = \\exp(g_{max}) \\int_{-\\infty}^{\\infty} \\exp(g(\\beta) - g_{max}) d\\beta $$\n$3$. The corresponding log marginal likelihood is:\n$$ \\log p(\\mathbf{y} | \\mathcal{M}_1) = g_{max} + \\log\\left( \\int_{-\\infty}^{\\infty} \\exp(g(\\beta) - g_{max}) \\, d\\beta \\right) $$\nThe integrand $\\exp(g(\\beta) - g_{max})$ is well-behaved, with a maximum value of $1$, preventing floating-point underflow. The integral is computed using a standard quadrature algorithm.\n\nThe final log Bayes factor is then calculated by subtracting the log marginal likelihood of the Normal model from that of the Student's $t$ model. A positive value provides evidence for the heavy-tailed Student's $t$ model, suggesting robustness to outliers is beneficial for the given dataset.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\nfrom scipy.special import gammaln\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Computes the log Bayes factor for comparing a Student's t error model\n    against a Normal error model for a series of test cases.\n    \"\"\"\n\n    def log_marginal_likelihood_normal(y: np.ndarray, sigma: float, tau: float) -> float:\n        \"\"\"\n        Computes the log marginal likelihood for the Normal error model analytically.\n        y_i | beta ~ N(beta, sigma^2)\n        beta ~ N(0, tau^2)\n        \"\"\"\n        n = len(y)\n        sigma2 = sigma**2\n        tau2 = tau**2\n        \n        sum_y = np.sum(y)\n        sum_y_sq = np.sum(y**2)\n        \n        # Log-determinant of the marginal covariance matrix Sigma_y\n        log_det_Sigma_y = (n - 1) * np.log(sigma2) + np.log(sigma2 + n * tau2)\n        \n        # Quadratic form y^T * Sigma_y^-1 * y\n        quad_form = (1 / sigma2) * sum_y_sq - (tau2 * sum_y**2) / (sigma2 * (sigma2 + n * tau2))\n        \n        # Log marginal likelihood, which is the log-pdf of y ~ MVN(0, Sigma_y)\n        log_p_y = -0.5 * n * np.log(2 * np.pi) - 0.5 * log_det_Sigma_y - 0.5 * quad_form\n        \n        return log_p_y\n\n    def log_marginal_likelihood_student_t(y: np.ndarray, s: float, nu: float, tau: float) -> float:\n        \"\"\"\n        Computes the log marginal likelihood for the Student's t error model\n        using robust numerical integration.\n        y_i | beta ~ t_nu(loc=beta, scale=s)\n        beta ~ N(0, tau^2)\n        \"\"\"\n        n = len(y)\n        s2 = s**2\n        tau2 = tau**2\n\n        # Constant part of the log student-t pdf\n        log_t_const = gammaln((nu + 1) / 2) - gammaln(nu / 2) - 0.5 * np.log(nu * np.pi) - np.log(s)\n\n        def log_integrand(beta: float) -> float:\n            # Log-likelihood term (sum over observations)\n            log_lik_t = n * log_t_const - ((nu + 1) / 2) * np.sum(np.log(1 + (y - beta)**2 / (nu * s2)))\n            \n            # Log-prior term for beta\n            log_prior_beta = -0.5 * np.log(2 * np.pi * tau2) - beta**2 / (2 * tau2)\n            \n            return log_lik_t + log_prior_beta\n\n        # To find the maximum of the log-integrand for numerical stability,\n        # we minimize its negative.\n        # Use a reasonable bound for optimization based on data range and prior width.\n        y_mean = np.mean(y) if n > 0 else 0\n        y_std = np.std(y) if n > 1 else 1\n        opt_bounds = [y_mean - 5 * y_std - 3 * tau, y_mean + 5 * y_std + 3 * tau]\n        \n        # Find mode of the posterior (maximum of log_integrand)\n        res = minimize_scalar(lambda b: -log_integrand(b), bounds=opt_bounds, method='bounded')\n        g_max = -res.fun\n        \n        # Define the scaled integrand for robust numerical integration\n        def scaled_integrand(beta: float) -> float:\n            return np.exp(log_integrand(beta) - g_max)\n\n        # Perform numerical integration\n        integral_val, _ = quad(scaled_integrand, -np.inf, np.inf)\n        \n        # Combine terms for the final log marginal likelihood\n        log_p_y = g_max + np.log(integral_val)\n        \n        return log_p_y\n\n    test_cases = [\n        {'id': 'A', 'y': np.array([0.20,-0.11,0.38,-0.27,0.05,0.02,-0.18,0.31,-0.44,0.19]), 'sigma': 1, 's': 1, 'nu': 5, 'tau': 10},\n        {'id': 'B', 'y': np.array([0.20,-0.11,0.38,-0.27,0.05,0.02,-0.18,0.31,-0.44,0.19,8.00]), 'sigma': 1, 's': 1, 'nu': 5, 'tau': 10},\n        {'id': 'C', 'y': np.array([-0.20,0.10,0.30,-0.10,6.50,-7.00,0.20,0.00]), 'sigma': 1, 's': 1, 'nu': 3, 'tau': 10},\n        {'id': 'D', 'y': np.array([0.50,-0.40,0.20]), 'sigma': 1, 's': 1, 'nu': 100, 'tau': 10},\n        {'id': 'E', 'y': np.array([-0.10,0.00,0.10,10.00]), 'sigma': 1, 's': 1, 'nu': 1, 'tau': 10},\n    ]\n\n    results = []\n    for case in test_cases:\n        y_data = case['y']\n        sigma_val = case['sigma']\n        s_val = case['s']\n        nu_val = case['nu']\n        tau_val = case['tau']\n\n        log_ml0 = log_marginal_likelihood_normal(y=y_data, sigma=sigma_val, tau=tau_val)\n        log_ml1 = log_marginal_likelihood_student_t(y=y_data, s=s_val, nu=nu_val, tau=tau_val)\n        \n        log_bf10 = log_ml1 - log_ml0\n        results.append(f\"{log_bf10:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4896241"}]}