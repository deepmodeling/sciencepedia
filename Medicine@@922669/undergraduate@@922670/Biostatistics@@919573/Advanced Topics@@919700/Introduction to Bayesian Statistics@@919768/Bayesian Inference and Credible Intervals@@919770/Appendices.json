{"hands_on_practices": [{"introduction": "The cornerstone of Bayesian inference is the process of updating our beliefs in light of new evidence. This first exercise provides a foundational walkthrough of this process for a continuous outcome, using the elegant and widely-used Gaussian conjugate model. By deriving the posterior distribution from first principles, you will gain a concrete understanding of how a prior distribution for a population mean, $\\mu$, is mathematically combined with the likelihood from observed data to yield an updated, more informed posterior distribution and its corresponding credible interval [@problem_id:4896865].", "problem": "A clinical trial investigates the average change in systolic blood pressure for a new lifestyle intervention. Let $\\{Y_{i}\\}_{i=1}^{n}$ denote the observed changes (in $\\mathrm{mmHg}$), modeled as independent and identically distributed draws from a Gaussian (Normal) distribution with unknown mean $\\mu$ and known variance $\\sigma^{2}$. The prior for $\\mu$ is Gaussian with mean $\\mu_{0}$ and variance $\\tau_{0}^{2}$. You observe $n$ independent measurements with sample mean $\\bar{Y}$.\n\nUsing only the definitions of likelihood, prior, and Bayes’ theorem, start from first principles to derive the posterior distribution for $\\mu$ under the Gaussian likelihood with known $\\sigma^{2}$ and Gaussian prior. Then, using the resulting posterior, compute the $95\\%$ equal-tailed credible interval for $\\mu$.\n\nUse the following values, all in $\\mathrm{mmHg}$ units where applicable: $n = 15$, $\\sigma^{2} = 36$, $\\mu_{0} = 5$, $\\tau_{0}^{2} = 100$, and $\\bar{Y} = 3.2$. Express the final credible interval endpoints in $\\mathrm{mmHg}$. Round your numerical answers to four significant figures.\n\nIn your final numeric answer, report the lower and upper credible interval endpoints as a single row matrix.", "solution": "The problem requires the derivation of the posterior distribution for the mean parameter $\\mu$ of a Gaussian likelihood, given a Gaussian prior, and then the computation of a $95\\%$ credible interval for $\\mu$.\n\n### Problem Validation\n**Step 1: Extract Givens**\n-   Data model: $Y_i \\sim N(\\mu, \\sigma^2)$ for $i=1, ..., n$, where $Y_i$ are independent and identically distributed.\n-   Likelihood: Gaussian with unknown mean $\\mu$ and known variance $\\sigma^2$.\n-   Prior: The prior distribution for $\\mu$ is Gaussian, $\\mu \\sim N(\\mu_0, \\tau_0^2)$.\n-   Observations: A sample of size $n$ with sample mean $\\bar{Y}$.\n-   Numerical values:\n    -   Sample size, $n = 15$.\n    -   Known data variance, $\\sigma^2 = 36$.\n    -   Prior mean, $\\mu_0 = 5$.\n    -   Prior variance, $\\tau_0^2 = 100$.\n    -   Sample mean, $\\bar{Y} = 3.2$.\n-   Task:\n    1.  Derive the posterior distribution for $\\mu$ from first principles.\n    2.  Compute the $95\\%$ equal-tailed credible interval for $\\mu$.\n    3.  Round the numerical interval endpoints to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem uses a standard conjugate prior model (Gaussian-Gaussian), which is a fundamental and widely-used technique in Bayesian statistics, particularly in biostatistics. The model is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is well-posed. Given the specified conjugate prior and likelihood, a unique analytical solution for the posterior distribution exists. From this posterior, a unique credible interval can be determined.\n-   **Objective**: The problem is stated in precise, objective, and mathematical language, free from ambiguity or subjective claims.\n-   **Completeness and Consistency**: All necessary parameters ($n$, $\\bar{Y}$, $\\sigma^2$, $\\mu_0$, $\\tau_0^2$) for deriving the posterior and calculating the credible interval are provided. There are no contradictions in the setup.\n-   **Realism**: The context of a clinical trial for blood pressure and the given numerical values are plausible.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution process will now proceed.\n\n### Derivation of the Posterior Distribution\n\nThe derivation begins with Bayes' theorem, which states that the posterior distribution is proportional to the product of the likelihood and the prior distribution.\n$$\np(\\mu | \\mathbf{Y}) \\propto p(\\mathbf{Y} | \\mu) p(\\mu)\n$$\nHere, $\\mathbf{Y} = \\{Y_1, \\dots, Y_n\\}$ represents the observed data.\n\n**1. The Likelihood Function**\nThe observations $Y_i$ are i.i.d. draws from a $N(\\mu, \\sigma^2)$ distribution. The joint probability density function (the likelihood) is:\n$$\np(\\mathbf{Y} | \\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(Y_i - \\mu)^2}{2\\sigma^2}\\right)\n$$\nFor a Gaussian distribution with known variance $\\sigma^2$, the sample mean $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ is a sufficient statistic for $\\mu$. The distribution of the sample mean is $\\bar{Y} \\sim N(\\mu, \\sigma^2/n)$. We can therefore use the likelihood of $\\bar{Y}$ instead of the full data $\\mathbf{Y}$, which simplifies the calculation without loss of information about $\\mu$.\nThe likelihood function of $\\mu$ given $\\bar{Y}$ is:\n$$\nL(\\mu | \\bar{Y}) = p(\\bar{Y} | \\mu) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2/n)}} \\exp\\left(-\\frac{(\\bar{Y} - \\mu)^2}{2(\\sigma^2/n)}\\right)\n$$\nAs a function of $\\mu$, this is proportional to:\n$$\nL(\\mu | \\bar{Y}) \\propto \\exp\\left(-\\frac{n(\\mu - \\bar{Y})^2}{2\\sigma^2}\\right)\n$$\n\n**2. The Prior Distribution**\nThe problem states a Gaussian prior for $\\mu$: $\\mu \\sim N(\\mu_0, \\tau_0^2)$. The probability density function is:\n$$\np(\\mu) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2\\tau_0^2}\\right)\n$$\nProportionally,\n$$\np(\\mu) \\propto \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2\\tau_0^2}\\right)\n$$\n\n**3. The Posterior Distribution**\nUsing Bayes' theorem, the posterior $p(\\mu | \\bar{Y})$ is proportional to the product of the likelihood and the prior:\n$$\np(\\mu | \\bar{Y}) \\propto \\exp\\left(-\\frac{n(\\mu - \\bar{Y})^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2\\tau_0^2}\\right)\n$$\n$$\np(\\mu | \\bar{Y}) \\propto \\exp\\left[-\\frac{1}{2}\\left(\\frac{n(\\mu - \\bar{Y})^2}{\\sigma^2} + \\frac{(\\mu - \\mu_0)^2}{\\tau_0^2}\\right)\\right]\n$$\nTo identify the form of this distribution, we expand the terms in the exponent and complete the square with respect to $\\mu$. The exponent, ignoring the factor of $-1/2$, is:\n$$\n\\frac{n}{\\sigma^2}(\\mu^2 - 2\\mu\\bar{Y} + \\bar{Y}^2) + \\frac{1}{\\tau_0^2}(\\mu^2 - 2\\mu\\mu_0 + \\mu_0^2)\n$$\nCollecting terms based on powers of $\\mu$:\n$$\n\\mu^2\\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) - 2\\mu\\left(\\frac{n\\bar{Y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) + \\left(\\frac{n\\bar{Y}^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)\n$$\nThe terms not involving $\\mu$ can be absorbed into the proportionality constant. The posterior is a Gaussian distribution, $p(\\mu | \\bar{Y}) \\sim N(\\mu_1, \\tau_1^2)$, whose density is proportional to $\\exp\\left(-\\frac{(\\mu-\\mu_1)^2}{2\\tau_1^2}\\right)$. The exponent of this form is:\n$$\n-\\frac{1}{2\\tau_1^2}(\\mu^2 - 2\\mu\\mu_1 + \\mu_1^2) = -\\frac{1}{2}\\left[\\frac{1}{\\tau_1^2}\\mu^2 - \\frac{2\\mu_1}{\\tau_1^2}\\mu + \\frac{\\mu_1^2}{\\tau_1^2}\\right]\n$$\nBy comparing the coefficients of the $\\mu^2$ and $\\mu$ terms, we can find the posterior parameters $\\mu_1$ and $\\tau_1^2$.\nComparing the $\\mu^2$ coefficients:\n$$\n\\frac{1}{\\tau_1^2} = \\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}\n$$\nThis shows that the posterior precision ($1/\\tau_1^2$) is the sum of the data precision ($n/\\sigma^2$) and the prior precision ($1/\\tau_0^2$).\nComparing the $\\mu$ coefficients:\n$$\n\\frac{\\mu_1}{\\tau_1^2} = \\frac{n\\bar{Y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\n$$\nSolving for the posterior mean $\\mu_1$:\n$$\n\\mu_1 = \\tau_1^2 \\left(\\frac{n\\bar{Y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) = \\frac{\\frac{n\\bar{Y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}}\n$$\nThus, the posterior distribution for $\\mu$ is a Gaussian distribution, $p(\\mu|\\mathbf{Y}) \\sim N(\\mu_1, \\tau_1^2)$, with the parameters as derived.\n\n### Calculation of Posterior Parameters\nNow, we substitute the given numerical values: $n = 15$, $\\sigma^2 = 36$, $\\mu_0 = 5$, $\\tau_0^2 = 100$, and $\\bar{Y} = 3.2$.\n\nFirst, we calculate the posterior precision and variance:\n$$\n\\frac{1}{\\tau_1^2} = \\frac{15}{36} + \\frac{1}{100} = \\frac{5}{12} + \\frac{1}{100} = \\frac{125}{300} + \\frac{3}{300} = \\frac{128}{300} = \\frac{32}{75}\n$$\nThe posterior variance is:\n$$\n\\tau_1^2 = \\frac{75}{32} = 2.34375\n$$\nNext, we calculate the posterior mean:\n$$\n\\mu_1 = \\tau_1^2 \\left( \\frac{n\\bar{Y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) = \\frac{75}{32} \\left( \\frac{15 \\times 3.2}{36} + \\frac{5}{100} \\right)\n$$\n$$\n\\mu_1 = \\frac{75}{32} \\left( \\frac{48}{36} + \\frac{1}{20} \\right) = \\frac{75}{32} \\left( \\frac{4}{3} + \\frac{1}{20} \\right)\n$$\n$$\n\\mu_1 = \\frac{75}{32} \\left( \\frac{80 + 3}{60} \\right) = \\frac{75}{32} \\left( \\frac{83}{60} \\right) = \\frac{5 \\times 15}{32} \\frac{83}{4 \\times 15} = \\frac{5 \\times 83}{128} = \\frac{415}{128}\n$$\n$$\n\\mu_1 = 3.2421875\n$$\nSo, the posterior distribution is $\\mu|\\mathbf{Y} \\sim N(3.2421875, 2.34375)$.\n\n### Calculation of the 95% Credible Interval\nA $95\\%$ equal-tailed credible interval for a normally distributed parameter $\\mu \\sim N(\\mu_1, \\tau_1^2)$ is given by $[\\mu_1 - z_{0.975} \\tau_1, \\mu_1 + z_{0.975} \\tau_1]$, where $\\tau_1 = \\sqrt{\\tau_1^2}$ is the posterior standard deviation and $z_{0.975}$ is the $97.5$-th percentile of the standard normal distribution.\n\nThe value of $z_{0.975}$ is approximately $1.959964$.\nThe posterior standard deviation is:\n$$\n\\tau_1 = \\sqrt{\\tau_1^2} = \\sqrt{\\frac{75}{32}} \\approx 1.5309311\n$$\nThe margin of error for the interval is:\n$$\nw = z_{0.975} \\tau_1 \\approx 1.959964 \\times 1.5309311 \\approx 3.000570\n$$\nThe lower bound of the credible interval is:\n$$\nL = \\mu_1 - w \\approx 3.2421875 - 3.000570 = 0.2416175\n$$\nThe upper bound of the credible interval is:\n$$\nU = \\mu_1 + w \\approx 3.2421875 + 3.000570 = 6.2427575\n$$\nRounding these values to four significant figures, as requested:\nLower bound: $0.2416$\nUpper bound: $6.243$\n\nThe $95\\%$ credible interval for the average change in systolic blood pressure, $\\mu$, is approximately $[0.2416, 6.243]$ $\\mathrm{mmHg}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.2416 & 6.243\n\\end{pmatrix}\n}\n$$", "id": "4896865"}, {"introduction": "While the logic of Bayesian inference is universal, its practical application often involves careful choices, especially regarding the prior distribution. This exercise delves into the analysis of proportions using the Beta-Binomial conjugate model, a workhorse of biostatistics for binary outcomes. You will explore how different priors—from the \"uninformative\" Jeffreys prior to a weakly informative prior designed for rare events—shape the resulting credible intervals, particularly when data is sparse, and contrast these with the classic frequentist Clopper-Pearson interval [@problem_id:4896696].", "problem": "A study concerns a rare disease with binary outcomes across small cohorts. Let $X$ denote the number of observed cases among $n$ independent individuals, with $X \\sim \\mathrm{Binomial}(n, \\theta)$ where $\\theta \\in (0,1)$ is the unknown disease probability. Starting from the fundamental base of the binomial likelihood $\\mathcal{L}(\\theta \\mid x,n) \\propto \\theta^{x} (1-\\theta)^{n-x}$ and Beta prior conjugacy, use the following definitions to construct interval estimates and compute a program to compare them:\n\n1. Bayesian approach with Beta priors: A Beta prior $\\mathrm{Beta}(a,b)$ is conjugate to the binomial likelihood, yielding a posterior distribution $\\theta \\mid x,n \\sim \\mathrm{Beta}(a + x, b + n - x)$. An equal-tailed Bayesian credible interval at level $1-\\alpha$ is defined by the posterior quantiles $q_{\\alpha/2}$ and $q_{1-\\alpha/2}$ satisfying $\\mathbb{P}(\\theta \\le q_{\\alpha/2} \\mid x,n) = \\alpha/2$ and $\\mathbb{P}(\\theta \\le q_{1-\\alpha/2} \\mid x,n) = 1-\\alpha/2$.\n\n2. Exact frequentist approach by inversion: The exact Clopper–Pearson (CP) interval at level $1-\\alpha$ is obtained by inverting the binomial test, defined as the set of $\\theta$ values not rejected by a two-sided exact test at level $\\alpha$. This inversion can be expressed in terms of Beta cumulative distribution functions for computing interval endpoints and requires careful handling at the boundaries $x=0$ and $x=n$.\n\nImplement a program that, for each test case $(n,x)$ in the test suite below, computes:\n- The $1-\\alpha$ exact Clopper–Pearson interval for $\\theta$.\n- The $1-\\alpha$ equal-tailed Bayesian credible intervals for $\\theta$ under three priors:\n  - Prior J (Jeffreys): $\\mathrm{Beta}(a,b)$ with $(a,b)=(0.5,0.5)$.\n  - Prior U (Uniform): $\\mathrm{Beta}(a,b)$ with $(a,b)=(1,1)$.\n  - Prior R (Rare-leaning weakly informative): $\\mathrm{Beta}(a,b)$ with $(a,b)=(0.5,9.5)$.\n\nUse $\\alpha = 0.05$ (so the nominal level is $1-\\alpha = 0.95$). For each test case, output a single list containing the integers $n$ and $x$, followed by the lower and upper endpoints of the four intervals in the fixed order: CP, J, U, R. Each interval endpoint must be rounded to $6$ decimal places and expressed as a decimal (no percentage signs). The final output of the program must be a single line containing a list of these per-test-case lists, comma-separated and enclosed in square brackets.\n\nTest suite:\n- Case A (happy path, rare event observed): $(n,x) = (20,1)$.\n- Case B (boundary, no events): $(n,x) = (5,0)$.\n- Case C (boundary, all events): $(n,x) = (10,10)$.\n- Case D (small sample, a few events): $(n,x) = (8,2)$.\n- Case E (rare and larger $n$ with no events): $(n,x) = (50,0)$.\n- Case F (boundary, no events with moderate $n$): $(n,x) = (20,0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list in the order:\n$[n, x, \\mathrm{CP\\_lower}, \\mathrm{CP\\_upper}, \\mathrm{J\\_lower}, \\mathrm{J\\_upper}, \\mathrm{U\\_lower}, \\mathrm{U\\_upper}, \\mathrm{R\\_lower}, \\mathrm{R\\_upper}]$.\n\nAll numerical answers must be in decimal form rounded to $6$ decimal places. No physical units are involved. Angles are not involved. Percentages must not be used; probabilities must be decimals in $(0,1)$.", "solution": "The problem requires the computation of frequentist and Bayesian interval estimates for a binomial proportion, $\\theta$, based on an observation of $x$ successes in $n$ trials. We are given the functional form of the binomial likelihood, $\\mathcal{L}(\\theta \\mid x,n) \\propto \\theta^{x} (1-\\theta)^{n-x}$, and the definitions for the exact Clopper-Pearson confidence interval and equal-tailed Bayesian credible intervals derived from Beta posterior distributions. The nominal confidence/credibility level is specified as $1-\\alpha = 0.95$, corresponding to $\\alpha = 0.05$.\n\nThe analysis proceeds by deriving the computational formulas for each of the four requested intervals: the Clopper-Pearson (CP) interval, and three Bayesian credible intervals corresponding to Jeffreys (J), Uniform (U), and a Rare-leaning (R) prior.\n\n### Bayesian Credible Intervals\n\nThe Bayesian framework combines a prior distribution for the parameter $\\theta$ with the likelihood of the observed data to form a posterior distribution, which represents our updated belief about $\\theta$. The problem specifies the use of a Beta prior, $\\theta \\sim \\mathrm{Beta}(a,b)$. The Beta distribution is the conjugate prior for the binomial likelihood, meaning the posterior distribution is also a Beta distribution.\n\nGiven a prior $\\theta \\sim \\mathrm{Beta}(a,b)$ and data $X=x$, the posterior distribution for $\\theta$ is:\n$$ \\theta \\mid x, n \\sim \\mathrm{Beta}(a' = a + x, b' = b + n - x) $$\n\nA $(1-\\alpha)$ equal-tailed credible interval is defined by the $\\alpha/2$ and $1-\\alpha/2$ quantiles of this posterior distribution. Let $F^{-1}_{\\mathrm{Beta}(a',b')}$ denote the percent point function (PPF), or the inverse of the cumulative distribution function (CDF), for a $\\mathrm{Beta}(a', b')$ distribution. The interval endpoints $[\\theta_L, \\theta_U]$ are:\n$$ \\theta_L = F^{-1}_{\\mathrm{Beta}(a',b')}(\\alpha/2) $$\n$$ \\theta_U = F^{-1}_{\\mathrm{Beta}(a',b')}(1-\\alpha/2) $$\n\nWe apply this general formula to the three specified priors with $\\alpha = 0.05$.\n\n1.  **Prior J (Jeffreys)**: $\\mathrm{Beta}(0.5, 0.5)$. This is a common non-informative prior.\n    - Posterior distribution: $\\theta \\mid x,n \\sim \\mathrm{Beta}(x + 0.5, n - x + 0.5)$.\n    - The interval endpoints are the $0.025$ and $0.975$ quantiles of this distribution.\n\n2.  **Prior U (Uniform)**: $\\mathrm{Beta}(1, 1)$. This corresponds to a uniform prior on $\\theta$ over $(0,1)$, representing initial indifference to any particular value of $\\theta$.\n    - Posterior distribution: $\\theta \\mid x,n \\sim \\mathrm{Beta}(x + 1, n - x + 1)$.\n    - The interval endpoints are the $0.025$ and $0.975$ quantiles of this distribution.\n\n3.  **Prior R (Rare-leaning)**: $\\mathrm{Beta}(0.5, 9.5)$. This is a weakly informative prior that places more prior probability on smaller values of $\\theta$, suitable for situations where the event is believed to be rare. The prior mean is $0.5 / (0.5+9.5) = 0.05$.\n    - Posterior distribution: $\\theta \\mid x,n \\sim \\mathrm{Beta}(x + 0.5, n - x + 9.5)$.\n    - The interval endpoints are the $0.025$ and $0.975$ quantiles of this distribution.\n\n### Frequentist Clopper-Pearson (CP) Interval\n\nThe Clopper-Pearson interval is an \"exact\" confidence interval constructed by inverting a two-sided test for the binomial proportion $\\theta$. The interval $[\\theta_L, \\theta_U]$ includes all values $\\theta_0$ for which the null hypothesis $H_0: \\theta = \\theta_0$ would not be rejected at a significance level of $\\alpha$.\n\nThe lower bound, $\\theta_L$, is defined by the equation:\n$$ \\mathbb{P}(X \\ge x \\mid \\theta = \\theta_L) = \\sum_{k=x}^{n} \\binom{n}{k} \\theta_L^k (1-\\theta_L)^{n-k} = \\alpha/2 $$\n\nThe upper bound, $\\theta_U$, is defined by:\n$$ \\mathbb{P}(X \\le x \\mid \\theta = \\theta_U) = \\sum_{k=0}^{x} \\binom{n}{k} \\theta_U^k (1-\\theta_U)^{n-k} = \\alpha/2 $$\n\nThese equations can be solved for $\\theta_L$ and $\\theta_U$ by leveraging a well-known identity that connects the binomial tail probabilities to the CDF of the Beta distribution.\nThe lower bound $\\theta_L$ is the value that satisfies the first equation, which is equivalent to being the $\\alpha/2$ quantile of a $\\mathrm{Beta}(x, n - x + 1)$ distribution.\nThe upper bound $\\theta_U$ is the value that satisfies the second equation, which is equivalent to being the $1-\\alpha/2$ quantile of a $\\mathrm{Beta}(x + 1, n - x)$ distribution.\n\nTherefore, the endpoints are calculated as:\n$$ \\theta_L = F^{-1}_{\\mathrm{Beta}(x, n-x+1)}(\\alpha/2) $$\n$$ \\theta_U = F^{-1}_{\\mathrm{Beta}(x+1, n-x)}(1-\\alpha/2) $$\n\nBoundary cases require special attention:\n-   If $x=0$, the lower bound equation becomes $\\mathbb{P}(X \\ge 0 \\mid \\theta_L) = 1 = \\alpha/2$, which has no solution for $\\theta_L \\in (0,1)$. By convention, and to maintain coverage, the lower bound is set to $\\theta_L = 0$. The formula for the upper bound remains valid: $\\theta_U = F^{-1}_{\\mathrm{Beta}(1, n)}(1-\\alpha/2)$.\n-   If $x=n$, the upper bound equation becomes $\\mathbb{P}(X \\le n \\mid \\theta_U) = 1 = \\alpha/2$, which similarly has no solution. The upper bound is set to $\\theta_U = 1$. The formula for the lower bound remains valid: $\\theta_L = F^{-1}_{\\mathrm{Beta}(n, 1)}(\\alpha/2)$.\n\nThese formulas provide the basis for the implementation. The required computations for the quantiles of the Beta distribution are performed using numerical libraries. For each test case $(n,x)$, we compute the lower and upper bounds for the four intervals and format them as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Computes Clopper-Pearson and Bayesian credible intervals for a binomial proportion\n    for a given set of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (20, 1),   # Case A\n        (5, 0),    # Case B\n        (10, 10),  # Case C\n        (8, 2),    # Case D\n        (50, 0),   # Case E\n        (20, 0)    # Case F\n    ]\n\n    alpha = 0.05\n    results = []\n\n    for n, x in test_cases:\n        # --- 1. Clopper-Pearson (CP) Interval ---\n        # Special handling for boundary cases as scipy.stats.beta.ppf requires\n        # shape parameters a, b > 0.\n        if x == 0:\n            cp_lower = 0.0\n        else:\n            cp_lower = beta.ppf(alpha / 2, x, n - x + 1)\n        \n        if x == n:\n            cp_upper = 1.0\n        else:\n            cp_upper = beta.ppf(1 - alpha / 2, x + 1, n - x)\n\n        # --- 2. Bayesian Credible Intervals ---\n        \n        # Prior J (Jeffreys): Beta(0.5, 0.5)\n        a_j, b_j = 0.5, 0.5\n        post_a_j = a_j + x\n        post_b_j = b_j + n - x\n        j_lower = beta.ppf(alpha / 2, post_a_j, post_b_j)\n        j_upper = beta.ppf(1 - alpha / 2, post_a_j, post_b_j)\n\n        # Prior U (Uniform): Beta(1, 1)\n        a_u, b_u = 1.0, 1.0\n        post_a_u = a_u + x\n        post_b_u = b_u + n - x\n        u_lower = beta.ppf(alpha / 2, post_a_u, post_b_u)\n        u_upper = beta.ppf(1 - alpha / 2, post_a_u, post_b_u)\n\n        # Prior R (Rare-leaning): Beta(0.5, 9.5)\n        a_r, b_r = 0.5, 9.5\n        post_a_r = a_r + x\n        post_b_r = b_r + n - x\n        r_lower = beta.ppf(alpha / 2, post_a_r, post_b_r)\n        r_upper = beta.ppf(1 - alpha / 2, post_a_r, post_b_r)\n\n        # Assemble the list for the current test case, with rounding\n        case_result = [\n            n, x,\n            round(cp_lower, 6), round(cp_upper, 6),\n            round(j_lower, 6), round(j_upper, 6),\n            round(u_lower, 6), round(u_upper, 6),\n            round(r_lower, 6), round(r_upper, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The format required is a list of lists, printed as a string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4896696"}, {"introduction": "In many real-world statistical models, the posterior distribution does not have a simple, clean analytical form. This practice introduces the Laplace approximation, a powerful technique that uses a Gaussian distribution to approximate the posterior, making inference tractable. By analyzing an epidemiological incidence rate, you will not only apply this approximation but also explore the critical concept of parameter transformation, contrasting the symmetric, additive credible interval on the log-rate scale ($\\eta = \\ln \\lambda$) with the asymmetric, multiplicative interval on the natural rate scale ($\\lambda$) [@problem_id:4896872].", "problem": "A cohort study in infectious disease epidemiology followed individuals for a total exposure time of $T=120$ person-years and recorded $y=25$ incident events. Assume a Poisson process model for the event count with an incidence rate parameter $\\lambda$ (events per person-year), so that $Y \\mid \\lambda \\sim \\text{Poisson}(T \\lambda)$. Define the log-rate parameter $\\eta = \\ln \\lambda$.\n\nUsing only the following foundational elements:\n- The Poisson likelihood for $y$ given $\\eta$ obtained by the reparameterization $\\lambda = \\exp(\\eta)$.\n- A weakly informative prior for $\\eta$ that is approximately flat in a neighborhood of the posterior mode.\n- A second-order Taylor expansion of the log-posterior around its mode (Laplace approximation) to justify an approximately Gaussian posterior for $\\eta$ with variance given by the inverse of the observed information.\n\nCarry out the following steps:\n1. Derive the log-likelihood for $\\eta$, find the posterior mode $\\hat{\\eta}$ under the approximately flat prior, and compute the observed information at $\\hat{\\eta}$.\n2. Using the quadratic approximation, identify the approximate posterior distribution for $\\eta \\mid y, T$ and construct an equal-tailed $0.95$ credible interval for $\\eta$.\n3. Use the monotonicity of the exponential transformation to map the credible interval for $\\eta$ to a corresponding equal-tailed $0.95$ credible interval for $\\lambda$.\n4. Briefly articulate the interpretational differences between reporting a credible interval on the log-rate scale versus the rate scale in epidemiologic applications (e.g., symmetry, multiplicative versus additive interpretation).\n\nRound all four interval endpoints to four significant figures. Express rates in units of events per person-year when discussing, but do not include units in your final numeric answer. Report your final numeric answer as a row matrix $(\\eta_{\\text{L}},\\ \\eta_{\\text{U}},\\ \\lambda_{\\text{L}},\\ \\lambda_{\\text{U}})$ in that order.", "solution": "The problem requires us to perform a Bayesian analysis of an incidence rate from a Poisson process model using a Laplace approximation for the posterior distribution. We are given the total exposure time $T=120$ person-years and the number of observed events $y=25$.\n\nThe model for the number of events $Y$ is a Poisson distribution with parameter $T\\lambda$, where $\\lambda$ is the incidence rate. The probability mass function is:\n$$p(Y=y \\mid \\lambda) = \\frac{(T\\lambda)^y \\exp(-T\\lambda)}{y!}$$\nWe are working with the log-rate parameter $\\eta = \\ln \\lambda$, which implies the reparameterization $\\lambda = \\exp(\\eta)$.\n\n**Step 1: Log-likelihood, Posterior Mode, and Observed Information**\n\nFirst, we derive the likelihood for $\\eta$ by substituting $\\lambda = \\exp(\\eta)$ into the Poisson probability mass function. The likelihood function $L(\\eta \\mid y, T)$ is proportional to $p(y \\mid \\eta)$:\n$$L(\\eta \\mid y, T) \\propto (T\\exp(\\eta))^y \\exp(-T\\exp(\\eta))$$\nThe log-likelihood function, $\\ell(\\eta)$, is the natural logarithm of the likelihood. We can disregard any terms that do not depend on $\\eta$:\n$$\\ell(\\eta) = \\ln[ (T\\exp(\\eta))^y \\exp(-T\\exp(\\eta)) ] + C_1$$\n$$\\ell(\\eta) = y \\ln(T\\exp(\\eta)) - T\\exp(\\eta) + C_2$$\n$$\\ell(\\eta) = y(\\ln T + \\ln(\\exp(\\eta))) - T\\exp(\\eta) + C_2$$\nDropping the term $y\\ln T$ which is constant with respect to $\\eta$, we define the kernel of the log-likelihood as:\n$$\\ell(\\eta) = y\\eta - T\\exp(\\eta)$$\nThe problem states we should use a weakly informative prior for $\\eta$ that is approximately flat. This can be expressed as $p(\\eta) \\propto 1$. The log-posterior, denoted $g(\\eta)$, is the sum of the log-likelihood and the log-prior:\n$$g(\\eta) = \\ell(\\eta) + \\ln(p(\\eta)) \\approx y\\eta - T\\exp(\\eta) + \\text{constant}$$\nThe posterior mode, $\\hat{\\eta}$, is the value of $\\eta$ that maximizes the log-posterior. We find it by taking the first derivative with respect to $\\eta$ and setting it to $0$:\n$$\\frac{dg}{d\\eta} = \\frac{d}{d\\eta}(y\\eta - T\\exp(\\eta)) = y - T\\exp(\\eta)$$\nSetting the derivative to $0$:\n$$y - T\\exp(\\hat{\\eta}) = 0 \\implies \\exp(\\hat{\\eta}) = \\frac{y}{T}$$\n$$\\hat{\\eta} = \\ln\\left(\\frac{y}{T}\\right)$$\nThe observed information, $I(\\eta)$, is defined as the negative of the second derivative of the log-posterior (or, in this case with a flat prior, the log-likelihood) with respect to $\\eta$:\n$$I(\\eta) = -\\frac{d^2g}{d\\eta^2}$$\nThe second derivative is:\n$$\\frac{d^2g}{d\\eta^2} = \\frac{d}{d\\eta}(y - T\\exp(\\eta)) = -T\\exp(\\eta)$$\nTherefore, the observed information is:\n$$I(\\eta) = -(-T\\exp(\\eta)) = T\\exp(\\eta)$$\nWe evaluate the observed information at the posterior mode, $\\hat{\\eta}$:\n$$I(\\hat{\\eta}) = T\\exp(\\hat{\\eta}) = T\\left(\\frac{y}{T}\\right) = y$$\nGiven $y=25$, the observed information at the mode is $I(\\hat{\\eta}) = 25$.\n\n**Step 2: Approximate Posterior and Credible Interval for $\\eta$**\n\nThe Laplace approximation states that the posterior distribution of a parameter can be approximated by a Gaussian (Normal) distribution centered at its mode, with variance equal to the inverse of the observed information at the mode.\nThus, the approximate posterior distribution for $\\eta$ is:\n$$\\eta \\mid y, T \\sim \\mathcal{N}(\\text{mean}=\\hat{\\eta}, \\text{variance}=[I(\\hat{\\eta})]^{-1})$$\nSubstituting the expressions we found:\n$$\\eta \\mid y, T \\approx \\mathcal{N}\\left(\\ln\\left(\\frac{y}{T}\\right), \\frac{1}{y}\\right)$$\nFor an equal-tailed $0.95$ credible interval, we use the quantiles of the standard Normal distribution. The confidence level is $1-\\alpha = 0.95$, so $\\alpha=0.05$. The required z-score is $z_{1-\\alpha/2} = z_{0.975}$, which is approximately $1.959964$. The interval is given by:\n$$[\\eta_L, \\eta_U] = \\hat{\\eta} \\pm z_{0.975} \\sqrt{\\text{variance}} = \\hat{\\eta} \\pm z_{0.975} \\frac{1}{\\sqrt{y}}$$\nPlugging in the given values $y=25$ and $T=120$:\n$$\\hat{\\eta} = \\ln\\left(\\frac{25}{120}\\right) = \\ln\\left(\\frac{5}{24}\\right) \\approx -1.56861$$\nThe standard deviation (or standard error of the estimate) is $\\frac{1}{\\sqrt{y}} = \\frac{1}{\\sqrt{25}} = \\frac{1}{5} = 0.2$.\nThe margin of error is $z_{0.975} \\times 0.2 \\approx 1.959964 \\times 0.2 \\approx 0.39199$.\nThe lower bound is:\n$$\\eta_L = -1.56861 - 0.39199 = -1.96060$$\nThe upper bound is:\n$$\\eta_U = -1.56861 + 0.39199 = -1.17662$$\nRounding to four significant figures, we get $\\eta_L = -1.961$ and $\\eta_U = -1.177$.\n\n**Step 3: Credible Interval for $\\lambda$**\n\nThe relationship between $\\lambda$ and $\\eta$ is $\\lambda = \\exp(\\eta)$. Since the exponential function is strictly monotonic (increasing), a credible interval for $\\eta$ can be transformed directly into a credible interval for $\\lambda$ by applying the function to the endpoints.\n$$[\\lambda_L, \\lambda_U] = [\\exp(\\eta_L), \\exp(\\eta_U)]$$\nUsing the unrounded values for $\\eta_L$ and $\\eta_U$ for better precision:\n$$\\lambda_L = \\exp(-1.96060) \\approx 0.14077$$\n$$\\lambda_U = \\exp(-1.17662) \\approx 0.30832$$\nRounding to four significant figures, we get $\\lambda_L = 0.1408$ and $\\lambda_U = 0.3083$.\n\n**Step 4: Interpretation Differences**\n\nThere are important interpretational differences between the credible intervals for the log-rate $\\eta$ and the rate $\\lambda$.\n\n*   **Credible Interval for $\\eta$ (log-rate scale)**: The interval for $\\eta$, $[\\eta_L, \\eta_U]$, is symmetric around the posterior mode $\\hat{\\eta}$. This is a direct consequence of the Gaussian approximation. This interval implies an **additive** model for uncertainty. For example, $\\eta$ is given as $\\hat{\\eta} \\pm \\text{margin of error}$. This scale is often preferred for statistical modeling (e.g., in a Poisson regression like $\\ln(\\lambda) = \\beta_0 + \\beta_1 x$) because the parameter space is the entire real line ($-\\infty$ to $\\infty$), which makes the Normal approximation more accurate.\n\n*   **Credible Interval for $\\lambda$ (rate scale)**: The interval for $\\lambda$, $[\\lambda_L, \\lambda_U]$, is obtained by exponentiating the endpoints of the interval for $\\eta$. This transformation results in an interval that is **asymmetric** around the point estimate $\\hat{\\lambda} = \\exp(\\hat{\\eta}) = y/T \\approx 0.2083$. The distance from the lower bound to the estimate ($\\hat{\\lambda}-\\lambda_L$) is smaller than the distance from the estimate to the upper bound ($\\lambda_U-\\hat{\\lambda}$). This asymmetry reflects the underlying log-normal distribution of $\\lambda$ (since $\\eta$ is approximately Normal) and respects the physical constraint that a rate $\\lambda$ must be positive. The uncertainty on this scale is best understood in **multiplicative** terms. The interval can be written as $[\\hat{\\lambda} \\cdot k^{-1}, \\hat{\\lambda} \\cdot k]$, where $k = \\exp(z_{0.975} / \\sqrt{y})$. The point estimate is multiplied and divided by the same factor. In epidemiology, the rate scale is more directly interpretable (events per person-time) for communication and public health decisions, while the log-rate scale is more convenient for statistical inference.\n\nFinal numeric values to report are $\\eta_L=-1.961$, $\\eta_U=-1.177$, $\\lambda_L=0.1408$, and $\\lambda_U=0.3083$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1.961 & -1.177 & 0.1408 & 0.3083\n\\end{pmatrix}\n}\n$$", "id": "4896872"}]}