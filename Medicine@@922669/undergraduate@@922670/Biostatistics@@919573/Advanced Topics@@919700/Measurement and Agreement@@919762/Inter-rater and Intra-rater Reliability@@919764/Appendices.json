{"hands_on_practices": [{"introduction": "Measuring reliability begins with a foundational question: is the agreement we observe between raters genuine, or could it have happened by chance? This exercise provides a hands-on guide to calculating Cohen's kappa, a cornerstone metric for inter-rater reliability with categorical data. By dissecting a clinical scenario, you will learn to distinguish between raw observed agreement and chance-corrected agreement, a critical skill for interpreting the true consistency of raters' judgments. [@problem_id:4917600]", "problem": "A hospital quality study evaluates the inter-rater reliability (IRR) of two board-certified radiologists classifying computed tomography scans as either \"lesion present\" or \"lesion absent.\" Across a set of $200$ independent scans, the following joint outcomes were recorded:\n- Both radiologists recorded \"lesion present\" for $56$ scans.\n- Radiologist A recorded \"lesion present\" and Radiologist B recorded \"lesion absent\" for $24$ scans.\n- Radiologist A recorded \"lesion absent\" and Radiologist B recorded \"lesion present\" for $14$ scans.\n- Both radiologists recorded \"lesion absent\" for $106$ scans.\n\nUsing only fundamental definitions, proceed as follows:\n1. Define the observed proportion of agreement, denoted $P_{o}$, from the joint outcomes in the $2 \\times 2$ table.\n2. Under the assumption that each radiologist’s label assignment is statistically independent of the other’s (conditional on the case), derive the expected proportion of agreement $P_{e}$ from the marginal label probabilities.\n3. Construct a chance-corrected agreement index that is $0$ when $P_{o} = P_{e}$ and $1$ when $P_{o} = 1$, and use it to compute Cohen’s kappa $\\kappa$.\n\nCompute the raw agreement $P_{o}$ as a decimal and briefly compare it to the chance-corrected agreement embodied in $\\kappa$ to illustrate the role of chance correction. Round your final value of Cohen’s $\\kappa$ to four significant figures. Provide only the final numerical value of $\\kappa$ as your answer.", "solution": "The goal is to quantify agreement between two raters beyond chance. The foundational quantities for a $2 \\times 2$ classification table are the observed proportion of agreement $P_{o}$ and the expected proportion of agreement $P_{e}$ under independence of rater assignments given the marginal label probabilities.\n\nFirst, we compute the observed proportion of agreement $P_{o}$. Agreement occurs when both raters assign the same label. Let the total number of cases be $N$. From the data, the diagonal counts are $56$ (both \"lesion present\") and $106$ (both \"lesion absent\"), with $N = 200$. Therefore,\n$$\nP_{o} \\equiv \\frac{\\text{number of agreements}}{N} = \\frac{56 + 106}{200} = \\frac{162}{200} = 0.81.\n$$\n\nSecond, we derive the expected proportion of agreement $P_{e}$ under the assumption that the two raters’ label assignments are statistically independent, conditional on the case. Under independence, the probability that both assign \"lesion present\" equals the product of their marginal probabilities of assigning \"lesion present\"; similarly for \"lesion absent.\"\n\nCompute the marginal probabilities:\n- Radiologist A: \"present\" total is $56 + 24 = 80$, so $P(\\text{A present}) = \\frac{80}{200} = 0.4$; \"absent\" total is $14 + 106 = 120$, so $P(\\text{A absent}) = \\frac{120}{200} = 0.6$.\n- Radiologist B: \"present\" total is $56 + 14 = 70$, so $P(\\text{B present}) = \\frac{70}{200} = 0.35$; \"absent\" total is $24 + 106 = 130$, so $P(\\text{B absent}) = \\frac{130}{200} = 0.65$.\n\nUnder independence,\n$$\nP_{e} \\equiv P(\\text{both present}) + P(\\text{both absent}) \n= \\left(\\frac{80}{200}\\right)\\left(\\frac{70}{200}\\right) + \\left(\\frac{120}{200}\\right)\\left(\\frac{130}{200}\\right)\n= (0.4)(0.35) + (0.6)(0.65)\n= 0.14 + 0.39\n= 0.53.\n$$\n\nThird, we construct a chance-corrected agreement index that satisfies two properties:\n- It equals $0$ when observed agreement equals expected agreement, $P_{o} = P_{e}$.\n- It equals $1$ when observed agreement is perfect, $P_{o} = 1$.\n\nLet the above-chance agreement be $P_{o} - P_{e}$. The maximum attainable above-chance agreement is $1 - P_{e}$ (since $P_{o}$ cannot exceed $1$). A natural normalization is to divide by this maximum, yielding\n$$\n\\kappa \\equiv \\frac{P_{o} - P_{e}}{1 - P_{e}}.\n$$\nThis quantity is known as Cohen’s kappa.\n\nSubstitute the computed values:\n$$\n\\kappa = \\frac{0.81 - 0.53}{1 - 0.53} = \\frac{0.28}{0.47} \\approx 0.5957446809\\ldots\n$$\nRounded to four significant figures, \n$$\n\\kappa \\approx 0.5957.\n$$\n\nInterpretation and comparison to raw agreement: The raw observed agreement is $P_{o} = 0.81$, but a substantial portion of this agreement is attributable to chance under the observed marginals, with $P_{e} = 0.53$. Cohen’s $\\kappa \\approx 0.5957$ quantifies the proportion of the potential agreement beyond chance that was actually achieved, demonstrating how chance correction yields a lower value than the raw agreement when marginal label distributions are imbalanced or when chance agreement is high.", "answer": "$$\\boxed{0.5957}$$", "id": "4917600"}, {"introduction": "While Cohen's kappa is a powerful tool, its interpretation requires care, especially in situations with highly imbalanced data. This practice explores a phenomenon known as the \"kappa paradox,\" where a very high percentage of observed agreement can paradoxically yield a low, seemingly poor kappa value. Working through this hypothetical case of diagnosing a rare disease will build your critical thinking skills and deepen your understanding of how chance correction works, ensuring you can interpret reliability statistics with nuance and accuracy. [@problem_id:4917670]", "problem": "A diagnostic study investigates agreement between two independent raters classifying the presence of a rare disease (binary outcome: Positive/Negative) on a cohort of $N=200$ subjects. The joint classifications are as follows:\n- Both raters say Positive: $1$\n- Rater A says Positive and Rater B says Negative: $5$\n- Rater A says Negative and Rater B says Positive: $3$\n- Both raters say Negative: $191$\n\nThese counts imply highly skewed marginal distributions for the Positive category. Using only foundational definitions of observed agreement and of chance agreement under independence derived from the marginal distributions, compute the inter-rater agreement coefficient known as Cohen’s kappa, then quantify the discrepancy between the observed agreement and kappa by the scalar\n$$\\Delta = p_o - \\kappa,$$\nwhere $p_o$ is the observed agreement probability and $\\kappa$ is Cohen’s kappa. Round your final value of $\\Delta$ to four significant figures. Report only the value of $\\Delta$ with no units or additional text.", "solution": "The problem requires the computation of the discrepancy, $\\Delta$, between the observed agreement probability, $p_o$, and Cohen's kappa coefficient, $\\kappa$. We begin by organizing the given data into a standard $2 \\times 2$ contingency table. Let the two outcomes be 'Positive' (Pos) and 'Negative' (Neg).\n\nThe counts are as follows:\n- Cell $a$: Rater A classifies as Pos, Rater B classifies as Pos. $a = 1$.\n- Cell $b$: Rater A classifies as Pos, Rater B classifies as Neg. $b = 5$.\n- Cell $c$: Rater A classifies as Neg, Rater B classifies as Pos. $c = 3$.\n- Cell $d$: Rater A classifies as Neg, Rater B classifies as Neg. $d = 191$.\n\nThe total number of subjects is $N = a + b + c + d = 1 + 5 + 3 + 191 = 200$.\n\nThe contingency table is:\n$$\n\\begin{array}{c|cc|c}\n & \\text{Rater B: Pos} & \\text{Rater B: Neg} & \\text{Total} \\\\\n\\hline\n\\text{Rater A: Pos} & a = 1 & b = 5 & a+b = 6 \\\\\n\\text{Rater A: Neg} & c = 3 & d = 191 & c+d = 194 \\\\\n\\hline\n\\text{Total} & a+c = 4 & b+d = 196 & N = 200\n\\end{array}\n$$\n\nFirst, we calculate the observed proportional agreement, $p_o$. This is the sum of the proportions of cases where both raters agreed. Agreement occurs in cells $a$ (both Positive) and $d$ (both Negative).\n$$\np_o = \\frac{a + d}{N}\n$$\nSubstituting the given values:\n$$\np_o = \\frac{1 + 191}{200} = \\frac{192}{200} = 0.96\n$$\n\nNext, we calculate the expected probability of agreement by chance, $p_e$. This is calculated based on the marginal totals, under the assumption of statistical independence between the two raters' judgments. The probability of both raters classifying a subject as Positive by chance is the product of their individual probabilities of classifying as Positive. Similarly for the Negative classification.\n\nThe probability that Rater A classifies as Positive is $P_{A,Pos} = \\frac{a+b}{N} = \\frac{6}{200} = 0.03$.\nThe probability that Rater A classifies as Negative is $P_{A,Neg} = \\frac{c+d}{N} = \\frac{194}{200} = 0.97$.\n\nThe probability that Rater B classifies as Positive is $P_{B,Pos} = \\frac{a+c}{N} = \\frac{4}{200} = 0.02$.\nThe probability that Rater B classifies as Negative is $P_{B,Neg} = \\frac{b+d}{N} = \\frac{196}{200} = 0.98$.\n\nThe probability of chance agreement on 'Positive' is $P_{\\text{chance,Pos}} = P_{A,Pos} \\times P_{B,Pos}$.\nThe probability of chance agreement on 'Negative' is $P_{\\text{chance,Neg}} = P_{A,Neg} \\times P_{B,Neg}$.\n\nThe total expected chance agreement, $p_e$, is the sum of these probabilities:\n$$\np_e = (P_{A,Pos} \\times P_{B,Pos}) + (P_{A,Neg} \\times P_{B,Neg})\n$$\n$$\np_e = \\left(\\frac{a+b}{N}\\right)\\left(\\frac{a+c}{N}\\right) + \\left(\\frac{c+d}{N}\\right)\\left(\\frac{b+d}{N}\\right)\n$$\nSubstituting the numerical values:\n$$\np_e = \\left(\\frac{6}{200}\\right)\\left(\\frac{4}{200}\\right) + \\left(\\frac{194}{200}\\right)\\left(\\frac{196}{200}\\right)\n$$\n$$\np_e = (0.03)(0.02) + (0.97)(0.98) = 0.0006 + 0.9506 = 0.9512\n$$\n\nNow, we compute Cohen's kappa coefficient, $\\kappa$, which measures the agreement corrected for chance. The formula is:\n$$\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n$$\nSubstituting the calculated values of $p_o$ and $p_e$:\n$$\n\\kappa = \\frac{0.96 - 0.9512}{1 - 0.9512} = \\frac{0.0088}{0.0488}\n$$\n$$\n\\kappa \\approx 0.180327868...\n$$\n\nFinally, we compute the required discrepancy, $\\Delta$, defined as $\\Delta = p_o - \\kappa$.\n$$\n\\Delta = 0.96 - 0.180327868...\n$$\n$$\n\\Delta \\approx 0.779672131...\n$$\n\nThe problem requires rounding the final value of $\\Delta$ to four significant figures.\nThe value is $\\Delta \\approx 0.779672131...$. The first four significant figures are $7, 7, 9, 6$. The fifth significant figure is $7$, which is $5$ or greater, so we round up the fourth digit.\n$$\n\\Delta \\approx 0.7797\n$$\nThis result highlights the \"kappa paradox,\" where a high observed agreement ($p_o = 0.96$) can be accompanied by a low kappa value ($\\kappa \\approx 0.18$) due to highly skewed marginal distributions, which produce a high probability of chance agreement ($p_e \\approx 0.95$). The discrepancy $\\Delta$ is large, indicating that most of the observed agreement is attributable to chance under the model's assumptions.", "answer": "$$\\boxed{0.7797}$$", "id": "4917670"}, {"introduction": "Real-world studies often involve multiple sources of measurement error, such as different raters, equipment, or occasions. Generalizability Theory (G-theory) provides a comprehensive framework to dissect these various sources of error and plan more reliable studies. This advanced practice moves beyond simply measuring past performance to proactively designing future research. You will conduct a \"Decision Study\" to determine the optimal number of raters and occasions that maximizes reliability while staying within a fixed budget, demonstrating how reliability theory directly informs practical and efficient study design. [@problem_id:4917664]", "problem": "A biostatistics team is planning a study to assess a clinical severity score for patients using multiple raters on multiple occasions. The facets are raters and occasions, both treated as random and exchangeable. A fully crossed Generalizability Theory (GT) design is envisioned where each person is rated by $n_r$ raters on $n_o$ occasions, and the reliability of interest is for the relative ranking of persons based on their mean score across raters and occasions (inter-rater and intra-rater reliability combined).\n\nFrom a prior Generalizability (G) study with $n_r = 4$ raters and $n_o = 2$ occasions, variance components were estimated under a random-effects model:\n- Person variance: $\\sigma_{p}^{2} = 18$.\n- Rater variance: $\\sigma_{r}^{2} = 1.5$.\n- Occasion variance: $\\sigma_{o}^{2} = 2.0$.\n- Rater-by-occasion variance: $\\sigma_{ro}^{2} = 0.5$.\n- Person-by-rater variance: $\\sigma_{pr}^{2} = 6$.\n- Person-by-occasion variance: $\\sigma_{po}^{2} = 9$.\n- Person-by-rater-by-occasion plus residual variance: $\\sigma_{pro,e}^{2} = 12$.\n\nAssume $n_r$ and $n_o$ are positive integers. The study cost per additional rater per person is $c_{r} = 100$, the cost per additional occasion per person is $c_{o} = 80$, and the available per-person budget is $B = 640$.\n\nUsing core definitions from Generalizability Theory (GT), derive from first principles the predicted reliability for relative decisions (the generalizability coefficient for relative decisions) of the person mean across $n_r$ raters and $n_o$ occasions as a function of $n_r$ and $n_o$. Then, under the budget constraint $c_{r} n_{r} + c_{o} n_{o} \\leq B$, determine the design that maximizes this predicted reliability among all admissible integer pairs $(n_r, n_o)$, and report the maximal predicted value of the generalizability coefficient. Express your final result as a decimal and round your answer to four significant figures. No units are required.", "solution": "The goal is to find the optimal number of raters ($n_r$) and occasions ($n_o$) that maximizes the generalizability coefficient for relative decisions, subject to a budget constraint.\n\n**Step 1: Define the Generalizability Coefficient**\nIn Generalizability Theory, the generalizability coefficient for relative decisions, $G$, is the ratio of the universe score variance ($\\sigma_p^2$) to the expected observed score variance for relative decisions. The expected observed score variance is the sum of the universe score variance and the relative error variance ($\\sigma_\\delta^2$).\n$$G(n_r, n_o) = \\frac{\\sigma_p^2}{\\sigma_p^2 + \\sigma_\\delta^2}$$\nFor a fully crossed $p \\times r \\times o$ design, the relative error variance is composed of all variance components that involve an interaction with the person facet ($p$), scaled by the number of conditions in the decision (D) study:\n$$\\sigma_\\delta^2 = \\frac{\\sigma_{pr}^2}{n_r} + \\frac{\\sigma_{po}^2}{n_o} + \\frac{\\sigma_{pro,e}^2}{n_r n_o}$$\nPlugging in the given variance components:\n$$\\sigma_\\delta^2 = \\frac{6}{n_r} + \\frac{9}{n_o} + \\frac{12}{n_r n_o}$$\nAnd the generalizability coefficient is:\n$$G(n_r, n_o) = \\frac{18}{18 + \\frac{6}{n_r} + \\frac{9}{n_o} + \\frac{12}{n_r n_o}}$$\n\n**Step 2: Define the Budget Constraint and Find Feasible Designs**\nThe total cost per person must not exceed the budget $B=640$. The cost is given by $c_r n_r + c_o n_o$.\n$$100 n_r + 80 n_o \\le 640$$\nDividing by 20 simplifies the constraint:\n$$5 n_r + 4 n_o \\le 32$$\nSince $n_r$ and $n_o$ must be positive integers, we can list all feasible pairs $(n_r, n_o)$:\n*   If $n_r = 1: 4 n_o \\le 27 \\implies n_o \\in \\{1, 2, 3, 4, 5, 6\\}$\n*   If $n_r = 2: 4 n_o \\le 22 \\implies n_o \\in \\{1, 2, 3, 4, 5\\}$\n*   If $n_r = 3: 4 n_o \\le 17 \\implies n_o \\in \\{1, 2, 3, 4\\}$\n*   If $n_r = 4: 4 n_o \\le 12 \\implies n_o \\in \\{1, 2, 3\\}$\n*   If $n_r = 5: 4 n_o \\le 7 \\implies n_o \\in \\{1\\}$\n*   If $n_r = 6: 4 n_o \\le 2 \\implies$ No integer $n_o \\ge 1$ is possible.\n\n**Step 3: Evaluate and Optimize**\nTo maximize $G(n_r, n_o)$, we must minimize the denominator, which means minimizing the relative error term $\\sigma_\\delta^2$. We will evaluate $\\sigma_\\delta^2$ for the most promising feasible pairs (those with the largest values of $n_r$ and $n_o$, which will be on the boundary of the feasible region).\n*   $(n_r, n_o) = (1, 6): \\sigma_\\delta^2 = \\frac{6}{1} + \\frac{9}{6} + \\frac{12}{6} = 6 + 1.5 + 2 = 9.5$\n*   $(n_r, n_o) = (2, 5): \\sigma_\\delta^2 = \\frac{6}{2} + \\frac{9}{5} + \\frac{12}{10} = 3 + 1.8 + 1.2 = 6.0$\n*   $(n_r, n_o) = (3, 4): \\sigma_\\delta^2 = \\frac{6}{3} + \\frac{9}{4} + \\frac{12}{12} = 2 + 2.25 + 1 = 5.25$\n*   $(n_r, n_o) = (4, 3): \\sigma_\\delta^2 = \\frac{6}{4} + \\frac{9}{3} + \\frac{12}{12} = 1.5 + 3 + 1 = 5.5$\n*   $(n_r, n_o) = (5, 1): \\sigma_\\delta^2 = \\frac{6}{5} + \\frac{9}{1} + \\frac{12}{5} = 1.2 + 9 + 2.4 = 12.6$\n\nThe minimum relative error variance is $\\sigma_\\delta^2 = 5.25$, achieved with the design $(n_r, n_o) = (3, 4)$.\n\n**Step 4: Calculate the Maximal Generalizability Coefficient**\nUsing the optimal design $(3, 4)$, the maximal predicted reliability is:\n$$G_{\\text{max}} = G(3, 4) = \\frac{18}{18 + 5.25} = \\frac{18}{23.25}$$\n$$G_{\\text{max}} \\approx 0.7741935...$$\nRounding to four significant figures gives $0.7742$.", "answer": "$$\\boxed{0.7742}$$", "id": "4917664"}]}