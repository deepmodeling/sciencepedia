{"hands_on_practices": [{"introduction": "The cornerstone of measurement reliability is understanding how much of the variation in our scores reflects true differences between individuals versus random measurement noise. Classical Test Theory provides an elegant framework for this by modeling an observed score ($X$) as the sum of a true score ($T$) and an error component ($E$). This first exercise [@problem_id:4926563] will guide you through the fundamental definition of the reliability coefficient as the proportion of total score variance that is attributable to true score variance, a crucial first step in evaluating any measurement instrument.", "problem": "A biostatistics team is evaluating a continuous composite score used to index a physiological risk construct. Under Classical Test Theory (CTT), each observed score $X$ decomposes as $X = T + E$, where $T$ is the true score and $E$ is random measurement error. Assume that $E$ has mean $0$, $T$ and $E$ are uncorrelated, and population variances exist. From a reliability study using replicate measurements, the team estimates the observed-score variance as $10$ and the error variance as $4$. Using only the CTT assumptions and fundamental definitions, derive an expression for the reliability coefficient in terms of the relevant variances, and compute its value for these estimates. Then, briefly interpret what this reliability means for using the score to make individual-level clinical decisions. Provide the reliability as a unitless decimal. No rounding is required.", "solution": "### Solution\nThe problem requires the derivation of the reliability coefficient, its computation, and its interpretation.\n\nThe foundation of Classical Test Theory (CTT) is the linear model for an observed score $X$:\n$$X = T + E$$\nwhere $T$ is the true score and $E$ is the random measurement error. A key assumption in CTT is that the true score and the error component are uncorrelated. This is stated in the problem as $\\text{Cov}(T, E) = 0$.\n\nThe variance of the observed scores, $\\sigma_X^2$, can be expressed in terms of the variances of the true scores, $\\sigma_T^2$, and error, $\\sigma_E^2$. Using the properties of variance, we have:\n$$\\sigma_X^2 = \\text{Var}(X) = \\text{Var}(T + E)$$\nFor the sum of two random variables, the variance is given by $\\text{Var}(A+B) = \\text{Var}(A) + \\text{Var}(B) + 2\\text{Cov}(A,B)$. Applying this to our model:\n$$\\sigma_X^2 = \\text{Var}(T) + \\text{Var}(E) + 2\\text{Cov}(T, E)$$\nBy definition, $\\text{Var}(T) = \\sigma_T^2$ and $\\text{Var}(E) = \\sigma_E^2$. Given the assumption that $\\text{Cov}(T, E) = 0$, the equation simplifies to the fundamental decomposition of variance in CTT:\n$$\\sigma_X^2 = \\sigma_T^2 + \\sigma_E^2$$\nThis equation states that the total variance in the observed scores is the sum of the variance of the true scores (the systematic variance between individuals) and the variance of the measurement error (the random noise).\n\nThe reliability coefficient, denoted here by $\\rho$, is conceptually defined as the proportion of the observed-score variance that is attributable to true-score variance. Mathematically, this is expressed as:\n$$\\rho = \\frac{\\sigma_T^2}{\\sigma_X^2}$$\nTo express this in terms of the given quantities, $\\sigma_X^2$ and $\\sigma_E^2$, we first rearrange the variance decomposition equation to solve for the true-score variance, $\\sigma_T^2$:\n$$\\sigma_T^2 = \\sigma_X^2 - \\sigma_E^2$$\nSubstituting this expression for $\\sigma_T^2$ into the definition of the reliability coefficient $\\rho$:\n$$\\rho = \\frac{\\sigma_X^2 - \\sigma_E^2}{\\sigma_X^2}$$\nThis can also be written as:\n$$\\rho = 1 - \\frac{\\sigma_E^2}{\\sigma_X^2}$$\nThis is the required expression for the reliability coefficient in terms of the relevant (and provided) variances.\n\nNow, we compute its value using the estimates from the reliability study:\n-   Observed-score variance: $\\sigma_X^2 = 10$\n-   Error variance: $\\sigma_E^2 = 4$\n\nSubstituting these values into the derived formula:\n$$\\rho = 1 - \\frac{4}{10} = 1 - 0.4 = 0.6$$\nThe reliability coefficient for this composite score is $0.6$.\n\n### Interpretation\nA reliability coefficient of $\\rho = 0.6$ signifies that $60\\%$ of the variance observed in the composite scores from the sample is due to actual, true differences in the underlying physiological risk construct among individuals. The remaining $40\\%$ of the variance is attributable to random measurement error.\n\nFor making individual-level clinical decisions, reliability is paramount. A score with a reliability of $0.6$ is generally considered poor to moderate. Conventional guidelines (while context-dependent) often suggest a minimum reliability of $0.80$ for low-stakes decisions and $0.90$ or higher for high-stakes clinical or diagnostic purposes. A value of $0.6$ indicates that a substantial portion of any given score is noise, which limits its utility for precisely placing an individual on the continuum of risk. While the score may have value for group-level research, where random errors can average out across a large sample, its high degree of measurement error makes it insufficiently precise for dependable use in individual patient assessment or management.", "answer": "$$\\boxed{0.6}$$", "id": "4926563"}, {"introduction": "Once we can quantify a test's reliability, a natural question arises: how can we improve it? One of the most powerful and practical insights from Classical Test Theory is the relationship between test length and reliability. This practice [@problem_id:4926538] delves into this relationship by asking you to derive and apply the Spearman-Brown prophecy formula, which predicts how reliability increases when a test is lengthened. This principle is fundamental to instrument design, helping researchers decide how many items are needed to achieve a desired level of precision.", "problem": "A research team evaluates the measurement reliability of a new patient-reported outcomes instrument for symptom severity in oncology. The instrument consists of $20$ items. For reliability assessment under Classical Test Theory (CTT), they randomly split the items into two equal halves of $10$ items that can be assumed to be parallel forms: observed scores on each half, denoted $X_{1}$ and $X_{2}$, satisfy $X_{i} = T_{i} + E_{i}$ with true score $T_{i}$ and error $E_{i}$, where $T_{1}$ and $T_{2}$ have equal variance across persons, $E_{1}$ and $E_{2}$ have equal variance across persons, $T_{i}$ is uncorrelated with $E_{j}$ for all $i,j$, and errors across forms are uncorrelated. The split-half correlation is estimated to be $r_{hh} = 0.6$.\n\nUsing only the CTT definition of reliability as the ratio of true-score variance to observed-score variance and the stated assumptions for parallel forms, derive the predicted reliability for the test formed by summing the two halves (which doubles the test length relative to each half). Then evaluate this expression at $r_{hh} = 0.6$ to obtain a single numerical value for the predicted reliability of the $20$-item full test. Express the final answer as a decimal. No rounding instruction is necessary because the exact value can be obtained.", "solution": "The objective is to derive the reliability of the full test, which is composed of the two halves. Let the score on the full test be $X_{full}$.\nThe full test score is the sum of the scores of the two halves:\n$$X_{full} = X_1 + X_2$$\nThe reliability of the full test, which we will denote as $\\rho_{full}$, is defined as the ratio of the true-score variance of the full test to the observed-score variance of the full test:\n$$\\rho_{full} = \\frac{\\text{Var}(T_{full})}{\\text{Var}(X_{full})}$$\nFirst, we express the full test score in terms of its true and error components using the CTT model:\n$$X_{full} = (T_1 + E_1) + (T_2 + E_2) = (T_1 + T_2) + (E_1 + E_2)$$\nFrom this, we can identify the true score and error score for the full test:\n$$T_{full} = T_1 + T_2$$\n$$E_{full} = E_1 + E_2$$\nNow, we derive expressions for the numerator and denominator of the reliability formula.\n\n**Numerator: Variance of the True Score of the Full Test, $\\text{Var}(T_{full})$**\nThe variance of the sum of two random variables is given by $\\text{Var}(A+B) = \\text{Var}(A) + \\text{Var}(B) + 2\\text{Cov}(A,B)$.\n$$\\text{Var}(T_{full}) = \\text{Var}(T_1 + T_2) = \\text{Var}(T_1) + \\text{Var}(T_2) + 2\\text{Cov}(T_1, T_2)$$\nThe assumption of parallel forms posits that the two halves of the test are measuring the exact same underlying construct. This implies that their true scores are identical, i.e., $T_1 = T_2$. Let's denote this common true score as $T_{half}$.\nTherefore, $T_{full} = T_{half} + T_{half} = 2T_{half}$.\nThe variance of the full test's true score is:\n$$\\text{Var}(T_{full}) = \\text{Var}(2T_{half}) = 2^2 \\text{Var}(T_{half}) = 4\\text{Var}(T_{half})$$\nFor notation, let $\\sigma_{T_{half}}^2 = \\text{Var}(T_{half})$. So, $\\text{Var}(T_{full}) = 4\\sigma_{T_{half}}^2$.\n\n**Denominator: Variance of the Observed Score of the Full Test, $\\text{Var}(X_{full})$**\nUsing the same variance of a sum rule:\n$$\\text{Var}(X_{full}) = \\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\text{Cov}(X_1, X_2)$$\nThe parallel forms assumption implies that the two half-tests have equal observed score variances because they have equal true score variances and equal error variances ($\\text{Var}(X_i) = \\text{Var}(T_i) + \\text{Var}(E_i)$ since $\\text{Cov}(T_i, E_i)=0$). Let's denote this common observed variance as $\\sigma_{X_{half}}^2$, so $\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma_{X_{half}}^2$.\nThe covariance term $\\text{Cov}(X_1, X_2)$ can be expressed using the given split-half correlation, $r_{hh}$:\n$$r_{hh} = \\text{Corr}(X_1, X_2) = \\frac{\\text{Cov}(X_1, X_2)}{\\sqrt{\\text{Var}(X_1)\\text{Var}(X_2)}} = \\frac{\\text{Cov}(X_1, X_2)}{\\sqrt{\\sigma_{X_{half}}^2 \\cdot \\sigma_{X_{half}}^2}} = \\frac{\\text{Cov}(X_1, X_2)}{\\sigma_{X_{half}}^2}$$\nFrom this, we have $\\text{Cov}(X_1, X_2) = r_{hh}\\sigma_{X_{half}}^2$.\nSubstituting these into the expression for $\\text{Var}(X_{full})$:\n$$\\text{Var}(X_{full}) = \\sigma_{X_{half}}^2 + \\sigma_{X_{half}}^2 + 2(r_{hh}\\sigma_{X_{half}}^2) = 2\\sigma_{X_{half}}^2 + 2r_{hh}\\sigma_{X_{half}}^2 = 2\\sigma_{X_{half}}^2(1 + r_{hh})$$\n\n**Assembling the Reliability Formula**\nWe can now write the reliability of the full test by combining the numerator and denominator:\n$$\\rho_{full} = \\frac{\\text{Var}(T_{full})}{\\text{Var}(X_{full})} = \\frac{4\\sigma_{T_{half}}^2}{2\\sigma_{X_{half}}^2(1 + r_{hh})} = \\frac{2\\sigma_{T_{half}}^2}{\\sigma_{X_{half}}^2(1 + r_{hh})}$$\nTo simplify further, we must establish a relationship between $r_{hh}$ and the component variances. Let's expand the covariance $\\text{Cov}(X_1, X_2)$:\n$$\\text{Cov}(X_1, X_2) = \\text{Cov}(T_1 + E_1, T_2 + E_2)$$\nUsing the bilinearity property of covariance:\n$$\\text{Cov}(X_1, X_2) = \\text{Cov}(T_1, T_2) + \\text{Cov}(T_1, E_2) + \\text{Cov}(E_1, T_2) + \\text{Cov}(E_1, E_2)$$\nFrom the given CTT assumptions for parallel forms, true scores and errors are uncorrelated ($\\text{Cov}(T_i, E_j)=0$) and errors between forms are uncorrelated ($\\text{Cov}(E_1, E_2)=0$). This simplifies the expression to:\n$$\\text{Cov}(X_1, X_2) = \\text{Cov}(T_1, T_2)$$\nAgain, using the assumption that $T_1 = T_2 = T_{half}$, we get:\n$$\\text{Cov}(T_1, T_2) = \\text{Cov}(T_{half}, T_{half}) = \\text{Var}(T_{half}) = \\sigma_{T_{half}}^2$$\nSo, we have $\\text{Cov}(X_1, X_2) = \\sigma_{T_{half}}^2$.\nNow we substitute this back into our definition of $r_{hh}$:\n$$r_{hh} = \\frac{\\text{Cov}(X_1, X_2)}{\\sigma_{X_{half}}^2} = \\frac{\\sigma_{T_{half}}^2}{\\sigma_{X_{half}}^2}$$\nThis confirms that the correlation between two parallel test halves is equal to the reliability of a single half-test.\nWe can now substitute this result, which represents the ratio of true-score variance to observed-score variance for a half-test, into our expression for $\\rho_{full}$:\n$$\\rho_{full} = \\frac{2\\left(\\frac{\\sigma_{T_{half}}^2}{\\sigma_{X_{half}}^2}\\right)}{1 + r_{hh}} = \\frac{2r_{hh}}{1 + r_{hh}}$$\nThis is the Spearman-Brown prediction formula, which we have derived as requested.\n\n**Final Calculation**\nThe problem provides the value of the split-half correlation as $r_{hh} = 0.6$. We substitute this value into the derived formula:\n$$\\rho_{full} = \\frac{2(0.6)}{1 + 0.6} = \\frac{1.2}{1.6}$$\nSimplifying the fraction:\n$$\\rho_{full} = \\frac{12}{16} = \\frac{3}{4} = 0.75$$\nThe predicted reliability for the $20$-item full test is $0.75$.", "answer": "$$\\boxed{0.75}$$", "id": "4926538"}, {"introduction": "Shifting from continuous scores to categorical ratings—common in medical diagnoses or behavioral coding—requires a different approach to assessing reliability. Simply calculating the percentage of times raters agree can be highly misleading, as a high level of agreement can occur by chance alone, especially when one category is very common. This exercise [@problem_id:4926567] introduces Cohen’s kappa, a robust statistic that corrects for chance agreement, and uses a carefully constructed scenario to demonstrate the \"prevalence and bias paradox,\" highlighting why a principled approach to measuring inter-rater reliability is essential.", "problem": "A diagnostic reproducibility study evaluates inter-rater agreement between two independent raters who classify each of $N$ items into two mutually exclusive categories: “positive” and “negative.” In this setting, define a principled index of agreement that corrects the observed agreement for the agreement expected by chance if the two raters were statistically independent. Begin from core probability definitions: the observed proportion of agreement, marginal category proportions for each rater, and the expected chance agreement computed under the independence assumption. Use these building blocks to derive a normalized index of agreement for two raters known as Cohen’s kappa.\n\nTo concretely demonstrate the prevalence and bias paradox, consider $N=200$ histopathology slides screened by two pathologists. Let $n_{11}$ denote the count classified positive by both raters, $n_{10}$ the count positive by Rater A and negative by Rater B, $n_{01}$ the count negative by Rater A and positive by Rater B, and $n_{00}$ the count negative by both. The compiled counts are:\n- $n_{11} = 0$,\n- $n_{10} = 2$,\n- $n_{01} = 2$,\n- $n_{00} = 196$.\n\nStarting from the independence-based chance model and the definition of observed agreement, derive the expression for Cohen’s kappa in terms of the observed and expected agreements, and then compute its value for the counts above. Round your final numerical answer to four significant figures. Do not include units.", "solution": "**Derivation and Calculation**\n\nLet the two categories be denoted by $1$ (positive) and $0$ (negative). For a study with two raters, say Rater A and Rater B, the results for $N$ subjects can be summarized in a $2 \\times 2$ contingency table with cell counts $n_{ij}$, where $i$ is the rating by Rater A and $j$ is the rating by Rater B.\n\n|            | Rater B: Positive ($1$) | Rater B: Negative ($0$) | Row Total |\n|------------|-------------------------|-------------------------|-----------|\n| Rater A: Positive ($1$) | $n_{11}$                | $n_{10}$                | $n_{1+}$  |\n| Rater A: Negative ($0$) | $n_{01}$                | $n_{00}$                | $n_{0+}$  |\n| Column Total | $n_{+1}$                | $n_{+0}$                | $N$       |\n\nThe total number of items is $N = n_{11} + n_{10} + n_{01} + n_{00}$.\n\n**1. Observed Proportion of Agreement ($P_o$)**\nThe observed agreement is the proportion of items on which the two raters agree. Agreement occurs in the cells where both raters give the same classification, i.e., $(1,1)$ and $(0,0)$.\n$$P_o = \\frac{\\text{Number of agreements}}{\\text{Total number of items}} = \\frac{n_{11} + n_{00}}{N}$$\n\n**2. Expected Proportion of Agreement by Chance ($P_e$)**\nTo calculate the agreement expected purely by chance, we assume the raters' judgments are statistically independent. We first compute the marginal proportions for each rater's classifications.\n\nThe proportion of 'positive' ratings by Rater A is $P_{A,pos} = \\frac{n_{11} + n_{10}}{N}$.\nThe proportion of 'negative' ratings by Rater A is $P_{A,neg} = \\frac{n_{01} + n_{00}}{N}$.\n\nThe proportion of 'positive' ratings by Rater B is $P_{B,pos} = \\frac{n_{11} + n_{01}}{N}$.\nThe proportion of 'negative' ratings by Rater B is $P_{B,neg} = \\frac{n_{10} + n_{00}}{N}$.\n\nUnder the independence assumption, the joint probability of an outcome is the product of the marginal probabilities.\nThe probability of both raters agreeing on 'positive' by chance is $P_{A,pos} \\times P_{B,pos}$.\nThe probability of both raters agreeing on 'negative' by chance is $P_{A,neg} \\times P_{B,neg}$.\n\nThe total expected proportion of agreement by chance, $P_e$, is the sum of these probabilities:\n$$P_e = (P_{A,pos} \\times P_{B,pos}) + (P_{A,neg} \\times P_{B,neg})$$\n\n**3. Cohen's Kappa ($\\kappa$)**\nCohen's kappa ($\\kappa$) is defined as the level of agreement achieved beyond chance, normalized by the maximum possible agreement beyond chance. The total agreement is $P_o$, and the portion attributable to chance is $P_e$. Thus, the agreement achieved beyond chance is $P_o - P_e$. The maximum possible agreement is $1$, so the maximum possible agreement beyond chance is $1 - P_e$.\n\nThe formula for Cohen's kappa is therefore:\n$$\\kappa = \\frac{P_o - P_e}{1 - P_e}$$\nThis completes the derivation.\n\n**Application to the Given Data**\nThe problem provides the following counts:\n- $n_{11} = 0$\n- $n_{10} = 2$\n- $n_{01} = 2$\n- $n_{00} = 196$\n- $N = 200$\n\nFirst, we calculate the observed proportion of agreement, $P_o$:\n$$P_o = \\frac{n_{11} + n_{00}}{N} = \\frac{0 + 196}{200} = \\frac{196}{200} = 0.98$$\nThe observed agreement is $98\\%$, which appears very high.\n\nNext, we calculate the expected proportion of agreement by chance, $P_e$. We first find the marginal totals:\n- Rater A positive total: $n_{1+} = n_{11} + n_{10} = 0 + 2 = 2$\n- Rater A negative total: $n_{0+} = n_{01} + n_{00} = 2 + 196 = 198$\n- Rater B positive total: $n_{+1} = n_{11} + n_{01} = 0 + 2 = 2$\n- Rater B negative total: $n_{+0} = n_{10} + n_{00} = 2 + 196 = 198$\n\nNow, we compute the marginal proportions:\n- $P_{A,pos} = \\frac{2}{200} = 0.01$\n- $P_{A,neg} = \\frac{198}{200} = 0.99$\n- $P_{B,pos} = \\frac{2}{200} = 0.01$\n- $P_{B,neg} = \\frac{198}{200} = 0.99$\n\nUsing these, we calculate $P_e$:\n$$P_e = (P_{A,pos} \\times P_{B,pos}) + (P_{A,neg} \\times P_{B,neg})$$\n$$P_e = (0.01 \\times 0.01) + (0.99 \\times 0.99) = 0.0001 + 0.9801 = 0.9802$$\n\nFinally, we compute Cohen's kappa, $\\kappa$:\n$$\\kappa = \\frac{P_o - P_e}{1 - P_e} = \\frac{0.98 - 0.9802}{1 - 0.9802} = \\frac{-0.0002}{0.0198}$$\n$$\\kappa = -\\frac{2}{198} = -\\frac{1}{99} \\approx -0.01010101...$$\n\nThe problem requires rounding the result to four significant figures. The first significant figure is the first non-zero digit, which is $1$. The first four significant figures are $1$, $0$, $1$, $0$. The fifth digit is $1$, so we do not round up.\n$$\\kappa \\approx -0.01010$$\n\nThis result exemplifies the \"prevalence and bias paradox.\" The observed agreement ($P_o = 98\\%$) is very high, but this is misleading. The high prevalence of the \"negative\" category means that both raters rating a slide as negative is a very common event. This inflates the chance agreement level ($P_e = 98.02\\%$) to be almost identical to the observed agreement. Because kappa corrects for this chance agreement, its value is close to zero, correctly indicating that the raters' agreement is no better than what would be expected if they were rating independently based on the underlying prevalence. The slightly negative value indicates the observed agreement is marginally worse than chance.", "answer": "$$\\boxed{-0.01010}$$", "id": "4926567"}]}