{"hands_on_practices": [{"introduction": "A common pitfall in agreement analysis is to equate high correlation with high agreement. This exercise is designed to build your intuition by demonstrating this crucial difference. By analyzing a dataset with a perfect linear relationship but a large systematic bias, you will see firsthand why the Pearson correlation coefficient can be misleading and how the Intraclass Correlation Coefficient (ICC) for absolute agreement provides a more accurate measure of interchangeability between raters [@problem_id:4893311].", "problem": "A biostatistician is assessing agreement between two continuous raters on $n$ subjects, where each subject is measured once by each rater. Consider $n=8$ subjects and two raters, labeled rater A and rater B. The observed values are:\n\n- Subject $1$: rater A $10$, rater B $30$.\n- Subject $2$: rater A $12$, rater B $32$.\n- Subject $3$: rater A $14$, rater B $34$.\n- Subject $4$: rater A $16$, rater B $36$.\n- Subject $5$: rater A $18$, rater B $38$.\n- Subject $6$: rater A $20$, rater B $40$.\n- Subject $7$: rater A $22$, rater B $42$.\n- Subject $8$: rater A $24$, rater B $44$.\n\nStarting from the two-way random effects framework for agreement with continuous outcomes and its associated analysis of variance partition, compute the Intraclass Correlation Coefficient (ICC) for absolute agreement for a single measurement, denoted Intraclass Correlation Coefficient $(\\text{absolute agreement}, 1)$, and briefly explain why it can differ from the Pearson product-moment correlation between the two raters for this dataset.\n\nReport only the value of the Intraclass Correlation Coefficient $(\\text{absolute agreement}, 1)$ for this dataset, rounded to $4$ significant figures. No units are required.", "solution": "The problem asks for the computation of the Intraclass Correlation Coefficient for absolute agreement for a single measurement, denoted as ICC$(\\text{absolute agreement}, 1)$. This analysis is performed within a two-way random effects framework. The data consists of measurements from $k=2$ raters on $n=8$ subjects.\n\nThe two-way random effects model for an observation $y_{ij}$ from subject $i$ by rater $j$ is given by:\n$$y_{ij} = \\mu + s_i + r_j + e_{ij}$$\nwhere $\\mu$ is the grand mean, $s_i$ is the random effect for subject $i$ ($s_i \\sim \\mathcal{N}(0, \\sigma_s^2)$), $r_j$ is the random effect for rater $j$ ($r_j \\sim \\mathcal{N}(0, \\sigma_r^2)$), and $e_{ij}$ is the random error term ($e_{ij} \\sim \\mathcal{N}(0, \\sigma_e^2)$). The terms $\\sigma_s^2$, $\\sigma_r^2$, and $\\sigma_e^2$ represent the between-subject variance, between-rater variance, and residual variance, respectively. All effects are assumed to be independent.\n\nThe ICC for absolute agreement for a single measurement is defined as the proportion of total variance that is attributable to the subjects. The formula is:\n$$ \\text{ICC}(\\text{absolute agreement}, 1) = \\frac{\\sigma_s^2}{\\sigma_s^2 + \\sigma_r^2 + \\sigma_e^2} $$\nTo estimate the variance components ($\\sigma_s^2, \\sigma_r^2, \\sigma_e^2$), we perform a two-way analysis of variance (ANOVA) on the provided data.\n\nThe data for $n=8$ subjects and $k=2$ raters are:\n- Rater A ($j=1$): $10, 12, 14, 16, 18, 20, 22, 24$\n- Rater B ($j=2$): $30, 32, 34, 36, 38, 40, 42, 44$\n\nFirst, we calculate the necessary means:\n- Rater A mean: $\\bar{y}_{.1} = \\frac{1}{8}\\sum_{i=1}^8 y_{i1} = \\frac{136}{8} = 17$\n- Rater B mean: $\\bar{y}_{.2} = \\frac{1}{8}\\sum_{i=1}^8 y_{i2} = \\frac{296}{8} = 37$\n- Subject means ($\\bar{y}_{i.}$):\n  $\\bar{y}_{1.} = \\frac{10+30}{2}=20, \\bar{y}_{2.} = \\frac{12+32}{2}=22, \\dots, \\bar{y}_{8.} = \\frac{24+44}{2}=34$\n- Grand mean: $\\bar{y}_{..} = \\frac{1}{nk}\\sum_{i=1}^n \\sum_{j=1}^k y_{ij} = \\frac{17+37}{2} = 27$\n\nNext, we compute the sums of squares (SS) for the ANOVA:\n- Between-Subjects Sum of Squares (SSB):\n$$ SSB = k \\sum_{i=1}^n (\\bar{y}_{i.} - \\bar{y}_{..})^2 = 2 \\sum_{i=1}^8 (\\bar{y}_{i.} - 27)^2 $$\n$$ SSB = 2 \\left[ (20-27)^2 + (22-27)^2 + (24-27)^2 + (26-27)^2 + (28-27)^2 + (30-27)^2 + (32-27)^2 + (34-27)^2 \\right] $$\n$$ SSB = 2 \\left[ (-7)^2 + (-5)^2 + (-3)^2 + (-1)^2 + 1^2 + 3^2 + 5^2 + 7^2 \\right] $$\n$$ SSB = 2 [49+25+9+1+1+9+25+49] = 2(168) = 336 $$\n\n- Between-Raters Sum of Squares (SSJ):\n$$ SSJ = n \\sum_{j=1}^k (\\bar{y}_{.j} - \\bar{y}_{..})^2 = 8 \\left[ (17-27)^2 + (37-27)^2 \\right] $$\n$$ SSJ = 8 [(-10)^2 + 10^2] = 8(100+100) = 8(200) = 1600 $$\n\n- Total Sum of Squares (SST):\n$$ SST = \\sum_{i=1}^n \\sum_{j=1}^k (y_{ij} - \\bar{y}_{..})^2 $$\n$$ SST = \\sum_{i=1}^8 (y_{i1}-27)^2 + \\sum_{i=1}^8 (y_{i2}-27)^2 $$\n$$ SST = \\left[ (-17)^2 + (-15)^2 + \\dots + (-3)^2 \\right] + \\left[ 3^2 + 5^2 + \\dots + 17^2 \\right] $$\n$$ SST = 2(3^2+5^2+7^2+9^2+11^2+13^2+15^2+17^2) = 2(9+25+49+81+121+169+225+289) = 2(968) = 1936 $$\n\n- Error Sum of Squares (SSE):\n$$ SSE = SST - SSB - SSJ = 1936 - 336 - 1600 = 0 $$\nThe zero error sum of squares indicates that there is no subject-rater interaction; the difference between raters is constant for all subjects.\n\nNow, we construct the ANOVA table to find the Mean Squares (MS):\n- Mean Square for Subjects: $MSB = \\frac{SSB}{n-1} = \\frac{336}{8-1} = \\frac{336}{7} = 48$\n- Mean Square for Raters: $MSJ = \\frac{SSJ}{k-1} = \\frac{1600}{2-1} = 1600$\n- Mean Square for Error: $MSE = \\frac{SSE}{(n-1)(k-1)} = \\frac{0}{(7)(1)} = 0$\n\nThe expected mean squares (EMS) for the two-way random effects model are:\n- $E(MSB) = k\\sigma_s^2 + \\sigma_e^2$\n- $E(MSJ) = n\\sigma_r^2 + \\sigma_e^2$\n- $E(MSE) = \\sigma_e^2$\n\nWe can now estimate the variance components by equating the observed MS to their expectations:\n- $\\hat{\\sigma}_e^2 = MSE = 0$\n- $MSB = k\\hat{\\sigma}_s^2 + \\hat{\\sigma}_e^2 \\implies 48 = 2\\hat{\\sigma}_s^2 + 0 \\implies \\hat{\\sigma}_s^2 = 24$\n- $MSJ = n\\hat{\\sigma}_r^2 + \\hat{\\sigma}_e^2 \\implies 1600 = 8\\hat{\\sigma}_r^2 + 0 \\implies \\hat{\\sigma}_r^2 = 200$\n\nWith the estimated variance components, we calculate the ICC:\n$$ \\text{ICC}(\\text{absolute agreement}, 1) = \\frac{\\hat{\\sigma}_s^2}{\\hat{\\sigma}_s^2 + \\hat{\\sigma}_r^2 + \\hat{\\sigma}_e^2} = \\frac{24}{24 + 200 + 0} = \\frac{24}{224} = \\frac{3}{28} $$\n$$ \\frac{3}{28} \\approx 0.107142857... $$\nRounding to $4$ significant figures, the value is $0.1071$.\n\nThe problem also requires a brief explanation of why this ICC value differs from the Pearson product-moment correlation coefficient, $r$. For this dataset, the scores from Rater B are exactly $20$ points higher than the scores from Rater A for every subject ($y_{i,B} = y_{i,A} + 20$). This is a perfect positive linear relationship. The Pearson correlation coefficient measures only the strength of linear association and is insensitive to such additive systematic biases. Consequently, the Pearson correlation for this data is $r=1$.\n\nIn contrast, the ICC for absolute agreement is designed to be sensitive to systematic biases. Its formula incorporates the between-rater variance, $\\sigma_r^2$. The large and consistent difference between the two raters results in a very large estimated between-rater variance component ($\\hat{\\sigma}_r^2 = 200$), which is a major part of the total variance in the denominator of the ICC formula. This large $\\sigma_r^2$ term heavily penalizes the ICC value, reflecting the poor absolute agreement between the raters' scores, despite their perfect consistency. Thus, while Pearson correlation indicates perfect consistency ($r=1$), the ICC for absolute agreement correctly indicates poor absolute agreement (ICC $\\approx 0.1071$).", "answer": "$$\n\\boxed{0.1071}\n$$", "id": "4893311"}, {"introduction": "Beyond assessing existing data, the Intraclass Correlation Coefficient ($ICC$) is a powerful tool for planning future studies and ensuring their quality. This practice puts you in the role of a clinical laboratory scientist tasked with improving an assay's reliability. You will use the principles of variance decomposition to determine the minimum number of replicate measurements needed to achieve a target level of reliability, a core task in assay validation and clinical research [@problem_id:4893345].", "problem": "A clinical laboratory is validating a quantitative assay for a continuous biomarker. For subject $i$, the $j$-th replicate measurement is modeled by the classical random-effects decomposition $Y_{ij} = \\mu + S_{i} + E_{ij}$, where $S_{i}$ represents the subject-specific random effect with variance $\\sigma_{b}^{2}$, $E_{ij}$ represents independent measurement error with variance $\\sigma_{w}^{2}$, and all components are mutually independent with zero mean. The reliability for a single measurement is quantified by the Intraclass Correlation Coefficient (ICC), defined for single measures as $ICC = \\sigma_{b}^{2} / (\\sigma_{b}^{2} + \\sigma_{w}^{2})$. A pilot study estimates the single-measure Intraclass Correlation Coefficient (ICC) as $0.70$. The laboratory plans to average $k$ independent replicates per subject to report the subject’s assay value.\n\nUsing only the variance decomposition above and the definition of the Intraclass Correlation Coefficient (ICC), derive the reliability of the $k$-replicate average as a function of $k$ and the single-measure $ICC$, then determine the smallest integer $k$ such that the average-measure ICC is at least $0.85$. Report the final $k$ as an integer. No rounding to significant figures is required; $k$ must be an integer by design.", "solution": "The problem requires two parts: first, a derivation for the reliability of an average of $k$ measurements ($ICC_k$), and second, the calculation of the minimum integer $k$ to meet a target reliability.\n\n**Part 1: Derivation of the Average-Measure ICC ($ICC_k$)**\n\nLet $\\bar{Y}_i$ be the average of $k$ replicate measurements for subject $i$:\n$$ \\bar{Y}_i = \\frac{1}{k} \\sum_{j=1}^{k} Y_{ij} = \\mu + S_{i} + \\frac{1}{k} \\sum_{j=1}^{k} E_{ij} $$\nThe reliability of this average measure, $ICC_k$, is the ratio of true subject variance ($\\sigma_b^2$) to the total variance of the average measurement, $\\text{Var}(\\bar{Y}_i)$. The total variance of the average is:\n$$ \\text{Var}(\\bar{Y}_i) = \\text{Var}(S_i) + \\text{Var}\\left(\\frac{1}{k} \\sum_{j=1}^{k} E_{ij}\\right) = \\sigma_{b}^{2} + \\frac{k\\sigma_{w}^{2}}{k^2} = \\sigma_{b}^{2} + \\frac{\\sigma_{w}^{2}}{k} $$\nTherefore, the average-measure ICC is:\n$$ ICC_k = \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{w}^{2}/k} $$\nTo express this in terms of the single-measure $ICC = \\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{w}^{2}}$, we note that $\\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}} = \\frac{1 - ICC}{ICC}$. Substituting this into the $ICC_k$ formula (after dividing numerator and denominator by $\\sigma_b^2$) yields the Spearman-Brown prophecy formula:\n$$ ICC_k = \\frac{k \\cdot ICC}{1 + (k-1)ICC} $$\nThis completes the derivation.\n\n**Part 2: Calculation of minimum $k$**\n\nWe are given $ICC = 0.70$ and the target reliability is $ICC_k \\geq 0.85$. We must find the smallest integer $k$ that satisfies:\n$$ \\frac{k \\cdot (0.70)}{1 + (k-1)(0.70)} \\geq 0.85 $$\nSolving the inequality for $k$:\n$$ 0.70k \\geq 0.85 (1 + 0.70k - 0.70) $$\n$$ 0.70k \\geq 0.85 (0.30 + 0.70k) $$\n$$ 0.70k \\geq 0.255 + 0.595k $$\n$$ 0.105k \\geq 0.255 $$\n$$ k \\geq \\frac{0.255}{0.105} \\approx 2.42857... $$\nSince $k$ must be an integer representing the number of replicates, we must round up to the next whole number. The smallest integer $k$ is $3$.", "answer": "$$\n\\boxed{3}\n$$", "id": "4893345"}, {"introduction": "This final practice transitions from static calculation to dynamic exploration using simulation, a core skill in modern biostatistics. You will programmatically generate data to investigate how different types of rater errors—such as additive and multiplicative biases—uniquely affect the $ICC$ for absolute agreement versus consistency. This exercise deepens your understanding of which $ICC$ form to choose based on the research question and the expected sources of disagreement [@problem_id:4893327].", "problem": "You are to write a complete, runnable program that simulates rater measurements under a two-way design and computes two types of Intraclass Correlation Coefficient (ICC): absolute agreement and consistency, then demonstrates how they respond to additive and multiplicative rater biases. Work in purely mathematical and algorithmic terms.\n\nBase model and definitions:\n- The Intraclass Correlation Coefficient (ICC) quantifies agreement for continuous outcomes by comparing between-subject variation to total variation under a two-way random effects analysis of variance model.\n- Consider $n$ subjects each rated by $k$ raters. Let $Y_{ij}$ denote the rating for subject $i$ by rater $j$. Assume the data are generated by the following stochastic model:\n  - True subject scores: $T_i \\sim \\mathcal{N}(0, \\sigma_S^2)$ independently for $i \\in \\{1,\\dots,n\\}$.\n  - Rater-specific scale multipliers: $S_j$ (fixed constants per case).\n  - Rater-specific additive biases: $B_j$ (fixed constants per case).\n  - Measurement noise: $E_{ij} \\sim \\mathcal{N}(0, \\sigma_E^2)$ independently across all $(i,j)$.\n  - Observed score: $Y_{ij} = S_j \\, T_i + B_j + E_{ij}$.\n- Agreement is assessed with two ICCs computed from a two-way random effects analysis of variance decomposition:\n  - Absolute agreement ICC corresponds to treating systematic rater mean differences as disagreement.\n  - Consistency ICC corresponds to being invariant to systematic rater mean differences (additive shifts), focusing on consistency of subject rankings across raters.\n\nYour program must:\n1) For each test case below, simulate the data matrix $Y \\in \\mathbb{R}^{n \\times k}$ exactly as specified above, using a pseudorandom number generator seeded for reproducibility with seed $20231105$ offset by the zero-based index of the test case (i.e., seed $20231105 + \\text{case\\_index}$). Use the normal distribution with mean $0$ and specified standard deviations for $T_i$ and $E_{ij}$.\n2) For each simulated matrix $Y$, compute the following quantities from the two-way random effects model using analysis of variance mean squares:\n   - Between-subject mean square, between-rater mean square, and residual mean square (subject-by-rater interaction plus measurement noise).\n   - From these mean squares, derive and compute:\n     - The absolute agreement single-measure ICC (often referred to as two-way random effects, absolute agreement).\n     - The consistency single-measure ICC (often referred to as two-way mixed/random effects, consistency).\n   The derivation must be based on variance decomposition and expected mean squares; do not rely on external libraries' black-box ICC routines.\n3) Output, for each test case, two floating-point values: first the absolute agreement ICC, then the consistency ICC. Express each as a decimal number (not a percentage), rounded to $6$ decimal places.\n4) If any denominator encountered in the ICC formula equals $0$, output a Not-a-Number value for that ICC.\n\nTest suite:\nUse $k=2$ raters in all cases. For each case specify $(n, \\sigma_S, \\sigma_E, \\mathbf{B}, \\mathbf{S})$ where $\\mathbf{B} = [B_1, B_2]$ and $\\mathbf{S} = [S_1, S_2]$.\n- Case $1$ (baseline, no bias): $(n=40, \\sigma_S=10, \\sigma_E=3, \\mathbf{B}=[0, 0], \\mathbf{S}=[1, 1])$.\n- Case $2$ (additive bias): $(n=40, \\sigma_S=10, \\sigma_E=3, \\mathbf{B}=[0, 5], \\mathbf{S}=[1, 1])$.\n- Case $3$ (multiplicative scale bias): $(n=40, \\sigma_S=10, \\sigma_E=3, \\mathbf{B}=[0, 0], \\mathbf{S}=[1, 1.2])$.\n- Case $4$ (low between-subject variability with additive bias): $(n=40, \\sigma_S=0.1, \\sigma_E=3, \\mathbf{B}=[0, 5], \\mathbf{S}=[1, 1])$.\n\nNumerical and output requirements:\n- Angles or physical units are not involved.\n- All ICCs must be expressed as decimals, not percentages.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order and formatting:\n  $[\\text{ICC\\_A1\\_case1}, \\text{ICC\\_C1\\_case1}, \\text{ICC\\_A1\\_case2}, \\text{ICC\\_C1\\_case2}, \\text{ICC\\_A1\\_case3}, \\text{ICC\\_C1\\_case3}, \\text{ICC\\_A1\\_case4}, \\text{ICC\\_C1\\_case4}]$\n- Round each value to $6$ decimal places exactly before printing.", "solution": "### Theoretical Foundation: ANOVA and ICC\n\nThe Intraclass Correlation Coefficient (ICC) is a descriptive statistic that quantifies the homogeneity or agreement of measurements within groups. In the context of rater reliability, it measures how much of the total variability in ratings is due to genuine differences between the subjects being rated. We consider a study with $n$ subjects and $k$ raters. The data are organized in an $n \\times k$ matrix $Y$, where $Y_{ij}$ is the rating for subject $i$ by rater $j$.\n\nThe calculation of the ICCs requested is based on the components of variance estimated from a two-way Analysis of Variance (ANOVA). The total variability in the data is partitioned into sources attributable to subjects, raters, and residual error (including subject-rater interaction).\n\nLet $\\bar{Y}_{i.}$ be the mean score for subject $i$, $\\bar{Y}_{.j}$ be the mean score for rater $j$, and $\\bar{Y}_{..}$ be the grand mean of all scores. The sums of squares are calculated as:\n\n- **Between-Subject Sum of Squares ($SS_S$)**: $SS_S = k \\sum_{i=1}^n (\\bar{Y}_{i.} - \\bar{Y}_{..})^2$\n- **Between-Rater Sum of Squares ($SS_R$)**: $SS_R = n \\sum_{j=1}^k (\\bar{Y}_{.j} - \\bar{Y}_{..})^2$\n- **Total Sum of Squares ($SS_{Total}$)**: $SS_{Total} = \\sum_{i=1}^n \\sum_{j=1}^k (Y_{ij} - \\bar{Y}_{..})^2$\n- **Residual (Error) Sum of Squares ($SS_E$)**: $SS_E = SS_{Total} - SS_S - SS_R$\n\nThe Mean Squares ($MS$) are the sums of squares divided by their respective degrees of freedom ($df$):\n- **Between-Subject Mean Square ($MS_S$)**: $MS_S = \\frac{SS_S}{n-1}$, with $df_S = n-1$\n- **Between-Rater Mean Square ($MS_R$)**: $MS_R = \\frac{SS_R}{k-1}$, with $df_R = k-1$\n- **Residual (Error) Mean Square ($MS_E$)**: $MS_E = \\frac{SS_E}{(n-1)(k-1)}$, with $df_E = (n-1)(k-1)$\n\nThese mean squares serve as estimates for linear combinations of the underlying variance components of the population: subject variance ($\\sigma_S^2$), rater variance ($\\sigma_R^2$), and error variance ($\\sigma_E^2$). The expected mean squares for a two-way random effects model are:\n- $E[MS_S] = k \\sigma_S^2 + \\sigma_E^2$\n- $E[MS_R] = n \\sigma_R^2 + \\sigma_E^2$\n- $E[MS_E] = \\sigma_E^2$\n\nFrom these, we can derive method-of-moments estimators for the variance components:\n- $\\hat{\\sigma}_S^2 = \\frac{MS_S - MS_E}{k}$\n- $\\hat{\\sigma}_R^2 = \\frac{MS_R - MS_E}{n}$\n- $\\hat{\\sigma}_E^2 = MS_E$\n\nWe now derive the two required ICCs.\n\n**1. ICC for Absolute Agreement (Single Measure)**\nThis ICC, often denoted $ICC(A,1)$ or $ICC(2,1)$ in the literature, is appropriate for a two-way random effects model where both subjects and raters are considered random samples from larger populations. It quantifies the degree of absolute agreement among ratings by treating systematic differences between raters as a source of error. The ICC is the proportion of total variance attributable to subject variability.\nThe total variance for a single observation is the sum of all variance components: $\\sigma_S^2 + \\sigma_R^2 + \\sigma_E^2$.\n$$ ICC_{abs} = \\frac{\\text{Subject Variance}}{\\text{Total Variance}} = \\frac{\\sigma_S^2}{\\sigma_S^2 + \\sigma_R^2 + \\sigma_E^2} $$\nSubstituting the estimators based on mean squares:\n$$ ICC_{abs} = \\frac{\\frac{MS_S - MS_E}{k}}{\\frac{MS_S - MS_E}{k} + \\frac{MS_R - MS_E}{n} + MS_E} $$\nA more common computational form, algebraically equivalent to the above, is:\n$$ ICC_{abs} = \\frac{MS_S - MS_E}{MS_S + (k-1)MS_E + \\frac{k}{n}(MS_R - MS_E)} $$\nAn additive bias in a rater will increase $\\bar{Y}_{.j}$ for that rater, leading to a large $MS_R$ and thereby decreasing the $ICC_{abs}$ value, correctly reflecting poorer absolute agreement.\n\n**2. ICC for Consistency (Single Measure)**\nThis ICC, often denoted $ICC(C,1)$ or $ICC(3,1)$, is appropriate for a two-way mixed-effects model where subjects are random but the $k$ raters are considered fixed. It measures the consistency of scores, meaning it is insensitive to additive biases in the raters. Since rater effects are fixed, rater variance is not part of the random variability. The relevant \"total\" variance is thus just the sum of subject and error variance.\n$$ ICC_{cons} = \\frac{\\text{Subject Variance}}{\\text{Subject Variance + Error Variance}} = \\frac{\\sigma_S^2}{\\sigma_S^2 + \\sigma_E^2} $$\nSubstituting the estimators:\n$$ ICC_{cons} = \\frac{\\frac{MS_S - MS_E}{k}}{\\frac{MS_S - MS_E}{k} + MS_E} $$\nMultiplying the numerator and denominator by $k$ yields the standard computational formula:\n$$ ICC_{cons} = \\frac{MS_S - MS_E}{MS_S + (k-1)MS_E} $$\nThis formula does not contain the $MS_R$ term. Consequently, large systematic differences between raters, which inflate $MS_R$, do not affect the value of $ICC_{cons}$.\n\n### Algorithmic Methodology\n\nThe program will execute the following steps for each test case:\n1.  **Initialize**: Set the parameters $(n, k, \\sigma_S, \\sigma_E, \\mathbf{B}, \\mathbf{S})$ and the pseudorandom number generator seed ($20231105 + \\text{case\\_index}$).\n2.  **Simulate Data**:\n    -   Generate $n$ true subject scores, $T_i \\sim \\mathcal{N}(0, \\sigma_S^2)$.\n    -   Generate an $n \\times k$ matrix of error terms, $E_{ij} \\sim \\mathcal{N}(0, \\sigma_E^2)$.\n    -   Construct the $n \\times k$ data matrix $Y$ according to the model $Y_{ij} = S_j T_i + B_j + E_{ij}$ using NumPy's broadcasting capabilities.\n3.  **Compute Mean Squares**:\n    -   Calculate $\\bar{Y}_{..}$, $\\bar{Y}_{i.}$, and $\\bar{Y}_{.j}$ from the matrix $Y$.\n    -   Use these means to compute $SS_S$, $SS_R$, and $SS_{Total}$.\n    -   Calculate $SS_E = SS_{Total} - SS_S - SS_R$.\n    -   Divide the sums of squares by their respective degrees of freedom ($n-1$, $k-1$, $(n-1)(k-1)$) to obtain $MS_S$, $MS_R$, and $MS_E$.\n4.  **Compute ICCs**:\n    -   Calculate $ICC_{abs}$ and $ICC_{cons}$ using the derived formulas.\n    -   For each ICC, check if its denominator is zero. If so, the result is Not-a-Number (`NaN`). Otherwise, perform the division.\n5.  **Store and Format**: Store the two calculated ICC values, and after processing all cases, format the collected results into the required single-line string output, with each value rounded to 6 decimal places.\n\n### Expected Behavior of Test Cases\n-   **Case 1 (Baseline)**: With no additive or multiplicative bias ($B_j=0, S_j=1$), the between-rater variance is expected to be small (only due to sampling error). Thus, $MS_R \\approx MS_E$, causing the term $\\frac{k}{n}(MS_R - MS_E)$ to be near zero. As a result, $ICC_{abs}$ and $ICC_{cons}$ should be very close in value and high, reflecting good agreement and consistency.\n-   **Case 2 (Additive Bias)**: The additive bias ($B=[0,5]$) will create a large, systematic difference between the mean ratings of the two raters, inflating $MS_R$. This will substantially increase the denominator of $ICC_{abs}$, lowering its value. $ICC_{cons}$ is insensitive to $MS_R$ and should remain high, similar to its value in Case 1.\n-   **Case 3 (Multiplicative Bias)**: The multiplicative bias ($S=[1, 1.2]$) introduces a subject-by-rater interaction, as the difference between raters' scores depends on the subject's true score. This interaction inflates the residual mean square, $MS_E$. An increased $MS_E$ reduces the numerator ($MS_S-MS_E$) and increases the denominator for both ICCs, thus degrading both consistency and absolute agreement.\n-   **Case 4 (Low Subject Variability)**: When the true between-subject standard deviation ($\\sigma_S=0.1$) is much smaller than the error standard deviation ($\\sigma_E=3$), the \"signal\" is swamped by \"noise\". The estimated subject variance, proportional to $MS_S - MS_E$, is likely to be very small or even negative (if $MS_S < MS_E$ due to sampling), leading to ICC values near zero or negative. The large additive bias will further depress $ICC_{abs}$. This case demonstrates that ICC values are low when subjects are not meaningfully distinguishable from one another.", "answer": "```python\nimport numpy as np\n\ndef calculate_iccs(Y: np.ndarray):\n    \"\"\"\n    Computes absolute agreement and consistency ICCs from a data matrix.\n\n    Args:\n        Y: An n x k numpy array of ratings, where n is the number of subjects\n           and k is the number of raters.\n\n    Returns:\n        A tuple containing (icc_abs, icc_cons).\n    \"\"\"\n    n, k = Y.shape\n\n    if n <= 1 or k <= 1:\n        return (np.nan, np.nan)\n\n    # Calculate mean squares\n    grand_mean = np.mean(Y)\n    subject_means = np.mean(Y, axis=1)\n    rater_means = np.mean(Y, axis=0)\n\n    # Sum of Squares\n    ss_total = np.sum((Y - grand_mean) ** 2)\n    ss_s = k * np.sum((subject_means - grand_mean) ** 2)\n    ss_r = n * np.sum((rater_means - grand_mean) ** 2)\n    ss_e = ss_total - ss_s - ss_r\n    \n    # Degrees of Freedom\n    df_s = n - 1\n    df_r = k - 1\n    df_e = (n - 1) * (k - 1)\n\n    # Mean Squares\n    ms_s = ss_s / df_s if df_s > 0 else 0\n    ms_r = ss_r / df_r if df_r > 0 else 0\n    ms_e = ss_e / df_e if df_e > 0 else 0\n\n    # Common numerator for both ICCs\n    numerator = ms_s - ms_e\n\n    # Calculate ICC for Absolute Agreement (ICC(A,1) or ICC(2,1))\n    denominator_abs = ms_s + (k - 1) * ms_e + (k / n) * (ms_r - ms_e)\n    if denominator_abs == 0:\n        icc_abs = np.nan\n    else:\n        icc_abs = numerator / denominator_abs\n\n    # Calculate ICC for Consistency (ICC(C,1) or ICC(3,1))\n    denominator_cons = ms_s + (k - 1) * ms_e\n    if denominator_cons == 0:\n        icc_cons = np.nan\n    else:\n        icc_cons = numerator / denominator_cons\n        \n    return icc_abs, icc_cons\n\ndef solve():\n    \"\"\"\n    Main function to run simulations and print results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma_S, sigma_E, B, S)\n        (40, 10, 3, np.array([0, 0]), np.array([1, 1])),    # Case 1: baseline\n        (40, 10, 3, np.array([0, 5]), np.array([1, 1])),    # Case 2: additive bias\n        (40, 10, 3, np.array([0, 0]), np.array([1, 1.2])),  # Case 3: multiplicative bias\n        (40, 0.1, 3, np.array([0, 5]), np.array([1, 1]))    # Case 4: low subject variability\n    ]\n\n    results = []\n    base_seed = 20231105\n    k = 2\n\n    for i, case in enumerate(test_cases):\n        n, sigma_s, sigma_e, B, S = case\n        seed = base_seed + i\n        rng = np.random.default_rng(seed)\n\n        # Simulate data according to the model Y_ij = S_j * T_i + B_j + E_ij\n        T = rng.normal(loc=0, scale=sigma_s, size=n)\n        E = rng.normal(loc=0, scale=sigma_e, size=(n, k))\n        \n        # Use broadcasting to construct the Y matrix\n        # T[:, np.newaxis] changes T from (n,) to (n, 1)\n        # S and B are broadcast from (k,) to (n, k)\n        Y = T[:, np.newaxis] * S + B + E\n\n        # Calculate ICCs\n        icc_abs, icc_cons = calculate_iccs(Y)\n        results.extend([icc_abs, icc_cons])\n\n    # Format the output string with values rounded to 6 decimal places.\n    # NaN values are formatted as \"nan\".\n    output_str = f\"[{','.join(f'{val:.6f}' for val in results)}]\"\n    print(output_str)\n\nsolve()\n\n```", "id": "4893327"}]}