{"hands_on_practices": [{"introduction": "Understanding a statistical measure begins with its mathematical foundation. This first exercise takes you back to basics, asking you to derive Cohen's Kappa for the simplest scenario: a $2 \\times 2$ table. By algebraically manipulating the core definitions, you will see how Kappa can be re-expressed not just in terms of agreement, but more intuitively as a function of the disagreement between raters, providing a deeper insight into what this coefficient truly quantifies. [@problem_id:4892815]", "problem": "Two independent clinicians, denoted by rater $A$ and rater $B$, classify each subject in a large study into one of two nominal categories, labeled $1$ and $2$. Let $p_{ij}$ denote the joint probability that rater $A$ assigns category $i \\in \\{1,2\\}$ and rater $B$ assigns category $j \\in \\{1,2\\}$, with $p_{11}+p_{12}+p_{21}+p_{22}=1$. Define the marginal probabilities $p_{i\\cdot}=p_{i1}+p_{i2}$ for rater $A$ and $p_{\\cdot j}=p_{1j}+p_{2j}$ for rater $B$. The observed agreement proportion $p_{o}$ is the probability that the two raters agree, and the expected agreement proportion $p_{e}$ is the probability of agreement under the assumption that the two raters classify independently, preserving their marginal distributions.\n\nStarting only from these definitions and the laws of probability, first derive $p_{o}$ and $p_{e}$ in terms of the joint and marginal probabilities in the $2\\times 2$ case. Then, using the definition of Cohen’s kappa as the chance-corrected agreement, algebraically simplify the expression for the kappa coefficient $\\kappa$ to make explicit how it depends on the total off-diagonal discordance $p_{12}+p_{21}$. Express your final answer for $\\kappa$ as a single closed-form symbolic expression in terms of $p_{12}+p_{21}$, $p_{1\\cdot}$, and $p_{\\cdot 1}$. No rounding is required, and no units are involved.", "solution": "The setting involves two raters classifying into two categories with joint probabilities $p_{ij}$ and corresponding marginals $p_{i\\cdot}$ and $p_{\\cdot j}$. By the law of total probability, the probability of agreement is the sum of the diagonal joint probabilities. Therefore, the observed agreement proportion is\n$$\np_{o} = \\Pr(\\text{agree}) = \\Pr(\\text{both }1) + \\Pr(\\text{both }2) = p_{11} + p_{22}.\n$$\nUnder the assumption of independence between the raters while preserving their marginal distributions, the expected agreement is the sum over categories of the product of the corresponding marginals. Specifically,\n$$\np_{e} = \\Pr(\\text{agree under independence}) = p_{1\\cdot} p_{\\cdot 1} + p_{2\\cdot} p_{\\cdot 2}.\n$$\nThese are the canonical expressions for observed and expected agreement in the $2\\times 2$ case.\n\nCohen’s kappa $\\kappa$ is defined as the proportionate reduction in disagreement relative to what would be expected by chance:\n$$\n\\kappa = \\frac{p_{o} - p_{e}}{1 - p_{e}}.\n$$\nTo reveal the dependence on off-diagonal discordance, note that the total probability satisfies\n$$\np_{11} + p_{12} + p_{21} + p_{22} = 1,\n$$\nwhich implies\n$$\np_{o} = p_{11} + p_{22} = 1 - (p_{12} + p_{21}).\n$$\nSubstituting $p_{o} = 1 - (p_{12} + p_{21})$ into the definition of $\\kappa$ yields\n$$\n\\kappa = \\frac{1 - (p_{12} + p_{21}) - p_{e}}{1 - p_{e}} = 1 - \\frac{p_{12} + p_{21}}{1 - p_{e}}.\n$$\nThis form already shows that $\\kappa$ decreases linearly with the off-diagonal discordance $p_{12}+p_{21}$, scaled by the factor $1 - p_{e}$, which encodes the chance agreement based on the marginals. To express the denominator purely in terms of the marginals $p_{1\\cdot}$ and $p_{\\cdot 1}$, observe that\n$$\np_{2\\cdot} = 1 - p_{1\\cdot}, \\quad p_{\\cdot 2} = 1 - p_{\\cdot 1},\n$$\nso\n$$\np_{e} = p_{1\\cdot} p_{\\cdot 1} + (1 - p_{1\\cdot})(1 - p_{\\cdot 1}) = p_{1\\cdot} p_{\\cdot 1} + 1 - p_{1\\cdot} - p_{\\cdot 1} + p_{1\\cdot} p_{\\cdot 1} = 1 - p_{1\\cdot} - p_{\\cdot 1} + 2 p_{1\\cdot} p_{\\cdot 1}.\n$$\nTherefore,\n$$\n1 - p_{e} = p_{1\\cdot} + p_{\\cdot 1} - 2 p_{1\\cdot} p_{\\cdot 1}.\n$$\nSubstituting this into the simplified $\\kappa$ expression gives the fully explicit dependence of $\\kappa$ on the off-diagonal discordance and the marginals:\n$$\n\\kappa = 1 - \\frac{p_{12} + p_{21}}{p_{1\\cdot} + p_{\\cdot 1} - 2 p_{1\\cdot} p_{\\cdot 1}}.\n$$\nThis final form demonstrates that $\\kappa$ is equal to one minus the ratio of the observed discordance $p_{12}+p_{21}$ to the discordance expected under independent classifications with the given marginals.", "answer": "$$\\boxed{1 - \\frac{p_{12}+p_{21}}{p_{1\\cdot}+p_{\\cdot 1}-2\\,p_{1\\cdot}\\,p_{\\cdot 1}}}$$", "id": "4892815"}, {"introduction": "While the Kappa formula is straightforward, its interpretation can be surprisingly complex, especially in cases with unbalanced marginal distributions. This practice explores these \"paradoxes of Kappa\" through a series of hypothetical studies where high agreement might not translate to a high Kappa value. Working through these scenarios will sharpen your critical thinking and help you avoid common misinterpretations, while also introducing best practices for reporting results when Kappa is undefined or potentially misleading. [@problem_id:4892729]", "problem": "Two independent clinical raters classify each subject into nominal categories. Agreement beyond chance is assessed using a chance-corrected agreement coefficient that compares the observed proportion of agreement to the agreement expected under independence given the empirical marginal distributions of the raters. Consider a binary classification with categories “Absent” and “Present,” and three hypothetical studies, each with $N=50$ subjects:\n\nStudy $S1$:\n- Rater A says “Absent” for all $50$ subjects.\n- Rater B says “Absent” for all $50$ subjects.\n- Cross-classification counts: “Absent/Absent” $=50$, all other cells $=0$.\n\nStudy $S2$:\n- Rater A says “Absent” for all $50$ subjects.\n- Rater B says “Absent” for $40$ subjects and “Present” for $10$ subjects.\n- Cross-classification counts: “Absent/Absent” $=40$, “Absent/Present” $=10$, all other cells $=0$.\n\nStudy $S3$:\n- Rater A’s classifications: “Absent” for $35$ subjects and “Present” for $15$ subjects.\n- Rater B exactly matches A on every subject.\n- Cross-classification counts: “Absent/Absent” $=35$, “Present/Present” $=15$, all other cells $=0$.\n\nUsing only the foundational definitions that (i) the observed agreement is the proportion of subjects on which the raters’ categories match and (ii) the expected agreement “by chance” is computed under independence from the empirical marginal category proportions, determine which of the following statements are correct. Select all that apply.\n\nA. In $S1$, the chance agreement equals $1$, making the agreement coefficient undefined; a principled reporting rule is to avoid forcing a numeric coefficient, report the observed agreement and the degenerate marginal distributions, and note that beyond-chance agreement is not estimable due to no variation.\n\nB. In $S2$, the agreement coefficient is undefined whenever one rater uses only a single category; a standard decision rule is to set the coefficient to $0$ by convention.\n\nC. In $S3$, perfect observed agreement does not make the coefficient undefined because the chance agreement is strictly less than $1$ when the marginals are nondegenerate; here the coefficient is well-defined and equals its maximum value.\n\nD. In any binary study where the two raters always disagree (for example, one always “Present” and the other always “Absent”), the coefficient is undefined because both the observed and the chance agreements equal $0$; thus no agreement can be quantified.\n\nE. When the coefficient is undefined because the chance agreement equals $1$ (for example, both raters always choose the same single category), a reasonable decision rule is to report observed agreement and marginal distributions, optionally supplementing with category-specific agreements (for example, positive and negative specific agreement) and an alternative index such as the prevalence-adjusted bias-adjusted kappa (PABAK) or Gwet’s Agreement Coefficient (AC1) as a sensitivity analysis, rather than forcing a kappa value.", "solution": "The problem asks us to evaluate five statements about a chance-corrected agreement coefficient, which is defined as Cohen's kappa coefficient ($\\kappa$). The coefficient is given by the formula:\n$$\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n$$\nwhere $p_o$ is the observed proportion of agreement and $p_e$ is the expected proportion of agreement under the assumption of independence between the two raters. The coefficient is undefined when the denominator is zero, which occurs if and only if $p_e = 1$.\n\nLet's represent the data for a binary classification in a $2 \\times 2$ contingency table with $N$ total subjects:\n\n| | Rater B: Absent | Rater B: Present | Row Total |\n| :--- | :---: | :---: | :---: |\n| **Rater A: Absent** | $a$ | $b$ | $r_1 = a+b$ |\n| **Rater A: Present**| $c$ | $d$ | $r_2 = c+d$ |\n| **Column Total** | $c_1 = a+c$ | $c_2 = b+d$ | $N = a+b+c+d$ |\n\nBased on the problem's definitions:\n(i) The observed proportion of agreement is the sum of diagonal elements divided by the total number of subjects:\n$$\np_o = \\frac{a+d}{N}\n$$\n(ii) The expected proportion of agreement by chance is calculated from the marginal proportions. The probability of Rater A choosing \"Absent\" is $P(\\text{A=Absent}) = (a+b)/N$, and for Rater B it is $P(\\text{B=Absent}) = (a+c)/N$. The probability of both choosing \"Present\" involves marginals $(c+d)/N$ and $(b+d)/N$. Under independence, the expected agreement is:\n$$\np_e = P(\\text{A=Absent})P(\\text{B=Absent}) + P(\\text{A=Present})P(\\text{B=Present}) = \\frac{(a+b)(a+c)}{N^2} + \\frac{(c+d)(b+d)}{N^2}\n$$\n\nNow we analyze each study.\n\n**Study S1:**\nThe cross-classification is $a=50$, $b=0$, $c=0$, $d=0$, with $N=50$.\n- Observed agreement: $p_o = \\frac{50+0}{50} = 1$.\n- Marginal totals: $r_1=50, r_2=0, c_1=50, c_2=0$.\n- Marginal proportions: $P(\\text{A=Absent}) = 50/50 = 1$, $P(\\text{A=Present}) = 0/50 = 0$. Similarly, $P(\\text{B=Absent}) = 1$, $P(\\text{B=Present}) = 0$.\n- Expected agreement: $p_e = (1)(1) + (0)(0) = 1$.\n- Kappa coefficient: $\\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{1 - 1}{1 - 1} = \\frac{0}{0}$. The coefficient is mathematically indeterminate (undefined).\n\n**Study S2:**\nThe cross-classification is $a=40$, $b=10$, $c=0$, $d=0$, with $N=50$.\n- Observed agreement: $p_o = \\frac{40+0}{50} = 0.8$.\n- Marginal totals: $r_1=50, r_2=0, c_1=40, c_2=10$.\n- Marginal proportions: $P(\\text{A=Absent}) = 50/50 = 1$, $P(\\text{A=Present}) = 0/50 = 0$. For Rater B, $P(\\text{B=Absent}) = 40/50 = 0.8$, $P(\\text{B=Present}) = 10/50 = 0.2$.\n- Expected agreement: $p_e = (1)(0.8) + (0)(0.2) = 0.8$.\n- Kappa coefficient: $\\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{0.8 - 0.8}{1 - 0.8} = \\frac{0}{0.2} = 0$. The coefficient is well-defined and equals $0$.\n\n**Study S3:**\nThe cross-classification is $a=35$, $b=0$, $c=0$, $d=15$, with $N=50$.\n- Observed agreement: $p_o = \\frac{35+15}{50} = \\frac{50}{50} = 1$.\n- Marginal totals: $r_1=35, r_2=15, c_1=35, c_2=15$.\n- Marginal proportions: $P(\\text{A=Absent}) = 35/50 = 0.7$, $P(\\text{A=Present}) = 15/50 = 0.3$. The marginals for Rater B are identical.\n- Expected agreement: $p_e = (0.7)(0.7) + (0.3)(0.3) = 0.49 + 0.09 = 0.58$.\n- Kappa coefficient: $\\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{1 - 0.58}{1 - 0.58} = 1$. The coefficient is well-defined and equals its maximum value of $1$.\n\nWith these calculations, we evaluate each statement.\n\n**A. In $S1$, the chance agreement equals $1$, making the agreement coefficient undefined; a principled reporting rule is to avoid forcing a numeric coefficient, report the observed agreement and the degenerate marginal distributions, and note that beyond-chance agreement is not estimable due to no variation.**\nOur analysis of S1 showed $p_e = 1$ and $\\kappa$ is undefined ($0/0$). This is a well-known issue when there is no variability in ratings from either rater (both use only one category). In such a case, the concept of \"chance\" agreement is degenerate, as chance can only lead to the single observed outcome. The most scientifically rigorous approach is to describe the data as they are: perfect observed agreement ($p_o=1$), but with degenerate marginals that make a chance-corrected measure like kappa inestimable. The statement accurately describes the mathematical result and the correct statistical reporting practice.\nVerdict: **Correct**.\n\n**B. In $S2$, the agreement coefficient is undefined whenever one rater uses only a single category; a standard decision rule is to set the coefficient to $0$ by convention.**\nOur analysis of S2 showed that $\\kappa$ is well-defined and its value is $0$. The premise that the coefficient is undefined is false. Rater A uses a single category, which leads to a \"paradoxical\" result where high observed agreement ($p_o=0.8$) yields a kappa of $0$, but it does not make the coefficient undefined. The expression for $\\kappa$ was $\\frac{0}{0.2}$, which is unambiguously $0$. The value is not set to $0$ by convention; it is calculated to be $0$.\nVerdict: **Incorrect**.\n\n**C. In $S3$, perfect observed agreement does not make the coefficient undefined because the chance agreement is strictly less than $1$ when the marginals are nondegenerate; here the coefficient is well-defined and equals its maximum value.**\nOur analysis of S3 confirms all parts of this statement. Observed agreement is perfect ($p_o=1$). The marginals are non-degenerate (raters used both \"Absent\" and \"Present\" categories), leading to $p_e=0.58$, which is strictly less than $1$. Consequently, the denominator $1-p_e$ is non-zero ($0.42$), and $\\kappa$ is well-defined. The value is $\\kappa = \\frac{1-0.58}{1-0.58} = 1$, which is the maximum possible value for kappa, indicating perfect agreement beyond what is expected by chance.\nVerdict: **Correct**.\n\n**D. In any binary study where the two raters always disagree (for example, one always “Present” and the other always “Absent”), the coefficient is undefined because both the observed and the chance agreements equal $0$; thus no agreement can be quantified.**\nLet's analyze the given example: Rater A always says \"Absent\", and Rater B always says \"Present\". The table has $a=0, d=0$ and non-zero off-diagonal cells. Let's say for $N$ subjects, $b=N, c=0$.\n- Observed agreement: $p_o = \\frac{0+0}{N} = 0$.\n- Marginal proportions: $P(\\text{A=Absent}) = N/N = 1$, $P(\\text{A=Present}) = 0$. $P(\\text{B=Absent}) = 0$, $P(\\text{B=Present}) = N/N = 1$.\n- Expected agreement: $p_e = (1)(0) + (0)(1) = 0$.\n- Kappa coefficient: $\\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{0 - 0}{1 - 0} = 0$.\nThe coefficient is not undefined; it is $0$. The statement's claim that it's undefined because $p_o=0$ and $p_e=0$ is mathematically false. The condition for being undefined is $p_e=1$. In a general case of total disagreement ($p_o=0$), $\\kappa = \\frac{-p_e}{1-p_e}$, which is typically a defined negative number. It can never be undefined, as $p_o=0$ implies that for any non-trivial study, $p_e  1$.\nVerdict: **Incorrect**.\n\n**E. When the coefficient is undefined because the chance agreement equals $1$ (for example, both raters always choose the same single category), a reasonable decision rule is to report observed agreement and marginal distributions, optionally supplementing with category-specific agreements (for example, positive and negative specific agreement) and an alternative index such as the prevalence-adjusted bias-adjusted kappa (PABAK) or Gwet’s Agreement Coefficient (AC1) as a sensitivity analysis, rather than forcing a kappa value.**\nThis statement addresses the scenario in S1 where $\\kappa$ is undefined. It proposes a reporting strategy. Reporting observed agreement ($p_o=1$) and the degenerate marginals is the primary correct action, as stated in A. The suggestion to supplement this with other measures is a reflection of modern best practices in statistics. Indices like PABAK and Gwet's AC1 are specifically designed to handle the \"paradoxes\" of kappa, including cases where it is undefined. For S1, PABAK would be $2p_o - 1 = 2(1)-1=1$, and Gwet's AC1 would also be $1$. These measures provide a defined value reflecting perfect agreement, which is often more interpretable than reporting an undefined kappa. This comprehensive reporting strategy is indeed a reasonable and recommended rule.\nVerdict: **Correct**.\n\nIn summary, statements A, C, and E are correct.", "answer": "$$\\boxed{ACE}$$", "id": "4892729"}, {"introduction": "Real-world data is rarely homogeneous, and studies often involve distinct subgroups or strata. This practice moves from a single contingency table to a stratified analysis, a common scenario in clinical and epidemiological research. You will calculate Kappa for different strata and compare two common methods for obtaining an overall summary of agreement, discovering the important statistical property of non-collapsibility and its practical implications for data analysis. [@problem_id:4892755]", "problem": "Two independent human raters classify a binary clinical outcome (Yes/No) on patients drawn from three strata: outpatient clinic ($s=1$), inpatient ward ($s=2$), and emergency department ($s=3$). Within each stratum, the cross-classification between Rater $1$ (rows) and Rater $2$ (columns) is recorded as a $2 \\times 2$ contingency table of counts. The recorded counts are:\n\nStratum $1$ (outpatient clinic, total $n_1 = 50$):\n- $(\\text{Rater }1=\\text{Yes}, \\text{Rater }2=\\text{Yes}) = 30$\n- $(\\text{Rater }1=\\text{Yes}, \\text{Rater }2=\\text{No}) = 5$\n- $(\\text{Rater }1=\\text{No}, \\text{Rater }2=\\text{Yes}) = 10$\n- $(\\text{Rater }1=\\text{No}, \\text{Rater }2=\\text{No}) = 5$\n\nStratum $2$ (inpatient ward, total $n_2 = 40$):\n- $(\\text{Rater }1=\\text{Yes}, \\text{Rater }2=\\text{Yes}) = 18$\n- $(\\text{Rater }1=\\text{Yes}, \\text{Rater }2=\\text{No}) = 6$\n- $(\\text{Rater }1=\\text{No}, \\text{Rater }2=\\text{Yes}) = 4$\n- $(\\text{Rater }1=\\text{No}, \\text{Rater }2=\\text{No}) = 12$\n\nStratum $3$ (emergency department, total $n_3 = 30$):\n- $(\\text{Rater }1=\\text{Yes}, \\text{Rater }2=\\text{Yes}) = 8$\n- $(\\text{Rater }1=\\text{Yes}, \\text{Rater }2=\\text{No}) = 7$\n- $(\\text{Rater }1=\\text{No}, \\text{Rater }2=\\text{Yes}) = 3$\n- $(\\text{Rater }1=\\text{No}, \\text{Rater }2=\\text{No}) = 12$\n\nYou plan to compute stratum-specific and pooled Cohen’s kappa ($\\kappa$) using array-based, vectorized operations over strata (for example, building arrays of diagonal sums and marginal totals and applying elementwise arithmetic), and to verify the implementation by unit tests on synthetic tables. Consider the following candidate statements about the results and tests:\n\n- Pooled $\\kappa$ computed on the aggregated $2 \\times 2$ table obtained by summing cell counts across strata is approximately $0.386$ (rounded to three decimals).\n- The weighted average of stratum-specific $\\kappa$ values using stratum totals $(n_s)$ as weights is approximately $0.334$ and, in these data, is not equal to the pooled $\\kappa$.\n- A unit test on a synthetic $2 \\times 2$ table with perfect agreement (all mass on the diagonal, off-diagonals equal to $0$) should return $\\kappa = 1$.\n- A unit test on a synthetic $2 \\times 2$ table constructed to reflect independence (row and column marginal proportions equal, and observed cell proportions equal to products of marginals) should return $\\kappa = 0$.\n- A correctly written vectorized implementation that computes per-stratum observed agreement and expected agreement from marginals and then aggregates should match the results from an equivalent loop-based implementation to machine precision on these integer tables.\n\nWhich option reports the correct combination of numeric results and unit test expectations?\n\nA. All five statements above are correct.\n\nB. The pooled $\\kappa$ is approximately $0.334$, the weighted average is approximately $0.386$, perfect agreement yields $\\kappa = 0.95$, independence yields $\\kappa = 0$, and vectorized and loop-based implementations match.\n\nC. The pooled $\\kappa$ is approximately $0.386$, the weighted average equals the pooled value to three decimals, perfect agreement yields $\\kappa = 1$, independence yields $\\kappa = 0.5$, and vectorized and loop-based implementations match.\n\nD. The pooled $\\kappa$ is approximately $0.334$, the weighted average is approximately $0.334$, perfect agreement yields $\\kappa = 1$, independence yields $\\kappa = 0$, and vectorized and loop-based implementations do not match.", "solution": "The user wants a critical validation of the problem statement, followed by a full solution deriving the answer and evaluating all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following data and context:\n- Raters: Two independent human raters.\n- Outcome: Binary clinical outcome (Yes/No).\n- Strata: $s=1$ (outpatient, $n_1=50$), $s=2$ (inpatient, $n_2=40$), $s=3$ (emergency, $n_3=30$).\n- Contingency Tables: The cross-classification counts for Rater $1$ (rows) vs. Rater $2$ (columns) are given for each stratum. Let's denote these as $C_s = \\begin{pmatrix} a_s  b_s \\\\ c_s  d_s \\end{pmatrix}$.\n\n- For Stratum $s=1$: $C_1 = \\begin{pmatrix} 30  5 \\\\ 10  5 \\end{pmatrix}$ with total $n_1 = 30+5+10+5 = 50$.\n- For Stratum $s=2$: $C_2 = \\begin{pmatrix} 18  6 \\\\ 4  12 \\end{pmatrix}$ with total $n_2 = 18+6+4+12 = 40$.\n- For Stratum $s=3$: $C_3 = \\begin{pmatrix} 8  7 \\\\ 3  12 \\end{pmatrix}$ with total $n_3 = 8+7+3+12 = 30$.\n\nThe task is to evaluate five statements concerning the calculation of Cohen's kappa ($\\kappa$) and related software implementation concepts.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding:** The problem is a standard, realistic application of biostatistics. Cohen's kappa is a widely used measure of inter-rater reliability for categorical items. The use of stratification and different methods of pooling (aggregating the table vs. averaging statistics) are common and important topics in epidemiology and biostatistics. The problem is firmly grounded in established statistical principles.\n- **Well-Posedness:** The data provided are complete, consistent, and sufficient to perform all required calculations. The stratum totals ($n_s$) match the sums of the cell counts in each contingency table. The questions posed in the five candidate statements are specific and have unique, verifiable answers.\n- **Objectivity:** The problem is stated in precise, objective language. The data are numerical, and the questions pertain to verifiable calculations and established theoretical properties of the kappa statistic and computational methods.\n\nThe problem does not exhibit any flaws. It is not scientifically unsound, incomplete, contradictory, unrealistic, or ill-posed.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The solution will proceed by evaluating each of the five candidate statements.\n\n### Solution Derivation\n\nThe formula for Cohen's kappa ($\\kappa$) for a $2 \\times 2$ table $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ with total count $n = a+b+c+d$ is:\n$$ \\kappa = \\frac{p_o - p_e}{1 - p_e} $$\nwhere the observed agreement is $p_o = \\frac{a+d}{n}$ and the expected agreement by chance is $p_e = \\frac{(a+b)(a+c) + (c+d)(b+d)}{n^2}$.\n\nWe will now evaluate each of the five statements from the problem description.\n\n**Evaluation of Statement 1:** \"Pooled $\\kappa$ computed on the aggregated $2 \\times 2$ table obtained by summing cell counts across strata is approximately $0.386$ (rounded to three decimals).\"\n\nFirst, we create the aggregated table, $C_{agg}$, by summing the cell counts from $C_1, C_2, C_3$:\n$$ C_{agg} = C_1+C_2+C_3 = \\begin{pmatrix} 30  5 \\\\ 10  5 \\end{pmatrix} + \\begin{pmatrix} 18  6 \\\\ 4  12 \\end{pmatrix} + \\begin{pmatrix} 8  7 \\\\ 3  12 \\end{pmatrix} = \\begin{pmatrix} 30+18+8  5+6+7 \\\\ 10+4+3  5+12+12 \\end{pmatrix} = \\begin{pmatrix} 56  18 \\\\ 17  29 \\end{pmatrix} $$\nThe total count for the aggregated table is $n_{agg} = n_1+n_2+n_3 = 50+40+30 = 120$. Checking the sum of cells in $C_{agg}$: $56+18+17+29 = 120$.\n\nNow, we compute $\\kappa$ for this aggregated table.\n- Observed agreement, $p_o$:\n$$ p_o = \\frac{\\text{sum of diagonal elements}}{n_{agg}} = \\frac{56+29}{120} = \\frac{85}{120} \\approx 0.70833 $$\n- Expected agreement, $p_e$:\nThe marginal totals are:\nRow 1 total (Rater 1 'Yes'): $56+18 = 74$\nRow 2 total (Rater 1 'No'): $17+29 = 46$\nCol 1 total (Rater 2 'Yes'): $56+17 = 73$\nCol 2 total (Rater 2 'No'): $18+29 = 47$\n$$ p_e = \\frac{(74)(73) + (46)(47)}{120^2} = \\frac{5402 + 2162}{14400} = \\frac{7564}{14400} \\approx 0.52528 $$\n- Pooled kappa, $\\kappa_{pooled}$:\n$$ \\kappa_{pooled} = \\frac{p_o - p_e}{1 - p_e} = \\frac{0.70833 - 0.52528}{1 - 0.52528} = \\frac{0.18305}{0.47472} \\approx 0.38559 $$\nRounding to three decimal places gives $0.386$.\n**Verdict on Statement 1: Correct.**\n\n**Evaluation of Statement 2:** \"The weighted average of stratum-specific $\\kappa$ values using stratum totals $(n_s)$ as weights is approximately $0.334$ and, in these data, is not equal to the pooled $\\kappa$.\"\n\nWe must first compute $\\kappa_s$ for each stratum $s \\in \\{1, 2, 3\\}$.\n\n- **Stratum 1 ($n_1=50$):** $C_1 = \\begin{pmatrix} 30  5 \\\\ 10  5 \\end{pmatrix}$.\n  $p_{o,1} = \\frac{30+5}{50} = 0.7$. Marginals: rows $(35, 15)$, cols $(40, 10)$.\n  $p_{e,1} = \\frac{(35)(40) + (15)(10)}{50^2} = \\frac{1400+150}{2500} = 0.62$.\n  $\\kappa_1 = \\frac{0.7 - 0.62}{1 - 0.62} = \\frac{0.08}{0.38} = \\frac{4}{19} \\approx 0.2105$.\n\n- **Stratum 2 ($n_2=40$):** $C_2 = \\begin{pmatrix} 18  6 \\\\ 4  12 \\end{pmatrix}$.\n  $p_{o,2} = \\frac{18+12}{40} = 0.75$. Marginals: rows $(24, 16)$, cols $(22, 18)$.\n  $p_{e,2} = \\frac{(24)(22) + (16)(18)}{40^2} = \\frac{528+288}{1600} = 0.51$.\n  $\\kappa_2 = \\frac{0.75 - 0.51}{1 - 0.51} = \\frac{0.24}{0.49} = \\frac{24}{49} \\approx 0.4898$.\n\n- **Stratum 3 ($n_3=30$):** $C_3 = \\begin{pmatrix} 8  7 \\\\ 3  12 \\end{pmatrix}$.\n  $p_{o,3} = \\frac{8+12}{30} = \\frac{2}{3} \\approx 0.6667$. Marginals: rows $(15, 15)$, cols $(11, 19)$.\n  $p_{e,3} = \\frac{(15)(11) + (15)(19)}{30^2} = \\frac{165+285}{900} = 0.5$.\n  $\\kappa_3 = \\frac{2/3 - 0.5}{1 - 0.5} = \\frac{1/6}{1/2} = \\frac{1}{3} \\approx 0.3333$.\n\nNow, we compute the weighted average, $\\kappa_{w-avg}$:\n$$ \\kappa_{w-avg} = \\frac{\\sum n_s \\kappa_s}{\\sum n_s} = \\frac{50(\\frac{4}{19}) + 40(\\frac{24}{49}) + 30(\\frac{1}{3})}{120} $$\n$$ \\kappa_{w-avg} = \\frac{10.5263... + 19.5918... + 10}{120} = \\frac{40.1181...}{120} \\approx 0.3343 $$\nRounding to three decimal places gives $0.334$.\nThe statement also claims this value is not equal to the pooled $\\kappa$. We found $\\kappa_{pooled} \\approx 0.386$ and $\\kappa_{w-avg} \\approx 0.334$. Clearly, $0.334 \\neq 0.386$. This inequality is a known statistical phenomenon, as the kappa statistic is not a collapsible measure.\n**Verdict on Statement 2: Correct.**\n\n**Evaluation of Statement 3:** \"A unit test on a synthetic $2 \\times 2$ table with perfect agreement (all mass on the diagonal, off-diagonals equal to $0$) should return $\\kappa = 1$.\"\n\nA table with perfect agreement has off-diagonal cells equal to $0$: $b=0, c=0$.\nThe table is $\\begin{pmatrix} a  0 \\\\ 0  d \\end{pmatrix}$ with $n=a+d$.\nThe observed agreement is $p_o = \\frac{a+d}{n} = 1$.\nThe expected agreement is $p_e = \\frac{(a+0)(a+0) + (0+d)(0+d)}{(a+d)^2} = \\frac{a^2+d^2}{(a+d)^2}$.\nThe kappa is $\\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{1 - p_e}{1 - p_e}$.\nThis expression equals $1$ as long as the denominator is not zero, i.e., $p_e \\neq 1$. $p_e=1$ only if $a=0$ or $d=0$, which means all ratings fall into a single category (e.g., all raters say 'Yes' for all subjects). In this degenerate case, $\\kappa$ is undefined. A proper unit test would use a non-degenerate case (e.g., $a0$ and $d0$), for which $\\kappa$ is unequivocally $1$.\n**Verdict on Statement 3: Correct.**\n\n**Evaluation of Statement 4:** \"A unit test on a synthetic $2 \\times 2$ table constructed to reflect independence (row and column marginal proportions equal, and observed cell proportions equal to products of marginals) should return $\\kappa = 0$.\"\n\nBy definition, Cohen's kappa measures the agreement beyond that which is expected by chance. The chance-expected agreement, $p_e$, is calculated under the assumption of statistical independence between the raters' judgments. If the observed agreement, $p_o$, is exactly equal to the chance-expected agreement, i.e., $p_o = p_e$, then the numerator of the kappa formula becomes $p_o - p_e = 0$.\n$$ \\kappa = \\frac{0}{1 - p_e} = 0 $$\nThis is true as long as $p_e \\neq 1$. A table reflecting independence is precisely the case where observed agreement equals chance agreement.\n**Verdict on Statement 4: Correct.**\n\n**Evaluation of Statement 5:** \"A correctly written vectorized implementation that computes per-stratum observed agreement and expected agreement from marginals and then aggregates should match the results from an equivalent loop-based implementation to machine precision on these integer tables.\"\n\nThis statement deals with computational methods. A vectorized implementation performs operations on entire arrays at once (e.g., `po = (a_counts + d_counts) / n_counts`). A loop-based implementation iterates through each stratum, performing scalar calculations. For the arithmetic operations required to calculate kappa (addition, multiplication, division), the sequence of floating-point operations for each stratum's data is identical in both approaches. Since the inputs are exact integers and the operational sequence is the same, two correctly written programs, one vectorized and one using a loop, will produce bit-for-bit identical floating-point results. There is no source of numerical discrepancy.\n**Verdict on Statement 5: Correct.**\n\n### Option-by-Option Analysis\n\nBased on the evaluation of the five statements, all are correct. We can now evaluate the given options.\n\n- **A. All five statements above are correct.**\nThis aligns perfectly with our detailed analysis.\n\n- **B. The pooled $\\kappa$ is approximately $0.334$, the weighted average is approximately $0.386$, perfect agreement yields $\\kappa = 0.95$, independence yields $\\kappa = 0$, and vectorized and loop-based implementations match.**\nThis option incorrectly swaps the values of the pooled and weighted average kappas. It also incorrectly states that perfect agreement yields $\\kappa = 0.95$. Therefore, it is incorrect.\n\n- **C. The pooled $\\kappa$ is approximately $0.386$, the weighted average equals the pooled value to three decimals, perfect agreement yields $\\kappa = 1$, independence yields $\\kappa = 0.5$, and vectorized and loop-based implementations match.**\nThis option incorrectly claims the weighted average equals the pooled value, and incorrectly states that independence yields $\\kappa = 0.5$. Therefore, it is incorrect.\n\n- **D. The pooled $\\kappa$ is approximately $0.334$, the weighted average is approximately $0.334$, perfect agreement yields $\\kappa = 1$, independence yields $\\kappa = 0$, and vectorized and loop-based implementations do not match.**\nThis option incorrectly states the value of the pooled kappa, and incorrectly claims that vectorized and loop-based implementations would not match. Therefore, it is incorrect.\n\nThe only option that is fully consistent with the derived results and theoretical principles is A.\nFinal summary of our findings:\n- Statement 1: Correct ($\\kappa_{pooled} \\approx 0.386$)\n- Statement 2: Correct ($\\kappa_{w-avg} \\approx 0.334$ and $\\kappa_{w-avg} \\neq \\kappa_{pooled}$)\n- Statement 3: Correct ($\\kappa=1$ for perfect agreement)\n- Statement 4: Correct ($\\kappa=0$ for independence)\n- Statement 5: Correct (implementations match)\n\nAll five are correct.", "answer": "$$\\boxed{A}$$", "id": "4892755"}]}