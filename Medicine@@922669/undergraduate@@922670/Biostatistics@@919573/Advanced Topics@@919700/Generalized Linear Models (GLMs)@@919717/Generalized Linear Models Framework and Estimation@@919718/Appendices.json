{"hands_on_practices": [{"introduction": "This practice builds the engine of Generalized Linear Model (GLM) estimation from the ground up. By deriving the log-likelihood function, its gradient (the score vector), and its second derivative (the Hessian matrix) for a logistic regression model, you will uncover the mathematical machinery that statistical software uses to find maximum likelihood estimates. This fundamental exercise is crucial for demystifying the fitting process and for understanding the foundations of statistical inference in GLMs. [@problem_id:4914224]", "problem": "A biostatistics study records binary outcomes for the presence of a specific biomarker in blood samples. For each subject $i$ with $i \\in \\{1,\\dots,n\\}$, the outcome $Y_{i} \\in \\{0,1\\}$ indicates absence or presence of the biomarker, and a single continuous covariate $X_{i} \\in \\mathbb{R}$ records a standardized exposure score. Assume subjects are independent and that conditional on $X_{i}$, $Y_{i}$ follows a Bernoulli model with mean $\\mu_{i} = \\mathbb{E}(Y_{i} \\mid X_{i})$. Consider a generalized linear model (GLM) with the canonical logit link for the Bernoulli distribution, so that the link function satisfies $\\ln\\!\\big(\\mu_{i}/(1-\\mu_{i})\\big) = \\beta_{0} + \\beta_{1} X_{i}$, where $\\beta = (\\beta_{0},\\beta_{1})^{\\top}$ is an unknown parameter vector with intercept and one slope. Starting only from the independence assumption, the Bernoulli probability mass function, and the definition of the canonical link, derive the following quantities as explicit functions of $\\beta$ and the observed data $\\{(y_{i},x_{i})\\}_{i=1}^{n}$:\n\n1. The log-likelihood $\\ell(\\beta)$.\n2. The gradient (score) vector $\\nabla \\ell(\\beta)$.\n3. The Hessian matrix $\\nabla^{2} \\ell(\\beta)$.\n\nExpress each quantity using summations over $i$ and elementary functions of $\\beta$ and the data. Your final expressions must depend only on $\\beta_{0}$, $\\beta_{1}$, $\\{x_{i}\\}_{i=1}^{n}$, and $\\{y_{i}\\}_{i=1}^{n}$, without introducing any undefined shorthand. The final answer should present these three derived expressions together as a single analytic expression. No numerical computation is required, and no rounding is needed.", "solution": "The problem statement is a standard derivation exercise in the context of generalized linear models (GLMs) for binary data and has been validated as scientifically grounded, well-posed, objective, and complete. There are no flaws. We proceed with the derivation of the log-likelihood, its gradient, and its Hessian.\n\nThe model is specified for $n$ independent subjects, indexed by $i=1, \\dots, n$. For each subject, we have a binary outcome $Y_i \\in \\{0, 1\\}$ and a continuous covariate $X_i \\in \\mathbb{R}$.\nThe conditional distribution of $Y_i$ given $X_i=x_i$ is a Bernoulli distribution with mean $\\mu_i = \\mathbb{E}(Y_i \\mid X_i)$. The probability mass function (PMF) for a single observation $y_i$ is:\n$$ P(Y_i = y_i \\mid X_i=x_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i} $$\nThe model uses a logit link function, which connects the mean $\\mu_i$ to a linear predictor $\\eta_i$:\n$$ \\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) = \\beta_0 + \\beta_1 x_i $$\nwhere $\\beta = (\\beta_0, \\beta_1)^\\top$ is the vector of parameters.\n\nTo express the likelihood in terms of $\\beta$, we must first express $\\mu_i$ as a function of $\\eta_i$. Inverting the logit link function gives:\n$$ \\frac{\\mu_i}{1-\\mu_i} = \\exp(\\eta_i) \\implies \\mu_i = \\exp(\\eta_i) - \\mu_i \\exp(\\eta_i) \\implies \\mu_i(1+\\exp(\\eta_i)) = \\exp(\\eta_i) $$\n$$ \\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)} $$\nAlso, we find the expression for $1-\\mu_i$:\n$$ 1 - \\mu_i = 1 - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1+\\exp(\\eta_i)-\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(\\eta_i)} $$\n\nWith $\\mu_i$ defined in terms of $\\eta_i = \\beta_0 + \\beta_1 x_i$, we can now derive the required quantities. The observed data are denoted by $\\{(y_i, x_i)\\}_{i=1}^n$.\n\n**1. The Log-Likelihood Function $\\ell(\\beta)$**\n\nDue to the independence of subjects, the total likelihood $L(\\beta)$ is the product of the individual probabilities:\n$$ L(\\beta) = \\prod_{i=1}^n P(Y_i=y_i \\mid X_i=x_i) = \\prod_{i=1}^n \\mu_i^{y_i} (1-\\mu_i)^{1-y_i} $$\nThe log-likelihood $\\ell(\\beta)$ is the natural logarithm of $L(\\beta)$:\n$$ \\ell(\\beta) = \\ln(L(\\beta)) = \\sum_{i=1}^n \\ln(\\mu_i^{y_i} (1-\\mu_i)^{1-y_i}) = \\sum_{i=1}^n \\left[ y_i \\ln(\\mu_i) + (1-y_i)\\ln(1-\\mu_i) \\right] $$\nUsing $\\ln(\\mu_i) = \\ln(\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i)) = \\eta_i - \\ln(1+\\exp(\\eta_i))$ and $\\ln(1-\\mu_i) = -\\ln(1+\\exp(\\eta_i))$, we substitute these into the expression for $\\ell(\\beta)$:\n$$ \\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i (\\eta_i - \\ln(1+\\exp(\\eta_i))) + (1-y_i)(-\\ln(1+\\exp(\\eta_i))) \\right] $$\n$$ \\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i \\eta_i - y_i \\ln(1+\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i)) + y_i \\ln(1+\\exp(\\eta_i)) \\right] $$\n$$ \\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i \\eta_i - \\ln(1+\\exp(\\eta_i)) \\right] $$\nSubstituting $\\eta_i = \\beta_0 + \\beta_1 x_i$:\n$$ \\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_{i}(\\beta_{0} + \\beta_{1}x_{i}) - \\ln(1 + \\exp(\\beta_{0} + \\beta_{1}x_{i})) \\right] $$\n\n**2. The Gradient (Score) Vector $\\nabla \\ell(\\beta)$**\n\nThe gradient vector contains the partial derivatives of $\\ell(\\beta)$ with respect to $\\beta_0$ and $\\beta_1$. Let's compute the partial derivative with respect to a generic parameter $\\beta_j$, where $j \\in \\{0, 1\\}$.\nUsing the chain rule, $\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$, where $\\ell_i$ is the $i$-th term in the log-likelihood sum.\n$$ \\frac{\\partial \\ell_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left[ y_i \\eta_i - \\ln(1+\\exp(\\eta_i)) \\right] = y_i - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = y_i - \\mu_i $$\nThe derivatives of the linear predictor are $\\frac{\\partial \\eta_i}{\\partial \\beta_0} = 1$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_1} = x_i$.\n\nFor $\\beta_0$:\n$$ \\frac{\\partial \\ell}{\\partial \\beta_0} = \\sum_{i=1}^n (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_0} = \\sum_{i=1}^n (y_i - \\mu_i) \\cdot 1 = \\sum_{i=1}^{n} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) $$\nFor $\\beta_1$:\n$$ \\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{i=1}^n (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_1} = \\sum_{i=1}^n (y_i - \\mu_i) x_i = \\sum_{i=1}^{n} x_{i} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) $$\nThe gradient vector is:\n$$ \\nabla \\ell(\\beta) = \\begin{pmatrix} \\frac{\\partial \\ell}{\\partial \\beta_0} \\\\ \\frac{\\partial \\ell}{\\partial \\beta_1} \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\\\ \\sum_{i=1}^{n} x_{i} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\end{pmatrix} $$\n\n**3. The Hessian Matrix $\\nabla^2 \\ell(\\beta)$**\n\nThe Hessian matrix consists of the second-order partial derivatives. We compute $\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}$ for $j,k \\in \\{0, 1\\}$.\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\frac{\\partial \\ell}{\\partial \\beta_j} \\right) = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^n (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\sum_{i=1}^n \\left( -\\frac{\\partial \\mu_i}{\\partial \\beta_k} \\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j} $$\nWe need the derivative of $\\mu_i$ with respect to $\\beta_k$:\n$$ \\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} $$\nThe derivative $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$ is:\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i}\\left(\\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\\right) = \\frac{\\exp(\\eta_i)(1+\\exp(\\eta_i)) - \\exp(\\eta_i)\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} = \\frac{\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} $$\nThis can be written as $\\mu_i (1-\\mu_i)$. Thus, $\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\mu_i(1-\\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}$.\nSubstituting this into the second derivative expression:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = - \\sum_{i=1}^n \\left(\\mu_i (1-\\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}\\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j} = - \\sum_{i=1}^n \\mu_i (1-\\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\frac{\\partial \\eta_i}{\\partial \\beta_k} $$\nLet $x_{i0}=1$ and $x_{i1}=x_i$, so $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}$.\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = - \\sum_{i=1}^n x_{ij} x_{ik} \\mu_i(1-\\mu_i) $$\nwhere $\\mu_i(1-\\mu_i) = \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}}$.\n\nThe four elements of the Hessian matrix are:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_0^2} = - \\sum_{i=1}^n (1)^2 \\mu_i(1-\\mu_i) = -\\sum_{i=1}^{n} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_0 \\partial \\beta_1} = \\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_0} = - \\sum_{i=1}^n (1)(x_i) \\mu_i(1-\\mu_i) = -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_1^2} = - \\sum_{i=1}^n (x_i)^2 \\mu_i(1-\\mu_i) = -\\sum_{i=1}^{n} x_{i}^{2} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} $$\nThe Hessian matrix is therefore:\n$$ \\nabla^2 \\ell(\\beta) = \\begin{pmatrix} -\\sum_{i=1}^{n} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} & -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\\\ -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} & -\\sum_{i=1}^{n} x_{i}^{2} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\end{pmatrix} $$", "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\ell(\\beta) &= \\sum_{i=1}^{n} \\left[ y_{i}(\\beta_{0} + \\beta_{1}x_{i}) - \\ln(1 + \\exp(\\beta_{0} + \\beta_{1}x_{i})) \\right] \\\\\n\\nabla \\ell(\\beta) &= \\begin{pmatrix} \\sum_{i=1}^{n} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\\\ \\sum_{i=1}^{n} x_{i} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\end{pmatrix} \\\\\n\\nabla^2 \\ell(\\beta) &= \\begin{pmatrix} -\\sum_{i=1}^{n} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} & -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\\\ -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} & -\\sum_{i=1}^{n} x_{i}^{2} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\end{pmatrix}\n\\end{aligned}\n}\n$$", "id": "4914224"}, {"introduction": "After a model is fit, its real value lies in generating meaningful predictions and insights. This exercise transitions from theory to application by using the results of a fitted Poisson regression model. You will calculate fitted incidence rates and expected event counts, providing a concrete example of how to interpret model coefficients and correctly handle offsets, such as exposure time in epidemiological studies. [@problem_id:4914209]", "problem": "A cohort study in biostatistics measures incident counts of a condition across three sites, with exposures recorded in person-years. The counts are modeled using a Poisson Generalized Linear Model (GLM), where the response at site $i$ is a count $Y_i$ assumed to follow a Poisson distribution with mean $E(Y_i)=\\mu_i$. The model uses a natural logarithm link and incorporates exposure through an offset so that the linear predictor is additive in covariates and exposure.\n\nThe covariate vector is $x_i=\\begin{pmatrix}1 & a_i & z_i\\end{pmatrix}^{\\top}$, where $a_i$ is age deviation in decades from a $50$-year baseline and $z_i$ is a binary indicator for treatment. Suppose the GLM has been fit and yielded an estimated coefficient vector $\\hat{\\beta}=\\begin{pmatrix}-3.2 & 0.4 & 0.6\\end{pmatrix}^{\\top}$. For three sites, the observed covariates and exposures are:\n- Site $1$: $x_1=\\begin{pmatrix}1 & 1.0 & 0\\end{pmatrix}^{\\top}$, exposure $t_1=180$ person-years.\n- Site $2$: $x_2=\\begin{pmatrix}1 & -0.5 & 1\\end{pmatrix}^{\\top}$, exposure $t_2=320$ person-years.\n- Site $3$: $x_3=\\begin{pmatrix}1 & 2.0 & 1\\end{pmatrix}^{\\top}$, exposure $t_3=140$ person-years.\n\nStarting from the core definitions of the Poisson distribution and the GLM framework with a logarithm link and exposure offset, derive expressions for the fitted incidence rates $\\hat{\\lambda}_i$ and the expected counts $\\hat{\\mu}_i$ in terms of $x_i$, $t_i$, and $\\hat{\\beta}$. Then compute the numerical values of $\\hat{\\lambda}_i$ and $\\hat{\\mu}_i$ for $i=1,2,3$ using the values above, and verify that the units are consistent for rates and counts. Round all numerical results to four significant figures. Express your final answer as a single row matrix containing $\\hat{\\lambda}_1$, $\\hat{\\lambda}_2$, $\\hat{\\lambda}_3$, $\\hat{\\mu}_1$, $\\hat{\\mu}_2$, $\\hat{\\mu}_3$ in that order. No units should appear in the final matrix.", "solution": "The problem is valid as it is scientifically grounded in standard biostatistical modeling, well-posed with all necessary information, and stated objectively. We can proceed with the solution.\n\nThe problem describes a Poisson Generalized Linear Model (GLM) for count data $Y_i$ at site $i$, where $i \\in \\{1, 2, 3\\}$. The core components of the GLM are as follows:\n$1$. The **random component** specifies the probability distribution of the response variable. Here, the count $Y_i$ is assumed to follow a Poisson distribution with mean $\\mu_i$:\n$$Y_i \\sim \\text{Poisson}(\\mu_i)$$\nThe expected value is $E[Y_i] = \\mu_i$.\n\n$2$. The **systematic component** is the linear predictor $\\eta_i$, which is a linear combination of the covariates $x_i$ and coefficients $\\beta$:\n$$\\eta_i = x_i^{\\top}\\beta$$\n\n$3$. The **link function** $g$ connects the expected value of the response $\\mu_i$ to the linear predictor $\\eta_i$. The problem states a natural logarithm link, $g(\\cdot) = \\ln(\\cdot)$. Additionally, the model incorporates exposure $t_i$ (in person-years) as an offset. In a Poisson GLM, this is formalized by modeling the rate $\\lambda_i = \\mu_i / t_i$. The link function applies to the mean, so the model equation is:\n$$g(\\mu_i) = \\eta_i + \\text{offset}_i$$\nWith a log link and a log-exposure offset, this becomes:\n$$\\ln(\\mu_i) = x_i^{\\top}\\beta + \\ln(t_i)$$\nThis structure is equivalent to modeling the logarithm of the incidence rate $\\lambda_i$ directly. To show this, we substitute $\\mu_i = \\lambda_i t_i$:\n$$\\ln(\\lambda_i t_i) = x_i^{\\top}\\beta + \\ln(t_i)$$\nUsing the property of logarithms, $\\ln(a b) = \\ln(a) + \\ln(b)$, we get:\n$$\\ln(\\lambda_i) + \\ln(t_i) = x_i^{\\top}\\beta + \\ln(t_i)$$\n$$\\ln(\\lambda_i) = x_i^{\\top}\\beta$$\nThis confirms that the model is for the log-incidence rate.\n\nThe problem provides the estimated coefficient vector $\\hat{\\beta}$. We use this to find the fitted values. The fitted linear predictor for site $i$ is $\\hat{\\eta}_i = x_i^{\\top}\\hat{\\beta}$.\n\nThe expression for the fitted incidence rate, $\\hat{\\lambda}_i$, is derived by exponentiating the fitted model for the log-rate:\n$$\\ln(\\hat{\\lambda}_i) = x_i^{\\top}\\hat{\\beta}$$\n$$\\hat{\\lambda}_i = \\exp(x_i^{\\top}\\hat{\\beta})$$\nThe unit of $\\hat{\\lambda}_i$ is events per person-year, determined by the unit of exposure $t_i$.\n\nThe expression for the fitted expected count, $\\hat{\\mu}_i$, is derived from the relationship $\\hat{\\mu}_i = \\hat{\\lambda}_i t_i$:\n$$\\hat{\\mu}_i = t_i \\hat{\\lambda}_i = t_i \\exp(x_i^{\\top}\\hat{\\beta})$$\nThe units are consistent: $\\hat{\\mu}_i$ (a dimensionless count) is the product of $t_i$ (person-years) and $\\hat{\\lambda}_i$ (events per person-year).\n\nThe given data are:\n- Estimated coefficient vector: $\\hat{\\beta}=\\begin{pmatrix}-3.2 & 0.4 & 0.6\\end{pmatrix}^{\\top}$.\n- Site $1$: $x_1=\\begin{pmatrix}1 & 1.0 & 0\\end{pmatrix}^{\\top}$, $t_1=180$ person-years.\n- Site $2$: $x_2=\\begin{pmatrix}1 & -0.5 & 1\\end{pmatrix}^{\\top}$, $t_2=320$ person-years.\n- Site $3$: $x_3=\\begin{pmatrix}1 & 2.0 & 1\\end{pmatrix}^{\\top}$, $t_3=140$ person-years.\n\nWe now compute the numerical values for each site.\n\n**For Site 1:**\nThe linear predictor is:\n$$\\hat{\\eta}_1 = x_1^{\\top}\\hat{\\beta} = (1)(-3.2) + (1.0)(0.4) + (0)(0.6) = -3.2 + 0.4 = -2.8$$\nThe fitted incidence rate is:\n$$\\hat{\\lambda}_1 = \\exp(-2.8) \\approx 0.06081006$$\nRounding to four significant figures, $\\hat{\\lambda}_1 = 0.06081$.\nThe fitted expected count is:\n$$\\hat{\\mu}_1 = t_1 \\hat{\\lambda}_1 = 180 \\times \\exp(-2.8) \\approx 10.94581$$\nRounding to four significant figures, $\\hat{\\mu}_1 = 10.95$.\n\n**For Site 2:**\nThe linear predictor is:\n$$\\hat{\\eta}_2 = x_2^{\\top}\\hat{\\beta} = (1)(-3.2) + (-0.5)(0.4) + (1)(0.6) = -3.2 - 0.2 + 0.6 = -2.8$$\nThe fitted incidence rate is:\n$$\\hat{\\lambda}_2 = \\exp(-2.8) \\approx 0.06081006$$\nRounding to four significant figures, $\\hat{\\lambda}_2 = 0.06081$.\nThe fitted expected count is:\n$$\\hat{\\mu}_2 = t_2 \\hat{\\lambda}_2 = 320 \\times \\exp(-2.8) \\approx 19.45922$$\nRounding to four significant figures, $\\hat{\\mu}_2 = 19.46$.\n\n**For Site 3:**\nThe linear predictor is:\n$$\\hat{\\eta}_3 = x_3^{\\top}\\hat{\\beta} = (1)(-3.2) + (2.0)(0.4) + (1)(0.6) = -3.2 + 0.8 + 0.6 = -1.8$$\nThe fitted incidence rate is:\n$$\\hat{\\lambda}_3 = \\exp(-1.8) \\approx 0.16529888$$\nRounding to four significant figures, $\\hat{\\lambda}_3 = 0.1653$.\nThe fitted expected count is:\n$$\\hat{\\mu}_3 = t_3 \\hat{\\lambda}_3 = 140 \\times \\exp(-1.8) \\approx 23.14184$$\nRounding to four significant figures, $\\hat{\\mu}_3 = 23.14$.\n\nThe final results, rounded to four significant figures, are:\n$\\hat{\\lambda}_1 = 0.06081$\n$\\hat{\\lambda}_2 = 0.06081$\n$\\hat{\\lambda}_3 = 0.1653$\n$\\hat{\\mu}_1 = 10.95$\n$\\hat{\\mu}_2 = 19.46$\n$\\hat{\\mu}_3 = 23.14$\n\nThese are consolidated into a single row matrix as requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.06081 & 0.06081 & 0.1653 & 10.95 & 19.46 & 23.14\n\\end{pmatrix}\n}\n$$", "id": "4914209"}, {"introduction": "A key part of statistical modeling is not just providing a point estimate but also quantifying its uncertainty. This advanced practice delves into the construction of confidence intervals, a critical task where standard methods can sometimes fail. You will derive and compare the commonly used Wald interval with the more robust score-based (Wilson) interval, using both mathematical theory and computational simulation to explore their performance and understand why superior methods are often necessary, especially in studies with small sample sizes or rare events. [@problem_id:4914180]", "problem": "Consider a binomial generalized linear model with canonical logistic link, focused on an intercept-only model. Let $Y \\sim \\operatorname{Binomial}(n,p)$ be the count of successes in $n$ independent Bernoulli trials with success probability $p \\in (0,1)$. In the generalized linear model framework with logistic link, the canonical parameter is $\\theta = \\log\\left(\\frac{p}{1-p}\\right)$, and the inverse link yields $p = \\frac{e^\\theta}{1+e^\\theta}$. The maximum likelihood estimate on the probability scale is $\\hat{p} = \\frac{Y}{n}$.\n\nFrom the foundation of likelihood-based estimation and asymptotic theory, the Wald confidence interval arises by invoking the approximate normality of the maximum likelihood estimate: for large $n$, $\\hat{p}$ is approximately normal with mean $p$ and variance $p(1-p)/n$, or equivalently the canonical parameter estimate $\\hat{\\theta}$ is approximately normal with variance given by the inverse of the Fisher information. In small samples and for extreme probabilities near $0$ or $1$, Wald intervals are known to have poor coverage and can degenerate or extend beyond the parameter space. An alternative approach constructs score-based intervals by inverting the score test, yielding the Wilson interval on the probability scale.\n\nStarting strictly from the binomial model and generalized linear model definitions, and without using any pre-derived confidence interval formulas, derive the following:\n\n1. Using the asymptotic distribution of the maximum likelihood estimate and the Fisher information, derive the Wald interval for $p$ at level $1-\\alpha$ as an approximation grounded in first principles. Explicitly state the assumptions required and the path from the Fisher information to an interval for $p$.\n\n2. Derive the score-based interval for $p$ at level $1-\\alpha$ by inverting the score test. That is, define the score function for the binomial model under the logistic link, construct the standardized score test for the null value $p$ treated as a candidate parameter, and show that inverting the acceptance region yields a quadratic inequality in $p$. Solve this inequality to obtain the Wilson interval endpoints in terms of $\\hat{p}$, $n$, and the standard normal quantile $z_{1-\\alpha/2}$.\n\nWith these derivations, implement a program that, for each test case $(n,p,\\alpha)$, computes the following five quantities on the probability scale:\n\n- The exact coverage probability of the Wald interval at nominal level $1-\\alpha$, defined as $\\sum_{y=0}^{n} \\mathbf{1}\\{p \\in [L_{\\text{Wald}}(y),U_{\\text{Wald}}(y)]\\} \\cdot \\Pr(Y=y)$, where $L_{\\text{Wald}}(y)$ and $U_{\\text{Wald}}(y)$ are the Wald endpoints computed from $\\hat{p}=y/n$, and $\\Pr(Y=y)$ is the binomial probability mass function with the true $p$.\n- The exact coverage probability of the score-based (Wilson) interval at the same level, defined analogously with $L_{\\text{Score}}(y)$ and $U_{\\text{Score}}(y)$ as the Wilson endpoints computed from $\\hat{p}=y/n$.\n- The expected interval length of the Wald interval, computed as $\\sum_{y=0}^{n} \\left(U_{\\text{Wald}}(y)-L_{\\text{Wald}}(y)\\right)\\Pr(Y=y)$.\n- The expected interval length of the score-based interval, computed as $\\sum_{y=0}^{n} \\left(U_{\\text{Score}}(y)-L_{\\text{Score}}(y)\\right)\\Pr(Y=y)$.\n- The probability of a degenerate Wald interval (zero length) caused by $\\hat{p} \\in \\{0,1\\}$, computed as $\\Pr(Y=0)+\\Pr(Y=n)$.\n\nFor numerical stability and scientific realism, truncate all interval endpoints to the parameter space $[0,1]$ after computation. Express all coverage probabilities and expected lengths as decimals. No percentage signs are permitted.\n\nYour program must compute these five quantities for each of the following test cases, which are designed to probe typical performance, small-sample behavior, and extreme-probability edge cases:\n\n- Test case $1$: $(n,p,\\alpha) = (10, 0.5, 0.05)$.\n- Test case $2$: $(n,p,\\alpha) = (10, 0.05, 0.05)$.\n- Test case $3$: $(n,p,\\alpha) = (5, 0.1, 0.05)$.\n- Test case $4$: $(n,p,\\alpha) = (20, 0.95, 0.05)$.\n- Test case $5$: $(n,p,\\alpha) = (50, 0.5, 0.10)$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a bracketed comma-separated list in the order $[$coverage$_{\\text{Wald}}$, coverage$_{\\text{Score}}$, mean\\_length$_{\\text{Wald}}$, mean\\_length$_{\\text{Score}}$, degenerate\\_wald\\_fraction$]$. For example: $[[a_1,b_1,c_1,d_1,e_1],[a_2,b_2,c_2,d_2,e_2],\\dots]$. Round every reported decimal to $4$ places before output.", "solution": "We begin from the binomial generalized linear model with canonical logistic link. Let $Y \\sim \\operatorname{Binomial}(n,p)$, with probability mass function $\\Pr(Y=y) = \\binom{n}{y} p^y (1-p)^{n-y}$ for $y \\in \\{0,1,\\dots,n\\}$. In the generalized linear model framework, the canonical (natural) parameter is $\\theta = \\log\\left(\\frac{p}{1-p}\\right)$ and the inverse link recovers $p = \\frac{e^\\theta}{1+e^\\theta}$.\n\nThe log-likelihood for the canonical parameter, up to an additive constant, is\n$$\n\\ell(\\theta; y) = y \\theta - n \\log\\left(1 + e^\\theta\\right).\n$$\nThe score function is the derivative of the log-likelihood with respect to $\\theta$,\n$$\nU(\\theta) = \\frac{\\partial \\ell(\\theta;y)}{\\partial \\theta} = y - n \\frac{e^\\theta}{1 + e^\\theta} = y - n p(\\theta),\n$$\nwhere $p(\\theta) = \\frac{e^\\theta}{1 + e^\\theta}$. The Fisher information for $\\theta$ is\n$$\nI(\\theta) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\theta;Y)}{\\partial \\theta^2}\\right] = n \\frac{e^\\theta}{\\left(1 + e^\\theta\\right)^2} = n p(\\theta)\\left(1 - p(\\theta)\\right).\n$$\n\nWald interval derivation. The maximum likelihood estimate on the probability scale is $\\hat{p} = y/n$, and the delta method or direct computation yields that for large $n$, $\\hat{p}$ is approximately normal:\n$$\n\\hat{p} \\approx \\mathcal{N}\\left(p, \\frac{p(1-p)}{n}\\right).\n$$\nReplacing the unknown variance by its plug-in estimate $\\hat{p}(1-\\hat{p})/n$ gives the Wald confidence interval on the probability scale\n$$\n\\left[\\,\\hat{p} - z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\,,\\, \\hat{p} + z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\,\\right],\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution. This approximation relies on the large-sample normality of $\\hat{p}$ and the validity of replacing $p$ by $\\hat{p}$ in the standard error. For small $n$ or extreme $p$, this can be inaccurate; the interval can also extend outside $[0,1]$ or degenerate when $\\hat{p} \\in \\{0,1\\}$.\n\nScore-based interval derivation via inversion of the score test. The score test for a candidate parameter value $p$ evaluates the standardized score statistic\n$$\nZ(p) = \\frac{U(\\theta(p))}{\\sqrt{I(\\theta(p))}} = \\frac{y - n p}{\\sqrt{n p(1-p)}}.\n$$\nUnder the null hypothesis at the candidate $p$, $Z(p)$ is approximately standard normal for moderate sample sizes. The acceptance region for a two-sided level $1-\\alpha$ test is\n$$\n\\left|\\,\\frac{y - n p}{\\sqrt{n p(1-p)}}\\,\\right| \\le z_{1-\\alpha/2}.\n$$\nTo obtain a confidence interval, we invert this test: for fixed data $y$, we find all $p \\in [0,1]$ such that the inequality holds. Squaring both sides yields\n$$\n\\frac{(y - n p)^2}{n p(1-p)} \\le z_{1-\\alpha/2}^2.\n$$\nMultiplying both sides by $n p(1-p)$ and rearranging gives a quadratic inequality in $p$:\n$$\n(y - n p)^2 \\le z_{1-\\alpha/2}^2 \\, n \\, p(1-p).\n$$\nLet $\\hat{p} = y/n$ and $z = z_{1-\\alpha/2}$. Divide by $n$ to obtain\n$$\nn (p - \\hat{p})^2 \\le z^2 p(1-p).\n$$\nExpanding and rearranging, we have\n$$\nn(p^2 - 2 p \\hat{p} + \\hat{p}^2) \\le z^2(p - p^2),\n$$\nwhich can be written as\n$$\n(n + z^2) p^2 - 2 n \\hat{p} \\, p + n \\hat{p}^2 - z^2 p \\le 0,\n$$\nor equivalently\n$$\n(n + z^2) p^2 - 2 n \\hat{p} \\, p - z^2 p + n \\hat{p}^2 \\le 0.\n$$\nGrouping terms in $p$ gives the quadratic inequality\n$$\n(n + z^2) p^2 - \\left(2 n \\hat{p} + z^2\\right) p + n \\hat{p}^2 \\le 0.\n$$\nSolving the corresponding quadratic equality for $p$ yields the endpoints of the interval. Using the quadratic formula, the roots are\n$$\np = \\frac{2 n \\hat{p} + z^2 \\pm z \\sqrt{4 n \\hat{p}(1-\\hat{p}) + z^2}}{2(n + z^2)}.\n$$\nDividing numerator and denominator appropriately simplifies to the Wilson interval form. A commonly presented equivalent form expresses the center and radius as\n$$\n\\text{center} = \\frac{\\hat{p} + \\frac{z^2}{2n}}{1 + \\frac{z^2}{n}}, \\quad\n\\text{radius} = \\frac{z}{1 + \\frac{z^2}{n}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z^2}{4 n^2}},\n$$\nso the score-based (Wilson) interval is\n$$\n\\left[\\,\\max\\left\\{0,\\, \\text{center} - \\text{radius}\\right\\},\\, \\min\\left\\{1,\\, \\text{center} + \\text{radius}\\right\\}\\,\\right].\n$$\nThe truncation to $[0,1]$ enforces the parameter space and is natural for probability parameters; the Wilson interval endpoints already lie within $[0,1]$ in most cases.\n\nExact coverage and expected length computation. For fixed $(n,p,\\alpha)$, define the binomial probability mass function weights $w_y = \\Pr(Y=y) = \\binom{n}{y} p^y (1-p)^{n-y}$ for $y=0,1,\\dots,n$. For each $y$, compute $\\hat{p}=y/n$, the Wald interval endpoints\n$$\nL_{\\text{Wald}}(y) = \\max\\left\\{0,\\, \\hat{p} - z \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right\\}, \\quad\nU_{\\text{Wald}}(y) = \\min\\left\\{1,\\, \\hat{p} + z \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right\\},\n$$\nand the Wilson endpoints as above. The exact coverage probabilities are\n$$\n\\text{Coverage}_{\\text{Wald}} = \\sum_{y=0}^{n} \\mathbf{1}\\left\\{p \\in [L_{\\text{Wald}}(y),U_{\\text{Wald}}(y)]\\right\\} \\, w_y,\n$$\n$$\n\\text{Coverage}_{\\text{Score}} = \\sum_{y=0}^{n} \\mathbf{1}\\left\\{p \\in [L_{\\text{Score}}(y),U_{\\text{Score}}(y)]\\right\\} \\, w_y.\n$$\nThe expected lengths are\n$$\n\\mathbb{E}[\\text{length}_{\\text{Wald}}] = \\sum_{y=0}^{n} \\left(U_{\\text{Wald}}(y) - L_{\\text{Wald}}(y)\\right) w_y, \\quad\n\\mathbb{E}[\\text{length}_{\\text{Score}}] = \\sum_{y=0}^{n} \\left(U_{\\text{Score}}(y) - L_{\\text{Score}}(y)\\right) w_y.\n$$\nThe degeneracy probability for the Wald interval is\n$$\n\\Pr(\\text{degenerate Wald}) = \\Pr(Y=0) + \\Pr(Y=n) = (1-p)^n + p^n,\n$$\nbecause when $y=0$ or $y=n$, the plug-in standard error $\\sqrt{\\hat{p}(1-\\hat{p})/n}$ equals $0$.\n\nAlgorithmic design. For each test case $(n,p,\\alpha)$:\n- Compute $z = z_{1-\\alpha/2}$ from the standard normal distribution.\n- For $y$ from $0$ to $n$, compute $\\hat{p} = y/n$, the binomial probability $w_y$, the Wald endpoints with truncation to $[0,1]$, and the Wilson endpoints with truncation to $[0,1]$.\n- Accumulate coverage indicators weighted by $w_y$ to obtain exact coverage probabilities, accumulate lengths to obtain expected lengths, and compute degeneracy probability as $w_0 + w_n$.\n- Round each result to $4$ decimal places for output formatting.\n\nTest suite and output. The program implements the above steps for the five specified test cases: $(10, 0.5, 0.05)$, $(10, 0.05, 0.05)$, $(5, 0.1, 0.05)$, $(20, 0.95, 0.05)$, $(50, 0.5, 0.10)$. The final output is a single line with a bracketed list of bracketed per-test-case results in the order $[$coverage$_{\\text{Wald}}$, coverage$_{\\text{Score}}$, mean\\_length$_{\\text{Wald}}$, mean\\_length$_{\\text{Score}}$, degenerate\\_wald\\_fraction$]$, with each decimal rounded to $4$ places.", "answer": "[[0.8906,0.9785,0.5197,0.5513,0.0020],[0.3585,0.9345,0.1268,0.2016,0.5987],[0.4095,0.9153,0.2458,0.3802,0.5905],[0.5166,0.9525,0.1348,0.1654,0.3585],[0.9363,0.9189,0.2235,0.2272,0.0000]]", "id": "4914180"}]}