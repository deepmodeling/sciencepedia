{"hands_on_practices": [{"introduction": "To truly master a statistical model, it is essential to look \"under the hood\" at how its parameters are estimated. This exercise demystifies the powerful Newton-Raphson algorithm, the engine behind many maximum likelihood estimation procedures, by walking you through a single, concrete update step for a multinomial logistic regression model. By calculating the core components like working residuals and weight matrices by hand for a small dataset, you will gain a tangible understanding of how the algorithm uses information from the data to iteratively search for the best-fitting parameters [@problem_id:4929771].", "problem": "Consider a baseline-category multinomial logistic regression model with $K=3$ outcome categories and $p=2$ predictors applied to a toy biostatistics dataset. Let category $3$ be the baseline. For observation $i$ with predictor vector $\\mathbf{x}_i \\in \\mathbb{R}^2$, define the linear predictors for categories $k=1,2$ as $\\eta_{ik} = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}_k$, where $\\boldsymbol{\\beta}_k \\in \\mathbb{R}^2$, and the category probabilities by\n$$\np_{ik} = \\frac{\\exp(\\eta_{ik})}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}, \\quad k=1,2, \\quad \\text{and} \\quad p_{i3} = \\frac{1}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}.\n$$\nLet $y_{ik} \\in \\{0,1\\}$ denote the indicator of category $k$ for observation $i$ for $k=1,2,3$, with $\\sum_{k=1}^{3} y_{ik} = 1$. The log-likelihood for $n$ observations is\n$$\n\\ell(\\boldsymbol{\\beta}_1,\\boldsymbol{\\beta}_2) = \\sum_{i=1}^{n} \\sum_{k=1}^{3} y_{ik} \\ln(p_{ik}).\n$$\nStarting from the definitions of maximum likelihood and the above model, derive the score and the negative Hessian needed for a one-step Newton–Raphson (NR) or Iteratively Reweighted Least Squares (IRLS) update. Then, implement this one-step update at the initial parameter values $\\boldsymbol{\\beta}_1 = (0,0)^{\\top}$ and $\\boldsymbol{\\beta}_2 = (0,0)^{\\top}$ for the following toy dataset with $n=3$ observations:\n- Predictors: $\\mathbf{x}_1 = (1,0)^{\\top}$, $\\mathbf{x}_2 = (0,1)^{\\top}$, $\\mathbf{x}_3 = (1,1)^{\\top}$.\n- Observed categories: $y_1$ is category $1$, $y_2$ is category $2$, $y_3$ is category $3$.\n\nExplicitly compute, at the initial parameters:\n- The working residuals $r_i = y_i^{*} - p_i$ for each observation $i$, where $y_i^{*} = (y_{i1}, y_{i2})^{\\top}$ and $p_i = (p_{i1}, p_{i2})^{\\top}$.\n- The weight matrices $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$ for each observation $i$.\nUsing these, assemble the block negative Hessian and score, solve for the Newton step $\\Delta = (\\Delta\\boldsymbol{\\beta}_1^{\\top}, \\Delta\\boldsymbol{\\beta}_2^{\\top})^{\\top} \\in \\mathbb{R}^{4}$ satisfying $H \\Delta = U$, and report the four components of $\\Delta$ exactly. No rounding is required. Provide your final answer as a single $1 \\times 4$ row matrix.", "solution": "The user wants me to solve a biostatistics problem involving multinomial logistic regression.\n\nHere's my plan:\n\n1.  **Validate the Problem:**\n    *   **Step 1: Extract Givens:** I will list all the formulas, definitions, data points, and initial conditions provided.\n        *   Model: Baseline-category multinomial logistic regression.\n        *   Number of categories: $K=3$.\n        *   Number of predictors: $p=2$.\n        *   Baseline category: Category $3$.\n        *   Linear predictors: $\\eta_{ik} = x_i^{\\top}\\beta_k$ for $k=1,2$.\n        *   Probabilities:\n            *   $p_{ik} = \\frac{\\exp(\\eta_{ik})}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}$ for $k=1,2$.\n            *   $p_{i3} = \\frac{1}{1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})}$.\n        *   Outcome indicators: $y_{ik} \\in \\{0,1\\}$, $\\sum_{k=1}^{3} y_{ik} = 1$.\n        *   Log-likelihood: $\\ell(\\beta_1,\\beta_2) = \\sum_{i=1}^{n} \\sum_{k=1}^{3} y_{ik} \\ln(p_{ik})$.\n        *   Task: Derive score and negative Hessian, then perform a one-step Newton-Raphson update.\n        *   Initial parameters: $\\beta_1 = (0,0)^{\\top}$, $\\beta_2 = (0,0)^{\\top}$.\n        *   Dataset ($n=3$):\n            *   Predictors: $x_1 = (1,0)^{\\top}$, $x_2 = (0,1)^{\\top}$, $x_3 = (1,1)^{\\top}$.\n            *   Outcomes: $y_1$ is cat 1, $y_2$ is cat 2, $y_3$ is cat 3. This means:\n                *   For $i=1$: $y_{11}=1, y_{12}=0, y_{13}=0$.\n                *   For $i=2$: $y_{21}=0, y_{22}=1, y_{23}=0$.\n                *   For $i=3$: $y_{31}=0, y_{32}=0, y_{33}=1$.\n        *   Specific quantities to compute at initial parameters:\n            *   Working residuals $r_i = y_i^{*} - p_i$, where $y_i^{*} = (y_{i1}, y_{i2})^{\\top}$ and $p_i = (p_{i1}, p_{i2})^{\\top}$.\n            *   Weight matrices $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$.\n        *   Goal: Solve $H \\Delta = U$ for $\\Delta = (\\Delta\\beta_1^{\\top}, \\Delta\\beta_2^{\\top})^{\\top} \\in \\mathbb{R}^{4}$ and report the four components of $\\Delta$.\n\n    *   **Step 2: Validate Using Extracted Givens:**\n        *   **Scientifically Grounded:** Yes, multinomial logistic regression is a standard statistical model. The formulas for probabilities, log-likelihood, score, and Hessian are well-established in generalized linear models theory. The task is a standard application of the Newton-Raphson algorithm for finding maximum likelihood estimates.\n        *   **Well-Posed:** Yes, the problem provides all necessary data and initial conditions to perform one step of the Newton-Raphson algorithm. The existence and uniqueness of the solution for this one step are guaranteed as long as the Hessian is invertible. I'll need to check that.\n        *   **Objective:** Yes, the language is precise, and the quantities to be computed are clearly defined.\n        *   No flaws detected. The problem is valid.\n\n    *   **Step 3: Verdict and Action:** The problem is valid. I will proceed with the solution.\n\n2.  **Solve the Problem:**\n\n    *   **Part 1: General Derivations (Score and Hessian)**\n        *   Let $\\boldsymbol{\\beta} = (\\beta_1^{\\top}, \\beta_2^{\\top})^{\\top}$ be the vector of all parameters. It's a $4 \\times 1$ vector.\n        *   The log-likelihood can be rewritten.\n            $$ \\ell = \\sum_{i=1}^n \\left( y_{i1} \\eta_{i1} + y_{i2} \\eta_{i2} - \\ln(1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})) \\right) $$\n            This comes from substituting the expressions for $p_{ik}$ into the log-likelihood:\n            $$ \\sum_{k=1}^3 y_{ik} \\ln(p_{ik}) = y_{i1} (\\eta_{i1} - D_i) + y_{i2} (\\eta_{i2} - D_i) + y_{i3} (0 - D_i) $$\n            where $D_i = \\ln(1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2}))$.\n            Using $\\sum_k y_{ik} = 1$, this simplifies to:\n            $$ y_{i1} \\eta_{i1} + y_{i2} \\eta_{i2} - (y_{i1} + y_{i2} + y_{i3})D_i = y_{i1} \\eta_{i1} + y_{i2} \\eta_{i2} - D_i $$\n            This matches my rewritten form.\n        *   Now, I need to find the score vector $U$, which is the gradient of $\\ell$ with respect to $\\boldsymbol{\\beta}$. The score has two block components, $U_1 = \\frac{\\partial \\ell}{\\partial \\beta_1}$ and $U_2 = \\frac{\\partial \\ell}{\\partial \\beta_2}$.\n        *   Let's find the derivatives of $\\ell$ with respect to $\\beta_{1j}$ and $\\beta_{2j}$ (the $j$-th components of $\\beta_1$ and $\\beta_2$).\n        *   We have $\\eta_{i1} = x_i^{\\top}\\beta_1$ and $\\eta_{i2} = x_i^{\\top}\\beta_2$.\n        *   $\\frac{\\partial \\eta_{i1}}{\\partial \\beta_1} = x_i$ and $\\frac{\\partial \\eta_{i1}}{\\partial \\beta_2} = 0$.\n        *   $\\frac{\\partial \\eta_{i2}}{\\partial \\beta_1} = 0$ and $\\frac{\\partial \\eta_{i2}}{\\partial \\beta_2} = x_i$.\n        *   Also need derivatives of $D_i$:\n            *   $\\frac{\\partial D_i}{\\partial \\eta_{i1}} = \\frac{\\exp(\\eta_{i1})}{1+\\dots} = p_{i1}$.\n            *   $\\frac{\\partial D_i}{\\partial \\eta_{i2}} = \\frac{\\exp(\\eta_{i2})}{1+\\dots} = p_{i2}$.\n        *   Now for the score components:\n            *   $\\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{i=1}^n \\left( y_{i1} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_1} - \\frac{\\partial D_i}{\\partial \\eta_{i1}}\\frac{\\partial \\eta_{i1}}{\\partial \\beta_1} - \\frac{\\partial D_i}{\\partial \\eta_{i2}}\\frac{\\partial \\eta_{i2}}{\\partial \\beta_1} \\right) = \\sum_{i=1}^n (y_{i1} x_i - p_{i1} x_i - p_{i2} \\cdot 0) = \\sum_{i=1}^n (y_{i1} - p_{i1})x_i$.\n            *   $\\frac{\\partial \\ell}{\\partial \\beta_2} = \\sum_{i=1}^n \\left( y_{i2} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_2} - \\frac{\\partial D_i}{\\partial \\eta_{i1}}\\frac{\\partial \\eta_{i1}}{\\partial \\beta_2} - \\frac{\\partial D_i}{\\partial \\eta_{i2}}\\frac{\\partial \\eta_{i2}}{\\partial \\beta_2} \\right) = \\sum_{i=1}^n (y_{i2} x_i - p_{i1} \\cdot 0 - p_{i2} x_i) = \\sum_{i=1}^n (y_{i2} - p_{i2})x_i$.\n        *   So the score vector $U$ is $U = \\begin{pmatrix} U_1 \\\\ U_2 \\end{pmatrix} = \\begin{pmatrix} \\sum_i (y_{i1}-p_{i1})x_i \\\\ \\sum_i (y_{i2}-p_{i2})x_i \\end{pmatrix}$. This can be written in matrix form. Let $X$ be the design matrix ($n \\times p$), but here it's more complex. Let's use the given notation with $r_i = y_i^* - p_i$. Then $U = \\sum_i \\begin{pmatrix} r_{i1} x_i \\\\ r_{i2} x_i \\end{pmatrix}$. This can be written as $\\sum_i (x_i \\otimes r_i)$, where $\\otimes$ is the Kronecker product.\n        *   More concisely, if we define a design matrix for each observation $X_i = \\operatorname{diag}(x_i, x_i) = I_2 \\otimes x_i$, which is a $2p \\times 2$ matrix (here $4 \\times 2$), then $U = \\sum_i X_i^{\\top} r_i$ is not quite right because matrix dimensions don't match. $X_i$ should probably be $I_{K-1} \\otimes x_i^{\\top}$. Let's stick to the summation form. $U = \\sum_i \\begin{pmatrix} x_i(y_{i1}-p_{i1}) \\\\ x_i(y_{i2}-p_{i2}) \\end{pmatrix}$.\n\n        *   Next, the Hessian matrix $\\mathcal{H}$. It's the matrix of second derivatives. The negative Hessian is the Fisher Information matrix, $H = -\\mathcal{H}$.\n        *   $\\mathcal{H} = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_1^{\\top}}  \\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}} \\\\ \\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_1^{\\top}}  \\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_2^{\\top}} \\end{pmatrix}$.\n        *   Let's find the derivatives of the score components.\n            *   $\\frac{\\partial}{\\partial \\beta_1^{\\top}} (\\sum_i (y_{i1} - p_{i1})x_i) = -\\sum_i x_i \\frac{\\partial p_{i1}}{\\partial \\beta_1^{\\top}}$.\n            *   We need $\\frac{\\partial p_{i1}}{\\partial \\beta_1^{\\top}}$. $p_{i1} = \\exp(\\eta_{i1}) / D_i'$, where $D_i' = 1 + \\exp(\\eta_{i1}) + \\exp(\\eta_{i2})$.\n            *   $\\frac{\\partial p_{i1}}{\\partial \\eta_{i1}} = \\frac{\\exp(\\eta_{i1})D_i' - \\exp(\\eta_{i1})\\exp(\\eta_{i1})}{(D_i')^2} = p_{i1}(1-p_{i1})$.\n            *   $\\frac{\\partial p_{i1}}{\\partial \\eta_{i2}} = \\frac{-\\exp(\\eta_{i1})\\exp(\\eta_{i2})}{(D_i')^2} = -p_{i1}p_{i2}$.\n            *   So, $\\frac{\\partial p_{i1}}{\\partial \\beta_1^{\\top}} = \\frac{\\partial p_{i1}}{\\partial \\eta_{i1}} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_1^{\\top}} + \\frac{\\partial p_{i1}}{\\partial \\eta_{i2}} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_1^{\\top}} = p_{i1}(1-p_{i1}) x_i^{\\top}$.\n            *   $\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_1^{\\top}} = -\\sum_i x_i (p_{i1}(1-p_{i1})) x_i^{\\top} = -\\sum_i p_{i1}(1-p_{i1}) x_i x_i^{\\top}$.\n            *   Now for the off-diagonal block: $\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}} = \\frac{\\partial}{\\partial \\beta_2^{\\top}} (\\sum_i (y_{i1} - p_{i1})x_i) = -\\sum_i x_i \\frac{\\partial p_{i1}}{\\partial \\beta_2^{\\top}}$.\n            *   $\\frac{\\partial p_{i1}}{\\partial \\beta_2^{\\top}} = \\frac{\\partial p_{i1}}{\\partial \\eta_{i1}} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_2^{\\top}} + \\frac{\\partial p_{i1}}{\\partial \\eta_{i2}} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_2^{\\top}} = -p_{i1}p_{i2} x_i^{\\top}$.\n            *   $\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}} = -\\sum_i x_i (-p_{i1}p_{i2}) x_i^{\\top} = \\sum_i p_{i1}p_{i2} x_i x_i^{\\top}$.\n            *   By symmetry, $\\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_1^{\\top}} = (\\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_2^{\\top}})^{\\top} = \\sum_i p_{i1}p_{i2} x_i x_i^{\\top}$.\n            *   Finally, the last block: $\\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_2^{\\top}} = \\frac{\\partial}{\\partial \\beta_2^{\\top}} (\\sum_i (y_{i2} - p_{i2})x_i) = -\\sum_i x_i \\frac{\\partial p_{i2}}{\\partial \\beta_2^{\\top}}$.\n            *   $\\frac{\\partial p_{i2}}{\\partial \\eta_{i1}} = -p_{i1}p_{i2}$.\n            *   $\\frac{\\partial p_{i2}}{\\partial \\eta_{i2}} = p_{i2}(1-p_{i2})$.\n            *   $\\frac{\\partial p_{i2}}{\\partial \\beta_2^{\\top}} = \\frac{\\partial p_{i2}}{\\partial \\eta_{i1}} \\frac{\\partial \\eta_{i1}}{\\partial \\beta_2^{\\top}} + \\frac{\\partial p_{i2}}{\\partial \\eta_{i2}} \\frac{\\partial \\eta_{i2}}{\\partial \\beta_2^{\\top}} = p_{i2}(1-p_{i2}) x_i^{\\top}$.\n            *   $\\frac{\\partial^2 \\ell}{\\partial \\beta_2 \\partial \\beta_2^{\\top}} = -\\sum_i x_i (p_{i2}(1-p_{i2})) x_i^{\\top} = -\\sum_i p_{i2}(1-p_{i2}) x_i x_i^{\\top}$.\n        *   So the Hessian is $\\mathcal{H} = \\sum_i \\begin{pmatrix} -p_{i1}(1-p_{i1}) x_i x_i^{\\top}  p_{i1}p_{i2} x_i x_i^{\\top} \\\\ p_{i1}p_{i2} x_i x_i^{\\top}  -p_{i2}(1-p_{i2}) x_i x_i^{\\top} \\end{pmatrix}$.\n        *   The negative Hessian $H = -\\mathcal{H}$ is:\n            $$ H = \\sum_i \\begin{pmatrix} p_{i1}(1-p_{i1}) x_i x_i^{\\top}  -p_{i1}p_{i2} x_i x_i^{\\top} \\\\ -p_{i1}p_{i2} x_i x_i^{\\top}  p_{i2}(1-p_{i2}) x_i x_i^{\\top} \\end{pmatrix} $$\n        *   This can be written using the weight matrix $W_i$. The problem defines $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$ where $p_i = (p_{i1}, p_{i2})^{\\top}$.\n        *   Let's see what $W_i$ is explicitly: $W_i = \\begin{pmatrix} p_{i1}  0 \\\\ 0  p_{i2} \\end{pmatrix} - \\begin{pmatrix} p_{i1} \\\\ p_{i2} \\end{pmatrix} \\begin{pmatrix} p_{i1}  p_{i2} \\end{pmatrix} = \\begin{pmatrix} p_{i1} - p_{i1}^2  -p_{i1}p_{i2} \\\\ -p_{i1}p_{i2}  p_{i2} - p_{i2}^2 \\end{pmatrix} = \\begin{pmatrix} p_{i1}(1-p_{i1})  -p_{i1}p_{i2} \\\\ -p_{i1}p_{i2}  p_{i2}(1-p_{i2}) \\end{pmatrix}$.\n        *   This is exactly the matrix in the sum for $H$, premultiplied and postmultiplied by $x_i$ block-wise.\n        *   The block matrix inside the sum for $H$ is $\\begin{pmatrix} (W_i)_{11} x_i x_i^{\\top}  (W_i)_{12} x_i x_i^{\\top} \\\\ (W_i)_{21} x_i x_i^{\\top}  (W_i)_{22} x_i x_i^{\\top} \\end{pmatrix}$. This is the Kronecker product $W_i \\otimes (x_i x_i^{\\top})$.\n        *   So, $H = \\sum_i W_i \\otimes (x_i x_i^{\\top})$. This confirms the general forms.\n\n    *   **Part 2: Calculation for one-step NR update**\n        *   The equation to solve is $H \\Delta = U$.\n        *   The update is $\\boldsymbol{\\beta}^{(1)} = \\boldsymbol{\\beta}^{(0)} + \\Delta$. We need to find $\\Delta$.\n        *   **Step 2a: Evaluate at initial parameters**\n            *   We are given $\\beta_1^{(0)} = (0,0)^{\\top}$ and $\\beta_2^{(0)} = (0,0)^{\\top}$.\n            *   First, calculate $\\eta_{ik}$ and $p_{ik}$ for each observation $i=1,2,3$.\n            *   For any $i$, $\\eta_{i1} = x_i^{\\top}\\beta_1^{(0)} = x_i^{\\top}(0,0)^{\\top} = 0$.\n            *   For any $i$, $\\eta_{i2} = x_i^{\\top}\\beta_2^{(0)} = x_i^{\\top}(0,0)^{\\top} = 0$.\n            *   So, for all $i$, the denominator is $1 + \\exp(0) + \\exp(0) = 1+1+1=3$.\n            *   Probabilities:\n                *   $p_{i1} = \\exp(0)/3 = 1/3$.\n                *   $p_{i2} = \\exp(0)/3 = 1/3$.\n                *   $p_{i3} = 1/3$.\n            *   So, for all $i$, $p_i = (p_{i1}, p_{i2})^{\\top} = (1/3, 1/3)^{\\top}$.\n\n        *   **Step 2b: Compute working residuals $r_i$**\n            *   $r_i = y_i^* - p_i$.\n            *   Data:\n                *   $i=1$: category 1. $y_1^* = (1, 0)^{\\top}$.\n                *   $i=2$: category 2. $y_2^* = (0, 1)^{\\top}$.\n                *   $i=3$: category 3. $y_3^* = (0, 0)^{\\top}$.\n            *   Residuals:\n                *   $r_1 = (1, 0)^{\\top} - (1/3, 1/3)^{\\top} = (2/3, -1/3)^{\\top}$.\n                *   $r_2 = (0, 1)^{\\top} - (1/3, 1/3)^{\\top} = (-1/3, 2/3)^{\\top}$.\n                *   $r_3 = (0, 0)^{\\top} - (1/3, 1/3)^{\\top} = (-1/3, -1/3)^{\\top}$.\n\n        *   **Step 2c: Compute weight matrices $W_i$**\n            *   $W_i = \\operatorname{diag}(p_i) - p_i p_i^{\\top}$.\n            *   Since $p_i = (1/3, 1/3)^{\\top}$ for all $i$, the weight matrix $W_i$ is the same for all $i$. Let's call it $W$.\n            *   $p_i p_i^{\\top} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\end{pmatrix} \\begin{pmatrix} 1/3  1/3 \\end{pmatrix} = \\begin{pmatrix} 1/9  1/9 \\\\ 1/9  1/9 \\end{pmatrix}$.\n            *   $\\operatorname{diag}(p_i) = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix}$.\n            *   $W = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix} - \\begin{pmatrix} 1/9  1/9 \\\\ 1/9  1/9 \\end{pmatrix} = \\begin{pmatrix} 2/9  -1/9 \\\\ -1/9  2/9 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix}$.\n            *   So $W_1 = W_2 = W_3 = W = \\frac{1}{9}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix}$.\n\n        *   **Step 2d: Assemble the Score vector $U$**\n            *   $U = \\sum_{i=1}^3 \\begin{pmatrix} (y_{i1}-p_{i1})x_i \\\\ (y_{i2}-p_{i2})x_i \\end{pmatrix} = \\sum_{i=1}^3 \\begin{pmatrix} r_{i1} x_i \\\\ r_{i2} x_i \\end{pmatrix}$.\n            *   Predictors: $x_1 = (1,0)^{\\top}$, $x_2 = (0,1)^{\\top}$, $x_3 = (1,1)^{\\top}$.\n            *   Term for $i=1$: $r_1 = (2/3, -1/3)^{\\top}$.\n                *   $\\begin{pmatrix} (2/3)x_1 \\\\ (-1/3)x_1 \\end{pmatrix} = \\begin{pmatrix} 2/3 (1,0)^{\\top} \\\\ -1/3 (1,0)^{\\top} \\end{pmatrix} = (2/3, 0, -1/3, 0)^{\\top}$.\n            *   Term for $i=2$: $r_2 = (-1/3, 2/3)^{\\top}$.\n                *   $\\begin{pmatrix} (-1/3)x_2 \\\\ (2/3)x_2 \\end{pmatrix} = \\begin{pmatrix} -1/3 (0,1)^{\\top} \\\\ 2/3 (0,1)^{\\top} \\end{pmatrix} = (0, -1/3, 0, 2/3)^{\\top}$.\n            *   Term for $i=3$: $r_3 = (-1/3, -1/3)^{\\top}$.\n                *   $\\begin{pmatrix} (-1/3)x_3 \\\\ (-1/3)x_3 \\end{pmatrix} = \\begin{pmatrix} -1/3 (1,1)^{\\top} \\\\ -1/3 (1,1)^{\\top} \\end{pmatrix} = (-1/3, -1/3, -1/3, -1/3)^{\\top}$.\n            *   Summing them up:\n                *   $U = (2/3, 0, -1/3, 0)^{\\top} + (0, -1/3, 0, 2/3)^{\\top} + (-1/3, -1/3, -1/3, -1/3)^{\\top}$.\n                *   $U_1 = 2/3 + 0 - 1/3 = 1/3$.\n                *   $U_2 = 0 - 1/3 - 1/3 = -2/3$.\n                *   $U_3 = -1/3 + 0 - 1/3 = -2/3$.\n                *   $U_4 = 0 + 2/3 - 1/3 = 1/3$.\n                *   $U = (1/3, -2/3, -2/3, 1/3)^{\\top}$.\n\n        *   **Step 2e: Assemble the Negative Hessian matrix $H$**\n            *   $H = \\sum_{i=1}^3 W_i \\otimes (x_i x_i^{\\top})$.\n            *   $W_i = W = \\frac{1}{9}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix}$.\n            *   Predictor outer products:\n                *   $x_1 x_1^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\n                *   $x_2 x_2^{\\top} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0  1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}$.\n                *   $x_3 x_3^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$.\n            *   Term for $i=1$: $W \\otimes (x_1 x_1^{\\top})$.\n                *   $\\frac{1}{9}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} \\otimes \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}  -1\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\\\ -1\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}  2\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2  0  -1  0 \\\\ 0  0  0  0 \\\\ -1  0  2  0 \\\\ 0  0  0  0 \\end{pmatrix}$.\n            *   Term for $i=2$: $W \\otimes (x_2 x_2^{\\top})$.\n                *   $\\frac{1}{9}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} \\otimes \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2\\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}  -1\\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\\\ -1\\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}  2\\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 0  0  0  0 \\\\ 0  2  0  -1 \\\\ 0  0  0  0 \\\\ 0  -1  0  2 \\end{pmatrix}$.\n            *   Term for $i=3$: $W \\otimes (x_3 x_3^{\\top})$.\n                *   $\\frac{1}{9}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} \\otimes \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}  -1\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\\\ -1\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}  2\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 2  2  -1  -1 \\\\ 2  2  -1  -1 \\\\ -1  -1  2  2 \\\\ -1  -1  2  2 \\end{pmatrix}$.\n            *   Summing them up to get $H$:\n                *   $H = \\frac{1}{9} \\left[ \\begin{pmatrix} 2  0  -1  0 \\\\ 0  0  0  0 \\\\ -1  0  2  0 \\\\ 0  0  0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0  0  0 \\\\ 0  2  0  -1 \\\\ 0  0  0  0 \\\\ 0  -1  0  2 \\end{pmatrix} + \\begin{pmatrix} 2  2  -1  -1 \\\\ 2  2  -1  -1 \\\\ -1  -1  2  2 \\\\ -1  -1  2  2 \\end{pmatrix} \\right]$\n                *   $H = \\frac{1}{9} \\begin{pmatrix} 4  2  -2  -1 \\\\ 2  4  -1  -2 \\\\ -2  -1  4  2 \\\\ -1  -2  2  4 \\end{pmatrix}$.\n            *   Let's check the structure of $H$. It should be $\\begin{pmatrix} H_{11}  H_{12} \\\\ H_{21}  H_{22} \\end{pmatrix}$.\n                *   $H_{11} = \\sum_i p_{i1}(1-p_{i1}) x_i x_i^{\\top} = \\frac{2}{9} \\sum_i x_i x_i^{\\top}$.\n                *   $H_{22} = \\sum_i p_{i2}(1-p_{i2}) x_i x_i^{\\top} = \\frac{2}{9} \\sum_i x_i x_i^{\\top}$.\n                *   $H_{12} = -\\sum_i p_{i1}p_{i2} x_i x_i^{\\top} = -\\frac{1}{9} \\sum_i x_i x_i^{\\top}$.\n                *   $H_{21} = H_{12}^{\\top} = H_{12}$ since $x_i x_i^{\\top}$ is symmetric.\n                *   $\\sum_i x_i x_i^{\\top} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$.\n                *   $H_{11} = H_{22} = \\frac{2}{9}\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 4  2 \\\\ 2  4 \\end{pmatrix}$.\n                *   $H_{12} = H_{21} = -\\frac{1}{9}\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} -2  -1 \\\\ -1  -2 \\end{pmatrix}$.\n                *   Putting it together:\n                    $H = \\frac{1}{9} \\begin{pmatrix} 4  2  -2  -1 \\\\ 2  4  -1  -2 \\\\ -2  -1  4  2 \\\\ -1  -2  2  4 \\end{pmatrix}$. This matches my previous calculation. Good.\n\n        *   **Step 2f: Solve the linear system $H \\Delta = U$**\n            *   $\\frac{1}{9} \\begin{pmatrix} 4  2  -2  -1 \\\\ 2  4  -1  -2 \\\\ -2  -1  4  2 \\\\ -1  -2  2  4 \\end{pmatrix} \\begin{pmatrix} \\Delta_1 \\\\ \\Delta_2 \\\\ \\Delta_3 \\\\ \\Delta_4 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n            *   Let's multiply by 9:\n                $\\begin{pmatrix} 4  2  -2  -1 \\\\ 2  4  -1  -2 \\\\ -2  -1  4  2 \\\\ -1  -2  2  4 \\end{pmatrix} \\begin{pmatrix} \\Delta_1 \\\\ \\Delta_2 \\\\ \\Delta_3 \\\\ \\Delta_4 \\end{pmatrix} = 3 \\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -6 \\\\ -6 \\\\ 3 \\end{pmatrix}$.\n            *   Let's call the matrix $\\tilde{H}$. $\\tilde{H} \\Delta = \\tilde{U}$.\n            *   Notice the structure of $\\tilde{H}$. $\\tilde{H} = \\begin{pmatrix} A  B \\\\ B  A \\end{pmatrix}$ where $A = \\begin{pmatrix} 4  2 \\\\ 2  4 \\end{pmatrix}$ and $B = \\begin{pmatrix} -2  -1 \\\\ -1  -2 \\end{pmatrix}$. Also $B = - \\frac{1}{2} A$. Let's check: $-\\frac{1}{2} \\begin{pmatrix} 4  2 \\\\ 2  4 \\end{pmatrix} = \\begin{pmatrix} -2  -1 \\\\ -1  -2 \\end{pmatrix}$. Yes.\n            *   So the system is:\n                $A \\Delta_{1,2} + B \\Delta_{3,4} = \\tilde{U}_{1,2}$\n                $B \\Delta_{1,2} + A \\Delta_{3,4} = \\tilde{U}_{3,4}$\n                where $\\Delta = (\\Delta_{1,2}^{\\top}, \\Delta_{3,4}^{\\top})^{\\top}$.\n            *   Note that $\\tilde{U}_{3,4} = \\begin{pmatrix} -6 \\\\ 3 \\end{pmatrix}$ and $\\tilde{U}_{1,2} = \\begin{pmatrix} 3 \\\\ -6 \\end{pmatrix}$. Let me recheck my $U$.\n                *   $U = (1/3, -2/3, -2/3, 1/3)^{\\top}$.\n                *   So $\\tilde{U}$ should be $(3, -6, -6, 3)^\\top$. Let me re-check again.\n                *   $U = [ (2/3, 0) + (0, -1/3) + (-1/3, -1/3) ]_1 + [ (-1/3, 0) + (0, 2/3) + (-1/3, -1/3) ]_2$.\n                *   Let's re-sum the components of $U$.\n                    *   $U_1 = (2/3)(1) + (-1/3)(0) + (-1/3)(1) = 2/3 - 1/3 = 1/3$.\n                    *   $U_2 = (2/3)(0) + (-1/3)(1) + (-1/3)(1) = -1/3 - 1/3 = -2/3$.\n                    *   $U_3 = (-1/3)(1) + (2/3)(0) + (-1/3)(1) = -1/3 - 1/3 = -2/3$.\n                    *   $U_4 = (-1/3)(0) + (2/3)(1) + (-1/3)(1) = 2/3 - 1/3 = 1/3$.\n                *   So $U = (1/3, -2/3, -2/3, 1/3)^{\\top}$ is correct.\n                *   And $\\tilde{U} = (3, -6, -6, 3)^{\\top}$ is correct.\n                *   Let's check the structure of $\\tilde{U}$. Let $\\tilde{U}_{1,2} = (3, -6)^{\\top}$ and $\\tilde{U}_{3,4} = (-6, 3)^{\\top}$.\n                *   Notice that $\\tilde{U}_{3,4}$ is not equal to $\\tilde{U}_{1,2}$.\n            *   Let's try to solve a different way. Notice a symmetry in the system.\n                *   (1) $4\\Delta_1 + 2\\Delta_2 - 2\\Delta_3 - \\Delta_4 = 3$\n                *   (2) $2\\Delta_1 + 4\\Delta_2 - \\Delta_3 - 2\\Delta_4 = -6$\n                *   (3) $-2\\Delta_1 - \\Delta_2 + 4\\Delta_3 + 2\\Delta_4 = -6$\n                *   (4) $-\\Delta_1 - 2\\Delta_2 + 2\\Delta_3 + 4\\Delta_4 = 3$\n            *   Add (1) and (4): $3\\Delta_1 + 0\\Delta_2 + 0\\Delta_3 + 3\\Delta_4 = 6 \\implies \\Delta_1 + \\Delta_4 = 2$.\n            *   Add (2) and (3): $0\\Delta_1 + 3\\Delta_2 + 3\\Delta_3 + 0\\Delta_4 = -12 \\implies \\Delta_2 + \\Delta_3 = -4$.\n            *   This is helpful. Let's see if we can find more relations.\n            *   Add (1) and (3): $2\\Delta_1+\\Delta_2+2\\Delta_3+\\Delta_4 = -3$.\n            *   Add (2) and (4): $\\Delta_1+2\\Delta_2+ \\Delta_3+2\\Delta_4 = -3$.\n            *   Let's use the two relations I found. $\\Delta_4 = 2 - \\Delta_1$ and $\\Delta_3 = -4 - \\Delta_2$.\n            *   Substitute these into (1) and (2).\n            *   Into (1):\n                $4\\Delta_1 + 2\\Delta_2 - 2(-4 - \\Delta_2) - (2 - \\Delta_1) = 3$\n                $4\\Delta_1 + 2\\Delta_2 + 8 + 2\\Delta_2 - 2 + \\Delta_1 = 3$\n                $5\\Delta_1 + 4\\Delta_2 + 6 = 3$\n                $5\\Delta_1 + 4\\Delta_2 = -3$ (Eq. A)\n            *   Into (2):\n                $2\\Delta_1 + 4\\Delta_2 - (-4 - \\Delta_2) - 2(2 - \\Delta_1) = -6$\n                $2\\Delta_1 + 4\\Delta_2 + 4 + \\Delta_2 - 4 + 2\\Delta_1 = -6$\n                $4\\Delta_1 + 5\\Delta_2 = -6$ (Eq. B)\n            *   Now I have a $2 \\times 2$ system for $\\Delta_1$ and $\\Delta_2$:\n                $5\\Delta_1 + 4\\Delta_2 = -3$\n                $4\\Delta_1 + 5\\Delta_2 = -6$\n            *   Multiply first eq by 5, second by 4:\n                $25\\Delta_1 + 20\\Delta_2 = -15$\n                $16\\Delta_1 + 20\\Delta_2 = -24$\n            *   Subtract second from first:\n                $9\\Delta_1 = -15 - (-24) = 9 \\implies \\Delta_1 = 1$.\n            *   Substitute $\\Delta_1=1$ into Eq. A:\n                $5(1) + 4\\Delta_2 = -3 \\implies 4\\Delta_2 = -8 \\implies \\Delta_2 = -2$.\n            *   Now find $\\Delta_3$ and $\\Delta_4$:\n                *   $\\Delta_3 = -4 - \\Delta_2 = -4 - (-2) = -2$.\n                *   $\\Delta_4 = 2 - \\Delta_1 = 2 - 1 = 1$.\n            *   So the solution is $\\Delta = (1, -2, -2, 1)^{\\top}$.\n\n        *   **Step 2g: Verify the solution**\n            *   Let's plug $\\Delta = (1, -2, -2, 1)^{\\top}$ back into the original system $\\tilde{H} \\Delta = \\tilde{U}$.\n            *   $\\tilde{H} \\Delta = \\begin{pmatrix} 4  2  -2  -1 \\\\ 2  4  -1  -2 \\\\ -2  -1  4  2 \\\\ -1  -2  2  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\\\ 1 \\end{pmatrix}$\n            *   Row 1: $4(1) + 2(-2) - 2(-2) - 1(1) = 4 - 4 + 4 - 1 = 3$. Correct.\n            *   Row 2: $2(1) + 4(-2) - 1(-2) - 2(1) = 2 - 8 + 2 - 2 = -6$. Correct.\n            *   Row 3: $-2(1) - 1(-2) + 4(-2) + 2(1) = -2 + 2 - 8 + 2 = -6$. Correct.\n            *   Row 4: $-1(1) - 2(-2) + 2(-2) + 4(1) = -1 + 4 - 4 + 4 = 3$. Correct.\n            *   The solution is correct. $\\Delta = (\\Delta\\beta_{11}, \\Delta\\beta_{12}, \\Delta\\beta_{21}, \\Delta\\beta_{22})^{\\top} = (1, -2, -2, 1)^{\\top}$.\n\n3.  **Final Formatting:**\n\n    *   The solution needs to be written out logically, starting with the general derivations and then applying them to the specific data.\n    *   All mathematical entities must be in LaTeX.\n    *   The final answer must be in a box, as a $1 \\times 4$ row matrix.\n    *   The final answer is $\\Delta\\beta_1 = (1, -2)^{\\top}$ and $\\Delta\\beta_2 = (-2, 1)^{\\top}$.\n    *   So $\\Delta = (1, -2, -2, 1)^{\\top}$.\n    *   The final answer as a row matrix is $\\begin{pmatrix} 1  -2  -2  1 \\end{pmatrix}$.\n\n    Double-check the problem statement: \"report the four components of $\\Delta$ exactly\". \"Provide your final answer as a single $1 \\times 4$ row matrix\". This all aligns.\n    No rounding, so the integer results are perfect.\n    All steps seem correct and logically sound. The derivation of the score and Hessian is standard for multinomial logistic regression. The calculations are straightforward arithmetic and matrix operations. The final linear system solving is also standard.\n    I am confident in the result.", "answer": "$$ \\boxed{ \\begin{pmatrix} 1  -2  -2  1 \\end{pmatrix} } $$", "id": "4929771"}, {"introduction": "Once a model is fit, its primary value lies in prediction and interpretation. This practice shifts our focus from the mechanics of estimation to the application of a fitted ordinal logistic regression model. You will take a pre-existing proportional odds model and use its estimated parameters—the thresholds $\\theta_j$ and slope $\\beta$—to calculate predicted probabilities for a new subject. This process illuminates the role of each parameter and solidifies your understanding of how the model translates covariate information into clinically meaningful, ordered outcome probabilities [@problem_id:4929834].", "problem": "An ordinal outcome $Y \\in \\{1,2,3,4,5\\}$ records disease severity on a $5$-level scale, with higher values indicating worse severity. Consider a proportional odds cumulative logit model with a single covariate $x$ (e.g., a standardized biomarker). The proportional odds model assumes that for each cutoff $j \\in \\{1,2,3,4\\}$, the cumulative log-odds is linear in $x$ with a category-specific intercept (threshold) and a common slope:\n$$\n\\ln\\!\\left(\\frac{\\Pr(Y \\le j \\mid x)}{1 - \\Pr(Y \\le j \\mid x)}\\right) \\;=\\; \\theta_j \\;-\\; \\beta\\, x.\n$$\nSuppose a study has estimated thresholds $\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$ and a slope $\\beta=0.9$. For a new individual with covariate value $x=1.6$:\n- Starting from the definitions of the logit link and its inverse logistic function, compute the predicted cumulative probabilities $\\gamma_j(x)=\\Pr(Y \\le j \\mid x)$ for $j=1,2,3,4$.\n- Then compute the category probabilities $\\pi_k(x)=\\Pr(Y=k \\mid x)$ for $k=1,2,3,4,5$.\n- Briefly state at least two numerical stability considerations relevant to evaluating these probabilities in practice.\n\nExpress all probabilities as decimals. Your final reported quantity must be the predicted probability $\\Pr(Y=4 \\mid x=1.6)$, rounded to four significant figures.", "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   The outcome variable is an ordinal variable $Y \\in \\{1,2,3,4,5\\}$, representing disease severity.\n-   The model is a proportional odds cumulative logit model with a single covariate $x$.\n-   The model equation is: $\\ln\\!\\left(\\frac{\\Pr(Y \\le j \\mid x)}{1 - \\Pr(Y \\le j \\mid x)}\\right) \\;=\\; \\theta_j \\;-\\; \\beta\\, x$, for $j \\in \\{1,2,3,4\\}$.\n-   The estimated thresholds are: $\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$.\n-   The estimated slope is: $\\beta=0.9$.\n-   The covariate value for the new individual is: $x=1.6$.\n-   Task 1: Compute cumulative probabilities $\\gamma_j(x)=\\Pr(Y \\le j \\mid x)$ for $j=1,2,3,4$.\n-   Task 2: Compute category probabilities $\\pi_k(x)=\\Pr(Y=k \\mid x)$ for $k=1,2,3,4,5$.\n-   Task 3: State at least two numerical stability considerations.\n-   Task 4: Report the final quantity $\\Pr(Y=4 \\mid x=1.6)$ rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem describes a proportional odds model, which is a standard and widely-used statistical method for analyzing ordinal data in biostatistics and other fields. The model equation is correctly stated. The given parameters are plausible. A necessary condition for a valid proportional odds model is that the thresholds must be strictly increasing, i.e., $\\theta_1  \\theta_2  \\dots  \\theta_{J-1}$. The given thresholds, $\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$, satisfy this condition ($-2.1  -0.8  0.6  1.8$). This ensures that the cumulative probabilities $\\Pr(Y \\le j \\mid x)$ will be non-decreasing with $j$, a logical necessity. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem is well-posed. It provides all the necessary information—the model form, all parameter values ($\\theta_j$ and $\\beta$), and the specific value of the covariate $x$—to calculate the requested probabilities. The tasks are clearly defined and lead to a unique solution.\n3.  **Objective**: The problem is stated in precise, objective, and quantitative terms. There is no subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed, scientifically grounded application of a standard statistical model. I will proceed with the solution.\n\nThe proportional odds model is defined by the logit link function for cumulative probabilities:\n$$\n\\text{logit}(\\Pr(Y \\le j \\mid x)) = \\ln\\left(\\frac{\\Pr(Y \\le j \\mid x)}{1 - \\Pr(Y \\le j \\mid x)}\\right) = \\theta_j - \\beta x\n$$\nTo find the cumulative probabilities, $\\gamma_j(x) = \\Pr(Y \\le j \\mid x)$, we must apply the inverse of the logit link function, which is the standard logistic function, $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n$$\n\\gamma_j(x) = \\sigma(\\theta_j - \\beta x) = \\frac{1}{1 + \\exp(-(\\theta_j - \\beta x))}\n$$\nWe are given the parameters $\\theta_1=-2.1$, $\\theta_2=-0.8$, $\\theta_3=0.6$, $\\theta_4=1.8$, and $\\beta=0.9$. We need to evaluate the probabilities for an individual with covariate value $x=1.6$.\n\nFirst, we calculate the linear predictor term $\\beta x$:\n$$\n\\beta x = 0.9 \\times 1.6 = 1.44\n$$\nNow, we compute the logits for each cumulative probability:\n$$\n\\text{logit}(\\gamma_1(1.6)) = \\theta_1 - \\beta x = -2.1 - 1.44 = -3.54\n$$\n$$\n\\text{logit}(\\gamma_2(1.6)) = \\theta_2 - \\beta x = -0.8 - 1.44 = -2.24\n$$\n$$\n\\text{logit}(\\gamma_3(1.6)) = \\theta_3 - \\beta x = 0.6 - 1.44 = -0.84\n$$\n$$\n\\text{logit}(\\gamma_4(1.6)) = \\theta_4 - \\beta x = 1.8 - 1.44 = 0.36\n$$\nNext, we apply the logistic function to these logit values to obtain the cumulative probabilities $\\gamma_j(x)$:\n$$\n\\gamma_1(1.6) = \\Pr(Y \\le 1 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(-3.54))} = \\frac{1}{1 + \\exp(3.54)} \\approx 0.02819\n$$\n$$\n\\gamma_2(1.6) = \\Pr(Y \\le 2 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(-2.24))} = \\frac{1}{1 + \\exp(2.24)} \\approx 0.09621\n$$\n$$\n\\gamma_3(1.6) = \\Pr(Y \\le 3 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(-0.84))} = \\frac{1}{1 + \\exp(0.84)} \\approx 0.30154\n$$\n$$\n\\gamma_4(1.6) = \\Pr(Y \\le 4 \\mid x=1.6) = \\frac{1}{1 + \\exp(-(0.36))} = \\frac{1}{1 + \\exp(-0.36)} \\approx 0.58904\n$$\nWith the cumulative probabilities computed, we can find the individual category probabilities $\\pi_k(x) = \\Pr(Y=k \\mid x)$. The relationships are as follows:\n$$\n\\pi_1(x) = \\Pr(Y=1 \\mid x) = \\Pr(Y \\le 1 \\mid x) = \\gamma_1(x)\n$$\n$$\n\\pi_k(x) = \\Pr(Y=k \\mid x) = \\Pr(Y \\le k \\mid x) - \\Pr(Y \\le k-1 \\mid x) = \\gamma_k(x) - \\gamma_{k-1}(x) \\text{ for } k=2, 3, 4\n$$\n$$\n\\pi_5(x) = \\Pr(Y=5 \\mid x) = 1 - \\Pr(Y \\le 4 \\mid x) = 1 - \\gamma_4(x)\n$$\nUsing the values we calculated for $x=1.6$:\n$$\n\\pi_1(1.6) = \\gamma_1(1.6) \\approx 0.02819\n$$\n$$\n\\pi_2(1.6) = \\gamma_2(1.6) - \\gamma_1(1.6) \\approx 0.09621 - 0.02819 = 0.06802\n$$\n$$\n\\pi_3(1.6) = \\gamma_3(1.6) - \\gamma_2(1.6) \\approx 0.30154 - 0.09621 = 0.20533\n$$\n$$\n\\pi_4(1.6) = \\gamma_4(1.6) - \\gamma_3(1.6) \\approx 0.58904 - 0.30154 = 0.28750\n$$\n$$\n\\pi_5(1.6) = 1 - \\gamma_4(1.6) \\approx 1 - 0.58904 = 0.41096\n$$\nAs a check, the sum of these probabilities is $0.02819 + 0.06802 + 0.20533 + 0.28750 + 0.41096 = 1.00000$.\n\nFinally, we address the numerical stability considerations.\n1.  **Evaluation of the logistic function $\\sigma(z) = 1/(1 + \\exp(-z))$**: For large negative values of the argument $z$, the term $-z$ becomes large and positive, causing $\\exp(-z)$ to overflow standard floating-point representations. For instance, if $z=-1000$, $\\exp(1000)$ is computationally infinite. A numerically stable implementation uses an alternative algebraic form for negative arguments: $\\sigma(z) = \\exp(z) / (1 + \\exp(z))$. When $z$ is large and negative, $\\exp(z)$ underflows to $0$, yielding the stable and correct result of $0$. A common practice is to use a conditional evaluation: if $z  0$, compute $\\exp(z)/(1+\\exp(z))$; otherwise, compute $1/(1+\\exp(-z))$. Many statistical software packages also work on the log scale, using functions like `log1p(x)` which computes $\\ln(1+x)$ accurately for small $x$, to avoid underflow and overflow issues when computing log-probabilities (the \"log-sum-exp\" trick).\n2.  **Catastrophic cancellation in probability differences**: The category probabilities $\\pi_k$ for $k \\in \\{2, 3, 4\\}$ are computed by subtracting consecutive cumulative probabilities: $\\pi_k(x) = \\gamma_k(x) - \\gamma_{k-1}(x)$. If the thresholds $\\theta_k$ and $\\theta_{k-1}$ are very close to each other, their corresponding logits will also be close, leading to $\\gamma_k(x)$ and $\\gamma_{k-1}(x)$ being nearly equal. Subtracting two nearly equal floating-point numbers results in a catastrophic cancellation, a severe loss of relative precision. For example, if $\\gamma_k(x) = 0.51234567$ and $\\gamma_{k-1}(x) = 0.51234500$, their difference is $0.00000067$, which has far fewer significant digits than the original numbers. This can lead to highly inaccurate estimates for small category probabilities. More stable numerical routines might be required to compute this difference accurately in such scenarios.\n\nThe required final quantity is $\\Pr(Y=4 \\mid x=1.6)$, which is $\\pi_4(1.6)$. From our calculation, $\\pi_4(1.6) \\approx 0.28750$. Rounding to four significant figures gives $0.2875$.", "answer": "$$\\boxed{0.2875}$$", "id": "4929834"}, {"introduction": "This final practice is an advanced challenge that bridges statistical theory with computational implementation, addressing a common and important issue in fitting logistic models. You will explore the phenomenon of \"perfect separation,\" where a predictor perfectly distinguishes an outcome category, causing standard maximum likelihood estimates to diverge to infinity. By implementing a penalized regression model from scratch, you will not only see this problem unfold numerically but also code the solution using ridge regularization, gaining invaluable insight into the practical side of modern computational statistics [@problem_id:4929787].", "problem": "You are given the task of constructing and analyzing a small synthetic dataset for a nominal multinomial logistic regression model with three outcome categories, denoted by $K=3$. Work within the standard baseline-category logit parameterization in which category $0$ is the reference, and the linear predictors for categories $1$ and $2$ are given by $\\eta_{i1} = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}_1$ and $\\eta_{i2} = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}_2$, respectively. The probability model for subject $i$ with covariate vector $\\mathbf{x}_i \\in \\mathbb{R}^p$ is\n$$\n\\pi_{ik} = \\mathbb{P}(Y_i = k \\mid \\mathbf{x}_i) = \\frac{\\exp(\\eta_{ik})}{\\exp(0) + \\sum_{j=1}^{2} \\exp(\\eta_{ij})}, \\quad \\text{for } k \\in \\{1,2\\},\n$$\nand $\\pi_{i0} = \\frac{1}{1 + \\sum_{j=1}^2 \\exp(\\eta_{ij})}$ for the baseline $k=0$. The dataset you construct must include an intercept and one scalar predictor $x$ (so $p=2$), and must satisfy the property that one predictor perfectly separates one category from the others. Specifically, ensure that there exists a linear function of the predictor such that all observations with $x0$ belong to category $2$ and no observations with $x \\le 0$ belong to category $2$. This is known as perfect separation.\n\nStarting from the fundamental definition of the multinomial likelihood under independent observations,\n$$\nL(\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2) = \\prod_{i=1}^n \\prod_{k=0}^2 \\pi_{ik}^{\\mathbb{I}(Y_i = k)},\n$$\nwork with the log-likelihood and augment it with an $\\ell_2$ (ridge) penalty of strength $\\lambda0$ applied to all non-baseline coefficients. That is, maximize\n$$\n\\ell_\\lambda(\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2) = \\sum_{i=1}^n \\sum_{k=0}^2 \\mathbb{I}(Y_i = k) \\log \\pi_{ik} - \\frac{\\lambda}{2}\\left(\\lVert \\boldsymbol{\\beta}_1 \\rVert_2^2 + \\lVert \\boldsymbol{\\beta}_2 \\rVert_2^2\\right).\n$$\nUse this penalized formulation to numerically demonstrate the divergence phenomenon: as $\\lambda \\to 0^+$, the estimated coefficient corresponding to the separating predictor in the separating category grows without bound. Your algorithm must be derived from the above definitions and should employ Newton–Raphson updates using the exact gradient and Hessian of $\\ell_\\lambda$, ensuring numerical stability of the softmax probabilities via a log-sum-exp transformation.\n\nYour program must:\n- Construct two fixed datasets, each with an intercept and one predictor $x$:\n  1. A separated dataset with $n=12$ where $x_i \\in \\{-1, +1\\}$ and category $2$ is observed if and only if $x_i=+1$, while categories $0$ and $1$ occur only when $x_i=-1$.\n  2. A non-separated control dataset with the same $n$ and $x_i \\in \\{-1, +1\\}$ in which category $2$ appears with both $x=-1$ and $x=+1$.\n- Implement a Newton–Raphson optimizer for the ridge-penalized log-likelihood using the exact gradient and Hessian derived from the model above, with category $0$ as baseline. The optimizer must:\n  - Represent parameters as $(\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2) \\in \\mathbb{R}^{2 \\times 2}$ (intercept and slope for each non-baseline category).\n  - Use a numerically stable computation of $\\pi_{ik}$ by subtracting $\\max\\{0, \\eta_{i1}, \\eta_{i2}\\}$ before exponentiation.\n  - Terminate when the Newton step norm is below a tolerance or a maximum number of iterations is reached.\n- For each test case, fit the model and report the absolute value of the slope coefficient (the coefficient multiplying $x$) associated with category $2$, denoted by $|\\beta_{2,x}|$.\n\nTest suite and outputs:\n- Use the following five test cases, each a pair $(\\text{dataset\\_id}, \\lambda)$:\n  1. $(\\text{\"sep\"}, 1.0)$\n  2. $(\\text{\"sep\"}, 0.1)$\n  3. $(\\text{\"sep\"}, 0.01)$\n  4. $(\\text{\"sep\"}, 0.001)$\n  5. $(\\text{\"ctrl\"}, 0.001)$\n- For each test case, compute the fitted parameter estimates and output the value of $|\\beta_{2,x}|$ as a floating-point number rounded to six decimal places. No physical units are involved in this problem.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[z1,z2,z3,z4,z5]\"), where each $z_j$ is the float for test case $j$ in the specified order.\n\nConstraints and requirements:\n- Implement all necessary computations; do not use any external datasets or inputs.\n- Use only definitions and principles specified above: the multinomial model, the log-likelihood, and the ridge-penalized objective. Do not use shortcut formulas not derivable from these principles.\n- Ensure scientific realism by constructing internally consistent datasets and by using stable numerical methods for exponentials and linear algebra.", "solution": "The user wants to solve a multinomial logistic regression problem using a custom Newton-Raphson optimizer.\n\n### Step 1: Problem Validation\n\nThe user's request is to validate the problem statement *before* attempting to solve it. This is a critical step in my process.\n\n#### 1.1. Extraction of Givens\n\nI will list all the data, variables, constants, definitions, and conditions provided in the problem statement.\n\n-   **Model:** Nominal multinomial logistic regression with $K=3$ outcome categories ($0, 1, 2$).\n-   **Parameterization:** Baseline-category logit with category $0$ as the reference.\n-   **Linear Predictors:** For an observation $i$, the linear predictors for the non-baseline categories are $\\eta_{i1} = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}_1$ and $\\eta_{i2} = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}_2$. The predictor for the baseline category is fixed at $\\eta_{i0}=0$.\n-   **Probability Model:** \n    -   For $k \\in \\{1,2\\}$, $\\pi_{ik} = \\mathbb{P}(Y_i = k \\mid \\mathbf{x}_i) = \\frac{\\exp(\\eta_{ik})}{\\exp(0) + \\sum_{j=1}^{2} \\exp(\\eta_{ij})}$.\n    -   For $k=0$, $\\pi_{i0} = \\frac{1}{1 + \\sum_{j=1}^2 \\exp(\\eta_{ij})}$.\n-   **Design Matrix:** Covariate vectors $\\mathbf{x}_i \\in \\mathbb{R}^p$ where $p=2$, comprising an intercept and one scalar predictor $x$. Thus, $\\mathbf{x}_i = [1, x_i]^\\top$.\n-   **Coefficient Vectors:** $\\boldsymbol{\\beta}_1 = [\\beta_{1,0}, \\beta_{1,x}]^\\top$ and $\\boldsymbol{\\beta}_2 = [\\beta_{2,0}, \\beta_{2,x}]^\\top$.\n-   **Optimization Objective:** Maximize the ridge-penalized log-likelihood:\n    $$\n    \\ell_\\lambda(\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2) = \\sum_{i=1}^n \\left( \\sum_{k=0}^2 \\mathbb{I}(Y_i = k) \\log \\pi_{ik} \\right) - \\frac{\\lambda}{2}\\left(\\lVert \\boldsymbol{\\beta}_1 \\rVert_2^2 + \\lVert \\boldsymbol{\\beta}_2 \\rVert_2^2\\right)\n    $$\n    with penalty strength $\\lambda  0$.\n-   **Numerical Method:** Implement a Newton-Raphson optimizer using the exact gradient and Hessian of $\\ell_\\lambda$.\n-   **Numerical Stability:** Use a log-sum-exp transformation for stable computation of softmax probabilities.\n-   **Datasets:**\n    1.  **Separated (\"sep\"):** $n=12$, $x_i \\in \\{-1, +1\\}$. Category $2$ is observed if and only if $x_i=+1$. Categories $0$ and $1$ are observed only when $x_i=-1$.\n    2.  **Control (\"ctrl\"):** $n=12$, $x_i \\in \\{-1, +1\\}$. Category $2$ appears for observations with both $x_i=-1$ and $x_i=+1$.\n-   **Optimizer Implementation Details:**\n    -   Parameters are represented as a $2 \\times 2$ matrix for $(\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2)$.\n    -   Termination is based on the Newton step norm or a maximum number of iterations.\n-   **Required Output:** For each test case, report the value of $|\\beta_{2,x}|$, rounded to six decimal places.\n-   **Test Cases:** A list of five pairs $(\\text{dataset\\_id}, \\lambda)$:\n    1.  $(\\text{\"sep\"}, 1.0)$\n    2.  $(\\text{\"sep\"}, 0.1)$\n    3.  $(\\text{\"sep\"}, 0.01)$\n    4.  $(\\text{\"sep\"}, 0.001)$\n    5.  $(\\text{\"ctrl\"}, 0.001)$\n-   **Final Output Format:** A single line of text with a comma-separated list of results in square brackets: `[z1,z2,z3,z4,z5]`.\n\n#### 1.2. Validation of Givens\n\nI will now validate the extracted givens based on the specified criteria.\n\n-   **Scientifically Grounded:** The problem is firmly rooted in the theory of generalized linear models, a core topic in statistics and biostatistics. Multinomial logistic regression, ridge penalization, the Newton-Raphson method, and the phenomenon of perfect separation are all standard, well-documented concepts. The request to demonstrate the effect of perfect separation on coefficient estimates is a classic pedagogical exercise. The problem is scientifically and mathematically sound.\n-   **Well-Posed:** For any strictly positive penalty $\\lambda  0$, the objective function $\\ell_\\lambda$ is strictly concave. This is because the log-likelihood for the multinomial model is concave, and the ridge penalty term $-\\frac{\\lambda}{2} \\sum_k \\lVert \\boldsymbol{\\beta}_k \\rVert^2$ is strictly concave. A strictly concave function has at most one global maximum. The existence of this maximum is guaranteed because the penalty term ensures that $\\ell_\\lambda \\to -\\infty$ as $\\lVert \\boldsymbol{\\beta} \\rVert \\to \\infty$. Therefore, for each test case, a unique, stable solution exists. The problem is well-posed.\n-   **Objective:** The problem is specified using precise mathematical language and unambiguous terminology. The tasks, datasets, and output format are defined with complete clarity. There are no subjective or opinion-based components.\n\nBased on this analysis, the problem statement does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, complete, and non-trivial.\n\n#### 1.3. Verdict and Action\n\nThe problem is **valid**. I will proceed to derive and implement the solution.\n\n### Step 2: Principled Solution Derivation\n\nThe solution requires implementing a Newton-Raphson algorithm to maximize the penalized log-likelihood $\\ell_\\lambda$. This involves deriving the gradient vector and the Hessian matrix of the objective function.\n\nLet the full parameter vector be flattened as $\\boldsymbol{\\beta}_{\\text{flat}} = [\\boldsymbol{\\beta}_1^\\top, \\boldsymbol{\\beta}_2^\\top]^\\top \\in \\mathbb{R}^4$. The parameters to estimate are $\\beta_{1,0}, \\beta_{1,x}, \\beta_{2,0}, \\beta_{2,x}$.\n\n#### 2.1. Gradient of the Penalized Log-Likelihood\n\nThe objective function is $\\ell_\\lambda(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - P(\\boldsymbol{\\beta})$, where $\\ell(\\boldsymbol{\\beta})$ is the log-likelihood and $P(\\boldsymbol{\\beta}) = \\frac{\\lambda}{2}(\\lVert \\boldsymbol{\\beta}_1 \\rVert_2^2 + \\lVert \\boldsymbol{\\beta}_2 \\rVert_2^2)$ is the penalty.\n\nThe derivative of the log-likelihood portion with respect to a single coefficient vector $\\boldsymbol{\\beta}_r$ ($r \\in \\{1, 2\\}$) is a standard result:\n$$\n\\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}_r} = \\sum_{i=1}^n (y_{ir} - \\pi_{ir})\\mathbf{x}_i\n$$\nwhere $y_{ir} = \\mathbb{I}(Y_i=r)$.\n\nThe derivative of the penalty term is straightforward:\n$$\n\\frac{\\partial P}{\\partial \\boldsymbol{\\beta}_r} = \\lambda \\boldsymbol{\\beta}_r\n$$\n\nCombining these gives the gradient of the full objective function $\\ell_\\lambda$:\n$$\n\\mathbf{g}_r = \\frac{\\partial \\ell_\\lambda}{\\partial \\boldsymbol{\\beta}_r} = \\sum_{i=1}^n (y_{ir} - \\pi_{ir})\\mathbf{x}_i - \\lambda \\boldsymbol{\\beta}_r\n$$\nThe full gradient vector $\\mathbf{g} = \\nabla \\ell_\\lambda$ is formed by stacking these results: $\\mathbf{g} = [\\mathbf{g}_1^\\top, \\mathbf{g}_2^\\top]^\\top$.\n\n#### 2.2. Hessian of the Penalized Log-Likelihood\n\nThe Hessian matrix $H_\\lambda = \\nabla^2 \\ell_\\lambda$ is the matrix of second derivatives. We compute the block of the Hessian corresponding to $\\boldsymbol{\\beta}_r$ and $\\boldsymbol{\\beta}_{r'}$.\nThe second derivative of the log-likelihood is:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta}_r \\partial \\boldsymbol{\\beta}_{r'}^\\top} = -\\sum_{i=1}^n \\pi_{ir}(\\delta_{rr'} - \\pi_{ir'}) \\mathbf{x}_i \\mathbf{x}_i^\\top\n$$\nwhere $\\delta_{rr'}$ is the Kronecker delta.\n\nThe second derivative of the penalty term contributes $-\\lambda I_{p(K-1)}$ to the Hessian, where $p=2$, $K=3$. So it adds $-\\lambda$ to the diagonal elements.\n\nThe full Hessian matrix $H_\\lambda$ is a $4 \\times 4$ block matrix:\n$$\nH_\\lambda = \\begin{pmatrix} H_{11}  H_{12} \\\\ H_{21}  H_{22} \\end{pmatrix} - \\lambda I_4\n$$\nwhere the blocks ($p \\times p$, so $2 \\times 2$) are:\n-   $H_{11} = \\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta}_1 \\partial \\boldsymbol{\\beta}_1^\\top} = -\\sum_{i=1}^n \\pi_{i1}(1-\\pi_{i1}) \\mathbf{x}_i \\mathbf{x}_i^\\top$\n-   $H_{22} = \\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta}_2 \\partial \\boldsymbol{\\beta}_2^\\top} = -\\sum_{i=1}^n \\pi_{i2}(1-\\pi_{i2}) \\mathbf{x}_i \\mathbf{x}_i^\\top$\n-   $H_{12} = H_{21}^\\top = \\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta}_1 \\partial \\boldsymbol{\\beta}_2^\\top} = \\sum_{i=1}^n \\pi_{i1}\\pi_{i2} \\mathbf{x}_i \\mathbf{x}_i^\\top$\n\nFor any $\\lambda > 0$, the matrix $-H_\\lambda$ is positive definite, ensuring that $H_\\lambda$ is invertible and the Newton-Raphson step is well-defined.\n\n#### 2.3. Newton-Raphson Algorithm\n\nStarting with an initial estimate $\\boldsymbol{\\beta}_{\\text{flat}}^{(0)} = \\mathbf{0}$, the algorithm iteratively refines the estimate using the update rule:\n$$\n\\boldsymbol{\\beta}_{\\text{flat}}^{(t+1)} = \\boldsymbol{\\beta}_{\\text{flat}}^{(t)} - (H_\\lambda^{(t)})^{-1} \\mathbf{g}^{(t)}\n$$\nwhere $\\mathbf{g}^{(t)}$ and $H_\\lambda^{(t)}$ are the gradient and Hessian evaluated at $\\boldsymbol{\\beta}_{\\text{flat}}^{(t)}$. In practice, the update step $\\Delta \\boldsymbol{\\beta}^{(t)} = - (H_\\lambda^{(t)})^{-1} \\mathbf{g}^{(t)}$ is found by solving the linear system $H_\\lambda^{(t)} \\Delta \\boldsymbol{\\beta}^{(t)} = -\\mathbf{g}^{(t)}$. The iteration continues until the Euclidean norm of the step, $\\lVert \\Delta \\boldsymbol{\\beta}^{(t)} \\rVert_2$, falls below a specified tolerance.\n\n#### 2.4. Implementation Details\n\n-   **Dataset Construction:** The two datasets (`\"sep\"` and `\"ctrl\"`) are constructed as specified. The separated dataset will have $Y_i=2$ for all six observations where $x_i=1$, and $Y_i \\in \\{0, 1\\}$ for all six observations where $x_i=-1$. The control dataset will have outcomes from all three categories for both $x_i=1$ and $x_i=-1$.\n-   **Stable Softmax:** To compute $\\pi_{ik}$, we first find $m_i = \\max(0, \\eta_{i1}, \\eta_{i2})$ for each observation $i$. Then, the probabilities are computed as $\\pi_{ik} = \\frac{\\exp(\\eta_{ik} - m_i)}{\\sum_{j=0}^2 \\exp(\\eta_{ij} - m_i)}$, which avoids numerical overflow from large positive values of $\\eta_{ij}$.\n-   **Algorithm:** A function will implement the Newton-Raphson loop. Within the loop, it will:\n    1.  Compute the linear predictors $\\eta_{i1}, \\eta_{i2}$ for all $i$.\n    2.  Compute the probability matrix $\\boldsymbol{\\Pi}$ of size $n \\times 3$ using the stable softmax.\n    3.  Compute the $4 \\times 1$ gradient vector $\\mathbf{g}$.\n    4.  Compute the $4 \\times 4$ Hessian matrix $H_\\lambda$. Vectorized operations will be used for efficiency.\n    5.  Solve for the update step $\\Delta \\boldsymbol{\\beta}$ and update the parameters.\n    6.  Check for convergence.\n-   **Final Output:** The algorithm returns the absolute value of the coefficient for the predictor $x$ for category $2$, which is $|\\beta_{2,x}|$. This corresponds to the element at index `[1, 1]` of the $2 \\times 2$ parameter matrix $\\boldsymbol{\\beta}$. This procedure is run for each of the five test cases. The results are formatted as requested.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to construct datasets, run the multinomial ridge regression\n    for each test case, and print the results.\n    \"\"\"\n\n    def create_datasets():\n        \"\"\"Constructs the separated and control datasets.\"\"\"\n        datasets = {}\n\n        # 1. Separated dataset (\"sep\")\n        # n=12, x in {-1, 1}. Y=2 iff x=1. Y in {0,1} if x=-1.\n        X_sep = np.ones((12, 2))\n        X_sep[:6, 1] = -1.0\n        X_sep[6:, 1] = 1.0\n        # For x=-1 (first 6 obs), 3 are Y=0, 3 are Y=1\n        # For x=1 (last 6 obs), all 6 are Y=2\n        Y_sep = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2], dtype=int)\n        datasets[\"sep\"] = (X_sep, Y_sep)\n\n        # 2. Control dataset (\"ctrl\")\n        # n=12, x in {-1, 1}. Y=2 appears for both x values.\n        X_ctrl = np.ones((12, 2))\n        X_ctrl[:6, 1] = -1.0\n        X_ctrl[6:, 1] = 1.0\n        # For x=-1: 2 Y=0, 2 Y=1, 2 Y=2\n        # For x=1: 2 Y=0, 2 Y=1, 2 Y=2\n        Y_ctrl = np.array([0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2], dtype=int)\n        datasets[\"ctrl\"] = (X_ctrl, Y_ctrl)\n        \n        return datasets\n\n    def fit_multinomial_ridge(X, Y, lambda_val, max_iter=100, tol=1e-8):\n        \"\"\"\n        Fits a multinomial logistic regression model with L2 penalty using Newton-Raphson.\n\n        Args:\n            X (np.ndarray): Design matrix of shape (n, p) with intercept.\n            Y (np.ndarray): Outcome vector of shape (n,).\n            lambda_val (float): Ridge penalty strength.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance for the norm of the update step.\n\n        Returns:\n            float: The absolute value of the slope coefficient for category 2, |beta_2x|.\n        \"\"\"\n        n, p = X.shape\n        K = len(np.unique(Y))  # Number of categories, K=3\n\n        # Convert Y to one-hot encoding for K categories\n        Y_indicator = np.zeros((n, K))\n        Y_indicator[np.arange(n), Y] = 1\n\n        # Initialize parameters beta for K-1 categories. Shape is ((K-1), p) = (2, 2).\n        # beta[0, :] are coefficients for category 1\n        # beta[1, :] are coefficients for category 2\n        beta = np.zeros((K - 1, p))\n\n        for _ in range(max_iter):\n            # 1. Compute linear predictors and probabilities\n            # eta has shape (n, K-1)\n            eta = X @ beta.T\n            \n            # Stable softmax calculation\n            # Add a column of zeros for the baseline category eta_0\n            eta_full = np.hstack([np.zeros((n, 1)), eta])\n            \n            # log-sum-exp trick to prevent overflow\n            m = np.max(eta_full, axis=1, keepdims=True)\n            stable_exp_eta = np.exp(eta_full - m)\n            denom = np.sum(stable_exp_eta, axis=1, keepdims=True)\n            pi = stable_exp_eta / denom\n            \n            # 2. Compute gradient of the penalized log-likelihood\n            # residual has shape (n, K-1)\n            residual = Y_indicator[:, 1:] - pi[:, 1:]\n            # grad_unpenalized has shape ((K-1), p)\n            grad_unpenalized = residual.T @ X\n            grad = grad_unpenalized - lambda_val * beta\n            \n            # 3. Compute Hessian of the penalized log-likelihood\n            # The full Hessian is a p(K-1) x p(K-1) = 4x4 matrix\n            \n            # Efficient vectorized calculation of Hessian blocks\n            # H_rr' = -sum_i pi_ir*(delta_rr' - pi_ir') * x_i*x_i'\n            d1 = pi[:, 1] * (1 - pi[:, 1])\n            H11 = -(X.T * d1) @ X\n            \n            d2 = pi[:, 2] * (1 - pi[:, 2])\n            H22 = -(X.T * d2) @ X\n            \n            d12 = pi[:, 1] * pi[:, 2]\n            H12 = (X.T * d12) @ X\n            \n            Hess = np.block([[H11, H12], [H12.T, H22]])\n            \n            # Add penalty term to the Hessian\n            Hess -= lambda_val * np.eye(p * (K - 1))\n            \n            # 4. Perform Newton-Raphson update\n            # Flatten gradient and beta for the linear system\n            # Flattening order must be consistent: (beta_10, beta_1x, beta_20, beta_2x)\n            grad_flat = grad.flatten()\n            \n            # Solve H * step = -g for the step\n            try:\n                step = np.linalg.solve(Hess, -grad_flat)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular (should not happen for lambda > 0)\n                step = np.linalg.pinv(Hess) @ -grad_flat\n\n            # Update beta\n            beta_flat = beta.flatten()\n            beta = (beta_flat + step).reshape((K - 1, p))\n            \n            # 5. Check for convergence\n            step_norm = np.linalg.norm(step)\n            if step_norm  tol:\n                break\n        \n        # The required coefficient is beta_{2,x}, which is at beta[1, 1]\n        beta_2x_abs = np.abs(beta[1, 1])\n        \n        return beta_2x_abs\n\n    # --- Main execution ---\n    datasets = create_datasets()\n    \n    test_cases = [\n        (\"sep\", 1.0),\n        (\"sep\", 0.1),\n        (\"sep\", 0.01),\n        (\"sep\", 0.001),\n        (\"ctrl\", 0.001),\n    ]\n    \n    results = []\n    for dataset_id, lambda_val in test_cases:\n        X, Y = datasets[dataset_id]\n        result = fit_multinomial_ridge(X, Y, lambda_val)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4929787"}]}