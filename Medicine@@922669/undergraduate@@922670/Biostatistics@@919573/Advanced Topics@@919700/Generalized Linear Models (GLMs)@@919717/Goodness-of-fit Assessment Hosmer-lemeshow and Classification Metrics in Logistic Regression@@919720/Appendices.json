{"hands_on_practices": [{"introduction": "The first step in evaluating any statistical model is often to examine its residuals—the discrepancies between observed outcomes and model predictions. In logistic regression, deviance residuals provide a powerful tool for this purpose, acting as a measure of \"surprise\" for each data point. This exercise provides direct practice in calculating these residuals from first principles, helping to build an intuition for how they quantify the disagreement between the model's fitted probabilities and the actual binary outcomes. [@problem_id:4914512]", "problem": "A biostatistics study uses logistic regression to model a binary outcome $Y_i \\in \\{0,1\\}$ given covariates $x_i$, yielding fitted probabilities $\\hat{p}_i = \\Pr(Y_i=1 \\mid x_i)$ for $i=1,\\dots,n$. Consider $n=8$ independent observations with outcomes and fitted probabilities\n- $(y_1,\\hat{p}_1)=(1,0.8)$,\n- $(y_2,\\hat{p}_2)=(0,0.8)$,\n- $(y_3,\\hat{p}_3)=(1,0.2)$,\n- $(y_4,\\hat{p}_4)=(0,0.2)$,\n- $(y_5,\\hat{p}_5)=(1,0.95)$,\n- $(y_6,\\hat{p}_6)=(0,0.95)$,\n- $(y_7,\\hat{p}_7)=(1,0.05)$,\n- $(y_8,\\hat{p}_8)=(0,0.05)$.\n\nStarting from the Bernoulli log-likelihood and the definition of the model deviance as twice the difference between the saturated and fitted log-likelihoods, derive the individual deviance residuals $r_i$ for each observation $i=1,\\dots,8$. Then, compute the root-mean-square of the deviance residuals across the $8$ observations. Briefly interpret the magnitudes of the residuals in terms of goodness-of-fit, referring to how they relate to calibration assessments such as the Hosmer–Lemeshow goodness-of-fit test (HL) and to classification metrics such as accuracy and the Receiver Operating Characteristic (ROC) curve. Round your final numerical answer (the root-mean-square deviance residual) to four significant figures.", "solution": "The user wants me to validate the following problem and, if it is valid, provide a solution.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n-   A logistic regression model is used for a binary outcome $Y_i \\in \\{0,1\\}$ given covariates $x_i$.\n-   The fitted probabilities are $\\hat{p}_i = \\Pr(Y_i=1 \\mid x_i)$ for $i=1,\\dots,n$.\n-   The number of independent observations is $n=8$.\n-   The observed outcomes and fitted probabilities are:\n    -   $(y_1,\\hat{p}_1)=(1,0.8)$\n    -   $(y_2,\\hat{p}_2)=(0,0.8)$\n    -   $(y_3,\\hat{p}_3)=(1,0.2)$\n    -   $(y_4,\\hat{p}_4)=(0,0.2)$\n    -   $(y_5,\\hat{p}_5)=(1,0.95)$\n    -   $(y_6,\\hat{p}_6)=(0,0.95)$\n    -   $(y_7,\\hat{p}_7)=(1,0.05)$\n    -   $(y_8,\\hat{p}_8)=(0,0.05)$\n-   The task is to:\n    1.  Derive the individual deviance residuals $r_i$ for each observation.\n    2.  Compute the root-mean-square of the deviance residuals.\n    3.  Briefly interpret the magnitudes of the residuals in relation to goodness-of-fit (calibration, Hosmer-Lemeshow test) and classification metrics (accuracy, ROC curve).\n    4.  Round the final numerical answer to four significant figures.\n\n#### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is firmly grounded in the statistical theory of generalized linear models, specifically logistic regression. Deviance residuals, the Hosmer-Lemeshow test, and classification metrics are standard tools for model assessment in this context. The premises and concepts are factually sound.\n2.  **Well-Posed**: The problem provides all necessary data and definitions to calculate the requested quantities. The question is unambiguous and leads to a unique numerical answer and a standard interpretation.\n3.  **Objective**: The problem is stated in precise, objective language. It requests calculations and interpretations based on established statistical principles, not subjective opinions.\n4.  **Completeness and Consistency**: The provided data are complete and internally consistent. There are no contradictions.\n5.  **Relevance**: The problem is directly relevant to the topic of goodness-of-fit assessment and classification metrics in logistic regression within biostatistics.\n\n#### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed, scientifically sound exercise in applying and interpreting diagnostic measures for a logistic regression model. I will proceed with the solution.\n\n### Solution Derivation\n\nThe analysis begins with the log-likelihood function for a single observation $i$ from a Bernoulli distribution, which is the basis for logistic regression. Given the binary outcome $y_i \\in \\{0, 1\\}$ and the model's predicted probability $p_i = \\Pr(Y_i=1)$, the log-likelihood is:\n$$\nl(p_i; y_i) = y_i \\ln(p_i) + (1-y_i)\\ln(1-p_i)\n$$\nThe total log-likelihood for the fitted model across $n$ observations is the sum of individual log-likelihoods, evaluated at the fitted probabilities $\\hat{p}_i$:\n$$\nL(\\hat{\\mathbf{p}}) = \\sum_{i=1}^n \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right]\n$$\nA saturated model is a model that fits the data perfectly. For binary data, this is achieved by setting the predicted probability for each observation equal to its observed outcome, i.e., $\\hat{p}_i^{\\text{sat}} = y_i$. The log-likelihood of the saturated model is:\n$$\nL(\\mathbf{y}) = \\sum_{i=1}^n \\left[ y_i \\ln(y_i) + (1-y_i)\\ln(1-y_i) \\right]\n$$\nFor any $y_i \\in \\{0,1\\}$, the term inside the sum is $1\\ln(1) + (1-1)\\ln(0)$ or $0\\ln(0) + (1-0)\\ln(1)$. Since $\\lim_{x\\to 0} x\\ln(x) = 0$, each term in the sum is $0$. Therefore, the log-likelihood of the saturated model is $L(\\mathbf{y}) = 0$.\n\nThe model deviance, $D$, is defined as twice the difference between the log-likelihood of the saturated model and the fitted model. It measures the extent to which the fitted model deviates from the perfect fit of the saturated model.\n$$\nD = 2[L(\\mathbf{y}) - L(\\hat{\\mathbf{p}})] = 2[0 - L(\\hat{\\mathbf{p}})] = -2 \\sum_{i=1}^n \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right]\n$$\nThe total deviance $D$ is the sum of the individual deviance components, $d_i$, for each observation, where $D = \\sum_{i=1}^n d_i$. The individual deviance component is given by:\n$$\nd_i = -2 \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right]\n$$\nThe deviance residual, $r_i$, is the signed square root of the individual deviance component. The sign is determined by the sign of the raw residual, $y_i - \\hat{p}_i$. This ensures that the residual is positive for an unexpected success ($y_i=1$ with small $\\hat{p}_i$) and negative for an unexpected failure ($y_i=0$ with large $\\hat{p}_i$).\n$$\nr_i = \\text{sign}(y_i - \\hat{p}_i) \\sqrt{d_i}\n$$\nWe can write this out for the two possible outcomes:\n-   If $y_i=1$: $d_i = -2 \\ln(\\hat{p}_i)$. Since $0  \\hat{p}_i  1$, $\\text{sign}(1-\\hat{p}_i) = +1$. The residual is $r_i = \\sqrt{-2\\ln(\\hat{p}_i)}$.\n-   If $y_i=0$: $d_i = -2 \\ln(1-\\hat{p}_i)$. Since $0  \\hat{p}_i  1$, $\\text{sign}(0-\\hat{p}_i) = -1$. The residual is $r_i = -\\sqrt{-2\\ln(1-\\hat{p}_i)}$.\n\nNow, we apply these formulas to the $n=8$ observations provided.\n\n-   For $(y_1, \\hat{p}_1) = (1, 0.8)$: $r_1 = \\sqrt{-2\\ln(0.8)} \\approx \\sqrt{-2(-0.22314)} \\approx \\sqrt{0.44629} \\approx 0.6680$\n-   For $(y_2, \\hat{p}_2) = (0, 0.8)$: $r_2 = -\\sqrt{-2\\ln(1-0.8)} = -\\sqrt{-2\\ln(0.2)} \\approx -\\sqrt{-2(-1.60944)} \\approx -\\sqrt{3.21888} \\approx -1.7941$\n-   For $(y_3, \\hat{p}_3) = (1, 0.2)$: $r_3 = \\sqrt{-2\\ln(0.2)} \\approx \\sqrt{3.21888} \\approx 1.7941$\n-   For $(y_4, \\hat{p}_4) = (0, 0.2)$: $r_4 = -\\sqrt{-2\\ln(1-0.2)} = -\\sqrt{-2\\ln(0.8)} \\approx -\\sqrt{0.44629} \\approx -0.6680$\n-   For $(y_5, \\hat{p}_5) = (1, 0.95)$: $r_5 = \\sqrt{-2\\ln(0.95)} \\approx \\sqrt{-2(-0.05129)} \\approx \\sqrt{0.10259} \\approx 0.3203$\n-   For $(y_6, \\hat{p}_6) = (0, 0.95)$: $r_6 = -\\sqrt{-2\\ln(1-0.95)} = -\\sqrt{-2\\ln(0.05)} \\approx -\\sqrt{-2(-2.99573)} \\approx -\\sqrt{5.99146} \\approx -2.4477$\n-   For $(y_7, \\hat{p}_7) = (1, 0.05)$: $r_7 = \\sqrt{-2\\ln(0.05)} \\approx \\sqrt{5.99146} \\approx 2.4477$\n-   For $(y_8, \\hat{p}_8) = (0, 0.05)$: $r_8 = -\\sqrt{-2\\ln(1-0.05)} = -\\sqrt{-2\\ln(0.95)} \\approx -\\sqrt{0.10259} \\approx -0.3203$\n\nThe next step is to compute the root-mean-square (RMS) of these deviance residuals. The RMS is defined as:\n$$\n\\text{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n r_i^2}\n$$\nSince $r_i^2 = d_i$, the sum $\\sum_{i=1}^n r_i^2$ is simply the total deviance $D$. So, $\\text{RMS} = \\sqrt{D/n}$.\nLet's compute the total deviance $D$ by summing the individual squared residuals ($d_i=r_i^2$):\n$D = r_1^2+r_2^2+r_3^2+r_4^2+r_5^2+r_6^2+r_7^2+r_8^2$\n$D = (-2\\ln(0.8)) + (-2\\ln(0.2)) + (-2\\ln(0.2)) + (-2\\ln(0.8)) + (-2\\ln(0.95)) + (-2\\ln(0.05)) + (-2\\ln(0.05)) + (-2\\ln(0.95))$\n$D = -4(\\ln(0.8) + \\ln(0.2) + \\ln(0.95) + \\ln(0.05))$\n$D \\approx 0.44629 + 3.21888 + 3.21888 + 0.44629 + 0.10259 + 5.99146 + 5.99146 + 0.10259 \\approx 19.5184$\n\nWith $n=8$, the RMS is:\n$$\n\\text{RMS} = \\sqrt{\\frac{19.5184}{8}} \\approx \\sqrt{2.4398} \\approx 1.5620499\n$$\nRounding to four significant figures, the RMS of the deviance residuals is $1.562$.\n\n### Interpretation\nThe magnitudes of the deviance residuals provide insight into the model's goodness-of-fit at both the individual and aggregate levels. For a well-fitting model, deviance residuals are approximately standard normal, so values with magnitude greater than $2$ are considered large and indicative of poor fit for that specific observation.\n\n-   Observations $1$, $4$, $5$, and $8$ have small residuals ($|r_i|  1$), indicating the model's predictions were reasonable for these outcomes (e.g., for obs. 1, $\\hat{p}_1=0.8$ and $y_1=1$).\n-   Observations $2$ and $3$ have moderately large residuals ($|r_i| \\approx 1.79$), indicating a notable discrepancy between prediction and outcome (e.g., for obs. 3, the model predicted a low probability $\\hat{p}_3=0.2$ for an event that occurred, $y_3=1$).\n-   Observations $6$ and $7$ have large residuals ($|r_i| \\approx 2.45$), highlighting a severe lack of fit. For these cases, the model was very confident in its prediction (e.g., $\\hat{p}_6=0.95$), but the outcome was the opposite of what was expected ($y_6=0$). These points are \"surprising\" to the model and contribute heavily to the overall deviance.\n\n**Relation to Calibration and Hosmer-Lemeshow (HL) Test:**\nGoodness-of-fit assessment focuses heavily on **calibration**, which is a model's ability to produce predicted probabilities that match the observed event rates. The HL test formalizes this by grouping subjects by predicted risk and comparing observed to expected event counts in each group. The large residuals we observe, especially for observations $6$ and $7$, signify poor calibration. The model predicts probabilities near the boundaries ($0.05$ and $0.95$) where the opposite outcomes occurred. If these observations were part of an HL test, they would create large discrepancies between observed and expected counts in the corresponding risk groups, leading to a large HL chi-square statistic and a small p-value, indicating that the model is poorly calibrated. The large overall RMS deviance residual ($1.562$) also suggests a global lack of fit.\n\n**Relation to Classification Metrics (Accuracy, ROC Curve):**\nClassification metrics assess **discrimination**, the model's ability to distinguish between the two outcome classes ($Y=1$ vs. $Y=0$). This is distinct from calibration.\n-   **Accuracy:** Using a standard threshold of $0.5$, the model correctly classifies only $4$ out of $8$ observations (correct: $1,4,5,8$; incorrect: $2,3,6,7$), for an accuracy of $50\\%$. This is no better than random guessing.\n-   **ROC Curve:** The ROC curve is generated by plotting the true positive rate against the false positive rate at all possible thresholds. The area under this curve (AUC) quantifies discrimination. The set of predicted probabilities for the subjects with $y=1$ is $\\{\\hat{p}_1, \\hat{p}_3, \\hat{p}_5, \\hat{p}_7\\} = \\{0.8, 0.2, 0.95, 0.05\\}$. The set for subjects with $y=0$ is $\\{\\hat{p}_2, \\hat{p}_4, \\hat{p}_6, \\hat{p}_8\\} = \\{0.8, 0.2, 0.95, 0.05\\}$. The distributions of predicted probabilities for the two classes are identical. This means the model has absolutely no ability to discriminate between cases and controls. The ROC curve would lie on the diagonal line of no discrimination, and the AUC would be exactly $0.5$.\n\nIn conclusion, the deviance residuals reveal a model with a severe pathology. The large magnitudes point to poor calibration (a failure in goodness-of-fit), and the specific structure of the data shows a complete failure of discrimination as well. The model is effectively useless for both prediction and classification.", "answer": "$$\n\\boxed{1.562}\n$$", "id": "4914512"}, {"introduction": "While residuals help identify poor fits for individual observations, a crucial global property of a prediction model is its calibration. A well-calibrated model produces probabilities that are trustworthy; for example, among a group of patients assigned a predicted risk of 0.2, the observed proportion of events should be close to 0.2. This hands-on problem demystifies the widely used Hosmer-Lemeshow test by guiding you through its step-by-step calculation, providing a concrete understanding of how it formally assesses model calibration. [@problem_id:4914555]", "problem": "A biostatistics team fits a logistic regression model to a binary clinical outcome $y_{i} \\in \\{0,1\\}$ for $n=20$ patients and obtains predicted probabilities $\\hat{p}_{i} \\in (0,1)$. The model is evaluated using goodness-of-fit and classification quality metrics derived from first principles.\n\nStarting from the Bernoulli likelihood for independent observations and its logarithm, define the entropy-based measure of classification quality as the negative average log-likelihood per observation. Also consider the mean squared error between predicted probabilities and outcomes. Use these foundations to compute both metrics for the dataset below and to discuss their local relationship via quadratic approximation.\n\nIn addition, assess model calibration using the Hosmer–Lemeshow (HL) goodness-of-fit statistic by grouping patients into $G=5$ risk groups of equal size based on sorting by $\\hat{p}_{i}$, and by comparing observed to expected counts of events within each group. Use the canonical HL grouping procedure and the Pearson-type grouping statistic.\n\nThe dataset is given as ordered pairs $(\\hat{p}_{i}, y_{i})$ for patients $i=1,\\dots,20$:\n$(0.05,0)$, $(0.08,0)$, $(0.10,0)$, $(0.12,1)$, $(0.18,0)$, $(0.22,0)$, $(0.25,1)$, $(0.28,0)$, $(0.32,0)$, $(0.35,1)$, $(0.45,1)$, $(0.50,0)$, $(0.55,1)$, $(0.60,1)$, $(0.65,0)$, $(0.70,1)$, $(0.78,1)$, $(0.82,1)$, $(0.88,1)$, $(0.92,1)$.\n\nTasks:\n- Derive, from the Bernoulli likelihood, the entropy-based negative average log-likelihood per observation and compute its value for the dataset.\n- Define the Brier score as the mean squared difference between $\\hat{p}_{i}$ and $y_{i}$ and compute its value for the dataset.\n- Using a second-order Taylor expansion around correct classification, explain the local relationship between the entropy-based measure and the Brier score.\n- Compute the Hosmer–Lemeshow statistic for $G=5$ equal-sized groups (sorted by $\\hat{p}_{i}$), showing expected and observed counts and the contribution of each group to the statistic.\n\nFinally, report the single quantity $R$ defined as the ratio of the entropy-based negative average log-likelihood to the Brier score. Round $R$ to four significant figures. No units are required, and express any intermediate proportions or probabilities in decimal form rather than as percentages.", "solution": "The problem is valid as it is scientifically grounded in established biostatistical principles, well-posed with all necessary data and clear objectives, and expressed in precise, objective language.\n\n### Part 1: Entropy-Based Negative Average Log-Likelihood\n\nThe model is for a binary outcome $y_i \\in \\{0, 1\\}$ for $i=1, \\dots, n$ independent patients. The predicted probability of the event ($y_i=1$) is $\\hat{p}_i$. The probability of observing the outcome $y_i$ is given by the Bernoulli probability mass function:\n$$ P(Y_i = y_i | \\hat{p}_i) = \\hat{p}_i^{y_i} (1-\\hat{p}_i)^{1-y_i} $$\nFor $n$ independent observations, the total likelihood is the product of the individual probabilities:\n$$ L(\\{\\hat{p}_i\\}; \\{y_i\\}) = \\prod_{i=1}^{n} \\hat{p}_i^{y_i} (1-\\hat{p}_i)^{1-y_i} $$\nThe log-likelihood, $\\ell$, is the natural logarithm of the likelihood:\n$$ \\ell = \\ln(L) = \\sum_{i=1}^{n} \\ln\\left( \\hat{p}_i^{y_i} (1-\\hat{p}_i)^{1-y_i} \\right) = \\sum_{i=1}^{n} [y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i)] $$\nThe entropy-based measure of classification quality, which we denote as $E$, is defined as the negative average log-likelihood per observation. This is also known as the cross-entropy loss.\n$$ E = -\\frac{1}{n} \\ell = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i)] $$\nTo compute its value for the given dataset, we have $n=20$. We separate the sum into two parts: one for observations where $y_i=1$ and one for $y_i=0$.\n\nFor cases with $y_i=1$ ($11$ observations):\n$$ \\sum_{y_i=1} \\ln(\\hat{p}_i) = \\ln(0.12) + \\ln(0.25) + \\ln(0.35) + \\ln(0.45) + \\ln(0.55) + \\ln(0.60) + \\ln(0.70) + \\ln(0.78) + \\ln(0.82) + \\ln(0.88) + \\ln(0.92) \\approx -7.478345 $$\nFor cases with $y_i=0$ ($9$ observations):\n$$ \\sum_{y_i=0} \\ln(1-\\hat{p}_i) = \\ln(1-0.05) + \\ln(1-0.08) + \\ln(1-0.10) + \\ln(1-0.18) + \\ln(1-0.22) + \\ln(1-0.28) + \\ln(1-0.32) + \\ln(1-0.50) + \\ln(1-0.65) \\approx -3.144083 $$\nThe total log-likelihood is $\\ell \\approx -7.478345 - 3.144083 = -10.622428$.\nThe entropy-based measure is:\n$$ E = -\\frac{1}{20} (-10.622428) \\approx 0.5311214 $$\n\n### Part 2: Brier Score (Mean Squared Error)\n\nThe Brier score, $BS$, is defined as the mean squared difference between the predicted probabilities $\\hat{p}_i$ and the actual outcomes $y_i$.\n$$ BS = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{p}_i)^2 $$\nTo compute its value for the dataset ($n=20$):\nFor cases with $y_i=1$:\n$$ \\sum_{y_i=1} (1-\\hat{p}_i)^2 = (1-0.12)^2 + (1-0.25)^2 + \\dots + (1-0.92)^2 = 0.88^2 + 0.75^2 + 0.65^2 + 0.55^2 + 0.45^2 + 0.40^2 + 0.30^2 + 0.22^2 + 0.18^2 + 0.12^2 + 0.08^2 = 2.616 $$\nFor cases with $y_i=0$:\n$$ \\sum_{y_i=0} (0-\\hat{p}_i)^2 = (0.05)^2 + (0.08)^2 + \\dots + (0.65)^2 = 0.05^2 + 0.08^2 + 0.10^2 + 0.18^2 + 0.22^2 + 0.28^2 + 0.32^2 + 0.50^2 + 0.65^2 = 0.953 $$\nThe total sum of squares is $2.616 + 0.953 = 3.569$.\nThe Brier score is:\n$$ BS = \\frac{1}{20} (3.569) = 0.17845 $$\n\n### Part 3: Local Relationship between Entropy and Brier Score\n\nLet's analyze the per-observation loss for the entropy measure, $E_i$, and the Brier score, $BS_i$.\n$$ E_i = -[y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i)] $$\n$$ BS_i = (y_i - \\hat{p}_i)^2 $$\nWe examine the local behavior of $E_i$ around \"correct classification,\" which means the predicted probability $\\hat{p}_i$ is very close to the true outcome $y_i$. Let the deviation be $\\epsilon = |y_i - \\hat{p}_i|$, where $\\epsilon \\to 0^+$.\n\nCase 1: $y_i=1$. Correct classification implies $\\hat{p}_i \\to 1$. Let $\\hat{p}_i = 1-\\epsilon$.\nThe entropy loss is $E_i = -\\ln(1-\\epsilon)$.\nThe Brier score is $BS_i = (1 - (1-\\epsilon))^2 = \\epsilon^2$.\nUsing the second-order Taylor expansion of $-\\ln(1-\\epsilon)$ around $\\epsilon=0$:\n$$ E_i = - \\left( -\\epsilon - \\frac{\\epsilon^2}{2} - O(\\epsilon^3) \\right) = \\epsilon + \\frac{\\epsilon^2}{2} + O(\\epsilon^3) $$\nCase 2: $y_i=0$. Correct classification implies $\\hat{p}_i \\to 0$. Let $\\hat{p}_i = \\epsilon$.\nThe entropy loss is $E_i = -\\ln(1-\\epsilon)$.\nThe Brier score is $BS_i = (0 - \\epsilon)^2 = \\epsilon^2$.\nThe Taylor expansion is identical:\n$$ E_i = \\epsilon + \\frac{\\epsilon^2}{2} + O(\\epsilon^3) $$\nIn both cases, for a small deviation $\\epsilon = |y_i - \\hat{p}_i|$, we have $BS_i = \\epsilon^2$. Substituting $\\epsilon = \\sqrt{BS_i}$ into the expansion for $E_i$:\n$$ E_i \\approx \\sqrt{BS_i} + \\frac{1}{2} BS_i $$\nThis relationship shows that for predictions very close to the true outcome, the entropy loss is dominated by a term linear in $\\epsilon$ ($\\sqrt{BS_i}$), while the Brier score is quadratic ($\\epsilon^2$). This means the entropy loss penalizes small prediction errors more severely than the Brier score.\n\n### Part 4: Hosmer–Lemeshow Statistic\n\nWe sort patients by $\\hat{p}_i$ and form $G=5$ groups of equal size, $n_g = 20/5=4$. For each group $g$, we calculate the observed number of events ($O_g = \\sum y_i$) and the expected number of events ($E_g = \\sum \\hat{p}_i$). The Hosmer-Lemeshow statistic $H$ is a Pearson-type chi-squared statistic calculated from these counts:\n$$ H = \\sum_{g=1}^{G} \\left[ \\frac{(O_g - E_g)^2}{E_g} + \\frac{((n_g - O_g) - (n_g - E_g))^2}{n_g - E_g} \\right] $$\nThe calculations are summarized in the following table.\n\n| Group (g) | Patients (by index $i$) | Predicted Probabilities ($\\hat{p}_i$) | Outcomes ($y_i$) | $n_g$ | $O_g$ | $E_g$ | $O_g-E_g$ | Contribution to $H$ |\n| :---: | :---: | :--- | :--- | :---: | :---: | :---: | :---: | :---: |\n| 1 | 1-4 | $0.05, 0.08, 0.10, 0.12$ | $0, 0, 0, 1$ | 4 | 1 | 0.35 | 0.65 | $\\frac{0.65^2}{0.35} + \\frac{(-0.65)^2}{3.65} \\approx 1.32290$ |\n| 2 | 5-8 | $0.18, 0.22, 0.25, 0.28$ | $0, 0, 1, 0$ | 4 | 1 | 0.93 | 0.07 | $\\frac{0.07^2}{0.93} + \\frac{(-0.07)^2}{3.07} \\approx 0.00687$ |\n| 3 | 9-12 | $0.32, 0.35, 0.45, 0.50$ | $0, 1, 1, 0$ | 4 | 2 | 1.62 | 0.38 | $\\frac{0.38^2}{1.62} + \\frac{(-0.38)^2}{2.38} \\approx 0.14981$ |\n| 4 | 13-16 | $0.55, 0.60, 0.65, 0.70$ | $1, 1, 0, 1$ | 4 | 3 | 2.50 | 0.50 | $\\frac{0.50^2}{2.50} + \\frac{(-0.50)^2}{1.50} \\approx 0.26667$ |\n| 5 | 17-20 | $0.78, 0.82, 0.88, 0.92$ | $1, 1, 1, 1$ | 4 | 4 | 3.40 | 0.60 | $\\frac{0.60^2}{3.40} + \\frac{(-0.60)^2}{0.60} \\approx 0.70588$ |\n\nSumming the contributions from each group:\n$$ H \\approx 1.32290 + 0.00687 + 0.14981 + 0.26667 + 0.70588 \\approx 2.45213 $$\n\n### Part 5: Final Ratio Calculation\n\nThe final task is to compute the ratio $R$ of the entropy-based negative average log-likelihood to the Brier score.\n$$ R = \\frac{E}{BS} = \\frac{0.5311214}{0.17845} \\approx 2.976296 $$\nRounding to four significant figures, we get:\n$$ R \\approx 2.976 $$", "answer": "$$\n\\boxed{2.976}\n$$", "id": "4914555"}, {"introduction": "After assessing a model's calibration and goodness-of-fit, we often want a single summary statistic that quantifies its overall performance, analogous to the $R^{2}$ coefficient of determination in linear regression. In logistic regression, this is achieved with \"pseudo $R^{2}$\" measures, which compare the likelihood of the fitted model to a baseline null model. This practice explores the derivation and interpretation of the popular Cox-Snell and Nagelkerke $R^{2}$ measures, clarifying why the latter is often preferred for its intuitive 0-to-1 scale. [@problem_id:4914507]", "problem": "A hospital biostatistics team fits a logistic regression model for a binary outcome indicating whether a patient experiences a postoperative complication. They estimate both an intercept-only model (the null model) and a predictor-rich model (the fitted model) with Maximum Likelihood Estimation (MLE). Assume independent Bernoulli outcomes. The sample size is $n=200$, the null-model log-likelihood is $\\ell_{0}=-138.4$, and the fitted-model log-likelihood is $\\ell_{m}=-124.1$.\n\nStarting from the independent Bernoulli likelihood definition $L=\\prod_{i=1}^{n} p_{i}^{y_{i}}(1-p_{i})^{1-y_{i}}$ and its logarithm, derive expressions for the Cox–Snell pseudo $R^{2}$ and the Nagelkerke pseudo $R^{2}$ in terms of $n$, $\\ell_{0}$, and $\\ell_{m}$. Explain, using these definitions and the concept of the saturated model, why Cox–Snell’s measure is not bounded above by $1$ in logistic regression, whereas Nagelkerke’s measure is.\n\nThen, using your derived formulas and the given values of $n$, $\\ell_{0}$, and $\\ell_{m}$, compute the Nagelkerke pseudo $R^{2}$. Round your final numerical answer to four significant figures and express it as a decimal (do not use a percentage sign).", "solution": "The problem requires the derivation and explanation of two pseudo $R^2$ measures used in logistic regression, followed by a numerical calculation. The problem is scientifically grounded, well-posed, and contains all necessary information. It is therefore deemed valid.\n\nThe foundation of the analysis is the likelihood function for $n$ independent Bernoulli trials. The likelihood $L$ given the observed outcomes $y_i \\in \\{0, 1\\}$ and the model-predicted probabilities $p_i$ is:\n$$L = \\prod_{i=1}^{n} p_{i}^{y_{i}}(1-p_{i})^{1-y_{i}}$$\nThe log-likelihood, denoted by $\\ell$, is the natural logarithm of the likelihood:\n$$\\ell = \\ln(L) = \\sum_{i=1}^{n} [y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]$$\nThe problem provides the maximized log-likelihood for two models: the null model (intercept-only), $\\ell_0 = -138.4$, and the fitted model, $\\ell_m = -124.1$, for a sample of size $n=200$.\n\nFirst, we derive the expressions for the Cox–Snell and Nagelkerke pseudo $R^2$ measures. These measures are designed to mimic the coefficient of determination, $R^2$, from linear regression by comparing the likelihood of the fitted model ($L_m$) to that of the null model ($L_0$).\n\nThe Cox–Snell pseudo $R^2$, denoted $R^2_{CS}$, is defined as:\n$$R^2_{CS} = 1 - \\left(\\frac{L_0}{L_m}\\right)^{2/n}$$\nTo express this in terms of the given log-likelihoods, we use the property that $\\ln(L_0/L_m) = \\ell_0 - \\ell_m$. Exponentiating both sides gives $L_0/L_m = \\exp(\\ell_0 - \\ell_m)$. Substituting this into the definition of $R^2_{CS}$:\n$$R^2_{CS} = 1 - \\left(\\exp(\\ell_0 - \\ell_m)\\right)^{2/n} = 1 - \\exp\\left(\\frac{2(\\ell_0 - \\ell_m)}{n}\\right)$$\nThis is the required expression for the Cox–Snell pseudo $R^2$ in terms of $n$, $\\ell_0$, and $\\ell_m$.\n\nNext, we address why the Cox–Snell measure is not bounded above by $1$. The upper bound of any pseudo $R^2$ measure is reached when the fitted model is a \"saturated model,\" which is a model that fits the data perfectly. For $n$ independent Bernoulli outcomes, a saturated model would have as many parameters as observations, achieving a perfect fit. In such a model, the predicted probability $p_i$ for each observation $y_i$ would be $p_i=y_i$. The likelihood for an individual observation is $p_i^{y_i}(1-p_i)^{1-y_i}$. If $y_i=1$, the term is $1^1(0)^0$. If $y_i=0$, the term is $0^0(1)^1$. Using the convention that $0^0=1$, the likelihood for each observation in a saturated model is $1$. Therefore, the total likelihood for the saturated model is $L_{sat} = \\prod_{i=1}^{n} 1 = 1$. The corresponding log-likelihood is $\\ell_{sat} = \\ln(1) = 0$.\n\nThe maximum possible value of $L_m$ is $L_{sat} = 1$. Substituting this into the definition of $R^2_{CS}$ gives the maximum attainable value for this measure:\n$$\\max(R^2_{CS}) = 1 - \\left(\\frac{L_0}{L_{sat}}\\right)^{2/n} = 1 - \\left(\\frac{L_0}{1}\\right)^{2/n} = 1 - (L_0)^{2/n}$$\nSince the null model (which assumes a single probability for all outcomes) does not fit the data perfectly, its likelihood $L_0$ is strictly less than $1$. Consequently, $(L_0)^{2/n}$ is a positive value, which implies that $\\max(R^2_{CS})  1$. This is why the Cox–Snell measure is not bounded above by $1$; its achievable maximum is data-dependent and always less than $1$.\n\nThe Nagelkerke pseudo $R^2$, denoted $R^2_N$, was proposed to correct this limitation by normalizing the Cox–Snell measure by its maximum possible value:\n$$R^2_N = \\frac{R^2_{CS}}{\\max(R^2_{CS})} = \\frac{1 - (L_0/L_m)^{2/n}}{1 - (L_0)^{2/n}}$$\nBy this construction, when the fitted model is the saturated model ($L_m = 1$), the numerator becomes equal to the denominator, and $R^2_N = 1$. Thus, the Nagelkerke measure is bounded above by $1$, with a range of $[0, 1]$.\n\nTo derive the expression for $R^2_N$ in terms of log-likelihoods, we substitute the exponential forms:\n$$(L_0/L_m)^{2/n} = \\exp\\left(\\frac{2(\\ell_0 - \\ell_m)}{n}\\right)$$\n$$(L_0)^{2/n} = (\\exp(\\ell_0))^{2/n} = \\exp\\left(\\frac{2\\ell_0}{n}\\right)$$\nSubstituting these into the formula for $R^2_N$ yields:\n$$R^2_N = \\frac{1 - \\exp\\left(\\frac{2(\\ell_0 - \\ell_m)}{n}\\right)}{1 - \\exp\\left(\\frac{2\\ell_0}{n}\\right)}$$\nThis is the required expression for the Nagelkerke pseudo $R^2$.\n\nFinally, we compute the numerical value of $R^2_N$ using the provided data: $n=200$, $\\ell_0=-138.4$, and $\\ell_m=-124.1$.\n\nFirst, we calculate the terms in the exponents:\n- Numerator exponent term: $\\frac{2(\\ell_0 - \\ell_m)}{n} = \\frac{2(-138.4 - (-124.1))}{200} = \\frac{2(-14.3)}{200} = \\frac{-28.6}{200} = -0.143$\n- Denominator exponent term: $\\frac{2\\ell_0}{n} = \\frac{2(-138.4)}{200} = \\frac{-276.8}{200} = -1.384$\n\nNow, substitute these values into the formula for $R^2_N$:\n$$R^2_N = \\frac{1 - \\exp(-0.143)}{1 - \\exp(-1.384)}$$\nWe evaluate the exponential functions:\n$$\\exp(-0.143) \\approx 0.8667528$$\n$$\\exp(-1.384) \\approx 0.2505705$$\nSubstituting these numerical values back into the expression for $R^2_N$:\n$$R^2_N \\approx \\frac{1 - 0.8667528}{1 - 0.2505705} = \\frac{0.1332472}{0.7494295} \\approx 0.1778005$$\nRounding the result to four significant figures gives $0.1778$.", "answer": "$$\\boxed{0.1778}$$", "id": "4914507"}]}