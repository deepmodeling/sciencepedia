{"hands_on_practices": [{"introduction": "The core principle of Analysis of Variance is the partitioning of total variation in a dataset into distinct, meaningful sources. This first-principles exercise guides you through the algebraic foundation of a two-way ANOVA in a balanced design. By deriving the sum of squares for the main effects and their interaction [@problem_id:4963601], you will prove the orthogonal decomposition of variance and gain a deep, structural understanding of how the model separates these independent sources of variability.", "problem": "A biostatistics experiment investigates the effect of two categorical factors on a continuous biomarker response, within a balanced two-way Analysis of Variance (ANOVA) with interaction framework. Factor $A$ has $a$ levels ($i=1,\\dots,a$), factor $B$ has $b$ levels ($j=1,\\dots,b$), and there are $r$ independent replicates per $(i,j)$ cell ($k=1,\\dots,r$). Let the observed response be $Y_{ijk}$, and define the following sample means:\n- The cell mean $\\bar{Y}_{ij\\cdot} = \\frac{1}{r}\\sum_{k=1}^{r} Y_{ijk}$,\n- The factor $A$ marginal mean $\\bar{Y}_{i\\cdot\\cdot} = \\frac{1}{br}\\sum_{j=1}^{b}\\sum_{k=1}^{r} Y_{ijk}$,\n- The factor $B$ marginal mean $\\bar{Y}_{\\cdot j\\cdot} = \\frac{1}{ar}\\sum_{i=1}^{a}\\sum_{k=1}^{r} Y_{ijk}$,\n- The grand mean $\\bar{Y}_{\\cdot\\cdot\\cdot} = \\frac{1}{abr}\\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r} Y_{ijk}$.\n\nConsider the standard two-way ANOVA decomposition with interaction that partitions the total sum of squares into components attributable to factor $A$, factor $B$, the $A\\times B$ interaction, and error (within-cell). Starting from the fundamental definition of total variation as the sum of squared deviations from the grand mean,\n$$SS_{T} = \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r}\\left(Y_{ijk}-\\bar{Y}_{\\cdot\\cdot\\cdot}\\right)^{2},$$\nand the within-cell variation\n$$SS_{E} = \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r}\\left(Y_{ijk}-\\bar{Y}_{ij\\cdot}\\right)^{2},$$\nderive closed-form expressions, in terms of $a$, $b$, $r$, $\\bar{Y}_{i\\cdot\\cdot}$, $\\bar{Y}_{\\cdot j\\cdot}$, $\\bar{Y}_{ij\\cdot}$, and $\\bar{Y}_{\\cdot\\cdot\\cdot}$, for the sums of squares attributable to factor $A$ ($SS_{A}$), factor $B$ ($SS_{B}$), and their interaction ($SS_{AB}$) in this balanced design. Then, verify the additivity (orthogonal decomposition) of the sums of squares under the balanced design by showing that\n$$SS_{T} = SS_{A} + SS_{B} + SS_{AB} + SS_{E}.$$\nNo numerical computation is required; provide the requested expressions in closed form. If any simplification is performed, retain exact algebraic forms; no rounding is needed.", "solution": "The problem requires the derivation of the sums of squares for the main effects ($SS_{A}$, $SS_{B}$) and the interaction effect ($SS_{AB}$) in a balanced two-way ANOVA, and verification of their additivity. The derivation proceeds by partitioning the total sum of squares ($SS_{T}$) into its constituent components.\n\nFirst, we establish the fundamental identity by decomposing the total deviation of an observation $Y_{ijk}$ from the grand mean $\\bar{Y}_{\\cdot\\cdot\\cdot}$ into two parts: the deviation of the observation from its cell mean ($\\bar{Y}_{ij\\cdot}$), and the deviation of the cell mean from the grand mean.\n$$ Y_{ijk} - \\bar{Y}_{\\cdot\\cdot\\cdot} = (Y_{ijk} - \\bar{Y}_{ij\\cdot}) + (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) $$\nThe total sum of squares, $SS_T$, is defined as:\n$$ SS_{T} = \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r} (Y_{ijk} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 $$\nSubstituting the identity into this definition and expanding the square:\n$$ SS_{T} = \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r} \\left[ (Y_{ijk} - \\bar{Y}_{ij\\cdot}) + (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) \\right]^2 $$\n$$ SS_{T} = \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r} (Y_{ijk} - \\bar{Y}_{ij\\cdot})^2 + \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 + 2 \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r} (Y_{ijk} - \\bar{Y}_{ij\\cdot})(\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) $$\nThe first term is, by definition, the sum of squares for error, $SS_{E}$. Let's examine the cross-product term:\n$$ 2 \\sum_{i=1}^{a}\\sum_{j=1}^{b} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) \\left[ \\sum_{k=1}^{r} (Y_{ijk} - \\bar{Y}_{ij\\cdot}) \\right] $$\nBy definition of the cell mean $\\bar{Y}_{ij\\cdot} = \\frac{1}{r}\\sum_{k=1}^{r} Y_{ijk}$, the inner sum is:\n$$ \\sum_{k=1}^{r} Y_{ijk} - \\sum_{k=1}^{r} \\bar{Y}_{ij\\cdot} = r\\bar{Y}_{ij\\cdot} - r\\bar{Y}_{ij\\cdot} = 0 $$\nThus, the cross-product term is zero. This demonstrates the orthogonality of the within-cell and between-cell variations.\nThe second term can be simplified since the summand is constant with respect to the index $k$:\n$$ \\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{r} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 = r \\sum_{i=1}^{a}\\sum_{j=1}^{b} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 $$\nThis term is known as the sum of squares between cells, let's call it $SS_{cells}$. So we have shown that:\n$$ SS_{T} = SS_{E} + SS_{cells} $$\n\nNext, we partition the deviation of a cell mean from the grand mean, $(\\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})$, to isolate the main effects and the interaction effect. The identity is:\n$$ \\bar{Y}_{ij\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot} = (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) + (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) + (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}) $$\nThe three terms on the right-hand side represent the main effect of factor $A$, the main effect of factor $B$, and the interaction effect of $A \\times B$, respectively.\nWe now substitute this decomposition into the expression for $SS_{cells}$:\n$$ SS_{cells} = r \\sum_{i=1}^{a}\\sum_{j=1}^{b} \\left[ (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) + (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) + (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}) \\right]^2 $$\nExpanding the square of this three-term expression results in three squared terms and three cross-product terms.\n\nThe squared terms are:\n1. $r \\sum_{i=1}^{a}\\sum_{j=1}^{b} (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 = r \\sum_{i=1}^{a} b (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 = br \\sum_{i=1}^{a} (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2$. This is defined as $SS_{A}$.\n2. $r \\sum_{i=1}^{a}\\sum_{j=1}^{b} (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 = r \\sum_{j=1}^{b} a (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2 = ar \\sum_{j=1}^{b} (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2$. This is defined as $SS_{B}$.\n3. $r \\sum_{i=1}^{a}\\sum_{j=1}^{b} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot})^2$. This is defined as $SS_{AB}$.\n\nThe cross-product terms must sum to zero for the decomposition to be orthogonal.\n1. Cross-product of A and B effects:\n$2r \\sum_{i=1}^{a}\\sum_{j=1}^{b} (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})(\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) = 2r \\left[ \\sum_{i=1}^{a} (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) \\right] \\left[ \\sum_{j=1}^{b} (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) \\right] $.\nFor a balanced design, $\\frac{1}{b}\\sum_{j=1}^{b} \\bar{Y}_{\\cdot j\\cdot} = \\bar{Y}_{\\cdot\\cdot\\cdot}$, so $\\sum_{j=1}^{b} (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) = (\\sum_{j=1}^{b} \\bar{Y}_{\\cdot j\\cdot}) - b\\bar{Y}_{\\cdot\\cdot\\cdot} = b\\bar{Y}_{\\cdot\\cdot\\cdot} - b\\bar{Y}_{\\cdot\\cdot\\cdot} = 0$. Thus, this cross-product term is zero.\n\n2. Cross-product of A and AB effects:\n$2r \\sum_{i=1}^{a}\\sum_{j=1}^{b} (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})(\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}) = 2r \\sum_{i=1}^{a} (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) \\left[ \\sum_{j=1}^{b} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}) \\right]$.\nThe inner sum over $j$ is: $\\sum_{j=1}^{b}\\bar{Y}_{ij\\cdot} - \\sum_{j=1}^{b}\\bar{Y}_{i\\cdot\\cdot} - \\sum_{j=1}^{b}\\bar{Y}_{\\cdot j\\cdot} + \\sum_{j=1}^{b}\\bar{Y}_{\\cdot\\cdot\\cdot}$. Using the mean definitions for a balanced design ($\\bar{Y}_{i\\cdot\\cdot} = \\frac{1}{b}\\sum_{j=1}^{b}\\bar{Y}_{ij\\cdot}$ and $\\bar{Y}_{\\cdot\\cdot\\cdot} = \\frac{1}{b}\\sum_{j=1}^{b}\\bar{Y}_{\\cdot j\\cdot}$), this becomes $(b\\bar{Y}_{i\\cdot\\cdot}) - (b\\bar{Y}_{i\\cdot\\cdot}) - (b\\bar{Y}_{\\cdot\\cdot\\cdot}) + (b\\bar{Y}_{\\cdot\\cdot\\cdot}) = 0$. This cross-product term is also zero.\n\n3. Cross-product of B and AB effects:\nA symmetric argument shows this term is also zero. $2r \\sum_{j=1}^{b} (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}) \\left[ \\sum_{i=1}^{a} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}) \\right]$.\nThe inner sum over $i$ is: $\\sum_{i=1}^{a}\\bar{Y}_{ij\\cdot} - \\sum_{i=1}^{a}\\bar{Y}_{i\\cdot\\cdot} - \\sum_{i=1}^{a}\\bar{Y}_{\\cdot j\\cdot} + \\sum_{i=1}^{a}\\bar{Y}_{\\cdot\\cdot\\cdot} = (a\\bar{Y}_{\\cdot j\\cdot}) - (a\\bar{Y}_{\\cdot\\cdot\\cdot}) - (a\\bar{Y}_{\\cdot j\\cdot}) + (a\\bar{Y}_{\\cdot\\cdot\\cdot}) = 0$.\n\nSince all cross-product terms are zero, the decomposition of $SS_{cells}$ is orthogonal:\n$$ SS_{cells} = SS_{A} + SS_{B} + SS_{AB} $$\nThe derived closed-form expressions are therefore:\n- For factor $A$: $SS_{A} = br \\sum_{i=1}^{a} (\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2$\n- For factor $B$: $SS_{B} = ar \\sum_{j=1}^{b} (\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot})^2$\n- For interaction $A \\times B$: $SS_{AB} = r \\sum_{i=1}^{a} \\sum_{j=1}^{b} (\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot})^2$\n\nFinally, to verify the additivity, we substitute the decomposition of $SS_{cells}$ back into the decomposition of $SS_T$:\n$$ SS_{T} = SS_{E} + SS_{cells} = SS_{E} + (SS_{A} + SS_{B} + SS_{AB}) $$\nRearranging the terms confirms the required identity:\n$$ SS_{T} = SS_{A} + SS_{B} + SS_{AB} + SS_{E} $$\nThis completes the derivation and verification.", "answer": "$$\n\\boxed{\\begin{pmatrix} br \\sum_{i=1}^{a} \\left(\\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}\\right)^{2} & ar \\sum_{j=1}^{b} \\left(\\bar{Y}_{\\cdot j\\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}\\right)^{2} & r \\sum_{i=1}^{a} \\sum_{j=1}^{b} \\left(\\bar{Y}_{ij\\cdot} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j\\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}\\right)^{2} \\end{pmatrix}}\n$$", "id": "4963601"}, {"introduction": "After identifying a statistically significant interaction, the next question is, 'How large is this effect in practical terms?' An F-test alone doesn't answer this; we need a measure of effect size. This practice problem [@problem_id:4963624] focuses on partial eta squared ($\\eta_{p}^{2}$), a common metric for quantifying the magnitude of an effect in ANOVA. You will calculate its value and, crucially, learn to interpret it as the proportion of variability associated with the interaction after the main effects have been accounted for.", "problem": "A biostatistician analyzes a randomized dietary intervention in which participants are assigned to one of $3$ diets (factor $A$ with levels $A_1$, $A_2$, $A_3$) and classified by genotype (factor $B$ with levels $B_1$, $B_2$). The primary outcome is low-density lipoprotein cholesterol (LDL-C) measured after $12$ weeks. The design is balanced with $n = 20$ participants per cell. A two-way Analysis of Variance (ANOVA) with interaction is fitted under the standard linear model $y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}$, with independent, identically distributed errors $\\varepsilon_{ijk}$ having mean $0$ and common variance $\\sigma^2$. The ANOVA summary reports the following sums of squares (SS) and degrees of freedom (df):\n- Factor $A$: $SS_A = 480$, $df_A = 2$.\n- Factor $B$: $SS_B = 300$, $df_B = 1$.\n- Interaction $A \\times B$: $SS_{AB} = 180$, $df_{AB} = 2$.\n- Error: $SS_E = 720$, $df_E = 114$.\n\nUsing these outputs, select the option that both correctly defines the partial eta squared for the interaction effect, denoted $\\eta^2_{AB}$, and provides a scientifically appropriate interpretation of its meaning in this biostatistical application together with its numerical value.\n\nA. $\\eta^2_{AB}$ is defined as $SS_{AB}/(SS_{AB} + SS_E)$, which quantifies the proportion of variability attributable to the $A \\times B$ interaction relative to the variability not accounted for by other effects (i.e., the interaction plus residual error), holding the main effects constant. Here, $\\eta^2_{AB} = 180/(180+720) = 0.20$. In this study, about $20\\%$ of the variability in LDL-C remaining after accounting for the main effects of diet and genotype is associated with their interaction.\n\nB. $\\eta^2_{AB}$ is defined as $SS_{AB}/SS_T$, where $SS_T$ is the total sum of squares across all sources. Here, $\\eta^2_{AB} = 180/(480+300+180+720) \\approx 0.11$. In this study, about $11\\%$ of the total variability in LDL-C is explained by the interaction.\n\nC. $\\eta^2_{AB}$ is defined as $SS_{AB}/(SS_A + SS_B + SS_{AB})$. Here, $\\eta^2_{AB} = 180/(480+300+180) = 0.1875$. In this study, about $18.75\\%$ of the model-explained variability is due to the interaction.\n\nD. $\\eta^2_{AB}$ is defined as $MS_{AB}/(MS_{AB} + MS_E)$, where $MS$ denotes mean squares. Here, $MS_{AB} = 180/2 = 90$ and $MS_E = 720/114 \\approx 6.32$, so $\\eta^2_{AB} \\approx 90/(90+6.32) \\approx 0.93$. In this study, about $93\\%$ of the adjusted variance (accounting for degrees of freedom) is due to the interaction, indicating a very large effect independent of sample size.", "solution": "We begin from the standard two-way Analysis of Variance (ANOVA) framework and variance partitioning. Under the linear model $y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}$ with independent, identically distributed errors $\\varepsilon_{ijk}$ having mean $0$ and variance $\\sigma^2$, the total variability can be decomposed into orthogonal components:\n$$SS_T = SS_A + SS_B + SS_{AB} + SS_E.$$\nHere $SS_A$, $SS_B$, and $SS_{AB}$ quantify the variability attributable to the main effects of $A$ and $B$ and their interaction, respectively, while $SS_E$ quantifies the residual (error) variability.\n\nEffect size metrics in ANOVA are designed to express the magnitude of a term independent of the units of measurement and less sensitive to sample size than statistical significance. Partial eta squared for a specific effect (here, the interaction) is defined conceptually as the proportion of variation attributable to that focal effect relative to the variation not attributed to other model terms. For the interaction $A \\times B$, “partialing out” the main effects $A$ and $B$ leads to a denominator that includes the interaction plus the residual error, reflecting the variability remaining when the other effects are held constant. Therefore, the partial eta squared for the interaction is\n$$\\eta^2_{AB} = \\frac{SS_{AB}}{SS_{AB} + SS_E}.$$\nWith the reported values $SS_{AB} = 180$ and $SS_E = 720$, we compute\n$$\\eta^2_{AB} = \\frac{180}{180 + 720} = \\frac{180}{900} = 0.20.$$\nInterpretation in biostatistical applications: $\\eta^2_{AB} = 0.20$ indicates that, after accounting for the main effects of diet and genotype, approximately $20\\%$ of the remaining variability (the combination of interaction plus residual error) in LDL-C is associated with the diet-by-genotype interaction. This provides a measure of the practical importance of the interaction effect, complementing inferential statistics such as the $F$-test.\n\nWe now evaluate each option:\n\nA. Definition: $SS_{AB}/(SS_{AB} + SS_E)$. This matches the conceptual derivation above, wherein partial eta squared for the interaction compares the interaction sum of squares to the sum of the interaction and error sums of squares. Computation: $180/(180+720) = 180/900 = 0.20$, which is correct. Interpretation: It states that about $20\\%$ of the variability in LDL-C remaining after accounting for the main effects is associated with the interaction, which aligns with the proper interpretation of partial eta squared as a proportion of interaction-plus-error variance while main effects are held constant. Verdict: Correct.\n\nB. Definition: $SS_{AB}/SS_T$. This uses the total sum of squares in the denominator and corresponds to the “eta squared” for the interaction with respect to total variance, not the “partial” version that conditions on other effects. Computation: $180/(480+300+180+720) = 180/1680 \\approx 0.1071$, which is the wrong quantity for partial eta squared. Interpretation: It claims a proportion of total variability explained by the interaction, which is not the partial effect size defined for ANOVA with multiple terms. Verdict: Incorrect.\n\nC. Definition: $SS_{AB}/(SS_A + SS_B + SS_{AB})$. This compares the interaction sum of squares only to the model-explained variability, omitting the residual error in the denominator. This is not the standard definition of partial eta squared. Computation: $180/(480+300+180) = 180/960 = 0.1875$, which again is not the partial eta squared value. Interpretation: It refers to the proportion of model-explained variance due to the interaction, which may resemble a different descriptive ratio but is not partial eta squared. Verdict: Incorrect.\n\nD. Definition: $MS_{AB}/(MS_{AB} + MS_E)$. While mean squares $MS_{AB} = SS_{AB}/df_{AB}$ and $MS_E = SS_E/df_E$ are central to inferential testing via the $F$-statistic, partial eta squared is defined using sums of squares, not mean squares. Because $df_{AB} \\neq df_E$, the ratio $MS_{AB}/(MS_{AB}+MS_E)$ does not equal $SS_{AB}/(SS_{AB}+SS_E)$. Computation: $MS_{AB} = 180/2 = 90$, $MS_E = 720/114 \\approx 6.32$, yielding $90/(90+6.32) \\approx 0.934$, an inflated and incorrect value for partial eta squared. Interpretation: It incorrectly suggests a very large effect and incorrectly attributes invariance to sample size; moreover, partial eta squared is not defined via mean squares. Verdict: Incorrect.\n\nTherefore, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "4963624"}, {"introduction": "A significant omnibus F-test for an interaction tells us that the effect of one factor changes across the levels of another, but it doesn't reveal the specific pattern of that change. To answer more focused scientific questions, we use planned contrasts. This hands-on exercise [@problem_id:4963571] demonstrates how to translate a specific research hypothesis into a mathematical contrast, test it, and interpret the results, allowing you to dissect the overall interaction effect into its meaningful components.", "problem": "A biostatistics team investigates whether the effect of a dietary intervention factor $A$ with two levels ($A_{1}$: standard diet; $A_{2}$: high-fiber diet) on a continuous inflammatory biomarker depends on the metabolic status factor $B$ with three levels ($B_{1}$: normoglycemic; $B_{2}$: prediabetic; $B_{3}$: diabetic). A balanced two-way layout was used with $n=10$ independent subjects in each cell. The experiment is modeled by the standard fixed-effects two-way Analysis of Variance (ANOVA) with interaction:\n$$\ny_{ijk} \\;=\\; \\mu \\;+\\; \\alpha_{i} \\;+\\; \\beta_{j} \\;+\\; (\\alpha\\beta)_{ij} \\;+\\; \\varepsilon_{ijk},\n$$\nwhere $i \\in \\{1,2\\}$, $j \\in \\{1,2,3\\}$, $k \\in \\{1,\\dots,10\\}$, and $\\varepsilon_{ijk}$ are independent normal errors with mean $0$ and common variance $\\sigma^{2}$.\n\nFrom the study, the observed cell means (in arbitrary biomarker units) are:\n- $A_{1}$ at $B_{1}$: $\\bar{y}_{11\\cdot} = 12.0$, $A_{2}$ at $B_{1}$: $\\bar{y}_{21\\cdot} = 10.0$,\n- $A_{1}$ at $B_{2}$: $\\bar{y}_{12\\cdot} = 13.0$, $A_{2}$ at $B_{2}$: $\\bar{y}_{22\\cdot} = 11.0$,\n- $A_{1}$ at $B_{3}$: $\\bar{y}_{13\\cdot} = 16.0$, $A_{2}$ at $B_{3}$: $\\bar{y}_{23\\cdot} = 11.0$.\n\nA preliminary two-way ANOVA yields the pooled within-cell mean square (error mean square) $MSE = 4.0$ with $df_{E} = 54$.\n\nConstruct and justify a planned interaction contrast that tests whether the difference between $A_{1}$ and $A_{2}$ at $B_{1}$ differs from the average of the $A_{1}$ versus $A_{2}$ differences at $B_{2}$ and $B_{3}$. Then, using the provided data and the fixed-effects model assumptions, compute the corresponding $F$-statistic for this contrast. Round your final answer to four significant figures.", "solution": "The problem asks for the construction and testing of a specific planned interaction contrast in a two-way ANOVA setting. The validity of the problem has been confirmed, as all necessary data are provided, the experimental setup is standard and scientifically sound, and the question is well-posed.\n\nLet $a$ be the number of levels for factor $A$ and $b$ be the number of levels for factor $B$. Here, $a=2$ and $b=3$. The number of replicates per cell is $n=10$. The model for an observation is given by:\n$$\ny_{ijk} \\;=\\; \\mu \\;+\\; \\alpha_{i} \\;+\\; \\beta_{j} \\;+\\; (\\alpha\\beta)_{ij} \\;+\\; \\varepsilon_{ijk}\n$$\nwhere $i \\in \\{1,2\\}$, $j \\in \\{1,2,3\\}$, $k \\in \\{1,\\dots,10\\}$. The expected value for an observation in the cell $(i,j)$ is $E[y_{ijk}] = \\mu_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij}$.\n\nThe problem requires us to construct a contrast that tests whether the effect of factor $A$ (the difference between levels $A_1$ and $A_2$) at level $B_1$ is different from the average of the effects of factor $A$ at levels $B_2$ and $B_3$.\n\nThe effect of factor $A$ at a specific level $j$ of factor $B$ can be measured by the difference in true cell means, $\\mu_{1j} - \\mu_{2j}$.\nThe effect of $A$ at $B_1$ is $\\mu_{11} - \\mu_{21}$.\nThe effect of $A$ at $B_2$ is $\\mu_{12} - \\mu_{22}$.\nThe effect of $A$ at $B_3$ is $\\mu_{13} - \\mu_{23}$.\n\nThe average of the effects of $A$ at levels $B_2$ and $B_3$ is $\\frac{(\\mu_{12} - \\mu_{22}) + (\\mu_{13} - \\mu_{23})}{2}$.\n\nThe contrast, denoted by the parameter $\\Psi$, represents the comparison between the effect at $B_1$ and the average effect at $B_2$ and $B_3$:\n$$\n\\Psi = (\\mu_{11} - \\mu_{21}) - \\frac{(\\mu_{12} - \\mu_{22}) + (\\mu_{13} - \\mu_{23})}{2}\n$$\nThe null hypothesis for testing this contrast is $H_0: \\Psi = 0$.\n\nTo express $\\Psi$ as a linear combination of the cell means, $\\Psi = \\sum_{i=1}^{2} \\sum_{j=1}^{3} c_{ij} \\mu_{ij}$, we expand the expression:\n$$\n\\Psi = 1 \\cdot \\mu_{11} - 1 \\cdot \\mu_{21} - \\frac{1}{2} \\mu_{12} + \\frac{1}{2} \\mu_{22} - \\frac{1}{2} \\mu_{13} + \\frac{1}{2} \\mu_{23}\n$$\nThe coefficients $c_{ij}$ are:\n$c_{11} = 1$, $c_{12} = -1/2$, $c_{13} = -1/2$\n$c_{21} = -1$, $c_{22} = 1/2$, $c_{23} = 1/2$\n\nThis is an interaction contrast. This is justified by showing that the coefficients sum to zero across each row (for each level of factor $A$) and down each column (for each level of factor $B$):\nSum for row $1$ ($A_1$): $\\sum_{j} c_{1j} = 1 - \\frac{1}{2} - \\frac{1}{2} = 0$.\nSum for row $2$ ($A_2$): $\\sum_{j} c_{2j} = -1 + \\frac{1}{2} + \\frac{1}{2} = 0$.\nSum for column $1$ ($B_1$): $\\sum_{i} c_{i1} = 1 - 1 = 0$.\nSum for column $2$ ($B_2$): $\\sum_{i} c_{i2} = -\\frac{1}{2} + \\frac{1}{2} = 0$.\nSum for column $3$ ($B_3$): $\\sum_{i} c_{i3} = -\\frac{1}{2} + \\frac{1}{2} = 0$.\nSince these conditions hold, the contrast is independent of the main effects and is a component of the interaction effect.\n\nThe point estimate for $\\Psi$ is the linear combination of the sample cell means, denoted by $L$:\n$$\nL = \\sum_{i=1}^{2} \\sum_{j=1}^{3} c_{ij} \\bar{y}_{ij\\cdot}\n$$\nSubstituting the given cell means:\n$\\bar{y}_{11\\cdot} = 12.0$, $\\bar{y}_{21\\cdot} = 10.0$\n$\\bar{y}_{12\\cdot} = 13.0$, $\\bar{y}_{22\\cdot} = 11.0$\n$\\bar{y}_{13\\cdot} = 16.0$, $\\bar{y}_{23\\cdot} = 11.0$\n$$\nL = (1)(12.0) - (1)(10.0) - \\frac{1}{2}(13.0) + \\frac{1}{2}(11.0) - \\frac{1}{2}(16.0) + \\frac{1}{2}(11.0)\n$$\n$$\nL = (12.0 - 10.0) - \\frac{1}{2}(13.0 - 11.0) - \\frac{1}{2}(16.0 - 11.0)\n$$\n$$\nL = 2.0 - \\frac{1}{2}(2.0) - \\frac{1}{2}(5.0) = 2.0 - 1.0 - 2.5 = -1.5\n$$\nThe value of the estimated contrast is $L = -1.5$.\n\nTo compute the $F$-statistic, we first need the sum of squares for the contrast, $SS_L$. For a balanced design ($n_{ij} = n$ for all $i,j$), the formula is:\n$$\nSS_L = \\frac{n L^2}{\\sum_{i=1}^{2} \\sum_{j=1}^{3} c_{ij}^2}\n$$\nFirst, we compute the sum of the squared coefficients:\n$$\n\\sum_{i} \\sum_{j} c_{ij}^2 = (1)^2 + (-1)^2 + \\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2\n$$\n$$\n\\sum_{i} \\sum_{j} c_{ij}^2 = 1 + 1 + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} = 2 + 1 = 3\n$$\nNow we can compute $SS_L$ with $n=10$, $L=-1.5$, and $\\sum c_{ij}^2 = 3$:\n$$\nSS_L = \\frac{10 \\times (-1.5)^2}{3} = \\frac{10 \\times 2.25}{3} = \\frac{22.5}{3} = 7.5\n$$\nThe sum of squares for this contrast is $SS_L = 7.5$.\n\nA single contrast has $df_L = 1$ degree of freedom. The mean square for the contrast is $MS_L = \\frac{SS_L}{df_L} = \\frac{7.5}{1} = 7.5$.\n\nThe $F$-statistic for testing $H_0: \\Psi = 0$ is the ratio of the mean square for the contrast to the mean square error ($MSE$) from the ANOVA table:\n$$\nF = \\frac{MS_L}{MSE}\n$$\nThe problem provides $MSE = 4.0$ and its degrees of freedom $df_E = 54$.\n$$\nF = \\frac{7.5}{4.0} = 1.875\n$$\nThe calculated $F$-statistic has degrees of freedom $(1, 54)$. The value $1.875$ has four significant figures as requested.", "answer": "$$\n\\boxed{1.875}\n$$", "id": "4963571"}]}