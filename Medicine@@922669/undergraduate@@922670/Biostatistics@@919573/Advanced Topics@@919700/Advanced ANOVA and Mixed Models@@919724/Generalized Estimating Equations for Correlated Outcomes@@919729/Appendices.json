{"hands_on_practices": [{"introduction": "A key component of Generalized Estimating Equations (GEE) is the \"working\" correlation matrix, which captures the assumed dependence among repeated measurements within a cluster. This first practice focuses on the first-order autoregressive, or AR(1), structure, a common and intuitive choice for longitudinal data where observations closer in time are more strongly correlated. This exercise will guide you through the derivation of the AR(1) matrix, solidifying the connection between the theoretical concept of a time-dependent process and its practical matrix representation used in GEE [@problem_id:4913842].", "problem": "A longitudinal cohort study measures a continuous biomarker for each subject at $4$ equally spaced clinic visits, indexed by times $t \\in \\{0,1,2,3\\}$. To fit a marginal mean model using Generalized Estimating Equations (GEE), a working correlation matrix is required to capture within-subject correlation across visits. Consider modeling the within-subject dependence using a first-order autoregressive (AR(1)) mechanism that is compatible with weak stationarity and the Markov property over equally spaced times. Assume the following fundamental conditions for a centered process: for each subject, define $X_{t} = Y_{t} - \\mu$, and suppose $X_{t}$ satisfies $X_{t} = \\rho X_{t-1} + \\epsilon_{t}$ where $|\\rho|<1$, $\\{\\epsilon_{t}\\}$ is a white noise process with mean $0$, variance $\\sigma_{\\epsilon}^{2}$, and is uncorrelated across time and with past values of $X_{t}$. \n\nUsing only these assumptions and the definition of the autocovariance function under weak stationarity, justify why an AR(1) working correlation is appropriate for equally spaced longitudinal measurements and derive the implied working correlation matrix for a single subject with $4$ visits as a function of the unknown parameter $\\rho$. Provide your final answer as a single closed-form analytic expression for the $4 \\times 4$ working correlation matrix in terms of $\\rho$. Do not round your answer.", "solution": "The task is to justify the appropriateness of a first-order autoregressive, AR(1), working correlation structure for equally spaced longitudinal data and to derive the corresponding $4 \\times 4$ working correlation matrix, $R(\\rho)$.\n\nFirst, we address the justification. An AR(1) model is particularly well-suited for longitudinal data collected at equally spaced time intervals for several reasons.\n1.  It posits that the measurement at a given time $t$ is a function of the measurement at the immediately preceding time $t-1$, plus random noise. This reflects the Markov property, which is a reasonable simplifying assumption for many biological or physical processes where the immediate past state is the most salient predictor of the current state.\n2.  The model implies that the correlation between two measurements decays exponentially as the time lag between them increases. For repeated measures on a subject, it is highly plausible that measurements taken closer in time (e.g., at visit $1$ and visit $2$) are more strongly correlated than measurements taken further apart in time (e.g., at visit $1$ and visit $4$). The AR(1) structure mathematically formalizes this intuitive decay.\n3.  The assumption of weak stationarity, implied by the GEE working correlation framework and the constraint $|\\rho|<1$, means that the mean, variance, and autocorrelation structure do not change over time. This provides a stable and parsimonious model for the within-subject dependence, characterized by a single parameter, $\\rho$.\n\nNext, we derive the working correlation matrix. The correlation matrix $R$ for a single subject has elements $R_{ij} = \\text{Corr}(Y_i, Y_j)$, where $Y_i$ and $Y_j$ are the outcomes at time indices $i$ and $j$ from the set $\\{0, 1, 2, 3\\}$. Let the matrix indices run from $1$ to $4$, corresponding to times $t=0, 1, 2, 3$. Thus, the element $R_{ij}$ corresponds to $\\text{Corr}(Y_{i-1}, Y_{j-1})$.\n\nGiven the centered process $X_t = Y_t - \\mu$, the covariance and variance properties of $Y_t$ are identical to those of $X_t$. Specifically, $\\text{Var}(Y_t) = \\text{Var}(X_t)$ and $\\text{Cov}(Y_i, Y_j) = \\text{Cov}(X_i, X_j)$. The correlation is given by:\n$$\n\\text{Corr}(Y_i, Y_j) = \\frac{\\text{Cov}(Y_i, Y_j)}{\\sqrt{\\text{Var}(Y_i)\\text{Var}(Y_j)}} = \\frac{\\text{Cov}(X_i, X_j)}{\\sqrt{\\text{Var}(X_i)\\text{Var}(X_j)}}\n$$\nUnder the assumption of weak stationarity, the variance is constant for all $t$, i.e., $\\text{Var}(X_t) = \\gamma(0)$, and the covariance depends only on the time lag $k = |i-j|$, i.e., $\\text{Cov}(X_i, X_j) = \\gamma(k)$. The autocorrelation function (ACF) is therefore $\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}$. Our goal is to find the form of $\\rho(k)$.\n\n1.  Derivation of the variance $\\gamma(0)$:\nFrom the model definition, $X_t = \\rho X_{t-1} + \\epsilon_t$. We compute the variance of $X_t$:\n$$\n\\text{Var}(X_t) = \\text{Var}(\\rho X_{t-1} + \\epsilon_t)\n$$\nSince $\\epsilon_t$ is uncorrelated with past values of $X_t$, including $X_{t-1}$, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(X_t) = \\text{Var}(\\rho X_{t-1}) + \\text{Var}(\\epsilon_t) = \\rho^2 \\text{Var}(X_{t-1}) + \\sigma_{\\epsilon}^2\n$$\nBy weak stationarity, $\\text{Var}(X_t) = \\text{Var}(X_{t-1}) = \\gamma(0)$. Substituting this into the equation gives:\n$$\n\\gamma(0) = \\rho^2 \\gamma(0) + \\sigma_{\\epsilon}^2\n$$\nSolving for $\\gamma(0)$:\n$$\n\\gamma(0)(1 - \\rho^2) = \\sigma_{\\epsilon}^2 \\implies \\gamma(0) = \\frac{\\sigma_{\\epsilon}^2}{1 - \\rho^2}\n$$\nThis result is valid because the problem states that $|\\rho|<1$, ensuring $1 - \\rho^2 > 0$.\n\n2.  Derivation of the autocovariance function $\\gamma(k)$ for $k > 0$:\nThe autocovariance at lag $k$ is $\\gamma(k) = \\text{Cov}(X_t, X_{t-k})$. Since the process is centered ($E[X_t] = 0$), this is $E[X_t X_{t-k}]$.\n$$\n\\gamma(k) = E[X_t X_{t-k}] = E[(\\rho X_{t-1} + \\epsilon_t) X_{t-k}]\n$$\nBy linearity of expectation:\n$$\n\\gamma(k) = \\rho E[X_{t-1} X_{t-k}] + E[\\epsilon_t X_{t-k}]\n$$\nFor any lag $k > 0$, the time point $t-k$ is in the past relative to time $t$. By assumption, the noise term $\\epsilon_t$ is uncorrelated with all past values of the process $X$, so $E[\\epsilon_t X_{t-k}] = 0$. The equation simplifies to:\n$$\n\\gamma(k) = \\rho E[X_{t-1} X_{t-k}] = \\rho \\text{Cov}(X_{t-1}, X_{t-k})\n$$\nBy stationarity, $\\text{Cov}(X_{t-1}, X_{t-k}) = \\gamma((t-1) - (t-k)) = \\gamma(k-1)$. This yields the Yule-Walker equation for an AR(1) process:\n$$\n\\gamma(k) = \\rho \\gamma(k-1)\n$$\nThis is a recursive relationship. Starting with $k=1$:\n$\\gamma(1) = \\rho \\gamma(0)$\n$\\gamma(2) = \\rho \\gamma(1) = \\rho (\\rho \\gamma(0)) = \\rho^2 \\gamma(0)$\nBy induction, for any integer lag $k \\ge 0$, the autocovariance is:\n$$\n\\gamma(k) = \\rho^k \\gamma(0)\n$$\n\n3.  Derivation of the autocorrelation function $\\rho(k)$:\nThe ACF is defined as $\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}$. Substituting the expression for $\\gamma(k)$:\n$$\n\\rho(k) = \\frac{\\rho^k \\gamma(0)}{\\gamma(0)} = \\rho^k \\quad \\text{for } k \\ge 0\n$$\nSince $\\rho(k) = \\rho(-k)$, the general form for any integer lag $k$ is $\\rho(k) = \\rho^{|k|}$.\n\n4.  Construction of the $4 \\times 4$ working correlation matrix $R(\\rho)$:\nThe matrix has elements $R_{ij} = \\text{Corr}(Y_{i-1}, Y_{j-1}) = \\rho(|(i-1)-(j-1)|) = \\rho^{|i-j|}$, for $i,j \\in \\{1, 2, 3, 4\\}$.\nThe diagonal elements are for lag $k=0$: $R_{ii} = \\rho^{|i-i|} = \\rho^0 = 1$.\nThe off-diagonal elements are:\n$R_{12} = R_{21} = \\rho^{|1-2|} = \\rho^1 = \\rho$\n$R_{13} = R_{31} = \\rho^{|1-3|} = \\rho^2$\n$R_{14} = R_{41} = \\rho^{|1-4|} = \\rho^3$\n$R_{23} = R_{32} = \\rho^{|2-3|} = \\rho^1 = \\rho$\n$R_{24} = R_{42} = \\rho^{|2-4|} = \\rho^2$\n$R_{34} = R_{43} = \\rho^{|3-4|} = \\rho^1 = \\rho$\n\nAssembling these elements into a $4 \\times 4$ matrix gives the AR(1) working correlation matrix:\n$$\nR(\\rho) = \\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\rho^3 \\\\\n\\rho & 1 & \\rho & \\rho^2 \\\\\n\\rho^2 & \\rho & 1 & \\rho \\\\\n\\rho^3 & \\rho^2 & \\rho & 1\n\\end{pmatrix}\n$$\nThis is a symmetric Toeplitz matrix, which is characteristic of stationary time series processes.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\rho^3 \\\\\n\\rho & 1 & \\rho & \\rho^2 \\\\\n\\rho^2 & \\rho & 1 & \\rho \\\\\n\\rho^3 & \\rho^2 & \\rho & 1\n\\end{pmatrix}\n}\n$$", "id": "4913842"}, {"introduction": "After mastering a specific correlation structure, the next logical step is to see how all the components of a GEE model integrate to form the final estimating equation. This practice challenges you to derive this full equation for a common biostatistical scenario: correlated binary outcomes with an exchangeable correlation structure. Working through this derivation will build your understanding of the GEE \"engine,\" revealing how the design matrix, variance function, and working correlation matrix combine to produce estimates of the model parameters [@problem_id:4913837].", "problem": "A cohort study follows $n$ clusters of related individuals (e.g., siblings), where cluster $i$ has $m_i$ repeated binary outcomes $\\{Y_{ij}: j=1,\\dots,m_i\\}$ and covariate vectors $\\{x_{ij} \\in \\mathbb{R}^p\\}$. Assume a generalized linear model with logit link, so that the marginal mean is $\\mu_{ij} = \\Pr(Y_{ij}=1 \\mid x_{ij}) = \\exp(\\eta_{ij})/(1+\\exp(\\eta_{ij}))$ with linear predictor $\\eta_{ij} = x_{ij}^{\\top}\\beta$. Let $X_i$ be the $m_i \\times p$ design matrix whose $j$th row is $x_{ij}^{\\top}$, $\\mu_i = (\\mu_{i1},\\dots,\\mu_{im_i})^{\\top}$, and $y_i = (y_{i1},\\dots,y_{im_i})^{\\top}$. Under the generalized estimating equations approach for correlated outcomes, use the following foundational facts:\n- For a Bernoulli variable with mean $\\mu_{ij}$, the variance is $\\mu_{ij}(1-\\mu_{ij})$.\n- For the logit link, the derivative of the inverse link satisfies $\\frac{d\\mu_{ij}}{d\\eta_{ij}} = \\mu_{ij}(1-\\mu_{ij})$.\n- The working covariance takes the form $V_i = A_i^{1/2} R_i(\\alpha) A_i^{1/2}$, where $A_i$ is a diagonal matrix of marginal variances and $R_i(\\alpha)$ is the working correlation. For the exchangeable structure, $R_i(\\alpha)$ has $1$ on the diagonal and $\\alpha$ off-diagonal, with $-1/(m_i-1) < \\alpha < 1$ to ensure positive definiteness.\n\nTasks:\n1. Express $A_i$ explicitly in terms of $\\mu_{ij}$ for $j=1,\\dots,m_i$.\n2. Derive the generalized estimating equations system for $\\beta$ under the exchangeable working correlation, writing the estimating function $U(\\beta)$ in closed form that depends only on $X_i$, $\\mu_i$, $y_i$, $\\alpha$, $m_i$, the $m_i \\times m_i$ identity matrix $I_{m_i}$, and the $m_i \\times m_i$ all-ones matrix $J_{m_i}$. Your final expression should show the explicit inverse $R_i(\\alpha)^{-1}$ in terms of $\\alpha$, $m_i$, $I_{m_i}$, and $J_{m_i}$.\n\nProvide your final answer as two expressions, in the order: first the explicit $A_i$, and second the explicit $U(\\beta)$. No numerical evaluation is required. Do not include any equality signs in your final answer. No rounding is required, and no units are involved.", "solution": "The solution is divided into two parts as per the request. First, we determine the matrix $A_i$. Second, we derive the GEE estimating function $U(\\beta)$.\n\n### Task 1: Express $A_i$ Explicitly\n\nThe problem states that $A_i$ is a diagonal matrix of the marginal variances of the outcomes $Y_{ij}$ for cluster $i$. The outcomes $\\{Y_{ij}\\}$ are binary, following a Bernoulli distribution. For a Bernoulli random variable $Y$ with mean $\\mu = \\Pr(Y=1)$, the variance is given by $\\text{Var}(Y) = \\mu(1-\\mu)$.\n\nIn this problem, the mean of $Y_{ij}$ is given as $\\mu_{ij}$. Therefore, the marginal variance of $Y_{ij}$ is $\\text{Var}(Y_{ij}) = \\mu_{ij}(1-\\mu_{ij})$.\n\nThe matrix $A_i$ is an $m_i \\times m_i$ diagonal matrix where the $j$-th diagonal element is $\\text{Var}(Y_{ij})$.\nSo, the elements of $A_i$ are $(A_i)_{jk} = \\delta_{jk} \\mu_{ij}(1-\\mu_{ij})$, where $\\delta_{jk}$ is the Kronecker delta.\nThis can be written using the $\\text{diag}(\\cdot)$ operator as:\n$$\nA_i = \\text{diag}(\\mu_{i1}(1-\\mu_{i1}), \\mu_{i2}(1-\\mu_{i2}), \\dots, \\mu_{im_i}(1-\\mu_{im_i}))\n$$\n\n### Task 2: Derive the Generalized Estimating Equations System\n\nThe generalized estimating function $U(\\beta)$ is the sum of contributions from each of the $n$ clusters:\n$$\nU(\\beta) = \\sum_{i=1}^{n} U_i(\\beta)\n$$\nThe contribution from the $i$-th cluster, $U_i(\\beta)$, is given by the general formula:\n$$\nU_i(\\beta) = D_i^{\\top} V_i^{-1} S_i\n$$\nwhere $S_i = y_i - \\mu_i$ is the vector of residuals for cluster $i$, $V_i$ is the working covariance matrix, and $D_i = \\frac{\\partial \\mu_i}{\\partial \\beta^{\\top}}$ is the $m_i \\times p$ matrix of derivatives of the mean vector with respect to the regression coefficients $\\beta$. We derive each component in turn.\n\n1.  **The Derivative Matrix $D_i$**:\n    The $(j,k)$-th element of $D_i$ is $\\frac{\\partial \\mu_{ij}}{\\partial \\beta_k}$. Using the chain rule:\n    $$\n    \\frac{\\partial \\mu_{ij}}{\\partial \\beta_k} = \\frac{d\\mu_{ij}}{d\\eta_{ij}} \\frac{\\partial \\eta_{ij}}{\\partial \\beta_k}\n    $$\n    The problem provides two necessary facts:\n    -   For the logit link, $\\frac{d\\mu_{ij}}{d\\eta_{ij}} = \\mu_{ij}(1-\\mu_{ij})$. This is the $j$-th diagonal element of $A_i$.\n    -   The linear predictor is $\\eta_{ij} = x_{ij}^{\\top}\\beta = \\sum_{l=1}^{p} x_{ijl}\\beta_l$. Thus, $\\frac{\\partial \\eta_{ij}}{\\partial \\beta_k} = x_{ijk}$. This is the $k$-th element of the covariate vector $x_{ij}$.\n    \n    Combining these, the $j$-th row of the matrix $D_i$ is given by $\\frac{\\partial \\mu_{ij}}{\\partial \\beta^{\\top}} = \\mu_{ij}(1-\\mu_{ij}) x_{ij}^{\\top}$.\n    By stacking the $m_i$ rows, we can write $D_i$ in matrix form as the product of the diagonal matrix of variances $A_i$ and the design matrix $X_i$:\n    $$\n    D_i = A_i X_i\n    $$\n\n2.  **The Inverse Working Covariance Matrix $V_i^{-1}$**:\n    The working covariance matrix is given as $V_i = A_i^{1/2} R_i(\\alpha) A_i^{1/2}$. To find its inverse, we use the property $(BCD)^{-1} = D^{-1}C^{-1}B^{-1}$.\n    Since $A_i$ is a diagonal matrix, $A_i^{1/2}$ is also diagonal, and its inverse $(A_i^{1/2})^{-1}$ is a diagonal matrix with elements $1/\\sqrt{\\mu_{ij}(1-\\mu_{ij})}$. We denote this by $A_i^{-1/2}$.\n    Thus, the inverse of $V_i$ is:\n    $$\n    V_i^{-1} = (A_i^{1/2} R_i(\\alpha) A_i^{1/2})^{-1} = (A_i^{1/2})^{-1} R_i(\\alpha)^{-1} (A_i^{1/2})^{-1} = A_i^{-1/2} R_i(\\alpha)^{-1} A_i^{-1/2}\n    $$\n\n3.  **The Inverse Working Correlation Matrix $R_i(\\alpha)^{-1}$**:\n    The exchangeable working correlation matrix $R_i(\\alpha)$ has $1$s on the diagonal and $\\alpha$ on the off-diagonals. This structure can be expressed as a linear combination of the identity matrix $I_{m_i}$ and the all-ones matrix $J_{m_i}$:\n    $$\n    R_i(\\alpha) = (1-\\alpha)I_{m_i} + \\alpha J_{m_i}\n    $$\n    The inverse of a matrix of this form is a standard result. It takes the same form, $c_1 I_{m_i} + c_2 J_{m_i}$. By solving the equation $( (1-\\alpha)I_{m_i} + \\alpha J_{m_i} ) (c_1 I_{m_i} + c_2 J_{m_i}) = I_{m_i}$ and using the property $J_{m_i}^2 = m_i J_{m_i}$, we find the coefficients $c_1$ and $c_2$.\n    The resulting inverse is:\n    $$\n    R_i(\\alpha)^{-1} = \\frac{1}{1-\\alpha} I_{m_i} - \\frac{\\alpha}{(1-\\alpha)(1 + (m_i-1)\\alpha)} J_{m_i}\n    $$\n    The condition $-1/(m_i-1) < \\alpha < 1$ ensures that the denominators are non-zero and the matrix is positive definite.\n\n4.  **Assembling the Estimating Function $U(\\beta)$**:\n    We now substitute the derived components into the expression for $U_i(\\beta)$:\n    $$\n    U_i(\\beta) = D_i^{\\top} V_i^{-1} S_i = (A_i X_i)^{\\top} (A_i^{-1/2} R_i(\\alpha)^{-1} A_i^{-1/2}) (y_i - \\mu_i)\n    $$\n    Using the transpose property $(AB)^\\top = B^\\top A^\\top$:\n    $$\n    U_i(\\beta) = X_i^{\\top} A_i^{\\top} A_i^{-1/2} R_i(\\alpha)^{-1} A_i^{-1/2} (y_i - \\mu_i)\n    $$\n    Since $A_i$ is a diagonal matrix, it is symmetric ($A_i^{\\top} = A_i$). Also, by definition, $A_i = A_i^{1/2} A_i^{1/2}$. The product $A_i^{\\top} A_i^{-1/2}$ simplifies:\n    $$\n    A_i^{\\top} A_i^{-1/2} = A_i A_i^{-1/2} = (A_i^{1/2} A_i^{1/2}) A_i^{-1/2} = A_i^{1/2} (A_i^{1/2} A_i^{-1/2}) = A_i^{1/2} I_{m_i} = A_i^{1/2}\n    $$\n    Substituting this back into the expression for $U_i(\\beta)$:\n    $$\n    U_i(\\beta) = X_i^{\\top} A_i^{1/2} R_i(\\alpha)^{-1} A_i^{-1/2} (y_i - \\mu_i)\n    $$\n    Finally, we sum over all clusters $i=1, \\dots, n$ and substitute the explicit expression for $R_i(\\alpha)^{-1}$:\n    $$\n    U(\\beta) = \\sum_{i=1}^{n} X_i^{\\top} A_i^{1/2} \\left( \\frac{1}{1-\\alpha} I_{m_i} - \\frac{\\alpha}{(1-\\alpha)(1 + (m_i-1)\\alpha)} J_{m_i} \\right) A_i^{-1/2} (y_i - \\mu_i)\n    $$\n    This is the final closed-form expression for the GEE estimating function under the specified conditions. It depends on the data ($X_i, y_i$), model parameters ($\\beta$ through $\\mu_i$, and $\\alpha$), and cluster size ($m_i$). The matrix $A_i$ is a function of $\\mu_i$ as established in the first part.", "answer": "$$\n\\boxed{\n\\begin{gather*}\n\\text{diag}(\\mu_{i1}(1-\\mu_{i1}), \\mu_{i2}(1-\\mu_{i2}), \\dots, \\mu_{im_i}(1-\\mu_{im_i})) \\\\\n\\sum_{i=1}^{n} X_i^{\\top} A_i^{1/2} \\left[ \\frac{1}{1-\\alpha} I_{m_i} - \\frac{\\alpha}{(1-\\alpha)(1 + (m_i-1)\\alpha)} J_{m_i} \\right] A_i^{-1/2} (y_i - \\mu_i)\n\\end{gather*}\n}\n$$", "id": "4913837"}, {"introduction": "Constructing a statistical model is only half the task; interpreting its output to generate scientific insight is equally important. This final practice shifts the focus from mechanical derivation to practical application, exploring how the choice of a link function impacts the interpretation of model coefficients. By comparing a log link (which models the risk ratio) with a logit link (which models the odds ratio) and calculating predicted risks, you will gain hands-on experience in how model specification directly influences the interpretation of exposure effects, particularly in common epidemiological settings like the study of rare events [@problem_id:4913822].", "problem": "A respiratory infection surveillance study follows $n$ households over $T$ consecutive weeks, yielding correlated binary outcomes $Y_{it} \\in \\{0,1\\}$ indicating whether household $i$ reports at least one new infection in week $t$. Because weekly infections are uncommon, empirical marginal risks are small. To account for within-household correlation while modeling the marginal mean, analysts fit two Generalized Estimating Equations (GEE) with exchangeable working correlation: one with a log link and one with a logit link. Both models use the same covariate vector $X_{it} = (1, \\text{exposure}_{it}, \\text{age}_{i}^{*})^{\\top}$, where $\\text{exposure}_{it} \\in \\{0,1\\}$ indicates whether a targeted preventive action was taken during week $t$, and $\\text{age}_{i}^{*} = (\\text{age}_{i} - 40)/10$ scales age in decades centered at $40$ years. The within-household working correlation estimate is $\\hat{\\rho} = 0.20$.\n\nFrom the log-link GEE, the estimated regression coefficients are $\\hat{\\beta}^{\\log} = (\\hat{\\beta}^{\\log}_{0}, \\hat{\\beta}^{\\log}_{1}, \\hat{\\beta}^{\\log}_{2}) = (-4.70, 0.40, 0.03)$. From the logit-link GEE, the estimated regression coefficients are $\\hat{\\beta}^{\\mathrm{logit}} = (\\hat{\\beta}^{\\mathrm{logit}}_{0}, \\hat{\\beta}^{\\mathrm{logit}}_{1}, \\hat{\\beta}^{\\mathrm{logit}}_{2}) = (-4.90, 0.70, 0.05)$.\n\nUsing only first principles for marginal models with canonical link mappings, and beginning from the definition that a GEE specifies a mean model through a link function $g$ such that $g\\!\\left(\\mu_{it}\\right) = X_{it}^{\\top}\\beta$ for the marginal mean $\\mu_{it} = \\mathbb{E}(Y_{it} \\mid X_{it})$, do the following:\n\n1. Explain, in terms of the parameters of the marginal mean model, how the exposure effect differs in interpretability under the log link versus the logit link, specifically for rare events.\n2. For a household with $\\text{exposure}_{it} = 1$ and $\\text{age}_{i} = 30$ years, compute the predicted marginal risk under each fitted GEE. Use $X_{0} = (1, 1, -1)^{\\top}$ because $\\text{age}_{i}^{*} = (30 - 40)/10 = -1$. Round both predictions to four significant figures. Report the two predictions as a two-entry row matrix, with the first entry corresponding to the log-link GEE and the second entry corresponding to the logit-link GEE. No units are required.", "solution": "The problem asks for two tasks based on a GEE framework for correlated binary outcomes $Y_{it}$. The marginal mean $\\mu_{it} = \\mathbb{E}(Y_{it} \\mid X_{it})$ is modeled via a link function $g$ as $g(\\mu_{it}) = X_{it}^{\\top}\\beta$. Here, $\\mu_{it}$ represents the marginal risk of infection for household $i$ in week $t$.\n\n### Part 1: Interpretation of the Exposure Effect\n\nThe interpretability of the exposure effect, captured by the coefficient $\\beta_1$ corresponding to the $\\text{exposure}_{it}$ covariate, depends directly on the chosen link function $g$.\n\n**Log Link (Relative Risk Model)**\n\nFor the first GEE, the link function is the natural logarithm, $g(\\mu_{it}) = \\ln(\\mu_{it})$. The marginal mean model is therefore:\n$$\n\\ln(\\mu_{it}) = \\beta^{\\log}_{0} + \\beta^{\\log}_{1} \\cdot \\text{exposure}_{it} + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\n$$\nExponentiating both sides yields a model for the risk itself:\n$$\n\\mu_{it} = \\exp\\left(\\beta^{\\log}_{0} + \\beta^{\\log}_{1} \\cdot \\text{exposure}_{it} + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\\right)\n$$\nTo interpret $\\beta^{\\log}_{1}$, we examine the ratio of risks for an exposed individual ($\\text{exposure}_{it} = 1$) versus an unexposed individual ($\\text{exposure}_{it} = 0$), holding all other covariates constant. This ratio is the Risk Ratio (RR).\n$$\n\\text{RR} = \\frac{\\mu_{it}(\\text{exposure}_{it}=1)}{\\mu_{it}(\\text{exposure}_{it}=0)} = \\frac{\\exp\\left(\\beta^{\\log}_{0} + \\beta^{\\log}_{1}(1) + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\\right)}{\\exp\\left(\\beta^{\\log}_{0} + \\beta^{\\log}_{1}(0) + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\\right)} = \\exp(\\beta^{\\log}_{1})\n$$\nThus, $\\exp(\\beta^{\\log}_{1})$ is the Risk Ratio. The log-link model directly estimates the log of the risk ratio. The effect measure, $\\exp(\\beta^{\\log}_{1})$, is constant across all levels of the other covariates (in this case, age).\n\n**Logit Link (Logistic Regression Model)**\n\nFor the second GEE, the link function is the logit, $g(\\mu_{it}) = \\text{logit}(\\mu_{it}) = \\ln\\left(\\frac{\\mu_{it}}{1-\\mu_{it}}\\right)$. The term $\\frac{\\mu_{it}}{1-\\mu_{it}}$ is the odds of the outcome. The marginal mean model is:\n$$\n\\ln\\left(\\frac{\\mu_{it}}{1-\\mu_{it}}\\right) = \\beta^{\\mathrm{logit}}_{0} + \\beta^{\\mathrm{logit}}_{1} \\cdot \\text{exposure}_{it} + \\beta^{\\mathrm{logit}}_{2} \\cdot \\text{age}_{i}^{*}\n$$\nTo interpret $\\beta^{\\mathrm{logit}}_{1}$, we examine the ratio of odds for an exposed individual versus an unexposed one, holding other covariates constant. This is the Odds Ratio (OR).\n$$\n\\text{OR} = \\frac{\\text{odds}(\\text{exposure}_{it}=1)}{\\text{odds}(\\text{exposure}_{it}=0)} = \\frac{\\exp\\left(\\beta^{\\mathrm{logit}}_{0} + \\beta^{\\mathrm{logit}}_{1}(1) + \\beta^{\\mathrm{logit}}_{2} \\cdot \\text{age}_{i}^{*}\\right)}{\\exp\\left(\\beta^{\\mathrm{logit}}_{0} + \\beta^{\\mathrm{logit}}_{1}(0) + \\beta^{\\mathrm{logit}}_{2} \\cdot \\text{age}_{i}^{*}\\right)} = \\exp(\\beta^{\\mathrm{logit}}_{1})\n$$\nThus, $\\exp(\\beta^{\\mathrm{logit}}_{1})$ is the Odds Ratio. The logit-link model directly estimates the log of the odds ratio. This effect measure is also constant across all levels of the other covariates.\n\n**Comparison for Rare Events**\n\nThe problem states that the infections are uncommon, meaning the marginal risk $\\mu_{it}$ is small. When $\\mu_{it} \\to 0$, the denominator in the odds calculation, $1-\\mu_{it}$, approaches $1$. Consequently, the odds approximate the risk:\n$$\n\\text{odds} = \\frac{\\mu_{it}}{1-\\mu_{it}} \\approx \\mu_{it} = \\text{risk}\n$$\nFollowing this approximation, the Odds Ratio (OR) approximates the Risk Ratio (RR):\n$$\n\\text{OR} = \\frac{\\mu_1/(1-\\mu_1)}{\\mu_0/(1-\\mu_0)} = \\frac{\\mu_1}{\\mu_0} \\cdot \\frac{1-\\mu_0}{1-\\mu_1} = \\text{RR} \\cdot \\frac{1-\\mu_0}{1-\\mu_1} \\approx \\text{RR}\n$$\nTherefore, for rare events, $\\exp(\\beta^{\\mathrm{logit}}_{1})$ (the OR) is an approximation of the RR. In contrast, $\\exp(\\beta^{\\log}_{1})$ is a direct estimate of the RR, not an approximation. The log link provides a more direct and readily interpretable measure of relative risk, which is often the quantity of primary interest in epidemiology, especially for rare outcomes.\n\n### Part 2: Prediction of Marginal Risk\n\nWe are asked to compute the predicted marginal risk, $\\hat{\\mu}$, for a household with $\\text{exposure}_{it} = 1$ and $\\text{age}_{i} = 30$ years. The scaled age is $\\text{age}_{i}^{*} = (30-40)/10 = -1$. The covariate vector is $X_{0} = (1, 1, -1)^{\\top}$.\n\n**Log-link GEE Prediction**\n\nThe estimated linear predictor, $\\hat{\\eta}^{\\log}$, is calculated using the coefficients $\\hat{\\beta}^{\\log} = (-4.70, 0.40, 0.03)^{\\top}$:\n$$\n\\hat{\\eta}^{\\log} = X_{0}^{\\top}\\hat{\\beta}^{\\log} = (1)(-4.70) + (1)(0.40) + (-1)(0.03) = -4.70 + 0.40 - 0.03 = -4.33\n$$\nThe predicted risk, $\\hat{\\mu}^{\\log}$, is found by applying the inverse link function, which is the exponential function:\n$$\n\\hat{\\mu}^{\\log} = \\exp(\\hat{\\eta}^{\\log}) = \\exp(-4.33) \\approx 0.0131682\n$$\nRounding to four significant figures, the predicted risk is $0.01317$.\n\n**Logit-link GEE Prediction**\n\nThe estimated linear predictor, $\\hat{\\eta}^{\\mathrm{logit}}$, is calculated using the coefficients $\\hat{\\beta}^{\\mathrm{logit}} = (-4.90, 0.70, 0.05)^{\\top}$:\n$$\n\\hat{\\eta}^{\\mathrm{logit}} = X_{0}^{\\top}\\hat{\\beta}^{\\mathrm{logit}} = (1)(-4.90) + (1)(0.70) + (-1)(0.05) = -4.90 + 0.70 - 0.05 = -4.25\n$$\nThe predicted risk, $\\hat{\\mu}^{\\mathrm{logit}}$, is found by applying the inverse logit (logistic) function:\n$$\n\\hat{\\mu}^{\\mathrm{logit}} = \\frac{\\exp(\\hat{\\eta}^{\\mathrm{logit}})}{1 + \\exp(\\hat{\\eta}^{\\mathrm{logit}})} = \\frac{\\exp(-4.25)}{1 + \\exp(-4.25)}\n$$\nFirst, $\\exp(-4.25) \\approx 0.0142641$. Then,\n$$\n\\hat{\\mu}^{\\mathrm{logit}} = \\frac{0.0142641}{1 + 0.0142641} = \\frac{0.0142641}{1.0142641} \\approx 0.0140635\n$$\nRounding to four significant figures, the predicted risk is $0.01406$.\n\nThe two predictions, $0.01317$ (from the log-link model) and $0.01406$ (from the logit-link model), are indeed small, which is consistent with the problem's description of rare events. The final answer requires these two values in a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.01317 & 0.01406\n\\end{pmatrix}\n}\n$$", "id": "4913822"}]}