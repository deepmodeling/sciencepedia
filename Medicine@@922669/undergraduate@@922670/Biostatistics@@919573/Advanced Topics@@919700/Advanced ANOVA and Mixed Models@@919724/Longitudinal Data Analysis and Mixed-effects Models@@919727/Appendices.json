{"hands_on_practices": [{"introduction": "Mixed-effects models provide a powerful framework for analyzing longitudinal data, where measurements are clustered within subjects. At the heart of these models is the ability to parse variability into different sources. This first exercise [@problem_id:4924237] takes you back to first principles with the simplest case: the random-intercept model. By deriving the variance and correlation of outcomes from the same subject, you will gain a concrete understanding of how the model separates between-subject heterogeneity from within-subject noise and induces a specific correlation structure.", "problem": "A biostatistics study repeatedly measures a stable biomarker (for example, baseline serum albumin) on subjects indexed by $i \\in \\{1,\\dots,N\\}$ at visit times $t \\in \\{1,\\dots,T\\}$. To capture between-subject heterogeneity and within-subject measurement noise, consider the following random-intercept linear mixed-effects model (Linear Mixed-Effects Model (LMM)) for the longitudinal outcome:\n$$\ny_{it} \\;=\\; \\beta_0 \\;+\\; b_i \\;+\\; \\epsilon_{it},\n$$\nwhere $\\beta_0$ is a fixed intercept, $b_i$ is a subject-specific random effect with $b_i \\sim N(0,\\sigma_b^2)$, and $\\epsilon_{it}$ is the within-subject error with $\\epsilon_{it} \\sim N(0,\\sigma^2)$. Assume that $\\{b_i\\}$ are independent across subjects, $\\{\\epsilon_{it}\\}$ are independent across subjects and times, and $b_i$ is independent of $\\epsilon_{it}$ for all $i$ and $t$.\n\nUsing only core probability definitions and properties of variance, covariance, and correlation for sums of independent random variables, derive expressions for $\\operatorname{Var}(y_{it})$ and $\\operatorname{Corr}(y_{it}, y_{is})$ when $t \\neq s$. Express your final answers symbolically in terms of $\\sigma_b^2$ and $\\sigma^2$. No rounding is required.", "solution": "The user has provided a problem statement that requires the derivation of the variance and correlation for outcomes in a random-intercept linear mixed-effects model.\n\nFirst, a validation of the problem statement is conducted.\n\n**Step 1: Extract Givens**\n- The model for the longitudinal outcome $y_{it}$ is:\n$$\ny_{it} \\;=\\; \\beta_0 \\;+\\; b_i \\;+\\; \\epsilon_{it}\n$$\n- $i \\in \\{1,\\dots,N\\}$ indexes subjects.\n- $t \\in \\{1,\\dots,T\\}$ indexes visit times.\n- $\\beta_0$ is a fixed intercept (a constant).\n- $b_i$ is a subject-specific random effect, with the distribution $b_i \\sim N(0,\\sigma_b^2)$.\n- $\\epsilon_{it}$ is a within-subject error, with the distribution $\\epsilon_{it} \\sim N(0,\\sigma^2)$.\n- Independence assumptions:\n  1. $\\{b_i\\}$ are independent across subjects $i$.\n  2. $\\{\\epsilon_{it}\\}$ are independent across subjects $i$ and times $t$.\n  3. $b_i$ is independent of $\\epsilon_{it}$ for all $i$ and $t$.\n- The task is to derive expressions for $\\operatorname{Var}(y_{it})$ and $\\operatorname{Corr}(y_{it}, y_{is})$ for $t \\neq s$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on a standard random-intercept linear mixed-effects model, which is a fundamental tool in biostatistics for analyzing longitudinal data. The model and its assumptions are standard and scientifically sound.\n- **Well-Posed:** The problem is well-posed. It clearly defines the model, its parameters, and the distributional assumptions. It asks for specific quantities ($\\operatorname{Var}(y_{it})$ and $\\operatorname{Corr}(y_{it}, y_{is})$) that can be uniquely derived from the provided information.\n- **Objective:** The problem is stated using precise and objective mathematical and statistical language, free of any subjectivity or ambiguity.\n- The problem does not violate any of the invalidity criteria. It is a complete, consistent, and formalizable statistical problem.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The derivation of the solution will now proceed.\n\n**Derivation of $\\operatorname{Var}(y_{it})$**\n\nThe variance of the outcome $y_{it}$ is defined as $\\operatorname{Var}(y_{it})$. We substitute the model definition for $y_{it}$:\n$$\n\\operatorname{Var}(y_{it}) = \\operatorname{Var}(\\beta_0 + b_i + \\epsilon_{it})\n$$\nThe variance of a constant is zero, so $\\operatorname{Var}(\\beta_0) = 0$. Adding a constant to a random variable does not change its variance. Thus,\n$$\n\\operatorname{Var}(y_{it}) = \\operatorname{Var}(b_i + \\epsilon_{it})\n$$\nA core property of variance is that for two independent random variables $X$ and $Y$, $\\operatorname{Var}(X+Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)$. The problem statement specifies that the random intercept $b_i$ and the error term $\\epsilon_{it}$ are independent. Therefore, we can write:\n$$\n\\operatorname{Var}(y_{it}) = \\operatorname{Var}(b_i) + \\operatorname{Var}(\\epsilon_{it})\n$$\nFrom the problem statement, we are given the variances of $b_i$ and $\\epsilon_{it}$:\n$$\n\\operatorname{Var}(b_i) = \\sigma_b^2\n$$\n$$\n\\operatorname{Var}(\\epsilon_{it}) = \\sigma^2\n$$\nSubstituting these into the expression for $\\operatorname{Var}(y_{it})$ yields:\n$$\n\\operatorname{Var}(y_{it}) = \\sigma_b^2 + \\sigma^2\n$$\nThis expression represents the total variance of an observation, which is the sum of the between-subject variance ($\\sigma_b^2$) and the within-subject variance ($\\sigma^2$).\n\n**Derivation of $\\operatorname{Corr}(y_{it}, y_{is})$ for $t \\neq s$**\n\nThe correlation between two random variables $X$ and $Y$ is defined as:\n$$\n\\operatorname{Corr}(X, Y) = \\frac{\\operatorname{Cov}(X, Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}\n$$\nIn our case, we need to find the correlation between two measurements, $y_{it}$ and $y_{is}$, taken from the same subject $i$ at two different times, $t$ and $s$ (where $t \\neq s$).\n$$\n\\operatorname{Corr}(y_{it}, y_{is}) = \\frac{\\operatorname{Cov}(y_{it}, y_{is})}{\\sqrt{\\operatorname{Var}(y_{it})\\operatorname{Var}(y_{is})}}\n$$\nFirst, we derive the covariance term in the numerator, $\\operatorname{Cov}(y_{it}, y_{is})$. We substitute the model definitions:\n$$\n\\operatorname{Cov}(y_{it}, y_{is}) = \\operatorname{Cov}(\\beta_0 + b_i + \\epsilon_{it}, \\; \\beta_0 + b_i + \\epsilon_{is})\n$$\nUsing the bilinearity property of covariance, and knowing that the covariance of a constant with any random variable is zero, we can expand this expression:\n$$\n\\operatorname{Cov}(y_{it}, y_{is}) = \\operatorname{Cov}(b_i, b_i) + \\operatorname{Cov}(b_i, \\epsilon_{is}) + \\operatorname{Cov}(\\epsilon_{it}, b_i) + \\operatorname{Cov}(\\epsilon_{it}, \\epsilon_{is})\n$$\nWe evaluate each term based on the given assumptions:\n1.  $\\operatorname{Cov}(b_i, b_i) = \\operatorname{Var}(b_i) = \\sigma_b^2$.\n2.  $\\operatorname{Cov}(b_i, \\epsilon_{is})$: The problem states that $b_i$ is independent of $\\epsilon_{it'}$ for all $t'$. Since $s$ is a specific time point, $b_i$ and $\\epsilon_{is}$ are independent. The covariance of independent random variables is zero. Thus, $\\operatorname{Cov}(b_i, \\epsilon_{is}) = 0$.\n3.  $\\operatorname{Cov}(\\epsilon_{it}, b_i)$: Similarly, this is zero due to independence.\n4.  $\\operatorname{Cov}(\\epsilon_{it}, \\epsilon_{is})$: The problem states that the error terms $\\{\\epsilon_{it'}\\}$ are independent across time. Since $t \\neq s$, the random variables $\\epsilon_{it}$ and $\\epsilon_{is}$ are independent. Their covariance is therefore zero.\n\nSubstituting these results back into the covariance expression:\n$$\n\\operatorname{Cov}(y_{it}, y_{is}) = \\sigma_b^2 + 0 + 0 + 0 = \\sigma_b^2\n$$\nThe covariance between two measurements on the same subject at different times is equal to the between-subject variance. This is because the shared component between $y_{it}$ and $y_{is}$ is the subject-specific random intercept $b_i$.\n\nNext, we evaluate the denominator of the correlation formula. We already derived $\\operatorname{Var}(y_{it}) = \\sigma_b^2 + \\sigma^2$. The variance of $y_{is}$ is found by the same logic:\n$$\n\\operatorname{Var}(y_{is}) = \\operatorname{Var}(\\beta_0 + b_i + \\epsilon_{is}) = \\operatorname{Var}(b_i) + \\operatorname{Var}(\\epsilon_{is}) = \\sigma_b^2 + \\sigma^2\n$$\nSo, the denominator is:\n$$\n\\sqrt{\\operatorname{Var}(y_{it})\\operatorname{Var}(y_{is})} = \\sqrt{(\\sigma_b^2 + \\sigma^2)(\\sigma_b^2 + \\sigma^2)} = \\sigma_b^2 + \\sigma^2\n$$\nFinally, we assemble the correlation expression:\n$$\n\\operatorname{Corr}(y_{it}, y_{is}) = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma^2}\n$$\nThis quantity is the intraclass correlation coefficient (ICC). It represents the proportion of the total variance that is attributable to the between-subject variation.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sigma_b^2 + \\sigma^2  \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma^2} \\end{pmatrix}}\n$$", "id": "4924237"}, {"introduction": "Moving from theory to application requires translating your conceptual model into the language of linear algebraâ€”the design matrices. This exercise [@problem_id:4924264] guides you through the practical task of constructing the fixed-effects ($X$) and random-effects ($Z$) design matrices for a more complex random-intercept and random-slope model. You will learn critical techniques like centering covariates and decomposing time-varying predictors, which are essential for building stable, interpretable models and avoiding issues like multicollinearity.", "problem": "A cohort study follows three participants, each measured at three equally spaced time points with time indices $t \\in \\{0,1,2\\}$. The continuous response $y_{it}$ (for participant $i$ at time $t$) is modeled using a Linear Mixed-Effects (LME) model with a random intercept and a random slope for time. There is one baseline covariate (participant age at baseline) and one time-varying covariate (body weight).\n\nData:\n- Participants: $i \\in \\{1,2,3\\}$; time points per participant: $t \\in \\{0,1,2\\}$.\n- Times (in years): $t = 0, 1, 2$.\n- Baseline ages (in years): participant $1$: $48$, participant $2$: $55$, participant $3$: $62$.\n- Weights (in kilograms) at each time:\n  - Participant $1$: $(78, 83, 85)$ at $t=(0,1,2)$.\n  - Participant $2$: $(74, 78, 79)$ at $t=(0,1,2)$.\n  - Participant $3$: $(73, 74, 78)$ at $t=(0,1,2)$.\n\nYou are to construct the fixed-effects design matrix $X$ and the random-effects design matrix $Z$ for a random intercept and random slope model that avoids unnecessary collinearity by principled coding. Use only fundamental definitions to justify your coding choices. Specifically:\n- Center time at the study midpoint, i.e., use $t_{c} = t - 1$.\n- Decompose the time-varying covariate $w_{it}$ (weight) into a within-participant component and a between-participant component via the decomposition $w_{it} = \\big(w_{it} - \\bar{w}_{i}\\big) + \\big(\\bar{w}_{i} - \\bar{w}\\big)$, where $\\bar{w}_{i}$ is the participant-specific mean across observed times and $\\bar{w}$ is the grand mean of $\\bar{w}_{i}$ values across participants.\n- Center baseline age by the sample mean.\n\nLet the fixed-effects portion include an intercept, the centered time $t_{c}$, the centered baseline age, the within-participant weight component, and the between-participant weight component. Let the random-effects portion include a random intercept and a random slope on $t_{c}$ for each participant.\n\nConstruct $X$ and $Z$ accordingly. Then, using your constructed $X$, compute the determinant of $X^{\\top}X$. Provide the exact integer value; no rounding is required.", "solution": "The user wants me to validate the problem statement, and if it is valid, to construct the fixed-effects and random-effects design matrices for a specified Linear Mixed-Effects (LME) model and compute the determinant of $X^{\\top}X$.\n\n### Step 1: Extract Givens\n- **Model Type**: Linear Mixed-Effects (LME) model with a random intercept and a random slope for time.\n- **Participants**: $i \\in \\{1, 2, 3\\}$.\n- **Time points**: $t \\in \\{0, 1, 2\\}$, equally spaced.\n- **Response**: $y_{it}$ is a continuous response for participant $i$ at time $t$.\n- **Baseline Covariate**: Baseline age.\n  - Participant $1$: $48$ years.\n  - Participant $2$: $55$ years.\n  - Participant $3$: $62$ years.\n- **Time-varying Covariate**: Body weight, $w_{it}$.\n  - Participant $1$: $(78, 83, 85)$ kg at $t=(0,1,2)$.\n  - Participant $2$: $(74, 78, 79)$ kg at $t=(0,1,2)$.\n  - Participant $3$: $(73, 74, 78)$ kg at $t=(0,1,2)$.\n- **Coding Instructions for Fixed Effects**:\n  - The model includes a fixed intercept.\n  - Time is centered at the study midpoint: $t_c = t - 1$.\n  - Baseline age is centered by the sample mean.\n  - Weight ($w_{it}$) is decomposed into two components:\n    1. Within-participant: $w_{it} - \\bar{w}_{i}$, where $\\bar{w}_{i}$ is the mean weight for participant $i$.\n    2. Between-participant: $\\bar{w}_{i} - \\bar{w}$, where $\\bar{w}$ is the grand mean of the $\\bar{w}_{i}$ values.\n- **Coding Instructions for Random Effects**:\n  - A random intercept for each participant.\n  - A random slope on centered time ($t_c$) for each participant.\n- **Task**:\n  1. Construct the fixed-effects design matrix, $X$.\n  2. Construct the random-effects design matrix, $Z$.\n  3. Compute the determinant of $X^{\\top}X$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the setup for a standard LME model, a ubiquitous and fundamental tool in biostatistics for analyzing longitudinal data. The use of centering and covariate decomposition are established best practices to improve model stability and interpretability. All aspects are consistent with established statistical theory.\n- **Well-Posed**: The instructions for constructing the matrices $X$ and $Z$ are precise and unambiguous. The data provided are complete and sufficient to perform the required calculations. The task of finding the determinant of $X^{\\top}X$ is a well-defined mathematical operation that will yield a unique answer.\n- **Objective**: The problem is stated in precise, technical language, free from subjectivity or opinion.\n- **Summary of Flaws**:\n  - Scientific or Factual Unsoundness: None.\n  - Non-Formalizable or Irrelevant: None. The problem is directly relevant to biostatistics and formalizable.\n  - Incomplete or Contradictory Setup: None.\n  - Unrealistic or Infeasible: None.\n  - Ill-Posed or Poorly Structured: None.\n  - Pseudo-Profound, Trivial, or Tautological: None.\n  - Outside Scientific Verifiability: None.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the full solution.\n\nThe LME model is specified as $Y = X\\beta + Zb + \\epsilon$, where $X$ is the fixed-effects design matrix and $Z$ is the random-effects design matrix. We must construct these matrices based on the provided data and coding rules. The data is ordered by participant, and within each participant, by time. There are $N=9$ total observations ($3$ participants $\\times$ $3$ time points).\n\nFirst, we calculate the necessary derived covariates.\n\n1.  **Centered Time ($t_c$)**:\n    For $t \\in \\{0, 1, 2\\}$, the centered time is $t_c = t - 1$. This vector is $(-1, 0, 1)$ for each participant.\n\n2.  **Centered Baseline Age**:\n    The mean baseline age is $\\bar{\\text{age}} = \\frac{48 + 55 + 62}{3} = \\frac{165}{3} = 55$ years.\n    The centered ages for participants $1, 2, 3$ are:\n    - Participant $1$: $48 - 55 = -7$\n    - Participant $2$: $55 - 55 = 0$\n    - Participant $3$: $62 - 55 = 7$\n    This value is constant for all observations from a given participant.\n\n3.  **Decomposed Weight Covariate ($w_{it}$)**:\n    - **Participant-specific mean weights ($\\bar{w}_i$)**:\n      - $\\bar{w}_1 = \\frac{78 + 83 + 85}{3} = \\frac{246}{3} = 82$ kg.\n      - $\\bar{w}_2 = \\frac{74 + 78 + 79}{3} = \\frac{231}{3} = 77$ kg.\n      - $\\bar{w}_3 = \\frac{73 + 74 + 78}{3} = \\frac{225}{3} = 75$ kg.\n    - **Grand mean of mean weights ($\\bar{w}$)**:\n      - $\\bar{w} = \\frac{82 + 77 + 75}{3} = \\frac{234}{3} = 78$ kg.\n    - **Between-participant component ($\\bar{w}_i - \\bar{w}$)**:\n      - Participant $1$: $82 - 78 = 4$.\n      - Participant $2$: $77 - 78 = -1$.\n      - Participant $3$: $75 - 78 = -3$.\n    - **Within-participant component ($w_{it} - \\bar{w}_i$)**:\n      - Participant $1$: $(78-82, 83-82, 85-82) = (-4, 1, 3)$.\n      - Participant $2$: $(74-77, 78-77, 79-77) = (-3, 1, 2)$.\n      - Participant $3$: $(73-75, 74-75, 78-75) = (-2, -1, 3)$.\n\nNow, we construct the design matrices.\n\n**Fixed-Effects Design Matrix ($X$)**\nThe columns of $X$ correspond to the fixed effects: $(1)$ Intercept, $(2)$ centered time $t_c$, $(3)$ centered age, $(4)$ within-participant weight, and $(5)$ between-participant weight. The matrix $X$ is a $9 \\times 5$ matrix.\n$$\nX =\n\\begin{pmatrix}\n1  -1  -7  -4  4 \\\\\n1  0  -7  1  4 \\\\\n1  1  -7  3  4 \\\\\n1  -1  0  -3  -1 \\\\\n1  0  0  1  -1 \\\\\n1  1  0  2  -1 \\\\\n1  -1  7  -2  -3 \\\\\n1  0  7  -1  -3 \\\\\n1  1  7  3  -3\n\\end{pmatrix}\n$$\n\n**Random-Effects Design Matrix ($Z$)**\nThe random effects are a random intercept and a random slope for $t_c$ for each of the $3$ participants. $Z$ is a block-diagonal matrix of size $9 \\times (3 \\times 2) = 9 \\times 6$. Each block $Z_i$ is a $3 \\times 2$ matrix for participant $i$ with columns for the intercept and $t_c$.\n$$\nZ_i = \\begin{pmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\end{pmatrix}\n$$\nSo, the full matrix $Z$ is:\n$$\nZ =\n\\begin{pmatrix}\nZ_1  0  0 \\\\\n0  Z_2  0 \\\\\n0  0  Z_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  -1  0  0  0  0 \\\\\n1  0  0  0  0  0 \\\\\n1  1  0  0  0  0 \\\\\n0  0  1  -1  0  0 \\\\\n0  0  1  0  0  0 \\\\\n0  0  1  1  0  0 \\\\\n0  0  0  0  1  -1 \\\\\n0  0  0  0  1  0 \\\\\n0  0  0  0  1  1\n\\end{pmatrix}\n$$\n\n**Compute the Determinant of $X^{\\top}X$**\nWe first calculate the $5 \\times 5$ matrix $X^{\\top}X$. The $(j,k)$-th element is the dot product of the $j$-th and $k$-th columns of $X$. Let the columns be $x_1, x_2, x_3, x_4, x_5$.\nDue to the centering, many cross-product terms are zero. For example, $\\sum_i \\sum_t t_{cit} = 0$, $\\sum_i \\sum_t (\\text{age}_i-\\overline{\\text{age}}) = 0$, $\\sum_i \\sum_t (w_{it}-\\bar{w}_i) = 0$, and $\\sum_i \\sum_t (\\bar{w}_i-\\bar{w}) = 0$. This orthogonality implies that the dot product of the intercept column $x_1$ with any other column is $0$.\nSimilarly, dot products between covariates that are orthogonal by block are zero (e.g., $x_2^\\top x_3=0, x_2^\\top x_5=0, x_3^\\top x_4=0, x_4^\\top x_5=0$).\nThe non-zero dot products are:\n- $x_1^\\top x_1 = \\sum 1^2 = 9$.\n- $x_2^\\top x_2 = \\sum t_c^2 = 3 \\times ((-1)^2 + 0^2 + 1^2) = 3 \\times 2 = 6$.\n- $x_3^\\top x_3 = \\sum (\\text{age}_c)^2 = 3 \\times (-7)^2 + 3 \\times 0^2 + 3 \\times 7^2 = 3 \\times 49 + 0 + 3 \\times 49 = 294$.\n- $x_4^\\top x_4 = \\sum (w_{it} - \\bar{w}_i)^2 = [(-4)^2+1^2+3^2] + [(-3)^2+1^2+2^2] + [(-2)^2+(-1)^2+3^2] = 26 + 14 + 14 = 54$.\n- $x_5^\\top x_5 = \\sum (\\bar{w}_i - \\bar{w})^2 = 3 \\times 4^2 + 3 \\times (-1)^2 + 3 \\times (-3)^2 = 3 \\times 16 + 3 \\times 1 + 3 \\times 9 = 48 + 3 + 27 = 78$.\n- $x_2^\\top x_4 = \\sum t_c (w_{it}-\\bar{w}_i) = [(-1)(-4)+0(1)+1(3)] + [(-1)(-3)+0(1)+1(2)] + [(-1)(-2)+0(-1)+1(3)] = 7+5+5 = 17$.\n- $x_3^\\top x_5 = \\sum (\\text{age}_c)(\\bar{w}_i-\\bar{w}) = [3(-7)(4)]+[3(0)(-1)]+[3(7)(-3)] = -84 + 0 - 63 = -147$.\n\nSo, the matrix $X^{\\top}X$ is:\n$$\nX^{\\top}X =\n\\begin{pmatrix}\n9  0  0  0  0 \\\\\n0  6  0  17  0 \\\\\n0  0  294  0  -147 \\\\\n0  17  0  54  0 \\\\\n0  0  -147  0  78\n\\end{pmatrix}\n$$\nThis matrix can be rearranged by simultaneously swapping rows and columns to form a block-diagonal matrix without changing the determinant.\n$$\n\\begin{pmatrix}\n9  0  0  0  0 \\\\\n0  6  17  0  0 \\\\\n0  17  54  0  0 \\\\\n0  0  0  294  -147 \\\\\n0  0  0  -147  78\n\\end{pmatrix}\n$$\nThe determinant is the product of the determinants of the diagonal blocks.\n- Block 1: $\\det( [9] ) = 9$.\n- Block 2: $\\det \\begin{pmatrix} 6  17 \\\\ 17  54 \\end{pmatrix} = (6)(54) - (17)(17) = 324 - 289 = 35$.\n- Block 3: $\\det \\begin{pmatrix} 294  -147 \\\\ -147  78 \\end{pmatrix}$. Note that $294 = 2 \\times 147$.\n  $$\n  \\det \\begin{pmatrix} 2 \\times 147  -1 \\times 147 \\\\ -147  78 \\end{pmatrix} = (147)(1) \\det \\begin{pmatrix} 2  -1 \\\\ -147  78 \\end{pmatrix} = 147 \\times (2 \\times 78 - (-1) \\times (-147)) = 147 \\times (156 - 147) = 147 \\times 9 = 1323.\n  $$\n\nFinally, we compute the total determinant of $X^{\\top}X$:\n$$\n\\det(X^{\\top}X) = 9 \\times 35 \\times 1323 = 315 \\times 1323\n$$\n$$\n315 \\times 1323 = 416745\n$$", "answer": "$$ \\boxed{416745} $$", "id": "4924264"}, {"introduction": "One of the most compelling applications of mixed-effects models is their ability to generate personalized predictions. By estimating subject-specific deviations from the average trend, we can forecast an individual's future trajectory based on their observed history. In this final practice problem [@problem_id:4924243], you will apply the principles of conditional probability to a random-intercept and random-slope model. You will calculate a subject's predicted future outcome and derive the formula for its prediction interval, revealing how the model combines population-level trends with individual data to make tailored forecasts.", "problem": "A cohort study follows a biomarker on the logarithmic scale for individuals over time. For a fixed individual $i$, suppose the data at times $t_{i1} = 0$, $t_{i2} = 1$, and $t_{i3} = 3$ are $y_{i1} = 10.2$, $y_{i2} = 11.1$, and $y_{i3} = 12.9$ on the log scale (dimensionless). The analysis uses a linear mixed-effects model (LME) with a random intercept and a random slope for time:\n$$\ny_{ij} = \\beta_0 + \\beta_1 t_{ij} + b_{0i} + b_{1i} t_{ij} + \\varepsilon_{ij},\n$$\nwhere $\\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix} \\sim \\mathcal{N}\\!\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, G\\right)$, $\\varepsilon_{ij} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$, and the random effects and residual errors are independent across observations and mutually independent. You may treat the following parameter values as known: $\\beta_0 = 10.0$, $\\beta_1 = 0.9$, \n$$\nG = \\begin{pmatrix} 0.09  0.015 \\\\ 0.015  0.01 \\end{pmatrix}, \\quad \\sigma^2 = 0.04,\n$$\nand $R_i = \\sigma^2 I_{3}$ for the three past observations of subject $i$.\n\nUsing only the hierarchical Gaussian model structure and standard results for conditioning in multivariate normal distributions, do the following:\n\n1. Derive, from first principles, the expression for the conditional distribution of the subject-specific random effects $\\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix}$ given the past data vector $y_i = (y_{i1}, y_{i2}, y_{i3})^{\\top}$ and the fixed parameters $(\\beta_0,\\beta_1,G,\\sigma^2)$.\n2. Using your result from part 1, compute the conditional prediction (the conditional mean) of the future outcome $y_{i,t^*}$ at a new time $t^* = 2$ for this subject. Report your final numerical answer to four significant figures. No physical units are required.\n3. Present a formula for a two-sided $(1 - \\alpha)$ prediction interval for $y_{i,t^*}$ that incorporates the contributions of both the random effects covariance $G$ and the residual covariance $R_i$. Your formula should make explicit how $G$ and $R_i$ enter the prediction variance. Do not compute a numerical interval; provide only the analytic formula in terms of the model components, $t^*$, and $\\alpha$.", "solution": "The problem presented is a standard application of linear mixed-effects models and the theory of multivariate normal distributions. It is scientifically grounded, well-posed, objective, and internally consistent. All necessary parameters and data are provided. The covariance matrix $G$ is symmetric with diagonal entries $0.09 > 0$ and $0.01 > 0$, and its determinant is $\\det(G) = (0.09)(0.01) - (0.015)^2 = 0.0009 - 0.000225 = 0.000675 > 0$, confirming it is positive definite and thus a valid covariance matrix. The residual variance $\\sigma^2 = 0.04$ is also positive. Therefore, the problem is valid, and I will proceed with the solution.\n\nThe linear mixed-effects model for subject $i$ can be written in matrix form as:\n$$\ny_i = X_i\\beta + Z_ib_i + \\varepsilon_i\n$$\nwhere $y_i$ is the $n_i \\times 1$ vector of observations for subject $i$, $X_i$ is the $n_i \\times p$ design matrix for the fixed effects, $\\beta$ is the $p \\times 1$ vector of fixed effects, $Z_i$ is the $n_i \\times q$ design matrix for the random effects, $b_i$ is the $q \\times 1$ vector of random effects, and $\\varepsilon_i$ is the $n_i \\times 1$ vector of residual errors.\n\nFrom the problem statement, we have:\n-   The data for subject $i$ consists of $n_i=3$ observations. The vector of outcomes is $y_i = \\begin{pmatrix} 10.2 \\\\ 11.1 \\\\ 12.9 \\end{pmatrix}$.\n-   The measurement times are $t_{i1} = 0$, $t_{i2} = 1$, and $t_{i3} = 3$.\n-   The model includes a fixed intercept and a fixed slope for time, so $\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}$. The corresponding design matrix is $X_i = \\begin{pmatrix} 1  t_{i1} \\\\ 1  t_{i2} \\\\ 1  t_{i3} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  3 \\end{pmatrix}$.\n-   The model includes a random intercept and a random slope for time, so $b_i = \\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix}$. The corresponding design matrix is $Z_i = \\begin{pmatrix} 1  t_{i1} \\\\ 1  t_{i2} \\\\ 1  t_{i3} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  3 \\end{pmatrix}$.\n-   The distributions are given as $b_i \\sim \\mathcal{N}(0, G)$ and $\\varepsilon_i \\sim \\mathcal{N}(0, R_i)$, where $G = \\begin{pmatrix} 0.09  0.015 \\\\ 0.015  0.01 \\end{pmatrix}$ and $R_i = \\sigma^2 I_3 = 0.04 I_3$.\n\n### Part 1: Conditional Distribution of Random Effects\n\nTo find the conditional distribution of $b_i$ given $y_i$, we use the properties of the multivariate normal distribution. We first establish the joint distribution of $b_i$ and $y_i$.\nThe model can be rewritten as $y_i - X_i\\beta = Z_ib_i + \\varepsilon_i$.\nLet us consider the joint distribution of the random vectors $b_i$ and $y_i$.\nThe mean of $b_i$ is $E[b_i] = 0$.\nThe mean of $y_i$ is $E[y_i] = E[X_i\\beta + Z_ib_i + \\varepsilon_i] = X_i\\beta + Z_iE[b_i] + E[\\varepsilon_i] = X_i\\beta$.\nThe variance of $b_i$ is $\\text{Var}(b_i) = G$.\nThe variance of $y_i$ is $\\text{Var}(y_i) = \\text{Var}(X_i\\beta + Z_ib_i + \\varepsilon_i) = \\text{Var}(Z_ib_i + \\varepsilon_i)$. Since $b_i$ and $\\varepsilon_i$ are independent, this is $\\text{Var}(Z_ib_i) + \\text{Var}(\\varepsilon_i) = Z_i\\text{Var}(b_i)Z_i^\\top + R_i = Z_iGZ_i^\\top + R_i$. Let us denote this matrix by $V_i$.\nThe covariance between $b_i$ and $y_i$ is $\\text{Cov}(b_i, y_i) = \\text{Cov}(b_i, X_i\\beta + Z_ib_i + \\varepsilon_i) = \\text{Cov}(b_i, Z_ib_i) = \\text{Var}(b_i)Z_i^\\top = GZ_i^\\top$.\nSo, the joint distribution is:\n$$\n\\begin{pmatrix} b_i \\\\ y_i \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ X_i\\beta \\end{pmatrix}, \\begin{pmatrix} G  GZ_i^\\top \\\\ Z_iG  V_i \\end{pmatrix} \\right)\n$$\nUsing the standard formula for conditional distributions in a multivariate normal setting, the distribution of $b_i$ conditional on $y_i$ is also normal, $b_i | y_i \\sim \\mathcal{N}(\\mu_{b|y}, \\Sigma_{b|y})$, with:\n-   Conditional mean: $E[b_i|y_i] = E[b_i] + \\text{Cov}(b_i, y_i) \\text{Var}(y_i)^{-1} (y_i - E[y_i]) = 0 + GZ_i^\\top V_i^{-1} (y_i - X_i\\beta)$. This is also known as the Best Linear Unbiased Predictor (BLUP) of the random effects, denoted $\\hat{b}_i$.\n-   Conditional variance: $\\text{Var}(b_i|y_i) = \\text{Var}(b_i) - \\text{Cov}(b_i, y_i) \\text{Var}(y_i)^{-1} \\text{Cov}(y_i, b_i) = G - GZ_i^\\top V_i^{-1} Z_iG$.\n\nSo, the conditional distribution of $\\begin{pmatrix} b_{0i} \\\\ b_{1i} \\end{pmatrix}$ given $y_i$ is\n$$\n\\mathcal{N}\\left( GZ_i^\\top(Z_iGZ_i^\\top + R_i)^{-1}(y_i - X_i\\beta), \\quad G - GZ_i^\\top(Z_iGZ_i^\\top + R_i)^{-1}Z_iG \\right)\n$$\n\n### Part 2: Conditional Prediction of Future Outcome\n\nThe goal is to compute the conditional prediction $E[y_{i,t^*} | y_i]$ for a new observation at time $t^*=2$. The model for this new observation is:\n$$\ny_{i,t^*} = \\beta_0 + \\beta_1 t^* + b_{0i} + b_{1i} t^* + \\varepsilon_{i,t^*} = x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top b_i + \\varepsilon_{i,t^*}\n$$\nwhere $x_{i,t^*} = z_{i,t^*} = \\begin{pmatrix} 1 \\\\ t^* \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe conditional expectation is:\n$$\nE[y_{i,t^*} | y_i] = E[x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top b_i + \\varepsilon_{i,t^*} | y_i]\n$$\nBy linearity of expectation, and noting that $x_{i,t^*}^\\top\\beta$ is a fixed quantity:\n$$\nE[y_{i,t^*} | y_i] = x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top E[b_i | y_i] + E[\\varepsilon_{i,t^*} | y_i]\n$$\nThe future error $\\varepsilon_{i,t^*}$ is independent of past data $y_i$, so $E[\\varepsilon_{i,t^*} | y_i] = E[\\varepsilon_{i,t^*}] = 0$.\nThe prediction is therefore $\\hat{y}_{i,t^*} = x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top \\hat{b}_i$, where $\\hat{b}_i = E[b_i | y_i]$ is the conditional mean derived in Part 1.\n\nWe now compute this value.\nFirst, the residual vector based on fixed effects:\n$$\ny_i - X_i\\beta = \\begin{pmatrix} 10.2 \\\\ 11.1 \\\\ 12.9 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 10.0 \\\\ 0.9 \\end{pmatrix} = \\begin{pmatrix} 10.2 \\\\ 11.1 \\\\ 12.9 \\end{pmatrix} - \\begin{pmatrix} 10.0 \\\\ 10.9 \\\\ 12.7 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ 0.2 \\\\ 0.2 \\end{pmatrix}\n$$\nNext, we compute $V_i = Z_iGZ_i^\\top + R_i$:\n$$\nZ_iG = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 0.09  0.015 \\\\ 0.015  0.01 \\end{pmatrix} = \\begin{pmatrix} 0.09  0.015 \\\\ 0.105  0.025 \\\\ 0.135  0.045 \\end{pmatrix}\n$$\n$$\nZ_iGZ_i^\\top = \\begin{pmatrix} 0.09  0.015 \\\\ 0.105  0.025 \\\\ 0.135  0.045 \\end{pmatrix} \\begin{pmatrix} 1  1  1 \\\\ 0  1  3 \\end{pmatrix} = \\begin{pmatrix} 0.09  0.105  0.135 \\\\ 0.105  0.130  0.180 \\\\ 0.135  0.180  0.270 \\end{pmatrix}\n$$\n$$\nV_i = Z_iGZ_i^\\top + R_i = \\begin{pmatrix} 0.09  0.105  0.135 \\\\ 0.105  0.130  0.180 \\\\ 0.135  0.180  0.270 \\end{pmatrix} + \\begin{pmatrix} 0.04  0  0 \\\\ 0  0.04  0 \\\\ 0  0  0.04 \\end{pmatrix} = \\begin{pmatrix} 0.13  0.105  0.135 \\\\ 0.105  0.170  0.180 \\\\ 0.135  0.180  0.310 \\end{pmatrix}\n$$\nThe inverse of $V_i$ is required. Calculation yields:\n$$\nV_i^{-1} \\approx \\begin{pmatrix} 16.55791  -6.72920  -3.30343 \\\\ -6.72920  18.00571  -7.52447 \\\\ -3.30343  -7.52447  9.03344 \\end{pmatrix}\n$$\nNow we compute $\\hat{b}_i = GZ_i^\\top V_i^{-1} (y_i - X_i\\beta)$:\n$$\nV_i^{-1}(y_i - X_i\\beta) \\approx \\begin{pmatrix} 16.55791  -6.72920  -3.30343 \\\\ -6.72920  18.00571  -7.52447 \\\\ -3.30343  -7.52447  9.03344 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ 0.2 \\\\ 0.2 \\end{pmatrix} = 0.2 \\begin{pmatrix} 6.52528 \\\\ 3.75204 \\\\ -1.79446 \\end{pmatrix} = \\begin{pmatrix} 1.305056 \\\\ 0.750408 \\\\ -0.358892 \\end{pmatrix}\n$$\nAnd $GZ_i^\\top = (Z_iG)^\\top = \\begin{pmatrix} 0.09  0.105  0.135 \\\\ 0.015  0.025  0.045 \\end{pmatrix}$.\n$$\n\\hat{b}_i = \\begin{pmatrix} 0.09  0.105  0.135 \\\\ 0.015  0.025  0.045 \\end{pmatrix} \\begin{pmatrix} 1.305056 \\\\ 0.750408 \\\\ -0.358892 \\end{pmatrix} \\approx \\begin{pmatrix} 0.117455 + 0.078793 - 0.048450 \\\\ 0.019576 + 0.018760 - 0.016150 \\end{pmatrix} = \\begin{pmatrix} 0.147798 \\\\ 0.022186 \\end{pmatrix}\n$$\nFinally, we compute the prediction for $t^*=2$:\n$$\n\\hat{y}_{i,2} = x_{i,2}^\\top\\beta + z_{i,2}^\\top \\hat{b}_i = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 10.0 \\\\ 0.9 \\end{pmatrix} + \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 0.147798 \\\\ 0.022186 \\end{pmatrix}\n$$\n$$\n\\hat{y}_{i,2} = (10.0 + 1.8) + (0.147798 + 2 \\times 0.022186) = 11.8 + (0.147798 + 0.044372) = 11.8 + 0.19217 = 11.99217\n$$\nRounding to four significant figures, the prediction is $11.99$.\n\n### Part 3: Prediction Interval Formula\n\nA two-sided $(1-\\alpha)$ prediction interval for $y_{i,t^*}$ is given by $\\hat{y}_{i,t^*} \\pm z_{1-\\alpha/2} \\sqrt{\\text{Var}(y_{i,t^*} - \\hat{y}_{i,t^*})}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution $\\mathcal{N}(0,1)$. The term under the square root is the prediction error variance.\nThe prediction error is:\n$$\ny_{i,t^*} - \\hat{y}_{i,t^*} = (x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top b_i + \\varepsilon_{i,t^*}) - (x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top \\hat{b}_i) = z_{i,t^*}^\\top (b_i - \\hat{b}_i) + \\varepsilon_{i,t^*}\n$$\nThe variance of the prediction error is $\\text{Var}(y_{i,t^*} - \\hat{y}_{i,t^*})$. The error term $\\varepsilon_{i,t^*}$ is independent of past data $y_i$ and hence of $\\hat{b}_i=E[b_i|y_i]$. It is also independent of $b_i$ by model assumption. Therefore:\n$$\n\\text{Var}(y_{i,t^*} - \\hat{y}_{i,t^*}) = \\text{Var}(z_{i,t^*}^\\top (b_i - \\hat{b}_i)) + \\text{Var}(\\varepsilon_{i,t^*}) = z_{i,t^*}^\\top \\text{Var}(b_i - \\hat{b}_i) z_{i,t^*} + \\sigma^2\n$$\nThe variance of the error in predicting the random effects, $\\text{Var}(b_i - \\hat{b}_i)$, is equal to the posterior variance of the random effects, $\\text{Var}(b_i|y_i)$, which was derived in Part 1. This is because $\\text{Var}(b_i|y_i)$ is not a function of $y_i$ in a linear Gaussian model.\n$$\n\\text{Var}(b_i|y_i) = G - GZ_i^\\top(Z_iGZ_i^\\top + R_i)^{-1}Z_iG\n$$\nThe prediction error variance is thus:\n$$\n\\text{Var}(y_{i,t^*} - \\hat{y}_{i,t^*}) = z_{i,t^*}^\\top \\left[ G - GZ_i^\\top(Z_iGZ_i^\\top + R_i)^{-1}Z_iG \\right] z_{i,t^*} + \\sigma^2\n$$\nThe first term represents the uncertainty from predicting the subject-specific trajectory (i.e., uncertainty about $b_i$), which depends on both the prior random effects covariance $G$ and the residual covariance $R_i$ (through the inverse term). The second term, $\\sigma^2$ (a diagonal element of $R_i$), is the uncertainty of a new measurement.\nThe complete formula for the $(1-\\alpha)$ prediction interval is:\n$$\n\\hat{y}_{i,t^*} \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{ z_{i,t^*}^\\top \\left( G - GZ_i^\\top(Z_iGZ_i^\\top + R_i)^{-1}Z_iG \\right) z_{i,t^*} + \\sigma^2 }\n$$\nwhere $\\hat{y}_{i,t^*} = x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top GZ_i^\\top(Z_iGZ_i^\\top + R_i)^{-1}(y_i - X_i\\beta)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\mathcal{N} \\left( GZ_i^\\top V_i^{-1}(y_i - X_i\\beta), G - GZ_i^\\top V_i^{-1}Z_iG \\right)  11.99  x_{i,t^*}^\\top\\beta + z_{i,t^*}^\\top GZ_i^\\top V_i^{-1}(y_i - X_i\\beta) \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{z_{i,t^*}^\\top (G - GZ_i^\\top V_i^{-1}Z_iG) z_{i,t^*} + \\sigma^2} \\end{pmatrix}}\n$$", "id": "4924243"}]}