## Applications and Interdisciplinary Connections

Having established the theoretical principles and mechanisms of [propensity score](@entry_id:635864) methods, we now turn to their application. The true value of any statistical methodology is realized in its capacity to solve real-world problems. This chapter explores how propensity score methods are employed across a diverse range of disciplines—from clinical medicine and public health to pharmacoepidemiology and oncology—to address the fundamental challenge of confounding in observational research. Our objective is not to reiterate the core principles but to demonstrate their utility, extension, and integration in applied settings. We will examine how these methods are adapted for various study designs, [data structures](@entry_id:262134), and research questions, and how practitioners assess the robustness of their findings to the key, untestable assumption of no unmeasured confounding.

### Addressing Confounding by Indication in Clinical and Public Health Research

One of the most pervasive challenges in clinical and epidemiological research is **confounding by indication**. This form of bias arises when the clinical factors that prompt a physician to prescribe a certain treatment are also prognostic factors for the outcome of interest. Patients who receive a particular therapy are often systematically different from those who do not, in ways that are directly related to their expected outcome. A naive comparison of outcomes between treated and untreated groups can therefore be profoundly misleading.

A stark illustration of this phenomenon, a variant of Simpson's Paradox, can be found in observational studies of childbirth. Consider a hypothetical study evaluating outcomes for breech (feet-first) presentations, comparing planned vaginal delivery to planned cesarean delivery. Suppose that clinicians preferentially recommend cesarean delivery for fetuses with high-risk features (e.g., low estimated weight or hyperextended neck), which are themselves associated with a higher risk of neonatal morbidity. In data from such a setting, it is possible to observe that the overall rate of morbidity is higher in the planned cesarean group than in the planned vaginal delivery group. However, upon stratifying by the high-risk and low-risk indications, one might find that within each stratum, planned cesarean delivery is associated with a *lower* risk of morbidity. The crude, unadjusted comparison is biased because the cesarean group is disproportionately composed of high-risk patients, whose poor outcomes are attributable to their underlying risk status rather than the delivery method itself. Propensity score methods are designed to dissect and correct precisely this type of confounding, allowing for a more valid comparison of the treatment strategies [@problem_id:4408686].

The successful application of [propensity score](@entry_id:635864) methods to control for confounding by indication hinges on the careful construction of the propensity score model. A critical principle is that the model must only include covariates measured *before* the treatment decision is made. Including variables measured after treatment can introduce substantial bias. For example, in a study evaluating the effect of initiating antidepressants during pregnancy on postpartum depression, a researcher might be tempted to include covariates like a second-trimester mood score or medication adherence in the [propensity score](@entry_id:635864) model, perhaps to improve its predictive accuracy. This is a crucial error. These are post-treatment variables; they may be mediators on the causal pathway from treatment to outcome or may be affected by other factors that also influence the outcome (acting as colliders). Adjusting for them can block the estimation of the total causal effect or induce spurious associations. A valid analysis must restrict the propensity score model to pre-exposure characteristics, such as baseline depression severity, clinical history, and sociodemographic factors, to ensure that the adjustment mimics the conditions of a randomized trial at baseline [@problem_id:4738455].

It is also vital to distinguish confounding by indication, which arises from pre-treatment factors, from other sources of bias such as selection bias, which can arise from the process of assembling the study cohort. Consider a study on a new anticoagulant where inclusion in the final analysis requires a follow-up visit. If the treatment itself or a patient's underlying prognosis influences their likelihood of returning for follow-up, restricting the analysis to only those with complete data can induce selection bias. This bias occurs because the selection into the sample is a common effect of factors related to treatment and outcome. Standard propensity score adjustment based on pre-treatment covariates cannot, in general, correct for this type of post-treatment selection bias. Addressing it requires different, specialized methods, such as inverse probability of selection weighting. Differentiating these sources of bias is a prerequisite for choosing the appropriate analytic tool [@problem_id:4830539].

### The Propensity Score Workflow in Practice: Estimation and Diagnostics

Once a set of appropriate pre-treatment covariates has been selected, the [propensity score](@entry_id:635864) is typically estimated using a statistical model. The most common choice is [logistic regression](@entry_id:136386), where the binary treatment indicator is regressed on the selected covariates. The goal of this model is to estimate the conditional probability of treatment for each individual. The parameters of the logistic regression model are determined through the principle of maximum likelihood, which involves finding the parameter values that make the observed data most probable. This procedure yields a set of score equations that can be solved numerically to obtain the parameter estimates, which in turn define the estimated propensity score for each subject in the study [@problem_id:4943106].

However, simply fitting a model is not sufficient. The ultimate purpose of the propensity score model is not to predict treatment assignment with maximum accuracy, but to achieve balance in the distribution of baseline covariates between the treated and control groups. Therefore, a critical part of any propensity score analysis is a thorough diagnostic assessment to verify that this goal has been met.

Several diagnostic tools are essential:

*   **Covariate Balance Assessment**: This is the most important diagnostic. The goal is to check whether the distributions of the baseline covariates are similar in the treated and control groups after adjustment (e.g., after matching or weighting). The most common metric for this is the **Standardized Mean Difference (SMD)**, which expresses the difference in means between groups in units of standard deviation. A common rule of thumb suggests that an absolute SMD below $0.1$ indicates negligible imbalance. Balance should be checked for all covariates included in the [propensity score](@entry_id:635864) model, as well as their higher-order terms (e.g., squares) and interactions, to ensure robust adjustment.

*   **Overlap Assessment**: This diagnostic addresses the positivity assumption, which requires that individuals at every level of the covariates have a non-zero probability of being in either treatment group. In practice, this is assessed by examining the distribution of the estimated propensity scores in the treated and control groups, often via histograms or density plots. A lack of substantial overlap in the distributions indicates a practical violation of positivity, where certain types of individuals in one group have no comparable counterparts in the other. This can lead to highly variable and biased estimates, particularly in weighting analyses where individuals with scores near $0$ or $1$ receive extreme weights.

*   **Model Calibration**: A calibration plot compares the average predicted probabilities from the propensity score model to the observed proportion of treated individuals within strata (e.g., quintiles or deciles) of the propensity score. A well-calibrated model will have points lying close to the $45$-degree line. While good calibration is desirable, it does not by itself guarantee covariate balance. Balance must always be checked directly.

It is crucial to recognize that some common statistical metrics are not appropriate for assessing propensity score models. For instance, a high Area Under the Receiver Operating Characteristic Curve (AUC), which measures the model's ability to discriminate between treated and untreated individuals, is often a sign of poor overlap rather than a good model for confounding adjustment. An AUC close to $1.0$ suggests that the treatment groups are nearly perfectly separated by their covariates, making it difficult to find comparable subjects and thus undermining causal inference. The primary criterion for a good propensity score model is its ability to produce balanced covariate distributions, not its predictive power [@problem_id:4548969].

### Methodological Extensions and Diverse Applications

The [propensity score](@entry_id:635864) framework is highly flexible and can be extended beyond the basic comparison of two groups to accommodate a wide variety of research questions and data structures.

#### Comparing Methods and Estimands: ATE vs. ATT

Different [propensity score](@entry_id:635864) methods—primarily matching, stratification, and [inverse probability](@entry_id:196307) of treatment weighting (IPW)—not only have different statistical properties but can also target different causal quantities, or **estimands**. Two of the most common estimands are the **Average Treatment Effect (ATE)**, defined as $E[Y(1) - Y(0)]$, and the **Average Treatment Effect on the Treated (ATT)**, defined as $E[Y(1) - Y(0) \mid A=1]$. The ATE represents the average effect if the entire population were treated versus not treated, while the ATT represents the average effect for those individuals who actually received the treatment.

*   **Inverse Probability of Treatment Weighting (IPW)**: Standard IPW, which weights treated subjects by $1/e(\mathbf{X})$ and control subjects by $1/(1-e(\mathbf{X}))$, typically targets the ATE. This method can be sensitive to [model misspecification](@entry_id:170325), especially when propensity scores are close to $0$ or $1$, as the resulting large weights can amplify errors and increase variance.

*   **Propensity Score Matching**: When one-to-one matching is performed by finding a control subject for each treated subject (a common approach), the method inherently targets the ATT. The resulting matched control group is constructed to have the same covariate distribution as the treated group, making the comparison relevant for that specific population. Matching is often considered more robust to moderate [model misspecification](@entry_id:170325) than IPW because it relies more on the rank-ordering of the propensity scores than their precise values.

*   **Propensity Score Stratification**: This method, which involves calculating treatment effects within quintiles or deciles of the [propensity score](@entry_id:635864), can approximate either the ATE or ATT depending on how the stratum-specific effects are aggregated. Weighting by the proportion of all subjects in each stratum targets the ATE. This method is also somewhat robust to extreme scores but can suffer from residual confounding within the finite strata.

The choice of method and estimand should be driven by the specific research question. For instance, a policy maker might be interested in the ATE to understand the population-level impact of a program, while a clinician might be more interested in the ATT to understand the effect for the types of patients who are currently receiving a therapy [@problem_id:5001924].

To further refine matching procedures, researchers often match on the **logit of the propensity score** rather than the raw score itself. The logit transformation, $\ln(e(\mathbf{X})/(1-e(\mathbf{X})))$, can spread out the scores more evenly, especially near the boundaries of $0$ and $1$. A small difference on the raw score scale near these boundaries can correspond to a large difference in the odds of treatment. Matching on the logit scale can therefore prevent poor matches that would appear acceptable on the raw scale. Additionally, **calipers** are often imposed, which are pre-specified limits on the maximum allowable distance between matched pairs on the [propensity score](@entry_id:635864) (or logit-score) scale. Using a caliper helps to avoid poor-quality matches by ensuring that only sufficiently similar individuals are compared, which can reduce bias at the potential cost of discarding some subjects from the analysis [@problem_id:4943113].

#### Generalizations for Complex Data Structures

Propensity score methods have been successfully generalized to handle more complex scenarios than a simple binary treatment.

*   **Multiple Treatment Groups**: When a study involves a treatment with $K2$ levels, the propensity score becomes a vector, $\mathbf{e}(\mathbf{X}) = (P(A=1|\mathbf{X}), \dots, P(A=K|\mathbf{X}))$. Conditioning on this entire vector is sufficient to balance covariates across all $K$ treatment arms. Weighting methods can be directly extended, with the weight for an individual in treatment group $k$ being $1/e_k(\mathbf{X})$. For [pairwise comparisons](@entry_id:173821) (e.g., comparing treatment $k$ to treatment $l$), one can restrict the analysis to individuals in those two groups and compute a new, binary-like [propensity score](@entry_id:635864) for that specific comparison, defined as $P(A=k | A \in \{k,l\}, \mathbf{X})$, which can be shown to be equal to $e_k(\mathbf{X}) / (e_k(\mathbf{X}) + e_l(\mathbf{X}))$ [@problem_id:4943107].

*   **Time-to-Event Outcomes**: In studies with censored survival data, [propensity score](@entry_id:635864) weighting can be integrated with survival analysis methods. For example, an **inverse-probability-of-treatment-weighted (IPTW) Kaplan-Meier estimator** can be constructed. In this approach, the standard Kaplan-Meier procedure is modified so that at each event time, the number of events and the number of individuals at risk are calculated as the sum of the weights of the individuals, rather than as simple counts. This allows for the estimation of marginal survival curves for each treatment group that are adjusted for baseline confounding. This method requires the standard assumption of [non-informative censoring](@entry_id:170081), conditional on the baseline covariates used in the [propensity score](@entry_id:635864) model [@problem_id:4943079].

*   **Longitudinal Data and Time-Varying Confounding**: A particularly challenging scenario arises in longitudinal studies where a treatment is administered over time and there are **time-varying confounders** that are themselves affected by past treatment. For example, in an HIV study, a physician's decision to start a therapy at a given time point may depend on the patient's current CD4 count, but that CD4 count is also affected by whether the patient received therapy in the past. Standard propensity score methods fail here because adjusting for the time-varying confounder (CD4 count) also means adjusting for a consequence of past treatment. **Marginal Structural Models (MSMs)**, estimated via **stabilized IPTW**, are designed to handle this situation. This involves calculating a weight for each person at each time point. The weight is a ratio where the denominator is the probability of receiving the observed treatment at that time, given the full past history of treatments and covariates, and the numerator is the probability of receiving that treatment given only baseline covariates and past treatments. By weighting the data in this way, one can create a pseudo-population in which the effect of treatment on the time-varying confounders is broken, allowing for an unbiased estimate of the causal effect of the treatment strategy on the outcome [@problem_id:4943110].

### Propensity Scores in Specialized Research Contexts

Propensity score methods are also tailored for use in specific data environments and study designs that present unique challenges.

#### Pharmacoepidemiology and High-Dimensional Propensity Scores

Modern pharmacoepidemiology often relies on massive administrative claims databases or electronic health records (EHRs). While these databases are large, they often lack direct measures of important confounders like lifestyle factors or disease severity. To address this, the **high-dimensional propensity score (hdPS)** was developed. This is an algorithmic approach that generates thousands of potential covariates from the database (e.g., binary indicators for every diagnosis, procedure, and drug dispensing code in a pre-treatment look-back period). The algorithm then prioritizes variables that are empirically associated with both treatment and outcome to serve as proxies for the true, unmeasured confounders. While hdPS can improve confounding control by systematically leveraging the richness of the data, it does not guarantee elimination of confounding. It can also be vulnerable to variance inflation if it includes variables that are strong predictors of treatment but unrelated to the outcome (so-called "[instrumental variables](@entry_id:142324)"), and it makes the assessment of positivity even more critical [@problem_id:4620082].

#### Generalizability and Transporting Effects to a Target Population

Often, the results of a randomized trial or [observational study](@entry_id:174507) are obtained from a study sample that is not representative of the target population to which we wish to apply the findings. For example, a clinical trial may have stricter inclusion criteria than the real-world patient population. Propensity-score-like reweighting techniques can be used to **transport** the causal effect from the source study to a different target population. This is achieved by weighting individuals in the source study to match the covariate distribution of the target population. The weight for each individual in the source study is proportional to the ratio of the probability of having their covariates in the target population versus the source population. This "sampling score" can be estimated by pooling the source and target samples and fitting a [logistic regression model](@entry_id:637047) for population membership. This reweighting allows for the estimation of the average treatment effect in the target population, provided that key assumptions hold, including that the treatment effect itself does not vary between populations after accounting for covariates ($Y(a) \perp S \mid \mathbf{X}$) and that there is sufficient overlap in covariate distributions [@problem_id:4943094].

#### Case-Control Studies

The case-control study design, which samples individuals based on their outcome status, presents a unique challenge for propensity score estimation. Because sampling depends on the outcome, the relationship between treatment and covariates in the case-control sample is distorted compared to the underlying source population. A standard [logistic regression](@entry_id:136386) of treatment on covariates in the case-control sample does not consistently estimate the true [propensity score](@entry_id:635864). There are two primary strategies to address this. First, if the sampling fractions for cases and controls are known, one can use **[inverse probability](@entry_id:196307) of sampling weighting (IPSW)** to re-weight the case-control sample to reconstruct the source population, and then estimate the propensity score on this weighted sample. Second, under a **rare disease assumption**, the distribution of covariates among the controls is approximately the same as in the entire source population. Therefore, one can estimate the propensity score by fitting a model for treatment on covariates using only the control group [@problem_id:4943119].

### Assessing Robustness to Unmeasured Confounding

The foundational assumption of all propensity score methods is that of conditional exchangeability, or "no unmeasured confounding." Since this assumption is untestable, a rigorous analysis must include sensitivity analyses to probe how the conclusions might change if this assumption were violated.

#### Negative Control Methods

One powerful approach is the use of **negative controls**. This involves testing for a causal effect where one is known, *a priori*, not to exist.

*   A **negative control outcome** ($Y^{\text{nc}}$) is an outcome that is known not to be causally affected by the treatment of interest ($A$), but is thought to be subject to the same confounding structure. After performing a [propensity score](@entry_id:635864) analysis, if a non-zero association is found between the treatment $A$ and the [negative control](@entry_id:261844) outcome $Y^{\text{nc}}$, it suggests the presence of residual confounding that the adjustment failed to remove. This would cast doubt on the validity of the primary finding for the outcome of interest.

*   A **negative control exposure** ($A^{\text{nc}}$) is an exposure known not to cause the outcome of interest ($Y$), but which is likely to have the same confounders as the primary exposure. Finding an association between $A^{\text{nc}}$ and $Y$ after adjustment would again suggest the presence of residual confounding.

These methods act as important [falsification](@entry_id:260896) tests; observing an association where none should exist provides strong evidence that the primary analysis may also be biased [@problem_id:5221131].

#### Quantitative Sensitivity Analysis: The E-Value

While negative controls provide a qualitative check, the **E-value** provides a quantitative measure for sensitivity analysis. For an observed risk ratio (RR), the E-value is the minimum strength of association, on the risk ratio scale, that an unmeasured confounder would need to have with both the treatment and the outcome to fully "explain away" the observed association (i.e., to reduce the true causal effect to null). For an observed risk ratio $\text{RR}_{\text{obs}} > 1$, the E-value can be calculated as:
$$ \text{E-value} = \text{RR}_{\text{obs}} + \sqrt{\text{RR}_{\text{obs}}(\text{RR}_{\text{obs}}-1)} $$
For instance, an observed risk ratio of $1.45$ yields an E-value of approximately $2.26$. This means that an unmeasured confounder associated with both the exposure and the outcome by risk ratios of at least $2.26$ each would be needed to explain away the finding. A separate E-value can be calculated for the confidence interval limit closest to the null to assess the strength of confounding needed to shift the interval to include no effect. By providing a quantitative benchmark, the E-value helps researchers and readers judge the plausibility of unmeasured confounding as an alternative explanation for an observed association [@problem_id:4943084].

In conclusion, [propensity score](@entry_id:635864) methods represent a versatile and powerful suite of tools for enhancing the credibility of causal claims from observational data. Their effective application, however, is not a simple mechanical exercise. It demands deep subject-matter knowledge to select covariates, careful diagnostic checking to ensure balance, a clear understanding of the target estimand, and a humble appreciation for the untestable assumption of no unmeasured confounding, which must be rigorously probed with sensitivity analyses. When applied with this level of care, [propensity score](@entry_id:635864) methods are an indispensable component of modern scientific inquiry in a wide array of disciplines.