## Introduction
Drawing causal conclusions from observational data is a central challenge in science, frequently complicated by confounding. When comparing groups that were not formed by random assignment, systematic differences in their characteristics can distort the true effect of a treatment or exposure, leading to biased results. Propensity score methods offer a powerful statistical framework to address this problem by mimicking the baseline balance achieved in a randomized controlled trial. This article provides a comprehensive guide to understanding and applying these essential techniques for causal inference.

Over the next three chapters, you will build a robust understanding of [propensity score](@entry_id:635864) analysis. We will begin in **Principles and Mechanisms** by establishing the theoretical foundation, including the potential outcomes framework, the critical assumptions of consistency, unconfoundedness, and positivity, and the core properties that make the propensity score such an effective tool for dimensionality reduction. Next, **Applications and Interdisciplinary Connections** will demonstrate how these methods are used to solve real-world problems in fields like epidemiology and clinical research, covering the practical workflow of model building, diagnostic checking, and methodological extensions for complex [data structures](@entry_id:262134). Finally, **Hands-On Practices** will offer opportunities to apply these concepts through targeted exercises, reinforcing your ability to implement and interpret propensity score analyses correctly.

## Principles and Mechanisms

In the preceding chapter, we introduced the fundamental challenge of causal inference in observational studies: confounding. When the groups we wish to compare—for instance, those who received a treatment and those who did not—differ systematically in their baseline characteristics, a simple comparison of their outcomes is likely to be biased. Propensity score methods provide a powerful and flexible framework for addressing this challenge. This chapter delves into the theoretical principles that underpin these methods and the mechanisms through which they operate to mitigate [confounding bias](@entry_id:635723).

### Core Assumptions for Causal Inference

Before we can use observational data to answer causal questions, we must establish a clear theoretical framework. The potential outcomes model provides this foundation. For a binary treatment $A \in \{0,1\}$, we posit that each individual possesses two potential outcomes: $Y(1)$, the outcome that would be observed if the individual received the treatment, and $Y(0)$, the outcome that would be observed if they did not. The causal effect for that individual is the unobservable quantity $Y(1) - Y(0)$. Our goal is often to estimate a population average of these effects, such as the **Average Treatment Effect (ATE)**, $\mathbb{E}[Y(1) - Y(0)]$.

The difficulty, of course, is that we only observe one of these potential outcomes for each person. To connect the data we do observe—the treatment $A$, the covariates $\mathbf{X}$, and the observed outcome $Y$—to the potential outcomes we care about, we rely on a set of critical, untestable assumptions.

#### Consistency and SUTVA

The first two assumptions are foundational, linking the abstract world of potential outcomes to the concrete world of observed data.
The **Consistency** assumption states that an individual's observed outcome is their potential outcome corresponding to the treatment they actually received. Formally, for any individual $i$, if their received treatment is $A_i=a$, then their observed outcome is $Y_i = Y_i(a)$. Without this link, our observed data would be irrelevant to the causal question.

The **Stable Unit Treatment Value Assumption (SUTVA)** comprises two related ideas. First, it assumes **no interference** between individuals, meaning that one person's potential outcome is not affected by the treatment status of anyone else. This allows us to write the potential outcome as $Y_i(a)$, a function only of individual $i$'s treatment, rather than a more complex function of the entire population's treatment assignments. Second, it assumes **no hidden variations of treatment**, meaning that for any treatment level $A=a$, the potential outcome is unambiguous. For example, all individuals receiving the treatment $A=1$ are assumed to receive the same version of it. Together, SUTVA and consistency ensure that the potential outcomes $\mathbb{E}[Y(1)]$ and $\mathbb{E}[Y(0)]$ are well-defined quantities that our observed data can, in principle, inform [@problem_id:4943091].

#### Unconfoundedness and Positivity

With the framework established, we turn to the central problem of confounding. In an ideal randomized experiment, treatment assignment $A$ is independent of the potential outcomes $(Y(1), Y(0))$, a condition we write as $(Y(1), Y(0)) \perp A$. This ensures that the treated and control groups are, on average, identical before treatment begins, so any subsequent difference in outcomes can be attributed to the treatment. In observational studies, this is rarely true. The groups often differ systematically, meaning that treatment assignment is associated with the potential outcomes. This association is the formal definition of confounding: $Y(a) \not\perp A$ [@problem_id:4943089].

To proceed, we invoke the assumption of **conditional exchangeability**, also known as **unconfoundedness** or **ignorability**. This assumption states that, conditional on a sufficient set of pre-treatment covariates $\mathbf{X}$, treatment assignment is independent of the potential outcomes.
$$ (Y(1), Y(0)) \perp A \mid \mathbf{X} $$
This is the crucial "no unmeasured confounders" assumption. It posits that within strata defined by the covariates in $\mathbf{X}$, the treatment assignment is "as-if" random. If we can measure all variables that are common causes of both treatment and outcome, we can adjust for them to remove [confounding bias](@entry_id:635723).

Finally, for adjustment to be possible, we need the **positivity** assumption, also known as the **common support** or **overlap** condition. This requires that for any set of covariate values $\mathbf{x}$ present in the population, there is a non-zero probability of being in either treatment group.
$$ 0  P(A=1 \mid \mathbf{X}=\mathbf{x})  1 $$
Positivity ensures that for any type of individual defined by a covariate profile $\mathbf{x}$, we can find some who were treated and some who were not, making comparison possible. A violation of positivity is a fundamental failure of identification. For example, consider a hypothetical scenario where for a particular covariate profile, say $\mathbf{X}=\mathbf{x}'$, every single person receives the treatment, such that $P(A=1 \mid \mathbf{X}=\mathbf{x}')=1$. In this case, we have no data on what would have happened to these individuals under the control condition ($A=0$). The counterfactual mean $\mathbb{E}[Y(0) \mid \mathbf{X}=\mathbf{x}']$ is simply not identifiable from the data. Any method that relies on comparing treated and untreated individuals, including propensity score weighting, will fail. In inverse probability weighting, for instance, the weight for an untreated individual involves the term $1/(1 - P(A=1 \mid \mathbf{X}))$. If $P(A=1 \mid \mathbf{X}=\mathbf{x}')=1$, this weight becomes infinite, and the estimator is undefined [@problem_id:4943066]. Any potential solution, such as trimming the sample to a region of common support, necessarily changes the target of inference from the ATE in the full population to the ATE in the restricted (trimmed) subpopulation where overlap exists [@problem_id:4943066].

### The Propensity Score and its Core Properties

Assuming we have measured a sufficient set of covariates $\mathbf{X}$ to ensure unconfoundedness, we face a practical problem. If $\mathbf{X}$ is high-dimensional (contains many covariates, some of which may be continuous), conditioning on it directly—for example, by stratifying on every unique combination of covariates—becomes infeasible. This is often called the "[curse of dimensionality](@entry_id:143920)."

This is where the propensity score, introduced by Rosenbaum and Rubin in 1983, provides a powerful solution. The **propensity score**, denoted $e(\mathbf{X})$, is the conditional probability of receiving the treatment given the set of pre-treatment covariates.
$$ e(\mathbf{X}) = P(A=1 \mid \mathbf{X}) $$
The utility of the propensity score stems from two remarkable properties.

1.  **The Balancing Property**: The [propensity score](@entry_id:635864) is a **balancing score**. This means that conditional on the [propensity score](@entry_id:635864), the distribution of the full covariate vector $\mathbf{X}$ is the same for the treated ($A=1$) and control ($A=0$) groups. Formally, $\mathbf{X} \perp A \mid e(\mathbf{X})$. This property implies that if we compare individuals who have the same [propensity score](@entry_id:635864), we are comparing individuals who, on average, have the same distribution of baseline covariates $\mathbf{X}$. For example, a group of treated individuals with $e(\mathbf{X})=0.7$ and a group of untreated individuals with $e(\mathbf{X})=0.7$ will have similar distributions of age, disease severity, and all other covariates included in $\mathbf{X}$. This mimics the balance we seek in a randomized trial.

2.  **Sufficiency for Unconfoundedness**: The second key property is that if unconfoundedness holds given the full covariate set $\mathbf{X}$, it also holds given the scalar [propensity score](@entry_id:635864) $e(\mathbf{X})$.
    $$ (Y(1), Y(0)) \perp A \mid \mathbf{X} \implies (Y(1), Y(0)) \perp A \mid e(\mathbf{X}) $$
    This is a profound result. It means that to remove [confounding bias](@entry_id:635723) caused by a potentially vast number of covariates in $\mathbf{X}$, we only need to condition on the single variable $e(\mathbf{X})$ [@problem_id:4943089] [@problem_id:4515359]. The [propensity score](@entry_id:635864) acts as a "summary" of all the confounding information in $\mathbf{X}$, allowing for a dramatic reduction in dimensionality.

It is worth noting that these properties are not unique to the [propensity score](@entry_id:635864) itself. Any [one-to-one transformation](@entry_id:148028) of the propensity score, for instance the logit transformation $\log(e(\mathbf{X})/(1-e(\mathbf{X})))$, is also a balancing score and is sufficient for confounding adjustment. This is because a [one-to-one transformation](@entry_id:148028) preserves all the information contained within the original score, meaning conditioning on the transformed score is equivalent to conditioning on the original [@problem_id:4943085].

### Mechanisms of Propensity Score Adjustment

The properties of the propensity score give rise to several methods for estimating causal effects. The general strategy is to first estimate the [propensity score](@entry_id:635864) for each individual (typically using a [logistic regression model](@entry_id:637047) of $A$ on $\mathbf{X}$) and then use the estimated scores to create a sample in which the covariate distributions are balanced between the treatment groups.

#### Subclassification on the Propensity Score

One of the most intuitive methods is **subclassification** or **stratification**. In this approach, we partition the range of the propensity score (from $0$ to $1$) into a number of strata (e.g., five strata, or quintiles). Within each stratum, individuals will have similar propensity scores and thus, due to the balancing property, will have similar distributions of baseline covariates. The treatment effect is then calculated within each stratum by simply taking the difference in mean outcomes between the treated and untreated subjects. The overall ATE is estimated by taking a weighted average of these stratum-specific effects, with each stratum weighted by its proportion of the total sample.

Theoretically, any residual confounding within a stratum exists because the propensity scores are similar, but not identical. However, as the sample size grows, we can increase the number of subclasses such that the width of each subclass shrinks. As this width approaches zero, the bias within each subclass vanishes. Provided the number of observations within each shrinking subclass still grows, this method provides an asymptotically unbiased estimate of the ATE [@problem_id:4943134].

#### Inverse Probability of Treatment Weighting (IPTW)

A second major approach is **Inverse Probability of Treatment Weighting (IPTW)**. This method uses the [propensity score](@entry_id:635864) to create a "pseudo-population" in which the treatment is independent of the covariates. Each individual is assigned a weight that is the inverse of the probability of receiving the treatment they actually received.
-   For a treated individual ($A=1$), the weight is $w_1 = 1/e(\mathbf{X})$.
-   For an untreated individual ($A=0$), the weight is $w_0 = 1/(1-e(\mathbf{X}))$.

By weighting the sample in this way, we up-weight individuals who were "underrepresented" for their covariate profile (e.g., an untreated person who had a high probability of being treated) and down-weight those who were "overrepresented." The result is a weighted sample where the covariate distributions are balanced, and we can estimate the ATE by calculating the difference in the weighted means of the outcomes.

This weighting scheme can be adapted to target different causal estimands of interest [@problem_id:4943120]:
-   **Average Treatment Effect (ATE)**, $\mathbb{E}[Y(1) - Y(0)]$: As described above, the weights are $1/e(\mathbf{X})$ for the treated and $1/(1-e(\mathbf{X}))$ for the untreated. This re-weights both groups to be representative of the total population.
-   **Average Treatment Effect on the Treated (ATT)**, $\mathbb{E}[Y(1) - Y(0) \mid A=1]$: Here, the goal is to estimate the effect for the type of people who actually received the treatment. We can leave the treated group as is (weight of $1$) and re-weight the untreated group to look like the treated group. The appropriate weights are $1$ for the treated and $e(\mathbf{X})/(1-e(\mathbf{X}))$ for the untreated.
-   **Average Treatment Effect on the Controls (ATC)**, $\mathbb{E}[Y(1) - Y(0) \mid A=0]$: Symmetrically, to estimate the effect for the type of people who were not treated, we leave the untreated group with a weight of $1$ and re-weight the treated group with weights $(1-e(\mathbf{X}))/e(\mathbf{X})$.

### Practical Considerations in Propensity Score Analysis

While theoretically elegant, the successful application of [propensity score](@entry_id:635864) methods hinges on careful implementation and diagnostic checking.

#### Covariate Selection for the Propensity Score Model

The validity of propensity score methods rests on the unconfoundedness assumption, which depends entirely on the set of covariates $\mathbf{X}$ included in the [propensity score](@entry_id:635864) model. The choice of which variables to include is therefore of paramount importance. Causal knowledge, often encoded in a Directed Acyclic Graph (DAG), provides the best guidance.
-   **Include**: Variables that are common causes of the treatment and the outcome (i.e., confounders) must be included. Proxies for unmeasured confounders should also be included, as they can partially adjust for the unmeasured variable. For instance, if $U$ is an unmeasured common cause but we have a measured proxy $R$ correlated with $U$, including $R$ is critical [@problem_id:4943072].
-   **Include (for precision)**: Variables that are predictors of the outcome but not the treatment (often called [instrumental variables](@entry_id:142324) for the outcome) do not cause confounding but should be included in the model. While not necessary for bias reduction, their inclusion can increase the precision of the final effect estimate.
-   **Exclude**: Variables that predict only the treatment but not the outcome (pure instruments for treatment) should generally be excluded. They are not confounders, and their inclusion can inflate the variance of the effect estimate and create practical positivity problems if the instrument is strong.
-   **Exclude (Critically)**: Any variable affected by the treatment must be excluded. This includes **mediators**, which lie on the causal pathway from treatment to outcome ($A \to M \to Y$), and **colliders**, which are common effects of two other variables. Adjusting for a mediator will block the indirect effect of the treatment, leading to an estimate of the direct effect rather than the total effect. Adjusting for a collider can induce a spurious association between treatment and outcome, creating bias where none existed [@problem_id:4943072] [@problem_id:4515359]. The cardinal rule is that all covariates in the [propensity score](@entry_id:635864) model must be measured pre-treatment.

#### Assessing the Model: The Primacy of Balance

A common misconception is that the goal of fitting a propensity score model is to predict the treatment assignment as accurately as possible. This leads practitioners to assess their model using standard predictive metrics like the **c-statistic** (Area Under the ROC Curve, or AUC). This is a profound mistake. The purpose of the propensity score model is not prediction; it is to serve as a tool for balancing covariates.

A high c-statistic (e.g., > $0.85$) indicates that the treated and untreated groups are highly separable based on their covariates. This is a sign of poor overlap, a "practical" positivity problem that makes finding comparable subjects difficult and can lead to highly variable estimates. A "good" [propensity score](@entry_id:635864) model is not one that predicts treatment well, but one that, when used in a method like matching or weighting, results in good covariate balance [@problem_id:4943097].

Therefore, the primary diagnostic for a propensity score model is to assess **covariate balance** after adjustment. The most common metric for this is the **Absolute Standardized Mean Difference (SMD)**. For a continuous covariate, the SMD is defined as the absolute difference in the (weighted) means of the covariate between the treatment and control groups, divided by a [pooled standard deviation](@entry_id:198759).
$$ \text{SMD} = \frac{|\bar{X}_{1} - \bar{X}_{0}|}{\sqrt{\frac{s_{1}^{2} + s_{0}^{2}}{2}}} $$
where $\bar{X}_1, \bar{X}_0, s_1^2, s_0^2$ are the (weighted) means and variances in the two groups. This metric is independent of the unit of measurement and sample size. A widely accepted convention is that an absolute SMD below $0.1$ for a covariate indicates adequate balance, as simulation studies have shown this level of imbalance is typically associated with negligible [confounding bias](@entry_id:635723) [@problem_id:4943098]. The balance of all covariates in $\mathbf{X}$ should be checked both before and after [propensity score](@entry_id:635864) adjustment to demonstrate that the procedure has successfully reduced baseline differences. This direct assessment of balance, not predictive accuracy, is the true measure of a propensity score model's success.