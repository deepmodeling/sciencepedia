{"hands_on_practices": [{"introduction": "One of the most fundamental challenges in drawing causal conclusions from observational data is confounding. This practice exercise guides you through a classic scenario where a variable confounds the relationship between a treatment and an outcome. By calculating both the naive association and the causal effect adjusted using the backdoor criterion [@problem_id:4912969], you will gain a concrete, quantitative understanding of how confounding can bias our estimates and how to correct for it.", "problem": "An observational biostatistics study seeks to estimate the causal effect of a binary antiviral treatment $X \\in \\{0,1\\}$ on a binary recovery outcome $Y \\in \\{0,1\\}$ among hospitalized patients. A binary comorbidity indicator $Z \\in \\{0,1\\}$ is a common cause of both $X$ and $Y$. The causal structure is represented by a Directed Acyclic Graph (DAG) with arrows $Z \\rightarrow X$, $Z \\rightarrow Y$, and $X \\rightarrow Y$. Assume there are no unmeasured common causes of $X$ and $Y$ after conditioning on $Z$, and that the positivity condition holds for all strata of $Z$.\n\nThe following probabilities are observed in the data:\n- The marginal distribution of $Z$ is $P(Z=1)=0.6$ and $P(Z=0)=0.4$.\n- Treatment assignment depends on $Z$: $P(X=1 \\mid Z=1)=0.8$ and $P(X=1 \\mid Z=0)=0.2$.\n- The outcome model is given by the conditional probabilities $P(Y=1 \\mid X,Z)$:\n  - $P(Y=1 \\mid X=1, Z=1)=0.7$,\n  - $P(Y=1 \\mid X=0, Z=1)=0.5$,\n  - $P(Y=1 \\mid X=1, Z=0)=0.4$,\n  - $P(Y=1 \\mid X=0, Z=0)=0.2$.\n\nStarting from the potential outcomes definition of the Average Treatment Effect (ATE), $ATE = \\mathbb{E}[Y(1) - Y(0)]$, and from the causal identification conditions implied by the DAG (backdoor criterion satisfied by $Z$), compute:\n1. The backdoor-adjusted estimate of $ATE$ using the observational quantities provided.\n2. The naive difference in outcome means, $\\mathbb{E}[Y \\mid X=1] - \\mathbb{E}[Y \\mid X=0]$, using the law of total probability and Bayes’ rule as needed.\n\nRound each of your final numeric answers to four significant figures. Report your two numbers as a row vector in the order specified in items $1$ and $2$.", "solution": "The goal is to compute two quantities: the causal Average Treatment Effect (ATE) adjusted for confounding, and the naive (associational) difference in outcome means.\n\n1.  **Computation of the Backdoor-Adjusted Average Treatment Effect (ATE)**\n\nThe ATE is defined as the expected difference in potential outcomes, $ATE = \\mathbb{E}[Y(1) - Y(0)]$, where $Y(x)$ is the potential outcome under treatment level $x$. For a binary outcome $Y$, this expectation simplifies to the difference in probabilities of the outcome being $1$:\n$$ATE = P(Y(1)=1) - P(Y(0)=1)$$\nThe problem states that the causal structure is given by the Directed Acyclic Graph (DAG) $Z \\rightarrow X$, $Z \\rightarrow Y$, and $X \\rightarrow Y$. The variable $Z$ is a common cause of treatment $X$ and outcome $Y$, and is thus a confounder. The problem also states that there are no unmeasured common causes of $X$ and $Y$ after conditioning on $Z$. This means the set $\\{Z\\}$ satisfies the backdoor criterion for identifying the causal effect of $X$ on $Y$. We can therefore use the backdoor adjustment formula (also known as standardization or the g-formula) to express the potential outcome probabilities in terms of observational probabilities:\n$$P(Y(x)=1) = \\sum_{z \\in \\{0,1\\}} P(Y=1 \\mid X=x, Z=z) P(Z=z)$$\nWe compute $P(Y(1)=1)$ and $P(Y(0)=1)$ using the provided data:\n-   $P(Z=1)=0.6$\n-   $P(Z=0)=0.4$\n-   $P(Y=1 \\mid X=1, Z=1)=0.7$\n-   $P(Y=1 \\mid X=0, Z=1)=0.5$\n-   $P(Y=1 \\mid X=1, Z=0)=0.4$\n-   $P(Y=1 \\mid X=0, Z=0)=0.2$\n\nFor the potential outcome under treatment ($x=1$):\n$$P(Y(1)=1) = P(Y=1 \\mid X=1, Z=1)P(Z=1) + P(Y=1 \\mid X=1, Z=0)P(Z=0)$$\n$$P(Y(1)=1) = (0.7)(0.6) + (0.4)(0.4) = 0.42 + 0.16 = 0.58$$\nFor the potential outcome under control ($x=0$):\n$$P(Y(0)=1) = P(Y=1 \\mid X=0, Z=1)P(Z=1) + P(Y=1 \\mid X=0, Z=0)P(Z=0)$$\n$$P(Y(0)=1) = (0.5)(0.6) + (0.2)(0.4) = 0.30 + 0.08 = 0.38$$\nThe ATE is the difference between these two quantities:\n$$ATE = P(Y(1)=1) - P(Y(0)=1) = 0.58 - 0.38 = 0.20$$\n\n2.  **Computation of the Naive Difference in Outcome Means**\n\nThe naive difference in outcome means is the associational measure $\\mathbb{E}[Y \\mid X=1] - \\mathbb{E}[Y \\mid X=0]$. For a binary outcome $Y$, this is equivalent to:\n$$P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0)$$\nTo compute these conditional probabilities, we must first determine the marginal distribution of $X$ and then use the law of total probability and Bayes' rule.\n\nFirst, we find the marginal probability of receiving treatment, $P(X=1)$, by marginalizing over $Z$:\n$$P(X=1) = P(X=1 \\mid Z=1)P(Z=1) + P(X=1 \\mid Z=0)P(Z=0)$$\nUsing the given data ($P(X=1 \\mid Z=1)=0.8$ and $P(X=1 \\mid Z=0)=0.2$):\n$$P(X=1) = (0.8)(0.6) + (0.2)(0.4) = 0.48 + 0.08 = 0.56$$\nThe probability of not receiving treatment is $P(X=0) = 1 - P(X=1) = 1 - 0.56 = 0.44$.\n\nNext, we compute $P(Y=1 \\mid X=1)$ and $P(Y=1 \\mid X=0)$. We can express these using the joint probabilities $P(Y=1, X=x)$ as $P(Y=1 \\mid X=x) = \\frac{P(Y=1, X=x)}{P(X=x)}$.\nThe joint probability $P(Y=1, X=x)$ is found by marginalizing over $Z$:\n$$P(Y=1, X=x) = \\sum_{z \\in \\{0,1\\}} P(Y=1, X=x, Z=z)$$\nUsing the chain rule of probability, $P(A,B,C) = P(A|B,C)P(B|C)P(C)$:\n$$P(Y=1, X=x) = \\sum_{z \\in \\{0,1\\}} P(Y=1 \\mid X=x, Z=z) P(X=x \\mid Z=z) P(Z=z)$$\nFor $x=1$:\n$$P(Y=1, X=1) = P(Y=1 \\mid X=1, Z=1)P(X=1 \\mid Z=1)P(Z=1) + P(Y=1 \\mid X=1, Z=0)P(X=1 \\mid Z=0)P(Z=0)$$\n$$P(Y=1, X=1) = (0.7)(0.8)(0.6) + (0.4)(0.2)(0.4) = 0.336 + 0.032 = 0.368$$\nSo, the conditional probability of recovery given treatment is:\n$$P(Y=1 \\mid X=1) = \\frac{P(Y=1, X=1)}{P(X=1)} = \\frac{0.368}{0.56} = \\frac{368}{560} = \\frac{23}{35}$$\nFor $x=0$:\nWe need $P(X=0 \\mid Z=1) = 1 - 0.8 = 0.2$ and $P(X=0 \\mid Z=0) = 1 - 0.2 = 0.8$.\n$$P(Y=1, X=0) = P(Y=1 \\mid X=0, Z=1)P(X=0 \\mid Z=1)P(Z=1) + P(Y=1 \\mid X=0, Z=0)P(X=0 \\mid Z=0)P(Z=0)$$\n$$P(Y=1, X=0) = (0.5)(0.2)(0.6) + (0.2)(0.8)(0.4) = 0.060 + 0.064 = 0.124$$\nSo, the conditional probability of recovery given no treatment is:\n$$P(Y=1 \\mid X=0) = \\frac{P(Y=1, X=0)}{P(X=0)} = \\frac{0.124}{0.44} = \\frac{124}{440} = \\frac{31}{110}$$\nThe naive difference in means is:\n$$P(Y=1 \\mid X=1) - P(Y=1 \\mid X=0) = \\frac{23}{35} - \\frac{31}{110} = \\frac{23 \\times 22 - 31 \\times 7}{770} = \\frac{506 - 217}{770} = \\frac{289}{770}$$\nAs a decimal, this is approximately $0.37532467...$.\n\n**Summary and Rounding**\n\nThe two requested quantities are:\n1.  ATE = $0.20$\n2.  Naive difference = $\\frac{289}{770} \\approx 0.37532467...$\n\nRounding these to four significant figures as requested:\n1.  ATE = $0.2000$\n2.  Naive difference = $0.3753$\n\nThe discrepancy between the ATE ($0.2000$) and the naive association ($0.3753$) is due to the confounding effect of $Z$. The naive difference is a biased estimate of the causal effect.", "answer": "$$ \\boxed{\\begin{pmatrix} 0.2000 & 0.3753 \\end{pmatrix}} $$", "id": "4912969"}, {"introduction": "After learning to control for confounders, a common impulse is to adjust for any available variable. This practice demonstrates a critical pitfall of that approach: collider bias. By working through a scenario where two independent variables, a treatment $X$ and an outcome $Y$, both cause a third variable $C$ (a \"collider\"), you will see how conditioning on this common effect can create a spurious statistical association between them [@problem_id:4912830]. This exercise is essential for understanding when *not* to adjust and for appreciating the structural logic of Directed Acyclic Graphs.", "problem": "A biostatistician is studying a binary exposure $X \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$ in a population where there is no causal effect of $X$ on $Y$ and no shared common causes between $X$ and $Y$. The causal structure is given by a directed acyclic graph (DAG) in which $X$ and $Y$ are marginally independent causes of a third binary variable $C \\in \\{0,1\\}$, that is, $X \\to C \\leftarrow Y$, and there are no other arrows. Assume the following data-generating mechanism:\n\n- $X$ and $Y$ are independent with $P(X=1)=0.4$ and $P(Y=1)=0.3$. Consequently, the joint distribution of $(X,Y)$ is:\n  - $P(X=1,Y=1)=0.12$,\n  - $P(X=1,Y=0)=0.28$,\n  - $P(X=0,Y=1)=0.18$,\n  - $P(X=0,Y=0)=0.42$.\n\n- The collider variable $C$ is generated stochastically from its parents $(X,Y)$ with the following conditional probabilities:\n  - $P(C=1 \\mid X=1,Y=1)=0.9$,\n  - $P(C=1 \\mid X=1,Y=0)=0.6$,\n  - $P(C=1 \\mid X=0,Y=1)=0.6$,\n  - $P(C=1 \\mid X=0,Y=0)=0.1$,\n  and $P(C=0 \\mid X,Y)=1-P(C=1 \\mid X,Y)$ for each $(X,Y)$.\n\nUsing only the definitions of independence and conditional probability, and the probability tables above, compute the induced difference\n$$D \\equiv P(Y=1 \\mid X=1, C=1) - P(Y=1 \\mid X=1).$$\nYour final answer must be a single exact number (no rounding).", "solution": "The objective is to compute the induced difference $D$, defined as:\n$$D \\equiv P(Y=1 \\mid X=1, C=1) - P(Y=1 \\mid X=1)$$\n\nWe will compute each term in this expression separately.\n\nFirst, we evaluate the term $P(Y=1 \\mid X=1)$. The problem statement specifies that the variables $X$ and $Y$ are marginally independent. By the definition of statistical independence, the conditional probability of $Y$ given $X$ is equal to the marginal probability of $Y$.\n$$P(Y=1 \\mid X=1) = P(Y=1)$$\nThe problem provides the marginal probability of $Y=1$:\n$$P(Y=1) = 0.3$$\nTherefore, the second term of the expression for $D$ is $0.3$.\n\nNext, we evaluate the term $P(Y=1 \\mid X=1, C=1)$. Using the definition of conditional probability, we can write:\n$$P(Y=1 \\mid X=1, C=1) = \\frac{P(X=1, Y=1, C=1)}{P(X=1, C=1)}$$\nTo evaluate this expression, we must compute the numerator, $P(X=1, Y=1, C=1)$, and the denominator, $P(X=1, C=1)$.\n\nThe numerator is the joint probability of all three variables being equal to $1$. Using the chain rule of probability, we can express this as:\n$$P(X=1, Y=1, C=1) = P(C=1 \\mid X=1, Y=1) \\, P(X=1, Y=1)$$\nThe problem provides the necessary values:\n$$P(C=1 \\mid X=1, Y=1) = 0.9$$\n$$P(X=1, Y=1) = 0.12$$\nSubstituting these values, we get:\n$$P(X=1, Y=1, C=1) = 0.9 \\times 0.12 = 0.108$$\n\nThe denominator, $P(X=1, C=1)$, is a marginal joint probability. We can obtain it by applying the law of total probability and marginalizing over the variable $Y$:\n$$P(X=1, C=1) = \\sum_{y \\in \\{0,1\\}} P(X=1, Y=y, C=1)$$\n$$P(X=1, C=1) = P(X=1, Y=1, C=1) + P(X=1, Y=0, C=1)$$\nWe have already computed the first term: $P(X=1, Y=1, C=1) = 0.108$.\nWe now compute the second term, $P(X=1, Y=0, C=1)$, again using the chain rule:\n$$P(X=1, Y=0, C=1) = P(C=1 \\mid X=1, Y=0) \\, P(X=1, Y=0)$$\nThe problem provides the values:\n$$P(C=1 \\mid X=1, Y=0) = 0.6$$\n$$P(X=1, Y=0) = 0.28$$\nSubstituting these values, we get:\n$$P(X=1, Y=0, C=1) = 0.6 \\times 0.28 = 0.168$$\nNow, we can compute the denominator by summing the two terms:\n$$P(X=1, C=1) = 0.108 + 0.168 = 0.276$$\n\nWith both the numerator and the denominator calculated, we can find the conditional probability $P(Y=1 \\mid X=1, C=1)$:\n$$P(Y=1 \\mid X=1, C=1) = \\frac{0.108}{0.276}$$\nTo express this as an exact fraction, we write it as a ratio of integers and simplify:\n$$\\frac{108}{276} = \\frac{27 \\times 4}{69 \\times 4} = \\frac{27}{69} = \\frac{9 \\times 3}{23 \\times 3} = \\frac{9}{23}$$\n\nFinally, we compute the induced difference $D$:\n$$D = P(Y=1 \\mid X=1, C=1) - P(Y=1 \\mid X=1)$$\n$$D = \\frac{9}{23} - 0.3$$\nTo perform the subtraction, we express $0.3$ as the fraction $\\frac{3}{10}$ and find a common denominator, which is $23 \\times 10 = 230$:\n$$D = \\frac{9}{23} - \\frac{3}{10} = \\frac{9 \\times 10}{23 \\times 10} - \\frac{3 \\times 23}{10 \\times 23} = \\frac{90}{230} - \\frac{69}{230}$$\n$$D = \\frac{90 - 69}{230} = \\frac{21}{230}$$\nThis positive value demonstrates the phenomenon of collider bias, where conditioning on a common effect ($C=1$) induces a statistical association between two marginally independent causes ($X$ and $Y$).", "answer": "$$\\boxed{\\frac{21}{230}}$$", "id": "4912830"}, {"introduction": "What happens when a critical confounder cannot be measured, making the backdoor adjustment impossible? This advanced practice introduces a powerful alternative: the frontdoor adjustment. This exercise presents a scenario where the effect of a treatment on an outcome is fully mediated by a third variable, but an unmeasured factor confounds the treatment-outcome relationship [@problem_id:4912922]. By applying the frontdoor formula, you will learn how to identify the causal effect by leveraging the causal pathway itself, demonstrating the remarkable power of DAGs to find solutions even in complex situations.", "problem": "Consider a biostatistical observational study of a drug exposure variable $X \\in \\{0,1\\}$, a post-exposure biomarker $M \\in \\{0,1\\}$, final clinical outcome $Y \\in \\{0,1\\}$, and a latent subject-level characteristic $U$ (for example, underlying genetic liability) that is not measured. Suppose the causal structure is represented by a Directed Acyclic Graph (DAG) with edges $X \\rightarrow M \\rightarrow Y$, $U \\rightarrow X$, and $U \\rightarrow Y$, with no edge $U \\rightarrow M$ and no direct edge $X \\rightarrow Y$. This means $U$ confounds the relationship between $X$ and $Y$, but does not affect $M$, and all directed causal influence from $X$ to $Y$ is mediated through $M$.\n\nUsing the definitions of interventions via Pearl’s $do$-operator, the truncated factorization of a DAG, and $d$-separation, first argue from first principles why the interventional estimand $E[Y_{x}]$ is identifiable by the frontdoor adjustment under this graph. Then, compute the interventional mean $E[Y_{x=1}]$ using the following observational quantities, which are known from a large representative cohort with positivity ($P(X=x)>0$ for $x \\in \\{0,1\\}$):\n\n- $P(X=1)=0.4$, $P(X=0)=0.6$.\n- $P(M=1 \\mid X=1)=0.7$, $P(M=1 \\mid X=0)=0.2$.\n- $P(Y=1 \\mid M=1, X=1)=0.8$, $P(Y=1 \\mid M=1, X=0)=0.6$.\n- $P(Y=1 \\mid M=0, X=1)=0.3$, $P(Y=1 \\mid M=0, X=0)=0.2$.\n\nYour final numerical answer must be the value of $E[Y_{x=1}]$ expressed as a decimal. Round your answer to four significant figures.", "solution": "The solution is presented in two parts as requested: first, the theoretical justification for identifiability, and second, the numerical computation.\n\n**Part 1: Justification of Identifiability**\n\nThe objective is to show that the interventional quantity $P(Y=y \\mid do(X=x))$ can be expressed solely in terms of the observational probability distribution $P(X, M, Y)$. The expectation $E[Y_x]$ is equivalent to $P(Y=1 \\mid do(X=x))$ for a binary outcome $Y$.\n\nThe joint probability distribution over all variables, including the latent variable $U$, factorizes according to the DAG structure:\n$$ P(u, x, m, y) = P(u) P(x \\mid u) P(m \\mid x) P(y \\mid m, u) $$\nThe distribution resulting from an intervention $do(X=x_0)$ is obtained by modifying this factorization. The intervention severs the arrow into $X$ (from $U$) and sets the value of $X$ to $x_0$. The post-intervention joint distribution is:\n$$ P(u, m, y \\mid do(X=x_0)) = P(u) P(m \\mid x_0) P(y \\mid m, u) $$\nTo find the desired interventional probability $P(y \\mid do(x_0))$, we must marginalize out the latent variable $U$ and the mediator $M$:\n$$ P(y \\mid do(X=x_0)) = \\sum_{m} \\sum_u P(u, m, y \\mid do(X=x_0)) $$\n$$ P(y \\mid do(X=x_0)) = \\sum_{m} \\sum_u P(u) P(m \\mid x_0) P(y \\mid m, u) $$\nSince $P(m \\mid x_0)$ does not depend on $u$, we can rearrange and rewrite this as:\n$$ P(y \\mid do(X=x_0)) = \\sum_m P(m \\mid x_0) \\left( \\sum_u P(y \\mid m, u) P(u) \\right) $$\nThe term $P(m \\mid x_0)$ is an observable conditional probability. However, the term in parentheses, $\\sum_u P(y \\mid m, u) P(u)$, involves the unobserved variable $U$ and cannot be computed directly. The core of the identification task is to express this term using observables.\n\nLet's analyze the quantity $\\sum_{x'} P(y \\mid m, x')P(x')$, which is composed entirely of observable probabilities. By the law of total probability, we can expand $P(y \\mid m, x')$ by conditioning on $U$:\n$$ P(y \\mid m, x') = \\sum_u P(y \\mid m, x', u) P(u \\mid m, x') $$\nFrom the DAG, $Y$ is d-separated from $X$ by the set $\\{M, U\\}$. Thus, $P(y \\mid m, x', u) = P(y \\mid m, u)$.\nAlso from the DAG, $U$ is d-separated from $M$ by $X$ (the path is $U \\rightarrow X \\rightarrow M$). Thus, $P(u \\mid m, x') = P(u \\mid x')$.\nSubstituting these two simplifications back:\n$$ P(y \\mid m, x') = \\sum_u P(y \\mid m, u) P(u \\mid x') $$\nNow, let's substitute this back into the expression $\\sum_{x'} P(y \\mid m, x')P(x')$:\n$$ \\sum_{x'} P(y \\mid m, x')P(x') = \\sum_{x'} \\left( \\sum_u P(y \\mid m, u) P(u \\mid x') \\right) P(x') $$\nBy swapping the order of summation:\n$$ = \\sum_u P(y \\mid m, u) \\left( \\sum_{x'} P(u \\mid x')P(x') \\right) $$\nThe inner sum $\\sum_{x'} P(u \\mid x')P(x')$ is equivalent to $\\sum_{x'} P(u, x')$, which by the law of total probability is simply $P(u)$.\nSo we have shown that:\n$$ \\sum_{x'} P(y \\mid m, x') P(x') = \\sum_u P(y \\mid m, u) P(u) $$\nThe left side is an expression of observable quantities. The right side is exactly the unobservable term from our expression for $P(y \\mid do(x_0))$.\n\nBy substituting this result back into the equation for $P(y \\mid do(x_0))$, we get:\n$$ P(Y=y \\mid do(X=x)) = \\sum_{m} P(M=m \\mid X=x) \\left( \\sum_{x'} P(Y=y \\mid M=m, X=x') P(X=x') \\right) $$\nThis is the frontdoor adjustment formula. Since every term on the right-hand side—$P(M=m \\mid X=x)$, $P(Y=y \\mid M=m, X=x')$, and $P(X=x')$—is an estimable quantity from observational data, the interventional estimand $P(Y=y \\mid do(X=x))$ is identifiable. Consequently, $E[Y_x] = P(Y=1 \\mid do(X=x))$ is identifiable.\n\n**Part 2: Numerical Computation**\n\nWe are asked to compute $E[Y_{x=1}] = P(Y=1 \\mid do(X=1))$. Using the frontdoor adjustment formula derived above with $y=1$ and $x=1$:\n$$ E[Y_{x=1}] = \\sum_{m \\in \\{0,1\\}} P(M=m \\mid X=1) \\left( \\sum_{x' \\in \\{0,1\\}} P(Y=1 \\mid M=m, X=x') P(X=x') \\right) $$\nLet's first compute the inner sum, which represents the causal effect of $M$ on $Y$, for $m=1$ and $m=0$.\n\nFor $m=1$:\n$$ \\sum_{x' \\in \\{0,1\\}} P(Y=1 \\mid M=1, X=x') P(X=x') $$\n$$ = P(Y=1 \\mid M=1, X=1)P(X=1) + P(Y=1 \\mid M=1, X=0)P(X=0) $$\nSubstituting the given values:\n$$ = (0.8)(0.4) + (0.6)(0.6) = 0.32 + 0.36 = 0.68 $$\n\nFor $m=0$:\n$$ \\sum_{x' \\in \\{0,1\\}} P(Y=1 \\mid M=0, X=x') P(X=x') $$\n$$ = P(Y=1 \\mid M=0, X=1)P(X=1) + P(Y=1 \\mid M=0, X=0)P(X=0) $$\nSubstituting the given values:\n$$ = (0.3)(0.4) + (0.2)(0.6) = 0.12 + 0.12 = 0.24 $$\n\nNow, we can compute the final quantity by combining these results:\n$$ E[Y_{x=1}] = P(M=1 \\mid X=1) \\cdot (0.68) + P(M=0 \\mid X=1) \\cdot (0.24) $$\nWe are given $P(M=1 \\mid X=1) = 0.7$, which implies $P(M=0 \\mid X=1) = 1 - 0.7 = 0.3$.\nSubstituting all values:\n$$ E[Y_{x=1}] = (0.7)(0.68) + (0.3)(0.24) $$\n$$ E[Y_{x=1}] = 0.476 + 0.072 $$\n$$ E[Y_{x=1}] = 0.548 $$\nThe problem asks for the answer to be rounded to four significant figures.\n$$ E[Y_{x=1}] = 0.5480 $$", "answer": "$$\n\\boxed{0.5480}\n$$", "id": "4912922"}]}