{"hands_on_practices": [{"introduction": "Before we can apply instrumental variable methods, we must first understand how to calculate the causal effect from our data. This foundational exercise guides you through the derivation of the Wald estimator, revealing how the causal estimate elegantly emerges as the ratio of the instrument-outcome association to the instrument-exposure association. By working through this derivation, you will demystify the formula and build a strong intuition for how Mendelian Randomization isolates a causal effect. [@problem_id:4916926]", "problem": "Consider a Mendelian randomization (MR) study with a single biallelic genetic variant used as an instrumental variable. Let the instrument be a binary indicator $Z \\in \\{0,1\\}$ denoting the presence ($Z=1$) or absence ($Z=0$) of a risk allele, the exposure $X$ be continuous, and the outcome $Y$ be continuous. Assume the following linear structural relationships and instrumental variable assumptions hold:\n- The exposure model is $X = \\gamma_{0} + \\gamma_{Z} Z + \\gamma_{U} U + \\varepsilon_{X}$.\n- The outcome model is $Y = \\beta X + \\delta U + \\varepsilon_{Y}$.\n- The unobserved confounder $U$ may affect both $X$ and $Y$, but the instrument $Z$ is independent of $U$ and the errors: $\\operatorname{Cov}(Z,U)=0$, $\\operatorname{Cov}(Z,\\varepsilon_{X})=0$, and $\\operatorname{Cov}(Z,\\varepsilon_{Y})=0$.\n- The exclusion restriction holds: $Z$ affects $Y$ only through $X$.\n- Instrument relevance holds: $\\gamma_{Z} \\neq 0$.\n\nStarting from the instrumental variable moment condition and the definition of covariance, derive a closed-form expression for the coefficient $\\beta$ that can be identified using $Z$ as an instrument and is estimable from observed data through differences in conditional means across $Z=1$ versus $Z=0$. Your final expression must be written solely in terms of $\\mathbb{E}[Y \\mid Z=z]$ and $\\mathbb{E}[X \\mid Z=z]$ for $z \\in \\{0,1\\}$. Then, based on your derivation, explain what the numerator and denominator quantify in this MR setting. Provide the final expression only as your answer. No rounding is required, and no units are needed.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe explicitly provided information is as follows:\n- **Instrumental Variable (IV):** A binary indicator $Z \\in \\{0,1\\}$.\n- **Exposure:** A continuous variable $X$.\n- **Outcome:** A continuous variable $Y$.\n- **Unobserved Confounder:** A variable $U$.\n- **Structural Models:**\n    - Exposure model: $X = \\gamma_{0} + \\gamma_{Z} Z + \\gamma_{U} U + \\varepsilon_{X}$.\n    - Outcome model: $Y = \\beta X + \\delta U + \\varepsilon_{Y}$.\n- **IV Assumptions:**\n    1.  **Independence:** The instrument $Z$ is independent of the confounder $U$ and the error terms $\\varepsilon_{X}$ and $\\varepsilon_{Y}$. This is formally stated as $\\operatorname{Cov}(Z,U)=0$, $\\operatorname{Cov}(Z,\\varepsilon_{X})=0$, and $\\operatorname{Cov}(Z,\\varepsilon_{Y})=0$.\n    2.  **Exclusion Restriction:** $Z$ affects $Y$ only through its effect on $X$. This is implicitly encoded in the outcome model, where $Z$ does not appear as a direct term influencing $Y$.\n    3.  **Instrument Relevance:** The instrument is associated with the exposure, formally stated as $\\gamma_{Z} \\neq 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded:** The problem presents a canonical formulation of an instrumental variable analysis, specifically within the biostatistical context of Mendelian randomization. The structural equations and the three core IV assumptions are the foundational principles of this well-established methodology. The setup is scientifically sound.\n- **Well-Posed:** The problem asks for the derivation of a closed-form expression for the causal parameter $\\beta$. Given the complete set of standard linear IV assumptions, a unique solution for $\\beta$ is identifiable. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical and statistical language, free from ambiguity or subjective content.\n\nThe problem exhibits no flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard, formalizable problem in biostatistics.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete, reasoned solution will now be provided.\n\n### Derivation\nThe objective is to derive an expression for the causal coefficient $\\beta$ using the instrumental variable $Z$. The parameter $\\beta$ represents the causal effect of the exposure $X$ on the outcome $Y$.\n\nThe core instrumental variable moment condition states that the instrument $Z$ must be uncorrelated with the error term in the outcome equation. The outcome model is specified as $Y = \\beta X + \\delta U + \\varepsilon_{Y}$. In an ordinary least squares regression of $Y$ on $X$, the error term would be $\\eta = \\delta U + \\varepsilon_{Y}$. The presence of the confounder $U$, which is correlated with $X$, leads to $\\operatorname{Cov}(X, \\eta) \\neq 0$, causing bias in the OLS estimator of $\\beta$.\n\nThe IV approach leverages the fact that the instrument $Z$ is, by assumption, uncorrelated with $\\eta$. We can formally verify this:\n$$\n\\operatorname{Cov}(Z, \\eta) = \\operatorname{Cov}(Z, \\delta U + \\varepsilon_{Y}) = \\delta \\operatorname{Cov}(Z, U) + \\operatorname{Cov}(Z, \\varepsilon_{Y})\n$$\nFrom the given assumptions, $\\operatorname{Cov}(Z, U) = 0$ and $\\operatorname{Cov}(Z, \\varepsilon_{Y}) = 0$. Therefore, $\\operatorname{Cov}(Z, \\eta) = 0$.\n\nWe can rewrite the outcome model as $\\eta = Y - \\beta X$. The moment condition $\\operatorname{Cov}(Z, \\eta) = 0$ thus becomes:\n$$\n\\operatorname{Cov}(Z, Y - \\beta X) = 0\n$$\nUsing the linearity property of the covariance operator:\n$$\n\\operatorname{Cov}(Z, Y) - \\operatorname{Cov}(Z, \\beta X) = 0\n$$\n$$\n\\operatorname{Cov}(Z, Y) - \\beta \\operatorname{Cov}(Z, X) = 0\n$$\nSolving for $\\beta$, under the condition that $\\operatorname{Cov}(Z, X) \\neq 0$ (which we will show is equivalent to the relevance assumption $\\gamma_{Z} \\neq 0$), yields the general IV estimator:\n$$\n\\beta = \\frac{\\operatorname{Cov}(Z, Y)}{\\operatorname{Cov}(Z, X)}\n$$\nThis form is known as the Wald estimator. The problem requires this expression to be rewritten in terms of conditional expectations with respect to $Z$. For a binary instrument $Z \\in \\{0, 1\\}$, the covariance terms have a particularly simple form.\n\nLet us analyze the denominator, $\\operatorname{Cov}(Z, X)$. We can use the law of total expectation to relate the covariance to differences in conditional means.\nFrom the exposure model, $X = \\gamma_{0} + \\gamma_{Z} Z + \\gamma_{U} U + \\varepsilon_{X}$, we can take the expectation conditional on $Z=z$:\n$$\n\\mathbb{E}[X \\mid Z=z] = \\mathbb{E}[\\gamma_{0} + \\gamma_{Z} z + \\gamma_{U} U + \\varepsilon_{X} \\mid Z=z]\n$$\n$$\n\\mathbb{E}[X \\mid Z=z] = \\gamma_{0} + \\gamma_{Z} z + \\gamma_{U} \\mathbb{E}[U \\mid Z=z] + \\mathbb{E}[\\varepsilon_{X} \\mid Z=z]\n$$\nDue to the independence assumption, $\\mathbb{E}[U \\mid Z=z] = \\mathbb{E}[U]$ and $\\mathbb{E}[\\varepsilon_{X} \\mid Z=z] = \\mathbb{E}[\\varepsilon_{X}]$. Assuming without loss of generality that $\\mathbb{E}[\\varepsilon_{X}] = 0$:\n$$\n\\mathbb{E}[X \\mid Z=z] = \\gamma_{0} + \\gamma_{Z} z + \\gamma_{U} \\mathbb{E}[U]\n$$\nNow, we can evaluate this for $z=1$ and $z=0$:\n$$\n\\mathbb{E}[X \\mid Z=1] = \\gamma_{0} + \\gamma_{Z} + \\gamma_{U} \\mathbb{E}[U]\n$$\n$$\n\\mathbb{E}[X \\mid Z=0] = \\gamma_{0} + \\gamma_{U} \\mathbb{E}[U]\n$$\nThe difference between these conditional means is:\n$$\n\\mathbb{E}[X \\mid Z=1] - \\mathbb{E}[X \\mid Z=0] = (\\gamma_{0} + \\gamma_{Z} + \\gamma_{U} \\mathbb{E}[U]) - (\\gamma_{0} + \\gamma_{U} \\mathbb{E}[U]) = \\gamma_{Z}\n$$\nThe difference in the mean exposure between the two genotype groups quantifies the strength of the gene-exposure association, $\\gamma_{Z}$. Since $\\gamma_{Z} \\neq 0$ is assumed, this difference is non-zero. This confirms that $\\operatorname{Cov}(Z, X) \\neq 0$.\n\nNext, let us analyze the numerator, $\\operatorname{Cov}(Z, Y)$. We first substitute the model for $X$ into the model for $Y$ to express $Y$ as a function of $Z$ and the error terms:\n$$\nY = \\beta(\\gamma_{0} + \\gamma_{Z} Z + \\gamma_{U} U + \\varepsilon_{X}) + \\delta U + \\varepsilon_{Y}\n$$\n$$\nY = \\beta\\gamma_{0} + \\beta\\gamma_{Z} Z + (\\beta\\gamma_{U} + \\delta)U + \\beta\\varepsilon_{X} + \\varepsilon_{Y}\n$$\nTaking the expectation of $Y$ conditional on $Z=z$:\n$$\n\\mathbb{E}[Y \\mid Z=z] = \\mathbb{E}[\\beta\\gamma_{0} + \\beta\\gamma_{Z} z + (\\beta\\gamma_{U} + \\delta)U + \\beta\\varepsilon_{X} + \\varepsilon_{Y} \\mid Z=z]\n$$\nUsing the independence assumptions and assuming $\\mathbb{E}[\\varepsilon_{X}] = 0$ and $\\mathbb{E}[\\varepsilon_{Y}] = 0$:\n$$\n\\mathbb{E}[Y \\mid Z=z] = \\beta\\gamma_{0} + \\beta\\gamma_{Z} z + (\\beta\\gamma_{U} + \\delta)\\mathbb{E}[U]\n$$\nEvaluating for $z=1$ and $z=0$:\n$$\n\\mathbb{E}[Y \\mid Z=1] = \\beta\\gamma_{0} + \\beta\\gamma_{Z} + (\\beta\\gamma_{U} + \\delta)\\mathbb{E}[U]\n$$\n$$\n\\mathbb{E}[Y \\mid Z=0] = \\beta\\gamma_{0} + (\\beta\\gamma_{U} + \\delta)\\mathbb{E}[U]\n$$\nThe difference between these conditional means is:\n$$\n\\mathbb{E}[Y \\mid Z=1] - \\mathbb{E}[Y \\mid Z=0] = (\\beta\\gamma_{0} + \\beta\\gamma_{Z} + (\\beta\\gamma_{U} + \\delta)\\mathbb{E}[U]) - (\\beta\\gamma_{0} + (\\beta\\gamma_{U} + \\delta)\\mathbb{E}[U]) = \\beta\\gamma_{Z}\n$$\nThis difference in the mean outcome between the two genotype groups quantifies the gene-outcome association.\n\nWe now have two results:\n1.  $\\mathbb{E}[X \\mid Z=1] - \\mathbb{E}[X \\mid Z=0] = \\gamma_{Z}$\n2.  $\\mathbb{E}[Y \\mid Z=1] - \\mathbb{E}[Y \\mid Z=0] = \\beta\\gamma_{Z}$\n\nSubstituting these into the Wald estimator form $\\beta = \\frac{\\beta\\gamma_{Z}}{\\gamma_{Z}}$ yields the final expression:\n$$\n\\beta = \\frac{\\mathbb{E}[Y \\mid Z=1] - \\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1] - \\mathbb{E}[X \\mid Z=0]}\n$$\nThis expression is estimable from observed data by replacing the population conditional expectations with their sample mean analogues.\n\n### Interpretation of Numerator and Denominator\n- **Denominator:** The term $\\mathbb{E}[X \\mid Z=1] - \\mathbb{E}[X \\mid Z=0]$ represents the average difference in the exposure $X$ between individuals carrying the risk allele ($Z=1$) and those who do not ($Z=0$). In this Mendelian randomization context, it is the magnitude of the \"gene-exposure\" association. It quantifies how much the genetic instrument perturbs the exposure level.\n- **Numerator:** The term $\\mathbb{E}[Y \\mid Z=1] - \\mathbb{E}[Y \\mid Z=0]$ represents the average difference in the outcome $Y$ between individuals with the risk allele and those without. This is the \"gene-outcome\" association. Due to the exclusion restriction, any effect of the gene on the outcome must be mediated through the exposure. Therefore, this difference reflects the downstream consequence on the outcome of the gene's initial effect on the exposure.\n\nThe causal effect $\\beta$ is thus identified as the ratio of the gene-outcome association to the gene-exposure association. This elegantly removes the confounding by dividing the total effect of the gene on the outcome by the total effect of the gene on the exposure.", "answer": "$$\n\\boxed{\\frac{\\mathbb{E}[Y \\mid Z=1] - \\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1] - \\mathbb{E}[X \\mid Z=0]}}\n$$", "id": "4916926"}, {"introduction": "Having derived the theoretical form of the Wald estimator, the next step is to see it in action with a concrete numerical example. This practice provides summary data from a hypothetical Mendelian Randomization study, allowing you to directly apply the Wald formula. Calculating the causal effect from these sample means will solidify your understanding and bridge the gap between abstract theory and practical data analysis. [@problem_id:4966574]", "problem": "A researcher conducts a single-sample Mendelian Randomization (MR) study to estimate the causal effect of a continuous exposure $X$ on a continuous outcome $Y$ using a binary genetic variant $Z \\in \\{0,1\\}$ as an Instrumental Variable (IV). The genetic variant $Z$ is coded such that $Z=1$ indicates carriers of the effect allele and $Z=0$ indicates non-carriers. Assume the following standard IV assumptions hold in the underlying population: instrument relevance ($Z$ is associated with $X$), instrument independence ($Z$ is independent of all unmeasured confounders of $X$ and $Y$), and the exclusion restriction (the effect of $Z$ on $Y$ operates only through $X$, with no direct pathway). Also assume monotonicity (no individuals for whom increasing $Z$ decreases $X$), so that the estimand corresponds to a local average treatment effect among compliers. Both $X$ and $Y$ are measured on standardized logarithmic scales (dimensionless), so any causal effect is dimensionless.\n\nYou are given the sample averages of $X$ and $Y$ stratified by $Z$:\n$$\\bar{Y}_{1}=6,\\quad \\bar{Y}_{0}=4,\\quad \\bar{X}_{1}=3,\\quad \\bar{X}_{0}=2.$$\nUsing only the assumptions stated and fundamental definitions of causal effects and instruments, derive an estimator for the causal effect of $X$ on $Y$ that depends on the differences in the group means by $Z$, and compute its numerical value $\\hat{\\beta}_{W}$ from the provided data. State the interpretation of this estimate as a causal effect under the stated IV assumptions and monotonicity. Provide the exact numerical value; do not round.", "solution": "The problem requires the derivation and calculation of an estimator for the causal effect of a continuous exposure $X$ on a continuous outcome $Y$, using a binary genetic variant $Z$ as an Instrumental Variable (IV) in a Mendelian Randomization (MR) context. The provided data consists of sample means of $X$ and $Y$ stratified by the instrument $Z$.\n\nLet $\\beta_{XY}$ denote the causal effect of $X$ on $Y$. The core challenge in observational studies is that a simple regression of $Y$ on $X$ may yield a biased estimate of $\\beta_{XY}$ due to unmeasured confounding. An instrumental variable $Z$ allows for the identification of this causal effect under a set of assumptions. The standard IV assumptions are given:\n1.  **Relevance**: $Z$ is associated with $X$. Formally, the covariance $\\text{Cov}(Z, X) \\neq 0$.\n2.  **Independence (Exchangeability)**: $Z$ is independent of any unmeasured confounder $U$ that affects both $X$ and $Y$. Formally, $\\text{Cov}(Z, U) = 0$.\n3.  **Exclusion Restriction**: $Z$ affects $Y$ only through its effect on $X$. There is no direct causal path from $Z$ to $Y$.\n\nLet us consider the causal relationships. The total effect of the instrument $Z$ on the outcome $Y$ can be decomposed into the effect of $Z$ on the exposure $X$, followed by the effect of $X$ on $Y$. This is a consequence of the exclusion restriction assumption.\n\nIn a simplified linear framework, we can represent these relationships as follows. The effect of the instrument $Z$ on the exposure $X$ is given by the difference in the expected value of $X$ conditional on $Z$. Let us denote the population average effect of $Z$ on $X$ as $\\Delta_X$.\n$$ \\Delta_X = E[X | Z=1] - E[X | Z=0] $$\nSimilarly, the effect of the instrument $Z$ on the outcome $Y$ is given by the difference in the expected value of $Y$ conditional on $Z$. Let us denote this population average effect as $\\Delta_Y$.\n$$ \\Delta_Y = E[Y | Z=1] - E[Y | Z=0] $$\nThe exclusion restriction assumption implies that the entire effect of $Z$ on $Y$ is mediated through $X$. Therefore, the total effect $\\Delta_Y$ can be expressed as the product of the effect of $Z$ on $X$ and the causal effect of $X$ on $Y$, which we denote by $\\beta_{XY}$.\n$$ \\Delta_Y = \\beta_{XY} \\times \\Delta_X $$\nThis relationship is the foundation of the IV estimator. The relevance assumption ensures that $\\Delta_X \\neq 0$, which is necessary to solve for $\\beta_{XY}$. The independence assumption ensures that the relationship between $Z$ and $Y$ is not confounded, making $\\Delta_Y$ a valid measure of the total effect of $Z$ on $Y$.\n\nFrom the equation above, we can algebraically isolate the causal effect $\\beta_{XY}$:\n$$ \\beta_{XY} = \\frac{\\Delta_Y}{\\Delta_X} = \\frac{E[Y | Z=1] - E[Y | Z=0]}{E[X | Z=1] - E[X | Z=0]} $$\nThis ratio is the theoretical expression for the causal effect under the IV assumptions. In a real-world study, we do not have access to the true population expectations ($E[\\cdot]$). Instead, we have sample estimates. We can form an estimator for $\\beta_{XY}$ by substituting the sample means for the population expectations. This specific form of the IV estimator, based on the ratio of differences in means, is known as the Wald estimator.\n\nLet $\\bar{Y}_{z}$ and $\\bar{X}_{z}$ be the sample means of $Y$ and $X$ for the group where the instrument takes the value $z \\in \\{0, 1\\}$. The estimator for $\\beta_{XY}$, which the problem denotes as $\\hat{\\beta}_{W}$, is given by:\n$$ \\hat{\\beta}_{W} = \\frac{\\bar{Y}_{1} - \\bar{Y}_{0}}{\\bar{X}_{1} - \\bar{X}_{0}} $$\nThis estimator is a consistent estimator of the causal effect $\\beta_{XY}$ given that the IV assumptions hold.\n\nWe are given the following sample averages:\n- $\\bar{Y}_{1} = 6$ (average outcome for carriers of the effect allele, $Z=1$)\n- $\\bar{Y}_{0} = 4$ (average outcome for non-carriers, $Z=0$)\n- $\\bar{X}_{1} = 3$ (average exposure for carriers, $Z=1$)\n- $\\bar{X}_{0} = 2$ (average exposure for non-carriers, $Z=0$)\n\nFirst, we can compute the sample estimate of the difference in means for the outcome $Y$:\n$$ \\Delta\\bar{Y} = \\bar{Y}_{1} - \\bar{Y}_{0} = 6 - 4 = 2 $$\nNext, we compute the sample estimate of the difference in means for the exposure $X$:\n$$ \\Delta\\bar{X} = \\bar{X}_{1} - \\bar{X}_{0} = 3 - 2 = 1 $$\nThe non-zero value of $\\Delta\\bar{X}$ ($1 \\neq 0$) provides empirical evidence from the sample that the instrument relevance assumption holds.\n\nNow, we can compute the numerical value of the Wald estimator $\\hat{\\beta}_{W}$:\n$$ \\hat{\\beta}_{W} = \\frac{\\Delta\\bar{Y}}{\\Delta\\bar{X}} = \\frac{2}{1} = 2 $$\n\nThe problem also asks for the interpretation of this estimate. With the addition of the monotonicity assumption (that the instrument $Z$ does not decrease the exposure $X$ for any individual), this estimator provides an estimate of the Local Average Treatment Effect (LATE). The LATE is the average causal effect of the exposure on the outcome specifically within the subpopulation of 'compliers'. In this context, compliers are individuals whose exposure level $X$ is affected by the genetic variant $Z$ in the expected direction (i.e., their value of $X$ is higher if they are a carrier of the effect allele, $Z=1$, than if they were a non-carrier, $Z=0$). Therefore, $\\hat{\\beta}_{W} = 2$ is interpreted as the estimated average causal effect of a $1$-unit increase in the exposure $X$ on the outcome $Y$ for those individuals whose exposure level is sensitive to the genetic instrument $Z$. Since $X$ and $Y$ are on standardized logarithmic scales, this corresponds to a $2$-unit increase in the log-transformed outcome for a $1$-unit increase in the log-transformed exposure, among compliers.", "answer": "$$\n\\boxed{2}\n$$", "id": "4966574"}, {"introduction": "A critical step in any reliable Mendelian Randomization study is to verify that the chosen genetic variant is a sufficiently \"strong\" instrument for the exposure. This exercise introduces the first-stage $F$-statistic, the primary diagnostic tool used to quantify instrument strength. You will not only compute this statistic but also explore its crucial link to the potential bias of your causal estimate, learning why a weak instrument can lead to misleading conclusions. [@problem_id:4916953]", "problem": "Consider a Mendelian randomization (MR) study in which a single genetic variant $Z$ is used as an instrumental variable (IV) for an exposure $X$. Assume the first-stage linear model $X_{i} = \\pi Z_{i} + V_{i}$ for individuals $i = 1, \\dots, n$, where both $X$ and $Z$ have been mean-centered so that an intercept is not required. You will work from the foundational definitions of ordinary least squares and classical sum-of-squares decomposition to derive the first-stage $F$-statistic for testing the null hypothesis $H_{0}: \\pi = 0$.\n\nThe dataset summary is:\n- Sample size $n = 1000$.\n- $\\sum_{i=1}^{n} Z_{i}^{2} = 1000$.\n- $\\sum_{i=1}^{n} X_{i}^{2} = 1000$.\n- $\\sum_{i=1}^{n} Z_{i} X_{i} = 210$.\n\nStarting from the definitions of the least squares estimator and the residual sum of squares, derive the first-stage $F$-statistic for testing $H_{0}: \\pi = 0$ in this one-regressor, no-intercept regression, and explain how it captures instrument strength and its connection to the approximate bias of the two-stage least squares estimator relative to ordinary least squares.\n\nCompute the numerical value of the $F$-statistic using the provided dataset summary and express the final answer rounded to four significant figures. No units are required.", "solution": "The user has provided a valid biostatistics problem.\n\nThe problem asks for the derivation of the first-stage $F$-statistic in a Mendelian randomization (MR) context, its interpretation, and its numerical calculation based on provided summary statistics. The model is a simple linear regression without an intercept, for which we must test the significance of a single predictor.\n\n### Step 1: Derivation of the F-statistic\n\nThe model specified is the first-stage regression for an MR study:\n$$X_{i} = \\pi Z_{i} + V_{i}$$\nwhere $i = 1, \\dots, n$. The variables $X$ (exposure) and $Z$ (instrumental variable) are mean-centered, so no intercept term is needed. The null hypothesis to be tested is $H_{0}: \\pi = 0$.\n\nThe ordinary least squares (OLS) estimator $\\hat{\\pi}$ is found by minimizing the Residual Sum of Squares (RSS):\n$$RSS = \\sum_{i=1}^{n} (X_{i} - \\pi Z_{i})^{2}$$\nTo find the minimum, we take the derivative with respect to $\\pi$ and set it to zero:\n$$\\frac{d(RSS)}{d\\pi} = \\sum_{i=1}^{n} 2(X_{i} - \\pi Z_{i})(-Z_{i}) = -2 \\left( \\sum_{i=1}^{n} X_{i}Z_{i} - \\pi \\sum_{i=1}^{n} Z_{i}^{2} \\right) = 0$$\nSolving for $\\pi$ gives the OLS estimator $\\hat{\\pi}$:\n$$\\hat{\\pi} \\sum_{i=1}^{n} Z_{i}^{2} = \\sum_{i=1}^{n} X_{i}Z_{i} \\implies \\hat{\\pi} = \\frac{\\sum_{i=1}^{n} X_{i}Z_{i}}{\\sum_{i=1}^{n} Z_{i}^{2}}$$\nThe $F$-statistic is a ratio of mean squares. For a regression model, it is the ratio of the Mean Square for Regression (MSR) to the Mean Square for Error (MSE).\n$$F = \\frac{\\text{MSR}}{\\text{MSE}} = \\frac{\\text{ESS}/k}{\\text{RSS}/(n-k)}$$\nHere, the number of predictors is $k=1$. The degrees of freedom for the regression are $k=1$, and the degrees of freedom for the residuals are $n-k = n-1$.\n\nThe Total Sum of Squares (TSS) for the mean-centered variable $X$ is $\\sum_{i=1}^{n} X_{i}^{2}$. This is decomposed into the Explained Sum of Squares (ESS) and the Residual Sum of Squares (RSS).\n$$TSS = ESS + RSS \\implies \\sum_{i=1}^{n} X_{i}^{2} = ESS + RSS$$\nThe ESS is the sum of squares of the predicted values, $\\hat{X}_{i} = \\hat{\\pi}Z_{i}$:\n$$ESS = \\sum_{i=1}^{n} \\hat{X}_{i}^{2} = \\sum_{i=1}^{n} (\\hat{\\pi}Z_{i})^{2} = \\hat{\\pi}^{2} \\sum_{i=1}^{n} Z_{i}^{2}$$\nSubstituting the expression for $\\hat{\\pi}$:\n$$ESS = \\left( \\frac{\\sum_{i=1}^{n} X_{i}Z_{i}}{\\sum_{i=1}^{n} Z_{i}^{2}} \\right)^{2} \\sum_{i=1}^{n} Z_{i}^{2} = \\frac{\\left( \\sum_{i=1}^{n} X_{i}Z_{i} \\right)^{2}}{\\sum_{i=1}^{n} Z_{i}^{2}}$$\nThe RSS is the portion of the total variation not explained by the model:\n$$RSS = TSS - ESS = \\sum_{i=1}^{n} X_{i}^{2} - \\frac{\\left( \\sum_{i=1}^{n} X_{i}Z_{i} \\right)^{2}}{\\sum_{i=1}^{n} Z_{i}^{2}}$$\nNow, we can construct the $F$-statistic:\n$$F = \\frac{ESS/1}{RSS/(n-1)} = \\frac{ESS \\cdot (n-1)}{RSS}$$\nSubstituting the expressions for ESS and RSS:\n$$F = \\frac{\\frac{\\left( \\sum_{i=1}^{n} X_{i}Z_{i} \\right)^{2}}{\\sum_{i=1}^{n} Z_{i}^{2}} \\cdot (n-1)}{\\sum_{i=1}^{n} X_{i}^{2} - \\frac{\\left( \\sum_{i=1}^{n} X_{i}Z_{i} \\right)^{2}}{\\sum_{i=1}^{n} Z_{i}^{2}}}$$\nTo simplify, multiply the numerator and denominator by $\\sum_{i=1}^{n} Z_{i}^{2}$:\n$$F = \\frac{(n-1) \\left( \\sum_{i=1}^{n} X_{i}Z_{i} \\right)^{2}}{\\left(\\sum_{i=1}^{n} X_{i}^{2}\\right) \\left(\\sum_{i=1}^{n} Z_{i}^{2}\\right) - \\left( \\sum_{i=1}^{n} X_{i}Z_{i} \\right)^{2}}$$\nThis is the derived expression for the first-stage $F$-statistic.\n\n### Step 2: Interpretation and Connection to 2SLS Bias\n\nThe $F$-statistic tests the null hypothesis $H_{0}: \\pi=0$. A large value of $F$ leads to the rejection of $H_{0}$, indicating that the instrument $Z$ is a statistically significant predictor of the exposure $X$. This corresponds to the \"relevance\" or \"strength\" assumption for an instrumental variable. An instrument is considered \"weak\" if this association is not strong, which is typically diagnosed by a small first-stage $F$-statistic (a common rule of thumb is $F < 10$).\n\nWeak instruments pose a serious problem in two-stage least squares (2SLS) estimation. The conventional 2SLS estimator for the causal effect $\\beta$ in the second-stage model $Y = \\beta X + U$ is biased in finite samples. The magnitude of this bias is related to the strength of the instrument. Staiger and Stock (1997) showed that the approximate bias of the 2SLS estimator, $E[\\hat{\\beta}_{2SLS}] - \\beta$, is related to the bias of the OLS estimator, $E[\\hat{\\beta}_{OLS}] - \\beta$, by the population F-statistic. A common approximation is:\n$$ E[\\hat{\\beta}_{2SLS}] - \\beta \\approx \\frac{E[\\hat{\\beta}_{OLS}] - \\beta}{E[F]}$$\nThis reveals that the bias of the 2SLS estimator is inversely proportional to the expected value of the first-stage $F$-statistic. When the instrument is weak, $F$ is small, and the bias of the 2SLS estimator can be substantial, approaching the bias of the OLS estimator. Conversely, a strong instrument (large $F$) leads to a large denominator, significantly reducing the bias of the 2SLS estimator compared to OLS. Therefore, the first-stage $F$-statistic is a crucial diagnostic for the reliability of an MR analysis, as it directly quantifies the extent to which weak instrument bias is a concern.\n\n### Step 3: Numerical Calculation\n\nWe use the derived formula and the provided summary statistics:\n- $n = 1000$\n- $\\sum_{i=1}^{n} Z_{i}^{2} = 1000$\n- $\\sum_{i=1}^{n} X_{i}^{2} = 1000$\n- $\\sum_{i=1}^{n} Z_{i} X_{i} = 210$\n\nPlugging these values into the formula for the $F$-statistic:\n$$F = \\frac{(n-1) \\left( \\sum Z_{i} X_{i} \\right)^{2}}{(\\sum X_{i}^{2})(\\sum Z_{i}^{2}) - (\\sum Z_{i} X_{i})^{2}}$$\n$$F = \\frac{(1000-1) (210)^{2}}{(1000)(1000) - (210)^{2}}$$\n$$F = \\frac{999 \\times 44100}{1000000 - 44100}$$\n$$F = \\frac{44055900}{955900}$$\n$$F = \\frac{440559}{9559} \\approx 46.08829375$$\nRounding to four significant figures, we get $F = 46.09$. This value is substantially greater than $10$, indicating that the genetic variant $Z$ is a strong instrument for the exposure $X$ in this dataset.", "answer": "$$\n\\boxed{46.09}\n$$", "id": "4916953"}]}