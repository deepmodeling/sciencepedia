{"hands_on_practices": [{"introduction": "To truly understand the log-rank test, it is essential to work through its mechanics by hand. This exercise [@problem_id:4850238] provides a step-by-step guide to calculating the chi-square statistic from summary data. By computing the observed events, expected events, and variance at each distinct event time based on the underlying hypergeometric model, you will gain a concrete grasp of how the test aggregates evidence over time to compare survival curves.", "problem": "A randomized two-arm oncology study compares time-to-progression between Treatment A and Treatment B. Investigators plan a nonparametric comparison of survival curves based on the principle that, under the null hypothesis of equal hazard functions over time, the group labels of events at each distinct event time are, conditional on the current risk set and total number of events at that time, randomly allocated according to a finite-population sampling model. Specifically, at any event time, given the number at risk in each arm and the total number of events, the number of events in Treatment A follows a finite-population draw without replacement from the combined risk set.\n\nThe study begins with $12$ patients in Treatment A and $10$ patients in Treatment B. Ties (multiple events at the same recorded time) and right censoring occur, but the risk set counts given below already incorporate all prior events and censoring. There are $6$ distinct event times with the following observed data; for each time $t_j$ the quantities are:\n- $r_{A,j}$: number at risk in Treatment A just prior to $t_j$,\n- $r_{B,j}$: number at risk in Treatment B just prior to $t_j$,\n- $d_j$: total number of events at $t_j$ across both groups,\n- $d_{A,j}$: number of events at $t_j$ in Treatment A.\n\nThe observed counts at the event times are:\n- $t_1$: $r_{A,1} = 12$, $r_{B,1} = 10$, $d_1 = 2$, $d_{A,1} = 1$.\n- $t_2$: $r_{A,2} = 11$, $r_{B,2} = 9$, $d_2 = 1$, $d_{A,2} = 1$.\n- $t_3$: $r_{A,3} = 10$, $r_{B,3} = 8$, $d_3 = 3$, $d_{A,3} = 1$.\n- $t_4$: $r_{A,4} = 9$, $r_{B,4} = 6$, $d_4 = 2$, $d_{A,4} = 0$.\n- $t_5$: $r_{A,5} = 8$, $r_{B,5} = 4$, $d_5 = 1$, $d_{A,5} = 0$.\n- $t_6$: $r_{A,6} = 8$, $r_{B,6} = 3$, $d_6 = 1$, $d_{A,6} = 1$.\n\nUsing only the foundational assumption of equal hazards under the null and the finite-population sampling model at each event time, compute the one-degree-of-freedom log-rank chi-square test statistic that compares the Treatment A and Treatment B survival curves for time-to-progression. Round your final numeric answer to four significant figures. Express the final answer as a pure number without any units.", "solution": "The problem requires the computation of the log-rank chi-square test statistic for comparing survival curves between Treatment A and Treatment B. The log-rank test assesses the null hypothesis that there is no difference in the hazard functions of the two groups. The test statistic is constructed by summing the observed and expected number of events over the distinct event times.\n\nThe one-degree-of-freedom log-rank test statistic, denoted as $\\chi^2$, is given by the formula:\n$$ \\chi^2 = \\frac{(\\sum_{j=1}^{k} (O_j - E_j))^2}{\\sum_{j=1}^{k} V_j} = \\frac{(O - E)^2}{V} $$\nwhere the summations are over the $k=6$ distinct event times.\n\nFor each event time $t_j$, we define the following quantities for a chosen group, which we will take to be Treatment A:\n- $O_j = d_{A,j}$: The observed number of events in Treatment A at time $t_j$.\n- $E_j$: The expected number of events in Treatment A at time $t_j$, under the null hypothesis of equal hazards. This is calculated based on the hypergeometric distribution.\n- $V_j$: The variance of the number of events in Treatment A at time $t_j$, also based on the hypergeometric distribution.\n\nThe total number of patients at risk at time $t_j$ is $r_j = r_{A,j} + r_{B,j}$, where $r_{A,j}$ and $r_{B,j}$ are the number of patients at risk in Treatment A and Treatment B, respectively, just prior to $t_j$. With a total of $d_j$ events at $t_j$, the expected number of events in Treatment A is:\n$$ E_j = d_j \\frac{r_{A,j}}{r_j} $$\nThe variance of the number of events in Treatment A is:\n$$ V_j = \\frac{r_{A,j} r_{B,j} d_j (r_j - d_j)}{r_j^2 (r_j - 1)} $$\nThe case where $r_j = 1$ is not applicable here, but would result in $V_j=0$.\n\nWe can systematically compute $E_j$ and $V_j$ for each of the $6$ event times using the provided data.\n\nFor $t_1$: $r_{A,1} = 12$, $r_{B,1} = 10$, $r_1 = 22$, $d_1 = 2$, $d_{A,1} = 1$.\n$E_1 = 2 \\times \\frac{12}{22} = \\frac{12}{11}$.\n$V_1 = \\frac{12 \\times 10 \\times 2 \\times (22-2)}{22^2 \\times (22-1)} = \\frac{120 \\times 2 \\times 20}{484 \\times 21} = \\frac{4800}{10164} = \\frac{400}{847}$.\n\nFor $t_2$: $r_{A,2} = 11$, $r_{B,2} = 9$, $r_2 = 20$, $d_2 = 1$, $d_{A,2} = 1$.\n$E_2 = 1 \\times \\frac{11}{20} = \\frac{11}{20}$.\n$V_2 = \\frac{11 \\times 9 \\times 1 \\times (20-1)}{20^2 \\times (20-1)} = \\frac{99 \\times 19}{400 \\times 19} = \\frac{99}{400}$.\n\nFor $t_3$: $r_{A,3} = 10$, $r_{B,3} = 8$, $r_3 = 18$, $d_3 = 3$, $d_{A,3} = 1$.\n$E_3 = 3 \\times \\frac{10}{18} = \\frac{5}{3}$.\n$V_3 = \\frac{10 \\times 8 \\times 3 \\times (18-3)}{18^2 \\times (18-1)} = \\frac{80 \\times 3 \\times 15}{324 \\times 17} = \\frac{3600}{5508} = \\frac{100}{153}$.\n\nFor $t_4$: $r_{A,4} = 9$, $r_{B,4} = 6$, $r_4 = 15$, $d_4 = 2$, $d_{A,4} = 0$.\n$E_4 = 2 \\times \\frac{9}{15} = \\frac{6}{5}$.\n$V_4 = \\frac{9 \\times 6 \\times 2 \\times (15-2)}{15^2 \\times (15-1)} = \\frac{108 \\times 13}{225 \\times 14} = \\frac{1404}{3150} = \\frac{78}{175}$.\n\nFor $t_5$: $r_{A,5} = 8$, $r_{B,5} = 4$, $r_5 = 12$, $d_5 = 1$, $d_{A,5} = 0$.\n$E_5 = 1 \\times \\frac{8}{12} = \\frac{2}{3}$.\n$V_5 = \\frac{8 \\times 4 \\times 1 \\times (12-1)}{12^2 \\times (12-1)} = \\frac{32 \\times 11}{144 \\times 11} = \\frac{32}{144} = \\frac{2}{9}$.\n\nFor $t_6$: $r_{A,6} = 8$, $r_{B,6} = 3$, $r_6 = 11$, $d_6 = 1$, $d_{A,6} = 1$.\n$E_6 = 1 \\times \\frac{8}{11} = \\frac{8}{11}$.\n$V_6 = \\frac{8 \\times 3 \\times 1 \\times (11-1)}{11^2 \\times (11-1)} = \\frac{24 \\times 10}{121 \\times 10} = \\frac{24}{121}$.\n\nNext, we sum these quantities to get the totals $O$, $E$, and $V$.\nThe total observed number of events in Treatment A is:\n$$ O = \\sum_{j=1}^{6} d_{A,j} = 1 + 1 + 1 + 0 + 0 + 1 = 4 $$\nThe total expected number of events in Treatment A is:\n$$ E = \\sum_{j=1}^{6} E_j = \\frac{12}{11} + \\frac{11}{20} + \\frac{5}{3} + \\frac{6}{5} + \\frac{2}{3} + \\frac{8}{11} $$\n$$ E = \\left(\\frac{12}{11} + \\frac{8}{11}\\right) + \\left(\\frac{5}{3} + \\frac{2}{3}\\right) + \\frac{11}{20} + \\frac{6}{5} = \\frac{20}{11} + \\frac{7}{3} + \\frac{11}{20} + \\frac{24}{20} = \\frac{20}{11} + \\frac{7}{3} + \\frac{35}{20} = \\frac{20}{11} + \\frac{7}{3} + \\frac{7}{4} $$\nThe least common denominator of $11$, $3$, and $4$ is $132$.\n$$ E = \\frac{20 \\times 12}{132} + \\frac{7 \\times 44}{132} + \\frac{7 \\times 33}{132} = \\frac{240 + 308 + 231}{132} = \\frac{779}{132} $$\nThe total variance is the sum of the individual variances:\n$$ V = \\sum_{j=1}^{6} V_j = \\frac{400}{847} + \\frac{99}{400} + \\frac{100}{153} + \\frac{78}{175} + \\frac{2}{9} + \\frac{24}{121} $$\nWe convert these fractions to decimal values for calculation, maintaining high precision:\n$V \\approx 0.47225502 + 0.2475 + 0.65359477 + 0.44571429 + 0.22222222 + 0.19834711$\n$V \\approx 2.23963341$\n\nNow we compute the numerator of the test statistic, $(O-E)^2$:\n$O - E = 4 - \\frac{779}{132} = \\frac{528 - 779}{132} = -\\frac{251}{132}$\n$(O - E)^2 = \\left(-\\frac{251}{132}\\right)^2 = \\frac{63001}{17424} \\approx 3.61564514$\n\nFinally, we compute the chi-square statistic:\n$$ \\chi^2 = \\frac{(O-E)^2}{V} \\approx \\frac{3.61564514}{2.23963341} \\approx 1.61444158 $$\nThe problem requires the answer rounded to four significant figures.\n$$ \\chi^2 \\approx 1.614 $$", "answer": "$$\\boxed{1.614}$$", "id": "4850238"}, {"introduction": "While manual calculations are insightful, real-world survival analysis requires processing raw data efficiently. This practice [@problem_id:4923234] challenges you to move beyond a single calculation and develop a computational algorithm for the log-rank test. By structuring the logic to handle unsorted data, tied event times, and censoring, you will build a robust tool that mirrors the functionality of statistical software and deepens your understanding of the test's implementation.", "problem": "You are given raw individual-level survival data for exactly two groups. Each subject is represented by a triple $(X_i,\\ \\delta_i,\\ G_i)$, where $X_i$ is the observed time, $\\delta_i \\in \\{0,1\\}$ is the event indicator ($\\delta_i = 1$ indicates an event at time $X_i$, $\\delta_i = 0$ indicates right-censoring at time $X_i$), and $G_i \\in \\{0,1\\}$ is the group indicator (group $0$ versus group $1$). Assume that, under the null hypothesis, the two groups share the same hazard function over time. Construct an algorithm, starting only from the following fundamental base, to compute the log-rank score statistic $U$ and its large-sample variance estimator $\\hat{V}$ using the raw data:\n\n- The survival function $S(t)$ and hazard function $\\lambda(t)$ are related by $S(t) = \\exp\\left(-\\int_0^t \\lambda(u)\\,du\\right)$, and the hazard functions for the two groups are equal under the null hypothesis.\n- At any observed event time $t$, conditional on the number at risk just before $t$ and the total number of events that occur at $t$, the allocation of those events across the two groups follows a sampling-without-replacement mechanism that is well-modeled by the hypergeometric distribution when the null hypothesis holds.\n- The risk set at time $t$ consists of all individuals with observed times $X_i \\ge t$ just prior to time $t$.\n- If there is only a single subject at risk at an event time (that is, the total at risk is $1$), then there is no uncertainty in which group experiences the event at that time.\n\nYour algorithm must explicitly:\n- Sort the unique observed times in increasing order.\n- Maintain and update the numbers at risk in each group across event times using only cumulative removals of subjects with $X_i$ equal to the current time.\n- Pool all events that occur at exactly the same observed time and treat them as a tied set at that time.\n- Accumulate the log-rank score statistic $U$ as a sum over observed event times and accumulate its variance estimator $\\hat{V}$ from the conditional variance at each event time, using only the principles stated above.\n\nThen, implement the algorithm as a program that takes no input and computes $(U,\\ \\hat{V})$ and the standardized statistic $Z = U / \\sqrt{\\hat{V}}$ (define $Z = 0$ if $\\hat{V} = 0$) for each of the following four test cases. In each test case, the data are given as a list of $(X_i,\\ \\delta_i,\\ G_i)$ triples.\n\nTest case A (general case with ties and censoring across both groups):\n- Group $0$: $(5, 1, 0)$, $(6, 0, 0)$, $(7, 1, 0)$, $(10, 1, 0)$, $(15, 0, 0)$\n- Group $1$: $(4, 1, 1)$, $(6, 1, 1)$, $(9, 0, 1)$, $(10, 1, 1)$, $(12, 0, 1)$, $(16, 1, 1)$\n\nTest case B (boundary case with a time having only one subject at risk):\n- Group $0$: $(1, 1, 0)$\n- Group $1$: $(2, 0, 1)$, $(3, 1, 1)$\n\nTest case C (edge case with one group fully censored):\n- Group $0$: $(2, 0, 0)$, $(5, 0, 0)$, $(7, 0, 0)$\n- Group $1$: $(1, 1, 1)$, $(3, 1, 1)$, $(4, 1, 1)$\n\nTest case D (multiple events and censorings tied at the same time across groups):\n- Group $0$: $(10, 1, 0)$, $(10, 1, 0)$, $(10, 0, 0)$\n- Group $1$: $(10, 1, 1)$, $(10, 0, 1)$, $(10, 1, 1)$, $(12, 0, 1)$\n\nDesign requirements for the program:\n- For each test case, compute $U$, $\\hat{V}$, and $Z$ as real numbers.\n- The final output must be a single line containing a list of results, one per test case, where each result is the list $[U,\\ \\hat{V},\\ Z]$ rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[[u_A,\\ v_A,\\ z_A],[u_B,\\ v_B,\\ z_B],[u_C,\\ v_C,\\ z_C],[u_D,\\ v_D,\\ z_D]]$), with each numeric value rounded to $6$ decimal places.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the established biostatistical theory of survival analysis, specifically the derivation of the log-rank test. The problem is well-posed, objective, self-contained, and formalizable. It presents a standard but substantive task: to implement the log-rank test from its fundamental principles based on the hypergeometric distribution of events at each event time under the null hypothesis. The provided test cases are well-defined and cover general, boundary, and edge scenarios, making them suitable for validating the correctness of the derived algorithm.\n\nThe objective is to construct an algorithm to compute the log-rank score statistic $U$, its variance estimator $\\hat{V}$, and the standardized statistic $Z = U / \\sqrt{\\hat{V}}$ from raw survival data for two groups. The data for each subject $i$ is a triple $(X_i, \\delta_i, G_i)$, where $X_i$ is the observed time, $\\delta_i \\in \\{0, 1\\}$ is the event indicator ($1$ for an event, $0$ for censoring), and $G_i \\in \\{0, 1\\}$ is the group indicator.\n\nThe fundamental principle of the log-rank test is to compare the observed number of events to the expected number of events in each group at every distinct event time, under the null hypothesis ($H_0$) that the hazard functions of the two groups are identical.\n\nLet the distinct event times observed in the combined data be $t_1  t_2  \\dots  t_k$. At each event time $t_j$, we can construct a $2 \\times 2$ contingency table summarizing the status of the subjects at risk just prior to time $t_j$.\n\nThe notation for the contingency table at a specific event time $t_j$ is as follows:\n- $n_{0j}$: Number of subjects at risk in group $0$ just before time $t_j$. This is the count of subjects in group $0$ with an observed time $X_i \\ge t_j$.\n- $n_{1j}$: Number of subjects at risk in group $1$ just before time $t_j$. This is the count of subjects in group $1$ with an observed time $X_i \\ge t_j$.\n- $n_j = n_{0j} + n_{1j}$: Total number of subjects at risk just before time $t_j$.\n- $d_{0j}$: Number of subjects in group $0$ who experience an event at time $t_j$. This is the count of subjects with $G_i=0$, $\\delta_i=1$, and $X_i=t_j$.\n- $d_{1j}$: Number of subjects in group $1$ who experience an event at time $t_j$. This is the count of subjects with $G_i=1$, $\\delta_i=1$, and $X_i=t_j$.\n- $d_j = d_{0j} + d_{1j}$: Total number of events at time $t_j$.\n\nUnder $H_0$, given the marginal totals of this table ($n_{0j}$, $n_{1j}$, $d_j$, and $n_j - d_j$), the number of events in group $1$, $d_{1j}$, follows a hypergeometric distribution. This is because we are sampling $d_j$ individuals (who have the event) without replacement from the total risk set of $n_j$ individuals, which contains $n_{1j}$ individuals from group $1$.\n\nThe expected value and variance of a random variable following this hypergeometric distribution are:\n- Expected number of events in group $1$ at time $t_j$:\n  $$E[d_{1j}] = d_j \\frac{n_{1j}}{n_j}$$\n- Variance of the number of events in group $1$ at time $t_j$:\n  $$\\text{Var}(d_{1j}) = d_j \\frac{n_{1j}}{n_j} \\left(1 - \\frac{n_{1j}}{n_j}\\right) \\frac{n_j - d_j}{n_j - 1} = \\frac{n_{1j} n_{0j} d_j (n_j - d_j)}{n_j^2 (n_j - 1)}$$\n\nThe log-rank score statistic, $U$, is defined as the sum of the differences between the observed and expected number of events in group $1$ over all event times.\n$$U = \\sum_{j=1}^{k} (d_{1j} - E[d_{1j}]) = \\sum_{j=1}^{k} \\left(d_{1j} - d_j \\frac{n_{1j}}{n_j}\\right)$$\n\nAssuming independence of the statistics at different event times, the variance of $U$ can be estimated by summing the conditional variances at each event time. This yields the Mantel-Haenszel variance estimator, $\\hat{V}$. The special case where the total number at risk $n_j \\le 1$ is handled as follows: if $n_j \\le 1$, the term $n_j-1$ in the denominator becomes non-positive. However, if $n_j=1$, there is no uncertainty about which group the event occurred in, so the variance contribution is $0$. This is also consistent with the formula, as if $n_j=1$ and an event occurs ($d_j=1$), then $n_j-d_j = 0$, making the numerator and thus the variance term zero. If $n_j=0$, no events can occur. Therefore, the variance contribution is added only if $n_j  1$.\n\n$$\\hat{V} = \\sum_{j=1}^{k} \\text{Var}(d_{1j}) = \\sum_{j=1}^{k} \\frac{n_{1j} n_{0j} d_j (n_j - d_j)}{n_j^2 (n_j - 1)}$$\nThis summation is performed only over event times $t_j$ where $n_j  1$.\n\nFinally, the standardized test statistic $Z$ is calculated, which, under $H_0$, follows approximately a standard normal distribution for large sample sizes.\n$$Z = \\frac{U}{\\sqrt{\\hat{V}}}$$\nIf $\\hat{V} = 0$, $Z$ is defined to be $0$.\n\nThe algorithm to implement this is as follows:\n1.  Compile all raw data $(X_i, \\delta_i, G_i)$ into a single dataset.\n2.  Identify the set of unique times at which one or more events occurred. Sort these event times in ascending order: $t_1, t_2, \\ldots, t_k$.\n3.  Initialize the score statistic $U = 0$ and variance estimator $\\hat{V} = 0$.\n4.  For each unique event time $t_j$ in the sorted list:\n    a. Determine the number of subjects at risk in each group just before $t_j$:\n       $n_{0j} = |\\{i : G_i=0 \\text{ and } X_i \\ge t_j\\}|$\n       $n_{1j} = |\\{i : G_i=1 \\text{ and } X_i \\ge t_j\\}|$\n       $n_j = n_{0j} + n_{1j}$\n    b. Determine the number of events that occurred in each group at time $t_j$:\n       $d_{0j} = |\\{i : G_i=0, \\delta_i=1, \\text{ and } X_i = t_j\\}|$\n       $d_{1j} = |\\{i : G_i=1, \\delta_i=1, \\text{ and } X_i = t_j\\}|$\n       $d_j = d_{0j} + d_{1j}$\n    c. If $n_j  0$, calculate the contribution to $U$:\n       $E_{1j} = d_j \\frac{n_{1j}}{n_j}$\n       $U \\leftarrow U + (d_{1j} - E_{1j})$\n    d. If $n_j  1$, calculate the contribution to $\\hat{V}$:\n       $V_j = \\frac{n_{1j} n_{0j} d_j (n_j - d_j)}{n_j^2 (n_j - 1)}$\n       $\\hat{V} \\leftarrow \\hat{V} + V_j$\n5.  After iterating through all event times, calculate the final standardized statistic: $Z = U / \\sqrt{\\hat{V}}$ if $\\hat{V}  0$, otherwise $Z = 0$.\n6.  The result for a given test case is the triplet $[U, \\hat{V}, Z]$. This procedure is applied systematically to each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the log-rank test for the specified test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each data point is a tuple (time, event_indicator, group_indicator).\n    test_cases = [\n        # Test case A\n        [\n            (5, 1, 0), (6, 0, 0), (7, 1, 0), (10, 1, 0), (15, 0, 0),\n            (4, 1, 1), (6, 1, 1), (9, 0, 1), (10, 1, 1), (12, 0, 1), (16, 1, 1)\n        ],\n        # Test case B\n        [\n            (1, 1, 0),\n            (2, 0, 1), (3, 1, 1)\n        ],\n        # Test case C\n        [\n            (2, 0, 0), (5, 0, 0), (7, 0, 0),\n            (1, 1, 1), (3, 1, 1), (4, 1, 1)\n        ],\n        # Test case D\n        [\n            (10, 1, 0), (10, 1, 0), (10, 0, 0),\n            (10, 1, 1), (10, 0, 1), (10, 1, 1), (12, 0, 1)\n        ]\n    ]\n\n    def compute_log_rank(data):\n        \"\"\"\n        Computes the log-rank statistic U, its variance V_hat, and Z-score.\n        \n        Args:\n            data: A list of tuples (time, event, group).\n            \n        Returns:\n            A list containing [U, V_hat, Z].\n        \"\"\"\n        # Identify unique event times and sort them\n        event_times = sorted(list(set(t for t, d, g in data if d == 1)))\n\n        U = 0.0\n        V_hat = 0.0\n\n        if not event_times:\n            return [0.0, 0.0, 0.0]\n\n        for t in event_times:\n            # Subjects at risk just before time t\n            n0 = sum(1 for x, d, g in data if g == 0 and x >= t)\n            n1 = sum(1 for x, d, g in data if g == 1 and x >= t)\n            n = n0 + n1\n\n            # Events at time t\n            d0 = sum(1 for x, d, g in data if g == 0 and d == 1 and x == t)\n            d1 = sum(1 for x, d, g in data if g == 1 and d == 1 and x == t)\n            d_total = d0 + d1\n            \n            if n > 0:\n                # Expected events in group 1\n                expected_d1 = d_total * (n1 / n)\n                # Update score statistic U\n                U += (d1 - expected_d1)\n\n            # Variance calculation\n            # Denominator n-1 must be > 0, so n > 1\n            if n > 1:\n                var_term = (n1 * n0 * d_total * (n - d_total)) / (n**2 * (n - 1))\n                V_hat += var_term\n        \n        # Calculate Z-score\n        if V_hat > 0:\n            Z = U / np.sqrt(V_hat)\n        else:\n            Z = 0.0\n            \n        return [U, V_hat, Z]\n\n    results = []\n    for case_data in test_cases:\n        u, v, z = compute_log_rank(case_data)\n        results.append(f\"[{u:.6f},{v:.6f},{z:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "4923234"}, {"introduction": "A statistical test is only as useful as its interpretation. This exercise [@problem_id:4923201] bridges the gap between calculation and communication by presenting a complete analysis scenario from raw data to final report. Your task is to compute the key statistics—the score $U$, its variance $\\hat{V}$, and the standardized statistic $Z$—and then critically evaluate how these results, along with graphical diagnostics, should be correctly interpreted and reported to draw valid scientific conclusions.", "problem": "A randomized, two-arm study followed $10$ participants, with $5$ assigned to Treatment and $5$ to Control. Each participant has a follow-up time (in months) and an indicator of event occurrence ($1$ for event, $0$ for right-censoring). Assume independent, non-informative censoring and no tied event times. The data are:\n\n- Treatment arm: $(2, 1)$, $(5, 1)$, $(7.5, 0)$, $(8, 1)$, $(9, 0)$.\n- Control arm: $(3, 1)$, $(4, 0)$, $(6, 1)$, $(7, 1)$, $(10, 0)$.\n\nUnder the null hypothesis of equal hazard functions over time between the two arms, the log-rank test compares the observed and expected numbers of events in the Treatment arm at each distinct event time, aggregating across all event times and using an appropriate variance based on the variability of hypergeometric allocation of events given the risk sets. The standard normal approximation is used for the test statistic under the null.\n\nYour task is to select the option that correctly and completely reports the log-rank test results for these data, including the statistic $U$ (observed minus expected events summed over event times for the Treatment arm), its variance estimate $\\hat{V}$, the standardized statistic $Z$, the two-sided $p$-value, and guidance for graphical diagnostics using Kaplan–Meier (KM) curves. Only one option adheres to correct calculation and best-practice reporting.\n\nA. Report $U$, $\\hat{V}$, $Z$, and the two-sided $p$-value, and present KM curves with a risk table aligned to the distinct event times, marking censorings. Specifically: $U \\approx -0.283$, $\\hat{V} \\approx 1.454$, $Z \\approx -0.234$, two-sided $p \\approx 0.815$. State that the sign of $U$ indicates fewer events in Treatment than expected under the null at the observed event times, and that the KM plot and risk table (numbers at risk shown at the exact event times) aid interpretation. Conclude no evidence of a difference between survival curves at conventional significance levels.\n\nB. Report $U \\approx -0.283$, $\\hat{V} \\approx 1.454$, and compute $Z$ as $U / \\hat{V} \\approx -0.195$ with a one-sided $p \\approx 0.423$. Present KM curves but align the risk table to evenly spaced monthly ticks rather than the event times. Conclude that the Treatment arm shows a trend toward benefit based on the one-sided result.\n\nC. Report $U \\approx +0.717$, $\\hat{V} \\approx 0.612$, $Z \\approx +0.918$, two-sided $p \\approx 0.359$, and declare statistical significance at the $\\alpha = 0.05$ level. Present KM curves without censoring marks and align the risk table to the median survival time of each arm.\n\nD. Report only the two-sided $p \\approx 0.815$ (omit $U$, $\\hat{V}$, and $Z$), and present KM curves without a risk table, noting that the curves alone suffice for diagnostic purposes. State that the null hypothesis pertains to equal median survival times between arms, which were similar in this study.\n\nWhich option is correct?", "solution": "The user wants me to critically evaluate a biostatistics problem involving a log-rank test and select the correct option among the provided choices.\n\n### Step 1: Extract Givens\n\n-   **Study Design**: Randomized, two-arm study (Treatment vs. Control).\n-   **Sample Size**: $n=10$ total participants.\n-   **Group Allocation**: $n_T = 5$ in Treatment, $n_C = 5$ in Control.\n-   **Data Format**: (time in months, event status), where $1$ = event, $0$ = right-censoring.\n-   **Treatment Arm Data**: $(2, 1)$, $(5, 1)$, $(7.5, 0)$, $(8, 1)$, $(9, 0)$.\n-   **Control Arm Data**: $(3, 1)$, $(4, 0)$, $(6, 1)$, $(7, 1)$, $(10, 0)$.\n-   **Assumptions**: Independent, non-informative censoring; no tied event times.\n-   **Null Hypothesis ($H_0$)**: The hazard functions of the two arms are equal over time.\n-   **Test**: Log-rank test.\n-   **Statistic $U$**: Sum of (observed - expected) events for the Treatment arm across all distinct event times.\n-   **Statistic $\\hat{V}$**: Variance estimate of $U$ based on the hypergeometric distribution.\n-   **Statistic $Z$**: Standardized statistic, $Z = U / \\sqrt{\\hat{V}}$, approximated by a standard normal distribution under $H_0$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientific Grounding**: The problem describes a classic application of survival analysis using the log-rank test, a fundamental tool in biostatistics for comparing time-to-event data between groups. The setup is scientifically and statistically sound.\n2.  **Well-Posedness**: The data are complete, the statistical method is specified, and the assumptions are clearly stated. The assumption of \"no tied event times\" is verified by the data (distinct event times are $2$, $3$, $5$, $6$, $7$, and $8$). The problem is well-posed, allowing for a unique calculation of the test statistics.\n3.  **Objectivity**: The problem is stated using objective, quantitative language, free of bias or subjective claims.\n\nThe problem statement is **valid**. It is a standard, calculable biostatistics problem. I will now proceed with the solution derivation.\n\n### Derivation of Log-Rank Test Statistics\n\nThe log-rank test involves creating a contingency table at each distinct event time to compare the observed number of events in the treatment group ($o_{1j}$) with the expected number ($e_{1j}$) under the null hypothesis.\n\nLet's organize the data by time and construct the calculation table. The distinct event times are $t_j \\in \\{2, 3, 5, 6, 7, 8\\}$.\n\n| Event Time ($t_j$) | At Risk (Trt, $n_{1j}$) | At Risk (Ctrl, $n_{2j}$) | Total At Risk ($n_j$) | Total Events ($d_j$) | Observed (Trt, $o_{1j}$) | Expected (Trt, $e_{1j}$) | Variance ($v_j$) |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| | $5$ | $5$ | $10$ | | | | |\n| $t_1=2$ | $5$ | $5$ | $10$ | $1$ | $1$ | $5 \\times \\frac{1}{10} = 0.5$ | $\\frac{5 \\times 5 \\times 1 \\times (10-1)}{10^2(10-1)} = 0.25$ |\n| | $4$ | $5$ | $9$ | | | | |\n| $t_2=3$ | $4$ | $5$ | $9$ | $1$ | $0$ | $4 \\times \\frac{1}{9} \\approx 0.444$ | $\\frac{4 \\times 5 \\times 1 \\times (9-1)}{9^2(9-1)} \\approx 0.247$ |\n| (Censor @ 4) | $4$ | $4$ | $8$ | | | | |\n| | $4$ | $3$ | $7$ | | | | |\n| $t_3=5$ | $4$ | $3$ | $7$ | $1$ | $1$ | $4 \\times \\frac{1}{7} \\approx 0.571$ | $\\frac{4 \\times 3 \\times 1 \\times (7-1)}{7^2(7-1)} \\approx 0.245$ |\n| | $3$ | $3$ | $6$ | | | | |\n| $t_4=6$ | $3$ | $3$ | $6$ | $1$ | $0$ | $3 \\times \\frac{1}{6} = 0.5$ | $\\frac{3 \\times 3 \\times 1 \\times (6-1)}{6^2(6-1)} = 0.25$ |\n| | $3$ | $2$ | $5$ | | | | |\n| $t_5=7$ | $3$ | $2$ | $5$ | $1$ | $0$ | $3 \\times \\frac{1}{5} = 0.6$ | $\\frac{3 \\times 2 \\times 1 \\times (5-1)}{5^2(5-1)} = 0.24$ |\n| (Censor @ 7.5) | $3$ | $1$ | $4$ | | | | |\n| | $2$ | $1$ | $3$ | | | | |\n| $t_6=8$ | $2$ | $1$ | $3$ | $1$ | $1$ | $2 \\times \\frac{1}{3} \\approx 0.667$ | $\\frac{2 \\times 1 \\times 1 \\times (3-1)}{3^2(3-1)} \\approx 0.222$ |\n| **Total** | | | | $\\sum_j d_j = 6$ | $\\sum_j o_{1j} = 3$ | $\\sum_j e_{1j} \\approx 3.282$ | $\\sum_j v_j \\approx 1.454$ |\n\n**Notes on Risk Set Calculation:**\n-   At $t_1=2$: All $10$ participants are at risk. A Treatment patient has an event.\n-   At $t_2=3$: $9$ are at risk ($5-1=4$ in Trt, $5$ in Ctrl). A Control patient has an event.\n-   At $t_3=5$: Before this time, a Control patient at $t=4$ is censored. So, at risk are $4$ in Trt and $5-1-1=3$ in Ctrl, totaling $7$. A Treatment patient has an event.\n-   At $t_4=6$: $3$ in Trt, $3$ in Ctrl are at risk. A Control patient has an event.\n-   At $t_5=7$: $3$ in Trt, $2$ in Ctrl are at risk. A Control patient has an event.\n-   At $t_6=8$: Before this time, a Treatment patient at $t=7.5$ is censored. So, at risk are $3-1=2$ in Trt and $1$ in Ctrl, totaling $3$. A Treatment patient has an event.\n\nThe variance at each step $j$ is calculated using the hypergeometric variance formula:\n$$v_j = \\frac{n_{1j} n_{2j} d_j (n_j - d_j)}{n_j^2 (n_j - 1)}$$\nSince all $d_j = 1$, this simplifies to $v_j = \\frac{n_{1j} n_{2j}}{n_j^2}$. Using this simplified formula gives the same numerical results as the full formula: $0.25$, $0.2469...$, $0.2448...$, $0.25$, $0.24$, $0.2222...$.\n\n**1. Calculate U (Observed - Expected for Treatment):**\n$$U = \\sum_{j=1}^{6} (o_{1j} - e_{1j}) = (\\sum_{j=1}^{6} o_{1j}) - (\\sum_{j=1}^{6} e_{1j})$$\n$$U = 3 - (0.5 + 0.444... + 0.571... + 0.5 + 0.6 + 0.666...)$$\n$$U = 3 - 3.2825... \\approx -0.283$$\n\n**2. Calculate $\\hat{V}$ (Variance Estimate):**\n$$\\hat{V} = \\sum_{j=1}^{6} v_j$$\n$$\\hat{V} \\approx 0.25 + 0.247 + 0.245 + 0.25 + 0.24 + 0.222$$\n$$\\hat{V} \\approx 1.454$$\n\n**3. Calculate Z (Standardized Statistic):**\n$$Z = \\frac{U}{\\sqrt{\\hat{V}}}$$\n$$Z \\approx \\frac{-0.283}{\\sqrt{1.454}} \\approx \\frac{-0.283}{1.2058} \\approx -0.235$$\n\n**4. Calculate the two-sided p-value:**\nThe p-value is the probability of observing a result as or more extreme than $Z$ under the standard normal distribution $\\mathcal{N}(0,1)$.\n$$p = 2 \\times P(\\mathcal{N}(0,1) \\le -0.235)$$\nFrom a standard normal table or calculator, $P(Z \\le -0.235) \\approx 0.407$.\n$$p \\approx 2 \\times 0.407 = 0.814$$\nThe numerical results ($U \\approx -0.283, \\hat{V} \\approx 1.454, Z \\approx -0.234, p \\approx 0.815$) are consistent with the calculations. Small rounding differences account for the minor discrepancy in Z and p-value. The values in option A are precise.\n\n### Option-by-Option Analysis\n\n**A. Report $U$, $\\hat{V}$, $Z$, and the two-sided $p$-value, and present KM curves with a risk table aligned to the distinct event times, marking censorings. Specifically: $U \\approx -0.283$, $\\hat{V} \\approx 1.454$, $Z \\approx -0.234$, two-sided $p \\approx 0.815$. State that the sign of $U$ indicates fewer events in Treatment than expected under the null at the observed event times, and that the KM plot and risk table (numbers at risk shown at the exact event times) aid interpretation. Conclude no evidence of a difference between survival curves at conventional significance levels.**\n\n-   **Calculations**: The reported values for $U$, $\\hat{V}$, $Z$, and $p$ are all correct based on our derivation.\n-   **Interpretation of U**: A negative $U$ means the observed events in the Treatment group were fewer than expected under the null hypothesis, which correctly interprets the statistic's sign.\n-   **Graphical Guidance**: Recommending Kaplan-Meier (KM) curves with censoring marks and a risk table aligned to event times represents best practice for transparent and informative data presentation.\n-   **Conclusion**: A $p$-value of $\\approx 0.815$ is far from significant at any conventional level (e.g., $\\alpha=0.05$). The conclusion of \"no evidence of a difference\" is appropriate.\n\n**Verdict**: **Correct**. This option is numerically accurate, interpretively sound, and adheres to best practices in statistical reporting and visualization.\n\n**B. Report $U \\approx -0.283$, $\\hat{V} \\approx 1.454$, and compute $Z$ as $U / \\hat{V} \\approx -0.195$ with a one-sided $p \\approx 0.423$. Present KM curves but align the risk table to evenly spaced monthly ticks rather than the event times. Conclude that the Treatment arm shows a trend toward benefit based on the one-sided result.**\n\n-   **Calculations**: The calculation of $Z$ is fundamentally flawed. It divides $U$ by the variance $\\hat{V}$ instead of the standard deviation $\\sqrt{\\hat{V}}$. $Z = U / \\sqrt{\\hat{V}}$, not $U / \\hat{V}$. This is a critical error.\n-   **Statistical Practice**: Reporting a one-sided $p$-value is generally inappropriate for a typical randomized trial unless pre-specified with strong justification. Furthermore, concluding a \"trend toward benefit\" from a $p$-value of $\\approx 0.4$ is highly misleading and poor scientific practice. Aligning a risk table to arbitrary time ticks can be less informative than aligning to actual event times for sparse data.\n\n**Verdict**: **Incorrect**. This option contains a severe calculation error and draws an unjustifiable, misleading conclusion.\n\n**C. Report $U \\approx +0.717$, $\\hat{V} \\approx 0.612$, $Z \\approx +0.918$, two-sided $p \\approx 0.359$, and declare statistical significance at the $\\alpha = 0.05$ level. Present KM curves without censoring marks and align the risk table to the median survival time of each arm.**\n\n-   **Calculations**: The reported values for $U$, $\\hat{V}$, and $Z$ are all numerically incorrect. Our detailed derivation shows completely different results.\n-   **Conclusion**: Declaring statistical significance is false; the correct $p$-value is $\\approx 0.815$.\n-   **Graphical Guidance**: Omitting censoring marks from a KM plot is poor practice as it hides crucial information. Aligning a risk table only to median survival times is not standard and less informative than showing the numbers at risk at several key time points.\n\n**Verdict**: **Incorrect**. The calculations are wrong, the conclusion is false, and the graphical advice is substandard.\n\n**D. Report only the two-sided $p \\approx 0.815$ (omit $U$, $\\hat{V}$, and $Z$), and present KM curves without a risk table, noting that the curves alone suffice for diagnostic purposes. State that the null hypothesis pertains to equal median survival times between arms, which were similar in this study.**\n\n-   **Reporting Practice**: While the $p$-value is correct, omitting the test statistics ($U$, $\\hat{V}$, $Z$) constitutes incomplete reporting. These statistics provide important information about the direction of the effect and its precision.\n-   **Graphical Guidance**: Stating that KM curves \"alone suffice\" is incorrect. A risk table is essential for proper interpretation, especially with small sample sizes where the curves can become unstable at later time points due to few subjects remaining at risk.\n-   **Statistical Theory**: The statement that the log-rank null hypothesis \"pertains to equal median survival times\" is a common but formal error. The log-rank test compares the entire hazard functions ($H_0: h_1(t) = h_2(t)$ for all $t$). While different medians often result from different hazard functions, the test is not specifically a test of medians.\n\n**Verdict**: **Incorrect**. This option promotes poor reporting practices, provides flawed guidance on graphical display, and misrepresents the formal null hypothesis of the log-rank test.", "answer": "$$\\boxed{A}$$", "id": "4923201"}]}