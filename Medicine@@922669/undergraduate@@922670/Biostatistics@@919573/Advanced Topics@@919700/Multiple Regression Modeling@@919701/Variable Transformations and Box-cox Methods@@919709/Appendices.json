{"hands_on_practices": [{"introduction": "One of the primary motivations for transforming data is to satisfy the assumption of constant variance, or homoscedasticity, a requirement for many statistical models. This exercise provides a foundational look at how this works by using a first-order Taylor expansion, a technique known as the delta method. You will formally prove why the classic square-root transformation is effective for stabilizing the variance of Poisson-distributed count data, where the variance is inherently equal to the mean [@problem_id:4965046].", "problem": "A biostatistician models event counts per subject as a Poisson random variable $Y$ with mean $ \\mu > 0 $, so that $ \\mathbb{E}[Y] = \\mu $ and $ \\operatorname{Var}(Y) = \\mu $. To facilitate linear modeling assumptions that require approximately constant variance across levels of $ \\mu $, the biostatistician considers the square-root transformation $ g(Y) = \\sqrt{Y} $, which is a special case of the Box-Cox family of transformations. Using a first-order Taylor expansion justification (the delta method), derive the approximate variance of $ g(Y) $ in terms of $ \\mu $ and determine whether this transformation stabilizes the variance, in the sense that the leading-order approximation does not depend on $ \\mu $. Provide the approximate variance as a single exact constant. No rounding is required and no units are involved.", "solution": "The problem is to derive the approximate variance of the square-root transformation of a Poisson-distributed random variable using a first-order Taylor expansion (the delta method) and to determine if this transformation is variance-stabilizing.\n\nThe problem statement has been validated and is deemed sound. It is a well-posed problem in biostatistics that is scientifically grounded and internally consistent.\n\nLet $Y$ be a random variable representing the event counts. The problem states that $Y$ follows a Poisson distribution with mean $\\mu > 0$. The key properties of the Poisson distribution are its mean and variance:\n$$ \\mathbb{E}[Y] = \\mu $$\n$$ \\operatorname{Var}(Y) = \\mu $$\nThe goal is to analyze the variance of a new random variable, $Z = g(Y)$, where the transformation is given by $g(y) = \\sqrt{y}$.\n\nWe will use the delta method, which is based on a first-order Taylor series expansion of the function $g(Y)$ around the mean of $Y$, which is $\\mu$. The expansion is:\n$$ g(Y) \\approx g(\\mu) + g'(\\mu)(Y - \\mu) $$\nHere, $g'(\\mu)$ is the first derivative of $g(y)$ with respect to $y$, evaluated at $y = \\mu$.\n\nThe variance of this linear approximation of $g(Y)$ is used to approximate the variance of $g(Y)$ itself. Using the properties of variance, where $\\operatorname{Var}(aX + b) = a^2 \\operatorname{Var}(X)$ for constants $a$ and $b$, we have:\n$$ \\operatorname{Var}(g(Y)) \\approx \\operatorname{Var}(g(\\mu) + g'(\\mu)(Y - \\mu)) $$\nSince $g(\\mu)$ and $g'(\\mu)$ are constants with respect to the random variable $Y$, the term $g(\\mu)$ does not contribute to the variance. Thus,\n$$ \\operatorname{Var}(g(Y)) \\approx \\operatorname{Var}(g'(\\mu)(Y - \\mu)) = [g'(\\mu)]^2 \\operatorname{Var}(Y - \\mu) $$\nFurthermore, since $\\operatorname{Var}(Y - c) = \\operatorname{Var}(Y)$ for any constant $c$, we arrive at the general formula for the delta method approximation of variance:\n$$ \\operatorname{Var}(g(Y)) \\approx [g'(\\mu)]^2 \\operatorname{Var}(Y) $$\n\nNow, we apply this formula to the specific problem.\nThe transformation is $g(y) = \\sqrt{y} = y^{1/2}$.\nThe first derivative of $g(y)$ is:\n$$ g'(y) = \\frac{d}{dy}(y^{1/2}) = \\frac{1}{2} y^{-1/2} = \\frac{1}{2\\sqrt{y}} $$\nWe evaluate this derivative at the mean of $Y$, $y = \\mu$:\n$$ g'(\\mu) = \\frac{1}{2\\sqrt{\\mu}} $$\nWe are given that for the Poisson distribution, $\\operatorname{Var}(Y) = \\mu$. Substituting these components into the delta method formula:\n$$ \\operatorname{Var}(\\sqrt{Y}) \\approx [g'(\\mu)]^2 \\operatorname{Var}(Y) = \\left( \\frac{1}{2\\sqrt{\\mu}} \\right)^2 \\cdot \\mu $$\nNow, we simplify the expression:\n$$ \\operatorname{Var}(\\sqrt{Y}) \\approx \\left( \\frac{1}{4\\mu} \\right) \\cdot \\mu $$\n$$ \\operatorname{Var}(\\sqrt{Y}) \\approx \\frac{\\mu}{4\\mu} = \\frac{1}{4} $$\n\nThe result of this first-order approximation is that the variance of the transformed variable $\\sqrt{Y}$ is approximately $\\frac{1}{4}$.\nA transformation is said to be variance-stabilizing if the variance of the transformed variable is approximately constant, i.e., it does not depend on the mean $\\mu$ of the original variable. In this case, the approximate variance is the constant $\\frac{1}{4}$, which is independent of $\\mu$. Therefore, the square-root transformation $g(Y) = \\sqrt{Y}$ is a variance-stabilizing transformation for the Poisson distribution, at least to the first order of approximation provided by the delta method.\n\nThe problem asks for the approximate variance as a single exact constant. This value is $\\frac{1}{4}$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "4965046"}, {"introduction": "While standard transformations like the square root or logarithm are useful, the Box-Cox method provides a powerful, data-driven approach to find the optimal power transformation parameter, $\\lambda$. This practice guides you through the cornerstone of this method: maximizing the profile log-likelihood function. You will derive this function from first principles and then design a computational algorithm to find the value of $\\lambda$ that makes the transformed data most closely fit the normality assumption [@problem_id:4965153].", "problem": "A biostatistician is evaluating monotone power transformations for a strictly positive response variable to improve approximate normality. For a sample of $n$ independent and identically distributed (i.i.d.) observations $\\{y_i\\}_{i=1}^n$ with $y_i \\gt 0$, consider the Box-Cox family of transformations, defined by\n$$\nz_i(\\lambda)=\n\\begin{cases}\n\\dfrac{y_i^{\\lambda}-1}{\\lambda}, & \\lambda \\neq 0, \\\\\n\\log(y_i), & \\lambda = 0,\n\\end{cases}\n$$\nand suppose that, after transformation, the working model assumes $z_i(\\lambda)$ are i.i.d. normal with mean $\\mu$ and variance $\\sigma^2$. The inferential target is the value of the transformation parameter $\\lambda$ that maximizes the profile log-likelihood under this model.\n\nYour tasks are as follows:\n\n1) Starting from the normal likelihood for transformed data, the independence assumption, and the Jacobian for the change of variables $y \\mapsto z(\\lambda)$, derive the profile log-likelihood for $\\lambda$ by eliminating the nuisance parameters $\\mu$ and $\\sigma^2$ via their maximum likelihood estimates (MLEs). Express the result as a function of $\\lambda$ and the data $\\{y_i\\}_{i=1}^n$ only, up to an additive constant that does not depend on $\\lambda$.\n\n2) Design an algorithm to maximize the derived function over $\\lambda$ using:\n   - A coarse grid search over a bounded interval to obtain an initial estimate.\n   - A refinement using the Newton-Raphson (NR) method applied to the profile log-likelihood function. For Newton-Raphson, use numerical differentiation with symmetric finite differences for the first and second derivatives at step size $h$, incorporate a backtracking line search to ensure ascent, and specify clear stopping criteria.\n\n3) Implement a complete program that:\n   - Contains no input and defines the following test suite internally. Each test case is a list of strictly positive real numbers representing a sample $\\{y_i\\}$.\n     - Test case A (log-symmetric, expected solution near $\\lambda=0$): $y = \\exp([-2,-1,0,1,2,-2,-1,0,1,2])$.\n     - Test case B (nearly symmetric on the original scale, expected solution near $\\lambda=1$): $y = [8.0,9.0,9.5,10.0,10.5,11.0,12.0,12.5,13.0,14.0]$.\n     - Test case C (highly right-skewed, wide dynamic range): $y = [0.1,0.2,0.3,0.5,1.0,2.0,4.0,8.0,16.0,32.0]$.\n     - Test case D (log-spread values, expected solution near $\\lambda=0$): $y = \\exp([-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0])$.\n   - For each test case, performs:\n     - A grid search over $\\lambda \\in [-2,2]$ with step size $0.01$ to find an initial maximizer.\n     - A Newton-Raphson refinement initialized at the grid maximizer, using symmetric finite differences with step size $h = 10^{-4}$, a maximum of $50$ iterations, a numerical tolerance of $10^{-10}$ on the parameter update, clipping $\\lambda$ to the range $[-5,5]$ after each update, and backtracking line search with step-halving until the profile log-likelihood increases.\n     - A numerically stable evaluation of the Box-Cox transform near $\\lambda = 0$ using its continuous limit.\n   - Rounds each final $\\lambda$ estimate to six digits after the decimal point.\n\n4) Final output format:\n   - Your program should produce a single line of output containing the four final $\\lambda$ estimates as a comma-separated list enclosed in square brackets (e.g., \"[x1,x2,x3,x4]\"), with each value printed as a floating-point number rounded to six digits after the decimal point.\n\nAll mathematical symbols must be interpreted in the following way: $\\lambda$ denotes the Box-Cox power parameter, $\\{y_i\\}_{i=1}^n$ are observations on the original positive scale, and $z_i(\\lambda)$ are the transformed observations. Angles are not used in this problem, and there are no physical units to report. The answers for each test case are floats. The problem is purely mathematical and algorithmic and does not require any domain-specific units or context beyond the definitions given here. Ensure all numerical computations are consistent with the standard assumptions and that the data remain strictly positive throughout.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a complete solution. It requires the derivation of the profile log-likelihood for the Box-Cox transformation parameter $\\lambda$ and its numerical maximization.\n\n### 1. Derivation of the Profile Log-Likelihood\n\nThe problem starts with a set of $n$ independent and identically distributed (i.i.d.) positive observations $\\{y_i\\}_{i=1}^n$. The Box-Cox transformation is applied to each observation:\n$$\nz_i(\\lambda) =\n\\begin{cases}\n\\dfrac{y_i^{\\lambda}-1}{\\lambda}, & \\lambda \\neq 0, \\\\\n\\log(y_i), & \\lambda = 0.\n\\end{cases}\n$$\nThe working model assumes that the transformed variables $z_i(\\lambda)$ are i.i.d. following a normal distribution $N(\\mu, \\sigma^2)$.\n\nThe probability density function (PDF) of a single transformed variable $z_i$ is given by:\n$$\nf_{Z}(z_i; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z_i - \\mu)^2}{2\\sigma^2}\\right)\n$$\nTo find the likelihood function in terms of the original data $y_i$, we perform a change of variables. The relationship is $z_i = z_i(y_i, \\lambda)$. The PDF of $Y_i$ is $f_Y(y_i) = f_Z(z_i(\\lambda)) \\cdot |J|$, where $J$ is the Jacobian of the transformation.\n\nThe derivative of $z_i$ with respect to $y_i$ is:\n$$\n\\frac{dz_i}{dy_i} = \\frac{d}{dy_i}\\left(\\frac{y_i^{\\lambda}-1}{\\lambda}\\right) = \\frac{1}{\\lambda} (\\lambda y_i^{\\lambda-1}) = y_i^{\\lambda-1}\n$$\nThis result also holds in the limit as $\\lambda \\to 0$, where $\\frac{dz_i}{dy_i} = \\frac{1}{y_i}$. Since $y_i > 0$, the Jacobian is $|J| = \\left|\\frac{dz_i}{dy_i}\\right| = y_i^{\\lambda-1}$.\n\nThe likelihood function for the entire sample $\\{y_i\\}_{i=1}^n$ is the product of the individual densities, due to the i.i.d. assumption:\n$$\nL(\\lambda, \\mu, \\sigma^2 | \\{y_i\\}) = \\prod_{i=1}^n f_Y(y_i) = \\prod_{i=1}^n \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z_i(\\lambda) - \\mu)^2}{2\\sigma^2}\\right) \\cdot y_i^{\\lambda-1} \\right]\n$$\nThe log-likelihood function, $\\ell = \\log L$, is:\n$$\n\\ell(\\lambda, \\mu, \\sigma^2) = \\sum_{i=1}^n \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(z_i(\\lambda) - \\mu)^2}{2\\sigma^2} + (\\lambda-1)\\log(y_i) \\right]\n$$\n$$\n\\ell(\\lambda, \\mu, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (z_i(\\lambda) - \\mu)^2 + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\n\nTo obtain the profile log-likelihood for $\\lambda$, we must first find the maximum likelihood estimates (MLEs) of the nuisance parameters $\\mu$ and $\\sigma^2$ for a fixed value of $\\lambda$. We differentiate $\\ell$ with respect to $\\mu$ and $\\sigma^2$ and set the results to zero.\n\nFor $\\mu$:\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (z_i(\\lambda) - \\mu) = 0 \\implies \\sum_{i=1}^n z_i(\\lambda) - n\\mu = 0\n$$\n$$\n\\hat{\\mu}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n z_i(\\lambda)\n$$\nThis is the sample mean of the transformed data.\n\nFor $\\sigma^2$:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (z_i(\\lambda) - \\mu)^2 = 0\n$$\nSubstituting $\\hat{\\mu}(\\lambda)$ for $\\mu$:\n$$\n\\hat{\\sigma}^2(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n (z_i(\\lambda) - \\hat{\\mu}(\\lambda))^2\n$$\nThis is the sample variance (with denominator $n$) of the transformed data.\n\nSubstituting these MLEs back into the log-likelihood function gives the profile log-likelihood $\\ell_p(\\lambda)$:\n$$\n\\ell_p(\\lambda) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) - \\frac{1}{2\\hat{\\sigma}^2(\\lambda)}\\sum_{i=1}^n (z_i(\\lambda) - \\hat{\\mu}(\\lambda))^2 + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\nUsing the definition of $\\hat{\\sigma}^2(\\lambda)$, the sum of squares term simplifies: $\\sum_{i=1}^n (z_i(\\lambda) - \\hat{\\mu}(\\lambda))^2 = n\\hat{\\sigma}^2(\\lambda)$.\n$$\n\\ell_p(\\lambda) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) - \\frac{n\\hat{\\sigma}^2(\\lambda)}{2\\hat{\\sigma}^2(\\lambda)} + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\n$$\n\\ell_p(\\lambda) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2} - \\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\nTo maximize $\\ell_p(\\lambda)$, we can drop the terms that are constant with respect to $\\lambda$, which are $-\\frac{n}{2}\\log(2\\pi)$ and $-\\frac{n}{2}$. Let the resulting function to be maximized be $L_{prof}(\\lambda)$:\n$$\nL_{prof}(\\lambda) = -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^n \\log(y_i)\n$$\nThis is the desired profile log-likelihood function expressed in terms of $\\lambda$ and the data $\\{y_i\\}$, up to an additive constant.\n\n### 2. Algorithm Design\n\nThe maximization of $L_{prof}(\\lambda)$ is performed in two stages as specified.\n\n**Stage 1: Coarse Grid Search**\nAn initial estimate for $\\lambda$ is found by evaluating $L_{prof}(\\lambda)$ on a discrete grid of points.\n1.  Define a grid of $\\lambda$ values over the interval $[-2, 2]$ with a step size of $0.01$.\n2.  For each $\\lambda_j$ in the grid, compute $L_{prof}(\\lambda_j)$.\n3.  The initial estimate, $\\lambda_{init}$, is the grid point that yields the maximum value of $L_{prof}(\\lambda_j)$.\n\n**Stage 2: Newton-Raphson Refinement**\nStarting from $\\lambda_{init}$, the estimate is refined using the Newton-Raphson method to find the root of the derivative of $L_{prof}(\\lambda)$. The iterative update rule is:\n$$\n\\lambda_{k+1} = \\lambda_k - \\frac{g(\\lambda_k)}{H(\\lambda_k)}\n$$\nwhere $g(\\lambda) = \\frac{d}{d\\lambda}L_{prof}(\\lambda)$ is the gradient (first derivative) and $H(\\lambda) = \\frac{d^2}{d\\lambda^2}L_{prof}(\\lambda)$ is the Hessian (second derivative).\n\n- **Numerical Derivatives**: The derivatives are computed numerically using symmetric finite differences with a step size $h = 10^{-4}$:\n  - Gradient: $g(\\lambda) \\approx \\frac{L_{prof}(\\lambda+h) - L_{prof}(\\lambda-h)}{2h}$\n  - Hessian: $H(\\lambda) \\approx \\frac{L_{prof}(\\lambda+h) - 2L_{prof}(\\lambda) + L_{prof}(\\lambda-h)}{h^2}$\n\n- **Backtracking Line Search**: A pure Newton step is not guaranteed to increase the function value, especially if far from the maximum or if the function is not locally concave. A backtracking line search is employed to ensure ascent.\n  1.  Calculate the Newton direction: $p_k = -g(\\lambda_k)/H(\\lambda_k)$. For maximization, this is an ascent direction only if $H(\\lambda_k) < 0$. If $H(\\lambda_k) \\ge 0$, the method is not applicable in its pure form; the iteration should stop, as we are not in a region of local concavity.\n  2.  Initialize a step size factor $\\alpha = 1$.\n  3.  Repeatedly halve $\\alpha$ until the ascent condition $L_{prof}(\\lambda_k + \\alpha p_k) > L_{prof}(\\lambda_k)$ is met.\n  4.  The update is then $\\lambda_{k+1} = \\lambda_k + \\alpha p_k$.\n\n- **Clipping**: After each update, the new estimate $\\lambda_{k+1}$ is clipped to the interval $[-5, 5]$ to prevent divergent steps and maintain a reasonable parameter range.\n\n- **Stopping Criteria**: The iteration terminates when either of the following conditions is met:\n  1.  The absolute change in the parameter is below a tolerance: $|\\lambda_{k+1} - \\lambda_k| < 10^{-10}$.\n  2.  A maximum of $50$ iterations is reached.\n\n- **Numerical Stability**: For values of $\\lambda$ close to zero (e.g., $|\\lambda| < 10^{-6}$), the computation of $z_i(\\lambda)$ can be numerically unstable. Using the second-order Taylor series expansion of $y_i^\\lambda = \\exp(\\lambda \\log y_i)$ avoids this issue:\n$$\nz_i(\\lambda) = \\frac{\\exp(\\lambda \\log y_i) - 1}{\\lambda} \\approx \\frac{(1 + \\lambda \\log y_i + \\frac{1}{2}(\\lambda \\log y_i)^2) - 1}{\\lambda} = \\log y_i + \\frac{\\lambda}{2}(\\log y_i)^2\n$$\nThis approximation provides a smooth and accurate transition through $\\lambda=0$.", "answer": "```python\nimport numpy as np\n\ndef box_cox_transform(y: np.ndarray, lambda_val: float, log_y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the Box-Cox transformation z(lambda) for a given y and lambda.\n    Uses a Taylor expansion for lambda close to 0 for numerical stability.\n\n    Args:\n        y: A numpy array of strictly positive data.\n        lambda_val: The Box-Cox power parameter.\n        log_y: The precomputed natural logarithm of y.\n\n    Returns:\n        A numpy array of the transformed data.\n    \"\"\"\n    if abs(lambda_val) < 1e-6:\n        # Taylor series approximation for lambda near 0\n        # (y^lambda - 1)/lambda approx log(y) + (lambda/2)*(log(y))^2\n        return log_y + 0.5 * lambda_val * log_y**2\n    else:\n        return (y**lambda_val - 1) / lambda_val\n\ndef profile_log_likelihood(lambda_val: float, y: np.ndarray, n: int, sum_log_y: float, log_y: np.ndarray) -> float:\n    \"\"\"\n    Computes the profile log-likelihood for the Box-Cox parameter lambda.\n\n    Args:\n        lambda_val: The Box-Cox power parameter to evaluate.\n        y: The original data array.\n        n: The number of observations.\n        sum_log_y: The precomputed sum of the natural logarithms of y.\n        log_y: The precomputed natural logarithm of y.\n\n    Returns:\n        The value of the profile log-likelihood (up to a constant).\n    \"\"\"\n    # 1. Transform data\n    z = box_cox_transform(y, lambda_val, log_y)\n    \n    # 2. Compute MLEs for mu and sigma^2 for the given lambda\n    mu_hat = np.mean(z)\n    # Using ddof=0 for MLE variance (denominator n)\n    sigma2_hat = np.var(z, ddof=0)\n\n    # 3. Handle case where variance is zero\n    if sigma2_hat <= 0:\n        return -np.inf # Log of non-positive number is undefined or -inf\n\n    # 4. Compute profile log-likelihood\n    # L_prof = -n/2 * log(sigma^2) + (lambda - 1) * sum(log(y_i))\n    log_lik = -0.5 * n * np.log(sigma2_hat) + (lambda_val - 1) * sum_log_y\n    \n    return log_lik\n\ndef solve():\n    \"\"\"\n    Main function to find the optimal lambda for each test case.\n    \"\"\"\n    test_cases = [\n        # Test case A (log-symmetric, expected lambda ~ 0)\n        np.exp([-2, -1, 0, 1, 2, -2, -1, 0, 1, 2]).tolist(),\n        # Test case B (nearly symmetric, expected lambda ~ 1)\n        [8.0, 9.0, 9.5, 10.0, 10.5, 11.0, 12.0, 12.5, 13.0, 14.0],\n        # Test case C (highly right-skewed, expected lambda ~ 0)\n        [0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0],\n        # Test case D (log-spread, expected lambda ~ 0)\n        np.exp([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]).tolist(),\n    ]\n\n    final_lambdas = []\n    \n    for case_data in test_cases:\n        y = np.array(case_data, dtype=float)\n        n = len(y)\n        log_y = np.log(y)\n        sum_log_y = np.sum(log_y)\n\n        # === Stage 1: Coarse Grid Search ===\n        grid_lambdas = np.arange(-2.0, 2.001, 0.01)\n        \n        # Using a lambda function to pass extra arguments easily\n        likelihood_func = lambda l: profile_log_likelihood(l, y, n, sum_log_y, log_y)\n        \n        grid_likelihoods = [likelihood_func(l) for l in grid_lambdas]\n        \n        lambda_init = grid_lambdas[np.argmax(grid_likelihoods)]\n\n        # === Stage 2: Newton-Raphson Refinement ===\n        lambda_k = lambda_init\n        h = 1e-4  # Step size for numerical differentiation\n        max_iter = 50\n        tolerance = 1e-10\n        clip_range = [-5.0, 5.0]\n\n        for _ in range(max_iter):\n            l_k = likelihood_func(lambda_k)\n            \n            # Numerical derivatives using symmetric finite differences\n            l_plus_h = likelihood_func(lambda_k + h)\n            l_minus_h = likelihood_func(lambda_k - h)\n            \n            grad = (l_plus_h - l_minus_h) / (2 * h)\n            hess = (l_plus_h - 2 * l_k + l_minus_h) / (h**2)\n\n            # Check for non-concavity: hessian must be negative for a maximum.\n            # If not, Newton's method is not guaranteed to find an ascent direction.\n            if hess >= 0 or np.isclose(hess, 0):\n                break\n\n            # Newton step\n            step = -grad / hess\n            \n            # Backtracking line search to ensure ascent\n            alpha = 1.0\n            while True:\n                lambda_next_prop = lambda_k + alpha * step\n                if likelihood_func(lambda_next_prop) > l_k:\n                    break\n                alpha /= 2.0\n                if alpha < 1e-8:  # Failsafe to prevent infinite loop\n                    alpha = 0.0\n                    break\n            \n            lambda_next = lambda_k + alpha * step\n            \n            # Clipping\n            lambda_next = np.clip(lambda_next, clip_range[0], clip_range[1])\n            \n            # Check for convergence\n            if abs(lambda_next - lambda_k) < tolerance:\n                lambda_k = lambda_next\n                break\n            \n            lambda_k = lambda_next\n\n        # Round to six decimal places for the final result\n        final_lambdas.append(round(lambda_k, 6))\n\n    # Format output as specified\n    print(f\"[{','.join(f'{l:.6f}' for l in final_lambdas)}]\")\n\nsolve()\n```", "id": "4965153"}, {"introduction": "After fitting a model on a transformed scale, we often need to convert predictions back to their original, interpretable units. A naive back-transformation, such as simply exponentiating the result of a log-linear model, leads to systematically biased predictions due to a mathematical principle known as Jensen's inequality. This exercise will have you derive the correct expression for the conditional mean on the original scale, revealing the necessary bias-correction factor and ensuring your predictions are accurate in expectation [@problem_id:4965126].", "problem": "A clinical biostatistics study models a strictly positive biomarker $Y$ (e.g., a concentration) as a function of covariates $X$ using the Box-Cox (BC) transformation, defined by\n$$\nT_{\\lambda}(y)=\\begin{cases}\n\\frac{y^{\\lambda}-1}{\\lambda}, & \\lambda\\neq 0, \\\\\n\\ln(y), & \\lambda=0.\n\\end{cases}\n$$\nSuppose model selection has chosen $\\lambda=0$, so the logarithmic transformation is used. Assume the following log-linear model holds:\n$$\n\\ln(Y)=X\\beta+\\varepsilon,\n$$\nwhere $\\varepsilon$ is independent of $X$ and $\\varepsilon\\sim \\mathcal{N}(0,\\sigma^{2})$, with $\\mathcal{N}(0,\\sigma^{2})$ denoting the Normal distribution with mean $0$ and variance $\\sigma^{2}$.\n\nStarting from the core definitions of conditional expectation and widely accepted distributional properties, derive the analytic expression for the conditional mean $E(Y\\mid X)$ on the original scale and explain why naively back-transforming the linear predictor from the log scale introduces bias. Identify the multiplicative factor that corrects this bias and express the final conditional mean compactly in terms of $X$, $\\beta$, and $\\sigma^{2}$. Provide your final answer as a single closed-form expression for $E(Y\\mid X)$. No numerical evaluation is required, and no units are necessary for the final expression.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for a unique, meaningful solution. It represents a standard and fundamental derivation in the context of log-linear models, frequently encountered in biostatistics and related quantitative fields. The problem is therefore deemed **valid** and a full solution can be derived.\n\nThe problem asks for the conditional expectation of a biomarker $Y$ on the original scale, given a set of covariates $X$. The relationship is defined by the log-linear model:\n$$\n\\ln(Y) = X\\beta + \\varepsilon\n$$\nwhere $\\varepsilon$ is a random error term that is independent of $X$ and follows a normal distribution with mean $0$ and variance $\\sigma^2$, denoted as $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nTo find the conditional expectation $E(Y \\mid X)$, we must first express $Y$ in terms of the variables on the right-hand side of the model equation. We can do this by exponentiating both sides of the equation:\n$$\nY = \\exp(\\ln(Y)) = \\exp(X\\beta + \\varepsilon)\n$$\nNow, we can apply the conditional expectation operator $E(\\cdot \\mid X)$ to $Y$:\n$$\nE(Y \\mid X) = E[\\exp(X\\beta + \\varepsilon) \\mid X]\n$$\nUsing the property of the exponential function $\\exp(a+b) = \\exp(a)\\exp(b)$, we can rewrite the expression as:\n$$\nE(Y \\mid X) = E[\\exp(X\\beta) \\cdot \\exp(\\varepsilon) \\mid X]\n$$\nWhen we condition on $X$, any function of $X$ is treated as a constant. Therefore, the term $\\exp(X\\beta)$ can be factored out of the conditional expectation:\n$$\nE(Y \\mid X) = \\exp(X\\beta) \\cdot E[\\exp(\\varepsilon) \\mid X]\n$$\nThe problem states that the error term $\\varepsilon$ is independent of the covariates $X$. A consequence of this independence is that the conditional expectation of any function of $\\varepsilon$ given $X$ is equal to the unconditional expectation of that function. That is, for any function $g(\\cdot)$:\n$$\nE[g(\\varepsilon) \\mid X] = E[g(\\varepsilon)]\n$$\nApplying this to our specific case where $g(\\varepsilon) = \\exp(\\varepsilon)$, we have:\n$$\nE[\\exp(\\varepsilon) \\mid X] = E[\\exp(\\varepsilon)]\n$$\nSubstituting this result back into our expression for $E(Y \\mid X)$ gives:\n$$\nE(Y \\mid X) = \\exp(X\\beta) \\cdot E[\\exp(\\varepsilon)]\n$$\nThe remaining task is to calculate the expectation $E[\\exp(\\varepsilon)]$. This term is recognized as the moment-generating function (MGF) of the random variable $\\varepsilon$, evaluated at the point $t=1$. Let $M_{\\varepsilon}(t)$ denote the MGF of $\\varepsilon$. By definition:\n$$\nM_{\\varepsilon}(t) = E[\\exp(t\\varepsilon)]\n$$\nSo, we need to find $M_{\\varepsilon}(1)$. For a normally distributed random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the MGF is given by the standard formula:\n$$\nM_{Z}(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nIn our problem, the error term $\\varepsilon$ follows a normal distribution with mean $\\mu=0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. Thus, its MGF is:\n$$\nM_{\\varepsilon}(t) = \\exp\\left(0 \\cdot t + \\frac{1}{2}\\sigma^2 t^2\\right) = \\exp\\left(\\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nEvaluating this MGF at $t=1$ yields the value of $E[\\exp(\\varepsilon)]$:\n$$\nE[\\exp(\\varepsilon)] = M_{\\varepsilon}(1) = \\exp\\left(\\frac{1}{2}\\sigma^2 (1)^2\\right) = \\exp\\left(\\frac{\\sigma^2}{2}\\right)\n$$\nThis term is the mean of the log-normal distribution with parameters $\\mu=0$ and $\\sigma^2$.\n\nFinally, we substitute this result back into the equation for the conditional mean of $Y$:\n$$\nE(Y \\mid X) = \\exp(X\\beta) \\cdot \\exp\\left(\\frac{\\sigma^2}{2}\\right)\n$$\nThis can be written more compactly by combining the exponential terms:\n$$\nE(Y \\mid X) = \\exp\\left(X\\beta + \\frac{\\sigma^2}{2}\\right)\n$$\nThis derivation also explains the source of bias in a naive back-transformation. On the log scale, the conditional mean of the transformed response is $E[\\ln(Y) \\mid X] = E[X\\beta + \\varepsilon \\mid X] = X\\beta + E[\\varepsilon] = X\\beta$. A naive back-transformation would simply be to exponentiate this conditional mean: $\\exp(E[\\ln(Y) \\mid X]) = \\exp(X\\beta)$. However, as we have shown, the correct conditional mean on the original scale is $E[Y \\mid X] = E[\\exp(\\ln(Y)) \\mid X] = \\exp(X\\beta + \\frac{\\sigma^2}{2})$.\n\nThe discrepancy exists because the expectation of a non-linear function is not equal to the function of the expectation. Specifically, for the strictly convex function $g(z) = \\exp(z)$, Jensen's inequality states that $E[g(Z)] \\ge g(E[Z])$, with strict inequality if $Z$ is not a constant. In our case, $Z = \\ln(Y) \\mid X$ is a random variable with non-zero variance $\\sigma^2$, so:\n$$\nE[Y \\mid X] = E[\\exp(\\ln Y) \\mid X] > \\exp(E[\\ln Y \\mid X]) = \\exp(X\\beta)\n$$\nThe naive back-transformation $\\exp(X\\beta)$ systematically underestimates the true conditional mean $E(Y \\mid X)$. The multiplicative factor required to correct for this bias is precisely the term we derived, $\\exp(\\frac{\\sigma^2}{2})$. This factor, often called a \"smearing\" estimate in non-parametric contexts or simply a log-normal correction factor, accounts for the variance of the errors on the log scale when transforming the prediction back to the original scale of $Y$.", "answer": "$$\\boxed{\\exp\\left(X\\beta + \\frac{\\sigma^2}{2}\\right)}$$", "id": "4965126"}]}