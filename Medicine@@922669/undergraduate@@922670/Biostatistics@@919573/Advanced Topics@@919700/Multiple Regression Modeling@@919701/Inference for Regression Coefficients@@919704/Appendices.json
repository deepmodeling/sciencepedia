{"hands_on_practices": [{"introduction": "Before we can test hypotheses about regression coefficients, we must first be sure that they are uniquely defined by the data. This exercise explores the issue of collinearity, where one predictor is a linear function of others, making the individual coefficients non-identifiable. By working through this problem [@problem_id:4916052], you will learn to diagnose this structural issue by examining the design matrix and, more importantly, how to resolve it by reparameterizing the model to define a set of estimable coefficients.", "problem": "A biostatistician models a continuous biomarker response as a linear function of two continuous exposures with an additive composite term. For subject index $i \\in \\{1,\\dots,n\\}$, the linear model is\n$$\nY_{i} \\;=\\; \\beta_{0} + \\beta_{A}\\,A_{i} + \\beta_{B}\\,B_{i} + \\beta_{S}\\,(A_{i}+B_{i}) + \\varepsilon_{i},\n$$\nwith error terms satisfying the standard Gaussâ€“Markov conditions for the linear model: $E(\\varepsilon_{i}\\mid A_{i},B_{i})=0$, $\\operatorname{Var}(\\varepsilon_{i}\\mid A_{i},B_{i})=\\sigma^{2}$, and $\\operatorname{Cov}(\\varepsilon_{i},\\varepsilon_{j}\\mid A_{i},B_{i},A_{j},B_{j})=0$ for $i\\ne j$. Let the design matrix be $\\mathbf{X}=[\\mathbf{1},\\mathbf{A},\\mathbf{B},\\mathbf{A}+\\mathbf{B}]$ with coefficient vector $\\boldsymbol{\\beta}=(\\beta_{0},\\beta_{A},\\beta_{B},\\beta_{S})^{\\top}$, where $\\mathbf{1}$ is the $n$-vector of ones and $\\mathbf{A},\\mathbf{B}$ are the observed exposure columns. Assume $n\\geq 3$ and that the three columns $\\mathbf{1}$, $\\mathbf{A}$, and $\\mathbf{B}$ are linearly independent in $\\mathbb{R}^{n}$.\n\nStarting from the definitions of linear models, ordinary least squares (OLS), and the notion of estimability of linear functionals of coefficients, determine the column rank of $\\mathbf{X}$, characterize the set of estimable linear combinations $c^{\\top}\\boldsymbol{\\beta}$, and propose a minimal-dimension reparameterization $\\boldsymbol{\\gamma}$ expressed as linear functions of $(\\beta_{0},\\beta_{A},\\beta_{B},\\beta_{S})$ such that the mean function is unchanged and all components of $\\boldsymbol{\\gamma}$ are identifiable.\n\nReport your final answer as a single row matrix with three entries, in the following order:\n- the rank of $\\mathbf{X}$ as a single integer,\n- a $3\\times 4$ matrix whose rows form a basis for the space of coefficient functionals that are estimable,\n- a $3\\times 1$ vector giving your proposed reparameterization $\\boldsymbol{\\gamma}$ as linear combinations of $(\\beta_{0},\\beta_{A},\\beta_{B},\\beta_{S})$.\n\nNo numerical rounding is required. Express all quantities symbolically.", "solution": "The problem requires an analysis of a given linear regression model characterized by a rank-deficient design matrix. The analysis involves determining the rank of the design matrix, characterizing the set of estimable linear functions of the model coefficients, and proposing a minimal identifiable reparameterization.\n\nThe model is given by\n$$\nY_{i} \\;=\\; \\beta_{0} + \\beta_{A}\\,A_{i} + \\beta_{B}\\,B_{i} + \\beta_{S}\\,(A_{i}+B_{i}) + \\varepsilon_{i}\n$$\nfor subjects $i \\in \\{1, \\dots, n\\}$. In matrix notation, this is $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{Y}$ is the $n \\times 1$ vector of responses, $\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{A}, \\beta_{B}, \\beta_{S})^{\\top}$ is the $4 \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the vector of errors satisfying the Gauss-Markov conditions. The $n \\times 4$ design matrix $\\mathbf{X}$ is specified by its columns:\n$$\n\\mathbf{X} = [\\mathbf{X}_1, \\mathbf{X}_2, \\mathbf{X}_3, \\mathbf{X}_4] = [\\mathbf{1}, \\mathbf{A}, \\mathbf{B}, \\mathbf{A}+\\mathbf{B}]\n$$\nwhere $\\mathbf{1}$, $\\mathbf{A}$, and $\\mathbf{B}$ are $n \\times 1$ column vectors.\n\nFirst, we determine the column rank of the design matrix $\\mathbf{X}$. The rank of a matrix is the dimension of its column space, which is equivalent to the maximum number of linearly independent columns. By inspection of the columns of $\\mathbf{X}$, we observe a linear dependency: the fourth column is the sum of the second and third columns.\n$$\n\\mathbf{X}_4 = \\mathbf{A} + \\mathbf{B} = 1 \\cdot \\mathbf{X}_2 + 1 \\cdot \\mathbf{X}_3\n$$\nThis can be written as $0 \\cdot \\mathbf{X}_1 + 1 \\cdot \\mathbf{X}_2 + 1 \\cdot \\mathbf{X}_3 - 1 \\cdot \\mathbf{X}_4 = \\mathbf{0}$, which explicitly demonstrates that the four columns of $\\mathbf{X}$ are linearly dependent. Therefore, the rank of $\\mathbf{X}$ must be less than $4$. The problem statement assumes that the three columns $\\mathbf{1}$, $\\mathbf{A}$, and $\\mathbf{B}$ are linearly independent. These are the first three columns of $\\mathbf{X}$. Since the fourth column $\\mathbf{A}+\\mathbf{B}$ is in the span of the first three columns (specifically, columns $2$ and $3$), it does not contribute to increasing the dimension of the column space. The column space of $\\mathbf{X}$ is spanned by $\\{\\mathbf{1}, \\mathbf{A}, \\mathbf{B}\\}$. As this set is assumed to be linearly independent and of size $3$, the dimension of the column space is $3$. Thus, the rank of $\\mathbf{X}$ is $3$.\n\nSecond, we characterize the set of estimable linear combinations $c^{\\top}\\boldsymbol{\\beta}$, where $c=(c_0, c_A, c_B, c_S)^{\\top}$. A linear combination of the parameters $c^{\\top}\\boldsymbol{\\beta}$ is said to be estimable if there exists a linear combination of the observations, $a^{\\top}\\mathbf{Y}$, that is an unbiased estimator for it, i.e., $E[a^{\\top}\\mathbf{Y}] = c^{\\top}\\boldsymbol{\\beta}$. A fundamental theorem in linear model theory states that $c^{\\top}\\boldsymbol{\\beta}$ is estimable if and only if the vector $c$ lies in the row space of the design matrix $\\mathbf{X}$. The row space of $\\mathbf{X}$ is the orthogonal complement of its null space, $N(\\mathbf{X})$. We proceed by finding a basis for $N(\\mathbf{X})$. A vector $v=(v_0, v_A, v_B, v_S)^{\\top}$ belongs to $N(\\mathbf{X})$ if $\\mathbf{X}v = \\mathbf{0}$.\n$$\nv_0 \\mathbf{1} + v_A \\mathbf{A} + v_B \\mathbf{B} + v_S (\\mathbf{A}+\\mathbf{B}) = \\mathbf{0}\n$$\nRearranging the terms, we get:\n$$\nv_0 \\mathbf{1} + (v_A + v_S) \\mathbf{A} + (v_B + v_S) \\mathbf{B} = \\mathbf{0}\n$$\nBecause the vectors $\\mathbf{1}$, $\\mathbf{A}$, and $\\mathbf{B}$ are linearly independent, the coefficients in this linear combination must all be zero. This yields the system of equations:\n$v_0 = 0$\n$v_A + v_S = 0 \\implies v_A = -v_S$\n$v_B + v_S = 0 \\implies v_B = -v_S$\nAny vector $v \\in N(\\mathbf{X})$ must be of the form $(0, -v_S, -v_S, v_S)^{\\top} = v_S(0, -1, -1, 1)^{\\top}$ for some scalar $v_S$. Thus, the null space $N(\\mathbf{X})$ is a one-dimensional space spanned by the vector $k = (0, -1, -1, 1)^{\\top}$.\nFor $c$ to be in the row space of $\\mathbf{X}$, it must be orthogonal to every vector in $N(\\mathbf{X})$, which means $c^{\\top}k = 0$.\n$$\nc^{\\top}k = (c_0, c_A, c_B, c_S) \\begin{pmatrix} 0 \\\\ -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = -c_A - c_B + c_S = 0\n$$\nThe condition for $c^{\\top}\\boldsymbol{\\beta}$ to be estimable is $c_S = c_A + c_B$. The set of all such vectors $c$ forms a $3$-dimensional subspace of $\\mathbb{R}^4$. We can construct a basis for this subspace. A simple choice for a basis is:\n$c^{(1)} = (1, 0, 0, 0)^{\\top}$, which satisfies $0 = 0+0$.\n$c^{(2)} = (0, 1, 0, 1)^{\\top}$, which satisfies $1 = 1+0$.\n$c^{(3)} = (0, 0, 1, 1)^{\\top}$, which satisfies $1 = 0+1$.\nThese three vectors are linearly independent and span the space of estimable coefficient vectors. A matrix whose rows form this basis is:\n$$\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n$$\n\nThird, we propose a minimal-dimension reparameterization. The original model is not identifiable because the design matrix $\\mathbf{X}$ is not of full column rank. We seek a new parameter vector $\\boldsymbol{\\gamma}$ of dimension $p=\\text{rank}(\\mathbf{X})=3$ and a new design matrix $\\mathbf{Z}$ such that $E[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{Z}\\boldsymbol{\\gamma}$ and $\\mathbf{Z}$ has full column rank. This ensures that all components of $\\boldsymbol{\\gamma}$ are identifiable (and thus estimable).\nWe examine the mean function for observation $i$:\n$$\nE[Y_i] = \\beta_0 + \\beta_A A_i + \\beta_B B_i + \\beta_S(A_i+B_i)\n$$\nBy regrouping terms by the covariates $1$, $A_i$, and $B_i$, we obtain a structure that suggests the reparameterization:\n$$\nE[Y_i] = \\beta_0 + (\\beta_A + \\beta_S) A_i + (\\beta_B + \\beta_S) B_i\n$$\nThis leads to the following choice for the new parameters:\n$\\gamma_0 = \\beta_0$\n$\\gamma_1 = \\beta_A + \\beta_S$\n$\\gamma_2 = \\beta_B + \\beta_S$\nThe new parameter vector is $\\boldsymbol{\\gamma} = (\\gamma_0, \\gamma_1, \\gamma_2)^{\\top}$. The mean model becomes $E[Y_i] = \\gamma_0 + \\gamma_1 A_i + \\gamma_2 B_i$. The corresponding design matrix is $\\mathbf{Z} = [\\mathbf{1}, \\mathbf{A}, \\mathbf{B}]$. By assumption, the columns of $\\mathbf{Z}$ are linearly independent, so $\\mathbf{Z}$ has rank $3$. Since $\\mathbf{Z}$ is an $n \\times 3$ matrix of rank $3$, it has full column rank. The dimension of $\\boldsymbol{\\gamma}$ is $3$, which is the minimal possible dimension for an identifiable reparameterization. The components of $\\boldsymbol{\\gamma}$ are the specific estimable functions that form a natural basis. The vector of new parameters is expressed as a linear transformation of the original parameters $\\boldsymbol{\\beta}$ as:\n$$\n\\boldsymbol{\\gamma} = \\begin{pmatrix} \\beta_{0} \\\\ \\beta_{A} + \\beta_{S} \\\\ \\beta_{B} + \\beta_{S} \\end{pmatrix}\n$$\nThis constitutes a valid reparameterization satisfying all requirements.", "answer": "$$\n\\boxed{\\begin{pmatrix} 3 & \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 1 \\end{pmatrix} & \\begin{pmatrix} \\beta_{0} \\\\ \\beta_{A} + \\beta_{S} \\\\ \\beta_{B} + \\beta_{S} \\end{pmatrix} \\end{pmatrix}}\n$$", "id": "4916052"}, {"introduction": "Once we have a well-defined model with identifiable parameters, we can proceed to ask scientifically meaningful questions. Often, these questions involve comparing the effects of different predictors, such as evaluating two different drug dosages. This practice [@problem_id:4916000] introduces the concept of a linear contrast, a powerful and general framework for translating such comparisons into a formal hypothesis test on the regression coefficients.", "problem": "A randomized clinical study investigates the effect of two dosing regimens of a new anti-inflammatory drug on the logarithm of C-reactive protein concentration, adjusted for baseline covariates. A multiple linear regression is fit with design matrix $X$ comprising an intercept, two binary indicators for the dosing regimens (relative to placebo), and additional covariates (age, sex, baseline body mass index, and site), yielding a parameter vector $\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\dots, \\beta_p)^{\\top}$. The regression model is $Y = X \\beta + \\varepsilon$, where $\\varepsilon$ are independent errors with mean $0$ and variance $\\sigma^{2}$, and coefficients are estimated by ordinary least squares (OLS).\n\nThe clinical question is whether the two dosing regimens have the same effect on the outcome. This can be expressed as the null hypothesis $H_{0} : \\beta_{1} = \\beta_{2}$, which corresponds to the linear contrast $c = (0, 1, -1, 0, \\dots, 0)^{\\top}$ and $H_{0} : c^{\\top} \\beta = 0$. Suppose the OLS estimates satisfy $\\hat{\\beta}_{1} = 0.5$ and $\\hat{\\beta}_{2} = 0.1$. From the fitted model, the estimated variance of the contrast is reported as $\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2}) = 0.09$.\n\nStarting from first principles of the linear model and the sampling distribution of linear contrasts of OLS coefficients, derive the appropriate test statistic for $H_{0} : \\beta_{1} = \\beta_{2}$ and then compute its numerical value using the provided quantities. Give your final numerical answer as an exact value (do not round).", "solution": "The problem asks for the derivation of a test statistic for a linear contrast of coefficients in a multiple linear regression model, and for the computation of its value.\n\nLet the multiple linear regression model be specified as:\n$$\nY = X \\beta + \\varepsilon\n$$\nwhere $Y$ is the $n \\times 1$ vector of outcomes, $X$ is the $n \\times (p+1)$ design matrix of rank $p+1$, $\\beta$ is the $(p+1) \\times 1$ vector of unknown parameters, and $\\varepsilon$ is the $n \\times 1$ vector of random errors. The problem states that the errors $\\varepsilon_i$ are independent with mean $0$ and constant variance $\\sigma^2$. For exact finite-sample inference, we make the standard assumption that the errors are normally distributed. Thus, $\\varepsilon \\sim N(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe coefficients are estimated using ordinary least squares (OLS). The OLS estimator for $\\beta$ is given by:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}Y\n$$\nSince $\\hat{\\beta}$ is a linear transformation of the normally distributed vector $Y$, $\\hat{\\beta}$ is also normally distributed. Its distribution is:\n$$\n\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X^{\\top}X)^{-1})\n$$\nThe mean $\\mathbb{E}[\\hat{\\beta}] = \\beta$ shows that $\\hat{\\beta}$ is an unbiased estimator of $\\beta$. The variance-covariance matrix is $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X^{\\top}X)^{-1}$.\n\nThe null hypothesis of interest is $H_{0} : \\beta_{1} = \\beta_{2}$, which can be written as a linear contrast $H_{0} : c^{\\top}\\beta = 0$, where the contrast vector $c$ is given by $c = (0, 1, -1, 0, \\dots, 0)^{\\top}$. The corresponding estimator for the contrast is $c^{\\top}\\hat{\\beta} = \\hat{\\beta}_{1} - \\hat{\\beta}_{2}$.\n\nAs a linear combination of the elements of $\\hat{\\beta}$, the contrast estimator $c^{\\top}\\hat{\\beta}$ is also normally distributed:\n$$\n\\mathbb{E}[c^{\\top}\\hat{\\beta}] = c^{\\top}\\mathbb{E}[\\hat{\\beta}] = c^{\\top}\\beta\n$$\n$$\n\\operatorname{Var}(c^{\\top}\\hat{\\beta}) = c^{\\top}\\operatorname{Var}(\\hat{\\beta})c = c^{\\top}(\\sigma^2 (X^{\\top}X)^{-1})c = \\sigma^2 c^{\\top}(X^{\\top}X)^{-1}c\n$$\nThus, $c^{\\top}\\hat{\\beta} \\sim N(c^{\\top}\\beta, \\sigma^2 c^{\\top}(X^{\\top}X)^{-1}c)$.\n\nUnder the null hypothesis $H_{0} : c^{\\top}\\beta = 0$, we have $\\mathbb{E}[c^{\\top}\\hat{\\beta}] = 0$. If $\\sigma^2$ were known, we could form the following standard normal statistic:\n$$\nZ = \\frac{c^{\\top}\\hat{\\beta}}{\\sqrt{\\operatorname{Var}(c^{\\top}\\hat{\\beta})}} = \\frac{c^{\\top}\\hat{\\beta}}{\\sigma\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}} \\sim N(0, 1)\n$$\nHowever, the error variance $\\sigma^2$ is typically unknown. It must be estimated from the data. The unbiased estimator for $\\sigma^2$ is the mean squared error (MSE), denoted $\\hat{\\sigma}^2$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-(p+1)} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{SSE}{n-(p+1)}\n$$\nwhere $SSE$ is the sum of squared errors and $df = n-(p+1)$ are the residual degrees of freedom. It is a fundamental result in linear model theory that the random variable $\\frac{(n-(p+1))\\hat{\\sigma}^2}{\\sigma^2} = \\frac{SSE}{\\sigma^2}$ follows a chi-squared distribution with $n-(p+1)$ degrees of freedom, i.e., $\\chi^2_{n-(p+1)}$. It is also known that $\\hat{\\sigma}^2$ is statistically independent of $\\hat{\\beta}$.\n\nTo form a test statistic that does not depend on the unknown $\\sigma^2$, we replace $\\sigma^2$ with its estimate $\\hat{\\sigma}^2$. This leads to the Student's t-statistic. The general form of a t-distributed random variable with $df$ degrees of freedom is the ratio of a standard normal random variable to the square root of an independent chi-squared random variable divided by its degrees of freedom.\n$$\nT = \\frac{Z}{\\sqrt{\\chi^2_{df}/df}} \\sim t_{df}\n$$\nIn our context, the statistic is constructed as:\n$$\nT = \\frac{\\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\sigma\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}}}{\\sqrt{\\frac{(n-(p+1))\\hat{\\sigma}^2}{\\sigma^2} / (n-(p+1))}} = \\frac{\\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\sigma\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}}}{\\sqrt{\\hat{\\sigma}^2/\\sigma^2}} = \\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\hat{\\sigma}\\sqrt{c^{\\top}(X^{\\top}X)^{-1}c}}\n$$\nThe denominator is the estimated standard error of the contrast estimator:\n$$\n\\operatorname{SE}(c^{\\top}\\hat{\\beta}) = \\sqrt{\\widehat{\\operatorname{Var}}(c^{\\top}\\hat{\\beta})} = \\sqrt{\\hat{\\sigma}^2 c^{\\top}(X^{\\top}X)^{-1}c}\n$$\nSo, the statistic $T = \\frac{c^{\\top}\\hat{\\beta} - c^{\\top}\\beta}{\\operatorname{SE}(c^{\\top}\\hat{\\beta})}$ follows a t-distribution with $n-(p+1)$ degrees of freedom.\n\nTo test the specific null hypothesis $H_{0} : c^{\\top}\\beta = 0$, we set $c^{\\top}\\beta = 0$ in the expression for $T$. The appropriate test statistic is therefore:\n$$\nt = \\frac{c^{\\top}\\hat{\\beta}}{\\operatorname{SE}(c^{\\top}\\hat{\\beta})} = \\frac{\\hat{\\beta}_{1} - \\hat{\\beta}_{2}}{\\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2})}}\n$$\nThis is the required derived test statistic.\n\nWe now compute its numerical value using the information provided in the problem statement.\nThe given quantities are:\n- Estimated coefficient for regimen 1: $\\hat{\\beta}_{1} = 0.5$\n- Estimated coefficient for regimen 2: $\\hat{\\beta}_{2} = 0.1$\n- Estimated variance of the contrast: $\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2}) = 0.09$\n\nFirst, compute the value of the estimated contrast:\n$$\n\\hat{\\beta}_{1} - \\hat{\\beta}_{2} = 0.5 - 0.1 = 0.4\n$$\nNext, compute the standard error of the contrast, which is the square root of the estimated variance:\n$$\n\\operatorname{SE}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2}) = \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\beta}_{1} - \\hat{\\beta}_{2})} = \\sqrt{0.09} = 0.3\n$$\nFinally, compute the value of the t-statistic:\n$$\nt = \\frac{0.4}{0.3} = \\frac{4/10}{3/10} = \\frac{4}{3}\n$$\nThe numerical value of the test statistic is $\\frac{4}{3}$.", "answer": "$$\\boxed{\\frac{4}{3}}$$", "id": "4916000"}, {"introduction": "Statistical models are powerful tools, but they are almost always simplifications of a more complex reality. This raises a critical question: what do our regression coefficients and hypothesis tests actually mean when our model does not perfectly match the true data-generating process? This thought-provoking exercise [@problem_id:4916046] challenges you to explore the concept of a regression coefficient as the \"best linear approximation\" and to correctly interpret the meaning of a hypothesis test in the face of model misspecification.", "problem": "A biostatistician collects independent observations of a continuous biomarker-exposure pair $(X_1,Y)$ where the data-generating mechanism satisfies $Y=\\sin(X_1)+\\varepsilon$, with $X_1\\sim \\mathcal{N}(0,1)$ and $\\varepsilon\\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X_1$. They fit the misspecified linear model $Y=\\beta_0+\\beta_1 X_1+\\text{error}$ by ordinary least squares (OLS). Interpret $\\beta_1$ as the population best linear approximation coefficient, defined as the value that minimizes the expected squared error between $Y$ and its linear predictor in $X_1$. They then consider the usual large-sample test of $H_0:\\beta_1=0$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. In this setting, the population slope equals the derivative of $\\sin(x)$ at $x=0$, so $\\beta_1^\\star=\\cos(0)=1$.\n\nB. In this setting, the population slope equals $\\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$; because $X_1\\sim \\mathcal{N}(0,1)$, this yields $\\beta_1^\\star=\\mathbb{E}[\\cos(X_1)]=e^{-1/2}$.\n\nC. A large-sample test of $H_0:\\beta_1=0$ in the misspecified linear model is a test that there is no association at all between $X_1$ and $Y=\\sin(X_1)+\\varepsilon$.\n\nD. It is possible that $H_0:\\beta_1=0$ is not rejected even if $X_1$ has a strong nonlinear effect on $Y$; the test only targets the zero linear component in the best linear approximation of $\\sin(X_1)$ by $X_1$.\n\nE. Under the stated independence and moment conditions, the OLS estimator $\\hat{\\beta}_1$ is consistent for the population projection coefficient $\\beta_1^\\star$ even though the linear model is misspecified.", "solution": "The problem asks us to evaluate several statements concerning a misspecified linear regression. The true data-generating process is $Y = \\sin(X_1) + \\varepsilon$, where $X_1 \\sim \\mathcal{N}(0, 1)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ are independent. We are fitting the model $Y = \\beta_0 + \\beta_1 X_1 + \\text{error}$ via OLS. The population coefficients $(\\beta_0^\\star, \\beta_1^\\star)$ are defined as the values that minimize the mean squared error of the linear approximation:\n$$L(\\beta_0, \\beta_1) = \\mathbb{E}[(Y - (\\beta_0 + \\beta_1 X_1))^2]$$\n\nTo find the minimum, we take the partial derivatives with respect to $\\beta_0$ and $\\beta_1$ and set them to zero. The first-order condition for $\\beta_1$ is:\n$$\\frac{\\partial L}{\\partial \\beta_1} = \\mathbb{E}[-2 X_1 (Y - \\beta_0 - \\beta_1 X_1)] = 0$$\n$$\\mathbb{E}[X_1 Y] - \\beta_0 \\mathbb{E}[X_1] - \\beta_1 \\mathbb{E}[X_1^2] = 0$$\nThe first-order condition for $\\beta_0$ gives $\\beta_0 = \\mathbb{E}[Y] - \\beta_1 \\mathbb{E}[X_1]$.\nSubstituting this into the equation for $\\beta_1$:\n$$\\mathbb{E}[X_1 Y] - (\\mathbb{E}[Y] - \\beta_1 \\mathbb{E}[X_1]) \\mathbb{E}[X_1] - \\beta_1 \\mathbb{E}[X_1^2] = 0$$\n$$\\mathbb{E}[X_1 Y] - \\mathbb{E}[X_1]\\mathbb{E}[Y] = \\beta_1 (\\mathbb{E}[X_1^2] - (\\mathbb{E}[X_1])^2)$$\nThis simplifies to the well-known formula for the population slope coefficient:\n$$\\beta_1^\\star = \\frac{\\mathrm{Cov}(X_1, Y)}{\\mathrm{Var}(X_1)}$$\n\nNow, we compute the components using the given information:\n- $\\mathrm{Var}(X_1)$: Since $X_1 \\sim \\mathcal{N}(0, 1)$, $\\mathrm{Var}(X_1) = 1$.\n- $\\mathrm{Cov}(X_1, Y)$: We substitute $Y = \\sin(X_1) + \\varepsilon$.\n$$\\mathrm{Cov}(X_1, Y) = \\mathrm{Cov}(X_1, \\sin(X_1) + \\varepsilon) = \\mathrm{Cov}(X_1, \\sin(X_1)) + \\mathrm{Cov}(X_1, \\varepsilon)$$\nSince $X_1$ and $\\varepsilon$ are independent, $\\mathrm{Cov}(X_1, \\varepsilon) = 0$.\n$$\\mathrm{Cov}(X_1, \\sin(X_1)) = \\mathbb{E}[X_1 \\sin(X_1)] - \\mathbb{E}[X_1] \\mathbb{E}[\\sin(X_1)]$$\nGiven $X_1 \\sim \\mathcal{N}(0, 1)$, we have $\\mathbb{E}[X_1] = 0$.\nAlso, the function $\\sin(x) \\cdot \\phi(x)$, where $\\phi(x)$ is the standard normal probability density function, is an odd function. Therefore, its integral over the symmetric interval $(-\\infty, \\infty)$ is zero.\n$$\\mathbb{E}[\\sin(X_1)] = \\int_{-\\infty}^{\\infty} \\sin(x) \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx = 0$$\nThus, $\\mathrm{Cov}(X_1, \\sin(X_1)) = \\mathbb{E}[X_1 \\sin(X_1)]$.\n\nPutting it all together:\n$$\\beta_1^\\star = \\frac{\\mathbb{E}[X_1 \\sin(X_1)]}{1} = \\mathbb{E}[X_1 \\sin(X_1)]$$\n\nTo calculate this expectation, we use Stein's Lemma. For a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and a differentiable function $g$ with $\\mathbb{E}|g'(X)| < \\infty$, the lemma states $\\mathbb{E}[(X-\\mu)g(X)] = \\sigma^2 \\mathbb{E}[g'(X)]$.\nIn our case, $X_1 \\sim \\mathcal{N}(0, 1)$, so $\\mu=0$ and $\\sigma^2=1$. Let $g(X_1) = \\sin(X_1)$, so $g'(X_1) = \\cos(X_1)$. The condition $\\mathbb{E}|\\cos(X_1)| < \\infty$ is met since $|\\cos(X_1)| \\le 1$.\nApplying Stein's Lemma:\n$$\\beta_1^\\star = \\mathbb{E}[X_1 \\sin(X_1)] = (1) \\cdot \\mathbb{E}[\\cos(X_1)]$$\n\nFinally, we compute $\\mathbb{E}[\\cos(X_1)]$. The characteristic function of a standard normal random variable is $\\phi_{X_1}(t) = \\mathbb{E}[e^{itX_1}] = e^{-t^2/2}$.\nBy Euler's formula, $\\mathbb{E}[e^{itX_1}] = \\mathbb{E}[\\cos(tX_1) + i\\sin(tX_1)] = \\mathbb{E}[\\cos(tX_1)] + i\\mathbb{E}[\\sin(tX_1)]$.\nSince the distribution of $X_1$ is symmetric about $0$, $\\mathbb{E}[\\sin(tX_1)]$ is $0$ for any $t$.\nTherefore, $\\mathbb{E}[\\cos(tX_1)] = e^{-t^2/2}$.\nWe need $\\mathbb{E}[\\cos(X_1)]$, which corresponds to setting $t=1$:\n$$\\mathbb{E}[\\cos(X_1)] = e^{-1^2/2} = e^{-1/2}$$\nSo, the population slope is $\\beta_1^\\star = e^{-1/2}$.\n\nWith this result, $\\beta_1^\\star = e^{-1/2} \\approx 0.6065$, we can evaluate the options.\n\n**A. In this setting, the population slope equals the derivative of $\\sin(x)$ at $x=0$, so $\\beta_1^\\star=\\cos(0)=1$.**\nThis statement proposes that the population slope is determined by a first-order Taylor expansion of the true mean function $g(x)=\\sin(x)$ around the mean of the predictor, $\\mathbb{E}[X_1]=0$. The derivative is $g'(x) = \\cos(x)$, so $g'(0) = \\cos(0) = 1$. However, the population OLS slope is a global property of the joint distribution of $(X_1, Y)$, averaging over the entire support of $X_1$, not a local property at a single point. Our calculation showed that $\\beta_1^\\star = e^{-1/2}$, which is not equal to $1$. The approximation only holds if the distribution of $X_1$ is highly concentrated around $0$.\nVerdict: **Incorrect**.\n\n**B. In this setting, the population slope equals $\\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$; because $X_1\\sim \\mathcal{N}(0,1)$, this yields $\\beta_1^\\star=\\mathbb{E}[\\cos(X_1)]=e^{-1/2}$.**\nThis statement makes three claims:\n1. $\\beta_1^\\star = \\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$: As shown in our derivation, $\\beta_1^\\star = \\mathrm{Cov}(X_1, Y)/\\mathrm{Var}(X_1)$ and since $Y = \\sin(X_1) + \\varepsilon$ with $\\varepsilon$ independent of $X_1$, this correctly simplifies to $\\mathrm{Cov}(X_1,\\sin(X_1))/\\mathrm{Var}(X_1)$.\n2. This expression equals $\\mathbb{E}[\\cos(X_1)]$: As shown using Stein's Lemma with $\\mathrm{Var}(X_1)=1$, $\\mathrm{Cov}(X_1, \\sin(X_1)) = \\mathbb{E}[X_1 \\sin(X_1)] = \\mathbb{E}[\\cos(X_1)]$. This is correct.\n3. The value is $e^{-1/2}$: As calculated using the characteristic function of the standard normal distribution, $\\mathbb{E}[\\cos(X_1)] = e^{-1/2}$. This is also correct.\nAll parts of the statement are mathematically correct and follow from our derivation.\nVerdict: **Correct**.\n\n**C. A large-sample test of $H_0:\\beta_1=0$ in the misspecified linear model is a test that there is no association at all between $X_1$ and $Y=\\sin(X_1)+\\varepsilon$.**\nThe hypothesis test for $H_0: \\beta_1=0$ assesses whether the population coefficient of the *best linear approximation* is zero. It tests for the absence of a linear component of association. \"No association at all\" between $X_1$ and $Y$ would imply that the conditional distribution of $Y$ given $X_1$ is the same as the marginal distribution of $Y$, or at least that $\\mathbb{E}[Y|X_1]$ is constant. In this problem, $\\mathbb{E}[Y|X_1] = \\mathbb{E}[\\sin(X_1) + \\varepsilon | X_1] = \\sin(X_1)$, which is a non-constant function of $X_1$. There is a clear, strong nonlinear association. The test of $H_0:\\beta_1=0$ is not a test for any and all associations; it is specific to the linear projection.\nVerdict: **Incorrect**.\n\n**D. It is possible that $H_0:\\beta_1=0$ is not rejected even if $X_1$ has a strong nonlinear effect on $Y$; the test only targets the zero linear component in the best linear approximation of $\\sin(X_1)$ by $X_1$.**\nThis statement correctly distinguishes between the overall relationship and the specific linear component targeted by OLS. First, there is indeed a strong nonlinear effect, as $\\mathbb{E}[Y|X_1] = \\sin(X_1)$. Second, a statistical test for $H_0: \\beta_1=0$ may fail to reject the null. This can happen if the null is true (i.e., $\\beta_1^\\star=0$, as in the case of $Y=X_1^2$ with $X_1 \\sim \\mathcal{N}(0,1)$), or if the null is false but the test lacks statistical power (a Type II error), which is always possible with finite samples. The statement correctly identifies that the test's scope is limited to the \"zero linear component in the best linear approximation,\" which is precisely what $\\beta_1=0$ means in this context.\nVerdict: **Correct**.\n\n**E. Under the stated independence and moment conditions, the OLS estimator $\\hat{\\beta}_1$ is consistent for the population projection coefficient $\\beta_1^\\star$ even though the linear model is misspecified.**\nThis is a standard and fundamental result in the theory of OLS. The OLS estimator for the slope is $\\hat{\\beta}_1 = \\frac{\\sum_i (X_{1i}-\\bar{X}_1)(Y_i-\\bar{Y})}{\\sum_i (X_{1i}-\\bar{X}_1)^2}$. By the Law of Large Numbers, the sample covariance in the numerator converges in probability to $\\mathrm{Cov}(X_1, Y)$, and the sample variance in the denominator converges in probability to $\\mathrm{Var}(X_1)$. Therefore, their ratio $\\hat{\\beta}_1$ converges in probability to $\\frac{\\mathrm{Cov}(X_1, Y)}{\\mathrm{Var}(X_1)}$, which is the definition of the population projection coefficient $\\beta_1^\\star$. This consistency holds as long as the necessary moments exist and the observations are independent, which is true under the problem's setup. The fact that the linear model is misspecified does not affect the consistency of the OLS estimator for its population analogue, $\\beta_1^\\star$.\nVerdict: **Correct**.", "answer": "$$\\boxed{BDE}$$", "id": "4916046"}]}