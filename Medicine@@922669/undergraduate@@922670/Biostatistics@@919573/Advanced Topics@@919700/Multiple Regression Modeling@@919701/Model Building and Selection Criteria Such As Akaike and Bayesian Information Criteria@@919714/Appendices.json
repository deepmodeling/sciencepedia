{"hands_on_practices": [{"introduction": "The first step in applying information criteria like AIC and BIC is to correctly determine the number of estimated parameters, denoted by $k$. This value is central to the penalty term that discourages overfitting, but counting parameters is not always straightforward. This practice problem [@problem_id:4928632] illustrates how to handle common modeling situations, such as sum-to-zero constraints and offsets, which affect the 'effective' number of free parameters and thus the final criterion value.", "problem": "A clinical surveillance study models counts of hospital-acquired infections across $J=5$ hospitals using a Poisson generalized linear model with a log link. For patient $i$ in hospital $j$, the mean count $ \\mu_{ij} $ is modeled via\n$$\n\\log(\\mu_{ij}) = \\alpha + \\beta X_{ij} + \\gamma_j + \\log(E_{ij}),\n$$\nwhere $X_{ij}$ is a measured risk score, $E_{ij}$ is the exposure time (a known quantity incorporated as an offset), $\\gamma_j$ is a hospital-specific effect subject to a sum-to-zero identifiability constraint $\\sum_{j=1}^{5} \\gamma_j = 0$, and $\\alpha$ is a global intercept. A second candidate model uses prior surveillance knowledge to fix the baseline rate by incorporating a known constant $r_0$ into the offset and removing the intercept:\n$$\n\\log(\\mu_{ij}) = \\beta X_{ij} + \\gamma_j + \\log(E_{ij}) + \\log(r_0),\n$$\nwith the same sum-to-zero constraint on $\\gamma_j$. Both models are fitted by maximum likelihood.\n\nLet the maximized log-likelihoods be $\\hat{\\ell}_1 = -120.3$ for the first model and $\\hat{\\ell}_2 = -119.1$ for the second model. For each model, the biostatistician must determine the appropriate parameter count $k$ that reflects the effective dimensionality of the parameter space under the stated constraint and offset treatment, and then compute the Akaike Information Criterion (AIC) to compare models.\n\nWhich option correctly identifies the effective parameter counts $k$ for both models and the corresponding AIC values?\n\nA) Model $1$: $k=6$, $\\mathrm{AIC}_1 = 252.6$; Model $2$: $k=5$, $\\mathrm{AIC}_2 = 248.2$.\n\nB) Model $1$: $k=7$ (counting the offset as a parameter), $\\mathrm{AIC}_1 = 254.6$; Model $2$: $k=6$, $\\mathrm{AIC}_2 = 250.2$.\n\nC) Model $1$: $k=5$ (sum-to-zero imposes two constraints), $\\mathrm{AIC}_1 = 250.6$; Model $2$: $k=4$, $\\mathrm{AIC}_2 = 246.2$.\n\nD) Model $1$: $k=6$, $\\mathrm{AIC}_1 = 252.6$; Model $2$: $k=6$ (intercept still counts despite being fixed in the offset), $\\mathrm{AIC}_2 = 250.2$.", "solution": "### Step 1: Extract Givens\nThe problem provides the following information:\n-   Number of hospitals: $J=5$.\n-   Model 1 (with intercept $\\alpha$):\n    $$ \\log(\\mu_{ij}) = \\alpha + \\beta X_{ij} + \\gamma_j + \\log(E_{ij}) $$\n-   Model 2 (without intercept $\\alpha$, using known constant $r_0$):\n    $$ \\log(\\mu_{ij}) = \\beta X_{ij} + \\gamma_j + \\log(E_{ij}) + \\log(r_0) $$\n-   For both models, there is a sum-to-zero identifiability constraint on the hospital-specific effects:\n    $$ \\sum_{j=1}^{5} \\gamma_j = 0 $$\n-   $E_{ij}$ is a known quantity, and $\\log(E_{ij})$ is an offset.\n-   $r_0$ is a known constant.\n-   Models are fitted by maximum likelihood.\n-   Maximized log-likelihood for Model 1: $\\hat{\\ell}_1 = -120.3$.\n-   Maximized log-likelihood for Model 2: $\\hat{\\ell}_2 = -119.1$.\n-   The task is to find the effective number of parameters, $k$, for each model and calculate the Akaike Information Criterion (AIC).\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective.\n-   **Scientific Grounding**: The problem describes a standard biostatistical analysis using Poisson generalized linear models, which is a cornerstone of count data modeling. The concepts of log-link, offsets for exposure, fixed effects with sum-to-zero constraints for identifiability, and model comparison using AIC are all fundamental and well-established in the field of statistics.\n-   **Well-Posed**: The problem provides all necessary components to arrive at a unique solution. The models are clearly specified, as are the constraints and the maximized log-likelihood values. The definition of AIC is standard. The core task, determining the effective number of parameters, is a well-defined statistical question.\n-   **Objectivity**: The problem is stated using precise, unambiguous terminology common in statistics. There are no subjective or opinion-based elements.\n\nThe problem does not violate any of the invalidity criteria. It is a valid, solvable problem in biostatistics.\n\n### Step 3: Derivation of the Solution\n\nThe Akaike Information Criterion (AIC) is defined as:\n$$ \\mathrm{AIC} = 2k - 2\\hat{\\ell} $$\nwhere $k$ is the number of estimated parameters in the model and $\\hat{\\ell}$ is the maximized value of the log-likelihood function. The key to solving this problem is to correctly determine the effective number of parameters, $k$, for each model.\n\n**Analysis of Model 1**\n\nThe first model is:\n$$ \\log(\\mu_{ij}) = \\alpha + \\beta X_{ij} + \\gamma_j + \\log(E_{ij}) $$\nThe parameters to be estimated by maximum likelihood are:\n1.  The global intercept, $\\alpha$. This is $1$ parameter.\n2.  The coefficient for the risk score, $\\beta$. This is $1$ parameter.\n3.  The hospital-specific effects, $\\gamma_1, \\gamma_2, \\gamma_3, \\gamma_4, \\gamma_5$. There are $J=5$ such terms.\n\nNaively, this would be $1 + 1 + 5 = 7$ parameters. However, these parameters are subject to the constraint $\\sum_{j=1}^{5} \\gamma_j = 0$. This is a single linear constraint that reduces the number of free parameters by $1$. If we know any $4$ of the $\\gamma_j$ values, the $5$-th is automatically determined. For instance, $\\gamma_5 = -(\\gamma_1 + \\gamma_2 + \\gamma_3 + \\gamma_4)$. Therefore, the $5$ hospital effects $\\gamma_j$ contribute only $J-1 = 5-1=4$ effective parameters to the model.\n\nThe term $\\log(E_{ij})$ is an offset. By definition, an offset is a predictor variable whose coefficient is fixed to $1$, not estimated from the data. Therefore, it does not contribute to the count of estimated parameters, $k$.\n\nThe total effective number of parameters for Model 1 is:\n$$ k_1 = (\\text{for } \\alpha) + (\\text{for } \\beta) + (\\text{for } \\gamma_j \\text{ set}) = 1 + 1 + (5-1) = 6 $$\nWith $k_1 = 6$ and the given maximized log-likelihood $\\hat{\\ell}_1 = -120.3$, the AIC for Model 1 is:\n$$ \\mathrm{AIC}_1 = 2k_1 - 2\\hat{\\ell}_1 = 2(6) - 2(-120.3) = 12 + 240.6 = 252.6 $$\n\n**Analysis of Model 2**\n\nThe second model is:\n$$ \\log(\\mu_{ij}) = \\beta X_{ij} + \\gamma_j + \\log(E_{ij}) + \\log(r_0) $$\nIn this model, the intercept $\\alpha$ has been removed. The term $\\log(r_0)$ is added, where $r_0$ is a known constant. This means $\\log(r_0)$ is not a parameter to be estimated; it is part of the offset, which is now $\\log(E_{ij}) + \\log(r_0)$.\n\nThe parameters to be estimated by maximum likelihood are:\n1.  The coefficient for the risk score, $\\beta$. This is $1$ parameter.\n2.  The hospital-specific effects, $\\gamma_1, \\ldots, \\gamma_5$, which are still subject to the constraint $\\sum_{j=1}^{5} \\gamma_j = 0$. As in Model 1, these contribute $J-1 = 4$ effective parameters.\n\nThe total effective number of parameters for Model 2 is:\n$$ k_2 = (\\text{for } \\beta) + (\\text{for } \\gamma_j \\text{ set}) = 1 + (5-1) = 5 $$\nWith $k_2 = 5$ and the given maximized log-likelihood $\\hat{\\ell}_2 = -119.1$, the AIC for Model 2 is:\n$$ \\mathrm{AIC}_2 = 2k_2 - 2\\hat{\\ell}_2 = 2(5) - 2(-119.1) = 10 + 238.2 = 248.2 $$\n\n**Summary of Results:**\n-   For Model 1: $k_1 = 6$, $\\mathrm{AIC}_1 = 252.6$.\n-   For Model 2: $k_2 = 5$, $\\mathrm{AIC}_2 = 248.2$.\n\n### Option-by-Option Analysis\n\n**A) Model $1$: $k=6$, $\\mathrm{AIC}_1 = 252.6$; Model $2$: $k=5$, $\\mathrm{AIC}_2 = 248.2$.**\nThis option perfectly matches our derived results for both models.\n-   Model 1: $k_1 = 1(\\alpha) + 1(\\beta) + (5-1)(\\gamma_j) = 6$. $\\mathrm{AIC}_1 = 2(6) - 2(-120.3) = 252.6$.\n-   Model 2: $k_2 = 1(\\beta) + (5-1)(\\gamma_j) = 5$. $\\mathrm{AIC}_2 = 2(5) - 2(-119.1) = 248.2$.\n**Verdict: Correct.**\n\n**B) Model $1$: $k=7$ (counting the offset as a parameter), $\\mathrm{AIC}_1 = 254.6$; Model $2$: $k=6$, $\\mathrm{AIC}_2 = 250.2$.**\nThis option is based on a misunderstanding of what constitutes an estimated parameter. An offset is a known quantity with a fixed coefficient of $1$; it is not estimated and therefore does not contribute to $k$. This option also incorrectly counts the number of parameters for the $\\gamma_j$ set by ignoring the sum-to-zero constraint. For Model 1, it appears to count $k_1 = 1(\\alpha) + 1(\\beta) + 5(\\gamma_j) = 7$. For Model 2, it counts $k_2 = 1(\\beta) + 5(\\gamma_j) = 6$. Both counts are incorrect.\n**Verdict: Incorrect.**\n\n**C) Model $1$: $k=5$ (sum-to-zero imposes two constraints), $\\mathrm{AIC}_1 = 250.6$; Model $2$: $k=4$, $\\mathrm{AIC}_2 = 246.2$.**\nThis option incorrectly states that the sum-to-zero constraint imposes two constraints. The equation $\\sum_{j=1}^{5} \\gamma_j = 0$ is a single linear constraint, which reduces the degrees of freedom by $1$, not $2$. The parameter counts of $k_1=5$ and $k_2=4$ are therefore incorrect.\n**Verdict: Incorrect.**\n\n**D) Model $1$: $k=6$, $\\mathrm{AIC}_1 = 252.6$; Model $2$: $k=6$ (intercept still counts despite being fixed in the offset), $\\mathrm{AIC}_2 = 250.2$.**\nThis option correctly analyzes Model 1. However, its analysis of Model 2 is flawed. The reasoning \"intercept still counts despite being fixed in the offset\" is nonsensical. In Model 2, the parameter $\\alpha$ is removed and replaced by a known constant $\\log(r_0)$. A known constant is not an estimated parameter and does not contribute to $k$. The correct parameter count for Model 2 is $k_2=5$, not $k_2=6$.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "4928632"}, {"introduction": "Once we can confidently count parameters, we can compare models using different criteria. The two most prominent tools, the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), appear similar but are derived from different philosophies and have different goals. This exercise [@problem_id:4815010] guides you through calculating both for a pair of nested models, forcing a direct comparison and revealing how their different penalties can lead to different conclusions about the balance between model fit and complexity.", "problem": "A biobank cohort study investigates time-to-cardiovascular-event outcomes using the Cox proportional hazards model. Two nested models are fit on the same dataset with $n=600$ observed events. Model $\\mathcal{M}_1$ includes $k_1=8$ covariates; model $\\mathcal{M}_2$ augments $\\mathcal{M}_1$ with $4$ additional covariates for a total of $k_2=12$. The maximized log partial likelihoods are $\\ell_{p,1}=-420.5$ and $\\ell_{p,2}=-418.0$. Assume the usual asymptotic regularity conditions for maximum likelihood estimation and model selection hold.\n\nStarting from first principles:\n- Use the definition of Kullback–Leibler (KL) divergence as a measure of expected out-of-sample predictive discrepancy and the classical bias-correction argument to derive the large-sample criterion that targets minimizing expected predictive loss among likelihood-based models.\n- Use the Bayesian marginal likelihood (model evidence) with a regular prior and a Laplace approximation to derive the large-sample criterion that targets model identification by approximating the log evidence up to an additive constant common across models.\n\nIn the Cox setting, take the effective sample size for the partial likelihood to be the number of events, so use $n=600$ in any sample size dependent term, and take $k$ to be the number of regression coefficients in the linear predictor (excluding the baseline hazard). Compute both criteria for $\\mathcal{M}_1$ and $\\mathcal{M}_2$ using their maximized log partial likelihoods and parameter counts. Then, state which model each criterion prefers and explain, in terms of the goals of prediction versus identification, how to interpret situations in which the two criteria would disagree. \n\nReport your numerical results as the row vector $(\\mathrm{AIC}_1,\\mathrm{AIC}_2,\\mathrm{BIC}_1,\\mathrm{BIC}_2)$, rounding to $4$ significant figures.", "solution": "The problem requires the derivation of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) from first principles, their application to two nested Cox proportional hazards models, and an interpretation of their respective goals. The problem is well-posed and scientifically grounded.\n\nThe givens for the problem are:\n- Effective sample size (number of events): $n=600$.\n- Model $\\mathcal{M}_1$: Number of covariates $k_1=8$.\n- Model $\\mathcal{M}_2$: Number of covariates $k_2=12$.\n- Maximized log partial likelihood for $\\mathcal{M}_1$: $\\ell_{p,1} = -420.5$.\n- Maximized log partial likelihood for $\\mathcal{M}_2$: $\\ell_{p,2} = -418.0$.\n\n**Derivation of the Akaike Information Criterion (AIC)**\n\nThe goal of AIC is to select a model that provides the best predictive performance for new data. This is framed as finding the model that minimizes the expected information loss, as measured by the Kullback-Leibler (KL) divergence, between the true data-generating distribution and the fitted model.\n\nLet the true, unknown probability distribution of the data be $f(x)$. Let $g(x|\\theta)$ be a family of models parameterized by a vector $\\theta$ of dimension $k$. The KL divergence from $g$ to $f$ is defined as:\n$$ D_{KL}(f || g(\\cdot|\\theta)) = \\int f(x) \\ln\\left(\\frac{f(x)}{g(x|\\theta)}\\right) dx = \\mathbb{E}_{f}[\\ln f(x)] - \\mathbb{E}_{f}[\\ln g(x|\\theta)] $$\nSince $\\mathbb{E}_{f}[\\ln f(x)]$ is constant with respect to the model $g$, minimizing the KL divergence is equivalent to maximizing the expected log-likelihood of the model, $\\mathbb{E}_{f}[\\ln g(x|\\theta)]$.\n\nLet $\\hat{\\theta}_{obs}$ be the maximum likelihood estimate (MLE) of $\\theta$ obtained from an observed dataset $y_{obs}$ of size $n$. The quantity we wish to estimate is the expected predictive accuracy of this fitted model on a new, independent dataset $y_{new}$ drawn from the same true distribution $f$. This is $\\mathbb{E}_{y_{new}}[\\ln g(y_{new}|\\hat{\\theta}_{obs})]$.\n\nThe in-sample maximized log-likelihood, $\\ell(\\hat{\\theta}_{obs}) = \\ln g(y_{obs}|\\hat{\\theta}_{obs})$, is an optimistically biased estimate of this target quantity. The bias-correction argument, central to AIC, quantifies this optimism. Under standard regularity conditions, for large $n$, Hirotugu Akaike showed that:\n$$ \\mathbb{E}_{y_{obs}} \\left[ \\mathbb{E}_{y_{new}}[\\ln g(y_{new}|\\hat{\\theta}_{obs})] - \\ell(\\hat{\\theta}_{obs}) \\right] \\approx -k $$\nThe term on the left is the expected optimism of the in-sample log-likelihood as an estimator of the out-of-sample predictive log-likelihood. The expectation $\\mathbb{E}_{y_{obs}}$ is over all possible training sets of size $n$. This suggests that for a single realization of the data, an approximately unbiased estimator for the target predictive quantity $\\mathbb{E}_{y_{new}}[\\ln g(y_{new}|\\hat{\\theta}_{obs})]$ is the bias-corrected log-likelihood:\n$$ \\ell(\\hat{\\theta}_{obs}) - k $$\nAIC is defined by multiplying this quantity by $-2$. The multiplication by $-2$ places AIC on the deviance scale, so lower values indicate a better model fit relative to complexity.\n$$ \\mathrm{AIC} = -2 (\\ell(\\hat{\\theta}_{obs}) - k) = -2\\ell(\\hat{\\theta}_{obs}) + 2k $$\nIn the context of the Cox model, the log-likelihood is replaced by the maximized log partial likelihood, $\\ell_p$.\n\n**Derivation of the Bayesian Information Criterion (BIC)**\n\nThe goal of BIC is to identify the \"true\" data-generating model from a set of candidates, based on its posterior probability. By Bayes' theorem, the posterior probability of a model $\\mathcal{M}_i$ given data $y$ is:\n$$ P(\\mathcal{M}_i|y) = \\frac{P(y|\\mathcal{M}_i) P(\\mathcal{M}_i)}{\\sum_j P(y|\\mathcal{M}_j) P(\\mathcal{M}_j)} $$\nAssuming equal prior probabilities for all models ($P(\\mathcal{M}_i) = P(\\mathcal{M}_j)$), selecting the model with the highest posterior probability is equivalent to selecting the model with the highest marginal likelihood or \"model evidence,\" $P(y|\\mathcal{M})$. The BIC is a large-sample approximation to $-2 \\ln P(y|\\mathcal{M})$.\n\nThe marginal likelihood is obtained by integrating the likelihood over the prior distribution of the parameters $\\theta$ (of dimension $k$) for a given model $\\mathcal{M}$:\n$$ P(y|\\mathcal{M}) = \\int P(y|\\theta, \\mathcal{M}) P(\\theta|\\mathcal{M}) d\\theta = \\int \\exp(\\ln(P(y|\\theta, \\mathcal{M})P(\\theta|\\mathcal{M}))) d\\theta $$\nLet $q(\\theta) = \\ln(P(y|\\theta, \\mathcal{M})P(\\theta|\\mathcal{M})) = \\ell(\\theta|y) + \\ln P(\\theta|\\mathcal{M})$, where $\\ell(\\theta|y)$ is the log-likelihood. We use the Laplace approximation to evaluate this integral for a large sample size $n$. We expand $q(\\theta)$ in a Taylor series around its mode, which for large $n$ is approximated by the MLE $\\hat{\\theta}$:\n$$ q(\\theta) \\approx q(\\hat{\\theta}) + (\\theta - \\hat{\\theta})^T \\nabla q(\\hat{\\theta}) + \\frac{1}{2}(\\theta - \\hat{\\theta})^T \\nabla^2 q(\\hat{\\theta}) (\\theta - \\hat{\\theta}) $$\nAt the MLE $\\hat{\\theta}$, the gradient of the log-likelihood is zero. Assuming a regular prior where its influence is small, $\\nabla q(\\hat{\\theta}) \\approx 0$. The expansion simplifies to:\n$$ q(\\theta) \\approx q(\\hat{\\theta}) - \\frac{1}{2}(\\theta - \\hat{\\theta})^T H (\\theta - \\hat{\\theta}) $$\nwhere $H = -\\nabla^2 q(\\hat{\\theta})$ is the negative of the Hessian matrix evaluated at the mode. For large $n$, $H$ is well-approximated by the observed Fisher information matrix, $I(\\hat{\\theta})$.\nThe integral becomes:\n$$ P(y|\\mathcal{M}) \\approx \\exp(q(\\hat{\\theta})) \\int \\exp\\left(-\\frac{1}{2}(\\theta - \\hat{\\theta})^T H (\\theta - \\hat{\\theta})\\right) d\\theta $$\nThe integral is the normalizing constant of a multivariate Gaussian distribution, which is $(2\\pi)^{k/2} (\\det H)^{-1/2}$. Thus:\n$$ P(y|\\mathcal{M}) \\approx P(y|\\hat{\\theta}, \\mathcal{M}) P(\\hat{\\theta}|\\mathcal{M}) (2\\pi)^{k/2} (\\det H)^{-1/2} $$\nTaking the logarithm:\n$$ \\ln P(y|\\mathcal{M}) \\approx \\ell(\\hat{\\theta}) + \\ln P(\\hat{\\theta}|\\mathcal{M}) + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det H) $$\nFor large $n$, the Fisher information matrix $I(\\hat{\\theta})$ has elements that are $O(n)$, so its determinant $\\det H \\approx \\det I(\\hat{\\theta})$ is $O(n^k)$. Therefore, $\\ln(\\det H) \\approx k \\ln(n) + O(1)$. The prior term $\\ln P(\\hat{\\theta}|\\mathcal{M})$ and the $\\ln(2\\pi)$ term are $O(1)$. Dropping all terms that do not grow with $n$ (or grow slower than $\\ln(n)$), we get the approximation:\n$$ \\ln P(y|\\mathcal{M}) \\approx \\ell(\\hat{\\theta}) - \\frac{k}{2}\\ln(n) $$\nBIC is defined by multiplying this by $-2$:\n$$ \\mathrm{BIC} = -2\\ell(\\hat{\\theta}) + k\\ln(n) $$\nAgain, for the Cox model, $\\ell(\\hat{\\theta})$ is replaced by $\\ell_p$.\n\n**Calculations for Models $\\mathcal{M}_1$ and $\\mathcal{M}_2$**\n\nWe are given $n=600$, $k_1=8$, $\\ell_{p,1}=-420.5$, $k_2=12$, and $\\ell_{p,2}=-418.0$.\nFirst, we compute $\\ln(n) = \\ln(600) \\approx 6.3969$.\n\nFor Model $\\mathcal{M}_1$:\n$$ \\mathrm{AIC}_1 = -2\\ell_{p,1} + 2k_1 = -2(-420.5) + 2(8) = 841.0 + 16 = 857.0 $$\n$$ \\mathrm{BIC}_1 = -2\\ell_{p,1} + k_1\\ln(n) = -2(-420.5) + 8\\ln(600) \\approx 841.0 + 8(6.3969) \\approx 841.0 + 51.1752 = 892.1752 $$\n\nFor Model $\\mathcal{M}_2$:\n$$ \\mathrm{AIC}_2 = -2\\ell_{p,2} + 2k_2 = -2(-418.0) + 2(12) = 836.0 + 24 = 860.0 $$\n$$ \\mathrm{BIC}_2 = -2\\ell_{p,2} + k_2\\ln(n) = -2(-418.0) + 12\\ln(600) \\approx 836.0 + 12(6.3969) \\approx 836.0 + 76.7628 = 912.7628 $$\n\nRounding the results to $4$ significant figures, we have:\n$\\mathrm{AIC}_1 = 857.0$\n$\\mathrm{AIC}_2 = 860.0$\n$\\mathrm{BIC}_1 = 892.2$\n$\\mathrm{BIC}_2 = 912.8$\n\n**Model Preference and Interpretation**\n\nFor both AIC and BIC, lower values indicate a preferred model.\n- **AIC Preference**: Since $\\mathrm{AIC}_1=857.0  \\mathrm{AIC}_2=860.0$, the AIC criterion prefers the simpler model, $\\mathcal{M}_1$.\n- **BIC Preference**: Since $\\mathrm{BIC}_1=892.2  \\mathrm{BIC}_2=912.8$, the BIC criterion also prefers the simpler model, $\\mathcal{M}_1$.\n\nIn this instance, both criteria agree. However, the problem asks to explain how to interpret situations where they might disagree. A disagreement would typically occur when AIC favors a more complex model while BIC favors a simpler one. This arises because BIC's penalty term, $k\\ln(n)$, is larger than AIC's penalty term, $2k$, for any sample size $n > e^2 \\approx 7.39$. In this problem, $\\ln(600) \\approx 6.4$, so BIC penalizes complexity more than three times as harshly as AIC.\n\n- **Interpretation of AIC's Goal (Prediction)**: AIC aims to select a model that will have the best predictive accuracy on future data. It is asymptotically efficient, meaning that if the true underlying reality is of infinite dimension (i.e., no simple finite-parameter model is perfectly \"true\"), AIC tends to select models that provide the best finite-dimensional approximation. It may retain a parameter in a model if it offers a slight but real improvement in predictive power, even if that parameter is not part of a hypothetical \"true\" generating model.\n\n- **Interpretation of BIC's Goal (Identification)**: BIC aims to identify the \"true\" model among the set of candidates. It is consistent, meaning that as the sample size $n \\to \\infty$, the probability of BIC selecting the true model (if it is in the candidate set) approaches $1$. Its stronger penalty reflects a higher burden of proof: there must be substantial evidence in the data (a large increase in likelihood) to justify adding more parameters and concluding that the more complex model is the true one.\n\n- **Interpreting Disagreement**: If AIC had selected $\\mathcal{M}_2$ and BIC selected $\\mathcal{M}_1$, it would imply that the $4$ additional covariates in $\\mathcal{M}_2$ improve the log-likelihood enough to be considered useful for prediction (according to AIC), but not enough to provide strong evidence that $\\mathcal{M}_2$ is a better representation of the true underlying data-generating process (according to BIC). The choice between them would depend on the research goal: if building the best possible predictive risk score is the objective, the AIC-preferred model might be chosen. If the goal is a more parsimonious, explanatory model that identifies only the most robustly supported risk factors, the BIC-preferred model would be the choice.", "answer": "$$ \\boxed{ \\begin{pmatrix} 857.0  860.0  892.2  912.8 \\end{pmatrix} } $$", "id": "4815010"}, {"introduction": "Model selection criteria are powerful but not universally applicable in their standard forms; they rely on assumptions that must be checked. The standard AIC, for instance, is derived under the assumption of a large sample size. When this assumption is not met, AIC can favor overly complex models, and a small-sample correction is necessary. This hands-on practice [@problem_id:4928685] introduces the corrected AIC (AICc) and has you calculate the additional penalty it imposes, highlighting its importance in studies where data is limited.", "problem": "A biostatistician fits a generalized linear model (GLM) with a logistic link to binary outcomes from a study of independent subjects. The sample size is $n=60$. The model includes $k=10$ unknown parameters (including the intercept). The maximized log-likelihood of the fitted model is $\\hat{\\ell}=-36.7$. Using core principles connecting model selection to expected Kullback–Leibler divergence, determine the model’s Akaike Information Criterion (AIC) and the corrected Akaike Information Criterion (AICc). Then quantify the difference in the penalty terms implied by AICc versus AIC for this model size and sample size. Report this penalty difference as your final answer, rounded to four significant figures.", "solution": "The problem requires the calculation of the difference between the penalty terms of the corrected Akaike Information Criterion (AICc) and the standard Akaike Information Criterion (AIC) for a given generalized linear model.\n\nFirst, let us define the AIC. The Akaike Information Criterion is a widely used metric for model selection. It is founded on the principle of estimating the expected, relative Kullback–Leibler (KL) divergence between the fitted model and the unknown true data-generating process. The formula for AIC is given by:\n$$\nAIC = -2 \\hat{\\ell} + 2k\n$$\nwhere $\\hat{\\ell}$ is the maximized value of the log-likelihood function for the model, and $k$ is the number of estimated parameters in the model. The term $-2 \\hat{\\ell}$ is a measure of the goodness-of-fit, while the term $2k$ is the penalty for model complexity. Thus, the penalty term for AIC is:\n$$\nP_{AIC} = 2k\n$$\n\nThe AIC is an asymptotically unbiased estimator of the expected KL divergence. However, for small sample sizes, its performance can be suboptimal. The corrected Akaike Information Criterion, AICc, adjusts for this small-sample bias by introducing a more substantial penalty term. The formula for AICc is:\n$$\nAICc = AIC + \\frac{2k(k+1)}{n-k-1}\n$$\nwhere $n$ is the sample size. By substituting the expression for AIC, we can write AICc as:\n$$\nAICc = -2 \\hat{\\ell} + 2k + \\frac{2k(k+1)}{n-k-1}\n$$\nFrom this expression, we can identify the total penalty term for AICc as the quantity added to the goodness-of-fit term $-2 \\hat{\\ell}$:\n$$\nP_{AICc} = 2k + \\frac{2k(k+1)}{n-k-1}\n$$\nThis correction term becomes more influential when the ratio of the sample size to the number of parameters, $n/k$, is small (a common rule of thumb is when $n/k  40$).\n\nThe problem asks for the difference in the penalty terms implied by AICc versus AIC. This difference, which we denote as $\\Delta P$, is:\n$$\n\\Delta P = P_{AICc} - P_{AIC}\n$$\nSubstituting the expressions for the penalty terms:\n$$\n\\Delta P = \\left( 2k + \\frac{2k(k+1)}{n-k-1} \\right) - (2k)\n$$\nThis simplifies to the correction term itself:\n$$\n\\Delta P = \\frac{2k(k+1)}{n-k-1}\n$$\n\nThe problem provides the following values:\n- Sample size: $n = 60$\n- Number of unknown parameters: $k = 10$\n- Maximized log-likelihood: $\\hat{\\ell} = -36.7$\n\nNote that the value of the maximized log-likelihood, $\\hat{\\ell}$, is not required to calculate the difference in the penalty terms, as this difference depends only on $n$ and $k$.\n\nWe can now substitute the given values of $n$ and $k$ into the expression for $\\Delta P$:\n$$\n\\Delta P = \\frac{2(10)(10+1)}{60-10-1}\n$$\n$$\n\\Delta P = \\frac{2(10)(11)}{49}\n$$\n$$\n\\Delta P = \\frac{220}{49}\n$$\nNow, we compute the numerical value:\n$$\n\\Delta P \\approx 4.489795918...\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant digits are $4$, $4$, $8$, and $9$. The fifth digit is $7$, which is greater than or equal to $5$, so we round up the fourth significant digit. Rounding $9$ up results in $0$ and carrying over $1$ to the previous digit.\n$$\n\\Delta P \\approx 4.490\n$$\nThe trailing zero is significant and must be included to indicate the precision of four significant figures. This value represents the additional penalty imposed by AICc compared to AIC for a model with $k=10$ parameters and a sample size of $n=60$.", "answer": "$$\n\\boxed{4.490}\n$$", "id": "4928685"}]}