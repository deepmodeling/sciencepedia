{"hands_on_practices": [{"introduction": "To begin, we'll practice the fundamental calculation of the Variance Inflation Factor (VIF), the primary tool for diagnosing multicollinearity. This exercise [@problem_id:1938207] grounds the abstract concept of VIF in a common scenario involving two correlated predictors. By connecting VIF directly to the simple correlation coefficient, you will build a solid intuition for how this essential diagnostic measure works.", "problem": "An econometrician is developing a linear regression model to predict the annual profit of a company. Among the potential predictors, two variables are of particular concern: $X_1$, the company's annual marketing expenditure, and $X_2$, the size of its sales team. The econometrician suspects that these two variables might be highly correlated, which could inflate the variance of the estimated regression coefficients. To investigate this, they compute the sample correlation coefficient between the marketing expenditure and the sales team size from their dataset, finding it to be $r = 0.960$.\n\nTo quantify the severity of this multicollinearity, calculate the Variance Inflation Factor (VIF) for either of these predictors. The VIF is a measure of how much the variance of an estimated regression coefficient is increased because of collinearity. Express your answer as a numerical value rounded to three significant figures.", "solution": "The variance inflation factor for the coefficient of predictor $X_{j}$ is defined as $\\mathrm{VIF}_{j}=\\frac{1}{1-R_{j}^{2}}$, where $R_{j}^{2}$ is the coefficient of determination from regressing $X_{j}$ on all other predictors. With only two predictors $X_{1}$ and $X_{2}$, the regression of one on the other has $R_{1}^{2}=R_{2}^{2}=r^{2}$, where $r$ is the sample correlation between $X_{1}$ and $X_{2}$. Given $r=0.960$, we have\n$$\nR_{j}^{2}=r^{2}=(0.960)^{2}=0.9216,\n$$\nso\n$$\n\\mathrm{VIF}=\\frac{1}{1-R_{j}^{2}}=\\frac{1}{1-0.9216}=\\frac{1}{0.0784}=\\frac{625}{49}\\approx 12.7551020408.\n$$\nRounding to three significant figures gives $12.8$.", "answer": "$$\\boxed{12.8}$$", "id": "1938207"}, {"introduction": "With the basic calculation mastered, we now explore one of the most confusing consequences of severe multicollinearity: paradoxical coefficient signs. This practice [@problem_id:1938238] presents a hypothetical but realistic scenario where a predictor known to have a positive effect on an outcome surprisingly shows a negative coefficient in a multiple regression model. Calculating this effect firsthand will solidify your understanding of how collinearity can distort model interpretation.", "problem": "An agricultural scientist is studying the yield of a new variety of corn. They conduct an experiment with varying levels of two different liquid fertilizers, \"Gro-Fast\" (applied in mL per plant) and \"Yield-Max\" (applied in mL per plant). The scientist proposes a multiple linear regression model for the corn yield (in kg per plant), $Y$, based on the amounts of the two fertilizers applied, $X_1$ for Gro-Fast and $X_2$ for Yield-Max:\n$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon$$\nBased on prior knowledge of crop science, both fertilizers are expected to have a positive impact on yield. However, the two fertilizers have similar chemical compositions, and the experimental design resulted in a high positive correlation between the amounts of $X_1$ and $X_2$ applied.\n\nAfter the experiment, the following summary statistics are calculated from the collected data:\n- Sample correlation between yield ($Y$) and Gro-Fast ($X_1$): $r_{y1} = 0.80$\n- Sample correlation between yield ($Y$) and Yield-Max ($X_2$): $r_{y2} = 0.90$\n- Sample correlation between Gro-Fast ($X_1$) and Yield-Max ($X_2$): $r_{12} = 0.95$\n\n- Sample standard deviation of yield ($Y$): $s_y = 1.5$ kg\n- Sample standard deviation of Gro-Fast ($X_1$): $s_1 = 10.0$ mL\n- Sample standard deviation of Yield-Max ($X_2$): $s_2 = 12.0$ mL\n\nDespite the strong positive simple correlation between Gro-Fast and yield ($r_{y1} = 0.80$), the multiple regression analysis surprisingly produces a negative estimated coefficient for Gro-Fast, $\\hat{\\beta}_1$. Calculate the value of $\\hat{\\beta}_1$. Express your answer in kg/mL, rounded to three significant figures.", "solution": "We start from the multiple linear regression of $Y$ on $X_{1}$ and $X_{2}$. Let $Z_{y}=(Y-\\bar{Y})/s_{y}$, $Z_{1}=(X_{1}-\\bar{X}_{1})/s_{1}$, and $Z_{2}=(X_{2}-\\bar{X}_{2})/s_{2}$ be standardized variables. The standardized regression takes the form\n$$\nZ_{y}=b_{1}^{\\ast}Z_{1}+b_{2}^{\\ast}Z_{2}+\\varepsilon,\n$$\nwhere the standardized coefficients satisfy\n$$\n\\begin{pmatrix}\nb_{1}^{\\ast}\\\\\nb_{2}^{\\ast}\n\\end{pmatrix}\n=R^{-1}\n\\begin{pmatrix}\nr_{y1}\\\\\nr_{y2}\n\\end{pmatrix},\n\\quad\nR=\n\\begin{pmatrix}\n1 & r_{12}\\\\\nr_{12} & 1\n\\end{pmatrix}.\n$$\nThe inverse of the $2\\times 2$ correlation matrix is\n$$\nR^{-1}=\\frac{1}{1-r_{12}^{2}}\n\\begin{pmatrix}\n1 & -r_{12}\\\\\n-r_{12} & 1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\nb_{1}^{\\ast}=\\frac{r_{y1}-r_{12}r_{y2}}{1-r_{12}^{2}},\n\\qquad\nb_{2}^{\\ast}=\\frac{r_{y2}-r_{12}r_{y1}}{1-r_{12}^{2}}.\n$$\nThe unstandardized coefficient for $X_{1}$ is obtained by rescaling:\n$$\n\\hat{\\beta}_{1}=b_{1}^{\\ast}\\frac{s_{y}}{s_{1}}=\\frac{r_{y1}-r_{12}r_{y2}}{1-r_{12}^{2}}\\cdot\\frac{s_{y}}{s_{1}}\n$$\nSubstituting the given values $r_{y1}=0.80$, $r_{y2}=0.90$, $r_{12}=0.95$, $s_{y}=1.5$, and $s_{1}=10.0$, we compute\n$$\nr_{y1}-r_{12}r_{y2}=0.80-0.95\\cdot 0.90=0.80-0.855=-0.055,\n$$\n$$\n1-r_{12}^{2}=1-0.95^{2}=1-0.9025=0.0975,\n$$\nso\n$$\nb_{1}^{\\ast}=\\frac{-0.055}{0.0975}=-\\frac{22}{39}\\approx -0.56410256.\n$$\nThen\n$$\n\\hat{\\beta}_{1}=b_{1}^{\\ast}\\frac{s_{y}}{s_{1}}=-\\frac{22}{39}\\cdot\\frac{1.5}{10}=-\\frac{22}{39}\\cdot 0.15\\approx -0.08461538.\n$$\nRounding to three significant figures gives\n$$\n\\hat{\\beta}_{1}\\approx -0.0846.\n$$\nThis negative coefficient arises despite the positive simple correlation due to the strong positive collinearity between $X_{1}$ and $X_{2}$, which shifts the partial effect estimate.", "answer": "$$\\boxed{-0.0846}$$", "id": "1938238"}, {"introduction": "Multicollinearity is not just an issue between distinct variables; it can also be introduced structurally within a model, for instance, in polynomial regression. This final practice [@problem_id:4929491] investigates the collinearity between a predictor $X$ and its square $X^2$. More importantly, it demonstrates a widely-used and effective technique—mean-centering—to mitigate this issue, providing you with a practical tool for building more stable and interpretable models.", "problem": "Consider a quadratic polynomial regression in a biostatistics study of a continuous covariate $X$ on an outcome $Y$, specified as $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^{2} + \\varepsilon$, where $\\varepsilon$ is a mean-zero error term independent of $X$. Investigators are concerned about multicollinearity between $X$ and $X^{2}$ and wish to quantify it via the Variance Inflation Factor (VIF).\n\nYou are given a simple, scientifically plausible covariate sample of size $n=7$: $X_{i} \\in \\{1, 2, 3, 4, 5, 6, 7\\}$ for $i=1,\\dots,7$. Using only the fundamental definitions of least squares regression and the coefficient of determination $R^{2}$ (without relying on any shortcut formulas not derived from these definitions), compute the VIFs for the predictors $X$ and $X^{2}$ in the following two design specifications:\n\n- Before centering: the design matrix has columns corresponding to the intercept, $X$, and $X^{2}$.\n- After centering: define $Z = X - \\bar{X}$, where $\\bar{X}$ is the sample mean of $X$. The design matrix has columns corresponding to the intercept, $Z$, and $Z^{2}$.\n\nIn each specification, treat multicollinearity according to its foundational definition via the regression of each predictor on the remaining predictor(s) and the intercept to obtain the relevant $R_{j}^{2}$ for the predictor indexed by $j$, and then compute the corresponding VIF for $X$ and for $X^{2}$ (or for $Z$ and $Z^{2}$ after centering). Finally, explain the observed differences in $R_{j}^{2}$ between the uncentered and centered specifications based on properties of moments of $X$.\n\nExpress your final answer as four numbers in a single row matrix in the order: VIF for $X$ before centering, VIF for $X^{2}$ before centering, VIF for $Z$ after centering, VIF for $Z^{2}$ after centering. No rounding is required.", "solution": "The problem asks for the calculation of Variance Inflation Factors (VIFs) for the predictors in a quadratic regression model, $Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^{2} + \\varepsilon$, both before and after centering the covariate $X$. The VIF for a predictor $j$, denoted $VIF_j$, is a measure of multicollinearity and is defined by the formula:\n$$VIF_j = \\frac{1}{1 - R_j^2}$$\nHere, $R_j^2$ is the coefficient of determination from an auxiliary ordinary least squares (OLS) regression of the predictor $X_j$ on all other predictors in the model, including the intercept. For a simple linear regression of a variable $U$ on a variable $V$, $U = \\gamma_0 + \\gamma_1 V + \\text{error}$, the coefficient of determination $R^2$ is given by the square of the Pearson correlation coefficient between $U$ and $V$. It can be calculated fundamentally as:\n$$R^2 = \\frac{\\left( \\sum_{i=1}^n (U_i - \\bar{U})(V_i - \\bar{V}) \\right)^2}{\\left( \\sum_{i=1}^n (U_i - \\bar{U})^2 \\right) \\left( \\sum_{i=1}^n (V_i - \\bar{V})^2 \\right)}$$\nWe are given a sample of size $n=7$ for the covariate $X$: $X_i \\in \\{1, 2, 3, 4, 5, 6, 7\\}$.\n\nFirst, we compile the necessary sample sums and moments for the given data.\nThe values for $X$ are $\\{1, 2, 3, 4, 5, 6, 7\\}$.\n$\\sum_{i=1}^7 X_i = 1+2+3+4+5+6+7 = 28$.\nThe sample mean of $X$ is $\\bar{X} = \\frac{28}{7} = 4$.\nThe values for $X^2$ are $\\{1^2, 2^2, \\dots, 7^2\\} = \\{1, 4, 9, 16, 25, 36, 49\\}$.\n$\\sum_{i=1}^7 X_i^2 = 1+4+9+16+25+36+49 = 140$.\nThe sample mean of $X^2$ is $\\overline{X^2} = \\frac{140}{7} = 20$.\nWe will also need higher moments for cross-product calculations:\n$\\sum_{i=1}^7 X_i^3 = 1^3+2^3+\\dots+7^3 = 1+8+27+64+125+216+343 = 784$.\n$\\sum_{i=1}^7 X_i^4 = 1^4+2^4+\\dots+7^4 = 1+16+81+256+625+1296+2401 = 4676$.\n\n**Part 1: VIFs Before Centering**\n\nThe predictors are the intercept, $P_1 = X$, and $P_2 = X^2$.\n\nTo find the VIF for $X$, we must compute $R_X^2$ from the regression of $X$ on $X^2$ and an intercept.\nLet $U=X$ and $V=X^2$. We calculate the terms for the $R^2$ formula:\nThe total sum of squares for $X$ is:\n$$SS_{tot}(X) = \\sum_{i=1}^7 (X_i - \\bar{X})^2 = \\sum X_i^2 - n\\bar{X}^2 = 140 - 7(4^2) = 140 - 112 = 28$$\nThe total sum of squares for $X^2$ is:\n$$SS_{tot}(X^2) = \\sum_{i=1}^7 (X_i^2 - \\overline{X^2})^2 = \\sum X_i^4 - n(\\overline{X^2})^2 = 4676 - 7(20^2) = 4676 - 2800 = 1876$$\nThe sum of cross-products between $X$ and $X^2$ is:\n$$S_{X,X^2} = \\sum_{i=1}^7 (X_i - \\bar{X})(X_i^2 - \\overline{X^2}) = \\sum X_i^3 - n\\bar{X}\\overline{X^2} = 784 - 7(4)(20) = 784 - 560 = 224$$\nNow, we compute $R_X^2$:\n$$R_X^2 = \\frac{S_{X,X^2}^2}{SS_{tot}(X) \\cdot SS_{tot}(X^2)} = \\frac{224^2}{28 \\cdot 1876} = \\frac{50176}{52528} = \\frac{448}{469}$$\nThe VIF for $X$ is therefore:\n$$VIF_X = \\frac{1}{1 - R_X^2} = \\frac{1}{1 - \\frac{448}{469}} = \\frac{1}{\\frac{469 - 448}{469}} = \\frac{469}{21}$$\nTo find the VIF for $X^2$, we need $R_{X^2}^2$ from the regression of $X^2$ on $X$ and an intercept. Because the coefficient of determination $R^2$ in a simple linear regression is symmetric with respect to the two variables, $R_{X^2}^2 = R_X^2 = \\frac{448}{469}$.\nThus, the VIF for $X^2$ is identical to the VIF for $X$:\n$$VIF_{X^2} = \\frac{1}{1 - R_{X^2}^2} = \\frac{469}{21}$$\n\n**Part 2: VIFs After Centering**\n\nWe define the centered variable $Z = X - \\bar{X} = X - 4$. The predictors are now the intercept, $P'_1 = Z$, and $P'_2 = Z^2$.\nThe values for $Z$ are $\\{1-4, 2-4, \\dots, 7-4\\} = \\{-3, -2, -1, 0, 1, 2, 3\\}$.\nThe sample mean of $Z$ is $\\bar{Z} = \\frac{1}{7}\\sum Z_i = \\frac{0}{7} = 0$.\nThe values for $Z^2$ are $\\{(-3)^2, (-2)^2, \\dots, 3^2\\} = \\{9, 4, 1, 0, 1, 4, 9\\}$.\n$\\sum_{i=1}^7 Z_i^2 = 9+4+1+0+1+4+9 = 28$.\nThe sample mean of $Z^2$ is $\\overline{Z^2} = \\frac{28}{7} = 4$.\n\nTo find the VIF for $Z$, we compute $R_Z^2$ from the regression of $Z$ on $Z^2$ and an intercept.\nLet $U=Z$ and $V=Z^2$. We calculate the sum of cross-products $S_{Z,Z^2}$:\n$$S_{Z,Z^2} = \\sum_{i=1}^7 (Z_i - \\bar{Z})(Z_i^2 - \\overline{Z^2})$$\nSince $\\bar{Z}=0$, this simplifies to:\n$$S_{Z,Z^2} = \\sum_{i=1}^7 Z_i(Z_i^2 - \\overline{Z^2}) = \\sum_{i=1}^7 Z_i^3 - \\overline{Z^2} \\sum_{i=1}^7 Z_i$$\nWe know $\\sum Z_i = 0$. We calculate $\\sum Z_i^3$:\n$\\sum_{i=1}^7 Z_i^3 = (-3)^3 + (-2)^3 + (-1)^3 + 0^3 + 1^3 + 2^3 + 3^3 = -27 - 8 - 1 + 0 + 1 + 8 + 27 = 0$.\nTherefore, the sum of cross-products is:\n$$S_{Z,Z^2} = 0 - 4(0) = 0$$\nSince the numerator of the $R^2$ formula is $S_{Z,Z^2}^2 = 0^2 = 0$, it immediately follows that $R_Z^2 = 0$.\nThe VIF for $Z$ is:\n$$VIF_Z = \\frac{1}{1 - R_Z^2} = \\frac{1}{1 - 0} = 1$$\nBy the same symmetry argument as in the uncentered case, $R_{Z^2}^2 = R_Z^2 = 0$. Thus, the VIF for $Z^2$ is:\n$$VIF_{Z^2} = \\frac{1}{1 - R_{Z^2}^2} = 1$$\n\n**Part 3: Explanation of Differences**\n\nThe substantial difference in VIFs between the uncentered and centered specifications arises from the sample correlation between a variable and its square, which is fundamentally tied to the symmetry properties of the variable's sample distribution.\n\nIn the **uncentered case**, the predictors are $X$ and $X^2$. The high VIFs ($\\frac{469}{21} \\approx 22.3$) indicate strong multicollinearity. This collinearity is quantified by the non-zero sample correlation between $X$ and $X^2$. The correlation is non-zero because the sum of cross-products $S_{X,X^2} = \\sum (X_i-\\bar{X})(X_i^2-\\overline{X^2})$ is non-zero. The term $(X_i-\\bar{X})$ represents a variable that is symmetric about zero, but the term $(X_i^2-\\overline{X^2})$ is not symmetric. The product of these terms summed over the sample does not cancel to zero, resulting in a non-zero covariance and thus a high correlation.\n\nIn the **centered case**, the predictors are $Z = X-\\bar{X}$ and $Z^2$. The VIFs are both $1$, the minimum possible value, indicating a complete absence of multicollinearity between $Z$ and $Z^2$. This is because the sample correlation between $Z$ and $Z^2$ is exactly zero. The sum of cross-products, $S_{Z,Z^2} = \\sum (Z_i-\\bar{Z})(Z_i^2-\\overline{Z^2})$, is zero. As shown in the calculation, with $\\bar{Z}=0$, this sum simplifies to $\\sum Z_i^3$. The sample data for $Z$, $\\{-3, -2, -1, 0, 1, 2, 3\\}$, is perfectly symmetric about its mean of $0$. A fundamental property of any data distribution that is symmetric about zero is that all its odd-ordered sample moments are zero. Thus, the third sample moment $\\sum Z_i^3$ is zero. This zero covariance leads to a zero correlation and $R^2=0$, which in turn yields $VIF=1$.\n\nIn summary, centering the covariate $X$ creates a new variable $Z$ whose sample distribution is symmetric about zero. This specific symmetry property makes $Z$ and $Z^2$ uncorrelated (orthogonal in the regression context, given an intercept), thereby resolving the multicollinearity problem that exists between the original uncentered predictor $X$ and its square $X^2$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{469}{21} & \\frac{469}{21} & 1 & 1\n\\end{pmatrix}\n}\n$$", "id": "4929491"}]}