{"hands_on_practices": [{"introduction": "A primary reason we use multiple linear regression is to isolate the relationship between an exposure and an outcome while accounting for other variables, known as confounders. This practice explores the fundamental concept of confounding by comparing the coefficient of an exposure in a simple model versus a multiple regression model [@problem_id:4930822]. By working through this exercise, you will understand the mathematical basis of omitted variable bias and the conditions under which an association can be misleading, a phenomenon famously illustrated by Simpson's paradox.", "problem": "A biomedical researcher models a continuous outcome $Y$ (e.g., a plasma biomarker concentration) as a linear function of an exposure $X$ (e.g., a nutrient intake) and a covariate $Z$ (e.g., age), with additive noise. The data-generating mechanism is specified as\n$$\nY \\;=\\; \\alpha \\;+\\; \\beta_X X \\;+\\; \\beta_Z Z \\;+\\; \\varepsilon,\n$$\nwhere $\\varepsilon$ has $\\mathrm{E}[\\varepsilon \\mid X,Z] = 0$, and $\\varepsilon$ is independent of $(X,Z)$. The coefficient $\\beta_X$ represents the multiple regression coefficient of $X$ when both $X$ and $Z$ are included. Throughout, assume $\\mathrm{E}[X]=0$ and $\\mathrm{E}[Z]=0$.\n\nConsider two scientifically realistic study designs:\n\nStudy A (randomized exposure): $X$ is randomized and independent of $Z$, so $X \\perp Z$. The joint distribution of $(X,Z)$ is such that $\\operatorname{Var}(X)>0$ and $\\operatorname{Var}(Z)>0$.\n\nStudy B (observational with confounding structure): $X$ and $Z$ are correlated with $\\operatorname{Cov}(X,Z) < 0$. For concreteness, suppose $\\operatorname{Var}(X) = 1$, $\\operatorname{Var}(Z) = 1$, and $\\operatorname{Cov}(X,Z) = -0.6$, with $\\beta_X = 0.5$ and $\\beta_Z = 2.0$.\n\nCollapsibility over $Z$ of the coefficient of $X$ means that the coefficient of $X$ obtained by regressing $Y$ on $X$ alone (i.e., the simple regression coefficient) equals the multiple regression coefficient $\\beta_X$ obtained when regressing $Y$ on $(X,Z)$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. In Study A, the simple regression coefficient of $X$ from regressing $Y$ on $X$ alone equals the multiple regression coefficient of $X$ from regressing $Y$ on $(X,Z)$ under the stated model.\n\nB. In Study A, even with $X$ independent of $Z$, the simple regression coefficient of $X$ will generally differ from its multiple regression coefficient unless $\\beta_Z = 0$.\n\nC. In Study B, it is possible for the simple regression coefficient of $X$ to be negative even when the multiple regression coefficient $\\beta_X$ is positive, provided $\\operatorname{Cov}(X,Z) < 0$ and $\\beta_Z > 0$ with sufficient magnitude.\n\nD. Simpson’s paradox refers to the phenomenon that the direction of the association between $X$ and $Y$ can reverse upon adjustment for $Z$ when $X$ and $Z$ are correlated and $Z$ affects $Y$.\n\nE. If $\\operatorname{Cov}(X,Z) = 0$, then the simple regression coefficient of $X$ equals its multiple regression coefficient $\\beta_X$ regardless of the magnitude of $\\beta_Z$.", "solution": "The user wants me to validate and solve a problem concerning the relationship between simple and multiple linear regression coefficients under different assumptions about the covariance structure of the predictors.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following information:\n-   **Data-Generating Model**: $Y \\;=\\; \\alpha \\;+\\; \\beta_X X \\;+\\; \\beta_Z Z \\;+\\; \\varepsilon$.\n-   **Outcome**: $Y$ is a continuous variable.\n-   **Predictors**: $X$ (exposure) and $Z$ (covariate) are variables.\n-   **Error Term**: $\\varepsilon$ is a random error with $\\mathrm{E}[\\varepsilon \\mid X,Z] = 0$ and $\\varepsilon$ is independent of $(X,Z)$.\n-   **Coefficient of Interest**: $\\beta_X$ is the multiple regression coefficient of $X$.\n-   **Assumptions**: $\\mathrm{E}[X]=0$ and $\\mathrm{E}[Z]=0$.\n-   **Study A (Randomized Exposure)**: $X$ is independent of $Z$ ($X \\perp Z$). Also, $\\operatorname{Var}(X)>0$ and $\\operatorname{Var}(Z)>0$.\n-   **Study B (Observational with Confounding)**: $X$ and $Z$ are correlated. Specifically, $\\operatorname{Var}(X) = 1$, $\\operatorname{Var}(Z) = 1$, $\\operatorname{Cov}(X,Z) = -0.6$, $\\beta_X = 0.5$, and $\\beta_Z = 2.0$.\n-   **Definition of Collapsibility**: The simple regression coefficient of $X$ (from regressing $Y$ on $X$ alone) equals the multiple regression coefficient $\\beta_X$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is based on the fundamental principles of multiple linear regression, a core topic in biostatistics and many other scientific fields. The concepts of confounding, collapsibility, randomization, and observational studies are central to statistical modeling and causal inference. The model specified is the standard linear model. All aspects are scientifically sound.\n-   **Well-Posed**: The problem is clearly stated. It asks for an evaluation of the relationship between the simple and multiple regression coefficients of $X$ under well-defined conditions. The provided information is sufficient to derive the necessary formulae and evaluate each statement. A unique, stable, and meaningful solution exists.\n-   **Objective**: The terminology is precise and standard in statistics (e.g., \"multiple regression coefficient,\" \"covariance,\" \"independent\"). The problem is free of subjective or ambiguous language. The numerical values in Study B are consistent; the specified covariance matrix for $(X, Z)$ is $\\begin{pmatrix} 1 & -0.6 \\\\ -0.6 & 1 \\end{pmatrix}$, which is positive definite since its determinant is $1 - (-0.6)^2 = 1 - 0.36 = 0.64 > 0$.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and contains sufficient information for a rigorous solution. I will proceed with deriving the solution.\n\n### Derivation of the Solution\n\nThe core of the problem is to compare the multiple regression coefficient $\\beta_X$ from the model $Y = \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon$ with the simple regression coefficient obtained from a model that omits $Z$.\n\nLet the simple linear regression model be $Y = \\alpha' + \\beta_{X, \\text{simple}} X + \\eta$.\nThe population coefficient $\\beta_{X, \\text{simple}}$ is given by the formula:\n$$\n\\beta_{X, \\text{simple}} = \\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}\n$$\nWe must compute $\\operatorname{Cov}(X,Y)$ using the true data-generating model, $Y = \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon$.\n\nUsing the properties of covariance:\n$$\n\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(X, \\alpha + \\beta_X X + \\beta_Z Z + \\varepsilon)\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\operatorname{Cov}(X, \\alpha) + \\operatorname{Cov}(X, \\beta_X X) + \\operatorname{Cov}(X, \\beta_Z Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\nSince $\\alpha$ and $\\beta_X, \\beta_Z$ are constants:\n$$\n\\operatorname{Cov}(X,Y) = 0 + \\beta_X \\operatorname{Cov}(X,X) + \\beta_Z \\operatorname{Cov}(X,Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\n$$\n\\operatorname{Cov}(X,Y) = \\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z) + \\operatorname{Cov}(X, \\varepsilon)\n$$\nThe problem states that $\\varepsilon$ is independent of $(X,Z)$, which implies $\\operatorname{Cov}(X, \\varepsilon) = 0$. Therefore:\n$$\n\\operatorname{Cov}(X,Y) = \\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z)\n$$\nSubstituting this back into the formula for $\\beta_{X, \\text{simple}}$:\n$$\n\\beta_{X, \\text{simple}} = \\frac{\\beta_X \\operatorname{Var}(X) + \\beta_Z \\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\nThis fundamental equation relates the simple regression coefficient to the multiple regression coefficient. The term $\\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$ is known as the omitted variable bias.\nCollapsibility occurs when this bias term is zero, meaning $\\beta_{X, \\text{simple}} = \\beta_X$. This happens if and only if:\n1.  $\\beta_Z = 0$ (the omitted variable $Z$ has no effect on the outcome $Y$ in the multiple regression model).\n2.  $\\operatorname{Cov}(X,Z) = 0$ (the exposure $X$ is uncorrelated with the omitted variable $Z$).\n\nNow we evaluate each option.\n\n### Option-by-Option Analysis\n\n**A. In Study A, the simple regression coefficient of $X$ from regressing $Y$ on $X$ alone equals the multiple regression coefficient of $X$ from regressing $Y$ on $(X,Z)$ under the stated model.**\n\nIn Study A, it is given that $X$ is independent of $Z$ ($X \\perp Z$). A consequence of independence is that the covariance is zero, $\\operatorname{Cov}(X,Z) = 0$. Plugging this into our derived formula:\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{0}{\\operatorname{Var}(X)} = \\beta_X\n$$\nSince $\\beta_{X, \\text{simple}}$ equals $\\beta_X$, the statement is correct. This is the key advantage of randomization: it eliminates confounding by breaking the association between the exposure and other covariates.\n**Verdict: Correct.**\n\n**B. In Study A, even with $X$ independent of $Z$, the simple regression coefficient of $X$ will generally differ from its multiple regression coefficient unless $\\beta_Z = 0$.**\n\nThis statement contradicts the analysis for option A. As shown above, in Study A, the condition $X \\perp Z$ implies $\\operatorname{Cov}(X,Z) = 0$. This makes the bias term $\\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$ equal to zero for *any* value of $\\beta_Z$. The simple and multiple regression coefficients will be equal. The statement that they will generally differ unless $\\beta_Z = 0$ is false.\n**Verdict: Incorrect.**\n\n**C. In Study B, it is possible for the simple regression coefficient of $X$ to be negative even when the multiple regression coefficient $\\beta_X$ is positive, provided $\\operatorname{Cov}(X,Z) < 0$ and $\\beta_Z > 0$ with sufficient magnitude.**\n\nLet's use the formula $\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}$.\nThe problem provides the specific values for Study B: $\\beta_X = 0.5$, $\\beta_Z = 2.0$, $\\operatorname{Cov}(X,Z) = -0.6$, and $\\operatorname{Var}(X) = 1$. Let's calculate $\\beta_{X, \\text{simple}}$:\n$$\n\\beta_{X, \\text{simple}} = 0.5 + (2.0) \\frac{-0.6}{1} = 0.5 - 1.2 = -0.7\n$$\nIn this case, the multiple regression coefficient $\\beta_X$ is positive ($0.5$), but the simple regression coefficient $\\beta_{X, \\text{simple}}$ is negative ($-0.7$). This occurs because the conditions specified in the option ($\\operatorname{Cov}(X,Z) < 0$ and $\\beta_Z > 0$) create a negative bias term. If the magnitude of this bias ($|-1.2|=1.2$) is greater than the true coefficient $\\beta_X$ ($0.5$), the sign will reverse. This confirms the possibility.\n**Verdict: Correct.**\n\n**D. Simpson’s paradox refers to the phenomenon that the direction of the association between $X$ and $Y$ can reverse upon adjustment for $Z$ when $X$ and $Z$ are correlated and $Z$ affects $Y$.**\n\nThis is a qualitative statement describing the phenomenon of Simpson's paradox in the context of linear regression. The \"direction of the association\" corresponds to the sign of the regression coefficient. The unadjusted association is measured by $\\beta_{X, \\text{simple}}$, while the adjusted association is measured by $\\beta_X$. The paradox occurs when $\\mathrm{sign}(\\beta_{X, \\text{simple}}) \\neq \\mathrm{sign}(\\beta_X)$. As demonstrated in the analysis for option C, this can happen when a variable $Z$ is omitted from a model, provided that $Z$ is correlated with $X$ ($\\operatorname{Cov}(X,Z) \\neq 0$) and $Z$ is a predictor of $Y$ ($\\beta_Z \\neq 0$). This statement correctly describes these conditions and the resulting phenomenon.\n**Verdict: Correct.**\n\n**E. If $\\operatorname{Cov}(X,Z) = 0$, then the simple regression coefficient of $X$ equals its multiple regression coefficient $\\beta_X$ regardless of the magnitude of $\\beta_Z$.**\n\nThis statement presents one of the two general conditions for collapsibility. Using our derived formula:\n$$\n\\beta_{X, \\text{simple}} = \\beta_X + \\beta_Z \\frac{\\operatorname{Cov}(X,Z)}{\\operatorname{Var}(X)}\n$$\nIf we set $\\operatorname{Cov}(X,Z) = 0$, the bias term becomes $\\beta_Z \\frac{0}{\\operatorname{Var}(X)} = 0$. This holds true for any finite value of $\\beta_Z$ (and provided $\\operatorname{Var}(X) \\neq 0$, which is a necessary condition for estimating the coefficient in the first place). Therefore, under the condition $\\operatorname{Cov}(X,Z) = 0$, it is always true that $\\beta_{X, \\text{simple}} = \\beta_X$.\n**Verdict: Correct.**", "answer": "$$\\boxed{ACDE}$$", "id": "4930822"}, {"introduction": "Once you have selected variables for your model, preparing them properly is a critical step in building a robust and interpretable model. This practice delves into the practical techniques of centering and standardizing predictors, which can simplify coefficient interpretation and improve the numerical stability of your analysis [@problem_id:4930773]. This exercise is particularly valuable for understanding how these transformations affect multicollinearity, especially when constructing more complex models with interaction or polynomial terms.", "problem": "A biostatistician models a continuous clinical outcome $Y$ (for example, systolic blood pressure) using multiple linear regression with an intercept and three predictors: age $A$ (in years), body mass index $B$ (in $\\mathrm{kg}/\\mathrm{m}^2$), and dietary sodium intake $S$ (in $\\mathrm{mg}/\\mathrm{day}$). Ordinary least squares (OLS) is used under the standard assumption that the error vector has mean $0$ and constant variance. Because the predictors are measured on different scales, she considers two common preprocessing steps applied to the predictor columns (not to $Y$): centering (subtracting each predictor’s sample mean so that its mean is $0$) and standardizing (subtracting each predictor’s sample mean and dividing by its sample standard deviation so that its mean is $0$ and variance is $1$). She fits models that include only linear main effects, and she also considers models that add interaction and polynomial terms (for example, $A \\times S$ and $B^2$), potentially formed after centering.\n\nAssume that multicollinearity is assessed using the variance inflation factor (VIF), defined via $R^2$ from regressing one predictor on the remaining predictors. Which of the following statements about the effects of centering and standardizing the predictor matrix $X$ on the interpretation of regression coefficients $\\beta$ and on multicollinearity are correct? Select all that apply.\n\nA. In a model with only linear main effects, centering $A$, $B$, and $S$ so that each has mean $0$ leaves the slope estimates for $A$, $B$, and $S$ numerically unchanged, but changes the intercept to estimate the expected value of $Y$ when $A$, $B$, and $S$ are at their sample means.\n\nB. Standardizing $A$, $B$, and $S$ (mean $0$, variance $1$) changes the fitted values and residuals because the predictors are on a different scale.\n\nC. In a model with only linear main effects, after standardizing, each slope can be interpreted as the expected change in $Y$ associated with a $1$ standard deviation increase in the corresponding predictor, and numerically equals the original unstandardized slope multiplied by that predictor’s sample standard deviation.\n\nD. Centering or standardizing linear predictors by themselves reduces the VIFs for those predictors because it reduces correlations among the predictors.\n\nE. Centering predictors before creating interaction or polynomial terms (for example, replacing $A$ by $A - \\bar{A}$ before forming $(A - \\bar{A}) \\times (S - \\bar{S})$ or $(B - \\bar{B})^2$) tends to reduce multicollinearity between main effects and those higher-order terms, without changing the fitted values.\n\nF. Standardizing predictors improves the numerical conditioning of the normal equations matrix used to compute OLS estimates, even though statistical multicollinearity measures based on $R^2$ (and VIFs) are unchanged by such linear rescaling of the original linear predictors.", "solution": "The problem statement is a valid exercise in understanding the properties of multiple linear regression modeling. It is scientifically grounded in established statistical theory, well-posed, and objective. There are no contradictions, ambiguities, or missing information that would prevent a rigorous analysis of the assertions.\n\nThe general form of the multiple linear regression model is $Y = X\\beta + \\epsilon$, where $Y$ is the $n \\times 1$ vector of outcomes, $X$ is the $n \\times (p+1)$ design matrix (with a leading column of ones for an intercept and $p$ predictor columns), $\\beta$ is the $(p+1) \\times 1$ vector of coefficients, and $\\epsilon$ is the $n \\times 1$ vector of errors. The Ordinary Least Squares (OLS) estimate of $\\beta$ is $\\hat{\\beta} = (X^T X)^{-1} X^T Y$.\n\nThe predictors are age ($A$), body mass index ($B$), and dietary sodium intake ($S$). The sample means are denoted $\\bar{A}$, $\\bar{B}$, $\\bar{S}$, and the sample standard deviations are $s_A$, $s_B$, $s_S$.\n\nCentering a predictor $A$ results in a new predictor $A_c = A - \\bar{A}$.\nStandardizing a predictor $A$ results in a new predictor $A_s = (A - \\bar{A}) / s_A$.\n\nWe will now evaluate each statement.\n\nA. In a model with only linear main effects, centering $A$, $B$, and $S$ so that each has mean $0$ leaves the slope estimates for $A$, $B$, and $S$ numerically unchanged, but changes the intercept to estimate the expected value of $Y$ when $A$, $B$, and $S$ are at their sample means.\n\nLet the original model be $Y_i = \\beta_0 + \\beta_A A_i + \\beta_B B_i + \\beta_S S_i + \\epsilon_i$.\nThe centered model is $Y_i = \\beta_0^* + \\beta_A^* (A_i - \\bar{A}) + \\beta_B^* (B_i - \\bar{B}) + \\beta_S^* (S_i - \\bar{S}) + \\epsilon_i$.\n\nAccording to the Frisch-Waugh-Lovell theorem, the coefficient for a given predictor in a multiple regression is equivalent to the coefficient obtained by regressing the residuals of the outcome (from a regression on all other predictors) on the residuals of the given predictor (from a regression on all other predictors). Centering all predictors is equivalent to residualizing them with respect to the intercept. The slope coefficients in a model with an intercept measure the effect of each predictor after accounting for the intercept. Therefore, adding a centered intercept term (by centering the predictors) to a model that already contains an intercept does not change the estimated slope coefficients. Thus, $\\hat{\\beta}_A = \\hat{\\beta}_A^*$, $\\hat{\\beta}_B = \\hat{\\beta}_B^*$, and $\\hat{\\beta}_S = \\hat{\\beta}_S^*$. The slope estimates are unchanged.\n\nFor the intercept in the centered model, the predictor-columns of the design matrix are orthogonal to the intercept-column (a vector of ones), since their sum (and mean) is $0$. This block-orthogonality simplifies the OLS estimation, and the intercept estimate becomes $\\hat{\\beta}_0^* = \\bar{Y}$. This is the predicted value of $Y$ when the centered predictors are all $0$, which occurs when $A_i = \\bar{A}$, $B_i = \\bar{B}$, and $S_i = \\bar{S}$.\nThe original intercept, $\\hat{\\beta}_0$, can be related to the new one:\n$\\hat{\\beta}_0^* + \\hat{\\beta}_A (A - \\bar{A}) = \\hat{\\beta}_0 + \\hat{\\beta}_A A$\n$\\hat{\\beta}_0 = \\hat{\\beta}_0^* - \\hat{\\beta}_A \\bar{A}$.\nFor all predictors, $\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_A \\bar{A} - \\hat{\\beta}_B \\bar{B} - \\hat{\\beta}_S \\bar{S}$. This is the predicted value of $Y$ when $A=0$, $B=0$, and $S=0$, which is different from $\\bar{Y}$ in general. The statement is correct on all points.\n\nVerdict: **Correct**.\n\nB. Standardizing $A$, $B$, and $S$ (mean $0$, variance $1$) changes the fitted values and residuals because the predictors are on a different scale.\n\nLet the original design matrix be $X = [\\mathbf{1}, X_A, X_B, X_S]$.\nLet the standardized design matrix be $X_{std} = [\\mathbf{1}, X_{A,s}, X_{B,s}, X_{S,s}]$, where $X_{A,s} = (X_A - \\bar{A}\\mathbf{1})/s_A$.\nEach column of $X_{std}$ is a linear combination of the columns of $X$. For example, $X_{A,s}$ is a linear combination of $X_A$ and $\\mathbf{1}$. Conversely, each column of $X$ is a linear combination of the columns of $X_{std}$. For example, $X_A = s_A X_{A,s} + \\bar{A}\\mathbf{1}$.\nThis means that the column space of $X$ is identical to the column space of $X_{std}$, i.e., $\\text{Col}(X) = \\text{Col}(X_{std})$.\nThe fitted values $\\hat{Y}$ are the orthogonal projection of the outcome vector $Y$ onto the column space of the design matrix. The projection matrix, or hat matrix, is given by $H = X(X^T X)^{-1} X^T$. Since the column space is unchanged, the projection matrix is also unchanged. Therefore, the fitted values, $\\hat{Y} = HY$, are identical for both models.\nThe residuals are given by $e = Y - \\hat{Y}$. Since both $Y$ and $\\hat{Y}$ are unchanged, the residuals are also identical.\n\nVerdict: **Incorrect**.\n\nC. In a model with only linear main effects, after standardizing, each slope can be interpreted as the expected change in $Y$ associated with a $1$ standard deviation increase in the corresponding predictor, and numerically equals the original unstandardized slope multiplied by that predictor’s sample standard deviation.\n\nLet the standardized model be $Y = \\beta_0^{std} + \\beta_A^{std} A_s + \\dots + \\epsilon$. The predictor is $A_s = (A - \\bar{A})/s_A$.\nThe coefficient $\\beta_A^{std}$ represents the change in $E[Y]$ for a $1$-unit increase in $A_s$, holding other standardized predictors constant. A $1$-unit increase in $A_s$ corresponds to an $s_A$-unit increase in the original predictor $A$. Thus, the interpretation is correct.\n\nNow, let's find the numerical relationship. Let the unstandardized model be $Y = \\beta_0 + \\beta_A A + \\dots + \\epsilon$.\nWe know from (A) that centering does not change the slope estimates. So, if we regress $Y$ on centered predictors $A_c = A - \\bar{A}$, the slope estimate is still $\\hat{\\beta}_A$.\nThe relationship between centered and standardized predictors is $A_c = s_A A_s$.\nSubstituting this into the centered model equation:\n$Y = ... + \\hat{\\beta}_A A_c + ... = ... + \\hat{\\beta}_A (s_A A_s) + ...$\nComparing this with the standardized model form, $Y = ... + \\hat{\\beta}_A^{std} A_s + ...$, we must have:\n$\\hat{\\beta}_A^{std} = \\hat{\\beta}_A s_A$.\nThe standardized slope is the original unstandardized slope multiplied by the predictor's sample standard deviation. The statement is fully correct.\n\nVerdict: **Correct**.\n\nD. Centering or standardizing linear predictors by themselves reduces the VIFs for those predictors because it reduces correlations among the predictors.\n\nThe Variance Inflation Factor for predictor $X_j$ is $\\text{VIF}_j = 1/(1 - R_j^2)$, where $R_j^2$ is the coefficient of determination from regressing $X_j$ on the other $p-1$ predictors.\nThe Pearson correlation coefficient between two variables $U$ and $V$ is invariant to scaling and shifting, i.e., $\\text{Corr}(aU+b, cV+d) = \\text{Corr}(U,V)$ for $a, c$ of the same sign. Centering ($A_c = A - \\bar{A}$) and standardizing ($A_s = (A-\\bar{A})/s_A$) are linear transformations.\nTherefore, the correlation between any two predictors $A$ and $B$ is unchanged by centering or standardizing them:\n$\\text{Corr}(A, B) = \\text{Corr}(A_c, B_c) = \\text{Corr}(A_s, B_s)$.\nThe value $R_j^2$ depends on the correlations among the predictors. Since these correlations are not changed, $R_j^2$ is not changed. Consequently, $\\text{VIF}_j$ is not changed. The premise \"because it reduces correlations\" is false, and the conclusion \"reduces the VIFs\" is also false.\n\nVerdict: **Incorrect**.\n\nE. Centering predictors before creating interaction or polynomial terms (for example, replacing $A$ by $A - \\bar{A}$ before forming $(A - \\bar{A}) \\times (S - \\bar{S})$ or $(B - \\bar{B})^2$) tends to reduce multicollinearity between main effects and those higher-order terms, without changing the fitted values.\n\nConsider a model with $A$ and $A^2$. The correlation between $A$ and $A^2$ is often high. Now consider the model with the centered predictor $A_c = A - \\bar{A}$ and its square $A_c^2_c = (A - \\bar{A})^2$. The covariance is $E[A_c \\cdot A_c^2] - E[A_c]E[A_c^2] = E[(A-\\bar{A})^3]$, which is related to the skewness of $A$. For a symmetric distribution, this correlation is $0$. For many real-world distributions, $\\text{Corr}(A_c, A_c^2)$ is substantially smaller than $\\text{Corr}(A, A^2)$. Similarly, the correlation between a main effect $A_c$ and an interaction term $A_c S_c$ is often much lower than the correlation between $A$ and $AS$. So, centering does tend to reduce this \"structural\" multicollinearity.\n\nNow, consider whether the fitted values change. Let's compare Model 1 ($Y \\sim 1 + A + S + AS$) with Model 2 ($Y \\sim 1 + A_c + S_c + A_c S_c$).\nThe predictors in Model 2 are $1$, $A - \\bar{A}$, $S - \\bar{S}$, and $(A - \\bar{A})(S - \\bar{S}) = AS - \\bar{A}S - A\\bar{S} + \\bar{A}\\bar{S}$.\nEach of these predictors is a linear combination of the predictors in Model 1, which are $1$, $A$, $S$, and $AS$. For example, $A-\\bar{A} = 1 \\cdot A - \\bar{A} \\cdot 1$.\nConversely, we can express the predictors of Model 1 as linear combinations of those in Model 2. For example, $A = (A-\\bar{A}) + \\bar{A} \\cdot 1$. And $AS = (A_c S_c) + \\bar{S}A_c + \\bar{A}S_c + \\bar{A}\\bar{S} \\cdot 1$.\nSince the set of predictors for each model can be formed by_linear combinations of the predictors from the other model, their design matrices span the same vector space. As established in the analysis of (B), if the column spaces are identical, the projection matrix $H$ is identical, and thus the fitted values $\\hat{Y}=HY$ and residuals $e=Y-\\hat{Y}$ are identical.\n\nVerdict: **Correct**.\n\nF. Standardizing predictors improves the numerical conditioning of the normal equations matrix used to compute OLS estimates, even though statistical multicollinearity measures based on $R^2$ (and VIFs) are unchanged by such linear rescaling of the original linear predictors.\n\nThe matrix to be inverted in OLS is $X^T X$. The numerical stability of this inversion is related to the matrix's condition number, $\\kappa(X^T X)$. A high condition number indicates an ill-conditioned matrix, prone to numerical errors.\nIf predictors are on vastly different scales (e.g., age in tens, sodium intake in thousands), the elements of $X^T X$ (which are sums of squares and cross-products) will have vastly different magnitudes. This often leads to a high condition number.\nWhen predictors are standardized, they all have mean $0$ and variance $1$. The predictor part of the design matrix, $X_p$, has columns with roughly equal norms. The matrix $(X_{p,std})^T X_{p,std}$ is $(n-1)$ times the correlation matrix of the predictors. Its diagonal elements are all equal ($(n-1)$), and off-diagonal elements are bounded. This brings the matrix elements to a similar scale, which is a standard technique for preconditioning a matrix. It generally reduces the condition number of $X^T X$ significantly, thus improving numerical stability.\nThe second part of the statement, \"even though statistical multicollinearity measures based on $R^2$ (and VIFs) are unchanged by such linear rescaling of the original linear predictors,\" is a restatement of the finding in (D) for a model with only main effects. As shown, VIFs are indeed unchanged. The statement correctly contrasts the numerical benefit with the unchanged statistical measures of collinearity.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ACEF}$$", "id": "4930773"}, {"introduction": "Adding more predictors to a model will almost always increase the standard coefficient of determination, $R^2$, even if the added variables are useless. This exercise introduces a more honest metric for model evaluation, the adjusted $R^2$, which penalizes the inclusion of non-informative predictors [@problem_id:4930824]. By deriving the formula for adjusted $R^2$ from first principles, you will gain a deeper understanding of the trade-off between goodness-of-fit and model complexity, a core principle in statistical modeling.", "problem": "A biostatistician is building a multiple linear regression model for fasting plasma glucose, denoted by the response variable $Y$, using predictors age $X_{1}$, body mass index $X_{2}$, and sleep duration $X_{3}$. Let the linear model be $Y = X\\beta + \\varepsilon$, where $X$ includes an intercept column and $p$ predictor columns (so the total number of regression coefficients is $p+1$), the sample size is $n$, and $\\varepsilon$ is a random error term with mean $0$ and constant variance. The coefficient of determination $R^{2}$ is defined using the Total Sum of Squares (TSS) and the Residual Sum of Squares (RSS) by $R^{2} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$, where $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$ and $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$. Starting from the fundamental definitions of $\\text{TSS}$ and $\\text{RSS}$ and the principle that unbiased estimators of variances divide sums of squares by their appropriate degrees of freedom, derive a closed-form expression for the adjusted coefficient of determination in terms of $R^{2}$, $n$, and $p$ (with $p$ counting predictors excluding the intercept). Then, using your derived expression, explain in words why the adjusted coefficient of determination can decrease when adding a predictor that does not materially improve model fit, referring to the role of degrees of freedom. Your final answer must be the single analytic expression for the adjusted coefficient of determination.", "solution": "The problem statement is valid. It is a well-posed, scientifically grounded problem in biostatistics that asks for the derivation of a standard statistical formula, the adjusted coefficient of determination ($R^2_{\\text{adj}}$), from first principles and an explanation of its properties. All provided definitions and conditions are standard and consistent.\n\nThe objective is to derive an expression for the adjusted coefficient of determination, $R^2_{\\text{adj}}$, and explain its behavior. The derivation begins from the principle that adjusted metrics should use unbiased estimators of variance, which are obtained by dividing a sum of squares by its corresponding degrees of freedom.\n\nThe coefficient of determination, $R^2$, is defined as $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$, where $\\text{TSS}$ is the Total Sum of Squares and $\\text{RSS}$ is the Residual Sum of Squares. This metric measures the proportion of variance in the response variable that is predictable from the predictor variables. However, $R^2$ mechanically increases or remains constant whenever a new predictor is added to the model, regardless of the predictor's actual explanatory power. This is because the least-squares optimization will always reduce or hold constant the $\\text{RSS}$, even if just by chance. The adjusted $R^2$ corrects for this by penalizing the inclusion of non-informative predictors.\n\nWe start with the unbiased estimators for the total variance of the response variable, $\\sigma^2_Y$, and the variance of the model's random error term, $\\sigma^2_{\\varepsilon}$.\n\nThe Total Sum of Squares, $\\text{TSS} = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$, has $n-1$ degrees of freedom, because one degree of freedom is lost in the estimation of the sample mean $\\bar{y}$. Therefore, an unbiased estimator for the total variance of $Y$ is the Mean Square Total, $\\text{MST}$:\n$$ \\text{MST} = \\frac{\\text{TSS}}{n-1} $$\n\nThe Residual Sum of Squares, $\\text{RSS} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$, has $n-(p+1)$ degrees of freedom. This is because $p+1$ parameters are estimated in the multiple linear regression model: $p$ coefficients for the predictor variables and one intercept term. An unbiased estimator for the error variance $\\sigma^2_{\\varepsilon}$ is the Mean Square Error, $\\text{MSE}$:\n$$ \\text{MSE} = \\frac{\\text{RSS}}{n-p-1} $$\n\nThe adjusted coefficient of determination, $R^2_{\\text{adj}}$, is defined analogously to $R^2$, but it uses the ratio of these unbiased variance estimators instead of the ratio of the sums of squares:\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{MSE}}{\\text{MST}} $$\nSubstituting the expressions for $\\text{MSE}$ and $\\text{MST}$, we get:\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\frac{\\text{RSS}}{(n-p-1)}}{\\frac{\\text{TSS}}{(n-1)}} $$\nThis expression can be rearranged as:\n$$ R^2_{\\text{adj}} = 1 - \\left(\\frac{\\text{RSS}}{\\text{TSS}}\\right) \\left(\\frac{n-1}{n-p-1}\\right) $$\nFrom the definition of the standard coefficient of determination, $R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$, we can express the ratio $\\frac{\\text{RSS}}{\\text{TSS}}$ in terms of $R^2$:\n$$ \\frac{\\text{RSS}}{\\text{TSS}} = 1 - R^2 $$\nSubstituting this into our equation for $R^2_{\\text{adj}}$ yields the final expression in terms of $R^2$, $n$, and $p$:\n$$ R^2_{\\text{adj}} = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right) $$\n\nNow, we use this expression to explain why $R^2_{\\text{adj}}$ can decrease when a predictor that does not materially improve the model fit is added. When a new predictor is added to the model, the number of predictors, $p$, increases by $1$. This has two competing effects on the formula for $R^2_{\\text{adj}}$:\n\n1.  **Effect on $R^2$**: The addition of any predictor, useful or not, will cause the standard $R^2$ to increase or, in a trivial case, stay the same. This means the term $(1 - R^2)$ will decrease or stay the same. This part of the expression pushes $R^2_{\\text{adj}}$ to increase.\n\n2.  **Effect on the Penalty Factor**: The term $\\left(\\frac{n-1}{n-p-1}\\right)$ is a penalty for model complexity. When $p$ increases to $p+1$, the denominator $(n-p-1)$ decreases, causing the entire penalty factor to increase. This factor is always greater than or equal to $1$ for $p \\ge 0$. This part of the expression pushes $R^2_{\\text{adj}}$ to decrease.\n\nThe overall change in $R^2_{\\text{adj}}$ depends on the balance between these two effects. If the new predictor \"does not materially improve model fit,\" it means the resulting increase in $R^2$ is very small. The corresponding decrease in the $(1 - R^2)$ term is slight. However, the penalty factor $\\left(\\frac{n-1}{n-p-1}\\right)$ strictly increases with the addition of the new predictor. If the small reduction in $(1 - R^2)$ is not large enough to overcome the increase in the penalty factor, their product, $(1 - R^2)\\left(\\frac{n-1}{n-p-1}\\right)$, will increase. Since this product is subtracted from $1$, an increase in its value will cause $R^2_{\\text{adj}}$ to decrease.\n\nIn essence, the adjusted $R^2$ only increases if the improvement in fit (the increase in $R^2$) is substantial enough to justify the \"cost\" of spending an additional degree of freedom on the new predictor. This mechanism ensures that $R^2_{\\text{adj}}$ is a more honest measure of model quality, as it rewards goodness of fit while penalizing model complexity.", "answer": "$$\n\\boxed{1 - (1 - R^2) \\frac{n-1}{n-p-1}}\n$$", "id": "4930824"}]}