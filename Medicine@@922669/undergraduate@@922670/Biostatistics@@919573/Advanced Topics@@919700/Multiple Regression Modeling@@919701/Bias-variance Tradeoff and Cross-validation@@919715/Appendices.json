{"hands_on_practices": [{"introduction": "To truly grasp the bias-variance tradeoff, we must move beyond abstract definitions and see it in action. This first practice is a foundational simulation exercise where you will bring the concepts of overfitting and cross-validation to life. By generating data from a known \"true\" model and fitting both simple and overly complex polynomial models, you will directly observe how a flexible model can achieve near-perfect performance on the data it was trained on, yet fail spectacularly on unseen data. This exercise [@problem_id:4897610] will solidify your intuition for why training error can be misleading and how techniques like $K$-fold cross-validation provide a much more honest estimate of a model's real-world predictive power.", "problem": "You are to design and implement a complete simulation study that exposes the bias-variance tradeoff using polynomial regression models under a realistic data-generating mechanism, and to use Cross-Validation (CV) to estimate expected test error. The goal is to construct scenarios where a model with more parameters achieves lower training error but worse expected test error, and to demonstrate that this behavior is driven by an increase in the explicit variance term.\n\nFundamental base and setup:\n- Let the predictor be a real-valued covariate $x \\in [-1,1]$ distributed as $x \\sim \\mathrm{Uniform}([-1,1])$.\n- Let the response be generated by $y = f(x) + \\epsilon$, where $f(x)$ is a deterministic function and $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent noise with variance $\\sigma^2$.\n- Define the true signal $f(x)$ by $f(x) = \\alpha \\sin(\\pi x) + \\beta x$, where $\\alpha$ and $\\beta$ are fixed real constants.\n- Consider polynomial regression models of degree $d$ (including an intercept), which estimate a function $\\hat{f}_d(x)$ via least squares on the training data. The “simple” model uses degree $d_s$ and the “complex” model uses degree $d_c$ with $d_c > d_s$.\n\nCore definitions to be used:\n- The training mean squared error (MSE) is $\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i - \\hat{f}(x_i)\\right)^2$ for a training dataset of size $n$.\n- The $K$-fold cross-validation estimator of expected test MSE is the average of held-out fold MSEs over $K$ folds formed by partitioning the training dataset.\n- The bias-variance decomposition at a fixed point $x$ under the data-generating model $y = f(x) + \\epsilon$ states\n$$\n\\mathbb{E}\\left[(\\hat{f}(x) - y)^2\\right] = \\left(\\mathbb{E}[\\hat{f}(x)] - f(x)\\right)^2 + \\mathrm{Var}(\\hat{f}(x)) + \\sigma^2,\n$$\nwhere the expectation and variance are taken over the randomness in the training sample (and $\\epsilon$ when applicable). For this simulation, you will estimate the variance term $\\mathrm{Var}(\\hat{f}(x))$ explicitly by Monte Carlo replicates.\n\nTask requirements:\n1. For each specified test case, generate $B$ independent training datasets of size $n$ according to $x \\sim \\mathrm{Uniform}([-1,1])$ and $y = f(x) + \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ and fixed $f(x) = \\alpha \\sin(\\pi x) + \\beta x$. Use $\\alpha = 1.5$ and $\\beta = 0.5$.\n2. For each dataset and each model degree $d \\in \\{d_s, d_c\\}$:\n   - Fit the least squares polynomial model of degree $d$.\n   - Compute the training MSE.\n   - Compute the $K$-fold cross-validation MSE, with $K=5$ folds.\n3. To estimate the variance term $\\mathrm{Var}(\\hat{f}(x))$ for each degree $d$, construct a fixed grid of $m$ points $\\{x_j\\}_{j=1}^m$ covering $[-1,1]$, compute $\\hat{f}_d^{(b)}(x_j)$ for each Monte Carlo replicate $b \\in \\{1,\\dots,B\\}$, then estimate $\\mathrm{Var}(\\hat{f}(x_j))$ by the sample variance across replicates. Average these variances over the grid to obtain a single scalar summary\n$$\nV_d = \\frac{1}{m} \\sum_{j=1}^m \\widehat{\\mathrm{Var}}\\left(\\hat{f}_d(x_j)\\right).\n$$\n4. For each test case, aggregate across the $B$ replicates to compute:\n   - The average training MSE for the simple model and the complex model.\n   - The average $K$-fold CV MSE for the simple model and the complex model.\n   - The variance summaries $V_{d_s}$ and $V_{d_c}$.\n5. Define the overfitting manifestation indicator for a test case as $1$ (true) if all of the following three conditions hold, and $0$ (false) otherwise:\n   - The average training MSE of the complex model is strictly less than that of the simple model.\n   - The average $K$-fold CV MSE of the complex model is strictly greater than that of the simple model.\n   - The variance summary $V_{d_c}$ is strictly greater than $V_{d_s}$.\n\nParameter values (test suite):\n- Use $B = 200$ and $m = 201$ grid points.\n- Use $K = 5$ folds.\n- Use the following three test cases, which together probe a general scenario, a large-sample boundary, and a noise-free edge case:\n  1. Case A (happy path): $n = 60$, $\\sigma = 0.5$, $d_s = 3$, $d_c = 18$.\n  2. Case B (large sample boundary): $n = 400$, $\\sigma = 0.5$, $d_s = 3$, $d_c = 18$.\n  3. Case C (noise-free edge case): $n = 60$, $\\sigma = 0$, $d_s = 3$, $d_c = 18$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resultk$ is the integer $0$ or $1$ for the $k$-th test case, evaluated according to the indicator described above. No other output or text should be printed.", "solution": "The problem presents a valid and well-posed simulation study in the field of biostatistics, designed to explore the bias-variance tradeoff using polynomial regression. All parameters and procedures are clearly defined, scientifically grounded in established statistical principles, and computationally feasible. The problem is objective and free of ambiguity, satisfying all criteria for validation.\n\nThe core of the task is to simulate data from a known true function, fit models of varying complexity, and evaluate their performance to illustrate the concept of overfitting. The simulation framework is defined as follows:\n\nThe data-generating process is based on a predictor variable $x$ sampled uniformly from the interval $[-1, 1]$, i.e., $x \\sim \\mathrm{Uniform}([-1,1])$. The corresponding response variable $y$ is generated by the model $y = f(x) + \\epsilon$, where $f(x)$ is the deterministic true signal and $\\epsilon$ is a random noise term. The signal is given by the function $f(x) = \\alpha \\sin(\\pi x) + \\beta x$, with specified constants $\\alpha = 1.5$ and $\\beta = 0.5$. The noise follows a normal distribution, $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, with its variance $\\sigma^2$ being one of the parameters of the simulation.\n\nWe compare two models: a \"simple\" polynomial regression model of degree $d_s$ and a \"complex\" model of degree $d_c$, where $d_c > d_s$. For each training dataset of size $n$, we fit the models $\\hat{f}_{d_s}(x)$ and $\\hat{f}_{d_c}(x)$ by minimizing the residual sum of squares (ordinary least squares).\n\nThe simulation protocol is executed for $B=200$ Monte Carlo replicates for each of the three test cases provided. Each replicate involves these steps:\n1.  **Data Generation**: A new training dataset of size $n$, $\\{(x_i, y_i)\\}_{i=1}^n$, is created according to the specified data-generating process.\n2.  **Model Fitting and Evaluation**: For each degree $d \\in \\{d_s, d_c\\}$:\n    -   The polynomial model $\\hat{f}_d(x)$ is fitted to the training data.\n    -   The **training mean squared error (MSE)** is calculated as $\\mathrm{MSE}_{\\text{train}} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i - \\hat{f}_d(x_i)\\right)^2$. This measures how well the model fits the data it was trained on.\n    -   The expected test MSE is estimated using **$K$-fold cross-validation (CV)**, with $K=5$. The training set is partitioned into $K$ disjoint subsets (folds). For each fold, a model is trained on the remaining $K-1$ folds and tested on the held-out fold. The CV error, $\\mathrm{MSE}_{\\text{CV}}$, is the average of the MSEs from these $K$ validation folds. This metric provides a more robust estimate of the model's generalization performance on unseen data.\n\n3.  **Explicit Variance Estimation**: The variance of the model itself is a key component of the bias-variance tradeoff. We estimate it directly via the Monte Carlo simulation. A fixed grid of $m=201$ points, $\\{x_j\\}_{j=1}^m$, is established over the domain $[-1,1]$. For each replicate $b$ and each model degree $d$, we compute the predictions $\\hat{f}_d^{(b)}(x_j)$ on this grid. The variance of the estimator at a point $x_j$ is estimated by the sample variance of predictions at that point across all $B$ replicates. A single scalar summary of the model variance, $V_d$, is then computed by averaging these point-wise variance estimates over the entire grid: $V_d = \\frac{1}{m} \\sum_{j=1}^m \\widehat{\\mathrm{Var}}\\left(\\hat{f}_d(x_j)\\right)$.\n\nAfter all $B$ replicates are completed, the collected metrics are averaged to yield $\\overline{\\mathrm{MSE}}_{\\text{train}}$, $\\overline{\\mathrm{MSE}}_{\\text{CV}}$, and the aggregate variance term $V_d$ for both the simple and complex models.\n\nFinally, an **overfitting manifestation indicator** is computed for each test case. This binary indicator is set to $1$ (true) if and only if all three of the following conditions are simultaneously satisfied, and $0$ (false) otherwise:\n1.  $\\overline{\\mathrm{MSE}}_{\\text{train}, d_c} < \\overline{\\mathrm{MSE}}_{\\text{train}, d_s}$: The complex model achieves a lower error on the training data.\n2.  $\\overline{\\mathrm{MSE}}_{\\text{CV}, d_c} > \\overline{\\mathrm{MSE}}_{\\text{CV}, d_s}$: The complex model exhibits a higher estimated test error, indicating poor generalization (overfitting).\n3.  $V_{d_c} > V_{d_s}$: The complex model is more variable (less stable) than the simple model.\n\nThis structured analysis is applied to three distinct test cases to probe the behavior of the models under different conditions of sample size and noise level. The implementation will proceed by systematically executing this simulation design and reporting a list of the resulting indicator values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs a simulation study to demonstrate the bias-variance tradeoff\n    and computes an overfitting indicator for three test cases.\n    \"\"\"\n    # Set a random seed for reproducibility of the simulation results.\n    np.random.seed(42)\n\n    # --- Simulation parameters from the problem statement ---\n    B = 200      # Number of Monte Carlo replicates\n    m = 201      # Number of grid points for variance estimation\n    K = 5        # Number of folds for cross-validation\n    alpha = 1.5  # Parameter for the true function f(x)\n    beta = 0.5   # Parameter for the true function f(x)\n\n    # Test cases: (n, sigma, d_s, d_c)\n    test_cases = [\n        (60, 0.5, 3, 18),   # Case A: Canonical overfitting scenario\n        (400, 0.5, 3, 18),  # Case B: Large sample size scenario\n        (60, 0.0, 3, 18),   # Case C: Noise-free scenario\n    ]\n\n    def calculate_cv_mse(x, y, d, k_folds):\n        \"\"\"\n        Calculates the K-fold cross-validation MSE for a polynomial model.\n        \"\"\"\n        fold_mses = []\n        n_samples = len(x)\n        indices = np.arange(n_samples)\n        \n        # Shuffle indices to ensure random distribution of data across folds.\n        np.random.shuffle(indices)\n        \n        # Split indices into k_folds.\n        fold_indices = np.array_split(indices, k_folds)\n\n        for k in range(k_folds):\n            val_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[i] for i in range(k_folds) if i != k])\n\n            x_fold_train, y_fold_train = x[train_idx], y[train_idx]\n            x_fold_val, y_fold_val = x[val_idx], y[val_idx]\n\n            # Fit model on the training part of the fold.\n            # numpy might issue a RankWarning for high-degree polynomials, \n            # which is expected and can be ignored for this problem.\n            coeffs = np.polyfit(x_fold_train, y_fold_train, d)\n            \n            # Predict on the validation part.\n            y_pred_val = np.polyval(coeffs, x_fold_val)\n\n            # Calculate and store MSE for the fold.\n            fold_mse = np.mean((y_fold_val - y_pred_val)**2)\n            fold_mses.append(fold_mse)\n        \n        return np.mean(fold_mses)\n\n    final_results = []\n\n    for case in test_cases:\n        n, sigma, d_s, d_c = case\n\n        # Accumulators for metrics over B replicates\n        train_mses_s, train_mses_c = [], []\n        cv_mses_s, cv_mses_c = [], []\n        \n        # Grid for variance estimation\n        grid = np.linspace(-1, 1, m)\n        \n        # Storage for predictions on the grid across all replicates\n        all_preds_s = np.zeros((B, m))\n        all_preds_c = np.zeros((B, m))\n\n        for b in range(B):\n            # 1. Generate data for the current replicate\n            x_train = np.random.uniform(-1, 1, n)\n            y_true = alpha * np.sin(np.pi * x_train) + beta * x_train\n            if sigma > 0:\n                noise = np.random.normal(0, sigma, n)\n                y_train = y_true + noise\n            else:\n                y_train = y_true\n\n            # 2. Process simple model (degree d_s)\n            coeffs_s = np.polyfit(x_train, y_train, d_s)\n            y_pred_train_s = np.polyval(coeffs_s, x_train)\n            train_mses_s.append(np.mean((y_train - y_pred_train_s)**2))\n            cv_mses_s.append(calculate_cv_mse(x_train, y_train, d_s, K))\n            all_preds_s[b, :] = np.polyval(coeffs_s, grid)\n\n            # 3. Process complex model (degree d_c)\n            coeffs_c = np.polyfit(x_train, y_train, d_c)\n            y_pred_train_c = np.polyval(coeffs_c, x_train)\n            train_mses_c.append(np.mean((y_train - y_pred_train_c)**2))\n            cv_mses_c.append(calculate_cv_mse(x_train, y_train, d_c, K))\n            all_preds_c[b, :] = np.polyval(coeffs_c, grid)\n\n        # 4. Aggregate results across B replicates\n        avg_train_mse_s = np.mean(train_mses_s)\n        avg_train_mse_c = np.mean(train_mses_c)\n        avg_cv_mse_s = np.mean(cv_mses_s)\n        avg_cv_mse_c = np.mean(cv_mses_c)\n\n        # Calculate variance summary V_d using sample variance (ddof=1)\n        V_s = np.mean(np.var(all_preds_s, axis=0, ddof=1))\n        V_c = np.mean(np.var(all_preds_c, axis=0, ddof=1))\n        \n        # 5. Apply the overfitting manifestation indicator logic\n        cond1 = avg_train_mse_c  avg_train_mse_s\n        cond2 = avg_cv_mse_c > avg_cv_mse_s\n        cond3 = V_c > V_s\n        \n        indicator = 1 if (cond1 and cond2 and cond3) else 0\n        final_results.append(indicator)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "4897610"}, {"introduction": "After observing the bias-variance tradeoff empirically, our next step is to understand it analytically. This exercise [@problem_id:4897593] examines ridge regression, a cornerstone of modern statistics used to prevent overfitting by penalizing large coefficients. You will derive the exact mathematical expressions for the bias and variance of a ridge regression prediction, allowing you to see precisely how the regularization parameter $\\lambda$ creates a tradeoff: as you increase $\\lambda$, you increase bias but decrease variance. The ultimate goal is to find the \"sweet spot\" for $\\lambda$ that minimizes the total expected test error, providing a powerful theoretical justification for the model tuning procedures we use in practice.", "problem": "A biostatistician is modeling a continuous health outcome using a linear model with a fixed design matrix $X \\in \\mathbb{R}^{n \\times p}$ that has full column rank with $p \\leq n$. The response vector is generated by the linear model $y = X \\beta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ independently of $X$, and $\\sigma^{2}  0$. Consider the ridge estimator with penalty parameter $\\lambda  0$ defined by $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} y$. Let $X^{\\top} X$ admit the eigendecomposition $X^{\\top} X = V D V^{\\top}$ with $V \\in \\mathbb{R}^{p \\times p}$ orthogonal and $D = \\operatorname{diag}(d_{1}, \\dots, d_{p})$ with $d_{j}  0$ for all $j \\in \\{1, \\dots, p\\}$. For a new covariate vector $x_{0} \\in \\mathbb{R}^{p}$, the ridge prediction is $\\hat{y}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$.\n\na) Starting from the definitions of statistical bias and variance, derive expressions for the conditional bias $\\operatorname{Bias}_{\\lambda}(x_{0}) = \\mathbb{E}[\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_{0}] - x_{0}^{\\top} \\beta$ and the conditional variance $\\operatorname{Var}_{\\lambda}(x_{0}) = \\operatorname{Var}(\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_{0})$ as explicit functions of $\\lambda$, the eigenvalues $\\{d_{j}\\}_{j=1}^{p}$, the eigenvectors $\\{v_{j}\\}_{j=1}^{p}$ (the columns of $V$), and the coordinates of $x_{0}$ and $\\beta$ in the eigenbasis of $X^{\\top} X$.\n\nb) To connect with model selection by cross-validation, consider the expected test mean squared error at a new point. Let $y_{0} = x_{0}^{\\top} \\beta + \\varepsilon_{0}$ with $\\varepsilon_{0} \\sim \\mathcal{N}(0, \\sigma^{2})$ independent of everything else. Assume the new covariate vector is random with $x_{0} \\sim \\mathcal{N}(0, I_{p})$ and that the true coefficients satisfy $\\beta \\sim \\mathcal{N}(0, \\tau^{2} I_{p})$, with $\\tau^{2}  0$, independent of $x_{0}$ and $\\varepsilon$. Define the conditional expected test risk\n$$\nR(\\lambda) = \\mathbb{E}\\!\\left[ \\left(\\hat{y}_{\\lambda}(x_{0}) - y_{0}\\right)^{2} \\,\\middle|\\, X \\right],\n$$\nwhere the expectation is taken over the joint distribution of $(x_{0}, \\beta, \\varepsilon, \\varepsilon_{0})$ given $X$. Derive $R(\\lambda)$ in terms of $\\lambda$ and $\\{d_{j}\\}_{j=1}^{p}$, and then find the value $\\lambda^{\\star}$ that minimizes $R(\\lambda)$. Provide your final answer for $\\lambda^{\\star}$ as a single closed-form analytic expression. You may briefly comment on how $K$-fold cross-validation (K-CV) can be used to approximate the minimizer using data, but your final reported answer must be $\\lambda^{\\star}$ only. No rounding is needed.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in statistical learning theory, well-posed with all necessary information provided, and stated in objective, formal language. There are no contradictions, ambiguities, or unsound premises. The problem constitutes a standard, non-trivial exercise in deriving and analyzing the properties of ridge regression estimators. Therefore, a full solution is presented.\n\nThe problem is divided into two parts. Part (a) requires the derivation of the conditional bias and variance of the ridge prediction. Part (b) requires the derivation and minimization of the conditional expected test risk under specific distributional assumptions for the covariates and model coefficients.\n\n**Part a) Conditional Bias and Variance**\n\nThe ridge prediction for a new covariate vector $x_{0} \\in \\mathbb{R}^{p}$ is given by $\\hat{y}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$. The ridge estimator $\\hat{\\beta}_{\\lambda}$ is defined as $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} y$. The response vector $y$ is generated by the model $y = X \\beta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$.\n\nFirst, we express the prediction $\\hat{y}_{\\lambda}(x_{0})$ in terms of the true coefficient vector $\\beta$ and the error term $\\varepsilon$:\n$$\n\\hat{y}_{\\lambda}(x_{0}) = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} (X \\beta + \\varepsilon)\n$$\n$$\n\\hat{y}_{\\lambda}(x_{0}) = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta + x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\varepsilon\n$$\nThis expression separates the prediction into a component related to the true signal and a component related to the noise.\n\n**Derivation of Conditional Bias**\n\nThe conditional bias is defined as $\\operatorname{Bias}_{\\lambda}(x_{0}) = \\mathbb{E}[\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_{0}] - x_{0}^{\\top} \\beta$. The expectation is taken with respect to the distribution of $y$, which is determined by the random error $\\varepsilon$, conditional on $X$ and $x_{0}$.\n$$\n\\mathbb{E}[\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_{0}] = \\mathbb{E}[x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta + x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\varepsilon \\mid X, x_{0}]\n$$\nSince $X$, $x_{0}$, and $\\beta$ are considered fixed in this conditioning, and $\\mathbb{E}[\\varepsilon \\mid X] = \\mathbb{E}[\\varepsilon] = 0$, the second term's expectation is zero.\n$$\n\\mathbb{E}[\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_{0}] = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta\n$$\nNow, we substitute this back into the bias definition:\n$$\n\\operatorname{Bias}_{\\lambda}(x_{0}) = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta - x_{0}^{\\top} \\beta\n$$\n$$\n\\operatorname{Bias}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\left[ (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - I_{p} \\right] \\beta\n$$\nTo simplify the term in the brackets, let $A = X^{\\top}X$. The expression is $(A + \\lambda I_{p})^{-1}A - I_{p}$.\n$$\n(A + \\lambda I_{p})^{-1}A - I_{p} = (A + \\lambda I_{p})^{-1}A - (A + \\lambda I_{p})^{-1}(A + \\lambda I_{p}) = (A + \\lambda I_{p})^{-1} (A - (A + \\lambda I_{p})) = -\\lambda (A + \\lambda I_{p})^{-1}\n$$\nThus, the bias is:\n$$\n\\operatorname{Bias}_{\\lambda}(x_{0}) = -\\lambda x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} \\beta\n$$\nTo express this in terms of the eigendecomposition $X^{\\top} X = V D V^{\\top}$, we substitute it into the expression:\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} = (V D V^{\\top} + \\lambda V V^{\\top})^{-1} = (V(D + \\lambda I_{p})V^{\\top})^{-1} = V(D + \\lambda I_{p})^{-1}V^{\\top}\n$$\nThe bias becomes:\n$$\n\\operatorname{Bias}_{\\lambda}(x_{0}) = -\\lambda (x_{0}^{\\top} V) (D + \\lambda I_{p})^{-1} (V^{\\top} \\beta)\n$$\nLet $\\tilde{x}_{0} = V^{\\top}x_{0}$ and $\\tilde{\\beta} = V^{\\top}\\beta$ be the coordinates of $x_{0}$ and $\\beta$ in the basis of eigenvectors $\\{v_{j}\\}_{j=1}^{p}$. The components are $\\tilde{x}_{0,j} = v_{j}^{\\top}x_{0}$ and $\\tilde{\\beta}_{j} = v_{j}^{\\top}\\beta$. The matrix $(D + \\lambda I_{p})^{-1}$ is diagonal with entries $1/(d_{j} + \\lambda)$. The expression becomes a sum:\n$$\n\\operatorname{Bias}_{\\lambda}(x_{0}) = -\\lambda \\sum_{j=1}^{p} \\frac{\\tilde{x}_{0,j} \\tilde{\\beta}_{j}}{d_{j} + \\lambda} = -\\lambda \\sum_{j=1}^{p} \\frac{(v_{j}^{\\top} x_{0}) (v_{j}^{\\top} \\beta)}{d_{j} + \\lambda}\n$$\n\n**Derivation of Conditional Variance**\n\nThe conditional variance is defined as $\\operatorname{Var}_{\\lambda}(x_{0}) = \\operatorname{Var}(\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_{0})$. Given $X$ and $x_{0}$, the randomness in $\\hat{y}_{\\lambda}(x_{0})$ comes solely from $\\varepsilon$.\n$$\n\\operatorname{Var}(\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_{0}) = \\operatorname{Var}(x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta + x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\varepsilon \\mid X, x_{0})\n$$\nThe first term is constant with respect to $\\varepsilon$. Let the vector $c^{\\top} = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top}$. The variance is $\\operatorname{Var}(c^{\\top}\\varepsilon \\mid X, x_{0})$. Using properties of variance of linear combinations of random variables, and that $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I_n$:\n$$\n\\operatorname{Var}(c^{\\top}\\varepsilon) = c^{\\top} \\operatorname{Var}(\\varepsilon) c = c^{\\top}(\\sigma^{2} I_{n})c = \\sigma^{2} c^{\\top}c\n$$\nWe compute the quadratic form $c^{\\top}c$:\n$$\nc^{\\top}c = \\left( x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\right) \\left( X (X^{\\top} X + \\lambda I_{p})^{-1} x_{0} \\right)\n$$\n$$\nc^{\\top}c = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X) (X^{\\top} X + \\lambda I_{p})^{-1} x_{0}\n$$\nUsing the eigendecomposition $X^{\\top} X = VDV^{\\top}$:\n$$\nc^{\\top}c = (x_{0}^{\\top} V) (D + \\lambda I_{p})^{-1} V^{\\top} (VDV^{\\top}) V (D + \\lambda I_{p})^{-1} (V^{\\top} x_{0})\n$$\nSince $V^{\\top}V = I_{p}$, this simplifies to:\n$$\nc^{\\top}c = (V^{\\top} x_{0})^{\\top} (D + \\lambda I_{p})^{-1} D (D + \\lambda I_{p})^{-1} (V^{\\top} x_{0})\n$$\nThe matrix in the middle is diagonal, with entries $d_{j} / (d_{j} + \\lambda)^{2}$. In terms of the coordinates $\\tilde{x}_{0,j} = v_{j}^{\\top}x_{0}$:\n$$\nc^{\\top}c = \\sum_{j=1}^{p} \\frac{d_{j}}{(d_{j} + \\lambda)^{2}} \\tilde{x}_{0,j}^{2} = \\sum_{j=1}^{p} \\frac{d_{j} (v_{j}^{\\top} x_{0})^{2}}{(d_{j} + \\lambda)^{2}}\n$$\nTherefore, the conditional variance is:\n$$\n\\operatorname{Var}_{\\lambda}(x_{0}) = \\sigma^{2} \\sum_{j=1}^{p} \\frac{d_{j} (v_{j}^{\\top} x_{0})^{2}}{(d_{j} + \\lambda)^{2}}\n$$\n\n**Part b) Minimization of Conditional Expected Test Risk**\n\nThe conditional expected test risk is $R(\\lambda) = \\mathbb{E}[ (\\hat{y}_{\\lambda}(x_{0}) - y_{0})^{2} \\mid X ]$, where the expectation is over the joint distribution of $(x_{0}, \\beta, \\varepsilon, \\varepsilon_{0})$. We use the law of total expectation: $R(\\lambda) = \\mathbb{E}_{x_0, \\beta} [ \\mathbb{E}_{\\varepsilon, \\varepsilon_0} [ (\\hat{y}_{\\lambda}(x_{0}) - y_{0})^{2} \\mid X, x_0, \\beta ] \\mid X ]$.\n\nFor a fixed $x_0$ and $\\beta$, the term $\\hat{y}_{\\lambda}(x_0) - y_0$ can be decomposed:\n$$\n\\hat{y}_{\\lambda}(x_{0}) - y_{0} = (\\mathbb{E}[\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_0, \\beta] - x_{0}^{\\top}\\beta) + (\\hat{y}_{\\lambda}(x_{0}) - \\mathbb{E}[\\hat{y}_{\\lambda}(x_{0}) \\mid X, x_0, \\beta]) - \\varepsilon_{0}\n$$\nThe three terms are the model bias, the estimation error from $\\varepsilon$, and the irreducible error $\\varepsilon_0$. Squaring and taking the expectation $\\mathbb{E}_{\\varepsilon, \\varepsilon_{0}}[\\cdot]$, the cross-product terms vanish because the three components are uncorrelated and have zero mean. This yields the familiar decomposition:\n$$\n\\mathbb{E}_{\\varepsilon, \\varepsilon_0}[(\\hat{y}_{\\lambda}(x_{0}) - y_{0})^2 | X, x_0, \\beta] = (\\operatorname{Bias}_{\\lambda}(x_{0}))^2 + \\operatorname{Var}_{\\lambda}(x_{0}) + \\sigma^{2}\n$$\nNow we compute $R(\\lambda)$ by taking the expectation over $x_0 \\sim \\mathcal{N}(0, I_{p})$ and $\\beta \\sim \\mathcal{N}(0, \\tau^{2} I_{p})$:\n$$\nR(\\lambda) = \\mathbb{E}_{x_{0},\\beta}[(\\operatorname{Bias}_{\\lambda}(x_{0}))^2 | X] + \\mathbb{E}_{x_{0}}[\\operatorname{Var}_{\\lambda}(x_{0}) | X] + \\sigma^{2}\n$$\nLet's evaluate the two expectation terms.\n\n1. Integrated Squared Bias:\nLet $A = -\\lambda(X^{\\top}X + \\lambda I_{p})^{-1}$. The bias is $x_{0}^{\\top} A \\beta$.\n$$\n\\mathbb{E}_{x_{0},\\beta}[(x_{0}^{\\top} A \\beta)^2] = \\mathbb{E}_{x_{0},\\beta}[\\beta^{\\top} A^{\\top} x_0 x_0^{\\top} A \\beta] = \\mathbb{E}_{\\beta}[\\beta^{\\top} A^{\\top} \\mathbb{E}_{x_{0}}[x_0 x_0^{\\top}] A \\beta]\n$$\nGiven $x_0 \\sim \\mathcal{N}(0, I_p)$, $\\mathbb{E}[x_0 x_0^{\\top}] = I_p$.\n$$\n= \\mathbb{E}_{\\beta}[\\beta^{\\top} A^{\\top} A \\beta] = \\mathbb{E}_{\\beta}[\\operatorname{tr}(\\beta^{\\top} A^{\\top} A \\beta)] = \\mathbb{E}_{\\beta}[\\operatorname{tr}(A^{\\top} A \\beta \\beta^{\\top})] = \\operatorname{tr}(A^{\\top} A \\mathbb{E}_{\\beta}[\\beta \\beta^{\\top}])\n$$\nGiven $\\beta \\sim \\mathcal{N}(0, \\tau^2 I_p)$, $\\mathbb{E}[\\beta \\beta^{\\top}] = \\tau^2 I_p$. The matrix $A$ is symmetric.\n$$\n= \\operatorname{tr}(A^2 (\\tau^2 I_p)) = \\tau^2 \\operatorname{tr}(A^2) = \\tau^2 \\operatorname{tr}(\\lambda^2 (X^{\\top}X+\\lambda I_p)^{-2})\n$$\nIn the eigenbasis, the matrix inside the trace is $\\operatorname{diag}(\\lambda^2 / (d_j+\\lambda)^2)$.\n$$\n\\mathbb{E}_{x_{0},\\beta}[(\\operatorname{Bias}_{\\lambda}(x_{0}))^2 | X] = \\tau^2 \\lambda^2 \\sum_{j=1}^{p} \\frac{1}{(d_{j}+\\lambda)^{2}}\n$$\n\n2. Integrated Variance:\nThe variance is $\\sigma^2 x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X) (X^{\\top} X + \\lambda I_{p})^{-1} x_0$. Let $B = \\sigma^{2} (X^{\\top}X + \\lambda I_{p})^{-1} (X^{\\top} X) (X^{\\top} X + \\lambda I_{p})^{-1}$.\n$$\n\\mathbb{E}_{x_{0}}[\\operatorname{Var}_{\\lambda}(x_{0}) | X] = \\mathbb{E}_{x_{0}}[x_{0}^{\\top} B x_{0}] = \\mathbb{E}_{x_{0}}[\\operatorname{tr}(x_{0}^{\\top} B x_{0})] = \\operatorname{tr}(B \\mathbb{E}_{x_{0}}[x_0 x_0^{\\top}])\n$$\nSince $\\mathbb{E}[x_0 x_0^{\\top}]=I_p$, this is $\\operatorname{tr}(B)$.\n$$\n\\mathbb{E}_{x_{0}}[\\operatorname{Var}_{\\lambda}(x_{0}) | X] = \\operatorname{tr}(\\sigma^{2} (X^{\\top}X + \\lambda I)^{-1} X^{\\top}X (X^{\\top}X + \\lambda I)^{-1})\n$$\nIn the eigenbasis, the matrix inside the trace is $\\operatorname{diag}(\\sigma^2 d_j / (d_j+\\lambda)^2)$.\n$$\n\\mathbb{E}_{x_{0}}[\\operatorname{Var}_{\\lambda}(x_{0}) | X] = \\sigma^{2} \\sum_{j=1}^{p} \\frac{d_{j}}{(d_{j}+\\lambda)^{2}}\n$$\n\nCombining the terms, the full risk expression is:\n$$\nR(\\lambda) = \\tau^{2} \\sum_{j=1}^{p} \\frac{\\lambda^{2}}{(d_{j}+\\lambda)^{2}} + \\sigma^{2} \\sum_{j=1}^{p} \\frac{d_{j}}{(d_{j}+\\lambda)^{2}} + \\sigma^{2}\n$$\nTo find the optimal $\\lambda^{\\star}$, we differentiate $R(\\lambda)$ with respect to $\\lambda$ and set it to zero.\n$$\n\\frac{d}{d\\lambda} \\left( \\frac{\\lambda^2}{(d_j+\\lambda)^2} \\right) = \\frac{2\\lambda(d_j+\\lambda)^2 - \\lambda^2 \\cdot 2(d_j+\\lambda)}{(d_j+\\lambda)^4} = \\frac{2\\lambda(d_j+\\lambda) - 2\\lambda^2}{(d_j+\\lambda)^3} = \\frac{2\\lambda d_j}{(d_j+\\lambda)^3}\n$$\n$$\n\\frac{d}{d\\lambda} \\left( \\frac{d_j}{(d_j+\\lambda)^2} \\right) = d_j(-2)(d_j+\\lambda)^{-3} = \\frac{-2d_j}{(d_j+\\lambda)^3}\n$$\nThe derivative of $R(\\lambda)$ is:\n$$\n\\frac{dR}{d\\lambda} = \\tau^{2} \\sum_{j=1}^{p} \\frac{2\\lambda d_j}{(d_{j}+\\lambda)^{3}} - \\sigma^{2} \\sum_{j=1}^{p} \\frac{2d_j}{(d_{j}+\\lambda)^{3}}\n$$\n$$\n\\frac{dR}{d\\lambda} = (\\tau^{2} \\lambda - \\sigma^{2}) \\sum_{j=1}^{p} \\frac{2d_j}{(d_{j}+\\lambda)^{3}}\n$$\nWe set the derivative to zero. Since $d_j  0$ and $\\lambda  0$, the sum term $\\sum_{j=1}^{p} \\frac{2d_j}{(d_{j}+\\lambda)^{3}}$ is strictly positive. Therefore, the derivative is zero if and only if the other factor is zero:\n$$\n\\tau^{2} \\lambda - \\sigma^{2} = 0\n$$\nThis gives the optimal value $\\lambda^{\\star}$:\n$$\n\\lambda^{\\star} = \\frac{\\sigma^{2}}{\\tau^{2}}\n$$\nThe second derivative test confirms this is a minimum, as the second derivative evaluated at $\\lambda^{\\star}$ is $\\tau^2 \\sum_{j=1}^{p} \\frac{2d_j}{(d_{j}+\\lambda^\\star)^{3}}  0$.\n\nThis result indicates that the optimal regularization strength is the ratio of noise variance to signal (coefficient) variance. In practice, $\\sigma^2$ and $\\tau^2$ are unknown. $K$-fold cross-validation (K-CV) is a method to estimate the minimizer of the predictive risk from data. One splits the data into $K$ folds, trains the model on $K-1$ folds for a range of $\\lambda$ values, and evaluates the prediction error on the held-out fold. The average error across all folds serves as an estimate of the expected test risk $R(\\lambda)$. The $\\lambda$ that minimizes this average cross-validated error is chosen as an empirical approximation of $\\lambda^{\\star}$.", "answer": "$$\n\\boxed{\\frac{\\sigma^2}{\\tau^2}}\n$$", "id": "4897593"}, {"introduction": "Properly implementing cross-validation is critical for obtaining reliable model performance estimates, but it is fraught with subtle pitfalls. This advanced exercise tackles one of the most common and dangerous errors in applied machine learning: data leakage. Here, feature selection is performed on the entire dataset *before* the cross-validation process begins, allowing information about the test sets to \"leak\" into the model training procedure. This practice [@problem_id:4897575] challenges you to analytically derive the exact amount of optimistic bias this mistake introduces, demonstrating quantitatively how it can make a useless model appear highly predictive. Completing this exercise will instill a deep appreciation for the discipline required to maintain a strict separation between training and validation data.", "problem": "A biostatistics team is evaluating a linear predictor to forecast a standardized quantitative phenotype. For each subject $i \\in \\{1,\\dots,n\\}$ and each biomarker $j \\in \\{1,\\dots,p\\}$, assume the following data-generating process under a null model with no true associations:\n- $y_i \\sim \\mathcal{N}(0,1)$ independently across $i$.\n- $x_{ij} \\sim \\mathcal{N}(0,1)$ independently across $i$ and $j$, and independent of $y_i$.\n\nThey mistakenly perform feature selection using the full dataset before model evaluation as follows.\n- Step $1$ (leaky feature selection on full data): compute the sample correlation $r_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij} y_i$ for each biomarker $j$ on the full dataset, and select the single biomarker $j^{\\star}$ with the largest absolute correlation, that is $j^{\\star} \\in \\arg\\max_{1 \\leq j \\leq p} |r_j|$.\n- Step $2$ (evaluation by cross-validation reusing the selected feature): estimate the mean squared prediction error by Leave-One-Out Cross-Validation (LOOCV), refitting ordinary least squares without an intercept on the training set of size $n-1$ in each fold to obtain a coefficient $\\hat{\\beta}_{(-i)}$ for the fixed biomarker $x_{ij^{\\star}}$, and predicting $\\hat{y}_i = \\hat{\\beta}_{(-i)} x_{ij^{\\star}}$ for the left-out subject $i$.\n\nLet $\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}$ denote the resulting LOOCV estimate of mean squared error produced by the above leakage-prone procedure, and let $\\mathrm{MSE}_{\\mathrm{test}}$ denote the true expected mean squared prediction error on an independent new subject when the procedure is trained on $n$ observations and then applied to the new subject (still using the feature $j^{\\star}$ selected on the full $n$ training observations as above). Define the bias\n$$\nB(n,p) \\equiv \\mathbb{E}\\!\\left[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}\\right] \\;-\\; \\mathbb{E}\\!\\left[\\mathrm{MSE}_{\\mathrm{test}}\\right].\n$$\n\nAssume $n$ is large, $p$ may grow with $n$ with $\\ln p = o(n)$, and ignore all terms that are lower order than $\\frac{\\ln p}{n}$. Starting from fundamental definitions and standard large-sample approximations, derive the leading-order asymptotic expression for $B(n,p)$ as a function of $n$ and $p$ under the null model described. Provide your answer as a single closed-form analytic expression. No rounding is required and no units are needed.", "solution": "The user wants to find the leading-order asymptotic expression for the bias $B(n,p) = \\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}] - \\mathbb{E}[\\mathrm{MSE}_{\\text{test}}]$. We will derive expressions for the two expectations separately and then compute their difference.\n\n### Step 1: Preliminary Definitions and Approximations\n\nLet $X$ be the $n \\times p$ matrix of biomarker data, and $y$ be the $n \\times 1$ vector of phenotype data. The elements $x_{ij}$ and $y_i$ are i.i.d. $\\mathcal{N}(0,1)$.\nThe sample correlation for biomarker $j$ is $r_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij} y_i$. Let $C_j = \\sum_{i=1}^{n} x_{ij} y_i = n r_j$.\nThe sum of squares for predictor $j$ is $S_j = \\sum_{i=1}^{n} x_{ij}^2$. By the law of large numbers, for large $n$, $S_j/n \\to \\mathbb{E}[x_{ij}^2] = 1$. We will use the approximation $S_j \\approx n$. This holds for all $j$, including the selected one $j^{\\star}$.\nThe feature selection rule is $j^{\\star} = \\arg\\max_{1 \\leq j \\leq p} |r_j| = \\arg\\max_{1 \\leq j \\leq p} |C_j|$.\n\nTo analyze the selection process, we define normalized variables $\\tilde{r}_j = C_j / \\sqrt{S_j}$. Conditional on $X$, each $\\tilde{r}_j$ is a linear combination of i.i.d. standard normal variables $y_i$: $\\tilde{r}_j = \\sum_i (x_{ij}/\\sqrt{S_j}) y_i$. Thus, $\\mathbb{E}[\\tilde{r}_j|X] = 0$ and $\\mathrm{Var}(\\tilde{r}_j|X) = \\sum_i (x_{ij}/\\sqrt{S_j})^2 \\mathrm{Var}(y_i) = S_j/S_j = 1$. So, $\\tilde{r}_j | X \\sim \\mathcal{N}(0,1)$.\nThe covariance is $\\mathrm{Cov}(\\tilde{r}_j, \\tilde{r}_k|X) = \\frac{\\sum_i x_{ij}x_{ik}}{\\sqrt{S_j S_k}}$, which for large $n$ converges to $0$ for $j \\neq k$. Therefore, for large $n$, the variables $\\{\\tilde{r}_j\\}_{j=1}^p$ can be treated as i.i.d. $\\mathcal{N}(0,1)$ random variables.\n\nThe selection rule $j^{\\star} = \\arg\\max_j |C_j| = \\arg\\max_j |\\sqrt{S_j} \\tilde{r}_j|$ is approximately equivalent to $j^{\\star} \\approx \\arg\\max_j |\\tilde{r}_j|$ since $S_j \\approx n$ for all $j$. Let $M_p^2 = \\max_{1 \\leq j \\leq p} \\tilde{r}_j^2$. For large $p$, the expectation of the maximum of $p$ i.i.d. $\\chi^2(1)$ variables is well-approximated by $\\mathbb{E}[M_p^2] \\approx 2 \\ln p$.\n\n### Step 2: Derivation of $\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}]$\n\nThe true test error, $\\mathrm{MSE}_{\\text{test}}$, is the expected squared prediction error for a new subject $(x_{\\text{new}}, y_{\\text{new}})$ drawn from the same distribution, independent of the training data. The prediction is $\\hat{y}_{\\text{new}} = \\hat{\\beta} x_{\\text{new}, j^{\\star}}$, where $\\hat{\\beta}$ is the OLS coefficient estimated from the full training set of size $n$.\n$$\n\\mathrm{MSE}_{\\text{test}} = \\mathbb{E}_{\\text{new}}[(y_{\\text{new}} - \\hat{\\beta} x_{\\text{new}, j^{\\star}})^2 | X, y]\n$$\nExpanding this and using the independence of the new subject ($\\mathbb{E}[y_{\\text{new}}] = 0$, $\\mathbb{E}[y_{\\text{new}}^2]=1$, $\\mathbb{E}[x_{\\text{new}, j^{\\star}}^2]=1$):\n$$\n\\mathrm{MSE}_{\\text{test}} = \\mathbb{E}_{\\text{new}}[y_{\\text{new}}^2] - 2\\hat{\\beta}\\mathbb{E}_{\\text{new}}[y_{\\text{new}}x_{\\text{new}, j^{\\star}}] + \\hat{\\beta}^2\\mathbb{E}_{\\text{new}}[x_{\\text{new}, j^{\\star}}^2] = 1 - 0 + \\hat{\\beta}^2 = 1 + \\hat{\\beta}^2\n$$\nThe OLS estimate without an intercept is $\\hat{\\beta} = \\frac{\\sum_i x_{ij^{\\star}}y_i}{\\sum_i x_{ij^{\\star}}^2} = \\frac{C_{j^{\\star}}}{S_{j^{\\star}}}$.\nThus, $\\mathrm{MSE}_{\\text{test}} = 1 + \\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}^2}$.\nWe need to compute the expectation over the training data distribution:\n$$\n\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}] = 1 + \\mathbb{E}\\left[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}^2}\\right]\n$$\nUsing the approximation $S_{j^{\\star}} \\approx n$:\n$$\n\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}] \\approx 1 + \\frac{1}{n^2}\\mathbb{E}[C_{j^{\\star}}^2]\n$$\nWe have $C_{j^{\\star}} = \\sqrt{S_{j^{\\star}}} \\tilde{r}_{j^{\\star}}$, where $|\\tilde{r}_{j^{\\star}}|^2 \\approx M_p^2$.\nSo, $\\mathbb{E}[C_{j^{\\star}}^2] = \\mathbb{E}[S_{j^{\\star}} \\tilde{r}_{j^{\\star}}^2] \\approx \\mathbb{E}[S_{j^{\\star}}] \\mathbb{E}[\\tilde{r}_{j^{\\star}}^2] \\approx n \\mathbb{E}[M_p^2]$.\nUsing $\\mathbb{E}[M_p^2] \\approx 2 \\ln p$, we get $\\mathbb{E}[C_{j^{\\star}}^2] \\approx 2n \\ln p$.\nSubstituting this into the expression for the expected test error:\n$$\n\\mathbb{E}[\\mathrm{MSE}_{\\text{test}}] \\approx 1 + \\frac{1}{n^2}(2n \\ln p) = 1 + \\frac{2 \\ln p}{n}\n$$\n\n### Step 3: Derivation of $\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}]$\n\nThe LOOCV error estimate is $\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$, where $\\hat{y}_i = \\hat{\\beta}_{(-i)} x_{ij^{\\star}}$.\nA standard result for OLS relates the LOOCV residual to the residual from the full model fit. For a no-intercept model, this identity is:\n$$\ny_i - \\hat{y}_i = \\frac{y_i - \\hat{\\beta} x_{ij^{\\star}}}{1 - h_{ii}}\n$$\nwhere $h_{ii}$ is the $i$-th diagonal element of the hat matrix $H = x_{j^{\\star}}(x_{j^{\\star}}^Tx_{j^{\\star}})^{-1}x_{j^{\\star}}^T$.\n$h_{ii} = \\frac{x_{ij^{\\star}}^2}{\\sum_{k=1}^n x_{kj^{\\star}}^2} = \\frac{x_{ij^{\\star}}^2}{S_{j^{\\star}}}$.\nFor large $n$, $S_{j^{\\star}} \\approx n$, so $h_{ii} = O_p(1/n)$.\nThe LOOCV estimate can be written as:\n$$\n\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{(y_i - \\hat{\\beta}x_{ij^{\\star}})^2}{(1-h_{ii})^2}\n$$\nLet $\\widehat{\\mathrm{MSE}}_{\\text{in}} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{\\beta}x_{ij^{\\star}})^2$ be the in-sample (training) MSE. Using the expansion $(1-h_{ii})^{-2} = 1 + 2h_{ii} + O_p(h_{ii}^2)$:\n$$\n\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}} \\approx \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\beta}x_{ij^{\\star}})^2(1+2h_{ii}) = \\widehat{\\mathrm{MSE}}_{\\text{in}} + \\frac{2}{n}\\sum_{i=1}^{n} (y_i - \\hat{\\beta}x_{ij^{\\star}})^2 h_{ii}\n$$\nFirst, we compute the expectation of the in-sample MSE:\n$$\n\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_i(y_i - \\hat{\\beta}x_{ij^{\\star}})^2\\right] = \\frac{1}{n}\\mathbb{E}\\left[\\sum_i y_i^2 - \\hat{\\beta}^2 S_{j^{\\star}}\\right]\n$$\nThis uses the orthogonality property of OLS residuals. $\\mathbb{E}[\\sum y_i^2]=n$.\n$\\hat{\\beta}^2 S_{j^{\\star}} = (\\frac{C_{j^{\\star}}}{S_{j^{\\star}}})^2 S_{j^{\\star}} = \\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}$.\n$\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] = \\frac{1}{n}\\left(n - \\mathbb{E}\\left[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}\\right]\\right) = 1 - \\frac{1}{n}\\mathbb{E}\\left[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}\\right]$.\nAs established before, $\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}} \\approx M_p^2$, so $\\mathbb{E}[\\frac{C_{j^{\\star}}^2}{S_{j^{\\star}}}] \\approx 2 \\ln p$.\nTherefore, the expected in-sample error is highly optimistic:\n$$\n\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] \\approx 1 - \\frac{2 \\ln p}{n}\n$$\nNext, consider the correction term $\\frac{2}{n}\\mathbb{E}[\\sum_i (y_i - \\hat{\\beta}x_{ij^{\\star}})^2 h_{ii}]$.\nBy symmetry, this is $2\\mathbb{E}[(y_1 - \\hat{\\beta}x_{1j^{\\star}})^2 h_{11}]$.\n$h_{11} = x_{1j^{\\star}}^2/S_{j^{\\star}} \\approx x_{1j^{\\star}}^2/n$. The term is $\\frac{2}{n}\\mathbb{E}[(y_1 - \\hat{\\beta}x_{1j^{\\star}})^2 x_{1j^{\\star}}^2]$.\nFor large $n$, $\\hat{\\beta} = O_p(\\sqrt{\\ln p / n})$ which goes to $0$. Thus, $y_1 - \\hat{\\beta}x_{1j^{\\star}} \\approx y_1$. The expectation becomes $\\frac{2}{n}\\mathbb{E}[y_1^2 x_{1j^{\\star}}^2]$. Since $y_1$ and $x_{1j^{\\star}}$ are independent standard normals (selection of $j^{\\star}$ is a low-probability event for any single row), $\\mathbb{E}[y_1^2 x_{1j^{\\star}}^2] = \\mathbb{E}[y_1^2]\\mathbb{E}[x_{1j^{\\star}}^2]=1$.\nSo the correction term is approximately $2/n$.\nCombining the terms for the expected LOOCV error:\n$$\n\\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}] \\approx \\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\text{in}}] + \\frac{2}{n} \\approx \\left(1 - \\frac{2 \\ln p}{n}\\right) + \\frac{2}{n}\n$$\n\n### Step 4: Calculation of the Bias $B(n,p)$\n\nThe bias is the difference between the two derived expectations.\n$$\nB(n,p) = \\mathbb{E}[\\widehat{\\mathrm{MSE}}_{\\mathrm{LOOCV, leak}}] - \\mathbb{E}[\\mathrm{MSE}_{\\text{test}}]\n$$\nSubstituting the derived expressions:\n$$\nB(n,p) \\approx \\left(1 - \\frac{2 \\ln p}{n} + \\frac{2}{n}\\right) - \\left(1 + \\frac{2 \\ln p}{n}\\right)\n$$\n$$\nB(n,p) \\approx -\\frac{4 \\ln p}{n} + \\frac{2}{n}\n$$\nThe problem asks for the leading-order asymptotic expression and to ignore terms of lower order than $\\frac{\\ln p}{n}$. Assuming $p$ is large enough or grows with $n$ such that $\\ln p \\to \\infty$, the term $\\frac{2}{n}$ is of lower order than $\\frac{4 \\ln p}{n}$. We therefore drop it.\nThe leading-order asymptotic expression for the bias is:\n$$\nB(n,p) \\approx -\\frac{4 \\ln p}{n}\n$$\nThis negative bias indicates that the leaky cross-validation procedure is substantially optimistic, underestimating the true prediction error due to the feature selection being performed on the full dataset before cross-validation.", "answer": "$$\n\\boxed{-\\frac{4 \\ln(p)}{n}}\n$$", "id": "4897575"}]}