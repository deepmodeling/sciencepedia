{"hands_on_practices": [{"introduction": "To understand influence, we must first grasp the concept of leverage, which measures a data point's potential to affect the regression model based on its position in the predictor space. This exercise provides a clear, hands-on demonstration by exploring a dataset with duplicate design points, showing why their leverages must be identical [@problem_id:4916300]. You will then see how this potential influence translates into actual influence by calculating Cook's distance, a metric that combines leverage with the model's prediction error for that point.", "problem": "Consider a simple linear regression model with an intercept and a single covariate, fit by Ordinary Least Squares (OLS). Let the design matrix be\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4\n\\end{pmatrix},\n$$\nso that observations $i=3$ and $i=4$ are duplicate design points with identical covariate vectors. Assume the classical linear model assumptions for OLS hold. Let the response vector be\n$$\ny \\;=\\; \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 + d \\\\\n3 \\\\\n4 \\\\\n5\n\\end{pmatrix},\n$$\nwhere $d$ is a real constant. Starting only from the core definitions of OLS as an orthogonal projection onto the column space of $X$, and the standard influence diagnostics framework, proceed as follows:\n\n- Use the projection framework to compute the leverages $h_{33}$ and $h_{44}$ and show they are identical. Briefly explain how such duplicate design points affect leverage and influence diagnostics conceptually.\n\n- Then, derive and compute the Cook’s distance for observation $i=3$ in this model. Your derivation should begin from first principles (the OLS projection and the definition of influence via case-deletion), and lead to a closed-form numerical value for the Cook’s distance of observation $i=3$ in terms of the given $X$ and $y$. \n\nProvide the final Cook’s distance for observation $i=3$ as a single exact number. No rounding is required.", "solution": "The problem is subjected to validation against the specified criteria.\n\n### Step 1: Extract Givens\n- **Model:** Simple linear regression, $y = \\beta_0 + \\beta_1 x + \\epsilon$, fit by Ordinary Least Squares (OLS).\n- **Design Matrix, $X$**:\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4\n\\end{pmatrix}\n$$\n- **Response Vector, $y$**:\n$$\ny \\;=\\; \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 + d \\\\\n3 \\\\\n4 \\\\\n5\n\\end{pmatrix}\n$$\n- **Parameters:** $n=6$ observations, $p=2$ regression coefficients (intercept and slope). $d$ is a real constant.\n- **Assumptions:** Classical linear model assumptions for OLS hold.\n- **Tasks:**\n    1. Compute leverages $h_{33}$ and $h_{44}$ and show they are identical.\n    2. Explain the effect of duplicate design points.\n    3. Derive and compute the Cook's distance for observation $i=3$, $D_3$, from first principles.\n    4. Provide $D_3$ as a single exact number.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is set within the standard framework of linear regression theory, specifically influence diagnostics. All concepts (OLS, leverage, Cook's distance) are well-defined and fundamental in statistics. The problem is scientifically sound.\n2.  **Well-Posed:** The design matrix $X$ has two columns. The first is a vector of ones. The second is $[0, 1, 2, 2, 3, 4]^T$. These vectors are linearly independent, so the matrix $X$ has full column rank. This ensures that the matrix $X^T X$ is invertible and the OLS estimates are unique. The problem provides all necessary information. The presence of the symbolic constant $d$ might suggest the result depends on it, but as will be shown, it cancels out in the final calculation of Cook's distance. Therefore, the request for a \"single exact number\" is consistent with the problem's structure. The problem is well-posed.\n3.  **Objective:** The problem is stated using precise mathematical definitions and objective language. There are no subjective or opinion-based statements.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution\n\nThe Ordinary Least Squares (OLS) framework seeks to find the vector of coefficients $\\hat{\\beta}$ that minimizes the sum of squared residuals $S(\\beta) = (y-X\\beta)^T(y-X\\beta)$. The solution to this minimization problem is geometrically interpreted as the orthogonal projection of the response vector $y$ onto the column space of the design matrix $X$. The vector of fitted values $\\hat{y}$ is this projection:\n$$\n\\hat{y} = P_X y = X(X^T X)^{-1} X^T y\n$$\nThe matrix $H = X(X^T X)^{-1} X^T$ is known as the \"hat matrix\" because it maps $y$ to $\\hat{y}$. The diagonal elements of this matrix, $h_{ii}$, are called the leverages of the observations.\n\n**Part 1: Leverage Calculation and Interpretation**\n\nThe leverage of the $i$-th observation is given by $h_{ii} = x_i^T (X^T X)^{-1} x_i$, where $x_i^T$ is the $i$-th row of the design matrix $X$. To compute the leverages, we first need to calculate $(X^T X)^{-1}$.\n\nThe design matrix is $X = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 2 \\\\ 1 & 3 \\\\ 1 & 4 \\end{pmatrix}$.\nIts transpose is $X^T = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\\\ 0 & 1 & 2 & 2 & 3 & 4 \\end{pmatrix}$.\n\nThe product $X^T X$ is:\n$$\nX^T X = \\begin{pmatrix} 6 & \\sum_{i=1}^6 x_{i2} \\\\ \\sum_{i=1}^6 x_{i2} & \\sum_{i=1}^6 x_{i2}^2 \\end{pmatrix} = \\begin{pmatrix} 6 & 0+1+2+2+3+4 \\\\ 0+1+2+2+3+4 & 0^2+1^2+2^2+2^2+3^2+4^2 \\end{pmatrix} = \\begin{pmatrix} 6 & 12 \\\\ 12 & 34 \\end{pmatrix}\n$$\nThe determinant is $\\det(X^T X) = (6)(34) - (12)(12) = 204 - 144 = 60$.\nThe inverse is:\n$$\n(X^T X)^{-1} = \\frac{1}{60} \\begin{pmatrix} 34 & -12 \\\\ -12 & 6 \\end{pmatrix} = \\begin{pmatrix} \\frac{34}{60} & -\\frac{12}{60} \\\\ -\\frac{12}{60} & \\frac{6}{60} \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{30} & -\\frac{1}{5} \\\\ -\\frac{1}{5} & \\frac{1}{10} \\end{pmatrix}\n$$\nNow, we compute the leverage for observation $i=3$. The covariate vector is $x_3^T = \\begin{pmatrix} 1 & 2 \\end{pmatrix}$.\n$$\nh_{33} = x_3^T (X^T X)^{-1} x_3 = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{17}{30} & -\\frac{1}{5} \\\\ -\\frac{1}{5} & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nh_{33} = \\begin{pmatrix} 1 \\cdot \\frac{17}{30} + 2 \\cdot (-\\frac{1}{5}) & 1 \\cdot (-\\frac{1}{5}) + 2 \\cdot \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{30} - \\frac{12}{30} & -\\frac{1}{5} + \\frac{1}{5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{30} & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nh_{33} = \\frac{5}{30} \\cdot 1 + 0 \\cdot 2 = \\frac{5}{30} = \\frac{1}{6}\n$$\nFor observation $i=4$, the covariate vector is $x_4^T = \\begin{pmatrix} 1 & 2 \\end{pmatrix}$, which is identical to $x_3^T$. Therefore, the calculation for $h_{44}$ is identical:\n$$\nh_{44} = x_4^T (X^T X)^{-1} x_4 = x_3^T (X^T X)^{-1} x_3 = h_{33} = \\frac{1}{6}\n$$\nThus, $h_{33}$ and $h_{44}$ are identical, with a value of $\\frac{1}{6}$.\n\nConceptually, leverage, $h_{ii}$, measures the influence of the response $y_i$ on its own fitted value $\\hat{y}_i$, since $\\hat{y}_i = \\sum_j h_{ij} y_j$. It is a measure of the \"remoteness\" or \"potential\" for influence of an observation in the predictor space. Since leverage depends only on the design matrix $X$, any two observations with identical covariate vectors (duplicate design points) will have the same position in the predictor space and thus must have identical leverages. Their potential for influence, as determined by their distance from the center of the data cloud in the $X$-space, is the same.\n\n**Part 2: Cook's Distance Derivation and Calculation**\n\nCook's distance for observation $i$ is a measure of the aggregate change in the estimated coefficients when the $i$-th observation is deleted. The derivation begins from this case-deletion principle. The formal definition is:\n$$\nD_i = \\frac{(\\hat{\\beta} - \\hat{\\beta}_{(i)})^T (X^T X) (\\hat{\\beta} - \\hat{\\beta}_{(i)})}{p s^2}\n$$\nwhere $\\hat{\\beta}$ is the OLS estimate from the full dataset, $\\hat{\\beta}_{(i)}$ is the estimate with observation $i$ removed, $p$ is the number of parameters, and $s^2$ is the mean squared error from the full model. We need to compute $D_3$.\n\n1.  **Compute the full-model estimate $\\hat{\\beta}$:**\n    First, we calculate $X^T y$:\n    $$\n    X^T y = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\\\ 0 & 1 & 2 & 2 & 3 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3+d \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 1+2+3+d+3+4+5 \\\\ 0(1)+1(2)+2(3+d)+2(3)+3(4)+4(5) \\end{pmatrix} = \\begin{pmatrix} 18+d \\\\ 46+2d \\end{pmatrix}\n    $$\n    Then, $\\hat{\\beta} = (X^T X)^{-1} X^T y$:\n    $$\n    \\hat{\\beta} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{30} & -\\frac{1}{5} \\\\ -\\frac{1}{5} & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 18+d \\\\ 46+2d \\end{pmatrix}\n    $$\n    $$\n    \\hat{\\beta}_0 = \\frac{17}{30}(18+d) - \\frac{1}{5}(46+2d) = \\frac{17(18)+17d - 6(46+2d)}{30} = \\frac{306+17d-276-12d}{30} = \\frac{30+5d}{30} = 1 + \\frac{d}{6}\n    $$\n    $$\n    \\hat{\\beta}_1 = -\\frac{1}{5}(18+d) + \\frac{1}{10}(46+2d) = \\frac{-2(18+d) + (46+2d)}{10} = \\frac{-36-2d+46+2d}{10} = \\frac{10}{10} = 1\n    $$\n    So, $\\hat{\\beta} = \\begin{pmatrix} 1 + d/6 \\\\ 1 \\end{pmatrix}$.\n\n2.  **Compute the case-deleted estimate $\\hat{\\beta}_{(3)}$:**\n    We remove the 3rd observation. The reduced dataset is $(X_{(3)}, y_{(3)})$.\n    $$\n    X_{(3)} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\\\ 1 & 4 \\end{pmatrix}, \\quad y_{(3)} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix}\n    $$\n    For this reduced dataset, it is clear that $y_i = x_{i2} + 1$. The OLS fit will be perfect. Thus, $\\hat{\\beta}_{(3)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. For completeness, we verify this:\n    $$\n    X_{(3)}^T X_{(3)} = \\begin{pmatrix} 5 & 10 \\\\ 10 & 30 \\end{pmatrix}, \\quad X_{(3)}^T y_{(3)} = \\begin{pmatrix} 15 \\\\ 40 \\end{pmatrix}\n    $$\n    $$\n    (X_{(3)}^T X_{(3)})^{-1} = \\frac{1}{50} \\begin{pmatrix} 30 & -10 \\\\ -10 & 5 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} & -\\frac{1}{5} \\\\ -\\frac{1}{5} & \\frac{1}{10} \\end{pmatrix}\n    $$\n    $$\n    \\hat{\\beta}_{(3)} = \\begin{pmatrix} \\frac{3}{5} & -\\frac{1}{5} \\\\ -\\frac{1}{5} & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 15 \\\\ 40 \\end{pmatrix} = \\begin{pmatrix} 9-8 \\\\ -3+4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n    $$\n\n3.  **Compute the numerator of $D_3$:**\n    The difference in coefficients is $\\hat{\\beta} - \\hat{\\beta}_{(3)} = \\begin{pmatrix} 1 + d/6 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix}$.\n    The numerator is $(\\hat{\\beta} - \\hat{\\beta}_{(3)})^T (X^T X) (\\hat{\\beta} - \\hat{\\beta}_{(3)})$:\n    $$\n    \\begin{pmatrix} \\frac{d}{6} & 0 \\end{pmatrix} \\begin{pmatrix} 6 & 12 \\\\ 12 & 34 \\end{pmatrix} \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{d}{6} \\cdot 6 + 0 \\cdot 12 & \\frac{d}{6} \\cdot 12 + 0 \\cdot 34 \\end{pmatrix} \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} d & 2d \\end{pmatrix} \\begin{pmatrix} d/6 \\\\ 0 \\end{pmatrix} = d \\cdot \\frac{d}{6} = \\frac{d^2}{6}\n    $$\n\n4.  **Compute the denominator of $D_3$:**\n    The denominator is $p s^2$. Here $p=2$. We need $s^2$, the mean squared error from the full model.\n    $s^2 = \\frac{SSE}{n-p} = \\frac{e^T e}{6-2}$.\n    First, find the fitted values $\\hat{y} = X\\hat{\\beta}$:\n    $$\n    \\hat{y} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 2 \\\\ 1 & 3 \\\\ 1 & 4 \\end{pmatrix} \\begin{pmatrix} 1+d/6 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+d/6 \\\\ 2+d/6 \\\\ 3+d/6 \\\\ 3+d/6 \\\\ 4+d/6 \\\\ 5+d/6 \\end{pmatrix}\n    $$\n    Now, find the residuals $e = y - \\hat{y}$:\n    $$\n    e = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3+d \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\begin{pmatrix} 1+d/6 \\\\ 2+d/6 \\\\ 3+d/6 \\\\ 3+d/6 \\\\ 4+d/6 \\\\ 5+d/6 \\end{pmatrix} = \\begin{pmatrix} -d/6 \\\\ -d/6 \\\\ d-d/6 \\\\ -d/6 \\\\ -d/6 \\\\ -d/6 \\end{pmatrix} = \\begin{pmatrix} -d/6 \\\\ -d/6 \\\\ 5d/6 \\\\ -d/6 \\\\ -d/6 \\\\ -d/6 \\end{pmatrix}\n    $$\n    The Sum of Squared Errors is $SSE = e^T e$:\n    $$\n    SSE = 5 \\cdot \\left(-\\frac{d}{6}\\right)^2 + \\left(\\frac{5d}{6}\\right)^2 = 5 \\cdot \\frac{d^2}{36} + \\frac{25d^2}{36} = \\frac{30d^2}{36} = \\frac{5d^2}{6}\n    $$\n    The Mean Squared Error is:\n    $$\n    s^2 = \\frac{SSE}{n-p} = \\frac{5d^2/6}{4} = \\frac{5d^2}{24}\n    $$\n    The denominator of $D_3$ is $p s^2 = 2 \\cdot \\frac{5d^2}{24} = \\frac{10d^2}{24} = \\frac{5d^2}{12}$.\n\n5.  **Assemble the final value for $D_3$:**\n    $$\n    D_3 = \\frac{\\text{Numerator}}{\\text{Denominator}} = \\frac{d^2/6}{5d^2/12}\n    $$\n    Assuming $d \\neq 0$ (otherwise $y$ fits a perfect line and all residuals and $D_3$ are $0$), the $d^2$ terms cancel:\n    $$\n    D_3 = \\frac{1/6}{5/12} = \\frac{1}{6} \\cdot \\frac{12}{5} = \\frac{12}{30} = \\frac{2}{5}\n    $$\nThe Cook's distance for observation $i=3$ is $\\frac{2}{5}$.", "answer": "$$\\boxed{\\frac{2}{5}}$$", "id": "4916300"}, {"introduction": "While Cook's distance assesses an observation's overall impact on the model, we often need to know how it influences a single, critical coefficient, such as a treatment effect. This is the role of the $\\mathrm{DFBETAS}$ diagnostic, which you will explore in this practice [@problem_id:4916346]. By deriving the $\\mathrm{DFBETAS}$ formula from the fundamental principles of ordinary least squares, you will gain a rigorous understanding of how a single case can alter a specific parameter estimate and its statistical inference.", "problem": "A biostatistics team is fitting a multiple linear regression to assess a clinically relevant treatment effect in a cardiovascular study. The outcome is systolic blood pressure, and the predictors are an intercept, a binary treatment indicator, and age in decades. The analysis uses ordinary least squares (OLS). Let the design matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$, the parameter vector by $\\beta \\in \\mathbb{R}^{p}$, and the OLS estimate by $\\hat{\\beta}$. Let $C = (X^{\\top}X)^{-1}$ and let the $i$-th subject have covariate row vector $x_{i}^{\\top}$ and OLS residual $e_{i} = y_{i} - x_{i}^{\\top}\\hat{\\beta}$. Consider the standardized deletion effect on the clinically relevant treatment coefficient, denoted by $\\mathrm{DFBETAS}_{j(i)}$ for the coefficient index $j = 2$ (the treatment effect) and a particular subject $i$.\n\nStarting from the standard linear model identities for OLS (i.e., the model $y = X\\beta + \\varepsilon$, the normal equations, the definition of the projection (hat) matrix, and the definitions of residuals and residual sum of squares), derive the explicit expression for $\\mathrm{DFBETAS}_{j(i)}$ in terms of $x_{i}$, $e_{i}$, and $C$, together with any needed quantities that can be written in terms of these. Explain, using this derivation, why a large value of $|\\mathrm{DFBETAS}_{j(i)}|$ for a clinically relevant coefficient corresponds to a case that materially changes its inference. Your derivation must not assume or quote any pre-packaged influence formulas; it should follow from first principles of the OLS estimator and the effect of deleting one case.\n\nThen, for the particular subject $i^{\\ast}$ with covariate vector\n$$\nx_{i^{\\ast}}^{\\top} = \\begin{pmatrix} 1 & 1 & 6 \\end{pmatrix},\n$$\ngiven\n$$\nC = \\begin{pmatrix}\n0.02 & 0 & 0 \\\\\n0 & 0.05 & 0.002 \\\\\n0 & 0.002 & 0.01\n\\end{pmatrix}, \\quad n = 50, \\quad p = 3, \\quad s = 2, \\quad e_{i^{\\ast}} = 3,\n$$\ncompute the numerical value of $\\mathrm{DFBETAS}_{2(i^{\\ast})}$ using your derived formula. Here $s$ denotes the residual standard deviation from the full model fit. Round your final numerical answer to three significant figures. No units are required for the final value.", "solution": "The problem asks for the derivation of the expression for $\\mathrm{DFBETAS}_{j(i)}$ from first principles of ordinary least squares (OLS) regression, an explanation of its interpretation, and a numerical calculation for a specific case.\n\nThe linear model is given by $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the vector of outcomes, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ is the vector of parameters, and $\\varepsilon \\in \\mathbb{R}^{n}$ is the vector of errors with $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I$. The OLS estimate of $\\beta$ is obtained by solving the normal equations, $X^{\\top}X \\hat{\\beta} = X^{\\top}y$, which yields $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$. Let $C = (X^{\\top}X)^{-1}$.\n\nThe quantity $\\mathrm{DFBETAS}_{j(i)}$ measures the change in the $j$-th coefficient, $\\hat{\\beta}_j$, when the $i$-th observation is removed from the dataset, scaled by an estimate of the standard error of $\\hat{\\beta}_j$. The standard definition is:\n$$\n\\mathrm{DFBETAS}_{j(i)} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\mathrm{SE}_{(i)}(\\hat{\\beta}_j)}\n$$\nwhere $\\hat{\\beta}_{(i)}$ is the OLS estimate of $\\beta$ computed without observation $i$, and $\\mathrm{SE}_{(i)}(\\hat{\\beta}_j)$ is an estimate of the standard error of $\\hat{\\beta}_j$ computed from the dataset with observation $i$ deleted. By convention, this standard error is taken to be $s_{(i)}\\sqrt{C_{jj}}$, where $s_{(i)}$ is the residual standard error from the model fit without observation $i$, and $C_{jj}$ is the $(j,j)$-th element of $C = (X^{\\top}X)^{-1}$ from the full data fit.\n\nThe derivation proceeds in three main steps:\n1. Find an expression for the difference $\\hat{\\beta} - \\hat{\\beta}_{(i)}$.\n2. Find an expression for the scaling factor $s_{(i)}$.\n3. Combine these to form the final expression for $\\mathrm{DFBETAS}_{j(i)}$.\n\n**1. Derivation of $\\hat{\\beta} - \\hat{\\beta}_{(i)}$**\n\nLet $X_{(i)}$ and $y_{(i)}$ be the design matrix and response vector with the $i$-th row (observation) removed. The OLS estimate from this reduced dataset is $\\hat{\\beta}_{(i)} = (X_{(i)}^{\\top}X_{(i)})^{-1}X_{(i)}^{\\top}y_{(i)}$.\n\nThe full data matrices can be expressed as a sum of the reduced data matrices and the contribution of the $i$-th observation (with covariate vector $x_i^{\\top}$ and outcome $y_i$):\n$$\nX^{\\top}X = \\sum_{k=1}^{n} x_k x_k^{\\top} = X_{(i)}^{\\top}X_{(i)} + x_i x_i^{\\top}\n$$\n$$\nX^{\\top}y = \\sum_{k=1}^{n} x_k y_k = X_{(i)}^{\\top}y_{(i)} + x_i y_i\n$$\nFrom the first identity, we have $X_{(i)}^{\\top}X_{(i)} = X^{\\top}X - x_i x_i^{\\top}$. To find its inverse, we use the Sherman-Morrison-Woodbury formula for a rank-$1$ update: $(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$.\nLetting $A = X^{\\top}X$ and $u=v=x_i$, we get:\n$$\n(X_{(i)}^{\\top}X_{(i)})^{-1} = (X^{\\top}X - x_i x_i^{\\top})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - x_i^{\\top}(X^{\\top}X)^{-1}x_i}\n$$\nLet $C = (X^{\\top}X)^{-1}$ and define the leverage of the $i$-th observation as $h_{ii} = x_i^{\\top}Cx_i$. The expression simplifies to:\n$$\n(X_{(i)}^{\\top}X_{(i)})^{-1} = C + \\frac{Cx_i x_i^{\\top}C}{1 - h_{ii}}\n$$\nNow, using $X_{(i)}^{\\top}y_{(i)} = X^{\\top}y - x_i y_i$, we can write $\\hat{\\beta}_{(i)}$:\n$$\n\\hat{\\beta}_{(i)} = \\left(C + \\frac{Cx_i x_i^{\\top}C}{1 - h_{ii}}\\right) (X^{\\top}y - x_i y_i)\n$$\nExpanding this product:\n$$\n\\hat{\\beta}_{(i)} = C(X^{\\top}y) - C x_i y_i + \\frac{Cx_i x_i^{\\top}C(X^{\\top}y)}{1 - h_{ii}} - \\frac{Cx_i x_i^{\\top}C x_i y_i}{1 - h_{ii}}\n$$\nWe recognize that $\\hat{\\beta} = C(X^{\\top}y)$, so $x_i^{\\top}\\hat{\\beta} = x_i^{\\top}C(X^{\\top}y)$. Also, $x_i^{\\top}C x_i = h_{ii}$. Substituting these gives:\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - C x_i y_i + \\frac{Cx_i(x_i^{\\top}\\hat{\\beta})}{1 - h_{ii}} - \\frac{Cx_i(h_{ii})y_i}{1 - h_{ii}}\n$$\nCombining terms with a common factor of $Cx_i/(1 - h_{ii})$:\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - \\frac{Cx_i}{1 - h_{ii}} \\left( (1-h_{ii})y_i - x_i^{\\top}\\hat{\\beta} + h_{ii}y_i \\right)\n$$\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - \\frac{Cx_i}{1 - h_{ii}} (y_i - h_{ii}y_i - x_i^{\\top}\\hat{\\beta} + h_{ii}y_i)\n$$\n$$\n\\hat{\\beta}_{(i)} = \\hat{\\beta} - \\frac{Cx_i(y_i - x_i^{\\top}\\hat{\\beta})}{1 - h_{ii}}\n$$\nThe term $y_i - x_i^{\\top}\\hat{\\beta}$ is the OLS residual for the $i$-th observation, $e_i$. Thus, we have the elegant result:\n$$\n\\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{C x_i e_i}{1 - h_{ii}}\n$$\nThe $j$-th component of this vector difference is:\n$$\n\\hat{\\beta}_j - \\hat{\\beta}_{j(i)} = \\frac{(C x_i)_j e_i}{1 - h_{ii}}\n$$\nwhere $(Cx_i)_j$ is the $j$-th element of the vector $Cx_i$.\n\n**2. Derivation of $s_{(i)}$**\n\nThe residual standard error from the full model is $s = \\sqrt{\\frac{\\mathrm{RSS}}{n-p}}$, where $\\mathrm{RSS} = \\sum_{k=1}^{n} e_k^2$. The residual standard error from the model with observation $i$ deleted is $s_{(i)} = \\sqrt{\\frac{\\mathrm{RSS}_{(i)}}{n-p-1}}$. A standard linear model identity relates the residual sum of squares of the two models:\n$$\n\\mathrm{RSS}_{(i)} = \\mathrm{RSS} - \\frac{e_i^2}{1-h_{ii}}\n$$\nSubstituting $\\mathrm{RSS} = (n-p)s^2$:\n$$\n(n-p-1)s_{(i)}^2 = (n-p)s^2 - \\frac{e_i^2}{1-h_{ii}}\n$$\nThis allows us to compute $s_{(i)}$ from quantities available from the full model fit.\n\n**3. Final Expression for $\\mathrm{DFBETAS}_{j(i)}$**\n\nSubstituting the derived expressions for the numerator and the definition of the denominator into the definition of $\\mathrm{DFBETAS}_{j(i)}$:\n$$\n\\mathrm{DFBETAS}_{j(i)} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{s_{(i)}\\sqrt{C_{jj}}} = \\frac{\\frac{(C x_i)_j e_i}{1 - h_{ii}}}{s_{(i)}\\sqrt{C_{jj}}} = \\frac{(C x_i)_j e_i}{(1-h_{ii})s_{(i)}\\sqrt{C_{jj}}}\n$$\nThis is the required explicit expression.\n\n**Interpretation:**\nA large value of $|\\mathrm{DFBETAS}_{j(i)}|$ indicates that observation $i$ is influential for the coefficient $\\hat{\\beta}_j$. The formula shows that this influence arises from two main sources:\n- **Residual ($e_i$):** An observation that is poorly predicted by the model (large $|e_i|$) has the potential to be influential.\n- **Leverage ($h_{ii}$):** An observation with an unusual covariate pattern (large $h_{ii}$, approaching $1$) will have its influence magnified by the term $1/(1-h_{ii})$. Such points are called high-leverage points.\nThe term $(C x_i)_j$ links the specific covariate values of observation $i$ to the variance-covariance structure of the estimates. A large $|\\mathrm{DFBETAS}_{j(i)}|$ (common thresholds are $1$ or $2/\\sqrt{n}$) suggests that removing observation $i$ would substantially change the estimate $\\hat{\\beta}_j$ relative to its standard error. For a clinically relevant coefficient, this means the conclusion about the treatment effect (e.g., its magnitude, sign, or statistical significance) may be dependent on this single observation, warranting further investigation.\n\n**Numerical Calculation:**\nWe are given the following for a subject $i^{\\ast}$:\n- $j=2$ (the treatment coefficient)\n- $x_{i^{\\ast}}^{\\top} = \\begin{pmatrix} 1 & 1 & 6 \\end{pmatrix}$\n- $C = \\begin{pmatrix} 0.02 & 0 & 0 \\\\ 0 & 0.05 & 0.002 \\\\ 0 & 0.002 & 0.01 \\end{pmatrix}$\n- $e_{i^{\\ast}} = 3$\n- $s = 2$\n- $n = 50$\n- $p = 3$\n\nWe need to compute $\\mathrm{DFBETAS}_{2(i^{\\ast})}$. Let's calculate the necessary components.\n\nFirst, the leverage $h_{i^{\\ast}i^{\\ast}}$:\n$h_{i^{\\ast}i^{\\ast}} = x_{i^{\\ast}}^{\\top} C x_{i^{\\ast}} = \\begin{pmatrix} 1 & 1 & 6 \\end{pmatrix} \\begin{pmatrix} 0.02 & 0 & 0 \\\\ 0 & 0.05 & 0.002 \\\\ 0 & 0.002 & 0.01 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 6 \\end{pmatrix}$\n$C x_{i^{\\ast}} = \\begin{pmatrix} 0.02(1) + 0(1) + 0(6) \\\\ 0(1) + 0.05(1) + 0.002(6) \\\\ 0(1) + 0.002(1) + 0.01(6) \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.05 + 0.012 \\\\ 0.002 + 0.06 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.062 \\\\ 0.062 \\end{pmatrix}$\n$h_{i^{\\ast}i^{\\ast}} = \\begin{pmatrix} 1 & 1 & 6 \\end{pmatrix} \\begin{pmatrix} 0.02 \\\\ 0.062 \\\\ 0.062 \\end{pmatrix} = 1(0.02) + 1(0.062) + 6(0.062) = 0.02 + 0.062 + 0.372 = 0.454$.\nSo, $1-h_{i^{\\ast}i^{\\ast}} = 1 - 0.454 = 0.546$.\n\nSecond, the term $(C x_{i^{\\ast}})_j$ for $j=2$:\nThis is the second element of the vector $C x_{i^{\\ast}}$, which is $0.062$.\n\nThird, the leave-one-out residual standard error $s_{(i^{\\ast})}$:\n$s_{(i^{\\ast})}^2 = \\frac{(n-p)s^2 - e_{i^{\\ast}}^2/(1-h_{i^{\\ast}i^{\\ast}})}{n-p-1}$\nWith $n=50$, $p=3$, $s=2$, $e_{i^{\\ast}}=3$, and $h_{i^{\\ast}i^{\\ast}}=0.454$:\n$s_{(i^{\\ast})}^2 = \\frac{(50-3)(2^2) - 3^2/(1-0.454)}{50-3-1} = \\frac{47(4) - 9/0.546}{46} = \\frac{188 - 16.483516...}{46} = \\frac{171.516483...}{46} = 3.728619...$\n$s_{(i^{\\ast})} = \\sqrt{3.728619...} \\approx 1.930963...$\n\nFourth, the term $\\sqrt{C_{jj}}$ for $j=2$:\n$C_{22} = 0.05$, so $\\sqrt{C_{22}} = \\sqrt{0.05} \\approx 0.223606...$\n\nFinally, we assemble $\\mathrm{DFBETAS}_{2(i^{\\ast})}$:\n$$\n\\mathrm{DFBETAS}_{2(i^{\\ast})} = \\frac{(C x_{i^{\\ast}})_2 e_{i^{\\ast}}}{(1-h_{i^{\\ast}i^{\\ast}})s_{(i^{\\ast})}\\sqrt{C_{22}}} = \\frac{(0.062)(3)}{(0.546)(1.930963...)(0.223606...)}\n$$\n$$\n\\mathrm{DFBETAS}_{2(i^{\\ast})} = \\frac{0.186}{0.235741...} \\approx 0.788998...\n$$\nRounding to three significant figures, we get $0.789$. This value is below the common threshold of $1$, suggesting that while this case has non-negligible influence, it might not be considered highly problematic on its own.", "answer": "$$\n\\boxed{0.789}\n$$", "id": "4916346"}, {"introduction": "In real-world data analysis, especially in fields like genomics, identifying influential data points is not just a calculation but a process of discovery that requires statistical validation. This exercise challenges you to build a complete computational pipeline that moves from theory to practice [@problem_id:4916287]. You will implement Cook's distance from scratch and then develop a screening rule based on a powerful combination of permutation testing and the Benjamini-Hochberg procedure to rigorously identify influential samples while controlling the false discovery rate.", "problem": "You are given the classical normal linear model with many predictors, representing a gene expression setting where each sample (row) is a profiled biological specimen and the predictors (columns) encode technical and biological covariates. The model assumes $y = X \\beta + \\varepsilon$ with $y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^p$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. Starting from this base, derive the mathematical expressions you need to compute the samplewise leverage and the Cook’s distance, explain why leverage alters the contribution of residuals to Cook’s distance, and design a screening rule that controls the False Discovery Rate (FDR) using Cook’s distance as the test statistic. Then implement this rule in a program that processes the following test suite.\n\nYour algorithmic tasks, all of which must be implemented based on the ordinary least squares (OLS) estimator and linear-model identities and without relying on any shortcut formulas given in this statement, are:\n- Compute the OLS fit.\n- Compute the residuals and the residual mean squared error.\n- Compute the diagonal elements of the hat matrix (leverages).\n- Compute Cook’s distance for each sample.\n- For each sample, compute an empirical p-value for Cook’s distance using a permutation-based null in which the response $y$ is randomly permuted (predictors $X$ are held fixed). Use $B = 800$ permutations with a fixed pseudorandom number generator seed specified per test case to ensure determinism.\n- Apply the Benjamini–Hochberg (BH) procedure to the set of p-values at level $\\alpha = 0.10$ to identify influential samples, thereby controlling the FDR in the set of flagged samples.\n\nScreening rule requirement:\n- The screening rule must be: “Flag a sample if its permutation-based p-value for Cook’s distance is selected by the Benjamini–Hochberg (BH) procedure at level $\\alpha = 0.10$.” You must justify this rule in your solution and implement it in code.\n\nData generation protocol for each test case:\n- For each case, generate $X$ as follows: draw $n \\times p$ independent standard normal entries, column-center to mean $0$ and scale each column to standard deviation $1$; then append an intercept column of ones to form a design matrix with $p+1$ columns. To induce high leverage in specific samples, multiply the specified rows (only the non-intercept columns) by the given amplification factor.\n- Generate $y$ as $y = X \\beta + \\eta$ where $\\eta$ is independent normal noise with mean $0$ and standard deviation $\\sigma$; then apply the specified residual shocks by adding the given values to the indicated entries of $y$.\n- Express all quantities using zero-based indexing for samples.\n- Ensure $n > p+1$ so that the residual degrees of freedom are strictly positive.\n\nTest suite:\n- Case $1$ (“happy path” with a single extreme profile):\n  - Dimensions: $n = 30$, $p = 8$ (not counting the intercept).\n  - Random seeds: predictor seed $11$, response seed $17$.\n  - Coefficients: $\\beta \\in \\mathbb{R}^{9}$ given by $\\beta = [0.5,\\, 0.9,\\,-0.5,\\,0.0,\\,0.7,\\,0.0,\\,-0.4,\\,0.2,\\,0.0]$ where the first entry is the intercept coefficient.\n  - Noise standard deviation: $\\sigma = 0.4$.\n  - High leverage: multiply row index $5$ (only the first $p$ columns, not the intercept) by factor $4.0$.\n  - Residual shock: add $3.0$ to $y[5]$.\n  - Permutation seed: $1017$.\n- Case $2$ (“boundary case” with no extremes):\n  - Dimensions: $n = 25$, $p = 5$.\n  - Random seeds: predictor seed $19$, response seed $23$.\n  - Coefficients: $\\beta \\in \\mathbb{R}^{6}$ given by $\\beta = [0.5,\\,0.6,\\,0.0,\\,-0.5,\\,0.3,\\,0.0]$.\n  - Noise standard deviation: $\\sigma = 0.5$.\n  - High leverage: none.\n  - Residual shock: none.\n  - Permutation seed: $2023$.\n- Case $3$ (“edge case” with many predictors and multiple extremes):\n  - Dimensions: $n = 18$, $p = 10$.\n  - Random seeds: predictor seed $29$, response seed $31$.\n  - Coefficients: $\\beta \\in \\mathbb{R}^{11}$ given by $\\beta = [0.5,\\,0.8,\\,-0.3,\\,0.0,\\,0.7,\\,0.0,\\,-0.2,\\,0.4,\\,0.0,\\,0.2,\\,-0.1]$.\n  - Noise standard deviation: $\\sigma = 0.6$.\n  - High leverage: multiply row index $2$ by factor $3.0$ and row index $10$ by factor $2.5$ (again, only the first $p$ columns).\n  - Residual shocks: add $2.4$ to $y[2]$ and subtract $2.4$ from $y[15]$.\n  - Permutation seed: $3031$.\n\nImplementation notes:\n- Use zero-based indices for samples throughout.\n- All linear algebra must be carried out exactly as implied by the OLS framework. Do not use any external approximations beyond what is mandated by the permutation method.\n- Angle units and physical units are not applicable; the output is unitless.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of zero-based indices flagged by the BH procedure for each test case, in order. For example, a valid output format is $[[i_1,i_2],[],[j_1]]$ (this is only an example format; your actual indices must be computed from the data above).", "solution": "The problem requires the derivation and implementation of a statistical screening rule to identify influential data points in a linear regression model. This involves several fundamental concepts in regression diagnostics and multiple hypothesis testing. I will first present the theoretical derivations and justifications, followed by the implementation details.\n\nThe analysis is based on the normal linear model:\n$$ y = X \\beta + \\varepsilon $$\nwhere $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p_{eff}}$ is the design matrix of rank $p_{eff}$, $\\beta \\in \\mathbb{R}^{p_{eff}}$ is the vector of unknown coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the vector of errors, assumed to be independent and identically distributed as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. The problem specifies constructing a design matrix with $p$ predictors plus an intercept, so the total number of parameters to be estimated is $p_{eff} = p+1$. We require $n > p_{eff}$ for the model to be identifiable.\n\n**1. Ordinary Least Squares (OLS) Estimation**\nThe OLS estimator $\\hat{\\beta}$ is chosen to minimize the Residual Sum of Squares (RSS), $RSS(\\beta) = (y - X\\beta)^T(y - X\\beta)$. To find the minimum, we set the gradient with respect to $\\beta$ to zero:\n$$ \\nabla_{\\beta} RSS = -2X^T(y - X\\beta) = 0 $$\n$$ X^T X \\beta = X^T y $$\nAssuming $X$ has full column rank, the matrix $X^T X$ is invertible. The OLS estimator for $\\beta$ is therefore unique and given by:\n$$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$\n\n**2. The Hat Matrix and Leverage**\nThe vector of fitted values, $\\hat{y}$, is obtained by applying the estimated coefficients:\n$$ \\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y $$\nThis introduces the \"hat matrix,\" $H$, defined as:\n$$ H = X(X^T X)^{-1} X^T $$\nThe hat matrix is an $n \\times n$ projection matrix that maps the response vector $y$ to the fitted values $\\hat{y} = Hy$. The diagonal elements of this matrix, $h_{ii} = [H]_{ii}$, are called the leverages. The leverage of the $i$-th observation, $h_{ii}$, can be written as $h_{ii} = x_i^T (X^T X)^{-1} x_i$, where $x_i^T$ is the $i$-th row of $X$. It measures the distance of the predictor vector $x_i$ from the center of the data cloud in the predictor space. A high leverage value indicates that the observation $i$ has an unusual combination of predictor values and thus has a large potential to influence the regression fit. Key properties of leverages are $0 \\le h_{ii} \\le 1$ and $\\sum_{i=1}^n h_{ii} = \\operatorname{tr}(H) = p_{eff}$.\n\n**3. Residuals and Cook's Distance**\nThe residuals are the differences between the observed and fitted values:\n$$ e = y - \\hat{y} = (I - H)y $$\nAn unbiased estimator for the error variance $\\sigma^2$ is the Mean Squared Error (MSE), also denoted $\\hat{\\sigma}^2$:\n$$ \\hat{\\sigma}^2 = \\frac{e^T e}{n - p_{eff}} = \\frac{RSS}{n - p_{eff}} $$\nCook's distance, $D_i$, is a measure of the influence of the $i$-th data point. It quantifies the effect of deleting observation $i$ on the set of all fitted values. It is defined as:\n$$ D_i = \\frac{(\\hat{y} - \\hat{y}_{(i)})^T (\\hat{y} - \\hat{y}_{(i)})}{p_{eff} \\hat{\\sigma}^2} = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p_{eff} \\hat{\\sigma}^2} $$\nwhere $\\hat{y}_{(i)}$ is the vector of predicted values obtained from a regression fit without observation $i$. A direct calculation using this definition would be computationally expensive, requiring $n$ separate regressions. A standard algebraic identity provides a more efficient formula. The identity relates the change in the coefficient vector to the properties of the left-out point: $\\hat{\\beta} - \\hat{\\beta}_{(i)} = (X^T X)^{-1} x_i \\frac{e_i}{1 - h_{ii}}$. Using this, the change in fitted values is:\n$$ \\hat{y} - \\hat{y}_{(i)} = X(\\hat{\\beta} - \\hat{\\beta}_{(i)}) = X(X^T X)^{-1} x_i \\frac{e_i}{1-h_{ii}} $$\nThe squared Euclidean norm of this vector is:\n$$ (\\hat{y} - \\hat{y}_{(i)})^T (\\hat{y} - \\hat{y}_{(i)}) = \\left(\\frac{e_i}{1-h_{ii}}\\right)^2 \\left(X(X^T X)^{-1} x_i\\right)^T \\left(X(X^T X)^{-1} x_i\\right) $$\n$$ = \\left(\\frac{e_i}{1-h_{ii}}\\right)^2 x_i^T (X^T X)^{-1} X^T X (X^T X)^{-1} x_i = \\left(\\frac{e_i}{1-h_{ii}}\\right)^2 x_i^T (X^T X)^{-1} x_i = \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2} $$\nSubstituting this into the definition of $D_i$ yields the computational formula:\n$$ D_i = \\frac{e_i^2}{p_{eff} \\hat{\\sigma}^2} \\frac{h_{ii}}{(1-h_{ii})^2} $$\nThis expression reveals that Cook's distance is a function of two components: the squared residual $e_i^2$ and the leverage $h_{ii}$. The $i$-th standardized residual is $r_i = e_i / (\\hat{\\sigma}\\sqrt{1-h_{ii}})$. Cook's distance can be written as $D_i = \\frac{r_i^2}{p_{eff}} \\frac{h_{ii}}{1-h_{ii}}$. This form clearly shows that $D_i$ is a product of the magnitude of the residual (standardized) and the leverage of the point. A sample can be influential (large $D_i$) if it has a large residual, high leverage, or a combination of both. The term $h_{ii}/(1-h_{ii})^2$ demonstrates that leverage non-linearly amplifies the effect of the residual on the influence score. As $h_{ii} \\to 1$, this amplification factor grows without bound, meaning that a very high leverage point can be extremely influential even with a modest residual.\n\n**4. A Statistical Screening Rule for Influence**\nTo systematically identify influential samples, we formulate a hypothesis test for each sample. The test statistic is its Cook's distance, $D_i$. We need a null distribution for this statistic to assess its significance.\n\nThe null hypothesis ($H_0$) is that there is no relationship between the predictors $X$ and the response $y$. Under this hypothesis, any permutation of the elements of $y$ is equally likely. This forms the basis for a permutation test. By repeatedly permuting $y$ and re-computing the Cook's distances for the resulting null datasets, we can generate an empirical null distribution for $D_i$.\n\nThe procedure is as follows:\n1.  Calculate the observed Cook's distances, $D_i^{obs}$, for $i=1, \\dots, n$ from the original data.\n2.  For $b=1, \\dots, B$, generate a permuted response vector $y^{(b)}$ by shuffling $y$.\n3.  For each $y^{(b)}$, fit the model $y^{(b)} \\sim X$ and compute the full set of Cook's distances, $\\{D_j^{(b)}\\}_{j=1}^n$. Note that the hat matrix $H$ and leverages $h_{jj}$ depend only on $X$ and remain fixed across permutations.\n4.  Pool all computed null statistics to form a single null distribution of size $N_{null} = n \\times B$: $\\mathcal{D}_{null} = \\{D_j^{(b)} \\mid j=1, \\dots, n; b=1, \\dots, B\\}$.\n5.  The empirical p-value for each observed statistic $D_i^{obs}$ is the proportion of null statistics that are at least as extreme:\n    $$ p_i = \\frac{1 + |\\{D \\in \\mathcal{D}_{null} \\mid D \\ge D_i^{obs}\\}|}{1 + N_{null}} $$\n    The addition of $1$ to the numerator and denominator is a standard practice to handle cases where $D_i^{obs}$ is larger than any value in the null distribution and to avoid p-values of exactly zero.\n\n**5. False Discovery Rate (FDR) Control**\nSince we are performing $n$ simultaneous tests (one for each sample), controlling the familywise error rate (e.g., Bonferroni correction) would be too conservative. A more suitable approach for screening problems is to control the False Discovery Rate (FDR), defined as the expected proportion of false positives among all discoveries. The Benjamini-Hochberg (BH) procedure accomplishes this.\n\nGiven a set of $n$ p-values $p_1, \\dots, p_n$ and a desired FDR level $\\alpha$:\n1.  Order the p-values from smallest to largest: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(n)}$.\n2.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{n} \\alpha$.\n3.  If such a $k$ exists, reject the null hypotheses corresponding to the p-values $p_{(1)}, \\dots, p_{(k)}$. Otherwise, reject none.\n\n**6. Justification for the Final Screening Rule**\nThe problem requires designing a screening rule to flag influential samples while controlling FDR at $\\alpha=0.10$. The rule is: \"Flag a sample if its permutation-based p-value for Cook’s distance is selected by the Benjamini–Hochberg (BH) procedure at level $\\alpha = 0.10$.\" This rule is well-justified:\n- It uses a principled measure of influence, Cook's distance, which synthesizes information from both residuals and leverage.\n- It employs a non-parametric, data-driven method (permutation testing) to determine the statistical significance of observed influence scores, which is robust and does not rely on strong distributional assumptions for the test statistic itself.\n- It correctly addresses the multiple testing problem inherent in screening all $n$ samples, providing a rigorous statistical guarantee (FDR control) on the set of flagged samples. This is crucial for ensuring the reliability of findings in a high-dimensional setting like genomics.\n\nThe following implementation executes this complete procedure for the provided test cases.", "answer": "```python\nimport numpy as np\n\ndef analyze_case(n, p, predictor_seed, response_seed, beta, sigma, leverage_mods, residual_shocks, perm_seed, B=800, alpha=0.10):\n    \"\"\"\n    Performs influence diagnostics and screening for a single test case.\n    \"\"\"\n    # === 1. Data Generation ===\n    p_eff = p + 1\n    \n    # Generate predictors X\n    rng_pred = np.random.default_rng(predictor_seed)\n    X_raw = rng_pred.standard_normal(size=(n, p))\n    \n    # Center and scale predictors\n    X_centered = X_raw - X_raw.mean(axis=0)\n    # Use ddof=1 for sample standard deviation, a common statistical convention\n    X_scaled = X_centered / X_raw.std(axis=0, ddof=1)\n    \n    # Prepend intercept column\n    X = np.c_[np.ones(n), X_scaled]\n    \n    # Apply high-leverage modifications\n    if leverage_mods:\n        for row_idx, factor in leverage_mods:\n            X[row_idx, 1:] *= factor\n            \n    # Generate response y\n    rng_resp = np.random.default_rng(response_seed)\n    noise = rng_resp.normal(0, sigma, size=n)\n    y = X @ np.array(beta) + noise\n    \n    # Apply residual shocks\n    if residual_shocks:\n        for row_idx, shock in residual_shocks:\n            y[row_idx] += shock\n\n    # === 2. Calculate Observed Statistics ===\n    # Pre-compute X^T*X and its inverse\n    XTX = X.T @ X\n    XTX_inv = np.linalg.inv(XTX)\n    \n    # OLS fit\n    beta_hat = XTX_inv @ X.T @ y\n    \n    # Hat matrix diagonal (leverages)\n    H = X @ XTX_inv @ X.T\n    h = np.diag(H)\n    \n    # Residuals and MSE\n    y_hat = X @ beta_hat\n    residuals = y - y_hat\n    rss = residuals.T @ residuals\n    df_resid = n - p_eff\n    mse = rss / df_resid\n    \n    # Observed Cook's Distances\n    # D_i = (e_i^2 / (p_eff * mse)) * (h_ii / (1 - h_ii)^2)\n    D_obs = (residuals**2 / (p_eff * mse)) * (h / (1 - h)**2)\n\n    # === 3. Permutation Test to Generate Null Distribution ===\n    rng_perm = np.random.default_rng(perm_seed)\n    # The number of null statistics will be n * B\n    null_cooks_distances = np.zeros(n * B)\n    y_perm = y.copy()\n\n    for b in range(B):\n        rng_perm.shuffle(y_perm)\n        \n        # OLS on permuted data\n        beta_hat_perm = XTX_inv @ X.T @ y_perm\n        \n        # Residuals and MSE for permuted data\n        y_hat_perm = X @ beta_hat_perm\n        residuals_perm = y_perm - y_hat_perm\n        rss_perm = residuals_perm.T @ residuals_perm\n        mse_perm = rss_perm / df_resid\n        \n        # Cook's distances for permuted data\n        # Note: h and p_eff are constant\n        D_perm = (residuals_perm**2 / (p_eff * mse_perm)) * (h / (1 - h)**2)\n        \n        null_cooks_distances[b*n:(b+1)*n] = D_perm\n\n    # === 4. Calculate P-values ===\n    p_values = np.zeros(n)\n    num_null_samples = len(null_cooks_distances)\n    \n    for i in range(n):\n        D_i_obs = D_obs[i]\n        num_exceeding = np.sum(null_cooks_distances >= D_i_obs)\n        p_values[i] = (1 + num_exceeding) / (1 + num_null_samples)\n\n    # === 5. Apply Benjamini-Hochberg (BH) Procedure ===\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = p_values[sorted_indices]\n    \n    bh_thresholds = (np.arange(1, n + 1) / n) * alpha\n    \n    is_significant = sorted_p_values = bh_thresholds\n    \n    if np.any(is_significant):\n        # Find the largest k such that p_(k) = (k/n) * alpha\n        k = np.where(is_significant)[0].max() + 1\n        significant_indices = sorted_indices[:k]\n    else:\n        significant_indices = np.array([], dtype=int)\n        \n    return sorted(significant_indices.tolist())\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 30, \"p\": 8,\n            \"predictor_seed\": 11, \"response_seed\": 17,\n            \"beta\": [0.5, 0.9, -0.5, 0.0, 0.7, 0.0, -0.4, 0.2, 0.0],\n            \"sigma\": 0.4,\n            \"leverage_mods\": [(5, 4.0)],\n            \"residual_shocks\": [(5, 3.0)],\n            \"perm_seed\": 1017\n        },\n        {\n            \"n\": 25, \"p\": 5,\n            \"predictor_seed\": 19, \"response_seed\": 23,\n            \"beta\": [0.5, 0.6, 0.0, -0.5, 0.3, 0.0],\n            \"sigma\": 0.5,\n            \"leverage_mods\": None,\n            \"residual_shocks\": None,\n            \"perm_seed\": 2023\n        },\n        {\n            \"n\": 18, \"p\": 10,\n            \"predictor_seed\": 29, \"response_seed\": 31,\n            \"beta\": [0.5, 0.8, -0.3, 0.0, 0.7, 0.0, -0.2, 0.4, 0.0, 0.2, -0.1],\n            \"sigma\": 0.6,\n            \"leverage_mods\": [(2, 3.0), (10, 2.5)],\n            \"residual_shocks\": [(2, 2.4), (15, -2.4)],\n            \"perm_seed\": 3031\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        flagged_indices = analyze_case(\n            case[\"n\"], case[\"p\"],\n            case[\"predictor_seed\"], case[\"response_seed\"],\n            case[\"beta\"], case[\"sigma\"],\n            case[\"leverage_mods\"], case[\"residual_shocks\"],\n            case[\"perm_seed\"]\n        )\n        results.append(flagged_indices)\n    \n    # Format the final output string exactly as requested\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "4916287"}]}