{"hands_on_practices": [{"introduction": "While Ordinary Least Squares (OLS) is a cornerstone of regression, its performance can degrade severely in the presence of multicollinearity—a common issue in biostatistics where predictors like biomarkers are often highly correlated. This first practice problem provides a direct, hands-on calculation to reveal just how unstable OLS estimates can become in such scenarios. By quantifying the massive variance of a coefficient estimate, you will gain a tangible understanding of why regularization techniques are not just theoretical curiosities, but essential tools for building robust and reliable statistical models [@problem_id:4947371].", "problem": "A biostatistics team is modeling a continuous clinical outcome $y$ from two laboratory biomarkers whose assays are known to be highly correlated. The data for $n=5$ patients and $p=2$ biomarkers are assembled into the design matrix $X$ (columns scaled to comparable units):\n$$\nX=\\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}.\n$$\nAssume the standard linear model $y = X\\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$, where $\\mathcal{N}$ denotes the normal distribution and $I$ is the identity matrix. Suppose prior measurement studies provide a reliable estimate of the residual variance $\\sigma^2 = 0.25$. Using only these model definitions and properties of Ordinary Least Squares (OLS), derive the sampling variance of the OLS estimator for the first coefficient $\\beta_1$, and then compute its numerical value for the given $X$ and $\\sigma^2$. Round your final numerical answer to four significant figures. Express the final answer as a real number with no units. Additionally, interpret the magnitude of this variance in the context of collinearity and explain why regularization methods such as ridge regression and the least absolute shrinkage and selection operator (lasso) are relevant here.", "solution": "The problem requires the derivation and calculation of the sampling variance for an Ordinary Least Squares (OLS) coefficient estimate, followed by an interpretation of the result in the context of multicollinearity and the relevance of regularization techniques.\n\n**Part 1: Derivation of the Sampling Variance of an OLS Estimator**\n\nThe standard linear model is given by $y = X\\beta + \\varepsilon$, where $y$ is the $n \\times 1$ vector of outcomes, $X$ is the $n \\times p$ design matrix, $\\beta$ is the $p \\times 1$ vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of errors. The OLS estimator $\\hat{\\beta}$ is derived by minimizing the sum of squared residuals, $SSR = (y - X\\beta)^T(y - X\\beta)$. This yields the normal equations $X^T X \\hat{\\beta} = X^T y$. Assuming $X^T X$ is invertible, the OLS estimator is:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\nTo find the sampling distribution of $\\hat{\\beta}$, we substitute the true model $y = X\\beta + \\varepsilon$ into the expression for $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T (X\\beta + \\varepsilon) = (X^T X)^{-1} (X^T X) \\beta + (X^T X)^{-1} X^T \\varepsilon = \\beta + (X^T X)^{-1} X^T \\varepsilon\n$$\nThe problem states that the errors follow a normal distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. This means the expected value of the error vector is $E[\\varepsilon] = 0$ and the covariance matrix of the errors is $E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$.\nThe expected value of the estimator is:\n$$\nE[\\hat{\\beta}] = E[\\beta + (X^T X)^{-1} X^T \\varepsilon] = \\beta + (X^T X)^{-1} X^T E[\\varepsilon] = \\beta + 0 = \\beta\n$$\nThis shows that the OLS estimator is unbiased. The covariance matrix of $\\hat{\\beta}$ is defined as $\\text{Cov}(\\hat{\\beta}) = E[(\\hat{\\beta} - E[\\hat{\\beta}])(\\hat{\\beta} - E[\\hat{\\beta}])^T]$. Since $E[\\hat{\\beta}] = \\beta$, we have $\\hat{\\beta} - \\beta = (X^T X)^{-1} X^T \\varepsilon$. Therefore:\n$$\n\\text{Cov}(\\hat{\\beta}) = E[((X^T X)^{-1} X^T \\varepsilon)((X^T X)^{-1} X^T \\varepsilon)^T]\n$$\nUsing the property $(AB)^T = B^T A^T$, this becomes:\n$$\n\\text{Cov}(\\hat{\\beta}) = E[(X^T X)^{-1} X^T \\varepsilon \\varepsilon^T X ((X^T X)^{-1})^T]\n$$\nSince $X$ is treated as fixed (non-stochastic), we can move it outside the expectation:\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T E[\\varepsilon \\varepsilon^T] X ((X^T X)^{-1})^T\n$$\nSubstituting $E[\\varepsilon \\varepsilon^T] = \\sigma^2 I$:\n$$\n\\text{Cov}(\\hat{\\beta}) = (X^T X)^{-1} X^T (\\sigma^2 I) X ((X^T X)^{-1})^T = \\sigma^2 (X^T X)^{-1} X^T X ((X^T X)^{-1})^T\n$$\nThis simplifies to $\\sigma^2 (X^T X)^{-1} I ((X^T X)^{-1})^T$. Since $X^T X$ is symmetric, its inverse $(X^T X)^{-1}$ is also symmetric, so $((X^T X)^{-1})^T = (X^T X)^{-1}$. The final expression for the covariance matrix of the OLS estimator is:\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n$$\nThe sampling variance of the estimator for the first coefficient, $\\hat{\\beta}_1$, is the first diagonal element of this covariance matrix. If we denote $C = (X^T X)^{-1}$, then:\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 C_{11} = \\sigma^2 [(X^T X)^{-1}]_{11}\n$$\nThis is the general formula we will use for the calculation.\n\n**Part 2: Numerical Calculation**\n\nThe given data are $n=5$, $p=2$, $\\sigma^2 = 0.25$, and the design matrix:\n$$\nX = \\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}\n$$\nFirst, we compute the matrix $X^T X$:\n$$\nX^T X = \\begin{pmatrix}\n1  2  3  4  5 \\\\\n1.002  2.001  3.000  4.001  5.002\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1.002 \\\\\n2  2.001 \\\\\n3  3.000 \\\\\n4  4.001 \\\\\n5  5.002\n\\end{pmatrix}\n$$\nThe elements are:\n$(X^T X)_{11} = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 1+4+9+16+25 = 55$\n$(X^T X)_{12} = (X^T X)_{21} = 1(1.002) + 2(2.001) + 3(3.000) + 4(4.001) + 5(5.002) = 1.002 + 4.002 + 9 + 16.004 + 25.010 = 55.018$\n$(X^T X)_{22} = 1.002^2 + 2.001^2 + 3.000^2 + 4.001^2 + 5.002^2 = 1.004004 + 4.004001 + 9 + 16.008001 + 25.020004 = 55.03601$\nSo, the matrix is:\n$$\nX^T X = \\begin{pmatrix} 55  55.018 \\\\ 55.018  55.03601 \\end{pmatrix}\n$$\nNext, we compute the inverse of this $2 \\times 2$ matrix. The inverse of a matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ is $\\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nThe determinant is:\n$$\n\\det(X^T X) = (55)(55.03601) - (55.018)^2 = 3026.98055 - 3026.980324 = 0.000226\n$$\nThe inverse is:\n$$\n(X^T X)^{-1} = \\frac{1}{0.000226} \\begin{pmatrix} 55.03601  -55.018 \\\\ -55.018  55 \\end{pmatrix}\n$$\nWe need the $(1,1)$ element of this matrix, which is:\n$$\n[(X^T X)^{-1}]_{11} = \\frac{55.03601}{0.000226}\n$$\nNow we can compute the variance of $\\hat{\\beta}_1$ using $\\sigma^2 = 0.25$:\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 [(X^T X)^{-1}]_{11} = 0.25 \\times \\frac{55.03601}{0.000226} \\approx 60880.542\n$$\nRounding to four significant figures, we get $60880$. In scientific notation, this is $6.088 \\times 10^4$.\n\n**Part 3: Interpretation and Relevance of Regularization**\n\nThe calculated variance, $\\text{Var}(\\hat{\\beta}_1) \\approx 6.088 \\times 10^4$, is an exceptionally large value. A large variance for a coefficient estimator implies that the estimate is very imprecise and unstable. If we were to collect a different sample of data from the same population, the estimated value of $\\hat{\\beta}_1$ could change dramatically. This makes the coefficient estimate unreliable for inference or prediction.\n\nThe root cause of this high variance is severe multicollinearity. Multicollinearity occurs when predictor variables in a regression model are highly correlated. In this problem, the two columns of the design matrix $X$ are nearly identical, indicating an extremely high correlation. The correlation coefficient between the two biomarker measurements is virtually $1$. When predictors are highly correlated, the matrix $X^T X$ becomes nearly singular (i.e., its determinant is close to zero). In our case, $\\det(X^T X) = 0.000226$, which is a very small number. The process of inverting a nearly-singular matrix is numerically unstable and leads to an inverse matrix with very large entries. Since the variances of the OLS estimators are proportional to the diagonal elements of this inverse matrix (as shown by the formula $\\text{Cov}(\\hat{\\beta}) = \\sigma^2(X^T X)^{-1}$), the variances are \"inflated\" to very large values. This phenomenon is often quantified by the Variance Inflation Factor (VIF).\n\nRegularization methods are specifically designed to address the problem of multicollinearity. OLS fails in this situation because it seeks an unbiased estimator, which comes at the cost of enormous variance. Regularization methods introduce a small amount of bias in the estimates to achieve a large reduction in variance, leading to a much lower overall mean squared error ($MSE = \\text{Variance} + \\text{Bias}^2$).\n\n-   **Ridge Regression**: Ridge regression adds a penalty to the OLS objective function, minimizing $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum \\beta_j^2$. The resulting estimator is $\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I)^{-1} X^T y$, where $\\lambda > 0$ is a tuning parameter. The addition of the term $\\lambda I$ to $X^T X$ makes the matrix to be inverted well-conditioned and non-singular, even when $X^T X$ is not. This stabilizes the inverse and produces coefficient estimates with much lower variance.\n-   **Lasso (Least Absolute Shrinkage and Selection Operator)**: The lasso minimizes $\\sum (y_i - X_i\\beta)^2 + \\lambda \\sum |\\beta_j|$. Like ridge, it shrinks coefficients toward zero to reduce variance. However, the $L_1$ penalty has the unique property of shrinking some coefficients to exactly zero. In a case of two highly correlated predictors like this, the lasso is likely to select one of the biomarkers and assign it a non-zero coefficient while eliminating the other by setting its coefficient to zero. This provides a simpler, more interpretable model by performing automatic variable selection.\n\nIn conclusion, the extreme variance of the OLS estimator highlights its breakdown in the presence of severe multicollinearity. Regularization techniques like ridge and lasso are crucial statistical tools that provide stable, reliable, and more interpretable models in such common biostatistical scenarios.", "answer": "$$\\boxed{6.088 \\times 10^4}$$", "id": "4947371"}, {"introduction": "Having established the need for regularization, we now turn to the practical question of how to compute the coefficient estimates for Ridge and Lasso models. Instead of solving a complex multivariate optimization problem all at once, we can use a powerful and intuitive algorithm called coordinate descent, which optimizes the model one coefficient at a time. This hands-on coding practice will guide you through implementing these single-coordinate updates, revealing how the mathematical forms of the Ridge and Lasso penalties translate into distinct computational steps [@problem_id:4947418].", "problem": "Consider a single-coordinate update in penalized linear regression, a method widely used in biostatistics for controlling variance and improving prediction in high-dimensional biological data. Let there be $n$ observations. Let the response vector be $y \\in \\mathbb{R}^n$, and suppose the current fitted model has residual vector $r \\in \\mathbb{R}^n$ defined by $r = y - X \\beta$, where $X \\in \\mathbb{R}^{n \\times p}$ is a design matrix and $\\beta \\in \\mathbb{R}^p$ is the current coefficient vector. Focus on a single predictor column $x \\in \\mathbb{R}^n$ corresponding to one coordinate, whose current coefficient is $b \\in \\mathbb{R}$. A one-step coordinate update changes $b$ by an amount $d \\in \\mathbb{R}$ to a new coefficient $b_{\\text{new}} = b + d$, producing a new residual vector $r' = r - x d$ and a new residual sum of squares (RSS) given by $RSS' = \\lVert r' \\rVert_2^2$.\n\nStarting only from the definitions of least squares and convex penalties:\n- The residual sum of squares is $RSS(\\beta) = \\lVert y - X \\beta \\rVert_2^2$.\n- The ridge objective is $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\tfrac{\\alpha}{2} \\lVert \\beta \\rVert_2^2$ for a penalty parameter $\\alpha \\ge 0$.\n- The lasso objective is $\\tfrac{1}{2} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1$ for a penalty parameter $\\lambda \\ge 0$.\n\nYour task is to implement a single-coordinate update for $b$ and compute the exact change in residual sum of squares as a function of the update. The update must be determined by minimizing the appropriate penalized objective with respect to the single coordinate, holding all other coordinates fixed. Use the partial residual $r_j = y - X_{-j} \\beta_{-j}$ conceptually, but in computation you are given $r$ and $b$, so you should form $r_j$ using $r_j = r + x b$ to eliminate dependence on the unknown submodel. Then perform the update for one of the following penalties:\n- Ridge: use the penalty parameter $\\alpha$.\n- Lasso: use the penalty parameter $\\lambda$.\n\nAfter computing $b_{\\text{new}}$, define $d = b_{\\text{new}} - b$, and compute the exact change in residual sum of squares $\\Delta RSS = RSS' - RSS$ as a function of $d$, $x$, and $r$.\n\nImplement a program that, for each test case below, performs:\n- Construct $r_j = r + x b$.\n- Compute the one-step coordinate update $b_{\\text{new}}$ minimizing the chosen penalized objective along coordinate $b$.\n- Compute $d = b_{\\text{new}} - b$.\n- Compute the exact change in residual sum of squares $\\Delta RSS$ after applying $d$ to $b$ (without changing any other coordinates).\n\nReport $\\Delta RSS$ for each test case as a float.\n\nTest suite (each case gives $n$, $x$, $r$, $b$, and the penalty specification):\n1. Ridge happy path:\n   - $n = 5$\n   - $x = [1.2, -0.3, 0.5, 0.0, 2.1]$\n   - $r = [0.8, -1.1, 0.3, 0.0, 2.0]$\n   - $b = 0.4$\n   - Ridge with $\\alpha = 0.7$.\n2. Lasso happy path (nonzero update expected):\n   - $n = 6$\n   - $x = [0.5, -1.0, 0.3, 1.2, -0.7, 0.4]$\n   - $r = [1.0, 0.5, -0.2, 0.3, -1.5, 0.1]$\n   - $b = -0.1$\n   - Lasso with $\\lambda = 0.2$.\n3. Lasso thresholding to zero (edge case):\n   - $n = 4$\n   - $x = [0.2, -0.1, 0.05, 0.0]$\n   - $r = [0.01, -0.02, 0.03, -0.04]$\n   - $b = 0.5$\n   - Lasso with $\\lambda = 0.2$.\n4. Ridge with zero predictor column (boundary case):\n   - $n = 5$\n   - $x = [0.0, 0.0, 0.0, 0.0, 0.0]$\n   - $r = [1.0, -1.0, 0.5, -0.5, 2.0]$\n   - $b = 3.0$\n   - Ridge with $\\alpha = 1.0$.\n5. Lasso with numerically tiny predictor (numerical stability case):\n   - $n = 3$\n   - $x = [10^{-8}, -2 \\cdot 10^{-8}, 3 \\cdot 10^{-8}]$\n   - $r = [0.0, 0.0, 0.0]$\n   - $b = 1.0$\n   - Lasso with $\\lambda = 0.1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[result_1,result_2,\\dots]$, in the exact order of the test cases listed above.", "solution": "The problem requires the derivation and implementation of a single-coordinate update for ridge and lasso regularized linear regression, and the computation of the resulting change in the residual sum of squares ($RSS$). The problem is scientifically grounded, well-posed, and self-contained. The provided definitions and test cases are consistent and valid. We may therefore proceed with a solution.\n\nThe general objective function for penalized linear regression is:\n$$L(\\beta) = \\frac{1}{2}\\lVert y - X \\beta \\rVert_2^2 + P(\\beta)$$\nwhere $P(\\beta)$ is a penalty function. We aim to update a single coefficient, denoted by $b$, corresponding to a predictor column $x$. All other coefficients are held fixed. The new coefficient will be $b_{\\text{new}}$. The objective function, viewed as a function of $b_{\\text{new}}$ alone, can be expressed using the partial residual $r_j = y - \\sum_{k \\neq j} x_k \\beta_k$. As specified, this can be computed from the full current residual $r = y - X\\beta$ as $r_j = r + xb$.\n\nThe $RSS$ portion of the objective can be written as:\n$$\\frac{1}{2}\\lVert r_j - x b_{\\text{new}} \\rVert_2^2 = \\frac{1}{2}(r_j - x b_{\\text{new}})^T(r_j - x b_{\\text{new}}) = \\frac{1}{2} (\\lVert r_j \\rVert_2^2 - 2 b_{\\text{new}} (r_j^T x) + b_{\\text{new}}^2 \\lVert x \\rVert_2^2)$$\nThis is a quadratic function of $b_{\\text{new}}$. The term $\\frac{1}{2}\\lVert r_j \\rVert_2^2$ is constant with respect to $b_{\\text{new}}$ and can be ignored during minimization.\n\n**1. Ridge Regression Update**\n\nFor ridge regression, the penalty is $P(\\beta) = \\frac{\\alpha}{2} \\lVert \\beta \\rVert_2^2 = \\frac{\\alpha}{2} \\sum_k \\beta_k^2$. The objective function for the single coordinate $b_{\\text{new}}$ is:\n$$L_{\\text{ridge}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\frac{\\alpha}{2} b_{\\text{new}}^2 + \\text{const}$$\nThis function is differentiable and convex. To find the minimum, we set its derivative with respect to $b_{\\text{new}}$ to zero:\n$$\\frac{\\partial L_{\\text{ridge}}}{\\partial b_{\\text{new}}} = -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\alpha b_{\\text{new}} = 0$$\nSolving for $b_{\\text{new}}$:\n$$b_{\\text{new}} (\\lVert x \\rVert_2^2 + \\alpha) = r_j^T x$$\n$$b_{\\text{new}} = \\frac{r_j^T x}{\\lVert x \\rVert_2^2 + \\alpha}$$\nSubstituting $r_j = r + xb$:\n$$r_j^T x = (r + xb)^T x = r^T x + b(x^T x) = r^T x + b \\lVert x \\rVert_2^2$$\nThus, the update rule for the new coefficient is:\n$$b_{\\text{new}} = \\frac{r^T x + b \\lVert x \\rVert_2^2}{\\lVert x \\rVert_2^2 + \\alpha}$$\nIf the predictor column $x$ is a zero vector, $\\lVert x \\rVert_2^2 = 0$ and $r^T x = 0$, leading to $b_{\\text{new}} = 0$. Since $\\alpha \\ge 0$, the denominator is always positive if $\\alpha > 0$ or $\\lVert x \\rVert_2^2 > 0$.\n\n**2. Lasso Regression Update**\n\nFor lasso regression, the penalty is $P(\\beta) = \\lambda \\lVert \\beta \\rVert_1 = \\lambda \\sum_k |\\beta_k|$. The objective for the single coordinate $b_{\\text{new}}$ is:\n$$L_{\\text{lasso}}(b_{\\text{new}}) = \\frac{1}{2} \\lVert r_j - x b_{\\text{new}} \\rVert_2^2 + \\lambda |b_{\\text{new}}| + \\text{const}$$\nThis objective is convex but not differentiable at $b_{\\text{new}} = 0$. We use subgradient optimization. The subgradient of the objective is:\n$$\\partial L_{\\text{lasso}}(b_{\\text{new}}) = -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\partial |b_{\\text{new}}|$$\nwhere $\\partial |u|$ is the subgradient of the absolute value function: $\\text{sgn}(u)$ for $u \\neq 0$ and the interval $[-1, 1]$ for $u=0$.\nAt the minimum, the subgradient set must contain $0$:\n$$0 \\in -r_j^T x + b_{\\text{new}} \\lVert x \\rVert_2^2 + \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\nLet $z = r_j^T x = r^T x + b \\lVert x \\rVert_2^2$ and assume $\\lVert x \\rVert_2^2 > 0$. The condition becomes:\n$$z - b_{\\text{new}} \\lVert x \\rVert_2^2 \\in \\lambda \\cdot \\text{sgn}(b_{\\text{new}})$$\nThis yields the solution:\n- If $z > \\lambda$, then $b_{\\text{new}} > 0$: $z - b_{\\text{new}} \\lVert x \\rVert_2^2 = \\lambda \\implies b_{\\text{new}} = (z - \\lambda) / \\lVert x \\rVert_2^2$.\n- If $z  -\\lambda$, then $b_{\\text{new}}  0$: $z - b_{\\text{new}} \\lVert x \\rVert_2^2 = -\\lambda \\implies b_{\\text{new}} = (z + \\lambda) / \\lVert x \\rVert_2^2$.\n- If $|z| \\le \\lambda$, then $b_{\\text{new}} = 0$.\n\nThis is the soft-thresholding function, $S(z, \\lambda) = \\text{sgn}(z) \\max(|z|-\\lambda, 0)$. The update rule is:\n$$b_{\\text{new}} = \\frac{S(r^T x + b \\lVert x \\rVert_2^2, \\lambda)}{\\lVert x \\rVert_2^2}$$\nIf $\\lVert x \\rVert_2^2 = 0$, the objective to minimize is simply $\\lambda |b_{\\text{new}}|$ (plus a constant), which is minimized at $b_{\\text{new}} = 0$. This case must be handled separately in implementation.\n\n**3. Change in Residual Sum of Squares ($\\Delta RSS$)**\n\nThe problem asks for the exact change in $RSS$, defined as $\\Delta RSS = RSS' - RSS$.\nThe current $RSS$ is $\\lVert r \\rVert_2^2$.\nThe new residual is $r' = r - xd$, where $d = b_{\\text{new}} - b$ is the change in the coefficient.\nThe new $RSS'$ is:\n$$RSS' = \\lVert r' \\rVert_2^2 = \\lVert r - xd \\rVert_2^2 = (r - xd)^T(r - xd)$$\n$$RSS' = r^T r - (xd)^T r - r^T(xd) + (xd)^T(xd)$$\nSince $r^T(xd) = d(r^T x)$ is a scalar, it equals its transpose $(xd)^T r = d(x^T r)$.\n$$RSS' = \\lVert r \\rVert_2^2 - 2d(r^T x) + d^2(x^T x) = RSS - 2d(r^T x) + d^2 \\lVert x \\rVert_2^2$$\nTherefore, the change in $RSS$ is:\n$$\\Delta RSS = RSS' - RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^T x)$$\nThis formula allows for direct computation of $\\Delta RSS$ from the update $d$, the predictor $x$, and the current residual $r$, without needing to compute the new residual vector $r'$.\n\n**Algorithm Summary**\nFor each test case:\n1.  Given $x, r, b$ and penalty parameters.\n2.  Compute scalar quantities: $r^T x$ and $\\lVert x \\rVert_2^2$.\n3.  Based on the penalty type (Ridge or Lasso), compute the updated coefficient $b_{\\text{new}}$ using the derived formulae. Handle the special case $\\lVert x \\rVert_2^2 = 0$.\n4.  Compute the change in the coefficient: $d = b_{\\text{new}} - b$.\n5.  Compute the change in $RSS$: $\\Delta RSS = d^2 \\lVert x \\rVert_2^2 - 2d(r^T x)$.\n6.  Report the value of $\\Delta RSS$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of single-coordinate update problems for Ridge and Lasso\n    regression and computes the change in Residual Sum of Squares (RSS).\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Ridge happy path\n        {'x': [1.2, -0.3, 0.5, 0.0, 2.1],\n         'r': [0.8, -1.1, 0.3, 0.0, 2.0],\n         'b': 0.4,\n         'penalty': 'ridge',\n         'param': 0.7},\n\n        # Case 2: Lasso happy path (nonzero update expected)\n        {'x': [0.5, -1.0, 0.3, 1.2, -0.7, 0.4],\n         'r': [1.0, 0.5, -0.2, 0.3, -1.5, 0.1],\n         'b': -0.1,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 3: Lasso thresholding to zero (edge case)\n        {'x': [0.2, -0.1, 0.05, 0.0],\n         'r': [0.01, -0.02, 0.03, -0.04],\n         'b': 0.5,\n         'penalty': 'lasso',\n         'param': 0.2},\n\n        # Case 4: Ridge with zero predictor column (boundary case)\n        {'x': [0.0, 0.0, 0.0, 0.0, 0.0],\n         'r': [1.0, -1.0, 0.5, -0.5, 2.0],\n         'b': 3.0,\n         'penalty': 'ridge',\n         'param': 1.0},\n\n        # Case 5: Lasso with numerically tiny predictor (numerical stability case)\n        {'x': [1e-8, -2e-8, 3e-8],\n         'r': [0.0, 0.0, 0.0],\n         'b': 1.0,\n         'penalty': 'lasso',\n         'param': 0.1}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_vec = np.array(case['x'], dtype=np.float64)\n        r_vec = np.array(case['r'], dtype=np.float64)\n        b = case['b']\n        penalty_type = case['penalty']\n        param = case['param']\n        \n        b_new = 0.0\n\n        # Compute scalar quantities needed for the updates\n        x_sq_norm = np.dot(x_vec, x_vec) # Corresponds to ||x||_2^2\n        r_dot_x = np.dot(r_vec, x_vec)   # Corresponds to r^T x\n\n        if penalty_type == 'ridge':\n            # Ridge update: b_new = (r^T x + b ||x||^2) / (||x||^2 + alpha)\n            alpha = param\n            numerator = r_dot_x + b * x_sq_norm\n            denominator = x_sq_norm + alpha\n            # Denominator is guaranteed to be non-zero since alpha = 0 and we\n            # handle the pure ||x||^2 = 0 case implicitly. If ||x||^2 is 0,\n            # numerator is 0, so b_new becomes 0 correctly.\n            if denominator != 0:\n                b_new = numerator / denominator\n            else:\n                 # This case only happens if x_sq_norm=0 and alpha=0,\n                 # which is unpenalized OLS and ill-defined for a zero predictor.\n                 # With alpha0, this will not occur.\n                 b_new = 0.0\n\n        elif penalty_type == 'lasso':\n            # Lasso update: b_new = S(r^T x + b ||x||^2, lambda) / ||x||^2\n            # where S is the soft-thresholding operator.\n            lmbda = param\n            \n            # Handle the case where the predictor is a zero vector.\n            # In this case, the coordinate has no effect on RSS, so the\n            # L1 penalty lambda * |b_new| is minimized at b_new = 0.\n            if x_sq_norm == 0:\n                b_new = 0.0\n            else:\n                z = r_dot_x + b * x_sq_norm\n                if z  lmbda:\n                    b_new = (z - lmbda) / x_sq_norm\n                elif z  -lmbda:\n                    b_new = (z + lmbda) / x_sq_norm\n                else: # |z| = lmbda\n                    b_new = 0.0\n        \n        # Compute the change in coefficient\n        d = b_new - b\n        \n        # Compute the exact change in RSS: Delta_RSS = d^2 ||x||^2 - 2d (r^T x)\n        delta_rss = d**2 * x_sq_norm - 2 * d * r_dot_x\n        \n        results.append(delta_rss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4947418"}, {"introduction": "The coordinate descent algorithm generates a 'solution path' for each coefficient, showing how its value changes as the penalty parameter $\\lambda$ is varied. A common intuition is that as we relax the penalty by decreasing $\\lambda$, coefficient magnitudes should only increase. This final practice problem challenges that simple notion by exploring a fascinating and counterintuitive property of the Lasso in non-orthogonal designs. By carefully tracing the solution path for a specific coefficient, you will demonstrate that its value can actually decrease as $\\lambda$ gets smaller, a non-monotonic behavior that highlights the sophisticated nature of the Lasso solution [@problem_id:4947423].", "problem": "A biostatistics team is modeling a continuous clinical outcome using three standardized biomarker predictors collected on the same set of individuals. Let $X \\in \\mathbb{R}^{n \\times 3}$ be the centered design matrix with non-orthogonal columns, and let $y \\in \\mathbb{R}^{n}$ be the centered outcome. The Gram matrix is $G = X^{\\top} X$ and the predictor–outcome inner products are $c = X^{\\top} y$. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator, defined by the optimization problem\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{3}} \\left\\{ \\frac{1}{2} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\}, \\quad \\lambda \\ge 0,\n$$\nwhere $\\|\\beta\\|_{1} = | \\beta_{1} | + | \\beta_{2} | + | \\beta_{3} |$. The columns of $X$ are non-orthogonal and satisfy\n$$\nG = X^{\\top} X = \\frac{1}{12} \\begin{pmatrix}\n24  18  18 \\\\\n18  16  14 \\\\\n18  14  16\n\\end{pmatrix}\n= \\begin{pmatrix}\n2  \\frac{3}{2}  \\frac{3}{2} \\\\\n\\frac{3}{2}  \\frac{4}{3}  \\frac{7}{6} \\\\\n\\frac{3}{2}  \\frac{7}{6}  \\frac{4}{3}\n\\end{pmatrix},\n\\qquad\nc = \\begin{pmatrix}\n3 \\\\ 2.5 \\\\ 2.2\n\\end{pmatrix}.\n$$\nStarting from the fundamental subgradient optimality conditions for the LASSO, derive the piecewise linear path for the first coefficient $\\beta_{1}(\\lambda)$ as $\\lambda$ decreases from $\\lambda = 3$ toward $0$, up to the point at which the third predictor becomes active. Show that $\\beta_{1}(\\lambda)$ initially increases as $\\lambda$ decreases, and then decreases over a subsequent range of $\\lambda$, thereby exhibiting non-monotonic path behavior due to the non-orthogonal design.\n\nCompute the exact value of the maximum of $\\beta_{1}(\\lambda)$ over all $\\lambda \\ge 0$ for this design and these $c$ values. Give the exact numerical value; no rounding is required.", "solution": "We begin from the LASSO objective\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\left\\{ \\frac{1}{2} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\},\n$$\nand use the Karush–Kuhn–Tucker (KKT) subgradient optimality conditions. Write $G = X^{\\top} X$ and $c = X^{\\top} y$. The KKT conditions state there exists a vector $s \\in \\mathbb{R}^{3}$ with components $s_{j} \\in [-1,1]$ and $s_{j} = \\operatorname{sign}(\\beta_{j})$ when $\\beta_{j} \\neq 0$, such that\n$$\nc - G \\beta = \\lambda s,\n$$\nwith the constraint $| c_{j} - (G \\beta)_{j} | \\le \\lambda$ for any inactive coordinate $j$ where $\\beta_{j} = 0$.\n\nWe will track the path as $\\lambda$ decreases.\n\nStep 1: The first variable enters. At $\\beta = 0$, the KKT conditions reduce to $c = \\lambda s$, so no coefficient is nonzero when $\\lambda  \\max_{j} |c_{j}| = 3$. As $\\lambda$ decreases to $\\lambda = 3$, $\\beta_{1}$ becomes active with positive sign since $c_{1} = 3$. With active set $A = \\{1\\}$ and $s_{1} = +1$, the KKT condition for $j = 1$ gives\n$$\nc_{1} - (G \\beta)_{1} = \\lambda s_{1} \\quad \\Rightarrow \\quad 3 - g_{11} \\beta_{1} = \\lambda,\n$$\nso\n$$\n\\beta_{1}(\\lambda) = \\frac{3 - \\lambda}{g_{11}} = \\frac{3 - \\lambda}{2}, \\quad \\text{for } \\lambda \\in [\\lambda_{2}^{\\text{enter}}, 3].\n$$\nIn this single-variable regime, $\\frac{d \\beta_{1}}{d \\lambda} = - \\frac{1}{2}  0$, so as $\\lambda$ decreases, $\\beta_{1}$ increases.\n\nTo find when variable $2$ enters, we monitor the correlation of predictor $2$ with the current residual:\n$$\nr_{2}(\\lambda) = c_{2} - g_{21} \\beta_{1}(\\lambda) = 2.5 - \\frac{3}{2} \\cdot \\frac{3 - \\lambda}{2} = 2.5 - \\frac{3}{4} (3 - \\lambda) = 2.5 - \\frac{9}{4} + \\frac{3}{4} \\lambda = \\frac{1}{4} + \\frac{3}{4} \\lambda.\n$$\nVariable $2$ becomes active when $| r_{2}(\\lambda) | = \\lambda$. Because $r_{2}(\\lambda)  0$ for $\\lambda \\ge 0$, we solve $r_{2}(\\lambda) = \\lambda$:\n$$\n\\frac{1}{4} + \\frac{3}{4} \\lambda = \\lambda \\quad \\Rightarrow \\quad \\frac{1}{4} = \\frac{1}{4} \\lambda \\quad \\Rightarrow \\quad \\lambda = 1.\n$$\nThus variable $2$ enters with positive sign $s_{2} = +1$ at $\\lambda = 1$. At that moment, from the one-variable expression, $\\beta_{1}(1) = \\frac{3 - 1}{2} = 1$.\n\nStep 2: Active set $A = \\{1,2\\}$ with $s_{A} = (1,1)$. Restricting to this active set, the KKT equation restricted to $A$ is\n$$\nc_{A} - G_{A} \\beta_{A} = \\lambda s_{A},\n$$\nso\n$$\n\\beta_{A}(\\lambda) = G_{A}^{-1} \\left( c_{A} - \\lambda s_{A} \\right).\n$$\nCompute $G_{A}$ and its inverse:\n$$\nG_{A} = \\begin{pmatrix}\ng_{11}  g_{12} \\\\\ng_{21}  g_{22}\n\\end{pmatrix}\n= \\begin{pmatrix}\n2  \\frac{3}{2} \\\\\n\\frac{3}{2}  \\frac{4}{3}\n\\end{pmatrix}, \n\\quad \\det(G_{A}) = \\frac{5}{12}, \n\\quad G_{A}^{-1} = \\frac{12}{5} \\begin{pmatrix}\n\\frac{4}{3}  -\\frac{3}{2} \\\\\n-\\frac{3}{2}  2\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{16}{5}  -\\frac{18}{5} \\\\\n-\\frac{18}{5}  \\frac{24}{5}\n\\end{pmatrix}.\n$$\nWith $c_{A} = (3, 2.5)^{\\top}$ and $s_{A} = (1,1)^{\\top}$,\n$$\n\\beta_{A}(\\lambda) = \\begin{pmatrix}\n\\frac{16}{5}  -\\frac{18}{5} \\\\\n-\\frac{18}{5}  \\frac{24}{5}\n\\end{pmatrix}\n\\begin{pmatrix}\n3 - \\lambda \\\\ 2.5 - \\lambda\n\\end{pmatrix}.\n$$\nCompute the components:\n$$\n\\beta_{1}(\\lambda) = \\frac{16}{5}(3 - \\lambda) - \\frac{18}{5}(2.5 - \\lambda) = \\frac{48 - 16 \\lambda - 45 + 18 \\lambda}{5} = \\frac{3 + 2 \\lambda}{5},\n$$\n$$\n\\beta_{2}(\\lambda) = -\\frac{18}{5}(3 - \\lambda) + \\frac{24}{5}(2.5 - \\lambda) = \\frac{-54 + 18 \\lambda + 60 - 24 \\lambda}{5} = \\frac{6 - 6 \\lambda}{5} = \\frac{6}{5}(1 - \\lambda).\n$$\nIn this two-variable regime, we have \n$$\n\\frac{d \\beta_{1}}{d \\lambda} = \\frac{2}{5}  0,\n$$\nso as $\\lambda$ decreases, $\\beta_{1}(\\lambda)$ decreases. Thus $\\beta_{1}(\\lambda)$ increased for $\\lambda$ decreasing from $3$ to $1$, and then decreases for $\\lambda$ decreasing below $1$, exhibiting non-monotonic behavior.\n\nStep 3: Check when variable $3$ enters to bound the interval of decrease. The residual correlation for predictor $3$ in the two-variable regime is\n$$\nr_{3}(\\lambda) = c_{3} - g_{31} \\beta_{1}(\\lambda) - g_{32} \\beta_{2}(\\lambda)\n= 2.2 - \\frac{3}{2} \\cdot \\frac{3 + 2 \\lambda}{5} - \\frac{7}{6} \\cdot \\frac{6}{5}(1 - \\lambda)\n= 2.2 - \\frac{9 + 6 \\lambda}{10} - \\frac{7}{5}(1 - \\lambda).\n$$\nSimplify to\n$$\nr_{3}(\\lambda) = 2.2 - \\frac{9}{10} - \\frac{7}{5} + \\left( - \\frac{6}{10} + \\frac{7}{5} \\right) \\lambda\n= 2.2 - \\frac{23}{10} + \\frac{4}{5} \\lambda\n= -\\frac{1}{10} + \\frac{4}{5} \\lambda.\n$$\nVariable $3$ becomes active when $| r_{3}(\\lambda) | = \\lambda$. Since $r_{3}(\\lambda)$ is negative near $\\lambda = 0$, we solve $r_{3}(\\lambda) = - \\lambda$:\n$$\n-\\frac{1}{10} + \\frac{4}{5} \\lambda = - \\lambda\n\\quad \\Rightarrow \\quad\n-\\frac{1}{10} = - \\frac{9}{5} \\lambda\n\\quad \\Rightarrow \\quad\n\\lambda = \\frac{1}{18}.\n$$\nThus the decreasing phase for $\\beta_{1}(\\lambda)$ persists on $(\\frac{1}{18}, 1)$.\n\nStep 4: Identify the maximum of $\\beta_{1}(\\lambda)$. From the single-variable phase,\n$$\n\\beta_{1}(\\lambda) = \\frac{3 - \\lambda}{2}, \\quad \\lambda \\in [1, 3],\n$$\nwhich increases as $\\lambda$ decreases. At $\\lambda = 1$, the value is $\\beta_{1}(1) = 1$. In the two-variable phase,\n$$\n\\beta_{1}(\\lambda) = \\frac{3 + 2 \\lambda}{5}, \\quad \\lambda \\in \\left[\\frac{1}{18}, 1\\right],\n$$\nwhich decreases as $\\lambda$ decreases. Therefore, $\\beta_{1}(\\lambda)$ attains a local (and, as we will see, global) maximum at $\\lambda = 1$ with value $1$. For completeness, the unpenalized limit at $\\lambda = 0$ is the ordinary least squares value $\\beta(0) = G^{-1} c$, and the first component is\n$$\n\\beta_{1}(0) = (G^{-1} c)_{1} = \\begin{pmatrix} 5  -3  -3 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 2.5 \\\\ 2.2 \\end{pmatrix} = 15 - 7.5 - 6.6 = 0.9  1,\n$$\nconfirming that the maximum value $1$ at $\\lambda = 1$ is indeed the global maximum over $\\lambda \\ge 0$.\n\nHence, the specified non-orthogonal design produces a lasso path for $\\beta_{1}(\\lambda)$ that increases as $\\lambda$ decreases from $3$ to $1$, then decreases on $(\\frac{1}{18}, 1)$, illustrating non-monotonic behavior, and its maximum value over $\\lambda \\ge 0$ is exactly $1$.", "answer": "$$\\boxed{1}$$", "id": "4947423"}]}