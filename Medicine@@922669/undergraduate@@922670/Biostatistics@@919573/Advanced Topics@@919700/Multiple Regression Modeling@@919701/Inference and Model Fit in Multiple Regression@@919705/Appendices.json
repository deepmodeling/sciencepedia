{"hands_on_practices": [{"introduction": "This exercise grounds your understanding of multiple regression in its fundamental principle: Ordinary Least Squares (OLS). By deriving the estimator from the goal of minimizing squared errors and applying it to a simple dataset, you will solidify your grasp of how regression coefficients are determined [@problem_id:4915381]. This practice also reinforces a key geometric property of OLSâ€”the orthogonality of residuals to the predictor space.", "problem": "A biostatistician studies the association between a log-transformed inflammatory marker and two standardized covariates in a small pilot dataset of four adult participants. The linear multiple regression model is specified as $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\varepsilon_{i}$, where $y_{i}$ is the log marker, $x_{1i}$ is a centered body mass index contrast, $x_{2i}$ is a centered physical activity contrast, and $\\varepsilon_{i}$ is the error term. Ordinary Least Squares (OLS) estimates are defined by minimizing the sum of squared residuals $S(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left(y_{i} - \\beta_{0} - \\beta_{1} x_{1i} - \\beta_{2} x_{2i}\\right)^{2}$ under full column rank of the design matrix. The observed design matrix $X$ and outcome vector $\\boldsymbol{y}$ are\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & -1 & -1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & 1 & 1\n\\end{pmatrix}, \n\\qquad\n\\boldsymbol{y} \\;=\\; \\begin{pmatrix}\n1 \\\\ 3 \\\\ 5 \\\\ 7\n\\end{pmatrix}.\n$$\nUsing only the fundamental definition of OLS as the minimizer of $S(\\boldsymbol{\\beta})$, derive the estimator $\\hat{\\boldsymbol{\\beta}}$ from first principles and compute its exact numerical values. Then, verify the residual orthogonality condition by explicitly computing $X^{\\top} \\hat{\\boldsymbol{\\varepsilon}}$ where $\\hat{\\boldsymbol{\\varepsilon}} = \\boldsymbol{y} - X \\hat{\\boldsymbol{\\beta}}$, and check that each component is $0$. Express your final coefficient estimates exactly (no rounding) and report them as a single row vector. No units are required for the final reported values.", "solution": "The problem requires the derivation and computation of the Ordinary Least Squares (OLS) estimator for a multiple linear regression model, followed by a verification of the residual orthogonality condition. The derivation must start from the fundamental definition of OLS, which is the minimization of the sum of squared residuals, $S(\\boldsymbol{\\beta})$.\n\nThe model is given by $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\varepsilon_{i}$. In matrix form, this is $\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{y}$ is the $n \\times 1$ vector of outcomes, $X$ is the $n \\times (p+1)$ design matrix, $\\boldsymbol{\\beta}$ is the $(p+1) \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of errors. Here, $n=4$ and $p=2$.\n\nThe design matrix $X$ and outcome vector $\\boldsymbol{y}$ are provided as:\n$$\nX = \\begin{pmatrix}\n1 & -1 & -1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & 1 & 1\n\\end{pmatrix},\n\\qquad\n\\boldsymbol{y} = \\begin{pmatrix}\n1 \\\\ 3 \\\\ 5 \\\\ 7\n\\end{pmatrix}\n$$\nThe vector of coefficients is $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^{\\top}$.\n\nThe sum of squared residuals $S(\\boldsymbol{\\beta})$ is defined as:\n$$\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}))^2\n$$\nIn matrix notation, this is equivalent to the squared Euclidean norm of the residual vector $\\boldsymbol{\\varepsilon} = \\boldsymbol{y} - X\\boldsymbol{\\beta}$:\n$$\nS(\\boldsymbol{\\beta}) = (\\boldsymbol{y} - X\\boldsymbol{\\beta})^{\\top} (\\boldsymbol{y} - X\\boldsymbol{\\beta})\n$$\nExpanding this expression gives:\n$$\nS(\\boldsymbol{\\beta}) = (\\boldsymbol{y}^{\\top} - \\boldsymbol{\\beta}^{\\top}X^{\\top}) (\\boldsymbol{y} - X\\boldsymbol{\\beta}) = \\boldsymbol{y}^{\\top}\\boldsymbol{y} - \\boldsymbol{y}^{\\top}X\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y} + \\boldsymbol{\\beta}^{\\top}X^{\\top}X\\boldsymbol{\\beta}\n$$\nSince the term $\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y}$ is a scalar, it is equal to its transpose, $(\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y})^{\\top} = \\boldsymbol{y}^{\\top}X\\boldsymbol{\\beta}$. Thus, we can combine the two middle terms:\n$$\nS(\\boldsymbol{\\beta}) = \\boldsymbol{y}^{\\top}\\boldsymbol{y} - 2\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y} + \\boldsymbol{\\beta}^{\\top}X^{\\top}X\\boldsymbol{\\beta}\n$$\nTo find the vector of coefficients $\\hat{\\boldsymbol{\\beta}}$ that minimizes $S(\\boldsymbol{\\beta})$, we take the gradient of $S(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and set it to the zero vector. Using standard matrix calculus rules for differentiation, we have:\n$$\n\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} (\\boldsymbol{y}^{\\top}\\boldsymbol{y} - 2\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y} + \\boldsymbol{\\beta}^{\\top}X^{\\top}X\\boldsymbol{\\beta}) = -2X^{\\top}\\boldsymbol{y} + 2X^{\\top}X\\boldsymbol{\\beta}\n$$\nSetting the gradient to zero to find the OLS estimator $\\hat{\\boldsymbol{\\beta}}$:\n$$\n-2X^{\\top}\\boldsymbol{y} + 2X^{\\top}X\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\n$$\n$$\nX^{\\top}X\\hat{\\boldsymbol{\\beta}} = X^{\\top}\\boldsymbol{y}\n$$\nThis is the system of linear equations known as the normal equations. Since the problem states that the design matrix has full column rank, the matrix $X^{\\top}X$ is invertible. We can therefore solve for $\\hat{\\boldsymbol{\\beta}}$ by pre-multiplying by the inverse of $X^{\\top}X$:\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^{\\top}X)^{-1}X^{\\top}\\boldsymbol{y}\n$$\nThis completes the derivation of the OLS estimator from first principles.\n\nNext, we compute the numerical values. First, we compute $X^{\\top}X$:\n$$\nX^{\\top}X = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & 1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} = 4I_{3}\n$$\nThe calculation shows that the columns of $X$ are orthogonal, which simplifies $X^{\\top}X$ to a diagonal matrix. The inverse is readily computed:\n$$\n(X^{\\top}X)^{-1} = (4I_{3})^{-1} = \\frac{1}{4}I_{3} = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix}\n$$\nNow, we compute $X^{\\top}\\boldsymbol{y}$:\n$$\nX^{\\top}\\boldsymbol{y} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1+3+5+7 \\\\ -1-3+5+7 \\\\ -1+3-5+7 \\end{pmatrix} = \\begin{pmatrix} 16 \\\\ 8 \\\\ 4 \\end{pmatrix}\n$$\nFinally, we compute $\\hat{\\boldsymbol{\\beta}}$:\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^{\\top}X)^{-1}X^{\\top}\\boldsymbol{y} = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 16 \\\\ 8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{4} \\\\ \\frac{8}{4} \\\\ \\frac{4}{4} \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\nSo, the estimated coefficients are $\\hat{\\beta}_0 = 4$, $\\hat{\\beta}_1 = 2$, and $\\hat{\\beta}_2 = 1$.\n\nThe last task is to verify the residual orthogonality condition, $X^{\\top}\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0}$, where $\\hat{\\boldsymbol{\\varepsilon}} = \\boldsymbol{y} - X\\hat{\\boldsymbol{\\beta}}$. First, we compute the vector of predicted values, $\\hat{\\boldsymbol{y}} = X\\hat{\\boldsymbol{\\beta}}$:\n$$\n\\hat{\\boldsymbol{y}} = X\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & 1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(4) - 1(2) - 1(1) \\\\ 1(4) - 1(2) + 1(1) \\\\ 1(4) + 1(2) - 1(1) \\\\ 1(4) + 1(2) + 1(1) \\end{pmatrix} = \\begin{pmatrix} 4-2-1 \\\\ 4-2+1 \\\\ 4+2-1 \\\\ 4+2+1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix}\n$$\nNext, we compute the vector of residuals, $\\hat{\\boldsymbol{\\varepsilon}}$:\n$$\n\\hat{\\boldsymbol{\\varepsilon}} = \\boldsymbol{y} - \\hat{\\boldsymbol{y}} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\mathbf{0}_{4 \\times 1}\n$$\nThe residual vector is the zero vector, which indicates a perfect fit of the model to the data. This is a special case that arises from the specific numerical values provided in the problem.\n\nNow we verify the orthogonality condition:\n$$\nX^{\\top}\\hat{\\boldsymbol{\\varepsilon}} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(0) + 1(0) + 1(0) + 1(0) \\\\ -1(0) - 1(0) + 1(0) + 1(0) \\\\ -1(0) + 1(0) - 1(0) + 1(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nEach component of the resulting vector $X^{\\top}\\hat{\\boldsymbol{\\varepsilon}}$ is $0$. This explicitly verifies the residual orthogonality condition, which is a direct consequence of the normal equations from which $\\hat{\\boldsymbol{\\beta}}$ was derived.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4 & 2 & 1\n\\end{pmatrix}\n}\n$$", "id": "4915381"}, {"introduction": "A common pitfall in regression analysis is misinterpreting coefficients when interaction terms are present. This practice challenges you to move beyond a simplistic view of \"main effects\" by deriving the conditional effect of a predictor, which is essential for accurately describing relationships in your model [@problem_id:4915325]. Mastering this concept is crucial for drawing valid scientific conclusions from models that include interactions.", "problem": "A biostatistics team is modeling the expected log-concentration of a circulating cytokine, denoted by $y$, as a function of two continuous exposures measured on each participant: $x_{j}$ (e.g., standardized dosage) and $x_{k}$ (e.g., standardized physical activity), with an interaction between them. They fit a multiple linear regression with an interaction term and possibly other covariates not involving $x_{j}$ or $x_{k}$. The model for a generic participant is\n$$\ny \\;=\\; \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\;+\\; \\varepsilon,\n$$\nwhere $\\varepsilon$ is a mean-zero error term independent of the predictors, and $\\mathcal{S}$ indexes additional covariates distinct from $j$ and $k$. Under the general linear model assumptions, the conditional expectation of $y$ given the predictors is the regression function obtained by removing $\\varepsilon$.\n\nUsing only (i) the definition of conditional expectation under the linear model and (ii) the definition of a partial derivative while holding other arguments fixed, derive the conditional effect of $x_{j}$ on the expected outcome at fixed $x_{k}$, that is, compute\n$$\n\\frac{\\partial\\, E\\!\\left[y \\,\\middle|\\, x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}} \\right]}{\\partial x_{j}}\n$$\nas a closed-form algebraic expression in terms of $\\beta_{j}$, $\\beta_{k}$, $\\beta_{jk}$, $x_{j}$, and $x_{k}$.\n\nYour final answer must be a single symbolic expression. Do not include units. Do not provide an inequality or an equation. Do not round.", "solution": "The objective is to compute the partial derivative of the conditional expectation of $y$ with respect to the predictor $x_{j}$. The derivation proceeds in two steps as specified by the problem statement.\n\nFirst, we determine the conditional expectation of $y$ given the full set of predictors. The model is given as:\n$$\ny \\;=\\; \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\;+\\; \\varepsilon\n$$\nThe conditional expectation of $y$ given the set of all predictors, which we denote as $\\mathbf{X} = \\{x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}}\\}$, is:\n$$\nE\\!\\left[y \\,\\middle|\\, \\mathbf{X} \\right] \\;=\\; E\\!\\left[ \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\;+\\; \\varepsilon \\,\\middle|\\, \\mathbf{X} \\right]\n$$\nBy the linearity of the expectation operator, we can write:\n$$\nE\\!\\left[y \\,\\middle|\\, \\mathbf{X} \\right] \\;=\\; E\\!\\left[ \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\,\\middle|\\, \\mathbf{X} \\right] \\;+\\; E\\!\\left[ \\varepsilon \\,\\middle|\\, \\mathbf{X} \\right]\n$$\nThe entire regression function, being composed of constants (the $\\beta$ coefficients) and the given values of the predictors, is treated as a constant with respect to the expectation. That is, for a function $g(\\mathbf{X})$, $E[g(\\mathbf{X})|\\mathbf{X}] = g(\\mathbf{X})$. The error term $\\varepsilon$ is independent of the predictors and has a mean of zero, so $E[\\varepsilon | \\mathbf{X}] = E[\\varepsilon] = 0$.\n\nSubstituting these results back, we obtain the expression for the conditional expectation of $y$:\n$$\nE\\!\\left[y \\,\\middle|\\, x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}} \\right] \\;=\\; \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell}\n$$\nSecond, we compute the partial derivative of this conditional expectation with respect to $x_j$, holding all other predictors ($x_k$ and $\\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}}$) fixed.\n$$\n\\frac{\\partial}{\\partial x_{j}} E\\!\\left[y \\,\\middle|\\, x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}} \\right] \\;=\\; \\frac{\\partial}{\\partial x_{j}} \\left( \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\right)\n$$\nWe apply the sum rule for differentiation to differentiate the expression term by term with respect to $x_j$:\n$$\n\\frac{\\partial}{\\partial x_{j}} (\\beta_{0}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} (\\beta_{j}\\,x_{j}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} (\\beta_{k}\\,x_{k}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} (\\beta_{jk}\\,x_{j}\\,x_{k}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} \\left(\\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell}\\right)\n$$\nWe evaluate each derivative:\n- The derivative of the constant term $\\beta_0$ is zero.\n- The derivative of $\\beta_{j}\\,x_{j}$ with respect to $x_j$ is $\\beta_j$.\n- The derivative of $\\beta_{k}\\,x_{k}$ with respect to $x_j$ is zero, as $x_k$ is held constant.\n- For the interaction term $\\beta_{jk}\\,x_{j}\\,x_{k}$, the derivative with respect to $x_j$ is $\\beta_{jk}\\,x_{k}$.\n- The derivative of the summation term is zero, as no term in the sum involves $x_j$.\n\nSumming these results gives the final expression for the conditional effect of $x_{j}$:\n$$\n\\frac{\\partial E[y | \\mathbf{X}]}{\\partial x_{j}} = 0 + \\beta_{j} + 0 + \\beta_{jk}\\,x_{k} + 0 = \\beta_{j} + \\beta_{jk}\\,x_{k}\n$$\nThis expression represents the change in the expected value of $y$ for a one-unit increase in $x_j$, conditional on a specific value of $x_k$ and all other covariates. It demonstrates that in the presence of an interaction term, the effect of one predictor is not constant but depends linearly on the level of the other predictor.", "answer": "$$\\boxed{\\beta_{j} + \\beta_{jk}x_{k}}$$", "id": "4915325"}, {"introduction": "Fitting a regression model is only the first step; a robust analysis requires diagnosing its integrity and identifying influential data points. This exercise introduces the concept of leverage, a fundamental diagnostic for measuring the potential influence of an observation based on its predictor values [@problem_id:4915371]. By calculating leverage values and applying a standard rule-of-thumb, you will learn a key skill for spotting unusual data points that could disproportionately affect your regression results.", "problem": "A biostatistics team studies how a biomarker responds to clinical covariates using multiple linear regression. The model includes an intercept and two predictors: a binary indicator coded as $+1$ for presence and $-1$ for absence of a condition, and a centered continuous laboratory measure. The design matrix $\\mathbf{X}$ for $n=8$ patients is given by\n$$\n\\mathbf{X} \\;=\\;\n\\begin{pmatrix}\n1 & 1 & 16 \\\\\n1 & 1 & -8 \\\\\n1 & 1 & -4 \\\\\n1 & 1 & -4 \\\\\n1 & -1 & 0 \\\\\n1 & -1 & 0 \\\\\n1 & -1 & 0 \\\\\n1 & -1 & 0\n\\end{pmatrix}.\n$$\nThe number of parameters (including the intercept) is $p=3$. Compute the leverage values $h_{ii}$ for $i=1,\\dots,8$ and determine which observations are high-leverage relative to the rule-of-thumb threshold $2p/n$. Report only the number of high-leverage observations as your final answer. No rounding is required; provide the exact count.", "solution": "The leverage value, $h_{ii}$, for the $i$-th observation is the $i$-th diagonal element of the hat matrix, $\\mathbf{H}$. The hat matrix is defined as $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$. The leverage value for observation $i$ can be computed as $h_{ii} = \\mathbf{x}_i^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_i$, where $\\mathbf{x}_i^T$ is the $i$-th row of $\\mathbf{X}$.\n\nFirst, we compute the matrix $\\mathbf{X}^T\\mathbf{X}$. The columns of the design matrix $\\mathbf{X}$ are mutually orthogonal. Therefore, $\\mathbf{X}^T\\mathbf{X}$ is a diagonal matrix where the diagonal entries are the sum of squares of the elements in each column.\nLet the columns be $\\mathbf{c}_0$, $\\mathbf{c}_1$, and $\\mathbf{c}_2$.\n$$\n\\mathbf{c}_0^T\\mathbf{c}_0 = \\sum_{i=1}^{8} 1^2 = 8\n$$\n$$\n\\mathbf{c}_1^T\\mathbf{c}_1 = 4 \\times 1^2 + 4 \\times (-1)^2 = 8\n$$\n$$\n\\mathbf{c}_2^T\\mathbf{c}_2 = 16^2 + (-8)^2 + (-4)^2 + (-4)^2 + 4 \\times 0^2 = 256 + 64 + 16 + 16 = 352\n$$\nSo, the matrix $\\mathbf{X}^T\\mathbf{X}$ is:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 8 & 0 & 0 \\\\ 0 & 8 & 0 \\\\ 0 & 0 & 352 \\end{pmatrix}\n$$\nThe inverse, $(\\mathbf{X}^T\\mathbf{X})^{-1}$, is also a diagonal matrix with the reciprocals of the diagonal elements:\n$$\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{pmatrix} \\frac{1}{8} & 0 & 0 \\\\ 0 & \\frac{1}{8} & 0 \\\\ 0 & 0 & \\frac{1}{352} \\end{pmatrix}\n$$\nNow we can compute the leverage $h_{ii} = \\mathbf{x}_i^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_i$ for each observation $i$. For a row vector $\\mathbf{x}_i^T = (x_{i0}, x_{i1}, x_{i2})$, the leverage is:\n$$\nh_{ii} = \\frac{x_{i0}^2}{8} + \\frac{x_{i1}^2}{8} + \\frac{x_{i2}^2}{352}\n$$\nSince $x_{i0}=1$ for all observations, this simplifies to:\n$$\nh_{ii} = \\frac{1}{8} + \\frac{x_{i1}^2}{8} + \\frac{x_{i2}^2}{352}\n$$\nWe compute $h_{ii}$ for the distinct rows of $\\mathbf{X}$:\n-   For $i=1$, $\\mathbf{x}_1^T = (1, 1, 16)$:\n    $h_{11} = \\frac{1}{8} + \\frac{1^2}{8} + \\frac{16^2}{352} = \\frac{1}{4} + \\frac{256}{352} = \\frac{1}{4} + \\frac{8}{11} = \\frac{11 + 32}{44} = \\frac{43}{44}$.\n-   For $i=2$, $\\mathbf{x}_2^T = (1, 1, -8)$:\n    $h_{22} = \\frac{1}{8} + \\frac{1^2}{8} + \\frac{(-8)^2}{352} = \\frac{1}{4} + \\frac{64}{352} = \\frac{1}{4} + \\frac{2}{11} = \\frac{11 + 8}{44} = \\frac{19}{44}$.\n-   For $i=3, 4$, $\\mathbf{x}_{3,4}^T = (1, 1, -4)$:\n    $h_{33} = h_{44} = \\frac{1}{8} + \\frac{1^2}{8} + \\frac{(-4)^2}{352} = \\frac{1}{4} + \\frac{16}{352} = \\frac{1}{4} + \\frac{1}{22} = \\frac{11 + 2}{44} = \\frac{13}{44}$.\n-   For $i=5, 6, 7, 8$, $\\mathbf{x}_{5-8}^T = (1, -1, 0)$:\n    $h_{55} = \\dots = h_{88} = \\frac{1}{8} + \\frac{(-1)^2}{8} + \\frac{0^2}{352} = \\frac{2}{8} = \\frac{1}{4} = \\frac{11}{44}$.\n\nThe set of leverage values is $\\{\\frac{43}{44}, \\frac{19}{44}, \\frac{13}{44}, \\frac{13}{44}, \\frac{11}{44}, \\frac{11}{44}, \\frac{11}{44}, \\frac{11}{44}\\}$.\nThe rule-of-thumb threshold for a high-leverage point is $2p/n$. With $p=3$ and $n=8$, the threshold is:\n$$\n\\frac{2p}{n} = \\frac{2 \\times 3}{8} = \\frac{6}{8} = \\frac{3}{4}\n$$\nTo compare, we express the threshold with a denominator of $44$: $\\frac{3}{4} = \\frac{33}{44}$. We identify observations with $h_{ii} > \\frac{33}{44}$:\n-   $h_{11} = \\frac{43}{44} > \\frac{33}{44}$. This is a high-leverage observation.\n-   $h_{22} = \\frac{19}{44} < \\frac{33}{44}$. This is not a high-leverage observation.\n-   $h_{33} = h_{44} = \\frac{13}{44} < \\frac{33}{44}$. These are not high-leverage observations.\n-   $h_{55}$ through $h_{88} = \\frac{11}{44} < \\frac{33}{44}$. These are not high-leverage observations.\n\nOnly one observation, $i=1$, meets the criterion for being a high-leverage point. The number of high-leverage observations is 1.", "answer": "$$\n\\boxed{1}\n$$", "id": "4915371"}]}