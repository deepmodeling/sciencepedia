{"hands_on_practices": [{"introduction": "Before we can analyze the effects of categorical variables like treatment groups or disease stages, we must first represent them numerically within a regression model. This is achieved through various \"coding schemes,\" which translate qualitative levels into a set of quantitative predictors. This practice explores how different coding schemes affect the interpretation of model coefficients while demonstrating a crucial invariant principle: the model's overall fit and predictions remain unchanged, regardless of the scheme you choose [@problem_id:4899207].", "problem": "A single-factor linear regression model with a categorical predictor having $K=3$ levels is fit by ordinary least squares (OLS) with an intercept. Consider the following dataset of $n=6$ independent observations, each belonging to exactly one of three levels $L_{1}$, $L_{2}$, $L_{3}$ of the categorical predictor:\n- Observation $1$: $y_{1}=4$, level $L_{1}$\n- Observation $2$: $y_{2}=6$, level $L_{1}$\n- Observation $3$: $y_{3}=8$, level $L_{2}$\n- Observation $4$: $y_{4}=10$, level $L_{2}$\n- Observation $5$: $y_{5}=5$, level $L_{3}$\n- Observation $6$: $y_{6}=3$, level $L_{3}$\n\nStart from the core definitions of the linear model, least squares projection, and the coefficient of determination. Use three different full-rank coding schemes for the factor (always including an intercept) and fit the model $y_{i}=\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}+\\varepsilon_{i}$ by OLS under each scheme:\n1. Treatment (dummy) coding with $L_{1}$ as the reference level. Use regressors $\\{1,D_{2},D_{3}\\}$ where $D_{k}=1$ if the observation is in level $L_{k}$ and $0$ otherwise.\n2. Effect (sum-to-zero) coding with regressors $\\{1,E_{1},E_{2}\\}$ defined by the per-level codes\n   - $L_{1}$: $(E_{1},E_{2})=(-1,-1)$,\n   - $L_{2}$: $(E_{1},E_{2})=(1,0)$,\n   - $L_{3}$: $(E_{1},E_{2})=(0,1)$.\n3. Helmert coding with regressors $\\{1,H_{1},H_{2}\\}$ defined by the per-level codes\n   - $L_{1}$: $(H_{1},H_{2})=(-1,-1)$,\n   - $L_{2}$: $(H_{1},H_{2})=(1,-1)$,\n   - $L_{3}$: $(H_{1},H_{2})=(0,2)$.\n\nFor each coding:\n- Compute the OLS coefficient vector $\\boldsymbol{\\beta}$.\n- Compute the fitted value $\\hat{y}_{i}$ for every observation $i\\in\\{1,\\dots,6\\}$.\n- Compute the coefficient of determination (R-squared), defined as $R^{2}=1-\\mathrm{SSE}/\\mathrm{SST}$, where $\\mathrm{SSE}=\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$ and $\\mathrm{SST}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$ with $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$.\n\nUsing first principles, justify why the fitted values and $R^{2}$ are invariant across these full-rank codings, even though the individual coefficients differ in interpretation and numerical value.\n\nReport as your final answer the common numerical value of $R^{2}$ for this dataset. Round your answer to four significant figures.", "solution": "The user wants me to solve a problem.\nMy analysis of the user's request is as follows:\n1.  **Extract Givens**:\n    -   Model: Single-factor linear regression, $y_{i}=\\boldsymbol{x}_{i}^{\\top}\\boldsymbol{\\beta}+\\varepsilon_{i}$, fit by OLS with an intercept.\n    -   Categorical predictor with $K=3$ levels: $L_{1}$, $L_{2}$, $L_{3}$.\n    -   Dataset ($n=6$ observations):\n        -   $y_{1}=4$, level $L_{1}$\n        -   $y_{2}=6$, level $L_{1}$\n        -   $y_{3}=8$, level $L_{2}$\n        -   $y_{4}=10$, level $L_{2}$\n        -   $y_{5}=5$, level $L_{3}$\n        -   $y_{6}=3$, level $L_{3}$\n    -   Coding Scheme 1 (Treatment/Dummy): Reference level $L_{1}$. Regressors $\\{1,D_{2},D_{3}\\}$ where $D_{k}=1$ if in level $L_{k}$, $0$ otherwise.\n    -   Coding Scheme 2 (Effect/Sum-to-zero): Regressors $\\{1,E_{1},E_{2}\\}$ with per-level codes: $L_{1}: (-1,-1)$, $L_{2}: (1,0)$, $L_{3}: (0,1)$.\n    -   Coding Scheme 3 (Helmert): Regressors $\\{1,H_{1},H_{2}\\}$ with per-level codes: $L_{1}: (-1,-1)$, $L_{2}: (1,-1)$, $L_{3}: (0,2)$.\n    -   Tasks: For each coding scheme, compute $\\boldsymbol{\\beta}$, $\\hat{y}_{i}$ for all $i$, and $R^{2}$.\n    -   Definition of $R^2$: $R^{2}=1-\\mathrm{SSE}/\\mathrm{SST}$, where $\\mathrm{SSE}=\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$ and $\\mathrm{SST}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}$.\n    -   Justification: Explain from first principles why $\\hat{y}_{i}$ and $R^{2}$ are invariant.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientifically Grounded**: The problem is a standard exercise in linear regression analysis, a fundamental topic in statistics and biostatistics. It is based on established principles of OLS.\n    -   **Well-Posed**: All data, model forms, and definitions required for the solution are provided. The coding schemes specified are full-rank, ensuring a unique OLS solution exists for each.\n    -   **Objective**: The problem is stated using precise, standard mathematical and statistical terminology, free of subjectivity.\n    -   The problem does not violate any of the invalidity criteria. It is complete, consistent, realistic, and well-structured.\n\n3.  **Verdict and Action**:\n    -   The problem is **valid**. A full solution will be provided.\n\nThe problem asks for an analysis of a single-factor linear model under three different parameterizations. We will first provide the theoretical justification for the invariance of fitted values and $R^2$, then perform the calculations for the specific dataset.\n\n**Theoretical Justification for Invariance**\n\nThe general linear model is expressed in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of responses, $\\mathbf{X}$ is the $n \\times p$ design matrix, $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the vector of errors. The Ordinary Least Squares (OLS) estimator for $\\boldsymbol{\\beta}$ is given by $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$, assuming $\\mathbf{X}$ has full column rank.\n\nThe vector of fitted values, $\\hat{\\mathbf{y}}$, is given by:\n$$ \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y} $$\nThis can be written as $\\hat{\\mathbf{y}} = \\mathbf{P}\\mathbf{y}$, where the matrix $\\mathbf{P} = \\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}$ is the projection matrix. Geometrically, $\\hat{\\mathbf{y}}$ is the orthogonal projection of the data vector $\\mathbf{y}$ onto the column space of the design matrix, $C(\\mathbf{X})$. The projection matrix $\\mathbf{P}$ is uniquely determined by the subspace $C(\\mathbf{X})$, not by the specific choice of basis vectors (columns) for that subspace.\n\nIn this problem, we have a single categorical factor with $K=3$ levels. Any full-rank parameterization with an intercept will require $p=3$ columns in the design matrix $\\mathbf{X}$. Let $\\mathbf{X}_A$ and $\\mathbf{X}_B$ be two such design matrices corresponding to different coding schemes (e.g., treatment and effect coding). Both matrices are constructed to span the same subspace: the subspace of vectors that are constant within each of the $K$ levels. This subspace has dimension $K=3$. Because both $\\mathbf{X}_A$ and $\\mathbf{X}_B$ are $n \\times K$ matrices of rank $K$, their columns form a basis for this same $K$-dimensional subspace. This means that their column spaces are identical: $C(\\mathbf{X}_A) = C(\\mathbf{X}_B)$.\n\nSince the column spaces are identical, the projection matrix $\\mathbf{P}$ onto this space is also identical for both codings. Consequently, the vector of fitted values, $\\hat{\\mathbf{y}} = \\mathbf{P}\\mathbf{y}$, must be invariant to the choice of a full-rank coding scheme.\n\nThis invariance directly extends to the Sum of Squared Errors ($\\mathrm{SSE}$) and the coefficient of determination ($R^2$):\n- $\\mathrm{SSE} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = ||\\mathbf{y} - \\hat{\\mathbf{y}}||^2$. Since $\\mathbf{y}$ is fixed and $\\hat{\\mathbf{y}}$ is invariant, $\\mathrm{SSE}$ is invariant.\n- The Total Sum of Squares, $\\mathrm{SST} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$, depends only on the response data $\\mathbf{y}$ and is therefore constant regardless of the model.\n- The coefficient of determination, $R^2 = 1 - \\mathrm{SSE}/\\mathrm{SST}$, must therefore also be invariant across these different coding schemes.\n\nFor any one-way ANOVA model, the OLS projection yields fitted values equal to the group means. Let's find these first as they will be the common set of fitted values.\nThe data are $y = (4, 6, 8, 10, 5, 3)$. The levels are $(L_1, L_1, L_2, L_2, L_3, L_3)$.\n- Mean of level $L_1$: $\\bar{y}_1 = \\frac{4+6}{2} = 5$.\n- Mean of level $L_2$: $\\bar{y}_2 = \\frac{8+10}{2} = 9$.\n- Mean of level $L_3$: $\\bar{y}_3 = \\frac{5+3}{2} = 4$.\nThe vector of fitted values for all schemes will be $\\hat{\\mathbf{y}} = (5, 5, 9, 9, 4, 4)^{\\top}$.\n\n**Computation of $R^2$**\n\nFirst, we compute the overall mean $\\bar{y}$:\n$$ \\bar{y} = \\frac{1}{6}(4+6+8+10+5+3) = \\frac{36}{6} = 6 $$\nNext, we compute the Total Sum of Squares ($\\mathrm{SST}$):\n$$ \\mathrm{SST} = \\sum_{i=1}^{6}(y_i - \\bar{y})^2 = (4-6)^2 + (6-6)^2 + (8-6)^2 + (10-6)^2 + (5-6)^2 + (3-6)^2 $$\n$$ \\mathrm{SST} = (-2)^2 + 0^2 + 2^2 + 4^2 + (-1)^2 + (-3)^2 = 4 + 0 + 4 + 16 + 1 + 9 = 34 $$\nUsing the invariant fitted values $\\hat{\\mathbf{y}} = (5, 5, 9, 9, 4, 4)^{\\top}$, we compute the Sum of Squared Errors ($\\mathrm{SSE}$):\n$$ \\mathrm{SSE} = \\sum_{i=1}^{6}(y_i - \\hat{y}_i)^2 = (4-5)^2 + (6-5)^2 + (8-9)^2 + (10-9)^2 + (5-4)^2 + (3-4)^2 $$\n$$ \\mathrm{SSE} = (-1)^2 + 1^2 + (-1)^2 + 1^2 + 1^2 + (-1)^2 = 1+1+1+1+1+1 = 6 $$\nFinally, we compute the coefficient of determination $R^2$:\n$$ R^2 = 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}} = 1 - \\frac{6}{34} = 1 - \\frac{3}{17} = \\frac{14}{17} $$\n\nNow, we demonstrate the calculations for each coding scheme as required. The response vector is $\\mathbf{y} = (4, 6, 8, 10, 5, 3)^{\\top}$.\n\n**1. Treatment (Dummy) Coding with $L_1$ as Reference**\n\nThe model is $y_i = \\beta_0 + \\beta_1 D_{i2} + \\beta_2 D_{i3} + \\varepsilon_i$. The design matrix $\\mathbf{X}_D$ is:\n$$ \\mathbf{X}_D = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\nWe compute $\\mathbf{X}_D^{\\top}\\mathbf{X}_D$ and $\\mathbf{X}_D^{\\top}\\mathbf{y}$:\n$$ \\mathbf{X}_D^{\\top}\\mathbf{X}_D = \\begin{pmatrix} 6 & 2 & 2 \\\\ 2 & 2 & 0 \\\\ 2 & 0 & 2 \\end{pmatrix}, \\quad \\mathbf{X}_D^{\\top}\\mathbf{y} = \\begin{pmatrix} 36 \\\\ 18 \\\\ 8 \\end{pmatrix} $$\nThe inverse of $\\mathbf{X}_D^{\\top}\\mathbf{X}_D$ is:\n$$ (\\mathbf{X}_D^{\\top}\\mathbf{X}_D)^{-1} = \\frac{1}{8}\\begin{pmatrix} 4 & -4 & -4 \\\\ -4 & 8 & 4 \\\\ -4 & 4 & 8 \\end{pmatrix} = \\begin{pmatrix} 1/2 & -1/2 & -1/2 \\\\ -1/2 & 1 & 1/2 \\\\ -1/2 & 1/2 & 1 \\end{pmatrix} $$\nThe OLS coefficients are:\n$$ \\hat{\\boldsymbol{\\beta}}_D = (\\mathbf{X}_D^{\\top}\\mathbf{X}_D)^{-1}\\mathbf{X}_D^{\\top}\\mathbf{y} = \\begin{pmatrix} 1/2 & -1/2 & -1/2 \\\\ -1/2 & 1 & 1/2 \\\\ -1/2 & 1/2 & 1 \\end{pmatrix} \\begin{pmatrix} 36 \\\\ 18 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 18 - 9 - 4 \\\\ -18 + 18 + 4 \\\\ -18 + 9 + 8 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 4 \\\\ -1 \\end{pmatrix} $$\nThe fitted values are $\\hat{\\mathbf{y}}_D = \\mathbf{X}_D\\hat{\\boldsymbol{\\beta}}_D$:\n$$ \\hat{\\mathbf{y}}_D = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 5+4 \\\\ 5+4 \\\\ 5-1 \\\\ 5-1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 9 \\\\ 9 \\\\ 4 \\\\ 4 \\end{pmatrix} $$\nThese are indeed the group means.\n\n**2. Effect (Sum-to-Zero) Coding**\n\nThe model is $y_i = \\beta_0 + \\beta_1 E_{i1} + \\beta_2 E_{i2} + \\varepsilon_i$. The design matrix $\\mathbf{X}_E$ is:\n$$ \\mathbf{X}_E = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\nWe compute $\\mathbf{X}_E^{\\top}\\mathbf{X}_E$ and $\\mathbf{X}_E^{\\top}\\mathbf{y}$:\n$$ \\mathbf{X}_E^{\\top}\\mathbf{X}_E = \\begin{pmatrix} 6 & 0 & 0 \\\\ 0 & 4 & 2 \\\\ 0 & 2 & 4 \\end{pmatrix}, \\quad \\mathbf{X}_E^{\\top}\\mathbf{y} = \\begin{pmatrix} 36 \\\\ 8 \\\\ -2 \\end{pmatrix} $$\nThe inverse is $(\\mathbf{X}_E^{\\top}\\mathbf{X}_E)^{-1} = \\frac{1}{72}\\begin{pmatrix} 12 & 0 & 0 \\\\ 0 & 24 & -12 \\\\ 0 & -12 & 24 \\end{pmatrix} = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/3 & -1/6 \\\\ 0 & -1/6 & 1/3 \\end{pmatrix}$.\nThe OLS coefficients are:\n$$ \\hat{\\boldsymbol{\\beta}}_E = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/3 & -1/6 \\\\ 0 & -1/6 & 1/3 \\end{pmatrix} \\begin{pmatrix} 36 \\\\ 8 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 8/3 + 2/6 \\\\ -8/6 - 2/3 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 3 \\\\ -2 \\end{pmatrix} $$\nThe fitted values are $\\hat{\\mathbf{y}}_E = \\mathbf{X}_E\\hat{\\boldsymbol{\\beta}}_E$:\n$$ \\hat{\\mathbf{y}}_E = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 6 - 3 + 2 \\\\ 6 - 3 + 2 \\\\ 6 + 3 + 0 \\\\ 6 + 3 + 0 \\\\ 6 + 0 - 2 \\\\ 6 + 0 - 2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 9 \\\\ 9 \\\\ 4 \\\\ 4 \\end{pmatrix} $$\nThe fitted values are identical to the previous case.\n\n**3. Helmert Coding**\n\nThe model is $y_i = \\beta_0 + \\beta_1 H_{i1} + \\beta_2 H_{i2} + \\varepsilon_i$. The design matrix is $\\mathbf{X}_H$:\n$$ \\mathbf{X}_H = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 0 & 2 \\\\ 1 & 0 & 2 \\end{pmatrix} $$\nThe columns corresponding to the contrasts $H_1$ and $H_2$ are orthogonal to each other and to the intercept column. Thus, $\\mathbf{X}_H^{\\top}\\mathbf{X}_H$ is a diagonal matrix.\n$$ \\mathbf{X}_H^{\\top}\\mathbf{X}_H = \\begin{pmatrix} 6 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 12 \\end{pmatrix}, \\quad \\mathbf{X}_H^{\\top}\\mathbf{y} = \\begin{pmatrix} 36 \\\\ 8 \\\\ -12 \\end{pmatrix} $$\nThe inverse is trivial: $(\\mathbf{X}_H^{\\top}\\mathbf{X}_H)^{-1} = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/12 \\end{pmatrix}$.\nThe OLS coefficients are:\n$$ \\hat{\\boldsymbol{\\beta}}_H = \\begin{pmatrix} 1/6 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 1/12 \\end{pmatrix} \\begin{pmatrix} 36 \\\\ 8 \\\\ -12 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\\\ -1 \\end{pmatrix} $$\nThe fitted values are $\\hat{\\mathbf{y}}_H = \\mathbf{X}_H\\hat{\\boldsymbol{\\beta}}_H$:\n$$ \\hat{\\mathbf{y}}_H = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & -1 \\\\ 1 & 0 & 2 \\\\ 1 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 6-2+1 \\\\ 6-2+1 \\\\ 6+2+1 \\\\ 6+2+1 \\\\ 6-2 \\\\ 6-2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\\\ 9 \\\\ 9 \\\\ 4 \\\\ 4 \\end{pmatrix} $$\nThe fitted values are again identical.\n\nAs demonstrated, while the coefficient vectors $\\hat{\\boldsymbol{\\beta}}_D$, $\\hat{\\boldsymbol{\\beta}}_E$, and $\\hat{\\boldsymbol{\\beta}}_H$ are numerically different and have different interpretations, they all produce the same vector of fitted values $\\hat{\\mathbf{y}}$. This confirms the theoretical principle that the projection of $\\mathbf{y}$ onto the model's column space is invariant to the choice of basis for that space. As $\\hat{\\mathbf{y}}$ is invariant, so are $\\mathrm{SSE}$ and $R^2$.\n\nThe final numerical value for $R^2$ is $\\frac{14}{17} \\approx 0.8235294...$. Rounded to four significant figures, this is $0.8235$.", "answer": "$$ \\boxed{0.8235} $$", "id": "4899207"}, {"introduction": "In biostatistical analysis, it's rarely enough to know the effect of a single factor; we must also understand how different exposures interact and how other variables might confound our results. This exercise moves from simple representation to the critical concepts of interaction and confounding. You will learn to distinguish interaction on an additive scale (risk difference) from that on a multiplicative scale (risk ratio) and use standardization to calculate an adjusted measure of interaction, the Relative Excess Risk due to Interaction (RERI) [@problem_id:4899240].", "problem": "A prospective cohort study investigates the joint effects of two binary exposures, $A \\in \\{0,1\\}$ and $B \\in \\{0,1\\}$, on the $1$-year risk of a disease outcome $Y \\in \\{0,1\\}$. Age, denoted $C \\in \\{0,1\\}$, is a potential confounder, with $C=0$ indicating younger participants and $C=1$ indicating older participants. The target population for inference has $P(C=0)=0.7$ and $P(C=1)=0.3$. Within strata of $C$, the observed risks for $(A,B)$ are as follows (risks are conditional on $C$ and on $(A,B)$):\n- For $C=0$: $R_{00}^{(0)}=0.02$, $R_{10}^{(0)}=0.03$, $R_{01}^{(0)}=0.04$, $R_{11}^{(0)}=0.08$.\n- For $C=1$: $R_{00}^{(1)}=0.08$, $R_{10}^{(1)}=0.12$, $R_{01}^{(1)}=0.16$, $R_{11}^{(1)}=0.30$.\n\nHere, $R_{ab}^{(c)}$ denotes $P(Y=1 \\mid A=a, B=b, C=c)$, and the category $(A,B)=(0,0)$ is the reference. \n\nTask:\n1. In words, state what it means for $A$ and $B$ to exhibit no interaction on the additive scale in terms of risks and risk differences, using $(A,B)=(0,0)$ as the reference.\n2. Starting from this definition, derive an estimand that quantifies departure from additivity on the risk difference scale but is expressed using risk ratios relative to $(A,B)=(0,0)$. Name this estimand as the relative excess risk due to interaction (RERI).\n3. Adjusting for confounding by $C$ via standardization to the target population distribution of $C$, compute the numerical value of the relative excess risk due to interaction (RERI) for $A$ and $B$.\n\nRound your final numerical answer to four significant figures and express it as a pure number without units.", "solution": "The problem is valid as it is scientifically grounded in biostatistical principles, well-posed with sufficient information for a unique solution, and objectively framed. We will proceed by addressing the three tasks in sequence.\n\nLet $R_{ab}$ denote the risk of the disease, $P(Y=1)$, for individuals with exposure status $A=a$ and $B=b$. The risk difference (RD) associated with an exposure profile $(a,b)$ relative to the unexposed reference group $(0,0)$ is $RD_{ab} = R_{ab} - R_{00}$.\n\nTask 1: Definition of no additive interaction.\nNo interaction on the additive scale means that the excess risk from the joint exposure to both $A$ and $B$ is equal to the sum of the excess risks from each exposure considered individually. The excess risk for exposure $A$ alone, relative to the unexposed group, is the risk difference $RD_{10} = R_{10} - R_{00}$. Similarly, the excess risk for exposure $B$ alone is $RD_{01} = R_{01} - R_{00}$. If the effects are purely additive, the predicted excess risk for the joint exposure would be the sum of these individual excess risks, $RD_{10} + RD_{01}$. The observed excess risk for the joint exposure is $RD_{11} = R_{11} - R_{00}$. Therefore, the condition for no additive interaction is that the observed joint excess risk equals the sum of the individual excess risks:\n$$\nRD_{11} = RD_{10} + RD_{01}\n$$\nSubstituting the definitions of the risk differences, this condition becomes:\n$$\n(R_{11} - R_{00}) = (R_{10} - R_{00}) + (R_{01} - R_{00})\n$$\n\nTask 2: Derivation of the Relative Excess Risk due to Interaction (RERI).\nThe departure from additivity, or the interaction contrast on an additive scale ($I_{add}$), is the difference between the observed joint excess risk and the sum of the individual excess risks:\n$$\nI_{add} = RD_{11} - (RD_{10} + RD_{01})\n$$\nSubstituting the risks, we have:\n$$\nI_{add} = (R_{11} - R_{00}) - (R_{10} - R_{00}) - (R_{01} - R_{00}) = R_{11} - R_{10} - R_{01} + R_{00}\n$$\nA value of $I_{add}=0$ signifies no additive interaction. To express this departure in relative terms using risk ratios (RR), we first define the risk ratios relative to the baseline risk $R_{00}$:\n$$\nRR_{10} = \\frac{R_{10}}{R_{00}}, \\quad RR_{01} = \\frac{R_{01}}{R_{00}}, \\quad RR_{11} = \\frac{R_{11}}{R_{00}}\n$$\nThe problem asks for an estimand that quantifies the departure from additivity using these risk ratios. A natural way to create a relative measure is to scale the interaction contrast $I_{add}$ by the baseline risk $R_{00}$. Dividing the expression for $I_{add}$ by $R_{00}$ (assuming $R_{00} > 0$) yields the estimand known as the Relative Excess Risk due to Interaction (RERI):\n$$\n\\text{RERI} = \\frac{I_{add}}{R_{00}} = \\frac{R_{11} - R_{10} - R_{01} + R_{00}}{R_{00}}\n$$\nSeparating the terms gives:\n$$\n\\text{RERI} = \\frac{R_{11}}{R_{00}} - \\frac{R_{10}}{R_{00}} - \\frac{R_{01}}{R_{00}} + \\frac{R_{00}}{R_{00}}\n$$\nSubstituting the definitions of the risk ratios, we arrive at the desired expression:\n$$\n\\text{RERI} = RR_{11} - RR_{10} - RR_{01} + 1\n$$\nThis estimand can also be interpreted as the excess risk ratio for the joint exposure ($ERR_{11} = RR_{11} - 1$) minus the sum of the excess risk ratios for the individual exposures ($ERR_{10} = RR_{10} - 1$ and $ERR_{01} = RR_{01} - 1$):\n$$\n\\text{RERI} = (RR_{11} - 1) - [(RR_{10} - 1) + (RR_{01} - 1)] = RR_{11} - RR_{10} - RR_{01} + 1\n$$\n\nTask 3: Calculation of the standardized RERI.\nTo adjust for confounding by $C$, we first calculate the standardized risks for each exposure group $(A=a, B=b)$, denoted $R_{ab}^{S}$. Standardization is performed by taking a weighted average of the stratum-specific risks $R_{ab}^{(c)}$, where the weights are the proportions of the confounder $C$ in the target population, $P(C=c)$.\nGiven $P(C=0)=0.7$ and $P(C=1)=0.3$. The formula for a standardized risk is:\n$$\nR_{ab}^{S} = R_{ab}^{(0)} P(C=0) + R_{ab}^{(1)} P(C=1)\n$$\nWe compute the four required standardized risks:\n- For $(A,B)=(0,0)$:\n$$\nR_{00}^{S} = (0.02)(0.7) + (0.08)(0.3) = 0.014 + 0.024 = 0.038\n$$\n- For $(A,B)=(1,0)$:\n$$\nR_{10}^{S} = (0.03)(0.7) + (0.12)(0.3) = 0.021 + 0.036 = 0.057\n$$\n- For $(A,B)=(0,1)$:\n$$\nR_{01}^{S} = (0.04)(0.7) + (0.16)(0.3) = 0.028 + 0.048 = 0.076\n$$\n- For $(A,B)=(1,1)$:\n$$\nR_{11}^{S} = (0.08)(0.7) + (0.30)(0.3) = 0.056 + 0.090 = 0.146\n$$\nNext, we calculate the standardized risk ratios ($RR^S$) using the standardized risks, with $R_{00}^{S}$ as the reference:\n$$\nRR_{10}^{S} = \\frac{R_{10}^{S}}{R_{00}^{S}} = \\frac{0.057}{0.038} = 1.5\n$$\n$$\nRR_{01}^{S} = \\frac{R_{01}^{S}}{R_{00}^{S}} = \\frac{0.076}{0.038} = 2.0\n$$\n$$\nRR_{11}^{S} = \\frac{R_{11}^{S}}{R_{00}^{S}} = \\frac{0.146}{0.038} = \\frac{146}{38} = \\frac{73}{19}\n$$\nFinally, we compute the standardized RERI using the formula derived in Task 2:\n$$\n\\text{RERI} = RR_{11}^{S} - RR_{10}^{S} - RR_{01}^{S} + 1\n$$\n$$\n\\text{RERI} = \\frac{73}{19} - 1.5 - 2.0 + 1 = \\frac{73}{19} - 2.5\n$$\nTo calculate the numerical value:\n$$\n\\text{RERI} = \\frac{73}{19} - \\frac{5}{2} = \\frac{2 \\times 73 - 5 \\times 19}{38} = \\frac{146 - 95}{38} = \\frac{51}{38}\n$$\nAs a decimal, this is approximately $1.342105...$. Rounding to four significant figures gives $1.342$.", "answer": "$$\\boxed{1.342}$$", "id": "4899240"}, {"introduction": "Observing an apparent interaction is one thing; formally testing its statistical significance is another, especially when a categorical variable has multiple levels. This requires a joint hypothesis test that evaluates several model coefficients at once. This final practice introduces the multivariate Wald test, a cornerstone of statistical inference, to formally assess evidence for effect modification in a logistic regression model [@problem_id:4899236].", "problem": "A randomized epidemiologic study investigates whether the association between a binary treatment indicator $A \\in \\{0,1\\}$ and a binary clinical outcome $Y \\in \\{0,1\\}$ varies across levels of a categorical biomarker $B$ having $J=4$ levels $\\{B_{1},B_{2},B_{3},B_{4}\\}$, while adjusting for a continuous confounder $C$. A logistic Generalized Linear Model (GLM) with logit link is fit using reference cell coding for $B$ with $B_{1}$ as the reference and including first-order interactions between $A$ and $B$. The model is\n$$\n\\log\\left(\\frac{\\Pr(Y=1 \\mid A,B,C)}{1-\\Pr(Y=1 \\mid A,B,C)}\\right)\n=\n\\beta_{0}+\\beta_{A}A+\\beta_{B_{2}}\\mathbf{1}(B=B_{2})+\\beta_{B_{3}}\\mathbf{1}(B=B_{3})+\\beta_{B_{4}}\\mathbf{1}(B=B_{4})\n+\\beta_{AB_{2}}\\,A\\,\\mathbf{1}(B=B_{2})+\\beta_{AB_{3}}\\,A\\,\\mathbf{1}(B=B_{3})+\\beta_{AB_{4}}\\,A\\,\\mathbf{1}(B=B_{4})\n+\\beta_{C}C.\n$$\nLet the regression coefficient vector be ordered as\n$$\n\\boldsymbol{\\beta}=\n\\begin{pmatrix}\n\\beta_{0} & \\beta_{A} & \\beta_{B_{2}} & \\beta_{B_{3}} & \\beta_{B_{4}} & \\beta_{AB_{2}} & \\beta_{AB_{3}} & \\beta_{AB_{4}} & \\beta_{C}\n\\end{pmatrix}^{\\top}.\n$$\nYou are interested in testing whether there is any interaction between $A$ and $B$ on the log-odds scale. Formulate the null hypothesis that all interaction terms are zero. Then, using the Wald test based on Maximum Likelihood Estimation (MLE), construct the appropriate contrast matrix that tests the interaction terms jointly with degrees of freedom equal to $J-1$.\n\nFrom the fitted model, the estimated interaction coefficients and their estimated covariance matrix are:\n$$\n\\widehat{\\boldsymbol{\\beta}}_{AB}=\n\\begin{pmatrix}\n\\widehat{\\beta}_{AB_{2}} \\\\\n\\widehat{\\beta}_{AB_{3}} \\\\\n\\widehat{\\beta}_{AB_{4}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.42 \\\\\n-0.18 \\\\\n0.30\n\\end{pmatrix},\n\\qquad\n\\widehat{\\boldsymbol{\\Sigma}}_{AB}=\n\\begin{pmatrix}\n0.040 & 0.005 & -0.004 \\\\\n0.005 & 0.050 & 0.006 \\\\\n-0.004 & 0.006 & 0.045\n\\end{pmatrix}.\n$$\nUse the large-sample Wald test to compute the test statistic for the interaction, taking the contrast to target these three interaction coefficients. Explicitly specify the contrast matrix relative to the ordering of $\\boldsymbol{\\beta}$ above, and then compute the Wald test statistic\n$$\nW=(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}})^{\\top}\\left[\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}\\right]^{-1}(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}),\n$$\nnoting that $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$ is the submatrix $\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}$ for the interaction terms. State the degrees of freedom and report the value of $W$.\n\nRound your final numerical answer for $W$ to four significant figures. The final answer you provide must be the Wald statistic $W$ only, with no units.", "solution": "The user wants me to solve a biostatistics problem involving a logistic regression model.\n\n### Step 1: Extract Givens\n-   **Model:** A logistic GLM with a logit link is given by:\n    $$\n    \\log\\left(\\frac{\\Pr(Y=1 \\mid A,B,C)}{1-\\Pr(Y=1 \\mid A,B,C)}\\right)\n    =\n    \\beta_{0}+\\beta_{A}A+\\beta_{B_{2}}\\mathbf{1}(B=B_{2})+\\beta_{B_{3}}\\mathbf{1}(B=B_{3})+\\beta_{B_{4}}\\mathbf{1}(B=B_{4})\n    +\\beta_{AB_{2}}\\,A\\,\\mathbf{1}(B=B_{2})+\\beta_{AB_{3}}\\,A\\,\\mathbf{1}(B=B_{3})+\\beta_{AB_{4}}\\,A\\,\\mathbf{1}(B=B_{4})\n    +\\beta_{C}C.\n    $$\n-   **Variables and Parameters:**\n    -   $Y$: binary clinical outcome, $Y \\in \\{0,1\\}$.\n    -   $A$: binary treatment indicator, $A \\in \\{0,1\\}$.\n    -   $B$: categorical biomarker with $J=4$ levels, $\\{B_{1}, B_{2}, B_{3}, B_{4}\\}$. $B_{1}$ is the reference level.\n    -   $C$: continuous confounder.\n    -   $\\mathbf{1}(\\cdot)$: indicator function.\n-   **Coefficient Vector:** The ordered coefficient vector is:\n    $$\n    \\boldsymbol{\\beta}=\n    \\begin{pmatrix}\n    \\beta_{0} & \\beta_{A} & \\beta_{B_{2}} & \\beta_{B_{3}} & \\beta_{B_{4}} & \\beta_{AB_{2}} & \\beta_{AB_{3}} & \\beta_{AB_{4}} & \\beta_{C}\n    \\end{pmatrix}^{\\top}.\n    $$\n-   **Estimated Interaction Coefficients and Covariance Matrix:**\n    $$\n    \\widehat{\\boldsymbol{\\beta}}_{AB}=\n    \\begin{pmatrix}\n    \\widehat{\\beta}_{AB_{2}} \\\\\n    \\widehat{\\beta}_{AB_{3}} \\\\\n    \\widehat{\\beta}_{AB_{4}}\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    0.42 \\\\\n    -0.18 \\\\\n    0.30\n    \\end{pmatrix},\n    \\qquad\n    \\widehat{\\boldsymbol{\\Sigma}}_{AB}=\n    \\begin{pmatrix}\n    0.040 & 0.005 & -0.004 \\\\\n    0.005 & 0.050 & 0.006 \\\\\n    -0.004 & 0.006 & 0.045\n    \\end{pmatrix}.\n    $$\n-   **Task:** Formulate the null hypothesis for no interaction between $A$ and $B$. Construct the contrast matrix $\\mathbf{C}$ for a Wald test. Compute the Wald test statistic $W$ and state its degrees of freedom.\n-   **Wald Test Statistic Formula:**\n    $$\n    W=(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}})^{\\top}\\left[\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}\\right]^{-1}(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}).\n    $$\n-   **Rounding:** The final numerical answer for $W$ must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is a standard application of generalized linear models (logistic regression) in biostatistics. The model specification, use of reference cell coding for a categorical predictor, inclusion of interaction terms, and the application of a multivariate Wald test are all well-established and fundamental statistical concepts. The problem is scientifically and mathematically sound.\n-   **Well-Posed:** All necessary information is provided. A clear question is asked (compute the Wald statistic for interaction), and the required data (estimated coefficients and their covariance matrix) are supplied. The model structure is explicit. A unique, stable, and meaningful solution exists.\n-   **Objective:** The problem is posed using precise, standard statistical terminology. There are no subjective or ambiguous statements.\n-   The problem does not exhibit any of the flaws listed in the instructions (e.g., non-formalizable, incomplete, unrealistic). The given covariance matrix is symmetric with positive diagonal elements, as required for a valid covariance matrix.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\nThe problem asks for a joint test of the interaction between the treatment $A$ and the categorical biomarker $B$. The specified logistic regression model includes terms for this interaction.\n\nFirst, we formulate the null hypothesis. The interaction between $A$ and $B$ is captured by the coefficients $\\beta_{AB_{j}}$ for $j=2, 3, 4$. The interaction term for the reference level, $\\beta_{AB_{1}}$, is structurally zero due to the reference cell coding scheme used. Therefore, the null hypothesis of no interaction is that all non-redundant interaction coefficients are simultaneously zero.\n$$ H_{0}: \\beta_{AB_{2}} = 0, \\beta_{AB_{3}} = 0, \\beta_{AB_{4}} = 0. $$\nThis can be written in matrix form as $\\mathbf{C}\\boldsymbol{\\beta} = \\mathbf{0}$, where $\\mathbf{C}$ is a contrast matrix and $\\boldsymbol{\\beta}$ is the vector of all model coefficients.\n\nThe full coefficient vector $\\boldsymbol{\\beta}$ is a $9 \\times 1$ vector ordered as:\n$$ \\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_{0} & \\beta_{A} & \\beta_{B_{2}} & \\beta_{B_{3}} & \\beta_{B_{4}} & \\beta_{AB_{2}} & \\beta_{AB_{3}} & \\beta_{AB_{4}} & \\beta_{C} \\end{pmatrix}^{\\top} $$\nThe coefficients of interest, $\\beta_{AB_{2}}$, $\\beta_{AB_{3}}$, and $\\beta_{AB_{4}}$, are the 6th, 7th, and 8th elements of this vector. To test the null hypothesis that these three coefficients are zero, we construct a $3 \\times 9$ contrast matrix $\\mathbf{C}$ where each row selects one of these coefficients.\n$$ \\mathbf{C} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\end{pmatrix} $$\nThe degrees of freedom for this test are equal to the number of rows in $\\mathbf{C}$, which is the number of parameters being jointly tested. Here, the biomarker $B$ has $J=4$ levels, so there are $J-1=3$ interaction terms. The degrees of freedom are $3$.\n\nThe large-sample Wald test statistic is given by:\n$$ W = (\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}})^{\\top}\\left[\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}\\right]^{-1}(\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}) $$\nBased on our definition of $\\mathbf{C}$:\n- The term $\\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}}$ extracts the estimated interaction coefficients:\n$$ \\mathbf{C}\\,\\widehat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\widehat{\\beta}_{AB_{2}} \\\\ \\widehat{\\beta}_{AB_{3}} \\\\ \\widehat{\\beta}_{AB_{4}} \\end{pmatrix} = \\widehat{\\boldsymbol{\\beta}}_{AB} = \\begin{pmatrix} 0.42 \\\\ -0.18 \\\\ 0.30 \\end{pmatrix} $$\n- The term $\\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top}$ extracts the submatrix of the full covariance matrix corresponding to these interaction coefficients. This submatrix is provided in the problem as $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$:\n$$ \\mathbf{C}\\,\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\,\\mathbf{C}^{\\top} = \\widehat{\\boldsymbol{\\Sigma}}_{AB} = \\begin{pmatrix} 0.040 & 0.005 & -0.004 \\\\ 0.005 & 0.050 & 0.006 \\\\ -0.004 & 0.006 & 0.045 \\end{pmatrix} $$\nThus, the Wald statistic calculation simplifies to:\n$$ W = \\widehat{\\boldsymbol{\\beta}}_{AB}^{\\top} \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} \\widehat{\\boldsymbol{\\beta}}_{AB} $$\nWe must first compute the inverse of $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$. The determinant of $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$ is:\n$$ \\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = 0.040(0.050 \\times 0.045 - 0.006 \\times 0.006) - 0.005(0.005 \\times 0.045 - 0.006 \\times (-0.004)) + (-0.004)(0.005 \\times 0.006 - 0.050 \\times (-0.004)) $$\n$$ \\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = 0.040(0.002214) - 0.005(0.000249) - 0.004(0.000230) $$\n$$ \\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = 0.00008856 - 0.000001245 - 0.00000092 = 0.000086395 $$\nThe adjugate matrix, $\\operatorname{adj}(\\widehat{\\boldsymbol{\\Sigma}}_{AB})$, is the transpose of the cofactor matrix. Since $\\widehat{\\boldsymbol{\\Sigma}}_{AB}$ is symmetric, its cofactor matrix is also symmetric, and thus equals the adjugate.\n$$ \\operatorname{adj}(\\widehat{\\boldsymbol{\\Sigma}}_{AB}) = \\begin{pmatrix} 0.002214 & -0.000249 & 0.000230 \\\\ -0.000249 & 0.001784 & -0.000260 \\\\ 0.000230 & -0.000260 & 0.001975 \\end{pmatrix} $$\nThe inverse matrix is $\\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} = \\frac{1}{\\det(\\widehat{\\boldsymbol{\\Sigma}}_{AB})} \\operatorname{adj}(\\widehat{\\boldsymbol{\\Sigma}}_{AB})$:\n$$ \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} = \\frac{1}{0.000086395} \\begin{pmatrix} 0.002214 & -0.000249 & 0.000230 \\\\ -0.000249 & 0.001784 & -0.000260 \\\\ 0.000230 & -0.000260 & 0.001975 \\end{pmatrix} \\approx \\begin{pmatrix} 25.6263 & -2.8821 & 2.6622 \\\\ -2.8821 & 20.6493 & -3.0094 \\\\ 2.6622 & -3.0094 & 22.8601 \\end{pmatrix} $$\nNow we compute the quadratic form for $W$:\n$$ W = \\begin{pmatrix} 0.42 & -0.18 & 0.30 \\end{pmatrix} \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1} \\begin{pmatrix} 0.42 \\\\ -0.18 \\\\ 0.30 \\end{pmatrix} $$\nFirst, we compute the product $\\widehat{\\boldsymbol{\\beta}}_{AB}^{\\top} \\widehat{\\boldsymbol{\\Sigma}}_{AB}^{-1}$:\n$$ \\begin{pmatrix} 0.42 & -0.18 & 0.30 \\end{pmatrix} \\begin{pmatrix} 25.6263 & -2.8821 & 2.6622 \\\\ -2.8821 & 20.6493 & -3.0094 \\\\ 2.6622 & -3.0094 & 22.8601 \\end{pmatrix} $$\nThis gives a row vector:\n$$ \\approx \\begin{pmatrix} (10.7630 + 0.5188 + 0.7987) & (-1.2105 - 3.7169 - 0.9028) & (1.1181 + 0.5417 + 6.8580) \\end{pmatrix} $$\n$$ \\approx \\begin{pmatrix} 12.0805 & -5.8302 & 8.5178 \\end{pmatrix} $$\nFinally, we multiply this row vector by $\\widehat{\\boldsymbol{\\beta}}_{AB}$:\n$$ W \\approx \\begin{pmatrix} 12.0805 & -5.8302 & 8.5178 \\end{pmatrix} \\begin{pmatrix} 0.42 \\\\ -0.18 \\\\ 0.30 \\end{pmatrix} $$\n$$ W \\approx 12.0805 \\times 0.42 + (-5.8302) \\times (-0.18) + 8.5178 \\times 0.30 $$\n$$ W \\approx 5.07381 + 1.04944 + 2.55534 = 8.67859 $$\nUnder the null hypothesis, $W$ follows a chi-squared distribution with $3$ degrees of freedom. The calculated value of the statistic is approximately $8.67859$.\nRounding to four significant figures, we get $W = 8.679$.", "answer": "$$\n\\boxed{8.679}\n$$", "id": "4899236"}]}