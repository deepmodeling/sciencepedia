{"hands_on_practices": [{"introduction": "Regression splines offer a powerful way to model nonlinear trends flexibly, but their inner workings can seem opaque. This exercise peels back the curtain by guiding you through the construction of the design matrix for a cubic spline model. By creating the basis vectors yourself, you will gain a concrete understanding of how these sophisticated models are fundamentally built upon the principles of linear algebra and piecewise polynomials [@problem_id:4938191].", "problem": "A biostatistician is modeling a nonlinear doseâ€“response relationship between a continuous dose variable $x$ and a continuous biomarker outcome using a regression spline within a linear model. The model uses a truncated power basis of degree $3$ (cubic) with internal knots to allow localized flexibility. Let the observation points be $x_{1:7} = \\{\\,0.5,\\,1.2,\\,1.7,\\,2.0,\\,2.3,\\,3.1,\\,3.8\\,\\}$ and the internal knots be $\\kappa_{1:3}=\\{\\,1.5,\\,2.5,\\,3.0\\,\\}$. The truncated power basis is built from the constant and polynomial terms together with cubic hinge functions defined by $(u)_{+} = \\max\\{u,0\\}$. \n\nTasks:\n1. Starting from the definition of a linear model with basis expansion and the definition of the truncated power function, construct the column vectors of the design matrix corresponding to each basis function for the cubic truncated power basis: the intercept $b_{0}(x)=1$, the polynomial terms $b_{1}(x)=x$, $b_{2}(x)=x^{2}$, $b_{3}(x)=x^{3}$, and the truncated cubic terms $b_{3+m}(x)=(x-\\kappa_{m})_{+}^{3}$ for $m=1,2,3$. Each column vector should be the evaluation of the corresponding basis function at $x_{1},\\ldots,x_{7}$.\n2. Using your constructed columns, state the sparsity pattern of the truncated-power columns in terms of which entries are zero and nonzero relative to each knot location, and then compute the total number of nonzero entries across the three truncated cubic columns combined. \n\nProvide your final answer as that single total count. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in standard statistical methodology (regression splines), well-posed with all necessary information provided, and objective in its language. We will proceed with the solution.\n\nThe problem requires the construction of column vectors for a design matrix based on a cubic truncated power spline basis. A linear model with a basis expansion represents the conditional expectation of an outcome $Y$ as a linear combination of basis functions $b_j(x)$ evaluated at a predictor value $x$:\n$$ E[Y|X=x] = \\sum_{j=0}^{p} \\beta_j b_j(x) $$\nThe design matrix for a set of observations $x_1, \\dots, x_n$ is an $n \\times (p+1)$ matrix where the entry in the $i$-th row and $j$-th column is $b_j(x_i)$. Each column is thus the vector of evaluations of a single basis function across all observation points.\n\nThe given observation points are the set $x_{1:7} = \\{\\,0.5,\\,1.2,\\,1.7,\\,2.0,\\,2.3,\\,3.1,\\,3.8\\,\\}$.\nThe given internal knots are $\\kappa_{1:3}=\\{\\,1.5,\\,2.5,\\,3.0\\,\\}$.\nThe spline basis is of degree $p_{poly}=3$ (cubic), resulting in $p_{poly}+1=4$ polynomial basis functions and $3$ truncated power basis functions, for a total of $p+1 = 7$ basis functions.\n\nThe basis functions are:\n- Intercept: $b_{0}(x)=1$\n- Polynomial terms: $b_{1}(x)=x$, $b_{2}(x)=x^{2}$, $b_{3}(x)=x^{3}$\n- Truncated power terms: $b_{3+m}(x)=(x-\\kappa_{m})_{+}^{3}$ for $m \\in \\{1,2,3\\}$, where $(u)_{+} = \\max\\{u,0\\}$.\n\n### Task 1: Construct the Column Vectors\n\nWe will compute each of the $7$ column vectors, where each vector has $7$ entries corresponding to the evaluation of the basis function at each point in $x_{1:7}$.\n\n**Column $\\mathbf{b}_0$ (for basis function $b_0(x) = 1$):**\nThis column consists of all ones.\n$$ \\mathbf{b}_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\n\n**Column $\\mathbf{b}_1$ (for basis function $b_1(x) = x$):**\nThis column consists of the observation points themselves.\n$$ \\mathbf{b}_1 = \\begin{pmatrix} 0.5 \\\\ 1.2 \\\\ 1.7 \\\\ 2.0 \\\\ 2.3 \\\\ 3.1 \\\\ 3.8 \\end{pmatrix} $$\n\n**Column $\\mathbf{b}_2$ (for basis function $b_2(x) = x^2$):**\nEach entry is the square of the corresponding observation point.\n$$ \\mathbf{b}_2 = \\begin{pmatrix} (0.5)^2 \\\\ (1.2)^2 \\\\ (1.7)^2 \\\\ (2.0)^2 \\\\ (2.3)^2 \\\\ (3.1)^2 \\\\ (3.8)^2 \\end{pmatrix} = \\begin{pmatrix} 0.25 \\\\ 1.44 \\\\ 2.89 \\\\ 4.00 \\\\ 5.29 \\\\ 9.61 \\\\ 14.44 \\end{pmatrix} $$\n\n**Column $\\mathbf{b}_3$ (for basis function $b_3(x) = x^3$):**\nEach entry is the cube of the corresponding observation point.\n$$ \\mathbf{b}_3 = \\begin{pmatrix} (0.5)^3 \\\\ (1.2)^3 \\\\ (1.7)^3 \\\\ (2.0)^3 \\\\ (2.3)^3 \\\\ (3.1)^3 \\\\ (3.8)^3 \\end{pmatrix} = \\begin{pmatrix} 0.125 \\\\ 1.728 \\\\ 4.913 \\\\ 8.000 \\\\ 12.167 \\\\ 29.791 \\\\ 54.872 \\end{pmatrix} $$\n\n**Column $\\mathbf{b}_4$ (for basis function $b_4(x) = (x - \\kappa_1)_+^3 = (x - 1.5)_+^3$):**\nEach entry is $(\\max\\{x_i - 1.5, 0\\})^3$.\n- $x_1=0.5: (\\max\\{0.5-1.5, 0\\})^3 = 0^3 = 0$\n- $x_2=1.2: (\\max\\{1.2-1.5, 0\\})^3 = 0^3 = 0$\n- $x_3=1.7: (\\max\\{1.7-1.5, 0\\})^3 = (0.2)^3 = 0.008$\n- $x_4=2.0: (\\max\\{2.0-1.5, 0\\})^3 = (0.5)^3 = 0.125$\n- $x_5=2.3: (\\max\\{2.3-1.5, 0\\})^3 = (0.8)^3 = 0.512$\n- $x_6=3.1: (\\max\\{3.1-1.5, 0\\})^3 = (1.6)^3 = 4.096$\n- $x_7=3.8: (\\max\\{3.8-1.5, 0\\})^3 = (2.3)^3 = 12.167$\n$$ \\mathbf{b}_4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0.008 \\\\ 0.125 \\\\ 0.512 \\\\ 4.096 \\\\ 12.167 \\end{pmatrix} $$\n\n**Column $\\mathbf{b}_5$ (for basis function $b_5(x) = (x - \\kappa_2)_+^3 = (x - 2.5)_+^3$):**\nEach entry is $(\\max\\{x_i - 2.5, 0\\})^3$.\n- For $x_i \\in \\{0.5, 1.2, 1.7, 2.0, 2.3\\}$, $x_i - 2.5 \\le 0$, so the entry is $0$.\n- $x_6=3.1: (\\max\\{3.1-2.5, 0\\})^3 = (0.6)^3 = 0.216$\n- $x_7=3.8: (\\max\\{3.8-2.5, 0\\})^3 = (1.3)^3 = 2.197$\n$$ \\mathbf{b}_5 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0.216 \\\\ 2.197 \\end{pmatrix} $$\n\n**Column $\\mathbf{b}_6$ (for basis function $b_6(x) = (x - \\kappa_3)_+^3 = (x - 3.0)_+^3$):**\nEach entry is $(\\max\\{x_i - 3.0, 0\\})^3$.\n- For $x_i \\in \\{0.5, 1.2, 1.7, 2.0, 2.3\\}$, $x_i - 3.0 \\le 0$, so the entry is $0$.\n- $x_6=3.1: (\\max\\{3.1-3.0, 0\\})^3 = (0.1)^3 = 0.001$\n- $x_7=3.8: (\\max\\{3.8-3.0, 0\\})^3 = (0.8)^3 = 0.512$\n$$ \\mathbf{b}_6 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0.001 \\\\ 0.512 \\end{pmatrix} $$\n\n### Task 2: Sparsity Pattern and Total Nonzero Entries\n\nThe problem focuses on the sparsity pattern and count of nonzero entries for the truncated-power columns, which are $\\mathbf{b}_4$, $\\mathbf{b}_5$, and $\\mathbf{b}_6$.\n\n**Sparsity Pattern:**\nThe sparsity pattern of a truncated power basis column $\\mathbf{b}_{3+m}$ (corresponding to knot $\\kappa_m$) is structurally determined by the location of the knot relative to the ordered observation points $x_i$. The entry $(x_i - \\kappa_m)_+^3$ is strictly positive if and only if $x_i > \\kappa_m$. It is zero if and only if $x_i \\le \\kappa_m$. This results in a column with a leading block of zeros for all observations up to the knot, followed by a block of non-zero (positive) values for all observations beyond the knot.\n\n**Count of Nonzero Entries:**\nWe count the number of nonzero entries in each of the three truncated cubic columns.\n\n1.  **Column $\\mathbf{b}_4$ (knot $\\kappa_1 = 1.5$):**\n    The observation points $x_i$ greater than $1.5$ are $\\{1.7, 2.0, 2.3, 3.1, 3.8\\}$. There are $5$ such points.\n    The number of nonzero entries in $\\mathbf{b}_4$ is $5$.\n\n2.  **Column $\\mathbf{b}_5$ (knot $\\kappa_2 = 2.5$):**\n    The observation points $x_i$ greater than $2.5$ are $\\{3.1, 3.8\\}$. There are $2$ such points.\n    The number of nonzero entries in $\\mathbf{b}_5$ is $2$.\n\n3.  **Column $\\mathbf{b}_6$ (knot $\\kappa_3 = 3.0$):**\n    The observation points $x_i$ greater than $3.0$ are $\\{3.1, 3.8\\}$. There are $2$ such points.\n    The number of nonzero entries in $\\mathbf{b}_6$ is $2$.\n\n**Total Number of Nonzero Entries:**\nThe total number of nonzero entries across the three truncated cubic columns combined is the sum of the counts from each column.\nTotal count = (Nonzero entries in $\\mathbf{b}_4$) + (Nonzero entries in $\\mathbf{b}_5$) + (Nonzero entries in $\\mathbf{b}_6$)\n$$ \\text{Total} = 5 + 2 + 2 = 9 $$", "answer": "$$\\boxed{9}$$", "id": "4938191"}, {"introduction": "In a simple linear model, the effect of a one-unit change in a predictor is constant. However, in models with polynomial or other nonlinear terms, this effect becomes a function of the predictor's current value. This practice challenges you to derive and interpret the change in prediction for a complex model, solidifying your understanding of how to describe effects in a nonlinear world [@problem_id:4938192].", "problem": "A biostatistics study models the expected log-concentration of C-reactive protein (CRP) in blood as a nonlinear function of a single exposure measurement $x$ using a General Linear Model (GLM), where the basis functions include polynomial and logarithmic terms. Specifically, the fitted prediction function is\n$$\n\\hat{y}(x) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x + \\hat{\\beta}_{2} x^{2} + \\hat{\\beta}_{3} x^{3} + \\hat{\\beta}_{4} \\ln(x+1),\n$$\nwith $x \\geq 0$ representing the observed exposure on a nonnegative scale and the logarithm taken to be the natural logarithm. Consider a clinically meaningful increment $\\Delta > 0$ in exposure, and define the predicted change\n$$\n\\hat{y}(x+\\Delta) - \\hat{y}(x).\n$$\nStarting from core definitions of the General Linear Model and properties of polynomials and logarithms, derive an explicit analytic expression for $\\hat{y}(x+\\Delta) - \\hat{y}(x)$ in terms of $\\{\\hat{\\beta}_{k}\\}$, $x$, and $\\Delta$. Then interpret how the predicted change depends on the current value of $x$ and on $\\Delta$ by identifying which components of the expression vary with $x$ and which depend only on $\\Delta$.\n\nExpress the final answer as a single closed-form analytic expression. No rounding is required, and no units are to be included in your expression.", "solution": "The problem requires the derivation of an explicit analytic expression for the change in a predicted outcome, $\\hat{y}(x+\\Delta) - \\hat{y}(x)$, based on a given General Linear Model, and an interpretation of this change. The problem is scientifically sound, well-posed, and all terms are formally defined. Therefore, a solution can be derived.\n\nThe predicted outcome, $\\hat{y}(x)$, is given by the function:\n$$\n\\hat{y}(x) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x + \\hat{\\beta}_{2} x^{2} + \\hat{\\beta}_{3} x^{3} + \\hat{\\beta}_{4} \\ln(x+1)\n$$\nwhere $x \\geq 0$ is the exposure measurement and $\\hat{\\beta}_{k}$ are the estimated model coefficients.\n\nFirst, we evaluate the prediction function at the new exposure level, $x+\\Delta$, where $\\Delta > 0$:\n$$\n\\hat{y}(x+\\Delta) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} (x+\\Delta) + \\hat{\\beta}_{2} (x+\\Delta)^{2} + \\hat{\\beta}_{3} (x+\\Delta)^{3} + \\hat{\\beta}_{4} \\ln((x+\\Delta)+1)\n$$\nThe quantity of interest is the difference $\\hat{y}(x+\\Delta) - \\hat{y}(x)$. We compute this by subtracting the expression for $\\hat{y}(x)$ from the expression for $\\hat{y}(x+\\Delta)$:\n$$\n\\hat{y}(x+\\Delta) - \\hat{y}(x) = \\left( \\hat{\\beta}_{0} + \\hat{\\beta}_{1} (x+\\Delta) + \\hat{\\beta}_{2} (x+\\Delta)^{2} + \\hat{\\beta}_{3} (x+\\Delta)^{3} + \\hat{\\beta}_{4} \\ln(x+\\Delta+1) \\right) - \\left( \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x + \\hat{\\beta}_{2} x^{2} + \\hat{\\beta}_{3} x^{3} + \\hat{\\beta}_{4} \\ln(x+1) \\right)\n$$\nWe proceed by grouping terms associated with each coefficient $\\hat{\\beta}_{k}$:\nThe constant terms $\\hat{\\beta}_{0}$ cancel out:\n$$\n\\hat{\\beta}_{0} - \\hat{\\beta}_{0} = 0\n$$\nFor the term with $\\hat{\\beta}_{1}$:\n$$\n\\hat{\\beta}_{1} (x+\\Delta) - \\hat{\\beta}_{1} x = \\hat{\\beta}_{1} x + \\hat{\\beta}_{1} \\Delta - \\hat{\\beta}_{1} x = \\hat{\\beta}_{1} \\Delta\n$$\nFor the term with $\\hat{\\beta}_{2}$, we expand the quadratic $(x+\\Delta)^{2} = x^{2} + 2x\\Delta + \\Delta^{2}$:\n$$\n\\hat{\\beta}_{2} (x+\\Delta)^{2} - \\hat{\\beta}_{2} x^{2} = \\hat{\\beta}_{2} ( (x^{2} + 2x\\Delta + \\Delta^{2}) - x^{2} ) = \\hat{\\beta}_{2} (2x\\Delta + \\Delta^{2})\n$$\nFor the term with $\\hat{\\beta}_{3}$, we use the binomial expansion for the cube, $(x+\\Delta)^{3} = x^{3} + 3x^{2}\\Delta + 3x\\Delta^{2} + \\Delta^{3}$:\n$$\n\\hat{\\beta}_{3} (x+\\Delta)^{3} - \\hat{\\beta}_{3} x^{3} = \\hat{\\beta}_{3} ( (x^{3} + 3x^{2}\\Delta + 3x\\Delta^{2} + \\Delta^{3}) - x^{3} ) = \\hat{\\beta}_{3} (3x^{2}\\Delta + 3x\\Delta^{2} + \\Delta^{3})\n$$\nFor the term with $\\hat{\\beta}_{4}$, we use the property of logarithms, $\\ln(a) - \\ln(b) = \\ln(a/b)$:\n$$\n\\hat{\\beta}_{4} \\ln(x+\\Delta+1) - \\hat{\\beta}_{4} \\ln(x+1) = \\hat{\\beta}_{4} \\left( \\ln(x+\\Delta+1) - \\ln(x+1) \\right) = \\hat{\\beta}_{4} \\ln\\left(\\frac{x+\\Delta+1}{x+1}\\right)\n$$\nCombining these results yields the final explicit analytic expression for the predicted change:\n$$\n\\hat{y}(x+\\Delta) - \\hat{y}(x) = \\hat{\\beta}_{1} \\Delta + \\hat{\\beta}_{2} (2x\\Delta + \\Delta^{2}) + \\hat{\\beta}_{3} (3x^{2}\\Delta + 3x\\Delta^{2} + \\Delta^{3}) + \\hat{\\beta}_{4} \\ln\\left(\\frac{x+\\Delta+1}{x+1}\\right)\n$$\nTo interpret this expression, we analyze the dependence of each component on the current exposure $x$ and the increment $\\Delta$.\n\n1.  The term $\\hat{\\beta}_{1} \\Delta$ arises from the linear component of the model. For a fixed increment $\\Delta$, its contribution to the total change is constant and does not depend on the initial exposure level $x$.\n\n2.  The term $\\hat{\\beta}_{2} (2x\\Delta + \\Delta^{2})$ arises from the quadratic component. This contribution depends linearly on $x$. This means that the effect of the increment $\\Delta$ mediated by the $x^2$ term is larger at higher baseline levels of exposure $x$.\n\n3.  The term $\\hat{\\beta}_{3} (3x^{2}\\Delta + 3x\\Delta^{2} + \\Delta^{3})$ arises from the cubic component. This contribution has a quadratic dependence on $x$ (via the $3x^2\\Delta$ term), indicating that its effect on the change grows even more rapidly with the baseline exposure $x$.\n\n4.  The term $\\hat{\\beta}_{4} \\ln\\left(\\frac{x+\\Delta+1}{x+1}\\right)$ is from the logarithmic component. We can rewrite the argument of the logarithm as $1 + \\frac{\\Delta}{x+1}$. As $x \\to \\infty$, the fraction $\\frac{\\Delta}{x+1} \\to 0$, and so $\\ln\\left(1 + \\frac{\\Delta}{x+1}\\right) \\to \\ln(1) = 0$. Therefore, the contribution of the logarithmic term to the change diminishes as the baseline exposure $x$ increases. This term captures a form of saturation effect, where the impact of an additional exposure increment lessens at higher overall exposure levels.\n\nIn summary, the predicted change in log-CRP concentration for a fixed exposure increment $\\Delta$ is not a constant value but rather a function of the initial exposure level $x$. This is the defining characteristic of a nonlinear relationship. The change is a combination of a constant effect, an effect that grows linearly with $x$, an effect that grows quadratically with $x$, and an effect that diminishes as $x$ increases.", "answer": "$$\\boxed{\\hat{\\beta}_{1}\\Delta + \\hat{\\beta}_{2}(2x\\Delta + \\Delta^{2}) + \\hat{\\beta}_{3}(3x^{2}\\Delta + 3x\\Delta^{2} + \\Delta^{3}) + \\hat{\\beta}_{4}\\ln\\left(\\frac{x+\\Delta+1}{x+1}\\right)}$$", "id": "4938192"}, {"introduction": "Interpreting a model goes beyond calculating a single number; it requires quantifying our confidence in that number. This exercise introduces a critical skill in statistical inference: using the delta method to compute the standard error of a derived quantity, specifically the marginal effect of a predictor. Mastering this allows you to assess the statistical significance of your findings and build confidence intervals around your model's predictions [@problem_id:4938223].", "problem": "A cohort study investigates how systolic blood pressure depends on age using a polynomial in a linear model estimated by ordinary least squares (OLS). Let $Y$ denote systolic blood pressure in millimeters of mercury and $x$ denote age in decades centered at $50$ years, i.e., $x = (\\text{age in years} - 50)/10$. The working model for the conditional mean is\n$$\nm(x;\\beta) = \\mathbb{E}[Y \\mid x] = \\beta_{0} + \\beta_{1} x + \\beta_{2} x^{2} + \\beta_{3} x^{3}.\n$$\nFrom a large sample, the OLS estimator $\\hat{\\beta} = (\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\hat{\\beta}_{2}, \\hat{\\beta}_{3})^{\\top}$ is approximately multivariate normal with estimated covariance matrix\n$$\n\\widehat{\\mathrm{Var}}(\\hat{\\beta}) =\n\\begin{pmatrix}\n9.00000 & -0.60000 & 0.06000 & -0.01200 \\\\\n-0.60000 & 0.07240 & -0.00940 & 0.00170 \\\\\n0.06000 & -0.00940 & 0.001444 & -0.000254 \\\\\n-0.01200 & 0.00170 & -0.000254 & 0.00004549\n\\end{pmatrix}.\n$$\nConsider the marginal effect of age on the mean outcome at age $55$ years, i.e., at $x_{0} = 0.5$, defined as the derivative $\\partial m(x;\\beta)/\\partial x$ evaluated at $x = x_{0}$. Treat this marginal effect as a smooth function of $\\beta$ and use the first-order multivariate delta method, together with the covariance matrix above, to compute the standard error of the estimated marginal effect at $x_{0} = 0.5$. Express your final answer in millimeters of mercury per decade and round your answer to $4$ significant figures.", "solution": "The problem is valid. It is a well-posed application of standard statistical methods, specifically the delta method, to a common problem in biostatistical modeling. All necessary information is provided, and the context is scientifically sound.\n\nThe problem requires the computation of the standard error for the estimated marginal effect of age on systolic blood pressure at a specific age. The marginal effect is defined as the derivative of the conditional mean function with respect to the age-related predictor variable.\n\nThe conditional mean model for systolic blood pressure $Y$ given the centered age variable $x$ is:\n$$\nm(x;\\beta) = \\mathbb{E}[Y \\mid x] = \\beta_{0} + \\beta_{1} x + \\beta_{2} x^{2} + \\beta_{3} x^{3}\n$$\nThe predictor $x$ is age in decades centered at $50$ years, $x = (\\text{age in years} - 50)/10$. We are interested in the marginal effect at age $55$ years, which corresponds to:\n$$\nx_{0} = \\frac{55 - 50}{10} = \\frac{5}{10} = 0.5\n$$\nThe marginal effect of age, as a function of $x$, is the derivative of $m(x;\\beta)$ with respect to $x$:\n$$\n\\frac{\\partial m(x;\\beta)}{\\partial x} = \\frac{\\partial}{\\partial x} (\\beta_{0} + \\beta_{1} x + \\beta_{2} x^{2} + \\beta_{3} x^{3}) = \\beta_{1} + 2\\beta_{2} x + 3\\beta_{3} x^{2}\n$$\nWe need to evaluate this marginal effect at $x_{0} = 0.5$. Let this quantity be denoted by $g(\\beta)$:\n$$\ng(\\beta) = \\beta_{1} + 2\\beta_{2} (0.5) + 3\\beta_{3} (0.5)^{2} = \\beta_{1} + \\beta_{2} + 3(0.25)\\beta_{3} = \\beta_{1} + \\beta_{2} + 0.75\\beta_{3}\n$$\nThis is a linear combination of the parameters in the vector $\\beta = (\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3})^{\\top}$. We can express $g(\\beta)$ as a dot product $c^{\\top}\\beta$, where $c$ is a constant vector:\n$$\nc = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0.75 \\end{pmatrix}\n$$\nThe estimated marginal effect is $g(\\hat{\\beta}) = c^{\\top}\\hat{\\beta}$, where $\\hat{\\beta}$ is the OLS estimator of $\\beta$. The variance of this linear combination is given by the exact formula $\\mathrm{Var}(c^{\\top}\\hat{\\beta}) = c^{\\top}\\mathrm{Var}(\\hat{\\beta})c$. The problem states to use the first-order multivariate delta method. For a linear function $g(\\beta) = c^{\\top}\\beta$, the gradient is $\\nabla g(\\beta) = c$. The delta method approximation for the variance is $(\\nabla g)^{\\top}\\mathrm{Var}(\\hat{\\beta})(\\nabla g) = c^{\\top}\\mathrm{Var}(\\hat{\\beta})c$, which is identical to the exact formula.\n\nLet $\\Sigma = \\widehat{\\mathrm{Var}}(\\hat{\\beta})$ be the estimated covariance matrix provided in the problem statement:\n$$\n\\Sigma =\n\\begin{pmatrix}\n9.00000 & -0.60000 & 0.06000 & -0.01200 \\\\\n-0.60000 & 0.07240 & -0.00940 & 0.00170 \\\\\n0.06000 & -0.00940 & 0.001444 & -0.000254 \\\\\n-0.01200 & 0.00170 & -0.000254 & 0.00004549\n\\end{pmatrix}\n$$\nThe estimated variance of the marginal effect, $\\widehat{\\mathrm{Var}}(g(\\hat{\\beta}))$, is computed by the quadratic form $c^{\\top}\\Sigma c$:\n$$\n\\widehat{\\mathrm{Var}}(g(\\hat{\\beta})) = c^{\\top}\\Sigma c =\n\\begin{pmatrix} 0 & 1 & 1 & 0.75 \\end{pmatrix}\n\\begin{pmatrix}\n9.00000 & -0.60000 & 0.06000 & -0.01200 \\\\\n-0.60000 & 0.07240 & -0.00940 & 0.00170 \\\\\n0.06000 & -0.00940 & 0.001444 & -0.000254 \\\\\n-0.01200 & 0.00170 & -0.000254 & 0.00004549\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0.75 \\end{pmatrix}\n$$\nExpanding the quadratic form, with $c_i$ being the components of $c$ and $\\Sigma_{ij}$ being the elements of $\\Sigma$:\n$$\n\\widehat{\\mathrm{Var}}(g(\\hat{\\beta})) = \\sum_{i=1}^{4}\\sum_{j=1}^{4} c_i c_j \\Sigma_{ij}\n$$\nSince $c_1 = 0$, we only need to consider terms where $i,j \\in \\{2, 3, 4\\}$:\n$$\n\\widehat{\\mathrm{Var}}(g(\\hat{\\beta})) = c_2^2\\Sigma_{22} + c_3^2\\Sigma_{33} + c_4^2\\Sigma_{44} + 2c_2c_3\\Sigma_{23} + 2c_2c_4\\Sigma_{24} + 2c_3c_4\\Sigma_{34}\n$$\nSubstituting the numerical values for $c_i$ and $\\Sigma_{ij}$:\n$$\nc_2 = 1, \\quad c_3 = 1, \\quad c_4 = 0.75\n$$\n$$\n\\Sigma_{22} = 0.07240, \\quad \\Sigma_{33} = 0.001444, \\quad \\Sigma_{44} = 0.00004549\n$$\n$$\n\\Sigma_{23} = -0.00940, \\quad \\Sigma_{24} = 0.00170, \\quad \\Sigma_{34} = -0.000254\n$$\nNow, we compute each term:\n\\begin{itemize}\n    \\item $c_2^2\\Sigma_{22} = (1)^2(0.07240) = 0.07240$\n    \\item $c_3^2\\Sigma_{33} = (1)^2(0.001444) = 0.001444$\n    \\item $c_4^2\\Sigma_{44} = (0.75)^2(0.00004549) = (0.5625)(0.00004549) = 0.000025588125$\n    \\item $2c_2c_3\\Sigma_{23} = 2(1)(1)(-0.00940) = -0.01880$\n    \\item $2c_2c_4\\Sigma_{24} = 2(1)(0.75)(0.00170) = 1.5(0.00170) = 0.00255$\n    \\item $2c_3c_4\\Sigma_{34} = 2(1)(0.75)(-0.000254) = 1.5(-0.000254) = -0.000381$\n\\end{itemize}\nSumming these values gives the variance:\n$$\n\\widehat{\\mathrm{Var}}(g(\\hat{\\beta})) = 0.07240 + 0.001444 + 0.000025588125 - 0.01880 + 0.00255 - 0.000381\n$$\n$$\n\\widehat{\\mathrm{Var}}(g(\\hat{\\beta})) = 0.057238588125\n$$\nThe standard error (SE) is the square root of the variance:\n$$\n\\mathrm{SE}(g(\\hat{\\beta})) = \\sqrt{\\widehat{\\mathrm{Var}}(g(\\hat{\\beta}))} = \\sqrt{0.057238588125} \\approx 0.23924587\n$$\nThe problem asks for the answer to be rounded to $4$ significant figures. The first four significant figures are $2$, $3$, $9$, and $2$. The fifth significant digit is $4$, so we round down.\n$$\n\\mathrm{SE}(g(\\hat{\\beta})) \\approx 0.2392\n$$\nThe units of the marginal effect are the units of $Y$ (millimeters of mercury) divided by the units of $x$ (decades), which is millimeters of mercury per decade. The standard error has the same units.", "answer": "$$\\boxed{0.2392}$$", "id": "4938223"}]}