{"hands_on_practices": [{"introduction": "Understanding regression diagnostics begins with leverage, a measure of an observation's potential to influence the model fit. This practice grounds this abstract concept in a concrete calculation, starting from first principles. You will derive the leverage formula for a simple linear model and discover what makes a data point have high leverage [@problem_id:4908310].", "problem": "A biostatistics study models a continuous clinical outcome $y$ by simple linear regression with an intercept and a single predictor $x$ that has been centered so that $\\sum_{i=1}^{n} x_{i} = 0$. The design matrix for Ordinary Least Squares (OLS) regression has two columns: a column of ones for the intercept and the centered predictor $x$. Consider the specific dataset of $n = 6$ subjects with centered predictor values\n$$\nx = \\{-4,\\,-2,\\,-1,\\,1,\\,3,\\,3\\}.\n$$\nStarting from the definition of the OLS hat matrix $H$ as $H = X\\left(X^{\\top}X\\right)^{-1}X^{\\top}$, derive the diagonal leverage element $h_{ii}$ for this centered design and compute the leverage $h_{66}$ for the subject with $x_{6} = 3$. Express your final numerical answer as an exact fraction. In your derivation, explain which values of $x_{i}$ drive leverage in this centered-predictor design and why.", "solution": "The solution proceeds by first deriving the general formula for the leverage $h_{ii}$ in a simple linear regression model with a centered predictor, and then applying this formula to the specific data provided.\n\nThe model is $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$. The design matrix $\\mathbf{X}$ for $n$ observations is an $n \\times 2$ matrix, where the first column is a vector of ones for the intercept term $\\beta_0$, and the second column contains the values of the centered predictor $x$.\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{pmatrix}\n$$\nThe hat matrix is defined as $\\mathbf{H} = \\mathbf{X}\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\top}$. The diagonal elements, $h_{ii}$, are the leverage scores for each observation. The leverage $h_{ii}$ can be calculated as the product of the $i$-th row of $\\mathbf{X}$, the matrix $\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1}$, and the $i$-th column of $\\mathbf{X}^{\\top}$ (which is the transpose of the $i$-th row of $\\mathbf{X}$). Let $\\mathbf{X}_{i\\cdot}$ denote the $i$-th row of $\\mathbf{X}$, so $\\mathbf{X}_{i\\cdot} = \\begin{pmatrix} 1 & x_i \\end{pmatrix}$. Then, $h_{ii} = \\mathbf{X}_{i\\cdot}\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1}\\mathbf{X}_{i\\cdot}^{\\top}$.\n\nFirst, we compute the matrix $\\mathbf{X}^{\\top}\\mathbf{X}$:\n$$\n\\mathbf{X}^{\\top}\\mathbf{X} = \\begin{pmatrix}\n1 & 1 & \\cdots & 1 \\\\\nx_1 & x_2 & \\cdots & x_n\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\sum_{j=1}^{n} 1 & \\sum_{j=1}^{n} x_j \\\\\n\\sum_{j=1}^{n} x_j & \\sum_{j=1}^{n} x_j^2\n\\end{pmatrix}\n$$\nThe problem states that the predictor $x$ is centered, which means $\\sum_{j=1}^{n} x_j = 0$. Substituting this into the matrix gives:\n$$\n\\mathbf{X}^{\\top}\\mathbf{X} = \\begin{pmatrix}\nn & 0 \\\\\n0 & \\sum_{j=1}^{n} x_j^2\n\\end{pmatrix}\n$$\nThe centering of the predictor results in a diagonal $\\mathbf{X}^{\\top}\\mathbf{X}$ matrix, which simplifies the inversion. The inverse, $\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1}$, is found by taking the reciprocal of each diagonal element:\n$$\n\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1} = \\begin{pmatrix}\n\\frac{1}{n} & 0 \\\\\n0 & \\frac{1}{\\sum_{j=1}^{n} x_j^2}\n\\end{pmatrix}\n$$\nNow, we can derive the formula for $h_{ii}$:\n$$\nh_{ii} = \\mathbf{X}_{i\\cdot}\\left(\\mathbf{X}^{\\top}\\mathbf{X}\\right)^{-1}\\mathbf{X}_{i\\cdot}^{\\top} =\n\\begin{pmatrix} 1 & x_i \\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{n} & 0 \\\\\n0 & \\frac{1}{\\sum_{j=1}^{n} x_j^2}\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\nPerforming the matrix multiplication from left to right:\n$$\nh_{ii} = \\begin{pmatrix} 1 \\cdot \\frac{1}{n} + x_i \\cdot 0 & 1 \\cdot 0 + x_i \\cdot \\frac{1}{\\sum_{j=1}^{n} x_j^2} \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{n} & \\frac{x_i}{\\sum_{j=1}^{n} x_j^2} \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\nCompleting the final multiplication:\n$$\nh_{ii} = \\left(\\frac{1}{n}\\right)(1) + \\left(\\frac{x_i}{\\sum_{j=1}^{n} x_j^2}\\right)(x_i) = \\frac{1}{n} + \\frac{x_i^2}{\\sum_{j=1}^{n} x_j^2}\n$$\nThis is the general expression for the leverage of the $i$-th observation in a simple linear regression with a centered predictor.\n\nThis formula directly addresses the question of what drives leverage. The leverage $h_{ii}$ for observation $i$ is composed of two terms. The first term, $\\frac{1}{n}$, is constant for all observations in the dataset. The second term, $\\frac{x_i^2}{\\sum_{j=1}^{n} x_j^2}$, is specific to observation $i$. Since the denominator $\\sum_{j=1}^{n} x_j^2$ is a constant for the given dataset, the value of this second term is proportional to $x_i^2$. Because the predictor $x$ is centered, its mean is $0$. Thus, $x_i^2 = (x_i - \\bar{x})^2$ is the squared distance of the predictor value from its mean. Consequently, observations with predictor values $x_i$ that are far from the center of the data (i.e., large $|x_i|$) will have a large $x_i^2$ and therefore high leverage. These points are influential because their position in the predictor space gives them a greater potential to pull the regression line towards them.\n\nNow, we compute the specific value $h_{66}$ for the given dataset.\nThe data are $n=6$ with predictor values $x = \\{-4,\\,-2,\\,-1,\\,1,\\,3,\\,3\\}$.\nFirst, we compute the sum of squares of the centered predictor values:\n$$\n\\sum_{j=1}^{6} x_j^2 = (-4)^2 + (-2)^2 + (-1)^2 + 1^2 + 3^2 + 3^2\n$$\n$$\n\\sum_{j=1}^{6} x_j^2 = 16 + 4 + 1 + 1 + 9 + 9 = 40\n$$\nThe observation of interest is the sixth subject, for which $x_6 = 3$.\nUsing the derived formula for $h_{ii}$ with $i=6$, $n=6$, $x_6=3$, and $\\sum x_j^2 = 40$:\n$$\nh_{66} = \\frac{1}{n} + \\frac{x_6^2}{\\sum_{j=1}^{6} x_j^2} = \\frac{1}{6} + \\frac{3^2}{40} = \\frac{1}{6} + \\frac{9}{40}\n$$\nTo express this as a single fraction, we find a common denominator, which is the least common multiple of $6$ and $40$. $\\text{lcm}(6, 40) = 120$.\n$$\nh_{66} = \\frac{1 \\cdot 20}{6 \\cdot 20} + \\frac{9 \\cdot 3}{40 \\cdot 3} = \\frac{20}{120} + \\frac{27}{120} = \\frac{47}{120}\n$$\nThe leverage for the subject with $x_6=3$ is $\\frac{47}{120}$.", "answer": "$$\\boxed{\\frac{47}{120}}$$", "id": "4908310"}, {"introduction": "A large raw residual, $y_i - \\hat{y}_i$, might seem like a clear sign of an outlier, but this can be misleading, especially for high-leverage points. To get a more reliable measure of outlyingness, we must standardize residuals in a way that accounts for leverage. This exercise has you calculate and compare internally and externally studentized residuals to see why the latter is a more robust diagnostic tool for outlier detection [@problem_id:4908256].", "problem": "A biostatistics study uses a multiple linear regression to relate a continuous biomarker measurement to several covariates. Assume the standard homoscedastic Gaussian linear model: for subjects indexed by $i=1,\\dots,n$, the model is $y_i = \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta} + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independent and identically distributed, and the least squares estimator yields fitted values $\\hat{y}_i$ and residuals $e_i = y_i - \\hat{y}_i$. The “hat matrix” $\\boldsymbol{H}$ maps observed responses to fitted values via $\\hat{\\boldsymbol{y}} = \\boldsymbol{H}\\boldsymbol{y}$, and the leverage for observation $i$ is $h_{ii}$, the $i$th diagonal element of $\\boldsymbol{H}$. For an observation suspected to be high leverage, you are given the following from the full fit:\n- Sample size $n = 30$ and number of fitted coefficients $p = 3$ (including intercept),\n- Observation-level quantities: $y_i = 12.5$, $\\hat{y}_i = 10.8$, $h_{ii} = 0.82$,\n- The unbiased error variance estimate from the full model, $\\hat{\\sigma}^2 = 1.44$, which equals the Mean Squared Error (MSE), $\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}$ where $\\text{SSE} = \\sum_{j=1}^{n} e_j^2$ is the Sum of Squared Errors (SSE).\n\nStarting from first principles of the linear model and the definitions above, compute the internally studentized residual and the externally studentized (deleted) residual for observation $i$. Express both values as a row vector $\\begin{pmatrix}\\text{internal} & \\text{external}\\end{pmatrix}$. Round your numerical answers to four significant figures. No units are required.", "solution": "The problem requires the calculation of the internally and externally studentized residuals for a specific observation $i$. We will proceed from the fundamental definitions of these quantities.\n\nThe linear model is defined as $y_i = \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta} + \\varepsilon_i$, with errors $\\varepsilon_i$ being independent and identically distributed as $\\mathcal{N}(0, \\sigma^2)$. The ordinary least squares (OLS) estimate of $\\boldsymbol{\\beta}$ leads to fitted values $\\hat{y}_i$ and raw residuals $e_i = y_i - \\hat{y}_i$.\n\nThe variance of the $i$-th residual can be derived from the properties of the hat matrix $\\mathbf{H}$. The residual vector is $\\boldsymbol{e} = (\\mathbf{I}-\\mathbf{H})\\boldsymbol{y}$. Since $\\mathbb{E}[\\boldsymbol{y}] = \\mathbf{X}\\boldsymbol{\\beta}$ and $\\mathbf{H}\\mathbf{X} = \\mathbf{X}$, we have $\\mathbb{E}[\\boldsymbol{e}] = (\\mathbf{I}-\\mathbf{H})\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{X}\\boldsymbol{\\beta} = \\boldsymbol{0}$. The variance-covariance matrix of $\\boldsymbol{e}$ is $\\text{Var}(\\boldsymbol{e}) = (\\mathbf{I}-\\mathbf{H})\\text{Var}(\\boldsymbol{y})(\\mathbf{I}-\\mathbf{H})^{\\top}$. As $\\text{Var}(\\boldsymbol{y}) = \\sigma^2\\mathbf{I}$ and $\\mathbf{I}-\\mathbf{H}$ is symmetric and idempotent, this simplifies to $\\text{Var}(\\boldsymbol{e}) = \\sigma^2(\\mathbf{I}-\\mathbf{H})$. The variance of a single residual $e_i$ is the $i$-th diagonal element of this matrix, which is $\\text{Var}(e_i) = \\sigma^2(1 - h_{ii})$, where $h_{ii}$ is the leverage of observation $i$.\n\nThe unknown error variance $\\sigma^2$ is estimated by the Mean Squared Error (MSE), $\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{n-p}$, where $\\text{SSE} = \\sum_{j=1}^{n} e_j^2$ is the sum of squared errors, $n$ is the sample size, and $p$ is the number of estimated coefficients.\n\nFirst, we calculate the raw residual $e_i$ from the given data: $y_i = 12.5$ and $\\hat{y}_i = 10.8$.\n$$e_i = y_i - \\hat{y}_i = 12.5 - 10.8 = 1.7$$\n\nNext, we establish the value of the estimated error standard deviation, $\\hat{\\sigma}$, from the given $\\hat{\\sigma}^2 = 1.44$.\n$$\\hat{\\sigma} = \\sqrt{1.44} = 1.2$$\n\nThe internally studentized residual, denoted $r_i$, is the raw residual divided by its estimated standard error, where the estimate $\\hat{\\sigma}$ is from the full model.\n$$r_i = \\frac{e_i}{\\widehat{\\text{SE}}(e_i)} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}$$\nSubstituting the given values $h_{ii} = 0.82$, $n = 30$, and $p = 3$:\n$$r_i = \\frac{1.7}{1.2\\sqrt{1-0.82}} = \\frac{1.7}{1.2\\sqrt{0.18}}$$\n$$r_i \\approx \\frac{1.7}{1.2 \\times 0.424264} \\approx \\frac{1.7}{0.509117} \\approx 3.33905$$\nRounding to four significant figures, the internally studentized residual is $3.339$.\n\nThe externally studentized residual, denoted $t_i$, is conceptually similar but uses an estimate of $\\sigma^2$ obtained from the dataset with observation $i$ removed. This estimate is denoted by $\\hat{\\sigma}_{(i)}^2$. The formula for $t_i$ is:\n$$t_i = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1-h_{ii}}}$$\nTo find $\\hat{\\sigma}_{(i)}^2$ without re-running the regression, we use the relationship between the leave-one-out sum of squared errors, $\\text{SSE}_{(i)}$, and the full-model sum of squared errors, $\\text{SSE}$. The identity is $\\text{SSE}_{(i)} = \\text{SSE} - \\frac{e_i^2}{1-h_{ii}}$.\nThe leave-one-out variance estimate is $\\hat{\\sigma}_{(i)}^2 = \\frac{\\text{SSE}_{(i)}}{n-1-p}$.\nWe can express this in terms of full-model quantities:\n$$\\hat{\\sigma}_{(i)}^2 = \\frac{(n-p)\\hat{\\sigma}^2 - \\frac{e_i^2}{1-h_{ii}}}{n-p-1}$$\nUsing the given data:\n- $(n-p)\\hat{\\sigma}^2 = (30-3) \\times 1.44 = 27 \\times 1.44 = 38.88$\n- $e_i^2 = (1.7)^2 = 2.89$\n- $\\frac{e_i^2}{1-h_{ii}} = \\frac{2.89}{1-0.82} = \\frac{2.89}{0.18} \\approx 16.0556$\nThe degrees of freedom for the leave-one-out model are $n-p-1 = 30-3-1=26$.\n$$\\hat{\\sigma}_{(i)}^2 = \\frac{38.88 - 16.0555...}{26} = \\frac{22.8244...}{26} \\approx 0.87786$$\nThen, $\\hat{\\sigma}_{(i)} = \\sqrt{0.87786...} \\approx 0.93694$.\nNow we compute $t_i$:\n$$t_i = \\frac{1.7}{0.93694... \\times \\sqrt{0.18}} \\approx \\frac{1.7}{0.93694... \\times 0.424264...} \\approx \\frac{1.7}{0.39750...} \\approx 4.27666$$\nRounding to four significant figures, the externally studentized residual is $4.277$.\n\nFor verification, we can use the direct relationship between $t_i$ and $r_i$:\n$$t_i = r_i \\sqrt{\\frac{n-p-1}{n-p-r_i^2}}$$\nUsing the unrounded value $r_i \\approx 3.33905$:\n$$t_i \\approx 3.33905 \\sqrt{\\frac{30 - 3 - 1}{30 - 3 - (3.33905)^2}} = 3.33905 \\sqrt{\\frac{26}{27 - 11.14929}}$$\n$$t_i \\approx 3.33905 \\sqrt{\\frac{26}{15.85071}} \\approx 3.33905 \\sqrt{1.64029} \\approx 3.33905 \\times 1.28074 \\approx 4.27666$$\nThe result is consistent.\n\nThe internally studentized residual is $3.339$ and the externally studentized residual is $4.277$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.339 & 4.277\n\\end{pmatrix}\n}\n$$", "id": "4908256"}, {"introduction": "High-leverage points can sometimes distort a regression model so much that their own residuals appear small, a phenomenon known as \"masking.\" This makes them difficult to spot with standard residual plots. In this hands-on coding exercise, you will use simulation to demonstrate this masking effect and see how advanced diagnostics, such as externally studentized residuals and Cook’s distance, are essential for uncovering these hidden influential points [@problem_id:4908265].", "problem": "You must write a complete, runnable program that constructs and analyzes simulated linear regression datasets in order to demonstrate how leverage can mask large residuals in ordinary least squares (OLS) residual plots. The analysis must be grounded in the core definitions of ordinary least squares, linear models, residuals, leverage, and standardized diagnostics, without relying on any pre-supplied shortcut formulas in this statement.\n\nConstruct datasets under a simple linear model with an intercept, where the response vector is generated from a line with additive Gaussian noise. The independent variable values must be simulated as specified below for each test case. For reproducibility, use the given seeds. The program must fit an ordinary least squares model with an intercept to each dataset, then compute the following for each case: leverage values for all observations, ordinary residuals, externally Studentized residuals, and Cook’s distance. Use these diagnostics to evaluate whether two nearby extreme values of the independent variable can mask large residuals in an ordinary least squares residual plot.\n\nFundamental base and assumptions to rely on:\n- An ordinary least squares model minimizes the sum of squared residuals.\n- The fitted values are linear in the observed responses, as determined by the model matrix with an intercept.\n- Leverage quantifies the sensitivity of fitted values to observed responses, on a per-observation basis.\n- Externally Studentized residuals adjust ordinary residuals by an estimate of residual variance that leaves each observation out, and account for leverage.\n- Cook’s distance aggregates residual magnitude and leverage to quantify influence.\n\nFor each test case below, simulate a base dataset of size $n_{0}$ from a line $y = \\beta_{0} + \\beta_{1} x + \\varepsilon$, where $\\varepsilon$ is Gaussian noise with mean $0$ and standard deviation $\\sigma$. The base independent variable values $x$ must be sampled uniformly in the interval $[x_{\\min}, x_{\\max}]$. Then, depending on the case, optionally append two additional observations with extreme and nearby independent variable values $x_{\\text{ext},1}$ and $x_{\\text{ext},2}$ that are both far to the right of the base range. In cases where extreme points are appended, either align their responses close to the generating line with small noise, or shift them by a fixed offset to create a strong directional departure shared by both extremes.\n\nYou must use the following test suite of three cases, each defined by $(n_{0}, \\beta_{0}, \\beta_{1}, \\sigma, x_{\\min}, x_{\\max}, \\text{seed}, \\text{extreme\\_mode})$ and, when applicable, the extreme-configuration parameters $(x_{\\text{ext},1}, x_{\\text{ext},2}, \\sigma_{\\text{ext}}, \\Delta)$:\n- Case A (happy path with masking): $(n_{0} = 30, \\beta_{0} = 1.0, \\beta_{1} = 2.0, \\sigma = 1.0, x_{\\min} = 0.0, x_{\\max} = 10.0, \\text{seed} = 20231111, \\text{extreme\\_mode} = \\text{\"aligned\"})$ with $(x_{\\text{ext},1} = 20.0, x_{\\text{ext},2} = 20.1, \\sigma_{\\text{ext}} = 0.3, \\Delta = 0.0)$.\n- Case B (boundary without extremes): $(n_{0} = 30, \\beta_{0} = 1.0, \\beta_{1} = 2.0, \\sigma = 1.0, x_{\\min} = 0.0, x_{\\max} = 10.0, \\text{seed} = 20231112, \\text{extreme\\_mode} = \\text{\"none\"})$; no extra points are added in this case.\n- Case C (edge case with strong directional shift): $(n_{0} = 30, \\beta_{0} = 1.0, \\beta_{1} = 2.0, \\sigma = 1.0, x_{\\min} = 0.0, x_{\\max} = 10.0, \\text{seed} = 20231113, \\text{extreme\\_mode} = \\text{\"shifted\"})$ with $(x_{\\text{ext},1} = 20.0, x_{\\text{ext},2} = 20.1, \\sigma_{\\text{ext}} = 0.0, \\Delta = 8.0)$.\n\nFor each case, fit an ordinary least squares model with an intercept to the full dataset. Let $n$ denote the total number of observations in the fitted dataset (including any appended extremes), and let $p$ denote the number of regression parameters including the intercept, with $p = 2$. Let $h_{i}$ denote the leverage of observation $i$, $r_{i}$ its ordinary residual, $t_{i}$ its externally Studentized residual, $D_{i}$ its Cook’s distance, and let $\\alpha = 0.05$.\n\nDefine the following decision quantities for each case:\n- High-leverage threshold: $h_{\\text{thr}} = \\dfrac{2p}{n}$.\n- Bonferroni-corrected two-sided externally Studentized residual threshold: $t_{\\text{crit}}$ is the $(1 - \\alpha/(2n))$ quantile of the Student distribution with $n - p - 1$ degrees of freedom.\n- Masking boolean, defined as true if and only if all of the following hold simultaneously:\n  1. Exactly two appended extreme points exist in the case, and both satisfy $h_{i} > h_{\\text{thr}}$.\n  2. Both appended extreme points have $|r_{i}|$ strictly less than the median of $\\{|r_{j}|\\}_{j=1}^{n}$.\n  3. The index of $\\max_{1 \\le j \\le n} |r_{j}|$ is not one of the two extreme-point indices.\n\nCompute and report, for each case, the tuple $[\\text{masking}, \\text{count}_{\\text{high-lev}}, \\text{count}_{\\text{bonf-out}}, \\text{argmax}_{\\text{Cook}}]$, where:\n- $\\text{masking}$ is the boolean defined above.\n- $\\text{count}_{\\text{high-lev}}$ is the number of observations with $h_{i} > h_{\\text{thr}}$.\n- $\\text{count}_{\\text{bonf-out}}$ is the number of observations with $|t_{i}| > t_{\\text{crit}}$.\n- $\\text{argmax}_{\\text{Cook}}$ is the zero-based index of the observation with the largest $D_{i}$.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of these tuples enclosed in square brackets, for example, $[[\\text{maskA},\\text{cA1},\\text{cA2},\\text{iA}],[\\text{maskB},\\text{cB1},\\text{cB2},\\text{iB}],[\\text{maskC},\\text{cC1},\\text{cC2},\\text{iC}]]$. Booleans must be printed as the programming language’s native boolean literals, and counts and indices as integers. No physical units are involved. All angles, if any appear, should be treated purely as abstract mathematical quantities without units. No percentages should be printed; the probability level $\\alpha$ must be treated as the decimal value specified above.", "solution": "### Principle-Based Solution Design\n\nThe objective is to demonstrate the phenomenon of residual masking, where observations with high leverage can unduly influence a fitted regression line, causing their own residuals to be deceptively small. This analysis is conducted by simulating datasets under different conditions and computing standard ordinary least squares (OLS) regression diagnostics.\n\n#### 1. Theoretical Framework of OLS and Diagnostics\n\nThe analysis is based on a simple linear regression model with an intercept:\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $$\nwhere $\\varepsilon_i$ are independent and identically distributed errors from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. In matrix form, this is expressed as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of responses, $\\mathbf{X}$ is the $n \\times p$ model matrix (with $n$ observations and $p=2$ parameters), $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the vector of errors. For our model, the matrix $\\mathbf{X}$ is:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}\n$$\n\nThe OLS method finds the coefficient estimates $\\hat{\\boldsymbol{\\beta}}$ that minimize the residual sum of squares (RSS), $\\sum r_i^2 = \\mathbf{r}^T\\mathbf{r}$. The solution is given by:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n\nThe fitted values $\\hat{\\mathbf{y}}$ are a linear transformation of the observed values $\\mathbf{y}$:\n$$ \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\mathbf{H}\\mathbf{y} $$\nThe matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ is known as the \"hat matrix\" because it projects $\\mathbf{y}$ onto its estimate $\\hat{\\mathbf{y}}$.\n\n**Leverage**: The diagonal elements of the hat matrix, $h_i = H_{ii}$, are the leverage values. Leverage $h_i$ quantifies the influence of the observed response $y_i$ on its own fitted value $\\hat{y}_i$, since $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_i$. A high leverage value indicates that an observation has an unusual predictor value $x_i$ (i.e., far from the mean of the $x$ values) and thus has the potential to be highly influential on the regression line. A common heuristic flags observations with $h_i > 2p/n$ as having high leverage.\n\n**Ordinary Residuals**: The residuals are the differences between observed and fitted values, $r_i = y_i - \\hat{y}_i$. A plot of residuals is a primary tool for assessing model fit. However, high-leverage points can \"pull\" the regression line towards themselves, resulting in small residuals $r_i$ that mask their true discrepancy from the model that would have been fitted by the other data points.\n\n**Externally Studentized Residuals**: To address the issues with ordinary residuals (non-constant variance and masking), we use externally Studentized residuals ($t_i$). The $i$-th residual is scaled by an estimate of its standard deviation, calculated from a model fitted to the data with observation $i$ removed. This provides a more objective measure of the \"outlyingness\" of an observation. The formula is:\n$$ t_i = \\frac{r_i}{s_{(i)}\\sqrt{1-h_i}} = r_i \\sqrt{\\frac{n-p-1}{\\text{RSS}(1-h_i) - r_i^2}} $$\nwhere $s_{(i)}$ is the residual standard error from the model without observation $i$. Under the null hypothesis that observation $i$ is not an outlier, $t_i$ follows a Student's t-distribution with $n-p-1$ degrees of freedom. To test for outliers in the whole dataset, a Bonferroni correction is applied to the significance level $\\alpha$, leading to a critical value $t_{\\text{crit}}$ from the $t_{n-p-1}$ distribution at the $(1 - \\alpha/(2n))$ quantile.\n\n**Cook's Distance**: This diagnostic measures the overall influence of an observation on the full set of estimated coefficients. It combines information from both the residual and leverage of an observation. A large Cook's distance indicates that removing the observation would substantially change the model's coefficient estimates. The formula is:\n$$ D_i = \\frac{(\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)})^T (\\mathbf{X}^T\\mathbf{X}) (\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)})}{p s^2} = \\frac{r_i^2}{p s^2} \\left[ \\frac{h_i}{(1-h_i)^2} \\right] $$\nwhere $s^2 = \\text{RSS}/(n-p)$. The `argmax_Cook` identifies the single most influential point according to this metric.\n\n#### 2. Simulation and Analysis Strategy\n\nThe program will implement the above principles for three distinct test cases.\n\n-   **Data Generation**: For each case, a base dataset is generated using a `numpy` random number generator initialized with a specific seed for reproducibility. The independent variable $x$ is drawn from a uniform distribution, and the response $y$ is computed from the specified linear equation with added Gaussian noise. In Cases A and C, two additional points with extreme $x$-values are appended.\n    -   **Case A (Aligned Extremes)**: The two extreme points have $y$-values that are consistent with the underlying linear relationship. They are designed to have high leverage and pull the line strongly, but because they are \"correct\", their own residuals will be small, demonstrating the masking effect.\n    -   **Case B (No Extremes)**: Serves as a baseline, representing a standard dataset without intentionally introduced influential points.\n    -   **Case C (Shifted Extremes)**: The two extreme points have $y$-values that are systematically shifted away from the true line. These are true outliers that also have high leverage. Their residuals are expected to be large, and they will be identified as influential points without masking.\n\n-   **Computation**: A single function will process each case.\n    1.  Construct the model matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$.\n    2.  Compute $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$.\n    3.  Compute the hat matrix $\\mathbf{H}$ and extract the leverages $h_i$ from its diagonal.\n    4.  Calculate ordinary residuals $\\mathbf{r}$, externally Studentized residuals $\\mathbf{t}$, and Cook's distances $\\mathbf{D}$ using the formulas described above.\n    5.  Evaluate the decision quantities:\n        -   `masking`: A boolean value determined by applying the three specified conditions, which check for the existence of high-leverage extreme points with paradoxically small residuals.\n        -   `count_high-lev`: The number of points where $h_i > 2p/n$.\n        -   `count_bonf-out`: The number of points where $|t_i|$ exceeds the Bonferroni-corrected critical value $t_{\\text{crit}}$, obtained from `scipy.stats.t.ppf`.\n        -   `argmax_Cook`: The index of the observation with the maximum Cook's distance, found using `numpy.argmax`.\n    6.  The final results for each case are collated and formatted into the specified string output.\n\nThis principled approach ensures that the output is a direct and correct application of regression diagnostic theory to the simulated data, thereby faithfully addressing the problem statement.\n\n```python\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the analysis for all test cases.\n    \"\"\"\n    \n    # Define the test cases as per the problem statement.\n    # Format: (n0, b0, b1, sigma, x_min, x_max, seed, extreme_mode, ext_params)\n    # ext_params: (x_ext1, x_ext2, sigma_ext, delta)\n    test_cases = [\n        (30, 1.0, 2.0, 1.0, 0.0, 10.0, 20231111, \"aligned\", (20.0, 20.1, 0.3, 0.0)),\n        (30, 1.0, 2.0, 1.0, 0.0, 10.0, 20231112, \"none\", None),\n        (30, 1.0, 2.0, 1.0, 0.0, 10.0, 20231113, \"shifted\", (20.0, 20.1, 0.0, 8.0)),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = analyze_case(case_params)\n        results.append(result)\n        \n    # Format the final output string.\n    # str() on a list produces the desired '[...]' format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef analyze_case(case_params):\n    \"\"\"\n    Analyzes a single regression case to compute diagnostic statistics.\n    \"\"\"\n    n0, b0, b1, sigma, x_min, x_max, seed, extreme_mode, ext_params = case_params\n    \n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    \n    # Base dataset\n    x_base = rng.uniform(x_min, x_max, n0)\n    eps_base = rng.normal(0, sigma, n0)\n    y_base = b0 + b1 * x_base + eps_base\n    \n    x_full, y_full = x_base, y_base\n    \n    if extreme_mode != \"none\":\n        x_ext1, x_ext2, sigma_ext, delta = ext_params\n        x_extreme = np.array([x_ext1, x_ext2])\n        eps_extreme = rng.normal(0, sigma_ext, 2)\n        y_extreme = (b0 + b1 * x_extreme) + eps_extreme + delta\n        \n        x_full = np.concatenate((x_base, x_extreme))\n        y_full = np.concatenate((y_base, y_extreme))\n        \n    n = len(x_full)\n    p = 2  # number of parameters: intercept and slope\n\n    # 2. OLS Regression\n    X = np.vstack((np.ones(n), x_full)).T\n    \n    # Compute OLS estimator and related quantities\n    XTX_inv = np.linalg.inv(X.T @ X)\n    beta_hat = XTX_inv @ X.T @ y_full\n    y_hat = X @ beta_hat\n    r = y_full - y_hat\n    \n    # 3. Diagnostic Statistics Calculation\n    # Hat matrix H and leverages h_i\n    H = X @ XTX_inv @ X.T\n    h = np.diag(H)\n    \n    # Residual sum of squares (RSS) and residual standard error (s)\n    RSS = r.T @ r\n    s2 = RSS / (n - p)\n    \n    # Externally Studentized residuals t_i\n    # Formula: t_i = r_i * sqrt((n - p - 1) / (RSS * (1 - h_i) - r_i^2))\n    denom_sq = RSS * (1 - h) - r**2\n    # Clamp small negative values from floating point error to 0\n    denom_sq[denom_sq  0] = 0\n    \n    with np.errstate(divide='ignore', invalid='ignore'):\n        t = r * np.sqrt((n - p - 1) / denom_sq)\n        t[np.isnan(t)] = 0.0 # Handle case where denominator and numerator are zero\n    \n    # Cook's distance D_i\n    # Formula: D_i = (r_i^2 / (p * s2)) * (h_i / (1 - h_i)^2)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        D = (r**2 / (p * s2)) * (h / (1 - h)**2)\n        D[np.isnan(D)] = 0.0\n\n    # 4. Compute Output Quantities\n    # High-leverage count\n    h_thr = (2 * p) / n\n    count_high_lev = np.sum(h > h_thr)\n    \n    # Bonferroni-corrected outlier count\n    alpha = 0.05\n    df = n - p - 1\n    if df > 0:\n        t_crit = student_t.ppf(1 - alpha / (2 * n), df=df)\n        count_bonf_out = np.sum(np.abs(t) > t_crit)\n    else:\n        count_bonf_out = 0\n        \n    # Index of max Cook's Distance\n    argmax_cook = np.argmax(D)\n    \n    # Masking boolean evaluation\n    masking = False\n    if extreme_mode != \"none\":\n        extreme_indices = [n - 2, n - 1]\n        \n        # Condition 1: Both extreme points have high leverage\n        cond1 = np.all(h[extreme_indices] > h_thr)\n        \n        # Condition 2: Both extreme points have small absolute residuals\n        median_abs_r = np.median(np.abs(r))\n        cond2 = np.all(np.abs(r[extreme_indices])  median_abs_r)\n        \n        # Condition 3: The max absolute residual is not at an extreme point\n        idx_max_abs_r = np.argmax(np.abs(r))\n        cond3 = idx_max_abs_r not in extreme_indices\n        \n        if cond1 and cond2 and cond3:\n            masking = True\n            \n    return [bool(masking), int(count_high_lev), int(count_bonf_out), int(argmax_cook)]\n\nif __name__ == '__main__':\n    solve()\n```", "answer": "[[True, 2, 0, 15], [False, 2, 0, 16], [False, 2, 2, 31]]", "id": "4908265"}]}