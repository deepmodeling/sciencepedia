{"hands_on_practices": [{"introduction": "The ANOVA table provides a concise summary of the sources of variation in a regression model. But where do its numbers come from? This first practice takes you behind the scenes, guiding you to compute the Total, Regression, and Error Sums of Squares ($SST$, $SSR$, and $SSE$) directly from a small dataset. By performing these fundamental calculations yourself, you will gain a concrete understanding of how ANOVA partitions the total variability in an outcome variable [@problem_id:4893782].", "problem": "A biostatistician examines a simple linear regression of a continuous health outcome on a dietary exposure. The design matrix $X$ includes an intercept and a single continuous predictor $x$. For $n=5$ subjects, the predictor values and observed outcomes are\n$$\nx = \\begin{pmatrix}0 & 1 & 2 & 3 & 4\\end{pmatrix}, \\quad y = \\begin{pmatrix}2.2 & 3.5 & 5.3 & 6.7 & 8.0\\end{pmatrix}.\n$$\nAssume the usual fixed-design linear model $y = X\\beta + \\varepsilon$ with an intercept and one predictor, and fit the model by ordinary least squares (OLS). Using the analysis of variance (ANOVA) framework for regression, compute the Total Sum of Squares (SST), the Regression Sum of Squares (SSR), and the Error Sum of Squares (SSE) explicitly from this dataset, and verify numerically that the decomposition $\\,\\text{SST} = \\text{SSR} + \\text{SSE}\\,$ holds. Round your three reported sums of squares to four significant figures. Express your final answer as a row vector in the order $\\big(\\text{SST},\\,\\text{SSR},\\,\\text{SSE}\\big)$.", "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- **Model**: Simple linear regression of a continuous outcome on a dietary exposure, of the form $y = X\\beta + \\varepsilon$, with an intercept and one predictor. The model is to be fit by ordinary least squares (OLS).\n- **Sample size**: $n = 5$ subjects.\n- **Predictor values**: The vector of predictor values is $x = \\begin{pmatrix}0 & 1 & 2 & 3 & 4\\end{pmatrix}$.\n- **Outcome values**: The vector of observed outcomes is $y = \\begin{pmatrix}2.2 & 3.5 & 5.3 & 6.7 & 8.0\\end{pmatrix}$.\n- **Task**: Compute the Total Sum of Squares (SST), the Regression Sum of Squares (SSR), and the Error Sum of Squares (SSE).\n- **Verification**: Numerically verify that the decomposition $\\text{SST} = \\text{SSR} + \\text{SSE}$ holds.\n- **Output format**: Report the three sums of squares rounded to four significant figures, expressed as a row vector in the order $(\\text{SST}, \\text{SSR}, \\text{SSE})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is a standard application of analysis of variance (ANOVA) for simple linear regression, a fundamental and well-established topic in biostatistics. The model, data, and requested calculations are all standard. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly defined. It provides a specific dataset, a specific statistical model (OLS simple linear regression), and asks for the calculation of specific, uniquely defined quantities (SST, SSR, SSE). A unique solution exists and can be derived from the provided information.\n- **Objective**: The problem is stated using precise, objective mathematical and statistical language. The data are numerical, and the tasks are computational. There are no subjective or ambiguous statements.\n- The problem is **self-contained**, **consistent**, and **computationally feasible**.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will be derived by following the standard procedures for ANOVA in the context of simple linear regression.\n\n## SOLUTION\n\nThe problem requires the computation of the components of variance from a simple linear regression analysis: Total Sum of Squares (SST), Regression Sum of Squares (SSR), and Error Sum of Squares (SSE). The model is given by $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ for $i=1, \\dots, n$.\n\nFirst, we calculate the sample means of the predictor $x$ and the outcome $y$. The sample size is $n=5$.\n\nThe sum of the predictor values is:\n$$ \\sum_{i=1}^5 x_i = 0 + 1 + 2 + 3 + 4 = 10 $$\nThe mean of the predictor is:\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^5 x_i = \\frac{10}{5} = 2 $$\n\nThe sum of the outcome values is:\n$$ \\sum_{i=1}^5 y_i = 2.2 + 3.5 + 5.3 + 6.7 + 8.0 = 25.7 $$\nThe mean of the outcome is:\n$$ \\bar{y} = \\frac{1}{n} \\sum_{i=1}^5 y_i = \\frac{25.7}{5} = 5.14 $$\n\n**1. Total Sum of Squares (SST)**\nSST measures the total variability in the outcome variable $y$. It is defined as the sum of the squared deviations of each observation from the sample mean $\\bar{y}$.\n$$ \\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2 $$\nUsing the given data:\n$$ \\text{SST} = (2.2 - 5.14)^2 + (3.5 - 5.14)^2 + (5.3 - 5.14)^2 + (6.7 - 5.14)^2 + (8.0 - 5.14)^2 $$\n$$ \\text{SST} = (-2.94)^2 + (-1.64)^2 + (0.16)^2 + (1.56)^2 + (2.86)^2 $$\n$$ \\text{SST} = 8.6436 + 2.6896 + 0.0256 + 2.4336 + 8.1796 = 21.972 $$\n\n**2. Ordinary Least Squares (OLS) Estimation**\nTo calculate SSR and SSE, we must first fit the regression line $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ by OLS. The estimators for the slope ($\\hat{\\beta}_1$) and intercept ($\\hat{\\beta}_0$) are:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nFirst, we compute $S_{xx}$ and $S_{xy}$:\n$$ S_{xx} = (0-2)^2 + (1-2)^2 + (2-2)^2 + (3-2)^2 + (4-2)^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10 $$\n$$ S_{xy} = (0-2)(2.2-5.14) + (1-2)(3.5-5.14) + (2-2)(5.3-5.14) + (3-2)(6.7-5.14) + (4-2)(8.0-5.14) $$\n$$ S_{xy} = (-2)(-2.94) + (-1)(-1.64) + (0)(0.16) + (1)(1.56) + (2)(2.86) = 5.88 + 1.64 + 0 + 1.56 + 5.72 = 14.8 $$\nNow we can compute the coefficients:\n$$ \\hat{\\beta}_1 = \\frac{14.8}{10} = 1.48 $$\n$$ \\hat{\\beta}_0 = 5.14 - (1.48)(2) = 5.14 - 2.96 = 2.18 $$\nThe fitted regression equation is $\\hat{y}_i = 2.18 + 1.48 x_i$.\n\n**3. Predicted Values and Residuals**\nUsing the fitted equation, we find the predicted values ($\\hat{y}_i$) and the residuals ($e_i = y_i - \\hat{y}_i$):\n- For $x_1=0$: $\\hat{y}_1 = 2.18 + 1.48(0) = 2.18$. Residual $e_1 = 2.2 - 2.18 = 0.02$.\n- For $x_2=1$: $\\hat{y}_2 = 2.18 + 1.48(1) = 3.66$. Residual $e_2 = 3.5 - 3.66 = -0.16$.\n- For $x_3=2$: $\\hat{y}_3 = 2.18 + 1.48(2) = 5.14$. Residual $e_3 = 5.3 - 5.14 = 0.16$.\n- For $x_4=3$: $\\hat{y}_4 = 2.18 + 1.48(3) = 6.62$. Residual $e_4 = 6.7 - 6.62 = 0.08$.\n- For $x_5=4$: $\\hat{y}_5 = 2.18 + 1.48(4) = 8.10$. Residual $e_5 = 8.0 - 8.10 = -0.10$.\n\n**4. Regression Sum of Squares (SSR)**\nSSR measures the variability explained by the regression model. It is the sum of squared deviations of the predicted values from the mean of the observed values.\n$$ \\text{SSR} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 $$\nUsing the calculated values:\n$$ \\text{SSR} = (2.18 - 5.14)^2 + (3.66 - 5.14)^2 + (5.14 - 5.14)^2 + (6.62 - 5.14)^2 + (8.10 - 5.14)^2 $$\n$$ \\text{SSR} = (-2.96)^2 + (-1.48)^2 + 0^2 + (1.48)^2 + (2.96)^2 $$\n$$ \\text{SSR} = 8.7616 + 2.1904 + 0 + 2.1904 + 8.7616 = 21.904 $$\n\n**5. Error Sum of Squares (SSE)**\nSSE measures the variability not explained by the model (the residual variability). It is the sum of squared residuals.\n$$ \\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n e_i^2 $$\nUsing the calculated residuals:\n$$ \\text{SSE} = (0.02)^2 + (-0.16)^2 + (0.16)^2 + (0.08)^2 + (-0.10)^2 $$\n$$ \\text{SSE} = 0.0004 + 0.0256 + 0.0256 + 0.0064 + 0.0100 = 0.068 $$\n\n**6. Verification of the Decomposition**\nThe fundamental identity of ANOVA for regression is $\\text{SST} = \\text{SSR} + \\text{SSE}$. We verify this numerically with our computed values:\n$$ \\text{SSR} + \\text{SSE} = 21.904 + 0.068 = 21.972 $$\nThis value is identical to our computed SST, so the identity $\\text{SST} = \\text{SSR} + \\text{SSE}$ is confirmed.\n\n**7. Final Answer Formulation**\nThe problem requires the values of SST, SSR, and SSE rounded to four significant figures.\n- $\\text{SST} = 21.972 \\approx 21.97$\n- $\\text{SSR} = 21.904 \\approx 21.90$\n- $\\text{SSE} = 0.06800$\n\nThe final answer is presented as a row vector $(\\text{SST}, \\text{SSR}, \\text{SSE})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n21.97 & 21.90 & 0.06800\n\\end{pmatrix}\n}\n$$", "id": "4893782"}, {"introduction": "In simple linear regression, you can test the significance of the predictor using either a t-test for the slope coefficient ($\\beta_1$) or an F-test from the ANOVA table. While they arise from different calculations, these tests are not independent. This exercise will guide you to calculate both the t-statistic and the F-statistic for the same dataset, allowing you to empirically verify the fundamental relationship that exists between them in the context of a single predictor [@problem_id:1895391].", "problem": "An undergraduate student in chemical engineering is investigating the relationship between the concentration of a novel catalyst and the rate of a chemical reaction. The student performs the reaction five times, each with a different catalyst concentration, and measures the resulting reaction rate. The collected data are as follows:\n\n-   Catalyst Concentration, $x$ (in mol/L): `{1.0, 2.0, 3.0, 4.0, 5.0}`\n-   Reaction Rate, $y$ (in mol/L/s): `{2.5, 4.0, 4.8, 6.0, 7.5}`\n\nThe student proposes a simple linear regression model of the form $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, where the errors $\\epsilon_i$ are assumed to be independent and normally distributed with mean 0 and constant variance $\\sigma^2$.\n\nYour task is to perform two key statistical tests on this model.\nFirst, calculate the t-statistic for testing the null hypothesis $H_0: \\beta_1 = 0$ against the two-sided alternative $H_a: \\beta_1 \\neq 0$.\nSecond, construct an Analysis of Variance (ANOVA) table for the regression and calculate the F-statistic.\n\nProvide the values of the t-statistic and the F-statistic. Report your answers, rounded to four significant figures, as a row matrix in the format `[t-statistic, F-statistic]`.", "solution": "We adopt the simple linear regression model $Y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i}$ with independent, normally distributed errors of constant variance. For simple linear regression with intercept, the least-squares slope estimator and its standard error are\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}},\\quad S_{xx}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2},\\quad S_{xy}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y}),\n$$\nand\n$$\n\\operatorname{SE}(\\hat{\\beta}_{1})=\\sqrt{\\frac{\\operatorname{MSE}}{S_{xx}}},\\quad \\operatorname{MSE}=\\frac{\\operatorname{SSE}}{n-2},\\quad \\operatorname{SSE}=S_{yy}-\\frac{S_{xy}^{2}}{S_{xx}},\\quad S_{yy}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}.\n$$\nThe $t$-statistic for testing $H_{0}:\\beta_{1}=0$ against $H_{a}:\\beta_{1}\\neq 0$ is\n$$\nt=\\frac{\\hat{\\beta}_{1}}{\\operatorname{SE}(\\hat{\\beta}_{1})}.\n$$\n\nUsing the data $x=\\{1,2,3,4,5\\}$ and $y=\\{2.5,4.0,4.8,6.0,7.5\\}$ with $n=5$, compute the sample means $\\bar{x}=3$ and $\\bar{y}=\\frac{2.5+4.0+4.8+6.0+7.5}{5}=4.96$. Then\n$$\nS_{xx}=\\sum(x_{i}-\\bar{x})^{2}=10,\\quad S_{xy}=\\sum(x_{i}-\\bar{x})(y_{i}-\\bar{y})=12.\n$$\nHence the slope estimate is\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}}=\\frac{12}{10}=1.2.\n$$\nNext, compute\n$$\nS_{yy}=\\sum(y_{i}-\\bar{y})^{2}=\\left(\\sum y_{i}^{2}\\right)-n\\bar{y}^{2}=(6.25+16+23.04+36+56.25)-5(4.96)^{2}=137.54-123.008=14.532.\n$$\nTherefore,\n$$\n\\operatorname{SSE}=S_{yy}-\\frac{S_{xy}^{2}}{S_{xx}}=14.532-\\frac{144}{10}=14.532-14.4=0.132,\\quad \\operatorname{MSE}=\\frac{\\operatorname{SSE}}{n-2}=\\frac{0.132}{3}=0.044.\n$$\nThe standard error of the slope is\n$$\n\\operatorname{SE}(\\hat{\\beta}_{1})=\\sqrt{\\frac{\\operatorname{MSE}}{S_{xx}}}=\\sqrt{\\frac{0.044}{10}}=\\sqrt{0.0044}=\\sqrt{\\frac{11}{2500}}=\\frac{\\sqrt{11}}{50}.\n$$\nThus the $t$-statistic is\n$$\nt=\\frac{\\hat{\\beta}_{1}}{\\operatorname{SE}(\\hat{\\beta}_{1})}=\\frac{1.2}{\\sqrt{11}/50}=\\frac{60}{\\sqrt{11}}\\approx 18.09\\quad(\\text{with }3\\text{ degrees of freedom}).\n$$\n\nFor the ANOVA, the regression sum of squares is\n$$\n\\operatorname{SSR}=\\frac{S_{xy}^{2}}{S_{xx}}=\\frac{144}{10}=14.4,\n$$\nwith $1$ degree of freedom, and the error sum of squares is $\\operatorname{SSE}=0.132$ with $n-2=3$ degrees of freedom. The mean squares are\n$$\n\\operatorname{MSR}=\\frac{\\operatorname{SSR}}{1}=14.4,\\quad \\operatorname{MSE}=\\frac{\\operatorname{SSE}}{3}=0.044.\n$$\nThe $F$-statistic for testing the overall regression is\n$$\nF=\\frac{\\operatorname{MSR}}{\\operatorname{MSE}}=\\frac{14.4}{0.044}=\\frac{3600}{11}\\approx 327.3.\n$$\nIn simple linear regression, $F=t^{2}$ holds, and indeed $\\left(\\frac{60}{\\sqrt{11}}\\right)^{2}=\\frac{3600}{11}$.\n\nRounded to four significant figures, the requested statistics are $t\\approx 18.09$ and $F\\approx 327.3$.", "answer": "$$\\boxed{\\begin{bmatrix}18.09 & 327.3\\end{bmatrix}}$$", "id": "1895391"}, {"introduction": "Real-world data rarely fits a simple model perfectly on the first try. This practice demonstrates how ANOVA serves as a critical tool in the iterative process of model building and assessment. You will analyze a scenario where an initial model is inadequate and a more complex model is proposed, using the F-statistic to evaluate whether the changes—a data transformation and the addition of a quadratic term—lead to a significantly better explanation of the data's variability [@problem_id:4893869].", "problem": "A biostatistics team studies the relationship between a continuous dose variable $x$ and a nonnegative response $y$ representing a count of enzyme aggregates per field in histological images for $n=20$ animals. An initial simple linear regression model $y_i=\\beta_0+\\beta_1 x_i+\\varepsilon_i$ is fit. The following summary quantities are obtained for this initial fit:\n- Total sum of squares about the mean: $\\sum_{i=1}^{n}(y_i-\\bar{y})^2 = 520$.\n- Regression sum of squares: $\\sum_{i=1}^{n}(\\hat{y}_i-\\bar{y})^2 = 120$.\n- Error sum of squares: $\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 = 400$.\n\nDiagnostic plots show pronounced curvature and increasing variance with the fitted mean, suggesting violations of linearity and homoscedasticity. Motivated by approximate variance stabilization for count-like responses, the team applies the square-root transformation $z_i=\\sqrt{y_i}$ and extends the mean function to a quadratic in $x$:\n$$\nz_i=\\gamma_0+\\gamma_1 x_i+\\gamma_2 x_i^2+\\varepsilon_i',\n$$\nwhere $\\varepsilon_i'$ are independent errors with constant variance on the transformed scale.\n\nFor the transformed quadratic model, the team reports:\n- Total sum of squares about the transformed mean: $\\sum_{i=1}^{n}(z_i-\\bar{z})^2 = 49.98$.\n- Sequential regression sums of squares: for adding $x$ to the intercept-only model, $14.04$; for adding $x^2$ given $x$ is already in the model, $22.68$. Thus the total regression sum of squares is $36.72$.\n- Error sum of squares: $\\sum_{i=1}^{n}(z_i-\\hat{z}_i)^2 = 13.26$.\n\nAssume the standard conditions for the analysis of variance for regression apply on each scale: in particular, independent errors with constant variance and normality on the relevant scale. Using the core definitions of regression sum of squares, error sum of squares, mean squares, and the $F$-statistic for testing that all slope coefficients are zero, do the following:\n- Identify the degrees of freedom for regression, error, and total in both the initial linear model and the transformed quadratic model, and state the corresponding analysis of variance decompositions verbally (components and their degrees of freedom).\n- Using the transformed quadratic model, compute the $F$-statistic for testing $H_0:\\gamma_1=\\gamma_2=0$ versus the general alternative.\n\nRound your final numerical answer to four significant figures. No units are required.", "solution": "The problem requires an analysis of variance (ANOVA) for two different regression models. The first part of the task is to identify the degrees of freedom ($df$) for each model and state the corresponding ANOVA decompositions. The second part is to compute the overall $F$-statistic for the second model.\n\nA general linear regression model with a sample size of $n$ and $p$ predictor variables relates a response variable to the predictors. The total variability in the response is partitioned into variability explained by the model and unexplained variability (error). The degrees of freedom are partitioned analogously.\n- The total degrees of freedom are $df_{Total} = n - 1$.\n- The regression degrees of freedom, associated with the predictors, are $df_{Reg} = p$.\n- The error (or residual) degrees of freedom are $df_{Error} = n - p - 1$.\nIt is a fundamental identity that $df_{Total} = df_{Reg} + df_{Error}$.\n\n**Analysis of the Initial Simple Linear Model**\nThe first model is the simple linear regression $y_i=\\beta_0+\\beta_1 x_i+\\varepsilon_i$.\nThe sample size is given as $n=20$.\nThis model has one predictor variable, $x_i$. Therefore, the number of predictors is $p=1$.\nWe can now calculate the degrees of freedom for this model:\n- $df_{Total} = n - 1 = 20 - 1 = 19$.\n- $df_{Reg} = p = 1$.\n- $df_{Error} = n - p - 1 = 20 - 1 - 1 = 18$.\nThe verbal statement of the ANOVA decomposition for this model is as follows: The total sum of squares ($SSTO = \\sum_{i=1}^{n}(y_i-\\bar{y})^2 = 520$) with $19$ degrees of freedom is partitioned into the regression sum of squares ($SSR = \\sum_{i=1}^{n}(\\hat{y}_i-\\bar{y})^2 = 120$) with $1$ degree of freedom, and the error sum of squares ($SSE = \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 = 400$) with $18$ degrees of freedom.\n\n**Analysis of the Transformed Quadratic Model**\nThe second model is the quadratic regression on the transformed response: $z_i=\\gamma_0+\\gamma_1 x_i+\\gamma_2 x_i^2+\\varepsilon_i'$.\nThe sample size remains $n=20$.\nThis model includes two predictor terms, $x_i$ and $x_i^2$. Therefore, the number of predictors is $p=2$.\nThe degrees of freedom for this model (denoted with a prime to distinguish them) are:\n- $df'_{Total} = n - 1 = 20 - 1 = 19$.\n- $df'_{Reg} = p = 2$.\n- $df'_{Error} = n - p - 1 = 20 - 2 - 1 = 17$.\nThe verbal statement of the ANOVA decomposition for this model is: The total sum of squares on the transformed scale ($SSTO' = \\sum_{i=1}^{n}(z_i-\\bar{z})^2 = 49.98$) with $19$ degrees of freedom is partitioned into the regression sum of squares ($SSR' = 36.72$) with $2$ degrees of freedom, and the error sum of squares ($SSE' = \\sum_{i=1}^{n}(z_i-\\hat{z}_i)^2 = 13.26$) with $17$ degrees of freedom.\n\n**Calculation of the F-statistic for the Transformed Model**\nThe task is to compute the $F$-statistic for testing the null hypothesis $H_0: \\gamma_1 = \\gamma_2 = 0$. This is the overall $F$-test for the significance of the regression model. The $F$-statistic is defined as the ratio of the Mean Square for Regression ($MSR'$) to the Mean Square for Error ($MSE'$).\n\nFirst, we calculate the Mean Square for Regression:\n$$\nMSR' = \\frac{SSR'}{df'_{Reg}}\n$$\nUsing the given sum of squares $SSR' = 36.72$ and the calculated degrees of freedom $df'_{Reg} = 2$:\n$$\nMSR' = \\frac{36.72}{2} = 18.36\n$$\n\nNext, we calculate the Mean Square for Error:\n$$\nMSE' = \\frac{SSE'}{df'_{Error}}\n$$\nUsing the given sum of squares $SSE' = 13.26$ and the calculated degrees of freedom $df'_{Error} = 17$:\n$$\nMSE' = \\frac{13.26}{17}\n$$\n\nFinally, the $F$-statistic is the ratio of these two mean squares:\n$$\nF = \\frac{MSR'}{MSE'} = \\frac{18.36}{\\frac{13.26}{17}} = \\frac{18.36 \\times 17}{13.26} = \\frac{312.12}{13.26} \\approx 23.5384615\n$$\nThe problem requires rounding the final answer to four significant figures. This yields $23.54$.", "answer": "$$\\boxed{23.54}$$", "id": "4893869"}]}