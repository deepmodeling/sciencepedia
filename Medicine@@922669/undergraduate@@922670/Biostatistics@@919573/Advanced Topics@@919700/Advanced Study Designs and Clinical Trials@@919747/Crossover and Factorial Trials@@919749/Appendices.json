{"hands_on_practices": [{"introduction": "This first practice explores the elegant efficiency of the $2 \\times 2$ factorial design. By deriving the sample size required to detect an interaction, you will gain insight into how these trials simultaneously test multiple hypotheses with the same precision as a simple two-group experiment. This exercise grounds the abstract concept of statistical power in the concrete context of trial planning [@problem_id:4907254].", "problem": "A biostatistics research team plans a balanced two-by-two factorial clinical trial to study two binary treatments, denoted by $A$ and $B$. Each participant is randomized to one of the four cells $\\{(A+,B+),(A+,B-),(A-,B+),(A-,B-)\\}$ with equal allocation. Let the outcome $Y$ be continuous. Assume the following linear model with homoscedastic, independent, normally distributed errors and orthogonal treatment coding:\n$$\nY \\;=\\; \\mu \\;+\\; \\beta_{1} x_{A} \\;+\\; \\beta_{2} x_{B} \\;+\\; \\beta_{12}\\, x_{A} x_{B} \\;+\\; \\varepsilon,\n$$\nwhere $x_{A},x_{B}\\in\\{-1,+1\\}$ denote the coded treatment indicators, $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$, and the total sample size is $N$ with $N/4$ participants per cell.\n\nSuppose the scientific aim is to detect the interaction between $A$ and $B$ at two-sided significance level $\\alpha$ with power $1-\\beta$ when the true interaction parameter equals $\\beta_{12}\\neq 0$. Starting from first principles of the normal linear model and large-sample Wald testing, derive the minimal total sample size $N$ required to achieve this power for the two-sided test of $H_{0}:\\beta_{12}=0$ versus $H_{1}:\\beta_{12}\\neq 0$. Express your final answer as a closed-form analytic expression in terms of $\\alpha$, $\\beta$, $\\sigma$, and $|\\beta_{12}|$. No rounding is required and no physical units are needed.\n\nIn addition, in your solution, explain the power trade-offs for detecting main effects ($\\beta_{1}$ or $\\beta_{2}$) versus the interaction ($\\beta_{12}$) under this balanced factorial design.", "solution": "The problem specifies a balanced $2 \\times 2$ factorial design with a total sample size of $N$. There are four treatment cells corresponding to the combinations of treatments $A$ and $B$, coded as $x_A, x_B \\in \\{-1, +1\\}$. The four cells are $(x_A, x_B) \\in \\{(+1, +1), (+1, -1), (-1, +1), (-1, -1)\\}$, each containing $N/4$ participants.\n\nThe specified linear model is:\n$$\nY = \\mu + \\beta_{1} x_{A} + \\beta_{2} x_{B} + \\beta_{12}\\, x_{A} x_{B} + \\varepsilon\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. In matrix form, this is $\\mathbf{Y} = X \\mathbf{b} + \\mathbf{\\varepsilon}$, where $\\mathbf{b} = (\\mu, \\beta_1, \\beta_2, \\beta_{12})^T$. The design matrix $X$ is an $N \\times 4$ matrix whose columns correspond to the intercept, $x_A$, $x_B$, and the interaction term $x_A x_B$.\n\nLet the columns of $X$ be $\\mathbf{c}_0$, $\\mathbf{c}_1$, $\\mathbf{c}_2$, $\\mathbf{c}_3$. Due to the balanced design with $N/4$ subjects per cell, we can compute the matrix product $X^T X$. The $(i, j)$-th element of $X^T X$ is the dot product $\\mathbf{c}_i^T \\mathbf{c}_j$. The design uses orthogonal coding, which we verify:\n- For the diagonal elements:\n  - $\\mathbf{c}_0^T \\mathbf{c}_0 = \\sum_{k=1}^N 1^2 = N$.\n  - $\\mathbf{c}_1^T \\mathbf{c}_1 = \\sum_{k=1}^N x_{kA}^2 = \\sum_{k=1}^N (\\pm 1)^2 = N$.\n  - $\\mathbf{c}_2^T \\mathbf{c}_2 = \\sum_{k=1}^N x_{kB}^2 = \\sum_{k=1}^N (\\pm 1)^2 = N$.\n  - $\\mathbf{c}_3^T \\mathbf{c}_3 = \\sum_{k=1}^N (x_{kA}x_{kB})^2 = \\sum_{k=1}^N (\\pm 1)^2 = N$.\n- For the off-diagonal elements (exploiting the balanced allocation, with $N/4$ subjects per unique $(x_A, x_B)$ pair):\n  - $\\mathbf{c}_0^T \\mathbf{c}_1 = \\sum x_{kA} = \\frac{N}{4}(+1) + \\frac{N}{4}(+1) + \\frac{N}{4}(-1) + \\frac{N}{4}(-1) = 0$.\n  - $\\mathbf{c}_1^T \\mathbf{c}_2 = \\sum x_{kA}x_{kB} = \\frac{N}{4}(+1)(+1) + \\frac{N}{4}(+1)(-1) + \\frac{N}{4}(-1)(+1) + \\frac{N}{4}(-1)(-1) = 0$.\n  - All other dot products of distinct columns are similarly zero. For example, $\\mathbf{c}_1^T \\mathbf{c}_3 = \\sum x_{kA}(x_{kA}x_{kB}) = \\sum x_{kA}^2 x_{kB} = \\sum x_{kB} = 0$.\n\nThus, the $X^T X$ matrix is diagonal:\n$$\nX^T X = \\begin{pmatrix} N & 0 & 0 & 0 \\\\ 0 & N & 0 & 0 \\\\ 0 & 0 & N & 0 \\\\ 0 & 0 & 0 & N \\end{pmatrix} = N I_4\n$$\nwhere $I_4$ is the $4 \\times 4$ identity matrix.\n\nThe Ordinary Least Squares (OLS) estimator for the parameter vector $\\mathbf{b}$ is $\\hat{\\mathbf{b}} = (X^T X)^{-1} X^T \\mathbf{Y}$. The variance-covariance matrix of this estimator is given by $\\operatorname{Var}(\\hat{\\mathbf{b}}) = \\sigma^2 (X^T X)^{-1}$.\nUsing our result for $X^T X$, its inverse is $(X^T X)^{-1} = \\frac{1}{N} I_4$.\nTherefore, the variance-covariance matrix of the estimators is:\n$$\n\\operatorname{Var}(\\hat{\\mathbf{b}}) = \\sigma^2 \\left(\\frac{1}{N} I_4\\right) = \\frac{\\sigma^2}{N} I_4\n$$\nThis diagonal structure implies that the estimators $\\hat{\\mu}$, $\\hat{\\beta}_1$, $\\hat{\\beta}_2$, and $\\hat{\\beta}_{12}$ are uncorrelated (and, due to normality, independent). The variance of the specific estimator for the interaction term, $\\hat{\\beta}_{12}$, is the fourth diagonal element of this matrix:\n$$\n\\operatorname{Var}(\\hat{\\beta}_{12}) = \\frac{\\sigma^2}{N}\n$$\nThe standard error is $\\operatorname{SE}(\\hat{\\beta}_{12}) = \\sqrt{\\frac{\\sigma^2}{N}} = \\frac{\\sigma}{\\sqrt{N}}$.\nSince the outcome $Y$ is normally distributed, the OLS estimator $\\hat{\\beta}_{12}$ is also normally distributed:\n$$\n\\hat{\\beta}_{12} \\sim \\mathcal{N}\\left(\\beta_{12}, \\frac{\\sigma^2}{N}\\right)\n$$\nThe scientific objective is to test the null hypothesis $H_0: \\beta_{12} = 0$ against the alternative $H_1: \\beta_{12} \\neq 0$. The large-sample Wald test statistic is:\n$$\nZ = \\frac{\\hat{\\beta}_{12} - 0}{\\operatorname{SE}(\\hat{\\beta}_{12})} = \\frac{\\hat{\\beta}_{12}}{\\sigma/\\sqrt{N}}\n$$\nUnder $H_0$, where $\\beta_{12}=0$, the statistic $Z$ follows a standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$. We reject $H_0$ at a two-sided significance level $\\alpha$ if $|Z| > z_{1-\\alpha/2}$, where $z_{q}$ is the $q$-th quantile of the standard normal distribution.\n\nUnder $H_1$, the true value of the interaction parameter is some non-zero value $\\beta_{12}$. The test statistic $Z$ is then distributed as:\n$$\nZ = \\frac{\\hat{\\beta}_{12}}{\\sigma/\\sqrt{N}} \\sim \\mathcal{N}\\left(\\frac{\\beta_{12}}{\\sigma/\\sqrt{N}}, 1\\right)\n$$\nThe power of the test, $1-\\beta$, is the probability of correctly rejecting $H_0$ when $H_1$ is true.\n$$\n1-\\beta = P\\left(|Z| > z_{1-\\alpha/2} \\mid \\beta_{12} \\neq 0\\right) = P\\left(Z > z_{1-\\alpha/2}\\right) + P\\left(Z  -z_{1-\\alpha/2}\\right)\n$$\nLet $\\delta = \\frac{\\beta_{12}\\sqrt{N}}{\\sigma}$ be the non-centrality parameter. The distribution of $Z$ is $\\mathcal{N}(\\delta, 1)$. Let $Z' \\sim \\mathcal{N}(0, 1)$. The power is:\n$$\n1-\\beta = P(Z' > z_{1-\\alpha/2} - \\delta) + P(Z'  -z_{1-\\alpha/2} - \\delta)\n$$\nFor a reasonably powered study, the non-centrality parameter $\\delta$ has the same sign as $\\beta_{12}$. If $\\beta_{12}>0$, then $\\delta>0$, and the term $P(Z'  -z_{1-\\alpha/2} - \\delta)$ becomes negligible. The power is dominated by the first term.\n$$\n1-\\beta \\approx P(Z' > z_{1-\\alpha/2} - \\delta)\n$$\nThis is equivalent to stating that the argument of the probability must satisfy $z_{1-\\alpha/2} - \\delta = z_{\\beta} = -z_{1-\\beta}$.\n$$\n\\delta \\approx z_{1-\\alpha/2} + z_{1-\\beta}\n$$\nIf $\\beta_{12}0$, then $\\delta0$, and the second term $P(Z'  -z_{1-\\alpha/2} - \\delta)$ dominates, leading to the condition $-z_{1-\\alpha/2} - \\delta = z_{1-\\beta}$, which gives $-\\delta \\approx z_{1-\\alpha/2} + z_{1-\\beta}$. Both cases are covered by using the absolute value:\n$$\n|\\delta| = \\frac{|\\beta_{12}|\\sqrt{N}}{\\sigma} \\approx z_{1-\\alpha/2} + z_{1-\\beta}\n$$\nSolving for the minimal total sample size $N$:\n$$\n\\sqrt{N} = \\frac{\\sigma}{|\\beta_{12}|} (z_{1-\\alpha/2} + z_{1-\\beta})\n$$\n$$\nN = \\frac{\\sigma^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\beta_{12}^2}\n$$\nThis is the minimal required total sample size to detect an interaction effect of magnitude $|\\beta_{12}|$ with power $1-\\beta$ at significance level $\\alpha$.\n\nRegarding the power trade-offs, we must examine the precision with which the main effects, $\\beta_1$ and $\\beta_2$, are estimated. From our earlier derivation of the variance-covariance matrix $\\operatorname{Var}(\\hat{\\mathbf{b}}) = \\frac{\\sigma^2}{N} I_4$, we find that the variances of the estimators for the main effects are:\n$$\n\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{N} \\quad \\text{and} \\quad \\operatorname{Var}(\\hat{\\beta}_2) = \\frac{\\sigma^2}{N}\n$$\nThese variances are identical to the variance of the interaction estimator, $\\operatorname{Var}(\\hat{\\beta}_{12})$. This is a remarkable property of a balanced $2 \\times 2$ factorial design with orthogonal coding. It implies that for a given effect magnitude $|\\theta|$, the statistical power to detect a main effect $\\beta_1=\\theta$, a main effect $\\beta_2=\\theta$, or an interaction effect $\\beta_{12}=\\theta$ is exactly the same. The sample size required to detect a main effect $\\beta_1$ would be $N = \\sigma^2(z_{1-\\alpha/2} + z_{1-\\beta})^2 / \\beta_1^2$, which has the same form as the formula for the interaction.\n\nTherefore, under this design, there is no \"trade-off\" in power between detecting main effects versus the interaction in the sense that improving power for one comes at the expense of the other. On the contrary, the factorial design is exceptionally efficient. It allows for the estimation of two main effects and one interaction effect with the same precision (and thus power, for a given effect size) that would be achieved for a single effect in a two-group experiment using the same total sample size $N$. Effectively, the entire sample size $N$ contributes to the estimation of each effect. This simultaneous, high-powered estimation of multiple effects is the primary advantage of factorial designs.", "answer": "$$\n\\boxed{\\frac{\\sigma^2 \\left(z_{1-\\alpha/2} + z_{1-\\beta}\\right)^2}{\\beta_{12}^2}}\n$$", "id": "4907254"}, {"introduction": "Shifting our focus to crossover trials, this exercise delves into the heart of their statistical efficiency: the within-subject comparison. You will derive a sample size formula for a typical bioequivalence study, linking the required number of participants directly to the within-subject coefficient of variation. This practice highlights how harnessing each participant as their own control leads to powerful and efficient study designs [@problem_id:4907228].", "problem": "A pharmacokinetic bioequivalence study is planned as a balanced $2 \\times 2$ randomized crossover trial comparing two treatments $A$ and $B$. For each subject $i$, let $X_{iA}$ and $X_{iB}$ denote the Area Under the Curve (AUC), and let $Y_{iA} = \\ln(X_{iA})$ and $Y_{iB} = \\ln(X_{iB})$ denote the natural logarithms of AUC. Assume the following scientifically standard model and design features:\n- Within each subject, the log-transformed AUC in a period is $Y \\sim \\mathcal{N}(\\mu, s_{w}^{2})$ with within-subject variance $s_{w}^{2}$ common to both treatments and periods, and period-specific errors are independent across periods.\n- The within-subject coefficient of variation (CV) on the original AUC scale is defined by $CV_{w} = \\frac{\\sqrt{\\operatorname{Var}(X)}}{\\operatorname{E}(X)}$, where $X$ denotes AUC within a period.\n- The sample mean difference in log AUC across $n$ subjects is $\\bar{D} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(Y_{iA} - Y_{iB}\\right)$, and inference will be based on a large-sample normal approximation for $\\bar{D}$.\n\nUsing only fundamental definitions of mean, variance, and coefficient of variation, along with well-tested moment formulas for the lognormal distribution, perform the following:\n\n1. Derive an expression that links $CV_{w}$ to $s_{w}^{2}$, and then derive the standard error of the sample mean difference $\\bar{D}$ in terms of $CV_{w}$ and $n$ under the stated assumptions.\n\n2. Suppose the design targets a two-sided type I error rate $\\alpha = 0.05$ and power $1-\\beta = 0.90$ to detect a true mean difference on the log scale of $\\delta = 0.15$. Using a $z$-test based on $\\bar{D}$, derive the closed-form expression for the required total sample size $n$ in terms of $CV_{w}$, $\\alpha$, $\\beta$, and $\\delta$. For quantiles, use $z_{q} = \\Phi^{-1}(q)$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n\nFor the computation in part 2, take $CV_{w} = 0.30$, $\\alpha = 0.05$, $1-\\beta = 0.90$, and $\\delta = 0.15$. Express your final answer as a single closed-form analytic expression for $n$. No numerical rounding is required or permitted in the final answer.", "solution": "### Part 1: Derivation of Relationships\n\n**Relationship between $CV_{w}$ and $s_{w}^{2}$**\n\nThe problem states that the log-transformed AUC, $Y = \\ln(X)$, is normally distributed with variance $s_{w}^{2}$, i.e., $Y \\sim \\mathcal{N}(\\mu, s_{w}^{2})$ for some mean $\\mu$. This implies that the original AUC, $X = \\exp(Y)$, follows a lognormal distribution. We use the standard formulas for the moments of a lognormal distribution.\n\nThe expected value of $X$ is:\n$$ \\operatorname{E}(X) = \\exp\\left(\\mu + \\frac{1}{2}s_{w}^{2}\\right) $$\nThe variance of $X$ is:\n$$ \\operatorname{Var}(X) = \\left[\\exp(s_{w}^{2}) - 1\\right] \\exp\\left(2\\mu + s_{w}^{2}\\right) = \\left[\\exp(s_{w}^{2}) - 1\\right] (\\operatorname{E}(X))^2 $$\nThe square of the coefficient of variation, $CV_{w}$, is defined as:\n$$ CV_{w}^{2} = \\frac{\\operatorname{Var}(X)}{(\\operatorname{E}(X))^2} $$\nSubstituting the expression for the variance of a lognormal distribution:\n$$ CV_{w}^{2} = \\frac{\\left[\\exp(s_{w}^{2}) - 1\\right] (\\operatorname{E}(X))^2}{(\\operatorname{E}(X))^2} = \\exp(s_{w}^{2}) - 1 $$\nThis is the desired relationship. We can also express $s_w^2$ in terms of $CV_w$:\n$$ s_{w}^{2} = \\ln(CV_{w}^{2} + 1) $$\n\n**Standard Error of the Sample Mean Difference $\\bar{D}$**\n\nThe estimator is $\\bar{D} = \\frac{1}{n}\\sum_{i=1}^{n} D_i$, where $D_i = Y_{iA} - Y_{iB}$ is the within-subject difference in log-AUC. As subjects are independent, the variance of the sample mean $\\bar{D}$ is $\\operatorname{Var}(\\bar{D}) = \\frac{\\operatorname{Var}(D_i)}{n}$.\n\nIn a crossover trial, the two measurements on the same subject, $Y_{iA}$ and $Y_{iB}$, are correlated because they share a common subject-specific effect. Let the model for an observation be $Y = \\mu_{\\text{treatment}} + \\text{subject_effect} + \\text{error}$, where the error term has variance $s_w^2$. The variance of the within-subject difference $D_i$ is:\n$$ \\operatorname{Var}(D_i) = \\operatorname{Var}(Y_{iA} - Y_{iB}) = \\operatorname{Var}(Y_{iA}) + \\operatorname{Var}(Y_{iB}) - 2\\operatorname{Cov}(Y_{iA}, Y_{iB}) $$\nThe total variance of an observation is the sum of between-subject variance ($s_b^2$) and within-subject variance ($s_w^2$), so $\\operatorname{Var}(Y_{iA}) = \\operatorname{Var}(Y_{iB}) = s_b^2 + s_w^2$. The covariance between the two measurements is the between-subject variance, $\\operatorname{Cov}(Y_{iA}, Y_{iB}) = s_b^2$. Substituting these gives:\n$$ \\operatorname{Var}(D_i) = (s_b^2 + s_w^2) + (s_b^2 + s_w^2) - 2(s_b^2) = 2s_w^2 $$\nThe between-subject variance cancels, illustrating the efficiency of the crossover design. The variance of $\\bar{D}$ is:\n$$ \\operatorname{Var}(\\bar{D}) = \\frac{2s_w^2}{n} $$\nThe standard error of $\\bar{D}$ is the square root of its variance:\n$$ \\operatorname{SE}(\\bar{D}) = \\sqrt{\\frac{2s_w^2}{n}} $$\nSubstituting the expression for $s_w^2$ in terms of $CV_w$:\n$$ \\operatorname{SE}(\\bar{D}) = \\sqrt{\\frac{2\\ln(CV_{w}^{2} + 1)}{n}} $$\n\n### Part 2: Derivation of Sample Size Formula\n\nThe hypothesis test for the mean difference is $H_0: \\mu_A - \\mu_B = 0$ against $H_1: \\mu_A - \\mu_B \\neq 0$. We use a two-sided $z$-test based on the statistic $Z = \\frac{\\bar{D}}{\\operatorname{SE}(\\bar{D})}$. We reject $H_0$ if $|Z| > z_{1-\\alpha/2}$.\n\nThe standard formula for sample size in a two-sided test relates the effect size, power, and significance level:\n$$ \\frac{|\\delta|}{\\operatorname{SE}(\\bar{D})} = z_{1-\\alpha/2} + z_{1-\\beta} $$\nwhere $\\delta$ is the true mean difference we wish to detect, $1-\\beta$ is the power, and $\\alpha$ is the significance level.\n\nSquaring both sides and substituting $\\operatorname{SE}(\\bar{D})^2 = \\frac{2s_w^2}{n}$:\n$$ \\frac{\\delta^2}{2s_w^2/n} = (z_{1-\\alpha/2} + z_{1-\\beta})^2 $$\nSolving for the total sample size $n$:\n$$ n = \\frac{2s_w^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nFinally, we substitute $s_w^2 = \\ln(1 + CV_w^2)$:\n$$ n = \\frac{2\\ln(1 + CV_w^2) (z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nThe problem requires the final answer as a closed-form analytic expression using the provided values: $CV_w=0.30$, $\\alpha=0.05$ (so $1-\\alpha/2 = 0.975$), $1-\\beta=0.90$ (so $1-\\beta = 0.90$), and $\\delta=0.15$. The quantile notation is $z_q = \\Phi^{-1}(q)$.\nSubstituting these values gives the final expression:\n$$ n = \\frac{2\\ln(1 + (0.30)^2) (\\Phi^{-1}(0.975) + \\Phi^{-1}(0.90))^2}{(0.15)^2} $$", "answer": "$$ \\boxed{\\frac{2 \\ln(1 + (0.30)^2) (\\Phi^{-1}(0.975) + \\Phi^{-1}(0.90))^2}{(0.15)^2}} $$", "id": "4907228"}, {"introduction": "Our final practice moves from ideal design to a common real-world challenge: missing data. This problem demonstrates how differential dropout between sequences in a crossover trial can introduce bias, confounding the treatment effect with period effects. By applying an Inverse Probability Weighting (IPW) scheme, you will learn a powerful method for correcting this bias and recovering a valid estimate of the treatment effect [@problem_id:4907236].", "problem": "A two-period, two-sequence crossover design assigns participants to either sequence $AB$ (treatment $A$ in period $1$, treatment $B$ in period $2$) or sequence $BA$ (treatment $B$ in period $1$, treatment $A$ in period $2$). Assume no carryover effects and an additive period effect, so each observed outcome satisfies the model $Y_{it} = \\mu_{d_{it}} + \\pi_t + \\varepsilon_{it}$ where $d_{it} \\in \\{A,B\\}$ is the treatment received by participant $i$ in period $t$, $\\mu_{A}$ and $\\mu_{B}$ are treatment means, $\\pi_t$ is a period effect, and $\\varepsilon_{it}$ has mean $0$ and is independent of sequence, period, and treatment assignment. Subjects are randomized so that the initial sample sizes are $n_{AB} = 100$ and $n_{BA} = 100$. Dropout occurs only before period $2$ and depends on sequence, with retention probabilities $r_{AB} = 0.60$ and $r_{BA} = 0.90$ for period $2$.\n\nThe observed cell means (sample averages) are:\n- Sequence $AB$, period $1$ (treatment $A$): $10.1$ from $100$ observations.\n- Sequence $AB$, period $2$ (treatment $B$): $12.9$ from $60$ observations.\n- Sequence $BA$, period $1$ (treatment $B$): $12.1$ from $100$ observations.\n- Sequence $BA$, period $2$ (treatment $A$): $10.9$ from $90$ observations.\n\nUsing the core definitions of unbiasedness, expected values, and the additive model above, first show how differential dropout by sequence breaks the balance condition that equalizes period contributions between treatments when estimating the treatment contrast $\\Delta = \\mu_{B} - \\mu_{A}$ by a naive difference of observed treatment means (averaging all observed $B$ outcomes and subtracting the average of all observed $A$ outcomes, without accounting for period or sequence). Then, construct an inverse probability weighting (IPW) scheme to restore balance. Specifically, define weights that upweight period $2$ observations by the inverse of their retention probabilities within each sequence, leaving period $1$ weights equal to $1$. Use these weights to compute the reweighted difference between the weighted average of all observed $B$ outcomes and the weighted average of all observed $A$ outcomes.\n\nCompute the IPW-reweighted estimator of $\\Delta$ from the given data and report its value. Round your answer to four significant figures. Express the final answer as a pure number with no units.", "solution": "The problem requires an analysis of a two-period, two-sequence crossover trial with differential dropout. We must first demonstrate how this dropout pattern introduces bias in a naive estimator of the treatment effect, and then use an inverse probability weighting (IPW) scheme to compute a corrected estimate. The treatment contrast of interest is $\\Delta = \\mu_B - \\mu_A$.\n\nThe underlying additive model for an outcome $Y_{it}$ for subject $i$ in period $t$ is given as $Y_{it} = \\mu_{d_{it}} + \\pi_t + \\varepsilon_{it}$, where $\\mu_{d_{it}}$ is the mean for the treatment $d_{it} \\in \\{A,B\\}$, $\\pi_t$ is the period effect for $t \\in \\{1,2\\}$, and $\\varepsilon_{it}$ is a mean-zero error term.\n\nThe given data are:\n- Initial sample sizes: $n_{AB} = 100$, $n_{BA} = 100$.\n- Retention probabilities: $r_{AB} = 0.60$, $r_{BA} = 0.90$.\n- Resulting final sample sizes:\n  - Sequence $AB$, Period $1$: $N_{AB,1} = n_{AB} = 100$.\n  - Sequence $AB$, Period $2$: $N_{AB,2} = n_{AB} r_{AB} = 100 \\times 0.60 = 60$.\n  - Sequence $BA$, Period $1$: $N_{BA,1} = n_{BA} = 100$.\n  - Sequence $BA$, Period $2$: $N_{BA,2} = n_{BA} r_{BA} = 100 \\times 0.90 = 90$.\n- Observed cell means: $\\bar{Y}_{AB,1} = 10.1$, $\\bar{Y}_{AB,2} = 12.9$, $\\bar{Y}_{BA,1} = 12.1$, $\\bar{Y}_{BA,2} = 10.9$.\n\n**Part 1: Bias of the Naive Estimator**\nA naive estimator, $\\hat{\\Delta}_{naive}$, is the simple difference between the average of all observed outcomes for treatment $B$ and the average for treatment $A$. The total number of observed outcomes for treatment $A$ is $N_A = N_{AB,1} + N_{BA,2} = 100 + 90 = 190$. The total for treatment $B$ is $N_B = N_{BA,1} + N_{AB,2} = 100 + 60 = 160$.\n\nThe expectation of the naive estimator is:\n$$ E[\\hat{\\Delta}_{naive}] = E[\\bar{Y}_B] - E[\\bar{Y}_A] = (\\mu_B - \\mu_A) + \\left( \\frac{N_{BA,1}\\pi_1 + N_{AB,2}\\pi_2}{N_B} - \\frac{N_{AB,1}\\pi_1 + N_{BA,2}\\pi_2}{N_A} \\right) $$\nThe bias is the term added to the true treatment effect $\\Delta$. Rearranging this term by period effects gives:\n$$ \\text{Bias} = \\left( \\frac{N_{BA,1}}{N_B} - \\frac{N_{AB,1}}{N_A} \\right)\\pi_1 + \\left( \\frac{N_{AB,2}}{N_B} - \\frac{N_{BA,2}}{N_A} \\right)\\pi_2 $$\nSubstituting the sample sizes:\n$$ \\text{Bias} = \\left( \\frac{100}{160} - \\frac{100}{190} \\right)\\pi_1 + \\left( \\frac{60}{160} - \\frac{90}{190} \\right)\\pi_2 $$\n$$ \\text{Bias} = \\left( \\frac{5}{8} - \\frac{10}{19} \\right)\\pi_1 + \\left( \\frac{3}{8} - \\frac{9}{19} \\right)\\pi_2 = \\left( \\frac{95-80}{152} \\right)\\pi_1 + \\left( \\frac{57-72}{152} \\right)\\pi_2 = \\frac{15}{152}(\\pi_1 - \\pi_2) $$\nThe bias is non-zero if there is a period effect ($\\pi_1 \\neq \\pi_2$). This bias arises because the differential dropout ($r_{AB} \\neq r_{BA}$) breaks the balance of period representation within each treatment group.\n\n**Part 2: The IPW Estimator**\nTo correct for this bias, we use an IPW scheme. The weights are the inverse of the probability of being observed:\n- For period $1$ observations, the weight is $w_1 = 1$.\n- For period $2$ observations in sequence $AB$, the weight is $w_{AB,2} = 1/r_{AB} = 1/0.60$.\n- For period $2$ observations in sequence $BA$, the weight is $w_{BA,2} = 1/r_{BA} = 1/0.90$.\n\nThe IPW estimator is the difference between the weighted averages for each treatment, $\\hat{\\Delta}_{IPW} = \\bar{Y}_{B,w} - \\bar{Y}_{A,w}$.\n\nFirst, we calculate the weighted average for treatment $A$, $\\bar{Y}_{A,w}$:\nThe sum of weighted outcomes for treatment $A$ is:\n$$ \\sum_{i \\in A} w_i Y_i = (N_{AB,1} \\bar{Y}_{AB,1}) \\cdot w_1 + (N_{BA,2} \\bar{Y}_{BA,2}) \\cdot w_{BA,2} $$\n$$ \\sum_{i \\in A} w_i Y_i = (100 \\times 10.1) \\cdot 1 + (90 \\times 10.9) \\cdot \\frac{1}{0.90} = 1010 + 1090 = 2100 $$\nThe sum of weights for treatment $A$ is:\n$$ \\sum_{i \\in A} w_i = N_{AB,1} \\cdot w_1 + N_{BA,2} \\cdot w_{BA,2} = 100 \\cdot 1 + 90 \\cdot \\frac{1}{0.90} = 100 + 100 = 200 $$\nThe weighted average for treatment $A$ is $\\bar{Y}_{A,w} = \\frac{2100}{200} = 10.5$.\n\nNext, we calculate the weighted average for treatment $B$, $\\bar{Y}_{B,w}$:\nThe sum of weighted outcomes for treatment $B$ is:\n$$ \\sum_{i \\in B} w_i Y_i = (N_{BA,1} \\bar{Y}_{BA,1}) \\cdot w_1 + (N_{AB,2} \\bar{Y}_{AB,2}) \\cdot w_{AB,2} $$\n$$ \\sum_{i \\in B} w_i Y_i = (100 \\times 12.1) \\cdot 1 + (60 \\times 12.9) \\cdot \\frac{1}{0.60} = 1210 + 1290 = 2500 $$\nThe sum of weights for treatment $B$ is:\n$$ \\sum_{i \\in B} w_i = N_{BA,1} \\cdot w_1 + N_{AB,2} \\cdot w_{AB,2} = 100 \\cdot 1 + 60 \\cdot \\frac{1}{0.60} = 100 + 100 = 200 $$\nThe weighted average for treatment $B$ is $\\bar{Y}_{B,w} = \\frac{2500}{200} = 12.5$.\n\nFinally, we compute the IPW-reweighted estimator of $\\Delta$:\n$$ \\hat{\\Delta}_{IPW} = \\bar{Y}_{B,w} - \\bar{Y}_{A,w} = 12.5 - 10.5 = 2.0 $$\nThe problem requires the answer to be rounded to four significant figures.", "answer": "$$\n\\boxed{2.000}\n$$", "id": "4907236"}]}