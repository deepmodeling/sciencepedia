{"hands_on_practices": [{"introduction": "Repeatedly testing data as it accumulates in a clinical trial inflates the Type I error rate. To counteract this, group sequential methods use an $\\alpha$-spending function, which carefully allocates the total allowable Type I error, $\\alpha$, across the planned interim analyses. This exercise [@problem_id:4892107] provides hands-on practice with this core concept, asking you to calculate the portion of the error budget spent by a specific point in a trial using a common linear spending function.", "problem": "A one-sided group sequential clinical trial is designed to test a treatment effect with planned total type I error level $\\alpha$ and $K=4$ interim looks at cumulative information times $t \\in [0,1]$. The design uses an $\\alpha$-spending function, defined as a non-decreasing function $\\alpha(t)$ with $\\alpha(0)=0$ and $\\alpha(1)=\\alpha$, to control the cumulative probability of a false positive across looks. Suppose the design specifies the linear spending function $\\alpha(t)=\\alpha t$. The third look occurs at information fraction $t_3=0.75$.\n\nCompute the fraction of the total type I error $\\alpha$ that has been cumulatively spent by look $3$. Express your answer as a decimal. No rounding is required. In one or two sentences, interpret what this fraction implies about the potential for early stopping at look $3$ compared to earlier looks, but only report the numerical fraction as your final answer.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n-   The trial is a one-sided group sequential clinical trial.\n-   The total type I error level is $\\alpha$.\n-   The number of interim looks is $K=4$.\n-   The cumulative information time is denoted by $t \\in [0,1]$.\n-   An $\\alpha$-spending function, $\\alpha(t)$, is used, which is a non-decreasing function with $\\alpha(0)=0$ and $\\alpha(1)=\\alpha$.\n-   The specific spending function is the linear function $\\alpha(t) = \\alpha t$.\n-   The third interim look occurs at an information fraction of $t_3 = 0.75$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-grounded in the theory of biostatistics, specifically concerning group sequential methods and adaptive designs. The concept of an $\\alpha$-spending function, introduced by Lan and DeMets, is a standard and fundamental tool for designing interim analyses in clinical trials. The linear spending function $\\alpha(t) = \\alpha t$ is a common, valid choice.\n-   **Well-Posed**: The problem is well-posed. It provides a specific mathematical function and all necessary values to compute the required quantity. The question is unambiguous and has a unique, stable solution.\n-   **Objective**: The problem is stated using objective, precise terminology standard to the field of biostatistics.\n-   **Completeness and Consistency**: The provided information is complete and internally consistent. No contradictions are present.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be derived.\n\nThe core of this problem is the application of the defined $\\alpha$-spending function. The function $\\alpha(t)$ specifies the cumulative proportion of the total type I error, $\\alpha$, that is allocated for rejection of the null hypothesis up to an information fraction $t$.\n\nThe problem states that the $\\alpha$-spending function is a linear function of the information time $t$:\n$$\n\\alpha(t) = \\alpha t\n$$\nwhere $\\alpha$ is the total type I error for the trial.\n\nWe are asked to compute the fraction of the total type I error $\\alpha$ that has been cumulatively spent by look 3. This look occurs at an information fraction of $t_3 = 0.75$.\n\nFirst, we calculate the cumulative type I error spent up to and including the third look by evaluating the spending function $\\alpha(t)$ at $t = t_3$:\n$$\n\\alpha(t_3) = \\alpha \\times t_3\n$$\nSubstituting the given value of $t_3 = 0.75$, we get:\n$$\n\\alpha(0.75) = \\alpha \\times 0.75\n$$\nThis expression, $0.75\\alpha$, represents the absolute amount of cumulative type I error spent by the third look.\n\nThe problem asks for the *fraction* of the total type I error $\\alpha$ that this amount represents. This fraction is the ratio of the cumulative $\\alpha$ spent at look 3 to the total $\\alpha$ planned for the entire trial.\n$$\n\\text{Fraction} = \\frac{\\alpha(t_3)}{\\alpha}\n$$\nSubstituting the expression for $\\alpha(t_3)$:\n$$\n\\text{Fraction} = \\frac{0.75\\alpha}{\\alpha}\n$$\nThe $\\alpha$ terms in the numerator and the denominator cancel, provided $\\alpha \\neq 0$, which is a fundamental assumption of hypothesis testing.\n$$\n\\text{Fraction} = 0.75\n$$\nThe problem requires the answer to be expressed as a decimal, which it already is.\n\nAs for the interpretation, this fraction implies a linear allocation of the total type I error, meaning that by the time 75% of the information is collected, 75% of the opportunity for a false positive claim has been used. This makes stopping for efficacy at look 3 more probable than it would be at earlier looks (as more cumulative $\\alpha$ is spent) and also more probable than under a more conservative spending plan (like O'Brien-Fleming) that saves more $\\alpha$ for later in the trial.", "answer": "$$\n\\boxed{0.75}\n$$", "id": "4892107"}, {"introduction": "A key motivation for using a group sequential design is efficiencyâ€”the potential to stop a trial early for success or futility saves time, resources, and exposes fewer patients to potentially inferior treatments. The metric used to quantify this benefit is the expected sample size, $E[N]$, which averages the sample size over all possible outcomes of the trial. This practice problem [@problem_id:4892080] walks you through the derivation and calculation of $E[N]$, demonstrating how it depends on the true treatment effect and the pre-specified stopping boundaries.", "problem": "A two-arm randomized clinical trial compares a new treatment to control using a group sequential design with one interim analysis and one final analysis. The primary endpoint is approximately normally distributed with equal variances across arms, and the standardized test statistic at analysis $k$ is denoted by $Z_k$. Assume the canonical joint normal model for group sequential statistics: by the Central Limit Theorem (CLT), for information fraction $t_k \\in (0,1]$ accumulated at analysis $k$, the statistic satisfies $Z_k \\sim \\mathcal{N}(\\theta \\sqrt{t_k}, 1)$ under a true standardized treatment effect $\\theta$, where $\\mathcal{N}(\\mu,\\sigma^2)$ denotes a normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n\nThe design proceeds as follows:\n- The planned maximum total sample size is $n_{\\max} = 240$ subjects.\n- The interim analysis occurs after $n_1 = 120$ subjects, so the information fraction is $t_1 = n_1 / n_{\\max}$.\n- A two-sided efficacy stopping rule is used at the interim: stop the trial early for efficacy if $|Z_1| \\ge c_1$ with $c_1 = 2.0$. If the interim stopping rule is not met, the trial continues to the final analysis at $n_{\\max}$.\n\nDefine the expected sample size of this group sequential design from first principles in terms of the stopping distribution of the analysis index and derive an analytical expression for the expected sample size as a function of the true effect $\\theta$, the interim boundary $c_1$, and the planned maximum sample size $n_{\\max}$. Then, evaluate this expected sample size for $\\theta = 0.3$ using the above design parameters. Use the standard normal cumulative distribution function $\\Phi(\\cdot)$ where appropriate. Round your final expected sample size to $4$ significant figures. Express the answer in the unit \"subjects.\"", "solution": "The problem is well-posed, scientifically grounded in the principles of biostatistics, and contains sufficient information for a unique solution. Therefore, the problem is deemed valid.\n\nThe expected sample size, denoted $E[N|\\theta]$, for a group sequential design depends on the true treatment effect $\\theta$. The sample size $N$ is a discrete random variable, which in this case can take on two possible values: $n_1$ if the trial is stopped at the interim analysis, or $n_{\\max}$ if the trial continues to the final analysis.\n\nThe definition of the expected value of a discrete random variable is the sum of the products of each possible value of the variable and its corresponding probability. Therefore, the expected sample size is given by:\n$$\nE[N|\\theta] = n_1 \\cdot P(N=n_1 | \\theta) + n_{\\max} \\cdot P(N=n_{\\max} | \\theta)\n$$\nThe trial stops at the interim analysis if the stopping condition $|Z_1| \\ge c_1$ is met. The sample size will be $N=n_1$ in this event. The probability of this event is $P(N=n_1 | \\theta) = P(|Z_1| \\ge c_1 | \\theta)$.\nIf the stopping condition is not met, i.e., $|Z_1| < c_1$, the trial continues to the final analysis, and the sample size is $N=n_{\\max}$. The probability of continuing is $P(N=n_{\\max} | \\theta) = P(|Z_1| < c_1 | \\theta)$.\n\nSince these are the only two outcomes at the first stage, the probabilities sum to one:\n$$\nP(|Z_1| < c_1 | \\theta) = 1 - P(|Z_1| \\ge c_1 | \\theta)\n$$\nLet us denote the probability of stopping at the interim analysis as $p_1(\\theta) = P(|Z_1| \\ge c_1 | \\theta)$. The expected sample size expression can be rewritten as:\n$$\nE[N|\\theta] = n_1 \\cdot p_1(\\theta) + n_{\\max} \\cdot (1 - p_1(\\theta))\n$$\nRearranging terms, we obtain a more convenient form for calculation:\n$$\nE[N|\\theta] = n_1 p_1(\\theta) + n_{\\max} - n_{\\max} p_1(\\theta) = n_{\\max} - (n_{\\max} - n_1)p_1(\\theta)\n$$\nThe next step is to derive an analytical expression for the stopping probability $p_1(\\theta)$. The problem states that the test statistic $Z_1$ at the interim analysis follows a normal distribution $Z_1 \\sim \\mathcal{N}(\\theta\\sqrt{t_1}, 1)$, where $t_1$ is the information fraction at the first stage. The stopping probability is:\n$$\np_1(\\theta) = P(|Z_1| \\ge c_1 | \\theta) = P(Z_1 \\ge c_1 \\text{ or } Z_1 \\le -c_1 | \\theta)\n$$\nThese two events are mutually exclusive, so we can sum their probabilities:\n$$\np_1(\\theta) = P(Z_1 \\ge c_1 | \\theta) + P(Z_1 \\le -c_1 | \\theta)\n$$\nTo calculate these probabilities, we standardize the random variable $Z_1$. Let $W$ be a standard normal random variable, $W \\sim \\mathcal{N}(0, 1)$. Then $Z_1$ can be expressed as $Z_1 = W + \\theta\\sqrt{t_1}$.\nThe first term is:\n$$\nP(Z_1 \\ge c_1) = P(W + \\theta\\sqrt{t_1} \\ge c_1) = P(W \\ge c_1 - \\theta\\sqrt{t_1})\n$$\nUsing the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(x) = P(W \\le x)$, we have $P(W \\ge a) = 1 - P(W < a) = 1 - \\Phi(a)$. Therefore:\n$$\nP(Z_1 \\ge c_1) = 1 - \\Phi(c_1 - \\theta\\sqrt{t_1})\n$$\nThe second term is:\n$$\nP(Z_1 \\le -c_1) = P(W + \\theta\\sqrt{t_1} \\le -c_1) = P(W \\le -c_1 - \\theta\\sqrt{t_1}) = \\Phi(-c_1 - \\theta\\sqrt{t_1})\n$$\nCombining these, the stopping probability is:\n$$\np_1(\\theta) = 1 - \\Phi(c_1 - \\theta\\sqrt{t_1}) + \\Phi(-c_1 - \\theta\\sqrt{t_1})\n$$\nUsing the identity $\\Phi(-x) = 1 - \\Phi(x)$, which implies $1 - \\Phi(x) = \\Phi(-x)$, we can rewrite the first term as:\n$$\n1 - \\Phi(c_1 - \\theta\\sqrt{t_1}) = \\Phi(-(c_1 - \\theta\\sqrt{t_1})) = \\Phi(\\theta\\sqrt{t_1} - c_1)\n$$\nThis gives the final expression for the stopping probability:\n$$\np_1(\\theta) = \\Phi(\\theta\\sqrt{t_1} - c_1) + \\Phi(-c_1 - \\theta\\sqrt{t_1})\n$$\nThe information fraction $t_1$ is defined as $t_1 = n_1 / n_{\\max}$. Substituting this and the expression for $p_1(\\theta)$ into the equation for $E[N|\\theta]$, we get the required analytical expression:\n$$\nE[N|\\theta] = n_{\\max} - (n_{\\max} - n_1) \\left[ \\Phi\\left(\\theta\\sqrt{\\frac{n_1}{n_{\\max}}} - c_1\\right) + \\Phi\\left(-c_1 - \\theta\\sqrt{\\frac{n_1}{n_{\\max}}}\\right) \\right]\n$$\nNow, we evaluate this expression for the given parameters:\n- Maximum sample size: $n_{\\max} = 240$\n- Interim sample size: $n_1 = 120$\n- Interim stopping boundary: $c_1 = 2.0$\n- True standardized effect: $\\theta = 0.3$\n\nFirst, we calculate the information fraction $t_1$:\n$$\nt_1 = \\frac{n_1}{n_{\\max}} = \\frac{120}{240} = 0.5\n$$\nNext, we calculate the mean of $Z_1$, which is $\\mu_1 = \\theta\\sqrt{t_1}$:\n$$\n\\mu_1 = 0.3 \\sqrt{0.5} = 0.3 \\times \\frac{1}{\\sqrt{2}} \\approx 0.212132\n$$\nNow we compute the arguments of the $\\Phi$ function for the stopping probability $p_1(\\theta=0.3)$:\n$$\n\\theta\\sqrt{t_1} - c_1 \\approx 0.212132 - 2.0 = -1.787868\n$$\n$$\n-c_1 - \\theta\\sqrt{t_1} \\approx -2.0 - 0.212132 = -2.212132\n$$\nThe stopping probability is:\n$$\np_1(0.3) = \\Phi(-1.787868) + \\Phi(-2.212132)\n$$\nUsing standard statistical tables or software for the standard normal CDF:\n$$\n\\Phi(-1.787868) \\approx 0.036894\n$$\n$$\n\\Phi(-2.212132) \\approx 0.013481\n$$\n$$\np_1(0.3) \\approx 0.036894 + 0.013481 = 0.050375\n$$\nFinally, we calculate the expected sample size:\n$$\nE[N | \\theta=0.3] = n_{\\max} - (n_{\\max} - n_1) p_1(0.3)\n$$\n$$\nE[N | \\theta=0.3] \\approx 240 - (240 - 120) \\times 0.050375\n$$\n$$\nE[N | \\theta=0.3] \\approx 240 - 120 \\times 0.050375\n$$\n$$\nE[N | \\theta=0.3] \\approx 240 - 6.045\n$$\n$$\nE[N | \\theta=0.3] \\approx 233.955\n$$\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$\n233.955 \\approx 234.0\n$$\nThe expected sample size is approximately $234.0$ subjects.", "answer": "$$\\boxed{234.0}$$", "id": "4892080"}, {"introduction": "In many clinical trials, particularly in fields like oncology, the primary endpoint is a time-to-event outcome, and \"information\" is measured by the number of observed events, not simply the number of patients enrolled. Planning such event-driven trials requires translating the target event counts for interim analyses into projected calendar dates, a crucial step for logistics and resource management. This exercise [@problem_id:4892079] simulates this real-world planning challenge, asking you to map out a trial's timeline based on a realistic, non-constant model of event accrual.", "problem": "A biostatistics team is planning an event-driven group sequential trial that will compare two treatments on a time-to-event endpoint using the logrank test. The trial uses a group sequential design with four interim analyses at cumulative event counts $d = (75, 150, 225, 300)$, where $D = 300$ is the final targeted total number of events. Interim analyses are triggered by crossing these event thresholds, not by calendar time.\n\nTo plan the calendar timing of interim looks, the team models the cumulative event process as a nonhomogeneous Poisson process with instantaneous event intensity $\\lambda(t)$ (events per month) as a function of calendar time $t$ (in months) measured from first patient randomization. Due to anticipated start-up and ramp-up phases, the expected event accrual rate is piecewise constant:\n$$\n\\lambda(t) = \n\\begin{cases}\n0, & 0 \\leq t < 3, \\\\\n6, & 3 \\leq t < 9, \\\\\n15, & 9 \\leq t < 15, \\\\\n25, & t \\geq 15.\n\\end{cases}\n$$\nUsing this model, the expected cumulative number of events by time $t$ is the cumulative intensity function $\\Lambda(t) = \\int_{0}^{t} \\lambda(u)\\,du$. Define $t_i$ as the calendar time at which the $i$-th look occurs, determined by the equation $\\Lambda(t_i) = d_i$ for $i = 1, 2, 3, 4$.\n\nCompute the vector of calendar times $t_i$ at which the looks are expected to occur. Explain, using the structure of $\\Lambda(t)$ and $\\lambda(t)$, how event accrual delays impact the timing of interim analyses in event-driven group sequential trials.\n\nRound each $t_i$ to three significant figures. Express the times in months.", "solution": "The problem requires the calculation of the expected calendar times for four interim analyses in an event-driven group sequential trial. These times, denoted $t_i$, are determined by when the expected cumulative number of events reaches specific thresholds $d_i$. The event accrual process is modeled as a nonhomogeneous Poisson process with a given piecewise instantaneous event intensity $\\lambda(t)$.\n\nFirst, we must determine the cumulative intensity function, $\\Lambda(t)$, which represents the expected number of events by calendar time $t$. This is found by integrating the instantaneous intensity $\\lambda(t)$ from $0$ to $t$. The given intensity function is:\n$$\n\\lambda(u) = \n\\begin{cases}\n0, & 0 \\leq u < 3 \\\\\n6, & 3 \\leq u < 9 \\\\\n15, & 9 \\leq u < 15 \\\\\n25, & u \\geq 15.\n\\end{cases}\n$$\nWe compute $\\Lambda(t) = \\int_{0}^{t} \\lambda(u)\\,du$ for each interval of $t$.\n\nFor $0 \\leq t < 3$:\n$$ \\Lambda(t) = \\int_{0}^{t} 0 \\, du = 0 $$\n\nFor $3 \\leq t < 9$:\n$$ \\Lambda(t) = \\int_{0}^{3} 0 \\, du + \\int_{3}^{t} 6 \\, du = 0 + [6u]_{3}^{t} = 6t - 18 $$\nAt the boundary $t=9$, the cumulative events are $\\Lambda(9) = 6(9) - 18 = 54 - 18 = 36$.\n\nFor $9 \\leq t < 15$:\n$$ \\Lambda(t) = \\Lambda(9) + \\int_{9}^{t} 15 \\, du = 36 + [15u]_{9}^{t} = 36 + 15t - 15(9) = 36 + 15t - 135 = 15t - 99 $$\nAt the boundary $t=15$, the cumulative events are $\\Lambda(15) = 15(15) - 99 = 225 - 99 = 126$.\n\nFor $t \\geq 15$:\n$$ \\Lambda(t) = \\Lambda(15) + \\int_{15}^{t} 25 \\, du = 126 + [25u]_{15}^{t} = 126 + 25t - 25(15) = 126 + 25t - 375 = 25t - 249 $$\n\nThus, the complete piecewise cumulative intensity function is:\n$$\n\\Lambda(t) = \n\\begin{cases}\n0, & 0 \\leq t < 3 \\\\\n6t - 18, & 3 \\leq t < 9 \\\\\n15t - 99, & 9 \\leq t < 15 \\\\\n25t - 249, & t \\geq 15.\n\\end{cases}\n$$\nThe times of the interim analyses, $t_i$, are found by solving the equation $\\Lambda(t_i) = d_i$ for the target event counts $d = (75, 150, 225, 300)$.\n\nTo find $t_1$, we set $\\Lambda(t_1) = d_1 = 75$. We check the intervals:\nSince $\\Lambda(9) = 36$ and $\\Lambda(15) = 126$, the condition $36 < 75 < 126$ implies that $t_1$ must lie in the interval $[9, 15)$. We use the corresponding formula for $\\Lambda(t)$:\n$$ 15t_1 - 99 = 75 $$\n$$ 15t_1 = 174 $$\n$$ t_1 = \\frac{174}{15} = 11.6 $$\nRounding to three significant figures, $t_1 = 11.6$ months.\n\nTo find $t_2$, we set $\\Lambda(t_2) = d_2 = 150$.\nSince $\\Lambda(15) = 126$, and $150 > 126$, $t_2$ must be in the interval $t \\geq 15$. We use the formula for this interval:\n$$ 25t_2 - 249 = 150 $$\n$$ 25t_2 = 399 $$\n$$ t_2 = \\frac{399}{25} = 15.96 $$\nRounding to three significant figures, $t_2 = 16.0$ months.\n\nTo find $t_3$, we set $\\Lambda(t_3) = d_3 = 225$.\nSince $225 > 126$, $t_3$ is also in the interval $t \\geq 15$:\n$$ 25t_3 - 249 = 225 $$\n$$ 25t_3 = 474 $$\n$$ t_3 = \\frac{474}{25} = 18.96 $$\nRounding to three significant figures, $t_3 = 19.0$ months.\n\nTo find $t_4$, we set $\\Lambda(t_4) = d_4 = 300$.\nSince $300 > 126$, $t_4$ is also in the interval $t \\geq 15$:\n$$ 25t_4 - 249 = 300 $$\n$$ 25t_4 = 549 $$\n$$ t_4 = \\frac{549}{25} = 21.96 $$\nRounding to three significant figures, $t_4 = 22.0$ months.\n\nThe vector of calendar times is $(11.6, 16.0, 19.0, 22.0)$ months.\n\nRegarding the impact of event accrual delays, the structure of $\\lambda(t)$ and $\\Lambda(t)$ provides a clear explanation. In an event-driven trial, the calendar time $t_i$ of an analysis is determined by the inverse of the cumulative intensity function, i.e., $t_i = \\Lambda^{-1}(d_i)$. The function $\\Lambda(t)$ is, by definition, the integral of the event rate $\\lambda(t)$. The fundamental theorem of calculus states that $\\frac{d}{dt}\\Lambda(t) = \\lambda(t)$, meaning the instantaneous event rate is the slope of the cumulative event curve.\n\nThe provided $\\lambda(t)$ explicitly models accrual delays:\n$1$. A start-up delay: For $t \\in [0, 3)$, $\\lambda(t) = 0$. This results in $\\Lambda(t) = 0$ over this period. The \"clock\" for event accumulation does not start until $t=3$ months, directly delaying all subsequent milestones.\n$2$. A ramp-up phase: For $t \\in [3, 9)$ and $t \\in [9, 15)$, the rates $\\lambda(t) = 6$ and $\\lambda(t) = 15$ are lower than the final steady-state rate of $\\lambda(t) = 25$.\n\nAny period where $\\lambda(t)$ is low corresponds to a segment of the $\\Lambda(t)$ curve with a shallow slope. To achieve a specified increase in cumulative events (a vertical rise on the graph of $\\Lambda(t)$), a longer duration in calendar time (a horizontal run) is required when the slope is shallow. Therefore, the initial zero-rate period and the subsequent slower-rate ramp-up phases stretch the trial timeline. If the accrual rate had been a constant $\\lambda(t) = 25$ from the beginning ($t=0$), the time to reach any event count $d_i$ would be $t_i = d_i/25$, which would be significantly shorter. For example, the first look ($d_1=75$) would be expected at $t_1=3$ months, rather than the calculated $11.6$ months. Thus, accrual delays, as modeled by the structure of $\\lambda(t)$, directly and quantitatively determine the extended calendar time required to reach the event-based endpoints of the trial.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11.6 & 16.0 & 19.0 & 22.0\n\\end{pmatrix}\n}\n$$", "id": "4892079"}]}