{"hands_on_practices": [{"introduction": "The foundation of sample size calculation rests on a clear relationship between precision, confidence, and population variability. This exercise asks you to derive the classic sample size formula from first principles, showing how the Central Limit Theorem allows us to plan a study to achieve a specific margin of error [@problem_id:4950548]. Mastering this derivation is crucial because it demystifies the formula, revealing the assumptions it relies on and building the skills needed to adapt it for different study designs.", "problem": "A public health laboratory plans a cross-sectional study to estimate the difference between an unknown infection prevalence and a benchmark value. Let $p$ denote the true population proportion infected in the target population, and let $p_{0}$ denote a fixed benchmark proportion from a previous surveillance year. The estimand of interest is the risk difference $d = p - p_{0}$. An investigator will collect an independent and identically distributed sample of size $n$ from the target population and compute the sample proportion $\\hat{p}$ to form the plug-in estimator $\\hat{d} = \\hat{p} - p_{0}$. The design requirement is that a two-sided $(1-\\alpha)$ confidence interval for $d$ based on a large-sample normal approximation has half-width no larger than a pre-specified margin $m > 0$.\n\nStarting from first principles for sampling of Bernoulli outcomes, derive the exact analytic expression for the minimum required $n$ in terms of $p$, $\\alpha$, and $m$ that guarantees the margin requirement under the stated normal approximation. Then, explain qualitatively how the required $n$ depends on $p$ under this approximation and identify which values of $p$ maximize $n$. Your final reported answer must be the closed-form symbolic expression for $n$; do not plug in numerical values or report inequalities.", "solution": "The problem statement is evaluated and found to be valid. It is a well-posed, scientifically grounded question in biostatistics that is free of contradictions or ambiguities. We may therefore proceed with the derivation.\n\nLet the outcome for an individual subject be a random variable $X$, where $X=1$ if the subject is infected and $X=0$ otherwise. We are given that sampling is from a large population where the true proportion of infected individuals is $p$. Thus, $X$ follows a Bernoulli distribution with parameter $p$, written as $X \\sim \\text{Bernoulli}(p)$. The expected value of $X$ is $E[X] = p$ and its variance is $\\text{Var}(X) = p(1-p)$.\n\nAn independent and identically distributed sample of size $n$ is drawn, yielding outcomes $X_1, X_2, \\ldots, X_n$. The sample proportion, $\\hat{p}$, is the sample mean of these outcomes:\n$$ \\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\nBy the linearity of expectation, the expected value of the sample proportion is:\n$$ E[\\hat{p}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_i] = \\frac{1}{n} \\sum_{i=1}^{n} p = \\frac{np}{n} = p $$\nSince the observations are independent, the variance of the sample proportion is:\n$$ \\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} p(1-p) = \\frac{n p(1-p)}{n^2} = \\frac{p(1-p)}{n} $$\nThe estimand of interest is the risk difference $d = p - p_0$, where $p_0$ is a fixed, known constant. The estimator for $d$ is given as $\\hat{d} = \\hat{p} - p_0$. The expected value of $\\hat{d}$ is $E[\\hat{d}] = E[\\hat{p} - p_0] = E[\\hat{p}] - p_0 = p - p_0 = d$. This confirms $\\hat{d}$ is an unbiased estimator for $d$. The variance of $\\hat{d}$ is:\n$$ \\text{Var}(\\hat{d}) = \\text{Var}(\\hat{p} - p_0) = \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n} $$\nsince $p_0$ is a constant and does not contribute to the variance. The standard error of the estimator $\\hat{d}$ is the square root of its variance:\n$$ SE(\\hat{d}) = \\sqrt{\\frac{p(1-p)}{n}} $$\nThe problem states that a two-sided $(1-\\alpha)$ confidence interval for $d$ is to be constructed based on a large-sample normal approximation. According to the Central Limit Theorem, for a sufficiently large sample size $n$, the sampling distribution of $\\hat{d}$ is approximately normal with mean $d$ and variance $\\frac{p(1-p)}{n}$. A generic two-sided $(1-\\alpha)$ confidence interval for $d$ is given by:\n$$ \\hat{d} \\pm z_{1-\\alpha/2} \\times SE(\\hat{d}) $$\nwhere $z_{1-\\alpha/2}$ is the quantile of the standard normal distribution that leaves a tail probability of $\\alpha/2$ to its right (i.e., $\\Phi(z_{1-\\alpha/2}) = 1-\\alpha/2$, where $\\Phi$ is the standard normal cumulative distribution function).\n\nThe half-width of this confidence interval, which we can denote as $HW$, is the quantity multiplied by the $z$-score:\n$$ HW = z_{1-\\alpha/2} \\times SE(\\hat{d}) = z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} $$\nThe design requirement is that this half-width must be no larger than a pre-specified margin $m > 0$. To find the minimum sample size $n$ that guarantees this condition, we set the half-width equal to $m$:\n$$ m = z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} $$\nWe now solve this equation for $n$. Squaring both sides yields:\n$$ m^2 = \\left(z_{1-\\alpha/2}\\right)^2 \\left(\\frac{p(1-p)}{n}\\right) $$\nMultiplying both sides by $n$ gives:\n$$ n m^2 = z_{1-\\alpha/2}^{2} p(1-p) $$\nFinally, dividing by $m^2$ isolates $n$, providing the exact analytic expression for the minimum required sample size:\n$$ n = \\frac{z_{1-\\alpha/2}^{2} p(1-p)}{m^2} $$\nThis expression gives $n$ in terms of the true proportion $p$, the significance level $\\alpha$ (which determines $z_{1-\\alpha/2}$), and the desired margin of error $m$.\n\nTo explain qualitatively how the required sample size $n$ depends on $p$, we inspect the derived formula. For a fixed confidence level $(1-\\alpha)$ and margin $m$, the terms $z_{1-\\alpha/2}^2$ and $m^2$ are positive constants. Therefore, $n$ is directly proportional to the product $p(1-p)$:\n$$ n \\propto p(1-p) $$\nThe term $p(1-p)$ represents the variance of a single Bernoulli trial. The sample size required to achieve a certain precision is thus determined by the underlying variance of the population. The function $f(p) = p(1-p) = p-p^2$ is a quadratic function of $p$, defined on the interval $p \\in [0, 1]$. To find the value of $p$ that maximizes this function, and therefore maximizes the required sample size $n$, we can use calculus. Taking the first derivative with respect to $p$ and setting it to zero:\n$$ \\frac{df}{dp} = \\frac{d}{dp}(p-p^2) = 1 - 2p = 0 $$\nSolving for $p$ gives $p = \\frac{1}{2} = 0.5$. The second derivative is $\\frac{d^2f}{dp^2} = -2$, which is negative, confirming that $p=0.5$ corresponds to a maximum.\n\nThus, the required sample size $n$ is maximized when the true population proportion $p$ is $0.5$. This is the \"worst-case\" or most conservative scenario, as it corresponds to the maximum possible heterogeneity (variance) in the population. When the population is evenly split between infected and non-infected individuals, the uncertainty is at its peak, demanding the largest sample size to estimate the proportion with a given level of precision. As $p$ approaches either $0$ or $1$, the population becomes more homogeneous, the variance $p(1-p)$ decreases, and consequently a smaller sample size is needed.", "answer": "$$\\boxed{\\frac{z_{1-\\alpha/2}^{2} p(1-p)}{m^2}}$$", "id": "4950548"}, {"introduction": "Not all research questions require a two-sided conclusion; sometimes, we are only interested in whether a proportion is greater or less than a certain value. This practice explores the distinction between two-sided and one-sided precision targets, demonstrating how the choice impacts the required sample size [@problem_id:4950450]. By comparing the respective formulas, you will learn how to design more efficient studies when the scientific question has a directional hypothesis.", "problem": "A public health team plans a cross-sectional survey to estimate a binary prevalence $p$ (the probability that an individual has a specified health attribute). Let $\\hat{p}$ denote the sample proportion from a sample of size $n$. The team requires a maximum absolute error $m$ between $\\hat{p}$ and $p$ that is guaranteed with a confidence statement. They are considering two precision targets based on large-sample approximations:\n- A two-sided confidence statement requiring $\\Pr(|\\hat{p}-p|\\le m)\\approx 1-\\alpha$.\n- A one-sided confidence statement requiring $\\Pr(\\hat{p}-p\\le m)\\approx 1-\\alpha$.\n\nStarting only from the binomial model for $\\hat{p}$ and the Central Limit Theorem (CLT), derive how the margin $m$ relates to $n$ through the sampling variability of $\\hat{p}$ and appropriate standard normal quantiles. Use this to compare the required sample sizes under the two targets when $m$ and $\\alpha$ are held fixed. Define $z_{\\gamma}$ to be the upper $\\gamma$-quantile of the standard normal distribution, that is, the number satisfying $\\Pr(Z>z_{\\gamma})=\\gamma$ for $Z\\sim \\mathcal{N}(0,1)$.\n\nExpress the multiplicative factor by which the one-sided design’s required sample size changes relative to the two-sided design’s required sample size, and then evaluate this factor numerically for $\\alpha=0.05$. Round your final numerical answer to four significant figures.", "solution": "We begin with the binomial sampling model: if $X\\sim \\mathrm{Binomial}(n,p)$ denotes the number of individuals with the attribute in a sample of size $n$, then the sample proportion is $\\hat{p}=X/n$. The expectation and variance of $\\hat{p}$ are\n$$\n\\mathbb{E}[\\hat{p}]=p, \\qquad \\mathrm{Var}(\\hat{p})=\\frac{p(1-p)}{n}.\n$$\nBy the Central Limit Theorem (CLT), for large $n$,\n$$\n\\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}} \\approx Z,\n$$\nwhere $Z\\sim \\mathcal{N}(0,1)$ is a standard normal random variable.\n\nTwo-sided precision target. The two-sided requirement $\\Pr(|\\hat{p}-p|\\le m)\\approx 1-\\alpha$ can be written, under the normal approximation, as\n$$\n\\Pr\\!\\left(\\left|\\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}}\\right|\\le \\frac{m}{\\sqrt{p(1-p)/n}}\\right)\\approx \\Pr(|Z|\\le c)=1-\\alpha,\n$$\nwhere $c$ is the cutoff that achieves $1-\\alpha$ coverage for the absolute normal variable. Equivalently, $c=z_{\\alpha/2}$, with $z_{\\gamma}$ defined by $\\Pr(Z>z_{\\gamma})=\\gamma$. Enforcing the margin $m$ gives\n$$\n\\frac{m}{\\sqrt{p(1-p)/n}}=z_{\\alpha/2} \\quad \\Longrightarrow \\quad n_{\\text{two}}=\\frac{z_{\\alpha/2}^{2}\\,p(1-p)}{m^{2}}.\n$$\n\nOne-sided precision target. The one-sided requirement $\\Pr(\\hat{p}-p\\le m)\\approx 1-\\alpha$ translates, under the same approximation, to\n$$\n\\Pr\\!\\left(\\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}}\\le \\frac{m}{\\sqrt{p(1-p)/n}}\\right)\\approx \\Pr(Z\\le c)=1-\\alpha,\n$$\nwhich is achieved when $c=z_{\\alpha}$, since $\\Pr(Z>z_{\\alpha})=\\alpha$. Thus the margin condition implies\n$$\n\\frac{m}{\\sqrt{p(1-p)/n}}=z_{\\alpha} \\quad \\Longrightarrow \\quad n_{\\text{one}}=\\frac{z_{\\alpha}^{2}\\,p(1-p)}{m^{2}}.\n$$\n\nMultiplicative change in sample size. Holding $m$ and $\\alpha$ fixed, the ratio of the required sample sizes is\n$$\n\\frac{n_{\\text{one}}}{n_{\\text{two}}}=\\frac{z_{\\alpha}^{2}\\,p(1-p)/m^{2}}{z_{\\alpha/2}^{2}\\,p(1-p)/m^{2}}=\\left(\\frac{z_{\\alpha}}{z_{\\alpha/2}}\\right)^{2}.\n$$\nThis factor is independent of $p$ and $m$ under the normal approximation.\n\nNumerical evaluation for $\\alpha=0.05$. Here $z_{\\alpha}=z_{0.05}$ and $z_{\\alpha/2}=z_{0.025}$. Using standard normal quantiles,\n$$\nz_{0.05}\\approx 1.64485362695147, \\qquad z_{0.025}\\approx 1.95996398454005,\n$$\nso the multiplicative factor is\n$$\n\\left(\\frac{z_{0.05}}{z_{0.025}}\\right)^{2}\\approx \\left(\\frac{1.64485362695147}{1.95996398454005}\\right)^{2}\\approx 0.704300999\\ldots\n$$\nRounded to four significant figures, this factor is $0.7043$.", "answer": "$$\\boxed{0.7043}$$", "id": "4950450"}, {"introduction": "When studying rare events like adverse drug reactions, the event probability $p$ is very small, and the standard normal approximation may not be reliable. This advanced practice introduces the Poisson approximation to the binomial distribution, a powerful tool for this 'rare events' regime [@problem_id:4950477]. You will derive sample size rules based on this approximation, tackling a practical problem common in safety monitoring where the goal is to demonstrate that an event rate is below a critical threshold.", "problem": "A safety monitoring study will enroll $n$ independent subjects, each with a binary adverse event outcome modeled as independently and identically distributed Bernoulli random variables with event probability $p$. Let $X$ denote the total number of adverse events among the $n$ subjects, so $X$ follows a Binomial distribution with parameters $n$ and $p$. In the regime of rare events where $p$ is small and $n$ is large but the product $np$ remains of moderate magnitude, carefully justify the approximation of the Binomial distribution for $X$ by a Poisson distribution with mean parameter $\\lambda = np$. Starting from fundamental definitions, establish the limiting form of the Binomial probability mass function under $p \\to 0$, $n \\to \\infty$, with $np \\to \\lambda$, and explain the role of the exponential limit in this derivation.\n\nUsing this Poisson approximation, derive an approximate two-sided $(1-\\alpha)$ confidence interval (CI) for the event probability $p$ based on the observed count $X$, taking as a starting point the large-sample behavior of the maximum likelihood estimator (MLE) under the Poisson model and the invariance of MLEs under smooth transformations. Also derive the $(1-\\alpha)$ one-sided upper confidence bound for $p$ in the special case where the observed count is $X=0$ by inverting the exact Poisson tail probability.\n\nPlanning problem: A device safety study aims to demonstrate that if zero adverse events are observed, the one-sided upper $95\\%$ confidence bound for $p$ is strictly below a prespecified safety threshold $p^{\\star} = 1.5 \\times 10^{-4}$. Take $\\alpha = 0.05$ for the one-sided bound. Using the Poisson approximation results derived above, determine the minimal integer sample size $n$ required so that, when $X=0$, the upper $(1-\\alpha)$ bound for $p$ is less than $p^{\\star}$. Express the final numerical answer as the single integer $n$. No rounding by significant figures is required; report the minimal integer that meets the specification.", "solution": "The total number of adverse events $X$ among $n$ independent Bernoulli trials with event probability $p$ follows a Binomial distribution with probability mass function\n$$\n\\Pr(X=x) = \\binom{n}{x} p^{x} (1-p)^{n-x}, \\quad x=0,1,2,\\dots,n.\n$$\nWe seek a limiting approximation when $p$ is small and $n$ is large, while the product $np$ remains of moderate order. Set $\\lambda = np$ and consider the limit $n \\to \\infty$, $p \\to 0$, such that $np \\to \\lambda$. A standard approach is to substitute $p = \\lambda/n$ and examine the limiting form:\n$$\n\\Pr(X=x) = \\binom{n}{x} \\left(\\frac{\\lambda}{n}\\right)^{x} \\left(1 - \\frac{\\lambda}{n}\\right)^{n-x}.\n$$\nRewrite the binomial coefficient and separate terms:\n$$\n\\binom{n}{x} \\left(\\frac{\\lambda}{n}\\right)^{x} = \\frac{n(n-1)\\cdots(n-x+1)}{x!} \\cdot \\frac{\\lambda^{x}}{n^{x}}.\n$$\nAs $n \\to \\infty$ with fixed $x$, we have\n$$\n\\frac{n(n-1)\\cdots(n-x+1)}{n^{x}} \\to 1,\n$$\nso the first factor converges to $\\lambda^{x}/x!$. For the remaining term, use the well-tested exponential limit\n$$\n\\lim_{n \\to \\infty} \\left(1 - \\frac{\\lambda}{n}\\right)^{n} = \\exp(-\\lambda).\n$$\nAdditionally,\n$$\n\\left(1 - \\frac{\\lambda}{n}\\right)^{-x} \\to 1 \\quad \\text{as} \\quad n \\to \\infty \\quad \\text{for fixed } x.\n$$\nCombining these limits yields\n$$\n\\Pr(X=x) \\to \\frac{\\lambda^{x}}{x!} \\exp(-\\lambda),\n$$\nwhich is the probability mass function of a Poisson random variable with mean $\\lambda$. Therefore, under $p \\to 0$, $n \\to \\infty$, and $np \\to \\lambda$, we have the approximation $X \\sim \\text{Poisson}(\\lambda)$ with $\\lambda = np$.\n\nWe next derive approximate confidence intervals for $p$ via the Poisson approximation. Under $X \\sim \\text{Poisson}(\\lambda)$, the maximum likelihood estimator (MLE) of $\\lambda$ is $\\hat{\\lambda} = X$. The Fisher information for a single Poisson observation in $\\lambda$ is $I(\\lambda) = 1/\\lambda$, and by standard asymptotic theory, the MLE is approximately normal:\n$$\n\\hat{\\lambda} \\approx \\text{Normal}\\!\\left(\\lambda, \\lambda\\right),\n$$\nor equivalently,\n$$\n\\frac{\\hat{\\lambda} - \\lambda}{\\sqrt{\\lambda}} \\approx \\text{Normal}(0,1).\n$$\nReplacing $\\lambda$ by $\\hat{\\lambda}$ in the standard error yields the familiar large-sample two-sided $(1-\\alpha)$ confidence interval for $\\lambda$:\n$$\n\\hat{\\lambda} \\pm z_{1-\\alpha/2} \\sqrt{\\hat{\\lambda}},\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution. Since $p = \\lambda/n$, the MLE for $p$ is $\\hat{p} = \\hat{\\lambda}/n = X/n$. By the invariance of MLEs under smooth transformations and the delta method, the approximate two-sided $(1-\\alpha)$ confidence interval for $p$ is\n$$\n\\hat{p} \\pm z_{1-\\alpha/2} \\frac{\\sqrt{\\hat{\\lambda}}}{n} \\;=\\; \\frac{X}{n} \\pm z_{1-\\alpha/2} \\frac{\\sqrt{X}}{n}.\n$$\nThis interval leverages the Poisson approximation to the variance when events are rare.\n\nWe now derive the one-sided upper bound for $p$ when $X=0$ by inverting the exact Poisson tail probability. For $X \\sim \\text{Poisson}(\\lambda)$, the probability of observing zero events is\n$$\n\\Pr(X=0) = \\exp(-\\lambda).\n$$\nA one-sided upper $(1-\\alpha)$ confidence bound for $\\lambda$ given $X=0$ satisfies\n$$\n\\Pr_{\\lambda_U}(X=0) = \\alpha \\quad \\Rightarrow \\quad \\exp(-\\lambda_U) = \\alpha \\quad \\Rightarrow \\quad \\lambda_U = -\\ln(\\alpha).\n$$\nTransforming back to $p$ via $p = \\lambda/n$ gives the corresponding upper bound\n$$\np_U = \\frac{\\lambda_U}{n} = \\frac{-\\ln(\\alpha)}{n}.\n$$\n\nWe apply this to the planning problem. The study aims to ensure that if $X=0$, the one-sided upper bound for $p$ is strictly below $p^{\\star} = 1.5 \\times 10^{-4}$ with confidence level $(1-\\alpha)$ where $\\alpha = 0.05$. The requirement is\n$$\np_U = \\frac{-\\ln(\\alpha)}{n} < p^{\\star}.\n$$\nSolving for $n$ yields\n$$\nn > \\frac{-\\ln(\\alpha)}{p^{\\star}}.\n$$\nTo guarantee the bound is strictly below $p^{\\star}$, we choose the minimal integer $n$ satisfying the inequality:\n$$\nn_{\\min} = \\left\\lceil \\frac{-\\ln(\\alpha)}{p^{\\star}} \\right\\rceil.\n$$\nSubstituting $\\alpha = 0.05$ and $p^{\\star} = 1.5 \\times 10^{-4}$,\n$$\n-\\ln(0.05) \\approx 2.99573227355,\n$$\nso\n$$\n\\frac{-\\ln(0.05)}{1.5 \\times 10^{-4}} \\approx \\frac{2.99573227355}{0.00015} \\approx 19971.54849.\n$$\nTherefore, the minimal integer sample size is\n$$\nn_{\\min} = 19972.\n$$\nThis $n$ ensures that, under zero observed events, the one-sided upper $95\\%$ bound for $p$ is strictly below $1.5 \\times 10^{-4}$, as required.", "answer": "$$\\boxed{19972}$$", "id": "4950477"}]}