{"hands_on_practices": [{"introduction": "Before diving into complex imputation methods, it is crucial to understand why simpler approaches can be misleading. This first exercise provides a formal proof of a fundamental flaw in deterministic mean imputation: the artificial reduction of variance. By working through this problem [@problem_id:4928120], you will use the law of total variance to quantify exactly how much this naive method underestimates the true data variability, building a strong intuition for the necessity of proper imputation techniques.", "problem": "A biostatistician is analyzing a continuous biomarker $Y$ measured on study participants, where some values are missing. Let $R$ be a missingness indicator with $R=1$ if $Y$ is observed and $R=0$ if $Y$ is missing. Suppose the missingness is Missing Completely At Random (MCAR), meaning $R$ is independent of $Y$. Assume $Y$ has a population distribution with mean $\\mu$ and variance $\\sigma^{2}$, and the probability that a value is missing is $p$, so $\\mathbb{P}(R=0)=p$ and $\\mathbb{P}(R=1)=1-p$. Consider two strategies for handling missing values:\n- Deterministic mean imputation: replace every missing $Y$ with the constant $\\mu$.\n- Multiple imputation (MI): replace each missing $Y$ with a random draw from the predictive distribution of $Y$ under MCAR, which in this setup coincides with the population distribution of $Y$ with mean $\\mu$ and variance $\\sigma^{2}$.\n\nStarting from the law of total variance, $\\operatorname{Var}(Z)=\\mathbb{E}[\\operatorname{Var}(Z\\mid R)]+\\operatorname{Var}(\\mathbb{E}[Z\\mid R])$ for any random variable $Z$, explain why deterministic mean imputation biases the estimated variance downward, and derive a closed-form expression, in terms of $p$, for the multiplicative attenuation factor\n$$A(p)=\\frac{\\operatorname{Var}(Y^{\\ast}_{\\text{mean}})}{\\operatorname{Var}(Y^{\\ast}_{\\text{MI}})},$$\nwhere $Y^{\\ast}_{\\text{mean}}$ denotes the completed data under deterministic mean imputation and $Y^{\\ast}_{\\text{MI}}$ denotes the completed data under multiple imputation. Express your final answer as a single closed-form analytic expression in terms of $p$. No numerical rounding is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-contained. It presents a standard theoretical problem in biostatistics concerning the impact of missing data handling techniques on variance estimation. All provided information is consistent and sufficient for deriving a solution.\n\nLet $Y$ be the continuous biomarker with population mean $\\mathbb{E}[Y]=\\mu$ and variance $\\operatorname{Var}(Y)=\\sigma^2$. Let $R$ be the missingness indicator, where $R=1$ if $Y$ is observed and $R=0$ if $Y$ is missing. The missingness is Missing Completely At Random (MCAR), which implies that the random variables $R$ and $Y$ are statistically independent. The probability of a value being missing is $\\mathbb{P}(R=0)=p$, and the probability of it being observed is $\\mathbb{P}(R=1)=1-p$.\n\nWe define the random variables representing the data after imputation.\nFor deterministic mean imputation, the imputed variable $Y^{\\ast}_{\\text{mean}}$ is defined as:\n$$Y^{\\ast}_{\\text{mean}} = \\begin{cases} Y & \\text{if } R=1 \\\\ \\mu & \\text{if } R=0 \\end{cases}$$\nThis can be written as $Y^{\\ast}_{\\text{mean}} = R \\cdot Y + (1-R) \\cdot \\mu$.\n\nFor multiple imputation (MI), each missing value is replaced by an independent random draw, let's call it $Y_{draw}$, from the same distribution as $Y$. Therefore, $\\mathbb{E}[Y_{draw}]=\\mu$, $\\operatorname{Var}(Y_{draw})=\\sigma^2$, and $Y_{draw}$ is independent of both $Y$ and $R$. The imputed variable $Y^{\\ast}_{\\text{MI}}$ is defined as:\n$$Y^{\\ast}_{\\text{MI}} = \\begin{cases} Y & \\text{if } R=1 \\\\ Y_{draw} & \\text{if } R=0 \\end{cases}$$\nThis can be written as $Y^{\\ast}_{\\text{MI}} = R \\cdot Y + (1-R) \\cdot Y_{draw}$.\n\nOur goal is to compute the variances of $Y^{\\ast}_{\\text{mean}}$ and $Y^{\\ast}_{\\text{MI}}$ using the law of total variance:\n$$\\operatorname{Var}(Z) = \\mathbb{E}[\\operatorname{Var}(Z\\mid R)] + \\operatorname{Var}(\\mathbb{E}[Z\\mid R])$$\n\nFirst, we analyze the multiple imputation case, $Y^{\\ast}_{\\text{MI}}$. We need the conditional expectation and conditional variance of $Y^{\\ast}_{\\text{MI}}$ given $R$.\n\nConditional expectation $\\mathbb{E}[Y^{\\ast}_{\\text{MI}}\\mid R]$:\n- For $R=1$, $Y^{\\ast}_{\\text{MI}} = Y$. Since $Y$ and $R$ are independent (MCAR), $\\mathbb{E}[Y^{\\ast}_{\\text{MI}}\\mid R=1] = \\mathbb{E}[Y\\mid R=1] = \\mathbb{E}[Y] = \\mu$.\n- For $R=0$, $Y^{\\ast}_{\\text{MI}} = Y_{draw}$. Since $Y_{draw}$ is independent of $R$, $\\mathbb{E}[Y^{\\ast}_{\\text{MI}}\\mid R=0] = \\mathbb{E}[Y_{draw}\\mid R=0] = \\mathbb{E}[Y_{draw}] = \\mu$.\nThe random variable $\\mathbb{E}[Y^{\\ast}_{\\text{MI}}\\mid R]$ takes the value $\\mu$ regardless of the value of $R$. Thus, it is a constant, $\\mathbb{E}[Y^{\\ast}_{\\text{MI}}\\mid R] = \\mu$.\n\nConditional variance $\\operatorname{Var}(Y^{\\ast}_{\\text{MI}}\\mid R)$:\n- For $R=1$, $Y^{\\ast}_{\\text{MI}} = Y$. Due to MCAR, $\\operatorname{Var}(Y^{\\ast}_{\\text{MI}}\\mid R=1) = \\operatorname{Var}(Y\\mid R=1) = \\operatorname{Var}(Y) = \\sigma^2$.\n- For $R=0$, $Y^{\\ast}_{\\text{MI}} = Y_{draw}$. Due to independence, $\\operatorname{Var}(Y^{\\ast}_{\\text{MI}}\\mid R=0) = \\operatorname{Var}(Y_{draw}\\mid R=0) = \\operatorname{Var}(Y_{draw}) = \\sigma^2$.\nThe random variable $\\operatorname{Var}(Y^{\\ast}_{\\text{MI}}\\mid R)$ takes the value $\\sigma^2$ regardless of the value of $R$. Thus, it is a constant, $\\operatorname{Var}(Y^{\\ast}_{\\text{MI}}\\mid R) = \\sigma^2$.\n\nNow we compute the two terms of the law of total variance for $Y^{\\ast}_{\\text{MI}}$:\n1. $\\mathbb{E}[\\operatorname{Var}(Y^{\\ast}_{\\text{MI}}\\mid R)] = \\mathbb{E}[\\sigma^2] = \\sigma^2$.\n2. $\\operatorname{Var}(\\mathbb{E}[Y^{\\ast}_{\\text{MI}}\\mid R]) = \\operatorname{Var}(\\mu) = 0$.\n\nSumming these terms gives the total variance for multiple imputation:\n$$\\operatorname{Var}(Y^{\\ast}_{\\text{MI}}) = \\sigma^2 + 0 = \\sigma^2$$\nThis result is intuitive: under ideal MCAR conditions, proper multiple imputation correctly reproduces the original population variance.\n\nNext, we analyze the deterministic mean imputation case, $Y^{\\ast}_{\\text{mean}}$.\n\nConditional expectation $\\mathbb{E}[Y^{\\ast}_{\\text{mean}}\\mid R]$:\n- For $R=1$, $Y^{\\ast}_{\\text{mean}} = Y$. So, $\\mathbb{E}[Y^{\\ast}_{\\text{mean}}\\mid R=1] = \\mathbb{E}[Y\\mid R=1] = \\mathbb{E}[Y] = \\mu$.\n- For $R=0$, $Y^{\\ast}_{\\text{mean}} = \\mu$ (a constant). So, $\\mathbb{E}[Y^{\\ast}_{\\text{mean}}\\mid R=0] = \\mathbb{E}[\\mu] = \\mu$.\nAs with MI, the conditional expectation $\\mathbb{E}[Y^{\\ast}_{\\text{mean}}\\mid R]$ is the constant $\\mu$.\n\nConditional variance $\\operatorname{Var}(Y^{\\ast}_{\\text{mean}}\\mid R)$:\n- For $R=1$, $Y^{\\ast}_{\\text{mean}} = Y$. So, $\\operatorname{Var}(Y^{\\ast}_{\\text{mean}}\\mid R=1) = \\operatorname{Var}(Y\\mid R=1) = \\operatorname{Var}(Y) = \\sigma^2$.\n- For $R=0$, $Y^{\\ast}_{\\text{mean}} = \\mu$. The variance of a constant is $0$, so $\\operatorname{Var}(Y^{\\ast}_{\\text{mean}}\\mid R=0) = 0$.\nThe random variable $\\operatorname{Var}(Y^{\\ast}_{\\text{mean}}\\mid R)$ takes the value $\\sigma^2$ with probability $\\mathbb{P}(R=1) = 1-p$ and the value $0$ with probability $\\mathbb{P}(R=0) = p$.\n\nNow we compute the two terms of the law of total variance for $Y^{\\ast}_{\\text{mean}}$:\n1. $\\mathbb{E}[\\operatorname{Var}(Y^{\\ast}_{\\text{mean}}\\mid R)] = \\sigma^2 \\cdot \\mathbb{P}(R=1) + 0 \\cdot \\mathbb{P}(R=0) = \\sigma^2(1-p) + 0 = (1-p)\\sigma^2$.\n2. $\\operatorname{Var}(\\mathbb{E}[Y^{\\ast}_{\\text{mean}}\\mid R]) = \\operatorname{Var}(\\mu) = 0$.\n\nSumming these terms gives the total variance for deterministic mean imputation:\n$$\\operatorname{Var}(Y^{\\ast}_{\\text{mean}}) = (1-p)\\sigma^2 + 0 = (1-p)\\sigma^2$$\n\nThis result explains why deterministic mean imputation biases the estimated variance downward. The true population variance is $\\sigma^2$. The variance of the data after mean imputation is $(1-p)\\sigma^2$. Since $p$ is the probability of missing data, we have $0 \\le p \\le 1$. If any data is missing ($p>0$), then $1-p < 1$, and consequently $\\operatorname{Var}(Y^{\\ast}_{\\text{mean}}) < \\sigma^2$. The downward bias arises because this method artificially introduces a subset of data (the imputed values) with zero variance, thereby reducing the overall variance of the completed dataset. The total variance is only the contribution from the non-missing proportion ($1-p$) of the data.\n\nFinally, we derive the multiplicative attenuation factor $A(p)$:\n$$A(p) = \\frac{\\operatorname{Var}(Y^{\\ast}_{\\text{mean}})}{\\operatorname{Var}(Y^{\\ast}_{\\text{MI}})}$$\nSubstituting the expressions we derived:\n$$A(p) = \\frac{(1-p)\\sigma^2}{\\sigma^2} = 1-p$$\nThe attenuation factor is simply $1-p$, the proportion of observed data.", "answer": "$$\\boxed{1-p}$$", "id": "4928120"}, {"introduction": "The validity of any analysis with missing data hinges on the underlying missingness mechanism. This next exercise [@problem_id:4928144] presents a carefully constructed hypothetical scenario to sharpen your understanding of the distinction between data that are Missing At Random (MAR) and Missing Not At Random (MNAR). By calculating the bias of a complete-case analysis in this setting, you will see firsthand how failing to account for the variable driving the missingness can lead to incorrect scientific conclusions.", "problem": "Consider a cohort study with a binary outcome $Y$ indicating the presence ($Y=1$) or absence ($Y=0$) of a condition and a continuous covariate $X$ representing a standardized risk score. Let $X \\sim \\mathcal{N}(0,1)$, and suppose the data collection protocol records the outcome $Y$ only for participants whose covariate exceeds a threshold. Define the missingness indicator $R$ for $Y$ by $R=1$ if $Y$ is observed and $R=0$ if $Y$ is missing. Assume the following data-generating process:\n- The outcome is determined by a threshold rule $Y=\\mathbf{1}\\{X>0\\}$.\n- The missingness mechanism is $R=\\mathbf{1}\\{X>-0.5\\}$.\n\nUsing the standard definitions of Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), treat the mechanism $R$ as MAR when conditioning on $X$ but MNAR when $X$ is omitted from the conditioning set, and show this formally by computing $P(R=1 \\mid Y=1)$ and $P(R=1 \\mid Y=0)$ under the specified joint distribution.\n\nThen, consider the complete-case estimator of the marginal prevalence $p=\\Pr(Y=1)$, defined as $p_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)$. Derive $p$ and $p_{\\mathrm{cc}}$, and compute the bias $b = p_{\\mathrm{cc}} - p$ as a closed-form analytic expression in terms of the standard normal cumulative distribution function $\\Phi$. Finally, briefly explain the implications of this mechanism for complete-case analysis and for the validity of Multiple Imputation (MI).\n\nYour final answer should be a single closed-form analytic expression for $b$ in terms of $\\Phi$.", "solution": "We begin with the fundamental definitions. Let $R$ denote the missingness indicator for $Y$. Missing Completely At Random (MCAR) is defined by $P(R \\mid Y_{\\mathrm{mis}}, Y_{\\mathrm{obs}}, X)=P(R)$, Missing At Random (MAR) is defined by $P(R \\mid Y_{\\mathrm{mis}}, Y_{\\mathrm{obs}}, X)=P(R \\mid Y_{\\mathrm{obs}}, X)$, and Missing Not At Random (MNAR) is the complement where the distribution of $R$ depends on unobserved data even after conditioning on observed quantities.\n\nIn the given setup, $X \\sim \\mathcal{N}(0,1)$, the outcome is $Y=\\mathbf{1}\\{X>0\\}$, and missingness is $R=\\mathbf{1}\\{X>-0.5\\}$. Because $R$ depends only on $X$, we have\n$$\nP(R=1 \\mid X,Y)=P(R=1 \\mid X)=\\mathbf{1}\\{X>-0.5\\},\n$$\nwhich shows that, conditional on $X$, missingness does not depend on unobserved values of $Y$, satisfying Missing At Random (MAR) when conditioning on $X$.\n\nTo show that the mechanism is Missing Not At Random (MNAR) when $X$ is omitted, we compute $P(R=1 \\mid Y)$ marginalizing $X$ but conditioning on $Y$. First observe that $Y=1 \\iff X>0$ and $Y=0 \\iff X \\le 0$ due to the threshold rule. Then:\n- For $Y=1$, we have $X>0$, which implies $X>-0.5$, thus\n$$\nP(R=1 \\mid Y=1)=P(X>-0.5 \\mid X>0)=1.\n$$\n- For $Y=0$, we have $X \\le 0$, and missingness is $R=1$ if $X>-0.5$. Therefore\n$$\nP(R=1 \\mid Y=0)=P(-0.5 < X \\le 0 \\mid X \\le 0)\n=\\frac{P(-0.5 < X \\le 0)}{P(X \\le 0)}\n=\\frac{\\Phi(0)-\\Phi(-0.5)}{\\Phi(0)},\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. Using the symmetry $\\Phi(-a)=1-\\Phi(a)$ and $\\Phi(0)=\\frac{1}{2}$, this simplifies to\n$$\nP(R=1 \\mid Y=0)=\\frac{\\frac{1}{2}-\\left(1-\\Phi(0.5)\\right)}{\\frac{1}{2}}\n=\\frac{\\Phi(0.5)-\\frac{1}{2}}{\\frac{1}{2}}\n=2\\Phi(0.5)-1.\n$$\nSince $P(R=1 \\mid Y=1)=1$ and $P(R=1 \\mid Y=0)=2\\Phi(0.5)-1<1$, we have $P(R=1 \\mid Y=1) \\neq P(R=1 \\mid Y=0)$, which demonstrates MNAR when $X$ is omitted.\n\nNext, we derive the true marginal prevalence $p=\\Pr(Y=1)$. Because $Y=\\mathbf{1}\\{X>0\\}$ and $X \\sim \\mathcal{N}(0,1)$, we have\n$$\np=\\Pr(Y=1)=\\Pr(X>0)=1-\\Phi(0)=\\frac{1}{2}.\n$$\n\nWe now derive the complete-case prevalence $p_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)$. Since $R=1$ if and only if $X>-0.5$, and $Y=1$ if and only if $X>0$, we have\n$$\np_{\\mathrm{cc}}=\\Pr(Y=1 \\mid R=1)=\\Pr(X>0 \\mid X>-0.5)\n=\\frac{\\Pr(X>0)}{\\Pr(X>-0.5)}=\\frac{\\frac{1}{2}}{\\Phi(0.5)}.\n$$\nTherefore, the bias of the complete-case estimator of the marginal prevalence is\n$$\nb = p_{\\mathrm{cc}} - p \n= \\frac{\\frac{1}{2}}{\\Phi(0.5)} - \\frac{1}{2}\n= \\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right).\n$$\n\nImplications for analysis:\n- Complete-case analysis estimates $p_{\\mathrm{cc}}$ instead of $p$ and is biased because the observed data are enriched for larger $X$ values, which deterministically correspond to $Y=1$ under the threshold rule; hence $p_{\\mathrm{cc}}>p$ and the bias $b$ is positive.\n- Multiple Imputation (MI) would yield valid inference under the Missing At Random (MAR) assumption if the imputation model conditions on $X$ (the variable driving missingness). Because $R$ depends only on $X$, including $X$ in the imputation model aligns with MAR and allows consistent recovery of the marginal distribution of $Y$. In contrast, omitting $X$ would mis-specify the missingness as Missing Not At Random (MNAR) relative to the imputation model, generally leading to biased imputations and biased estimates of $p$.\n\nThe required closed-form analytic expression for the bias $b$ is\n$$\nb=\\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right).\n$$", "answer": "$$\\boxed{\\frac{1}{2}\\left(\\frac{1}{\\Phi(0.5)} - 1\\right)}$$", "id": "4928144"}, {"introduction": "Having established the pitfalls of simple methods and the importance of the missingness mechanism, we now turn to a state-of-the-art solution: Multiple Imputation by Chained Equations (MICE). A key challenge in MICE is ensuring the imputation models are \"compatible\" with the final analysis model. This advanced exercise [@problem_id:4928113] guides you through the design of a Substantive Model Compatible workflow, requiring you to derive the correct conditional distribution for imputation from first principles, a core skill for any serious practitioner.", "problem": "A biostatistics study investigates a continuous clinical outcome denoted by $Y$ and two covariates denoted by $X_{1}$ (continuous biomarker) and $X_{2}$ (binary exposure, where $X_{2} \\in \\{0,1\\}$). The substantive analysis model is a homoscedastic normal linear regression for $Y$ with Gaussian errors, specifically\n$$\nY = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$. The covariate distribution for $X_{1}$ given $X_{2}$ is Gaussian with $X_{1} \\mid X_{2} \\sim \\mathcal{N}(\\mu_{1 \\mid x_{2}}, \\tau^{2})$, where $\\mu_{1 \\mid x_{2}} = \\mu_{10} + \\delta x_{2}$. The exposure $X_{2}$ follows a Bernoulli distribution with parameter $p$, denoted $X_{2} \\sim \\mathrm{Bernoulli}(p)$.\n\nSuppose $X_{1}$ is missing for some individuals, with the missingness mechanism Missing At Random (MAR): the indicator for $X_{1}$ missingness, denoted $R_{1}$, satisfies $P(R_{1} = 0 \\mid X_{1}, Y, X_{2}) = P(R_{1} = 0 \\mid Y, X_{2})$. The proportion of missing data is 0.30 for $X_1$ and 0.15 for $Y$; $X_2$ is fully observed. You will use Multiple Imputation by Chained Equations (MICE) with Substantive Model Compatible Fully Conditional Specification (SMC-FCS) to ensure that the imputation models are compatible with the substantive model.\n\nTask:\n- Starting from foundational principles (Bayes rule and Gaussian distribution properties), design a workflow that ensures substantive-model compatibility: describe how to jointly update the substantive model parameters with imputation using SMC-FCS, including the role of the conditional imputation models for $X_{1}$ and $Y$.\n- Derive, from first principles, the conditional imputation distribution for $X_{1} \\mid Y, X_{2}$ implied by the substantive model and the covariate model.\n- Using the derived conditional distribution, compute the conditional mean for a single subject with $y = 2.8$ and $x_{2} = 1$ under the following parameter values: $\\beta_{0} = 1.5$, $\\beta_{1} = 2.0$, $\\beta_{2} = -0.5$, $\\sigma^{2} = 1.44$, $\\mu_{10} = 0.0$, $\\delta = 1.0$, and $\\tau^{2} = 4.0$. Express your final answer as a single real number. Round your answer to four significant figures.", "solution": "The problem requires a three-part answer: a description of the Substantive Model Compatible Fully Conditional Specification (SMC-FCS) workflow, the derivation of a specific conditional distribution, and a numerical calculation based on this derivation.\n\n**Part 1: The SMC-FCS MICE Workflow**\n\nThe goal of Multiple Imputation by Chained Equations (MICE) is to handle missing data by creating multiple ($m > 1$) plausible completed datasets. This is achieved by imputing missing values with draws from their posterior predictive distributions, conditional on the observed data. The \"chained equations\" or \"fully conditional specification\" (FCS) aspect means this is done iteratively, variable by variable.\n\nA critical issue in MICE is ensuring \"compatibility\" (or \"congeniality\") between the imputation models and the final substantive analysis model. If the imputation models are misspecified relative to the substantive model (e.g., they omit interactions or non-linearities present in the substantive model), the final parameter estimates can be biased.\n\nSubstantive Model Compatible Fully Conditional Specification (SMC-FCS) resolves this by deriving the imputation models directly from the joint distribution implied by the substantive model and the specified models for covariates. For the given problem, the joint model is $P(Y, X_1, X_2) = P(Y \\mid X_1, X_2) P(X_1 \\mid X_2) P(X_2)$. All parameters of this joint model are denoted by $\\boldsymbol{\\theta} = \\{\\beta_0, \\beta_1, \\beta_2, \\sigma^2, \\mu_{10}, \\delta, \\tau^2, p\\}$.\n\nThe SMC-FCS workflow proceeds as follows, resembling a Gibbs sampler:\n\n1.  **Initialization**: Start with an incomplete dataset. For each of the $m$ imputations, fill in initial guesses for the missing values of $Y$ and $X_1$ (e.g., via mean imputation). This creates $m$ 'complete' datasets.\n\n2.  **Iterative Cycling**: For each of the $m$ datasets, and for a sufficient number of iterations (e.g., $10-20$ cycles):\n    a.  **Parameter Update Step**: Given the current state of the completed data, draw a new set of parameters $\\boldsymbol{\\theta}^*$ from their posterior distribution, $P(\\boldsymbol{\\theta} \\mid \\mathbf{Y}_{\\text{comp}}, \\mathbf{X}_{1, \\text{comp}}, \\mathbf{X}_2)$. In practice, this is done by drawing from the posteriors for each part of the model:\n        - Draw $(\\boldsymbol{\\beta}, \\sigma^2)^*$ from $P(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathbf{Y}_{\\text{comp}}, \\mathbf{X}_{1, \\text{comp}}, \\mathbf{X}_2)$.\n        - Draw $(\\mu_{10}, \\delta, \\tau^2)^*$ from $P(\\mu_{10}, \\delta, \\tau^2 \\mid \\mathbf{X}_{1, \\text{comp}}, \\mathbf{X}_2)$.\n        - Draw $p^*$ from $P(p \\mid \\mathbf{X}_2)$.\n\n    b.  **Imputation Step for $X_1$**: Go through each subject $i$ for whom $X_{1,i}$ is missing. Draw a new imputed value $X_{1,i}^*$ from its fully conditional distribution, using the newly drawn parameters $\\boldsymbol{\\theta}^*$:\n        $$ X_{1,i}^* \\sim P(X_1 \\mid Y_i, X_{2,i}, \\boldsymbol{\\theta}^*) $$\n        This is the distribution to be derived in Part 2. For subjects where $Y_i$ is also missing, the current imputed value of $Y_i$ is used.\n\n    c.  **Imputation Step for $Y$**: Go through each subject $i$ for whom $Y_i$ is missing. Draw a new imputed value $Y_i^*$ from its fully conditional distribution, using the parameters $\\boldsymbol{\\theta}^*$ and the most current values of the covariates (including any newly imputed values $X_{1,i}^*$):\n        $$ Y_i^* \\sim P(Y \\mid X_{1,i}, X_{2,i}, \\boldsymbol{\\theta}^*) = \\mathcal{N}(\\beta_0^* + \\beta_1^* X_{1,i} + \\beta_2^* X_{2,i}, (\\sigma^2)^*) $$\n        This imputation model is directly the substantive model, ensuring compatibility.\n\n3.  **Analysis and Pooling**: After the specified number of cycles, the final $m$ completed datasets are stored. The substantive model $Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$ is fitted to each of the $m$ datasets, yielding $m$ sets of estimates $(\\hat{\\boldsymbol{\\beta}}_k, \\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}_k))$ for $k=1, \\dots, m$. These are then combined using Rubin's Rules to produce a single set of parameter estimates and standard errors that properly account for the uncertainty due to missing data.\n\n**Part 2: Derivation of the Conditional Imputation Distribution for $X_1 \\mid Y, X_2$**\n\nThe imputation distribution for $X_1$ is its conditional distribution given the other variables, $P(X_1 \\mid Y, X_2)$. This distribution is derived from the joint model. We can do this by first specifying the joint distribution of $(Y, X_1)$ conditional on $X_2=x_2$.\n\nThe problem specifies:\n1.  $X_1 \\mid X_2=x_2 \\sim \\mathcal{N}(\\mu_{1 \\mid x_2}, \\tau^2)$\n2.  $Y \\mid X_1=x_1, X_2=x_2 \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2, \\sigma^2)$\n\nThis structure implies that the joint distribution of $(Y, X_1)$ conditional on $X_2=x_2$ is a bivariate normal distribution. We need to find its parameters (mean vector and covariance matrix).\n\nThe mean of $X_1$ is $E[X_1 \\mid x_2] = \\mu_{1 \\mid x_2}$.\nThe mean of $Y$ can be found using the law of total expectation:\n$E[Y \\mid x_2] = E[E[Y \\mid X_1, x_2] \\mid x_2] = E[\\beta_0 + \\beta_1 X_1 + \\beta_2 x_2 \\mid x_2] = \\beta_0 + \\beta_1 E[X_1 \\mid x_2] + \\beta_2 x_2 = \\beta_0 + \\beta_1 \\mu_{1 \\mid x_2} + \\beta_2 x_2$.\n\nThe variance of $X_1$ is $Var(X_1 \\mid x_2) = \\tau^2$.\nThe variance of $Y$ is found using the law of total variance:\n$Var(Y \\mid x_2) = E[Var(Y \\mid X_1, x_2) \\mid x_2] + Var(E[Y \\mid X_1, x_2] \\mid x_2)$\n$Var(Y \\mid x_2) = E[\\sigma^2 \\mid x_2] + Var(\\beta_0 + \\beta_1 X_1 + \\beta_2 x_2 \\mid x_2) = \\sigma^2 + \\beta_1^2 Var(X_1 \\mid x_2) = \\sigma^2 + \\beta_1^2 \\tau^2$.\n\nThe covariance between $Y$ and $X_1$ is:\n$Cov(Y, X_1 \\mid x_2) = Cov(\\beta_0 + \\beta_1 X_1 + \\beta_2 x_2 + \\varepsilon, X_1 \\mid x_2)$\n$Cov(Y, X_1 \\mid x_2) = Cov(\\beta_1 X_1, X_1 \\mid x_2) + Cov(\\varepsilon, X_1 \\mid x_2) = \\beta_1 Var(X_1 \\mid x_2) + 0 = \\beta_1 \\tau^2$.\n\nThus, the joint distribution is:\n$$ \\begin{pmatrix} Y \\\\ X_1 \\end{pmatrix} \\mid X_2=x_2 \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\beta_0 + \\beta_1 \\mu_{1 \\mid x_2} + \\beta_2 x_2 \\\\ \\mu_{1 \\mid x_2} \\end{pmatrix}, \\begin{pmatrix} \\sigma^2 + \\beta_1^2 \\tau^2 & \\beta_1 \\tau^2 \\\\ \\beta_1 \\tau^2 & \\tau^2 \\end{pmatrix} \\right) $$\n\nUsing the standard formula for conditional distributions of a multivariate normal, for $\\begin{pmatrix} Z_1 \\\\ Z_2 \\end{pmatrix} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, the distribution $Z_2 \\mid Z_1=z_1$ is normal with:\n- Mean: $E[Z_2 \\mid z_1] = \\mu_2 + \\Sigma_{21}\\Sigma_{11}^{-1}(z_1 - \\mu_1)$\n- Variance: $Var(Z_2 \\mid z_1) = \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}$\n\nApplying this to our case, where $Z_1=Y$ and $Z_2=X_1$:\nThe conditional mean of $X_1$ given $Y=y$ and $X_2=x_2$ is:\n$E[X_1 \\mid y, x_2] = \\mu_{1 \\mid x_2} + (\\beta_1 \\tau^2)(\\sigma^2 + \\beta_1^2 \\tau^2)^{-1}(y - (\\beta_0 + \\beta_1 \\mu_{1 \\mid x_2} + \\beta_2 x_2))$\n$$ E[X_1 \\mid y, x_2] = \\mu_{1 \\mid x_2} + \\frac{\\beta_1 \\tau^2}{\\sigma^2 + \\beta_1^2 \\tau^2} (y - \\beta_0 - \\beta_2 x_2 - \\beta_1 \\mu_{1 \\mid x_2}) $$\nThe conditional variance of $X_1$ given $Y=y$ and $X_2=x_2$ is:\n$Var(X_1 \\mid y, x_2) = \\tau^2 - (\\beta_1 \\tau^2)(\\sigma^2 + \\beta_1^2 \\tau^2)^{-1}(\\beta_1 \\tau^2) = \\tau^2 - \\frac{\\beta_1^2 \\tau^4}{\\sigma^2 + \\beta_1^2 \\tau^2}$\n$Var(X_1 \\mid y, x_2) = \\frac{\\tau^2(\\sigma^2 + \\beta_1^2 \\tau^2) - \\beta_1^2 \\tau^4}{\\sigma^2 + \\beta_1^2 \\tau^2} = \\frac{\\tau^2 \\sigma^2}{\\sigma^2 + \\beta_1^2 \\tau^2} = \\left(\\frac{1}{\\tau^2} + \\frac{\\beta_1^2}{\\sigma^2}\\right)^{-1}$\n\nThe conditional imputation distribution is therefore Gaussian:\n$X_1 \\mid Y=y, X_2=x_2 \\sim \\mathcal{N}\\left(E[X_1 \\mid y, x_2], Var(X_1 \\mid y, x_2)\\right)$.\n\n**Part 3: Calculation of the Conditional Mean**\n\nWe need to compute $E[X_1 \\mid y=2.8, x_2=1]$ using the provided parameters:\n$\\beta_{0} = 1.5$, $\\beta_{1} = 2.0$, $\\beta_{2} = -0.5$, $\\sigma^{2} = 1.44$, $\\mu_{10} = 0.0$, $\\delta = 1.0$, $\\tau^{2} = 4.0$.\nThe subject has $y = 2.8$ and $x_2 = 1$.\n\nFirst, calculate the mean of $X_1$ conditional on $x_2=1$:\n$\\mu_{1 \\mid x_2} = \\mu_{10} + \\delta x_2 \\implies \\mu_{1 \\mid 1} = 0.0 + 1.0 \\times 1 = 1.0$.\n\nNext, we use the derived formula for the conditional mean of $X_1$. For clarity, we can use the alternative but equivalent form:\n$$ E[X_1 \\mid y, x_2] = \\frac{\\beta_1 \\tau^2 (y - \\beta_0 - \\beta_2 x_2) + \\mu_{1 \\mid x_2} \\sigma^2}{\\beta_1^2 \\tau^2 + \\sigma^2} $$\nCalculate the denominator:\nDenominator $= \\beta_1^2 \\tau^2 + \\sigma^2 = (2.0)^2 \\times 4.0 + 1.44 = 4.0 \\times 4.0 + 1.44 = 16.0 + 1.44 = 17.44$.\n\nCalculate the numerator:\nNumerator $= \\beta_1 \\tau^2 (y - \\beta_0 - \\beta_2 x_2) + \\mu_{1 \\mid 1} \\sigma^2$\n$= (2.0)(4.0)(2.8 - 1.5 - (-0.5)(1)) + (1.0)(1.44)$\n$= 8.0(2.8 - 1.5 + 0.5) + 1.44$\n$= 8.0(1.8) + 1.44$\n$= 14.4 + 1.44 = 15.84$.\n\nFinally, compute the conditional mean:\n$E[X_1 \\mid y=2.8, x_2=1] = \\frac{15.84}{17.44} \\approx 0.90825688...$\n\nRounding to four significant figures, we get $0.9083$.", "answer": "$$\\boxed{0.9083}$$", "id": "4928113"}]}