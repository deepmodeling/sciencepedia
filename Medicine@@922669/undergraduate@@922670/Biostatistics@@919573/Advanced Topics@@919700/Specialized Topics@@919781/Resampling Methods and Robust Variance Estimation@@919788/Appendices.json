{"hands_on_practices": [{"introduction": "This first exercise provides a fundamental introduction to the bootstrap, a powerful computational tool for estimating the standard error of a statistic. We focus on the sample median, a robust estimator of central tendency whose standard error lacks a simple analytical formula, making it a perfect candidate for resampling methods. By implementing the core bootstrap algorithm, you will gain hands-on experience in how repeated resampling from the observed data can be used to approximate a statistic's sampling variability. [@problem_id:4948755]", "problem": "You are given independent and identically distributed (IID) observations of systolic blood pressure measured in millimeters of mercury (mmHg). Let the sample be denoted by $\\{X_1,\\dots,X_n\\}$ with $n=20$. The sample median $\\hat{m}$ is the central robust location estimator obtained from the order statistics of the sample, defined for even $n$ as the average of the two central order statistics. An investigator wants a robust variance estimate for $\\hat{m}$ by approximating its sampling variability using the bootstrap principle: resampling with replacement from the empirical distribution of the observed data and re-computing the median across multiple bootstrap replicates. The empirical bootstrap distribution of the medians is then used to estimate the Standard Error (SE) of $\\hat{m}$. \n\nStarting from the core definitions that:\n- The empirical distribution $\\hat{F}_n$ places probability mass $1/n$ on each observed value in $\\{X_i\\}_{i=1}^n$.\n- A bootstrap resample of size $n$ is generated by sampling with replacement from $\\{X_i\\}_{i=1}^n$ according to $\\hat{F}_n$.\n- The bootstrap estimate of the SE of the sample median is the empirical standard deviation of the set of bootstrap medians.\n\nWrite a program that:\n- For each provided test case, computes the bootstrap estimate of the SE of the sample median using $B=1000$ IID bootstrap resamples of size $n=20$ drawn with replacement from the observed data.\n- Uses a fixed pseudorandom number generator seed for reproducibility equal to $s=20231111$.\n- Reports the empirical standard deviation of the $B$ bootstrap medians as a floating-point number in mmHg.\n\nExpress all results explicitly in mmHg. The final outputs should be floats, rounded in the program to six decimal places.\n\nTest Suite (each is a list of $n=20$ systolic blood pressure values, in mmHg):\n1. General variability, typical adult values:\n   $\\{118,122,135,128,150,142,130,125,138,145,132,129,140,136,127,133,149,121,137,134\\}$\n2. Presence of a high outlier:\n   $\\{110,115,120,125,130,135,140,145,150,155,160,165,170,175,180,185,190,195,200,240\\}$\n3. Multiple ties (repeated measurements):\n   $\\{130,130,130,130,135,135,135,140,140,140,145,145,145,150,150,150,155,155,160,160\\}$\n4. Degenerate case (all equal):\n   $\\{130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130\\}$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each \"result\" is the bootstrap SE (mmHg) for the corresponding test case, formatted to six decimal places. Angles are not involved. No percentages are involved.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established statistical theory of bootstrap resampling, is well-posed with all necessary parameters provided for a unique and reproducible solution, and is expressed in objective, formal language. We may therefore proceed with the solution.\n\nThe problem requires the computation of a bootstrap estimate for the standard error (SE) of the sample median for four distinct datasets of systolic blood pressure measurements. The standard error of a statistic is a measure of the variability of that statistic's sampling distribution; it quantifies the precision of the sample statistic as an estimate of the corresponding population parameter. The bootstrap is a powerful and widely used computational method for approximating this sampling distribution when its theoretical form is unknown or difficult to derive.\n\nThe fundamental principle of the bootstrap, as proposed by Efron, is to use the observed sample itself as an approximation to the true underlying data-generating distribution. The empirical distribution function, $\\hat{F}_n$, which assigns a probability mass of $1/n$ to each observed data point $X_i$ in the sample $\\{X_1, \\dots, X_n\\}$, serves as this approximation.\n\nThe procedure is as follows:\n$1$. A large number, $B$, of bootstrap samples are generated. Each bootstrap sample, denoted $\\{X_{b,1}^*, \\dots, X_{b,n}^*\\}$ for $b \\in \\{1, \\dots, B\\}$, is a random sample of size $n$ drawn *with replacement* from the original data $\\{X_1, \\dots, X_n\\}$. Drawing from the original sample with replacement is equivalent to drawing an i.i.d. sample from the empirical distribution $\\hat{F}_n$. For this problem, the sample size is $n=20$ and the number of bootstrap replicates is $B=1000$.\n\n$2$. For each bootstrap sample, the statistic of interest—in this case, the sample median—is computed. Let $\\hat{m}_b^*$ be the median of the $b$-th bootstrap sample. The sample median for a sample of even size $n$ is defined as the arithmetic mean of the two central order statistics, i.e., the values at positions $n/2$ and $(n/2)+1$ in the sorted sample. For $n=20$, this is the average of the $10$-th and $11$-th sorted values.\n\n$3$. The collection of these $B$ bootstrap medians, $\\{\\hat{m}_1^*, \\hat{m}_2^*, \\dots, \\hat{m}_B^*\\}$, constitutes an empirical approximation of the sampling distribution of the sample median $\\hat{m}$.\n\n$4$. The bootstrap estimate of the standard error of the sample median, $\\text{SE}_{\\text{boot}}(\\hat{m})$, is then calculated as the sample standard deviation of this collection of bootstrap medians:\n$$ \\text{SE}_{\\text{boot}}(\\hat{m}) = \\sqrt{ \\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{m}_b^* - \\bar{m}^*)^2 } $$\nwhere $\\bar{m}^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{m}_b^*$ is the mean of the $B$ bootstrap medians. The use of the $B-1$ denominator (Bessel's correction) is standard practice for estimating a population standard deviation from a sample, which is analogous to our current task of estimating the standard deviation of the sampling distribution from our bootstrap-generated sample of medians.\n\nTo ensure reproducibility, the entire process is executed using a pseudorandom number generator initialized with a fixed seed, $s=20231111$. This guarantees that the sequence of bootstrap samples is identical upon every execution, leading to a deterministic final result.\n\nThe algorithm to be implemented is as follows for each test case:\na. Initialize a pseudorandom number generator with the specified seed $s=20231111$. This is done only once for the entire script to ensure a continuous stream of random numbers across all test cases.\nb. For a given data sample of size $n=20$, create an array to store $B=1000$ bootstrap median values.\nc. Loop $B$ times:\n    i. Generate a bootstrap resample of size $n=20$ by choosing elements from the original sample with replacement.\n    ii. Compute the median of this resample.\n    iii. Store the computed median.\nd. After the loop, calculate the sample standard deviation of the $B$ stored medians using the formula above (i.e., with a divisor of $B-1$).\ne. Round the resulting standard error to $6$ decimal places.\nf. Repeat for all four test cases and format the results as specified.\nThis entire procedure is implemented in the provided Python code, utilizing the `numpy` library for efficient array manipulation, random sampling, and statistical calculations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bootstrap estimate of the standard error of the sample median\n    for four given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. General variability, typical adult values\n        [118, 122, 135, 128, 150, 142, 130, 125, 138, 145, 132, 129, 140, 136, 127, 133, 149, 121, 137, 134],\n        # 2. Presence of a high outlier\n        [110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 240],\n        # 3. Multiple ties (repeated measurements)\n        [130, 130, 130, 130, 135, 135, 135, 140, 140, 140, 145, 145, 145, 150, 150, 150, 155, 155, 160, 160],\n        # 4. Degenerate case (all equal)\n        [130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130]\n    ]\n\n    # Parameters from the problem statement\n    n = 20  # Sample size\n    B = 1000 # Number of bootstrap replicates\n    seed = 20231111 # RNG seed\n\n    # Initialize the pseudorandom number generator once for reproducibility\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for data in test_cases:\n        # Convert data to a NumPy array for efficient computation\n        original_sample = np.array(data)\n        \n        # Array to store the median from each bootstrap replicate\n        bootstrap_medians = np.empty(B, dtype=np.float64)\n\n        # Main bootstrap loop\n        for i in range(B):\n            # Generate a bootstrap resample of size n by sampling with replacement\n            resample = rng.choice(original_sample, size=n, replace=True)\n            \n            # Compute and store the median of the resample\n            bootstrap_medians[i] = np.median(resample)\n\n        # Compute the bootstrap estimate of the standard error of the median.\n        # This is the empirical standard deviation of the distribution of bootstrap medians.\n        # ddof=1 is used for sample standard deviation (Bessel's correction).\n        se_median = np.std(bootstrap_medians, ddof=1)\n        \n        # Round the result to six decimal places\n        rounded_result = round(se_median, 6)\n        \n        results.append(rounded_result)\n\n    # Format the final list of results into the specified output string.\n    # The f-string format specifier ensures each float is printed with six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4948755"}, {"introduction": "Building on the concept of the bootstrap distribution, this practice demonstrates how to move from simply estimating standard errors to constructing confidence intervals. You will apply the intuitive bootstrap percentile method to estimate a confidence interval for an infection proportion, a common parameter in biostatistics. This exercise also involves a valuable comparison between the bootstrap-derived standard error and a classic analytical robust variance estimator, deepening your understanding of how these different approaches relate to one another. [@problem_id:4948632]", "problem": "You are given a binary infection status dataset consisting of independent outcomes $Y_i \\in \\{0,1\\}$ for $i=1,\\dots,n$, where $n=50$. The parameter of interest is the population infection proportion $p \\in [0,1]$. The estimator considered is the sample proportion, defined as the sample mean $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i$. Using nonparametric resampling, construct a bootstrap confidence interval for $\\hat{p}$ by the percentile method and estimate variability via a robust variance estimator.\n\nStarting from first principles, assume the outcomes are independent and identically distributed under the empirical distribution of the observed data. The bootstrap approach approximates the sampling distribution of $\\hat{p}$ by repeatedly resampling with replacement from the observed binary outcomes and recomputing $\\hat{p}$ on each resample. The percentile method uses empirical quantiles of the bootstrap distribution of $\\hat{p}$ to form a confidence interval. Separately, a robust variance estimator for $\\hat{p}$ may be obtained from the empirical second moment of $Y_i$ about $\\hat{p}$, scaled appropriately by $n$, reflecting the Eicker-Huber-White style of variance estimation for the sample mean.\n\nYour task is to implement the following, all in purely mathematical and algorithmic terms:\n\n1. For each dataset, generate $B=2000$ nonparametric bootstrap resamples of size $n=50$ by sampling with replacement from the observed $Y_i$, recompute $\\hat{p}$ on each resample, and use the percentile method to obtain a two-sided confidence interval with nominal level $1-\\alpha=0.95$ (so $\\alpha=0.05$). Use empirical quantiles at levels $\\alpha/2$ and $1-\\alpha/2$.\n2. Compute the robust standard error of $\\hat{p}$ based on the empirical second moment about $\\hat{p}$ divided by $n$, and take the square root to obtain the standard error.\n3. Compute the bootstrap standard error of $\\hat{p}$ as the sample standard deviation across the $B$ bootstrap estimates.\n\nUse a fixed pseudorandom seed $\\text{seed}=2025$ to ensure reproducibility of the bootstrap procedure.\n\nTest Suite:\nConsider $4$ infection datasets, each of length $n=50$:\n- Dataset $1$: $25$ ones and $25$ zeros.\n- Dataset $2$: $5$ ones and $45$ zeros.\n- Dataset $3$: $0$ ones and $50$ zeros.\n- Dataset $4$: $50$ ones and $0$ zeros.\n\nFor each dataset, compute and return a list of $5$ floats in the order:\n$[\\hat{p}, \\text{CI}_{\\text{lower}}, \\text{CI}_{\\text{upper}}, \\text{SE}_{\\text{robust}}, \\text{SE}_{\\text{bootstrap}}]$,\nwhere $\\hat{p}$ is the sample proportion, $\\text{CI}_{\\text{lower}}$ and $\\text{CI}_{\\text{upper}}$ are the percentile bootstrap confidence interval bounds at levels $\\alpha/2$ and $1-\\alpha/2$, $\\text{SE}_{\\text{robust}}$ is the robust standard error derived from the empirical second moment divided by $n$, and $\\text{SE}_{\\text{bootstrap}}$ is the bootstrap standard error computed as the sample standard deviation of the $B$ bootstrap estimates.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the $4$ datasets as a comma-separated list enclosed in square brackets, where each dataset’s result is itself a list of the $5$ floats rounded to six decimal places. For example, the output must be of the form $[\\,[r_{11},r_{12},r_{13},r_{14},r_{15}],\\,[r_{21},r_{22},r_{23},r_{24},r_{25}],\\,[r_{31},r_{32},r_{33},r_{34},r_{35}],\\,[r_{41},r_{42},r_{43},r_{44},r_{45}]\\,]$ with each $r_{jk}$ being a float presented to six decimal places.", "solution": "The user-provided problem has been analyzed and is deemed valid. It is scientifically sound, well-posed, and objective. All necessary data and definitions are provided, and there are no contradictions. The problem represents a standard application of fundamental biostatistical methods.\n\nThe task is to analyze four distinct datasets of binary outcomes using nonparametric bootstrap and robust variance estimation techniques. For each dataset, we must compute five quantities: the sample proportion ($\\hat{p}$), a $95\\%$ percentile bootstrap confidence interval ($[\\text{CI}_{\\text{lower}}, \\text{CI}_{\\text{upper}}]$), a robust standard error ($\\text{SE}_{\\text{robust}}$), and a bootstrap standard error ($\\text{SE}_{\\text{bootstrap}}$).\n\nLet a given dataset be a collection of $n$ independent and identically distributed binary outcomes $Y = \\{Y_1, Y_2, \\dots, Y_n\\}$, where $Y_i \\in \\{0, 1\\}$. The sample size is given as $n=50$.\n\n**1. Sample Proportion ($\\hat{p}$)**\nThe estimator for the population proportion $p$ is the sample proportion $\\hat{p}$, which is the arithmetic mean of the observed outcomes.\n$$\n\\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\n\n**2. Nonparametric Bootstrap and Percentile Confidence Interval**\nThe bootstrap method is used to approximate the sampling distribution of the estimator $\\hat{p}$.\nThe procedure is as follows:\na. Generate $B=2000$ bootstrap samples. Each bootstrap sample, denoted $Y_j^* = \\{Y_{j,1}^*, \\dots, Y_{j,n}^*\\}$ for $j \\in \\{1, \\dots, B\\}$, is created by drawing $n$ observations from the original dataset $Y$ with replacement.\nb. For each bootstrap sample $Y_j^*$, compute the sample proportion:\n$$\n\\hat{p}_j^* = \\frac{1}{n} \\sum_{i=1}^{n} Y_{j,i}^*\n$$\nThis results in a collection of $B$ bootstrap estimates $\\{\\hat{p}_1^*, \\hat{p}_2^*, \\dots, \\hat{p}_B^*\\}$.\nc. The percentile confidence interval is constructed from the empirical quantiles of the bootstrap distribution. For a nominal confidence level of $1-\\alpha = 0.95$, we have $\\alpha=0.05$. The lower and upper bounds of the confidence interval are the $100(\\alpha/2)$-th and $100(1-\\alpha/2)$-th percentiles of the sorted bootstrap estimates, respectively.\nLet $\\hat{q}_{\\tau}$ be the $\\tau$-th quantile of the set $\\{\\hat{p}_j^*\\}$. The confidence interval is:\n$$\n[\\text{CI}_{\\text{lower}}, \\text{CI}_{\\text{upper}}] = [\\hat{q}_{\\alpha/2}, \\hat{q}_{1-\\alpha/2}] = [\\hat{q}_{0.025}, \\hat{q}_{0.975}]\n$$\nAll bootstrap resampling will be performed using a pseudorandom number generator initialized with a fixed seed of $\\text{seed}=2025$ for reproducibility.\n\n**3. Robust Standard Error ($\\text{SE}_{\\text{robust}}$)**\nThe robust standard error is derived from the Eicker-Huber-White variance estimator for the sample mean. The variance of $\\hat{p}$ is estimated as:\n$$\n\\widehat{\\text{Var}}_{\\text{robust}}(\\hat{p}) = \\frac{1}{n^2} \\sum_{i=1}^{n} (Y_i - \\hat{p})^2\n$$\nThis formula represents the sample variance of the data (using $n$ in the denominator) divided by $n$. For binary data $Y_i \\in \\{0, 1\\}$, the sum of squared deviations simplifies significantly:\n$$\n\\sum_{i=1}^{n} (Y_i - \\hat{p})^2 = n\\hat{p}(1-\\hat{p})\n$$\nSubstituting this into the variance formula yields:\n$$\n\\widehat{\\text{Var}}_{\\text{robust}}(\\hat{p}) = \\frac{n\\hat{p}(1-\\hat{p})}{n^2} = \\frac{\\hat{p}(1-\\hat{p})}{n}\n$$\nThe robust standard error is the square root of this variance:\n$$\n\\text{SE}_{\\text{robust}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\n\n**4. Bootstrap Standard Error ($\\text{SE}_{\\text{bootstrap}}$)**\nThe bootstrap standard error is an alternative measure of the variability of $\\hat{p}$. It is estimated by the sample standard deviation of the $B$ bootstrap estimates $\\{\\hat{p}_1^*, \\dots, \\hat{p}_B^*\\}$.\n$$\n\\text{SE}_{\\text{bootstrap}} = \\sqrt{\\frac{1}{B-1} \\sum_{j=1}^{B} (\\hat{p}_j^* - \\bar{p}^*)^2}\n$$\nwhere $\\bar{p}^* = \\frac{1}{B} \\sum_{j=1}^{B} \\hat{p}_j^*$ is the mean of the bootstrap estimates.\n\n**5. Application to Test Datasets**\nThe above procedures will be applied to each of the four specified datasets:\n- **Dataset 1:** $n=50$, with $25$ ones and $25$ zeros. Thus, $\\hat{p} = 25/50 = 0.5$.\n- **Dataset 2:** $n=50$, with $5$ ones and $45$ zeros. Thus, $\\hat{p} = 5/50 = 0.1$.\n- **Dataset 3:** $n=50$, with $0$ ones and $50$ zeros. Thus, $\\hat{p} = 0/50 = 0.0$.\n- **Dataset 4:** $n=50$, with $50$ ones and $0$ zeros. Thus, $\\hat{p} = 50/50 = 1.0$.\n\nFor datasets $3$ and $4$, the original data lacks variability. Consequently, any bootstrap resample will be identical to the original sample. This results in a degenerate bootstrap distribution where all $\\hat{p}_j^*$ are equal to the original $\\hat{p}$. Therefore, the percentile confidence interval will have zero width (e.g., $[0,0]$ for dataset $3$), and the bootstrap standard error will be $0$. The robust standard error will also be $0$ since $\\hat{p}(1-\\hat{p})=0$ for $\\hat{p}=0$ or $\\hat{p}=1$. The algorithmic implementation will handle these cases naturally.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes statistical measures for binary infection datasets using\n    nonparametric bootstrap and robust variance estimation.\n    \"\"\"\n\n    def analyze_dataset(y_obs, n_obs, b_resamples, alpha_level, rng):\n        \"\"\"\n        Performs the full analysis for a single dataset.\n        \n        Args:\n            y_obs (np.ndarray): The observed binary data.\n            n_obs (int): The sample size.\n            b_resamples (int): The number of bootstrap resamples.\n            alpha_level (float): The significance level for the confidence interval.\n            rng (np.random.Generator): The random number generator.\n            \n        Returns:\n            list: A list of 5 floats: [p_hat, ci_lower, ci_upper, se_robust, se_bootstrap].\n        \"\"\"\n        # 1. Sample Proportion\n        p_hat = np.mean(y_obs)\n\n        # 2. Nonparametric Bootstrap\n        p_hat_bootstrap_dist = np.zeros(b_resamples)\n        for j in range(b_resamples):\n            y_resample = rng.choice(y_obs, size=n_obs, replace=True)\n            p_hat_bootstrap_dist[j] = np.mean(y_resample)\n            \n        # 3. Percentile Confidence Interval\n        ci_lower = np.quantile(p_hat_bootstrap_dist, alpha_level / 2.0)\n        ci_upper = np.quantile(p_hat_bootstrap_dist, 1.0 - alpha_level / 2.0)\n        \n        # 4. Robust Standard Error\n        var_robust = (p_hat * (1.0 - p_hat)) / n_obs\n        se_robust = np.sqrt(var_robust) if var_robust = 0 else 0.0\n\n        # 5. Bootstrap Standard Error\n        # Use ddof=1 for sample standard deviation.\n        se_bootstrap = np.std(p_hat_bootstrap_dist, ddof=1)\n        \n        return [p_hat, ci_lower, ci_upper, se_robust, se_bootstrap]\n\n    # --- Problem Parameters ---\n    n = 50\n    B = 2000\n    alpha = 0.05\n    seed = 2025\n\n    # --- Test Suite ---\n    test_cases = [\n        # Dataset 1: 25 ones, 25 zeros\n        np.concatenate([np.ones(25, dtype=np.int8), np.zeros(25, dtype=np.int8)]),\n        # Dataset 2: 5 ones, 45 zeros\n        np.concatenate([np.ones(5, dtype=np.int8), np.zeros(45, dtype=np.int8)]),\n        # Dataset 3: 0 ones, 50 zeros\n        np.zeros(50, dtype=np.int8),\n        # Dataset 4: 50 ones, 0 zeros\n        np.ones(50, dtype=np.int8),\n    ]\n\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    all_results = []\n    for y_data in test_cases:\n        result = analyze_dataset(y_data, n, B, alpha, rng)\n        all_results.append(result)\n\n    # --- Final Output Formatting ---\n    # Construct the final output string to match the required format:\n    # [[r11,r12,...],[r21,r22,...],...] with 6 decimal places.\n    # No spaces after commas.\n    list_of_lists_str = []\n    for res_list in all_results:\n        formatted_numbers = [f\"{num:.6f}\" for num in res_list]\n        list_str = f\"[{','.join(formatted_numbers)}]\"\n        list_of_lists_str.append(list_str)\n        \n    final_output = f\"[{','.join(list_of_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "4948632"}, {"introduction": "This final practice showcases the remarkable flexibility of bootstrap principles in tackling specific challenges that arise in regression modeling. You will explore a scenario with heteroskedasticity—where the error variance is not constant—a common issue in biostatistical data. You will implement the wild bootstrap, a sophisticated variant that provides robust standard errors for regression coefficients by cleverly preserving the data's variance structure, demonstrating how resampling techniques can be adapted for reliable inference even when standard model assumptions are violated. [@problem_id:4948717]", "problem": "Consider a simple linear regression model appropriate for biostatistics applications where conditional error variance increases with a biomarker level. Let observations be indexed by $i \\in \\{1,\\dots,n\\}$, with a scalar predictor (biomarker) $x_i$ and an outcome $y_i$. Assume the data are generated from the data-generating process\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\varepsilon_i,\n$$\nwhere $(\\varepsilon_i \\mid x_i)$ has conditional mean $E[\\varepsilon_i \\mid x_i] = 0$ and conditional variance $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma_0^2 \\, \\bigl(1 + \\alpha x_i\\bigr)^2$, with $\\sigma_0  0$ and $\\alpha \\ge 0$. The increasing variance in $x_i$ induces heteroskedasticity. The ordinary least squares estimator $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^\\top$ minimizes the sum of squared residuals $\\sum_{i=1}^n \\bigl(y_i - \\beta_0 - \\beta_1 x_i\\bigr)^2$.\n\nYou must compute heteroskedasticity-robust uncertainty via the wild bootstrap using Mammen multipliers. The wild bootstrap uses residuals $\\hat{e}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$ and generates bootstrap pseudo-responses $y_i^{\\ast} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{e}_i v_i$, where $v_i$ are independent and identically distributed across $i$ from the Mammen two-point distribution defined by\n$$\nv_i \\;=\\; \\frac{1 - \\sqrt{5}}{2} \\quad \\text{with probability} \\quad \\frac{\\sqrt{5} + 1}{2\\sqrt{5}}, \n\\qquad\nv_i \\;=\\; \\frac{1 + \\sqrt{5}}{2} \\quad \\text{with probability} \\quad \\frac{\\sqrt{5} - 1}{2\\sqrt{5}}.\n$$\nFor each bootstrap sample, refit ordinary least squares to $\\{(x_i, y_i^\\ast)\\}_{i=1}^n$ to obtain $\\hat{\\beta}^{\\ast} = (\\hat{\\beta}^{\\ast}_0, \\hat{\\beta}^{\\ast}_1)^\\top$. Repeat this for $B$ bootstrap replicates and estimate the wild bootstrap standard error for each component of $\\hat{\\beta}$ as the sample standard deviation across the $B$ bootstrap estimates. Use $B = 2000$.\n\nYour program must implement this procedure and return the wild bootstrap standard errors for the intercept and slope for multiple synthetic test datasets constructed as follows. For each test case, generate $x_i$ independently from a continuous uniform distribution on a specified interval, then generate $\\varepsilon_i$ independently from a normal distribution with mean $0$ and standard deviation $\\sigma_0 (1 + \\alpha x_i)$, and finally set $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. All randomness must be controlled by a fixed seed per test case to ensure reproducibility.\n\nUse the following test suite of four test cases, each specified by $(\\text{seed}, n, \\beta_0, \\beta_1, \\sigma_0, \\alpha, x_{\\min}, x_{\\max})$:\n- Test case $1$: $(\\,123,\\, 60,\\, 0.5,\\, 1.2,\\, 1.0,\\, 0.5,\\, 0,\\, 5\\,)$.\n- Test case $2$: $(\\,42,\\, 30,\\, -0.3,\\, 2.0,\\, 0.5,\\, 1.0,\\, 0,\\, 3\\,)$.\n- Test case $3$: $(\\,777,\\, 200,\\, 0.0,\\, 0.8,\\, 0.8,\\, 0.2,\\, 0,\\, 10\\,)$.\n- Test case $4$: $(\\,999,\\, 40,\\, 1.0,\\, -0.5,\\, 1.2,\\, 0.0,\\, 0,\\, 4\\,)$.\n\nFor each test case:\n- Set the pseudorandom number generator with the given $\\text{seed}$.\n- Draw $x_i \\sim \\mathrm{Uniform}(x_{\\min}, x_{\\max})$ independently for $i \\in \\{1,\\dots,n\\}$.\n- Draw $\\varepsilon_i \\sim \\mathcal{N}\\!\\left(0, \\bigl[\\sigma_0 (1 + \\alpha x_i)\\bigr]^2\\right)$ independently across $i$.\n- Form $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n- Fit ordinary least squares with an intercept to obtain $\\hat{\\beta}$ and residuals $\\hat{e}_i$.\n- Perform the wild bootstrap with Mammen multipliers and $B = 2000$ to obtain the bootstrap standard errors for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n\nFinal output format requirement:\n- Your program must produce a single line containing a comma-separated list enclosed in square brackets.\n- The list must contain, in order, for each test case $k \\in \\{1,2,3,4\\}$, first the wild bootstrap standard error for the intercept $\\hat{\\beta}_0$ and then for the slope $\\hat{\\beta}_1$.\n- There are thus $8$ numbers total.\n- Each number must be rounded to $6$ decimal places.\n- Example of the required formatting (with placeholders): $[\\text{se}_{0,1},\\text{se}_{1,1},\\text{se}_{0,2},\\text{se}_{1,2},\\text{se}_{0,3},\\text{se}_{1,3},\\text{se}_{0,4},\\text{se}_{1,4}]$.", "solution": "The problem requires the computation of heteroskedasticity-robust standard errors for the coefficients of a simple linear regression model using the wild bootstrap method with Mammen multipliers.\n\n### Step 1: Problem Validation\n\nThe first step is to validate the problem statement.\n\n#### 1.1. Extraction of Givens\nThe problem provides the following information:\n- **Model**: A simple linear regression model $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n- **Data Generating Process (DGP)**:\n    - The conditional mean of the error term is $E[\\varepsilon_i \\mid x_i] = 0$.\n    - The conditional variance of the error term is heteroskedastic: $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma_0^2 (1 + \\alpha x_i)^2$, with $\\sigma_0  0$ and $\\alpha \\ge 0$.\n    - For simulation, covariates $x_i$ are drawn independently from a continuous uniform distribution, $x_i \\sim \\mathrm{Uniform}(x_{\\min}, x_{\\max})$.\n    - Error terms $\\varepsilon_i$ are drawn independently from a normal distribution, $\\varepsilon_i \\sim \\mathcal{N}(0, [\\sigma_0 (1 + \\alpha x_i)]^2)$.\n- **Estimator**: The ordinary least squares (OLS) estimator $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^\\top$, which minimizes the sum of squared residuals.\n- **Uncertainty Estimation Method**: Wild bootstrap with Mammen multipliers.\n    - Residuals are calculated as $\\hat{e}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$.\n    - Bootstrap pseudo-responses are generated as $y_i^{\\ast} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{e}_i v_i$.\n    - The multipliers $v_i$ are drawn independently from the Mammen two-point distribution:\n        - $v_i = \\frac{1 - \\sqrt{5}}{2}$ with probability $p_1 = \\frac{\\sqrt{5} + 1}{2\\sqrt{5}}$.\n        - $v_i = \\frac{1 + \\sqrt{5}}{2}$ with probability $p_2 = \\frac{\\sqrt{5} - 1}{2\\sqrt{5}}$.\n    - For each bootstrap sample, an OLS model is refit to get $\\hat{\\beta}^{\\ast} = (\\hat{\\beta}^{\\ast}_0, \\hat{\\beta}^{\\ast}_1)^\\top$.\n    - The number of bootstrap replicates is $B = 2000$.\n    - The standard error for each coefficient is the sample standard deviation of the $B$ bootstrap estimates.\n- **Test Cases**: Four specific sets of parameters $(\\text{seed}, n, \\beta_0, \\beta_1, \\sigma_0, \\alpha, x_{\\min}, x_{\\max})$ are provided.\n    1. $(123, 60, 0.5, 1.2, 1.0, 0.5, 0, 5)$\n    2. $(42, 30, -0.3, 2.0, 0.5, 1.0, 0, 3)$\n    3. $(777, 200, 0.0, 0.8, 0.8, 0.2, 0, 10)$\n    4. $(999, 40, 1.0, -0.5, 1.2, 0.0, 0, 4)$\n- **Output Format**: A single list of $8$ numbers (SE for intercept, SE for slope for each of the $4$ cases), rounded to $6$ decimal places.\n\n#### 1.2. Validation against Criteria\n- **Scientifically Grounded**: The problem is grounded in established statistical theory. Linear regression, heteroskedasticity, and the wild bootstrap are standard topics in biostatistics and econometrics. The specified variance structure is a plausible model for phenomena where variability increases with the level of a biomarker. The Mammen distribution is a standard and theoretically justified choice for multipliers in wild bootstrap.\n- **Well-Posed**: The problem is well-posed. It provides all necessary parameters, a clear step-by-step procedure, and specific random seeds for reproducibility. This ensures that a unique and meaningful numerical solution exists.\n- **Objective**: The problem is stated in precise, objective, and mathematical language. There are no subjective or ambiguous terms.\n\nThe problem statement does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, feasible, and well-structured.\n\n#### 1.3. Verdict\nThe problem is **valid**.\n\n### Step 2: Solution Derivation\n\nThe solution involves implementing a computational statistics algorithm. The core task is to estimate the standard errors of OLS coefficients in the presence of heteroskedasticity.\n\n#### 2.1. Theoretical Framework\nThe assumed model is $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. In matrix form, this is $\\mathbf{y} = \\mathbf{X}\\beta + \\varepsilon$, where $\\mathbf{y}$ is the $n \\times 1$ vector of outcomes, $\\mathbf{X}$ is the $n \\times 2$ design matrix with a column of ones and a column of predictor values $x_i$, $\\beta = (\\beta_0, \\beta_1)^\\top$ is the vector of coefficients, and $\\varepsilon$ is the vector of errors.\n\nThe OLS estimator is given by $\\hat{\\beta} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$. Under the standard assumption of homoskedasticity (i.e., $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2$ for all $i$), the variance of $\\hat{\\beta}$ is $\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2(\\mathbf{X}^\\top\\mathbf{X})^{-1}$. However, the problem specifies heteroskedastic errors, $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma_i^2 = \\sigma_0^2 (1 + \\alpha x_i)^2$. In this case, the true variance of $\\hat{\\beta}$ is the sandwich estimator form: $\\mathrm{Var}(\\hat{\\beta}) = (\\mathbf{X}^\\top\\mathbf{X})^{-1}(\\mathbf{X}^\\top\\mathbf{\\Omega}\\mathbf{X})(\\mathbf{X}^\\top\\mathbf{X})^{-1}$, where $\\mathbf{\\Omega}$ is a diagonal matrix with diagonal entries $\\sigma_i^2$.\n\nSince $\\mathbf{\\Omega}$ is unknown, we need to estimate this variance. The wild bootstrap is a resampling method particularly suited for this. It generates bootstrap samples in a way that preserves the heteroskedasticity structure found in the original data.\n\nThe procedure is as follows:\n1.  Fit the OLS model to the original data $(\\mathbf{y}, \\mathbf{X})$ to obtain the estimates $\\hat{\\beta}$ and the residuals $\\hat{e}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$.\n2.  Generate $B$ bootstrap samples. For each bootstrap replicate $b \\in \\{1, \\dots, B\\}$:\n    a.  For each observation $i \\in \\{1, \\dots, n\\}$, draw a random value $v_i$ from a distribution with mean $0$ and variance $1$. The problem specifies the Mammen two-point distribution.\n    b.  Construct a bootstrap pseudo-response $y_i^{\\ast} = \\hat{y}_i + \\hat{e}_i v_i$. This is equivalent to creating a bootstrap error term $\\varepsilon_i^{\\ast} = \\hat{e}_i v_i$.\n    c.  Fit an OLS model using the original predictors $x_i$ and the new responses $y_i^{\\ast}$ to get a bootstrap coefficient estimate $\\hat{\\beta}^{\\ast,b}$.\n3.  The collection of bootstrap estimates $\\{\\hat{\\beta}^{\\ast,1}, \\dots, \\hat{\\beta}^{\\ast,B}\\}$ forms an empirical distribution that approximates the true sampling distribution of $\\hat{\\beta}$.\n4.  The bootstrap standard error for each coefficient is the sample standard deviation of the corresponding bootstrap estimates. For $\\hat{\\beta}_j$, the standard error is $\\mathrm{SE}(\\hat{\\beta}_j) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B (\\hat{\\beta}_j^{\\ast,b} - \\bar{\\beta}_j^{\\ast})^2}$, where $\\bar{\\beta}_j^{\\ast} = \\frac{1}{B}\\sum_{b=1}^B \\hat{\\beta}_j^{\\ast,b}$.\n\nThe choice of the Mammen distribution for $v_i$ is deliberate. It has $E[v_i] = 0$, $E[v_i^2] = 1$, and $E[v_i^3] = 1$. The first two moments ensure that in the bootstrap world, conditional on the original data, $E^{\\ast}[\\varepsilon_i^{\\ast} \\mid x_i] = \\hat{e}_i E[v_i] = 0$ and $\\mathrm{Var}^{\\ast}(\\varepsilon_i^{\\ast} \\mid x_i) = \\hat{e}_i^2 E[v_i^2] = \\hat{e}_i^2$. Since $\\hat{e}_i^2$ is an observation-specific estimate of the true error variance $\\sigma_i^2$, this procedure generates bootstrap data with a variance structure that mimics the heteroskedasticity of the original data. The third moment property helps in obtaining higher-order refinements for confidence intervals, though it is not strictly necessary for standard error estimation.\n\n#### 2.2. Algorithmic Implementation\nFor each test case defined by $(\\text{seed}, n, \\beta_0, \\beta_1, \\sigma_0, \\alpha, x_{\\min}, x_{\\max})$:\n1.  **Initialize RNG**: Set the seed of the pseudorandom number generator for reproducibility.\n2.  **Generate Data**:\n    - Draw $n$ values for the predictor $x_i$ from $\\mathrm{Uniform}(x_{\\min}, x_{\\max})$.\n    - For each $x_i$, compute the specific standard deviation $\\sigma_i = \\sigma_0 (1 + \\alpha x_i)$.\n    - Draw $n$ error terms $\\varepsilon_i$ from $\\mathcal{N}(0, \\sigma_i^2)$.\n    - Construct the outcome variable $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n3.  **Initial OLS Fit**:\n    - Construct the design matrix $\\mathbf{X}$ of size $n \\times 2$ with first column being all ones and second column being the vector of $x_i$.\n    - Compute the OLS estimate $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^\\top$ by solving the normal equations, for instance, using a least-squares solver on $\\mathbf{y}$ and $\\mathbf{X}$.\n    - Compute the fitted values $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\beta}$.\n    - Compute the residuals $\\hat{\\mathbf{e}} = \\mathbf{y} - \\hat{\\mathbf{y}}$.\n4.  **Wild Bootstrap Loop**:\n    - Initialize a storage matrix for bootstrap coefficients, e.g., a $B \\times 2$ NumPy array, where $B=2000$.\n    - Define the values and probabilities for the Mammen multipliers:\n        - $v_1 = (1 - \\sqrt{5})/2$, $p_1 = (\\sqrt{5} + 1)/(2\\sqrt{5})$.\n        - $v_2 = (1 + \\sqrt{5})/2$, $p_2 = 1 - p_1$.\n    - Loop $B$ times:\n        a. Generate an $n \\times 1$ vector of multipliers $\\mathbf{v}$ by drawing from the Mammen distribution.\n        b. Create the bootstrap response vector $\\mathbf{y}^{\\ast} = \\hat{\\mathbf{y}} + \\hat{\\mathbf{e}} \\odot \\mathbf{v}$, where $\\odot$ denotes element-wise multiplication.\n        c. Compute the bootstrap OLS estimate $\\hat{\\beta}^{\\ast}$ by solving the least-squares problem for $\\mathbf{y}^{\\ast}$ and $\\mathbf{X}$.\n        d. Store $\\hat{\\beta}^{\\ast}$ as a row in the storage matrix.\n5.  **Compute Standard Errors**:\n    - After the loop, calculate the sample standard deviation (with $1$ degree of freedom correction, `ddof=1`) for each column of the storage matrix. These are the wild bootstrap standard errors for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n6.  **Store and format results**: The two computed standard errors are appended to a list that aggregates results from all test cases. The final list is formatted as a string according to the problem specification.\n\nThis procedure will be repeated for all four test cases.", "answer": "```python\nimport numpy as np\n\ndef compute_wild_bootstrap_se(seed, n, beta0, beta1, sigma0, alpha, x_min, x_max, B=2000):\n    \"\"\"\n    Computes heteroskedasticity-robust standard errors using the wild bootstrap.\n\n    Args:\n        seed (int): The seed for the random number generator.\n        n (int): Sample size.\n        beta0 (float): True intercept.\n        beta1 (float): True slope.\n        sigma0 (float): Baseline standard deviation parameter.\n        alpha (float): Heteroskedasticity parameter.\n        x_min (float): Minimum of the uniform distribution for x.\n        x_max (float): Maximum of the uniform distribution for x.\n        B (int): Number of bootstrap replicates.\n\n    Returns:\n        tuple: A tuple containing the bootstrap standard errors for the intercept and slope.\n    \"\"\"\n    # 1. Initialize RNG and Generate Data\n    rng = np.random.default_rng(seed)\n    \n    # Generate predictors x_i\n    x = rng.uniform(x_min, x_max, size=n)\n    \n    # Generate heteroskedastic errors\n    true_sd = sigma0 * (1 + alpha * x)\n    epsilon = rng.normal(0, true_sd)\n    \n    # Generate outcomes y_i\n    y = beta0 + beta1 * x + epsilon\n    \n    # 2. Initial OLS Fit\n    # Construct design matrix X with an intercept\n    X = np.c_[np.ones(n), x]\n    \n    # Compute OLS estimates beta_hat\n    beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n    \n    # Compute fitted values and residuals\n    y_hat = X @ beta_hat\n    e_hat = y - y_hat\n    \n    # 3. Wild Bootstrap Loop\n    beta_boot_samples = np.zeros((B, 2))\n    \n    # Define Mammen multipliers\n    sqrt5 = np.sqrt(5)\n    v1 = (1 - sqrt5) / 2\n    v2 = (1 + sqrt5) / 2\n    p1 = (sqrt5 + 1) / (2 * sqrt5)\n    p2 = 1 - p1\n    mammen_values = [v1, v2]\n    mammen_probs = [p1, p2]\n    \n    for i in range(B):\n        # Generate multipliers v_i\n        v = rng.choice(mammen_values, size=n, p=mammen_probs)\n        \n        # Create bootstrap responses y_star\n        y_star = y_hat + e_hat * v\n        \n        # Fit OLS on bootstrap sample\n        beta_star = np.linalg.lstsq(X, y_star, rcond=None)[0]\n        beta_boot_samples[i, :] = beta_star\n        \n    # 4. Compute Standard Errors\n    # The sample standard deviation of the bootstrap estimates\n    se_beta0 = np.std(beta_boot_samples[:, 0], ddof=1)\n    se_beta1 = np.std(beta_boot_samples[:, 1], ddof=1)\n    \n    return se_beta0, se_beta1\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Test suite: (seed, n, beta0, beta1, sigma0, alpha, x_min, x_max)\n    test_cases = [\n        (123, 60, 0.5, 1.2, 1.0, 0.5, 0, 5),\n        (42, 30, -0.3, 2.0, 0.5, 1.0, 0, 3),\n        (777, 200, 0.0, 0.8, 0.8, 0.2, 0, 10),\n        (999, 40, 1.0, -0.5, 1.2, 0.0, 0, 4),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        seed, n, beta0, beta1, sigma0, alpha, x_min, x_max = case\n        se0, se1 = compute_wild_bootstrap_se(seed, n, beta0, beta1, sigma0, alpha, x_min, x_max)\n        all_results.extend([se0, se1])\n\n    # Format the results as required\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "4948717"}]}