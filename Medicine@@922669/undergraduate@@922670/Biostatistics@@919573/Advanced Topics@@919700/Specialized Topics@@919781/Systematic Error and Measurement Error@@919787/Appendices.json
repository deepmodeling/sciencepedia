{"hands_on_practices": [{"introduction": "This problem is a cornerstone of biostatistical practice, addressing the fundamental question of how to separate true biological variability between individuals from the random error introduced by our measurement tools. By analyzing data with repeated measures per subject, you will use the powerful framework of Analysis of Variance (ANOVA) to derive estimators for these two distinct sources of variation. This skill is essential for assessing the reliability of a biomarker and understanding the structure of your data. [@problem_id:4956444]", "problem": "A biostatistics team is studying a biomarker using repeated measurements per individual to quantify measurement error and true between-subject variability. For $i \\in \\{1,\\dots,n\\}$ subjects and $j \\in \\{1,\\dots,r\\}$ technical replicates per subject, the observed measurement is modeled as $W_{ij} = X_i + U_{ij}$, where $X_i$ is the unobserved true subject-level quantity and $U_{ij}$ is the measurement error. Assume $X_i$ are independent and identically distributed with $\\mathbb{E}[X_i] = \\mu$ and $\\mathbb{V}(X_i) = \\sigma_{X}^{2}$, and $U_{ij}$ are independent and identically distributed with $\\mathbb{E}[U_{ij}] = 0$ and $\\mathbb{V}(U_{ij}) = \\sigma_{U}^{2}$, independent of $\\{X_i\\}$. Any potential systematic bias of the instrument is modeled as an unknown constant offset $b$ that is common to all observations, and the team is interested in variance components, which should be unaffected by $b$. Using the standard framework of Analysis of Variance (ANOVA), derive unbiased estimators for the within-subject variance component $\\sigma_{U}^{2}$ and the between-subject variance component $\\sigma_{X}^{2}$ under this balanced design. Express your final estimators as closed-form analytic expressions in terms of the observed data $\\{W_{ij}\\}$, the subject-specific means $\\bar{W}_{i\\cdot} = \\frac{1}{r}\\sum_{j=1}^{r} W_{ij}$, and the grand mean $\\bar{W}_{\\cdot\\cdot} = \\frac{1}{nr}\\sum_{i=1}^{n}\\sum_{j=1}^{r} W_{ij}$. Present the final answer as a row matrix ordered as $\\left(\\hat{\\sigma}_{U}^{2}, \\hat{\\sigma}_{X}^{2}\\right)$. No numerical rounding is required; provide exact symbolic expressions.", "solution": "The problem is well-posed and scientifically grounded. It describes a standard one-way random-effects model, common in biostatistics for assessing sources of variability. We can proceed to derive the estimators for the variance components $\\sigma_{U}^{2}$ and $\\sigma_{X}^{2}$ using the Analysis of Variance (ANOVA) framework.\n\nThe model for the observed measurement for subject $i$ and replicate $j$ is given as $W_{ij} = X_i + U_{ij}$. A systematic bias $b$ is stated to be a constant offset for all observations, so we can write the full model as $W_{ij} = b + X_i + U_{ij}$. The random variables $X_i$ represent the true subject-level quantities and are independent and identically distributed (i.i.d.) with mean $\\mathbb{E}[X_i] = \\mu$ and variance $\\mathbb{V}(X_i) = \\sigma_{X}^{2}$. The terms $U_{ij}$ represent the measurement errors and are i.i.d. with mean $\\mathbb{E}[U_{ij}] = 0$ and variance $\\mathbb{V}(U_{ij}) = \\sigma_{U}^{2}$. The sets of variables $\\{X_i\\}$ and $\\{U_{ij}\\}$ are mutually independent.\n\nOur goal is to find unbiased estimators for the within-subject variance component, $\\sigma_{U}^{2}$, and the between-subject variance component, $\\sigma_{X}^{2}$. We use the method of moments, which in this context involves calculating the expected values of the ANOVA mean squares and equating them to the observed mean squares.\n\nThe key quantities in a one-way ANOVA are the sums of squares.\nThe Within-Subject Sum of Squares ($SSW$) is defined as:\n$$SSW = \\sum_{i=1}^{n} \\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2$$\nwhere $\\bar{W}_{i\\cdot} = \\frac{1}{r} \\sum_{j=1}^{r} W_{ij}$ is the mean for subject $i$.\n\nThe Between-Subjects Sum of Squares ($SSB$) is defined as:\n$$SSB = \\sum_{i=1}^{n} \\sum_{j=1}^{r} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2 = r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2$$\nwhere $\\bar{W}_{\\cdot\\cdot} = \\frac{1}{nr} \\sum_{i=1}^{n} \\sum_{j=1}^{r} W_{ij}$ is the grand mean.\n\nThe corresponding Mean Squares are the sums of squares divided by their degrees of freedom.\nThe Mean Square Within ($MSW$) has $n(r-1)$ degrees of freedom:\n$$MSW = \\frac{SSW}{n(r-1)}$$\nThe Mean Square Between ($MSB$) has $n-1$ degrees of freedom:\n$$MSB = \\frac{SSB}{n-1}$$\n\nTo find unbiased estimators, we first compute the expected values of $MSW$ and $MSB$.\n\nLet's analyze the term $(W_{ij} - \\bar{W}_{i\\cdot})$:\n$\\bar{W}_{i\\cdot} = \\frac{1}{r} \\sum_{j=1}^{r} (b + X_i + U_{ij}) = b + X_i + \\frac{1}{r} \\sum_{j=1}^{r} U_{ij} = b + X_i + \\bar{U}_{i\\cdot}$.\n$W_{ij} - \\bar{W}_{i\\cdot} = (b + X_i + U_{ij}) - (b + X_i + \\bar{U}_{i\\cdot}) = U_{ij} - \\bar{U}_{i\\cdot}$.\nNotice that the systematic bias $b$ and the subject-specific true value $X_i$ cancel out, meaning $SSW$ and $MSW$ are not affected by them.\n\nThe expected value of $SSW$ is:\n$$\\mathbb{E}[SSW] = \\mathbb{E}\\left[\\sum_{i=1}^{n} \\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2\\right] = \\sum_{i=1}^{n} \\mathbb{E}\\left[\\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2\\right]$$\nFor any given subject $i$, the term $\\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2$ is $(r-1)$ times the sample variance of the $r$ i.i.d. error terms $\\{U_{ij}\\}_{j=1}^r$. The expected value of the sample variance is the population variance, $\\sigma_{U}^{2}$.\nTherefore, $\\mathbb{E}\\left[\\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2\\right] = (r-1)\\sigma_{U}^{2}$.\nSumming over all $n$ subjects, we get:\n$$\\mathbb{E}[SSW] = \\sum_{i=1}^{n} (r-1)\\sigma_{U}^{2} = n(r-1)\\sigma_{U}^{2}$$\nNow, we find the expectation of $MSW$:\n$$\\mathbb{E}[MSW] = \\frac{\\mathbb{E}[SSW]}{n(r-1)} = \\frac{n(r-1)\\sigma_{U}^{2}}{n(r-1)} = \\sigma_{U}^{2}$$\nThis shows that $MSW$ is an unbiased estimator for the within-subject variance $\\sigma_{U}^{2}$.\n\nNext, we compute the expected value of $MSB$. First, consider the terms $(\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})$:\n$\\bar{W}_{\\cdot\\cdot} = \\frac{1}{n} \\sum_{i=1}^{n} \\bar{W}_{i\\cdot} = \\frac{1}{n} \\sum_{i=1}^{n} (b + X_i + \\bar{U}_{i\\cdot}) = b + \\bar{X}_{\\cdot} + \\bar{U}_{\\cdot\\cdot}$.\n$\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot} = (b + X_i + \\bar{U}_{i\\cdot}) - (b + \\bar{X}_{\\cdot} + \\bar{U}_{\\cdot\\cdot}) = (X_i - \\bar{X}_{\\cdot}) + (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot})$.\nAgain, the bias $b$ cancels. The expected value of $SSB$ is:\n$$\\mathbb{E}[SSB] = \\mathbb{E}\\left[r \\sum_{i=1}^{n} ( (X_i - \\bar{X}_{\\cdot}) + (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot}) )^2\\right]$$\nDue to the independence of $X_i$ and $U_{ij}$, the expectation of the cross-product term is zero.\n$$\\mathbb{E}[SSB] = r \\mathbb{E}\\left[\\sum_{i=1}^{n} (X_i - \\bar{X}_{\\cdot})^2\\right] + r \\mathbb{E}\\left[\\sum_{i=1}^{n} (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot})^2\\right]$$\nThe first term involves the sample variance of $\\{X_i\\}$. Since $\\mathbb{E}[\\sum_{i=1}^{n} (X_i - \\bar{X}_{\\cdot})^2] = (n-1)\\mathbb{V}(X_i) = (n-1)\\sigma_{X}^{2}$.\nThe second term involves the sample variance of $\\{\\bar{U}_{i\\cdot}\\}$. The variables $\\bar{U}_{i\\cdot}$ are i.i.d. with mean $0$ and variance $\\mathbb{V}(\\bar{U}_{i\\cdot}) = \\mathbb{V}(\\frac{1}{r}\\sum_j U_{ij}) = \\frac{1}{r^2} \\sum_j \\mathbb{V}(U_{ij}) = \\frac{r\\sigma_{U}^{2}}{r^2} = \\frac{\\sigma_{U}^{2}}{r}$.\nSo, $\\mathbb{E}[\\sum_{i=1}^{n} (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot})^2] = (n-1)\\mathbb{V}(\\bar{U}_{i\\cdot}) = (n-1)\\frac{\\sigma_{U}^{2}}{r}$.\nCombining these results:\n$$\\mathbb{E}[SSB] = r(n-1)\\sigma_{X}^{2} + r(n-1)\\frac{\\sigma_{U}^{2}}{r} = r(n-1)\\sigma_{X}^{2} + (n-1)\\sigma_{U}^{2}$$\nNow, we find the expectation of $MSB$:\n$$\\mathbb{E}[MSB] = \\frac{\\mathbb{E}[SSB]}{n-1} = \\frac{r(n-1)\\sigma_{X}^{2} + (n-1)\\sigma_{U}^{2}}{n-1} = r\\sigma_{X}^{2} + \\sigma_{U}^{2}$$\nWe have a system of two equations for the two unknown variance components:\n$1$. $\\mathbb{E}[MSW] = \\sigma_{U}^{2}$\n$2$. $\\mathbb{E}[MSB] = r\\sigma_{X}^{2} + \\sigma_{U}^{2}$\n\nUsing the method of moments, we substitute the observed statistics for their expectations to obtain the estimators $\\hat{\\sigma}_{U}^{2}$ and $\\hat{\\sigma}_{X}^{2}$.\nFrom equation $1$, we get the estimator for the within-subject variance:\n$$\\hat{\\sigma}_{U}^{2} = MSW = \\frac{SSW}{n(r-1)} = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)}$$\nFrom equation $2$, we substitute $\\hat{\\sigma}_{U}^{2}$ for $\\sigma_{U}^{2}$:\n$MSB = r\\hat{\\sigma}_{X}^{2} + \\hat{\\sigma}_{U}^{2} = r\\hat{\\sigma}_{X}^{2} + MSW$.\nSolving for $\\hat{\\sigma}_{X}^{2}$:\n$$r\\hat{\\sigma}_{X}^{2} = MSB - MSW$$\n$$\\hat{\\sigma}_{X}^{2} = \\frac{MSB - MSW}{r}$$\nSubstituting the expressions for $MSB$ and $MSW$:\n$$\\hat{\\sigma}_{X}^{2} = \\frac{1}{r} \\left( \\frac{SSB}{n-1} - \\frac{SSW}{n(r-1)} \\right)$$\n$$\\hat{\\sigma}_{X}^{2} = \\frac{1}{r} \\left( \\frac{r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2}{n-1} - \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} \\right)$$\nThese two expressions for $\\hat{\\sigma}_{U}^{2}$ and $\\hat{\\sigma}_{X}^{2}$ are the required unbiased estimators.\n\nThe final estimators are:\nFor the within-subject variance component $\\sigma_U^2$:\n$$\\hat{\\sigma}_{U}^{2} = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)}$$\nFor the between-subject variance component $\\sigma_X^2$:\n$$\\hat{\\sigma}_{X}^{2} = \\frac{1}{r} \\left( \\frac{r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2}{n-1} - \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} \\right)$$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} & \\frac{1}{r} \\left( \\frac{r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2}{n-1} - \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} \\right) \\end{pmatrix} } $$", "id": "4956444"}, {"introduction": "Measurement error isn't just a theoretical nuisance; it has real-world costs in research, and this exercise demonstrates one of its most practical consequences: the loss of statistical power. You will explore how random error in an outcome measure can make it harder to detect a true treatment effect in a clinical trial, forcing investigators to recruit more participants. By deriving the sample size inflation factor, you will quantify the direct impact of measurement imprecision on study resources and feasibility. [@problem_id:4956457]", "problem": "Consider a balanced Randomized Controlled Trial (RCT) comparing a binary treatment ($X_{i} \\in \\{0,1\\}$) with equal allocation. The continuous outcome for participant $i$ follows the classical normal linear model\n$$\nY_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\varepsilon_{i},\n$$\nwhere $\\varepsilon_{i}$ are independent and identically distributed with $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$ and independent of $X_{i}$. The scientific estimand of interest is the treatment effect $\\beta_{1} = \\Delta \\neq 0$. Due to imperfect measurement, the observed outcome is subject to additive, independent measurement error:\n$$\nY_{i}^{*} = Y_{i} + V_{i}, \\quad V_{i} \\sim \\mathcal{N}(0,\\sigma_{V}^{2}),\n$$\nwith $V_{i}$ independent of $Y_{i}$ and $X_{i}$. An investigator uses the usual two-sample $t$-test (equivalently, ordinary least squares regression of $Y_{i}^{*}$ on $X_{i}$ with an intercept) to test $H_{0}:\\beta_{1}=0$ at a fixed significance level, and considers power through the noncentrality parameter of the $t$-statistic under the alternative $\\beta_{1}=\\Delta$.\n\nLet the total sample size be $n$, with $n/2$ individuals per arm. Using only the definitions of the $t$-statistic and the variance of sample means under independent, identically distributed normal errors, first derive the noncentrality parameter under $Y^{*}$ in terms of $\\Delta$, $n$, $\\sigma_{Y}^{2}$, and $\\sigma_{V}^{2}$. Then, by equating this to the noncentrality parameter obtained under the error-free outcome $Y$ (i.e., with variance $\\sigma_{Y}^{2}$), derive the sample size inflation factor $k = n^{*}/n$ required under $Y^{*}$ to maintain the same noncentrality parameter as with $Y$. Express your final answer as a single simplified analytic expression for $k$. No numerical evaluation or rounding is required, and no units should be included in the final expression.", "solution": "The user wants me to solve the problem by following these steps:\n1. Validate the problem statement.\n2. If valid, derive the noncentrality parameter for the t-test on the observed outcomes.\n3. Derive the sample size inflation factor required to maintain the same power as a test on the true outcomes.\n\n### Step 1: Extract Givens\n-   **Model for true outcome:** $Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\varepsilon_{i}$\n-   **Treatment variable:** $X_{i} \\in \\{0,1\\}$\n-   **Treatment allocation:** Balanced RCT, equal allocation, total sample size $n$, so $n/2$ individuals per arm.\n-   **Error term for true outcome:** $\\varepsilon_{i}$ are independent and identically distributed (i.i.d.), $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$, and $\\varepsilon_{i}$ is independent of $X_{i}$.\n-   **Estimand of interest:** $\\beta_{1} = \\Delta \\neq 0$.\n-   **Model for measurement error:** Observed outcome is $Y_{i}^{*} = Y_{i} + V_{i}$.\n-   **Measurement error term:** $V_{i} \\sim \\mathcal{N}(0,\\sigma_{V}^{2})$, i.i.d.\n-   **Independence of measurement error:** $V_{i}$ is independent of $Y_{i}$ and $X_{i}$.\n-   **Statistical test:** Two-sample $t$-test on $Y_{i}^{*}$ to test $H_{0}:\\beta_{1}=0$.\n-   **Objective:** First, derive the noncentrality parameter for the test using $Y_{i}^{*}$. Second, derive the sample size inflation factor $k = n^{*}/n$ needed to maintain the same noncentrality parameter as a test on the error-free outcome $Y_i$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem uses the classical normal linear model and classical additive measurement error. These are standard, well-established models in statistics and biostatistics. The framework is scientifically sound.\n-   **Well-Posed:** The problem provides all necessary definitions and asks for the derivation of specific analytical quantities (noncentrality parameter and inflation factor). The task is clear, and a unique mathematical solution exists.\n-   **Objective:** The language is formal and devoid of subjective claims.\n-   **Completeness:** The problem specifies the distributions, independence assumptions, experimental design, and the analysis method. All required information is present.\n-   **Consistency:** There are no contradictory statements. The assumptions are mutually compatible.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard exercise in statistical theory concerning the impact of measurement error on statistical power. I will proceed with the solution.\n\n### Derivation\nThe solution proceeds in three main parts: first, we characterize the statistical properties of the observed outcome $Y_i^*$; second, we derive the noncentrality parameter (NCP) of the two-sample $t$-test based on $Y_i^*$; third, we compare this to the NCP of a hypothetical test on the true outcome $Y_i$ to find the required sample size inflation factor.\n\nThe model for the true outcome for participant $i$ is given by\n$$\nY_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})\n$$\nThe observed outcome, $Y_i^*$, is subject to an independent, additive measurement error $V_i$:\n$$\nY_{i}^{*} = Y_{i} + V_{i}, \\quad V_{i} \\sim \\mathcal{N}(0,\\sigma_{V}^{2})\n$$\nWe can substitute the first equation into the second to obtain a model for the observed outcome:\n$$\nY_{i}^{*} = (\\beta_{0} + \\beta_{1} X_{i} + \\varepsilon_{i}) + V_{i} = \\beta_{0} + \\beta_{1} X_{i} + U_{i}\n$$\nwhere $U_{i} = \\varepsilon_{i} + V_{i}$. Since $\\varepsilon_{i}$ and $V_{i}$ are independent and normally distributed, their sum $U_i$ is also normally distributed. The mean of $U_i$ is $E[U_{i}] = E[\\varepsilon_{i}] + E[V_{i}] = 0 + 0 = 0$. The variance of $U_i$ is $\\text{Var}(U_{i}) = \\text{Var}(\\varepsilon_{i}) + \\text{Var}(V_{i}) = \\sigma_{Y}^{2} + \\sigma_{V}^{2}$, due to their independence. Let us denote the variance of the new error term as $\\sigma_{Y^{*}}^{2} = \\sigma_{Y}^{2} + \\sigma_{V}^{2}$.\nSo, the model for the observed outcome is\n$$\nY_{i}^{*} = \\beta_{0} + \\beta_{1} X_{i} + U_{i}, \\quad U_{i} \\sim \\mathcal{N}(0, \\sigma_{Y^{*}}^{2})\n$$\nThis demonstrates that additive measurement error in the outcome does not induce bias in the parameter estimates but inflates the residual variance.\n\nThe two-sample $t$-test is used to test the null hypothesis $H_{0}: \\beta_{1}=0$. This is equivalent to an ordinary least squares regression of $Y_i^*$ on $X_i$. The OLS estimator for $\\beta_1$ is $\\hat{\\beta}_1 = \\bar{Y}_1^* - \\bar{Y}_0^*$, where $\\bar{Y}_1^*$ and $\\bar{Y}_0^*$ are the sample means of the observed outcomes in the treatment ($X_i=1$) and control ($X_i=0$) arms, respectively. The trial is balanced with $n/2$ subjects per arm.\n\nUnder the alternative hypothesis $H_A: \\beta_1 = \\Delta$, the expected value of the estimator is:\n$$\nE[\\hat{\\beta}_1] = E[\\bar{Y}_1^* - \\bar{Y}_0^*] = E[\\bar{Y}_1^*] - E[\\bar{Y}_0^*]\n$$\nThe expected value of the outcome in the treatment group is $E[Y_i^* | X_i=1] = \\beta_0 + \\Delta$, and in the control group is $E[Y_i^* | X_i=0] = \\beta_0$. Thus, $E[\\hat{\\beta}_1] = (\\beta_0 + \\Delta) - \\beta_0 = \\Delta$.\n\nThe variance of the estimator is:\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\text{Var}(\\bar{Y}_1^* - \\bar{Y}_0^*) = \\text{Var}(\\bar{Y}_1^*) + \\text{Var}(\\bar{Y}_0^*)\n$$\nThe variance of the sample means are $\\text{Var}(\\bar{Y}_1^*) = \\frac{\\sigma_{Y^{*}}^{2}}{n/2}$ and $\\text{Var}(\\bar{Y}_0^*) = \\frac{\\sigma_{Y^{*}}^{2}}{n/2}$. Therefore,\n$$\n\\text{Var}(\\hat{\\beta}_1) = \\frac{\\sigma_{Y^{*}}^{2}}{n/2} + \\frac{\\sigma_{Y^{*}}^{2}}{n/2} = \\frac{2 \\sigma_{Y^{*}}^{2}}{n/2} = \\frac{4 \\sigma_{Y^{*}}^{2}}{n}\n$$\nThe $t$-statistic is $T = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}$. Under the alternative hypothesis, this statistic follows a noncentral $t$-distribution. The noncentrality parameter (NCP), which we denote as $\\delta^*$, is given by the true mean of the numerator divided by its true standard deviation:\n$$\n\\delta^{*} = \\frac{E[\\hat{\\beta}_1]}{\\sqrt{\\text{Var}(\\hat{\\beta}_1)}} = \\frac{\\Delta}{\\sqrt{\\frac{4 \\sigma_{Y^{*}}^{2}}{n}}} = \\frac{\\Delta \\sqrt{n}}{2 \\sigma_{Y^{*}}}\n$$\nSubstituting $\\sigma_{Y^{*}}^{2} = \\sigma_{Y}^{2} + \\sigma_{V}^{2}$, we arrive at the expression for the noncentrality parameter for the test using the observed data $Y^*$:\n$$\n\\delta^{*} = \\frac{\\Delta \\sqrt{n}}{2 \\sqrt{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}}\n$$\nThis concludes the first part of the derivation.\n\nFor the second part, we consider the hypothetical scenario with error-free outcomes $Y_i$. In this case, the measurement error $V_i$ is zero, so $\\sigma_V^2=0$. The variance of the outcome is simply $\\text{Var}(Y_i | X_i) = \\sigma_Y^2$. The noncentrality parameter for a test on $Y_i$ with sample size $n$, which we denote $\\delta$, is obtained by setting $\\sigma_V^2=0$ in the expression for $\\delta^*$:\n$$\n\\delta = \\frac{\\Delta \\sqrt{n}}{2 \\sqrt{\\sigma_{Y}^{2}}}\n$$\nThe problem requires us to find the sample size $n^*$ needed for the study with measurement error (using outcome $Y^*$) to achieve the same noncentrality parameter $\\delta$ as the error-free study with sample size $n$. We set the NCP for the observed-data study with sample size $n^*$ equal to $\\delta$:\n$$\n\\frac{\\Delta \\sqrt{n^{*}}}{2 \\sqrt{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}} = \\frac{\\Delta \\sqrt{n}}{2 \\sqrt{\\sigma_{Y}^{2}}}\n$$\nSince $\\Delta \\neq 0$, we can cancel $\\Delta/2$ from both sides:\n$$\n\\frac{\\sqrt{n^{*}}}{\\sqrt{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}} = \\frac{\\sqrt{n}}{\\sqrt{\\sigma_{Y}^{2}}}\n$$\nSquaring both sides of the equation gives:\n$$\n\\frac{n^{*}}{\\sigma_{Y}^{2} + \\sigma_{V}^{2}} = \\frac{n}{\\sigma_{Y}^{2}}\n$$\nWe can now solve for the required sample size $n^{*}$:\n$$\nn^{*} = n \\left( \\frac{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}{\\sigma_{Y}^{2}} \\right)\n$$\nThe sample size inflation factor is $k = n^{*}/n$. Dividing the above equation by $n$ yields the final expression for $k$:\n$$\nk = \\frac{n^{*}}{n} = \\frac{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}{\\sigma_{Y}^{2}} = 1 + \\frac{\\sigma_{V}^{2}}{\\sigma_{Y}^{2}}\n$$\nThis factor represents the proportional increase in sample size required to offset the loss of statistical power due to the additive measurement error in the outcome variable.", "answer": "$$ \\boxed{1 + \\frac{\\sigma_V^2}{\\sigma_Y^2}} $$", "id": "4956457"}, {"introduction": "While error in an outcome variable inflates variance, error in a predictor variable can be more insidious, leading to systematic bias in your results. This practice dives into the classic problem of regressing an outcome on a noisy predictor, a common scenario in epidemiological studies. You will derive the bias that arises in both the slope and the intercept of the regression model, revealing how measurement error can lead to incorrect conclusions about the strength and nature of an association. [@problem_id:4956417]", "problem": "A biomedical researcher studies a continuous outcome $Y$ and a continuous exposure $X$ that is measured with additive classical error. The data-generating mechanism is the linear model\n$$\nY \\;=\\; \\alpha \\;+\\; \\beta X \\;+\\; \\varepsilon,\n$$\nwhere $\\alpha$ is the intercept, $\\beta$ is the slope, and the noise $\\varepsilon$ satisfies $\\operatorname{E}[\\varepsilon]=0$, $\\operatorname{Var}(\\varepsilon)=\\sigma_{\\varepsilon}^{2}$, and is independent of $X$. The exposure is not observed directly; instead the researcher observes the surrogate\n$$\nW \\;=\\; X \\;+\\; U,\n$$\nwhere the measurement error $U$ is independent of $X$ and $\\varepsilon$, has $\\operatorname{E}[U]=0$, and $\\operatorname{Var}(U)=\\sigma_{U}^{2}$. Let $\\mu_{X}=\\operatorname{E}[X]$ and $\\sigma_{X}^{2}=\\operatorname{Var}(X)$.\n\nThe researcher naively regresses $Y$ on $W$ by ordinary least squares (ordinary least squares (OLS)), obtaining the population slope $\\beta^{\\ast}$ and intercept $\\alpha^{\\ast}$ that minimize the expected squared error $\\operatorname{E}[(Y-(\\alpha^{\\ast}+\\beta^{\\ast}W))^{2}]$. Starting from the definitions above and first principles for OLS in the population (i.e., in terms of means, variances, and covariances), derive the bias of the intercept, defined as $\\alpha^{\\ast}-\\alpha$, expressed only in terms of $\\beta$, $\\mu_{X}$, $\\sigma_{X}^{2}$, and $\\sigma_{U}^{2}$. Then, explain how centering the observed variables $W$ and $Y$ (i.e., regressing $Y-\\operatorname{E}[Y]$ on $W-\\operatorname{E}[W]$) affects the intercept bias, and contrast this with the special case where the true exposure $X$ is centered so that $\\mu_{X}=0$.\n\nExpress the final bias as a single closed-form analytic expression; no numerical rounding is required.", "solution": "The problem requires the derivation of the bias of the ordinary least squares (OLS) intercept when a response variable $Y$ is regressed on a surrogate exposure variable $W$, which is a measurement of the true exposure $X$ with additive classical error.\n\nFirst, we establish the population OLS estimators for the naive regression of $Y$ on $W$. The model being fit is $Y = \\alpha^{\\ast} + \\beta^{\\ast} W + \\text{error}$. The population parameters $\\alpha^{\\ast}$ and $\\beta^{\\ast}$ are those that minimize the mean squared error, $\\operatorname{E}[(Y-(\\alpha^{\\ast}+\\beta^{\\ast}W))^{2}]$. These are given by the standard formulas:\n$$\n\\beta^{\\ast} = \\frac{\\operatorname{Cov}(Y, W)}{\\operatorname{Var}(W)}\n$$\n$$\n\\alpha^{\\ast} = \\operatorname{E}[Y] - \\beta^{\\ast} \\operatorname{E}[W]\n$$\nTo proceed, we must derive expressions for $\\operatorname{E}[Y]$, $\\operatorname{E}[W]$, $\\operatorname{Var}(W)$, and $\\operatorname{Cov}(Y, W)$ based on the provided data-generating mechanisms:\n$Y = \\alpha + \\beta X + \\varepsilon$\n$W = X + U$\n\nThe given properties are:\n- $\\operatorname{E}[\\varepsilon]=0$, $\\operatorname{Var}(\\varepsilon)=\\sigma_{\\varepsilon}^{2}$, and $\\varepsilon$ is independent of $X$.\n- $\\operatorname{E}[U]=0$, $\\operatorname{Var}(U)=\\sigma_{U}^{2}$, and $U$ is independent of $X$ and $\\varepsilon$.\n- $\\operatorname{E}[X]=\\mu_{X}$ and $\\operatorname{Var}(X)=\\sigma_{X}^{2}$.\n\nWe now compute the necessary expected values, variances, and covariances.\n\n1.  **Expected value of $W$**:\n    Using the linearity of expectation,\n    $$\n    \\operatorname{E}[W] \\;=\\; \\operatorname{E}[X + U] \\;=\\; \\operatorname{E}[X] + \\operatorname{E}[U] \\;=\\; \\mu_{X} + 0 \\;=\\; \\mu_{X}.\n    $$\n\n2.  **Variance of $W$**:\n    Because $X$ and $U$ are independent, their covariance is $0$. Thus,\n    $$\n    \\operatorname{Var}(W) \\;=\\; \\operatorname{Var}(X + U) \\;=\\; \\operatorname{Var}(X) + \\operatorname{Var}(U) + 2\\operatorname{Cov}(X, U) \\;=\\; \\sigma_{X}^{2} + \\sigma_{U}^{2} + 0 \\;=\\; \\sigma_{X}^{2} + \\sigma_{U}^{2}.\n    $$\n\n3.  **Expected value of $Y$**:\n    Using the linearity of expectation,\n    $$\n    \\operatorname{E}[Y] \\;=\\; \\operatorname{E}[\\alpha + \\beta X + \\varepsilon] \\;=\\; \\alpha + \\beta\\operatorname{E}[X] + \\operatorname{E}[\\varepsilon] \\;=\\; \\alpha + \\beta\\mu_{X} + 0 \\;=\\; \\alpha + \\beta\\mu_{X}.\n    $$\n\n4.  **Covariance of $Y$ and $W$**:\n    Using the bilinearity property of covariance,\n    $$\n    \\operatorname{Cov}(Y, W) \\;=\\; \\operatorname{Cov}(\\alpha + \\beta X + \\varepsilon, X + U).\n    $$\n    We expand this expression:\n    $$\n    \\operatorname{Cov}(Y, W) = \\operatorname{Cov}(\\alpha, X+U) + \\operatorname{Cov}(\\beta X, X+U) + \\operatorname{Cov}(\\varepsilon, X+U).\n    $$\n    - The covariance with a constant is zero: $\\operatorname{Cov}(\\alpha, X+U) = 0$.\n    - For the second term: $\\operatorname{Cov}(\\beta X, X+U) = \\beta\\operatorname{Cov}(X, X+U) = \\beta(\\operatorname{Cov}(X,X) + \\operatorname{Cov}(X,U))$. Since $X$ and $U$ are independent, $\\operatorname{Cov}(X,U)=0$. Thus, this term simplifies to $\\beta\\operatorname{Var}(X) = \\beta\\sigma_{X}^{2}$.\n    - For the third term: $\\operatorname{Cov}(\\varepsilon, X+U) = \\operatorname{Cov}(\\varepsilon, X) + \\operatorname{Cov}(\\varepsilon, U)$. Since $\\varepsilon$ is independent of both $X$ and $U$, both covariances are $0$.\n    Combining these results, we get:\n    $$\n    \\operatorname{Cov}(Y, W) \\;=\\; \\beta\\sigma_{X}^{2}.\n    $$\n\nNow, we can find the naive OLS slope $\\beta^{\\ast}$:\n$$\n\\beta^{\\ast} \\;=\\; \\frac{\\operatorname{Cov}(Y, W)}{\\operatorname{Var}(W)} \\;=\\; \\frac{\\beta\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}.\n$$\nThis expression shows the well-known attenuation bias in the slope estimate; $\\beta^{\\ast}$ is biased toward zero.\n\nNext, we find the naive OLS intercept $\\alpha^{\\ast}$:\n$$\n\\alpha^{\\ast} \\;=\\; \\operatorname{E}[Y] - \\beta^{\\ast}\\operatorname{E}[W] \\;=\\; (\\alpha + \\beta\\mu_{X}) - \\left(\\frac{\\beta\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right)\\mu_{X}.\n$$\nThe problem asks for the bias of the intercept, which is defined as $\\alpha^{\\ast} - \\alpha$:\n$$\n\\alpha^{\\ast} - \\alpha \\;=\\; (\\alpha + \\beta\\mu_{X}) - \\left(\\frac{\\beta\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right)\\mu_{X} - \\alpha.\n$$\n$$\n\\alpha^{\\ast} - \\alpha \\;=\\; \\beta\\mu_{X} - \\beta\\mu_{X}\\left(\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right).\n$$\nFactoring out $\\beta\\mu_{X}$:\n$$\n\\alpha^{\\ast} - \\alpha \\;=\\; \\beta\\mu_{X} \\left(1 - \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right).\n$$\nFinding a common denominator for the term in the parentheses:\n$$\n\\alpha^{\\ast} - \\alpha \\;=\\; \\beta\\mu_{X} \\left(\\frac{(\\sigma_{X}^{2} + \\sigma_{U}^{2}) - \\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right).\n$$\nThis simplifies to the final expression for the intercept bias:\n$$\n\\alpha^{\\ast} - \\alpha \\;=\\; \\beta\\mu_{X} \\left(\\frac{\\sigma_{U}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right).\n$$\nThis expression depends only on $\\beta$, $\\mu_{X}$, $\\sigma_{X}^{2}$, and $\\sigma_{U}^{2}$, as required.\n\nThe second part of the problem asks how centering the observed variables $W$ and $Y$ affects the intercept bias. Let the centered variables be $W' = W - \\operatorname{E}[W]$ and $Y' = Y - \\operatorname{E}[Y]$. By definition, $\\operatorname{E}[W'] = 0$ and $\\operatorname{E}[Y'] = 0$. When we perform OLS regression of $Y'$ on $W'$, the resulting intercept, let's call it $\\alpha_{center}^{\\ast}$, is given by:\n$$\n\\alpha_{center}^{\\ast} \\;=\\; \\operatorname{E}[Y'] - \\beta_{center}^{\\ast}\\operatorname{E}[W'] \\;=\\; 0 - \\beta_{center}^{\\ast}(0) \\;=\\; 0.\n$$\nThe \"true\" relationship for the centered variables is $Y' = (Y - \\operatorname{E}[Y]) = (\\alpha + \\beta X + \\varepsilon) - (\\alpha + \\beta \\mu_X) = \\beta(X - \\mu_X) + \\varepsilon$. This transformed true model has an intercept of $0$. Since the estimated intercept is $\\alpha_{center}^{\\ast}=0$ and the true intercept for the centered model is also $0$, the intercept bias is $0 - 0 = 0$. Thus, centering the observed variables $W$ and $Y$ before regression completely eliminates the intercept bias.\n\nFinally, we contrast this with the special case where the true exposure $X$ is centered, so that $\\mu_{X}=0$. We can use the general bias formula derived earlier:\n$$\n\\alpha^{\\ast} - \\alpha \\;=\\; \\beta\\mu_{X} \\left(\\frac{\\sigma_{U}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right).\n$$\nIf we substitute $\\mu_{X}=0$ into this formula, we get:\n$$\n\\alpha^{\\ast} - \\alpha \\;=\\; \\beta(0) \\left(\\frac{\\sigma_{U}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\\right) \\;=\\; 0.\n$$\nThis shows that if the true exposure has a mean of zero, the intercept estimate is unbiased, i.e., $\\alpha^{\\ast} = \\alpha$, even in the presence of measurement error.\nThe contrast is as follows: Centering the observed data ($Y, W$) is a data processing step that forces a zero intercept in the regression, thereby procedurally removing any intercept bias. In contrast, the condition $\\mu_{X}=0$ is a specific property of the underlying population distribution of the true exposure. In this scenario, the intercept bias is naturally zero without any data transformation. The bias arises because the naive regression line pivots around the point of means, $(\\operatorname{E}[W], \\operatorname{E}[Y])$. If $\\mu_X=0$, then $\\operatorname{E}[W]=0$, and this pivot point lies on the y-axis, which constrains the intercept to be equal to $\\operatorname{E}[Y] = \\alpha$. Thus, no bias is induced.", "answer": "$$\n\\boxed{\\beta \\mu_{X} \\frac{\\sigma_{U}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}\n$$", "id": "4956417"}]}