{"hands_on_practices": [{"introduction": "The Pearson chi-squared test relies on large-sample approximations, which may not hold for small datasets common in biostatistics. This exercise challenges you to explore Fisher's Exact Test, a powerful alternative that calculates a precise $p$-value by considering all possible tables with the same marginal totals. By deriving the underlying hypergeometric distribution from first principles, you will gain a deep understanding of the combinatorial logic behind this fundamental exact test. [@problem_id:4899807]", "problem": "A randomized biostatistics study investigates whether an antiviral therapy and the occurrence of seroconversion are independent. The study allocates $n_{T}=10$ participants to the therapy and $n_{C}=6$ participants to placebo, for a total of $N=16$ participants. The observed $2 \\times 2$ table shows $a_{\\text{obs}}=5$ seroconversions in the therapy arm and $b_{\\text{obs}}=2$ seroconversions in the placebo arm, so the observed total number of seroconversions is $K=a_{\\text{obs}}+b_{\\text{obs}}=7$ and the total number without seroconversion is $N-K=9$. Assume the null hypothesis of independence (homogeneity), that is, the seroconversion probability $p$ is the same in both arms.\n\nStarting from fundamental definitions, condition on the fixed row totals $n_{T}$ and $n_{C}$ and the fixed column totals $K$ and $N-K$. Derive the conditional distribution of the number $A$ of seroconversions in the therapy arm given the margins, and enumerate its probability values for all feasible $a \\in \\{\\max(0, K-n_{C}), \\ldots, \\min(n_{T}, K)\\}$. Using this enumeration, compute the two-sided Fisher’s exact test (Fisher’s Exact Test (FET)) $p$-value defined as the sum of the probabilities of all tables whose conditional probability is less than or equal to that of the observed table, that is, $\\sum_{a: \\mathbb{P}(A=a \\mid \\text{margins}) \\le \\mathbb{P}(A=a_{\\text{obs}} \\mid \\text{margins})} \\mathbb{P}(A=a \\mid \\text{margins})$. Express the final $p$-value as a decimal and round your answer to four significant figures.", "solution": "The problem asks for the derivation of the conditional distribution for the number of seroconversions in the therapy arm, its enumeration, and the computation of a two-sided $p$-value.\n\nLet $A$ be the random variable representing the number of seroconversions in the therapy arm, and $B$ be the random variable for the number of seroconversions in the placebo arm. The respective sample sizes are $n_{T}=10$ and $n_{C}=6$, with a total of $N = n_{T} + n_{C} = 16$ participants.\n\nUnder the null hypothesis ($H_0$) of homogeneity, the probability of seroconversion, denoted by $p$, is assumed to be identical for both arms. The number of seroconversions in each arm thus follows a binomial distribution, as the outcomes for individual participants are independent trials.\n$$ A \\sim \\text{Binomial}(n_{T}, p) \\implies \\mathbb{P}(A=a) = \\binom{n_{T}}{a} p^{a} (1-p)^{n_{T}-a} $$\n$$ B \\sim \\text{Binomial}(n_{C}, p) \\implies \\mathbb{P}(B=b) = \\binom{n_{C}}{b} p^{b} (1-p)^{n_{C}-b} $$\nThe two groups are independent, so the joint probability of observing $a$ events in the therapy arm and $b$ events in the placebo arm is $\\mathbb{P}(A=a, B=b) = \\mathbb{P}(A=a)\\mathbb{P}(B=b)$.\n\nFisher's Exact Test (FET) is based on the distribution of $A$ conditional on the fixed marginal totals of the contingency table. The row totals ($n_{T}=10, n_{C}=6$) are fixed by the study design. For the FET, we also condition on the column totals. The observed data are $a_{\\text{obs}}=5$ successes in the therapy arm and $b_{\\text{obs}}=2$ in the placebo arm, leading to a total of $K = a_{\\text{obs}} + b_{\\text{obs}} = 5+2=7$ seroconversions and $N-K = 16-7=9$ non-seroconversions.\n\nWe derive the conditional probability $\\mathbb{P}(A=a \\mid A+B=K)$. By the definition of conditional probability:\n$$ \\mathbb{P}(A=a \\mid A+B=K) = \\frac{\\mathbb{P}(A=a \\text{ and } A+B=K)}{\\mathbb{P}(A+B=K)} $$\nThe event in the numerator is equivalent to $\\{A=a \\text{ and } B=K-a\\}$. Due to independence, its probability is:\n$$ \\mathbb{P}(A=a, B=K-a) = \\mathbb{P}(A=a)\\mathbb{P}(B=K-a) = \\left[ \\binom{n_{T}}{a}p^{a}(1-p)^{n_{T}-a} \\right] \\left[ \\binom{n_{C}}{K-a}p^{K-a}(1-p)^{n_{C}-(K-a)} \\right] $$\n$$ = \\binom{n_{T}}{a}\\binom{n_{C}}{K-a} p^{a+K-a} (1-p)^{n_{T}-a+n_{C}-K+a} = \\binom{n_{T}}{a}\\binom{n_{C}}{K-a} p^{K} (1-p)^{N-K} $$\nThe random variable for the total number of successes, $A+B$, follows a binomial distribution $\\text{Binomial}(N, p)$, so the probability of the conditioning event is:\n$$ \\mathbb{P}(A+B=K) = \\binom{N}{K} p^{K} (1-p)^{N-K} $$\nSubstituting these into the conditional probability formula yields:\n$$ \\mathbb{P}(A=a \\mid A+B=K) = \\frac{\\binom{n_{T}}{a}\\binom{n_{C}}{K-a} p^{K} (1-p)^{N-K}}{\\binom{N}{K} p^{K} (1-p)^{N-K}} = \\frac{\\binom{n_{T}}{a}\\binom{n_{C}}{K-a}}{\\binom{N}{K}} $$\nThis is the probability mass function of a hypergeometric distribution. The unknown parameter $p$ has canceled out, which is the key property allowing for an \"exact\" test.\n\nNext, we enumerate the probabilities for all feasible values of $a$ using the given data: $n_{T}=10$, $n_{C}=6$, $N=16$, and $K=7$. The possible values for $a$ are determined by the constraints on the binomial coefficients, which require all arguments to be non-negative:\n$a \\ge 0$, $n_{T}-a \\ge 0 \\implies a \\le n_{T}=10$.\n$K-a \\ge 0 \\implies a \\le K=7$.\n$n_{C}-(K-a) \\ge 0 \\implies a \\ge K-n_{C} = 7-6=1$.\nThus, the support for $a$ is $\\{1, 2, 3, 4, 5, 6, 7\\}$, which matches the specified range $[\\max(0, K-n_C), \\min(n_T, K)]$.\n\nThe denominator of the probability mass function is the total number of ways to choose $K=7$ seroconverters from $N=16$ participants:\n$$ \\binom{N}{K} = \\binom{16}{7} = \\frac{16!}{7!9!} = 11440 $$\nWe now compute the probability for each feasible value of $a$:\n$$ \\mathbb{P}(A=a) = \\frac{\\binom{10}{a}\\binom{6}{7-a}}{11440} $$\nFor $a=1$: $\\mathbb{P}(A=1) = \\frac{\\binom{10}{1}\\binom{6}{6}}{11440} = \\frac{10 \\times 1}{11440} = \\frac{10}{11440}$\nFor $a=2$: $\\mathbb{P}(A=2) = \\frac{\\binom{10}{2}\\binom{6}{5}}{11440} = \\frac{45 \\times 6}{11440} = \\frac{270}{11440}$\nFor $a=3$: $\\mathbb{P}(A=3) = \\frac{\\binom{10}{3}\\binom{6}{4}}{11440} = \\frac{120 \\times 15}{11440} = \\frac{1800}{11440}$\nFor $a=4$: $\\mathbb{P}(A=4) = \\frac{\\binom{10}{4}\\binom{6}{3}}{11440} = \\frac{210 \\times 20}{11440} = \\frac{4200}{11440}$\nFor $a=5$: $\\mathbb{P}(A=5) = \\frac{\\binom{10}{5}\\binom{6}{2}}{11440} = \\frac{252 \\times 15}{11440} = \\frac{3780}{11440}$\nFor $a=6$: $\\mathbb{P}(A=6) = \\frac{\\binom{10}{6}\\binom{6}{1}}{11440} = \\frac{210 \\times 6}{11440} = \\frac{1260}{11440}$\nFor $a=7$: $\\mathbb{P}(A=7) = \\frac{\\binom{10}{7}\\binom{6}{0}}{11440} = \\frac{120 \\times 1}{11440} = \\frac{120}{11440}$\n\nFinally, we compute the two-sided FET $p$-value. The observed value is $a_{\\text{obs}}=5$. The probability of this observation is $\\mathbb{P}(A=5) = \\frac{3780}{11440}$. The $p$-value is defined as the sum of probabilities of all tables with a conditional probability less than or equal to that of the observed table.\n$$ p\\text{-value} = \\sum_{a: \\mathbb{P}(A=a) \\le \\mathbb{P}(A=a_{\\text{obs}})} \\mathbb{P}(A=a) $$\nWe compare each probability to $\\mathbb{P}(A=5) = \\frac{3780}{11440}$.\n$\\mathbb{P}(A=1) = \\frac{10}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=2) = \\frac{270}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=3) = \\frac{1800}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=4) = \\frac{4200}{11440} > \\frac{3780}{11440}$ (False)\n$\\mathbb{P}(A=5) = \\frac{3780}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=6) = \\frac{1260}{11440} \\le \\frac{3780}{11440}$ (True)\n$\\mathbb{P}(A=7) = \\frac{120}{11440} \\le \\frac{3780}{11440}$ (True)\nThe p-value is the sum of probabilities for $a \\in \\{1, 2, 3, 5, 6, 7\\}$:\n$$ p\\text{-value} = \\mathbb{P}(A=1) + \\mathbb{P}(A=2) + \\mathbb{P}(A=3) + \\mathbb{P}(A=5) + \\mathbb{P}(A=6) + \\mathbb{P}(A=7) $$\n$$ p\\text{-value} = \\frac{10+270+1800+3780+1260+120}{11440} = \\frac{7240}{11440} $$\nConverting this fraction to a decimal:\n$$ p\\text{-value} = \\frac{7240}{11440} \\approx 0.63286713... $$\nRounding to four significant figures, we get $0.6329$.", "answer": "$$\n\\boxed{0.6329}\n$$", "id": "4899807"}, {"introduction": "Real-world study designs sometimes make certain combinations of variables impossible, leading to structural zeros in a contingency table where the standard independence model does not apply. This practice introduces the concept of quasi-independence and the classic Iterative Proportional Fitting (IPF) algorithm used to analyze such tables. You will learn to fit a model that accounts for these structural constraints and test for association in the presence of incomplete table structures. [@problem_id:4899809]", "problem": "A biostatistics study examines the association between disease status and exposure category in a designed observational protocol that prohibits enrolling already diseased individuals into the highest exposure category. This design yields a structural zero in the contingency table at the cell corresponding to diseased and highest exposure. Investigators recorded the following table of observed counts, row totals, and column totals: disease status rows $i \\in \\{1,2\\}$ are disease present ($i=1$) and disease absent ($i=2$), exposure columns $j \\in \\{1,2,3\\}$ are low ($j=1$), medium ($j=2$), and high ($j=3$), with a structural zero at cell $(i,j)=(1,3)$:\n$$\nO=\\begin{pmatrix}\n12 & 18 & 0 \\\\\n28 & 22 & 20\n\\end{pmatrix},\\quad\n\\text{row totals } R=\\begin{pmatrix}30 \\\\ 70\\end{pmatrix},\\quad\n\\text{column totals } C=\\begin{pmatrix}40 \\\\ 40 \\\\ 20\\end{pmatrix},\\quad\nN=100.\n$$\nAssume the quasi-independence model on the admissible support (all cells except the structural zero), and treat the counts as arising under a multinomial sampling scheme with fixed margins. Starting from first principles, use the independence log-linear structure and maximum likelihood to justify the Iterative Proportional Fitting (IPF) procedure for two-way tables, and explain how to incorporate the structural zero constraint so that the cell $(1,3)$ remains exactly zero at every update. Begin the IPF with initial expected counts $m_{ij}^{(0)}=1$ for all admissible cells and $m_{13}^{(0)}=0$ for the structural zero, then perform one full row-scaling step followed by one full column-scaling step, showing explicitly how the structural constraint is preserved. Next, derive the analytic limit of the IPF iterates by solving the multiplicative system for the admissible cells subject to the fixed margins and the structural zero, and state the fitted expected counts for all cells. Finally, compute the Pearson chi-squared statistic for quasi-independence,\n$$\nX^{2}=\\sum_{\\text{admissible }(i,j)}\\frac{\\left(O_{ij}-E_{ij}\\right)^{2}}{E_{ij}},\n$$\nwhere $E_{ij}$ are the fitted expected counts you obtained, and the summation excludes the structural zero cell. Round your final numeric answer to four significant figures.", "solution": "**Justification of the Iterative Proportional Fitting (IPF) Procedure**\n\nLet $m_{ij}$ be the expected count in cell $(i,j)$. The quasi-independence model states that for the set of admissible cells $S$ (all cells except the structural zero at $(1,3)$), the expected counts follow the structure of an independence model. This can be expressed multiplicatively as $m_{ij} = \\alpha_i \\beta_j$ for $(i,j) \\in S$, or in the equivalent log-linear form:\n$$ \\log(m_{ij}) = \\lambda + \\lambda_i^R + \\lambda_j^C \\quad \\text{for } (i,j) \\in S $$\nwhere $\\lambda$ is an overall effect, $\\lambda_i^R$ is the effect for row $i$, and $\\lambda_j^C$ is the effect for column $j$. For the structural zero, we have the constraint $m_{13}=0$.\n\nUnder a multinomial sampling assumption, the log-likelihood function (ignoring constants) for the observed counts $O_{ij}$ is:\n$$ \\ell(\\{m_{ij}\\}) = \\sum_{(i,j) \\in S} O_{ij} \\log(m_{ij}) $$\nMaximizing this log-likelihood subject to the log-linear model structure and the constraints imposed by the fixed margins (which are the sufficient statistics for the model parameters) leads to a set of likelihood equations. These equations state that the expected marginal totals must equal the observed marginal totals:\n$$ \\sum_{j} \\hat{m}_{ij} = \\sum_{j} O_{ij} = R_i \\quad \\text{for } i=1,2 $$\n$$ \\sum_{i} \\hat{m}_{ij} = \\sum_{i} O_{ij} = C_j \\quad \\text{for } j=1,2,3 $$\nwhere $\\hat{m}_{ij}$ are the maximum likelihood estimates (MLEs) of the expected counts, and we enforce $\\hat{m}_{13}=0$.\n\nThe IPF procedure is an algorithm for finding these MLEs $\\hat{m}_{ij}$. It does not solve for the $\\lambda$ parameters directly, but instead finds the cell counts that satisfy the multiplicative form of the model and the marginal constraints. Starting with an initial set of estimates $m_{ij}^{(0)}$ that conform to the model (e.g., $m_{ij}^{(0)}=1$ for admissible cells), IPF iteratively adjusts the counts to match the row and column margins.\n\n**Incorporating the Structural Zero Constraint**\n\nThe structural zero constraint, $m_{13}=0$, is maintained throughout the IPF procedure by setting its initial value to zero, i.e., $m_{13}^{(0)}=0$. Since the update steps are multiplicative, a cell count that is initially zero will remain zero in all subsequent iterations.\n\n**One Full IPF Iteration**\n\nThe initial counts are $m_{ij}^{(0)}=1$ for all $(i,j) \\in S$ and $m_{13}^{(0)}=0$.\nThe initial matrix of counts is $M^{(0)} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix}$.\nThe observed margins are $R=\\begin{pmatrix} 30 \\\\ 70 \\end{pmatrix}$ and $C=\\begin{pmatrix} 40 \\\\ 40 \\\\ 20 \\end{pmatrix}$.\n\n1.  **Row-Scaling Step:**\n    The initial row totals are $R_1^{(0)} = 1+1+0=2$ and $R_2^{(0)}=1+1+1=3$.\n    The scaling factors are $\\frac{30}{2} = 15$ for row 1 and $\\frac{70}{3}$ for row 2.\n    Applying these factors gives $M^{(0.5)} = \\begin{pmatrix} 15 & 15 & 0 \\\\ \\frac{70}{3} & \\frac{70}{3} & \\frac{70}{3} \\end{pmatrix}$.\n\n2.  **Column-Scaling Step:**\n    The column totals of $M^{(0.5)}$ are $C_1^{(0.5)} = 15 + \\frac{70}{3} = \\frac{115}{3}$, $C_2^{(0.5)} = 15 + \\frac{70}{3} = \\frac{115}{3}$, and $C_3^{(0.5)} = 0 + \\frac{70}{3} = \\frac{70}{3}$.\n    The scaling factors are $\\frac{40}{115/3} = \\frac{24}{23}$ for column 1, $\\frac{40}{115/3} = \\frac{24}{23}$ for column 2, and $\\frac{20}{70/3} = \\frac{6}{7}$ for column 3.\n    Applying these factors to the columns of $M^{(0.5)}$ gives the matrix after one full iteration:\n    $M^{(1)} = \\begin{pmatrix} 15 \\times \\frac{24}{23} & 15 \\times \\frac{24}{23} & 0 \\times \\frac{6}{7} \\\\ \\frac{70}{3} \\times \\frac{24}{23} & \\frac{70}{3} \\times \\frac{24}{23} & \\frac{70}{3} \\times \\frac{6}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{360}{23} & \\frac{360}{23} & 0 \\\\ \\frac{560}{23} & \\frac{560}{23} & 20 \\end{pmatrix}$.\n\n**Analytic Limit of IPF (Fitted Expected Counts)**\n\nLet $E_{ij}$ be the fitted expected counts. They must satisfy the marginal constraints and the quasi-independence condition.\nThe marginal constraints are:\n1.  $E_{11} + E_{12} = 30$\n2.  $E_{21} + E_{22} + E_{23} = 70$\n3.  $E_{11} + E_{21} = 40$\n4.  $E_{12} + E_{22} = 40$\n5.  $E_{13} + E_{23} = 20 \\implies E_{23} = 20$ (since $E_{13}=0$)\n\nThe quasi-independence condition for the $2 \\times 2$ subtable of admissible cells implies the odds ratio is 1:\n$ \\frac{E_{11} E_{22}}{E_{12} E_{21}} = 1 \\implies E_{11} E_{22} = E_{12} E_{21} $\nWe solve this system. From (5), we have $E_{23}=20$. Substituting into (2) gives $E_{21} + E_{22} = 50$.\nLet's express other terms via $E_{11}$:\nFrom (1): $E_{12} = 30 - E_{11}$.\nFrom (3): $E_{21} = 40 - E_{11}$.\nFrom $E_{21} + E_{22} = 50$: $E_{22} = 50 - E_{21} = 50 - (40 - E_{11}) = 10 + E_{11}$.\nNow substitute these into the quasi-independence equation:\n$$ E_{11} (10 + E_{11}) = (30 - E_{11})(40 - E_{11}) $$\n$$ 10 E_{11} + E_{11}^2 = 1200 - 70 E_{11} + E_{11}^2 $$\n$$ 80 E_{11} = 1200 \\implies E_{11} = 15 $$\nThis gives the other values: $E_{12} = 30 - 15 = 15$, $E_{21} = 40 - 15 = 25$, $E_{22} = 10 + 15 = 25$.\nThe fitted expected counts are:\n$$ E = \\begin{pmatrix} 15 & 15 & 0 \\\\ 25 & 25 & 20 \\end{pmatrix} $$\n\n**Pearson Chi-Squared Statistic for Quasi-Independence**\n\nThe statistic is calculated by summing over all admissible cells:\n$$ X^{2}=\\sum_{(i,j) \\in S}\\frac{\\left(O_{ij}-E_{ij}\\right)^{2}}{E_{ij}} $$\nUsing the observed counts $O=\\begin{pmatrix} 12 & 18 & 0 \\\\ 28 & 22 & 20 \\end{pmatrix}$:\n$$ X^2 = \\frac{(12-15)^2}{15} + \\frac{(18-15)^2}{15} + \\frac{(28-25)^2}{25} + \\frac{(22-25)^2}{25} + \\frac{(20-20)^2}{20} $$\n$$ X^2 = \\frac{(-3)^2}{15} + \\frac{3^2}{15} + \\frac{3^2}{25} + \\frac{(-3)^2}{25} + 0 $$\n$$ X^2 = \\frac{18}{15} + \\frac{18}{25} = 1.2 + 0.72 = 1.92 $$\nRounded to four significant figures, the value is $1.920$.", "answer": "$$\\boxed{1.920}$$", "id": "4899809"}, {"introduction": "A key responsibility for a biostatistician is to ensure a study is designed with sufficient statistical power to detect a meaningful effect. This hands-on exercise shifts the focus from analyzing existing data to planning future studies by estimating the power of a chi-squared test via Monte Carlo simulation. You will practice generating data under a specified alternative hypothesis and use the results to quantify the probability that your test will correctly identify an association. [@problem_id:4899810]", "problem": "A biostatistics laboratory is planning a study to detect association between two categorical variables using the standard test of independence based on Pearson’s chi-squared statistic. The investigators will plan sample sizes by estimating statistical power under specified alternatives via simulation. You must design a program that simulates power under given alternatives using multinomial sampling and summarizes Monte Carlo error.\n\nBase facts to use:\n- Under the null hypothesis of independence for an $r \\times c$ contingency table, the Pearson chi-squared statistic computed from observed cell counts and the maximum likelihood expected counts has, under regularity conditions and large sample sizes, an approximate chi-squared distribution with $(r-1)(c-1)$ degrees of freedom. A test at level $\\alpha$ rejects when the computed $p$-value is less than $\\alpha$.\n- Under a fixed alternative distribution with cell probabilities $p_{ij}$ that do not factor as a product of row and column margins, independent and identically distributed samples of size $N$ generate a random $r \\times c$ table of counts distributed as a multinomial with parameters $N$ and cell probabilities $\\{p_{ij}\\}$.\n- The Monte Carlo estimator of power, $\\hat{\\pi}$, is the empirical mean of rejection indicators from $R$ independent simulation replicates. By basic properties of independent Bernoulli random variables and the Central Limit Theorem, $\\mathbb{E}[\\hat{\\pi}] = \\pi$ and $\\mathrm{Var}(\\hat{\\pi}) = \\pi(1-\\pi)/R$, so a conservative estimate of the Monte Carlo standard error is $\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$, and an approximate two-sided $95\\%$ confidence interval half-width is $z_{0.975}\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$, where $z_{0.975}$ is the standard normal quantile.\n\nYour task:\n- For each test case below, simulate $R$ independent contingency tables of size $N$ from a multinomial distribution with the specified $r \\times c$ alternative cell probability matrix $\\{p_{ij}\\}$.\n- For each simulated table, compute the standard Pearson chi-squared test of independence against the null of independence using the asymptotic chi-squared distribution with $(r-1)(c-1)$ degrees of freedom to compute the $p$-value.\n- Estimate power as $\\hat{\\pi}$, the proportion of simulations with $p$-value less than $\\alpha$.\n- Compute the Monte Carlo standard error $\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$ and the approximate $95\\%$ confidence interval half-width $1.96 \\times \\sqrt{\\hat{\\pi}(1-\\hat{\\pi})/R}$.\n- Use a fixed random number generator seed of $123456789$ for reproducibility. If you need separate seeds per case, you may use deterministic offsets of this seed (e.g., add the case index).\n- Handle any zero expected counts during computation robustly so that terms with zero expected counts contribute $0$ to the chi-squared statistic.\n- Round each reported numeric result to $6$ decimal places.\n\nTest suite:\n- Case $1$ (happy path): $r=3$, $c=3$, $\\alpha=0.05$, $N=400$, $R=20000$, and\n  \n$$\n  \\{p_{ij}\\}=\n  \\begin{bmatrix}\n  0.18 & 0.07 & 0.05\\\\\n  0.06 & 0.20 & 0.04\\\\\n  0.05 & 0.05 & 0.30\n  \\end{bmatrix}.\n  $$\n\n- Case $2$ (small-sample boundary): $r=2$, $c=3$, $\\alpha=0.05$, $N=50$, $R=15000$, and\n  \n$$\n  \\{p_{ij}\\}=\n  \\begin{bmatrix}\n  0.10 & 0.05 & 0.15\\\\\n  0.20 & 0.10 & 0.40\n  \\end{bmatrix}.\n  $$\n\n- Case $3$ (edge with unbalanced margins): $r=4$, $c=2$, $\\alpha=0.05$, $N=300$, $R=20000$, and\n  \n$$\n  \\{p_{ij}\\}=\n  \\begin{bmatrix}\n  0.08 & 0.02\\\\\n  0.03 & 0.17\\\\\n  0.15 & 0.05\\\\\n  0.40 & 0.10\n  \\end{bmatrix}.\n  $$\n\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for Case $1$ then Case $2$ then Case $3$: the estimated power $\\hat{\\pi}$, the Monte Carlo standard error, and the $95\\%$ confidence interval half-width. Thus the output will have $9$ floating-point numbers rounded to $6$ decimal places in the order $[\\hat{\\pi}_1,\\mathrm{se}_1,\\mathrm{hw}_1,\\hat{\\pi}_2,\\mathrm{se}_2,\\mathrm{hw}_2,\\hat{\\pi}_3,\\mathrm{se}_3,\\mathrm{hw}_3]$.", "solution": "The objective is to estimate the statistical power of the Pearson's chi-squared test of independence for specified alternative hypotheses via Monte Carlo simulation. Power is the probability of correctly rejecting the null hypothesis when a specific alternative hypothesis is true.\n\n### 1. Theoretical Framework: Pearson's Chi-Squared Test\n\nThe Pearson's chi-squared test is used to assess whether there is a statistically significant association between two categorical variables. The data are summarized in an $r \\times c$ contingency table, where $r$ is the number of categories for the row variable and $c$ is the number for the column variable.\n\n- **Hypotheses**: The null hypothesis, $H_0$, states that the two variables are independent. This implies that the joint probability $p_{ij}$ of an observation falling into cell $(i, j)$ is the product of the marginal probabilities: $p_{ij} = p_{i \\cdot} \\times p_{\\cdot j}$, where $p_{i \\cdot} = \\sum_j p_{ij}$ and $p_{\\cdot j} = \\sum_i p_{ij}$. The alternative hypothesis, $H_1$, is that the variables are not independent.\n\n- **Test Statistic**: The test statistic is calculated from the observed counts, $O_{ij}$, and the expected counts under the null hypothesis, $E_{ij}$. The statistic is given by:\n$$\n\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n$$\nThe expected counts $E_{ij}$ are estimated from the data by multiplying the row and column marginal totals and dividing by the total sample size $N$:\n$$\nE_{ij} = \\frac{(\\sum_{k=1}^{c} O_{ik}) (\\sum_{l=1}^{r} O_{lj})}{N}\n$$\nIf an expected count $E_{ij}$ is zero, its corresponding term in the sum is $0$.\n\n- **Null Distribution and Decision Rule**: Under $H_0$, for a sufficiently large sample size $N$, the $\\chi^2$ statistic follows an approximate chi-squared distribution with $(r-1)(c-1)$ degrees of freedom. A $p$-value is computed as the probability of observing a test statistic as extreme as, or more extreme than, the one calculated, assuming $H_0$ is true. The null hypothesis is rejected at a significance level $\\alpha$ if the $p$-value is less than $\\alpha$.\n\n### 2. Monte Carlo Simulation for Power Estimation\n\nSince the analytical calculation of power under a specific alternative distribution $\\{p_{ij}\\}$ is complex, we use Monte Carlo simulation. The power, $\\pi$, is defined as $\\pi = P(\\text{Reject } H_0 | H_1 \\text{ is true})$.\n\nThe simulation process unfolds in three stages:\n\n**Stage 1: Data Generation**\nWe are given a specific alternative hypothesis, defined by a matrix of cell probabilities $\\{p_{ij}\\}$ where $\\sum_{i,j} p_{ij} = 1$. We simulate a large number, $R$, of independent experiments. In each experiment, we generate a random contingency table of size $N$ by drawing from a multinomial distribution with parameters $N$ and the flattened probability vector $\\{p_{ij}\\}$.\n\n**Stage 2: Hypothesis Testing**\nEach simulated contingency table is treated as an observed dataset. For each of the $R$ tables, we perform the Pearson's chi-squared test of independence as described above. This involves computing the test statistic and its corresponding $p$-value based on the asymptotic $\\chi^2_{(r-1)(c-1)}$ distribution.\n\n**Stage 3: Power Estimation**\nThe statistical power is estimated as the proportion of simulation replicates in which the null hypothesis was rejected. Let $I_k$ be an indicator variable that is $1$ if the $p$-value for the $k$-th simulated table is less than $\\alpha$, and $0$ otherwise. The Monte Carlo estimator of power, $\\hat{\\pi}$, is the mean of these indicators:\n$$\n\\hat{\\pi} = \\frac{1}{R} \\sum_{k=1}^{R} I_k\n$$\n\n### 3. Quantifying Monte Carlo Error\n\nThe estimator $\\hat{\\pi}$ is itself a random quantity, and its precision depends on the number of replicates $R$. Each replicate is a Bernoulli trial with success probability $\\pi$. The total number of rejections across $R$ trials follows a Binomial distribution, $\\text{Bin}(R, \\pi)$.\n\n- **Standard Error**: The variance of the power estimator is $\\mathrm{Var}(\\hat{\\pi}) = \\frac{\\pi(1-\\pi)}{R}$. The Monte Carlo Standard Error (MCSE) is the standard deviation of the estimator, which we estimate by substituting $\\hat{\\pi}$ for $\\pi$:\n$$\n\\text{MCSE}(\\hat{\\pi}) = \\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{R}}\n$$\nThis quantity measures the typical error of our Monte Carlo estimate.\n\n- **Confidence Interval**: By the Central Limit Theorem, for large $R$, the distribution of $\\hat{\\pi}$ is approximately normal. An approximate two-sided $95\\%$ confidence interval for the true power $\\pi$ is given by:\n$$\n\\hat{\\pi} \\pm z_{0.975} \\times \\text{MCSE}(\\hat{\\pi})\n$$\nwhere $z_{0.975} \\approx 1.96$ is the $0.975$ quantile of the standard normal distribution. The half-width of this interval, $1.96 \\times \\text{MCSE}(\\hat{\\pi})$, provides a measure of the uncertainty in our power estimate.\n\n### 4. Implementation Details\n\nThe simulation will be implemented in Python using `numpy` for numerical operations and `scipy.stats` for the chi-squared test.\n\n- **Reproducibility**: A fixed base random number generator seed of $123456789$ is used. For each test case, a unique, deterministic seed is created by adding the case index to the base seed to ensure independent and reproducible simulation streams.\n- **Simulation Loop**: For each test case, a `numpy.random.default_rng` instance is created. All $R$ contingency tables are generated in a single vectorized call to `rng.multinomial(n=N, pvals=p_flat, size=R)`.\n- **Chi-Squared Test**: We iterate through the $R$ simulated tables. For each table, `scipy.stats.chi2_contingency` is called with `correction=False` to obtain the $p$-value. This function correctly handles cases with zero-sum rows or columns, as required.\n- **Aggregation and Output**: The number of rejections (where $p$-value < $\\alpha$) is counted. The estimated power $\\hat{\\pi}$, MCSE, and $95\\%$ CI half-width are calculated using the formulas above. The final results are rounded to $6$ decimal places and formatted as specified.\n\nThis structured approach ensures that the simulation is scientifically sound, computationally efficient, and yields reproducible results with a clear measure of their precision.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef solve():\n    \"\"\"\n    Simulates the power of the Pearson's chi-squared test of independence\n    for several test cases and reports the estimated power, Monte Carlo\n    standard error, and 95% confidence interval half-width.\n    \"\"\"\n    test_cases = [\n        {\n            \"r\": 3, \"c\": 3, \"alpha\": 0.05, \"N\": 400, \"R\": 20000,\n            \"p_matrix\": np.array([\n                [0.18, 0.07, 0.05],\n                [0.06, 0.20, 0.04],\n                [0.05, 0.05, 0.30]\n            ]),\n            \"case_index\": 0\n        },\n        {\n            \"r\": 2, \"c\": 3, \"alpha\": 0.05, \"N\": 50, \"R\": 15000,\n            \"p_matrix\": np.array([\n                [0.10, 0.05, 0.15],\n                [0.20, 0.10, 0.40]\n            ]),\n            \"case_index\": 1\n        },\n        {\n            \"r\": 4, \"c\": 2, \"alpha\": 0.05, \"N\": 300, \"R\": 20000,\n            \"p_matrix\": np.array([\n                [0.08, 0.02],\n                [0.03, 0.17],\n                [0.15, 0.05],\n                [0.40, 0.10]\n            ]),\n            \"case_index\": 2\n        }\n    ]\n\n    base_seed = 123456789\n    all_results = []\n    z_975 = 1.96\n\n    for case in test_cases:\n        r = case[\"r\"]\n        c = case[\"c\"]\n        alpha = case[\"alpha\"]\n        N = case[\"N\"]\n        R = case[\"R\"]\n        p_matrix = case[\"p_matrix\"]\n        case_index = case[\"case_index\"]\n\n        # Use a deterministic seed for each case for reproducibility\n        rng = np.random.default_rng(base_seed + case_index)\n        \n        # Flatten the probability matrix for multinomial sampling\n        p_flat = p_matrix.flatten()\n\n        # Generate R samples from the multinomial distribution.\n        # Each row of the output is a flattened contingency table.\n        simulated_counts = rng.multinomial(n=N, pvals=p_flat, size=R)\n        \n        rejection_count = 0\n        for i in range(R):\n            # Reshape the flattened counts into an r x c table\n            observed_table = simulated_counts[i].reshape((r, c))\n            \n            # chi2_contingency handles tables with zero-sum rows/columns robustly.\n            # A ValueError can be raised for degenerate tables, which we can safely ignore\n            # as they would not lead to a rejection of the null hypothesis.\n            try:\n                # Perform a Pearson's chi-squared test of independence (correction=False)\n                _, p_value, _, _ = chi2_contingency(observed_table, correction=False)\n\n                if p_value  alpha:\n                    rejection_count += 1\n            except ValueError:\n                # This may occur for degenerate tables (e.g., all but one row/col sum to 0)\n                # scipy.stats.chi2_contingency handles most cases, but this is a safeguard.\n                # Such tables provide no evidence against independence.\n                pass\n        \n        # Estimate power\n        power_hat = rejection_count / R\n        \n        # Compute Monte Carlo standard error\n        # The variance is pi*(1-pi)/R. We plug in the estimate power_hat for pi.\n        # The case of power_hat being 0 or 1 is highly unlikely with large R and\n        # a true power not at the boundary, so np.sqrt is safe.\n        mcse = np.sqrt(power_hat * (1 - power_hat) / R)\n        \n        # Compute 95% CI half-width\n        half_width = z_975 * mcse\n        \n        all_results.extend([\n            round(power_hat, 6), \n            round(mcse, 6), \n            round(half_width, 6)\n        ])\n\n    # Format and print the final results as a single line\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "4899810"}]}