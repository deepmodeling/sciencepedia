{"hands_on_practices": [{"introduction": "Understanding a statistical method begins with mastering its fundamental mechanics. This exercise takes you \"under the hood\" of Analysis of Variance (ANOVA) by asking you to build the entire procedure from first principles. By calculating sums of squares, mean squares, and the final $F$-statistic from raw data, you will gain a concrete understanding of how ANOVA partitions total variability into meaningful components, demystifying the output of standard statistical software. [@problem_id:4965592]", "problem": "You are to write a complete program that constructs a one-way Analysis of Variance (ANOVA) table from raw data using first principles of variance partitioning and then reports only the computed $F$-statistic for each provided dataset. Your implementation must not call any built-in ANOVA routines; it must compute all quantities from core definitions of averages and sums of squared deviations. Begin from the following fundamental base: the definitions of sample means and sums of squared deviations, the identity that total variation partitions additively into within-group and between-group components, and the independence and homoscedasticity assumptions of the one-way ANOVA model under which the $F$-statistic has an $F$ distribution. Specifically, use only the following principles as starting points:\n- For any finite set of real-valued observations $\\{y\\}$, the sample mean is $\\bar{y} = \\frac{1}{n}\\sum_{j=1}^{n} y_{j}$ and the total sum of squared deviations is $\\sum_{j=1}^{n} (y_{j} - \\bar{y})^{2}$.\n- In a one-way grouping with $k$ groups and group sizes $n_{i}$, the total sum of squares partitions as $SS_{T} = SS_{W} + SS_{B}$, where $SS_{W}$ is the within-groups sum of squares computed from deviations to group means and $SS_{B}$ is the between-groups sum of squares computed from deviations of group means to the overall mean, weighted by group sizes.\n- Under the one-way ANOVA model with independent, normally distributed errors of equal variance across groups, the ratio of mean squares $F = MS_{B}/MS_{W}$ follows an $F$ distribution with numerator degrees of freedom $k-1$ and denominator degrees of freedom $N - k$ under the null hypothesis that all group means are equal, where $N = \\sum_{i=1}^{k} n_{i}$.\n\nYour program must, for each dataset:\n1. Compute each group mean $\\bar{y}_{i}$, the overall mean $\\bar{y}$, the within-groups sum of squares $SS_{W} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(y_{ij} - \\bar{y}_{i})^{2}$, the total sum of squares $SS_{T} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(y_{ij} - \\bar{y})^{2}$, and the between-groups sum of squares $SS_{B} = SS_{T} - SS_{W}$ (thus verifying the variance partitioning identity).\n2. Compute the degrees of freedom $df_{B} = k - 1$ and $df_{W} = N - k$, the mean squares $MS_{B} = SS_{B}/df_{B}$ and $MS_{W} = SS_{W}/df_{W}$, and the $F$-statistic $F = MS_{B}/MS_{W}$.\n3. For numerical robustness, if $MS_{W} = 0$, define $F$ as $+\\infty$ if $SS_{B} > 0$, and define $F$ as $\\mathrm{nan}$ if also $SS_{B} = 0$.\n\nTest Suite:\nUse exactly the following four datasets, each represented as a list of groups, where each group is a list of real numbers.\n\n- Test case A (balanced, clear between-group separation): $k = 3$, group data\n  - Group $1$: $[4.1, 4.3, 4.2, 4.0]$\n  - Group $2$: $[5.0, 5.1, 4.9, 5.2]$\n  - Group $3$: $[6.0, 5.8, 6.2, 5.9]$\n- Test case B (no between-group variation; all group means exactly equal): $k = 3$, group data\n  - Group $1$: $[10.0, 12.0, 8.0]$\n  - Group $2$: $[9.0, 10.0, 11.0]$\n  - Group $3$: $[7.0, 10.0, 13.0]$\n- Test case C (small and unequal sample sizes): $k = 2$, group data\n  - Group $1$: $[2.0, 2.1]$\n  - Group $2$: $[2.5, 2.4, 2.6]$\n- Test case D (unequal group sizes with one group shifted): $k = 4$, group data\n  - Group $1$: $[15.0, 16.0, 14.0]$\n  - Group $2$: $[15.0, 15.5]$\n  - Group $3$: $[14.8, 15.2, 15.0, 15.1]$\n  - Group $4$: $[18.0, 17.5, 18.2]$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output only the $F$-statistic as a floating-point number rounded to $6$ decimal places, in the order A, B, C, D. For example, the final output must look like\n$[\\text{FA},\\text{FB},\\text{FC},\\text{FD}]$\nwith no additional text or whitespace beyond what is necessary to separate the numbers with commas.\n\nNo physical units are involved in this problem, so do not report any units. Do not read any input; the program must be fully self-contained and reproducible as specified above.", "solution": "The problem requires the implementation of a one-way Analysis of Variance (ANOVA) from first principles to compute the $F$-statistic for several datasets. The solution is derived by systematically applying the definitions of variance components as specified.\n\nThe core principle of one-way ANOVA is the partitioning of total variation in a dataset into variation between groups and variation within groups. Let the data consist of $k$ groups. The $i$-th group (for $i \\in \\{1, 2, \\dots, k\\}$) contains $n_i$ observations, denoted by $y_{ij}$ where $j \\in \\{1, 2, \\dots, n_i\\}$. The total number of observations in the dataset is $N = \\sum_{i=1}^{k} n_i$.\n\nThe algorithmic process for calculating the $F$-statistic for a given dataset proceeds as follows:\n\n1.  **Calculation of Means**:\n    First, we compute the necessary sample means.\n    -   The mean of each group $i$, denoted $\\bar{y}_i$, is calculated as:\n        $$ \\bar{y}_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} y_{ij} $$\n    -   The overall mean of all observations, denoted $\\bar{y}$, is calculated as:\n        $$ \\bar{y} = \\frac{1}{N} \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} y_{ij} $$\n\n2.  **Calculation of Sums of Squares (SS)**:\n    The variance partitioning is performed by calculating the sums of squared deviations.\n    -   The **Total Sum of Squares** ($SS_T$) measures the total variation of all data points around the overall mean. It is defined as:\n        $$ SS_T = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y})^2 $$\n    -   The **Within-Groups Sum of Squares** ($SS_W$), also known as the error sum of squares, measures the variation of data points around their respective group means. It is calculated by summing the squared deviations within each group and then summing these results across all groups:\n        $$ SS_W = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_i)^2 $$\n    -   The **Between-Groups Sum of Squares** ($SS_B$) measures the variation of the group means around the overall mean, weighted by group size. As per the problem's directive, this quantity is computed using the fundamental partitioning identity of ANOVA:\n        $$ SS_B = SS_T - SS_W $$\n        This identity, $SS_T = SS_B + SS_W$, is a cornerstone of ANOVA, stating that total variation is the sum of between-group and within-group variations.\n\n3.  **Calculation of Degrees of Freedom (df)**:\n    Each sum of squares term is associated with a number of degrees of freedom.\n    -   The degrees of freedom for $SS_B$ are $df_B = k - 1$, corresponding to the $k$ group means minus $1$ constraint (the overall mean).\n    -   The degrees of freedom for $SS_W$ are $df_W = N - k$, corresponding to the $N$ total observations minus the $k$ group means that were calculated from them.\n\n4.  **Calculation of Mean Squares (MS)**:\n    The mean squares are the sums of squares normalized by their respective degrees of freedom, representing average variation.\n    -   The **Mean Square Between Groups** is:\n        $$ MS_B = \\frac{SS_B}{df_B} $$\n    -   The **Mean Square Within Groups** is:\n        $$ MS_W = \\frac{SS_W}{df_W} $$\n    $MS_B$ represents the variance between the groups, while $MS_W$ represents the pooled variance within the groups.\n\n5.  **Calculation of the F-Statistic**:\n    The $F$-statistic is the ratio of the between-group variance to the within-group variance.\n    $$ F = \\frac{MS_B}{MS_W} $$\n    A large $F$-value suggests that the variation between groups is significantly larger than the variation within groups, providing evidence against the null hypothesis that all group means are equal.\n\n6.  **Handling of Special Cases**:\n    The problem specifies rules for robustness when the denominator, $MS_W$, is zero. This occurs if and only if $SS_W=0$, which happens when all observations within each group are identical.\n    -   If $MS_W = 0$ (i.e., $SS_W = 0$) and $SS_B > 0$, the variation between groups is non-zero while the variation within is zero. This implies infinitely strong evidence for a difference, so $F$ is defined as $+\\infty$.\n    -   If $MS_W = 0$ (i.e., $SS_W = 0$) and $SS_B = 0$, all observations in the entire dataset are identical. In this case, the F-statistic is indeterminate, and is defined as Not-a-Number ($\\mathrm{nan}$).\n\nThe implementation translates these steps into a function that processes each dataset, calculates all intermediate quantities ($N$, $k$, means, $SS_T$, $SS_W$, $SS_B$, $df_B$, $df_W$, $MS_B$, $MS_W$), and returns the final $F$-statistic, including the specified handling for special cases. The function is then called for each test case, and the results are formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef compute_f_statistic(data: list[list[float]]) -> float:\n    \"\"\"\n    Computes the F-statistic for a one-way ANOVA from first principles.\n\n    Args:\n        data: A list of lists, where each inner list represents a group's data.\n\n    Returns:\n        The calculated F-statistic as a float.\n    \"\"\"\n    # 1. Compute counts and consolidate data into a single array.\n    k = len(data)\n    if k <= 1:\n        # F-statistic is not well-defined for a single group or no groups.\n        return math.nan\n\n    all_obs_list = [obs for group in data for obs in group]\n    N = len(all_obs_list)\n    \n    if N <= k:\n        # This implies at least one group is empty or all groups have 1 obs,\n        # leading to df_w <= 0.\n        if N == k and all(len(g)==1 for g in data):\n             # SS_W will be 0. Need to check SSB\n             ss_b_check = np.var(all_obs_list) * N\n             if ss_b_check > 0: return float('inf')\n             else: return float('nan')\n        return math.nan\n\n    all_obs_np = np.array(all_obs_list, dtype=np.float64)\n\n    # 2. Compute the group means and the overall mean.\n    overall_mean = np.mean(all_obs_np)\n    group_means = [np.mean(np.array(g, dtype=np.float64)) for g in data]\n\n    # 3. Compute the Sums of Squares (SS).\n    # SS_T: Total Sum of Squares\n    ss_t = np.sum((all_obs_np - overall_mean)**2)\n\n    # SS_W: Within-Groups Sum of Squares\n    ss_w = 0.0\n    for i in range(k):\n        group_data = np.array(data[i], dtype=np.float64)\n        ss_w += np.sum((group_data - group_means[i])**2)\n    \n    # SS_B: Between-Groups Sum of Squares, derived from the partitioning identity.\n    ss_b = ss_t - ss_w\n    \n    # Numpy's float precision can sometimes make a very small positive number\n    # slightly negative. We correct this for ss_b, which must be non-negative.\n    if ss_b < 0 and np.isclose(ss_b, 0):\n        ss_b = 0.0\n\n    # 4. Compute degrees of freedom.\n    df_b = k - 1\n    df_w = N - k\n\n    # 5. Handle special cases as per problem description (based on SS_W).\n    # MS_W = 0 if and only if SS_W = 0.\n    if np.isclose(ss_w, 0):\n        ss_w = 0.0\n\n    if ss_w == 0:\n        if ss_b > 0:\n            return float('inf')\n        else: # ss_b is also 0\n            return float('nan')\n\n    # 6. Compute Mean Squares (MS) and the F-statistic.\n    ms_b = ss_b / df_b\n    ms_w = ss_w / df_w\n    \n    f_statistic = ms_b / ms_w\n    \n    return f_statistic\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A\n        [[4.1, 4.3, 4.2, 4.0], [5.0, 5.1, 4.9, 5.2], [6.0, 5.8, 6.2, 5.9]],\n        # Test case B\n        [[10.0, 12.0, 8.0], [9.0, 10.0, 11.0], [7.0, 10.0, 13.0]],\n        # Test case C\n        [[2.0, 2.1], [2.5, 2.4, 2.6]],\n        # Test case D\n        [[15.0, 16.0, 14.0], [15.0, 15.5], [14.8, 15.2, 15.0, 15.1], [18.0, 17.5, 18.2]]\n    ]\n\n    results = []\n    for case in test_cases:\n        f_value = compute_f_statistic(case)\n        results.append(f_value)\n\n    # Final print statement in the exact required format.\n    # The format specifier {:.6f} correctly handles regular floats, 'inf', and 'nan'.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "4965592"}, {"introduction": "In an ideal, perfectly balanced experiment, factors are orthogonal, and the variance explained by each is unambiguous. Real-world data, however, is often \"messy\" and non-orthogonal, meaning predictors are correlated. This practice challenges you to explore the consequences of non-orthogonality by calculating Type I and Type III sums of squares, revealing how the order in which factors are entered into a model can change the variance attributed to them. [@problem_id:4965582]", "problem": "A biostatistician is analyzing a continuous response variable $y$ measured on $n=6$ subjects. Two binary predictors are recorded: $A$ (e.g., exposure present $=1$ or absent $=0$) and $B$ (e.g., co-factor present $=1$ or absent $=0$). The design is intentionally non-orthogonal because $A$ and $B$ are associated in the sample. Consider the linear model with an intercept and the two main effects, with no interaction. The design matrix $X$ and the response vector $y$ are given by\n$$\nX \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\n\\end{pmatrix}\n,\\qquad\ny \\;=\\;\n\\begin{pmatrix}\n2\\\\\n3\\\\\n5\\\\\n6\\\\\n8\\\\\n9\n\\end{pmatrix},\n$$\nwhere the columns of $X$ represent the intercept, the indicator for $A$, and the indicator for $B$, respectively.\n\nUsing only the fundamental definitions of variance partitioning in ordinary least squares (OLS), namely that the sequential (Type I) model sum of squares for a term is the increase in regression sum of squares when the term is added to the current model, and that the partial (Type III) sum of squares for a term is the increment in regression sum of squares for that term given all other terms already in the model:\n\n1. Verify that the factors are non-orthogonal in this sample by exhibiting a nonzero off-diagonal element of $X^{\\top}X$ corresponding to the columns for $A$ and $B$.\n2. Compute the sequential (Type I) model sum of squares attributed to $B$ when $B$ is entered first after the intercept.\n3. Compute the sequential (Type I) model sum of squares attributed to $B$ when $B$ is entered second, after the intercept and $A$.\n4. Compute the partial (Type III) sum of squares attributed to $B$ in the full model with intercept, $A$, and $B$.\n5. For each of the three tests in parts $2$â€“$4$, compute the corresponding $F$-statistic using the ratio of the mean square for $B$ to the error mean square from the model at that step. State the numerator and denominator degrees of freedom for each $F$-statistic.\n\nFinally, report the single quantity\n$$\n\\Delta \\;=\\; SS_{I}(B\\ \\text{first}) \\;-\\; SS_{I}(B\\ \\text{second}).\n$$\nGive your final answer for $\\Delta$ as an exact fraction with no rounding and no units.", "solution": "The problem as stated constitutes a well-posed and standard exercise in the theory of linear models. All necessary data and definitions are provided, the context is scientifically sound within the field of biostatistics, and the questions are objective and formalizable. The problem is therefore deemed valid and a full solution follows.\n\nLet the linear model be written as $y = X\\beta + \\epsilon$, where $\\beta = (\\beta_0, \\beta_A, \\beta_B)^\\top$. The columns of the design matrix $X$ will be denoted $X_0$ for the intercept, $X_A$ for predictor $A$, and $X_B$ for predictor $B$. The uncorrected regression sum of squares for a set of parameters is denoted by $R(\\cdot)$, and the sequential sum of squares for a parameter $\\beta_k$ given a set of parameters $S$ is $R(\\beta_k|S)$. The total sum of squares, corrected for the mean, is $SST = y^\\top y - (\\sum y_i)^2/n$.\n\nFirst, we compute some basic quantities from the given response vector $y$:\n$n=6$\n$\\sum y_i = 2+3+5+6+8+9 = 33$\n$\\bar{y} = \\frac{33}{6} = \\frac{11}{2} = 5.5$\n$\\sum y_i^2 = 2^2+3^2+5^2+6^2+8^2+9^2 = 4+9+25+36+64+81 = 219$\nThe correction factor for the mean is $C = R(\\beta_0) = \\frac{(\\sum y_i)^2}{n} = \\frac{33^2}{6} = \\frac{1089}{6} = \\frac{363}{2} = 181.5$.\nThe total sum of squares is $SST = \\sum y_i^2 - C = 219 - 181.5 = 37.5 = \\frac{75}{2}$.\n\n**1. Verification of Non-Orthogonality**\nTo verify that factors $A$ and $B$ are non-orthogonal, we compute the matrix $X^\\top X$. Two factors are orthogonal if the inner product of their corresponding column vectors in the design matrix is zero.\n$$\nX^\\top X =\n\\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1 \\\\\n0 & 0 & 1 & 0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 1 & 1\\\\\n1 & 1 & 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n6 & 3 & 3 \\\\\n3 & 3 & 2 \\\\\n3 & 2 & 3\n\\end{pmatrix}\n$$\nThe columns corresponding to factors $A$ and $B$ are the second and third columns of $X$. The off-diagonal element in $X^\\top X$ corresponding to these two factors is at position $(2,3)$ (or $(3,2)$), which represents the inner product $X_A^\\top X_B$. From the matrix, we see that $(X^\\top X)_{23} = 2$. Since this value is non-zero, the factors $A$ and $B$ are non-orthogonal in this sample.\n\n**2. Sequential (Type I) SS for $B$ entered first**\nThis sum of squares is $SS_{I}(B\\ \\text{first}) = R(\\beta_B|\\beta_0) = R(\\beta_0, \\beta_B) - R(\\beta_0)$. We need to fit a model with the intercept and factor $B$. The design matrix is $X_{0B} = (X_0, X_B)$.\n$X_{0B}^\\top X_{0B} = \\begin{pmatrix} 6 & 3 \\\\ 3 & 3 \\end{pmatrix}$.\nThe inverse is $(X_{0B}^\\top X_{0B})^{-1} = \\frac{1}{6(3)-3(3)} \\begin{pmatrix} 3 & -3 \\\\ -3 & 6 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 3 & -3 \\\\ -3 & 6 \\end{pmatrix}$.\n$X_{0B}^\\top y = \\begin{pmatrix} \\sum y_i \\\\ y_3+y_5+y_6 \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 5+8+9 \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 22 \\end{pmatrix}$.\nThe uncorrected sum of squares is $R(\\beta_0, \\beta_B) = (X_{0B}^\\top y)^\\top (X_{0B}^\\top X_{0B})^{-1} (X_{0B}^\\top y)$.\nFirst, $\\hat{\\beta}_{0B} = (X_{0B}^\\top X_{0B})^{-1} (X_{0B}^\\top y) = \\frac{1}{9}\\begin{pmatrix} 3 & -3 \\\\ -3 & 6 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 22 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 99-66 \\\\ -99+132 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 33 \\\\ 33 \\end{pmatrix} = \\begin{pmatrix} 11/3 \\\\ 11/3 \\end{pmatrix}$.\nThen $R(\\beta_0, \\beta_B) = \\hat{\\beta}_{0B}^\\top (X_{0B}^\\top y) = \\begin{pmatrix} 11/3 & 11/3 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 22 \\end{pmatrix} = \\frac{11}{3}(33) + \\frac{11}{3}(22) = 121 + \\frac{242}{3} = \\frac{363+242}{3} = \\frac{605}{3}$.\nThe sequential sum of squares for $B$ is:\n$SS_{I}(B|\\beta_0) = R(\\beta_0, \\beta_B) - R(\\beta_0) = \\frac{605}{3} - \\frac{363}{2} = \\frac{1210 - 1089}{6} = \\frac{121}{6}$.\n\n**3. Sequential (Type I) SS for $B$ entered second**\nThis sum of squares is $SS_{I}(B\\ \\text{second}) = R(\\beta_B|\\beta_0, \\beta_A) = R(\\beta_0, \\beta_A, \\beta_B) - R(\\beta_0, \\beta_A)$. This requires calculating the sum of squares for the full model and the model with only intercept and $A$.\n\nFor the full model (Intercept, $A$, $B$):\nFrom part 1, $X^\\top X = \\begin{pmatrix} 6 & 3 & 3 \\\\ 3 & 3 & 2 \\\\ 3 & 2 & 3 \\end{pmatrix}$. The determinant is $12$.\n$(X^\\top X)^{-1} = \\frac{1}{12} \\begin{pmatrix} 5 & -3 & -3 \\\\ -3 & 9 & -3 \\\\ -3 & -3 & 9 \\end{pmatrix}$.\n$X^\\top y = \\begin{pmatrix} \\sum y_i \\\\ y_4+y_5+y_6 \\\\ y_3+y_5+y_6 \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 6+8+9 \\\\ 5+8+9 \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 23 \\\\ 22 \\end{pmatrix}$.\n$\\hat{\\beta} = (X^\\top X)^{-1}(X^\\top y) = \\frac{1}{12} \\begin{pmatrix} 5(33)-3(23)-3(22) \\\\ -3(33)+9(23)-3(22) \\\\ -3(33)-3(23)+9(22) \\end{pmatrix} = \\frac{1}{12} \\begin{pmatrix} 165-69-66 \\\\ -99+207-66 \\\\ -99-69+198 \\end{pmatrix} = \\frac{1}{12} \\begin{pmatrix} 30 \\\\ 42 \\\\ 30 \\end{pmatrix} = \\begin{pmatrix} 5/2 \\\\ 7/2 \\\\ 5/2 \\end{pmatrix}$.\n$R(\\beta_0, \\beta_A, \\beta_B) = \\hat{\\beta}^\\top (X^\\top y) = \\begin{pmatrix} 5/2 & 7/2 & 5/2 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 23 \\\\ 22 \\end{pmatrix} = \\frac{165+161+110}{2} = \\frac{436}{2} = 218$.\n\nFor the model with Intercept and $A$:\n$X_{0A} = (X_0, X_A)$.\n$X_{0A}^\\top X_{0A} = \\begin{pmatrix} 6 & 3 \\\\ 3 & 3 \\end{pmatrix}$, with inverse $\\frac{1}{9} \\begin{pmatrix} 3 & -3 \\\\ -3 & 6 \\end{pmatrix}$.\n$X_{0A}^\\top y = \\begin{pmatrix} 33 \\\\ 23 \\end{pmatrix}$.\n$\\hat{\\beta}_{0A} = \\frac{1}{9} \\begin{pmatrix} 3 & -3 \\\\ -3 & 6 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 23 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 99-69 \\\\ -99+138 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} 30 \\\\ 39 \\end{pmatrix} = \\begin{pmatrix} 10/3 \\\\ 13/3 \\end{pmatrix}$.\n$R(\\beta_0, \\beta_A) = \\hat{\\beta}_{0A}^\\top (X_{0A}^\\top y) = \\begin{pmatrix} 10/3 & 13/3 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 23 \\end{pmatrix} = \\frac{330+299}{3} = \\frac{629}{3}$.\nThe sequential sum of squares for $B$ is:\n$SS_{I}(B|\\beta_0, \\beta_A) = R(\\beta_0, \\beta_A, \\beta_B) - R(\\beta_0, \\beta_A) = 218 - \\frac{629}{3} = \\frac{654-629}{3} = \\frac{25}{3}$.\n\n**4. Partial (Type III) SS for $B$**\nThe partial sum of squares for $B$ is defined as the increment in regression sum of squares when $B$ is added to a model containing all other terms. In this case, the other terms are the intercept and $A$. Therefore, $SS_{III}(B) = R(\\beta_B|\\beta_0, \\beta_A)$.\nThis is identical to the calculation in part 3.\n$SS_{III}(B|\\beta_0, \\beta_A) = R(\\beta_0, \\beta_A, \\beta_B) - R(\\beta_0, \\beta_A) = \\frac{25}{3}$.\n\n**5. F-statistics and Degrees of Freedom**\nThe $F$-statistic is given by $F = \\frac{MS_{effect}}{MS_{error}} = \\frac{SS_{effect}/df_{effect}}{SSE/df_{error}}$. For a single binary predictor, $df_{effect}=1$.\n\n- **For the test in part 2 ($B$ entered first):**\nThe effect sum of squares is $SS_B = SS_I(B|\\beta_0) = \\frac{121}{6}$. The relevant error term is from the model $y \\sim \\text{Intercept} + B$.\n$SSE(\\beta_0, \\beta_B) = \\sum y_i^2 - R(\\beta_0, \\beta_B) = 219 - \\frac{605}{3} = \\frac{657-605}{3} = \\frac{52}{3}$.\nThe error degrees of freedom are $df_{error} = n - (\\text{number of parameters}) = 6 - 2 = 4$.\n$MS_B = \\frac{121/6}{1} = \\frac{121}{6}$.\n$MSE(\\beta_0, \\beta_B) = \\frac{52/3}{4} = \\frac{13}{3}$.\n$F = \\frac{MS_B}{MSE} = \\frac{121/6}{13/3} = \\frac{121}{6} \\times \\frac{3}{13} = \\frac{121}{26}$.\nThe degrees of freedom are $(1, 4)$.\n\n- **For the test in part 3 ($B$ entered second):**\nThe effect sum of squares is $SS_B = SS_I(B|\\beta_0, \\beta_A) = \\frac{25}{3}$. The relevant error term is from the full model $y \\sim \\text{Intercept} + A + B$.\n$SSE(\\beta_0, \\beta_A, \\beta_B) = \\sum y_i^2 - R(\\beta_0, \\beta_A, \\beta_B) = 219 - 218 = 1$.\nThe error degrees of freedom are $df_{error} = n - 3 = 3$.\n$MS_B = \\frac{25/3}{1} = \\frac{25}{3}$.\n$MSE(\\beta_0, \\beta_A, \\beta_B) = \\frac{1}{3}$.\n$F = \\frac{MS_B}{MSE} = \\frac{25/3}{1/3} = 25$.\nThe degrees of freedom are $(1, 3)$.\n\n- **For the test in part 4 (Partial SS for B):**\nThe effect sum of squares is $SS_B = SS_{III}(B|\\beta_0, \\beta_A) = \\frac{25}{3}$. The error term for a Type III test is always from the full model.\nThis test is identical to the sequential test for the last term entered into the model.\n$F = 25$ with $(1, 3)$ degrees of freedom.\n\n**Final Quantity $\\Delta$**\nThe problem asks for the quantity $\\Delta = SS_{I}(B\\ \\text{first}) - SS_{I}(B\\ \\text{second})$.\nUsing our results from parts 2 and 3:\n$\\Delta = \\frac{121}{6} - \\frac{25}{3} = \\frac{121}{6} - \\frac{50}{6} = \\frac{71}{6}$.\nThis difference is non-zero, directly reflecting the non-orthogonality of the predictors $A$ and $B$.", "answer": "$$\\boxed{\\frac{71}{6}}$$", "id": "4965582"}, {"introduction": "Effective experimental design is as crucial as correct analysis. A primary question before starting any study is, \"How large does my sample need to be?\" This practice shifts focus from analyzing existing data to planning future experiments by implementing a power analysis tool. You will use the noncentral $F$-distribution to determine the minimum sample size required to detect a specified effect with a desired probability (power), a critical skill for designing efficient and ethical research. [@problem_id:4965568]", "problem": "You are tasked with constructing a program that performs a power analysis for a balanced one-way fixed-effects Analysis of Variance (ANOVA). The program must determine the minimal integer per-group sample size required to achieve a specified probability of correctly rejecting the null hypothesis, under assumed normality and a common within-group variance. The approach must explicitly compare a central critical threshold to an inverse survival quantile from the noncentral F distribution.\n\nThe foundational setting is a balanced one-way fixed-effects ANOVA model with $k$ groups, per-group sample size $n$, and total sample size $N = k n$. Group $i$ has population mean $\\mu_i$ and all groups share a common error variance $\\sigma^2$. The hypothesis test targets $H_0$ that all group means are equal versus the fixed-effects alternative that not all $\\mu_i$ are equal. The error probability threshold is the significance level $\\alpha \\in (0,1)$, and the desired probability of rejecting $H_0$ under the fixed-effects alternative is the power level $1 - \\beta \\in (0,1)$.\n\nYour program must:\n- Assume a balanced design with equal $n$ across all groups.\n- Compute the minimal integer per-group sample size $n \\geq 2$ that achieves at least the specified power level for the provided group means and common within-group variance. If the specified means imply no difference (i.e., all group means are equal in the sense required by the model), the program must output $-1$ for that test case to indicate that the target power is unattainable at any finite sample size.\n- Use the central F distribution to determine the critical threshold at tail probability $\\alpha$ for degrees of freedom $df_1 = k - 1$ and $df_2 = N - k$, and use the noncentral F inverse survival quantile at the specified power with the appropriate noncentrality parameter under the fixed-effects alternative to assess whether the power requirement is met.\n- Ensure the computed per-group sample size is the smallest integer satisfying the requirement.\n\nThe program must implement a monotone search strategy over $n$ (for example, bracketing followed by binary search), and it must base the decision logic on a comparison between the central F critical threshold and the noncentral F inverse survival quantile at the target power.\n\nThere are no physical units involved in this computation. All outputs must be integers. If the target power is unattainable for the provided parameters (which occurs if the group means are all equal), output $-1$.\n\nTest Suite:\nEvaluate your program on the following four cases. In each case, the input is the list of group means, the common within-group variance, the significance level, and the desired power.\n\n- Case $1$: means $[0.0, 0.5, 1.0]$, variance $\\sigma^2 = 1.0$, significance $\\alpha = 0.05$, desired power $1 - \\beta = 0.8$.\n- Case $2$: means $[0.0, 0.2]$, variance $\\sigma^2 = 1.0$, significance $\\alpha = 0.05$, desired power $1 - \\beta = 0.9$.\n- Case $3$: means $[1.0, 1.0, 1.0, 1.0]$, variance $\\sigma^2 = 1.0$, significance $\\alpha = 0.05$, desired power $1 - \\beta = 0.8$.\n- Case $4$: means $[-1.0, -0.5, 0.0, 0.5, 1.0]$, variance $\\sigma^2 = 0.5$, significance $\\alpha = 0.01$, desired power $1 - \\beta = 0.95$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the integer results for the four cases as a comma-separated list enclosed in square brackets, in the same order as the cases above. For example, the output format must be like $[n_1,n_2,n_3,n_4]$ where each $n_i$ is the computed minimal per-group sample size for Case $i$, or $-1$ if unattainable.", "solution": "The user-provided problem has been rigorously validated and is determined to be **valid**. It is a well-posed, scientifically sound problem in the domain of biostatistical power analysis. All necessary parameters are specified, the terminology is precise, and the objective is clear. The problem asks for the implementation of a standard, albeit non-trivial, statistical procedure.\n\nHerein, a complete, reasoned solution is provided.\n\n### Theoretical Foundation of One-Way ANOVA Power Analysis\n\nThe problem is situated in the context of a one-way fixed-effects Analysis of Variance (ANOVA). The statistical model for an observation $Y_{ij}$ from group $i \\in \\{1, \\dots, k\\}$ and subject $j \\in \\{1, \\dots, n\\}$ is:\n$$\nY_{ij} = \\mu_i + \\epsilon_{ij}\n$$\nwhere $\\mu_i$ is the true mean of group $i$, and the error terms $\\epsilon_{ij}$ are assumed to be independent and identically distributed normal random variables, $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, with a common variance $\\sigma^2$ across all $k$ groups. The design is balanced, meaning each group has the same sample size, $n$. The total sample size is $N = kn$.\n\nThe hypothesis test for ANOVA is:\n-   Null Hypothesis $H_0$: All group means are equal, i.e., $\\mu_1 = \\mu_2 = \\dots = \\mu_k$.\n-   Alternative Hypothesis $H_1$: At least one group mean is different from the others.\n\nThe test statistic is the ratio of the Mean Square Between groups ($MSB$) to the Mean Square Within groups ($MSW$):\n$$\nF = \\frac{MSB}{MSW} = \\frac{SSB / (k-1)}{SSW / (N-k)}\n$$\nThe distributions of this F-statistic under the null and alternative hypotheses are central to power analysis:\n1.  Under $H_0$, the statistic $F$ follows a **central F-distribution** with $df_1 = k-1$ and $df_2 = N-k$ degrees of freedom. We denote this as $F \\sim F(df_1, df_2)$.\n2.  Under $H_1$, the statistic $F$ follows a **noncentral F-distribution** with the same degrees of freedom and a noncentrality parameter (NCP), $\\lambda$. We denote this as $F \\sim F(df_1, df_2, \\lambda)$.\n\nThe noncentrality parameter $\\lambda$ quantifies the degree to which the null hypothesis is false. For a balanced one-way ANOVA, it is defined as:\n$$\n\\lambda = \\frac{n \\sum_{i=1}^{k} (\\mu_i - \\bar{\\mu})^2}{\\sigma^2}\n$$\nwhere $\\bar{\\mu} = \\frac{1}{k} \\sum_{i=1}^{k} \\mu_i$ is the grand mean of the population means.\n\n### Power Calculation and Computational Strategy\n\nStatistical power ($1-\\beta$) is the probability of correctly rejecting a false null hypothesis. We reject $H_0$ if the observed F-statistic, $F_{obs}$, exceeds a critical value, $F_{crit}$. This critical value is determined by the significance level $\\alpha$. Specifically, $F_{crit}$ is the value for which the probability of observing an F-statistic greater than it, under $H_0$, is exactly $\\alpha$. This corresponds to the upper $\\alpha$-quantile of the central F-distribution:\n$$\nF_{crit} = F_{\\text{isf}}(\\alpha; df_1, df_2)\n$$\nwhere $F_{\\text{isf}}$ is the inverse survival function (or quantile point function for the upper tail) of the central F-distribution.\n\nThe power is then the probability of this rejection event occurring when $H_1$ is true:\n$$\n\\text{Power} = P(F > F_{crit} | H_1)\n$$\nSince under $H_1$ the F-statistic follows a noncentral F-distribution, the power is calculated using the survival function (SF) of this distribution:\n$$\n\\text{Power} = \\text{SF}_{ncf}(F_{crit}; df_1, df_2, \\lambda)\n$$\nThe goal is to find the minimum integer per-group sample size $n \\geq 2$ that satisfies:\n$$\n\\text{Power} \\geq 1-\\beta\n$$\nwhere $1-\\beta$ is the desired power level.\n\nThe problem specifies a particular method for this check, which involves comparing the central critical threshold to an inverse survival quantile from the noncentral F-distribution. This is an elegant reformulation of the power condition. The condition $\\text{SF}_{ncf}(F_{crit}; df_1, df_2, \\lambda) \\geq 1-\\beta$ is equivalent to:\n$$\nF_{crit} \\leq F_{ncf, \\text{isf}}(1-\\beta; df_1, df_2, \\lambda)\n$$\nThis equivalence holds because the survival function is a monotonically decreasing function, so applying its inverse (the inverse survival function) to both sides of the inequality reverses the inequality sign. The right-hand side of this inequality is precisely the \"inverse survival quantile from the noncentral F distribution at the specified power,\" as required.\n\n### Search Algorithm for Minimal Sample Size $n$\n\nThe core of the problem is to find the smallest integer $n \\geq 2$ that satisfies the inequality above. Critically, both sides of the inequality depend on $n$:\n-   $df_2(n) = k(n-1)$\n-   $\\lambda(n) = n \\cdot C$, where the constant $C = \\frac{\\sum (\\mu_i - \\bar{\\mu})^2}{\\sigma^2}$ depends only on the given problem parameters.\n\nAn analysis of the inequality's dependence on $n$ reveals its monotonic nature:\n-   The left-hand side, $F_{crit}(n) = F_{\\text{isf}}(\\alpha; k-1, k(n-1))$, is a **decreasing** function of $n$, as increasing the denominator degrees of freedom $df_2$ makes the central F-distribution less spread out.\n-   The right-hand side, $F_{ncf, \\text{isf}}(1-\\beta; k-1, k(n-1), \\lambda(n))$, is an **increasing** function of $n$. This is because both an increase in $df_2$ and a linear increase in the NCP $\\lambda$ shift the noncentral F-distribution to the right, increasing its quantiles.\n\nSince a decreasing function is being compared to an increasing function, the condition will be met for all $n$ above some threshold. This structure makes the problem amenable to an efficient search.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Calculate $k$ from the list of means. Compute the sum of squared deviations of means, $\\sum_{i=1}^{k} (\\mu_i - \\bar{\\mu})^2$. If this sum is zero (or numerically indistinguishable from zero), it implies $\\lambda=0$ for all $n$. In this scenario, the power is always equal to the significance level $\\alpha$. If $\\alpha$ is less than the target power $1-\\beta$, the goal is unattainable. The program must return $-1$ as stipulated.\n2.  **Search for $n$**: We seek the smallest integer $n \\geq 2$ satisfying the power condition.\n    -   A robust approach is to first establish a search range $[n_{low}, n_{high}]$ where the condition is not met at $n_{low}$ and is met at $n_{high}$. This can be done by starting with a low value (e.g., $n_{low}=2$) and exponentially increasing a test value until the power requirement is met, which then becomes $n_{high}$.\n    -   Once this bracket is established, a **binary search** is performed within the range $[n_{low}, n_{high}]$ to efficiently pinpoint the smallest integer $n$ for which the power condition holds. This strategy is efficient and guaranteed to find the minimal integer solution due to the established monotonicity.\n\nThe final Python implementation will use `numpy` for numerical calculations and `scipy.stats.f.isf` and `scipy.stats.ncf.isf` to compute the required statistical quantiles, directly embodying the described logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f, ncf\n\ndef compute_min_n_power_anova(\n    means: list[float], variance: float, alpha: float, power: float\n) -> int:\n    \"\"\"\n    Computes the minimal integer per-group sample size for a one-way ANOVA.\n\n    Args:\n        means: A list of population group means.\n        variance: The common within-group variance (sigma^2).\n        alpha: The significance level (Type I error probability).\n        power: The desired statistical power (1 - beta).\n\n    Returns:\n        The minimal integer sample size n per group (n >= 2), or -1 if\n        the target power is unattainable (i.e., all group means are equal).\n    \"\"\"\n\n    k = len(means)\n    if k < 2:\n        # ANOVA requires at least 2 groups.\n        # This case is not in the test suite but is a logical check.\n        return -1\n\n    # Calculate the sum of squared deviations of group means from the grand mean\n    mu_array = np.array(means)\n    grand_mean = np.mean(mu_array)\n    sum_sq_dev = np.sum((mu_array - grand_mean) ** 2)\n\n    # If all means are equal, the noncentrality parameter is always 0.\n    # Power will be equal to alpha, so target power is unattainable.\n    if np.isclose(sum_sq_dev, 0.0):\n        return -1\n\n    effect_size_term = sum_sq_dev / variance\n\n    def is_power_sufficient(n: int) -> bool:\n        \"\"\"\n        Checks if a given sample size n achieves the target power.\n        The check is based on the problem's specified comparison.\n        \"\"\"\n        if n < 2:\n            return False\n\n        df1 = k - 1\n        df2 = k * (n - 1)\n        \n        # Noncentrality parameter lambda\n        ncp = n * effect_size_term\n\n        # Central F critical threshold for significance level alpha\n        f_crit = f.isf(alpha, df1, df2)\n\n        # Inverse survival quantile from the noncentral F distribution at the target power\n        f_power_quantile = ncf.isf(power, df1, df2, ncp)\n        \n        # The condition Power >= target_power is equivalent to F_crit <= F_power_quantile\n        return f_crit <= f_power_quantile\n\n    # Initial check at the lower bound n=2\n    if is_power_sufficient(2):\n        return 2\n\n    # --- Bracketing phase to find a search range [n_low, n_high] ---\n    n_low = 2\n    n_high = 4\n    # Set a practical limit to prevent potential infinite loops with extreme parameters\n    MAX_N_BRACKET = 1000000  \n    while not is_power_sufficient(n_high):\n        n_low = n_high\n        n_high *= 2\n        if n_high > MAX_N_BRACKET:\n            # Power goal is practically unattainable\n            return -1\n\n    # --- Binary search phase to find the minimal integer n ---\n    min_n = n_high\n    while n_low <= n_high:\n        n_mid = n_low + (n_high - n_low) // 2\n        if n_mid < 2:  # Ensure sample size is at least 2\n            n_low = n_mid + 1\n            continue\n\n        if is_power_sufficient(n_mid):\n            min_n = n_mid      # n_mid is a potential answer, try for smaller n\n            n_high = n_mid - 1\n        else:\n            n_low = n_mid + 1  # n_mid is too small, need larger n\n            \n    return min_n\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # format: (means, variance, alpha, power)\n    test_cases = [\n        ([0.0, 0.5, 1.0], 1.0, 0.05, 0.8),\n        ([0.0, 0.2], 1.0, 0.05, 0.9),\n        ([1.0, 1.0, 1.0, 1.0], 1.0, 0.05, 0.8),\n        ([-1.0, -0.5, 0.0, 0.5, 1.0], 0.5, 0.01, 0.95),\n    ]\n\n    results = []\n    for case in test_cases:\n        means, variance, alpha, power = case\n        result = compute_min_n_power_anova(means, variance, alpha, power)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4965568"}]}