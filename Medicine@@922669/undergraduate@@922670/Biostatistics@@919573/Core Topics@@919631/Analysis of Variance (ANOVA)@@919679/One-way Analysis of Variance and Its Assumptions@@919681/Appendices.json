{"hands_on_practices": [{"introduction": "Beyond a simple declaration of statistical significance, the true power of ANOVA lies in its ability to answer specific, nuanced questions about group means. This is achieved through linear contrasts, which allow us to test focused hypotheses, such as comparing a new treatment against a placebo or examining dose-response relationships. This first practice will guide you through calculating a confidence interval for a specific contrast, demonstrating how to extract precise, clinically meaningful insights from your experimental data [@problem_id:4919608].", "problem": "A randomized clinical trial evaluates the effect of three doses of an anti-inflammatory drug versus placebo on the reduction in C-reactive protein (CRP) after $8$ weeks. The four arms are placebo, low dose, medium dose, and high dose. The one-way analysis of variance (ANOVA) model assumes independent normally distributed errors with a common variance. The sample sizes are $n_{\\text{placebo}} = 28$, $n_{\\text{low}} = 30$, $n_{\\text{medium}} = 32$, and $n_{\\text{high}} = 26$. The sample means of CRP reduction (in $\\mathrm{mg/L}$) are $\\bar{y}_{\\text{placebo}} = 0.7$, $\\bar{y}_{\\text{low}} = 2.9$, $\\bar{y}_{\\text{medium}} = 3.5$, and $\\bar{y}_{\\text{high}} = 4.8$. The pooled within-group variance estimate from the ANOVA, denoted $\\hat{\\sigma}^2$, is $1.44$.\n\nConsider the contrast that compares the high dose to the average of the low and medium doses, defined at the population level by $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$. Under the one-way ANOVA assumptions and using the pooled variance estimate, construct the two-sided $95\\%$ confidence interval for $\\psi$ based on the residual degrees of freedom. Report the lower bound of this confidence interval in $\\mathrm{mg/L}$, rounded to four significant figures. Briefly interpret what the sign of the entire interval implies for clinical inference about the superiority of the high dose relative to the average of the low and medium doses.", "solution": "The problem requires the construction of a $95\\%$ confidence interval for a specific linear contrast of population means in the context of a one-way analysis of variance (ANOVA).\n\nFirst, we validate the problem statement.\nThe problem provides the following givens:\n-   Number of groups, $k=4$.\n-   Group sample sizes: $n_{\\text{placebo}} = 28$, $n_{\\text{low}} = 30$, $n_{\\text{medium}} = 32$, and $n_{\\text{high}} = 26$.\n-   Group sample means: $\\bar{y}_{\\text{placebo}} = 0.7$, $\\bar{y}_{\\text{low}} = 2.9$, $\\bar{y}_{\\text{medium}} = 3.5$, and $\\bar{y}_{\\text{high}} = 4.8$.\n-   The pooled within-group variance estimate (Mean Squared Error, MSE): $\\hat{\\sigma}^2 = 1.44$.\n-   The population contrast of interest: $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$.\n-   The confidence level is $95\\%$, which corresponds to $\\alpha = 0.05$.\n\nThe problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is a standard biostatistical problem. Therefore, the problem is deemed valid.\n\nThe general form of a two-sided $(1-\\alpha) \\times 100\\%$ confidence interval for a linear contrast $\\psi = \\sum_{i=1}^{k} c_i \\mu_i$ is given by:\n$$ \\hat{\\psi} \\pm t_{\\alpha/2, df} \\cdot SE(\\hat{\\psi}) $$\nwhere $\\hat{\\psi}$ is the sample estimate of the contrast, $t_{\\alpha/2, df}$ is the critical value from the t-distribution with $df$ degrees of freedom, and $SE(\\hat{\\psi})$ is the standard error of the estimate.\n\nWe proceed by calculating each component.\n\n1.  **Estimate of the contrast, $\\hat{\\psi}$**:\n    The coefficients for the contrast $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\mu_{\\text{low}} - \\frac{1}{2}\\mu_{\\text{medium}}$ are $c_{\\text{high}}=1$, $c_{\\text{low}}=-1/2$, $c_{\\text{medium}}=-1/2$, and $c_{\\text{placebo}}=0$.\n    The sample estimate $\\hat{\\psi}$ is calculated by substituting the sample means for the population means:\n    $$ \\hat{\\psi} = \\bar{y}_{\\text{high}} - \\frac{1}{2}\\left(\\bar{y}_{\\text{low}} + \\bar{y}_{\\text{medium}}\\right) $$\n    Substituting the given values:\n    $$ \\hat{\\psi} = 4.8 - \\frac{1}{2}(2.9 + 3.5) = 4.8 - \\frac{1}{2}(6.4) = 4.8 - 3.2 = 1.6 $$\n\n2.  **Degrees of Freedom, $df$**:\n    The degrees of freedom for the pooled variance estimate (the residual degrees of freedom) are given by $df = N - k$, where $N$ is the total sample size and $k$ is the number of groups.\n    The total sample size is $N = n_{\\text{placebo}} + n_{\\text{low}} + n_{\\text{medium}} + n_{\\text{high}} = 28 + 30 + 32 + 26 = 116$.\n    The number of groups is $k=4$.\n    Therefore, the degrees of freedom are:\n    $$ df = 116 - 4 = 112 $$\n\n3.  **Standard Error of the estimate, $SE(\\hat{\\psi})$**:\n    The variance of the estimated contrast, $\\widehat{\\text{Var}}(\\hat{\\psi})$, is calculated using the pooled variance estimate $\\hat{\\sigma}^2$ and the sample sizes of the groups involved in the contrast.\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) = \\hat{\\sigma}^2 \\sum_{i=1}^{k} \\frac{c_i^2}{n_i} $$\n    For our specific contrast:\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) = \\hat{\\sigma}^2 \\left( \\frac{c_{\\text{high}}^2}{n_{\\text{high}}} + \\frac{c_{\\text{low}}^2}{n_{\\text{low}}} + \\frac{c_{\\text{medium}}^2}{n_{\\text{medium}}} \\right) = \\hat{\\sigma}^2 \\left( \\frac{1^2}{n_{\\text{high}}} + \\frac{(-1/2)^2}{n_{\\text{low}}} + \\frac{(-1/2)^2}{n_{\\text{medium}}} \\right) $$\n    Substituting the given values:\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) = 1.44 \\left( \\frac{1}{26} + \\frac{1/4}{30} + \\frac{1/4}{32} \\right) = 1.44 \\left( \\frac{1}{26} + \\frac{1}{120} + \\frac{1}{128} \\right) $$\n    Calculating the terms inside the parentheses:\n    $$ \\frac{1}{26} \\approx 0.03846154, \\quad \\frac{1}{120} \\approx 0.00833333, \\quad \\frac{1}{128} = 0.0078125 $$\n    $$ \\sum \\frac{c_i^2}{n_i} \\approx 0.03846154 + 0.00833333 + 0.0078125 = 0.05460737 $$\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) \\approx 1.44 \\times 0.05460737 \\approx 0.07863461 $$\n    The standard error is the square root of the variance:\n    $$ SE(\\hat{\\psi}) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\psi})} \\approx \\sqrt{0.07863461} \\approx 0.2804186 $$\n\n4.  **Critical value, $t_{\\alpha/2, df}$**:\n    For a $95\\%$ confidence interval, $\\alpha=0.05$, so we need the critical value $t_{0.025, 112}$. Using statistical software or a detailed t-distribution table for $df=112$, the value is approximately:\n    $$ t_{0.025, 112} \\approx 1.98137 $$\n\n5.  **Confidence Interval Construction**:\n    Now we calculate the margin of error (ME):\n    $$ ME = t_{0.025, 112} \\cdot SE(\\hat{\\psi}) \\approx 1.98137 \\times 0.2804186 \\approx 0.555627 $$\n    The $95\\%$ confidence interval is $\\hat{\\psi} \\pm ME$:\n    $$ CI_{95\\%} = 1.6 \\pm 0.555627 $$\n    Lower bound: $L = 1.6 - 0.555627 = 1.044373$\n    Upper bound: $U = 1.6 + 0.555627 = 2.155627$\n    The confidence interval is approximately $(1.044, 2.156)$.\n\n6.  **Final Answer and Rounding**:\n    The problem asks for the lower bound of the confidence interval, rounded to four significant figures.\n    The calculated lower bound is $1.044373$. The first four significant figures are $1$, $0$, $4$, and $4$. The fifth significant digit is $3$, which is less than $5$, so we round down.\n    The lower bound rounded to four significant figures is $1.044$.\n\n7.  **Interpretation of the Confidence Interval**:\n    The $95\\%$ confidence interval for the contrast $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$ is approximately $(1.044, 2.156)$. Since the entire interval is positive (the lower bound $1.044$ is greater than $0$), we can be $95\\%$ confident that the true value of the contrast $\\psi$ is positive. A positive value for $\\psi$ implies that $\\mu_{\\text{high}} > \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$.\n    Clinically, this provides statistically significant evidence that the mean reduction in C-reactive protein for the high-dose group is greater than the average of the mean reductions for the low- and medium-dose groups. This suggests superiority of the high dose compared to the average effect of the lower, active doses.", "answer": "$$ \\boxed{1.044} $$", "id": "4919608"}, {"introduction": "The statistical inferences drawn from ANOVA, including the contrasts we just explored, are only reliable if the underlying assumptions of the model hold true. One of the most critical assumptions is homoscedasticity, or the equality of variances across all groups. This next exercise confronts a common scenario in biostatistics where the variance of a measurement is dependent on its mean, and guides you through the process of diagnosing and correcting this issue using a variance-stabilizing transformation [@problem_id:4935078].", "problem": "A biostatistical investigator is studying three independent treatment groups with continuous outcomes in a one-way factor design. The one-way analysis of variance (ANOVA) model posits that for group index $g \\in \\{1,2,3\\}$ and subject index $i \\in \\{1,\\dots,n_g\\}$, the observation satisfies $Y_{gi} = \\mu_g + \\varepsilon_{gi}$, where the errors $\\varepsilon_{gi}$ are independent and identically distributed within groups with $E(\\varepsilon_{gi}) = 0$ and $Var(\\varepsilon_{gi}) = \\sigma_g^2$. Classical one-way ANOVA assumes homoscedasticity, i.e., $\\sigma_1^2 = \\sigma_2^2 = \\sigma_3^2$. In practice, in many biological measurement systems, the variance depends on the mean, and a variance-stabilizing transformation is applied to seek approximate homoscedasticity.\n\nAssume a mean–variance relationship of the form $Var(Y \\mid \\text{group } g) \\approx c \\,\\mu_g^{p}$ for some constant $c > 0$ and exponent $p \\in \\mathbb{R}$. Consider the Box–Cox transformation family defined by\n$$\ng_{\\lambda}(y) = \n\\begin{cases}\n\\dfrac{y^{\\lambda} - 1}{\\lambda}, & \\lambda \\neq 0, \\\\\n\\log(y), & \\lambda = 0,\n\\end{cases}\n$$\nfor $y > 0$. Using a first-order Taylor (delta method) approximation $Var(g_{\\lambda}(Y)) \\approx \\left(g_{\\lambda}'(\\mu)\\right)^2 Var(Y)$ and the goal that $Var(g_{\\lambda}(Y))$ be approximately constant across groups, derive the recommended parameter $\\lambda$ as a function of $p$. Then, given observed group sample means $\\bar{y}_g$ and sample variances $s_g^2$, estimate $p$ by regressing $\\log(s_g^2)$ on $\\log(\\bar{y}_g)$ using ordinary least squares and take the slope as $\\hat{p}$.\n\nNext, use the delta method to approximate, for each group $g$, the transformed mean $E[g_{\\lambda}(Y)] \\approx g_{\\lambda}(\\bar{y}_g) + \\dfrac{1}{2} g_{\\lambda}''(\\bar{y}_g) s_g^2$ and transformed variance $Var(g_{\\lambda}(Y)) \\approx \\left(g_{\\lambda}'(\\bar{y}_g)\\right)^2 s_g^2$. Using these approximations, compute the one-way ANOVA $F$-statistic and associated upper-tail probability (p-value) both before and after transformation, using the fundamental definitions of sums of squares:\n$$\nSS_{\\text{between}} = \\sum_{g=1}^{3} n_g \\left(\\bar{y}_g - \\bar{y}_{\\cdot}\\right)^2, \\quad \\bar{y}_{\\cdot} = \\dfrac{\\sum_{g=1}^{3} n_g \\bar{y}_g}{\\sum_{g=1}^{3} n_g},\n$$\n$$\nSS_{\\text{within}} = \\sum_{g=1}^{3} (n_g - 1) s_g^2,\n$$\n$$\nF = \\dfrac{MS_{\\text{between}}}{MS_{\\text{within}}}, \\quad MS_{\\text{between}} = \\dfrac{SS_{\\text{between}}}{3 - 1}, \\quad MS_{\\text{within}} = \\dfrac{SS_{\\text{within}}}{\\left(\\sum_{g=1}^{3} n_g\\right) - 3}.\n$$\nFor the transformed analysis, replace $\\bar{y}_g$ by the approximated transformed mean and $s_g^2$ by the approximated transformed variance in the above definitions.\n\nFinally, quantify heteroscedasticity before and after transformation using the ratio of largest to smallest group variance, i.e., $\\rho_{\\text{before}} = \\dfrac{\\max_g s_g^2}{\\min_g s_g^2}$ and $\\rho_{\\text{after}} = \\dfrac{\\max_g Var(g_{\\lambda}(Y))}{\\min_g Var(g_{\\lambda}(Y))}$, and report whether the ratio decreases after transformation.\n\nYour program must implement the above procedure and run on the following three test cases, each consisting of group sizes, sample means, and sample variances:\n- Test case $1$: $n = [12, 15, 18]$, $\\bar{y} = [10.0, 20.0, 40.0]$, $s^2 = [4.0, 16.0, 64.0]$.\n- Test case $2$: $n = [10, 12, 14]$, $\\bar{y} = [5.0, 10.0, 20.0]$, $s^2 = [5.0, 10.0, 20.0]$.\n- Test case $3$: $n = [20, 22, 24]$, $\\bar{y} = [10.0, 12.0, 11.0]$, $s^2 = [9.0, 9.5, 10.0]$.\n\nFor each test case, the program should:\n- Estimate $\\hat{p}$ from the regression of $\\log(s_g^2)$ on $\\log(\\bar{y}_g)$ and compute the recommended $\\hat{\\lambda}$.\n- Compute $\\rho_{\\text{before}}$ and $\\rho_{\\text{after}}$ and the boolean indicator $\\text{improved} = \\left(\\rho_{\\text{after}} < \\rho_{\\text{before}}\\right)$.\n- Compute $F_{\\text{before}}$ and $F_{\\text{after}}$ and their p-values (upper-tail probabilities) under the $F$ distribution with corresponding degrees of freedom.\n\nAll numerical results must be expressed as floating-point numbers (rounded to $6$ decimal places) or booleans. Your program should produce a single line of output containing a list of results, one per test case, where each result is the list\n$$\n[\\hat{\\lambda}, \\rho_{\\text{before}}, \\rho_{\\text{after}}, \\text{improved}, F_{\\text{before}}, p_{\\text{before}}, F_{\\text{after}}, p_{\\text{after}}].\n$$\nFor example, the overall output format must be\n$$\n[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]],\n$$\nwith no extra text.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in applied biostatistics, specifically concerning the one-way analysis of variance (ANOVA) and the management of heteroscedasticity. The provided data and procedural steps are complete and logically consistent, allowing for a unique and meaningful solution. All concepts, including the Box-Cox transformation, the delta method for approximating moments, and the F-test, are standard in the field. Therefore, the problem is deemed valid. We now proceed with the derivation and computational strategy.\n\nThe core objective is to stabilize the variance across multiple groups before performing a one-way ANOVA. The procedure involves several distinct theoretical and computational steps.\n\n**1. Derivation of the Variance-Stabilizing Transformation Parameter $\\lambda$**\n\nWe begin with the assumed mean-variance relationship within each group $g$:\n$$\nVar(Y_g) \\approx c \\mu_g^p\n$$\nwhere $\\mu_g = E[Y_g]$ is the true mean of group $g$, $c > 0$ is a constant, and $p$ is a real-valued exponent. The goal is to find a transformation $g_{\\lambda}(y)$ from the Box-Cox family such that the variance of the transformed variable, $Var(g_{\\lambda}(Y_g))$, is approximately constant across groups, independent of the mean $\\mu_g$.\n\nThe Box-Cox transformation is defined as:\n$$\ng_{\\lambda}(y) = \n\\begin{cases}\n\\dfrac{y^{\\lambda} - 1}{\\lambda}, & \\lambda \\neq 0, \\\\\n\\log(y), & \\lambda = 0.\n\\end{cases}\n$$\nUsing a first-order Taylor expansion (the delta method), the variance of the transformed variable can be approximated as:\n$$\nVar(g_{\\lambda}(Y_g)) \\approx \\left( g_{\\lambda}'(\\mu_g) \\right)^2 Var(Y_g)\n$$\nSubstituting the assumed mean-variance relationship gives:\n$$\nVar(g_{\\lambda}(Y_g)) \\approx \\left( g_{\\lambda}'(\\mu_g) \\right)^2 c \\mu_g^p\n$$\nFor this variance to be constant, say $K$, for all groups, the term dependent on $\\mu_g$ must be constant.\n$$\n\\left( g_{\\lambda}'(\\mu_g) \\right)^2 \\mu_g^p = \\text{constant}\n$$\nThis implies $g_{\\lambda}'(\\mu_g) \\propto \\mu_g^{-p/2}$.\n\nWe now compute the derivative $g_{\\lambda}'(y)$:\n- If $\\lambda \\neq 0$, $g_{\\lambda}'(y) = \\dfrac{d}{dy}\\left(\\dfrac{y^{\\lambda} - 1}{\\lambda}\\right) = y^{\\lambda - 1}$.\n- If $\\lambda = 0$, $g_{\\lambda}'(y) = \\dfrac{d}{dy}(\\log(y)) = y^{-1}$.\n\nEquating the exponents:\n- For $\\lambda \\neq 0$, we require $y^{\\lambda - 1} \\propto y^{-p/2}$. This implies $\\lambda - 1 = -p/2$, which yields $\\lambda = 1 - p/2$.\n- For $\\lambda = 0$, we have $y^{-1} \\propto y^{-p/2}$. This implies $-1 = -p/2$, which yields $p=2$. Note that if $p=2$, our general formula gives $\\lambda = 1 - 2/2 = 0$, which is consistent.\n\nTherefore, the recommended Box-Cox parameter $\\lambda$ is a simple function of the mean-variance exponent $p$:\n$$\n\\lambda = 1 - \\frac{p}{2}\n$$\n\n**2. Estimation of the Exponent $p$**\n\nIn practice, $p$ is unknown and must be estimated from the observed data. The relationship $s_g^2 \\approx c \\bar{y}_g^p$ can be linearized by taking the logarithm:\n$$\n\\log(s_g^2) \\approx \\log(c) + p \\log(\\bar{y}_g)\n$$\nThis suggests that $p$ can be estimated as the slope of a simple linear regression of $\\log(s_g^2)$ on $\\log(\\bar{y}_g)$. For a set of $G=3$ groups, the ordinary least squares (OLS) estimator for the slope $\\hat{p}$ is:\n$$\n\\hat{p} = \\frac{\\sum_{g=1}^{3} (\\log(\\bar{y}_g) - \\overline{\\log y})(\\log(s_g^2) - \\overline{\\log s^2})}{\\sum_{g=1}^{3} (\\log(\\bar{y}_g) - \\overline{\\log y})^2}\n$$\nwhere $\\overline{\\log y}$ and $\\overline{\\log s^2}$ are the means of the log-transformed sample means and variances, respectively. Once $\\hat{p}$ is obtained, the estimated transformation parameter is $\\hat{\\lambda} = 1 - \\hat{p}/2$.\n\n**3. Second-Order Approximations for Transformed Moments**\n\nFor the subsequent ANOVA, we require approximations for the means and variances of the transformed data. A more accurate approximation for the mean, using a second-order Taylor expansion, is given by:\n$$\n\\mu_{g, \\text{trans}} = E[g_{\\hat{\\lambda}}(Y_g)] \\approx g_{\\hat{\\lambda}}(\\bar{y}_g) + \\frac{1}{2} g_{\\hat{\\lambda}}''(\\bar{y}_g) s_g^2\n$$\nThe first-order approximation for the variance is used as specified:\n$$\n\\sigma_{g, \\text{trans}}^2 = Var(g_{\\hat{\\lambda}}(Y_g)) \\approx \\left( g_{\\hat{\\lambda}}'(\\bar{y}_g) \\right)^2 s_g^2\n$$\nThe necessary second derivatives are:\n- If $\\hat{\\lambda} \\neq 0$, $g_{\\hat{\\lambda}}''(y) = (\\hat{\\lambda}-1)y^{\\hat{\\lambda}-2}$.\n- If $\\hat{\\lambda} = 0$, $g_{\\hat{\\lambda}}''(y) = -y^{-2}$.\n\n**4. ANOVA F-Test Calculation**\n\nThe one-way ANOVA F-statistic is the ratio of the mean square between groups to the mean square within groups. Let $k=3$ be the number of groups and $N = \\sum_{g=1}^k n_g$ be the total sample size.\n\n**Before Transformation:**\nThe statistics are based on the original sample moments $\\bar{y}_g$ and $s_g^2$.\n- Overall mean: $\\bar{y}_{\\cdot} = \\frac{1}{N} \\sum_{g=1}^{k} n_g \\bar{y}_g$\n- Sum of Squares Between: $SS_{\\text{between}} = \\sum_{g=1}^{k} n_g (\\bar{y}_g - \\bar{y}_{\\cdot})^2$\n- Sum of Squares Within: $SS_{\\text{within}} = \\sum_{g=1}^{k} (n_g - 1) s_g^2$\n- Mean Square Between: $MS_{\\text{between}} = SS_{\\text{between}} / (k - 1)$\n- Mean Square Within: $MS_{\\text{within}} = SS_{\\text{within}} / (N - k)$\n- F-statistic: $F_{\\text{before}} = MS_{\\text{between}} / MS_{\\text{within}}$\n- The p-value, $p_{\\text{before}}$, is the upper-tail probability $P(F_{k-1, N-k} \\ge F_{\\text{before}})$.\n\n**After Transformation:**\nThe same formulas are applied, but with the approximated transformed moments $\\mu_{g, \\text{trans}}$ and $\\sigma_{g, \\text{trans}}^2$.\n- Overall transformed mean: $\\bar{\\mu}_{\\cdot, \\text{trans}} = \\frac{1}{N} \\sum_{g=1}^{k} n_g \\mu_{g, \\text{trans}}$\n- $SS_{\\text{between, after}} = \\sum_{g=1}^{k} n_g (\\mu_{g, \\text{trans}} - \\bar{\\mu}_{\\cdot, \\text{trans}})^2$\n- $SS_{\\text{within, after}} = \\sum_{g=1}^{k} (n_g - 1) \\sigma_{g, \\text{trans}}^2$\n- An F-statistic $F_{\\text{after}}$ and p-value $p_{\\text{after}}$ are computed analogously.\n\n**5. Quantifying Heteroscedasticity**\n\nThe degree of heteroscedasticity is quantified by the ratio of the largest to the smallest group variance.\n- Before transformation: $\\rho_{\\text{before}} = \\max_g(s_g^2) / \\min_g(s_g^2)$\n- After transformation: $\\rho_{\\text{after}} = \\max_g(\\sigma_{g, \\text{trans}}^2) / \\min_g(\\sigma_{g, \\text{trans}}^2)$\nThe success of the transformation in reducing variance heterogeneity is assessed by the boolean indicator $\\text{improved} = (\\rho_{\\text{after}}  \\rho_{\\text{before}})$.\n\nThe program will implement this entire sequence of calculations for each provided test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Implements the full biostatistical analysis procedure for one-way ANOVA\n    with variance stabilization as described in the problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': [12, 15, 18], 'y_bar': [10.0, 20.0, 40.0], 's_sq': [4.0, 16.0, 64.0]},\n        {'n': [10, 12, 14], 'y_bar': [5.0, 10.0, 20.0], 's_sq': [5.0, 10.0, 20.0]},\n        {'n': [20, 22, 24], 'y_bar': [10.0, 12.0, 11.0], 's_sq': [9.0, 9.5, 10.0]},\n    ]\n    \n    # Use a small tolerance to check if lambda is close to zero\n    LAMBDA_ZERO_TOL = 1e-9\n\n    def g_lambda(y, lam):\n        if abs(lam)  LAMBDA_ZERO_TOL:\n            return np.log(y)\n        else:\n            return (y**lam - 1) / lam\n\n    def g_lambda_prime(y, lam):\n        if abs(lam)  LAMBDA_ZERO_TOL:\n            return 1 / y\n        else:\n            return y**(lam - 1)\n\n    def g_lambda_double_prime(y, lam):\n        if abs(lam)  LAMBDA_ZERO_TOL:\n            return -1 / y**2\n        else:\n            return (lam - 1) * y**(lam - 2)\n\n    def perform_anova(n, means, variances):\n        \"\"\"Helper function to compute ANOVA F-statistic and p-value.\"\"\"\n        k = len(n)\n        N = np.sum(n)\n        \n        # Grand mean\n        grand_mean = np.sum(n * means) / N\n        \n        # Sum of squares\n        ss_between = np.sum(n * (means - grand_mean)**2)\n        ss_within = np.sum((n - 1) * variances)\n        \n        # Degrees of freedom\n        df_between = k - 1\n        df_within = N - k\n        \n        # Mean squares\n        ms_between = ss_between / df_between\n        ms_within = ss_within / df_within\n        \n        # F-statistic and p-value\n        if ms_within == 0: # Avoid division by zero, though unlikely\n            f_stat = np.inf\n            p_val = 0.0\n        else:\n            f_stat = ms_between / ms_within\n            p_val = stats.f.sf(f_stat, df_between, df_within)\n            \n        return f_stat, p_val, df_between, df_within\n\n    all_results = []\n    \n    for case in test_cases:\n        n = np.array(case['n'])\n        y_bar = np.array(case['y_bar'])\n        s_sq = np.array(case['s_sq'])\n        \n        # Step 1: Estimate p and lambda\n        log_y_bar = np.log(y_bar)\n        log_s_sq = np.log(s_sq)\n        \n        p_hat = np.polyfit(log_y_bar, log_s_sq, 1)[0]\n        lambda_hat = 1 - p_hat / 2\n        \n        # Step 2: ANOVA before transformation\n        f_before, p_before, df_b, df_w = perform_anova(n, y_bar, s_sq)\n        \n        # Step 3: Compute transformed moments\n        mu_trans = g_lambda(y_bar, lambda_hat) + 0.5 * g_lambda_double_prime(y_bar, lambda_hat) * s_sq\n        sigma_sq_trans = (g_lambda_prime(y_bar, lambda_hat)**2) * s_sq\n        \n        # Step 4: ANOVA after transformation\n        f_after, p_after, _, _ = perform_anova(n, mu_trans, sigma_sq_trans)\n        \n        # Step 5: Quantify heteroscedasticity\n        rho_before = np.max(s_sq) / np.min(s_sq)\n        rho_after = np.max(sigma_sq_trans) / np.min(sigma_sq_trans)\n        improved = rho_after  rho_before\n        \n        # Step 6: Assemble results\n        case_results = [\n            lambda_hat,\n            rho_before,\n            rho_after,\n            improved,\n            f_before,\n            p_before,\n            f_after,\n            p_after\n        ]\n        all_results.append(case_results)\n        \n    # Final formatting of the output string to match the required format precisely\n    # without extra spaces and with specified rounding.\n    outer_list_str = []\n    for row in all_results:\n        formatted_items = []\n        for item in row:\n            if isinstance(item, bool):\n                # Python's bool string representation is 'True'/'False'\n                # Using .lower() to get 'true'/'false' as often seen in other formats\n                formatted_items.append(str(item).lower())\n            elif isinstance(item, (float, np.floating)):\n                # Round to 6 decimal places\n                formatted_items.append(f'{item:.6f}')\n            else:\n                formatted_items.append(str(item))\n        inner_list_str = f\"[{','.join(formatted_items)}]\"\n        outer_list_str.append(inner_list_str)\n        \n    final_output = f\"[{','.join(outer_list_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4935078"}, {"introduction": "After an ANOVA test indicates a significant difference among three or more groups, the immediate follow-up question is: which specific groups are different from one another? Answering this requires post-hoc tests, which are designed to conduct multiple pairwise comparisons without inflating the overall Type I error rate. This final practice demonstrates the application of the Tukey Honestly Significant Difference (HSD) test, a widely used method for controlling the familywise error rate while exploring pairwise differences after a significant overall F-test [@problem_id:4827786].", "problem": "A multicenter randomized trial evaluates the mean reduction in systolic blood pressure across $k=4$ treatment arms, each with $n=25$ participants, under a one-way analysis of variance (ANOVA) model. Assume the standard one-way ANOVA assumptions: independent and identically distributed errors $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, equal variance across groups, and independence between groups. The pooled error mean square is observed as $\\mathrm{MSE}=36$ and the error degrees of freedom are $\\nu=96$. Investigators wish to perform all pairwise comparisons of the $k$ group means while controlling the familywise error rate at $\\alpha=0.05$ using the Tukey Honestly Significant Difference (HSD) procedure.\n\nLet $q_{0.95;4,96}=3.63$ denote the $0.95$ quantile of the Studentized range distribution with $k=4$ groups and $\\nu=96$ degrees of freedom. Starting from the fundamental ANOVA model and the definition of the Studentized range, derive the Tukey HSD decision threshold for pairwise mean differences at familywise error rate $\\alpha=0.05$ in this balanced design, then compute its numerical value for the given $\\mathrm{MSE}$ and $n$. Using this threshold, determine whether an observed absolute mean difference of $9$ (in the same units as the outcome) is statistically significant under Tukey HSD.\n\nDefine the decision indicator $D$ by $D=1$ if the absolute mean difference of $9$ is significant at familywise error rate $\\alpha=0.05$ under Tukey HSD, and $D=0$ otherwise. Provide the derivation and computation, and for grading, report only $D$ in your final answer. No rounding instruction applies to $D$; if you compute intermediate numerical quantities, you may carry exact arithmetic through to the conclusion. Do not include units in the final reported value.", "solution": "The problem requires the derivation and application of the Tukey Honestly Significant Difference (HSD) procedure to determine if an observed pairwise mean difference is statistically significant. The final answer is an indicator variable $D$.\n\nThe analysis starts from the one-way analysis of variance (ANOVA) model, given by:\n$$ y_{ij} = \\mu_i + \\epsilon_{ij} $$\nwhere $y_{ij}$ is the observation for the $j$-th participant in the $i$-th treatment group, for $i \\in \\{1, 2, 3, 4\\}$ and $j \\in \\{1, 2, \\dots, 25\\}$. The term $\\mu_i$ represents the true mean for treatment group $i$. The errors $\\epsilon_{ij}$ are assumed to be independent and identically distributed random variables from a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThe sample mean for group $i$ is denoted by $\\bar{y}_i$. Under the ANOVA assumptions, each sample mean is normally distributed, $\\bar{y}_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2/n)$. The goal of the Tukey HSD procedure is to test all pairwise null hypotheses of the form $H_{0,ij}: \\mu_i = \\mu_j$ while controlling the familywise error rate (FWER) at a specified level $\\alpha$. The FWER is the probability of making one or more Type I errors across all comparisons.\n\nThe Tukey HSD procedure is based on the Studentized range distribution. The statistic for comparing any pair of means, $\\bar{y}_i$ and $\\bar{y}_j$, is given by:\n$$ q_{ij} = \\frac{|\\bar{y}_i - \\bar{y}_j|}{\\mathrm{SE}} $$\nwhere $\\mathrm{SE}$ is the standard error of a group mean. In the context of ANOVA, the pooled variance from all groups, the Mean Squared Error ($\\mathrm{MSE}$), provides the best estimate of the common population variance $\\sigma^2$. The $\\mathrm{MSE}$ is defined as $\\mathrm{MSE} = \\hat{\\sigma}^2$ and has $\\nu$ degrees of freedom. For a balanced design with $k$ groups and $n$ subjects per group, the error degrees of freedom are $\\nu = k(n-1)$. Here, $\\nu = 4(25-1) = 96$, which matches the provided information.\n\nThe standard error for a single group mean $\\bar{y}_i$ is estimated as $\\sqrt{\\mathrm{MSE}/n}$. Thus, the test statistic becomes:\n$$ q_{ij} = \\frac{|\\bar{y}_i - \\bar{y}_j|}{\\sqrt{\\mathrm{MSE}/n}} $$\nThe Tukey HSD procedure states that the null hypothesis $H_{0,ij}$ should be rejected if the observed mean difference is large enough. To control the FWER at $\\alpha$, we compare the observed statistic to the critical value from the Studentized range distribution, $q_{\\alpha;k,\\nu}$. This critical value is the $(1-\\alpha)$-th quantile of the distribution with parameters $k$ (the number of groups) and $\\nu$ (the error degrees of freedom).\n\nThe decision rule is to reject $H_{0,ij}$ if:\n$$ \\frac{|\\bar{y}_i - \\bar{y}_j|}{\\sqrt{\\mathrm{MSE}/n}} > q_{\\alpha;k,\\nu} $$\nWe can rearrange this inequality to find the minimum absolute difference between two means that would be considered statistically significant. This minimum difference is the \"Honestly Significant Difference\" (HSD), which we denote as $W_{Tukey}$:\n$$ W_{Tukey} = q_{\\alpha;k,\\nu} \\sqrt{\\frac{\\mathrm{MSE}}{n}} $$\nAny observed absolute mean difference $|\\bar{y}_i - \\bar{y}_j|$ that is greater than $W_{Tukey}$ is declared statistically significant.\n\nNow, we substitute the values provided in the problem statement:\n- Number of groups, $k = 4$.\n- Sample size per group, $n = 25$.\n- Pooled error mean square, $\\mathrm{MSE} = 36$.\n- Error degrees of freedom, $\\nu = 96$.\n- Familywise error rate, $\\alpha = 0.05$.\n- The critical value from the Studentized range distribution is given as the $0.95$ quantile, $q_{0.95;4,96} = 3.63$. This corresponds to our $q_{\\alpha;k,\\nu}$ for $\\alpha=0.05$, so $q_{0.05;4,96} = 3.63$.\n\nWe compute the value of the Tukey HSD threshold, $W_{Tukey}$:\n$$ W_{Tukey} = 3.63 \\times \\sqrt{\\frac{36}{25}} $$\n$$ W_{Tukey} = 3.63 \\times \\frac{\\sqrt{36}}{\\sqrt{25}} $$\n$$ W_{Tukey} = 3.63 \\times \\frac{6}{5} $$\n$$ W_{Tukey} = 3.63 \\times 1.2 $$\n$$ W_{Tukey} = 4.356 $$\n\nThe decision threshold is $4.356$. The problem states that an observed absolute mean difference is $9$. To determine if this difference is statistically significant, we compare it to the computed threshold:\n$$ |\\bar{y}_i - \\bar{y}_j|_{obs} = 9 $$\nWe must check if $9 > 4.356$. This inequality is true.\n\nSince the observed absolute mean difference of $9$ is greater than the Tukey HSD threshold of $4.356$, we reject the corresponding null hypothesis and conclude that this difference is statistically significant at a familywise error rate of $\\alpha=0.05$.\n\nThe problem defines an indicator variable $D$ such that $D=1$ if the difference is significant and $D=0$ otherwise. Based on our conclusion, the value of the indicator is $D=1$.", "answer": "$$\\boxed{1}$$", "id": "4827786"}]}