{"hands_on_practices": [{"introduction": "The foundation of Analysis of Variance (ANOVA) is the elegant partitioning of total data variability into distinct, meaningful sources. This first exercise provides essential practice in this core skill by guiding you to construct a one-way ANOVA table from the ground up using only summary statistics. By calculating the sums of squares, degrees of freedom, and mean squares, you will see how the F-statistic emerges as a ratio comparing the variability *between* groups to the variability *within* groups [@problem_id:4957544].", "problem": "A biostatistician is studying enzyme activity measured in arbitrary activity units across four independent genotypes using a one-way Analysis of Variance (ANOVA). For each genotype, the sample size, sample mean, and unbiased sample variance are reported as follows: genotype $1$: $n_{1}=15$, $\\bar{x}_{1}=12.0$, $s_{1}^{2}=1.2$; genotype $2$: $n_{2}=15$, $\\bar{x}_{2}=11.5$, $s_{2}^{2}=0.9$; genotype $3$: $n_{3}=15$, $\\bar{x}_{3}=10.0$, $s_{3}^{2}=1.5$; genotype $4$: $n_{4}=15$, $\\bar{x}_{4}=13.0$, $s_{4}^{2}=2.1$. Assume the standard one-way layout with independent, normally distributed errors having a common variance across genotypes. Starting from the core definitions of variability attributable to group differences and to within-group scatter, construct the ANOVA table by partitioning the total variability into “between groups” and “within groups” components and matching each component with its appropriate degrees of freedom. Then, compute the ANOVA $F$ statistic for testing equality of the genotype means.\n\nRound your final numeric answer to four significant figures.", "solution": "The user wants me to solve the following problem:\nA biostatistician is studying enzyme activity measured in arbitrary activity units across four independent genotypes using a one-way Analysis of Variance (ANOVA). For each genotype, the sample size, sample mean, and unbiased sample variance are reported as follows: genotype $1$: $n_{1}=15$, $\\bar{x}_{1}=12.0$, $s_{1}^{2}=1.2$; genotype $2$: $n_{2}=15$, $\\bar{x}_{2}=11.5$, $s_{2}^{2}=0.9$; genotype $3$: $n_{3}=15$, $\\bar{x}_{3}=10.0$, $s_{3}^{2}=1.5$; genotype $4$: $n_{4}=15$, $\\bar{x}_{4}=13.0$, $s_{4}^{2}=2.1$. Assume the standard one-way layout with independent, normally distributed errors having a common variance across genotypes. Starting from the core definitions of variability attributable to group differences and to within-group scatter, construct the ANOVA table by partitioning the total variability into “between groups” and “within groups” components and matching each component with its appropriate degrees of freedom. Then, compute the ANOVA $F$ statistic for testing equality of the genotype means.\n\nRound your final numeric answer to four significant figures.\n\n### Step 1: Extract Givens\n- Number of groups (genotypes): $k=4$.\n- Group 1: $n_1=15$, $\\bar{x}_1=12.0$, $s_1^2=1.2$.\n- Group 2: $n_2=15$, $\\bar{x}_2=11.5$, $s_2^2=0.9$.\n- Group 3: $n_3=15$, $\\bar{x}_3=10.0$, $s_3^2=1.5$.\n- Group 4: $n_4=15$, $\\bar{x}_4=13.0$, $s_4^2=2.1$.\n- Assumptions: One-way ANOVA model with independent, normally distributed errors and a common variance $\\sigma^2$ across groups.\n\n### Step 2: Validate Using Extracted Givens\nThe problem presented is a standard application of one-way Analysis of Variance (ANOVA), a fundamental statistical technique.\n- **Scientifically Grounded:** The problem is based on established statistical theory. ANOVA is a core method used in biostatistics and many other scientific fields to compare means across multiple groups. The assumptions listed are the standard requirements for a valid one-way ANOVA. The problem is scientifically and factually sound.\n- **Well-Posed:** All necessary data (sample sizes, means, and variances) required to calculate the sums of squares, degrees of freedom, mean squares, and the final F statistic are provided. The problem is self-contained, consistent, and has a unique, meaningful solution.\n- **Objective:** The problem is stated using precise, unambiguous statistical terminology. There are no subjective or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\nThe core principle of Analysis of Variance (ANOVA) is to partition the total variability in a dataset into components attributable to different sources of variation. In a one-way ANOVA, the total sum of squares ($SST$) is partitioned into the sum of squares between groups ($SSB$) and the sum of squares within groups ($SSW$).\n\n$$SST = SSB + SSW$$\n\nThe $SSB$ component measures the variability of the group means around the overall grand mean, representing the effect of the treatment or factor (in this case, genotype). The $SSW$ component measures the pooled variability of the individual observations within each group, representing the random error or unexplained scatter.\n\nFirst, we determine the constants for the analysis.\n- The number of groups is $k=4$.\n- The sample sizes are equal for all groups: $n_1=n_2=n_3=n_4=15$.\n- The total number of observations is $N = \\sum_{i=1}^{k} n_i = 15+15+15+15 = 60$.\n\nNext, we calculate the grand mean, $\\bar{x}_{grand}$, which is the mean of all observations. For a balanced design where all $n_i$ are equal, this simplifies to the average of the group means.\n$$ \\bar{x}_{grand} = \\frac{1}{N} \\sum_{i=1}^{k} n_i \\bar{x}_i = \\frac{1}{4} \\sum_{i=1}^{4} \\bar{x}_i $$\n$$ \\bar{x}_{grand} = \\frac{12.0 + 11.5 + 10.0 + 13.0}{4} = \\frac{46.5}{4} = 11.625 $$\n\nNow, we compute the sums of squares.\n\n1.  **Sum of Squares Within Groups ($SSW$)**: This represents the pooled within-group scatter. It is the sum of the sums of squared deviations of observations from their respective group means. The unbiased sample variance for group $i$ is given by $s_i^2 = \\frac{1}{n_i-1} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i)^2$. The sum of squares for group $i$ is therefore $(n_i-1)s_i^2$. The total $SSW$ is the sum of these values across all groups.\n    $$ SSW = \\sum_{i=1}^{k} (n_i-1) s_i^2 $$\n    $$ SSW = (15-1)(1.2) + (15-1)(0.9) + (15-1)(1.5) + (15-1)(2.1) $$\n    $$ SSW = 14(1.2 + 0.9 + 1.5 + 2.1) = 14(5.7) = 79.8 $$\n\n2.  **Sum of Squares Between Groups ($SSB$)**: This represents the variability attributable to differences between the group means. It is the sum of the squared differences between each group mean and the grand mean, weighted by the sample size of each group.\n    $$ SSB = \\sum_{i=1}^{k} n_i (\\bar{x}_i - \\bar{x}_{grand})^2 $$\n    $$ SSB = 15(12.0 - 11.625)^2 + 15(11.5 - 11.625)^2 + 15(10.0 - 11.625)^2 + 15(13.0 - 11.625)^2 $$\n    $$ SSB = 15 [ (0.375)^2 + (-0.125)^2 + (-1.625)^2 + (1.375)^2 ] $$\n    $$ SSB = 15 [ 0.140625 + 0.015625 + 2.640625 + 1.890625 ] $$\n    $$ SSB = 15 [ 4.6875 ] = 70.3125 $$\n\nNext, we determine the degrees of freedom ($df$) associated with each sum of squares.\n-   Degrees of Freedom Between Groups: $df_B = k - 1 = 4 - 1 = 3$.\n-   Degrees of Freedom Within Groups: $df_W = N - k = 60 - 4 = 56$.\n-   Total Degrees of Freedom: $df_T = N - 1 = 60 - 1 = 59$. Note that $df_T = df_B + df_W$.\n\nNow we compute the Mean Squares ($MS$), which are the sums of squares divided by their respective degrees of freedom.\n-   Mean Square Between Groups: $MSB = \\frac{SSB}{df_B} = \\frac{70.3125}{3} = 23.4375$.\n-   Mean Square Within Groups: $MSW = \\frac{SSW}{df_W} = \\frac{79.8}{56} = 1.425$. The $MSW$ is the pooled estimate of the common variance $\\sigma^2$.\n\nFinally, we compute the ANOVA $F$ statistic, which is the ratio of the between-group variance estimate to the within-group variance estimate.\n$$ F = \\frac{MSB}{MSW} = \\frac{23.4375}{1.425} \\approx 16.44736842... $$\n\nThe complete ANOVA table is constructed as follows:\n| Source of Variation | Sum of Squares ($SS$) | Degrees of Freedom ($df$) | Mean Square ($MS$) | $F$ Statistic |\n|---------------------|-----------------------|---------------------------|--------------------|---------------|\n| Between Groups      | $70.3125$             | $3$                       | $23.4375$          | $16.45$       |\n| Within Groups       | $79.8$                | $56$                      | $1.425$            |               |\n| Total               | $150.1125$            | $59$                      |                    |               |\n\nThe problem requires the final answer to be rounded to four significant figures.\n$$ F \\approx 16.447368... \\rightarrow 16.45 $$", "answer": "$$\n\\boxed{16.45}\n$$", "id": "4957544"}, {"introduction": "A calculated F-statistic is only as reliable as the assumptions that underpin the test, namely independence, normality, and homogeneity of variances. This next practice moves from rote calculation to critical evaluation, presenting a realistic scenario where you must interpret diagnostic evidence to assess the validity of the standard ANOVA assumptions. This exercise will challenge you to think like a practicing biostatistician and choose the most appropriate inferential strategy when the data do not perfectly match the idealized model [@problem_id:4957543].", "problem": "A study in biostatistics investigates whether mean levels of a circulating biomarker differ among $4$ treatment groups in a randomized parallel trial. Let the one-way model be conceptualized as group-specific means plus random errors. The data consist of independent observations within and across groups, with group sizes $n_1 = 10$, $n_2 = 10$, $n_3 = 30$, and $n_4 = 30$. Preliminary diagnostics on residuals from the fitted group-means model reveal the following:\n\n- A quantile–quantile plot against a Gaussian reference shows mild heavy tails, and the Shapiro–Wilk test yields $p = 0.06$.\n- A residual-versus-fitted plot suggests increasing spread of residuals with larger fitted values, and sample variances by group are $s_1^2 \\approx 10$, $s_2^2 \\approx 9$, $s_3^2 \\approx 40$, and $s_4^2 \\approx 36$.\n- Levene’s test for equality of variances yields $p = 0.01$.\n- One observation has a studentized residual of approximately $3.2$.\n\nAssuming independence is credible from the trial conduct, the investigator wants to compare mean biomarker levels across groups using an $F$-based approach or a close alternative and asks which strategy most appropriately respects the assumptions and achieves valid inference under the observed diagnostics.\n\nWhich option is most appropriate?\n\nA. Proceed with the classical one-way Analysis of Variance (ANOVA) $F$ test without modification; its nominal type I error rate is maintained even under unequal variances and unequal sample sizes, regardless of how the variances and sample sizes are paired.\n\nB. Use Welch’s heteroscedastic one-way ANOVA, which adjusts for unequal variances and sample sizes via weighting and a degrees-of-freedom correction; the mild non-normality suggested by $p = 0.06$ is unlikely to compromise its approximate validity under independence.\n\nC. Use the Kruskal–Wallis test because it is unaffected by heteroscedasticity and outliers and directly tests equality of means across groups.\n\nD. Use a permutation test that permutes group labels to calibrate the classical ANOVA $F$ statistic; under independence it is exact even when groups have different variances.\n\nE. Remove the single outlier (studentized residual $3.2$) and then apply the classical ANOVA $F$ test; with the outlier gone, the heteroscedasticity indicated by Levene’s test is inconsequential.", "solution": "The user wants a critical evaluation of a biostatistics problem concerning the choice of an appropriate statistical test, followed by a detailed solution.\n\n### Step 1: Extract Givens\n- **Study Objective**: To compare mean biomarker levels across $k=4$ treatment groups.\n- **Study Design**: Randomized parallel trial.\n- **Model**: One-way analysis of variance (ANOVA) framework.\n- **Data Properties**:\n    - Number of groups: $k=4$.\n    - Sample sizes: $n_1 = 10$, $n_2 = 10$, $n_3 = 30$, $n_4 = 30$.\n    - Total sample size: $N = n_1 + n_2 + n_3 + n_4 = 80$.\n    - Independence: Assumed to be credible.\n- **Diagnostic Findings on Residuals**:\n    1.  **Normality**:\n        - A quantile–quantile plot shows \"mild heavy tails\".\n        - The Shapiro–Wilk test for normality yields a p-value of $p = 0.06$.\n    2.  **Homoscedasticity (Equality of Variances)**:\n        - A residual-versus-fitted plot suggests \"increasing spread of residuals with larger fitted values\".\n        - Sample variances are $s_1^2 \\approx 10$, $s_2^2 \\approx 9$, $s_3^2 \\approx 40$, and $s_4^2 \\approx 36$.\n        - Levene’s test for equality of variances yields a p-value of $p = 0.01$.\n    3.  **Outliers**:\n        - One observation has a studentized residual of approximately $3.2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scrutinized for validity.\n\n- **Scientific Grounding**: The problem is well-grounded in the field of biostatistics. It describes a common and realistic scenario: checking the assumptions of a one-way ANOVA and deciding on an appropriate analytical strategy when violations are found. The methods and diagnostics mentioned (ANOVA, Shapiro–Wilk test, Levene's test, studentized residuals, Q-Q plots) are standard in statistical practice. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The question is well-posed. It provides sufficient information from diagnostic tests to assess the validity of the classical ANOVA assumptions and asks for the most appropriate inferential strategy. A single best answer can be rigorously defended based on established statistical theory.\n- **Objective**: The problem is stated in objective, quantitative, and precise language. The descriptions of diagnostic plots are standard, and the numerical results (p-values, sample variances, sample sizes) are clear.\n\nThe problem is free of the specified flaws (e.g., scientific unsoundness, incompleteness, ambiguity). The combination of unequal sample sizes ($n_1, n_2$ are small; $n_3, n_4$ are large) and unequal variances ($s_1^2, s_2^2$ are small; $s_3^2, s_4^2$ are large) presents a classic, non-trivial challenge for which standard solutions exist.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Principle-Based Derivation\nThe classical one-way ANOVA $F$-test is used to test the null hypothesis $H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k$ against the alternative $H_A$: at least one mean $\\mu_i$ is different. This test relies on three core assumptions:\n1.  **Independence**: The observations are independent both within and between groups.\n2.  **Normality**: The residuals $\\epsilon_{ij} = Y_{ij} - \\mu_i$ are normally distributed for each group $i$.\n3.  **Homoscedasticity**: The variance of the residuals is constant across all groups, i.e., $\\sigma_1^2 = \\sigma_2^2 = \\dots = \\sigma_k^2$.\n\nLet's evaluate the status of these assumptions based on the provided diagnostics.\n- **Independence**: The problem states this is \"credible\". We accept this as satisfied.\n- **Normality**: The evidence against normality is weak. A Shapiro–Wilk test p-value of $p=0.06$ does not lead to a rejection of the null hypothesis of normality at a typical significance level of $\\alpha = 0.05$. The \"mild heavy tails\" and the single large studentized residual ($r \\approx 3.2$) hint at a slight deviation, but the ANOVA $F$-test and its robust variants are known to be relatively insensitive to mild deviations from normality, especially with a total sample size of $N=80$.\n- **Homoscedasticity**: This assumption is clearly violated. The evidence is strong and consistent across multiple diagnostics:\n    - The visual inspection (residual-versus-fitted plot) shows increasing spread.\n    - The sample variances show a substantial difference: the variances in groups $3$ and $4$ (approx. $36-40$) are about $4$ times larger than those in groups $1$ and $2$ (approx. $9-10$).\n    - The formal test (Levene’s test) provides statistically significant evidence against the null hypothesis of equal variances ($p = 0.01$).\n\nThe combination of heteroscedasticity and unequal sample sizes is particularly problematic for the classical ANOVA $F$-test. The performance of the test depends critically on how the sample sizes are paired with the variances. In this problem, we have a \"direct pairing\": the groups with small sample sizes ($n_1=10, n_2=10$) have small variances ($s_1^2 \\approx 10, s_2^2 \\approx 9$), and the groups with large sample sizes ($n_3=30, n_4=30$) have large variances ($s_3^2 \\approx 40, s_4^2 \\approx 36$). This configuration makes the classical $F$-test overly conservative, meaning its actual Type I error rate is lower than the nominal level $\\alpha$, leading to a loss of statistical power. Regardless, the test is not calibrated correctly.\n\nTherefore, the most appropriate strategy must be one that formally accounts for the observed heteroscedasticity.\n\n### Option-by-Option Analysis\n\n**A. Proceed with the classical one-way Analysis of Variance (ANOVA) F test without modification; its nominal type I error rate is maintained even under unequal variances and unequal sample sizes, regardless of how the variances and sample sizes are paired.**\nThis statement is factually incorrect. The classical ANOVA $F$-test is sensitive to violations of the homoscedasticity assumption when sample sizes are unequal. As established, the pairing of variances and sample sizes determines whether the test becomes liberal (inflated Type I error) or conservative (reduced power). The claim that the Type I error rate is \"maintained ... regardless of how the variances and sample sizes are paired\" is false. Given the clear evidence of heteroscedasticity, proceeding with the classical test is inappropriate.\n**Verdict: Incorrect.**\n\n**B. Use Welch’s heteroscedastic one-way ANOVA, which adjusts for unequal variances and sample sizes via weighting and a degrees-of-freedom correction; the mild non-normality suggested by p = 0.06 is unlikely to compromise its approximate validity under independence.**\nWelch's ANOVA is an adaptation of the classical ANOVA that does not assume equal variances. It modifies the $F$ statistic by weighting each group's contribution by the inverse of its estimated variance ($w_i = n_i / s_i^2$) and uses the Welch-Satterthwaite equation to approximate the degrees of freedom for the test statistic's reference $F$-distribution. This method is specifically designed to handle the primary issue identified in the data: heteroscedasticity. Furthermore, like the classical ANOVA, Welch’s test is robust to mild deviations from normality. The description of the method and its suitability for the current situation is accurate. This is the professionally recommended approach.\n**Verdict: Correct.**\n\n**C. Use the Kruskal–Wallis test because it is unaffected by heteroscedasticity and outliers and directly tests equality of means across groups.**\nThis option contains two significant inaccuracies. First, the Kruskal-Wallis test is **not** unaffected by heteroscedasticity. Its null hypothesis is that the distributions of the groups are identical ($H_0: F_1 = F_2 = \\dots = F_k$). If the groups have the same mean or median but different variances, their distributions are not identical, and the null hypothesis is false. Under these conditions, the test can have an inflated Type I error rate; it is sensitive to differences in variance, not just location. Second, the Kruskal-Wallis test does **not** directly test for equality of means. It is a test on ranks, and it tests for stochastic dominance. It can be interpreted as a test of medians only under the additional assumption that the shapes of the distributions are the same, an assumption violated here due to heteroscedasticity.\n**Verdict: Incorrect.**\n\n**D. Use a permutation test that permutes group labels to calibrate the classical ANOVA F statistic; under independence it is exact even when groups have different variances.**\nThis statement is incorrect. A standard permutation test is valid under the \"strong null hypothesis\" that the distributions are identical across groups. If this holds, permuting group labels is a valid way to generate the null distribution. However, when the groups have different variances (heteroscedasticity), the distributions are not identical, even if the means are equal. Therefore, the exchangeability of labels under the null is violated. The permutation test for the ANOVA $F$-statistic is not robust to heteroscedasticity, especially with unequal sample sizes; it does not test the null hypothesis of equal means while allowing for unequal variances. The claim that it is \"exact\" under these conditions is false.\n**Verdict: Incorrect.**\n\n**E. Remove the single outlier (studentized residual 3.2) and then apply the classical ANOVA F test; with the outlier gone, the heteroscedasticity indicated by Levene’s test is inconsequential.**\nThis strategy is unsound. First, removing data points solely because they are outliers, without an external reason (e.g., known procedural error), is poor statistical practice. A studentized residual of $3.2$ is large but can occur by chance or indicate a true heavy-tailed distribution. Second, it makes the unsubstantiated and highly improbable claim that removing a single data point will resolve the strong and systematic evidence of heteroscedasticity. The variance difference (a factor of $4$), the clear pattern in the residual plot, and the highly significant Levene's test ($p=0.01$) all point to a structural difference in variability between the groups, not an artifact of one observation. This approach ignores the primary statistical problem.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "4957543"}, {"introduction": "The omnibus F-test can tell us *if* at least one group mean is different, but it cannot tell us *how*. Often, we have more specific questions, such as whether a treatment shows a dose-dependent effect. This final practice introduces the powerful concept of linear contrasts, demonstrating how to formulate and test a precise hypothesis about a trend across groups, moving beyond a simple \"the means are different\" conclusion to a more insightful scientific finding [@problem_id:4957537].", "problem": "A randomized clinical experiment compares the mean serum cytokine level across $4$ treatment arms: control $(i=1)$ and three increasing dose levels $(i=2,3,4)$. Let the observed cytokine levels in group $i$ be $Y_{ij}$ for $j=1,\\dots,n_i$. Adopt the one-way cell means parameterization of the linear model with homoscedastic Gaussian errors: $Y_{ij} = \\mu_i + \\varepsilon_{ij}$ with independent $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ and $i=1,\\dots,4$. The study reports the following summary statistics (sample size $n_i$, sample mean $\\bar{Y}_i$, and sample standard deviation $s_i$):\n- Group $1$ (control): $n_1 = 15$, $\\bar{Y}_1 = 22.4$, $s_1 = 4.8$.\n- Group $2$ (low dose): $n_2 = 12$, $\\bar{Y}_2 = 18.9$, $s_2 = 5.1$.\n- Group $3$ (medium dose): $n_3 = 17$, $\\bar{Y}_3 = 17.3$, $s_3 = 4.5$.\n- Group $4$ (high dose): $n_4 = 14$, $\\bar{Y}_4 = 16.8$, $s_4 = 4.2$.\n\nConsider the contrast of cell means that encodes a linear dose trend,\n$$L \\;=\\; -3\\,\\mu_1 \\;-\\; 1\\,\\mu_2 \\;+\\; 1\\,\\mu_3 \\;+\\; 3\\,\\mu_4,$$\nwhich satisfies $\\sum_{i=1}^{4} c_i = 0$ with coefficients $c_1=-3$, $c_2=-1$, $c_3=1$, $c_4=3$. Starting from the model assumptions and the analysis of variance decomposition for one-way experiments, derive from first principles an appropriate $F$ statistic to test the null hypothesis $H_0: L = 0$ versus $H_1: L \\neq 0$, and then compute its numerical value using the given summary statistics. Express the final $F$ statistic and round your answer to four significant figures.", "solution": "The derivation of the $F$ statistic proceeds from the foundational principle that an $F$-distributed random variable is the ratio of two independent chi-squared random variables, each divided by its respective degrees of freedom. We must therefore construct a numerator term related to the contrast $L$ and a denominator term related to the error variance $\\sigma^2$ that satisfy these conditions.\n\n**1. Numerator: Sum of Squares for the Contrast ($SS_L$)**\n\nThe contrast of interest is $L = \\sum_{i=1}^{4} c_i \\mu_i$. The natural estimator for $L$ is $\\hat{L} = \\sum_{i=1}^{4} c_i \\bar{Y}_i$, where $\\bar{Y}_i$ is the sample mean for group $i$.\n\nUnder the model assumptions, the sample means are independent and normally distributed: $\\bar{Y}_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2/n_i)$.\nSince $\\hat{L}$ is a linear combination of independent normal variables, it is also normally distributed. Its expectation and variance are:\n$E[\\hat{L}] = E[\\sum_{i=1}^{4} c_i \\bar{Y}_i] = \\sum_{i=1}^{4} c_i E[\\bar{Y}_i] = \\sum_{i=1}^{4} c_i \\mu_i = L$.\n$\\text{Var}(\\hat{L}) = \\text{Var}(\\sum_{i=1}^{4} c_i \\bar{Y}_i) = \\sum_{i=1}^{4} c_i^2 \\text{Var}(\\bar{Y}_i) = \\sum_{i=1}^{4} c_i^2 \\frac{\\sigma^2}{n_i} = \\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i}$.\n\nSo, $\\hat{L} \\sim \\mathcal{N} \\left( L, \\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i} \\right)$.\n\nUnder the null hypothesis, $H_0: L=0$, the distribution simplifies to $\\hat{L} \\sim \\mathcal{N} \\left( 0, \\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i} \\right)$.\nStandardizing this variable yields a standard normal distribution:\n$$Z = \\frac{\\hat{L} - 0}{\\sqrt{\\text{Var}(\\hat{L})}} = \\frac{\\hat{L}}{\\sigma \\sqrt{\\sum_{i=1}^{4} \\frac{c_i^2}{n_i}}} \\sim \\mathcal{N}(0, 1)$$\nSquaring a standard normal variable produces a chi-squared distribution with $1$ degree of freedom:\n$$Z^2 = \\frac{\\hat{L}^2}{\\sigma^2 \\sum_{i=1}^{4} \\frac{c_i^2}{n_i}} \\sim \\chi^2_1$$\nWe define the Sum of Squares for the Contrast, $SS_L$, as the part of this expression that does not depend on the unknown $\\sigma^2$:\n$$SS_L = \\frac{\\hat{L}^2}{\\sum_{i=1}^{4} \\frac{c_i^2}{n_i}}$$\nFrom this definition, it follows that $\\frac{SS_L}{\\sigma^2} \\sim \\chi^2_1$. The degrees of freedom for the numerator is $df_L = 1$. The Mean Square for the contrast is $MS_L = \\frac{SS_L}{df_L} = SS_L$.\n\n**2. Denominator: Mean Squared Error ($MS_E$)**\n\nWe need an independent estimator for the error variance $\\sigma^2$. This is provided by the Mean Squared Error, $MS_E$, which is the pooled variance from all $k=4$ groups.\nThe Sum of Squared Errors ($SS_E$) is the sum of the squared deviations from each observation to its group mean:\n$$SS_E = \\sum_{i=1}^{4} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2$$\nUsing the definition of the sample variance for each group, $s_i^2 = \\frac{1}{n_i-1} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2$, we can write $SS_E$ in terms of the given summary statistics:\n$$SS_E = \\sum_{i=1}^{4} (n_i-1) s_i^2$$\nThe degrees of freedom for error is $df_E = \\sum_{i=1}^{4} (n_i-1) = N-k$, where $N=\\sum n_i$ is the total sample size.\nBy Cochran's theorem, the quantity $\\frac{SS_E}{\\sigma^2}$ follows a chi-squared distribution with $df_E$ degrees of freedom: $\\frac{SS_E}{\\sigma^2} \\sim \\chi^2_{N-k}$.\nThe Mean Squared Error is the unbiased estimator of $\\sigma^2$: $MS_E = \\frac{SS_E}{df_E} = \\frac{\\sum (n_i-1)s_i^2}{N-k}$.\nCrucially, Cochran's theorem also establishes that $SS_L$ and $SS_E$ are independent random variables.\n\n**3. The F-Statistic**\n\nThe $F$ statistic is formed as the ratio of the two independent mean squares:\n$$F = \\frac{MS_L}{MS_E} = \\frac{SS_L/df_L}{SS_E/df_E} = \\frac{SS_L}{MS_E}$$\nSubstituting the derived expressions:\n$$F = \\frac{ \\frac{\\hat{L}^2}{\\sum_{i=1}^{4} (c_i^2/n_i)} }{ \\frac{\\sum_{i=1}^{4} (n_i-1)s_i^2}{N-k} }$$\nUnder $H_0$, this statistic follows an $F$-distribution with $df_1 = 1$ and $df_2 = N-k$ degrees of freedom.\n\n**4. Numerical Calculation**\n\nFirst, calculate the total sample size $N$ and the error degrees of freedom $df_E$:\n$N = n_1 + n_2 + n_3 + n_4 = 15 + 12 + 17 + 14 = 58$.\n$df_E = N-k = 58 - 4 = 54$.\n\nNext, compute the estimated contrast $\\hat{L}$:\n$\\hat{L} = c_1\\bar{Y}_1 + c_2\\bar{Y}_2 + c_3\\bar{Y}_3 + c_4\\bar{Y}_4$\n$\\hat{L} = (-3)(22.4) + (-1)(18.9) + (1)(17.3) + (3)(16.8) = -67.2 - 18.9 + 17.3 + 50.4 = -18.4$.\n\nNow, calculate the terms for the numerator sum of squares, $SS_L$:\n$\\sum_{i=1}^{4} \\frac{c_i^2}{n_i} = \\frac{(-3)^2}{15} + \\frac{(-1)^2}{12} + \\frac{1^2}{17} + \\frac{3^2}{14} = \\frac{9}{15} + \\frac{1}{12} + \\frac{1}{17} + \\frac{9}{14}$\n$\\sum_{i=1}^{4} \\frac{c_i^2}{n_i} \\approx 0.6 + 0.083333 + 0.058824 + 0.642857 \\approx 1.385014$.\n$SS_L = MS_L = \\frac{\\hat{L}^2}{\\sum (c_i^2/n_i)} = \\frac{(-18.4)^2}{1.385014} = \\frac{338.56}{1.385014} \\approx 244.4447$.\n\nNext, compute the terms for the denominator mean square, $MS_E$:\n$SS_E = (15-1)(4.8)^2 + (12-1)(5.1)^2 + (17-1)(4.5)^2 + (14-1)(4.2)^2$\n$SS_E = 14(23.04) + 11(26.01) + 16(20.25) + 13(17.64)$\n$SS_E = 322.56 + 286.11 + 324.00 + 229.32 = 1161.99$.\n$MS_E = \\frac{SS_E}{df_E} = \\frac{1161.99}{54} \\approx 21.518333$.\n\nFinally, compute the $F$ statistic:\n$F = \\frac{MS_L}{MS_E} = \\frac{244.4447}{21.518333} \\approx 11.36000$.\n\nRounding to four significant figures, the value of the $F$ statistic is $11.36$.", "answer": "$$\n\\boxed{11.36}\n$$", "id": "4957537"}]}