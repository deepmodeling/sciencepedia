{"hands_on_practices": [{"introduction": "The first step in mastering a new statistical tool is applying it to a concrete dataset. This exercise guides you through the fundamental process of calculating a confidence interval for a correlation coefficient, $\\rho$, using the Fisher Z transformation. We will also explore a subtle but important refinement—a bias correction—that improves accuracy, particularly for the small sample sizes often encountered in pilot studies or clinical research [@problem_id:4915699].", "problem": "A clinical study investigates the association between a continuous biomarker and a continuous clinical severity score measured simultaneously on a sample of $n=12$ independent patients. Assume the paired measurements arise from a bivariate normal distribution with unknown population correlation $\\rho$. The observed sample Pearson correlation is $r=0.70$. Using the framework of large-sample inference for correlations based on Fisher's $z$ transformation and its small-sample bias properties:\n\n1. Construct a $95\\%$ confidence interval (CI) for $\\rho$ using the standard Fisher $z$ approach.\n2. Construct a $95\\%$ CI for $\\rho$ using a bias-corrected Fisher $z$ approach that removes the first-order small-sample bias in the transformed scale.\n3. Compute the widths of both CIs and determine whether the plausible true correlation value $\\rho_0=0.65$ would be contained in each CI.\n4. Report, as a single number, the ratio of the width of the bias-corrected CI to the width of the standard CI. Round your final reported ratio to four significant figures.", "solution": "The problem requires the construction and comparison of two types of confidence intervals for a population correlation coefficient, $\\rho$, based on a sample correlation, $r$, from a sample of size $n$. The underlying theory is Fisher's $z$-transformation, which stabilizes the variance and transforms the sampling distribution of the correlation coefficient to be approximately normal.\n\nThe data and parameters provided are:\n- Sample size: $n=12$\n- Sample Pearson correlation: $r=0.70$\n- Confidence level: $95\\%$, which corresponds to a standard normal critical value $z_{\\alpha/2} = z_{0.025}$ for a two-sided interval. The value is $z_{0.025} \\approx 1.95996$.\n- A hypothetical true correlation for evaluation: $\\rho_0=0.65$.\n\nThe problem is scientifically grounded, well-posed, objective, and complete. All necessary information is provided, and the methods requested are standard techniques in biostatistics for inference on correlation coefficients. The problem is therefore valid.\n\n**1. Standard 95% Confidence Interval for $\\rho$**\n\nThe standard approach uses Fisher's $z$-transformation, defined as $z = \\arctanh(r)$. The transformed variable $z$ is approximately normally distributed with mean $\\zeta = \\arctanh(\\rho)$ and standard deviation (which we call the standard error) $\\sigma_z = \\frac{1}{\\sqrt{n-3}}$.\n\nFirst, we compute the transformed sample correlation $z$:\n$$z = \\arctanh(r) = \\arctanh(0.70) = \\frac{1}{2} \\ln\\left(\\frac{1+0.70}{1-0.70}\\right) = \\frac{1}{2} \\ln\\left(\\frac{1.7}{0.3}\\right) \\approx 0.86730$$\n\nNext, we compute the standard error, $SE(z)$:\n$$SE(z) = \\frac{1}{\\sqrt{n-3}} = \\frac{1}{\\sqrt{12-3}} = \\frac{1}{\\sqrt{9}} = \\frac{1}{3}$$\n\nThe $95\\%$ confidence interval for the transformed population parameter $\\zeta$ is given by $z \\pm z_{0.025} \\cdot SE(z)$.\n$$CI_{\\zeta} = 0.86730 \\pm 1.95996 \\times \\frac{1}{3} = 0.86730 \\pm 0.65332$$\nThis gives the interval for $\\zeta$ as $[0.21398, 1.52062]$.\n\nTo obtain the confidence interval for $\\rho$, we apply the inverse transformation, $\\rho = \\tanh(\\zeta)$, to the lower and upper bounds of $CI_{\\zeta}$.\n$$ \\rho_L = \\tanh(0.21398) \\approx 0.21102 $$\n$$ \\rho_U = \\tanh(1.52062) \\approx 0.90897 $$\nThe standard $95\\%$ CI for $\\rho$ is approximately $[0.2110, 0.9090]$.\n\n**2. Bias-Corrected 95% Confidence Interval for $\\rho$**\n\nFisher's $z$-transformation has a small-sample bias. The expected value of $z$ is approximately $E[z] \\approx \\zeta + \\frac{\\rho}{2(n-1)}$. To correct for this, we can subtract an estimate of the bias term from our sample $z$. The bias-corrected estimate of $\\zeta$ is $z_{adj} = z - \\frac{r}{2(n-1)}$.\n\nFirst, we calculate the estimated bias:\n$$ \\text{Bias}(z) \\approx \\frac{r}{2(n-1)} = \\frac{0.70}{2(12-1)} = \\frac{0.70}{22} \\approx 0.03182 $$\n\nNext, we find the adjusted $z$-value:\n$$ z_{adj} = 0.86730 - 0.03182 = 0.83548 $$\n\nThe confidence interval for $\\zeta$ is now centered at $z_{adj}$, using the same standard error and margin of error:\n$$ CI_{\\zeta, corr} = z_{adj} \\pm z_{0.025} \\cdot SE(z) = 0.83548 \\pm 0.65332 $$\nThis gives the bias-corrected interval for $\\zeta$ as $[0.18216, 1.48880]$.\n\nWe transform these bounds back to the $\\rho$ scale:\n$$ \\rho_{L, corr} = \\tanh(0.18216) \\approx 0.18025 $$\n$$ \\rho_{U, corr} = \\tanh(1.48880) \\approx 0.90302 $$\nThe bias-corrected $95\\%$ CI for $\\rho$ is approximately $[0.1803, 0.9030]$.\n\n**3. CI Widths and Containment of $\\rho_0 = 0.65$**\n\nWe compute the widths of both confidence intervals.\nThe width of the standard CI is:\n$$ W_{std} = \\rho_U - \\rho_L = 0.90897 - 0.21102 = 0.69795 $$\n\nThe width of the bias-corrected CI is:\n$$ W_{corr} = \\rho_{U, corr} - \\rho_{L, corr} = 0.90302 - 0.18025 = 0.72277 $$\n\nThe correction shifts the center of the interval on the $z$-scale closer to $0$. Due to the non-linear nature of the $\\tanh$ function (it is less steep near $z=0$), this shift results in a wider confidence interval on the $\\rho$-scale.\n\nNext, we check if $\\rho_0=0.65$ is contained in each interval.\n- Standard CI: $[0.2110, 0.9090]$. Since $0.2110 \\le 0.65 \\le 0.9090$, $\\rho_0$ is contained in the standard CI.\n- Bias-corrected CI: $[0.1803, 0.9030]$. Since $0.1803 \\le 0.65 \\le 0.9030$, $\\rho_0$ is also contained in the bias-corrected CI.\n\n**4. Ratio of CI Widths**\n\nFinally, we compute the ratio of the width of the bias-corrected CI to that of the standard CI.\n$$ \\text{Ratio} = \\frac{W_{corr}}{W_{std}} = \\frac{0.72277}{0.69795} \\approx 1.03556 $$\n\nRounding to four significant figures, the ratio is $1.036$.", "answer": "$$\n\\boxed{1.036}\n$$", "id": "4915699"}, {"introduction": "Statistical methods rely on assumptions, and the validity of our conclusions hinges on these assumptions being reasonably met. This next practice addresses a critical question: what should we do when our data is not bivariate normal, violating the core assumption of the Fisher Z method? Through this hypothetical scenario, we will explore a principled, modern statistical approach that uses data transformation to satisfy the method's requirements while preserving the essential relationship between the variables [@problem_id:4915681].", "problem": "A biostatistics team studies the association between two serum biomarkers, denoted by $X$ and $Y$, measured in $n=120$ independent patients. Exploratory analysis shows that each marginal distribution is continuous, strictly positive, and markedly right-skewed with heavy tails. A scatterplot suggests a monotone increasing association but with non-elliptical contours due to the skewness. The investigators want to report a $95\\%$ confidence interval for the Pearson correlation of $X$ and $Y$ using the classical Fisher $z$ approach, but they are concerned that the usual large-sample normal approximation underlying this approach relies on approximate bivariate normality, which is questionable here.\n\nStarting from fundamental facts that (i) the Pearson correlation measures linear association and is not invariant to nonlinear marginal transformations, (ii) Fisher’s $z$-based inference is derived under the bivariate normal model where the Fisher-transformed sample correlation has an approximately normal distribution with variance depending only on $n$, and (iii) strictly monotone marginal transformations preserve the rank-based dependence structure while altering higher moments and marginal shapes, choose the single most appropriate transformation strategy that would justify using Fisher’s $z$-based inference in this non-normal setting. Your choice must be based on a principle that preserves the underlying monotone dependence while rendering the transformed marginals approximately normal, so that the assumptions behind Fisher’s $z$ are approximately satisfied.\n\nWhich option best accomplishes this aim and provides the correct justification?\n\nA. Apply a rank-based inverse normal transformation to each marginal (marginal normal scores), then compute the Pearson correlation on the transformed variables and apply Fisher’s $z$ to that correlation. Justification: strictly monotone, rank-based marginal transformations preserve the copula (dependence structure). Under a Gaussian copula model, the transformed pair has approximately bivariate normal margins and elliptical dependence, so Fisher’s $z$ inference with variance approximately $1/(n-3)$ is appropriate.\n\nB. Standardize $X$ and $Y$ to have sample mean $0$ and sample variance $1$, then compute the Pearson correlation and apply Fisher’s $z$. Justification: standardization, together with the Central Limit Theorem, makes the data approximately normal, which legitimizes Fisher’s $z$.\n\nC. Replace Pearson’s correlation with Spearman’s rank correlation to reduce sensitivity to marginal non-normality, then apply Fisher’s $z$ transformation to the Spearman correlation. Justification: ranks are robust to skewness and Fisher’s $z$ applies to any correlation measure.\n\nD. Apply the same strictly increasing nonlinear transformation to both $X$ and $Y$ (for example, a logarithm) until the histograms look visually symmetric, then compute the Pearson correlation and apply Fisher’s $z$. Justification: any monotone transformation preserves correlation, so making the histograms look symmetric is sufficient for Fisher’s $z$ to be valid.\n\nE. Keep the data on the original scale, compute the Pearson correlation, but use a nonparametric bootstrap to estimate the sampling distribution of the Fisher $z$ statistic. Justification: the bootstrap makes the Fisher $z$ statistic approximately normal regardless of marginal distributions, so the usual Fisher interval is valid.", "solution": "The problem asks for the most appropriate transformation strategy to justify the use of Fisher's $z$-transformation for constructing a confidence interval for the Pearson correlation between two biomarkers, $X$ and $Y$. The data consists of $n=120$ pairs, and the marginal distributions are known to be non-normal (continuous, strictly positive, right-skewed, heavy-tailed).\n\nFirst, let us recall the fundamentals of the Fisher $z$-transformation. Given a sample Pearson correlation coefficient $r$ calculated from a sample of size $n$ drawn from a bivariate normal distribution with population correlation $\\rho$, the Fisher $z$-transformation is defined as:\n$$z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) = \\text{arctanh}(r)$$\nThe key result is that, under the assumption of bivariate normality, the sampling distribution of $z$ is approximately normal, even for small $n$:\n$$z \\sim \\mathcal{N}\\left( \\text{arctanh}(\\rho), \\frac{1}{n-3} \\right)$$\nThis allows for the construction of a $(1-\\alpha) \\times 100\\%$ confidence interval for $\\text{arctanh}(\\rho)$ as $z \\pm z_{\\alpha/2} \\frac{1}{\\sqrt{n-3}}$, which can then be back-transformed to obtain a confidence interval for $\\rho$.\n\nThe critical issue here is that the data for $X$ and $Y$ are explicitly stated to be non-normal, which invalidates the primary assumption underlying this procedure. The goal is therefore to find a transformation strategy that makes the data conform as closely as possible to the bivariate normal assumption, while preserving the underlying dependence structure.\n\nAccording to Sklar's theorem, any continuous multivariate distribution can be decomposed into its marginal distributions and a copula, which describes the dependence structure between the variables. A strictly monotone transformation applied to the marginals of a distribution does not change the copula. Thus, we can change the marginal distributions to be of a more desirable form (e.g., normal) while preserving the intrinsic dependence structure.\n\nThe problem states that there is a \"monotone increasing association\". We seek a transformation that preserves this rank-based dependence structure while inducing normality on the marginals. A powerful method to achieve this is the rank-based inverse normal transformation, also known as obtaining normal scores. For each variable, say $X$, the observations $x_1, x_2, \\dots, x_n$ are replaced by their normal scores. A common version of this transformation is $x'_i = \\Phi^{-1}\\left(\\frac{\\text{rank}(x_i)}{n+1}\\right)$, where $\\Phi^{-1}$ is the quantile function (inverse CDF) of the standard normal distribution, and the denominator is adjusted to $n+1$ to avoid evaluating at $1$. After this transformation, the new variables, say $X'$ and $Y'$, will have marginal distributions that are, by construction, approximately standard normal.\n\nIf the original dependence structure (copula) of $(X, Y)$ was a Gaussian copula, then the transformed variables $(X', Y')$ will have an approximately bivariate normal distribution. This is the ideal scenario for applying Fisher's $z$-transformation. We can then compute the Pearson correlation of the transformed data, $r' = \\text{corr}(X', Y')$, and proceed with the classical Fisher $z$ inference, as its assumptions are now approximately met.\n\nWith this theoretical framework, we now evaluate the given options.\n\nA. Apply a rank-based inverse normal transformation to each marginal (marginal normal scores), then compute the Pearson correlation on the transformed variables and apply Fisher’s $z$ to that correlation. Justification: strictly monotone, rank-based marginal transformations preserve the copula (dependence structure). Under a Gaussian copula model, the transformed pair has approximately bivariate normal margins and elliptical dependence, so Fisher’s $z$ inference with variance approximately $1/(n-3)$ is appropriate.\nThis option precisely describes the procedure derived above. The transformation induces approximate marginal normality. The justification correctly states that this rank-based monotone transformation preserves the copula. It correctly notes that if the underlying copula is Gaussian, the transformed data become approximately bivariate normal, which is the exact condition required for the classical Fisher $z$ approach to be valid. The variance of approximately $1/(n-3)$ is then appropriate. This is a theoretically sound and statistically principled approach.\nVerdict: **Correct**.\n\nB. Standardize $X$ and $Y$ to have sample mean $0$ and sample variance $1$, then compute the Pearson correlation and apply Fisher’s $z$. Justification: standardization, together with the Central Limit Theorem, makes the data approximately normal, which legitimizes Fisher’s $z$.\nThis option is fundamentally flawed. Standardization is a linear transformation, $X_{\\text{std}} = (X - \\mu_X)/\\sigma_X$. A linear transformation does not alter the shape of a distribution; a right-skewed distribution remains right-skewed after standardization. The justification invokes the Central Limit Theorem (CLT), which states that the distribution of the *sample mean* tends to normality as the sample size increases. The CLT says nothing about the distribution of the data itself. Therefore, this procedure fails to address the non-normality of the marginals.\nVerdict: **Incorrect**.\n\nC. Replace Pearson’s correlation with Spearman’s rank correlation to reduce sensitivity to marginal non-normality, then apply Fisher’s $z$ transformation to the Spearman correlation. Justification: ranks are robust to skewness and Fisher’s $z$ applies to any correlation measure.\nWhile switching to Spearman's rank correlation, $r_S$, is a valid robust strategy, the justification for the subsequent step is false. The Fisher $z$-transformation and its associated variance formula, $1/(n-3)$, were specifically derived for the Pearson correlation coefficient $r$ under the assumption of bivariate normality. It does not apply universally to any correlation measure. While a similar variance-stabilizing transformation exists for $r_S$, its variance is approximately $1.06/(n-3)$, not $1/(n-3)$. Applying the classical Fisher machinery directly to $r_S$ is incorrect.\nVerdict: **Incorrect**.\n\nD. Apply the same strictly increasing nonlinear transformation to both $X$ and $Y$ (for example, a logarithm) until the histograms look visually symmetric, then compute the Pearson correlation and apply Fisher’s $z$. Justification: any monotone transformation preserves correlation, so making the histograms look symmetric is sufficient for Fisher’s $z$ to be valid.\nThis option contains two significant errors in its justification. First, \"any monotone transformation preserves correlation\" is false. Pearson correlation measures *linear* association and is not invariant under nonlinear transformations; i.e., in general, $\\text{corr}(g(X), g(Y)) \\ne \\text{corr}(X,Y)$ for a nonlinear function $g$. It is rank-based correlations that are preserved. Second, \"making the histograms look symmetric is sufficient\" is also false. Achieving normal marginals is a necessary, but not sufficient, condition for bivariate normality. The dependence structure (copula) might still be non-Gaussian. The approach in Option A is more principled than an ad-hoc visual approach, and the justification here is factually incorrect.\nVerdict: **Incorrect**.\n\nE. Keep the data on the original scale, compute the Pearson correlation, but use a nonparametric bootstrap to estimate the sampling distribution of the Fisher $z$ statistic. Justification: the bootstrap makes the Fisher $z$ statistic approximately normal regardless of marginal distributions, so the usual Fisher interval is valid.\nThis option describes a valid alternative inferential procedure (bootstrapping), but it does not fulfill the stated task, which is to find a transformation strategy that *justifies using the classical Fisher's z-based inference*. Moreover, the justification provided is misleading. The bootstrap does not \"make\" the statistic normal; it provides an empirical approximation of the statistic's true sampling distribution, whatever shape it may have. One would then typically use the percentiles of this empirical distribution to form a confidence interval (e.g., a percentile bootstrap interval), which can be asymmetric and is distinct from the \"usual Fisher interval\" that relies on the symmetric normal approximation. The bootstrap is a way to bypass the assumptions of the classical method, not a way to satisfy them.\nVerdict: **Incorrect**.\n\nIn conclusion, Option A presents the only method and justification that is fully consistent with modern statistical theory for handling this specific problem. It correctly identifies a principled transformation that directly addresses the violation of the normality assumption, thereby legitimizing the subsequent use of the classical Fisher $z$ procedure.", "answer": "$$\\boxed{A}$$", "id": "4915681"}, {"introduction": "Beyond analyzing data we already have, a key task for a biostatistician is helping to design future studies. This exercise demonstrates how the Fisher Z transformation is an essential tool for power analysis, which allows us to estimate the probability that a study will detect a true effect. By calculating statistical power, researchers can determine an adequate sample size and avoid conducting underpowered, inconclusive experiments [@problem_id:4915697].", "problem": "Consider a sequence of independent and identically distributed paired observations $\\{(X_i,Y_i)\\}_{i=1}^n$ drawn from a Bivariate Normal distribution (BVN) with population Pearson correlation $\\rho$. Let $r$ denote the sample Pearson correlation computed from $(X_1,Y_1),\\dots,(X_n,Y_n)$. You will compare the approximate power of two different two-sided hypothesis tests across sample sizes: the Student's $t$-based test for the null hypothesis $H_0:\\rho=0$, and the Fisher $z$-based test for the general null hypothesis $H_0:\\rho=\\rho_0$ (where $\\rho_0$ may be nonzero). The power should be obtained without using any external data files and must be derived from fundamental definitions and well-tested facts about sampling distributions and transformations under the BVN model.\n\nDesign an algorithm that, starting from core definitions of Pearson correlation and the BVN model together with widely accepted distributional approximations, produces analytic approximations of the two-sided test power for each test:\n- For the Student's $t$-based test of $H_0:\\rho=0$, use an approximation consistent with large-sample behavior to obtain the power under a true $\\rho$.\n- For the Fisher $z$-based test of $H_0:\\rho=\\rho_0$, use an approximation consistent with large-sample behavior to obtain the power under a true $\\rho$.\n\nYour program must implement both power functions and evaluate them on the following test suite of parameter sets. Each test case is specified by $(n,\\alpha,\\rho,\\rho_0)$:\n- Case $1$: $(n,\\alpha,\\rho,\\rho_0)=(8,0.05,0.0,0.0)$, a type I error check under the Bivariate Normal model.\n- Case $2$: $(n,\\alpha,\\rho,\\rho_0)=(8,0.05,0.3,0.0)$, small sample with moderate true correlation against zero.\n- Case $3$: $(n,\\alpha,\\rho,\\rho_0)=(30,0.05,0.3,0.0)$, moderate sample with moderate true correlation against zero.\n- Case $4$: $(n,\\alpha,\\rho,\\rho_0)=(100,0.05,0.1,0.0)$, large sample with small true correlation against zero.\n- Case $5$: $(n,\\alpha,\\rho,\\rho_0)=(30,0.05,0.5,0.2)$, moderate sample testing a nonzero null value.\n- Case $6$: $(n,\\alpha,\\rho,\\rho_0)=(12,0.01,0.8,0.5)$, stricter significance level with high true correlation against a nonzero null.\n\nNo physical units are involved. All significance levels $\\alpha$ must be treated as decimals (for example, $\\alpha=0.05$). For each case, compute:\n- The approximate two-sided power of the Student's $t$-based test for $H_0:\\rho=0$ under the true $\\rho$ and sample size $n$ at level $\\alpha$.\n- The approximate two-sided power of the Fisher $z$-based test for $H_0:\\rho=\\rho_0$ under the true $\\rho$ and sample size $n$ at level $\\alpha$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and with no spaces. For each case, output the two powers in order, first the Student's $t$-based power and second the Fisher $z$-based power. Concatenate all case results into one flat list. Each numeric result must be rounded to $6$ decimal places. For example, the output format must be of the form $[p_{t,1},p_{z,1},p_{t,2},p_{z,2},\\dots]$, where $p_{t,i}$ is the Student's $t$-based power for case $i$ and $p_{z,i}$ is the Fisher $z$-based power for case $i$.\n\nTo be graded automatically, ensure the program is self-contained and relies only on well-tested distributional approximations under the BVN model. The test suite is designed to probe:\n- A \"happy path\" moderate-sample scenario,\n- Boundary behavior near type I error control,\n- Edge cases with small $n$ and with strict $\\alpha$,\n- Scenarios with nonzero null hypotheses $\\rho_0\\neq 0$.", "solution": "The user-provided problem has been validated and is determined to be a well-posed, scientifically grounded problem in the domain of biostatistics. It is free of contradictions, ambiguities, and factual errors. The task is to derive and implement analytic approximations for the statistical power of two common hypothesis tests concerning the Pearson correlation coefficient, $\\rho$, under the Bivariate Normal (BVN) distribution assumption. We will now proceed with the solution.\n\nThe problem requires a comparative analysis of the statistical power for two distinct hypothesis tests for the Pearson correlation coefficient $\\rho$. The first is the Student's $t$-based test, which is exact for the null hypothesis $H_0: \\rho=0$. The second is the Fisher's $z$-based test, which is an approximate test applicable to a general null hypothesis $H_0: \\rho=\\rho_0$. For both, we will derive the power using an approximation based on Fisher's z-transformation, consistent with the problem's requirement for large-sample behavior.\n\nLet $\\{(X_i,Y_i)\\}_{i=1}^n$ be a sample of size $n$ drawn from a BVN distribution with population correlation $\\rho$. Let $r$ be the sample Pearson correlation coefficient.\n\n### Test 1: Student's $t$-based Test for $H_0: \\rho = 0$\n\nThis test is specifically designed for the null hypothesis that the population correlation is zero.\n\n**Test Statistic and Rejection Region:**\nUnder the null hypothesis $H_0: \\rho=0$, the statistic\n$$\nT = r \\sqrt{\\frac{n-2}{1-r^2}}\n$$\nfollows a Student's $t$-distribution with $\\nu = n-2$ degrees of freedom. For a two-sided test at a significance level $\\alpha$, we reject $H_0$ if the absolute value of the observed statistic, $|T|$, exceeds the critical value $t_{1-\\alpha/2, n-2}$.\n$$\n|T| > t_{1-\\alpha/2, n-2}\n$$\nThis inequality defines the rejection region. To calculate power, it is more convenient to express this region in terms of the sample correlation $r$. By rearranging the inequality, we find that we reject $H_0$ if $|r| > r_{\\text{crit}}$, where the critical value for $r$ is:\n$$\nr_{\\text{crit}} = \\sqrt{\\frac{t_{1-\\alpha/2, n-2}^2}{n-2 + t_{1-\\alpha/2, n-2}^2}}\n$$\n\n**Power Calculation:**\nThe power of the test is the probability of rejecting $H_0$ given that the true population correlation is $\\rho \\neq 0$.\n$$\n\\text{Power} = P(|r| > r_{\\text{crit}} | \\rho) = P(r > r_{\\text{crit}} | \\rho) + P(r < -r_{\\text{crit}} | \\rho)\n$$\nWhen $\\rho \\neq 0$, the sampling distribution of $r$ is skewed and complex. To approximate this probability, we use Fisher's z-transformation, which is a variance-stabilizing and normalizing transformation for the correlation coefficient. The transformation is defined as $z = \\text{arctanh}(r)$. The key result is that for a sample from a BVN distribution with true correlation $\\rho$, the statistic $z$ is approximately normally distributed:\n$$\nz \\dot{\\sim} N\\left(\\zeta, \\sigma_z^2\\right)\n$$\nwhere $\\zeta = \\text{arctanh}(\\rho)$ is the transformed population correlation and the variance is $\\sigma_z^2 = \\frac{1}{n-3}$.\n\nWe transform the critical values of $r$ to the $z$-scale: $z_{\\text{crit},t} = \\text{arctanh}(r_{\\text{crit}})$ and $-z_{\\text{crit},t} = \\text{arctanh}(-r_{\\text{crit}})$. The power calculation then becomes:\n$$\n\\text{Power} \\approx P(z > z_{\\text{crit},t} | \\zeta) + P(z < -z_{\\text{crit},t} | \\zeta)\n$$\nStandardizing the variable $z$ using its approximate distribution $N(\\zeta, 1/(n-3))$, we get:\n$$\n\\text{Power} \\approx P\\left(\\frac{z - \\zeta}{\\sigma_z} > \\frac{z_{\\text{crit},t} - \\zeta}{\\sigma_z}\\right) + P\\left(\\frac{z - \\zeta}{\\sigma_z} < \\frac{-z_{\\text{crit},t} - \\zeta}{\\sigma_z}\\right)\n$$\nLet $\\Phi$ be the cumulative distribution function (CDF) of the standard normal distribution $N(0,1)$. The power is then:\n$$\n\\text{Power}_t = \\left(1 - \\Phi\\left(\\frac{z_{\\text{crit},t} - \\zeta}{\\sigma_z}\\right)\\right) + \\Phi\\left(\\frac{-z_{\\text{crit},t} - \\zeta}{\\sigma_z}\\right)\n$$\nSubstituting $\\sigma_z = 1/\\sqrt{n-3}$, the final expression is:\n$$\n\\text{Power}_t = 1 - \\Phi\\left( (z_{\\text{crit},t} - \\zeta)\\sqrt{n-3} \\right) + \\Phi\\left( (-z_{\\text{crit},t} - \\zeta)\\sqrt{n-3} \\right)\n$$\n\n### Test 2: Fisher's $z$-based Test for $H_0: \\rho = \\rho_0$\n\nThis test is more general and can be used for any null correlation value $\\rho_0 \\in (-1, 1)$.\n\n**Test Statistic and Rejection Region:**\nThe test is based entirely on the Fisher z-transformation. The test statistic is:\n$$\nZ = (z - \\zeta_0) \\sqrt{n-3}\n$$\nwhere $z = \\text{arctanh}(r)$ and $\\zeta_0 = \\text{arctanh}(\\rho_0)$. Under the null hypothesis $H_0: \\rho = \\rho_0$, the statistic $Z$ is approximately distributed as a standard normal, $Z \\dot{\\sim} N(0,1)$. For a two-sided test at significance level $\\alpha$, we reject $H_0$ if $|Z| > z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the upper $\\alpha/2$ critical value of the standard normal distribution.\n\n**Power Calculation:**\nThe power is the probability of this rejection event occurring when the true population correlation is $\\rho$. The rejection rule $|Z| > z_{1-\\alpha/2}$ is equivalent to:\n$$\nz - \\zeta_0 > \\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}} \\quad \\text{or} \\quad z - \\zeta_0 < -\\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}}\n$$\nThis gives the rejection region in terms of $z$:\n$$\nz > \\zeta_0 + \\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}} \\quad \\text{or} \\quad z < \\zeta_0 - \\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}}\n$$\nTo find the power, we calculate the probability of $z$ falling into this region, using its distribution under the alternative hypothesis, $z \\dot{\\sim} N(\\zeta, 1/(n-3))$, where $\\zeta = \\text{arctanh}(\\rho)$.\n$$\n\\text{Power} = P\\left(z > \\zeta_0 + \\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}} \\:\\middle|\\: \\zeta \\right) + P\\left(z < \\zeta_0 - \\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}} \\:\\middle|\\: \\zeta \\right)\n$$\nStandardizing the variable $z$:\n$$\n\\text{Power} \\approx P\\left(\\frac{z - \\zeta}{\\sigma_z} > \\frac{\\zeta_0 + \\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}} - \\zeta}{\\sigma_z} \\right) + P\\left(\\frac{z - \\zeta}{\\sigma_z} < \\frac{\\zeta_0 - \\frac{z_{1-\\alpha/2}}{\\sqrt{n-3}} - \\zeta}{\\sigma_z} \\right)\n$$\nSubstituting $\\sigma_z = 1/\\sqrt{n-3}$ simplifies the arguments:\n$$\n\\text{Power}_z = 1 - \\Phi\\left( (\\zeta_0 - \\zeta)\\sqrt{n-3} + z_{1-\\alpha/2} \\right) + \\Phi\\left( (\\zeta_0 - \\zeta)\\sqrt{n-3} - z_{1-\\alpha/2} \\right)\n$$\nThis formula provides the approximate power for the general Fisher's $z$-based test.\n\n### Algorithmic Implementation\n\nThe algorithm will proceed as follows for each test case $(n, \\alpha, \\rho, \\rho_0)$:\n1.  **Student's t-test Power**:\n    -   Calculate degrees of freedom $\\nu = n-2$.\n    -   Determine the critical t-value $t_{1-\\alpha/2, \\nu}$.\n    -   Compute the corresponding critical r-value, $r_{\\text{crit}}$.\n    -   Transform $r_{\\text{crit}}$ and the true $\\rho$ to the z-scale: $z_{\\text{crit},t} = \\text{arctanh}(r_{\\text{crit}})$ and $\\zeta = \\text{arctanh}(\\rho)$.\n    -   Apply the derived power formula `Power_t`.\n\n2.  **Fisher's z-test Power**:\n    -   Determine the critical standard normal value $z_{1-\\alpha/2}$.\n    -   Transform the null hypothesis correlation $\\rho_0$ and the true correlation $\\rho$ to the z-scale: $\\zeta_0 = \\text{arctanh}(\\rho_0)$ and $\\zeta = \\text{arctanh}(\\rho)$.\n    -   Apply the derived power formula `Power_z`.\n\nThe results from both calculations for each test case will be collected and formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t, norm\n\ndef solve():\n    \"\"\"\n    Computes and compares the approximate statistical power of the Student's t-based test\n    and Fisher's z-based test for Pearson correlation across several scenarios.\n    \"\"\"\n    # Test cases are specified by the tuple (n, alpha, rho, rho_0), where:\n    # n: sample size\n    # alpha: significance level\n    # rho: true population correlation under the alternative hypothesis\n    # rho_0: population correlation under the null hypothesis (for z-test only)\n    test_cases = [\n        # Case 1: Type I error check (n=8, alpha=0.05, rho=0.0, rho_0=0.0)\n        (8, 0.05, 0.0, 0.0),\n        # Case 2: Small sample, moderate effect (n=8, alpha=0.05, rho=0.3, rho_0=0.0)\n        (8, 0.05, 0.3, 0.0),\n        # Case 3: Moderate sample, moderate effect (n=30, alpha=0.05, rho=0.3, rho_0=0.0)\n        (30, 0.05, 0.3, 0.0),\n        # Case 4: Large sample, small effect (n=100, alpha=0.05, rho=0.1, rho_0=0.0)\n        (100, 0.05, 0.1, 0.0),\n        # Case 5: Nonzero null hypothesis (n=30, alpha=0.05, rho=0.5, rho_0=0.2)\n        (30, 0.05, 0.5, 0.2),\n        # Case 6: Stricter alpha, high correlation (n=12, alpha=0.01, rho=0.8, rho_0=0.5)\n        (12, 0.01, 0.8, 0.5),\n    ]\n\n    results = []\n\n    for n, alpha, rho, rho_0 in test_cases:\n        # --- Power calculation for Student's t-based test (H0: rho = 0) ---\n        \n        # The t-test is valid for n > 2. The z-transform approximation requires n > 3.\n        # All test cases satisfy these conditions.\n        \n        # 1. Find the rejection region for the t-test in terms of r.\n        df = n - 2\n        t_crit = t.ppf(1 - alpha / 2, df)\n        r_crit_sq = t_crit**2 / (df + t_crit**2)\n        r_crit = np.sqrt(r_crit_sq)\n\n        # 2. Transform to the z-scale for power calculation.\n        # z_crit_t is the critical value on the z-scale corresponding to the t-test's rejection region.\n        z_crit_t = np.arctanh(r_crit)\n        # zeta is the true population correlation on the z-scale.\n        zeta = np.arctanh(rho)\n        sqrt_n_minus_3 = np.sqrt(n - 3)\n\n        # 3. Calculate power using the normal approximation for the distribution of z.\n        # Power is P(z > z_crit_t) + P(z < -z_crit_t) under the alternative.\n        # Standardize z: (z - zeta) * sqrt(n-3) ~ N(0,1)\n        arg1_t = (z_crit_t - zeta) * sqrt_n_minus_3\n        arg2_t = (-z_crit_t - zeta) * sqrt_n_minus_3\n        # norm.sf (survival function) is 1 - norm.cdf, often more accurate for large arguments.\n        power_t = norm.sf(arg1_t) + norm.cdf(arg2_t)\n        results.append(round(power_t, 6))\n\n        # --- Power calculation for Fisher's z-based test (H0: rho = rho_0) ---\n        \n        # 1. Find the rejection region for the z-test on the z-scale.\n        z_crit_norm = norm.ppf(1 - alpha / 2)\n\n        # 2. Transform null and alternative correlations to the z-scale.\n        zeta_0 = np.arctanh(rho_0)\n        # zeta is the same as above.\n\n        # 3. Calculate power using the normal approximation.\n        # The test statistic is Z = (z - zeta_0)*sqrt(n-3).\n        # We need the probability of |Z| > z_crit_norm under the alternative where z ~ N(zeta, 1/(n-3)).\n        # We standardize z from the alternative distribution to find this probability.\n        arg1_z = (zeta_0 - zeta) * sqrt_n_minus_3 + z_crit_norm\n        arg2_z = (zeta_0 - zeta) * sqrt_n_minus_3 - z_crit_norm\n        power_z = norm.sf(arg1_z) + norm.cdf(arg2_z)\n        results.append(round(power_z, 6))\n\n    # Format the final output as a single comma-separated string in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "4915697"}]}