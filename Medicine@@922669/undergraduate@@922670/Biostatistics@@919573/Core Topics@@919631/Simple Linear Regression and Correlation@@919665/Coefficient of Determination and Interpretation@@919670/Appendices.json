{"hands_on_practices": [{"introduction": "We begin by exploring a fundamental, and sometimes surprising, property of the coefficient of determination in simple linear regression. This exercise reveals that the value of $R^2$ remains the same regardless of which of two variables is designated as the predictor and which is the response [@problem_id:1955424]. Understanding this symmetry is key to grasping that in a simple two-variable context, $R^2$ functions as a pure measure of the strength of linear association, equivalent to the squared Pearson correlation coefficient, $r^2$.", "problem": "A materials scientist is studying the properties of a new polymer adhesive. For a batch of $n$ samples, two variables are measured: the curing time, $T$, in hours, and the resulting shear strength, $S$, in megapascals (MPa). It is assumed that there is some variation in both the measured curing times and shear strengths across the samples, so their sample variances are non-zero.\n\nTwo different analyses are performed on this dataset.\n\nAnalysis 1: A simple linear regression model is fitted to predict the shear strength ($S$) using the curing time ($T$) as the predictor variable. The model is of the form $S = \\beta_0 + \\beta_1 T + \\epsilon$. The coefficient of determination for this model is calculated and denoted as $R^2_{S|T}$.\n\nAnalysis 2: A second simple linear regression model is fitted to the same dataset, but this time to predict the curing time ($T$) using the shear strength ($S$) as the predictor variable. The model is of the form $T = \\gamma_0 + \\gamma_1 S + \\delta$. The coefficient of determination for this second model is calculated and denoted as $R^2_{T|S}$.\n\nHow does the value of $R^2_{S|T}$ from Analysis 1 compare to the value of $R^2_{T|S}$ from Analysis 2?\n\nA. $R^2_{S|T}$ is always greater than $R^2_{T|S}$.\n\nB. $R^2_{S|T}$ is always less than $R^2_{T|S}$.\n\nC. $R^2_{S|T}$ is always equal to $R^2_{T|S}$.\n\nD. The relationship depends on the specific values in the dataset and cannot be determined in general.\n\nE. The relationship depends on which variable, $T$ or $S$, has a larger sample variance.", "solution": "Let $(T_{i},S_{i})$, for $i=1,\\dots,n$, denote the observed pairs, and assume $\\sum_{i=1}^{n}(T_{i}-\\bar T)^{2}>0$ and $\\sum_{i=1}^{n}(S_{i}-\\bar S)^{2}>0$. Define the centered sums\n$$\nS_{TT}=\\sum_{i=1}^{n}(T_{i}-\\bar T)^{2},\\quad\nS_{SS}=\\sum_{i=1}^{n}(S_{i}-\\bar S)^{2},\\quad\nS_{ST}=\\sum_{i=1}^{n}(S_{i}-\\bar S)(T_{i}-\\bar T).\n$$\nAlso define the sample correlation\n$$\nr_{ST}=\\frac{S_{ST}}{\\sqrt{S_{SS}S_{TT}}}.\n$$\n\nAnalysis 1 (regress $S$ on $T$): The ordinary least squares slope is\n$$\n\\hat{\\beta}_{1}=\\frac{S_{ST}}{S_{TT}},\n$$\nand the fitted values are $\\hat S_{i}=\\bar S+\\hat{\\beta}_{1}(T_{i}-\\bar T)$. The explained sum of squares is\n$$\n\\mathrm{SSR}_{S|T}=\\sum_{i=1}^{n}(\\hat S_{i}-\\bar S)^{2}\n=\\hat{\\beta}_{1}^{2}\\sum_{i=1}^{n}(T_{i}-\\bar T)^{2}\n=\\hat{\\beta}_{1}^{2}S_{TT}.\n$$\nThe total sum of squares is $\\mathrm{SST}_{S}=S_{SS}$. Thus\n$$\nR^{2}_{S|T}=\\frac{\\mathrm{SSR}_{S|T}}{\\mathrm{SST}_{S}}\n=\\frac{\\hat{\\beta}_{1}^{2}S_{TT}}{S_{SS}}\n=\\frac{\\left(\\frac{S_{ST}}{S_{TT}}\\right)^{2}S_{TT}}{S_{SS}}\n=\\frac{S_{ST}^{2}}{S_{SS}S_{TT}}\n=r_{ST}^{2}.\n$$\n\nAnalysis 2 (regress $T$ on $S$): The slope is\n$$\n\\hat{\\gamma}_{1}=\\frac{S_{ST}}{S_{SS}},\n$$\nand the explained sum of squares is\n$$\n\\mathrm{SSR}_{T|S}=\\hat{\\gamma}_{1}^{2}S_{SS}.\n$$\nThe total sum of squares for $T$ is $\\mathrm{SST}_{T}=S_{TT}$. Therefore\n$$\nR^{2}_{T|S}=\\frac{\\mathrm{SSR}_{T|S}}{\\mathrm{SST}_{T}}\n=\\frac{\\hat{\\gamma}_{1}^{2}S_{SS}}{S_{TT}}\n=\\frac{\\left(\\frac{S_{ST}}{S_{SS}}\\right)^{2}S_{SS}}{S_{TT}}\n=\\frac{S_{ST}^{2}}{S_{SS}S_{TT}}\n=r_{ST}^{2}.\n$$\n\nHence both $R^{2}$ values equal the squared sample correlation between $S$ and $T$, provided both variables have nonzero sample variance and the intercept is included. Therefore,\n$$\nR^{2}_{S|T}=R^{2}_{T|S}.\n$$\nThis equality does not depend on which variable has larger variance nor on specific dataset values beyond the nonzero variance condition.", "answer": "$$\\boxed{C}$$", "id": "1955424"}, {"introduction": "A high $R^2$ value can be misleadingly reassuring, often mistaken as definitive proof that a model fits the data well. This practice directly challenges that assumption by presenting a scenario where an almost perfect $R^2$ value conceals a systematic misspecification of the model [@problem_id:1455423]. Working through this problem provides a crucial lesson in model diagnostics, emphasizing that one must always look beyond a single summary statistic and visually inspect the residuals to truly validate a model's fit.", "problem": "An undergraduate student is developing a novel analytical method for detecting nitrite ($\\text{NO}_2^âˆ’$) in water. The method involves a chemical reaction that produces a colored compound, and the student uses a smartphone camera to quantify the color intensity. To create a calibration curve, the student prepares a series of standard solutions of known nitrite concentration and measures the corresponding signal intensity. The collected data is as follows:\n\n-   Standard Concentrations ($C$, in $\\mu M$): 1.0, 5.0, 10.0, 15.0, 20.0\n-   Measured Signals ($S$, in arbitrary units): 7.0, 14.5, 23.0, 30.5, 37.0\n\nA linear regression analysis on this data yields the best-fit line equation $S = (1.57 \\times C) + 5.61$, with a coefficient of determination $R^2 = 0.9996$.\n\nDespite the very high $R^2$ value, which suggests an excellent linear fit, the student decides to examine the data more closely by plotting the residuals. The residuals are calculated as (Measured Signal - Predicted Signal). The student observes a distinct, non-random pattern in the residuals: they are slightly negative at the lowest and highest concentrations, but clearly positive for the intermediate concentrations.\n\nBased on all the information provided, which of the following statements is the most accurate conclusion about the analytical method?\n\nA. The high $R^2$ value confirms the linear model is appropriate and accurate for predicting unknown concentrations. The observed pattern in the residuals is likely due to random experimental error.\n\nB. The instrument suffers from a constant systematic error, such as an incorrectly prepared blank sample, which is the cause of the non-zero residuals.\n\nC. The pattern in the residuals suggests that the true relationship between concentration and signal is non-linear. The linear model is a poor choice for this system and should be discarded.\n\nD. The systematic pattern in the residuals indicates that while the relationship is strongly linear, it is not perfectly linear. A linear model may not be the most accurate model, and a non-linear model (e.g., quadratic) would likely provide a better fit and more accurate results.\n\nE. The student most likely made a significant error in preparing the standard solution with the largest residual, and this single point is skewing an otherwise perfect linear relationship.", "solution": "Let the linear calibration model be $S_{\\text{pred}}(C) = mC + b$ with $m = 1.57$ and $b = 5.61$, obtained from regression with $R^{2} = 0.9996$. Define the residual at concentration $C$ as\n$$\nr(C) = S_{\\text{meas}}(C) - S_{\\text{pred}}(C).\n$$\nThe observed residual pattern is $r(C) < 0$ at the lowest and highest $C$ values and $r(C) > 0$ at intermediate $C$ values. Such a systematic, non-random pattern implies model misspecification rather than random error. Specifically, the sign changes indicate curvature: the data lie below the fitted line at the extremes and above it in the middle, consistent with a concave-down relationship. A quadratic model,\n$$\nS(C) = a C^{2} + b C + c \\quad \\text{with} \\quad a  0,\n$$\nwould naturally produce $S_{\\text{true}}(C)  S_{\\text{lin}}(C)$ in the midrange and $S_{\\text{true}}(C)  S_{\\text{lin}}(C)$ at the ends, removing the systematic residual pattern.\n\nA constant systematic error (e.g., an incorrect blank) would yield approximately constant residuals $r(C) \\approx \\Delta b$ with the same sign across $C$, which is not observed. A single gross preparation error would produce one large outlying residual, not a smooth, concentration-dependent pattern spanning multiple points. Finally, a high $R^{2}$ alone does not validate the functional form; it can remain high over a limited range even when curvature is present, and residual analysis is the appropriate diagnostic for such nonlinearity.\n\nTherefore, the most accurate conclusion is that the relationship is strongly but not perfectly linear, and a non-linear model (such as a quadratic) would likely improve the fit and prediction accuracy.", "answer": "$$\\boxed{D}$$", "id": "1455423"}, {"introduction": "As we expand our models to include multiple predictors, the standard coefficient of determination, $R^2$, develops a critical flaw: it never decreases when a new variable is added, regardless of that variable's true utility. This exercise introduces the adjusted coefficient of determination, $R^2_{adj}$, which provides a more honest measure of model fit by penalizing complexity [@problem_id:4795902]. By working through its derivation and application, you will learn how this adjustment helps guard against overfitting and yields a more realistic assessment of a model's true explanatory power.", "problem": "A clinical research team fits a multiple linear regression by Ordinary Least Squares (OLS) to model baseline log-transformed C-reactive protein from routinely collected predictors in a cohort of $n=60$ adult patients. The model includes an intercept and $p=10$ explanatory variables (excluding the intercept). The coefficient of determination is reported as $R^2=0.35$. In the spirit of statistical inference and the evidence base in medicine, the investigators wish to quantify model fit in a way that accounts for finite-sample bias due to multiple predictors.\n\nStarting only from the analysis-of-variance decomposition for OLS regression and the classical unbiased variance estimators based on degrees of freedom, derive an expression for the adjusted coefficient of determination in terms of $n$, $p$, and $R^{2}$. Then, using $n=60$, $p=10$, and $R^{2}=0.35$, compute the adjusted coefficient of determination. Round your numerical answer to four significant figures and express it as a decimal. Finally, assess whether the degrees-of-freedom penalty meaningfully changes the interpretation of model fit in this medical context, focusing on whether the magnitude of change is likely to affect conclusions about explanatory strength.", "solution": "**Part 1: Derivation of the Adjusted Coefficient of Determination ($R_{adj}^2$)**\n\nThe derivation starts from the analysis-of-variance (ANOVA) decomposition for an Ordinary Least Squares (OLS) linear regression model. The total variability in the response variable is partitioned into the variability explained by the model and the unexplained (residual) variability.\n\nThe Total Sum of Squares ($SST$) measures the total variance of the dependent variable $y$. It is defined as $SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$, where $n$ is the sample size and $\\bar{y}$ is the sample mean of $y$.\n\nThe Regression Sum of Squares ($SSR$) measures the amount of variance in the dependent variable explained by the regression model. It is defined as $SSR = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2$, where $\\hat{y}_i$ are the fitted values from the model.\n\nThe Residual Sum of Squares ($SSE$), also known as the Sum of Squared Errors, measures the unexplained variance. It is defined as $SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$.\n\nThe fundamental ANOVA identity is:\n$$SST = SSR + SSE$$\n\nThe standard coefficient of determination, $R^2$, is defined as the proportion of the total variance in the dependent variable that is predictable from the independent variables.\n$$R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$$\nA known issue with $R^2$ is that it is a biased estimator of the population coefficient of determination. It never decreases when new predictors are added to the model, even if they are irrelevant. This can lead to an overly optimistic assessment of model fit, a phenomenon known as overfitting, especially when the number of predictors, $p$, is large relative to the sample size, $n$.\n\nThe adjusted coefficient of determination, $R_{adj}^2$, corrects for this by penalizing the inclusion of extraneous predictors. It does so by using unbiased estimators for the variances. The unbiased estimator of the total population variance, $\\sigma_y^2$, is the Mean Square Total ($MST$), which is the total sum of squares divided by its degrees of freedom, $df_T = n-1$.\n$$MST = \\frac{SST}{n-1}$$\nThe unbiased estimator of the error variance, $\\sigma^2$, is the Mean Square Error ($MSE$), which is the residual sum of squares divided by its degrees of freedom, $df_E$. For a multiple linear regression with $p$ predictors and an intercept, the total number of estimated coefficients is $k = p+1$. Therefore, the degrees of freedom for error are:\n$df_E = n - (p+1) = n-p-1$\nThe Mean Square Error is thus:\n$$MSE = \\frac{SSE}{n-p-1}$$\nThe adjusted $R^2$ is defined analogously to $R^2$ but uses these unbiased estimators of variance:\n$$R_{adj}^2 = 1 - \\frac{MSE}{MST} = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)}$$\nTo express $R_{adj}^2$ in terms of the original $R^2$, we can rearrange this expression:\n$$R_{adj}^2 = 1 - \\left(\\frac{SSE}{SST}\\right) \\left(\\frac{n-1}{n-p-1}\\right)$$\nFrom the definition of $R^2$, we know that $\\frac{SSE}{SST} = 1 - R^2$. Substituting this into the equation for $R_{adj}^2$ yields the desired relationship:\n$$R_{adj}^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}$$\nThis expression shows how the adjusted $R^2$ is derived from $R^2$ by applying a penalty factor, $\\frac{n-1}{n-p-1}$, which is always greater than $1$ for $p0$. This factor increases as $p$ increases or as $n$ decreases, making the adjustment more severe for models with many predictors relative to the sample size.\n\n**Part 2: Numerical Computation**\n\nThe problem provides the following values:\nSample size, $n = 60$.\nNumber of explanatory variables, $p = 10$.\nCoefficient of determination, $R^2 = 0.35$.\n\nUsing the derived formula:\n$$R_{adj}^2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1}$$\nSubstitute the given values into the expression:\n$$R_{adj}^2 = 1 - (1 - 0.35) \\frac{60-1}{60-10-1}$$\n$$R_{adj}^2 = 1 - (0.65) \\frac{59}{49}$$\nNow we compute the numerical value:\n$$R_{adj}^2 = 1 - 0.65 \\times \\left(\\frac{59}{49}\\right) \\approx 1 - 0.65 \\times 1.2040816...$$\n$$R_{adj}^2 \\approx 1 - 0.78265306...$$\n$$R_{adj}^2 \\approx 0.2173469...$$\nRounding the result to four significant figures as requested gives:\n$$R_{adj}^2 \\approx 0.2173$$\n\n**Part 3: Assessment of the Change in Interpretation**\n\nThe original coefficient of determination, $R^2 = 0.35$, suggests that the model explains $35\\%$ of the variance in the baseline log-transformed C-reactive protein. In many medical research contexts, this would be considered a modest but potentially meaningful level of explanatory power.\n\nThe adjusted coefficient of determination, $R_{adj}^2 \\approx 0.2173$, indicates that after accounting for the number of predictors used in the model relative to the sample size, the estimated proportion of variance explained is only about $21.7\\%$.\n\nThe decrease from $0.35$ to $0.2173$ represents a substantial drop. The absolute reduction is $0.35 - 0.2173 = 0.1327$. The relative reduction is $\\frac{0.1327}{0.35} \\approx 0.379$, which is a nearly $38\\%$ decrease from the original $R^2$ value.\n\nThis change is highly meaningful and would almost certainly affect conclusions about the model's explanatory strength. The relatively large number of predictors ($p=10$) for a modest sample size ($n=60$) creates significant potential for overfitting. The unadjusted $R^2$ is inflated by this fact. The adjusted $R^2$ provides a more realistic (and sobering) assessment of the model's performance on the population from which the sample was drawn.\n\nIn the evidence-based medical field, this is a critical distinction. A model explaining $35\\%$ of the variance might be seen as a promising starting point for developing a predictive tool. In contrast, a model explaining only $21.7\\%$ of the variance would be considered to have limited explanatory strength. Such a result would temper enthusiasm and suggest that the set of predictors, while perhaps statistically significant, does not constitute a powerful explanatory framework. It would prompt investigators to seek more influential predictors, consider model simplification, or conclude that the outcome is inherently difficult to predict with the available data. The penalty for model complexity has, in this case, meaningfully changed the interpretation from \"modest utility\" to \"limited utility\".", "answer": "$$\\boxed{0.2173}$$", "id": "4795902"}]}