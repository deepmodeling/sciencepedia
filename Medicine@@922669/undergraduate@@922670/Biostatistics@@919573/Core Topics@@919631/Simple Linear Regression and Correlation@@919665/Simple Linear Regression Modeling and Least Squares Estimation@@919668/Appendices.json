{"hands_on_practices": [{"introduction": "After estimating the parameters of a linear model, the next critical step is to quantify our uncertainty about these estimates. This practice guides you through the fundamental derivation of the standard errors for the Ordinary Least Squares (OLS) intercept and slope estimators. By starting from first principles and applying them to a practical biostatistical dataset, you will gain a deep understanding of the factors that influence the precision of a regression model, such as sample size and the spread of the predictor variable. [@problem_id:4952531]", "problem": "A clinical biostatistics team studies the relationship between systolic blood pressure and age. Let the response be systolic blood pressure $y_i$ in millimeters of mercury (mmHg), and the predictor be age $x_i$ in years. Assume the classical simple linear regression model with independent, identically distributed Normal errors,\n$$\nY_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\varepsilon_i,\\quad \\varepsilon_i \\sim \\text{Normal}(0,\\sigma^2),\\quad i=1,\\dots,n,\n$$\nand that parameters are estimated by Ordinary Least Squares (OLS).\n\nThe observed data for $n=8$ individuals are\n$$\n(x_i,y_i) \\in \\{(20,113),\\,(30,117),\\,(40,118.5),\\,(50,126),\\,(60,129),\\,(70,136.5),\\,(80,138),\\,(90,142)\\}.\n$$\n\nTasks:\n- Starting from the model definition and the normal equations implied by minimizing $\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1 x_i)^2$, derive the sampling variances of the OLS slope and intercept estimators under the stated assumptions, and from these derive their standard errors. Express your derivation in terms of the quantities $n$, $\\bar{x}$, and $\\sum_{i=1}^{n}(x_i-\\bar{x})^2$. Explain the contribution of each term to the magnitude of the standard errors.\n- Then, for the given dataset, compute the numerical values of the standard errors using the unbiased error variance estimator\n$$\n\\hat{\\sigma}^2 \\;=\\; \\frac{\\sum_{i=1}^{n} e_i^2}{n-2},\\quad e_i = y_i - \\hat{y}_i,\n$$\nwhere $\\hat{y}_i$ are the OLS fitted values. Report the standard error of the slope in millimeters of mercury per year and the standard error of the intercept in millimeters of mercury. Round your two numerical answers to four significant figures. Provide your final numeric answers as the ordered pair $(SE(\\hat{\\beta}_1),\\,SE(\\hat{\\beta}_0))$.", "solution": "### Part 1: Derivation of Sampling Variances and Standard Errors\n\nThe classical simple linear regression model is given by\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\nwhere the errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.) as $\\text{Normal}(0,\\sigma^2)$ for $i=1,\\dots,n$. The Ordinary Least Squares (OLS) estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are the values of $\\beta_0$ and $\\beta_1$ that minimize the sum of squared residuals, $S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2$.\n\nTo find the minimum, we take the partial derivatives of $S$ with respect to $\\beta_0$ and $\\beta_1$ and set them to zero. This gives the normal equations:\n$$ \\frac{\\partial S}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\implies \\sum_{i=1}^{n} y_i = n \\hat{\\beta}_0 + \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i $$\n$$ \\frac{\\partial S}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\implies \\sum_{i=1}^{n} x_i y_i = \\hat{\\beta}_0 \\sum_{i=1}^{n} x_i + \\hat{\\beta}_1 \\sum_{i=1}^{n} x_i^2 $$\n\nFrom the first normal equation, dividing by $n$, we get $\\bar{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}$, which gives the estimator for the intercept as a function of the slope estimator:\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n\nSubstituting this expression for $\\hat{\\beta}_0$ into the second normal equation:\n$$ \\sum x_i y_i = (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) \\sum x_i + \\hat{\\beta}_1 \\sum x_i^2 $$\n$$ \\sum x_i y_i = \\bar{y} (n\\bar{x}) - \\hat{\\beta}_1 \\bar{x} (n\\bar{x}) + \\hat{\\beta}_1 \\sum x_i^2 $$\n$$ \\sum x_i y_i - n\\bar{x}\\bar{y} = \\hat{\\beta}_1 \\left( \\sum x_i^2 - n\\bar{x}^2 \\right) $$\nUsing the notation $S_{xy} = \\sum (x_i-\\bar{x})(y_i-\\bar{y}) = \\sum x_i y_i - n\\bar{x}\\bar{y}$ and $S_{xx} = \\sum (x_i-\\bar{x})^2 = \\sum x_i^2 - n\\bar{x}^2$, we solve for $\\hat{\\beta}_1$:\n$$ \\hat{\\beta}_1 = \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum (x_i-\\bar{x})^2} = \\frac{S_{xy}}{S_{xx}} $$\n\nTo find the variance of $\\hat{\\beta}_1$, we express it as a linear combination of the random variables $Y_i$. Note that $\\sum (x_i-\\bar{x})\\bar{Y} = \\bar{Y} \\sum(x_i-\\bar{x}) = 0$.\n$$ \\hat{\\beta}_1 = \\frac{\\sum (x_i-\\bar{x})(Y_i-\\bar{Y})}{S_{xx}} = \\frac{\\sum (x_i-\\bar{x})Y_i - \\sum (x_i-\\bar{x})\\bar{Y}}{S_{xx}} = \\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}} = \\sum_{i=1}^{n} c_i Y_i $$\nwhere the constants are $c_i = \\frac{x_i-\\bar{x}}{S_{xx}}$.\nSince the $Y_i$ are independent with $\\text{Var}(Y_i) = \\sigma^2$, the variance of $\\hat{\\beta}_1$ is:\n$$ \\text{Var}(\\hat{\\beta}_1) = \\text{Var}\\left(\\sum_{i=1}^{n} c_i Y_i\\right) = \\sum_{i=1}^{n} c_i^2 \\text{Var}(Y_i) = \\sigma^2 \\sum_{i=1}^{n} c_i^2 $$\nWe calculate the sum of squared constants:\n$$ \\sum_{i=1}^{n} c_i^2 = \\sum_{i=1}^{n} \\left( \\frac{x_i-\\bar{x}}{S_{xx}} \\right)^2 = \\frac{1}{S_{xx}^2} \\sum_{i=1}^{n} (x_i-\\bar{x})^2 = \\frac{S_{xx}}{S_{xx}^2} = \\frac{1}{S_{xx}} $$\nThus, the sampling variance of the slope estimator is:\n$$ \\text{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n\nNext, we find the variance of $\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x}$.\n$$ \\text{Var}(\\hat{\\beta}_0) = \\text{Var}(\\bar{Y} - \\hat{\\beta}_1\\bar{x}) = \\text{Var}(\\bar{Y}) + \\bar{x}^2 \\text{Var}(\\hat{\\beta}_1) - 2\\bar{x} \\text{Cov}(\\bar{Y}, \\hat{\\beta}_1) $$\nWe have $\\text{Var}(\\bar{Y}) = \\text{Var}(\\frac{1}{n}\\sum Y_i) = \\frac{1}{n^2} \\sum \\text{Var}(Y_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$.\nThe covariance term is:\n$$ \\text{Cov}(\\bar{Y}, \\hat{\\beta}_1) = \\text{Cov}\\left(\\sum_{i=1}^n \\frac{1}{n} Y_i, \\sum_{j=1}^n c_j Y_j\\right) = \\sum_{i=1}^n \\frac{1}{n} c_i \\text{Var}(Y_i) = \\frac{\\sigma^2}{n} \\sum_{i=1}^n c_i $$\nsince $\\text{Cov}(Y_i, Y_j)=0$ for $i \\neq j$. The sum of the constants $c_i$ is:\n$$ \\sum_{i=1}^n c_i = \\sum_{i=1}^n \\frac{x_i-\\bar{x}}{S_{xx}} = \\frac{1}{S_{xx}} \\sum_{i=1}^n (x_i-\\bar{x}) = 0 $$\nTherefore, $\\text{Cov}(\\bar{Y}, \\hat{\\beta}_1) = 0$. The slope estimator $\\hat{\\beta}_1$ is uncorrelated with the sample mean of the response $\\bar{Y}$.\nSubstituting these results back into the variance formula for $\\hat{\\beta}_0$:\n$$ \\text{Var}(\\hat{\\beta}_0) = \\frac{\\sigma^2}{n} + \\bar{x}^2 \\frac{\\sigma^2}{\\sum (x_i-\\bar{x})^2} = \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\right) $$\n\nThe standard error ($SE$) of an estimator is the square root of its estimated sampling variance. The variance $\\sigma^2$ is unknown and is estimated by the unbiased estimator $\\hat{\\sigma}^2 = \\frac{\\sum e_i^2}{n-2}$, where $e_i = y_i - \\hat{y}_i$ are the residuals.\nThe standard error of the slope estimator is:\n$$ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2}} = \\frac{\\hat{\\sigma}}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}} $$\nThe standard error of the intercept estimator is:\n$$ SE(\\hat{\\beta}_0) = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\right)} = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2}} $$\n\nInterpretation of terms:\n- For both $SE(\\hat{\\beta}_0)$ and $SE(\\hat{\\beta}_1)$, the standard error is directly proportional to $\\hat{\\sigma}$, the estimated standard deviation of the error term. Larger residual variability about the regression line implies greater uncertainty in the parameter estimates.\n- For $SE(\\hat{\\beta}_1)$, the term $\\sum (x_i-\\bar{x})^2$ is in the denominator. This term measures the spread or dispersion of the predictor values $x_i$. A larger spread in $x_i$ provides a more stable \"lever arm\" for estimating the slope, thus reducing its standard error.\n- For $SE(\\hat{\\beta}_0)$, the term $\\frac{1}{n}$ reflects uncertainty in estimating the mean response level; this uncertainty decreases as sample size $n$ increases. The term $\\frac{\\bar{x}^2}{\\sum (x_i-\\bar{x})^2}$ reflects the uncertainty from extrapolating the line from the center of the data ($\\bar{x}$) to the y-axis ($x=0$). This extrapolation error is larger when $\\bar{x}$ is far from $0$ and smaller when the spread of $x_i$ is large.\n\n### Part 2: Numerical Computation for the Given Dataset\n\nFirst, we compute the necessary summary statistics from the data, where $n=8$.\n$$ x_i: \\{20, 30, 40, 50, 60, 70, 80, 90\\} $$\n$$ y_i: \\{113, 117, 118.5, 126, 129, 136.5, 138, 142\\} $$\n\n$$ \\sum_{i=1}^8 x_i = 440 \\implies \\bar{x} = \\frac{440}{8} = 55 \\text{ years} $$\n$$ \\sum_{i=1}^8 y_i = 1020 \\implies \\bar{y} = \\frac{1020}{8} = 127.5 \\text{ mmHg} $$\n\nNext, we compute the sums of squares:\n$$ S_{xx} = \\sum_{i=1}^8 (x_i - \\bar{x})^2 = \\sum x_i^2 - n\\bar{x}^2 = (20^2 + \\dots + 90^2) - 8(55^2) = 28400 - 8(3025) = 28400 - 24200 = 4200 $$\n$$ S_{xy} = \\sum_{i=1}^8 (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum x_i y_i - n\\bar{x}\\bar{y} = (20 \\cdot 113 + \\dots + 90 \\cdot 142) - 8(55)(127.5) = 57925 - 56100 = 1825 $$\n$$ S_{yy} = \\sum_{i=1}^8 (y_i - \\bar{y})^2 = \\sum y_i^2 - n\\bar{y}^2 = (113^2 + \\dots + 142^2) - 8(127.5^2) = 130857.5 - 8(16256.25) = 130857.5 - 130050 = 807.5 $$\n\nNow, calculate the OLS estimates:\n$$ \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{1825}{4200} = \\frac{73}{168} \\approx 0.43452 $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = 127.5 - \\left(\\frac{1825}{4200}\\right)(55) \\approx 103.601 $$\n\nNext, compute the sum of squared errors ($SSE$) and the unbiased estimator for the error variance, $\\hat{\\sigma}^2$:\n$$ SSE = \\sum e_i^2 = S_{yy} - \\frac{S_{xy}^2}{S_{xx}} = 807.5 - \\frac{(1825)^2}{4200} = 807.5 - \\frac{3330625}{4200} \\approx 807.5 - 792.9940476 = 14.5059524 $$\nUsing fractions for precision:\n$$ SSE = \\frac{1615}{2} - \\frac{133225}{168} = \\frac{135660 - 133225}{168} = \\frac{2435}{168} $$\nThe unbiased variance estimator is:\n$$ \\hat{\\sigma}^2 = \\frac{SSE}{n-2} = \\frac{2435/168}{8-2} = \\frac{2435}{168 \\times 6} = \\frac{2435}{1008} \\approx 2.41567 $$\n\nFinally, we compute the standard errors:\nFor the slope, $\\hat{\\beta}_1$:\n$$ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{2435/1008}{4200}} = \\sqrt{\\frac{2435}{1008 \\times 4200}} = \\sqrt{\\frac{2435}{4233600}} \\approx 0.0239825 $$\nRounding to four significant figures, $SE(\\hat{\\beta}_1) \\approx 0.02398$ mmHg/year.\n\nFor the intercept, $\\hat{\\beta}_0$:\n$$ SE(\\hat{\\beta}_0) = \\sqrt{\\hat{\\sigma}^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)} = \\sqrt{\\frac{2435}{1008} \\left(\\frac{1}{8} + \\frac{55^2}{4200}\\right)} $$\n$$ SE(\\hat{\\beta}_0) = \\sqrt{\\frac{2435}{1008} \\left(\\frac{1}{8} + \\frac{3025}{4200}\\right)} = \\sqrt{\\frac{2435}{1008} \\left(\\frac{525+3025}{4200}\\right)} = \\sqrt{\\frac{2435}{1008} \\left(\\frac{3550}{4200}\\right)} = \\sqrt{\\frac{2435}{1008} \\left(\\frac{71}{84}\\right)} $$\n$$ SE(\\hat{\\beta}_0) = \\sqrt{\\frac{172885}{84672}} \\approx \\sqrt{2.04185} \\approx 1.428933 $$\nRounding to four significant figures, $SE(\\hat{\\beta}_0) \\approx 1.429$ mmHg.\n\nThe required ordered pair is $(SE(\\hat{\\beta}_1), SE(\\hat{\\beta}_0))$.", "answer": "$$ \\boxed{\\begin{pmatrix} 0.02398 & 1.429 \\end{pmatrix}} $$", "id": "4952531"}, {"introduction": "The interpretability and stability of regression parameters can often be improved through simple transformations of the predictor variable. This exercise explores the powerful technique of centering, where the predictor is shifted by a constant. You will demonstrate that while the slope estimate remains unchanged, the intercept takes on a new, more meaningful interpretation and that its estimation variance can be minimized, a key principle for building robust and interpretable models. [@problem_id:4952529]", "problem": "A biostatistics study investigates a linear relationship between a continuous biomarker response $Y$ and a continuous exposure $x$ across $n$ individuals. Assume the simple linear regression model\n$$\nY_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\varepsilon_i,\\quad i=1,\\dots,n,\n$$\nwith $\\varepsilon_i$ independent and identically distributed (i.i.d.) with $\\mathbb{E}[\\varepsilon_i]=0$ and $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2$, and let $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i$ and $S_{xx}=\\sum_{i=1}^n (x_i-\\bar{x})^2$. Consider refitting the model after centering the predictor at an arbitrary constant $c\\in\\mathbb{R}$ by defining $z_i=x_i-c$ and estimating\n$$\nY_i \\;=\\; \\alpha_c \\;+\\; \\beta_1^{(c)} z_i \\;+\\; \\varepsilon_i.\n$$\nStarting only from the least squares definitions and the stated assumptions, derive how the refitted intercept $\\hat{\\alpha}_c$ and slope $\\hat{\\beta}_1^{(c)}$ relate to the original least squares estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, and explain the interpretation of $\\hat{\\alpha}_c$ in terms of the conditional mean $\\mathbb{E}[Y\\mid x=c]$. Then, obtain an explicit expression for $\\operatorname{Var}(\\hat{\\alpha}_c)$ as a function of $c$, $\\bar{x}$, $S_{xx}$, $n$, and $\\sigma^2$, and determine the centering value $c$ that minimizes $\\operatorname{Var}(\\hat{\\alpha}_c)$ over all real $c$.\n\nYour final answer must be the single analytic expression for the value of $c$ that minimizes $\\operatorname{Var}(\\hat{\\alpha}_c)$. No numerical rounding is required, and no units should be included in your final answer.", "solution": "The first step is to state the ordinary least squares (OLS) estimators for the parameters of the original model, $Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. The OLS estimators, denoted $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, are the values that minimize the sum of squared residuals, $S(\\beta_0, \\beta_1) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 x_i)^2$. The standard solutions, derived from the normal equations $\\frac{\\partial S}{\\partial \\beta_0}=0$ and $\\frac{\\partial S}{\\partial \\beta_1}=0$, are:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\n$$\n$$\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x}\n$$\nwhere $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ and $S_{xy} = \\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})$.\n\nNext, we consider the refitted model, $Y_i = \\alpha_c + \\beta_1^{(c)} z_i + \\varepsilon_i$, where $z_i = x_i - c$. The OLS estimators for this model, $\\hat{\\alpha}_c$ and $\\hat{\\beta}_1^{(c)}$, are found by minimizing $S(\\alpha_c, \\beta_1^{(c)}) = \\sum_{i=1}^n (Y_i - \\alpha_c - \\beta_1^{(c)} z_i)^2$. Following the same procedure, the estimators are:\n$$\n\\hat{\\beta}_1^{(c)} = \\frac{\\sum_{i=1}^n (z_i - \\bar{z})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (z_i - \\bar{z})^2}\n$$\n$$\n\\hat{\\alpha}_c = \\bar{Y} - \\hat{\\beta}_1^{(c)} \\bar{z}\n$$\nwhere $\\bar{z} = \\frac{1}{n} \\sum_{i=1}^n z_i = \\frac{1}{n} \\sum_{i=1}^n (x_i - c) = (\\frac{1}{n} \\sum_{i=1}^n x_i) - c = \\bar{x} - c$.\n\nTo relate these estimators, we note that the deviation term $z_i - \\bar{z}$ is $(x_i - c) - (\\bar{x} - c) = x_i - \\bar{x}$. Substituting this into the expression for $\\hat{\\beta}_1^{(c)}$:\n$$\n\\hat{\\beta}_1^{(c)} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\hat{\\beta}_1\n$$\nThis demonstrates that the least squares estimator for the slope coefficient is invariant to a location shift of the predictor variable.\n\nNow, we find the relationship for the intercept estimator $\\hat{\\alpha}_c$:\n$$\n\\hat{\\alpha}_c = \\bar{Y} - \\hat{\\beta}_1^{(c)} \\bar{z} = \\bar{Y} - \\hat{\\beta}_1 (\\bar{x} - c) = (\\bar{Y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 c\n$$\nRecognizing that $\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x}$, we have:\n$$\n\\hat{\\alpha}_c = \\hat{\\beta}_0 + \\hat{\\beta}_1 c\n$$\nThe intercept of the refitted model is a linear function of the original intercept, the original slope, and the centering constant $c$.\n\nThe interpretation of $\\hat{\\alpha}_c$ follows from its definition. In the model $Y_i = \\alpha_c + \\beta_1^{(c)} z_i + \\varepsilon_i$, the intercept $\\hat{\\alpha}_c$ is the predicted value of the response $Y$ when the predictor $z$ is zero. The condition $z=0$ is equivalent to $x-c=0$, or $x=c$. Thus, $\\hat{\\alpha}_c$ is the predicted value of $Y$ at $x=c$. This can be seen from the original fitted regression line, $\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$. Evaluating this at $x=c$ gives $\\hat{Y}|_{x=c} = \\hat{\\beta}_0 + \\hat{\\beta}_1 c$, which is precisely our derived expression for $\\hat{\\alpha}_c$. Therefore, $\\hat{\\alpha}_c$ is the estimator for the conditional mean of $Y$ given $x=c$, i.e., $\\hat{\\alpha}_c = \\hat{\\mathbb{E}}[Y \\mid x=c]$.\n\nTo find the variance of $\\hat{\\alpha}_c$, we use the expression $\\hat{\\alpha}_c = \\hat{\\beta}_0 + c \\hat{\\beta}_1$. The variance is:\n$$\n\\operatorname{Var}(\\hat{\\alpha}_c) = \\operatorname{Var}(\\hat{\\beta}_0 + c \\hat{\\beta}_1) = \\operatorname{Var}(\\hat{\\beta}_0) + c^2 \\operatorname{Var}(\\hat{\\beta}_1) + 2c \\operatorname{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1)\n$$\nWe need the variances and covariance of the OLS estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. Treating the predictors $x_i$ as fixed constants, the estimators are linear combinations of the random variables $Y_i$.\n$\\hat{\\beta}_1 = \\sum_{i=1}^n k_i Y_i$ where $k_i = \\frac{x_i - \\bar{x}}{S_{xx}}$.\n$$\n\\operatorname{Var}(\\hat{\\beta}_1) = \\operatorname{Var}\\left(\\sum_{i=1}^n k_i Y_i\\right) = \\sum_{i=1}^n k_i^2 \\operatorname{Var}(Y_i) = \\sum_{i=1}^n k_i^2 \\sigma^2 = \\sigma^2 \\sum_{i=1}^n \\left(\\frac{x_i - \\bar{x}}{S_{xx}}\\right)^2 = \\frac{\\sigma^2}{S_{xx}^2} \\sum_{i=1}^n (x_i - \\bar{x})^2 = \\frac{\\sigma^2 S_{xx}}{S_{xx}^2} = \\frac{\\sigma^2}{S_{xx}}\n$$\n$\\hat{\\beta}_0 = \\bar{Y} - \\bar{x}\\hat{\\beta}_1$.\nThe covariance between $\\bar{Y}$ and $\\hat{\\beta}_1$ is:\n$$\n\\operatorname{Cov}(\\bar{Y}, \\hat{\\beta}_1) = \\operatorname{Cov}\\left(\\frac{1}{n}\\sum_{j=1}^n Y_j, \\sum_{i=1}^n k_i Y_i\\right) = \\frac{1}{n} \\sum_{i=1}^n k_i \\operatorname{Var}(Y_i) = \\frac{\\sigma^2}{n} \\sum_{i=1}^n k_i = \\frac{\\sigma^2}{n} \\frac{\\sum_{i=1}^n(x_i - \\bar{x})}{S_{xx}} = 0\n$$\nSince $\\operatorname{Cov}(\\bar{Y}, \\hat{\\beta}_1) = 0$, $\\bar{Y}$ and $\\hat{\\beta}_1$ are uncorrelated. This simplifies the remaining calculations.\n$$\n\\operatorname{Var}(\\hat{\\beta}_0) = \\operatorname{Var}(\\bar{Y} - \\bar{x}\\hat{\\beta}_1) = \\operatorname{Var}(\\bar{Y}) + \\bar{x}^2 \\operatorname{Var}(\\hat{\\beta}_1) - 2\\bar{x}\\operatorname{Cov}(\\bar{Y}, \\hat{\\beta}_1) = \\frac{\\sigma^2}{n} + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}} - 0 = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\n$$\nAnd the covariance is:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\operatorname{Cov}(\\bar{Y} - \\bar{x}\\hat{\\beta}_1, \\hat{\\beta}_1) = \\operatorname{Cov}(\\bar{Y}, \\hat{\\beta}_1) - \\bar{x}\\operatorname{Var}(\\hat{\\beta}_1) = 0 - \\bar{x}\\frac{\\sigma^2}{S_{xx}} = -\\frac{\\sigma^2 \\bar{x}}{S_{xx}}\n$$\nSubstituting these expressions into the formula for $\\operatorname{Var}(\\hat{\\alpha}_c)$:\n$$\n\\operatorname{Var}(\\hat{\\alpha}_c) = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) + c^2 \\left(\\frac{\\sigma^2}{S_{xx}}\\right) + 2c \\left(-\\frac{\\sigma^2 \\bar{x}}{S_{xx}}\\right)\n$$\n$$\n\\operatorname{Var}(\\hat{\\alpha}_c) = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{2c\\bar{x}}{S_{xx}} + \\frac{c^2}{S_{xx}} \\right]\n$$\n$$\n\\operatorname{Var}(\\hat{\\alpha}_c) = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{c^2 - 2c\\bar{x} + \\bar{x}^2}{S_{xx}} \\right]\n$$\nThis simplifies to the explicit expression:\n$$\n\\operatorname{Var}(\\hat{\\alpha}_c) = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{(c - \\bar{x})^2}{S_{xx}} \\right]\n$$\nFinally, to find the value of $c$ that minimizes $\\operatorname{Var}(\\hat{\\alpha}_c)$, we analyze this function of $c$. The terms $\\sigma^2$, $n$, and $S_{xx}$ are all positive constants (assuming $S_{xx} > 0$, i.e., not all $x_i$ are identical, which is a necessary condition for fitting the slope). The first term, $\\sigma^2/n$, is constant with respect to $c$. The second term, $\\sigma^2 \\frac{(c - \\bar{x})^2}{S_{xx}}$, depends on $c$. Since $(c - \\bar{x})^2 \\ge 0$, this term is non-negative and its minimum value is $0$. This minimum is achieved when $c - \\bar{x} = 0$, which implies $c = \\bar{x}$.\nAlternatively, we can use calculus. Let $f(c) = \\operatorname{Var}(\\hat{\\alpha}_c)$. We find the derivative with respect to $c$:\n$$\n\\frac{d}{dc}f(c) = \\frac{d}{dc} \\left( \\sigma^2 \\left[ \\frac{1}{n} + \\frac{(c - \\bar{x})^2}{S_{xx}} \\right] \\right) = \\frac{\\sigma^2}{S_{xx}} \\cdot 2(c-\\bar{x})\n$$\nSetting the derivative to zero to find the critical point(s):\n$$\n\\frac{2\\sigma^2}{S_{xx}}(c-\\bar{x}) = 0\n$$\nThis implies $c - \\bar{x} = 0$, so $c = \\bar{x}$. The second derivative is $\\frac{d^2}{dc^2}f(c) = \\frac{2\\sigma^2}{S_{xx}} > 0$, which confirms that $c=\\bar{x}$ corresponds to a minimum.\nTherefore, the variance of the estimated intercept is minimized when the predictor variable is centered at its sample mean, $\\bar{x}$.", "answer": "$$\\boxed{\\bar{x}}$$", "id": "4952529"}, {"introduction": "Averages can be misleading, and in regression, the overall trend can be disproportionately affected by a few unusual data points. This practice provides a striking, hands-on demonstration of how a single high-leverage observation can reverse the direction of a relationship, highlighting the importance of regression diagnostics. By calculating key metrics like leverage and Cook's Distance, you will learn to identify and quantify the influence of individual data points, a crucial skill for ensuring your model's conclusions are robust and reliable. [@problem_id:4952475]", "problem": "A biostatistician is modeling the relationship between standardized daily sodium intake and standardized systolic blood pressure using simple linear regression with an intercept. Let the model be $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$, where $Y$ is the standardized systolic blood pressure, $X$ is the standardized daily sodium intake (z-score), and $\\varepsilon$ is a random error term with mean $0$ and constant variance. The biostatistician begins with data from $5$ subjects:\n- Subject $1$: $(x_{1}, y_{1}) = (-1, -1)$\n- Subject $2$: $(x_{2}, y_{2}) = \\left(-\\frac{1}{2}, -\\frac{1}{2}\\right)$\n- Subject $3$: $(x_{3}, y_{3}) = (0, 0)$\n- Subject $4$: $(x_{4}, y_{4}) = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$\n- Subject $5$: $(x_{5}, y_{5}) = (1, 1)$\n\nShe fits the simple linear regression model to these $5$ subjects and records the slope estimate $\\hat{\\beta}_{1}^{(5)}$.\n\nShe then adds a $6$-th subject with an extreme sodium intake but a low blood pressure:\n- Subject $6$: $(x_{6}, y_{6}) = (4, -2)$\n\nShe refits the model to all $6$ subjects and records the new slope $\\hat{\\beta}_{1}^{(6)}$. Using the standard definitions of the hat matrix and Cook’s Distance (Cook’s $D$), compute the leverage $h_{66}$ of Subject $6$ under the $6$-subject fit and Cook’s Distance $D_{6}$ that quantifies the influence of Subject $6$.\n\nProvide exact values (no rounding) and express your final answer as a row matrix containing, in order, $\\hat{\\beta}_{1}^{(5)}$, $\\hat{\\beta}_{1}^{(6)}$, $h_{66}$, and $D_{6}$.", "solution": "The task is to compute four quantities: the regression slope for the initial $5$ subjects, $\\hat{\\beta}_{1}^{(5)}$; the slope for all $6$ subjects, $\\hat{\\beta}_{1}^{(6)}$; the leverage of the $6$-th subject, $h_{66}$; and the Cook's Distance for the $6$-th subject, $D_{6}$.\n\n**Part 1: Calculation of the slope for the initial 5 subjects, $\\hat{\\beta}_{1}^{(5)}$**\n\nThe simple linear regression model is $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$. The least squares estimate for the slope, $\\hat{\\beta}_{1}$, is given by the formula:\n$$ \\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\nFor the first $n=5$ subjects, the data are $(x_1, y_1) = (-1, -1)$, $(x_2, y_2) = (-\\frac{1}{2}, -\\frac{1}{2})$, $(x_3, y_3) = (0, 0)$, $(x_4, y_4) = (\\frac{1}{2}, \\frac{1}{2})$, and $(x_5, y_5) = (1, 1)$.\n\nFirst, we compute the sample means $\\bar{x}^{(5)}$ and $\\bar{y}^{(5)}$:\n$$ \\bar{x}^{(5)} = \\frac{1}{5} \\sum_{i=1}^{5} x_i = \\frac{1}{5} \\left( -1 - \\frac{1}{2} + 0 + \\frac{1}{2} + 1 \\right) = \\frac{0}{5} = 0 $$\n$$ \\bar{y}^{(5)} = \\frac{1}{5} \\sum_{i=1}^{5} y_i = \\frac{1}{5} \\left( -1 - \\frac{1}{2} + 0 + \\frac{1}{2} + 1 \\right) = \\frac{0}{5} = 0 $$\nSince the means are zero, the formula for the slope estimate simplifies to:\n$$ \\hat{\\beta}_{1}^{(5)} = \\frac{\\sum_{i=1}^{5} x_i y_i}{\\sum_{i=1}^{5} x_i^2} $$\nWe calculate the necessary sums:\n$$ \\sum_{i=1}^{5} x_i y_i = (-1)(-1) + \\left(-\\frac{1}{2}\\right)\\left(-\\frac{1}{2}\\right) + (0)(0) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + (1)(1) = 1 + \\frac{1}{4} + 0 + \\frac{1}{4} + 1 = \\frac{5}{2} $$\n$$ \\sum_{i=1}^{5} x_i^2 = (-1)^2 + \\left(-\\frac{1}{2}\\right)^2 + 0^2 + \\left(\\frac{1}{2}\\right)^2 + 1^2 = 1 + \\frac{1}{4} + 0 + \\frac{1}{4} + 1 = \\frac{5}{2} $$\nThus, the slope is:\n$$ \\hat{\\beta}_{1}^{(5)} = \\frac{5/2}{5/2} = 1 $$\n\n**Part 2: Calculation of the slope for all 6 subjects, $\\hat{\\beta}_{1}^{(6)}$**\n\nNow we add the $6$-th subject, $(x_6, y_6) = (4, -2)$. The dataset has $n=6$ points. We compute the new sample means:\n$$ \\bar{x}^{(6)} = \\frac{1}{6} \\sum_{i=1}^{6} x_i = \\frac{1}{6} \\left( (\\sum_{i=1}^{5} x_i) + x_6 \\right) = \\frac{1}{6}(0 + 4) = \\frac{4}{6} = \\frac{2}{3} $$\n$$ \\bar{y}^{(6)} = \\frac{1}{6} \\sum_{i=1}^{6} y_i = \\frac{1}{6} \\left( (\\sum_{i=1}^{5} y_i) + y_6 \\right) = \\frac{1}{6}(0 - 2) = -\\frac{2}{6} = -\\frac{1}{3} $$\nWe need the terms $\\sum_{i=1}^{6} (x_i - \\bar{x}^{(6)})(y_i - \\bar{y}^{(6)})$ and $\\sum_{i=1}^{6} (x_i - \\bar{x}^{(6)})^2$. We can use the computational formula $\\sum(x_i-\\bar{x})(y_i-\\bar{y}) = \\sum x_iy_i - n\\bar{x}\\bar{y}$.\n$$ \\sum_{i=1}^{6} x_i y_i = \\left( \\sum_{i=1}^{5} x_i y_i \\right) + x_6 y_6 = \\frac{5}{2} + (4)(-2) = \\frac{5}{2} - 8 = -\\frac{11}{2} $$\nThe numerator for $\\hat{\\beta}_{1}^{(6)}$ is:\n$$ \\sum_{i=1}^{6} (x_i - \\bar{x}^{(6)})(y_i - \\bar{y}^{(6)}) = \\sum_{i=1}^{6} x_i y_i - 6 \\bar{x}^{(6)} \\bar{y}^{(6)} = -\\frac{11}{2} - 6 \\left(\\frac{2}{3}\\right) \\left(-\\frac{1}{3}\\right) = -\\frac{11}{2} + \\frac{12}{9} = -\\frac{11}{2} + \\frac{4}{3} = \\frac{-33+8}{6} = -\\frac{25}{6} $$\nFor the denominator, we use $\\sum(x_i-\\bar{x})^2 = \\sum x_i^2 - n\\bar{x}^2$:\n$$ \\sum_{i=1}^{6} x_i^2 = \\left( \\sum_{i=1}^{5} x_i^2 \\right) + x_6^2 = \\frac{5}{2} + 4^2 = \\frac{5}{2} + 16 = \\frac{37}{2} $$\nThe denominator for $\\hat{\\beta}_{1}^{(6)}$ is:\n$$ \\sum_{i=1}^{6} (x_i - \\bar{x}^{(6)})^2 = \\sum_{i=1}^{6} x_i^2 - 6 (\\bar{x}^{(6)})^2 = \\frac{37}{2} - 6 \\left(\\frac{2}{3}\\right)^2 = \\frac{37}{2} - 6 \\left(\\frac{4}{9}\\right) = \\frac{37}{2} - \\frac{8}{3} = \\frac{111 - 16}{6} = \\frac{95}{6} $$\nThe new slope is:\n$$ \\hat{\\beta}_{1}^{(6)} = \\frac{-25/6}{95/6} = -\\frac{25}{95} = -\\frac{5}{19} $$\n\n**Part 3: Calculation of the leverage of Subject 6, $h_{66}$**\n\nThe leverage $h_{ii}$ of the $i$-th data point measures its influence on the fitted values. For simple linear regression, it is given by:\n$$ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2} $$\nWe need to calculate this for the $6$-th subject ($i=6$) in the $n=6$ dataset.\nUsing the values calculated in Part 2:\n- $n = 6$\n- $x_6 = 4$\n- $\\bar{x}^{(6)} = \\frac{2}{3}$\n- $\\sum_{j=1}^{6} (x_j - \\bar{x}^{(6)})^2 = \\frac{95}{6}$\nFirst, we find $(x_6 - \\bar{x}^{(6)})^2$:\n$$ (x_6 - \\bar{x}^{(6)})^2 = \\left(4 - \\frac{2}{3}\\right)^2 = \\left(\\frac{12-2}{3}\\right)^2 = \\left(\\frac{10}{3}\\right)^2 = \\frac{100}{9} $$\nNow we compute $h_{66}$:\n$$ h_{66} = \\frac{1}{6} + \\frac{100/9}{95/6} = \\frac{1}{6} + \\frac{100}{9} \\cdot \\frac{6}{95} = \\frac{1}{6} + \\frac{100 \\cdot 2}{3 \\cdot 95} = \\frac{1}{6} + \\frac{200}{285} $$\nSimplifying the second term by dividing numerator and denominator by $5$: $\\frac{200}{285} = \\frac{40}{57}$.\n$$ h_{66} = \\frac{1}{6} + \\frac{40}{57} = \\frac{19}{114} + \\frac{80}{114} = \\frac{99}{114} $$\nSimplifying the fraction by dividing by $3$:\n$$ h_{66} = \\frac{33}{38} $$\n\n**Part 4: Calculation of Cook's Distance for Subject 6, $D_6$**\n\nCook's Distance $D_i$ measures the effect of deleting the $i$-th observation on the regression coefficients. A standard formula is:\n$$ D_i = \\frac{e_i^2}{p \\cdot \\text{MSE}} \\frac{h_{ii}}{(1-h_{ii})^2} $$\nwhere $e_i$ is the residual for the $i$-th point, $p$ is the number of estimated parameters ($p=2$ for $\\beta_0, \\beta_1$), and MSE is the Mean Squared Error.\n\nFirst, we need the fitted model for $n=6$ to compute the residual $e_6$. The intercept estimate is:\n$$ \\hat{\\beta}_{0}^{(6)} = \\bar{y}^{(6)} - \\hat{\\beta}_{1}^{(6)} \\bar{x}^{(6)} = -\\frac{1}{3} - \\left(-\\frac{5}{19}\\right)\\left(\\frac{2}{3}\\right) = -\\frac{1}{3} + \\frac{10}{57} = \\frac{-19+10}{57} = -\\frac{9}{57} = -\\frac{3}{19} $$\nThe fitted regression line is $\\hat{y} = -\\frac{3}{19} - \\frac{5}{19}x$.\nThe fitted value for $x_6=4$ is:\n$$ \\hat{y}_6 = -\\frac{3}{19} - \\frac{5}{19}(4) = -\\frac{3}{19} - \\frac{20}{19} = -\\frac{23}{19} $$\nThe residual for the $6$-th subject is:\n$$ e_6 = y_6 - \\hat{y}_6 = -2 - \\left(-\\frac{23}{19}\\right) = \\frac{-38+23}{19} = -\\frac{15}{19} $$\nNext, we calculate the Sum of Squared Errors (SSE) and the Mean Squared Error (MSE).\n$$ \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{6} (y_i - \\bar{y}^{(6)})^2 - (\\hat{\\beta}_{1}^{(6)})^2 \\sum_{i=1}^{6} (x_i - \\bar{x}^{(6)})^2 $$\nWe need $\\sum_{i=1}^{6} (y_i - \\bar{y}^{(6)})^2 = \\sum y_i^2 - n(\\bar{y}^{(6)})^2$.\n$$ \\sum_{i=1}^{6} y_i^2 = \\left(\\sum_{i=1}^{5} y_i^2\\right) + y_6^2 = \\left( (-1)^2 + (-\\frac{1}{2})^2 + 0^2 + (\\frac{1}{2})^2 + 1^2 \\right) + (-2)^2 = \\left(1 + \\frac{1}{4} + 0 + \\frac{1}{4} + 1\\right) + 4 = \\frac{5}{2} + 4 = \\frac{13}{2} $$\n$$ \\sum_{i=1}^{6} (y_i - \\bar{y}^{(6)})^2 = \\frac{13}{2} - 6\\left(-\\frac{1}{3}\\right)^2 = \\frac{13}{2} - 6\\left(\\frac{1}{9}\\right) = \\frac{13}{2} - \\frac{2}{3} = \\frac{39-4}{6} = \\frac{35}{6} $$\nNow, calculate SSE using an alternative formulation for precision:\n$$ \\text{SSE} = \\sum_{i=1}^{6} (y_i - \\bar{y}^{(6)})^2 - \\hat{\\beta}_{1}^{(6)} \\sum_{i=1}^{6} (x_i - \\bar{x}^{(6)})(y_i - \\bar{y}^{(6)}) = \\frac{35}{6} - \\left(-\\frac{5}{19}\\right) \\left(-\\frac{25}{6}\\right) = \\frac{35}{6} - \\frac{125}{114} = \\frac{35 \\times 19}{114} - \\frac{125}{114} = \\frac{665 - 125}{114} = \\frac{540}{114} = \\frac{90}{19} $$\nThe MSE is SSE divided by the degrees of freedom ($n-p = 6-2 = 4$):\n$$ \\text{MSE} = \\frac{\\text{SSE}}{n-p} = \\frac{90/19}{4} = \\frac{90}{76} = \\frac{45}{38} $$\nWe have all components for $D_6$:\n- $e_6^2 = \\left(-\\frac{15}{19}\\right)^2 = \\frac{225}{361}$\n- $p=2$\n- $\\text{MSE} = \\frac{45}{38}$\n- $h_{66} = \\frac{33}{38}$\n- $1-h_{66} = 1 - \\frac{33}{38} = \\frac{5}{38}$\n- $(1-h_{66})^2 = \\left(\\frac{5}{38}\\right)^2 = \\frac{25}{1444}$\n\nSubstituting these into the Cook's Distance formula:\n$$ D_6 = \\frac{e_6^2}{p \\cdot \\text{MSE}} \\frac{h_{66}}{(1-h_{66})^2} = \\frac{\\frac{225}{361}}{2 \\cdot \\frac{45}{38}} \\cdot \\frac{\\frac{33}{38}}{\\frac{25}{1444}} $$\nLet's evaluate the parts. Note that $361 = 19^2$, $38=2 \\times 19$, and $1444=38^2=(2 \\times 19)^2$.\n$$ D_6 = \\frac{\\frac{225}{19^2}}{\\frac{90}{38}} \\cdot \\frac{\\frac{33}{38}}{\\frac{25}{38^2}} = \\left(\\frac{225}{19^2} \\cdot \\frac{38}{90}\\right) \\cdot \\left(\\frac{33}{38} \\cdot \\frac{38^2}{25}\\right) $$\n$$ D_6 = \\left(\\frac{225}{19^2} \\cdot \\frac{2 \\cdot 19}{90}\\right) \\cdot \\left(\\frac{33 \\cdot 38}{25}\\right) = \\left(\\frac{225 \\cdot 2}{19 \\cdot 90}\\right) \\cdot \\left(\\frac{33 \\cdot 38}{25}\\right) $$\nSince $225 = 2.5 \\times 90$, the first parenthesis becomes $\\frac{2.5 \\times 2}{19} = \\frac{5}{19}$.\n$$ D_6 = \\frac{5}{19} \\cdot \\frac{33 \\cdot 38}{25} = \\frac{5}{19} \\cdot \\frac{33 \\cdot (2 \\cdot 19)}{25} = \\frac{5 \\cdot 33 \\cdot 2}{25} = \\frac{10 \\cdot 33}{25} = \\frac{2 \\cdot 33}{5} = \\frac{66}{5} $$\n\n**Summary of Results**\nThe four required values are:\n1. $\\hat{\\beta}_{1}^{(5)} = 1$\n2. $\\hat{\\beta}_{1}^{(6)} = -\\frac{5}{19}$\n3. $h_{66} = \\frac{33}{38}$\n4. $D_6 = \\frac{66}{5}$\nThese will be presented in a row matrix as requested.", "answer": "$$ \\boxed{ \\begin{pmatrix} 1 & -\\frac{5}{19} & \\frac{33}{38} & \\frac{66}{5} \\end{pmatrix} } $$", "id": "4952475"}]}