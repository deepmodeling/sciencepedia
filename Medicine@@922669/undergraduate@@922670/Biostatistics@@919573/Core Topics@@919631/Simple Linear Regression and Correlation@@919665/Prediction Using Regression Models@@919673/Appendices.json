{"hands_on_practices": [{"introduction": "To truly master predictive modeling, it's essential to look under the hood of the statistical software and understand the core mechanics. This first practice exercise guides you through the fundamental steps of Ordinary Least Squares (OLS) from the ground up. By manually calculating a prediction and its associated standard error for a multiple regression model, you will gain a concrete understanding of how predictor variables and their relationships influence the final prediction and its uncertainty [@problem_id:4940014].", "problem": "A biostatistical study investigates systolic blood pressure (in $\\mathrm{mm}\\,\\mathrm{Hg}$) as a function of patient age and body mass index. Assume the classical linear model with independent homoscedastic Gaussian errors: for $i \\in \\{1,2,3,4,5\\}$, the response $y_i$ satisfies $y_i = \\beta_0 + \\beta_1 A_i + \\beta_2 B_i + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, and $A_i$ and $B_i$ are mean-centered covariates defined by $A_i = \\text{age}_i - 50$ (in years) and $B_i = \\text{BMI}_i - 27$ (in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$). The design matrix $X$ has rows $x_i^{\\top} = (1, A_i, B_i)$ and the response vector is $y = (y_1,y_2,y_3,y_4,y_5)^{\\top}$. The observed dataset is:\n- $(A_1,B_1,y_1) = (-10,-1,120)$,\n- $(A_2,B_2,y_2) = (-5,2,132)$,\n- $(A_3,B_3,y_3) = (0,0,127)$,\n- $(A_4,B_4,y_4) = (5,-2,131)$,\n- $(A_5,B_5,y_5) = (10,1,140)$.\nA new patient has $\\text{age} = 52$ and $\\text{BMI} = 29$, yielding the covariate vector $x_*^{\\top} = (1, 2, 2)$ under the same centering.\n\nUsing the ordinary least squares (OLS) framework under the stated assumptions, compute the point prediction for the conditional mean systolic blood pressure at $x_*$ and the standard error of this predicted mean. Round both quantities to four significant figures. Express both numerical results in $\\mathrm{mm}\\,\\mathrm{Hg}$.", "solution": "The user wants me to solve a statistical estimation problem. I will first perform the required validation.\n\n### Step 1: Extract Givens\n- **Model:** The classical linear model $y_i = \\beta_0 + \\beta_1 A_i + \\beta_2 B_i + \\varepsilon_i$ for $i \\in \\{1, 2, 3, 4, 5\\}$.\n- **Error Term:** $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, independent and homoscedastic.\n- **Covariates:** $A_i = \\text{age}_i - 50$ (in years) and $B_i = \\text{BMI}_i - 27$ (in $\\mathrm{kg}\\,\\mathrm{m}^{-2}$). These are mean-centered.\n- **Design Matrix:** Rows are $x_i^{\\top} = (1, A_i, B_i)$.\n- **Response Vector:** $y = (y_1, y_2, y_3, y_4, y_5)^{\\top}$.\n- **Dataset:** \n  - $(A_1, B_1, y_1) = (-10, -1, 120)$\n  - $(A_2, B_2, y_2) = (-5, 2, 132)$\n  - $(A_3, B_3, y_3) = (0, 0, 127)$\n  - $(A_4, B_4, y_4) = (5, -2, 131)$\n  - $(A_5, B_5, y_5) = (10, 1, 140)$\n- **New Covariate Vector:** $x_*^{\\top} = (1, 2, 2)$, corresponding to $\\text{age} = 52$ and $\\text{BMI} = 29$.\n- **Task:** Compute the point prediction for the conditional mean systolic blood pressure at $x_*$ and its standard error.\n- **Rounding:** Round both quantities to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard application of the ordinary least squares (OLS) linear regression model, a fundamental method in biostatistics. All necessary data and model assumptions are explicitly provided, and there are no internal contradictions. The number of data points ($n=5$) exceeds the number of parameters ($p=3$), so the model is not underdetermined. The scientific context (blood pressure vs. age and BMI) is realistic. The problem is formalizable and solvable using established statistical principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\nThe linear model is given by $y = X\\beta + \\varepsilon$, where $y$ is the response vector, $X$ is the design matrix, $\\beta$ is the vector of parameters, and $\\varepsilon$ is the vector of errors.\nThe design matrix $X$ and the response vector $y$ are constructed from the given data:\n$$\nX = \\begin{pmatrix}\n1  A_1  B_1 \\\\\n1  A_2  B_2 \\\\\n1  A_3  B_3 \\\\\n1  A_4  B_4 \\\\\n1  A_5  B_5\n\\end{pmatrix} = \\begin{pmatrix}\n1  -10  -1 \\\\\n1  -5  2 \\\\\n1  0  0 \\\\\n1  5  -2 \\\\\n1  10  1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n120 \\\\\n132 \\\\\n127 \\\\\n131 \\\\\n140\n\\end{pmatrix}\n$$\nThe OLS estimator for $\\beta = (\\beta_0, \\beta_1, \\beta_2)^{\\top}$ is $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$.\nFirst, we compute $X^{\\top}X$:\n$$\nX^{\\top}X = \\begin{pmatrix}\n\\sum_{i=1}^5 1  \\sum_{i=1}^5 A_i  \\sum_{i=1}^5 B_i \\\\\n\\sum_{i=1}^5 A_i  \\sum_{i=1}^5 A_i^2  \\sum_{i=1}^5 A_i B_i \\\\\n\\sum_{i=1}^5 B_i  \\sum_{i=1}^5 A_i B_i  \\sum_{i=1}^5 B_i^2\n\\end{pmatrix}\n$$\nThe sums are:\n$\\sum A_i = -10 - 5 + 0 + 5 + 10 = 0$\n$\\sum B_i = -1 + 2 + 0 - 2 + 1 = 0$\n$\\sum A_i^2 = (-10)^2 + (-5)^2 + 0^2 + 5^2 + 10^2 = 100 + 25 + 0 + 25 + 100 = 250$\n$\\sum B_i^2 = (-1)^2 + 2^2 + 0^2 + (-2)^2 + 1^2 = 1 + 4 + 0 + 4 + 1 = 10$\n$\\sum A_i B_i = (-10)(-1) + (-5)(2) + (0)(0) + (5)(-2) + (10)(1) = 10 - 10 + 0 - 10 + 10 = 0$\nThe fact that the off-diagonal terms involving covariates are zero indicates an orthogonal design.\n$$\nX^{\\top}X = \\begin{pmatrix}\n5  0  0 \\\\\n0  250  0 \\\\\n0  0  10\n\\end{pmatrix}\n$$\nThe inverse is straightforward to compute:\n$$\n(X^{\\top}X)^{-1} = \\begin{pmatrix}\n\\frac{1}{5}  0  0 \\\\\n0  \\frac{1}{250}  0 \\\\\n0  0  \\frac{1}{10}\n\\end{pmatrix} = \\begin{pmatrix}\n0.2  0  0 \\\\\n0  0.004  0 \\\\\n0  0  0.1\n\\end{pmatrix}\n$$\nNext, we compute $X^{\\top}y$:\n$$\nX^{\\top}y = \\begin{pmatrix}\n\\sum y_i \\\\\n\\sum A_i y_i \\\\\n\\sum B_i y_i\n\\end{pmatrix}\n$$\n$\\sum y_i = 120 + 132 + 127 + 131 + 140 = 650$\n$\\sum A_i y_i = (-10)(120) + (-5)(132) + (0)(127) + (5)(131) + (10)(140) = -1200 - 660 + 0 + 655 + 1400 = 195$\n$\\sum B_i y_i = (-1)(120) + (2)(132) + (0)(127) + (-2)(131) + (1)(140) = -120 + 264 + 0 - 262 + 140 = 22$\nSo,\n$$\nX^{\\top}y = \\begin{pmatrix}\n650 \\\\\n195 \\\\\n22\n\\end{pmatrix}\n$$\nNow we can compute the OLS estimates $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = (X^{\\top}X)^{-1}X^{\\top}y = \\begin{pmatrix}\n0.2  0  0 \\\\\n0  0.004  0 \\\\\n0  0  0.1\n\\end{pmatrix} \\begin{pmatrix}\n650 \\\\\n195 \\\\\n22\n\\end{pmatrix} = \\begin{pmatrix}\n0.2 \\times 650 \\\\\n0.004 \\times 195 \\\\\n0.1 \\times 22\n\\end{pmatrix} = \\begin{pmatrix}\n130 \\\\\n0.78 \\\\\n2.2\n\\end{pmatrix}\n$$\nThe point prediction for the conditional mean blood pressure at a new covariate vector $x_*$ is $\\hat{y}_* = x_*^{\\top}\\hat{\\beta}$. With $x_*^{\\top} = (1, 2, 2)$:\n$$\n\\hat{y}_* = \\begin{pmatrix} 1  2  2 \\end{pmatrix} \\begin{pmatrix} 130 \\\\ 0.78 \\\\ 2.2 \\end{pmatrix} = 1(130) + 2(0.78) + 2(2.2) = 130 + 1.56 + 4.4 = 135.96\n$$\nThe standard error of this predicted mean is given by $s.e.(\\hat{y}_*) = \\sqrt{\\widehat{\\text{Var}}(\\hat{y}_*)} = \\sqrt{\\hat{\\sigma}^2 x_*^{\\top}(X^{\\top}X)^{-1}x_*}$.\nFirst, we need an unbiased estimate of the error variance, $\\sigma^2$. This is $\\hat{\\sigma}^2 = \\frac{SSE}{n-p}$, where $SSE$ is the sum of squared errors, $n$ is the number of observations ($5$), and $p$ is the number of parameters ($3$). The degrees of freedom for the error is $n-p = 5-3=2$.\n$SSE = \\sum (y_i - \\hat{y}_i)^2 = y^{\\top}y - \\hat{\\beta}^{\\top}X^{\\top}y$.\n$y^{\\top}y = 120^2 + 132^2 + 127^2 + 131^2 + 140^2 = 14400 + 17424 + 16129 + 17161 + 19600 = 84714$.\n$\\hat{\\beta}^{\\top}X^{\\top}y = \\begin{pmatrix} 130  0.78  2.2 \\end{pmatrix} \\begin{pmatrix} 650 \\\\ 195 \\\\ 22 \\end{pmatrix} = (130)(650) + (0.78)(195) + (2.2)(22) = 84500 + 152.1 + 48.4 = 84700.5$.\n$SSE = 84714 - 84700.5 = 13.5$.\nSo, $\\hat{\\sigma}^2 = \\frac{13.5}{2} = 6.75$.\nNext, we compute the term $x_*^{\\top}(X^{\\top}X)^{-1}x_*$:\n$$\nx_*^{\\top}(X^{\\top}X)^{-1}x_* = \\begin{pmatrix} 1  2  2 \\end{pmatrix} \\begin{pmatrix}\n0.2  0  0 \\\\\n0  0.004  0 \\\\\n0  0  0.1\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 1  2  2 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ 0.008 \\\\ 0.2 \\end{pmatrix} = 1(0.2) + 2(0.008) + 2(0.2) = 0.2 + 0.016 + 0.4 = 0.616\n$$\nNow we can compute the estimated variance of the predicted mean:\n$$\n\\widehat{\\text{Var}}(\\hat{y}_*) = \\hat{\\sigma}^2 x_*^{\\top}(X^{\\top}X)^{-1}x_* = (6.75)(0.616) = 4.158\n$$\nThe standard error is the square root of this value:\n$$\ns.e.(\\hat{y}_*) = \\sqrt{4.158} \\approx 2.03911745...\n$$\nThe problem requires rounding both quantities to four significant figures.\nPoint prediction: $\\hat{y}_* = 135.96 \\approx 136.0$.\nStandard error: $s.e.(\\hat{y}_*)\n\\approx 2.039$.\nBoth quantities are in units of $\\mathrm{mm}\\,\\mathrm{Hg}$.\n\nThe final results are:\n- Point prediction for the conditional mean systolic blood pressure: $136.0\\,\\mathrm{mm}\\,\\mathrm{Hg}$.\n- Standard error of this predicted mean: $2.039\\,\\mathrm{mm}\\,\\mathrm{Hg}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 136.0  2.039 \\end{pmatrix}}\n$$", "id": "4940014"}, {"introduction": "A point prediction provides a single best guess, but the real power of a statistical model lies in quantifying the uncertainty around that guess. This exercise addresses a critical, and often confusing, distinction: the difference between a confidence interval for a *mean response* and a prediction interval for a *single new observation*. By computing both intervals for the same scenario, you will discover why a prediction interval is necessarily wider and develop a robust intuition for the two sources of error that regression models must account for [@problem_id:4940038].", "problem": "A biostatistics team models fasting blood glucose concentration, measured in milligrams per deciliter (mg/dL), as a linear function of Body Mass Index (BMI). Let the response be $y$ and the predictor be $x$; the model is $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$, where the errors $\\varepsilon_{i}$ are independent and identically distributed Gaussian with mean $0$ and constant variance $\\sigma^{2}$. The clinical dataset has $n = 10$ adults. The least-squares fit yielded an intercept estimate $\\hat{\\beta}_{0} = 60.0$ (mg/dL), a slope estimate $\\hat{\\beta}_{1} = 0.90$ (mg/dL per BMI unit), and a residual standard error $s = 6.0$ (mg/dL). The predictor summaries are $\\sum_{i=1}^{n} x_{i} = 268$ and $\\sum_{i=1}^{n} x_{i}^{2} = 7354$. Consider predicting at $x_{*} = 29$. For a two-sided $95$ confidence level with $n - 2 = 8$ degrees of freedom, the Student’s $t$ critical value is $t_{0.975,8} = 2.306$.\n\nUsing the normal-error linear regression framework and the provided design summaries, compute the two-sided $95$ confidence interval for the mean fasting blood glucose at $x_{*}$ and the two-sided $95$ predictive interval for an individual fasting blood glucose outcome at $x_{*}$. Then, compute the ratio of their total widths (predictive interval width divided by confidence interval width) at $x_{*} = 29$. Round your final ratio to four significant figures. Express your final answer as a dimensionless number.", "solution": "The user wants to compute the ratio of the total widths of a $95\\%$ prediction interval and a $95\\%$ confidence interval for a simple linear regression model.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- **Model**: Simple linear regression, $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$.\n- **Error Distribution**: $\\varepsilon_{i} \\sim \\text{i.i.d. } N(0, \\sigma^2)$.\n- **Sample Size**: $n = 10$.\n- **Least-Squares Estimates**: $\\hat{\\beta}_{0} = 60.0$, $\\hat{\\beta}_{1} = 0.90$.\n- **Residual Standard Error**: $s = 6.0$.\n- **Predictor Summaries**: $\\sum_{i=1}^{n} x_{i} = 268$, $\\sum_{i=1}^{n} x_{i}^{2} = 7354$.\n- **Prediction Point**: $x_{*} = 29$.\n- **Confidence Level**: $95\\%$, two-sided.\n- **Degrees of Freedom**: $n - 2 = 8$.\n- **Critical Value**: $t_{0.975,8} = 2.306$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem uses the standard normal-error simple linear regression framework, a fundamental topic in biostatistics and statistics in general. The context (BMI and blood glucose) is a well-established area of medical research. The given values are plausible. The problem is scientifically grounded.\n- **Well-Posedness**: The problem provides all necessary information to compute the requested intervals and their ratio. The task is clearly defined.\n- **Objectivity**: The problem is stated using precise, objective, and standard statistical terminology.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, self-contained, and well-posed. It is deemed **valid**. I will proceed with the solution.\n\nThe solution requires calculating a confidence interval for the mean response and a prediction interval for an individual response, and then finding the ratio of their widths. Let $\\alpha = 0.05$ for a $95\\%$ confidence level.\n\n**1. Preliminary Calculations**\nFirst, we compute the sample mean of the predictor, $\\bar{x}$, and the sum of squared deviations, $S_{xx}$.\nThe sample mean is:\n$$ \\bar{x} = \\frac{\\sum_{i=1}^{n} x_{i}}{n} = \\frac{268}{10} = 26.8 $$\nThe sum of squared deviations is:\n$$ S_{xx} = \\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2} = \\sum_{i=1}^{n} x_{i}^{2} - n \\bar{x}^{2} $$\nSubstituting the given values:\n$$ S_{xx} = 7354 - 10 \\cdot (26.8)^{2} = 7354 - 10 \\cdot 718.24 = 7354 - 7182.4 = 171.6 $$\nThe predicted value of the response at $x_{*} = 29$ is:\n$$ \\hat{y}_{*} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{*} = 60.0 + 0.90 \\cdot 29 = 60.0 + 26.1 = 86.1 $$\n\n**2. Confidence Interval for the Mean Response**\nThe $100(1-\\alpha)\\%$ confidence interval for the mean response $E(y_{*}) = \\beta_{0} + \\beta_{1} x_{*}$ is given by:\n$$ \\hat{y}_{*} \\pm t_{1-\\alpha/2, n-2} \\cdot s \\cdot \\sqrt{\\frac{1}{n} + \\frac{(x_{*} - \\bar{x})^{2}}{S_{xx}}} $$\nThe standard error of the mean response, $SE(\\hat{y}_{*})$, is:\n$$ SE(\\hat{y}_{*}) = s \\cdot \\sqrt{\\frac{1}{n} + \\frac{(x_{*} - \\bar{x})^{2}}{S_{xx}}} $$\nLet's compute the term under the square root:\n$$ \\frac{1}{n} + \\frac{(x_{*} - \\bar{x})^{2}}{S_{xx}} = \\frac{1}{10} + \\frac{(29 - 26.8)^{2}}{171.6} = 0.1 + \\frac{(2.2)^{2}}{171.6} = 0.1 + \\frac{4.84}{171.6} $$\nTo work with exact fractions, we write:\n$$ \\frac{4.84}{171.6} = \\frac{484}{17160} = \\frac{121}{4290} = \\frac{11}{390} $$\nSo, the term is:\n$$ \\frac{1}{10} + \\frac{11}{390} = \\frac{39}{390} + \\frac{11}{390} = \\frac{50}{390} = \\frac{5}{39} $$\nNow we can compute the standard error:\n$$ SE(\\hat{y}_{*}) = 6.0 \\cdot \\sqrt{\\frac{5}{39}} $$\nThe margin of error for the confidence interval is:\n$$ ME_{CI} = t_{0.975, 8} \\cdot SE(\\hat{y}_{*}) = 2.306 \\cdot 6.0 \\cdot \\sqrt{\\frac{5}{39}} = 13.836 \\sqrt{\\frac{5}{39}} $$\nNumerically, $ME_{CI} \\approx 13.836 \\cdot 0.358057 \\approx 4.9546$.\nThe $95\\%$ confidence interval for the mean glucose at $x_{*} = 29$ is:\n$$ CI = 86.1 \\pm 4.9546 \\implies [81.1454, 91.0546] $$\nThe total width of the confidence interval is $W_{CI} = 2 \\cdot ME_{CI} = 2 \\cdot 13.836 \\sqrt{\\frac{5}{39}}$.\n\n**3. Prediction Interval for an Individual Response**\nThe $100(1-\\alpha)\\%$ prediction interval for a new individual observation $y_{*}$ is given by:\n$$ \\hat{y}_{*} \\pm t_{1-\\alpha/2, n-2} \\cdot s \\cdot \\sqrt{1 + \\frac{1}{n} + \\frac{(x_{*} - \\bar{x})^{2}}{S_{xx}}} $$\nThe standard error of prediction, $SE(\\text{pred})$, is:\n$$ SE(\\text{pred}) = s \\cdot \\sqrt{1 + \\frac{1}{n} + \\frac{(x_{*} - \\bar{x})^{2}}{S_{xx}}} $$\nUsing our previous calculation:\n$$ SE(\\text{pred}) = 6.0 \\cdot \\sqrt{1 + \\frac{5}{39}} = 6.0 \\cdot \\sqrt{\\frac{39+5}{39}} = 6.0 \\cdot \\sqrt{\\frac{44}{39}} $$\nThe margin of error for the prediction interval is:\n$$ ME_{PI} = t_{0.975, 8} \\cdot SE(\\text{pred}) = 2.306 \\cdot 6.0 \\cdot \\sqrt{\\frac{44}{39}} = 13.836 \\sqrt{\\frac{44}{39}} $$\nNumerically, $ME_{PI} \\approx 13.836 \\cdot 1.06217 \\approx 14.7002$.\nThe $95\\%$ prediction interval for an individual glucose outcome at $x_{*} = 29$ is:\n$$ PI = 86.1 \\pm 14.7002 \\implies [71.3998, 100.8002] $$\nThe total width of the prediction interval is $W_{PI} = 2 \\cdot ME_{PI} = 2 \\cdot 13.836 \\sqrt{\\frac{44}{39}}$.\n\n**4. Ratio of the Interval Widths**\nThe problem asks for the ratio of the total widths, predictive interval width divided by confidence interval width.\n$$ R = \\frac{W_{PI}}{W_{CI}} = \\frac{2 \\cdot t_{1-\\alpha/2, n-2} \\cdot s \\cdot \\sqrt{1 + \\frac{1}{n} + \\frac{(x_{*} - \\bar{x})^{2}}{S_{xx}}}}{2 \\cdot t_{1-\\alpha/2, n-2} \\cdot s \\cdot \\sqrt{\\frac{1}{n} + \\frac{(x_{*} - \\bar{x})^{2}}{S_{xx}}}} $$\nThe common terms cancel out, simplifying the ratio to:\n$$ R = \\frac{\\sqrt{1 + \\frac{5}{39}}}{\\sqrt{\\frac{5}{39}}} = \\frac{\\sqrt{\\frac{44}{39}}}{\\sqrt{\\frac{5}{39}}} = \\sqrt{\\frac{44/39}{5/39}} = \\sqrt{\\frac{44}{5}} $$\nNow, we compute the numerical value:\n$$ R = \\sqrt{8.8} \\approx 2.96647939... $$\nRounding the result to four significant figures, we get $2.966$. This is a dimensionless number, as expected from a ratio of two quantities with the same units.", "answer": "$$\\boxed{2.966}$$", "id": "4940038"}, {"introduction": "Real-world datasets rarely conform to textbook ideals; a common challenge is multicollinearity, where predictors are highly correlated. This can destabilize Ordinary Least Squares estimates, leading to unreliable predictions. This practice explores how ridge regression provides a powerful solution by introducing a penalty term that stabilizes the model [@problem_id:4940018]. Through both conceptual explanation and direct calculation, you will see how this technique trades a small amount of bias for a large reduction in variance, resulting in a more robust and trustworthy predictive model.", "problem": "A biostatistician is modeling a continuous inflammation index $y$ using two laboratory biomarkers $x_{1}$ and $x_{2}$ that are highly collinear. The predictors have been centered to mean $0$ and scaled to unit variance across $n=50$ patients, so the design matrix $X \\in \\mathbb{R}^{50 \\times 2}$ has standardized columns. You are given the sufficient cross-product summaries based on these $50$ patients:\n- The predictor cross-product matrix is\n$$\nX^{\\top}X=\\begin{pmatrix}\n50  49.95\\\\\n49.95  50\n\\end{pmatrix},\n$$\nwhich indicates near-collinearity of $x_{1}$ and $x_{2}$.\n- The predictor–outcome cross-product vector is\n$$\nX^{\\top}y=\\begin{pmatrix}\n5\\\\\n-5\n\\end{pmatrix}.\n$$\n\nUsing only core definitions of least squares and penalized least squares as your starting point, do the following:\n1. Explain, from first principles, why severe collinearity inflates the variability of linear predictions produced by the Ordinary Least Squares (OLS) estimator and how ridge regression mitigates this problem. Your explanation should use the eigendecomposition point of view on $X^{\\top}X$ and must explicitly relate the effect of the penalty to the directions associated with small eigenvalues.\n2. For a new patient with standardized biomarker values $x_{0}=\\begin{pmatrix}1.00\\\\0.98\\end{pmatrix}$, compute:\n   - The OLS prediction $\\hat{y}_{\\text{OLS}}=x_{0}^{\\top}\\hat{\\beta}$.\n   - The ridge prediction $\\hat{y}_{\\lambda}=x_{0}^{\\top}\\hat{\\beta}_{\\lambda}$ with penalty parameter $\\lambda=10$, where $\\hat{\\beta}_{\\lambda}$ minimizes the penalized residual sum of squares with an $\\ell_{2}$ penalty.\n3. Report only the ridge-predicted value $\\hat{y}_{\\lambda}$ for the new patient. Round your answer to four significant figures. Express the final numerical value in arbitrary standardized units (a.u.).", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem in the field of biostatistics, specifically concerning prediction with regression models. The provided information is self-contained, numerically consistent, and sufficient for a complete solution. The task requires demonstrating a fundamental understanding of the effects of multicollinearity on Ordinary Least Squares (OLS) regression and the mechanism by which ridge regression provides a robust alternative, followed by a direct application of these principles.\n\nThe solution proceeds in two parts, per the problem's request. First, an explanation from first principles is provided, followed by the specific calculations.\n\nPart 1: Theoretical Explanation of Collinearity, OLS, and Ridge Regression\n\nThe standard linear model is given by $y = X\\beta + \\epsilon$, where $y$ is the response vector, $X$ is the design matrix, $\\beta$ is the vector of coefficients, and $\\epsilon$ is a vector of random errors with mean $0$ and variance $\\sigma^2 I$.\n\nThe OLS estimator, $\\hat{\\beta}$, is the vector that minimizes the residual sum of squares, $S(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta)$. This minimization leads to the normal equations, $(X^{\\top}X)\\hat{\\beta} = X^{\\top}y$, which yield the solution:\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\nThe covariance matrix of this estimator is given by $\\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\sigma^2(X^{\\top}X)^{-1}$.\n\nTo understand the effect of collinearity, we use the eigendecomposition of the symmetric, positive semi-definite matrix $X^{\\top}X$. Let $X^{\\top}X = VDV^{\\top}$, where $V$ is an orthogonal matrix whose columns $v_j$ are the eigenvectors of $X^{\\top}X$, and $D$ is a diagonal matrix with the corresponding non-negative eigenvalues $d_j$ on its diagonal. The inverse is then $(X^{\\top}X)^{-1} = V D^{-1} V^{\\top}$, where $D^{-1}$ is a diagonal matrix with elements $1/d_j$.\n\nSevere collinearity between predictors means that some columns of $X$ are nearly linearly dependent. This implies that the matrix $X^{\\top}X$ is nearly singular, which in turn means that at least one of its eigenvalues, say $d_k$, is very close to zero ($d_k \\approx 0$).\n\nThe variance of the OLS estimator is directly affected by these small eigenvalues. The variance of the estimated coefficient projected onto the direction of an eigenvector $v_k$ is $\\text{Var}(v_k^{\\top}\\hat{\\beta}_{\\text{OLS}}) = v_k^{\\top} \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) v_k = \\sigma^2 v_k^{\\top} (V D^{-1} V^{\\top}) v_k = \\sigma^2 (1/d_k)$. As $d_k \\to 0$, this variance term approaches infinity. This \"variance inflation\" means that the OLS estimates are highly unstable; small changes in the input data can lead to massive swings in the coefficient estimates. This instability translates to poor predictive performance, as the prediction for a new data point $x_0$, given by $\\hat{y}_{\\text{OLS}} = x_0^{\\top}\\hat{\\beta}_{\\text{OLS}}$, will also have high variance.\n\nRidge regression mitigates this problem by introducing a penalty term to the minimization objective. The ridge estimator, $\\hat{\\beta}_{\\lambda}$, minimizes the penalized residual sum of squares:\n$$\nS_{\\lambda}(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) + \\lambda \\beta^{\\top}\\beta\n$$\nwhere $\\lambda > 0$ is the penalty parameter, and the penalty is on the squared $\\ell_2$-norm of the coefficient vector. The solution to this minimization problem is:\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y\n$$\nUsing the eigendecomposition of $X^{\\top}X$, the inverse term becomes:\n$$\n(X^{\\top}X + \\lambda I)^{-1} = (VDV^{\\top} + \\lambda VIV^{\\top})^{-1} = (V(D + \\lambda I)V^{\\top})^{-1} = V(D + \\lambda I)^{-1}V^{\\top}\n$$\nThe matrix $(D + \\lambda I)$ is a diagonal matrix with elements $d_j + \\lambda$. Its inverse, $(D + \\lambda I)^{-1}$, is a diagonal matrix with elements $1/(d_j + \\lambda)$.\n\nThe crucial difference is that even if an eigenvalue $d_k$ is nearly zero, the corresponding term in the inverse, $1/(d_k + \\lambda)$, is bounded by $1/\\lambda$. The penalty parameter $\\lambda$ ensures that all the eigenvalues of the matrix to be inverted are shifted away from zero. This stabilizes the inverse, thus controlling the variance of the coefficient estimates. The directions in the parameter space corresponding to small eigenvalues (the directions of high collinearity) are shrunk the most significantly, as the addition of $\\lambda$ has the largest relative effect on small $d_j$. This introduces a small bias into the estimates (since $\\hat{\\beta}_{\\lambda}$ is no longer an unbiased estimator for $\\beta$) but drastically reduces the variance, leading to a more stable model with better predictive performance, a classic example of the bias-variance tradeoff.\n\nPart 2: Numerical Calculation\n\nWe are given the following:\n$$\nX^{\\top}X = \\begin{pmatrix} 50  49.95 \\\\ 49.95  50 \\end{pmatrix}, \\quad X^{\\top}y = \\begin{pmatrix} 5 \\\\ -5 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 1.00 \\\\ 0.98 \\end{pmatrix}, \\quad \\lambda=10\n$$\n\nFirst, we compute the OLS prediction. We need the inverse of $X^{\\top}X$. The determinant is $\\det(X^{\\top}X) = (50)(50) - (49.95)(49.95) = 2500 - 2495.0025 = 4.9975$.\nThe inverse is:\n$$\n(X^{\\top}X)^{-1} = \\frac{1}{4.9975} \\begin{pmatrix} 50  -49.95 \\\\ -49.95  50 \\end{pmatrix}\n$$\nThe OLS coefficient vector is:\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y = \\frac{1}{4.9975} \\begin{pmatrix} 50  -49.95 \\\\ -49.95  50 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -5 \\end{pmatrix} = \\frac{1}{4.9975} \\begin{pmatrix} 50(5) + (-49.95)(-5) \\\\ -49.95(5) + 50(-5) \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}_{\\text{OLS}} = \\frac{1}{4.9975} \\begin{pmatrix} 250 + 249.75 \\\\ -249.75 - 250 \\end{pmatrix} = \\frac{1}{4.9975} \\begin{pmatrix} 499.75 \\\\ -499.75 \\end{pmatrix} = \\begin{pmatrix} 100 \\\\ -100 \\end{pmatrix}\n$$\nThe OLS prediction for the new patient is:\n$$\n\\hat{y}_{\\text{OLS}} = x_0^{\\top}\\hat{\\beta}_{\\text{OLS}} = \\begin{pmatrix} 1.00  0.98 \\end{pmatrix} \\begin{pmatrix} 100 \\\\ -100 \\end{pmatrix} = 1.00(100) + 0.98(-100) = 100 - 98 = 2\n$$\n\nNext, we compute the ridge prediction with $\\lambda = 10$. First, we compute the matrix $(X^{\\top}X + \\lambda I)$:\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 50  49.95 \\\\ 49.95  50 \\end{pmatrix} + \\begin{pmatrix} 10  0 \\\\ 0  10 \\end{pmatrix} = \\begin{pmatrix} 60  49.95 \\\\ 49.95  60 \\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(X^{\\top}X + \\lambda I) = (60)(60) - (49.95)(49.95) = 3600 - 2495.0025 = 1104.9975$.\nThe inverse is:\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\frac{1}{1104.9975} \\begin{pmatrix} 60  -49.95 \\\\ -49.95  60 \\end{pmatrix}\n$$\nThe ridge coefficient vector is:\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y = \\frac{1}{1104.9975} \\begin{pmatrix} 60  -49.95 \\\\ -49.95  60 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -5 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}_{\\lambda} = \\frac{1}{1104.9975} \\begin{pmatrix} 60(5) + (-49.95)(-5) \\\\ -49.95(5) + 60(-5) \\end{pmatrix} = \\frac{1}{1104.9975} \\begin{pmatrix} 300 + 249.75 \\\\ -249.75 - 300 \\end{pmatrix} = \\frac{1}{1104.9975} \\begin{pmatrix} 549.75 \\\\ -549.75 \\end{pmatrix}\n$$\nThe ridge prediction for the new patient is:\n$$\n\\hat{y}_{\\lambda} = x_0^{\\top}\\hat{\\beta}_{\\lambda} = \\begin{pmatrix} 1.00  0.98 \\end{pmatrix} \\frac{1}{1104.9975} \\begin{pmatrix} 549.75 \\\\ -549.75 \\end{pmatrix}\n$$\n$$\n\\hat{y}_{\\lambda} = \\frac{1}{1104.9975} \\left( 1.00(549.75) + 0.98(-549.75) \\right) = \\frac{549.75}{1104.9975} (1.00 - 0.98)\n$$\n$$\n\\hat{y}_{\\lambda} = \\frac{549.75}{1104.9975} (0.02) = \\frac{10.995}{1104.9975} \\approx 0.009950248...\n$$\nThe problem requires this value to be rounded to four significant figures. The first four significant figures are $9, 9, 5, 0$. The fifth significant figure is $2$, which is less than $5$, so we round down (i.e., keep the fourth figure as is).\nThe rounded value is $0.009950$.", "answer": "$$\n\\boxed{0.009950}\n$$", "id": "4940018"}]}