{"hands_on_practices": [{"introduction": "The coefficient of determination, $R^2$, is a cornerstone metric for evaluating how well a regression model fits the data. This first exercise takes you back to basics, asking you to compute $R^2$ from a small dataset. By working through the calculation for a log-transformed linear model—a common approach in biological studies—you will solidify your understanding of how total variability is partitioned into explained and unexplained components [@problem_id:4914681].", "problem": "A biostatistics study investigates how a serum biomarker $y$ varies with drug dose $x$ when the relationship is multiplicative. The scientific premise is that multiplicative error in $y$ is stabilized by the natural logarithm, and the transformed relationship between the biomarker and dose is modeled linearly. Specifically, the working model is\n$$\\ln(y_i) = a + b \\ln(x_i) + e_i,$$\nwhere $a$ and $b$ are unknown parameters and $e_i$ represents zero-mean random error on the log scale. Ordinary Least Squares (OLS) is used to fit the line on the transformed scale.\n\nA set of $n=5$ paired observations was collected under exponentially increasing doses chosen to span a wide dynamic range. For each subject $i$, both $\\ln(x_i)$ and $\\ln(y_i)$ were recorded as follows (ordered by increasing $\\ln(x_i)$):\n$$(\\ln(x_i), \\ln(y_i)) \\in \\{(0, 0.45), (1, 1.25), (2, 2.20), (3, 3.05), (4, 3.95)\\}.$$\n\nStarting from the core definitions of least squares fitting and the variance decomposition for the transformed response, compute the coefficient of determination $R^2$ for this fitted linear model on the log-transformed scale. Express your final answer as a pure number (no units), and round your answer to four significant figures.", "solution": "The problem statement is evaluated and found to be valid. It is a well-posed, scientifically grounded problem in biostatistics that provides a complete and consistent set of data and a clear objective. The model specified, which involves a logarithmic transformation to linearize a power-law relationship and stabilize variance, is a standard and respected technique in statistical modeling. All necessary information is provided to compute the requested quantity.\n\nThe problem asks for the coefficient of determination, denoted as $R^2$, for a linear model fitted to log-transformed data. The model is given by:\n$$ \\ln(y_i) = a + b \\ln(x_i) + e_i $$\nFor clarity and convenience in the calculations that follow, let us define the transformed variables as $X_i' = \\ln(x_i)$ and $Y_i' = \\ln(y_i)$. The model is then a simple linear regression model:\n$$ Y_i' = a + b X_i' + e_i $$\nThe dataset consists of $n=5$ paired observations $(X_i', Y_i')$:\n$$ (X_i', Y_i') \\in \\{(0, 0.45), (1, 1.25), (2, 2.20), (3, 3.05), (4, 3.95)\\} $$\n\nThe coefficient of determination, $R^2$, represents the proportion of the total variance in the dependent variable, $Y_i'$, that is explained by the linear relationship with the independent variable, $X_i'$. It is defined as:\n$$ R^2 = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} $$\nwhere $\\text{SS}_{\\text{tot}}$ is the total sum of squares and $\\text{SS}_{\\text{res}}$ is the residual sum of squares.\n\nThe total sum of squares measures the total variability in the response variable $Y_i'$:\n$$ \\text{SS}_{\\text{tot}} = \\sum_{i=1}^{n} (Y_i' - \\overline{Y'})^2 $$\nwhere $\\overline{Y'}$ is the sample mean of the $Y_i'$ values.\n\nThe residual sum of squares measures the variability that remains unexplained after fitting the regression line:\n$$ \\text{SS}_{\\text{res}} = \\sum_{i=1}^{n} (Y_i' - \\hat{Y}_i')^2 $$\nwhere $\\hat{Y}_i' = \\hat{a} + \\hat{b}X_i'$ are the fitted values obtained from the Ordinary Least Squares (OLS) regression.\n\nFor a simple linear regression model with a single predictor, $R^2$ is equivalent to the square of the Pearson product-moment correlation coefficient, $r$, between $X'$ and $Y'$. This provides a more direct computational path. The formula for $r^2$ is:\n$$ R^2 = r^2 = \\left( \\frac{\\sum_{i=1}^{n} (X_i' - \\overline{X'})(Y_i' - \\overline{Y'})}{\\sqrt{\\sum_{i=1}^{n} (X_i' - \\overline{X'})^2 \\sum_{i=1}^{n} (Y_i' - \\overline{Y'})^2}} \\right)^2 = \\frac{S_{X'Y'}^2}{S_{X'X'} S_{Y'Y'}} $$\nwhere we have defined the corrected sums of squares and cross-products:\n$S_{X'X'} = \\sum_{i=1}^{n} (X_i' - \\overline{X'})^2$\n$S_{Y'Y'} = \\sum_{i=1}^{n} (Y_i' - \\overline{Y'})^2$\n$S_{X'Y'} = \\sum_{i=1}^{n} (X_i' - \\overline{X'})(Y_i' - \\overline{Y'})$\n\nTo compute these quantities, we first calculate the necessary sums and means from the data.\nThe data points are:\n$X': \\{0, 1, 2, 3, 4\\}$\n$Y': \\{0.45, 1.25, 2.20, 3.05, 3.95\\}$\n\nFirst, we compute the sums:\n$$ \\sum_{i=1}^{5} X_i' = 0 + 1 + 2 + 3 + 4 = 10 $$\n$$ \\sum_{i=1}^{5} Y_i' = 0.45 + 1.25 + 2.20 + 3.05 + 3.95 = 10.90 $$\nNext, we compute the sample means:\n$$ \\overline{X'} = \\frac{1}{n}\\sum_{i=1}^{n} X_i' = \\frac{10}{5} = 2 $$\n$$ \\overline{Y'} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i' = \\frac{10.90}{5} = 2.18 $$\n\nNext, we use the computational formulas for the corrected sums of squares and cross-products, which are more convenient than using the definitional formulas.\n$S_{X'X'} = \\sum_{i=1}^{n} (X_i')^2 - \\frac{(\\sum_{i=1}^{n} X_i')^2}{n}$\n$S_{Y'Y'} = \\sum_{i=1}^{n} (Y_i')^2 - \\frac{(\\sum_{i=1}^{n} Y_i')^2}{n}$\n$S_{X'Y'} = \\sum_{i=1}^{n} X_i' Y_i' - \\frac{(\\sum_{i=1}^{n} X_i')(\\sum_{i=1}^{n} Y_i')}{n}$\n\nWe need the following additional sums:\n$$ \\sum_{i=1}^{5} (X_i')^2 = 0^2 + 1^2 + 2^2 + 3^2 + 4^2 = 0 + 1 + 4 + 9 + 16 = 30 $$\n$$ \\sum_{i=1}^{5} (Y_i')^2 = (0.45)^2 + (1.25)^2 + (2.20)^2 + (3.05)^2 + (3.95)^2 $$\n$$ = 0.2025 + 1.5625 + 4.84 + 9.3025 + 15.6025 = 31.51 $$\n$$ \\sum_{i=1}^{5} X_i' Y_i' = (0)(0.45) + (1)(1.25) + (2)(2.20) + (3)(3.05) + (4)(3.95) $$\n$$ = 0 + 1.25 + 4.40 + 9.15 + 15.80 = 30.60 $$\n\nNow we can compute $S_{X'X'}$, $S_{Y'Y'}$, and $S_{X'Y'}$:\n$$ S_{X'X'} = 30 - \\frac{(10)^2}{5} = 30 - \\frac{100}{5} = 30 - 20 = 10 $$\n$$ S_{Y'Y'} = 31.51 - \\frac{(10.90)^2}{5} = 31.51 - \\frac{118.81}{5} = 31.51 - 23.762 = 7.748 $$\n$$ S_{X'Y'} = 30.60 - \\frac{(10)(10.90)}{5} = 30.60 - \\frac{109}{5} = 30.60 - 21.80 = 8.80 $$\n\nFinally, we substitute these values into the formula for $R^2$:\n$$ R^2 = \\frac{S_{X'Y'}^2}{S_{X'X'} S_{Y'Y'}} = \\frac{(8.80)^2}{(10)(7.748)} = \\frac{77.44}{77.48} $$\n$$ R^2 \\approx 0.9994837377... $$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $9, 9, 9, 4$. The fifth significant figure is $8$, so we round the fourth one up.\n$$ R^2 \\approx 0.9995 $$\nThis value indicates that approximately $99.95\\%$ of the variance in the logarithm of the biomarker concentration is explained by the logarithm of the drug dose, signifying an exceptionally strong linear fit on the transformed scale.", "answer": "$$ \\boxed{0.9995} $$", "id": "4914681"}, {"introduction": "While overall $R^2$ is useful, it doesn't reveal the specific contribution of each predictor in a multiple regression model. This practice introduces the concepts of partial and semi-partial coefficients of determination, which are essential for dissecting a model's explanatory power. By calculating these values, you will learn how to quantify the unique variance a single predictor explains, providing deeper insight into variable importance [@problem_id:4914694].", "problem": "A biostatistics study analyzes systolic blood pressure using a multiple linear regression model with predictors age, body mass index, and plasma sodium concentration. The response variable is continuous systolic blood pressure. Assume the standard linear model with independent, homoscedastic errors holds. Let the model with all three predictors be referred to as the full model and the model excluding plasma sodium (but including age and body mass index) be referred to as the reduced model. You are given the following least squares summary quantities: the total sum of squares $SST = 24000$, the sum of squared errors (SSE) for the reduced model $SSE_{\\mathrm{red}} = 18000$, and the sum of squared errors for the full model $SSE_{\\mathrm{full}} = 17100$. Starting from the core definitions of the total sum of squares, residual sum of squares, and the coefficient of determination ($R^2$), derive the expressions needed to compute both the partial coefficient of determination for plasma sodium (controlling for age and body mass index) and the semi-partial coefficient of determination for plasma sodium. Using these derivations, compute the ratio of the partial coefficient of determination to the semi-partial coefficient of determination for plasma sodium. Express your final answer as a pure number (no units).", "solution": "The problem statement is evaluated and found to be valid. It is a well-posed, scientifically grounded problem in biostatistics that provides a complete and consistent set of data and a clear objective. All necessary information is provided to compute the requested quantity.\n\nThe problem asks for the ratio of the partial coefficient of determination to the semi-partial coefficient of determination for the predictor \"plasma sodium\". Let's define the terms based on the provided sums of squares.\n\n**Given:**\n-   Total Sum of Squares: $SST = 24000$\n-   Sum of Squared Errors for the reduced model (without plasma sodium): $SSE_{\\mathrm{red}} = 18000$\n-   Sum of Squared Errors for the full model (with plasma sodium): $SSE_{\\mathrm{full}} = 17100$\n\n**1. Define the Partial Coefficient of Determination**\n\nThe partial coefficient of determination for a predictor (in this case, plasma sodium) measures the proportion of variance in the response variable *that is unexplained by the other predictors* (age and BMI) that is now explained by the addition of the new predictor.\n\n-   The amount of variance unexplained by the reduced model is $SSE_{\\mathrm{red}}$. This becomes the \"total\" variance that the new predictor has a chance to explain.\n-   The reduction in unexplained variance after adding the new predictor is the difference between the sum of squared errors of the two models: $SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}$.\n-   Therefore, the partial coefficient of determination is:\n$$ r^2_{\\text{partial}} = \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SSE_{\\mathrm{red}}} $$\n\n**2. Define the Semi-Partial Coefficient of Determination**\n\nThe semi-partial (or part) coefficient of determination for a predictor measures the proportion of the *total* variance in the response variable that is uniquely explained by that predictor.\n\n-   The amount of variance uniquely explained by the new predictor is again the reduction in the sum of squared errors: $SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}$.\n-   This value is expressed as a proportion of the total sum of squares, $SST$.\n-   Therefore, the semi-partial coefficient of determination is:\n$$ r^2_{\\text{semi-partial}} = \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SST} $$\n\n**3. Compute the Ratio**\n\nThe problem asks for the ratio of the partial coefficient to the semi-partial coefficient.\n$$ \\text{Ratio} = \\frac{r^2_{\\text{partial}}}{r^2_{\\text{semi-partial}}} = \\frac{\\left( \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SSE_{\\mathrm{red}}} \\right)}{\\left( \\frac{SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}}}{SST} \\right)} $$\n\nThe term $(SSE_{\\mathrm{red}} - SSE_{\\mathrm{full}})$ cancels out from the numerator and denominator, leaving a simplified expression:\n$$ \\text{Ratio} = \\frac{1 / SSE_{\\mathrm{red}}}{1 / SST} = \\frac{SST}{SSE_{\\mathrm{red}}} $$\n\n**4. Substitute Values and Calculate**\n\nNow, we substitute the given values into the simplified formula:\n$$ \\text{Ratio} = \\frac{24000}{18000} $$\n\nSimplifying the fraction:\n$$ \\text{Ratio} = \\frac{24}{18} = \\frac{4 \\times 6}{3 \\times 6} = \\frac{4}{3} $$\nThe ratio is $\\frac{4}{3}$.", "answer": "$$\n\\boxed{\\frac{4}{3}}\n$$", "id": "4914694"}, {"introduction": "Goodness-of-fit assessment becomes more nuanced when moving from continuous responses to binary outcomes, such as predicting disease presence or absence. This problem explores the behavior of $R^2$-type measures in logistic regression, particularly in the challenging context of rare events. You will analyze a scenario that contrasts a model's discrimination (measured by AUC) with its explained variance, revealing why a high-performing model can have a surprisingly low $R^2$ and underscoring the importance of a multi-faceted evaluation strategy [@problem_id:4914679].", "problem": "A biostatistics team validates a logistic regression risk model for postoperative sepsis using an external cohort of $n=1000$ adult patients. The observed event prevalence is $\\pi=0.01$. The model outputs individual predicted probabilities $\\hat{p}_i$, and the following summary metrics are reported on the validation cohort: the Area Under the Receiver Operating Characteristic (ROC) curve (AUC) is $0.90$, the average predicted probability equals the observed event rate (that is, the model is calibrated in-the-large), and the Brier score is $0.0098$. Investigators are interested in interpreting calibration, discrimination, and a coefficient of determination in this rare-event setting.\n\nBased strictly on the foundational definitions of discrimination, calibration, and squared-error goodness-of-fit for binary outcomes, which option is most consistent with the reported results and the behavior of $R^2$-type measures for rare events?\n\nA. A high AUC with only a slight reduction in the Brier score relative to the null implies a small coefficient of determination based on Brier scaling (approximately $0.01$), and under a reasonable approximation for rare events, Tjur’s coefficient of discrimination (the difference in mean predicted probabilities between cases and controls) is also small (approximately $0.005$); both can be small even when AUC is high.\n\nB. Given AUC $= 0.90$, the coefficient of determination must be at least $0.81$ because one can square the AUC to obtain $R^2$, regardless of the event prevalence.\n\nC. Because the model’s average predicted probability equals the observed prevalence, the calibration slope must be exactly $1$, and any $R^2$ measure equals the AUC.\n\nD. In rare events, any well-calibrated logistic regression necessarily has large $R^2$ if AUC is high, because the variance of the binary outcome is negligible.", "solution": "The problem statement is scientifically valid and internally consistent. It presents a realistic scenario of clinical prediction model validation and tests the understanding of key performance metrics in a rare-event context.\n\n**Analysis of the Provided Information**\n\n-   **Givens:**\n    -   Sample size $n=1000$\n    -   Event prevalence $\\pi=0.01$ (so 10 events, 990 non-events)\n    -   Discrimination metric: $AUC = 0.90$ (This is a high value, indicating good ability to rank subjects by risk).\n    -   Calibration metric: Calibrated-in-the-large (mean prediction equals mean observation, $\\bar{\\hat{p}} = \\pi$).\n    -   Overall accuracy metric: Brier Score $BS = 0.0098$.\n\n**Step 1: Calculate the Brier-scaled Coefficient of Determination ($R^2_{Brier}$)**\n\nAn $R^2$-type measure can be constructed to show the proportional improvement over a baseline \"null\" model. For the Brier score, the null model is one that predicts the overall prevalence $\\pi$ for every subject. The Brier score of this null model is:\n$$ BS_{null} = \\frac{1}{n} \\sum (y_i - \\pi)^2 = \\pi(1-\\pi) $$\n$$ BS_{null} = 0.01 \\times (1-0.01) = 0.01 \\times 0.99 = 0.0099 $$\nThe Brier-scaled $R^2$ is the proportional reduction in error:\n$$ R^2_{Brier} = 1 - \\frac{BS}{BS_{null}} = 1 - \\frac{0.0098}{0.0099} = \\frac{0.0001}{0.0099} = \\frac{1}{99} \\approx 0.0101 $$\nThis shows that despite a high AUC, the model only improves upon the null model's squared error by about 1%. This is a key feature of rare event models: the total variance is low, so the proportional improvement is often small.\n\n**Step 2: Estimate Tjur’s Coefficient of Discrimination ($D$)**\n\nTjur's $D$ is the difference between the mean predicted probability for the cases (events, $y=1$) and the mean predicted probability for the controls (non-events, $y=0$):\n$$ D = \\overline{\\hat{p}}_{y=1} - \\overline{\\hat{p}}_{y=0} $$\nWe can use a reasonable approximation to estimate $D$. Let's assume the model assigns one probability ($p_1$) to all cases and another ($p_0$) to all controls.\nThe calibration-in-the-large constraint is $\\pi \\cdot p_1 + (1-\\pi) \\cdot p_0 = \\pi$.\n$$ 0.01 \\cdot p_1 + 0.99 \\cdot p_0 = 0.01 \\implies p_1 = 1 - 99 p_0 $$\nThe Brier score for this simplified model is:\n$$ BS = \\pi (1-p_1)^2 + (1-\\pi)(0-p_0)^2 = 0.0098 $$\n$$ 0.01(1-p_1)^2 + 0.99 p_0^2 = 0.0098 $$\nSubstitute $p_1 = 1 - 99p_0$ into the Brier score equation:\n$$ 0.01(1-(1-99p_0))^2 + 0.99 p_0^2 = 0.0098 $$\n$$ 0.01(99p_0)^2 + 0.99 p_0^2 = 0.0098 $$\n$$ 0.01 \\cdot (9801) p_0^2 + 0.99 p_0^2 = 0.0098 $$\n$$ 98.01 p_0^2 + 0.99 p_0^2 = 0.0098 $$\n$$ 99 p_0^2 = 0.0098 \\implies p_0 = \\sqrt{\\frac{0.0098}{99}} \\approx 0.00995 $$\nNow find $p_1$:\n$$ p_1 = 1 - 99(0.00995) \\approx 1 - 0.98505 = 0.01495 $$\nSo, Tjur's $D$ is approximately:\n$$ D = p_1 - p_0 \\approx 0.01495 - 0.00995 = 0.005 $$\nThis shows that even with high discrimination (AUC), the absolute separation between the mean predictions for cases and controls is very small, a direct consequence of the low event rate.\n\n**Step 3: Evaluate the Options**\n\n-   **A. A high AUC with only a slight reduction in the Brier score relative to the null implies a small coefficient of determination based on Brier scaling (approximately $0.01$), and under a reasonable approximation for rare events, Tjur’s coefficient of discrimination (the difference in mean predicted probabilities between cases and controls) is also small (approximately $0.005$); both can be small even when AUC is high.**\n    This statement is fully consistent with our calculations. The $R^2_{Brier}$ is indeed ~0.01, Tjur's $D$ is plausibly ~0.005, and the core insight that these metrics can be small despite a high AUC in a rare event setting is correct.\n\n-   **B. Given AUC $= 0.90$, the coefficient of determination must be at least $0.81$ because one can square the AUC to obtain $R^2$, regardless of the event prevalence.**\n    This is incorrect. There is no general rule that $R^2 = AUC^2$. Our calculation shows $R^2 \\approx 0.01$, disproving this.\n\n-   **C. Because the model’s average predicted probability equals the observed prevalence, the calibration slope must be exactly $1$, and any $R^2$ measure equals the AUC.**\n    This is incorrect. Calibration-in-the-large does not guarantee a calibration slope of 1. Furthermore, $R^2$ and AUC are different metrics and are not equal.\n\n-   **D. In rare events, any well-calibrated logistic regression necessarily has large $R^2$ if AUC is high, because the variance of the binary outcome is negligible.**\n    This reasoning is flawed and the conclusion is the opposite of the truth. The negligible outcome variance, $\\pi(1-\\pi)$, is the *reason* why $R^2$-type measures are often numerically small, as there is little variance to be \"explained\".\n\nTherefore, option A is the only correct and consistent choice. It correctly captures the counterintuitive but statistically sound behavior of these metrics in a rare-event scenario.", "answer": "$$\\boxed{A}$$", "id": "4914679"}]}