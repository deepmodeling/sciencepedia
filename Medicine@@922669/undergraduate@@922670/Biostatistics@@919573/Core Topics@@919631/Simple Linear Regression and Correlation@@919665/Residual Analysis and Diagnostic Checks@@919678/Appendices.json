{"hands_on_practices": [{"introduction": "Understanding a model's sensitivity to individual data points is a cornerstone of robust statistical analysis. This practice guides you through the derivation of leverage, a key diagnostic measure that quantifies the potential influence an observation has on its own fitted value based solely on its position in the predictor space [@problem_id:4949183]. By working through the algebra, you will gain a concrete understanding of why points that are 'extreme' in their predictor values can disproportionately steer the regression line.", "problem": "A biostatistics study models a continuous biomarker response as a linear function of a continuous dose, with an intercept. Specifically, for observations indexed by $i \\in \\{1,\\dots,n\\}$, the model is $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$, where the errors $\\varepsilon_{i}$ satisfy the usual ordinary least squares (OLS) assumptions used to justify projection properties of fitted values. Let the $n \\times 2$ design matrix be $X = \\begin{pmatrix} 1  x_{1} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix}$ and let the projection (hat) matrix be the unique linear operator that maps observed responses to their fitted values under OLS. The diagonal elements of this projection matrix are the leverages, denoted $h_{ii}$, which quantify the potential of observation $i$ to influence its own fitted value.\n\nStarting only from standard linear model definitions and linear algebra facts for $2 \\times 2$ matrices, derive a closed-form expression for the leverage $h_{ii}$ in terms of $n$, $x_{i}$, $\\bar{x}$, and $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$, where $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_{j}$. Your derivation should make explicit any intermediate algebraic steps needed to eliminate nuisance sums in favor of $\\bar{x}$ and $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$.\n\nAdditionally, explain qualitatively, based on your derivation, how the spacing of the design points $\\{x_{j}\\}_{j=1}^{n}$ determines the potential influence of an observation, and why more extreme $x_{i}$ values relative to $\\bar{x}$ have higher leverage.\n\nProvide, as your final answer, the simplified analytic expression for $h_{ii}$. No numerical rounding is required, and no units are involved.", "solution": "The problem requires the derivation of the leverage, $h_{ii}$, for a simple linear regression model and a qualitative explanation of its properties. The derivation must begin from fundamental definitions in linear model theory.\n\nThe simple linear regression model is given by $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$. In matrix form, this is $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of responses, $X$ is the $n \\times 2$ design matrix, $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}$ is the vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the vector of errors.\n\nThe design matrix $X$ is given as:\n$$\nX = \\begin{pmatrix} 1  x_{1} \\\\ 1  x_{2} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix}\n$$\nThe vector of ordinary least squares (OLS) fitted values, $\\hat{\\mathbf{y}}$, is obtained by projecting the observed response vector $\\mathbf{y}$ onto the column space of $X$. The projection matrix, or hat matrix, is denoted by $H$ and is defined as:\n$$\nH = X(X^T X)^{-1} X^T\n$$\nThe leverage $h_{ii}$ of the $i$-th observation is the $i$-th diagonal element of $H$. It can be computed as $h_{ii} = \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i$, where $\\mathbf{x}_i^T = \\begin{pmatrix} 1  x_i \\end{pmatrix}$ is the $i$-th row of the design matrix $X$.\n\nOur derivation proceeds in several steps.\n\n**Step 1: Compute the matrix $X^T X$.**\nThe matrix $X^T X$ is a $2 \\times 2$ symmetric matrix.\n$$\nX^T X = \\begin{pmatrix} 1  1  \\dots  1 \\\\ x_1  x_2  \\dots  x_n \\end{pmatrix} \\begin{pmatrix} 1  x_{1} \\\\ 1  x_{2} \\\\ \\vdots  \\vdots \\\\ 1  x_{n} \\end{pmatrix} = \\begin{pmatrix} \\sum_{j=1}^{n} 1  \\sum_{j=1}^{n} x_j \\\\ \\sum_{j=1}^{n} x_j  \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\nUsing the definition of the sample mean, $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_j$, we can write $\\sum_{j=1}^{n} x_j = n\\bar{x}$.\nThus,\n$$\nX^T X = \\begin{pmatrix} n  n\\bar{x} \\\\ n\\bar{x}  \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\n\n**Step 2: Compute the determinant of $X^T X$ and its inverse.**\nThe determinant of $X^T X$ is:\n$$\n\\det(X^T X) = n \\left(\\sum_{j=1}^{n} x_j^2\\right) - (n\\bar{x})^2 = n \\sum_{j=1}^{n} x_j^2 - n^2\\bar{x}^2\n$$\nTo express this in the required terms, we use the definition of the sum of squared deviations from the mean, which we denote as $S_{xx}$:\n$$\nS_{xx} = \\sum_{j=1}^{n} (x_j - \\bar{x})^2 = \\sum_{j=1}^{n} (x_j^2 - 2x_j\\bar{x} + \\bar{x}^2) = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}\\left(\\sum_{j=1}^{n} x_j\\right) + \\sum_{j=1}^{n} \\bar{x}^2\n$$\nSubstituting $\\sum_{j=1}^{n} x_j = n\\bar{x}$, we get:\n$$\nS_{xx} = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2 = \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2\n$$\nTherefore, the determinant can be simplified:\n$$\n\\det(X^T X) = n \\left( \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2 \\right) = n S_{xx} = n \\sum_{j=1}^{n} (x_j - \\bar{x})^2\n$$\nNow, we find the inverse of the $2 \\times 2$ matrix $X^T X$ using the formula for the inverse of a general $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$:\n$$\n(X^T X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} \\sum_{j=1}^{n} x_j^2  -n\\bar{x} \\\\ -n\\bar{x}  n \\end{pmatrix}\n$$\nTo simplify this expression, we substitute $\\sum_{j=1}^{n} x_j^2 = S_{xx} + n\\bar{x}^2$:\n$$\n(X^T X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} S_{xx} + n\\bar{x}^2  -n\\bar{x} \\\\ -n\\bar{x}  n \\end{pmatrix} = \\begin{pmatrix} \\frac{S_{xx} + n\\bar{x}^2}{n S_{xx}}  -\\frac{n\\bar{x}}{n S_{xx}} \\\\ -\\frac{n\\bar{x}}{n S_{xx}}  \\frac{n}{n S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix}\n$$\n\n**Step 3: Compute the leverage $h_{ii}$.**\nUsing the formula $h_{ii} = \\mathbf{x}_i^T (X^T X)^{-1} \\mathbf{x}_i$ with $\\mathbf{x}_i^T = \\begin{pmatrix} 1  x_i \\end{pmatrix}$:\n$$\nh_{ii} = \\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\nFirst, multiply the row vector by the matrix:\n$$\n\\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}  -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}}  \\frac{1}{S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right)  \\left(-\\frac{\\bar{x}}{S_{xx}} + \\frac{x_i}{S_{xx}}\\right) \\end{pmatrix}\n$$\nNext, multiply this resulting $1 \\times 2$ vector by the column vector $\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}$:\n$$\nh_{ii} = \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right) \\cdot 1 + \\left(\\frac{x_i - \\bar{x}}{S_{xx}}\\right) \\cdot x_i\n$$\n$$\nh_{ii} = \\frac{1}{n} + \\frac{\\bar{x}^2 - x_i \\bar{x} + x_i^2 - \\bar{x} x_i}{S_{xx}} = \\frac{1}{n} + \\frac{x_i^2 - 2x_i\\bar{x} + \\bar{x}^2}{S_{xx}}\n$$\nRecognizing the numerator of the second term as $(x_i - \\bar{x})^2$, we arrive at the final expression:\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\n$$\nSubstituting back the definition of $S_{xx}$:\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}\n$$\nThis is the required closed-form expression for the leverage of observation $i$.\n\n**Qualitative Explanation of Leverage**\n\nThe derived expression for leverage, $h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$, provides direct insight into the factors that determine an observation's potential influence on its own fitted value. The leverage is composed of two non-negative terms:\n\n$1$. The term $\\frac{1}{n}$ represents a baseline component of leverage that is uniform for all observations. It only depends on the sample size $n$. As the number of observations increases, this baseline influence decreases, reflecting the reduced importance of any single point in a larger dataset.\n\n$2$. The second term, $\\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$, represents the component of leverage due to the position of the predictor value $x_i$ relative to the other predictor values.\n   - The numerator, $(x_i - \\bar{x})^2$, is the squared distance of $x_i$ from the center of the predictor data, $\\bar{x}$. This term shows that as an observation's predictor value becomes more extreme (i.e., farther from the mean $\\bar{x}$), its leverage increases quadratically. An observation with a large value of $|x_i - \\bar{x}|$ is called a high-leverage point. Such points act like long lever arms, giving them greater potential to pivot the fitted regression line.\n   - The denominator, $\\sum_{j=1}^{n} (x_j - \\bar{x})^2$, is the sum of squared deviations of all predictor values. It quantifies the total spread or variability of the design points $\\{x_j\\}$. This term sets the scale by which the \"extremeness\" of a given point $x_i$ is judged.\n     - If the design points are widely spaced, the denominator will be large. This reduces the magnitude of the positional leverage component for all points, meaning the regression line is more robustly \"anchored\" by the dispersed data, and the influence of any single point is diminished.\n     - Conversely, if the design points are tightly clustered, the denominator will be small. This amplifies the positional leverage component. In such a scenario, an observation that is only moderately distant from $\\bar{x}$ can have very high leverage because it is an outlier relative to the narrow distribution of the remaining data.\n\nIn conclusion, an observation's leverage is high if its predictor value is far from the mean of all predictor values, particularly when the majority of other predictor values are not widely spread out. This gives a precise, quantitative basis for the intuitive idea that remote data points in the predictor space are the most influential in a regression analysis.", "answer": "$$\n\\boxed{\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}}\n$$", "id": "4949183"}, {"introduction": "While leverage tells us about an observation's *potential* to be influential, it doesn't tell the whole story, as a high-leverage point might fit the overall trend perfectly and have little actual impact. This exercise challenges you to derive the formula for Cook's distance, a classic and powerful metric that synthesizes both an observation's leverage and its residual size into a single number measuring its overall influence on the regression coefficients [@problem_id:4949167]. This derivation elegantly reveals how influence is a delicate interplay between being an outlier in the predictor space and an outlier in the response space.", "problem": "A biostatistician fits an ordinary least squares (OLS) linear regression model with response vector $y \\in \\mathbb{R}^{n}$ and full-rank design matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n > p \\ge 2$. Assume the classical linear model $y = X\\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Let $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$ be the OLS estimator, $\\hat{y} = X \\hat{\\beta}$ the fitted values, and $e = y - \\hat{y}$ the residual vector. Define the hat matrix $H = X (X^{\\top} X)^{-1} X^{\\top}$, and let $h_{ii}$ denote its $i$-th diagonal element (the leverage of observation $i$). Let $s^{2} = \\frac{1}{n-p} \\|y - X \\hat{\\beta}\\|_{2}^{2}$ be the mean squared error. The internally studentized residual for observation $i$ is defined as $t_{i} = \\frac{e_{i}}{s \\sqrt{1 - h_{ii}}}$. For the leave-one-out fit that omits observation $i$, denote its coefficient estimator by $\\hat{\\beta}_{(i)}$. Cook’s distance for observation $i$ is defined by\n$$\nD_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}}.\n$$\nStarting only from these definitions and standard linear algebra identities for rank-one updates, derive an explicit expression for $D_{i}$ written solely in terms of $p$, $t_{i}$, and $h_{ii}$. Then, based on your derivation, explain briefly how leverage $h_{ii}$ and residual magnitude (as captured by $t_{i}$) each influence $D_{i}$.\n\nProvide your final answer as the closed-form analytical expression for $D_{i}$ in terms of $p$, $t_{i}$, and $h_{ii}$. No numerical evaluation is required.", "solution": "The problem is well-posed, scientifically grounded in the theory of linear models, and contains all necessary information for a unique analytical solution. The definitions and premises are standard and internally consistent.\n\nThe objective is to derive an expression for Cook's distance, $D_{i}$, in terms of the number of predictors $p$, the internally studentized residual $t_{i}$, and the leverage $h_{ii}$. The provided definition of Cook's distance for observation $i$ is:\n$$ D_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}} $$\nThe core of the derivation is to find a simplified expression for the change in the coefficient vector, $\\hat{\\beta} - \\hat{\\beta}_{(i)}$, that results from omitting the $i$-th observation. The leave-one-out estimator, $\\hat{\\beta}_{(i)}$, is calculated using the data matrices $X_{(i)}$ and $y_{(i)}$, which are formed by removing the $i$-th row from $X$ and $y$, respectively. Let $x_i^{\\top}$ be the $i$-th row of $X$. The required matrix products for the leave-one-out estimator are rank-$1$ modifications of the full-data products:\n$$ X_{(i)}^{\\top} X_{(i)} = X^{\\top}X - x_i x_i^{\\top} $$\n$$ X_{(i)}^{\\top} y_{(i)} = X^{\\top}y - x_i y_i $$\nTo compute $\\hat{\\beta}_{(i)} = (X_{(i)}^{\\top} X_{(i)})^{-1} X_{(i)}^{\\top} y_{(i)}$, we first need the inverse of $X_{(i)}^{\\top} X_{(i)}$. We use the Sherman-Morrison-Woodbury formula for a rank-$1$ update, which states $(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$. Setting $A = X^{\\top}X$ and $u = v = x_i$, we get:\n$$ (X^{\\top}X - x_i x_i^{\\top})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - x_i^{\\top}(X^{\\top}X)^{-1}x_i} $$\nThe term in the denominator, $x_i^{\\top}(X^{\\top}X)^{-1}x_i$, corresponds to the $i$-th diagonal element of the hat matrix $H = X(X^{\\top}X)^{-1}X^{\\top}$, which is the leverage $h_{ii}$. The inverse is therefore:\n$$ (X_{(i)}^{\\top} X_{(i)})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} $$\nNow, we can express $\\hat{\\beta}_{(i)}$:\n$$ \\hat{\\beta}_{(i)} = \\left( (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} \\right) (X^{\\top}y - x_i y_i) $$\nExpanding this expression yields:\n$$ \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}(X^{\\top}y) - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} \\left( x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y - x_i^{\\top}(X^{\\top}X)^{-1}x_i y_i \\right) $$\nWe identify several terms: $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$, the $i$-th fitted value $\\hat{y}_i = x_i^{\\top}\\hat{\\beta} = x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y$, and $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$. Substituting these gives:\n$$ \\hat{\\beta}_{(i)} = \\hat{\\beta} - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) $$\nRearranging to find the difference vector $\\hat{\\beta} - \\hat{\\beta}_{(i)}$:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}x_i y_i - \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) = (X^{\\top}X)^{-1}x_i \\left( y_i - \\frac{\\hat{y}_i - h_{ii} y_i}{1 - h_{ii}} \\right) $$\nThe term in the parenthesis simplifies:\n$$ \\frac{y_i(1 - h_{ii}) - (\\hat{y}_i - h_{ii} y_i)}{1 - h_{ii}} = \\frac{y_i - y_i h_{ii} - \\hat{y}_i + y_i h_{ii}}{1 - h_{ii}} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} = \\frac{e_i}{1-h_{ii}} $$\nwhere $e_i = y_i - \\hat{y}_i$ is the $i$-th residual. This leads to the key result for the change in coefficients:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{(X^{\\top}X)^{-1}x_i e_i}{1 - h_{ii}} $$\nNext, we substitute this into the numerator of Cook's distance:\n$$ (\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)}) = \\left( \\frac{e_i}{1-h_{ii}} x_i^{\\top}(X^{\\top}X)^{-1} \\right) (X^{\\top}X) \\left( \\frac{(X^{\\top}X)^{-1}x_i e_i}{1-h_{ii}} \\right) $$\n$$ = \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 x_i^{\\top} \\left( (X^{\\top}X)^{-1} (X^{\\top}X) (X^{\\top}X)^{-1} \\right) x_i = \\frac{e_i^2}{(1-h_{ii})^2} x_i^{\\top} (X^{\\top}X)^{-1} x_i $$\nUsing $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$, the numerator becomes $\\frac{e_i^2 h_{ii}}{(1-h_{ii})^2}$. Substituting this into the definition of $D_i$:\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2} \\right] $$\nThe final step is to incorporate the internally studentized residual $t_{i} = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}$. Squaring this gives $t_i^2 = \\frac{e_i^2}{s^2(1-h_{ii})}$, from which we get $e_i^2 = t_i^2 s^2 (1 - h_{ii})$. Substituting this expression for $e_i^2$:\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{t_i^2 s^2 (1 - h_{ii}) h_{ii}}{(1-h_{ii})^2} \\right] $$\nCanceling $s^2$ and a factor of $(1-h_{ii})$ yields the final expression for Cook's distance:\n$$ D_i = \\frac{t_i^2}{p} \\frac{h_{ii}}{1-h_{ii}} $$\nThis derived expression illuminates the nature of an observation's influence. The influence, measured by $D_i$, is a product of two factors: one related to the residual size ($t_i^2$) and one related to the observation's leverage ($h_{ii}$).\nThe term $t_i^2$ shows that $D_i$ is proportional to the squared studentized residual. Points that are far from the regression hyperplane (outliers in the response space) will have large $|t_i|$ values, contributing to a larger $D_i$.\nThe term $\\frac{h_{ii}}{1-h_{ii}}$ captures the effect of leverage. Leverage $h_{ii}$ quantifies how far an observation's predictor values $x_i$ are from the center of the predictor space. The function $\\frac{h_{ii}}{1-h_{ii}}$ increases non-linearly as leverage $h_{ii}$ approaches $1$. This means that high-leverage points act as a strong multiplier on the residual term.\nIn summary, an observation is influential if it has a large residual, high leverage, or both. A high-leverage point has the potential for high influence; this potential is realized if the point also has a non-negligible residual. Conversely, a point with low leverage must have a very large residual to be influential.", "answer": "$$\\boxed{\\frac{t_{i}^{2}}{p} \\frac{h_{ii}}{1 - h_{ii}}}$$", "id": "4949167"}, {"introduction": "Diagnostic checks are not merely academic exercises; they have profound implications for the validity of our statistical inferences. This computational practice puts theory into action by having you implement a residual bootstrap, a widely used method for estimating the uncertainty of model parameters [@problem_id:4949168]. By testing your bootstrap procedure under different scenarios—including violations of homoscedasticity and independence—you will directly observe how and why diagnostic checks are essential for ensuring that methods like the bootstrap produce reliable results.", "problem": "You are given a fixed-design linear regression setting in which the response vector $y \\in \\mathbb{R}^n$ is modeled as $y = X \\beta + \\varepsilon$ with a fixed design matrix $X \\in \\mathbb{R}^{n \\times p}$ that includes an intercept column. The ordinary least squares estimator $\\hat{\\beta}$ is defined as the vector that minimizes the sum of squared residuals $\\sum_{i=1}^n (y_i - x_i^\\top b)^2$ over all $b \\in \\mathbb{R}^p$, where $x_i^\\top$ denotes the $i$-th row of $X$. Residual analysis and diagnostic checks are used to assess conditions under which a residual bootstrap procedure is consistent for approximating the sampling distribution of $\\hat{\\beta}$.\n\nConstruct a residual bootstrap for linear regression that uses standardized and centered residuals to mimic the error distribution. Your program must:\n\n1. Use the principle of least squares to compute $\\hat{\\beta}$, the residuals $e = y - X \\hat{\\beta}$, the diagonal elements of the hat matrix $h_{ii}$, and the fitted values $\\hat{y} = X \\hat{\\beta}$.\n\n2. Construct standardized centered residuals $u_i = \\frac{e_i - \\bar{e}}{\\sqrt{1 - h_{ii}}}$, where $\\bar{e}$ is the empirical mean of $e$. When computing the denominator, ensure numerical stability by replacing $\\sqrt{1 - h_{ii}}$ with $\\sqrt{\\max(1 - h_{ii}, 10^{-8})}$.\n\n3. Implement a residual bootstrap with $B$ resamples, where in each resample you:\n   - Draw $n$ residuals with replacement from $\\{u_i\\}_{i=1}^n$ to obtain $u^\\ast$.\n   - Form $y^\\ast = X \\hat{\\beta} + u^\\ast$ and recompute $\\hat{\\beta}^\\ast$ by least squares.\n   - Collect the bootstrap replicates $\\{\\hat{\\beta}^\\ast\\}$ and compute the bootstrap standard deviation vector $s^\\ast \\in \\mathbb{R}^p$ across the $B$ replicates.\n\n4. For each test case, construct a Monte Carlo reference distribution of $\\hat{\\beta}$ using $M$ independent datasets generated from the specified true error mechanism for that case, with the same fixed $X$ and true $\\beta$. Compute the Monte Carlo standard deviation vector $s^{\\mathrm{MC}} \\in \\mathbb{R}^p$ across the $M$ replicates.\n\n5. Compute the model-based \"true\" covariance for $\\hat{\\beta}$ under the specified error covariance $\\Sigma_\\varepsilon$ for the case, using the identity $\\mathrm{Var}(\\hat{\\beta}) = (X^\\top X)^{-1} X^\\top \\Sigma_\\varepsilon X (X^\\top X)^{-1}$. From this, obtain $s^{\\mathrm{true}} = \\sqrt{\\mathrm{diag}((X^\\top X)^{-1} X^\\top \\Sigma_\\varepsilon X (X^\\top X)^{-1})}$.\n\n6. Declare bootstrap consistency for the distribution of $\\hat{\\beta}$ if and only if both of the following criteria hold:\n   - The maximum relative deviation in standard deviations is below a threshold $\\tau_{\\mathrm{sd}}$, i.e., $\\max_{j=1,\\dots,p} \\left| s^\\ast_j - s^{\\mathrm{true}}_j \\right| / s^{\\mathrm{true}}_j  \\tau_{\\mathrm{sd}}$.\n   - The maximum Kolmogorov–Smirnov distance between the marginal bootstrap distribution of each coefficient and its Monte Carlo reference distribution is below a threshold $\\tau_{\\mathrm{KS}}$.\n\n7. Compute two diagnostic checks from the residuals $e$:\n   - Heteroscedasticity diagnostic: Use the Breusch–Pagan auxiliary regression of $e_i^2$ on the non-intercept columns of $X$, including an intercept in the auxiliary regression. Let $R^2$ be the coefficient of determination of this auxiliary regression. Define $T_{\\mathrm{BP}} = n R^2$ and the $p$-value $p_{\\mathrm{BP}} = 1 - F_{\\chi^2}(T_{\\mathrm{BP}}; p-1)$, where $F_{\\chi^2}$ is the cumulative distribution function of the chi-square distribution with $p-1$ degrees of freedom. Declare heteroscedasticity detected if $p_{\\mathrm{BP}}  0.05$.\n   - Autocorrelation diagnostic: Compute the lag-$1$ sample autocorrelation $r_1$ of the residuals (using the usual sample correlation between $(e_1,\\dots,e_{n-1})$ and $(e_2,\\dots,e_n)$). Declare autocorrelation detected if $|r_1| \\ge \\rho_{\\mathrm{thr}}$.\n\nYour program must implement the above using the following fixed thresholds and resample sizes:\n- Set the number of bootstrap resamples to $B = 800$.\n- Set the number of Monte Carlo replicates to $M = 1500$.\n- Set the standard deviation relative error threshold to $\\tau_{\\mathrm{sd}} = 0.25$.\n- Set the Kolmogorov–Smirnov distance threshold to $\\tau_{\\mathrm{KS}} = 0.15$.\n- Set the autocorrelation detection threshold to $\\rho_{\\mathrm{thr}} = 0.25$.\n\nTest Suite:\nImplement the program for the following four scientifically plausible test cases. All cases share the true coefficient vector $\\beta = [0.7, -1.2, 0.5]^\\top$. In each case, the design matrix $X$ has $p = 3$ columns: an intercept column of all ones and two predictor columns. The predictors are generated as independent standard normal random variables unless otherwise specified. Use the provided random seeds for reproducibility.\n\n- Case A (homoscedastic independent errors, \"happy path\"): $n = 200$, $\\sigma^2 = 1.0$, errors $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, seed $123$.\n- Case B (heteroscedastic independent errors): $n = 200$, $\\sigma^2 = 1.0$, errors $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$ independently with $\\sigma_i^2 = \\sigma^2 (1 + |x_{i1}|)$ where $x_{i1}$ is the first non-intercept predictor for observation $i$, seed $456$.\n- Case C (positively autocorrelated errors): $n = 200$, $\\sigma^2 = 1.0$, errors follow a stationary autoregressive model of order $1$ with parameter $\\rho = 0.6$, i.e., $\\varepsilon_t = \\rho \\varepsilon_{t-1} + \\eta_t$ where $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ and $\\sigma_\\eta^2 = \\sigma^2 (1 - \\rho^2)$ to ensure $\\mathrm{Var}(\\varepsilon_t) = \\sigma^2$, seed $789$.\n- Case D (extreme leverage, small sample): $n = 40$, $\\sigma^2 = 1.0$, errors $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, design has extreme leverage by setting the first row’s predictors to $x_{11} = 20$ and $x_{12} = 20$, with the remaining rows generated as independent standard normal predictors, seed $321$.\n\nFor each case, construct $X$, generate $y$ according to the case mechanism with the given $\\beta$ and $\\sigma^2$, fit the model, run the residual bootstrap, and compute the diagnostics and consistency decision as specified.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of three booleans in the order $[$consistent, heteroscedasticity\\_detected, autocorrelation\\_detected$]$. For example, an output could look like $[[\\mathrm{True},\\mathrm{False},\\mathrm{False}],[\\mathrm{False},\\mathrm{True},\\mathrm{False}],[\\mathrm{False},\\mathrm{False},\\mathrm{True}],[\\mathrm{False},\\mathrm{False},\\mathrm{False}]]$.", "solution": "The problem statement is scientifically sound, well-posed, and provides a complete and formal specification for a computational experiment in biostatistics. It requires the implementation and evaluation of a residual bootstrap procedure for linear regression under various data-generating assumptions. The problem is valid and a rigorous solution can be constructed.\n\nThe fundamental model under consideration is the linear regression model $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is a known, fixed design matrix of full column rank that includes an intercept, $\\beta \\in \\mathbb{R}^p$ is the vector of unknown coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is a vector of unobserved random errors.\n\nThe first step is to obtain the Ordinary Least Squares (OLS) estimator, $\\hat{\\beta}$, which minimizes the sum of squared residuals. The solution is given by the normal equations:\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\nFrom this, we derive several key quantities: the vector of fitted values $\\hat{y} = X \\hat{\\beta}$, the vector of residuals $e = y - \\hat{y}$, and the hat matrix $H = X(X^\\top X)^{-1}X^\\top$. The diagonal elements of the hat matrix, $h_{ii} = x_i^\\top(X^\\top X)^{-1}x_i$, measure the leverage of each observation $i$.\n\nThe core of the problem is to construct and evaluate a specific residual bootstrap procedure. This procedure aims to approximate the sampling distribution of $\\hat{\\beta}$ by simulating new datasets. The standard residual bootstrap assumes that the errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.). The procedure attempts to mimic the unknown error distribution by resampling from a set of modified residuals. The specified modification involves centering and standardization:\n$$\nu_i = \\frac{e_i - \\bar{e}}{\\sqrt{1 - h_{ii}}}\n$$\nwhere $\\bar{e}$ is the sample mean of the residuals. For a model with an intercept, $\\bar{e}$ is theoretically zero, but its subtraction provides numerical robustness. The division by $\\sqrt{1 - h_{ii}}$ accounts for the fact that raw residuals $e_i$ do not have constant variance even if the true errors $\\varepsilon_i$ do; specifically, $\\mathrm{Var}(e_i) = \\sigma^2 (1 - h_{ii})$ under the classical assumptions. The set $\\{u_i\\}_{i=1}^n$ thus forms a sample that, under ideal conditions, better approximates the distribution of the true errors. The denominator is numerically stabilized by replacing $\\sqrt{1 - h_{ii}}$ with $\\sqrt{\\max(1 - h_{ii}, 10^{-8})}$.\n\nThe bootstrap simulation proceeds for $B$ iterations. In each iteration $b=1, \\dots, B$:\n1. A bootstrap sample of residuals $u^\\ast = \\{u_1^\\ast, \\dots, u_n^\\ast\\}^\\top$ is drawn with replacement from the set $\\{u_1, \\dots, u_n\\}$.\n2. A bootstrap response vector is generated as $y^\\ast = X\\hat{\\beta} + u^\\ast$. This construction assumes the estimated model $y \\approx X\\hat{\\beta}$ is a good baseline, and new data is synthesized by adding resampled errors.\n3. A bootstrap coefficient estimate $\\hat{\\beta}^\\ast$ is computed by applying OLS to the bootstrap data: $\\hat{\\beta}^\\ast = (X^\\top X)^{-1}X^\\top y^\\ast$.\n\nThe collection of $B$ replicates, $\\{\\hat{\\beta}^\\ast\\}$, forms an empirical approximation to the sampling distribution of $\\hat{\\beta}$. The bootstrap standard deviation vector, $s^\\ast$, is computed as the column-wise sample standard deviation of the $B \\times p$ matrix of bootstrap estimates.\n\nTo assess the \"consistency\" or quality of this bootstrap approximation, we require a gold standard for comparison. The problem specifies two such standards. First, the analytical or \"true\" variance-covariance matrix of the OLS estimator is given by the sandwich formula:\n$$\n\\mathrm{Var}(\\hat{\\beta}) = (X^\\top X)^{-1} X^\\top \\mathrm{Var}(\\varepsilon) X (X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top \\Sigma_\\varepsilon X (X^\\top X)^{-1}\n$$\nwhere $\\Sigma_\\varepsilon$ is the true covariance matrix of the errors. The vector of true standard deviations, $s^{\\mathrm{true}}$, is the square root of the diagonal elements of this matrix. Secondly, a high-fidelity Monte Carlo simulation is performed. For $M$ iterations, new response vectors are generated from the true model $y_{\\mathrm{MC}} = X\\beta + \\varepsilon_{\\mathrm{MC}}$, where $\\varepsilon_{\\mathrm{MC}}$ is a fresh draw from the specified true error distribution. The resulting collection of estimators $\\{\\hat{\\beta}_{\\mathrm{MC}}\\}$ provides a highly accurate empirical reference for the true sampling distribution of $\\hat{\\beta}$.\n\nConsistency is declared if two criteria are met:\n1. The bootstrap standard deviations must be close to the true ones: $\\max_{j} |s^\\ast_j - s^{\\mathrm{true}}_j| / s^{\\mathrm{true}}_j  \\tau_{\\mathrm{sd}}$.\n2. The shape of the marginal bootstrap distributions must match the shape of the marginal Monte Carlo distributions, verified by ensuring the maximum Kolmogorov-Smirnov distance is below a threshold $\\tau_{\\mathrm{KS}}$.\n\nFinally, two diagnostic checks are performed on the original residuals $e$ to detect potential violations of the assumptions that underpin the simple residual bootstrap.\n1. **Heteroscedasticity**: The Breusch-Pagan test is employed. It is based on an auxiliary regression of the squared residuals, $e_i^2$, on the predictors in $X$. The test statistic $T_{\\mathrm{BP}} = nR^2$, where $R^2$ is the coefficient of determination from this auxiliary regression, is compared against a $\\chi^2_{p-1}$ distribution. A small $p$-value ($p_{\\mathrm{BP}}  0.05$) indicates evidence of heteroscedasticity.\n2. **Autocorrelation**: The lag-$1$ sample autocorrelation of the residuals, $r_1$, is computed. A value $|r_1| \\ge \\rho_{\\mathrm{thr}}$ suggests the presence of serial correlation in the errors.\n\nThis framework is applied to four test cases designed to probe the bootstrap's performance under ideal conditions (Case A), heteroscedasticity (Case B), autocorrelation (Case C), and in the presence of a high-leverage data point (Case D). The results from these tests provide insight into when the specified residual bootstrap is a reliable tool and how its failure modes correlate with signals from standard diagnostic checks.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2, ks_2samp\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the analysis for all test cases.\n    \"\"\"\n    B = 800\n    M = 1500\n    TAU_SD = 0.25\n    TAU_KS = 0.15\n    RHO_THR = 0.25\n    P_VAL_THR = 0.05\n    BETA_TRUE = np.array([0.7, -1.2, 0.5])\n    P = len(BETA_TRUE)\n\n    test_cases = [\n        {\"name\": \"A\", \"n\": 200, \"sigma2\": 1.0, \"seed\": 123},\n        {\"name\": \"B\", \"n\": 200, \"sigma2\": 1.0, \"seed\": 456},\n        {\"name\": \"C\", \"n\": 200, \"sigma2\": 1.0, \"seed\": 789},\n        {\"name\": \"D\", \"n\": 40, \"sigma2\": 1.0, \"seed\": 321},\n    ]\n\n    final_results = []\n    for case in test_cases:\n        result = run_case(case, P, BETA_TRUE, B, M, TAU_SD, TAU_KS, RHO_THR, P_VAL_THR)\n        final_results.append(result)\n        \n    print(f\"[{','.join(map(str, final_results))}]\")\n\ndef run_case(case_params, p, beta_true, B, M, tau_sd, tau_ks, rho_thr, p_val_thr):\n    \"\"\"\n    Executes the full simulation and analysis for a single test case.\n    \"\"\"\n    n = case_params[\"n\"]\n    sigma2 = case_params[\"sigma2\"]\n    rng = np.random.default_rng(case_params[\"seed\"])\n\n    # 1. Generate design matrix X\n    if case_params[\"name\"] == \"D\":\n        X_pred = rng.standard_normal(size=(n - 1, p - 1))\n        X_pred = np.vstack([np.array([20.0, 20.0]), X_pred])\n    else:\n        X_pred = rng.standard_normal(size=(n, p - 1))\n    X = np.hstack([np.ones((n, 1)), X_pred])\n\n    # 2. Generate true error structure and one realization of y\n    eps_single, sigma_eps_mat, error_generator = get_error_structure(\n        case_params[\"name\"], X, n, sigma2, rng\n    )\n    y_single = X @ beta_true + eps_single\n    \n    # 3. Fit OLS model and get residuals\n    beta_hat, e, h_ii, y_hat = least_squares_and_residuals(X, y_single)\n    \n    # 4. Perform diagnostics\n    hetero_detected, ac_detected = run_diagnostics(e, X, n, p, rho_thr, p_val_thr)\n    \n    # 5. Perform residual bootstrap\n    s_star, bootstrap_betas = run_bootstrap(X, beta_hat, e, h_ii, n, p, B, rng)\n    \n    # 6. Perform Monte Carlo simulation\n    s_mc, mc_betas = run_monte_carlo(X, beta_true, error_generator, n, p, M)\n    \n    # 7. Compute true standard deviation\n    s_true = compute_true_sd(X, sigma_eps_mat)\n\n    # 8. Check for bootstrap consistency\n    rel_dev_check = np.max(np.abs(s_star - s_true) / s_true)  tau_sd\n    \n    ks_stats = [ks_2samp(bootstrap_betas[:, j], mc_betas[:, j]).statistic for j in range(p)]\n    ks_check = np.max(ks_stats)  tau_ks\n    \n    consistent = rel_dev_check and ks_check\n\n    return [consistent, hetero_detected, ac_detected]\n\ndef get_error_structure(case_name, X, n, sigma2, rng):\n    \"\"\"\n    Generates the error structure based on the case name.\n    \"\"\"\n    def generate_ar1_errors(rho, n_samples):\n        sigma_eta_sq = sigma2 * (1 - rho**2)\n        errors = np.zeros(n_samples)\n        errors[0] = rng.normal(0, np.sqrt(sigma2))\n        for t in range(1, n_samples):\n            errors[t] = rho * errors[t-1] + rng.normal(0, np.sqrt(sigma_eta_sq))\n        return errors\n\n    if case_name == \"A\" or case_name == \"D\":\n        eps_single = rng.normal(0, np.sqrt(sigma2), size=n)\n        sigma_eps_mat = sigma2 * np.identity(n)\n        error_generator = lambda: rng.normal(0, np.sqrt(sigma2), size=n)\n    elif case_name == \"B\":\n        x1 = X[:, 1]\n        sigma_i_sq = sigma2 * (1 + np.abs(x1))\n        eps_single = rng.normal(0, np.sqrt(sigma_i_sq))\n        sigma_eps_mat = np.diag(sigma_i_sq)\n        error_generator = lambda: rng.normal(0, np.sqrt(sigma_i_sq))\n    elif case_name == \"C\":\n        rho = 0.6\n        eps_single = generate_ar1_errors(rho, n)\n        r = rho ** np.arange(n)\n        sigma_eps_mat = sigma2 * toeplitz(r)\n        error_generator = lambda: generate_ar1_errors(rho, n)\n\n    return eps_single, sigma_eps_mat, error_generator\n\ndef least_squares_and_residuals(X, y):\n    \"\"\"\n    Computes OLS estimates, residuals, and hat matrix diagonals.\n    \"\"\"\n    XTX = X.T @ X\n    XTY = X.T @ y\n    beta_hat = np.linalg.solve(XTX, XTY)\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    h_ii = np.sum(X * np.linalg.solve(XTX, X.T).T, axis=1)\n    return beta_hat, e, h_ii, y_hat\n\ndef run_diagnostics(e, X, n, p, rho_thr, p_val_thr):\n    \"\"\"\n    Computes Breusch-Pagan and autocorrelation diagnostics.\n    \"\"\"\n    # Breusch-Pagan Test\n    e_sq = e**2\n    Z = X # Auxiliary regression uses same design matrix with intercept.\n    gamma_hat = np.linalg.solve(Z.T @ Z, Z.T @ e_sq)\n    e_sq_hat = Z @ gamma_hat\n    sst = np.sum((e_sq - np.mean(e_sq))**2)\n    sse = np.sum((e_sq - e_sq_hat)**2)\n    r_squared = 1 - sse / sst if sst > 0 else 0\n    t_bp = n * r_squared\n    p_bp = 1 - chi2.cdf(t_bp, df=p - 1)\n    hetero_detected = p_bp  p_val_thr\n\n    # Autocorrelation Test\n    r1 = np.corrcoef(e[:-1], e[1:])[0, 1]\n    ac_detected = np.abs(r1) >= rho_thr\n    \n    return hetero_detected, ac_detected\n\ndef run_bootstrap(X, beta_hat, e, h_ii, n, p, B, rng):\n    \"\"\"\n    Performs the residual bootstrap procedure.\n    \"\"\"\n    e_bar = np.mean(e)\n    h_ii_stable = np.maximum(1 - h_ii, 1e-8)\n    u = (e - e_bar) / np.sqrt(h_ii_stable)\n    \n    bootstrap_betas = np.zeros((B, p))\n    XTX_inv = np.linalg.inv(X.T @ X)\n    XT = X.T\n    y_hat = X @ beta_hat\n    \n    for i in range(B):\n        u_star = rng.choice(u, size=n, replace=True)\n        y_star = y_hat + u_star\n        bootstrap_betas[i, :] = XTX_inv @ XT @ y_star\n\n    s_star = np.std(bootstrap_betas, axis=0, ddof=1)\n    return s_star, bootstrap_betas\n\ndef run_monte_carlo(X, beta_true, error_generator, n, p, M):\n    \"\"\"\n    Performs the Monte Carlo simulation for the reference distribution.\n    \"\"\"\n    mc_betas = np.zeros((M, p))\n    XTX_inv = np.linalg.inv(X.T @ X)\n    XT = X.T\n\n    for i in range(M):\n        eps_mc = error_generator()\n        y_mc = X @ beta_true + eps_mc\n        mc_betas[i, :] = XTX_inv @ XT @ y_mc\n        \n    s_mc = np.std(mc_betas, axis=0, ddof=1)\n    return s_mc, mc_betas\n    \ndef compute_true_sd(X, sigma_eps_mat):\n    \"\"\"\n    Computes the true model-based standard deviation of beta_hat.\n    \"\"\"\n    XTX_inv = np.linalg.inv(X.T @ X)\n    var_beta = XTX_inv @ X.T @ sigma_eps_mat @ X @ XTX_inv\n    s_true = np.sqrt(np.diag(var_beta))\n    return s_true\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4949168"}]}