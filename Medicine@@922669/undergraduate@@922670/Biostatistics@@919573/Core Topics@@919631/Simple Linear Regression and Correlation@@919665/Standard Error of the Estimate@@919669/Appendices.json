{"hands_on_practices": [{"introduction": "The standard error of the estimate, $\\hat{\\sigma}$, provides a crucial scale for interpreting the magnitude of residuals. One of the core assumptions in linear regression for inference is that the errors are normally distributed, which implies that residuals should follow a predictable pattern. This practice challenges you to move beyond simple visual inspection and apply probability theory to rigorously test the normality assumption by comparing the observed number of large residuals to the number expected under a normal distribution.", "problem": "A biostatistics team fits an Ordinary Least Squares (OLS) multiple linear regression to model log-transformed C-reactive protein in a cohort of $n = 200$ adults using age, body mass index, and sex as predictors. The standard error of the estimate, denoted $\\hat{\\sigma}$, computed from the residuals $e_{i}$ of the fitted model, equals $\\hat{\\sigma} = 0.62$ on the log scale. Under the standard linear model assumptions, the residuals are independent and identically distributed with mean $0$ and variance $\\sigma^{2}$, and $\\hat{\\sigma}$ is a consistent estimator of $\\sigma$. In the observed data, $O = 15$ residuals satisfy $|e_{i}| > 2\\hat{\\sigma}$.\n\nStarting from the definitions of residuals and the cumulative distribution function (CDF) of the standard normal distribution, and without assuming any specific numeric tail probability, proceed as follows:\n\n1) Using only the normality and independence assumptions for residuals and the fact that $\\hat{\\sigma}$ estimates $\\sigma$, derive an exact expression for the expected number $E$ of residuals with $|e_{i}| > 2\\hat{\\sigma}$ in terms of the standard normal CDF.\n\n2) Model the indicator of the event $\\{|e_{i}| > 2\\hat{\\sigma}\\}$ as a Bernoulli random variable and derive the sampling variance of the count of exceedances. From this, construct the standardized deviation\n$$\nZ \\;=\\; \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}},\n$$\nwhere $X$ is the count of residuals with $|e_{i}| > 2\\hat{\\sigma}$.\n\n3) Evaluate $Z$ numerically under the normality assumption. Round your answer to four significant figures. Express your final answer as a pure number with no units.", "solution": "The problem is assessed to be valid. It is scientifically grounded in standard statistical theory, internally consistent, and well-posed under the explicitly stated assumptions. The problem provides all necessary data to proceed with a unique solution. We will now derive the solution in three parts as requested.\n\nThe core of the problem is to evaluate whether the observed number of large residuals, $O=15$, is consistent with the assumption that the residuals follow a normal distribution. We are given a sample of size $n=200$, and the standard error of the estimate is $\\hat{\\sigma} = 0.62$. The residuals $e_i$ are assumed to be independent and identically distributed (i.i.d.) as $N(0, \\sigma^2)$, where $\\hat{\\sigma}$ is a consistent estimator of $\\sigma$.\n\nFirst, we derive an exact expression for the expected number, $E$, of residuals whose absolute value exceeds twice the standard error of the estimate. Let $I_i$ be an indicator random variable for the event $|e_i| > 2\\hat{\\sigma}$, for $i=1, 2, \\dots, n$. That is,\n$$ I_i = \\begin{cases} 1 & \\text{if } |e_i| > 2\\hat{\\sigma} \\\\ 0 & \\text{if } |e_i| \\le 2\\hat{\\sigma} \\end{cases} $$\nThe total number of such residuals is a random variable $X = \\sum_{i=1}^{n} I_i$. The expected number of exceedances, $E$, is the expectation of $X$. By the linearity of expectation,\n$$ E = E[X] = E\\left[\\sum_{i=1}^{n} I_i\\right] = \\sum_{i=1}^{n} E[I_i] $$\nThe expectation of an indicator variable is the probability of the event it indicates. Let this probability be $p$.\n$$ E[I_i] = P(|e_i| > 2\\hat{\\sigma}) = p $$\nSince the residuals are assumed to be identically distributed, this probability $p$ is the same for all $i$. Therefore, the expected number is $E = np$.\n\nTo calculate $p$, we use the normality assumption, $e_i \\sim N(0, \\sigma^2)$. Dividing by $\\sigma$, we get that $e_i/\\sigma$ follows a standard normal distribution, $N(0, 1)$. The problem states that $\\hat{\\sigma}$ is a consistent estimator of $\\sigma$. For a large sample size ($n=200$), we can approximate $\\sigma$ with its estimate $\\hat{\\sigma}$ to calculate the probability.\n$$ p = P(|e_i| > 2\\hat{\\sigma}) \\approx P\\left(\\left|\\frac{e_i}{\\sigma}\\right| > 2\\right) $$\nLet $Z_{std}$ be a random variable with the standard normal distribution, $Z_{std} \\sim N(0, 1)$. Let $\\Phi(z)$ be its cumulative distribution function (CDF), i.e., $\\Phi(z) = P(Z_{std} \\le z)$. The probability $p$ can be expressed as:\n$$ p = P(|Z_{std}| > 2) = P(Z_{std} > 2) + P(Z_{std} < -2) $$\nUsing the properties of the CDF and the symmetry of the normal distribution about $0$, we have:\n$$ P(Z_{std} > 2) = 1 - P(Z_{std} \\le 2) = 1 - \\Phi(2) $$\n$$ P(Z_{std} < -2) = \\Phi(-2) = 1 - \\Phi(2) $$\nTherefore, the probability $p$ is:\n$$ p = [1 - \\Phi(2)] + [1 - \\Phi(2)] = 2(1 - \\Phi(2)) $$\nSubstituting this expression for $p$ into the formula for $E$, we obtain the exact expression for the expected number of exceedances in terms of $n$ and the standard normal CDF:\n$$ E = np = 2n(1 - \\Phi(2)) $$\n\nSecond, we model the count of exceedances and derive the standardized deviation $Z$. The indicator variables $I_i$ are Bernoulli random variables with success probability $p = 2(1 - \\Phi(2))$. Since the residuals $e_i$ are assumed to be independent, the indicator variables $I_i$ are also independent. The count of exceedances, $X = \\sum_{i=1}^{n} I_i$, is the sum of $n$ i.i.d. Bernoulli trials. Thus, $X$ follows a binomial distribution, $X \\sim \\text{Bin}(n, p)$.\nThe variance of a binomial random variable is given by $\\operatorname{Var}(X) = np(1-p)$. Substituting the expression for $p$:\n$$ \\operatorname{Var}(X) = n [2(1 - \\Phi(2))] [1 - 2(1 - \\Phi(2))] = 2n(1 - \\Phi(2))(2\\Phi(2) - 1) $$\nThe standardized deviation $Z$ is defined as the difference between the observed count $O$ and the expected count $E$, divided by the standard deviation of the count.\n$$ Z = \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}} $$\nSubstituting the derived expressions for $E$ and $\\operatorname{Var}(X)$, we have:\n$$ Z = \\frac{O - 2n(1 - \\Phi(2))}{\\sqrt{2n(1 - \\Phi(2))(2\\Phi(2) - 1)}} $$\n\nThird, we evaluate $Z$ numerically. The given values are $n=200$ for the sample size and $O=15$ for the observed number of exceedances. We use the standard value for the normal CDF at $z=2$, which is $\\Phi(2) \\approx 0.97724987$.\nFirst, we calculate the probability $p$:\n$$ p = 2(1 - \\Phi(2)) \\approx 2(1 - 0.97724987) = 2(0.02275013) = 0.04550026 $$\nNext, we calculate the expected number of exceedances, $E$:\n$$ E = np \\approx 200 \\times 0.04550026 = 9.100052 $$\nThen, we calculate the variance of the count, $\\operatorname{Var}(X)$:\n$$ \\operatorname{Var}(X) = np(1-p) \\approx 9.100052 \\times (1 - 0.04550026) = 9.100052 \\times 0.95449974 \\approx 8.68594 $$\nThe standard deviation is the square root of the variance:\n$$ \\sqrt{\\operatorname{Var}(X)} \\approx \\sqrt{8.68594} \\approx 2.947192 $$\nFinally, we compute the value of the $Z$ statistic:\n$$ Z = \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}} \\approx \\frac{15 - 9.100052}{2.947192} = \\frac{5.899948}{2.947192} \\approx 2.00190 $$\nRounding the result to four significant figures, as requested, gives $Z = 2.002$. This value indicates that the observed number of residuals with absolute values greater than $2\\hat{\\sigma}$ is approximately $2$ standard deviations above what would be expected under the assumption of normality.", "answer": "$$\\boxed{2.002}$$", "id": "4953182"}, {"introduction": "Ideally, the standard error of the estimate, $\\hat{\\sigma}$, quantifies the irreducible error in the data-generating process. However, its value can be misleadingly inflated if the model is misspecified, for instance, by ignoring measurement error in predictor variables—a common issue in biostatistics. This exercise provides a theoretical framework to dissect the components of residual variance, demonstrating precisely how unaccounted-for predictor error contributes to an inflated $\\hat{\\sigma}$ and a biased understanding of model fit.", "problem": "A calibration study investigates a linear relationship between a laboratory instrument response and a true biomarker concentration subject to measurement error. Let the true concentration be the latent variable $X$, and let the observed surrogate $W$ be measured with additive error $U$, so that $W = X + U$ with $\\operatorname{Var}(U) = \\tau^{2}$. The instrument response $Y$ follows the linear model $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$, where $\\varepsilon$ is independent of $(X,U)$ with $\\operatorname{Var}(\\varepsilon) = \\sigma^{2}$. In a large sample from this study, the following are known or well-estimated from external data and design:\n- The slope parameter is $\\beta_{1} = 1.8$.\n- The residual standard deviation about the true $X$ is $\\sigma = 2.0$ (in fluorescence units).\n- The measurement error variance in $W$ is known from replicates to be $\\tau^{2} = 9$ (in squared concentration units).\n- The marginal variance of the observed surrogate $W$ in the study is $\\operatorname{Var}(W) = 25$.\n\nAssume all standard independence conditions stated above hold and that large-sample limits apply so that sample covariances equal their population counterparts. Consider two regression analyses of $Y$ on the predictor information:\n1. The naive ordinary least squares (OLS) analysis that regresses $Y$ on $W$ and ignores measurement error.\n2. The correctly specified error-in-variables (EIV) analysis that accounts for the known $\\tau^{2}$ and targets residual variation about $X$.\n\nUsing only fundamental definitions of variance, covariance, and the characterization of the OLS linear projection, derive from first principles the large-sample expected difference between the naive OLS residual standard error and the EIV residual standard error. Express your final numerical answer in fluorescence units and round to four significant figures.", "solution": "The objective is to compute the large-sample difference between the naive ordinary least squares (OLS) residual standard error and the correctly specified error-in-variables (EIV) residual standard error. Let $\\sigma_{Y|W}$ denote the naive OLS residual standard error from regressing $Y$ on $W$, and let $\\sigma_{Y|X}$ denote the true residual standard error from the regression of $Y$ on the latent variable $X$. We are asked to calculate $\\sigma_{Y|W} - \\sigma_{Y|X}$.\n\nThe problem provides the following relationships and parameters:\nThe true data generating process is $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$, where $\\operatorname{Var}(\\varepsilon) = \\sigma^{2}$.\nThe observed surrogate for $X$ is $W = X + U$, where $\\operatorname{Var}(U) = \\tau^{2}$.\nThe error term $\\varepsilon$ is independent of both the true predictor $X$ and the measurement error $U$. A standard assumption in classical measurement error models, implied by the problem structure, is that the measurement error $U$ is uncorrelated with the true value $X$, i.e., $\\operatorname{Cov}(X, U) = 0$.\n\nThe given values are:\n- True slope: $\\beta_{1} = 1.8$\n- True residual standard deviation: $\\sigma = 2.0$, which implies the true residual variance is $\\sigma^2 = (2.0)^2 = 4$.\n- Measurement error variance: $\\tau^{2} = 9$.\n- Variance of the observed surrogate: $\\operatorname{Var}(W) = 25$.\n\nFirst, we identify the residual standard error for the correctly specified EIV analysis. This analysis correctly models the relationship between $Y$ and the true predictor $X$. The residuals of this model are the $\\varepsilon$ terms themselves. Therefore, the EIV residual standard error is simply the standard deviation of $\\varepsilon$.\n$$\n\\sigma_{Y|X} = \\sqrt{\\operatorname{Var}(\\varepsilon)} = \\sigma = 2.0\n$$\n\nNext, we derive the residual standard error for the naive OLS analysis, which regresses $Y$ on the observed surrogate $W$. The naive linear model is $Y = \\beta_{0, \\text{naive}} + \\beta_{1, \\text{naive}} W + \\varepsilon_{\\text{naive}}$. The residual standard error for this model is $\\sigma_{Y|W} = \\sqrt{\\operatorname{Var}(\\varepsilon_{\\text{naive}})}$.\n\nThe residual variance in a simple OLS regression is given by the variance of the outcome variable minus the variance explained by the predictor.\n$$\n\\sigma^2_{Y|W} = \\operatorname{Var}(Y) - \\frac{[\\operatorname{Cov}(Y, W)]^2}{\\operatorname{Var}(W)}\n$$\nTo use this formula, we must first derive $\\operatorname{Var}(Y)$ and $\\operatorname{Cov}(Y, W)$ from the given information.\n\n1.  **Derive $\\operatorname{Var}(X)$**:\n    Since $W = X + U$ and we assume $\\operatorname{Cov}(X, U) = 0$, the variance of $W$ is additive:\n    $\\operatorname{Var}(W) = \\operatorname{Var}(X) + \\operatorname{Var}(U) = \\operatorname{Var}(X) + \\tau^{2}$.\n    We can solve for $\\operatorname{Var}(X)$:\n    $$\n    \\operatorname{Var}(X) = \\operatorname{Var}(W) - \\tau^{2} = 25 - 9 = 16\n    $$\n\n2.  **Derive $\\operatorname{Cov}(Y, W)$**:\n    Using the definitions $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$ and $W = X + U$:\n    $$\n    \\operatorname{Cov}(Y, W) = \\operatorname{Cov}(\\beta_{0} + \\beta_{1} X + \\varepsilon, X + U)\n    $$\n    Using the bilinearity of covariance and the independence assumptions ($\\operatorname{Cov}(X, \\varepsilon)=0$, $\\operatorname{Cov}(U, \\varepsilon)=0$, $\\operatorname{Cov}(X, U)=0$):\n    $$\n    \\operatorname{Cov}(Y, W) = \\beta_{1}\\operatorname{Cov}(X, X) + \\beta_{1}\\operatorname{Cov}(X, U) + \\operatorname{Cov}(\\varepsilon, X) + \\operatorname{Cov}(\\varepsilon, U) = \\beta_{1}\\operatorname{Var}(X)\n    $$\n    Substituting the known values:\n    $$\n    \\operatorname{Cov}(Y, W) = (1.8)(16) = 28.8\n    $$\n\n3.  **Derive $\\operatorname{Var}(Y)$**:\n    From $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$, and since $\\varepsilon$ is independent of $X$ ($\\operatorname{Cov}(X, \\varepsilon)=0$):\n    $$\n    \\operatorname{Var}(Y) = \\operatorname{Var}(\\beta_{1} X + \\varepsilon) = \\operatorname{Var}(\\beta_{1} X) + \\operatorname{Var}(\\varepsilon) = \\beta_{1}^{2}\\operatorname{Var}(X) + \\sigma^{2}\n    $$\n    Substituting the known values:\n    $$\n    \\operatorname{Var(Y)} = (1.8)^{2}(16) + (2.0)^{2} = (3.24)(16) + 4 = 51.84 + 4 = 55.84\n    $$\n\n4.  **Calculate the naive residual variance $\\sigma^2_{Y|W}$**:\n    Now substitute these derived quantities back into the formula for the OLS residual variance:\n    $$\n    \\sigma^2_{Y|W} = 55.84 - \\frac{(28.8)^{2}}{25} = 55.84 - \\frac{829.44}{25} = 55.84 - 33.1776 = 22.6624\n    $$\n\nAlternatively, we can derive this from first principles by analyzing the components of the naive residual. The naive OLS slope is:\n$$\n\\beta_{1, \\text{naive}} = \\frac{\\operatorname{Cov}(Y, W)}{\\operatorname{Var}(W)} = \\frac{28.8}{25} = 1.152\n$$\nThe naive residual is $\\varepsilon_{\\text{naive}} = Y - (\\beta_{0, \\text{naive}} + \\beta_{1, \\text{naive}} W)$. Its variance, $\\sigma_{Y|W}^2$, is found by substituting the structural equations for $Y$ and $W$ (ignoring intercepts, which do not affect variance):\n$$\n\\sigma_{Y|W}^2 = \\operatorname{Var}( (\\beta_1 X + \\varepsilon) - \\beta_{1, \\text{naive}} (X+U) ) = \\operatorname{Var}( (\\beta_1 - \\beta_{1, \\text{naive}})X - \\beta_{1, \\text{naive}} U + \\varepsilon )\n$$\nBecause $X$, $U$, and $\\varepsilon$ are mutually uncorrelated, the variance of the sum is the sum of the variances:\n$$\n\\sigma_{Y|W}^2 = (\\beta_1 - \\beta_{1, \\text{naive}})^2 \\operatorname{Var}(X) + (-\\beta_{1, \\text{naive}})^2 \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon)\n$$\nSubstituting the numerical values:\n$$\n\\sigma_{Y|W}^2 = (1.8 - 1.152)^2 (16) + (1.152)^2 (9) + 4\n$$\n$$\n\\sigma_{Y|W}^2 = (0.648)^2 (16) + (1.327104)(9) + 4\n$$\n$$\n\\sigma_{Y|W}^2 = (0.419904)(16) + 11.943936 + 4\n$$\n$$\n\\sigma_{Y|W}^2 = 6.718464 + 11.943936 + 4 = 22.6624\n$$\nThis confirms the previous calculation.\n\n5.  **Calculate the naive residual standard error $\\sigma_{Y|W}$**:\n    $$\n    \\sigma_{Y|W} = \\sqrt{22.6624} \\approx 4.7605041...\n    $$\n\n6.  **Compute the final difference**:\n    The required difference is $\\sigma_{Y|W} - \\sigma_{Y|X}$.\n    $$\n    \\sigma_{Y|W} - \\sigma_{Y|X} = 4.7605041... - 2.0 = 2.7605041...\n    $$\n    Rounding to four significant figures gives $2.761$.\n\nThis difference represents the inflation in the residual standard error caused by ignoring the measurement error in the predictor. The naive residual variance incorporates not only the true model error ($\\sigma^2$) but also variance due to slope attenuation and the propagation of the measurement error $U$ through the biased slope estimate.", "answer": "$$\\boxed{2.761}$$", "id": "4953166"}, {"introduction": "A key distinction in statistical modeling is between a model's performance on the data used to train it and its performance on new, unseen data. The in-sample standard error of the estimate, $\\hat{\\sigma}$, often presents an overly optimistic view of a model's predictive power due to overfitting. This computational practice will guide you through implementing the bootstrap—a powerful, modern resampling technique—to estimate a more realistic out-of-sample predictive error and directly quantify the model's \"optimism bias.\"", "problem": "You will implement a complete, runnable program that quantifies and contrasts two notions of variability for linear regression in a biostatistical prediction setting: an in-sample residual standard error and an out-of-sample predictive residual standard error estimated by a nonparametric bootstrap. Work from the fundamental linear model and sampling definitions, and avoid using any pre-stated shortcut formulas for the target quantities. Your implementation must be self-contained and reproducible.\n\nConsider the ordinary least squares linear model with intercept, where a response vector $y \\in \\mathbb{R}^n$ relates to a design matrix with an intercept column $X_{\\text{tilde}} \\in \\mathbb{R}^{n \\times q}$, where $q = p + 1$ includes $p$ predictors and the intercept. The model assumes $Y = X_{\\text{tilde}} \\beta + \\varepsilon$ with independent errors of mean $0$ and constant variance $\\sigma^2$. Let $\\hat{\\beta}$ be the ordinary least squares estimator, fitted by minimizing the sum of squared residuals, and let fitted values be $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$ and residuals be $e = y - \\hat{y}$. Define the in-sample residual standard error as the square root of an unbiased estimator of the error variance based on the residual degrees of freedom.\n\nDefine the out-of-sample predictive residual standard error as the square root of the expected mean squared prediction error for new observations from the same data-generating mechanism. Because the finite-sample distribution of this predictive loss depends on the unknown population and on estimation variability, you must estimate it using a nonparametric bootstrap-of-pairs with out-of-bag evaluation, according to the following procedure.\n\nYou must implement the following algorithmic steps from first principles.\n\n- In-sample residual standard error.\n  1. Fit the ordinary least squares model with an intercept to the full sample.\n  2. Compute the vector of residuals and the residual degrees of freedom as $n - q$, where $q$ is the number of fitted coefficients including the intercept.\n  3. Form the unbiased estimator of the error variance from the residuals and take its square root to obtain the in-sample residual standard error.\n\n- Out-of-sample bootstrap predictive residual standard error (bootstrap-of-pairs with out-of-bag evaluation).\n  1. For a specified number of bootstrap replicates $B$, repeat:\n     a. Draw a bootstrap sample by sampling $n$ indices with replacement from $\\{1,\\dots,n\\}$.\n     b. Fit the ordinary least squares model with an intercept to the bootstrap sample.\n     c. Identify the out-of-bag set for this replicate as those indices in $\\{1,\\dots,n\\}$ not selected in the bootstrap sample.\n     d. For each out-of-bag index $i$, compute the out-of-bag prediction error as $y_i - \\hat{y}_i^{(b)}$, where $\\hat{y}_i^{(b)}$ is the prediction from the model fitted on the bootstrap sample.\n  2. Aggregate all out-of-bag squared prediction errors across all replicates and divide by the total number of out-of-bag predictions to obtain the out-of-bag mean squared error.\n  3. Take the square root to obtain the bootstrap predictive residual standard error.\n\nYour program must compute, for each test case, the following three quantities:\n- the in-sample residual standard error,\n- the bootstrap predictive residual standard error,\n- the difference between these two values (bootstrap predictive minus in-sample).\n\nAll quantities must be floats.\n\nData generation for the test suite must follow a reproducible design that is fully specified here. For each test case, you must:\n- Set the random seed to the given value using a modern pseudorandom number generator.\n- Generate $X \\in \\mathbb{R}^{n \\times p}$ with independent standard normal entries.\n- Form $X_{\\text{tilde}} = [\\mathbf{1}, X]$, where $\\mathbf{1}$ is a column of ones of length $n$.\n- Generate $Y$ according to $Y = \\beta_0 \\mathbf{1} + X \\beta_{\\text{slopes}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ independently across observations, with all parameters specified below.\n\nTest suite to implement exactly:\n- Case $1$: seed $= 1729$, $n = 80$, $p = 3$, $\\beta_0 = 0.7$, slope vector $\\beta_{\\text{slopes}} = [1.5, -2.0, 0.5]$, $\\sigma = 1.2$, $B = 500$.\n- Case $2$: seed $= 2021$, $n = 25$, $p = 5$, $\\beta_0 = -1.0$, slope vector $\\beta_{\\text{slopes}} = [0.6, -0.4, 1.2, 0.0, 0.3]$, $\\sigma = 0.8$, $B = 600$.\n- Case $3$: seed $= 777$, $n = 15$, $p = 8$, $\\beta_0 = 1.0$, slope vector $\\beta_{\\text{slopes}} = [0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]$, $\\sigma = 1.5$, $B = 800$.\n\nImplementation requirements:\n- Use ordinary least squares with an intercept for all fits. If a bootstrap sample is rank-deficient, use a least-squares solution that produces fitted values minimizing the sum of squared residuals (for example, via a Moore–Penrose pseudoinverse approach).\n- The bootstrap out-of-bag aggregation must count each out-of-bag prediction equally. If a replicate has zero out-of-bag observations, it contributes nothing to the aggregate.\n- Numerical stability and reproducibility must be ensured by using the stated seeds.\n\nFinal output format:\n- For each test case, output a list with three floats in the order specified above.\n- Round each float to exactly $6$ decimal places in the final output.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of the three-element lists, enclosed in square brackets. For example: $[[a\\_1,b\\_1,c\\_1],[a\\_2,b\\_2,c\\_2],[a\\_3,b\\_3,c\\_3]]$, where each $a\\_j$, $b\\_j$, $c\\_j$ is a float rounded to $6$ decimals.", "solution": "This solution presents a computational framework to quantify and compare two distinct measures of regression model variability: the in-sample residual standard error and an out-of-sample predictive residual standard error estimated via a non-parametric bootstrap. The analysis is grounded in the principles of ordinary least squares (OLS) linear regression and resampling-based model validation.\n\n### The Linear Model and Ordinary Least Squares Estimation\n\nWe consider the standard linear regression model, which posits a linear relationship between a response vector $y \\in \\mathbb{R}^n$ and a set of $p$ predictor variables. The model is expressed as:\n$$\nY = X_{\\text{tilde}} \\beta + \\varepsilon\n$$\nwhere:\n- $Y$ is the $n \\times 1$ random vector of responses.\n- $X_{\\text{tilde}}$ is the $n \\times q$ design matrix, which includes a leading column of ones for the intercept and $p$ columns for the predictors. Thus, $q = p + 1$.\n- $\\beta$ is the $q \\times 1$ vector of unknown true coefficients, with $\\beta = [\\beta_0, \\beta_1, \\dots, \\beta_p]^T$.\n- $\\varepsilon$ is the $n \\times 1$ vector of unobserved random errors, with the assumption that $\\varepsilon_i$ are independent and identically distributed with mean $E[\\varepsilon_i] = 0$ and constant variance $Var(\\varepsilon_i) = \\sigma^2$.\n\nThe ordinary least squares (OLS) estimator, denoted $\\hat{\\beta}$, is found by minimizing the residual sum of squares (RSS). The OLS solution for $\\hat{\\beta}$ is given by the normal equations:\n$$\n\\hat{\\beta} = (X_{\\text{tilde}}^T X_{\\text{tilde}})^{-1} X_{\\text{tilde}}^T y\n$$\nIn cases where $X_{\\text{tilde}}$ is not of full column rank (i.e., its columns are linearly dependent), the matrix $X_{\\text{tilde}}^T X_{\\text{tilde}}$ is singular and cannot be inverted. This can occur in bootstrap samples. In such scenarios, a least-squares solution is obtained using the Moore-Penrose pseudoinverse, $(X_{\\text{tilde}}^T X_{\\text{tilde}})^{\\dagger}$, which provides a solution that minimizes the Euclidean norm of the residuals, $\\|y - X_{\\text{tilde}}\\beta\\|_2$.\n\nOnce $\\hat{\\beta}$ is computed, the fitted values are $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$, and the residuals are $e = y - \\hat{y}$.\n\n### In-Sample Residual Standard Error (RSE)\n\nThe in-sample RSE is an estimate of $\\sigma$, the standard deviation of the error term $\\varepsilon$. It quantifies the typical deviation of the observed data points from the fitted regression line based on the same data used to fit the model.\n\n1.  **Residual Sum of Squares (RSS)**: This is the sum of the squared differences between the observed and fitted values:\n    $$\n    RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = e^T e\n    $$\n\n2.  **Unbiased Variance Estimator**: To obtain an unbiased estimator of the error variance $\\sigma^2$, the RSS is divided by the residual degrees of freedom, $df = n - q$. The degrees of freedom are reduced by $q$ to account for the $q$ coefficients estimated from the data.\n    $$\n    \\hat{\\sigma}^2 = \\frac{RSS}{n - q}\n    $$\n\n3.  **Residual Standard Error**: The RSE is the square root of this unbiased variance estimator:\n    $$\n    RSE_{\\text{in-sample}} = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{e^T e}{n - q}}\n    $$\nThe in-sample RSE is often an optimistic (i.e., downwardly biased) estimate of the model's predictive error on new, unseen data, as the model has been optimized to minimize error on the training sample.\n\n### Bootstrap Predictive Residual Standard Error\n\nTo obtain a more realistic estimate of a model's predictive performance, we use a resampling method that simulates the process of fitting the model to new data. The non-parametric bootstrap-of-pairs with out-of-bag (OOB) evaluation is a computationally intensive but powerful technique for this purpose. It estimates the expected prediction error on data points not used in a model's training.\n\nThe algorithm proceeds as follows for $B$ bootstrap replicates:\n\n1.  **Bootstrap Sampling**: For each replicate $b \\in \\{1, \\dots, B\\}$, a bootstrap sample of size $n$ is created by drawing indices with replacement from the original set of indices $\\{1, \\dots, n\\}$. Let the set of chosen indices be $I^{(b)}$. The bootstrap data is $(X_{\\text{tilde}}^{(b)}, y^{(b)})$, corresponding to these indices.\n\n2.  **Model Fitting**: An OLS model is fitted to the bootstrap sample $(X_{\\text{tilde}}^{(b)}, y^{(b)})$, yielding a coefficient vector $\\hat{\\beta}^{(b)}$.\n\n3.  **Out-of-Bag (OOB) Evaluation**: The OOB sample for replicate $b$ consists of the original data points whose indices were not selected in $I^{(b)}$. Let the set of OOB indices be $OOB^{(b)} = \\{1, \\dots, n\\} \\setminus I^{(b)}$.\n    For each index $i \\in OOB^{(b)}$, the model fitted on the bootstrap sample is used to predict the response:\n    $$\n    \\hat{y}_i^{(b)} = \\tilde{x}_i^T \\hat{\\beta}^{(b)}\n    $$\n    where $\\tilde{x}_i^T$ is the $i$-th row of the original design matrix $X_{\\text{tilde}}$. The squared prediction error for this OOB point is $(y_i - \\hat{y}_i^{(b)})^2$.\n\n4.  **Aggregation**: The out-of-bag Mean Squared Error (MSE$_{\\text{OOB}}$) is calculated by averaging all squared prediction errors computed across all OOB sets from all $B$ replicates.\n    $$\n    \\text{MSE}_{\\text{OOB}} = \\frac{\\sum_{b=1}^{B} \\sum_{i \\in OOB^{(b)}} (y_i - \\hat{y}_i^{(b)})^2}{\\sum_{b=1}^{B} |OOB^{(b)}|}\n    $$\n    where $|OOB^{(b)}|$ is the number of out-of-bag observations for replicate $b$.\n\n5.  **Bootstrap Predictive RSE**: The square root of the MSE$_{\\text{OOB}}$ gives the bootstrap-estimated predictive residual standard error.\n    $$\n    RSE_{\\text{pred}} = \\sqrt{\\text{MSE}_{\\text{OOB}}}\n    $$\nThis value provides a less biased estimate of the model's performance on new data.\n\n### Comparison\nThe difference, $\\Delta = RSE_{\\text{pred}} - RSE_{\\text{in-sample}}$, serves as a measure of \"optimism bias.\" A larger positive value indicates a greater degree of overfitting, where the model performs substantially better on the data it was trained on compared to its expected performance on new data. This discrepancy is particularly pronounced in high-dimensional settings where the number of predictors $p$ is large relative to the sample size $n$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and contrasts in-sample and bootstrap out-of-bag predictive \n    residual standard errors for linear regression models.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"seed\": 1729, \"n\": 80, \"p\": 3, \"beta_0\": 0.7, \n            \"beta_slopes\": np.array([1.5, -2.0, 0.5]), \"sigma\": 1.2, \"B\": 500\n        },\n        {\n            \"seed\": 2021, \"n\": 25, \"p\": 5, \"beta_0\": -1.0, \n            \"beta_slopes\": np.array([0.6, -0.4, 1.2, 0.0, 0.3]), \"sigma\": 0.8, \"B\": 600\n        },\n        {\n            \"seed\": 777, \"n\": 15, \"p\": 8, \"beta_0\": 1.0, \n            \"beta_slopes\": np.array([0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]), \"sigma\": 1.5, \"B\": 800\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Unpack parameters\n        seed = case[\"seed\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n        beta_0 = case[\"beta_0\"]\n        beta_slopes = case[\"beta_slopes\"]\n        sigma = case[\"sigma\"]\n        B = case[\"B\"]\n        \n        # Initialize a modern pseudorandom number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X = rng.normal(size=(n, p))\n        X_tilde = np.hstack((np.ones((n, 1)), X))\n        beta_full = np.concatenate(([beta_0], beta_slopes))\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = X_tilde @ beta_full + epsilon\n\n        # --- In-sample residual standard error ---\n        q = p + 1\n        # np.linalg.lstsq handles potential rank-deficiency and returns RSS\n        _, rss_array, _, _ = np.linalg.lstsq(X_tilde, y, rcond=None)\n        rss = rss_array[0]\n        df = n - q\n        \n        # Handle case where df = 0 (e.g., more predictors than samples)\n        # In this situation, the in-sample error is 0, and RSE is undefined or 0.\n        if df > 0:\n            mse_in_sample = rss / df\n            rse_in_sample = np.sqrt(mse_in_sample)\n        else:\n            rse_in_sample = 0.0\n\n\n        # --- Out-of-sample bootstrap predictive residual standard error ---\n        total_oob_sq_err = 0.0\n        total_oob_count = 0\n        all_indices = np.arange(n)\n\n        for _ in range(B):\n            # Draw a bootstrap sample of indices\n            boot_indices = rng.choice(all_indices, size=n, replace=True)\n            \n            # Identify out-of-bag (OOB) indices\n            # unique() is important as bootstrap samples may have duplicates\n            oob_indices = np.setdiff1d(all_indices, np.unique(boot_indices), assume_unique=True)\n\n            if len(oob_indices) == 0:\n                continue\n            \n            # Create bootstrap and OOB data sets\n            X_boot, y_boot = X_tilde[boot_indices], y[boot_indices]\n            X_oob, y_oob = X_tilde[oob_indices], y[oob_indices]\n\n            # Fit OLS model on bootstrap sample\n            # This automatically uses a pseudoinverse for rank-deficient matrices\n            beta_boot, _, _, _ = np.linalg.lstsq(X_boot, y_boot, rcond=None)\n\n            # Predict on OOB data\n            y_pred_oob = X_oob @ beta_boot\n\n            # Accumulate squared errors and counts\n            total_oob_sq_err += np.sum((y_oob - y_pred_oob)**2)\n            total_oob_count += len(oob_indices)\n        \n        # Calculate OOB MSE and the predictive RSE\n        if total_oob_count > 0:\n            mse_oob = total_oob_sq_err / total_oob_count\n            rse_pred = np.sqrt(mse_oob)\n        else:\n            # Fallback if no OOB samples were ever generated (highly unlikely)\n            rse_pred = np.nan\n\n        # Calculate the difference\n        difference = rse_pred - rse_in_sample\n\n        results.append([rse_in_sample, rse_pred, difference])\n\n    # Format the final output string as per requirements\n    output_parts = []\n    for res in results:\n        # rounding to exactly 6 decimal places\n        part = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_str = f\"[{','.join(output_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "4953185"}]}