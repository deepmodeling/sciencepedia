{"hands_on_practices": [{"introduction": "Many foundational statistical methods, from the t-test to linear regression, rely on the assumption of normality. But what happens when this assumption is violated? This first exercise provides a quantitative look at the consequences by comparing the stability of the sample mean for data from a normal distribution versus a heavy-tailed one. By calculating this ratio, you will directly measure the 'cost' of non-normality and understand why diagnostic checks are essential for valid inference. [@problem_id:4894228]", "problem": "A biostatistician is comparing the behavior of the sample mean for two biomarker measurements collected in independent and identically distributed samples of size $n$: one biomarker whose measurement errors are modeled by a heavy-tailed Student's $t$ distribution with degrees of freedom $\\nu=3$, location $0$, and scale parameter $s=1$, and another biomarker modeled by a normal distribution with mean $0$ and variance $1$. Using only general properties of variance, independence, and well-tested distributional facts, compute the ratio of the variance of the sample mean under the heavy-tailed Student's $t$ model to the variance of the sample mean under the normal model. Round your final answer to $4$ significant figures. In your reasoning, briefly explain how this ratio connects to the practical need for normality diagnostics such as quantile-quantile (Q-Q) plots and the Shapiro–Wilk test when assessing biomarker distributions.", "solution": "The problem is deemed valid as it is scientifically grounded in established statistical theory, is well-posed with all necessary information provided, and is free of ambiguities or contradictions.\n\nLet the two sets of biomarker measurements be represented by two independent and identically distributed (i.i.d.) random samples.\n\nThe first sample, $X_1, X_2, \\dots, X_n$, is drawn from a location-scale Student's $t$ distribution. Let's denote the random variable for a single observation as $X$. The parameters are given as:\n- Degrees of freedom: $\\nu = 3$\n- Location parameter: $\\mu = 0$\n- Scale parameter: $s = 1$\n\nThe second sample, $Y_1, Y_2, \\dots, Y_n$, is drawn from a normal distribution. Let's denote the random variable for a single observation as $Y$. The parameters are given as:\n- Mean: $\\mu_Y = 0$\n- Variance: $\\sigma^2_Y = 1$\n\nWe are asked to compute the ratio of the variance of the sample mean for the Student's $t$ model to the variance of the sample mean for the normal model.\n\nLet $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ be the sample mean for the Student's $t$ model.\nLet $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ be the sample mean for the normal model.\n\nThe variance of the sample mean for an i.i.d. sample of size $n$ from a population with variance $\\sigma^2$ is given by the general formula:\n$$\n\\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n}\n$$\nThis is derived from the properties of variance:\n$$\n\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} X_i\\right)\n$$\nSince the observations $X_i$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(\\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} (n \\cdot \\text{Var}(X)) = \\frac{\\text{Var}(X)}{n}\n$$\nSimilarly, for the normal model, $\\text{Var}(\\bar{Y}) = \\frac{\\text{Var}(Y)}{n}$.\n\nThe ratio we need to compute is:\n$$\n\\frac{\\text{Var}(\\bar{X})}{\\text{Var}(\\bar{Y})} = \\frac{\\text{Var}(X)/n}{\\text{Var}(Y)/n} = \\frac{\\text{Var}(X)}{\\text{Var}(Y)}\n$$\nWe must now find the population variances, $\\text{Var}(X)$ and $\\text{Var}(Y)$.\n\nFor the normal model, the variance is explicitly given as $\\text{Var}(Y) = \\sigma^2_Y = 1$.\n\nFor the Student's $t$ model, we need to calculate the variance from the given parameters. A random variable $X$ that follows a location-scale Student's $t$ distribution can be expressed as $X = \\mu + s \\cdot T$, where $T$ is a random variable from the standard Student's $t$ distribution with $\\nu$ degrees of freedom. The variance of the standard Student's $t$ distribution, $\\text{Var}(T)$, is defined for $\\nu > 2$ and is given by:\n$$\n\\text{Var}(T) = \\frac{\\nu}{\\nu - 2}\n$$\nThe variance of the location-scale variable $X$ is then:\n$$\n\\text{Var}(X) = \\text{Var}(\\mu + s \\cdot T) = s^2 \\cdot \\text{Var}(T)\n$$\nIn our problem, we have $\\nu = 3$, $\\mu = 0$, and $s = 1$. Since $\\nu=3 > 2$, the variance is well-defined.\nFirst, we find the variance of the standard $t$-distribution with $\\nu=3$ degrees of freedom:\n$$\n\\text{Var}(T) = \\frac{3}{3 - 2} = \\frac{3}{1} = 3\n$$\nNow, we find the variance of our specific biomarker measurement variable $X$:\n$$\n\\text{Var}(X) = s^2 \\cdot \\text{Var}(T) = 1^2 \\cdot 3 = 3\n$$\nSo, the population variance for the heavy-tailed model is $\\text{Var}(X) = 3$.\n\nNow we can compute the required ratio:\n$$\n\\frac{\\text{Var}(\\bar{X})}{\\text{Var}(\\bar{Y})} = \\frac{\\text{Var}(X)}{\\text{Var}(Y)} = \\frac{3}{1} = 3\n$$\nThe problem requires the answer to be rounded to $4$ significant figures. The exact answer is $3$, which can be written as $3.000$.\n\nThe practical connection of this ratio to normality diagnostics is profound. The ratio of $3$ demonstrates that for a sample of any size $n$, the sample mean derived from the heavy-tailed $t(\\nu=3)$ distribution has a variance three times larger than that of a sample mean from a standard normal distribution. This is despite both parent distributions being symmetric and centered at $0$. The higher variance is a direct consequence of the \"heavy tails\" of the Student's $t$ distribution, which allows for more frequent extreme observations (outliers) compared to the normal distribution.\nIn practice, many classical statistical procedures, such as the $t$-test or ANOVA, rely on the assumption of normality. If a researcher assumes normality when the underlying data are in fact heavy-tailed, they will be using a model ($\\sigma^2_{\\text{pop}} = 1$) that dramatically underestimates the true variability of their sample mean ($\\sigma^2_{\\text{pop}} = 3$). This leads to invalid statistical inferences: confidence intervals for the mean will be erroneously narrow, and hypothesis tests will have an inflated Type I error rate, leading to false-positive findings.\nThis is precisely why diagnostics for assessing normality are a critical step in data analysis.\n- **Quantile-Quantile (Q-Q) plots** are a graphical tool to check for normality. They plot the quantiles of the sample data against the theoretical quantiles of a normal distribution. For data from a heavy-tailed distribution like our $t(\\nu=3)$ case, the points on the Q-Q plot will deviate systematically from the straight reference line, particularly at the ends, visually revealing the presence of heavier-than-normal tails.\n- **The Shapiro–Wilk test** is a formal hypothesis test where the null hypothesis is that the data are drawn from a normally distributed population. For data from a heavy-tailed distribution, this test would likely yield a low $p$-value, leading to a rejection of the normality assumption.\nBy using these tools, a biostatistician can identify departures from normality, such as the heavy-tailedness explored here. This detection allows them to switch to more appropriate statistical methods, such as robust statistics or non-parametric tests, that are less sensitive to outliers, thereby ensuring the validity and reliability of their scientific conclusions. The calculated ratio of $3$ quantifies the high cost of failing to do so.", "answer": "$$\n\\boxed{3.000}\n$$", "id": "4894228"}, {"introduction": "Once we appreciate the need to check for normality, we must understand the tools we use. The Shapiro-Wilk test is a workhorse for this task, but how does it behave in the face of outliers? This practice challenges you to derive the test statistic's behavior in the presence of an extreme observation, revealing its sensitivity and introducing the critical concept of statistical robustness. [@problem_id:4894182]", "problem": "A biostatistics researcher is assessing normality of a univariate biomarker using the Shapiro–Wilk test. Let $x_{(1)} \\leq x_{(2)} \\leq \\dots \\leq x_{(n)}$ denote the ordered sample of size $n \\geq 5$. The Shapiro–Wilk statistic $W$ is based on a linear contrast of the order statistics and is defined by the well-tested construction:\n$$\nW \\;=\\; \\frac{\\left(\\sum_{i=1}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right)\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i} - \\bar{x}\\right)^{2}},\n$$\nwhere $m = \\lfloor n/2 \\rfloor$, the weights $a_{i}$ depend only on $n$ through the expected values and covariance structure of the order statistics of a standard normal distribution, and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$ is the sample mean. Assume $a_{1} \\geq a_{2} \\geq \\dots \\geq a_{m}  0$.\n\nSuppose a single extreme observation occurs in the upper tail, replacing $x_{(n)}$ by a value $t$ that increases without bound while all other observations remain fixed and finite. Define $W(t)$ as the Shapiro–Wilk statistic computed on the sample with $x_{(n)}=t$.\n\nUsing only the fundamental definitions above, derive the closed-form analytic expression for the limit\n$$\n\\lim_{t \\to \\infty} W(t)\n$$\nin terms of $n$ and $a_{1}$, where $a_{1}$ is the largest Shapiro–Wilk weight in the contrast. Express your final answer as a single analytic expression. No rounding is required.\n\nBriefly propose at least two robust alternatives for assessing normality that mitigate the influence of a single extreme observation, but note that these proposals do not affect the required final expression for the limit.", "solution": "The problem as stated is mathematically well-defined, statistically relevant, and scientifically sound. It is a rigorous examination of the asymptotic behavior of the Shapiro-Wilk statistic under the influence of an extreme outlier. All provided definitions are standard and correct. We may therefore proceed with a formal derivation to determine the specified limit.\n\nThe problem requires us to find the limit of the Shapiro–Wilk statistic, $W(t)$, as a single extreme observation $t$ in the upper tail of a sample goes to infinity. The sample consists of $n-1$ fixed, finite observations, which we can denote as $\\{y_1, y_2, \\dots, y_{n-1}\\}$, and the variable observation $t$. As $t \\to \\infty$, it will eventually exceed all fixed observations, so we can set $x_{(n)} = t$. The remaining order statistics, $x_{(1)}, \\ldots, x_{(n-1)}$, are the sorted values of the fixed set $\\{y_1, y_2, \\dots, y_{n-1}\\}$ and are thus constant with respect to $t$.\n\nThe Shapiro–Wilk statistic is given by:\n$$ W(t) \\;=\\; \\frac{\\left(\\sum_{i=1}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right)\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i} - \\bar{x}\\right)^{2}} $$\nwhere $m = \\lfloor n/2 \\rfloor$ and the sample under consideration is $\\{y_1, \\dots, y_{n-1}, t\\}$. We will analyze the asymptotic behavior of the numerator and the denominator separately as $t \\to \\infty$.\n\n**Analysis of the Numerator**\n\nLet $N(t)$ denote the numerator of $W(t)$. The term inside the square is the linear contrast, which we denote by $S(t)$:\n$$ S(t) = \\sum_{i=1}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nWe can separate the term involving $x_{(n)}$ from the sum. This occurs when $i=1$, as $x_{(n+1-1)} = x_{(n)}$.\n$$ S(t) = a_{1}\\left(x_{(n)} - x_{(1)}\\right) + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nSubstituting $x_{(n)} = t$, we have:\n$$ S(t) = a_{1}\\left(t - x_{(1)}\\right) + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nThe order statistics $x_{(1)}, x_{(2)}, \\dots, x_{(n-1)}$ are fixed values, independent of $t$. The weights $a_i$ are also constant for a given $n$. Therefore, the expression can be written as a linear function of $t$:\n$$ S(t) = a_{1}t + \\left( -a_{1}x_{(1)} + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) \\right) $$\nLet $C_{\\text{num}}$ be the constant term:\n$$ C_{\\text{num}} = -a_{1}x_{(1)} + \\sum_{i=2}^{m} a_{i}\\left(x_{(n+1-i)} - x_{(i)}\\right) $$\nThus, $S(t) = a_1t + C_{\\text{num}}$. The numerator is $N(t) = (S(t))^2 = (a_1t + C_{\\text{num}})^2$.\nAs $t \\to \\infty$, $N(t)$ is a quadratic polynomial in $t$. The leading term determines its asymptotic behavior:\n$$ N(t) = a_1^2 t^2 + 2a_1 C_{\\text{num}} t + C_{\\text{num}}^2 $$\nThe dominant term for large $t$ is $a_1^2 t^2$.\n\n**Analysis of the Denominator**\n\nLet $D(t)$ denote the denominator, which is the sum of squared deviations from the sample mean $\\bar{x}$.\n$$ D(t) = \\sum_{j=1}^{n}\\left(x_{j} - \\bar{x}\\right)^{2} $$\nThe sample mean, $\\bar{x}(t)$, depends on $t$:\n$$ \\bar{x}(t) = \\frac{1}{n} \\left( \\sum_{j=1}^{n-1} y_j + t \\right) $$\nLet $S_{n-1} = \\sum_{j=1}^{n-1} y_j$. Since the $y_j$ values are fixed, $S_{n-1}$ is a constant.\n$$ \\bar{x}(t) = \\frac{S_{n-1} + t}{n} $$\nFor analytical convenience, we use the identity for the sum of squares:\n$$ D(t) = \\sum_{j=1}^{n} x_j^2 - n(\\bar{x}(t))^2 $$\nThe sum of squares of the observations is:\n$$ \\sum_{j=1}^{n} x_j^2 = \\sum_{j=1}^{n-1} y_j^2 + t^2 $$\nLet $S_{sq, n-1} = \\sum_{j=1}^{n-1} y_j^2$, which is also a constant.\nSo, $\\sum_{j=1}^{n} x_j^2 = S_{sq, n-1} + t^2$.\nNow, substitute the expressions for $\\sum x_j^2$ and $\\bar{x}(t)$ into the formula for $D(t)$:\n$$ D(t) = (S_{sq, n-1} + t^2) - n\\left(\\frac{S_{n-1} + t}{n}\\right)^2 $$\n$$ D(t) = (S_{sq, n-1} + t^2) - \\frac{1}{n}(S_{n-1} + t)^2 $$\n$$ D(t) = S_{sq, n-1} + t^2 - \\frac{1}{n}(S_{n-1}^2 + 2S_{n-1}t + t^2) $$\nCollecting terms by powers of $t$:\n$$ D(t) = \\left(1 - \\frac{1}{n}\\right)t^2 - \\frac{2S_{n-1}}{n}t + \\left(S_{sq, n-1} - \\frac{S_{n-1}^2}{n}\\right) $$\n$$ D(t) = \\frac{n-1}{n}t^2 - \\frac{2S_{n-1}}{n}t + C_{\\text{den}} $$\nwhere $C_{\\text{den}} = S_{sq, n-1} - \\frac{S_{n-1}^2}{n}$ is a constant. As $t \\to \\infty$, $D(t)$ is also a quadratic polynomial in $t$. Its asymptotic behavior is governed by the leading term $\\frac{n-1}{n}t^2$.\n\n**Calculation of the Limit**\n\nWe now compute the limit of $W(t) = N(t)/D(t)$ as $t \\to \\infty$. This is the ratio of two quadratic polynomials in $t$. The limit is the ratio of their leading coefficients.\n$$ \\lim_{t \\to \\infty} W(t) = \\lim_{t \\to \\infty} \\frac{a_1^2 t^2 + 2a_1 C_{\\text{num}} t + C_{\\text{num}}^2}{\\frac{n-1}{n}t^2 - \\frac{2S_{n-1}}{n}t + C_{\\text{den}}} $$\nTo formalize this, we divide the numerator and denominator by $t^2$, the highest power of $t$:\n$$ \\lim_{t \\to \\infty} W(t) = \\lim_{t \\to \\infty} \\frac{a_1^2 + \\frac{2a_1 C_{\\text{num}}}{t} + \\frac{C_{\\text{num}}^2}{t^2}}{\\frac{n-1}{n} - \\frac{2S_{n-1}}{nt} + \\frac{C_{\\text{den}}}{t^2}} $$\nAs $t \\to \\infty$, all terms with $t$ or $t^2$ in the denominator approach zero:\n$$ \\lim_{t \\to \\infty} W(t) = \\frac{a_1^2 + 0 + 0}{\\frac{n-1}{n} - 0 + 0} = \\frac{a_1^2}{\\frac{n-1}{n}} $$\nSimplifying this expression gives the final result:\n$$ \\lim_{t \\to \\infty} W(t) = \\frac{n a_1^2}{n-1} $$\nThis result depends only on the sample size $n$ and the largest weight coefficient $a_1$, as required.\n\n**Robust Alternatives for Assessing Normality**\n\nThe calculation above demonstrates that a single extreme observation causes the Shapiro-Wilk statistic to converge to a specific non-zero, non-unity value, typically leading to a rejection of the null hypothesis of normality. This highlights the test's lack of robustness. To mitigate the influence of such outliers, one can employ robust methods for assessing normality. Two such alternatives are:\n\n$1$. **Quantile-Quantile (QQ) Plot with Robust Scaling**: Standard QQ-plots are susceptible to distortion by outliers, which can affect the visual scaling of the plot and the interpretation of the pattern for the bulk of the data. A more robust approach involves standardizing the data not with the sample mean and standard deviation, but with robust estimators of location and scale, such as the median and the Median Absolute Deviation (MAD). The transformed data points $z_i = (x_i - \\text{median}(X)) / \\text{MAD}(X)$ are then plotted against the quantiles of a standard normal distribution. This method contains the influence of the outlier and provides a clearer picture of the normality of the central data.\n\n$2$. **Normality Testing on a Trimmed Sample**: Another strategy is to apply a normality test, such as the Shapiro-Wilk test itself, to a trimmed version of the data. For example, a $5\\%$ symmetrically trimmed sample would involve removing the lowest $5\\%$ and highest $5\\%$ of the observations before computing the test statistic. This explicitly removes extreme values, allowing the test to assess the distributional properties of the core, or bulk, of the data, providing a more robust test of normality for the underlying population, assuming the outliers are contaminants.", "answer": "$$\\boxed{\\frac{n a_{1}^{2}}{n-1}}$$", "id": "4894182"}, {"introduction": "Identifying non-normality is diagnostic; correcting it is therapeutic. What can we do when our data is not normally distributed? This hands-on coding exercise guides you through applying the Box-Cox transformation, a powerful technique to stabilize variance and make data more-normal. You will build a complete pipeline to find the optimal transformation and verify its success, moving from problem detection to a practical solution. [@problem_id:4894199]", "problem": "You are given four strictly positive biomarker datasets. Your task is to implement an end-to-end normality assessment pipeline that, for each dataset, computes the optimal Box-Cox transformation parameter $\\hat{\\lambda}$ by maximizing the normal profile log-likelihood, transforms the data with the estimated $\\hat{\\lambda}$, and then assesses normality using a quantile-quantile (Q-Q) correlation summary and the Shapiro-Wilk (SW) test.\n\nBase definitions and assumptions:\n- For strictly positive measurements $x_1,\\dots,x_n$, the Box-Cox transformation is defined by\n$$\nT_{\\lambda}(x)=\n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0,\\\\\n\\log(x),  \\lambda = 0.\n\\end{cases}\n$$\n- Assume there exists a $\\lambda$ such that $Z_i = T_{\\lambda}(x_i)$ are independent and identically distributed as a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n- The optimal $\\hat{\\lambda}$ is the maximizer of the normal profile log-likelihood in $\\lambda$ under this model.\n\nQuantifying the Q-Q linearity without plotting:\n- Let $z_{(1)} \\le \\dots \\le z_{(n)}$ denote the order statistics of the transformed sample $z_i = T_{\\hat{\\lambda}}(x_i)$.\n- Define Blom plotting positions $p_i = \\dfrac{i - 0.375}{n + 0.25}$ for $i=1,\\dots,n$, and let $q_i = \\Phi^{-1}(p_i)$ where $\\Phi^{-1}$ is the inverse cumulative distribution function of the standard Normal distribution.\n- Compute the Pearson correlation between the vectors $(z_{(1)},\\dots,z_{(n)})$ and $(q_1,\\dots,q_n)$; denote this correlation by $r_{\\text{QQ}}$. Values of $r_{\\text{QQ}}$ closer to $1$ indicate stronger linearity in the Q-Q relationship.\n\nNormality test:\n- Apply the Shapiro-Wilk test to the transformed data $\\{z_i\\}$ and record the test’s $p$-value. Use significance level $\\alpha = 0.05$ and declare “normal” if the $p$-value is greater than or equal to $\\alpha$.\n\nYour program must:\n- For each dataset, compute $\\hat{\\lambda}$ by maximizing the normal profile log-likelihood in $\\lambda$ over a bounded interval, transform the data with $T_{\\hat{\\lambda}}$, compute $r_{\\text{QQ}}$, compute the Shapiro–Wilk $p$-value, and finally report a boolean normality decision using $\\alpha = 0.05$.\n- Round each real-valued output ($\\hat{\\lambda}$, $p$-value, $r_{\\text{QQ}}$) to $4$ decimal places.\n\nTest suite (datasets):\nAll datasets use $n = 30$ points with $i \\in \\{1,\\dots,30\\}$ and $p_i^{(u)} = \\dfrac{i - 0.5}{n}$. Let $\\Phi^{-1}$ denote the inverse cumulative distribution function of the standard Normal distribution, and let $G^{-1}_{k,\\theta}$ denote the inverse cumulative distribution function of the Gamma distribution with shape $k$ and scale $\\theta$.\n\n- Dataset $\\mathcal{D}_1$ (log-normal-like): $x_i^{(1)} = \\exp\\!\\big(2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$.\n- Dataset $\\mathcal{D}_2$ (approximately Normal but positive): $x_i^{(2)} = 10.0 + 1.2 \\cdot \\Phi^{-1}(p_i^{(u)})$.\n- Dataset $\\mathcal{D}_3$ (Gamma-like skew): $x_i^{(3)} = G^{-1}_{2.0,\\,3.0}(p_i^{(u)})$.\n- Dataset $\\mathcal{D}_4$ (small-magnitude positive): $x_i^{(4)} = 0.02 + \\exp\\!\\big(-2.0 + 0.5 \\cdot \\Phi^{-1}(p_i^{(u)})\\big)$.\n\nOutput specification:\n- For each dataset $\\mathcal{D}_j$ with $j \\in \\{1,2,3,4\\}$, return a list containing [$\\hat{\\lambda}_j$, $p_j$, $r_{\\text{QQ},j}$, `normal_j`] where $\\hat{\\lambda}_j$ is the estimated Box-Cox parameter, $p_j$ is the Shapiro–Wilk $p$-value on the transformed data, $r_{\\text{QQ},j}$ is the Q-Q correlation, and `normal_j` is a boolean that is true if $p_j \\ge 0.05$ and false otherwise.\n- Round $\\hat{\\lambda}_j$, $p_j$, and $r_{\\text{QQ},j}$ to $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must be of the form:\n`[[`$\\hat{\\lambda}_1$`, `$p_1$`, `$r_{\\text{QQ},1}$`, `normal_1`], [`$\\hat{\\lambda}_2$`, `$p_2$`, `$r_{\\text{QQ},2}$`, `normal_2`], [`$\\hat{\\lambda}_3$`, `$p_3$`, `$r_{\\text{QQ},3}$`, `normal_3`], [`$\\hat{\\lambda}_4$`, `$p_4$`, `$r_{\\text{QQ},4}$`, `normal_4`]]`.", "solution": "The user has provided a valid, well-posed problem in biostatistics. The task is to construct and apply a statistical pipeline to assess the normality of four given datasets, with a preliminary Box-Cox transformation step to improve normality. The solution proceeds by first detailing the theoretical underpinnings of the required methods and then implementing them computationally.\n\n### 1. Theoretical Framework\n\n#### 1.1. Box-Cox Transformation and Normality Assumption\nGiven a set of strictly positive data points $x_1, x_2, \\dots, x_n$, the Box-Cox transformation is a power transformation parameterized by $\\lambda$. It is defined as:\n$$\nT_{\\lambda}(x) =\n\\begin{cases}\n\\frac{x^{\\lambda}-1}{\\lambda},  \\text{if } \\lambda \\neq 0, \\\\\n\\log(x),  \\text{if } \\lambda = 0.\n\\end{cases}\n$$\nThe case for $\\lambda=0$ is the limit of the expression for $\\lambda \\neq 0$ as $\\lambda \\to 0$, which can be shown using L'Hôpital's rule. The core assumption of the procedure is that for some value of $\\lambda$, the transformed variables $Z_i = T_{\\lambda}(x_i)$ are independent and identically distributed (i.i.d.) samples from a normal distribution, $N(\\mu, \\sigma^2)$.\n\n#### 1.2. Estimation of the Optimal Parameter $\\hat{\\lambda}$\nThe optimal parameter $\\hat{\\lambda}$ is determined by the method of maximum likelihood. The probability density function of the original data $x_i$ can be found from the density of the transformed data $z_i$ using the change of variables formula. The likelihood function for the original observations $\\mathbf{x} = (x_1, \\dots, x_n)$ given the parameters $\\mu$, $\\sigma^2$, and $\\lambda$ is:\n$$\nL(\\mu, \\sigma^2, \\lambda | \\mathbf{x}) = \\left( \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(T_\\lambda(x_i) - \\mu)^2}{2\\sigma^2}\\right) \\right) \\cdot J(\\lambda; \\mathbf{x})\n$$\nThe term $J(\\lambda; \\mathbf{x})$ is the Jacobian of the transformation from $\\mathbf{x}$ to $\\mathbf{z}$:\n$$\nJ(\\lambda; \\mathbf{x}) = \\prod_{i=1}^{n} \\left| \\frac{d T_\\lambda(x_i)}{d x_i} \\right| = \\prod_{i=1}^{n} x_i^{\\lambda-1}\n$$\nThe log-likelihood function is therefore:\n$$\n\\ell(\\mu, \\sigma^2, \\lambda | \\mathbf{x}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (T_\\lambda(x_i) - \\mu)^2 + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\nFor a fixed $\\lambda$, the maximum likelihood estimators (MLEs) for $\\mu$ and $\\sigma^2$ are the sample mean and variance of the transformed data $z_i(\\lambda) = T_\\lambda(x_i)$:\n$$\n\\hat{\\mu}(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} z_i(\\lambda) = \\bar{z}(\\lambda)\n$$\n$$\n\\hat{\\sigma}^2(\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} (z_i(\\lambda) - \\bar{z}(\\lambda))^2\n$$\nSubstituting these back into the log-likelihood function yields the profile log-likelihood for $\\lambda$:\n$$\n\\ell_p(\\lambda | \\mathbf{x}) = -\\frac{n}{2}\\log(2\\pi) -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) - \\frac{n}{2} + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\nTo find the optimal $\\hat{\\lambda}$, we maximize $\\ell_p(\\lambda | \\mathbf{x})$ with respect to $\\lambda$. Since the terms $-\\frac{n}{2}\\log(2\\pi)$ and $-\\frac{n}{2}$ are constant with respect to $\\lambda$, this is equivalent to maximizing the simplified function:\n$$\nL_{\\text{profile}}(\\lambda) = -\\frac{n}{2}\\log(\\hat{\\sigma}^2(\\lambda)) + (\\lambda-1)\\sum_{i=1}^{n} \\log(x_i)\n$$\nThis maximization will be performed numerically over a bounded interval, for instance $\\lambda \\in [-5, 5]$, by minimizing the negative of this function.\n\n#### 1.3. Assessment of Normality of Transformed Data\n\nOnce $\\hat{\\lambda}$ is found, the data are transformed as $z_i = T_{\\hat{\\lambda}}(x_i)$. The normality of this new sample $\\{z_i\\}$ is then assessed using two methods.\n\n**Quantile-Quantile (Q-Q) Correlation, $r_{\\text{QQ}}$:**\nThis metric quantifies the linearity of a Q-Q plot. It involves comparing the ordered data with the theoretical quantiles of a standard normal distribution.\n1.  Compute the order statistics of the transformed data: $z_{(1)} \\le z_{(2)} \\le \\dots \\le z_{(n)}$.\n2.  Calculate the Blom plotting positions: $p_i = \\frac{i - 0.375}{n + 0.25}$ for $i=1, \\dots, n$.\n3.  Determine the corresponding theoretical quantiles from the standard normal distribution: $q_i = \\Phi^{-1}(p_i)$, where $\\Phi^{-1}$ is the inverse cumulative distribution function (CDF) of $N(0, 1)$.\n4.  Compute the Pearson correlation coefficient between the vector of ordered statistics $\\mathbf{z}_{\\text{ord}} = (z_{(1)}, \\dots, z_{(n)})$ and the vector of theoretical quantiles $\\mathbf{q} = (q_1, \\dots, q_n)$. This correlation is denoted $r_{\\text{QQ}}$. A value of $r_{\\text{QQ}}$ close to $1$ indicates a strong linear relationship, supporting the hypothesis of normality.\n\n**Shapiro-Wilk (SW) Test:**\nThe Shapiro-Wilk test is a formal hypothesis test for normality. The null hypothesis ($H_0$) is that the data sample comes from a normally distributed population. The test statistic is calculated and converted to a $p$-value.\n-   If the $p$-value is less than a chosen significance level $\\alpha$ (here, $\\alpha=0.05$), $H_0$ is rejected, and the data are considered not normally distributed.\n-   If the $p$-value is greater than or equal to $\\alpha$, there is insufficient evidence to reject $H_0$, and the data are declared consistent with a normal distribution for the purpose of this problem.\n\n### 2. Computational Procedure\n\nThe following steps are performed for each of the four datasets provided:\n\n1.  **Generate Dataset**: The data points for each dataset $\\mathcal{D}_j$ are generated according to the specified formulas.\n2.  **Estimate $\\hat{\\lambda}$**: The negative of the profile log-likelihood, $-L_{\\text{profile}}(\\lambda)$, is defined as an objective function. A numerical optimization routine (`scipy.optimize.minimize_scalar`) is used to find the value $\\hat{\\lambda}$ that minimizes this function within a bounded interval like $[-5, 5]$.\n3.  **Transform Data**: The original dataset $x_i$ is transformed into $z_i = T_{\\hat{\\lambda}}(x_i)$ using the estimated $\\hat{\\lambda}$.\n4.  **Calculate $r_{\\text{QQ}}$**: The Q-Q correlation is computed for the transformed data $\\{z_i\\}$ as described in Section 1.3.\n5.  **Perform Shapiro-Wilk Test**: The Shapiro-Wilk test is applied to the transformed data $\\{z_i\\}$ to obtain a $p$-value.\n6.  **Report Results**: A boolean decision on normality is made based on whether the $p$-value is $\\ge 0.05$. The final results for the dataset—$\\hat{\\lambda}$, the SW $p$-value, $r_{\\text{QQ}}$, and the normality decision—are collected. Real values are rounded to four decimal places.\n7.  **Final Output**: The collected results for all four datasets are assembled into a list of lists and printed in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats, optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform normality assessment on four datasets.\n    \"\"\"\n\n    def generate_datasets():\n        \"\"\"Generates the four datasets as specified in the problem.\"\"\"\n        n = 30\n        i = np.arange(1, n + 1)\n        p_u = (i - 0.5) / n\n        norm_quantiles = stats.norm.ppf(p_u)\n\n        # Dataset D1 (log-normal-like)\n        d1 = np.exp(2.0 + 0.5 * norm_quantiles)\n\n        # Dataset D2 (approximately Normal but positive)\n        d2 = 10.0 + 1.2 * norm_quantiles\n\n        # Dataset D3 (Gamma-like skew)\n        gamma_quantiles = stats.gamma.ppf(p_u, a=2.0, scale=3.0)\n        d3 = gamma_quantiles\n\n        # Dataset D4 (small-magnitude positive)\n        d4 = 0.02 + np.exp(-2.0 + 0.5 * norm_quantiles)\n\n        return [d1, d2, d3, d4]\n\n    def boxcox_transform(x, lam):\n        \"\"\"Applies the Box-Cox transformation.\"\"\"\n        if lam == 0:\n            return np.log(x)\n        else:\n            return (x**lam - 1) / lam\n\n    def profile_log_likelihood(lam, x):\n        \"\"\"\n        Computes the profile log-likelihood for a given lambda.\n        Note: The function returns the *negative* for minimization purposes.\n        \"\"\"\n        n = len(x)\n        z = boxcox_transform(x, lam)\n        # Using n-ddof=0 for MLE of variance (division by n)\n        log_lik = -n / 2 * np.log(np.var(z, ddof=0)) + (lam - 1) * np.sum(np.log(x))\n        return -log_lik\n\n    def analyze_dataset(x):\n        \"\"\"\n        Performs the full analysis pipeline for a single dataset.\n        \"\"\"\n        # 1. Compute optimal lambda\n        # We use a bounded search for robustness, as is common practice.\n        res = optimize.minimize_scalar(\n            profile_log_likelihood, \n            args=(x,), \n            bounds=(-5, 5), \n            method='bounded'\n        )\n        lambda_hat = res.x\n\n        # 2. Transform the data\n        z_transformed = boxcox_transform(x, lambda_hat)\n\n        # 3. Compute Q-Q correlation (r_QQ)\n        n = len(z_transformed)\n        z_ordered = np.sort(z_transformed)\n        i = np.arange(1, n + 1)\n        p_blom = (i - 0.375) / (n + 0.25)\n        q_theoretical = stats.norm.ppf(p_blom)\n        r_qq = np.corrcoef(z_ordered, q_theoretical)[0, 1]\n\n        # 4. Compute Shapiro-Wilk test p-value\n        _, p_value = stats.shapiro(z_transformed)\n\n        # 5. Make normality decision\n        alpha = 0.05\n        is_normal = p_value = alpha\n\n        # 6. Format results\n        return [\n            round(lambda_hat, 4),\n            round(p_value, 4),\n            round(r_qq, 4),\n            is_normal\n        ]\n\n    datasets = generate_datasets()\n    all_results = [analyze_dataset(d) for d in datasets]\n    \n    # Final print statement in the exact required format.\n    # The str() representation of a list includes spaces after commas,\n    # which matches the visual style of the problem's output format example.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "4894199"}]}