{"hands_on_practices": [{"introduction": "The Pearson correlation coefficient, $r$, is a cornerstone of biostatistics for quantifying the strength and direction of a linear relationship between two continuous variables. While statistical software calculates this value effortlessly, understanding its derivation from fundamental summary statistics is essential for appreciating its structure. This practice [@problem_id:4897905] guides you through deriving the computational formula for $r$ from summary data and applying it, reinforcing the mathematical foundation of correlation.", "problem": "A cardiovascular cohort study investigates bivariate association between low-density lipoprotein cholesterol and systemic inflammation. For each of $n$ adults, let $x_i$ denote low-density lipoprotein cholesterol measured in milligrams per deciliter and $y_i$ denote high-sensitivity C-reactive protein measured in milligrams per liter. Suppose that for a dataset of $n = 40$ participants, the following sufficient summaries are available from the data system: $\\sum_{i=1}^{n} x_i = 5200$, $\\sum_{i=1}^{n} y_i = 80$, $\\sum_{i=1}^{n} x_i^2 = 712000$, $\\sum_{i=1}^{n} y_i^2 = 217.6$, and $\\sum_{i=1}^{n} x_i y_i = 10976$.\n\nStarting from the foundational definitions of the sample mean $\\bar{x}$ and $\\bar{y}$, the sample covariance $s_{xy}$, and the sample standard deviations $s_x$ and $s_y$, derive a formula for the Pearson product-moment correlation coefficient $r$ that is expressed only in terms of $n$, $\\sum x_i$, $\\sum y_i$, $\\sum x_i^2$, $\\sum y_i^2$, and $\\sum x_i y_i$. Then, using this derivation, explain how to obtain numerically stable expressions that avoid catastrophic cancellation by appropriate centering and show their equivalence to the formula in terms of the provided summaries. Finally, compute the numerical value of $r$ using the given summaries. Round your answer to four significant figures. Provide the final answer as a unitless correlation coefficient.", "solution": "The problem requires the derivation of a computational formula for the Pearson product-moment correlation coefficient, $r$, an explanation of numerical stability, and the calculation of $r$ from provided summary statistics. The problem is statistically and mathematically well-posed and contains all necessary information.\n\nThe foundational definition of the Pearson product-moment correlation coefficient, $r$, is the ratio of the sample covariance, $s_{xy}$, to the product of the sample standard deviations, $s_x$ and $s_y$.\n$$\nr = \\frac{s_{xy}}{s_x s_y}\n$$\nThe sample covariance is defined as:\n$$\ns_{xy} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n$$\nThe sample standard deviations are defined as:\n$$\ns_x = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\quad \\text{and} \\quad s_y = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n$$\nwhere $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$ are the sample means.\n\nSubstituting these definitions into the formula for $r$:\n$$\nr = \\frac{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n$$\nThe factor $\\frac{1}{n-1}$ cancels from the numerator and denominator, yielding:\n$$\nr = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n$$\nThe terms in this expression are the sum of corrected cross-products ($SS_{xy}$), the sum of corrected squares for $x$ ($SS_{xx}$), and the sum of corrected squares for $y$ ($SS_{yy}$).\n$$\nSS_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) \\\\\nSS_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\\\\nSS_{yy} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n$$\nTo derive a formula in terms of the given summary statistics, we expand each of these terms.\n\nFor $SS_{xy}$:\n$$\nSS_{xy} = \\sum_{i=1}^{n} (x_i y_i - x_i \\bar{y} - y_i \\bar{x} + \\bar{x}\\bar{y}) \\\\\n= \\sum_{i=1}^{n} x_i y_i - \\bar{y}\\sum_{i=1}^{n} x_i - \\bar{x}\\sum_{i=1}^{n} y_i + \\sum_{i=1}^{n} \\bar{x}\\bar{y} \\\\\n= \\sum_{i=1}^{n} x_i y_i - \\left(\\frac{\\sum y_i}{n}\\right) (\\sum x_i) - \\left(\\frac{\\sum x_i}{n}\\right) (\\sum y_i) + n\\left(\\frac{\\sum x_i}{n}\\right)\\left(\\frac{\\sum y_i}{n}\\right) \\\\\n= \\sum_{i=1}^{n} x_i y_i - \\frac{(\\sum x_i)(\\sum y_i)}{n} - \\frac{(\\sum x_i)(\\sum y_i)}{n} + \\frac{(\\sum x_i)(\\sum y_i)}{n} \\\\\n= \\sum_{i=1}^{n} x_i y_i - \\frac{(\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} y_i)}{n}\n$$\n\nFor $SS_{xx}$:\n$$\nSS_{xx} = \\sum_{i=1}^{n} (x_i^2 - 2x_i\\bar{x} + \\bar{x}^2) \\\\\n= \\sum_{i=1}^{n} x_i^2 - 2\\bar{x}\\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} \\bar{x}^2 \\\\\n= \\sum_{i=1}^{n} x_i^2 - 2\\left(\\frac{\\sum x_i}{n}\\right)(\\sum x_i) + n\\left(\\frac{\\sum x_i}{n}\\right)^2 \\\\\n= \\sum_{i=1}^{n} x_i^2 - 2\\frac{(\\sum x_i)^2}{n} + \\frac{(\\sum x_i)^2}{n} \\\\\n= \\sum_{i=1}^{n} x_i^2 - \\frac{(\\sum_{i=1}^{n} x_i)^2}{n}\n$$\nBy analogy, for $SS_{yy}$:\n$$\nSS_{yy} = \\sum_{i=1}^{n} y_i^2 - \\frac{(\\sum_{i=1}^{n} y_i)^2}{n}\n$$\nSubstituting these expressions back into the formula for $r = \\frac{SS_{xy}}{\\sqrt{SS_{xx}SS_{yy}}}$ gives the desired computational formula:\n$$\nr = \\frac{\\sum_{i=1}^{n} x_i y_i - \\frac{(\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} y_i)}{n}}{\\sqrt{\\left(\\sum_{i=1}^{n} x_i^2 - \\frac{(\\sum_{i=1}^{n} x_i)^2}{n}\\right)\\left(\\sum_{i=1}^{n} y_i^2 - \\frac{(\\sum_{i=1}^{n} y_i)^2}{n}\\right)}}\n$$\nThis formula is expressed solely in terms of $n$ and the provided sums.\n\nThe issue of numerical stability arises from the computations like $SS_{xx} = \\sum x_i^2 - \\frac{(\\sum x_i)^2}{n}$. If the data values $x_i$ are large but have a small variance, the two terms $\\sum x_i^2$ and $\\frac{(\\sum x_i)^2}{n}$ can be very large and nearly equal. The subtraction of two such numbers in floating-point arithmetic can lead to a drastic loss of relative precision, an effect known as catastrophic cancellation.\n\nTo obtain numerically stable expressions, one should use the formulas based on centered data: $SS_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2$. This involves a two-pass algorithm: first, compute the mean $\\bar{x}$, and second, compute the sum of squared differences from that mean. This approach is more stable because the subtraction $(x_i - \\bar{x})$ is performed on individual data points, which are typically much smaller in magnitude than the sums $\\sum x_i^2$ and $(\\sum x_i)^2$. The subsequent summation of the squared differences, which are all non-negative, is a numerically benign operation. The equivalence of these centered expressions to the computational formula derived above has already been demonstrated algebraically during the derivation. The derivation shows that $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ is algebraically identical to $\\sum_{i=1}^{n} x_i^2 - \\frac{(\\sum_{i=1}^{n} x_i)^2}{n}$, and likewise for $SS_{yy}$ and $SS_{xy}$. While algebraically equivalent, they are not computationally equivalent in finite-precision arithmetic. The expressions based on centered data prevent catastrophic cancellation and are therefore numerically superior.\n\nWe now compute the numerical value of $r$ using the given summaries and the derived computational formula.\nThe given data are:\n$n = 40$\n$\\sum_{i=1}^{n} x_i = 5200$\n$\\sum_{i=1}^{n} y_i = 80$\n$\\sum_{i=1}^{n} x_i^2 = 712000$\n$\\sum_{i=1}^{n} y_i^2 = 217.6$\n$\\sum_{i=1}^{n} x_i y_i = 10976$\n\nFirst, we calculate the components $SS_{xy}$, $SS_{xx}$, and $SS_{yy}$.\n$$\nSS_{xy} = \\sum_{i=1}^{n} x_i y_i - \\frac{(\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} y_i)}{n} = 10976 - \\frac{(5200)(80)}{40} = 10976 - \\frac{416000}{40} = 10976 - 10400 = 576\n$$\n$$\nSS_{xx} = \\sum_{i=1}^{n} x_i^2 - \\frac{(\\sum_{i=1}^{n} x_i)^2}{n} = 712000 - \\frac{(5200)^2}{40} = 712000 - \\frac{27040000}{40} = 712000 - 676000 = 36000\n$$\n$$\nSS_{yy} = \\sum_{i=1}^{n} y_i^2 - \\frac{(\\sum_{i=1}^{n} y_i)^2}{n} = 217.6 - \\frac{(80)^2}{40} = 217.6 - \\frac{6400}{40} = 217.6 - 160 = 57.6\n$$\nNow, we compute $r$:\n$$\nr = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}} = \\frac{576}{\\sqrt{36000 \\times 57.6}} = \\frac{576}{\\sqrt{2073600}}\n$$\nThe value of the denominator is:\n$$\n\\sqrt{2073600} = 1440\n$$\nTherefore, the correlation coefficient is:\n$$\nr = \\frac{576}{1440} = 0.4\n$$\nThe problem requires the answer to be rounded to four significant figures. The exact value is $0.4$. To express this with four significant figures, we write $0.4000$.", "answer": "$$\n\\boxed{0.4000}\n$$", "id": "4897905"}, {"introduction": "A statistically significant correlation does not always tell the whole story, as the apparent association can be disproportionately driven by a few influential data points. This exercise [@problem_id:4897906] simulates a crucial step in real-world data analysis: performing diagnostics to assess the robustness of your findings. By interpreting leverage, residuals, and influence measures from a hypothetical study, you will learn to identify observations that may be distorting the overall trend and decide if a reported association is truly representative of the data.", "problem": "A biostatistics study relates a continuous biomarker $X$ (measured in $\\text{ng}/\\text{mL}$) to a clinical outcome $Y$ using simple linear regression $Y = \\beta_0 + \\beta_1 X + \\varepsilon$, where $n = 48$ patients were observed. A scatterplot of $Y$ versus $X$ suggests a positive association. The Pearson correlation is $r = 0.57$, and the Ordinary Least Squares (OLS) estimate of the slope is $\\hat{\\beta}_1 = 0.42$ with $p = 0.004$.\n\nDiagnostics for three highlighted observations are available from the fitted model: for observation $i = 7$, the leverage is $h_{77} = 0.36$, the studentized residual is $t_7 = 3.4$, and Cook’s distance is $D_7 = 1.8$; for observation $i = 21$, $h_{21,21} = 0.31$, $t_{21} = -0.5$, and $D_{21} = 0.03$; for observation $i = 43$, $h_{43,43} = 0.09$, $t_{43} = -2.9$, and $D_{43} = 0.12$. When refitting the model excluding observation $i = 7$, the slope becomes $\\hat{\\beta}_1 = 0.15$ with $p = 0.16$. When excluding observations $i = 7$ and $i = 43$, the slope becomes $\\hat{\\beta}_1 = 0.09$ with $p = 0.38$.\n\nWhich diagnostic workflow best integrates residuals, leverage, and influence to decide whether the apparent association is driven by a few points?\n\nA. Begin with visual inspection of the scatterplot to understand the $X$ range and potential leverage; then quantify leverage via $h_{ii}$ and screen for high values using a rule-of-thumb threshold such as $h_{ii} > 2p/n$ or $h_{ii} > 3p/n$ (with $p = 2$ in simple linear regression). Next, assess outlying fits via studentized residuals $t_i$ (e.g., flagging large $|t_i|$), and compute Cook’s distances $D_i$ to integrate leverage and residual magnitude. Finally, conduct sensitivity analyses by refitting the model after excluding observations with both high leverage and large residuals/influence, comparing $\\hat{\\beta}_1$ and $p$ across refits; if the estimate and $p$ change materially, conclude the association may be driven by a few points.\n\nB. Rely on the overall correlation $r$ and the OLS $p$-value; do not examine residuals, leverage, or influence because they are secondary to statistical significance. If the $p$-value is small, the association is genuine regardless of a few unusual points.\n\nC. Transform $X$ (e.g., $X \\mapsto \\log X$) immediately, then fit the model and declare the association genuine if the slope remains positive and significant; leverage and influence diagnostics are unnecessary because transformation resolves outliers.\n\nD. Sort observations by the absolute value of their raw residuals $|e_i|$ and remove the top $k$ (e.g., $k = 2$); if the refitted slope remains positive, retain the original conclusion. Leverage and influence need not be computed because large residuals alone identify the problematic points.\n\nE. Identify high leverage solely by the criterion $h_{ii} > 0.20$ and remove all such points; declare the association genuine if the remaining data still yield a significant slope. Residuals and Cook’s distances are not needed because leverage captures the main risk to inference.", "solution": "The problem asks to identify the best diagnostic workflow for assessing whether a statistically significant association is robust or is driven by a few influential points. The correct answer is **A**, which describes a comprehensive and standard approach to regression diagnostics.\n\n1.  **Analyze the Options:**\n    *   **A. Correct Workflow:** This option outlines the gold standard. It involves:\n        *   Visual inspection (scatterplot).\n        *   Quantifying leverage (potential to influence) using $h_{ii}$.\n        *   Quantifying outlyingness (poor fit) using studentized residuals, $t_i$.\n        *   Integrating both into a single influence measure (Cook's distance, $D_i$).\n        *   Performing a sensitivity analysis by removing influential points to see if conclusions change.\n    *   **B. Flawed Logic:** This option relies solely on the initial $p$-value, ignoring diagnostics. This is a common but dangerous mistake, as a small $p$-value can be an artifact of a single influential point. The data in the problem explicitly demonstrate this: the initial $p=0.004$ becomes $p=0.16$ after removing just one point.\n    *   **C. Misguided Solution:** This option proposes data transformation as a first step. While transformation can address non-linearity or heteroscedasticity, it is not a direct solution for influential points and does not replace the need for diagnostics.\n    *   **D. Incomplete Approach:** This option focuses only on residuals. It fails to account for leverage. A point can have a small residual but high leverage and still be influential (by pulling the line towards itself).\n    *   **E. Incomplete Approach:** This option focuses only on leverage. It fails to account for the residual. A high-leverage point is not necessarily influential if it fits the trend of the other data well (i.e., has a small residual). Observation 21 in the problem is a perfect example, with high leverage ($h_{21,21}=0.31$) but a small residual and negligible influence ($D_{21}=0.03$).\n\n2.  **Apply to the Provided Data:**\n    The diagnostic data for the three points perfectly illustrate the principles in option A.\n    *   **Observation 7:** High leverage ($h_{77} = 0.36$) AND a large studentized residual ($t_7 = 3.4$). This combination makes it extremely influential, confirmed by a massive Cook's distance ($D_7 = 1.8$; a common rule of thumb is $D_i > 1$). The sensitivity analysis confirms its influence: removing it changes the slope estimate from $0.42$ to $0.15$ and the $p$-value from significant ($0.004$) to non-significant ($0.16$).\n    *   **Observation 21:** High leverage ($h_{21,21} = 0.31$) but a small residual ($t_{21} = -0.5$). The point follows the overall trend, so its influence is minimal ($D_{21}=0.03$). Removing it would be inappropriate.\n    *   **Observation 43:** An outlier (large residual, $t_{43} = -2.9$) but with modest leverage ($h_{43,43} = 0.09$). It has some influence ($D_{43} = 0.12$), but much less than observation 7.\n\nThis analysis shows that only the comprehensive workflow described in option A allows the analyst to correctly identify observation 7 as the source of the misleading association, while correctly deciding not to remove a point like observation 21. Therefore, option A is the only robust and correct procedure.", "answer": "$$\\boxed{A}$$", "id": "4897906"}, {"introduction": "One of the most counter-intuitive phenomena in statistics is Simpson's paradox, where a trend that appears in different groups of data reverses when these groups are combined. This exercise [@problem_id:4897914] provides a formal framework for understanding this paradox, using a hypothetical scenario involving two clinical centers to illustrate the concept. You will derive the mathematical conditions that lead to this reversal, gaining a deeper insight into the powerful and sometimes misleading effects of confounding variables.", "problem": "A biostatistician studies the association between two continuous biomarkers, denoted $X$ and $Y$, measured on participants recruited from two clinical centers indexed by a binary variable $Z \\in \\{0,1\\}$. The centers enroll participants in proportions $\\mathbb{P}(Z=1)=p$ and $\\mathbb{P}(Z=0)=1-p$, where $p \\in (0,1)$. Conditional on $Z=z$, the joint distribution of $(X,Y)$ is bivariate Normal with mean vector $(\\mu_{x,z},\\mu_{y,z})$, marginal standard deviations $\\sigma_{x,z}>0$ and $\\sigma_{y,z}>0$, and within-center correlation $\\rho_{z} \\in (-1,1)$; that is, $(X,Y)\\mid Z=z \\sim \\mathcal{N}_2\\big((\\mu_{x,z},\\mu_{y,z}), \\Sigma_{z}\\big)$ with\n$$\n\\Sigma_{z}=\\begin{pmatrix}\n\\sigma_{x,z}^{2} & \\rho_{z}\\,\\sigma_{x,z}\\sigma_{y,z}\\\\\n\\rho_{z}\\,\\sigma_{x,z}\\sigma_{y,z} & \\sigma_{y,z}^{2}\n\\end{pmatrix}.\n$$\nThe biostatistician considers the following specific parameterization intended to formalize Simpson’s paradox in scatterplots: for $Z=0$, let $(\\mu_{x,0},\\mu_{y,0})=(0,10)$, $\\sigma_{x,0}=\\sigma_{y,0}=1$, and $\\rho_{0}=0.4$; for $Z=1$, let $(\\mu_{x,1},\\mu_{y,1})=(10,0)$, $\\sigma_{x,1}=\\sigma_{y,1}=1$, and $\\rho_{1}=0.4$.\n\nUsing only the definitions of covariance and conditional expectation as the starting point, carry out the following:\n\n- Derive from first principles an expression for the marginal covariance $\\operatorname{Cov}(X,Y)$ in terms of $p$, $\\{\\mu_{x,z},\\mu_{y,z},\\sigma_{x,z},\\sigma_{y,z},\\rho_{z}\\}_{z\\in\\{0,1\\}}$, and show explicitly how within-center and between-center components contribute.\n\n- From your derived expression, state a general condition under which the marginal correlation $\\operatorname{Corr}(X,Y)$ is negative while the within-center correlations $\\operatorname{Corr}(X,Y\\mid Z=0)$ and $\\operatorname{Corr}(X,Y\\mid Z=1)$ are both positive. Explain in words how this condition manifests in a pooled scatterplot versus stratified scatterplots.\n\n- Specialize your result to the numeric parameterization above, simplify $\\operatorname{Cov}(X,Y)$ as a function of $p$, and solve for the two mixing proportions at which the marginal covariance is zero. Denote the smaller root by $p_{-}$.\n\nReport $p_{-}$ rounded to four significant figures. Do not include units. Express the final number without a percentage sign. Assume throughout that variances are finite and nonzero so that correlations are well-defined, and that denominators in any correlation formulas are strictly positive. Angle measures are not involved in this problem.", "solution": "This problem will be addressed in three parts as requested. First, we will derive the general expression for the marginal covariance $\\operatorname{Cov}(X,Y)$. Second, we will use this expression to articulate the conditions for Simpson's paradox. Third, we will apply the specific parameter values provided to find the numerical solution.\n\n**Part 1: Derivation of the Marginal Covariance**\n\nThe marginal covariance between $X$ and $Y$ is defined as $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. To derive an expression in terms of the given conditional parameters, we use the law of total expectation and the law of total covariance. The law of total covariance states that for random variables $X$, $Y$, and $Z$:\n$$\n\\operatorname{Cov}(X,Y) = \\mathbb{E}[\\operatorname{Cov}(X,Y \\mid Z)] + \\operatorname{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])\n$$\nThis formula elegantly decomposes the total covariance into two components: a within-center component and a between-center component.\n\nThe first term, $\\mathbb{E}[\\operatorname{Cov}(X,Y \\mid Z)]$, represents the average of the within-center covariances, weighted by the probability of each center. The random variable $\\operatorname{Cov}(X,Y \\mid Z)$ takes the value $\\operatorname{Cov}(X,Y \\mid Z=z)$ when the conditioning variable $Z$ is equal to $z$. From the problem statement, $\\operatorname{Cov}(X,Y \\mid Z=z) = \\rho_z \\sigma_{x,z} \\sigma_{y,z}$. The expectation is taken over the distribution of $Z$:\n\\begin{align*} \\mathbb{E}[\\operatorname{Cov}(X,Y \\mid Z)] &= \\sum_{z \\in \\{0,1\\}} \\mathbb{P}(Z=z) \\operatorname{Cov}(X,Y \\mid Z=z) \\\\ &= \\mathbb{P}(Z=0) \\operatorname{Cov}(X,Y \\mid Z=0) + \\mathbb{P}(Z=1) \\operatorname{Cov}(X,Y \\mid Z=1) \\\\ &= (1-p) (\\rho_0 \\sigma_{x,0} \\sigma_{y,0}) + p (\\rho_1 \\sigma_{x,1} \\sigma_{y,1})\\end{align*}\nThis is the **within-center component** of the total covariance.\n\nThe second term, $\\operatorname{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])$, represents the covariance of the conditional means. Let $U = \\mathbb{E}[X \\mid Z]$ and $V = \\mathbb{E}[Y \\mid Z]$. These are random variables that depend on $Z$.\n$U$ takes the value $\\mu_{x,0}$ with probability $1-p$ (when $Z=0$) and $\\mu_{x,1}$ with probability $p$ (when $Z=1$).\n$V$ takes the value $\\mu_{y,0}$ with probability $1-p$ (when $Z=0$) and $\\mu_{y,1}$ with probability $p$ (when $Z=1$).\nThe covariance is $\\operatorname{Cov}(U,V) = \\mathbb{E}[UV] - \\mathbb{E}[U]\\mathbb{E}[V]$.\nThe expectations are:\n$$\n\\mathbb{E}[U] = (1-p)\\mu_{x,0} + p\\mu_{x,1}\n$$\n$$\n\\mathbb{E}[V] = (1-p)\\mu_{y,0} + p\\mu_{y,1}\n$$\nAnd the expectation of the product is:\n$$\n\\mathbb{E}[UV] = \\mathbb{P}(Z=0) \\mu_{x,0}\\mu_{y,0} + \\mathbb{P}(Z=1) \\mu_{x,1}\\mu_{y,1} = (1-p)\\mu_{x,0}\\mu_{y,0} + p\\mu_{x,1}\\mu_{y,1}\n$$\nSubstituting these into the covariance formula:\n\\begin{align*} \\operatorname{Cov}(U,V) &= [(1-p)\\mu_{x,0}\\mu_{y,0} + p\\mu_{x,1}\\mu_{y,1}] - [(1-p)\\mu_{x,0} + p\\mu_{x,1}][(1-p)\\mu_{y,0} + p\\mu_{y,1}] \\\\ &= (1-p)\\mu_{x,0}\\mu_{y,0} + p\\mu_{x,1}\\mu_{y,1} - \\left( (1-p)^2\\mu_{x,0}\\mu_{y,0} + p(1-p)\\mu_{x,0}\\mu_{y,1} + p(1-p)\\mu_{x,1}\\mu_{y,0} + p^2\\mu_{x,1}\\mu_{y,1} \\right) \\\\ &= \\mu_{x,0}\\mu_{y,0} ((1-p) - (1-p)^2) + \\mu_{x,1}\\mu_{y,1} (p - p^2) - p(1-p)\\mu_{x,0}\\mu_{y,1} - p(1-p)\\mu_{x,1}\\mu_{y,0} \\\\ &= \\mu_{x,0}\\mu_{y,0} (p(1-p)) + \\mu_{x,1}\\mu_{y,1} (p(1-p)) - p(1-p)\\mu_{x,0}\\mu_{y,1} - p(1-p)\\mu_{x,1}\\mu_{y,0} \\\\ &= p(1-p) [ \\mu_{x,0}\\mu_{y,0} - \\mu_{x,0}\\mu_{y,1} - \\mu_{x,1}\\mu_{y,0} + \\mu_{x,1}\\mu_{y,1} ] \\\\ &= p(1-p) [ (\\mu_{x,1} - \\mu_{x,0})(\\mu_{y,1} - \\mu_{y,0}) ] \\end{align*}\nThis is the **between-center component** of the total covariance.\n\nCombining both components, the final expression for the marginal covariance is:\n$$\n\\operatorname{Cov}(X,Y) = \\underbrace{(1-p)(\\rho_0 \\sigma_{x,0} \\sigma_{y,0}) + p(\\rho_1 \\sigma_{x,1} \\sigma_{y,1})}_\\text{Within-center (average covariance)} + \\underbrace{p(1-p)(\\mu_{x,1}-\\mu_{x,0})(\\mu_{y,1}-\\mu_{y,0})}_\\text{Between-center (covariance of means)}\n$$\n\n**Part 2: Condition for Negative Marginal Correlation with Positive Conditional Correlations**\n\nThe phenomenon where the marginal (pooled) correlation has a different sign from the conditional (stratified) correlations is known as Simpson's paradox. We are asked for the condition where $\\operatorname{Corr}(X,Y) < 0$ while $\\operatorname{Corr}(X,Y\\mid Z=0) > 0$ and $\\operatorname{Corr}(X,Y\\mid Z=1) > 0$.\n\nSince the marginal standard deviations $\\sigma_X = \\sqrt{\\operatorname{Var}(X)}$ and $\\sigma_Y = \\sqrt{\\operatorname{Var}(Y)}$ are strictly positive, the sign of the correlation is determined by the sign of the covariance. Thus, $\\operatorname{Corr}(X,Y) < 0 \\iff \\operatorname{Cov}(X,Y) < 0$.\nThe conditions on the within-center correlations are $\\operatorname{Corr}(X,Y\\mid Z=z) = \\rho_z > 0$ for $z \\in \\{0, 1\\}$.\n\nGiven $\\rho_0 > 0$ and $\\rho_1 > 0$, the within-center component of covariance, $(1-p)(\\rho_0 \\sigma_{x,0} \\sigma_{y,0}) + p(\\rho_1 \\sigma_{x,1} \\sigma_{y,1})$, is always positive for $p \\in (0,1)$.\n\nFor the total covariance $\\operatorname{Cov}(X,Y)$ to be negative, the between-center component must be negative and its magnitude must exceed that of the positive within-center component.\nThe factor $p(1-p)$ is always positive for $p \\in (0,1)$. Therefore, for the between-center component to be negative, we must have:\n$$\n(\\mu_{x,1}-\\mu_{x,0})(\\mu_{y,1}-\\mu_{y,0}) < 0\n$$\nThis is the crucial condition. It means that the centers of the two conditional distributions must be negatively associated: as the mean of $X$ increases from one center to the other, the mean of $Y$ must decrease, or vice versa.\n\nThe full condition for the paradox to occur is:\n$$\n\\underbrace{p(1-p)(\\mu_{x,1}-\\mu_{x,0})(\\mu_{y,1}-\\mu_{y,0})}_\\text{Negative between-center term} < - \\underbrace{((1-p)\\rho_0 \\sigma_{x,0} \\sigma_{y,0} + p\\rho_1 \\sigma_{x,1} \\sigma_{y,1})}_\\text{Positive within-center term}\n$$\n\nIn a scatterplot, this manifests as follows:\n- **Stratified Scatterplots**: When plotting $(X,Y)$ data separately for each center ($Z=0$ and $Z=1$), we would see two distinct clouds of points. Because $\\rho_0 > 0$ and $\\rho_1 > 0$, each cloud would exhibit a clear positive (upward-sloping) trend.\n- **Pooled Scatterplot**: When plotting all data points together and ignoring the center label $Z$, the overall visual trend can be negative. This occurs because the centers of the two clouds are positioned in a way that creates a dominant negative association. For example, the cloud for $Z=0$ is centered at a low $X$ and high $Y$ (e.g., $(\\mu_{x,0}, \\mu_{y,0})$), while the cloud for $Z=1$ is centered at a high $X$ and low $Y$ (e.g., $(\\mu_{x,1}, \\mu_{y,1})$). This strong negative trend between the group means can overpower the weaker positive trends within each group, leading to a negative overall correlation.\n\n**Part 3: Specialization and Calculation**\n\nWe now substitute the given numerical parameterization:\n- For $Z=0$: $(\\mu_{x,0},\\mu_{y,0})=(0,10)$, $\\sigma_{x,0}=1$, $\\sigma_{y,0}=1$, $\\rho_{0}=0.4$.\n- For $Z=1$: $(\\mu_{x,1},\\mu_{y,1})=(10,0)$, $\\sigma_{x,1}=1$, $\\sigma_{y,1}=1$, $\\rho_{1}=0.4$.\n\nFirst, we calculate the within-center covariance component:\n$$\n\\mathbb{E}[\\operatorname{Cov}(X,Y \\mid Z)] = (1-p)(0.4 \\times 1 \\times 1) + p(0.4 \\times 1 \\times 1) = 0.4(1-p) + 0.4p = 0.4\n$$\nNext, we calculate the between-center covariance component:\n$$\n\\operatorname{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z]) = p(1-p)(\\mu_{x,1}-\\mu_{x,0})(\\mu_{y,1}-\\mu_{y,0})\n$$\nThe differences in the means are:\n$$\n\\mu_{x,1}-\\mu_{x,0} = 10 - 0 = 10\n$$\n$$\n\\mu_{y,1}-\\mu_{y,0} = 0 - 10 = -10\n$$\nSo the between-center component is:\n$$\np(1-p)(10)(-10) = -100p(1-p)\n$$\nThe marginal covariance as a function of $p$ is:\n$$\n\\operatorname{Cov}(X,Y) = 0.4 - 100p(1-p) = 0.4 - 100p + 100p^2\n$$\nWe are asked to find the values of $p$ for which the marginal covariance is zero. We set $\\operatorname{Cov}(X,Y)=0$:\n$$\n100p^2 - 100p + 0.4 = 0\n$$\nThis is a quadratic equation for $p$. We use the quadratic formula $p = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ with $a=100$, $b=-100$, and $c=0.4$.\n$$\np = \\frac{-(-100) \\pm \\sqrt{(-100)^2 - 4(100)(0.4)}}{2(100)} = \\frac{100 \\pm \\sqrt{10000 - 160}}{200} = \\frac{100 \\pm \\sqrt{9840}}{200}\n$$\nNow, we calculate the numerical value of the square root:\n$$\n\\sqrt{9840} \\approx 99.1967741\n$$\nThe two roots for $p$ are:\n$$\np_{+} = \\frac{100 + 99.1967741}{200} = \\frac{199.1967741}{200} \\approx 0.99598387\n$$\n$$\np_{-} = \\frac{100 - 99.1967741}{200} = \\frac{0.8032259}{200} \\approx 0.00401613\n$$\nThe problem asks for the smaller root, denoted $p_{-}$.\n$$\np_{-} \\approx 0.00401613\n$$\nRounding to four significant figures gives:\n$$\np_{-} \\approx 0.004016\n$$", "answer": "$$\\boxed{0.004016}$$", "id": "4897914"}]}