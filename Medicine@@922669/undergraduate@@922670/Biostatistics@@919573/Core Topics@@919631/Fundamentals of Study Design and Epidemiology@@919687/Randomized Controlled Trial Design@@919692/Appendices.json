{"hands_on_practices": [{"introduction": "The cornerstone of any well-designed Randomized Controlled Trial (RCT) is determining the appropriate number of participants. This foundational exercise guides you to derive the sample size formula for comparing two means from first principles, building a direct link between statistical power, significance level ($\\alpha$), and the target effect size ($\\Delta$). By completing this practice [@problem_id:4945771], you will master the core logic that underpins the scale and feasibility of clinical research.", "problem": "A parallel-group randomized controlled trial (RCT) is planned to compare the mean of a continuous primary outcome between an experimental arm and a control arm. Let the true arm-specific means be $\\mu_{T}$ and $\\mu_{C}$, respectively. The outcome in both arms is assumed to be independently and identically distributed with a common variance $\\sigma^{2}$, which is treated as known for planning purposes. The allocation ratio is fixed at $r\\!:\\!1$ (experimental:control), meaning the experimental arm will enroll $r$ times as many participants as the control arm. The trial will use a two-sided significance level $\\alpha$ and is designed to have power $1-\\beta$ to detect a true difference in means of magnitude $\\Delta>0$ (in either direction), that is, to test the null hypothesis $H_{0}: \\mu_{T}-\\mu_{C}=0$ against the two-sided alternative $H_{1}: \\mu_{T}-\\mu_{C}\\neq 0$ with target detection threshold $|\\mu_{T}-\\mu_{C}|=\\Delta$.\n\nStarting from first principles, including definitions of Type I error and power, properties of the sample mean under independence, and the Central Limit Theorem (CLT), derive a closed-form expression for the minimum total sample size $N$ required to achieve the design criteria under a large-sample normal approximation. Express your final result in terms of $r$, $\\alpha$, $\\beta$, $\\sigma$, and $\\Delta$, using the upper quantiles $z_{1-\\alpha/2}$ and $z_{1-\\beta}$ of the standard normal distribution, where $z_{p}$ denotes the $p$-th quantile of a standard normal random variable.\n\nProvide your final answer as a single closed-form analytic expression for $N$. No rounding is required and no units are to be reported.", "solution": "The derivation is based on first principles of hypothesis testing, including the definitions of Type I and Type II errors, and the large-sample properties of the sample mean guaranteed by the Central Limit Theorem (CLT).\n\n### Derivation of the Sample Size Formula\n\n**1. Sample Sizes and Estimators**\nLet $n_{T}$ and $n_{C}$ be the sample sizes for the experimental and control arms, respectively. The total sample size is $N = n_{T} + n_{C}$.\nThe allocation ratio is given as $n_{T}/n_{C} = r$, so $n_{T} = r n_{C}$.\nWe can express $n_{T}$ and $n_{C}$ in terms of the total sample size $N$:\n$$N = r n_{C} + n_{C} = (r+1)n_{C} \\implies n_{C} = \\frac{N}{r+1}$$\n$$n_{T} = r n_{C} = \\frac{rN}{r+1}$$\nThe sample means, $\\bar{X}_{T}$ and $\\bar{X}_{C}$, are the estimators for the true population means, $\\mu_{T}$ and $\\mu_{C}$. The estimator for the difference in means is $\\hat{\\Delta} = \\bar{X}_{T} - \\bar{X}_{C}$.\n\n**2. Distribution of the Estimator**\nAccording to the Central Limit Theorem (CLT), for large sample sizes, the distribution of the sample mean is approximately normal. Given that the individual observations are independent and identically distributed with mean $\\mu$ and variance $\\sigma^2$, we have:\n$$\\bar{X}_{T} \\sim \\mathcal{N}\\left(\\mu_{T}, \\frac{\\sigma^2}{n_{T}}\\right) \\quad \\text{and} \\quad \\bar{X}_{C} \\sim \\mathcal{N}\\left(\\mu_{C}, \\frac{\\sigma^2}{n_{C}}\\right)$$\nSince the two trial arms are independent, the difference of the sample means, $\\hat{\\Delta}$, is also approximately normally distributed. The mean of the difference is the difference of the means, and the variance of the difference is the sum of the variances:\n$$\\hat{\\Delta} = \\bar{X}_{T} - \\bar{X}_{C} \\sim \\mathcal{N}\\left(\\mu_{T} - \\mu_{C}, \\frac{\\sigma^2}{n_{T}} + \\frac{\\sigma^2}{n_{C}}\\right)$$\nThe variance of the estimator can be simplified:\n$$\\text{Var}(\\hat{\\Delta}) = \\sigma^2 \\left(\\frac{1}{n_{T}} + \\frac{1}{n_{C}}\\right) = \\sigma^2 \\left(\\frac{r+1}{rN} + \\frac{r+1}{N}\\right) = \\sigma^2 \\frac{r+1}{N}\\left(\\frac{1}{r} + 1\\right) = \\sigma^2 \\frac{r+1}{N}\\left(\\frac{1+r}{r}\\right) = \\frac{\\sigma^2(r+1)^2}{rN}$$\n\n**3. Condition for Type I Error (Significance Level)**\nThe null hypothesis is $H_{0}: \\mu_{T} - \\mu_{C} = 0$. Under $H_{0}$, the test statistic $Z$ follows a standard normal distribution:\n$$Z = \\frac{(\\bar{X}_{T} - \\bar{X}_{C}) - 0}{\\sqrt{\\text{Var}(\\hat{\\Delta})}} = \\frac{\\bar{X}_{T} - \\bar{X}_{C}}{\\sigma\\sqrt{\\frac{(r+1)^2}{rN}}} \\sim \\mathcal{N}(0,1)$$\nA Type I error occurs if we reject $H_{0}$ when it is true. For a two-sided test with significance level $\\alpha$, we reject $H_{0}$ if $|Z| > z_{1-\\alpha/2}$. This is equivalent to rejecting if:\n$$|\\bar{X}_{T} - \\bar{X}_{C}| > z_{1-\\alpha/2} \\cdot \\sigma \\sqrt{\\frac{(r+1)^2}{rN}}$$\nThis inequality defines the rejection region for the test.\n\n**4. Condition for Power (Type II Error)**\nPower is the probability of correctly rejecting $H_{0}$ when the alternative hypothesis $H_{1}$ is true. We are designing the trial to have power $1-\\beta$ to detect a specific alternative where the true difference is $|\\mu_{T} - \\mu_{C}|=\\Delta$. Without loss of generality, let's consider the case where $\\mu_{T} - \\mu_{C} = \\Delta > 0$.\nUnder this specific alternative, the estimator $\\hat{\\Delta} = \\bar{X}_{T} - \\bar{X}_{C}$ is distributed as:\n$$\\hat{\\Delta} \\sim \\mathcal{N}\\left(\\Delta, \\frac{\\sigma^2(r+1)^2}{rN}\\right)$$\nPower is the probability that $\\hat{\\Delta}$ falls into the rejection region, given this alternative is true.\n$$\\text{Power} = 1-\\beta = P\\left(|\\hat{\\Delta}| > z_{1-\\alpha/2} \\cdot \\sigma \\sqrt{\\frac{(r+1)^2}{rN}} \\;\\bigg|\\; \\mu_{T} - \\mu_{C} = \\Delta\\right)$$\nThis expands to two terms:\n$$1-\\beta = P\\left(\\hat{\\Delta} > z_{1-\\alpha/2} \\cdot \\sigma \\sqrt{\\frac{(r+1)^2}{rN}}\\right) + P\\left(\\hat{\\Delta}  -z_{1-\\alpha/2} \\cdot \\sigma \\sqrt{\\frac{(r+1)^2}{rN}}\\right)$$\nFor a well-powered study, the mean of the distribution under the alternative, $\\Delta$, is far from the mean under the null, $0$. The second term corresponds to observing a large negative difference when the true difference is large and positive, which is an event with negligible probability. Therefore, we can accurately approximate the power using only the first term:\n$$1-\\beta \\approx P\\left(\\hat{\\Delta} > z_{1-\\alpha/2} \\cdot \\sigma \\sqrt{\\frac{(r+1)^2}{rN}}\\right)$$\nTo evaluate this probability, we standardize the variable $\\hat{\\Delta}$ using its distribution under the alternative hypothesis:\n$$Z' = \\frac{\\hat{\\Delta} - \\Delta}{\\sigma\\sqrt{\\frac{(r+1)^2}{rN}}} \\sim \\mathcal{N}(0,1)$$\nRewriting the inequality in terms of $Z'$:\n$$1-\\beta \\approx P\\left(Z' > \\frac{z_{1-\\alpha/2} \\cdot \\sigma \\sqrt{\\frac{(r+1)^2}{rN}} - \\Delta}{\\sigma\\sqrt{\\frac{(r+1)^2}{rN}}}\\right)$$\n$$1-\\beta \\approx P\\left(Z' > z_{1-\\alpha/2} - \\frac{\\Delta}{\\sigma\\sqrt{\\frac{(r+1)^2}{rN}}}\\right)$$\nFor a standard normal variable $Z'$, the statement $P(Z' > k) = 1-\\beta$ implies that $k$ is the $\\beta$-th quantile, $z_{\\beta}$. By the symmetry of the normal distribution, $z_{\\beta} = -z_{1-\\beta}$. Thus, we must have:\n$$z_{1-\\alpha/2} - \\frac{\\Delta}{\\sigma\\sqrt{\\frac{(r+1)^2}{rN}}} = -z_{1-\\beta}$$\n\n**5. Solving for the Total Sample Size N**\nWe now rearrange the equation to solve for $N$:\n$$\\frac{\\Delta}{\\sigma\\sqrt{\\frac{(r+1)^2}{rN}}} = z_{1-\\alpha/2} + z_{1-\\beta}$$\n$$\\frac{\\Delta \\sqrt{rN}}{\\sigma(r+1)} = z_{1-\\alpha/2} + z_{1-\\beta}$$\nIsolating $\\sqrt{N}$:\n$$\\sqrt{N} = \\frac{\\sigma(r+1)}{\\Delta\\sqrt{r}} (z_{1-\\alpha/2} + z_{1-\\beta})$$\nSquaring both sides gives the final expression for the minimum total sample size $N$:\n$$N = \\left(\\frac{\\sigma(r+1)}{\\Delta\\sqrt{r}}\\right)^2 (z_{1-\\alpha/2} + z_{1-\\beta})^2$$\n$$N = \\frac{\\sigma^2 (r+1)^2}{r \\Delta^2} (z_{1-\\alpha/2} + z_{1-\\beta})^2$$\nThis formula provides the minimum total number of participants required to achieve a power of $1-\\beta$ for detecting a true mean difference of $\\Delta$ with a two-sided significance level of $\\alpha$, given a common variance $\\sigma^2$ and an allocation ratio of $r:1$.", "answer": "$$\\boxed{\\frac{\\sigma^2 (r+1)^2}{r \\Delta^2} (z_{1-\\alpha/2} + z_{1-\\beta})^2}$$", "id": "4945771"}, {"introduction": "Once a basic sample size is estimated, can the trial's design be made more efficient? This practice introduces Analysis of Covariance (ANCOVA) as a key strategy to increase statistical power by adjusting for a relevant baseline variable [@problem_id:4945752]. You will derive the exact efficiency gain, revealing how this technique reduces outcome variability and allows for more precise effect estimation.", "problem": "Consider a two-arm Randomized Controlled Trial (RCT) with equal allocation probability $p=1/2$. Let $T_{i} \\in \\{0,1\\}$ denote the treatment indicator for participant $i$ with $P(T_{i}=1)=1/2$, and let $X_{i}$ be a pre-treatment baseline covariate measured for all participants. Suppose the post-treatment continuous outcome $Y_{i}$ follows the linear model\n$$\nY_{i} \\;=\\; \\alpha \\;+\\; \\tau T_{i} \\;+\\; \\gamma X_{i} \\;+\\; \\varepsilon_{i},\n$$\nwhere $E[\\varepsilon_{i}\\,|\\,T_{i},X_{i}] = 0$, $\\operatorname{Var}(\\varepsilon_{i}\\,|\\,T_{i},X_{i}) = \\sigma^{2}$ (homoscedasticity), the errors $\\varepsilon_{i}$ are independent across $i$, and randomization ensures $T_{i} \\perp X_{i}$. Assume $E[X_{i}] = 0$ and $\\operatorname{Var}(X_{i}) = \\sigma_{X}^{2} \\in (0,\\infty)$.\n\nDefine two estimators of the treatment effect $\\tau$:\n- the unadjusted estimator $\\hat{\\tau}_{U}$, which is the difference of sample means of $Y_{i}$ between the treatment and control arms;\n- the Analysis of Covariance (ANCOVA) estimator $\\hat{\\tau}_{A}$, which is the Ordinary Least Squares (OLS) coefficient on $T_{i}$ in the multiple linear regression of $Y_{i}$ on an intercept, $T_{i}$, and $X_{i}$.\n\nStarting from the stated model assumptions and first principles, derive the leading-order (in $1/n$) variance expressions $\\operatorname{Var}(\\hat{\\tau}_{U})$ and $\\operatorname{Var}(\\hat{\\tau}_{A})$ under homoscedasticity and equal allocation. Then show that the multiplicative variance ratio $\\operatorname{Var}(\\hat{\\tau}_{A}) / \\operatorname{Var}(\\hat{\\tau}_{U})$ can be written solely in terms of the within-arm correlation coefficient $\\rho$ between $Y_{i}$ and $X_{i}$, defined for example in the control arm by\n$$\n\\rho \\;=\\; \\frac{\\operatorname{Cov}(Y_{i}, X_{i} \\mid T_{i}=0)}{\\sqrt{\\operatorname{Var}(Y_{i} \\mid T_{i}=0)\\,\\operatorname{Var}(X_{i})}}.\n$$\nProvide your final answer as the simplified closed-form analytic expression for the variance ratio in terms of $\\rho$. No numerical rounding is required; do not include units in the final expression.", "solution": "Let $n$ be the total sample size, with $n_1$ participants in the treatment arm ($T_i=1$) and $n_0$ in the control arm ($T_i=0$), such that $n = n_0 + n_1$. With an equal allocation probability of $1/2$, for large $n$ we have $n_1 \\approx n/2$ and $n_0 \\approx n/2$.\n\nFirst, we derive the variance of the unadjusted estimator, $\\hat{\\tau}_{U}$. This estimator is the difference in sample means of the outcome $Y_i$ between the two arms.\n$$\n\\hat{\\tau}_{U} = \\frac{1}{n_1} \\sum_{i: T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i: T_i=0} Y_i\n$$\nThe variance of $\\hat{\\tau}_{U}$ is the sum of the variances of the two sample means.\n$$\n\\operatorname{Var}(\\hat{\\tau}_{U}) = \\frac{\\operatorname{Var}(Y_i | T_i=1)}{n_1} + \\frac{\\operatorname{Var}(Y_i | T_i=0)}{n_0}\n$$\nWe find the conditional variance of $Y_i$ from the model $Y_{i} = \\alpha + \\tau T_{i} + \\gamma X_{i} + \\varepsilon_{i}$.\n$$\n\\operatorname{Var}(Y_i | T_i=t) = \\operatorname{Var}(\\alpha + \\tau t + \\gamma X_i + \\varepsilon_i | T_i=t)\n$$\nSince randomization ensures $T_i \\perp X_i$ and the model implies $\\varepsilon_i$ is uncorrelated with $X_i$ conditional on $T_i$, we have:\n$$\n\\operatorname{Var}(Y_i | T_i=t) = \\operatorname{Var}(\\gamma X_i | T_i=t) + \\operatorname{Var}(\\varepsilon_i | T_i=t) = \\gamma^2 \\operatorname{Var}(X_i) + \\sigma^2 = \\gamma^2 \\sigma_{X}^2 + \\sigma^2\n$$\nThe conditional variance is the same in both arms. Substituting this into the variance expression for $\\hat{\\tau}_{U}$ with $n_1 \\approx n_0 \\approx n/2$:\n$$\n\\operatorname{Var}(\\hat{\\tau}_{U}) \\approx \\frac{\\gamma^2 \\sigma_{X}^2 + \\sigma^2}{n/2} + \\frac{\\gamma^2 \\sigma_{X}^2 + \\sigma^2}{n/2} = \\frac{4(\\gamma^2 \\sigma_{X}^2 + \\sigma^2)}{n}\n$$\nNext, we derive the variance of the ANCOVA estimator, $\\hat{\\tau}_{A}$. From the theory of Ordinary Least Squares (OLS), the variance of a coefficient is given by:\n$$\n\\operatorname{Var}(\\hat{\\tau}_A | \\mathbf{X}, \\mathbf{T}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (T_i - \\bar{T})^2 (1 - R^2_{T \\sim X})}\n$$\nwhere $R^2_{T \\sim X}$ is the R-squared from regressing $T_i$ on the other covariates. Due to randomization ($T_i \\perp X_i$), $R^2_{T \\sim X}$ approaches $0$ in large samples. The sum of squares for $T_i$ is $\\sum (T_i - \\bar{T})^2 = \\frac{n_1 n_0}{n} \\approx \\frac{(n/2)(n/2)}{n} = \\frac{n}{4}$.\nThe unconditional variance is thus:\n$$\n\\operatorname{Var}(\\hat{\\tau}_{A}) \\approx \\frac{\\sigma^2}{(n/4)(1 - 0)} = \\frac{4\\sigma^2}{n}\n$$\nThe ratio of the two variances is:\n$$\n\\frac{\\operatorname{Var}(\\hat{\\tau}_{A})}{\\operatorname{Var}(\\hat{\\tau}_{U})} \\approx \\frac{4\\sigma^2/n}{4(\\gamma^2 \\sigma_{X}^2 + \\sigma^2)/n} = \\frac{\\sigma^2}{\\gamma^2 \\sigma_{X}^2 + \\sigma^2}\n$$\nFinally, we express this ratio in terms of the within-arm correlation coefficient $\\rho$.\n$$\n\\rho = \\frac{\\operatorname{Cov}(Y_{i}, X_{i} \\mid T_{i}=0)}{\\sqrt{\\operatorname{Var}(Y_{i} \\mid T_{i}=0)\\,\\operatorname{Var}(X_{i})}}\n$$\nThe numerator is $\\operatorname{Cov}(\\alpha + \\gamma X_i + \\varepsilon_i, X_i | T_i=0) = \\gamma \\operatorname{Var}(X_i) = \\gamma \\sigma_X^2$.\nThe denominator contains $\\operatorname{Var}(Y_i | T_i=0) = \\gamma^2 \\sigma_{X}^2 + \\sigma^2$ and $\\operatorname{Var}(X_i) = \\sigma_X^2$.\nSubstituting these:\n$$\n\\rho = \\frac{\\gamma \\sigma_X^2}{\\sqrt{(\\gamma^2 \\sigma_X^2 + \\sigma^2)\\sigma_X^2}} = \\frac{\\gamma \\sigma_X}{\\sqrt{\\gamma^2 \\sigma_X^2 + \\sigma^2}}\n$$\nSquaring both sides gives $\\rho^2 = \\frac{\\gamma^2 \\sigma_X^2}{\\gamma^2 \\sigma_X^2 + \\sigma^2}$.\nWe can see that $1 - \\rho^2 = 1 - \\frac{\\gamma^2 \\sigma_X^2}{\\gamma^2 \\sigma_X^2 + \\sigma^2} = \\frac{\\sigma^2}{\\gamma^2 \\sigma_X^2 + \\sigma^2}$.\nThis is identical to the variance ratio. Therefore:\n$$\n\\frac{\\operatorname{Var}(\\hat{\\tau}_{A})}{\\operatorname{Var}(\\hat{\\tau}_{U})} = 1 - \\rho^2\n$$\nThis result shows that adjusting for a baseline covariate $X$ reduces the variance of the treatment effect estimator by a factor of $1-\\rho^2$, where $\\rho$ is the correlation between the outcome and the covariate within treatment arms.", "answer": "$$\\boxed{1 - \\rho^{2}}$$", "id": "4945752"}, {"introduction": "Ideal trial plans must be adapted for real-world complexities like participant dropouts and non-adherence. This crucial exercise demonstrates how to 'buffer' your study by calculating sample size inflation factors that preserve statistical power under these conditions [@problem_id:4945755]. You will explore the distinct impacts on Intention-to-Treat (ITT) and Per-Protocol (PP) analyses, a vital skill for robust trial planning.", "problem": "A two-arm, parallel-group randomized controlled trial is planned to compare a continuous outcome between an intervention arm and a control arm under equal allocation. Suppose the outcome in each arm has common variance $\\sigma^{2}$ and that, among individuals who actually receive the intervention, the intervention causes a constant additive shift in the mean outcome by $\\Delta$ relative to those who do not receive it.\n\nIn the ideal design with complete follow-up and perfect adherence to assignment, the per-arm sample size required to achieve fixed Type I error and power is denoted $n_{0}$. Use as your fundamental base the well-tested fact that, for fixed Type I error and power, the required sample size is proportional to the variance of the estimator divided by the squared magnitude of the targeted mean difference.\n\nNow consider two anticipated departures from the ideal design:\n\n1. Loss to follow-up at rate $L \\in (0,1)$, independent of treatment assignment, treatment receipt, and outcome, so that only a fraction $1-L$ of randomized participants contribute outcome data in either arm.\n\n2. Symmetric two-sided noncompliance at rate $p \\in (0,1/2)$: among those randomized to intervention, a fraction $p$ does not receive the intervention, and among those randomized to control, a fraction $p$ receives the intervention (contamination). Assume the exclusion restriction that randomization affects the outcome only through actual receipt of the intervention, so the average effect of randomization on the outcome equals the true treatment effect $\\Delta$ multiplied by the difference in treatment receipt probabilities between arms.\n\nUnder these assumptions:\n\n- In the Intention-To-Treat (ITT) analysis, outcomes are compared by randomized assignment, and all randomized participants with observed outcomes are included.\n\n- In the Per-Protocol (PP) analysis, outcomes are compared among those who adhered to their randomized assignment, excluding nonadherent participants.\n\nDerive closed-form expressions for the sample size inflation factors $F_{\\mathrm{ITT}}$ and $F_{\\mathrm{PP}}$ defined by $n_{\\mathrm{ITT}} = F_{\\mathrm{ITT}}\\,n_{0}$ and $n_{\\mathrm{PP}} = F_{\\mathrm{PP}}\\,n_{0}$, where $n_{\\mathrm{ITT}}$ and $n_{\\mathrm{PP}}$ are the per-arm sample sizes required under the ITT and PP analyses, respectively, to preserve the same Type I error and power as in the ideal design. Express your final answers symbolically in terms of $L$ and $p$. No rounding is required, and no units should be used in your final expressions.", "solution": "The required sample size per arm, $n$, is proportional to the variance of the effect estimator and inversely proportional to the square of the targeted mean difference, $\\delta$: $n \\propto \\frac{\\text{Var}(\\text{estimator})}{\\delta^2}$. Let $K$ be the constant of proportionality, which is fixed across all scenarios.\n\n**1. The Ideal Case (Baseline)**\nIn the ideal design with perfect adherence and no loss to follow-up, we randomize $n_0$ subjects per arm. The targeted effect is the true treatment effect, $\\delta_{\\text{ideal}} = \\Delta$. The estimator is the difference in sample means, and its variance is $\\text{Var}(\\hat{\\Delta}_{\\text{ideal}}) = \\frac{\\sigma^2}{n_0} + \\frac{\\sigma^2}{n_0} = \\frac{2\\sigma^2}{n_0}$. The required sample size is:\n$$n_0 = K \\frac{2\\sigma^2}{\\Delta^2}$$\n\n**2. Intention-To-Treat (ITT) Analysis**\nLet $n_{\\mathrm{ITT}}$ be the per-arm sample size randomized. Only $n'_{\\mathrm{ITT}} = n_{\\mathrm{ITT}}(1-L)$ subjects per arm provide data. The ITT analysis compares subjects by randomized assignment.\n\n- **ITT Effect Size ($\\delta_{\\mathrm{ITT}}$)**: The expected outcome in the intervention arm is $\\mu_{I, \\text{ITT}} = (1-p)(\\mu+\\Delta) + p(\\mu) = \\mu + (1-p)\\Delta$. The expected outcome in the control arm is $\\mu_{C, \\text{ITT}} = (1-p)(\\mu) + p(\\mu+\\Delta) = \\mu + p\\Delta$. The effect is the difference:\n$$\\delta_{\\mathrm{ITT}} = \\mu_{I, \\text{ITT}} - \\mu_{C, \\text{ITT}} = (\\mu + (1-p)\\Delta) - (\\mu + p\\Delta) = \\Delta(1-2p)$$\n\n- **Variance of the ITT Estimator**: The estimator is the difference in sample means from the $n'_{\\mathrm{ITT}}$ subjects with data in each arm. The variance of the outcome in each arm is given as $\\sigma^2$. The variance of the estimator is:\n$$\\text{Var}(\\hat{\\Delta}_{\\mathrm{ITT}}) = \\frac{\\sigma^2}{n_{\\mathrm{ITT}}(1-L)} + \\frac{\\sigma^2}{n_{\\mathrm{ITT}}(1-L)} = \\frac{2\\sigma^2}{n_{\\mathrm{ITT}}(1-L)}$$\n\n- **Required Sample Size for ITT ($n_{\\mathrm{ITT}}$)**:\n$$n_{\\mathrm{ITT}} = K \\frac{\\text{Var}(\\hat{\\Delta}_{\\mathrm{ITT}}) \\cdot n_{\\mathrm{ITT}}}{(\\delta_{\\mathrm{ITT}})^2} = K \\frac{2\\sigma^2 / (1-L)}{(\\Delta(1-2p))^2} = \\left(K \\frac{2\\sigma^2}{\\Delta^2}\\right) \\frac{1}{(1-L)(1-2p)^2}$$\nSubstituting the expression for $n_0$:\n$$n_{\\mathrm{ITT}} = n_0 \\frac{1}{(1-L)(1-2p)^2}$$\nThe inflation factor is $F_{\\mathrm{ITT}} = \\frac{1}{(1-L)(1-2p)^2}$.\n\n**3. Per-Protocol (PP) Analysis**\nLet $n_{\\mathrm{PP}}$ be the per-arm sample size randomized. The PP analysis includes only the subset of subjects who adhered to their assigned treatment and were not lost to follow-up.\n\n- **Sample Size for PP Analysis**: The number of analyzed subjects per arm is $n'_{\\mathrm{PP}} = n_{\\mathrm{PP}}(1-p)(1-L)$.\n\n- **PP Effect Size ($\\delta_{\\mathrm{PP}}$)**: The PP analysis compares adherent subjects. Intervention arm adherers have mean $\\mu+\\Delta$, and control arm adherers have mean $\\mu$. The effect is the true treatment effect:\n$$\\delta_{\\mathrm{PP}} = (\\mu+\\Delta) - \\mu = \\Delta$$\n\n- **Variance of the PP Estimator**: The estimator is the difference in sample means from the $n'_{\\mathrm{PP}}$ subjects in each group. The outcome variance in these pure (adherent) groups is $\\sigma^2$.\n$$\\text{Var}(\\hat{\\Delta}_{\\mathrm{PP}}) = \\frac{\\sigma^2}{n_{\\mathrm{PP}}(1-p)(1-L)} + \\frac{\\sigma^2}{n_{\\mathrm{PP}}(1-p)(1-L)} = \\frac{2\\sigma^2}{n_{\\mathrm{PP}}(1-p)(1-L)}$$\n\n- **Required Sample Size for PP ($n_{\\mathrm{PP}}$)**:\n$$n_{\\mathrm{PP}} = K \\frac{\\text{Var}(\\hat{\\Delta}_{\\mathrm{PP}}) \\cdot n_{\\mathrm{PP}}}{(\\delta_{\\mathrm{PP}})^2} = K \\frac{2\\sigma^2 / ((1-p)(1-L))}{\\Delta^2} = \\left(K \\frac{2\\sigma^2}{\\Delta^2}\\right) \\frac{1}{(1-p)(1-L)}$$\nSubstituting the expression for $n_0$:\n$$n_{\\mathrm{PP}} = n_0 \\frac{1}{(1-p)(1-L)}$$\nThe inflation factor is $F_{\\mathrm{PP}} = \\frac{1}{(1-L)(1-p)}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nF_{\\mathrm{ITT}}  F_{\\mathrm{PP}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{(1-L)(1-2p)^2}  \\frac{1}{(1-L)(1-p)}\n\\end{pmatrix}\n}\n$$", "id": "4945755"}]}