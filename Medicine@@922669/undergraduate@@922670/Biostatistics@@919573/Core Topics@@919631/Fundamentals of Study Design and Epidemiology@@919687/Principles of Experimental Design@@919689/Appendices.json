{"hands_on_practices": [{"introduction": "A well-designed experiment must be powerful enough to detect a meaningful effect, yet efficient enough to be practical. This foundational exercise guides you through the derivation of the sample size formula for comparing two means. By working through this problem [@problem_id:4941140], you will develop a deep understanding of how significance level ($\\alpha$), power ($1-\\beta$), effect size ($\\delta$), and data variability ($\\sigma^2$) intertwine to determine the necessary number of participants for a robust study.", "problem": "A biostatistician is planning a randomized two-arm study to compare the mean of a continuous outcome under Treatment A and Treatment B. Outcomes in arm A are independent and identically distributed as $N(\\mu_{A}, \\sigma^{2})$ and in arm B as $N(\\mu_{B}, \\sigma^{2})$, with the same variance $\\sigma^{2}$ in each arm. The allocation is equal, with $n$ subjects per arm. The primary analysis will use a two-sided pooled two-sample $t$-test of the null hypothesis $H_{0}: \\mu_{A} - \\mu_{B} = 0$ at significance level $\\alpha$, against the two-sided alternative $H_{1}: \\mu_{A} - \\mu_{B} \\neq 0$. The design target is to have power $1 - \\beta$ to detect a prespecified difference $\\delta = \\mu_{A} - \\mu_{B}$ under the alternative, where $\\delta \\neq 0$.\n\nStarting from first principles about sampling distributions and the definition of Type I error and power, and using a scientifically standard large-sample planning approximation for the $t$-test based on the normal distribution, derive a closed-form analytic expression for the required per-arm sample size $n$ in terms of $\\sigma^{2}$, $\\alpha$, $\\beta$, and $\\delta$. Explicitly state the assumptions used at each step, including distributional assumptions, independence, equal variances, equal allocation, and any large-sample approximations. Define the standard normal quantile $z_{p}$ such that $\\Pr(Z \\leq z_{p}) = p$ for $Z \\sim N(0,1)$, and express your answer using this notation.\n\nExpress the final answer as a single closed-form analytic expression for $n$ without rounding. No physical units apply to $n$ in this problem. Do not provide any numerical evaluation.", "solution": "The user wants to solve the problem: A biostatistician is planning a randomized two-arm study to compare the mean of a continuous outcome under Treatment A and Treatment B. Outcomes in arm A are independent and identically distributed as $N(\\mu_{A}, \\sigma^{2})$ and in arm B as $N(\\mu_{B}, \\sigma^{2})$, with the same variance $\\sigma^{2}$ in each arm. The allocation is equal, with $n$ subjects per arm. The primary analysis will use a two-sided pooled two-sample $t$-test of the null hypothesis $H_{0}: \\mu_{A} - \\mu_{B} = 0$ at significance level $\\alpha$, against the two-sided alternative $H_{1}: \\mu_{A} - \\mu_{B} \\neq 0$. The design target is to have power $1 - \\beta$ to detect a prespecified difference $\\delta = \\mu_{A} - \\mu_{B}$ under the alternative, where $\\delta \\neq 0$.\n\nStarting from first principles about sampling distributions and the definition of Type I error and power, and using a scientifically standard large-sample planning approximation for the $t$-test based on the normal distribution, derive a closed-form analytic expression for the required per-arm sample size $n$ in terms of $\\sigma^{2}$, $\\alpha$, $\\beta$, and $\\delta$. Explicitly state the assumptions used at each step, including distributional assumptions, independence, equal variances, equal allocation, and any large-sample approximations. Define the standard normal quantile $z_{p}$ such that $\\Pr(Z \\leq z_{p}) = p$ for $Z \\sim N(0,1)$, and express your answer using this notation.\n\nExpress the final answer as a single closed-form analytic expression for $n$ without rounding. No physical units apply to $n$ in this problem. Do not provide any numerical evaluation.\n\n### Step 1: Extract Givens\n- Study design: Randomized two-arm study (Treatment A, Treatment B).\n- Outcome data in arm A: Independent and identically distributed (i.i.d.) from a normal distribution $N(\\mu_{A}, \\sigma^{2})$.\n- Outcome data in arm B: i.i.d. from a normal distribution $N(\\mu_{B}, \\sigma^{2})$.\n- Variance: Common variance $\\sigma^{2}$ for both arms.\n- Allocation: Equal sample sizes, $n$ subjects per arm.\n- Analysis method: Two-sided pooled two-sample $t$-test.\n- Null Hypothesis $H_{0}$: $\\mu_{A} - \\mu_{B} = 0$.\n- Alternative Hypothesis $H_{1}$: $\\mu_{A} - \\mu_{B} \\neq 0$.\n- Significance level (Type I error probability): $\\alpha$.\n- Power (probability of correctly rejecting $H_{0}$): $1 - \\beta$.\n- Type II error probability: $\\beta$.\n- Effect size to detect: $\\delta = \\mu_{A} - \\mu_{B}$, with $\\delta \\neq 0$.\n- Required approximation: Large-sample planning approximation based on the normal distribution.\n- Notation: $z_{p}$ is the standard normal quantile where $\\Pr(Z \\leq z_{p}) = p$ for $Z \\sim N(0,1)$.\n- Objective: Derive a closed-form expression for the per-arm sample size $n$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a classic and fundamental exercise in biostatistical experimental design.\n- **Scientifically Grounded**: The premises are based on standard statistical theory, including normal distributions, hypothesis testing, and power analysis. The large-sample approximation of a $t$-test by a $z$-test is a standard, scientifically-accepted technique for sample size planning.\n- **Well-Posed**: The problem provides all necessary parameters ($\\alpha$, $\\beta$, $\\delta$, $\\sigma^2$) and assumptions to uniquely derive the requested formula for $n$.\n- **Objective**: The problem is stated using precise, unambiguous statistical language.\n\nThe problem does not exhibit any of the listed flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, pseudo-profound, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete derivation of the per-arm sample size $n$ will be provided.\n\n### Derivation\nWe begin by formalizing the problem based on the provided assumptions.\n\n**1. Assumptions:**\n- Let $X_{A,i}$ for $i=1, \\dots, n$ be the i.i.d. outcomes from arm A, where $X_{A,i} \\sim N(\\mu_A, \\sigma^2)$.\n- Let $X_{B,i}$ for $i=1, \\dots, n$ be the i.i.d. outcomes from arm B, where $X_{B,i} \\sim N(\\mu_B, \\sigma^2)$.\n- The two samples are independent of each other.\n- The population variance $\\sigma^2$ is assumed to be known for planning purposes, which is the essence of the large-sample approximation for the $t$-test.\n\n**2. Sampling Distribution of the Difference in Means:**\nLet $\\bar{X}_A = \\frac{1}{n} \\sum_{i=1}^{n} X_{A,i}$ and $\\bar{X}_B = \\frac{1}{n} \\sum_{i=1}^{n} X_{B,i}$ be the sample means for arm A and arm B, respectively.\nFrom the properties of the normal distribution, the sampling distributions of the means are:\n$\\bar{X}_A \\sim N\\left(\\mu_A, \\frac{\\sigma^2}{n}\\right)$\n$\\bar{X}_B \\sim N\\left(\\mu_B, \\frac{\\sigma^2}{n}\\right)$\n\nThe difference in sample means is $\\bar{X}_A - \\bar{X}_B$. Since the samples are independent, the distribution of this difference is also normal.\nThe mean of the difference is $E[\\bar{X}_A - \\bar{X}_B] = E[\\bar{X}_A] - E[\\bar{X}_B] = \\mu_A - \\mu_B$.\nThe variance of the difference is $Var(\\bar{X}_A - \\bar{X}_B) = Var(\\bar{X}_A) + Var(\\bar{X}_B) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n}$.\nThus, the sampling distribution of the difference in means is:\n$$(\\bar{X}_A - \\bar{X}_B) \\sim N\\left(\\mu_A - \\mu_B, \\frac{2\\sigma^2}{n}\\right)$$\n\n**3. The Test Statistic and Rejection Region:**\nUnder the large-sample approximation where $\\sigma^2$ is treated as known, the test statistic is:\n$$Z_{\\text{stat}} = \\frac{(\\bar{X}_A - \\bar{X}_B) - (\\mu_A - \\mu_B)_0}{\\sqrt{\\frac{2\\sigma^2}{n}}}$$\nUnder the null hypothesis $H_0: \\mu_A - \\mu_B = 0$, the value $(\\mu_A - \\mu_B)_0$ is $0$. The statistic becomes:\n$$Z_{\\text{stat}} = \\frac{\\bar{X}_A - \\bar{X}_B}{\\sigma\\sqrt{\\frac{2}{n}}}$$\nUnder $H_0$, this statistic follows a standard normal distribution, $Z_{\\text{stat}} \\sim N(0,1)$.\n\nFor a two-sided test at a significance level of $\\alpha$, we reject $H_0$ if the observed value of $|Z_{\\text{stat}}|$ is greater than the critical value $z_{1-\\alpha/2}$. The rejection region is defined by $|Z_{\\text{stat}}|  z_{1-\\alpha/2}$. The quantile $z_{1-\\alpha/2}$ is chosen such that $\\Pr(Z  z_{1-\\alpha/2}) = \\alpha/2$ for $Z \\sim N(0,1)$.\n\n**4. Power Calculation:**\nPower is the probability of rejecting $H_0$ when the alternative hypothesis $H_1$ is true. The power is $1-\\beta$. We are designing the study to detect a specific difference $\\delta = \\mu_A - \\mu_B$.\nUnder $H_1$, the true mean of $(\\bar{X}_A - \\bar{X}_B)$ is $\\delta$. The test statistic $Z_{\\text{stat}}$ is no longer centered at $0$. Its distribution under $H_1$ is:\n$$E[Z_{\\text{stat}}] = E\\left[\\frac{\\bar{X}_A - \\bar{X}_B}{\\sigma\\sqrt{\\frac{2}{n}}}\\right] = \\frac{E[\\bar{X}_A - \\bar{X}_B]}{\\sigma\\sqrt{\\frac{2}{n}}} = \\frac{\\delta}{\\sigma\\sqrt{\\frac{2}{n}}}$$\n$$Var(Z_{\\text{stat}}) = Var\\left(\\frac{\\bar{X}_A - \\bar{X}_B}{\\sigma\\sqrt{\\frac{2}{n}}}\\right) = \\frac{Var(\\bar{X}_A - \\bar{X}_B)}{(\\sigma\\sqrt{\\frac{2}{n}})^2} = \\frac{2\\sigma^2/n}{2\\sigma^2/n} = 1$$\nSo, under $H_1$, $Z_{\\text{stat}} \\sim N\\left(\\frac{\\delta}{\\sigma\\sqrt{2/n}}, 1\\right)$.\n\nPower is the probability of the test statistic falling into the rejection region, given $H_1$ is true:\n$$1 - \\beta = \\Pr(\\text{Reject } H_0 | H_1 \\text{ is true}) = \\Pr\\left(|Z_{\\text{stat}}|  z_{1-\\alpha/2} | \\mu_A - \\mu_B = \\delta\\right)$$\nThis expands to:\n$$1 - \\beta = \\Pr\\left(Z_{\\text{stat}}  z_{1-\\alpha/2} | H_1\\right) + \\Pr\\left(Z_{\\text{stat}}  -z_{1-\\alpha/2} | H_1\\right)$$\nWithout loss of generality, assume $\\delta  0$. The mean of $Z_{\\text{stat}}$ under $H_1$ is positive, so the distribution is shifted to the right. The probability $\\Pr\\left(Z_{\\text{stat}}  -z_{1-\\alpha/2} | H_1\\right)$ becomes negligible for typical power levels (e.g., $ 0.5$). Thus, the power is dominated by the first term:\n$$1 - \\beta \\approx \\Pr\\left(Z_{\\text{stat}}  z_{1-\\alpha/2} | H_1\\right)$$\nTo evaluate this probability, we standardize $Z_{\\text{stat}}$ by subtracting its mean under $H_1$:\n$$1 - \\beta = \\Pr\\left(Z_{\\text{stat}} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}  z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right)$$\nLet $W = Z_{\\text{stat}} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}$. Under $H_1$, $W \\sim N(0,1)$.\n$$1 - \\beta = \\Pr\\left(W  z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right)$$\nBy the definition of power $1-\\beta$, the quantile for the standard normal distribution $W$ that has a cumulative probability of $\\beta$ to its left is $z_\\beta$. This is equivalent to having a probability of $1-\\beta$ to its right. Therefore, the argument of the probability function must be equal to $z_\\beta$:\n$$z_{\\beta} = z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}$$\nIf we had assumed $\\delta  0$, the dominant term for power would be $\\Pr(Z_{\\text{stat}}  -z_{1-\\alpha/2})$ leading to $z_{1-\\beta} = -z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}$. Both cases are handled by using $|\\delta|$ and ensuring the signs are consistent. Re-expressing the first case using $|\\delta|$ for $\\delta0$: $z_\\beta = z_{1-\\alpha/2} - \\frac{|\\delta|}{\\sigma\\sqrt{2/n}}$.\nUsing the symmetry property of the standard normal quantiles, $z_{\\beta} = -z_{1-\\beta}$, we have:\n$$-z_{1-\\beta} = z_{1-\\alpha/2} - \\frac{|\\delta|}{\\sigma\\sqrt{2/n}}$$\n\n**5. Solving for Sample Size n:**\nWe now solve the equation for the per-arm sample size $n$.\n$$z_{1-\\alpha/2} + z_{1-\\beta} = \\frac{|\\delta|}{\\sigma\\sqrt{2/n}}$$\nRearranging the terms to isolate $n$:\n$$z_{1-\\alpha/2} + z_{1-\\beta} = \\frac{|\\delta|\\sqrt{n}}{\\sigma\\sqrt{2}}$$\n$$\\sqrt{n} = \\frac{\\sigma\\sqrt{2}(z_{1-\\alpha/2} + z_{1-\\beta})}{|\\delta|}$$\nSquaring both sides gives the expression for $n$:\n$$n = \\left(\\frac{\\sigma\\sqrt{2}(z_{1-\\alpha/2} + z_{1-\\beta})}{|\\delta|}\\right)^2$$\n$$n = \\frac{2\\sigma^2(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2}$$\nSince $\\delta$ is squared, the absolute value is no longer necessary. This is the final closed-form expression for the required sample size per arm.", "answer": "$$\n\\boxed{\\frac{2\\sigma^{2}(z_{1-\\alpha/2} + z_{1-\\beta})^{2}}{\\delta^{2}}}\n$$", "id": "4941140"}, {"introduction": "Once data from a randomized trial are collected, we must choose how to summarize the treatment's effect. This practice [@problem_id:4941285] explores the calculation of three common measures—the risk difference, risk ratio, and odds ratio—from a hypothetical trial. More importantly, it challenges you to connect these statistical measures back to the potential outcomes framework, explaining why the risk difference provides the most direct estimate of the average causal effect that randomization is designed to identify.", "problem": "A clinical team conducts a randomized controlled trial (RCT) to evaluate a prophylactic intervention intended to reduce the risk of a laboratory-confirmed binary outcome (infection within $30$ days). Participants are randomized to either the intervention arm or the control arm under complete randomization with perfect adherence and no loss to follow-up. Let $Y$ denote the binary outcome ($Y=1$ if infected; $Y=0$ otherwise). The potential outcomes are $Y(1)$ under intervention and $Y(0)$ under control, and the Stable Unit Treatment Value Assumption (SUTVA) holds. The realized sample sizes and event counts are: intervention arm $n_{1}=184$ with $x_{1}=28$ infections, and control arm $n_{0}=176$ with $x_{0}=44$ infections.\n\nUsing foundational definitions of risk, odds, and design-based causal inference:\n- Compute the observed risks $p_{1}$ and $p_{0}$ in the two arms,\n- Compute the risk difference $p_{1}-p_{0}$, the risk ratio $p_{1}/p_{0}$, and the odds ratio $\\frac{p_{1}/(1-p_{1})}{p_{0}/(1-p_{0})}$,\n- Starting from the potential outcomes framework and the principle of randomized allocation, explain which of these estimands most directly aligns with the randomized design’s target causal effect and why, making explicit the causal quantity the design identifies.\n\nReport the risk difference $p_{1}-p_{0}$ as your final numeric answer, rounded to four significant figures and expressed as a decimal (do not use a percentage sign). No other quantities should be reported in the final answer box.", "solution": "The problem is valid as it presents a complete, consistent, and scientifically grounded scenario in biostatistics and causal inference. All data and assumptions required for the solution are explicitly provided.\n\nThe task is to compute several measures of effect from the provided data of a randomized controlled trial (RCT) and to explain which measure most directly represents the causal quantity targeted by the design.\n\nFirst, we compute the observed risks (proportions of infections) in the intervention and control arms. Let $p_1$ be the risk in the intervention arm and $p_0$ be the risk in the control arm. The givens are:\n- Intervention arm: sample size $n_1 = 184$, number of infections $x_1 = 28$.\n- Control arm: sample size $n_0 = 176$, number of infections $x_0 = 44$.\n\nThe observed risks are calculated as the number of events divided by the number of participants in each arm:\n$$p_1 = \\frac{x_1}{n_1} = \\frac{28}{184}$$\n$$p_0 = \\frac{x_0}{n_0} = \\frac{44}{176} = \\frac{1}{4} = 0.25$$\n\nNow, we compute the three requested estimands: the risk difference ($RD$), the risk ratio ($RR$), and the odds ratio ($OR$).\n\n1.  **Risk Difference ($RD$)**:\n    $$RD = p_1 - p_0 = \\frac{28}{184} - \\frac{44}{176} = \\frac{28}{184} - \\frac{1}{4} = \\frac{7 \\times 4 - 46 \\times 1}{46 \\times 4} = \\frac{28-46}{184} = \\frac{-18}{184} = -\\frac{9}{92}$$\n    As a decimal, $RD \\approx -0.097826...$\n\n2.  **Risk Ratio ($RR$)**:\n    $$RR = \\frac{p_1}{p_0} = \\frac{28/184}{1/4} = \\frac{28 \\times 4}{184} = \\frac{112}{184} = \\frac{28}{46} = \\frac{14}{23}$$\n    As a decimal, $RR \\approx 0.608695...$\n\n3.  **Odds Ratio ($OR$)**:\n    The odds of infection in the intervention arm are $o_1 = \\frac{p_1}{1-p_1}$.\n    $$1-p_1 = 1 - \\frac{28}{184} = \\frac{184-28}{184} = \\frac{156}{184}$$\n    $$o_1 = \\frac{28/184}{156/184} = \\frac{28}{156} = \\frac{7}{39}$$\n    The odds of infection in the control arm are $o_0 = \\frac{p_0}{1-p_0}$.\n    $$1-p_0 = 1 - \\frac{1}{4} = \\frac{3}{4}$$\n    $$o_0 = \\frac{1/4}{3/4} = \\frac{1}{3}$$\n    The odds ratio is the ratio of these odds:\n    $$OR = \\frac{o_1}{o_0} = \\frac{7/39}{1/3} = \\frac{7 \\times 3}{39} = \\frac{21}{39} = \\frac{7}{13}$$\n    As a decimal, $OR \\approx 0.538461...$\n\nThe second part of the task is to explain which of these estimands most directly aligns with the causal effect targeted by a randomized design, within the potential outcomes framework.\n\nThe primary causal estimand of interest in many contexts is the Average Causal Effect (ACE), also known as the Average Treatment Effect (ATE). For an individual $i$, the causal effect of the intervention is defined as the difference between their potential outcomes: $Y_i(1) - Y_i(0)$, where $Y_i(1)$ is the outcome if they receive the intervention and $Y_i(0)$ is the outcome if they are in the control group. The fundamental problem of causal inference is that we can only observe one of these potential outcomes for any given individual.\n\nThe ATE is the expectation of this individual-level causal effect over the entire population:\n$$ATE = E[Y(1) - Y(0)]$$\nBy the linearity of expectation, the ATE can be written as:\n$$ATE = E[Y(1)] - E[Y(0)]$$\nSince the outcome $Y$ is a binary variable ($0$ or $1$), its expectation is equal to the probability that it takes the value $1$. Therefore:\n$$E[Y(1)] = P(Y(1)=1)$$\n$$E[Y(0)] = P(Y(0)=1)$$\nSo, the ATE is the difference in the probabilities of the outcome under the two conditions, had the entire population been exposed to each:\n$$ATE = P(Y(1)=1) - P(Y(0)=1)$$\nThis is the causal risk difference.\n\nThe key feature of a randomized controlled trial is that the treatment assignment, let's call it $Z$ (where $Z=1$ for intervention, $Z=0$ for control), is independent of the potential outcomes $(Y(1), Y(0))$. This is the property of exchangeability, formally written as $Z \\perp \\{Y(1), Y(0)\\}$. This property, guaranteed by randomization, allows us to identify the ATE from observed data.\n\nThe observed risk in the intervention arm, $p_1$, is an estimate of the conditional probability $P(Y=1 | Z=1)$. Under the potential outcomes framework and the Stable Unit Treatment Value Assumption (SUTVA), the observed outcome is $Y = Y(1)$ for those with $Z=1$. Thus:\n$$P(Y=1 | Z=1) = P(Y(1)=1 | Z=1)$$\nBecause of randomization ($Z \\perp Y(1)$), a person's potential outcome under treatment is independent of whether they were actually assigned to the treatment group. Therefore:\n$$P(Y(1)=1 | Z=1) = P(Y(1)=1)$$\nThis means the observed risk in the intervention group, $p_1$, is a consistent and unbiased estimator of the population-level risk if everyone were treated, $P(Y(1)=1)$.\n\nSimilarly, for the control arm:\n$$P(Y=1 | Z=0) = P(Y(0)=1 | Z=0) = P(Y(0)=1)$$\nThe observed risk in the control group, $p_0$, is a consistent and unbiased estimator of the population-level risk if no one were treated, $P(Y(0)=1)$.\n\nGiven these two results, the observed risk difference, $p_1 - p_0$, is a consistent and unbiased estimator of the Average Treatment Effect:\n$$E[p_1 - p_0] = E[p_1] - E[p_0] = P(Y(1)=1) - P(Y(0)=1) = ATE$$\nThe risk difference directly corresponds to the ATE, which is defined on an additive scale and represents the average of the individual-level causal effects, $E[Y_i(1) - Y_i(0)]$.\n\nThe risk ratio and odds ratio are also valid causal estimands, representing the causal effect on multiplicative scales. The causal risk ratio is $\\frac{P(Y(1)=1)}{P(Y(0)=1)}$ and the causal odds ratio is $\\frac{P(Y(1)=1)/P(Y(1)=0)}{P(Y(0)=1)/P(Y(0)=0)}$. While randomization allows for their unbiased estimation (on the log scale, for instance), they are non-linear transformations of the probabilities. They are not the average of individual-level effects. For instance, the risk ratio is the ratio of averages, $\\frac{E[Y(1)]}{E[Y(0)]}$, not the average of ratios, $E[\\frac{Y(1)}{Y(0)}]$. For this reason, the risk difference is considered the most direct and fundamental measure of the average causal effect targeted by a randomized design, as it preserves the additive structure from the individual level to the population average. It quantifies the absolute change in average risk attributable to the intervention.\n\nThe final answer requested is the numerical value of the risk difference, $p_1 - p_0$, rounded to four significant figures.\n$$p_1 - p_0 = -\\frac{9}{92} \\approx -0.09782608...$$\nRounding to four significant figures yields $-0.09783$.", "answer": "$$ \\boxed{-0.09783} $$", "id": "4941285"}, {"introduction": "Real-world experiments often face the challenge of non-adherence, where participants do not follow their assigned treatment protocol. This exercise [@problem_id:4941311] introduces a simple model to quantify how such 'contamination' impacts the results of an intention-to-treat (ITT) analysis. By deriving the relationship between the true causal effect ($\\tau$) and the observed ITT effect, you will learn why the ITT principle is vital for preserving the benefits of randomization and how to understand the inherent bias it introduces as an estimator of the treatment's full efficacy.", "problem": "A randomized controlled trial is conducted with equal allocation to assignment indicator $Z \\in \\{0,1\\}$, where $Z=1$ indicates assignment to the active intervention and $Z=0$ indicates assignment to control. Due to symmetric contamination (crossover and nonadherence), a fraction $p$ of participants in each arm receive the treatment status opposite to their assignment. Let $D \\in \\{0,1\\}$ denote the actual treatment received. Assume the following simple dilution model:\n- $\\Pr(D=1 \\mid Z=1) = 1 - p$ and $\\Pr(D=1 \\mid Z=0) = p$, with $0  p  \\tfrac{1}{2}$.\n- Stable Unit Treatment Value Assumption (SUTVA) holds, and the treatment has a constant additive effect $\\tau$ on the outcome.\n- Let $Y(0)$ denote the potential outcome under no treatment, and $Y(1) = Y(0) + \\tau$ the potential outcome under treatment.\n- The observed outcome is $Y = Y(0) + D \\tau$.\n- Randomization implies $Z$ is independent of $(Y(0), D(0), D(1))$, and the sample is large so that sample averages equal their corresponding expectations.\n\nThe intention-to-treat (ITT) estimand is defined as the difference in expected observed outcomes by assignment,\n$$\n\\Delta_{\\mathrm{ITT}} = \\mathbb{E}[Y \\mid Z=1] - \\mathbb{E}[Y \\mid Z=0].\n$$\n\nStarting from the above definitions and assumptions, derive an expression for $\\Delta_{\\mathrm{ITT}}$ as a function of $p$ and $\\tau$ that quantifies the impact of contamination rate $p$ on the observed effect. Then, using this expression, compute the bias of $\\Delta_{\\mathrm{ITT}}$ when it is used to estimate the causal effect of treatment receipt $\\tau$.\n\nFinally, evaluate the bias numerically for $p = 0.17$ and $\\tau = 0.73$. Round your answer to four significant figures.", "solution": "The problem is valid and well-posed, providing a clear model for analyzing the effect of treatment contamination in a randomized trial.\n\n**1. Derivation of the Intention-to-Treat (ITT) Estimand**\n\nThe ITT estimand is defined as $\\Delta_{\\mathrm{ITT}} = \\mathbb{E}[Y \\mid Z=1] - \\mathbb{E}[Y \\mid Z=0]$. We first express the conditional expectation of the observed outcome $Y$ given the assignment $Z$.\n\nBased on the model, the observed outcome is $Y = Y(0) + D\\tau$. So, we can write:\n$$ \\mathbb{E}[Y \\mid Z] = \\mathbb{E}[Y(0) + D\\tau \\mid Z] $$\nBy linearity of expectation:\n$$ \\mathbb{E}[Y \\mid Z] = \\mathbb{E}[Y(0) \\mid Z] + \\mathbb{E}[D\\tau \\mid Z] $$\nThe problem states that randomization makes the assignment $Z$ independent of the potential outcome $Y(0)$. Therefore, $\\mathbb{E}[Y(0) \\mid Z] = \\mathbb{E}[Y(0)]$, a constant. The treatment effect $\\tau$ is also a constant, so we can factor it out:\n$$ \\mathbb{E}[Y \\mid Z] = \\mathbb{E}[Y(0)] + \\tau \\mathbb{E}[D \\mid Z] $$\nNow we can express the ITT estimand using this result:\n$$ \\Delta_{\\mathrm{ITT}} = \\left(\\mathbb{E}[Y(0)] + \\tau \\mathbb{E}[D \\mid Z=1]\\right) - \\left(\\mathbb{E}[Y(0)] + \\tau \\mathbb{E}[D \\mid Z=0]\\right) $$\nThe $\\mathbb{E}[Y(0)]$ terms cancel out, leaving:\n$$ \\Delta_{\\mathrm{ITT}} = \\tau \\left(\\mathbb{E}[D \\mid Z=1] - \\mathbb{E}[D \\mid Z=0]\\right) $$\nFor a binary variable $D$, the expectation $\\mathbb{E}[D \\mid Z]$ is simply the probability $\\Pr(D=1 \\mid Z)$. Using the contamination model provided:\n- $\\Pr(D=1 \\mid Z=1) = 1 - p$\n- $\\Pr(D=1 \\mid Z=0) = p$\nSubstituting these probabilities into the expression for $\\Delta_{\\mathrm{ITT}}$:\n$$ \\Delta_{\\mathrm{ITT}} = \\tau \\left((1 - p) - p\\right) $$\n$$ \\Delta_{\\mathrm{ITT}} = \\tau(1 - 2p) $$\nThis expression quantifies how the observed ITT effect is a diluted version of the true causal effect $\\tau$, with the dilution factor being $(1 - 2p)$.\n\n**2. Calculation of the Bias**\n\nThe bias of an estimator is defined as the expected value of the estimator minus the true parameter value it is intended to estimate. Here, we are using $\\Delta_{\\mathrm{ITT}}$ to estimate the causal effect of treatment receipt, $\\tau$.\n$$ \\text{Bias}(\\Delta_{\\mathrm{ITT}}) = \\Delta_{\\mathrm{ITT}} - \\tau $$\nSubstituting the expression we derived for $\\Delta_{\\mathrm{ITT}}$:\n$$ \\text{Bias} = \\tau(1 - 2p) - \\tau $$\n$$ \\text{Bias} = \\tau - 2p\\tau - \\tau $$\n$$ \\text{Bias} = -2p\\tau $$\nThis result shows that the ITT effect, when used as an estimate of the per-protocol effect $\\tau$, is biased towards the null. The magnitude of the bias is directly proportional to both the contamination rate $p$ and the true effect size $\\tau$.\n\n**3. Numerical Evaluation**\n\nFinally, we evaluate the bias for the given values of $p = 0.17$ and $\\tau = 0.73$.\n$$ \\text{Bias} = -2 \\times 0.17 \\times 0.73 $$\n$$ \\text{Bias} = -0.34 \\times 0.73 $$\n$$ \\text{Bias} = -0.2482 $$\nRounded to four significant figures, the bias is $-0.2482$.", "answer": "$$\\boxed{-0.2482}$$", "id": "4941311"}]}