{"hands_on_practices": [{"introduction": "The foundation of evaluating any diagnostic test lies in its ability to correctly classify individuals into diseased and non-diseased groups. This first exercise provides practice in calculating the four fundamental performance metrics—sensitivity, specificity, and their corresponding error rates—directly from a standard confusion matrix. Mastering these calculations is the first step toward understanding a test's intrinsic accuracy, independent of the disease's rarity or commonness in a population [@problem_id:4332588].", "problem": "A targeted next-generation sequencing assay is evaluated for detecting a pathogenic variant in a precision oncology setting. In a multi-center case-control evaluation, samples are classified by the assay as positive or negative, and a gold-standard orthogonal method determines the true variant status. Let the true positive count be $TP = 48$, false negative count be $FN = 12$, false positive count be $FP = 18$, and true negative count be $TN = 122$. Using only the foundational definitions of conditional probability and error rates in binary classification, compute the following four performance characteristics: sensitivity, specificity, false positive rate, and false negative rate. Then, using a first-principles argument about conditional probabilities under retrospective (case-control) sampling, determine which of these four quantities are invariant to disease prevalence under case-control sampling.\n\nReport the final answer as a single row matrix in the order $\\bigl(\\text{sensitivity},\\ \\text{specificity},\\ \\text{false positive rate},\\ \\text{false negative rate},\\ I_{\\text{sensitivity}},\\ I_{\\text{specificity}},\\ I_{\\text{false positive rate}},\\ I_{\\text{false negative rate}}\\bigr)$, where $I_{\\cdot}$ is $1$ if the quantity is prevalence-invariant under case-control sampling and $0$ otherwise. Express the four rates as exact reduced fractions. No rounding is permitted. The invariance indicators must be reported as exact integers $0$ or $1$.", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and self-contained. All data required for the calculations are provided, and the concepts are fundamental to statistics and clinical diagnostics. The problem is therefore valid.\n\nLet $D^+$ be the event that an individual has the pathogenic variant (is a case), and $D^-$ be the event that an individual does not have the variant (is a control). Let $T^+$ be the event that the assay result is positive, and $T^-$ be the event that the assay result is negative.\n\nThe provided data are the counts for the four possible outcomes in a binary classification confusion matrix:\n- True Positives ($TP$): The number of individuals with the variant who test positive. $TP = 48$.\n- False Negatives ($FN$): The number of individuals with the variant who test negative. $FN = 12$.\n- False Positives ($FP$): The number of individuals without the variant who test positive. $FP = 18$.\n- True Negatives ($TN$): The number of individuals without the variant who test negative. $TN = 122$.\n\nFrom these counts, we can determine the total number of cases and controls in the study:\n- Total number of cases (individuals with the variant): $P = TP + FN = 48 + 12 = 60$.\n- Total number of controls (individuals without the variant): $N = FP + TN = 18 + 122 = 140$.\n\nThe four performance characteristics are computed based on their definitions as conditional probabilities.\n\n1.  **Sensitivity (True Positive Rate, $TPR$)**\n    Sensitivity is the probability of a positive test result given that the individual truly has the variant. It is a measure of how well the test identifies true cases.\n    $$ \\text{Sensitivity} = P(T^+ | D^+) = \\frac{\\text{Number of individuals with } T^+ \\text{ and } D^+}{\\text{Total number of individuals with } D^+} = \\frac{TP}{TP + FN} $$\n    Substituting the given values:\n    $$ \\text{Sensitivity} = \\frac{48}{48 + 12} = \\frac{48}{60} = \\frac{4 \\times 12}{5 \\times 12} = \\frac{4}{5} $$\n\n2.  **Specificity (True Negative Rate, $TNR$)**\n    Specificity is the probability of a negative test result given that the individual truly does not have the variant. It is a measure of how well the test identifies true non-cases.\n    $$ \\text{Specificity} = P(T^- | D^-) = \\frac{\\text{Number of individuals with } T^- \\text{ and } D^-}{\\text{Total number of individuals with } D^-} = \\frac{TN}{TN + FP} $$\n    Substituting the given values:\n    $$ \\text{Specificity} = \\frac{122}{122 + 18} = \\frac{122}{140} = \\frac{61 \\times 2}{70 \\times 2} = \\frac{61}{70} $$\n\n3.  **False Positive Rate ($FPR$)**\n    The false positive rate is the probability of a positive test result given that the individual truly does not have the variant. This is a Type I error rate.\n    $$ FPR = P(T^+ | D^-) = \\frac{\\text{Number of individuals with } T^+ \\text{ and } D^-}{\\text{Total number of individuals with } D^-} = \\frac{FP}{FP + TN} $$\n    It is also definitionally related to specificity: $FPR = 1 - \\text{Specificity}$.\n    Substituting the given values:\n    $$ FPR = \\frac{18}{18 + 122} = \\frac{18}{140} = \\frac{9 \\times 2}{70 \\times 2} = \\frac{9}{70} $$\n\n4.  **False Negative Rate ($FNR$)**\n    The false negative rate is the probability of a negative test result given that the individual truly has the variant. This is a Type II error rate.\n    $$ FNR = P(T^- | D^+) = \\frac{\\text{Number of individuals with } T^- \\text{ and } D^+}{\\text{Total number of individuals with } D^+} = \\frac{FN}{TP + FN} $$\n    It is also definitionally related to sensitivity: $FNR = 1 - \\text{Sensitivity}$.\n    Substituting the given values:\n    $$ FNR = \\frac{12}{48 + 12} = \\frac{12}{60} = \\frac{1 \\times 12}{5 \\times 12} = \\frac{1}{5} $$\n\nNext, we must determine which of these quantities are invariant to disease prevalence under case-control sampling.\n\nA case-control study is a retrospective study design where subjects are selected based on their known outcome status. Here, a group of \"cases\" (individuals with the variant, $D^+$) and a group of \"controls\" (individuals without the variant, $D^-$) are assembled. The proportion of cases to controls in the study sample is determined by the study design, not by the natural prevalence of the disease in the population. Prevalence, $\\pi = P(D^+)$, is the a priori probability of having the disease in the general population, from which a study sample might be drawn. In a case-control design, the sample prevalence is artificially fixed and is not an estimate of $\\pi$.\n\nWe analyze the definitions of the four metrics in light of this sampling scheme:\n\n-   **Sensitivity, $P(T^+|D^+)$**: This probability is conditioned on the event $D^+$. The calculation $\\frac{TP}{TP+FN}$ is performed entirely within the group of cases. Since the estimation of sensitivity only depends on the subjects who are known to have the disease, it is independent of how many subjects without the disease were included in the study. Therefore, sensitivity is an intrinsic characteristic of the test's performance on the diseased population and is **invariant** to the disease prevalence in the general population. The indicator $I_{\\text{sensitivity}}$ is $1$.\n\n-   **Specificity, $P(T^-|D^-)$**: This probability is conditioned on the event $D^-$. The calculation $\\frac{TN}{TN+FP}$ is performed entirely within the group of controls. Similar to sensitivity, the estimation of specificity only depends on the subjects known to be disease-free. It is therefore an intrinsic characteristic of the test's performance on the non-diseased population and is **invariant** to disease prevalence. The indicator $I_{\\text{specificity}}$ is $1$.\n\n-   **False Positive Rate, $P(T^+|D^-)$**: This probability is also conditioned on the event $D^-$. Its calculation, $\\frac{FP}{FP+TN}$, is performed entirely within the control group. As it is simply $1 - \\text{Specificity}$, it shares the same properties. The FPR is **invariant** to disease prevalence. The indicator $I_{\\text{false positive rate}}$ is $1$.\n\n-   **False Negative Rate, $P(T^-|D^+)$**: This probability is also conditioned on the event $D^+$. Its calculation, $\\frac{FN}{TP+FN}$, is performed entirely within the case group. As it is simply $1 - \\text{Sensitivity}$, it also shares the same properties. The FNR is **invariant** to disease prevalence. The indicator $I_{\\text{false negative rate}}$ is $1$.\n\nIn summary, all four metrics—sensitivity, specificity, false positive rate, and false negative rate—are defined by probabilities conditional on the true disease status. A case-control study design allows for the direct estimation of these quantities precisely because it samples from the distinct strata of diseased and non-diseased individuals. These metrics are properties of the test itself, not the population in which it is used. In contrast, metrics like Positive Predictive Value ($PPV = P(D^+|T^+)$) and Negative Predictive Value ($NPV = P(D^-|T^-)$) are critically dependent on prevalence $\\pi = P(D^+)$ via Bayes' theorem and cannot be directly estimated from a case-control study without external knowledge of $\\pi$.\n\nThe calculated values and their invariance indicators are:\n- Sensitivity: $\\frac{4}{5}$, Invariant ($I=1$)\n- Specificity: $\\frac{61}{70}$, Invariant ($I=1$)\n- False Positive Rate: $\\frac{9}{70}$, Invariant ($I=1$)\n- False Negative Rate: $\\frac{1}{5}$, Invariant ($I=1$)\n\nThe final answer is presented as a single row matrix in the specified order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{4}{5}  \\frac{61}{70}  \\frac{9}{70}  \\frac{1}{5}  1  1  1  1\n\\end{pmatrix}\n}\n$$", "id": "4332588"}, {"introduction": "While sensitivity and specificity are crucial, a practicing clinician is often more concerned with the question: \"Given this test result, what is the probability my patient has the disease?\" This practice shifts the focus to calculating the Positive and Negative Predictive Values ($PPV$ and $NPV$) using Bayes' theorem. This exercise powerfully illustrates how a test's real-world utility is profoundly influenced by the prevalence of the disease, a concept known as the base rate fallacy [@problem_id:4908588].", "problem": "A biotechnology company is evaluating a new blood-based screening assay for a rare genetic disorder in an asymptomatic adult population. Let $D$ denote the event that a randomly selected individual has the disorder, and let $T^{+}$ and $T^{-}$ denote, respectively, a positive and negative screening test outcome. The assay has sensitivity $\\operatorname{Se}=P(T^{+}\\mid D)=0.95$ and specificity $\\operatorname{Sp}=P(T^{-}\\mid D^{c})=0.90$, and the disorder has prevalence $\\pi=P(D)=0.01$ in the target screening population.\n\nStarting from the definitions of sensitivity, specificity, and prevalence, and using only the axioms of probability together with Bayes’ theorem and the law of total probability, derive expressions for the Positive Predictive Value (PPV), $P(D\\mid T^{+})$, and the Negative Predictive Value (NPV), $P(D^{c}\\mid T^{-})$. Then evaluate these expressions numerically for the given values $\\operatorname{Se}=0.95$, $\\operatorname{Sp}=0.90$, and $\\pi=0.01$.\n\nFinally, provide a brief interpretation of the computed $P(D\\mid T^{+})$ and $P(D^{c}\\mid T^{-})$ in the context of rare disease screening, focusing on how low prevalence influences the credibility of positive versus negative results.\n\nRound each of your final numerical results to four significant figures, and report them as a two-entry row vector $\\big(\\operatorname{PPV},\\operatorname{NPV}\\big)$ with no units.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\nThe problem provides the following definitions and values:\n- $D$: The event that a randomly selected individual has the disorder.\n- $D^c$: The event that a randomly selected individual does not have the disorder.\n- $T^{+}$: The event of a positive screening test outcome.\n- $T^{-}$: The event of a negative screening test outcome.\n- Sensitivity: $\\operatorname{Se} = P(T^{+} \\mid D) = 0.95$.\n- Specificity: $\\operatorname{Sp} = P(T^{-} \\mid D^c) = 0.90$.\n- Prevalence: $\\pi = P(D) = 0.01$.\n\nThe objectives are to derive expressions for the Positive Predictive Value, $\\operatorname{PPV} = P(D \\mid T^{+})$, and the Negative Predictive Value, $\\operatorname{NPV} = P(D^c \\mid T^{-})$, evaluate them numerically, and provide an interpretation.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard application of Bayesian probability to the evaluation of diagnostic tests, a core topic in biostatistics and epidemiology.\n- **Scientifically Grounded:** The concepts of sensitivity, specificity, prevalence, PPV, and NPV are fundamental and well-established principles in medical diagnostics. The values provided ($\\operatorname{Se}=0.95$, $\\operatorname{Sp}=0.90$, $\\pi=0.01$) are realistic for a screening test for a rare disease.\n- **Well-Posed:** The problem is clearly stated, providing all necessary information to derive and calculate the required probabilities. The quantities to be calculated are uniquely determined by the givens.\n- **Objective:** The problem is phrased in precise, objective, and standard mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. The solution will proceed by first deriving the general symbolic expressions for PPV and NPV and then substituting the given numerical values.\n\n**Derivation of Positive Predictive Value (PPV)**\nThe Positive Predictive Value is the probability that an individual has the disorder given that they tested positive, denoted as $P(D \\mid T^{+})$. We use Bayes' theorem to express this quantity:\n$$ \\operatorname{PPV} = P(D \\mid T^{+}) = \\frac{P(T^{+} \\mid D) P(D)}{P(T^{+})} $$\nThe denominator, $P(T^{+})$, is the overall probability of a positive test result. We can expand this using the law of total probability over the partition of the sample space into those with the disorder ($D$) and those without ($D^c$):\n$$ P(T^{+}) = P(T^{+} \\mid D) P(D) + P(T^{+} \\mid D^c) P(D^c) $$\nWe are given or can derive each term in this expansion:\n- $P(T^{+} \\mid D) = \\operatorname{Se}$ (Sensitivity)\n- $P(D) = \\pi$ (Prevalence)\n- $P(D^c) = 1 - P(D) = 1 - \\pi$ (Axiom of probability for complements)\n- $P(T^{+} \\mid D^c)$ is the probability of a positive test in a disease-free individual (the false positive rate). It is the complement of the specificity, since for a given disease state, the test must be either positive or negative. Thus, $P(T^{+} \\mid D^c) = 1 - P(T^{-} \\mid D^c) = 1 - \\operatorname{Sp}$.\n\nSubstituting these into the expression for $P(T^{+})$:\n$$ P(T^{+}) = (\\operatorname{Se})(\\pi) + (1 - \\operatorname{Sp})(1 - \\pi) $$\nNow, substituting this entire expression back into the formula for PPV:\n$$ \\operatorname{PPV} = \\frac{(\\operatorname{Se})(\\pi)}{(\\operatorname{Se})(\\pi) + (1 - \\operatorname{Sp})(1 - \\pi)} $$\nThis is the general expression for the Positive Predictive Value in terms of sensitivity, specificity, and prevalence.\n\n**Derivation of Negative Predictive Value (NPV)**\nThe Negative Predictive Value is the probability that an individual is free of the disorder given that they tested negative, denoted as $P(D^c \\mid T^{-})$. Again, applying Bayes' theorem:\n$$ \\operatorname{NPV} = P(D^c \\mid T^{-}) = \\frac{P(T^{-} \\mid D^c) P(D^c)}{P(T^{-})} $$\nThe denominator, $P(T^{-})$, is the overall probability of a negative test. Using the law of total probability:\n$$ P(T^{-}) = P(T^{-} \\mid D^c) P(D^c) + P(T^{-} \\mid D) P(D) $$\nWe identify the terms:\n- $P(T^{-} \\mid D^c) = \\operatorname{Sp}$ (Specificity)\n- $P(D^c) = 1 - \\pi$\n- $P(D) = \\pi$\n- $P(T^{-} \\mid D)$ is the probability of a negative test in a diseased individual (the false negative rate). It is the complement of the sensitivity: $P(T^{-} \\mid D) = 1 - P(T^{+} \\mid D) = 1 - \\operatorname{Se}$.\n\nSubstituting these into the expression for $P(T^{-})$:\n$$ P(T^{-}) = (\\operatorname{Sp})(1 - \\pi) + (1 - \\operatorname{Se})(\\pi) $$\nNow, substituting this back into the formula for NPV:\n$$ \\operatorname{NPV} = \\frac{(\\operatorname{Sp})(1 - \\pi)}{(\\operatorname{Sp})(1 - \\pi) + (1 - \\operatorname{Se})(\\pi)} $$\nThis is the general expression for the Negative Predictive Value.\n\n**Numerical Evaluation**\nWe substitute the given values: $\\operatorname{Se} = 0.95$, $\\operatorname{Sp} = 0.90$, and $\\pi = 0.01$.\n\nFor PPV:\n$$ \\operatorname{PPV} = \\frac{(0.95)(0.01)}{(0.95)(0.01) + (1 - 0.90)(1 - 0.01)} = \\frac{0.0095}{0.0095 + (0.10)(0.99)} = \\frac{0.0095}{0.0095 + 0.099} = \\frac{0.0095}{0.1085} $$\n$$ \\operatorname{PPV} \\approx 0.087557603... $$\nRounding to four significant figures, $\\operatorname{PPV} \\approx 0.08756$.\n\nFor NPV:\n$$ \\operatorname{NPV} = \\frac{(0.90)(1 - 0.01)}{(0.90)(1 - 0.01) + (1 - 0.95)(0.01)} = \\frac{(0.90)(0.99)}{(0.90)(0.99) + (0.05)(0.01)} = \\frac{0.891}{0.891 + 0.0005} = \\frac{0.891}{0.8915} $$\n$$ \\operatorname{NPV} \\approx 0.999439147... $$\nRounding to four significant figures, $\\operatorname{NPV} \\approx 0.9994$.\n\n**Interpretation**\nThe computed Positive Predictive Value is $\\operatorname{PPV} \\approx 0.08756$. This means that if a person from this population receives a positive test result, there is only an $8.756\\%$ chance that they actually have the disorder. Despite the test's high sensitivity ($\\operatorname{Se}=0.95$), a positive result is far more likely to be a false positive than a true positive. This counterintuitive result is a direct consequence of the low prevalence of the disease ($\\pi=0.01$). Because the disorder is rare, the absolute number of healthy individuals who test positive (false positives, determined by $1-\\operatorname{Sp}$) vastly outnumbers the absolute number of diseased individuals who test positive (true positives, determined by $\\operatorname{Se}$). Specifically, in a population of $100,000$, there are $1,000$ people with the disease and $99,000$ without. The test would identify $1000 \\times 0.95 = 950$ true positives and $99000 \\times (1-0.90) = 9900$ false positives. The PPV is thus $950 / (950 + 9900) \\approx 0.08756$.\n\nThe computed Negative Predictive Value is $\\operatorname{NPV} \\approx 0.9994$. This means that if a person receives a negative test result, there is a $99.94\\%$ chance that they are indeed free of the disorder. A negative result from this screening test is extremely reliable.\n\nIn summary, for screening rare diseases, a negative result is highly credible and effective at ruling out the condition. A positive result, however, has low credibility on its own and should be interpreted not as a diagnosis, but as an indication for further, more specific (and often more invasive or expensive) confirmatory diagnostic testing. This phenomenon is known as the base rate fallacy.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.08756  0.9994\n\\end{pmatrix}\n}\n$$", "id": "4908588"}, {"introduction": "Many modern diagnostic markers provide a continuous score rather than a simple positive/negative result, requiring a more nuanced evaluation. This practice moves beyond static $2 \\times 2$ tables to the dynamic world of continuous data, introducing the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) as comprehensive, threshold-independent measures of discriminatory power. Through a guided simulation, you will actively explore how the separation between the score distributions for diseased and non-diseased populations directly impacts overall test performance [@problem_id:4908600].", "problem": "You are given two continuous diagnostic score distributions under disease status: $S \\mid D=1$ for diseased subjects and $S \\mid D=0$ for non-diseased subjects. Both distributions are assumed to be normal (Gaussian) with means $\\mu_1$ and $\\mu_0$, and standard deviations $\\sigma_1$ and $\\sigma_0$, respectively. A binary decision rule classifies a subject as test-positive if $S \\ge t$ for a threshold $t$, and test-negative otherwise. The fundamental base for this problem consists of the following definitions: sensitivity (true positive rate) is $\\operatorname{Se}(t) = \\mathbb{P}(S \\ge t \\mid D=1)$, specificity (true negative rate) is $\\operatorname{Sp}(t) = \\mathbb{P}(S  t \\mid D=0)$, and the Area Under the Receiver Operating Characteristic (ROC) curve (AUC) is the probability $\\mathbb{P}(S_1  S_0)$ where $S_1 \\sim S \\mid D=1$ and $S_0 \\sim S \\mid D=0$ are independent.\n\nDesign a simulation to explore how varying the separation between $S \\mid D=1$ and $S \\mid D=0$ affects $\\operatorname{Se}$, $\\operatorname{Sp}$, and AUC under the normal model. Your program must:\n\n- Simulate $n_1$ independent samples from $S \\mid D=1 \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)$ and $n_0$ independent samples from $S \\mid D=0 \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ for each parameter set.\n- Choose the threshold $t^\\ast$ that maximizes Youden's index $J(t) = \\operatorname{Se}(t) + \\operatorname{Sp}(t) - 1$ over a grid of thresholds spanning the range of the combined sample. Thresholds must be evaluated on a uniform grid of size $K$ across the empirical range; take $K$ as $1001$.\n- Estimate $\\operatorname{Se}$ and $\\operatorname{Sp}$ at $t^\\ast$ empirically as proportions using the simulated data.\n- Estimate AUC empirically using the Mann–Whitney probability interpretation $\\mathbb{P}(S_1  S_0)$ computed from ranks of the combined sample (express AUC as a decimal, not a percentage).\n- Use fixed random seeds for reproducibility for each test case.\n\nExpress all answers as decimals. No physical units are involved. The final output from your program must be a single line containing a comma-separated list enclosed in square brackets. Each element of the list must itself be a list of three floats $[\\operatorname{Se},\\operatorname{Sp},\\mathrm{AUC}]$ rounded to exactly four decimal places, with no spaces anywhere in the line. For example, the format must be like $[[0.5000,0.5000,0.5000],[0.8000,0.7000,0.8500]]$.\n\nTest suite (each case is $(\\mu_1,\\mu_0,\\sigma_1,\\sigma_0,n_1,n_0,\\text{seed})$):\n- Case $1$: $(0,0,1,1,50000,50000,1)$, boundary condition of no separation.\n- Case $2$: $(1,0,1,1,50000,50000,2)$, modest separation with equal variances.\n- Case $3$: $(2,0,1,1,50000,50000,3)$, larger separation with equal variances.\n- Case $4$: $(1,0,1.5,1,50000,50000,4)$, unequal variances with modest mean separation.\n- Case $5$: $(4,0,1,1,50000,50000,5)$, very large separation with equal variances.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact format $[[\\operatorname{Se}_1,\\operatorname{Sp}_1,\\mathrm{AUC}_1],[\\operatorname{Se}_2,\\operatorname{Sp}_2,\\mathrm{AUC}_2],\\dots,]$, with each numeric entry rounded to four decimal places and no spaces.", "solution": "The problem requires the design of a numerical simulation to evaluate the performance of a diagnostic test under a binormal model. The performance is quantified by sensitivity ($\\operatorname{Se}$), specificity ($\\operatorname{Sp}$), and the Area Under the Receiver Operating Characteristic curve (AUC). The simulation will explore how these metrics change as the parameters of the underlying score distributions are varied.\n\n### 1. Problem Formulation and Statistical Model\n\nThe core of the problem lies in the statistical modeling of diagnostic scores. We are given that scores for diseased subjects, $S \\mid D=1$, and non-diseased subjects, $S \\mid D=0$, follow normal (Gaussian) distributions:\n$$\nS \\mid D=1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2) \\\\\nS \\mid D=0 \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)\n$$\nA subject is classified as positive if their score $S$ exceeds a certain threshold $t$, i.e., $S \\ge t$. Otherwise, they are classified as negative.\n\n### 2. Performance Metrics\nThe quality of this classification rule is assessed using three standard metrics:\n\n-   **Sensitivity ($\\operatorname{Se}$)**: The probability of correctly identifying a diseased subject. It is the true positive rate.\n    $$\n    \\operatorname{Se}(t) = \\mathbb{P}(S \\ge t \\mid D=1)\n    $$\n\n-   **Specificity ($\\operatorname{Sp}$)**: The probability of correctly identifying a non-diseased subject. It is the true negative rate.\n    $$\n    \\operatorname{Sp}(t) = \\mathbb{P}(S  t \\mid D=0)\n    $$\n\n-   **Area Under the ROC Curve (AUC)**: The ROC curve is a plot of $\\operatorname{Se}(t)$ versus $1 - \\operatorname{Sp}(t)$ for all possible thresholds $t$. The AUC represents the overall discriminative ability of the score, independent of a particular threshold. It is equivalent to the probability that a randomly selected diseased subject has a higher score than a randomly selected non-diseased subject.\n    $$\n    \\mathrm{AUC} = \\mathbb{P}(S_1  S_0)\n    $$\n    where $S_1$ and $S_0$ are independent random variables drawn from the diseased and non-diseased score distributions, respectively.\n\n### 3. Simulation Methodology\n\nFor each parameter set $(\\mu_1, \\mu_0, \\sigma_1, \\sigma_0, n_1, n_0, \\text{seed})$, the following procedure is executed:\n\n#### 3.1. Data Generation\nUsing a pseudorandom number generator initialized with the specified seed, we simulate two sets of scores:\n-   A sample of $n_1$ scores for the diseased group: $\\{s_{1,i}\\}_{i=1}^{n_1}$ from $\\mathcal{N}(\\mu_1, \\sigma_1^2)$.\n-   A sample of $n_0$ scores for the non-diseased group: $\\{s_{0,j}\\}_{j=1}^{n_0}$ from $\\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n\n#### 3.2. Empirical AUC Estimation\nThe AUC is estimated using the Mann-Whitney U statistic, which is directly related to the probability $\\mathbb{P}(S_1  S_0)$. The procedure is as follows:\n1.  Combine the two samples $\\{s_{1,i}\\}$ and $\\{s_{0,j}\\}$ into a single dataset of size $N = n_1 + n_0$.\n2.  Assign ranks to all scores in the combined dataset. If ties occur, the average rank is used. Let $R_1$ be the sum of the ranks corresponding to the scores from the diseased group, $\\{s_{1,i}\\}$.\n3.  Calculate the Mann-Whitney U statistic for the diseased group:\n    $$\n    U_1 = R_1 - \\frac{n_1(n_1+1)}{2}\n    $$\n4.  The empirical AUC is then given by the ratio of $U_1$ to the total number of pairs of scores between the two groups:\n    $$\n    \\widehat{\\mathrm{AUC}} = \\frac{U_1}{n_1 n_0}\n    $$\nThis estimator is a non-parametric and robust measure of the separation between the two distributions.\n\n#### 3.3. Optimal Threshold Selection\nThe problem specifies selecting a single \"optimal\" threshold, $t^\\ast$, by maximizing Youden's index, $J(t)$. Youden's index measures the vertical distance from the ROC curve to the chance line and is defined as:\n$$\nJ(t) = \\operatorname{Se}(t) + \\operatorname{Sp}(t) - 1\n$$\nTo find the threshold $\\hat{t}^\\ast$ that maximizes the empirical Youden's index $\\hat{J}(t)$, we perform a grid search:\n1.  A uniform grid of $K=1001$ candidate thresholds is created. This grid spans the entire range of observed scores, from the minimum to the maximum value in the combined sample. Let this grid be $\\{t_k\\}_{k=1}^{K}$.\n2.  For each threshold $t_k$ in the grid, we compute the empirical sensitivity and specificity from our simulated samples:\n    $$\n    \\widehat{\\operatorname{Se}}(t_k) = \\frac{1}{n_1} \\sum_{i=1}^{n_1} \\mathbb{I}(s_{1,i} \\ge t_k) \\\\\n    \\widehat{\\operatorname{Sp}}(t_k) = \\frac{1}{n_0} \\sum_{j=1}^{n_0} \\mathbb{I}(s_{0,j}  t_k)\n    $$\n    where $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if the condition is true and $0$ otherwise.\n3.  The empirical Youden's index is calculated for each threshold:\n    $$\n    \\hat{J}(t_k) = \\widehat{\\operatorname{Se}}(t_k) + \\widehat{\\operatorname{Sp}}(t_k) - 1\n    $$\n4.  The optimal threshold, $\\hat{t}^\\ast$, is the threshold $t_k$ that yields the maximum value of $\\hat{J}(t_k)$.\n    $$\n    \\hat{t}^\\ast = \\underset{t_k}{\\operatorname{argmax}} \\left\\{ \\hat{J}(t_k) \\right\\}\n    $$\n\n#### 3.4. Final Metric Estimation\nThe final values for sensitivity and specificity are the empirical estimates calculated at this optimal threshold $\\hat{t}^\\ast$:\n-   $\\widehat{\\operatorname{Se}} = \\widehat{\\operatorname{Se}}(\\hat{t}^\\ast)$\n-   $\\widehat{\\operatorname{Sp}} = \\widehat{\\operatorname{Sp}}(\\hat{t}^\\ast)$\n\nThe three resulting metrics, $[\\widehat{\\operatorname{Se}}, \\widehat{\\operatorname{Sp}}, \\widehat{\\mathrm{AUC}}]$, are then collected for each test case. This entire process is deterministic for a given seed, ensuring reproducibility. The large sample sizes ($n_1=n_0=50000$) ensure that the empirical estimates are precise and close to their theoretical counterparts.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef solve():\n    \"\"\"\n    Runs a simulation to evaluate diagnostic test performance for several parameter sets.\n    \"\"\"\n\n    # Test suite (each case is (mu1, mu0, sigma1, sigma0, n1, n0, seed))\n    test_cases = [\n        (0, 0, 1, 1, 50000, 50000, 1),\n        (1, 0, 1, 1, 50000, 50000, 2),\n        (2, 0, 1, 1, 50000, 50000, 3),\n        (1, 0, 1.5, 1, 50000, 50000, 4),\n        (4, 0, 1, 1, 50000, 50000, 5),\n    ]\n\n    results = []\n    K = 1001  # Number of thresholds for grid search\n\n    for case in test_cases:\n        mu1, mu0, sigma1, sigma0, n1, n0, seed = case\n\n        # 1. Set random seed and generate samples\n        rng = np.random.default_rng(seed)\n        s1 = rng.normal(loc=mu1, scale=sigma1, size=n1)  # Diseased scores\n        s0 = rng.normal(loc=mu0, scale=sigma0, size=n0)  # Non-diseased scores\n\n        # 2. Estimate AUC using Mann-Whitney U statistic from ranks\n        combined_samples = np.concatenate((s1, s0))\n        # Assign ranks, using 'average' method for ties\n        ranks = rankdata(combined_samples, method='average')\n        # Sum of ranks for the diseased group (s1)\n        r1 = np.sum(ranks[:n1])\n        # Calculate U statistic for the diseased group\n        u1 = r1 - (n1 * (n1 + 1) / 2.0)\n        # Calculate AUC\n        auc = u1 / (n1 * n0)\n\n        # 3. Find the optimal threshold t* by maximizing Youden's Index J(t)\n        # Create a grid of thresholds spanning the empirical range\n        min_val = np.min(combined_samples)\n        max_val = np.max(combined_samples)\n        thresholds = np.linspace(min_val, max_val, K)\n\n        # Vectorized calculation of Se and Sp for all thresholds\n        # s1[:, np.newaxis] broadcasts s1 to compare against all thresholds\n        se_values = np.sum(s1[:, np.newaxis] = thresholds, axis=0) / n1\n        sp_values = np.sum(s0[:, np.newaxis]  thresholds, axis=0) / n0\n\n        # Calculate Youden's index for all thresholds\n        j_values = se_values + sp_values - 1\n\n        # Find the index of the threshold that maximizes J\n        best_idx = np.argmax(j_values)\n        \n        # 4. Get the optimal Se and Sp at that threshold\n        se_star = se_values[best_idx]\n        sp_star = sp_values[best_idx]\n\n        results.append([se_star, sp_star, auc])\n\n    # Format the final output string as specified\n    inner_parts = []\n    for se, sp, auc_val in results:\n        inner_parts.append(f'[{se:.4f},{sp:.4f},{auc_val:.4f}]')\n    \n    final_output = f\"[{','.join(inner_parts)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "4908600"}]}