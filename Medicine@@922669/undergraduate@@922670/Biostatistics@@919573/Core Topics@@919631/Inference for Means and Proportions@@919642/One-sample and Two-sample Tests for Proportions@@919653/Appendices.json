{"hands_on_practices": [{"introduction": "Before relying on large-sample approximations, it is essential to grasp the exact probability model that underpins tests for proportions. This exercise grounds your understanding in the binomial distribution, demonstrating how to calculate a p-value directly from its definition. It also serves as a critical lesson in checking assumptions, helping you recognize when approximations are unreliable and an exact test is necessary for valid inference [@problem_id:4934180].", "problem": "A clinical laboratory evaluates a rapid antibody assay for a pathogen. In a validation run, the assay is administered to $n=20$ independent individuals from a population where the manufacturer claims the true positivity probability is $p=0.2$. In the validation run, exactly $x=2$ individuals test positive. You are asked to perform a one-sample test for a proportion with the null hypothesis $H_{0}: p=0.2$ against the one-sided alternative $H_{1}: p0.2$.\n\nStarting from the definition of independent Bernoulli trials and the induced count distribution for the number of positives, derive the exact lower-tailed $p$-value under $H_{0}: p=0.2$ and compute its numerical value. Round your answer to four significant figures and express it as a decimal.\n\nThen, starting from the Central Limit Theorem (CLT) and the usual regularity conditions under which a binomial count can be well-approximated by a normal distribution, explain why the normal approximation is unreliable in this setting. Your discussion should be grounded in the properties of the sampling distribution under $H_{0}$ (including discreteness, skewness, and effective sample size considerations) and should not rely on any shortcut formulas.", "solution": "The problem is assessed to be valid as it is scientifically grounded in established principles of biostatistics, is well-posed with a clear objective and sufficient data, and is free from ambiguity or contradiction.\n\nThe first part of the problem requires the calculation of an exact $p$-value for a one-sample test for a proportion. The second part requires an explanation of why a normal approximation is not suitable in this case, based on first principles.\n\n### Part 1: Calculation of the Exact $p$-value\n\nThe experimental setup consists of $n=20$ independent trials, where each trial (an individual being tested) can result in one of two outcomes: 'positive' or 'negative'. The probability of a 'positive' outcome is denoted by $p$. Under the null hypothesis, $H_{0}$, this probability is specified as $p_0 = 0.2$. This scenario is precisely described by a sequence of independent Bernoulli trials.\n\nLet $X$ be the random variable representing the total number of positive tests out of the $n=20$ individuals. The distribution of $X$ is a binomial distribution, denoted as $X \\sim \\text{Binomial}(n, p)$. The probability mass function (PMF) for observing exactly $k$ successes in $n$ trials is given by:\n$$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n\nThe null and alternative hypotheses are:\n$$H_{0}: p = 0.2$$\n$$H_{1}: p  0.2$$\n\nThe alternative hypothesis is one-sided (lower-tailed). The observed number of positive tests in the validation run is $x=2$. The $p$-value is defined as the probability of observing a result as extreme as or more extreme than the observed result, assuming the null hypothesis is true. For a lower-tailed test, this means calculating the probability of observing $x=2$ or fewer positive tests.\n\nTherefore, the $p$-value is $P(X \\le 2)$ under the assumption that $p=0.2$. We calculate this by summing the probabilities for $k=0$, $k=1$, and $k=2$:\n$$p\\text{-value} = P(X \\le 2 | p=0.2) = \\sum_{k=0}^{2} P(X=k)$$\n$$p\\text{-value} = \\sum_{k=0}^{2} \\binom{20}{k} (0.2)^k (1-0.2)^{20-k}$$\n\nLet's compute each term:\nFor $k=0$:\n$$P(X=0) = \\binom{20}{0} (0.2)^0 (0.8)^{20} = 1 \\cdot 1 \\cdot (0.8)^{20} = (0.8)^{20}$$\nFor $k=1$:\n$$P(X=1) = \\binom{20}{1} (0.2)^1 (0.8)^{19} = 20 \\cdot (0.2) \\cdot (0.8)^{19} = 4 \\cdot (0.8)^{19}$$\nFor $k=2$:\n$$P(X=2) = \\binom{20}{2} (0.2)^2 (0.8)^{18} = \\frac{20 \\cdot 19}{2 \\cdot 1} \\cdot (0.04) \\cdot (0.8)^{18} = 190 \\cdot (0.04) \\cdot (0.8)^{18} = 7.6 \\cdot (0.8)^{18}$$\n\nNow we compute the numerical values:\n$$(0.8)^{20} \\approx 0.011529215046$$\n$$4 \\cdot (0.8)^{19} = 4 \\cdot \\frac{(0.8)^{20}}{0.8} = 5 \\cdot (0.8)^{20} \\approx 0.05764607523$$\n$$7.6 \\cdot (0.8)^{18} = 7.6 \\cdot \\frac{(0.8)^{20}}{(0.8)^2} = \\frac{7.6}{0.64} \\cdot (0.8)^{20} = 11.875 \\cdot (0.8)^{20} \\approx 0.1369094285$$\n\nSumming these probabilities gives the $p$-value:\n$$p\\text{-value} \\approx 0.011529215 + 0.057646075 + 0.136909428 = 0.206084718$$\nRounding the result to four significant figures, we get $0.2061$.\n\n### Part 2: Unreliability of the Normal Approximation\n\nThe Central Limit Theorem (CLT) provides the foundation for approximating the distribution of a sum or average of independent and identically distributed random variables with a normal distribution, provided the sample size is sufficiently large. A binomial random variable $X \\sim \\text{Binomial}(n, p)$ can be expressed as the sum of $n$ independent Bernoulli($p$) random variables. Thus, for large $n$, its distribution can be approximated by a normal distribution with mean $\\mu = np$ and variance $\\sigma^2 = np(1-p)$.\n\nHowever, this approximation is unreliable in the current setting for the following fundamental reasons grounded in the properties of the sampling distribution under $H_0$.\n\n1.  **Skewness of the Sampling Distribution**: The binomial distribution is symmetric only when $p=0.5$. For $p \\neq 0.5$, the distribution is skewed. The skewness is exacerbated when $p$ is close to $0$ or $1$, and when $n$ is small. Under our null hypothesis, $p=0.2$. This value is far from $0.5$, leading to a sampling distribution that is markedly right-skewed (i.e., it has a long tail to the right). The normal distribution, by contrast, is perfectly symmetric. Approximating a skewed distribution with a symmetric one is inherently inaccurate, particularly for calculating tail probabilities like a $p$-value. The error in approximation is greatest in the tails, which is precisely where the $p$-value calculation occurs.\n\n2.  **Insufficient Effective Sample Size**: The reliability of the normal approximation depends not just on $n$, but on the expected number of successes, $np$, and failures, $n(1-p)$. These quantities determine how much of the probability mass is concentrated away from the boundaries of the sample space ($0$ and $n$). Under $H_0$, the expected number of successes is:\n    $$\\mu = np = 20 \\times 0.2 = 4$$\n    The expected number of failures is:\n    $$n(1-p) = 20 \\times 0.8 = 16$$\n    With an expected count of only $4$ successes, the distribution is compressed against the lower boundary of $0$. A significant portion of the probability mass is concentrated on a few small integer values ($0, 1, 2, 3, 4, ...$). The shape of the probability histogram is not bell-shaped; instead, it rises sharply from $0$ to a peak around $4$ and then tails off. This is far from the symmetric, bell-like shape of a normal density curve. Heuristic rules, such as requiring $np \\ge 5$ and $n(1-p) \\ge 5$ (or more stringently, $\\ge 10$), are practical formalizations of this principle. In this case, the condition $np \\ge 5$ is violated.\n\n3.  **Discreteness of the Distribution**: The binomial distribution is a discrete probability distribution, defined only for integer values. The normal distribution is continuous. When $n$ is large and $p$ is not extreme, the probability histogram of the binomial distribution has many bars, and their tops can be well-approximated by a smooth, continuous curve. However, for $n=20$, the distribution is still highly discrete. The probability is concentrated in \"lumps\" at integer values. A continuous approximation, even with a continuity correction (e.g., approximating $P(X \\le 2)$ with the area under the normal curve up to $x=2.5$), cannot fully account for the \"blocky\" nature of a discrete distribution when the expected count is low, as the shape itself is not close to normal.\n\nIn summary, the use of the normal approximation is invalid because the sampling distribution under $H_0$ (Binomial($20, 0.2$)) is too skewed and too discrete, a consequence of the expected number of successes ($np=4$) being too small. The conditions for the CLT to provide a good approximation are not met.", "answer": "$$\\boxed{0.2061}$$", "id": "4934180"}, {"introduction": "Moving from analysis to design, a crucial skill in biostatistics is determining the required resources for a conclusive study. This practice problem walks you through the process of calculating the minimum sample size needed to detect a scientifically meaningful effect with a desired level of confidence, or power. Mastering this one-sample calculation is fundamental for planning efficient and ethical research, ensuring that a study has a high probability of achieving its goals [@problem_id:4934175].", "problem": "A public health laboratory is designing a surveillance study to estimate the proportion $p$ of patients who achieve seroconversion after a new vaccine schedule. Prior studies suggest a baseline seroconversion proportion of $p_{0}=0.30$ under the existing schedule. The investigators plan a one-sample large-sample test for a single proportion, formulated as the null hypothesis $H_{0}: p = p_{0}$ against the two-sided alternative $H_{1}: p \\ne p_{0}$, using the normal approximation to the binomial distribution and the one-sample $z$-statistic constructed with the null standard error. They will control the Type I error at level $\\alpha = 0.05$ (two-sided). The design target is to have statistical power $0.80$ to detect a true seroconversion proportion of $p = 0.35$.\n\nStarting from the binomial model and the normal approximation justified by the Central Limit Theorem (CLT), derive the rejection region in terms of the sample proportion $\\hat{p}$ under $H_{0}$. Then, under the fixed alternative $p = 0.35$, express the power in terms of the cumulative distribution function of the standard normal distribution, and use this to determine the smallest sample size $n$ for which the power is at least $0.80$ at $\\alpha = 0.05$ (two-sided), using the one-sample $z$-test with the null standard error and the normal approximation. Report the minimal integer value of $n$.", "solution": "The problem requires the derivation of a sample size for a one-sample test for a proportion. We begin by formalizing the statistical framework based on the provided information.\n\nLet $p$ be the true proportion of patients who achieve seroconversion. The null hypothesis is $H_{0}: p = p_{0}$, where $p_{0}=0.30$. The alternative hypothesis is two-sided, $H_{1}: p \\ne p_{0}$. The test will be conducted at a Type I error level of $\\alpha = 0.05$. We want to find the minimum sample size $n$ required to achieve a statistical power of at least $0.80$ to detect a specific alternative proportion $p_{A} = 0.35$.\n\nLet $X$ be the number of patients who seroconvert in a sample of size $n$. Under the binomial model, $X \\sim \\text{Binomial}(n, p)$. The sample proportion is the estimator for $p$, given by $\\hat{p} = \\frac{X}{n}$. For a sufficiently large sample size $n$, the Central Limit Theorem (CLT) justifies approximating the sampling distribution of $\\hat{p}$ with a normal distribution.\n\nUnder the null hypothesis $H_{0}$, the true proportion is $p_{0} = 0.30$. The distribution of the sample proportion is approximately normal:\n$$ \\hat{p} \\mid H_{0} \\sim N\\left(p_{0}, \\frac{p_{0}(1-p_{0})}{n}\\right) $$\nThe problem specifies using the one-sample $z$-statistic constructed with the null standard error, which is $\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}$. The test statistic is:\n$$ Z = \\frac{\\hat{p} - p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}} $$\nUnder $H_{0}$, $Z$ follows a standard normal distribution, $Z \\sim N(0,1)$.\n\nFirst, we derive the rejection region in terms of $\\hat{p}$. For a two-sided test with significance level $\\alpha = 0.05$, we reject $H_{0}$ if the absolute value of the test statistic exceeds the critical value $z_{\\alpha/2} = z_{0.025}$. From standard normal tables, $z_{0.025} \\approx 1.96$. The rejection condition is $|Z|  z_{\\alpha/2}$, which expands to:\n$$ \\frac{\\hat{p} - p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}  z_{\\alpha/2} \\quad \\text{or} \\quad \\frac{\\hat{p} - p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}  -z_{\\alpha/2} $$\nSolving for $\\hat{p}$, we obtain the rejection region:\n$$ \\hat{p}  p_{0} + z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}} \\quad \\text{or} \\quad \\hat{p}  p_{0} - z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}} $$\n\nNext, we express the power of the test. Power is the probability of correctly rejecting $H_{0}$ when the alternative hypothesis is true. We consider the specific alternative where the true proportion is $p = p_{A} = 0.35$. Under this alternative, the sampling distribution of $\\hat{p}$ is approximately:\n$$ \\hat{p} \\mid H_{A} \\sim N\\left(p_{A}, \\frac{p_{A}(1-p_{A})}{n}\\right) $$\nThe power is the probability that $\\hat{p}$ falls into the rejection region, calculated under the assumption that $p=p_{A}$:\n$$ \\text{Power} = P\\left(\\hat{p}  p_{0} + z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}} \\mid p=p_{A}\\right) + P\\left(\\hat{p}  p_{0} - z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}} \\mid p=p_{A}\\right) $$\nTo express this in terms of the standard normal cumulative distribution function (CDF), $\\Phi(z)$, we standardize $\\hat{p}$ using its distribution under $H_{A}$:\n$$ Z' = \\frac{\\hat{p} - p_{A}}{\\sqrt{\\frac{p_{A}(1-p_{A})}{n}}} \\sim N(0,1) $$\nThe power expression becomes:\n$$ \\text{Power} = P\\left(Z'  \\frac{p_{0} - p_{A} + z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}{\\sqrt{\\frac{p_{A}(1-p_{A})}{n}}}\\right) + P\\left(Z'  \\frac{p_{0} - p_{A} - z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}{\\sqrt{\\frac{p_{A}(1-p_A)}{n}}}\\right) $$\nLet $C_1 = \\frac{p_{0} - p_{A} + z_{\\alpha/2}\\sqrt{p_{0}(1-p_{0})/n}}{\\sqrt{p_{A}(1-p_A)/n}}$ and $C_2 = \\frac{p_{0} - p_{A} - z_{\\alpha/2}\\sqrt{p_{0}(1-p_{0})/n}}{\\sqrt{p_{A}(1-p_A)/n}}$. The power is $P(Z'  C_1) + P(Z'  C_2) = (1 - \\Phi(C_1)) + \\Phi(C_2)$.\n\nWe are given $p_{A} = 0.35  p_{0} = 0.30$. The second term, $\\Phi(C_2)$, corresponds to rejecting $H_0$ by observing an unexpectedly low value of $\\hat{p}$. The argument $C_2$ will be a large negative number for any reasonable sample size, making $\\Phi(C_2)$ negligible. Thus, the power is dominated by the first term. We require the power to be at least $1-\\beta = 0.80$, where $\\beta = 0.20$ is the Type II error rate.\n$$ 1 - \\Phi\\left( \\frac{p_{0} - p_{A} + z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}{\\sqrt{\\frac{p_{A}(1-p_{A})}{n}}} \\right) \\ge 1-\\beta $$\nThis implies:\n$$ \\Phi\\left( \\frac{p_{0} - p_{A} + z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}{\\sqrt{\\frac{p_{A}(1-p_{A})}{n}}} \\right) \\le \\beta $$\nThe argument of $\\Phi$ must be less than or equal to the $\\beta$-quantile of the standard normal distribution, which is $z_{\\beta}$ (or $-z_{1-\\beta}$). For a one-tailed power calculation, as is standard in this scenario, we use $z_{1-\\beta}$ (corresponding to the upper tail of the correctly rejected region), so the argument should be less than or equal to $-z_{1-\\beta}$. Since we defined $\\beta=0.20$, we use $z_{0.80}$ or, equivalently, what is commonly written as $z_\\beta$ where it's understood to be the one-tailed critical value for power $1-\\beta$. So we use $z_{0.20}$ (upper tail cutoff for $p=0.20$).\nThus, we require:\n$$ \\frac{p_{0} - p_{A} + z_{\\alpha/2}\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}{\\sqrt{\\frac{p_{A}(1-p_{A})}{n}}} \\le -z_{\\beta} $$\nwhere $z_{\\beta}$ is the value such that $P(Z  z_\\beta) = \\beta$, i.e., the $(1-\\beta)$-quantile.\nMultiplying by the denominator and rearranging terms to solve for $\\sqrt{n}$:\n$$ p_{0} - p_{A} + \\frac{z_{\\alpha/2}\\sqrt{p_{0}(1-p_{0})}}{\\sqrt{n}} \\le -\\frac{z_{\\beta}\\sqrt{p_{A}(1-p_{A})}}{\\sqrt{n}} $$\n$$ p_{A} - p_{0} \\ge \\frac{z_{\\alpha/2}\\sqrt{p_{0}(1-p_{0})}}{\\sqrt{n}} + \\frac{z_{\\beta}\\sqrt{p_{A}(1-p_{A})}}{\\sqrt{n}} $$\n$$ \\sqrt{n} \\ge \\frac{z_{\\alpha/2}\\sqrt{p_{0}(1-p_{0})} + z_{\\beta}\\sqrt{p_{A}(1-p_{A})}}{p_{A} - p_{0}} $$\nSquaring both sides gives the formula for the minimum sample size $n$:\n$$ n \\ge \\left( \\frac{z_{\\alpha/2}\\sqrt{p_{0}(1-p_{0})} + z_{\\beta}\\sqrt{p_{A}(1-p_{A})}}{p_{A} - p_{0}} \\right)^2 $$\nNow we substitute the provided values:\n$p_{0} = 0.30$\n$p_{A} = 0.35$\n$\\alpha = 0.05 \\implies z_{\\alpha/2} = z_{0.025} \\approx 1.95996$\nPower $= 0.80 \\implies \\beta = 0.20 \\implies z_{\\beta} = z_{0.20} \\approx 0.84162$\n\nThe necessary components for the formula are:\n$\\sqrt{p_{0}(1-p_{0})} = \\sqrt{0.30 \\times 0.70} = \\sqrt{0.21}$\n$\\sqrt{p_{A}(1-p_{A})} = \\sqrt{0.35 \\times 0.65} = \\sqrt{0.2275}$\n$p_{A} - p_{0} = 0.35 - 0.30 = 0.05$\n\nSubstituting these into the inequality for $n$:\n$$ n \\ge \\left( \\frac{1.95996 \\sqrt{0.21} + 0.84162 \\sqrt{0.2275}}{0.05} \\right)^2 $$\n$$ n \\ge \\left( \\frac{1.95996 \\times 0.458258 + 0.84162 \\times 0.476970}{0.05} \\right)^2 $$\n$$ n \\ge \\left( \\frac{0.898168 + 0.401452}{0.05} \\right)^2 $$\n$$ n \\ge \\left( \\frac{1.29962}{0.05} \\right)^2 $$\n$$ n \\ge (25.9924)^2 $$\n$$ n \\ge 675.605 $$\nSince the sample size $n$ must be an integer, we must round up to the next whole number to ensure the power is at least $0.80$. Therefore, the smallest required sample size is $n=676$.\nFor this sample size, the normal approximation is well justified as $n p_0 = 676 \\times 0.30 = 202.8  10$ and $n p_A = 676 \\times 0.35 = 236.6  10$.", "answer": "$$\\boxed{676}$$", "id": "4934175"}, {"introduction": "We now extend the principles of study design to one of the most common scenarios in clinical and public health research: comparing two independent groups. This exercise builds on the one-sample logic to determine the sample size needed to power a two-sample test for proportions, such as in a randomized controlled trial. This calculation is a cornerstone of evidence-based medicine, allowing researchers to design studies capable of detecting differences between a new intervention and a standard of care [@problem_id:4934162].", "problem": "A multisite biostatistics team plans a randomized study comparing two independent proportions with equal allocation. Let $p_1$ denote the true event probability in the intervention group and $p_2$ the true event probability in the control group. They wish to be able to detect a difference between $p_1 = 0.60$ and $p_2 = 0.50$ using a two-sample pooled Wald $z$-test for proportions under a two-sided Type I error rate $\\alpha = 0.05$ and target power $1-\\beta = 0.90$. Assume independent Bernoulli outcomes, equal group sizes, and the usual large-sample normal approximation justified by the Central Limit Theorem (CLT). For design-stage planning, take the pooled null proportion to be the average $\\bar{p} = (p_1 + p_2)/2$. Ignore continuity corrections.\n\nStarting from first principles of sampling distributions and hypothesis testing, derive the sample size criterion implied by the pooled Wald test and determine the minimal equal group size $n$ (the same in each group) that achieves at least the target power when the true proportions are $p_1 = 0.60$ and $p_2 = 0.50$. Report the minimal integer $n$ per group. Do not include any units in your final answer.", "solution": "The problem requires the derivation of the sample size $n$ per group for a study comparing two independent proportions, designed to achieve a specified power for a given effect size. We will start from the first principles of hypothesis testing.\n\nLet $p_1$ and $p_2$ be the true event probabilities in the intervention and control groups, respectively. The corresponding sample proportions, estimated from independent samples of equal size $n$, are $\\hat{p}_1$ and $\\hat{p}_2$. The objective is to test the following two-sided hypotheses:\n$$H_0: p_1 = p_2$$\n$$H_A: p_1 \\ne p_2$$\n\nThe test to be used is the pooled Wald $z$-test. Under the null hypothesis that $p_1=p_2$, the best estimate of the common proportion is the pooled sample proportion, $\\hat{p}_{pool}$:\n$$\\hat{p}_{pool} = \\frac{n\\hat{p}_1 + n\\hat{p}_2}{n+n} = \\frac{\\hat{p}_1 + \\hat{p}_2}{2}$$\nThe standard error of the difference in proportions, under $H_0$, is estimated using $\\hat{p}_{pool}$:\n$$SE_0(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{n} + \\frac{1}{n}\\right)} = \\sqrt{\\frac{2\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n}}$$\nThe test statistic $Z$ is then:\n$$Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE_0(\\hat{p}_1 - \\hat{p}_2)} = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\frac{2\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n}}}$$\nFor a large sample size $n$, this statistic follows a standard normal distribution, $N(0,1)$, under $H_0$.\n\nFor a two-sided Type I error rate of $\\alpha = 0.05$, $H_0$ is rejected if $|Z|  z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-th quantile of the standard normal distribution. For $\\alpha = 0.05$, this critical value is $z_{0.975}$.\n\nPower, $1-\\beta = 0.90$, is the probability of correctly rejecting $H_0$ when a specific alternative $H_A$ is true. Here, the specific alternative is $p_1 = 0.60$ and $p_2 = 0.50$. Since $p_1  p_2$, the difference is positive, and the power is almost entirely concentrated in the upper tail of the rejection region. We can therefore approximate the power by:\n$$1-\\beta \\approx P(Z  z_{1-\\alpha/2} | p_1=0.60, p_2=0.50)$$\nSubstituting the expression for the test statistic, the condition for achieving the target power is:\n$$P\\left( \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\frac{2\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n}}}  z_{1-\\alpha/2} \\Bigg| p_1, p_2 \\right) \\approx 1-\\beta$$\nRearranging the inequality, we get:\n$$P\\left( \\hat{p}_1 - \\hat{p}_2  z_{1-\\alpha/2} \\sqrt{\\frac{2\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n}} \\Bigg| p_1, p_2 \\right) \\approx 1-\\beta$$\nFor design purposes, the sample-dependent $\\hat{p}_{pool}$ in the standard error term must be replaced by a fixed value. The problem specifies using the average of the alternative proportions, $\\bar{p} = (p_1+p_2)/2 = (0.60+0.50)/2 = 0.55$. The rejection boundary for planning is thus $z_{1-\\alpha/2} \\sqrt{2\\bar{p}(1-\\bar{p})/n}$.\n\nTo evaluate this probability, we must standardize the random variable $\\hat{p}_1 - \\hat{p}_2$ using its distribution under the alternative hypothesis. Under $H_A$, by the Central Limit Theorem:\n$$E[\\hat{p}_1 - \\hat{p}_2] = p_1 - p_2$$\n$$Var(\\hat{p}_1 - \\hat{p}_2) = Var(\\hat{p}_1) + Var(\\hat{p}_2) = \\frac{p_1(1-p_1)}{n} + \\frac{p_2(1-p_2)}{n}$$\nThe standard error of the difference under $H_A$ is $SE_A = \\sqrt{\\frac{p_1(1-p_1) + p_2(1-p_2)}{n}}$.\n\nStandardizing the power inequality by subtracting the mean under $H_A$ and dividing by the standard error under $H_A$:\n$$P\\left( \\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{SE_A}  \\frac{z_{1-\\alpha/2} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} - (p_1 - p_2)}{SE_A} \\right) \\approx 1-\\beta$$\nThe left side of the inequality is a standard normal variable. The probability that a standard normal variable exceeds a certain value $c$ is $1-\\beta$, which implies that $c = z_\\beta = -z_{1-\\beta}$. Therefore, we set the right-hand side of the inequality to $-z_{1-\\beta}$:\n$$-z_{1-\\beta} = \\frac{z_{1-\\alpha/2} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}} - (p_1 - p_2)}{\\sqrt{\\frac{p_1(1-p_1) + p_2(1-p_2)}{n}}}$$\nWe can now solve for the sample size $n$:\n$$-(p_1 - p_2) = -z_{1-\\beta} \\sqrt{\\frac{p_1(1-p_1) + p_2(1-p_2)}{n}} - z_{1-\\alpha/2} \\sqrt{\\frac{2\\bar{p}(1-\\bar{p})}{n}}$$\n$$p_1 - p_2 = \\frac{1}{\\sqrt{n}} \\left( z_{1-\\alpha/2}\\sqrt{2\\bar{p}(1-\\bar{p})} + z_{1-\\beta}\\sqrt{p_1(1-p_1) + p_2(1-p_2)} \\right)$$\nIsolating $\\sqrt{n}$ and squaring both sides gives the general formula for the sample size per group:\n$$n = \\left( \\frac{z_{1-\\alpha/2}\\sqrt{2\\bar{p}(1-\\bar{p})} + z_{1-\\beta}\\sqrt{p_1(1-p_1) + p_2(1-p_2)}}{p_1 - p_2} \\right)^2$$\nWe now substitute the given numerical values:\n$p_1 = 0.60$\n$p_2 = 0.50$\n$\\alpha = 0.05 \\implies z_{1-\\alpha/2} = z_{0.975} \\approx 1.95996$\n$1-\\beta = 0.90 \\implies \\beta = 0.10 \\implies z_{1-\\beta} = z_{0.90} \\approx 1.28155$\n$\\bar{p} = (0.60 + 0.50) / 2 = 0.55$\nThe difference in proportions is $p_1 - p_2 = 0.10$.\n\nWe calculate the variance components:\nVariance term for the null hypothesis: $2\\bar{p}(1-\\bar{p}) = 2(0.55)(1 - 0.55) = 2(0.55)(0.45) = 0.495$.\nVariance term for the alternative hypothesis: $p_1(1-p_1) + p_2(1-p_2) = 0.60(1-0.60) + 0.50(1-0.50) = 0.24 + 0.25 = 0.49$.\n\nSubstituting these into the sample size formula:\n$$n = \\left( \\frac{1.95996 \\sqrt{0.495} + 1.28155 \\sqrt{0.49}}{0.10} \\right)^2$$\n$$n \\approx \\left( \\frac{1.95996 \\times 0.703562 + 1.28155 \\times 0.7}{0.10} \\right)^2$$\n$$n \\approx \\left( \\frac{1.378953 + 0.897085}{0.10} \\right)^2$$\n$$n \\approx \\left( \\frac{2.276038}{0.10} \\right)^2$$\n$$n \\approx (22.76038)^2 \\approx 518.035$$\nSince the sample size must be an integer and we must achieve at least the target power of $0.90$, we must take the ceiling of this value.\n$$n_{min} = \\lceil 518.035 \\rceil = 519$$\nThus, the minimal integer sample size required in each group is $519$.", "answer": "$$\\boxed{519}$$", "id": "4934162"}]}