{"hands_on_practices": [{"introduction": "Estimating the variance of measurement error is a fundamental task in experimental science, from calibrating new instruments to assessing the reliability of a lab assay. This first practice grounds our theoretical knowledge in a common scientific scenario: analyzing the results of a linear regression model. By working through this problem [@problem_id:4903186], you will apply the chi-squared distribution to construct a confidence interval for the error variance, $\\sigma^2$, directly from the residual sum of squares, a standard output in regression analysis.", "problem": "A biomedical engineering team is calibrating a new sensor that estimates a continuous biomarker from measured covariates using a linear model. They fit a normal linear regression with $p=5$ unknown coefficients (including the intercept) to $n=40$ independent observations, where the error term represents the sensor’s measurement noise. The residual sum of squares is reported as $\\text{RSS}=78$ in the squared units of the biomarker. Assume the classical normal linear model with independent, identically distributed Gaussian errors of mean $0$ and variance $\\sigma^{2}$.\n\nStarting from fundamental distributional properties of the residual sum of squares under the normal linear model, construct a two-sided $95\\%$ confidence interval for the error variance $\\sigma^{2}$, and then compute its numerical endpoints using appropriate quantiles. Finally, interpret this interval as describing plausible values of the sensor’s measurement error variance for the underlying data-generating process.\n\nRound your interval endpoints to three significant figures and express them in the squared units of the biomarker response variable.", "solution": "The problem is well-posed and scientifically sound, resting on the fundamental principles of statistical inference for the normal linear model. We may proceed with a solution.\n\nThe problem asks for a two-sided $95\\%$ confidence interval for the error variance, $\\sigma^2$, of a normal linear regression model. The model is fitted to $n=40$ independent observations with $p=5$ unknown coefficients. The residual sum of squares is given as $\\text{RSS}=78$.\n\nThe theoretical foundation for this problem is a key result in linear model theory. For a classical normal linear model, $Y = X\\beta + \\epsilon$, where the errors $\\epsilon_i$ are independent and identically distributed as $N(0, \\sigma^2)$, the quantity $\\frac{\\text{RSS}}{\\sigma^2}$ follows a chi-squared distribution. The degrees of freedom for this distribution are $df = n - p$, where $n$ is the number of observations and $p$ is the number of estimated regression coefficients (including the intercept).\n\nFirst, we identify the given parameters:\n- Number of observations, $n = 40$.\n- Number of regression coefficients, $p = 5$.\n- Residual Sum of Squares, $\\text{RSS} = 78$.\n- The confidence level is $95\\%$, which corresponds to a significance level $\\alpha = 1 - 0.95 = 0.05$.\n\nThe degrees of freedom for the chi-squared distribution are calculated as:\n$$ df = n - p = 40 - 5 = 35 $$\nThus, the pivotal quantity $\\frac{\\text{RSS}}{\\sigma^2}$ follows a chi-squared distribution with $35$ degrees of freedom:\n$$ \\frac{\\text{RSS}}{\\sigma^2} \\sim \\chi^2_{35} $$\nTo construct a two-sided $100(1-\\alpha)\\%$ confidence interval for $\\sigma^2$, we find two quantiles from the $\\chi^2_{35}$ distribution that cut off an area of $\\alpha/2$ in each tail. Let $\\chi^2_{\\alpha/2, df}$ be the lower-tail critical value (the quantile leaving $\\alpha/2$ area to its left) and $\\chi^2_{1-\\alpha/2, df}$ be the upper-tail critical value. This establishes the probability statement:\n$$ P\\left( \\chi^2_{\\alpha/2, df}  \\frac{\\text{RSS}}{\\sigma^2}  \\chi^2_{1-\\alpha/2, df} \\right) = 1 - \\alpha $$\nWith $\\alpha = 0.05$, we need the quantiles for $\\alpha/2 = 0.025$ and $1 - \\alpha/2 = 0.975$.\n- The lower critical value is $\\chi^2_{0.025, 35}$.\n- The upper critical value is $\\chi^2_{0.975, 35}$.\n\nUsing a statistical table or software for the $\\chi^2$ distribution with $df=35$:\n- The lower critical value is $\\chi^2_{0.025, 35} \\approx 20.569$.\n- The upper critical value is $\\chi^2_{0.975, 35} \\approx 53.203$.\n\nNow, we rearrange the inequalities in the probability statement to isolate $\\sigma^2$. This yields the general formula for the $95\\%$ confidence interval for $\\sigma^2$:\n$$ \\left( \\frac{\\text{RSS}}{\\chi^2_{1-\\alpha/2, df}}, \\frac{\\text{RSS}}{\\chi^2_{\\alpha/2, df}} \\right) $$\nSubstituting the numerical values:\n$$ \\left( \\frac{78}{53.203}, \\frac{78}{20.569} \\right) $$\n- Lower bound: $\\frac{78}{53.203} \\approx 1.46608$\n- Upper bound: $\\frac{78}{20.569} \\approx 3.79211$\n\nThe problem requires rounding the interval endpoints to three significant figures.\n- Lower bound rounded to three significant figures is $1.47$.\n- Upper bound rounded to three significant figures is $3.79$.\n\nThe resulting $95\\%$ confidence interval for the error variance $\\sigma^2$ is $(1.47, 3.79)$, with units being the squared units of the biomarker.\n\nFinally, we interpret this interval. We are $95\\%$ confident that the true, unknown variance of the sensor's measurement error, $\\sigma^2$, lies between $1.47$ and $3.79$ squared units of the biomarker. This interval provides a range of plausible values for the population error variance that are consistent with the observed data from the experiment. It quantifies the uncertainty in our estimate of the sensor's precision.", "answer": "$$\n\\boxed{(1.47, 3.79)}\n$$", "id": "4903186"}, {"introduction": "A reliable statistical procedure should become more precise as we gather more data. This theoretical exercise [@problem_id:4903162] invites you to explore this fundamental property of consistency for the variance confidence interval. By analyzing the asymptotic behavior of the chi-squared quantiles, you will mathematically demonstrate that the interval's relative width shrinks as the sample size $n$ grows, reinforcing why large samples lead to more definitive conclusions.", "problem": "A statistical investigator is quantifying the variability of a biomarker measured across $n$ independent patients, where each patient's measurement is modeled as an independent and identically distributed draw from a normal distribution with unknown variance $\\sigma^{2}$. Let the sample variance be $S^{2}$. The ratio of the upper endpoint to the lower endpoint of the two-sided confidence interval (CI) for $\\sigma^2$ at level $1-\\alpha$ is given by\n$$R_{\\alpha,n}=\\frac{\\chi^{2}_{1-\\alpha/2,n-1}}{\\chi^{2}_{\\alpha/2,n-1}},$$\nwhere $\\chi^{2}_{p,\\nu}$ denotes the $p$-quantile (with area $p$ to the left) of the chi-square distribution with $\\nu$ degrees of freedom. Using the asymptotic normality of the chi-square distribution, determine the single limiting value\n$$L=\\lim_{n\\to\\infty}R_{\\alpha,n}$$\nfor any fixed $\\alpha\\in\\{0.10,0.05,0.01\\}$. Provide the value of $L$ as your final answer. No rounding is required, and no units are needed. Also, briefly justify how this limiting value relates to the reduction in CI width as $n$ grows.", "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. It is based on established principles of statistical inference, specifically the construction of confidence intervals for a population variance and the asymptotic theory of distributions. The question asks for the derivation of a limit based on these principles, which is a valid and non-trivial mathematical exercise. I will proceed with the solution.\n\nThe objective is to determine the limit $L = \\lim_{n\\to\\infty}R_{\\alpha,n}$, where the ratio $R_{\\alpha,n}$ is defined as\n$$R_{\\alpha,n}=\\frac{\\chi^{2}_{1-\\alpha/2,n-1}}{\\chi^{2}_{\\alpha/2,n-1}}$$\nHere, $\\chi^{2}_{p,\\nu}$ denotes the standard $p$-quantile (lower-tail probability $p$) of the chi-square distribution with $\\nu$ degrees of freedom. The calculation of this limit depends on the asymptotic behavior of the chi-square distribution as its degrees of freedom, $\\nu = n-1$, tend to infinity.\n\nAccording to the Central Limit Theorem, the chi-square distribution with $\\nu$ degrees of freedom, which can be viewed as the sum of $\\nu$ independent squared standard normal variables, converges to a normal distribution for large $\\nu$. Let $X_\\nu$ be a random variable following a $\\chi^{2}_\\nu$ distribution. The mean of $X_\\nu$ is $E[X_\\nu] = \\nu$ and its variance is $\\text{Var}(X_\\nu) = 2\\nu$. The standardized variable thus converges in distribution to a standard normal variable $Z \\sim N(0,1)$:\n$$ \\frac{X_\\nu - \\nu}{\\sqrt{2\\nu}} \\xrightarrow{d} Z \\quad \\text{as } \\nu \\to \\infty $$\nThis convergence in distribution implies the convergence of quantiles. Let $z_p$ be the $p$-quantile of the standard normal distribution. For large $\\nu$, we can form the approximation:\n$$ \\frac{\\chi^{2}_{p,\\nu} - \\nu}{\\sqrt{2\\nu}} \\approx z_p $$\nRearranging this expression gives us an asymptotic formula for the chi-square quantile:\n$$ \\chi^{2}_{p,\\nu} \\approx \\nu + z_p\\sqrt{2\\nu} $$\nWe apply this approximation to the numerator and denominator of $R_{\\alpha,n}$. Let $\\nu = n-1$.\n\nFor the numerator, the quantile is $p = 1-\\alpha/2$. Thus, we have:\n$$ \\chi^{2}_{1-\\alpha/2, n-1} \\approx (n-1) + z_{1-\\alpha/2}\\sqrt{2(n-1)} $$\nFor the denominator, the quantile is $p = \\alpha/2$. Thus, we have:\n$$ \\chi^{2}_{\\alpha/2, n-1} \\approx (n-1) + z_{\\alpha/2}\\sqrt{2(n-1)} $$\nNow, we can substitute these asymptotic expressions into the definition of $R_{\\alpha,n}$:\n$$ R_{\\alpha,n} \\approx \\frac{(n-1) + z_{1-\\alpha/2}\\sqrt{2(n-1)}}{(n-1) + z_{\\alpha/2}\\sqrt{2(n-1)}} $$\nTo find the limit as $n \\to \\infty$, which is equivalent to $\\nu = n-1 \\to \\infty$, we divide both the numerator and the denominator by $(n-1)$:\n$$ R_{\\alpha,n} \\approx \\frac{1 + z_{1-\\alpha/2}\\frac{\\sqrt{2(n-1)}}{n-1}}{1 + z_{\\alpha/2}\\frac{\\sqrt{2(n-1)}}{n-1}} = \\frac{1 + z_{1-\\alpha/2}\\sqrt{\\frac{2}{n-1}}}{1 + z_{\\alpha/2}\\sqrt{\\frac{2}{n-1}}} $$\nWe now take the limit as $n \\to \\infty$:\n$$ L = \\lim_{n\\to\\infty} R_{\\alpha,n} = \\lim_{n\\to\\infty} \\frac{1 + z_{1-\\alpha/2}\\sqrt{\\frac{2}{n-1}}}{1 + z_{\\alpha/2}\\sqrt{\\frac{2}{n-1}}} $$\nFor any fixed $\\alpha \\in (0,1)$, $z_{1-\\alpha/2}$ and $z_{\\alpha/2}$ are finite constants. As $n \\to \\infty$, the term $n-1 \\to \\infty$, and thus $\\sqrt{\\frac{2}{n-1}} \\to 0$. Therefore, the limit evaluates to:\n$$ L = \\frac{1 + z_{1-\\alpha/2} \\cdot 0}{1 + z_{\\alpha/2} \\cdot 0} = \\frac{1}{1} = 1 $$\nThis limiting value $L=1$ is independent of the specific value of $\\alpha$, as long as it is a fixed value in the set $\\{0.10, 0.05, 0.01\\}$ or any other value in $(0,1)$.\n\nJustification of the relationship to CI width:\nThe standard $1-\\alpha$ confidence interval for the variance $\\sigma^2$ is $\\left[L_{CI}, U_{CI}\\right] = \\left[ \\frac{(n-1)S^2}{\\chi^{2}_{1-\\alpha/2, n-1}}, \\frac{(n-1)S^2}{\\chi^{2}_{\\alpha/2, n-1}} \\right]$.\nThe ratio of the upper endpoint to the lower endpoint is exactly the quantity $R_{\\alpha,n}$:\n$$ \\frac{U_{CI}}{L_{CI}} = \\frac{(n-1)S^2 / \\chi^{2}_{\\alpha/2, n-1}}{(n-1)S^2 / \\chi^{2}_{1-\\alpha/2, n-1}} = \\frac{\\chi^{2}_{1-\\alpha/2, n-1}}{\\chi^{2}_{\\alpha/2, n-1}} = R_{\\alpha,n} $$\nOur result shows that $\\lim_{n\\to\\infty} \\frac{U_{CI}}{L_{CI}} = 1$. A ratio of $1$ implies that as $n \\to \\infty$, the upper endpoint $U_{CI}$ and the lower endpoint $L_{CI}$ converge to the same value. This means the width of the confidence interval, $W_n = U_{CI} - L_{CI}$, must converge to $0$. This is a manifestation of statistical consistency: as the sample size grows, the uncertainty in the estimate of $\\sigma^2$ diminishes, and the confidence interval collapses to a single point, which is the true population variance $\\sigma^2$. The limit $L=1$ is the formal mathematical underpinning of this desirable property.", "answer": "$$\\boxed{1}$$", "id": "4903162"}, {"introduction": "The classical confidence interval for variance rests on the crucial assumption that all data points are independent. However, in many scientific studies, data are naturally clustered—for instance, measurements from patients within the same clinic or observations from members of the same family. This hands-on coding simulation [@problem_id:4903180] challenges you to quantify the failure of standard methods when this independence assumption is violated and to implement a modern, design-based solution using the cluster bootstrap to obtain a reliable interval.", "problem": "You are to write a complete and runnable program that uses Monte Carlo simulation to compare the empirical coverage of two different confidence interval constructions for a population variance under clustered (intracluster-correlated) sampling. The two methods are: (i) an independence-based confidence interval obtained by inverting the known sampling distribution of the sample variance under independent and identically distributed normal data, and (ii) a design-based confidence interval constructed via cluster bootstrap resampling. Your task is to quantify the undercoverage risk when the intracluster correlation is substantial.\n\nStart from the following fundamental base:\n- Under independent and identically distributed normal sampling, the sampling distribution of the sample variance admits an exact form that enables interval inversion.\n- Under multistage or clustered sampling, Primary Sampling Units (PSUs) can be used for design-based resampling that preserves the dependence structure within clusters.\n- In a two-level Gaussian random effects model, intracluster correlation can be generated by a shared cluster-level effect added to independent within-cluster errors.\n\nThe data generating mechanism for each dataset is as follows. For given numbers of clusters $C$ and cluster size $m$, generate observations $\\{Y_{ij} : i = 1,\\dots,C,\\ j = 1,\\dots,m\\}$ from\n$$\nY_{ij} \\;=\\; \\mu \\;+\\; U_i \\;+\\; \\varepsilon_{ij},\n$$\nwhere $U_i \\sim \\mathcal{N}(0,\\sigma_b^2)$, $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma_w^2)$, all mutually independent, with the total marginal variance $\\sigma^2 = \\sigma_b^2 + \\sigma_w^2$ and the intracluster correlation $\\rho = \\sigma_b^2/\\sigma^2$. Set $\\mu = 0$ without loss of generality. For each simulated dataset, compute the usual unbiased sample variance\n$$\nS^2 \\;=\\; \\frac{1}{n-1}\\sum_{i=1}^C \\sum_{j=1}^m \\big(Y_{ij} - \\overline{Y}\\big)^2, \\quad \\text{where } n = C m \\text{ and } \\overline{Y} \\text{ is the overall sample mean.}\n$$\n\nFor each dataset, construct two two-sided confidence intervals at nominal level $1-\\alpha$ for the population variance $\\sigma^2$:\n1. Independence-based interval: invert the exact sampling distribution of $S^2$ under independent and identically distributed normal observations (treating the $n$ observations as independent).\n2. Design-based interval: perform cluster bootstrap resampling by resampling the $C$ clusters with replacement, keeping all $m$ observations within any resampled cluster intact. For each bootstrap resample, recompute $S^2$ from the pooled resampled observations. Use the percentile method from the bootstrap empirical distribution of $S^2$ to form a two-sided confidence interval.\n\nDefine the empirical coverage for a method as the proportion of simulated datasets for which the interval contains the true $\\sigma^2$. Define the undercoverage risk as the nonnegative difference between the nominal coverage and the empirical coverage:\n$$\n\\text{undercoverage} \\;=\\; \\max\\{(1-\\alpha) - \\widehat{\\text{coverage}},\\ 0\\}.\n$$\n\nUse the following test suite of parameter values, which spans both substantial intracluster correlation and a boundary condition:\n- Case A: $C = 20$, $m = 10$, $\\sigma^2 = 1.0$, $\\rho = 0.3$.\n- Case B: $C = 8$, $m = 20$, $\\sigma^2 = 1.5$, $\\rho = 0.6$.\n- Case C: $C = 50$, $m = 5$, $\\sigma^2 = 2.0$, $\\rho = 0.4$.\n- Case D (boundary, no clustering): $C = 20$, $m = 10$, $\\sigma^2 = 1.0$, $\\rho = 0.0$.\n\nImplement the following simulation settings for all cases:\n- Nominal level: $\\alpha = 0.05$.\n- Monte Carlo replicates per case: $R = 400$.\n- Bootstrap resamples per dataset for the design-based procedure: $B = 200$.\n- Random seed: set to $12345$ for reproducibility.\n\nYour program must:\n- For each case, generate $R$ datasets under the model above using the specified $C$, $m$, $\\sigma^2$, and $\\rho$.\n- For each dataset, compute the two confidence intervals, and record whether the true $\\sigma^2$ lies within each interval.\n- For each case, report four quantities in the following order: empirical coverage of the design-based interval, empirical coverage of the independence-based interval, undercoverage risk of the design-based interval, undercoverage risk of the independence-based interval. Each quantity must be represented as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of length $4$, where each element is a list of the four floats corresponding to one case, in the order A, B, C, D. For example:\n$[\\,[\\text{cov\\_design\\_A},\\text{cov\\_indep\\_A},\\text{under\\_design\\_A},\\text{under\\_indep\\_A}],\\dots]$.\n- Round each reported float to three decimal places.", "solution": "The problem as stated is valid. It is scientifically grounded in statistical theory, well-posed with all necessary parameters and definitions provided, and presented in an objective, formal manner. It poses a non-trivial comparison between two standard statistical methods under conditions designed to highlight their differing theoretical underpinnings. We may therefore proceed with a solution.\n\nThe objective is to conduct a Monte Carlo simulation to evaluate and compare the empirical coverage properties of two distinct confidence interval estimators for a population variance, $\\sigma^2$. The comparison is performed under conditions of clustered data, which introduces intracluster correlation, a feature known to violate the assumptions of classical statistical methods. The two estimators are: ($i$) a parametric interval based on the chi-squared distribution, which assumes independent and identically distributed (IID) normal data, and ($ii$) a non-parametric, design-based interval constructed using a cluster bootstrap procedure that accounts for the data's dependence structure.\n\n**Data Generation Process**\n\nFor each simulation, we generate a dataset of $n=Cm$ observations, organized into $C$ clusters of size $m$. The data are generated from a two-level Gaussian random effects model:\n$$\nY_{ij} = \\mu + U_i + \\varepsilon_{ij}\n$$\nfor cluster $i \\in \\{1, \\dots, C\\}$ and observation $j \\in \\{1, \\dots, m\\}$. The parameters are defined as:\n- $\\mu$: The overall population mean, which can be set to $0$ without loss of generality as the sample variance is location-invariant.\n- $U_i$: A cluster-specific random effect, drawn from a normal distribution $U_i \\sim \\mathcal{N}(0, \\sigma_b^2)$. These effects are independent across clusters.\n- $\\varepsilon_{ij}$: An individual-level random error, drawn from a normal distribution $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_w^2)$. These errors are mutually independent and independent of the $U_i$.\n\nThe total marginal variance of any observation $Y_{ij}$ is the target parameter $\\sigma^2$, given by the sum of the variance components: $\\sigma^2 = \\sigma_b^2 + \\sigma_w^2$.\nThe intracluster correlation coefficient (ICC), $\\rho$, measures the correlation between two distinct observations within the same cluster. It is defined as:\n$$\n\\rho = \\text{Corr}(Y_{ij}, Y_{ik}) = \\frac{\\text{Cov}(U_i+\\varepsilon_{ij}, U_i+\\varepsilon_{ik})}{\\sigma^2} = \\frac{\\text{Var}(U_i)}{\\sigma^2} = \\frac{\\sigma_b^2}{\\sigma^2} \\quad \\text{for } j \\neq k.\n$$\nGiven the simulation parameters $\\sigma^2$ and $\\rho$, the variance components are determined by $\\sigma_b^2 = \\rho \\sigma^2$ and $\\sigma_w^2 = (1-\\rho)\\sigma^2$.\n\nFor each generated dataset, the unbiased sample variance is computed as:\n$$\nS^2 = \\frac{1}{n-1} \\sum_{i=1}^C \\sum_{j=1}^m \\left(Y_{ij} - \\overline{Y}\\right)^2\n$$\nwhere $n=Cm$ is the total number of observations and $\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^C \\sum_{j=1}^m Y_{ij}$ is the overall sample mean.\n\n**Confidence Interval Methodologies**\n\nWe construct two types of two-sided confidence intervals for $\\sigma^2$ at a nominal confidence level of $1-\\alpha$.\n\n1.  **Independence-Based (Chi-Squared) Interval:**\n    This method proceeds by incorrectly assuming the $n=Cm$ observations are IID draws from a $\\mathcal{N}(\\mu, \\sigma^2)$ distribution. Under this assumption, the pivotal quantity $\\frac{(n-1)S^2}{\\sigma^2}$ follows a chi-squared distribution with $n-1$ degrees of freedom, denoted $\\chi^2_{n-1}$.\n    By inverting the probability statement $P\\left(\\chi^2_{n-1, \\alpha/2} \\le \\frac{(n-1)S^2}{\\sigma^2} \\le \\chi^2_{n-1, 1-\\alpha/2}\\right) = 1-\\alpha$, where $\\chi^2_{df, q}$ is the $q$-th quantile of the $\\chi^2_{df}$ distribution, we obtain the $(1-\\alpha)$ confidence interval for $\\sigma^2$:\n    $$\n    CI_{\\text{indep}} = \\left[ \\frac{(n-1)S^2}{\\chi^2_{n-1, 1-\\alpha/2}}, \\frac{(n-1)S^2}{\\chi^2_{n-1, \\alpha/2}} \\right]\n    $$\n    When $\\rho > 0$, the IID assumption is violated, and this interval is expected to exhibit poor coverage performance, typically undercovering the true $\\sigma^2$ because the data contain less information than $n$ independent observations would.\n\n2.  **Design-Based (Cluster Bootstrap) Interval:**\n    This method is designed to be robust to the dependence structure within clusters. It treats the clusters (Primary Sampling Units, or PSUs) as the units of observation for resampling. The procedure is as follows:\n    a.  From the original set of $C$ clusters $\\{ (Y_{11}, \\dots, Y_{1m}), \\dots, (Y_{C1}, \\dots, Y_{Cm}) \\}$, draw a sample of $C$ clusters with replacement.\n    b.  Combine the observations from these resampled clusters to form a bootstrap dataset.\n    c.  Compute the sample variance $S^2_*$ from this bootstrap dataset.\n    d.  Repeat steps a-c $B$ times to obtain a collection of bootstrap variance estimates $\\{S^2_{*b}\\}_{b=1}^B$.\n    e.  The $(1-\\alpha)$ percentile bootstrap confidence interval is formed by the empirical $\\alpha/2$ and $1-\\alpha/2$ quantiles of the bootstrap distribution:\n    $$\n    CI_{\\text{design}} = \\left[ Q_{\\alpha/2}(S^2_{*}), Q_{1-\\alpha/2}(S^2_{*}) \\right]\n    $$\n    This method is expected to provide coverage closer to the nominal level $1-\\alpha$ because the resampling scheme preserves the correlation structure of the original data generation process.\n\n**Simulation Execution and Analysis**\n\nThe simulation will be run for each of the four specified cases (A, B, C, D) using $R=400$ Monte Carlo replicates. For each replicate, both confidence intervals are constructed and tested for coverage of the true $\\sigma^2$. The empirical coverage for a method is the fraction of the $R$ intervals that contain $\\sigma^2$. The undercoverage risk is calculated as $\\max\\{(1-\\alpha) - \\widehat{\\text{coverage}}, 0\\}$. For the specified $\\alpha=0.05$, the nominal coverage is $0.95$. A random seed is fixed at $12345$ for reproducibility. Finally, the results—empirical coverages and undercoverage risks for both methods for all four cases—are reported, rounded to three decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef run_simulation_case(C, m, sigma2, rho, alpha, R, B, rng):\n    \"\"\"\n    Runs a Monte Carlo simulation for a single parameter case.\n\n    Args:\n        C (int): Number of clusters.\n        m (int): Size of each cluster.\n        sigma2 (float): True population variance.\n        rho (float): Intracluster correlation.\n        alpha (float): Significance level for confidence intervals.\n        R (int): Number of Monte Carlo replicates.\n        B (int): Number of bootstrap resamples.\n        rng (np.random.Generator): A NumPy random number generator instance.\n\n    Returns:\n        tuple: A tuple containing four floats:\n               (empirical_coverage_design, empirical_coverage_indep,\n                undercoverage_risk_design, undercoverage_risk_indep).\n    \"\"\"\n    n = C * m\n    df = n - 1\n    nominal_coverage = 1.0 - alpha\n\n    # Derived variance components\n    sigma_b2 = rho * sigma2\n    sigma_w2 = (1.0 - rho) * sigma2\n    sigma_b = np.sqrt(sigma_b2) if sigma_b2 > 0 else 0\n    sigma_w = np.sqrt(sigma_w2) if sigma_w2 > 0 else 0\n\n    # Chi-squared quantiles for the independence-based interval\n    # These are constant for a given case\n    chi2_q_upper = chi2.ppf(1 - alpha / 2, df)\n    chi2_q_lower = chi2.ppf(alpha / 2, df)\n\n    coverage_indep_count = 0\n    coverage_design_count = 0\n\n    for _ in range(R):\n        # 1. Generate data from the random effects model\n        # Cluster effects U_i ~ N(0, sigma_b^2)\n        U = rng.normal(loc=0.0, scale=sigma_b, size=C)\n        # Individual errors e_ij ~ N(0, sigma_w^2)\n        epsilon = rng.normal(loc=0.0, scale=sigma_w, size=(C, m))\n        # Form observations Y_ij = U_i + e_ij\n        Y = U[:, np.newaxis] + epsilon\n\n        # 2. Compute the overall sample variance S^2\n        y_flat = Y.flatten()\n        S2 = np.var(y_flat, ddof=1)\n\n        # 3. Construct and check independence-based CI\n        ci_indep_low = (df * S2) / chi2_q_upper\n        ci_indep_high = (df * S2) / chi2_q_lower\n        if ci_indep_low = sigma2 = ci_indep_high:\n            coverage_indep_count += 1\n\n        # 4. Construct and check design-based (cluster bootstrap) CI\n        bootstrap_S2_values = []\n        for _ in range(B):\n            # Resample clusters with replacement\n            cluster_indices = rng.choice(C, size=C, replace=True)\n            Y_bootstrap = Y[cluster_indices, :]\n            \n            # Compute variance for the bootstrap sample\n            S2_bootstrap = np.var(Y_bootstrap.flatten(), ddof=1)\n            bootstrap_S2_values.append(S2_bootstrap)\n        \n        # Percentile method\n        ci_design_low = np.percentile(bootstrap_S2_values, 100 * (alpha / 2))\n        ci_design_high = np.percentile(bootstrap_S2_values, 100 * (1 - alpha / 2))\n        \n        if ci_design_low = sigma2 = ci_design_high:\n            coverage_design_count += 1\n            \n    # 5. Calculate empirical coverages\n    emp_cov_design = coverage_design_count / R\n    emp_cov_indep = coverage_indep_count / R\n\n    # 6. Calculate undercoverage risks\n    under_risk_design = max(nominal_coverage - emp_cov_design, 0)\n    under_risk_indep = max(nominal_coverage - emp_cov_indep, 0)\n\n    # 7. Round results to three decimal places\n    return (\n        round(emp_cov_design, 3),\n        round(emp_cov_indep, 3),\n        round(under_risk_design, 3),\n        round(under_risk_indep, 3)\n    )\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Define problem parameters\n    alpha = 0.05\n    R = 400\n    B = 200\n    seed = 12345\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(seed)\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Case A\n        {'C': 20, 'm': 10, 'sigma2': 1.0, 'rho': 0.3},\n        # Case B\n        {'C': 8, 'm': 20, 'sigma2': 1.5, 'rho': 0.6},\n        # Case C\n        {'C': 50, 'm': 5, 'sigma2': 2.0, 'rho': 0.4},\n        # Case D (boundary, no clustering)\n        {'C': 20, 'm': 10, 'sigma2': 1.0, 'rho': 0.0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = run_simulation_case(\n            case['C'], case['m'], case['sigma2'], case['rho'],\n            alpha, R, B, rng\n        )\n        all_results.append(list(case_results))\n\n    # Format the output as specified\n    print(all_results)\n\nsolve()\n```", "id": "4903180"}]}