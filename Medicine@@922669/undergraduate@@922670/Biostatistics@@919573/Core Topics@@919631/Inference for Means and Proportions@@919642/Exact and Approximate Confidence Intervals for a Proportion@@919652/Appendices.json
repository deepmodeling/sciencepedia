{"hands_on_practices": [{"introduction": "The Wald interval is often the first approximate confidence interval students learn due to its simplicity. However, its performance can be deceptively poor, especially in small samples or for proportions near the boundaries. This practice challenges you to move beyond the asymptotic theory and compute the *exact* coverage probability of the Wald interval using the binomial distribution [@problem_id:4911344]. By programming this calculation and identifying where the coverage is worst, you will gain a deep, practical understanding of why more robust methods are essential in biostatistics.", "problem": "Consider a sequence of independent and identically distributed Bernoulli trials with success probability $p \\in [0,1]$ and sample size $n \\in \\mathbb{N}$. Let $X \\sim \\mathrm{Binomial}(n,p)$ and let the sample proportion be $\\hat{p} = X/n$. A two-sided nominal $(1-\\alpha)$ confidence interval has coverage function $C(p;n,\\alpha) = \\mathbb{P}_{p}\\{p \\in I(X)\\}$, where $I(X)$ is a data-dependent interval. Define the undercoverage at $p$ as $U(p;n,\\alpha) = (1-\\alpha) - C(p;n,\\alpha)$.\n\nIn this task, take $I(X)$ to be the two-sided Wald interval constructed by inverting the asymptotic normal approximation to the distribution of $\\hat{p}$ with a plug-in variance estimate, using the $(1-\\alpha/2)$ quantile of the standard normal distribution. Work from first principles under the binomial model to compute the exact coverage $C(p;n,\\alpha)$ and thereby the undercoverage $U(p;n,\\alpha)$ as a function of $p$, for fixed $n$ and $\\alpha$.\n\nYour program must:\n- For each specified test case, approximate $\\sup_{p \\in [0,1]} U(p;n,\\alpha)$ by evaluating $U(p;n,\\alpha)$ on a uniform grid $p_1,\\dots,p_M$ over $[\\varepsilon,1-\\varepsilon]$ with $M$ points and a small exclusion $\\varepsilon > 0$, and return both the maximum undercoverage value and the grid point $p^\\star$ at which the maximum occurs.\n- Additionally, quantify whether the maximizer $p^\\star$ is near a boundary by returning a boolean that is $\\mathrm{True}$ if $p^\\star \\le \\delta$ or $p^\\star \\ge 1-\\delta$, and $\\mathrm{False}$ otherwise.\n\nFundamental base and definitions to use:\n- The binomial model $X \\sim \\mathrm{Binomial}(n,p)$ and the exact coverage definition $C(p;n,\\alpha) = \\sum_{k=0}^{n} \\mathbb{P}_{p}(X=k) \\cdot \\mathbf{1}\\{p \\in I(k)\\}$.\n- The Wald interval is built by inverting the asymptotic normality of $\\hat{p}$ with a plug-in variance estimate for $p(1-p)$, and uses the $(1-\\alpha/2)$ standard normal quantile. No other interval constructions are permitted.\n\nNumerical and output requirements:\n- Express all probabilities (including coverage and undercoverage) as decimals in $[0,1]$.\n- For each test case, return a list of three items: the maximum undercoverage value on the grid, the corresponding $p^\\star$ on the grid, and the boundary-peaking boolean. Round all floating-point outputs to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list. For example: $[ [a_1,b_1,c_1],[a_2,b_2,c_2] ]$.\n\nTest suite:\n- Case $1$: $n=10$, $\\alpha=0.05$, $M=2001$, $\\varepsilon=10^{-6}$, $\\delta=0.1$.\n- Case $2$: $n=10$, $\\alpha=0.10$, $M=2001$, $\\varepsilon=10^{-6}$, $\\delta=0.1$.\n- Case $3$: $n=50$, $\\alpha=0.05$, $M=2001$, $\\varepsilon=10^{-6}$, $\\delta=0.1$.\n- Case $4$: $n=200$, $\\alpha=0.05$, $M=2001$, $\\varepsilon=10^{-6}$, $\\delta=0.1$.\n\nFinal output format:\n- A single line containing a list of the four case results, each formatted as $[\\text{max\\_undercoverage}, \\text{p\\_star}, \\text{near\\_boundary}]$, with floats rounded to $6$ decimals, for example: $[[0.123456,0.001000,\\mathrm{True}],[\\dots],\\dots]$.", "solution": "The problem requires the computation of the maximum undercoverage of the two-sided Wald confidence interval for a binomial proportion $p$. The calculation must be performed using the exact binomial probability distribution, not the normal approximation that motivates the interval itself. We are asked to approximate the supremum of the undercoverage function, $\\sup_{p \\in [0,1]} U(p;n,\\alpha)$, by evaluating it on a fine, uniform grid of $p$ values.\n\nLet $X \\sim \\mathrm{Binomial}(n,p)$ be the number of successes in $n$ independent Bernoulli trials with success probability $p$. The sample proportion is $\\hat{p} = X/n$.\n\nFirst, we define the Wald confidence interval for $p$. It is derived from the asymptotic normality of the sample proportion, where the statistic $Z = (\\hat{p}-p)/\\sqrt{p(1-p)/n}$ converges in distribution to a standard normal distribution. The Wald interval replaces the unknown standard error $\\sqrt{p(1-p)/n}$ with its plug-in estimate $\\sqrt{\\hat{p}(1-\\hat{p})/n}$. For a given number of successes $k$, where $\\hat{p}=k/n$, the nominal $(1-\\alpha)$ Wald interval $I(k)$ is given by:\n$$\nI(k) = \\left[ \\frac{k}{n} - z_{1-\\alpha/2} \\sqrt{\\frac{\\frac{k}{n}(1-\\frac{k}{n})}{n}}, \\frac{k}{n} + z_{1-\\alpha/2} \\sqrt{\\frac{\\frac{k}{n}(1-\\frac{k}{n})}{n}} \\right]\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution. Note that for the boundary cases $k=0$ and $k=n$, the estimated standard error is $0$, causing the interval to collapse to a single point, $[0,0]$ or $[1,1]$ respectively.\n\nThe exact coverage probability of this interval, for a true proportion $p$, is the sum of probabilities of all possible outcomes $k \\in \\{0, 1, \\dots, n\\}$ for which the resulting interval $I(k)$ contains the true parameter $p$. This is formally expressed as:\n$$\nC(p; n, \\alpha) = \\mathbb{P}_{p}\\{p \\in I(X)\\} = \\sum_{k=0}^{n} \\mathbb{P}_{p}(X=k) \\cdot \\mathbf{1}\\{p \\in I(k)\\}\n$$\nwhere $\\mathbb{P}_{p}(X=k)$ is the binomial probability mass function (PMF),\n$$\n\\mathbb{P}_{p}(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\n$$\nand $\\mathbf{1}\\{p \\in I(k)\\}$ is an indicator function that equals $1$ if $p$ is in the interval $I(k)$ and $0$ otherwise.\n\nThe undercoverage at a given $p$ is the difference between the nominal coverage level, $(1-\\alpha)$, and the actual coverage probability $C(p; n, \\alpha)$:\n$$\nU(p; n, \\alpha) = (1-\\alpha) - C(p; n, \\alpha)\n$$\nA positive value of $U(p)$ indicates that the interval is performing worse than its nominal level.\n\nThe problem requires us to find the maximum undercoverage over $p \\in [0,1]$. An analytical solution for $\\sup_{p} U(p)$ is intractable. Therefore, we will employ a numerical approach as specified. We will evaluate $U(p; n, \\alpha)$ on a dense, uniform grid of $M$ points for $p$ in the interval $[\\varepsilon, 1-\\varepsilon]$, where $\\varepsilon$ is a small positive number to avoid numerical issues at the boundaries $p=0$ and $p=1$.\n\nThe algorithmic procedure is as follows:\n1.  For a given test case specified by $(n, \\alpha, M, \\varepsilon, \\delta)$, we first pre-compute the $(1-\\alpha/2)$ standard normal quantile, $z_{1-\\alpha/2}$.\n2.  We then pre-compute the $n+1$ Wald intervals, $I(k)$, for each possible outcome $k \\in \\{0, 1, \\dots, n\\}$. Let the lower and upper bounds of these intervals be $L_k$ and $U_k$.\n3.  A uniform grid of $M$ candidate values for $p$ is generated: $p_1, \\dots, p_M$ in $[\\varepsilon, 1-\\varepsilon]$.\n4.  For each $p_j$ in the grid, we calculate the exact coverage $C(p_j; n, \\alpha)$. This is computationally intensive if done naively. A vectorized approach is more efficient:\n    a. Construct a matrix of binomial probabilities, $\\mathbf{B}$, of size $M \\times (n+1)$, where $\\mathbf{B}_{jk} = \\mathbb{P}_{p_j}(X=k)$.\n    b. Construct a boolean indicator matrix, $\\mathbf{J}$, of size $M \\times (n+1)$, where $\\mathbf{J}_{jk} = \\mathbf{1}\\{p_j \\in [L_k, U_k]\\}$. This is efficiently computed using broadcasting operations.\n    c. The coverage probabilities for all $p_j$ are then calculated in a single vectorized operation by taking the element-wise product of $\\mathbf{B}$ and $\\mathbf{J}$ and summing along the rows (the $k$ axis): $C(p_j) = \\sum_{k=0}^{n} \\mathbf{B}_{jk} \\mathbf{J}_{jk}$.\n5.  With the vector of coverage values, we compute the vector of undercoverage values: $U(p_j) = (1-\\alpha) - C(p_j)$.\n6.  We find the maximum value in the undercoverage vector. This value is our approximation for $\\sup_{p} U(p)$. We also identify the corresponding proportion $p^\\star$ from the grid at which this maximum occurs.\n7.  Finally, we determine if $p^\\star$ is close to the boundaries $0$ or $1$ by checking if $p^\\star \\le \\delta$ or $p^\\star \\ge 1-\\delta$.\n8.  The results—maximum undercoverage, $p^\\star$, and the boundary-peaking boolean—are rounded and formatted as required.\n\nThis procedure accurately implements the first-principles definition of coverage and provides a robust numerical solution to the specified problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, binom\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, alpha, M, epsilon, delta)\n        (10, 0.05, 2001, 1e-6, 0.1),\n        (10, 0.10, 2001, 1e-6, 0.1),\n        (50, 0.05, 2001, 1e-6, 0.1),\n        (200, 0.05, 2001, 1e-6, 0.1),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _calculate_max_undercoverage(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in results:\n        max_u_str = f\"{res[0]:.6f}\"\n        p_star_str = f\"{res[1]:.6f}\"\n        near_b_str = str(res[2])\n        formatted_results.append(f\"[{max_u_str},{p_star_str},{near_b_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _calculate_max_undercoverage(n, alpha, M, epsilon, delta):\n    \"\"\"\n    Calculates the maximum undercoverage and related metrics for a single test case.\n    \n    Args:\n        n (int): Sample size.\n        alpha (float): Significance level for (1-alpha) confidence.\n        M (int): Number of points in the grid for p.\n        epsilon (float): Small exclusion for the p-grid boundaries.\n        delta (float): Threshold for boundary-peaking check.\n\n    Returns:\n        list: [max_undercoverage, p_star, near_boundary]\n    \"\"\"\n    # 1. Set up constants and grids\n    z_crit = norm.ppf(1.0 - alpha / 2.0)\n    p_grid = np.linspace(epsilon, 1.0 - epsilon, M)\n    k_values = np.arange(0, n + 1)\n\n    # 2. Pre-calculate Wald intervals for all possible outcomes k=0,...,n\n    p_hat = k_values / n\n\n    # The variance term is p_hat * (1 - p_hat), which is 0 for k=0 and k=n.\n    # np.sqrt is safe as the term is always non-negative.\n    std_err = np.sqrt(p_hat * (1.0 - p_hat) / n)\n    margin_of_error = z_crit * std_err\n    \n    lower_bounds = p_hat - margin_of_error\n    upper_bounds = p_hat + margin_of_error\n\n    # 3. Calculate Binomial PMF matrix for all p in grid and all k\n    # Create meshgrids for vectorized calculation\n    k_mesh, p_mesh = np.meshgrid(k_values, p_grid)\n    binom_pmf_matrix = binom.pmf(k_mesh, n, p_mesh)\n\n    # 4. Calculate coverage vector using broadcasting\n    # Reshape p_grid to (M, 1) to broadcast against interval bounds of shape (1, n+1)\n    p_grid_col = p_grid[:, np.newaxis]\n    \n    # is_covered is a boolean matrix of shape (M, n+1)\n    # is_covered[j, k] is True if p_grid[j] is in I(k)\n    is_covered = (p_grid_col >= lower_bounds) & (p_grid_col <= upper_bounds)\n\n    # Element-wise product and sum over k-axis gives coverage for each p\n    coverage_vector = np.sum(is_covered * binom_pmf_matrix, axis=1)\n\n    # 5. Calculate undercoverage and find its maximum\n    target_coverage = 1.0 - alpha\n    undercoverage_vector = target_coverage - coverage_vector\n    \n    max_idx = np.argmax(undercoverage_vector)\n    max_undercoverage = undercoverage_vector[max_idx]\n    p_star = p_grid[max_idx]\n\n    # 6. Determine if the maximizer p_star is near a boundary\n    near_boundary = (p_star <= delta) or (p_star >= 1.0 - delta)\n\n    return [max_undercoverage, p_star, near_boundary]\n\nsolve()\n```", "id": "4911344"}, {"introduction": "After diagnosing the failures of the Wald interval, the natural next step is to explore superior alternatives. This exercise guides you through a Monte Carlo simulation to empirically compare the performance of the Wald, Wilson score, and Agresti-Coull intervals [@problem_id:4911278]. You will assess these methods based on two key criteria: how often their actual coverage meets the nominal $95\\%$ level and the average width of the intervals they produce. This \"horse race\" provides a practical framework for method selection and highlights the excellent balance of properties offered by the Wilson and Agresti-Coull methods.", "problem": "A single population proportion is modeled by a Binomial law. Let $X$ denote the number of observed successes in $n$ independent Bernoulli trials with success probability $p$, so $X \\sim \\mathrm{Binomial}(n,p)$ and the sample proportion is $\\hat{p} = X/n$. Confidence intervals for $p$ can be derived by combining this model with large-sample approximations and test inversion. Your task is to derive, implement, and compare three intervals: the Wald interval, the Wilson score interval, and the Agresti-Coull interval.\n\nStarting from the Binomial model and the definition of the score and Wald tests, derive each interval using only fundamental principles:\n- Begin with the Binomial likelihood and the definition of the score and Wald tests.\n- Justify the normal approximation of $\\hat{p}$ for large $n$ where appropriate (via the Central Limit Theorem for independent and identically distributed Bernoulli variables).\n- Obtain the Wilson score interval by inverting the score test for $p$ based on the Binomial model.\n- Obtain the Agresti-Coull interval by applying the add-$z^2$ method motivated by the Wilson score interval and a Wald-type form on an adjusted sample proportion.\n\nAll intervals must be truncated to the valid parameter range $[0,1]$ for $p$.\n\nFor a fixed nominal confidence level $1-\\alpha$, define the coverage probability of an interval procedure as the probability, under the data-generating $p$, that the true $p$ lies within the computed interval based on $X$. Define the expected length of an interval as the expectation, under the data-generating $p$, of the interval length. In this task, approximate both quantities empirically by Monte Carlo simulation.\n\nImplementation requirements:\n- For each test case, simulate $R$ independent realizations $X_r \\sim \\mathrm{Binomial}(n,p)$, $r=1,\\dots,R$, compute the three intervals from $X_r$, and estimate coverage as the fraction of simulations in which the interval contains the true $p$. Estimate expected length as the average interval length across the $R$ simulations.\n- Use the standard normal quantile $z$ corresponding to the two-sided level $\\alpha$ obtained from the standard normal distribution.\n- Use a fixed random number generator seed equal to $20231111$ for reproducibility.\n- All coverage probabilities must be expressed as decimals (not with a percentage sign).\n- The output must aggregate results across all test cases into a single line. Each test case contributes six numbers in the following order: Wald coverage, Wald expected length, Wilson coverage, Wilson expected length, Agresti-Coull coverage, Agresti-Coull expected length. Each number must be rounded to four decimal places.\n\nTest suite:\n- Confidence level: $1-\\alpha = 0.95$, i.e., $\\alpha = 0.05$ in decimal form.\n- For moderate sample sizes and a range of success probabilities including interior and boundary-adjacent values, use the following cases (with $R$ denoting the number of Monte Carlo draws per case):\n  1. $(n,p,\\alpha,R) = (40, 0.10, 0.05, 100000)$,\n  2. $(n,p,\\alpha,R) = (80, 0.50, 0.05, 100000)$,\n  3. $(n,p,\\alpha,R) = (60, 0.90, 0.05, 100000)$,\n  4. $(n,p,\\alpha,R) = (50, 0.02, 0.05, 100000)$,\n  5. $(n,p,\\alpha,R) = (50, 0.98, 0.05, 100000)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated Python list of lists, one inner list per test case, in the exact order specified above. For example, an output line must look like\n$[\\,[c_{1,\\mathrm{Wald}},\\ell_{1,\\mathrm{Wald}},c_{1,\\mathrm{Wilson}},\\ell_{1,\\mathrm{Wilson}},c_{1,\\mathrm{AC}},\\ell_{1,\\mathrm{AC}}],\\dots,[c_{5,\\mathrm{Wald}},\\ell_{5,\\mathrm{Wald}},c_{5,\\mathrm{Wilson}},\\ell_{5,\\mathrm{Wilson}},c_{5,\\mathrm{AC}},\\ell_{5,\\mathrm{AC}}]\\,]$,\nwhere each $c$ is a coverage probability in decimal form and each $\\ell$ is an expected length in decimal form, all rounded to four decimal places.", "solution": "The problem requires the derivation, implementation, and empirical comparison of three common approximate confidence intervals for a single binomial proportion, $p$. The statistical model assumes that the number of observed successes, $X$, in a sample of size $n$, follows a binomial distribution, $X \\sim \\mathrm{Binomial}(n,p)$. The sample proportion, $\\hat{p} = X/n$, serves as the point estimator for $p$.\n\nThe foundation for these large-sample approximate intervals is the Central Limit Theorem (CLT). Since $X$ is a sum of $n$ independent and identically distributed $\\mathrm{Bernoulli}(p)$ random variables, the CLT ensures that for a sufficiently large sample size $n$, the sampling distribution of $\\hat{p}$ is approximately normal:\n$$\n\\hat{p} \\approx \\mathcal{N}\\left(p, \\frac{p(1-p)}{n}\\right)\n$$\nThis implies that the standardized random variable $Z = \\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}}$ converges in distribution to a standard normal distribution, $\\mathcal{N}(0,1)$. All three intervals are constructed by inverting a statistical test for the null hypothesis $H_0: p = p_0$, but they differ in their treatment of the standard error term, $\\sqrt{p(1-p)/n}$, which depends on the unknown parameter $p$. For a nominal confidence level of $1-\\alpha$, we use the critical value $z_{\\alpha/2}$, which is the upper $\\alpha/2$ quantile of the standard normal distribution.\n\n**The Wald Interval**\n\nThe Wald interval is derived by inverting the Wald test. In the Wald test statistic, the unknown parameter $p$ within the standard error term is replaced by its maximum likelihood estimator, $\\hat{p}$. The test statistic for $H_0: p = p_0$ is thus $Z = \\frac{\\hat{p}-p_0}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}$. A $100(1-\\alpha)\\%$ confidence interval for $p$ is defined as the set of all values $p_0$ for which this test would not reject the null hypothesis at significance level $\\alpha$. This corresponds to the inequality:\n$$\n\\left| \\frac{\\hat{p}-p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}} \\right| \\le z_{\\alpha/2}\n$$\nSolving this inequality for $p$ yields the symmetric Wald interval:\n$$\n\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n$$\nWhile simple, this interval exhibits poor performance, especially for small $n$ or when $p$ is close to the boundaries of $0$ or $1$. Its actual coverage probability frequently falls below the nominal level. A critical flaw occurs when $\\hat{p}=0$ or $\\hat{p}=1$, where the standard error becomes $0$, leading to an absurd zero-width interval. As per the problem, the resulting interval must be truncated to lie within the valid parameter space $[0,1]$.\n\n**The Wilson Score Interval**\n\nThe Wilson score interval is derived by inverting the score test. The score test retains the null-hypothesized value of the parameter, $p$, in the standard error term. The confidence interval is the set of all $p$ values that satisfy the inequality:\n$$\n\\left| \\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}} \\right| \\le z_{\\alpha/2}\n$$\nLetting $z = z_{\\alpha/2}$, squaring both sides gives $(\\hat{p}-p)^2 \\le z^2 \\frac{p(1-p)}{n}$. Rearranging this expression results in a quadratic inequality in $p$. The endpoints of the interval are the roots of the corresponding quadratic equation:\n$$\n(n+z^2)p^2 - (2n\\hat{p} + z^2)p + n\\hat{p}^2 = 0\n$$\nApplying the quadratic formula, the two roots for $p$ that define the interval are:\n$$\n\\frac{n\\hat{p} + z^2/2}{n+z^2} \\pm \\frac{z}{n+z^2} \\sqrt{n\\hat{p}(1-\\hat{p}) + z^2/4}\n$$\nThe Wilson interval provides substantially better performance than the Wald interval. It is generally asymmetric (unless $\\hat{p}=0.5$) and maintains a coverage probability much closer to the nominal $1-\\alpha$ level, even for smaller sample sizes and boundary-adjacent proportions. It is inherently contained within $[0,1]$ and thus does not require explicit truncation.\n\n**The Agresti-Coull Interval**\n\nThe Agresti-Coull (AC) interval is a highly-regarded adjustment that combines the superior performance characteristics of the Wilson interval with the simple structure of the Wald interval. Its derivation is motivated by the center of the Wilson interval, which is $\\frac{X + z^2/2}{n+z^2}$. This expression can be viewed as a new sample proportion based on an adjusted sample size $\\tilde{n} = n+z^2$ and an adjusted number of successes $\\tilde{X} = X + z^2/2$. The AC interval is simply a Wald-type interval computed using these adjusted values.\nLet $z = z_{\\alpha/2}$. First, we define the adjusted sample proportion:\n$$\n\\tilde{p} = \\frac{X + z^2/2}{n + z^2}\n$$\nThe AC interval is then constructed as:\n$$\n\\tilde{p} \\pm z \\sqrt{\\frac{\\tilde{p}(1-\\tilde{p})}{\\tilde{n}}} \\quad \\text{where } \\tilde{n} = n+z^2\n$$\nFor a $95\\%$ confidence level, $z \\approx 1.96$ and $z^2 \\approx 3.84$, which is often simplified to $z^2 \\approx 4$. This leads to the well-known \"add $2$ successes and $2$ failures\" rule. We will use the more general form with the exact value of $z^2$. The AC interval must also be truncated to the range $[0,1]$.\n\n**Monte Carlo Evaluation Methodology**\n\nTo empirically evaluate and compare the properties of these three intervals, we employ a Monte Carlo simulation. For each parameter combination $(n, p, \\alpha, R)$ specified in the test suite, we perform the following steps:\n1.  Generate $R$ independent random variates from the binomial distribution: $X_r \\sim \\mathrm{Binomial}(n,p)$ for $r = 1, \\dots, R$.\n2.  For each realization $X_r$, compute the sample proportion $\\hat{p}_r = X_r/n$ and then calculate the lower and upper bounds for each of the three confidence intervals (Wald, Wilson, Agresti-Coull).\n3.  The empirical coverage probability for each interval type is calculated as the fraction of the $R$ simulations in which the true parameter value $p$ is contained within the computed interval:\n    $$\n    \\text{Coverage} = \\frac{1}{R} \\sum_{r=1}^R \\mathbb{I}(p \\in [L_r, U_r])\n    $$\n    where $[L_r, U_r]$ is the interval for the $r$-th simulation and $\\mathbb{I}(\\cdot)$ is the indicator function.\n4.  The expected length of each interval is estimated by averaging the lengths of the $R$ computed intervals:\n    $$\n    \\text{Expected Length} = \\frac{1}{R} \\sum_{r=1}^R (U_r - L_r)\n    $$\nA superior interval is one that achieves a coverage probability close to the nominal $1-\\alpha$ level while having a minimal expected length. The simulation is conducted with a fixed random seed of $20231111$ to ensure reproducibility.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Derives, implements, and compares Wald, Wilson score, and Agresti-Coull \n    confidence intervals for a binomial proportion via Monte Carlo simulation.\n    \"\"\"\n    # Define test cases: (n, p, alpha, R)\n    test_cases = [\n        (40, 0.10, 0.05, 100000),\n        (80, 0.50, 0.05, 100000),\n        (60, 0.90, 0.05, 100000),\n        (50, 0.02, 0.05, 100000),\n        (50, 0.98, 0.05, 100000),\n    ]\n\n    # Set random seed for reproducibility\n    seed = 20231111\n    rng = np.random.default_rng(seed)\n\n    all_results = []\n\n    for n, p_true, alpha, R in test_cases:\n        # Calculate the standard normal quantile z_{alpha/2}\n        z = norm.ppf(1 - alpha / 2)\n        z_sq = z**2\n\n        # Generate R binomial random variates\n        X = rng.binomial(n, p_true, size=R)\n        phat = X / n\n\n        # --- 1. Wald Interval ---\n        # Standard error for Wald interval\n        se_wald = np.sqrt(phat * (1 - phat) / n)\n        \n        # Calculate interval bounds\n        lower_wald = phat - z * se_wald\n        upper_wald = phat + z * se_wald\n        \n        # Truncate interval to [0, 1]\n        lower_wald = np.maximum(0, lower_wald)\n        upper_wald = np.minimum(1, upper_wald)\n        \n        # Calculate coverage and expected length\n        coverage_wald = np.mean((lower_wald <= p_true) & (upper_wald >= p_true))\n        length_wald = np.mean(upper_wald - lower_wald)\n        \n        # --- 2. Wilson Score Interval ---\n        center_wilson = (phat + z_sq / (2 * n)) / (1 + z_sq / n)\n        width_term_wilson = (z / (1 + z_sq / n)) * np.sqrt(phat * (1 - phat) / n + z_sq / (4 * n**2))\n        \n        lower_wilson = center_wilson - width_term_wilson\n        upper_wilson = center_wilson + width_term_wilson\n        \n        # Wilson interval is naturally within [0,1], but clipping for numerical stability\n        lower_wilson = np.maximum(0, lower_wilson)\n        upper_wilson = np.minimum(1, upper_wilson)\n\n        # Calculate coverage and expected length\n        coverage_wilson = np.mean((lower_wilson <= p_true) & (upper_wilson >= p_true))\n        length_wilson = np.mean(upper_wilson - lower_wilson)\n\n        # --- 3. Agresti-Coull Interval ---\n        n_tilde = n + z_sq\n        p_tilde = (X + z_sq / 2) / n_tilde\n\n        se_ac = np.sqrt(p_tilde * (1 - p_tilde) / n_tilde)\n        \n        lower_ac = p_tilde - z * se_ac\n        upper_ac = p_tilde + z * se_ac\n\n        # Truncate interval to [0, 1]\n        lower_ac = np.maximum(0, lower_ac)\n        upper_ac = np.minimum(1, upper_ac)\n        \n        # Calculate coverage and expected length\n        coverage_ac = np.mean((lower_ac <= p_true) & (upper_ac >= p_true))\n        length_ac = np.mean(upper_ac - lower_ac)\n\n        # Store results for the current test case\n        case_results = [\n            coverage_wald, length_wald,\n            coverage_wilson, length_wilson,\n            coverage_ac, length_ac\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists with 4-decimal rounding\n    formatted_results = []\n    for res_list in all_results:\n        # Format each number to 4 decimal places\n        formatted_list = [f\"{val:.4f}\" for val in res_list]\n        # Create the string representation of the inner list\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    # Create the final string representation of the list of lists\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "4911278"}, {"introduction": "Beyond score-based methods, likelihood theory itself provides a powerful and general principle for constructing confidence intervals. This advanced practice introduces the likelihood ratio (LR) interval, a method with excellent theoretical properties that is derived from Wilks' theorem [@problem_id:4911332]. You will implement the Newton-Raphson numerical method to find the interval's endpoints by solving the deviance equation, offering valuable experience in both statistical theory and computational practice while comparing its performance to the robust Wilson interval.", "problem": "Consider a single Binomial sampling scenario, where an observed count $x$ of \"successes\" out of $n$ independent trials is modeled by a Binomial random variable with success probability $p \\in (0,1)$. The goal is to compute both an exact likelihood ratio confidence interval and an approximate Wilson score confidence interval for $p$, starting from first principles, without using pre-packaged formulas.\n\nFundamental base for derivation:\n- Binomial model: the probability mass function is $P(X=x \\mid p)=\\binom{n}{x}p^x(1-p)^{n-x}$, and the log-likelihood is $\\ell(p)=x\\log p+(n-x)\\log(1-p)+\\text{constant}$, where the additive constant does not depend on $p$.\n- The maximum likelihood estimator is $\\hat p=x/n$.\n- The likelihood ratio test statistic (also called the deviance) comparing a fixed $p$ to $\\hat p$ is defined by the difference in twice the log-likelihoods. The one-parameter Wilks' theorem implies that for large samples, the likelihood ratio statistic is approximately chi-squared distributed with one degree of freedom.\n- The score test for the Binomial model is based on the standardized score under a fixed $p$, and inversion of the score test inequality yields the Wilson score confidence interval.\n\nYour task:\n1. Using the Binomial model and the definition of the likelihood ratio statistic as the difference in twice the log-likelihoods, derive the scalar nonlinear equation in $p$ whose roots define the endpoints of the $(1-\\alpha)$ likelihood ratio confidence interval, i.e., the set of $p$ such that the deviance equals the upper $(1-\\alpha)$ quantile of a chi-squared distribution with one degree of freedom. Implement a robust Newton–Raphson root finder on this deviance equation to compute the endpoints. Your solver must enforce $p \\in (0,1)$ and handle boundary cases $x=0$ and $x=n$ by correctly returning one interval endpoint at $0$ or $1$ respectively.\n2. Starting from the Binomial score function and its variance under the model, invert the two-sided score test inequality to obtain the Wilson score confidence interval. Implement its computation directly from the derived inequality, ensuring outputs are in the unit interval.\n3. For each test case, compute and return the following five values as decimal floats, rounded to six decimal places: the maximum likelihood estimate $\\hat p$, the lower and upper endpoints of the likelihood ratio interval, and the lower and upper endpoints of the Wilson score interval. All probability values must be reported as decimals (not percentages).\n\nTest suite:\n- Case A: $n=50$, $x=10$, $\\alpha=0.05$.\n- Case B: $n=50$, $x=0$, $\\alpha=0.05$ (boundary at $0$).\n- Case C: $n=50$, $x=50$, $\\alpha=0.05$ (boundary at $1$).\n- Case D: $n=50$, $x=1$, $\\alpha=0.05$ (near-boundary with sparse successes).\n- Case E: $n=200$, $x=100$, $\\alpha=0.01$ (larger sample, central proportion, stricter $\\alpha$).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a five-element list in the order $[\\hat p,\\text{LR}_{\\text{lower}},\\text{LR}_{\\text{upper}},\\text{Wilson}_{\\text{lower}},\\text{Wilson}_{\\text{upper}}]$, with each entry rounded to six decimal places. For example, a valid format is $[[0.200000,0.106000,0.324000,0.116000,0.322000],[\\dots],\\dots]$.", "solution": "### 1. Likelihood Ratio (LR) Confidence Interval\n\nThe log-likelihood for a binomial proportion $p$, given $x$ successes in $n$ trials, is given by $\\ell(p) = x\\log(p) + (n-x)\\log(1-p)$, ignoring constants that do not depend on $p$. The Maximum Likelihood Estimator (MLE) is $\\hat{p} = x/n$.\n\nThe likelihood ratio test statistic, or deviance, for testing a null hypothesis $H_0: p=p_0$ is:\n$$ D(p_0) = 2 \\left( \\ell(\\hat{p}) - \\ell(p_0) \\right) $$\nSubstituting the log-likelihood expressions, we get:\n$$ D(p_0) = 2 \\left( \\left[ x\\log(\\hat{p}) + (n-x)\\log(1-\\hat{p}) \\right] - \\left[ x\\log(p_0) + (n-x)\\log(1-p_0) \\right] \\right) $$\n$$ D(p_0) = 2 \\left( x\\log\\left(\\frac{\\hat{p}}{p_0}\\right) + (n-x)\\log\\left(\\frac{1-\\hat{p}}{1-p_0}\\right) \\right) $$\nAccording to Wilks' theorem, $D(p_0)$ follows an approximate chi-squared distribution with one degree of freedom, $\\chi^2_1$. A $(1-\\alpha)$ confidence interval is formed by the set of all values $p_0$ that are not rejected by the test at level $\\alpha$. This corresponds to the set of $p$ such that $D(p) \\le c_\\alpha$, where $c_\\alpha = \\chi^2_{1, 1-\\alpha}$ is the upper $(1-\\alpha)$ quantile of the $\\chi^2_1$ distribution. The endpoints of the interval are the roots of the nonlinear equation:\n$$ f(p) = 2 \\left( x\\log\\left(\\frac{\\hat{p}}{p}\\right) + (n-x)\\log\\left(\\frac{1-\\hat{p}}{1-p}\\right) \\right) - c_\\alpha = 0 $$\nTo solve this equation, we use the Newton-Raphson method, which requires the derivative of $f(p)$:\n$$ f'(p) = \\frac{d}{dp} D(p) = 2 \\left( -\\frac{x}{p} + \\frac{n-x}{1-p} \\right) = 2 \\frac{np-x}{p(1-p)} $$\nThe iterative update rule is $p_{k+1} = p_k - f(p_k)/f'(p_k)$. A robust implementation requires handling numerical stability and ensuring the solution remains within the valid range of $(0,1)$. A hybrid Newton-Bisection method will be used, where a bisection step is taken if the Newton step falls outside the search bounds.\n\n**Boundary Cases:**\n-   If $x=0$, then $\\hat{p}=0$. The log-likelihood $\\ell(\\hat{p}) = \\ell(0) = 0$. The deviance simplifies to $D(p) = -2n\\log(1-p)$. The lower bound of the interval is $0$. The upper bound is found by solving $-2n\\log(1-p) = c_\\alpha$, which yields $p_U = 1 - \\exp(-c_\\alpha / (2n))$.\n-   If $x=n$, then $\\hat{p}=1$. The log-likelihood $\\ell(\\hat{p}) = \\ell(1) = 0$. The deviance simplifies to $D(p) = -2n\\log(p)$. The upper bound is $1$. The lower bound is found by solving $-2n\\log(p) = c_\\alpha$, which yields $p_L = \\exp(-c_\\alpha / (2n))$.\n\n### 2. Wilson Score Confidence Interval\n\nThe score function is the first derivative of the log-likelihood:\n$$ U(p) = \\frac{\\partial \\ell}{\\partial p} = \\frac{x-np}{p(1-p)} $$\nThe variance of the score is the Fisher information, $I(p) = n/(p(1-p))$. The standardized score test statistic is $Z(p) = U(p)/\\sqrt{I(p)}$, which simplifies to:\n$$ Z(p) = \\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}} $$\nFor large $n$, $Z(p)$ is approximately standard normal. The $(1-\\alpha)$ confidence interval is the set of $p$ for which the score test does not reject, i.e., $|Z(p)| \\le z_{\\alpha/2}$, where $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution. This is equivalent to $Z(p)^2 \\le z_{\\alpha/2}^2$:\n$$ \\frac{(\\hat{p}-p)^2}{p(1-p)/n} \\le z_{\\alpha/2}^2 $$\nRearranging this inequality yields a quadratic in $p$:\n$$ \\left(1 + \\frac{z_{\\alpha/2}^2}{n}\\right)p^2 - \\left(2\\hat{p} + \\frac{z_{\\alpha/2}^2}{n}\\right)p + \\hat{p}^2 \\le 0 $$\nThe roots of the corresponding quadratic equation $Ap^2+Bp+C=0$ provide the endpoints of the interval. Using the quadratic formula, the endpoints are:\n$$ p = \\frac{\\left(\\hat{p} + \\frac{z_{\\alpha/2}^2}{2n}\\right) \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{\\alpha/2}^2}{4n^2}}}{1 + \\frac{z_{\\alpha/2}^2}{n}} $$\nThis formula is implemented directly.\n\n### 3. Implementation and Final Output\n\nThe following Python code implements these derivations. A robust Newton-Raphson solver is created to find the LR interval roots for the general case, while specific formulae are used for boundary cases. The Wilson interval is computed using its closed-form solution. The results for each test case are collected and formatted into a single string as required.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2, norm\n\ndef _robust_newton_solver(f, f_prime, bounds, tol=1e-9, max_iter=100):\n    \"\"\"\n    A robust Newton-Raphson solver that falls back to bisection if a\n    step goes out of the specified bounds. This is suitable for finding\n    the roots of the well-behaved deviance function.\n    \"\"\"\n    low, high = bounds\n    p = (low + high) / 2.0  # Start with a bisection step\n\n    f_low = f(low)\n    \n    for _ in range(max_iter):\n        fp = f(p)\n        if abs(high - low) < tol or abs(fp) < tol:\n            return p\n\n        # Update bracket for bisection fallback\n        if f_low * fp < 0:\n            high = p\n        else:\n            low = p\n            f_low = fp\n\n        f_prime_p = f_prime(p)\n        if abs(f_prime_p) < 1e-12: # Avoid division by zero\n            p_next = (low + high) / 2.0\n        else:\n            p_next = p - fp / f_prime_p\n\n        # If Newton step is out of bounds, fall back to bisection\n        if not (low <= p_next <= high):\n            p_next = (low + high) / 2.0\n        \n        p = p_next\n\n    return p\n\ndef get_lr_interval(n, x, alpha):\n    \"\"\"\n    Computes the Likelihood Ratio (LR) confidence interval for a binomial proportion.\n    \"\"\"\n    p_hat = x / n\n    crit_val = chi2.ppf(1 - alpha, df=1)\n\n    # Handle boundary cases with derived closed-form solutions\n    if x == 0:\n        lower = 0.0\n        upper = 1 - np.exp(-crit_val / (2 * n))\n        return lower, upper\n\n    if x == n:\n        lower = np.exp(-crit_val / (2 * n))\n        upper = 1.0\n        return lower, upper\n    \n    # Deviance function to find roots for: D(p) - crit_val = 0\n    def f(p):\n        if p <= 0 or p >= 1:\n            return np.inf\n        term1 = x * np.log(p_hat / p)\n        term2 = (n - x) * np.log((1 - p_hat) / (1 - p))\n        return 2 * (term1 + term2) - crit_val\n\n    # Derivative of the deviance function\n    def f_prime(p):\n        if p <= 0 or p >= 1:\n            return np.inf\n        return 2 * (n * p - x) / (p * (1 - p))\n\n    # Find lower root\n    lower_bound_search = (1e-12, p_hat)\n    lower = _robust_newton_solver(f, f_prime, lower_bound_search)\n    \n    # Find upper root\n    upper_bound_search = (p_hat, 1 - 1e-12)\n    upper = _robust_newton_solver(f, f_prime, upper_bound_search)\n\n    return lower, upper\n\ndef get_wilson_interval(n, x, alpha):\n    \"\"\"\n    Computes the Wilson score confidence interval for a binomial proportion.\n    \"\"\"\n    p_hat = x / n\n    z = norm.ppf(1 - alpha / 2)\n    z_sq = z**2\n    \n    denominator = 1 + z_sq / n\n    center_adj = p_hat + z_sq / (2 * n)\n    spread = z * np.sqrt((p_hat * (1 - p_hat) / n) + (z_sq / (4 * n**2)))\n    \n    lower = (center_adj - spread) / denominator\n    upper = (center_adj + spread) / denominator\n    \n    return lower, upper\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        (50, 10, 0.05), # Case A\n        (50, 0, 0.05),  # Case B\n        (50, 50, 0.05), # Case C\n        (50, 1, 0.05),  # Case D\n        (200, 100, 0.01),# Case E\n    ]\n\n    all_results = []\n    for n, x, alpha in test_cases:\n        p_hat = x / n\n        \n        lr_lower, lr_upper = get_lr_interval(n, x, alpha)\n        wilson_lower, wilson_upper = get_wilson_interval(n, x, alpha)\n        \n        case_results = [\n            p_hat, \n            lr_lower, \n            lr_upper, \n            wilson_lower, \n            wilson_upper\n        ]\n        all_results.append(case_results)\n\n    # Format the output string exactly as specified.\n    result_strings = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        res_list_str = [f\"{v:.6f}\" for v in res_list]\n        result_strings.append(f\"[{','.join(res_list_str)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "4911332"}]}