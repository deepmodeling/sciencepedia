{"hands_on_practices": [{"introduction": "The degrees of freedom, $\\nu$, in a Welch's t-test is not a simple count but a crucial measure of uncertainty derived from the data. The test's reliability is adjusted based on the sample variances and sizes of the two groups, a concept captured by the Welch-Satterthwaite approximation. This exercise [@problem_id:4966248] develops your intuition for this principle by having you explore how heteroscedasticity (unequal variances) and unbalanced sample sizes combine to affect the test's power, particularly in the \"worst-case scenario\" where a small group has large variance.", "problem": "A study compares the mean serum concentration of a biomarker between two independent patient groups receiving two formulations of the same drug. The two groups are known to be heteroscedastic, with unequal population variances. You are tasked with quantifying how the imbalance in sample sizes and the ratio of sample variances affect the approximate degrees of freedom used in the unequal-variance two-sample Student’s $t$ test (commonly called Welch’s $t$ test).\n\nConsider the following two scenarios, each with group $1$ and group $2$:\n\n- Scenario A (high variance in the smaller group): group $1$ has sample size $n_{1} = 15$ and sample variance $s_{1}^{2} = 25$; group $2$ has sample size $n_{2} = 65$ and sample variance $s_{2}^{2} = 9$.\n- Scenario B (high variance in the larger group): group $1$ has sample size $n_{1} = 15$ and sample variance $s_{1}^{2} = 9$; group $2$ has sample size $n_{2} = 65$ and sample variance $s_{2}^{2} = 25$.\n\nStarting from first principles appropriate for biostatistics, namely the independence of samples, the sampling distribution properties of the sample mean, and the sampling distribution properties of the sample variance, derive the approximate degrees of freedom $\\nu$ used in Welch’s $t$ test. Then, compute $\\nu$ for Scenario A and Scenario B, and report the ratio\n$$\nR \\;=\\; \\frac{\\nu_{\\text{A}}}{\\nu_{\\text{B}}}.\n$$\nRound your final numerical answer for $R$ to four significant figures. No units are required.\n\nIn your reasoning, explain why smaller values of $\\nu$ indicate greater uncertainty arising from heteroscedasticity and sample-size imbalance, but do not use any shortcut formulas stated without derivation.", "solution": "The problem asks for the derivation of the approximate degrees of freedom, $\\nu$, for the Welch's $t$-test, and to compute its value for two scenarios to understand the impact of sample size imbalance and heteroscedasticity.\n\nThe Welch's $t$-test is used to test the hypothesis of equal means between two independent samples, $\\bar{X}_1$ and $\\bar{X}_2$, drawn from populations with means $\\mu_1$ and $\\mu_2$ and variances $\\sigma_1^2$ and $\\sigma_2^2$, where it is not assumed that $\\sigma_1^2 = \\sigma_2^2$. The samples have sizes $n_1$ and $n_2$, and sample variances $s_1^2$ and $s_2^2$.\n\nUnder the null hypothesis $H_0: \\mu_1 = \\mu_2$, the Welch's $t$-statistic is given by:\n$$\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n$$\nThe numerator, $\\bar{X}_1 - \\bar{X}_2$, assuming the populations are normally distributed or by the Central Limit Theorem for large sample sizes, follows a normal distribution with mean $\\mu_1 - \\mu_2$ and variance $\\text{Var}(\\bar{X}_1 - \\bar{X}_2) = \\text{Var}(\\bar{X}_1) + \\text{Var}(\\bar{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$.\nThe denominator is the standard error of the difference between the means, where the unknown population variances $\\sigma_1^2$ and $\\sigma_2^2$ are estimated by the sample variances $s_1^2$ and $s_2^2$.\n\nThe statistic $t$ is approximately distributed as a Student's $t$-distribution with an effective number of degrees of freedom, $\\nu$. To find $\\nu$, we use the Welch-Satterthwaite approximation. This method approximates the distribution of a linear combination of independent sample variances.\n\nLet the random variable for the squared standard error be $Y = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}$. The key idea is to approximate the distribution of $Y$ by a scaled chi-squared distribution, $c\\chi^2_{\\nu}$, by matching the first two moments (mean and variance).\n\nFirst, we determine the mean and variance of $Y$.\nThe sample variance $s_i^2$ is an unbiased estimator of the population variance $\\sigma_i^2$, so $E[s_i^2] = \\sigma_i^2$. The expectation of $Y$ is:\n$$\nE[Y] = E\\left[\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right] = \\frac{E[s_1^2]}{n_1} + \\frac{E[s_2^2]}{n_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n$$\nNext, we find the variance of $Y$. Assuming the underlying populations are normal, the quantity $\\frac{(n_i-1)s_i^2}{\\sigma_i^2}$ follows a chi-squared distribution with $n_i-1$ degrees of freedom, $\\chi^2_{n_i-1}$. The variance of a $\\chi^2_k$ distribution is $2k$.\n$$\n\\text{Var}\\left(\\frac{(n_i-1)s_i^2}{\\sigma_i^2}\\right) = 2(n_i-1)\n$$\nUsing the property $\\text{Var}(aX) = a^2\\text{Var}(X)$, we can find the variance of $s_i^2$:\n$$\n\\left(\\frac{n_i-1}{\\sigma_i^2}\\right)^2 \\text{Var}(s_i^2) = 2(n_i-1) \\implies \\text{Var}(s_i^2) = \\frac{2(\\sigma_i^2)^2}{n_i-1}\n$$\nSince the two samples are independent, $s_1^2$ and $s_2^2$ are independent random variables. Therefore, the variance of their sum is the sum of their variances:\n$$\n\\text{Var}(Y) = \\text{Var}\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right) = \\frac{\\text{Var}(s_1^2)}{n_1^2} + \\frac{\\text{Var}(s_2^2)}{n_2^2} = \\frac{1}{n_1^2}\\left(\\frac{2(\\sigma_1^2)^2}{n_1-1}\\right) + \\frac{1}{n_2^2}\\left(\\frac{2(\\sigma_2^2)^2}{n_2-1}\\right)\n$$\n\nNow, we match these moments to those of a scaled chi-squared variable, $Z = c\\chi^2_{\\nu}$.\nThe mean and variance of $Z$ are:\n$$\nE[Z] = E[c\\chi^2_{\\nu}] = c E[\\chi^2_{\\nu}] = c\\nu\n$$\n$$\n\\text{Var}(Z) = \\text{Var}[c\\chi^2_{\\nu}] = c^2 \\text{Var}[\\chi^2_{\\nu}] = c^2(2\\nu)\n$$\nEquating the means, $E[Y] = E[Z]$:\n$$\n\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} = c\\nu\n$$\nEquating the variances, $\\text{Var}(Y) = \\text{Var}(Z)$:\n$$\n\\frac{2(\\sigma_1^2)^2}{n_1^2(n_1-1)} + \\frac{2(\\sigma_2^2)^2}{n_2^2(n_2-1)} = 2c^2\\nu\n$$\nFrom the mean equation, we have $c = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)$. Substituting this into the variance equation:\n$$\n\\frac{(\\sigma_1^2)^2}{n_1^2(n_1-1)} + \\frac{(\\sigma_2^2)^2}{n_2^2(n_2-1)} = c^2\\nu = \\frac{1}{\\nu^2}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2 \\nu = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2\n$$\nSolving for $\\nu$, we obtain the Welch-Satterthwaite equation for degrees of freedom:\n$$\n\\nu = \\frac{\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2}{\\frac{(\\sigma_1^2/n_1)^2}{n_1-1} + \\frac{(\\sigma_2^2/n_2)^2}{n_2-1}}\n$$\nIn practice, the population variances $\\sigma_1^2$ and $\\sigma_2^2$ are unknown. We substitute their sample estimates $s_1^2$ and $s_2^2$ to obtain the final form for the approximate degrees of freedom:\n$$\n\\nu \\approx \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\n$$\nThe value of $\\nu$ represents the effective amount of information available to estimate the variance of the difference in means. A smaller value of $\\nu$ corresponds to a $t$-distribution with heavier tails, which reflects greater uncertainty. This greater uncertainty requires a larger observed $t$-statistic to achieve statistical significance. From the formula, it can be seen that the denominator term $\\frac{(s_i^2/n_i)^2}{n_i-1}$ is the variance of the variance estimate for group $i$. If a group with a small sample size $n_i$ (and thus a small $n_i-1$ in the denominator) also has a large variance $s_i^2$, its contribution to the overall uncertainty becomes disproportionately large. This combination results in a smaller value of $\\nu$, penalizing the test for the higher uncertainty.\n\nNow we compute $\\nu$ for the two scenarios.\n\nScenario A: $n_1 = 15$, $s_1^2 = 25$; $n_2 = 65$, $s_2^2 = 9$. The smaller group has the larger variance.\nThe terms for the variance components are:\n$v_1 = \\frac{s_1^2}{n_1} = \\frac{25}{15} = \\frac{5}{3}$\n$v_2 = \\frac{s_2^2}{n_2} = \\frac{9}{65}$\nThe degrees of freedom $\\nu_A$ are:\n$$\n\\nu_A = \\frac{\\left(\\frac{5}{3} + \\frac{9}{65}\\right)^2}{\\frac{(5/3)^2}{15-1} + \\frac{(9/65)^2}{65-1}} = \\frac{\\left(\\frac{325+27}{195}\\right)^2}{\\frac{25/9}{14} + \\frac{81/4225}{64}} = \\frac{(352/195)^2}{\\frac{25}{126} + \\frac{81}{270400}}\n$$\nNumerically:\n$$\n\\nu_A \\approx \\frac{(1.66667 + 0.13846)^2}{\\frac{(1.66667)^2}{14} + \\frac{(0.13846)^2}{64}} \\approx \\frac{(1.80513)^2}{0.198413 + 0.0002996} \\approx \\frac{3.25849}{0.1987126} \\approx 16.398\n$$\n\nScenario B: $n_1 = 15$, $s_1^2 = 9$; $n_2 = 65$, $s_2^2 = 25$. The larger group has the larger variance.\nThe terms for the variance components are:\n$v_1 = \\frac{s_1^2}{n_1} = \\frac{9}{15} = \\frac{3}{5}$\n$v_2 = \\frac{s_2^2}{n_2} = \\frac{25}{65} = \\frac{5}{13}$\nThe degrees of freedom $\\nu_B$ are:\n$$\n\\nu_B = \\frac{\\left(\\frac{3}{5} + \\frac{5}{13}\\right)^2}{\\frac{(3/5)^2}{15-1} + \\frac{(5/13)^2}{65-1}} = \\frac{\\left(\\frac{39+25}{65}\\right)^2}{\\frac{9/25}{14} + \\frac{25/169}{64}} = \\frac{(64/65)^2}{\\frac{9}{350} + \\frac{25}{10816}}\n$$\nNumerically:\n$$\n\\nu_B \\approx \\frac{(0.6 + 0.38462)^2}{\\frac{(0.6)^2}{14} + \\frac{(0.38462)^2}{64}} \\approx \\frac{(0.98462)^2}{0.025714 + 0.002311} \\approx \\frac{0.96947}{0.028025} \\approx 34.595\n$$\nThe value of $\\nu_A$ is much smaller than $\\nu_B$, confirming that placing high variance in the small sample group leads to greater overall uncertainty and thus fewer effective degrees of freedom.\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\nu_A}{\\nu_B} = \\frac{16.39829}{34.59544} \\approx 0.473998\n$$\nRounding to four significant figures, we get $R \\approx 0.4740$.", "answer": "$$\\boxed{0.4740}$$", "id": "4966248"}, {"introduction": "While statistical tests have clear mathematical foundations, their application to real-world data requires careful judgment, especially when outliers are present. An outlier can disproportionately inflate a group's sample variance, $S^2$, which in turn affects both the standard error and the degrees of freedom in a Welch's t-test. This practice [@problem_id:4966276] challenges you to think like a practicing biostatistician, moving beyond simple calculation to diagnose the influence of individual data points and ensuring your conclusions are robust.", "problem": "A biostatistics study compares a continuous biomarker between two independent groups using Welch’s two-sample $t$ test. Group $C$ (control) has $n_C=24$, sample mean $\\bar{X}_C=4.1$, and sample variance $S_C^2=1.44$. Group $T$ (treatment) has $n_T=12$. An exploratory plot reveals one extremely high value in Group $T$. With all Group $T$ observations included, the sample mean and variance are $\\bar{X}_T=5.2$ and $S_T^2=6.76$. Temporarily excluding the suspected outlier yields $\\bar{X}_{T,(-)}=4.8$, $S_{T,(-)}^2=3.24$, and $n_{T,(-)}=11$. You plan to proceed with Welch’s two-sample $t$ test, which uses group-specific variability to form the denominator (the standard error (SE)) of the test statistic and adjusts the degrees of freedom to account for unequal variances.\n\nBased on first principles for sampling variability of independent sample means and how sample variance is computed, select all statements that are correct about how an outlier in the higher-variance group can affect the test and about diagnostics suited to detect such influence on $S_k^2$ and the standard error.\n\nA. In Welch’s two-sample $t$ test, an outlier that inflates the high-variance group’s $S_k^2$ will increase the estimated standard error of the mean difference, typically decreasing the magnitude of the test statistic and reducing power.\n\nB. In Welch’s two-sample $t$ test, the degrees of freedom are unaffected by changes in group variances, so an outlier does not influence them.\n\nC. A case-deletion check that recomputes the group variance and the test’s standard error with and without each observation is a direct diagnostic for identifying points that disproportionately inflate the denominator of the test statistic.\n\nD. Within-group quantile–quantile plots and robust scale estimates such as the Median Absolute Deviation (MAD) can help distinguish variance inflation due to a few outliers from generally high dispersion in that group.\n\nE. Levene’s test based on means will always flag a single outlier as variance heterogeneity, while the Brown–Forsythe variant based on medians will never do so.\n\nF. Computing within-group externally studentized residuals and comparing the Welch standard error under case deletion for each observation are diagnostics that directly assess an observation’s impact on the denominator of the Welch test statistic.\n\nSelect all that apply.", "solution": "This problem assesses understanding of how outliers affect the components of Welch's $t$-test and the diagnostics used to identify influential points. The key components are the standard error ($SE$) and the degrees of freedom ($\\nu$), both of which depend on the sample variances ($S_k^2$).\n\n**A. Correct.** The standard error is $SE = \\sqrt{S_C^2/n_C + S_T^2/n_T}$. An outlier that inflates $S_T^2$ will increase the $SE$. The magnitude of the test statistic is $|t| = |\\bar{X}_T - \\bar{X}_C| / SE$. While an outlier also affects the numerator by changing $\\bar{X}_T$, its influence on the variance (which depends on squared deviations) is often much larger than its influence on the mean. A proportionally larger increase in the denominator ($SE$) compared to the numerator ($|\\bar{X}_T - \\bar{X}_C|$) leads to a smaller test statistic $|t|$, a larger p-value, and thus reduced statistical power. The qualifier \"typically\" is appropriate as this is a strong tendency, not a mathematical certainty.\n\n**B. Incorrect.** The Welch-Satterthwaite formula for degrees of freedom,\n$$ \\nu \\approx \\frac{\\left(S_1^2/n_1 + S_2^2/n_2\\right)^2}{\\frac{\\left(S_1^2/n_1\\right)^2}{n_1 - 1} + \\frac{\\left(S_2^2/n_2\\right)^2}{n_2 - 1}} $$\nis an explicit function of the sample variances $S_1^2$ and $S_2^2$. Since an outlier changes the sample variance of its group, it must also change the calculated degrees of freedom.\n\n**C. Correct.** A case-deletion diagnostic is a \"leave-one-out\" procedure. By removing one observation at a time and recalculating the statistic of interest (in this case, the group variance $S_k^2$ and the overall $SE$), one can directly measure the influence of each individual point. An observation whose removal causes a large change in the $SE$ is identified as being influential on the denominator of the test statistic. This is a standard and direct method for influence diagnostics.\n\n**D. Correct.** These are standard exploratory tools for diagnosing the nature of variability. A quantile-quantile (Q-Q) plot will show isolated points deviating from the line in the case of outliers, whereas a generally heavy-tailed distribution would show systematic deviation (e.g., an S-curve). A robust scale estimate like the Median Absolute Deviation (MAD) is insensitive to outliers. Comparing the standard deviation ($S$) to a scaled MAD provides a quantitative check: if $S$ is much larger than the scaled MAD, it suggests that the high variance is driven by outliers rather than the bulk of the data.\n\n**E. Incorrect.** This statement uses absolute terms (\"always,\" \"never\") which are inappropriate for probabilistic statistical tests. Levene's test (based on means) is sensitive to outliers and is *likely* to flag heterogeneity, but it is not guaranteed to do so. The Brown-Forsythe test (based on medians) is more robust but not immune; a sufficiently extreme outlier can still cause it to reject the null hypothesis of equal variances. Therefore, it will not \"never\" flag an outlier.\n\n**F. Correct.** Both methods are direct diagnostics for an observation's impact. As noted for option C, comparing the Welch standard error under case deletion directly quantifies influence on the denominator. Externally studentized residuals are specifically designed to identify outliers by measuring how far a point is from the rest of the data, using a standard deviation estimate that is not influenced by that point. Identifying such outliers is a primary way to find points that disproportionately impact the sample variance and, by extension, the test's denominator.\n\nTherefore, the correct statements are A, C, D, and F.", "answer": "$$\\boxed{ACDF}$$", "id": "4966276"}, {"introduction": "A key responsibility of a biostatistician is not just to analyze existing data, but to design effective and efficient studies from the start. Power analysis allows us to determine the sample sizes needed to have a high probability of detecting a clinically meaningful effect, and for Welch's test, this calculation must account for potentially unequal variances. This advanced exercise [@problem_id:4966295] puts you in the role of a study designer, tasking you with balancing statistical power, resource constraints, and the specific assumptions of the Welch's t-test framework to plan a robust clinical trial.", "problem": "A biostatistics team plans a two-arm randomized study comparing the mean reduction in fasting plasma glucose between a new diet (group $1$) and a standard diet (group $2$). From a pilot study, they anticipate population standard deviations of approximately $\\sigma_{1}=8$ and $\\sigma_{2}=12$ (in the same glucose units for both groups). They wish to design the study to detect a true mean difference of $\\Delta=\\mu_{1}-\\mu_{2}=5$ in favor of the new diet. The primary analysis will use the unequal-variance two-sample $t$ test (Welch’s $t$ test) at a two-sided type I error rate $\\alpha=0.05$. Assume that the per-participant costs are equal in both groups and that the study will be powered against a difference of exactly $\\Delta$.\n\nStarting from first principles appropriate for this context (distribution of sample means, construction of the Welch test statistic, and large-sample approximations to its distribution under alternatives), determine the minimal integer sample sizes $(n_{1},n_{2})$ that achieve power $1-\\beta=0.90$ under the Welch framework. Specifically, derive the allocation rule that minimizes total sample size for a given detection threshold, obtain a continuous solution for $(n_{1},n_{2})$ from this rule, and then verify and adjust to the smallest integers that still attain at least the target power when using the Welch test with the Satterthwaite degrees-of-freedom approximation. Report your final answer as the ordered pair $(n_{1},n_{2})$ with no units. There is no rounding by significant figures; instead, choose the smallest integers that meet the power requirement.", "solution": "The objective is to find the minimal integer sample sizes, $(n_1, n_2)$, to achieve a power of $1-\\beta = 0.90$ for a two-sided Welch's t-test at significance level $\\alpha=0.05$. The anticipated parameters are a true mean difference $\\Delta = 5$, and population standard deviations $\\sigma_1 = 8$ and $\\sigma_2 = 12$.\n\n**1. Optimal Allocation**\n\nFor a fixed total sample size $N = n_1 + n_2$, the variance of the estimated mean difference, $\\text{Var}(\\bar{X}_1 - \\bar{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$, is minimized when the samples are allocated in proportion to their standard deviations. This minimizes the standard error and maximizes power. The optimal allocation rule is:\n$$\n\\frac{n_1}{n_2} = \\frac{\\sigma_1}{\\sigma_2} = \\frac{8}{12} = \\frac{2}{3}\n$$\n\n**2. Large-Sample Power Approximation**\n\nFor an initial estimate, we use a large-sample approximation. The power of a t-test can be analyzed by approximating the non-central t-distribution with a normal distribution. For a two-sided test, the power requirement can be expressed as:\n$$\n\\frac{\\Delta}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\ge z_{\\alpha/2} + z_{\\beta}\n$$\nHere, $z_{q}$ is the upper $q$ quantile of the standard normal distribution. For $\\alpha=0.05$, $z_{\\alpha/2} = z_{0.025} \\approx 1.95996$. For a power of $0.90$, $\\beta=0.10$, and $z_{\\beta} = z_{0.10} \\approx 1.28155$. The required sum is $z_{\\alpha/2} + z_{\\beta} \\approx 3.24151$.\n\nWe substitute the optimal allocation rule $n_1 = \\frac{2}{3}n_2$ into the power formula:\n$$\n\\left( \\frac{\\sigma_1^2}{(2/3)n_2} + \\frac{\\sigma_2^2}{n_2} \\right) \\le \\left( \\frac{\\Delta}{z_{\\alpha/2} + z_{\\beta}} \\right)^2\n$$\n$$\n\\frac{1}{n_2}\\left(\\frac{3}{2}\\sigma_1^2 + \\sigma_2^2\\right) \\le \\left( \\frac{5}{3.24151} \\right)^2 \\approx 2.3734\n$$\nSolving for $n_2$:\n$$\nn_2 \\ge \\frac{\\frac{3}{2}(8^2) + 12^2}{2.3734} = \\frac{96 + 144}{2.3734} = \\frac{240}{2.3734} \\approx 101.12\n$$\nThen, $n_1 = \\frac{2}{3}n_2 \\ge \\frac{2}{3}(101.12) \\approx 67.41$. The total size is $N \\approx 168.53$. This suggests we should start searching for integer solutions with a total size of $N=169$.\n\n**3. Iterative Refinement with Satterthwaite Approximation**\n\nThe large-sample formula is an approximation. We must find the smallest integers that satisfy the power requirement using the more accurate Welch-Satterthwaite framework. The power condition is more precisely stated as $\\delta \\ge t_{\\alpha/2, \\nu} + z_{\\beta}$, where $\\delta = \\Delta/SE$ is the non-centrality parameter and $t_{\\alpha/2, \\nu}$ is the critical value from the t-distribution with Satterthwaite degrees of freedom $\\nu$.\n\nLet's test integer pairs $(n_1, n_2)$ with total size $N$ starting from $169$, keeping the allocation ratio close to $2/3$.\n\n*   **Test $N=169$:** The closest integer pair is $(n_1, n_2) = (68, 101)$, since $169 \\times (2/5) = 67.6$.\n    *   $SE = \\sqrt{8^2/68 + 12^2/101} \\approx 1.5385$\n    *   $\\delta = 5 / 1.5385 \\approx 3.2500$\n    *   $\\nu = \\frac{(8^2/68 + 12^2/101)^2}{\\frac{(8^2/68)^2}{67} + \\frac{(12^2/101)^2}{100}} \\approx 166.99$. We use $\\nu = 166$.\n    *   $t_{0.025, 166} \\approx 1.9742$\n    *   Power check: Is $\\delta \\ge t_{\\alpha/2, \\nu} + z_{\\beta}$?\n    *   $1.9742 + 1.2816 = 3.2558$. Since $3.2500  3.2558$, power is insufficient.\n\n*   **Test $N=170$:** The optimal allocation is exactly $(n_1, n_2) = (68, 102)$.\n    *   $SE = \\sqrt{8^2/68 + 12^2/102} = \\sqrt{16/17 + 24/17} = \\sqrt{40/17} \\approx 1.5339$\n    *   $\\delta = 5 / 1.5339 \\approx 3.2597$\n    *   $\\nu = \\frac{(40/17)^2}{\\frac{(16/17)^2}{67} + \\frac{(24/17)^2}{101}} \\approx 167.99$. We use $\\nu=167$.\n    *   $t_{0.025, 167} \\approx 1.9741$\n    *   Power check: Is $\\delta \\ge t_{\\alpha/2, \\nu} + z_{\\beta}$?\n    *   $1.9741 + 1.2816 = 3.2557$. Since $3.2597 > 3.2557$, power is sufficient.\n\nThe pair $(68, 102)$ is the smallest integer pair that satisfies the power requirement under the specified conditions.", "answer": "$$\n\\boxed{\\begin{pmatrix} 68  102 \\end{pmatrix}}\n$$", "id": "4966295"}]}