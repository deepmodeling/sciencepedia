{"hands_on_practices": [{"introduction": "Hypothesis testing and confidence interval estimation are not separate procedures but are two sides of the same inferential coin. This exercise demonstrates the fundamental duality between them. By calculating a $95\\%$ confidence interval and performing a two-sided hypothesis test at an $\\alpha = 0.05$ significance level on the same dataset, you will see firsthand how the confidence interval provides a range of plausible values for the mean that is perfectly consistent with the test's decision to reject or fail to reject the null hypothesis [@problem_id:4934484].", "problem": "A clinical research team is studying the mean concentration of C-reactive protein in blood serum following a standardized intervention. Assume the underlying population distribution of concentrations is approximately normal with unknown variance. A simple random sample of size $n=25$ yields a sample mean $\\bar{X}=9.8$ and a sample standard deviation $s=2.5$ (both in $\\mathrm{mg}/\\mathrm{L}$). Consider the two-sided hypothesis test at significance level $\\alpha=0.05$ for $H_{0}:\\ \\mu=\\mu_{0}$ versus $H_{1}:\\ \\mu\\neq\\mu_{0}$, where $\\mu_{0}=9$ (in $\\mathrm{mg}/\\mathrm{L}$). Using first principles appropriate to the one-sample $t$-procedure, compute the $95\\%$ $t$-based confidence interval for the true mean $\\mu$, and verify whether the decision of the corresponding two-sided one-sample $t$-test at $\\alpha=0.05$ matches the inclusion or exclusion of $\\mu_{0}$ in the confidence interval. Report the two interval endpoints and encode the test decision numerically as $1$ for “reject $H_{0}$” and $0$ for “fail to reject $H_{0}$.” Round all reported numerical values to four significant figures. Express the interval endpoints in $\\mathrm{mg}/\\mathrm{L}$.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n-   Population distribution: Approximately normal.\n-   Population variance: Unknown.\n-   Sample size: $n=25$.\n-   Sample mean: $\\bar{X}=9.8$ $\\mathrm{mg}/\\mathrm{L}$.\n-   Sample standard deviation: $s=2.5$ $\\mathrm{mg}/\\mathrm{L}$.\n-   Hypothesis test: Two-sided, $H_{0}:\\ \\mu=\\mu_{0}$ versus $H_{1}:\\ \\mu\\neq\\mu_{0}$.\n-   Hypothesized mean: $\\mu_{0}=9$ $\\mathrm{mg}/\\mathrm{L}$.\n-   Significance level: $\\alpha=0.05$.\n-   Required calculation 1: $95\\%$ $t$-based confidence interval for $\\mu$.\n-   Required calculation 2: Decision for the corresponding one-sample $t$-test.\n-   Required verification: Consistency between the confidence interval and the test decision.\n-   Reporting requirements: Report two interval endpoints and a numerical test decision code ($1$ for reject, $0$ for fail to reject), with all values rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It provides a standard biostatistical exercise involving a one-sample $t$-procedure. The assumptions (approximate normality of the population) and the provided data ($n$, $\\bar{X}$, $s$) are complete and consistent for applying this procedure. The use of a $t$-test is appropriate because the population variance is unknown and the sample size is relatively small ($n  30$), although the normality assumption is the primary driver for using the $t$-distribution. The problem is a direct application of core statistical principles and is free of any invalidating flaws.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe problem requires the calculation of a confidence interval for the population mean $\\mu$ and the execution of a hypothesis test concerning $\\mu$. Since the population standard deviation is unknown and the underlying population is assumed to be normally distributed, the one-sample $t$-procedure is the correct statistical method.\n\nFirst, we compute the $95\\%$ confidence interval for the true mean concentration $\\mu$. The formula for a $(1-\\alpha) \\times 100\\%$ confidence interval for $\\mu$ is given by:\n$$ \\bar{X} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}} $$\nThe given values are:\n-   Sample mean, $\\bar{X} = 9.8$.\n-   Sample standard deviation, $s = 2.5$.\n-   Sample size, $n = 25$.\n-   Significance level, $\\alpha = 0.05$.\n\nThe confidence level is $1-\\alpha = 1 - 0.05 = 0.95$, or $95\\%$. The degrees of freedom for the $t$-distribution are $df = n-1 = 25-1 = 24$. For a two-sided interval, we need the critical value $t_{\\alpha/2, df}$, which corresponds to $t_{0.05/2, 24} = t_{0.025, 24}$. Consulting a $t$-distribution table or using statistical software, we find this critical value to be $t_{0.025, 24} \\approx 2.0639$.\n\nNext, we calculate the standard error of the mean ($SE_{\\bar{X}}$):\n$$ SE_{\\bar{X}} = \\frac{s}{\\sqrt{n}} = \\frac{2.5}{\\sqrt{25}} = \\frac{2.5}{5} = 0.5 $$\nThe margin of error ($ME$) is the product of the critical value and the standard error:\n$$ ME = t_{0.025, 24} \\times SE_{\\bar{X}} \\approx 2.0639 \\times 0.5 = 1.03195 $$\nThe confidence interval is then calculated as $\\bar{X} \\pm ME$:\n$$ 9.8 \\pm 1.03195 $$\nThis gives the interval $(9.8 - 1.03195, 9.8 + 1.03195)$, which is $(8.76805, 10.83195)$.\nRounding the endpoints to four significant figures, we obtain:\n-   Lower endpoint: $8.768$\n-   Upper endpoint: $10.83$\n\nSecond, we perform the two-sided one-sample $t$-test and verify its correspondence with the confidence interval.\nThe hypotheses are:\n-   $H_{0}: \\mu = 9$\n-   $H_{1}: \\mu \\neq 9$\n\nThe test statistic is calculated as:\n$$ t = \\frac{\\bar{X} - \\mu_{0}}{s/\\sqrt{n}} = \\frac{\\bar{X} - \\mu_{0}}{SE_{\\bar{X}}} $$\nSubstituting the given values:\n$$ t = \\frac{9.8 - 9}{0.5} = \\frac{0.8}{0.5} = 1.6 $$\nFor a two-sided test at significance level $\\alpha=0.05$ with $24$ degrees of freedom, the rejection region is $|t| > t_{0.025, 24}$. We already established that $t_{0.025, 24} \\approx 2.0639$.\nWe compare our calculated test statistic to the critical value:\n$$ |t| = |1.6| = 1.6 $$\nSince $1.6 \\ngtr 2.0639$, the test statistic does not fall into the rejection region. Therefore, we fail to reject the null hypothesis $H_{0}$. The numerical code for \"fail to reject $H_{0}$\" is $0$.\n\nFinally, we verify the correspondence between the two results. The fundamental duality principle states that a $(1-\\alpha) \\times 100\\%$ confidence interval for $\\mu$ contains all values of $\\mu_{0}$ for which the null hypothesis $H_{0}: \\mu = \\mu_{0}$ would not be rejected in a two-sided test at significance level $\\alpha$.\nOur calculated $95\\%$ confidence interval is approximately $(8.768, 10.83)$. The hypothesized mean is $\\mu_{0}=9$.\nSince $8.768  9  10.83$, the value $\\mu_{0}=9$ is contained within the confidence interval.\nThis outcome is consistent with our hypothesis test decision to \"fail to reject $H_{0}$\". The verification is successful.\n\nThe requested numerical results are the lower and upper endpoints of the confidence interval and the test decision code, all rounded to four significant figures where applicable.\n-   Lower endpoint: $8.768$ $\\mathrm{mg}/\\mathrm{L}$\n-   Upper endpoint: $10.83$ $\\mathrm{mg}/\\mathrm{L}$\n-   Test decision code: $0$", "answer": "$$\n\\boxed{\\begin{pmatrix} 8.768  10.83  0 \\end{pmatrix}}\n$$", "id": "4934484"}, {"introduction": "Statistical tests are built on assumptions, and the one-sample $t$-test is no exception, relying on the assumption that the data is drawn from a normal distribution. In practice, data is rarely perfectly normal, forcing us to make a judgment call. This problem places you in a common scenario where a formal test of normality returns a significant result, and you must decide on the best analytical path forward, weighing the known robustness of the $t$-test against potential assumption violations [@problem_id:4934529].", "problem": "A clinical trial evaluates the mean change in systolic blood pressure (in millimeters of mercury) after a new dietary intervention. For $n=20$ participants, the observed sample mean change is $\\bar{x}=-6.2$ and the sample standard deviation is $s=8.5$. The primary objective is to test $H_0:\\mu=0$ versus $H_a:\\mu0$, where $\\mu$ is the population mean change. Exploratory plots indicate mild right-skew without extreme outliers. A Shapiro–Wilk normality test (Shapiro–Wilk) applied to the changes yields a $p$-value of $p=0.03$.\n\nYour task is to decide how to proceed with inference on $\\mu$ and justify the choice by explicitly appealing to foundational principles such as the sampling distribution of the mean, the role of distributional assumptions in the one-sample $t$ test, and the interpretation of a normality test $p$-value. Choose the single best option.\n\nA. Proceed with the one-sample $t$ test for $\\mu$ at the prespecified significance level, noting its robustness to mild non-normality at $n=20$ in the absence of extreme outliers and complementing with a sensitivity analysis (e.g., Wilcoxon signed-rank test) to confirm conclusions.\n\nB. Abandon the $t$ test and use a one-sample $z$ test because $p=0.03$ invalidates the $t$ test, and the $z$ test does not require distributional assumptions.\n\nC. Do not use any parametric method whenever $p0.05$ in a normality test; use only the Wilcoxon signed-rank test because a rejected normality null automatically makes the $t$ test inappropriate.\n\nD. Increase the significance level to $\\alpha=0.10$ to compensate for non-normality detected by the Shapiro–Wilk test, which makes the test more conservative.\n\nE. Transform the data until exact normality is achieved and then apply the one-sample $t$ test; if exact normality cannot be achieved, inference on $\\mu$ is not possible.", "solution": "**Validation**\n- **Givens**:\n    - Sample size: $n=20$\n    - Sample mean change: $\\bar{x}=-6.2$ mmHg\n    - Sample standard deviation of change: $s=8.5$ mmHg\n    - Hypothesis test: $H_0: \\mu=0$ vs. $H_a: \\mu0$\n    - Data characteristics: Mild right-skew, no extreme outliers.\n    - Normality test result: Shapiro–Wilk test gives $p=0.03$.\n- **Verdict**: The problem is valid. It presents a common and realistic scenario in applied statistics where a formal assumption test conflicts with the known robustness of a standard procedure, requiring a reasoned judgment.\n\n**Solution Derivation**\n\nThe core of the problem is to decide on the appropriate statistical procedure when there is evidence of non-normality in the data, but the deviation is described as \"mild\".\n\n1.  **The Role of the Normality Assumption**: The one-sample $t$-test formally assumes that the data are sampled from a normal distribution. This assumption ensures that the test statistic $T = (\\bar{x} - \\mu_0) / (s/\\sqrt{n})$ follows an exact Student's $t$-distribution under the null hypothesis.\n\n2.  **The Central Limit Theorem (CLT) and Robustness**: The CLT states that the *sampling distribution of the sample mean* ($\\bar{x}$) approaches a normal distribution as the sample size $n$ increases, regardless of the underlying data's distribution (as long as it has a finite variance). Because the $t$-test is based on the sample mean, it is **robust** to violations of the normality assumption. This robustness increases with sample size. For $n=20$, the test is considered reasonably robust, especially when the non-normality is mild (i.e., slight skewness and no heavy tails or extreme outliers).\n\n3.  **Interpreting the Shapiro–Wilk Test**: A significant result ($p=0.03  0.05$) from the Shapiro–Wilk test provides formal evidence to reject the null hypothesis of normality. However, with small to moderate sample sizes, normality tests can be overly sensitive to minor, inconsequential deviations from perfect normality. A rigid rule to abandon the $t$-test whenever a normality test is significant is often not the best practice. The visual assessment (\"mild right-skew without extreme outliers\") is crucial context.\n\n**Analysis of Options**\n\n*   **A. Proceed with the one-sample $t$ test for $\\mu$ at the prespecified significance level, noting its robustness to mild non-normality at $n=20$ in the absence of extreme outliers and complementing with a sensitivity analysis (e.g., Wilcoxon signed-rank test) to confirm conclusions.**: This is the most sound and pragmatic approach. It correctly acknowledges the $t$-test's robustness in this context ($n=20$, mild skew). Using the $t$-test as the primary analysis is justified. Performing a non-parametric test like the Wilcoxon signed-rank test as a sensitivity analysis is excellent practice. If both tests lead to the same conclusion, it strengthens the finding. If they differ, it warrants further investigation.\n\n*   **B. Abandon the $t$ test and use a one-sample $z$ test...**: This is incorrect. A $z$-test requires the population standard deviation $\\sigma$ to be known. Here, we only have the sample standard deviation $s=8.5$. Using a $z$-test would be a fundamental error.\n\n*   **C. Do not use any parametric method whenever $p0.05$ in a normality test...**: This is an overly rigid and often suboptimal rule. It ignores the well-established robustness of the $t$-test. While the Wilcoxon signed-rank test is a valid alternative, the $t$-test is generally more powerful if its assumptions are approximately met.\n\n*   **D. Increase the significance level to $\\alpha=0.10$ to compensate...**: This is incorrect. Changing the significance level $\\alpha$ does not \"compensate\" for an assumption violation. Increasing $\\alpha$ simply increases the probability of a Type I error, making the test less conservative, not more.\n\n*   **E. Transform the data until exact normality is achieved...**: This is not a good strategy. It may not be possible to find a transformation that achieves \"exact normality.\" More importantly, transforming the data changes the hypothesis being tested (e.g., a test on $\\log(x)$ is a test of the geometric mean, not the arithmetic mean $\\mu$). Declaring inference \"not possible\" is an unjustified and overly pessimistic conclusion.\n\nTherefore, option A represents the best course of action, reflecting a nuanced understanding of statistical practice.", "answer": "$$\\boxed{A}$$", "id": "4934529"}, {"introduction": "The sample mean and standard deviation, the core components of the $t$-statistic, are highly sensitive to extreme values, or outliers. A single erroneous data point can dramatically inflate the sample variance and shift the sample mean, potentially leading to incorrect conclusions. This practice provides a striking, quantitative demonstration of this vulnerability by comparing an analysis with and without an outlier, and introduces a robust statistical method—winsorization—as a practical tool to mitigate the influence of such extreme observations [@problem_id:4934504].", "problem": "A clinical study monitors fasting blood glucose concentrations (mg/dL) in a cohort of healthy adults to assess whether the population mean equals a reference value. Assume observations are independent and identically distributed from a continuous population, and the null hypothesis is that the population mean equals $100$ mg/dL. Consider the following sample of $10$ participants: $96$, $102$, $105$, $98$, $110$, $94$, $101$, $99$, $107$, $100$. Then suppose a single erroneous extreme measurement is inadvertently included from a miscalibrated device: $300$ mg/dL, giving an augmented sample of $11$ observations.\n\nStarting from first principles—the definitions of the sample mean, the unbiased sample variance, and the one-sample $t$ statistic under normal sampling—carry out the following:\n\n- Using the $10$-observation sample, compute the unbiased sample variance $S_{\\text{clean}}^{2}$ and the one-sample $t$ statistic for testing whether the mean equals $100$ mg/dL.\n- Using the $11$-observation sample (the $10$ original observations plus the single extreme outlier at $300$ mg/dL), compute the unbiased sample variance $S_{\\text{out}}^{2}$ and the one-sample $t$ statistic for testing whether the mean equals $100$ mg/dL.\n- Quantify the inflation of variability due to the outlier by the ratio $R = S_{\\text{out}}^{2} / S_{\\text{clean}}^{2}$.\n- For a robust comparison, construct a $20\\%$ winsorized analysis of the $11$-observation sample: replace the lowest $g$ and highest $g$ observations by the $(g+1)$-th smallest and the $(n-g)$-th largest observations respectively, where $g = \\lfloor 0.2 n \\rfloor$ and $n = 11$. Use the winsorized sample to compute a winsorized mean and a winsorized unbiased sample variance $S_{\\text{win}}^{2}$. Form a $t$-like statistic by replacing the classical mean and standard deviation with the winsorized mean and $\\sqrt{S_{\\text{win}}^{2}}$ in the usual one-sample $t$ construction. Compare, at significance level $\\alpha = 0.05$ (two-sided), whether the test decision changes between the classical outlier-inclusive analysis and the winsorized robust analysis.\n- Let $t_{\\text{out}}$ denote the classical $t$ statistic computed on the $11$-observation sample (including the outlier), and let $t_{\\text{win}}$ denote the winsorized $t$-like statistic on the $11$-observation sample. Define the scalar\n$$\nQ \\;=\\; R \\;-\\; \\left(|t_{\\text{win}}| \\;-\\; |t_{\\text{out}}|\\right).\n$$\n\nRound your final numerical value of $Q$ to four significant figures. Express the final answer as a dimensionless number.", "solution": "The problem is a valid exercise in biostatistics, requiring the calculation of standard and robust test statistics and the assessment of an outlier's impact. I will proceed with the solution by following the specified tasks.\n\nThe fundamental definitions are as follows. For a sample of observations $\\{x_1, x_2, \\dots, x_n\\}$, the sample mean $\\bar{x}$ is defined as:\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i $$\nThe unbiased sample variance $S^2$ is defined as:\n$$ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 $$\nThe one-sample $t$-statistic for testing the null hypothesis $H_0: \\mu = \\mu_0$ is given by:\n$$ t = \\frac{\\bar{x} - \\mu_0}{S / \\sqrt{n}} $$\nwhere $S = \\sqrt{S^2}$ is the sample standard deviation.\n\nThe problem provides a null hypothesis mean $\\mu_0 = 100$ mg/dL.\n\n**Analysis of the 10-observation (clean) sample**\n\nThe sample data, which we denote as the \"clean\" sample, consists of $n_{\\text{clean}} = 10$ observations:\n$\\{96, 102, 105, 98, 110, 94, 101, 99, 107, 100\\}$.\n\nFirst, we compute the sample mean, $\\bar{x}_{\\text{clean}}$:\n$$ \\sum_{i=1}^{10} x_i = 96+102+105+98+110+94+101+99+107+100 = 1012 $$\n$$ \\bar{x}_{\\text{clean}} = \\frac{1012}{10} = 101.2 $$\n\nNext, we compute the unbiased sample variance, $S_{\\text{clean}}^{2}$. The sum of squared deviations from the mean is:\n$$ \\sum_{i=1}^{10} (x_i - \\bar{x}_{\\text{clean}})^2 = (96-101.2)^2 + \\dots + (100-101.2)^2 $$\n$$ = (-5.2)^2 + (0.8)^2 + (3.8)^2 + (-3.2)^2 + (8.8)^2 + (-7.2)^2 + (-0.2)^2 + (-2.2)^2 + (5.8)^2 + (-1.2)^2 $$\n$$ = 27.04 + 0.64 + 14.44 + 10.24 + 77.44 + 51.84 + 0.04 + 4.84 + 33.64 + 1.44 = 221.6 $$\n$$ S_{\\text{clean}}^{2} = \\frac{221.6}{10-1} = \\frac{221.6}{9} \\approx 24.622 $$\n\nFinally, we compute the one-sample $t$-statistic, $t_{\\text{clean}}$:\n$$ t_{\\text{clean}} = \\frac{\\bar{x}_{\\text{clean}} - \\mu_0}{S_{\\text{clean}} / \\sqrt{n_{\\text{clean}}}} = \\frac{101.2 - 100}{\\sqrt{24.622...} / \\sqrt{10}} = \\frac{1.2}{\\sqrt{2.4622...}} \\approx 0.76472 $$\n\n**Analysis of the 11-observation (outlier) sample**\n\nThis sample consists of the original $10$ observations plus an outlier, $300$. Let's call this the \"outlier\" sample, with $n_{\\text{out}} = 11$.\nThe data are: $\\{96, 102, 105, 98, 110, 94, 101, 99, 107, 100, 300\\}$.\n\nThe sample mean, $\\bar{x}_{\\text{out}}$, is:\n$$ \\sum_{i=1}^{11} x_i = 1012 + 300 = 1312 $$\n$$ \\bar{x}_{\\text{out}} = \\frac{1312}{11} \\approx 119.27 $$\n\nFor the variance, we can compute the sum of squared deviations:\n$$ \\sum_{i=1}^{11} (x_i - \\bar{x}_{\\text{out}})^2 = \\sum_{i=1}^{11} (x_i - 119.27...)^2 \\approx 36150.18 $$\n$$ S_{\\text{out}}^{2} = \\frac{36150.18...}{11-1} \\approx 3615.018 $$\n\nThe one-sample $t$-statistic for this sample, $t_{\\text{out}}$, is:\n$$ t_{\\text{out}} = \\frac{\\bar{x}_{\\text{out}} - \\mu_0}{S_{\\text{out}} / \\sqrt{n_{\\text{out}}}} = \\frac{119.27... - 100}{\\sqrt{3615.018...} / \\sqrt{11}} = \\frac{19.27...}{\\sqrt{328.638...}} \\approx 1.0631 $$\n\n**Quantifying Variability Inflation**\n\nThe ratio $R$ is defined as $R = S_{\\text{out}}^{2} / S_{\\text{clean}}^{2}$:\n$$ R = \\frac{3615.018...}{24.622...} \\approx 146.82 $$\n\n**Robust Winsorized Analysis**\n\nFor the $11$-observation sample, we perform a $20\\%$ winsorization. The number of observations to be replaced at each end is $g = \\lfloor 0.2 \\times n \\rfloor = \\lfloor 0.2 \\times 11 \\rfloor = \\lfloor 2.2 \\rfloor = 2$.\nFirst, we sort the outlier sample:\n$$ \\{94, 96, 98, 99, 100, 101, 102, 105, 107, 110, 300\\} $$\nThe values to be replaced are the $g=2$ lowest ($94, 96$) and $g=2$ highest ($110, 300$). The lowest two are replaced by the $(g+1)$-th smallest value, which is $x_{(3)} = 98$. The highest two are replaced by the $(n-g)$-th value, which is $x_{(11-2)} = x_{(9)} = 107$.\nThe winsorized sample, $x_{\\text{win}}$, is:\n$$ \\{98, 98, 98, 99, 100, 101, 102, 105, 107, 107, 107\\} $$\nThe winsorized mean, $\\bar{x}_{\\text{win}}$, is:\n$$ \\sum x_{i, \\text{win}} = 3(98) + 99 + 100 + 101 + 102 + 105 + 3(107) = 294 + 607 + 321 = 1122 $$\n$$ \\bar{x}_{\\text{win}} = \\frac{1122}{11} = 102 $$\nThe winsorized unbiased sample variance, $S_{\\text{win}}^{2}$, is the variance of this new sample:\n$$ \\sum (x_{i, \\text{win}} - \\bar{x}_{\\text{win}})^2 = 3(98-102)^2 + (99-102)^2 + (100-102)^2 + (101-102)^2 + (102-102)^2 + (105-102)^2 + 3(107-102)^2 $$\n$$ = 3(-4)^2 + (-3)^2 + (-2)^2 + (-1)^2 + 0^2 + 3^2 + 3(5)^2 = 3(16)+9+4+1+0+9+3(25) = 48+9+4+1+0+9+75 = 146 $$\n$$ S_{\\text{win}}^{2} = \\frac{146}{11-1} = 14.6 $$\nThe winsorized $t$-like statistic, $t_{\\text{win}}$, is:\n$$ t_{\\text{win}} = \\frac{\\bar{x}_{\\text{win}} - \\mu_0}{\\sqrt{S_{\\text{win}}^{2}} / \\sqrt{n}} = \\frac{102 - 100}{\\sqrt{14.6} / \\sqrt{11}} = \\frac{2}{\\sqrt{14.6/11}} \\approx 1.7360 $$\n\nTo compare test decisions at $\\alpha = 0.05$ (two-sided):\nFor the outlier-inclusive test, we use $t_{\\text{out}} \\approx 1.0631$ with $df_{\\text{out}} = n-1 = 10$. The critical value is $t_{0.025, 10} = 2.228$. Since $|1.0631|  2.228$, we fail to reject $H_0$.\nFor the winsorized test, a common practice is to use degrees of freedom $df_{\\text{win}} = n - 2g - 1 = 11 - 2(2) - 1 = 6$. The critical value is $t_{0.025, 6} = 2.447$. Since $|t_{\\text{win}}| \\approx 1.7360  2.447$, we also fail to reject $H_0$. The test decision does not change.\n\n**Final Calculation of $Q$**\n\nThe problem asks for the value of $Q = R - (|t_{\\text{win}}| - |t_{\\text{out}}|)$.\nUsing the computed values:\n$R \\approx 146.8193$\n$|t_{\\text{win}}| \\approx 1.7360$\n$|t_{\\text{out}}| \\approx 1.0631$\n$$ Q \\approx 146.8193 - (1.7360 - 1.0631) = 146.8193 - 0.6729 = 146.1464 $$\nRounding to four significant figures, we get $Q = 146.1$.", "answer": "$$\\boxed{146.1}$$", "id": "4934504"}]}