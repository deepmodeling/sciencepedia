{"hands_on_practices": [{"introduction": "Why choose a paired design over a simpler independent two-sample design? This exercise demonstrates the superior statistical efficiency of paired designs, showing how within-subject correlation reduces variability and thus the required sample size. By deriving the ratio of samples needed for each design [@problem_id:4936040], you will quantify the powerful advantage of pairing measurements.", "problem": "A biostatistician is planning a hypothesis test for a mean difference between two conditions, denoted by the parameter $\\Delta = \\mu_{1} - \\mu_{2}$, where $\\mu_{1}$ and $\\mu_{2}$ are the population means under the two conditions. The biostatistician can choose between:\n\n1. A paired design, where each of $n_{p}$ individuals is measured under both conditions, producing paired measurements $(X_{i}, Y_{i})$ on the same individual $i$.\n\n2. An independent two-sample design, where two groups of equal size $n_{i}$ are formed; the first group is measured under condition $1$ and the second group under condition $2$, producing independent samples $\\{X_{j}\\}$ and $\\{Y_{k}\\}$.\n\nAssume the following scientifically plausible conditions hold:\n\n- The marginal variances are equal, $\\sigma_{1}^{2} = \\sigma_{2}^{2} = \\sigma^{2}$.\n- In the paired design, the within-subject correlation between $X_{i}$ and $Y_{i}$ is $\\rho$.\n- In the independent design, samples across groups are independent.\n- A two-sided test at fixed type I error level $\\alpha$ is used, and for large samples the power is governed by the standardized effect size $\\Delta$ divided by the standard error of the estimator for $\\mu_{1} - \\mu_{2}$, consistent with the Central Limit Theorem (CLT).\n\nStarting from first principles—specifically, the properties of variances and covariances for linear combinations, and the sampling variance of sample means—derive an expression for the ratio of the total number of individuals required in the independent design to the number of individuals required in the paired design to achieve the same power. Then, compute this ratio when $\\rho = 0.5$. Express the final ratio as a pure number with no units. No rounding is required; provide the exact value.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains a complete and consistent set of formalizable conditions. We may therefore proceed with a solution.\n\nThe core principle for achieving the same statistical power for a hypothesis test on the mean difference $\\Delta = \\mu_1 - \\mu_2$, given a fixed Type I error rate $\\alpha$, is that the variance of the estimator of $\\Delta$ must be the same for both the paired and independent study designs. This is because power in a test based on the Central Limit Theorem is a function of the non-centrality parameter, which is proportional to $|\\Delta| / \\text{SE}(\\hat{\\Delta})$, where $\\text{SE}(\\hat{\\Delta}) = \\sqrt{\\text{Var}(\\hat{\\Delta})}$. Thus, for the same power, we must have $\\text{Var}(\\hat{\\Delta}_{\\text{paired}}) = \\text{Var}(\\hat{\\Delta}_{\\text{independent}})$.\n\nWe begin by deriving the variance of the estimator for the mean difference in each design, starting from first principles of variance for linear combinations of random variables and the variance of a sample mean.\n\nFirst, consider the paired design.\nIn this design, we have $n_p$ individuals, with paired measurements $(X_i, Y_i)$ for each individual $i$. The difference for each individual is a random variable $D_i = X_i - Y_i$. The estimator for the mean difference $\\Delta = \\mu_1 - \\mu_2$ is the sample mean of these differences:\n$$ \\hat{\\Delta}_{\\text{paired}} = \\bar{D} = \\frac{1}{n_p} \\sum_{i=1}^{n_p} D_i $$\nThe variance of this estimator is the variance of the sample mean:\n$$ \\text{Var}(\\hat{\\Delta}_{\\text{paired}}) = \\text{Var}(\\bar{D}) = \\frac{\\text{Var}(D_i)}{n_p} $$\nTo find $\\text{Var}(D_i)$, we use the property for the variance of a difference of two random variables:\n$$ \\text{Var}(D_i) = \\text{Var}(X_i - Y_i) = \\text{Var}(X_i) + \\text{Var}(Y_i) - 2\\text{Cov}(X_i, Y_i) $$\nWe are given that the marginal variances are equal, $\\text{Var}(X_i) = \\sigma^2$ and $\\text{Var}(Y_i) = \\sigma^2$. The covariance is related to the correlation coefficient $\\rho$ by the definition $\\rho = \\frac{\\text{Cov}(X_i, Y_i)}{\\sqrt{\\text{Var}(X_i)\\text{Var}(Y_i)}}$. This gives $\\text{Cov}(X_i, Y_i) = \\rho \\sqrt{\\sigma^2 \\cdot \\sigma^2} = \\rho\\sigma^2$.\nSubstituting these into the expression for $\\text{Var}(D_i)$:\n$$ \\text{Var}(D_i) = \\sigma^2 + \\sigma^2 - 2\\rho\\sigma^2 = 2\\sigma^2(1 - \\rho) $$\nTherefore, the variance of the estimator in the paired design is:\n$$ \\text{Var}(\\hat{\\Delta}_{\\text{paired}}) = \\frac{2\\sigma^2(1 - \\rho)}{n_p} $$\n\nNext, consider the independent two-sample design.\nIn this design, we have two independent groups of size $n_i$ each. The estimator for the mean difference $\\Delta$ is the difference between the two sample means:\n$$ \\hat{\\Delta}_{\\text{independent}} = \\bar{X} - \\bar{Y} $$\nwhere $\\bar{X} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} X_j$ and $\\bar{Y} = \\frac{1}{n_i} \\sum_{k=1}^{n_i} Y_k$.\nThe variance of this estimator is found using the property for the variance of a difference of independent random variables:\n$$ \\text{Var}(\\hat{\\Delta}_{\\text{independent}}) = \\text{Var}(\\bar{X} - \\bar{Y}) = \\text{Var}(\\bar{X}) + \\text{Var}(\\bar{Y}) $$\nThe variance of each sample mean is given by:\n$$ \\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n_i} = \\frac{\\sigma^2}{n_i} $$\n$$ \\text{Var}(\\bar{Y}) = \\frac{\\text{Var}(Y)}{n_i} = \\frac{\\sigma^2}{n_i} $$\nSubstituting these into the expression for $\\text{Var}(\\hat{\\Delta}_{\\text{independent}})$:\n$$ \\text{Var}(\\hat{\\Delta}_{\\text{independent}}) = \\frac{\\sigma^2}{n_i} + \\frac{\\sigma^2}{n_i} = \\frac{2\\sigma^2}{n_i} $$\n\nTo achieve the same power, we equate the variances of the estimators from the two designs:\n$$ \\text{Var}(\\hat{\\Delta}_{\\text{paired}}) = \\text{Var}(\\hat{\\Delta}_{\\text{independent}}) $$\n$$ \\frac{2\\sigma^2(1 - \\rho)}{n_p} = \\frac{2\\sigma^2}{n_i} $$\nAssuming $\\sigma^2 > 0$, we can cancel the term $2\\sigma^2$ from both sides, yielding:\n$$ \\frac{1 - \\rho}{n_p} = \\frac{1}{n_i} $$\nThis gives a relationship between the sample sizes $n_i$ and $n_p$:\n$$ n_i = \\frac{n_p}{1 - \\rho} $$\nThe problem asks for the ratio of the total number of individuals required.\nFor the paired design, the total number of individuals is $N_{\\text{paired}} = n_p$.\nFor the independent design, the total number of individuals is the sum of the individuals in both groups, $N_{\\text{indep}} = n_i + n_i = 2n_i$.\n\nThe desired ratio is $\\frac{N_{\\text{indep}}}{N_{\\text{paired}}}$:\n$$ \\frac{N_{\\text{indep}}}{N_{\\text{paired}}} = \\frac{2n_i}{n_p} $$\nSubstituting the expression for $n_i$ in terms of $n_p$:\n$$ \\frac{N_{\\text{indep}}}{N_{\\text{paired}}} = \\frac{2 \\left(\\frac{n_p}{1 - \\rho}\\right)}{n_p} $$\nCanceling $n_p$ (since $n_p > 0$ for a study to exist), we obtain the general expression for the ratio:\n$$ \\frac{N_{\\text{indep}}}{N_{\\text{paired}}} = \\frac{2}{1 - \\rho} $$\nFinally, we compute this ratio for the specific case where the within-subject correlation is $\\rho = 0.5$:\n$$ \\text{Ratio} = \\frac{2}{1 - 0.5} = \\frac{2}{0.5} = 4 $$\nThus, to achieve the same statistical power, the independent design requires $4$ times the total number of individuals as the paired design when the correlation between paired measurements is $0.5$.", "answer": "$$\\boxed{4}$$", "id": "4936040"}, {"introduction": "Once you have decided on a paired design, the next critical step in planning a study is determining the necessary sample size. This practice will guide you through a full sample size calculation to ensure your study has adequate power to detect a scientifically meaningful effect [@problem_id:4936035]. Mastering this process is a core skill for designing experiments that are both efficient and likely to yield conclusive results.", "problem": "A crossover study in advanced undergraduate biostatistics is planned to evaluate the effect of a short-term dietary intervention on a biomarker measured on a continuous scale. Each participant is measured twice: once at baseline and once after the intervention, yielding a paired difference for participant $i$ denoted by $D_i = \\text{post}_i - \\text{pre}_i$. Assume the paired differences $\\{D_i\\}_{i=1}^{n}$ are independently and identically distributed and follow a normal distribution with mean $\\mu_D$ and standard deviation $\\sigma_D$. Investigators will perform a two-sided paired Student’s $t$ test of $H_0:\\mu_D=0$ versus $H_1:\\mu_D\\neq 0$ at significance level $\\alpha=0.05$. They wish to design the study to have power $0.80$ to detect a true mean difference $\\mu_D=0.5$ when the standard deviation of differences is $\\sigma_D=1.2$.\n\nUsing the framework in which the test statistic for the paired $t$ procedure has a noncentral Student’s $t$ distribution under the alternative, compute the smallest integer sample size $n$ (number of paired observations) required to achieve the target power. Your derivation must start from core definitions of the paired $t$ procedure and the noncentrality parameter, and should justify the approximation strategy you use to obtain $n$ without relying on software for exact noncentral $t$ quantiles. Provide the minimal integer $n$ that meets or exceeds the target power. No rounding instruction is needed beyond reporting an integer.", "solution": "The problem requires the calculation of the minimum sample size $n$ for a two-sided, one-sample Student's $t$-test on paired differences to achieve a specified power.\n\nFirst, we formalize the statistical framework. The paired differences $D_i = \\text{post}_i - \\text{pre}_i$ are assumed to be independent and identically distributed (i.i.d.) from a normal distribution, $D_i \\sim N(\\mu_D, \\sigma_D^2)$. We are given the true standard deviation of the differences, $\\sigma_D = 1.2$.\n\nThe null and alternative hypotheses for the two-sided test are:\n$$H_0: \\mu_D = 0$$\n$$H_1: \\mu_D \\neq 0$$\n\nThe test statistic for the paired $t$-test is given by:\n$$T = \\frac{\\bar{D} - 0}{S_D / \\sqrt{n}}$$\nwhere $\\bar{D}$ is the sample mean of the $n$ differences, and $S_D$ is the sample standard deviation of the differences.\n\nUnder the null hypothesis $H_0$, the statistic $T$ follows a central Student's $t$-distribution with $\\nu = n-1$ degrees of freedom, denoted $T \\sim t_{n-1}$. The test is conducted at a significance level of $\\alpha = 0.05$. We reject $H_0$ if the observed value of the test statistic, $|t|$, is greater than the critical value $t_{1-\\alpha/2, n-1}$.\n\nUnder the alternative hypothesis $H_1$, where the true mean difference is $\\mu_D \\neq 0$, the test statistic $T$ follows a noncentral Student's $t$-distribution with $\\nu = n-1$ degrees of freedom and a noncentrality parameter $\\delta$. The noncentrality parameter is defined as:\n$$\\delta = \\frac{\\mu_D}{\\sigma_D / \\sqrt{n}} = \\frac{\\mu_D \\sqrt{n}}{\\sigma_D}$$\nThe problem specifies the parameters for the power calculation: a true mean difference $\\mu_D = 0.5$ to be detected, with $\\sigma_D = 1.2$, for a desired power of $1-\\beta = 0.80$.\n\nThe power of the test is the probability of correctly rejecting $H_0$ when $H_1$ is true.\n$$\\text{Power} = P(|T| > t_{1-\\alpha/2, n-1} \\mid H_1)$$\nGiven that $T \\sim t_{n-1, \\delta}$ under $H_1$, this can be written as:\n$$\\text{Power} = P(T > t_{1-\\alpha/2, n-1}) + P(T  -t_{1-\\alpha/2, n-1})$$\n\nThe problem requires a solution that does not rely on software for the exact noncentral $t$-distribution. We therefore use a well-established approximation. For a non-trivial number of degrees of freedom, the noncentral $t$-distribution $t_{\\nu, \\delta}$ can be approximated by a normal distribution with mean $\\delta$ and variance $1$, i.e., $N(\\delta, 1)$.\n\nApplying this approximation, and letting $Z$ be a standard normal random variable, $Z \\sim N(0,1)$, we have $T \\approx \\delta + Z$. The power expression becomes:\n$$\\text{Power} \\approx P(\\delta + Z > t_{1-\\alpha/2, n-1}) + P(\\delta + Z  -t_{1-\\alpha/2, n-1})$$\n$$\\text{Power} \\approx P(Z > t_{1-\\alpha/2, n-1} - \\delta) + P(Z  -t_{1-\\alpha/2, n-1} - \\delta)$$\nSince we are evaluating power for $\\mu_D = 0.5 > 0$, the noncentrality parameter $\\delta$ is positive. The term $t_{1-\\alpha/2, n-1}$ is also positive. Thus, the argument of the second term, $-t_{1-\\alpha/2, n-1} - \\delta$, is a large negative number. The probability $P(Z  -t_{1-\\alpha/2, n-1} - \\delta)$ is therefore negligible and can be omitted from the power calculation. This yields the simplified approximation:\n$$\\text{Power} \\approx P(Z > t_{1-\\alpha/2, n-1} - \\delta)$$\nWe require the power to be $1-\\beta = 0.80$. Let $z_{1-\\beta}$ be the quantile of the standard normal distribution such that $P(Z  z_{1-\\beta}) = 1-\\beta$, which is equivalent to $P(Z > -z_{1-\\beta}) = 1-\\beta$. Comparing this with our power approximation, we must have:\n$$t_{1-\\alpha/2, n-1} - \\delta \\approx -z_{1-\\beta}$$\nRearranging for the noncentrality parameter $\\delta$:\n$$\\delta \\approx t_{1-\\alpha/2, n-1} + z_{1-\\beta}$$\nSubstituting the expression for $\\delta$:\n$$\\frac{\\mu_D \\sqrt{n}}{\\sigma_D} \\approx t_{1-\\alpha/2, n-1} + z_{1-\\beta}$$\nWe can now solve for $n$:\n$$n \\approx \\left( \\frac{\\sigma_D (\\,t_{1-\\alpha/2, n-1} + z_{1-\\beta})}{\\mu_D} \\right)^2$$\nThis equation cannot be solved directly for $n$ because $n$ appears on both sides (explicitly on the left, and within the degrees of freedom of the $t$-quantile on the right). We must employ an iterative approach.\n\nFirst, we approximate the $t$-quantile with the corresponding normal quantile, $t_{1-\\alpha/2, n-1} \\approx z_{1-\\alpha/2}$, to obtain an initial estimate for $n$.\nGiven values are:\n$\\alpha = 0.05 \\implies 1-\\alpha/2 = 0.975 \\implies z_{0.975} \\approx 1.9600$\n$1-\\beta = 0.80 \\implies z_{0.80} \\approx 0.8416$\n$\\mu_D = 0.5$\n$\\sigma_D = 1.2$\n\nInitial estimate for $n$:\n$$n_0 = \\left( \\frac{1.2 (1.9600 + 0.8416)}{0.5} \\right)^2 = \\left( \\frac{1.2 \\times 2.8016}{0.5} \\right)^2 = (2.4 \\times 2.8016)^2 \\approx (6.7238)^2 \\approx 45.21$$\nSince the sample size must be an integer, our first guess should be $n=46$.\n\nNow, we iterate to refine this estimate. We seek the smallest integer $n$ satisfying the inequality:\n$$n \\geq \\left( \\frac{\\sigma_D (\\,t_{1-\\alpha/2, n-1} + z_{1-\\beta})}{\\mu_D} \\right)^2$$\n\n**Iteration 1:** Let's test if $n=46$ is sufficient.\nFor $n=46$, the degrees of freedom are $\\nu = 45$. The required $t$-quantile is $t_{0.975, 45} \\approx 2.0141$.\nWe compute the right-hand side (RHS) of the inequality:\n$$\\text{RHS} = \\left( \\frac{1.2 (2.0141 + 0.8416)}{0.5} \\right)^2 = \\left( \\frac{1.2 \\times 2.8557}{0.5} \\right)^2 = (2.4 \\times 2.8557)^2 \\approx (6.8537)^2 \\approx 46.97$$\nThe inequality is $46 \\geq 46.97$, which is false. Therefore, a sample size of $n=46$ is insufficient.\n\n**Iteration 2:** Let's test if $n=47$ is sufficient.\nFor $n=47$, the degrees of freedom are $\\nu = 46$. The required $t$-quantile is $t_{0.975, 46} \\approx 2.0129$.\nWe compute the RHS of the inequality:\n$$\\text{RHS} = \\left( \\frac{1.2 (2.0129 + 0.8416)}{0.5} \\right)^2 = \\left( \\frac{1.2 \\times 2.8545}{0.5} \\right)^2 = (2.4 \\times 2.8545)^2 \\approx (6.8508)^2 \\approx 46.93$$\nThe inequality is $47 \\geq 46.93$, which is true. Therefore, a sample size of $n=47$ is sufficient to achieve the desired power of $0.80$.\n\nSince $n=46$ is insufficient and $n=47$ is sufficient, the smallest integer sample size required is $47$.", "answer": "$$\\boxed{47}$$", "id": "4936035"}, {"introduction": "The mathematical validity of the paired $t$-test relies on the assumption that the paired differences are approximately normally distributed. This final practice simulates a realistic research scenario, guiding you to develop a robust protocol for checking this critical assumption [@problem_id:4936017]. You will learn to synthesize information from formal tests and graphical plots to decide whether the $t$-test is appropriate or if a nonparametric alternative is needed, reinforcing the importance of due diligence in statistical analysis.", "problem": "A researcher plans a paired analysis of pre-intervention and post-intervention systolic blood pressure in a cohort of $n=28$ participants. For each participant $i$, the researcher computes the paired differences $D_i=X_{i,\\text{post}}-X_{i,\\text{pre}}$. The inferential goal is to test whether the population mean of the paired differences is zero using a paired $t$ procedure, but the researcher first wants to assess the normality of the $D_i$ to justify using the paired $t$ procedure.\n\nThe researcher adopts the following diagnostic tools and obtains the following outputs on the sample of differences: a Shapiro–Wilk test statistic of $W=0.958$ and a corresponding $p$-value of $p=0.12$ at a planned significance level of $\\alpha=0.05$, and a quantile–quantile (QQ) plot (quantile–quantile) comparing the sample quantiles of the $D_i$ to the theoretical quantiles under a normal distribution that appears approximately linear with mild tail deviations and no extreme outliers.\n\nStarting from the following fundamental bases:\n- The definition of the paired difference $D_i$ and that the paired $t$ procedure assumes the $D_i$ are independent and drawn from a population that is approximately normal.\n- The Shapiro–Wilk test for normality has null hypothesis that the sample is drawn from a normal distribution, with interpretation of the $p$-value as the probability, under the null, of observing a test statistic as extreme or more extreme than the one observed.\n- The concept of the QQ plot as a graphical tool to assess how closely sample quantiles align with theoretical normal quantiles.\n- The robustness of the paired $t$ procedure to mild deviations from normality, particularly for symmetric distributions and moderate sample sizes, and the availability of the Wilcoxon signed-rank test as a nonparametric alternative when normality is not tenable.\n\nWhich of the following protocols correctly specifies what to test for normality, how to combine Shapiro–Wilk and QQ plot interpretation, and how to state decision rules for whether to proceed with the paired $t$ procedure on the mean difference, including appropriate alternatives if assumptions are violated?\n\nA. Compute $D_i$ for all pairs; apply the Shapiro–Wilk test to $\\{D_i\\}$ at $\\alpha=0.05$; inspect the QQ plot of $\\{D_i\\}$ against theoretical normal quantiles. If $p\\ge\\alpha$ and the QQ plot is approximately linear without severe curvature or outliers, proceed with the paired $t$ procedure on the mean of $D_i$. If $p\\alpha$ or the QQ plot shows marked S-shaped curvature, heavy tails, or outliers suggesting clear non-normality, prefer the Wilcoxon signed-rank test on $D_i$. If $p$ is borderline and $n$ is moderate (for example, $n\\approx 25$ to $n\\approx 30$) with a QQ plot showing only mild tail deviations and approximate symmetry, proceed with the paired $t$ procedure acknowledging its robustness and consider a sensitivity analysis using the Wilcoxon signed-rank test.\n\nB. Test normality separately on the pre-intervention values $\\{X_{i,\\text{pre}}\\}$ and post-intervention values $\\{X_{i,\\text{post}}\\}$ using the Shapiro–Wilk test at $\\alpha=0.05$. If both are normal, proceed with the paired $t$ procedure; if either is non-normal, apply a logarithmic transformation to both series to force normality, then proceed with the paired $t$ procedure regardless of the QQ plot of $D_i$.\n\nC. Compute $D_i$ and apply the Shapiro–Wilk test to $\\{D_i\\}$ at $\\alpha=0.05$. If $p\\alpha$, interpret this as evidence that the $D_i$ are normal and proceed with the paired $t$ procedure; if $p\\ge\\alpha$, interpret this as non-normality and use the Mann–Whitney $U$ test on unpaired observations.\n\nD. Ignore the Shapiro–Wilk test and rely solely on the QQ plot. If the QQ plot of $\\{D_i\\}$ shows any deviation from a straight line, do not use the paired $t$ procedure and instead always use the Wilcoxon signed-rank test; if the QQ plot is perfectly straight, use the paired $t$ procedure. Do not consider $p$-values or sample size because graphical judgment suffices.\n\nE. Since $n=28$, invoke the Central Limit Theorem (CLT) to conclude that the $D_i$ themselves are normal regardless of the underlying distribution. Proceed with the paired $t$ procedure without any normality checks; only if $n25$ should one consider the Wilcoxon signed-rank test based on a histogram of $D_i$.", "solution": "The problem requires selecting the correct protocol for assessing the normality assumption of a paired $t$-test. A correct protocol must be based on established statistical principles.\n\n1.  **Identify the Correct Data for Assessment:** A paired $t$-test is equivalent to a one-sample $t$-test performed on the set of paired differences, $\\{D_i\\}$. Therefore, the normality assumption applies to these differences, not the original pre- and post-intervention measurements. Any diagnostic check must be performed on the sample of differences, $\\{D_i\\}$.\n\n2.  **Combine Formal and Graphical Methods:** Best practice in assumption checking involves synthesizing information from both formal hypothesis tests and graphical plots.\n    *   **Formal Test (Shapiro–Wilk):** This provides an objective measure. The null hypothesis ($H_0$) is that the data are from a normal distribution. A small $p$-value (e.g., $p  \\alpha$) suggests rejecting $H_0$, indicating evidence *against* normality. A large $p$-value ($p \\ge \\alpha$) means we fail to reject $H_0$, indicating a lack of strong evidence against normality. Here, $p=0.12 > 0.05$, so the Shapiro-Wilk test does not find significant evidence of non-normality.\n    *   **Graphical Plot (QQ Plot):** This provides visual insight into the *nature* of the distribution (e.g., skewness, heavy tails, outliers). The description \"approximately linear with mild tail deviations and no extreme outliers\" corroborates the formal test result, suggesting the data are reasonably close to normal.\n\n3.  **Consider Robustness and Sample Size:** The $t$-test is known to be robust to mild deviations from normality, especially for symmetric distributions and as the sample size increases. With a moderate sample size of $n=28$ and no indication of severe skewness or outliers, the $t$-test is likely to perform well. A good protocol acknowledges this robustness.\n\n4.  **Identify the Correct Alternative Test:** If the normality assumption is clearly violated, the appropriate nonparametric alternative for paired data is the **Wilcoxon signed-rank test**. The Mann-Whitney U test is for independent samples and would be incorrect.\n\n**Evaluation of the Options:**\n\n*   **A:** This protocol is correct. It tests the differences $\\{D_i\\}$, combines the Shapiro-Wilk test and QQ plot with correct interpretations, identifies the correct alternative (Wilcoxon signed-rank test), and includes a nuanced rule for borderline cases that accounts for the $t$-test's robustness. This represents statistical best practice.\n\n*   **B:** This protocol is incorrect. It tests the wrong distributions (pre- and post-intervention values instead of the differences) and proposes an inappropriate, automatic transformation strategy.\n\n*   **C:** This protocol is incorrect. It completely misinterprets the meaning of the $p$-value from the normality test ($p\\alpha$ is evidence *against* normality, not for it) and suggests the wrong nonparametric alternative (Mann-Whitney U test).\n\n*   **D:** This protocol is incorrect. It relies solely on graphical judgment and sets an impossibly high standard (\"perfectly straight\" QQ plot) while ignoring important context like sample size and the robustness of the $t$-test.\n\n*   **E:** This protocol is incorrect. It is based on a fundamental misinterpretation of the Central Limit Theorem (CLT), which applies to the distribution of the sample mean, not the individual data points. While the CLT is the reason the $t$-test is robust for large samples, a sample size of $n=28$ is not typically considered large enough to waive assumption checks entirely.\n\nBased on this analysis, Protocol A is the only one that correctly specifies how to test for normality, interpret the results, and make an appropriate decision.", "answer": "$$\\boxed{A}$$", "id": "4936017"}]}