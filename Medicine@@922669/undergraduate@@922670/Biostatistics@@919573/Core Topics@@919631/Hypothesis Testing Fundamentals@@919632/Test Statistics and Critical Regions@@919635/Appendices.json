{"hands_on_practices": [{"introduction": "The foundation of rigorous hypothesis testing lies in our ability to construct a decision rule that precisely controls the rate of Type I errors. This exercise is a fundamental practice in mathematical statistics, where you will derive a test from first principles. By working through this problem, you will build the rejection rule for the common Z-test to achieve an exact size $\\alpha$ and then derive its power function, which quantifies the test's ability to correctly detect a true effect [@problem_id:4956821].", "problem": "A biostatistics research team is designing a one-sided hypothesis test for the mean concentration of a biomarker measured by a calibrated assay. For a single study arm, the observations are modeled as independent and identically distributed random variables $X_1,\\dots,X_n$ from a normal distribution with mean $\\mu$ and known variance $\\sigma^2$, written $X_i \\sim N(\\mu,\\sigma^2)$. The scientific question is whether the true mean exceeds a clinically established reference $\\mu_0$. The team adopts the hypotheses $H_0:\\mu=\\mu_0$ versus $H_1:\\mu\\mu_0$ and requires a test that has exact size $\\alpha$, where $0\\alpha1$ is prespecified.\n\nUsing only the following foundational facts:\n- If $X_1,\\dots,X_n$ are independent and identically distributed as $N(\\mu,\\sigma^2)$ with known $\\sigma^2$, then the sample mean $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n X_i$ is normally distributed with mean $\\mu$ and variance $\\sigma^2/n$.\n- For any real-valued constant $a$, if $Y\\sim N(\\mu_Y,\\sigma_Y^2)$, then $(Y-\\mu_Y)/\\sigma_Y$ has the standard normal distribution with cumulative distribution function $\\Phi$ and corresponding quantile function $\\Phi^{-1}$.\n\nDerive, from first principles, a rejection rule of exact size $\\alpha$ for testing $H_0:\\mu=\\mu_0$ versus $H_1:\\mu\\mu_0$ expressed as a threshold on $\\overline{X}$, and then derive the power function $\\pi(\\mu)$, defined as $\\pi(\\mu)=\\mathbb{P}_{\\mu}(\\text{reject }H_0)$ as a function of the true mean $\\mu$. Express your final results in terms of $n$, $\\sigma$, $\\mu_0$, $\\alpha$, $\\Phi$, and $\\Phi^{-1}$. Do not provide any numerical approximations.\n\nYour final answer must be a single composite expression that simultaneously reports the critical value on $\\overline{X}$ and the power function $\\pi(\\mu)$, written as a row matrix. No rounding is required and no units apply.", "solution": "The problem as stated is a standard, well-posed problem in introductory mathematical statistics and biostatistics. It is scientifically grounded, internally consistent, and contains all necessary information to derive the requested quantities. The problem is therefore deemed valid and a full solution is warranted.\n\nThe goal is to first derive a rejection rule of exact size $\\alpha$ for the one-sided hypothesis test $H_0: \\mu = \\mu_0$ versus $H_1: \\mu  \\mu_0$, and then to derive the power function $\\pi(\\mu)$.\n\n**Part 1: Derivation of the Rejection Rule**\n\nThe size of a test, denoted by $\\alpha$, is the probability of a Type I error. This is the probability of rejecting the null hypothesis $H_0$ when it is, in fact, true. The problem requires this size to be exactly $\\alpha$.\nA rejection rule for the one-sided alternative $H_1: \\mu  \\mu_0$ will naturally be of the form \"reject $H_0$ if the sample mean $\\overline{X}$ is large\". We formalize this by defining a rejection region $\\mathcal{R} = \\{\\overline{X} > c\\}$, where $c$ is a critical value we must determine.\n\nThe size constraint is mathematically expressed as:\n$$\n\\mathbb{P}(\\text{reject } H_0 \\mid H_0 \\text{ is true}) = \\alpha\n$$\nSubstituting our rejection rule and the condition for $H_0$ being true ($\\mu = \\mu_0$), we get:\n$$\n\\mathbb{P}_{\\mu_0}(\\overline{X} > c) = \\alpha\n$$\nThe subscript $\\mu_0$ indicates that the probability is calculated under the distribution where the true mean is $\\mu_0$.\n\nAccording to the provided foundational facts, when the true mean is $\\mu_0$, the sample mean $\\overline{X}$ is normally distributed with mean $\\mu_0$ and variance $\\sigma^2/n$. That is, $\\overline{X} \\sim N(\\mu_0, \\sigma^2/n)$.\n\nTo evaluate the probability and solve for $c$, we standardize the random variable $\\overline{X}$. Under $H_0$, the standardized statistic is:\n$$\nZ = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}}\n$$\nThis statistic $Z$ follows the standard normal distribution, $N(0,1)$. We can transform the inequality $\\overline{X} > c$ into an equivalent inequality for $Z$:\n$$\n\\overline{X} > c \\quad \\iff \\quad \\overline{X} - \\mu_0 > c - \\mu_0 \\quad \\iff \\quad \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}} > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\n$$\nThus, the size constraint becomes:\n$$\n\\mathbb{P}\\left(Z > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\\right) = \\alpha\n$$\nFor a standard normal variable $Z$ with cumulative distribution function (CDF) $\\Phi$, we know that $\\mathbb{P}(Z > k) = 1 - \\mathbb{P}(Z \\le k) = 1 - \\Phi(k)$. The value $k$ for which this probability equals $\\alpha$ is a quantile of the standard normal distribution. Specifically, it is the upper $\\alpha$-quantile, which we can denote as $z_{\\alpha}$. This value is defined by the property $\\mathbb{P}(Z \\le z_{\\alpha}) = 1-\\alpha$. Using the provided quantile function notation $\\Phi^{-1}$, this is $z_{\\alpha} = \\Phi^{-1}(1-\\alpha)$.\n\nBy equating the argument of the probability with this quantile, we get:\n$$\n\\frac{c - \\mu_0}{\\sigma/\\sqrt{n}} = \\Phi^{-1}(1-\\alpha)\n$$\nWe now solve for the critical value $c$, which is the threshold on $\\overline{X}$:\n$$\nc = \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\n$$\nThis is the first part of the answer. The rejection rule is to reject $H_0$ if a calculated sample mean $\\overline{X}$ is greater than this value $c$.\n\n**Part 2: Derivation of the Power Function**\n\nThe power of a test, $\\pi(\\mu)$, is the probability of correctly rejecting the null hypothesis when the true mean is $\\mu$, where $\\mu$ is a value specified by the alternative hypothesis (i.e., $\\mu > \\mu_0$). The power function is defined for any value of $\\mu$:\n$$\n\\pi(\\mu) = \\mathbb{P}_{\\mu}(\\text{reject } H_0)\n$$\nUsing the rejection rule derived above, this becomes:\n$$\n\\pi(\\mu) = \\mathbb{P}_{\\mu}\\left(\\overline{X} > \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\\right)\n$$\nTo evaluate this probability, we must consider the distribution of $\\overline{X}$ when the true mean is $\\mu$. From the given facts, this distribution is $\\overline{X} \\sim N(\\mu, \\sigma^2/n)$. We standardize $\\overline{X}$ with respect to this true distribution:\n$$\nZ' = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n$$\nWe now re-express the inequality in our power calculation in terms of this new standard normal variable $Z'$.\n$$\n\\overline{X} > \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\n$$\nSubtract $\\mu$ from both sides and divide by the standard error $\\sigma/\\sqrt{n}$:\n$$\n\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} > \\frac{\\left(\\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\\right) - \\mu}{\\sigma/\\sqrt{n}}\n$$\nThe left side is $Z'$. We simplify the right side:\n$$\nZ' > \\frac{\\mu_0 - \\mu}{\\sigma/\\sqrt{n}} + \\frac{\\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)}{\\sigma/\\sqrt{n}}\n$$\n$$\nZ' > \\frac{\\mu_0 - \\mu}{\\sigma/\\sqrt{n}} + \\Phi^{-1}(1-\\alpha)\n$$\nThe power function is the probability of this event occurring:\n$$\n\\pi(\\mu) = \\mathbb{P}\\left(Z' > \\Phi^{-1}(1-\\alpha) + \\frac{\\mu_0 - \\mu}{\\sigma/\\sqrt{n}}\\right)\n$$\nThis can be rewritten as:\n$$\n\\pi(\\mu) = \\mathbb{P}\\left(Z' > \\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)\n$$\nUsing the property that for a standard normal variable $Z'$, $\\mathbb{P}(Z' > k) = 1 - \\Phi(k)$, we obtain the final expression for the power function:\n$$\n\\pi(\\mu) = 1 - \\Phi\\left(\\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)\n$$\nThis is the second part of the answer, expressed solely in terms of the required parameters.\n\nThe final answer requires a composite expression for the critical value on $\\overline{X}$ and the power function $\\pi(\\mu)$.\nThe critical value is $c = \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)$.\nThe power function is $\\pi(\\mu) = 1 - \\Phi\\left(\\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)  1 - \\Phi\\left(\\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)\n\\end{pmatrix}\n}\n$$", "id": "4956821"}, {"introduction": "Theoretical derivations find their true value when applied to practical challenges in research design. Before a clinical trial or experiment begins, a critical question is, \"How large must my sample be?\". This practice directly applies the concepts of size and power to answer that question, guiding you through the calculation of the minimum sample size needed to ensure a study has a high chance of detecting a meaningful effect, thereby linking statistical theory to the essential task of efficient and ethical study planning [@problem_id:4956820].", "problem": "A clinical trial plans to evaluate whether a new intervention increases the mean concentration of a circulating biomarker relative to a historical reference mean. Assume that observations $X_{1}, X_{2}, \\dots, X_{n}$ are independent and identically distributed as normal with mean $\\mu$ and known variance $\\sigma^{2}$. The null hypothesis is $H_{0}: \\mu \\le \\mu_{0}$ against the one-sided alternative $H_{1}: \\mu > \\mu_{0}$. The test will reject $H_{0}$ if the sample mean $\\bar{X}$ exceeds a cutoff $c$. The design requirements are a type I error rate $\\alpha = 0.025$ and power $0.9$ at the clinically meaningful effect $\\mu = \\mu_{0} + \\delta$.\n\nUse the following scientifically plausible parameters: reference mean $\\mu_{0} = 10$ mg/dL, clinically meaningful increase $\\delta = 3$ mg/dL, and known standard deviation $\\sigma = 8$ mg/dL. Determine the minimal integer sample size $n$ such that the test with rejection region $\\bar{X} > c$ has size $\\alpha$ and power $0.9$ at $\\mu = \\mu_{0} + \\delta$. Then define the corresponding critical cutoff $c$ for the sample mean. Express the cutoff in mg/dL and round the cutoff to four significant figures.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of biostatistics, specifically hypothesis testing and power analysis for a one-sided Z-test. The problem is well-posed, objective, and provides a complete and consistent set of parameters, allowing for a unique solution.\n\nThe problem requires the determination of the minimum integer sample size, $n$, and the corresponding critical cutoff value, $c$, for a hypothesis test concerning a population mean.\n\nThe observations $X_{1}, X_{2}, \\dots, X_{n}$ are independent and identically distributed (i.i.d.) from a normal distribution with mean $\\mu$ and known variance $\\sigma^{2}$. The sample mean, $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, is therefore also normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^{2}}{n}$. We write this as $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n})$.\n\nThe hypotheses are:\nNull hypothesis $H_{0}: \\mu \\le \\mu_{0}$\nAlternative hypothesis $H_{1}: \\mu > \\mu_{0}$\n\nThe test rejects $H_{0}$ if $\\bar{X} > c$. The given parameters are:\nReference mean: $\\mu_{0} = 10 \\text{ mg/dL}$\nStandard deviation: $\\sigma = 8 \\text{ mg/dL}$\nClinically meaningful increase: $\\delta = 3 \\text{ mg/dL}$, which defines the specific alternative mean $\\mu_1 = \\mu_{0} + \\delta = 10 + 3 = 13 \\text{ mg/dL}$.\nType I error rate (size): $\\alpha = 0.025$\nPower at $\\mu_1$: $1-\\beta = 0.9$, which implies a Type II error rate of $\\beta = 0.1$.\n\nFirst, we formulate the constraint on the Type I error rate. The size of the test, $\\alpha$, is the maximum probability of rejecting $H_0$ when $H_0$ is true. This maximum occurs at the boundary of the null hypothesis region, i.e., when $\\mu = \\mu_{0}$.\n$$ \\alpha = P(\\text{Reject } H_0 \\mid \\mu = \\mu_0) = P(\\bar{X} > c \\mid \\mu = \\mu_0) $$\nTo evaluate this probability, we standardize the sample mean $\\bar{X}$. Under the condition $\\mu = \\mu_0$, the statistic $Z = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}$ follows a standard normal distribution, $N(0,1)$.\n$$ \\alpha = P\\left(\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\\right) = P\\left(Z > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\\right) $$\nLet $z_{\\alpha}$ be the upper $\\alpha$-quantile of the standard normal distribution, defined by $P(Z > z_{\\alpha}) = \\alpha$. We can then write:\n$$ \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}} = z_{\\alpha} $$\nThis gives our first equation relating $c$ and $n$:\n$$ c = \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} $$\n\nSecond, we formulate the constraint on the power of the test. The power is the probability of rejecting $H_0$ when the alternative hypothesis is true. We are given that the power must be $1-\\beta = 0.9$ at the specific alternative mean $\\mu_1 = \\mu_0 + \\delta$.\n$$ 1 - \\beta = P(\\text{Reject } H_0 \\mid \\mu = \\mu_1) = P(\\bar{X} > c \\mid \\mu = \\mu_1) $$\nUnder the condition $\\mu = \\mu_1$, the statistic $\\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}}$ follows a standard normal distribution, $N(0,1)$.\n$$ 1 - \\beta = P\\left(\\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}} > \\frac{c - \\mu_1}{\\sigma/\\sqrt{n}}\\right) = P\\left(Z > \\frac{c - \\mu_1}{\\sigma/\\sqrt{n}}\\right) $$\nLet $z_{\\beta}$ be the upper $\\beta$-quantile of the standard normal distribution, defined by $P(Z > z_{\\beta}) = \\beta$. The probability $P(Z > x) = 1-\\beta$ implies $x = -z_{\\beta}$. Thus, we have:\n$$ \\frac{c - \\mu_1}{\\sigma/\\sqrt{n}} = -z_{\\beta} $$\nThis gives our second equation relating $c$ and $n$:\n$$ c = \\mu_1 - z_{\\beta} \\frac{\\sigma}{\\sqrt{n}} $$\n\nWe now have a system of two equations for the two unknowns, $n$ and $c$. We can solve for $n$ by equating the two expressions for $c$:\n$$ \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} = \\mu_1 - z_{\\beta} \\frac{\\sigma}{\\sqrt{n}} $$\nRearranging the terms to solve for $\\sqrt{n}$:\n$$ (z_{\\alpha} + z_{\\beta}) \\frac{\\sigma}{\\sqrt{n}} = \\mu_1 - \\mu_0 $$\nRecognizing that $\\mu_1 - \\mu_0 = \\delta$:\n$$ \\sqrt{n} = \\frac{(z_{\\alpha} + z_{\\beta})\\sigma}{\\delta} $$\nSquaring both sides gives the formula for the required sample size $n$:\n$$ n = \\left( \\frac{(z_{\\alpha} + z_{\\beta})\\sigma}{\\delta} \\right)^2 $$\n\nWe now substitute the given numerical values.\nFor $\\alpha=0.025$, the corresponding upper quantile is $z_{0.025} \\approx 1.95996$.\nFor a power of $0.9$, $\\beta=0.1$, and the corresponding upper quantile is $z_{0.1} \\approx 1.28155$.\nThe remaining parameters are $\\sigma=8$ and $\\delta=3$.\n$$ n = \\left( \\frac{(1.95996 + 1.28155) \\times 8}{3} \\right)^2 $$\n$$ n = \\left( \\frac{3.24151 \\times 8}{3} \\right)^2 = \\left( \\frac{25.93208}{3} \\right)^2 \\approx (8.644027)^2 \\approx 74.7192 $$\nSince the sample size $n$ must be an integer and the power requirement is a minimum, we must round the calculated value of $n$ up to the next whole number to ensure the power is at least $0.9$.\nTherefore, the minimal integer sample size is $n=75$.\n\nWith $n=75$, we can now determine the critical cutoff $c$. The cutoff is defined by the Type I error rate constraint. Using the first equation derived:\n$$ c = \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} $$\nSubstituting the values $\\mu_0=10$, $z_{0.025} \\approx 1.95996$, $\\sigma=8$, and $n=75$:\n$$ c = 10 + 1.95996 \\times \\frac{8}{\\sqrt{75}} $$\n$$ c = 10 + 1.95996 \\times \\frac{8}{5\\sqrt{3}} \\approx 10 + 1.95996 \\times 0.9237604 $$\n$$ c \\approx 10 + 1.81055 $$\n$$ c \\approx 11.81055 $$\nThe problem requires rounding the cutoff $c$ to four significant figures.\n$$ c \\approx 11.81 \\text{ mg/dL} $$\n\nThe results are the sample size $n=75$ and the critical cutoff $c=11.81$.", "answer": "$$\\boxed{\\begin{pmatrix} 75  11.81 \\end{pmatrix}}$$", "id": "4956820"}, {"introduction": "While many classical tests rely on the assumption that data follows a specific distribution like the normal distribution, what happens when this assumption is not met? This hands-on exercise introduces you to the world of non-parametric statistics through the powerful and intuitive permutation test. Instead of relying on a theoretical distribution, this method uses computational power to generate its own null distribution directly from the data, providing a robust way to test hypotheses based on the principle of exchangeability [@problem_id:4956794].", "problem": "You are asked to implement an exact, two-sided permutation test for the difference in medians between two independent groups. The test must be constructed by enumerating all label assignments, using all $\\binom{n_1+n_2}{n_1}$ permutations of group labels, and the rejection region must be defined by extreme values of an absolute-difference median statistic. The design must start from first principles appropriate to biostatistics and hypothesis testing: under the null hypothesis that the two groups are exchangeable, every assignment of $n_1$ labels among $n_1+n_2$ pooled observations is equally likely.\n\nDefinitions and fundamental base:\n- Let the pooled sample be $x = (x_1, x_2, \\dots, x_{n_1+n_2})$ with $n_1$ observations initially labeled as group $A$ and $n_2$ observations initially labeled as group $B$, where $n_1, n_2 \\in \\mathbb{N}$ and $n_1 \\geq 1$, $n_2 \\geq 1$.\n- The sample median of a finite list $y = (y_1, y_2, \\dots, y_m)$ is defined as the middle value if $m$ is odd and the average of the two middle values if $m$ is even. Denote this by $\\operatorname{median}(y)$.\n- The test statistic is the absolute difference in medians, $T = \\left| \\operatorname{median}(A) - \\operatorname{median}(B) \\right|$ computed on the observed labels. Under the null hypothesis $H_0$ of exchangeability, the exact null distribution of $T$ is obtained by computing $T$ for each way to assign $n_1$ labels among the $n_1+n_2$ pooled observations.\n\nProcedure requirements:\n1. Compute the observed test statistic $T_{\\text{obs}}$ from the provided two groups.\n2. Enumerate all $\\binom{n_1+n_2}{n_1}$ distinct label assignments of size $n_1$ on the pooled data, compute $T$ for each assignment, and form the exact null distribution.\n3. Compute the exact two-sided permutation $p$-value as the proportion of permutations whose statistic $T$ is at least as extreme as $T_{\\text{obs}}$, that is\n   $$ p = \\frac{\\#\\{ \\text{permutations } \\pi : T(\\pi) \\ge T_{\\text{obs}} \\}}{\\binom{n_1+n_2}{n_1}}. $$\n4. For a prespecified significance level $\\alpha \\in (0,1)$, define the rejection rule: reject $H_0$ if $p \\le \\alpha$. This rule corresponds to a critical region consisting of those permutations for which $T$ is in the upper tail of the exact null distribution with tail probability not exceeding $\\alpha$.\n\nYour task:\n- Implement the above exact two-sided permutation test and apply it to the test suite below.\n- For each test case, return a two-element result $[p,\\text{reject}]$ where $p$ is the exact permutation $p$-value as a real number and $\\text{reject}$ is a boolean indicating whether $H_0$ is rejected at the given $\\alpha$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list $[p,\\text{reject}]$ in the order of the test cases (for example, $[[0.0123,True],[0.5432,False]]$).\n\nTest suite:\n- Case $1$ (balanced groups, clear separation): $A = [\\,4.8,\\,5.1,\\,5.0,\\,4.9\\,]$, $B = [\\,6.2,\\,6.0,\\,6.1,\\,6.3\\,]$, $\\alpha = 0.05$.\n- Case $2$ (boundary case with complete equality): $A = [\\,3.0,\\,3.0,\\,3.0\\,]$, $B = [\\,3.0,\\,3.0\\,]$, $\\alpha = 0.05$.\n- Case $3$ (unbalanced groups with ties): $A = [\\,1.2,\\,2.5,\\,2.5\\,]$, $B = [\\,2.0,\\,2.2,\\,2.4,\\,2.5\\,]$, $\\alpha = 0.10$.\n- Case $4$ (moderate difference, borderline scenario): $A = [\\,10.1,\\,10.0,\\,10.2,\\,9.9,\\,10.0\\,]$, $B = [\\,10.3,\\,10.4,\\,10.2,\\,10.3\\,]$, $\\alpha = 0.05$.\n\nNotes:\n- All values are unitless real numbers; do not introduce any physical units.\n- Express all significance levels $\\alpha$ and all $p$-values as decimals (not as percentages).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with elements ordered as $[\\,[p_1,\\text{reject}_1],\\,[p_2,\\text{reject}_2],\\,[p_3,\\text{reject}_3],\\,[p_4,\\text{reject}_4]\\,]$ exactly corresponding to the four cases in the test suite.", "solution": "The problem requires the implementation of an exact, two-sided permutation test for the difference in medians between two independent groups, $A$ and $B$, of sizes $n_1$ and $n_2$ respectively. The solution adheres to the fundamental principles of non-parametric hypothesis testing.\n\nThe core principle underlying the permutation test is that of **exchangeability** under the null hypothesis, $H_0$. This hypothesis posits that the labels assigning observations to group $A$ or group $B$ are arbitrary and have no relationship with the observed values. Consequently, if $H_0$ is true, every possible partition of the $n_1+n_2$ pooled observations into two groups of sizes $n_1$ and $n_2$ is equally likely. There are $\\binom{n_1+n_2}{n_1}$ such partitions, and each occurs with probability $1/\\binom{n_1+n_2}{n_1}$.\n\nThe procedure involves the following steps:\n\n1.  **Define the Test Statistic**: The test statistic, $T$, is defined as the absolute difference between the sample medians of the two groups:\n    $$ T = \\left| \\operatorname{median}(A) - \\operatorname{median}(B) \\right| $$\n    The sample median of a list of $m$ numbers is the middle value of the sorted list if $m$ is odd, or the arithmetic mean of the two middle values if $m$ is even.\n\n2.  **Calculate the Observed Statistic**: The test statistic is first computed for the originally observed groups $A$ and $B$. This value is denoted as $T_{\\text{obs}}$.\n\n3.  **Construct the Exact Null Distribution**: The core of the method is to generate the complete set of all possible values of the test statistic $T$ under the null hypothesis. This is achieved by:\n    a.  Pooling all $N = n_1 + n_2$ observations into a single dataset.\n    b.  Enumerating all distinct ways to choose $n_1$ observations from the pooled dataset to form a new group $A'$, with the remaining $n_2$ observations forming group $B'$. This is equivalent to iterating through all combinations of $n_1$ indices from the set $\\{0, 1, \\dots, N-1\\}$.\n    c.  For each such permutation of labels, the test statistic $T$ is re-calculated.\n    d.  The collection of all $\\binom{N}{n_1}$ computed $T$ values constitutes the exact null distribution.\n\n4.  **Compute the p-value**: The two-sided $p$-value is the probability of observing a test statistic at least as extreme as $T_{\\text{obs}}$, assuming $H_0$ is true. Since each permutation is equally likely, this is calculated as the proportion of permutations for which the statistic $T$ is greater than or equal to $T_{\\text{obs}}$:\n    $$ p = \\frac{\\#\\{ \\text{permutations } \\pi : T(\\pi) \\ge T_{\\text{obs}} \\}}{\\binom{n_1+n_2}{n_1}} $$\n    When dealing with floating-point arithmetic, the comparison $T(\\pi) \\ge T_{\\text{obs}}$ must be handled with care. A robust implementation checks for values that are either strictly greater than $T_{\\text{obs}}$ or are numerically close to $T_{\\text{obs}}$, to account for potential floating-point inaccuracies. This is equivalent to checking if `T_perm > T_obs` or `isclose(T_perm, T_obs)`.\n\n5.  **Make a Decision**: The null hypothesis $H_0$ is rejected at a pre-specified significance level $\\alpha$ if the computed $p$-value is less than or equal to $\\alpha$.\n    $$ \\text{Reject } H_0 \\text{ if } p \\le \\alpha $$\n    Otherwise, we fail to reject $H_0$.\n\nThe algorithm is implemented by first pooling the data, then using `itertools.combinations` to generate all possible index sets for the first group. For each combination, the permuted groups are formed, their respective medians are computed using `numpy.median`, and the resulting test statistic is stored. Finally, the $p$-value is determined by counting the proportion of these statistics that meet or exceed the observed statistic, and this $p$-value is compared against $\\alpha$ to determine the outcome. This procedure is applied systematically to each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef _run_test(group_a_list, group_b_list, alpha):\n    \"\"\"\n    Performs an exact permutation test for the difference in medians.\n\n    Args:\n        group_a_list (list): Data for the first group.\n        group_b_list (list): Data for the second group.\n        alpha (float): The significance level.\n\n    Returns:\n        list: A two-element list containing the p-value and a boolean for rejection.\n    \"\"\"\n    group_a = np.array(group_a_list)\n    group_b = np.array(group_b_list)\n    \n    n1 = len(group_a)\n    n2 = len(group_b)\n    n = n1 + n2\n\n    # 1. Compute the observed test statistic, T_obs.\n    t_obs = np.abs(np.median(group_a) - np.median(group_b))\n\n    # Pool the data for permutation.\n    pooled_data = np.concatenate((group_a, group_b))\n    indices = np.arange(n)\n\n    # 2. Enumerate all permutations to form the exact null distribution.\n    null_distribution = []\n    \n    # Use itertools.combinations to get all unique ways to form group A by choosing n1 indices.\n    group_a_indices_iter = combinations(indices, n1)\n    \n    total_permutations = 0\n    for group_a_indices in group_a_indices_iter:\n        total_permutations += 1\n        \n        # The remaining indices form group B.\n        # np.setdiff1d is a clean way to get the complement set of indices.\n        group_b_indices = np.setdiff1d(indices, group_a_indices, assume_unique=True)\n        \n        perm_a = pooled_data[list(group_a_indices)]\n        perm_b = pooled_data[group_b_indices]\n        \n        # Compute the statistic for the current permutation.\n        t_perm = np.abs(np.median(perm_a) - np.median(perm_b))\n        null_distribution.append(t_perm)\n    \n    null_distribution_np = np.array(null_distribution)\n\n    # 3. Compute the p-value.\n    # The p-value is the proportion of permutations with a statistic\n    # at least as extreme as the observed one (T_perm = T_obs).\n    # To handle floating-point comparisons robustly, we check for values\n    # that are either strictly greater or numerically close to T_obs.\n    is_greater = null_distribution_np  t_obs\n    is_close = np.isclose(null_distribution_np, t_obs)\n    num_extreme = np.sum(is_greater | is_close)\n\n    p_value = num_extreme / total_permutations\n\n    # 4. Make a decision based on the significance level alpha.\n    reject = p_value = alpha\n    \n    return [p_value, bool(reject)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: balanced groups, clear separation\n        {'A': [4.8, 5.1, 5.0, 4.9], 'B': [6.2, 6.0, 6.1, 6.3], 'alpha': 0.05},\n        # Case 2: boundary case with complete equality\n        {'A': [3.0, 3.0, 3.0], 'B': [3.0, 3.0], 'alpha': 0.05},\n        # Case 3: unbalanced groups with ties\n        {'A': [1.2, 2.5, 2.5], 'B': [2.0, 2.2, 2.4, 2.5], 'alpha': 0.10},\n        # Case 4: moderate difference, borderline scenario\n        {'A': [10.1, 10.0, 10.2, 9.9, 10.0], 'B': [10.3, 10.4, 10.2, 10.3], 'alpha': 0.05}\n    ]\n\n    results = []\n    for case in test_cases:\n        p_val, rej = _run_test(case['A'], case['B'], case['alpha'])\n        results.append([p_val, rej])\n\n    # Final print statement in the exact required format.\n    result_strings = [f\"[{p},{'True' if r else 'False'}]\" for p, r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "4956794"}]}