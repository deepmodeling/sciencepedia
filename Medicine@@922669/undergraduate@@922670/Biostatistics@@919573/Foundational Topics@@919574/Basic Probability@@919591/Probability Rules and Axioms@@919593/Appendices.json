{"hands_on_practices": [{"introduction": "To build a strong foundation in biostatistics, one must first achieve fluency in the fundamental rules of probability. This initial exercise [@problem_id:6] serves as a warm-up, allowing you to practice manipulating probabilities using the core axioms. By applying the complement rule and the inclusion-exclusion principle, you will see how the probability of one event can be determined from information about related events, a common task in data analysis.", "problem": "Consider a sample space $S$ and two events, $A$ and $B$, which are subsets of $S$. The probabilities of event $A$, the intersection of events $A$ and $B$, and the complement of the union of events $A$ and $B$ are given. Specifically, let:\n- The probability of event $A$ be $P(A) = p_A$.\n- The probability of the intersection of $A$ and $B$ be $P(A \\cap B) = p_{AB}$.\n- The probability of the complement of the union of $A$ and $B$ be $P((A \\cup B)^c) = p_C$.\n\nUsing only the fundamental axioms of probability, derive an expression for the probability of event $B$, denoted as $P(B)$, in terms of $p_A$, $p_{AB}$, and $p_C$.", "solution": "By the complement rule,\n$$P((A\\cup B)^c)=p_C\\quad\\implies\\quad P(A\\cup B)=1-p_C.$$\nBy the inclusion–exclusion principle,\n$$P(A\\cup B)=P(A)+P(B)-P(A\\cap B).$$\nSubstitute the known probabilities:\n$$1-p_C=p_A+P(B)-p_{AB}.$$\nRearrange to solve for $P(B)$:\n$$P(B)=1-p_C-p_A+p_{AB}.$$", "answer": "$$\\boxed{1 - p_A - p_C + p_{AB}}$$", "id": "6"}, {"introduction": "Moving beyond manipulating given probabilities, this exercise [@problem_id:4942639] challenges you to build a probability model from first principles. Starting with only the Kolmogorov axioms, you will construct the entire probability space for a series of independent patient outcomes in a clinical trial. This practice is invaluable as it demystifies one of the most important tools in biostatistics, the binomial distribution, by showing how it is formally derived from foundational theory.", "problem": "A randomized clinical study enrolls $n$ patients to receive a new therapy. For each patient $i \\in \\{1,\\dots,n\\}$, define a binary outcome $X_{i}$ where $X_{i}=1$ denotes a clinically validated response and $X_{i}=0$ denotes no response. Assume each patient has the same response risk $p \\in (0,1)$ and that outcomes across patients are independent in the sense of product construction from first principles.\n\nUsing only the Kolmogorov axioms of probability (nonnegativity, normalization, countable additivity) and the definition of independence, construct a probability space that represents the $n$ independent and identically distributed Bernoulli risks. Explicitly specify the sample space $\\Omega$, the sigma-algebra $\\mathcal{F}$, and the probability measure $\\mathbb{P}$ so that the coordinate outcomes are independent and identically distributed with $\\mathbb{P}(X_{i}=1)=p$ and $\\mathbb{P}(X_{i}=0)=1-p$ for each $i \\in \\{1,\\dots,n\\}$. Then, derive from these axioms and counting arguments the probability that there are exactly $k$ responses, where $k \\in \\{0,1,\\dots,n\\}$.\n\nExpress your final answer as a single closed-form analytic expression in terms of $n$, $k$, and $p$. No rounding is required.", "solution": "The problem statement is a well-posed and fundamental exercise in probability theory, specifically the construction of a product probability space for a sequence of independent and identically distributed (i.i.d.) Bernoulli trials. It is scientifically sound, internally consistent, and contains all necessary information to derive the requested probability. We therefore proceed with the solution.\n\nThe objective is to first construct a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ that models the outcomes of $n$ independent patients, each with a binary response. We must then use this construction and foundational principles to derive the probability of observing exactly $k$ responses.\n\n**Step 1: Construction of the Probability Space $(\\Omega, \\mathcal{F}, \\mathbb{P})$**\n\nA probability space is a triple consisting of a sample space $\\Omega$, a sigma-algebra of events $\\mathcal{F}$, and a probability measure $\\mathbb{P}$.\n\n**1. The Sample Space $\\Omega$**\nThe outcome of the study for $n$ patients is a sequence of length $n$, where the $i$-th element represents the outcome for patient $i$. Since each outcome $X_i$ is binary ($1$ for response, $0$ for no response), an elementary outcome of the entire experiment is a tuple $\\omega = (\\omega_1, \\omega_2, \\dots, \\omega_n)$ where each $\\omega_i \\in \\{0, 1\\}$. The sample space $\\Omega$ is the set of all such possible sequences. This is the $n$-fold Cartesian product of the set $\\{0, 1\\}$ with itself.\n$$ \\Omega = \\{0, 1\\}^n = \\{(\\omega_1, \\omega_2, \\dots, \\omega_n) \\mid \\omega_i \\in \\{0, 1\\} \\text{ for } i=1, \\dots, n\\} $$\nThis is a finite sample space with $|\\Omega| = 2^n$ elements.\n\n**2. The Sigma-Algebra $\\mathcal{F}$**\nAn event is a subset of $\\Omega$. The collection of all events must form a sigma-algebra $\\mathcal{F}$. For a finite sample space such as $\\Omega$, the most comprehensive and standard choice for the sigma-algebra is the power set of $\\Omega$, denoted $\\mathcal{P}(\\Omega)$. The power set is the set of all subsets of $\\Omega$.\n$$ \\mathcal{F} = \\mathcal{P}(\\Omega) $$\nThis choice trivially satisfies the axioms of a sigma-algebra:\n   - $\\Omega \\in \\mathcal{F}$ (since $\\Omega \\subseteq \\Omega$).\n   - If $A \\in \\mathcal{F}$, then its complement $A^c = \\Omega \\setminus A$ is also a subset of $\\Omega$ and thus $A^c \\in \\mathcal{F}$.\n   - If $A_1, A_2, \\dots$ is any (finite or countable) collection of sets in $\\mathcal{F}$, their union $\\bigcup_i A_i$ is also a subset of $\\Omega$ and thus is in $\\mathcal{F}$.\n\n**3. The Probability Measure $\\mathbb{P}$**\nThe probability measure $\\mathbb{P}: \\mathcal{F} \\to [0, 1]$ assigns a probability to every event in $\\mathcal{F}$. To satisfy the Kolmogorov axioms and the problem's conditions of independence and identical distribution, we first define the probability of each elementary outcome $\\omega \\in \\Omega$.\n\nThe problem states that the patient outcomes are independent and that for each patient $i$, the probability of a response is $\\mathbb{P}(X_i=1) = p$ and of no response is $\\mathbb{P}(X_i=0) = 1-p$. An elementary outcome $\\omega = (\\omega_1, \\dots, \\omega_n)$ represents the joint occurrence of $n$ individual outcomes. The event $\\{\\omega\\}$ is the intersection of the events $E_i = \\{\\text{outcome of patient } i \\text{ is } \\omega_i\\}$ for $i=1, \\dots, n$.\n\nBy the definition of independence, the probability of the intersection of independent events is the product of their individual probabilities.\n$$ \\mathbb{P}(\\{\\omega\\}) = \\mathbb{P}(\\text{outcome 1 is } \\omega_1, \\dots, \\text{outcome } n \\text{ is } \\omega_n) = \\prod_{i=1}^{n} \\mathbb{P}(\\text{outcome } i \\text{ is } \\omega_i) $$\nWe are given $\\mathbb{P}(\\text{outcome } i \\text{ is } 1) = p$ and $\\mathbb{P}(\\text{outcome } i \\text{ is } 0) = 1-p$. This can be written compactly as $\\mathbb{P}(\\text{outcome } i \\text{ is } \\omega_i) = p^{\\omega_i} (1-p)^{1-\\omega_i}$, since if $\\omega_i=1$, this is $p^1(1-p)^0=p$, and if $\\omega_i=0$, this is $p^0(1-p)^1=1-p$.\n\nTherefore, for an elementary outcome $\\omega = (\\omega_1, \\dots, \\omega_n)$, its probability is:\n$$ \\mathbb{P}(\\{\\omega\\}) = \\prod_{i=1}^{n} p^{\\omega_i} (1-p)^{1-\\omega_i} = p^{\\sum_{i=1}^n \\omega_i} (1-p)^{\\sum_{i=1}^n (1-\\omega_i)} = p^{\\sum_{i=1}^n \\omega_i} (1-p)^{n - \\sum_{i=1}^n \\omega_i} $$\nLet $s(\\omega) = \\sum_{i=1}^n \\omega_i$ be the number of responses (ones) in the sequence $\\omega$. Then the probability of this specific sequence is:\n$$ \\mathbb{P}(\\{\\omega\\}) = p^{s(\\omega)} (1-p)^{n-s(\\omega)} $$\nFor any event $A \\in \\mathcal{F}$, which is a set of elementary outcomes, the third Kolmogorov axiom (countable additivity, which for a finite space simplifies to finite additivity) dictates that its probability is the sum of the probabilities of the elementary outcomes it contains:\n$$ \\mathbb{P}(A) = \\sum_{\\omega \\in A} \\mathbb{P}(\\{\\omega\\}) $$\nWe must verify that this measure satisfies the first two Kolmogorov axioms.\n- **Axiom 1 (Nonnegativity):** Since $p \\in (0, 1)$, we have $p > 0$ and $1-p > 0$. Thus, for any $\\omega$, $\\mathbb{P}(\\{\\omega\\}) = p^{s(\\omega)} (1-p)^{n-s(\\omega)} \\ge 0$. The probability of any event $A$ is a sum of these non-negative terms, so $\\mathbb{P}(A) \\ge 0$.\n- **Axiom 2 (Normalization):** We must show $\\mathbb{P}(\\Omega)=1$.\n$$ \\mathbb{P}(\\Omega) = \\sum_{\\omega \\in \\Omega} \\mathbb{P}(\\{\\omega\\}) = \\sum_{\\omega \\in \\Omega} p^{s(\\omega)} (1-p)^{n-s(\\omega)} $$\nWe can group the elementary outcomes by the number of responses, $k = s(\\omega)$. For a fixed $k \\in \\{0, 1, \\dots, n\\}$, the number of sequences $\\omega$ with exactly $k$ responses is the number of ways to choose $k$ positions for the '1's out of $n$ positions, which is given by the binomial coefficient $\\binom{n}{k}$. Each such sequence has the same probability $p^k(1-p)^{n-k}$.\n$$ \\mathbb{P}(\\Omega) = \\sum_{k=0}^{n} \\left( \\sum_{\\omega: s(\\omega)=k} p^k (1-p)^{n-k} \\right) = \\sum_{k=0}^{n} \\binom{n}{k} p^k (1-p)^{n-k} $$\nThis sum is the binomial expansion of $(p + (1-p))^n$.\n$$ \\mathbb{P}(\\Omega) = (p + (1-p))^n = 1^n = 1 $$\nThe normalization axiom is satisfied. The probability space is now fully and correctly constructed.\n\n**Step 2: Derivation of the Probability of Exactly $k$ Responses**\n\nWe are asked to find the probability of the event that there are exactly $k$ responses. Let us denote this event by $E_k$. In the formalism of our sample space, this event is the set of all elementary outcomes $\\omega$ that contain exactly $k$ ones.\n$$ E_k = \\{\\omega \\in \\Omega \\mid s(\\omega) = \\sum_{i=1}^n \\omega_i = k\\} $$\nSince $E_k \\subseteq \\Omega$, $E_k$ is an event in our sigma-algebra $\\mathcal{F}$. Using the additivity property of the probability measure, its probability is the sum of the probabilities of its constituent elementary outcomes:\n$$ \\mathbb{P}(E_k) = \\sum_{\\omega \\in E_k} \\mathbb{P}(\\{\\omega\\}) $$\nAs established in Step 1, for any single outcome $\\omega \\in E_k$, the number of responses is $s(\\omega)=k$. The probability of this specific outcome is:\n$$ \\mathbb{P}(\\{\\omega\\}) = p^k (1-p)^{n-k} $$\nCrucially, every elementary outcome in the event $E_k$ has the exact same probability. The sum therefore simplifies to the number of such outcomes multiplied by this common probability value:\n$$ \\mathbb{P}(E_k) = |E_k| \\times p^k (1-p)^{n-k} $$\nThe final task is to determine $|E_k|$, the number of elements in $E_k$. This is a purely combinatorial problem: we need to count the number of distinct binary sequences of length $n$ that have exactly $k$ ones (and consequently, $n-k$ zeros). This is equivalent to choosing $k$ positions for the ones from the $n$ available positions. The number of ways to do this is given by the binomial coefficient \"n choose k\":\n$$ |E_k| = \\binom{n}{k} = \\frac{n!}{k!(n-k)!} $$\nSubstituting this counting result into the probability expression, we obtain the probability of observing exactly $k$ responses:\n$$ \\mathbb{P}(E_k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nThis result, known as the probability mass function of the binomial distribution, has been derived from first principles as required.", "answer": "$$ \\boxed{\\binom{n}{k} p^k (1-p)^{n-k}} $$", "id": "4942639"}, {"introduction": "A deep understanding of probability often involves confronting concepts that challenge our intuition. This problem [@problem_id:4942611] explores the critical distinction between pairwise and joint independence, a frequent source of confusion. By working through a concrete example from a clinical study, you will demonstrate how a set of events can be independent when considered in pairs, yet fail to be mutually independent as a group, sharpening your understanding of how to correctly model complex dependencies.", "problem": "In a cohort study of an emerging viral infection, each participant is tested at two visits using polymerase chain reaction (PCR). Let the baseline test result be encoded by a binary variable $X \\in \\{0,1\\}$, where $X=1$ denotes PCR-positive and $X=0$ denotes PCR-negative. Let the follow-up test result be encoded by a binary variable $Y \\in \\{0,1\\}$ defined analogously. Assume the following simplified modeling assumptions to isolate probability structure: \n- Across participants, the baseline and follow-up PCR positivity are exchangeable with $P(X=1)=P(Y=1)=\\frac{1}{2}$.\n- Conditional on the underlying latent infection dynamics, $X$ and $Y$ are independent and the four joint outcomes $(X,Y) \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$ are equally likely.\n\nDefine the three events\n- $A=\\{X=1\\}$, \n- $B=\\{Y=1\\}$, \n- $C=\\{X \\neq Y\\}$ (the event that the participant’s PCR status changes between visits, corresponding to either clearance or incident detection).\n\nStarting only from the foundational axioms of probability (Kolmogorov axioms) and the definitions of independence of events, construct the sample space and the probability measure for this model, compute $P(A)$, $P(B)$, $P(C)$, $P(A \\cap B)$, $P(A \\cap C)$, $P(B \\cap C)$, and $P(A \\cap B \\cap C)$, and use these quantities to determine whether $A$, $B$, and $C$ are pairwise independent and whether they are jointly independent.\n\nProvide the value of $P(A \\cap B \\cap C)$ as your final answer. No rounding is required. Express all probabilities as exact fractions or decimals.", "solution": "The problem statement will first be validated for scientific soundness, consistency, and completeness.\n\n### Step 1: Extract Givens\n-   The baseline test result is a binary variable $X \\in \\{0,1\\}$, where $X=1$ is positive.\n-   The follow-up test result is a binary variable $Y \\in \\{0,1\\}$, where $Y=1$ is positive.\n-   Assumption 1: $P(X=1) = P(Y=1) = \\frac{1}{2}$.\n-   Assumption 2: \"Conditional on the underlying latent infection dynamics, $X$ and $Y$ are independent and the four joint outcomes $(X,Y) \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$ are equally likely.\"\n-   Event definitions:\n    -   $A = \\{X=1\\}$\n    -   $B = \\{Y=1\\}$\n    -   $C = \\{X \\neq Y\\}$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-posed and scientifically sound for a simplified stochastic model. We analyze the provided assumptions. The statement \"the four joint outcomes $(X,Y) \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$ are equally likely\" is a precise and unambiguous definition of the probability measure on the sample space. This single statement is sufficient to construct the entire model. If each of the four outcomes has a probability of $\\frac{1}{4}$, then:\n-   $P(X=1) = P(\\{(1,0)\\}) + P(\\{(1,1)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n-   $P(Y=1) = P(\\{(0,1)\\}) + P(\\{(1,1)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\nThis shows that Assumption 1, $P(X=1)=P(Y=1)=\\frac{1}{2}$, is a direct consequence of the \"equally likely outcomes\" assumption and is therefore redundant.\n\nFurthermore, we can check for the independence of $X$ and $Y$:\n-   $P(X=1 \\text{ and } Y=1) = P(\\{(1,1)\\}) = \\frac{1}{4}$.\n-   $P(X=1)P(Y=1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$.\nSince $P(X=1, Y=1) = P(X=1)P(Y=1)$, the variables $X$ and $Y$ are independent. This is also a consequence of the \"equally likely outcomes\" assumption.\n\nThe phrase \"Conditional on the underlying latent infection dynamics, $X$ and $Y$ are independent\" is superfluous and potentially confusing, as the unconditional independence of $X$ and $Y$ is already established by the primary assumption that all joint outcomes are equally likely. It appears to be narrative framing rather than a separate mathematical constraint.\n\nThe problem, however, remains valid as it is hinged on the clear and self-contained statement that the four outcomes are equally likely. This provides a sufficient basis for a unique solution. The problem does not violate any scientific principles, is not ill-posed, and all terms are well-defined.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed by constructing the probability space based on the equally likely outcomes.\n\n### Solution\nThe experiment consists of observing the pair of test results $(X,Y)$. The set of all possible outcomes, the sample space $\\Omega$, is given by:\n$$\n\\Omega = \\{(0,0), (0,1), (1,0), (1,1)\\}\n$$\nThe problem states that these four elementary outcomes are equally likely. According to the axioms of probability, the sum of probabilities of all outcomes must be $1$. Therefore, the probability measure $P$ assigns a probability of $\\frac{1}{4}$ to each elementary outcome:\n$$\nP(\\{(0,0)\\}) = P(\\{(0,1)\\}) = P(\\{(1,0)\\}) = P(\\{(1,1)\\}) = \\frac{1}{4}\n$$\nNow, we define the events $A$, $B$, and $C$ as subsets of the sample space $\\Omega$.\n-   Event $A = \\{X=1\\}$ corresponds to the outcomes where the first element is $1$.\n    $$\n    A = \\{(1,0), (1,1)\\}\n    $$\n-   Event $B = \\{Y=1\\}$ corresponds to the outcomes where the second element is $1$.\n    $$\n    B = \\{(0,1), (1,1)\\}\n    $$\n-   Event $C = \\{X \\neq Y\\}$ corresponds to the outcomes where the two elements are different.\n    $$\n    C = \\{(0,1), (1,0)\\}\n    $$\nNext, we compute the probabilities of these events by summing the probabilities of their constituent elementary outcomes.\n$$\nP(A) = P(\\{(1,0)\\}) + P(\\{(1,1)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\n$$\nP(B) = P(\\{(0,1)\\}) + P(\\{(1,1)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\n$$\nP(C) = P(\\{(0,1)\\}) + P(\\{(1,0)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\nTo check for independence, we must compute the probabilities of the intersections of these events.\n-   $A \\cap B = \\{(1,0), (1,1)\\} \\cap \\{(0,1), (1,1)\\} = \\{(1,1)\\}$.\n    $$\n    P(A \\cap B) = P(\\{(1,1)\\}) = \\frac{1}{4}\n    $$\n-   $A \\cap C = \\{(1,0), (1,1)\\} \\cap \\{(0,1), (1,0)\\} = \\{(1,0)\\}$.\n    $$\n    P(A \\cap C) = P(\\{(1,0)\\}) = \\frac{1}{4}\n    $$\n-   $B \\cap C = \\{(0,1), (1,1)\\} \\cap \\{(0,1), (1,0)\\} = \\{(0,1)\\}$.\n    $$\n    P(B \\cap C) = P(\\{(0,1)\\}) = \\frac{1}{4}\n    $$\nNow we check for pairwise independence. Two events $E_1$ and $E_2$ are independent if $P(E_1 \\cap E_2) = P(E_1)P(E_2)$.\n-   For $A$ and $B$: $P(A)P(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. Since $P(A \\cap B) = \\frac{1}{4}$, events $A$ and $B$ are independent.\n-   For $A$ and $C$: $P(A)P(C) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. Since $P(A \\cap C) = \\frac{1}{4}$, events $A$ and $C$ are independent.\n-   For $B$ and $C$: $P(B)P(C) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. Since $P(B \\cap C) = \\frac{1}{4}$, events $B$ and $C$ are independent.\nThus, the events $A$, $B$, and $C$ are pairwise independent.\n\nFor the events to be jointly (or mutually) independent, they must be pairwise independent and also satisfy the condition $P(A \\cap B \\cap C) = P(A)P(B)P(C)$. We calculate the intersection $A \\cap B \\cap C$:\n$$\nA \\cap B \\cap C = (A \\cap B) \\cap C = \\{(1,1)\\} \\cap \\{(0,1), (1,0)\\} = \\emptyset\n$$\nThe intersection of the three events is the empty set. The probability of the empty set is, by axiom, zero.\n$$\nP(A \\cap B \\cap C) = P(\\emptyset) = 0\n$$\nNow we compare this to the product of the individual probabilities:\n$$\nP(A)P(B)P(C) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}\n$$\nSince $P(A \\cap B \\cap C) = 0$ but $P(A)P(B)P(C) = \\frac{1}{8}$, we have:\n$$\nP(A \\cap B \\cap C) \\neq P(A)P(B)P(C)\n$$\nTherefore, the events $A$, $B$, and $C$ are not jointly independent, despite being pairwise independent.\n\nThe problem asks for the value of $P(A \\cap B \\cap C)$, which we have calculated to be $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "4942611"}]}