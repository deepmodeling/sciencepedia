{"hands_on_practices": [{"introduction": "This first exercise provides a practical application of core probability theorems in a classic biostatistical scenario: clinical diagnosis. You will take on the role of a biostatistician, synthesizing prior knowledge about disease prevalence with new diagnostic test data. By systematically constructing a full joint probability distribution, this practice reinforces how the law of total probability and Bayes' rule work together to update our beliefs in the face of new evidence, a fundamental skill in medical research and epidemiology [@problem_id:4913399].", "problem": "A hospital surveillance study of adult patients admitted with acute lower respiratory infection has determined that, conditional on admission with this syndrome, exactly one of three etiologies is present: Influenza A ($A$), Respiratory Syncytial Virus ($B$), or Bacterial Pneumonia ($C$). Based on historical prevalence in this admitted population, the prior probabilities are $P(A)=0.24$, $P(B)=0.31$, and $P(C)=0.45$.\n\nEach admitted patient undergoes two diagnostic assessments:\n1. A molecular respiratory panel (polymerase chain reaction (PCR)-based), which returns either positive $(M^{+})$ or negative $(M^{-})$ for viral targets.\n2. A serum procalcitonin test, dichotomized at a clinically validated threshold, which returns either high $(P^{+})$ or low $(P^{-})$.\n\nFrom independent clinical validation data, the following conditional probabilities are known:\n- Given $A$: $P(M^{+}\\mid A)=0.94$ and $P(P^{+}\\mid A)=0.28$.\n- Given $B$: $P(M^{+}\\mid B)=0.88$ and $P(P^{+}\\mid B)=0.35$.\n- Given $C$: $P(M^{+}\\mid C)=0.17$ and $P(P^{+}\\mid C)=0.81$.\n\nIt is reasonable to assume that, conditional on the true etiology, the two test outcomes are independent.\n\nUsing only the core definitions of probability and conditional probability, and the independence assumption stated above, do the following:\n- Derive the full joint probability table over the sample space consisting of disease $\\{A,B,C\\}$ and test outcome pairs $\\{(M^{+},P^{+}), (M^{+},P^{-}), (M^{-},P^{+}), (M^{-},P^{-})\\}$.\n- Then, compute the posterior probability that an admitted patient has Bacterial Pneumonia $(C)$ given that both tests are positive, that is, $P(C\\mid M^{+},P^{+})$.\n\nExpress the final numeric answer as a decimal rounded to four significant figures. Do not use a percentage sign.", "solution": "The problem is deemed valid.\n**Step 1: Extract Givens:** The problem provides a set of mutually exclusive and exhaustive etiologies $\\{A, B, C\\}$ with prior probabilities $P(A)=0.24$, $P(B)=0.31$, and $P(C)=0.45$. It specifies two diagnostic tests with binary outcomes, $(M^{+}, M^{-})$ and $(P^{+}, P^{-})$. It provides conditional probabilities of positive test results for each etiology: $P(M^{+}\\mid A)=0.94$, $P(P^{+}\\mid A)=0.28$; $P(M^{+}\\mid B)=0.88$, $P(P^{+}\\mid B)=0.35$; $P(M^{+}\\mid C)=0.17$, $P(P^{+}\\mid C)=0.81$. A key assumption of conditional independence of the tests given the etiology is stated. The tasks are to derive a joint probability table and compute a specific posterior probability, $P(C\\mid M^{+},P^{+})$, rounded to four significant figures.\n**Step 2: Validate Using Extracted Givens:** The problem is scientifically grounded, describing a standard scenario in clinical diagnostics and epidemiology. It is well-posed, as all necessary probabilities are provided, the priors sum to $1$, and the tasks are clearly defined, leading to a unique solution. The language is objective and precise. The problem is a direct application of fundamental probability theory, specifically the law of total probability and Bayes' theorem, and is not trivial or ill-posed. All conditions for a valid problem are met.\n\nThe solution proceeds as follows. Let $E$ be a random variable for the etiology, taking values in $\\{A, B, C\\}$. Let $M$ and $P$ be random variables for the outcomes of the molecular panel and procalcitonin test, respectively.\n\nThe given prior probabilities are:\n$$P(A) = 0.24$$\n$$P(B) = 0.31$$\n$$P(C) = 0.45$$\nThese are mutually exclusive and exhaustive, as $0.24 + 0.31 + 0.45 = 1$.\n\nThe given conditional probabilities for positive test results are:\n- For etiology $A$: $P(M^{+} \\mid A) = 0.94$ and $P(P^{+} \\mid A) = 0.28$.\n- For etiology $B$: $P(M^{+} \\mid B) = 0.88$ and $P(P^{+} \\mid B) = 0.35$.\n- For etiology $C$: $P(M^{+} \\mid C) = 0.17$ and $P(P^{+} \\mid C) = 0.81$.\n\nThe problem states that, conditional on the true etiology, the two test outcomes are independent. This can be expressed for any etiology $E \\in \\{A, B, C\\}$ and test outcomes $M_i \\in \\{M^{+}, M^{-}\\}$, $P_j \\in \\{P^{+}, P^{-}\\}$ as:\n$$P(M_i, P_j \\mid E) = P(M_i \\mid E) P(P_j \\mid E)$$\n\nThe first task is to derive the full joint probability table for $P(E, M, P)$. The joint probability is found using the chain rule: $P(E, M, P) = P(M, P \\mid E)P(E)$. Applying the conditional independence assumption, this becomes:\n$$P(E, M, P) = P(M \\mid E) P(P \\mid E) P(E)$$\n\nFirst, we compute the conditional probabilities for negative test outcomes using the complement rule, $P(\\text{event}^c) = 1 - P(\\text{event})$:\n- $P(M^{-} \\mid A) = 1 - P(M^{+} \\mid A) = 1 - 0.94 = 0.06$\n- $P(P^{-} \\mid A) = 1 - P(P^{+} \\mid A) = 1 - 0.28 = 0.72$\n- $P(M^{-} \\mid B) = 1 - P(M^{+} \\mid B) = 1 - 0.88 = 0.12$\n- $P(P^{-} \\mid B) = 1 - P(P^{+} \\mid B) = 1 - 0.35 = 0.65$\n- $P(M^{-} \\mid C) = 1 - P(M^{+} \\mid C) = 1 - 0.17 = 0.83$\n- $P(P^{-} \\mid C) = 1 - P(P^{+} \\mid C) = 1 - 0.81 = 0.19$\n\nNow we can compute all $3 \\times 2 \\times 2 = 12$ joint probabilities:\n\nFor etiology $A$ (with $P(A) = 0.24$):\n- $P(A, M^{+}, P^{+}) = P(M^{+} \\mid A) P(P^{+} \\mid A) P(A) = 0.94 \\times 0.28 \\times 0.24 = 0.063168$\n- $P(A, M^{+}, P^{-}) = P(M^{+} \\mid A) P(P^{-} \\mid A) P(A) = 0.94 \\times 0.72 \\times 0.24 = 0.162432$\n- $P(A, M^{-}, P^{+}) = P(M^{-} \\mid A) P(P^{+} \\mid A) P(A) = 0.06 \\times 0.28 \\times 0.24 = 0.004032$\n- $P(A, M^{-}, P^{-}) = P(M^{-} \\mid A) P(P^{-} \\mid A) P(A) = 0.06 \\times 0.72 \\times 0.24 = 0.010368$\n\nFor etiology $B$ (with $P(B) = 0.31$):\n- $P(B, M^{+}, P^{+}) = P(M^{+} \\mid B) P(P^{+} \\mid B) P(B) = 0.88 \\times 0.35 \\times 0.31 = 0.095480$\n- $P(B, M^{+}, P^{-}) = P(M^{+} \\mid B) P(P^{-} \\mid B) P(B) = 0.88 \\times 0.65 \\times 0.31 = 0.177320$\n- $P(B, M^{-}, P^{+}) = P(M^{-} \\mid B) P(P^{+} \\mid B) P(B) = 0.12 \\times 0.35 \\times 0.31 = 0.013020$\n- $P(B, M^{-}, P^{-}) = P(M^{-} \\mid B) P(P^{-} \\mid B) P(B) = 0.12 \\times 0.65 \\times 0.31 = 0.024180$\n\nFor etiology $C$ (with $P(C) = 0.45$):\n- $P(C, M^{+}, P^{+}) = P(M^{+} \\mid C) P(P^{+} \\mid C) P(C) = 0.17 \\times 0.81 \\times 0.45 = 0.061965$\n- $P(C, M^{+}, P^{-}) = P(M^{+} \\mid C) P(P^{-} \\mid C) P(C) = 0.17 \\times 0.19 \\times 0.45 = 0.014535$\n- $P(C, M^{-}, P^{+}) = P(M^{-} \\mid C) P(P^{+} \\mid C) P(C) = 0.83 \\times 0.81 \\times 0.45 = 0.302535$\n- $P(C, M^{-}, P^{-}) = P(M^{-} \\mid C) P(P^{-} \\mid C) P(C) = 0.83 \\times 0.19 \\times 0.45 = 0.070965$\n\nThe full joint probability table is as follows:\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Etiology } E  \\text{Outcomes } (M,P)  \\text{Joint Probability } P(E,M,P) \\\\\n\\hline\n\\hline\nA  (M^{+}, P^{+})  0.063168 \\\\\nA  (M^{+}, P^{-})  0.162432 \\\\\nA  (M^{-}, P^{+})  0.004032 \\\\\nA  (M^{-}, P^{-})  0.010368 \\\\\n\\hline\nB  (M^{+}, P^{+})  0.095480 \\\\\nB  (M^{+}, P^{-})  0.177320 \\\\\nB  (M^{-}, P^{+})  0.013020 \\\\\nB  (M^{-}, P^{-})  0.024180 \\\\\n\\hline\nC  (M^{+}, P^{+})  0.061965 \\\\\nC  (M^{+}, P^{-})  0.014535 \\\\\nC  (M^{-}, P^{+})  0.302535 \\\\\nC  (M^{-}, P^{-})  0.070965 \\\\\n\\hline\n\\end{array}\n$$\n\nThe second task is to compute the posterior probability $P(C \\mid M^{+}, P^{+})$. Using the definition of conditional probability, which is the foundation of Bayes' theorem:\n$$P(C \\mid M^{+}, P^{+}) = \\frac{P(C, M^{+}, P^{+})}{P(M^{+}, P^{+})}$$\nThe numerator is a joint probability we have already calculated:\n$$P(C, M^{+}, P^{+}) = 0.061965$$\nThe denominator, $P(M^{+}, P^{+})$, is the marginal probability of observing the outcome $(M^{+}, P^{+})$. It is obtained by summing the joint probabilities of this outcome over all possible etiologies, according to the law of total probability:\n$$P(M^{+}, P^{+}) = P(A, M^{+}, P^{+}) + P(B, M^{+}, P^{+}) + P(C, M^{+}, P^{+})$$\nUsing the values from our calculations:\n$$P(M^{+}, P^{+}) = 0.063168 + 0.095480 + 0.061965 = 0.220613$$\nNow we can compute the posterior probability:\n$$P(C \\mid M^{+}, P^{+}) = \\frac{0.061965}{0.220613} \\approx 0.2808775$$\nThe problem requires the answer to be rounded to four significant figures. The fifth significant figure is $7$, so we round up the fourth.\n$$P(C \\mid M^{+}, P^{+}) \\approx 0.2809$$\nThis is the posterior probability that a patient has Bacterial Pneumonia given a positive molecular panel and a high procalcitonin level.", "answer": "$$\\boxed{0.2809}$$", "id": "4913399"}, {"introduction": "Moving from direct application to a deeper conceptual challenge, this problem explores the subtle but critical distinction between pairwise and mutual independence. While it is easy to assume that variables independent in pairs are independent as a group, this is not always true. This thought experiment, framed in a simplified genetic context, provides a classic counterexample that solidifies your understanding of the precise definition of mutual independence and helps you avoid common pitfalls in statistical modeling [@problem_id:4913391].", "problem": "A biobank study considers two unlinked biallelic loci, denoted by $L_A$ and $L_B$, each with minor allele frequency $0.5$ in a large, randomly mating population. For a randomly sampled individual, define the indicator variables $X_A=\\mathbf{1}\\{\\text{minor allele present at }L_A\\}$ and $X_B=\\mathbf{1}\\{\\text{minor allele present at }L_B\\}$. Assume the standard model in which independence across unlinked loci holds, so that $X_A$ and $X_B$ are independent and each is $\\text{Bernoulli}(0.5)$. Define a third indicator $X_C=\\mathbf{1}\\{\\text{minor allele present at exactly one of }L_A\\text{ or }L_B\\}$, that is, $X_C=\\mathbf{1}\\{X_A\\neq X_B\\}$. Using the foundational definitions of probability (the Kolmogorov axioms), event independence, and indicator variables, determine whether $X_A$, $X_B$, and $X_C$ are pairwise independent and whether they are mutually independent. Choose the single best option that correctly characterizes the independence structure and supports it with a valid numerical consequence.\n\nA. $X_A$, $X_B$, and $X_C$ are mutually independent; for example, $P(X_A=1,X_B=1,X_C=1)=\\left(\\tfrac{1}{2}\\right)^3=\\tfrac{1}{8}$.\n\nB. $X_A$, $X_B$, and $X_C$ are pairwise independent but not mutually independent; specifically, $P(X_A=1,X_B=1,X_C=1)=0$ while $P(X_A=1)P(X_B=1)P(X_C=1)=\\tfrac{1}{8}$.\n\nC. $X_A$ and $X_C$ are not independent because $P(X_C=1\\mid X_A=1)=1$.\n\nD. Pairwise independence among $X_A$, $X_B$, and $X_C$ holds only if $P(X_A=1)=P(X_B=1)=\\tfrac{1}{4}$.", "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n**Step 1: Extract Givens**\n- Two unlinked biallelic loci, $L_A$ and $L_B$.\n- Minor allele frequency for each locus is $0.5$ in a large, randomly mating population.\n- $X_A = \\mathbf{1}\\{\\text{minor allele present at }L_A\\}$.\n- $X_B = \\mathbf{1}\\{\\text{minor allele present at }L_B\\}$.\n-Explicit Assumption 1: $X_A$ and $X_B$ are independent.\n- Explicit Assumption 2: $X_A \\sim \\text{Bernoulli}(0.5)$ and $X_B \\sim \\text{Bernoulli}(0.5)$.\n- $X_C = \\mathbf{1}\\{\\text{minor allele present at exactly one of }L_A\\text{ or }L_B\\}$, which is defined as $X_C = \\mathbf{1}\\{X_A \\neq X_B\\}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem presents a scenario from biostatistics and then provides explicit mathematical assumptions. There is a potential inconsistency between the biological framing and the mathematical assumption. In a diploid organism under Hardy-Weinberg equilibrium with a minor allele frequency $p=0.5$, the probability of an individual having at least one minor allele is $P(\\text{genotype is Aa or aa}) = 2p(1-p) + p^2 = 2(0.5)(0.5) + (0.5)^2 = 0.5 + 0.25 = 0.75$. This would imply $P(X_A=1) = 0.75$. However, the problem explicitly states to \"Assume... $X_A$ and $X_B$ are... each is $\\text{Bernoulli}(0.5)$\". In the context of abstract problem-solving, an explicit assumption overrides derivations from a framing scenario. The problem is thus a well-defined question in probability theory about three random variables, $X_A$, $X_B$, and $X_C$, whose properties are defined by these assumptions. The problem is scientifically grounded within the domain of probability theory, is well-posed, objective, and contains no other logical flaws.\n\n**Step 3: Verdict and Action**\nThe problem is valid as a mathematical exercise. The solution will proceed based on the given explicit assumptions.\n\n**Derivation**\n\nWe are given that $X_A$ and $X_B$ are independent random variables, with $P(X_A=1) = P(X_A=0) = 0.5$ and $P(X_B=1) = P(X_B=0) = 0.5$. The third variable is $X_C = \\mathbf{1}\\{X_A \\neq X_B\\}$. This can be written as $X_C = |X_A - X_B|$ or $X_C = (X_A + X_B) \\pmod 2$.\n\nFirst, we determine the probability distribution of $X_C$. The event $\\{X_C=1\\}$ occurs if and only if $\\{X_A=1, X_B=0\\}$ or $\\{X_A=0, X_B=1\\}$. These are disjoint events.\n$$P(X_C=1) = P(\\{X_A=1, X_B=0\\} \\cup \\{X_A=0, X_B=1\\})$$\n$$P(X_C=1) = P(X_A=1, X_B=0) + P(X_A=0, X_B=1)$$\nBy the independence of $X_A$ and $X_B$:\n$$P(X_C=1) = P(X_A=1)P(X_B=0) + P(X_A=0)P(X_B=1)$$\n$$P(X_C=1) = (0.5)(0.5) + (0.5)(0.5) = 0.25 + 0.25 = 0.5$$\nThus, $X_C$ is also a Bernoulli random variable with parameter $0.5$, so $P(X_C=0) = 1 - P(X_C=1) = 0.5$.\n\nNext, we check for pairwise independence.\n1.  **Independence of $X_A$ and $X_B$**: This is given in the problem statement.\n\n2.  **Independence of $X_A$ and $X_C$**: We must check if $P(X_A=i, X_C=j) = P(X_A=i)P(X_C=j)$ for all $i, j \\in \\{0, 1\\}$. Let's check the case $i=1, j=1$.\n    $$P(X_A=1, X_C=1) = P(X_A=1 \\text{ and } X_A \\neq X_B)$$\n    This is equivalent to the event $\\{X_A=1, X_B=0\\}$.\n    $$P(X_A=1, X_B=0) = P(X_A=1)P(X_B=0) = (0.5)(0.5) = 0.25$$\n    The product of the marginal probabilities is:\n    $$P(X_A=1)P(X_C=1) = (0.5)(0.5) = 0.25$$\n    Since $P(X_A=1, X_C=1) = P(X_A=1)P(X_C=1)$, this condition holds. All four combinations must be checked for full rigor, but they all hold due to symmetry. For instance, $P(X_A=0, X_C=1) = P(X_A=0, X_B=1) = (0.5)(0.5) = 0.25$, and $P(X_A=0)P(X_C=1) = (0.5)(0.5) = 0.25$. Therefore, $X_A$ and $X_C$ are independent.\n\n3.  **Independence of $X_B$ and $X_C$**: The definition of $X_C$ is symmetric with respect to $X_A$ and $X_B$. Since $X_A$ and $X_B$ have identical distributions, the same argument used for the independence of $X_A$ and $X_C$ applies to $X_B$ and $X_C$. Therefore, $X_B$ and $X_C$ are independent.\n\nConclusion on pairwise independence: $X_A$, $X_B$, and $X_C$ are pairwise independent.\n\nFinally, we check for mutual independence. For mutual independence, it must hold that $P(X_A=i, X_B=j, X_C=k) = P(X_A=i)P(X_B=j)P(X_C=k)$ for all $i, j, k \\in \\{0, 1\\}$.\nLet's examine the event $\\{X_A=1, X_B=1, X_C=1\\}$.\nThe probability of this joint event is:\n$$P(X_A=1, X_B=1, X_C=1) = P(X_A=1, X_B=1 \\text{ and } \\mathbf{1}\\{X_A \\neq X_B\\}=1)$$\nIf $X_A=1$ and $X_B=1$, then $X_A=X_B$, which means $\\mathbf{1}\\{X_A \\neq X_B\\}=0$. Thus, $X_C$ must be $0$. The event $\\{X_A=1, X_B=1, X_C=1\\}$ is an impossible event, so its probability is $0$.\n$$P(X_A=1, X_B=1, X_C=1) = 0$$\nNow, we compute the product of the marginal probabilities:\n$$P(X_A=1)P(X_B=1)P(X_C=1) = (0.5)(0.5)(0.5) = \\left(\\frac{1}{2}\\right)^3 = \\frac{1}{8}$$\nSince $0 \\neq \\frac{1}{8}$, the condition for mutual independence is violated.\n$$P(X_A=1, X_B=1, X_C=1) \\neq P(X_A=1)P(X_B=1)P(X_C=1)$$\nConclusion on mutual independence: $X_A$, $X_B$, and $X_C$ are not mutually independent. This is because knowledge of any two variables determines the value of the third. For example, if $X_A=i$ and $X_B=j$, then $X_C$ is fixed as $|i-j|$.\n\n**Option-by-Option Analysis**\n\nA. $X_A$, $X_B$, and $X_C$ are mutually independent; for example, $P(X_A=1,X_B=1,X_C=1)=\\left(\\tfrac{1}{2}\\right)^3=\\tfrac{1}{8}$.\nThis is **Incorrect**. The variables are not mutually independent. The specific numerical claim is also false; as derived, $P(X_A=1,X_B=1,X_C=1)=0$, not $\\frac{1}{8}$.\n\nB. $X_A$, $X_B$, and $X_C$ are pairwise independent but not mutually independent; specifically, $P(X_A=1,X_B=1,X_C=1)=0$ while $P(X_A=1)P(X_B=1)P(X_C=1)=\\tfrac{1}{8}$.\nThis is **Correct**. The characterization of the independence structure is precisely what was derived. The provided numerical example is the correct justification for the lack of mutual independence.\n\nC. $X_A$ and $X_C$ are not independent because $P(X_C=1\\mid X_A=1)=1$.\nThis is **Incorrect**. As shown above, $X_A$ and $X_C$ are independent. The supporting reason is also factually wrong. We can calculate the conditional probability:\n$$P(X_C=1 \\mid X_A=1) = \\frac{P(X_C=1, X_A=1)}{P(X_A=1)} = \\frac{0.25}{0.5} = 0.5$$\nSince $P(X_C=1 \\mid X_A=1) = 0.5 = P(X_C=1)$, this confirms their independence, directly contradicting the claim.\n\nD. Pairwise independence among $X_A$, $X_B$, and $X_C$ holds only if $P(X_A=1)=P(X_B=1)=\\tfrac{1}{4}$.\nThis is **Incorrect**. We have demonstrated that pairwise independence holds for the given case where $P(X_A=1)=P(X_B=1)=\\frac{1}{2}$. In general, for i.i.d. $X_A, X_B \\sim \\text{Bernoulli}(p)$, pairwise independence with $X_C = X_A \\oplus X_B$ holds if and only if $p=\\frac{1}{2}$ (for the non-trivial cases $p \\in (0,1)$). The condition stated in the option, $p=\\frac{1}{4}$, is false.", "answer": "$$\\boxed{B}$$", "id": "4913391"}, {"introduction": "This final practice bridges the gap between abstract probability theory and the practical realities of data analysis. While the definition of independence, $\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\mathbb{P}(Y=y)$, is exact, real-world data is subject to sampling variability. This computational exercise guides you through simulating data and implementing a statistical check for independence, introducing the essential concepts of empirical estimation and tolerance for random error, which are central to modern biostatistical work [@problem_id:4913365].", "problem": "Consider two binary random variables $X \\in \\{0,1\\}$ and $Y \\in \\{0,1\\}$ representing the presence ($1$) or absence ($0$) of two distinct biomarkers recorded per trial in a biostatistical simulation study. The foundational base is the axiomatic definition of probability on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, the definition of independence of random variables, and the frequency-based estimator of probabilities. Independence is defined as $X$ independent of $Y$ if and only if for all $x \\in \\{0,1\\}$ and $y \\in \\{0,1\\}$, the equality $\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\,\\mathbb{P}(Y=y)$ holds.\n\nYou will generate synthetic data across repeated trials, estimate empirical probabilities from the generated data, and verify independence by demonstrating equality of joint and product marginals to within a specified tolerance. For each parameter set, simulate $N$ independent and identically distributed trials producing pairs $(X_t, Y_t)$ for $t = 1, 2, \\ldots, N$. Let the empirical joint probabilities be\n$$\n\\hat{p}_{ij} = \\frac{1}{N}\\sum_{t=1}^{N} \\mathbf{1}\\{X_t=i, Y_t=j\\}, \\quad i,j \\in \\{0,1\\},\n$$\nand let the empirical marginals be\n$$\n\\hat{p}_{i\\cdot} = \\sum_{j=0}^{1}\\hat{p}_{ij} \\quad \\text{and} \\quad \\hat{p}_{\\cdot j} = \\sum_{i=0}^{1}\\hat{p}_{ij}.\n$$\nDefine the verification statistic\n$$\nD = \\max_{i \\in \\{0,1\\},\\, j \\in \\{0,1\\}} \\left| \\hat{p}_{ij} - \\hat{p}_{i\\cdot}\\,\\hat{p}_{\\cdot j} \\right|.\n$$\nFor a user-specified tolerance $\\varepsilon  0$, declare independence verified if and only if $D \\le \\varepsilon$.\n\nSimulation models:\n- Independent Bernoulli marginals: draw $X_t \\sim \\text{Bernoulli}(p_X)$ and $Y_t \\sim \\text{Bernoulli}(p_Y)$ independently for each $t$.\n- Specified joint distribution with given marginals and dependence parameter: construct a joint probability table using\n$$\np_{11} = p_X p_Y + c,\\quad\np_{10} = p_X(1 - p_Y) - c,\\quad\np_{01} = (1 - p_X)p_Y - c,\\quad\np_{00} = (1 - p_X)(1 - p_Y) + c,\n$$\nsubject to the constraint that all $p_{ij} \\ge 0$ and $\\sum_{i,j} p_{ij} = 1$. When $c \\neq 0$ and the constraints hold, this yields a dependent model with fixed marginals $p_X$ and $p_Y$.\n\nYour task is to implement a program that, for each parameter set in the test suite below, performs the simulation, computes $D$, and outputs a boolean indicating whether independence is verified to within the supplied $\\varepsilon$. The program must be self-contained and must not read any input.\n\nThe Random Number Generator (RNG) must be seeded per test case for reproducibility. Use an RNG that produces independent draws across the $N$ trials and adheres to the specified model.\n\nTest suite parameter sets (each test case is a tuple of parameters):\n1. $(\\text{model}=\\text{independent},\\, N=200000,\\, p_X=0.3,\\, p_Y=0.6,\\, \\varepsilon=0.002,\\, \\text{seed}=12345)$\n2. $(\\text{model}=\\text{dependent},\\, N=200000,\\, p_X=0.3,\\, p_Y=0.6,\\, c=0.05,\\, \\varepsilon=0.002,\\, \\text{seed}=54321)$\n3. $(\\text{model}=\\text{independent},\\, N=500,\\, p_X=0.3,\\, p_Y=0.6,\\, \\varepsilon=0.06,\\, \\text{seed}=111)$\n4. $(\\text{model}=\\text{independent},\\, N=50000,\\, p_X=0.02,\\, p_Y=0.03,\\, \\varepsilon=0.001,\\, \\text{seed}=222)$\n5. $(\\text{model}=\\text{dependent},\\, N=200000,\\, p_X=0.02,\\, p_Y=0.03,\\, c=0.0004,\\, \\varepsilon=0.0003,\\, \\text{seed}=333)$\n\nNotes:\n- For the dependent model, the parameter $c$ must satisfy\n$$\n- \\min\\left\\{p_X(1-p_Y),\\, (1-p_X)p_Y \\right\\} \\le c \\le \\min\\left\\{p_X p_Y,\\, (1-p_X)(1-p_Y) \\right\\}\n$$\nto maintain nonnegativity of all $p_{ij}$.\n- The outputs are booleans. There are no physical units, no angles, and no percentages; all probabilities must be handled as decimals in $[0,1]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,\\text{ }result_2,\\text{ }result_3,\\text{ }result_4,\\text{ }result_5]$), where each $result_k$ is a boolean indicating whether independence is verified under the $k$-th parameter set.", "solution": "The problem requires the verification of statistical independence between two binary random variables, $X, Y \\in \\{0,1\\}$, through Monte Carlo simulation. The process is grounded in the axiomatic foundations of probability theory and employs standard statistical estimation techniques.\n\nThe theoretical basis for this problem is the definition of independence for two random variables. On a given probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, two random variables $X$ and $Y$ are said to be independent if and only if their joint probability distribution function is equal to the product of their marginal distribution functions. For discrete binary variables, this condition simplifies to:\n$$\n\\mathbb{P}(X=i, Y=j) = \\mathbb{P}(X=i)\\,\\mathbb{P}(Y=j) \\quad \\forall i,j \\in \\{0,1\\}\n$$\nLet $p_{ij} = \\mathbb{P}(X=i, Y=j)$ denote the joint probabilities, and let $p_{i\\cdot} = \\mathbb{P}(X=i)$ and $p_{\\cdot j} = \\mathbb{P}(Y=j)$ denote the marginal probabilities. The independence condition is thus $p_{ij} = p_{i\\cdot} p_{\\cdot j}$ for all $i,j$.\n\nIn a simulation study, the true probabilities $\\{p_{ij}\\}$ are defined by the data-generating model. We draw a sample of size $N$ of independent and identically distributed pairs $(X_t, Y_t)_{t=1}^N$. From this sample, we estimate the true probabilities using the frequency-based estimators. The empirical joint probabilities, $\\hat{p}_{ij}$, are calculated as the proportion of trials that result in the outcome $(i,j)$:\n$$\n\\hat{p}_{ij} = \\frac{1}{N} \\sum_{t=1}^{N} \\mathbf{1}\\{X_t=i, Y_t=j\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The empirical marginal probabilities, $\\hat{p}_{i\\cdot}$ and $\\hat{p}_{\\cdot j}$, are subsequently calculated by summing over the empirical joint probabilities, in accordance with the law of total probability:\n$$\n\\hat{p}_{i\\cdot} = \\sum_{j=0}^{1} \\hat{p}_{ij} = \\hat{p}_{i0} + \\hat{p}_{i1} \\quad \\text{and} \\quad \\hat{p}_{\\cdot j} = \\sum_{i=0}^{1} \\hat{p}_{ij} = \\hat{p}_{0j} + \\hat{p}_{1j}\n$$\nDue to finite sample size $N$, sampling variability will cause the estimated probabilities to deviate from their true values. Consequently, even if the underlying data-generating process is perfectly independent, we do not expect the equality $\\hat{p}_{ij} = \\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j}$ to hold exactly. Instead, we assess whether the deviation from this equality is small. The verification statistic $D$ is defined as the maximum absolute deviation over all four possible outcomes:\n$$\nD = \\max_{i,j \\in \\{0,1\\}} |\\hat{p}_{ij} - \\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j}|\n$$\nIndependence is considered \"verified\" if this maximum deviation is less than or equal to a specified tolerance, $\\varepsilon$, i.e., $D \\le \\varepsilon$.\n\nThe problem specifies two simulation models:\n1.  **Independent Model**: Samples for $X_t$ and $Y_t$ are drawn independently from Bernoulli distributions with parameters $p_X$ and $p_Y$, respectively. By construction, the true joint probabilities are $p_{ij} = p_{i\\cdot}p_{\\cdot j}$, so any measured deviation $D  0$ arises purely from sampling error.\n2.  **Dependent Model**: A joint probability distribution is constructed using marginal probabilities $p_X$, $p_Y$, and a dependence parameter $c$. The joint probabilities are given by:\n    $p_{11} = p_X p_Y + c$,\n    $p_{10} = p_X(1-p_Y) - c$,\n    $p_{01} = (1-p_X)p_Y - c$,\n    $p_{00} = (1-p_X)(1-p_Y) + c$.\n    This construction preserves the marginals, i.e., $p_{1\\cdot} = p_{11}+p_{10} = p_X$ and $p_{\\cdot 1} = p_{11}+p_{01} = p_Y$. The parameter $c$ is equal to the covariance between $X$ and $Y$, $\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = p_{11} - p_X p_Y = c$. When $c \\neq 0$, the variables are dependent, and the theoretical deviation is $|c|$.\n\nThe algorithmic implementation proceeds on a case-by-case basis. For each test case, the random number generator is seeded for reproducibility.\n-   Data Generation: For the independent model, two separate arrays of random numbers are generated. For the dependent model, the four joint probabilities $\\{p_{00}, p_{01}, p_{10}, p_{11}\\}$ are first calculated. These probabilities define a categorical distribution over the four outcomes $\\{(0,0), (0,1), (1,0), (1,1)\\}$. $N$ samples are drawn from this distribution. A vectorized approach maps the sampled category index (from $0$ to $3$) back to the corresponding $(X,Y)$ pair.\n-   Computation of $D$: A $2 \\times 2$ contingency table of counts is efficiently constructed from the generated $X$ and $Y$ data. This table is normalized by $N$ to produce the empirical joint probability matrix, $\\hat{P} = (\\hat{p}_{ij})$. The marginal probability vectors, $\\hat{p}_{i\\cdot}$ and $\\hat{p}_{\\cdot j}$, are found by summing the rows and columns of $\\hat{P}$, respectively. The matrix of products of marginals, $(\\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j})$, is computed via the outer product of these vectors. Finally, $D$ is obtained by finding the maximum element in the matrix of absolute differences between $\\hat{P}$ and the product-of-marginals matrix. The resulting $D$ is compared to $\\varepsilon$ to produce the final boolean output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for each test case\n    and verifying independence based on the specified criterion.\n    \"\"\"\n\n    test_cases = [\n        ('independent', 200000, 0.3, 0.6, 0.002, 12345),\n        ('dependent', 200000, 0.3, 0.6, 0.05, 0.002, 54321),\n        ('independent', 500, 0.3, 0.6, 0.06, 111),\n        ('independent', 50000, 0.02, 0.03, 0.001, 222),\n        ('dependent', 200000, 0.02, 0.03, 0.0004, 0.0003, 333)\n    ]\n\n    results = []\n\n    def _calculate_verification(X, Y, N, epsilon):\n        \"\"\"\n        Common logic to calculate the verification statistic D and check against epsilon.\n        \"\"\"\n        # Create a 2x2 contingency table of counts\n        counts = np.zeros((2, 2), dtype=np.int64)\n        np.add.at(counts, (X, Y), 1)\n\n        # Calculate empirical joint probabilities\n        p_hat_ij = counts / N\n\n        # Calculate empirical marginal probabilities\n        p_hat_i_dot = np.sum(p_hat_ij, axis=1)\n        p_hat_dot_j = np.sum(p_hat_ij, axis=0)\n\n        # Calculate the matrix of product-of-marginals\n        p_hat_prod = np.outer(p_hat_i_dot, p_hat_dot_j)\n\n        # Calculate the verification statistic D\n        D = np.max(np.abs(p_hat_ij - p_hat_prod))\n\n        # Check if D is within the tolerance epsilon\n        return D = epsilon\n\n    for case in test_cases:\n        model = case[0]\n        seed = case[-1]\n        epsilon = case[-2]\n        N = case[1]\n        p_X = case[2]\n        p_Y = case[3]\n        \n        # Initialize the random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        if model == 'independent':\n            # Generate X and Y from independent Bernoulli distributions\n            X = (rng.random(N)  p_X).astype(np.int8)\n            Y = (rng.random(N)  p_Y).astype(np.int8)\n        \n        elif model == 'dependent':\n            c = case[4]\n            \n            # Calculate the joint probability distribution\n            p11 = p_X * p_Y + c\n            p10 = p_X * (1 - p_Y) - c\n            p01 = (1 - p_X) * p_Y - c\n            p00 = 1.0 - p11 - p10 - p01\n            \n            # Probabilities for outcomes (0,0), (0,1), (1,0), (1,1)\n            probs = [p00, p01, p10, p11]\n            \n            # Draw N samples from the categorical distribution\n            choices = rng.choice(4, size=N, p=probs)\n            \n            # Map choice indices to (X, Y) pairs efficiently\n            # choice 0 - (0,0), 1 - (0,1), 2 - (1,0), 3 - (1,1)\n            X = (choices // 2).astype(np.int8)\n            Y = (choices % 2).astype(np.int8)\n\n        # Calculate the result for the current case\n        result = _calculate_verification(X, Y, N, epsilon)\n        results.append(result.item())\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "4913365"}]}