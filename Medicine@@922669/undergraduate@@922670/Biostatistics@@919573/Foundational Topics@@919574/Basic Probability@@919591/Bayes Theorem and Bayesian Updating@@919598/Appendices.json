{"hands_on_practices": [{"introduction": "A core task in Bayesian analysis is updating our knowledge about a continuous parameter, such as the mean effect of a treatment. This exercise guides you through the fundamental mechanics of this process using the Normal-Normal conjugate model, a workhorse in biostatistics. By deriving the posterior distribution from first principles, you will gain a deep understanding of how prior beliefs and observed data are mathematically combined to produce an updated state of knowledge [@problem_id:4896445].", "problem": "A clinical study evaluates the change in systolic blood pressure (in millimeters of mercury (mm Hg)) after a four-week intervention. Let the changes for independent participants be modeled as $y_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$, where $\\mu$ is the unknown population mean change and $\\sigma^2$ is known from prior calibration. Suppose a prior belief about $\\mu$ is $\\mu \\sim \\mathcal{N}(m_0,s_0^2)$ based on earlier trials.\n\nIn this study, the known calibration variance is $\\sigma^2 = 16$ and the prior parameters are $m_0 = -2$ and $s_0^2 = 9$. The observed changes are the $n=8$ values\n$$\n\\{4,\\,-1,\\,3,\\,2,\\,0,\\,1,\\,1,\\,2\\}.\n$$\n\nStarting from Bayes' theorem and the standard form of the normal probability density function, use first principles (i.e., write the likelihood $p(y\\mid \\mu)$ and the prior $p(\\mu)$, and then algebraically complete the square in $\\mu$) to derive the posterior density $p(\\mu\\mid y)$, identify its family, and express its parameters as functions of $m_0$, $s_0^2$, $\\sigma^2$, and the observed data. Then, using the provided numerical values, compute the posterior mean of $\\mu$ for this dataset.\n\nExpress the final numerical answer (the posterior mean) in millimeters of mercury and round your answer to four significant figures.", "solution": "The problem requires the derivation of the posterior distribution for the population mean $\\mu$ from first principles, followed by a numerical calculation of the posterior mean.\n\nLet the observed data be $y = \\{y_1, y_2, \\dots, y_n\\}$. The model specifies that the data points $y_i$ are independent and identically distributed according to a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$.\n$$ y_i \\mid \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2) $$\nThe prior belief about $\\mu$ is also modeled by a normal distribution with mean $m_0$ and variance $s_0^2$.\n$$ \\mu \\sim \\mathcal{N}(m_0, s_0^2) $$\n\nAccording to Bayes' theorem, the posterior probability density function (PDF) $p(\\mu \\mid y)$ is proportional to the product of the likelihood function $p(y \\mid \\mu)$ and the prior PDF $p(\\mu)$.\n$$ p(\\mu \\mid y) \\propto p(y \\mid \\mu) p(\\mu) $$\n\nFirst, we write the PDF for the prior distribution of $\\mu$:\n$$ p(\\mu) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left(-\\frac{(\\mu - m_0)^2}{2s_0^2}\\right) $$\nSince the observations $y_i$ are independent, the likelihood function is the product of the individual densities:\n$$ p(y \\mid \\mu) = \\prod_{i=1}^{n} p(y_i \\mid \\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) $$\nNow, we find the posterior PDF by multiplying the prior and the likelihood. We can drop any constants that do not depend on $\\mu$.\n$$ p(\\mu \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - \\mu)^2\\right) \\exp\\left(-\\frac{(\\mu - m_0)^2}{2s_0^2}\\right) $$\nCombining the exponents gives:\n$$ p(\\mu \\mid y) \\propto \\exp\\left[-\\frac{1}{2}\\left(\\frac{\\sum_{i=1}^{n}(y_i - \\mu)^2}{\\sigma^2} + \\frac{(\\mu - m_0)^2}{s_0^2}\\right)\\right] $$\nTo identify the posterior distribution, we expand the terms in the exponent and treat it as a quadratic function of $\\mu$. Let $E$ be the term inside the parentheses:\n$$ E = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_i^2 - 2y_i\\mu + \\mu^2) + \\frac{1}{s_0^2}(\\mu^2 - 2m_0\\mu + m_0^2) $$\nWe collect terms involving $\\mu^2$ and $\\mu$:\n$$ E = \\frac{1}{\\sigma^2}(n\\mu^2 - 2\\mu\\sum_{i=1}^{n}y_i + \\sum_{i=1}^{n}y_i^2) + \\frac{1}{s_0^2}(\\mu^2 - 2\\mu m_0 + m_0^2) $$\n$$ E = \\mu^2\\left(\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}\\right) - 2\\mu\\left(\\frac{\\sum_{i=1}^{n}y_i}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) + \\left(\\frac{\\sum_{i=1}^{n}y_i^2}{\\sigma^2} + \\frac{m_0^2}{s_0^2}\\right) $$\nLet $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$ be the sample mean. The expression for $E$ can be written as:\n$$ E = \\mu^2\\left(\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}\\right) - 2\\mu\\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) + C $$\nwhere $C$ contains all terms not depending on $\\mu$.\nThe structure of the exponent, a quadratic in $\\mu$, shows that the posterior distribution is also normal. Let the posterior be $\\mu \\mid y \\sim \\mathcal{N}(m_n, s_n^2)$. Its PDF is proportional to $\\exp\\left(-\\frac{(\\mu-m_n)^2}{2s_n^2}\\right)$. Expanding the exponent:\n$$ -\\frac{(\\mu-m_n)^2}{2s_n^2} = -\\frac{1}{2s_n^2}(\\mu^2 - 2m_n\\mu + m_n^2) = -\\frac{1}{2}\\left(\\frac{1}{s_n^2}\\mu^2 - \\frac{2m_n}{s_n^2}\\mu + \\frac{m_n^2}{s_n^2}\\right) $$\nBy comparing the coefficients of $\\mu^2$ and $\\mu$ in our two expressions for the exponent, we can identify the posterior parameters $m_n$ and $s_n^2$.\nComparing the $\\mu^2$ coefficients:\n$$ \\frac{1}{s_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{s_0^2} $$\nThis gives the posterior precision (inverse variance). The posterior variance $s_n^2$ is:\n$$ s_n^2 = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1} $$\nComparing the $\\mu$ coefficients:\n$$ \\frac{m_n}{s_n^2} = \\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2} $$\nSolving for the posterior mean $m_n$:\n$$ m_n = s_n^2 \\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}} $$\nThus, the posterior distribution $p(\\mu \\mid y)$ is a normal distribution with mean $m_n$ and variance $s_n^2$ as derived above.\n\nNow, we substitute the provided numerical values to compute the posterior mean.\nThe given data are:\nKnown variance: $\\sigma^2 = 16$.\nPrior parameters: $m_0 = -2$ and $s_0^2 = 9$.\nSample size: $n=8$.\nObserved changes: $y = \\{4, -1, 3, 2, 0, 1, 1, 2\\}$.\n\nFirst, we calculate the sample mean $\\bar{y}$:\n$$ \\sum_{i=1}^{8} y_i = 4 - 1 + 3 + 2 + 0 + 1 + 1 + 2 = 12 $$\n$$ \\bar{y} = \\frac{\\sum_{i=1}^{8} y_i}{n} = \\frac{12}{8} = 1.5 $$\nNext, we substitute these values into the formula for the posterior mean $m_n$:\n$$ m_n = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\frac{m_0}{s_0^2}}{\\frac{n}{\\sigma^2} + \\frac{1}{s_0^2}} = \\frac{\\frac{8 \\times 1.5}{16} + \\frac{-2}{9}}{\\frac{8}{16} + \\frac{1}{9}} $$\nLet's compute the numerator and denominator separately.\nDenominator:\n$$ \\frac{8}{16} + \\frac{1}{9} = \\frac{1}{2} + \\frac{1}{9} = \\frac{9}{18} + \\frac{2}{18} = \\frac{11}{18} $$\nNumerator:\n$$ \\frac{8 \\times 1.5}{16} + \\frac{-2}{9} = \\frac{12}{16} - \\frac{2}{9} = \\frac{3}{4} - \\frac{2}{9} = \\frac{27}{36} - \\frac{8}{36} = \\frac{19}{36} $$\nNow, we compute the ratio:\n$$ m_n = \\frac{\\frac{19}{36}}{\\frac{11}{18}} = \\frac{19}{36} \\times \\frac{18}{11} = \\frac{19}{2 \\times 11} = \\frac{19}{22} $$\nFinally, we convert this fraction to a decimal and round to four significant figures:\n$$ m_n = \\frac{19}{22} \\approx 0.86363636\\dots $$\nRounding to four significant figures, we get $0.8636$. The units are millimeters of mercury (mm Hg).\nThe posterior mean of $\\mu$ is $0.8636$ mm Hg.", "answer": "$$\\boxed{0.8636}$$", "id": "4896445"}, {"introduction": "Biostatistical studies often involve count data, such as the number of adverse events or disease cases, which are modeled using the Poisson distribution. This practice explores the Gamma-Poisson conjugate model, demonstrating the versatility of the Bayesian method beyond continuous data. You will see how the same logic of combining a prior with a likelihood applies, reinforcing the concept of conjugacy with a different family of distributions [@problem_id:4896487].", "problem": "In a biostatistical study of adverse event counts per patient-day, suppose the counts per day for a given ward are modeled as independent and identically distributed (i.i.d.) observations $y_{1},\\dots,y_{n}$ from a Poisson distribution with common rate parameter $\\lambda$, that is, $y_{i} \\sim \\mathrm{Poisson}(\\lambda)$ for $i=1,\\dots,n$. Prior belief about $\\lambda$ is specified by a Gamma distribution with shape-rate parameterization, $\\lambda \\sim \\mathrm{Gamma}(a,b)$, where $a>0$ and $b>0$. The Gamma distribution in shape-rate form has probability density function (PDF)\n$$\nf(\\lambda \\mid a,b) \\;=\\; \\frac{b^{a}}{\\Gamma(a)}\\,\\lambda^{a-1}\\exp(-b\\lambda), \\quad \\lambda>0.\n$$\nUsing Bayes’ theorem and starting from the definitions of the Poisson probability mass function and the Gamma prior density given above, derive the posterior distribution of $\\lambda$ given the data $y_{1},\\dots,y_{n}$. Express your final answer as a named distribution with its updated parameters in terms of $a$, $b$, $n$, and $\\sum_{i=1}^{n} y_{i}$. Your final answer must be a single closed-form analytic expression. No rounding is required.", "solution": "The objective is to find the posterior distribution of the parameter $\\lambda$ given the observed data $\\mathbf{y} = (y_{1}, \\dots, y_{n})$. According to Bayes' theorem for a continuous parameter, the posterior probability density function, $p(\\lambda \\mid \\mathbf{y})$, is proportional to the product of the likelihood function, $L(\\lambda \\mid \\mathbf{y})$, and the prior probability density function, $p(\\lambda)$.\n\n$$p(\\lambda \\mid \\mathbf{y}) \\propto L(\\lambda \\mid \\mathbf{y}) \\, p(\\lambda)$$\n\nFirst, we construct the likelihood function. Since the observations $y_{i}$ are i.i.d. draws from a $\\mathrm{Poisson}(\\lambda)$ distribution, the joint probability mass function of the data, which serves as the likelihood function, is the product of the individual PMFs:\n\n$$L(\\lambda \\mid y_{1}, \\dots, y_{n}) = P(y_1, \\dots, y_n \\mid \\lambda) = \\prod_{i=1}^{n} P(y_i \\mid \\lambda)$$\n\nThe PMF for a single observation $y_i$ from a $\\mathrm{Poisson}(\\lambda)$ distribution is:\n$$P(y_i \\mid \\lambda) = \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}$$\n\nSubstituting this into the product gives:\n$$L(\\lambda \\mid y_{1}, \\dots, y_{n}) = \\prod_{i=1}^{n} \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} = \\frac{\\left(\\prod_{i=1}^{n} \\lambda^{y_i}\\right) \\left(\\prod_{i=1}^{n} \\exp(-\\lambda)\\right)}{\\prod_{i=1}^{n} y_i!} = \\frac{\\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n\\lambda)}{\\prod_{i=1}^{n} y_i!}$$\n\nFor the purpose of finding the posterior distribution of $\\lambda$, any terms that do not depend on $\\lambda$ can be treated as part of the normalizing constant. Therefore, the likelihood is proportional to:\n$$L(\\lambda \\mid y_{1}, \\dots, y_{n}) \\propto \\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n\\lambda)$$\n\nNext, we identify the prior distribution. The problem states that the prior for $\\lambda$ is a Gamma distribution, $\\lambda \\sim \\mathrm{Gamma}(a,b)$, with the PDF:\n$$p(\\lambda) = f(\\lambda \\mid a,b) = \\frac{b^{a}}{\\Gamma(a)}\\,\\lambda^{a-1}\\exp(-b\\lambda)$$\n\nAs a function of $\\lambda$, the prior is proportional to:\n$$p(\\lambda) \\propto \\lambda^{a-1}\\exp(-b\\lambda)$$\n\nNow, we apply Bayes' theorem by multiplying the likelihood and the prior:\n$$p(\\lambda \\mid y_{1}, \\dots, y_{n}) \\propto \\left( \\lambda^{\\sum_{i=1}^{n} y_i} \\exp(-n\\lambda) \\right) \\cdot \\left( \\lambda^{a-1}\\exp(-b\\lambda) \\right)$$\n\nWe combine the terms involving $\\lambda$ by adding the exponents:\n$$p(\\lambda \\mid y_{1}, \\dots, y_{n}) \\propto \\lambda^{\\left(\\sum_{i=1}^{n} y_i\\right) + a - 1} \\exp(-n\\lambda - b\\lambda)$$\n$$p(\\lambda \\mid y_{1}, \\dots, y_{n}) \\propto \\lambda^{\\left(a + \\sum_{i=1}^{n} y_i\\right) - 1} \\exp\\left(-(b+n)\\lambda\\right)$$\n\nThis resulting expression is the kernel of a probability density function for $\\lambda$. We recognize its form. A Gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$, notated as $\\mathrm{Gamma}(\\alpha, \\beta)$, has a PDF whose kernel is of the form $\\lambda^{\\alpha-1} \\exp(-\\beta\\lambda)$.\n\nBy comparing our derived posterior kernel, $\\lambda^{\\left(a + \\sum_{i=1}^{n} y_i\\right) - 1} \\exp\\left(-(b+n)\\lambda\\right)$, to the general Gamma kernel, we can identify the parameters of the posterior distribution.\nThe new shape parameter, which we can call $a'$, is:\n$$a' = a + \\sum_{i=1}^{n} y_i$$\nThe new rate parameter, which we can call $b'$, is:\n$$b' = b+n$$\n\nTherefore, the posterior distribution of $\\lambda$ given the data $y_{1}, \\dots, y_{n}$ is a Gamma distribution with these updated parameters. Formally, we write this as:\n$$\\lambda \\mid y_{1}, \\dots, y_{n} \\sim \\mathrm{Gamma}\\left(a + \\sum_{i=1}^{n} y_i, b+n\\right)$$\nThis result demonstrates the concept of conjugacy, where the posterior distribution (Gamma) belongs to the same family as the prior distribution (Gamma). The parameters of the prior, $(a, b)$, are updated by the information from the data, summarized by the sufficient statistics $n$ (the number of observations) and $\\sum_{i=1}^{n} y_i$ (the total number of events).", "answer": "$$\n\\boxed{\\mathrm{Gamma}\\left(a + \\sum_{i=1}^{n} y_i, b+n\\right)}\n$$", "id": "4896487"}, {"introduction": "A key advantage of Bayesian inference is its ability to move from parameter estimation to prediction. This exercise uses the classic Beta-Binomial model for proportions to derive the posterior predictive probability—the probability of a future success, averaged over your updated uncertainty about the success rate $p$. This practice demonstrates how to leverage the full posterior distribution to make tangible forecasts, a crucial step in scientific and clinical decision-making [@problem_id:4896497].", "problem": "A biostatistician is modeling the probability that a new patient exhibits a binary clinical outcome labeled \"success\" in a molecular assay. Let the unknown success probability be denoted by $p$, and let the data consist of $n$ independent and identically distributed binary outcomes with $y$ observed successes. The prior belief about $p$ is modeled by a Beta distribution with parameters $\\alpha$ and $\\beta$, written $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$. The observed data likelihood, given $p$, follows the Binomial model.\n\nUsing Bayes' theorem and integrating out the parameter $p$, derive from first principles the posterior predictive probability that the next, new Bernoulli trial will be a success, expressed as a single closed-form analytic expression in terms of $\\alpha$, $\\beta$, $y$, and $n$. Do not substitute numerical values. No rounding is required. Provide the final expression only, simplified as far as possible.", "solution": "The problem requires the derivation of the posterior predictive probability of a success for a new Bernoulli trial, given a set of existing observations. Let the unknown success probability be $p$. The prior belief about $p$ is described by a Beta distribution, and the data likelihood is modeled by a Binomial distribution. This is a classic application of Bayesian inference using conjugate priors.\n\nLet $D$ represent the observed data, which consists of $y$ successes in $n$ independent and identically distributed (i.i.d.) trials. We are given:\n1.  **Prior Distribution**: The prior probability density function (PDF) for $p$ is a Beta distribution with parameters $\\alpha$ and $\\beta$.\n    $$f(p|\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1} (1-p)^{\\beta-1}$$\n    where $\\Gamma(\\cdot)$ is the Gamma function. This is denoted as $p \\sim \\mathrm{Beta}(\\alpha, \\beta)$.\n\n2.  **Likelihood Function**: The probability of observing $y$ successes in $n$ trials, given the success probability $p$, follows a Binomial distribution. The likelihood function is:\n    $$L(p|D) = P(y, n|p) = \\binom{n}{y} p^y (1-p)^{n-y}$$\n\nOur goal is to find the posterior predictive probability of a success in a new trial, which we denote as $\\tilde{y}=1$. This probability is conditioned on the observed data $D$. By the law of total probability, we can express this by integrating over all possible values of the unknown parameter $p$, weighted by its posterior probability density.\n$$P(\\tilde{y}=1|D) = \\int_0^1 P(\\tilde{y}=1|p, D) f(p|D) dp$$\nSince the trials are i.i.d. conditional on $p$, the probability of the next outcome depends only on $p$, not on the past data $D$. Thus, $P(\\tilde{y}=1|p, D) = P(\\tilde{y}=1|p)$. For a single Bernoulli trial with success probability $p$, this is simply $p$.\n$$P(\\tilde{y}=1|D) = \\int_0^1 p \\cdot f(p|D) dp$$\nThis integral is, by definition, the expected value of $p$ with respect to its posterior distribution, $E[p|D]$.\n\nThe first step is to derive the posterior distribution, $f(p|D)$, using Bayes' theorem:\n$$f(p|D) \\propto L(p|D) \\cdot f(p|\\alpha, \\beta)$$\nSubstituting the expressions for the likelihood and prior, and ignoring the normalization constants that do not depend on $p$:\n$$f(p|D) \\propto \\left(p^y (1-p)^{n-y}\\right) \\cdot \\left(p^{\\alpha-1} (1-p)^{\\beta-1}\\right)$$\nCombining the terms with base $p$ and $(1-p)$:\n$$f(p|D) \\propto p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1}$$\nThis functional form is the kernel of a Beta distribution. We can identify the updated parameters for the posterior distribution:\nThe posterior shape parameter $\\alpha'$ is $y+\\alpha$.\nThe posterior shape parameter $\\beta'$ is $n-y+\\beta$.\nThus, the posterior distribution of $p$ given the data $D$ is a Beta distribution:\n$$p|D \\sim \\mathrm{Beta}(y+\\alpha, n-y+\\beta)$$\nThe normalized posterior PDF is:\n$$f(p|D) = \\frac{\\Gamma((y+\\alpha)+(n-y+\\beta))}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1}$$\n$$f(p|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1}$$\nNow we compute the posterior predictive probability by calculating the expectation $E[p|D]$:\n$$P(\\tilde{y}=1|D) = \\int_0^1 p \\cdot f(p|D) dp$$\n$$= \\int_0^1 p \\cdot \\left( \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1} \\right) dp$$\nWe can pull the constant normalization factor out of the integral:\n$$= \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} \\int_0^1 p \\cdot p^{y+\\alpha-1} (1-p)^{n-y+\\beta-1} dp$$\n$$= \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} \\int_0^1 p^{y+\\alpha} (1-p)^{n-y+\\beta-1} dp$$\nThe integral is recognized as the Beta function, $B(a,b) = \\int_0^1 t^{a-1}(1-t)^{b-1}dt = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$. In our case, the parameters for the Beta function in the integral are $a=y+\\alpha+1$ and $b=n-y+\\beta$.\n$$\\int_0^1 p^{(y+\\alpha+1)-1} (1-p)^{(n-y+\\beta)-1} dp = B(y+\\alpha+1, n-y+\\beta)$$\n$$= \\frac{\\Gamma(y+\\alpha+1)\\Gamma(n-y+\\beta)}{\\Gamma((y+\\alpha+1)+(n-y+\\beta))}$$\nSubstituting this back into the expression for the posterior predictive probability:\n$$P(\\tilde{y}=1|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)} \\cdot \\frac{\\Gamma(y+\\alpha+1)\\Gamma(n-y+\\beta)}{\\Gamma(n+\\alpha+\\beta+1)}$$\nWe can cancel the term $\\Gamma(n-y+\\beta)$.\n$$P(\\tilde{y}=1|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)} \\cdot \\frac{\\Gamma(y+\\alpha+1)}{\\Gamma(n+\\alpha+\\beta+1)}$$\nUsing the fundamental property of the Gamma function, $\\Gamma(z+1) = z\\Gamma(z)$, we can rewrite $\\Gamma(y+\\alpha+1)$ and $\\Gamma(n+\\alpha+\\beta+1)$:\n$$\\Gamma(y+\\alpha+1) = (y+\\alpha)\\Gamma(y+\\alpha)$$\n$$\\Gamma(n+\\alpha+\\beta+1) = (n+\\alpha+\\beta)\\Gamma(n+\\alpha+\\beta)$$\nSubstituting these into the expression:\n$$P(\\tilde{y}=1|D) = \\frac{\\Gamma(n+\\alpha+\\beta)}{\\Gamma(y+\\alpha)} \\cdot \\frac{(y+\\alpha)\\Gamma(y+\\alpha)}{(n+\\alpha+\\beta)\\Gamma(n+\\alpha+\\beta)}$$\nNow, we can cancel the terms $\\Gamma(n+\\alpha+\\beta)$ and $\\Gamma(y+\\alpha)$:\n$$P(\\tilde{y}=1|D) = \\frac{y+\\alpha}{n+\\alpha+\\beta}$$\nThis is the final, simplified closed-form expression for the posterior predictive probability that the next trial will be a success. It represents the mean of the posterior Beta distribution.", "answer": "$$\\boxed{\\frac{y+\\alpha}{n+\\alpha+\\beta}}$$", "id": "4896497"}]}