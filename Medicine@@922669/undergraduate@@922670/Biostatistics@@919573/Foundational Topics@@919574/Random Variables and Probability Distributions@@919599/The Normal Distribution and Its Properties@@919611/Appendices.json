{"hands_on_practices": [{"introduction": "In biostatistics, evaluating very small p-values often requires understanding the tail behavior of the normal distribution. This practice challenges you to use calculus to derive famous and elegant bounds for the normal tail probability, often related to what is known as Mills' ratio. By working through this derivation, you will gain a deeper appreciation for the mathematical properties of the normal distribution and the theoretical foundation for approximations used in analyzing extreme statistical results [@problem_id:4960564].", "problem": "A biostatistician is evaluating extreme one-sided p-values arising from standardized test statistics in a large-sample gene expression study. Under the null hypothesis, the test statistic is well-approximated by a standard normal random variable with cumulative distribution function $\\Phi(z)$ and probability density function $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right)$. The survival function is $\\bar{\\Phi}(z) = 1 - \\Phi(z) = \\int_{z}^{\\infty} \\phi(t)\\,dt$ for $z  0$. Practitioners often approximate $\\bar{\\Phi}(z)$ by $\\phi(z)/z$ to rapidly screen extreme values.\n\nStarting only from the standard normal density $\\phi(z)$, its derivative, and the integral representation of $\\bar{\\Phi}(z)$, derive an explicit upper bound and an explicit lower bound on $\\bar{\\Phi}(z)$ that are functions of $z0$ alone. Then, use these bounds to obtain a closed-form upper bound on the relative error of the approximation $\\bar{\\Phi}(z) \\approx \\phi(z)/z$, defined by\n$$\ne(z) \\equiv \\frac{\\frac{\\phi(z)}{z} - \\bar{\\Phi}(z)}{\\bar{\\Phi}(z)} \\quad \\text{for } z0.\n$$\nProvide your final answer as a single simplified analytic expression in $z$. No numerical evaluation is required; do not round. The final answer must be a single closed-form expression.", "solution": "The objective is to first derive upper and lower bounds for the standard normal survival function $\\bar{\\Phi}(z)$ for $z0$, and then use these bounds to find an upper bound on the relative error $e(z)$ of a common approximation.\n\nThe survival function is defined by the integral $\\bar{\\Phi}(z) = \\int_{z}^{\\infty} \\phi(t) dt$, where $\\phi(t)$ is the standard normal probability density function (PDF), $\\phi(t) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^{2}}{2}\\right)$. The derivative of the PDF is $\\phi'(t) = \\frac{d}{dt}\\left[\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^{2}}{2}\\right)\\right] = -t \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^{2}}{2}\\right) = -t\\phi(t)$. This implies $\\phi(t) = -\\frac{1}{t}\\phi'(t)$ for $t \\neq 0$.\n\nWe will establish a key identity for $\\bar{\\Phi}(z)$ using integration by parts. The method relies on rewriting the integrand. Notice that the derivative of $\\exp(-t^2/2)$ is $-t \\exp(-t^2/2)$.\nLet us evaluate the integral $I = \\int_{z}^{\\infty} \\exp(-t^2/2) dt$. We can write the integrand as $\\frac{1}{t} \\cdot (t \\exp(-t^2/2))$.\nWe use integration by parts, $\\int u \\, dv = uv - \\int v \\, du$.\nLet $u = \\frac{1}{t}$ and $dv = t \\exp(-t^2/2) dt$.\nThen $du = -\\frac{1}{t^2} dt$ and $v = \\int t \\exp(-t^2/2) dt = -\\exp(-t^2/2)$.\nApplying the formula for integration by parts on the definite integral:\n$$\nI = \\int_{z}^{\\infty} \\frac{1}{t} (t \\exp(-t^2/2)) dt = \\left[ \\frac{1}{t} \\left(-\\exp(-t^2/2)\\right) \\right]_{z}^{\\infty} - \\int_{z}^{\\infty} (-\\exp(-t^2/2)) \\left(-\\frac{1}{t^2}\\right) dt\n$$\nThe first term is evaluated at the limits:\n$$\n\\left[ -\\frac{\\exp(-t^2/2)}{t} \\right]_{z}^{\\infty} = \\lim_{t\\to\\infty} \\left(-\\frac{\\exp(-t^2/2)}{t}\\right) - \\left(-\\frac{\\exp(-z^2/2)}{z}\\right)\n$$\nThe limit as $t \\to \\infty$ is $0$. Thus, the boundary term evaluates to $0 - (-\\frac{\\exp(-z^2/2)}{z}) = \\frac{\\exp(-z^2/2)}{z}$.\nThe integral becomes:\n$$\nI = \\frac{\\exp(-z^2/2)}{z} - \\int_{z}^{\\infty} \\frac{\\exp(-t^2/2)}{t^2} dt\n$$\nSince $\\bar{\\Phi}(z) = \\frac{1}{\\sqrt{2\\pi}}I$ and $\\phi(t) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-t^2/2)$, we can divide the entire identity by $\\sqrt{2\\pi}$:\n$$\n\\bar{\\Phi}(z) = \\frac{1}{\\sqrt{2\\pi}}\\frac{\\exp(-z^2/2)}{z} - \\int_{z}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}\\frac{\\exp(-t^2/2)}{t^2} dt\n$$\nThis yields the fundamental identity:\n$$\n\\bar{\\Phi}(z) = \\frac{\\phi(z)}{z} - \\int_{z}^{\\infty} \\frac{\\phi(t)}{t^2} dt\n$$\nFrom this identity, we can derive the required bounds.\n\n**Derivation of the Upper Bound on $\\bar{\\Phi}(z)$**\nFor $z  0$, the integration variable $t$ is always in the range $[z, \\infty)$, so $t0$. The PDF $\\phi(t)$ is strictly positive for all $t \\in \\mathbb{R}$. Therefore, the integrand $\\frac{\\phi(t)}{t^2}$ is strictly positive for all $t \\ge z  0$. This implies that the integral is strictly positive:\n$$\n\\int_{z}^{\\infty} \\frac{\\phi(t)}{t^2} dt  0\n$$\nSubstituting this into the identity gives:\n$$\n\\bar{\\Phi}(z)  \\frac{\\phi(z)}{z}\n$$\nThis is the required upper bound for $\\bar{\\Phi}(z)$.\n\n**Derivation of the Lower Bound on $\\bar{\\Phi}(z)$**\nTo find a lower bound, we need to find an upper bound for the integral term $\\int_{z}^{\\infty} \\frac{\\phi(t)}{t^2} dt$. In the domain of integration, $t \\ge z$. Since $z  0$, we have $t^2 \\ge z^2$, which implies $\\frac{1}{t^2} \\le \\frac{1}{z^2}$.\nWe can now bound the integral:\n$$\n\\int_{z}^{\\infty} \\frac{\\phi(t)}{t^2} dt \\le \\int_{z}^{\\infty} \\frac{\\phi(t)}{z^2} dt = \\frac{1}{z^2} \\int_{z}^{\\infty} \\phi(t) dt = \\frac{\\bar{\\Phi}(z)}{z^2}\n$$\nNow, substitute this inequality back into the fundamental identity:\n$$\n\\bar{\\Phi}(z) = \\frac{\\phi(z)}{z} - \\int_{z}^{\\infty} \\frac{\\phi(t)}{t^2} dt \\ge \\frac{\\phi(z)}{z} - \\frac{\\bar{\\Phi}(z)}{z^2}\n$$\nWe can solve this inequality for $\\bar{\\Phi}(z)$:\n$$\n\\bar{\\Phi}(z) + \\frac{\\bar{\\Phi}(z)}{z^2} \\ge \\frac{\\phi(z)}{z}\n$$\n$$\n\\bar{\\Phi}(z) \\left(1 + \\frac{1}{z^2}\\right) \\ge \\frac{\\phi(z)}{z}\n$$\n$$\n\\bar{\\Phi}(z) \\left(\\frac{z^2+1}{z^2}\\right) \\ge \\frac{\\phi(z)}{z}\n$$\nSince $z  0$, we can multiply by $\\frac{z^2}{z^2+1}$:\n$$\n\\bar{\\Phi}(z) \\ge \\frac{z^2}{z^2+1} \\frac{\\phi(z)}{z} = \\frac{z}{z^2+1}\\phi(z)\n$$\nThis is the required lower bound for $\\bar{\\Phi}(z)$.\n\n**Derivation of the Upper Bound on the Relative Error $e(z)$**\nThe relative error is defined as:\n$$\ne(z) = \\frac{\\frac{\\phi(z)}{z} - \\bar{\\Phi}(z)}{\\bar{\\Phi}(z)} = \\frac{\\phi(z)}{z \\bar{\\Phi}(z)} - 1\n$$\nTo find an upper bound for $e(z)$, we must find an upper bound for the term $\\frac{\\phi(z)}{z \\bar{\\Phi}(z)}$. This, in turn, requires a lower bound for its denominator, $z \\bar{\\Phi}(z)$.\nWe use the lower bound for $\\bar{\\Phi}(z)$ that we have just derived:\n$$\n\\bar{\\Phi}(z) \\ge \\frac{z}{z^2+1}\\phi(z)\n$$\nSince $z  0$, we can multiply by $z$ without changing the inequality direction:\n$$\nz \\bar{\\Phi}(z) \\ge z \\cdot \\frac{z}{z^2+1}\\phi(z) = \\frac{z^2}{z^2+1}\\phi(z)\n$$\nAll terms are positive for $z0$, so taking the reciprocal reverses the inequality:\n$$\n\\frac{1}{z \\bar{\\Phi}(z)} \\le \\frac{1}{\\frac{z^2}{z^2+1}\\phi(z)} = \\frac{z^2+1}{z^2 \\phi(z)}\n$$\nNow, we multiply by $\\phi(z)$, which is also positive:\n$$\n\\frac{\\phi(z)}{z \\bar{\\Phi}(z)} \\le \\phi(z) \\cdot \\frac{z^2+1}{z^2 \\phi(z)} = \\frac{z^2+1}{z^2}\n$$\nFinally, we substitute this upper bound back into the expression for $e(z)$:\n$$\ne(z) = \\frac{\\phi(z)}{z \\bar{\\Phi}(z)} - 1 \\le \\frac{z^2+1}{z^2} - 1\n$$\nSimplifying the right-hand side gives the final result:\n$$\n\\frac{z^2+1}{z^2} - 1 = \\frac{z^2+1-z^2}{z^2} = \\frac{1}{z^2}\n$$\nTherefore, an upper bound on the relative error is $\\frac{1}{z^2}$.", "answer": "$$\\boxed{\\frac{1}{z^{2}}}$$", "id": "4960564"}, {"introduction": "Real-world data collection is often imperfect; for example, a scientific instrument may have a lower limit of detection, resulting in truncated data. This exercise guides you through the fundamental process of deriving the probability density function, cumulative distribution function, and mean for a left-truncated normal distribution. Mastering this concept is crucial for accurately modeling and interpreting data in fields like biomarker analysis, where such limits are a common practical issue [@problem_id:4960602].", "problem": "In a biomarker assay study with a hard limit of detection at threshold $a$, suppose the latent biomarker concentration on the logarithmic scale, $X$, follows a normal distribution $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. Because measurements below $a$ are not recorded, the observed measurements arise from the left-truncated distribution at $a$, that is, the conditional distribution of $X$ given $X \\ge a$. Using only first-principles definitions in probability and calculus, do the following in order:\n- Precisely define the left-truncated distribution at $a$ as a conditional distribution of $X$ and derive its probability density function (probability density function (pdf)) on its support.\n- Derive the corresponding cumulative distribution function (cumulative distribution function (cdf)).\n- Derive a closed-form expression for the conditional mean $\\mathbb{E}[X \\mid X \\ge a]$ by mapping to the standard normal variable and evaluating the necessary integral.\n\nYour derivation should start from the definitions of the normal probability density function and cumulative distribution function for the standard normal, denoted by $\\phi(z)$ and $\\Phi(z)$, respectively, and the survivor function $\\bar{\\Phi}(z) = 1 - \\Phi(z)$, together with the change-of-variables rule. Define the standardized threshold $\\alpha = (a - \\mu)/\\sigma$ and the inverse Mills ratio $\\lambda(\\alpha) = \\phi(\\alpha)/\\bar{\\Phi}(\\alpha)$. State clearly the support of each function you derive. Report as your final answer only the simplified analytic expression for $\\mathbb{E}[X \\mid X \\ge a]$ in terms of $\\mu$, $\\sigma$, and $\\lambda(\\alpha)$. No numerical approximation is required.", "solution": "Let the random variable $X$ follow a normal distribution with mean $\\mu$ and variance $\\sigma^2$, denoted as $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Its probability density function (pdf) is given by\n$$f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\nfor $x \\in (-\\infty, \\infty)$.\nLet $Z = \\frac{X-\\mu}{\\sigma}$ be the standard normal variable, $Z \\sim \\mathcal{N}(0,1)$. The pdf of $Z$ is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$, and its cumulative distribution function (cdf) is $\\Phi(z) = \\int_{-\\infty}^z \\phi(t) dt$. The survivor function is $\\bar{\\Phi}(z) = 1 - \\Phi(z) = P(Z  z)$.\nThe problem defines a standardized threshold $\\alpha = (a-\\mu)/\\sigma$.\n\nWe are interested in the conditional distribution of $X$ given that $X \\ge a$. Let us denote the random variable from this truncated distribution as $Y$.\n\n**1. Probability Density Function (PDF) of the Left-Truncated Distribution**\n\nBy definition, the pdf of a conditional distribution is given by $f_{Y}(y) = f_{X|X \\ge a}(y)$. For a value $y$ in the support of the conditional distribution (i.e., for $y \\ge a$), this is:\n$$f_Y(y) = \\frac{f_X(y)}{P(X \\ge a)}$$\nThe pdf is $0$ for $y  a$.\n\nFirst, we must evaluate the denominator, $P(X \\ge a)$. We standardize the variable $X$:\n$$P(X \\ge a) = P\\left(\\frac{X-\\mu}{\\sigma} \\ge \\frac{a-\\mu}{\\sigma}\\right) = P(Z \\ge \\alpha)$$\nIn terms of the standard normal cdf and survivor function:\n$$P(Z \\ge \\alpha) = 1 - \\Phi(\\alpha) = \\bar{\\Phi}(\\alpha)$$\nSubstituting this back into the expression for $f_Y(y)$, we obtain the pdf of the left-truncated normal distribution:\n$$f_Y(y) = \\frac{1}{\\bar{\\Phi}(\\alpha)} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$$\nThe support of this distribution is the interval $[a, \\infty)$.\n\n**2. Cumulative Distribution Function (CDF) of the Left-Truncated Distribution**\n\nThe cdf of $Y$ is $F_Y(y) = P(Y \\le y)$. Given the support of $Y$ is $[a, \\infty)$, $F_Y(y) = 0$ for $y  a$. For $y \\ge a$, the cdf is given by the conditional probability:\n$$F_Y(y) = P(X \\le y \\mid X \\ge a) = \\frac{P(a \\le X \\le y)}{P(X \\ge a)}$$\nThe numerator can be expressed in terms of the cdf of $X$, $F_X(x) = P(X \\le x)$:\n$$P(a \\le X \\le y) = P(X \\le y) - P(X  a)$$\nSince the normal distribution is continuous, $P(X  a) = P(X \\le a) = F_X(a)$. Thus, the numerator is $F_X(y) - F_X(a)$.\nThe cdf of $X$ can be written in terms of the standard normal cdf $\\Phi(z)$:\n$$F_X(x) = P\\left(\\frac{X-\\mu}{\\sigma} \\le \\frac{x-\\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$$\nSo, $F_X(y) = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right)$ and $F_X(a) = \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right) = \\Phi(\\alpha)$.\nThe denominator is $P(X \\ge a) = 1 - F_X(a) = 1 - \\Phi(\\alpha) = \\bar{\\Phi}(\\alpha)$.\nCombining these results, the cdf of the left-truncated distribution for $y \\ge a$ is:\n$$F_Y(y) = \\frac{\\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) - \\Phi(\\alpha)}{1 - \\Phi(\\alpha)} = \\frac{\\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) - \\Phi(\\alpha)}{\\bar{\\Phi}(\\alpha)}$$\nThe cdf is defined on the full real line:\n$$\nF_Y(y) =\n\\begin{cases}\n0  \\text{for } y  a \\\\\n\\frac{\\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) - \\Phi(\\alpha)}{\\bar{\\Phi}(\\alpha)}  \\text{for } y \\ge a\n\\end{cases}\n$$\n\n**3. Conditional Mean $\\mathbb{E}[X \\mid X \\ge a]$**\n\nThe conditional mean is the expectation of $X$ with respect to the conditional pdf derived in the first part.\n$$\\mathbb{E}[X \\mid X \\ge a] = \\int_a^\\infty x f_{X|X \\ge a}(x) dx = \\int_a^\\infty x \\frac{f_X(x)}{P(X \\ge a)} dx$$\nSubstituting the expressions for $f_X(x)$ and $P(X \\ge a)$:\n$$\\mathbb{E}[X \\mid X \\ge a] = \\frac{1}{\\bar{\\Phi}(\\alpha)} \\int_a^\\infty x \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\nWe perform a change of variables to the standard normal scale. Let $z = \\frac{x-\\mu}{\\sigma}$. This implies $x = \\sigma z + \\mu$ and $dx = \\sigma dz$. The lower limit of integration becomes $x=a \\implies z = \\frac{a-\\mu}{\\sigma} = \\alpha$. The upper limit remains infinity.\nThe integral becomes:\n$$\\int_\\alpha^\\infty (\\sigma z + \\mu) \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) (\\sigma dz) = \\int_\\alpha^\\infty (\\sigma z + \\mu) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$$\nRecalling that $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$, we can write the integral as:\n$$\\int_\\alpha^\\infty (\\sigma z + \\mu) \\phi(z) dz = \\sigma \\int_\\alpha^\\infty z \\phi(z) dz + \\mu \\int_\\alpha^\\infty \\phi(z) dz$$\nWe evaluate each integral separately. The second integral is the definition of the survivor function for a standard normal variable evaluated at $\\alpha$:\n$$\\int_\\alpha^\\infty \\phi(z) dz = P(Z  \\alpha) = \\bar{\\Phi}(\\alpha)$$\nFor the first integral, we note that the derivative of $\\phi(z)$ with respect to $z$ is $\\frac{d}{dz}\\phi(z) = \\frac{d}{dz}\\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) \\cdot (-z) = -z\\phi(z)$.\nTherefore, $\\int z\\phi(z) dz = -\\phi(z) + C$.\n$$\\int_\\alpha^\\infty z \\phi(z) dz = [-\\phi(z)]_\\alpha^\\infty = \\lim_{z\\to\\infty}(-\\phi(z)) - (-\\phi(\\alpha))$$\nSince $\\lim_{z\\to\\infty} \\exp(-z^2/2) = 0$, we have $\\lim_{z\\to\\infty} \\phi(z) = 0$. Thus, the integral evaluates to:\n$$\\int_\\alpha^\\infty z \\phi(z) dz = 0 - (-\\phi(\\alpha)) = \\phi(\\alpha)$$\nSubstituting these results back into the expression for the original integral:\n$$\\int_a^\\infty x f_X(x) dx = \\sigma \\phi(\\alpha) + \\mu \\bar{\\Phi}(\\alpha)$$\nFinally, we compute the conditional expectation by dividing by $P(X \\ge a) = \\bar{\\Phi}(\\alpha)$:\n$$\\mathbb{E}[X \\mid X \\ge a] = \\frac{\\sigma \\phi(\\alpha) + \\mu \\bar{\\Phi}(\\alpha)}{\\bar{\\Phi}(\\alpha)} = \\mu + \\sigma \\frac{\\phi(\\alpha)}{\\bar{\\Phi}(\\alpha)}$$\nUsing the problem's definition of the inverse Mills ratio, $\\lambda(\\alpha) = \\frac{\\phi(\\alpha)}{\\bar{\\Phi}(\\alpha)}$, the final expression for the conditional mean is:\n$$\\mathbb{E}[X \\mid X \\ge a] = \\mu + \\sigma \\lambda(\\alpha)$$", "answer": "$$\\boxed{\\mu + \\sigma \\lambda(\\alpha)}$$", "id": "4960602"}, {"introduction": "Beyond just describing data, a primary goal of statistics is to make inferences about a population from a sample. This hands-on problem asks you to derive a confidence interval for the variance of a normal population from first principles, rather than just plugging numbers into a formula. This process solidifies the crucial connection between the normal distribution, the sample variance, and the chi-squared distribution, which is a cornerstone of hypothesis testing and interval estimation [@problem_id:4960593].", "problem": "A molecular epidemiology team studies baseline log-transformed plasma metabolite concentrations in a cohort and models the measurements as independent and identically distributed draws from a normal distribution with unknown mean $\\mu$ and unknown variance $\\sigma^{2}$. Specifically, let $Y_{1}, \\dots, Y_{n}$ be independent with $Y_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$. In one assay batch, the team observes $n = 23$ samples and computes the sample mean $\\overline{Y}$ and the centered sum of squares $\\sum_{i=1}^{n} (Y_{i} - \\overline{Y})^{2} = 34.8$. \n\nStarting from fundamental definitions and well-tested facts about normal samples and the chi-squared distribution, derive an exact $(1-\\alpha)$ confidence interval for $\\sigma^{2}$ that uses chi-squared quantiles. Your derivation should begin with the definition of the chi-squared distribution as the distribution of a sum of squares of independent standard normal variables and the basic properties of the normal sample $(Y_{1}, \\dots, Y_{n})$, without assuming any pre-packaged interval formula. Clearly invert the relevant probability statement to isolate $\\sigma^{2}$.\n\nTake $\\alpha = 0.08$. Express your final interval endpoints in closed form using the notation $\\chi^{2}_{p, \\nu}$ for the $p$ quantile of a chi-squared distribution with $\\nu$ degrees of freedom. Report the two endpoints as a single row matrix. No numerical evaluation of chi-squared quantiles is required, and no rounding is needed.", "solution": "The goal is to derive a $(1-\\alpha)$ confidence interval for the unknown variance $\\sigma^2$ of a normal distribution, based on a sample $Y_1, \\dots, Y_n$. The derivation must begin from fundamental principles.\n\nLet $Y_1, \\dots, Y_n$ be an independent and identically distributed (i.i.d.) random sample from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, where both the mean $\\mu$ and the variance $\\sigma^2$ are unknown.\n\nThe derivation begins with the properties of standardized normal variables. For each observation $Y_i$, the standardized variable $Z_i = \\frac{Y_i - \\mu}{\\sigma}$ follows a standard normal distribution, $Z_i \\sim \\mathcal{N}(0, 1)$. By definition, a chi-squared distribution with $k$ degrees of freedom, denoted $\\chi^2_k$, is the distribution of the sum of the squares of $k$ independent standard normal random variables. Therefore, the sum $\\sum_{i=1}^{n} Z_i^2 = \\sum_{i=1}^{n} \\left( \\frac{Y_i - \\mu}{\\sigma} \\right)^2$ follows a $\\chi^2_n$ distribution.\n\nSince $\\mu$ is unknown, we cannot directly use this quantity. We must relate it to a statistic that can be calculated from the sample. A key result in statistics, often derived from Cochran's theorem, provides the necessary tool. We can decompose the sum of squares as follows:\n$$ \\sum_{i=1}^{n} (Y_i - \\mu)^2 = \\sum_{i=1}^{n} (Y_i - \\overline{Y} + \\overline{Y} - \\mu)^2 $$\nExpanding the term on the right-hand side gives:\n$$ \\sum_{i=1}^{n} \\left( (Y_i - \\overline{Y}) + (\\overline{Y} - \\mu) \\right)^2 = \\sum_{i=1}^{n} (Y_i - \\overline{Y})^2 + \\sum_{i=1}^{n} (\\overline{Y} - \\mu)^2 + 2\\sum_{i=1}^{n} (Y_i - \\overline{Y})(\\overline{Y} - \\mu) $$\nThe cross-product term is zero:\n$$ 2(\\overline{Y} - \\mu) \\sum_{i=1}^{n} (Y_i - \\overline{Y}) = 2(\\overline{Y} - \\mu) \\left( \\left(\\sum_{i=1}^{n} Y_i\\right) - n\\overline{Y} \\right) = 2(\\overline{Y} - \\mu) (n\\overline{Y} - n\\overline{Y}) = 0 $$\nSo, the identity simplifies to:\n$$ \\sum_{i=1}^{n} (Y_i - \\mu)^2 = \\sum_{i=1}^{n} (Y_i - \\overline{Y})^2 + n(\\overline{Y} - \\mu)^2 $$\nDividing the entire equation by $\\sigma^2$, we get:\n$$ \\frac{\\sum_{i=1}^{n} (Y_i - \\mu)^2}{\\sigma^2} = \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\sigma^2} + \\frac{n(\\overline{Y} - \\mu)^2}{\\sigma^2} $$\nLet's analyze the distributional properties of these terms.\n1. The left-hand side is $\\sum_{i=1}^{n} \\left( \\frac{Y_i - \\mu}{\\sigma} \\right)^2 = \\sum_{i=1}^{n} Z_i^2$, which follows a $\\chi^2_n$ distribution.\n2. The second term on the right-hand side involves the sample mean $\\overline{Y}$. We know that $\\overline{Y} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)$. Therefore, $\\frac{\\overline{Y}-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1)$. Squaring this standard normal variable gives a chi-squared variable with $1$ degree of freedom: $\\left(\\frac{\\overline{Y}-\\mu}{\\sigma/\\sqrt{n}}\\right)^2 = \\frac{n(\\overline{Y}-\\mu)^2}{\\sigma^2} \\sim \\chi^2_1$.\n\nCochran's theorem states that the two terms on the right-hand side are statistically independent. Since the sum of two independent chi-squared variables is also a chi-squared variable with degrees of freedom equal to the sum of the individual degrees of freedom, we can deduce the distribution of the first term on the right-hand side.\nLet the pivotal quantity be $Q = \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\sigma^2}$. Then we have:\n$\\chi^2_n = Q + \\chi^2_1$.\nBy the properties of moment-generating functions for independent variables, it follows that $Q$ must have a chi-squared distribution with $n-1$ degrees of freedom. So,\n$$ Q = \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\sigma^2} \\sim \\chi^2_{n-1} $$\nThis quantity $Q$ is pivotal because its distribution does not depend on the unknown parameters $\\mu$ or $\\sigma^2$. We can now use it to construct a confidence interval for $\\sigma^2$.\n\nA $(1-\\alpha)$ confidence interval is found by identifying two values, $a$ and $b$, such that $P(a \\le Q \\le b) = 1-\\alpha$. For an equal-tailed interval, we choose $a$ and $b$ to be the $\\alpha/2$ and $1-\\alpha/2$ quantiles of the $\\chi^2_{n-1}$ distribution. Let $\\chi^2_{p, \\nu}$ denote the $p$-th quantile of a chi-squared distribution with $\\nu$ degrees of freedom. We choose:\n$a = \\chi^2_{\\alpha/2, n-1}$\n$b = \\chi^2_{1-\\alpha/2, n-1}$\n\nThe probability statement is:\n$$ P\\left( \\chi^2_{\\alpha/2, n-1} \\le \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\sigma^2} \\le \\chi^2_{1-\\alpha/2, n-1} \\right) = 1-\\alpha $$\n\nTo find the confidence interval for $\\sigma^2$, we must invert the inequalities to isolate $\\sigma^2$.\nThe left inequality is:\n$$ \\chi^2_{\\alpha/2, n-1} \\le \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\sigma^2} $$\nMultiplying by $\\sigma^2$ (which is positive) and dividing by the chi-squared quantile (which is also positive) yields:\n$$ \\sigma^2 \\le \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\chi^2_{\\alpha/2, n-1}} $$\nThis gives the upper bound of the confidence interval.\n\nThe right inequality is:\n$$ \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\sigma^2} \\le \\chi^2_{1-\\alpha/2, n-1} $$\nTaking the reciprocal of both sides reverses the inequality sign:\n$$ \\frac{\\sigma^2}{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2} \\ge \\frac{1}{\\chi^2_{1-\\alpha/2, n-1}} $$\nMultiplying by the sum of squares gives:\n$$ \\sigma^2 \\ge \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\chi^2_{1-\\alpha/2, n-1}} $$\nThis gives the lower bound of the confidence interval.\n\nCombining these two bounds, the $(1-\\alpha)$ confidence interval for $\\sigma^2$ is:\n$$ \\left[ \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\chi^2_{1-\\alpha/2, n-1}}, \\frac{\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2}{\\chi^2_{\\alpha/2, n-1}} \\right] $$\n\nNow, we substitute the values provided in the problem:\n- Sample size $n = 23$, so the degrees of freedom are $\\nu = n-1 = 22$.\n- Centered sum of squares $\\sum_{i=1}^{n} (Y_i - \\overline{Y})^2 = 34.8$.\n- Significance level $\\alpha = 0.08$.\n\nWe need the quantiles for $\\alpha/2$ and $1-\\alpha/2$:\n- $\\alpha/2 = 0.08 / 2 = 0.04$.\n- $1-\\alpha/2 = 1 - 0.04 = 0.96$.\n\nThe lower bound of the interval is:\n$$ \\text{Lower Bound} = \\frac{34.8}{\\chi^2_{0.96, 22}} $$\nThe upper bound of the interval is:\n$$ \\text{Upper Bound} = \\frac{34.8}{\\chi^2_{0.04, 22}} $$\n\nThe endpoints of the confidence interval are thus given, in closed form as requested.", "answer": "$$ \\boxed{\n\\begin{pmatrix}\n\\frac{34.8}{\\chi^{2}_{0.96, 22}}  \\frac{34.8}{\\chi^{2}_{0.04, 22}}\n\\end{pmatrix}\n} $$", "id": "4960593"}]}