{"hands_on_practices": [{"introduction": "Before embarking on a clinical study or experiment, one of the most critical questions to answer is: \"How much data do we need to collect?\" This practice guides you through the fundamental process of sample size calculation. By using the Central Limit Theorem to approximate the binomial distribution, you will derive and apply a formula to determine the minimum number of observations required to estimate a population proportion $p$ within a desired margin of error and with a specified level of confidence, a cornerstone of rigorous study design [@problem_id:4957558].", "problem": "In a cross-sectional biostatistics study, you will estimate the prevalence $p$ of a binary biomarker using independent and identically distributed Bernoulli observations. You plan to report a two-sided confidence interval for $p$ with nominal confidence level $1-\\alpha$ under a normal approximation justified by the Central Limit Theorem (CLT). Starting from the definitions of a Bernoulli random variable and a binomial sample proportion, and using the fact that for large $n$ the standardized sample proportion is approximately standard normal, derive a design expression for the required sample size $n$ that ensures the absolute margin of error does not exceed a prespecified tolerance $\\epsilon$ with confidence $1-\\alpha$. Then, identify the choice of $p$ that yields a conservative (worst-case) design and use it to compute the minimal integer $n$ ensuring the margin of error is at most $\\epsilon$ for any true $p \\in [0,1]$.\n\nAssume $\\alpha = 0.05$ and $\\epsilon = 0.04$, and take the standard normal upper quantile $z_{1-\\alpha/2}$ corresponding to the confidence level $1-\\alpha$ as the usual value used in practice. Report the minimal integer $n$ that meets the requirement. Do not include units. No rounding by significant figures is required; report the exact minimal integer that satisfies the design criterion.", "solution": "The user has provided a valid problem statement. The problem is scientifically grounded in standard statistical theory, is well-posed, and is stated using objective and precise language. All necessary information is provided.\n\nThe problem asks for the derivation of a sample size formula for estimating a population proportion $p$ and its application to a specific case. We begin by defining the statistical framework.\n\nLet $X_1, X_2, \\dots, X_n$ be a set of $n$ independent and identically distributed (i.i.d.) random variables from a Bernoulli distribution with parameter $p$, denoted as $X_i \\sim \\text{Bernoulli}(p)$. The parameter $p$ represents the true prevalence of the biomarker in the population. For each observation $i$, $X_i=1$ if the biomarker is present (with probability $p$) and $X_i=0$ if it is absent (with probability $1-p$). The expected value of each observation is $E[X_i] = p$ and the variance is $\\text{Var}(X_i) = p(1-p)$.\n\nThe sample proportion, $\\hat{p}$, is the point estimator for $p$ and is defined as the mean of the observations:\n$$ \\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\nAs an estimator, $\\hat{p}$ has an expected value $E[\\hat{p}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} E[X_i] = \\frac{1}{n}(np) = p$, making it an unbiased estimator of $p$. The variance of $\\hat{p}$ is given by $\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2}(np(1-p)) = \\frac{p(1-p)}{n}$, due to the independence of the observations.\n\nBy the Central Limit Theorem (CLT), for a sufficiently large sample size $n$, the sampling distribution of $\\hat{p}$ is approximately normal:\n$$ \\hat{p} \\approx N\\left(p, \\frac{p(1-p)}{n}\\right) $$\nStandardizing the random variable $\\hat{p}$ yields a variable that is approximately standard normal:\n$$ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\approx N(0,1) $$\nA two-sided confidence interval for $p$ with a confidence level of $1-\\alpha$ is constructed based on the pivotal quantity $Z$. We seek to find the bounds that satisfy:\n$$ P\\left(-z_{1-\\alpha/2} \\le \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} \\le z_{1-\\alpha/2}\\right) \\approx 1-\\alpha $$\nwhere $z_{1-\\alpha/2}$ is the upper $1-\\alpha/2$ quantile of the standard normal distribution.\n\nThe absolute margin of error, which we can denote by $E_{margin}$, is the half-width of the confidence interval. From the inequality above, it is clear that the expression for the margin of error in estimating $p$ is $E_{margin} = z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}$. The problem states a requirement that this margin of error should not exceed a prespecified tolerance $\\epsilon$.\n$$ z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} \\le \\epsilon $$\nOur goal is to find the minimum sample size $n$ that satisfies this condition. We rearrange the inequality to solve for $n$:\n$$ \\sqrt{\\frac{p(1-p)}{n}} \\le \\frac{\\epsilon}{z_{1-\\alpha/2}} $$\n$$ \\frac{p(1-p)}{n} \\le \\left(\\frac{\\epsilon}{z_{1-\\alpha/2}}\\right)^2 $$\n$$ n \\ge p(1-p) \\left(\\frac{z_{1-\\alpha/2}}{\\epsilon}\\right)^2 $$\nThis is the general design expression for the required sample size. However, this expression depends on the unknown proportion $p$. To guarantee that the margin of error requirement is met for *any* possible value of $p \\in [0, 1]$, we must adopt a conservative (worst-case) approach. This involves finding the value of $p$ that maximizes the required sample size $n$. The sample size $n$ is maximized when the term $p(1-p)$ is maximized.\n\nLet $f(p) = p(1-p) = p - p^2$. To find the maximum of this function on the interval $[0, 1]$, we can use calculus. The first derivative is $f'(p) = 1 - 2p$. Setting the derivative to zero, $1 - 2p = 0$, yields $p = \\frac{1}{2} = 0.5$. Since the second derivative $f''(p) = -2$ is negative, $p=0.5$ is a local maximum. This is the global maximum on the interval $[0, 1]$, with a maximum value of $f(0.5) = 0.5(1-0.5) = 0.25$.\n\nBy substituting this worst-case value for $p(1-p)$ into the sample size inequality, we obtain a conservative formula for $n$ that is independent of $p$:\n$$ n \\ge 0.25 \\left(\\frac{z_{1-\\alpha/2}}{\\epsilon}\\right)^2 $$\nWe are given the following values:\n- Confidence level $1-\\alpha = 1 - 0.05 = 0.95$.\n- This implies $\\alpha/2 = 0.025$, so we need the $1 - 0.025 = 0.975$ quantile of the standard normal distribution. The problem specifies to use the \"usual value,\" which is $z_{0.975} = 1.96$.\n- Maximum tolerable margin of error $\\epsilon = 0.04$.\n\nSubstituting these values into the conservative sample size formula:\n$$ n \\ge 0.25 \\left(\\frac{1.96}{0.04}\\right)^2 $$\nFirst, we compute the ratio inside the parentheses:\n$$ \\frac{1.96}{0.04} = \\frac{196}{4} = 49 $$\nNow, we substitute this back into the inequality:\n$$ n \\ge 0.25 \\times (49)^2 $$\n$$ n \\ge 0.25 \\times 2401 $$\n$$ n \\ge 600.25 $$\nSince the sample size $n$ must be an integer, we must choose the smallest integer that satisfies this condition. Therefore, we take the ceiling of the calculated value:\n$$ n_{min} = \\lceil 600.25 \\rceil = 601 $$\nThe minimal integer sample size required to ensure the margin of error is at most $0.04$ with $95\\%$ confidence, for any true prevalence $p$, is $601$.", "answer": "$$ \\boxed{601} $$", "id": "4957558"}, {"introduction": "After collecting data, a primary goal of statistical analysis is to evaluate evidence for or against a specific scientific hypothesis. This exercise introduces the powerful and universally important likelihood ratio test (LRT) for a binomial proportion. Starting from the basic definition of a likelihood function, you will derive the MLE, construct the test statistic, and apply it to test a hypothesis about an assay's positivity rate, providing a solid foundation in the principles of likelihood-based inference [@problem_id:4957552].", "problem": "A clinical laboratory validates a new binary assay that returns a positive result with unknown probability $p$ when applied to a randomly selected specimen from a target population. Suppose $X_{1}, X_{2}, \\ldots, X_{n}$ are independent and identically distributed (i.i.d.) Bernoulli random variables with success probability $p$, where $X_{i}=1$ denotes a positive test and $X_{i}=0$ denotes a negative test. Let $K=\\sum_{i=1}^{n} X_{i}$ be the total number of positives observed.\n\nUsing only the fundamental definitions of the Bernoulli probability mass function, independence, and the definition of a likelihood function as the joint probability treated as a function of the parameter given observed data, do the following:\n\n1) Derive the likelihood for $p$ based on the observed data and reduce it to a function of the sufficient statistic $K$. Interpret this as the profile likelihood for $p$ in this model.\n\n2) Determine the maximum likelihood estimator $\\hat{p}$ by maximizing the likelihood over $p \\in [0,1]$, paying careful attention to the boundary cases $K=0$ and $K=n$.\n\n3) Construct the Likelihood Ratio Test (LRT) statistic for testing the simple null hypothesis $H_{0}: p=p_{0}$ versus the two-sided alternative $H_{1}: p \\neq p_{0}$. Express the LRT statistic $W=-2\\ln \\lambda$ in terms of $n$, $K$, $p_{0}$, and $\\hat{p}$ for the interior case $0<K<n$, and then obtain the limiting expressions for the boundary cases $K=0$ and $K=n$.\n\n4) For a validation run with $n=120$ assays and $K=18$ positives, test $H_{0}: p=0.10$ by computing the numerical value of the LRT statistic $W$. Round your answer to four significant figures. Express the final answer as a pure number with no units.", "solution": "**1) Derivation of the Likelihood Function**\n\nLet $x_{1}, x_{2}, \\ldots, x_{n}$ be the specific observed outcomes of the random variables $X_{1}, X_{2}, \\ldots, X_{n}$. The probability mass function (PMF) for a single Bernoulli random variable $X_{i}$ with success probability $p$ is given by:\n$$P(X_{i}=x_{i} | p) = p^{x_{i}}(1-p)^{1-x_{i}}, \\quad \\text{for } x_{i} \\in \\{0, 1\\}$$\nThe likelihood function, $L(p)$, is the joint probability of observing the data $(x_{1}, x_{2}, \\ldots, x_{n})$, treated as a function of the parameter $p$. Since the observations are independent, the joint probability is the product of the individual probabilities:\n$$L(p) = L(p | x_{1}, \\ldots, x_{n}) = \\prod_{i=1}^{n} P(X_{i}=x_{i} | p) = \\prod_{i=1}^{n} p^{x_{i}}(1-p)^{1-x_{i}}$$\nUsing the properties of exponents, we can combine the terms:\n$$L(p) = p^{\\sum_{i=1}^{n} x_{i}} (1-p)^{\\sum_{i=1}^{n} (1-x_{i})}$$\nThe sum of the outcomes, $\\sum_{i=1}^{n} x_{i}$, is the total number of observed positives, which is denoted by $K$. The sum of the exponents on the $(1-p)$ term is $\\sum_{i=1}^{n} 1 - \\sum_{i=1}^{n} x_{i} = n - K$.\nSubstituting these into the expression for the likelihood gives:\n$$L(p) = p^{K}(1-p)^{n-K}$$\nThis expression depends on the data only through the statistic $K$. Thus, $K$ is a sufficient statistic for $p$. This function is the likelihood, and in this one-parameter context, it can also be referred to as the profile likelihood for $p$.\n\n**2) Determination of the Maximum Likelihood Estimator (MLE)**\n\nTo find the value of $p$ that maximizes the likelihood function $L(p)$, it is mathematically more convenient to maximize the natural logarithm of the likelihood, a function known as the log-likelihood, $\\ell(p)$:\n$$\\ell(p) = \\ln(L(p)) = \\ln(p^{K}(1-p)^{n-K}) = K \\ln(p) + (n-K) \\ln(1-p)$$\nThe maximum of $\\ell(p)$ will occur at the same value of $p$ as the maximum of $L(p)$ because the natural logarithm is a strictly increasing function. We find the maximum by taking the first derivative of $\\ell(p)$ with respect to $p$ and setting it to zero. This is valid for the interior case where $0 < K < n$, which implies $p \\in (0,1)$.\n$$\\frac{d\\ell}{dp} = \\frac{K}{p} - \\frac{n-K}{1-p}$$\nSetting the derivative to zero:\n$$\\frac{K}{p} = \\frac{n-K}{1-p} \\implies K(1-p) = p(n-K) \\implies K - Kp = np - Kp \\implies K = np$$\nSolving for $p$, we obtain the candidate estimator:\n$$\\hat{p} = \\frac{K}{n}$$\nTo confirm this is a maximum, we examine the second derivative:\n$$\\frac{d^2\\ell}{dp^2} = -\\frac{K}{p^2} - \\frac{n-K}{(1-p)^2}$$\nFor $0 < K < n$, both $K$ and $n-K$ are positive. Since $p^2 > 0$ and $(1-p)^2 > 0$, the second derivative is always negative, confirming that $\\hat{p} = K/n$ is a local maximum. Since it is the only critical point in the interval $(0,1)$, it is the global maximum for the interior case.\n\nWe must also consider the boundary cases:\n- If $K=0$: The likelihood is $L(p) = p^{0}(1-p)^{n-0} = (1-p)^{n}$. On the interval $p \\in [0,1]$, this function is maximized when $p$ is minimal, so $\\hat{p}=0$. The formula $\\hat{p}=K/n=0/n=0$ holds.\n- If $K=n$: The likelihood is $L(p) = p^{n}(1-p)^{n-n} = p^{n}$. On the interval $p \\in [0,1]$, this function is maximized when $p$ is maximal, so $\\hat{p}=1$. The formula $\\hat{p}=K/n=n/n=1$ holds.\nThus, the maximum likelihood estimator for $p$ is $\\hat{p} = K/n$ for all $K \\in \\{0, 1, \\ldots, n\\}$.\n\n**3) Construction of the Likelihood Ratio Test (LRT) Statistic**\n\nThe likelihood ratio $\\lambda$ for testing $H_{0}: p=p_{0}$ versus $H_{1}: p \\neq p_{0}$ is the ratio of the maximum likelihood under the null hypothesis to the maximum likelihood under the alternative (unrestricted) hypothesis.\n$$\\lambda = \\frac{\\sup_{p \\in \\{p_{0}\\}} L(p)}{\\sup_{p \\in [0,1]} L(p)}$$\nThe numerator is simply the likelihood evaluated at $p=p_{0}$: $L(p_{0}) = p_{0}^{K}(1-p_{0})^{n-K}$.\nThe denominator is the likelihood evaluated at the MLE $\\hat{p}$: $L(\\hat{p}) = \\hat{p}^{K}(1-\\hat{p})^{n-K}$.\nThe likelihood ratio is therefore:\n$$\\lambda = \\frac{p_{0}^{K}(1-p_{0})^{n-K}}{\\hat{p}^{K}(1-\\hat{p})^{n-K}} = \\left(\\frac{p_{0}}{\\hat{p}}\\right)^{K} \\left(\\frac{1-p_{0}}{1-\\hat{p}}\\right)^{n-K}$$\nThe LRT statistic is $W = -2\\ln \\lambda$.\n$$W = -2\\ln \\left[ \\left(\\frac{p_{0}}{\\hat{p}}\\right)^{K} \\left(\\frac{1-p_{0}}{1-\\hat{p}}\\right)^{n-K} \\right]$$\nUsing properties of logarithms:\n$$W = -2 \\left[ K\\ln\\left(\\frac{p_{0}}{\\hat{p}}\\right) + (n-K)\\ln\\left(\\frac{1-p_{0}}{1-\\hat{p}}\\right) \\right]$$\nThis can be rewritten as:\n$$W = 2 \\left[ K\\ln\\left(\\frac{\\hat{p}}{p_{0}}\\right) + (n-K)\\ln\\left(\\frac{1-\\hat{p}}{1-p_{0}}\\right) \\right]$$\nThis formula is valid for the interior case $0 < K < n$, where $\\hat{p} \\in (0,1)$.\n\nFor the boundary cases, we must take limits or re-evaluate. It is simpler to use the form $W = 2[\\ell(\\hat{p}) - \\ell(p_{0})]$.\n- If $K=0$: Then $\\hat{p}=0$. The log-likelihood at the MLE is $\\ell(\\hat{p})=\\ell(0) = 0\\ln(0) + n\\ln(1) = 0$ (using the convention $0\\ln 0=0$). The log-likelihood under the null is $\\ell(p_{0}) = 0\\ln(p_{0}) + n\\ln(1-p_{0}) = n\\ln(1-p_{0})$.\nSo, $W = 2[0 - n\\ln(1-p_{0})] = -2n\\ln(1-p_{0})$.\n- If $K=n$: Then $\\hat{p}=1$. The log-likelihood at the MLE is $\\ell(\\hat{p})=\\ell(1) = n\\ln(1) + 0\\ln(0) = 0$. The log-likelihood under the null is $\\ell(p_{0}) = n\\ln(p_{0}) + 0\\ln(1-p_{0}) = n\\ln(p_{0})$.\nSo, $W = 2[0 - n\\ln(p_{0})] = -2n\\ln(p_{0})$.\n\n**4) Numerical Calculation**\n\nWe are given $n=120$, $K=18$, and we are testing $H_{0}: p=0.10$.\nThis is an interior case, as $0 < 18 < 120$.\nFirst, we calculate the MLE $\\hat{p}$:\n$$\\hat{p} = \\frac{K}{n} = \\frac{18}{120} = \\frac{3}{20} = 0.15$$\nNow we use the derived formula for $W$ with the given values:\n$$p_{0} = 0.10, \\quad \\hat{p}=0.15, \\quad n=120, \\quad K=18, \\quad n-K=102$$\n$$W = 2 \\left[ K\\ln\\left(\\frac{\\hat{p}}{p_{0}}\\right) + (n-K)\\ln\\left(\\frac{1-\\hat{p}}{1-p_{0}}\\right) \\right]$$\n$$W = 2 \\left[ 18 \\ln\\left(\\frac{0.15}{0.10}\\right) + 102 \\ln\\left(\\frac{1-0.15}{1-0.10}\\right) \\right]$$\n$$W = 2 \\left[ 18 \\ln(1.5) + 102 \\ln\\left(\\frac{0.85}{0.90}\\right) \\right]$$\nUsing a calculator for the natural logarithms:\n$\\ln(1.5) \\approx 0.405465108$\n$\\ln(0.85/0.90) = \\ln(17/18) \\approx -0.057158309$\nSubstituting these values:\n$$W \\approx 2 \\left[ 18 \\times 0.405465108 + 102 \\times (-0.057158309) \\right]$$\n$$W \\approx 2 \\left[ 7.298371944 - 5.830147518 \\right]$$\n$$W \\approx 2 \\left[ 1.468224426 \\right]$$\n$$W \\approx 2.936448852$$\nRounding the result to four significant figures, we get $W = 2.936$.", "answer": "$$\\boxed{2.936}$$", "id": "4957552"}, {"introduction": "While a point estimate like the sample proportion $\\hat{p}$ provides a single best guess for a parameter, it doesn't convey the uncertainty associated with that estimate. This hands-on programming exercise delves into the crucial task of constructing confidence intervals, which provide a range of plausible values for the true proportion $p$. By implementing and comparing the performance of the Wald, Wilson, and Clopper-Pearson intervals, you will gain practical insight into their respective strengths and weaknesses, especially regarding their actual coverage probability and expected length in various scenarios [@problem_id:4957555].", "problem": "A sequence of $n$ independent and identically distributed Bernoulli trials with success probability $p$ yields a total of $k$ successes, where $k \\in \\{0,1,\\dots,n\\}$. Let $\\ell(p)$ denote the binomial log-likelihood, $U(p)$ the score function, and $I(p)$ the Fisher information. Starting only from the following foundational elements:\n- The definition of the Bernoulli and binomial models.\n- The binomial log-likelihood and its derivatives.\n- The large-sample normal approximation for the score test statistic under a simple null hypothesis.\nyou must derive an algorithm that constructs the Wilson score interval for $p$ by inverting the two-sided score test of level $\\alpha$. Specifically, construct the acceptance region for the score test of $H_0: p = p_0$ at level $\\alpha$ and mathematically invert it to obtain an interval in $p$ that depends on $n$ and the observed $k$.\n\nThen implement three interval estimators for $p$ given $n$ and $k$:\n1. The Wald interval based on the maximum likelihood estimate (MLE) $\\hat{p}$ and the large-sample normal approximation.\n2. The Wilson score interval obtained by inverting the score test as derived above.\n3. The Clopper–Pearson interval obtained by inverting the exact binomial cumulative distribution function (CDF).\n\nFor numerical validity, for any interval estimator with endpoints outside the unit interval, truncate endpoints to the unit interval $[0,1]$. For the Clopper–Pearson interval, handle the extreme outcomes $k=0$ and $k=n$ in a way consistent with the exact binomial inversion so that admissible endpoints are always within $[0,1]$.\n\nDefine the coverage probability of an interval procedure for a fixed true $p$ and sample size $n$ as the probability (with respect to the binomial sampling distribution for $K \\sim \\text{Binomial}(n,p)$) that the random interval contains the true parameter $p$:\n$$\n\\text{coverage}(n,p,\\alpha) \\;=\\; \\sum_{k=0}^{n} \\mathbb{I}\\{L(n,k,\\alpha) \\le p \\le U(n,k,\\alpha)\\} \\cdot \\binom{n}{k} p^k (1-p)^{n-k},\n$$\nwhere $[L(n,k,\\alpha),U(n,k,\\alpha)]$ is the interval produced by the method for the realized $k$, and $\\mathbb{I}\\{\\cdot\\}$ is the indicator function. Define the expected length of an interval method at $(n,p,\\alpha)$ as\n$$\n\\text{Elen}(n,p,\\alpha) \\;=\\; \\sum_{k=0}^{n} \\left(U(n,k,\\alpha) - L(n,k,\\alpha)\\right) \\cdot \\binom{n}{k} p^k (1-p)^{n-k}.\n$$\n\nYour program must, for each test case $(n,p,\\alpha)$, compute two floats for each method: the coverage probability and the expected length, using exact summation over $k \\in \\{0,\\dots,n\\}$ and the binomial probability mass function. No Monte Carlo simulation is permitted.\n\nTest Suite:\n- Case 1: $n=10$, $p=0.2$, $\\alpha=0.05$.\n- Case 2: $n=40$, $p=0.5$, $\\alpha=0.05$.\n- Case 3: $n=5$, $p=0.05$, $\\alpha=0.05$.\n- Case 4: $n=30$, $p=0.9$, $\\alpha=0.05$.\n- Case 5: $n=2$, $p=0.5$, $\\alpha=0.10$.\n- Case 6: $n=100$, $p=0.01$, $\\alpha=0.05$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one sub-list per test case. For each test case, output a list of six numbers in the order\n$[\\text{Wald coverage}, \\text{Wald expected length}, \\text{Wilson coverage}, \\text{Wilson expected length}, \\text{Clopper–Pearson coverage}, \\text{Clopper–Pearson expected length}]$.\nThe final line must look like\n$$\n[\\,[w_1,w_2,w_3,w_4,w_5,w_6],\\,[\\dots],\\dots\\,]\n$$\nwith all numeric quantities expressed as decimals (no percentage signs), and the entire output produced on a single line with no additional text.", "solution": "The problem requires the derivation and implementation of three different confidence interval estimators for a binomial proportion $p$, and their subsequent evaluation based on coverage probability and expected length. We will first provide the theoretical background and derivation for each method, as stipulated.\n\n### 1. Derivation and Formulation of Confidence Intervals\n\nLet $K$ be the number of successes in a sequence of $n$ independent and identically distributed Bernoulli trials with success probability $p$. The random variable $K$ follows a binomial distribution, $K \\sim \\text{Binomial}(n,p)$, with probability mass function (PMF) $f(k;p) = \\binom{n}{k} p^k (1-p)^{n-k}$ for $k \\in \\{0, 1, \\dots, n\\}$. Given an observed count of successes $k$, we aim to construct a $100(1-\\alpha)\\%$ confidence interval for $p$.\n\n#### 1.1. Wilson Score Interval\n\nThis interval is derived by inverting the score test for the null hypothesis $H_0: p = p_0$. The derivation proceeds from the binomial log-likelihood function.\n\nThe likelihood for an observed $k$ is $L(p) = \\binom{n}{k} p^k (1-p)^{n-k}$. The log-likelihood is:\n$$ \\ell(p) = \\log L(p) = \\log\\binom{n}{k} + k \\log(p) + (n-k) \\log(1-p) $$\nThe score function, $U(p)$, is the first derivative of the log-likelihood with respect to $p$:\n$$ U(p) = \\frac{d\\ell(p)}{dp} = \\frac{k}{p} - \\frac{n-k}{1-p} = \\frac{k - np}{p(1-p)} $$\nThe Fisher information, $I(p)$, is the negative expectation of the second derivative of the log-likelihood. The second derivative is:\n$$ \\frac{d^2\\ell(p)}{dp^2} = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2} $$\nTaking the negative expectation, with $E[K] = np$, we get:\n$$ I(p) = -E\\left[\\frac{d^2\\ell(p)}{dp^2}\\right] = E\\left[\\frac{K}{p^2} + \\frac{n-K}{(1-p)^2}\\right] = \\frac{np}{p^2} + \\frac{n-np}{(1-p)^2} = \\frac{n}{p} + \\frac{n}{1-p} = \\frac{n}{p(1-p)} $$\nThe score test statistic for the null hypothesis $H_0: p = p_0$ is given by $Z = U(p_0) / \\sqrt{I(p_0)}$. For large $n$, $Z$ is approximately standard normal, $Z \\sim N(0,1)$.\n$$ Z = \\frac{ (k - np_0) / (p_0(1-p_0)) }{ \\sqrt{n / (p_0(1-p_0))} } = \\frac{k - np_0}{\\sqrt{n p_0 (1-p_0)}} $$\nLet $\\hat{p} = k/n$ be the maximum likelihood estimate (MLE). The statistic can be written as $Z = (\\hat{p}-p_0)/\\sqrt{p_0(1-p_0)/n}$. The two-sided score test at significance level $\\alpha$ does not reject $H_0$ if $|Z| \\le z_{\\alpha/2}$, where $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution. The acceptance region for $p_0$ is defined by the inequality:\n$$ \\frac{(\\hat{p} - p_0)^2}{p_0(1-p_0)/n} \\le z_{\\alpha/2}^2 $$\nTo obtain the confidence interval for $p$, we solve this inequality for $p_0$, which we now treat as the variable $p$:\n$$ n(\\hat{p}-p)^2 \\le z_{\\alpha/2}^2 p(1-p) $$\n$$ n(\\hat{p}^2 - 2\\hat{p}p + p^2) \\le z_{\\alpha/2}^2(p-p^2) $$\nRearranging the terms yields a quadratic inequality in $p$:\n$$ (n + z_{\\alpha/2}^2)p^2 - (2n\\hat{p} + z_{\\alpha/2}^2)p + n\\hat{p}^2 \\le 0 $$\nThe endpoints of the confidence interval are the roots of the corresponding quadratic equation $Ap^2 + Bp + C = 0$, where $A = n+z_{\\alpha/2}^2$, $B = -(2n\\hat{p} + z_{\\alpha/2}^2)$, and $C = n\\hat{p}^2$. Using the quadratic formula $p = (-B \\pm \\sqrt{B^2-4AC})/(2A)$, the endpoints are:\n$$ p = \\frac{2n\\hat{p} + z_{\\alpha/2}^2 \\pm \\sqrt{(2n\\hat{p} + z_{\\alpha/2}^2)^2 - 4(n+z_{\\alpha/2}^2)(n\\hat{p}^2)}}{2(n+z_{\\alpha/2}^2)} $$\nThe discriminant simplifies to $B^2 - 4AC = z_{\\alpha/2}^2(4n\\hat{p}(1-\\hat{p}) + z_{\\alpha/2}^2)$. Substituting $\\hat{p}=k/n$:\n$$ p = \\frac{2k + z_{\\alpha/2}^2 \\pm z_{\\alpha/2}\\sqrt{4k(1-k/n) + z_{\\alpha/2}^2}}{2(n+z_{\\alpha/2}^2)} $$\nThis formula defines the lower and upper bounds of the Wilson score interval, $[L_W(k,n,\\alpha), U_W(k,n,\\alpha)]$. The resulting interval is contained within $[0,1]$ by construction.\n\n#### 1.2. Wald Interval\nThe Wald interval is based on the large-sample normal approximation of the distribution of the MLE, $\\hat{p}=k/n$. The variance of $\\hat{p}$ is $p(1-p)/n$, which is estimated by plugging in the MLE, yielding the estimated standard error $\\text{SE}(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}$.\nThe Wald interval is centered at $\\hat{p}$ with a margin of error determined by $z_{\\alpha/2}$:\n$$ \\left[ \\hat{p} - z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\quad \\hat{p} + z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right] $$\nThis interval performs poorly for small $n$ and for $p$ near $0$ or $1$. When $k=0$ or $k=n$, $\\hat{p}$ is $0$ or $1$ respectively, causing the standard error to become $0$ and the interval to collapse to a single point. Per the problem statement, endpoints are truncated to the interval $[0,1]$.\n\n#### 1.3. Clopper–Pearson Interval\nThe Clopper–Pearson interval is an \"exact\" method based on inverting two one-sided binomial tests, ensuring the coverage probability is at least $1-\\alpha$ for all $p$. The endpoints $[p_L, p_U]$ are defined by:\n1. The lower bound $p_L$ is the solution to $P(K \\ge k | p=p_L) = \\sum_{i=k}^{n} \\binom{n}{i}p_L^i(1-p_L)^{n-i} = \\alpha/2$.\n2. The upper bound $p_U$ is the solution to $P(K \\le k | p=p_U) = \\sum_{i=0}^{k} \\binom{n}{i}p_U^i(1-p_U)^{n-i} = \\alpha/2$.\n\nThese equations can be solved using the relationship between the binomial CDF and the regularized incomplete Beta function. The solutions are given by quantiles of the Beta distribution:\n- $p_L = \\text{BetaInv}(\\alpha/2; k, n-k+1)$\n- $p_U = \\text{BetaInv}(1-\\alpha/2; k+1, n-k)$\nwhere $\\text{BetaInv}(q; a, b)$ is the $q$-th quantile of a Beta distribution with shape parameters $a$ and $b$.\nFor the extreme cases:\n- If $k=0$, the lower bound equation is $P(K \\ge 0 | p_L) = 1 = \\alpha/2$, which has no solution. The lower bound is taken to be $p_L=0$.\n- If $k=n$, the upper bound equation is $P(K \\le n | p_U) = 1 = \\alpha/2$, again with no solution. The upper bound is taken to be $p_U=1$.\nThese conventions are consistent with the limits of the Beta distribution quantile function.\n\n### 2. Evaluation Metrics and Algorithm\n\nThe performance of these interval estimators is evaluated using two metrics, calculated by summing over all possible outcomes $k \\in \\{0, 1, \\dots, n\\}$ weighted by their binomial probabilities.\n\n- **Coverage Probability**: The probability that the random interval $[L(K), U(K)]$ contains the true parameter $p$.\n$$ \\text{coverage}(n,p,\\alpha) = \\sum_{k=0}^{n} \\mathbb{I}\\{L(n,k,\\alpha) \\le p \\le U(n,k,\\alpha)\\} \\cdot \\binom{n}{k} p^k (1-p)^{n-k} $$\n- **Expected Length**: The average length of the interval.\n$$ \\text{Elen}(n,p,\\alpha) = \\sum_{k=0}^{n} \\left(U(n,k,\\alpha) - L(n,k,\\alpha)\\right) \\cdot \\binom{n}{k} p^k (1-p)^{n-k} $$\nHere, $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n\nThe algorithm to compute these metrics for a given $(n, p, \\alpha)$ is as follows:\n1. Initialize six accumulators to $0$: one for coverage and one for length for each of the three methods (Wald, Wilson, Clopper-Pearson).\n2. Calculate the standard normal quantile $z_{\\alpha/2}$.\n3. For each integer $k$ from $0$ to $n$:\n   a. Calculate the binomial probability $P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$.\n   b. For each of the three methods, compute the confidence interval $[L, U]$ corresponding to the observed $k$.\n   c. For each method, if the true parameter $p$ lies within its computed interval $[L, U]$, add the probability from step 3a to the method's coverage accumulator.\n   d. For each method, add the product of the interval length $(U-L)$ and the probability from step 3a to the method's expected length accumulator.\n4. After iterating through all $k$, the accumulators will hold the final values for the coverage probability and expected length for each method. The implementation follows this exact summation procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, binom, beta\n\ndef wald_interval(k: int, n: int, z_alpha_2: float) -> tuple[float, float]:\n    \"\"\"Calculates the Wald confidence interval for a binomial proportion.\"\"\"\n    if n == 0:\n        return 0.0, 1.0 # Undefined, return full range\n    \n    p_hat = k / n\n    \n    # For k=0 or k=n, the standard error is 0, leading to a zero-width interval.\n    if p_hat == 0 or p_hat == 1:\n        return p_hat, p_hat\n        \n    se = np.sqrt(p_hat * (1 - p_hat) / n)\n    margin = z_alpha_2 * se\n    lower = p_hat - margin\n    upper = p_hat + margin\n    \n    # Truncate interval to [0, 1] as required.\n    return max(0.0, lower), min(1.0, upper)\n\ndef wilson_interval(k: int, n: int, z_alpha_2: float) -> tuple[float, float]:\n    \"\"\"Calculates the Wilson score confidence interval for a binomial proportion.\"\"\"\n    if n == 0:\n        return 0.0, 1.0\n        \n    z2 = z_alpha_2**2\n    \n    # Use the derived formula which is more numerically stable\n    numerator_center = 2 * k + z2\n    denominator = 2 * (n + z2)\n    \n    sqrt_term = np.sqrt(z2**2 - 4 * z2 * n * (k/n - (k/n)**2) + 4 * z2 * k) # this is = z*sqrt(4k(1-k/n)+z^2)\n    # The term inside the sqrt is: z_alpha_2 * sqrt(4k(n-k)/n + z_alpha_2^2)\n    sqrt_full = z_alpha_2 * np.sqrt(4*k*(1-k/n) + z2)\n    \n    lower = (numerator_center - sqrt_full) / denominator\n    upper = (numerator_center + sqrt_full) / denominator\n    \n    return lower, upper\n\ndef clopper_pearson_interval(k: int, n: int, alpha: float) -> tuple[float, float]:\n    \"\"\"Calculates the Clopper-Pearson exact confidence interval.\"\"\"\n    if n == 0:\n        return 0.0, 1.0\n        \n    alpha_2 = alpha / 2.0\n    \n    # Lower bound: Handle k=0 case explicitly as beta.ppf requires shape > 0.\n    if k == 0:\n        lower = 0.0\n    else:\n        lower = beta.ppf(alpha_2, k, n - k + 1)\n        \n    # Upper bound: Handle k=n case explicitly.\n    if k == n:\n        upper = 1.0\n    else:\n        upper = beta.ppf(1 - alpha_2, k + 1, n - k)\n        \n    return lower, upper\n\ndef calculate_metrics(n: int, p_true: float, alpha: float) -> list[float]:\n    \"\"\"\n    Calculates coverage probability and expected length for the three interval types\n    by summing over all possible outcomes k.\n    \"\"\"\n    z_alpha_2 = norm.ppf(1 - alpha / 2.0)\n    \n    k_values = np.arange(0, n + 1)\n    pmf_values = binom.pmf(k_values, n, p_true)\n    \n    # Initialize metrics accumulators\n    metrics = {\n        'wald': {'coverage': 0.0, 'length': 0.0},\n        'wilson': {'coverage': 0.0, 'length': 0.0},\n        'cp': {'coverage': 0.0, 'length': 0.0},\n    }\n    \n    for k, prob in zip(k_values, pmf_values):\n        # Wald interval\n        lw, uw = wald_interval(k, n, z_alpha_2)\n        if lw <= p_true <= uw:\n            metrics['wald']['coverage'] += prob\n        metrics['wald']['length'] += (uw - lw) * prob\n        \n        # Wilson score interval\n        lwi, uwi = wilson_interval(k, n, z_alpha_2)\n        if lwi <= p_true <= uwi:\n            metrics['wilson']['coverage'] += prob\n        metrics['wilson']['length'] += (uwi - lwi) * prob\n        \n        # Clopper-Pearson interval\n        lcp, ucp = clopper_pearson_interval(k, n, alpha)\n        if lcp <= p_true <= ucp:\n            metrics['cp']['coverage'] += prob\n        metrics['cp']['length'] += (ucp - lcp) * prob\n        \n    return [\n        metrics['wald']['coverage'], metrics['wald']['length'],\n        metrics['wilson']['coverage'], metrics['wilson']['length'],\n        metrics['cp']['coverage'], metrics['cp']['length'],\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results in the specified format.\n    \"\"\"\n    test_cases = [\n        (10, 0.2, 0.05),\n        (40, 0.5, 0.05),\n        (5, 0.05, 0.05),\n        (30, 0.9, 0.05),\n        (2, 0.5, 0.10),\n        (100, 0.01, 0.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, p, alpha = case\n        case_results = calculate_metrics(n, p, alpha)\n        all_results.append(case_results)\n\n    # Format the final output string as per the problem specification.\n    # It must be a single line: [[res1_1,...,res1_6],[res2_1,...,res2_6],...]\n    formatted_results = []\n    for res in all_results:\n        # Format each number to a reasonable number of decimal places for consistency\n        formatted_res_list = [f\"{x:.10f}\".rstrip('0').rstrip('.') for x in res]\n        formatted_res = f\"[{','.join(formatted_res_list)}]\"\n        formatted_results.append(formatted_res)\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "4957555"}]}