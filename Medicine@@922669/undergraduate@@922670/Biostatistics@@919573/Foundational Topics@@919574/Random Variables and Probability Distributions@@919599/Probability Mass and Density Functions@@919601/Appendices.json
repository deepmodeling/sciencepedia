{"hands_on_practices": [{"introduction": "Understanding a probability distribution goes beyond theory; a crucial skill is the ability to generate data that follows it. This practice provides a hands-on method to simulate values from an Exponential distribution, a key model for time-to-event phenomena in biostatistics. By working through this exercise [@problem_id:4942189], you will see how the abstract concept of a cumulative distribution function (CDF) becomes a powerful and practical algorithm for creating synthetic data.", "problem": "In a biostatistics simulation study of time-to-event data with a constant hazard, suppose the waiting time until the next event in a homogeneous Poisson process is modeled by an Exponential distribution with rate parameter $\\lambda>0$. You seek to implement inverse-transform sampling to generate synthetic survival times from a single draw of a Uniform random variable on the unit interval.\n\nStarting from the foundational definitions that: (i) the probability density function (PDF) of an Exponential distribution with rate $\\lambda$ is $f(x)$ supported on $x\\geq 0$ and is nonnegative and integrates to $1$, and (ii) the cumulative distribution function (CDF) $F(x)$ of a continuous random variable is defined by $F(x)=\\int_{-\\infty}^{x} f(t)\\,dt$, derive the explicit form of the CDF $F(x)$ for the Exponential$(\\lambda)$ distribution. Then solve the equation $u=F(x)$ for $x$ to obtain a closed-form expression for the inverse CDF $F^{-1}(u)$ for $0<u<1$, and define the inverse-CDF sampled variable by $X=F^{-1}(U)$ where $U\\sim \\mathrm{Unif}(0,1)$. Using the transformation rule for continuous random variables (change-of-variables) or distribution functions, rigorously verify that $X$ has the Exponential$(\\lambda)$ PDF. Your derivation must begin from the definitions above and proceed step by step without invoking pre-stated formulas for the Exponential CDF or inverse CDF.\n\nReport only the final expression for $X$ in terms of $U$ and $\\lambda$ as your answer. No rounding is required, and no physical units are needed in the final expression.", "solution": "The problem is to derive the formula for generating a random variable $X$ from an Exponential distribution with rate parameter $\\lambda$ using the inverse transform method, starting from a uniform random variable $U$, and to verify that the resulting variable $X$ indeed follows the specified Exponential distribution.\n\nFirst, we validate the problem statement.\nThe problem is scientifically and mathematically sound, as it is based on fundamental principles of probability theory, specifically the Exponential distribution, probability density functions (PDFs), cumulative distribution functions (CDFs), and the inverse transform sampling method. It is a standard, well-posed problem in computational statistics and biostatistics. The problem is objective, complete, and contains no contradictions or ambiguities. The verdict is that the problem is valid.\n\nWe begin the derivation as requested.\n\nThe first principle is the definition of the PDF for an Exponential distribution with a rate parameter $\\lambda > 0$. The random variable, representing waiting time, must be non-negative. The PDF $f(x)$ is given by:\n$$\nf(x) = \\begin{cases} \\lambda \\exp(-\\lambda x) & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\n$$\nThis function is non-negative for $\\lambda > 0$ and integrates to $1$, as $\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x) \\,dx = [-\\exp(-\\lambda x)]_{0}^{\\infty} = 0 - (-1) = 1$.\n\nNext, we derive the cumulative distribution function (CDF), $F(x)$, which is defined as $F(x) = \\int_{-\\infty}^{x} f(t)\\,dt$.\nFor any $x < 0$, the PDF $f(t)$ is $0$, so $F(x) = \\int_{-\\infty}^{x} 0 \\,dt = 0$.\nFor any $x \\geq 0$, the CDF is calculated as:\n$$\nF(x) = \\int_{-\\infty}^{x} f(t)\\,dt = \\int_{-\\infty}^{0} 0\\,dt + \\int_{0}^{x} \\lambda \\exp(-\\lambda t)\\,dt\n$$\n$$\nF(x) = 0 + \\lambda \\int_{0}^{x} \\exp(-\\lambda t)\\,dt\n$$\n$$\nF(x) = \\lambda \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda t) \\right]_{0}^{x} = \\left[ -\\exp(-\\lambda t) \\right]_{0}^{x}\n$$\n$$\nF(x) = (-\\exp(-\\lambda x)) - (-\\exp(-\\lambda \\cdot 0)) = -\\exp(-\\lambda x) - (-1) = 1 - \\exp(-\\lambda x)\n$$\nSo, the CDF for the Exponential($\\lambda$) distribution is:\n$$\nF(x) = \\begin{cases} 1 - \\exp(-\\lambda x) & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\n$$\n\nThe inverse transform method involves setting the CDF equal to a value $u$ drawn from a Uniform distribution on the interval $(0, 1)$, i.e., $u = F(x)$, and solving for $x$. This gives the inverse CDF, $x = F^{-1}(u)$. We are given $U \\sim \\mathrm{Unif}(0,1)$, and $X = F^{-1}(U)$. Since $x \\geq 0$, a valid solution for $x$ will map $u \\in (0, 1)$ to $x \\in [0, \\infty)$.\nWe solve the equation for $x$ for a given $u \\in (0, 1)$:\n$$\nu = 1 - \\exp(-\\lambda x)\n$$\nRearranging the terms to solve for $x$:\n$$\n\\exp(-\\lambda x) = 1 - u\n$$\nTaking the natural logarithm of both sides:\n$$\n\\ln(\\exp(-\\lambda x)) = \\ln(1 - u)\n$$\n$$\n-\\lambda x = \\ln(1 - u)\n$$\n$$\nx = -\\frac{1}{\\lambda} \\ln(1 - u)\n$$\nThis is the expression for the inverse CDF, $F^{-1}(u)$. The random variable $X$ is generated by substituting the uniform random variable $U$ for $u$:\n$$\nX = F^{-1}(U) = -\\frac{1}{\\lambda} \\ln(1 - U)\n$$\nThis is the final expression.\n\nFinally, the problem requires a rigorous verification that $X = -\\frac{1}{\\lambda} \\ln(1 - U)$ has the Exponential($\\lambda$) distribution. We will do this by finding the CDF of $X$, let's call it $G_X(x)$, and showing it is identical to $F(x)$ derived above. The CDF of $X$ is defined as $G_X(x) = P(X \\leq x)$.\n\nFirst, we determine the support of $X$. Since $U \\sim \\mathrm{Unif}(0,1)$, $U$ takes values in the interval $(0, 1)$.\nThen $1-U$ also takes values in $(0, 1)$.\nThe natural logarithm, $\\ln(1-U)$, takes values in $(-\\infty, 0)$.\nSince $\\lambda > 0$, $-\\frac{1}{\\lambda}$ is negative.\nTherefore, $X = -\\frac{1}{\\lambda} \\ln(1 - U)$ takes values in $(0, \\infty)$.\nThe support of $X$ is $x > 0$. This means that for any $x \\leq 0$, $G_X(x) = P(X \\leq x) = 0$.\n\nNow, for $x > 0$, we compute $G_X(x)$:\n$$\nG_X(x) = P(X \\leq x) = P\\left(-\\frac{1}{\\lambda} \\ln(1 - U) \\leq x\\right)\n$$\nWe manipulate the inequality to isolate $U$:\n$$\n\\ln(1 - U) \\geq -\\lambda x\n$$\n(The inequality reverses because we multiply by $-\\lambda$, which is a negative number).\nWe exponentiate both sides. Since the exponential function $y \\mapsto \\exp(y)$ is monotonically increasing, the inequality direction is preserved:\n$$\n\\exp(\\ln(1 - U)) \\geq \\exp(-\\lambda x)\n$$\n$$\n1 - U \\geq \\exp(-\\lambda x)\n$$\n$$\n-U \\geq \\exp(-\\lambda x) - 1\n$$\n$$\nU \\leq 1 - \\exp(-\\lambda x)\n$$\n(The inequality reverses again because we multiply by $-1$).\n\nSo we have:\n$$\nG_X(x) = P(U \\leq 1 - \\exp(-\\lambda x))\n$$\nSince $U \\sim \\mathrm{Unif}(0,1)$, its CDF is $F_U(v) = P(U \\leq v) = v$ for $v \\in [0, 1]$.\nFor $x > 0$, the term $1 - \\exp(-\\lambda x)$ is in the interval $(0, 1)$. Let $v = 1 - \\exp(-\\lambda x)$.\nThen, $G_X(x) = F_U(1 - \\exp(-\\lambda x)) = 1 - \\exp(-\\lambda x)$.\n\nCombining the cases, the CDF of $X$ is:\n$$\nG_X(x) = \\begin{cases} 1 - \\exp(-\\lambda x) & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\n$$\nThis is identical to the CDF of the Exponential($\\lambda$) distribution, $F(x)$, which we derived earlier. By differentiating $G_X(x)$ with respect to $x$ for $x \\geq 0$, we would obtain the PDF $f(x) = \\lambda \\exp(-\\lambda x)$, confirming that $X$ is indeed an Exponentially distributed random variable with rate $\\lambda$. The verification is complete.\n\nThe problem asks for the final expression for $X$ in terms of $U$ and $\\lambda$.", "answer": "$$\n\\boxed{-\\frac{1}{\\lambda}\\ln(1 - U)}\n$$", "id": "4942189"}, {"introduction": "In biostatistical analysis, we often transform variables to better suit our models, for example, by converting probabilities from the $(0,1)$ interval to the entire real line. This practice explores the mathematical consequences of such transformations on the underlying probability density function (PDF). You will derive the new PDF for a logit-transformed variable that originally follows a Beta distribution, a process central to understanding models for proportions and risk [@problem_id:4942226].", "problem": "In a biostatistical model of individual-level risk, suppose the latent risk probability $X \\in (0,1)$ follows a $\\mathrm{Beta}(\\alpha,\\beta)$ distribution with probability density function (PDF)\n$$\nf_{X}(x) \\;=\\; \\frac{1}{B(\\alpha,\\beta)}\\, x^{\\alpha - 1}\\,(1-x)^{\\beta - 1}, \\quad x \\in (0,1),\n$$\nwhere $\\alpha>0$, $\\beta>0$, and $B(\\alpha,\\beta)$ denotes the Beta function. Define the logistic (logit) transformation $Z = \\log\\!\\left(\\frac{X}{1-X}\\right)$, which maps $(0,1)$ onto $\\mathbb{R}$.\n\nStarting from first principles—namely, the definition of a probability density function and the definition of a differentiable, monotone change of variable—derive:\n- the absolute value of the Jacobian determinant of the transformation from $z$ to $x$, and\n- the induced PDF of $Z$.\n\nYour derivation must clearly identify the support of each variable and use only the foundational definitions and well-tested facts, without invoking any pre-stated transformation formulas. Express your final answer as the pair consisting of the Jacobian (as a function of $z$) and the induced PDF $f_{Z}(z)$, both in exact symbolic form. No numerical approximation is required.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in probability theory—the derivation of the probability density function (PDF) of a transformed random variable—using established concepts like the Beta distribution and the logit transformation, which are fundamental in biostatistics. No inconsistencies, ambiguities, or factual errors are present. We may therefore proceed with the derivation.\n\nThe core principle for this derivation is the change of variable theorem for probability density functions. Let $X$ be a continuous random variable with PDF $f_{X}(x)$ defined on a support $S_{X}$. Let $Z = g(X)$ be a new random variable, where the function $g$ is continuously differentiable and strictly monotonic on $S_{X}$. This ensures the existence of a unique inverse function $X = h(Z) = g^{-1}(Z)$. The PDF of $Z$, denoted $f_{Z}(z)$, is given by the formula:\n$$\nf_{Z}(z) = f_{X}(h(z)) \\left| \\frac{dh}{dz}(z) \\right|\n$$\nThe term $\\left| \\frac{dh}{dz} \\right|$ is the absolute value of the Jacobian determinant of the inverse transformation. For a one-dimensional transformation, this is simply the absolute value of the derivative of the inverse function. The support of $Z$, denoted $S_{Z}$, is the image of $S_{X}$ under the transformation $g$.\n\nFirst, we must find the inverse transformation $x = h(z)$. The given transformation is $z = g(x) = \\log\\left(\\frac{x}{1-x}\\right)$.\nTo find the inverse, we solve for $x$:\n$$\nz = \\ln\\left(\\frac{x}{1-x}\\right)\n$$\nExponentiating both sides with base $e$:\n$$\n\\exp(z) = \\frac{x}{1-x}\n$$\nNow, we rearrange the equation to isolate $x$:\n$$\n\\exp(z)(1-x) = x\n$$\n$$\n\\exp(z) - x\\exp(z) = x\n$$\n$$\n\\exp(z) = x + x\\exp(z)\n$$\n$$\n\\exp(z) = x(1 + \\exp(z))\n$$\n$$\nx = \\frac{\\exp(z)}{1+\\exp(z)}\n$$\nThis is the inverse function, $x = h(z)$. This function is often called the logistic sigmoid function.\n\nNext, we identify the support of the new variable $Z$. The support of $X$ is the open interval $(0,1)$. We examine the behavior of $z = \\log\\left(\\frac{x}{1-x}\\right)$ at the boundaries of this interval.\nAs $x \\to 0^{+}$, the ratio $\\frac{x}{1-x} \\to 0^{+}$, and thus $z = \\ln\\left(\\frac{x}{1-x}\\right) \\to -\\infty$.\nAs $x \\to 1^{-}$, the denominator $1-x \\to 0^{+}$, the ratio $\\frac{x}{1-x} \\to +\\infty$, and thus $z = \\ln\\left(\\frac{x}{1-x}\\right) \\to +\\infty$.\nTherefore, the support of $Z$ is the entire real line, $S_Z = (-\\infty, \\infty)$ or $\\mathbb{R}$, as stated in the problem.\n\nNow, we derive the first required quantity: the absolute value of the Jacobian determinant of the transformation from $z$ to $x$. This is $\\left|\\frac{dx}{dz}\\right|$.\nUsing the quotient rule for differentiation on $x(z) = \\frac{\\exp(z)}{1+\\exp(z)}$:\nLet $u = \\exp(z)$ and $v = 1+\\exp(z)$. Then $u' = \\exp(z)$ and $v' = \\exp(z)$.\n$$\n\\frac{dx}{dz} = \\frac{u'v - uv'}{v^2} = \\frac{\\exp(z)(1+\\exp(z)) - \\exp(z)(\\exp(z))}{(1+\\exp(z))^2}\n$$\n$$\n\\frac{dx}{dz} = \\frac{\\exp(z) + \\exp(2z) - \\exp(2z)}{(1+\\exp(z))^2} = \\frac{\\exp(z)}{(1+\\exp(z))^2}\n$$\nSince $\\exp(z) > 0$ for all $z \\in \\mathbb{R}$, the derivative $\\frac{dx}{dz}$ is always positive. This confirms the transformation is strictly monotonic (increasing). The absolute value of the Jacobian is therefore equal to the derivative itself:\n$$\n\\left|\\frac{dx}{dz}\\right| = \\frac{\\exp(z)}{(1+\\exp(z))^2}\n$$\n\nFinally, we derive the induced PDF of $Z$, $f_Z(z)$. We start with the PDF of $X$:\n$$\nf_{X}(x) = \\frac{1}{B(\\alpha,\\beta)}\\, x^{\\alpha - 1}\\,(1-x)^{\\beta - 1}\n$$\nWe substitute $x$ with $h(z) = \\frac{\\exp(z)}{1+\\exp(z)}$. We also need the expression for $1-x$ in terms of $z$:\n$$\n1-x = 1 - \\frac{\\exp(z)}{1+\\exp(z)} = \\frac{1+\\exp(z)-\\exp(z)}{1+\\exp(z)} = \\frac{1}{1+\\exp(z)}\n$$\nNow, substitute these into $f_{X}(x)$:\n$$\nf_{X}(h(z)) = \\frac{1}{B(\\alpha,\\beta)} \\left(\\frac{\\exp(z)}{1+\\exp(z)}\\right)^{\\alpha-1} \\left(\\frac{1}{1+\\exp(z)}\\right)^{\\beta-1}\n$$\nCombine the terms in the denominator:\n$$\nf_{X}(h(z)) = \\frac{1}{B(\\alpha,\\beta)} \\frac{(\\exp(z))^{\\alpha-1}}{(1+\\exp(z))^{\\alpha-1}} \\frac{1}{(1+\\exp(z))^{\\beta-1}} = \\frac{1}{B(\\alpha,\\beta)} \\frac{\\exp(z(\\alpha-1))}{(1+\\exp(z))^{\\alpha+\\beta-2}}\n$$\nNow, we apply the change of variable formula:\n$$\nf_Z(z) = f_X(h(z)) \\left|\\frac{dx}{dz}\\right|\n$$\n$$\nf_Z(z) = \\left( \\frac{1}{B(\\alpha,\\beta)} \\frac{\\exp(z(\\alpha-1))}{(1+\\exp(z))^{\\alpha+\\beta-2}} \\right) \\left( \\frac{\\exp(z)}{(1+\\exp(z))^2} \\right)\n$$\nCombine the numerators and denominators by adding the exponents:\n$$\nf_Z(z) = \\frac{1}{B(\\alpha,\\beta)} \\frac{\\exp(z(\\alpha-1)) \\exp(z)}{(1+\\exp(z))^{\\alpha+\\beta-2} (1+\\exp(z))^2}\n$$\n$$\nf_Z(z) = \\frac{1}{B(\\alpha,\\beta)} \\frac{\\exp(z(\\alpha-1)+z)}{(1+\\exp(z))^{(\\alpha+\\beta-2)+2}}\n$$\n$$\nf_Z(z) = \\frac{1}{B(\\alpha,\\beta)} \\frac{\\exp(\\alpha z)}{(1+\\exp(z))^{\\alpha+\\beta}}\n$$\nThis is the PDF of the random variable $Z$, defined for all $z \\in \\mathbb{R}$. This distribution is known as a generalized logistic distribution or a log-beta distribution.\n\nThe two required quantities are the absolute value of the Jacobian, $\\frac{\\exp(z)}{(1+\\exp(z))^2}$, and the PDF $f_Z(z)$, $\\frac{1}{B(\\alpha,\\beta)} \\frac{\\exp(\\alpha z)}{(1+\\exp(z))^{\\alpha+\\beta}}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\exp(z)}{(1+\\exp(z))^{2}} & \\frac{1}{B(\\alpha,\\beta)} \\frac{\\exp(\\alpha z)}{(1+\\exp(z))^{\\alpha+\\beta}} \\end{pmatrix}}\n$$", "id": "4942226"}, {"introduction": "This practice bridges the gap from probability theory to statistical inference, addressing a fundamental question: once we choose a plausible probability model for our data, how do we estimate its unknown parameters? This exercise walks through the method of maximum likelihood estimation (MLE), a cornerstone of modern biostatistics. You will apply this powerful technique to find the best-fit parameter for a Gamma distribution based on a set of observations, a common task when modeling waiting times in biological processes [@problem_id:4942238].", "problem": "A biostatistics team models the total observation time until a fixed number of molecular binding events occur for each subject in a standardized assay. For each subject $i \\in \\{1,\\dots,n\\}$, the total waiting time $X_i$ until $k$ events occur is assumed to be independent and identically distributed (i.i.d.) according to a Gamma distribution with known shape parameter $k>0$ and unknown scale parameter $\\theta>0$, denoted $X_i \\sim \\mathrm{Gamma}(k,\\theta)$. The probability density function (PDF) of $X_i$ is\n$$\nf(x_i; k,\\theta) = \\frac{x_i^{k-1} \\exp(-x_i/\\theta)}{\\Gamma(k)\\,\\theta^{k}}, \\quad x_i>0,\n$$\nwhere $\\Gamma(k)$ is the Gamma function. You observe realizations $x_1,\\dots,x_n$.\n\nStarting from the definition of the likelihood for i.i.d. samples and the definition of the score function $U(\\theta)=\\frac{\\partial}{\\partial \\theta}\\ell(\\theta)$, where $\\ell(\\theta)$ is the log-likelihood, and the Fisher Information (FI) $I(\\theta)=-\\mathbb{E}\\!\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\ell(\\theta)\\right]$, derive the exact forms of the score $U(\\theta)$ and the Fisher Information $I(\\theta)$ and solve for the Maximum Likelihood Estimator (MLE) of $\\theta$, denoted $\\hat{\\theta}$. Express your final answer as a single row vector whose entries are $U(\\theta)$, $I(\\theta)$, and $\\hat{\\theta}$, respectively, in terms of $n$, $k$, and the observed data $\\{x_i\\}_{i=1}^n$. Provide exact expressions; no rounding is necessary.", "solution": "The user has provided a problem in the field of biostatistics, specifically focusing on parameter estimation for the Gamma distribution. I will first validate the problem statement and then, if it is valid, provide a complete derivation.\n\n### Step 1: Extract Givens\n-   The data consists of $n$ independent and identically distributed (i.i.d.) random variables $X_i$, for $i \\in \\{1, \\dots, n\\}$.\n-   Each $X_i$ represents the total waiting time for $k$ events for a subject.\n-   The distribution of each $X_i$ is a Gamma distribution, denoted $X_i \\sim \\mathrm{Gamma}(k,\\theta)$.\n-   The shape parameter $k>0$ is known.\n-   The scale parameter $\\theta>0$ is unknown.\n-   The probability density function (PDF) is given as $f(x_i; k,\\theta) = \\frac{x_i^{k-1} \\exp(-x_i/\\theta)}{\\Gamma(k)\\,\\theta^{k}}$ for $x_i>0$.\n-   The observed data are a set of realizations $\\{x_1, \\dots, x_n\\}$.\n-   The log-likelihood function is denoted by $\\ell(\\theta)$.\n-   The score function is defined as $U(\\theta)=\\frac{\\partial}{\\partial \\theta}\\ell(\\theta)$.\n-   The Fisher Information (FI) is defined as $I(\\theta)=-\\mathbb{E}\\!\\left[\\frac{\\partial^2}{\\partial \\theta^2}\\ell(\\theta)\\right]$.\n-   The Maximum Likelihood Estimator (MLE) of $\\theta$ is denoted by $\\hat{\\theta}$.\n-   The task is to derive the exact forms of $U(\\theta)$, $I(\\theta)$, and $\\hat{\\theta}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It is a standard exercise in maximum likelihood estimation theory, which is a cornerstone of classical statistics and widely applied in biostatistics. The Gamma distribution is a common and appropriate model for waiting times. The definitions of the score function and Fisher information are standard. The problem is self-contained, well-posed, and free of ambiguity or contradiction. It is neither trivial nor overly complex, representing a standard derivation in the field. Therefore, the problem is deemed **valid**.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed to the solution.\n\n###\nThe solution involves three main parts: deriving the score function $U(\\theta)$, finding the Maximum Likelihood Estimator $\\hat{\\theta}$, and calculating the Fisher Information $I(\\theta)$.\n\n#### Likelihood and Log-Likelihood Functions\nGiven that the observations $x_1, \\dots, x_n$ are realizations of i.i.d. random variables $X_1, \\dots, X_n$, the likelihood function $L(\\theta)$ is the product of the individual probability density functions:\n$$\nL(\\theta) = \\prod_{i=1}^{n} f(x_i; k, \\theta) = \\prod_{i=1}^{n} \\frac{x_i^{k-1} \\exp(-x_i/\\theta)}{\\Gamma(k)\\,\\theta^{k}}\n$$\nWe can separate the terms that depend on $\\theta$ from those that do not:\n$$\nL(\\theta) = \\left( \\prod_{i=1}^{n} \\frac{x_i^{k-1}}{\\Gamma(k)} \\right) \\left( \\frac{1}{\\theta^k} \\right)^n \\exp\\left(-\\sum_{i=1}^{n} \\frac{x_i}{\\theta}\\right) = \\left( \\prod_{i=1}^{n} \\frac{x_i^{k-1}}{\\Gamma(k)} \\right) \\frac{1}{\\theta^{nk}} \\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^{n} x_i\\right)\n$$\nThe log-likelihood function $\\ell(\\theta)$ is the natural logarithm of the likelihood function:\n$$\n\\ell(\\theta) = \\ln(L(\\theta)) = \\ln\\left[ \\left( \\prod_{i=1}^{n} \\frac{x_i^{k-1}}{\\Gamma(k)} \\right) \\frac{1}{\\theta^{nk}} \\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^{n} x_i\\right) \\right]\n$$\nUsing the properties of logarithms, we can expand this expression:\n$$\n\\ell(\\theta) = \\sum_{i=1}^{n} \\ln\\left( \\frac{x_i^{k-1}}{\\Gamma(k)} \\right) + \\ln(\\theta^{-nk}) + \\ln\\left( \\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^{n} x_i\\right) \\right)\n$$\n$$\n\\ell(\\theta) = \\sum_{i=1}^{n} ((k-1)\\ln(x_i) - \\ln(\\Gamma(k))) - nk\\ln(\\theta) - \\frac{1}{\\theta}\\sum_{i=1}^{n} x_i\n$$\nThe first term is a constant with respect to $\\theta$, so we can write $\\ell(\\theta)$ up to an additive constant as:\n$$\n\\ell(\\theta) = \\mathrm{const} - nk\\ln(\\theta) - \\frac{1}{\\theta}\\sum_{i=1}^{n} x_i\n$$\n\n#### Derivation of the Score Function $U(\\theta)$\nThe score function $U(\\theta)$ is the first derivative of the log-likelihood function with respect to the parameter $\\theta$.\n$$\nU(\\theta) = \\frac{\\partial \\ell(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( \\mathrm{const} - nk\\ln(\\theta) - \\left(\\sum_{i=1}^{n} x_i\\right)\\theta^{-1} \\right)\n$$\n$$\nU(\\theta) = 0 - nk\\left(\\frac{1}{\\theta}\\right) - \\left(\\sum_{i=1}^{n} x_i\\right)(-1)\\theta^{-2}\n$$\n$$\nU(\\theta) = -\\frac{nk}{\\theta} + \\frac{\\sum_{i=1}^{n} x_i}{\\theta^2}\n$$\nThis can be written over a common denominator:\n$$\nU(\\theta) = \\frac{\\sum_{i=1}^{n} x_i - nk\\theta}{\\theta^2}\n$$\nThis is the first required expression.\n\n#### Derivation of the Maximum Likelihood Estimator $\\hat{\\theta}$\nThe Maximum Likelihood Estimator (MLE) $\\hat{\\theta}$ is the value of $\\theta$ that maximizes the likelihood function, which is equivalent to maximizing the log-likelihood function. We find this by setting the score function (the first derivative) to zero and solving for $\\theta$.\n$$\nU(\\hat{\\theta}) = 0 \\implies \\frac{\\sum_{i=1}^{n} x_i - nk\\hat{\\theta}}{\\hat{\\theta}^2} = 0\n$$\nSince $\\theta > 0$, we know $\\hat{\\theta}^2 \\neq 0$. Thus, the numerator must be zero:\n$$\n\\sum_{i=1}^{n} x_i - nk\\hat{\\theta} = 0\n$$\nSolving for $\\hat{\\theta}$:\n$$\nnk\\hat{\\theta} = \\sum_{i=1}^{n} x_i \\implies \\hat{\\theta} = \\frac{\\sum_{i=1}^{n} x_i}{nk}\n$$\nTo confirm this is a maximum, we check the sign of the second derivative of $\\ell(\\theta)$.\n$$\n\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2} = \\frac{\\partial U(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( -nk\\theta^{-1} + \\left(\\sum_{i=1}^{n} x_i\\right)\\theta^{-2} \\right) = -nk(-1)\\theta^{-2} + \\left(\\sum_{i=1}^{n} x_i\\right)(-2)\\theta^{-3}\n$$\n$$\n\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2} = \\frac{nk}{\\theta^2} - \\frac{2\\sum_{i=1}^{n} x_i}{\\theta^3}\n$$\nEvaluating at $\\theta=\\hat{\\theta}$ and substituting $\\sum_{i=1}^{n} x_i = nk\\hat{\\theta}$:\n$$\n\\left. \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2} \\right|_{\\theta=\\hat{\\theta}} = \\frac{nk}{\\hat{\\theta}^2} - \\frac{2(nk\\hat{\\theta})}{\\hat{\\theta}^3} = \\frac{nk}{\\hat{\\theta}^2} - \\frac{2nk}{\\hat{\\theta}^2} = -\\frac{nk}{\\hat{\\theta}^2}\n$$\nSince $n > 0$, $k > 0$, and $\\hat{\\theta}^2 > 0$, the second derivative is negative. This confirms that $\\hat{\\theta}$ is a local maximum. As it's the sole critical point in the parameter space, it is the global maximum.\nThe MLE is $\\hat{\\theta} = \\frac{\\sum_{i=1}^{n} x_i}{nk}$.\n\n#### Derivation of the Fisher Information $I(\\theta)$\nThe Fisher Information $I(\\theta)$ is defined as the negative of the expected value of the second derivative of the log-likelihood function.\n$$\nI(\\theta) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2}\\right] = -\\mathbb{E}\\left[\\frac{nk}{\\theta^2} - \\frac{2\\sum_{i=1}^{n} X_i}{\\theta^3}\\right]\n$$\nUsing the linearity of expectation:\n$$\nI(\\theta) = -\\left( \\mathbb{E}\\left[\\frac{nk}{\\theta^2}\\right] - \\mathbb{E}\\left[\\frac{2\\sum_{i=1}^{n} X_i}{\\theta^3}\\right] \\right) = -\\frac{nk}{\\theta^2} + \\frac{2}{\\theta^3}\\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right]\n$$\nThe mean of a $\\mathrm{Gamma}(k,\\theta)$ distribution with the given PDF is $\\mathbb{E}[X_i] = k\\theta$. Since the $X_i$ are identically distributed:\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_i] = \\sum_{i=1}^{n} k\\theta = nk\\theta\n$$\nSubstituting this result into the expression for $I(\\theta)$:\n$$\nI(\\theta) = -\\frac{nk}{\\theta^2} + \\frac{2}{\\theta^3}(nk\\theta) = -\\frac{nk}{\\theta^2} + \\frac{2nk}{\\theta^2} = \\frac{nk}{\\theta^2}\n$$\nThis is the Fisher Information for the parameter $\\theta$.\n\nThe three derived quantities are:\n1.  Score Function: $U(\\theta) = \\frac{\\sum_{i=1}^{n} x_i - nk\\theta}{\\theta^2}$\n2.  Fisher Information: $I(\\theta) = \\frac{nk}{\\theta^2}$\n3.  Maximum Likelihood Estimator: $\\hat{\\theta} = \\frac{\\sum_{i=1}^{n} x_i}{nk}$\nThese are to be presented as a single row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sum_{i=1}^n x_i - nk\\theta}{\\theta^2} & \\frac{nk}{\\theta^2} & \\frac{\\sum_{i=1}^n x_i}{nk} \\end{pmatrix}}\n$$", "id": "4942238"}]}