{"hands_on_practices": [{"introduction": "In many real-world scenarios, outcomes are not purely discrete or continuous. This practice explores a mixed random variable, which combines a discrete probability mass at a single point with a continuous distribution over an interval. Such models are common in biostatistics, for instance, when studying treatment responses that can be either immediate or delayed over time. By deriving the expectation from first principles, you will solidify your understanding of how to handle these powerful hybrid distributions [@problem_id:4944773].", "problem": "A biostatistics team is studying the time to immediate biochemical response in a cohort where some individuals respond at baseline while others respond later. Let $X$ denote the time to response (in hours). Empirically, there is a point mass at $0$ representing immediate response with probability $p$, and a continuous component for delayed response on $(0,\\infty)$ with probability density function (PDF) $(1-p)\\lambda \\exp(-\\lambda x)$, where $\\lambda0$ is a rate parameter. Thus, $X$ is a mixed random variable with a probability mass function (PMF) at $0$ equal to $p$ and a PDF on $(0,\\infty)$ equal to $(1-p)\\lambda \\exp(-\\lambda x)$. Assume $0 \\leq p \\leq 1$ and $\\lambda0$.\n\nStarting from the fundamental definition of expectation as a Lebesgue integral with respect to the distribution of $X$ (i.e., $E[g(X)] = \\int g(x)\\,\\mathrm{d}F_X(x)$), and using only well-established facts about integration and exponentials, compute the exact closed-form expressions for $E[X]$ and $E[|X|]$ in terms of $p$ and $\\lambda$. Then, discuss the integrability conditions for $X$ under the Lebesgue definition (that is, when $E[|X|]$ is finite) in terms of the parameters $p$ and $\\lambda$, and provide clear reasoning tied to the structure of the mixed distribution.\n\nExpress the final results for $E[X]$ and $E[|X|]$ as exact algebraic expressions in $p$ and $\\lambda$. No numerical rounding is required. Do not include units in your final expressions, but you may refer to units in your explanation.", "solution": "The problem statement is assessed to be valid as it is scientifically grounded, self-contained, and well-posed. It describes a mixed random variable, a standard concept in probability theory and biostatistics, and asks for the derivation of its expectation and a discussion of its integrability, which are well-defined mathematical tasks.\n\nThe random variable $X$, representing the time to response, has a mixed distribution. Its probability measure is composed of a discrete part and a continuous part.\n1.  A discrete point mass at $X=0$ with probability $P(X=0) = p$.\n2.  A continuous distribution on the interval $(0, \\infty)$ with a probability density function given by $f_c(x) = (1-p)\\lambda \\exp(-\\lambda x)$ for $x0$.\n\nThe total probability is the sum of the probability of the discrete event and the integral of the continuous density over its support:\n$$P(X=0) + \\int_{0}^{\\infty} f_c(x) \\, \\mathrm{d}x = p + \\int_0^\\infty (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThe integral evaluates to:\n$$(1-p)\\lambda \\int_0^\\infty \\exp(-\\lambda x) \\, \\mathrm{d}x = (1-p)\\lambda \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda x) \\right]_0^\\infty = (1-p) \\left( \\lim_{x\\to\\infty} (-\\exp(-\\lambda x)) - (-\\exp(0)) \\right)$$\nSince $\\lambda  0$, $\\lim_{x\\to\\infty} \\exp(-\\lambda x) = 0$. Thus, the integral is $(1-p)(0 - (-1)) = 1-p$.\nThe total probability is $p + (1-p) = 1$, which confirms the validity of the distribution.\n\nThe problem requires computing the expectation of a function $g(X)$ starting from the fundamental definition $E[g(X)] = \\int g(x)\\,\\mathrm{d}F_X(x)$, where $F_X(x)$ is the cumulative distribution function of $X$. For a mixed distribution like this, the Lebesgue-Stieltjes integral separates into a sum over the discrete mass points and a standard Riemann integral over the continuous domain.\n$$E[g(X)] = g(0)P(X=0) + \\int_0^\\infty g(x) f_c(x) \\, \\mathrm{d}x$$\n$$E[g(X)] = g(0) \\cdot p + \\int_0^\\infty g(x) (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\n\nFirst, we compute the expectation of $X$, denoted $E[X]$. This corresponds to the case where $g(x) = x$.\n$$E[X] = (0) \\cdot p + \\int_0^\\infty x (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\n$$E[X] = (1-p)\\lambda \\int_0^\\infty x \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nTo evaluate the integral, we use integration by parts, $\\int u \\, \\mathrm{d}v = uv - \\int v \\, \\mathrm{d}u$. Let $u=x$ and $\\mathrm{d}v = \\exp(-\\lambda x) \\, \\mathrm{d}x$. Then $\\mathrm{d}u = \\mathrm{d}x$ and $v = -\\frac{1}{\\lambda}\\exp(-\\lambda x)$.\n$$\\int_0^\\infty x \\exp(-\\lambda x) \\, \\mathrm{d}x = \\left[ x \\left(-\\frac{1}{\\lambda}\\exp(-\\lambda x)\\right) \\right]_0^\\infty - \\int_0^\\infty \\left(-\\frac{1}{\\lambda}\\exp(-\\lambda x)\\right) \\, \\mathrm{d}x$$\n$$= \\left[ -\\frac{x}{\\lambda}\\exp(-\\lambda x) \\right]_0^\\infty + \\frac{1}{\\lambda} \\int_0^\\infty \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThe first term is evaluated at the limits: $\\lim_{x\\to\\infty} \\left(-\\frac{x}{\\lambda}\\exp(-\\lambda x)\\right) - 0$. The limit as $x\\to\\infty$ is $0$, which can be verified using L'Hôpital's rule on $\\frac{x}{\\exp(\\lambda x)}$. The second term is $\\frac{1}{\\lambda}$ times the integral of an exponential PDF (without its rate constant), which we already know evaluates to $\\frac{1}{\\lambda}$.\n$$\\int_0^\\infty x \\exp(-\\lambda x) \\, \\mathrm{d}x = 0 + \\frac{1}{\\lambda} \\left[ -\\frac{1}{\\lambda}\\exp(-\\lambda x) \\right]_0^\\infty = \\frac{1}{\\lambda^2} (-\\exp(-\\infty) - (-\\exp(0))) = \\frac{1}{\\lambda^2}(0 - (-1)) = \\frac{1}{\\lambda^2}$$\nSubstituting this result back into the expression for $E[X]$:\n$$E[X] = (1-p)\\lambda \\left(\\frac{1}{\\lambda^2}\\right) = \\frac{1-p}{\\lambda}$$\n\nNext, we compute $E[|X|]$, which corresponds to the case $g(x) = |x|$.\n$$E[|X|] = |0| \\cdot p + \\int_0^\\infty |x| (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThe support of the random variable $X$ is the set $\\{0\\} \\cup (0, \\infty)$, where all values are non-negative. Therefore, for any value $x$ that $X$ can take, $|x|=x$.\nThis implies that $|X| = X$, and consequently, their expectations must be equal.\n$$E[|X|] = E[X] = \\frac{1-p}{\\lambda}$$\nTo be rigorous, we show this from the integral definition:\n$$E[|X|] = 0 \\cdot p + \\int_0^\\infty x (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThis is precisely the same expression as for $E[X]$, yielding the same result.\n$$E[|X|] = \\frac{1-p}{\\lambda}$$\n\nFinally, we discuss the integrability conditions for $X$ under the Lebesgue definition. A random variable $X$ is defined as integrable if its expectation exists and is finite, which is equivalent to the condition $E[|X|]  \\infty$.\nFrom our calculation, we have:\n$$E[|X|] = \\frac{1-p}{\\lambda}$$\nThe problem provides the constraints on the parameters: $0 \\le p \\le 1$ and $\\lambda  0$.\nUnder these constraints:\n- The numerator, $1-p$, is in the range $[0, 1]$.\n- The denominator, $\\lambda$, is strictly positive, so $\\lambda \\in (0, \\infty)$.\nThe ratio $\\frac{1-p}{\\lambda}$ is therefore always non-negative and finite. The only way it could be infinite is if $\\lambda=0$, but this is explicitly excluded by the problem statement.\nTherefore, $E[|X|]$ is always finite for any valid choice of parameters $p$ and $\\lambda$.\n\nThe reason for this guaranteed integrability is rooted in the structure of the distribution. The total expectation $E[|X|]$ is the sum of contributions from the discrete and continuous parts. The discrete part at $x=0$ contributes $|0| \\cdot p = 0$, which is finite. The continuous part's contribution is determined by the integral $\\int_0^\\infty x (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$. The convergence of this integral depends on the tail behavior of the density. The term $\\exp(-\\lambda x)$ decays to zero much faster than the term $x$ grows to infinity, for any $\\lambda  0$. This rapid exponential decay ensures that the integral is finite. Thus, the random variable $X$ is always integrable given the specified parameter ranges.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1-p}{\\lambda}  \\frac{1-p}{\\lambda} \\end{pmatrix}}$$", "id": "4944773"}, {"introduction": "A fundamental task in biostatistics is to estimate the parameters of a model using data from clinical studies, which is often incomplete. This exercise provides hands-on experience with maximum likelihood estimation (MLE) for survival data, a cornerstone of modern biostatistics. You will learn to construct a likelihood function that correctly accounts for right-censored observations—patients who leave a study before the event of interest occurs—and use it to estimate the event rate in a practical scenario [@problem_id:4944733].", "problem": "A clinical study follows $n=12$ patients from enrollment to either a biomedical event of interest or administrative right-censoring. Assume the time-to-event for each patient is a realization from an independent and identically distributed (i.i.d.) exponential random variable with rate parameter $\\lambda0$, and that right-censoring is non-informative. For patient $i$, let $t_i$ denote the observed follow-up time (in months), and let $\\delta_i$ be the observed event indicator, where $\\delta_i=1$ if the event occurred at time $t_i$ and $\\delta_i=0$ if the observation was right-censored at time $t_i$. The observed data are:\n- Follow-up times (months): $2.1, 3.5, 5.2, 1.8, 4.0, 6.7, 3.3, 7.5, 2.9, 5.0, 8.2, 4.6$.\n- Event indicators: $1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0$.\n\nStarting from first principles of likelihood for independent continuous-time survival data with right-censoring, write the full likelihood in terms of $\\lambda$, the observed times $\\{t_i\\}$, and event indicators $\\{\\delta_i\\}$. Differentiate the log-likelihood to obtain the score equation, solve for the maximum likelihood estimator of the rate parameter $\\lambda$, and then compute its numerical value for the dataset above. Round your final numerical answer to four significant figures. Express the final rate in per month.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Number of patients: $n=12$.\n- The time-to-event for each patient is a realization from an independent and identically distributed (i.i.d.) exponential random variable with rate parameter $\\lambda  0$.\n- Right-censoring is non-informative.\n- For patient $i$, $t_i$ is the observed follow-up time (in months).\n- For patient $i$, $\\delta_i$ is the observed event indicator, where $\\delta_i=1$ for an event and $\\delta_i=0$ for right-censoring.\n- Observed follow-up times $\\{t_i\\}_{i=1}^{12}$: $2.1, 3.5, 5.2, 1.8, 4.0, 6.7, 3.3, 7.5, 2.9, 5.0, 8.2, 4.6$.\n- Observed event indicators $\\{\\delta_i\\}_{i=1}^{12}$: $1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the exponential distribution and the principle of maximum likelihood for right-censored data, which are standard and fundamental concepts in biostatistics and survival analysis. The model is scientifically sound.\n- **Well-Posed:** The problem provides all necessary data and clear definitions. The objective is to find the maximum likelihood estimator (MLE) for a well-defined parameter of a standard distribution, which has a unique and meaningful solution.\n- **Objective:** The problem is stated with precise, quantitative, and unbiased language.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe time-to-event $T$ for a patient is assumed to follow an exponential distribution with rate parameter $\\lambda  0$. The probability density function (PDF) is given by:\n$$f(t|\\lambda) = \\lambda \\exp(-\\lambda t), \\quad t \\ge 0$$\nThe corresponding survival function, $S(t|\\lambda)$, which is the probability that the event has not occurred by time $t$, is:\n$$S(t|\\lambda) = P(T  t) = \\int_{t}^{\\infty} \\lambda \\exp(-\\lambda u) du = \\exp(-\\lambda t)$$\nThe likelihood function is constructed based on the contributions from each of the $n=12$ independent patients. The contribution of patient $i$ depends on whether their event was observed or their observation was right-censored.\n\nFor a patient $i$ who experiences the event at time $t_i$ (i.e., $\\delta_i=1$), their contribution to the likelihood is the probability density of the event occurring at that specific time, which is $f(t_i|\\lambda)$.\n\nFor a patient $i$ who is right-censored at time $t_i$ (i.e., $\\delta_i=0$), we only know that their true event time is greater than $t_i$. Their contribution to the likelihood is the probability of this, which is given by the survival function $S(t_i|\\lambda)$.\n\nWe can write the likelihood contribution for a single patient $i$ in a general form using the event indicator $\\delta_i$:\n$$L_i(\\lambda) = [f(t_i|\\lambda)]^{\\delta_i} [S(t_i|\\lambda)]^{1-\\delta_i}$$\nSubstituting the expressions for the exponential PDF and survival function:\n$$L_i(\\lambda) = [\\lambda \\exp(-\\lambda t_i)]^{\\delta_i} [\\exp(-\\lambda t_i)]^{1-\\delta_i}$$\nThis expression can be simplified by combining the exponential terms:\n$$L_i(\\lambda) = \\lambda^{\\delta_i} \\exp(-\\lambda t_i \\delta_i) \\exp(-\\lambda t_i (1-\\delta_i)) = \\lambda^{\\delta_i} \\exp(-\\lambda t_i (\\delta_i + 1 - \\delta_i)) = \\lambda^{\\delta_i} \\exp(-\\lambda t_i)$$\nSince the observations are independent, the total likelihood function $L(\\lambda)$ for the entire sample of $n$ patients is the product of the individual likelihood contributions:\n$$L(\\lambda) = \\prod_{i=1}^{n} L_i(\\lambda) = \\prod_{i=1}^{n} \\left(\\lambda^{\\delta_i} \\exp(-\\lambda t_i)\\right)$$\nThis simplifies to:\n$$L(\\lambda) = \\left(\\prod_{i=1}^{n} \\lambda^{\\delta_i}\\right) \\left(\\prod_{i=1}^{n} \\exp(-\\lambda t_i)\\right) = \\lambda^{\\sum_{i=1}^{n} \\delta_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)$$\nTo find the maximum likelihood estimator (MLE) for $\\lambda$, it is mathematically more convenient to maximize the log-likelihood function, $\\ell(\\lambda) = \\ln(L(\\lambda))$:\n$$\\ell(\\lambda) = \\ln\\left(\\lambda^{\\sum_{i=1}^{n} \\delta_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)\\right)$$\n$$\\ell(\\lambda) = \\left(\\sum_{i=1}^{n} \\delta_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i$$\nTo find the value of $\\lambda$ that maximizes $\\ell(\\lambda)$, we differentiate $\\ell(\\lambda)$ with respect to $\\lambda$ and set the result to zero. This derivative is known as the score function, $U(\\lambda)$.\n$$U(\\lambda) = \\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda}\\left[\\left(\\sum_{i=1}^{n} \\delta_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i\\right] = \\frac{\\sum_{i=1}^{n} \\delta_i}{\\lambda} - \\sum_{i=1}^{n} t_i$$\nThe score equation is $U(\\hat{\\lambda}) = 0$, where $\\hat{\\lambda}$ denotes the MLE:\n$$\\frac{\\sum_{i=1}^{n} \\delta_i}{\\hat{\\lambda}} - \\sum_{i=1}^{n} t_i = 0$$\nSolving for $\\hat{\\lambda}$:\n$$\\hat{\\lambda} = \\frac{\\sum_{i=1}^{n} \\delta_i}{\\sum_{i=1}^{n} t_i}$$\nThe MLE for the rate parameter is the total number of observed events divided by the total person-time of follow-up.\n\nNow, we compute the numerical value of $\\hat{\\lambda}$ using the provided data.\nFirst, we calculate the total number of events, $d = \\sum_{i=1}^{12} \\delta_i$:\n$$d = 1+1+0+1+0+1+1+0+1+1+0+0 = 7$$\nNext, we calculate the total person-time of follow-up, $T = \\sum_{i=1}^{12} t_i$:\n$$T = 2.1+3.5+5.2+1.8+4.0+6.7+3.3+7.5+2.9+5.0+8.2+4.6 = 54.8 \\text{ months}$$\nFinally, we compute the estimate $\\hat{\\lambda}$:\n$$\\hat{\\lambda} = \\frac{d}{T} = \\frac{7}{54.8} \\approx 0.127737226...$$\nThe problem requires the numerical answer to be rounded to four significant figures.\n$$\\hat{\\lambda} \\approx 0.1277$$\nThe units of the rate are events per month.", "answer": "$$\\boxed{0.1277}$$", "id": "4944733"}, {"introduction": "Computational simulation is an indispensable tool in modern statistics, used for everything from validating new methods to conducting complex analyses. This practice delves into the practical challenge of generating random samples from a specified, non-standard distribution—the truncated normal. By deriving and implementing an efficient algorithm based on the inverse transform method, you will gain insight into the mechanics of statistical simulation and appreciate the importance of algorithmic efficiency over more naive approaches [@problem_id:4944715].", "problem": "You are to design and analyze a method to simulate from a truncated normal distribution in a biostatistical setting and to quantify the efficiency of a naive rejection sampler. Consider a continuous random variable $X$ following a normal distribution with mean $\\mu$ and variance $\\sigma^2$, denoted $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$. You wish to draw samples from the conditional distribution of $X$ restricted to the open interval $(a,b)$, that is, from the truncated normal distribution on $(a,b)$. In addition, you wish to analyze the acceptance rate of a naive rejection sampler that proposes from $\\mathcal{N}(\\mu,\\sigma^2)$ and accepts if the proposal lies in $(a,b)$.\n\nYour tasks are as follows:\n\n- From first principles, using only core definitions such as the definition of a probability density function and a cumulative distribution function, derive a correct and computationally efficient algorithm for sampling from the truncated normal distribution of $X$ restricted to $(a,b)$, expressed entirely in terms of transforms of the standard normal distribution. Your algorithm must be valid for any finite real numbers $ab$ and any $\\sigma0$.\n- Starting from definitions, derive a general expression for the acceptance probability of the naive rejection sampler that proposes $X^\\ast \\sim \\mathcal{N}(\\mu,\\sigma^2)$ and accepts if $aX^\\astb$. Express your final result in terms of the standard normal cumulative distribution function.\n- Implement the sampling algorithm and the acceptance probability formula in a single runnable program. The program must be self-contained and must not require any input. It must compute the analytical acceptance probability for a fixed test suite and print only the acceptance probabilities for those test cases.\n\nUse the following test suite of parameter values, representing a diverse set of scenarios including typical, tail, wide, and narrow truncation intervals:\n\n- Test case $1$: $(\\mu,\\sigma,a,b)=(0,1,-1,1)$.\n- Test case $2$: $(\\mu,\\sigma,a,b)=(0,1,2,3)$.\n- Test case $3$: $(\\mu,\\sigma,a,b)=(5,2,4,6)$.\n- Test case $4$: $(\\mu,\\sigma,a,b)=(0,1,-10,10)$.\n- Test case $5$: $(\\mu,\\sigma,a,b)=(0,1,0,0.0001)$.\n\nYour program should produce a single line of output containing the analytical acceptance probabilities for the naive rejection sampler for each test case, in the same order as above, each rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets. For example, an output of the form $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is a decimal number rounded to $6$ decimal places, is required.\n\nNo physical units are involved in this problem. Express all final numerical answers as decimal numbers rounded to $6$ decimal places.", "solution": "We begin from fundamental definitions and standard properties of the normal distribution. Let $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$ with probability density function (PDF)\n$$\nf_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n$$\nand cumulative distribution function (CDF)\n$$\nF_X(x)=\\int_{-\\infty}^{x} f_X(t)\\,dt.\n$$\nDefine the standardized variable $Z=\\frac{X-\\mu}{\\sigma}$, which follows the standard normal distribution $Z \\sim \\mathcal{N}(0,1)$ with PDF $\\varphi(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}$ and CDF $\\Phi(z)=\\int_{-\\infty}^{z}\\varphi(t)\\,dt$.\n\nPart one: sampling algorithm for the truncated normal on $(a,b)$. The truncated distribution of $X$ on $(a,b)$ has conditional density\n$$\nf_{X\\mid aXb}(x)=\\frac{f_X(x)}{\\mathbb{P}(aXb)}\\mathbf{1}_{(a,b)}(x),\n$$\nwhere $\\mathbf{1}_{(a,b)}(x)$ is the indicator function of the interval $(a,b)$ and the normalizing constant is\n$$\n\\mathbb{P}(aXb)=\\int_{a}^{b} f_X(x)\\,dx=F_X(b)-F_X(a).\n$$\nUnder the transformation $Z=(X-\\mu)/\\sigma$, the truncation $(a,b)$ on $X$ corresponds to $(\\alpha,\\beta)$ on $Z$, where\n$$\n\\alpha=\\frac{a-\\mu}{\\sigma},\\qquad \\beta=\\frac{b-\\mu}{\\sigma}.\n$$\nThe conditional distribution of $Z$ given $\\alphaZ\\beta$ has CDF\n$$\nG(z)=\\mathbb{P}(Z\\le z\\mid \\alphaZ\\beta)=\\frac{\\Phi(z)-\\Phi(\\alpha)}{\\Phi(\\beta)-\\Phi(\\alpha)}\\quad \\text{for } z\\in(\\alpha,\\beta).\n$$\nBy the inverse transform method, if $U\\sim \\mathrm{Uniform}(0,1)$, then setting\n$$\nU=G(z)\\quad \\Longleftrightarrow\\quad z=G^{-1}(U)=\\Phi^{-1}\\big(\\Phi(\\alpha)+U\\big(\\Phi(\\beta)-\\Phi(\\alpha)\\big)\\big)\n$$\nproduces a draw $z$ from the truncated standard normal on $(\\alpha,\\beta)$. Transforming back gives a sample from the truncated $\\mathcal{N}(\\mu,\\sigma^2)$ on $(a,b)$:\n$$\nX=\\mu+\\sigma\\,Z=\\mu+\\sigma\\,\\Phi^{-1}\\big(\\Phi(\\alpha)+U\\big(\\Phi(\\beta)-\\Phi(\\alpha)\\big)\\big).\n$$\nThis algorithm is computationally efficient because it requires only evaluations of the standard normal CDF $\\Phi$ and its inverse $\\Phi^{-1}$, and it avoids the inefficiency of rejection when the acceptance region is small.\n\nPart two: acceptance probability for naive rejection sampling. The naive rejection sampler proposes $X^\\ast \\sim \\mathcal{N}(\\mu,\\sigma^2)$ and accepts if $aX^\\astb$. The acceptance probability $p_{\\text{acc}}$ is, by definition,\n$$\np_{\\text{acc}}=\\mathbb{P}(aX^\\astb)=\\int_{a}^{b} f_X(x)\\,dx=F_X(b)-F_X(a).\n$$\nUnder standardization $Z=(X^\\ast-\\mu)/\\sigma$, we have\n$$\np_{\\text{acc}}=\\mathbb{P}\\!\\left(\\alphaZ\\beta\\right)=\\Phi(\\beta)-\\Phi(\\alpha)=\\Phi\\!\\left(\\frac{b-\\mu}{\\sigma}\\right)-\\Phi\\!\\left(\\frac{a-\\mu}{\\sigma}\\right).\n$$\nThis closed-form expression in terms of the standard normal cumulative distribution function provides the exact acceptance rate for any finite $ab$ and any $\\sigma0$.\n\nAlgorithmic design for the program:\n- Implement a function that, given $(\\mu,\\sigma,a,b)$ and a positive integer $n$, generates $n$ independent samples from the truncated $\\mathcal{N}(\\mu,\\sigma^2)$ on $(a,b)$ using the inverse transform method derived above. This function computes $\\alpha$ and $\\beta$, transforms a vector of $n$ independent $\\mathrm{Uniform}(0,1)$ draws to the truncated standard normal via $z=\\Phi^{-1}(\\Phi(\\alpha)+u(\\Phi(\\beta)-\\Phi(\\alpha)))$, and then maps back to $x=\\mu+\\sigma z$.\n- Implement a function to compute the analytical acceptance probability $p_{\\text{acc}}=\\Phi\\!\\left(\\frac{b-\\mu}{\\sigma}\\right)-\\Phi\\!\\left(\\frac{a-\\mu}{\\sigma}\\right)$.\n- Define the test suite with the specified five parameter quadruples and compute $p_{\\text{acc}}$ for each, rounding each to $6$ decimal places.\n- Print the list $[p_{\\text{acc},1},p_{\\text{acc},2},p_{\\text{acc},3},p_{\\text{acc},4},p_{\\text{acc},5}]$ on a single line, as required.\n\nFor the given test suite, the analytical acceptance probabilities are:\n- Test case $1$: $(\\mu,\\sigma,a,b)=(0,1,-1,1)$ yields $p_{\\text{acc}}\\approx 0.682689$.\n- Test case $2$: $(\\mu,\\sigma,a,b)=(0,1,2,3)$ yields $p_{\\text{acc}}\\approx 0.021400$.\n- Test case $3$: $(\\mu,\\sigma,a,b)=(5,2,4,6)$ yields $p_{\\text{acc}}\\approx 0.382925$.\n- Test case $4$: $(\\mu,\\sigma,a,b)=(0,1,-10,10)$ yields $p_{\\text{acc}}\\approx 1.000000$.\n- Test case $5$: $(\\mu,\\sigma,a,b)=(0,1,0,0.0001)$ yields $p_{\\text{acc}}\\approx 0.000040$.\n\nThese values follow directly from $p_{\\text{acc}}=\\Phi\\!\\left(\\frac{b-\\mu}{\\sigma}\\right)-\\Phi\\!\\left(\\frac{a-\\mu}{\\sigma}\\right)$ and standard evaluations of the standard normal cumulative distribution function.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef truncated_normal_sample(mu, sigma, a, b, n, rng):\n    \"\"\"\n    Sample n values from N(mu, sigma^2) truncated to (a, b)\n    using inverse transform sampling on the standard normal.\n    \"\"\"\n    alpha = (a - mu) / sigma\n    beta = (b - mu) / sigma\n\n    # Compute CDF bounds\n    cdf_alpha = norm.cdf(alpha)\n    cdf_beta = norm.cdf(beta)\n\n    # Draw uniforms over the truncated CDF interval\n    u = rng.random(n)\n    # Map to the target quantiles in (cdf_alpha, cdf_beta)\n    target = cdf_alpha + u * (cdf_beta - cdf_alpha)\n    # Invert to get truncated standard normal samples\n    z = norm.ppf(target)\n    # Affine transform back to N(mu, sigma^2)\n    x = mu + sigma * z\n    return x\n\ndef acceptance_rate(mu, sigma, a, b):\n    \"\"\"\n    Analytical acceptance probability for naive rejection sampling\n    from N(mu, sigma^2) keeping samples in (a, b).\n    \"\"\"\n    alpha = (a - mu) / sigma\n    beta = (b - mu) / sigma\n    return float(norm.cdf(beta) - norm.cdf(alpha))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu, sigma, a, b).\n    test_cases = [\n        (0.0, 1.0, -1.0, 1.0),       # Case 1: central moderate interval\n        (0.0, 1.0, 2.0, 3.0),        # Case 2: right tail interval\n        (5.0, 2.0, 4.0, 6.0),        # Case 3: shifted-scaled moderate interval\n        (0.0, 1.0, -10.0, 10.0),     # Case 4: very wide interval ~ full support\n        (0.0, 1.0, 0.0, 0.0001),     # Case 5: very narrow near zero\n    ]\n\n    # Initialize RNG with fixed seed for determinism if sampling is used\n    rng = np.random.default_rng(12345)\n\n    results = []\n    # Also construct the sampler (demonstration; samples are not printed)\n    for mu, sigma, a, b in test_cases:\n        # Compute analytical acceptance rate\n        acc = acceptance_rate(mu, sigma, a, b)\n        # Demonstrate sampler correctness by drawing a small batch (not used in output)\n        _ = truncated_normal_sample(mu, sigma, a, b, n=10, rng=rng)\n        # Round to 6 decimals as required\n        results.append(f\"{acc:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "4944715"}]}