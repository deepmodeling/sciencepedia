{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, let's tackle a problem that highlights one of the most elegant and useful properties of expected value: the linearity of expectation. This principle states that the expected value of a sum of random variables is simply the sum of their individual expected values, a rule that holds true even if the variables are not independent. This exercise [@problem_id:1916150] uses the intuitive scenario of rolling non-standard dice to demonstrate how this property can greatly simplify calculations that would otherwise be quite tedious.", "problem": "A game designer is creating a new board game that uses two distinct, non-standard six-sided dice, referred to as Die A and Die B. Both dice are fair, meaning that for each die, any of its six faces is equally likely to land facing up after a roll. The set of integers on the six faces of Die A is $\\{0, 1, 1, 2, 4, 5\\}$. The set of integers on the six faces of Die B is $\\{2, 3, 4, 5, 6, 6\\}$. If a player rolls both dice simultaneously, what is the expected value of the sum of the numbers that appear on their top faces?", "solution": "Let $X$ be the random variable representing the numerical outcome of rolling Die A, and let $Y$ be the random variable representing the numerical outcome of rolling Die B. We are asked to find the expected value of the sum of these two random variables, which is denoted as $E[X+Y]$.\n\nA fundamental property of expected values is the linearity of expectation. This property states that for any two random variables $X$ and $Y$, the expected value of their sum is equal to the sum of their individual expected values:\n$$E[X+Y] = E[X] + E[Y]$$\nThis holds true regardless of whether the variables are independent or not. In this case, the rolls of the two dice are independent events.\n\nFirst, we calculate the expected value of the outcome of Die A, $E[X]$. The expected value of a discrete random variable is calculated by summing the product of each possible value and its probability. Since Die A is a fair six-sided die, the probability of any specific face landing up is $\\frac{1}{6}$. The values on the faces are $\\{0, 1, 1, 2, 4, 5\\}$.\nThe expected value $E[X]$ is the average of these values:\n$$E[X] = 0 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6}$$\n$$E[X] = \\frac{1}{6} (0 + 1 + 1 + 2 + 4 + 5)$$\n$$E[X] = \\frac{13}{6}$$\n\nNext, we calculate the expected value of the outcome of Die B, $E[Y]$. The values on the faces of Die B are $\\{2, 3, 4, 5, 6, 6\\}$. Since Die B is also a fair six-sided die, the probability of any face landing up is $\\frac{1}{6}$.\nThe expected value $E[Y]$ is the average of these values:\n$$E[Y] = 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6}$$\n$$E[Y] = \\frac{1}{6} (2 + 3 + 4 + 5 + 6 + 6)$$\n$$E[Y] = \\frac{26}{6}$$\n$$E[Y] = \\frac{13}{3}$$\n\nFinally, we use the linearity of expectation to find the expected sum $E[X+Y]$:\n$$E[X+Y] = E[X] + E[Y]$$\n$$E[X+Y] = \\frac{13}{6} + \\frac{26}{6}$$\n$$E[X+Y] = \\frac{13 + 26}{6}$$\n$$E[X+Y] = \\frac{39}{6}$$\nTo simplify the fraction, we divide the numerator and the denominator by their greatest common divisor, which is 3:\n$$E[X+Y] = \\frac{39 \\div 3}{6 \\div 3} = \\frac{13}{2}$$\n\nThus, the expected sum of the numbers shown on the top faces of the two dice is $\\frac{13}{2}$.", "answer": "$$\\boxed{\\frac{13}{2}}$$", "id": "1916150"}, {"introduction": "Moving from foundational properties to practical applications, we often use theoretical concepts to make sense of real-world data. This next problem [@problem_id:1913526] shows how the formulas for the expected value and variance of a binomial distribution can be used as a system of equations to work backward and determine the distribution's underlying parameters. This type of parameter estimation is a cornerstone of biostatistics, where we frequently model phenomena like the number of patients responding to a treatment or, as in this hypothetical case, the number of defective components in a manufacturing process.", "problem": "A technology company is developing a new type of multi-core processor. Each processor is designed with a specific number of identical, independent cores. Due to imperfections in the manufacturing process, each core has a fixed probability of being defective. Let the random variable $X$ represent the number of defective cores found in a single, randomly selected processor. Historical data from a large number of production test runs show that $X$ can be modeled by a binomial distribution. The company's quality control analysis has established that the expected number of defective cores per processor is 8, and the variance of the number of defective cores is 4.8.\n\nDetermine the total number of cores on each processor, $n$, and the probability that a single core is defective, $p$. Express your final answer for the ordered pair $(n,p)$ with $p$ as a decimal.", "solution": "Let $X$ denote the number of defective cores in a processor. The problem states that $X$ follows a binomial distribution with parameters $n$ (number of trials/cores) and $p$ (probability a core is defective). For a binomial random variable, the mean and variance are given by the standard formulas:\n$$\nE[X]=np, \\quad \\operatorname{Var}(X)=np(1-p).\n$$\nWe are given $E[X]=8$ and $\\operatorname{Var}(X)=4.8$. Therefore,\n$$\nnp=8,\n$$\n$$\nnp(1-p)=4.8.\n$$\nDivide the variance equation by the mean equation to eliminate $n$:\n$$\n\\frac{np(1-p)}{np}=\\frac{4.8}{8} \\quad \\Longrightarrow \\quad 1-p=0.6.\n$$\nThus,\n$$\np=0.4.\n$$\nSubstitute this value into $np=8$ to solve for $n$:\n$$\nn=\\frac{8}{p}=\\frac{8}{0.4}=20.\n$$\nVerification:\n$$\nnp(1-p)=20 \\cdot 0.4 \\cdot 0.6=4.8,\n$$\nwhich matches the given variance. Hence, the ordered pair is $(n,p)=(20,0.4)$.", "answer": "$$\\boxed{\\begin{pmatrix} 20  0.4 \\end{pmatrix}}$$", "id": "1913526"}, {"introduction": "Our final practice delves into a more sophisticated application of expectation and variance that is central to biostatistical modeling: partitioning sources of variability. Using the Law of Total Variance, we can decompose the overall variance of an outcome into the portion explained by a predictor and the portion that remains as random noise. This exercise [@problem_id:4911570] demonstrates this powerful concept within the common framework of logistic regression, providing critical insight into how we interpret the strength and meaning of a risk factor's effect on a health outcome.", "problem": "In a prospective cohort study, let $Y \\in \\{0,1\\}$ denote whether an individual develops a particular condition within one year ($Y=1$) or not ($Y=0$). Let $X \\in \\{0,1\\}$ be a binary biomarker indicator, where $X=1$ denotes a high expression of a certain protein and $X=0$ denotes low expression. Suppose the conditional risk of the condition is modeled via logistic regression, which is a special case of the Generalized Linear Model (GLM), so that the logit link satisfies\n$$\n\\ln\\!\\left(\\frac{P(Y=1 \\mid X)}{1 - P(Y=1 \\mid X)}\\right) = \\alpha + \\beta X.\n$$\nAssume $P(X=1)=0.35$ and $P(X=0)=0.65$, with $\\alpha=-1.2$ and $\\beta=2.0$. Define $p(X) = P(Y=1 \\mid X)$.\n\nTasks:\n1. Starting only from the definitions of expected value and variance, and the basic properties of conditional expectation, derive an expression for the unconditional variance $\\mathrm{Var}(Y)$ in terms of $p(X)$, an expectation operator over the distribution of $X$, and a variance operator over the distribution of $X$.\n2. Using the given model and distribution of $X$, compute the numerical value of $\\mathrm{Var}(Y)$.\n3. Briefly interpret the two additive components in your derived expression in the context of logistic regression, distinguishing within-covariate uncertainty from between-covariate heterogeneity.\n\nRound your final numerical value in Task 2 to four significant figures. Express the final answer as a pure number without any units.", "solution": "### Task 1: Derivation of the Unconditional Variance $\\mathrm{Var}(Y)$\nThe derivation begins with the Law of Total Variance, a fundamental theorem in probability theory. For any two random variables $Y$ and $X$, the variance of $Y$ can be decomposed as:\n$$\n\\mathrm{Var}(Y) = E[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(E[Y \\mid X])\n$$\nIn this context, the outer expectation $E[\\cdot]$ and variance $\\mathrm{Var}(\\cdot)$ operators are taken with respect to the distribution of the random variable $X$. To be explicit, we can write them as $E_X[\\cdot]$ and $\\mathrm{Var}_X(\\cdot)$.\n\nWe now identify the inner conditional terms based on the problem's definitions.\nThe outcome $Y$ conditional on $X$ is a binary variable, so it follows a Bernoulli distribution. For a fixed value of $X$, the probability of success ($Y=1$) is given by $P(Y=1 \\mid X)$. The problem defines this probability as $p(X)$.\n$$\np(X) = P(Y=1 \\mid X)\n$$\nFor a Bernoulli random variable with success probability $p$, the expected value is $p$ and the variance is $p(1-p)$. Therefore, the conditional expectation and conditional variance of $Y$ given $X$ are:\n1.  Conditional Expectation: $E[Y \\mid X] = p(X)$.\n2.  Conditional Variance: $\\mathrm{Var}(Y \\mid X) = p(X)(1 - p(X))$.\n\nSubstituting these expressions back into the Law of Total Variance formula gives the desired expression for the unconditional variance $\\mathrm{Var}(Y)$:\n$$\n\\mathrm{Var}(Y) = E_X[p(X)(1 - p(X))] + \\mathrm{Var}_X(p(X))\n$$\nThis expression decomposes the total variance of the outcome $Y$ into two components, described further in Task 3.\n\n### Task 2: Numerical Computation of $\\mathrm{Var}(Y)$\nTo compute the numerical value, we first need to determine the values of $p(X)$ for $X=0$ and $X=1$. The logistic regression model is given by $\\ln(\\frac{p(X)}{1-p(X)}) = \\alpha + \\beta X$. Solving for $p(X)$ yields the logistic function:\n$$\np(X) = \\frac{\\exp(\\alpha + \\beta X)}{1 + \\exp(\\alpha + \\beta X)}\n$$\nUsing the given parameters $\\alpha = -1.2$ and $\\beta = 2.0$:\n- For $X=0$:\n$$\np(0) = P(Y=1 \\mid X=0) = \\frac{\\exp(-1.2 + 2.0 \\cdot 0)}{1 + \\exp(-1.2)} = \\frac{\\exp(-1.2)}{1 + \\exp(-1.2)} \\approx 0.231475\n$$\n- For $X=1$:\n$$\np(1) = P(Y=1 \\mid X=1) = \\frac{\\exp(-1.2 + 2.0 \\cdot 1)}{1 + \\exp(-1.2 + 2.0)} = \\frac{\\exp(0.8)}{1 + \\exp(0.8)} \\approx 0.689974\n$$\nNow we compute the two terms in the variance decomposition. The random variable $p(X)$ takes value $p(0)$ with probability $P(X=0)=0.65$ and value $p(1)$ with probability $P(X=1)=0.35$.\n\n**First term: $E_X[p(X)(1 - p(X))]$**\nThis is the expected value of the conditional variance.\n$$\nE_X[p(X)(1-p(X))] = p(0)(1-p(0)) \\cdot P(X=0) + p(1)(1-p(1)) \\cdot P(X=1)\n$$\nWe calculate the components:\n- $p(0)(1-p(0)) \\approx 0.231475 \\cdot (1 - 0.231475) \\approx 0.177895$\n- $p(1)(1-p(1)) \\approx 0.689974 \\cdot (1 - 0.689974) \\approx 0.213909$\nSubstituting these values:\n$$\nE_X[p(X)(1-p(X))] \\approx (0.177895)(0.65) + (0.213909)(0.35) \\approx 0.115632 + 0.074868 = 0.190500\n$$\n\n**Second term: $\\mathrm{Var}_X(p(X))$**\nThis is the variance of the conditional expectation. We use the formula $\\mathrm{Var}(Z) = E[Z^2] - (E[Z])^2$.\nFirst, we compute the expectation of $p(X)$:\n$$\nE_X[p(X)] = p(0) \\cdot P(X=0) + p(1) \\cdot P(X=1)\n$$\n$$\nE_X[p(X)] \\approx (0.231475)(0.65) + (0.689974)(0.35) \\approx 0.150459 + 0.241491 = 0.391950\n$$\nNext, we compute the expectation of $(p(X))^2$:\n$$\nE_X[(p(X))^2] = (p(0))^2 \\cdot P(X=0) + (p(1))^2 \\cdot P(X=1)\n$$\n$$\nE_X[(p(X))^2] \\approx (0.231475)^2(0.65) + (0.689974)^2(0.35) \\approx (0.053581)(0.65) + (0.476065)(0.35) \\approx 0.034828 + 0.166623 = 0.201451\n$$\nNow, we calculate the variance:\n$$\n\\mathrm{Var}_X(p(X)) = E_X[(p(X))^2] - (E_X[p(X)])^2 \\approx 0.201451 - (0.391950)^2 \\approx 0.201451 - 0.153625 = 0.047826\n$$\n\n**Total Variance $\\mathrm{Var}(Y)$**\nFinally, we sum the two components:\n$$\n\\mathrm{Var}(Y) = E_X[p(X)(1 - p(X))] + \\mathrm{Var}_X(p(X)) \\approx 0.190500 + 0.047826 = 0.238326\n$$\nRounding to four significant figures, the numerical value is $0.2383$.\n\n### Task 3: Interpretation of the Variance Components\nThe decomposition $\\mathrm{Var}(Y) = E_X[p(X)(1 - p(X))] + \\mathrm{Var}_X(p(X))$ provides insight into the sources of variation in the outcome $Y$.\n\n1.  **First Component: $E_X[p(X)(1 - p(X))]$**\n    This term, $E[\\mathrm{Var}(Y \\mid X)]$, represents the average **within-covariate uncertainty**. For any given subgroup of the population with a fixed biomarker status $X=x$, the outcome $Y$ is still a random variable with variance $p(x)(1-p(x))$. This variability is inherent to the Bernoulli nature of the outcome and cannot be explained by the biomarker $X$. It is the irreducible randomness or noise that remains even after we have stratified the population by the predictor. The operator $E_X[\\cdot]$ computes the weighted average of this uncertainty across the different biomarker groups ($X=0$ and $X=1$). This term is often referred to as the \"unexplained variance\".\n\n2.  **Second Component: $\\mathrm{Var}_X(p(X))$**\n    This term, $\\mathrm{Var}(E[Y \\mid X])$, represents the **between-covariate heterogeneity**. The conditional expectation $E[Y \\mid X] = p(X)$ is the risk of the condition, which is a function of the biomarker $X$. This term measures how much this predicted risk varies across the population due to the variability in the biomarker $X$. If the biomarker had no effect on the outcome (i.e., $\\beta=0$), then $p(X)$ would be constant, and this variance component would be zero. Therefore, this component quantifies the portion of the total variance in $Y$ that is systematically \"explained by\" the logistic regression model through the predictor $X$. It captures the variation in $Y$ that arises because individuals have different risk levels associated with their different biomarker statuses.", "answer": "$$\\boxed{0.2383}$$", "id": "4911570"}]}