{"hands_on_practices": [{"introduction": "The binomial model's assumption of independent trials is fundamental, but it is technically violated whenever we sample *without* replacement from a finite population. This hands-on problem explores this common scenario, where drawing one individual affects the probability of the next. You will derive the finite population correction factor, which quantifies how much the standard binomial variance overestimates the true variance, a crucial adjustment for studies where the sample is a substantial fraction of the population [@problem_id:4895499].", "problem": "A finite population of size $N$ contains individuals with a binary disease status, where the true disease prevalence is $p$. An investigator draws a simple random sample without replacement of size $n$ to estimate the prevalence using the sample proportion $\\hat{p}$. The investigator then performs inference as if the data arose from independent and identically distributed Bernoulli trials with success probability $p$ (the binomial model), ignoring the finite population. Starting from the foundational definitions of the binomial and hypergeometric models and their well-tested distributional properties, derive an expression for the ratio of the variance of $\\hat{p}$ under sampling without replacement to the variance under the binomial model. Then evaluate this ratio for $N=1000$, $n=200$, and $p=0.1$, and briefly explain what this ratio implies about the direction of bias in the naive binomial variance when the sampling fraction is not negligible. Round your final numerical answer to four significant figures. Express $p$ as a decimal.", "solution": "The sample proportion, $\\hat{p}$, is defined as the number of diseased individuals in the sample, let's call this $Y$, divided by the sample size, $n$. So, $\\hat{p} = \\frac{Y}{n}$. The variance of $\\hat{p}$ is $\\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{Y}{n}\\right) = \\frac{1}{n^2}\\text{Var}(Y)$. We need to find $\\text{Var}(Y)$ under the two different sampling models.\n\n**1. Variance under the (incorrect) Binomial Model**\nThe binomial model assumes that the $n$ draws are independent and identically distributed (i.i.d.) Bernoulli trials. This scenario corresponds to sampling with replacement from the finite population or sampling from an infinite population. Let $X_i$ be an indicator variable for the $i$-th individual in the sample having the disease, for $i=1, 2, \\dots, n$.\nUnder this model, each $X_i$ is a Bernoulli random variable with success probability $p$.\n$$ E[X_i] = p $$\n$$ \\text{Var}(X_i) = E[X_i^2] - (E[X_i])^2 = p - p^2 = p(1-p) $$\nThe total number of diseased individuals in the sample is $Y_{\\text{binom}} = \\sum_{i=1}^{n} X_i$. Since the $X_i$ are assumed to be independent, the variance of the sum is the sum of the variances:\n$$ \\text{Var}(Y_{\\text{binom}}) = \\text{Var}\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} \\text{Var}(X_i) = n p (1-p) $$\nThe variance of the sample proportion under the binomial model is therefore:\n$$ \\text{Var}_{\\text{binom}}(\\hat{p}) = \\frac{1}{n^2} \\text{Var}(Y_{\\text{binom}}) = \\frac{n p (1-p)}{n^2} = \\frac{p(1-p)}{n} $$\n\n**2. Variance under Sampling Without Replacement (Correct Model)**\nIn reality, the sample is drawn without replacement from a finite population of size $N$. This means the trials are not independent. The correct model for the count $Y$ is the hypergeometric distribution. We can derive the variance from foundational principles using indicator variables, which does not require knowing the full variance formula for the hypergeometric distribution beforehand.\nLet $X_i$ be the indicator variable for the $i$-th draw resulting in a diseased individual.\nThe probability of any single draw being a success is $p$, so $E[X_i] = p$.\nThe variance of any single indicator is $\\text{Var}(X_i) = p(1-p)$. This is the same as in the binomial case.\nThe total count is $Y_{\\text{hyper}} = \\sum_{i=1}^{n} X_i$. The variance of this sum is:\n$$ \\text{Var}(Y_{\\text{hyper}}) = \\text{Var}\\left(\\sum_{i=1}^{n} X_i\\right) = \\sum_{i=1}^{n} \\text{Var}(X_i) + \\sum_{i \\neq j} \\text{Cov}(X_i, X_j) $$\nThere are $n$ variance terms and $n(n-1)$ covariance terms. We need to find the covariance between two different draws, $\\text{Cov}(X_i, X_j)$ for $i \\neq j$.\n$$ \\text{Cov}(X_i, X_j) = E[X_i X_j] - E[X_i]E[X_j] $$\nWe have $E[X_i] = p$ and $E[X_j] = p$. The term $E[X_i X_j]$ is the probability that both draw $i$ and draw $j$ are successes, $P(X_i=1 \\text{ and } X_j=1)$.\n$$ P(X_i=1 \\text{ and } X_j=1) = P(X_i=1) \\times P(X_j=1 | X_i=1) $$\nThe number of diseased individuals in the population is $Np$. After one diseased individual is drawn (for $X_i=1$), there are $Np-1$ diseased individuals remaining in a population of size $N-1$.\nThus, $P(X_j=1 | X_i=1) = \\frac{Np-1}{N-1}$.\n$$ E[X_i X_j] = p \\left( \\frac{Np-1}{N-1} \\right) $$\nNow, we can compute the covariance:\n$$ \\text{Cov}(X_i, X_j) = p \\left( \\frac{Np-1}{N-1} \\right) - p^2 = \\frac{p(Np-1) - p^2(N-1)}{N-1} = \\frac{Np^2 - p - Np^2 + p^2}{N-1} = \\frac{p^2-p}{N-1} = -\\frac{p(1-p)}{N-1} $$\nThe negative covariance reflects the fact that drawing a diseased individual reduces the proportion of diseased individuals remaining, making it less likely to draw another one.\nNow, we substitute this back into the variance of the sum formula:\n$$ \\text{Var}(Y_{\\text{hyper}}) = n \\cdot p(1-p) + n(n-1) \\left( -\\frac{p(1-p)}{N-1} \\right) $$\nFactoring out $n p(1-p)$:\n$$ \\text{Var}(Y_{\\text{hyper}}) = n p(1-p) \\left( 1 - \\frac{n-1}{N-1} \\right) = n p(1-p) \\left( \\frac{N-1 - (n-1)}{N-1} \\right) = n p(1-p) \\left( \\frac{N-n}{N-1} \\right) $$\nThe variance of the sample proportion under sampling without replacement is:\n$$ \\text{Var}_{\\text{hyper}}(\\hat{p}) = \\frac{1}{n^2} \\text{Var}(Y_{\\text{hyper}}) = \\frac{1}{n^2} \\left( n p(1-p) \\frac{N-n}{N-1} \\right) = \\frac{p(1-p)}{n} \\left( \\frac{N-n}{N-1} \\right) $$\n\n**3. The Ratio of Variances**\nWe are asked to find the ratio of the variance of $\\hat{p}$ under sampling without replacement (the true variance) to the variance under the binomial model (the naive variance).\n$$ \\text{Ratio} = \\frac{\\text{Var}_{\\text{hyper}}(\\hat{p})}{\\text{Var}_{\\text{binom}}(\\hat{p})} = \\frac{\\frac{p(1-p)}{n} \\left( \\frac{N-n}{N-1} \\right)}{\\frac{p(1-p)}{n}} $$\nSimplifying the expression, we get:\n$$ \\text{Ratio} = \\frac{N-n}{N-1} $$\nThis term is known as the Finite Population Correction (FPC) factor. Notice that this ratio is independent of the prevalence $p$.\n\n**4. Evaluation of the Ratio**\nWe are given $N=1000$ and $n=200$. The value $p=0.1$ is not needed for this calculation.\n$$ \\text{Ratio} = \\frac{1000 - 200}{1000 - 1} = \\frac{800}{999} $$\nNow, we compute the numerical value and round to four significant figures:\n$$ \\frac{800}{999} \\approx 0.800800800... $$\nRounded to four significant figures, the ratio is $0.8008$.\n\n**5. Interpretation of the Result**\nThe ratio, $\\frac{N-n}{N-1}$, is less than $1$ for any sample size $n1$. In this case, the ratio is approximately $0.8008$. This means that the true variance of the sample proportion (under sampling without replacement) is about $80.08\\%$ of the variance calculated using the naive binomial model.\nThe implication is that the binomial model, by assuming independence, overestimates the true sampling variability. The quantity $\\frac{p(1-p)}{n}$ is biased upwards as an estimator of the true variance of $\\hat{p}$. The direction of the bias is positive.\nWhen the sampling fraction $f = n/N$ is non-negligible (here, $f = 200/1000 = 0.2$), this overestimation can be substantial. Using the binomial variance for inference (e.g., constructing confidence intervals or performing hypothesis tests) will result in confidence intervals that are wider than necessary and tests that are less powerful (i.e., more conservative, with a higher Type II error rate) than they would be if the correct, smaller variance were used.", "answer": "$$\\boxed{0.8008}$$", "id": "4895499"}, {"introduction": "In many biostatistical studies, data is naturally grouped or \"clustered\"—for example, patients within hospitals or students within schools. Outcomes from individuals within the same cluster are often correlated, violating the assumption of independence. This exercise demonstrates how to handle such data by deriving the variance inflation factor, or design effect, which measures the impact of this correlation, and calculating an \"effective sample size\" to correctly adjust statistical inference [@problem_id:4895450].", "problem": "A cluster randomized vaccine efficacy study enrolls $J$ clusters, each with equal size $m$, and records whether each participant becomes infected ($1$) or not ($0$) over a follow-up period. Let $X_{ij}$ denote the Bernoulli outcome for participant $i$ in cluster $j$, and let $p$ denote the common marginal infection probability. The binomial model assumes independent and identically distributed Bernoulli trials with common success probability $p$, which implies $X_{ij}$ are independent with $\\operatorname{Var}(X_{ij}) = p(1-p)$ and $\\operatorname{Cov}(X_{ij}, X_{i'j'}) = 0$ for all $(i,j) \\neq (i',j')$. In cluster trials, however, outcomes within the same cluster are correlated. Suppose that within each cluster, outcomes have a common pairwise correlation equal to the Intraclass Correlation Coefficient (ICC), denoted by $\\rho$, and that outcomes from different clusters are independent.\n\nUsing only definitions of variance and covariance and the structure described above, derive the variance of the overall sample proportion $\\hat{p}$ under clustering and express it in the form\n$$\n\\operatorname{Var}(\\hat{p}) = \\frac{p(1-p)}{n} \\times D,\n$$\nwhere $n = J m$ is the total sample size and $D$ is a multiplicative variance inflation factor due to clustering. Then define an effective sample size $n_{\\text{eff}}$ so that\n$$\n\\operatorname{Var}(\\hat{p}) = \\frac{p(1-p)}{n_{\\text{eff}}},\n$$\nand use this to adjust the two-sided $95\\%$ normal-approximation (Wald) confidence interval for $p$ by replacing $n$ with $n_{\\text{eff}}$.\n\nNow consider the following realized data: there are $J = 30$ clusters, the common cluster size is $m = 10$, the Intraclass Correlation Coefficient is $\\rho = 0.05$, and the total number of infections observed is $x = 120$. Compute the adjusted half-width $E$ of the two-sided $95\\%$ Wald confidence interval for $p$ based on $n_{\\text{eff}}$. Express your final answer as a decimal and round your answer to four significant figures.", "solution": "The overall sample proportion, $\\hat{p}$, is defined as the total number of events (infections) divided by the total sample size, $n$.\n$$ \\hat{p} = \\frac{1}{n} \\sum_{j=1}^{J} \\sum_{i=1}^{m} X_{ij} $$\nwhere $n = Jm$ is the total number of participants, $J$ is the number of clusters, and $m$ is the size of each cluster. The variable $X_{ij}$ is a Bernoulli outcome for participant $i$ in cluster $j$.\n\nThe variance of the sample proportion $\\hat{p}$ is given by:\n$$ \\operatorname{Var}(\\hat{p}) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{j=1}^{J} \\sum_{i=1}^{m} X_{ij}\\right) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{j=1}^{J} \\sum_{i=1}^{m} X_{ij}\\right) $$\nThe variance of a sum of random variables is the sum of all their pairwise covariances:\n$$ \\operatorname{Var}\\left(\\sum_{j=1}^{J} \\sum_{i=1}^{m} X_{ij}\\right) = \\sum_{j=1}^{J} \\sum_{i=1}^{m} \\sum_{j'=1}^{J} \\sum_{i'=1}^{m} \\operatorname{Cov}(X_{ij}, X_{i'j'}) $$\nWe can decompose this sum by considering three distinct cases for the pairs of indices $(i,j)$ and $(i',j')$ based on the problem's correlation structure.\n1.  The indices are identical: $j=j'$ and $i=i'$. In this case, the covariance is the variance of the variable itself: $\\operatorname{Cov}(X_{ij}, X_{ij}) = \\operatorname{Var}(X_{ij}) = p(1-p)$, where $p$ is the marginal probability. There are $n=Jm$ such terms in the sum.\n2.  The individuals are different but within the same cluster: $j=j'$ and $i \\neq i'$. The problem states that the outcomes have a common pairwise correlation equal to the Intraclass Correlation Coefficient, $\\rho$. The covariance is $\\operatorname{Cov}(X_{ij}, X_{i'j}) = \\operatorname{Corr}(X_{ij}, X_{i'j})\\sqrt{\\operatorname{Var}(X_{ij})\\operatorname{Var}(X_{i'j})} = \\rho\\sqrt{p(1-p)p(1-p)} = \\rho p(1-p)$. Within each of the $J$ clusters, there are $m$ individuals, so there are $m(m-1)$ ordered pairs of distinct individuals. The total number of such pairs across all clusters is $Jm(m-1)$.\n3.  The individuals are in different clusters: $j \\neq j'$. The problem states that outcomes from different clusters are independent, so their covariance is zero: $\\operatorname{Cov}(X_{ij}, X_{i'j'}) = 0$.\n\nSumming the covariance terms from all cases gives the variance of the total sum:\n$$ \\operatorname{Var}\\left(\\sum_{j=1}^{J} \\sum_{i=1}^{m} X_{ij}\\right) = \\underbrace{Jm \\cdot p(1-p)}_{\\text{Case 1: Variances}} + \\underbrace{Jm(m-1) \\cdot \\rho p(1-p)}_{\\text{Case 2: Within-cluster covariances}} + \\underbrace{0}_{\\text{Case 3: Between-cluster covariances}} $$\nFactoring out the term $Jmp(1-p)$ yields:\n$$ \\operatorname{Var}\\left(\\sum_{j=1}^{J} \\sum_{i=1}^{m} X_{ij}\\right) = Jmp(1-p) [1 + (m-1)\\rho] $$\nSubstituting this back into the expression for $\\operatorname{Var}(\\hat{p})$:\n$$ \\operatorname{Var}(\\hat{p}) = \\frac{1}{(Jm)^2} \\left( Jmp(1-p) [1 + (m-1)\\rho] \\right) = \\frac{p(1-p)}{Jm} [1 + (m-1)\\rho] $$\nUsing the total sample size $n=Jm$, we obtain the variance under clustering:\n$$ \\operatorname{Var}(\\hat{p}) = \\frac{p(1-p)}{n} [1 + (m-1)\\rho] $$\nBy comparing this result to the specified form $\\operatorname{Var}(\\hat{p}) = \\frac{p(1-p)}{n} \\times D$, we can identify the multiplicative variance inflation factor, $D$, as:\n$$ D = 1 + (m-1)\\rho $$\nThis factor is also known as the design effect (DEFF).\n\nNext, we define the effective sample size, $n_{\\text{eff}}$, such that the variance has the same form as that from a simple random sample of size $n_{\\text{eff}}$:\n$$ \\operatorname{Var}(\\hat{p}) = \\frac{p(1-p)}{n_{\\text{eff}}} $$\nEquating this with our derived expression for the variance:\n$$ \\frac{p(1-p)}{n_{\\text{eff}}} = \\frac{p(1-p)}{n} [1 + (m-1)\\rho] $$\nAssuming $p \\in (0,1)$, we can cancel the $p(1-p)$ term from both sides and solve for $n_{\\text{eff}}$:\n$$ n_{\\text{eff}} = \\frac{n}{1 + (m-1)\\rho} = \\frac{n}{D} $$\nThe adjusted two-sided $95\\%$ Wald confidence interval for $p$ is given by $\\hat{p} \\pm E$, where $E$ is the half-width. We use the estimated variance of $\\hat{p}$, which is obtained by substituting $\\hat{p}$ for $p$ and using $n_{\\text{eff}}$. The half-width $E$ is:\n$$ E = z_{0.025} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_{\\text{eff}}}} $$\nwhere $z_{0.025}$ is the upper $0.025$ quantile of the standard normal distribution, for which we use the value $1.96$.\n\nWe are given the following data: $J=30$, $m=10$, $\\rho=0.05$, and total number of infections $x = 120$.\nFirst, we compute the total sample size $n$ and the sample proportion $\\hat{p}$:\n$$ n = Jm = 30 \\times 10 = 300 $$\n$$ \\hat{p} = \\frac{x}{n} = \\frac{120}{300} = 0.4 $$\nNext, we calculate the variance inflation factor $D$:\n$$ D = 1 + (m-1)\\rho = 1 + (10-1)(0.05) = 1 + 9 \\times 0.05 = 1 + 0.45 = 1.45 $$\nUsing $D$, we find the effective sample size $n_{\\text{eff}}$:\n$$ n_{\\text{eff}} = \\frac{n}{D} = \\frac{300}{1.45} $$\nNow we can compute the adjusted half-width $E$:\n$$ E = 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_{\\text{eff}}}} = 1.96 \\sqrt{\\frac{0.4(1-0.4)}{300/1.45}} $$\n$$ E = 1.96 \\sqrt{\\frac{0.24 \\times 1.45}{300}} = 1.96 \\sqrt{\\frac{0.348}{300}} = 1.96 \\sqrt{0.00116} $$\nPerforming the final calculation:\n$$ E \\approx 1.96 \\times 0.03405877... \\approx 0.06675519... $$\nRounding the result to four significant figures gives $0.06676$.", "answer": "$$\\boxed{0.06676}$$", "id": "4895450"}, {"introduction": "Beyond independence, the binomial model assumes a single, constant probability of success, $p$, for all trials. However, in many biological systems, there is natural heterogeneity where each individual or subgroup has a slightly different underlying probability. This practice introduces the beta-binomial model, a powerful hierarchical approach that explicitly accounts for this variability, and asks you to derive its key properties to understand how heterogeneity leads to \"overdispersion\"—more variance than predicted by the simpler binomial model [@problem_id:4895480].", "problem": "A biostatistician is evaluating repeated binary outcomes collected within each individual in a cohort study of microbial colonization. The standard binomial model assumes that, for a given individual, $n \\in \\mathbb{N}$ trials are conditionally independent and identically distributed Bernoulli outcomes with a fixed success probability $p \\in (0,1)$, so that the total number of successes $X$ in $n$ trials would follow a binomial distribution. In practice, biological heterogeneity can violate the fixed-$p$ assumption: within an individual, the underlying success probability $p$ may vary between days due to unobserved factors, inducing positive correlation among trials. To capture this, consider the hierarchical model with $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$ for parameters $\\alpha0$ and $\\beta0$, and $X \\mid p \\sim \\mathrm{Binomial}(n,p)$.\n\nStarting from fundamental definitions and laws (the binomial model assumptions, conditional independence given $p$, the law of total expectation, and the law of total variance), derive the closed-form expressions for the marginal mean $\\mathbb{E}[X]$ and marginal variance $\\mathrm{Var}(X)$ of the resulting beta-binomial distribution in terms of $n$, $\\alpha$, and $\\beta$. Then, define the intraclass correlation coefficient (ICC) $\\rho$ for this model as the correlation between two distinct trial indicators within the same individual, and express $\\rho$ solely in terms of $\\alpha$ and $\\beta$.\n\nProvide your final expressions as a single analytic row vector containing $\\mathbb{E}[X]$, $\\mathrm{Var}(X)$, and $\\rho$. No numerical approximation or rounding is required.", "solution": "We are tasked with deriving the marginal mean, marginal variance, and intraclass correlation for the beta-binomial model. The model is specified hierarchically:\n$$X \\mid p \\sim \\mathrm{Binomial}(n, p)$$\n$$p \\sim \\mathrm{Beta}(\\alpha, \\beta)$$\nwhere $n \\in \\mathbb{N}$ is the number of trials, and $\\alpha  0$, $\\beta  0$ are the parameters of the Beta prior on the success probability $p$.\n\nFirst, we recall the moments of the constituent distributions.\nFor the Binomial distribution, the conditional mean and variance of $X$ given $p$ are:\n$$\\mathbb{E}[X \\mid p] = np$$\n$$\\mathrm{Var}(X \\mid p) = np(1-p)$$\nFor the Beta distribution, the probability density function for $p \\in (0,1)$ is $f(p; \\alpha, \\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha, \\beta)}$, where $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function. The first two raw moments of $p$ are:\n$$\\mathbb{E}[p] = \\frac{\\alpha}{\\alpha+\\beta}$$\n$$\\mathbb{E}[p^2] = \\frac{\\alpha(\\alpha+1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)}$$\nFrom these, the variance of $p$ is:\n$$\\mathrm{Var}(p) = \\mathbb{E}[p^2] - (\\mathbb{E}[p])^2 = \\frac{\\alpha(\\alpha+1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)} - \\left(\\frac{\\alpha}{\\alpha+\\beta}\\right)^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n\nWith these properties established, we can derive the marginal moments of $X$.\n\n**Marginal Mean $\\mathbb{E}[X]$**\nWe use the Law of Total Expectation: $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid p]]$.\nSubstituting the conditional mean of $X$:\n$$\\mathbb{E}[X] = \\mathbb{E}[np] = n\\mathbb{E}[p]$$\nUsing the mean of the Beta distribution:\n$$\\mathbb{E}[X] = n \\frac{\\alpha}{\\alpha+\\beta}$$\n\n**Marginal Variance $\\mathrm{Var}(X)$**\nWe use the Law of Total Variance: $\\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X \\mid p)] + \\mathrm{Var}(\\mathbb{E}[X \\mid p])$.\nWe compute each term separately.\n\nFirst term: $\\mathbb{E}[\\mathrm{Var}(X \\mid p)]$\n$$\\mathrm{Var}(X \\mid p) = np(1-p) = np - np^2$$\nTaking the expectation with respect to $p$:\n$$\\mathbb{E}[\\mathrm{Var}(X \\mid p)] = \\mathbb{E}[np - np^2] = n\\mathbb{E}[p] - n\\mathbb{E}[p^2]$$\nSubstituting the first two moments of the Beta distribution:\n$$\\mathbb{E}[\\mathrm{Var}(X \\mid p)] = n\\frac{\\alpha}{\\alpha+\\beta} - n\\frac{\\alpha(\\alpha+1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)}$$\n$$= \\frac{n\\alpha(\\alpha+\\beta+1) - n\\alpha(\\alpha+1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)} = \\frac{n\\alpha(\\alpha+\\beta+1 - \\alpha - 1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)} = \\frac{n\\alpha\\beta}{(\\alpha+\\beta)(\\alpha+\\beta+1)}$$\n\nSecond term: $\\mathrm{Var}(\\mathbb{E}[X \\mid p])$\n$$\\mathbb{E}[X \\mid p] = np$$\nThe variance is taken with respect to $p$:\n$$\\mathrm{Var}(\\mathbb{E}[X \\mid p]) = \\mathrm{Var}(np) = n^2\\mathrm{Var}(p)$$\nSubstituting the variance of the Beta distribution:\n$$\\mathrm{Var}(\\mathbb{E}[X \\mid p]) = n^2 \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n\nNow, we sum the two terms to find $\\mathrm{Var}(X)$:\n$$\\mathrm{Var}(X) = \\frac{n\\alpha\\beta}{(\\alpha+\\beta)(\\alpha+\\beta+1)} + \\frac{n^2\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\nTo simplify, we find a common denominator and factor out common terms:\n$$\\mathrm{Var}(X) = \\frac{n\\alpha\\beta(\\alpha+\\beta) + n^2\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} = \\frac{n\\alpha\\beta((\\alpha+\\beta) + n)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n$$\\mathrm{Var}(X) = \\frac{n\\alpha\\beta(n+\\alpha+\\beta)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n\n**Intraclass Correlation Coefficient (ICC) $\\rho$**\nThe ICC, $\\rho$, is defined as the correlation between two distinct trial indicators. Let $Y_i$ be the Bernoulli indicator for trial $i$, for $i=1, \\dots, n$, such that $X = \\sum_{i=1}^n Y_i$. Given $p$, the trials are independent, so $Y_i \\mid p \\sim \\mathrm{Bernoulli}(p)$ for all $i$.\nThe ICC is $\\rho = \\mathrm{Corr}(Y_i, Y_j)$ for $i \\neq j$. By definition:\n$$\\rho = \\frac{\\mathrm{Cov}(Y_i, Y_j)}{\\sqrt{\\mathrm{Var}(Y_i)\\mathrm{Var}(Y_j)}}$$\nSince the trials are identically distributed (exchangeable), $\\mathrm{Var}(Y_i) = \\mathrm{Var}(Y_j)$, so the denominator is simply $\\mathrm{Var}(Y_i)$.\n\nFirst, we find the marginal variance $\\mathrm{Var}(Y_i)$. The marginal mean of $Y_i$ is:\n$$\\mathbb{E}[Y_i] = \\mathbb{E}[\\mathbb{E}[Y_i \\mid p]] = \\mathbb{E}[p] = \\frac{\\alpha}{\\alpha+\\beta}$$\nSince $Y_i$ is a Bernoulli random variable (marginally, it takes values $0$ or $1$), its variance is:\n$$\\mathrm{Var}(Y_i) = \\mathbb{E}[Y_i](1-\\mathbb{E}[Y_i]) = \\frac{\\alpha}{\\alpha+\\beta} \\left(1-\\frac{\\alpha}{\\alpha+\\beta}\\right) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}$$\n\nNext, we find the covariance $\\mathrm{Cov}(Y_i, Y_j)$ for $i \\neq j$:\n$$\\mathrm{Cov}(Y_i, Y_j) = \\mathbb{E}[Y_i Y_j] - \\mathbb{E}[Y_i]\\mathbb{E}[Y_j]$$\nWe already know $\\mathbb{E}[Y_i] = \\mathbb{E}[Y_j] = \\frac{\\alpha}{\\alpha+\\beta}$. We need $\\mathbb{E}[Y_i Y_j]$. Using the Law of Total Expectation:\n$$\\mathbb{E}[Y_i Y_j] = \\mathbb{E}[\\mathbb{E}[Y_i Y_j \\mid p]]$$\nGiven $p$, the trials $Y_i$ and $Y_j$ are independent. Therefore:\n$$\\mathbb{E}[Y_i Y_j \\mid p] = \\mathbb{E}[Y_i \\mid p]\\mathbb{E}[Y_j \\mid p] = p \\cdot p = p^2$$\nThus,\n$$\\mathbb{E}[Y_i Y_j] = \\mathbb{E}[p^2] = \\frac{\\alpha(\\alpha+1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)}$$\nNow, we can compute the covariance:\n$$\\mathrm{Cov}(Y_i, Y_j) = \\mathbb{E}[p^2] - (\\mathbb{E}[p])^2 = \\mathrm{Var}(p) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n\nFinally, we compute $\\rho$ by dividing the covariance by the variance:\n$$\\rho = \\frac{\\mathrm{Cov}(Y_i, Y_j)}{\\mathrm{Var}(Y_i)} = \\frac{\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}}{\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}} = \\frac{1}{\\alpha+\\beta+1}$$\nThis expression for $\\rho$ depends only on $\\alpha$ and $\\beta$, as required. It represents the proportion of total variance in a single observation that is attributable to the variance in the underlying success probability $p$.\n\nThe final results are $\\mathbb{E}[X] = \\frac{n\\alpha}{\\alpha+\\beta}$, $\\mathrm{Var}(X) = \\frac{n\\alpha\\beta(n+\\alpha+\\beta)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$, and $\\rho = \\frac{1}{\\alpha+\\beta+1}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n\\alpha}{\\alpha+\\beta}  \\frac{n\\alpha\\beta(n+\\alpha+\\beta)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}  \\frac{1}{\\alpha+\\beta+1}\n\\end{pmatrix}\n}\n$$", "id": "4895480"}]}