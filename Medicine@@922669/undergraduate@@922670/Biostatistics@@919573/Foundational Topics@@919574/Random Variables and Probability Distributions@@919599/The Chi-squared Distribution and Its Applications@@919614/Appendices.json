{"hands_on_practices": [{"introduction": "Understanding how transformations affect a random variable's properties is a cornerstone of statistical reasoning. This first exercise provides practice with a fundamental manipulation, asking you to calculate the variance of a chi-squared variable that has been scaled by its degrees of freedom. Working through this problem will solidify your understanding of basic variance properties and the chi-squared distribution's key parameters [@problem_id:2330].", "problem": "Let $X$ be a random variable following a chi-squared distribution with $k$ degrees of freedom, denoted as $X \\sim \\chi^2(k)$. The chi-squared distribution is a continuous probability distribution with applications in statistical inference.\n\nFor a random variable $X \\sim \\chi^2(k)$, it is given that its expected value (mean) is $\\mathbb{E}[X] = k$ and its variance is $\\text{Var}(X) = 2k$.\n\nConsider a new random variable $Y$ that is created by normalizing $X$ by its degrees of freedom:\n$$\nY = \\frac{X}{k}\n$$\nYour task is to derive an expression for the variance of the random variable $Y$, denoted as $\\text{Var}(Y)$, using the provided properties of $X$. Express your final answer in terms of the parameter $k$.", "solution": "We have $Y = \\frac{X}{k}$.  Using the property of variance under scalar multiplication,\n$$\n\\mathrm{Var}(aX) = a^2\\,\\mathrm{Var}(X),\n$$\nwith $a = \\frac{1}{k}$, it follows that\n$$\n\\mathrm{Var}\\bigl(Y\\bigr)\n= \\mathrm{Var}\\Bigl(\\frac{X}{k}\\Bigr)\n= \\frac{1}{k^2}\\,\\mathrm{Var}(X).\n$$\nSince $\\mathrm{Var}(X)=2k$, we obtain\n$$\n\\mathrm{Var}(Y)\n= \\frac{1}{k^2}\\,(2k)\n= \\frac{2}{k}.\n$$", "answer": "$$\\boxed{\\frac{2}{k}}$$", "id": "2330"}, {"introduction": "Beyond its mean and variance, the overall shape of a probability distribution holds crucial information. This practice takes a deeper dive into the analytical form of the chi-squared distribution's probability density function (PDF). By using calculus to find the distribution's mode—its peak—you will gain a more profound insight into its characteristic right-skew and the relationship between its central tendency measures [@problem_id:4958326].", "problem": "A biostatistician is analyzing a goodness-of-fit test for contingency table data. Under the null hypothesis, the Pearson test statistic $X$ is modeled by a chi-squared distribution with $k$ degrees of freedom, denoted $\\chi^{2}_{k}$. Starting from the fundamental probability density function (PDF) of the chi-squared distribution and the standard mean of the gamma distribution, determine the mode of the $\\chi^{2}_{k}$ distribution as a function of $k$ by directly optimizing its PDF over its support, properly accounting for the boundary at $x=0$. Then, for $k \\ge 2$, explain how the location of this mode compares to the mean, and interpret this comparison in terms of the distribution’s skewness. Express your final answer as a single closed-form analytic expression for the mode; no rounding is required.", "solution": "The problem asks for the derivation of the mode of the chi-squared distribution with $k$ degrees of freedom, denoted $\\chi^2_k$, and a subsequent comparison of this mode with the distribution's mean for $k \\ge 2$ to interpret its skewness.\n\nFirst, we establish the mean of the $\\chi^2_k$ distribution. The $\\chi^2_k$ distribution is a special case of the gamma distribution. A random variable $X$ following a gamma distribution with shape parameter $\\alpha$ and rate parameter $\\beta$, denoted $X \\sim \\text{Gamma}(\\alpha, \\beta)$, has a mean given by $E[X] = \\frac{\\alpha}{\\beta}$. The $\\chi^2_k$ distribution corresponds to a gamma distribution with parameters $\\alpha = \\frac{k}{2}$ and $\\beta = \\frac{1}{2}$. Therefore, the mean of the $\\chi^2_k$ distribution is $\\mu = E[X] = \\frac{k/2}{1/2} = k$.\n\nNext, we determine the mode by maximizing the probability density function (PDF) of the $\\chi^2_k$ distribution. The PDF for a random variable $X \\sim \\chi^2_k$ is given by:\n$$ f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2 - 1} \\exp(-x/2) $$\nfor $x > 0$, where $k$ is a positive integer representing the degrees of freedom and $\\Gamma(\\cdot)$ is the gamma function. The support of the distribution is the interval $(0, \\infty)$. The mode is the value of $x$ that maximizes $f(x; k)$ over this support.\n\nTo simplify the maximization, we can work with the natural logarithm of the PDF, $\\ln f(x; k)$, since the logarithm is a strictly increasing function. Maximizing $\\ln f(x; k)$ will yield the same value of $x$ that maximizes $f(x; k)$.\n$$ \\ln f(x; k) = \\ln\\left(\\frac{1}{2^{k/2} \\Gamma(k/2)}\\right) + \\ln\\left(x^{k/2 - 1}\\right) + \\ln\\left(\\exp(-x/2)\\right) $$\n$$ \\ln f(x; k) = -\\frac{k}{2}\\ln(2) - \\ln\\left(\\Gamma(k/2)\\right) + \\left(\\frac{k}{2} - 1\\right)\\ln(x) - \\frac{x}{2} $$\nTo find the maximum, we compute the first derivative with respect to $x$ and set it to zero. The terms not involving $x$ are constants with respect to differentiation.\n$$ \\frac{d}{dx} \\ln f(x; k) = \\frac{d}{dx} \\left[ \\left(\\frac{k}{2} - 1\\right)\\ln(x) - \\frac{x}{2} \\right] $$\n$$ \\frac{d}{dx} \\ln f(x; k) = \\left(\\frac{k}{2} - 1\\right)\\frac{1}{x} - \\frac{1}{2} $$\nSetting the derivative to zero to find critical points:\n$$ \\left(\\frac{k}{2} - 1\\right)\\frac{1}{x} - \\frac{1}{2} = 0 $$\n$$ \\frac{k-2}{2x} = \\frac{1}{2} $$\nAssuming $x \\neq 0$, which is true for the support, we find the critical point:\n$$ x = k - 2 $$\nWe must now analyze this result based on the value of $k$, considering that $k$ is a positive integer.\n\nCase 1: $k > 2$.\nFor $k > 2$, the critical point $x=k-2$ is positive, so it lies within the support $(0, \\infty)$. To determine if this is a maximum, we examine the second derivative:\n$$ \\frac{d^2}{dx^2} \\ln f(x; k) = \\frac{d}{dx} \\left[ \\left(\\frac{k-2}{2}\\right)x^{-1} - \\frac{1}{2} \\right] = -\\left(\\frac{k-2}{2}\\right)x^{-2} = -\\frac{k-2}{2x^2} $$\nSince $k > 2$, the term $k-2$ is positive. For any $x > 0$, $2x^2$ is also positive. Therefore, the second derivative is negative for all $x$ in the support. This confirms that the log-PDF is concave and the critical point $x=k-2$ is a unique maximum. The mode is $k-2$.\n\nCase 2: $k \\le 2$. Since $k$ must be a positive integer, this case includes $k=1$ and $k=2$.\nIf $k=2$, the first derivative becomes:\n$$ \\frac{d}{dx} \\ln f(x; 2) = \\left(\\frac{2-2}{2}\\right)\\frac{1}{x} - \\frac{1}{2} = 0 - \\frac{1}{2} = -\\frac{1}{2} $$\nSince the derivative is always negative for $x>0$, the function $f(x; 2)$ is strictly decreasing over its entire support. The maximum value is approached as $x$ approaches the left boundary of the support, i.e., $x \\to 0^+$. Thus, the mode is at $x=0$. Notice that the formula $k-2$ gives $2-2=0$.\n\nIf $k=1$, the first derivative becomes:\n$$ \\frac{d}{dx} \\ln f(x; 1) = \\left(\\frac{1-2}{2}\\right)\\frac{1}{x} - \\frac{1}{2} = -\\frac{1}{2x} - \\frac{1}{2} $$\nFor $x > 0$, both $-\\frac{1}{2x}$ and $-\\frac{1}{2}$ are negative, so the derivative is strictly negative. Again, the function $f(x; 1)$ is strictly decreasing. The PDF, $f(x;1) = (\\sqrt{2\\pi})^{-1} x^{-1/2} \\exp(-x/2)$, has a vertical asymptote at $x=0$. The supremum of the function is at $x=0$, so the mode is $0$. In this case, the critical point formula $k-2$ gives $1-2 = -1$, which is outside the support, confirming that the maximum is not an interior point.\n\nSummarizing the results for the mode:\n- If $k > 2$, the mode is $k-2$.\n- If $k \\in \\{1, 2\\}$, the mode is $0$.\n\nThis can be expressed as a single function of $k$: $\\text{Mode} = \\max(0, k-2)$.\n\nThe problem then asks for a comparison of the mode to the mean for $k \\ge 2$, and an interpretation in terms of skewness.\nThe mean is $\\mu = k$.\nThe mode is $\\text{Mode} = \\max(0, k-2)$.\n\nFor $k \\ge 2$:\n- If $k=2$, Mean $= 2$, Mode $= \\max(0, 2-2) = 0$. Here, Mean > Mode.\n- If $k>2$, Mean $= k$, Mode $= \\max(0, k-2) = k-2$. Clearly, $k > k-2$, so Mean > Mode.\n\nIn all cases where $k \\ge 2$, the mean of the chi-squared distribution is strictly greater than its mode.\n\nThis comparison, Mean > Mode, is a characteristic feature of a right-skewed or positively skewed distribution. For a unimodal distribution, this inequality indicates that the bulk of the probability mass is concentrated at lower values, while a longer tail extends towards higher values. The mean is pulled to the right (towards higher values) by these larger, albeit less frequent, values in the tail. The formal measure of skewness for a $\\chi^2_k$ distribution is $\\gamma_1 = \\frac{2\\sqrt{2}}{\\sqrt{k}}$, which is always positive for $k>0$, confirming that the distribution is always right-skewed. As $k$ increases, the skewness decreases, and the mean ($k$) and mode ($k-2$) become closer, reflecting the distribution's convergence towards the symmetric normal distribution.\n\nThe final answer required is the single closed-form expression for the mode as a function of $k$.", "answer": "$$ \\boxed{\\max(0, k-2)} $$", "id": "4958326"}, {"introduction": "One of the most important applications of the chi-squared distribution is in hypothesis testing, where it serves as the theoretical backbone for the goodness-of-fit test. This computational exercise bridges theory and practice by tasking you with simulating the Pearson test statistic. By observing how the statistic's empirical distribution converges to the theoretical chi-squared distribution as the sample size increases, you will witness a fundamental limit theorem of statistics in action [@problem_id:2405617].", "problem": "You will study the asymptotic null distribution of the Pearson chi-square goodness-of-fit test statistic via simulation and quantify its convergence to the theoretical chi-square distribution. Consider a categorical model with $K$ categories and a probability vector $\\mathbf{p} = (p_{1},\\ldots,p_{K})$ where each $p_{i} \\in (0,1)$ and $\\sum_{i=1}^{K} p_{i} = 1$. For a sample of size $n$, let $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ denote the category counts, assumed to follow a multinomial distribution under the null hypothesis, and let $\\mathbf{E} = (E_{1},\\ldots,E_{K})$ be the expected counts with $E_{i} = n p_{i}$ for each $i \\in \\{1,\\ldots,K\\}$. Define the Pearson chi-square statistic\n$$\nQ_{n} \\;=\\; \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\nUnder the null hypothesis with fixed $K$ and all $p_{i} \\in (0,1)$, it is known that as $n \\to \\infty$, the distribution of $Q_{n}$ converges to the chi-square distribution with $K-1$ degrees of freedom.\n\nYour task is to produce a deterministic simulation-based program that, for each specified test case, approximates the distribution of $Q_{n}$ using Monte Carlo replication and then reports the Kolmogorov-Smirnov distance between the empirical distribution of $Q_{n}$ and the theoretical chi-square distribution with $K-1$ degrees of freedom. The Kolmogorov-Smirnov (KS) distance is the supremum of the absolute difference between two Cumulative Distribution Functions (CDFs):\n$$\nD \\;=\\; \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) \\;-\\; F_{\\chi^{2}_{K-1}}(x) \\right|.\n$$\nYour program must be deterministic by controlling pseudorandomness with the provided seed in each test case.\n\nUse the following test suite. For each test case, you are given $(K,\\mathbf{p},n,R,s)$ where $K$ is the number of categories, $\\mathbf{p}$ is the probability vector, $n$ is the sample size, $R$ is the number of Monte Carlo replications, and $s$ is the seed for the pseudorandom number generator.\n\n- Test case $1$ (general case with moderate $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 50$\n  - $R = 20000$\n  - $s = 12345$\n- Test case $2$ (same model, larger $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 200$\n  - $R = 20000$\n  - $s = 12345$\n- Test case $3$ (same model, much larger $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 2000$\n  - $R = 20000$\n  - $s = 12345$\n- Test case $4$ (edge case with small $n$ and many categories relative to $n$):\n  - $K = 8$\n  - $\\mathbf{p} = (1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8)$\n  - $n = 16$\n  - $R = 20000$\n  - $s = 67890$\n- Test case $5$ (higher dimension model):\n  - $K = 20$\n  - $\\mathbf{p} = (1/20,\\,\\ldots,\\,1/20)$\n  - $n = 1000$\n  - $R = 15000$\n  - $s = 13579$\n\nYour program must compute, for each test case, a single real number equal to the Kolmogorov-Smirnov distance $D$ between the empirical distribution of $Q_{n}$ (based on $R$ replications) and the chi-square distribution with $K-1$ degrees of freedom. The answers must be real numbers rounded to exactly $6$ decimal places.\n\nFinal output format: Your program should produce a single line containing the results for the five test cases as a comma-separated list enclosed in square brackets (for example, $[d_{1},d_{2},d_{3},d_{4},d_{5}]$), where each $d_{j}$ is the rounded Kolmogorov-Smirnov distance for test case $j$.", "solution": "The problem posed is a well-defined exercise in computational statistics and is deemed valid. It is scientifically grounded in established statistical theory, namely Pearson's chi-square test and its asymptotic properties. The parameters for each test case are complete, consistent, and allow for a unique, verifiable solution. The task requires a numerical investigation of a fundamental limit theorem, which is a standard and meaningful procedure in quantitative disciplines.\n\nThe solution proceeds as follows. We address the problem of quantifying the convergence of the Pearson chi-square statistic, $Q_{n}$, to its asymptotic $\\chi^2$ distribution.\n\nFirst, we establish the theoretical foundation. Under the null hypothesis, the observed counts $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ for a sample of size $n$ drawn from a categorical distribution with $K$ categories and probability vector $\\mathbf{p}=(p_1, \\ldots, p_K)$ follow a multinomial distribution, denoted $\\text{Multinomial}(n, \\mathbf{p})$. The expected count for category $i$ is $E_i=np_i$. The Pearson chi-square statistic is given by\n$$\nQ_{n} = \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\nA fundamental result in statistics, Pearson's theorem, states that as the sample size $n$ approaches infinity, the distribution of $Q_n$ converges to a chi-square distribution with $K-1$ degrees of freedom, denoted $\\chi^2_{K-1}$. This convergence is the subject of our investigation. The rate of this convergence depends on $n$, $K$, and the specific probabilities $p_i$. The approximation is generally considered reliable when all expected counts $E_i$ are sufficiently large, typically $E_i \\ge 5$.\n\nTo numerically evaluate the quality of this approximation for finite $n$, we employ a Monte Carlo simulation. For each test case, defined by the parameters $(K, \\mathbf{p}, n, R, s)$, the procedure is as follows:\n\n1.  **Initialization**: We fix the seed of the pseudorandom number generator to the specified value $s$. This ensures that the simulation is deterministic and its results are perfectly reproducible.\n\n2.  **Simulation of $Q_n$**: We generate $R$ independent samples of the statistic $Q_n$. This is achieved by performing $R$ replications of the following steps:\n    a. Draw a vector of observed counts $\\mathbf{O} = (O_1, \\ldots, O_K)$ from the $\\text{Multinomial}(n, \\mathbf{p})$ distribution. Using a vectorized implementation, we can generate all $R$ vectors of counts in a single operation, resulting in an $R \\times K$ matrix of observed counts.\n    b. For each of the $R$ vectors of observed counts, we compute the corresponding value of $Q_n$. The expected counts $\\mathbf{E} = n\\mathbf{p}$ are constant across all replications. This calculation is also vectorized to efficiently compute an array of $R$ values of the $Q_n$ statistic.\n\n3.  **Quantification of Distributional Distance**: After generating $R$ samples of $Q_n$, we obtain an empirical Cumulative Distribution Function (CDF), denoted $\\widehat{F}_{Q_n}(x)$. We must measure the \"distance\" between this empirical CDF and the theoretical CDF of the target distribution, $F_{\\chi^2_{K-1}}(x)$. The problem specifies the use of the Kolmogorov-Smirnov (KS) distance, defined as the supremum of the absolute difference between the two CDFs:\n    $$\n    D = \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) - F_{\\chi^{2}_{K-1}}(x) \\right|.\n    $$\n    A smaller value of $D$ indicates a better fit between the empirical distribution of $Q_n$ and the theoretical $\\chi^2_{K-1}$ distribution. This computation is performed using the `kstest` function from the `scipy.stats` library, which directly compares the generated sample of $Q_n$ values against the theoretical chi-square distribution with $K-1$ degrees of freedom.\n\nThis entire procedure is applied to each of the five test cases. The final output for each case is the computed KS distance $D$, rounded to $6$ decimal places. The results from the test cases illustrate the properties of the asymptotic approximation: the distance $D$ is expected to decrease as $n$ increases (Cases $1-3$), and the approximation is expected to be poor when expected cell counts are small (Case $4$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Computes the Kolmogorov-Smirnov distance between the empirical distribution\n    of the Pearson chi-square statistic and the theoretical chi-square\n    distribution for a suite of test cases.\n    \"\"\"\n\n    def compute_ks_distance(K, p_vec, n, R, seed):\n        \"\"\"\n        Runs a Monte Carlo simulation to compute the KS distance for one test case.\n\n        Args:\n            K (int): Number of categories.\n            p_vec (list or np.ndarray): Probability vector.\n            n (int): Sample size.\n            R (int): Number of Monte Carlo replications.\n            seed (int): Seed for the pseudorandom number generator.\n\n        Returns:\n            float: The computed KS distance, rounded to 6 decimal places.\n        \"\"\"\n        # 1. Initialize the pseudorandom number generator for deterministic results.\n        rng = np.random.default_rng(seed)\n\n        # 2. Define model parameters and calculate theoretical expected counts.\n        p_vec = np.array(p_vec)\n        expected_counts = n * p_vec\n        df = K - 1  # Degrees of freedom for the chi-square distribution.\n\n        # 3. Generate R sets of observed counts from the multinomial distribution.\n        # This is a vectorized operation, creating an (R, K) array.\n        observed_counts = rng.multinomial(n, p_vec, size=R)\n\n        # 4. Calculate the Pearson chi-square statistic for each of the R replicates.\n        # This calculation is also vectorized for efficiency.\n        # We sum over the K categories (axis=1).\n        q_n_samples = np.sum((observed_counts - expected_counts)**2 / expected_counts, axis=1)\n\n        # 5. Compute the Kolmogorov-Smirnov statistic.\n        # This compares the empirical distribution of the simulated Q_n values\n        # against the theoretical chi-square CDF with K-1 degrees of freedom.\n        ks_statistic, _ = kstest(q_n_samples, 'chi2', args=(df,), N=R)\n\n        # 6. Return the result rounded to the specified precision.\n        return round(ks_statistic, 6)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (K, p_vector, n, R, seed)\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 50, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 200, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 2000, 20000, 12345),\n        (8, [1/8] * 8, 16, 20000, 67890),\n        (20, [1/20] * 20, 1000, 15000, 13579)\n    ]\n\n    results = []\n    for case in test_cases:\n        K, p, n, R, s = case\n        result = compute_ks_distance(K, p, n, R, s)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2405617"}]}