{"hands_on_practices": [{"introduction": "To truly understand the t-distribution, we must first appreciate its fundamental purpose: to handle uncertainty in variance when making inferences about the mean. This thought experiment explores a critical edge case—a sample size of one—to reveal why the concept of \"degrees of freedom\" is not just a mathematical footnote, but the very foundation that makes the t-test possible. By examining why the standard procedure breaks down with a single observation, you will gain a deeper intuition for the essential role that sample data plays in estimating population variability [@problem_id:4902390].", "problem": "A clinical study in a rare disease setting yields exactly one independent and identically distributed observation of a continuous biomarker from the target population. Let the single observation be denoted by $X_{1}$, and assume the population distribution is Normal with mean $\\mu$ and unknown variance $\\sigma^2$. The investigator asks for a $95\\%$ frequentist confidence interval for $\\mu$ based solely on $X_{1}$ and the model assumptions stated, and also considers what could be done if such an interval cannot be justified. Using only fundamental distributional properties of Normal samples and definitions of basic sample statistics, determine which of the following statements are correct about this edge case and viable ways forward.\n\nSelect all that apply.\n\nA. A standard Student’s $t$ interval based only on the single observation is valid, because one can use degrees of freedom $\\nu = n - 1 = 0$ and a finite critical value.\n\nB. Without any additional information about $\\sigma^2$, no finite-length frequentist confidence interval for $\\mu$ exists when $n = 1$, because the sampling variability of the mean cannot be estimated from $X_{1}$ alone.\n\nC. A nonparametric bootstrap from the single observation provides a consistent estimate of $\\sigma$ and therefore enables a large-sample Wald confidence interval for $\\mu$.\n\nD. Adopting a Bayesian model with a proper prior on $(\\mu, \\sigma^2)$ (for example, a Normal–Inverse-Gamma prior) yields a Student’s $t$ posterior for $\\mu$ and thus a $95\\%$ credible interval; its frequentist coverage depends on the prior choice.\n\nE. If an independent external sample from the same population of size $m \\ge 2$ is available to estimate $\\sigma^2$, then $(X_{1}-\\mu)/S_{\\text{ext}}$ has a Student’s $t$ distribution with degrees of freedom $\\nu = m - 1$, leading to a valid frequentist confidence interval for $\\mu$ under Normality and independence.", "solution": "The problem statement is a valid and well-posed question in mathematical statistics, specifically concerning the foundational principles of confidence intervals. It probes a critical edge case ($n=1$) that highlights the essential role of variance estimation.\n\nThe core of the problem lies in the construction of a frequentist confidence interval for the population mean $\\mu$ of a Normal distribution, $N(\\mu, \\sigma^2)$, when the variance $\\sigma^2$ is unknown and the sample size is $n=1$.\n\nLet the single observation be $X_1$.\nThe sample mean is the best estimator for $\\mu$, which is $\\bar{X} = X_1$.\nThe standard method for constructing a confidence interval for $\\mu$ with unknown $\\sigma^2$ relies on the Student's $t$-distribution. The pivotal quantity is:\n$$ T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} $$\nwhere $S^2$ is the unbiased sample variance:\n$$ S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2 $$\nFor a sample of size $n=1$, we have $\\bar{X} = X_1$. The sample variance becomes:\n$$ S^2 = \\frac{1}{1-1} (X_1 - X_1)^2 = \\frac{0}{0} $$\nThe sample variance is indeterminate. Consequently, the sample standard deviation $S$ is also undefined. It is impossible to compute the $t$-statistic from the sample data alone. This fundamental inability to estimate the population variance $\\sigma^2$ from a single data point is the central issue.\n\nNow, we will evaluate each statement.\n\n**A. A standard Student’s $t$ interval based only on the single observation is valid, because one can use degrees of freedom $\\nu = n - 1 = 0$ and a finite critical value.**\n\nThis statement is **Incorrect**.\nThe formula for the sample variance $S^2$ involves division by $n-1$. For $n=1$, this is a division by $0$, making $S^2$ and its square root $S$ undefined. The $t$-statistic, $T = (X_1 - \\mu)/S$, cannot be constructed.\nFurthermore, the degrees of freedom for the $t$-distribution would be $\\nu = n-1 = 1-1 = 0$. The Student's $t$-distribution with $\\nu > 0$ degrees of freedom has quantiles $t_{\\alpha/2, \\nu}$ that are finite. However, as $\\nu \\to 0^+$, the tails of the distribution become infinitely heavy, and the quantiles $t_{\\alpha/2, \\nu}$ diverge to infinity for any $\\alpha \\in (0, 1)$. Thus, there is no *finite* critical value for a $t$-distribution with $0$ degrees of freedom. A confidence interval would be of the form $X_1 \\pm t_{\\alpha/2, 0} S$, which is not only based on an undefined $S$ but would also have an infinite width, corresponding to the uninformative interval $(-\\infty, \\infty)$.\n\n**B. Without any additional information about $\\sigma^{2}$, no finite-length frequentist confidence interval for $\\mu$ exists when $n = 1$, because the sampling variability of the mean cannot be estimated from $X_{1}$ alone.**\n\nThis statement is **Correct**.\nA frequentist confidence interval for $\\mu$ must have a width that reflects the uncertainty in the estimate $\\bar{X}=X_1$. This uncertainty is governed by the standard error of the mean, which for $n=1$ is $\\sigma/\\sqrt{1} = \\sigma$. To construct an interval, we must use a pivotal quantity whose distribution is known and free of unknown parameters.\nThe quantity $Z = (X_1 - \\mu)/\\sigma$ follows a $N(0,1)$ distribution, but it cannot be used to construct a confidence interval because it contains the unknown parameter $\\sigma$.\nTo eliminate $\\sigma$, we must replace it with an estimate. As shown previously, the sample variance $S^2$ cannot be calculated from a single observation. There is no information in the single data point $X_1$ to estimate the population's scale or variability, $\\sigma^2$. Any positive value of $\\sigma$ is equally compatible with the observation $X_1$. Since the length of any confidence interval for $\\mu$ must be proportional to some measure of scale, and since scale cannot be estimated, no informative, finite-length confidence interval can be constructed. The statement correctly identifies that the sampling variability, $\\text{Var}(X_1) = \\sigma^2$, cannot be estimated from $X_1$ alone, which is the precise reason for this impossibility.\n\n**C. A nonparametric bootstrap from the single observation provides a consistent estimate of $\\sigma$ and therefore enables a large-sample Wald confidence interval for $\\mu$.**\n\nThis statement is **Incorrect**.\nThe nonparametric bootstrap procedure involves resampling with replacement from the observed sample. In this case, the sample is $\\{X_1\\}$. Any bootstrap sample of size $n=1$ drawn from this set will be $\\{X_1\\}$. In fact, any bootstrap sample of any size $m$ will be $\\{X_1, X_1, \\dots, X_1\\}$.\nThe mean of every such bootstrap sample is always $X_1$. The variance of the bootstrap means is therefore $0$. The bootstrap estimate of the standard error of the sample mean is $0$.\nSimilarly, the sample variance of any bootstrap sample is $\\frac{1}{m-1}\\sum_{i=1}^m(X_1 - X_1)^2 = 0$. Thus, the bootstrap estimate of $\\sigma$ is $0$. This is not a consistent or sensible estimate of $\\sigma$. A Wald confidence interval of the form $\\hat{\\mu} \\pm z_{\\alpha/2}\\widehat{\\text{SE}}(\\hat{\\mu})$ would collapse to the point $[X_1, X_1]$, which has a $0\\%$ probability of containing $\\mu$ (unless $\\sigma=0$, which is impossible for a continuous variable). The bootstrap method fails completely in this scenario.\n\n**D. Adopting a Bayesian model with a proper prior on $(\\mu,\\sigma^{2})$ (for example, a Normal–Inverse-Gamma prior) yields a Student’s $t$ posterior for $\\mu$ and thus a $95\\%$ credible interval; its frequentist coverage depends on the prior choice.**\n\nThis statement is **Correct**.\nThe frequentist impasse can be resolved by switching to a Bayesian framework. In this paradigm, we provide prior information about the unknown parameters $(\\mu, \\sigma^2)$ via a prior distribution, $p(\\mu, \\sigma^2)$. The Normal-Inverse-Gamma distribution is a conjugate prior for the mean and variance of a Normal distribution.\nWhen we combine this prior with the likelihood from the single observation, $L(\\mu, \\sigma^2 | X_1)$, we obtain a posterior distribution $p(\\mu, \\sigma^2 | X_1)$. By integrating out the nuisance parameter $\\sigma^2$, we can find the marginal posterior distribution for $\\mu$, $p(\\mu|X_1)$. For the standard conjugate prior, this marginal posterior is a non-standardized Student's $t$-distribution.\nFrom this posterior distribution, one can certainly compute a $95\\%$ credible interval for $\\mu$. This interval's location and width will depend on both the single data point $X_1$ and the hyperparameters of the chosen prior. Since $n=1$, the prior has a very strong influence on the posterior.\nThe final clause states that the interval's frequentist coverage depends on the prior choice. This is also true. A Bayesian credible interval is not guaranteed to have nominal frequentist coverage. The frequentist coverage of a Bayesian procedure is a property evaluated over repeated sampling, and it generally depends on the true parameter values and the choice of prior. For $n=1$, this dependence is extreme. Therefore, this statement accurately describes a viable path forward and its properties.\n\n**E. If an independent external sample from the same population of size $m \\ge 2$ is available to estimate $\\sigma^{2}$, then $(X_{1}-\\mu)/S_{\\text{ext}}$ has a Student’s $t$ distribution with degrees of freedom $\\nu = m - 1$, leading to a valid frequentist confidence interval for $\\mu$ under Normality and independence.**\n\nThis statement is **Correct**.\nThis proposes another valid way to overcome the lack of an internal variance estimate: use an external one. Let the external sample be $Y_1, \\dots, Y_m$ from a population with the same variance $\\sigma^2$. The external sample variance is $S_{\\text{ext}}^2 = \\frac{1}{m-1} \\sum_{i=1}^m (Y_i - \\bar{Y})^2$. This requires $m \\ge 2$ for $S_{\\text{ext}}^2$ to be defined.\nWe rely on the definition of the Student's $t$-distribution. A random variable $T$ has a $t$-distribution with $\\nu$ degrees of freedom if it can be written as $T = \\frac{Z}{\\sqrt{U/\\nu}}$, where $Z \\sim N(0,1)$ and $U \\sim \\chi_{\\nu}^2$ are independent.\nLet's define our components:\n1.  Since $X_1 \\sim N(\\mu, \\sigma^2)$, the standardized variable $Z = \\frac{X_1 - \\mu}{\\sigma}$ follows a standard normal distribution, $N(0,1)$.\n2.  From standard sampling theory, the quantity $U = \\frac{(m-1)S_{\\text{ext}}^2}{\\sigma^2}$ follows a chi-squared distribution with $\\nu = m-1$ degrees of freedom.\n3.  The problem states that the external sample is independent of the observation $X_1$. Therefore, $Z$ and $U$ are independent random variables.\nNow, construct the ratio:\n$$ T = \\frac{Z}{\\sqrt{U/\\nu}} = \\frac{\\frac{X_1 - \\mu}{\\sigma}}{\\sqrt{\\frac{(m-1)S_{\\text{ext}}^2/\\sigma^2}{m-1}}} = \\frac{\\frac{X_1 - \\mu}{\\sigma}}{\\sqrt{S_{\\text{ext}}^2/\\sigma^2}} = \\frac{X_1 - \\mu}{S_{\\text{ext}}} $$\nThis shows that the quantity $(X_1 - \\mu)/S_{\\text{ext}}$ indeed follows a Student's $t$-distribution with $\\nu=m-1$ degrees of freedom. This is a pivotal quantity as its distribution is known and it can be computed from the data (given $S_{\\text{ext}}$) and the parameter $\\mu$. This allows for the construction of a valid frequentist confidence interval: $X_1 \\pm t_{\\alpha/2, m-1} S_{\\text{ext}}$.", "answer": "$$\\boxed{BDE}$$", "id": "4902390"}, {"introduction": "With a solid grasp of why degrees of freedom are essential, we can now explore how the t-distribution's properties influence the practice of hypothesis testing. This exercise asks you to compare a one-sided test with a two-sided test, focusing on their respective critical values and statistical power [@problem_id:4960980]. This is a crucial practical consideration for any researcher, as the choice of hypothesis directly impacts the test's sensitivity and its ability to detect a true effect, all of which is governed by the geometry of the t-distribution.", "problem": "A biostatistician is studying a continuous biomarker whose population distribution is assumed Normal with unknown variance. A sample of size $n$ is drawn independently, and the goal is to test a mean shift relative to a clinically negligible baseline $\\mu_{0}$. Define the test statistic\n$$\nT \\;=\\; \\frac{\\bar{X} - \\mu_{0}}{S/\\sqrt{n}},\n$$\nwhere $\\bar{X}$ is the sample mean and $S^2$ is the unbiased sample variance. Under the null hypothesis $H_{0}:\\mu=\\mu_{0}$, $T$ has a Student $t$ distribution with $\\nu=n-1$ degrees of freedom. The biostatistician must choose between a $1$-sided alternative $H_{1}:\\mu>\\mu_{0}$ and a $2$-sided alternative $H_{1}:\\mu\\neq\\mu_{0}$, both conducted at the same significance level $\\alpha$, with $n$ fixed. Suppose the true standardized effect size $d=(\\mu-\\mu_{0})/\\sigma$ is strictly positive and fixed, where $\\sigma$ is the population standard deviation.\n\nWhich statement best characterizes the relationship between the $t$ critical values for the $1$-sided versus $2$-sided tests and the resulting impact on power for this positive effect, holding $n$ and $\\alpha$ fixed?\n\nA. For fixed $\\alpha$ and $\\nu=n-1$, the upper critical value for the $1$-sided test is the $\\left(1-\\alpha\\right)$ quantile of the Student $t$ distribution, which is smaller than the absolute critical value for the $2$-sided test that uses the $\\left(1-\\alpha/2\\right)$ quantile; therefore, for a true positive effect of fixed magnitude, the $1$-sided test has higher power than the $2$-sided test.\n\nB. Because the $2$-sided test rejects in both tails, its power is always higher than that of the $1$-sided test at the same $\\alpha$ for any fixed positive effect and $n$.\n\nC. The equality of degrees of freedom implies that the $1$-sided and $2$-sided critical values are equal at the same $\\alpha$, so their powers are equal.\n\nD. As $n$ increases with fixed positive effect size, both tests’ critical values approach those of the Standard Normal distribution, so the difference in power vanishes exactly for all finite $n$, making the tests equally powerful at any $n$.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n-   The continuous biomarker has a population distribution that is Normal with unknown variance.\n-   A sample of size $n$ is drawn independently.\n-   The null hypothesis is $H_{0}:\\mu=\\mu_{0}$, where $\\mu_0$ is a clinically negligible baseline.\n-   The test statistic is $T = \\frac{\\bar{X} - \\mu_{0}}{S/\\sqrt{n}}$.\n-   $\\bar{X}$ is the sample mean.\n-   $S^{2}$ is the unbiased sample variance.\n-   Under $H_{0}$, the distribution of $T$ is Student's $t$ with $\\nu=n-1$ degrees of freedom.\n-   Two alternative hypotheses are considered:\n    1.  $H_{1}:\\mu>\\mu_{0}$ (1-sided)\n    2.  $H_{1}:\\mu\\neq\\mu_{0}$ (2-sided)\n-   Both tests are conducted at the same significance level $\\alpha$.\n-   The sample size $n$ is fixed.\n-   The true standardized effect size is $d=(\\mu-\\mu_{0})/\\sigma$, where $\\mu$ is the true population mean and $\\sigma$ is the true population standard deviation.\n-   It is given that $d$ is strictly positive and fixed, i.e., $d > 0$.\n-   The question asks to characterize the relationship between the critical values of the two tests and the resulting impact on statistical power.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem describes a one-sample Student's t-test, a fundamental procedure in statistical inference and biostatistics. All concepts, including the test statistic, its distribution under the null hypothesis, null and alternative hypotheses, significance level, critical values, and statistical power, are standard and correctly defined.\n-   **Well-Posed**: The question is clear and unambiguous. It asks for a comparison of power between two well-defined hypothesis tests under a specific, well-defined condition (a true positive effect size). A unique and logical conclusion can be derived from the premises.\n-   **Objective**: The problem is stated in precise, formal statistical language. It is free of any subjective, ambiguous, or opinion-based claims.\n\nThere are no flaws detected:\n1.  **Scientific or Factual Unsoundness**: The setup is a textbook example of a t-test and is factually sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is a standard, formalizable question in hypothesis testing theory.\n3.  **Incomplete or Contradictory Setup**: All necessary information is provided ($H_0$, $H_1$, $\\alpha$, $n$, and the condition on the true effect size). There are no contradictions.\n4.  **Unrealistic or Infeasible**: This scenario is common in research, particularly in clinical trials where a new treatment is expected to be better than a baseline.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-structured and leads to a unique answer.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem addresses a core conceptual point in hypothesis testing that requires careful reasoning about the interplay between critical values and power. It is not trivial.\n7.  **Outside Scientific Verifiability**: The claims and the correct answer are mathematically provable within the framework of statistical theory.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n## Derivation of Solution\n\nLet the Student's $t$ distribution with $\\nu$ degrees of freedom be denoted by $t_{\\nu}$. The problem states that the sample size $n$ and significance level $\\alpha$ are fixed. The degrees of freedom are $\\nu = n-1$.\n\n**1. Critical Value for the 1-sided test:**\nThe alternative hypothesis is $H_{1}:\\mu>\\mu_{0}$. We reject the null hypothesis $H_0$ if the test statistic $T$ is sufficiently large and positive. The rejection region is of the form $T > c_{1s}$, where $c_{1s}$ is the critical value.\nThe critical value is defined such that the probability of a Type I error is $\\alpha$:\n$$P(T > c_{1s} \\mid H_{0}) = \\alpha$$\nUnder $H_0$, $T \\sim t_{n-1}$. The value $c_{1s}$ is the upper $\\alpha$ point of the $t_{n-1}$ distribution. This corresponds to the $(1-\\alpha)$ quantile. Let $t_{\\nu, p}$ denote the $p$-th quantile of the $t_{\\nu}$ distribution. Then:\n$$c_{1s} = t_{n-1, 1-\\alpha}$$\n\n**2. Critical Value for the 2-sided test:**\nThe alternative hypothesis is $H_{1}:\\mu\\neq\\mu_{0}$. We reject $H_0$ if $T$ is either too large and positive or too large and negative. The rejection region is of the form $|T| > c_{2s}$.\nThe critical value $c_{2s}$ is defined such that the total probability of a Type I error is $\\alpha$:\n$$P(|T| > c_{2s} \\mid H_{0}) = \\alpha$$\nDue to the symmetry of the Student's $t$ distribution about $0$, this probability is split equally between the two tails:\n$$P(T > c_{2s} \\mid H_{0}) + P(T < -c_{2s} \\mid H_{0}) = 2 \\times P(T > c_{2s} \\mid H_{0}) = \\alpha$$\n$$P(T > c_{2s} \\mid H_{0}) = \\alpha/2$$\nThis means $c_{2s}$ is the upper $\\alpha/2$ point of the $t_{n-1}$ distribution, which corresponds to the $(1-\\alpha/2)$ quantile:\n$$c_{2s} = t_{n-1, 1-\\alpha/2}$$\n\n**3. Comparison of Critical Values:**\nThe significance level $\\alpha$ is typically a small positive number (e.g., $0.05$). Thus, $0 < \\alpha < 1$. This implies $\\alpha/2 < \\alpha$, and therefore $1 - \\alpha/2 > 1 - \\alpha$.\nThe quantile function of the Student's $t$ distribution, $F_{t_{n-1}}^{-1}(p) = t_{n-1, p}$, is a strictly increasing function of the probability $p$.\nSince $1-\\alpha/2 > 1-\\alpha$, it follows that:\n$$t_{n-1, 1-\\alpha/2} > t_{n-1, 1-\\alpha}$$\n$$c_{2s} > c_{1s}$$\nThe positive critical value for the 2-sided test is larger than the critical value for the 1-sided test.\n\n**4. Power Analysis:**\nPower is the probability of correctly rejecting $H_0$ when the alternative hypothesis $H_1$ is true. We are given that the true effect size $d = (\\mu-\\mu_0)/\\sigma$ is strictly positive. This implies that the true mean $\\mu > \\mu_0$.\nUnder this condition, the test statistic $T = \\frac{\\bar{X} - \\mu_{0}}{S/\\sqrt{n}}$ follows a non-central t-distribution with $\\nu=n-1$ degrees of freedom and a non-centrality parameter $\\delta = \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}} = d\\sqrt{n}$. Since $d > 0$, we have $\\delta > 0$. The distribution of $T$ is shifted to the right, centered around a positive value.\n\nPower of the 1-sided test ($P_1$):\n$$P_1 = P(\\text{Reject } H_0 \\mid \\mu > \\mu_0) = P(T > c_{1s} \\mid \\mu > \\mu_0)$$\n$$P_1 = P(T > t_{n-1, 1-\\alpha} \\mid \\delta > 0)$$\n\nPower of the 2-sided test ($P_2$):\n$$P_2 = P(\\text{Reject } H_0 \\mid \\mu > \\mu_0) = P(|T| > c_{2s} \\mid \\mu > \\mu_0)$$\n$$P_2 = P(T > c_{2s} \\mid \\mu > \\mu_0) + P(T < -c_{2s} \\mid \\mu > \\mu_0)$$\n$$P_2 = P(T > t_{n-1, 1-\\alpha/2} \\mid \\delta > 0) + P(T < -t_{n-1, 1-\\alpha/2} \\mid \\delta > 0)$$\n\nSince the distribution of $T$ is shifted to the right (centered around $\\delta > 0$), the probability mass will be concentrated on the positive real axis. The term $P(T < -t_{n-1, 1-\\alpha/2} \\mid \\delta > 0)$ will be very small, often negligible.\nThe dominant term in the power of the 2-sided test is $P(T > t_{n-1, 1-\\alpha/2} \\mid \\delta > 0)$.\nLet's compare $P_1$ and $P_2$. Since we established that $c_{1s} < c_{2s}$ (i.e., $t_{n-1, 1-\\alpha} < t_{n-1, 1-\\alpha/2}$), the 1-sided test has a less stringent threshold for rejection in the positive tail.\nFor any random variable $T$ with a cumulative distribution function $F_T$, the probability $P(T > c) = 1 - F_T(c)$ is a decreasing function of $c$.\nSince $c_{1s} < c_{2s}$, we have:\n$$P(T > c_{1s} \\mid \\delta > 0) > P(T > c_{2s} \\mid \\delta > 0)$$\nTherefore, $P_1$ is greater than the main component of $P_2$. The 2-sided test \"wastes\" a small part of its Type I error probability $\\alpha/2$ in the left tail, which is the \"wrong\" tail given the true positive effect. This leads to a more stringent critical value in the \"right\" tail, which reduces its ability to detect the effect compared to the 1-sided test that concentrates all its $\\alpha$ in the correct tail. Consequently, the 1-sided test is more powerful.\n$P_1 > P_2$.\n\n## Evaluation of Options\n\n**A. For fixed $\\alpha$ and $\\nu=n-1$, the upper critical value for the $1$-sided test is the $\\left(1-\\alpha\\right)$ quantile of the Student $t$ distribution, which is smaller than the absolute critical value for the $2$-sided test that uses the $\\left(1-\\alpha/2\\right)$ quantile; therefore, for a true positive effect of fixed magnitude, the $1$-sided test has higher power than the $2$-sided test.**\nThis statement aligns perfectly with our detailed derivation.\n1. The 1-sided critical value is $t_{n-1, 1-\\alpha}$. **Correct**.\n2. The 2-sided critical value is $t_{n-1, 1-\\alpha/2}$. **Correct**.\n3. $t_{n-1, 1-\\alpha} < t_{n-1, 1-\\alpha/2}$. **Correct**.\n4. The conclusion that the 1-sided test has higher power for a positive effect is also **Correct**.\nThe entire statement is logically sound and factually accurate.\n\n**B. Because the $2$-sided test rejects in both tails, its power is always higher than that of the $1$-sided test at the same $\\alpha$ for any fixed positive effect and $n$.**\nThis is incorrect. The fact that it has a rejection region in both tails makes it less powerful for detecting an effect that lies in only one of those tails. It is less \"focused\". As shown above, the power of the 1-sided test is greater. **Incorrect**.\n\n**C. The equality of degrees of freedom implies that the $1$-sided and $2$-sided critical values are equal at the same $\\alpha$, so their powers are equal.**\nThe premise is false. While the degrees of freedom ($\\nu=n-1$) are the same for the distribution, the critical values are derived differently from that distribution. As shown, $t_{n-1, 1-\\alpha} \\neq t_{n-1, 1-\\alpha/2}$ for any $\\alpha \\in (0,1)$. The critical values are not equal, and therefore the powers are not equal. **Incorrect**.\n\n**D. As $n$ increases with fixed positive effect size, both tests’ critical values approach those of the Standard Normal distribution, so the difference in power vanishes exactly for all finite $n$, making the tests equally powerful at any $n$.**\nThis statement contains a flawed chain of reasoning. While it is true that as $n \\to \\infty$, the $t_{n-1}$ distribution converges to the standard normal distribution $N(0,1)$, this does not mean the difference in power vanishes for any finite $n$. The critical values converge to $z_{1-\\alpha}$ and $z_{1-\\alpha/2}$ respectively, which are themselves distinct values. For any finite $n$, $t_{n-1, 1-\\alpha} < t_{n-1, 1-\\alpha/2}$, which means the 1-sided test will always be more powerful for a positive effect. The claim that this makes them \"equally powerful at any $n$\" is false. **Incorrect**.\n\nFinal conclusion based on analysis is that Option A is the only correct statement.", "answer": "$$\\boxed{A}$$", "id": "4960980"}, {"introduction": "The final step in mastering the t-distribution is to apply it to a real-world design problem: determining the necessary sample size for a study. This practice challenges you to derive the sample size formula for a one-sample t-test from first principles, connecting statistical power ($1-\\beta$), significance ($\\alpha$), and effect size ($\\Delta$) through the noncentral t-distribution [@problem_id:4960987]. By performing this calculation, you will engage in the same process biostatisticians use to ensure that experiments are designed with enough statistical power to yield meaningful results.", "problem": "A clinical laboratory plans a one-sample study to determine whether the mean fasting high-sensitivity C-reactive protein (hs-CRP) concentration, denoted by $\\mu$ (in $\\mathrm{mg/L}$), differs from a standard reference value $\\mu_{0}$. Individual hs-CRP measurements are assumed to be independently and identically distributed as $\\mathcal{N}(\\mu, \\sigma^2)$ with unknown variance $\\sigma^2$, and the study will use the one-sample Student’s $t$ test with a two-sided type I error rate $\\alpha$.\n\nStarting from first principles—namely, the definition of the one-sample $t$ statistic $T=\\dfrac{\\bar{X}-\\mu_{0}}{S/\\sqrt{n}}$, where $\\bar{X}$ is the sample mean and $S$ is the sample standard deviation, and the fact that under $\\mu\\neq\\mu_{0}$ the statistic $T$ has a noncentral $t$ distribution with degrees of freedom $n-1$ and noncentrality parameter $\\delta$—derive the sample size equation one must solve to achieve a target power $1-\\beta$ for detecting a true mean shift of magnitude $\\Delta=|\\mu-\\mu_{0}|$.\n\nThen, for the design values $\\alpha=0.05$ (two-sided), desired power $1-\\beta=0.90$, assumed population standard deviation $\\sigma=10.0$ $\\mathrm{mg/L}$, and target difference $\\Delta=4.0$ $\\mathrm{mg/L}$, solve the derived power equation for the minimal integer sample size $n$ that achieves the target power. You may use scientifically justified asymptotic approximations to the noncentral $t$ distribution to solve for $n$, but your derivation must begin from the exact noncentral $t$ formulation. Express the final answer as the minimal integer $n$. No rounding instruction is needed, as the answer is an exact integer.", "solution": "The problem requires the derivation of a sample size equation for a one-sample Student's $t$-test and its subsequent solution for a specific set of design parameters.\n\nFirst, the problem is validated and deemed valid. It is scientifically grounded in standard statistical theory, is well-posed with a clear objective, and contains a complete and consistent set of givens.\n\nThe solution proceeds in two parts: first, the derivation of the general sample size equation, and second, the calculation of the minimal integer sample size $n$ for the specified parameters.\n\n**Part 1: Derivation of the Sample Size Equation**\n\nThe objective is to test the null hypothesis $H_0: \\mu = \\mu_0$ against the two-sided alternative hypothesis $H_A: \\mu \\neq \\mu_0$. The test statistic is the one-sample $t$-statistic:\n$$T = \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}}$$\nwhere $\\bar{X}$ is the sample mean, $S$ is the sample standard deviation, and $n$ is the sample size.\n\nUnder the null hypothesis $H_0$, the statistic $T$ follows a central Student's $t$-distribution with $\\nu = n-1$ degrees of freedom, denoted $t_{n-1}$. For a two-sided test with a Type I error rate of $\\alpha$, $H_0$ is rejected if $|T| > t_{1-\\alpha/2, n-1}$, where $t_{1-\\alpha/2, n-1}$ is the upper $100(1-\\alpha/2)\\%$ percentile of the central $t_{n-1}$ distribution.\n\nUnder the alternative hypothesis $H_A$, where the true mean is $\\mu \\neq \\mu_0$, the statistic $T$ follows a noncentral $t$-distribution with $\\nu = n-1$ degrees of freedom and a noncentrality parameter $\\delta$. The noncentrality parameter $\\delta$ is given by:\n$$\\delta = \\frac{(\\mu - \\mu_0)}{\\sigma/\\sqrt{n}}$$\nwhere $\\sigma$ is the true population standard deviation. Using the given definition $\\Delta = |\\mu - \\mu_0|$, the magnitude of the noncentrality parameter is $|\\delta| = \\frac{\\Delta\\sqrt{n}}{\\sigma}$.\n\nThe power of the test, $1-\\beta$, is the probability of correctly rejecting $H_0$ when $H_A$ is true. Let $T_{\\nu, \\delta}$ denote a random variable following a noncentral $t$-distribution with $\\nu$ degrees of freedom and noncentrality parameter $\\delta$. The power is:\n$$1-\\beta = P(|T_{n-1, \\delta}| > t_{1-\\alpha/2, n-1})$$\nThis can be written as the sum of two tail probabilities:\n$$1-\\beta = P(T_{n-1, \\delta} > t_{1-\\alpha/2, n-1}) + P(T_{n-1, \\delta} < -t_{1-\\alpha/2, n-1})$$\nThis is the exact power equation. Without loss of generality, let us assume $\\mu > \\mu_0$, which makes $\\delta > 0$. For a positive $\\delta$ and typical power values (e.g., $1-\\beta > 0.5$), the second term, $P(T_{n-1, \\delta} < -t_{1-\\alpha/2, n-1})$, is negligible. Thus, we can approximate the power as:\n$$1-\\beta \\approx P(T_{n-1, \\delta} > t_{1-\\alpha/2, n-1})$$\nThis is equivalent to stating that the probability of a Type II error, $\\beta$, is:\n$$\\beta \\approx P(T_{n-1, \\delta} \\le t_{1-\\alpha/2, n-1})$$\nTo solve this equation for $n$, we use a scientifically justified asymptotic approximation. For large degrees of freedom $\\nu$, the noncentral $t$-distribution $t_{\\nu, \\delta}$ can be approximated by a normal distribution $\\mathcal{N}(\\delta, 1)$. This approximation is justified as $S$ becomes a more precise estimator of $\\sigma$, causing the denominator of the $t$-statistic to approach a constant. Applying this approximation:\n$$\\beta \\approx P(\\mathcal{N}(\\delta, 1) \\le t_{1-\\alpha/2, n-1})$$\nTo evaluate this probability, we standardize the normal variable. Let $Z \\sim \\mathcal{N}(0,1)$.\n$$\\beta \\approx P\\left(\\frac{\\mathcal{N}(\\delta, 1) - \\delta}{1} \\le \\frac{t_{1-\\alpha/2, n-1} - \\delta}{1}\\right) = P(Z \\le t_{1-\\alpha/2, n-1} - \\delta)$$\nBy definition, the standard normal value $z_{\\beta}$ is the quantile for which $P(Z \\le z_{\\beta}) = \\beta$. Therefore, we have the relation:\n$$t_{1-\\alpha/2, n-1} - \\delta \\approx z_{\\beta}$$\nUsing the symmetry of the standard normal distribution, $z_{\\beta} = -z_{1-\\beta}$. Substituting this gives:\n$$t_{1-\\alpha/2, n-1} - \\delta \\approx -z_{1-\\beta}$$\nRearranging for the noncentrality parameter $\\delta$:\n$$\\delta \\approx t_{1-\\alpha/2, n-1} + z_{1-\\beta}$$\nFinally, we substitute the expression for the magnitude of the noncentrality parameter, $|\\delta| = \\frac{\\Delta\\sqrt{n}}{\\sigma}$:\n$$\\frac{\\Delta\\sqrt{n}}{\\sigma} \\approx t_{1-\\alpha/2, n-1} + z_{1-\\beta}$$\nThis is the required sample size equation. It is an implicit equation for $n$, as $n$ appears on both sides (in $\\sqrt{n}$ and in the degrees of freedom $n-1$ for the $t$-quantile).\n\n**Part 2: Solving for the Minimal Sample Size**\n\nWe are given the following design values:\n- Two-sided Type I error rate: $\\alpha = 0.05$.\n- Desired power: $1-\\beta = 0.90$, which implies $\\beta = 0.10$.\n- Assumed population standard deviation: $\\sigma = 10.0 \\, \\mathrm{mg/L}$.\n- Target difference to detect: $\\Delta = 4.0 \\, \\mathrm{mg/L}$.\n\nFirst, we find the necessary quantiles from the standard normal distribution:\n- For the significance level: $z_{1-\\alpha/2} = z_{1-0.025} = z_{0.975} \\approx 1.95996$.\n- For the power: $z_{1-\\beta} = z_{1-0.10} = z_{0.90} \\approx 1.28155$.\n\nThe ratio $\\sigma/\\Delta$ is $\\frac{10.0}{4.0} = 2.5$. Substituting these values into the sample size equation derived in Part 1 gives:\n$$\\sqrt{n} \\approx \\frac{\\sigma}{\\Delta} (t_{1-\\alpha/2, n-1} + z_{1-\\beta}) = 2.5 (t_{0.975, n-1} + 1.28155)$$\nSquaring both sides yields the equation to be solved for $n$:\n$$n \\approx (2.5)^2 (t_{0.975, n-1} + 1.28155)^2 = 6.25 (t_{0.975, n-1} + 1.28155)^2$$\nThis equation must be solved iteratively. We can start with an initial guess for $n$ by approximating the $t$-quantile with the corresponding normal quantile, $t_{0.975, n-1} \\approx z_{0.975} \\approx 1.95996$.\nInitial guess $n_0$:\n$$n_0 = 6.25 (1.95996 + 1.28155)^2 = 6.25 (3.24151)^2 \\approx 6.25 \\times 10.5074 \\approx 65.67$$\nThis suggests the required sample size is likely near $66$.\n\nLet's start the iteration with $n=66$.\nIteration 1: Let $n=66$, so the degrees of freedom are $\\nu = 65$. The required $t$-quantile is $t_{0.975, 65} \\approx 1.9971$.\nCalculate a new estimate for $n$:\n$$n_1 = 6.25 (1.9971 + 1.28155)^2 = 6.25 (3.27865)^2 \\approx 6.25 \\times 10.7495 \\approx 67.18$$\nSince $n_1 \\approx 67.18 > 66$, our guess of $n=66$ is too small.\n\nIteration 2: Let's try $n=68$, so $\\nu = 67$. The $t$-quantile is $t_{0.975, 67} \\approx 1.9960$.\nCalculate a new estimate for $n$:\n$$n_2 = 6.25 (1.9960 + 1.28155)^2 = 6.25 (3.27755)^2 \\approx 6.25 \\times 10.7426 \\approx 67.14$$\nWith a sample size of $n=68$, the formula indicates a required size of approximately $67.14$. Since $68 > 67.14$, a sample size of $68$ is sufficient to achieve the desired power.\n\nTo confirm that $n=67$ is not sufficient, let's check it. For $n=67$, $\\nu=66$, and $t_{0.975, 66} \\approx 1.9966$.\nRequired $n = 6.25(1.9966 + 1.28155)^2 = 6.25 (3.27815)^2 \\approx 67.16$.\nSince $67 < 67.16$, a sample size of $67$ is insufficient.\n\nTherefore, to ensure the power is at least $0.90$, we must round the required non-integer value up to the next whole number. The minimal integer sample size is $68$.", "answer": "$$\n\\boxed{68}\n$$", "id": "4960987"}]}