{"hands_on_practices": [{"introduction": "Moving from theory to practice, our first exercise focuses on a cornerstone of biostatistical modeling: Poisson regression. In many biological studies, we are interested in modeling rates—such as infections per day or events per unit area—rather than raw counts. This problem guides you through the process of setting up a Poisson Generalized Linear Model (GLM) with an 'offset' term to properly handle this, and you will derive the maximum likelihood estimators from first principles, revealing the mechanics behind the curtain of statistical software [@problem_id:4960760].", "problem": "A field study monitors the incidence of a parasitic infection in two habitats, with unequal observation effort across sampling units. Let $Y_{i}$ denote the count of new infections in unit $i$, and let $E_{i}$ denote the unit’s observation effort (exposure). Assume $Y_{i}$ are independent and follow a Poisson model appropriate for rare-event counts with a log link and an offset for exposure:\n$$\n\\log(\\mu_{i}) \\;=\\; \\log(E_{i}) \\;+\\; \\alpha \\;+\\; \\beta x_{i},\n$$\nwhere $\\mu_{i} = \\mathbb{E}[Y_{i}]$, $x_{i} \\in \\{0,1\\}$ is a single binary predictor indicating habitat ($x_{i}=0$ for habitat A, $x_{i}=1$ for habitat B), $\\alpha$ is an intercept, and $\\beta$ is the coefficient for the habitat effect. There is no covariate collinearity because there are observations in both levels of $x_{i}$. The data are:\n- Habitat A ($x_{i}=0$): $(E_{i},Y_{i}) \\in \\{(1.2,3),(0.8,1),(1.5,2),(0.5,0)\\}$,\n- Habitat B ($x_{i}=1$): $(E_{i},Y_{i}) \\in \\{(1.0,5),(2.0,9),(1.3,4),(0.7,3)\\}$.\n\nTasks:\n1. Starting from the Poisson probability mass function and the definition of the Generalized Linear Model (GLM) with a log link and offset, construct the score equations for $(\\alpha,\\beta)$ by taking partial derivatives of the log-likelihood and setting them to zero.\n2. Specialize the score equations to the case of a single binary predictor $x_{i}\\in\\{0,1\\}$ and derive closed-form expressions for the maximum likelihood estimators (MLEs) $\\widehat{\\alpha}$ and $\\widehat{\\beta}$ in terms of group-wise totals of $Y_{i}$ and $E_{i}$.\n3. Using the provided data, compute the numerical value of the MLE $\\widehat{\\beta}$. Round your answer to four significant figures. Do not include units in your final reported number.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the theory of Generalized Linear Models (GLMs), a cornerstone of modern statistics, and its application to biological count data is appropriate. The problem is well-posed, providing all necessary information, a clear model specification, and a dataset sufficient for the requested derivations and calculations. The terminology is precise and objective, and the task is a standard, non-trivial exercise in statistical theory and application. Therefore, a full solution is warranted.\n\n### 1. Derivation of the Score Equations\n\nThe first task is to construct the score equations for the parameters $(\\alpha, \\beta)$. The score equations are obtained by setting the first partial derivatives of the log-likelihood function with respect to the parameters to zero.\n\nLet $Y_i$ be the observed count for unit $i$, which is assumed to follow a Poisson distribution with mean $\\mu_i$, i.e., $Y_i \\sim \\text{Poisson}(\\mu_i)$. The probability mass function (PMF) for a single observation $Y_i=y_i$ is:\n$$\nP(Y_i = y_i | \\mu_i) = \\frac{\\mu_i^{y_i} \\exp(-\\mu_i)}{y_i!}\n$$\nThe log-likelihood for a single observation, $\\ell_i$, is the natural logarithm of the PMF:\n$$\n\\ell_i(\\mu_i | y_i) = \\log\\left( P(Y_i = y_i | \\mu_i) \\right) = y_i \\log(\\mu_i) - \\mu_i - \\log(y_i!)\n$$\nThe problem specifies a GLM with a log link function and an offset term for exposure $E_i$. The linear predictor is $\\eta_i = \\log(E_i) + \\alpha + \\beta x_i$. The link function connects the mean $\\mu_i$ to the linear predictor:\n$$\n\\log(\\mu_i) = \\eta_i = \\log(E_i) + \\alpha + \\beta x_i\n$$\nFrom this, we can express $\\mu_i$ as:\n$$\n\\mu_i = \\exp(\\log(E_i) + \\alpha + \\beta x_i) = E_i \\exp(\\alpha + \\beta x_i)\n$$\nSubstituting $\\log(\\mu_i)$ and $\\mu_i$ into the expression for $\\ell_i$, we obtain the log-likelihood for observation $i$ in terms of the parameters $\\alpha$ and $\\beta$:\n$$\n\\ell_i(\\alpha, \\beta) = y_i (\\log(E_i) + \\alpha + \\beta x_i) - E_i \\exp(\\alpha + \\beta x_i) - \\log(y_i!)\n$$\nAssuming the $n$ observations are independent, the total log-likelihood $L(\\alpha, \\beta)$ is the sum of the individual log-likelihoods:\n$$\nL(\\alpha, \\beta) = \\sum_{i=1}^{n} \\ell_i(\\alpha, \\beta) = \\sum_{i=1}^{n} \\left[ y_i (\\log(E_i) + \\alpha + \\beta x_i) - E_i \\exp(\\alpha + \\beta x_i) - \\log(y_i!) \\right]\n$$\nThe score vector consists of the partial derivatives of $L(\\alpha, \\beta)$ with respect to $\\alpha$ and $\\beta$.\n\nThe partial derivative with respect to $\\alpha$ is:\n$$\n\\frac{\\partial L}{\\partial \\alpha} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\alpha} \\left[ y_i \\alpha - E_i \\exp(\\alpha + \\beta x_i) \\right] = \\sum_{i=1}^{n} \\left[ y_i - E_i \\exp(\\alpha + \\beta x_i) \\right]\n$$\nRecalling that $\\mu_i = E_i \\exp(\\alpha + \\beta x_i)$, this simplifies to:\n$$\n\\frac{\\partial L}{\\partial \\alpha} = \\sum_{i=1}^{n} (y_i - \\mu_i)\n$$\nThe partial derivative with respect to $\\beta$ is:\n$$\n\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta} \\left[ y_i \\beta x_i - E_i \\exp(\\alpha + \\beta x_i) \\right] = \\sum_{i=1}^{n} \\left[ y_i x_i - E_i x_i \\exp(\\alpha + \\beta x_i) \\right]\n$$\nThis simplifies to:\n$$\n\\frac{\\partial L}{\\partial \\beta} = \\sum_{i=1}^{n} x_i (y_i - \\mu_i)\n$$\nThe score equations are found by setting these partial derivatives to zero. Let $\\widehat{\\alpha}$ and $\\widehat{\\beta}$ denote the maximum likelihood estimators (MLEs). The score equations are:\n$$\n\\sum_{i=1}^{n} (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta} x_i)) = 0\n$$\n$$\n\\sum_{i=1}^{n} x_i (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta} x_i)) = 0\n$$\n\n### 2. Derivation of Closed-Form MLEs\n\nThe second task is to find closed-form expressions for $\\widehat{\\alpha}$ and $\\widehat{\\beta}$ for the specific case where $x_i$ is a binary predictor, $x_i \\in \\{0, 1\\}$. We partition the sum in the score equations into two groups: those where $x_i=0$ (Habitat A) and those where $x_i=1$ (Habitat B).\n\nLet $I_0 = \\{i | x_i = 0\\}$ and $I_1 = \\{i | x_i = 1\\}$.\n\nThe first score equation becomes:\n$$\n\\sum_{i \\in I_0} (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta} \\cdot 0)) + \\sum_{i \\in I_1} (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta} \\cdot 1)) = 0\n$$\n$$\n\\implies \\sum_{i \\in I_0} (Y_i - E_i \\exp(\\widehat{\\alpha})) + \\sum_{i \\in I_1} (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta})) = 0\n$$\nThe second score equation involves the term $x_i$, which is zero for the first group:\n$$\n\\sum_{i \\in I_0} 0 \\cdot (Y_i - E_i \\exp(\\widehat{\\alpha})) + \\sum_{i \\in I_1} 1 \\cdot (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta})) = 0\n$$\n$$\n\\implies \\sum_{i \\in I_1} (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta})) = 0\n$$\nFrom this second, simplified equation, we can solve for the term $\\exp(\\widehat{\\alpha} + \\widehat{\\beta})$:\n$$\n\\sum_{i \\in I_1} Y_i = \\sum_{i \\in I_1} E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta}) = \\exp(\\widehat{\\alpha} + \\widehat{\\beta}) \\sum_{i \\in I_1} E_i\n$$\n$$\n\\implies \\exp(\\widehat{\\alpha} + \\widehat{\\beta}) = \\frac{\\sum_{i \\in I_1} Y_i}{\\sum_{i \\in I_1} E_i}\n$$\nNow substitute the result from the second equation back into the first equation. Notice that the second part of the first equation, $\\sum_{i \\in I_1} (Y_i - E_i \\exp(\\widehat{\\alpha} + \\widehat{\\beta}))$, is exactly what the second score equation sets to zero. Thus, the first equation simplifies to:\n$$\n\\sum_{i \\in I_0} (Y_i - E_i \\exp(\\widehat{\\alpha})) = 0\n$$\nSolving for $\\exp(\\widehat{\\alpha})$:\n$$\n\\sum_{i \\in I_0} Y_i = \\sum_{i \\in I_0} E_i \\exp(\\widehat{\\alpha}) = \\exp(\\widehat{\\alpha}) \\sum_{i \\in I_0} E_i\n$$\n$$\n\\implies \\exp(\\widehat{\\alpha}) = \\frac{\\sum_{i \\in I_0} Y_i}{\\sum_{i \\in I_0} E_i}\n$$\nTaking the natural logarithm of both expressions provides the closed-form MLEs. Let $Y_A = \\sum_{i \\in I_0} Y_i$, $E_A = \\sum_{i \\in I_0} E_i$, $Y_B = \\sum_{i \\in I_1} Y_i$, and $E_B = \\sum_{i \\in I_1} E_i$.\nThe MLE for $\\alpha$ is:\n$$\n\\widehat{\\alpha} = \\ln\\left( \\frac{\\sum_{i: x_i=0} Y_i}{\\sum_{i: x_i=0} E_i} \\right) = \\ln\\left( \\frac{Y_A}{E_A} \\right)\n$$\nThis represents the log of the background incidence rate in Habitat A.\nWe also have:\n$$\n\\widehat{\\alpha} + \\widehat{\\beta} = \\ln\\left( \\frac{\\sum_{i: x_i=1} Y_i}{\\sum_{i: x_i=1} E_i} \\right) = \\ln\\left( \\frac{Y_B}{E_B} \\right)\n$$\nThe MLE for $\\beta$ is found by subtraction:\n$$\n\\widehat{\\beta} = (\\widehat{\\alpha} + \\widehat{\\beta}) - \\widehat{\\alpha} = \\ln\\left( \\frac{Y_B}{E_B} \\right) - \\ln\\left( \\frac{Y_A}{E_A} \\right) = \\ln\\left( \\frac{Y_B/E_B}{Y_A/E_A} \\right)\n$$\nThis is the log of the ratio of the incidence rates between Habitat B and Habitat A, also known as the log-rate-ratio.\n\n### 3. Computation of the MLE for $\\beta$\n\nThe final task is to compute the numerical value for $\\widehat{\\beta}$ using the provided data.\n\nData for Habitat A ($x_i=0$): $(E_i, Y_i) \\in \\{(1.2,3), (0.8,1), (1.5,2), (0.5,0)\\}$.\n- Total observed counts in Habitat A: $Y_A = 3 + 1 + 2 + 0 = 6$.\n- Total observation effort in Habitat A: $E_A = 1.2 + 0.8 + 1.5 + 0.5 = 4.0$.\n\nData for Habitat B ($x_i=1$): $(E_i, Y_i) \\in \\{(1.0,5), (2.0,9), (1.3,4), (0.7,3)\\}$.\n- Total observed counts in Habitat B: $Y_B = 5 + 9 + 4 + 3 = 21$.\n- Total observation effort in Habitat B: $E_B = 1.0 + 2.0 + 1.3 + 0.7 = 5.0$.\n\nNow we calculate the estimated incidence rates for each habitat.\n- Estimated incidence rate for Habitat A: $\\frac{Y_A}{E_A} = \\frac{6}{4.0} = 1.5$.\n- Estimated incidence rate for Habitat B: $\\frac{Y_B}{E_B} = \\frac{21}{5.0} = 4.2$.\n\nUsing the derived formula for $\\widehat{\\beta}$:\n$$\n\\widehat{\\beta} = \\ln\\left( \\frac{Y_B/E_B}{Y_A/E_A} \\right) = \\ln\\left( \\frac{4.2}{1.5} \\right) = \\ln(2.8)\n$$\nCalculating the numerical value:\n$$\n\\widehat{\\beta} \\approx 1.029619417...\n$$\nRounding to four significant figures, we note the fifth significant digit is $6$, so we round the fourth digit ($9$) up. This causes a carry-over.\n$$\n\\widehat{\\beta} \\approx 1.030\n$$\nThe trailing zero is significant and must be included.", "answer": "$$\\boxed{1.030}$$", "id": "4960760"}, {"introduction": "A key assumption of the Poisson distribution is that the mean equals the variance. However, biological data often violate this, exhibiting 'overdispersion' where the variance is much larger than the mean. This exercise challenges you to explore two distinct mechanistic origins of overdispersion by comparing a compound Poisson process with a mixed Poisson process, helping you build the intuition needed to select more sophisticated and biologically plausible models [@problem_id:4960763].", "problem": "A biostatistician studies parasite burdens in fish. Two mechanistic hypotheses are under consideration for the count of parasites recovered per fish in a standardized gill-wash assay.\n\nHypothesis 1 (compound Poisson sum of cluster sizes): Parasite colonization occurs as discrete deposition events. The number of deposition events per fish, denoted by $N$, follows a Poisson distribution with rate parameter $\\lambda$. Given $N$, each event contributes a random cluster size $Y$ of parasites, and the total burden is $S = \\sum_{i=1}^{N} Y_i$, where the event sizes $(Y_i)$ are independent and identically distributed (i.i.d.) and independent of $N$. In a particular environment, colonization events occur with rate $\\lambda = 5$, and the event-size distribution has mean $E(Y) = 3$ and variance $\\operatorname{Var}(Y) = 2$.\n\nHypothesis 2 (mixed Poisson counts due to rate heterogeneity): Individual parasites arrive according to a Poisson process with a host-specific rate $\\Lambda$ that varies across fish due to biological heterogeneity. Conditional on $\\Lambda$, the parasite count is $N \\mid \\Lambda \\sim \\text{Poisson}(\\Lambda)$. Across fish, the rate has mean $E(\\Lambda) = \\mu$ and variance $\\operatorname{Var}(\\Lambda) = \\sigma^2$. In a comparable setting, host-specific rates satisfy $E(\\Lambda) = \\mu = 15$ and $\\operatorname{Var}(\\Lambda) = \\sigma^2 = 40$.\n\nWorking from the definitions of the Poisson process, the independence structure, and the laws of total expectation and total variance (without invoking shortcut formulas), determine which statements below are correct.\n\nA. Under Hypothesis $1$ with $\\lambda = 5$, $E(S) = 15$ and $\\operatorname{Var}(S) = 55$, and the excess variance beyond the mean reflects randomness in event sizes $Y$ in addition to the Poisson counting variation.\n\nB. Under Hypothesis $2$ with $E(\\Lambda) = 15$ and $\\operatorname{Var}(\\Lambda) = 40$, $E(N) = 15$ and $\\operatorname{Var}(N) = 55$, and the excess variance beyond the mean reflects heterogeneity in the arrival rate $\\Lambda$ across hosts.\n\nC. Because Hypotheses $1$ and $2$ yield the same mean and variance in this numerical example, they necessarily imply the same full distribution for counts and are therefore mechanistically indistinguishable from any dataset.\n\nD. If event size were deterministic with $Y \\equiv 1$ in Hypothesis $1$, then $S$ would have variance equal to its mean ($\\operatorname{Var}(S) = E(S)$), eliminating overdispersion unless rate heterogeneity is introduced.\n\nE. In Hypothesis $1$ with $N \\sim \\text{Poisson}(\\lambda)$ and independent i.i.d. event sizes $Y$, the variance of $S$ is simply $\\lambda \\operatorname{Var}(Y)$; any contribution involving $[E(Y)]^2$ occurs only in mixtures of Poisson rates.", "solution": "The user requires a critical validation of the problem statement, followed by a full derivation and evaluation of the provided options, adhering to strict formatting rules.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem statement provides two hypotheses for parasite counts in fish.\n\n**Hypothesis 1 (Compound Poisson):**\n-   The number of deposition events, $N$, follows a Poisson distribution: $N \\sim \\text{Poisson}(\\lambda)$.\n-   The total parasite burden is a sum of cluster sizes: $S = \\sum_{i=1}^{N} Y_i$.\n-   The cluster sizes, $Y_i$, are independent and identically distributed (i.i.d.) random variables.\n-   The cluster sizes $Y_i$ are independent of the number of events $N$.\n-   Parameter values: $\\lambda = 5$, $E(Y) = 3$, $\\operatorname{Var}(Y) = 2$.\n\n**Hypothesis 2 (Mixed Poisson):**\n-   The parasite count $N$, conditional on a host-specific rate $\\Lambda$, follows a Poisson distribution: $N \\mid \\Lambda \\sim \\text{Poisson}(\\Lambda)$.\n-   The rate $\\Lambda$ is a random variable that varies across fish.\n-   The moments of the rate distribution are given: $E(\\Lambda) = \\mu = 15$, $\\operatorname{Var}(\\Lambda) = \\sigma^2 = 40$.\n\n**Instruction:**\n-   Derivations must be from first principles: laws of total expectation and total variance. Shortcut formulas are not to be invoked directly.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is scientifically sound. Both the compound Poisson process and the mixed Poisson process are standard, widely used stochastic models in biostatistics and ecology to describe count data that exhibit overdispersion (variance greater than the mean), a common phenomenon in biological counts like parasite loads. The scenario is realistic.\n2.  **Well-Posed:** The problem is well-posed. It provides two clearly defined stochastic models with all necessary parameters to calculate the first two moments (mean and variance) of the resulting count distributions. The question asks for calculations and evaluation of statements based on these models, which is a solvable task with a unique answer.\n3.  **Objective:** The problem is stated in precise, objective mathematical and statistical terms. There is no subjective language.\n4.  **Complete and Consistent:** The setup is complete. All parameters needed for the required calculations ($\\lambda$, $E(Y)$, $\\operatorname{Var}(Y)$ for Hypothesis $1$; $E(\\Lambda)$, $\\operatorname{Var}(\\Lambda)$ for Hypothesis $2$) are provided. There are no contradictions in the given information.\n5.  **Unrealistic or Infeasible:** The parameter values are plausible for a biological study and are mathematically consistent.\n6.  **Trivial or Tautological:** The problem is non-trivial. It requires the correct application of the law of total expectation and the law of total variance to two different, nuanced stochastic models and an understanding of the sources of variance in each.\n7.  **No other flaws identified.**\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The solution will proceed with the derivation and option analysis.\n\n### Derivations from First Principles\n\n**Hypothesis 1: Compound Poisson Sum ($S = \\sum_{i=1}^{N} Y_i$)**\n\nThe problem requires deriving $E(S)$ and $\\operatorname{Var}(S)$ using the laws of total expectation and variance.\n\n**1. Mean of S, $E(S)$:**\nUsing the law of total expectation: $E(S) = E[E(S \\mid N)]$.\nFirst, we find the conditional expectation of $S$ given $N=n$:\n$E(S \\mid N=n) = E\\left[\\sum_{i=1}^{n} Y_i \\mid N=n\\right]$.\nSince the $Y_i$ are independent of $N$, this simplifies to $E\\left[\\sum_{i=1}^{n} Y_i\\right]$.\nBy linearity of expectation and because the $Y_i$ are identically distributed, we have:\n$E\\left[\\sum_{i=1}^{n} Y_i\\right] = \\sum_{i=1}^{n} E(Y_i) = n E(Y)$.\nSo, the random variable $E(S \\mid N)$ is $N E(Y)$.\nNow, we take the expectation over $N$:\n$E(S) = E[N E(Y)] = E(N) E(Y)$.\nSince $N \\sim \\text{Poisson}(\\lambda)$, its mean is $E(N) = \\lambda$.\nTherefore, $E(S) = \\lambda E(Y)$.\n\n**2. Variance of S, $\\operatorname{Var}(S)$:**\nUsing the law of total variance: $\\operatorname{Var}(S) = E[\\operatorname{Var}(S \\mid N)] + \\operatorname{Var}[E(S \\mid N)]$.\nWe evaluate each term:\n-   **First term: $E[\\operatorname{Var}(S \\mid N)]$**\n    We need the conditional variance of $S$ given $N=n$:\n    $\\operatorname{Var}(S \\mid N=n) = \\operatorname{Var}\\left[\\sum_{i=1}^{n} Y_i \\mid N=n\\right]$.\n    Since $Y_i$ are independent of $N$ and are i.i.d., the variance of the sum is the sum of the variances:\n    $\\operatorname{Var}\\left[\\sum_{i=1}^{n} Y_i\\right] = \\sum_{i=1}^{n} \\operatorname{Var}(Y_i) = n \\operatorname{Var}(Y)$.\n    So, the random variable $\\operatorname{Var(S \\mid N)}$ is $N \\operatorname{Var}(Y)$.\n    Taking the expectation over $N$: $E[N \\operatorname{Var}(Y)] = E(N) \\operatorname{Var}(Y) = \\lambda \\operatorname{Var}(Y)$.\n-   **Second term: $\\operatorname{Var}[E(S \\mid N)]$**\n    From the mean calculation, we know $E(S \\mid N) = N E(Y)$.\n    We need the variance of this random variable:\n    $\\operatorname{Var}[N E(Y)] = [E(Y)]^2 \\operatorname{Var}(N)$.\n    Since $N \\sim \\text{Poisson}(\\lambda)$, its variance is $\\operatorname{Var}(N) = \\lambda$.\n    So, $\\operatorname{Var}[E(S \\mid N)] = [E(Y)]^2 \\lambda$.\nCombining the two terms:\n$\\operatorname{Var}(S) = \\lambda \\operatorname{Var}(Y) + \\lambda [E(Y)]^2 = \\lambda (\\operatorname{Var}(Y) + [E(Y)]^2)$.\n\n**Hypothesis 2: Mixed Poisson Count ($N$)**\n\nThe problem requires deriving $E(N)$ and $\\operatorname{Var}(N)$ using the laws of total expectation and variance.\n\n**1. Mean of N, $E(N)$:**\nUsing the law of total expectation: $E(N) = E[E(N \\mid \\Lambda)]$.\nFor a Poisson distribution with rate $\\Lambda$, the conditional mean is $E(N \\mid \\Lambda) = \\Lambda$.\nTaking the expectation over the distribution of $\\Lambda$:\n$E(N) = E[\\Lambda]$.\nThe problem defines $E(\\Lambda) = \\mu$. So, $E(N) = \\mu$.\n\n**2. Variance of N, $\\operatorname{Var}(N)$:**\nUsing the law of total variance: $\\operatorname{Var}(N) = E[\\operatorname{Var}(N \\mid \\Lambda)] + \\operatorname{Var}[E(N \\mid \\Lambda)]$.\nWe evaluate each term:\n-   **First term: $E[\\operatorname{Var}(N \\mid \\Lambda)]$**\n    For a Poisson distribution with rate $\\Lambda$, the conditional variance is equal to the mean: $\\operatorname{Var}(N \\mid \\Lambda) = \\Lambda$.\n    Taking the expectation over the distribution of $\\Lambda$: $E[\\Lambda] = \\mu$.\n-   **Second term: $\\operatorname{Var}[E(N \\mid \\Lambda)]$**\n    From the mean calculation, we know $E(N \\mid \\Lambda) = \\Lambda$.\n    The variance of this random variable is simply $\\operatorname{Var}(\\Lambda)$.\n    The problem defines $\\operatorname{Var}(\\Lambda) = \\sigma^2$.\nCombining the two terms:\n$\\operatorname{Var}(N) = \\mu + \\sigma^2$.\n\n### Numerical Calculations and Option Analysis\n\n**Applying values for Hypothesis 1:**\n$\\lambda = 5$, $E(Y) = 3$, $\\operatorname{Var}(Y) = 2$.\n$E(S) = \\lambda E(Y) = 5 \\times 3 = 15$.\n$\\operatorname{Var}(S) = \\lambda (\\operatorname{Var}(Y) + [E(Y)]^2) = 5 \\times (2 + 3^2) = 5 \\times (2+9) = 5 \\times 11 = 55$.\n\n**Applying values for Hypothesis 2:**\n$\\mu = 15$, $\\sigma^2 = 40$.\n$E(N) = \\mu = 15$.\n$\\operatorname{Var}(N) = \\mu + \\sigma^2 = 15 + 40 = 55$.\n\n**Option-by-Option Analysis:**\n\n**A. Under Hypothesis $1$ with $\\lambda = 5$, $E(S) = 15$ and $\\operatorname{Var}(S) = 55$, and the excess variance beyond the mean reflects randomness in event sizes $Y$ in addition to the Poisson counting variation.**\nOur calculations confirm $E(S) = 15$ and $\\operatorname{Var}(S) = 55$. The variance is greater than the mean ($55  15$), a condition known as overdispersion. The variance formula $\\operatorname{Var}(S) = \\lambda \\operatorname{Var}(Y) + \\lambda [E(Y)]^2$ shows two sources of variation. The term $\\lambda [E(Y)]^2$ can be interpreted as variance arising from the random number of events ($N$), scaled by the squared mean event size. The term $\\lambda \\operatorname{Var}(Y)$ is variance arising from the variability of event sizes themselves. The statement correctly identifies that the total variance, and thus the overdispersion, is due to randomness in $Y$ (both its mean being greater than $1$ and its own variance) acting on top of the fundamental randomness of the Poisson event counter $N$. The statement is an accurate qualitative description.\n**Verdict: Correct.**\n\n**B. Under Hypothesis $2$ with $E(\\Lambda) = 15$ and $\\operatorname{Var}(\\Lambda) = 40$, $E(N) = 15$ and $\\operatorname{Var}(N) = 55$, and the excess variance beyond the mean reflects heterogeneity in the arrival rate $\\Lambda$ across hosts.**\nOur calculations confirm $E(N) = 15$ and $\\operatorname{Var}(N) = 55$. The excess variance (or overdispersion) is $\\operatorname{Var}(N) - E(N) = 55 - 15 = 40$. Our derived formula for variance is $\\operatorname{Var}(N) = E(\\Lambda) + \\operatorname{Var}(\\Lambda)$. Thus, the overdispersion is $\\operatorname{Var}(N) - E(N) = \\operatorname{Var}(\\Lambda) = \\sigma^2$. The value given for $\\sigma^2$ is $40$. The statement that the excess variance reflects heterogeneity in the arrival rate $\\Lambda$ is precisely correct; the overdispersion is mathematically identical to the variance of the rate parameter $\\Lambda$.\n**Verdict: Correct.**\n\n**C. Because Hypotheses $1$ and $2$ yield the same mean and variance in this numerical example, they necessarily imply the same full distribution for counts and are therefore mechanistically indistinguishable from any dataset.**\nThis statement is incorrect. A probability distribution is not uniquely defined by its first two moments (mean and variance). While some specific choices of sub-distributions (e.g., a Logarithmic series for $Y$ in H1 and a Gamma for $\\Lambda$ in H2) can both lead to a Negative Binomial distribution, this is not a general rule. In general, the full distributions will be different. For example, we can compare the probability of observing a zero count.\n- Under H1: $P(S=0) = P(N=0) = e^{-\\lambda} = e^{-5} \\approx 0.0067$.\n- Under H2: $P(N=0) = E[P(N=0 \\mid \\Lambda)] = E[e^{-\\Lambda}]$. By Jensen's inequality, for the convex function $f(x)=e^{-x}$, we have $E[e^{-\\Lambda}] \\ge e^{-E[\\Lambda]} = e^{-15}$. The inequality is strict if $\\Lambda$ is not a constant.\nThe probabilities of zero counts are different ($e^{-5} \\neq E[e^{-\\Lambda}]$), so the distributions are not the same. Therefore, the models are, in principle, distinguishable with sufficient data.\n**Verdict: Incorrect.**\n\n**D. If event size were deterministic with $Y \\equiv 1$ in Hypothesis $1$, then $S$ would have variance equal to its mean ($\\operatorname{Var}(S) = E(S)$), eliminating overdispersion unless rate heterogeneity is introduced.**\nIf $Y_i \\equiv 1$ for all $i$, then $E(Y)=1$ and $\\operatorname{Var}(Y)=0$. The total burden $S$ becomes $S = \\sum_{i=1}^{N} 1 = N$. Since $N \\sim \\text{Poisson}(\\lambda)$, it follows that $S \\sim \\text{Poisson}(\\lambda)$. For a Poisson distribution, the mean is $E(S) = \\lambda$ and the variance is $\\operatorname{Var}(S) = \\lambda$. Thus, $\\operatorname{Var}(S) = E(S)$. This property is called equidispersion. Overdispersion ($\\operatorname{Var}(S)  E(S)$) is indeed eliminated. To re-introduce overdispersion, a different mechanism would be needed, such as the rate heterogeneity described in Hypothesis 2. The statement is entirely correct.\n**Verdict: Correct.**\n\n**E. In Hypothesis $1$ with $N \\sim \\text{Poisson}(\\lambda)$ and independent i.i.d. event sizes $Y$, the variance of $S$ is simply $\\lambda \\operatorname{Var}(Y)$; any contribution involving $[E(Y)]^2$ occurs only in mixtures of Poisson rates.**\nThis statement is false. As derived from first principles, the variance under Hypothesis 1 is $\\operatorname{Var}(S) = \\lambda \\operatorname{Var}(Y) + \\lambda [E(Y)]^2$. The statement incorrectly omits the second term, $\\lambda [E(Y)]^2$. This term arises directly from the compound Poisson model structure, specifically from the variance of the conditional expectation, $\\operatorname{Var}[E(S \\mid N)]$. The claim that such a term only occurs in mixtures of Poisson rates is also false; this model is a compounding model, not a mixture model in the sense of Hypothesis 2.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABD}$$", "id": "4960763"}, {"introduction": "Another common feature of biological count data is an excess of zero values, more than would be predicted by a standard Poisson model. This phenomenon can occur if some subjects are truly immune or unexposed ('structural zeros'), while others simply had no events by chance ('sampling zeros'). This advanced practice introduces the Zero-Inflated Poisson (ZIP) model, a powerful tool for handling such data, and provides a look into the Expectation-Maximization (EM) algorithm used to fit these complex mixture models [@problem_id:4960764].", "problem": "An ecologist studies microscopic counts of parasite eggs per field-of-view in fecal samples from a host population. Many fields show zero eggs either because some hosts are truly uninfected (structural zeros) or because infected hosts occasionally yield zero eggs in a given field-of-view due to sampling variability (sampling zeros). To reflect this biological mechanism, model the count for field $i$ as a zero-inflated Poisson (ZIP) random variable: with probability $p$ the count is deterministically $0$ (structural zero), and with probability $1-p$ the count is drawn from a Poisson distribution with mean $\\lambda0$. Assume fields are independent and identically distributed and denote the observed counts by $\\{y_i\\}_{i=1}^{n}$, where each $y_i \\in \\{0,1,2,\\dots\\}$. Let $\\theta=(p,\\lambda)$ denote the parameter vector, with $p \\in (0,1)$ and $\\lambda \\in (0,\\infty)$.\n\nUsing only foundational definitions for mixture models and the probability mass function of the Poisson distribution, and the general construction of the Expectation-Maximization (EM) algorithm (Expectation-Maximization (EM) is an iterative procedure that alternates between computing the conditional expectation of the complete-data log-likelihood given the observed data and current parameters, and maximizing that expectation over the parameter space), complete the following:\n\n- Derive the observed-data likelihood $L(\\theta;\\{y_i\\}_{i=1}^{n})$ for the ZIP model in terms of $p$, $\\lambda$, and $\\{y_i\\}$. Your derivation must start from the independence assumption and the mixture interpretation, and must employ the Poisson probability mass function, $\\mathbb{P}(Y=y)=\\exp(-\\lambda)\\lambda^{y}/y!$ for $y\\in\\{0,1,2,\\dots\\}$.\n- Introduce a latent indicator $Z_i \\in \\{0,1\\}$ that encodes whether $y_i$ arose from the structural-zero component ($Z_i=1$) or from the Poisson component ($Z_i=0$). Write the complete-data log-likelihood $\\ln L_{\\mathrm{c}}(\\theta;\\{y_i,Z_i\\}_{i=1}^{n})$ and then derive the EM algorithm’s E-step by computing the conditional expectation $\\mathbb{E}[Z_i \\mid y_i,\\theta^{(t)}]$ in closed form as a function of $y_i$, $p^{(t)}$, and $\\lambda^{(t)}$.\n- Derive the EM algorithm’s M-step updates for $p$ and $\\lambda$ by maximizing the expected complete-data log-likelihood with respect to $p$ and $\\lambda$, subject to $p \\in (0,1)$ and $\\lambda0$.\n- State and justify conditions under which the observed-data likelihood $L(\\theta;\\{y_i\\})$ is guaranteed to be monotonically nondecreasing across EM iterations, i.e., $L(\\theta^{(t+1)};\\{y_i\\}) \\geq L(\\theta^{(t)};\\{y_i\\})$, explicitly connecting your justification to properties of the E-step and M-step.\n\nExpress your final answer as the two explicit analytic update formulas for $p^{(t+1)}$ and $\\lambda^{(t+1)}$ in terms of $n$, the data $\\{y_i\\}_{i=1}^{n}$, and the current parameters $p^{(t)}$ and $\\lambda^{(t)}$. Present these two expressions together as a single row matrix. No numerical evaluation is required, and no units are associated with the parameters or counts. Do not round; provide exact analytic expressions.", "solution": "The problem is valid as it is scientifically grounded in biostatistics, well-posed, objective, and contains a complete and consistent setup for deriving the Expectation-Maximization (EM) algorithm for a Zero-Inflated Poisson (ZIP) model. All necessary definitions and assumptions are provided. I will now proceed with the full derivation.\n\nThe problem asks for four main derivations concerning the Zero-Inflated Poisson (ZIP) model and the EM algorithm, followed by the final update equations.\n\n**1. Derivation of the Observed-Data Likelihood**\n\nThe ZIP model specifies that an observation $Y_i$ is a result of a two-stage process. First, a trial with probability $p$ leads to a structural zero, and with probability $1-p$ leads to a draw from a Poisson distribution with mean $\\lambda$. The probability mass function (PMF) of the Poisson distribution is given as $\\mathbb{P}(Y=y) = \\frac{\\exp(-\\lambda)\\lambda^{y}}{y!}$ for $y \\in \\{0, 1, 2, \\dots\\}$.\n\nLet's derive the PMF for a single ZIP random variable $Y_i$. We consider two cases for the observed count $y_i$:\n\nCase 1: $y_i  0$. An observation greater than zero can only arise from the Poisson component of the mixture. This happens with probability $1-p$. The conditional probability of observing $y_i$ given it came from the Poisson component is $\\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!}$. Therefore, the total probability is:\n$$ \\mathbb{P}(Y_i = y_i) = (1-p) \\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!}, \\quad \\text{for } y_i \\in \\{1, 2, 3, \\dots\\} $$\n\nCase 2: $y_i = 0$. An observation of zero can arise in two mutually exclusive ways:\n- It is a structural zero, which occurs with probability $p$.\n- It is a sampling zero from the Poisson component. This occurs with probability $1-p$, and the probability of a zero from a Poisson($\\lambda$) distribution is $\\mathbb{P}(Y=0) = \\frac{\\exp(-\\lambda)\\lambda^0}{0!} = \\exp(-\\lambda)$.\nThe total probability for an observation of zero is the sum of probabilities of these two events:\n$$ \\mathbb{P}(Y_i = 0) = p + (1-p)\\exp(-\\lambda) $$\n\nWe can combine these two cases into a single expression using the indicator function $I(y_i=0)$, which is $1$ if $y_i=0$ and $0$ otherwise.\n$$ \\mathbb{P}(Y_i = y_i; \\theta) = \\left[p + (1-p)\\exp(-\\lambda)\\right]^{I(y_i=0)} \\left[(1-p)\\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!}\\right]^{1-I(y_i=0)} $$\nThe observed-data likelihood function $L(\\theta; \\{y_i\\}_{i=1}^{n})$ is the product of the probabilities of each independent observation, given the parameters $\\theta=(p, \\lambda)$:\n$$ L(\\theta; \\{y_i\\}_{i=1}^{n}) = \\prod_{i=1}^{n} \\mathbb{P}(Y_i = y_i; \\theta) $$\nLet $n_0$ be the number of observations where $y_i=0$ (i.e., $n_0 = \\sum_{i=1}^n I(y_i=0)$) and $n_{0}$ be the number of observations where $y_i0$. The likelihood can be written by separating the product over zero and non-zero counts:\n$$ L(\\theta; \\{y_i\\}_{i=1}^{n}) = \\left(\\prod_{i:y_i=0} [p + (1-p)\\exp(-\\lambda)]\\right) \\left(\\prod_{i:y_i0} \\left[(1-p)\\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!}\\right]\\right) $$\n$$ L(\\theta; \\{y_i\\}_{i=1}^{n}) = [p + (1-p)\\exp(-\\lambda)]^{n_0} \\prod_{i:y_i0} \\left((1-p)\\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!}\\right) $$\nThis is the observed-data likelihood function for the ZIP model.\n\n**2. Complete-Data Log-Likelihood and E-Step Derivation**\n\nThe EM algorithm simplifies maximization by introducing latent variables. Let $Z_i \\in \\{0, 1\\}$ be a latent indicator for each observation $y_i$.\n- $Z_i = 1$: $y_i$ is a structural zero (from the zero-inflation component). This occurs with prior probability $\\mathbb{P}(Z_i=1)=p$. If $Z_i=1$, then $y_i$ must be $0$.\n- $Z_i = 0$: $y_i$ is from the Poisson component. This occurs with prior probability $\\mathbb{P}(Z_i=0)=1-p$. If $Z_i=0$, then $y_i \\sim \\text{Poisson}(\\lambda)$.\n\nThe complete data for observation $i$ is $(y_i, Z_i)$. The joint probability mass function $p(y_i, z_i; \\theta)$ is:\n- If $z_i=1$: The count must be $0$. $\\mathbb{P}(y_i, Z_i=1) = \\mathbb{P}(y_i|Z_i=1)\\mathbb{P}(Z_i=1) = I(y_i=0) \\cdot p$.\n- If $z_i=0$: The count follows a Poisson distribution. $\\mathbb{P}(y_i, Z_i=0) = \\mathbb{P}(y_i|Z_i=0)\\mathbb{P}(Z_i=0) = \\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!} \\cdot (1-p)$.\n\nThe complete-data likelihood for the entire dataset $\\{y_i, Z_i\\}_{i=1}^{n}$ is:\n$$ L_{\\mathrm{c}}(\\theta; \\{y_i, Z_i\\}) = \\prod_{i=1}^{n} [p \\cdot I(y_i=0)]^{Z_i} \\left[ (1-p)\\frac{\\exp(-\\lambda)\\lambda^{y_i}}{y_i!} \\right]^{1-Z_i} $$\nThe complete-data log-likelihood $\\ln L_{\\mathrm{c}}(\\theta)$ is:\n$$ \\ln L_{\\mathrm{c}}(\\theta) = \\sum_{i=1}^{n} \\left\\{ Z_i \\ln(p) + Z_i\\ln(I(y_i=0)) + (1-Z_i)\\left[\\ln(1-p) - \\lambda + y_i \\ln(\\lambda) - \\ln(y_i!)\\right] \\right\\} $$\nThe term $Z_i\\ln(I(y_i=0))$ is $0$ if $y_i=0$ (since $\\ln(1)=0$). If $y_i0$, $Z_i$ must be $0$ for the likelihood to be non-zero, so the term is also effectively zero. We can write the log-likelihood that depends on the parameters as:\n$$ \\ln L_{\\mathrm{c}}(\\theta) \\propto \\sum_{i=1}^{n} \\left\\{ Z_i \\ln(p) + (1-Z_i)\\ln(1-p) + (1-Z_i)[y_i \\ln(\\lambda) - \\lambda] \\right\\} $$\n\n**E-Step (Expectation):** In the E-step, we compute the expectation of the complete-data log-likelihood with respect to the conditional distribution of the latent variables given the observed data and the current parameter estimates $\\theta^{(t)}=(p^{(t)}, \\lambda^{(t)})$. This is equivalent to computing the conditional expectation of each $Z_i$. Let $w_i^{(t)} = \\mathbb{E}[Z_i \\mid y_i, \\theta^{(t)}]$.\n\n- If $y_i  0$: The observation cannot be a structural zero. Thus, $Z_i$ must be $0$.\n$$ w_i^{(t)} = \\mathbb{E}[Z_i \\mid y_i  0, \\theta^{(t)}] = \\mathbb{P}(Z_i=1 \\mid y_i0, \\theta^{(t)}) = 0 $$\n- If $y_i = 0$: The observation could be a structural zero ($Z_i=1$) or a sampling zero ($Z_i=0$). We use Bayes' theorem:\n$$ w_i^{(t)} = \\mathbb{P}(Z_i=1 \\mid y_i=0, \\theta^{(t)}) = \\frac{\\mathbb{P}(y_i=0 \\mid Z_i=1, \\theta^{(t)})\\mathbb{P}(Z_i=1 \\mid \\theta^{(t)})}{\\mathbb{P}(y_i=0 \\mid \\theta^{(t)})} $$\nThe components are:\n- $\\mathbb{P}(y_i=0 \\mid Z_i=1, \\theta^{(t)}) = 1$.\n- $\\mathbb{P}(Z_i=1 \\mid \\theta^{(t)}) = p^{(t)}$.\n- $\\mathbb{P}(y_i=0 \\mid \\theta^{(t)}) = p^{(t)} + (1-p^{(t)})\\exp(-\\lambda^{(t)})$.\nThus, for $y_i=0$:\n$$ w_i^{(t)} = \\frac{1 \\cdot p^{(t)}}{p^{(t)} + (1-p^{(t)})\\exp(-\\lambda^{(t)})} $$\nCombining both cases using the indicator function $I(y_i=0)$:\n$$ w_i^{(t)} = \\mathbb{E}[Z_i \\mid y_i, \\theta^{(t)}] = I(y_i=0) \\frac{p^{(t)}}{p^{(t)} + (1-p^{(t)})\\exp(-\\lambda^{(t)})} $$\nThe expected complete-data log-likelihood, denoted $Q(\\theta \\mid \\theta^{(t)})$, is:\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^{n} \\left\\{ w_i^{(t)} \\ln(p) + (1-w_i^{(t)})\\ln(1-p) + (1-w_i^{(t)})[y_i \\ln(\\lambda) - \\lambda] \\right\\} + \\text{const.} $$\n\n**3. M-Step Derivation**\n\n**M-Step (Maximization):** In the M-step, we find the parameters $\\theta^{(t+1)}=(p^{(t+1)}, \\lambda^{(t+1)})$ that maximize $Q(\\theta \\mid \\theta^{(t)})$. The terms for $p$ and $\\lambda$ in $Q$ are separate, so we can maximize them independently.\n\nMaximization with respect to $p$:\n$$ \\frac{\\partial Q}{\\partial p} = \\sum_{i=1}^n \\left( \\frac{w_i^{(t)}}{p} - \\frac{1-w_i^{(t)}}{1-p} \\right) = 0 $$\n$$ \\frac{1}{p}\\sum_{i=1}^n w_i^{(t)} = \\frac{1}{1-p}\\sum_{i=1}^n (1-w_i^{(t)}) = \\frac{n - \\sum_{i=1}^n w_i^{(t)}}{1-p} $$\n$$ (1-p)\\sum_{i=1}^n w_i^{(t)} = p(n - \\sum_{i=1}^n w_i^{(t)}) $$\n$$ \\sum_{i=1}^n w_i^{(t)} = pn $$\nSolving for $p$ gives the update rule:\n$$ p^{(t+1)} = \\frac{1}{n}\\sum_{i=1}^n w_i^{(t)} $$\n\nMaximization with respect to $\\lambda$:\n$$ \\frac{\\partial Q}{\\partial \\lambda} = \\sum_{i=1}^n (1-w_i^{(t)})\\left( \\frac{y_i}{\\lambda} - 1 \\right) = 0 $$\n$$ \\frac{1}{\\lambda}\\sum_{i=1}^n (1-w_i^{(t)})y_i = \\sum_{i=1}^n (1-w_i^{(t)}) $$\nSolving for $\\lambda$ gives the update rule:\n$$ \\lambda^{(t+1)} = \\frac{\\sum_{i=1}^n (1-w_i^{(t)})y_i}{\\sum_{i=1}^n (1-w_i^{(t)})} $$\nWe can simplify the numerator. The term $w_i^{(t)}$ is non-zero only if $y_i=0$. Therefore, the product $w_i^{(t)}y_i$ is always zero for all $i$.\n$$ \\sum_{i=1}^n (1-w_i^{(t)})y_i = \\sum_{i=1}^n y_i - \\sum_{i=1}^n w_i^{(t)}y_i = \\sum_{i=1}^n y_i - 0 = \\sum_{i=1}^n y_i $$\nThe update for $\\lambda$ simplifies to:\n$$ \\lambda^{(t+1)} = \\frac{\\sum_{i=1}^n y_i}{\\sum_{i=1}^n (1-w_i^{(t)})} $$\n\n**4. Justification for Monotonically Nondecreasing Likelihood**\n\nThe observed-data log-likelihood is $\\ln L(\\theta; \\{y_i\\}) = \\ln \\left(\\sum_{\\{Z_i\\}} L_{\\mathrm{c}}(\\theta; \\{y_i, Z_i\\})\\right)$. A fundamental property of the EM algorithm is that the observed-data likelihood is nondecreasing at each iteration, i.e., $L(\\theta^{(t+1)}; \\{y_i\\}) \\geq L(\\theta^{(t)}; \\{y_i\\})$.\n\nThis can be shown by considering the difference in log-likelihoods:\n$$ \\ln L(\\theta^{(t+1)}) - \\ln L(\\theta^{(t)}) = \\left[ Q(\\theta^{(t+1)} \\mid \\theta^{(t)}) - Q(\\theta^{(t)} \\mid \\theta^{(t)}) \\right] + D_{KL}\\left(p(Z|\\{y_i\\}, \\theta^{(t)}) \\parallel p(Z|\\{y_i\\}, \\theta^{(t+1)})\\right) $$\nwhere $Z = \\{Z_i\\}_{i=1}^n$, and $D_{KL}$ is the Kullback-Leibler divergence.\n\nThe guarantee of non-decreasing likelihood relies on two key properties tied to the EM steps:\n1.  **M-step Property**: The M-step is defined to choose $\\theta^{(t+1)}$ to maximize (or at least not decrease) the $Q$ function:\n    $$ \\theta^{(t+1)} = \\arg\\max_{\\theta} Q(\\theta \\mid \\theta^{(t)}) $$\n    By this construction, we are guaranteed that $Q(\\theta^{(t+1)} \\mid \\theta^{(t)}) \\geq Q(\\theta^{(t)} \\mid \\theta^{(t)})$. The first term in the difference equation is therefore non-negative. In our specific derivation, we found a unique global maximum for $Q$ with respect to $p$ and $\\lambda$, so this condition is strictly met.\n2.  **Kullback-Leibler Divergence Property**: The KL divergence is, by Gibbs' inequality, always non-negative. $D_{KL}(P \\parallel Q) \\geq 0$, with equality if and only if $P=Q$. Therefore, the second term in the difference equation is also non-negative.\n\nSince both terms are non-negative, their sum is non-negative, which implies $\\ln L(\\theta^{(t+1)}) \\geq \\ln L(\\theta^{(t)})$. This ensures that the likelihood of the observed data will montonically non-decrease with each EM iteration, converging to a local maximum or saddle point of the likelihood surface. The conditions for this guarantee are the correct execution of the E-step (computing the conditional expectation to form $Q$) and the M-step (maximizing $Q$), both of which have been analytically performed.\n\nFinally, substituting the expression for $w_i^{(t)}$ from the E-step into the M-step update rules yields the final explicit formulas.\nThe update for $p^{(t+1)}$ becomes:\n$$ p^{(t+1)} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(I(y_i=0) \\frac{p^{(t)}}{p^{(t)} + (1-p^{(t)})\\exp(-\\lambda^{(t)})}\\right) $$\nThe update for $\\lambda^{(t+1)}$ becomes:\n$$ \\lambda^{(t+1)} = \\frac{\\sum_{i=1}^{n} y_i}{\\sum_{i=1}^{n} \\left(1 - I(y_i=0) \\frac{p^{(t)}}{p^{(t)} + (1-p^{(t)})\\exp(-\\lambda^{(t)})}\\right)} $$\nThese are the final update formulas required by the problem.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{n} \\sum_{i=1}^{n} \\left(I(y_i=0) \\frac{p^{(t)}}{p^{(t)} + (1-p^{(t)})\\exp(-\\lambda^{(t)})}\\right)  \\frac{\\sum_{i=1}^{n} y_i}{\\sum_{i=1}^{n} \\left( 1 - I(y_i=0) \\frac{p^{(t)}}{p^{(t)} + (1-p^{(t)})\\exp(-\\lambda^{(t)})} \\right)} \\end{pmatrix} } $$", "id": "4960764"}]}