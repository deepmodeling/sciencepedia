{"hands_on_practices": [{"introduction": "A Moment Generating Function (MGF) acts like a unique fingerprint for a probability distribution. By recognizing the specific mathematical form of an MGF, we can immediately identify the underlying distribution and all its properties. This exercise challenges you to identify a familiar discrete distribution from its MGF and then use that knowledge to calculate a specific probability, reinforcing the powerful uniqueness property of MGFs [@problem_id:1937181].", "problem": "Let $X$ be a discrete random variable whose Moment Generating Function (MGF) is given by the expression:\n$$M_X(t) = \\exp(5(\\exp(t)-1))$$\nfor all real numbers $t$ for which the MGF is defined.\n\nCalculate the probability that the random variable $X$ takes the value of 3, i.e., find $P(X=3)$. Express your answer as a single closed-form analytic expression.", "solution": "By definition, the moment generating function (MGF) of a random variable $X$ is $M_{X}(t)=\\mathbb{E}[\\exp(tX)]$. For a discrete random variable supported on the nonnegative integers, we can write\n$$\nM_{X}(t)=\\sum_{n=0}^{\\infty} \\mathbb{P}(X=n)\\exp(t n).\n$$\nThe given MGF is\n$$\nM_{X}(t)=\\exp\\!\\big(5(\\exp(t)-1)\\big)=\\exp(-5)\\exp\\!\\big(5\\exp(t)\\big).\n$$\nExpanding the exponential series,\n$$\n\\exp\\!\\big(5\\exp(t)\\big)=\\sum_{n=0}^{\\infty}\\frac{\\big(5\\exp(t)\\big)^{n}}{n!}=\\sum_{n=0}^{\\infty}\\frac{5^{n}\\exp(t n)}{n!}.\n$$\nTherefore,\n$$\nM_{X}(t)=\\exp(-5)\\sum_{n=0}^{\\infty}\\frac{5^{n}\\exp(t n)}{n!}=\\sum_{n=0}^{\\infty}\\left(\\exp(-5)\\frac{5^{n}}{n!}\\right)\\exp(t n).\n$$\nComparing this with $M_{X}(t)=\\sum_{n=0}^{\\infty}\\mathbb{P}(X=n)\\exp(t n)$ and using the uniqueness of the representation in terms of the linearly independent functions $\\{\\exp(t n)\\}_{n\\geq 0}$, we identify\n$$\n\\mathbb{P}(X=n)=\\exp(-5)\\frac{5^{n}}{n!}.\n$$\nHence,\n$$\n\\mathbb{P}(X=3)=\\exp(-5)\\frac{5^{3}}{3!}.\n$$", "answer": "$$\\boxed{\\frac{5^{3}\\exp(-5)}{3!}}$$", "id": "1937181"}, {"introduction": "The primary power of the MGF lies in its ability to \"generate\" moments of a distribution through differentiation. This practice will guide you through the process of calculating the mean and variance directly from a given MGF. You will also see how these properties behave when the random variable undergoes a simple linear transformation, a common scenario in data standardization and analysis [@problem_id:1937144].", "problem": "Let $Y$ be a continuous random variable. The statistical properties of $Y$ are fully described by its Moment Generating Function (MGF), which is given by the expression:\n$$M_Y(t) = \\exp(3t + 8t^2)$$\nwhere $t$ is a real variable for which the function is defined.\n\nConsider a new random variable $X$ that is defined as a linear transformation of $Y$:\n$$X = 5 - 2Y$$\nCalculate the variance of the random variable $X$.", "solution": "We are given the moment generating function of $Y$ as $M_{Y}(t)=\\exp(3t+8t^{2})$. By definition of the MGF, the first and second moments of $Y$ are obtained from derivatives at $t=0$:\n- $\\mathbb{E}[Y]=M_{Y}'(0)$,\n- $\\mathbb{E}[Y^{2}]=M_{Y}''(0)$,\nand the variance is $\\operatorname{Var}(Y)=M_{Y}''(0)-\\left(M_{Y}'(0)\\right)^{2}$.\n\nDifferentiate $M_{Y}(t)$ using the chain rule. Let $g(t)=\\exp(3t+8t^{2})$. Then\n$$\nM_{Y}'(t)=\\frac{d}{dt}\\exp(3t+8t^{2})=(3+16t)\\exp(3t+8t^{2})=(3+16t)g(t).\n$$\nEvaluating at $t=0$ gives\n$$\nM_{Y}'(0)=(3+0)\\exp(0)=3,\n$$\nso $\\mathbb{E}[Y]=3$.\n\nFor the second derivative, use the product rule with $f(t)=3+16t$ and $g(t)=\\exp(3t+8t^{2})$:\n$$\nM_{Y}''(t)=f'(t)g(t)+f(t)g'(t).\n$$\nWe have $f'(t)=16$ and $g'(t)=(3+16t)g(t)$ by the chain rule, hence\n$$\nM_{Y}''(t)=16\\,g(t)+(3+16t)(3+16t)\\,g(t)=\\left(16+(3+16t)^{2}\\right)\\exp(3t+8t^{2}).\n$$\nEvaluating at $t=0$ yields\n$$\nM_{Y}''(0)=\\left(16+3^{2}\\right)\\exp(0)=25,\n$$\nso $\\mathbb{E}[Y^{2}]=25$. Therefore,\n$$\n\\operatorname{Var}(Y)=\\mathbb{E}[Y^{2}]-\\left(\\mathbb{E}[Y]\\right)^{2}=25-3^{2}=16.\n$$\n\nNow consider $X=5-2Y$. For a linear transformation $X=a+bY$, the variance satisfies $\\operatorname{Var}(X)=b^{2}\\operatorname{Var}(Y)$. Here $b=-2$, so\n$$\n\\operatorname{Var}(X)=(-2)^{2}\\operatorname{Var}(Y)=4\\cdot 16=64.\n$$", "answer": "$$\\boxed{64}$$", "id": "1937144"}, {"introduction": "Theoretical moments have a powerful practical application in estimating model parameters from data, a procedure known as the Method of Moments. In this problem, you will apply this method to a Zero-Inflated Poisson (ZIP) model, a distribution frequently used in biostatistics to handle datasets with an excess of zero counts. This exercise bridges the gap between abstract theory and the concrete task of fitting a statistical model to real-world observations [@problem_id:4929083].", "problem": "A biostatistics research team models the number of parasite eggs per stool sample, $Y$, as a Zero-Inflated Poisson (ZIP) distribution due to an excess of zeros from uninfected individuals. In this ZIP model, with probability $p$ the outcome is a structural zero, and with probability $(1-p)$ the outcome follows a Poisson distribution with rate $\\lambda$. Specifically, for an integer $k \\ge 0$, the probability mass function satisfies\n$$\n\\mathbb{P}(Y=0)=p+(1-p)\\exp(-\\lambda), \\quad \\mathbb{P}(Y=k)=(1-p)\\exp(-\\lambda)\\frac{\\lambda^{k}}{k!}\\ \\text{for}\\ k\\ge 1.\n$$\nSuppose $Y_{1},\\dots,Y_{n}$ are independent and identically distributed draws from this model. Define the first two empirical raw moments\n$$\nm_{1}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}, \\qquad m_{2}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}^{2}.\n$$\nUsing only the first two population raw moments of $Y$ and equating them to $m_{1}$ and $m_{2}$, derive the method of moments estimators $\\hat{p}$ and $\\hat{\\lambda}$ as simplified closed-form expressions in terms of $m_{1}$ and $m_{2}$.\n\nThen, discuss identifiability of $(p,\\lambda)$ when only the first two moments are matched: starting from the defining properties of moments and the model, determine conditions on $(m_{1},m_{2})$ under which the moment equations admit a unique solution with $0\\le p1$ and $\\lambda\\ge 0$, and explain boundary or failure cases. Your final reported estimators must be given as algebraic expressions in terms of $m_{1}$ and $m_{2}$. Do not round.", "solution": "The task is to derive the method of moments estimators for the parameters $p$ and $\\lambda$ of a Zero-Inflated Poisson (ZIP) distribution and to analyze the conditions for the existence and uniqueness of these estimators.\n\nLet $Y$ be a random variable following the specified ZIP distribution with parameters $p$ and $\\lambda$, where $p$ is the probability of a structural zero and $\\lambda$ is the rate of the Poisson component. The probability mass function (PMF) is given by\n$$\n\\mathbb{P}(Y=k) = \\begin{cases} p + (1-p)e^{-\\lambda}  \\text{if } k=0 \\\\ (1-p)e^{-\\lambda}\\frac{\\lambda^k}{k!}  \\text{if } k \\in \\{1, 2, 3, \\dots\\} \\end{cases}\n$$\nThe constraints on the parameters are $0 \\le p  1$ and $\\lambda \\ge 0$.\n\nTo derive the method of moments estimators, we first compute the first two population raw moments of $Y$, denoted $\\mu_1 = \\mathbb{E}[Y]$ and $\\mu_2 = \\mathbb{E}[Y^2]$. A ZIP random variable can be conceptualized as a mixture. With probability $p$, the outcome is $0$. With probability $1-p$, the outcome is a draw from a Poisson($\\lambda$) distribution. Let $X \\sim \\text{Poisson}(\\lambda)$.\n\nUsing the law of total expectation, the first raw moment is:\n$$\n\\mu_1 = \\mathbb{E}[Y] = p \\cdot \\mathbb{E}[Y | \\text{structural zero}] + (1-p) \\cdot \\mathbb{E}[Y | \\text{from Poisson}]\n$$\nThe expected value of a structural zero is $0$. The expected value of a Poisson random variable is its rate parameter $\\lambda$.\n$$\n\\mu_1 = p \\cdot 0 + (1-p) \\cdot \\mathbb{E}[X] = (1-p)\\lambda\n$$\n\nSimilarly, for the second raw moment:\n$$\n\\mu_2 = \\mathbb{E}[Y^2] = p \\cdot \\mathbb{E}[Y^2 | \\text{structural zero}] + (1-p) \\cdot \\mathbb{E}[Y^2 | \\text{from Poisson}]\n$$\nThe expected squared value of a structural zero is $0^2=0$. For the Poisson component $X$, we know its variance is $\\text{Var}(X) = \\lambda$ and its mean is $\\mathbb{E}[X] = \\lambda$. The second moment of $X$ is $\\mathbb{E}[X^2] = \\text{Var}(X) + (\\mathbb{E}[X])^2 = \\lambda + \\lambda^2$.\n$$\n\\mu_2 = p \\cdot 0 + (1-p) \\cdot \\mathbb{E}[X^2] = (1-p)(\\lambda + \\lambda^2)\n$$\n\nThe method of moments consists of equating these population moments to the corresponding empirical raw moments, $m_1$ and $m_2$, derived from the sample $Y_1, \\dots, Y_n$.\nThis gives the following system of equations for the estimators $\\hat{p}$ and $\\hat{\\lambda}$:\n$$\n\\begin{cases}\n(1-\\hat{p})\\hat{\\lambda} = m_1  (1) \\\\\n(1-\\hat{p})(\\hat{\\lambda} + \\hat{\\lambda}^2) = m_2  (2)\n\\end{cases}\n$$\n\nWe solve this system for $\\hat{p}$ and $\\hat{\\lambda}$. We can rewrite equation $(2)$ as:\n$$\n(1-\\hat{p})\\hat{\\lambda} + (1-\\hat{p})\\hat{\\lambda}^2 = m_2\n$$\nSubstituting equation $(1)$ into this expanded form of equation $(2)$:\n$$\nm_1 + (1-\\hat{p})\\hat{\\lambda}^2 = m_2\n$$\nThis gives $(1-\\hat{p})\\hat{\\lambda}^2 = m_2 - m_1$.\nNow, assuming $m_1 \\neq 0$ and $\\hat{\\lambda} \\neq 0$, we can divide this new equation by equation $(1)$:\n$$\n\\frac{(1-\\hat{p})\\hat{\\lambda}^2}{(1-\\hat{p})\\hat{\\lambda}} = \\frac{m_2 - m_1}{m_1}\n$$\n$$\n\\hat{\\lambda} = \\frac{m_2 - m_1}{m_1}\n$$\nNext, we solve for $\\hat{p}$ using equation $(1)$: $1-\\hat{p} = \\frac{m_1}{\\hat{\\lambda}}$.\nSubstituting the expression for $\\hat{\\lambda}$:\n$$\n1-\\hat{p} = \\frac{m_1}{\\frac{m_2 - m_1}{m_1}} = \\frac{m_1^2}{m_2 - m_1}\n$$\n$$\n\\hat{p} = 1 - \\frac{m_1^2}{m_2 - m_1}\n$$\n\nNow, we discuss the identifiability of $(p, \\lambda)$ and the conditions on $(m_1, m_2)$ for which the equations yield a valid and unique solution in the parameter space $0 \\le p  1$ and $\\lambda \\ge 0$.\n\nFirst, consider the denominators in the expressions. We require $m_1 \\neq 0$ and $m_2 - m_1 \\neq 0$.\nIf $m_1 = 0$, then all sample values $Y_i$ must be $0$, which also implies $m_2 = 0$. In this case, the estimators are indeterminate forms $0/0$. The moment equations become $(1-p)\\lambda = 0$ and $(1-p)(\\lambda+\\lambda^2)=0$, which are satisfied by $\\lambda=0$ (for any $p \\in [0,1)$) or by approaching $p=1$. The parameters are not identifiable. This is a failure case.\n\nAssume $m_1  0$.\nThe condition $\\hat{\\lambda} \\ge 0$ requires $\\frac{m_2 - m_1}{m_1} \\ge 0$. Since $m_1  0$, this is equivalent to $m_2 \\ge m_1$.\nLet's examine $m_2 - m_1 = \\frac{1}{n}\\sum_{i=1}^n Y_i^2 - \\frac{1}{n}\\sum_{i=1}^n Y_i = \\frac{1}{n}\\sum_{i=1}^n (Y_i^2 - Y_i)$.\nSince each $Y_i$ is a non-negative integer, $Y_i^2 - Y_i = Y_i(Y_i-1) \\ge 0$. Thus, $m_2 - m_1 \\ge 0$ is always true for count data.\nEquality $m_2=m_1$ holds if and only if all $Y_i$ are either $0$ or $1$. If $m_2=m_1$ and $m_10$, our expression for $\\hat{\\lambda}$ gives $\\hat{\\lambda}=0$, but the expression for $\\hat{p}$ involves division by $m_2-m_1=0$, making it undefined. The system of moment equations is inconsistent in this case, as $(1-\\hat{p})\\hat{\\lambda}=m_1  0$ cannot be satisfied if $\\hat{\\lambda}=0$. This is another failure case, indicating the ZIP model (via method of moments) is not suitable for data containing only zeros and ones.\n\nFor a non-degenerate solution, we require $m_2  m_1$, which implies there is at least one observation $Y_i1$. If this holds, then $\\hat{\\lambda}  0$.\n\nNext, consider the constraints on $\\hat{p}$: $0 \\le \\hat{p}  1$.\nThe condition $\\hat{p}  1$ implies $1 - \\frac{m_1^2}{m_2 - m_1}  1$, which simplifies to $-\\frac{m_1^2}{m_2-m_1}  0$. Since $m_1  0$, $m_1^2  0$. This inequality holds if $m_2 - m_1  0$, which is the same condition for $\\hat{\\lambda}  0$.\n\nThe condition $\\hat{p} \\ge 0$ implies $1 - \\frac{m_1^2}{m_2 - m_1} \\ge 0$, which means $1 \\ge \\frac{m_1^2}{m_2 - m_1}$.\nSince we established $m_2 - m_1  0$, we can write $m_2 - m_1 \\ge m_1^2$.\nRearranging this gives $m_2 - m_1^2 \\ge m_1$.\nThe term $m_2 - m_1^2$ is the biased sample variance of the data. Let's denote it $s^2_b = m_2-m_1^2$. The condition is $s_b^2 \\ge m_1$, meaning the sample variance must be at least as large as the sample mean. This property, known as overdispersion (or equi-dispersion at the boundary), is a characteristic of the ZIP model. The variance of a ZIP distribution is $\\text{Var}(Y) = (1-p)\\lambda + p(1-p)\\lambda^2 = \\mu_1 + \\frac{p}{1-p}\\mu_1^2$, which is always greater than or equal to its mean $\\mu_1$. The method of moments can only yield a valid estimate for $p$ if the sample data exhibit this property.\n\nSummary of conditions on $(m_1, m_2)$:\n1.  **Unique Valid Solution ($0  p  1, \\lambda  0$):** This occurs if the sample data are strictly overdispersed, i.e., $m_2 - m_1^2  m_1$. This single condition implies $m_10$ (as $m_2-m_1^2  m_1 \\ge 0$) and $m_2  m_1$ (since $m_2  m_1^2+m_1  m_1$), ensuring $\\hat{\\lambda}0$ and $\\hat{p} \\in (0,1)$.\n2.  **Boundary Solution (Poisson case, $p=0, \\lambda0$):** This occurs if the sample exhibits equi-dispersion, i.e., $m_2 - m_1^2 = m_1$. In this case, $\\hat{p}=0$ and $\\hat{\\lambda} = \\frac{m_1^2+m_1-m_1}{m_1}=m_1$. The data are consistent with a standard Poisson distribution.\n3.  **Failure Cases (No valid solution):**\n    a. **Underdispersion:** If $m_2 - m_1^2  m_1$, the method yields $\\hat{p}  0$, which is invalid. The ZIP model cannot accommodate underdispersed data.\n    b. **Bernoulli-like Data:** If $m_2 = m_1$ (and $m_10$), the data consist only of $0$s and $1$s. This leads to $\\hat{\\lambda}=0$ from its formula, but this contradicts the first moment equation $m_1 = (1-\\hat{p})\\hat{\\lambda}$ if $m_10$. The system is inconsistent.\n    c. **All Zeros Data:** If $m_1=0$ (and thus $m_2=0$), the estimators are indeterminate, reflecting non-identifiability of the parameters.\n\nThe derived estimators, under the conditions where they are valid, are simplified closed-form expressions in terms of $m_1$ and $m_2$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 1 - \\frac{m_{1}^{2}}{m_{2} - m_{1}}  \\frac{m_{2} - m_{1}}{m_{1}} \\end{pmatrix} } $$", "id": "4929083"}]}