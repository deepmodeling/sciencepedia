{"hands_on_practices": [{"introduction": "The journey into any probability distribution begins with mastering its most fundamental calculation: finding the probability of an event. This exercise is designed to solidify your understanding of the continuous uniform distribution's core property. By deriving the probability that a random variable falls within a specific sub-interval, you will directly engage with the concept that for a uniform distribution, probability is simply a ratio of lengthsâ€”a simple yet powerful idea [@problem_id:3209].", "problem": "A continuous random variable $X$ is said to follow a uniform distribution on the interval $[a, b]$, denoted as $X \\sim U[a, b]$, if its probability density function (PDF), $f(x)$, is constant over the interval and zero elsewhere. The PDF is defined as:\n\n$$\nf(x) = \\begin{cases}\n\\frac{1}{b-a} & \\text{for } a \\le x \\le b \\\\\n0 & \\text{for } x < a \\text{ or } x > b\n\\end{cases}\n$$\n\nThe probability that $X$ falls within a sub-interval is found by integrating the PDF over that sub-interval.\n\nGiven a random variable $X \\sim U[a, b]$, derive a general expression for the probability $P(c < X < d)$, where the interval $(c, d)$ is a subset of the interval $[a, b]$, i.e., $a \\le c < d \\le b$.", "solution": "The PDF of $X\\sim U[a,b]$ is\n\n$$\nf(x)=\\frac{1}{b-a}\\quad\\text{for }a\\le x\\le b.\n$$\n\nHence\n\n$$\nP(c<X<d)=\\int_{c}^{d}f(x)\\,dx\n=\\int_{c}^{d}\\frac{1}{b-a}\\,dx.\n$$\n\nCarrying out the integration,\n\n$$\nP(c<X<d)\n=\\left.\\frac{x}{b-a}\\right|_{x=c}^{x=d}\n=\\frac{d-c}{b-a}.\n$$", "answer": "$$\\boxed{\\frac{d - c}{b - a}}$$", "id": "3209"}, {"introduction": "Real-world systems often involve the combination of multiple random processes. This practice explores what happens when we add two independent random variables that are both uniformly distributed, a common scenario in fields like signal processing where noise from different sources accumulates. You will discover the intriguing result that the sum of two uniform distributions is not uniform itself but instead forms a triangular distribution, providing a practical introduction to the important concept of convolution [@problem_id:1347806].", "problem": "In the field of signal processing, the characterization of noise is a fundamental task. Consider a system where the total noise is the superposition of two independent noise sources. Let the contribution from the first source be represented by a random variable $X$, and the contribution from the second source by a random variable $Y$. Both $X$ and $Y$ are normalized and found to be independently and uniformly distributed over the interval $[0, 1]$. An analyst is studying the composite noise signal, a new random variable $Z$ defined as the sum of the individual contributions, $Z = X + Y$.\n\nCalculate the value of the probability density function for the composite noise, $f_Z(z)$, evaluated at $z = 1.4$.", "solution": "Let $X$ and $Y$ be two independent random variables, both uniformly distributed on the interval $[0, 1]$. Their probability density functions (PDFs) are given by:\n$$f_X(x) = \\begin{cases} 1 & \\text{if } 0 \\le x \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n$$f_Y(y) = \\begin{cases} 1 & \\text{if } 0 \\le y \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\nThe random variable $Z$ is defined as the sum $Z = X + Y$. The PDF of the sum of two independent continuous random variables is given by the convolution of their individual PDFs. The convolution integral is:\n$$f_Z(z) = \\int_{-\\infty}^{\\infty} f_X(x) f_Y(z-x) \\, dx$$\n\nThe integrand $f_X(x) f_Y(z-x)$ is non-zero only when both $f_X(x)$ and $f_Y(z-x)$ are non-zero. This requires two conditions to be met simultaneously:\n1.  $0 \\le x \\le 1$ (from the definition of $f_X(x)$)\n2.  $0 \\le z-x \\le 1$ (from the definition of $f_Y(z-x)$)\n\nThe second condition can be rewritten in terms of $x$ as $z-1 \\le x \\le z$.\nFor the integral to be non-zero, the variable of integration $x$ must satisfy both conditions:\n$$0 \\le x \\le 1 \\quad \\text{and} \\quad z-1 \\le x \\le z$$\nThis means we need to find the intersection of the interval $[0, 1]$ and the interval $[z-1, z]$. The nature of this intersection depends on the value of $z$. We also note that since $X$ and $Y$ are non-negative, their sum $Z$ must be non-negative. Since their maximum values are 1, the maximum value of $Z$ is 2. So, $f_Z(z) = 0$ for $z < 0$ or $z > 2$. We only need to consider $z \\in [0, 2]$.\n\nWe analyze the problem in two cases based on the value of $z$.\n\n**Case 1: $0 \\le z < 1$**\nIn this case, the interval for $x$ is the intersection of $[0, 1]$ and $[z-1, z]$. Since $z < 1$, we have $z-1 < 0$. The intersection is thus $[\\max(0, z-1), \\min(1, z)] = [0, z]$.\nThe integral becomes:\n$$f_Z(z) = \\int_{0}^{z} (1)(1) \\, dx = [x]_{0}^{z} = z$$\n\n**Case 2: $1 \\le z \\le 2$**\nIn this case, the interval for $x$ is the intersection of $[0, 1]$ and $[z-1, z]$. Since $z \\ge 1$, we have $z-1 \\ge 0$. Also, since $z \\le 2$, we have $z-1 \\le 1$. The intersection is $[\\max(0, z-1), \\min(1, z)] = [z-1, 1]$.\nThe integral becomes:\n$$f_Z(z) = \\int_{z-1}^{1} (1)(1) \\, dx = [x]_{z-1}^{1} = 1 - (z-1) = 2-z$$\n\nCombining these results, we get the complete PDF for $Z$, which is known as the triangular distribution:\n$$f_Z(z) = \\begin{cases} z & \\text{if } 0 \\le z < 1 \\\\ 2-z & \\text{if } 1 \\le z \\le 2 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\nThe problem asks for the value of this PDF at $z = 1.4$. Since $1 \\le 1.4 \\le 2$, we use the expression for the second case:\n$$f_Z(1.4) = 2 - 1.4 = 0.6$$", "answer": "$$\\boxed{0.6}$$", "id": "1347806"}, {"introduction": "Moving beyond direct probabilities, this exercise challenges you to think about functions of random variables and their expected values. Presented as the classic \"broken stick\" problem, you are asked to calculate the expected value of the ratio between the shorter and longer pieces of a rod cut at a random point. This problem sharpens your ability to define new random variables based on an initial one and reinforces the method for calculating the expectation of a complex function, demonstrating the deeper analytical power of probability theory [@problem_id:1347823].", "problem": "A robotics company is developing a new automated system for cutting metal rods. A particular rod has a total length of $L$. The system is programmed to make a single cut at a position $X$, which is chosen from a uniform probability distribution over the entire length of the rod, from one end (position 0) to the other (position $L$). After being cut, the two resulting pieces are sorted into a 'short' pile and a 'long' pile. In the case of an exactly centered cut, the assignment of either piece to the 'short' or 'long' category is arbitrary. A key performance metric for understanding the process variability is the expected value of the ratio of the length of the shorter piece to the length of the longer piece. Calculate this value as a closed-form analytic expression.", "solution": "Let the length of the rod be $L$. The position of the cut, $X$, is a random variable that follows a uniform distribution on the interval $[0, L]$. The probability density function (PDF) of $X$ is given by $f_X(x) = \\frac{1}{L}$ for $x \\in [0, L]$, and $f_X(x) = 0$ otherwise.\n\nWhen the rod is cut at position $X$, it results in two pieces of lengths $X$ and $L-X$.\nLet $S$ be the random variable representing the length of the shorter piece, and let $G$ be the random variable representing the length of the longer piece. We can express $S$ and $G$ in terms of $X$ as:\n$S = \\min(X, L-X)$\n$G = \\max(X, L-X)$\n\nWe are asked to find the expected value of the ratio of the length of the shorter piece to the length of the longer piece, which is $E\\left[\\frac{S}{G}\\right]$.\nLet's first express the ratio $\\frac{S}{G}$ in terms of $X$:\n$$ \\frac{S}{G} = \\frac{\\min(X, L-X)}{\\max(X, L-X)} $$\nTo simplify the problem, let's define a normalized random variable $Y = \\frac{X}{L}$. Since $X \\sim U(0, L)$, the normalized variable $Y$ will follow a uniform distribution on the interval $[0, 1]$, i.e., $Y \\sim U(0, 1)$, with a PDF $f_Y(y) = 1$ for $y \\in [0, 1]$.\nWe can rewrite the ratio in terms of $Y$:\n$$ \\frac{S}{G} = \\frac{\\min(L Y, L - L Y)}{\\max(L Y, L - L Y)} = \\frac{L \\min(Y, 1-Y)}{L \\max(Y, 1-Y)} = \\frac{\\min(Y, 1-Y)}{\\max(Y, 1-Y)} $$\nThe ratio is independent of the total length $L$. Therefore, we can solve the problem by assuming $L=1$ and using a random variable $X \\sim U(0,1)$. The quantity to be calculated is $E\\left[\\frac{\\min(X, 1-X)}{\\max(X, 1-X)}\\right]$.\n\nThe expected value is found by integrating the function of the random variable multiplied by its PDF over the support of the distribution:\n$$ E\\left[\\frac{S}{G}\\right] = \\int_0^1 \\frac{\\min(x, 1-x)}{\\max(x, 1-x)} f_X(x) dx $$\nSince $f_X(x)=1$ for $x \\in [0,1]$, the integral becomes:\n$$ E\\left[\\frac{S}{G}\\right] = \\int_0^1 \\frac{\\min(x, 1-x)}{\\max(x, 1-x)} dx $$\nWe need to split the integral based on the behavior of the $\\min$ and $\\max$ functions. The crossover point is when $x = 1-x$, which implies $2x=1$, so $x=1/2$.\nFor $x \\in [0, 1/2]$, we have $x \\leq 1-x$, so $\\min(x, 1-x) = x$ and $\\max(x, 1-x) = 1-x$.\nFor $x \\in (1/2, 1]$, we have $x > 1-x$, so $\\min(x, 1-x) = 1-x$ and $\\max(x, 1-x) = x$.\n\nWe can now split the integral into two parts:\n$$ E\\left[\\frac{S}{G}\\right] = \\int_0^{1/2} \\frac{x}{1-x} dx + \\int_{1/2}^1 \\frac{1-x}{x} dx $$\nLet's evaluate the first integral, $I_1 = \\int_0^{1/2} \\frac{x}{1-x} dx$. We can use algebraic manipulation:\n$$ \\frac{x}{1-x} = \\frac{-(1-x) + 1}{1-x} = -1 + \\frac{1}{1-x} $$\nSo, the integral becomes:\n$$ I_1 = \\int_0^{1/2} \\left(-1 + \\frac{1}{1-x}\\right) dx = \\left[-x - \\ln|1-x|\\right]_0^{1/2} $$\n$$ I_1 = \\left(-\\frac{1}{2} - \\ln\\left|1-\\frac{1}{2}\\right|\\right) - (-0 - \\ln|1-0|) = -\\frac{1}{2} - \\ln\\left(\\frac{1}{2}\\right) = -\\frac{1}{2} - (-\\ln(2)) = \\ln(2) - \\frac{1}{2} $$\nNow let's evaluate the second integral, $I_2 = \\int_{1/2}^1 \\frac{1-x}{x} dx$. We can use a substitution $u = 1-x$. Then $x = 1-u$ and $dx = -du$. The limits of integration change as follows: when $x=1/2$, $u=1/2$; when $x=1$, $u=0$.\n$$ I_2 = \\int_{1/2}^0 \\frac{u}{1-u} (-du) = \\int_0^{1/2} \\frac{u}{1-u} du $$\nThis is exactly the same integral as $I_1$. Therefore, $I_2 = I_1 = \\ln(2) - \\frac{1}{2}$.\n\nThe total expected value is the sum of the two integrals:\n$$ E\\left[\\frac{S}{G}\\right] = I_1 + I_2 = \\left(\\ln(2) - \\frac{1}{2}\\right) + \\left(\\ln(2) - \\frac{1}{2}\\right) = 2\\ln(2) - 1 $$\nThe calculation is complete. The expected value of the ratio is a constant value independent of the rod's length $L$.", "answer": "$$\\boxed{2\\ln(2) - 1}$$", "id": "1347823"}]}