{"hands_on_practices": [{"introduction": "Understanding measurement scales is crucial when comparing data from different sources. This practice problem uses the familiar conversion between Celsius and Fahrenheit temperature scales to explore the defining properties of interval-level data. By analyzing which statistical statements hold true after the affine transformation $F = \\frac{9}{5}C + 32$, you will gain a concrete understanding of why the interval scale is defined by the invariance of interval ratios, not value ratios. [@problem_id:4922421]", "problem": "In a clinical monitoring study, oral temperature measurements of $n = 8$ patients were recorded in degrees Celsius, denoted by the vector $C = (c_{1}, \\dots, c_{8})$, where the observed values are $36.6$, $37.0$, $37.8$, $38.1$, $36.9$, $39.0$, $38.4$, and $37.3$. To report results to a collaborating laboratory that uses degrees Fahrenheit, the team converts each measurement using the exact affine transformation $$F = \\frac{9}{5}C + 32,$$ where the multiplication and addition act componentwise on $C$.\n\nUsing the core definitions of the four levels of measurement—nominal, ordinal, interval, and ratio—jointly with the mathematical properties of affine transformations of the form $$Y = aX + b,$$ determine which classes of statistical statements are preserved when moving from $C$ to $F$ and which are not. Your reasoning must start from the formal definitions of these scale types and the transformation structure without appealing to shortcuts.\n\nThen, compute the Pearson product-moment correlation coefficient $r$ between the Celsius vector $C$ and its Fahrenheit transform $F$ using the exact conversion specified above. Round your final numeric answer to four significant figures. No units are required in the final answer.", "solution": "The problem presents two distinct tasks: first, to analyze the invariance of statistical statements under an affine transformation between temperature scales, based on the definitions of levels of measurement; and second, to compute the Pearson product-moment correlation coefficient between the original and transformed data vectors.\n\n### Part 1: Analysis of Measurement Scale Invariance\n\nThe foundation of this analysis rests upon the formal definitions of the four levels of measurement (nominal, ordinal, interval, ratio) and the class of transformations that leave the structure of each scale invariant. The transformation from Celsius ($C$) to Fahrenheit ($F$) is given by the positive affine transformation $F = aC + b$, where $a = \\frac{9}{5}  0$ and $b = 32$.\n\n1.  **Nominal Scale:** A nominal scale categorizes data without any intrinsic order. The only permissible formal relation is equality ($=$) or inequality ($\\neq$). A statement is preserved if the transformation is one-to-one. For any two measurements $c_i$ and $c_j$, if $c_i = c_j$, then $f_i = \\frac{9}{5}c_i + 32 = \\frac{9}{5}c_j + 32 = f_j$. Conversely, if $f_i = f_j$, then $\\frac{9}{5}c_i + 32 = \\frac{9}{5}c_j + 32$, which implies $\\frac{9}{5}c_i = \\frac{9}{5}c_j$ and thus $c_i = c_j$. The transformation is a bijection. Therefore, statements of identity and distinctness (e.g., \"patient A's temperature is the same as patient B's\") are preserved.\n\n2.  **Ordinal Scale:** An ordinal scale possesses the properties of a nominal scale but adds a meaningful order relation ($$, $$). Statements are preserved if the transformation is strictly monotonic. Let $c_i  c_j$. Since the coefficient $a = \\frac{9}{5}$ is positive, multiplying the inequality by $a$ preserves its direction: $\\frac{9}{5}c_i  \\frac{9}{5}c_j$. Adding the constant $b=32$ to both sides also preserves the inequality: $\\frac{9}{5}c_i + 32  \\frac{9}{5}c_j + 32$, which is $f_i  f_j$. The transformation is strictly monotonically increasing. Consequently, all statements concerning order (e.g., \"patient A is hotter than patient B\") are preserved.\n\n3.  **Interval Scale:** An interval scale has the properties of an ordinal scale, and additionally, the differences between values are meaningful and comparable. The hallmark of an interval scale is the invariance of ratios of intervals under positive affine transformations ($Y = aX+b$, $a0$). Let's examine the ratio of two intervals in Celsius, $\\frac{c_j - c_i}{c_l - c_k}$. The corresponding values in Fahrenheit are $f_i = ac_i+b$, $f_j = ac_j+b$, $f_k = ac_k+b$, and $f_l = ac_l+b$. The new ratio of intervals is:\n$$ \\frac{f_j - f_i}{f_l - f_k} = \\frac{(ac_j + b) - (ac_i + b)}{(ac_l + b) - (ac_k + b)} = \\frac{a(c_j - c_i)}{a(c_l - c_k)} = \\frac{c_j - c_i}{c_l - c_k} $$\nSince the ratio of intervals is unchanged, all statistical statements that rely on this property are preserved. This includes statements about the equality of differences (e.g., \"The temperature increase from patient A to patient B is the same as from patient C to patient D\") and the relative magnitude of differences. The Celsius scale is itself an interval scale, as its zero point is arbitrary (freezing point of water, not absolute absence of thermal energy). The transformation to Fahrenheit, another interval scale, is precisely the type that preserves interval properties.\n\n4.  **Ratio Scale:** A ratio scale possesses all the properties of an interval scale, plus a true, non-arbitrary zero point. This absolute zero allows for meaningful ratios of values. The class of transformations that preserves a ratio scale is the positive scalar transformation ($Y = aX$, $a0$). The given transformation, $F = \\frac{9}{5}C + 32$, includes an additive constant $b=32 \\neq 0$. Let's examine the ratio of two values, $\\frac{c_j}{c_i}$. The corresponding ratio in Fahrenheit is:\n$$ \\frac{f_j}{f_i} = \\frac{\\frac{9}{5}c_j + 32}{\\frac{9}{5}c_i + 32} $$\nIn general, $\\frac{\\frac{9}{5}c_j + 32}{\\frac{9}{5}c_i + 32} \\neq \\frac{c_j}{c_i}$. For instance, a temperature of $20^\\circ C$ is not \"twice as hot\" as $10^\\circ C$ in a thermodynamic sense. This statement is not preserved upon conversion: $f(20) = 68^\\circ F$ and $f(10)=50^\\circ F$, and $\\frac{68}{50} = 1.36 \\neq 2$. The presence of the additive constant $b$ breaks the invariance of value ratios. Therefore, statistical statements that depend on a true zero and meaningful ratios of values are not preserved.\n\nIn summary, statements that are valid for nominal, ordinal, and interval scales are preserved under the Celsius to Fahrenheit conversion. Statements that require a ratio scale are not.\n\n### Part 2: Calculation of the Pearson Correlation Coefficient\n\nThe Pearson product-moment correlation coefficient $r$ between two variables, here denoted $C$ and $F$, is defined as:\n$$ r_{CF} = \\frac{\\text{cov}(C, F)}{\\sigma_C \\sigma_F} $$\nwhere $\\text{cov}(C, F)$ is the covariance of $C$ and $F$, and $\\sigma_C$ and $\\sigma_F$ are their respective standard deviations.\n\nThe problem specifies a perfect linear relationship between $F$ and $C$: $F = aC + b$, with $a = \\frac{9}{5}$ and $b = 32$. We can determine the correlation coefficient by using the properties of covariance and variance under linear transformations, without needing to perform calculations on the specific data points provided.\n\nLet $c_i$ represent the individual measurements in Celsius and $f_i = ac_i + b$ be the corresponding measurements in Fahrenheit.\nThe covariance is given by $\\text{cov}(C, F) = E[(C - E[C])(F - E[F])]$.\nWe know that $E[F] = E[aC+b] = aE[C] + b$.\nSubstituting this into the covariance formula:\n$$ \\text{cov}(C, F) = E[(C - E[C])((aC + b) - (aE[C] + b))] $$\n$$ \\text{cov}(C, F) = E[(C - E[C])(a(C - E[C]))] $$\n$$ \\text{cov}(C, F) = a E[(C - E[C])^2] = a \\cdot \\text{var}(C) = a \\sigma_C^2 $$\nThe variance of $F$ is:\n$$ \\text{var}(F) = \\text{var}(aC + b) = a^2 \\text{var}(C) = a^2 \\sigma_C^2 $$\nThe standard deviation of $F$ is therefore:\n$$ \\sigma_F = \\sqrt{a^2 \\sigma_C^2} = |a| \\sqrt{\\sigma_C^2} = |a| \\sigma_C $$\nNow, we substitute these expressions for $\\text{cov}(C, F)$ and $\\sigma_F$ back into the formula for $r_{CF}$:\n$$ r_{CF} = \\frac{a \\sigma_C^2}{\\sigma_C (|a| \\sigma_C)} = \\frac{a \\sigma_C^2}{|a| \\sigma_C^2} = \\frac{a}{|a|} $$\nThis general result shows that the correlation coefficient between a variable and its affine transform depends only on the sign of the scaling coefficient $a$.\nFor the given transformation, $a = \\frac{9}{5}$, which is a positive value.\n$$ r_{CF} = \\frac{\\frac{9}{5}}{|\\frac{9}{5}|} = \\frac{\\frac{9}{5}}{\\frac{9}{5}} = 1 $$\nThe correlation coefficient is exactly $1$, indicating a perfect positive linear relationship. The data points $(c_i, f_i)$ lie perfectly on a line with a positive slope. The specific numerical values for the temperatures are not required to deduce this result.\n\nThe problem asks for the numerical answer to be rounded to four significant figures. An exact value of $1$ expressed to four significant figures is $1.000$.", "answer": "$$\n\\boxed{1.000}\n$$", "id": "4922421"}, {"introduction": "A core principle in statistics is that our conclusions should not depend on arbitrary choices in measurement. This exercise presents a striking scenario where analyzing ordinal pain scores using two different but equally valid numerical assignments leads to completely opposite conclusions about a treatment's effectiveness. By working through this paradox, you will see firsthand why statistics like the arithmetic mean are not \"meaningful\" for ordinal data and why methods that rely only on rank ordering are required. [@problem_id:4922405]", "problem": "A biostatistics team evaluates an analgesic using a post-treatment pain score recorded on a discrete scale from $0$ to $10$. Clinical experts affirm that this pain scale is ordinal: the only meaningful information is the ordering of scores, and any strictly increasing transformation of scores preserves the measurement content. Consider two randomized groups with the following observed pain scores:\nControl group $C$: $[3,4,5,6,7]$,\nTreatment group $T$: $[0,1,2,3,10]$.\nTwo analyses are proposed on these same data:\nAnalysis A computes the difference in sample means $\\bar{x}_{T}-\\bar{x}_{C}$ on the raw scale.\nAnalysis B applies a strictly increasing transformation $f(s)=s^{3}$ to every score and then computes the mean difference $\\overline{f(s)}_{T}-\\overline{f(s)}_{C}$.\nCarry out both analyses and identify whether they lead to conflicting conclusions about whether the analgesic reduces pain. Using only the foundational definitions of levels of measurement and their admissible transformations, determine which conclusion is meaningful by checking transformation invariance under strictly increasing transformations. Finally, to summarize the treatment effect in a way that is appropriate for an ordinal scale, compute the probability of superiority\n$$\\theta=\\Pr(S_{T}S_{C})+\\tfrac{1}{2}\\Pr(S_{T}=S_{C}),$$\nwhere $S_{T}$ and $S_{C}$ are independent draws from the treatment and control distributions represented by the observed samples, and each pairwise comparison is weighted equally. Express your final numerical value for $\\theta$ as a decimal rounded to four significant figures.", "solution": "The task is to analyze the provided data using two different methods, assess the validity of their conclusions based on measurement theory, and then apply an appropriate method for ordinal data.\n\n**Analysis A: Difference in Sample Means on the Raw Scale**\n\nThe control group data are $C = [3, 4, 5, 6, 7]$. The sample size is $n_C = 5$.\nThe sample mean for the control group is:\n$$ \\bar{x}_C = \\frac{3+4+5+6+7}{5} = \\frac{25}{5} = 5 $$\nThe treatment group data are $T = [0, 1, 2, 3, 10]$. The sample size is $n_T = 5$.\nThe sample mean for the treatment group is:\n$$ \\bar{x}_T = \\frac{0+1+2+3+10}{5} = \\frac{16}{5} = 3.2 $$\nThe difference in sample means is:\n$$ \\bar{x}_T - \\bar{x}_C = 3.2 - 5 = -1.8 $$\nThe negative result indicates that the average pain score in the treatment group is lower than in the control group. This suggests that the analgesic is effective at reducing pain.\n\n**Analysis B: Difference in Sample Means on Transformed Scores**\n\nThe analysis is repeated after applying the strictly increasing transformation $f(s) = s^3$ to each score.\nThe transformed control group data are $f(C) = [3^3, 4^3, 5^3, 6^3, 7^3] = [27, 64, 125, 216, 343]$.\nThe mean of the transformed control scores is:\n$$ \\overline{f(s)}_C = \\frac{27+64+125+216+343}{5} = \\frac{775}{5} = 155 $$\nThe transformed treatment group data are $f(T) = [0^3, 1^3, 2^3, 3^3, 10^3] = [0, 1, 8, 27, 1000]$.\nThe mean of the transformed treatment scores is:\n$$ \\overline{f(s)}_T = \\frac{0+1+8+27+1000}{5} = \\frac{1036}{5} = 207.2 $$\nThe difference in the means of the transformed scores is:\n$$ \\overline{f(s)}_T - \\overline{f(s)}_C = 207.2 - 155 = 52.2 $$\nThe positive result indicates that the average of the transformed pain scores in the treatment group is higher than in the control group. This suggests that the analgesic is not effective, and could even be detrimental.\n\n**Conflict and Meaningfulness of Conclusions**\n\nAnalysis A concludes that the treatment lowers pain scores ($\\bar{x}_T  \\bar{x}_C$), while Analysis B concludes that the treatment increases pain scores ($\\overline{f(s)}_T  \\overline{f(s)}_C$). These conclusions are contradictory.\n\nThe problem states the pain scale is **ordinal**. For an ordinal scale, the numerical values serve only to rank the observations. The magnitude of the differences between scale points is not meaningful. The admissible transformations for an ordinal scale are any strictly increasing monotonic functions, as these preserve the ordering of the data. The function $f(s) = s^3$ is a valid admissible transformation for non-negative scores.\n\nA conclusion or statistical statement about ordinal data is considered \"meaningful\" only if its truth value remains invariant under all such admissible transformations. Here, the statement \"the mean of the treatment group is lower than the mean of the control group\" is true for the raw scores but false for the transformed scores. Because the conclusion depends on the arbitrary numerical assignment to the ordered categories (i.e., it is not invariant), the comparison of arithmetic means is not a meaningful operation for ordinal data. The arithmetic mean is a statistic appropriate for interval or ratio scales, where additivity and differences are well-defined. Therefore, neither the conclusion from Analysis A nor Analysis B is valid, as the underlying method is inappropriate for the data's level of measurement.\n\n**Appropriate Analysis for Ordinal Data: Probability of Superiority**\n\nA meaningful comparison of two groups with ordinal data can be made using a non-parametric statistic that depends only on the ordering of the scores. The probability of superiority, $\\theta$, is one such measure. It is defined as:\n$$ \\theta = \\Pr(S_T  S_C) + \\frac{1}{2}\\Pr(S_T = S_C) $$\nwhere $S_T$ and $S_C$ are scores drawn randomly from the treatment and control populations, respectively. We estimate these probabilities from the samples by considering all possible pairwise comparisons. The total number of pairs $(s_i, s_j)$ with $s_i \\in T$ and $s_j \\in C$ is $n_T \\times n_C = 5 \\times 5 = 25$.\n\nWe enumerate the outcomes for each score in $T$ compared against all scores in $C$:\n-   $s_T = 0$: is less than $3, 4, 5, 6, 7$. ($5$ cases of $S_T  S_C$)\n-   $s_T = 1$: is less than $3, 4, 5, 6, 7$. ($5$ cases of $S_T  S_C$)\n-   $s_T = 2$: is less than $3, 4, 5, 6, 7$. ($5$ cases of $S_T  S_C$)\n-   $s_T = 3$: is equal to $3$, and less than $4, 5, 6, 7$. ($1$ case of $S_T = S_C$, $4$ cases of $S_T  S_C$)\n-   $s_T = 10$: is greater than $3, 4, 5, 6, 7$. ($5$ cases of $S_T  S_C$)\n\nTallying these outcomes:\n-   Number of pairs where $S_T  S_C$: $5 + 5 + 5 + 4 = 19$.\n-   Number of pairs where $S_T = S_C$: $1$.\n-   Number of pairs where $S_T  S_C$: $5$.\nThe sum is $19+1+5 = 25$, which is the total number of pairs.\n\nThe probabilities are estimated as:\n$$ \\Pr(S_T  S_C) = \\frac{19}{25} $$\n$$ \\Pr(S_T = S_C) = \\frac{1}{25} $$\nNow we compute $\\theta$:\n$$ \\theta = \\frac{19}{25} + \\frac{1}{2} \\left( \\frac{1}{25} \\right) = \\frac{19}{25} + \\frac{1}{50} = \\frac{38}{50} + \\frac{1}{50} = \\frac{39}{50} $$\nConverting to a decimal, $\\theta = 0.78$. Rounding to four significant figures as requested gives $0.7800$.\nThis value of $\\theta$ can be interpreted as the probability that a randomly chosen subject from the treatment group will have a lower (better) pain score than a randomly chosen subject from the control group, with ties being split. Since $\\theta = 0.7800  0.5$, this provides a meaningful conclusion that the analgesic is effective. This conclusion is invariant under any strictly increasing transformation.", "answer": "$$\n\\boxed{0.7800}\n$$", "id": "4922405"}, {"introduction": "In real-world biostatistical analysis, misclassifying a variable's measurement level can have serious consequences, leading to incorrect estimates of risk or treatment effects. This advanced problem takes you beyond simple descriptive statistics and into the realm of regression modeling, where these errors can be amplified. You will quantify the exact amount of bias introduced in a logistic regression model when an ordinal smoking exposure is improperly treated as an interval variable, providing a clear lesson on the importance of correct model specification. [@problem_id:4922401]", "problem": "A clinical data set records a four-level ordinal exposure, smoking intensity, classified by trained nurses as \"none,\" \"light,\" \"moderate,\" and \"heavy.\" Let the ordinal category score be $S \\in \\{0,1,2,3\\}$ corresponding to \"none,\" \"light,\" \"moderate,\" and \"heavy,\" respectively. Suppose that, in reality, each category corresponds to a typical number of cigarettes per day, denoted by $c$, with values $c=0$ for \"none,\" $c=5$ for \"light,\" $c=12$ for \"moderate,\" and $c=30$ for \"heavy.\" The binary outcome $Y \\in \\{0,1\\}$ is elevated C-reactive protein (CRP), and the data are collected in large, balanced strata with equal numbers of individuals in each smoking category.\n\nAssume the true data-generating mechanism for the outcome is a logistic regression with a linear predictor in the underlying continuous exposure $c$,\n$$\n\\operatorname{logit}\\big(P(Y=1 \\mid c)\\big) \\;=\\; \\alpha + \\gamma\\,c,\n$$\nwhere $\\alpha$ is an intercept and $\\gamma$ is the per-cigarette log-odds increment. Suppose $\\gamma = 0.02$.\n\nAn analyst, misclassifying the ordinal scale $S$ as if it were interval with equal spacing, instead fits a logistic model with the linear predictor\n$$\n\\operatorname{logit}\\big(P(Y=1 \\mid S)\\big) \\;=\\; \\alpha' + \\beta\\,S,\n$$\nusing the equally spaced scores $S=0,1,2,3$.\n\nDefine the target effect of interest as the severe-versus-none log-odds difference under the true model, which is $\\gamma\\,(c_{\\text{heavy}}-c_{\\text{none}})$, and define the analyst’s misclassified effect estimate as $\\beta\\,(S_{\\text{heavy}}-S_{\\text{none}})$. Using the fact that the best linear approximation of the true linear predictor $\\alpha + \\gamma c$ by $\\alpha' + \\beta S$ under equal weights across categories is obtained by least-squares projection, compute the bias\n$$\n\\text{Bias} \\;=\\; \\beta\\,(S_{\\text{heavy}}-S_{\\text{none}}) \\;-\\; \\gamma\\,(c_{\\text{heavy}}-c_{\\text{none}}),\n$$\nfor the given $c$ values and $\\gamma$. Round your answer to four significant figures. Express the final value as a pure number (log-odds units are dimensionless).", "solution": "The problem requires us to calculate the bias that arises from mis-specifying an ordinal predictor as an interval-scaled predictor in a logistic regression model. The bias is the difference between the effect estimated from the mis-specified model and the true target effect.\n\nThe true linear predictor is $\\eta(c) = \\alpha + \\gamma c$. The analyst's model uses an approximating linear predictor $\\eta'(S) = \\alpha' + \\beta S$. The problem states that the parameters $\\alpha'$ and $\\beta$ are determined by the least-squares projection of $\\eta(c)$ onto the space spanned by $1$ and $S$, with equal weights for the four exposure categories. This is equivalent to performing a simple linear regression of the true predictor values, $\\eta_i$, on the score values, $S_i$.\n\nThe four pairs of $(S_i, c_i)$ values are:\n\\begin{itemize}\n    \\item Category 1 (\"none\"): $(S_1, c_1) = (0, 0)$\n    \\item Category 2 (\"light\"): $(S_2, c_2) = (1, 5)$\n    \\item Category 3 (\"moderate\"): $(S_3, c_3) = (2, 12)$\n    \\item Category 4 (\"heavy\"): $(S_4, c_4) = (3, 30)$\n\\end{itemize}\nThe corresponding values of the true linear predictor are $\\eta_i = \\alpha + \\gamma c_i$.\n\nWe need to find the slope $\\beta$ of the regression line of $\\eta_i$ on $S_i$. The formula for the least-squares estimate of the slope is:\n$$ \\beta = \\frac{\\sum_{i=1}^{4} (S_i - \\bar{S})(\\eta_i - \\bar{\\eta})}{\\sum_{i=1}^{4} (S_i - \\bar{S})^2} $$\nLet's first calculate the means, $\\bar{S}$ and $\\bar{\\eta}$. Since the weights are equal, we use the standard arithmetic mean.\n$$ \\bar{S} = \\frac{0 + 1 + 2 + 3}{4} = \\frac{6}{4} = 1.5 $$\nThe mean of the true predictor values is:\n$$ \\bar{\\eta} = \\frac{1}{4} \\sum_{i=1}^{4} (\\alpha + \\gamma c_i) = \\alpha + \\gamma \\left(\\frac{1}{4} \\sum_{i=1}^{4} c_i\\right) = \\alpha + \\gamma \\bar{c} $$\nwhere $\\bar{c}$ is the mean of the $c_i$ values:\n$$ \\bar{c} = \\frac{0 + 5 + 12 + 30}{4} = \\frac{47}{4} = 11.75 $$\nSo, $\\bar{\\eta} = \\alpha + 11.75\\gamma$.\n\nNow, let's calculate the denominator of the expression for $\\beta$:\n$$ \\sum_{i=1}^{4} (S_i - \\bar{S})^2 = (0 - 1.5)^2 + (1 - 1.5)^2 + (2 - 1.5)^2 + (3 - 1.5)^2 $$\n$$ = (-1.5)^2 + (-0.5)^2 + (0.5)^2 + (1.5)^2 = 2.25 + 0.25 + 0.25 + 2.25 = 5 $$\n\nNext, let's calculate the numerator. The deviation of $\\eta_i$ from its mean is:\n$$ \\eta_i - \\bar{\\eta} = (\\alpha + \\gamma c_i) - (\\alpha + \\gamma \\bar{c}) = \\gamma(c_i - \\bar{c}) $$\nThe numerator is thus:\n$$ \\sum_{i=1}^{4} (S_i - \\bar{S})(\\eta_i - \\bar{\\eta}) = \\sum_{i=1}^{4} (S_i - \\bar{S}) \\gamma (c_i - \\bar{c}) = \\gamma \\sum_{i=1}^{4} (S_i - \\bar{S})(c_i - \\bar{c}) $$\nLet's compute the sum of products of deviations:\n$$ \\sum_{i=1}^{4} (S_i - 1.5)(c_i - 11.75) = (0-1.5)(0-11.75) + (1-1.5)(5-11.75) + (2-1.5)(12-11.75) + (3-1.5)(30-11.75) $$\n$$ = (-1.5)(-11.75) + (-0.5)(-6.75) + (0.5)(0.25) + (1.5)(18.25) $$\n$$ = 17.625 + 3.375 + 0.125 + 27.375 = 48.5 $$\nThe numerator is therefore $48.5\\gamma$.\n\nNow we can compute $\\beta$:\n$$ \\beta = \\frac{48.5\\gamma}{5} = 9.7\\gamma $$\n\nThe problem defines the bias as the difference between the analyst's estimated effect and the true target effect.\nThe true target effect is the log-odds difference between the \"heavy\" and \"none\" categories based on the true model:\n$$ E_{\\text{true}} = \\gamma(c_{\\text{heavy}} - c_{\\text{none}}) = \\gamma(30 - 0) = 30\\gamma $$\nThe analyst's estimated effect is the log-odds difference between the \"heavy\" and \"none\" categories based on the mis-specified model:\n$$ E_{\\text{analyst}} = \\beta(S_{\\text{heavy}} - S_{\\text{none}}) = \\beta(3 - 0) = 3\\beta $$\nSubstituting the expression for $\\beta$ in terms of $\\gamma$:\n$$ E_{\\text{analyst}} = 3(9.7\\gamma) = 29.1\\gamma $$\nThe bias is:\n$$ \\text{Bias} = E_{\\text{analyst}} - E_{\\text{true}} = 29.1\\gamma - 30\\gamma = -0.9\\gamma $$\nThe problem provides the value $\\gamma = 0.02$. Substituting this value, we find the numerical bias:\n$$ \\text{Bias} = -0.9 \\times 0.02 = -0.018 $$\nThe problem asks for the answer to be rounded to four significant figures. The number $-0.018$ has two significant figures (the digits $1$ and $8$). To express this with four significant figures, we add two trailing zeros.\n$$ \\text{Bias} = -0.01800 $$\nThe bias is negative, indicating that the analyst's mis-specified model underestimates the magnitude of the true effect difference between heavy smokers and non-smokers.", "answer": "$$\\boxed{-0.01800}$$", "id": "4922401"}]}