{"hands_on_practices": [{"introduction": "Sampling bias can often arise from seemingly logical analytical choices that fail to account for the underlying sampling design. This exercise presents a classic counterexample where disproportionate sampling across subregions, a common real-world scenario, leads to a significantly biased estimate when the data are naively pooled. By working through this problem [@problem_id:4951835], you will gain a crucial understanding of how weighting is necessary to correct for stratified sampling and produce a valid overall estimate.", "problem": "A public health agency aims to estimate the regional prevalence of a binary disease outcome by pooling samples collected from two subregions, denoted $A$ and $B$. The true subregion prevalences are constant within each subregion. The agency’s field teams, seeking efficiency, sampled more densely in the subregion they perceived to have higher prevalence. Consider the following explicit scenario, which is intended to serve as a counterexample illustrating how such a sampling plan can inflate the estimated regional prevalence if naive pooling is used.\n\n- Subregion $A$ has population size $N_{A} = 6000$ and true prevalence $p_{A} = 0.12$.\n- Subregion $B$ has population size $N_{B} = 14000$ and true prevalence $p_{B} = 0.03$.\n- The true regional prevalence is defined as the population-size-weighted mean of subregion prevalences.\n- The sampling plan draws $m_{A} = 400$ individuals from subregion $A$ and $m_{B} = 100$ individuals from subregion $B$, using simple random sampling without replacement within each subregion.\n- Let $\\hat{p}_{A}$ and $\\hat{p}_{B}$ denote the sample proportions of disease in the two subregions. The agency reports the pooled, unweighted sample proportion $\\hat{p}_{\\text{naive}} = \\dfrac{m_{A}\\hat{p}_{A} + m_{B}\\hat{p}_{B}}{m_{A} + m_{B}}$ as the regional estimate.\n\nUsing only core definitions from sampling theory and probability (definition of prevalence as a population mean of indicator variables, expectation of a sample proportion under simple random sampling without replacement, and the definition of bias as the difference between an estimator’s expectation and the target parameter), analytically show that this sampling design inflates the expected naive pooled estimate relative to the true regional prevalence. Then compute the bias magnitude, defined as $|\\mathbb{E}[\\hat{p}_{\\text{naive}}] - p_{\\text{true}}|$, where $p_{\\text{true}}$ denotes the true regional prevalence. Express your final answer as a decimal (no percent sign). No rounding is required.", "solution": "The problem is well-posed and scientifically sound, allowing for a complete analytical solution. We begin by formally defining the parameter of interest, the true regional prevalence, and the estimator proposed by the agency. We will then analyze the estimator's statistical properties, specifically its expectation, to quantify its bias.\n\nThe total population of the region is $N = N_{A} + N_{B}$. The number of individuals with the disease in subregion $A$ is $N_{A}p_{A}$, and in subregion $B$ is $N_{B}p_{B}$. The true regional prevalence, $p_{\\text{true}}$, is defined as the total number of diseased individuals in the region divided by the total population size. This is equivalent to the population-size-weighted mean of the subregion prevalences.\n$$p_{\\text{true}} = \\frac{N_{A}p_{A} + N_{B}p_{B}}{N_{A} + N_{B}}$$\nLet us define the population weight for subregion $A$ as $W_{A} = \\frac{N_{A}}{N_{A}+N_{B}}$ and for subregion $B$ as $W_{B} = \\frac{N_{B}}{N_{A}+N_{B}}$, where $W_{A}+W_{B}=1$. Then, the true prevalence can be written as:\n$$p_{\\text{true}} = W_{A}p_{A} + W_{B}p_{B}$$\n\nThe agency's proposed estimator, $\\hat{p}_{\\text{naive}}$, is the unweighted proportion of diseased individuals in the pooled sample. Let $X_{A} = m_{A}\\hat{p}_{A}$ be the number of diseased individuals in the sample from subregion $A$, and $X_{B} = m_{B}\\hat{p}_{B}$ be the number in the sample from subregion $B$. The estimator is:\n$$\\hat{p}_{\\text{naive}} = \\frac{X_{A} + X_{B}}{m_{A} + m_{B}} = \\frac{m_{A}\\hat{p}_{A} + m_{B}\\hat{p}_{B}}{m_{A} + m_{B}}$$\nTo assess the bias of this estimator, we first compute its expected value, $\\mathbb{E}[\\hat{p}_{\\text{naive}}]$. Using the linearity of the expectation operator:\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = \\mathbb{E}\\left[\\frac{m_{A}\\hat{p}_{A} + m_{B}\\hat{p}_{B}}{m_{A} + m_{B}}\\right] = \\frac{m_{A}\\mathbb{E}[\\hat{p}_{A}] + m_{B}\\mathbb{E}[\\hat{p}_{B}]}{m_{A} + m_{B}}$$\nThe sampling within each subregion is simple random sampling without replacement (SRSWOR). A fundamental result of sampling theory is that the sample proportion, $\\hat{p}$, is an unbiased estimator of the population proportion, $p$. Therefore, $\\mathbb{E}[\\hat{p}_{A}] = p_{A}$ and $\\mathbb{E}[\\hat{p}_{B}] = p_{B}$. Substituting these into the equation for the expected value gives:\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = \\frac{m_{A}p_{A} + m_{B}p_{B}}{m_{A} + m_{B}}$$\nThis expression shows that the expected value of the naive estimator is a weighted average of the subregion prevalences, where the weights are determined by the sample sizes, not the population sizes. Let us define the sample weights as $w_{A} = \\frac{m_{A}}{m_{A}+m_{B}}$ and $w_{B} = \\frac{m_{B}}{m_{A}+m_{B}}$, where $w_{A}+w_{B}=1$. Then,\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = w_{A}p_{A} + w_{B}p_{B}$$\n\nThe bias of the estimator is the difference between its expectation and the true parameter value:\n$$\\text{Bias}(\\hat{p}_{\\text{naive}}) = \\mathbb{E}[\\hat{p}_{\\text{naive}}] - p_{\\text{true}} = (w_{A}p_{A} + w_{B}p_{B}) - (W_{A}p_{A} + W_{B}p_{B})$$\nUsing $w_{B} = 1 - w_{A}$ and $W_{B} = 1 - W_{A}$, we can rearrange the terms:\n$$\\text{Bias}(\\hat{p}_{\\text{naive}}) = (w_{A} - W_{A})p_{A} + ((1-w_{A}) - (1-W_{A}))p_{B} = (w_{A} - W_{A})p_{A} - (w_{A} - W_{A})p_{B}$$\n$$\\text{Bias}(\\hat{p}_{\\text{naive}}) = (w_{A} - W_{A})(p_{A} - p_{B})$$\nThe problem states that the sampling plan oversampled the subregion with higher prevalence. The given data are $p_{A} = 0.12$ and $p_{B} = 0.03$, so $p_{A} > p_{B}$, which makes the term $(p_{A} - p_{B})$ positive.\nThe sampling weight for subregion $A$ is $w_{A} = \\frac{m_{A}}{m_{A}+m_{B}} = \\frac{400}{400+100} = \\frac{400}{500} = 0.8$.\nThe population weight for subregion $A$ is $W_{A} = \\frac{N_{A}}{N_{A}+N_{B}} = \\frac{6000}{6000+14000} = \\frac{6000}{20000} = 0.3$.\nSince $w_{A} > W_{A}$, the term $(w_{A} - W_{A})$ is also positive. Thus, the product $(w_{A} - W_{A})(p_{A} - p_{B})$ is positive, which analytically demonstrates that the bias is positive. This means $\\mathbb{E}[\\hat{p}_{\\text{naive}}] > p_{\\text{true}}$, and the naive estimator is inflated as claimed. The inflation arises because the sampling proportion from subregion $A$ ($80\\%$) is much larger than its population proportion ($30\\%$), giving undue weight to its higher prevalence.\n\nNow, we compute the numerical value of the bias magnitude.\nFirst, we calculate the true regional prevalence, $p_{\\text{true}}$:\n$$p_{\\text{true}} = \\frac{N_{A}p_{A} + N_{B}p_{B}}{N_{A} + N_{B}} = \\frac{(6000)(0.12) + (14000)(0.03)}{6000 + 14000} = \\frac{720 + 420}{20000} = \\frac{1140}{20000} = 0.057$$\nNext, we calculate the expected value of the naive pooled estimate, $\\mathbb{E}[\\hat{p}_{\\text{naive}}]$:\n$$\\mathbb{E}[\\hat{p}_{\\text{naive}}] = \\frac{m_{A}p_{A} + m_{B}p_{B}}{m_{A} + m_{B}} = \\frac{(400)(0.12) + (100)(0.03)}{400 + 100} = \\frac{48 + 3}{500} = \\frac{51}{500} = 0.102$$\nThe bias is the difference between these two values:\n$$\\text{Bias} = \\mathbb{E}[\\hat{p}_{\\text{naive}}] - p_{\\text{true}} = 0.102 - 0.057 = 0.045$$\nThe bias magnitude is the absolute value of the bias:\n$$|\\text{Bias}| = |0.045| = 0.045$$\nThis confirms a substantial positive bias, leading to an overestimation of the true regional prevalence by $4.5$ percentage points on average.", "answer": "$$\\boxed{0.045}$$", "id": "4951835"}, {"introduction": "Beyond bias, the second major concern in sampling is sampling error—the inherent variability that comes from observing a sample instead of the entire population. This practice problem [@problem_id:4951762] shifts our focus to controlling this error in a scenario where our estimator is unbiased. You will learn to calculate the minimum sample size required to achieve a desired margin of error, a fundamental skill in designing statistically powerful studies, and see how the finite population correction becomes important when the sample constitutes a substantial fraction of the population.", "problem": "A biostatistics team plans to estimate the mean body mass index in a finite registry of adult patients using Simple Random Sampling Without Replacement (SRSWOR). The registry contains a known, fixed population of size $N = 5000$. From a prior census of the same registry, the population variance is known to be $S^{2} = 36$ and is believed to have remained stable. The team desires a two-sided confidence interval for the population mean at confidence level $1 - \\alpha = 0.95$ and requires that the margin of error be at most $m = 0.5$ (in body mass index units). Assume that under SRSWOR the sampling estimator is unbiased and that a normal approximation is appropriate for the sampling distribution of the sample mean. Explicitly include the finite population correction in your derivation and computations.\n\nWork from fundamental definitions that distinguish sampling error from sampling bias, and from the core fact that under SRSWOR the sample mean is unbiased. Starting from the definition of a two-sided $(1 - \\alpha)$ confidence interval for the mean in terms of a standard normal quantile $z_{1-\\alpha/2}$ and the standard error of the sample mean under SRSWOR, derive an expression for the continuous (real-valued) sample size $n_{\\text{cont}}$ that achieves a margin of error exactly equal to $m$. Then, identify the minimal integer sample size $n_{\\min}$ that guarantees the margin of error requirement by taking the smallest integer that meets the inequality implied by your derivation.\n\nUse $z_{1-\\alpha/2} = z_{0.975}$ for $\\alpha = 0.05$ and evaluate numerically with $z_{0.975} \\approx 1.96$. Report the minimal integer sample size $n_{\\min}$. Do not round by significant figures; instead, provide the smallest integer $n$ that satisfies the requirement.", "solution": "The problem requires the determination of the minimum integer sample size, $n_{\\min}$, needed to estimate a population mean with a specified margin of error and confidence level, drawing from a finite population using Simple Random Sampling Without Replacement (SRSWOR). The solution must be rigorously derived, beginning with fundamental statistical principles.\n\nFirst, we must distinguish between sampling bias and sampling error. Sampling bias refers to a systematic discrepancy between an estimator's expected value and the true population parameter. An estimator $\\hat{\\theta}$ for a parameter $\\theta$ is unbiased if its expectation equals the true value, i.e., $E[\\hat{\\theta}] = \\theta$. Conversely, sampling error is the random variability of an estimator's value from one sample to another, inherent in the process of sampling. It is not a systematic error but a consequence of observing a subset, rather than the entire population. The magnitude of sampling error is typically quantified by the standard error of the estimator.\n\nThe problem specifies the use of SRSWOR, for which the sample mean, $\\bar{y}$, is a well-established unbiased estimator of the population mean, $\\mu$. Thus, $E[\\bar{y}] = \\mu$, and there is no sampling bias to consider. Our task is to control the sampling error by ensuring the sample size is sufficiently large.\n\nA two-sided confidence interval for the population mean $\\mu$ at a confidence level of $1 - \\alpha$ is constructed as:\n$$ \\bar{y} \\pm m $$\nwhere $m$ is the margin of error. Under the assumption that the sampling distribution of the sample mean is approximately normal, the margin of error is defined as:\n$$ m = z_{1-\\alpha/2} \\times \\text{SE}(\\bar{y}) $$\nHere, $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution, and $\\text{SE}(\\bar{y})$ is the standard error of the sample mean.\n\nFor Simple Random Sampling Without Replacement from a finite population of size $N$, the variance of the sample mean is given by:\n$$ \\text{Var}(\\bar{y}) = \\frac{S^2}{n} \\left( \\frac{N-n}{N} \\right) $$\nwhere $S^2$ is the population variance, $n$ is the sample size, and the term $\\left( \\frac{N-n}{N} \\right)$ is the finite population correction (FPC). The standard error is the square root of the variance:\n$$ \\text{SE}(\\bar{y}) = \\sqrt{\\frac{S^2}{n} \\left( \\frac{N-n}{N} \\right)} $$\nThe problem requires the margin of error to be at most $m = 0.5$. To find the minimum sample size, we set the margin of error to its maximum permissible value:\n$$ m = z_{1-\\alpha/2} \\sqrt{\\frac{S^2}{n} \\left( \\frac{N-n}{N} \\right)} $$\nWe now solve this equation for $n$. Squaring both sides yields:\n$$ m^2 = z_{1-\\alpha/2}^2 \\left( \\frac{S^2}{n} \\right) \\left( \\frac{N-n}{N} \\right) $$\n$$ m^2 = z_{1-\\alpha/2}^2 S^2 \\left( \\frac{1}{n} - \\frac{1}{N} \\right) $$\nTo isolate $n$, we first solve for $\\frac{1}{n}$:\n$$ \\frac{m^2}{z_{1-\\alpha/2}^2 S^2} = \\frac{1}{n} - \\frac{1}{N} $$\n$$ \\frac{1}{n} = \\frac{m^2}{z_{1-\\alpha/2}^2 S^2} + \\frac{1}{N} $$\nLet us define $n_0$ as the sample size that would be required for an infinite population (or, equivalently, if the FPC were ignored). This is found by setting the FPC factor to $1$:\n$$ n_0 = \\frac{z_{1-\\alpha/2}^2 S^2}{m^2} $$\nSubstituting this definition into our equation for $\\frac{1}{n}$:\n$$ \\frac{1}{n} = \\frac{1}{n_0} + \\frac{1}{N} $$\nThis elegantly relates the required sample size $n$ to the infinite-population size $n_0$ and the population size $N$. To find the expression for the continuous sample size, $n_{\\text{cont}}$, we solve for $n$:\n$$ \\frac{1}{n} = \\frac{N + n_0}{n_0 N} $$\n$$ n = n_{\\text{cont}} = \\frac{n_0 N}{n_0 + N} = \\frac{n_0}{1 + \\frac{n_0}{N}} $$\nThis is the derived expression for the continuous sample size $n_{\\text{cont}}$ that achieves a margin of error exactly equal to $m$.\n\nNow, we substitute the given numerical values:\nPopulation size $N = 5000$.\nPopulation variance $S^2 = 36$.\nDesired margin of error $m = 0.5$.\nConfidence level $1 - \\alpha = 0.95$, which gives $\\alpha = 0.05$. The corresponding standard normal quantile is $z_{1-\\alpha/2} = z_{0.975} \\approx 1.96$.\n\nFirst, we calculate the initial sample size estimate, $n_0$:\n$$ n_0 = \\frac{(1.96)^2 \\times 36}{(0.5)^2} = \\frac{3.8416 \\times 36}{0.25} = 3.8416 \\times 144 = 553.1904 $$\nNext, we use this value to calculate the continuous sample size $n_{\\text{cont}}$ including the finite population correction:\n$$ n_{\\text{cont}} = \\frac{n_0}{1 + \\frac{n_0}{N}} = \\frac{553.1904}{1 + \\frac{553.1904}{5000}} = \\frac{553.1904}{1 + 0.11063808} = \\frac{553.1904}{1.11063808} \\approx 498.0835 $$\nThe value $n_{\\text{cont}} \\approx 498.0835$ is the exact real-valued sample size that would produce a margin of error of precisely $m=0.5$. However, a sample size must be an integer. The margin of error is a decreasing function of the sample size $n$. To ensure that the margin of error is *at most* $m=0.5$, the chosen integer sample size $n$ must be greater than or equal to $n_{\\text{cont}}$. Therefore, the minimal integer sample size, $n_{\\min}$, is the smallest integer that satisfies this condition, which is the ceiling of $n_{\\text{cont}}$:\n$$ n_{\\min} = \\lceil n_{\\text{cont}} \\rceil = \\lceil 498.0835 \\rceil = 499 $$\nA sample size of $n=498$ would result in a margin of error slightly larger than $0.5$, thus failing to meet the requirement. A sample size of $n=499$ is the minimum integer value that guarantees the margin of error will be no more than $0.5$.", "answer": "$$\\boxed{499}$$", "id": "4951762"}, {"introduction": "While some sampling designs inadvertently introduce bias, others intentionally use unequal selection probabilities to improve efficiency or ensure representation of key units. The challenge then becomes correcting for this intentional design feature to recover an unbiased estimate. This hands-on exercise [@problem_id:4951792] introduces the Horvitz-Thompson estimator, a powerful and widely-used tool that achieves unbiasedness by weighting each observation by the inverse of its inclusion probability, thereby providing a clear method for analyzing data from complex surveys.", "problem": "A health surveillance team wants to estimate the total number of confirmed cases of a condition across a small, finite population of clinics using a single sample selected without replacement and with unequal probabilities. The population has $N=4$ clinics with fixed values $\\{y_i\\}$ for the number of cases:\n- $y_1 = 12$\n- $y_2 = 7$\n- $y_3 = 9$\n- $y_4 = 5$\n\nA fixed-size design with sample size $n=2$ is used. The first-order inclusion probabilities are\n- $\\pi_1 = 0.6$, $\\pi_2 = 0.5$, $\\pi_3 = 0.5$, $\\pi_4 = 0.4$,\n\nand the pairwise inclusion probabilities (for $i \\neq j$) are\n- $\\pi_{12} = 0.20$, $\\pi_{13} = 0.24$, $\\pi_{14} = 0.16$,\n- $\\pi_{23} = 0.16$, $\\pi_{24} = 0.14$, $\\pi_{34} = 0.10$.\n\nA single sample is realized and contains units $\\{1,3\\}$.\n\nUsing only the core definition of inclusion indicators, the Horvitz–Thompson (HT) estimator built from weights $\\pi_i^{-1}$, and the design-based variance definition in terms of inclusion indicators and inclusion probabilities (without resorting to any external shortcut formulas), compute:\n- the Horvitz–Thompson estimator $\\hat{Y}_{HT}$ of the population total $Y = \\sum_{i=1}^{4} y_i$, for the realized sample $\\{1,3\\}$,\n- the exact design variance $\\operatorname{Var}(\\hat{Y}_{HT})$ under the given sampling design, expressed in terms of $\\{\\pi_i\\}$, $\\{\\pi_{ij}\\}$, and $\\{y_i\\}$.\n\nProvide both final values as exact numbers with no rounding. Express the final answer as a pure number (no units). Your final answer must list the two numbers in order $\\hat{Y}_{HT}$, then $\\operatorname{Var}(\\hat{Y}_{HT})$.", "solution": "The provided problem is valid. It is a well-posed problem in survey sampling theory with all necessary data provided. The given inclusion probabilities are internally consistent for a fixed-size sampling design of size $n=2$ from a population of size $N=4$.\n\nThe first objective is to compute the Horvitz-Thompson (HT) estimator, $\\hat{Y}_{HT}$, of the population total $Y = \\sum_{i=1}^{4} y_i$ for the realized sample $S = \\{1, 3\\}$. The formula for the HT estimator is:\n$$ \\hat{Y}_{HT} = \\sum_{i \\in S} \\frac{y_i}{\\pi_i} $$\nwhere $y_i$ is the value of the variable of interest for unit $i$ and $\\pi_i$ is its first-order inclusion probability. The realized sample consists of units $1$ and $3$. The values for these units are $y_1 = 12$ and $y_3 = 9$, with corresponding inclusion probabilities $\\pi_1 = 0.6$ and $\\pi_3 = 0.5$.\nSubstituting these values into the estimator formula:\n$$ \\hat{Y}_{HT} = \\frac{y_1}{\\pi_1} + \\frac{y_3}{\\pi_3} = \\frac{12}{0.6} + \\frac{9}{0.5} $$\n$$ \\hat{Y}_{HT} = 20 + 18 = 38 $$\n\nThe second objective is to compute the exact design variance of $\\hat{Y}_{HT}$, denoted $\\operatorname{Var}(\\hat{Y}_{HT})$. As requested, we begin with the fundamental definition of the estimator in terms of inclusion indicators. Let $I_i$ be a random variable such that $I_i=1$ if unit $i$ is included in the sample and $I_i=0$ otherwise. The HT estimator can be expressed as a linear combination of these indicators:\n$$ \\hat{Y}_{HT} = \\sum_{i=1}^{N} \\frac{y_i}{\\pi_i} I_i $$\nThe variance of this estimator is found using the properties of variances of sums of random variables:\n$$ \\operatorname{Var}(\\hat{Y}_{HT}) = \\operatorname{Var}\\left(\\sum_{i=1}^{N} \\frac{y_i}{\\pi_i} I_i\\right) = \\sum_{i=1}^{N} \\left(\\frac{y_i}{\\pi_i}\\right)^2 \\operatorname{Var}(I_i) + \\sum_{i=1}^{N} \\sum_{j \\neq i} \\left(\\frac{y_i}{\\pi_i}\\right) \\left(\\frac{y_j}{\\pi_j}\\right) \\operatorname{Cov}(I_i, I_j) $$\nBy definition, the expectation of an inclusion indicator is $E[I_i] = \\pi_i = P(i \\in S)$. Since $I_i$ is a Bernoulli variable, its variance is $\\operatorname{Var}(I_i) = E[I_i^2] - (E[I_i])^2 = \\pi_i - \\pi_i^2 = \\pi_i(1-\\pi_i)$, because $I_i^2 = I_i$.\nThe covariance between two distinct indicators $I_i$ and $I_j$ is $\\operatorname{Cov}(I_i, I_j) = E[I_i I_j] - E[I_i]E[I_j] = \\pi_{ij} - \\pi_i \\pi_j$, where $\\pi_{ij} = P(i \\in S \\text{ and } j \\in S)$ is the pairwise inclusion probability. Substituting these into the variance expression yields the Horvitz-Thompson variance formula:\n$$ \\operatorname{Var}(\\hat{Y}_{HT}) = \\sum_{i=1}^{N} \\frac{y_i^2}{\\pi_i^2} \\pi_i(1-\\pi_i) + \\sum_{i=1}^{N} \\sum_{j \\neq i} \\frac{y_i y_j}{\\pi_i \\pi_j} (\\pi_{ij} - \\pi_i \\pi_j) $$\nThis can be simplified to:\n$$ \\operatorname{Var}(\\hat{Y}_{HT}) = \\sum_{i=1}^{N} \\frac{y_i^2(1-\\pi_i)}{\\pi_i} + \\sum_{i=1}^{N} \\sum_{j \\neq i} \\frac{y_i y_j (\\pi_{ij} - \\pi_i \\pi_j)}{\\pi_i \\pi_j} $$\nWe compute the two terms of this sum separately. The first term is:\n$$ \\sum_{i=1}^{4} \\frac{y_i^2(1-\\pi_i)}{\\pi_i} = \\frac{12^2(1-0.6)}{0.6} + \\frac{7^2(1-0.5)}{0.5} + \\frac{9^2(1-0.5)}{0.5} + \\frac{5^2(1-0.4)}{0.4} $$\n$$ = \\frac{144(0.4)}{0.6} + \\frac{49(0.5)}{0.5} + \\frac{81(0.5)}{0.5} + \\frac{25(0.6)}{0.4} $$\n$$ = 96 + 49 + 81 + 37.5 = 263.5 $$\nThe second term is a double summation. We compute the summand for each pair $(i, j)$ where $i \\neq j$. Let $w_i = y_i/\\pi_i$. The terms are $w_i w_j (\\pi_{ij} - \\pi_i \\pi_j)$.\n-  $i=1, j=2$: $(\\frac{12}{0.6})(\\frac{7}{0.5})(0.20 - 0.6 \\times 0.5) = (20)(14)(0.20 - 0.30) = 280(-0.1) = -28$.\n-  $i=1, j=3$: $(\\frac{12}{0.6})(\\frac{9}{0.5})(0.24 - 0.6 \\times 0.5) = (20)(18)(0.24 - 0.30) = 360(-0.06) = -21.6$.\n-  $i=1, j=4$: $(\\frac{12}{0.6})(\\frac{5}{0.4})(0.16 - 0.6 \\times 0.4) = (20)(12.5)(0.16 - 0.24) = 250(-0.08) = -20$.\n-  $i=2, j=3$: $(\\frac{7}{0.5})(\\frac{9}{0.5})(0.16 - 0.5 \\times 0.5) = (14)(18)(0.16 - 0.25) = 252(-0.09) = -22.68$.\n-  $i=2, j=4$: $(\\frac{7}{0.5})(\\frac{5}{0.4})(0.14 - 0.5 \\times 0.4) = (14)(12.5)(0.14 - 0.20) = 175(-0.06) = -10.5$.\n-  $i=3, j=4$: $(\\frac{9}{0.5})(\\frac{5}{0.4})(0.10 - 0.5 \\times 0.4) = (18)(12.5)(0.10 - 0.20) = 225(-0.1) = -22.5$.\nThe double summation includes each pair twice (e.g., $(1,2)$ and $(2,1)$). Since the summand is symmetric, we sum the computed values and multiply by $2$:\n$$ \\sum_{i \\neq j} \\dots = 2 \\times (-28 - 21.6 - 20 - 22.68 - 10.5 - 22.5) $$\n$$ = 2 \\times (-125.28) = -250.56 $$\nThe total variance is the sum of the two parts:\n$$ \\operatorname{Var}(\\hat{Y}_{HT}) = 263.5 + (-250.56) = 12.94 $$\nThe two requested values are $\\hat{Y}_{HT} = 38$ and $\\operatorname{Var}(\\hat{Y}_{HT}) = 12.94$.", "answer": "$$ \\boxed{\n\\begin{pmatrix}\n38 & 12.94\n\\end{pmatrix}\n} $$", "id": "4951792"}]}