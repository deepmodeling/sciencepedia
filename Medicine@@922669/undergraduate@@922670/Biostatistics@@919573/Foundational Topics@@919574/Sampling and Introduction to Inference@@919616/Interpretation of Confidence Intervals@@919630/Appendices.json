{"hands_on_practices": [{"introduction": "A foundational task in biostatistics is estimating a population mean, such as the average response to a treatment, from a sample of data. When the sample size is small and the population variance is unknown, we must account for the extra uncertainty. This practice guides you through the construction of a Student's $t$-interval for a population mean $\\mu$, a cornerstone of statistical inference that demonstrates how to use a pivotal quantity and highlights the role of degrees of freedom in determining an interval's precision. [@problem_id:4918389]", "problem": "A biostatistician draws a simple random sample of size $n=12$ from a population assumed to follow a Normal distribution $\\mathcal{N}(\\mu,\\sigma^{2})$, where $\\sigma^{2}$ is unknown. The sample has mean $\\bar{x}=5.1$ and sample standard deviation $s=1.9$. Using only foundational principles about sampling distributions and pivotal quantities, construct the two-sided $95$ confidence interval for the mean $\\mu$ based on the Student’s $t$ distribution, and briefly explain, from first principles, how and why the degrees of freedom $n-1$ affect the interval’s width. For grading purposes, report as your final numerical answer the margin of error (that is, the half-width of the $95$ confidence interval). Round your final numerical answer to four significant figures.", "solution": "The problem is well-posed and scientifically sound, containing all necessary information to construct the confidence interval and analyze its properties.\n\nWe are given a simple random sample of size $n=12$ from a population assumed to follow a Normal distribution, $\\mathcal{N}(\\mu, \\sigma^2)$, with unknown mean $\\mu$ and unknown variance $\\sigma^2$. The sample mean is $\\bar{x}=5.1$ and the sample standard deviation is $s=1.9$. We are tasked with constructing a two-sided $95\\%$ confidence interval for $\\mu$.\n\nThe foundational principle for constructing this interval is the use of a pivotal quantity, which is a function of the sample data and the parameter of interest whose sampling distribution is known and does not depend on any unknown parameters.\n\nSince the population variance $\\sigma^2$ is unknown, we cannot use the standard normal ($Z$) statistic. Instead, we use the sample standard deviation $s$ as an estimate for $\\sigma$. The resulting pivotal quantity is the $t$-statistic, defined as:\n$$\nT = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}}\n$$\nwhere $\\bar{X}$ is the sample mean random variable and $S$ is the sample standard deviation random variable. By a fundamental theorem of statistics (originally from W.S. Gosset), this quantity $T$ follows a Student's $t$-distribution with $\\nu = n-1$ degrees of freedom.\n\nFor a two-sided $95\\%$ confidence interval, the level of significance is $\\alpha = 1 - 0.95 = 0.05$. We need to find the critical values $\\pm t_{\\alpha/2, \\nu}$ from the $t$-distribution with $\\nu = n-1$ degrees of freedom that bound the central $1-\\alpha=0.95$ of the probability mass. This is expressed as:\n$$\nP(-t_{\\alpha/2, \\nu} < T < t_{\\alpha/2, \\nu}) = 1-\\alpha\n$$\nSubstituting the expression for $T$, we have:\n$$\nP\\left(-t_{\\alpha/2, \\nu} < \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} < t_{\\alpha/2, \\nu}\\right) = 1-\\alpha\n$$\nTo find the interval for $\\mu$, we rearrange the inequalities:\n$$\n-t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}} < \\bar{X} - \\mu < t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}}\n$$\n$$\n-\\bar{X} - t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}} < -\\mu < -\\bar{X} + t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}}\n$$\nMultiplying by $-1$ reverses the inequalities:\n$$\n\\bar{X} - t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}} < \\mu < \\bar{X} + t_{\\alpha/2, \\nu} \\frac{S}{\\sqrt{n}}\n$$\nThe confidence interval is therefore given by $\\bar{x} \\pm E$, where $E$ is the margin of error. The margin of error is defined as:\n$$\nE = t_{\\alpha/2, \\nu} \\frac{s}{\\sqrt{n}}\n$$\nNow, we substitute the given values:\nSample size $n=12$.\nSample standard deviation $s=1.9$.\nDegrees of freedom $\\nu = n-1 = 12-1 = 11$.\nSignificance level $\\alpha = 0.05$, so $\\alpha/2 = 0.025$.\n\nWe need the critical value $t_{0.025, 11}$ from the Student's $t$-distribution with $11$ degrees of freedom. This value is the point such that the area in the upper tail is $0.025$. Consulting a $t$-distribution table or using statistical software, we find:\n$$\nt_{0.025, 11} \\approx 2.201\n$$\nNext, we calculate the standard error of the mean (SEM):\n$$\n\\text{SEM} = \\frac{s}{\\sqrt{n}} = \\frac{1.9}{\\sqrt{12}}\n$$\nThe margin of error is then:\n$$\nE = t_{0.025, 11} \\cdot \\frac{s}{\\sqrt{n}} \\approx 2.201 \\times \\frac{1.9}{\\sqrt{12}}\n$$\n$$\nE \\approx 2.201 \\times \\frac{1.9}{3.46410} \\approx 2.201 \\times 0.54856 \\approx 1.20736\n$$\nRounding to four significant figures, the margin of error is $1.207$.\n\nThe second part of the problem asks for an explanation of how and why the degrees of freedom ($df = n-1$) affect the interval's width. The width of the confidence interval is $2E = 2 \\cdot t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{n}}$. The degrees of freedom, $\\nu = n-1$, influence the width primarily through the critical value $t_{\\alpha/2, \\nu}$.\n\nFrom first principles, the Student's $t$-distribution is used instead of the Normal distribution to account for the additional uncertainty introduced by estimating the unknown population standard deviation $\\sigma$ with the sample standard deviation $s$. The shape of the $t$-distribution depends on the degrees of freedom, $\\nu$. For small values of $\\nu$ (i.e., small sample sizes), the $t$-distribution has heavier tails compared to the standard normal distribution. This means there is more probability in the tails, reflecting the greater uncertainty in the estimate $s$.\n\nAs the degrees of freedom $\\nu=n-1$ increase, the sample standard deviation $s$ becomes a more reliable estimate of $\\sigma$. Consequently, the $t$-distribution converges in shape to the standard normal distribution $\\mathcal{N}(0,1)$. For a fixed confidence level $1-\\alpha$, the critical value $t_{\\alpha/2, \\nu}$ is a strictly decreasing function of $\\nu$. A larger $\\nu$ corresponds to a smaller critical value $t_{\\alpha/2, \\nu}$, which moves closer to the corresponding $z_{\\alpha/2}$ value from the normal distribution.\nA smaller critical value directly results in a smaller margin of error $E$, and thus a narrower confidence interval. In summary, increasing the degrees of freedom reduces the penalty for estimating $\\sigma$, leading to a smaller critical value and a more precise (narrower) confidence interval for the mean $\\mu$.", "answer": "$$\\boxed{1.207}$$", "id": "4918389"}, {"introduction": "In clinical safety trials, a common and critical outcome is observing zero adverse events. This scenario poses a challenge for standard statistical methods, as it's incorrect to conclude the event rate is zero. This exercise demonstrates how to construct an \"exact\" one-sided upper confidence bound for the true event probability $p$ by returning to first principles: directly inverting a hypothesis test based on the Binomial distribution. [@problem_id:4918350] This technique is essential for making sound inferences from rare-event data and understanding the conservative nature of exact methods.", "problem": "A single-arm Phase II trial monitors a binary adverse event during an observation window following a vaccine dose. Among $n=40$ independent participants, zero events are observed ($x=0$). Assume a Binomial model $X \\sim \\mathrm{Binomial}(n,p)$ with event probability $p$ that is common across participants and independent across trials.\n\nUsing only the definition of a one-sided upper confidence bound through its frequentist coverage property and the distributional assumption above (that is, without normal approximations), derive an exact one-sided upper confidence bound for $p$ at confidence level $0.95$. Your derivation should start from the definition that a one-sided upper $(1-\\alpha)$ confidence bound $U(X)$ must satisfy $\\inf_{p \\in [0,1]} \\Pr_{p}\\big(p \\le U(X)\\big) \\ge 1-\\alpha$, and proceed by inverting an appropriate family of hypothesis tests for $H_{0}: p=p_{0}$ versus a one-sided alternative.\n\nThen compute the numerical value of the bound for $x=0$ and $n=40$ at confidence level $0.95$. Express your final bound as a decimal, and round your answer to four significant figures.\n\nFinally, briefly interpret, in clinical terms, what this bound means about plausible values of $p$ given the data, and what its coverage statement means in repeated sampling.", "solution": "The problem as stated is formally sound. It presents a standard scenario in biostatistics for calculating an exact confidence interval for a binomial proportion. The givens are complete and consistent, the objective is clearly defined, and the task is grounded in established statistical theory. Therefore, we proceed with the solution.\n\nThe problem asks for the derivation and calculation of a one-sided upper confidence bound for a binomial proportion $p$, based on an observation of $X=x$ events in $n$ trials. The confidence level is specified as $1-\\alpha$.\n\nThe derivation begins with the frequentist definition of a confidence interval, which is constructed by inverting a family of hypothesis tests. For a one-sided upper bound, we are looking for an interval of the form $[0, U(x)]$. This interval should contain all values of the parameter $p_0$ for which the null hypothesis $H_0: p=p_0$ is not rejected in favor of a specific alternative, given the observed data $x$.\n\nSince we seek an upper bound for $p$, a high posited value $p_0$ would be considered less plausible if the observed number of events $x$ is small. This leads us to formulate the hypothesis test for each possible value $p_0 \\in [0, 1]$ as:\n- Null Hypothesis $H_0: p = p_0$\n- Alternative Hypothesis $H_1: p < p_0$\n\nThe test statistic is the number of observed events, $X$, which follows a binomial distribution, $X \\sim \\mathrm{Binomial}(n, p)$. Under the null hypothesis, $X \\sim \\mathrm{Binomial}(n, p_0)$. Evidence against $H_0$ in favor of $H_1$ is provided by small values of $X$.\n\nThe p-value for this test is the probability of observing a result as extreme as or more extreme than the observed data $x$, under the assumption that $H_0$ is true. \"More extreme\" in this context means a value of the test statistic that provides stronger evidence for the alternative, which corresponds to values of $X \\le x$.\nTherefore, the p-value is given by:\n$$ \\text{p-value} = \\Pr_{p_0}(X \\le x) = \\sum_{k=0}^{x} \\binom{n}{k} p_0^k (1-p_0)^{n-k} $$\n\nWe reject the null hypothesis $H_0: p=p_0$ at a significance level $\\alpha$ if the p-value is less than or equal to $\\alpha$.\n$$ \\Pr_{p_0}(X \\le x) \\le \\alpha $$\n\nThe $(1-\\alpha)$ confidence set for $p$ is the collection of all values $p_0$ for which we do *not* reject $H_0$. That is, the set of all $p_0$ such that:\n$$ \\Pr_{p_0}(X \\le x) > \\alpha $$\n\nThe one-sided upper confidence bound, which we denote as $p_U$, is the supremum of this set. The function $f(p_0) = \\Pr_{p_0}(X \\le x)$ is a monotonically decreasing function of $p_0$. Consequently, the upper bound $p_U$ is the value of $p$ that satisfies the boundary condition:\n$$ \\Pr_{p_U}(X \\le x) = \\alpha $$\nThis method of constructing an interval is known as the Clopper-Pearson method for exact confidence intervals.\n\nNow, we apply this general formula to the specific data provided in the problem:\n- Sample size: $n = 40$\n- Observed events: $x = 0$\n- Confidence level: $1-\\alpha = 0.95$, which implies $\\alpha = 0.05$\n\nSubstituting these values into the equation for the upper bound $p_U$:\n$$ \\Pr_{p_U}(X \\le 0) = 0.05 $$\nFor a binomial distribution, the event $X \\le 0$ is equivalent to the event $X=0$. Thus, the equation simplifies to:\n$$ \\Pr_{p_U}(X = 0) = 0.05 $$\nThe probability mass function for a binomial random variable at $k=0$ is $\\binom{n}{0} p^0 (1-p)^{n-0} = (1-p)^n$. Applying this to our case:\n$$ (1-p_U)^{40} = 0.05 $$\nTo solve for $p_U$, we take the $40$-th root of both sides:\n$$ 1 - p_U = (0.05)^{1/40} $$\nFinally, we isolate $p_U$:\n$$ p_U = 1 - (0.05)^{1/40} $$\n\nWe now compute the numerical value:\n$$ p_U = 1 - (0.05)^{0.025} $$\n$$ p_U \\approx 1 - 0.92784158 $$\n$$ p_U \\approx 0.07215842 $$\nRounding to four significant figures, the result is $0.07216$.\n\nThe one-sided $95\\%$ upper confidence bound for the adverse event probability $p$ is approximately $0.07216$.\n\nThe interpretation of this result is twofold:\n1.  **Interpretation of the Bound**: Given the observation of zero adverse events in $40$ participants, we are $95\\%$ confident that the true, unknown probability $p$ of an adverse event is no more than $0.07216$, or $7.216\\%$. Values of $p$ greater than $0.07216$ are considered inconsistent with the observed data, because if the true rate were higher than this, the probability of observing zero events would have been less than $5\\%$.\n2.  **Interpretation of the Coverage Property**: The statement \"95% confident\" refers to the long-run performance of the statistical procedure used to generate the bound. If we were to repeat this study (i.e., recruit many new samples of $40$ participants) and calculate a one-sided $95\\%$ upper bound from each sample, we would expect at least $95\\%$ of these calculated upper bounds to be greater than or equal to the true, fixed value of $p$. It is a statement about the reliability of the method, not a probabilistic statement about the true parameter $p$ itself. The true parameter is a fixed, not a random, quantity.", "answer": "$$\n\\boxed{0.07216}\n$$", "id": "4918350"}, {"introduction": "For skewed data like survival times, the median is often a more robust measure of central tendency than the mean. This hands-on coding practice explores two powerful, distribution-free methods for constructing a confidence interval for the population median $m$. By implementing and comparing an exact interval derived from the sign test with a modern computational approach—the bootstrap percentile method—you will gain practical experience in evaluating the trade-offs between classical and computer-intensive techniques in realistic data scenarios. [@problem_id:4918314]", "problem": "You are provided with $3$ independent and identically distributed samples of strictly positive survival times measured in months. For each sample, you must compute a two-sided confidence interval for the population median survival time at confidence level $1-\\alpha = 0.95$ using two approaches: the bootstrap percentile method and the exact sign-test-based method obtained by inverting a Binomial test. You must then compare the two intervals by reporting widths and simple inclusion relations. All survival-time quantities should be treated as being in months; express all outputs as real numbers without appending units.\n\nFundamental base and constraints:\n- Let $X_{1},\\dots,X_{n}$ be independent and identically distributed from a continuous distribution with cumulative distribution function $F$. The population median $m$ is the point satisfying $F(m)=0.5$.\n- If $m$ is the population median and the distribution is continuous, then the count $S = \\sum_{i=1}^{n} \\mathbf{1}\\{X_{i} \\le m\\}$ follows a Binomial distribution with parameters $(n, 0.5)$. This fact underlies exact, distribution-free, sign-test-based confidence intervals for $m$ that can be expressed in terms of order statistics $X_{(1)} \\le \\dots \\le X_{(n)}$.\n- The bootstrap percentile method constructs a confidence interval by resampling with replacement from the observed data to approximate the sampling distribution of an estimator, and then taking empirical quantiles of the bootstrap distribution of that estimator.\n\nProgram requirements:\n- Use the confidence level $1-\\alpha = 0.95$, with $\\alpha = 0.05$.\n- Use $B = 20000$ bootstrap replicates for each sample and set a fixed random seed to ensure reproducibility.\n- For each sample, compute the following in order:\n  1. The sample median $\\hat{m}$.\n  2. The bootstrap percentile interval $[L_{b}, U_{b}]$ formed by the empirical quantiles at levels $\\alpha/2$ and $1-\\alpha/2$ of the bootstrap distribution of the sample median.\n  3. The exact sign-test-based interval $[L_{s}, U_{s}]$ obtained by inverting the Binomial test for the median under the continuous-distribution assumption and expressing the interval via order statistics $[X_{(L)}, X_{(U)}]$ for appropriate integer indices $L$ and $U$, chosen so that the two-sided tail probabilities for $S \\sim \\text{Binomial}(n, 0.5)$ are each no greater than $\\alpha/2$.\n  4. The interval widths $W_{b} = U_{b} - L_{b}$ and $W_{s} = U_{s} - L_{s}$.\n  5. The boolean indicators $\\mathbf{1}\\{\\hat{m} \\in [L_{b}, U_{b}]\\}$, $\\mathbf{1}\\{\\hat{m} \\in [L_{s}, U_{s}]\\}$, and $\\mathbf{1}\\{[L_{b}, U_{b}] \\subseteq [L_{s}, U_{s}]\\}$.\n\nTest suite:\n- Use the following $3$ samples of survival times (months). Treat these as exact real numbers:\n  - Sample $1$ (moderately symmetric, $n=12$): $[12.1, 10.5, 9.7, 11.2, 13.4, 10.9, 12.6, 9.9, 11.5, 12.0, 10.8, 11.1]$.\n  - Sample $2$ (right-skewed, $n=14$): $[3.2, 5.1, 4.0, 6.7, 8.5, 12.3, 7.1, 4.8, 15.6, 9.4, 10.2, 5.5, 6.0, 20.0]$.\n  - Sample $3$ (ties near the middle, $n=12$): $[6.0, 6.0, 6.0, 7.0, 8.0, 9.0, 10.0, 10.0, 11.0, 12.0, 12.0, 13.0]$.\n\nFinal output format:\n- For each sample, return a list in the exact order $[\\hat{m}, L_{b}, U_{b}, L_{s}, U_{s}, W_{b}, W_{s}, \\mathbf{1}\\{\\hat{m} \\in [L_{b}, U_{b}]\\}, \\mathbf{1}\\{\\hat{m} \\in [L_{s}, U_{s}]\\}, \\mathbf{1}\\{[L_{b}, U_{b}] \\subseteq [L_{s}, U_{s}]\\}]$.\n- Aggregate the results for the $3$ samples into a single list of $3$ lists.\n- Your program should produce a single line of output containing this aggregate result as a comma-separated list enclosed in square brackets with no spaces (for example, $[r_{1},r_{2},r_{3}]$ where each $r_{i}$ is itself a list in the specified order).\n\nNotes on interpretation:\n- Express all time-related results as real numbers in months without appending units.\n- Any proportions must be expressed as decimals, not with a percent sign.\n- The exact sign-test-based interval is finite-sample and distribution-free under the continuity assumption, while the bootstrap percentile interval is an asymptotic approximation based on the observed sample; comparing widths and inclusion provides insight into potential conservatism or undercoverage in finite samples.", "solution": "We begin by formalizing the targets and the fundamental facts that justify the two intervals.\n\nLet $X_{1},\\dots,X_{n}$ be independent and identically distributed survival times drawn from a continuous distribution with cumulative distribution function $F$. The population median $m$ is the $0.5$ quantile, that is the unique value satisfying $F(m)=0.5$. The sample median $\\hat{m}$ is the empirical $0.5$ quantile and can be written in terms of the order statistics $X_{(1)} \\le \\dots \\le X_{(n)}$.\n\nDerivation of the exact sign-test-based interval. Under the assumption of continuity at the true median $m$, the indicators $\\mathbf{1}\\{X_{i} \\le m\\}$ are independent Bernoulli random variables with success probability $0.5$. Therefore, the sum $S=\\sum_{i=1}^{n} \\mathbf{1}\\{X_{i} \\le m\\}$ has the Binomial distribution with parameters $(n,0.5)$. Consider a two-sided confidence set for $m$ defined by the set of values $m$ that are not rejected by a two-sided sign test at level $\\alpha$, which yields an exact, finite-sample, distribution-free confidence interval expressed via order statistics. Specifically, choose an integer $k \\in \\{-1,0,1,\\dots\\}$ as large as possible such that $\\mathbb{P}(S \\le k) \\le \\alpha/2$, where $S \\sim \\text{Binomial}(n, 0.5)$. By symmetry, $\\mathbb{P}(S \\ge n-k) \\le \\alpha/2$. The corresponding order-statistic indices are $L = k+1$ and $U = n-k$. The exact sign-test-based confidence interval for $m$ is then\n$$\n[L_{s}, U_{s}] = [X_{(L)}, X_{(U)}].\n$$\nThis interval satisfies $\\mathbb{P}(m < X_{(L)}) \\le \\alpha/2$ and $\\mathbb{P}(m > X_{(U)}) \\le \\alpha/2$, hence has coverage at least $1-\\alpha$. For small $n$ and stringent $\\alpha$, the interval can be conservative because the Binomial cumulative probabilities are discrete. In implementation, we compute the Binomial cumulative distribution function $F_{S}(k)=\\mathbb{P}(S \\le k)$ and select $k$ accordingly; in the edge case that $k=-1$, the indices would imply unbounded endpoints, which can be practically truncated to the minimum and maximum observed values in the sample, though our test sizes avoid this degeneracy.\n\nDerivation of the bootstrap percentile interval. Let $\\hat{m}=\\hat{m}(X_{1},\\dots,X_{n})$ denote the sample median. The nonparametric bootstrap approximates the sampling distribution of $\\hat{m}$ by resampling with replacement from the empirical distribution $\\hat{F}_{n}$ that places mass $1/n$ at each $X_{i}$. Generate $B$ resamples of size $n$, compute the bootstrap medians $\\hat{m}^{*(1)},\\dots,\\hat{m}^{*(B)}$, and approximate the distribution of $\\hat{m}$ by the empirical distribution of these $\\hat{m}^{*}$. The percentile method defines the two-sided $(1-\\alpha)$ confidence interval as\n$$\n[L_{b}, U_{b}] = \\left[ Q_{\\hat{m}^{*}}(\\alpha/2), \\; Q_{\\hat{m}^{*}}(1-\\alpha/2) \\right],\n$$\nwhere $Q_{\\hat{m}^{*}}(p)$ is the empirical $p$-quantile of the bootstrap medians. This method is asymptotically valid under regularity and is transformation-respecting, but can underperform for small samples, severe skewness, or strong discreteness/ties.\n\nAlgorithmic steps for each sample:\n- Compute the sample median $\\hat{m}$.\n- Compute the bootstrap percentile interval:\n  - Fix a reproducible random seed.\n  - Generate $B$ bootstrap resamples by drawing indices uniformly with replacement from $\\{1,\\dots,n\\}$ and compute the median for each resample, obtaining $\\hat{m}^{*(b)}$ for $b=1,\\dots,B$.\n  - Compute the empirical quantiles at levels $\\alpha/2$ and $1-\\alpha/2$ to obtain $L_{b}$ and $U_{b}$.\n- Compute the sign-test-based interval:\n  - Let $F_{S}(k)=\\mathbb{P}(S \\le k)$ for $S \\sim \\text{Binomial}(n, 0.5)$. Find the largest integer $k$ such that $F_{S}(k) \\le \\alpha/2$.\n  - Set $L=k+1$ and $U=n-k$. Sort the sample and take $L_{s}=X_{(L)}$ and $U_{s}=X_{(U)}$.\n- Compute the widths $W_{b}=U_{b}-L_{b}$ and $W_{s}=U_{s}-L_{s}$, plus booleans $\\mathbf{1}\\{\\hat{m} \\in [L_{b}, U_{b}]\\}$, $\\mathbf{1}\\{\\hat{m} \\in [L_{s}, U_{s}]\\}$, and $\\mathbf{1}\\{[L_{b}, U_{b}] \\subseteq [L_{s}, U_{s}]\\}$.\n\nInterpretation relative to exact sign-test-based intervals:\n- The sign-test-based interval $[L_{s}, U_{s}]$ is exact in finite samples and distribution-free under the assumption that $F$ is continuous at the median, but it is typically conservative because it is based on discrete Binomial tail probabilities. This often produces larger $W_{s}$.\n- The bootstrap percentile interval $[L_{b}, U_{b}]$ is an approximate interval that uses the observed data as a stand-in for the population. It may be narrower ($W_{b} < W_{s}$), which can be beneficial in terms of precision but may risk undercoverage in small or skewed samples.\n- When $[L_{b}, U_{b}] \\subseteq [L_{s}, U_{s}]$, the bootstrap interval is consistent with and tighter than the exact nonparametric bounds, suggesting limited small-sample distortion. If $[L_{b}, U_{b}]$ protrudes outside $[L_{s}, U_{s}]$, it reflects the bootstrap’s approximation error or the effects of skewness/ties.\n- Across the provided test samples with $\\alpha=0.05$ and $B=20000$, we expect the sign-test-based intervals to be at least as wide as the bootstrap intervals due to conservatism, especially for moderate $n$. The inclusion indicators and widths quantify this relationship for each dataset in months.\n\nImplementation details:\n- We use $B=20000$ and a fixed seed to stabilize bootstrap quantiles.\n- The Binomial cumulative distribution function $F_{S}(k)$ is evaluated exactly using the Binomial model with probability $0.5$ to determine the indices $L$ and $U$.\n- All results are reported as real numbers in months without appending units, and the final output is a single line containing a list of $3$ lists in the specified order and format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom\nimport json\n\ndef bootstrap_percentile_ci(data, alpha=0.05, B=20000, rng=None):\n    \"\"\"\n    Bootstrap percentile CI for the sample median.\n    data: 1D numpy array\n    alpha: significance level\n    B: number of bootstrap replicates\n    rng: numpy random Generator\n    Returns (L_b, U_b)\n    \"\"\"\n    n = data.size\n    if rng is None:\n        rng = np.random.default_rng()\n    # Sample bootstrap indices and compute medians\n    idx = rng.integers(0, n, size=(B, n))\n    samples = data[idx]\n    boot_medians = np.median(samples, axis=1)\n    # Percentile method: empirical quantiles at alpha/2 and 1-alpha/2\n    lower = np.quantile(boot_medians, alpha/2, method=\"linear\")\n    upper = np.quantile(boot_medians, 1 - alpha/2, method=\"linear\")\n    return float(lower), float(upper)\n\ndef sign_test_exact_ci(data, alpha=0.05):\n    \"\"\"\n    Exact sign-test-based CI for the population median using order statistics.\n    data: 1D numpy array\n    alpha: significance level\n    Returns (L_s, U_s)\n    \"\"\"\n    x = np.sort(np.asarray(data, dtype=float))\n    n = x.size\n    # Find largest k such that P(S <= k) <= alpha/2, S ~ Bin(n, 0.5)\n    # Start from k = 0 upwards until CDF exceeds alpha/2\n    half_alpha = alpha / 2.0\n    k = -1\n    # We increment j and track cdf; when cdf > half_alpha, k is j-1\n    j = 0\n    while True:\n        cdf = binom.cdf(j, n, 0.5)\n        if cdf <= half_alpha:\n            k = j\n            j += 1\n            if j > n:\n                break\n        else:\n            break\n    # If k == -1, the interval would be unbounded; truncate to observed min/max.\n    L_index_1based = k + 1  # order statistic index L\n    U_index_1based = n - k  # order statistic index U\n    # Convert to 0-based indices and clip to [0, n-1]\n    lower_idx = max(L_index_1based - 1, 0)\n    upper_idx = min(U_index_1based - 1, n - 1)\n    return float(x[lower_idx]), float(x[upper_idx])\n\ndef analyze_sample(data, alpha=0.05, B=20000, rng=None, round_decimals=4):\n    data = np.asarray(data, dtype=float)\n    sample_median = float(np.median(data))\n    Lb, Ub = bootstrap_percentile_ci(data, alpha=alpha, B=B, rng=rng)\n    Ls, Us = sign_test_exact_ci(data, alpha=alpha)\n    Wb = Ub - Lb\n    Ws = Us - Ls\n    median_in_boot = (Lb <= sample_median <= Ub)\n    median_in_sign = (Ls <= sample_median <= Us)\n    boot_within_sign = (Lb >= Ls) and (Ub <= Us)\n    # Round floats for stable, clean output\n    def r(x):\n        return float(np.round(x, round_decimals))\n    return [\n        r(sample_median),\n        r(Lb),\n        r(Ub),\n        r(Ls),\n        r(Us),\n        r(Wb),\n        r(Ws),\n        bool(median_in_boot),\n        bool(median_in_sign),\n        bool(boot_within_sign),\n    ]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # All times are in months.\n    sample1 = [12.1, 10.5, 9.7, 11.2, 13.4, 10.9, 12.6, 9.9, 11.5, 12.0, 10.8, 11.1]\n    sample2 = [3.2, 5.1, 4.0, 6.7, 8.5, 12.3, 7.1, 4.8, 15.6, 9.4, 10.2, 5.5, 6.0, 20.0]\n    sample3 = [6.0, 6.0, 6.0, 7.0, 8.0, 9.0, 10.0, 10.0, 11.0, 12.0, 12.0, 13.0]\n\n    alpha = 0.05\n    B = 20000\n    rng = np.random.default_rng(20251111)\n\n    test_cases = [\n        sample1,\n        sample2,\n        sample3,\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_sample(case, alpha=alpha, B=B, rng=rng, round_decimals=4)\n        results.append(result)\n\n    # Final print statement in the exact required format: single line, no spaces.\n    print(json.dumps(results, separators=(\",\", \":\")))\n\nsolve()\n```", "id": "4918314"}]}