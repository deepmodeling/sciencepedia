{"hands_on_practices": [{"introduction": "Understanding a theoretical distribution can be greatly enhanced by learning how to simulate it. This first practice exercise provides a direct bridge between the abstract formula for the sampling distribution of the variance and a concrete computational algorithm. By figuring out how to generate a random value of the sample variance $S^2$ from a chi-squared distribution, you will gain a practical understanding of the pivotal relationship $\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}$ [@problem_id:1953257].", "problem": "A quality control engineer is performing a statistical simulation to study the variability of a manufacturing process. The process produces ball bearings whose diameters are known to be normally distributed with a mean $\\mu$ and a variance $\\sigma^2$. The engineer plans to simulate the collection of a random sample of $n$ ball bearings and the calculation of the sample variance, defined as $S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2$, where $\\bar{X}$ is the sample mean.\n\nTo perform the simulation efficiently, the engineer will use a statistical software library that provides a function, `chi2.rand(df)`, which returns a single random variate from a chi-squared distribution with `df` degrees of freedom. The goal is to design an algorithm that generates a single simulated value of the sample variance, denoted $s^2$, by directly using the `chi2.rand(df)` function.\n\nWhich of the following algorithms correctly describes how to generate one such value $s^2$ from the sampling distribution of $S^2$?\n\nA. Generate a random value $y$ using `y = chi2.rand(n-1)`. Calculate the simulated sample variance as $s^2 = \\frac{\\sigma^2 y}{n-1}$.\n\nB. Generate a random value $y$ using `y = chi2.rand(n)`. Calculate the simulated sample variance as $s^2 = \\frac{\\sigma^2 y}{n}$.\n\nC. Generate $n$ random values $x_1, \\dots, x_n$ from a standard normal distribution $N(0, 1)$. Calculate the simulated value as $s^2 = \\sum_{i=1}^n x_i^2$.\n\nD. Generate a random value $y$ using `y = chi2.rand(n-1)`. Calculate the simulated sample variance as $s^2 = \\frac{\\sigma^2 y}{n}$.\n\nE. Generate a random value $y$ using `y = chi2.rand(n)`. Calculate the simulated sample variance as $s^2 = \\frac{(n-1) y}{\\sigma^2}$.", "solution": "Let $X_{1},\\dots,X_{n}$ be independent and identically distributed as $N(\\mu,\\sigma^{2})$. The sample variance is\n$$\nS^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2},\n$$\nwhere $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$.\n\nStandardize by defining $Z_{i}=\\frac{X_{i}-\\mu}{\\sigma}$ for $i=1,\\dots,n$. Then $Z_{i}\\sim N(0,1)$ i.i.d., and the sample mean of the standardized variables is $\\bar{Z}=\\frac{\\bar{X}-\\mu}{\\sigma}$. Using the identity\n$$\n\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}=\\sigma^{2}\\sum_{i=1}^{n}\\left(Z_{i}-\\bar{Z}\\right)^{2},\n$$\nwe rewrite the sample variance as\n$$\nS^{2}=\\frac{\\sigma^{2}}{n-1}\\sum_{i=1}^{n}\\left(Z_{i}-\\bar{Z}\\right)^{2}.\n$$\nBy Cochran's theorem (or the standard result for normal samples), the quadratic form\n$$\n\\sum_{i=1}^{n}\\left(Z_{i}-\\bar{Z}\\right)^{2}\n$$\nhas a chi-squared distribution with $n-1$ degrees of freedom, i.e.,\n$$\n\\frac{(n-1)S^{2}}{\\sigma^{2}}\\sim \\chi^{2}_{n-1}.\n$$\nEquivalently, if $Y\\sim \\chi^{2}_{n-1}$, then\n$$\nS^{2}=\\frac{\\sigma^{2}}{n-1}Y.\n$$\nTherefore, to simulate one draw $s^{2}$ from the sampling distribution of $S^{2}$, generate $y=\\texttt{chi2.rand}(n-1)$ and set\n$$\ns^{2}=\\frac{\\sigma^{2}y}{n-1}.\n$$\nThis is exactly option A. The other options either use the wrong degrees of freedom, the wrong scaling, or do not produce the sample variance distribution centered about the sample mean (which reduces the degrees of freedom by one).", "answer": "$$\\boxed{A}$$", "id": "1953257"}, {"introduction": "In statistics, we often have multiple ways to estimate the same parameter, which raises the question: which estimator is \"best\"? This problem delves into the classic bias-variance trade-off by comparing the Mean Squared Error (MSE) of two different estimators for population variance: the unbiased sample variance $S^2$ and the biased Maximum Likelihood Estimator (MLE) $\\hat{\\sigma}^2$. Working through this exercise [@problem_id:1953254] will reveal the important lesson that an unbiased estimator is not always the one with the lowest overall error.", "problem": "In statistical estimation, the quality of an estimator is often evaluated using its Mean Squared Error (MSE). Consider a random sample $X_1, X_2, \\ldots, X_n$ of size $n \\ge 2$ drawn from a normal distribution with an unknown mean $\\mu$ and an unknown positive variance $\\sigma^2$.\n\nTwo common estimators for the population variance $\\sigma^2$ are:\n1.  The Maximum Likelihood Estimator (MLE), given by $\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$, where $\\bar{X}$ is the sample mean. This estimator is known to be biased.\n2.  The unbiased sample variance, given by $S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$.\n\nThe MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{MSE}(\\hat{\\theta}) = \\text{E}[(\\hat{\\theta} - \\theta)^2]$.\n\nFor which integer values of the sample size $n$ is the MSE of the biased MLE, $\\text{MSE}(\\hat{\\sigma}^2)$, strictly smaller than the MSE of the unbiased estimator, $\\text{MSE}(S^2)$?\n\nA. For all $n \\ge 2$\n\nB. For no values of $n \\ge 2$\n\nC. Only for $n=2$\n\nD. Only for $n > 21$\n\nE. For $2 \\le n \\le 20$", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. $\\mathcal{N}(\\mu,\\sigma^{2})$ with $n \\ge 2$. Define the unbiased sample variance $S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$ and the MLE $\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$. These satisfy the relation\n$$\n\\hat{\\sigma}^{2}=\\frac{n-1}{n}S^{2}.\n$$\n\nA standard result for normal samples is\n$$\n\\frac{(n-1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}.\n$$\nHence $E[S^{2}]=\\sigma^{2}$ and, using $\\mathrm{Var}(\\chi^{2}_{\\nu})=2\\nu$, we get\n$$\n\\mathrm{Var}(S^{2})=\\frac{\\sigma^{4}}{(n-1)^{2}}\\mathrm{Var}\\!\\left(\\chi^{2}_{n-1}\\right)=\\frac{\\sigma^{4}}{(n-1)^{2}}\\cdot 2(n-1)=\\frac{2\\sigma^{4}}{n-1}.\n$$\nTherefore the MSE of the unbiased estimator is\n$$\n\\mathrm{MSE}(S^{2})=\\mathrm{Var}(S^{2})=\\frac{2\\sigma^{4}}{n-1}.\n$$\n\nFor the MLE, using $\\hat{\\sigma}^{2}=\\frac{n-1}{n}S^{2}$ and the above distribution, we also have\n$$\n\\frac{n\\hat{\\sigma}^{2}}{\\sigma^{2}}=\\frac{(n-1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}_{n-1}.\n$$\nThus\n$$\nE[\\hat{\\sigma}^{2}]=\\sigma^{2}\\cdot \\frac{n-1}{n}, \\quad \\mathrm{Bias}(\\hat{\\sigma}^{2})=E[\\hat{\\sigma}^{2}]-\\sigma^{2}=-\\frac{\\sigma^{2}}{n},\n$$\nand\n$$\n\\mathrm{Var}(\\hat{\\sigma}^{2})=\\frac{\\sigma^{4}}{n^{2}}\\mathrm{Var}\\!\\left(\\chi^{2}_{n-1}\\right)=\\frac{\\sigma^{4}}{n^{2}}\\cdot 2(n-1)=\\frac{2\\sigma^{4}(n-1)}{n^{2}}.\n$$\nTherefore\n$$\n\\mathrm{MSE}(\\hat{\\sigma}^{2})=\\mathrm{Var}(\\hat{\\sigma}^{2})+\\mathrm{Bias}(\\hat{\\sigma}^{2})^{2}=\\sigma^{4}\\cdot \\frac{2n-1}{n^{2}}.\n$$\n\nTo compare the MSEs, we require\n$$\n\\mathrm{MSE}(\\hat{\\sigma}^{2})\\mathrm{MSE}(S^{2}) \\quad \\Longleftrightarrow \\quad \\frac{2n-1}{n^{2}}\\frac{2}{n-1}.\n$$\nSince $n \\ge 2$, denominators are positive; cross-multiplying yields\n$$\n(2n-1)(n-1)2n^{2} \\;\\Longleftrightarrow\\; 2n^{2}-3n+12n^{2} \\;\\Longleftrightarrow\\; -3n+10 \\;\\Longleftrightarrow\\; n\\frac{1}{3}.\n$$\nThis holds for all integers $n \\ge 2$. Hence $\\mathrm{MSE}(\\hat{\\sigma}^{2})$ is strictly smaller than $\\mathrm{MSE}(S^{2})$ for all $n \\ge 2$.", "answer": "$$\\boxed{A}$$", "id": "1953254"}, {"introduction": "A primary goal of understanding sampling distributions is to make inferences about population parameters. This exercise puts the theory into practice by asking you to determine the bounds that contain a specific percentage of the probable values for the pivotal quantity $Q = \\frac{(n-1)S^2}{\\sigma^2}$. Mastering this skill [@problem_id:1953268] is a direct stepping stone to constructing confidence intervals for the population variance $\\sigma^2$, a fundamental tool in quality control and scientific research.", "problem": "A precision engineering firm manufactures engine components whose critical dimension is designed to follow a normal distribution. While the mean dimension can be adjusted, the process variability, represented by the population variance $\\sigma^2$, is a key indicator of manufacturing quality. A quality control team randomly samples $n=25$ components from the production line.\n\nLet $S^2$ be the sample variance of the critical dimension, calculated from this sample. To monitor the process variability independently of the true variance $\\sigma^2$, the team analyzes the dimensionless statistic $Q = \\frac{(n-1)S^2}{\\sigma^2}$.\n\nThe objective is to find a central probability interval $[a, b]$ for $Q$ that contains 90% of the probability. This means the interval is defined by $P(a \\le Q \\le b) = 0.90$, with the remaining probability split equally in the tails, i.e., $P(Q  a) = P(Q  b)$.\n\nYou are given the following two facts about the probability distribution of the statistic $Q$ for a sample of this size:\n1. The probability that $Q$ is greater than 13.848 is 0.95.\n2. The probability that $Q$ is greater than 36.415 is 0.05.\n\nDetermine the numerical values for the lower bound $a$ and the upper bound $b$ of this central 90% probability interval. Provide your answer as two numerical values for $a$ and $b$, respectively. The values should be stated exactly as given in the problem information.", "solution": "For a normally distributed population, the statistic\n$$\nQ=\\frac{(n-1)S^{2}}{\\sigma^{2}}\n$$\nhas a chi-square distribution with degrees of freedom $n-1$. With $n=25$, we have $Q \\sim \\chi^{2}_{24}$.\n\nA central $90\\%$ probability interval $[a,b]$ satisfies\n$$\nP(Qa)=0.05, \\quad P(Qb)=0.05,\n$$\nso $a$ is the $0.05$ quantile and $b$ is the $0.95$ quantile of the $\\chi^{2}_{24}$ distribution.\n\nThe given facts translate these tail probabilities into quantiles:\n- $P(Q13.848)=0.95$ implies $P(Q \\le 13.848)=0.05$, so $a=13.848$.\n- $P(Q36.415)=0.05$ implies $P(Q \\le 36.415)=0.95$, so $b=36.415$.\n\nThus, the central $90\\%$ interval is $[13.848,\\,36.415]$.", "answer": "$$\\boxed{\\begin{pmatrix} 13.848  36.415 \\end{pmatrix}}$$", "id": "1953268"}]}