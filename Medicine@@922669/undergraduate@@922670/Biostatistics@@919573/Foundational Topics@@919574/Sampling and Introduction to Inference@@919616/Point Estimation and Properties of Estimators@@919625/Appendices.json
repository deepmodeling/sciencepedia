{"hands_on_practices": [{"introduction": "Maximum Likelihood Estimation (MLE) is one of the most powerful and widely used methods for deriving point estimators, based on the intuitive principle of finding the parameter value that makes the observed data most probable. This first exercise [@problem_id:4937856] provides foundational practice in deriving an MLE from first principles for the exponential distribution. Mastering this process is essential, as the exponential model is frequently used in biostatistics to represent waiting times or survival durations.", "problem": "In a study of inter-arrival times of rare biological events, suppose you observe an independent and identically distributed sample $X_{1}, X_{2}, \\ldots, X_{n}$ from an exponential model with rate parameter $\\theta$, where $\\theta0$. The probability density function is $f(x\\mid\\theta)=\\theta \\exp(-\\theta x)$ for $x0$. Using only the definition of the likelihood function for independent observations and the principle of maximizing it with respect to the parameter, derive from first principles the maximum likelihood estimator $\\hat{\\theta}$ of $\\theta$ under the assumptions stated. Carefully justify that your solution is indeed a maximizer over the parameter space $\\{\\theta0\\}$. Express your final answer as a single closed-form expression in terms of the sample mean $\\bar{X}$, where $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. No numerical approximation is required, and no units are involved.", "solution": "The problem requires the derivation of the maximum likelihood estimator (MLE) for the rate parameter $\\theta$ of an exponential distribution, based on an independent and identically distributed (i.i.d.) sample $X_{1}, X_{2}, \\ldots, X_{n}$. The derivation must proceed from first principles, and the solution must be rigorously justified as a maximum over the parameter space $\\theta  0$.\n\nFirst, we establish the likelihood function, $L(\\theta)$. For an i.i.d. sample, the likelihood function is the product of the individual probability density functions (PDFs) evaluated at each sample point. The PDF is given as $f(x_i \\mid \\theta) = \\theta \\exp(-\\theta x_i)$ for $x_i  0$.\nThe likelihood function is therefore:\n$$L(\\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta) = \\prod_{i=1}^{n} \\left[ \\theta \\exp(-\\theta x_i) \\right]$$\nBy properties of products and exponents, this simplifies to:\n$$L(\\theta) = \\theta^n \\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right)$$\n\nTo find the value of $\\theta$ that maximizes $L(\\theta)$, it is computationally more convenient to maximize the natural logarithm of the likelihood function, known as the log-likelihood function, $\\ell(\\theta) = \\ln(L(\\theta))$. Since the natural logarithm is a strictly increasing function, the value of $\\theta$ that maximizes $\\ell(\\theta)$ will also maximize $L(\\theta)$.\nThe log-likelihood function is:\n$$\\ell(\\theta) = \\ln\\left( \\theta^n \\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right) \\right)$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(a^b) = b\\ln(a)$, we get:\n$$\\ell(\\theta) = \\ln(\\theta^n) + \\ln\\left(\\exp\\left(-\\theta \\sum_{i=1}^{n} x_i\\right)\\right)$$\n$$\\ell(\\theta) = n \\ln(\\theta) - \\theta \\sum_{i=1}^{n} x_i$$\nThis function is defined for $\\theta  0$, which is the specified parameter space.\n\nTo find the maximum, we first find the critical points by taking the first derivative of $\\ell(\\theta)$ with respect to $\\theta$ and setting it to zero. This derivative is often called the score function.\n$$\\frac{d\\ell(\\theta)}{d\\theta} = \\frac{d}{d\\theta}\\left( n \\ln(\\theta) - \\theta \\sum_{i=1}^{n} x_i \\right) = \\frac{n}{\\theta} - \\sum_{i=1}^{n} x_i$$\nSetting the derivative to zero to find the candidate estimator, which we denote $\\hat{\\theta}$:\n$$\\frac{n}{\\hat{\\theta}} - \\sum_{i=1}^{n} x_i = 0$$\n$$\\frac{n}{\\hat{\\theta}} = \\sum_{i=1}^{n} x_i$$\nSolving for $\\hat{\\theta}$ yields:\n$$\\hat{\\theta} = \\frac{n}{\\sum_{i=1}^{n} x_i}$$\nThe problem asks for the answer in terms of the sample mean, $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. We can rewrite the denominator as $\\sum_{i=1}^{n} x_i = n \\bar{x}$. Substituting this into the expression for $\\hat{\\theta}$:\n$$\\hat{\\theta} = \\frac{n}{n\\bar{x}} = \\frac{1}{\\bar{x}}$$\nThus, the MLE for $\\theta$ is the reciprocal of the sample mean, $\\hat{\\theta} = \\frac{1}{\\bar{X}}$.\n\nNext, we must justify that this critical point corresponds to a maximum. We use the second derivative test. The second derivative of the log-likelihood function is:\n$$\\frac{d^2\\ell(\\theta)}{d\\theta^2} = \\frac{d}{d\\theta}\\left( \\frac{n}{\\theta} - \\sum_{i=1}^{n} x_i \\right) = -\\frac{n}{\\theta^2}$$\nThe parameter space for $\\theta$ is $\\{\\theta \\mid \\theta  0\\}$. The sample size $n$ is a positive integer. Therefore, for any $\\theta$ in the parameter space, $\\theta^2  0$ and $n  0$, which implies that the second derivative is always negative:\n$$\\frac{d^2\\ell(\\theta)}{d\\theta^2} = -\\frac{n}{\\theta^2}  0$$\nA negative second derivative over the entire domain indicates that the log-likelihood function $\\ell(\\theta)$ is strictly concave. A strictly concave function has at most one critical point, which, if it exists, must be a unique global maximum. Our calculation has found this unique critical point at $\\hat{\\theta} = \\frac{1}{\\bar{X}}$.\nTo be fully rigorous, we also examine the behavior of $\\ell(\\theta)$ at the boundaries of the parameter space $(0, \\infty)$.\nAs $\\theta \\to 0^+$, the term $n\\ln(\\theta) \\to -\\infty$, so $\\ell(\\theta) \\to -\\infty$.\nAs $\\theta \\to \\infty$, we analyze $\\ell(\\theta) = n \\ln(\\theta) - \\theta \\sum x_i$. In this limit, the negative linear term $-\\theta \\sum x_i$ dominates the logarithmic term $n\\ln(\\theta)$, causing $\\ell(\\theta) \\to -\\infty$. (This assumes $\\sum x_i  0$, which must be true for a non-trivial sample of inter-arrival times where each $x_i  0$).\nSince the function approaches $-\\infty$ at both boundaries of the domain and has a single critical point within the domain where the function is concave, this critical point must be the global maximum.\nTherefore, the maximum likelihood estimator for $\\theta$ is indeed $\\hat{\\theta} = \\frac{1}{\\bar{X}}$.", "answer": "$$\\boxed{\\frac{1}{\\bar{X}}}$$", "id": "4937856"}, {"introduction": "Deriving an estimator is only the first step; the next is to evaluate its quality, and one of the most important properties to assess is bias. This practice [@problem_id:4937922] reveals the subtle but critical fact that the MLE for the variance of a normal distribution is systematically incorrect, or biased. By working through the derivation of this bias and constructing a corrected, unbiased version, you will gain a deep appreciation for why the familiar sample variance formula uses a denominator of $n-1$ instead of $n$.", "problem": "A neuroscientist is quantifying trial-to-trial variability in an evoked local field potential (LFP) amplitude from a cortical area. Across $n$ independent trials, the amplitude measurements are modeled as independent and identically distributed samples $y_{1},\\dots,y_{n}$ from a Gaussian distribution with unknown mean $\\mu$ and unknown variance $\\sigma^{2}$, that is, $y_{i} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ independently for $i=1,\\dots,n$. Let $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ denote the sample mean.\n\nStarting from the likelihood function of the Gaussian model and the definition of bias $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$ for an estimator $\\hat{\\theta}$ of a scalar parameter $\\theta$, do the following:\n\n1. Derive the maximum likelihood estimator (MLE) for the variance $\\sigma^{2}$ under the assumption that the mean $\\mu$ is unknown, and express it in terms of $y_{i}$ and $\\bar{y}$.\n2. Using core properties of expectations and variances for independent Gaussian random variables, derive the expectation of the MLE for $\\sigma^{2}$ and obtain its bias as a function of $n$ and $\\sigma^{2}$. Your derivation should rely only on first principles such as independence, linearity of expectation, and the variance of the sample mean for Gaussian data.\n3. Propose an unbiased estimator for $\\sigma^{2}$ based on the residual sum of squares formed with $\\bar{y}$, and show it is unbiased.\n\nReport, as your final answer, the closed-form analytic expression for the bias of the MLE for $\\sigma^{2}$ in terms of $n$ and $\\sigma^{2}$. No rounding is required. The final answer must be a single expression.", "solution": "The problem requires us to perform three tasks related to the estimation of the variance $\\sigma^2$ of a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ from a set of $n$ independent and identically distributed (i.i.d.) samples $y_1, \\dots, y_n$, where both $\\mu$ and $\\sigma^2$ are unknown.\n\n**1. Derivation of the Maximum Likelihood Estimator (MLE) for $\\sigma^2$**\n\nThe probability density function (PDF) for a single observation $y_i$ drawn from a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ is:\n$$f(y_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$$\nGiven that the samples $y_1, \\dots, y_n$ are i.i.d., the likelihood function $L(\\mu, \\sigma^2 \\mid y_1, \\dots, y_n)$ is the product of the individual PDFs:\n$$L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(y_i \\mid \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right)$$\nTo find the maximum likelihood estimators, it is more convenient to work with the log-likelihood function, $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$:\n$$\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\nWe find the MLEs by taking the partial derivatives of $\\ell$ with respect to $\\mu$ and $\\sigma^2$ and setting them to zero.\n\nFirst, for the mean $\\mu$:\n$$\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)$$\nSetting this to zero yields $\\sum_{i=1}^{n} (y_i - \\mu) = 0$, which implies $\\sum_{i=1}^{n} y_i - n\\mu = 0$. The MLE for $\\mu$ is therefore:\n$$\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n} y_i = \\bar{y}$$\nNext, for the variance $\\sigma^2$. Let's set $\\theta = \\sigma^2$ for notational convenience.\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{n}{2\\theta} + \\frac{1}{2\\theta^2}\\sum_{i=1}^{n} (y_i - \\mu)^2$$\nTo find the joint maximum of the likelihood function, we substitute the MLE for $\\mu$, $\\hat{\\mu}_{\\text{MLE}} = \\bar{y}$, into the derivative with respect to $\\theta$ and set it to zero:\n$$\\left. \\frac{\\partial \\ell}{\\partial \\theta} \\right|_{\\mu=\\bar{y}, \\theta=\\hat{\\theta}_{\\text{MLE}}} = -\\frac{n}{2\\hat{\\theta}_{\\text{MLE}}} + \\frac{1}{2\\hat{\\theta}_{\\text{MLE}}^2}\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 0$$\nMultiplying by $2\\hat{\\theta}_{\\text{MLE}}^2$ gives:\n$$-n\\hat{\\theta}_{\\text{MLE}} + \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = 0$$\nSolving for $\\hat{\\theta}_{\\text{MLE}}$, we obtain the MLE for the variance $\\sigma^2$:\n$$\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n\n**2. Bias of the MLE for $\\sigma^2$**\n\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\operatorname{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$. We must compute the expectation of $\\hat{\\sigma}^2_{\\text{MLE}}$.\n$$\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\mathbb{E}\\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right]$$\nWe analyze the sum of squares term by adding and subtracting the true mean $\\mu$:\n$$\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\sum_{i=1}^{n} ((y_i - \\mu) - (\\bar{y} - \\mu))^2$$\nExpanding the square:\n$$= \\sum_{i=1}^{n} \\left[ (y_i - \\mu)^2 - 2(y_i - \\mu)(\\bar{y} - \\mu) + (\\bar{y} - \\mu)^2 \\right]$$\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2(\\bar{y} - \\mu) \\sum_{i=1}^{n} (y_i - \\mu) + \\sum_{i=1}^{n} (\\bar{y} - \\mu)^2$$\nRecognizing that $\\sum_{i=1}^{n} (y_i - \\mu) = n(\\bar{y} - \\mu)$, the expression becomes:\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2(\\bar{y} - \\mu) [n(\\bar{y} - \\mu)] + n(\\bar{y} - \\mu)^2$$\n$$= \\sum_{i=1}^{n} (y_i - \\mu)^2 - 2n(\\bar{y} - \\mu)^2 + n(\\bar{y} - \\mu)^2 = \\sum_{i=1}^{n} (y_i - \\mu)^2 - n(\\bar{y} - \\mu)^2$$\nNow, we take the expectation of this expression using the linearity of expectation:\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right] - n\\mathbb{E}\\left[ (\\bar{y} - \\mu)^2 \\right]$$\nWe evaluate each term separately.\nBy definition, the variance of $y_i$ is $\\operatorname{Var}(y_i) = \\mathbb{E}[(y_i - \\mathbb{E}[y_i])^2] = \\mathbb{E}[(y_i - \\mu)^2] = \\sigma^2$. Thus:\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right] = \\sum_{i=1}^{n} \\mathbb{E}[(y_i - \\mu)^2] = \\sum_{i=1}^{n} \\sigma^2 = n\\sigma^2$$\nThe term $\\mathbb{E}[(\\bar{y} - \\mu)^2]$ is the variance of the sample mean, $\\operatorname{Var}(\\bar{y})$, since $\\mathbb{E}[\\bar{y}] = \\mu$. We derive this from first principles. As samples are independent:\n$$\\operatorname{Var}(\\bar{y}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} y_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}(y_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\nSubstituting these results back:\n$$\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = n\\sigma^2 - n\\left(\\frac{\\sigma^2}{n}\\right) = n\\sigma^2 - \\sigma^2 = (n-1)\\sigma^2$$\nNow we can compute the expectation of the MLE:\n$$\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{1}{n} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n}(n-1)\\sigma^2 = \\frac{n-1}{n}\\sigma^2$$\nFinally, the bias of the MLE for $\\sigma^2$ is:\n$$\\operatorname{Bias}(\\hat{\\sigma}^2_{\\text{MLE}}) = \\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n\n**3. Unbiased Estimator for $\\sigma^2$**\n\nWe have shown that $\\hat{\\sigma}^2_{\\text{MLE}}$ is a biased estimator. We can construct an unbiased estimator by scaling $\\hat{\\sigma}^2_{\\text{MLE}}$. Let the unbiased estimator be $s^2$. We want $\\mathbb{E}[s^2] = \\sigma^2$.\nWe know $\\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{n-1}{n}\\sigma^2$. Let's define $s^2 = c \\cdot \\hat{\\sigma}^2_{\\text{MLE}}$ for some constant $c$.\n$$\\mathbb{E}[s^2] = c \\cdot \\mathbb{E}[\\hat{\\sigma}^2_{\\text{MLE}}] = c \\left(\\frac{n-1}{n}\\right)\\sigma^2$$\nFor this to be unbiased, we must have $c \\left(\\frac{n-1}{n}\\right) = 1$, which implies $c = \\frac{n}{n-1}$.\nThe proposed unbiased estimator, based on the residual sum of squares $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$, is therefore:\n$$s^2 = \\frac{n}{n-1} \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{n}{n-1} \\left( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right) = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\nThis is the well-known sample variance. To show it is unbiased, we calculate its expectation:\n$$\\mathbb{E}[s^2] = \\mathbb{E}\\left[ \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = \\frac{1}{n-1} \\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right]$$\nUsing our result from part 2, $\\mathbb{E}\\left[ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right] = (n-1)\\sigma^2$.\n$$\\mathbb{E}[s^2] = \\frac{1}{n-1} (n-1)\\sigma^2 = \\sigma^2$$\nSince $\\mathbb{E}[s^2] - \\sigma^2 = 0$, the estimator $s^2$ is unbiased.\n\nThe problem asks for the closed-form expression for the bias of the MLE for $\\sigma^2$. This was derived in the second part of the analysis.", "answer": "$$\\boxed{-\\frac{1}{n}\\sigma^2}$$", "id": "4159922"}, {"introduction": "In biostatistics, we often face a choice between multiple estimators for the same parameter, some biased and some not. To make an informed decision, we need a metric that captures an estimator's overall accuracy, which is precisely the role of the Mean Squared Error (MSE). This exercise [@problem_id:4937858] delves into the \"bias-variance tradeoff\" by using MSE to compare the standard unbiased sample proportion with a biased \"shrinkage\" estimator, illustrating a concept with profound implications in modern statistical modeling.", "problem": "A public health laboratory conducts $n$ independent assays, each on a different blood sample from the same population, to estimate the prevalence $p$ of a binary biomarker (present versus absent). Let $X_{1},\\dots,X_{n}$ be independent and identically distributed (I.I.D.) with $X_{i}\\sim \\mathrm{Bernoulli}(p)$, so that $X_{i}=1$ denotes presence and $X_{i}=0$ denotes absence, and let the sample proportion be $\\hat{p}=\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. Consider the shrinkage estimator $\\tilde{p}=\\frac{n\\bar{X}+\\alpha}{n+\\beta}$, where $\\alpha\\ge 0$ and $\\beta\\ge 0$ are fixed constants that do not depend on the data and encode pseudo-count regularization.\n\nStarting from the definitions of expectation, variance, bias, and Mean Squared Error (MSE), where the Mean Squared Error (MSE) of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is $\\mathrm{MSE}(\\hat{\\theta})= \\mathrm{Var}(\\hat{\\theta}) + \\left(\\mathrm{Bias}(\\hat{\\theta})\\right)^{2}$ with $\\mathrm{Bias}(\\hat{\\theta})=E[\\hat{\\theta}]-\\theta$, derive:\n- the bias and variance of $\\hat{p}$,\n- the bias and variance of $\\tilde{p}$,\n- the MSEs $\\mathrm{MSE}(\\hat{p})$ and $\\mathrm{MSE}(\\tilde{p})$ as functions of $p$.\n\nThen, compare the MSEs by simplifying the difference $\\mathrm{MSE}(\\tilde{p})-\\mathrm{MSE}(\\hat{p})$ to a single closed-form analytic expression in terms of $p$, $n$, $\\alpha$, and $\\beta$. Report, as your final answer, the simplified expression for $\\mathrm{MSE}(\\tilde{p})-\\mathrm{MSE}(\\hat{p})$. No rounding is required.", "solution": "The problem asks for the derivation of the bias, variance, and Mean Squared Error (MSE) for two estimators of a Bernoulli proportion $p$, and then for the simplified difference between their MSEs. Let $X_{1},\\dots,X_{n}$ be independent and identically distributed (I.I.D.) random variables from a $\\mathrm{Bernoulli}(p)$ distribution. We know that for each $X_i$, the expectation is $E[X_i] = p$ and the variance is $\\mathrm{Var}(X_i) = p(1-p)$. Let $S_n = \\sum_{i=1}^{n}X_{i}$. Due to the I.I.D. property, $S_n$ follows a binomial distribution, $S_n \\sim \\mathrm{Binomial}(n, p)$, with expectation $E[S_n] = np$ and variance $\\mathrm{Var}(S_n) = np(1-p)$.\n\nFirst, we analyze the standard sample proportion, $\\hat{p} = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i} = \\frac{S_n}{n}$.\n\nThe expectation of $\\hat{p}$ is:\n$$E[\\hat{p}] = E\\left[\\frac{S_n}{n}\\right] = \\frac{1}{n}E[S_n] = \\frac{1}{n}(np) = p$$\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\mathrm{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. Therefore, the bias of $\\hat{p}$ is:\n$$\\mathrm{Bias}(\\hat{p}) = E[\\hat{p}] - p = p - p = 0$$\nThe estimator $\\hat{p}$ is unbiased.\n\nThe variance of $\\hat{p}$ is:\n$$\\mathrm{Var}(\\hat{p}) = \\mathrm{Var}\\left(\\frac{S_n}{n}\\right) = \\frac{1}{n^2}\\mathrm{Var}(S_n) = \\frac{1}{n^2}(np(1-p)) = \\frac{p(1-p)}{n}$$\nThe MSE is defined as $\\mathrm{MSE}(\\hat{\\theta}) = \\mathrm{Var}(\\hat{\\theta}) + (\\mathrm{Bias}(\\hat{\\theta}))^2$. For $\\hat{p}$, the MSE is:\n$$\\mathrm{MSE}(\\hat{p}) = \\mathrm{Var}(\\hat{p}) + (\\mathrm{Bias}(\\hat{p}))^2 = \\frac{p(1-p)}{n} + 0^2 = \\frac{p(1-p)}{n}$$\n\nNext, we analyze the shrinkage estimator, $\\tilde{p} = \\frac{n\\bar{X}+\\alpha}{n+\\beta} = \\frac{n\\hat{p}+\\alpha}{n+\\beta}$.\n\nThe expectation of $\\tilde{p}$ is:\n$$E[\\tilde{p}] = E\\left[\\frac{n\\hat{p}+\\alpha}{n+\\beta}\\right] = \\frac{1}{n+\\beta}E[n\\hat{p}+\\alpha] = \\frac{1}{n+\\beta}(nE[\\hat{p}]+\\alpha) = \\frac{np+\\alpha}{n+\\beta}$$\nThe bias of $\\tilde{p}$ is:\n$$\\mathrm{Bias}(\\tilde{p}) = E[\\tilde{p}] - p = \\frac{np+\\alpha}{n+\\beta} - p = \\frac{np+\\alpha - p(n+\\beta)}{n+\\beta} = \\frac{np+\\alpha - np - p\\beta}{n+\\beta} = \\frac{\\alpha - \\beta p}{n+\\beta}$$\nThe variance of $\\tilde{p}$ is:\n$$\\mathrm{Var}(\\tilde{p}) = \\mathrm{Var}\\left(\\frac{n\\hat{p}+\\alpha}{n+\\beta}\\right) = \\left(\\frac{n}{n+\\beta}\\right)^2 \\mathrm{Var}(\\hat{p}) = \\frac{n^2}{(n+\\beta)^2}\\left(\\frac{p(1-p)}{n}\\right) = \\frac{np(1-p)}{(n+\\beta)^2}$$\nThe MSE of $\\tilde{p}$ is the sum of its variance and squared bias:\n$$\\mathrm{MSE}(\\tilde{p}) = \\mathrm{Var}(\\tilde{p}) + (\\mathrm{Bias}(\\tilde{p}))^2 = \\frac{np(1-p)}{(n+\\beta)^2} + \\left(\\frac{\\alpha - \\beta p}{n+\\beta}\\right)^2 = \\frac{np(1-p) + (\\alpha - \\beta p)^2}{(n+\\beta)^2}$$\n\nFinally, we are asked to find the difference $\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p})$ and simplify it to a single closed-form expression.\n$$\\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{np(1-p) + (\\alpha - \\beta p)^2}{(n+\\beta)^2} - \\frac{p(1-p)}{n}$$\nTo combine these terms, we use the common denominator $n(n+\\beta)^2$.\n$$ \\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{n\\left[np(1-p) + (\\alpha - \\beta p)^2\\right] - (n+\\beta)^2 p(1-p)}{n(n+\\beta)^2} $$\nLet's analyze the numerator. We can group terms involving $p(1-p)$:\n$$\n\\begin{aligned}\n\\text{Numerator} = n^2 p(1-p) - (n+\\beta)^2 p(1-p) + n(\\alpha - \\beta p)^2 \\\\\n= \\left[n^2 - (n+\\beta)^2\\right]p(1-p) + n(\\alpha - \\beta p)^2 \\\\\n= \\left[n^2 - (n^2 + 2n\\beta + \\beta^2)\\right]p(1-p) + n(\\alpha - \\beta p)^2 \\\\\n= (-2n\\beta - \\beta^2)p(1-p) + n(\\alpha - \\beta p)^2\n\\end{aligned}\n$$\nNow we expand the remaining terms and collect coefficients based on powers of $p$:\n$$\n\\begin{aligned}\n\\text{Numerator} = -\\beta(2n+\\beta)(p-p^2) + n(\\alpha^2 - 2\\alpha\\beta p + \\beta^2 p^2) \\\\\n= - (2n\\beta + \\beta^2)p + (2n\\beta + \\beta^2)p^2 + n\\alpha^2 - 2n\\alpha\\beta p + n\\beta^2 p^2\n\\end{aligned}\n$$\nCollecting coefficients for $p^2$:\n$$(2n\\beta + \\beta^2 + n\\beta^2)p^2 = ((n+1)\\beta^2 + 2n\\beta)p^2$$\nCollecting coefficients for $p$:\n$$(-(2n\\beta + \\beta^2) - 2n\\alpha\\beta)p = -(2n\\alpha\\beta + 2n\\beta + \\beta^2)p$$\nThe constant term is $n\\alpha^2$.\n\nAssembling the numerator gives the polynomial in $p$:\n$$ \\text{Numerator} = ((n+1)\\beta^2 + 2n\\beta)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2 $$\nThe final expression given in the original problem text used a slightly different but equivalent factorization: `(n\\beta^2 + 2n\\beta + \\beta^2)p^2`. This is $((n+1)\\beta^2 + 2n\\beta)p^2$. My re-derivation confirms the coefficients are correct.\nTherefore, the simplified difference of the MSEs is:\n$$ \\mathrm{MSE}(\\tilde{p}) - \\mathrm{MSE}(\\hat{p}) = \\frac{(n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2}{n(n+\\beta)^2} $$", "answer": "$$\n\\boxed{\\frac{(n\\beta^2 + 2n\\beta + \\beta^2)p^2 - (2n\\alpha\\beta + 2n\\beta + \\beta^2)p + n\\alpha^2}{n(n+\\beta)^2}}\n$$", "id": "4937858"}]}