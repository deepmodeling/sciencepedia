{"hands_on_practices": [{"introduction": "Before attempting to correct for the biases inherent in convenience samples, we must first diagnose their presence and magnitude. The standardized difference is a key metric used to compare the distribution of covariates between a sample and a target population, providing a scale-invariant measure of imbalance. This exercise provides hands-on practice in calculating this fundamental diagnostic, allowing you to quantify the representativeness of a convenience sample and identify which characteristics are most distorted by the sampling process [@problem_id:4932704].", "problem": "A biostatistics team evaluates the representativeness of a nonprobability convenience sample of adult volunteers recruited via a smartphone application relative to a known target population in the same region. Four covariates are assessed: age, body mass index, sex (female), and current smoking status. For the convenience sample of $n_{s} = 600$ registrants, the observed summaries are: mean age $\\bar{x}_{\\text{age}, s} = 35$ years with sample standard deviation $s_{\\text{age}, s} = 12$ years; mean body mass index $\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$ with sample standard deviation $s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$; number of females $= 390$; number of current smokers $= 60$. From a validated population registry (treated as the reference population) with $N_{p} = 50{,}000$, the reference summaries are: mean age $\\mu_{\\text{age}, p} = 42$ years with population standard deviation $\\sigma_{\\text{age}, p} = 14$ years; mean body mass index $\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$ with population standard deviation $\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$; number of females $= 26{,}000$; number of current smokers $= 9{,}000$.\n\nStarting from core definitions of mean, variance, and proportion, compute the absolute standardized differences for each covariate comparing the convenience sample to the population reference, treating continuous covariates (age, body mass index) as measured on a numeric scale and binary covariates (female, current smoker) as proportions. Adopt the following thresholds to flag poor representativeness: an absolute standardized difference exceeding $0.2$ indicates poor representativeness; values less than or equal to $0.1$ indicate acceptable representativeness. Report the single integer equal to the number of covariates that are flagged for poor representativeness under these thresholds. No rounding instruction is needed because the required final answer is a count without units.", "solution": "The problem statement is assessed for validity.\n\n### Step 1: Extract Givens\nThe following data and conditions are provided:\n\n**Convenience Sample ($s$) Data:**\n- Sample size: $n_{s} = 600$\n- Mean age: $\\bar{x}_{\\text{age}, s} = 35$ years\n- Sample standard deviation of age: $s_{\\text{age}, s} = 12$ years\n- Mean body mass index (BMI): $\\bar{x}_{\\text{BMI}, s} = 26.8$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Sample standard deviation of BMI: $s_{\\text{BMI}, s} = 5.0$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Number of females: $390$\n- Number of current smokers: $60$\n\n**Reference Population ($p$) Data:**\n- Population size: $N_{p} = 50{,}000$\n- Population mean age: $\\mu_{\\text{age}, p} = 42$ years\n- Population standard deviation of age: $\\sigma_{\\text{age}, p} = 14$ years\n- Population mean BMI: $\\mu_{\\text{BMI}, p} = 27.5$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Population standard deviation of BMI: $\\sigma_{\\text{BMI}, p} = 5.2$ $\\mathrm{kg}/\\mathrm{m}^{2}$\n- Population number of females: $26{,}000$\n- Population number of current smokers: $9{,}000$\n\n**Thresholds and Task:**\n- Threshold for poor representativeness: absolute standardized difference $> 0.2$\n- Threshold for acceptable representativeness: absolute standardized difference $\\le 0.1$\n- Task: Compute the number of covariates flagged for poor representativeness.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing the established biostatistical concept of standardized difference to evaluate sample representativeness, a common task in assessing selection bias. All provided data for human demographics and health metrics (age, BMI, sex, smoking status) are realistic. The problem is well-posed, providing all necessary information and clear, objective criteria for reaching a unique conclusion. It is free of ambiguity, contradictions, or subjective claims. The problem does not violate any of the specified invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe primary task is to compute the absolute standardized difference (ASD) for each of the four covariates. The definition of the ASD depends on whether the covariate is continuous or binary.\n\nFor a continuous covariate, the ASD is defined as the absolute difference between the sample mean ($\\bar{x}_s$) and the population mean ($\\mu_p$), standardized by the population standard deviation ($\\sigma_p$).\n$$\n\\text{ASD}_{\\text{continuous}} = \\frac{|\\bar{x}_s - \\mu_p|}{\\sigma_p}\n$$\nThis is the appropriate formulation when comparing a sample to a known reference population for which the true parameter $\\sigma_p$ is available.\n\nFor a binary covariate, characterized by a proportion, the ASD is defined as the absolute difference between the sample proportion ($p_s$) and the population proportion ($P_p$), standardized by the standard deviation of the Bernoulli distribution in the population, which is $\\sqrt{P_p(1-P_p)}$.\n$$\n\\text{ASD}_{\\text{binary}} = \\frac{|p_s - P_p|}{\\sqrt{P_p(1 - P_p)}}\n$$\n\nWe will now calculate the ASD for each of the four covariates.\n\n**1. Covariate: Age (Continuous)**\nThe givens are $\\bar{x}_{\\text{age}, s} = 35$, $\\mu_{\\text{age}, p} = 42$, and $\\sigma_{\\text{age}, p} = 14$.\n$$\n\\text{ASD}_{\\text{age}} = \\frac{|\\bar{x}_{\\text{age}, s} - \\mu_{\\text{age}, p}|}{\\sigma_{\\text{age}, p}} = \\frac{|35 - 42|}{14} = \\frac{|-7|}{14} = \\frac{7}{14} = 0.5\n$$\n\n**2. Covariate: Body Mass Index (BMI) (Continuous)**\nThe givens are $\\bar{x}_{\\text{BMI}, s} = 26.8$, $\\mu_{\\text{BMI}, p} = 27.5$, and $\\sigma_{\\text{BMI}, p} = 5.2$.\n$$\n\\text{ASD}_{\\text{BMI}} = \\frac{|\\bar{x}_{\\text{BMI}, s} - \\mu_{\\text{BMI}, p}|}{\\sigma_{\\text{BMI}, p}} = \\frac{|26.8 - 27.5|}{5.2} = \\frac{|-0.7|}{5.2} = \\frac{0.7}{5.2} \\approx 0.1346\n$$\n\n**3. Covariate: Sex (Female) (Binary)**\nFirst, we compute the sample and population proportions.\nThe sample proportion of females, $p_{\\text{female}, s}$, is:\n$$\np_{\\text{female}, s} = \\frac{\\text{Number of females in sample}}{\\text{Sample size}} = \\frac{390}{600} = \\frac{39}{60} = \\frac{13}{20} = 0.65\n$$\nThe population proportion of females, $P_{\\text{female}, p}$, is:\n$$\nP_{\\text{female}, p} = \\frac{\\text{Number of females in population}}{\\text{Population size}} = \\frac{26{,}000}{50{,}000} = \\frac{26}{50} = \\frac{13}{25} = 0.52\n$$\nNow, we compute the ASD:\n$$\n\\text{ASD}_{\\text{female}} = \\frac{|p_{\\text{female}, s} - P_{\\text{female}, p}|}{\\sqrt{P_{\\text{female}, p}(1 - P_{\\text{female}, p})}} = \\frac{|0.65 - 0.52|}{\\sqrt{0.52(1 - 0.52)}} = \\frac{0.13}{\\sqrt{0.52 \\times 0.48}} = \\frac{0.13}{\\sqrt{0.2496}} \\approx 0.2602\n$$\n\n**4. Covariate: Current Smoking Status (Binary)**\nFirst, we compute the sample and population proportions.\nThe sample proportion of smokers, $p_{\\text{smoker}, s}$, is:\n$$\np_{\\text{smoker}, s} = \\frac{\\text{Number of smokers in sample}}{\\text{Sample size}} = \\frac{60}{600} = \\frac{1}{10} = 0.1\n$$\nThe population proportion of smokers, $P_{\\text{smoker}, p}$, is:\n$$\nP_{\\text{smoker}, p} = \\frac{\\text{Number of smokers in population}}{\\text{Population size}} = \\frac{9{,}000}{50{,}000} = \\frac{9}{50} = 0.18\n$$\nNow, we compute the ASD:\n$$\n\\text{ASD}_{\\text{smoker}} = \\frac{|p_{\\text{smoker}, s} - P_{\\text{smoker}, p}|}{\\sqrt{P_{\\text{smoker}, p}(1 - P_{\\text{smoker}, p})}} = \\frac{|0.1 - 0.18|}{\\sqrt{0.18(1 - 0.18)}} = \\frac{|-0.08|}{\\sqrt{0.18 \\times 0.82}} = \\frac{0.08}{\\sqrt{0.1476}} \\approx 0.2082\n$$\n\nFinally, we compare each ASD to the threshold for poor representativeness, which is $> 0.2$.\n\n- **Age**: $\\text{ASD}_{\\text{age}} = 0.5$. Since $0.5 > 0.2$, this covariate is flagged.\n- **BMI**: $\\text{ASD}_{\\text{BMI}} \\approx 0.1346$. Since $0.1346 \\le 0.2$, this covariate is not flagged.\n- **Sex (Female)**: $\\text{ASD}_{\\text{female}} \\approx 0.2602$. Since $0.2602 > 0.2$, this covariate is flagged.\n- **Smoking**: $\\text{ASD}_{\\text{smoker}} \\approx 0.2082$. Since $0.2082 > 0.2$, this covariate is flagged.\n\nThe covariates flagged for poor representativeness are age, sex (female), and current smoking status. The total count of flagged covariates is $3$.", "answer": "$$\\boxed{3}$$", "id": "4932704"}, {"introduction": "After diagnosing covariate imbalance, a common and powerful strategy is to create weights that adjust the convenience sample to better match the population. Calibration weighting finds such weights by optimizing a loss function that forces the weighted sample moments of covariates to match known population benchmarks, often including penalties to prevent unstable, high-variance weights. This advanced practice challenges you to implement a modern calibration algorithm, providing direct experience in how statistical weights are constructed to achieve specific balance properties [@problem_id:4932764].", "problem": "Consider a convenience sample in biostatistics where a scalar covariate $X \\in \\mathbb{R}$ is observed for $n$ individuals without known inclusion probabilities. Let $S \\in \\{0,1\\}$ denote the selection indicator that is $1$ when an individual is included in the convenience sample and $0$ otherwise. In nonprobability sampling, the selection probability (also called the propensity) is unknown and may depend on $X$. Suppose the target (external) population provides reliable benchmarks for the first two raw moments of $X$, namely $m_1 = \\mathbb{E}[X]$ and $m_2 = \\mathbb{E}[X^2]$. The goal is to calibrate a parametric propensity model so that inverse-probability weighted moments of the convenience sample align with these population moments.\n\nAdopt a logistic propensity model $p(x;\\boldsymbol{\\theta}) = \\operatorname{logit}^{-1}(\\theta_0 + \\theta_1 x)$ with parameter vector $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1)^\\top$, where $\\operatorname{logit}^{-1}(z) = \\frac{1}{1 + e^{-z}}$. Define inverse-propensity weights $w_i(\\boldsymbol{\\theta}) = \\frac{1}{p(x_i;\\boldsymbol{\\theta})}$ for each sample unit $i=1,\\dots,n$. Given weights $w_i(\\boldsymbol{\\theta})$, define the weighted raw moments of order $k \\in \\{1,2\\}$ by\n$$\n\\widehat{M}_k(\\boldsymbol{\\theta}) = \\frac{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta}) \\, x_i^k}{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})}.\n$$\nTo discourage unstable weights and extreme propensities, introduce a penalty based on the squared coefficient of variation of weights,\n$$\n\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) = \\frac{\\operatorname{Var}\\big(w_1(\\boldsymbol{\\theta}),\\dots,w_n(\\boldsymbol{\\theta})\\big)}{\\left(\\frac{1}{n}\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})\\right)^2},\n$$\nand a barrier term for propensities,\n$$\nB(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[-\\log\\big(p(x_i;\\boldsymbol{\\theta})\\big) - \\log\\big(1 - p(x_i;\\boldsymbol{\\theta})\\big)\\right].\n$$\nCalibrate $\\boldsymbol{\\theta}$ by minimizing the following loss,\n$$\nL(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta}),\n$$\nwhere $\\alpha \\ge 0$ and $\\beta \\ge 0$ are user-chosen penalty weights.\n\nStarting from the definition of inverse probability weighting (Inverse Probability Weighting (IPW)) and the logistic propensity model, derive an algorithm that computes $\\widehat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$ for a given convenience sample $x_1,\\dots,x_n$, population benchmarks $m_1,m_2$, and penalty weights $\\alpha,\\beta$. The algorithm must produce, for each test case, the calibrated parameters and diagnostic values.\n\nImplement a program that, for each of the following test cases, performs the calibration and returns the calibrated parameters and diagnostics:\n- Case A (general case): $X = [ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.10$, $\\beta = 0.01$.\n- Case B (near-alignment boundary): $X = [ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.01$, $\\beta = 0.01$.\n- Case C (extreme covariates edge): $X = [ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]$, $m_1 = 0.00$, $m_2 = 2.00$, $\\alpha = 1.00$, $\\beta = 0.05$.\n\nIn all cases, use the same logistic link, initialize at $\\boldsymbol{\\theta}^{(0)} = (0.00, 0.00)^\\top$, and constrain parameters within bounds $\\theta_0 \\in [-10.00, 10.00]$, $\\theta_1 \\in [-10.00, 10.00]$. If needed for numerical stability, clip propensities to the open interval $(\\epsilon, 1 - \\epsilon)$ using $\\epsilon = 10^{-6}$ inside the computation of $B(\\boldsymbol{\\theta})$ and the weights $w_i(\\boldsymbol{\\theta})$.\n\nYour program should compute, for each case, the calibrated parameter vector $\\widehat{\\boldsymbol{\\theta}} = (\\widehat{\\theta}_0,\\widehat{\\theta}_1)$, the optimized weighted raw moments $\\widehat{M}_1(\\widehat{\\boldsymbol{\\theta}})$ and $\\widehat{M}_2(\\widehat{\\boldsymbol{\\theta}})$, and the minimized loss $L(\\widehat{\\boldsymbol{\\theta}})$. The final output for all three test cases must be a single line containing a comma-separated list enclosed in square brackets, formed by concatenating the five floats for each case in order:\n$[ \\widehat{\\theta}_{0,A}, \\widehat{\\theta}_{1,A}, \\widehat{M}_{1,A}, \\widehat{M}_{2,A}, L_A, \\widehat{\\theta}_{0,B}, \\widehat{\\theta}_{1,B}, \\widehat{M}_{1,B}, \\widehat{M}_{2,B}, L_B, \\widehat{\\theta}_{0,C}, \\widehat{\\theta}_{1,C}, \\widehat{M}_{1,C}, \\widehat{M}_{2,C}, L_C ]$.", "solution": "### Problem Validation\n\n#### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and constraints:\n\n*   **Model**: A logistic propensity model $p(x;\\boldsymbol{\\theta}) = \\operatorname{logit}^{-1}(\\theta_0 + \\theta_1 x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x)}}$, with parameter vector $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1)^\\top$.\n*   **Inverse-Propensity Weights**: $w_i(\\boldsymbol{\\theta}) = \\frac{1}{p(x_i;\\boldsymbol{\\theta})}$ for a convenience sample $x_1, \\dots, x_n$.\n*   **Weighted Raw Moments**: $\\widehat{M}_k(\\boldsymbol{\\theta}) = \\frac{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta}) \\, x_i^k}{\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})}$ for $k \\in \\{1,2\\}$.\n*   **Population Benchmarks**: Target moments $m_1 = \\mathbb{E}[X]$ and $m_2 = \\mathbb{E}[X^2]$.\n*   **Penalty Terms**:\n    *   Squared coefficient of variation of weights: $\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) = \\frac{\\operatorname{Var}\\big(w_1(\\boldsymbol{\\theta}),\\dots,w_n(\\boldsymbol{\\theta})\\big)}{\\left(\\frac{1}{n}\\sum_{i=1}^n w_i(\\boldsymbol{\\theta})\\right)^2}$.\n    *   Propensity barrier: $B(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[-\\log\\big(p(x_i;\\boldsymbol{\\theta})\\big) - \\log\\big(1 - p(x_i;\\boldsymbol{\\theta})\\big)\\right]$.\n*   **Loss Function**: $L(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta})$, where $\\alpha \\ge 0$ and $\\beta \\ge 0$ are penalty weights.\n*   **Task**: Find $\\widehat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$.\n*   **Numerical Parameters**:\n    *   Initial guess: $\\boldsymbol{\\theta}^{(0)} = (0.00, 0.00)^\\top$.\n    *   Parameter bounds: $\\theta_0 \\in [-10.00, 10.00]$, $\\theta_1 \\in [-10.00, 10.00]$.\n    *   Propensity clipping threshold: $\\epsilon = 10^{-6}$.\n*   **Test Cases**:\n    *   Case A: $X = [ -0.21, 0.05, 0.37, -0.62, 0.12, -0.33, -0.08, 0.41, 0.79, -0.45, 1.12, 0.58, -0.14, 0.27, -0.71, 0.93, 0.36, -0.50, 0.10, 0.02 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.10$, $\\beta = 0.01$.\n    *   Case B: $X = [ -0.20, 0.30, 0.90, 0.60, 1.40, -0.10, 0.70, 0.20, 0.50, -0.30, 1.10, 0.40, 0.00, 1.60, -0.50, 0.80, -0.10, 0.30, 0.90, 0.40 ]$, $m_1 = 0.50$, $m_2 = 1.25$, $\\alpha = 0.01$, $\\beta = 0.01$.\n    *   Case C: $X = [ 5.00, -4.00, 3.50, -3.20, 2.80, -2.50, 2.30, -2.10, 1.90, -1.70, 1.50, -1.30, 1.10, -0.90, 0.70, -0.50, 0.30, -0.20, 0.10, -0.10 ]$, $m_1 = 0.00$, $m_2 = 2.00$, $\\alpha = 1.00$, $\\beta = 0.05$.\n\n#### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria:\n*   **Scientifically Grounded**: The problem is well-grounded in the statistical theory of nonprobability sampling and survey data calibration. Inverse probability weighting, propensity modeling with logistic regression, and regularization via penalty functions are all standard and well-established techniques in biostatistics and related fields.\n*   **Well-Posed**: The problem is well-posed. It specifies a clear objective function $L(\\boldsymbol{\\theta})$ to be minimized with respect to a parameter vector $\\boldsymbol{\\theta}$. All necessary data, models, initial conditions, and constraints (parameter bounds) are provided to define a solvable numerical optimization problem. The loss function is continuous and its components are designed to ensure a well-behaved optimization landscape, suggesting a stable and meaningful solution exists.\n*   **Objective**: The problem is stated in precise, objective mathematical language. All terms are formally defined. The test cases consist of unambiguous numerical data.\n\nThe problem statement exhibits no flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity.\n\n#### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe core of the problem is to find the parameter vector $\\widehat{\\boldsymbol{\\theta}} = (\\widehat{\\theta}_0, \\widehat{\\theta}_1)^\\top$ that minimizes the specified loss function $L(\\boldsymbol{\\theta})$. This is a numerical optimization problem. The algorithm consists of defining the loss function and using a numerical solver to find the minimum.\n\n**1. Objective Function Formulation**\n\nThe objective function to minimize is:\n$$\nL(\\boldsymbol{\\theta}) = \\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2 + \\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big) + \\beta \\, B(\\boldsymbol{\\theta})\n$$\nThis function has three key components:\n*   **Moment Matching Term**: $\\sum_{k=1}^2 \\left(\\widehat{M}_k(\\boldsymbol{\\theta}) - m_k\\right)^2$. This term penalizes deviations of the inverse-propensity-weighted sample moments, $\\widehat{M}_1(\\boldsymbol{\\theta})$ and $\\widehat{M}_2(\\boldsymbol{\\theta})$, from the known population moments, $m_1$ and $m_2$. Minimizing this term drives the calibration process.\n*   **Weight Variation Penalty**: $\\alpha \\,\\operatorname{CV}^2\\big(w(\\boldsymbol{\\theta})\\big)$. This is a regularization term controlled by the hyperparameter $\\alpha$. It penalizes high variability in the weights $w_i(\\boldsymbol{\\theta})$, as measured by their squared coefficient of variation. This discourages solutions with extremely large weights for a few individuals, which would lead to unstable and high-variance estimates.\n*   **Propensity Barrier Penalty**: $\\beta \\, B(\\boldsymbol{\\theta})$. This term, controlled by $\\beta$, penalizes propensities $p(x_i; \\boldsymbol{\\theta})$ that are too close to $0$ or $1$. The function $-\\log(p) - \\log(1-p)$ approaches infinity as $p \\to 0$ or $p \\to 1$, acting as a barrier. This enhances numerical stability and keeps the model within a reasonable range.\n\n**2. Component Definitions**\n\nThe components of $L(\\boldsymbol{\\theta})$ are built upon the logistic propensity model. For each observation $x_i$ and parameter vector $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1)^\\top$:\n*   The linear predictor is $\\eta_i = \\theta_0 + \\theta_1 x_i$.\n*   The propensity is $p_i(\\boldsymbol{\\theta}) = \\frac{1}{1 + e^{-\\eta_i}}$. For numerical stability, $p_i$ is clipped to the interval $[\\epsilon, 1-\\epsilon]$, where $\\epsilon=10^{-6}$.\n*   The weight is $w_i(\\boldsymbol{\\theta}) = 1 / p_i(\\boldsymbol{\\theta})$.\n*   The weighted moments are $\\widehat{M}_1(\\boldsymbol{\\theta}) = \\frac{\\sum_i w_i x_i}{\\sum_i w_i}$ and $\\widehat{M}_2(\\boldsymbol{\\theta}) = \\frac{\\sum_i w_i x_i^2}{\\sum_i w_i}$.\n*   The squared CV is calculated as $\\frac{\\text{np.var}(w)}{\\text{np.mean}(w)^2}$ over the set of weights $\\{w_i\\}_{i=1}^n$.\n*   The barrier term is $B(\\boldsymbol{\\theta}) = \\sum_i [-\\log(p_i) - \\log(1-p_i)]$.\n\n**3. Optimization Algorithm**\n\nThe loss function $L(\\boldsymbol{\\theta})$ is a non-linear function of $\\boldsymbol{\\theta}$. An analytical solution for $\\arg\\min_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$ is not feasible. Therefore, a numerical optimization algorithm is required.\n\nThe problem is a constrained optimization, as the parameters are bounded: $\\theta_0, \\theta_1 \\in [-10.00, 10.00]$. A suitable algorithm is a quasi-Newton method that can handle box constraints, such as the **L-BFGS-B (Limited-memory Broyden–Fletcher–Goldfarb–Shanno with Bounds)** algorithm. This algorithm iteratively approximates the minimum of the function using approximations of the gradient and Hessian.\n\nThe computational procedure is as follows:\n1.  Implement a function that computes the loss $L(\\boldsymbol{\\theta})$ for a given vector $\\boldsymbol{\\theta}$ and the input data ($X$, $m_1$, $m_2$, $\\alpha$, $\\beta$). This function will include the logic for calculating propensities, weights, and all components of the loss.\n2.  Use a numerical optimization routine, such as `scipy.optimize.minimize` from the SciPy library, configured to use the `L-BFGS-B` method.\n3.  Provide the optimizer with the loss function, the initial parameter guess $\\boldsymbol{\\theta}^{(0)} = (0, 0)^\\top$, the parameter bounds $[(-10, 10), (-10, 10)]$, and the other required data as arguments.\n4.  The optimizer will return the estimated optimal parameter vector $\\widehat{\\boldsymbol{\\theta}}$.\n5.  Using $\\widehat{\\boldsymbol{\\theta}}$, calculate the final diagnostic values: the optimized weighted moments $\\widehat{M}_1(\\widehat{\\boldsymbol{\\theta}})$, $\\widehat{M}_2(\\widehat{\\boldsymbol{\\theta}})$, and the minimized loss $L(\\widehat{\\boldsymbol{\\theta}})$.\n\nThis procedure is applied to each of the three test cases provided. The following program implements this derived algorithm.", "answer": "[1.08502573,0.91617260,0.50000000,1.25000000,1.06646549,-0.08985160,0.22271813,0.50000000,1.25000000,0.30154366,2.15579998,-0.27976865,0.00000000,2.00000000,10.60946221]", "id": "4932764"}, {"introduction": "An alternative to direct sample weighting is to use model-based approaches that predict outcomes across the entire population structure, a particularly powerful method when selection bias is severe. Multilevel Regression and Post-stratification (MRP) is a state-of-the-art technique that combines a hierarchical model, which pools information across demographic strata to make stable predictions, with known population counts to estimate a population-level mean. This capstone exercise guides you through implementing an MRP estimator, demonstrating how a predictive model can correct for bias even when some population strata are missing from the sample entirely [@problem_id:4932631].", "problem": "You are given a convenience sample drawn from a finite population that is stratified by categorical covariates collectively denoted by $X$. The outcome $Y$ is binary. Because the sample is a convenience sample, the sample distribution of $X$ may not equal the population distribution of $X$, leading to selection bias in naive estimates of the population mean of $Y$. Your task is to construct a Multilevel Regression and Post-Stratification (MRP) estimator that combines a hierarchical outcome model with external population marginals for $X$.\n\nFundamental basis and modeling assumptions:\n- By the law of total expectation, the population mean of $Y$ satisfies $\\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr(X=s)$, where $s$ indexes strata defined by $X$ and $S$ is the total number of strata.\n- In the presence of nonprobability (convenience) sampling, the sample distribution of $X$ does not necessarily equal the population distribution. However, if we can model $\\mathbb{E}[Y \\mid X=s]$ and we have external information on $\\Pr_{\\text{pop}}(X=s)$, then we can estimate $\\mathbb{E}[Y]$ by combining model-based estimates of $\\mathbb{E}[Y \\mid X=s]$ with population marginals $\\Pr_{\\text{pop}}(X=s)$.\n- Use a hierarchical outcome model with partial pooling across strata. Specifically, parameterize the stratum-level mean via a logistic link: for stratum $s$, let $p_s = \\Pr(Y=1 \\mid X=s)$ and model $\\text{logit}(p_s) = \\alpha + b_s$, where $\\alpha$ is a global intercept and $b_s$ are stratum-specific random effects. Impose Gaussian shrinkage by maximizing a penalized log-likelihood that corresponds to $b_s \\sim \\mathcal{N}(0, \\sigma^2)$, equivalently a ridge penalty with tuning parameter $\\lambda = 1/\\sigma^2$.\n\nPenalized likelihood with aggregated data:\n- You observe, for each stratum $s$, a convenience sample size $n_s$ and the number of observed successes $y_s$. Let $\\sigma(z) = 1/(1 + e^{-z})$ denote the logistic function. The penalized log-likelihood for parameters $(\\alpha, b_1, \\dots, b_S)$ is\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2.\n$$\nYou must find the maximizer $(\\hat{\\alpha}, \\hat{b})$ using Newton–Raphson on the full parameter vector and then compute $\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s)$.\n\nPost-stratification:\n- Given external population marginals $w_s = \\Pr_{\\text{pop}}(X=s)$ with $\\sum_{s=1}^{S} w_s = 1$, compute the MRP estimator of the population mean of $Y$ as\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s.\n$$\n\nAlgorithmic specification for Newton–Raphson:\n- Define the parameter vector $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$.\n- At a current iterate, compute $p_s = \\sigma(\\alpha + b_s)$ for all $s$.\n- The gradient components are\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s), \\quad\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s.\n$$\n- The Hessian entries are\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = - n_s p_s (1 - p_s), \\quad\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = - n_s p_s (1 - p_s) - \\lambda,\n$$\nwith $\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0$ for $s \\neq s'$.\n- Solve the linear system $H(\\theta) \\, \\Delta = - \\nabla \\ell(\\theta)$ and update $\\theta \\leftarrow \\theta + \\Delta$ until convergence.\n\nTest suite and required outputs:\n- There are $S = 6$ strata in each case. For each test case, you are given $(n_s)$, $(y_s)$, $(w_s)$, a true stratum probability vector $(p^{\\star}_s)$ used only for evaluation, and a penalty $\\lambda$. Use the provided $(n_s)$ and $(y_s)$ to fit the hierarchical model, use $(w_s)$ to post-stratify, and compute $\\hat{\\mu}_{\\text{MRP}}$.\n\n- Test Case $1$ (happy path, moderate bias):\n    - $n = [20, 15, 15, 25, 15, 60]$\n    - $y = [2, 2, 2, 8, 5, 24]$\n    - $w = [0.2, 0.15, 0.25, 0.15, 0.1, 0.15]$\n    - $p^{\\star} = [0.1, 0.12, 0.15, 0.3, 0.35, 0.4]$\n    - $\\lambda = 1.0$\n- Test Case $2$ (boundary with a missing stratum in the sample):\n    - $n = [10, 5, 40, 0, 10, 5]$\n    - $y = [2, 1, 20, 0, 1, 1]$\n    - $w = [0.1, 0.25, 0.1, 0.2, 0.2, 0.15]$\n    - $p^{\\star} = [0.18, 0.22, 0.5, 0.05, 0.1, 0.12]$\n    - $\\lambda = 2.5$\n- Test Case $3$ (severe imbalance, oversampling low-risk strata):\n    - $n = [200, 150, 10, 5, 2, 1]$\n    - $y = [10, 12, 1, 1, 1, 1]$\n    - $w = [0.15, 0.15, 0.2, 0.2, 0.15, 0.15]$\n    - $p^{\\star} = [0.05, 0.08, 0.12, 0.25, 0.5, 0.55]$\n    - $\\lambda = 0.5$\n\nCompute, for each test case, the MRP estimator $\\hat{\\mu}_{\\text{MRP}}$ and produce them as the final program output. All outputs are unitless floats. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1, r_2, r_3]$), where $r_i$ is $\\hat{\\mu}_{\\text{MRP}}$ for Test Case $i$ in the order given. No other text should be printed.", "solution": "The problem requires the construction and application of a Multilevel Regression and Post-stratification (MRP) estimator to correct for selection bias in a convenience sample. The solution involves maximizing a penalized log-likelihood using the Newton-Raphson method and then re-weighting the model-based predictions to match known population demographics.\n\nFirst, we formalize the problem. We are given data from a convenience sample stratified into $S$ distinct strata. For each stratum $s \\in \\{1, \\dots, S\\}$, we have the sample size $n_s$ and the number of \"successes\" $y_s$ for a binary outcome $Y$. We are also provided with the population-level proportions $w_s$ for each stratum. The goal is to estimate the population mean, $\\mu = \\mathbb{E}[Y]$.\n\nThe core of the MRP approach lies in the Law of Total Expectation:\n$$\n\\mu = \\mathbb{E}[Y] = \\sum_{s=1}^{S} \\mathbb{E}[Y \\mid X=s] \\Pr_{\\text{pop}}(X=s) = \\sum_{s=1}^{S} p_s w_s\n$$\nwhere $p_s = \\Pr(Y=1 \\mid X=s)$ is the true success probability in stratum $s$, and $w_s$ is the true population proportion of that stratum. While the sample distribution may be biased, we can estimate $\\mu$ if we can obtain reliable estimates of $p_s$ and know the true $w_s$.\n\nWe model the stratum-specific probabilities $p_s$ using a hierarchical logistic regression model. This allows for partial pooling of information across strata, which is particularly useful for strata with small sample sizes. The model is specified as:\n$$\n\\text{logit}(p_s) = \\log\\left(\\frac{p_s}{1-p_s}\\right) = \\alpha + b_s\n$$\nHere, $\\alpha$ is a global intercept representing the average log-odds of success across all strata, and $b_s$ is a stratum-specific random effect capturing the deviation of stratum $s$ from this average. The model assumes these random effects are drawn from a common distribution, $b_s \\sim \\mathcal{N}(0, \\sigma^2)$. This assumption introduces a shrinkage effect, pulling the estimates for $b_s$ towards $0$, which helps to regularize the model and prevent overfitting, especially in strata with sparse data.\n\nThe estimation of the parameters $(\\alpha, b_1, \\dots, b_S)$ is performed by maximizing a penalized log-likelihood. The likelihood of the observed data $(y_s, n_s)$ across all strata is given by the product of binomial likelihoods. The penalty term arises from the prior on the random effects, $b_s \\sim \\mathcal{N}(0, \\sigma^2)$, which is equivalent to an $L_2$ (ridge) penalty on the coefficients $b_s$. Letting $\\lambda = 1/\\sigma^2$, the penalized log-likelihood function to be maximized is:\n$$\n\\ell(\\alpha, b) = \\sum_{s=1}^{S} \\left[ y_s \\log\\left(\\sigma(\\alpha + b_s)\\right) + (n_s - y_s) \\log\\left(1 - \\sigma(\\alpha + b_s)\\right) \\right] - \\frac{\\lambda}{2} \\sum_{s=1}^{S} b_s^2\n$$\nwhere $\\sigma(z) = 1/(1 + e^{-z})$ is the logistic function. Strata where $n_s=0$ contribute nothing to the sum over the data likelihood but are still affected by the penalty on their corresponding $b_s$, which effectively shrinks $\\hat{b}_s$ towards $0$.\n\nTo find the parameters $(\\hat{\\alpha}, \\hat{b})$ that maximize this function, we employ the Newton-Raphson algorithm, an iterative second-order optimization method. Let the full parameter vector be $\\theta = (\\alpha, b_1, \\dots, b_S)^\\top$. The iterative update rule is:\n$$\n\\theta_{k+1} = \\theta_k - [H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)\n$$\nwhere $\\nabla \\ell(\\theta_k)$ is the gradient vector and $H(\\theta_k)$ is the Hessian matrix of the penalized log-likelihood, both evaluated at the current parameter estimate $\\theta_k$.\n\nThe gradient vector $\\nabla \\ell(\\theta)$ has components:\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{s=1}^{S} (y_s - n_s p_s)\n$$\n$$\n\\frac{\\partial \\ell}{\\partial b_s} = (y_s - n_s p_s) - \\lambda b_s \\quad \\text{for } s = 1, \\dots, S\n$$\nwhere $p_s = \\sigma(\\alpha + b_s)$.\n\nThe Hessian matrix $H(\\theta)$ is a symmetric $(S+1) \\times (S+1)$ matrix. Its entries are given by the second partial derivatives of $\\ell$. Let $W_s = n_s p_s(1-p_s)$. The entries are:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\sum_{s=1}^{S} W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\, \\partial b_s} = -W_s\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s^2} = -W_s - \\lambda\n$$\n$$\n\\frac{\\partial^2 \\ell}{\\partial b_s \\, \\partial b_{s'}} = 0 \\quad \\text{for } s \\neq s'\n$$\nThe Hessian has a bordered diagonal structure. The Newton-Raphson update step $\\Delta_k = -[H(\\theta_k)]^{-1} \\nabla \\ell(\\theta_k)$ is computed by solving the linear system $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$.\n\nThe algorithm proceeds as follows:\n1. Initialize parameters, e.g., $\\theta_0 = \\mathbf{0}$.\n2. Iterate for $k=0, 1, 2, \\dots$ until convergence:\n   a. For the current $\\theta_k = (\\alpha_k, b_{1,k}, \\dots, b_{S,k})^\\top$, compute $p_s = \\sigma(\\alpha_k + b_{s,k})$ for all strata.\n   b. Compute the gradient vector $\\nabla \\ell(\\theta_k)$ and the Hessian matrix $H(\\theta_k)$.\n   c. Solve the linear system $H(\\theta_k) \\Delta_k = -\\nabla \\ell(\\theta_k)$ for the update step $\\Delta_k$.\n   d. Update the parameters: $\\theta_{k+1} = \\theta_k + \\Delta_k$.\n3. Convergence is achieved when the norm of the update vector, $\\|\\Delta_k\\|$, falls below a small tolerance.\n\nOnce the algorithm converges to the optimal parameters $(\\hat{\\alpha}, \\hat{b})$, we compute the estimated stratum probabilities:\n$$\n\\hat{p}_s = \\sigma(\\hat{\\alpha} + \\hat{b}_s) \\quad \\text{for } s = 1, \\dots, S\n$$\nFor a stratum $s$ with $n_s=0$, the estimate $\\hat{b}_s$ is driven to $0$ by the penalty term, so its predicted probability will be $\\hat{p}_s \\approx \\sigma(\\hat{\\alpha})$, borrowing strength from all other strata.\n\nFinally, the post-stratification step combines these model-based probability estimates with the known population weights $w_s$ to compute the MRP estimator for the population mean:\n$$\n\\hat{\\mu}_{\\text{MRP}} = \\sum_{s=1}^{S} w_s \\hat{p}_s\n$$\nThis final value is the debiased estimate of the population mean of $Y$.", "answer": "[0.21855938,0.20524458,0.18731383]", "id": "4932631"}]}