{"hands_on_practices": [{"introduction": "Stratified sampling is a cornerstone of efficient survey design, allowing us to improve precision by dividing a population into homogeneous subgroups. The true power of this method is unlocked by how we allocate our sample across these strata. This exercise challenges you to quantify the efficiency gain of using an optimal allocation strategy (Neyman allocation) compared to a more straightforward proportional allocation. By working through this problem [@problem_id:4942771], you will develop a concrete understanding of how variance is minimized and gain practical skills in evaluating different sampling designs.", "problem": "A biostatistics team is planning a stratified study to estimate the population mean of a continuous biomarker across three strata defined by clinical risk categories. The strata have population sizes $N_{1} = 1200$, $N_{2} = 800$, and $N_{3} = 500$, giving a total population size $N = 2500$. The stratum population standard deviations for the biomarker are $S_{1} = 12$, $S_{2} = 20$, and $S_{3} = 8$. The total sample size is fixed at $n = 300$, and sampling within each stratum is by Simple Random Sampling Without Replacement (SRSWOR).\n\nUsing only first principles and well-tested facts about stratified sampling, compute the ratio\n$$\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}}$$\nwhere $\\operatorname{Var}_{\\text{prop}}$ is the variance of the stratified estimator of the overall mean under proportional allocation $n_{h} = n \\cdot N_{h}/N$, and $\\operatorname{Var}_{\\text{Neyman}}$ is the variance under Neyman allocation $n_{h} \\propto N_{h} S_{h}$, both evaluated with the finite population correction within strata. For the purpose of calculation, treat $n_{h}$ as real-valued (ignore integer rounding). Round your final ratio to four significant figures. Then briefly interpret this ratio in terms of the efficiency gain achieved by Neyman allocation relative to proportional allocation, expressing any fractional reduction as a decimal (do not use the percentage sign).", "solution": "The problem is valid as it is scientifically grounded in the principles of biostatistics, self-contained, and well-posed. All necessary data for calculation are provided, and the objective is clearly defined. We can proceed with the solution.\n\nThe objective is to compute the ratio $\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}}$, where $\\operatorname{Var}_{\\text{prop}}$ and $\\operatorname{Var}_{\\text{Neyman}}$ are the variances of the stratified mean estimator under proportional and Neyman allocation, respectively.\n\nThe general formula for the variance of the stratified mean estimator, $\\bar{y}_{st} = \\sum_{h=1}^{L} W_h \\bar{y}_h$, with a finite population correction (FPC) is:\n$$ \\operatorname{Var}(\\bar{y}_{st}) = \\sum_{h=1}^{L} W_h^2 \\operatorname{Var}(\\bar{y}_h) = \\sum_{h=1}^{L} W_h^2 \\left(1-\\frac{n_h}{N_h}\\right) \\frac{S_h^2}{n_h} $$\nwhere $L$ is the number of strata, $W_h = N_h/N$ is the weight of stratum $h$, $N_h$ is the population size of stratum $h$, $N$ is the total population size, $n_h$ is the sample size in stratum $h$, and $S_h$ is the population standard deviation in stratum $h$. This can be rewritten as:\n$$ \\operatorname{Var}(\\bar{y}_{st}) = \\sum_{h=1}^{L} W_h^2 \\left(\\frac{S_h^2}{n_h} - \\frac{S_h^2}{N_h}\\right) = \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{n_h} - \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{N_h} $$\nSince $W_h = N_h/N$, the second term simplifies to $\\frac{1}{N^2} \\sum_{h=1}^{L} N_h S_h^2$, or equivalently $\\frac{1}{N} \\sum_{h=1}^{L} W_h S_h^2$. This term is constant regardless of the sample allocation method.\n\nThe given data are:\nTotal population size $N = 2500$.\nTotal sample size $n = 300$.\nStratum $1$: $N_1 = 1200$, $S_1 = 12$.\nStratum $2$: $N_2 = 800$, $S_2 = 20$.\nStratum $3$: $N_3 = 500$, $S_3 = 8$.\n\nFirst, we calculate the stratum weights $W_h$:\n$W_1 = \\frac{N_1}{N} = \\frac{1200}{2500} = 0.48$\n$W_2 = \\frac{N_2}{N} = \\frac{800}{2500} = 0.32$\n$W_3 = \\frac{N_3}{N} = \\frac{500}{2500} = 0.20$\n\nNext, we evaluate the variance for each allocation scheme.\n\n**1. Proportional Allocation**\nUnder proportional allocation, the sample size in each stratum is $n_h^{\\text{prop}} = n \\frac{N_h}{N} = n W_h$.\n$n_1^{\\text{prop}} = 300 \\cdot 0.48 = 144$\n$n_2^{\\text{prop}} = 300 \\cdot 0.32 = 96$\n$n_3^{\\text{prop}} = 300 \\cdot 0.20 = 60$\nFor proportional allocation, the FPC term $(1 - n_h/N_h)$ is constant across strata: $1 - \\frac{n}{N} = 1 - \\frac{300}{2500} = 1 - 0.12 = 0.88$.\nThe variance formula simplifies to:\n$$ \\operatorname{Var}_{\\text{prop}} = \\left(1 - \\frac{n}{N}\\right) \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{n_h^{\\text{prop}}} = \\left(1 - \\frac{n}{N}\\right) \\sum_{h=1}^{L} \\frac{W_h^2 S_h^2}{n W_h} = \\frac{1}{n}\\left(1 - \\frac{n}{N}\\right) \\sum_{h=1}^{L} W_h S_h^2 $$\nWe calculate the term $\\sum W_h S_h^2$:\n$$ \\sum_{h=1}^{3} W_h S_h^2 = (0.48)(12^2) + (0.32)(20^2) + (0.20)(8^2) $$\n$$ = (0.48)(144) + (0.32)(400) + (0.20)(64) = 69.12 + 128 + 12.8 = 209.92 $$\nNow we calculate $\\operatorname{Var}_{\\text{prop}}$:\n$$ \\operatorname{Var}_{\\text{prop}} = \\frac{1}{300}\\left(1 - \\frac{300}{2500}\\right) (209.92) = \\frac{0.88}{300}(209.92) = \\frac{184.7296}{300} \\approx 0.61576533 $$\n\n**2. Neyman Allocation**\nUnder Neyman allocation, the sample size is $n_h^{\\text{Neyman}} = n \\frac{N_h S_h}{\\sum_{k=1}^{L} N_k S_k}$.\nFirst, calculate the denominator $\\sum N_k S_k$:\n$$ \\sum_{k=1}^{3} N_k S_k = (1200)(12) + (800)(20) + (500)(8) = 14400 + 16000 + 4000 = 34400 $$\nThe allocatable sample sizes (as real values) are:\n$n_1^{\\text{Neyman}} = 300 \\cdot \\frac{14400}{34400} = \\frac{5400}{43} \\approx 125.58$\n$n_2^{\\text{Neyman}} = 300 \\cdot \\frac{16000}{34400} = \\frac{6000}{43} \\approx 139.53$\n$n_3^{\\text{Neyman}} = 300 \\cdot \\frac{4000}{34400} = \\frac{1500}{43} \\approx 34.88$\nThe variance under Neyman allocation can be calculated using a simplified formula that incorporates the FPC:\n$$ \\operatorname{Var}_{\\text{Neyman}} = \\frac{1}{n}\\left(\\sum_{h=1}^{L} W_h S_h\\right)^2 - \\frac{1}{N}\\sum_{h=1}^{L} W_h S_h^2 $$\nWe need the terms $\\sum W_h S_h$ and $\\sum W_h S_h^2$. We already found $\\sum W_h S_h^2 = 209.92$.\nLet's find $\\sum W_h S_h$:\n$$ \\sum_{h=1}^{3} W_h S_h = (0.48)(12) + (0.32)(20) + (0.20)(8) = 5.76 + 6.4 + 1.6 = 13.76 $$\nNow we calculate $\\operatorname{Var}_{\\text{Neyman}}$:\n$$ \\operatorname{Var}_{\\text{Neyman}} = \\frac{1}{300}(13.76)^2 - \\frac{1}{2500}(209.92) $$\n$$ = \\frac{189.3376}{300} - \\frac{209.92}{2500} = 0.63112533... - 0.083968 = 0.54715733... $$\n\n**3. Compute the Ratio**\nNow we compute the ratio $\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}}$:\n$$ \\text{Ratio} = \\frac{0.61576533...}{0.54715733...} \\approx 1.125390 $$\nRounding to four significant figures, the ratio is $1.125$.\n\n**Interpretation**\nThe ratio of the variances, $\\frac{\\operatorname{Var}_{\\text{prop}}}{\\operatorname{Var}_{\\text{Neyman}}} \\approx 1.125$, quantifies the relative efficiency of Neyman allocation compared to proportional allocation. A ratio greater than $1$ indicates that Neyman allocation is more efficient, resulting in a smaller variance for the same total sample size. The efficiency gain can be expressed as the fractional reduction in variance when using Neyman allocation instead of proportional allocation. This fractional reduction is given by $1 - \\frac{\\operatorname{Var}_{\\text{Neyman}}}{\\operatorname{Var}_{\\text{prop}}} = 1 - \\frac{1}{\\text{Ratio}}$. Using the unrounded ratio for better precision:\n$$ \\text{Fractional Reduction} = 1 - \\frac{1}{1.125390...} \\approx 1 - 0.88858 = 0.11142 $$\nThus, shifting from proportional to Neyman allocation for this study design would reduce the variance of the estimated population mean by a fraction of approximately $0.1114$. This gain in precision is achieved because Neyman allocation directs more sampling effort to strata that are larger (larger $N_h$) and more variable (larger $S_h$), which is the optimal strategy for minimizing the overall variance of the estimator.", "answer": "$$\\boxed{1.125}$$", "id": "4942771"}, {"introduction": "While stratified sampling often reduces variance, cluster sampling typically increases it due to correlations among units within the same cluster. This phenomenon is quantified by the design effect (DEFF), a crucial concept in biostatistics where subjects are often naturally grouped in clinics, households, or geographic areas. This practice [@problem_id:4942743] guides you through a first-principles derivation of the design effect, revealing how the intracluster correlation coefficient ($\\rho$) directly inflates the variance of an estimator and, consequently, the width of confidence intervals.", "problem": "A public health team is estimating a population proportion using a one-stage cluster sampling design. Suppose clusters are defined so that each sampled cluster contributes exactly $b=15$ observational units, and the outcome of interest is binary, with success indicator $Y \\in \\{0,1\\}$ and population success probability $p$. Assume a large population so that the finite population correction is negligible. Between distinct clusters the observations are independent. Within any given cluster, all distinct pairs of observations share the same intracluster correlation $\\rho=0.1$. Let the total sample size be $n=m b$, where $m$ is the number of sampled clusters.\n\nDefine the estimator $\\hat{p}$ as the overall sample mean across all sampled units. Using the core definitions of variance, covariance, and the intracluster correlation, and starting from the identity that for any finite collection of random variables,\n$$\n\\operatorname{Var}\\!\\Big(\\sum_{k} X_{k}\\Big) \\;=\\; \\sum_{k} \\operatorname{Var}(X_{k}) \\;+\\; 2 \\sum_{k<\\ell} \\operatorname{Cov}(X_{k}, X_{\\ell}),\n$$\nderive the multiplicative inflation factor in the variance of $\\hat{p}$ under this cluster sampling design relative to simple random sampling of $n$ independent units (with the same $n$ and the same $p$). Then determine the corresponding multiplicative inflation factor in the width of the standard large-sample normal (Wald) confidence interval for $p$ under the cluster design relative to simple random sampling, holding the confidence level fixed. Express both quantities as unitless factors.\n\nFinally, substitute $b=15$ and $\\rho=0.1$ to compute the two factors numerically, and report them in the order\n- first: the variance inflation factor relative to simple random sampling, and\n- second: the confidence-interval width inflation factor relative to simple random sampling.\n\nRound each factor to four significant figures. Express the final answer as pure numbers (no units).", "solution": "The problem asks for two multiplicative inflation factors related to a one-stage cluster sampling design compared to a simple random sampling (SRS) design with the same total sample size $n$. The first factor is for the variance of the proportion estimator $\\hat{p}$, and the second is for the width of the corresponding confidence interval.\n\nLet $m$ be the number of clusters sampled, and $b$ be the number of observational units per cluster. The total sample size is $n=mb$. Let $Y_{ij}$ be the binary outcome for the $j$-th unit in the $i$-th cluster, where $i \\in \\{1, \\dots, m\\}$ and $j \\in \\{1, \\dots, b\\}$. Since the outcome is binary with population success probability $p$, $Y_{ij}$ follows a Bernoulli distribution with parameter $p$. Thus, for any $i, j$:\n$$\n\\mathrm{E}[Y_{ij}] = p\n$$\n$$\n\\mathrm{Var}(Y_{ij}) = p(1-p)\n$$\nThe estimator for the population proportion, $\\hat{p}$, is the overall sample mean:\n$$\n\\hat{p} = \\frac{1}{n} \\sum_{i=1}^{m} \\sum_{j=1}^{b} Y_{ij} = \\frac{1}{mb} \\sum_{i=1}^{m} \\sum_{j=1}^{b} Y_{ij}\n$$\nFirst, we derive the variance of $\\hat{p}$ under this cluster sampling design, denoted as $\\mathrm{Var}_{\\text{cluster}}(\\hat{p})$.\n$$\n\\mathrm{Var}_{\\text{cluster}}(\\hat{p}) = \\mathrm{Var}\\left(\\frac{1}{mb} \\sum_{i=1}^{m} \\sum_{j=1}^{b} Y_{ij}\\right) = \\frac{1}{(mb)^2} \\mathrm{Var}\\left(\\sum_{i=1}^{m} \\sum_{j=1}^{b} Y_{ij}\\right)\n$$\nLet $T = \\sum_{i=1}^{m} \\sum_{j=1}^{b} Y_{ij}$. We can write $T = \\sum_{i=1}^{m} T_i$, where $T_i = \\sum_{j=1}^{b} Y_{ij}$ is the sum of outcomes within cluster $i$. The problem states that observations between distinct clusters are independent. Therefore, the covariance between the sums of two different clusters is zero, i.e., $\\mathrm{Cov}(T_i, T_k) = 0$ for $i \\neq k$. This means the variance of the total sum $T$ is the sum of the variances of the cluster sums:\n$$\n\\mathrm{Var}(T) = \\mathrm{Var}\\left(\\sum_{i=1}^{m} T_i\\right) = \\sum_{i=1}^{m} \\mathrm{Var}(T_i)\n$$\nSince each cluster is structurally identical, $\\mathrm{Var}(T_i)$ is the same for all $i$. Let's calculate $\\mathrm{Var}(T_1)$:\n$$\n\\mathrm{Var}(T_1) = \\mathrm{Var}\\left(\\sum_{j=1}^{b} Y_{1j}\\right)\n$$\nUsing the provided identity, $\\mathrm{Var}(\\sum_{k} X_{k}) = \\sum_{k} \\mathrm{Var}(X_{k}) + 2 \\sum_{k<\\ell} \\mathrm{Cov}(X_{k}, X_{\\ell})$, we have:\n$$\n\\mathrm{Var}(T_1) = \\sum_{j=1}^{b} \\mathrm{Var}(Y_{1j}) + 2 \\sum_{1 \\le j < k \\le b} \\mathrm{Cov}(Y_{1j}, Y_{1k})\n$$\nThe variance of each observation is $\\mathrm{Var}(Y_{1j}) = p(1-p)$. The intracluster correlation coefficient $\\rho$ is defined for any two distinct observations within the same cluster as:\n$$\n\\rho = \\frac{\\mathrm{Cov}(Y_{ij}, Y_{ik})}{\\sqrt{\\mathrm{Var}(Y_{ij})\\mathrm{Var}(Y_{ik})}} = \\frac{\\mathrm{Cov}(Y_{ij}, Y_{ik})}{p(1-p)} \\quad \\text{for } j \\neq k\n$$\nFrom this, we get $\\mathrm{Cov}(Y_{ij}, Y_{ik}) = \\rho p(1-p)$ for $j \\neq k$.\nSubstituting these into the expression for $\\mathrm{Var}(T_1)$:\nThe sum of variances is $\\sum_{j=1}^{b} \\mathrm{Var}(Y_{1j}) = b \\cdot p(1-p)$.\nThe sum of covariances involves $\\binom{b}{2} = \\frac{b(b-1)}{2}$ pairs of distinct observations within the cluster.\n$$\n2 \\sum_{1 \\le j < k \\le b} \\mathrm{Cov}(Y_{1j}, Y_{1k}) = 2 \\cdot \\frac{b(b-1)}{2} \\cdot \\rho p(1-p) = b(b-1)\\rho p(1-p)\n$$\nThus, the variance of a cluster sum is:\n$$\n\\mathrm{Var}(T_1) = b p(1-p) + b(b-1)\\rho p(1-p) = b p(1-p) [1 + (b-1)\\rho]\n$$\nNow, we can find the variance of the total sum $T$:\n$$\n\\mathrm{Var}(T) = \\sum_{i=1}^{m} \\mathrm{Var}(T_i) = m \\cdot \\mathrm{Var}(T_1) = m b p(1-p) [1 + (b-1)\\rho]\n$$\nFinally, we substitute this back into the expression for $\\mathrm{Var}_{\\text{cluster}}(\\hat{p})$:\n$$\n\\mathrm{Var}_{\\text{cluster}}(\\hat{p}) = \\frac{1}{(mb)^2} \\mathrm{Var}(T) = \\frac{m b p(1-p) [1 + (b-1)\\rho]}{(mb)^2} = \\frac{p(1-p)}{mb} [1 + (b-1)\\rho]\n$$\nSince $n=mb$, we have:\n$$\n\\mathrm{Var}_{\\text{cluster}}(\\hat{p}) = \\frac{p(1-p)}{n} [1 + (b-1)\\rho]\n$$\nNext, we consider the variance of $\\hat{p}$ under simple random sampling of $n$ units, denoted $\\mathrm{Var}_{\\text{SRS}}(\\hat{p})$. Under SRS, all $n$ observations are independent.\n$$\n\\mathrm{Var}_{\\text{SRS}}(\\hat{p}) = \\mathrm{Var}\\left(\\frac{1}{n} \\sum_{k=1}^{n} Y_k\\right) = \\frac{1}{n^2} \\sum_{k=1}^{n} \\mathrm{Var}(Y_k) = \\frac{1}{n^2} \\cdot n \\cdot p(1-p) = \\frac{p(1-p)}{n}\n$$\nThe first quantity required is the variance inflation factor (VIF), also known as the design effect (DEFF). This is the ratio of the variance under cluster sampling to the variance under simple random sampling.\n$$\n\\text{VIF} = \\frac{\\mathrm{Var}_{\\text{cluster}}(\\hat{p})}{\\mathrm{Var}_{\\text{SRS}}(\\hat{p})} = \\frac{\\frac{p(1-p)}{n} [1 + (b-1)\\rho]}{\\frac{p(1-p)}{n}} = 1 + (b-1)\\rho\n$$\nThe second quantity is the inflation factor for the width of a large-sample normal (Wald) confidence interval. The width of such an interval is proportional to the standard error of the estimator, $\\mathrm{SE}(\\hat{p}) = \\sqrt{\\mathrm{Var}(\\hat{p})}$.\nThe width of the interval is $2 z_{\\alpha/2} \\mathrm{SE}(\\hat{p})$, where $z_{\\alpha/2}$ is the critical value from the standard normal distribution for a given confidence level $1-\\alpha$.\nThe ratio of the widths is therefore:\n$$\n\\text{Width Inflation Factor} = \\frac{\\text{Width}_{\\text{cluster}}}{\\text{Width}_{\\text{SRS}}} = \\frac{2 z_{\\alpha/2} \\mathrm{SE}_{\\text{cluster}}(\\hat{p})}{2 z_{\\alpha/2} \\mathrm{SE}_{\\text{SRS}}(\\hat{p})} = \\frac{\\sqrt{\\mathrm{Var}_{\\text{cluster}}(\\hat{p})}}{\\sqrt{\\mathrm{Var}_{\\text{SRS}}(\\hat{p})}}\n$$\nThis factor is the square root of the VIF:\n$$\n\\text{Width Inflation Factor} = \\sqrt{\\frac{\\mathrm{Var}_{\\text{cluster}}(\\hat{p})}{\\mathrm{Var}_{\\text{SRS}}(\\hat{p})}} = \\sqrt{\\text{VIF}} = \\sqrt{1 + (b-1)\\rho}\n$$\nThis quantity is also known as the root design effect (DEFT).\n\nFinally, we substitute the given numerical values $b=15$ and $\\rho=0.1$ to compute these two factors.\nThe variance inflation factor is:\n$$\n\\text{VIF} = 1 + (15 - 1) \\times 0.1 = 1 + 14 \\times 0.1 = 1 + 1.4 = 2.4\n$$\nRounding to four significant figures gives $2.400$.\n\nThe confidence interval width inflation factor is:\n$$\n\\text{Width Inflation Factor} = \\sqrt{2.4} \\approx 1.5491933...\n$$\nRounding to four significant figures gives $1.549$.\n\nThe two factors are reported in the specified order: variance inflation factor first, then confidence-interval width inflation factor.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.400 & 1.549\n\\end{pmatrix}\n}\n$$", "id": "4942743"}, {"introduction": "Understanding the theoretical design effect is one thing; estimating variance from a real-world complex sample is another. This hands-on coding exercise [@problem_id:4942737] bridges that gap, tasking you with implementing two powerful, widely-used variance estimation techniques for clustered data: Taylor series linearization and the bootstrap. By comparing an analytical approximation with a computational resampling method, you will gain invaluable practical experience and a deeper appreciation for the methods that power modern survey analysis software.", "problem": "You are given a one-stage cluster sample consisting of Primary Sampling Units (PSUs), where each PSU contains a count of individuals and a count of individuals with a binary outcome of interest. You will estimate the population proportion and its variance under a with-replacement PSU sampling framework using two approaches: a cluster-level bootstrap that resamples PSUs with replacement, and a first-order Taylor linearization approximation for the ratio estimator. All quantities must be expressed as decimals (not percentages). The design-based logic must start from the following fundamental base: unbiasedness of the Horvitz–Thompson total under equal-probability sampling, the ratio estimator formed as a ratio of estimated totals for a proportion, and first-order Taylor expansion for variance linearization.\n\nDefinitions to use. Let there be $m$ PSUs indexed by $i \\in \\{1,\\dots,m\\}$. For PSU $i$, let $n_i$ be the number of individuals observed and $y_i$ be the number of individuals with the binary outcome. The estimator of the proportion is the sample ratio of totals,\n$$\n\\hat{p} \\;=\\; \\frac{\\sum_{i=1}^{m} y_i}{\\sum_{i=1}^{m} n_i}.\n$$\nThe cluster bootstrap (resampling PSUs with replacement) proceeds by generating $B$ bootstrap replicates. In replicate $b \\in \\{1,\\dots,B\\}$, sample $m$ PSUs with replacement from the set $\\{1,\\dots,m\\}$, compute\n$$\n\\hat{p}^{*(b)} \\;=\\; \\frac{\\sum_{j=1}^{m} y_{I^{(b)}_j}}{\\sum_{j=1}^{m} n_{I^{(b)}_j}},\n$$\nand then estimate the bootstrap variance as the sample variance across replicates:\n$$\n\\hat{V}_{\\text{boot}}(\\hat{p}) \\;=\\; \\frac{1}{B-1}\\sum_{b=1}^{B}\\Big(\\hat{p}^{*(b)} - \\bar{\\hat{p}}^{*}\\Big)^2,\\quad \\bar{\\hat{p}}^{*} \\;=\\; \\frac{1}{B}\\sum_{b=1}^{B}\\hat{p}^{*(b)}.\n$$\nThe first-order Taylor linearization for the ratio estimator uses the linearized variable\n$$\nu_i \\;=\\; y_i \\;-\\; \\hat{p}\\, n_i.\n$$\nUnder a with-replacement PSU sampling approximation, an estimator of the variance of $\\hat{p}$ is\n$$\n\\hat{V}_{\\text{lin}}(\\hat{p}) \\;=\\; \\frac{m}{m-1}\\,\\frac{\\sum_{i=1}^{m} u_i^2}{\\Big(\\sum_{i=1}^{m} n_i\\Big)^2}.\n$$\n\nImplement a program that, for each specified test case, computes $\\hat{p}$, the bootstrap variance $\\hat{V}_{\\text{boot}}(\\hat{p})$, and the linearization variance $\\hat{V}_{\\text{lin}}(\\hat{p})$, and returns the triple $[\\hat{p}, \\hat{V}_{\\text{boot}}(\\hat{p}), \\hat{V}_{\\text{lin}}(\\hat{p})]$ with each value rounded to $6$ decimal places.\n\nUse the following numeric datasets and test suite. Each dataset is a list of PSU pairs $(n_i,y_i)$ given explicitly, and each test specifies the number of bootstrap replicates $B$ and a pseudo-random seed value to ensure reproducibility. All numbers appear as integers; your computations must produce decimal outputs.\n\nDataset A (moderately sized, balanced cluster sizes):\n- (85, 12), (73, 9), (120, 20), (64, 6), (95, 14), (102, 13), (58, 5), (80, 10), (140, 25), (77, 8), (66, 7), (90, 11).\n\nDataset B (heterogeneous sizes and includes a zero-outcome PSU):\n- (150, 28), (45, 2), (60, 4), (110, 15), (90, 8), (200, 35), (30, 0), (75, 7).\n\nDataset C (small number of PSUs, boundary case):\n- (40, 3), (55, 7), (35, 2).\n\nTest suite to implement:\n- Case 1: Dataset A, $B = 10000$, seed = 12345.\n- Case 2: Dataset A, $B = 500$, seed = 12345.\n- Case 3: Dataset B, $B = 10000$, seed = 202311.\n- Case 4: Dataset C, $B = 10000$, seed = 777.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s triple is an inner list, for example:\n\"[[$p_1, v_{b,1}, v_{\\ell,1}$],[$p_2, v_{b,2}, v_{\\ell,2}$],...]\".\nYou must round each float to $6$ decimal places and return decimals (not percentages). No other text should be printed.\n\nYour implementation must be fully self-contained, take no input, and not access any files or networks. You must adhere to the specified test suite and output format.", "solution": "The problem is valid. It presents a clearly defined computational task within the established principles of biostatistics and survey sampling theory. All necessary data, formulas, and parameters are provided, and the problem is self-contained, objective, and scientifically sound.\n\nThe task is to estimate a proportion $\\hat{p}$ from one-stage cluster sampling data and to compute the variance of this estimate using two different methods: a cluster-level bootstrap and first-order Taylor series linearization.\n\nLet the data consist of $m$ Primary Sampling Units (PSUs), or clusters. For each PSU $i \\in \\{1, \\dots, m\\}$, we are given the cluster size, $n_i$, and the count of individuals with a binary outcome, $y_i$.\n\n**1. Proportion Estimation**\n\nThe estimator for the population proportion, $\\hat{p}$, is the ratio of two estimated totals: the total count of positive outcomes over the total count of individuals in the sample. This is a ratio estimator, which is consistent for the true population proportion. The formula is:\n$$\n\\hat{p} \\;=\\; \\frac{\\sum_{i=1}^{m} y_i}{\\sum_{i=1}^{m} n_i}\n$$\nThis calculation involves summing all $y_i$ values across the clusters and dividing by the sum of all $n_i$ values.\n\n**2. Variance Estimation via First-Order Taylor Linearization**\n\nThe Taylor linearization method is an analytical technique to approximate the variance of a complex, non-linear estimator like $\\hat{p}$. The variance of a ratio of two random variables $\\hat{Y}/\\hat{N}$ can be approximated by linearizing the function $f(\\hat{Y}, \\hat{N}) = \\hat{Y}/\\hat{N}$ around the true population totals $(Y, N)$. This yields:\n$$\nV(\\hat{p}) \\approx V\\left(\\frac{\\hat{Y}}{N} - \\frac{Y}{N^2}\\hat{N}\\right) = \\frac{1}{N^2} V(\\hat{Y} - p\\hat{N})\n$$\nwhere $p = Y/N$. To form an estimator, we substitute sample estimates for population parameters. We define a \"linearized variable\" $u_i$ for each cluster that represents the cluster's contribution to the numerator's variance:\n$$\nu_i \\;=\\; y_i - \\hat{p}\\,n_i\n$$\nThe problem specifies a variance estimator under a with-replacement sampling approximation for the PSUs. Under simple random sampling of $m$ clusters with replacement, the variance of a sample total $\\sum_{i=1}^{m} t_i$ is estimated by $m$ times the sample variance of the $t_i$ values. Since $\\sum u_i = 0$, the sample variance of the $u_i$ values simplifies. The variance of the linearized numerator, $\\sum_{i=1}^{m} u_i$, is estimated by $\\frac{m}{m-1}\\sum_{i=1}^{m} u_i^2$. Dividing by the square of the estimated denominator total, $(\\sum_{i=1}^{m} n_i)^2$, gives the final variance estimator for $\\hat{p}$:\n$$\n\\hat{V}_{\\text{lin}}(\\hat{p}) \\;=\\; \\frac{m}{m-1}\\,\\frac{\\sum_{i=1}^{m} u_i^2}{\\left(\\sum_{i=1}^{m} n_i\\right)^2}\n$$\nThe term $\\frac{m}{m-1}$ is an unbiasedness correction factor for the variance of a single mean/total from a simple random sample.\n\nThe algorithm is:\na. Compute $\\hat{p}$.\nb. Compute $u_i$ for each cluster $i=1, \\dots, m$.\nc. Compute $\\sum_{i=1}^{m} u_i^2$ and $\\sum_{i=1}^{m} n_i$.\nd. Substitute these quantities into the formula for $\\hat{V}_{\\text{lin}}(\\hat{p})$.\n\n**3. Variance Estimation via Cluster Bootstrap**\n\nThe bootstrap is a non-parametric, computational method for estimating the variance of an estimator. For clustered data, it is critical to resample the entire clusters (PSUs) to preserve the within-cluster correlation structure. The process is as follows:\n\na. Generate a large number, $B$, of bootstrap replicates. For each replicate $b$ from $1$ to $B$:\n    i. Create a bootstrap sample by drawing $m$ PSUs with replacement from the original set of $m$ PSUs. This results in a new dataset of $m$ clusters, $\\{(n_{I_j^{(b)}}, y_{I_j^{(b)}})\\}_{j=1}^m$, where each index $I_j^{(b)}$ is drawn uniformly from $\\{1, \\dots, m\\}$.\n    ii. Compute the proportion estimate, $\\hat{p}^{*(b)}$, for this bootstrap sample:\n    $$\n    \\hat{p}^{*(b)} \\;=\\; \\frac{\\sum_{j=1}^{m} y_{I^{(b)}_j}}{\\sum_{j=1}^{m} n_{I^{(b)}_j}}\n    $$\nb. The collection of $B$ estimates, $\\{\\hat{p}^{*(1)}, \\dots, \\hat{p}^{*(B)}\\}$, forms an empirical sampling distribution for $\\hat{p}$. The bootstrap variance estimate is the sample variance of these replicate estimates:\n$$\n\\hat{V}_{\\text{boot}}(\\hat{p}) \\;=\\; \\frac{1}{B-1}\\sum_{b=1}^{B}\\left(\\hat{p}^{*(b)} - \\bar{\\hat{p}}^{*}\\right)^2, \\quad \\text{where } \\bar{\\hat{p}}^{*} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{p}^{*(b)}\n$$\nTo ensure reproducibility of the random sampling process, a pseudo-random number generator is initialized with a specific seed for each test case.\n\nThe implementation will process each test case by first calculating $\\hat{p}$ and $\\hat{V}_{\\text{lin}}(\\hat{p})$. Then, it will run the bootstrap procedure for the specified number of replicates $B$ using the given seed to calculate $\\hat{V}_{\\text{boot}}(\\hat{p})$. Finally, it will format the three resulting values, rounded to $6$ decimal places, as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes proportion and variance estimates for cluster-sampled data\n    using both bootstrap and linearization methods for specified test cases.\n    \"\"\"\n\n    # Define the datasets from the problem statement.\n    dataset_A = [\n        (85, 12), (73, 9), (120, 20), (64, 6), (95, 14), (102, 13),\n        (58, 5), (80, 10), (140, 25), (77, 8), (66, 7), (90, 11)\n    ]\n    dataset_B = [\n        (150, 28), (45, 2), (60, 4), (110, 15), (90, 8), (200, 35),\n        (30, 0), (75, 7)\n    ]\n    dataset_C = [\n        (40, 3), (55, 7), (35, 2)\n    ]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'dataset': dataset_A, 'B': 10000, 'seed': 12345},\n        {'dataset': dataset_A, 'B': 500, 'seed': 12345},\n        {'dataset': dataset_B, 'B': 10000, 'seed': 202311},\n        {'dataset': dataset_C, 'B': 10000, 'seed': 777}\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        dataset = case['dataset']\n        B = case['B']\n        seed = case['seed']\n\n        # Convert dataset to numpy arrays for efficient computation\n        data = np.array(dataset, dtype=np.float64)\n        n_i = data[:, 0]\n        y_i = data[:, 1]\n        m = len(n_i)\n\n        # 1. Estimate the proportion, p_hat\n        sum_y_i = np.sum(y_i)\n        sum_n_i = np.sum(n_i)\n        p_hat = sum_y_i / sum_n_i\n\n        # 2. Calculate linearization variance (V_lin)\n        u_i = y_i - p_hat * n_i\n        sum_u_i_sq = np.sum(u_i**2)\n        v_lin = (m / (m - 1.0)) * sum_u_i_sq / (sum_n_i**2)\n\n        # 3. Calculate bootstrap variance (V_boot)\n        rng = np.random.default_rng(seed)\n        p_hat_star_b = np.empty(B)\n        original_indices = np.arange(m)\n        \n        for b in range(B):\n            # Draw m indices with replacement\n            bootstrap_indices = rng.choice(original_indices, size=m, replace=True)\n            \n            # Create bootstrap sample of clusters\n            y_i_star = y_i[bootstrap_indices]\n            n_i_star = n_i[bootstrap_indices]\n            \n            # Calculate proportion for the bootstrap sample\n            # Handle potential division by zero if all sampled clusters have size 0\n            sum_n_i_star = np.sum(n_i_star)\n            if sum_n_i_star == 0:\n                # This is unlikely with the given data but is good practice.\n                # A proportion is ill-defined. Can be set to 0 or NaN.\n                # Given the problem's data, this branch is not hit.\n                p_hat_star_b[b] = 0.0\n            else:\n                p_hat_star_b[b] = np.sum(y_i_star) / sum_n_i_star\n        \n        # Compute the sample variance of the bootstrap estimates\n        # ddof=1 ensures division by (B-1)\n        v_boot = np.var(p_hat_star_b, ddof=1)\n        \n        # Round results to 6 decimal places\n        p_hat_rounded = round(p_hat, 6)\n        v_boot_rounded = round(v_boot, 6)\n        v_lin_rounded = round(v_lin, 6)\n        \n        result_list = [p_hat_rounded, v_boot_rounded, v_lin_rounded]\n        \n        # Format for final output string. Use .6f to ensure 6 decimal places printed.\n        str_list = [f'{val:.6f}' for val in result_list]\n        all_results_str.append(f\"[{','.join(str_list)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "4942737"}]}