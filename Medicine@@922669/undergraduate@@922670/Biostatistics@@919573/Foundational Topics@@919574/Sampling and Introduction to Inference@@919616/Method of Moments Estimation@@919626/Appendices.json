{"hands_on_practices": [{"introduction": "This first practice problem gets to the heart of the Method of Moments by exploring the crucial concept of parameter identifiability. Using a simple, hypothetical biomarker model, you will first demonstrate why matching a single moment is not enough to pin down two unknown parameters. Then, by incorporating a second moment, you will see how the method provides a unique and elegant solution, solidifying your understanding of the core logic. [@problem_id:4927891]", "problem": "A biostatistician models a biomarker intensity $X$ measured on randomly selected cells from homogeneous tissue. Due to an all-or-none activation mechanism, the intensity is either baseline $0$ (inactive cell) or a fixed activated level $\\theta$ (active cell). Let $p \\in (0,1)$ denote the activation probability and $\\theta \\in (0,\\infty)$ denote the activated intensity. The distribution is\n$$\n\\Pr(X=0)=1-p, \\quad \\Pr(X=\\theta)=p,\n$$\nwith both $p$ and $\\theta$ unknown. A large independent and identically distributed (i.i.d.) sample is collected, and the first two empirical moments are computed as the sample mean $\\bar{X}$ and the sample second raw moment $\\overline{X^{2}}$. In one study, the observed values are $\\bar{X}=4$ and $\\overline{X^{2}}=28$.\n\nTasks:\n1) Using only the information $\\bar{X}=4$, explicitly construct two distinct parameter pairs $(p,\\theta)$ that yield the same theoretical first moment for $X$, thereby demonstrating that one moment alone does not identify $(p,\\theta)$ in this model.\n\n2) Using the method of moments (MoM), equate the first two theoretical moments of $X$ to the empirical moments $\\bar{X}$ and $\\overline{X^{2}}$ and solve symbolically for $\\theta$ in terms of the first two moments. Then substitute $\\bar{X}=4$ and $\\overline{X^{2}}=28$ to obtain a numerical estimate. Report only the numerical value of the MoM estimator of $\\theta$. No rounding is required.", "solution": "We begin from the core definitions in probability and statistics. For a random variable $X$, the first theoretical moment (the mean) is $E[X]$, and the second theoretical raw moment is $E[X^{2}]$. The method of moments (MoM) equates empirical moments from data to the corresponding theoretical moments and solves for the parameters.\n\nFor the given two-point distribution,\n$$\n\\Pr(X=0)=1-p, \\quad \\Pr(X=\\theta)=p,\n$$\nwe compute the first and second moments directly:\n$$\nE[X] = 0 \\cdot (1-p) + \\theta \\cdot p = p\\theta,\n$$\n$$\nE[X^{2}] = 0^{2} \\cdot (1-p) + \\theta^{2} \\cdot p = p\\theta^{2}.\n$$\n\nTask 1. Using only the information that the sample mean is $\\bar{X}=4$, the MoM approach based on the first moment alone would set $E[X]=p\\theta=4$. There are infinitely many pairs $(p,\\theta)$ with $p\\theta=4$ and $p \\in (0,1)$, $\\theta0$. Two concrete examples are:\n- $(p,\\theta)=(1,4)$, since $1 \\cdot 4 = 4$,\n- $(p,\\theta)=\\left(\\frac{1}{2},8\\right)$, since $\\frac{1}{2} \\cdot 8 = 4$.\nThese two distinct parameter pairs produce the same theoretical mean $E[X]=4$ but have different second moments:\n$$\nE[X^{2}]_{\\,(1,4)} = 1 \\cdot 4^{2} = 16, \\quad E[X^{2}]_{\\,(1/2,8)} = \\frac{1}{2} \\cdot 8^{2} = 32.\n$$\nThus, the first moment alone does not identify $(p,\\theta)$.\n\nTask 2. To identify $(p,\\theta)$, we use the first two moments. Let $\\mu_{1}=E[X]=p\\theta$ and $\\mu_{2}=E[X^{2}]=p\\theta^{2}$. Solving these equations, we eliminate $p$ by taking the ratio:\n$$\n\\frac{\\mu_{2}}{\\mu_{1}} = \\frac{p\\theta^{2}}{p\\theta} = \\theta.\n$$\nTherefore, symbolically,\n$$\n\\theta = \\frac{E[X^{2}]}{E[X]}.\n$$\nUnder the method of moments, we equate $\\mu_{1}$ to the sample mean $\\bar{X}$ and $\\mu_{2}$ to the sample second raw moment $\\overline{X^{2}}$. Substituting $\\bar{X}=4$ and $\\overline{X^{2}}=28$,\n$$\n\\hat{\\theta}_{\\text{MoM}} = \\frac{\\overline{X^{2}}}{\\bar{X}} = \\frac{28}{4} = 7.\n$$\nThis uses both moments to identify a unique value for $\\theta$ in this model.", "answer": "$$\\boxed{7}$$", "id": "4927891"}, {"introduction": "Moving from a simple discrete case to a widely used continuous distribution, this exercise tackles the lognormal model, which is essential for modeling right-skewed data like time-to-event or concentration levels in pharmacokinetics. This practice requires you to derive the population moments for the lognormal distribution and then solve the resulting non-linear system of equations for its underlying parameters, $\\mu$ and $\\sigma^2$. It is an excellent opportunity to connect the properties of the normal distribution to the MoM estimators for its log-transformed counterpart. [@problem_id:4814704]", "problem": "A clinical pharmacokinetics study models the time to peak plasma concentration after oral dosing as a positive random variable with multiplicative biological variability. Investigators posit a lognormal model: if $X$ denotes the time-to-peak for a randomly selected patient, then $Y=\\ln(X)$ is Gaussian with mean $\\mu$ and variance $\\sigma^{2}$. Let $X_{1},\\dots,X_{n}$ be independent and identically distributed observations from this model. Define the empirical mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ and the empirical second central moment $S^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}$, consistent with the method of moments approach.\n\nUsing the method of moments (MoM), derive closed-form estimators $\\hat{\\mu}_{\\text{MoM}}$ and $\\hat{\\sigma}^{2}_{\\text{MoM}}$ by equating the first two sample moments $\\bar{X}$ and $S^{2}$ to the corresponding population moments implied by the lognormal model. Your derivation must begin from fundamental definitions of the method of moments and established properties of the normal distribution for $Y=\\ln(X)$, and proceed to obtain expressions for $\\hat{\\mu}_{\\text{MoM}}$ and $\\hat{\\sigma}^{2}_{\\text{MoM}}$ solely in terms of $\\bar{X}$ and $S^{2}$.\n\nExpress your final answer as a single analytic row vector containing $\\hat{\\mu}_{\\text{MoM}}$ and $\\hat{\\sigma}^{2}_{\\text{MoM}}$. No numerical rounding is required.", "solution": "The method of moments (MoM) equates the first $k$ sample moments to the corresponding first $k$ population moments to obtain a system of $k$ equations for $k$ unknown parameters. The problem specifies that we should equate the sample mean $\\bar{X}$ and the sample variance $S^2$ to their population counterparts, the theoretical mean $E[X]$ and variance $\\text{Var}(X)$, respectively. This gives the following system of equations:\n$1.$ $\\bar{X} = E[X]$\n$2.$ $S^2 = \\text{Var}(X)$\n\nTo solve this system for $\\mu$ and $\\sigma^2$, we must first derive the expressions for $E[X]$ and $\\text{Var}(X)$ for a lognormal distribution. Let $X$ be a lognormally distributed random variable such that $Y = \\ln(X)$ is normally distributed with mean $\\mu$ and variance $\\sigma^2$, denoted $Y \\sim N(\\mu, \\sigma^2)$. The relationship is $X = \\exp(Y)$.\n\nThe moments of $X$ can be found using the moment-generating function (MGF) of the normal random variable $Y$. The MGF of $Y$ is given by:\n$$M_Y(t) = E[\\exp(tY)] = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right)$$\n\nThe $k$-th raw moment of $X$ is $E[X^k] = E[\\exp(kY)]$. This is precisely the MGF of $Y$ evaluated at $t=k$.\n$$E[X^k] = M_Y(k) = \\exp\\left(\\mu k + \\frac{1}{2}\\sigma^2 k^2\\right)$$\n\nUsing this formula, we find the first two raw moments of $X$:\nThe first raw moment (the mean) is found by setting $k=1$:\n$$E[X] = \\exp\\left(\\mu(1) + \\frac{1}{2}\\sigma^2 (1)^2\\right) = \\exp\\left(\\mu + \\frac{1}{2}\\sigma^2\\right)$$\n\nThe second raw moment is found by setting $k=2$:\n$$E[X^2] = \\exp\\left(\\mu(2) + \\frac{1}{2}\\sigma^2 (2)^2\\right) = \\exp\\left(2\\mu + 2\\sigma^2\\right)$$\n\nThe variance of $X$ is given by $\\text{Var}(X) = E[X^2] - (E[X])^2$. Substituting the expressions above:\n$$\\text{Var}(X) = \\exp(2\\mu + 2\\sigma^2) - \\left[\\exp\\left(\\mu + \\frac{1}{2}\\sigma^2\\right)\\right]^2$$\n$$\\text{Var}(X) = \\exp(2\\mu + 2\\sigma^2) - \\exp\\left(2\\mu + \\sigma^2\\right)$$\nWe can factor out a common term to simplify this expression. A convenient choice is $(E[X])^2 = \\exp(2\\mu + \\sigma^2)$.\n$$\\text{Var}(X) = \\exp(2\\mu + \\sigma^2) \\left(\\exp(\\sigma^2) - 1\\right) = (E[X])^2 \\left(\\exp(\\sigma^2) - 1\\right)$$\n\nNow, we substitute these population moments into our MoM system of equations. The estimators $\\hat{\\mu}_{\\text{MoM}}$ and $\\hat{\\sigma}^2_{\\text{MoM}}$ (heretofore denoted as $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ for simplicity) must satisfy:\n$$(1) \\quad \\bar{X} = \\exp\\left(\\hat{\\mu} + \\frac{1}{2}\\hat{\\sigma}^2\\right)$$\n$$(2) \\quad S^2 = (\\bar{X})^2 \\left(\\exp\\left(\\hat{\\sigma}^2\\right) - 1\\right)$$\n\nWe solve this system for $\\hat{\\mu}$ and $\\hat{\\sigma}^2$. From equation $(2)$, we can isolate $\\hat{\\sigma}^2$:\n$$\\frac{S^2}{\\bar{X}^2} = \\exp(\\hat{\\sigma}^2) - 1$$\n$$1 + \\frac{S^2}{\\bar{X}^2} = \\exp(\\hat{\\sigma}^2)$$\nTaking the natural logarithm of both sides gives the estimator for $\\sigma^2$:\n$$\\hat{\\sigma}^2_{\\text{MoM}} = \\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right)$$\n\nNext, we solve for $\\hat{\\mu}$ using equation $(1)$. Taking the natural logarithm of both sides of equation $(1)$:\n$$\\ln(\\bar{X}) = \\hat{\\mu} + \\frac{1}{2}\\hat{\\sigma}^2$$\nRearranging to solve for $\\hat{\\mu}$:\n$$\\hat{\\mu} = \\ln(\\bar{X}) - \\frac{1}{2}\\hat{\\sigma}^2$$\nSubstituting the expression we just found for $\\hat{\\sigma}^2_{\\text{MoM}}$:\n$$\\hat{\\mu}_{\\text{MoM}} = \\ln(\\bar{X}) - \\frac{1}{2}\\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right)$$\n\nThese are the method of moments estimators for $\\mu$ and $\\sigma^2$ expressed solely in terms of the sample moments $\\bar{X}$ and $S^2$. The problem requires the final answer to be a single analytic row vector containing $\\hat{\\mu}_{\\text{MoM}}$ and $\\hat{\\sigma}^2_{\\text{MoM}}$.\nThe resulting vector is $\\begin{pmatrix} \\hat{\\mu}_{\\text{MoM}}  \\hat{\\sigma}^2_{\\text{MoM}} \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\ln(\\bar{X}) - \\frac{1}{2}\\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right)  \\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right) \\end{pmatrix}}\n$$", "id": "4814704"}, {"introduction": "Our final practice problem demonstrates the flexibility and power of the Method of Moments when applied to more complex, modern statistical models. You will work with a zero-inflated binomial distribution, a type of mixture model crucial for handling biological data with excess zero counts (e.g., from clinical assays). This exercise illustrates a generalized approach to MoM, where you will equate not only the sample mean but also the sample proportion of zeros to their theoretical counterparts to estimate the model's parameters. [@problem_id:4927859]", "problem": "A clinical assay measures the number of detected target molecules in duplicate ($n=2$ technical replicates per specimen). Because some specimens are structurally non-expressing, the observed counts are modeled by a zero-inflated binomial distribution: with probability $\\omega$ a specimen is structurally zero, and with probability $1-\\omega$ the count arises from a $\\operatorname{Binomial}(n,p)$ distribution with success probability $p$ per replicate. A sample of $N=20$ independent specimens produced the following summary across the two replicates: $10$ specimens had $0$ detections, $8$ had $1$ detection, and $2$ had $2$ detections.\n\nUsing the method-of-moments estimation approach grounded in the principle that sample moments approximate the corresponding population moments, construct estimators for the parameters $\\omega$ and $p$ based on the sample mean count and the sample proportion of zeros. Then, using the given data, compute the numerical estimates of $\\omega$ and $p$. Round each estimator to four significant figures. Express your final answer as a single row matrix containing the two rounded estimates. No units are required.", "solution": "The problem requires the estimation of the parameters $\\omega$ and $p$ for a zero-inflated binomial distribution using the method of moments. Let $X$ be a random variable representing the number of detected target molecules for a single specimen. The distribution of $X$ is a mixture model. With probability $\\omega$, the count is $0$ (structural zero). With probability $1-\\omega$, the count follows a Binomial distribution with parameters $n=2$ and success probability $p$.\n\nThe probability mass function (PMF) of $X$, denoted $f(k) = P(X=k)$, is given by:\n$P(X=0) = \\omega + (1-\\omega)P(\\text{Binomial}(n,p)=0)$\n$P(X=k) = (1-\\omega)P(\\text{Binomial}(n,p)=k)$ for $k  0$.\n\nGiven $n=2$, the binomial probabilities are:\n$P(\\text{Binomial}(2,p)=0) = \\binom{2}{0}p^0(1-p)^2 = (1-p)^2$\n$P(\\text{Binomial}(2,p)=1) = \\binom{2}{1}p^1(1-p)^1 = 2p(1-p)$\n$P(\\text{Binomial}(2,p)=2) = \\binom{2}{2}p^2(1-p)^0 = p^2$\n\nThus, the PMF for $X$ is:\n$P(X=0) = \\omega + (1-\\omega)(1-p)^2$\n$P(X=1) = (1-\\omega)2p(1-p)$\n$P(X=2) = (1-\\omega)p^2$\n\nThe method of moments estimation procedure requires equating population moments (or other population-level quantities) to their corresponding sample estimates. The problem specifies using the population mean and the population proportion of zeros.\n\nFirst, we derive the population mean, $E[X]$.\n$$ E[X] = \\sum_{k=0}^{2} k \\cdot P(X=k) $$\n$$ E[X] = (0 \\cdot P(X=0)) + (1 \\cdot P(X=1)) + (2 \\cdot P(X=2)) $$\n$$ E[X] = (1-\\omega)2p(1-p) + 2(1-\\omega)p^2 $$\n$$ E[X] = 2p(1-\\omega)(1-p+p) $$\n$$ E[X] = 2p(1-\\omega) $$\n\nThe second population quantity is the probability of observing a zero count, $P(X=0)$.\n$$ P(X=0) = \\omega + (1-\\omega)(1-p)^2 $$\n\nNext, we calculate the corresponding sample quantities from the provided data. The sample consists of $N=20$ specimens, with observed counts: $10$ specimens with $0$ detections, $8$ with $1$ detection, and $2$ with $2$ detections.\n\nThe sample mean, $\\bar{x}$, is:\n$$ \\bar{x} = \\frac{\\sum_{i=1}^{N} x_i}{N} = \\frac{(10 \\times 0) + (8 \\times 1) + (2 \\times 2)}{20} = \\frac{0 + 8 + 4}{20} = \\frac{12}{20} = \\frac{3}{5} $$\n\nThe sample proportion of zeros, $p_0$, is:\n$$ p_0 = \\frac{\\text{Number of zero counts}}{N} = \\frac{10}{20} = \\frac{1}{2} $$\n\nNow, we construct the estimators $\\hat{\\omega}$ and $\\hat{p}$ by equating the population quantities to the sample quantities:\n1. $E[X] = \\bar{x} \\implies 2\\hat{p}(1-\\hat{\\omega}) = \\bar{x}$\n2. $P(X=0) = p_0 \\implies \\hat{\\omega} + (1-\\hat{\\omega})(1-\\hat{p})^2 = p_0$\n\nWe now solve this system of two equations for $\\hat{p}$ and $\\hat{\\omega}$. From equation (1), assuming $\\hat{p} \\neq 0$:\n$$ 1-\\hat{\\omega} = \\frac{\\bar{x}}{2\\hat{p}} $$\nThis also implies:\n$$ \\hat{\\omega} = 1 - \\frac{\\bar{x}}{2\\hat{p}} $$\nSubstitute these two expressions for $1-\\hat{\\omega}$ and $\\hat{\\omega}$ into equation (2):\n$$ \\left(1 - \\frac{\\bar{x}}{2\\hat{p}}\\right) + \\left(\\frac{\\bar{x}}{2\\hat{p}}\\right)(1-\\hat{p})^2 = p_0 $$\nExpanding the term $(1-\\hat{p})^2 = 1 - 2\\hat{p} + \\hat{p}^2$:\n$$ 1 - \\frac{\\bar{x}}{2\\hat{p}} + \\frac{\\bar{x}(1 - 2\\hat{p} + \\hat{p}^2)}{2\\hat{p}} = p_0 $$\nCombine the fractions:\n$$ 1 + \\frac{-\\bar{x} + \\bar{x} - 2\\bar{x}\\hat{p} + \\bar{x}\\hat{p}^2}{2\\hat{p}} = p_0 $$\n$$ 1 + \\frac{-2\\bar{x}\\hat{p} + \\bar{x}\\hat{p}^2}{2\\hat{p}} = p_0 $$\nFactor out $\\hat{p}$ from the numerator (since $\\bar{x}  0$, we must have $\\hat{p}  0$):\n$$ 1 + \\frac{\\hat{p}(-2\\bar{x} + \\bar{x}\\hat{p})}{2\\hat{p}} = p_0 $$\n$$ 1 + \\frac{-2\\bar{x} + \\bar{x}\\hat{p}}{2} = p_0 $$\n$$ 1 - \\bar{x} + \\frac{\\bar{x}\\hat{p}}{2} = p_0 $$\nNow, we isolate $\\hat{p}$:\n$$ \\frac{\\bar{x}\\hat{p}}{2} = p_0 + \\bar{x} - 1 $$\n$$ \\hat{p} = \\frac{2(p_0 + \\bar{x} - 1)}{\\bar{x}} $$\nThis is the general estimator for $p$.\n\nThe estimator for $\\omega$ is then:\n$$ \\hat{\\omega} = 1 - \\frac{\\bar{x}}{2\\hat{p}} $$\n\nNow, we compute the numerical estimates using $\\bar{x} = 3/5 = 0.6$ and $p_0 = 1/2 = 0.5$.\n$$ \\hat{p} = \\frac{2(0.5 + 0.6 - 1)}{0.6} = \\frac{2(0.1)}{0.6} = \\frac{0.2}{0.6} = \\frac{1}{3} $$\nTo find $\\hat{\\omega}$, we use its derived formula:\n$$ \\hat{\\omega} = 1 - \\frac{\\bar{x}}{2\\hat{p}} = 1 - \\frac{0.6}{2(1/3)} = 1 - \\frac{0.6}{2/3} = 1 - 0.9 = 0.1 $$\nThe estimates are $\\hat{p} = 1/3$ and $\\hat{\\omega} = 0.1$.\n\nThe problem requires rounding to four significant figures.\nFor $\\hat{\\omega}$:\n$$ \\hat{\\omega} = 0.1 = 0.1000 $$\nFor $\\hat{p}$:\n$$ \\hat{p} = \\frac{1}{3} = 0.333333... \\approx 0.3333 $$\nThe final answer is a row matrix of these two estimates.", "answer": "$$ \\boxed{\\begin{pmatrix} 0.1000  0.3333 \\end{pmatrix}} $$", "id": "4927859"}]}