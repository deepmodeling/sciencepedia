{"hands_on_practices": [{"introduction": "The Law of Large Numbers tells us that a sample average converges to the population mean, but its power extends further. This foundational exercise [@problem_id:1957103] demonstrates how the law applies not just to the random variables themselves, but also to functions of them. By calculating the long-term average signal power, you will practice applying the Strong Law of Large Numbers to find the expected value of squared measurements, connecting the law to the fundamental statistical concepts of mean and variance.", "problem": "In a digital signal processing system, a sequence of measurements $X_1, X_2, \\ldots, X_n, \\ldots$ are taken from a noisy source. These measurements are modeled as a sequence of Independent and Identically Distributed (i.i.d.) random variables. Each random variable $X_i$ has a finite expected value (mean) $E[X_i] = \\mu$ and a finite, non-zero variance $\\text{Var}(X_i) = \\sigma^2$.\n\nA key performance metric for this system is the long-term average signal power, which is mathematically represented by the sample mean of the squared measurements. Determine the value to which the quantity $S_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2$ converges with probability 1 as the number of measurements $n$ approaches infinity. Express your answer in terms of $\\mu$ and $\\sigma$.", "solution": "The problem asks for the almost sure limit of the quantity $S_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2$ as $n \\to \\infty$. This is a direct application of the Strong Law of Large Numbers (SLLN).\n\nLet us define a new sequence of random variables $Y_i = X_i^2$. The expression for $S_n$ can then be rewritten as the sample mean of this new sequence:\n$$ S_n = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}_n $$\n\nThe Strong Law of Large Numbers states that for a sequence of Independent and Identically Distributed (i.i.d.) random variables, say $Y_1, Y_2, \\ldots$, with a finite expected value $E[Y_i] = \\nu$, their sample mean $\\bar{Y}_n$ converges almost surely (i.e., with probability 1) to this expected value. Mathematically,\n$$ \\bar{Y}_n \\xrightarrow{a.s.} \\nu \\quad \\text{as } n \\to \\infty $$\n\nTo apply the SLLN to our problem, we must verify that the sequence $Y_i = X_i^2$ meets the necessary conditions:\n1.  **I.i.d. condition**: The problem states that the random variables $X_1, X_2, \\ldots$ are i.i.d. Since each $Y_i$ is derived from the corresponding $X_i$ using the same function (the square function, $g(x) = x^2$), the sequence of random variables $Y_1, Y_2, \\ldots$ is also i.i.d.\n2.  **Finite mean condition**: We need to calculate the expected value of $Y_i$ and check if it is finite.\n    $$ E[Y_i] = E[X_i^2] $$\n    We can find $E[X_i^2]$ using the definition of variance for any random variable $X$:\n    $$ \\text{Var}(X) = E[X^2] - (E[X])^2 $$\n    The problem provides the mean and variance for each $X_i$:\n    $$ E[X_i] = \\mu $$\n    $$ \\text{Var}(X_i) = \\sigma^2 $$\n    Substituting these values into the variance formula:\n    $$ \\sigma^2 = E[X_i^2] - \\mu^2 $$\n    Solving for $E[X_i^2]$, we get:\n    $$ E[X_i^2] = \\mu^2 + \\sigma^2 $$\n    Since both $\\mu$ and $\\sigma^2$ are given as finite constants, their sum $\\mu^2 + \\sigma^2$ is also a finite value. Thus, the expected value of $Y_i$ is finite, and is equal to $\\mu^2 + \\sigma^2$.\n\nSince both conditions of the SLLN are satisfied for the sequence $Y_i$, we can conclude that the sample mean $\\bar{Y}_n$ converges almost surely to the expected value $E[Y_i]$.\n$$ \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n Y_i \\xrightarrow{a.s.} E[Y_i] $$\nSubstituting $Y_i = X_i^2$ and $E[Y_i] = \\mu^2 + \\sigma^2$:\n$$ \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\xrightarrow{a.s.} \\mu^2 + \\sigma^2 $$\nThe phrase \"converges with probability 1\" is synonymous with \"converges almost surely\". Therefore, the long-term average signal power converges to $\\mu^2 + \\sigma^2$.", "answer": "$$\\boxed{\\mu^{2} + \\sigma^{2}}$$", "id": "1957103"}, {"introduction": "Real-world biostatistical analysis often involves comparing different quantities, such as the proportion of defective items to an average physical measurement. This problem [@problem_id:1967350] challenges you to determine the limit of a ratio of two different sample averages, a common scenario when dealing with rates. To solve it, you will combine the Law of Large Numbers with the Continuous Mapping Theorem, a powerful technique for finding the limits of functions of random variables.", "problem": "An electronics manufacturer is conducting a quality control study on a particular model of resistor. They model two key characteristics for a random sample of resistors. For the $i$-th resistor in the sample, let $X_i$ be an indicator variable such that $X_i=1$ if the resistor is defective and $X_i=0$ if it is not. Let $Y_i$ be the measured resistance in ohms ($\\Omega$).\n\nThe random variables $X_1, X_2, \\dots$ are Independent and Identically Distributed (IID) Bernoulli random variables with a probability of being defective given by $p = 0.04$.\n\nThe random variables $Y_1, Y_2, \\dots$ are also IID, and their values are drawn from a continuous Uniform distribution on the interval $[980, 1020]$. The sequences $\\{X_i\\}$ and $\\{Y_i\\}$ are independent of each other.\n\nThe manufacturer is interested in a specific metric derived from a large sample of size $n$. Let $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample proportion of defective resistors and $\\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^{n} Y_i$ be the sample average resistance.\n\nDetermine the numerical value that the ratio $R_n = \\frac{\\bar{X}_n}{\\bar{Y}_n}$ converges to as the sample size $n$ tends to infinity. Express your answer in scientific notation of the form $a \\times 10^b$, where $1 \\le a  10$. The value of $a$ should be rounded to two significant figures. Answer in units of $\\Omega^{-1}$.", "solution": "We have $X_{i} \\sim \\text{Bernoulli}(p)$ with $p=0.04$, $Y_{i} \\sim \\text{Uniform}[a,b]$ with $a=980$ and $b=1020$, and the sequences $\\{X_{i}\\}$ and $\\{Y_{i}\\}$ are independent. Define $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ and $\\bar{Y}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$.\n\nBy the Strong Law of Large Numbers, \n$$\n\\bar{X}_{n} \\to \\mathbb{E}[X_{1}] \\quad \\text{and} \\quad \\bar{Y}_{n} \\to \\mathbb{E}[Y_{1}] \\quad \\text{almost surely as } n \\to \\infty.\n$$\nCompute the expectations:\n$$\n\\mathbb{E}[X_{1}] = p = 0.04,\n$$\nand for the uniform distribution,\n$$\n\\mathbb{E}[Y_{1}] = \\frac{a+b}{2} = \\frac{980+1020}{2} = 1000.\n$$\nSince $\\mathbb{E}[Y_{1}] = 1000 \\neq 0$, by the Continuous Mapping Theorem,\n$$\nR_{n}=\\frac{\\bar{X}_{n}}{\\bar{Y}_{n}} \\to \\frac{\\mathbb{E}[X_{1}]}{\\mathbb{E}[Y_{1}]} = \\frac{0.04}{1000} = 4.0 \\times 10^{-5} \\quad \\text{almost surely}.\n$$\nThe units are $\\Omega^{-1}$, as $\\bar{X}_{n}$ is dimensionless and $\\bar{Y}_{n}$ has units of $\\Omega$. The required scientific notation with two significant figures is $4.0 \\times 10^{-5}$.", "answer": "$$\\boxed{4.0 \\times 10^{-5}}$$", "id": "1967350"}, {"introduction": "The Law of Large Numbers guarantees convergence, but for practical purposes like designing a study, we must ask: how many samples are enough? This exercise [@problem_id:4849433] bridges abstract theory and clinical application by tasking you with determining the minimum sample size for a medical trial. Using the Bienaymé–Chebyshev inequality, which provides a quantitative underpinning for the Weak Law of Large Numbers, you will calculate a sample size that guarantees a desired level of precision, a fundamental skill in evidence-based medicine.", "problem": "A single-arm early-phase medical trial monitors binary clinical response to a new antiviral therapy in patients with acute infection. Let $X_{1}, X_{2}, \\dots, X_{n}$ be independent and identically distributed Bernoulli random variables with success probability $p \\in (0,1)$, where $X_{i} = 1$ indicates a clinical response for patient $i$ and $X_{i} = 0$ indicates no response. The sample proportion is $\\hat{p}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$. Investigators require that, uniformly over all $p \\in (0,1)$, the Bienaymé–Chebyshev inequality yields an upper bound on the deviation probability $P\\!\\left(|\\hat{p}_{n} - p| \\ge \\epsilon\\right)$ that is no greater than a target level $\\alpha$. Using only the independence, the Bernoulli model, and the Bienaymé–Chebyshev inequality, determine the smallest integer $n$ such that this requirement holds when $\\epsilon = 0.05$ and $\\alpha = 0.01$. Provide your final answer as a single integer with no additional symbols. No rounding is required beyond selecting the smallest integer that meets the requirement.", "solution": "The problem requires finding the smallest sample size $n$ that guarantees a certain level of precision for the sample proportion $\\hat{p}_{n}$ as an estimator of the true population proportion $p$, uniformly for all possible values of $p \\in (0,1)$. The guarantee must be established using the Bienaymé–Chebyshev inequality.\n\nLet $X_{1}, X_{2}, \\dots, X_{n}$ be a sequence of independent and identically distributed (i.i.d.) Bernoulli random variables with parameter $p$, such that $P(X_i = 1) = p$ and $P(X_i = 0) = 1-p$. The expected value and variance of each $X_i$ are given by:\n$$E[X_i] = p$$\n$$\\text{Var}(X_i) = p(1-p)$$\n\nThe sample proportion is defined as $\\hat{p}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$. We first determine the expected value and variance of $\\hat{p}_{n}$.\n\nThe expected value of $\\hat{p}_{n}$ is:\n$$E[\\hat{p}_{n}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_{i}] = \\frac{1}{n} \\sum_{i=1}^{n} p = \\frac{1}{n} (np) = p$$\nThis shows that $\\hat{p}_{n}$ is an unbiased estimator of $p$.\n\nThe variance of $\\hat{p}_{n}$, using the independence of the $X_i$, is:\n$$\\text{Var}(\\hat{p}_{n}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_{i})$$\n$$\\text{Var}(\\hat{p}_{n}) = \\frac{1}{n^2} \\sum_{i=1}^{n} p(1-p) = \\frac{1}{n^2} (n p(1-p)) = \\frac{p(1-p)}{n}$$\n\nThe Bienaymé–Chebyshev inequality states that for any random variable $Y$ with finite mean $\\mu$ and finite non-zero variance $\\sigma^2$, and for any real number $\\epsilon  0$:\n$$P(|Y - \\mu| \\ge \\epsilon) \\le \\frac{\\sigma^2}{\\epsilon^2}$$\n\nApplying this inequality to the random variable $Y = \\hat{p}_{n}$, with mean $\\mu = p$ and variance $\\sigma^2 = \\frac{p(1-p)}{n}$, we get:\n$$P(|\\hat{p}_{n} - p| \\ge \\epsilon) \\le \\frac{\\text{Var}(\\hat{p}_{n})}{\\epsilon^2} = \\frac{p(1-p)}{n\\epsilon^2}$$\n\nThe problem requires that the upper bound provided by this inequality be no greater than a specified level $\\alpha$, i.e.,\n$$\\frac{p(1-p)}{n\\epsilon^2} \\le \\alpha$$\nThis condition must hold uniformly for all $p \\in (0,1)$. To satisfy this, we must consider the worst-case scenario for the term $p(1-p)$. Let $f(p) = p(1-p) = p-p^2$. We need to find the maximum value of $f(p)$ on the interval $(0,1)$. The derivative is $f'(p) = 1 - 2p$. Setting $f'(p) = 0$ gives $p = \\frac{1}{2}$. Since the second derivative is $f''(p) = -2  0$, this value of $p$ corresponds to a maximum.\nThe maximum value of $p(1-p)$ for $p \\in (0,1)$ is therefore:\n$$\\sup_{p \\in (0,1)} p(1-p) = \\left(\\frac{1}{2}\\right)\\left(1 - \\frac{1}{2}\\right) = \\frac{1}{4}$$\n\nSubstituting this maximum value into our inequality ensures that the condition holds for all $p$:\n$$\\frac{1/4}{n\\epsilon^2} \\le \\alpha$$\n$$\\frac{1}{4n\\epsilon^2} \\le \\alpha$$\n\nWe now solve for $n$. Rearranging the inequality, we get:\n$$1 \\le 4n\\epsilon^2\\alpha$$\n$$n \\ge \\frac{1}{4\\epsilon^2\\alpha}$$\n\nThe problem specifies the values $\\epsilon = 0.05$ and $\\alpha = 0.01$. We substitute these values into the inequality:\n$$n \\ge \\frac{1}{4(0.05)^2(0.01)}$$\nFirst, calculate the denominator:\n$$(0.05)^2 = 0.0025$$\n$$4 \\times (0.05)^2 \\times 0.01 = 4 \\times 0.0025 \\times 0.01 = 0.01 \\times 0.01 = 0.0001 = 10^{-4}$$\nSubstituting this back into the inequality for $n$:\n$$n \\ge \\frac{1}{10^{-4}}$$\n$$n \\ge 10000$$\n\nThe problem asks for the smallest integer $n$ that satisfies this condition. The smallest integer $n$ such that $n \\ge 10000$ is $n = 10000$.", "answer": "$$\\boxed{10000}$$", "id": "4849433"}]}