{"hands_on_practices": [{"introduction": "In statistical estimation, we often have a default, intuitive estimator for a parameter, such as the sample mean for the population mean. A common and desirable property for an estimator is to be unbiased, meaning its expected value equals the true parameter value. This exercise challenges the notion that an unbiased estimator is always the best choice by comparing the ubiquitous sample mean to a simple \"shrinkage\" estimator, which is intentionally biased. By analyzing the Mean Squared Error (MSE) of both, you will discover the core principle of the bias-variance tradeoff and see that sometimes, accepting a little bias can lead to a significant reduction in overall error. [@problem_id:4926200]", "problem": "A biostatistician is evaluating two estimators of a treatment effect parameter $\\theta$ in a small clinical study. Let $X_{1},\\dots,X_{n}$ be independent outcomes from a population with a known finite-sample distribution and finite second moment under parameter value $\\theta$. For any estimator $\\delta(X_{1},\\dots,X_{n})$, define the mean squared error at $\\theta$ as $\\mathrm{MSE}_{\\theta}(\\delta)=\\mathbb{E}_{\\theta}\\!\\left[(\\delta-\\theta)^{2}\\right]$, where the expectation is taken with respect to the joint law of the data under $\\theta$.\n\n1. In general, starting from the definition of $\\mathrm{MSE}_{\\theta}(\\delta)$ above, express $\\mathrm{MSE}_{\\theta}(\\delta)$ in terms of the variance and bias of $\\delta$ at $\\theta$, and hence state the condition on $\\theta$ under which $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{1})<\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{2})$ for two given estimators $\\hat{\\theta}_{1}$ and $\\hat{\\theta}_{2}$ with known finite-sample distributions.\n\n2. As a concrete parametric example grounded in a biostatistical setting, suppose $X_{1},\\dots,X_{n}\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{N}(\\theta,\\sigma^{2})$ with known $\\sigma^{2}>0$. Consider two estimators of $\\theta$:\n- $\\hat{\\theta}_{1}=\\bar{X}$, the sample mean.\n- $\\hat{\\theta}_{2}=\\alpha\\,\\bar{X}$, a shrinkage estimator with a known constant $\\alpha$ satisfying $0<\\alpha<1$.\nUsing only the model and the definition of $\\mathrm{MSE}_{\\theta}(\\delta)$, determine for which values of $\\theta$ the inequality $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{2})<\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{1})$ holds.\n\n3. Report, as your final answer, the exact closed-form expression for the positive critical threshold $T(n,\\sigma^{2},\\alpha)$ such that $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{2})<\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{1})$ if and only if $|\\theta|<T(n,\\sigma^{2},\\alpha)$. Provide only $T(n,\\sigma^{2},\\alpha)$ as a single analytic expression. Do not numerically approximate it.", "solution": "Part 1: The Mean Squared Error (MSE) Decomposition\n\nThe mean squared error of an estimator $\\delta$ for a parameter $\\theta$ is defined as $\\mathrm{MSE}_{\\theta}(\\delta) = \\mathbb{E}_{\\theta}[(\\delta - \\theta)^2]$. Let $\\mathbb{E}_{\\theta}(\\delta)$ denote the expected value of the estimator $\\delta$. The bias of the estimator is defined as $\\mathrm{Bias}_{\\theta}(\\delta) = \\mathbb{E}_{\\theta}(\\delta) - \\theta$. The variance of the estimator is $\\mathrm{Var}_{\\theta}(\\delta) = \\mathbb{E}_{\\theta}[(\\delta - \\mathbb{E}_{\\theta}(\\delta))^2]$.\n\nTo express the MSE in terms of bias and variance, we add and subtract $\\mathbb{E}_{\\theta}(\\delta)$ inside the squared term of the MSE definition:\n$$\n\\mathrm{MSE}_{\\theta}(\\delta) = \\mathbb{E}_{\\theta}[(\\delta - \\mathbb{E}_{\\theta}(\\delta) + \\mathbb{E}_{\\theta}(\\delta) - \\theta)^2]\n$$\nLet us expand the square:\n$$\n\\mathrm{MSE}_{\\theta}(\\delta) = \\mathbb{E}_{\\theta}[(\\delta - \\mathbb{E}_{\\theta}(\\delta))^2 + 2(\\delta - \\mathbb{E}_{\\theta}(\\delta))(\\mathbb{E}_{\\theta}(\\delta) - \\theta) + (\\mathbb{E}_{\\theta}(\\delta) - \\theta)^2]\n$$\nBy the linearity of expectation, we can distribute the expectation operator:\n$$\n\\mathrm{MSE}_{\\theta}(\\delta) = \\mathbb{E}_{\\theta}[(\\delta - \\mathbb{E}_{\\theta}(\\delta))^2] + \\mathbb{E}_{\\theta}[2(\\delta - \\mathbb{E}_{\\theta}(\\delta))(\\mathbb{E}_{\\theta}(\\delta) - \\theta)] + \\mathbb{E}_{\\theta}[(\\mathbb{E}_{\\theta}(\\delta) - \\theta)^2]\n$$\nWe analyze each term separately:\n1. The first term is the definition of the variance of $\\delta$: $\\mathbb{E}_{\\theta}[(\\delta - \\mathbb{E}_{\\theta}(\\delta))^2] = \\mathrm{Var}_{\\theta}(\\delta)$.\n2. The third term involves an expectation of a quantity that is constant with respect to the random sample, as $\\mathbb{E}_{\\theta}(\\delta)$ and $\\theta$ are not random variables. Thus, $\\mathbb{E}_{\\theta}[(\\mathbb{E}_{\\theta}(\\delta) - \\theta)^2] = (\\mathbb{E}_{\\theta}(\\delta) - \\theta)^2 = (\\mathrm{Bias}_{\\theta}(\\delta))^2$.\n3. For the second term (the cross-term), we can factor out the constant part $(\\mathbb{E}_{\\theta}(\\delta) - \\theta)$:\n$$\n\\mathbb{E}_{\\theta}[2(\\delta - \\mathbb{E}_{\\theta}(\\delta))(\\mathbb{E}_{\\theta}(\\delta) - \\theta)] = 2(\\mathbb{E}_{\\theta}(\\delta) - \\theta) \\mathbb{E}_{\\theta}[\\delta - \\mathbb{E}_{\\theta}(\\delta)]\n$$\nThe expectation $\\mathbb{E}_{\\theta}[\\delta - \\mathbb{E}_{\\theta}(\\delta)]$ is equal to $\\mathbb{E}_{\\theta}(\\delta) - \\mathbb{E}_{\\theta}[\\mathbb{E}_{\\theta}(\\delta)] = \\mathbb{E}_{\\theta}(\\delta) - \\mathbb{E}_{\\theta}(\\delta) = 0$. Therefore, the cross-term is $0$.\n\nCombining these results, we obtain the bias-variance decomposition of MSE:\n$$\n\\mathrm{MSE}_{\\theta}(\\delta) = \\mathrm{Var}_{\\theta}(\\delta) + (\\mathrm{Bias}_{\\theta}(\\delta))^2\n$$\nFor two estimators $\\hat{\\theta}_{1}$ and $\\hat{\\theta}_{2}$, the condition $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{1}) < \\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{2})$ is therefore equivalent to the inequality:\n$$\n\\mathrm{Var}_{\\theta}(\\hat{\\theta}_{1}) + (\\mathrm{Bias}_{\\theta}(\\hat{\\theta}_{1}))^2 < \\mathrm{Var}_{\\theta}(\\hat{\\theta}_{2}) + (\\mathrm{Bias}_{\\theta}(\\hat{\\theta}_{2}))^2\n$$\n\nPart 2: Comparison of Estimators for the Normal Mean\n\nWe are given $X_{1}, \\dots, X_{n} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(\\theta, \\sigma^2)$ with known $\\sigma^2 > 0$. The sample mean $\\bar{X}$ has the distribution $\\bar{X} \\sim \\mathcal{N}(\\theta, \\frac{\\sigma^2}{n})$.\n\nFirst, we analyze estimator $\\hat{\\theta}_{1} = \\bar{X}$:\n- Expectation: $\\mathbb{E}_{\\theta}(\\hat{\\theta}_{1}) = \\mathbb{E}_{\\theta}(\\bar{X}) = \\theta$.\n- Bias: $\\mathrm{Bias}_{\\theta}(\\hat{\\theta}_{1}) = \\mathbb{E}_{\\theta}(\\hat{\\theta}_{1}) - \\theta = \\theta - \\theta = 0$. This estimator is unbiased.\n- Variance: $\\mathrm{Var}_{\\theta}(\\hat{\\theta}_{1}) = \\mathrm{Var}_{\\theta}(\\bar{X}) = \\frac{\\sigma^2}{n}$.\n- MSE: Using the decomposition, $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{1}) = \\mathrm{Var}_{\\theta}(\\hat{\\theta}_{1}) + (\\mathrm{Bias}_{\\theta}(\\hat{\\theta}_{1}))^2 = \\frac{\\sigma^2}{n} + 0^2 = \\frac{\\sigma^2}{n}$.\n\nNext, we analyze the shrinkage estimator $\\hat{\\theta}_{2} = \\alpha\\bar{X}$, for a known constant $\\alpha$ with $0 < \\alpha < 1$:\n- Expectation: $\\mathbb{E}_{\\theta}(\\hat{\\theta}_{2}) = \\mathbb{E}_{\\theta}(\\alpha\\bar{X}) = \\alpha\\mathbb{E}_{\\theta}(\\bar{X}) = \\alpha\\theta$.\n- Bias: $\\mathrm{Bias}_{\\theta}(\\hat{\\theta}_{2}) = \\mathbb{E}_{\\theta}(\\hat{\\theta}_{2}) - \\theta = \\alpha\\theta - \\theta = (\\alpha - 1)\\theta$.\n- Variance: $\\mathrm{Var}_{\\theta}(\\hat{\\theta}_{2}) = \\mathrm{Var}_{\\theta}(\\alpha\\bar{X}) = \\alpha^2\\mathrm{Var}_{\\theta}(\\bar{X}) = \\alpha^2\\frac{\\sigma^2}{n}$.\n- MSE: $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{2}) = \\mathrm{Var}_{\\theta}(\\hat{\\theta}_{2}) + (\\mathrm{Bias}_{\\theta}(\\hat{\\theta}_{2}))^2 = \\alpha^2\\frac{\\sigma^2}{n} + ((\\alpha - 1)\\theta)^2 = \\frac{\\alpha^2\\sigma^2}{n} + (\\alpha - 1)^2\\theta^2$.\n\nNow, we determine the values of $\\theta$ for which $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{2}) < \\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{1})$:\n$$\n\\frac{\\alpha^2\\sigma^2}{n} + (\\alpha - 1)^2\\theta^2 < \\frac{\\sigma^2}{n}\n$$\nWe solve this inequality for $\\theta^2$:\n$$\n(\\alpha - 1)^2\\theta^2 < \\frac{\\sigma^2}{n} - \\frac{\\alpha^2\\sigma^2}{n}\n$$\n$$\n(\\alpha - 1)^2\\theta^2 < \\frac{\\sigma^2}{n}(1 - \\alpha^2)\n$$\nSince $0 < \\alpha < 1$, the term $(\\alpha-1)^2 = (1-\\alpha)^2$ is positive. Also, $1 - \\alpha^2 = (1-\\alpha)(1+\\alpha)$ is positive. We can divide by $(\\alpha-1)^2$ without changing the inequality's direction.\n$$\n\\theta^2 < \\frac{\\sigma^2(1 - \\alpha^2)}{n(\\alpha - 1)^2} = \\frac{\\sigma^2(1 - \\alpha)(1 + \\alpha)}{n(1 - \\alpha)^2}\n$$\n$$\n\\theta^2 < \\frac{\\sigma^2(1 + \\alpha)}{n(1 - \\alpha)}\n$$\nThis inequality is satisfied if and only if the absolute value of $\\theta$ is less than the square root of the right-hand side.\n$$\n|\\theta| < \\sqrt{\\frac{\\sigma^2(1 + \\alpha)}{n(1 - \\alpha)}}\n$$\nThis shows that the shrinkage estimator $\\hat{\\theta}_2$ has a lower MSE than the sample mean $\\hat{\\theta}_1$ if and only if the true parameter $\\theta$ is sufficiently close to $0$.\n\nPart 3: The Critical Threshold $T(n,\\sigma^{2},\\alpha)$\n\nThe result from Part $2$ establishes that the inequality $\\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{2}) < \\mathrm{MSE}_{\\theta}(\\hat{\\theta}_{1})$ holds if and only if $|\\theta| < T(n,\\sigma^{2},\\alpha)$. By comparing this form with our derived inequality, we can directly identify the positive critical threshold $T(n,\\sigma^{2},\\alpha)$.\nFrom $|\\theta| < \\sqrt{\\frac{\\sigma^2(1 + \\alpha)}{n(1 - \\alpha)}}$, we have:\n$$\nT(n,\\sigma^{2},\\alpha) = \\sqrt{\\frac{\\sigma^2(1 + \\alpha)}{n(1 - \\alpha)}}\n$$\nThis provides the exact closed-form expression for the threshold as requested.", "answer": "$$\n\\boxed{\\sqrt{\\frac{\\sigma^{2}(1 + \\alpha)}{n(1 - \\alpha)}}}\n$$", "id": "4926200"}, {"introduction": "Building on the bias-variance tradeoff, we now turn our attention from estimating a location (the mean) to estimating spread (the variance). You are likely familiar with the sample variance formula that uses a denominator of $n-1$, which provides an unbiased estimate of the population variance. But what if we used the more intuitive denominator of $n$? This practice asks you to derive the MSE for both of these estimators. This comparison will provide a powerful and classic illustration of how the unbiased estimator is not necessarily the one with the minimum MSE, reinforcing the idea that overall performance is a more nuanced concept than just the absence of bias. [@problem_id:4926139]", "problem": "A biostatistics team is analyzing variability of a continuous biomarker measured in $n$ independent patients. Let $X_1,\\dots,X_n$ denote the measurements, modeled as independent and identically distributed draws from a normal distribution $X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$ with unknown mean $\\mu$ and variance $\\sigma^2$. The team considers two variance estimators based on the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i$:\n- $S_n^2 = \\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\bar{X})^2$,\n- $S_{n-1}^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\bar{X})^2$.\n\nUsing only fundamental definitions and standard distributional results, derive closed-form expressions for the mean squared error (MSE) with respect to $\\sigma^2$ of each estimator,\n$$\\mathrm{MSE}_{\\sigma^2}(T)=\\mathbb{E}\\big[(T-\\sigma^2)^2\\big],$$\nfor $T \\in \\{S_n^2, S_{n-1}^2\\}$, as functions of $n$ and $\\sigma^2$, valid for $n \\ge 2$. Then decide which estimator has the smaller $\\mathrm{MSE}_{\\sigma^2}$ for a given fixed $n$.\n\nProvide your final answer as a single row matrix containing the two analytical expressions in the order $\\big(\\mathrm{MSE}_{\\sigma^2}(S_n^2),\\,\\mathrm{MSE}_{\\sigma^2}(S_{n-1}^2)\\big)$. No rounding is required.", "solution": "The objective is to derive and compare the mean squared error (MSE) for two estimators of the variance $\\sigma^2$ of a normal distribution. The estimators are $S_n^2 = \\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\bar{X})^2$ and $S_{n-1}^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\bar{X})^2$, where $X_1, \\dots, X_n$ are independent and identically distributed (i.i.d.) random variables from $\\mathcal{N}(\\mu, \\sigma^2)$, and $n \\ge 2$.\n\nThe mean squared error of an estimator $T$ for a parameter $\\theta$ is defined as $\\mathrm{MSE}_{\\theta}(T) = \\mathbb{E}[(T-\\theta)^2]$. A fundamental decomposition of MSE is into variance and squared bias:\n$$ \\mathrm{MSE}_{\\theta}(T) = \\mathrm{Var}(T) + (\\mathrm{Bias}_{\\theta}(T))^2 $$\nwhere the bias is $\\mathrm{Bias}_{\\theta}(T) = \\mathbb{E}[T] - \\theta$. In this problem, the parameter being estimated is $\\theta = \\sigma^2$.\n\nA cornerstone result in sampling theory for normal distributions (a consequence of Cochran's theorem) is that the quantity $\\sum_{i=1}^{n}(X_i - \\bar{X})^2$ scaled by the true variance $\\sigma^2$ follows a chi-squared distribution with $n-1$ degrees of freedom. Let $Q = \\sum_{i=1}^{n}(X_i - \\bar{X})^2$. Then the distributional fact is:\n$$ \\frac{Q}{\\sigma^2} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}{\\sigma^2} \\sim \\chi_{n-1}^2 $$\nLet $Y$ be a random variable such that $Y \\sim \\chi_{n-1}^2$. The expected value and variance of $Y$ are known to be:\n$$ \\mathbb{E}[Y] = n-1 $$\n$$ \\mathrm{Var}(Y) = 2(n-1) $$\nThe two estimators can be expressed in terms of $Q$. $S_{n-1}^2 = \\frac{Q}{n-1}$ and $S_n^2 = \\frac{Q}{n}$. We can rewrite them in terms of $Y$:\n$$ S_{n-1}^2 = \\frac{\\sigma^2}{n-1} Y $$\n$$ S_n^2 = \\frac{\\sigma^2}{n} Y $$\nNow we can compute the bias, variance, and MSE for each estimator.\n\nFirst, consider the estimator $S_{n-1}^2$:\nThe expected value is:\n$$ \\mathbb{E}[S_{n-1}^2] = \\mathbb{E}\\left[\\frac{\\sigma^2}{n-1} Y\\right] = \\frac{\\sigma^2}{n-1} \\mathbb{E}[Y] = \\frac{\\sigma^2}{n-1} (n-1) = \\sigma^2 $$\nThe bias of $S_{n-1}^2$ is:\n$$ \\mathrm{Bias}_{\\sigma^2}(S_{n-1}^2) = \\mathbb{E}[S_{n-1}^2] - \\sigma^2 = \\sigma^2 - \\sigma^2 = 0 $$\nAs its bias is zero, $S_{n-1}^2$ is an unbiased estimator of $\\sigma^2$.\nThe variance of $S_{n-1}^2$ is:\n$$ \\mathrm{Var}(S_{n-1}^2) = \\mathrm{Var}\\left(\\frac{\\sigma^2}{n-1} Y\\right) = \\left(\\frac{\\sigma^2}{n-1}\\right)^2 \\mathrm{Var}(Y) = \\frac{\\sigma^4}{(n-1)^2} \\cdot 2(n-1) = \\frac{2\\sigma^4}{n-1} $$\nThe MSE of $S_{n-1}^2$ is therefore:\n$$ \\mathrm{MSE}_{\\sigma^2}(S_{n-1}^2) = \\mathrm{Var}(S_{n-1}^2) + (\\mathrm{Bias}_{\\sigma^2}(S_{n-1}^2))^2 = \\frac{2\\sigma^4}{n-1} + 0^2 = \\frac{2\\sigma^4}{n-1} $$\n\nNext, consider the estimator $S_n^2$:\nThe expected value is:\n$$ \\mathbb{E}[S_n^2] = \\mathbb{E}\\left[\\frac{\\sigma^2}{n} Y\\right] = \\frac{\\sigma^2}{n} \\mathbb{E}[Y] = \\frac{\\sigma^2}{n} (n-1) = \\frac{n-1}{n}\\sigma^2 $$\nThe bias of $S_n^2$ is:\n$$ \\mathrm{Bias}_{\\sigma^2}(S_n^2) = \\mathbb{E}[S_n^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{\\sigma^2}{n} $$\nAs its bias is non-zero, $S_n^2$ is a biased estimator of $\\sigma^2$.\nThe variance of $S_n^2$ is:\n$$ \\mathrm{Var}(S_n^2) = \\mathrm{Var}\\left(\\frac{\\sigma^2}{n} Y\\right) = \\left(\\frac{\\sigma^2}{n}\\right)^2 \\mathrm{Var}(Y) = \\frac{\\sigma^4}{n^2} \\cdot 2(n-1) = \\frac{2(n-1)\\sigma^4}{n^2} $$\nThe MSE of $S_n^2$ is:\n$$ \\mathrm{MSE}_{\\sigma^2}(S_n^2) = \\mathrm{Var}(S_n^2) + (\\mathrm{Bias}_{\\sigma^2}(S_n^2))^2 = \\frac{2(n-1)\\sigma^4}{n^2} + \\left(-\\frac{\\sigma^2}{n}\\right)^2 $$\n$$ \\mathrm{MSE}_{\\sigma^2}(S_n^2) = \\frac{2(n-1)\\sigma^4}{n^2} + \\frac{\\sigma^4}{n^2} = \\frac{(2n - 2 + 1)\\sigma^4}{n^2} = \\frac{(2n-1)\\sigma^4}{n^2} $$\n\nThe derived expressions for the mean squared errors are:\n$$ \\mathrm{MSE}_{\\sigma^2}(S_n^2) = \\frac{(2n-1)\\sigma^4}{n^2} $$\n$$ \\mathrm{MSE}_{\\sigma^2}(S_{n-1}^2) = \\frac{2\\sigma^4}{n-1} $$\n\nTo determine which estimator has the smaller MSE for a fixed $n \\ge 2$, we compare these two quantities. We can examine the sign of their difference:\n$$ \\mathrm{MSE}_{\\sigma^2}(S_n^2) - \\mathrm{MSE}_{\\sigma^2}(S_{n-1}^2) = \\frac{(2n-1)\\sigma^4}{n^2} - \\frac{2\\sigma^4}{n-1} $$\nFactoring out $\\sigma^4$ and finding a common denominator:\n$$ = \\sigma^4 \\left( \\frac{2n-1}{n^2} - \\frac{2}{n-1} \\right) = \\sigma^4 \\left( \\frac{(2n-1)(n-1) - 2n^2}{n^2(n-1)} \\right) $$\n$$ = \\sigma^4 \\left( \\frac{(2n^2 - 2n - n + 1) - 2n^2}{n^2(n-1)} \\right) = \\sigma^4 \\left( \\frac{2n^2 - 3n + 1 - 2n^2}{n^2(n-1)} \\right) $$\n$$ = \\sigma^4 \\left( \\frac{1 - 3n}{n^2(n-1)} \\right) $$\nFor $n \\ge 2$, the denominator $n^2(n-1)$ is strictly positive. The numerator is $1 - 3n$. Since $n \\ge 2$, we have $3n \\ge 6$, which implies $1 - 3n \\le 1 - 6 = -5$. Thus, the numerator $1 - 3n$ is strictly negative.\nSince the numerator is negative and the denominator is positive, the entire fraction is negative. As $\\sigma^4 > 0$ (for a non-degenerate distribution), the difference is strictly negative:\n$$ \\mathrm{MSE}_{\\sigma^2}(S_n^2) - \\mathrm{MSE}_{\\sigma^2}(S_{n-1}^2) < 0 $$\nThis implies that:\n$$ \\mathrm{MSE}_{\\sigma^2}(S_n^2) < \\mathrm{MSE}_{\\sigma^2}(S_{n-1}^2) $$\nTherefore, the estimator $S_n^2$, despite being biased, has a smaller mean squared error than the unbiased estimator $S_{n-1}^2$ for all sample sizes $n \\ge 2$. This illustrates the classic bias-variance tradeoff in statistics.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{(2n-1)\\sigma^4}{n^2} & \\frac{2\\sigma^4}{n-1} \\end{pmatrix}}$$", "id": "4926139"}, {"introduction": "Let's apply our understanding of MSE to a practical scenario common in biostatistics: estimating a proportion, such as the prevalence of a disease or the success rate of a treatment. When sample sizes are small or the true proportion is near 0 or 1, the standard sample proportion can be highly variable and yield extreme estimates. This exercise introduces the Laplace-smoothed estimator, a technique that adds \"pseudo-counts\" to stabilize the estimate. Your task is to derive the MSE of this estimator, which will reveal analytically how the smoothing parameter creates a tradeoff between introducing a small amount of bias and substantially reducing variance, leading to better overall performance in many situations. [@problem_id:4926183]", "problem": "A biostatistician is estimating the proportion of individuals with a certain biomarker in a cohort of $n$ independent participants, each tested once. Let $X$ denote the number of biomarker-positive individuals in the cohort, modeled as $X \\sim \\operatorname{Bin}(n,p)$ with $0 < p < 1$. To reduce the risk of extreme estimates when $X$ is near $0$ or $n$, the biostatistician uses Laplace smoothing with a symmetric prior quantified by a pseudo-count parameter $\\alpha > 0$, yielding the estimator\n$$\n\\hat{p}_{\\alpha} \\;=\\; \\frac{X + \\alpha}{n + 2\\alpha}.\n$$\nUsing the definition of Mean Squared Error (MSE) as the expected squared deviation from the true parameter under the data-generating model, derive a closed-form expression for $\\operatorname{MSE}_{p}(\\hat{p}_{\\alpha})$ as a function of $\\alpha$, $n$, and $p$. Assume $n$ is a positive integer. Express your final answer as a single analytical expression. No rounding is required.", "solution": "The objective is to derive a closed-form expression for the Mean Squared Error (MSE) of the estimator $\\hat{p}_{\\alpha}$ for a binomial proportion $p$. The estimator is given by\n$$\n\\hat{p}_{\\alpha} = \\frac{X + \\alpha}{n + 2\\alpha}\n$$\nwhere $X$ is a random variable following a binomial distribution, $X \\sim \\operatorname{Bin}(n,p)$, and $\\alpha > 0$ is a pseudo-count parameter.\n\nThe Mean Squared Error of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as the expected value of the squared error, $\\operatorname{MSE}_{\\theta}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$. A fundamental identity in statistical theory allows for the decomposition of the MSE into two components: the variance of the estimator and the square of its bias.\n$$\n\\operatorname{MSE}_{p}(\\hat{p}_{\\alpha}) = \\operatorname{Var}_{p}(\\hat{p}_{\\alpha}) + (\\operatorname{Bias}_{p}(\\hat{p}_{\\alpha}))^2\n$$\nHere, the bias is defined as $\\operatorname{Bias}_{p}(\\hat{p}_{\\alpha}) = E[\\hat{p}_{\\alpha}] - p$. We will proceed by calculating the bias and the variance separately and then combining them.\n\nFirst, we calculate the bias of the estimator $\\hat{p}_{\\alpha}$. To do this, we need its expected value, $E[\\hat{p}_{\\alpha}]$.\nThe estimator $\\hat{p}_{\\alpha}$ is a linear transformation of the random variable $X$. Using the linearity of the expectation operator, we have:\n$$\nE[\\hat{p}_{\\alpha}] = E\\left[\\frac{X + \\alpha}{n + 2\\alpha}\\right] = \\frac{1}{n + 2\\alpha} E[X + \\alpha] = \\frac{E[X] + \\alpha}{n + 2\\alpha}\n$$\nSince $X \\sim \\operatorname{Bin}(n,p)$, its expected value is $E[X] = np$. Substituting this into the expression for $E[\\hat{p}_{\\alpha}]$ gives:\n$$\nE[\\hat{p}_{\\alpha}] = \\frac{np + \\alpha}{n + 2\\alpha}\n$$\nNow, we can compute the bias:\n$$\n\\operatorname{Bias}_{p}(\\hat{p}_{\\alpha}) = E[\\hat{p}_{\\alpha}] - p = \\frac{np + \\alpha}{n + 2\\alpha} - p\n$$\nTo simplify, we find a common denominator:\n$$\n\\operatorname{Bias}_{p}(\\hat{p}_{\\alpha}) = \\frac{np + \\alpha - p(n + 2\\alpha)}{n + 2\\alpha} = \\frac{np + \\alpha - np - 2\\alpha p}{n + 2\\alpha} = \\frac{\\alpha - 2\\alpha p}{n + 2\\alpha}\n$$\nFactoring out $\\alpha$ from the numerator, we obtain the final expression for the bias:\n$$\n\\operatorname{Bias}_{p}(\\hat{p}_{\\alpha}) = \\frac{\\alpha(1 - 2p)}{n + 2\\alpha}\n$$\nThe squared bias is therefore:\n$$\n(\\operatorname{Bias}_{p}(\\hat{p}_{\\alpha}))^2 = \\left(\\frac{\\alpha(1 - 2p)}{n + 2\\alpha}\\right)^2 = \\frac{\\alpha^2 (1 - 2p)^2}{(n + 2\\alpha)^2}\n$$\n\nNext, we calculate the variance of the estimator, $\\operatorname{Var}_{p}(\\hat{p}_{\\alpha})$.\nUsing the properties of variance, specifically $\\operatorname{Var}(aY + b) = a^2 \\operatorname{Var}(Y)$ for constants $a$ and $b$, we have:\n$$\n\\operatorname{Var}_{p}(\\hat{p}_{\\alpha}) = \\operatorname{Var}\\left(\\frac{X + \\alpha}{n + 2\\alpha}\\right) = \\operatorname{Var}\\left(\\frac{1}{n + 2\\alpha}X + \\frac{\\alpha}{n + 2\\alpha}\\right) = \\left(\\frac{1}{n + 2\\alpha}\\right)^2 \\operatorname{Var}(X)\n$$\nSince $X \\sim \\operatorname{Bin}(n,p)$, its variance is $\\operatorname{Var}(X) = np(1-p)$. Substituting this into the equation gives:\n$$\n\\operatorname{Var}_{p}(\\hat{p}_{\\alpha}) = \\frac{1}{(n + 2\\alpha)^2} [np(1-p)] = \\frac{np(1-p)}{(n + 2\\alpha)^2}\n$$\n\nFinally, we combine the squared bias and the variance to obtain the Mean Squared Error:\n$$\n\\operatorname{MSE}_{p}(\\hat{p}_{\\alpha}) = \\operatorname{Var}_{p}(\\hat{p}_{\\alpha}) + (\\operatorname{Bias}_{p}(\\hat{p}_{\\alpha}))^2\n$$\n$$\n\\operatorname{MSE}_{p}(\\hat{p}_{\\alpha}) = \\frac{np(1-p)}{(n + 2\\alpha)^2} + \\frac{\\alpha^2 (1 - 2p)^2}{(n + 2\\alpha)^2}\n$$\nSince both terms have the same denominator, we can combine them into a single fraction:\n$$\n\\operatorname{MSE}_{p}(\\hat{p}_{\\alpha}) = \\frac{np(1-p) + \\alpha^2 (1 - 2p)^2}{(n + 2\\alpha)^2}\n$$\nThis is the closed-form expression for the Mean Squared Error of the estimator $\\hat{p}_{\\alpha}$ as a function of the sample size $n$, the true proportion $p$, and the pseudo-count parameter $\\alpha$.", "answer": "$$\n\\boxed{\\frac{np(1-p) + \\alpha^2 (1 - 2p)^2}{(n + 2\\alpha)^2}}\n$$", "id": "4926183"}]}