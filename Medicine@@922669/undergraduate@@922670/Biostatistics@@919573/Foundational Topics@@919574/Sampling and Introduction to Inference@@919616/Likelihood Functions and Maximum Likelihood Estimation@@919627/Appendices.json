{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will start with the simplest non-trivial case: estimating the probability of a binary outcome. This exercise forms the bedrock of maximum likelihood estimation by guiding you through the process of writing a likelihood function, taking its logarithm for simplification, and finding the parameter value that maximizes it. By working through this fundamental problem, you will build intuition for why the sample proportion is the natural and mathematically justified estimator for the probability in a series of Bernoulli trials [@problem_id:4969170].", "problem": "In a multisite observational study of a binary clinical outcome, each patient either experiences a specific adverse event during a fixed follow-up period or does not. Let $X_1, \\dots, X_n$ denote independent and identically distributed (i.i.d.) Bernoulli random variables with unknown event probability $p$, where $X_i=1$ indicates the adverse event occurred for patient $i$ and $X_i=0$ otherwise. The parameter space for $p$ is the closed interval $[0,1]$, which is the natural space for a probability, but you should also analyze what happens if the parameter space is taken to be the open interval $(0,1)$.\n\nUsing only the definition of the Bernoulli probability mass function and the definition of the likelihood for i.i.d. data, derive from first principles the Maximum Likelihood Estimator (MLE) $\\hat{p}$ for $p$. Then, carefully analyze the existence and uniqueness of the MLE in the non-regular edge cases where all observations are $0$ (that is, $X_i=0$ for all $i$) or all observations are $1$ (that is, $X_i=1$ for all $i$), under both choices of parameter space $(0,1)$ and $[0,1]$. Your analysis should justify whether a maximizer exists in the specified parameter space and whether it is unique in each edge case by appealing to appropriate properties of the likelihood or log-likelihood function.\n\nProvide the final answer as a single closed-form expression for the MLE $\\hat{p}$. No numerical rounding is required.", "solution": "The problem requires the derivation of the Maximum Likelihood Estimator (MLE) for the parameter $p$ of a Bernoulli distribution and a careful analysis of the estimator's existence and uniqueness in edge cases under two different parameter spaces, $[0, 1]$ and $(0, 1)$.\n\nLet $X_{1}, \\dots, X_{n}$ be independent and identically distributed (i.i.d.) random variables from a Bernoulli distribution with parameter $p$, where $p$ is the probability of an event (denoted by $X_i=1$). The probability mass function (PMF) for a single observation $X_i$ is given by:\n$$ P(X_i = x_i; p) = p^{x_i} (1-p)^{1-x_i} \\quad \\text{for } x_i \\in \\{0, 1\\} $$\nThe parameter $p$ represents a probability, so its natural parameter space is the closed interval $[0, 1]$.\n\nFirst, we construct the likelihood function, $L(p)$. Since the observations are i.i.d., the likelihood is the product of the individual PMFs:\n$$ L(p; x_1, \\dots, x_n) = \\prod_{i=1}^{n} P(X_i = x_i; p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i} $$\nThis can be simplified by combining the exponents:\n$$ L(p) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^{\\sum x_i} (1-p)^{n - \\sum x_i} $$\nLet $S = \\sum_{i=1}^{n} X_i$ be the total number of observed events. $S$ is a sufficient statistic for $p$. The likelihood function can be written as:\n$$ L(p) = p^{S} (1-p)^{n-S} $$\nTo find the value of $p$ that maximizes $L(p)$, it is computationally more convenient to maximize the natural logarithm of the likelihood function, the log-likelihood, denoted by $\\ell(p)$. Since the natural logarithm is a strictly increasing function, the value of $p$ that maximizes $\\ell(p)$ also maximizes $L(p)$.\n$$ \\ell(p) = \\ln(L(p)) = \\ln(p^{S} (1-p)^{n-S}) = S \\ln(p) + (n-S) \\ln(1-p) $$\nThe log-likelihood is defined for $p \\in (0, 1)$. We will first find the MLE for the case where a solution exists in this open interval and then analyze the boundary points $p=0$ and $p=1$.\n\nWe find the maximum by differentiating $\\ell(p)$ with respect to $p$ and setting the derivative to zero.\n$$ \\frac{d\\ell}{dp} = \\frac{S}{p} - \\frac{n-S}{1-p} $$\nSetting the derivative to $0$:\n$$ \\frac{S}{p} = \\frac{n-S}{1-p} $$\n$$ S(1-p) = p(n-S) $$\n$$ S - Sp = np - Sp $$\n$$ S = np $$\n$$ p = \\frac{S}{n} $$\nThis critical point, $\\hat{p} = \\frac{S}{n}$, is the sample mean $\\bar{X}$. To verify that this is a maximum, we examine the second derivative:\n$$ \\frac{d^2\\ell}{dp^2} = -\\frac{S}{p^2} - \\frac{n-S}{(1-p)^2} $$\nFor the non-edge cases, we have $0 < S < n$. This means $S > 0$ and $n-S > 0$. Since $p^2 > 0$ and $(1-p)^2 > 0$, both terms in the second derivative are negative. Thus, $\\frac{d^2\\ell}{dp^2} < 0$ for all $p \\in (0, 1)$, which confirms that the log-likelihood function is strictly concave. Therefore, the critical point $\\hat{p} = \\frac{S}{n}$ is the unique maximum. In this case, since $0 < S < n$, we have $0 < \\hat{p} < 1$, so the MLE exists and is unique in both parameter spaces, $(0, 1)$ and $[0, 1]$.\n\nNow, we must analyze the edge cases as required.\nCase 1: All observations are $0$.\nThis corresponds to $S = \\sum_{i=1}^{n} X_i = 0$. The likelihood function becomes:\n$$ L(p) = p^0 (1-p)^{n-0} = (1-p)^n $$\nThe log-likelihood for $p \\in (0, 1)$ is $\\ell(p) = n \\ln(1-p)$.\nAnalysis on parameter space $[0, 1]$:\nThe function $L(p) = (1-p)^n$ is a strictly decreasing function of $p$ on the interval $[0, 1]$. A strictly decreasing function on a closed interval attains its unique maximum at the left-most point of the interval. Thus, the likelihood is maximized at $p=0$. The MLE is $\\hat{p} = 0$. This value is in the parameter space $[0, 1]$. The maximum value of the likelihood is $L(0) = (1-0)^n = 1$. So, for the space $[0, 1]$, a unique MLE exists and is $\\hat{p}=0$. This is consistent with the general formula $\\hat{p} = S/n = 0/n = 0$.\n\nAnalysis on parameter space $(0, 1)$:\nOn the open interval $(0, 1)$, the function $L(p) = (1-p)^n$ is still strictly decreasing. The supremum of the function is $\\sup_{p \\in (0, 1)} (1-p)^n = 1$, which is approached as $p \\to 0^+$. However, this supremum is never attained for any $p$ within the open interval $(0, 1)$. For any proposed maximizer $p^* \\in (0, 1)$, we can always find a value $p^{**} = p^*/2$ such that $0 < p^{**} < p^*$ and $L(p^{**}) = (1-p^*/2)^n > (1-p^*)^n = L(p^*)$. Therefore, no maximum exists within the parameter space $(0, 1)$. The MLE does not exist.\n\nCase 2: All observations are $1$.\nThis corresponds to $S = \\sum_{i=1}^{n} X_i = n$. The likelihood function becomes:\n$$ L(p) = p^n (1-p)^{n-n} = p^n $$\nThe log-likelihood for $p \\in (0, 1)$ is $\\ell(p) = n \\ln(p)$.\nAnalysis on parameter space $[0, 1]$:\nThe function $L(p) = p^n$ is a strictly increasing function of $p$ on the interval $[0, 1]$. A strictly increasing function on a closed interval attains its unique maximum at the right-most point of the interval. Thus, the likelihood is maximized at $p=1$. The MLE is $\\hat{p} = 1$. This value is in the parameter space $[0, 1]$. The maximum value of the likelihood is $L(1) = 1^n = 1$. So, for the space $[0, 1]$, a unique MLE exists and is $\\hat{p}=1$. This is consistent with the general formula $\\hat{p} = S/n = n/n = 1$.\n\nAnalysis on parameter space $(0, 1)$:\nOn the open interval $(0, 1)$, the function $L(p) = p^n$ is still strictly increasing. The supremum of the function is $\\sup_{p \\in (0, 1)} p^n = 1$, which is approached as $p \\to 1^-$. However, this supremum is never attained for any $p$ within the open interval $(0, 1)$. For any proposed maximizer $p^* \\in (0, 1)$, we can always find a value $p^{**} = (p^*+1)/2$ such that $p^* < p^{**} < 1$ and $L(p^{**}) = ((p^*+1)/2)^n > (p^*)^n = L(p^*)$. Therefore, no maximum exists within the parameter space $(0, 1)$. The MLE does not exist.\n\nIn summary, the MLE for $p$ is well-defined, exists, and is unique for all possible observation sets $\\{X_1, \\dots, X_n\\}$ if and only if the parameter space is the closed interval $[0, 1]$. This is the natural and standard choice for the Bernoulli parameter. The general expression $\\hat{p} = \\frac{\\sum_{i=1}^{n} X_i}{n}$ covers all three scenarios (interior, all $0$s, all $1$s) under this choice of parameter space.", "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{n} X_{i}}{n}}\n$$", "id": "4969170"}, {"introduction": "Moving from discrete outcomes to continuous measurements, we now tackle the Normal distribution, a cornerstone of statistical modeling in biomedical sciences. This practice extends the MLE framework to a two-parameter problem, requiring the use of partial derivatives to simultaneously estimate the mean ($\\mu$) and variance ($\\sigma^2$). Furthermore, this exercise introduces a critical concept in statistical inference: the bias of an estimator, revealing that while MLEs are powerful, they are not always unbiased [@problem_id:4969243].", "problem": "A biomedical laboratory calibrates a new assay that reports a continuous biomarker value for each patient sample. Suppose the measurement errors across patients are modeled as independent and identically distributed (i.i.d.) draws from a Normal distribution $\\mathcal{N}(\\mu,\\sigma^{2})$ with unknown mean bias $\\mu$ and unknown variance $\\sigma^{2}$. You observe a single calibration run with values $x_{1},\\dots,x_{n}$, where $n \\geq 2$. Starting from the definition of the likelihood for i.i.d. Normal observations and basic properties of expectation and variance, derive the maximum likelihood estimators (MLE) $\\hat{\\mu}$ and $\\hat{\\sigma}^{2}$, expressing each in terms of $x_{1},\\dots,x_{n}$. Then, relative to the usual unbiased variance estimator $s^{2}_{\\text{unb}}=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$, determine the exact bias of the MLE $\\hat{\\sigma}^{2}$, defined as $\\operatorname{Bias}(\\hat{\\sigma}^{2})=\\mathbb{E}[\\hat{\\sigma}^{2}]-\\sigma^{2}$. Provide your final answer for the bias as a single closed-form expression in terms of $n$ and $\\sigma^{2}$. No numerical rounding is required. Here $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$ denotes the sample mean.", "solution": "The problem is a standard exercise in mathematical statistics, grounded in the principles of maximum likelihood estimation for the parameters of a Normal distribution. It is well-posed, objective, and contains all necessary information for a unique solution.\n\nThe solution proceeds in two main parts: first, the derivation of the maximum likelihood estimators (MLE) for the mean $\\mu$ and variance $\\sigma^2$ of a Normal distribution; second, the calculation of the bias of the MLE for the variance, $\\hat{\\sigma}^2$.\n\nLet $x_1, \\dots, x_n$ be an independent and identically distributed (i.i.d.) sample from a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (PDF) for a single observation $x_i$ is given by:\n$$f(x_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)$$\n\nThe likelihood function, $L(\\mu, \\sigma^2 | \\mathbf{x})$, is the product of the individual PDFs for the $n$ observations, due to their independence:\n$$L(\\mu, \\sigma^2 | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)$$\n$$L(\\mu, \\sigma^2 | \\mathbf{x}) = \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right)$$\n\nTo simplify the process of maximization, we work with the natural logarithm of the likelihood function, known as the log-likelihood, $\\mathcal{L}(\\mu, \\sigma^2 | \\mathbf{x})$:\n$$\\mathcal{L} = \\ln[L(\\mu, \\sigma^2 | \\mathbf{x})] = \\ln\\left[ \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right) \\right]$$\n$$\\mathcal{L} = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n$$\\mathcal{L} = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n\nTo find the MLEs, we take the partial derivatives of $\\mathcal{L}$ with respect to $\\mu$ and $\\sigma^2$ and set them to zero.\n\nFirst, for the mean $\\mu$:\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left[ -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right]$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(x_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)$$\nSetting the derivative to zero and solving for $\\hat{\\mu}$:\n$$\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} \\hat{\\mu} = 0$$\n$$\\sum_{i=1}^{n} x_i - n\\hat{\\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}$$\nThe MLE for $\\mu$ is the sample mean, $\\hat{\\mu} = \\bar{x}$.\n\nNext, for the variance $\\sigma^2$. It is convenient to differentiate with respect to $\\sigma^2$ directly.\n$$\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left[ -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right]$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} - \\left(-\\frac{1}{2(\\sigma^2)^2}\\right) \\sum_{i=1}^{n} (x_i - \\mu)^2 = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\nSetting the derivative to zero and substituting the estimators $\\hat{\\mu}$ and $\\hat{\\sigma}^2$:\n$$-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0$$\nMultiplying by $2(\\hat{\\sigma}^2)^2$ yields:\n$$-n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2 = 0$$\nSubstituting $\\hat{\\mu} = \\bar{x}$, we solve for $\\hat{\\sigma}^2$:\n$$\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n\nNow, we determine the bias of this estimator. The bias is defined as $\\operatorname{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2$. We must compute the expectation of $\\hat{\\sigma}^2$.\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\frac{1}{n} \\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right]$$\nWe expand the sum of squares term:\n$$\\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\sum_{i=1}^{n} (x_i^2 - 2x_i\\bar{x} + \\bar{x}^2) = \\sum_{i=1}^{n} x_i^2 - 2\\bar{x}\\sum_{i=1}^{n} x_i + n\\bar{x}^2$$\nSince $\\sum_{i=1}^{n} x_i = n\\bar{x}$, this simplifies to:\n$$\\sum_{i=1}^{n} x_i^2 - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2 = \\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2$$\nNow we take the expectation of this expression:\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2\\right] = \\sum_{i=1}^{n} \\mathbb{E}[x_i^2] - n\\mathbb{E}[\\bar{x}^2]$$\nWe use the general property $\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) + (\\mathbb{E}[Y])^2$.\nFor each $x_i$, $\\mathbb{E}[x_i] = \\mu$ and $\\operatorname{Var}(x_i) = \\sigma^2$, so $\\mathbb{E}[x_i^2] = \\sigma^2 + \\mu^2$.\nFor the sample mean $\\bar{x}$, $\\mathbb{E}[\\bar{x}] = \\mathbb{E}[\\frac{1}{n}\\sum x_i] = \\mu$ and $\\operatorname{Var}(\\bar{x}) = \\operatorname{Var}(\\frac{1}{n}\\sum x_i) = \\frac{1}{n^2}\\sum\\operatorname{Var}(x_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$.\nTherefore, $\\mathbb{E}[\\bar{x}^2] = \\operatorname{Var}(\\bar{x}) + (\\mathbb{E}[\\bar{x}])^2 = \\frac{\\sigma^2}{n} + \\mu^2$.\nSubstituting these expectations back:\n$$\\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\sum_{i=1}^{n} (\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} + \\mu^2\\right)$$\n$$= n(\\sigma^2 + \\mu^2) - (\\sigma^2 + n\\mu^2) = n\\sigma^2 + n\\mu^2 - \\sigma^2 - n\\mu^2 = (n-1)\\sigma^2$$\nNow we find the expectation of $\\hat{\\sigma}^2$:\n$$\\mathbb{E}[\\hat{\\sigma}^2] = \\frac{1}{n} \\mathbb{E}\\left[\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\right] = \\frac{1}{n}(n-1)\\sigma^2$$\nFinally, we compute the bias:\n$$\\operatorname{Bias}(\\hat{\\sigma}^2) = \\mathbb{E}[\\hat{\\sigma}^2] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = \\left(\\frac{n-1}{n} - 1\\right)\\sigma^2 = \\left(\\frac{n-1-n}{n}\\right)\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\nThe bias of the MLE of the variance is $-\\frac{1}{n}\\sigma^2$. This shows that the MLE is a biased estimator, which tends to underestimate the true variance. The bias diminishes as the sample size $n$ increases.", "answer": "$$ \\boxed{-\\frac{1}{n}\\sigma^2} $$", "id": "4969243"}, {"introduction": "In many real-world applications, such as logistic regression, the equations that define the maximum likelihood estimates do not have a simple, closed-form solution. This challenge requires us to use numerical optimization methods to find the parameter values. This practice introduces the Newton-Raphson algorithm, a powerful iterative technique that uses the first and second derivatives of the log-likelihood (the score vector and the Hessian matrix) to find the MLE. By performing one step of this algorithm by hand, you will gain a concrete understanding of the computational engine that powers most modern statistical software [@problem_id:4969348].", "problem": "A clinical study investigates the probability of a postoperative complication as a function of a standardized biomarker. Let $y_i \\in \\{0,1\\}$ indicate whether patient $i$ experienced the complication and let $x_i \\in \\mathbb{R}$ denote the biomarker value. Suppose a parametric model with parameter vector $\\theta \\in \\mathbb{R}^{p}$ is fit by the principle of Maximum Likelihood Estimation (MLE), which selects $\\theta$ to maximize the log-likelihood $l(\\theta)$ of the observed data. Starting from the definitions of the score function $U(\\theta)$ as the gradient of $l(\\theta)$ and the Jacobian matrix $J(\\theta)$ of $U(\\theta)$, derive the Newton–Raphson update rule for iteratively solving the score equation $U(\\theta)=0$ to obtain the MLE.\n\nThen, specialize to binary logistic regression with a single predictor, where for each patient\n$$\np_i(\\beta_0,\\beta_1) \\equiv \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp\\!\\big(-\\eta_i\\big)}, \\quad \\text{with } \\eta_i = \\beta_0 + \\beta_1 x_i,\n$$\nand the log-likelihood of independent observations is\n$$\nl(\\beta_0,\\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right].\n$$\nUsing first principles and without introducing any shortcut formulas, derive the score vector $U(\\beta_0,\\beta_1)$ and the Jacobian $J(\\beta_0,\\beta_1)$ for this model.\n\nFinally, apply one Newton–Raphson iteration starting from the initial guess $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$ to the following dataset of $n=6$ patients:\n- biomarker values $x = (-2,\\, -1,\\, 0,\\, 1,\\, 2,\\, 0)$,\n- outcomes $y = (0,\\, 0,\\, 0,\\, 1,\\, 1,\\, 0)$.\n\nCompute the updated parameter vector $\\beta^{(1)}$ produced by one Newton–Raphson step. Express your final answer as a $1 \\times 2$ row matrix and round each entry to four significant figures.", "solution": "The problem is divided into three parts: first, a general derivation of the Newton–Raphson update rule for Maximum Likelihood Estimation (MLE); second, a specific derivation of the score vector and Jacobian matrix for a binary logistic regression model; and third, a numerical application of one Newton–Raphson step to a given dataset.\n\n### Part 1: Derivation of the Newton–Raphson Update Rule\n\nThe goal of Maximum Likelihood Estimation (MLE) is to find the parameter vector $\\theta$ that maximizes the log-likelihood function $l(\\theta)$. This maximum is found at a critical point where the gradient of the log-likelihood is zero. The score function, $U(\\theta)$, is defined as this gradient:\n$$\nU(\\theta) = \\nabla l(\\theta)\n$$\nThus, finding the MLE $\\hat\\theta$ is equivalent to solving the score equation $U(\\hat\\theta) = 0$.\n\nThe Newton–Raphson method is an iterative root-finding algorithm. To solve $U(\\theta) = 0$, we start with an initial guess $\\theta^{(k)}$ and aim to find a better approximation $\\theta^{(k+1)}$. We approximate $U(\\theta)$ by its first-order Taylor series expansion around $\\theta^{(k)}$:\n$$\nU(\\theta) \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta - \\theta^{(k)})\n$$\nwhere $J(\\theta^{(k)})$ is the Jacobian matrix of the score function $U(\\theta)$ evaluated at $\\theta^{(k)}$. The elements of the Jacobian are given by $J_{ij} = \\frac{\\partial U_i}{\\partial \\theta_j} = \\frac{\\partial^2 l}{\\partial \\theta_j \\partial \\theta_i}$. Note that this is the Hessian matrix of the log-likelihood function $l(\\theta)$.\n\nTo find the next iterate $\\theta^{(k+1)}$, we set $U(\\theta^{(k+1)}) = 0$ in the approximation:\n$$\n0 \\approx U(\\theta^{(k)}) + J(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)})\n$$\nRearranging this equation to solve for the update step $(\\theta^{(k+1)} - \\theta^{(k)})$, we get:\n$$\nJ(\\theta^{(k)}) (\\theta^{(k+1)} - \\theta^{(k)}) = -U(\\theta^{(k)})\n$$\nAssuming that the Jacobian matrix $J(\\theta^{(k)})$ is invertible, we can multiply by its inverse, $[J(\\theta^{(k)})]^{-1}$:\n$$\n\\theta^{(k+1)} - \\theta^{(k)} = -[J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\nThis leads to the Newton–Raphson update rule for iteratively finding the MLE:\n$$\n\\theta^{(k+1)} = \\theta^{(k)} - [J(\\theta^{(k)})]^{-1} U(\\theta^{(k)})\n$$\n\n### Part 2: Score and Jacobian for Logistic Regression\n\nWe now specialize to the binary logistic regression model with parameters $\\beta = (\\beta_0, \\beta_1)^{\\top}$. The log-likelihood for $n$ independent observations is given by:\n$$\nl(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} \\left[ y_i \\ln p_i + (1-y_i)\\ln(1-p_i) \\right]\n$$\nwhere $p_i = \\Pr(y_i=1 \\mid x_i) = \\frac{1}{1+\\exp(-\\eta_i)}$ and $\\eta_i = \\beta_0 + \\beta_1 x_i$.\n\nFirst, we find a crucial derivative of $p_i$ with respect to $\\eta_i$:\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left( (1+\\exp(-\\eta_i))^{-1} \\right) = -1 \\cdot (1+\\exp(-\\eta_i))^{-2} \\cdot (-\\exp(-\\eta_i)) = \\frac{\\exp(-\\eta_i)}{(1+\\exp(-\\eta_i))^2}\n$$\nThis can be rewritten as:\n$$\n\\frac{\\partial p_i}{\\partial \\eta_i} = \\frac{1}{1+\\exp(-\\eta_i)} \\cdot \\frac{\\exp(-\\eta_i)}{1+\\exp(-\\eta_i)} = p_i \\cdot \\left( \\frac{1+\\exp(-\\eta_i)-1}{1+\\exp(-\\eta_i)} \\right) = p_i \\cdot \\left( 1 - \\frac{1}{1+\\exp(-\\eta_i)} \\right) = p_i(1-p_i)\n$$\n\nThe score vector $U(\\beta_0, \\beta_1)$ has components $\\frac{\\partial l}{\\partial \\beta_0}$ and $\\frac{\\partial l}{\\partial \\beta_1}$. We use the chain rule:\n$$\n\\frac{\\partial l}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nThe derivative of the $i$-th log-likelihood term with respect to $p_i$ is:\n$$\n\\frac{\\partial l_i}{\\partial p_i} = \\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i} = \\frac{y_i(1-p_i) - (1-y_i)p_i}{p_i(1-p_i)} = \\frac{y_i - p_i}{p_i(1-p_i)}\n$$\nCombining these results:\n$$\n\\frac{\\partial l_i}{\\partial \\beta_j} = \\left( \\frac{y_i - p_i}{p_i(1-p_i)} \\right) \\cdot (p_i(1-p_i)) \\cdot \\frac{\\partial \\eta_i}{\\partial \\beta_j} = (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nThe derivatives of $\\eta_i$ are $\\frac{\\partial \\eta_i}{\\partial \\beta_0} = 1$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_1} = x_i$.\nThe components of the score vector are:\n$$\nU_0 = \\frac{\\partial l}{\\partial \\beta_0} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot 1 = \\sum_{i=1}^{n} (y_i - p_i)\n$$\n$$\nU_1 = \\frac{\\partial l}{\\partial \\beta_1} = \\sum_{i=1}^{n} (y_i - p_i) \\cdot x_i = \\sum_{i=1}^{n} x_i(y_i - p_i)\n$$\nSo the score vector is $U(\\beta_0, \\beta_1) = \\begin{pmatrix} \\sum_{i=1}^{n} (y_i - p_i) \\\\ \\sum_{i=1}^{n} x_i(y_i - p_i) \\end{pmatrix}$.\n\nNext, we derive the Jacobian matrix $J(\\beta_0, \\beta_1)$, whose elements are $J_{jk} = \\frac{\\partial U_j}{\\partial \\beta_k} = \\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j}$.\n$$\n\\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^{n} (y_i - p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\right) = \\sum_{i=1}^{n} \\left( -\\frac{\\partial p_i}{\\partial \\beta_k} \\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nUsing the chain rule again, $\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}$.\nSubstituting this gives the general form of the Jacobian elements:\n$$\nJ_{jk} = -\\sum_{i=1}^{n} p_i(1-p_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k} \\frac{\\partial \\eta_i}{\\partial \\beta_j}\n$$\nNow we compute the specific elements:\n$$\nJ_{00} = \\frac{\\partial^2 l}{\\partial \\beta_0^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (1)(1) = -\\sum_{i=1}^{n} p_i(1-p_i)\n$$\n$$\nJ_{01} = \\frac{\\partial^2 l}{\\partial \\beta_1 \\partial \\beta_0} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(1) = -\\sum_{i=1}^{n} x_i p_i(1-p_i)\n$$\nBy symmetry (Clairaut's theorem), $J_{10} = J_{01}$.\n$$\nJ_{11} = \\frac{\\partial^2 l}{\\partial \\beta_1^2} = -\\sum_{i=1}^{n} p_i(1-p_i) (x_i)(x_i) = -\\sum_{i=1}^{n} x_i^2 p_i(1-p_i)\n$$\nThus, the Jacobian matrix is $J(\\beta_0, \\beta_1) = \\begin{pmatrix} -\\sum p_i(1-p_i) & -\\sum x_i p_i(1-p_i) \\\\ -\\sum x_i p_i(1-p_i) & -\\sum x_i^2 p_i(1-p_i) \\end{pmatrix}$.\n\n### Part 3: Numerical Application\n\nWe apply one Newton–Raphson step starting from $\\beta^{(0)} = (\\beta_0^{(0)}, \\beta_1^{(0)})^{\\top} = (0,0)^{\\top}$.\nThe dataset is $x = (-2, -1, 0, 1, 2, 0)$ and $y = (0, 0, 0, 1, 1, 0)$ for $n=6$.\n\nFirst, evaluate probabilities at $\\beta^{(0)}$. For any $x_i$, $\\eta_i^{(0)} = \\beta_0^{(0)} + \\beta_1^{(0)} x_i = 0 + 0 \\cdot x_i = 0$.\nThe probability for each patient is $p_i^{(0)} = \\frac{1}{1+\\exp(-0)} = \\frac{1}{1+1} = 0.5$.\n\nNext, calculate the score vector $U(\\beta^{(0)})$:\nWe need the sums:\n$\\sum_{i=1}^6 y_i = 0+0+0+1+1+0 = 2$.\n$\\sum_{i=1}^6 x_i = -2-1+0+1+2+0 = 0$.\n$\\sum_{i=1}^6 x_i y_i = (-2)(0) + (-1)(0) + (0)(0) + (1)(1) + (2)(1) + (0)(0) = 3$.\nThe components of the score vector are:\n$U_0^{(0)} = \\sum_{i=1}^6 (y_i - p_i^{(0)}) = \\sum y_i - \\sum p_i^{(0)} = 2 - 6 \\times 0.5 = 2 - 3 = -1$.\n$U_1^{(0)} = \\sum_{i=1}^6 x_i (y_i - p_i^{(0)}) = \\sum x_i y_i - p_i^{(0)} \\sum x_i = 3 - 0.5 \\times 0 = 3$.\nSo, $U(\\beta^{(0)}) = \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$.\n\n*Next, calculate the Jacobian matrix $J(\\beta^{(0)})$:*\nFor all $i$, $p_i^{(0)}(1-p_i^{(0)}) = 0.5 \\times (1-0.5) = 0.25$.\nWe need the sum of squares of $x_i$:\n$\\sum_{i=1}^6 x_i^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 0^2 = 4+1+0+1+4+0 = 10$.\nThe elements of the Jacobian are:\n$J_{00}^{(0)} = -\\sum p_i^{(0)}(1-p_i^{(0)}) = -6 \\times 0.25 = -1.5$.\n$J_{01}^{(0)} = -\\sum x_i p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i = -0.25 \\times 0 = 0$.\n$J_{11}^{(0)} = -\\sum x_i^2 p_i^{(0)}(1-p_i^{(0)}) = -0.25 \\sum x_i^2 = -0.25 \\times 10 = -2.5$.\nSo, $J(\\beta^{(0)}) = \\begin{pmatrix} -1.5 & 0 \\\\ 0 & -2.5 \\end{pmatrix}$.\n\nFinally, perform the update to find $\\beta^{(1)} = (\\beta_0^{(1)}, \\beta_1^{(1)})^{\\top}$:\n$$\n\\beta^{(1)} = \\beta^{(0)} - [J(\\beta^{(0)})]^{-1} U(\\beta^{(0)})\n$$\nThe inverse of the diagonal Jacobian matrix is:\n$$\n[J(\\beta^{(0)})]^{-1} = \\begin{pmatrix} 1/(-1.5) & 0 \\\\ 0 & 1/(-2.5) \\end{pmatrix} = \\begin{pmatrix} -2/3 & 0 \\\\ 0 & -2/5 \\end{pmatrix}\n$$\nNow, we compute $\\beta^{(1)}$:\n$$\n\\beta^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -2/3 & 0 \\\\ 0 & -2/5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = -\\begin{pmatrix} (-2/3)(-1) + (0)(3) \\\\ (0)(-1) + (-2/5)(3) \\end{pmatrix} = -\\begin{pmatrix} 2/3 \\\\ -6/5 \\end{pmatrix} = \\begin{pmatrix} -2/3 \\\\ 6/5 \\end{pmatrix}\n$$\nIn decimal form, this is $\\beta^{(1)} = \\begin{pmatrix} -0.6666... \\\\ 1.2 \\end{pmatrix}$.\nRounding each entry to four significant figures, we get:\n$\\beta_0^{(1)} \\approx -0.6667$\n$\\beta_1^{(1)} = 1.200$\n\nThe updated parameter vector is $\\beta^{(1)} \\approx (-0.6667, 1.200)^{\\top}$. Expressed as a $1 \\times 2$ row matrix as requested:\n$\\begin{pmatrix} -0.6667 & 1.200 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -0.6667 & 1.200 \\end{pmatrix}}\n$$", "id": "4969348"}]}