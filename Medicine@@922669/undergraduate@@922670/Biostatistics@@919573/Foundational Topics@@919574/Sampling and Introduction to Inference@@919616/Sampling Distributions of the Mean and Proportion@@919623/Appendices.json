{"hands_on_practices": [{"introduction": "In biostatistics, comparing the effectiveness of a treatment against a control often involves comparing the proportion of successful outcomes in two independent groups. This exercise provides a foundational skill by guiding you through the derivation of the sampling distribution for the difference between two sample proportions, $\\hat{p}_1 - \\hat{p}_2$. By applying the Central Limit Theorem to Bernoulli trials, you will build from the ground up the statistical tool used for hypothesis testing and confidence intervals in countless clinical studies [@problem_id:4951512].", "problem": "A biostatistician is comparing the presence of a specific biomarker in two independent cohorts. In cohort $1$, each subject’s biomarker status is modeled as an independent Bernoulli random variable $X_{1,i}$ with success probability $p_{1}$, and in cohort $2$, each subject’s biomarker status is modeled as an independent Bernoulli random variable $X_{2,j}$ with success probability $p_{2}$. Let the sample sizes be $n_{1}$ and $n_{2}$, respectively, and define the sample proportions as $\\hat{p}_{1} = \\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} X_{1,i}$ and $\\hat{p}_{2} = \\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}} X_{2,j}$. Assume independence both within and across cohorts.\n\nUsing only the definitions of Bernoulli random variables and the Central Limit Theorem (CLT) for independent sums, derive the asymptotic distribution of $\\hat{p}_{1} - \\hat{p}_{2}$ as $n_{1} \\to \\infty$ and $n_{2} \\to \\infty$, and obtain its asymptotic variance in terms of $p_{1}$, $p_{2}$, $n_{1}$, and $n_{2}$.\n\nThen, evaluate the asymptotic variance numerically for $p_{1} = 0.12$, $p_{2} = 0.08$, $n_{1} = 800$, and $n_{2} = 1000$. Round your answer to four significant figures.", "solution": "The problem statement is first validated according to the specified procedure.\n\n### Step 1: Extract Givens\n- Cohort $1$: Subjects have a biomarker status modeled as independent Bernoulli random variables $X_{1,i}$ with success probability $p_{1}$. The sample size is $n_{1}$.\n- Cohort $2$: Subjects have a biomarker status modeled as independent Bernoulli random variables $X_{2,j}$ with success probability $p_{2}$. The sample size is $n_{2}$.\n- Sample proportion for cohort $1$: $\\hat{p}_{1} = \\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} X_{1,i}$.\n- Sample proportion for cohort $2$: $\\hat{p}_{2} = \\frac{1}{n_{2}}\\sum_{j=1}^{n_{2}} X_{2,j}$.\n- Independence is assumed both within and across cohorts.\n- The task is to derive the asymptotic distribution of $\\hat{p}_{1} - \\hat{p}_{2}$ as $n_{1} \\to \\infty$ and $n_{2} \\to \\infty$ using the Central Limit Theorem (CLT).\n- The task requires obtaining the asymptotic variance in terms of $p_{1}$, $p_{2}$, $n_{1}$, and $n_{2}$.\n- The task requires a numerical evaluation of the variance for $p_{1} = 0.12$, $p_{2} = 0.08$, $n_{1} = 800$, and $n_{2} = 1000$.\n- The numerical result must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is a standard exercise in biostatistics, relying on fundamental principles of probability theory, including Bernoulli distributions, sampling distributions, and the Central Limit Theorem. All premises are factually and scientifically sound.\n- **Well-Posed**: The problem is clearly defined. It provides all necessary information (distributions, parameters, sample sizes, independence assumption) to derive the requested asymptotic distribution and its variance. The question has a unique and meaningful solution.\n- **Objective**: The language is precise, formal, and free of any subjective or ambiguous terminology.\n\nThe problem does not exhibit any of the listed flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, or trivial. It is a standard, verifiable problem in statistical theory.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution is provided below.\n\nThe problem requires the derivation of the asymptotic distribution for the difference in sample proportions, $\\hat{p}_{1} - \\hat{p}_{2}$. This can be accomplished by first determining the asymptotic distribution of each sample proportion individually and then combining them.\n\nFor cohort $1$, each $X_{1,i}$ is an independent Bernoulli random variable with success probability $p_{1}$. The mean and variance of $X_{1,i}$ are:\n$$\nE[X_{1,i}] = p_{1}\n$$\n$$\n\\text{Var}(X_{1,i}) = p_{1}(1 - p_{1})\n$$\nThe sample proportion $\\hat{p}_{1}$ is the sample mean of $n_{1}$ independent and identically distributed (i.i.d.) random variables. The mean of $\\hat{p}_{1}$ is:\n$$\nE[\\hat{p}_{1}] = E\\left[\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} X_{1,i}\\right] = \\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} E[X_{1,i}] = \\frac{1}{n_{1}}(n_{1}p_{1}) = p_{1}\n$$\nThe variance of $\\hat{p}_{1}$, due to the independence of the $X_{1,i}$ variables, is:\n$$\n\\text{Var}(\\hat{p}_{1}) = \\text{Var}\\left(\\frac{1}{n_{1}}\\sum_{i=1}^{n_{1}} X_{1,i}\\right) = \\frac{1}{n_{1}^{2}}\\sum_{i=1}^{n_{1}} \\text{Var}(X_{1,i}) = \\frac{1}{n_{1}^{2}}(n_{1}p_{1}(1 - p_{1})) = \\frac{p_{1}(1 - p_{1})}{n_{1}}\n$$\nAccording to the Central Limit Theorem, as $n_{1} \\to \\infty$, the sampling distribution of $\\hat{p}_{1}$ converges to a Normal distribution with mean $p_{1}$ and variance $\\frac{p_{1}(1 - p_{1})}{n_{1}}$. We can write this as:\n$$\n\\hat{p}_{1} \\overset{d}{\\to} N\\left(p_{1}, \\frac{p_{1}(1 - p_{1})}{n_{1}}\\right)\n$$\nwhere $\\overset{d}{\\to}$ denotes convergence in distribution.\n\nSimilarly, for cohort $2$, we have $X_{2,j} \\sim \\text{Bernoulli}(p_{2})$ with $E[X_{2,j}] = p_{2}$ and $\\text{Var}(X_{2,j}) = p_{2}(1 - p_{2})$. The sample proportion $\\hat{p}_{2}$ has mean and variance:\n$$\nE[\\hat{p}_{2}] = p_{2}\n$$\n$$\n\\text{Var}(\\hat{p}_{2}) = \\frac{p_{2}(1 - p_{2})}{n_{2}}\n$$\nBy the CLT, as $n_{2} \\to \\infty$, the asymptotic distribution of $\\hat{p}_{2}$ is:\n$$\n\\hat{p}_{2} \\overset{d}{\\to} N\\left(p_{2}, \\frac{p_{2}(1 - p_{2})}{n_{2}}\\right)\n$$\nThe problem states that the two cohorts are independent. Therefore, the random variables $\\hat{p}_{1}$ and $\\hat{p}_{2}$ are independent. A property of Normal distributions is that the difference of two independent Normally distributed random variables is also Normally distributed.\n\nWe are interested in the distribution of the difference $D = \\hat{p}_{1} - \\hat{p}_{2}$. The mean of the difference is:\n$$\nE[D] = E[\\hat{p}_{1} - \\hat{p}_{2}] = E[\\hat{p}_{1}] - E[\\hat{p}_{2}] = p_{1} - p_{2}\n$$\nDue to the independence of $\\hat{p}_{1}$ and $\\hat{p}_{2}$, the variance of the difference is the sum of their variances:\n$$\n\\text{Var}(D) = \\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\text{Var}(\\hat{p}_{1}) + \\text{Var}(\\hat{p}_{2}) = \\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}\n$$\nThus, the asymptotic distribution of $\\hat{p}_{1} - \\hat{p}_{2}$ as $n_{1} \\to \\infty$ and $n_{2} \\to \\infty$ is a Normal distribution with mean $p_{1} - p_{2}$ and variance $\\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}$. Formally:\n$$\n\\hat{p}_{1} - \\hat{p}_{2} \\overset{d}{\\to} N\\left(p_{1} - p_{2}, \\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}\\right)\n$$\nThe asymptotic variance of $\\hat{p}_{1} - \\hat{p}_{2}$ is:\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{p_{1}(1 - p_{1})}{n_{1}} + \\frac{p_{2}(1 - p_{2})}{n_{2}}\n$$\nNow, we evaluate this variance numerically using the given values: $p_{1} = 0.12$, $p_{2} = 0.08$, $n_{1} = 800$, and $n_{2} = 1000$.\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{0.12(1 - 0.12)}{800} + \\frac{0.08(1 - 0.08)}{1000}\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{0.12(0.88)}{800} + \\frac{0.08(0.92)}{1000}\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = \\frac{0.1056}{800} + \\frac{0.0736}{1000}\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = 0.000132 + 0.0000736\n$$\n$$\n\\text{Var}(\\hat{p}_{1} - \\hat{p}_{2}) = 0.0002056\n$$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.0002056$. The first significant figure is the first non-zero digit, which is $2$. The four significant figures are $2$, $0$, $5$, and $6$. The number as calculated, $0.0002056$, already has exactly four significant figures. Therefore, no additional rounding is needed.\nThe result is $0.0002056$.", "answer": "$$\n\\boxed{0.0002056}\n$$", "id": "4951512"}, {"introduction": "While comparing the means of two groups is a common task, a frequent real-world complication is that the two populations may not have the same variance. This scenario, known as the Behrens-Fisher problem, means the standard Student's $t$-test is not appropriate. This practice explores why the test statistic in this case doesn't follow a simple $t$-distribution and guides you through the derivation of the Welch-Satterthwaite approximation, a robust solution used widely in modern statistical software [@problem_id:4951504].", "problem": "A biostatistics team is comparing the mean change in systolic blood pressure between two independent cohorts: a treatment group and a control group. The change (follow-up minus baseline) in each group is assumed to be a random sample from a normal population with potentially unequal variances. Let the treatment group have sample size $n_1$ and sample variance $s_1^{2}$, and the control group have sample size $n_2$ and sample variance $s_2^{2}$. The sample means are $\\bar{X}_1$ and $\\bar{X}_2$, respectively. Consider the standardized mean difference statistic\n$$\nT \\;=\\; \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^{2}}{n_1} + \\frac{s_2^{2}}{n_2}}}.\n$$\nStarting only from the following foundational facts and definitions:\n- If $X_1,\\dots,X_{n}$ are independent and identically distributed as normal with mean $\\mu$ and variance $\\sigma^{2}$, then the sample mean $\\bar{X}$ is normal with mean $\\mu$ and variance $\\sigma^{2}/n$.\n- For a normal sample, the unbiased sample variance $S^{2}$ satisfies $(n-1)\\,S^{2}/\\sigma^{2} \\sim \\chi^{2}_{n-1}$, and $S^{2}$ is independent of $\\bar{X}$.\n- If $U \\sim \\chi^{2}_{k}$, then $E[U] = k$ and $\\operatorname{Var}(U) = 2k$.\n- The Central Limit Theorem (CLT) ensures approximate normality of sums and averages under finite variance.\n\nTasks:\n1. Explain why, when variances are unequal, the ratio $T$ is not exactly distributed as a Student’s $t$ random variable, even under normality, and why an approximation is needed.\n2. Using moment-matching to a scaled chi-square distribution for the random denominator $\\frac{s_1^{2}}{n_1} + \\frac{s_2^{2}}{n_2}$, derive the Satterthwaite Degrees of Freedom (df) approximation for the sampling distribution of $T$ under unequal variances.\n3. A study records the following: $n_1 = 42$, $s_1^{2} = 12.1$, $n_2 = 67$, $s_2^{2} = 18.7$. Under the assumptions above, compute the approximate Satterthwaite df for the sampling distribution of $T$.\n\nRound your final numerical answer to four significant figures. Do not include any units in your final answer.", "solution": "This problem is a valid exercise in biostatistics concerning the Behrens-Fisher problem, which deals with the comparison of means from two normal populations with unknown and potentially unequal variances. The problem is well-posed, scientifically grounded, and provides all necessary information for its solution.\n\nThe problem asks for three tasks: an explanation of the distributional nature of the test statistic $T$, a derivation of the Satterthwaite approximation for the degrees of freedom, and a numerical calculation of these degrees of freedom for a given dataset.\n\n**Task 1: Why the Welch's $t$-statistic $T$ is not exactly Student's $t$-distributed**\n\nA random variable is defined to follow a Student's $t$-distribution with $k$ degrees of freedom, denoted $t_k$, if it can be written as the ratio $t_k = \\frac{Z}{\\sqrt{U/k}}$, where $Z$ is a standard normal random variable ($Z \\sim N(0,1)$), $U$ is a chi-square random variable with $k$ degrees of freedom ($U \\sim \\chi_k^2$), and $Z$ and $U$ are independent.\n\nLet's analyze the statistic $T = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^{2}}{n_1} + \\frac{s_2^{2}}{n_2}}}$.\n\nThe numerator is $\\bar{X}_1 - \\bar{X}_2$. Based on the provided foundational facts, since the samples are drawn from normal populations, $\\bar{X}_1 \\sim N(\\mu_1, \\sigma_1^2/n_1)$ and $\\bar{X}_2 \\sim N(\\mu_2, \\sigma_2^2/n_2)$. Since the two cohorts are independent, $\\bar{X}_1$ and $\\bar{X}_2$ are independent. The difference of two independent normal variables is also normal. Therefore, the distribution of the difference is $\\bar{X}_1 - \\bar{X}_2 \\sim N(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})$.\n\nTo construct a standard normal variable $Z$, we standardize this difference:\n$$\nZ = \\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0,1)\n$$\nThe numerator of the statistic $T$, under the null hypothesis $H_0: \\mu_1=\\mu_2$, is $\\bar{X}_1 - \\bar{X}_2$.\n\nThe denominator of $T$ is the square root of the quantity $V = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}$. This term $V$ is an estimator for the true variance of the numerator, $\\operatorname{Var}(\\bar{X}_1 - \\bar{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$. The independence of $\\bar{X}$ and $S^2$ for a normal sample implies that the numerator $\\bar{X}_1 - \\bar{X}_2$ is independent of the denominator term $V$.\n\nNow, let's examine the distribution of $V$. From the foundational facts, we know that for each sample $i \\in \\{1, 2\\}$, the quantity $\\frac{(n_i-1)s_i^2}{\\sigma_i^2}$ follows a chi-square distribution with $n_i-1$ degrees of freedom, i.e., $\\frac{(n_i-1)s_i^2}{\\sigma_i^2} \\sim \\chi_{n_i-1}^2$. This can be rewritten as $s_i^2 \\sim \\frac{\\sigma_i^2}{n_i-1}\\chi_{n_i-1}^2$.\n\nThe term $V$ is therefore a linear combination of two independent, scaled chi-square variables:\n$$\nV = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\sim \\frac{\\sigma_1^2}{n_1(n_1-1)}\\chi_{n_1-1}^2 + \\frac{\\sigma_2^2}{n_2(n_2-1)}\\chi_{n_2-1}^2\n$$\nA fundamental theorem in statistics states that a linear combination of independent chi-square variables, $Y = \\sum a_i \\chi_{k_i}^2$, is itself a scaled chi-square variable only in specific cases, such as when all the scaling coefficients $a_i$ are equal. In our case, the scaling coefficients are $\\frac{\\sigma_1^2}{n_1(n_1-1)}$ and $\\frac{\\sigma_2^2}{n_2(n_2-1)}$. These coefficients are equal only under very restrictive conditions on the population variances and sample sizes, which are not generally met, particularly when $\\sigma_1^2 \\neq \\sigma_2^2$.\n\nSince the random variable $V$ is not, in general, a single scaled chi-square variable, the ratio $T$ does not exactly follow a Student's $t$-distribution. Its exact distribution is complex, which necessitates the use of an approximation, such as the Satterthwaite approximation for the effective degrees of freedom.\n\n**Task 2: Derivation of the Satterthwaite Degrees of Freedom**\n\nThe Satterthwaite approximation aims to approximate the distribution of $V = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}$ by a scaled chi-square distribution of the form $c \\cdot \\chi_\\nu^2$, where $\\nu$ represents the effective degrees of freedom. The approximation is achieved by matching the first two moments (mean and variance) of $V$ with those of $c \\cdot \\chi_\\nu^2$.\n\nFirst, let's find the mean and variance of $V$.\nThe expectation of $V$ is:\n$E[V] = E\\left[\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right] = \\frac{1}{n_1}E[s_1^2] + \\frac{1}{n_2}E[s_2^2]$.\nSince $s_i^2$ is an unbiased estimator of the population variance $\\sigma_i^2$, we have $E[s_i^2] = \\sigma_i^2$.\nThus, $E[V] = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$.\n\nThe variance of $V$, using the independence of $s_1^2$ and $s_2^2$, is:\n$\\operatorname{Var}(V) = \\operatorname{Var}\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right) = \\frac{1}{n_1^2}\\operatorname{Var}(s_1^2) + \\frac{1}{n_2^2}\\operatorname{Var}(s_2^2)$.\nTo find $\\operatorname{Var}(s_i^2)$, we use the fact that $\\frac{(n_i-1)s_i^2}{\\sigma_i^2} \\sim \\chi_{n_i-1}^2$.\nThis implies $s_i^2 = \\frac{\\sigma_i^2}{n_i-1}\\chi_{n_i-1}^2$.\n$\\operatorname{Var}(s_i^2) = \\operatorname{Var}\\left(\\frac{\\sigma_i^2}{n_i-1}\\chi_{n_i-1}^2\\right) = \\left(\\frac{\\sigma_i^2}{n_i-1}\\right)^2 \\operatorname{Var}(\\chi_{n_i-1}^2)$.\nUsing the given fact that $\\operatorname{Var}(\\chi_k^2) = 2k$, we get:\n$\\operatorname{Var}(s_i^2) = \\frac{\\sigma_i^4}{(n_i-1)^2} \\cdot 2(n_i-1) = \\frac{2\\sigma_i^4}{n_i-1}$.\nSubstituting this back into the expression for $\\operatorname{Var}(V)$:\n$\\operatorname{Var}(V) = \\frac{1}{n_1^2}\\left(\\frac{2\\sigma_1^4}{n_1-1}\\right) + \\frac{1}{n_2^2}\\left(\\frac{2\\sigma_2^4}{n_2-1}\\right)$.\n\nNow, consider the approximating distribution $A = c \\cdot \\chi_\\nu^2$. Its moments are:\n$E[A] = c \\cdot E[\\chi_\\nu^2] = c\\nu$.\n$\\operatorname{Var}(A) = c^2 \\cdot \\operatorname{Var}(\\chi_\\nu^2) = c^2 (2\\nu) = 2c^2\\nu$.\n\nEquating the moments of $V$ and $A$:\n1. $E[V] = E[A] \\implies \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} = c\\nu$\n2. $\\operatorname{Var}(V) = \\operatorname{Var}(A) \\implies \\frac{2\\sigma_1^4}{n_1^2(n_1-1)} + \\frac{2\\sigma_2^4}{n_2^2(n_2-1)} = 2c^2\\nu$\n\nFrom equation (1), we solve for $c$: $c = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)$.\nSubstitute this into equation (2), after simplifying by a factor of 2:\n$\\frac{\\sigma_1^4}{n_1^2(n_1-1)} + \\frac{\\sigma_2^4}{n_2^2(n_2-1)} = c^2\\nu = \\left[\\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)\\right]^2\\nu = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2$.\n\nNow, we solve for the degrees of freedom, $\\nu$:\n$$\n\\nu = \\frac{\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2}{\\frac{\\sigma_1^4}{n_1^2(n_1-1)} + \\frac{\\sigma_2^4}{n_2^2(n_2-1)}}\n$$\nThis can be rewritten in a more memorable form:\n$$\n\\nu = \\frac{\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{\\sigma_1^2}{n_1}\\right)^2}{n_1-1} + \\frac{\\left(\\frac{\\sigma_2^2}{n_2}\\right)^2}{n_2-1}}\n$$\nSince the population variances $\\sigma_1^2$ and $\\sigma_2^2$ are unknown, we substitute their unbiased sample estimates, $s_1^2$ and $s_2^2$, to obtain the estimated degrees of freedom, which we will denote as $df$:\n$$\ndf = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{s_1^2}{n_1}\\right)^2}{n_1-1} + \\frac{\\left(\\frac{s_2^2}{n_2}\\right)^2}{n_2-1}}\n$$\nThis is the Welch-Satterthwaite equation for the approximate degrees of freedom.\n\n**Task 3: Numerical Calculation of Degrees of Freedom**\n\nThe problem provides the following data: $n_1 = 42$, $s_1^2 = 12.1$, $n_2 = 67$, and $s_2^2 = 18.7$. We substitute these values into the derived formula for $df$.\n\nFirst, calculate the individual terms $\\frac{s_i^2}{n_i}$:\n$v_1 = \\frac{s_1^2}{n_1} = \\frac{12.1}{42} \\approx 0.288095$\n$v_2 = \\frac{s_2^2}{n_2} = \\frac{18.7}{67} \\approx 0.279104$\n\nNow, substitute these into the Satterthwaite formula:\n$$\ndf = \\frac{(v_1 + v_2)^2}{\\frac{v_1^2}{n_1-1} + \\frac{v_2^2}{n_2-1}} = \\frac{\\left(\\frac{12.1}{42} + \\frac{18.7}{67}\\right)^2}{\\frac{\\left(\\frac{12.1}{42}\\right)^2}{42-1} + \\frac{\\left(\\frac{18.7}{67}\\right)^2}{67-1}}\n$$\n$$\ndf = \\frac{\\left(0.288095238... + 0.279104477...\\right)^2}{\\frac{\\left(0.288095238...\\right)^2}{41} + \\frac{\\left(0.279104477...\\right)^2}{66}}\n$$\n$$\ndf = \\frac{\\left(0.567199715...\\right)^2}{\\frac{0.083000997...}{41} + \\frac{0.077900003...}{66}}\n$$\n$$\ndf = \\frac{0.321715115...}{0.002024414... + 0.001180303...}\n$$\n$$\ndf = \\frac{0.321715115...}{0.003204717...}\n$$\n$$\ndf \\approx 100.38797\n$$\nRounding the result to four significant figures, we get $100.4$.", "answer": "$$\\boxed{100.4}$$", "id": "4951504"}, {"introduction": "Asymptotic theory provides powerful tools, but it is crucial to understand their limitations. This exercise shifts our focus from calculation to critical thinking, examining the performance of the common Wald-type pivot for a single proportion, especially when the true proportion $p$ is close to the boundaries of $0$ or $1$. By analyzing when this approximation holds and when it fails, you will develop a deeper, more practical understanding of the assumptions underpinning many of the statistical methods you use [@problem_id:4951553].", "problem": "A biostatistician observes independent and identically distributed Bernoulli trials $X_{1},\\dots,X_{n}$ with success probability $p \\in [0,1]$, and uses the sample proportion $\\hat{p} = n^{-1}\\sum_{i=1}^{n} X_{i}$ to make inference on $p$. Consider the standardized statistic built by centering $\\hat{p}$ at $p$ and dividing by a plug-in estimate of its standard deviation, often called a Wald-type pivot. Using only fundamental properties of the Bernoulli distribution, the Law of Large Numbers, and the Central Limit Theorem, determine which statements about this pivot’s large-sample behavior and its potential failure near the parameter space boundaries are correct.\n\nSelect all statements that are correct.\n\nA. For any fixed $p \\in (0,1)$, the Wald-type pivot that centers $\\hat{p}$ at $p$ and uses the plug-in standard error based on $\\hat{p}$ converges in distribution to a standard normal as $n \\to \\infty$.\n\nB. If $p = p_{n} \\to 0$ while $n p_{n} \\to \\lambda \\in (0,\\infty)$, the Wald-type pivot remains approximately standard normal as $n$ increases.\n\nC. For finite $n$, the Wald-type pivot can be undefined with positive probability when $p$ is near $0$ or near $1$, because the plug-in standard error can be zero when all observations are the same.\n\nD. The Wald-type pivot is exactly standard normal for any finite $n$ because $\\hat{p}$ is an unbiased estimator of $p$.\n\nE. If $p = p_{n}$ is allowed to vary with $n$ but remains bounded away from the boundaries in the sense that there exists $c \\in (0,\\tfrac{1}{2})$ with $p_{n} \\in [c,1-c]$ for all sufficiently large $n$, then the Wald-type pivot is asymptotically standard normal.\n\nF. The common rule-of-thumb that $n \\hat{p} \\ge 10$ and $n (1-\\hat{p}) \\ge 10$ is a necessary and sufficient condition for the Wald-type pivot to be approximately standard normal.\n\nG. Using the true (unknown) standard deviation in the denominator instead of the plug-in estimate would yield the same large-sample standard normal limit for the pivot.", "solution": "The problem asks for an evaluation of several statements regarding the large-sample behavior of a Wald-type pivot for a Bernoulli proportion. Let the independent and identically distributed (i.i.d.) random variables be $X_{1}, \\dots, X_{n}$, where $X_i \\sim \\text{Bernoulli}(p)$ for some success probability $p \\in [0,1]$. The sample proportion is given by $\\hat{p} = n^{-1}\\sum_{i=1}^{n} X_{i}$.\n\nThe fundamental properties of the random variable $X_i$ are its expectation $E[X_i] = p$ and variance $\\text{Var}(X_i) = p(1-p)$.\nFor the sample proportion $\\hat{p}$, we have:\n-   Expectation: $E[\\hat{p}] = E[n^{-1}\\sum_{i=1}^{n} X_i] = n^{-1}\\sum_{i=1}^{n} E[X_i] = n^{-1}(np) = p$.\n-   Variance: $\\text{Var}(\\hat{p}) = \\text{Var}(n^{-1}\\sum_{i=1}^{n} X_i) = n^{-2}\\sum_{i=1}^{n} \\text{Var}(X_i) = n^{-2}(np(1-p)) = \\frac{p(1-p)}{n}$.\nThe standard deviation of $\\hat{p}$ is $\\text{SD}(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}$.\n\nThe Wald-type pivot is formed by centering $\\hat{p}$ at its mean $p$ and standardizing using a plug-in estimate of the standard deviation. The plug-in estimate for the standard error of $\\hat{p}$ is $\\text{SE}(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$.\nThus, the Wald-type pivot is the statistic:\n$$ W = \\frac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} $$\nWe will use the Law of Large Numbers (LLN), which states that $\\hat{p}$ converges in probability to $p$ (denoted $\\hat{p} \\xrightarrow{p} p$), and the Central Limit Theorem (CLT), which states that for a fixed $p \\in (0,1)$, the standardized statistic\n$$ Z_n = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}} $$\nconverges in distribution to a standard normal distribution, $N(0,1)$, as $n \\to \\infty$ (denoted $Z_n \\xrightarrow{d} N(0,1)$). We will also use Slutsky's Theorem.\n\nNow, we evaluate each statement.\n\n**A. For any fixed $p \\in (0,1)$, the Wald-type pivot that centers $\\hat{p}$ at $p$ and uses the plug-in standard error based on $\\hat{p}$ converges in distribution to a standard normal as $n \\to \\infty$.**\n\nFor a fixed $p \\in (0,1)$, the variance $p(1-p)$ is positive. The CLT applies, so $Z_n = \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}} \\xrightarrow{d} N(0,1)$.\nThe Wald pivot $W$ can be written as:\n$$ W = \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}} \\cdot \\sqrt{\\frac{p(1-p)}{\\hat{p}(1-\\hat{p})}} = Z_n \\cdot \\sqrt{\\frac{p(1-p)}{\\hat{p}(1-\\hat{p})}} $$\nBy the LLN, $\\hat{p} \\xrightarrow{p} p$. Since the function $g(x) = x(1-x)$ is continuous, the Continuous Mapping Theorem implies that $\\hat{p}(1-\\hat{p}) \\xrightarrow{p} p(1-p)$.\nBecause $p \\in (0,1)$, $p(1-p) > 0$. Thus, the ratio $\\frac{\\hat{p}(1-\\hat{p})}{p(1-p)} \\xrightarrow{p} 1$, and its square root also converges in probability to $1$.\nBy Slutsky's Theorem, since $Z_n \\xrightarrow{d} N(0,1)$ and $\\sqrt{\\frac{p(1-p)}{\\hat{p}(1-\\hat{p})}} \\xrightarrow{p} 1$, their product $W$ converges in distribution to $1 \\cdot N(0,1)$, which is a standard normal distribution.\nVerdict: **Correct**.\n\n**B. If $p = p_{n} \\to 0$ while $n p_{n} \\to \\lambda \\in (0,\\infty)$, the Wald-type pivot remains approximately standard normal as $n$ increases.**\n\nThis scenario describes behavior at the boundary of the parameter space. Let $S_n = n\\hat{p} = \\sum_{i=1}^n X_i$. $S_n$ follows a binomial distribution, $S_n \\sim \\text{Bin}(n, p_n)$. Under the given conditions, the distribution of $S_n$ converges to a Poisson distribution with parameter $\\lambda$, i.e., $S_n \\xrightarrow{d} Y \\sim \\text{Poisson}(\\lambda)$.\nThe Wald pivot is $W_n = \\frac{\\hat{p}_n - p_n}{\\sqrt{\\hat{p}_n(1-\\hat{p}_n)/n}} = \\frac{n\\hat{p}_n - np_n}{\\sqrt{n\\hat{p}_n(1-\\hat{p}_n)}} = \\frac{S_n - np_n}{\\sqrt{S_n(1-S_n/n)}}$.\nAs $n \\to \\infty$:\n- The numerator $S_n - np_n \\xrightarrow{d} Y - \\lambda$.\n- In the denominator, $S_n \\xrightarrow{d} Y$, so $S_n$ is stochastically bounded. This implies $S_n^2/n \\xrightarrow{p} 0$. Thus, the term under the square root, $S_n(1-S_n/n) = S_n - S_n^2/n$, converges in distribution to $Y$. The denominator $\\sqrt{S_n(1-S_n/n)}$ converges in distribution to $\\sqrt{Y}$.\n- By the Continuous Mapping Theorem for convergence in distribution, $W_n \\xrightarrow{d} \\frac{Y-\\lambda}{\\sqrt{Y}}$.\nThe limiting distribution is a function of a Poisson random variable, which is discrete and not a standard normal distribution. For instance, $P(Y=0) = e^{-\\lambda} > 0$. When $Y=0$, the statistic is undefined.\nVerdict: **Incorrect**.\n\n**C. For finite $n$, the Wald-type pivot can be undefined with positive probability when $p$ is near $0$ or near $1$, because the plug-in standard error can be zero when all observations are the same.**\n\nThe pivot $W$ is undefined if its denominator is zero. The denominator is $\\sqrt{\\hat{p}(1-\\hat{p})/n}$, which is zero if and only if $\\hat{p}(1-\\hat{p}) = 0$. This occurs when $\\hat{p}=0$ or $\\hat{p}=1$.\n- $\\hat{p}=0$ corresponds to the event that all $X_i$ are $0$. The probability of this event is $P(\\sum_{i=1}^n X_i = 0) = (1-p)^n$.\n- $\\hat{p}=1$ corresponds to the event that all $X_i$ are $1$. The probability of this event is $P(\\sum_{i=1}^n X_i = n) = p^n$.\nFor any finite $n \\ge 1$ and any $p \\in (0,1)$, both $p^n$ and $(1-p)^n$ are strictly positive. The total probability of the pivot being undefined is $p^n + (1-p)^n > 0$. This probability is larger when $p$ is close to $0$ or $1$.\nVerdict: **Correct**.\n\n**D. The Wald-type pivot is exactly standard normal for any finite $n$ because $\\hat{p}$ is an unbiased estimator of $p$.**\n\nFor any finite sample size $n$, the number of successes $n\\hat{p}$ can only take integer values from $0$ to $n$. Thus, $\\hat{p}$ can only take values in the set $\\{0, 1/n, 2/n, \\dots, 1\\}$. The distribution of $\\hat{p}$ is a scaled binomial distribution, which is discrete. Any function of $\\hat{p}$, such as the Wald pivot, will also have a discrete distribution. A standard normal distribution is continuous over the entire real line. A discrete distribution cannot be exactly equal to a continuous one. The unbiasedness of $\\hat{p}$ ($E[\\hat{p}]=p$) is a property of its mean and does not determine the exact shape of its distribution.\nVerdict: **Incorrect**.\n\n**E. If $p = p_{n}$ is allowed to vary with $n$ but remains bounded away from the boundaries in the sense that there exists $c \\in (0,\\tfrac{1}{2})$ with $p_{n} \\in [c,1-c]$ for all sufficiently large $n$, then the Wald-type pivot is asymptotically standard normal.**\n\nThis is a more general case where $p_n$ is not fixed but stays in a compact subset of $(0,1)$. The condition $p_n \\in [c, 1-c]$ implies that the variance $\\text{Var}(X_{n,i}) = p_n(1-p_n)$ is bounded below by $c(1-c) > 0$. This allows for the application of a CLT for triangular arrays, such as the Lindeberg-Feller CLT. The Lindeberg condition is satisfied because the random variables are bounded ($|X_{n,i}-p_n| \\le 1$), while the variance of the sum grows with $n$. This ensures that $\\frac{\\hat{p}_n - p_n}{\\sqrt{p_n(1-p_n)/n}} \\xrightarrow{d} N(0,1)$.\nNext, we need to show the plug-in standard error is consistent. We need to show $\\hat{p}_n(1-\\hat{p}_n) / (p_n(1-p_n)) \\xrightarrow{p} 1$. This requires showing $\\hat{p}_n - p_n \\xrightarrow{p} 0$. By Chebyshev's inequality, $P(|\\hat{p}_n - p_n| \\ge \\epsilon) \\le \\frac{\\text{Var}(\\hat{p}_n)}{\\epsilon^2} = \\frac{p_n(1-p_n)}{n\\epsilon^2}$. Since $p_n(1-p_n) \\le 1/4$, this probability is bounded by $(4n\\epsilon^2)^{-1}$, which goes to $0$ as $n \\to \\infty$. Thus $\\hat{p}_n - p_n \\xrightarrow{p} 0$. By the Continuous Mapping Theorem, $\\hat{p}_n(1-\\hat{p}_n) - p_n(1-p_n) \\xrightarrow{p} 0$. Since $p_n(1-p_n)$ is bounded away from $0$, this implies consistency of the standard error estimate. By Slutsky's Theorem, the Wald pivot is asymptotically standard normal.\nVerdict: **Correct**.\n\n**F. The common rule-of-thumb that $n \\hat{p} \\ge 10$ and $n (1-\\hat{p}) \\ge 10$ is a necessary and sufficient condition for the Wald-type pivot to be approximately standard normal.**\n\nThis statement is false. A \"rule of thumb\" is a practical guideline, not a formal mathematical condition.\n-   It is not sufficient: \"Approximately standard normal\" is not a mathematically precise concept. For $p$ very close to $0$ or $1$, the approximation can be poor even if this rule is met. The accuracy of the approximation depends continuously on $n$ and $p$.\n-   It is not necessary: The theoretical result (asymptotic normality) holds for any fixed $p \\in (0,1)$ as $n \\to \\infty$, regardless of whether $np \\ge 10$. For a small $p$, a very large $n$ will be needed for a good approximation, but the convergence is still guaranteed. The condition is stated in terms of the random quantity $\\hat{p}$, making it a statement about a particular dataset, not about the underlying distribution of the pivot. A condition for a distributional property should not depend on a single realization of the data.\nVerdict: **Incorrect**.\n\n**G. Using the true (unknown) standard deviation in the denominator instead of the plug-in estimate would yield the same large-sample standard normal limit for the pivot.**\n\nThe pivot with the true standard deviation is $Z_n = \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}}$.\nThe pivot with the plug-in estimate is $W_n = \\frac{\\hat{p} - p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}$.\nThe phrase \"large-sample standard normal limit\" implies we are in a regime where this limit is attained, such as the conditions in statements A or E.\nUnder these conditions, the CLT establishes that $Z_n \\xrightarrow{d} N(0,1)$.\nAs shown in the analysis for statement A, the fact that $\\hat{p}$ is a consistent estimator for $p$ implies that the estimated standard error is a consistent estimator for the true standard error. Slutsky's Theorem then shows that replacing the true standard error with the estimated one does not alter the limiting distribution. Therefore, $W_n$ also converges to $N(0,1)$. So, both statistics have the same limiting distribution, a standard normal.\nVerdict: **Correct**.", "answer": "$$\\boxed{ACEG}$$", "id": "4951553"}]}