{"hands_on_practices": [{"introduction": "The arithmetic mean is often our first choice for summarizing a dataset, but it can be surprisingly misleading. When data is highly skewed, as is common with many biological measurements, the mean is pulled towards the long tail and may not represent a \"typical\" value. This exercise [@problem_id:4955570] challenges you to explore this phenomenon from first principles, using a constructed distribution that mimics a real-world scenario of a biomarker with a small, highly responsive subgroup. By deriving both the mean and the median, you will gain a fundamental understanding of why the median is often a more robust and informative summary in the face of extreme skewness.", "problem": "A clinical laboratory quantifies a bounded biomarker as a proportion, taking values in the interval $[0,1]$. In a particular population, most individuals have almost no detectable biomarker, while a small subgroup has nearly saturated measurements due to a genetic variant. To study how summary measures behave under such skewness, consider the following constructed distribution for a random variable $X$ representing the biomarker proportion:\n- With probability $0.95$, $X$ is continuously distributed as uniform on $[0,0.01]$.\n- With probability $0.05$, $X$ equals $1$ exactly.\n\nUsing only core definitions of the arithmetic mean (expected value), cumulative distribution function, and median, do the following:\n1. Derive the arithmetic mean of $X$ from first principles and interpret it in the context of this population. Explain why it may be misleading as a measure of a “typical” level in this setting.\n2. Derive the median of $X$ by solving from the cumulative distribution function, and justify why the median is a more appropriate summary for this highly skewed, bounded biomarker.\n\nProvide your final numeric answer as the value of the median of $X$, rounded to four significant figures. No units are required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- A random variable $X$ represents a biomarker proportion.\n- The sample space for $X$ is the interval $[0,1]$.\n- The distribution of $X$ is a mixture model defined as follows:\n    - With probability $0.95$, $X$ is drawn from a continuous uniform distribution on the interval $[0,0.01]$.\n    - With probability $0.05$, $X$ takes the discrete value $1$.\n- The task requires deriving the arithmetic mean and median from first principles, interpreting and comparing them as summary measures for this distribution, and providing the numerical value of the median rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses a mixture distribution, which is a standard statistical model. Such skewed distributions with a point mass at an extreme are common in biostatistics (e.g., representing a subpopulation with a saturated measurement). The concepts of mean, median, and cumulative distribution function are fundamental to probability theory. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The distribution of the random variable $X$ is fully and unambiguously specified. The tasks requested—calculating the mean and median—are well-defined operations for this distribution and will yield unique, stable solutions.\n- **Objective:** The problem is stated using precise, formal mathematical and statistical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is mathematically sound, well-posed, and objective. A complete solution will be provided.\n\n**Solution Derivation**\n\nLet the random variable $X$ denote the biomarker proportion. Its distribution is a mixture of a continuous uniform distribution and a discrete point mass. Let $U$ be a random variable with a uniform distribution on $[0, 0.01]$, i.e., $U \\sim \\text{Uniform}(0, 0.01)$. Let $C$ be a random variable representing the point mass at $1$, so $P(C=1)=1$. The distribution of $X$ can be written as a mixture:\n$$\nX = \n\\begin{cases} \nU  \\text{with probability } 0.95 \\\\\nC  \\text{with probability } 0.05 \n\\end{cases}\n$$\n\n**1. Arithmetic Mean of $X$**\nThe arithmetic mean of $X$ is its expected value, $E[X]$. We can calculate this using the law of total expectation. Let $A$ be the event that $X$ is drawn from the uniform distribution.\n$$E[X] = E[X|A]P(A) + E[X|A^c]P(A^c)$$\nHere, $P(A) = 0.95$ and $P(A^c) = 0.05$.\nThe conditional expectation $E[X|A]$ is the mean of the uniform distribution $\\text{Uniform}(0, 0.01)$, which is:\n$$E[X|A] = \\frac{0 + 0.01}{2} = 0.005$$\nThe conditional expectation $E[X|A^c]$ is the value of the random variable given it is a point mass at $1$:\n$$E[X|A^c] = 1$$\nSubstituting these values into the law of total expectation:\n$$E[X] = (0.005)(0.95) + (1)(0.05)$$\n$$E[X] = 0.00475 + 0.05 = 0.05475$$\nThe arithmetic mean of the biomarker proportion is $0.05475$.\n\n**Interpretation and Critique of the Mean:**\nThe mean value of $0.05475$ is arithmetically correct, but it is a misleading summary of a “typical” biomarker level in this population. $95\\%$ of the individuals have a biomarker level in the range $[0, 0.01]$, which is an order of magnitude smaller than the mean. The remaining $5\\%$ have a level of exactly $1$. The mean is heavily skewed upward by this small but extreme subgroup. No individual in the population has a biomarker level close to the mean value of $0.05475$, illustrating how the mean can fail to represent the central tendency in a highly skewed distribution.\n\n**2. Median of $X$**\nTo find the median, we first need to derive the Cumulative Distribution Function (CDF) of $X$, denoted by $F_X(x) = P(X \\le x)$. We use the law of total probability, conditioning on the same event $A$:\n$$F_X(x) = P(X \\le x | A)P(A) + P(X \\le x | A^c)P(A^c)$$\nThe CDF of $U \\sim \\text{Uniform}(0, 0.01)$ is:\n$$F_U(x) = P(U \\le x) = \n\\begin{cases} \n0  x  0 \\\\\n\\frac{x}{0.01} = 100x  0 \\le x  0.01 \\\\\n1  x \\ge 0.01\n\\end{cases}\n$$\nThe CDF of the point mass $C=1$ is a step function:\n$$F_C(x) = P(C \\le x) = \n\\begin{cases} \n0  x  1 \\\\\n1  x \\ge 1\n\\end{cases}\n$$\nCombining these with the mixture probabilities $P(A)=0.95$ and $P(A^c)=0.05$:\n- For $x  0$: $F_X(x) = (0.95)(0) + (0.05)(0) = 0$.\n- For $0 \\le x  0.01$: $F_X(x) = (0.95)(100x) + (0.05)(0) = 95x$.\n- For $0.01 \\le x  1$: $F_X(x) = (0.95)(1) + (0.05)(0) = 0.95$.\n- For $x \\ge 1$: $F_X(x) = (0.95)(1) + (0.05)(1) = 1$.\n\nSo, the complete CDF is:\n$$F_X(x) = \n\\begin{cases} \n0  x  0 \\\\\n95x  0 \\le x  0.01 \\\\\n0.95  0.01 \\le x  1 \\\\\n1  x \\ge 1\n\\end{cases}\n$$\nThe median, $m$, is the value for which $F_X(m) = 0.5$. We must find which interval of the CDF contains the value $0.5$.\nAs $x$ goes from $0$ to $0.01$, $F_X(x) = 95x$ goes from $0$ to $0.95$. Since $0.5$ lies within this range, the median $m$ must be in the interval $[0, 0.01)$. We solve for $m$:\n$$F_X(m) = 95m = 0.5$$\n$$m = \\frac{0.5}{95} = \\frac{1}{190}$$\nTo provide the final answer, we calculate the numerical value and round to four significant figures:\n$$m = \\frac{1}{190} \\approx 0.005263157...$$\nRounding to four significant figures gives $m \\approx 0.005263$.\n\n**Justification of the Median:**\nThe median is $m \\approx 0.005263$. This value is located within the range $[0, 0.01]$, which contains $95\\%$ of the population. The median, by definition, is the value that separates the lower $50\\%$ of the population from the upper $50\\%$. Unlike the mean, the median is not sensitive to the magnitude of extreme values (outliers). In this case, the $5\\%$ of the population with a value of $1$ does not pull the median upward. Therefore, the median provides a much more robust and representative measure of the central tendency, or \"typical\" biomarker level, for this highly skewed distribution. It accurately reflects that a typical individual has a very low biomarker level.", "answer": "$$\\boxed{0.005263}$$", "id": "4955570"}, {"introduction": "Real-world clinical data is rarely perfect and often contains outliers due to measurement error or unique patient characteristics. As we've seen, the sample mean is highly sensitive to these extreme values. This practice [@problem_id:4955542] introduces a powerful and practical tool for robust estimation: the trimmed mean. By calculating it for a sample of liver enzyme levels containing clear outliers, you will see how it provides a more stable estimate of central tendency, striking a balance between the sensitivity of the mean and the extreme robustness of the median.", "problem": "A clinical laboratory study records alanine aminotransferase (ALT) levels in units per liter (U/L) for a cohort of $n=20$ adult participants. Due to real-world data collection, the sample contains gross outliers. The observed ALT values are:\n$$12,\\ 18,\\ 22,\\ 25,\\ 27,\\ 29,\\ 31,\\ 33,\\ 35,\\ 36,\\ 38,\\ 40,\\ 42,\\ 45,\\ 47,\\ 50,\\ 52,\\ 55,\\ 310,\\ 420.$$\nUsing the definition of an $\\alpha$-trimmed mean in terms of order statistics, compute the $20\\%$-trimmed mean of this sample. Then, conceptually compare it to the ordinary sample mean $\\bar{X}_n$ in terms of bias-variance tradeoffs for summarizing the central tendency in the presence of contamination. Round your computed $20\\%$-trimmed mean to four significant figures and express it in $\\text{U/L}$. The final reported answer must be the single value of the $20\\%$-trimmed mean.", "solution": "The problem is deemed valid as it is scientifically grounded in biostatistics, well-posed with all necessary data provided, and objective in its formulation. It presents a standard task of computing a robust measure of central tendency and conceptually comparing it to a classical estimator.\n\nThe problem requires the computation of the $20\\%$-trimmed mean for a given sample of alanine aminotransferase (ALT) levels. The sample size is $n=20$, and the trimming proportion is $\\alpha = 0.20$.\n\nThe data provided are:\n$$12,\\ 18,\\ 22,\\ 25,\\ 27,\\ 29,\\ 31,\\ 33,\\ 35,\\ 36,\\ 38,\\ 40,\\ 42,\\ 45,\\ 47,\\ 50,\\ 52,\\ 55,\\ 310,\\ 420.$$\nThese values are already sorted in non-decreasing order, so they represent the order statistics, $X_{(1)}, X_{(2)}, \\ldots, X_{(20)}$.\n\nThe $\\alpha$-trimmed mean, denoted as $\\bar{X}_{\\alpha}$, is defined by removing a proportion $\\alpha$ of the observations from both the lower and upper tails of the sorted sample and then computing the arithmetic mean of the remaining data. The number of observations to be trimmed from each end, $k$, is given by $k = \\lfloor n\\alpha \\rfloor$, where $\\lfloor \\cdot \\rfloor$ is the floor function.\n\nFor this problem, we have:\n- Sample size, $n = 20$\n- Trimming proportion, $\\alpha = 0.20$\n\nThe number of observations to trim from each end is:\n$$ k = \\lfloor n\\alpha \\rfloor = \\lfloor 20 \\times 0.20 \\rfloor = \\lfloor 4 \\rfloor = 4 $$\nThis means we must remove the $4$ smallest values and the $4$ largest values from the sample.\n\nThe order statistics are $X_{(1)}, X_{(2)}, \\ldots, X_{(20)}$. We trim $X_{(1)}, \\ldots, X_{(4)}$ from the bottom and $X_{(17)}, \\ldots, X_{(20)}$ from the top.\nThe trimmed observations are:\n- Lower tail: $X_{(1)}=12,\\ X_{(2)}=18,\\ X_{(3)}=22,\\ X_{(4)}=25$\n- Upper tail: $X_{(17)}=52,\\ X_{(18)}=55,\\ X_{(19)}=310,\\ X_{(20)}=420$\n\nThe formula for the $\\alpha$-trimmed mean is:\n$$ \\bar{X}_{\\alpha} = \\frac{1}{n - 2k} \\sum_{i=k+1}^{n-k} X_{(i)} $$\nIn our case, this becomes:\n$$ \\bar{X}_{0.20} = \\frac{1}{20 - 2(4)} \\sum_{i=4+1}^{20-4} X_{(i)} = \\frac{1}{12} \\sum_{i=5}^{16} X_{(i)} $$\n\nThe remaining observations to be averaged are:\n$$ X_{(5)}=27,\\ X_{(6)}=29,\\ X_{(7)}=31,\\ X_{(8)}=33,\\ X_{(9)}=35,\\ X_{(10)}=36,\\ X_{(11)}=38,\\ X_{(12)}=40,\\ X_{(13)}=42,\\ X_{(14)}=45,\\ X_{(15)}=47,\\ X_{(16)}=50 $$\nThe sum of these observations is:\n$$ \\sum_{i=5}^{16} X_{(i)} = 27 + 29 + 31 + 33 + 35 + 36 + 38 + 40 + 42 + 45 + 47 + 50 = 453 $$\nThe number of observations in this sum is $n - 2k = 20 - 8 = 12$.\n\nNow, we compute the $20\\%$-trimmed mean:\n$$ \\bar{X}_{0.20} = \\frac{453}{12} = 37.75 $$\nThe problem requires rounding to four significant figures. The value $37.75$ already has four significant figures. Therefore, the $20\\%$-trimmed mean is $37.75 \\text{ U/L}$.\n\nFor the conceptual comparison, we first calculate the ordinary sample mean, $\\bar{X}_n$:\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{20} \\sum_{i=1}^{20} X_{(i)} $$\nThe sum of all observations is:\n$$ \\sum_{i=1}^{20} X_{(i)} = (12+18+22+25) + 453 + (52+55+310+420) = 77 + 453 + 837 = 1367 $$\n$$ \\bar{X}_{20} = \\frac{1367}{20} = 68.35 $$\nThe ordinary sample mean is $68.35 \\text{ U/L}$. This value is heavily influenced by the two extreme outliers ($310$ and $420$) and is larger than all but these two values, making it a poor measure of the central tendency for this dataset.\n\nThe bias-variance tradeoff comparison is as follows:\n- **Sample Mean ($\\bar{X}_n$)**: This estimator is the uniformly minimum variance unbiased estimator (UMVUE) for the mean of a normal distribution. It has the highest possible statistical efficiency (lowest variance) under this ideal assumption. However, its breakdown point is $0$, meaning a single arbitrarily large outlier can corrupt the estimate to an arbitrary degree. In the presence of contamination, as in this problem, the sample mean can become highly biased and its sampling variance can inflate dramatically. It is not a robust estimator.\n- **Trimmed Mean ($\\bar{X}_{\\alpha}$)**: This is a robust estimator. By discarding a fraction of the extreme values, it protects the estimate from the influence of outliers.\n    - **Bias**: If the underlying true distribution is symmetric, the trimmed mean is an unbiased estimator of the center of symmetry. If the distribution is skewed, the trimmed mean will be biased, but this bias is often smaller than the bias induced in the sample mean by a few extreme outliers. It provides a better estimate of the central tendency of the main part of the distribution.\n    - **Variance**: For data from a perfect normal distribution, the trimmed mean has a slightly higher variance than the sample mean, representing a loss of efficiency. This is the \"price\" of robustness. However, for data from heavy-tailed distributions or, as in this case, a distribution contaminated with outliers, the trimmed mean has a much lower variance than the sample mean.\nIn summary, the trimmed mean sacrifices some efficiency under ideal conditions to gain substantial robustness (lower bias and lower variance) in the more realistic scenario of contaminated or non-normal data. For the given ALT data, the trimmed mean of $37.75 \\text{ U/L}$ is a far more representative summary of the central value than the sample mean of $68.35 \\text{ U/L}$.\n\nThe final requested answer is the numerical value of the $20\\%$-trimmed mean.\n$$ \\bar{X}_{0.20} = 37.75 $$", "answer": "$$\\boxed{37.75}$$", "id": "4955542"}, {"introduction": "Not all biological processes are additive; many, like gene expression or biomarker fold-changes, are inherently multiplicative. In these situations, summarizing data with the standard arithmetic mean is conceptually flawed. This practice [@problem_id:4955574] guides you through the process of deriving a more appropriate measure of central tendency—the geometric mean—directly from a multiplicative error model. This exercise highlights a crucial principle: the choice of a summary statistic should be guided by an understanding of the underlying data-generating process.", "problem": "A laboratory quantifies the fold-change in a serum biomarker measured post-intervention relative to baseline for $n$ patients, where each fold-change $X_i$ is strictly positive. Under a biologically plausible multiplicative error model for fold-changes, each observation satisfies $X_i = \\theta \\cdot U_i$ with $U_i  0$, and the natural logarithm satisfies $\\mathbb{E}[\\ln(U_i)] = 0$. Using only core definitions and properties of averages and transformations, derive from first principles a single-number summary of central tendency for $\\{X_i\\}_{i=1}^{n}$ that is appropriate for multiplicative effects, compute it for the data below, and also compute the arithmetic mean of $\\{X_i\\}_{i=1}^{n}$. Finally, report the ratio of the arithmetic mean to the multiplicative-summary value for the following observed fold-changes:\n$$\n0.62,\\; 0.75,\\; 0.88,\\; 1.04,\\; 1.22,\\; 1.35,\\; 1.60,\\; 1.90,\\; 2.10,\\; 2.50.\n$$\nRound your final reported ratio to four significant figures. Express the final answer as a unitless decimal.", "solution": "The problem requires the derivation of a central tendency summary appropriate for data following a multiplicative error model, its calculation for a given dataset, the calculation of the arithmetic mean for the same dataset, and finally the ratio of these two summary statistics.\n\n**Step 1: Validation of the Problem Statement**\n\nThe problem is subjected to a rigorous validation procedure before any attempt at a solution.\n\n**1.1. Extraction of Givens:**\n- A set of $n$ fold-change measurements $\\{X_i\\}_{i=1}^{n}$, where each $X_i  0$.\n- A multiplicative error model: $X_i = \\theta \\cdot U_i$, where $\\theta$ is the parameter of central tendency and $U_i  0$ is a multiplicative error term.\n- A property of the error term: $\\mathbb{E}[\\ln(U_i)] = 0$.\n- A specific dataset of $n=10$ observations: $\\{0.62, 0.75, 0.88, 1.04, 1.22, 1.35, 1.60, 1.90, 2.10, 2.50\\}$.\n- An instruction to round the final ratio to four significant figures.\n\n**1.2. Validation against Criteria:**\n- **Scientific Grounding:** The problem is scientifically sound. Multiplicative models and log-transformations are standard and fundamental tools in biostatistics for analyzing data such as concentrations, fold-changes, or gene expression levels, which are often right-skewed and heteroscedastic. The assumption $\\mathbb{E}[\\ln(U_i)] = 0$ is a standard way to formalize that the error term is centered (in the multiplicative sense) around $1$.\n- **Well-Posedness:** The problem is well-posed. It provides all necessary information and a clear objective: derive an estimator for $\\theta$ based on the model, compute it, compute the arithmetic mean, and find their ratio. The existence and uniqueness of the solution are guaranteed.\n- **Objectivity:** The problem is stated in precise, objective mathematical and statistical language.\n- **Conclusion:** The problem statement is valid as it is scientifically grounded, well-posed, objective, and contains no discernible flaws.\n\n**Step 2: Derivation of the Multiplicative Summary Statistic**\n\nThe goal is to find a suitable estimator for the parameter $\\theta$ from the given model, $X_i = \\theta \\cdot U_i$. The structure of the model, involving multiplication, suggests a logarithmic transformation. Applying the natural logarithm to both sides of the model equation yields:\n$$\n\\ln(X_i) = \\ln(\\theta \\cdot U_i) = \\ln(\\theta) + \\ln(U_i)\n$$\nThis transformation converts the multiplicative relationship into an additive one. Let us define new variables for clarity: $Y_i = \\ln(X_i)$, $\\mu = \\ln(\\theta)$, and $\\epsilon_i = \\ln(U_i)$. The model in terms of these new variables becomes:\n$$\nY_i = \\mu + \\epsilon_i\n$$\nThis is a standard additive error model. The problem states that $\\mathbb{E}[\\ln(U_i)] = \\mathbb{E}[\\epsilon_i] = 0$. We can now find the expected value of our log-transformed data, $Y_i$:\n$$\n\\mathbb{E}[Y_i] = \\mathbb{E}[\\mu + \\epsilon_i] = \\mathbb{E}[\\mu] + \\mathbb{E}[\\epsilon_i]\n$$\nSince $\\mu$ is a constant, $\\mathbb{E}[\\mu] = \\mu$. Thus,\n$$\n\\mathbb{E}[Y_i] = \\mu + 0 = \\mu\n$$\nThis shows that $\\mu = \\ln(\\theta)$ is the population mean of the log-transformed observations. A natural and unbiased estimator for a population mean is the sample mean. Therefore, an estimator for $\\mu$, denoted $\\hat{\\mu}$, is the average of the observed $Y_i$ values:\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\frac{1}{n} \\sum_{i=1}^{n} \\ln(X_i)\n$$\nOur objective is to estimate $\\theta$, not $\\mu$. Since $\\mu = \\ln(\\theta)$, we can recover $\\theta$ by exponentiation: $\\theta = \\exp(\\mu)$. Applying this back-transformation to our estimator $\\hat{\\mu}$ gives us the estimator for $\\theta$, denoted $\\hat{\\theta}$:\n$$\n\\hat{\\theta} = \\exp(\\hat{\\mu}) = \\exp\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\ln(X_i)\\right)\n$$\nUsing the properties of logarithms, where $\\sum \\ln(a_i) = \\ln(\\prod a_i)$ and $c \\ln(a) = \\ln(a^c)$, we can rewrite the expression:\n$$\n\\hat{\\theta} = \\exp\\left( \\frac{1}{n} \\ln\\left(\\prod_{i=1}^{n} X_i\\right) \\right) = \\exp\\left( \\ln\\left[ \\left(\\prod_{i=1}^{n} X_i\\right)^{1/n} \\right] \\right)\n$$\nSince the exponential and natural logarithm are inverse functions, this simplifies to:\n$$\n\\hat{\\theta} = \\left(\\prod_{i=1}^{n} X_i\\right)^{1/n}\n$$\nThis is the definition of the geometric mean. Therefore, the geometric mean is the appropriate single-number summary of central tendency for this multiplicative model. We denote this as $G$.\n\n**Step 3: Calculation of the Geometric and Arithmetic Means**\n\nThe provided dataset is $\\{0.62, 0.75, 0.88, 1.04, 1.22, 1.35, 1.60, 1.90, 2.10, 2.50\\}$. The number of observations is $n=10$.\n\nFirst, we compute the geometric mean, $G$. For numerical stability, it is best to use the logarithmic form:\n$$\n\\ln(G) = \\frac{1}{10} \\sum_{i=1}^{10} \\ln(X_i)\n$$\nThe sum of the natural logarithms of the data is:\n$$\n\\sum_{i=1}^{10} \\ln(X_i) = \\ln(0.62) + \\ln(0.75) + \\ln(0.88) + \\ln(1.04) + \\ln(1.22) + \\ln(1.35) + \\ln(1.60) + \\ln(1.90) + \\ln(2.10) + \\ln(2.50)\n$$\n$$\n\\sum_{i=1}^{10} \\ln(X_i) \\approx -0.478036 - 0.287682 - 0.127833 + 0.039221 + 0.198851 + 0.300105 + 0.470004 + 0.641854 + 0.741937 + 0.916291 \\approx 2.414712\n$$\n$$\n\\ln(G) \\approx \\frac{2.414712}{10} = 0.2414712\n$$\nNow, we find $G$ by exponentiating:\n$$\nG = \\exp(\\ln(G)) \\approx \\exp(0.2414712) \\approx 1.273087\n$$\nNext, we compute the arithmetic mean, $A$:\n$$\nA = \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\frac{1}{10} \\sum_{i=1}^{10} X_i\n$$\nThe sum of the data is:\n$$\n\\sum_{i=1}^{10} X_i = 0.62 + 0.75 + 0.88 + 1.04 + 1.22 + 1.35 + 1.60 + 1.90 + 2.10 + 2.50 = 13.96\n$$\n$$\nA = \\frac{13.96}{10} = 1.396\n$$\n\n**Step 4: Calculation of the Ratio**\n\nThe problem asks for the ratio of the arithmetic mean to the multiplicative-summary value (the geometric mean). Let this ratio be $R$.\n$$\nR = \\frac{A}{G}\n$$\nSubstituting the computed values:\n$$\nR = \\frac{1.396}{1.273087} \\approx 1.096545\n$$\nThe problem requires this ratio to be rounded to four significant figures. The first four significant figures are $1$, $0$, $9$, $6$. The fifth digit is $5$, so we round the fourth digit up.\n$$\nR \\approx 1.097\n$$\nAs a check, the AM-GM inequality states that the arithmetic mean of a set of non-negative real numbers is greater than or equal to their geometric mean. Since our data points are not all equal, we expect $A > G$, which implies $R > 1$. Our result $R \\approx 1.097$ is consistent with this principle.", "answer": "$$\\boxed{1.097}$$", "id": "4955574"}]}