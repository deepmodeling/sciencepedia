{"hands_on_practices": [{"introduction": "In biostatistics, we often need to combine data from different groups or studies, such as pooling results from two different clinical cohorts. A common mistake is to simply average the means of the individual groups, a practice that is only valid if the groups are of equal size. This exercise [@problem_id:1934455] guides you through the derivation of the correct formula for a combined mean, revealing it to be a weighted average that properly accounts for the sample size of each subgroup.", "problem": "A financial analyst is studying the performance of two distinct investment portfolios, Portfolio A and Portfolio B. For Portfolio A, the analyst examines a sample of $n_A$ individual stocks and finds their average annual return to be $\\bar{r}_A$. Similarly, for Portfolio B, a different sample of $n_B$ individual stocks has an average annual return of $\\bar{r}_B$. The analyst now wishes to compute the overall average annual return for the combined group of all stocks from both portfolios.\n\nAssuming the two portfolios contain completely different sets of stocks, derive a general algebraic expression for the combined average annual return, $\\bar{r}_c$, in terms of the sample sizes $n_A$ and $n_B$, and the sample means $\\bar{r}_A$ and $\\bar{r}_B$.", "solution": "Let the individual annual returns in Portfolio A be denoted by $r_{A,i}$ for $i=1,2,\\dots,n_{A}$, and in Portfolio B by $r_{B,j}$ for $j=1,2,\\dots,n_{B}$. By the definition of the sample mean, the average annual return for Portfolio A is\n$$\n\\bar{r}_{A} = \\frac{1}{n_{A}} \\sum_{i=1}^{n_{A}} r_{A,i},\n$$\nwhich implies\n$$\n\\sum_{i=1}^{n_{A}} r_{A,i} = n_{A} \\bar{r}_{A}.\n$$\nSimilarly, for Portfolio B,\n$$\n\\bar{r}_{B} = \\frac{1}{n_{B}} \\sum_{j=1}^{n_{B}} r_{B,j}\n\\quad \\Rightarrow \\quad\n\\sum_{j=1}^{n_{B}} r_{B,j} = n_{B} \\bar{r}_{B}.\n$$\nSince the two portfolios contain disjoint sets of stocks, the combined group has $n_{A} + n_{B}$ stocks, and the total sum of their returns is the sum of the two separate totals:\n$$\nS_{c} = \\sum_{i=1}^{n_{A}} r_{A,i} + \\sum_{j=1}^{n_{B}} r_{B,j} = n_{A} \\bar{r}_{A} + n_{B} \\bar{r}_{B}.\n$$\nBy the definition of a sample mean over the combined group, the combined average annual return is the total sum divided by the total number of stocks:\n$$\n\\bar{r}_{c} = \\frac{S_{c}}{n_{A} + n_{B}} = \\frac{n_{A} \\bar{r}_{A} + n_{B} \\bar{r}_{B}}{n_{A} + n_{B}}.\n$$\nThis is the weighted average of the two portfolio means with weights $n_{A}$ and $n_{B}$, respectively.", "answer": "$$\\boxed{\\frac{n_{A}\\bar{r}_{A}+n_{B}\\bar{r}_{B}}{n_{A}+n_{B}}}$$", "id": "1934455"}, {"introduction": "Modern scientific instruments can generate data continuously, creating massive datasets that are inefficient or impossible to store fully in memory. How can we calculate a running statistical summary, like the mean, without reprocessing the entire dataset with each new observation? This practice problem [@problem_id:1934443] challenges you to think like a computational statistician and derive an elegant recursive formula that updates the mean in a single pass, using only the previous mean, the new data point, and the current count of observations.", "problem": "An onboard computer for a deep-space probe is tasked with analyzing the flux of high-energy particles. The probe's sensor generates a stream of measurements, $x_1, x_2, x_3, \\dots$, where $x_k$ is the measurement taken at time step $k$. Due to severe memory constraints, the computer cannot store the entire history of measurements. Instead, it must compute the running arithmetic mean in a single pass.\n\nLet $\\mu_n$ be the arithmetic mean of the first $n$ measurements, i.e., $\\mu_n = \\frac{1}{n}\\sum_{i=1}^{n} x_i$. When a new measurement $x_n$ arrives (for $n > 1$), the computer updates the previous mean $\\mu_{n-1}$ to the new mean $\\mu_n$. This update rule can be formulated as a linear combination of the previous mean and the new data point:\n$$ \\mu_n = A_n \\mu_{n-1} + B_n x_n $$\nwhere the coefficients $A_n$ and $B_n$ are functions of the number of measurements, $n$.\n\nDetermine the expression for the coefficient $B_n$ as a function of $n$.", "solution": "We start from the definition of the running mean of the first $n$ measurements:\n$$\n\\mu_{n}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}.\n$$\nSplit the sum into the first $n-1$ terms and the $n$th term:\n$$\n\\mu_{n}=\\frac{1}{n}\\left(\\sum_{i=1}^{n-1}x_{i}+x_{n}\\right).\n$$\nExpress the partial sum $\\sum_{i=1}^{n-1}x_{i}$ in terms of $\\mu_{n-1}$ using the definition $\\mu_{n-1}=\\frac{1}{n-1}\\sum_{i=1}^{n-1}x_{i}$, which implies\n$$\n\\sum_{i=1}^{n-1}x_{i}=(n-1)\\mu_{n-1}.\n$$\nSubstitute this into the expression for $\\mu_{n}$:\n$$\n\\mu_{n}=\\frac{1}{n}\\left((n-1)\\mu_{n-1}+x_{n}\\right)=\\frac{n-1}{n}\\mu_{n-1}+\\frac{1}{n}x_{n}.\n$$\nComparing with the update form $\\mu_{n}=A_{n}\\mu_{n-1}+B_{n}x_{n}$, we identify\n$$\nB_{n}=\\frac{1}{n}.\n$$", "answer": "$$\\boxed{\\frac{1}{n}}$$", "id": "1934443"}, {"introduction": "The arithmetic mean is a cornerstone of statistics, but it has a significant weakness: its sensitivity to extreme values, or outliers. In biological and clinical data, such outliers are common and can skew our understanding of a typical value. This hands-on practice [@problem_id:4811621] provides a direct comparison between the mean and more robust measures of central tendency—like the median, the trimmed mean, and the Winsorized mean—demonstrating their critical role in providing a stable and representative summary of data in the presence of contamination.", "problem": "A small pilot in a hospital laboratory evaluates a new automated assay for C-reactive protein. For a convenience sample of $n=7$ patients, the first run of the new assay yields the following values (in $\\mathrm{mg/L}$): $\\{2, 3, 3, 4, 4, 5, 50\\}$. A senior biostatistician notes that the value $50$ may reflect an instrument artifact. You are asked to quantify the center of this distribution using several measures that differ in robustness to contamination, starting only from core definitions: the sample mean (arithmetic average), the sample median (middle order statistic), the $\\gamma$-trimmed mean with $\\gamma=0.2$ (discarding equal fractions from both tails before averaging), and the $\\gamma$-Winsorized mean with $\\gamma=0.2$ (replacing the lowest and highest fractions by the nearest remaining values before averaging).\n\nUsing only these core definitions and first principles of order statistics, proceed as follows for the given dataset: compute the sample mean, the sample median, the $0.2$-trimmed mean, and the $0.2$-Winsorized mean. Then, compare these four summaries in terms of qualitative robustness, referring to breakdown under extreme contamination and whether an infinitesimal contamination at an extreme value can exert unbounded effect on the estimator.\n\nAs your final numerical answer, report the $0.2$-trimmed mean for this dataset. Round your final answer to four significant figures and express it in $\\mathrm{mg/L}$.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data and definitions from the field of statistics to calculate specified measures of central tendency and evaluate their robustness.\n\nThe initial step is to formalize the problem by defining the dataset and its order statistics. The sample size is $n=7$. The given dataset of C-reactive protein values is $X = \\{2, 3, 3, 4, 4, 5, 50\\}$. To compute the order-based statistics (median, trimmed mean, Winsorized mean), we must first sort the data in ascending order. Let $X_{(i)}$ denote the $i$-th order statistic.\n\nThe ordered dataset is:\n$X_{(1)} = 2$\n$X_{(2)} = 3$\n$X_{(3)} = 3$\n$X_{(4)} = 4$\n$X_{(5)} = 4$\n$X_{(6)} = 5$\n$X_{(7)} = 50$\n\nWe will now compute the four requested measures of central tendency.\n\n1.  **Sample Mean ($\\bar{x}$)**\n    The sample mean is the arithmetic average of all observations. Its definition is $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$.\n    For the given data:\n    $$ \\bar{x} = \\frac{1}{7} (2 + 3 + 3 + 4 + 4 + 5 + 50) = \\frac{71}{7} \\approx 10.1429 $$\n\n2.  **Sample Median ($\\tilde{x}$)**\n    The sample median is the value separating the higher half from the lower half of a data sample. For an odd sample size $n$, the median is the middle order statistic, $X_{((n+1)/2)}$.\n    For $n=7$, the median is the $((7+1)/2)$-th value, which is the $4$-th order statistic, $X_{(4)}$.\n    $$ \\tilde{x} = X_{(4)} = 4 $$\n\n3.  **$\\gamma$-Trimmed Mean ($\\bar{x}_{\\gamma}$)**\n    The $\\gamma$-trimmed mean is calculated by discarding a proportion $\\gamma$ of the lowest and highest values from the ordered data and then computing the mean of the remaining data. The number of observations to be trimmed from each end is $k = \\lfloor n\\gamma \\rfloor$.\n    Given $\\gamma=0.2$ and $n=7$:\n    $$ k = \\lfloor 7 \\times 0.2 \\rfloor = \\lfloor 1.4 \\rfloor = 1 $$\n    This means we must discard the single smallest value ($X_{(1)}=2$) and the single largest value ($X_{(7)}=50$). The remaining data points are $\\{X_{(2)}, X_{(3)}, X_{(4)}, X_{(5)}, X_{(6)}\\} = \\{3, 3, 4, 4, 5\\}$.\n    The number of remaining observations is $n - 2k = 7 - 2(1) = 5$.\n    The $0.2$-trimmed mean, $\\bar{x}_{0.2}$, is the average of these $5$ values:\n    $$ \\bar{x}_{0.2} = \\frac{1}{5} (3 + 3 + 4 + 4 + 5) = \\frac{19}{5} = 3.8 $$\n\n4.  **$\\gamma$-Winsorized Mean ($\\bar{x}_{W,\\gamma}$)**\n    The $\\gamma$-Winsorized mean is calculated by replacing the $k = \\lfloor n\\gamma \\rfloor$ smallest values with the $(k+1)$-th smallest value, and the $k$ largest values with the $(n-k)$-th largest value. The mean is then computed on this new, modified dataset.\n    As before, for $\\gamma=0.2$ and $n=7$, we have $k=1$.\n    The smallest value, $X_{(1)}=2$, is replaced by the $(1+1)$-th value, $X_{(2)}=3$.\n    The largest value, $X_{(7)}=50$, is replaced by the $(7-1)$-th value, $X_{(6)}=5$.\n    The original ordered set $\\{2, 3, 3, 4, 4, 5, 50\\}$ is transformed into the Winsorized set $\\{3, 3, 3, 4, 4, 5, 5\\}$.\n    The $0.2$-Winsorized mean, $\\bar{x}_{W,0.2}$, is the average of this new set of $n=7$ values:\n    $$ \\bar{x}_{W,0.2} = \\frac{1}{7} (3 + 3 + 3 + 4 + 4 + 5 + 5) = \\frac{27}{7} \\approx 3.8571 $$\n\n**Qualitative Comparison of Robustness**\n\nRobustness refers to an estimator's resistance to being unduly affected by outliers or deviations from model assumptions. We can compare the four estimators based on their breakdown point and the influence of extreme values.\n\n-   **Sample Mean**: The mean is not a robust estimator. Its breakdown point is $1/n$, which approaches $0$ for large samples. This means a single arbitrarily large observation can move the mean to any value. In our example, the single outlier $50$ pulls the mean ($\\approx 10.14$) far away from the central cluster of data points (which are all $\\leq 5$). An infinitesimal contamination at an extreme value has an unbounded effect, as its influence function is linear and unbounded.\n\n-   **Sample Median**: The median is a highly robust estimator. Its breakdown point is approximately $50\\%$, meaning that nearly half the data must be contaminated to make the median arbitrarily large or small. Its influence is bounded; an infinitesimal change to an extreme value has zero effect on the median, as long as that value does not cross the median itself. In our sample, the median is $4$, which perfectly reflects the center of the non-outlying data and is completely insensitive to the value $50$.\n\n-   **Trimmed Mean**: The trimmed mean is a robust estimator, with its robustness controlled by the trimming proportion $\\gamma$. Its breakdown point is equal to $\\gamma$. For $\\gamma=0.2$, its breakdown point is $20\\%$. Extreme values outside the trimming range are completely discarded, so their influence on the estimator is zero. The resulting estimator, $\\bar{x}_{0.2}=3.8$, is very close to the median and provides a stable estimate of the center, unaffected by the outlier.\n\n-   **Winsorized Mean**: The Winsorized mean is also a robust estimator with a breakdown point of $\\gamma=0.2$. Instead of discarding extreme values, it replaces them with the nearest retained values, thus preserving the sample size. This often results in higher statistical efficiency than the trimmed mean if the underlying distribution has heavy tails. Its influence on extreme values is bounded—the value of an outlier only matters insofar as it identifies itself as being in the tail to be replaced. The resulting estimate, $\\bar{x}_{W,0.2} \\approx 3.86$, is also stable and close to the median and trimmed mean.\n\nIn summary, the robustness ranking is: Median $>$ Trimmed/Winsorized Mean $>$ Sample Mean. The mean is highly sensitive to the outlier $50$, while the median, trimmed mean, and Winsorized mean all provide stable estimates of the central tendency around $4$.\n\nThe final answer requested is the $0.2$-trimmed mean, rounded to four significant figures.\nCalculated value: $\\bar{x}_{0.2} = 3.8$.\nTo four significant figures, this is written as $3.800$.", "answer": "$$ \\boxed{3.800} $$", "id": "4811621"}]}