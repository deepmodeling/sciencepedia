{"hands_on_practices": [{"introduction": "An AI model's raw performance metrics, such as sensitivity and specificity, can be deceiving without clinical context. This first practice explores the critical role of disease prevalence in determining a test's real-world predictive value, a concept essential for ethical patient communication and avoiding harm from over-testing or misdiagnosis. By calculating the Positive and Negative Predictive Values (PPV and NPV), you will gain a foundational understanding of how to interpret AI diagnostic outputs in a clinically meaningful and ethically responsible way [@problem_id:4850196].", "problem": "An Artificial Intelligence (AI) diagnostic classifier is deployed in a primary care setting to screen asymptomatic adults for a serious but uncommon condition. The classifier outputs a binary result, positive or negative. On prospective validation, the classifier’s sensitivity is $0.90$, its specificity is $0.95$, and the condition’s prevalence in the screened population is $0.01$. Positive Predictive Value (PPV) is defined as the probability that a patient has the condition given a positive result, and Negative Predictive Value (NPV) is defined as the probability that a patient does not have the condition given a negative result. Using only fundamental probability definitions, the law of total probability, and Bayes’ theorem, derive the expressions for PPV and NPV in terms of sensitivity, specificity, and prevalence, and then compute their numerical values for the given setting. Round both PPV and NPV to four significant figures and express them as decimals. Based on your computed values and the ethical principles of beneficence, nonmaleficence, justice, and respect for persons, reason whether positive AI results should be followed by a confirmatory gold-standard test before initiating treatment, noting the ethical trade-offs in low-prevalence screening. Provide your final numeric answer as the ordered pair $\\left(\\text{PPV}, \\text{NPV}\\right)$.", "solution": "The problem is assessed to be valid as it is scientifically grounded, self-contained, well-posed, and objective. It presents a standard biostatistical problem with a subsequent ethical analysis based on established principles.\n\nFirst, we define the events and translate the given information into probabilistic notation. Let $D$ be the event that a patient has the condition, and $D^c$ be the event that the patient does not have the condition. Let $T^+$ be the event of a positive test result and $T^-$ be the event of a negative test result.\n\nThe given parameters are:\n- Prevalence, $p = P(D) = 0.01$. Consequently, the probability of not having the condition is $P(D^c) = 1 - p = 1 - 0.01 = 0.99$.\n- Sensitivity, $\\text{sens} = P(T^+ | D) = 0.90$. This is the true positive rate.\n- Specificity, $\\text{spec} = P(T^- | D^c) = 0.95$. This is the true negative rate.\n\nFrom these, we can derive the probabilities of test errors:\n- False negative rate, $P(T^- | D) = 1 - P(T^+ | D) = 1 - \\text{sens} = 1 - 0.90 = 0.10$.\n- False positive rate, $P(T^+ | D^c) = 1 - P(T^- | D^c) = 1 - \\text{spec} = 1 - 0.95 = 0.05$.\n\nThe problem asks for the derivation and computation of the Positive Predictive Value (PPV) and the Negative Predictive Value (NPV).\n\n**Positive Predictive Value (PPV)**\nThe PPV is the probability that a patient has the condition given a positive test result, which is denoted as $P(D | T^+)$. We use Bayes' theorem to derive the expression for PPV:\n$$\n\\text{PPV} = P(D | T^+) = \\frac{P(T^+ | D) P(D)}{P(T^+)}\n$$\nThe denominator, $P(T^+)$, is the total probability of a positive test result. We can expand it using the law of total probability:\n$$\nP(T^+) = P(T^+ | D) P(D) + P(T^+ | D^c) P(D^c)\n$$\nSubstituting the terms with sensitivity, specificity, and prevalence:\n$$\nP(T^+) = (\\text{sens})(p) + (1 - \\text{spec})(1 - p)\n$$\nTherefore, the full expression for PPV in terms of the given parameters is:\n$$\n\\text{PPV} = \\frac{(\\text{sens})(p)}{(\\text{sens})(p) + (1 - \\text{spec})(1 - p)}\n$$\nSubstituting the numerical values:\n$$\n\\text{PPV} = \\frac{(0.90)(0.01)}{(0.90)(0.01) + (1 - 0.95)(1 - 0.01)} = \\frac{0.009}{0.009 + (0.05)(0.99)} = \\frac{0.009}{0.009 + 0.0495} = \\frac{0.009}{0.0585} \\approx 0.153846...\n$$\nRounding to four significant figures, the PPV is $0.1538$.\n\n**Negative Predictive Value (NPV)**\nThe NPV is the probability that a patient does not have the condition given a negative test result, denoted as $P(D^c | T^-)$. Using Bayes' theorem:\n$$\n\\text{NPV} = P(D^c | T^-) = \\frac{P(T^- | D^c) P(D^c)}{P(T^-)}\n$$\nThe denominator, $P(T^-)$, is the total probability of a negative test result. Using the law of total probability:\n$$\nP(T^-) = P(T^- | D) P(D) + P(T^- | D^c) P(D^c)\n$$\nSubstituting the terms with sensitivity, specificity, and prevalence:\n$$\nP(T^-) = (1 - \\text{sens})(p) + (\\text{spec})(1 - p)\n$$\nTherefore, the full expression for NPV is:\n$$\n\\text{NPV} = \\frac{(\\text{spec})(1 - p)}{(\\text{spec})(1 - p) + (1 - \\text{sens})(p)}\n$$\nSubstituting the numerical values:\n$$\n\\text{NPV} = \\frac{(0.95)(1 - 0.01)}{(0.95)(1 - 0.01) + (1 - 0.90)(0.01)} = \\frac{(0.95)(0.99)}{(0.95)(0.99) + (0.10)(0.01)} = \\frac{0.9405}{0.9405 + 0.001} = \\frac{0.9405}{0.9415} \\approx 0.998937...\n$$\nRounding to four significant figures, the NPV is $0.9989$.\n\n**Ethical Reasoning**\nThe computed values are $\\text{PPV} \\approx 0.1538$ and $\\text{NPV} \\approx 0.9989$. The very high NPV indicates that a negative result is highly reliable; the AI is effective at ruling out the condition. However, the PPV is strikingly low. A value of $0.1538$ means that only about $15.4\\%$ of patients with a positive AI result actually have the condition. The remaining $84.6\\%$ are false positives. This has profound ethical implications for clinical practice.\n\n- **Non-maleficence (Do no harm):** To initiate treatment for a serious condition based solely on a positive AI result would mean subjecting a large majority ($84.6\\%$) of those individuals to potentially harmful, costly, and psychologically distressing interventions unnecessarily. This represents a significant violation of the principle to do no harm.\n\n- **Beneficence (Act in the best interest of the patient):** While early detection is beneficial for the true positives, the low probability of a positive test being correct means that acting on it directly is not in the best interest of the overall group of patients with positive results. The potential benefit to the few true positives is outweighed by the certain harm (side effects, cost, anxiety) to the many false positives. True beneficence requires a higher degree of certainty before initiating treatment.\n\n- **Respect for Persons (Autonomy):** Informed consent is a cornerstone of this principle. A patient cannot provide genuine informed consent for treatment if they are not made aware that a \"positive\" result carries only a $15.4\\%$ probability of being correct. To present the AI result as a definitive diagnosis would be misleading and paternalistic, undermining the patient's right to make an autonomous decision based on accurate information.\n\n- **Justice (Fairness):** A screening program that generates a very high number of false positives can lead to an unjust allocation of healthcare resources. The costs of follow-up (confirmatory tests, specialist consultations, and unnecessary treatments) for the large cohort of false positives could strain the system, potentially diverting funds and clinical capacity from patients with more definite and urgent needs.\n\n**Conclusion:** The analysis demonstrates that despite high sensitivity and specificity, the utility of a screening test is profoundly affected by the prevalence of the condition. Given the low PPV, it is ethically imperative that any positive result from this AI classifier is treated as a preliminary finding that mandates follow-up with a confirmatory gold-standard test before any treatment decisions are made. The AI tool is valuable for screening to rule out disease (due to high NPV), not for diagnosing disease on its own.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1538 & 0.9989 \\end{pmatrix}}\n$$", "id": "4850196"}, {"introduction": "Beyond overall accuracy, a core ethical requirement for any medical AI is to function equitably across all patient populations. This exercise introduces a quantitative approach to auditing for algorithmic bias by examining performance disparities between demographic groups. You will apply the fairness criterion of \"equalized odds\" by calculating and comparing group-specific error rates, providing a concrete tool to assess whether an AI system upholds the principle of justice or risks perpetuating health inequities [@problem_id:4850172].", "problem": "An Artificial Intelligence (AI) diagnostic classifier is deployed to support clinical decision-making for a binary condition, where the ground truth label $Y \\in \\{0,1\\}$ indicates absence ($Y=0$) or presence ($Y=1$) of disease, and the classifier output $\\hat{Y} \\in \\{0,1\\}$ indicates predicted absence ($\\hat{Y}=0$) or predicted presence ($\\hat{Y}=1$). Hospital administrators are evaluating whether the system satisfies the fairness criterion commonly referred to as equalized odds, focusing on error parity across two demographic groups, $A$ and $B$. Each group’s performance is summarized by a confusion matrix consisting of true positives ($\\mathrm{TP}$), false positives ($\\mathrm{FP}$), true negatives ($\\mathrm{TN}$), and false negatives ($\\mathrm{FN}$), defined with respect to $Y$ and $\\hat{Y}$.\n\nFor group $A$, the aggregated counts over a recent evaluation period are:\n- $\\mathrm{TP}_A = 425$, $\\mathrm{FN}_A = 75$ among $Y=1$ cases ($425 + 75 = 500$),\n- $\\mathrm{FP}_A = 150$, $\\mathrm{TN}_A = 1350$ among $Y=0$ cases ($150 + 1350 = 1500$).\n\nFor group $B$, the aggregated counts are:\n- $\\mathrm{TP}_B = 480$, $\\mathrm{FN}_B = 120$ among $Y=1$ cases ($480 + 120 = 600$),\n- $\\mathrm{FP}_B = 140$, $\\mathrm{TN}_B = 1260$ among $Y=0$ cases ($140 + 1260 = 1400$).\n\nFrom first principles of probability and classification error definitions, derive the group-specific False Positive Rate (FPR) and False Negative Rate (FNR) for $A$ and $B$. Then compute the absolute differences $|\\mathrm{FPR}_A - \\mathrm{FPR}_B|$ and $|\\mathrm{FNR}_A - \\mathrm{FNR}_B|$. Finally, define the equalized odds violation metric\n$$V = \\max\\left(|\\mathrm{FPR}_A - \\mathrm{FPR}_B|,\\, |\\mathrm{FNR}_A - \\mathrm{FNR}_B|\\right),$$\nand compute $V$. The hospital’s tolerance for equalized odds is $\\tau = 0.02$, meaning equalized odds is considered satisfied if both absolute differences do not exceed $\\tau$; briefly assess whether the system satisfies equalized odds under this tolerance. Express the final metric $V$ as a decimal, rounded to four significant figures. No percent signs are permitted in the answer.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe data and definitions provided in the problem statement are as follows:\n- A binary classification setting with ground truth $Y \\in \\{0, 1\\}$ and classifier prediction $\\hat{Y} \\in \\{0, 1\\}$.\n- Two demographic groups, $A$ and $B$.\n- Performance is summarized by confusion matrix counts: True Positives ($\\mathrm{TP}$), False Positives ($\\mathrm{FP}$), True Negatives ($\\mathrm{TN}$), and False Negatives ($\\mathrm{FN}$).\n\nFor group $A$:\n- $\\mathrm{TP}_A = 425$\n- $\\mathrm{FN}_A = 75$\n- Total cases with $Y=1$ for group $A$ is $425 + 75 = 500$.\n- $\\mathrm{FP}_A = 150$\n- $\\mathrm{TN}_A = 1350$\n- Total cases with $Y=0$ for group $A$ is $150 + 1350 = 1500$.\n\nFor group $B$:\n- $\\mathrm{TP}_B = 480$\n- $\\mathrm{FN}_B = 120$\n- Total cases with $Y=1$ for group $B$ is $480 + 120 = 600$.\n- $\\mathrm{FP}_B = 140$\n- $\\mathrm{TN}_B = 1260$\n- Total cases with $Y=0$ for group $B$ is $140 + 1260 = 1400$.\n\nThe task is to compute the equalized odds violation metric, defined as:\n$$V = \\max\\left(|\\mathrm{FPR}_A - \\mathrm{FPR}_B|,\\, |\\mathrm{FNR}_A - \\mathrm{FNR}_B|\\right)$$\nThe hospital's tolerance for equalized odds is $\\tau = 0.02$. The final value of $V$ should be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it uses standard definitions from machine learning fairness and classification performance evaluation (e.g., FPR, FNR, equalized odds). It is well-posed, providing all necessary numerical data and definitions for a unique solution. The language is objective and quantitative. The provided counts are self-consistent within each group. The problem is a straightforward application of established principles and is neither trivial nor ill-posed.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A complete solution will be provided.\n\n### Solution Derivation\nThe solution proceeds by first defining the required error rates from fundamental principles of probability and classification theory.\n\nThe False Positive Rate ($\\mathrm{FPR}$) is the conditional probability of predicting the positive class ($\\hat{Y}=1$) given that the true class is negative ($Y=0$). It is calculated as the ratio of false positives to the total number of actual negative cases.\n$$ \\mathrm{FPR} = P(\\hat{Y}=1 | Y=0) = \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}} $$\n\nThe False Negative Rate ($\\mathrm{FNR}$) is the conditional probability of predicting the negative class ($\\hat{Y}=0$) given that the true class is positive ($Y=1$). It is calculated as the ratio of false negatives to the total number of actual positive cases.\n$$ \\mathrm{FNR} = P(\\hat{Y}=0 | Y=1) = \\frac{\\mathrm{FN}}{\\mathrm{FN} + \\mathrm{TP}} $$\n\nWe apply these definitions to each group.\n\nFor group $A$:\nThe total number of actual positive cases is $P_A = \\mathrm{TP}_A + \\mathrm{FN}_A = 425 + 75 = 500$.\nThe total number of actual negative cases is $N_A = \\mathrm{FP}_A + \\mathrm{TN}_A = 150 + 1350 = 1500$.\n\nThe False Positive Rate for group $A$ is:\n$$ \\mathrm{FPR}_A = \\frac{\\mathrm{FP}_A}{\\mathrm{FP}_A + \\mathrm{TN}_A} = \\frac{150}{1500} = 0.1 $$\nThe False Negative Rate for group $A$ is:\n$$ \\mathrm{FNR}_A = \\frac{\\mathrm{FN}_A}{\\mathrm{FN}_A + \\mathrm{TP}_A} = \\frac{75}{500} = \\frac{3}{20} = 0.15 $$\n\nFor group $B$:\nThe total number of actual positive cases is $P_B = \\mathrm{TP}_B + \\mathrm{FN}_B = 480 + 120 = 600$.\nThe total number of actual negative cases is $N_B = \\mathrm{FP}_B + \\mathrm{TN}_B = 140 + 1260 = 1400$.\n\nThe False Positive Rate for group $B$ is:\n$$ \\mathrm{FPR}_B = \\frac{\\mathrm{FP}_B}{\\mathrm{FP}_B + \\mathrm{TN}_B} = \\frac{140}{1400} = 0.1 $$\nThe False Negative Rate for group $B$ is:\n$$ \\mathrm{FNR}_B = \\frac{\\mathrm{FN}_B}{\\mathrm{FN}_B + \\mathrm{TP}_B} = \\frac{120}{600} = \\frac{1}{5} = 0.2 $$\n\nThe fairness criterion of \"equalized odds\" requires that the classifier has equal True Positive Rates (TPR) and equal False Positive Rates across groups. Since $\\mathrm{TPR} = 1 - \\mathrm{FNR}$, this is equivalent to requiring $\\mathrm{FNR}_A = \\mathrm{FNR}_B$ and $\\mathrm{FPR}_A = \\mathrm{FPR}_B$.\n\nWe compute the absolute differences in these rates:\n$$ |\\mathrm{FPR}_A - \\mathrm{FPR}_B| = |0.1 - 0.1| = 0 $$\n$$ |\\mathrm{FNR}_A - \\mathrm{FNR}_B| = |0.15 - 0.2| = |-0.05| = 0.05 $$\n\nNext, we compute the violation metric $V$, which is the maximum of these two absolute differences:\n$$ V = \\max\\left(|\\mathrm{FPR}_A - \\mathrm{FPR}_B|, |\\mathrm{FNR}_A - \\mathrm{FNR}_B|\\right) $$\n$$ V = \\max(0, 0.05) = 0.05 $$\n\nThe problem requires the final metric $V$ to be expressed as a decimal rounded to four significant figures.\n$$ V = 0.05000 $$\n\nFinally, we assess whether the system satisfies the hospital's tolerance of $\\tau = 0.02$. The criterion is met if both $|\\mathrm{FPR}_A - \\mathrm{FPR}_B| \\le \\tau$ and $|\\mathrm{FNR}_A - \\mathrm{FNR}_B| \\le \\tau$.\n- The difference in FPR is $0$, which is less than or equal to $0.02$.\n- The difference in FNR is $0.05$, which is greater than $0.02$.\nSince one of the conditions is not met, the system does not satisfy the specified equalized odds tolerance. The primary source of the violation is the disparity in the false negative rate between the two groups.\nThe calculated violation metric quantifies the extent of this failure.", "answer": "$$\n\\boxed{0.05000}\n$$", "id": "4850172"}, {"introduction": "The decision to adopt a new technology involves weighing its benefits against its costs, a process with deep ethical implications. This final practice moves from clinical performance to health-system value, using the tools of cost-effectiveness analysis to evaluate an AI intervention. By calculating the Incremental Cost-Effectiveness Ratio (ICER) and Net Monetary Benefit (NMB), you will learn to make a holistic assessment that crucially incorporates the costs of necessary ethical governance, ensuring that innovation is not only effective but also sustainable and just [@problem_id:4850187].", "problem": "A regional hospital is evaluating whether to adopt an Artificial Intelligence (AI)-augmented diagnostic decision support for early detection and treatment selection in suspected sepsis. Ethical deployment requires ongoing clinical governance, transparency audits, and bias monitoring. The hospital’s health economics team incorporates these oversight costs into the per-patient cost of the AI-augmented pathway to ensure that the evaluation reflects ethically compliant implementation. For a representative patient cohort, the team estimates the following expected lifetime outcomes on a per-patient basis:\n\n- Standard care: expected cost $C_{S} = \\$24{,}100$ and expected Quality-Adjusted Life Years (QALYs) $Q_{S} = 9.42$.\n- AI-augmented care with governance: expected cost $C_{A} = \\$25{,}400$ and expected QALYs $Q_{A} = 9.54$.\n\nQuality-Adjusted Life Year (QALY) is a measure that integrates length and quality of life into a single metric, and cost-effectiveness analysis compares alternatives based on incremental cost per incremental QALY. The health system’s willingness-to-pay (WTP) threshold for one additional QALY is $\\lambda = \\$100{,}000$ per QALY.\n\nUsing only foundational definitions from cost-effectiveness analysis in health economics applied to medical ethics in Artificial Intelligence (AI) deployment—namely, incremental changes in cost and QALYs, the incremental cost-effectiveness ratio (ICER), and the decision rule based on a given willingness-to-pay threshold—do the following:\n\n1. Compute the incremental cost, $\\Delta C$, and the incremental QALYs, $\\Delta Q$, of AI-augmented care relative to standard care.\n2. From first principles, derive and compute the ICER of AI-augmented care relative to standard care.\n3. Using the willingness-to-pay threshold $\\lambda$, assess whether AI-augmented care is acceptable on cost-effectiveness grounds, explicitly interpreting the result in the context of ethically compliant implementation costs being included.\n4. For your final reported value, compute the net monetary benefit (NMB) of adopting AI-augmented care relative to standard care at the threshold $\\lambda$, expressed per patient in thousands of United States dollars (USD thousands). Round your final reported value to four significant figures.\n\nProvide as your final answer only the numerical value of the net monetary benefit in USD thousands, with no units in your answer box.", "solution": "The problem has been validated and determined to be scientifically grounded, well-posed, objective, and self-contained. The data provided are consistent and sufficient for a complete solution. The problem requires the application of fundamental principles of cost-effectiveness analysis from health economics to a medically relevant scenario involving artificial intelligence. We will proceed with a full solution.\n\nThe problem asks for a four-part analysis of an AI-augmented diagnostic pathway compared to standard care for sepsis, based on provided cost and Quality-Adjusted Life Year (QALY) data. The analysis includes calculating incremental cost and effectiveness, the Incremental Cost-Effectiveness Ratio (ICER), a cost-effectiveness assessment, and the Net Monetary Benefit (NMB).\n\nThe givens are:\n- Standard care cost: $C_{S} = \\$24{,}100$\n- Standard care QALYs: $Q_{S} = 9.42$\n- AI-augmented care cost: $C_{A} = \\$25{,}400$\n- AI-augmented care QALYs: $Q_{A} = 9.54$\n- Willingness-to-pay (WTP) threshold: $\\lambda = \\$100{,}000$ per QALY\n\n**1. Incremental Cost and Incremental QALYs**\n\nThe incremental cost, denoted as $\\Delta C$, is the difference in cost between the AI-augmented care pathway and the standard care pathway.\n$$ \\Delta C = C_{A} - C_{S} $$\nSubstituting the given values:\n$$ \\Delta C = \\$25{,}400 - \\$24{,}100 = \\$1{,}300 $$\n\nThe incremental QALYs, denoted as $\\Delta Q$, is the difference in health outcome (QALYs) between the AI-augmented care pathway and the standard care pathway.\n$$ \\Delta Q = Q_{A} - Q_{S} $$\nSubstituting the given values:\n$$ \\Delta Q = 9.54 - 9.42 = 0.12 $$\nThus, the AI-augmented care pathway costs an additional $\\$1{,}300$ per patient to produce an additional $0.12$ QALYs per patient.\n\n**2. Incremental Cost-Effectiveness Ratio (ICER)**\n\nThe Incremental Cost-Effectiveness Ratio (ICER) is defined from first principles as the ratio of the incremental cost to the incremental effectiveness. It represents the additional cost incurred to gain one additional unit of health outcome (in this case, one QALY).\n$$ ICER = \\frac{\\Delta C}{\\Delta Q} $$\nUsing the calculated incremental values:\n$$ ICER = \\frac{\\$1{,}300}{0.12 \\text{ QALYs}} = \\$10{,}833.33... \\text{ per QALY} $$\nFor exactness, we can express this as a fraction:\n$$ ICER = \\frac{1300}{0.12} = \\frac{1300}{\\frac{12}{100}} = \\frac{1300 \\times 100}{12} = \\frac{130000}{12} = \\frac{32500}{3} \\text{ dollars per QALY} $$\n\n**3. Cost-Effectiveness Assessment**\n\nThe decision rule for cost-effectiveness is to compare the ICER to the health system's willingness-to-pay (WTP) threshold, $\\lambda$. An intervention is considered cost-effective if its ICER is less than or equal to the WTP threshold.\n$$ \\text{Decision Rule: Accept if } ICER \\leq \\lambda $$\nIn this case, we have:\n$$ ICER = \\$10{,}833.33... \\text{ per QALY} $$\n$$ \\lambda = \\$100{,}000 \\text{ per QALY} $$\nSince $\\$10{,}833.33... < \\$100{,}000$, the condition $ICER \\leq \\lambda$ is met.\n\nTherefore, the AI-augmented care pathway is considered acceptable on cost-effectiveness grounds. The interpretation is that the additional cost of $\\$10{,}833.33$ required to gain one extra QALY is well below the maximum amount ($\\$100{,}000$) that the health system is willing to pay for that same gain. This conclusion is particularly significant because the costs associated with ethical deployment (governance, audits, bias monitoring) have been explicitly included in $C_A$. This shows that even with these necessary ethical safeguards, the AI technology remains an economically efficient choice.\n\n**4. Net Monetary Benefit (NMB)**\n\nThe Net Monetary Benefit (NMB) quantifies the value of an intervention in monetary terms. The NMB of adopting AI-augmented care relative to standard care (i.e., the incremental NMB) is calculated by monetizing the incremental health gain ($\\Delta Q$) using the WTP threshold ($\\lambda$) and subtracting the incremental cost ($\\Delta C$).\nThe formula for incremental NMB is:\n$$ NMB = (\\lambda \\times \\Delta Q) - \\Delta C $$\nA positive NMB indicates that the intervention is cost-effective at the given WTP threshold.\nSubstituting the values:\n$$ NMB = (\\$100{,}000 \\text{ per QALY} \\times 0.12 \\text{ QALYs}) - \\$1{,}300 $$\n$$ NMB = \\$12{,}000 - \\$1{,}300 $$\n$$ NMB = \\$10{,}700 $$\nThe problem requires the NMB to be expressed per patient in thousands of United States dollars (USD thousands).\n$$ NMB_{\\text{thousands}} = \\frac{\\$10{,}700}{\\$1{,}000} = 10.7 $$\nFinally, this value must be rounded to four significant figures.\n$$ 10.70 $$\nThis positive value confirms the conclusion from the ICER analysis: adopting the AI-augmented pathway provides a net positive monetary value to the health system at its stated WTP threshold, and is therefore the preferred option.", "answer": "$$\n\\boxed{10.70}\n$$", "id": "4850187"}]}