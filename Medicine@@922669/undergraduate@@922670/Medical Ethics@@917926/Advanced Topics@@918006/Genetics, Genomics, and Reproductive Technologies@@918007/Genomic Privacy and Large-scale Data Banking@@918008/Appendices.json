{"hands_on_practices": [{"introduction": "Before we can discuss the ethics of sharing genomic data, we must first understand the fundamental source of the privacy risk. This exercise provides a mathematical framework for quantifying the re-identification risk inherent in genomic datasets. By deriving a formula for the minimum number of genetic markers needed to single out an individual from a large population, you will gain a concrete appreciation for why even supposedly \"de-identified\" data summaries can pose a significant privacy challenge [@problem_id:4863913].", "problem": "A national health agency is evaluating the re-identification risk posed by releasing de-identified genomic summaries derived from a large-scale biobank. For each individual, the summary encodes the presence ($1$) or absence ($0$) of a specified alternate allele at each of $m$ selected single-nucleotide variants (SNVs), where single-nucleotide variant (SNV) denotes a genomic position with two alleles segregating in the population. Assume that, after accounting for genomic correlation due to linkage disequilibrium (LD), only a fraction $\\alpha$ of the $m$ binary indicators behave as statistically independent across the population, with $0<\\alpha\\le 1$ treated as a constant reflecting the effective independence of the panel. The agency wishes to choose $m$ large enough so that, in principle and ignoring measurement error, each of $N$ distinct individuals in the population can be uniquely identified by their binary SNV signature.\n\nStarting only from the rules of counting under independence and the requirement that a unique label must be available for each of the $N$ individuals, derive a closed-form expression for the minimal integer $m_{\\min}(N,\\alpha)$ sufficient to make unique identification possible in principle under worst-case counting. Clearly state the assumptions you require about independence, LD, and allele frequency distributions to justify your derivation, and articulate how violations of these assumptions would change the expression structurally.\n\nExpress your final answer as a single symbolic expression for $m_{\\min}(N,\\alpha)$ using elementary functions, and do not include any units. No numerical rounding is required.", "solution": "The problem requires finding the minimum number of single-nucleotide variants (SNVs), $m$, needed to uniquely identify every individual in a population of size $N$.\n\n1.  **Calculate the Number of Independent Markers**: The problem states that due to linkage disequilibrium, only a fraction $\\alpha$ of the $m$ SNVs are effectively independent. The number of independent binary indicators is therefore $m_{\\text{eff}} = \\alpha m$.\n\n2.  **Calculate the Total Number of Unique Signatures**: Each of the $m_{\\text{eff}}$ independent markers can have one of two states (0 or 1). Based on the fundamental rule of counting, the total number of distinct binary signatures that can be generated from these markers is $K = 2^{m_{\\text{eff}}} = 2^{\\alpha m}$. This calculation represents the \"worst-case counting\" scenario, as it assumes that all combinatorial possibilities are available to create unique labels.\n\n3.  **Set Up the Uniqueness Condition**: To ensure that every individual can, in principle, be assigned a unique signature, the total number of available signatures $K$ must be greater than or equal to the population size $N$.\n    $$ 2^{\\alpha m} \\ge N $$\n\n4.  **Solve for m**: To solve for $m$, we take the base-2 logarithm of both sides:\n    $$ \\log_2(2^{\\alpha m}) \\ge \\log_2(N) $$\n    $$ \\alpha m \\ge \\log_2(N) $$\n    Using the change of base formula for logarithms, $\\log_2(N) = \\frac{\\ln(N)}{\\ln(2)}$, we get:\n    $$ m \\ge \\frac{\\ln(N)}{\\alpha \\ln(2)} $$\n\n5.  **Determine the Minimal Integer Value**: The problem asks for the minimal *integer* $m$ that satisfies this condition. This is found by taking the ceiling of the expression on the right-hand side.\n    $$ m_{\\min}(N, \\alpha) = \\left\\lceil \\frac{\\ln(N)}{\\alpha \\ln(2)} \\right\\rceil $$\n\n**Assumptions and Violations**:\nThis derivation relies on two key assumptions as stipulated:\n-   **Simplified Independence Model**: It assumes that the complex correlation structure due to LD can be simplified to $\\alpha m$ perfectly independent markers. In reality, correlations are graded, and the number of effectively independent markers may not scale linearly with $m$. If this were not true, a more complex function would replace $\\alpha m$, altering the final expression.\n-   **Uniform Allele Distribution**: The counting method implicitly assumes that for each independent marker, both alleles are present in the population, allowing both '0' and '1' states to contribute to combinatorial diversity. A more realistic model would incorporate the actual minor allele frequencies of the SNVs, moving from a combinatorial counting problem to a probabilistic one concerning the likelihood of two individuals having the same signature by chance.", "answer": "$$\n\\boxed{\\left\\lceil \\frac{\\ln(N)}{\\alpha \\ln(2)} \\right\\rceil}\n$$", "id": "4863913"}, {"introduction": "Ethical decisions about data sharing are rarely black and white; they almost always involve balancing the promise of scientific discovery against the potential for harm. This practice moves beyond qualitative discussion by introducing a quantitative tool from decision theory—expected utility—to formalize this trade-off. By modeling the potential benefits and harms, you will derive a clear decision rule for when releasing a dataset is ethically justifiable, translating abstract principles like beneficence into a concrete calculation [@problem_id:4863890].", "problem": "A national genomic data bank is considering releasing a controlled-access dataset of aggregated whole-genome variants linked to electronic health records. The governance plan includes technical safeguards and oversight. For ethical analysis, suppose the following simplified and scientifically plausible outcome structure holds:\n\n- With probability $p$, governance and security operate as intended, the release catalyzes high-quality biomedical research producing a net social research benefit valued at $B$ (for example, in Quality-Adjusted Life Year (QALY)-equivalent social welfare units), and no privacy breach occurs.\n- With complementary probability $1-p$, a privacy breach occurs that undermines the research program’s integrity such that no substantive research benefit materializes, and a net privacy harm valued at $H$ is incurred.\n\nAssume the decision-maker is risk neutral (so utilities add linearly), and that $B&gt;0$ and $H&gt;0$. Use only the fundamental definition of expected utility for discrete outcomes, namely that the expected utility equals the sum over outcomes of probability times utility. Under a utilitarian criterion, the data release is ethically justified if and only if the expected social utility of releasing the data is strictly positive.\n\nDerive, from first principles, the minimum success-probability threshold $p^{\\ast}$ as a function of $B$ and $H$ such that the release is ethically justified whenever $p&gt;p^{\\ast}$. Provide your final answer as a single closed-form expression for $p^{\\ast}$ in terms of $B$ and $H$. No rounding is required, and no units should be included with the final expression.", "solution": "The problem requires deriving the minimum probability threshold, $p^*$, for a data release to be ethically justified under a utilitarian framework. The justification criterion is that the expected social utility of the action must be strictly positive.\n\n1.  **Define the Outcomes and Utilities**: There are two possible, mutually exclusive outcomes:\n    -   **Success**: Occurs with probability $p$. The social utility is the net research benefit, $+B$.\n    -   **Failure**: Occurs with probability $1-p$. The social utility is the net privacy harm, which is a loss, represented as $-H$.\n\n2.  **Formulate the Expected Utility Equation**: The expected utility, $E[U]$, is the sum of the utilities of each outcome weighted by its probability. For a risk-neutral decision-maker, this is:\n    $$ E[U] = (p \\times \\text{Utility of Success}) + ((1-p) \\times \\text{Utility of Failure}) $$\n    Substituting the given values:\n    $$ E[U] = p(B) + (1-p)(-H) $$\n\n3.  **Simplify the Expression**:\n    $$ E[U] = pB - H + pH $$\n    $$ E[U] = p(B+H) - H $$\n\n4.  **Apply the Decision Criterion**: The release is justified if and only if $E[U] > 0$.\n    $$ p(B+H) - H > 0 $$\n\n5.  **Solve for the Probability Threshold**: To find the condition on $p$, we solve the inequality.\n    $$ p(B+H) > H $$\n    Since $B>0$ and $H>0$, the term $(B+H)$ is positive. We can divide by it without changing the direction of the inequality:\n    $$ p > \\frac{H}{B+H} $$\n\nThis inequality shows that the data release is justified only when the probability of success, $p$, exceeds the ratio of the harm to the sum of the harm and benefit. Therefore, the minimum threshold $p^*$ is this ratio.\n$$ p^* = \\frac{H}{B+H} $$", "answer": "$$ \\boxed{\\frac{H}{B+H}} $$", "id": "4863890"}, {"introduction": "Effective genomic data sharing requires more than just technical safeguards; it demands a robust and trustworthy governance system that operates on clear, ethical principles. This final, capstone exercise challenges you to design such a system from the ground up for a multi-institutional data trust. By evaluating and selecting the best governance model, you will synthesize concepts of fiduciary duty, ethical oversight, and privacy-preserving technology to construct a framework that can earn public trust while advancing scientific research [@problem_id:4863841].", "problem": "A consortium of $M = 5$ healthcare systems proposes a genomic data trust to steward whole-genome sequences and linked clinical data for $N = 100{,}000$ participants. The consortium intends to enable cross-site analyses without centralized raw data pooling. The trust must be designed to impose fiduciary duties to participants and to decide when cross-site analyses are approved under transparent, public criteria that align with core ethical principles.\n\nUse fundamental bases appropriate to medical ethics, including the Belmont Report principles of respect for persons, beneficence, and justice; the definition of fiduciary duties as duties of loyalty and care; and widely accepted Fair Information Practice Principles (FIPPs) such as transparency, purpose limitation, data minimization, access and correction rights, and accountability. Assume applicable oversight frameworks such as Institutional Review Board (IRB) review and General Data Protection Regulation (GDPR) style Data Protection Impact Assessment (DPIA) are considered best practice for evaluating privacy risks even if not legally mandated across all sites. For technical protection, consider privacy-preserving computation methods such as Secure Multiparty Computation (SMPC) and federated learning, and noise addition using Differential Privacy (DP). All decision procedures must be publicly documented in advance, applied consistently, and independently audited.\n\nWhich option best specifies a governance structure with fiduciary duties to participants and a decision procedure for approving cross-site analyses that operationalizes the stated first principles while maintaining scientifically realistic and ethically defensible protections?\n\nA. Establish an independent, legally constituted data trust whose board includes participant trustees with voting power and a majority of independent members. The charter explicitly imposes duties of loyalty and care to participants, with enforceable conflict-of-interest and recusal rules. Approval criteria are pre-published and include necessity (no reasonably less intrusive method), proportionality (expected social benefit outweighs residual risk), scientific merit (rigorous, pre-registered protocol), public interest, justice (equitable inclusion and benefit), purpose limitation, data minimization, and privacy risk thresholds. Cross-site analyses must use federated learning or Secure Multiparty Computation (SMPC) with Differential Privacy (DP) noise calibrated ex ante, with an $\\epsilon \\le 1$ per analysis unless an Institutional Review Board (IRB) justifies a higher bound with additional safeguards. Each proposal undergoes a Data Protection Impact Assessment (DPIA), multi-site IRB review, and independent statistical privacy risk quantification; decisions and rationales are published, with audit logs, an appeals process, and participant withdrawal rights respected. Equity oversight ensures that benefits do not systematically bypass high-risk groups, and periodic external audits test compliance with Fair Information Practice Principles (FIPPs).\n\nB. Create a consortium steering committee composed of representatives from each institution. Approvals are granted when at least $2/3$ of sites agree. De-identification of datasets is deemed sufficient, and raw data are pooled centrally when efficiency demands it. Participants are not represented in governance, and transparency is limited to internal memoranda. IRB review is optional and may be waived if prior de-identification is certified. Approval criteria emphasize operational feasibility and speed rather than pre-published ethical standards.\n\nC. Form a venture-funded foundation to own the platform. Consent is managed via blockchain tokens, and approvals are automated by smart contracts that trigger when pre-coded checks pass. Differential Privacy (DP) is optional, de-identification is primary, and broad consent is asserted to cover any future analytics. Participant withdrawal is not supported due to the immutability of on-chain records. Transparency is claimed through public code repositories rather than human-readable decision rationales. No independent ethics review beyond initial legal counsel vetting is included.\n\nD. Implement participant plebiscite governance: every cross-site analysis requires $>50\\%$ of participant votes in favor; any participant can veto analyses involving their data. Technical safeguards are limited to $k$-anonymity with $k=5$. There is no scientific merit or proportionality assessment; IRB review is optional. Decisions are justified by popularity rather than ex ante criteria, and there is no conflict-of-interest policy or independent audit.\n\nE. Place the trust under hospital control with an advisory participant panel. Differential Privacy (DP) with $\\epsilon \\le 1$ and federated analysis are mandated by policy, and scientific merit plus necessity are pre-specified criteria. Decisions and summaries are published, but the trustee is not independent of hospital interests, there is no enforceable duty of loyalty to participants, withdrawal rights are limited, and conflict-of-interest management is handled informally. Appeals are restricted to internal review by the hospital compliance office; Data Protection Impact Assessment (DPIA) is conducted selectively.", "solution": "To determine the best governance structure, we must evaluate each option against the principles specified in the problem: the Belmont principles (respect for persons, beneficence, justice), fiduciary duties (loyalty and care), Fair Information Practice Principles (FIPPs), and best practices in technical and procedural oversight.\n\n**Option A is the correct answer** because it provides the most comprehensive and ethically robust framework that integrates all the required elements:\n\n-   **Fiduciary Duty and Governance**: It establishes an **independent legal trust** with **participant trustees holding voting power**. This structure creates a clear, enforceable fiduciary duty of loyalty and care to participants, mitigating conflicts of interest. This directly aligns with the core mandate of the trust.\n-   **Principled Decision-Making**: It uses **pre-published, transparent criteria** for approving analyses that explicitly incorporate **necessity, proportionality, scientific merit, and justice**. This operationalizes the principles of *beneficence* (ensuring value and minimizing harm) and *justice* (fair distribution of benefits and burdens).\n-   **Respect for Persons and FIPPs**: It fully respects participant autonomy through an **appeals process** and **withdrawal rights**. It adheres to FIPPs like **purpose limitation** and **data minimization** through its procedures.\n-   **Technical Safeguards**: It mandates state-of-the-art **privacy-preserving computation (federated learning or SMPC)** combined with a strong, quantifiable **Differential Privacy (DP)** guarantee ($\\epsilon \\le 1$), representing the gold standard in privacy protection.\n-   **Accountability**: It requires a full suite of oversight, including **multi-site IRB review**, **DPIAs**, **independent audits**, and **published decisions**, ensuring transparency and accountability.\n\n**Analysis of Incorrect Options:**\n\n-   **Option B** fails because its governance is institution-centric, creating a conflict of interest, and its technical safeguards (simple de-identification) and oversight (optional IRB) are inadequate.\n-   **Option C** fails because its venture-funded model introduces a commercial conflict of interest. Its reliance on \"broad consent\" and the impossibility of withdrawal are severe violations of *respect for persons*.\n-   **Option D** fails because \"governance by plebiscite\" is impractical and replaces principled ethical review (beneficence, scientific merit) with popularity, while using weak technical safeguards ($k$-anonymity).\n-   **Option E** is a significant improvement over B, C, and D, but it has a fatal flaw: the trust is **not independent** and lacks an **enforceable duty of loyalty** to participants. Its oversight mechanisms (internal appeals, informal conflict management) are also weaker than those in Option A.\n\nIn conclusion, Option A is the only choice that creates a trustworthy ecosystem by combining an independent, participant-centric governance structure with principled decision criteria, cutting-edge technical protections, and rigorous accountability.", "answer": "$$\\boxed{A}$$", "id": "4863841"}]}