{"hands_on_practices": [{"introduction": "Ethical research begins long before the first participant is enrolled; it starts with a robust and principled study design. This exercise demonstrates that sample size calculation is not merely a statistical formality but a cornerstone of ethical research. By deriving and applying the formula for statistical power, you will see how adequate planning ensures that a study can realistically answer its question, thereby respecting participants' contributions and making efficient use of resources [@problem_id:4883162].", "problem": "A clinical team plans a two-arm Randomized Controlled Trial (RCT) to compare a new antihypertensive agent against an active comparator on the primary outcome of systolic blood pressure reduction after $12$ weeks. Based on prior pilot data, they expect a standardized mean difference (Cohen’s $d$) of $d = 0.5$ between arms, assume equal allocation ($1{:}1$), and that the outcome is approximately normally distributed with equal variance across arms. The team will use a two-sided test with Type I error rate $\\alpha = 0.05$ and require power $1 - \\beta = 0.8$.\n\nStarting only from the core definitions of Type I error $\\alpha$, Type II error $\\beta$, power $1-\\beta$, standardized effect size $d = (\\mu_1 - \\mu_2)/\\sigma$, and the normal approximation to the two-sample $t$-test for the difference in means, derive the minimum total sample size across both arms that achieves the stated power requirement at the stated significance level under equal allocation. Then, compute its value for the given parameters $d = 0.5$, $\\alpha = 0.05$, and power $0.8$, and express the final total required sample size as a single integer count.\n\nFinally, explain how conducting an underpowered study relates to research integrity and resource waste within medical ethics, linking your explanation to the role of sample size planning in minimizing participant burden and maximizing informational yield.", "solution": "The problem requires the derivation of the formula for the total sample size in a two-arm randomized controlled trial, followed by a numerical calculation for the given parameters, and an ethical discussion.\n\n### Part 1: Derivation of the Sample Size Formula\n\nLet the two trial arms be Arm 1 (new agent) and Arm 2 (active comparator). Let the true mean systolic blood pressure reductions be $\\mu_1$ and $\\mu_2$, respectively. We assume a common population standard deviation $\\sigma$ for the outcome in both arms. The sample sizes in the arms are $n_1$ and $n_2$. Due to equal allocation ($1{:}1$), we have $n_1 = n_2 = n$. The total sample size is $N = n_1 + n_2 = 2n$.\n\nThe null hypothesis ($H_0$) and the two-sided alternative hypothesis ($H_A$) for the difference in means are:\n$$H_0: \\mu_1 = \\mu_2 \\quad \\text{or} \\quad \\mu_1 - \\mu_2 = 0$$\n$$H_A: \\mu_1 \\neq \\mu_2 \\quad \\text{or} \\quad \\mu_1 - \\mu_2 \\neq 0$$\n\nFor sample size planning, we use the normal approximation to the two-sample $t$-test. The test statistic $Z$ for the difference in sample means $(\\bar{X}_1 - \\bar{X}_2)$ is given by:\n$$Z = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\text{SE}(\\bar{X}_1 - \\bar{X}_2)} = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}$$\nGiven $\\sigma_1 = \\sigma_2 = \\sigma$ and $n_1 = n_2 = n$, the standard error (SE) is $\\sigma\\sqrt{\\frac{1}{n} + \\frac{1}{n}} = \\sigma\\sqrt{\\frac{2}{n}}$.\n\nUnder the null hypothesis $H_0$, the sampling distribution of the difference in means $(\\bar{X}_1 - \\bar{X}_2)$ is normal with a mean of $0$ and a standard deviation equal to the standard error. Thus, the test statistic $Z$ follows a standard normal distribution, $Z \\sim N(0, 1)$.\n\nFor a two-sided test with a Type I error rate of $\\alpha$, we reject $H_0$ if the observed $|Z|$ is greater than the critical value $z_{1-\\alpha/2}$, where $z_{q}$ is the $q$-th quantile of the standard normal distribution.\n\nPower ($1-\\beta$) is the probability of correctly rejecting $H_0$ when $H_A$ is true. To calculate power, we must specify a particular alternative hypothesis. Let's assume the true difference in means is $\\mu_1 - \\mu_2 = \\delta$, where $\\delta \\neq 0$. The standardized effect size is Cohen's $d = \\frac{\\mu_1 - \\mu_2}{\\sigma} = \\frac{\\delta}{\\sigma}$.\n\nUnder this alternative, the sampling distribution of $(\\bar{X}_1 - \\bar{X}_2)$ is normal with a mean of $\\delta$. The test statistic $Z$ is therefore not standard normal; its distribution is $Z \\sim N(\\frac{\\delta}{\\sigma\\sqrt{2/n}}, 1)$, which can be rewritten using Cohen's $d$ as $Z \\sim N(d\\sqrt{n/2}, 1)$.\n\nWe reject $H_0$ if $Z > z_{1-\\alpha/2}$ or $Z < -z_{1-\\alpha/2}$. The power is the probability of this event under $H_A$:\n$$\\text{Power} = 1 - \\beta = P(Z > z_{1-\\alpha/2} | H_A) + P(Z < -z_{1-\\alpha/2} | H_A)$$\nAssuming $\\delta > 0$ (and thus $d > 0$), the second term $P(Z < -z_{1-\\alpha/2} | H_A)$ is typically negligible for standard power levels and effect sizes, as the distribution is shifted to the right. We therefore approximate the power using only the first term:\n$$1 - \\beta \\approx P(Z > z_{1-\\alpha/2} | H_A)$$\nTo evaluate this probability, we standardize the variable $Z$ under $H_A$. Let $Z' = \\frac{Z - d\\sqrt{n/2}}{1} \\sim N(0,1)$. Then $Z = Z' + d\\sqrt{n/2}$. Substituting this into the power condition:\n$$1 - \\beta \\approx P(Z' + d\\sqrt{n/2} > z_{1-\\alpha/2})$$\n$$1 - \\beta \\approx P(Z' > z_{1-\\alpha/2} - d\\sqrt{n/2})$$\nThe probability that a standard normal variable $Z'$ exceeds a value $x$ is $1 - \\Phi(x)$, where $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. This is also equal to $\\Phi(-x)$. So, the quantile $-x$ must be $z_{1-\\beta}$.\nThis means $z_{1-\\beta} = - (z_{1-\\alpha/2} - d\\sqrt{n/2})$.\n$$z_{1-\\beta} = d\\sqrt{n/2} - z_{1-\\alpha/2}$$\nRearranging to solve for the sample size per arm, $n$:\n$$z_{1-\\alpha/2} + z_{1-\\beta} = d\\sqrt{n/2}$$\n$$\\sqrt{n} = \\frac{\\sqrt{2}(z_{1-\\alpha/2} + z_{1-\\beta})}{d}$$\n$$n = \\frac{2(z_{1-\\alpha/2} + z_{1-\\beta})^2}{d^2}$$\nThis is the minimum sample size required for each arm. The total sample size $N$ is $2n$:\n$$N = \\frac{4(z_{1-\\alpha/2} + z_{1-\\beta})^2}{d^2}$$\n\n### Part 2: Numerical Calculation\n\nWe are given the following parameters:\n- Standardized mean difference, $d = 0.5$.\n- Type I error rate, $\\alpha = 0.05$.\n- Power, $1 - \\beta = 0.8$.\n\nFirst, we find the required quantiles from the standard normal distribution:\n- For a two-sided test with $\\alpha = 0.05$, the critical value is $z_{1-\\alpha/2} = z_{1-0.025} = z_{0.975} \\approx 1.95996$. We use the conventional value $1.96$.\n- For a power of $1 - \\beta = 0.8$, the corresponding quantile is $z_{1-\\beta} = z_{0.8} \\approx 0.84162$.\n\nNow, we substitute these values into the derived formula for the total sample size $N$:\n$$N = \\frac{4(z_{1-\\alpha/2} + z_{1-\\beta})^2}{d^2} = \\frac{4(1.96 + 0.84162)^2}{(0.5)^2}$$\n$$N = \\frac{4(2.80162)^2}{0.25} = \\frac{4(7.849074)}{0.25} = \\frac{31.396296}{0.25} = 125.585$$\nSince the sample size must be an integer and the calculation provides the minimum number to achieve the desired power, we must round up to the next integer. To maintain equal allocation, the total size should be an even number. Therefore, we round up to the nearest even integer.\n$$N = 126$$\nThis total sample size of $126$ would be allocated as $n=63$ participants per arm.\n\n### Part 3: Ethical Implications of Underpowered Studies\n\nThe calculated sample size of $N=126$ is the minimum required to provide a scientifically credible answer to the research question under the given assumptions. Conducting a study with a substantially smaller sample size would render it \"underpowered.\" An underpowered study has a low probability (i.e., low power) of detecting a true, clinically meaningful effect, leading to a high risk of a Type II error (a false negative result). This has profound implications for research integrity and medical ethics.\n\n1.  **Violation of Beneficence and Non-Maleficence**: The cornerstone of medical ethics is to maximize benefits while minimizing harm. Participants in clinical trials consent to risks—including potential side effects of a new agent, the burden of study procedures, and the possibility of receiving a less effective treatment—with the expectation that their participation will contribute to valuable scientific knowledge that can benefit future patients. An underpowered study is methodologically incapable of reliably generating such knowledge. It is therefore unethical to expose participants to any risk or burden, no matter how small, in a study that is futile from its inception. The risk-benefit ratio is inherently unfavorable.\n\n2.  **Waste of Resources and Violation of Justice**: Research is supported by finite resources, including public funding, institutional overhead, and the time and expertise of investigators. Conducting an underpowered study wastes these resources, as the investment is unlikely to yield a conclusive result. From the perspective of distributive justice, these squandered resources could have been allocated to a well-designed study with a higher probability of success.\n\n3.  **Damage to Research Integrity and the Scientific Record**: Scientifically, an underpowered study that yields a statistically non-significant result is uninformative; it is impossible to distinguish a true lack of effect from the study's inability to detect an existing one. Furthermore, such studies contribute to publication bias. Statistically significant results from small, potentially underpowered studies may be published (and may be false positives), while non-significant results from underpowered studies are often left unpublished. This systematically distorts the evidence base, potentially leading other researchers and clinicians to draw incorrect conclusions about an intervention's efficacy.\n\n**The Role of Sample Size Planning**: Proper *a priori* sample size planning, as demonstrated in the calculation above, is a critical component of ethical research design. It serves two ethical functions:\n- **Minimizing Participant Burden**: It ensures that no more participants than necessary are enrolled, thus limiting the total exposure to research-related risks. Enrolling too many participants is also unethical. The goal is to find the optimal number.\n- **Maximizing Informational Yield**: It ensures the study has a high probability of producing a definitive result, thereby honoring the contribution of the participants and justifying the resources expended. By planning for adequate power, researchers uphold their ethical obligation to conduct rigorous science that can genuinely advance medical knowledge and improve patient care.\n\nIn summary, sample size calculation is not merely a statistical formality but an ethical imperative. Failure to ensure adequate power constitutes poor scientific practice and a breach of the ethical contract between researchers and study participants.", "answer": "$$\\boxed{126}$$", "id": "4883162"}, {"introduction": "In the modern era, much of scientific analysis relies on complex computational pipelines, where integrity demands full transparency. This case study explores the critical distinction between *reproducibility* (getting the same result using the same data and code) and *replicability* (getting consistent results using new data). You will analyze a common scenario where a subtle software update changes a study's conclusion, forcing a decision on the nature of the failure and the appropriate ethical remedy [@problem_id:4883194].", "problem": "A clinical genomics team analyzes adverse drug reaction risk using a Cox proportional hazards model on a de-identified electronic health record cohort of $n = 1{,}240$ patients. Their original published pipeline, which used the statistical library \"SciStat\" version $2.3$, reported a hazard ratio $HR = 1.32$ with a $p$-value $p = 0.04$. After upgrading \"SciStat\" to version $2.4$, rerunning the same scripts on the same dataset yields $HR = 1.28$ with $p = 0.08$. The team discovers that between version $2.3$ and $2.4$, the default method for handling tied event times changed from the Breslow approximation to the Efron approximation, and the default optimizer tolerance was tightened.\n\nAssume the following well-tested definitions as the fundamental base for reasoning: Under the usage promoted by the National Academies of Sciences, Engineering, and Medicine (NASEM), reproducibility refers to obtaining consistent results using the same data, the same methods, and the same code, typically in the same or highly similar computational environment; replicability refers to obtaining consistent results across studies that aim to answer the same scientific question but use new data and potentially different code that implements the same methods. Research integrity in medical ethics requires transparency, accountability, and the faithful reporting of methods such that results can be reproduced and, where appropriate, replicated.\n\nBased on this case, select the best characterization of the failure and the most ethically appropriate primary remedy that aligns with research integrity obligations. Choose the single best option.\n\nA. This is a failure of replicability; the primary remedy is to collect a new independent dataset and re-estimate the model to see if the association persists.\n\nB. This is a failure of reproducibility; the primary remedy is to control and disclose the computational environment by pinning exact software versions, fixing random seeds, and providing a containerized reproduction package with an environment manifest (for example, a Software Bill of Materials), so reruns with the same data and code yield consistent results.\n\nC. This is evidence of scientific misconduct (for example, fabrication); the primary remedy is to retract the study and report the authors to the institution’s research integrity office.\n\nD. This is not an integrity issue because small numerical differences are expected; the primary remedy is to proceed without further action since both $p = 0.04$ and $p = 0.08$ are close to the threshold.\n\nE. This is a failure of reproducibility; the primary remedy is to switch to a more robust estimator and report sensitivity across library versions, without documenting or controlling the original computational environment.", "solution": "The problem statement will be validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n\nThe verbatim data, conditions, and definitions provided are:\n-   **Study Population**: A de-identified electronic health record cohort of $n = 1{,}240$ patients.\n-   **Analysis Model**: Cox proportional hazards model for adverse drug reaction risk.\n-   **Original Analysis Software**: \"SciStat\" version $2.3$.\n-   **Original Analysis Results**: For a key pharmacogenomic variant, the hazard ratio was $HR = 1.32$ with a $p$-value of $p = 0.04$.\n-   **Updated Analysis Software**: \"SciStat\" version $2.4$.\n-   **Updated Analysis Results**: Rerunning the same scripts on the same dataset yielded a hazard ratio of $HR = 1.28$ with a $p$-value of $p = 0.08$.\n-   **Reason for Discrepancy**: \"the default method for handling tied event times changed from the Breslow approximation to the Efron approximation, and the default optimizer tolerance was tightened.\"\n-   **NASEM Definition of Reproducibility**: \"reproducibility refers to obtaining consistent results using the same data, the same methods, and the same code, typically in the same or highly similar computational environment\".\n-   **NASEM Definition of Replicability**: \"replicability refers to obtaining consistent results across studies that aim to answer the same scientific question but use new data and potentially different code that implements the same methods\".\n-   **Definition of Research Integrity**: \"requires transparency, accountability, and the faithful reporting of methods such that results can be reproduced and, where appropriate, replicated\".\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is evaluated against the validation criteria.\n\n-   **Scientifically Grounded**: The scenario is highly plausible and common in computational life sciences. Cox proportional hazards models are standard for time-to-event analysis. The Breslow and Efron approximations for handling tied event times are well-established methods implemented in major statistical software. Changes in default settings, numerical optimizers, and their tolerances between software versions are a known source of variation in results. The numerical values ($HR$, $p$, $n$) are realistic for a pharmacogenomic study. The provided definitions of reproducibility and replicability are correctly attributed to and consistent with guidance from the National Academies of Sciences, Engineering, and Medicine (NASEM). The problem is scientifically sound.\n-   **Well-Posed**: The problem is clearly stated. It presents a case study and asks for the best characterization and remedy based on provided, standard definitions of research integrity concepts. A unique best answer can be determined through logical deduction.\n-   **Objective**: The language is precise and avoids subjective or biased phrasing. It presents a factual scenario without leading the user toward a specific conclusion outside the provided definitions.\n-   **Flaw Checklist**: The problem does not violate any of the listed flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, or trivial. It poses a substantive question about research ethics and methodology.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A full solution will be derived.\n\n### Derivation and Option Analysis\n\nThe core of the problem lies in the discrepancy of results ($HR = 1.32$, $p = 0.04$ versus $HR = 1.28$, $p = 0.08$) when the *same code* is run on the *same data*, with the only change being an update to the underlying software library (\"SciStat\" from version $2.3$ to $2.4$). This update implicitly changed the \"methods\" by altering the default algorithm for handling tied-times and the optimizer's numerical tolerance.\n\n**1. Characterization of the Failure:**\nWe apply the provided NASEM definitions.\n-   **Reproducibility**: This refers to \"obtaining consistent results using the same data, the same methods, and the same code, typically in the same or highly similar computational environment\". In this case, the results were *not* consistent. Although the high-level code (the user's script) and data were identical, the computational environment (software version) changed, which in turn altered the low-level numerical methods. This is a quintessential failure of computational reproducibility. The result is not robust to a minor, and often un-tracked, change in the computational environment.\n-   **Replicability**: This refers to \"obtaining consistent results... using new data\". The scenario explicitly involves using the *same* dataset. Therefore, the issue is not one of replicability.\n\nThe failure is correctly characterized as a **failure of reproducibility**.\n\n**2. Ethically Appropriate Primary Remedy:**\nThe problem states that research integrity \"requires transparency, accountability, and the faithful reporting of methods\". The discrepancy, especially the change in the $p$-value from below the conventional significance threshold of $0.05$ to above it, has a material impact on the study's conclusions. An ethical remedy must address the lack of transparency and control that led to this ambiguity.\n-   The root cause is the unspecified and uncontrolled computational environment. A change in a library version altered the result.\n-   Therefore, the primary remedy must be to make the computational environment explicit, specified, and controllable. This allows any other researcher (or the original authors at a later time) to perfectly reproduce the reported results by recreating the exact environment. This directly serves the principles of transparency and accountability.\n-   Actions that achieve this include explicitly stating all software versions used, providing configuration files (e.g., a \"Software Bill of Materials\" or manifest), and ideally, distributing the analysis code within a containerized environment (like Docker or Singularity) that encapsulates the entire software stack. Fixing random number generator seeds is also a part of this best practice, although not the specific cause of the issue here.\n\nWith this framework, we evaluate the given options.\n\n**Option A. This is a failure of replicability; the primary remedy is to collect a new independent dataset and re-estimate the model to see if the association persists.**\n-   **Analysis**: This incorrectly identifies the failure as one of replicability. While conducting a replication study with new data is scientifically valuable, it does not fix the methodological flaw in the original publication, which is its lack of reproducibility. The primary ethical obligation is to ensure the original work is transparent and verifiable on its own terms.\n-   **Verdict**: **Incorrect**.\n\n**Option B. This is a failure of reproducibility; the primary remedy is to control and disclose the computational environment by pinning exact software versions, fixing random seeds, and providing a containerized reproduction package with an environment manifest (for example, a Software Bill of Materials), so reruns with the same data and code yield consistent results.**\n-   **Analysis**: This correctly identifies the failure as one of reproducibility. The proposed remedy directly addresses the root cause of the problem by making the computational environment transparent and controllable. Pinning versions, providing manifests (like an SBOM), and using containers are the current best practices for ensuring computational reproducibility. This remedy aligns perfectly with the ethical requirements of transparency and accountability.\n-   **Verdict**: **Correct**.\n\n**Option C. This is evidence of scientific misconduct (for example, fabrication); the primary remedy is to retract the study and report the authors to the institution’s research integrity office.**\n-   **Analysis**: This is an incorrect and overly harsh characterization. Scientific misconduct implies intent to deceive (e.g., fabrication, falsification, plagiarism). The scenario describes an unintentional consequence of a software update, a common challenge in computational science. The team's discovery and deliberation on the issue is an act of integrity, not misconduct. The proposed remedy is disproportionate.\n-   **Verdict**: **Incorrect**.\n\n**Option D. This is not an integrity issue because small numerical differences are expected; the primary remedy is to proceed without further action since both $p = 0.04$ and $p = 0.08$ are close to the threshold.**\n-   **Analysis**: This is fundamentally wrong. A change that crosses a pre-defined significance threshold (e.g., $\\alpha = 0.05$) alters the study's primary conclusion. Treating $p = 0.04$ and $p=0.08$ as equivalent because they are \"close\" is poor statistical practice and a violation of the principle of faithful reporting. Ignoring such a sensitivity is an integrity failure, not an acceptable course of action.\n-   **Verdict**: **Incorrect**.\n\n**Option E. This is a failure of reproducibility; the primary remedy is to switch to a more robust estimator and report sensitivity across library versions, without documenting or controlling the original computational environment.**\n-   **Analysis**: The characterization of the failure is correct. However, the remedy is flawed. While analyzing sensitivity and using robust estimators are good scientific practices, the clause \"without documenting or controlling the original computational environment\" is a critical error. The primary ethical failure is the lack of transparency about how the original result was produced. An appropriate remedy must correct this failure. This option suggests improving the science but explicitly neglects the fundamental ethical duty of transparency and accountability regarding the original reported findings.\n-   **Verdict**: **Incorrect**.\n\nIn summary, Option B offers the correct diagnosis (a failure of reproducibility) and prescribes the most appropriate, state-of-the-art remedy that directly aligns with the ethical principles of research integrity.", "answer": "$$\\boxed{B}$$", "id": "4883194"}, {"introduction": "The credibility of a research finding depends not only on the data presented but also on the context in which it was produced, including potential conflicts of interest. This problem introduces a Bayesian framework as a formal way to weigh evidence while accounting for prior beliefs and biases. By adjusting the pre-study probability of a hypothesis based on a conflict of interest, you will quantitatively assess how skepticism can be integrated into the evaluation of a \"positive\" trial result [@problem_id:4883154].", "problem": "A randomized controlled trial evaluates whether a new antihypertensive drug reduces the rate of stroke compared to standard care. To assess the credibility of a positive trial result, an ethics review committee applies a Bayesian framework grounded in pre-study plausibility and the evidential weight of the result. Let the pre-study probability that the drug truly reduces stroke risk be $\\pi = 0.2$. Due to concerns about sponsor-related Conflict of Interest (COI), including potential selective reporting and publication bias, the committee reduces the effective prior to $0.15$. The evidential strength of the observed positive result is summarized by a likelihood ratio $L = 4$, defined as the ratio $\\frac{P(+ \\mid \\text{true effect})}{P(+ \\mid \\text{no true effect})}$, where $+$ denotes the observed positive result.\n\nStarting from the standard formulation of Bayes’ theorem and the definition of a likelihood ratio, derive an expression for the posterior probability $P(\\text{true effect} \\mid +)$ and compute its value using the COI-adjusted prior $0.15$ and the likelihood ratio $4$. Provide your final answer as an exact fraction. Do not use any shortcut formulas that are not derived from Bayes’ theorem and core definitions.", "solution": "The problem statement will first be validated for scientific soundness, self-consistency, and clarity before any attempt at a solution is made.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The event of interest is whether a new antihypertensive drug \"truly reduces stroke risk.\" This is the hypothesis.\n- The pre-study probability of a true effect is given as $\\pi = 0.2$.\n- An adjusted prior probability is provided due to Conflict of Interest (COI) concerns: $0.15$. This is the value to be used in the calculation.\n- A positive trial result is observed, denoted as `$+$`.\n- The evidential strength is given as a likelihood ratio, $L = 4$.\n- The likelihood ratio is defined as $L = \\frac{P(+ \\mid \\text{true effect})}{P(+ \\mid \\text{no true effect})}$.\n- The task is to derive an expression for the posterior probability $P(\\text{true effect} \\mid +)$ from Bayes’ theorem and then compute its value using the COI-adjusted prior and the given likelihood ratio.\n- The final answer must be an exact fraction.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is grounded in Bayesian statistics, a fundamental framework for reasoning under uncertainty. Its application to the interpretation of clinical trial results, including the adjustment of prior probabilities based on external factors like conflicts of interest, is a well-established (though debated) topic in medical statistics and ethics. The concepts are standard and scientifically valid.\n- **Well-Posed**: The problem is well-posed. It provides a clear objective (calculate a posterior probability), sufficient data (an adjusted prior probability and a likelihood ratio), and explicit definitions. From the given prior, the probability of the complementary event is implicitly defined. A unique, meaningful solution exists. The initial prior of $\\pi=0.2$ is contextual information, and the directive to use the COI-adjusted prior of $0.15$ is unambiguous.\n- **Objective**: The problem is stated in objective, formal language. It sets up a quantitative scenario without introducing subjective or opinion-based claims.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid as it is scientifically sound, well-posed, objective, and self-contained. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nLet $H_1$ be the hypothesis that the drug has a true effect (i.e., it reduces stroke risk).\nLet $H_0$ be the null hypothesis that the drug has no true effect. These two hypotheses are mutually exclusive and exhaustive.\nLet `$+$` denote the event that the randomized controlled trial yields a positive result.\n\nFrom the problem statement, we are given the following information:\nThe COI-adjusted prior probability of a true effect is $P(H_1) = 0.15$.\nThe prior probability of no true effect is therefore $P(H_0) = 1 - P(H_1) = 1 - 0.15 = 0.85$.\nThe likelihood ratio for a positive result is $L = \\frac{P(+ \\mid H_1)}{P(+ \\mid H_0)} = 4$.\n\nOur objective is to compute the posterior probability of a true effect given the positive result, which is $P(H_1 \\mid +)$.\n\nWe begin with the standard formulation of Bayes' theorem:\n$$\nP(H_1 \\mid +) = \\frac{P(+ \\mid H_1) P(H_1)}{P(+)}\n$$\n\nThe denominator, $P(+)$, is the total probability of observing a positive result. It can be expanded using the law of total probability with respect to the partition $\\{H_1, H_0\\}$:\n$$\nP(+) = P(+ \\mid H_1) P(H_1) + P(+ \\mid H_0) P(H_0)\n$$\n\nSubstituting this expansion back into Bayes' theorem, we get:\n$$\nP(H_1 \\mid +) = \\frac{P(+ \\mid H_1) P(H_1)}{P(+ \\mid H_1) P(H_1) + P(+ \\mid H_0) P(H_0)}\n$$\n\nThe problem provides the likelihood ratio $L$, not the individual conditional probabilities $P(+ \\mid H_1)$ and $P(+ \\mid H_0)$. To express the posterior probability in terms of $L$, we can divide both the numerator and the denominator of the fraction by $P(+ \\mid H_0)$. This is permissible as $P(+ \\mid H_0)$ must be greater than $0$ for the likelihood ratio $L=4$ to be well-defined and finite.\n$$\nP(H_1 \\mid +) = \\frac{\\frac{P(+ \\mid H_1) P(H_1)}{P(+ \\mid H_0)}}{\\frac{P(+ \\mid H_1) P(H_1)}{P(+ \\mid H_0)} + \\frac{P(+ \\mid H_0) P(H_0)}{P(+ \\mid H_0)}}\n$$\n\nBy rearranging the terms and simplifying, we can introduce the definition of the likelihood ratio $L = \\frac{P(+ \\mid H_1)}{P(+ \\mid H_0)}$:\n$$\nP(H_1 \\mid +) = \\frac{\\left(\\frac{P(+ \\mid H_1)}{P(+ \\mid H_0)}\\right) P(H_1)}{\\left(\\frac{P(+ \\mid H_1)}{P(+ \\mid H_0)}\\right) P(H_1) + P(H_0)}\n$$\n$$\nP(H_1 \\mid +) = \\frac{L \\cdot P(H_1)}{L \\cdot P(H_1) + P(H_0)}\n$$\nThis is the derived expression for the posterior probability in terms of the prior probabilities and the likelihood ratio, as required.\n\nNow, we substitute the given numerical values into this expression:\n- $P(H_1) = 0.15$ (the COI-adjusted prior)\n- $P(H_0) = 0.85$\n- $L = 4$\n\n$$\nP(H_1 \\mid +) = \\frac{4 \\cdot (0.15)}{4 \\cdot (0.15) + 0.85}\n$$\nFirst, we compute the products in the numerator and denominator:\n$$\n4 \\cdot 0.15 = 0.60\n$$\nSo the expression becomes:\n$$\nP(H_1 \\mid +) = \\frac{0.60}{0.60 + 0.85}\n$$\nNext, we compute the sum in the denominator:\n$$\n0.60 + 0.85 = 1.45\n$$\nThus, we have:\n$$\nP(H_1 \\mid +) = \\frac{0.60}{1.45}\n$$\nTo express this result as an exact fraction as requested, we can write the decimal values as fractions over $100$:\n$$\nP(H_1 \\mid +) = \\frac{60/100}{145/100} = \\frac{60}{145}\n$$\nFinally, we simplify the fraction by dividing the numerator and the denominator by their greatest common divisor. Both $60$ and $145$ are divisible by $5$:\n$$\n60 \\div 5 = 12\n$$\n$$\n145 \\div 5 = 29\n$$\nSince $29$ is a prime number and does not divide $12$, the fraction is in its simplest form.\n$$\nP(H_1 \\mid +) = \\frac{12}{29}\n$$\nThis is the final posterior probability.", "answer": "$$\\boxed{\\frac{12}{29}}$$", "id": "4883154"}]}