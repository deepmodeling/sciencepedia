{"hands_on_practices": [{"introduction": "One of the primary goals in genomics is to assemble a complete and continuous representation of a genome. The N50 statistic is a widely used metric to assess the contiguity of such an assembly. This exercise provides a hands-on opportunity to calculate and compare the N50 values for two different genome assemblies, illustrating the profound impact that long-read sequencing has on achieving a high-quality, contiguous result. [@problem_id:1501367]", "problem": "A team of molecular biologists is studying the genome of a newly discovered fungal species. The genome is estimated to be approximately 20 megabases (Mb) in size. To create a reference genome, they employ two different Deoxyribonucleic Acid (DNA) sequencing and assembly pipelines, resulting in two distinct draft assemblies, which they label Assembly A and Assembly B.\n\nThe quality of a genome assembly is often measured by its contiguity, which can be quantified using the N50 statistic. The N50 is defined as the minimum contig length such that at least half of the total assembled genome sequence is contained in contigs of that length or longer. For these calculations, assume 1 megabase (Mb) = 1,000 kilobases (kb).\n\nThe contents of the two assemblies are as follows:\n- **Assembly A** consists of: 5 contigs of 100 kb, 10 contigs of 80 kb, 20 contigs of 60 kb, 150 contigs of 50 kb, 100 contigs of 40 kb, and 300 contigs of 20 kb.\n- **Assembly B** consists of: one 7 Mb contig, one 5 Mb contig, one 3 Mb contig, one 2 Mb contig, one 1 Mb contig, and four 0.5 Mb contigs.\n\nBased on an analysis of these two assemblies, which one of the following statements is the most accurate conclusion regarding the sequencing technologies used?\n\nA. Assembly A was likely generated using long-read sequencing because it has a greater number of contigs, indicating more comprehensive coverage of the genome.\n\nB. Assembly A was likely generated using long-read sequencing because its N50 value is smaller, which is characteristic of the higher error rates in long reads.\n\nC. Assembly B was likely generated using long-read sequencing because a larger N50 indicates better assembly contiguity, a primary advantage of using longer reads to span repetitive regions.\n\nD. Assembly B was likely generated using long-read sequencing because its total assembled size is exactly 20 Mb, matching the estimate perfectly.\n\nE. It is impossible to determine the sequencing technology from this data alone; information about read error rates is also required.", "solution": "The goal of this problem is to compare two genome assemblies, calculate their respective N50 values, and use this information to infer which sequencing technology (long-read vs. short-read) was likely used for each.\n\nFirst, we must calculate the total assembled size for each assembly to verify they are comparable and to find the halfway point for the N50 calculation.\n\nFor Assembly A, the total size is the sum of the lengths of all its contigs:\nTotal size A = (5 * 100 kb) + (10 * 80 kb) + (20 * 60 kb) + (150 * 50 kb) + (100 * 40 kb) + (300 * 20 kb)\nTotal size A = 500 kb + 800 kb + 1,200 kb + 7,500 kb + 4,000 kb + 6,000 kb\nTotal size A = 20,000 kb = 20 Mb.\n\nFor Assembly B, the total size is:\nTotal size B = (1 * 7 Mb) + (1 * 5 Mb) + (1 * 3 Mb) + (1 * 2 Mb) + (1 * 1 Mb) + (4 * 0.5 Mb)\nTotal size B = 7 Mb + 5 Mb + 3 Mb + 2 Mb + 1 Mb + 2 Mb\nTotal size B = 20 Mb.\n\nBoth assemblies have a total size of 20 Mb. For the N50 calculation, we need to find the contig length at which the cumulative size of contigs of that length or greater reaches 50% of the total assembly size, which is 0.5 * 20 Mb = 10 Mb.\n\nNow, let's calculate the N50 for Assembly A. We need to work in kilobases (kb). The target cumulative size is 10,000 kb. We sort the contig groups by size in descending order and sum their total lengths:\n1.  Contigs of 100 kb: 5 * 100 kb = 500 kb. Cumulative sum = 500 kb.\n2.  Contigs of 80 kb: 10 * 80 kb = 800 kb. Cumulative sum = 500 kb + 800 kb = 1,300 kb.\n3.  Contigs of 60 kb: 20 * 60 kb = 1,200 kb. Cumulative sum = 1,300 kb + 1,200 kb = 2,500 kb.\n4.  Contigs of 50 kb: 150 * 50 kb = 7,500 kb. Cumulative sum = 2,500 kb + 7,500 kb = 10,000 kb.\n\nThe cumulative sum reaches the 10,000 kb mark exactly upon the inclusion of all contigs of 50 kb. By definition, the N50 is the minimum contig length in this set, so the N50 for Assembly A is 50 kb.\n\nNext, let's calculate the N50 for Assembly B. The target cumulative size is 10 Mb. We sort the contigs by size in descending order and sum their lengths:\n1.  Contig of 7 Mb: 1 * 7 Mb = 7 Mb. Cumulative sum = 7 Mb.\n2.  Contig of 5 Mb: 1 * 5 Mb = 5 Mb. Cumulative sum = 7 Mb + 5 Mb = 12 Mb.\n\nThe cumulative sum (12 Mb) surpasses the 10 Mb target upon the inclusion of the 5 Mb contig. Therefore, the N50 for Assembly B is the length of this contig, which is 5 Mb.\n\nNow we compare the two assemblies.\n- Assembly A: N50 = 50 kb = 0.05 Mb\n- Assembly B: N50 = 5 Mb\n\nAssembly B's N50 is 100 times larger than Assembly A's N50. A higher N50 indicates a more contiguous and less fragmented assembly. Long-read sequencing technologies (e.g., PacBio, Oxford Nanopore) produce very long reads that can span repetitive or complex regions of the genome. These regions are difficult to assemble from short reads (e.g., Illumina), leading to breaks in the assembly and a large number of smaller contigs. Consequently, long-read technologies typically yield assemblies with much higher contiguity and larger N50 values. Assembly B, with its massive 5 Mb N50, is characteristic of a long-read assembly. Assembly A, with its highly fragmented nature and low 50 kb N50, is characteristic of a short-read assembly.\n\nFinally, let's evaluate the given options:\nA: Incorrect. A greater number of contigs indicates a more fragmented, lower-quality assembly, which is typical of short-read technologies, not long-read.\nB: Incorrect. A smaller N50 indicates a more fragmented assembly, which is not a feature of a successful long-read assembly project. The primary advantage of long reads is achieving a high N50.\nC: Correct. Assembly B has a drastically larger N50 (5 Mb) compared to Assembly A (50 kb). This superior contiguity is the hallmark achievement of long-read sequencing technologies, which excel at resolving repetitive genomic elements that typically fragment assemblies built from short reads.\nD: Incorrect. Both assemblies sum to the same total size (20 Mb), so this is not a distinguishing feature between them.\nE: Incorrect. While other metrics like error rates are important for a full picture of assembly quality, the difference in N50 is so dramatic and is such a direct consequence of read length that it serves as a very strong primary indicator of the underlying sequencing technology.\n\nTherefore, the most accurate conclusion is that Assembly B was likely generated using long-read sequencing.", "answer": "$$\\boxed{C}$$", "id": "1501367"}, {"introduction": "Long-read sequencing has revolutionized our ability to detect large structural variants (SVs), such as insertions and deletions, which are often missed by short-read methods. This practice problem simulates a common scenario in genetic analysis where a long read unambiguously identifies a large insertion within a gene. By working through this example, you will connect the raw sequencing discovery to its direct biological consequence by calculating the size of the resulting mutant protein. [@problem_id:1501350]", "problem": "A molecular geneticist is studying a patient with a suspected genetic disorder affecting connective tissue. Using long-read sequencing, the geneticist analyzes the *Fibrillogen-Associated Protein 1 (FGAP1)* gene. The data reveals that many reads mapping to this gene contain a large insertion within exon 11. In the reference genome, exon 11 is 330 base pairs (bp) long. The patient's reads show that a novel 1,500 bp sequence has been inserted precisely after the 120th nucleotide of this exon. The wild-type FGAP1 protein, produced from the reference allele, is composed of 950 amino acids. Bioinformatic analysis of the 1,500 bp insertion confirms that it does not contain any in-frame stop codons. Assuming the average molecular weight of a single amino acid is 110 Daltons (Da), and that this altered allele is transcribed and translated into a stable protein, calculate the predicted molecular weight of the mutant FGAP1 protein. Express your answer in kilodaltons (kDa), rounded to three significant figures.", "solution": "We are told that the FGAP1 reference coding sequence produces a protein of length $L_{\\text{wt}}=950$ amino acids. A $1{,}500$ bp insertion occurs within exon 11 after nucleotide $120$. Coding is in triplets, so the number of amino acids encoded by the insertion is the number of codons it contains:\n$$\nL_{\\text{ins}}=\\frac{1{,}500\\ \\text{bp}}{3\\ \\text{bp/codon}}=500\\ \\text{amino acids}.\n$$\nBecause $120$ is divisible by $3$ and the insertion length is also a multiple of $3$, the reading frame is preserved. Bioinformatic analysis indicates no in-frame stop codons in the insertion, so translation proceeds through the insertion and continues normally. Therefore, the mutant protein length is\n$$\nL_{\\text{mut}}=L_{\\text{wt}}+L_{\\text{ins}}=950+500=1{,}450\\ \\text{amino acids}.\n$$\nUsing the average molecular weight per amino acid $m_{\\text{aa}}=110\\ \\text{Da/amino acid}$, the mutant protein molecular weight in Daltons is\n$$\nM_{\\text{mut}}=L_{\\text{mut}}\\,m_{\\text{aa}}=1{,}450\\times 110=159{,}500\\ \\text{Da}.\n$$\nConverting to kilodaltons,\n$$\nM_{\\text{mut}}=159.5\\ \\text{kDa}.\n$$\nRounding to three significant figures gives\n$$\n1.60\\times 10^{2}\\ \\text{kDa}.\n$$\nPer the instruction to report only the numerical value in the specified unit (kDa), we present $1.60\\times 10^{2}$.", "answer": "$$\\boxed{1.60 \\times 10^{2}}$$", "id": "1501350"}, {"introduction": "In modern genomics, combining the strengths of different technologies is a common and powerful approach. This exercise explores the concept of 'hybrid assembly,' where long reads are used to order and orient high-accuracy contigs generated from short-read sequencing. You will apply a probabilistic model to determine the likelihood of successfully bridging a gap between two contigs, providing insight into how sequencing experiments are designed and evaluated. [@problem_id:1501370]", "problem": "In a de novo genome assembly project for a novel extremophilic archaeon, an initial analysis using short-read sequencing has produced a high-quality set of contigs. To arrange these contigs into a complete chromosome, a bioinformatician employs long-read sequencing technology.\n\nConsider two specific contigs, Contig A and Contig B, that are believed to be adjacent in the genome. Based on other evidence, they are known to be separated by a single contiguous gap of length $L_g = 12,000$ base pairs (bp).\n\nTo bridge this gap, long-read sequencing is performed using a Pacific Biosciences (PacBio) instrument, which generates reads of a constant length $L_r = 15,000$ bp. The total dataset provides an average sequencing coverage of $C = 5$ across the genome.\n\nA \"bridging read\" is defined as a single long read that completely spans the gap between Contig A and Contig B. For this to occur, a read's starting point must be located in a position such that the read covers the entire $L_g$ region of the gap. Assume that the start positions of the long reads are randomly distributed across the genome, and the number of reads that successfully bridge the gap can be modeled as a random variable following a Poisson distribution.\n\nCalculate the probability that you obtain at least one bridging read for this specific gap from the sequencing experiment. Provide your answer as a decimal rounded to four significant figures.", "solution": "Let the genome length be $G$ and the total number of long reads be $N$. The average coverage is defined by\n$$\nC=\\frac{N L_{r}}{G}\\quad\\Rightarrow\\quad N=\\frac{C G}{L_{r}}.\n$$\nA read of length $L_{r}$ will bridge a gap of length $L_{g}$ if its start position falls within a window of length $L_{r}-L_{g}$ that ensures the read fully covers the entire gap. With start positions uniformly distributed over the genome, the probability that a given read is a bridging read is\n$$\np=\\frac{L_{r}-L_{g}}{G}.\n$$\nThe expected number of bridging reads is then\n$$\n\\lambda=N p=\\frac{C G}{L_{r}}\\cdot\\frac{L_{r}-L_{g}}{G}=C\\,\\frac{L_{r}-L_{g}}{L_{r}}.\n$$\nUnder the Poisson model, the probability of obtaining at least one bridging read is\n$$\nP(\\text{at least one})=1-\\exp(-\\lambda).\n$$\nSubstituting $C=5$, $L_{r}=15000$, and $L_{g}=12000$,\n$$\n\\lambda=5\\left(1-\\frac{12000}{15000}\\right)=5\\cdot 0.2=1,\n$$\nso\n$$\nP=1-\\exp(-1)\\approx 0.6321\\ \\text{(to four significant figures)}.\n$$", "answer": "$$\\boxed{0.6321}$$", "id": "1501370"}]}