{"hands_on_practices": [{"introduction": "A fundamental challenge in systems biology, especially in fields like pharmacogenomics, is to sift through vast amounts of genetic data to pinpoint the specific variations that influence a biological outcome. Decision trees offer an intuitive yet powerful machine learning framework for this kind of classification task. This exercise [@problem_id:1443753] walks you through the foundational step of building such a tree: calculating the information gain to identify the single most predictive feature from a set of candidates.", "problem": "In a pharmacogenomic study, researchers are investigating the influence of genetic variations on patient response to a new drug. They have collected data from 10 patients, focusing on three specific biallelic Single Nucleotide Polymorphisms (SNPs), which are variations at a single position in a DNA sequence. For each SNP, the allele is represented by either 0 or 1. The drug response for each patient is categorized as either 'High' or 'Low'.\n\nThe dataset is provided below:\n\n| Patient ID | SNP1 | SNP2 | SNP3 | Drug Response |\n|------------|------|------|------|---------------|\n| 1          | 0    | 0    | 1    | Low           |\n| 2          | 1    | 0    | 0    | Low           |\n| 3          | 0    | 0    | 1    | High          |\n| 4          | 1    | 0    | 0    | Low           |\n| 5          | 0    | 1    | 1    | High          |\n| 6          | 1    | 1    | 0    | High          |\n| 7          | 0    | 1    | 1    | High          |\n| 8          | 1    | 1    | 0    | High          |\n| 9          | 0    | 0    | 1    | Low           |\n| 10         | 1    | 1    | 1    | Low           |\n\nTo build a predictive model, a decision tree algorithm is chosen. The first step is to select the most informative SNP to serve as the root of the tree. This is determined by calculating the information gain for each SNP.\n\nCalculate the information gain for the SNP that provides the most information for classifying drug response. Your calculation should use the logarithm to the base 2. Round your final answer to three significant figures.", "solution": "We model the drug response $Y \\in \\{\\text{High}, \\text{Low}\\}$ and features $A \\in \\{\\text{SNP1, SNP2, SNP3}\\}$ with binary values $v \\in \\{0,1\\}$. The entropy of a binary distribution with probabilities $p$ and $1-p$ (using base-2 logarithms) is\n$$\nH(Y) = -\\sum_{y} P(y)\\log_{2}P(y).\n$$\nThe information gain of a feature $A$ is\n$$\nIG(A) = H(Y) - \\sum_{v \\in \\{0,1\\}} P(A=v)\\,H(Y \\mid A=v),\n$$\nwhere\n$$\nH(Y \\mid A=v) = -\\sum_{y} P(y \\mid A=v)\\log_{2}P(y \\mid A=v).\n$$\n\nFrom the dataset, there are $5$ High and $5$ Low responses, so\n$$\nH(Y) = -\\left(\\frac{1}{2}\\log_{2}\\frac{1}{2} + \\frac{1}{2}\\log_{2}\\frac{1}{2}\\right) = 1.\n$$\n\nFor SNP1: The value distribution is $P(\\text{SNP1}=0)=\\frac{5}{10}$ and $P(\\text{SNP1}=1)=\\frac{5}{10}$. For $\\text{SNP1}=0$, the class counts are High $3$, Low $2$, giving\n$$\nH(Y \\mid \\text{SNP1}=0) = -\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}.\n$$\nFor $\\text{SNP1}=1$, the class counts are High $2$, Low $3$, so\n$$\nH(Y \\mid \\text{SNP1}=1) = -\\frac{2}{5}\\log_{2}\\frac{2}{5} - \\frac{3}{5}\\log_{2}\\frac{3}{5},\n$$\nwhich is equal to the previous entropy. Therefore\n$$\nH(Y \\mid \\text{SNP1}) = \\frac{1}{2}\\left[-\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}\\right] + \\frac{1}{2}\\left[-\\frac{2}{5}\\log_{2}\\frac{2}{5} - \\frac{3}{5}\\log_{2}\\frac{3}{5}\\right]\n= -\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}.\n$$\nHence\n$$\nIG(\\text{SNP1}) = 1 - \\left[-\\frac{3}{5}\\log_{2}\\frac{3}{5} - \\frac{2}{5}\\log_{2}\\frac{2}{5}\\right].\n$$\nNumerically, $H(Y \\mid \\text{SNP1}) \\approx 0.970950594$, so $IG(\\text{SNP1}) \\approx 0.029049406$.\n\nFor SNP2: The value distribution is $P(\\text{SNP2}=0)=\\frac{5}{10}$ and $P(\\text{SNP2}=1)=\\frac{5}{10}$. For $\\text{SNP2}=0$, class counts are High $1$, Low $4$, so\n$$\nH(Y \\mid \\text{SNP2}=0) = -\\frac{1}{5}\\log_{2}\\frac{1}{5} - \\frac{4}{5}\\log_{2}\\frac{4}{5}.\n$$\nFor $\\text{SNP2}=1$, class counts are High $4$, Low $1$, giving the same entropy. Therefore\n$$\nH(Y \\mid \\text{SNP2}) = -\\frac{1}{5}\\log_{2}\\frac{1}{5} - \\frac{4}{5}\\log_{2}\\frac{4}{5},\n$$\nand\n$$\nIG(\\text{SNP2}) = 1 - \\left[-\\frac{1}{5}\\log_{2}\\frac{1}{5} - \\frac{4}{5}\\log_{2}\\frac{4}{5}\\right].\n$$\nNumerically, $H(Y \\mid \\text{SNP2}) \\approx 0.721928095$, so $IG(\\text{SNP2}) \\approx 0.278071905$.\n\nFor SNP3: The value distribution is $P(\\text{SNP3}=0)=\\frac{4}{10}$ and $P(\\text{SNP3}=1)=\\frac{6}{10}$. For $\\text{SNP3}=0$, class counts are High $2$, Low $2$, so\n$$\nH(Y \\mid \\text{SNP3}=0) = -\\frac{1}{2}\\log_{2}\\frac{1}{2} - \\frac{1}{2}\\log_{2}\\frac{1}{2} = 1.\n$$\nFor $\\text{SNP3}=1$, class counts are High $3$, Low $3$, so\n$$\nH(Y \\mid \\text{SNP3}=1) = -\\frac{1}{2}\\log_{2}\\frac{1}{2} - \\frac{1}{2}\\log_{2}\\frac{1}{2} = 1.\n$$\nThus\n$$\nH(Y \\mid \\text{SNP3}) = \\frac{4}{10}\\cdot 1 + \\frac{6}{10}\\cdot 1 = 1,\n$$\nand\n$$\nIG(\\text{SNP3}) = 1 - 1 = 0.\n$$\n\nThe SNP with the largest information gain is SNP2, with information gain $IG(\\text{SNP2}) \\approx 0.278071905$. Rounding to three significant figures gives $0.278$.", "answer": "$$\\boxed{0.278}$$", "id": "1443753"}, {"introduction": "While simple models are excellent for interpretation, many biological systems, such as gene regulatory networks (GRNs), are governed by complex, non-linear interactions that require more sophisticated tools. Neural networks excel at capturing these relationships, serving as powerful *in silico* models of biological processes. In this practice [@problem_id:1443740], you will work with a pre-trained neural network model of a GRN to simulate gene knockouts and quantify the system's overall robustness, a key concept in systems biology.", "problem": "A systems biologist has trained a small feed-forward Neural Network (NN) to model a core Gene Regulatory Network (GRN) within a synthetic bacterium. The model predicts the steady-state expression levels of two output genes, $G_4$ and $G_5$, based on the expression levels of three input transcription factor genes, $G_1$, $G_2$, and $G_3$.\n\nThe NN architecture is as follows:\n- An input layer with 3 neurons, representing the expression levels of $G_1, G_2, G_3$. Let the input vector be $x = [x_1, x_2, x_3]^T$.\n- A single hidden layer with 2 neurons.\n- An output layer with 2 neurons, representing the expression levels of $G_4, G_5$. Let the output vector be $y = [y_1, y_2]^T$.\n\nThe transformation from a layer $k-1$ to a layer $k$ is given by $y^{(k)} = f(W^{(k)}y^{(k-1)} + b^{(k)})$, where $y^{(0)} = x$, $W^{(k)}$ is the weight matrix, $b^{(k)}$ is the bias vector, and $f$ is the element-wise activation function. For this network, the activation function for both the hidden and output layers is the Rectified Linear Unit (ReLU), defined as $f(z) = \\max(0, z)$.\n\nThe trained parameters of the network are:\n- **From input to hidden layer:**\n  Weight matrix $W^{(1)} = \\begin{pmatrix} 1.0 & -1.2 & 0.4 \\\\ 0.5 & 0.8 & -2.0 \\end{pmatrix}$\n  Bias vector $b^{(1)} = \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix}$\n- **From hidden to output layer:**\n  Weight matrix $W^{(2)} = \\begin{pmatrix} 2.0 & 0.5 \\\\ -0.8 & 1.5 \\end{pmatrix}$\n  Bias vector $b^{(2)} = \\begin{pmatrix} 0.1 \\\\ -0.3 \\end{pmatrix}$\n\nThe \"wild-type\" (WT) strain of the bacterium has baseline input gene expression levels given by the vector $x_{WT} = [1.5, 2.0, 0.5]^T$.\n\nTo assess the robustness of this GRN, you will perform a series of *in silico* gene knockout experiments. A knockout of gene $G_i$ is modeled by setting its corresponding input expression level $x_i$ to zero, while keeping the other inputs at their WT levels.\n\nThe total output expression, $O$, for any given input is defined as the sum of the output gene expression levels: $O = y_1 + y_2$. The perturbation, $P_i$, caused by knocking out gene $G_i$ is defined as the absolute relative change in the total output expression: $P_i = \\frac{|O_{WT} - O_{KO,i}|}{O_{WT}}$, where $O_{WT}$ is the total output for the wild-type input and $O_{KO,i}$ is the total output for the knockout of gene $G_i$.\n\nThe overall robustness of the system, $R$, is defined as one minus the average perturbation over all possible single-gene knockouts of the input genes.\n\nCalculate the overall robustness $R$ of this GRN model. Express your answer as a dimensionless value rounded to three significant figures.", "solution": "The feed-forward computation for any input $x$ is given by the hidden pre-activation $a^{(1)} = W^{(1)}x + b^{(1)}$, hidden activation $h = f(a^{(1)})$, output pre-activation $a^{(2)} = W^{(2)}h + b^{(2)}$, and output $y = f(a^{(2)})$, where $f(z) = \\max(0, z)$ is applied element-wise. The total output is $O = y_{1} + y_{2}$, the perturbation for knocking out $G_{i}$ is $P_{i} = \\frac{|O_{WT} - O_{KO,i}|}{O_{WT}}$, and the robustness is $R = 1 - \\frac{1}{3}\\sum_{i=1}^{3} P_{i}$.\n\nWild-type input $x_{WT} = \\begin{pmatrix} 1.5 \\\\ 2.0 \\\\ 0.5 \\end{pmatrix}$:\n$$\nW^{(1)}x_{WT} = \\begin{pmatrix} 1.0(1.5) + (-1.2)(2.0) + 0.4(0.5) \\\\ 0.5(1.5) + 0.8(2.0) + (-2.0)(0.5) \\end{pmatrix}\n= \\begin{pmatrix} -0.7 \\\\ 1.35 \\end{pmatrix},\\quad\na^{(1)}_{WT} = \\begin{pmatrix} -0.7 \\\\ 1.35 \\end{pmatrix} + \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix} = \\begin{pmatrix} -0.9 \\\\ 1.45 \\end{pmatrix}.\n$$\nApplying ReLU gives $h_{WT} = \\begin{pmatrix} 0 \\\\ 1.45 \\end{pmatrix}$. Then\n$$\nW^{(2)}h_{WT} = \\begin{pmatrix} 2.0(0) + 0.5(1.45) \\\\ (-0.8)(0) + 1.5(1.45) \\end{pmatrix}\n= \\begin{pmatrix} 0.725 \\\\ 2.175 \\end{pmatrix},\\quad\na^{(2)}_{WT} = \\begin{pmatrix} 0.725 \\\\ 2.175 \\end{pmatrix} + \\begin{pmatrix} 0.1 \\\\ -0.3 \\end{pmatrix} = \\begin{pmatrix} 0.825 \\\\ 1.875 \\end{pmatrix}.\n$$\nThus $y_{WT} = \\begin{pmatrix} 0.825 \\\\ 1.875 \\end{pmatrix}$ and $O_{WT} = 0.825 + 1.875 = 2.7 = \\frac{27}{10}$.\n\nKnockout $G_{1}$: $x_{KO,1} = \\begin{pmatrix} 0 \\\\ 2.0 \\\\ 0.5 \\end{pmatrix}$.\n$$\nW^{(1)}x_{KO,1} = \\begin{pmatrix} 1.0(0) + (-1.2)(2.0) + 0.4(0.5) \\\\ 0.5(0) + 0.8(2.0) + (-2.0)(0.5) \\end{pmatrix}\n= \\begin{pmatrix} -2.2 \\\\ 0.6 \\end{pmatrix},\\quad\na^{(1)}_{KO,1} = \\begin{pmatrix} -2.2 \\\\ 0.6 \\end{pmatrix} + \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix} = \\begin{pmatrix} -2.4 \\\\ 0.7 \\end{pmatrix}.\n$$\nReLU gives $h_{KO,1} = \\begin{pmatrix} 0 \\\\ 0.7 \\end{pmatrix}$. Then\n$$\nW^{(2)}h_{KO,1} = \\begin{pmatrix} 2.0(0) + 0.5(0.7) \\\\ (-0.8)(0) + 1.5(0.7) \\end{pmatrix}\n= \\begin{pmatrix} 0.35 \\\\ 1.05 \\end{pmatrix},\\quad\na^{(2)}_{KO,1} = \\begin{pmatrix} 0.35 \\\\ 1.05 \\end{pmatrix} + \\begin{pmatrix} 0.1 \\\\ -0.3 \\end{pmatrix} = \\begin{pmatrix} 0.45 \\\\ 0.75 \\end{pmatrix}.\n$$\nThus $y_{KO,1} = \\begin{pmatrix} 0.45 \\\\ 0.75 \\end{pmatrix}$ and $O_{KO,1} = 0.45 + 0.75 = 1.2 = \\frac{12}{10}$. The perturbation is\n$$\nP_{1} = \\frac{|O_{WT} - O_{KO,1}|}{O_{WT}} = \\frac{\\left|\\frac{27}{10} - \\frac{12}{10}\\right|}{\\frac{27}{10}} = \\frac{\\frac{15}{10}}{\\frac{27}{10}} = \\frac{15}{27} = \\frac{5}{9}.\n$$\n\nKnockout $G_{2}$: $x_{KO,2} = \\begin{pmatrix} 1.5 \\\\ 0 \\\\ 0.5 \\end{pmatrix}$.\n$$\nW^{(1)}x_{KO,2} = \\begin{pmatrix} 1.0(1.5) + (-1.2)(0) + 0.4(0.5) \\\\ 0.5(1.5) + 0.8(0) + (-2.0)(0.5) \\end{pmatrix}\n= \\begin{pmatrix} 1.7 \\\\ -0.25 \\end{pmatrix},\\quad\na^{(1)}_{KO,2} = \\begin{pmatrix} 1.7 \\\\ -0.25 \\end{pmatrix} + \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ -0.15 \\end{pmatrix}.\n$$\nReLU gives $h_{KO,2} = \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix}$. Then\n$$\nW^{(2)}h_{KO,2} = \\begin{pmatrix} 2.0(1.5) + 0.5(0) \\\\ (-0.8)(1.5) + 1.5(0) \\end{pmatrix}\n= \\begin{pmatrix} 3.0 \\\\ -1.2 \\end{pmatrix},\\quad\na^{(2)}_{KO,2} = \\begin{pmatrix} 3.0 \\\\ -1.2 \\end{pmatrix} + \\begin{pmatrix} 0.1 \\\\ -0.3 \\end{pmatrix} = \\begin{pmatrix} 3.1 \\\\ -1.5 \\end{pmatrix}.\n$$\nThus $y_{KO,2} = \\begin{pmatrix} 3.1 \\\\ 0 \\end{pmatrix}$ and $O_{KO,2} = 3.1 = \\frac{31}{10}$. The perturbation is\n$$\nP_{2} = \\frac{|O_{WT} - O_{KO,2}|}{O_{WT}} = \\frac{\\left|\\frac{27}{10} - \\frac{31}{10}\\right|}{\\frac{27}{10}} = \\frac{\\frac{4}{10}}{\\frac{27}{10}} = \\frac{4}{27}.\n$$\n\nKnockout $G_{3}$: $x_{KO,3} = \\begin{pmatrix} 1.5 \\\\ 2.0 \\\\ 0 \\end{pmatrix}$.\n$$\nW^{(1)}x_{KO,3} = \\begin{pmatrix} 1.0(1.5) + (-1.2)(2.0) + 0.4(0) \\\\ 0.5(1.5) + 0.8(2.0) + (-2.0)(0) \\end{pmatrix}\n= \\begin{pmatrix} -0.9 \\\\ 2.35 \\end{pmatrix},\\quad\na^{(1)}_{KO,3} = \\begin{pmatrix} -0.9 \\\\ 2.35 \\end{pmatrix} + \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix} = \\begin{pmatrix} -1.1 \\\\ 2.45 \\end{pmatrix}.\n$$\nReLU gives $h_{KO,3} = \\begin{pmatrix} 0 \\\\ 2.45 \\end{pmatrix}$. Then\n$$\nW^{(2)}h_{KO,3} = \\begin{pmatrix} 2.0(0) + 0.5(2.45) \\\\ (-0.8)(0) + 1.5(2.45) \\end{pmatrix}\n= \\begin{pmatrix} 1.225 \\\\ 3.675 \\end{pmatrix},\\quad\na^{(2)}_{KO,3} = \\begin{pmatrix} 1.225 \\\\ 3.675 \\end{pmatrix} + \\begin{pmatrix} 0.1 \\\\ -0.3 \\end{pmatrix} = \\begin{pmatrix} 1.325 \\\\ 3.375 \\end{pmatrix}.\n$$\nThus $y_{KO,3} = \\begin{pmatrix} 1.325 \\\\ 3.375 \\end{pmatrix}$ and $O_{KO,3} = 1.325 + 3.375 = 4.7 = \\frac{47}{10}$. The perturbation is\n$$\nP_{3} = \\frac{|O_{WT} - O_{KO,3}|}{O_{WT}} = \\frac{\\left|\\frac{27}{10} - \\frac{47}{10}\\right|}{\\frac{27}{10}} = \\frac{\\frac{20}{10}}{\\frac{27}{10}} = \\frac{20}{27}.\n$$\n\nThe average perturbation over the three knockouts is\n$$\n\\overline{P} = \\frac{1}{3}\\left(\\frac{5}{9} + \\frac{4}{27} + \\frac{20}{27}\\right) = \\frac{1}{3}\\left(\\frac{15}{27} + \\frac{4}{27} + \\frac{20}{27}\\right) = \\frac{1}{3}\\cdot\\frac{39}{27} = \\frac{13}{27}.\n$$\nTherefore, the robustness is\n$$\nR = 1 - \\overline{P} = 1 - \\frac{13}{27} = \\frac{14}{27} \\approx 0.518518\\ldots\n$$\nRounded to three significant figures, $R = 0.519$.", "answer": "$$\\boxed{0.519}$$", "id": "1443740"}, {"introduction": "The power of complex models like neural networks often comes at the cost of interpretability, creating a \"black-box\" problem that can hinder their adoption in clinical settings. To build trust and gain biological insight, we need methods to explain why a model makes a specific prediction for an individual patient. This problem [@problem_id:1443750] introduces the concept of a local surrogate model, a popular technique to approximate and explain the behavior of any complex model in the vicinity of a single data point, thereby making its decision-making process transparent.", "problem": "A team of computational biologists has developed a complex, non-linear \"black-box\" machine learning model, denoted by $f(\\mathbf{x})$, to predict a patient's drug resistance score. The score is a continuous value, where higher values indicate greater resistance. The model's input, $\\mathbf{x} = (x_1, x_2)$, is a vector representing the normalized expression levels of two key genes, Gene-1 and Gene-2, derived from a patient's tumor biopsy.\n\nTo make the model's predictions interpretable for clinicians, the team uses a local surrogate model. For a specific patient, whose gene expression vector is $\\mathbf{x}_{\\text{pt}} = (10.0, 5.0)$, the black-box model predicts a high resistance score of $f(\\mathbf{x}_{\\text{pt}}) = 0.80$. To explain this prediction, they decide to approximate the behavior of $f$ in the immediate vicinity of $\\mathbf{x}_{\\text{pt}}$ with a simple linear model, $g(\\mathbf{z})$.\n\nThe linear model $g(\\mathbf{z})$ is designed to explain the deviation of the black-box model's output from the patient's predicted score. Its form is given by:\n$$g(\\mathbf{z}) = f(\\mathbf{x}_{\\text{pt}}) + w_1 (z_1 - x_{\\text{pt},1}) + w_2 (z_2 - x_{\\text{pt},2})$$\nwhere $\\mathbf{z}=(z_1, z_2)$ is a point in the neighborhood of $\\mathbf{x}_{\\text{pt}}$, and $w_1, w_2$ are the weights that represent the local importance of Gene-1 and Gene-2, respectively.\n\nThese weights are found by performing a weighted linear regression. A set of $N$ synthetic data points, $\\{\\mathbf{z}^{(i)}\\}_{i=1}^N$, are generated around $\\mathbf{x}_{\\text{pt}}$, and the black-box model $f$ is used to obtain their corresponding resistance scores, $\\{f(\\mathbf{z}^{(i)})\\}_{i=1}^N$. The weights $w_1$ and $w_2$ are those that minimize the weighted sum of squared errors:\n$$\\mathcal{L}(w_1, w_2) = \\sum_{i=1}^{N} \\pi_i \\left( f(\\mathbf{z}^{(i)}) - g(\\mathbf{z}^{(i)}) \\right)^2$$\nThe weight $\\pi_i$ for each synthetic point $\\mathbf{z}^{(i)}$ is determined by its proximity to the patient's data point $\\mathbf{x}_{\\text{pt}}$, calculated using an exponential kernel:\n$$\\pi_i = \\exp\\left(-\\frac{D(\\mathbf{x}_{\\text{pt}}, \\mathbf{z}^{(i)})^2}{\\sigma^2}\\right)$$\nwhere $D(\\mathbf{u}, \\mathbf{v})$ is the standard Euclidean distance between points $\\mathbf{u}$ and $\\mathbf{v}$, and the kernel width is given as $\\sigma = 0.2$.\n\nThe team generated the following three synthetic data points and their corresponding scores from the black-box model:\n1.  $\\mathbf{z}^{(1)} = (10.1, 5.0)$ with $f(\\mathbf{z}^{(1)}) = 0.75$\n2.  $\\mathbf{z}^{(2)} = (10.0, 5.1)$ with $f(\\mathbf{z}^{(2)}) = 0.82$\n3.  $\\mathbf{z}^{(3)} = (9.9, 5.2)$ with $f(\\mathbf{z}^{(3)}) = 0.85$\n\nYour task is to calculate the local importance weights, $w_1$ and $w_2$, for the two genes. Provide the values for $w_1$ and $w_2$ as your answer, rounded to three significant figures.", "solution": "We model the local deviation from $f(\\mathbf{x}_{\\text{pt}})$ by\n$$g(\\mathbf{z})=f(\\mathbf{x}_{\\text{pt}})+w_{1}(z_{1}-x_{\\text{pt},1})+w_{2}(z_{2}-x_{\\text{pt},2}).$$\nDefine for each synthetic point $\\mathbf{z}^{(i)}$ the offsets $d x_{i}=z^{(i)}_{1}-x_{\\text{pt},1}$, $d y_{i}=z^{(i)}_{2}-x_{\\text{pt},2}$ and the target $y_{i}=f(\\mathbf{z}^{(i)})-f(\\mathbf{x}_{\\text{pt}})$. Then the weighted least-squares problem\n$$\\min_{w_{1},w_{2}}\\sum_{i=1}^{N}\\pi_{i}\\left(y_{i}-w_{1} d x_{i}-w_{2} d y_{i}\\right)^{2}$$\nhas the solution $w=(w_{1},w_{2})^{\\top}=(X^{\\top}WX)^{-1}X^{\\top}Wy$, where\n$$X=\\begin{pmatrix}d x_{1} & d y_{1}\\\\ d x_{2} & d y_{2}\\\\ d x_{3} & d y_{3}\\end{pmatrix},\\quad W=\\operatorname{diag}(\\pi_{1},\\pi_{2},\\pi_{3}),\\quad y=\\begin{pmatrix}y_{1}\\\\ y_{2}\\\\ y_{3}\\end{pmatrix}.$$\nFrom the data, with $\\mathbf{x}_{\\text{pt}}=(10.0,5.0)$ and $f(\\mathbf{x}_{\\text{pt}})=0.80$, we have\n- $\\mathbf{z}^{(1)}=(10.1,5.0)$: $d x_{1}=0.1$, $d y_{1}=0$, $y_{1}=0.75-0.80=-0.05$.\n- $\\mathbf{z}^{(2)}=(10.0,5.1)$: $d x_{2}=0$, $d y_{2}=0.1$, $y_{2}=0.82-0.80=0.02$.\n- $\\mathbf{z}^{(3)}=(9.9,5.2)$: $d x_{3}=-0.1$, $d y_{3}=0.2$, $y_{3}=0.85-0.80=0.05$.\n\nKernel weights $\\pi_{i}$ use $\\pi_{i}=\\exp\\!\\left(-\\frac{D(\\mathbf{x}_{\\text{pt}},\\mathbf{z}^{(i)})^{2}}{\\sigma^{2}}\\right)$ with $\\sigma=0.2$ and Euclidean distance $D$. Distances are\n$$D_{1}^{2}=0.1^{2}=0.01,\\quad D_{2}^{2}=0.1^{2}=0.01,\\quad D_{3}^{2}=(-0.1)^{2}+0.2^{2}=0.05.$$\nThus\n$$\\pi_{1}=\\pi_{2}=\\exp(-0.25),\\quad \\pi_{3}=\\exp(-1.25).$$\nLet $A=\\exp(-0.25)$ and $B=\\exp(-1.25)$. Then\n$$X=\\begin{pmatrix}0.1 & 0\\\\ 0 & 0.1\\\\ -0.1 & 0.2\\end{pmatrix},\\quad W=\\operatorname{diag}(A,A,B),\\quad y=\\begin{pmatrix}-0.05\\\\ 0.02\\\\ 0.05\\end{pmatrix}.$$\nCompute the normal-equation components:\n$$S_{xx}=\\sum \\pi_{i} d x_{i}^{2}=0.01(A+B),\\quad S_{yy}=\\sum \\pi_{i} d y_{i}^{2}=0.01A+0.04B,\\quad S_{xy}=\\sum \\pi_{i} d x_{i} d y_{i}=-0.02B,$$\n$$b_{1}=\\sum \\pi_{i} d x_{i} y_{i}=-0.005(A+B),\\quad b_{2}=\\sum \\pi_{i} d y_{i} y_{i}=0.002A+0.01B.$$\nThe normal equations are\n$$\\begin{pmatrix}S_{xx} & S_{xy}\\\\ S_{xy} & S_{yy}\\end{pmatrix}\\begin{pmatrix}w_{1}\\\\ w_{2}\\end{pmatrix}=\\begin{pmatrix}b_{1}\\\\ b_{2}\\end{pmatrix},$$\nwith determinant\n$$\\Delta=S_{xx}S_{yy}-S_{xy}^{2}=0.0001A^{2}+0.0005AB=0.0001A(A+5B)>0.$$\nCramerâ€™s rule (or the $2\\times 2$ inverse) gives\n$$w_{1}=\\frac{b_{1}S_{yy}-b_{2}S_{xy}}{\\Delta}=-0.1\\cdot\\frac{5A+21B}{A+5B},\\qquad w_{2}=\\frac{S_{xx}b_{2}-S_{xy}b_{1}}{\\Delta}=0.2\\cdot\\frac{A+B}{A+5B}.$$\nSince $B=A\\exp(-1)$, writing $r=\\exp(-1)$, these simplify to\n$$w_{1}=-0.1\\cdot\\frac{5+21r}{1+5r},\\qquad w_{2}=0.2\\cdot\\frac{1+r}{1+5r}.$$\nWith $r=\\exp(-1)$, the numerical values are\n$$w_{1}\\approx -0.448175,\\qquad w_{2}\\approx 0.0963499.$$\nRounding to three significant figures yields\n$$w_{1}\\approx -0.448,\\qquad w_{2}\\approx 0.0963.$$", "answer": "$$\\boxed{\\begin{pmatrix}-0.448 & 0.0963\\end{pmatrix}}$$", "id": "1443750"}]}