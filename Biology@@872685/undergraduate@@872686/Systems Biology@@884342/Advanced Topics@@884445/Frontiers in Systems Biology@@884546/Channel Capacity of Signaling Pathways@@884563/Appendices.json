{"hands_on_practices": [{"introduction": "The first step in analyzing information flow through a signaling pathway is to quantify the statistical relationship between its input and output. This exercise [@problem_id:1422304] provides a foundational practice for this skill. Using a set of experimentally derived joint probabilities for a nutrient sensor, you will calculate the mutual information, which measures how much the state of a transcription factor tells us about the presence of an environmental nutrient.", "problem": "A systems biologist is studying a simple signaling pathway in a bacterium. The input to the pathway, which we can denote by a random variable $X$, is the presence or absence of a specific nutrient in the environment. The output, denoted by a random variable $Y$, is the activation state of a particular transcription factor (TF) that controls the expression of metabolic genes.\n\nThe input $X$ can take two states:\n- $X_0$: The nutrient is absent.\n- $X_1$: The nutrient is present.\n\nThe output $Y$ can also take two states:\n- $Y_0$: The transcription factor is inactive.\n- $Y_1$: The transcription factor is active.\n\nBased on a large number of single-cell observations under steady-state conditions, the biologist has determined the following joint probability distribution, $P(Y, X)$, for these events:\n- The probability of the TF being inactive AND the nutrient being absent, $P(Y=Y_0, X=X_0)$, is $0.45$.\n- The probability of the TF being inactive AND the nutrient being present, $P(Y=Y_0, X=X_1)$, is $0.10$.\n- The probability of the TF being active AND the nutrient being absent, $P(Y=Y_1, X=X_0)$, is $0.05$.\n- The probability of the TF being active AND the nutrient being present, $P(Y=Y_1, X=X_1)$, is $0.40$.\n\nUsing this information, calculate the amount of information that the state of the transcription factor provides about the presence of the nutrient. Express your answer in units of bits, rounded to three significant figures.", "solution": "We are asked for the mutual information between the input $X$ (nutrient presence) and the output $Y$ (TF state), which in bits is defined by\n$$\nI(X;Y)=\\sum_{x\\in\\{X_{0},X_{1}\\}}\\sum_{y\\in\\{Y_{0},Y_{1}\\}} P(x,y)\\,\\log_{2}\\!\\left(\\frac{P(x,y)}{P(x)P(y)}\\right).\n$$\nFrom the given joint distribution,\n$$\nP(Y_{0},X_{0})=0.45,\\quad P(Y_{0},X_{1})=0.10,\\quad P(Y_{1},X_{0})=0.05,\\quad P(Y_{1},X_{1})=0.40.\n$$\nCompute the marginals:\n$$\nP(X_{0})=0.45+0.05=0.50,\\quad P(X_{1})=0.10+0.40=0.50,\n$$\n$$\nP(Y_{0})=0.45+0.10=0.55,\\quad P(Y_{1})=0.05+0.40=0.45.\n$$\nPlugging into the mutual information formula, term by term,\n$$\n\\begin{aligned}\nI(X;Y)\n&=0.45\\,\\log_{2}\\!\\left(\\frac{0.45}{0.50\\cdot 0.55}\\right)\n+0.10\\,\\log_{2}\\!\\left(\\frac{0.10}{0.50\\cdot 0.55}\\right)\\\\\n&\\quad+0.05\\,\\log_{2}\\!\\left(\\frac{0.05}{0.50\\cdot 0.45}\\right)\n+0.40\\,\\log_{2}\\!\\left(\\frac{0.40}{0.50\\cdot 0.45}\\right).\n\\end{aligned}\n$$\nSimplify each likelihood ratio:\n$$\n\\frac{0.45}{0.50\\cdot 0.55}=\\frac{18}{11},\\quad\n\\frac{0.10}{0.50\\cdot 0.55}=\\frac{4}{11},\\quad\n\\frac{0.05}{0.50\\cdot 0.45}=\\frac{2}{9},\\quad\n\\frac{0.40}{0.50\\cdot 0.45}=\\frac{16}{9}.\n$$\nThus,\n$$\nI(X;Y)=0.45\\,\\log_{2}\\!\\left(\\frac{18}{11}\\right)\n+0.10\\,\\log_{2}\\!\\left(\\frac{4}{11}\\right)\n+0.05\\,\\log_{2}\\!\\left(\\frac{2}{9}\\right)\n+0.40\\,\\log_{2}\\!\\left(\\frac{16}{9}\\right).\n$$\nNumerically evaluating each term:\n$$\n\\log_{2}\\!\\left(\\frac{18}{11}\\right)\\approx 0.710493,\\quad\n\\log_{2}\\!\\left(\\frac{4}{11}\\right)\\approx -1.459432,\n$$\n$$\n\\log_{2}\\!\\left(\\frac{2}{9}\\right)= -2.169925,\\quad\n\\log_{2}\\!\\left(\\frac{16}{9}\\right)= 0.830075,\n$$\nso\n$$\nI(X;Y)\\approx 0.45(0.710493)+0.10(-1.459432)+0.05(-2.169925)+0.40(0.830075)\\approx 0.3973126.\n$$\nRounding to three significant figures yields $I(X;Y)\\approx 0.397$ bits.", "answer": "$$\\boxed{0.397}$$", "id": "1422304"}, {"introduction": "Biological signaling is inherently noisy, which limits its information-carrying capability. However, the amount of information transmitted is not fixed; it depends on the statistical properties of the input signal. This problem [@problem_id:1422306] models a kinase switch as a Binary Symmetric Channel and asks you to determine its channel capacityâ€”the theoretical maximum information it can transmit. This exercise shifts the focus from calculating mutual information for a single scenario to finding the optimal performance of the system.", "problem": "In a simplified model of a cellular signal transduction pathway, a kinase protein acts as a binary switch. Its activity state (output) depends on the presence or absence of a specific signaling molecule (input). Let the input state be \"present\" or \"absent\", and the output state be \"active\" or \"inactive\".\n\nThe signaling process is inherently noisy due to thermal fluctuations and stochastic molecular interactions.\n- When the signaling molecule is present, the kinase correctly becomes active with a probability of $0.90$.\n- When the signaling molecule is absent, the kinase correctly remains inactive with a probability of $0.90$.\n\nIn this system, an error occurs if the kinase is inactive when the molecule is present (a \"miss\") or if it is active when the molecule is absent (a \"false alarm\"). Assume the probabilities of these two types of errors are equal.\n\nBy modulating the frequency at which the cell is exposed to the signaling molecule, the system can be optimized to transmit the most information. What is the maximum amount of information, in bits, that the state of the kinase can convey about the presence or absence of the signaling molecule? Round your final answer to three significant figures.", "solution": "Let the input be a binary random variable $X \\in \\{0,1\\}$ with $X=1$ for molecule present and $X=0$ for absent. Let the output be $Y \\in \\{0,1\\}$ with $Y=1$ for kinase active and $Y=0$ for inactive. The problem states\n$$P(Y=1 \\mid X=1)=0.90, \\quad P(Y=0 \\mid X=0)=0.90,$$\nso the error probability in either direction is $p=0.10$. This defines a binary symmetric channel (BSC) with crossover probability $p=0.10$.\n\nLet $q=P(X=1)$ be the frequency of exposure. For a BSC, the mutual information is\n$$I(X;Y)=H(Y)-H(Y \\mid X).$$\nGiven $X \\sim \\text{Bernoulli}(q)$ and $Y=X \\oplus N$ with $N \\sim \\text{Bernoulli}(p)$ independent of $X$, we have\n$$P(Y=1)=q(1-p)+(1-q)p=p+q(1-2p),$$\nso\n$$H(Y)=H_{b}\\big(p+q(1-2p)\\big), \\quad H(Y \\mid X)=H_{b}(p),$$\nwhere the binary entropy in bits is\n$$H_{b}(x)=-x \\log_{2} x-(1-x) \\log_{2} (1-x).$$\nTherefore,\n$$I(X;Y)=H_{b}\\big(p+q(1-2p)\\big)-H_{b}(p).$$\nMaximizing over $q \\in [0,1]$, for a BSC the maximum occurs when $P(Y=1)=\\frac{1}{2}$, i.e.,\n$$p+q^{\\ast}(1-2p)=\\frac{1}{2} \\;\\;\\Rightarrow\\;\\; q^{\\ast}=\\frac{\\frac{1}{2}-p}{1-2p}=\\frac{1}{2} \\quad (\\text{for } p<\\tfrac{1}{2}).$$\nThus the channel capacity is\n$$C=\\max_{q} I(X;Y)=H_{b}\\left(\\tfrac{1}{2}\\right)-H_{b}(p)=1-H_{b}(p).$$\nFor $p=0.10$,\n$$H_{b}(0.10)=-0.10 \\log_{2}(0.10)-0.90 \\log_{2}(0.90).$$\nUsing $\\log_{2}(0.10)\\approx -3.32192809489$ and $\\log_{2}(0.90)\\approx -0.152003093445$, we get\n$$H_{b}(0.10)\\approx 0.332192809489+0.136802784101\\approx 0.468995593589.$$\nHence,\n$$C=1-H_{b}(0.10)\\approx 1-0.468995593589\\approx 0.531004406411 \\text{ bits},$$\nwhich to three significant figures is $0.531$ bits.", "answer": "$$\\boxed{0.531}$$", "id": "1422306"}, {"introduction": "The architecture of a signaling pathway directly dictates its function and its capacity to transmit information. Some pathways are designed not to transmit information about certain input features. This conceptual problem [@problem_id:1422356] explores the intriguing case of \"perfect adaptation,\" where a system's steady-state output is insensitive to the magnitude of a sustained input. By applying the principles of information theory, you will discover how this important biological property imposes a fundamental and stark limit on the pathway's channel capacity.", "problem": "In systems biology, a signaling pathway is said to exhibit \"perfect adaptation\" if its steady-state output is insensitive to the steady-state level of its input. Consider a hypothetical intracellular signaling pathway designed to respond to an external chemical stimulus.\n\nThe input to this pathway is a constant, sustained concentration of the stimulus, denoted by $S$. The output is the resulting steady-state concentration of an activated downstream protein, denoted by $R_{ss}$.\n\nThe defining characteristic of this pathway is its perfect adaptation property: for any constant stimulus concentration $S > 0$, the system's output eventually reaches a steady-state value $R_{ss}$ that is the same, regardless of the specific value of $S$. For $S=0$, the output is also this same steady-state value.\n\nChannel capacity, measured in bits, is defined as the maximum possible mutual information between the set of possible inputs and the corresponding outputs of a communication channel. What is the channel capacity for this pathway to transmit information about the steady-state value of the stimulus $S$ via the steady-state value of its response $R_{ss}$?\n\nA. 0 bits\n\nB. 1 bit\n\nC. Infinite bits\n\nD. The capacity depends on the kinetic parameters of the pathway's reactions.\n\nE. The capacity depends on the time it takes to reach the steady state.", "solution": "Let the input be the sustained stimulus concentration $S$ and the output be the steady-state response $R_{ss}$. Perfect adaptation implies that there exists a constant $R_{0}$ such that for every constant input $S \\geq 0$, the steady-state output satisfies\n$$\nR_{ss} = R_{0}.\n$$\nThus the channel mapping from $S$ to $R_{ss}$ is deterministic and constant. Equivalently, the conditional output distribution is independent of the input:\n$$\np_{R|S}(r|s) = \\delta(r - R_{0}) \\quad \\text{for all } s,\n$$\nso $p_{R|S}(r|s) = p_{R}(r)$ for all $s$, establishing statistical independence of $R$ and $S$ at steady state.\n\nThe mutual information between input $S$ and output $R$ is\n$$\nI(S;R) = \\int p_{S}(s) \\int p_{R|S}(r|s) \\ln \\left( \\frac{p_{R|S}(r|s)}{p_{R}(r)} \\right) \\, \\mathrm{d}r \\, \\mathrm{d}s.\n$$\nSince $p_{R|S}(r|s) = p_{R}(r)$ for all $s$, the logarithm is identically zero, and therefore\n$$\nI(S;R) = 0.\n$$\nEquivalently, using entropy, because $R$ is a constant almost surely, $H(R) = 0$ and for a deterministic channel $H(R|S) = 0$, hence\n$$\nI(S;R) = H(R) - H(R|S) = 0 - 0 = 0.\n$$\nChannel capacity is the supremum of mutual information over all input distributions $p_{S}$:\n$$\nC = \\sup_{p_{S}} I(S;R) = 0.\n$$\nTherefore, the channel cannot transmit any information about the steady-state value of $S$ via the steady-state output $R_{ss}$. Among the choices, this corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1422356"}]}