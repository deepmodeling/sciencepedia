{"hands_on_practices": [{"introduction": "A scientific claim is only as strong as the evidence supporting it, and a mean value presented without a measure of uncertainty can be misleading. This exercise simulates a common scenario in which a published result lacks essential information like error bars or sample size, making it impossible to assess its reliability. By working backward from hypothetical raw data, you will practice calculating a confidence interval, a fundamental tool for quantifying the uncertainty around an estimate and a critical component of reproducible research [@problem_id:1422054].", "problem": "A paper published in a cell biology journal claims that a novel drug, \"Inhibitor-7,\" significantly reduces the expression level of a protein called \"Kinase-Z.\" The paper presents a simple bar chart showing the mean expression level for the control group was 120 arbitrary fluorescence units (AFU), while the mean for the group treated with Inhibitor-7 was 85 AFU. Critically, the publication failed to report the sample size, the variance, or any form of error bars, making it difficult to assess the reliability of the finding.\n\nBy a stroke of luck, you uncover the original lab notebook which contains the raw data for the five samples in the drug-treated group. The measured expression levels of Kinase-Z for these five samples were:\n82, 95, 78, 80, 90 (all in AFU).\n\nTo properly evaluate the claim, calculate the upper bound of the 95% confidence interval for the true mean expression level of Kinase-Z in the population treated with Inhibitor-7. For your calculation, you may use the critical t-value for a 95% confidence level with 4 degrees of freedom, which is $t = 2.776$.\n\nExpress your answer in arbitrary fluorescence units (AFU), rounded to three significant figures.", "solution": "We are asked for the upper bound of the two-sided 95 percent confidence interval for the true mean based on a small sample, so we use the Student t interval. For a sample of size $n$, the $95$ percent CI has upper bound\n$$\n\\bar{x} + t_{\\alpha/2,\\,n-1}\\,\\frac{s}{\\sqrt{n}},\n$$\nwhere $\\bar{x}$ is the sample mean, $s$ is the sample standard deviation computed with $n-1$ in the denominator, and $t_{\\alpha/2,\\,n-1}$ is the critical t-value. Here $n=5$ and $t_{0.975,\\,4}=2.776$.\n\nLet the observed values be $x_{1}=82$, $x_{2}=95$, $x_{3}=78$, $x_{4}=80$, $x_{5}=90$ (in AFU). The sample mean is\n$$\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{5}x_{i}=\\frac{82+95+78+80+90}{5}=\\frac{425}{5}=85.\n$$\nCompute the sum of squared deviations:\n$$\n\\sum_{i=1}^{5}(x_{i}-\\bar{x})^{2}=(-3)^{2}+10^{2}+(-7)^{2}+(-5)^{2}+5^{2}=9+100+49+25+25=208.\n$$\nThe unbiased sample variance and standard deviation are\n$$\ns^{2}=\\frac{1}{n-1}\\sum_{i=1}^{5}(x_{i}-\\bar{x})^{2}=\\frac{208}{4}=52,\\quad s=\\sqrt{52}.\n$$\nThe standard error of the mean is\n$$\n\\mathrm{SE}=\\frac{s}{\\sqrt{n}}=\\frac{\\sqrt{52}}{\\sqrt{5}}=\\sqrt{\\frac{52}{5}}.\n$$\nTherefore, the upper bound of the $95$ percent confidence interval is\n$$\nU=\\bar{x}+t_{0.975,\\,4}\\,\\mathrm{SE}=85+2.776\\,\\sqrt{\\frac{52}{5}}.\n$$\nNumerically, $\\sqrt{\\frac{52}{5}}\\approx 3.224903099$, so\n$$\nU\\approx 85+2.776\\times 3.224903099\\approx 85+8.95233\\approx 93.95233.\n$$\nRounded to three significant figures, the upper bound is $94.0$ (in AFU).", "answer": "$$\\boxed{94.0}$$", "id": "1422054"}, {"introduction": "Before diving into complex statistical analyses, an essential first step in any high-throughput experiment is data quality control. Principal Component Analysis (PCA) is a powerful exploratory tool that provides a \"bird's-eye view\" of your data, often revealing unexpected patterns, outliers, or batch effects. This problem places you in the role of a data detective, using a PCA plot to diagnose a glaring inconsistency and deduce the most probable and simple experimental error that occurred, a crucial skill for troubleshooting in systems biology [@problem_id:1422075].", "problem": "A team of student researchers is conducting a study on the transcriptomic differences between healthy human bronchial epithelial cells and those infected with a common respiratory virus. They collect 20 independent cell culture samples. Ten samples are mock-infected and labeled `Control_01` through `Control_10`. The other ten are infected with the virus and labeled `Infected_01` through `Infected_10`. After 24 hours, they perform RNA sequencing (RNA-Seq) to measure the expression levels of thousands of genes in each sample.\n\nTo get a high-level view of their data, they perform a Principal Component Analysis (PCA), a dimensionality reduction technique used to visualize the overall variation between samples. When they plot the first two principal components, they observe two distinct and well-separated clusters of data points. As expected, nine of the `Control` samples form one cluster, and all ten `Infected` samples form the other. However, the data point for the sample originally labeled `Control_08` is located squarely within the center of the `Infected` sample cluster.\n\nAssuming there are no major errors in the computational analysis pipeline (e.g., the PCA algorithm was run correctly) and no undiscovered, complex biological phenomena at play, which of the following represents the most probable and parsimonious (simplest) explanation for this observation?\n\nA. A rare spontaneous mutation occurred in the `Control_08` cell line, causing its gene expression profile to coincidentally converge with that of virus-infected cells.\n\nB. During the RNA-Seq data processing, a random hardware error (e.g., a bit-flip in the computer's memory) corrupted the data file for `Control_08` in a way that made it resemble an infected sample.\n\nC. The RNA extraction procedure failed for `Control_08`, leading to no genetic material being sequenced and the analysis software assigning it a default position in the middle of the plot.\n\nD. The `Control_08` sample was mislabeled at some point during the experimental workflow (e.g., sample collection, plating, or harvesting) and is, in fact, an infected sample that was given the wrong label.\n\nE. The sequencing machine introduced a systematic bias that specifically affected only the `Control_08` sample, altering its entire gene expression profile.", "solution": "We first interpret the PCA result in terms of the underlying data. PCA maps high-dimensional gene expression profiles to a low-dimensional space by finding orthogonal directions that capture maximal variance. When samples form two well-separated clusters corresponding to control and infected groups, it indicates that the dominant axes of variation in the data reflect infection status and that within-group biological and technical variability is small relative to the infection effect.\n\nThe observation that nine control samples cluster together and all ten infected samples cluster together, while the sample labeled Control_08 lies squarely in the center of the infected cluster, implies that the global transcriptome of Control_08 is highly similar to those of infected samples and dissimilar to the other controls. Being in the center of the infected cluster specifically indicates close overall concordance across many genes, not merely a partial resemblance.\n\nWe now evaluate each proposed explanation against empirical plausibility, typical failure modes, and parsimony:\n\nA. A rare spontaneous mutation in Control_08 causing its expression to match infected cells is highly implausible. Viral infection induces coordinated, genome-wide changes involving antiviral responses, metabolic rewiring, and other pathway-level shifts across many genes. A single spontaneous mutation would not predictably reproduce this broad infected-like program across thousands of genes, nor would it place the sample at the cluster center of infected samples. This lacks parsimony.\n\nB. A random hardware error corrupting the Control_08 data to specifically resemble an infected profile is extraordinarily unlikely. Random bit-flips are effectively random noise and would not produce a coherent, biologically structured pattern that aligns closely with the infected cluster centroid. Moreover, such corruption typically produces detectable file errors or nonsensical metrics rather than a clean, class-consistent profile.\n\nC. RNA extraction failure would yield extremely low counts or missing data, which downstream QC would flag. PCA does not assign default positions; samples with little informative signal typically appear as outliers or are excluded, and they would not fall squarely at the center of a biologically coherent cluster. Thus, this is incompatible with the precise infected-like placement observed.\n\nD. Mislabeling of Control_08 is the most parsimonious and common explanation consistent with the data. Label swaps or mis-tracking during collection, plating, or harvesting are well-documented practical issues in experimental workflows. If Control_08 was actually infected but mislabeled, it should cluster with infected samples; being in the cluster center is exactly what one expects for a typical infected sample.\n\nE. A sequencing-machine-induced systematic bias affecting only Control_08 is unlikely. Platform or lane effects usually influence batches of samples rather than a single sample, and such effects do not typically transform a control profile into one that closely matches the infected centroid. A device-specific artifact so selectively aligned with the infected signal in one sample would be non-parsimonious.\n\nGiven the assumptions that the computational pipeline is correct and no undiscovered complex biology is at play, the explanation with the highest plausibility and parsimony is a sample mislabeling event in the workflow. Therefore, the most probable explanation is that Control_08 is actually an infected sample that was given the wrong label.", "answer": "$$\\boxed{D}$$", "id": "1422075"}, {"introduction": "When analyzing experiments with multiple conditions or time points, it can be tempting to compare every pair of groups with a simple statistical test. However, this approach contains a hidden statistical trap that can severely compromise the validity of your conclusions. This exercise exposes the \"multiple comparisons problem,\" demonstrating how performing numerous tests on the same dataset dramatically increases the probability of finding false positives, and underscores the need for appropriate statistical strategies to ensure your findings are robust and not merely due to chance [@problem_id:1422062].", "problem": "A team of systems biologists is investigating the temporal dynamics of a signaling pathway in response to a specific drug. They conduct a time-course experiment, collecting cell samples at 6 different time points: 0 hours (pre-treatment control), 2 hours, 4 hours, 8 hours, 16 hours, and 24 hours post-treatment. At each time point, they prepare three independent biological replicates. For a particular protein of interest, 'Protein P', they measure its phosphorylation level using a quantitative assay.\n\nA student on the team is tasked with determining when the phosphorylation of Protein P changes significantly. To do this, they decide to perform an independent two-sample t-test for every possible pair of time points (e.g., 0h vs 2h, 0h vs 4h, 2h vs 4h, etc.). They set a significance level, $\\alpha$, of 0.05 for each test. If the p-value for any comparison is less than 0.05, they conclude that there is a significant difference in phosphorylation between those two time points.\n\nWhat is the single most significant statistical flaw in this analytical strategy?\n\nA. The use of an independent two-sample t-test is incorrect; a paired t-test is required because the data are collected over time from the same experiment.\n\nB. The normality assumption of the t-test is likely violated by protein phosphorylation data, rendering the p-values invalid.\n\nC. The statistical power of the analysis is too low to detect real differences, as there are only three biological replicates per time point.\n\nD. The approach fails to correct for the multiple comparisons problem, leading to a high probability of finding false-positive results (Type I errors).\n\nE. It is statistically invalid to compare any time points other than the 0-hour control group; all comparisons must be made against the baseline.", "solution": "Let the number of time points be $T=6$. The student performs all pairwise comparisons, so the total number of independent hypothesis tests is the number of unordered pairs:\n$$\nm=\\binom{T}{2}=\\binom{6}{2}=15.\n$$\nFor each test, the per-comparison Type I error rate is set to $\\alpha=0.05$. When multiple tests are conducted without adjustment, the family-wise error rate (FWER), the probability of at least one false positive among the $m$ tests, satisfies\n$$\n\\text{FWER}=P(\\text{at least one false positive})=1-(1-\\alpha)^{m},\n$$\nunder independence of tests. More generally, by the Bonferroni inequality,\n$$\n\\text{FWER}\\leq m\\alpha.\n$$\nIn either case, with $m=15$ and $\\alpha=0.05$, the FWER is substantially larger than $\\alpha$, indicating a high probability of at least one false-positive result when no correction for multiple comparisons is applied. Therefore, the single most significant statistical flaw is the failure to address the multiple comparisons problem.\n\nRegarding the other options:\n- A is not the primary issue: with three independent biological replicates per time point and no explicit pairing across time, an independent two-sample $t$-test is not inherently invalid; a paired test requires matched measurements on the same experimental units.\n- B is not necessarily the main flaw: while normality is an assumption, the $t$-test is often reasonably robust, and the dominant problem here is multiplicity, not distributional form.\n- C concerns statistical power, which may be limited with three replicates, but low power increases Type II errors, not false-positive inflation; it is not the most significant flaw relative to the uncorrected multiple testing.\n- E is incorrect: comparisons among non-baseline time points are not inherently invalid; what is required is an analysis plan that controls error rates (e.g., ANOVA, linear mixed models, or adjusted pairwise tests).\n\nThus, the most critical flaw is the absence of multiple-comparisons correction leading to inflated Type I error.", "answer": "$$\\boxed{D}$$", "id": "1422062"}]}