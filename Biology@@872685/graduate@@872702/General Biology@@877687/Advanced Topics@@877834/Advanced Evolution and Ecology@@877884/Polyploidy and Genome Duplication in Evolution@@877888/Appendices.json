{"hands_on_practices": [{"introduction": "Whole-genome duplication often begins with a rare meiotic accident: the failure of chromosome reduction, which produces an unreduced gamete. This exercise challenges you to apply first principles of population genetics to quantify the initial emergence of polyploids in a diploid population [@problem_id:2825719]. By modeling the random fusion of gametes, you will derive the expected frequency of new tetraploid zygotes as a function of the error rate $r$, providing a crucial quantitative foundation for understanding how polyploid lineages originate.", "problem": "In a large, randomly mating diploid population undergoing no selection, assume that each meiosis produces an unreduced gamete of ploidy $2n$ with probability $r$ and a reduced gamete of ploidy $n$ with probability $1 - r$. Assume that this probability $r$ is the same for gametes contributed by all individuals and, if there are separate sexes, is equal in the two sexes. Mating occurs by random union of gametes (as in Hardy–Weinberg Equilibrium (HWE)), and all formed zygotes are equally viable at the moment of formation.\n\nUsing only the principles that (i) meiosis in diploids normally produces haploid gametes and may produce unreduced gametes at a per-gamete probability $r$, and (ii) random mating corresponds to random union of gametes so that joint events factor as products of marginal probabilities, derive from first principles the expected frequency (as a function of $r$) of newly formed tetraploid ($4n$) zygotes (neo-tetraploids) among all zygotes formed in a single generation.\n\nExpress your final answer as a closed-form analytic expression in terms of $r$. Do not round. No units are required.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\nThe givens are as follows:\n1.  The population is diploid ($2n$), large, and undergoes random mating.\n2.  There is no selection.\n3.  During meiosis, an individual produces unreduced, diploid ($2n$) gametes with probability $r$.\n4.  During meiosis, an individual produces reduced, haploid ($n$) gametes with probability $1-r$.\n5.  The probability $r$ is constant for all individuals and sexes.\n6.  Mating is by random union of gametes.\n7.  All zygotes formed are equally viable.\n8.  The task is to derive the expected frequency of newly formed tetraploid ($4n$) zygotes as a function of $r$.\n\nThe problem is evaluated against the validation criteria:\n-   **Scientific Grounding**: The problem is well-grounded in the principles of population genetics and evolutionary biology. The formation of polyploids via unreduced gametes is a known and fundamental mechanism, particularly in plants. The use of a Hardy-Weinberg-like framework (random union of gametes) is a standard and appropriate modeling approach. The problem does not violate any scientific laws or contain factual inaccuracies.\n-   **Well-Posedness**: The problem is well-posed. It clearly defines the initial state (a diploid population), the process (gamete formation with a specified probability of non-reduction), and the outcome to be calculated (frequency of a specific zygote type). All necessary parameters ($r$) are provided, and the question leads to a unique, derivable solution.\n-   **Objectivity**: The problem is stated using precise, objective, and unambiguous mathematical language. There are no subjective or opinion-based elements.\n\nThe verdict is that the problem is **valid**. It is a standard, solvable problem in theoretical population genetics. We may now proceed to the solution.\n\nThe solution is derived from first principles as requested. The population of diploid ($2n$) individuals produces a common gamete pool. The composition of this pool is determined by the probability of producing either reduced or unreduced gametes.\n\nLet $P(n)$ be the frequency of reduced, haploid ($n$) gametes in the pool.\nLet $P(2n)$ be the frequency of unreduced, diploid ($2n$) gametes in the pool.\nAccording to the problem statement:\n$$P(n) = 1 - r$$\n$$P(2n) = r$$\nThe sum of these frequencies is $(1-r) + r = 1$, as required.\n\nThe principle of random mating is stated to be equivalent to the random union of gametes. This implies that the formation of a zygote can be modeled as two independent random draws from the gamete pool. The probability of any specific combination of two gametes is the product of their individual frequencies in the pool.\n\nA zygote's ploidy is the sum of the ploidies of the two gametes that fuse to form it. We seek the frequency of newly formed tetraploid ($4n$) zygotes. A tetraploid zygote can only be formed by the fusion of two diploid ($2n$) gametes.\n\nLet $Z_{4n}$ denote the event that a newly formed zygote is tetraploid. This event occurs if and only if the first randomly selected gamete has ploidy $2n$ AND the second randomly selected gamete has ploidy $2n$.\n\nThe probability of this event, which corresponds to the expected frequency in a large population, is calculated as follows:\n$$P(Z_{4n}) = P(\\text{gamete 1 is } 2n) \\times P(\\text{gamete 2 is } 2n)$$\nSince the selection of the two gametes are independent events, we can substitute the frequency of $2n$ gametes for each probability:\n$$P(Z_{4n}) = P(2n) \\times P(2n)$$\n$$P(Z_{4n}) = r \\times r = r^2$$\n\nThis is the frequency of neo-tetraploids among all zygotes formed in a single generation.\n\nTo confirm the logical consistency of this framework, we can calculate the frequencies of all possible zygote types and verify that they sum to $1$.\n1.  **Diploid ($2n$) Zygotes:** Formed by the fusion of two haploid ($n$) gametes.\n    Frequency = $P(n) \\times P(n) = (1-r)(1-r) = (1-r)^2$.\n2.  **Triploid ($3n$) Zygotes:** Formed by the fusion of one haploid ($n$) gamete and one diploid ($2n$) gamete. This can occur in two ways (first gamete is $n$ and second is $2n$, or vice versa).\n    Frequency = $[P(n) \\times P(2n)] + [P(2n) \\times P(n)] = (1-r)r + r(1-r) = 2r(1-r)$.\n3.  **Tetraploid ($4n$) Zygotes:** Formed by the fusion of two diploid ($2n$) gametes.\n    Frequency = $P(2n) \\times P(2n) = r \\times r = r^2$.\n\nThe sum of all zygote frequencies is:\n$$ \\text{Total Frequency} = (1-r)^2 + 2r(1-r) + r^2 $$\nThis is the binomial expansion of $((1-r) + r)^2$.\n$$ \\text{Total Frequency} = (1 - 2r + r^2) + (2r - 2r^2) + r^2 $$\n$$ \\text{Total Frequency} = 1 - 2r + 2r + r^2 - 2r^2 + r^2 = 1 $$\nThe frequencies sum to $1$, which confirms the derivation is self-consistent and correct. The expected frequency of newly formed tetraploid ($4n$) zygotes is therefore $r^2$.", "answer": "$$\\boxed{r^{2}}$$", "id": "2825719"}, {"introduction": "Once a polyploid lineage is established, its genetics diverge significantly from its diploid ancestors due to multivalent chromosome pairings during meiosis. This practice delves into the fascinating and non-Mendelian world of tetrasomic inheritance, where alleles segregate in complex patterns [@problem_id:2825718]. By calculating the expected progeny ratios from a self-fertilized autotetraploid, you will gain hands-on experience with concepts like double reduction, a unique feature of polyploid meiosis that reshuffles genetic variation in novel ways.", "problem": "An autotetraploid individual with four homologous chromosomes at a single locus has simplex dosage genotype $Aaaa$ (one copy of allele $A$ and three copies of allele $a$). Meiosis proceeds via tetravalent pairing and random chromatid segregation (RCS), and the locus is located sufficiently distal to the centromere that the per-gamete probability of double reduction is $\\alpha = \\frac{1}{6}$, a standard result for tetravalent pairing with RCS at terminal loci. Double reduction is defined as the event that the two allelic copies at the locus carried by a gamete derive from sister chromatids of the same original homologue, making them identical by descent.\n\nUsing only first principles of tetrasomic inheritance, symmetry among homologues, and combinatorial counting of homologue contributions to gametes, derive the expected segregation among selfed progeny of this $Aaaa$ individual, expressed as the probability distribution over zygotic allele dosages $k \\in \\{0,1,2,3,4\\}$ copies of $A$ at the locus, where $k=0$ denotes $aaaa$, $k=1$ denotes $Aaaa$, $k=2$ denotes $AAaa$, $k=3$ denotes $AAAa$, and $k=4$ denotes $AAAA$.\n\nProvide your final answer as a single row matrix of five exact probabilities in the order $[P(k=0)\\; P(k=1)\\; P(k=2)\\; P(k=3)\\; P(k=4)]$. Do not approximate and do not include any units.", "solution": "We start from the definitions of tetrasomic inheritance in autotetraploids and the structure of meiosis. An autotetraploid has four homologous chromosomes at a locus. Under tetravalent pairing with random chromatid segregation (RCS), a locus located far from the centromere experiences double reduction with per-gamete probability $\\alpha = \\frac{1}{6}$; this is a well-established upper bound when the locus is terminal.\n\nLet the parental genotype be $Aaaa$, meaning one homologue carries allele $A$ and the other three carry allele $a$. A gamete in a tetraploid carries two allelic copies at the locus (two chromatids at that locus segment). Define $X$ as the number of $A$ alleles transmitted to a gamete from the $Aaaa$ parent. Then $X \\in \\{0,1,2\\}$.\n\nWe partition the meiosis into two mutually exclusive cases: no double reduction and double reduction.\n\n1. Case 1 (no double reduction): With probability $1 - \\alpha$, the two alleles in the gamete derive from two different homologues. Because there are four homologues (one $A$-bearing and three $a$-bearing), the unordered pair of homologues contributing to the gamete is a uniformly random pair from the $\\binom{4}{2} = 6$ possible pairs. Of these pairs, exactly $\\binom{3}{1} = 3$ include the $A$ homologue, and $3$ exclude it. Therefore, conditional on no double reduction,\n   - $P(X=1 \\mid \\text{no double reduction}) = \\frac{3}{6} = \\frac{1}{2}$,\n   - $P(X=0 \\mid \\text{no double reduction}) = \\frac{3}{6} = \\frac{1}{2}$,\n   - $P(X=2 \\mid \\text{no double reduction}) = 0$.\n\n2. Case 2 (double reduction): With probability $\\alpha$, the two alleles in the gamete derive from sister chromatids of the same homologue. By symmetry among homologues, the identity of the contributing homologue is uniformly distributed among the four homologues. Therefore,\n   - $P(\\text{contributing homologue is the }A\\text{ homologue} \\mid \\text{double reduction}) = \\frac{1}{4}$, which yields $X=2$,\n   - $P(\\text{contributing homologue is one of the }a\\text{ homologues} \\mid \\text{double reduction}) = \\frac{3}{4}$, which yields $X=0$.\n\nCombining the two cases by the law of total probability, we obtain the gametic distribution from an $Aaaa$ parent:\n- $P(X=2) = \\alpha \\cdot \\frac{1}{4} = \\frac{\\alpha}{4}$,\n- $P(X=1) = (1 - \\alpha) \\cdot \\frac{1}{2}$,\n- $P(X=0) = (1 - \\alpha) \\cdot \\frac{1}{2} + \\alpha \\cdot \\frac{3}{4} = \\frac{1}{2} + \\frac{\\alpha}{4}$.\n\nSubstituting $\\alpha = \\frac{1}{6}$ (RCS at a distal locus),\n- $P(X=2) = \\frac{1}{6} \\cdot \\frac{1}{4} = \\frac{1}{24}$,\n- $P(X=1) = \\left(1 - \\frac{1}{6}\\right) \\cdot \\frac{1}{2} = \\frac{5}{6} \\cdot \\frac{1}{2} = \\frac{5}{12}$,\n- $P(X=0) = \\frac{1}{2} + \\frac{1}{6} \\cdot \\frac{1}{4} = \\frac{1}{2} + \\frac{1}{24} = \\frac{13}{24}$.\n\nNow consider self-fertilization of the $Aaaa$ parent. Under independence between the two meiotic products that form the zygote, the zygotic dosage $K$ equals the sum of the independent gametic counts: $K = X_{1} + X_{2}$, where $X_{1}$ and $X_{2}$ are independent and identically distributed with the probabilities above. Thus $K \\in \\{0,1,2,3,4\\}$. We compute the distribution of $K$ by convolution:\n\n- $P(K=0) = P(X=0)^{2} = \\left(\\frac{13}{24}\\right)^{2} = \\frac{169}{576}$.\n- $P(K=1) = 2 P(X=0) P(X=1) = 2 \\cdot \\frac{13}{24} \\cdot \\frac{5}{12} = \\frac{130}{288} = \\frac{65}{144} = \\frac{260}{576}$.\n- $P(K=2) = 2 P(X=0) P(X=2) + P(X=1)^{2} = 2 \\cdot \\frac{13}{24} \\cdot \\frac{1}{24} + \\left(\\frac{5}{12}\\right)^{2} = \\frac{26}{576} + \\frac{25}{144} = \\frac{26}{576} + \\frac{100}{576} = \\frac{126}{576} = \\frac{7}{32}$.\n- $P(K=3) = 2 P(X=1) P(X=2) = 2 \\cdot \\frac{5}{12} \\cdot \\frac{1}{24} = \\frac{10}{288} = \\frac{5}{144} = \\frac{20}{576}$.\n- $P(K=4) = P(X=2)^{2} = \\left(\\frac{1}{24}\\right)^{2} = \\frac{1}{576}$.\n\nAs a check, the numerators $169 + 260 + 126 + 20 + 1 = 576$ sum to the common denominator $576$, confirming that the probabilities sum to $1$.\n\nFinally, mapping $K$ to genotypes in the order $[aaaa,\\; Aaaa,\\; AAaa,\\; AAAa,\\; AAAA]$ corresponds to $[K=0,\\; K=1,\\; K=2,\\; K=3,\\; K=4]$, so the requested row matrix of exact probabilities is\n$$\n\\left[\\frac{169}{576}\\;\\; \\frac{260}{576}\\;\\; \\frac{126}{576}\\;\\; \\frac{20}{576}\\;\\; \\frac{1}{576}\\right].\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{169}{576} & \\frac{260}{576} & \\frac{126}{576} & \\frac{20}{576} & \\frac{1}{576}\\end{pmatrix}}$$", "id": "2825718"}, {"introduction": "The legacy of ancient whole-genome duplications is imprinted in the structure of modern genomes, often visible as a distinct peak in the age distribution of duplicate genes, measured by synonymous divergence ($K_s$). This computational exercise guides you through the process of statistically testing for such a WGD signature using mixture models, a powerful tool in comparative genomics [@problem_id:2825757]. You will implement a decision framework based on the Bayesian Information Criterion (BIC) to distinguish a true WGD event from the continuous background noise of small-scale duplications, translating a biological hypothesis into a rigorous, data-driven test.", "problem": "You are given the following scientific context and asked to translate it into a rigorous statistical decision procedure that can be implemented programmatically. In comparative genomics, duplicate gene pairs accumulate synonymous substitutions over time. Under the Molecular Clock approximation, the expected number of synonymous substitutions per synonymous site, denoted by $K_s$, increases approximately linearly with time for neutral sites. Small-scale gene duplications are expected to occur continuously over evolutionary time, whereas a Whole-Genome Duplication (WGD) event produces a pulse of duplicate births concentrated in a short time window. Consequently, the empirical distribution of $K_s$ among duplicate pairs can often be modeled as a mixture of latent processes. The question is whether a prominent peak near $K_s \\approx 0.8$ is better explained by a distinct WGD component or arises from background processes alone.\n\nFrom first principles, assume the following foundational base:\n- Duplicate birth times arise from latent processes with distinct age distributions. Under multiplicative noise in rates and times, a log-normal mixture is a reasonable approximation for the marginal distribution of $K_s$ values. If $Y = \\log K_s$, a log-normal mixture for $K_s$ corresponds to a Gaussian mixture for $Y$.\n- Let $Y_i = \\log K_{s,i}$ for $i = 1,\\dots,n$. Under a $K$-component Gaussian mixture, $Y_i$ are independently and identically distributed from $p(y) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(y \\mid \\mu_k, \\sigma_k^2)$ with mixture weights $\\pi_k > 0$, $\\sum_{k=1}^{K} \\pi_k = 1$, means $\\mu_k \\in \\mathbb{R}$, and variances $\\sigma_k^2 > 0$.\n- Maximum likelihood estimation for the Gaussian mixture can be performed via the Expectation–Maximization algorithm, and model comparison between $K=1$ and $K=2$ components can be based on the Bayesian Information Criterion (BIC), defined as $\\mathrm{BIC} = -2 \\log \\hat{L} + p \\log n$, where $\\hat{L}$ is the maximized likelihood and $p$ is the number of free parameters. For a univariate Gaussian mixture with $K$ components, $p = (K-1) + K + K = 3K - 1$.\n- The log-normal component corresponding to $(\\mu, \\sigma^2)$ in log-space has mode at $\\exp(\\mu - \\sigma^2)$. A WGD component associated with a peak near $K_s \\approx 0.8$ would therefore correspond to a Gaussian component in $Y$ whose induced log-normal mode lies near $0.8$ and whose mixture weight is non-trivial.\n\nDesign a program that implements the following mixture modeling decision rule to test whether a peak at $K_s \\approx 0.8$ represents a distinct WGD component:\n- Fit the $K=1$ and $K=2$ Gaussian mixture models to $Y_i = \\log K_{s,i}$ via maximum likelihood using the Expectation–Maximization algorithm with multiple random initializations to avoid poor local optima.\n- Compute $\\Delta \\mathrm{BIC} = \\mathrm{BIC}_{K=1} - \\mathrm{BIC}_{K=2}$.\n- Declare that the peak at $K_s \\approx 0.8$ is explained by a distinct WGD component if and only if both of the following criteria are met:\n  1. $\\Delta \\mathrm{BIC} \\ge \\tau_{\\mathrm{BIC}}$, where $\\tau_{\\mathrm{BIC}} = 10.0$.\n  2. Under the fitted $K=2$ model, there exists at least one component whose induced log-normal mode $m_k = \\exp(\\mu_k - \\sigma_k^2)$ satisfies $\\lvert m_k - 0.8 \\rvert \\le \\tau_{\\mathrm{loc}}$ with $\\tau_{\\mathrm{loc}} = 0.1$, and whose weight satisfies $\\pi_k \\ge \\tau_{\\mathrm{w}}$ with $\\tau_{\\mathrm{w}} = 0.05$.\n\nThere are no physical units because $K_s$ is dimensionless. All numerical tolerances in this problem are pure numbers. Angles are not involved.\n\nTest suite. To ensure the solution is testable and covers distinct regimes, generate synthetic $K_s$ datasets from log-normal mixtures with known parameters. For a log-normal component having target mode $m$ and log-scale standard deviation $\\sigma$, set $\\mu = \\log m + \\sigma^2$ so that the mode in $K_s$ space equals $m$. For each case below, draw $n$ independent samples and mix components according to the specified weights.\n- Case $1$ (clear WGD near $0.8$): $n = 4000$, two components with modes `m = [0.2, 0.8]`, log-scale standard deviations `sigma = [0.15, 0.15]`, and weights `pi = [0.6, 0.4]`.\n- Case $2$ (subtle WGD, small weight): $n = 4000$, two components with modes `m = [0.2, 0.8]`, log-scale standard deviations `sigma = [0.15, 0.08]`, and weights `pi = [0.9, 0.1]`.\n- Case $3$ (no WGD; single component centered near $0.8$): $n = 4000$, one component with mode `m = [0.8]`, log-scale standard deviation `sigma = [0.12]`, and weight `pi = [1.0]`.\n- Case $4$ (two overlapping components flanking $0.8$ but none centered at $0.8$): $n = 4000$, two components with modes `m = [0.7, 0.9]`, log-scale standard deviations `sigma = [0.12, 0.12]`, and weights `pi = [0.5, 0.5]`.\n\nYour program must:\n- Deterministically simulate the four datasets above using fixed random seeds.\n- Fit the $K=1$ and $K=2$ Gaussian mixtures to $Y = \\log K_s$ for each dataset using the Expectation–Maximization algorithm with at least $5$ random initializations, maximum $200$ iterations per run, and a convergence tolerance of $10^{-6}$ in absolute change in log-likelihood. Impose a minimum variance floor of $10^{-6}$ in log-space during estimation to avoid degeneracy.\n- Apply the decision rule described above to produce, for each case, a boolean indicating whether a distinct WGD component at $K_s \\approx 0.8$ is supported.\n\nFinal output format:\n- Your program should produce a single line of output containing the four boolean decisions, in order for Cases $1$ through $4$, as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\").", "solution": "The problem requires the formulation and implementation of a statistical decision procedure to test for evidence of a Whole-Genome Duplication (WGD) event from the distribution of synonymous substitutions ($K_s$) between duplicate gene pairs. The procedure must be based on mixture modeling and information criteria. The problem is scientifically grounded, well-posed, and all necessary parameters and criteria are provided.\n\nThe scientific premise is that a WGD event creates a large number of gene duplicates at a single point in time, which manifest as a distinct peak in the $K_s$ distribution. Continuous, small-scale duplications form a background distribution. We model the overall $K_s$ distribution as a log-normal mixture, which implies that the log-transformed data, $Y_i = \\log K_{s,i}$, follow a Gaussian Mixture Model (GMM). The task is to distinguish between a single background process ($K=1$ model) and a background plus a WGD process ($K=2$ model).\n\nThe GMM density for $Y$ is given by:\n$$ p(y | \\theta) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(y \\mid \\mu_k, \\sigma_k^2) $$\nwhere $\\theta = \\{\\pi_k, \\mu_k, \\sigma_k^2\\}_{k=1}^K$ are the parameters: $\\pi_k$ are the mixture weights ($\\sum_k \\pi_k = 1, \\pi_k > 0$), $\\mu_k$ are the means, and $\\sigma_k^2$ are the variances of the Gaussian components.\n\nThe core of the solution involves three steps: parameter estimation, model comparison, and application of a decision rule.\n\n1.  **Parameter Estimation via Expectation-Maximization (EM)**\n    The parameters $\\theta$ of the GMM are estimated by maximizing the log-likelihood of the observed data, $\\mathcal{L}(\\theta) = \\sum_{i=1}^n \\log p(Y_i | \\theta)$. The Expectation-Maximization (EM) algorithm is an iterative procedure for this purpose. It consists of two steps:\n    -   **E-step (Expectation):** We compute the posterior probability, or \"responsibility,\" $\\gamma_{ik}$ that data point $Y_i$ was generated by component $k$, given the current parameter estimates $\\theta^{(t)}$.\n        $$ \\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(Y_i \\mid \\mu_k^{(t)}, (\\sigma_k^2)^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(Y_i \\mid \\mu_j^{(t)}, (\\sigma_j^2)^{(t)})} $$\n    -   **M-step (Maximization):** We update the parameters to maximize the expected complete-data log-likelihood, using the responsibilities calculated in the E-step. The updates are:\n        Let $N_k = \\sum_{i=1}^n \\gamma_{ik}$.\n        $$ \\pi_k^{(t+1)} = \\frac{N_k}{n} $$\n        $$ \\mu_k^{(t+1)} = \\frac{1}{N_k} \\sum_{i=1}^n \\gamma_{ik} Y_i $$\n        $$ (\\sigma_k^2)^{(t+1)} = \\frac{1}{N_k} \\sum_{i=1}^n \\gamma_{ik} (Y_i - \\mu_k^{(t+1)})^2 $$\n    To prevent numerical instability from variances approaching zero (a degenerate solution), a minimum variance floor of $10^{-6}$ is enforced. The algorithm is iterated until the change in log-likelihood falls below a tolerance of $10^{-6}$ or a maximum of $200$ iterations is reached. Since the likelihood surface for GMMs has multiple local optima, the EM algorithm will be run from $5$ different random initializations, and the solution with the highest log-likelihood will be selected.\n\n2.  **Model Comparison via Bayesian Information Criterion (BIC)**\n    To compare the $K=1$ model (null hypothesis: background only) with the $K=2$ model (alternative hypothesis: background + WGD), we use the Bayesian Information Criterion (BIC). BIC penalizes models for complexity, balancing goodness-of-fit with the number of parameters.\n    $$ \\mathrm{BIC} = -2 \\log \\hat{L} + p \\log n $$\n    Here, $\\hat{L}$ is the maximized likelihood, $n$ is the number of samples, and $p$ is the number of free parameters. For a univariate GMM with $K$ components, $p = (K-1) \\text{ weights} + K \\text{ means} + K \\text{ variances} = 3K-1$. We will compute $\\mathrm{BIC}_{K=1}$ (with $p_1=2$) and $\\mathrm{BIC}_{K=2}$ (with $p_2=5$). Strong evidence in favor of the more complex model is indicated by a lower BIC value.\n\n3.  **Decision Rule**\n    A WGD component associated with a peak at $K_s \\approx 0.8$ is declared to be present if and only if two conditions are met:\n    -   **Statistical Significance:** The $K=2$ model must provide a substantially better fit than the $K=1$ model, as measured by $\\Delta \\mathrm{BIC} = \\mathrm{BIC}_{K=1} - \\mathrm{BIC}_{K=2} \\ge \\tau_{\\mathrm{BIC}}$, with $\\tau_{\\mathrm{BIC}} = 10.0$. A difference of $10$ is generally considered very strong evidence.\n    -   **Biological Relevance:** The added component in the $K=2$ model must correspond to the hypothesized WGD event. This means there must exist at least one component $k$ whose induced log-normal mode, $m_k = \\exp(\\mu_k - \\sigma_k^2)$, is located near $0.8$ (specifically, $|m_k - 0.8| \\le \\tau_{\\mathrm{loc}} = 0.1$) and whose contribution to the mixture is non-trivial (weight $\\pi_k \\ge \\tau_{\\mathrm{w}} = 0.05$).\n\nThe implementation will proceed by first simulating four deterministic datasets according to the problem specification. For each dataset, we first estimate parameters for a single Gaussian ($K=1$)—for which the MLEs are the sample mean and variance—and compute $\\mathrm{BIC}_{K=1}$. Then, we fit a two-component GMM ($K=2$) using the described EM procedure and compute $\\mathrm{BIC}_{K=2}$. Finally, the two-part decision rule is applied to determine the outcome for each case. The entire process will be encapsulated in a Python program using `numpy` and `scipy` libraries.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import logsumexp\n\ndef generate_data(n, modes, log_stds, weights, seed):\n    \"\"\"\n    Generates synthetic log-transformed data (Y = log(K_s)) from a Gaussian mixture.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_components = len(modes)\n    \n    # Calculate log-space means (mu) from K_s-space modes (m) and log-stds (sigma)\n    # m = exp(mu - sigma^2) => mu = log(m) + sigma^2\n    log_means = np.log(modes) + np.array(log_stds)**2\n    \n    # Assign each sample to a component\n    component_choices = rng.choice(n_components, size=n, p=weights)\n    \n    # Generate data from the chosen components\n    y_samples = np.array([\n        rng.normal(loc=log_means[comp], scale=log_stds[comp])\n        for comp in component_choices\n    ])\n    \n    return y_samples\n\ndef _em_single_run(Y, K, max_iter, tol, var_floor, init_seed):\n    \"\"\"\n    A single run of the EM algorithm for a GMM.\n    \"\"\"\n    n_samples = len(Y)\n    rng = np.random.default_rng(init_seed)\n    \n    # Initialization\n    # Randomly choose K data points as initial means\n    initial_means = rng.choice(Y, K, replace=False)\n    # Set initial variances to the global variance\n    initial_vars = np.full(K, np.var(Y))\n    # Set initial weights to be uniform\n    weights = np.full(K, 1.0 / K)\n    \n    means = initial_means\n    variances = initial_vars\n\n    log_likelihood_old = -np.inf\n    \n    for i in range(max_iter):\n        # E-step: Calculate responsibilities\n        log_resp_num = np.zeros((n_samples, K))\n        for k in range(K):\n            log_resp_num[:, k] = np.log(weights[k]) + norm.logpdf(Y, means[k], np.sqrt(variances[k]))\n        \n        log_norm_const = logsumexp(log_resp_num, axis=1)\n        log_resp = log_resp_num - log_norm_const[:, np.newaxis]\n        responsibilities = np.exp(log_resp)\n\n        # M-step: Update parameters\n        Nk = np.sum(responsibilities, axis=0)\n        \n        weights = Nk / n_samples\n        \n        # Add a small constant to avoid division by zero if a component dies\n        means = np.sum(responsibilities * Y[:, np.newaxis], axis=0) / (Nk + 1e-9)\n        \n        variances_num = np.sum(responsibilities * (Y[:, np.newaxis] - means)**2, axis=0)\n        variances = variances_num / (Nk + 1e-9)\n        \n        # Apply variance floor\n        variances = np.maximum(variances, var_floor)\n\n        #_check for convergence\n        log_likelihood_new = np.sum(log_norm_const)\n        if abs(log_likelihood_new - log_likelihood_old) < tol:\n            break\n        log_likelihood_old = log_likelihood_new\n            \n    return weights, means, variances, log_likelihood_old\n\ndef em_gmm(Y, K, num_inits, max_iter, tol, var_floor, seed_seq):\n    \"\"\"\n    Fits a GMM using EM with multiple random initializations.\n    \"\"\"\n    best_log_likelihood = -np.inf\n    best_params = None\n\n    for i in range(num_inits):\n        init_seed = seed_seq.spawn(1)[0]\n        weights, means, variances, log_likelihood = _em_single_run(\n            Y, K, max_iter, tol, var_floor, init_seed\n        )\n        if log_likelihood > best_log_likelihood:\n            best_log_likelihood = log_likelihood\n            best_params = (weights, means, variances)\n            \n    return best_params, best_log_likelihood\n\ndef calculate_bic(log_likelihood, n_params, n_samples):\n    \"\"\"\n    Calculates the Bayesian Information Criterion (BIC).\n    \"\"\"\n    return -2 * log_likelihood + n_params * np.log(n_samples)\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the simulation, model fitting, and decision process.\n    \"\"\"\n    test_cases_params = [\n        # Case 1 (clear WGD)\n        {'n': 4000, 'modes': [0.2, 0.8], 'log_stds': [0.15, 0.15], 'weights': [0.6, 0.4]},\n        # Case 2 (subtle WGD)\n        {'n': 4000, 'modes': [0.2, 0.8], 'log_stds': [0.15, 0.08], 'weights': [0.9, 0.1]},\n        # Case 3 (no WGD, single component)\n        {'n': 4000, 'modes': [0.8], 'log_stds': [0.12], 'weights': [1.0]},\n        # Case 4 (overlapping components)\n        {'n': 4000, 'modes': [0.7, 0.9], 'log_stds': [0.12, 0.12], 'weights': [0.5, 0.5]},\n    ]\n\n    # EM Parameters\n    em_config = {\n        'num_inits': 5,\n        'max_iter': 200,\n        'tol': 1e-6,\n        'var_floor': 1e-6\n    }\n    \n    # Decision Rule Thresholds\n    tau_bic = 10.0\n    tau_loc = 0.1\n    tau_w = 0.05\n    target_mode = 0.8\n    \n    results = []\n    \n    # Master seed for deterministic execution\n    master_seed = 12345\n    rng_master = np.random.SeedSequence(master_seed)\n    case_seeds = rng_master.spawn(len(test_cases_params))\n    \n    for i, params in enumerate(test_cases_params):\n        data_seed = case_seeds[i]\n        Y = generate_data(\n            n=params['n'],\n            modes=params['modes'],\n            log_stds=params['log_stds'],\n            weights=params['weights'],\n            seed=data_seed\n        )\n        n = len(Y)\n\n        # --- Fit K=1 model ---\n        mu1_k1 = np.mean(Y)\n        var1_k1 = np.var(Y) # MLE for variance uses 1/n\n        \n        logL_1 = np.sum(norm.logpdf(Y, mu1_k1, np.sqrt(var1_k1)))\n        p_1 = 3 * 1 - 1\n        bic_1 = calculate_bic(logL_1, p_1, n)\n\n        # --- Fit K=2 model ---\n        em_seed_seq = case_seeds[i].spawn(1)[0] # Derive seed for EM initializations\n        params_2, logL_2 = em_gmm(\n            Y, K=2, \n            seed_seq=em_seed_seq,\n            **em_config\n        )\n        p_2 = 3 * 2 - 1\n        bic_2 = calculate_bic(logL_2, p_2, n)\n        \n        # --- Apply Decision Rule ---\n        delta_bic = bic_1 - bic_2\n        \n        # Condition 1: BIC evidence\n        cond1_met = delta_bic >= tau_bic\n        \n        # Condition 2: Biologically relevant component\n        cond2_met = False\n        if params_2 is not None:\n            weights_2, means_2, variances_2 = params_2\n            for k in range(2):\n                # Calculate induced log-normal mode: m = exp(mu - sigma^2)\n                mode_k = np.exp(means_2[k] - variances_2[k])\n                weight_k = weights_2[k]\n                \n                loc_check = abs(mode_k - target_mode) <= tau_loc\n                weight_check = weight_k >= tau_w\n                \n                if loc_check and weight_check:\n                    cond2_met = True\n                    break\n\n        decision = cond1_met and cond2_met\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2825757"}]}