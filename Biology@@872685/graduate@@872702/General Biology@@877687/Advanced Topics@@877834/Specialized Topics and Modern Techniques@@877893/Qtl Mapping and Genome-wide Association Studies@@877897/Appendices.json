{"hands_on_practices": [{"introduction": "Any robust genetic association study begins not with analysis, but with rigorous data cleaning. This exercise places you in the role of an analyst who must design a full quality control (QC) pipeline, using only the empirical distributions of standard QC metrics from a hypothetical large-scale study [@problem_id:2830645]. By working through this scenario, you will develop the critical skill of setting data-driven thresholds that balance the trade-off between removing technical artifacts and retaining the maximum amount of high-quality data for analysis.", "problem": "You are given a genome-wide association study (GWAS) dataset with $n = 12{,}000$ genotyped individuals (cases $= 5{,}000$, controls $= 7{,}000$) and $m = 700{,}000$ autosomal single nucleotide polymorphisms (SNPs) after standard array clustering and genotype calling. You plan to perform pre-association quality control (QC) using common GWAS metrics: per-individual call rate and missingness, per-SNP call rate and missingness, autosomal heterozygosity rate, sex inference on the X chromosome, relatedness, and Hardy–Weinberg Equilibrium (HWE). For concreteness, assume the following empirical summaries derived from the raw data:\n\n- Per-individual missingness (fraction of genotypes not called) has a primary mode near $0.003$, a $95$th percentile at $0.012$, and a long right tail up to $0.12$. There is a secondary cluster of approximately $150$ individuals with missingness between $0.06$ and $0.10$.\n- Per-SNP missingness has a mode near $0.002$, a $95$th percentile at $0.015$, and a long tail up to $0.25$. There is a minor mode around $0.06$ involving about $12{,}000$ SNPs.\n- Autosomal heterozygosity rate, computed on a linkage disequilibrium (LD)-pruned set of $100{,}000$ approximately independent SNPs, is approximately normally distributed with mean $0.315$ and standard deviation $0.012$. There are outlier clusters at $0.36$ (about $30$ individuals) and at $0.28$ (about $20$ individuals).\n- X chromosome sex inference using the inbreeding coefficient on the non-pseudoautosomal X ($F_X$) shows that reported males ($n \\approx 5{,}800$) have $F_X$ distributed around mean $0.95$ with standard deviation $0.05$, and reported females ($n \\approx 6{,}200$) around mean $0.02$ with standard deviation $0.03$. There are about $40$ reported females with $F_X  0.85$ and $25$ reported males with $F_X  0.15$.\n- Pairwise relatedness estimated by identity-by-descent (IBD) proportion $\\hat{\\pi}$ reveals approximately $60$ duplicate or monozygotic (MZ) twin pairs with $\\hat{\\pi}  0.98$, $220$ putative first-degree relative pairs with $\\hat{\\pi} \\in [0.40, 0.60]$, and $300$ putative second-degree relative pairs with $\\hat{\\pi} \\in [0.18, 0.25]$. The remaining pairs have $\\hat{\\pi}  0.05$.\n- HWE exact test $p$-values computed in controls show a near-uniform distribution except for an enrichment of small $p$-values: about $25{,}000$ SNPs have $p  10^{-4}$ and $1{,}200$ SNPs have $p  10^{-6}$. SNPs with $p  10^{-6}$ are enriched for higher missingness (median missingness around $0.06$).\n\nDefinitions: per-individual call rate is the proportion of non-missing genotype calls across all SNPs for an individual; per-SNP call rate is the proportion of non-missing genotype calls across all individuals for a SNP. Autosomal heterozygosity rate is the fraction of heterozygous genotypes across a pruned set of autosomal SNPs for an individual. The X chromosome inbreeding coefficient $F_X$ is near $1$ for males (hemizygous) and near $0$ for females under typical diploid female X genotypes. The IBD proportion $\\hat{\\pi}$ is the estimated proportion of the genome shared IBD between a pair of individuals, with typical expectations near $1.0$ for duplicates/MZ twins, $0.5$ for first-degree relatives, and $0.25$ for second-degree relatives. Under Hardy–Weinberg equilibrium, genotype frequencies satisfy $p^2$, $2pq$, $q^2$ given allele frequencies $p$ and $q = 1 - p$; in large outbred populations with random mating, HWE test $p$-values are approximately uniform under the null.\n\nYou must choose a QC protocol that both correctly defines these metrics and sets thresholds that balance retention of high-quality data with removal of likely artifacts, using only the empirical summaries above and first principles of GWAS QC. Which option best meets this goal?\n\nA. Individuals: exclude if missingness $ 0.02$ (equivalently call rate $ 0.98$); SNPs: exclude if missingness $ 0.02$ (equivalently call rate $ 0.98$); heterozygosity: flag and exclude autosomal outliers outside mean $\\pm 3$ standard deviations computed on LD-pruned SNPs; sex check: declare genetic sex male if $F_X \\ge 0.80$ and female if $F_X \\le 0.20$, and exclude discordant or ambiguous individuals; relatedness: remove one sample from each pair with $\\hat{\\pi}  0.185$ to limit to at most third-degree relatedness; HWE: compute in controls and exclude SNPs with $p  10^{-6}$.\n\nB. Individuals: exclude if missingness $ 0.005$; SNPs: exclude if missingness $ 0.005$; heterozygosity: exclude outside mean $\\pm 2$ standard deviations; sex check: declare genetic sex by $F_X \\ge 0.50$ for males and $F_X  0.50$ for females; relatedness: remove only duplicates and first-degree relatives using $\\hat{\\pi}  0.35$; HWE: compute in all samples and exclude SNPs with $p  10^{-4}$.\n\nC. Individuals: exclude if missingness $ 0.08$; SNPs: exclude if missingness $ 0.08$; heterozygosity: exclude outside mean $\\pm 5$ standard deviations; sex check: skip X-based sex inference to maximize retention; relatedness: remove one from each pair with $\\hat{\\pi}  0.10$; HWE: compute in controls and exclude SNPs with $p  10^{-2}$.\n\nD. Individuals: exclude if missingness $ 0.02$; SNPs: exclude if missingness $ 0.02$ when minor allele frequency (MAF) $\\ge 0.01$ but allow up to $0.05$ when MAF $ 0.01$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: use $F_X \\ge 0.80$ for males and $F_X \\le 0.20$ for females and remove discordant or ambiguous individuals; relatedness: remove one from each pair with $\\hat{\\pi}  0.125$; HWE: compute in controls and exclude SNPs with $p  10^{-6}$.\n\nE. Individuals: exclude if missingness $ 0.02$; SNPs: exclude if missingness $ 0.02$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: as in A; relatedness: remove one from each pair with $\\hat{\\pi}  0.185$; HWE: compute in controls and exclude SNPs with $p  7.1 \\times 10^{-8}$ by Bonferroni correction using $m = 700{,}000$.\n\nSelect the single best option.", "solution": "The task is to evaluate several proposed quality control (QC) protocols for a genome-wide association study (GWAS) and select the best one. The evaluation must be based on established principles of GWAS QC and the specific empirical data summaries provided in the problem statement.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- Sample: $n = 12,000$ individuals (cases $= 5,000$, controls $= 7,000$).\n- Genetic data: $m = 700,000$ autosomal single nucleotide polymorphisms (SNPs).\n- Per-individual missingness: mode $\\approx 0.003$, $95$th percentile $= 0.012$, tail up to $0.12$. A secondary cluster of $\\approx 150$ individuals has missingness $\\in [0.06, 0.10]$.\n- Per-SNP missingness: mode $\\approx 0.002$, $95$th percentile $= 0.015$, tail up to $0.25$. A minor mode at $\\approx 0.06$ involves $\\approx 12,000$ SNPs.\n- Autosomal heterozygosity rate: on $100,000$ LD-pruned SNPs, approximately normal with mean $\\mu = 0.315$ and standard deviation $\\sigma = 0.012$. Outlier clusters are present at $0.36$ ($\\approx 30$ individuals) and $0.28$ ($\\approx 20$ individuals).\n- X chromosome sex inference ($F_X$): for reported males, $F_X$ is distributed around mean $0.95$, $\\sigma = 0.05$. For reported females, $F_X$ is distributed around mean $0.02$, $\\sigma = 0.03$. About $40$ reported females have $F_X  0.85$, and about $25$ reported males have $F_X  0.15$.\n- Pairwise relatedness ($\\hat{\\pi}$): $\\approx 60$ duplicate/MZ twin pairs ($\\hat{\\pi}  0.98$), $\\approx 220$ first-degree pairs ($\\hat{\\pi} \\in [0.40, 0.60]$), $\\approx 300$ second-degree pairs ($\\hat{\\pi} \\in [0.18, 0.25]$).\n- Hardy–Weinberg Equilibrium (HWE): $p$-values in controls are enriched at the low end. $\\approx 25,000$ SNPs have $p  10^{-4}$ and $\\approx 1,200$ SNPs have $p  10^{-6}$. These low-$p$ SNPs are enriched for high missingness.\n- Definitions of all metrics are provided and are standard.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. All metrics and their described empirical distributions are realistic for a large human GWAS dataset. The problem is well-posed, asking for the selection of the best QC protocol based on provided data, a standard task in bioinformatics. The language is objective and precise. The problem is self-contained and free of contradictions. The data provided are sufficient to make a principled choice.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the analysis.\n\n### Derivation of a Sound QC Protocol from First Principles and Data\n\nA sound QC protocol must remove likely artifacts while retaining as much high-quality data as possible. Each threshold should be justified by the provided empirical distributions.\n\n1.  **Per-individual missingness**: Most individuals have very low missingness (mode at $0.003$, $95$th percentile at $0.012$). There is a distinct cluster of poor-quality samples around $0.06-0.10$. A standard threshold is $0.02$ or $0.05$. A threshold of $0.02$ would remove the long tail of low-quality samples and the entire outlier cluster, while retaining over $95\\%$ of the individuals. This is a sound choice.\n\n2.  **Per-SNP missingness**: The situation is analogous to individual missingness. The vast majority of SNPs have low missingness (mode $0.002$, $95$th percentile $0.015$). A minor mode of problematic SNPs exists around $0.06$. A threshold of $0.02$ is reasonable, as it removes the long tail while retaining over $95\\%$ of SNPs and targets the problematic mode.\n\n3.  **Autosomal heterozygosity**: The data follows a normal distribution with $\\mu=0.315$ and $\\sigma=0.012$, with outlier clusters. We check the deviation of these clusters in units of standard deviation.\n    -   High-heterozygosity cluster at $0.36$: $z = (0.36 - 0.315) / 0.012 = 0.045 / 0.012 = 3.75$.\n    -   Low-heterozygosity cluster at $0.28$: $z = (0.28 - 0.315) / 0.012 = -0.035 / 0.012 \\approx -2.92$.\n    A threshold of mean $\\pm 3\\sigma$ corresponds to the interval $[0.315 - 3(0.012), 0.315 + 3(0.012)] = [0.279, 0.351]$. This is an excellent choice as it correctly identifies the high-heterozygosity individuals as outliers ($0.36  0.351$). It also correctly flags individuals at the fringe of the low-heterozygosity cluster ($0.28$ is close to the boundary of $0.279$). A $\\pm 3\\sigma$ rule is standard for identifying outliers in normally distributed data.\n\n4.  **Sex check**: The distribution of the inbreeding coefficient on the X chromosome, $F_X$, is clearly bimodal. For males, the mean is $0.95$ ($\\sigma=0.05$). For females, the mean is $0.02$ ($\\sigma=0.03$). Well-defined thresholds are needed to classify individuals and identify discordances. Let's consider intervals of $\\mu \\pm 3\\sigma$:\n    -   Male range: $[0.95 - 3(0.05), 0.95 + 3(0.05)] = [0.80, 1.10]$.\n    -   Female range: $[0.02 - 3(0.03), 0.02 + 3(0.03)] = [-0.07, 0.11]$.\n    Based on this, setting thresholds such that genetic males have $F_X \\ge 0.80$ and genetic females have $F_X \\le 0.20$ is a robust choice. The space between $0.20$ and $0.80$ serves as a buffer zone for ambiguous cases (e.g., sex chromosome aneuploidies). The problem notes there are reported females with $F_X  0.85$ and reported males with $F_X  0.15$. These thresholds would correctly identify them as sex-discordant, and they must be excluded.\n\n5.  **Relatedness**: GWAS requires a sample of (mostly) unrelated individuals. Duplicates, MZ twins, and first-degree relatives must be removed. It is also standard practice to remove second-degree relatives to ensure independence. The data shows a cluster of second-degree relatives with $\\hat{\\pi}$ in the range $[0.18, 0.25]$. A threshold of $\\hat{\\pi}  0.185$ is well-chosen to remove pairs from this group and closer relationships (first-degree, duplicates). It is a data-driven threshold that targets the observed distribution.\n\n6.  **Hardy–Weinberg Equilibrium (HWE)**: HWE testing is a tool to detect genotyping errors. a) It must be performed in **controls only** in a case-control study, as a true association can cause HWE deviation in cases or in the combined sample. b) The $p$-value threshold should be stringent enough to remove likely errors but not so stringent that it is ineffective. The data shows an enrichment of SNPs with very low $p$-values ($1,200$ SNPs with $p  10^{-6}$), which are also correlated with high missingness. This strongly suggests that these SNPs are artifacts. Therefore, a threshold of $p  10^{-6}$ is well-justified by the data to specifically target this cluster of poor-quality SNPs. Applying a Bonferroni correction (e.g., $p  0.05/700,000 \\approx 7 \\times 10^{-8}$) is inappropriate for a QC filter; it is too stringent and would fail to remove many problematic SNPs.\n\nBased on this analysis, the optimal protocol combines these principled and data-driven choices. I will now evaluate each option against this ideal protocol.\n\n### Option-by-Option Analysis\n\n**A. Individuals: exclude if missingness $ 0.02$; SNPs: exclude if missingness $ 0.02$; heterozygosity: flag and exclude autosomal outliers outside mean $\\pm 3$ standard deviations; sex check: declare genetic sex male if $F_X \\ge 0.80$ and female if $F_X \\le 0.20$, and exclude discordant or ambiguous individuals; relatedness: remove one sample from each pair with $\\hat{\\pi}  0.185$; HWE: compute in controls and exclude SNPs with $p  10^{-6}$.**\n\n- **Assessment:** Every single step in this option aligns perfectly with the optimal protocol derived from the empirical data and first principles. The thresholds for missingness, heterozygosity, sex check, relatedness, and HWE are all correct and well-justified by the provided numbers.\n- **Verdict:** **Correct**.\n\n**B. Individuals: exclude if missingness $ 0.005$; SNPs: exclude if missingness $ 0.005$; heterozygosity: exclude outside mean $\\pm 2$ standard deviations; sex check: declare genetic sex by $F_X \\ge 0.50$ for males and $F_X  0.50$ for females; relatedness: remove only duplicates and first-degree relatives using $\\hat{\\pi}  0.35$; HWE: compute in all samples and exclude SNPs with $p  10^{-4}$.**\n\n- **Assessment:** This protocol is deeply flawed. The missingness ($0.005$) and heterozygosity ($\\pm 2\\sigma$) thresholds are too stringent and would discard excessive amounts of valid data. The sex check threshold ($F_X=0.5$) is crude and less robust. The relatedness filter ($\\hat{\\pi}  0.35$) is too lenient, retaining second-degree relatives. Critically, computing HWE in all samples is incorrect for a case-control study.\n- **Verdict:** **Incorrect**.\n\n**C. Individuals: exclude if missingness $ 0.08$; SNPs: exclude if missingness $ 0.08$; heterozygosity: exclude outside mean $\\pm 5$ standard deviations; sex check: skip X-based sex inference to maximize retention; relatedness: remove one from each pair with $\\hat{\\pi}  0.10$; HWE: compute in controls and exclude SNPs with $p  10^{-2}$.**\n\n- **Assessment:** This protocol is far too lenient, retaining low-quality data. Missingness thresholds of $0.08$ would keep known outlier samples and SNPs. A $\\pm 5\\sigma$ heterozygosity rule is ineffective. Skipping the sex check is a major procedural error. The HWE threshold of $p  10^{-2}$ is too accommodating. The relatedness threshold ($\\hat{\\pi}  0.10$) is paradoxically too strict, removing more distant relatives than is standard.\n- **Verdict:** **Incorrect**.\n\n**D. Individuals: exclude if missingness $ 0.02$; SNPs: exclude if missingness $ 0.02$ when minor allele frequency (MAF) $\\ge 0.01$ but allow up to $0.05$ when MAF $ 0.01$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: use $F_X \\ge 0.80$ for males and $F_X \\le 0.20$ for females and remove discordant or ambiguous individuals; relatedness: remove one from each pair with $\\hat{\\pi}  0.125$; HWE: compute in controls and exclude SNPs with $p  10^{-6}$.**\n\n- **Assessment:** This is a strong protocol, very similar to A. However, it has two subtle weaknesses relative to A *in the context of the problem statement*. First, the MAF-dependent SNP missingness rule, while a good practice, introduces information (MAF) not provided in the empirical summaries, violating the instruction to use \"only the empirical summaries above\". Second, the relatedness threshold $\\hat{\\pi}  0.125$ is a theoretical value for third-degree relatives, whereas the threshold $\\hat{\\pi}  0.185$ in option A is better tailored to the observed data, cutting at the boundary of the second-degree relative cluster. Therefore, A is more directly supported by the problem's data.\n- **Verdict:** **Incorrect**.\n\n**E. Individuals: exclude if missingness $ 0.02$; SNPs: exclude if missingness $ 0.02$; heterozygosity: exclude outside mean $\\pm 3$ standard deviations; sex check: as in A; relatedness: remove one from each pair with $\\hat{\\pi}  0.185$; HWE: compute in controls and exclude SNPs with $p  7.1 \\times 10^{-8}$ by Bonferroni correction using $m = 700{,}000$.**\n\n- **Assessment:** This option is identical to A except for the HWE threshold. The use of a strict Bonferroni-corrected threshold ($p  7.1 \\times 10^{-8}$) for HWE as a QC filter is conceptually incorrect. It is far too stringent and would fail to remove the cluster of $\\approx 1,200$ problematic SNPs observed at $p  10^{-6}$, defeating the purpose of this QC step.\n- **Verdict:** **Incorrect**.\n\n### Conclusion\nOption A presents a complete and coherent QC protocol where every step is logically sound and every threshold is directly justified by the empirical data distributions provided in the problem statement. It represents the best balance between removing artifacts and retaining high-quality data.", "answer": "$$\\boxed{A}$$", "id": "2830645"}, {"introduction": "Linear mixed models (LMMs) have become the standard for controlling confounding from population structure and cryptic relatedness in modern GWAS. However, naive application of LMMs can lead to a loss of power and biased effect estimates due to an artifact known as proximal contamination. This practice challenges you to dissect the statistical mechanism behind this problem and, from first principles, derive the Leave-One-Chromosome-Out (LOCO) strategy that resolves it, ensuring your association test statistics remain well-calibrated [@problem_id:2830658].", "problem": "In a genome-wide association study of a quantitative trait using a linear mixed model, you analyze $n$ individuals, with phenotype vector $y \\in \\mathbb{R}^{n}$. The standard linear mixed model writes the phenotype as the sum of fixed covariate effects, a single-variant effect under test, a polygenic background, and residual noise. The polygenic background is modeled as a mean-zero Gaussian with covariance proportional to a genetic relatedness (kinship) matrix computed from genome-wide single-nucleotide polymorphisms. Let $s \\in \\mathbb{R}^{n}$ denote the standardized genotype vector of a variant under test. The model is used both for null-hypothesis testing and for estimating the variant effect. A central requirement is that the association $p$-values are calibrated under the null hypothesis, that is, they have the correct distribution when the variant has no effect.\n\nStarting from well-tested facts about linear mixed models and properties of multivariate Gaussian vectors (for example, that conditioning and linear projections of Gaussian vectors remain Gaussian and that covariance defines the directions along which random effects can absorb signal), explain the mechanism of proximal contamination: when the kinship matrix is constructed from genome-wide markers that include the test variant (or variants in strong linkage disequilibrium on the same chromosome), the random polygenic effect absorbs part of the test-variant signal, deflating the test statistic and biasing effect estimates. Then, derive a principled modification to the kinship construction that breaks this absorption while preserving control of relatedness and population structure, and argue why this modification restores calibration of the test statistic under the null across the genome.\n\nWhich strategy below follows from your derivation and will preserve calibration while controlling relatedness and population structure in the presence of linkage disequilibrium?\n\nA. Use a single kinship matrix constructed from all markers once, including every variant, and use it for all tests.\n\nB. For each tested variant, recompute the kinship matrix after removing only that exact variant, keeping all other markers.\n\nC. For each tested variant on a given chromosome, recompute the kinship matrix after excluding all markers on that chromosome, and use that chromosome-specific exclusion for tests on that chromosome.\n\nD. Replace the mixed-model random effect by a small number of principal components from genome-wide markers and use ordinary least squares with those principal components as covariates for all tests.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n-   The study is a genome-wide association study (GWAS) of a quantitative trait.\n-   The number of individuals is $n$.\n-   The phenotype is represented by a vector $y \\in \\mathbb{R}^{n}$.\n-   The statistical model is a linear mixed model (LMM).\n-   The LMM for phenotype $y$ includes: fixed covariate effects, a single-variant effect, a polygenic background, and residual noise.\n-   The polygenic background is a random effect modeled as a mean-zero Gaussian with covariance proportional to a genetic relatedness (kinship) matrix, $K$.\n-   The kinship matrix $K$ is computed from genome-wide single-nucleotide polymorphisms (SNPs).\n-   The standardized genotype vector of a variant under test is denoted by $s \\in \\mathbb{R}^{n}$.\n-   A key requirement is the calibration of association $p$-values under the null hypothesis (i.e., they follow a uniform distribution).\n-   The problem posits the existence of \"proximal contamination\": when $K$ is constructed using markers that include the test variant or variants in strong linkage disequilibrium (LD), the random polygenic effect absorbs part of the test-variant signal.\n-   This absorption is stated to deflate the test statistic and bias effect estimates.\n-   The task is to explain this mechanism, derive a principled modification to the kinship construction to resolve it, and identify the correct strategy among the given options.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n\n-   **Scientifically Grounded:** The description of the linear mixed model is standard and accurate for applications in quantitative genetics and GWAS. The model structure, the use of a kinship matrix to model polygenic effects and control for population structure/relatedness, and the phenomenon of proximal contamination are all well-established, fundamental concepts in the field of statistical genetics. The problem is based on sound statistical and genetic principles.\n-   **Well-Posed:** The problem is clearly defined. It requests an explanation of a known statistical artifact and the derivation of a standard, accepted solution. The question is structured to lead to a unique conceptual conclusion that is widely implemented in modern GWAS software.\n-   **Objective:** The language is technical, precise, and devoid of subjectivity or ambiguity. Terms such as \"linear mixed model,\" \"kinship matrix,\" \"linkage disequilibrium,\" and \"p-value calibration\" are standard and have unambiguous meanings in this context.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-formulated question concerning a critical methodological issue in statistical genetics. A solution will be derived.\n\n### Derivation of Solution\n\nThe standard linear mixed model for a quantitative trait $y$ in a sample of $n$ individuals is given by:\n$$\ny = X\\beta + s\\gamma + u + \\epsilon\n$$\nwhere:\n-   $y$ is the $n \\times 1$ vector of phenotype values.\n-   $X$ is an $n \\times c$ matrix of $c$ fixed-effect covariates (e.g., age, sex, and principal components for ancestry).\n-   $\\beta$ is the $c \\times 1$ vector of corresponding effect sizes for the covariates.\n-   $s$ is the $n \\times 1$ standardized genotype vector for the SNP being tested.\n-   $\\gamma$ is the scalar fixed effect of the test SNP, which is the parameter of interest.\n-   $u$ is the $n \\times 1$ random vector representing the aggregated polygenic effect from all other variants across the genome. It is modeled as a draw from a multivariate normal distribution, $u \\sim \\mathcal{N}(0, \\sigma_g^2 K)$, where $\\sigma_g^2$ is the polygenic variance component and $K$ is the $n \\times n$ genetic relatedness matrix (kinship).\n-   $\\epsilon$ is the $n \\times 1$ random vector of non-genetic and environmental residuals, modeled as $\\epsilon \\sim \\mathcal{N}(0, \\sigma_e^2 I)$, where $\\sigma_e^2$ is the residual variance and $I$ is the $n \\times n$ identity matrix.\n\nThe null hypothesis for association testing is $H_0: \\gamma = 0$. The test statistic, typically from a likelihood ratio test or score test, evaluates the significance of including the term $s\\gamma$ in the model. For this test to be valid, the test statistic must follow its theoretical null distribution (e.g., a $\\chi^2_1$ distribution) when $H_0$ is true.\n\n**Mechanism of Proximal Contamination**\n\nThe kinship matrix $K$ is usually estimated from a large set of $M$ genome-wide markers. A standard estimator is $K = \\frac{1}{M} \\sum_{j=1}^M s_j s_j^T$, where $s_j$ is the standardized genotype vector for the $j$-th marker.\n\nThe problem arises when the set of markers $\\{s_j\\}_{j=1}^M$ used to construct $K$ includes the test SNP $s$ or SNPs in high linkage disequilibrium (LD) with $s$. Let the test SNP be $s_k$ for some index $k$.\n\nBy including $s_k$ in the sum to create $K$, we are explicitly including the term $\\frac{1}{M} s_k s_k^T$ in the covariance structure of the random effect $u$. This implies that the model assumes, a priori, that any phenotypic variance that projects onto the direction of the vector $s_k$ can be part of the random polygenic background.\n\nWhen we then test for a *fixed* effect $\\gamma$ for this same SNP $s_k$, we create a conflict. The model must partition the phenotypic signal associated with $s_k$ between the fixed effect term $s_k\\gamma$ and the random effect term $u$. Because the covariance of $u$ is explicitly designed to include variation along $s_k$, the random effect $u$ will \"absorb\" or \"soak up\" a portion of the true fixed-effect signal. This is a classic case of confounding between a fixed effect and a random effect when their defining vectors are not orthogonal.\n\nThis absorption has two main consequences:\n1.  **Biased Effect Estimate:** The estimate $\\hat{\\gamma}$ will be biased towards zero because part of the effect is incorrectly attributed to the random term $u$.\n2.  **Deflated Test Statistic:** The test statistic for $\\gamma$ will be smaller than it should be, leading to a loss of statistical power. Under the null hypothesis, this can also cause a subtle but systematic deflation of the test statistic across the genome, leading to miscalibration (i.e., the $p$-values are systematically larger than expected under the null). This is because even under the null for the tested SNP, LD with true causal variants nearby creates a local signal that gets absorbed by the contaminated random effect, again deflating the test statistic.\n\nThe same problem occurs if we test SNP $s_k$ but the kinship matrix $K$ includes a different SNP $s_j$ that is in high LD with $s_k$. In this case, their genotype vectors are highly correlated ($s_k \\approx \\rho s_j$), so including $s_j$ in the construction of $K$ provides a vector in the covariance of $u$ that is nearly collinear with the fixed effect vector $s_k$ being tested, leading to the same absorption phenomenon. This is \"proximal contamination\" because the contamination is sourced from markers genetically close (and thus in LD with) the test marker.\n\n**Principled Modification to Kinship Construction**\n\nTo eliminate this confounding, we must ensure that the random effect $u$ and the fixed effect $s\\gamma$ are not defined using overlapping information. The vector $s$ being tested for a fixed effect must be orthogonal to the space of variation defined for the random polygenic background.\n\nA direct way to achieve this is to construct the kinship matrix $K$ using only markers that are not in LD with the test marker $s$. Due to the nature of meiosis and recombination, markers on different chromosomes are, in general, not in LD (except for long-range correlations induced by population structure, which is what the LMM is designed to capture globally). Within a chromosome, LD is strong for nearby markers and decays with distance.\n\nTherefore, a principled and robust strategy is to partition the genome by chromosome. When testing any SNP on a specific chromosome, say chromosome $i$, we should model the polygenic background using a kinship matrix constructed *exclusively* from markers on all other chromosomes.\n\nLet $K_{(-i)}$ be the kinship matrix computed from all SNPs *not* on chromosome $i$. When testing a SNP $s$ located on chromosome $i$, the model becomes:\n$$\ny = X\\beta + s\\gamma + u_{(-i)} + \\epsilon\n$$\nwhere $u_{(-i)} \\sim \\mathcal{N}(0, \\sigma_g^2 K_{(-i)})$.\n\nThis approach, known as Leave-One-Chromosome-Out (LOCO), ensures that the fixed effect being tested, $s\\gamma$, is not confounded with the random effect $u_{(-i)}$. The random effect models the background from chromosomes $1, 2, ..., i-1, i+1, ...$, while the fixed effect models the local signal on chromosome $i$. This correctly separates the test signal from the background model, breaking the mechanism of proximal contamination. This restores the calibration of test statistics and provides unbiased effect estimates for variants on chromosome $i$, while still controlling for population structure and polygenic background arising from the rest of the genome. This procedure is repeated for each chromosome.\n\n### Evaluation of Options\n\n**A. Use a single kinship matrix constructed from all markers once, including every variant, and use it for all tests.**\nThis is the standard approach that *causes* proximal contamination. As derived above, including the test variant (or variants in LD) in the construction of the kinship matrix leads to deflated test statistics and biased estimates. This strategy fails to preserve calibration.\n**Verdict: Incorrect**\n\n**B. For each tested variant, recompute the kinship matrix after removing only that exact variant, keeping all other markers.**\nThis modification is insufficient. While it removes the direct inclusion of the test variant $s$, it fails to account for LD. Markers immediately adjacent to $s$ and in high LD with it will remain in the kinship calculation. Their high correlation with $s$ means the random effect will still absorb the signal from $s$, and proximal contamination will persist. This approach is also computationally prohibitive, requiring a new kinship matrix calculation for every SNP test.\n**Verdict: Incorrect**\n\n**C. For each tested variant on a given chromosome, recompute the kinship matrix after excluding all markers on that chromosome, and use that chromosome-specific exclusion for tests on that chromosome.**\nThis is the Leave-One-Chromosome-Out (LOCO) strategy derived above. It correctly separates the signal being tested (on one chromosome) from the polygenic background model (built from all other chromosomes), thereby eliminating proximal contamination. It preserves control of relatedness and population structure because these are genome-wide properties captured by the remaining chromosomes. This strategy restores test statistic calibration.\n**Verdict: Correct**\n\n**D. Replace the mixed-model random effect by a small number of principal components from genome-wide markers and use ordinary least squares with those principal components as covariates for all tests.**\nThis describes an alternative modeling strategy (PCA + OLS), not a modification to the LMM's kinship construction. While using principal components as covariates controls for population structure, it is well-known to be less powerful than an LMM when a substantial polygenic component to the trait exists, as it does not explicitly model the covariance due to cryptic relatedness and the thousands of small-effect variants. The problem asks for a principled modification *to the kinship construction within the LMM framework*, not for a replacement of the model. Furthermore, OLS with PCs can suffer from its own forms of test statistic inflation when relatedness is not fully captured. This option does not follow from the derivation.\n**Verdict: Incorrect**", "answer": "$$\\boxed{C}$$", "id": "2830658"}, {"introduction": "Identifying a statistically significant association is only the first step; accurately estimating its true effect size is a crucial subsequent challenge. This practice delves into the \"Winner's Curse,\" a form of selection bias inherent to any study where top results are chosen from a large number of tests, leading to systematic overestimation of effects. By analytically deriving the expected effect size conditional on significance, you will gain a deep, quantitative understanding of this bias and the principles behind methods used to correct it [@problem_id:2830616].", "problem": "In a quantitative trait locus (QTL) mapping or Genome-Wide Association Studies (GWAS) setting, suppose a single-locus additive linear model for a continuous phenotype is fit as follows: for individuals indexed by $i \\in \\{1,\\dots,n\\}$, the phenotype is modeled as $y_i = \\mu + g_i \\beta + \\varepsilon_i$, where $g_i$ is a standardized genotype dosage (centered with variance $1$), $\\beta$ is the true allelic effect, and the noise satisfies $\\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$. Let $\\hat{\\beta}$ denote the maximum likelihood estimator of $\\beta$ from simple linear regression, and suppose that, by standard large-sample theory, $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma_{\\hat{\\beta}}^2)$ for some known standard error $\\sigma_{\\hat{\\beta}}  0$. Define the $Z$-statistic $Z = \\hat{\\beta}/\\sigma_{\\hat{\\beta}}$. A discovery is declared when the two-sided test surpasses a fixed genome-wide significance threshold, i.e., when $|Z|  c$ for some $c  0$.\n\nWinner’s Curse refers to the selection-induced inflation of the reported effect size. Using only core properties of the normal distribution, the law of total expectation, and conditional probability, derive the exact closed-form analytic expression for the conditional expectation $\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c]$ as a function of $\\beta$, $\\sigma_{\\hat{\\beta}}$, $c$, the standard normal probability density function $\\phi(\\cdot)$, and the standard normal cumulative distribution function $\\Phi(\\cdot)$.\n\nThen, based on first principles of resampling, propose a principled parametric bootstrap scheme that would estimate and correct the selection-induced bias to produce a bias-reduced effect estimate. Your bootstrap proposal must specify: the data-generating mechanism under the fitted model, the selection event to be enforced in the bootstrap world, and how the resulting bootstrap distribution is used to compute a bias-corrected estimator.\n\nExpress your final answer as the single analytic expression for $\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c]$. No numerical evaluation is required, and no units are to be reported. If you introduce auxiliary symbols in your derivation, your final answer must eliminate them in favor of $\\beta$, $\\sigma_{\\hat{\\beta}}$, $c$, $\\phi(\\cdot)$, and $\\Phi(\\cdot)$ only. Your final answer must be a single closed-form expression, not an equation or inequality.", "solution": "The problem presented will first be subjected to a rigorous validation procedure. Only upon confirmation of its validity will a solution be attempted.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\nThe provided information is as follows:\n- The model for a continuous phenotype $y_i$ for individuals $i \\in \\{1,\\dots,n\\}$ is $y_i = \\mu + g_i \\beta + \\varepsilon_i$.\n- $g_i$ is a standardized genotype dosage, centered with variance $1$.\n- $\\beta$ is the true allelic effect.\n- The noise term is $\\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$.\n- $\\hat{\\beta}$ is the maximum likelihood estimator of $\\beta$.\n- The sampling distribution of the estimator is given as $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma_{\\hat{\\beta}}^2)$, with known $\\sigma_{\\hat{\\beta}}  0$.\n- The $Z$-statistic is defined as $Z = \\hat{\\beta}/\\sigma_{\\hat{\\beta}}$.\n- A discovery is declared under the condition $|Z|  c$ for a fixed threshold $c  0$.\n- The first task is to derive the exact closed-form analytic expression for the conditional expectation $\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c]$ as a function of $\\beta$, $\\sigma_{\\hat{\\beta}}$, $c$, the standard normal probability density function $\\phi(\\cdot)$, and the standard normal cumulative distribution function $\\Phi(\\cdot)$.\n- The second task is to propose a parametric bootstrap scheme to estimate and correct the selection-induced bias.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is evaluated against the required criteria:\n- **Scientifically Grounded**: The problem is based on the standard linear model used in quantitative genetics and the well-documented statistical phenomenon of \"Winner's Curse\". The use of a normal approximation for the effect size estimator is a standard result from large-sample theory in statistics. The problem is fundamentally sound.\n- **Well-Posed**: The problem asks for the derivation of a conditional expectation of a random variable with a specified distribution, conditioned on a well-defined event. This is a standard, solvable problem in probability theory. A unique solution exists. The request for a bootstrap procedure is also a standard topic in computational statistics.\n- **Objective**: The problem is stated in precise mathematical and statistical language, free of ambiguity or subjective claims.\n- **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary components—the distribution of the estimator, the definition of the statistic, and the selection event—are provided. There are no contradictions.\n- **Unrealistic or Infeasible**: The setup is a valid and common simplification used to model and understand statistical phenomena in genetics. It is entirely realistic within its theoretical context.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and complete. A full, reasoned solution will now be provided.\n\n**Part 1: Derivation of the Conditional Expectation**\n\nWe are tasked with deriving the expression for $\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c]$. We begin by analyzing the distribution of the $Z$-statistic.\nGiven that $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma_{\\hat{\\beta}}^2)$, the standardized statistic $Z = \\hat{\\beta}/\\sigma_{\\hat{\\beta}}$ follows a normal distribution:\n$$Z \\sim \\mathcal{N}\\left(\\frac{\\beta}{\\sigma_{\\hat{\\beta}}}, \\frac{\\sigma_{\\hat{\\beta}}^2}{\\sigma_{\\hat{\\beta}}^2}\\right) = \\mathcal{N}(\\theta, 1)$$\nwhere we define the non-centrality parameter $\\theta = \\beta/\\sigma_{\\hat{\\beta}}$.\n\nThe conditional expectation $\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c]$ can be related to the conditional expectation of $Z$:\n$$\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c] = \\mathbb{E}[\\sigma_{\\hat{\\beta}} Z \\mid |Z|  c] = \\sigma_{\\hat{\\beta}} \\mathbb{E}[Z \\mid |Z|  c]$$\nOur primary task is to compute $\\mathbb{E}[Z \\mid |Z|  c]$. By definition of conditional expectation, for a continuous random variable $Z$ with probability density function $f_Z(z)$, we have:\n$$\\mathbb{E}[Z \\mid |Z|  c] = \\frac{\\int_{|z|c} z f_Z(z) \\,dz}{\\int_{|z|c} f_Z(z) \\,dz} = \\frac{\\int_{-\\infty}^{-c} z f_Z(z) \\,dz + \\int_{c}^{\\infty} z f_Z(z) \\,dz}{P(|Z|  c)}$$\nHere, $f_Z(z)$ is the PDF of a $\\mathcal{N}(\\theta, 1)$ distribution, which is $f_Z(z) = \\phi(z-\\theta)$, where $\\phi(\\cdot)$ is the standard normal PDF.\nThe denominator is $P(|Z|  c) = P(Z  c) + P(Z  -c)$. Let us express these probabilities using the standard normal CDF, $\\Phi(\\cdot)$:\n$$P(Z  c) = 1 - \\Phi_{\\mathcal{N}(\\theta,1)}(c) = 1 - \\Phi(c-\\theta)$$\n$$P(Z  -c) = \\Phi_{\\mathcal{N}(\\theta,1)}(-c) = \\Phi(-c-\\theta)$$\nSo, the denominator is $P(|Z|  c) = 1 - \\Phi(c-\\theta) + \\Phi(-c-\\theta)$.\n\nNow we evaluate the numerator. Let's make the substitution $u = z - \\theta$, which implies $z = u + \\theta$ and $du = dz$. The limits of integration transform accordingly.\nThe numerator becomes:\n$$\\int_{-\\infty}^{-c-\\theta} (u+\\theta)\\phi(u) \\,du + \\int_{c-\\theta}^{\\infty} (u+\\theta)\\phi(u) \\,du$$\nWe can split these integrals:\n$$ \\left( \\int_{-\\infty}^{-c-\\theta} u\\phi(u) \\,du + \\int_{c-\\theta}^{\\infty} u\\phi(u) \\,du \\right) + \\theta \\left( \\int_{-\\infty}^{-c-\\theta} \\phi(u) \\,du + \\int_{c-\\theta}^{\\infty} \\phi(u) \\,du \\right) $$\nFor the first part, we use the fact that the antiderivative of $u\\phi(u)$ is $-\\phi(u)$.\n$$\\int_{a}^{b} u\\phi(u) \\,du = [-\\phi(u)]_{a}^{b} = \\phi(a) - \\phi(b)$$\nApplying this, the first integral is $\\phi(-\\infty) - \\phi(-c-\\theta) = 0 - \\phi(c+\\theta) = -\\phi(c+\\theta)$, using the even property of $\\phi(\\cdot)$. The second integral is $\\phi(c-\\theta) - \\phi(\\infty) = \\phi(c-\\theta) - 0$. The sum of these two is $\\phi(c-\\theta) - \\phi(c+\\theta)$.\n\nFor the second part of the expression, the integrals are simply CDFs:\n$$\\theta \\left( \\Phi(-c-\\theta) + (1 - \\Phi(c-\\theta)) \\right) = \\theta P(|Z|c)$$\nCombining the parts, the numerator is $\\phi(c-\\theta) - \\phi(c+\\theta) + \\theta P(|Z|c)$.\n\nNow, we can write the expression for $\\mathbb{E}[Z \\mid |Z|  c]$:\n$$\\mathbb{E}[Z \\mid |Z|  c] = \\frac{\\phi(c-\\theta) - \\phi(c+\\theta) + \\theta P(|Z|c)}{P(|Z|c)} = \\theta + \\frac{\\phi(c-\\theta) - \\phi(c+\\theta)}{P(|Z|c)}$$\nSubstituting the expression for $P(|Z|c)$, we have:\n$$\\mathbb{E}[Z \\mid |Z|  c] = \\theta + \\frac{\\phi(c-\\theta) - \\phi(c+\\theta)}{1 - \\Phi(c-\\theta) + \\Phi(-c-\\theta)}$$\nFinally, we substitute back $\\theta = \\beta/\\sigma_{\\hat{\\beta}}$ and multiply by $\\sigma_{\\hat{\\beta}}$ to find the conditional expectation of $\\hat{\\beta}$:\n$$\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c] = \\sigma_{\\hat{\\beta}} \\left( \\frac{\\beta}{\\sigma_{\\hat{\\beta}}} + \\frac{\\phi(c-\\beta/\\sigma_{\\hat{\\beta}}) - \\phi(c+\\beta/\\sigma_{\\hat{\\beta}})}{1 - \\Phi(c-\\beta/\\sigma_{\\hat{\\beta}}) + \\Phi(-c-\\beta/\\sigma_{\\hat{\\beta}})} \\right)$$\nThis simplifies to:\n$$\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c] = \\beta + \\sigma_{\\hat{\\beta}} \\frac{\\phi(c - \\beta/\\sigma_{\\hat{\\beta}}) - \\phi(c + \\beta/\\sigma_{\\hat{\\beta}})}{1 - \\Phi(c - \\beta/\\sigma_{\\hat{\\beta}}) + \\Phi(-c - \\beta/\\sigma_{\\hat{\\beta}})}$$\nThis is the required exact closed-form expression. The term added to $\\beta$ is the selection bias. If $\\beta  0$, this bias term is positive, indicating overestimation. If $\\beta  0$, it is negative, indicating overestimation of the effect magnitude. If $\\beta = 0$, the bias is $0$, because for a true null effect, positive and negative chance discoveries are equally likely and their expectations cancel.\n\n**Part 2: Parametric Bootstrap Proposal for Bias Correction**\n\nThe analytical formula derived above shows that the bias, $\\mathbb{E}[\\hat{\\beta} \\mid |Z|  c] - \\beta$, depends on the unknown true effect size $\\beta$. To estimate and correct for this bias, we use a parametric bootstrap procedure, which simulates the data-generating and selection processes. The procedure is based on first principles of resampling from a fitted model.\n\nLet $\\hat{\\beta}_{\\text{obs}}$ be the observed effect estimate from the original study, which is assumed to have been selected because $|\\hat{\\beta}_{\\text{obs}}/\\sigma_{\\hat{\\beta}}|  c$.\n\nThe steps of the bootstrap scheme are as follows:\n\n1.  **Specify the Data-Generating Mechanism**: The parametric model is $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma_{\\hat{\\beta}}^2)$. Our best guess for the unknown true parameter $\\beta$ is the observed estimate $\\hat{\\beta}_{\\text{obs}}$. We will, therefore, simulate from a world where the true effect is $\\hat{\\beta}_{\\text{obs}}$.\n    For a large number of bootstrap iterations, $b = 1, \\ldots, B$:\n    Generate a bootstrap effect estimate $\\hat{\\beta}^*_b$ by drawing from the fitted distribution:\n    $$\\hat{\\beta}^*_b \\sim \\mathcal{N}(\\hat{\\beta}_{\\text{obs}}, \\sigma_{\\hat{\\beta}}^2)$$\n\n2.  **Enforce the Selection Event**: The Winner's Curse arises from selection. We must mimic this filtering process in the bootstrap world. For each bootstrap sample $\\hat{\\beta}^*_b$, we calculate its corresponding Z-statistic, $Z^*_b = \\hat{\\beta}^*_b / \\sigma_{\\hat{\\beta}}$. We retain only those bootstrap samples that satisfy the original discovery criterion:\n    Keep $\\hat{\\beta}^*_b$ if and only if $|Z^*_b|  c$.\n    This creates a new (smaller) collection of selected bootstrap estimates, let us denote it by $S = \\{\\hat{\\beta}^*_b \\mid |\\hat{\\beta}^*_b / \\sigma_{\\hat{\\beta}}|  c\\}$.\n\n3.  **Compute the Bias-Corrected Estimator**: The bias in the bootstrap world is the difference between the average of the *selected* bootstrap estimates and the *true* parameter used for simulation.\n    - The true parameter in our simulation is $\\hat{\\beta}_{\\text{obs}}$.\n    - The expected value of a selected estimate is approximated by the mean of the samples in the set $S$:\n      $$\\bar{\\beta}^*_{\\text{sel}} = \\frac{1}{|S|}\\sum_{\\hat{\\beta}^* \\in S} \\hat{\\beta}^*$$\n    - The bootstrap estimate of the selection bias is therefore:\n      $$\\widehat{\\text{Bias}} = \\bar{\\beta}^*_{\\text{sel}} - \\hat{\\beta}_{\\text{obs}}$$\n    - The final bias-corrected estimator, $\\hat{\\beta}_{\\text{BC}}$, is obtained by subtracting this estimated bias from our original observation:\n      $$\\hat{\\beta}_{\\text{BC}} = \\hat{\\beta}_{\\text{obs}} - \\widehat{\\text{Bias}} = \\hat{\\beta}_{\\text{obs}} - (\\bar{\\beta}^*_{\\text{sel}} - \\hat{\\beta}_{\\text{obs}}) = 2\\hat{\\beta}_{\\text{obs}} - \\bar{\\beta}^*_{\\text{sel}}$$\n\nThis procedure provides a principled, simulation-based method to estimate the selection bias and produce a corrected effect size estimate that is, on average, closer to the true value.", "answer": "$$\\boxed{\\beta + \\sigma_{\\hat{\\beta}} \\frac{\\phi(c - \\beta/\\sigma_{\\hat{\\beta}}) - \\phi(c + \\beta/\\sigma_{\\hat{\\beta}})}{1 - \\Phi(c - \\beta/\\sigma_{\\hat{\\beta}}) + \\Phi(-c - \\beta/\\sigma_{\\hat{\\beta}})}}$$", "id": "2830616"}]}