{"hands_on_practices": [{"introduction": "A well-designed experiment is the foundation of reliable scientific conclusions. This first practice challenges you to perform a power analysis, a critical step that determines an experiment's ability to detect a true effect of a specified magnitude. By applying principles of Fisher information theory to a standard Hill-type dose-response model, you will learn how to plan studies with sufficient statistical power to compare the potency, measured by the $\\mathrm{EC}_{50}$, of two different treatments [@problem_id:2481284].", "problem": "You are tasked with constructing a program that performs a design-based power analysis to detect a two-fold difference in Effective Concentration 50 (EC50) between two treatments in a normalized dose–response experiment, under a set of specified assumptions. Your approach must start from a fundamental base in statistical modeling and information theory and proceed to a fully specified algorithm suitable for calculation.\n\nAssumptions and model structure:\n- The biological response at dose $x$ in treatment $t \\in \\{\\mathrm{A},\\mathrm{B}\\}$ is modeled as\n$$\nY \\mid x,t \\sim \\mathcal{N}\\!\\big(\\mu(x;\\theta_t),\\,\\sigma^2\\big),\n$$\nwith independent and identically distributed residuals of constant variance $\\sigma^2$ and independent across treatments and doses. Responses are normalized to span from $0$ to $1$ so that no additional scale parameters are needed.\n- The mean function is a Hill-type dose–response with fixed top and bottom asymptotes:\n$$\n\\mu(x;\\theta) \\;=\\; \\frac{1}{1 + \\left(\\frac{x}{\\mathrm{EC}_{50}}\\right)^h} \\;=\\; \\frac{1}{1 + \\exp\\!\\big(h(\\ln x - \\theta)\\big)},\n$$\nwhere $\\theta = \\ln(\\mathrm{EC}_{50})$ and $h$ is a known Hill slope shared by both treatments. The top is fixed at $1$ and the bottom at $0$.\n- Treatment $\\mathrm{A}$ has parameter $\\theta_{\\mathrm{A}} = \\ln(\\mathrm{EC}_{50,\\mathrm{A}})$, and treatment $\\mathrm{B}$ has $\\theta_{\\mathrm{B}} = \\theta_{\\mathrm{A}} + \\ln f$, where $f$ is the fold-change in EC50. The null hypothesis is $H_0: \\theta_{\\mathrm{B}} - \\theta_{\\mathrm{A}} = 0$, and the alternative is $H_1: \\theta_{\\mathrm{B}} - \\theta_{\\mathrm{A}} = \\ln f$. You must compute power under $H_1$ for a two-sided Wald test at significance level $\\alpha$.\n- Design is specified by a set of doses $\\{x_i\\}_{i=1}^m$ with $r_i$ independent replicates at each dose in each treatment. Assume the same allocation $\\{(x_i, r_i)\\}$ is used in both treatments.\n\nFundamental base for derivation:\n- For a nonlinear mean function under a Gaussian model with known variance, the per-observation Fisher information for a scalar parameter $\\theta$ is\n$$\n\\mathcal{I}(\\theta) \\;=\\; \\frac{1}{\\sigma^2} \\left(\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}\\right)^2.\n$$\n- For independent observations, Fisher information adds across observations and across dose groups (via replicate counts). In large samples, the maximum likelihood estimator has variance approximately equal to the inverse Fisher information.\n- A Wald statistic for testing $H_0: \\delta = 0$ with estimator $\\widehat{\\delta}$ satisfies\n$$\nZ \\;=\\; \\frac{\\widehat{\\delta}}{\\mathrm{SE}(\\widehat{\\delta})} \\;\\approx\\; \\mathcal{N}\\!\\big(\\delta/\\mathrm{SE}(\\widehat{\\delta}),\\,1\\big),\n$$\nso that two-sided power at level $\\alpha$ under true $\\delta \\neq 0$ is\n$$\n\\text{Power} \\;=\\; \\Pr\\big(|Z|  z_{1-\\alpha/2}\\big) \\;=\\; 1 - \\Phi\\!\\big(z_{1-\\alpha/2} - \\lambda\\big) + \\Phi\\!\\big(-z_{1-\\alpha/2} - \\lambda\\big),\n$$\nwhere $\\lambda = |\\delta|/\\mathrm{SE}(\\widehat{\\delta})$ is the noncentrality parameter and $\\Phi$ is the standard normal cumulative distribution function.\n\nRequired derivation and algorithmic steps to implement:\n1. Using the specified mean function, derive $\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}$ and express the total Fisher information for $\\theta$ for a given treatment in terms of $\\{x_i,r_i\\}$, $h$, and $\\sigma^2$.\n2. Express the asymptotic variance of $\\widehat{\\theta}_t$ for each treatment $t \\in \\{\\mathrm{A},\\mathrm{B}\\}$ as the inverse of its Fisher information evaluated at the true $\\theta_t$ under $H_1$.\n3. Since the two treatments are measured independently under the given assumptions, the variance of the difference estimator $\\widehat{\\delta} = \\widehat{\\theta}_{\\mathrm{B}} - \\widehat{\\theta}_{\\mathrm{A}}$ is the sum of the two variances. Thus,\n$$\n\\mathrm{SE}(\\widehat{\\delta}) \\;=\\; \\sqrt{\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) + \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}})}.\n$$\n4. Compute the noncentrality parameter $\\lambda = |\\ln f| / \\mathrm{SE}(\\widehat{\\delta})$, the critical value $z_{1-\\alpha/2}$, and finally the power using the two-sided normal formula above.\n\nAngle units are not applicable. If any quantity is to be expressed as a percentage, you must instead express it as a decimal fraction. Your program must output each power value as a decimal fraction in $[0,1]$, rounded to four decimal places.\n\nTest suite:\nFor each case below, compute the two-sided power to detect a two-fold shift, that is, use $f=2$.\n\n- Case 1 (balanced, informative, moderate variance):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{1,\\,3,\\,10,\\,30,\\,100\\}$\n  - Replicates per dose per treatment: $\\{6,\\,6,\\,6,\\,6,\\,6\\}$\n  - Residual standard deviation: $\\sigma = 0.1$\n  - Hill slope: $h = 1.2$\n  - Baseline $\\mathrm{EC}_{50,\\mathrm{A}} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\n- Case 2 (doses far below the baseline $\\mathrm{EC}_{50}$, lower information):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{0.01,\\,0.03,\\,0.1,\\,0.3,\\,1\\}$\n  - Replicates per dose per treatment: $\\{4,\\,4,\\,4,\\,4,\\,4\\}$\n  - Residual standard deviation: $\\sigma = 0.15$\n  - Hill slope: $h = 1.2$\n  - Baseline $\\mathrm{EC}_{50,\\mathrm{A}} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\n- Case 3 (highly informative near $\\mathrm{EC}_{50}$, small variance):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{6,\\,8,\\,10,\\,12,\\,15\\}$\n  - Replicates per dose per treatment: $\\{20,\\,20,\\,20,\\,20,\\,20\\}$\n  - Residual standard deviation: $\\sigma = 0.05$\n  - Hill slope: $h = 2.0$\n  - Baseline $\\mathrm{EC}_{50,\\mathrm{A}} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\n- Case 4 (sparse design, minimal replication):\n  - Doses (in $\\mu\\mathrm{M}$): $\\{5,\\,10,\\,20\\}$\n  - Replicates per dose per treatment: $\\{1,\\,1,\\,1\\}$\n  - Residual standard deviation: $\\sigma = 0.1$\n  - Hill slope: $h = 1.0$\n  - Baseline $\\mathrm{EC}_{50,\\mathrm{A}} = 10\\,\\mu\\mathrm{M}$, so $\\theta_{\\mathrm{A}} = \\ln(10)$\n  - Significance level: $\\alpha = 0.05$\n\nFinal output format:\n- Your program must produce a single line of output containing the four power values corresponding to the four cases, ordered as above, as a comma-separated list enclosed in square brackets, for example, $[\\;0.8021,\\,0.0543,\\,0.9999,\\,0.4120\\;]$. Each value must be rounded to four decimal places and expressed as a decimal fraction (no percentage sign).", "solution": "The problem is subjected to validation and is determined to be valid. It is scientifically grounded in established statistical theory for nonlinear regression and power analysis. The model, assumptions, and required derivations are clearly defined, objective, and self-contained. The problem is well-posed, and all necessary parameters for the specified test cases are provided. No inconsistencies, ambiguities, or factual unsoundness are present. We may therefore proceed with the solution.\n\nThe task is to compute the statistical power to detect a two-fold difference in the $\\mathrm{EC}_{50}$ parameter between two treatments, A and B, in a dose-response experiment. The derivation and computation will follow the structured approach based on large-sample properties of maximum likelihood estimators and the Fisher information matrix.\n\nThe biological response $Y$ at a given dose $x$ for a treatment $t$ is modeled by a normal distribution $Y \\mid x,t \\sim \\mathcal{N}(\\mu(x;\\theta_t), \\sigma^2)$, where the mean response function $\\mu(x;\\theta)$ follows a four-parameter logistic model (Hill equation) with fixed top and bottom asymptotes of $1$ and $0$, respectively. The mean function is given by:\n$$\n\\mu(x;\\theta) = \\frac{1}{1 + \\exp(h(\\ln x - \\theta))}\n$$\nHere, $\\theta = \\ln(\\mathrm{EC}_{50})$ is the parameter of interest representing the natural logarithm of the effective concentration giving $50\\%$ response, and $h$ is the Hill slope, assumed known and constant across treatments.\n\nThe analysis proceeds in four steps as prescribed.\n\n**Step 1: Derivation of the Partial Derivative and Fisher Information**\n\nFirst, we derive the partial derivative of the mean function $\\mu(x;\\theta)$ with respect to the parameter $\\theta$. This derivative is essential for computing the Fisher information. Using the chain rule, let $u(x;\\theta) = h(\\ln x - \\theta)$. Then $\\mu = (1 + e^u)^{-1}$.\n$$\n\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta} = -\\frac{1}{(1 + e^u)^2} \\cdot \\frac{\\partial}{\\partial \\theta}(e^u) = -\\frac{e^u}{(1 + e^u)^2} \\cdot \\frac{\\partial u}{\\partial \\theta}\n$$\nThe derivative of $u$ with respect to $\\theta$ is:\n$$\n\\frac{\\partial u}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta}(h(\\ln x - \\theta)) = -h\n$$\nSubstituting this back, we obtain:\n$$\n\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta} = -\\frac{\\exp(h(\\ln x - \\theta))}{(1 + \\exp(h(\\ln x - \\theta)))^2} \\cdot (-h) = \\frac{h \\exp(h(\\ln x - \\theta))}{(1 + \\exp(h(\\ln x - \\theta)))^2}\n$$\nThis expression can be elegantly simplified by observing that $\\mu(x;\\theta) = \\frac{1}{1 + e^u}$ and $1 - \\mu(x;\\theta) = \\frac{e^u}{1 + e^u}$. Therefore, the derivative is:\n$$\n\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta} = h \\cdot \\left(\\frac{1}{1+e^u}\\right) \\cdot \\left(\\frac{e^u}{1+e^u}\\right) = h \\cdot \\mu(x;\\theta) \\cdot (1 - \\mu(x;\\theta))\n$$\nThe per-observation Fisher information for $\\theta$ at a given dose $x$ is defined as $\\mathcal{I}(\\theta; x) = \\frac{1}{\\sigma^2} \\left(\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}\\right)^2$. Substituting our derivative, we get:\n$$\n\\mathcal{I}(\\theta; x) = \\frac{1}{\\sigma^2} \\left[ h \\cdot \\mu(x;\\theta)(1 - \\mu(x;\\theta)) \\right]^2 = \\frac{h^2}{\\sigma^2} [\\mu(x;\\theta)(1 - \\mu(x;\\theta))]^2\n$$\nFor an experimental design specified by a set of $m$ doses $\\{x_i\\}_{i=1}^m$ with $r_i$ replicates at each dose, the total Fisher information for one treatment is the sum over all independent observations:\n$$\n\\mathcal{I}_{\\text{total}}(\\theta) = \\sum_{i=1}^{m} r_i \\cdot \\mathcal{I}(\\theta; x_i) = \\frac{h^2}{\\sigma^2} \\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta)(1 - \\mu(x_i;\\theta))]^2\n$$\n\n**Step 2: Asymptotic Variance of Parameter Estimators**\n\nAccording to large-sample theory, the variance of the maximum likelihood estimator $\\widehat{\\theta}$ is approximated by the inverse of the total Fisher information, $\\mathrm{Var}(\\widehat{\\theta}) \\approx [\\mathcal{I}_{\\text{total}}(\\theta)]^{-1}$. We compute this variance for each treatment, evaluated at the true parameter values under the alternative hypothesis, $H_1$.\n\nUnder $H_1$, the true parameters are $\\theta_{\\mathrm{A}} = \\ln(\\mathrm{EC}_{50,\\mathrm{A}})$ for treatment A and $\\theta_{\\mathrm{B}} = \\theta_{\\mathrm{A}} + \\ln f$ for treatment B, where $f$ is the fold-change, given as $f=2$.\n\nThe variance for the estimator of $\\theta_{\\mathrm{A}}$ is:\n$$\n\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) \\approx \\left( \\frac{h^2}{\\sigma^2} \\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{A}})(1 - \\mu(x_i;\\theta_{\\mathrm{A}}))]^2 \\right)^{-1}\n$$\nSimilarly, the variance for the estimator of $\\theta_{\\mathrm{B}}$ is:\n$$\n\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}}) \\approx \\left( \\frac{h^2}{\\sigma^2} \\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{B}})(1 - \\mu(x_i;\\theta_{\\mathrm{B}}))]^2 \\right)^{-1}\n$$\n\n**Step 3: Standard Error of the Difference**\n\nWe are testing the difference $\\delta = \\theta_{\\mathrm{B}} - \\theta_{\\mathrm{A}}$. The estimator for this difference is $\\widehat{\\delta} = \\widehat{\\theta}_{\\mathrm{B}} - \\widehat{\\theta}_{\\mathrm{A}}$. Since the experiments for treatments A and B are independent, the variance of $\\widehat{\\delta}$ is the sum of the individual variances:\n$$\n\\mathrm{Var}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) + \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}})\n$$\nThe standard error of the difference, $\\mathrm{SE}(\\widehat{\\delta})$, is the square root of this variance:\n$$\n\\mathrm{SE}(\\widehat{\\delta}) = \\sqrt{\\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{A}}) + \\mathrm{Var}(\\widehat{\\theta}_{\\mathrm{B}})}\n$$\nSubstituting the expressions for the variances, we have:\n$$\n\\mathrm{SE}(\\widehat{\\delta}) = \\sqrt{ \\frac{\\sigma^2}{h^2} \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{A}})(1-\\mu(x_i;\\theta_{\\mathrm{A}}))]^2\\right)^{-1} + \\frac{\\sigma^2}{h^2} \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{B}})(1-\\mu(x_i;\\theta_{\\mathrm{B}}))]^2\\right)^{-1} }\n$$\nThis simplifies to:\n$$\n\\mathrm{SE}(\\widehat{\\delta}) = \\frac{\\sigma}{h} \\sqrt{ \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{A}})(1-\\mu(x_i;\\theta_{\\mathrm{A}}))]^2\\right)^{-1} + \\left(\\sum_{i=1}^{m} r_i [\\mu(x_i;\\theta_{\\mathrm{B}})(1-\\mu(x_i;\\theta_{\\mathrm{B}}))]^2\\right)^{-1} }\n$$\n\n**Step 4: Power Calculation**\n\nThe final step is to compute the power of the two-sided Wald test. The test statistic under $H_1$ is approximately normally distributed: $Z = \\frac{\\widehat{\\delta}}{\\mathrm{SE}(\\widehat{\\delta})} \\approx \\mathcal{N}(\\lambda, 1)$, where $\\lambda = \\frac{\\delta}{\\mathrm{SE}(\\widehat{\\delta})}$ is the noncentrality parameter. The true difference under $H_1$ is $\\delta = \\ln f$.\n$$\n\\lambda = \\frac{|\\ln f|}{\\mathrm{SE}(\\widehat{\\delta})}\n$$\nThe power of a two-sided test at significance level $\\alpha$ is the probability of rejecting the null hypothesis $H_0: \\delta=0$ when the alternative $H_1$ is true. This probability is:\n$$\n\\text{Power} = \\Pr\\big(|Z|  z_{1-\\alpha/2} \\mid H_1\\big)\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. This can be computed using the standard normal cumulative distribution function, $\\Phi$:\n$$\n\\text{Power} = 1 - (\\Phi(z_{1-\\alpha/2} - \\lambda) - \\Phi(-z_{1-\\alpha/2} - \\lambda))\n$$\nThis is the final formula for power. The implementation will follow these derived steps. For each test case, we will substitute the given values for $\\{x_i, r_i\\}$, $\\sigma$, $h$, $\\mathrm{EC}_{50,\\mathrm{A}}$, $f=2$, and $\\alpha=0.05$ to compute the power.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves for the statistical power in four dose-response experiment scenarios.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: balanced, informative, moderate variance\n        {\n            \"doses\": np.array([1.0, 3.0, 10.0, 30.0, 100.0]),\n            \"replicates\": np.array([6, 6, 6, 6, 6]),\n            \"sigma\": 0.1,\n            \"h\": 1.2,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        },\n        # Case 2: doses far below baseline EC50, lower information\n        {\n            \"doses\": np.array([0.01, 0.03, 0.1, 0.3, 1.0]),\n            \"replicates\": np.array([4, 4, 4, 4, 4]),\n            \"sigma\": 0.15,\n            \"h\": 1.2,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        },\n        # Case 3: highly informative near EC50, small variance\n        {\n            \"doses\": np.array([6.0, 8.0, 10.0, 12.0, 15.0]),\n            \"replicates\": np.array([20, 20, 20, 20, 20]),\n            \"sigma\": 0.05,\n            \"h\": 2.0,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        },\n        # Case 4: sparse design, minimal replication\n        {\n            \"doses\": np.array([5.0, 10.0, 20.0]),\n            \"replicates\": np.array([1, 1, 1]),\n            \"sigma\": 0.1,\n            \"h\": 1.0,\n            \"ec50_a\": 10.0,\n            \"f\": 2.0,\n            \"alpha\": 0.05\n        }\n    ]\n\n    def calculate_power(doses, replicates, sigma, h, ec50_a, f, alpha):\n        \"\"\"\n        Calculates statistical power based on Fisher information for a given design.\n        \"\"\"\n        # Step 1: Define parameters and mean function\n        theta_a = np.log(ec50_a)\n        theta_b = theta_a + np.log(f)\n\n        def mean_response(x, theta):\n            # Hill-type dose-response function\n            return 1.0 / (1.0 + np.exp(h * (np.log(x) - theta)))\n\n        # Step 2: Calculate total Fisher Information components\n        def get_fisher_info_summand(theta):\n            # Calculates the sum part of the Fisher Information formula\n            # Sum over i of r_i * [mu_i * (1-mu_i)]^2\n            sum_val = 0.0\n            for i in range(len(doses)):\n                mu = mean_response(doses[i], theta)\n                # The weight for each observation is [mu * (1-mu)]^2\n                weight = (mu * (1.0 - mu))**2\n                sum_val += replicates[i] * weight\n            return sum_val\n\n        sum_fisher_a = get_fisher_info_summand(theta_a)\n        sum_fisher_b = get_fisher_info_summand(theta_b)\n        \n        # Step 3: Calculate variances and standard error of the difference\n        # Var(theta_hat) = 1 / I_total(theta) = (sigma^2/h^2) / Sum(...)\n        if sum_fisher_a = 0 or sum_fisher_b = 0:\n            # Avoid division by zero for non-informative designs\n            return 0.0\n            \n        var_theta_a = (sigma**2 / h**2) / sum_fisher_a\n        var_theta_b = (sigma**2 / h**2) / sum_fisher_b\n        \n        se_delta = np.sqrt(var_theta_a + var_theta_b)\n\n        # Step 4: Calculate noncentrality parameter and power\n        delta = np.log(f)\n        if se_delta = 0:\n             return 1.0 if delta != 0 else alpha\n        \n        noncentrality_param = np.abs(delta) / se_delta\n        \n        z_crit = norm.ppf(1.0 - alpha / 2.0)\n        \n        power = 1.0 - (norm.cdf(z_crit - noncentrality_param) - norm.cdf(-z_crit - noncentrality_param))\n        \n        return power\n\n    results = []\n    for case in test_cases:\n        power = calculate_power(\n            doses=case[\"doses\"],\n            replicates=case[\"replicates\"],\n            sigma=case[\"sigma\"],\n            h=case[\"h\"],\n            ec50_a=case[\"ec50_a\"],\n            f=case[\"f\"],\n            alpha=case[\"alpha\"]\n        )\n        # Round to four decimal places\n        results.append(round(power, 4))\n    \n    # Format results as a string with no spaces as per template\n    # Example format required: \"[0.8021,0.0543,0.9999,0.4120]\"\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2481284"}, {"introduction": "Ecotoxicological studies often generate complex, longitudinal data, such as tracking the growth of individuals over time. This exercise moves from design to analysis, introducing nonlinear mixed-effects models (NLME) as the appropriate framework for such data structures. You will construct a model that incorporates individual-specific random effects and serially correlated errors to estimate the $\\mathrm{EC}_{50}$ from repeated growth measurements, providing a much richer analysis than is possible with simple summary statistics [@problem_id:2481338].", "problem": "You are given repeated-measures growth data generated under constant chemical exposure for multiple individuals in an ecological toxicity study. The scientific goal is to estimate the half-maximal effective concentration (EC50), defined as the concentration at which the effect on growth rate reaches one-half of its maximal difference relative to control, under a mechanistic nonlinear mixed-effects model. You must implement a program that computes the maximum likelihood estimate (MLE) of EC50 in milligrams per liter for each of several specified test cases, and outputs the estimates as floats in a single line.\n\nFundamental base and model components:\n- The effective concentration at half-maximal effect (EC50) is a parameter that scales exposure-response according to a monotone inhibitory effect function on the intrinsic growth rate. Let the growth rate under exposure concentration $C$ be $r(C)$.\n- The inhibitory effect function follows a Hill-type form with Hill coefficient fixed at $h = 1$, so that the concentration-dependent growth rate is given by the mapping $r(C) = \\dfrac{r_{\\max}}{1 + \\left(C/\\theta\\right)^{h}}$, where $\\theta$ is the EC50 to be estimated, $r_{\\max}$ is the maximal growth rate in the absence of toxicant effect, and $h$ is fixed at $1$.\n- For an individual $i$ exposed to a constant concentration $C_i$, observed at times $t_{i1}, \\dots, t_{iT_i}$ (in days), the mean structural growth trajectory is modeled as $m_{ij}(b_i, \\theta) = K_i \\left(1 - \\exp\\left(- r(C_i) \\, t_{ij}\\right)\\right)$, where the asymptotic size parameter is $K_i = \\exp(\\mu_K + b_i)$ with $b_i$ a subject-specific random effect.\n- The random effect is specified as $b_i \\sim \\mathcal{N}(0, \\tau^2)$ independently across individuals, and thus $K_i$ is lognormally distributed.\n- The residual error process within each individual follows a first-order autoregressive process (AR($1$)) with Gaussian innovations, defined as $e_{i1} \\sim \\mathcal{N}\\!\\left(0, \\sigma^2/(1-\\rho^2)\\right)$ and, for $j \\ge 2$, $e_{ij} = \\rho \\, e_{i,j-1} + u_{ij}$ with $u_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, where $|\\rho|  1$, $\\sigma^2  0$.\n- The observation model is $y_{ij} = m_{ij}(b_i, \\theta) + e_{ij}$.\n\nLikelihood basis:\n- Conditional on $b_i$, the observation vector $\\mathbf{y}_i = (y_{i1}, \\dots, y_{iT_i})^\\top$ has a multivariate normal distribution implied by the AR($1$) residual construction above. The conditional likelihood factors into $p(y_{i1} \\mid b_i) \\prod_{j=2}^{T_i} p(y_{ij} \\mid y_{i,j-1}, b_i)$ due to the Gaussian Markov property of the AR($1$) process.\n- The marginal likelihood for $\\mathbf{y}_i$ integrates over the random effect: $L_i(\\theta) = \\int \\left[\\prod_{j=1}^{T_i} p(y_{ij} \\mid b_i, \\theta)\\right] \\phi(b_i; 0, \\tau^2) \\, \\mathrm{d}b_i$, where $\\phi(\\cdot;0,\\tau^2)$ is the normal density with mean $0$ and variance $\\tau^2$.\n- The total log-likelihood is $\\ell(\\theta) = \\sum_{i=1}^{N} \\log L_i(\\theta)$.\n\nComputational requirement:\n- The integral over $b_i$ has no closed form for this nonlinear mean function; you must approximate it using Gaussian–Hermite quadrature with at least $n = 21$ nodes. Use the identity\n$$\n\\int_{-\\infty}^{\\infty} \\phi(b; 0, \\tau^2) \\, f(b) \\, \\mathrm{d}b \\;=\\; \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} e^{-x^2} f(\\sqrt{2}\\,\\tau x) \\, \\mathrm{d}x,\n$$\nso that the quadrature approximation is\n$$\n\\int \\phi(b;0,\\tau^2) f(b) \\, \\mathrm{d}b \\;\\approx\\; \\frac{1}{\\sqrt{\\pi}} \\sum_{k=1}^{n} w_k \\, f\\!\\left(\\sqrt{2}\\,\\tau \\, x_k\\right),\n$$\nwhere $\\{x_k, w_k\\}_{k=1}^{n}$ are the $n$-point Gaussian–Hermite nodes and weights.\n- You must maximize $\\ell(\\theta)$ over $\\theta  0$. Perform a univariate bounded optimization over $\\theta \\in [a, b]$ with $a = 10^{-2}$ and $b = 10^{2}$, and report the MLE of $\\theta$ (EC50) in milligrams per liter as a float, rounded to three decimal places.\n\nData generation protocol (to be reproduced exactly by your program):\n- For each test case, use the specified parameters and a pseudorandom seed to generate data according to the model. For individual $i$, draw $b_i \\sim \\mathcal{N}(0, \\tau^2)$ and set $K_i = \\exp(\\mu_K + b_i)$. For the AR($1$) residuals, draw $e_{i1} \\sim \\mathcal{N}(0, \\sigma^2/(1-\\rho^2))$ and then $e_{ij} = \\rho e_{i,j-1} + u_{ij}$ with $u_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ independently. The observation is $y_{ij} = K_i \\left(1 - \\exp\\left(- r(C_i) \\, t_{ij}\\right)\\right) + e_{ij}$, where $r(C_i) = \\dfrac{r_{\\max}}{1 + (C_i/\\theta_{\\mathrm{true}})^{h}}$ and $h = 1$. All times are in days, all concentrations and EC50 are in milligrams per liter, and $r_{\\max}$ is in day$^{-1}$. The true EC50 used to generate each dataset is provided, but must be treated as unknown in estimation.\n\nTest suite:\nFor each case, your program must generate the dataset using the specified seed and parameters, then compute the MLE of $\\theta$ (EC50) as described. The cases are:\n\n- Case A:\n    - Seed: $202311$\n    - Individuals: $N = 6$\n    - Times (days): $[1, 2, 4, 7, 10, 14]$\n    - Exposures (mg/L): $[0.5, 1.0, 2.0, 4.0, 8.0, 16.0]$\n    - True EC50 (mg/L): $\\theta_{\\mathrm{true}} = 4.0$\n    - $r_{\\max} = 0.6$ day$^{-1}$\n    - $\\mu_K = \\ln(100)$\n    - $\\tau = 0.2$\n    - $\\sigma = 3.0$\n    - $\\rho = 0.5$\n    - $h = 1$\n\n- Case B:\n    - Seed: $202312$\n    - Individuals: $N = 8$\n    - Times (days): $[1, 2, 3, 5, 8, 12]$\n    - Exposures (mg/L): $[0.25, 0.75, 1.5, 3.0, 6.0, 12.0, 18.0, 24.0]$\n    - True EC50 (mg/L): $\\theta_{\\mathrm{true}} = 6.0$\n    - $r_{\\max} = 0.5$ day$^{-1}$\n    - $\\mu_K = \\ln(80)$\n    - $\\tau = 0.15$\n    - $\\sigma = 2.0$\n    - $\\rho = 0.2$\n    - $h = 1$\n\n- Case C:\n    - Seed: $202313$\n    - Individuals: $N = 5$\n    - Times (days): $[2, 4, 6, 9, 13]$\n    - Exposures (mg/L): $[1.0, 2.0, 5.0, 10.0, 20.0]$\n    - True EC50 (mg/L): $\\theta_{\\mathrm{true}} = 5.0$\n    - $r_{\\max} = 0.7$ day$^{-1}$\n    - $\\mu_K = \\ln(150)$\n    - $\\tau = 0.3$\n    - $\\sigma = 4.0$\n    - $\\rho = 0.8$\n    - $h = 1$\n\n- Case D:\n    - Seed: $202314$\n    - Individuals: $N = 4$\n    - Times (days): $[1, 3, 7, 14]$\n    - Exposures (mg/L): $[0.5, 2.0, 4.0, 12.0]$\n    - True EC50 (mg/L): $\\theta_{\\mathrm{true}} = 3.0$\n    - $r_{\\max} = 0.4$ day$^{-1}$\n    - $\\mu_K = \\ln(60)$\n    - $\\tau = 0.05$\n    - $\\sigma = 1.0$\n    - $\\rho = 0.0$\n    - $h = 1$\n\nWhat you must compute and output:\n- For each case, compute the MLE $\\hat{\\theta}$ of EC50 in milligrams per liter by maximizing the marginal log-likelihood $\\ell(\\theta)$ constructed as above with Gaussian–Hermite quadrature using at least $n = 21$ nodes.\n- Use a bounded univariate optimizer over $\\theta \\in [10^{-2}, 10^{2}]$. Internally, you may enforce positivity by reparameterizing if desired, but the reported value must be on the original scale.\n- Report the four estimated EC50 values as floats, each rounded to three decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, Case D (e.g., \"[$\\hat{\\theta}_A,\\hat{\\theta}_B,\\hat{\\theta}_C,\\hat{\\theta}_D$]\"), but without any units in the output line.\n\nAll assumptions and parameter values must be implemented exactly as specified. No user input is allowed. The output must be reproducible. Express EC50 in milligrams per liter as floats rounded to three decimal places.", "solution": "The problem statement has been critically examined and found to be valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard, albeit computationally intensive, task in statistical modeling for ecotoxicology: the maximum likelihood estimation of a parameter in a nonlinear mixed-effects model. The model components, including the von Bertalanffy-type growth function, Hill-type dose-response relationship, log-normal random effects, and an AR($1$) residual error structure, are all standard constructs in this field. The instructions for data generation and numerical approximation are precise and allow for a unique, verifiable solution. We shall proceed with the derivation and implementation of the solution.\n\nThe objective is to find the maximum likelihood estimate (MLE) of the half-maximal effective concentration, denoted $\\theta$ (EC50). The MLE $\\hat{\\theta}$ is the value of $\\theta$ that maximizes the total log-likelihood function, $\\ell(\\theta)$, for the observed data. The data consists of $N$ individuals, each with a set of observations $\\mathbf{y}_i = (y_{i1}, \\dots, y_{iT_i})^\\top$ measured at times $\\mathbf{t}_i = (t_{i1}, \\dots, t_{iT_i})^\\top$ under a constant exposure concentration $C_i$.\n\nThe total log-likelihood is the sum of the log-likelihoods for each individual, assuming independence across individuals:\n$$\n\\ell(\\theta) = \\sum_{i=1}^{N} \\log L_i(\\theta)\n$$\nHere, $L_i(\\theta)$ is the marginal likelihood for the observation vector $\\mathbf{y}_i$ of individual $i$. This is obtained by integrating the conditional likelihood $p(\\mathbf{y}_i \\mid b_i, \\theta)$ over the distribution of the individual-specific random effect $b_i$. The random effect $b_i$ is specified to follow a normal distribution $b_i \\sim \\mathcal{N}(0, \\tau^2)$, with density $\\phi(b_i; 0, \\tau^2)$. Thus, the marginal likelihood is:\n$$\nL_i(\\theta) = \\int_{-\\infty}^{\\infty} p(\\mathbf{y}_i \\mid b_i, \\theta) \\, \\phi(b_i; 0, \\tau^2) \\, \\mathrm{d}b_i\n$$\nThe term $p(\\mathbf{y}_i \\mid b_i, \\theta)$ is the joint probability density of the observations for individual $i$, conditional on the random effect $b_i$ and the parameter $\\theta$. The problem specifies a first-order autoregressive, AR($1$), process for the residuals $e_{ij} = y_{ij} - m_{ij}(b_i, \\theta)$. Due to the Markov property of the AR($1$) process, the conditional likelihood can be factored as:\n$$\np(\\mathbf{y}_i \\mid b_i, \\theta) = p(y_{i1} \\mid b_i, \\theta) \\prod_{j=2}^{T_i} p(y_{ij} \\mid y_{i,j-1}, b_i, \\theta)\n$$\nThe terms in this product are densities of normal distributions:\n1.  The first observation $y_{i1}$ is conditional only on $b_i$ and $\\theta$. Its distribution is $y_{i1} \\sim \\mathcal{N}\\left(m_{i1}(b_i, \\theta), \\frac{\\sigma^2}{1-\\rho^2}\\right)$.\n2.  For subsequent observations ($j \\ge 2$), the distribution of $y_{ij}$ is conditional on the previous observation $y_{i,j-1}$, as well as $b_i$ and $\\theta$. The relationship $e_{ij} = \\rho e_{i,j-1} + u_{ij}$ implies $y_{ij} - m_{ij} = \\rho(y_{i,j-1} - m_{i,j-1}) + u_{ij}$, where $u_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$. Therefore, $y_{ij} \\mid y_{i,j-1}, b_i, \\theta \\sim \\mathcal{N}\\left(m_{ij} + \\rho(y_{i,j-1} - m_{i,j-1}), \\sigma^2\\right)$.\n\nThe mean trajectory function $m_{ij}(b_i, \\theta)$ is given by:\n$$\nm_{ij}(b_i, \\theta) = K_i \\left(1 - \\exp\\left(- r(C_i, \\theta) \\, t_{ij}\\right)\\right)\n$$\nwhere $K_i = \\exp(\\mu_K + b_i)$ and $r(C_i, \\theta) = \\dfrac{r_{\\max}}{1 + (C_i/\\theta)^{h}}$ with Hill coefficient $h=1$.\n\nThe integral for $L_i(\\theta)$ does not have a closed-form solution due to the nonlinear dependence of $m_{ij}$ on $b_i$. It must be approximated numerically. The problem specifies Gaussian-Hermite quadrature. Using the provided identity, the integral is approximated as:\n$$\nL_i(\\theta) \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{k=1}^{n} w_k \\, p(\\mathbf{y}_i \\mid b_k, \\theta)\n$$\nwhere $\\{x_k, w_k\\}_{k=1}^n$ are the $n$-point Gaussian-Hermite nodes and weights, and the random effect values are evaluated at $b_k = \\sqrt{2}\\tau x_k$. We will use $n=21$ nodes.\n\nTo implement this robustly, we compute the log of the marginal likelihood, $\\log L_i(\\theta)$. Direct computation of the sum can lead to numerical underflow or overflow. We apply the log-sum-exp trick. The individual log-likelihood is:\n$$\n\\log L_i(\\theta) \\approx \\log\\left(\\frac{1}{\\sqrt{\\pi}}\\right) + \\log\\left(\\sum_{k=1}^{n} w_k \\, p(\\mathbf{y}_i \\mid b_k, \\theta)\\right) = -\\frac{1}{2}\\log\\pi + \\log\\left(\\sum_{k=1}^{n} \\exp\\left(\\log w_k + \\log p(\\mathbf{y}_i \\mid b_k, \\theta)\\right)\\right)\n$$\nLetting $A_k = \\log w_k + \\log p(\\mathbf{y}_i \\mid b_k, \\theta)$ and $A_{\\max} = \\max_k A_k$, the sum is computed as $A_{\\max} + \\log(\\sum_k \\exp(A_k - A_{\\max}))$.\n\nThe computational procedure is as follows:\n1.  For each test case, generate the dataset precisely according to the specified protocol. This involves setting the random seed, drawing the random effects and residual errors from their specified normal distributions, and computing the observations.\n2.  Define an objective function that computes the negative total log-likelihood, $-\\ell(\\theta)$, given a value of $\\theta$ and the generated data. This function iterates through each individual, calculates their marginal log-likelihood $\\log L_i(\\theta)$ using $n=21$ point Gaussian-Hermite quadrature with the log-sum-exp stabilization, and sums them up.\n3.  Use a numerical optimization routine to find the value of $\\theta$ that minimizes this objective function, subject to the bounds $\\theta \\in [10^{-2}, 10^{2}]$. This yields the MLE, $\\hat{\\theta}$.\n4.  The final result for each case is rounded to three decimal places. The complete implementation is provided in the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom numpy.polynomial.hermite import hermgauss\n\n# Global constants as per problem specification\nN_QUAD = 21\nOPT_BOUNDS = (1e-2, 1e2)\nGH_NODES, GH_WEIGHTS = hermgauss(N_QUAD)\n\ndef generate_data(seed, N, times, exposures, theta_true, r_max, mu_K, tau, sigma, rho, h):\n    \"\"\"\n    Generates simulated toxicity data for N individuals based on the model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    dataset = []\n\n    for i in range(N):\n        C_i = exposures[i]\n        \n        # 1. Draw random effect b_i and determine individual asymptotic size K_i\n        b_i = rng.normal(loc=0.0, scale=tau)\n        K_i = np.exp(mu_K + b_i)\n        \n        # 2. Calculate the concentration-dependent growth rate r(C_i)\n        r_c = r_max / (1.0 + (C_i / theta_true)**h)\n        \n        # 3. Generate AR(1) residual error series e_ij\n        T_i = len(times)\n        errors = np.zeros(T_i)\n        \n        # Variance of the first error term e_i1\n        var_e1 = sigma**2 / (1.0 - rho**2)\n        errors[0] = rng.normal(loc=0.0, scale=np.sqrt(var_e1))\n        \n        # Subsequent error terms e_ij for j  1\n        for j in range(1, T_i):\n            u_ij = rng.normal(loc=0.0, scale=sigma)\n            errors[j] = rho * errors[j-1] + u_ij\n            \n        # 4. Compute the mean trajectory and the final observations\n        mean_trajectory = K_i * (1.0 - np.exp(-r_c * times))\n        observations = mean_trajectory + errors\n        \n        dataset.append({'C': C_i, 't': times, 'y': observations})\n        \n    return dataset\n\ndef neg_log_likelihood(theta, data, r_max, mu_K, tau, sigma, rho, h):\n    \"\"\"\n    Calculates the negative marginal log-likelihood for the entire dataset.\n    This is the objective function for the optimizer.\n    \"\"\"\n    total_log_lik = 0.0\n\n    # Iterate over each individual in the dataset\n    for ind_data in data:\n        C_i, t_i, y_i = ind_data['C'], ind_data['t'], ind_data['y']\n        T_i = len(t_i)\n        \n        # Terms for the log-sum-exp computation over quadrature nodes\n        log_lik_terms = np.zeros(N_QUAD)\n\n        # Growth rate for the current candidate theta\n        r_c = r_max / (1.0 + (C_i / theta)**h)\n\n        # Iterate over Gaussian-Hermite quadrature nodes\n        for k in range(N_QUAD):\n            x_k, w_k = GH_NODES[k], GH_WEIGHTS[k]\n            \n            # Map quadrature node to the random effect scale\n            b_k = np.sqrt(2.0) * tau * x_k\n            \n            # Individual's asymptotic size for this value of the random effect\n            K_k = np.exp(mu_K + b_k)\n            \n            # Mean growth trajectory for this b_k and theta\n            m_k = K_k * (1.0 - np.exp(-r_c * t_i))\n            \n            # Calculate conditional log-likelihood log p(y_i | b_k, theta)\n            cond_log_lik = 0.0\n            \n            # Log-likelihood contribution from the first time point (j=1)\n            var1 = sigma**2 / (1.0 - rho**2)\n            log_pdf_1 = -0.5 * np.log(2.0 * np.pi * var1) - ((y_i[0] - m_k[0])**2) / (2.0 * var1)\n            cond_log_lik += log_pdf_1\n            \n            # Log-likelihood contribution from subsequent time points (j=2)\n            if T_i  1.0:\n                innovations = (y_i[1:] - m_k[1:]) - rho * (y_i[:-1] - m_k[:-1])\n                log_pdf_ar1 = -0.5 * np.log(2.0 * np.pi * sigma**2) - (innovations**2) / (2.0 * sigma**2)\n                cond_log_lik += np.sum(log_pdf_ar1)\n                \n            log_lik_terms[k] = np.log(w_k) + cond_log_lik\n\n        # Use log-sum-exp for stable computation of log(integral)\n        max_log = np.max(log_lik_terms)\n        log_L_i = max_log + np.log(np.sum(np.exp(log_lik_terms - max_log)))\n        \n        # Final individual log-likelihood including quadrature constant\n        individual_log_lik = -0.5 * np.log(np.pi) + log_L_i\n        \n        total_log_lik += individual_log_lik\n        \n    return -total_log_lik\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            'seed': 202311, 'N': 6, 'times': np.array([1, 2, 4, 7, 10, 14]),\n            'exposures': np.array([0.5, 1.0, 2.0, 4.0, 8.0, 16.0]), 'theta_true': 4.0,\n            'r_max': 0.6, 'mu_K': np.log(100), 'tau': 0.2, 'sigma': 3.0, 'rho': 0.5, 'h': 1\n        },\n        {\n            'seed': 202312, 'N': 8, 'times': np.array([1, 2, 3, 5, 8, 12]),\n            'exposures': np.array([0.25, 0.75, 1.5, 3.0, 6.0, 12.0, 18.0, 24.0]), 'theta_true': 6.0,\n            'r_max': 0.5, 'mu_K': np.log(80), 'tau': 0.15, 'sigma': 2.0, 'rho': 0.2, 'h': 1\n        },\n        {\n            'seed': 202313, 'N': 5, 'times': np.array([2, 4, 6, 9, 13]),\n            'exposures': np.array([1.0, 2.0, 5.0, 10.0, 20.0]), 'theta_true': 5.0,\n            'r_max': 0.7, 'mu_K': np.log(150), 'tau': 0.3, 'sigma': 4.0, 'rho': 0.8, 'h': 1\n        },\n        {\n            'seed': 202314, 'N': 4, 'times': np.array([1, 3, 7, 14]),\n            'exposures': np.array([0.5, 2.0, 4.0, 12.0]), 'theta_true': 3.0,\n            'r_max': 0.4, 'mu_K': np.log(60), 'tau': 0.05, 'sigma': 1.0, 'rho': 0.0, 'h': 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate data for the current case\n        data = generate_data(\n            case['seed'], case['N'], case['times'], case['exposures'], case['theta_true'],\n            case['r_max'], case['mu_K'], case['tau'], case['sigma'], case['rho'], case['h']\n        )\n        \n        # 2. Define the objective function for optimization\n        objective_func = lambda theta: neg_log_likelihood(\n            theta, data, case['r_max'], case['mu_K'], case['tau'],\n            case['sigma'], case['rho'], case['h']\n        )\n        \n        # 3. Perform bounded minimization to find the MLE of theta (EC50)\n        opt_result = minimize_scalar(\n            objective_func,\n            bounds=OPT_BOUNDS,\n            method='bounded'\n        )\n        \n        mle_theta = opt_result.x\n        results.append(f\"{mle_theta:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "2481338"}, {"introduction": "Scientific progress often involves synthesizing findings from multiple sources to build a more general understanding. In this final practice, we shift from analyzing a single experiment to a meta-analytic perspective using a hierarchical Bayesian model. You will learn to \"borrow strength\" across different species by modeling their $\\mathrm{EC}_{50}$ values as draws from a common distribution, which allows for more robust estimates and enables predictions for new, unobserved species [@problem_id:2481192].", "problem": "You are modeling interspecific variation in the half maximal effective concentration (EC50) of a contaminant using a hierarchical Bayesian framework on the logarithmic scale to respect the positivity of concentrations and to stabilize variability. Let $S$ denote the number of species with available dose–response summaries from carefully designed experiments. For each species $s \\in \\{1,\\dots,S\\}$, let $y_s$ be its observed effective concentration in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$ and define $x_s = \\log(y_s)$, where $\\log(\\cdot)$ is the natural logarithm. Assume the following hierarchical model: the latent species-level log-effects $x_s$ are independent and identically distributed as $x_s \\mid \\mu,\\tau^2 \\sim \\mathcal{N}(\\mu,\\tau^2)$, where $\\mu$ is the across-species mean log-effect and $\\tau^2$ is the across-species variance on the log-scale. Place a conjugate Normal–Inverse-Gamma prior on $(\\mu,\\tau^2)$ specified by $\\mu \\mid \\tau^2 \\sim \\mathcal{N}(m_0,\\tau^2/\\kappa_0)$ and $\\tau^2 \\sim \\mathrm{Inv\\mbox{-}Gamma}(a_0,b_0)$, with the Inverse-Gamma density parameterized as $p(\\tau^2) \\propto (\\tau^2)^{-(a_0+1)} \\exp(-b_0/\\tau^2)$ for $\\tau^20$.\n\nYour tasks are:\n- Starting from the stated model and prior only, derive the posterior for $(\\mu,\\tau^2)$ given data $\\{x_s\\}_{s=1}^S$, and then derive the posterior predictive distribution for the log-effect of a new species, $x_{\\mathrm{new}} = \\log(\\mathrm{EC50}_{\\mathrm{new}})$, marginalized over $(\\mu,\\tau^2)$. From this posterior predictive distribution on the log-scale, deduce expressions for the posterior median and a central credible interval of level $q$ for $\\mathrm{EC50}_{\\mathrm{new}}$ on the original concentration scale by applying appropriate monotone transformations.\n- Implement these expressions in a program that, for each test case below, computes:\n  1. the posterior median of $\\mathrm{EC50}_{\\mathrm{new}}$ in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$, and\n  2. the lower and upper bounds of the central credible interval of level $q = 0.90$ for $\\mathrm{EC50}_{\\mathrm{new}}$ in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$,\n  where all three reported values must be expressed in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$ and rounded to $6$ decimal places. The credible interval probability level $q$ must be interpreted as a central interval, that is, with tail probabilities $(1-q)/2$ in each tail, so the lower and upper quantiles correspond to probabilities $0.05$ and $0.95$ respectively.\n\nTest suite (each test case specifies observed concentrations $\\{y_s\\}_{s=1}^S$ in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$, which you must transform via $x_s=\\log(y_s)$, and prior hyperparameters $(m_0,\\kappa_0,a_0,b_0)$ on the log-scale):\n- Test case $1$:\n  - Data (in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$): $[\\,1.0,\\,1.5,\\,2.0,\\,0.8,\\,1.2\\,]$.\n  - Prior: $m_0=\\log(1.0)$, $\\kappa_0=1.0$, $a_0=2.0$, $b_0=0.1$.\n- Test case $2$:\n  - Data (in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$): $[\\,0.5,\\,0.6\\,]$.\n  - Prior: $m_0=\\log(0.7)$, $\\kappa_0=5.0$, $a_0=3.0$, $b_0=0.05$.\n- Test case $3$:\n  - Data (in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$): $[\\,0.05,\\,0.08,\\,0.07,\\,0.06,\\,0.09,\\,0.10\\,]$.\n  - Prior: $m_0=\\log(0.07)$, $\\kappa_0=0.1$, $a_0=1.0$, $b_0=0.01$.\n\nFinal output format:\n- For each test case, your program must compute the triple $[\\,\\mathrm{median},\\,\\mathrm{lower},\\,\\mathrm{upper}\\,]$, where $\\mathrm{median}$ is the posterior median of $\\mathrm{EC50}_{\\mathrm{new}}$ and $(\\mathrm{lower},\\mathrm{upper})$ are the bounds of the central credible interval with level $q=0.90$, all in $\\mathrm{mg}\\,\\mathrm{L}^{-1}$ and rounded to $6$ decimal places.\n- Aggregate the three per-case triples into a single list in the same order as the test cases and print exactly one line containing this aggregate as a comma-separated list enclosed in square brackets, for example, $[[v_{11},v_{12},v_{13}],[v_{21},v_{22},v_{23}],[v_{31},v_{32},v_{33}]]$, with no extra text.", "solution": "The problem posed is a standard exercise in hierarchical Bayesian modeling and is entirely valid. It is scientifically grounded, well-posed, and all necessary information for its resolution is provided. We shall proceed with the derivation and subsequent computation.\n\nThe problem requires us to derive the posterior predictive distribution for a new species' half maximal effective concentration, $\\mathrm{EC50}_{\\mathrm{new}}$, based on observations from $S$ other species. The modeling is performed on the natural logarithm of the concentrations, which is a standard and sound practice to accommodate the positive domain of concentrations and to stabilize variance.\n\nLet the observed concentrations for $S$ species be $\\{y_s\\}_{s=1}^S$. We define the log-concentrations as $x_s = \\log(y_s)$. The hierarchical model is specified as follows:\n- **Likelihood**: The species-level log-effects $x_s$ are assumed to be independent and identically distributed draws from a normal distribution, given the hierarchical parameters $\\mu$ and $\\tau^2$:\n$$x_s \\mid \\mu, \\tau^2 \\sim \\mathcal{N}(\\mu, \\tau^2)$$\n- **Prior**: A conjugate Normal-Inverse-Gamma prior is placed on the parameters $(\\mu, \\tau^2)$:\n$$\\mu \\mid \\tau^2 \\sim \\mathcal{N}(m_0, \\tau^2/\\kappa_0)$$\n$$\\tau^2 \\sim \\mathrm{Inv\\mbox{-}Gamma}(a_0, b_0)$$\nThe joint prior density is $p(\\mu, \\tau^2) = p(\\mu \\mid \\tau^2)p(\\tau^2)$.\n\n**1. Derivation of the Posterior Distribution for $(\\mu, \\tau^2)$**\n\nThe posterior distribution is proportional to the product of the likelihood and the prior, $p(\\mu, \\tau^2 \\mid \\mathbf{x}) \\propto p(\\mathbf{x} \\mid \\mu, \\tau^2) p(\\mu, \\tau^2)$.\nThe likelihood function for the data $\\mathbf{x} = \\{x_s\\}_{s=1}^S$ is:\n$$p(\\mathbf{x} \\mid \\mu, \\tau^2) = \\prod_{s=1}^S \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{(x_s - \\mu)^2}{2\\tau^2}\\right) \\propto (\\tau^2)^{-S/2} \\exp\\left(-\\frac{1}{2\\tau^2} \\sum_{s=1}^S (x_s - \\mu)^2\\right)$$\nThe sum of squares can be decomposed as $\\sum_{s=1}^S (x_s - \\mu)^2 = \\sum_{s=1}^S (x_s - \\bar{x})^2 + S(\\bar{x} - \\mu)^2$, where $\\bar{x} = \\frac{1}{S}\\sum_{s=1}^S x_s$ is the sample mean.\n\nThe joint prior density is:\n$$p(\\mu, \\tau^2) \\propto (\\tau^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0(\\mu-m_0)^2}{2\\tau^2}\\right) \\cdot (\\tau^2)^{-(a_0+1)} \\exp\\left(-\\frac{b_0}{\\tau^2}\\right)$$\n$$p(\\mu, \\tau^2) \\propto (\\tau^2)^{-(a_0 + 3/2)} \\exp\\left(-\\frac{1}{2\\tau^2} [2b_0 + \\kappa_0(\\mu-m_0)^2]\\right)$$\n\nCombining the likelihood and prior, the posterior kernel is:\n$$p(\\mu, \\tau^2 \\mid \\mathbf{x}) \\propto (\\tau^2)^{-S/2} \\exp\\left(-\\frac{1}{2\\tau^2} \\left[\\sum(x_s-\\bar{x})^2 + S(\\bar{x}-\\mu)^2\\right]\\right) \\cdot (\\tau^2)^{-(a_0+3/2)} \\exp\\left(-\\frac{1}{2\\tau^2} [2b_0 + \\kappa_0(\\mu-m_0)^2]\\right)$$\n$$p(\\mu, \\tau^2 \\mid \\mathbf{x}) \\propto (\\tau^2)^{-(a_0+S/2+3/2)} \\exp\\left(-\\frac{1}{2\\tau^2} \\left[2b_0 + \\sum(x_s-\\bar{x})^2 + S(\\bar{x}-\\mu)^2 + \\kappa_0(\\mu-m_0)^2\\right]\\right)$$\nBy completing the square for the terms involving $\\mu$ in the exponent, we can identify the form of the posterior. The quadratic terms in $\\mu$ form a new Gaussian kernel for $\\mu \\mid \\tau^2, \\mathbf{x}$, and the remaining terms update the parameters of the Inverse-Gamma distribution for $\\tau^2$.\nDue to the conjugacy of the Normal-Inverse-Gamma prior for the normal likelihood, the posterior distribution $p(\\mu, \\tau^2 \\mid \\mathbf{x})$ is also a Normal-Inverse-Gamma distribution, $\\mathrm{NIG}(m_S, \\kappa_S, a_S, b_S)$, with updated hyperparameters:\n$$ \\kappa_S = \\kappa_0 + S $$\n$$ m_S = \\frac{\\kappa_0 m_0 + S\\bar{x}}{\\kappa_0 + S} $$\n$$ a_S = a_0 + \\frac{S}{2} $$\n$$ b_S = b_0 + \\frac{1}{2}\\sum_{s=1}^S (x_s - \\bar{x})^2 + \\frac{S\\kappa_0}{2(S+\\kappa_0)}(\\bar{x} - m_0)^2 $$\n\n**2. Derivation of the Posterior Predictive Distribution for $x_{\\mathrm{new}}$**\n\nThe posterior predictive distribution for the log-effect of a new species, $x_{\\mathrm{new}}$, is obtained by marginalizing over the posterior distribution of the parameters:\n$$p(x_{\\mathrm{new}} \\mid \\mathbf{x}) = \\int_0^\\infty \\int_{-\\infty}^\\infty p(x_{\\mathrm{new}} \\mid \\mu, \\tau^2) p(\\mu, \\tau^2 \\mid \\mathbf{x}) \\,d\\mu \\,d\\tau^2$$\nwhere $p(x_{\\mathrm{new}} \\mid \\mu, \\tau^2) = \\mathcal{N}(x_{\\mathrm{new}} \\mid \\mu, \\tau^2)$ and $p(\\mu, \\tau^2 \\mid \\mathbf{x})$ is the posterior derived above.\n\nThis is a standard integral in Bayesian statistics. We first integrate with respect to $\\mu$:\n$$p(x_{\\mathrm{new}} \\mid \\tau^2, \\mathbf{x}) = \\int_{-\\infty}^\\infty p(x_{\\mathrm{new}} \\mid \\mu, \\tau^2) p(\\mu \\mid \\tau^2, \\mathbf{x}) \\,d\\mu$$\nThis is a convolution of two normal distributions: $x_{\\mathrm{new}} \\sim \\mathcal{N}(\\mu, \\tau^2)$ and $\\mu \\sim \\mathcal{N}(m_S, \\tau^2/\\kappa_S)$. The result is another normal distribution:\n$$p(x_{\\mathrm{new}} \\mid \\tau^2, \\mathbf{x}) \\sim \\mathcal{N}\\left(m_S, \\tau^2 + \\frac{\\tau^2}{\\kappa_S}\\right) = \\mathcal{N}\\left(m_S, \\tau^2 \\frac{\\kappa_S+1}{\\kappa_S}\\right)$$\nNext, we integrate out $\\tau^2$:\n$$p(x_{\\mathrm{new}} \\mid \\mathbf{x}) = \\int_0^\\infty p(x_{\\mathrm{new}} \\mid \\tau^2, \\mathbf{x}) p(\\tau^2 \\mid \\mathbf{x}) \\,d\\tau^2$$\nwhere $\\tau^2 \\mid \\mathbf{x} \\sim \\mathrm{Inv\\mbox{-}Gamma}(a_S, b_S)$. This integral yields a non-standardized Student's t-distribution. The resulting distribution for $x_{\\mathrm{new}}$ is a location-scale t-distribution, $t_{\\nu}(\\mu_{\\text{pred}}, \\sigma^2_{\\text{pred}})$, with parameters:\n- Degrees of freedom: $\\nu = 2a_S$\n- Location (mean): $\\mu_{\\text{pred}} = m_S$\n- Scale squared: $\\sigma^2_{\\text{pred}} = \\frac{b_S}{a_S} \\left(1 + \\frac{1}{\\kappa_S}\\right) = \\frac{b_S(\\kappa_S+1)}{a_S\\kappa_S}$\nTherefore, $(x_{\\mathrm{new}} - m_S) / \\sqrt{\\sigma^2_{\\text{pred}}}$ follows a standard Student's t-distribution with $2a_S$ degrees of freedom.\n\n**3. Derivation of Estimands for $\\mathrm{EC50}_{\\mathrm{new}}$**\n\nWe are interested in the properties of $\\mathrm{EC50}_{\\mathrm{new}} = \\exp(x_{\\mathrm{new}})$. Since the exponential function is strictly monotone, we can transform the quantiles of the posterior predictive distribution of $x_{\\mathrm{new}}$ to find the corresponding quantiles for $\\mathrm{EC50}_{\\mathrm{new}}$.\n\n- **Posterior Median**: The Student's t-distribution is symmetric about its location parameter. Thus, the median of the posterior predictive distribution of $x_{\\mathrm{new}}$ is its mean, $m_S$. The posterior median of $\\mathrm{EC50}_{\\mathrm{new}}$ is:\n$$\\mathrm{Median}(\\mathrm{EC50}_{\\mathrm{new}}) = \\exp(\\mathrm{Median}(x_{\\mathrm{new}})) = \\exp(m_S)$$\n\n- **Central Credible Interval**: A central credible interval of level $q$ for $x_{\\mathrm{new}}$ is given by $[L, U]$, where $L$ and $U$ are the $(1-q)/2$ and $(1+q)/2$ quantiles of its posterior predictive distribution, respectively. Let $t^*_{\\nu}(\\alpha)$ denote the $\\alpha$-quantile of the standard Student's t-distribution with $\\nu$ degrees of freedom. The quantiles for $x_{\\mathrm{new}}$ are:\n$$L = m_S + \\sqrt{\\sigma^2_{\\text{pred}}} \\cdot t^*_{2a_S}\\left(\\frac{1-q}{2}\\right)$$\n$$U = m_S + \\sqrt{\\sigma^2_{\\text{pred}}} \\cdot t^*_{2a_S}\\left(\\frac{1+q}{2}\\right)$$\nThe corresponding central $q$-level credible interval for $\\mathrm{EC50}_{\\mathrm{new}}$ is $[\\exp(L), \\exp(U)]$. For the required level $q=0.90$, the quantiles are at probabilities $0.05$ and $0.95$. The final expressions for the interval bounds are:\n$$\\text{Lower bound} = \\exp\\left(m_S + \\sqrt{\\frac{b_S(\\kappa_S+1)}{a_S\\kappa_S}} \\cdot t^*_{2a_S}(0.05)\\right)$$\n$$\\text{Upper bound} = \\exp\\left(m_S + \\sqrt{\\frac{b_S(\\kappa_S+1)}{a_S\\kappa_S}} \\cdot t^*_{2a_S}(0.95)\\right)$$\nThese formulas provide a complete analytical solution to the problem, which we shall now implement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical Bayesian modeling problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"data\": [1.0, 1.5, 2.0, 0.8, 1.2],\n            \"prior\": {\"m0\": np.log(1.0), \"k0\": 1.0, \"a0\": 2.0, \"b0\": 0.1}\n        },\n        # Test case 2\n        {\n            \"data\": [0.5, 0.6],\n            \"prior\": {\"m0\": np.log(0.7), \"k0\": 5.0, \"a0\": 3.0, \"b0\": 0.05}\n        },\n        # Test case 3\n        {\n            \"data\": [0.05, 0.08, 0.07, 0.06, 0.09, 0.10],\n            \"prior\": {\"m0\": np.log(0.07), \"k0\": 0.1, \"a0\": 1.0, \"b0\": 0.01}\n        }\n    ]\n\n    q = 0.90\n    results = []\n\n    for case in test_cases:\n        y_data = np.array(case[\"data\"])\n        prior = case[\"prior\"]\n        m0, k0, a0, b0 = prior[\"m0\"], prior[\"k0\"], prior[\"a0\"], prior[\"b0\"]\n\n        # Step 1: Log-transform the data\n        x_data = np.log(y_data)\n        \n        # Step 2: Calculate summary statistics for the data\n        S = len(x_data)\n        x_bar = np.mean(x_data)\n        # Sum of squared differences from the mean\n        ss = np.sum((x_data - x_bar)**2)\n\n        # Step 3: Calculate the posterior hyperparameters for the Normal-Inverse-Gamma distribution\n        kS = k0 + S\n        mS = (k0 * m0 + S * x_bar) / kS\n        aS = a0 + S / 2.0\n        bS = b0 + 0.5 * ss + (S * k0 / (2.0 * kS)) * (x_bar - m0)**2\n\n        # Step 4: Calculate the parameters for the posterior predictive t-distribution of x_new\n        # Degrees of freedom\n        nu = 2 * aS\n        # Location (mean)\n        mu_pred = mS\n        # Scale\n        sigma_pred_sq = (bS / aS) * (kS + 1.0) / kS\n        sigma_pred = np.sqrt(sigma_pred_sq)\n        \n        # Step 5: Compute the posterior median of EC50_new\n        median_ec50 = np.exp(mu_pred)\n\n        # Step 6: Compute the central credible interval for EC50_new\n        # Get the quantiles from the standard t-distribution\n        alpha = (1.0 - q) / 2.0  # tail probability, e.g., 0.05 for q=0.90\n        t_quantile_lower = t.ppf(alpha, df=nu)\n        t_quantile_upper = t.ppf(1.0 - alpha, df=nu)\n\n        # Calculate interval bounds for x_new\n        x_new_lower = mu_pred + sigma_pred * t_quantile_lower\n        x_new_upper = mu_pred + sigma_pred * t_quantile_upper\n\n        # Transform bounds back to the original concentration scale\n        lower_bound_ec50 = np.exp(x_new_lower)\n        upper_bound_ec50 = np.exp(x_new_upper)\n\n        # Step 7: Format results to 6 decimal places\n        result_triple = [\n            round(median_ec50, 6),\n            round(lower_bound_ec50, 6),\n            round(upper_bound_ec50, 6)\n        ]\n        results.append(result_triple)\n        \n    # Final print statement in the exact required format.\n    # The format requires no spaces after commas.\n    case_results_str = [f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\" for r in results]\n    final_output_str = f\"[{','.join(case_results_str)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "2481192"}]}