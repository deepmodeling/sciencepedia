{"hands_on_practices": [{"introduction": "A cornerstone of ecological forecasting is understanding how populations respond to environmental variability. This exercise provides foundational practice by analyzing the stochastic Ricker model, a classic in population dynamics. By deriving the conditions for stability and approximating the population's fluctuations around its carrying capacity, you will apply core techniques of linearization that are essential for gaining analytical insight into complex, nonlinear systems under the influence of random environmental shocks [@problem_id:2482834]. This practice builds the mathematical intuition needed to predict how stable a population is and the magnitude of its expected fluctuations.", "problem": "Consider the stochastic Ricker model in discrete time for a single, closed population subject to environmental fluctuations,\n$$\nN_{t+1} \\;=\\; N_t \\,\\exp\\!\\big(r\\,(1 - N_t/K) + \\epsilon_t\\big),\n$$\nwhere $N_t$ denotes population abundance at time $t$, $r>0$ is the intrinsic growth rate, $K>0$ is the carrying capacity, and $\\{\\epsilon_t\\}_{t\\in\\mathbb{Z}}$ are independent and identically distributed environmental shocks with $\\mathbb{E}[\\epsilon_t]=0$ and $\\operatorname{Var}(\\epsilon_t)=\\sigma^2$, with $\\sigma^2$ small. Assume the noise is Gaussian, so that $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nYour tasks are:\n- Using one-dimensional discrete-time dynamical systems theory, analyze the noise-free ($\\sigma^2=0$) map to identify the positive equilibrium and derive the condition on $r$ for its local asymptotic stability.\n- Under small environmental noise, introduce the log-deviation variable $y_t = \\ln(N_t/K)$ and linearize the stochastic recurrence for $y_t$ around the equilibrium to leading order in $y_t$ and $\\epsilon_t$. Using properties of stationary solutions to linear stochastic difference equations driven by independent shocks, characterize the approximate stationary distribution of $y_t$ and, by transformation, the corresponding approximate stationary distribution of $N_t$.\n- Provide, as your final answer, the closed-form analytic expression for the stationary variance of $y_t$ in terms of $r$ and $\\sigma^2$ under the validity of the linearization and stability conditions. No rounding is required. Express your final answer without units.\n\nAll assumptions and steps must be justified from fundamental definitions of stability for one-dimensional maps and well-tested properties of linear stochastic recursions. Do not invoke any unproven shortcut formulas or cite results without derivation.", "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. It is a standard exercise in theoretical ecology concerning the analysis of a stochastic population model. We will proceed with a rigorous, step-by-step derivation.\n\nThe model under consideration is the stochastic Ricker map:\n$$\nN_{t+1} = N_t \\exp\\big(r(1 - N_t/K) + \\epsilon_t\\big)\n$$\nwhere $N_t$ is population abundance, $r > 0$ is the intrinsic growth rate, $K > 0$ is the carrying capacity, and $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ are independent and identically distributed random variables representing environmental noise, with $\\sigma^2$ assumed to be small.\n\nFirst, we analyze the deterministic system by setting the noise term to zero, $\\sigma^2 = 0$. The dynamics are governed by the one-dimensional map $N_{t+1} = f(N_t)$, where the function $f$ is given by:\n$$\nf(N) = N \\exp\\big(r(1 - N/K)\\big)\n$$\nThe equilibria, or fixed points $N^*$, of this map are solutions to the equation $N^* = f(N^*)$.\n$$\nN^* = N^* \\exp\\big(r(1 - N^*/K)\\big)\n$$\nThis equation immediately yields two possible solutions.\n$1$. The trivial equilibrium: $N_1^* = 0$.\n$2$. For a non-trivial equilibrium ($N^* > 0$), we may divide both sides by $N^*$:\n$$\n1 = \\exp\\big(r(1 - N^*/K)\\big)\n$$\nTaking the natural logarithm of both sides gives:\n$$\n\\ln(1) = 0 = r(1 - N^*/K)\n$$\nSince it is given that $r > 0$, this implies $1 - N^*/K = 0$, which leads to the positive equilibrium:\n$$\nN_2^* = K\n$$\nThis equilibrium represents the carrying capacity of the environment.\n\nNext, we investigate the local asymptotic stability of this positive equilibrium $N^* = K$. Stability is determined by the magnitude of the derivative of the map, $f'(N)$, evaluated at the equilibrium. If $|f'(N^*)|  1$, the equilibrium is locally asymptotically stable. We compute the derivative using the product rule:\n$$\n\\begin{aligned}\nf'(N) = \\frac{d}{dN} \\left[ N \\exp\\big(r(1 - N/K)\\big) \\right] \\\\\n= (1) \\cdot \\exp\\big(r(1 - N/K)\\big) + N \\cdot \\exp\\big(r(1 - N/K)\\big) \\cdot \\left(-\\frac{r}{K}\\right) \\\\\n= \\exp\\big(r(1 - N/K)\\big) \\left(1 - \\frac{rN}{K}\\right)\n\\end{aligned}\n$$\nEvaluating this derivative at the equilibrium $N^* = K$:\n$$\nf'(K) = \\exp\\big(r(1 - K/K)\\big) \\left(1 - \\frac{rK}{K}\\right) = \\exp(0) \\cdot (1 - r) = 1 - r\n$$\nThe condition for local asymptotic stability is $|f'(K)|  1$, which translates to:\n$$\n|1 - r|  1\n$$\nThis inequality is equivalent to the compound inequality $-1  1 - r  1$.\nThe right side, $1 - r  1$, implies $-r  0$, or $r  0$, which is already given.\nThe left side, $-1  1 - r$, implies $r  2$.\nThus, the condition for the local asymptotic stability of the positive equilibrium $N^* = K$ in the deterministic Ricker model is $0  r  2$.\n\nNow, we proceed to the stochastic analysis. We introduce the logarithmic deviation from carrying capacity, $y_t = \\ln(N_t/K)$. This implies $N_t = K \\exp(y_t)$. The equilibrium $N^*=K$ corresponds to $y^*=\\ln(K/K)=\\ln(1)=0$. Substituting $N_t = K\\exp(y_t)$ into the original stochastic equation:\n$$\nK\\exp(y_{t+1}) = K\\exp(y_t) \\exp\\big(r(1 - K\\exp(y_t)/K) + \\epsilon_t\\big)\n$$\nDividing by $K$ and then taking the natural logarithm of both sides yields an exact recurrence for $y_t$:\n$$\ny_{t+1} = y_t + r\\big(1 - \\exp(y_t)\\big) + \\epsilon_t\n$$\nThis is a nonlinear stochastic difference equation. Given the assumption that the noise variance $\\sigma^2$ is small and the deterministic system is stable ($0  r  2$), the population $N_t$ will exhibit small fluctuations around the stable equilibrium $K$. Consequently, $y_t$ will exhibit small fluctuations around its equilibrium value of $0$. This justifies a linearization of the recurrence for $y_t$ around $y_t=0$. We use the Taylor expansion for $\\exp(y_t)$ around $y_t=0$:\n$$\n\\exp(y_t) = 1 + y_t + O(y_t^2)\n$$\nSubstituting this into the equation for $y_{t+1}$ and retaining terms to leading order in $y_t$ and $\\epsilon_t$:\n$$\n\\begin{aligned}\ny_{t+1} \\approx y_t + r\\big(1 - (1+y_t)\\big) + \\epsilon_t \\\\\ny_{t+1} \\approx y_t - ry_t + \\epsilon_t \\\\\ny_{t+1} \\approx (1-r)y_t + \\epsilon_t\n\\end{aligned}\n$$\nThis is a linear stochastic difference equation, an autoregressive process of order $1$ (AR($1$)). A stationary solution to this process exists if the coefficient of the autoregressive term has a magnitude less than $1$, i.e., $|1-r|  1$, which is precisely the stability condition $0  r  2$ derived for the deterministic system.\n\nAssuming the process has reached its stationary state, the statistical moments of $y_t$ are constant in time. Let $\\mathbb{E}[y_t] = \\mu_y$ and $\\operatorname{Var}(y_t) = \\sigma_y^2$. In the stationary state, $\\mathbb{E}[y_{t+1}] = \\mathbb{E}[y_t] = \\mu_y$. Taking the expectation of the linearized equation:\n$$\n\\mathbb{E}[y_{t+1}] = \\mathbb{E}[(1-r)y_t + \\epsilon_t] = (1-r)\\mathbb{E}[y_t] + \\mathbb{E}[\\epsilon_t]\n$$\nGiven $\\mathbb{E}[\\epsilon_t]=0$, this becomes $\\mu_y = (1-r)\\mu_y$. This implies $r\\mu_y = 0$. Since $r0$, the stationary mean of $y_t$ is $\\mu_y=0$.\n\nNext, we find the stationary variance $\\sigma_y^2$. In the stationary state, $\\operatorname{Var}(y_{t+1}) = \\operatorname{Var}(y_t) = \\sigma_y^2$. We take the variance of the linearized equation:\n$$\n\\operatorname{Var}(y_{t+1}) = \\operatorname{Var}((1-r)y_t + \\epsilon_t)\n$$\nThe variable $y_t$ is a function of past shocks $\\{\\epsilon_{t-1}, \\epsilon_{t-2}, \\dots\\}$. Since the shocks $\\{\\epsilon_t\\}$ are independent, $y_t$ is uncorrelated with the current shock $\\epsilon_t$. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(y_{t+1}) = \\operatorname{Var}((1-r)y_t) + \\operatorname{Var}(\\epsilon_t) = (1-r)^2\\operatorname{Var}(y_t) + \\operatorname{Var}(\\epsilon_t)\n$$\nSubstituting the stationary variance $\\sigma_y^2$ and the given noise variance $\\sigma^2$:\n$$\n\\sigma_y^2 = (1-r)^2\\sigma_y^2 + \\sigma^2\n$$\nWe now solve for $\\sigma_y^2$:\n$$\n\\sigma_y^2 - (1-r)^2\\sigma_y^2 = \\sigma^2\n$$\n$$\n\\sigma_y^2 \\big(1 - (1-r)^2\\big) = \\sigma^2\n$$\n$$\n\\sigma_y^2 \\big(1 - (1 - 2r + r^2)\\big) = \\sigma^2\n$$\n$$\n\\sigma_y^2 (2r - r^2) = \\sigma^2\n$$\nThis gives the expression for the stationary variance of $y_t$:\n$$\n\\sigma_y^2 = \\frac{\\sigma^2}{2r - r^2}\n$$\nThis expression is valid under the stability condition $0  r  2$, which ensures the denominator is positive.\n\nAs a final characterization, since the process for $y_t$ is a linear transformation of Gaussian variables $\\epsilon_t$, its stationary distribution is also Gaussian. With the calculated mean and variance, the approximate stationary distribution for $y_t$ is $\\mathcal{N}(0, \\frac{\\sigma^2}{2r - r^2})$. Consequently, since $N_t = K\\exp(y_t)$, the population size $N_t$ follows an approximate log-normal distribution, specifically $N_t \\sim \\text{Log-Normal}(\\ln(K), \\frac{\\sigma^2}{2r-r^2})$.\n\nThe problem asks for the closed-form analytic expression for the stationary variance of $y_t$. This is the result derived above.", "answer": "$$\\boxed{\\frac{\\sigma^2}{2r - r^2}}$$", "id": "2482834"}, {"introduction": "While some systems are predictable, many ecological dynamics are inherently chaotic, placing a fundamental limit on our forecasting ability. This exercise explores this limit by examining the logistic map, a simple model capable of generating complex chaotic behavior. You will derive its Lyapunov exponent, a quantitative measure of chaos that describes the rate at which small initial uncertainties grow, and directly relate this abstract concept to the practical \"forecast horizon\"—the time frame over which a prediction remains useful [@problem_id:2482775]. This practice is crucial for developing a realistic understanding of when and for how long our ecological forecasts can be trusted.", "problem": "Consider the chaotic logistic map used as a simple, idealized population model,\n$$\nx_{t+1} = f(x_t) = r\\,x_t\\,(1-x_t),\n$$\nwith $r=4$ and state $x_t \\in (0,1)$. Assume the system is ergodic on $(0,1)$ with respect to its invariant probability density function (PDF), and that the invariant PDF exists and is absolutely continuous. \n\nPart A: Starting from the definition of the (maximal) Lyapunov exponent for a one-dimensional map,\n$$\n\\lambda = \\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{t=0}^{n-1} \\ln|f'(x_t)|,\n$$\nand invoking ergodicity to replace time averages by ensemble averages over the invariant PDF, derive a closed-form expression for the Lyapunov exponent $\\lambda$ at $r=4$. Your derivation must be explicit and self-contained.\n\nPart B: In a short-term ecological forecast, the initial condition $x_0$ is estimated from $m$ independent and identically distributed (i.i.d.) observations,\n$$\ny_i = x_0 + \\varepsilon_i,\\quad i=1,\\dots,m,\n$$\nwhere the observation errors $\\varepsilon_i$ are Gaussian, zero-mean, and independent with standard deviation $\\sigma_{\\text{obs}}$. The initial condition estimate is the sample mean $\\hat{x}_0 = \\frac{1}{m}\\sum_{i=1}^m y_i$, yielding an initial estimation error $e_0 = \\hat{x}_0 - x_0$ with standard deviation $\\sigma_0 = \\sigma_{\\text{obs}}/\\sqrt{m}$. Assume a free-running forecast thereafter (no further assimilation) and use linearized error dynamics together with the Lyapunov exponent from Part A and ergodicity to relate the root-mean-square error (RMSE) at lead time $n$,\n$$\n\\text{RMSE}(n) \\approx \\sigma_0 \\exp(\\lambda n),\n$$\nto a user-defined tolerance $\\delta0$. Define the forecast horizon $n^\\ast$ as the smallest nonnegative real $n$ for which $\\text{RMSE}(n)=\\delta$, and derive a closed-form expression for $n^\\ast$ in terms of $\\lambda$, $\\delta$, and $\\sigma_0$.\n\nPart C: Evaluate $n^\\ast$ numerically for $m=5$, $\\sigma_{\\text{obs}}=0.01$, $\\delta=0.1$, using the natural logarithm. Round your final numerical answer for the forecast horizon to three significant figures, and express it in time steps. The final answer to this problem must be a single real number corresponding to $n^\\ast$ in time steps.", "solution": "The problem is well-posed and scientifically sound, permitting a rigorous solution. The solution is presented in three parts as requested.\n\nPart A: Derivation of the Lyapunov exponent $\\lambda$.\nThe Lyapunov exponent $\\lambda$ for a one-dimensional map $f(x)$ is defined by the time average:\n$$\n\\lambda = \\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{t=0}^{n-1} \\ln|f'(x_t)|\n$$\nThe problem states that the system is ergodic. By the ergodic hypothesis, the time average can be replaced by the ensemble (space) average, weighted by the invariant probability density function (PDF), $\\rho(x)$:\n$$\n\\lambda = \\int_0^1 \\ln|f'(x)| \\rho(x) \\,dx\n$$\nFor the logistic map $x_{t+1} = f(x_t) = r x_t (1-x_t)$ with parameter $r=4$, the function is $f(x) = 4x(1-x)$. The derivative is $f'(x) = 4 - 8x$.\nFor $r=4$, the logistic map is known to possess an absolutely continuous invariant measure with the arcsine probability density function:\n$$\n\\rho(x) = \\frac{1}{\\pi\\sqrt{x(1-x)}}\n$$\nThe problem statement affirms the existence of such a PDF, so we proceed by substituting $f'(x)$ and $\\rho(x)$ into the integral for $\\lambda$:\n$$\n\\lambda = \\int_0^1 \\ln|4 - 8x| \\frac{1}{\\pi\\sqrt{x(1-x)}} \\,dx\n$$\nTo evaluate this integral, we perform a change of variables. The form of the denominator suggests the substitution $x = \\sin^2(\\theta)$. This implies $dx = 2\\sin(\\theta)\\cos(\\theta)\\,d\\theta$. The integration limits for $x$ from $0$ to $1$ correspond to limits for $\\theta$ from $0$ to $\\frac{\\pi}{2}$.\nThe terms in the integrand transform as follows:\nThe denominator term $\\sqrt{x(1-x)}$ becomes $\\sqrt{\\sin^2(\\theta)(1-\\sin^2(\\theta))} = \\sqrt{\\sin^2(\\theta)\\cos^2(\\theta)} = \\sin(\\theta)\\cos(\\theta)$ for $\\theta \\in [0, \\frac{\\pi}{2}]$.\nThe argument of the logarithm becomes $|4 - 8\\sin^2(\\theta)| = |4(1-2\\sin^2(\\theta))| = |4\\cos(2\\theta)|$.\nSubstituting these into the integral for $\\lambda$:\n$$\n\\lambda = \\frac{1}{\\pi} \\int_0^{\\pi/2} \\ln|4\\cos(2\\theta)| \\frac{1}{\\sin(\\theta)\\cos(\\theta)} (2\\sin(\\theta)\\cos(\\theta))\\,d\\theta\n$$\n$$\n\\lambda = \\frac{2}{\\pi} \\int_0^{\\pi/2} \\ln|4\\cos(2\\theta)|\\,d\\theta\n$$\nUsing the properties of the logarithm, we can split the integrand:\n$$\n\\lambda = \\frac{2}{\\pi} \\int_0^{\\pi/2} (\\ln(4) + \\ln|\\cos(2\\theta)|)\\,d\\theta = \\frac{2}{\\pi} \\left[ \\int_0^{\\pi/2} \\ln(4)\\,d\\theta + \\int_0^{\\pi/2} \\ln|\\cos(2\\theta)|\\,d\\theta \\right]\n$$\nThe first integral is trivial: $\\int_0^{\\pi/2} \\ln(4)\\,d\\theta = \\ln(4) \\left[\\theta\\right]_0^{\\pi/2} = \\frac{\\pi}{2}\\ln(4)$.\nThe second integral, $\\int_0^{\\pi/2} \\ln|\\cos(2\\theta)|\\,d\\theta$, is a known result. Let $u = 2\\theta$, so $du = 2 d\\theta$.\n$$\n\\int_0^{\\pi/2} \\ln|\\cos(2\\theta)|\\,d\\theta = \\frac{1}{2} \\int_0^\\pi \\ln|\\cos(u)|\\,du\n$$\nIt is a standard result of integral calculus that $\\int_0^{\\pi/2} \\ln(\\sin(x))\\,dx = \\int_0^{\\pi/2} \\ln(\\cos(x))\\,dx = -\\frac{\\pi}{2}\\ln(2)$.\nThe integral $\\int_0^\\pi \\ln|\\cos(u)|\\,du$ can be shown to be $\\int_0^{\\pi/2} \\ln(\\cos(u))\\,du + \\int_{\\pi/2}^\\pi \\ln(-\\cos(u))\\,du$. The second part evaluates to $\\int_0^{\\pi/2} \\ln(\\sin(v))\\,dv$. Thus, $\\frac{1}{2} \\int_0^\\pi \\ln|\\cos(u)|\\,du = \\frac{1}{2} (2 \\times (-\\frac{\\pi}{2}\\ln(2))) = -\\frac{\\pi}{2}\\ln(2)$.\nSubstituting these results back into the expression for $\\lambda$:\n$$\n\\lambda = \\frac{2}{\\pi} \\left[ \\frac{\\pi}{2}\\ln(4) - \\frac{\\pi}{2}\\ln(2) \\right] = \\ln(4) - \\ln(2) = \\ln(2^2) - \\ln(2) = 2\\ln(2) - \\ln(2) = \\ln(2)\n$$\nThe closed-form expression for the Lyapunov exponent is $\\lambda = \\ln(2)$.\n\nPart B: Derivation of the forecast horizon $n^\\ast$.\nThe problem provides the approximate formula for the root-mean-square error (RMSE) at lead time $n$:\n$$\n\\text{RMSE}(n) \\approx \\sigma_0 \\exp(\\lambda n)\n$$\nThe forecast horizon $n^\\ast$ is defined as the smallest non-negative real number $n$ for which the RMSE reaches a specified tolerance $\\delta  0$. We set $\\text{RMSE}(n^\\ast) = \\delta$:\n$$\n\\delta = \\sigma_0 \\exp(\\lambda n^\\ast)\n$$\nTo find the expression for $n^\\ast$, we solve this equation algebraically. First, isolate the exponential term:\n$$\n\\frac{\\delta}{\\sigma_0} = \\exp(\\lambda n^\\ast)\n$$\nTaking the natural logarithm of both sides gives:\n$$\n\\ln\\left(\\frac{\\delta}{\\sigma_0}\\right) = \\ln(\\exp(\\lambda n^\\ast)) = \\lambda n^\\ast\n$$\nFinally, solving for $n^\\ast$:\n$$\nn^\\ast = \\frac{1}{\\lambda} \\ln\\left(\\frac{\\delta}{\\sigma_0}\\right)\n$$\nThis is the closed-form expression for the forecast horizon $n^\\ast$ in terms of $\\lambda$, $\\delta$, and the initial error standard deviation $\\sigma_0$.\n\nPart C: Numerical evaluation of $n^\\ast$.\nWe are given the following values:\nNumber of observations $m=5$.\nObservation error standard deviation $\\sigma_{\\text{obs}}=0.01$.\nError tolerance $\\delta=0.1$.\nFrom Part A, we have the Lyapunov exponent $\\lambda = \\ln(2)$.\nThe initial error standard deviation $\\sigma_0$ is given by $\\sigma_0 = \\sigma_{\\text{obs}}/\\sqrt{m}$. Substituting the numerical values:\n$$\n\\sigma_0 = \\frac{0.01}{\\sqrt{5}}\n$$\nNow, substitute all values into the expression for $n^\\ast$:\n$$\nn^\\ast = \\frac{1}{\\ln(2)} \\ln\\left(\\frac{0.1}{0.01/\\sqrt{5}}\\right)\n$$\nSimplify the term inside the logarithm:\n$$\n\\frac{0.1}{0.01/\\sqrt{5}} = \\frac{10^{-1}}{10^{-2}/\\sqrt{5}} = 10\\sqrt{5}\n$$\nThe expression for $n^\\ast$ becomes:\n$$\nn^\\ast = \\frac{\\ln(10\\sqrt{5})}{\\ln(2)}\n$$\nWe now compute the numerical value:\n$$\nn^\\ast = \\frac{\\ln(10 \\times 2.2360679...)}{\\ln(2)} = \\frac{\\ln(22.360679...)}{0.693147...} = \\frac{3.107232...}{0.693147...} \\approx 4.4828\n$$\nThe problem requires the answer to be rounded to three significant figures.\n$$\nn^\\ast \\approx 4.48 \\text{ time steps}\n$$", "answer": "$$\n\\boxed{4.48}\n$$", "id": "2482775"}, {"introduction": "Modern ecological forecasting relies heavily on fitting models to time-series data, a process that often reveals a challenging issue known as equifinality—where multiple different parameter sets yield nearly identical forecast skill. This hands-on coding problem tackles this issue head-on by using profile likelihoods, a powerful statistical method to diagnose parameter non-identifiability within a state-space model framework [@problem_id:2482790]. By implementing a Kalman filter to calculate the exact likelihood and then profiling it, you will gain practical experience in assessing model uncertainty and making informed proposals for future data collection to resolve it.", "problem": "You are tasked with writing a complete, runnable program that detects equifinality among parameter sets in an ecological forecasting model using profile likelihoods and then proposes additional data to resolve it. The ecological context is a linear Gaussian state-space formulation of the Gompertz population model for log-abundance time series. Your program must follow a principle-based design starting from standard probability rules and well-tested facts about Gaussian state-space models and must produce a single line of output with specified content and format.\n\nModel and foundational base:\n- Consider the state process defined by\n$$\nz_{t+1} = \\phi z_t + c + \\eta_t,\n$$\nwhere $\\eta_t \\sim \\mathcal{N}(0,\\sigma_p^2)$ are independent and identically distributed process innovations, and $\\mathcal{N}$ denotes a Gaussian distribution with the given variance. The observation process is\n$$\ny_t = z_t + \\epsilon_t,\n$$\nwhere $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_o^2)$ are independent and identically distributed measurement errors, independent of $\\eta_t$. The parameter vector is $\\theta = (\\phi,c,\\sigma_p,\\sigma_o)$.\n\n- For a given time series $\\{y_t\\}_{t=1}^T$, the one-step-ahead predictive distribution under this linear Gaussian state-space model is Gaussian. The exact log-likelihood can be computed by iteratively applying the Kalman filter prediction-update recursions implied by Gaussian conjugacy:\n  1. Prediction step (state): from posterior $(m_t, P_t)$, get prior $(m_{t+1|t}, P_{t+1|t})$ via\n  $$\n  m_{t+1|t} = \\phi m_t + c,\\quad P_{t+1|t} = \\phi^2 P_t + \\sigma_p^2.\n  $$\n  2. Observation prediction for $y_{t+1}$:\n  $$\n  \\hat{y}_{t+1} = m_{t+1|t},\\quad S_{t+1} = P_{t+1|t} + \\sigma_o^2,\\quad v_{t+1} = y_{t+1} - \\hat{y}_{t+1}.\n  $$\n  3. Update step (state): with Kalman gain $K_{t+1} = P_{t+1|t} / S_{t+1}$,\n  $$\n  m_{t+1} = m_{t+1|t} + K_{t+1} v_{t+1},\\quad P_{t+1} = P_{t+1|t} - K_{t+1}^2 S_{t+1}.\n  $$\n  The one-step-ahead log-likelihood contribution for $y_{t+1}$ is\n  $$\n  \\ell_{t+1} = -\\tfrac{1}{2}\\left(\\log(2\\pi) + \\log S_{t+1} + \\frac{v_{t+1}^2}{S_{t+1}}\\right).\n  $$\n  Use a diffuse prior for the state by initializing $m_1 = y_1$ and $P_1$ as a large variance (e.g., $P_1 = 10^6$), then apply the above for $t = 1,\\dots,T-1$ to sum $\\ell = \\sum_{t=1}^{T-1} \\ell_{t+1}$.\n\n- Forecast skill metric: Define the point-forecast for $y_{t+1}$ as $\\hat{y}_{t+1}=m_{t+1|t}$. Define one-step-ahead root mean square error (RMSE) as\n$$\n\\mathrm{RMSE}(\\theta) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} \\left(y_{t+1} - \\hat{y}_{t+1}\\right)^2}.\n$$\n\nProfile likelihood and equifinality:\n- For a focal scalar parameter $\\psi \\in \\{\\phi, c, \\sigma_p, \\sigma_o\\}$, the profile log-likelihood over a grid $\\mathcal{G}_\\psi$ is\n$$\n\\ell_p(\\psi) = \\max_{\\theta \\setminus \\{\\psi\\}} \\ \\ell(\\theta),\n$$\nwhere $\\ell(\\theta)$ is the Kalman-filter log-likelihood. Similarly, define the minimal achievable forecast error at each fixed $\\psi$ as\n$$\n\\mathrm{RMSE}_p(\\psi) = \\min_{\\theta \\setminus \\{\\psi\\}} \\ \\mathrm{RMSE}(\\theta).\n$$\n- Detect equifinality as follows. For each $\\psi$, compute:\n  - The set $\\mathcal{A}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\ell_p(\\psi) \\ge \\ell_{\\max} - \\Delta\\}$ where $\\ell_{\\max} = \\max_{\\psi \\in \\mathcal{G}_\\psi} \\ell_p(\\psi)$ and $\\Delta = \\tfrac{1}{2}\\chi^2_{1,0.95}$, with $\\chi^2_{1,0.95}$ the $0.95$ quantile of a chi-square distribution with $1$ degree of freedom.\n  - The set $\\mathcal{B}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\mathrm{RMSE}_p(\\psi) \\le (1+\\varepsilon)\\min_{\\psi \\in \\mathcal{G}_\\psi} \\mathrm{RMSE}_p(\\psi)\\}$, with $\\varepsilon = 0.05$.\n  - The intersection $\\mathcal{I}_\\psi = \\mathcal{A}_\\psi \\cap \\mathcal{B}_\\psi$. Define the discrete width ratio\n  $$\n  w_\\psi = \\frac{|\\mathcal{I}_\\psi|}{|\\mathcal{G}_\\psi|}.\n  $$\n  Declare equifinality present if $\\max_{\\psi} w_\\psi \\ge \\tau$ with $\\tau = 0.3$. The parameter indicating the strongest equifinality is the $\\psi$ achieving the maximum $w_\\psi$ (break ties by choosing the smallest index as defined below).\n- To quantify the similarity in forecast skill among equifinal parameter values for the detected $\\psi^\\star$, compute the skill gap $g = \\frac{\\max_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi) - \\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}{\\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}$.\nIf no equifinality is detected, set $g = 0$.\n\nProgram requirements:\n- Implement the exact Kalman-filter-based log-likelihood and one-step-ahead RMSE as specified above.\n- Use the following fixed grids for reproducibility:\n  - Profile grids:\n    - $\\phi$: $\\mathcal{G}_\\phi = \\{\\text{linspace from } 0.2 \\text{ to } 1.2 \\text{ with } 21 \\text{ points}\\}$.\n    - $c$: $\\mathcal{G}_c = \\{\\text{linspace from } -0.1 \\text{ to } 0.5 \\text{ with } 21 \\text{ points}\\}$.\n    - $\\sigma_p$: $\\mathcal{G}_{\\sigma_p} = \\{\\text{logspace from } 0.03 \\text{ to } 0.6 \\text{ with } 21 \\text{ points}\\}$.\n    - $\\sigma_o$: $\\mathcal{G}_{\\sigma_o} = \\{\\text{logspace from } 0.03 \\text{ to } 0.6 \\text{ with } 21 \\text{ points}\\}$.\n  - Nuisance parameter grids for profiling:\n    - $\\phi$: $\\{\\text{linspace from } 0.0 \\text{ to } 1.2 \\text{ with } 11 \\text{ points}\\}$.\n    - $c$: $\\{\\text{linspace from } -0.3 \\text{ to } 0.7 \\text{ with } 11 \\text{ points}\\}$.\n    - $\\sigma_p$: $\\{\\text{logspace from } 0.01 \\text{ to } 1.0 \\text{ with } 11 \\text{ points}\\}$.\n    - $\\sigma_o$: $\\{\\text{logspace from } 0.01 \\text{ to } 1.0 \\text{ with } 11 \\text{ points}\\}$.\n  Here, “linspace” means evenly spaced in the linear scale and “logspace” denotes evenly spaced in the logarithmic scale for strictly positive bounds.\n\nTest suite:\n- Use exactly the following three time series (dimensionless log-abundances), each as a list of real numbers in the given order.\n  - Case $1$ (length $16$):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.1,\\,\n    0.285,\\,\n    0.44225,\\,\n    0.5769125,\\,\n    0.690375625,\\,\n    0.78681928125,\\,\n    0.8687963890625,\\,\n    0.938477930703125,\\,\n    0.9977062410976562,\\,\n    1.0480503049320078,\\,\n    1.0908427591922066,\\,\n    1.1272163453133756,\\,\n    1.1581338935163692,\\,\n    1.1844138094889148,\\,\n    1.2067517380655776,\\,\n    1.22573997735574\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n  - Case $2$ (length $8$):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.22,\\,\n    0.28,\\,\n    0.29,\\,\n    0.35,\\,\n    0.34,\\,\n    0.42,\\,\n    0.40,\\,\n    0.47\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n  - Case $3$ (length $16$):\n    $$\n    [\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n    0.52,\\,\n    0.49,\\,\n    0.51,\\,\n    0.5,\\,\n    0.53,\\,\n    0.48,\\,\n    0.5,\\,\n    0.51,\\,\n    0.49,\\,\n    0.5,\\,\n    0.52,\\,\n    0.47,\\,\n    0.5,\\,\n    0.51,\\,\n    0.5,\\,\n    0.49\n    \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,]\n    $$\n\nIndices, decision thresholds, and proposal codes:\n- Parameter indices must be encoded as integers as follows: $\\phi \\to 0$, $c \\to 1$, $\\sigma_p \\to 2$, $\\sigma_o \\to 3$.\n- Equifinality decision thresholds must be $\\Delta = \\tfrac{1}{2}\\chi^2_{1,0.95}$, $\\varepsilon = 0.05$, and $\\tau = 0.3$ exactly as defined above.\n- If equifinality is detected for the parameter with the largest width ratio $w_\\psi$, propose a single additional data type to most directly resolve it using the following integer code:\n  - $0$: no additional data needed (used only if no equifinality is detected).\n  - $1$: increase temporal resolution or time series length (more time points).\n  - $2$: replicate observations at each time to better estimate $\\sigma_o$.\n  - $3$: replicate process units (parallel populations) to better estimate $\\sigma_p$.\n  - $4$: measure an environmental covariate affecting the intercept $c$.\n  Mapping rule: if the detected parameter is $\\phi$, output $1$; if it is $c$, output $4$; if it is $\\sigma_o$, output $2$; if it is $\\sigma_p$, output $3$.\n\nOutput specification:\n- For each of the three cases, compute:\n  - an integer equifinality flag $f \\in \\{0,1\\}$ (with $1$ indicating equifinality detected),\n  - the integer index $p$ of the parameter with the largest $w_\\psi$ (use $-1$ if $f=0$),\n  - the integer proposal code $d$ (use $0$ if $f=0$),\n  - the skill gap float $g$ as defined above (use $0.0$ if $f=0$).\n- Your program should produce a single line of output containing the results as a comma-separated list of the three case results, each result itself being a list in the form `[f,p,d,g]`. For example, a syntactically correct output is\n$$\n[[1,0,1,0.0375],[0,-1,0,0.0],[1,3,2,0.0123]]\n$$\nwith the actual numeric values determined by your implementation on the provided test suite. No additional text must be printed.", "solution": "The problem posed is to develop and implement a computational procedure for detecting and characterizing equifinality in an ecological forecasting model. The model is a linear Gaussian state-space representation of Gompertz population dynamics. The procedure must be based on the principle of profile likelihood. If equifinality is detected, a recommendation for collecting additional data must be provided based on a predefined heuristic. The validity of the problem statement is confirmed; it is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution. We now proceed with the derivation of the required algorithm from first principles.\n\nThe system is described by a state process for the log-abundance $z_t$ and an observation process for the measurement $y_t$.\nState Process:\n$$\nz_{t+1} = \\phi z_t + c + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_p^2)\n$$\nObservation Process:\n$$\ny_t = z_t + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_o^2)\n$$\nThe complete parameter vector is $\\theta = (\\phi, c, \\sigma_p, \\sigma_o)$. For a given time series of observations $\\{y_t\\}_{t=1}^T$, our first task is to compute the log-likelihood function $\\ell(\\theta) = \\log P(\\{y_t\\}_{t=1}^T | \\theta)$ and the one-step-ahead forecast error.\n\n**1. Log-Likelihood and Forecast Error via Kalman Filter**\n\nFor a linear Gaussian state-space model, the exact log-likelihood is computable via the prediction-error decomposition, where the algorithm for recursively finding the conditional distributions is the Kalman filter. The distribution of the state $z_t$ conditional on observations up to time $t$, $Y_t = \\{y_1, \\dots, y_t\\}$, is Gaussian: $P(z_t | Y_t, \\theta) = \\mathcal{N}(z_t | m_t, P_t)$. The filter proceeds in two steps for each time point $t = 1, \\dots, T-1$.\n\nFirst, the **prediction step**: one projects the state distribution forward in time. The posterior at time $t$, $\\mathcal{N}(m_t, P_t)$, is propagated through the state equation to yield a prior for time $t+1$:\n$$\nP(z_{t+1} | Y_t, \\theta) = \\mathcal{N}(z_{t+1} | m_{t+1|t}, P_{t+1|t})\n$$\nwhere the moments are:\n$$\nm_{t+1|t} = \\phi m_t + c\n$$\n$$\nP_{t+1|t} = \\phi^2 P_t + \\sigma_p^2\n$$\nThis prior on the state $z_{t+1}$ induces a predictive distribution for the next observation $y_{t+1}$:\n$$\nP(y_{t+1} | Y_t, \\theta) = \\mathcal{N}(y_{t+1} | \\hat{y}_{t+1}, S_{t+1})\n$$\nwith mean $\\hat{y}_{t+1} = m_{t+1|t}$ and variance $S_{t+1} = P_{t+1|t} + \\sigma_o^2$. The term $v_{t+1} = y_{t+1} - \\hat{y}_{t+1}$ is the one-step-ahead prediction error or innovation.\n\nSecond, the **update step**: the new observation $y_{t+1}$ is used to update the state distribution via Bayes' rule, yielding the posterior at time $t+1$:\n$$\nP(z_{t+1} | Y_{t+1}, \\theta) = \\mathcal{N}(z_{t+1} | m_{t+1}, P_{t+1})\n$$\nThe updated moments are given by:\n$$\nK_{t+1} = P_{t+1|t} / S_{t+1} \\quad (\\text{Kalman Gain})\n$$\n$$\nm_{t+1} = m_{t+1|t} + K_{t+1} v_{t+1}\n$$\n$$\nP_{t+1} = P_{t+1|t} - K_{t+1}^2 S_{t+1}\n$$\nThe total log-likelihood is the sum of the log-likelihoods of the one-step-ahead predictions:\n$$\n\\ell(\\theta) = \\sum_{t=1}^{T-1} \\log P(y_{t+1} | Y_t, \\theta) = \\sum_{t=1}^{T-1} \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log S_{t+1} - \\frac{1}{2}\\frac{v_{t+1}^2}{S_{t+1}} \\right)\n$$\nThe recursion is initialized with a diffuse prior on the state $z_1$. Following the specification, we set $m_1 = y_1$ and $P_1 = 10^6$. The one-step-ahead root mean square error (RMSE) is calculated from the innovations:\n$$\n\\mathrm{RMSE}(\\theta) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} v_{t+1}^2}\n$$\n\n**2. Profile Likelihood and Profile RMSE**\n\nEquifinality, a condition where multiple distinct parameter sets yield similarly good fits to data, is analyzed using profile likelihoods. For a focal parameter $\\psi$ (which can be any of $\\phi, c, \\sigma_p, \\sigma_o$), the profile log-likelihood $\\ell_p(\\psi)$ is the maximal log-likelihood achievable by varying all other (nuisance) parameters.\n$$\n\\ell_p(\\psi) = \\max_{\\theta \\setminus \\{\\psi\\}} \\ell(\\theta)\n$$\nAnalogously, the profile RMSE measures the minimal forecast error for a fixed $\\psi$:\n$$\n\\mathrm{RMSE}_p(\\psi) = \\min_{\\theta \\setminus \\{\\psi\\}} \\mathrm{RMSE}(\\theta)\n$$\nThe optimization is performed via a grid search over the specified nuisance parameter grids. For each point $\\psi_j$ in the focal parameter's grid $\\mathcal{G}_\\psi$, we evaluate $\\ell(\\theta)$ and $\\mathrm{RMSE}(\\theta)$ for all combinations of nuisance parameter values from their respective grids and retain the maximum $\\ell$ and minimum RMSE.\n\n**3. Equifinality Detection**\n\nEquifinality is detected by examining the flatness of the profile likelihood and profile RMSE surfaces.\nA likelihood-based confidence set for $\\psi$ is defined as:\n$$\n\\mathcal{A}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\ell_p(\\psi) \\ge \\ell_{\\max} - \\Delta\\}\n$$\nwhere $\\ell_{\\max} = \\max_{\\psi \\in \\mathcal{G}_\\psi} \\ell_p(\\psi)$ and $\\Delta = \\frac{1}{2}\\chi^2_{1,0.95}$ is the threshold based on Wilks' theorem for a $95\\%$ confidence interval.\nA forecast-skill-based set of plausible parameters is defined as:\n$$\n\\mathcal{B}_\\psi = \\{\\psi \\in \\mathcal{G}_\\psi : \\mathrm{RMSE}_p(\\psi) \\le (1+\\varepsilon)\\min_{\\psi \\in \\mathcal{G}_\\psi} \\mathrm{RMSE}_p(\\psi)\\}\n$$\nwith a relative error tolerance of $\\varepsilon = 0.05$.\nThe intersection of these sets, $\\mathcal{I}_\\psi = \\mathcal{A}_\\psi \\cap \\mathcal{B}_\\psi$, contains parameter values that are both statistically plausible and produce near-optimal forecasts. The degree of equifinality for parameter $\\psi$ is quantified by the relative size of this set:\n$$\nw_\\psi = \\frac{|\\mathcal{I}_\\psi|}{|\\mathcal{G}_\\psi|}\n$$\nEquifinality is declared present if the maximum width ratio over all parameters exceeds a threshold $\\tau=0.3$: that is, if $\\max_\\psi w_\\psi \\ge 0.3$. The parameter $\\psi^\\star$ that maximizes $w_\\psi$ is deemed the one most affected by equifinality.\n\n**4. Skill Gap and Data Proposal**\n\nIf equifinality is detected for parameter $\\psi^\\star$, we quantify the variation in forecast skill among the equifinal parameter values by the skill gap $g$:\n$$\ng = \\frac{\\max_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi) - \\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE}_p(\\psi)}{\\min_{\\psi \\in \\mathcal{I}_{\\psi^\\star}} \\mathrm{RMSE_p}(\\psi)}\n$$\nThis quantifies the relative difference in forecast error between the best and worst-performing parameters within the equifinal set. If no equifinality is detected, $g$ is set to $0$.\n\nFinally, a recommendation for new data collection is made based on which parameter $\\psi^\\star$ exhibits the strongest equifinality. The mapping is based on the role of each parameter in the model:\n- $\\phi$ governs temporal dynamics: resolved by more time points (proposal code $1$).\n- $c$ is the intercept/mean-reversion level: resolved by measuring environmental drivers (proposal code $4$).\n- $\\sigma_p$ is process variance: resolved by replicating process units (e.g., parallel populations) (proposal code $3$).\n- $\\sigma_o$ is observation variance: resolved by replicating observations at each time point (proposal code $2$).\n\nIf no equifinality is detected, no new data is proposed (code $0$). The index of the maximally equifinal parameter is recorded, or set to $-1$ if none is detected. This completes the formal specification of the algorithm.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main solver function to perform equifinality analysis for all test cases.\n    \"\"\"\n    \n    # --- Define problem constants, thresholds, and grids ---\n    \n    # Test cases\n    test_cases = [\n        [0.1, 0.285, 0.44225, 0.5769125, 0.690375625, 0.78681928125,\n         0.8687963890625, 0.938477930703125, 0.9977062410976562, 1.0480503049320078,\n         1.0908427591922066, 1.1272163453133756, 1.1581338935163692, 1.1844138094889148,\n         1.2067517380655776, 1.22573997735574],\n        [0.22, 0.28, 0.29, 0.35, 0.34, 0.42, 0.40, 0.47],\n        [0.52, 0.49, 0.51, 0.5, 0.53, 0.48, 0.5, 0.51, 0.49, 0.5,\n         0.52, 0.47, 0.5, 0.51, 0.5, 0.49]\n    ]\n\n    # Parameter names and indices\n    PARAM_NAMES = ['phi', 'c', 'sigma_p', 'sigma_o']\n    PARAM_INDICES = {name: i for i, name in enumerate(PARAM_NAMES)}\n\n    # Parameter grids for profiling\n    PROFILE_GRIDS = {\n        'phi': np.linspace(0.2, 1.2, 21),\n        'c': np.linspace(-0.1, 0.5, 21),\n        'sigma_p': np.logspace(np.log10(0.03), np.log10(0.6), 21),\n        'sigma_o': np.logspace(np.log10(0.03), np.log10(0.6), 21)\n    }\n\n    # Nuisance parameter grids for optimization\n    NUISANCE_GRIDS = {\n        'phi': np.linspace(0.0, 1.2, 11),\n        'c': np.linspace(-0.3, 0.7, 11),\n        'sigma_p': np.logspace(np.log10(0.01), np.log10(1.0), 11),\n        'sigma_o': np.logspace(np.log10(0.01), np.log10(1.0), 11)\n    }\n    \n    # Decision thresholds and constants\n    DELTA = 0.5 * chi2.ppf(0.95, df=1)\n    EPSILON = 0.05\n    TAU = 0.3\n    \n    # Proposal code mapping: param_index -> data_code\n    PROPOSAL_MAP = {0: 1, 1: 4, 2: 3, 3: 2}\n\n    # --- Core functions ---\n\n    def kalman_filter_loglik_rmse(theta, y):\n        \"\"\"\n        Computes log-likelihood and RMSE using the Kalman filter.\n        theta: tuple (phi, c, sigma_p, sigma_o)\n        y: numpy array of time series data\n        \"\"\"\n        phi, c, sigma_p, sigma_o = theta\n        T = len(y)\n        \n        m_t = y[0]\n        P_t = 1e6\n        \n        log_likelihood_sum = 0.0\n        squared_error_sum = 0.0\n        \n        for t in range(1, T):\n            # Prediction\n            m_pred = phi * m_t + c\n            P_pred = phi**2 * P_t + sigma_p**2\n            \n            # Observation prediction\n            y_hat = m_pred\n            v = y[t] - y_hat\n            S = P_pred + sigma_o**2\n            \n            if S = 0: # Numerical stability\n                return -np.inf, np.inf\n\n            # Log-likelihood contribution\n            log_likelihood_sum += -0.5 * (np.log(2 * np.pi) + np.log(S) + v**2 / S)\n            squared_error_sum += v**2\n\n            # Update\n            K = P_pred / S\n            m_t = m_pred + K * v\n            P_t = P_pred - K**2 * S\n\n        rmse = np.sqrt(squared_error_sum / (T - 1))\n        return log_likelihood_sum, rmse\n\n    # --- Main processing loop ---\n    \n    final_results = []\n    \n    for y_case in test_cases:\n        y_data = np.array(y_case)\n        w_values = []\n        all_profiles = {}\n\n        # Profile each of the 4 parameters\n        for focal_param_idx, focal_param_name in enumerate(PARAM_NAMES):\n            profile_loglik = []\n            profile_rmse = []\n            \n            nuisance_param_names = [p for p in PARAM_NAMES if p != focal_param_name]\n            nuisance_grids = [NUISANCE_GRIDS[name] for name in nuisance_param_names]\n\n            for focal_val in PROFILE_GRIDS[focal_param_name]:\n                max_loglik = -np.inf\n                min_rmse = np.inf\n                \n                # Grid search over nuisance parameters\n                for nuisance_vals in itertools.product(*nuisance_grids):\n                    params = {focal_param_name: focal_val}\n                    for i, name in enumerate(nuisance_param_names):\n                        params[name] = nuisance_vals[i]\n                    \n                    theta = (params['phi'], params['c'], params['sigma_p'], params['sigma_o'])\n                    \n                    loglik, rmse = kalman_filter_loglik_rmse(theta, y_data)\n                    \n                    max_loglik = max(max_loglik, loglik)\n                    min_rmse = min(min_rmse, rmse)\n                \n                profile_loglik.append(max_loglik)\n                profile_rmse.append(min_rmse)\n\n            profile_loglik = np.array(profile_loglik)\n            profile_rmse = np.array(profile_rmse)\n            all_profiles[focal_param_name] = {'loglik': profile_loglik, 'rmse': profile_rmse}\n\n            # Calculate width ratio w_psi\n            l_max = np.max(profile_loglik)\n            rmse_min = np.min(profile_rmse)\n\n            # Check for invalid profiles (e.g., all -inf)\n            if np.isinf(l_max) or np.isinf(rmse_min):\n                w = 0.0\n            else:\n                A_indices = np.where(profile_loglik >= l_max - DELTA)[0]\n                B_indices = np.where(profile_rmse = (1.0 + EPSILON) * rmse_min)[0]\n                I_indices = np.intersect1d(A_indices, B_indices)\n                w = len(I_indices) / len(PROFILE_GRIDS[focal_param_name])\n            \n            w_values.append(w)\n\n        # Equifinality decision logic\n        max_w = np.max(w_values)\n        \n        if max_w >= TAU:\n            f = 1\n            p = np.argmax(w_values)\n            d = PROPOSAL_MAP[p]\n            \n            # Calculate skill gap g\n            winning_param_name = PARAM_NAMES[p]\n            profile_for_g = all_profiles[winning_param_name]\n            \n            l_max_g = np.max(profile_for_g['loglik'])\n            rmse_min_g = np.min(profile_for_g['rmse'])\n            \n            A_indices_g = np.where(profile_for_g['loglik'] >= l_max_g - DELTA)[0]\n            B_indices_g = np.where(profile_for_g['rmse'] = (1.0 + EPSILON) * rmse_min_g)[0]\n            I_indices_g = np.intersect1d(A_indices_g, B_indices_g)\n            \n            rmses_in_I = profile_for_g['rmse'][I_indices_g]\n            min_rmse_in_I = np.min(rmses_in_I)\n            max_rmse_in_I = np.max(rmses_in_I)\n            \n            g = (max_rmse_in_I - min_rmse_in_I) / min_rmse_in_I if min_rmse_in_I > 0 else 0.0\n\n        else:\n            f = 0\n            p = -1\n            d = 0\n            g = 0.0\n            \n        final_results.append([f, p, d, g])\n\n    # Format and print the final output\n    case_strings = [f\"[{res[0]},{res[1]},{res[2]},{res[3]:.4g}]\" for res in final_results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "2482790"}]}