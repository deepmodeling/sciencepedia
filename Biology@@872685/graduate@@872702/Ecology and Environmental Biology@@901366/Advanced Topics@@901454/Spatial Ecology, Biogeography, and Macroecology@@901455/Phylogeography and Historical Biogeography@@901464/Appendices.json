{"hands_on_practices": [{"introduction": "A central goal of phylogeography is to reconstruct a species' demographic past using patterns of genetic variation observed in the present. One of the most fundamental links between genetics and demography is the relationship between nucleotide diversity and effective population size. This exercise [@problem_id:2521267] provides hands-on practice with this core concept, challenging you to apply the theoretical expectation under mutation-drift equilibrium to estimate the long-term effective population size ($N_e$) from sequence data, a foundational skill in population genetic inference.", "problem": "A panmictic island-dwelling lizard lineage is being studied to infer its demographic history in a phylogeographic context. You sequence a single putatively neutral, nonrecombining nuclear locus of length $L$ base pairs from $n$ diploid individuals sampled across the species range. After rigorous quality control to exclude sites with evidence of selection, the observed average pairwise nucleotide diversity per site (the mean fraction of pairwise differences per site across all pairs of chromosomes), denoted by $\\pi$, is $\\pi = 0.0087$. An independent pedigree-based estimate of the per-site, per-generation mutation rate for this locus is $\\mu = 1.1 \\times 10^{-8}$.\n\nAssume a Wright–Fisher population model with constant size, diploid inheritance, neutrality, and the infinite-sites mutation model at mutation–drift equilibrium. Under these assumptions, estimate the diploid effective population size $N_{e}$ of the lineage. Round your final answer to $3$ significant figures. Express your final answer as a number of diploid individuals.", "solution": "The problem statement has been subjected to rigorous validation and is deemed valid. It is scientifically grounded in the principles of population genetics, well-posed with sufficient information for a unique solution, and objectively stated. The problem requires the application of fundamental theory to estimate a key demographic parameter from genetic data.\n\nThe core of this problem rests on the relationship between nucleotide diversity, the effective population size, and the mutation rate under the specific assumptions of the Wright–Fisher model at mutation–drift equilibrium. For a diploid, panmictic population of constant size, the expected average pairwise nucleotide diversity per site, denoted as $E[\\pi]$, is equal to the population mutation parameter, $\\theta$.\n\nThe formula for the population mutation parameter $\\theta$ in a diploid population is given by:\n$$\n\\theta = 4 N_{e} \\mu\n$$\nwhere $N_{e}$ is the diploid effective population size and $\\mu$ is the neutral mutation rate per site per generation.\n\nAt mutation–drift equilibrium, the expected nucleotide diversity equals this parameter:\n$$\nE[\\pi] = \\theta = 4 N_{e} \\mu\n$$\nThe problem provides the observed average pairwise nucleotide diversity, $\\pi = 0.0087$, which is our empirical estimate for the theoretical expectation $E[\\pi]$. We are also given the per-site, per-generation mutation rate $\\mu = 1.1 \\times 10^{-8}$. The other parameters provided, such as locus length $L$ and sample size $n$, are contextual for a real study but are not required for this specific calculation, as $\\pi$ is already provided as an average per-site value.\n\nWe can establish the following equation using the provided data:\n$$\n\\pi = 4 N_{e} \\mu\n$$\nOur objective is to solve for the diploid effective population size, $N_{e}$. We can rearrange the equation algebraically:\n$$\nN_{e} = \\frac{\\pi}{4 \\mu}\n$$\nNow, we substitute the given numerical values into this expression:\n$$\n\\pi = 0.0087\n$$\n$$\n\\mu = 1.1 \\times 10^{-8}\n$$\nSubstituting these into our equation for $N_{e}$:\n$$\nN_{e} = \\frac{0.0087}{4 \\times (1.1 \\times 10^{-8})}\n$$\nFirst, calculate the denominator:\n$$\n4 \\times 1.1 \\times 10^{-8} = 4.4 \\times 10^{-8}\n$$\nNow, perform the division:\n$$\nN_{e} = \\frac{0.0087}{4.4 \\times 10^{-8}} = \\frac{8.7 \\times 10^{-3}}{4.4 \\times 10^{-8}}\n$$\n$$\nN_{e} = \\left(\\frac{8.7}{4.4}\\right) \\times 10^{-3 - (-8)} = \\left(\\frac{8.7}{4.4}\\right) \\times 10^{5}\n$$\nThe division of the coefficients is:\n$$\n\\frac{8.7}{4.4} \\approx 1.977272...\n$$\nTherefore, the effective population size is:\n$$\nN_{e} \\approx 1.977272... \\times 10^{5} \\approx 197727.27...\n$$\nThe problem requires the final answer to be rounded to $3$ significant figures. The first three significant figures are $1$, $9$, and $7$. The fourth significant figure is $7$, which is greater than or equal to $5$, so we must round up the third significant figure.\n$$\nN_{e} \\approx 1.98 \\times 10^{5}\n$$\nThis corresponds to an estimated diploid effective population size of $198,000$ individuals.", "answer": "$$\\boxed{1.98 \\times 10^{5}}$$", "id": "2521267"}, {"introduction": "While single-population estimates are informative, phylogeography excels at revealing how historical processes have shaped genetic variation across entire landscapes. The Isolation-by-Distance (IBD) model provides a powerful null hypothesis, positing that genetic differentiation should increase with geographic distance simply due to limited dispersal. In this advanced practice [@problem_id:2521282], you will implement a complete IBD analysis pipeline, from calculating geographic distances and genetic differentiation ($F_{ST}$) to performing the final statistical regression, mirroring a common workflow in contemporary research.", "problem": "You are given diploid single-nucleotide polymorphism (SNP) genotypes for spatially referenced individuals sampled from multiple localities. Assume each individual is sampled at a locality and that multiple individuals may be sampled per locality, all sharing identical coordinates. Treat each locality as a subpopulation. Your task is to estimate the isolation-by-distance (IBD) slope by regressing the transformed genetic differentiation statistic on the logarithm of geographic distance.\n\nFundamental base and definitions:\n- The fixation index (F-statistics) between two subpopulations, denoted $F_{ST}$, measures the standardized variance in allele frequencies among subpopulations. A widely used unbiased estimator for biallelic markers between two samples is the Hudson estimator aggregated across loci, defined as follows. For each locus $l$, let $n_1 = 2 m_1$ and $n_2 = 2 m_2$ be the numbers of gene copies in subpopulations $1$ and $2$, where $m_1$ and $m_2$ are the numbers of diploid individuals genotyped at that locus in the respective subpopulations. Let $x_1$ and $x_2$ be the counts of the derived allele across the $n_1$ and $n_2$ gene copies, with sample frequencies $p_1 = x_1/n_1$ and $p_2 = x_2/n_2$. Define $h_1 = p_1(1 - p_1)$ and $h_2 = p_2(1 - p_2)$. The per-locus components are\n$$\na_l = (p_1 - p_2)^2 - \\frac{h_1}{n_1 - 1} - \\frac{h_2}{n_2 - 1}, \\quad\nb_l = p_1(1 - p_2) + p_2(1 - p_1).\n$$\nThe multi-locus Hudson estimator is\n$$\nF_{ST} = \\frac{\\sum_l a_l}{\\sum_l b_l},\n$$\nprovided $\\sum_l b_l > 0$. If $\\sum_l b_l = 0$, the locus set provides no information and should be treated as non-informative for that pair.\n- The IBD transformation for two subpopulations is $y = \\frac{F_{ST}}{1 - F_{ST}}$.\n- The geographic distance between two localities with latitude and longitude given in degrees is computed on a sphere of radius $R = 6371$ kilometers using the Haversine formula. For coordinates $(\\phi_1, \\lambda_1)$ and $(\\phi_2, \\lambda_2)$ in degrees, first convert to radians, then compute\n$$\n\\Delta \\phi = \\phi_2 - \\phi_1, \\quad \\Delta \\lambda = \\lambda_2 - \\lambda_1,\n$$\n$$\nA = \\sin^2\\left(\\frac{\\Delta \\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta \\lambda}{2}\\right),\n$$\n$$\nd = 2 R \\arcsin(\\sqrt{A}),\n$$\nwhich yields $d$ in kilometers. Use the natural logarithm $\\log$ of $d$ as the predictor.\n- Estimate the slope parameter $\\beta$ of the linear relationship\n$$\ny = \\alpha + \\beta \\log(d) + \\varepsilon\n$$\nby ordinary least squares (OLS) with an intercept $\\alpha$, using all unordered pairs of distinct subpopulations. The OLS slope is obtained from minimizing the sum of squared residuals across all pairwise data.\n\nProgram requirements:\n- Implement the above steps: group individuals by locality, compute pairwise $F_{ST}$ using the Hudson estimator across all loci, transform to $y = F_{ST}/(1 - F_{ST})$, compute geographic distances in kilometers via the Haversine formula (angles in degrees in input, radians internally), and fit an OLS regression of $y$ on $\\log(d)$ with an intercept. Exclude any pair with $d \\le 0$ (none should occur because only distinct localities are paired).\n- Units: All distances must be computed in kilometers; use $R = 6371$ kilometers. Angles in the input are given in degrees and must be converted to radians for trigonometric functions. The regression uses the natural logarithm (base $e$).\n- Output: For each dataset in the test suite below, output the OLS slope $\\beta$ as a floating-point number rounded to $6$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.123456,0.000001,1.234567]\").\n\nTest suite:\nEach dataset contains a set of localities. Each locality specifies a latitude (degrees), a longitude (degrees), and a list of diploid individuals. Each individual has genotypes at all loci encoded as counts of the derived allele in $\\{0, 1, 2\\}$. The loci are ordered consistently across all individuals within a dataset.\n\nDataset A (happy path; $4$ localities, $8$ loci, $4$ individuals per locality):\n- Localities and coordinates (latitude, longitude in degrees):\n  - A: $(0, 0)$\n  - B: $(0, 1)$\n  - C: $(0, 2)$\n  - D: $(0, 3)$\n- Genotypes by locality (rows are individuals, columns are loci $\\ell = 1,\\dots,8$):\n  - A:\n    - $[0, 1, 0, 1, 2, 1, 2, 1]$\n    - $[0, 0, 1, 1, 0, 1, 1, 1]$\n    - $[0, 0, 0, 0, 0, 1, 0, 1]$\n    - $[0, 0, 0, 0, 0, 0, 0, 1]$\n  - B:\n    - $[1, 1, 2, 1, 2, 1, 2, 2]$\n    - $[0, 1, 0, 1, 1, 1, 1, 2]$\n    - $[0, 0, 0, 1, 0, 1, 1, 1]$\n    - $[0, 0, 0, 0, 0, 1, 0, 0]$\n  - C:\n    - $[1, 2, 2, 1, 2, 2, 2, 2]$\n    - $[1, 1, 1, 1, 1, 2, 2, 2]$\n    - $[0, 0, 0, 1, 1, 1, 1, 2]$\n    - $[0, 0, 0, 1, 0, 0, 0, 0]$\n  - D:\n    - $[1, 2, 2, 2, 2, 2, 2, 2]$\n    - $[1, 2, 1, 2, 2, 2, 2, 2]$\n    - $[1, 1, 1, 2, 1, 2, 2, 2]$\n    - $[1, 0, 1, 0, 1, 1, 1, 2]$\n\nDataset B (boundary case with weak differentiation; $3$ localities, $6$ loci, $3$ individuals per locality):\n- Localities and coordinates:\n  - E: $(10, 10)$\n  - F: $(10, 11)$\n  - G: $(11, 10)$\n- Genotypes:\n  - E:\n    - $[1, 1, 1, 2, 1, 1]$\n    - $[1, 1, 1, 1, 1, 1]$\n    - $[1, 1, 0, 1, 1, 0]$\n  - F:\n    - $[1, 1, 1, 2, 1, 1]$\n    - $[1, 1, 1, 1, 1, 1]$\n    - $[1, 0, 1, 1, 0, 1]$\n  - G:\n    - $[1, 1, 1, 2, 1, 1]$\n    - $[1, 1, 1, 1, 1, 1]$\n    - $[0, 1, 1, 1, 1, 0]$\n\nDataset C (edge case with a distant, highly differentiated locality; $4$ localities, $7$ loci, $3$ individuals per locality):\n- Localities and coordinates:\n  - H: $(0, 0)$\n  - I: $(0, 1)$\n  - J: $(0, 2)$\n  - K: $(20, 20)$\n- Genotypes:\n  - H:\n    - $[1, 1, 1, 1, 1, 1, 1]$\n    - $[1, 1, 0, 1, 1, 0, 1]$\n    - $[0, 0, 0, 1, 0, 0, 0]$\n  - I:\n    - $[1, 1, 1, 1, 1, 1, 1]$\n    - $[1, 1, 1, 1, 1, 1, 1]$\n    - $[0, 0, 0, 1, 0, 0, 0]$\n  - J:\n    - $[1, 1, 1, 1, 1, 1, 1]$\n    - $[1, 0, 1, 1, 1, 0, 1]$\n    - $[0, 0, 0, 1, 0, 0, 0]$\n  - K:\n    - $[2, 2, 2, 2, 2, 2, 2]$\n    - $[2, 2, 1, 2, 2, 2, 1]$\n    - $[1, 1, 1, 2, 1, 1, 1]$\n\nYour program must internally construct these datasets exactly as specified, perform the computations as described, and print a single line containing the slopes for Dataset A, Dataset B, and Dataset C, in that order, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets.", "solution": "The problem presented is subjected to rigorous validation and is found to be scientifically grounded, well-posed, and objective. It describes a standard procedure in the field of population genetics for quantifying isolation by distance. The definitions, data, and objectives are specified with sufficient clarity and precision to permit a unique, verifiable solution. We may therefore proceed with the derivation and implementation of the solution.\n\nThe objective is to estimate the slope parameter, $\\beta$, of the linear relationship between transformed genetic differentiation and the natural logarithm of geographic distance. The model is given by $y = \\alpha + \\beta \\log(d) + \\varepsilon$. The estimation procedure involves three primary stages: data organization, pairwise metric calculation, and parameter estimation via Ordinary Least Squares (OLS) regression.\n\nFirst, the provided genotype data is organized by sampling locality. The input consists of diploid individuals with genotypes encoded as the count of a derived allele, an integer from the set $\\{0, 1, 2\\}$, at multiple single-nucleotide polymorphism (SNP) loci. We must process these data for each pair of distinct localities.\n\nSecond, for each unique unordered pair of localities, we compute two key metrics: the geographic distance, $d$, and the transformed genetic differentiation, $y$.\n\nThe geographic distance, $d$, is calculated using the Haversine formula, which determines the great-circle distance on a sphere. Given two localities with coordinates $(\\phi_1, \\lambda_1)$ and $(\\phi_2, \\lambda_2)$ in degrees, we first convert these to radians. The distance $d$ in kilometers ($km$) is then computed as:\n$$\nd = 2 R \\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\Delta \\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta \\lambda}{2}\\right)}\\right)\n$$\nwhere $\\Delta \\phi = \\phi_2 - \\phi_1$, $\\Delta \\lambda = \\lambda_2 - \\lambda_1$, and the Earth's radius is taken as $R = 6371$ km. The predictor variable for our regression is the natural logarithm of this distance, $X_i = \\log(d_i)$ for a given pair $i$. Any pair with a resulting distance $d \\le 0$ must be excluded from analysis.\n\nThe genetic differentiation is quantified using the fixation index, $F_{ST}$. We employ the multi-locus Hudson estimator, an unbiased estimator for biallelic markers. For a given pair of subpopulations (localities) with $m_1$ and $m_2$ diploid individuals respectively, the number of gene copies are $n_1 = 2m_1$ and $n_2 = 2m_2$. For each locus $l$, we first compute the sample allele frequencies of the derived allele, $p_{1,l} = x_{1,l}/n_1$ and $p_{2,l} = x_{2,l}/n_2$, where $x_{1,l}$ and $x_{2,l}$ are the counts of the derived allele in each subpopulation at that locus. The per-locus components of the estimator are:\n$$\na_l = (p_{1,l} - p_{2,l})^2 - \\frac{h_{1,l}}{n_1 - 1} - \\frac{h_{2,l}}{n_2 - 1}\n$$\n$$\nb_l = p_{1,l}(1 - p_{2,l}) + p_{2,l}(1 - p_{1,l})\n$$\nwhere $h_{1,l} = p_{1,l}(1 - p_{1,l})$ and $h_{2,l} = p_{2,l}(1 - p_{2,l})$ are the within-population heterozygosities. The terms containing $n-1$ in the denominator constitute the correction for finite sample sizes. The multi-locus estimator is the ratio of the sums of these components across all $L$ loci:\n$$\nF_{ST} = \\frac{\\sum_{l=1}^{L} a_l}{\\sum_{l=1}^{L} b_l}\n$$\nThis calculation is valid only if the denominator $\\sum_l b_l > 0$. If this condition is not met, the pair of populations is considered non-informative and is excluded from the subsequent analysis. The $F_{ST}$ value is then transformed to linearize its relationship with $\\log(d)$ using the formula:\n$$\ny = \\frac{F_{ST}}{1 - F_{ST}}\n$$\nThis transformation is undefined if $F_{ST} \\ge 1$. Such pairs, representing complete or extreme differentiation, are also excluded from the regression analysis.\n\nThird, after computing the set of data points $\\{(\\log(d_i), y_i)\\}$ for all valid pairs of localities $i$, we estimate the slope parameter $\\beta$. The linear model is $y_i = \\alpha + \\beta \\log(d_i) + \\varepsilon_i$. Using Ordinary Least Squares (OLS), the slope $\\beta$ that minimizes the sum of squared residuals $\\sum_i (y_i - (\\alpha + \\beta \\log(d_i)))^2$ is given by:\n$$\n\\beta = \\frac{\\sum_{i} (\\log(d_i) - \\overline{\\log(d)}) (y_i - \\bar{y})}{\\sum_{i} (\\log(d_i) - \\overline{\\log(d)})^2}\n$$\nwhere $\\overline{\\log(d)}$ and $\\bar{y}$ are the sample means of the predictor and response variables, respectively. This value will be computed for each dataset provided. The implementation will follow these steps precisely, utilizing numerical libraries for robust calculation of distances, genetic statistics, and the final OLS regression.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the isolation-by-distance problem for the provided test suite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            'localities': {\n                'A': {'coords': (0, 0), 'genotypes': np.array([\n                    [0, 1, 0, 1, 2, 1, 2, 1],\n                    [0, 0, 1, 1, 0, 1, 1, 1],\n                    [0, 0, 0, 0, 0, 1, 0, 1],\n                    [0, 0, 0, 0, 0, 0, 0, 1]\n                ])},\n                'B': {'coords': (0, 1), 'genotypes': np.array([\n                    [1, 1, 2, 1, 2, 1, 2, 2],\n                    [0, 1, 0, 1, 1, 1, 1, 2],\n                    [0, 0, 0, 1, 0, 1, 1, 1],\n                    [0, 0, 0, 0, 0, 1, 0, 0]\n                ])},\n                'C': {'coords': (0, 2), 'genotypes': np.array([\n                    [1, 2, 2, 1, 2, 2, 2, 2],\n                    [1, 1, 1, 1, 1, 2, 2, 2],\n                    [0, 0, 0, 1, 1, 1, 1, 2],\n                    [0, 0, 0, 1, 0, 0, 0, 0]\n                ])},\n                'D': {'coords': (0, 3), 'genotypes': np.array([\n                    [1, 2, 2, 2, 2, 2, 2, 2],\n                    [1, 2, 1, 2, 2, 2, 2, 2],\n                    [1, 1, 1, 2, 1, 2, 2, 2],\n                    [1, 0, 1, 0, 1, 1, 1, 2]\n                ])}\n            }\n        },\n        # Dataset B\n        {\n            'localities': {\n                'E': {'coords': (10, 10), 'genotypes': np.array([\n                    [1, 1, 1, 2, 1, 1],\n                    [1, 1, 1, 1, 1, 1],\n                    [1, 1, 0, 1, 1, 0]\n                ])},\n                'F': {'coords': (10, 11), 'genotypes': np.array([\n                    [1, 1, 1, 2, 1, 1],\n                    [1, 1, 1, 1, 1, 1],\n                    [1, 0, 1, 1, 0, 1]\n                ])},\n                'G': {'coords': (11, 10), 'genotypes': np.array([\n                    [1, 1, 1, 2, 1, 1],\n                    [1, 1, 1, 1, 1, 1],\n                    [0, 1, 1, 1, 1, 0]\n                ])}\n            }\n        },\n        # Dataset C\n        {\n            'localities': {\n                'H': {'coords': (0, 0), 'genotypes': np.array([\n                    [1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 0, 1, 1, 0, 1],\n                    [0, 0, 0, 1, 0, 0, 0]\n                ])},\n                'I': {'coords': (0, 1), 'genotypes': np.array([\n                    [1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1],\n                    [0, 0, 0, 1, 0, 0, 0]\n                ])},\n                'J': {'coords': (0, 2), 'genotypes': np.array([\n                    [1, 1, 1, 1, 1, 1, 1],\n                    [1, 0, 1, 1, 1, 0, 1],\n                    [0, 0, 0, 1, 0, 0, 0]\n                ])},\n                'K': {'coords': (20, 20), 'genotypes': np.array([\n                    [2, 2, 2, 2, 2, 2, 2],\n                    [2, 2, 1, 2, 2, 2, 1],\n                    [1, 1, 1, 2, 1, 1, 1]\n                ])}\n            }\n        }\n    ]\n\n    def haversine_distance(coords1, coords2, R=6371.0):\n        lat1, lon1 = np.radians(coords1)\n        lat2, lon2 = np.radians(coords2)\n        \n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n        \n        a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n        d = 2 * R * np.arcsin(np.sqrt(a))\n        return d\n\n    def calculate_fst_y(genotypes1, genotypes2):\n        m1 = genotypes1.shape[0]\n        m2 = genotypes2.shape[0]\n        \n        if m1 == 0 or m2 == 0:\n            return None\n\n        n1 = 2 * m1\n        n2 = 2 * m2\n\n        if n1 <= 1 or n2 <= 1:\n             return None\n\n        x1_l = np.sum(genotypes1, axis=0)\n        x2_l = np.sum(genotypes2, axis=0)\n\n        p1_l = x1_l / n1\n        p2_l = x2_l / n2\n\n        h1_l = p1_l * (1 - p1_l)\n        h2_l = p2_l * (1 - p2_l)\n\n        a_l = (p1_l - p2_l)**2 - h1_l / (n1 - 1) - h2_l / (n2 - 1)\n        b_l = p1_l * (1 - p2_l) + p2_l * (1 - p1_l)\n\n        sum_a = np.sum(a_l)\n        sum_b = np.sum(b_l)\n\n        if sum_b <= 0:\n            return None\n\n        fst = sum_a / sum_b\n        \n        if fst >= 1.0:\n            return None\n\n        y = fst / (1 - fst)\n        return y\n\n    all_results = []\n    for case in test_cases:\n        dataset = case['localities']\n        locality_names = list(dataset.keys())\n        pairs = list(combinations(locality_names, 2))\n\n        regression_x = []\n        regression_y = []\n\n        for loc1_name, loc2_name in pairs:\n            loc1_data = dataset[loc1_name]\n            loc2_data = dataset[loc2_name]\n            \n            d = haversine_distance(loc1_data['coords'], loc2_data['coords'])\n            if d <= 0:\n                continue\n            \n            log_d = np.log(d)\n            \n            y = calculate_fst_y(loc1_data['genotypes'], loc2_data['genotypes'])\n            if y is None:\n                continue\n\n            regression_x.append(log_d)\n            regression_y.append(y)\n        \n        slope = 0.0\n        if len(regression_x) >= 2:\n            x_arr = np.array(regression_x)\n            y_arr = np.array(regression_y)\n            \n            # Using np.polyfit for OLS regression of degree 1 (linear)\n            # It returns [slope, intercept]\n            coeffs = np.polyfit(x_arr, y_arr, 1)\n            slope = coeffs[0]\n\n        all_results.append(round(slope, 6))\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2521282"}, {"introduction": "After describing spatial genetic patterns, the ultimate goal of historical biogeography is to infer the underlying evolutionary processes—such as dispersal, vicariance, or in-situ extinction—that created them. Rather than relying on qualitative arguments, modern biogeography uses formal statistical model selection to weigh the evidence for competing historical scenarios. This exercise [@problem_id:2521298] places you in the role of a researcher comparing outputs from prominent biogeographic models, requiring you to apply and contrast both information-theoretic (AICc) and Bayesian model selection frameworks to make a rigorous, evidence-based conclusion.", "problem": "You are given three historical biogeography models fitted to the same phylogeographic dataset: Dispersal–Extinction–Cladogenesis (DEC), Dispersal–Vicariance Analysis-like (DIVA-like), and BAYAREALIKE. For each model and test case, you are provided the maximum log-likelihood and the log marginal likelihood (log-evidence) under a uniform prior over models. Assume the number of free parameters is $k_{\\mathrm{DEC}} = 2$, $k_{\\mathrm{DIVA\\text{-}like}} = 1$, and $k_{\\mathrm{BAYAREALIKE}} = 1$. Assume the effective sample size is $n$ as given per case.\n\nYour task is to implement a program that, for each test case:\n- Applies information-theoretic model selection grounded in Kullback–Leibler divergence minimization to compute the small-sample corrected Akaike Information Criterion (AICc). Select the model with the lowest AICc. Then compute the Akaike weight of the selected model by transforming the AICc differences into a normalized weight over the three models.\n- Applies Bayesian model comparison by converting the provided log marginal likelihoods (log-evidences) into posterior model probabilities under a uniform prior over models, using Bayes’ theorem. Select the model with the highest posterior probability. For numerical stability, when exponentiating log values, use a method that avoids underflow.\n- Report, for each test case, the index of the AICc-selected model, the index of the posterior-selected model, whether the two selections agree, the posterior probability of the posterior-selected model rounded to four decimals, and the Akaike weight of the AICc-selected model rounded to four decimals.\n\nInterpret the three model names using indices as follows: DEC $\\rightarrow$ index $0$, DIVA-like $\\rightarrow$ index $1$, BAYAREALIKE $\\rightarrow$ index $2$.\n\nUse the following test suite that covers multiple regimes:\n- Case A (general, moderate $n$): $n = 50$; maximum log-likelihoods $(\\ell_{\\mathrm{DEC}}, \\ell_{\\mathrm{DIVA\\text{-}like}}, \\ell_{\\mathrm{BAYAREALIKE}}) = (-120.5, -119.2, -121.0)$; log marginal likelihoods $(\\log Z_{\\mathrm{DEC}}, \\log Z_{\\mathrm{DIVA\\text{-}like}}, \\log Z_{\\mathrm{BAYAREALIKE}}) = (-123.0, -122.1, -124.5)$.\n- Case B (clear fit advantage to DEC, larger $n$): $n = 80$; maximum log-likelihoods $(\\ell_{\\mathrm{DEC}}, \\ell_{\\mathrm{DIVA\\text{-}like}}, \\ell_{\\mathrm{BAYAREALIKE}}) = (-100.0, -106.0, -104.0)$; log marginal likelihoods $(\\log Z_{\\mathrm{DEC}}, \\log Z_{\\mathrm{DIVA\\text{-}like}}, \\log Z_{\\mathrm{BAYAREALIKE}}) = (-101.5, -109.0, -106.0)$.\n- Case C (small $n$, stronger small-sample penalty): $n = 8$; maximum log-likelihoods $(\\ell_{\\mathrm{DEC}}, \\ell_{\\mathrm{DIVA\\text{-}like}}, \\ell_{\\mathrm{BAYAREALIKE}}) = (-15.0, -15.2, -15.25)$; log marginal likelihoods $(\\log Z_{\\mathrm{DEC}}, \\log Z_{\\mathrm{DIVA\\text{-}like}}, \\log Z_{\\mathrm{BAYAREALIKE}}) = (-16.5, -16.0, -16.1)$.\n- Case D (near tie in likelihoods, potential information-theoretic versus Bayesian disagreement): $n = 40$; maximum log-likelihoods $(\\ell_{\\mathrm{DEC}}, \\ell_{\\mathrm{DIVA\\text{-}like}}, \\ell_{\\mathrm{BAYAREALIKE}}) = (-80.0, -79.7, -79.8)$; log marginal likelihoods $(\\log Z_{\\mathrm{DEC}}, \\log Z_{\\mathrm{DIVA\\text{-}like}}, \\log Z_{\\mathrm{BAYAREALIKE}}) = (-81.2, -81.5, -82.0)$.\n\nRequirements and conventions:\n- Base your derivation on the core definitions that the Akaike Information Criterion approximates the expected Kullback–Leibler divergence between a fitted model and the data-generating process with a parameter-count penalty, and that the small-sample correction adjusts for finite sample bias; and that posterior model probabilities are proportional to the product of model priors and marginal likelihoods.\n- Angles are not involved.\n- Probabilities must be reported as decimals (not in percentage notation) and rounded to four decimals.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list in the form [$a$, $b$, $c$, $d$, $e$], where $a$ is the AICc-selected model index (an integer), $b$ is the posterior-selected model index (an integer), $c$ is a boolean indicating agreement, $d$ is the posterior probability of the posterior-selected model (a float rounded to four decimals), and $e$ is the Akaike weight of the AICc-selected model (a float rounded to four decimals). For example, a valid output with two hypothetical cases could look like \"[[0,1,False,0.6123,0.7011],[2,2,True,0.8450,0.7732]]\".", "solution": "The problem presented is a standard exercise in computational statistics as applied to the field of historical biogeography. It is scientifically grounded, well-posed, and contains all necessary information for a unique solution. The models cited—Dispersal–Extinction–Cladogenesis (DEC), Dispersal–Vicariance Analysis-like (DIVA-like), and BAYAREALIKE—are established tools for inferring ancestral ranges and biogeographic history on a phylogeny. The task of comparing these models using both information-theoretic and Bayesian frameworks is a cornerstone of modern scientific practice. The problem is therefore deemed valid.\n\nWe will proceed with a rigorous, step-by-step derivation of the quantities required for model selection. The objective is not merely to compute numbers, but to understand the fundamental principles that justify these computations.\n\nA scientific model is an approximation of reality. When presented with multiple competing models, we require an objective criterion to select the one that offers the best trade-off between goodness-of-fit and complexity. We will examine two such criteria.\n\nFirst, the information-theoretic approach, grounded in Kullback–Leibler divergence. The Akaike Information Criterion (AIC) is an estimator of the expected, relative information lost when a given model is used to represent the process that generates the data. For a model $i$, the AIC is defined as:\n$$\n\\mathrm{AIC}_i = -2\\ell_i + 2k_i\n$$\nwhere $\\ell_i$ is the maximum log-likelihood of the model and $k_i$ is the number of estimable parameters. This formula, however, assumes a large sample size $n$. When $n$ is not substantially larger than $k$, a second-order correction is necessary to account for small-sample bias. This gives the small-sample corrected Akaike Information Criterion ($\\mathrm{AICc}$):\n$$\n\\mathrm{AICc}_i = \\mathrm{AIC}_i + \\frac{2k_i(k_i+1)}{n-k_i-1} = -2\\ell_i + 2k_i + \\frac{2k_i(k_i+1)}{n-k_i-1}\n$$\nThis correction term increases the penalty for parameters, particularly when $n$ is small. The model with the minimum $\\mathrm{AICc}$ value is selected as the best-approximating model in the set.\n\nTo quantify the plausibility of each model $i$ being the best model, we use Akaike weights ($w_i$). First, we compute the difference between the $\\mathrm{AICc}$ of each model and the minimum $\\mathrm{AICc}$ in the set: $\\Delta_i = \\mathrm{AICc}_i - \\min_j(\\mathrm{AICc}_j)$. The Akaike weight for model $i$ is then:\n$$\nw_i = \\frac{\\exp(-\\frac{1}{2}\\Delta_i)}{\\sum_{j=1}^{M} \\exp(-\\frac{1}{2}\\Delta_j)}\n$$\nwhere $M$ is the number of models being compared. The weight $w_i$ can be interpreted as the probability that model $i$ is the best model in the set, given the data and the set of models.\n\nSecond, the Bayesian approach to model selection. This framework uses Bayes' theorem to update our prior beliefs about the models in light of the observed data. The posterior probability of a model $M_i$ given data $D$ is:\n$$\nP(M_i | D) = \\frac{P(D | M_i) P(M_i)}{P(D)}\n$$\nHere, $P(M_i)$ is the prior probability of the model. The problem specifies a uniform prior, so $P(M_i) = 1/M$ for all $i$. The term $P(D | M_i)$ is the marginal likelihood, or evidence, of the model, which we denote $Z_i$. It represents the probability of the data integrated over the entire parameter space of the model. The denominator, $P(D) = \\sum_{j=1}^{M} P(D|M_j)P(M_j)$, is a normalization constant ensuring the posterior probabilities sum to $1$. With a uniform prior, the posterior probability simplifies to:\n$$\nP(M_i | D) = \\frac{Z_i}{\\sum_{j=1}^{M} Z_j}\n$$\nThe model with the highest posterior probability is selected. Computationally, we are given log marginal likelihoods ($\\log Z_i$), so we must handle exponentiation carefully to avoid numerical underflow. Let $L_j = \\log Z_j$ and $L_{\\max} = \\max_j(L_j)$. The posterior probability is calculated as:\n$$\nP(M_i | D) = \\frac{\\exp(L_i)}{\\sum_{j=1}^{M} \\exp(L_j)} = \\frac{\\exp(L_i - L_{\\max})}{\\sum_{j=1}^{M} \\exp(L_j - L_{\\max})}\n$$\nThis maneuvering, known as the log-sum-exp trick, ensures numerical stability.\n\nThe program will implement these two distinct procedures for each test case. For each case, it will take as input the effective sample size $n$, the vector of maximum log-likelihoods $(\\ell_{\\mathrm{DEC}}, \\ell_{\\mathrm{DIVA\\text{-}like}}, \\ell_{\\mathrm{BAYAREALIKE}})$, and the vector of log marginal likelihoods $(\\log Z_{\\mathrm{DEC}}, \\log Z_{\\mathrm{DIVA\\text{-}like}}, \\log Z_{\\mathrm{BAYAREALIKE}})$. The number of parameters for each model are constants: $k_{\\mathrm{DEC}} = 2$, $k_{\\mathrm{DIVA\\text{-}like}} = 1$, and $k_{\\mathrm{BAYAREALIKE}} = 1$. The program will compute the $\\mathrm{AICc}$ for each model, identify the best model (minimum $\\mathrm{AICc}$), and calculate its Akaike weight. Concurrently, it will compute the posterior probability for each model, identify the best model (maximum posterior probability), and record this probability. Finally, it will report the indices of the selected models, whether the selections agree, and the corresponding weight and probability metrics, rounded as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs model selection using AICc and Bayesian posterior probabilities\n    for phylogeographic models.\n    \"\"\"\n    # Define the number of free parameters for each model.\n    # Index 0: DEC, Index 1: DIVA-like, Index 2: BAYAREALIKE\n    k_params = np.array([2, 1, 1])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 50,\n            \"log_L\": np.array([-120.5, -119.2, -121.0]),\n            \"log_Z\": np.array([-123.0, -122.1, -124.5])\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 80,\n            \"log_L\": np.array([-100.0, -106.0, -104.0]),\n            \"log_Z\": np.array([-101.5, -109.0, -106.0])\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 8,\n            \"log_L\": np.array([-15.0, -15.2, -15.25]),\n            \"log_Z\": np.array([-16.5, -16.0, -16.1])\n        },\n        {\n            \"name\": \"Case D\",\n            \"n\": 40,\n            \"log_L\": np.array([-80.0, -79.7, -79.8]),\n            \"log_Z\": np.array([-81.2, -81.5, -82.0])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n = case['n']\n        log_L = case['log_L']\n        log_Z = case['log_Z']\n\n        # --- Information-Theoretic (AICc) Model Selection ---\n        \n        # Calculate AICc for each model.\n        # AICc = -2*logL + 2*k + (2*k*(k+1))/(n-k-1)\n        aicc = -2 * log_L + 2 * k_params + (2 * k_params * (k_params + 1)) / (n - k_params - 1)\n        \n        # Select the model with the minimum AICc.\n        aicc_selected_idx = int(np.argmin(aicc))\n        \n        # Calculate Akaike weights.\n        min_aicc = aicc[aicc_selected_idx]\n        delta_aicc = aicc - min_aicc\n        \n        # Numerator of Akaike weights formula\n        exp_delta_half = np.exp(-0.5 * delta_aicc)\n        \n        # Denominator (sum over all models)\n        sum_exp_delta_half = np.sum(exp_delta_half)\n        \n        akaike_weights = exp_delta_half / sum_exp_delta_half\n        aicc_selected_weight = round(akaike_weights[aicc_selected_idx], 4)\n\n        # --- Bayesian Model Selection ---\n        \n        # Select the model with the maximum log marginal likelihood (log-evidence).\n        posterior_selected_idx = int(np.argmax(log_Z))\n        \n        # Calculate posterior model probabilities using the log-sum-exp trick for stability.\n        log_Z_max = np.max(log_Z)\n        rel_log_Z = log_Z - log_Z_max\n        \n        # Exponentiate relative log-evidences\n        exp_rel_log_Z = np.exp(rel_log_Z)\n        \n        # Sum is the normalization constant\n        sum_exp_rel_log_Z = np.sum(exp_rel_log_Z)\n        \n        posterior_probs = exp_rel_log_Z / sum_exp_rel_log_Z\n        posterior_selected_prob = round(posterior_probs[posterior_selected_idx], 4)\n        \n        # --- Final Reporting ---\n        \n        # Check if the two selection methods agree.\n        agreement = (aicc_selected_idx == posterior_selected_idx)\n        \n        # Assemble the result for the current test case.\n        result = [\n            aicc_selected_idx,\n            posterior_selected_idx,\n            agreement,\n            posterior_selected_prob,\n            aicc_selected_weight\n        ]\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str(all_results) representation includes spaces, which we remove.\n    # The default str() for booleans (e.g., 'True', 'False') matches the example format.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2521298"}]}