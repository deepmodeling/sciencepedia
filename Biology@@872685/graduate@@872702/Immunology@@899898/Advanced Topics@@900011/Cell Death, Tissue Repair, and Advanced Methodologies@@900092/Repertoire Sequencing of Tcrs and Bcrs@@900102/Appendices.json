{"hands_on_practices": [{"introduction": "To truly understand an immune repertoire, we must first model the fundamental biological process that creates it: V(D)J recombination. The generation probability, or $P_{\\text{gen}}(s)$, quantifies the likelihood that a specific T-cell or B-cell receptor sequence $s$ is produced by this complex stochastic process. This hands-on practice challenges you to implement a dynamic programming algorithm to calculate $P_{\\text{gen}}(s)$, providing a deep, quantitative understanding of how recombination events shape the landscape of observable receptor sequences [@problem_id:2886872]. Mastering this technique is essential for distinguishing between public clonotypes, which are common due to a high intrinsic probability of generation, and rare, private clonotypes that may be central to a specific immune response.", "problem": "You are given a probabilistic generative model for Complementarity Determining Region 3 (CDR3) nucleotide sequences produced by Variable Diversity Joining (VDJ) recombination in lymphocyte antigen receptors. A Complementarity Determining Region 3 (CDR3) sequence for a T cell receptor (TCR) beta chain or B cell receptor (BCR) heavy chain is modeled as a concatenation of a truncated Variable (V) gene segment, an insertion at the V–D junction, a truncated Diversity (D) gene segment, an insertion at the D–J junction, and a truncated Joining (J) gene segment. The goal is to compute the generation probability, denoted $P_{\\text{gen}}(s)$, of an observed CDR3 nucleotide sequence $s$ by summing over all hidden recombination scenarios that produce $s$ under the model. The total probability is computed by marginalization over all latent choices using the law of total probability and independence assumptions specified below. The task is to implement a dynamic programming algorithm that performs this summation exactly and efficiently for a small, fixed model and a given set of observed sequences.\n\nFundamental base and assumptions:\n- The process follows core immunological definitions of VDJ recombination: a V gene is chosen, a D gene is chosen, and a J gene is chosen; nucleotides at gene ends may be deleted by exonuclease trimming; and non-templated insertions are added at the V–D and D–J junctions by terminal deoxynucleotidyl transferase. Choices are independent except where constrained by sequence concatenation.\n- Let the alphabet be $\\{ \\text{A}, \\text{C}, \\text{G}, \\text{T} \\}$. All nucleotide emission probabilities at insertions are independent and identically distributed according to fixed base probabilities.\n- The probability factors into independent components: gene choices, independent deletions at V and J, independent $5'$ and $3'$ deletions at D, and independent insertions at the two junctions.\n- The generation probability of $s$ is the sum over all segmentations of $s$ into V, N1, D, N2, and J contributions that are consistent with some choice of genes and deletions.\n\nModel parameters to be used in all computations:\n- V genes: V1 = \"ATG\", V2 = \"ACG\".\n- D genes: D1 = \"GG\", D2 = \"GA\".\n- J genes: J1 = \"TAC\", J2 = \"CAC\".\n- Gene choice probabilities are uniform: $P(V=\\text{V1}) = P(V=\\text{V2}) = 1/2$, $P(D=\\text{D1}) = P(D=\\text{D2}) = 1/2$, $P(J=\\text{J1}) = P(J=\\text{J2}) = 1/2$.\n- V deletions: delete $d_V \\in \\{0,\\dots,\\ell_V\\}$ nucleotides from the $3'$ end of the chosen V gene of length $\\ell_V$, with $P(d_V \\mid V) \\propto \\rho_V^{d_V}$, where $\\rho_V = 0.4$ and probabilities are normalized over the allowed range.\n- J deletions: delete $d_J \\in \\{0,\\dots,\\ell_J\\}$ nucleotides from the $5'$ end of the chosen J gene of length $\\ell_J$, with $P(d_J \\mid J) \\propto \\rho_J^{d_J}$, where $\\rho_J = 0.4$ and probabilities are normalized over the allowed range.\n- D deletions: delete $d_{D5} \\in \\{0,\\dots,\\ell_D\\}$ from the $5'$ end and $d_{D3} \\in \\{0,\\dots,\\ell_D\\}$ from the $3'$ end of the chosen D gene of length $\\ell_D$, with the constraint $d_{D5} + d_{D3} \\le \\ell_D$. The joint deletion probability factors as $P(d_{D5}, d_{D3} \\mid D) \\propto \\rho_{D5}^{d_{D5}} \\rho_{D3}^{d_{D3}}$ with $\\rho_{D5} = 0.5$ and $\\rho_{D3} = 0.5$, normalized over all allowed pairs.\n- Insertions at V–D (N1) and D–J (N2) are independent. For an insertion of length $L \\in \\{0,1,2,\\dots\\}$, $P(L) = (1-\\lambda)\\lambda^{L}$ with $\\lambda = 0.5$. Given $L$, the inserted nucleotides are independent and identically distributed with base probabilities $P(\\text{A}) = 0.25$, $P(\\text{C}) = 0.25$, $P(\\text{G}) = 0.25$, $P(\\text{T}) = 0.25$.\n\nFormalization of sequence generation:\n- After deleting $d_V$ from the $3'$ end of a V gene of length $\\ell_V$, the V contribution equals the V prefix of length $\\ell_V - d_V$. After deleting $d_J$ from the $5'$ end of a J gene of length $\\ell_J$, the J contribution equals the J suffix of length $\\ell_J - d_J$. After deleting $d_{D5}$ and $d_{D3}$ from a D gene of length $\\ell_D$, the D contribution equals the D substring $D[d_{D5} : \\ell_D - d_{D3}]$ of length $\\ell_D - d_{D5} - d_{D3}$. The observed sequence $s$ must equal the concatenation V-prefix, N1, D-substring, N2, J-suffix.\n\nYour task:\n- Derive and implement a dynamic programming algorithm that computes $P_{\\text{gen}}(s)$ by summing over all consistent segmentations of $s$ into indices $(i,j,k,l)$ satisfying $0 \\le i \\le j \\le k \\le l \\le L$, where $L$ is the length of $s$, such that $s[0:i]$ is produced by some V truncation, $s[i:j]$ is an insertion at V–D, $s[j:k]$ is produced by some D truncation, $s[k:l]$ is an insertion at D–J, and $s[l:L]$ is produced by some J truncation. Precompute match tables for V prefixes, D substrings, and J suffixes, and insertion emission probabilities, to avoid exponential enumeration of scenarios.\n- Use exact string matching to enforce gene-derived segment compatibility; if a gene truncation does not match the corresponding substring of $s$, its contribution is zero. Sum over all genes and valid deletion values per segment.\n- Your program must compute results for the following test suite of observed sequences, in this exact order:\n  - Case $1$: $s = \\text{\"ACGGT\"}$.\n  - Case $2$: $s = \\text{\"ATGGGAT\"}$.\n  - Case $3$: $s = \\text{\"\"}$ (the empty string of length $0$).\n  - Case $4$: $s = \\text{\"G\"}$.\n- The required final output format is a single line containing a Python-style list of the four floating-point probabilities, in order, with no additional text, for example $[\\text{p1},\\text{p2},\\text{p3},\\text{p4}]$.\n\nConstraints and notes:\n- Your implementation must be a complete, runnable program that takes no input and prints the output in the specified single-line format.\n- All probabilities must be computed exactly under the model described above, using the law of total probability and independence assumptions. No approximations are allowed.\n- Design for coverage: ensure that your algorithm correctly handles the boundary condition $L=0$, the case with zero insertions, and cases with nonzero insertions.", "solution": "The problem requires the computation of the generation probability, $P_{\\text{gen}}(s)$, of a given nucleotide sequence $s$ under a probabilistic model of VDJ recombination. The total probability is the sum over all possible latent scenarios (gene choices, deletions, and insertions) that can produce the sequence $s$. A brute-force summation over all scenarios is computationally infeasible. Therefore, a dynamic programming approach is necessary for an efficient and exact computation.\n\nLet the observed sequence be $s$ of length $L$. The model assumes $s$ is a concatenation of five segments: a truncated V-gene prefix, a non-templated insertion (N1), a truncated D-gene substring, a second non-templated insertion (N2), and a truncated J-gene suffix.\n$$ s = s[0:i] \\cdot s[i:j] \\cdot s[j:k] \\cdot s[k:l] \\cdot s[l:L] $$\nwhere $0 \\le i \\le j \\le k \\le l \\le L$ are the split points.\n\nThe total probability $P_{\\text{gen}}(s)$ is obtained by marginalizing over all gene choices, deletion patterns, and insertion events, consistent with the observed sequence $s$. Due to the independence assumptions in the model, the probability of a specific scenario that generates $s$ factors into the probabilities of each independent choice. Summing over all scenarios can be formulated as a sum over all possible segmentation choices $(i, j, k, l)$:\n$$ P_{\\text{gen}}(s) = \\sum_{0 \\le i \\le j \\le k \\le l \\le L} P_{V}(s[0:i]) \\cdot P_{N1}(s[i:j]) \\cdot P_{D}(s[j:k]) \\cdot P_{N2}(s[k:l]) \\cdot P_{J}(s[l:L]) $$\nwhere each term represents the total probability of the respective substring being generated by the corresponding process (V-gene contribution, N1 insertion, etc.).\n\nThis structure is amenable to dynamic programming. We define a forward algorithm that computes the probability of generating prefixes of $s$ at each stage of the V-D-J model. Let $L$ be the length of $s$. We define the following DP arrays of size $L+1$:\n1.  $F_V[i]$: The probability that the prefix $s[0:i]$ is generated from a V-gene segment.\n2.  $F_{VN1}[j]$: The probability that the prefix $s[0:j]$ is generated from a V-gene segment followed by an N1 insertion.\n3.  $F_{VN1D}[k]$: The probability that the prefix $s[0:k]$ is generated from a V-N1-D sequence.\n4.  $F_{VN1DN2}[l]$: The probability that the prefix $s[0:l]$ is generated from a V-N1-D-N2 sequence.\n\nThe final generation probability is then the sum of probabilities of all scenarios ending with a J-segment that completes the full sequence $s$.\n$$ P_{\\text{gen}}(s) = \\sum_{l=0}^{L} F_{VN1DN2}[l] \\cdot P_J(s[l:L]) $$\n\nBefore stating the DP recurrences, we must precompute the probabilities for the elementary events based on the provided model parameters.\n\n**Model Parameter Precomputation:**\nLet $\\rho_V = 0.4$, $\\rho_J = 0.4$, $\\rho_{D5}=0.5$, $\\rho_{D3}=0.5$, and $\\lambda=0.5$.\nThe V/J genes have length $\\ell_V = \\ell_J = 3$. The D genes have length $\\ell_D = 2$.\n\n*   **V/J Deletion Probabilities**: $P(d | G) = \\frac{\\rho_G^d}{Z_G}$ for $d \\in \\{0, \\dots, \\ell_G\\}$.\n    The normalization constant for V and J deletions is $Z_{V/J} = \\sum_{d=0}^{3} (0.4)^d = 1 + 0.4 + 0.16 + 0.064 = 1.624$.\n*   **D Deletion Probabilities**: $P(d_{D5}, d_{D3} | D) = \\frac{\\rho_{D5}^{d_{D5}} \\rho_{D3}^{d_{D3}}}{Z_D}$ for $d_{D5}+d_{D3} \\le \\ell_D = 2$.\n    The normalization constant is $Z_D = \\sum_{d_{D5}+d_{D3} \\le 2} (0.5)^{d_{D5}} (0.5)^{d_{D3}} = \\sum_{k=0}^{2} (k+1)(0.5)^k = 1 \\cdot 1 + 2 \\cdot 0.5 + 3 \\cdot 0.25 = 1 + 1 + 0.75 = 2.75$.\n*   **Insertion Probability**: The probability of inserting a specific nucleotide sequence of length $n$ is $P_{\\text{ins\\_seq}}(n) = P(\\text{length}=n) \\cdot P(\\text{sequence}|\\text{length}=n) = \\left((1-\\lambda)\\lambda^n\\right) \\cdot (0.25)^n = (1-\\lambda)(\\lambda/4)^n = 0.5 \\cdot (0.125)^n$. We precompute an array $P_{\\text{ins}}[n] = 0.5 \\cdot (0.125)^n$.\n\n**Match Probability Precomputation:**\nFor a given sequence $s$ of length $L$, we precompute tables for the probabilities that its substrings match gene segments.\n*   **V-match table $P_V^{\\text{match}}[i]$**: The probability that $s[0:i]$ is a valid V-prefix, summed over all V-genes. For each V-gene, a match occurs if its prefix of length $i$ equals $s[0:i]$. The number of deletions is then fixed at $d_V = \\ell_V - i$.\n*   **D-match table $P_D^{\\text{match}}[j][k]$**: The probability that $s[j:k]$ is a valid D-substring, summed over all D-genes and all valid deletion pairs $(d_{D5}, d_{D3})$ that result in the substring $s[j:k]$.\n*   **J-match table $P_J^{\\text{match}}[l]$**: The probability that $s[l:L]$ is a valid J-suffix, summed over all J-genes. For each J-gene, a match occurs if its suffix of length $L-l$ equals $s[l:L]$. The number of deletions is then fixed at $d_J = \\ell_J - (L-l)$.\n\n**Dynamic Programming Recurrences:**\n1.  **V-segment contribution**: The initial DP array is the V-match table itself.\n    $$ F_V[i] = P_V^{\\text{match}}[i] \\quad \\text{for } i \\in [0, L] $$\n\n2.  **N1-insertion contribution**: We convolve $F_V$ with the insertion probability distribution.\n    $$ F_{VN1}[j] = \\sum_{i=0}^{j} F_V[i] \\cdot P_{\\text{ins}}[j-i] \\quad \\text{for } j \\in [0, L] $$\n\n3.  **D-segment contribution**: We sum over all possible start positions $j$ of the D-segment.\n    $$ F_{VN1D}[k] = \\sum_{j=0}^{k} F_{VN1}[j] \\cdot P_D^{\\text{match}}[j][k] \\quad \\text{for } k \\in [0, L] $$\n\n4.  **N2-insertion contribution**: We perform a second convolution.\n    $$ F_{VN1DN2}[l] = \\sum_{k=0}^{l} F_{VN1D}[k] \\cdot P_{\\text{ins}}[l-k] \\quad \\text{for } l \\in [0, L] $$\n\n5.  **J-segment and final probability**: We sum over all possible start positions $l$ of the J-segment.\n    $$ P_{\\text{gen}}(s) = \\sum_{l=0}^{L} F_{VN1DN2}[l] \\cdot P_J^{\\text{match}}[l] $$\n\nThis DP formulation correctly and efficiently sums over all valid scenarios. The convolutions can be implemented using `numpy.convolve`, and the sums for D and J contributions can be expressed as matrix-vector products, leading to a concise and efficient implementation. The special case of an empty sequence ($L=0$) is handled correctly by this framework, where all indices are $0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the VDJ generation probability problem for the given test cases.\n    \"\"\"\n    \n    # Model parameters as specified in the problem statement\n    model_params = {\n        'v_genes': {'V1': 'ATG', 'V2': 'ACG'},\n        'd_genes': {'D1': 'GG', 'D2': 'GA'},\n        'j_genes': {'J1': 'TAC', 'J2': 'CAC'},\n        'p_v': {'V1': 0.5, 'V2': 0.5},\n        'p_d': {'D1': 0.5, 'D2': 0.5},\n        'p_j': {'J1': 0.5, 'J2': 0.5},\n        'rho_v': 0.4,\n        'rho_j': 0.4,\n        'rho_d5': 0.5,\n        'rho_d3': 0.5,\n        'lambda_ins': 0.5,\n        'p_base': 0.25,\n    }\n\n    def _precompute_deletion_probs(params):\n        \"\"\"Precomputes deletion probability distributions.\"\"\"\n        \n        # V deletions\n        rho_v, ell_v = params['rho_v'], len(params['v_genes']['V1'])\n        z_v = sum(rho_v**d for d in range(ell_v + 1))\n        p_v_del = {d: rho_v**d / z_v for d in range(ell_v + 1)}\n        \n        # J deletions\n        rho_j, ell_j = params['rho_j'], len(params['j_genes']['J1'])\n        z_j = sum(rho_j**d for d in range(ell_j + 1))\n        p_j_del = {d: rho_j**d / z_j for d in range(ell_j + 1)}\n\n        # D deletions\n        rho_d5, rho_d3 = params['rho_d5'], params['rho_d3']\n        ell_d = len(params['d_genes']['D1'])\n        p_d_del_unnorm = {}\n        z_d = 0.0\n        for d5 in range(ell_d + 1):\n            for d3 in range(ell_d + 1):\n                if d5 + d3 <= ell_d:\n                    prob = rho_d5**d5 * rho_d3**d3\n                    p_d_del_unnorm[(d5, d3)] = prob\n                    z_d += prob\n        p_d_del = {k: v / z_d for k, v in p_d_del_unnorm.items()}\n        \n        params['p_v_del'] = p_v_del\n        params['p_j_del'] = p_j_del\n        params['p_d_del'] = p_d_del\n        \n        # Precompute D substring probabilities\n        d_substring_probs = {}\n        for name, seq in params['d_genes'].items():\n            ell_d = len(seq)\n            for (d5, d3), prob in p_d_del.items():\n                if d5 + d3 <= ell_d:\n                    sub = seq[d5 : ell_d - d3]\n                    # Total probability of a substring from ANY D gene\n                    # P(sub) = sum_D P(D) * P(sub|D)\n                    # Here we calculate P(D)*P(sub|D) and add it up\n                    term_prob = params['p_d'][name] * prob\n                    d_substring_probs[sub] = d_substring_probs.get(sub, 0.0) + term_prob\n        params['d_substring_probs'] = d_substring_probs\n\n    _precompute_deletion_probs(model_params)\n\n    def calculate_pgen(s: str, params: dict) -> float:\n        \"\"\"\n        Calculates the generation probability of a sequence s using dynamic programming.\n        \"\"\"\n        L = len(s)\n\n        # 1. Precompute match tables and insertion probabilities\n        \n        # V-match probability table: P_V_match[i] = P(s[0:i] from V)\n        p_v_match = np.zeros(L + 1)\n        for name, seq in params['v_genes'].items():\n            ell_v = len(seq)\n            for i in range(min(L, ell_v) + 1):\n                d_v = ell_v - i\n                if seq.startswith(s[:i]):\n                    p_v_match[i] += params['p_v'][name] * params['p_v_del'][d_v]\n\n        # D-match probability table: P_D_match[j, k] = P(s[j:k] from D)\n        p_d_match = np.zeros((L + 1, L + 1))\n        d_substring_probs = params['d_substring_probs']\n        for j in range(L + 1):\n            for k in range(j, L + 1):\n                sub = s[j:k]\n                if sub in d_substring_probs:\n                    p_d_match[j, k] = d_substring_probs[sub]\n        \n        # J-match probability table: P_J_match[l] = P(s[l:L] from J)\n        p_j_match = np.zeros(L + 1)\n        for name, seq in params['j_genes'].items():\n            ell_j = len(seq)\n            for l in range(L + 1):\n                suffix_len = L - l\n                if suffix_len <= ell_j:\n                    d_j = ell_j - suffix_len\n                    if seq.endswith(s[l:]):\n                        p_j_match[l] += params['p_j'][name] * params['p_j_del'][d_j]\n        \n        # Insertion probability array: P_ins[n] = P(specific sequence of length n)\n        p_ins = np.zeros(L + 1)\n        lambda_ins, p_base = params['lambda_ins'], params['p_base']\n        for n in range(L + 1):\n            p_ins[n] = (1.0 - lambda_ins) * (lambda_ins * p_base)**n\n\n        # 2. Dynamic Programming steps\n        \n        # F_V[i]: P(s[0:i] from V)\n        f_v = p_v_match\n        \n        # F_VN1[j]: P(s[0:j] from V-N1)\n        f_vn1 = np.convolve(f_v, p_ins)[:L + 1]\n        \n        # F_VN1D[k]: P(s[0:k] from V-N1-D)\n        # This is equivalent to a matrix-vector product: f_vn1 @ p_d_match\n        f_vn1d = np.zeros(L + 1)\n        for k in range(L + 1):\n            for j in range(k + 1):\n                f_vn1d[k] += f_vn1[j] * p_d_match[j, k]\n                \n        # F_VN1DN2[l]: P(s[0:l] from V-N1-D-N2)\n        f_vn1dn2 = np.convolve(f_vn1d, p_ins)[:L + 1]\n\n        # Final probability: sum over all final J contributions\n        # This is an inner product: f_vn1dn2 @ p_j_match\n        p_gen = np.dot(f_vn1dn2, p_j_match)\n        \n        return p_gen\n\n    test_cases = [\n        \"ACGGT\",\n        \"ATGGGAT\",\n        \"\",\n        \"G\"\n    ]\n\n    results = []\n    for s in test_cases:\n        result = calculate_pgen(s, model_params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2886872"}, {"introduction": "While the $P_{\\text{gen}}$ model describes how sequences are created, repertoire sequencing aims to accurately measure their frequencies in a sample. However, PCR amplification and sequencing errors can create artificial diversity, making it difficult to distinguish true, low-frequency clonotypes from noise. This practice addresses a critical data-cleaning step: correcting errors in Unique Molecular Identifiers (UMIs), which are used to tag and count original receptor molecules. You will design and implement a principled clustering algorithm that merges error-derived UMIs into their parent molecules, a foundational skill for obtaining accurate and reliable clonotype abundance data from modern repertoire sequencing experiments [@problem_id:2886900].", "problem": "You are analyzing paired-end repertoire sequencing data of T cell receptors (TCRs) and B cell receptors (BCRs) in which each original complementary DNA molecule is tagged with a Unique Molecular Identifier (UMI). A UMI is a random nucleotide string of fixed length $L$ drawn uniformly from the alphabet $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$. Sequencing introduces per-base substitution errors with probability $p$ that act independently across positions and reads. Each UMI accumulates $n$ reads from amplification and sequencing. Your task is to design and implement a principled, deterministic error-correction method for UMIs based on local clustering within Hamming distance $1$, with constraints to avoid over-merging distinct molecules.\n\nFundamental base:\n- Define the Hamming distance $d_{H}(u,v)$ between two UMIs $u$ and $v$ as the count of positions at which they differ.\n- The UMI space size is $K=4^{L}$.\n- For a given UMI $u$, the number of Hamming-$1$ neighbors is $3L$.\n- Under independent per-base error rate $p$ and for small $p$, the probability that a single read’s UMI is observed as any particular Hamming-$1$ neighbor of the true UMI is approximately $p/3$, and the probability to be any Hamming-$1$ neighbor is approximately $Lp$.\n- For $M$ distinct molecules labeled by UMIs drawn uniformly from $K$, the chance that two distinct true UMIs happen to be Hamming-$1$ neighbors by collision is approximately $M\\cdot(3L)/K$ in the sparse regime.\n\nDesign requirements:\n1) Propose and implement a UMI error-correction rule that clusters UMIs within Hamming distance $1$ while constraining merges to be directional from higher-abundance to lower-abundance UMIs to avoid over-merging. Your method must be purely algorithmic and use only the following parameters: the Hamming radius $d$ (set to $1$) and an abundance ratio threshold $\\alpha>1$. The rule must be:\n   - Directional adjacency: a UMI $u$ can absorb a neighbor $v$ only if $d_{H}(u,v)\\leq d$, $\\mathrm{count}(u)\\geq \\alpha\\cdot \\mathrm{count}(v)$, and $u$ is a current local maximum (i.e., not already absorbed by another UMI).\n   - No transitive over-merging: once a UMI $v$ is absorbed, it cannot act as a parent to absorb its own neighbors, thereby preventing chains that would bridge $d_{H}>1$ indirectly.\n   - Tie-handling: if $\\mathrm{count}(u)=\\mathrm{count}(v)$ or $\\mathrm{count}(u)<\\alpha\\cdot \\mathrm{count}(v)$, do not merge $v$ into $u$.\n   - Deterministic ordering: process UMIs in descending count; break ties lexicographically by the UMI string.\n\n2) Justify, from first principles and the base above, conditions on $\\alpha$ to avoid over-merging distinct true molecules while still correcting error-derived neighbors. Your justification must quantify why error-derived neighbors of a true UMI are expected to be much lower in count than the parent, using $p$, $L$, and read count $n$, while true-to-true collisions at Hamming distance $1$ should rarely satisfy the ratio threshold if both are bona fide molecules.\n\n3) Implement the algorithm and apply it to the following test suite. Each test case is a triple: a list of pairs $(\\mathrm{UMI}, \\mathrm{count})$, a Hamming radius $d$ (which will be $1$ in all cases), and a ratio threshold $\\alpha$.\n   - Test case $1$ (happy path, star-like merging): UMIs and counts\n     [($\\mathrm{AAAAAA}$, $100$), ($\\mathrm{AAAAAT}$, $5$), ($\\mathrm{AAAAAG}$, $3$), ($\\mathrm{AAATAA}$, $1$), ($\\mathrm{CCCCCC}$, $40$), ($\\mathrm{CCCCCG}$, $2$)], with $d=1$, $\\alpha=3$.\n   - Test case $2$ (boundary equal counts, no merge): [($\\mathrm{TTTTTT}$, $10$), ($\\mathrm{TTTTTC}$, $10$)], with $d=1$, $\\alpha=2$.\n   - Test case $3$ (prevent transitive over-merging across $d_{H}=2$): [($\\mathrm{GGGGGG}$, $50$), ($\\mathrm{GGGGGA}$, $15$), ($\\mathrm{GGGGAA}$, $5$)], with $d=1$, $\\alpha=3$.\n   - Test case $4$ (adjacent true molecules with similar counts, avoid merge): [($\\mathrm{ATGCAT}$, $30$), ($\\mathrm{ATGCCT}$, $28$)], with $d=1$, $\\alpha=2$.\n   - Test case $5$ (single UMI, trivial): [($\\mathrm{CACACA}$, $7$)], with $d=1$, $\\alpha=3$.\n   - Test case $6$ (threshold sensitivity where $\\alpha$ enables a merge): [($\\mathrm{GATACA}$, $12$), ($\\mathrm{GATACG}$, $5$)], with $d=1$, $\\alpha=2$.\n\nOutput specification:\n- For each test case, compute the number of corrected UMI groups (i.e., the number of cluster representatives that remain after applying your rule) and report that as an integer.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$r_{1},r_{2},\\dots$]\") in the same order as the test cases.\n\nAngle units are not applicable. No physical units are required in the answer. All numeric results must be integers. The final output must be a single line conforming to the specified format.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n\n- **Domain**: Repertoire sequencing of T cell receptors (TCRs) and B cell receptors (BCRs) using Unique Molecular Identifiers (UMIs).\n- **UMI Definition**: A random nucleotide string of fixed length $L$ from the alphabet $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$.\n- **Error Model**: Independent per-base substitution errors with probability $p$.\n- **Data**: Each UMI is associated with a read count $n$.\n- **Metric**: Hamming distance $d_{H}(u,v)$.\n- **UMI Space size**: $K=4^{L}$.\n- **Hamming-1 Neighbors**: The number of Hamming-1 neighbors for any UMI is $3L$.\n- **Error Probabilities (Approximations)**:\n    - Probability of a read's UMI being observed as a specific Hamming-1 neighbor: $\\approx p/3$.\n    - Probability of a read's UMI being observed as any Hamming-1 neighbor: $\\approx Lp$.\n- **Collision Probability**: The probability of two distinct true UMIs being Hamming-1 neighbors is $\\approx M\\cdot(3L)/K$ for $M$ molecules.\n- **Algorithm Design Requirements**:\n    1.  **Clustering Rule**: UMI $u$ absorbs neighbor $v$ if $d_{H}(u,v)\\leq d$, $\\mathrm{count}(u)\\geq \\alpha\\cdot \\mathrm{count}(v)$, and $u$ is a current local maximum.\n    2.  **Parameters**: Hamming radius $d=1$, abundance ratio threshold $\\alpha>1$.\n    3.  **Constraint**: An absorbed UMI cannot absorb other UMIs (no transitive over-merging).\n    4.  **Tie-Handling**: No merge if counts are equal or do not meet the $\\alpha$ ratio.\n    5.  **Determinism**: Process UMIs in descending order of count, with ties broken lexicographically by the UMI string.\n- **Justification Task**: Justify the choice of $\\alpha$ based on first principles, using $p$, $L$, and $n$.\n- **Implementation Task**: Apply the algorithm to six specified test cases and report the number of final UMI groups for each.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is well-grounded in the field of bioinformatics, specifically in the analysis of next-generation sequencing data with UMIs. The model of sequencing error, the use of Hamming distance, and abundance-based clustering are standard, accepted practices for UMI error correction.\n- **Well-Posed**: The problem is structured with a clear objective. The algorithmic rules are specified precisely, including deterministic ordering for tie-breaking, ensuring a unique solution for any given input. The input data and required output format are explicit.\n- **Objective**: The problem is stated using precise, unambiguous scientific and mathematical language.\n\nThe problem does not violate any of the invalidity criteria. It is a formal, solvable problem based on established scientific and algorithmic principles.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Justification of the Error-Correction Rule\n\nThe core of the method is the directional merging rule, where a high-abundance UMI $u$ absorbs a low-abundance neighbor $v$ if $\\mathrm{count}(u) \\ge \\alpha \\cdot \\mathrm{count}(v)$. The efficacy of this rule rests on the large, predictable disparity between the count of a true UMI and the counts of its neighbors generated by sequencing errors. This can be justified from first principles.\n\nLet $u_0$ be a true UMI that undergoes amplification and sequencing, resulting in $n$ total reads. The probability of a substitution error at any given base is $p$. The length of the UMI is $L$.\n\nThe probability that a single read of $u_0$ is sequenced without any errors is $P(\\text{no error}) = (1-p)^L$. The expected count of the true UMI, $E[C_0]$, is therefore:\n$$E[C_0] = n \\cdot (1-p)^L$$\n\nNow, consider a specific Hamming-1 neighbor of $u_0$, say $v$. For a read of $u_0$ to be observed as $v$, exactly one specific base must be substituted, and the other $L-1$ bases must be correct. The probability of this event for a single read is $P(\\text{specific 1-error}) = p \\cdot (1-p)^{L-1}$. The expected count of this error-derived UMI, $E[C_1]$, is:\n$$E[C_1] = n \\cdot p(1-p)^{L-1}$$\n\nThe ratio of the expected counts of the true UMI to its specific error-derived neighbor is:\n$$ \\frac{E[C_0]}{E[C_1]} = \\frac{n \\cdot (1-p)^L}{n \\cdot p(1-p)^{L-1}} = \\frac{1-p}{p} $$\n\nFor typical sequencing platforms, the error rate $p$ is small, for instance, $p \\approx 10^{-3}$. In this case, the expected count ratio is $\\frac{1-10^{-3}}{10^{-3}} = 999$. This large ratio indicates that an error-derived UMI is expected to have a count many orders of magnitude smaller than its parent. The rule $\\mathrm{count}(u) \\ge \\alpha \\cdot \\mathrm{count}(v)$ is thus highly effective. Setting a small threshold, such as $\\alpha=2$ or $\\alpha=3$, provides a robust criterion to merge genuine errors into their parent while requiring $p < 1/(\\alpha+1)$ (e.g., $p<0.25$ for $\\alpha=3$), a condition that is always met in practice.\n\nConversely, consider two distinct true UMIs, $u_1$ and $u_2$, which happen to be Hamming-1 neighbors by random chance. If they originated from two separate molecules with similar capture and amplification efficiencies, their true read counts $n_1$ and $n_2$ would be of a similar order of magnitude. Their observed counts would be approximately $C_1 \\approx n_1(1-p)^L$ and $C_2 \\approx n_2(1-p)^L$. The ratio of their counts, $C_1/C_2 \\approx n_1/n_2$, would be close to $1$. In this scenario, the condition $\\mathrm{count}(u_1) \\geq \\alpha \\cdot \\mathrm{count}(u_2)$ would very likely fail for $\\alpha > 1$, correctly preventing the erroneous merging of two distinct molecular species. The threshold $\\alpha$ thus serves as a critical parameter to distinguish sequencing noise from biological reality.\n\n### Algorithmic Design and Implementation\n\nThe proposed algorithm is deterministic and implements the specified rules for UMI error correction.\n\n**1. Data Structures:**\n- A dictionary, `counts`, to map each UMI string to its integer count. This provides $O(1)$ average-time lookup.\n- A set, `absorbed_umis`, to store UMIs that have been corrected (i.e., merged into a parent). This allows for efficient checking of a UMI's status.\n\n**2. Initialization and Sorting:**\n- The input list of (UMI, count) pairs is used to populate the `counts` dictionary.\n- A list of all unique UMIs is created. This list is then sorted to enforce a deterministic processing order. The primary sort key is the UMI count in descending order. The secondary sort key is the UMI string in ascending lexicographical order. This ensures that higher-abundance UMIs are always processed first, and any ties are handled consistently.\n\n**3. Iterative Clustering:**\n- The algorithm iterates through the sorted list of UMIs. For each UMI $u$ in the list:\n    - **Check for Absorption**: First, it checks if $u$ is already in the `absorbed_umis` set. If it is, $u$ has already been merged into a higher-count parent, so it is skipped. This step is critical as it enforces the \"no transitive over-merging\" constraint: an absorbed UMI cannot act as a parent itself.\n    - **Identify as Parent**: If $u$ has not been absorbed, it is treated as a potential cluster center (a \"parent\" UMI).\n    - **Find and Absorb Neighbors**: The algorithm then generates all $3L$ possible Hamming-1 neighbors of $u$. For each generated neighbor $v$:\n        a. It checks if $v$ exists in the original `counts` dictionary.\n        b. It checks that $v$ has not already been absorbed into another cluster.\n        c. It verifies the directional abundance rule: $\\mathrm{count}(u) \\geq \\alpha \\cdot \\mathrm{count}(v)$.\n        d. If all three conditions are met, $v$ is deemed an error-derived version of $u$. The UMI $v$ is then added to the `absorbed_umis` set.\n\n**4. Final Tally:**\n- After iterating through all UMIs, the `absorbed_umis` set contains all UMIs that have been corrected. The number of original UMIs is `len(counts)`, and the number of merged UMIs is `len(absorbed_umis)`.\n- The final number of corrected UMI groups is calculated as the initial number of unique UMIs minus the number of absorbed UMIs.\n$$ \\text{Number of Groups} = |\\text{Initial UMIs}| - |\\text{Absorbed UMIs}| $$\n\nThis algorithm correctly implements all design requirements, providing a deterministic, principled, and efficient method for UMI error correction.", "answer": "```python\nimport numpy as np\nfrom scipy import stats # This import adheres to the specification, though not strictly used.\n\ndef solve():\n    \"\"\"\n    Solves the UMI error correction problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test case 1 (happy path, star-like merging)\n        ([(\"AAAAAA\", 100), (\"AAAAAT\", 5), (\"AAAAAG\", 3), (\"AAATAA\", 1), \n          (\"CCCCCC\", 40), (\"CCCCCG\", 2)], 1, 3.0),\n        # Test case 2 (boundary equal counts, no merge)\n        ([(\"TTTTTT\", 10), (\"TTTTTC\", 10)], 1, 2.0),\n        # Test case 3 (prevent transitive over-merging across dH=2)\n        ([(\"GGGGGG\", 50), (\"GGGGGA\", 15), (\"GGGGAA\", 5)], 1, 3.0),\n        # Test case 4 (adjacent true molecules with similar counts, avoid merge)\n        ([(\"ATGCAT\", 30), (\"ATGCCT\", 28)], 1, 2.0),\n        # Test case 5 (single UMI, trivial)\n        ([(\"CACACA\", 7)], 1, 3.0),\n        # Test case 6 (threshold sensitivity where alpha enables a merge)\n        ([(\"GATACA\", 12), (\"GATACG\", 5)], 1, 2.0)\n    ]\n\n    results = []\n    for umis_with_counts, d, alpha in test_cases:\n        result = count_corrected_umis(umis_with_counts, d, alpha)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef hamming_distance(s1: str, s2: str) -> int:\n    \"\"\"Calculates the Hamming distance between two strings of equal length.\"\"\"\n    if len(s1) != len(s2):\n        raise ValueError(\"Strings must be of equal length.\")\n    return sum(c1 != c2 for c1, c2 in zip(s1, s2))\n\ndef generate_hamming1_neighbors(umi: str) -> set:\n    \"\"\"Generates all Hamming distance 1 neighbors for a given UMI.\"\"\"\n    neighbors = set()\n    alphabet = {'A', 'C', 'G', 'T'}\n    umi_list = list(umi)\n    for i in range(len(umi_list)):\n        original_char = umi_list[i]\n        for char in alphabet:\n            if char != original_char:\n                umi_list[i] = char\n                neighbors.add(\"\".join(umi_list))\n        umi_list[i] = original_char  # Restore original character\n    return neighbors\n\ndef count_corrected_umis(umis_with_counts: list[tuple[str, int]], d: int, alpha: float) -> int:\n    \"\"\"\n    Implements the directional UMI error correction algorithm.\n    The Hamming radius 'd' is included for generality, but the logic is fixed for d=1 as per problem.\n    \"\"\"\n    if not umis_with_counts:\n        return 0\n    if d != 1:\n        # The problem statement fixes d=1 and the neighbor generation is hardcoded for it.\n        # This check ensures compliance, although test cases all use d=1.\n        raise ValueError(\"This implementation is designed for Hamming radius d=1 only.\")\n\n    # 1. Store UMI counts in a dictionary for efficient lookup.\n    counts = {umi: count for umi, count in umis_with_counts}\n    \n    # 2. Create a list of UMIs sorted by count (desc) and then by UMI string (asc).\n    # This ensures deterministic processing.\n    sorted_umis = sorted(counts.keys(), key=lambda umi: (-counts[umi], umi))\n    \n    # 3. Initialize a set to track UMIs that have been absorbed into a parent cluster.\n    absorbed_umis = set()\n    \n    # 4. Iterate through the sorted UMIs to find cluster centers and absorb neighbors.\n    for umi_parent in sorted_umis:\n        # If a UMI has already been absorbed, it cannot be a parent.\n        # This prevents transitive merging.\n        if umi_parent in absorbed_umis:\n            continue\n            \n        # This UMI is a potential parent/cluster center.\n        # Generate its Hamming-1 neighbors and check for absorption.\n        neighbors = generate_hamming1_neighbors(umi_parent)\n        \n        for umi_child in neighbors:\n            # A child must exist in the original set and not be already absorbed.\n            if umi_child in counts and umi_child not in absorbed_umis:\n                # Check the directional adjacency rule.\n                if counts[umi_parent] >= alpha * counts[umi_child]:\n                    absorbed_umis.add(umi_child)\n                    \n    # 5. The number of corrected groups is the total unique UMIs minus the absorbed ones.\n    num_initial_umis = len(counts)\n    num_absorbed = len(absorbed_umis)\n    \n    return num_initial_umis - num_absorbed\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2886900"}, {"introduction": "With a robust generative model and accurately processed data, we can begin to test specific biological hypotheses. In B-cell repertoires, a key signal of affinity maturation is Somatic Hypermutation (SHM), where the enzyme AID introduces mutations at specific sequence motifs. This final practice guides you through the development of a statistical test to distinguish the non-random pattern of SHM from the background noise of residual sequencing errors [@problem_id:2886899]. By leveraging the known context-dependency of AID, you will implement a likelihood ratio test, demonstrating how to extract a clear biological signal from high-throughput sequencing data and perform rigorous hypothesis-driven analysis.", "problem": "You are tasked with designing and implementing a statistical hypothesis test to distinguish Somatic Hypermutation (SHM) from Polymerase Chain Reaction (PCR) or sequencing errors in immunoglobulin repertoire sequencing using Unique Molecular Identifier (UMI)-based consensus and context-dependent enrichment at Activation-Induced Cytidine Deaminase (AID) hotspots. Your solution must be a complete, runnable program that computes a p-value for each provided dataset and applies a decision rule.\n\nFundamental bases and core definitions:\n\n- Activation-Induced Cytidine Deaminase (AID) generates SHM enriched at sequence motifs known as hotspots, specifically the trinucleotide motifs WRC on the coding strand (where W is A or T, R is A or G, and the central base is C) and its reverse complement GYW on the opposite strand. In explicit forward-strand conditions, a position at index $i$ is an AID hotspot if either $s[i] = \\text{C}$ and $s[i-1] \\in \\{\\text{A},\\text{T}\\}$ and $s[i+1] \\in \\{\\text{A},\\text{G}\\}$ (WRC) or $s[i] = \\text{G}$ and $s[i-1] \\in \\{\\text{C},\\text{T}\\}$ and $s[i+1] \\in \\{\\text{A},\\text{T}\\}$ (GYW).\n- Unique Molecular Identifier (UMI)-based consensus collapses PCR duplicates by grouping reads derived from the same starting molecule; a consensus base call is accepted for a UMI family only when it exceeds a predetermined fraction threshold $\\tau$ of reads within that family.\n- Under the null hypothesis representing residual sequencing or PCR errors after UMI consensus, the per-opportunity error rate is approximately context-independent. Under the alternative hypothesis representing SHM, substitutions are enriched at AID hotspots, producing a higher rate at hotspots than at non-hotspots.\n\nStatistical model and test:\n\n- For each dataset, compute the number of opportunities (exposures) and the number of observed substitutions at hotspot and non-hotspot positions as follows. Let $E_h$ be the total number of UMI-family consensus opportunities at hotspot positions and $E_n$ be the total number at non-hotspot positions. Let $k_h$ and $k_n$ be the corresponding counts of observed consensus substitutions (consensus base different from germline reference) in these two context classes.\n- Model counts as independent Poisson random variables with means proportional to exposures. Under the null hypothesis $\\mathcal{H}_0$, a single rate $r$ applies to both classes: $k_h \\sim \\text{Poisson}(r E_h)$ and $k_n \\sim \\text{Poisson}(r E_n)$. Under the alternative hypothesis $\\mathcal{H}_1$, allow separate rates: $k_h \\sim \\text{Poisson}(r_h E_h)$ and $k_n \\sim \\text{Poisson}(r_n E_n)$ with $r_h \\neq r_n$ and biologically $r_h > r_n$. Use the likelihood ratio test statistic\n$$\n\\Lambda \\equiv -2 \\log\\left(\\frac{L_0}{L_1}\\right) = 2\\left[\\ell_1 - \\ell_0\\right],\n$$\nwhere $\\ell_0$ and $\\ell_1$ are the maximized log-likelihoods under $\\mathcal{H}_0$ and $\\mathcal{H}_1$. The maximum likelihood estimates are $ \\hat r = \\frac{k_h + k_n}{E_h + E_n}$ under $\\mathcal{H}_0$ and $\\hat r_h = \\frac{k_h}{E_h}$, $\\hat r_n = \\frac{k_n}{E_n}$ under $\\mathcal{H}_1$. Using the Poisson log-likelihoods and dropping $k!$ terms that cancel in the ratio, the statistic simplifies to\n$$\n\\Lambda = 2\\left[ k_h \\log\\left(\\frac{k_h}{\\hat r E_h}\\right) + k_n \\log\\left(\\frac{k_n}{\\hat r E_n}\\right) - \\left(k_h + k_n - \\hat r (E_h + E_n)\\right) \\right],\n$$\nwith the conventions that $0 \\log(0/x) \\equiv 0$ and if $k_h + k_n = 0$ then $\\Lambda = 0$. Under standard regularity conditions, $\\Lambda$ is approximately $\\chi^2$ distributed with $1$ degree of freedom; thus the p-value is $p = 1 - F_{\\chi^2_1}(\\Lambda)$.\n- Decision rule: reject $\\mathcal{H}_0$ in favor of enrichment consistent with SHM if $p < \\alpha$ with $\\alpha = 0.01$.\n\nUMI consensus calling:\n\n- For each UMI family at a given position, compute base counts for $\\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$. Let the family size be $n$. A consensus base call is accepted if the maximum count $m$ is unique and $\\frac{m}{n} \\ge \\tau$. If no consensus is accepted, the family is excluded from exposure counts.\n- Exposures are counted as the number of accepted-consensus UMI families per position. Substitutions are counted when the accepted consensus base differs from the germline reference base at that position.\n\nAngle units are not applicable. There are no physical units. All real-valued outputs must be rounded to $6$ decimal places. All decisions are booleans.\n\nYour program must implement the above and evaluate the following test suite. The UMI consensus threshold is $\\tau = 0.7$ for all cases. Each test case is defined by a germline reference sequence and, for each position, a list of UMI families represented as strings of base calls, one character per read. For simplicity, every UMI family here has $5$ reads.\n\nTest suite:\n\n- Case $1$ (enriched SHM-like pattern):\n  - Reference: \"ATCAGTCGTACG\"\n  - UMI families by position index $0$ to $11$:\n    - $0$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $1$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $2$: [\"TTTTT\",\"TTTTT\",\"CCCCC\"]\n    - $3$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $4$: [\"GGGGG\",\"GGGGG\",\"GGGGG\"]\n    - $5$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $6$: [\"TTTTT\",\"TTTTT\",\"CCCCC\"]\n    - $7$: [\"AAAAA\",\"AAAAA\",\"GGGGG\"]\n    - $8$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $9$: [\"AAAAA\",\"GGGGG\",\"AAAAA\"]\n    - $10$: [\"TTTTT\",\"TTTTT\",\"CCCCC\"]\n    - $11$: [\"GGGGG\",\"GGGGG\",\"GGGGG\"]\n- Case $2$ (null-like, errors distributed similarly in both contexts):\n  - Reference: \"ATCAGTCGTACG\"\n  - UMI families by position:\n    - $0$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $1$: [\"TTTTT\",\"TTTTT\",\"GGGGG\"]\n    - $2$: [\"CCCCC\",\"TTTTT\",\"CCCCC\"]\n    - $3$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $4$: [\"GGGGG\",\"GGGGG\",\"AAAAA\"]\n    - $5$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $6$: [\"CCCCC\",\"CCCCC\",\"CCCCC\"]\n    - $7$: [\"GGGGG\",\"GGGGG\",\"AAAAA\"]\n    - $8$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $9$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $10$: [\"CCCCC\",\"CCCCC\",\"CCCCC\"]\n    - $11$: [\"GGGGG\",\"GGGGG\",\"GGGGG\"]\n- Case $3$ (no detected substitutions):\n  - Reference: \"ATCAGTCGTACG\"\n  - UMI families by position:\n    - $0$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $1$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $2$: [\"CCCCC\",\"CCCCC\",\"CCCCC\"]\n    - $3$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $4$: [\"GGGGG\",\"GGGGG\",\"GGGGG\"]\n    - $5$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $6$: [\"CCCCC\",\"CCCCC\",\"CCCCC\"]\n    - $7$: [\"GGGGG\",\"GGGGG\",\"GGGGG\"]\n    - $8$: [\"TTTTT\",\"TTTTT\",\"TTTTT\"]\n    - $9$: [\"AAAAA\",\"AAAAA\",\"AAAAA\"]\n    - $10$: [\"CCCCC\",\"CCCCC\",\"CCCCC\"]\n    - $11$: [\"GGGGG\",\"GGGGG\",\"GGGGG\"]\n\nFinal output format requirements:\n\n- For each case, output a two-element list $[p, d]$ where $p$ is the p-value rounded to $6$ decimal places and $d$ is a boolean indicating whether $\\mathcal{H}_0$ is rejected at $\\alpha = 0.01$.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, with no spaces, for example: \"[[p1,d1],[p2,d2],[p3,d3]]\".", "solution": "The problem requires the design and implementation of a statistical hypothesis test to determine if observed substitutions in immunoglobulin sequences are due to Somatic Hypermutation (SHM) or are consistent with random sequencing errors. The method relies on the enrichment of SHM at specific DNA motifs (AID hotspots) and leverages Unique Molecular Identifiers (UMIs) to reduce noise from PCR and sequencing artifacts. The solution is executed by implementing the specified procedure, from data processing to statistical inference.\n\nThe overall workflow is structured as follows:\nFirst, for a given germline reference sequence, we identify all positions that qualify as Activation-Induced Cytidine Deaminase (AID) hotspots based on the provided trinucleotide context rules.\nSecond, we process the UMI-based sequencing data. For each UMI family at each position, we apply a consensus calling rule to determine the most likely base. A consensus is accepted only if the dominant base appears in a fraction of reads exceeding a threshold $\\tau$.\nThird, we aggregate counts across all positions and UMI families. We tabulate the total number of opportunities (exposures) and the number of observed substitutions, separately for hotspot and non-hotspot positions. This yields four crucial values: $k_h$ (substitutions at hotspots), $E_h$ (exposures at hotspots), $k_n$ (substitutions at non-hotspots), and $E_n$ (exposures at non-hotspots).\nFourth, we perform a likelihood ratio test based on a Poisson model of substitution counts. The null hypothesis, $\\mathcal{H}_0$, posits a single, context-independent substitution rate $r$, while the alternative hypothesis, $\\mathcal{H}_1$, allows for different rates, $r_h$ and $r_n$, for hotspot and non-hotspot contexts.\nFinally, we calculate the p-value and apply the specified decision rule.\n\nHere is a detailed breakdown of each step:\n\n**1. AID Hotspot Identification**\nThe biological premise is that the AID enzyme preferentially introduces mutations at specific sequence motifs. The problem defines these hotspots on the forward strand by the trinucleotide contexts $WRC$ (where $W \\in \\{A, T\\}$, $R \\in \\{A, G\\}$, and the central mutated base is $C$) and its reverse complement $GYW$ (where $Y \\in \\{C, T\\}$, $W \\in \\{A, T\\}$, and the central mutated base is $G$). For a reference sequence $s$, a position $i$ is a hotspot if it meets one of these conditions:\n- $s[i] = \\text{'C'}$, $s[i-1] \\in \\{\\text{'A'}, \\text{'T'}\\}$, and $s[i+1] \\in \\{\\text{'A'}, \\text{'G'}\\}$.\n- $s[i] = \\text{'G'}$, $s[i-1] \\in \\{\\text{'C'}, \\text{'T'}\\}$, and $s[i+1] \\in \\{\\text{'A'}, \\text{'T'}\\}$.\nThis definition requires inspecting the immediate neighbors of a position, so the first ($i=0$) and last ($i=N-1$) bases of a sequence of length $N$ cannot be identified as hotspots. A boolean mask is generated to classify each position of the reference sequence as either a hotspot or a non-hotspot.\n\n**2. UMI Consensus and Data Aggregation**\nUMIs allow for the computational bundling of sequencing reads originating from a single initial molecule. This helps distinguish true biological variation from stochastic PCR and sequencing errors. For a given UMI family of size $n$, a consensus base is called if a single base appears with a count $m$ that is both unique and satisfies the condition $m/n \\ge \\tau$. For this problem, $n=5$ and $\\tau=0.7$, which implies a consensus requires a base to be present in at least $m \\ge 3.5$, i.e., $m=4$ or $m=5$ reads. If a UMI family fails to produce a consensus, it is discarded.\n\nFor each position in the reference sequence, we iterate through its associated UMI families:\n- If a valid consensus base is determined, this UMI family contributes one count to the total exposures. Based on the pre-computed hotspot mask, this exposure is added to either $E_h$ or $E_n$.\n- If the consensus base differs from the corresponding base in the germline reference, it is counted as a substitution. This substitution count is added to either $k_h$ or $k_n$, again depending on the context.\n\n**3. The Likelihood Ratio Test**\nThe core of the statistical analysis is the comparison of two models using a likelihood ratio test. The counts are modeled as independent Poisson random variables.\n- **Null Hypothesis ($\\mathcal{H}_0$)**: There is no enrichment at hotspots. Substitutions occur with a single, uniform rate $r$. The expected counts are $\\lambda_h = r E_h$ and $\\lambda_n = r E_n$.\n- **Alternative Hypothesis ($\\mathcal{H}_1$)**: SHM causes enrichment at hotspots. The rates are different, $r_h \\neq r_n$. The expected counts are $\\lambda_h = r_h E_h$ and $\\lambda_n = r_n E_n$.\n\nThe maximum likelihood estimate (MLE) for the rate under $\\mathcal{H}_0$ is the pooled rate across both contexts:\n$$ \\hat{r} = \\frac{k_h + k_n}{E_h + E_n} $$\nUnder $\\mathcal{H}_1$, the MLEs are the individual rates for each context:\n$$ \\hat{r}_h = \\frac{k_h}{E_h} \\quad \\text{and} \\quad \\hat{r}_n = \\frac{k_n}{E_n} $$\nThe likelihood ratio test statistic $\\Lambda$ is given by $\\Lambda = -2 \\log(\\frac{L_0}{L_1})$, where $L_0$ and $L_1$ are the maximized likelihoods of the data under each hypothesis. Using the Poisson log-likelihood function and substituting the MLEs, this simplifies. The problem provides the formula:\n$$ \\Lambda = 2\\left[ k_h \\log\\left(\\frac{k_h}{\\hat{r} E_h}\\right) + k_n \\log\\left(\\frac{k_n}{\\hat{r} E_n}\\right) - \\left(k_h + k_n - \\hat{r} (E_h + E_n)\\right) \\right] $$\nThe term $(k_h + k_n - \\hat{r} (E_h + E_n))$ is identically zero by the definition of $\\hat{r}$. Thus, the calculation simplifies to the G-test statistic for homogeneity of Poisson rates:\n$$ \\Lambda = 2\\left[ k_h \\log\\left(\\frac{k_h}{\\hat{r} E_h}\\right) + k_n \\log\\left(\\frac{k_n}{\\hat{r} E_n}\\right) \\right] $$\nSpecial cases must be handled: by convention, if a count $k_i$ is $0$, the corresponding term $k_i \\log(\\dots)$ is taken as $0$. If there are no substitutions at all ($k_h + k_n = 0$), then $\\hat{r}=0$, and the statistic $\\Lambda$ is defined to be $0$, indicating no evidence against $\\mathcal{H}_0$.\n\n**4. P-value and Decision Rule**\nAccording to Wilks' theorem, under the null hypothesis, the $\\Lambda$ statistic is approximately distributed as a chi-squared ($\\chi^2$) variable. Since the alternative model ($\\mathcal{H}_1$) has one more free parameter ($r_h, r_n$) than the null model ($\\mathcal{H}_0$, with only $r$), the degrees of freedom for the $\\chi^2$ distribution is $1$. The p-value is the probability of observing a test statistic as extreme or more extreme than the one calculated, assuming $\\mathcal{H}_0$ is true:\n$$ p = P(\\chi^2_1 \\ge \\Lambda_{obs}) = 1 - F_{\\chi^2_1}(\\Lambda_{obs}) $$\nwhere $F_{\\chi^2_1}$ is the cumulative distribution function (CDF) of the $\\chi^2$ distribution with $1$ degree of freedom.\nThe final step is to apply the decision rule. If the calculated p-value is less than the significance level $\\alpha = 0.01$, we reject the null hypothesis $\\mathcal{H}_0$. This implies that the observed enrichment of substitutions at hotspots is statistically significant and consistent with the action of SHM rather than random error. Otherwise, we fail to reject $\\mathcal{H}_0$. The program implements this entire procedure and formats the results as requested.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the hypothesis test on the provided test suite.\n    \"\"\"\n\n    test_suite = [\n        {\n            \"name\": \"Case 1 (enriched SHM-like pattern)\",\n            \"reference\": \"ATCAGTCGTACG\",\n            \"umi_data\": {\n                0: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                1: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                2: [\"TTTTT\", \"TTTTT\", \"CCCCC\"],\n                3: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                4: [\"GGGGG\", \"GGGGG\", \"GGGGG\"],\n                5: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                6: [\"TTTTT\", \"TTTTT\", \"CCCCC\"],\n                7: [\"AAAAA\", \"AAAAA\", \"GGGGG\"],\n                8: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                9: [\"AAAAA\", \"GGGGG\", \"AAAAA\"],\n                10: [\"TTTTT\", \"TTTTT\", \"CCCCC\"],\n                11: [\"GGGGG\", \"GGGGG\", \"GGGGG\"],\n            }\n        },\n        {\n            \"name\": \"Case 2 (null-like, errors distributed similarly)\",\n            \"reference\": \"ATCAGTCGTACG\",\n            \"umi_data\": {\n                0: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                1: [\"TTTTT\", \"TTTTT\", \"GGGGG\"],\n                2: [\"CCCCC\", \"TTTTT\", \"CCCCC\"],\n                3: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                4: [\"GGGGG\", \"GGGGG\", \"AAAAA\"],\n                5: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                6: [\"CCCCC\", \"CCCCC\", \"CCCCC\"],\n                7: [\"GGGGG\", \"GGGGG\", \"AAAAA\"],\n                8: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                9: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                10: [\"CCCCC\", \"CCCCC\", \"CCCCC\"],\n                11: [\"GGGGG\", \"GGGGG\", \"GGGGG\"],\n            }\n        },\n        {\n            \"name\": \"Case 3 (no detected substitutions)\",\n            \"reference\": \"ATCAGTCGTACG\",\n            \"umi_data\": {\n                0: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                1: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                2: [\"CCCCC\", \"CCCCC\", \"CCCCC\"],\n                3: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                4: [\"GGGGG\", \"GGGGG\", \"GGGGG\"],\n                5: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                6: [\"CCCCC\", \"CCCCC\", \"CCCCC\"],\n                7: [\"GGGGG\", \"GGGGG\", \"GGGGG\"],\n                8: [\"TTTTT\", \"TTTTT\", \"TTTTT\"],\n                9: [\"AAAAA\", \"AAAAA\", \"AAAAA\"],\n                10: [\"CCCCC\", \"CCCCC\", \"CCCCC\"],\n                11: [\"GGGGG\", \"GGGGG\", \"GGGGG\"],\n            }\n        }\n    ]\n\n    tau = 0.7\n    alpha = 0.01\n\n    def get_hotspots(ref):\n        n = len(ref)\n        is_hotspot = [False] * n\n        for i in range(1, n - 1):\n            if ref[i] == 'C' and ref[i-1] in 'AT' and ref[i+1] in 'AG':  # WRC\n                is_hotspot[i] = True\n            elif ref[i] == 'G' and ref[i-1] in 'CT' and ref[i+1] in 'AT': # GYW\n                is_hotspot[i] = True\n        return is_hotspot\n\n    def get_consensus(umi_family_str, tau_val):\n        n = len(umi_family_str)\n        if n == 0:\n            return None, False\n        \n        counts = {'A': 0, 'C': 0, 'G': 0, 'T': 0}\n        for base in umi_family_str:\n            if base in counts:\n                counts[base] += 1\n        \n        sorted_bases = sorted(counts.items(), key=lambda item: item[1], reverse=True)\n        max_count = sorted_bases[0][1]\n        consensus_base = sorted_bases[0][0]\n        \n        is_max_unique = True\n        if len(sorted_bases) > 1 and sorted_bases[1][1] == max_count:\n            is_max_unique = False\n        \n        if is_max_unique and (max_count / n) >= tau_val:\n            return consensus_base, True\n        else:\n            return None, False\n\n    def process_case(case_data, tau_val):\n        reference = case_data[\"reference\"]\n        umi_data = case_data[\"umi_data\"]\n        \n        is_hotspot = get_hotspots(reference)\n        \n        k_h, E_h, k_n, E_n = 0, 0, 0, 0\n        \n        for i, ref_base in enumerate(reference):\n            if i in umi_data:\n                for umi_family in umi_data[i]:\n                    consensus_base, is_valid = get_consensus(umi_family, tau_val)\n                    if is_valid:\n                        if is_hotspot[i]:\n                            E_h += 1\n                            if consensus_base != ref_base:\n                                k_h += 1\n                        else:\n                            E_n += 1\n                            if consensus_base != ref_base:\n                                k_n += 1\n        return k_h, E_h, k_n, E_n\n\n    results = []\n    for case in test_suite:\n        k_h, E_h, k_n, E_n = process_case(case, tau)\n        \n        total_k = k_h + k_n\n        total_E = E_h + E_n\n        \n        Lambda = 0.0\n        if total_k > 0 and total_E > 0:\n            r_hat = total_k / total_E\n            \n            term_h = 0.0\n            if k_h > 0:\n                expected_h = r_hat * E_h\n                if expected_h > 0:\n                    term_h = k_h * np.log(k_h / expected_h)\n            \n            term_n = 0.0\n            if k_n > 0:\n                expected_n = r_hat * E_n\n                if expected_n > 0:\n                    term_n = k_n * np.log(k_n / expected_n)\n            \n            Lambda = 2 * (term_h + term_n)\n\n        p_value = chi2.sf(Lambda, 1) if Lambda > 0 else 1.0\n        decision = p_value < alpha\n        \n        results.append([round(p_value, 6), decision])\n        \n    formatted_results = []\n    for p, d in results:\n        # Use .6f for consistent rounding and str(d) for boolean representation\n        formatted_results.append(f\"[{p:.6f},{str(d).lower()}]\")\n    \n    final_output = f\"[[{','.join(formatted_results)}]]\".replace('],[', '], [')\n    print(final_output)\n\nsolve()\n```", "id": "2886899"}]}