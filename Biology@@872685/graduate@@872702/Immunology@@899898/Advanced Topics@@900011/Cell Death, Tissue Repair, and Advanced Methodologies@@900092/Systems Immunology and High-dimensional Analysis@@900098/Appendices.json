{"hands_on_practices": [{"introduction": "A foundational step in any systems immunology study is rigorous experimental design. This exercise guides you through a critical calculation for planning single-cell experiments: determining the minimum number of cells required to ensure the detection of a rare cell population. By working from first principles of probability, you will derive a formula that directly links statistical confidence to experimental scale, a crucial consideration for managing the cost and feasibility of high-throughput sequencing projects [@problem_id:2892322].", "problem": "A systems immunology group designs a single-cell RNA sequencing (scRNA-seq) study to detect a rare immune cell population present at true frequency $f \\in (0,1)$ in a large, well-mixed tissue. The experiment will yield exactly $n$ successfully profiled single cells, sampled independently from the tissue such that the binomial model is appropriate. The group will declare successful detection of the rare population if at least one of the $n$ profiled cells belongs to that population. Let $\\alpha \\in (0,1)$ denote a pre-specified tolerance for failing to detect the population when it is present at frequency $f$.\n\nStarting only from the fundamental probability model that independent draws from a large population with success probability $f$ yield a binomial count of successes, derive a closed-form expression for the minimal integer sample size $n^{\\ast}(f,\\alpha)$ that guarantees the probability of detecting the rare population (i.e., observing at least one such cell among the $n$ profiled cells) is at least $1-\\alpha$.\n\nYour final answer should be a single analytic expression for $n^{\\ast}(f,\\alpha)$ in terms of $f$ and $\\alpha$. No numerical evaluation or rounding is required.", "solution": "The appropriate starting point is the binomial model for independent sampling from a large population. Define indicator random variables $X_{i}$ for $i \\in \\{1,\\dots,n\\}$ such that $X_{i}=1$ if the $i$-th profiled cell belongs to the rare population and $X_{i}=0$ otherwise. Under the stated assumptions, the $X_{i}$ are independent and identically distributed with $\\mathbb{P}(X_{i}=1)=f$ and $\\mathbb{P}(X_{i}=0)=1-f$. The total number of rare cells among the $n$ profiled cells is\n$$\nS_{n} \\equiv \\sum_{i=1}^{n} X_{i} \\sim \\mathrm{Binomial}(n,f).\n$$\nThe event of detecting the population corresponds to observing at least one rare cell, that is, the event $\\{S_{n} \\ge 1\\}$. Its probability is\n$$\n\\mathbb{P}(S_{n} \\ge 1) \\;=\\; 1 - \\mathbb{P}(S_{n} = 0) \\;=\\; 1 - (1-f)^{n},\n$$\nwhere the last equality follows because $\\mathbb{P}(S_{n}=0)=\\mathbb{P}(X_{1}=0,\\dots,X_{n}=0)=(1-f)^{n}$ by independence.\n\nWe require this detection probability to be at least $1-\\alpha$, namely\n$$\n1 - (1-f)^{n} \\;\\ge\\; 1 - \\alpha.\n$$\nRearranging yields\n$$\n(1-f)^{n} \\;\\le\\; \\alpha,\n$$\nwith $\\alpha \\in (0,1)$ and $f \\in (0,1)$. Taking natural logarithms of both sides and using that $\\ln(1-f) < 0$ on $(0,1)$ yields\n$$\nn \\,\\ln(1-f) \\;\\le\\; \\ln(\\alpha).\n$$\nSince $\\ln(1-f) < 0$, dividing both sides by $\\ln(1-f)$ reverses the inequality:\n$$\nn \\;\\ge\\; \\frac{\\ln(\\alpha)}{\\ln(1-f)}.\n$$\nThe minimal integer $n^{\\ast}(f,\\alpha)$ satisfying the inequality is thus obtained by taking the ceiling:\n$$\nn^{\\ast}(f,\\alpha) \\;=\\; \\left\\lceil \\frac{\\ln(\\alpha)}{\\ln(1-f)} \\right\\rceil.\n$$\n\nThis is the exact binomial result. For intuition in the rare-event regime where $f$ is small, one may note the approximation $\\ln(1-f) \\approx -f$, which gives $n^{\\ast}(f,\\alpha) \\approx \\left\\lceil \\ln(\\alpha^{-1})/f \\right\\rceil$, but the exact required expression is the one above.", "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(\\alpha)}{\\ln(1 - f)} \\right\\rceil}$$", "id": "2892322"}, {"introduction": "High-dimensional measurements are powerful but can be compromised by technical artifacts that may lead to incorrect biological interpretations. This practice delves into a classic issue in Flow Cytometry: the propagation of errors from fluorescence spillover compensation. You will mathematically model how small inaccuracies in the compensation matrix create false-positive signals of marker co-expression, providing deep insight into data quality and motivating principled strategies for setting analysis gates [@problem_id:2892450].", "problem": "A two-parameter flow cytometry experiment measures two fluorophores with channels indexed by $1$ and $2$ for two markers on single cells. Assume linear optical mixing with a $2 \\times 2$ spillover matrix $S$ with ones on the diagonal and small off-diagonal spillovers $s_{12}$ and $s_{21}$, so that raw observed intensities satisfy $y = S x + \\eta$, where $x \\in \\mathbb{R}^{2}$ is the true marker intensity vector for a single cell, $y \\in \\mathbb{R}^{2}$ is the measured intensity vector, and $\\eta \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$ represents independent Gaussian measurement noise with variance $\\sigma^{2}$ in each channel. Compensation in software uses the inverse of an estimated spillover matrix $\\hat{S}$ to produce $\\hat{x} = \\hat{S}^{-1} y$, where $\\hat{S}$ has ones on the diagonal and off-diagonal elements $\\hat{s}_{12}$ and $\\hat{s}_{21}$. Define the estimation errors $\\epsilon_{12} = \\hat{s}_{12} - s_{12}$ and $\\epsilon_{21} = \\hat{s}_{21} - s_{21}$. Assume $|s_{12}|, |s_{21}|, |\\epsilon_{12}|, |\\epsilon_{21}| \\ll 1$.\n\nConsider a homogeneous single-positive cell population with true intensities $x = (\\mu, 0)^{\\top}$ for some fixed $\\mu > 0$. Cells are called coexpressing if both compensated intensities exceed a common gate threshold $\\tau > 0$, that is, if $\\hat{x}_{1} \\ge \\tau$ and $\\hat{x}_{2} \\ge \\tau$. Assume $\\mu \\gg \\tau$ so that $P(\\hat{x}_{1} \\ge \\tau) \\approx 1$ and the dominant contribution to false coexpression in this population arises from the event $\\hat{x}_{2} \\ge \\tau$.\n\nUsing only the linear mixing model, first-order approximations in $s_{12}, s_{21}, \\epsilon_{12}, \\epsilon_{21}$, and standard properties of Gaussian random variables, do the following:\n\n1. Derive the approximate mean and variance of the compensated intensity $\\hat{x}_{2}$ for a cell with $x = (\\mu, 0)^{\\top}$, to first order in $s_{12}, s_{21}, \\epsilon_{12}, \\epsilon_{21}$.\n\n2. Using your result and the cumulative distribution function $\\Phi$ of the standard normal distribution, derive an analytic approximation for the false coexpression probability $p_{\\mathrm{fp}}(\\mu, \\tau, s_{12}, s_{21}, \\epsilon_{12}, \\epsilon_{21}, \\sigma) \\approx P(\\hat{x}_{2} \\ge \\tau)$.\n\n3. Suggest a correction procedure that adjusts the gate on channel $2$ to achieve a target false coexpression probability $\\alpha \\in (0, 1)$. Let the adjusted gate be $\\tau' = \\tau + \\Delta$. Derive a closed-form symbolic expression for the required gate shift $\\Delta^{\\ast}(\\alpha)$ in terms of $\\mu, \\tau, s_{12}, s_{21}, \\epsilon_{12}, \\epsilon_{21}, \\sigma$ and the quantile function $\\Phi^{-1}$ of the standard normal distribution. Express your final answer as a single symbolic expression for $\\Delta^{\\ast}(\\alpha)$.\n\nNo numerical evaluation is required; provide exact symbolic expressions. If you refer to Flow Cytometry or Fluorescence-Activated Cell Sorting (FACS), spell out the acronym on first use. The final answer must be the single analytic expression for $\\Delta^{\\ast}(\\alpha)$ without units.", "solution": "The compensated intensity vector $\\hat{x}$ is given by:\n$$ \\hat{x} = \\hat{S}^{-1} y = \\hat{S}^{-1} (S x + \\eta) $$\nWe must first compute the product $\\hat{S}^{-1}S$ and the transformed noise term $\\hat{S}^{-1}\\eta$ to first order in the small parameters $s_{12}, s_{21}, \\epsilon_{12}, \\epsilon_{21}$.\n\nThe inverse of the estimated spillover matrix $\\hat{S}$ is:\n$$ \\hat{S}^{-1} = \\frac{1}{1 - \\hat{s}_{12}\\hat{s}_{21}} \\begin{pmatrix} 1 & -\\hat{s}_{12} \\\\ -\\hat{s}_{21} & 1 \\end{pmatrix} $$\nSince $|\\hat{s}_{12}|$ and $|\\hat{s}_{21}|$ are small, the product $\\hat{s}_{12}\\hat{s}_{21}$ is a second-order term. Using the approximation $(1 - z)^{-1} \\approx 1 + z$ for small $z$, we have $\\frac{1}{1 - \\hat{s}_{12}\\hat{s}_{21}} \\approx 1$. Thus, to first order:\n$$ \\hat{S}^{-1} \\approx \\begin{pmatrix} 1 & -\\hat{s}_{12} \\\\ -\\hat{s}_{21} & 1 \\end{pmatrix} $$\nNow we compute the product $\\hat{S}^{-1}S$:\n$$ \\hat{S}^{-1}S \\approx \\begin{pmatrix} 1 & -\\hat{s}_{12} \\\\ -\\hat{s}_{21} & 1 \\end{pmatrix} \\begin{pmatrix} 1 & s_{12} \\\\ s_{21} & 1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\hat{s}_{12}s_{21} & s_{12} - \\hat{s}_{12} \\\\ s_{21} - \\hat{s}_{21} & 1 - \\hat{s}_{21}s_{12} \\end{pmatrix} $$\nUsing the definitions of the errors, $\\epsilon_{12} = \\hat{s}_{12} - s_{12}$ and $\\epsilon_{21} = \\hat{s}_{21} - s_{21}$, we substitute these into the matrix:\n$$ \\hat{S}^{-1}S \\approx \\begin{pmatrix} 1 - (s_{12}+\\epsilon_{12})s_{21} & -\\epsilon_{12} \\\\ -\\epsilon_{21} & 1 - (s_{21}+\\epsilon_{21})s_{12} \\end{pmatrix} $$\nDropping all second-order terms (products of small parameters like $s_{12}s_{21}$, $\\epsilon_{12}s_{21}$, etc.), we obtain the first-order approximation:\n$$ \\hat{S}^{-1}S \\approx \\begin{pmatrix} 1 & -\\epsilon_{12} \\\\ -\\epsilon_{21} & 1 \\end{pmatrix} $$\nThe expression for the compensated intensities becomes:\n$$ \\hat{x} \\approx \\begin{pmatrix} 1 & -\\epsilon_{12} \\\\ -\\epsilon_{21} & 1 \\end{pmatrix} x + \\hat{S}^{-1}\\eta $$\nFor the given single-positive cell population, $x = (\\mu, 0)^{\\top}$:\n$$ \\hat{x} \\approx \\begin{pmatrix} 1 & -\\epsilon_{12} \\\\ -\\epsilon_{21} & 1 \\end{pmatrix} \\begin{pmatrix} \\mu \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 & -\\hat{s}_{12} \\\\ -\\hat{s}_{21} & 1 \\end{pmatrix} \\begin{pmatrix} \\eta_1 \\\\ \\eta_2 \\end{pmatrix} = \\begin{pmatrix} \\mu \\\\ -\\epsilon_{21}\\mu \\end{pmatrix} + \\begin{pmatrix} \\eta_1 - \\hat{s}_{12}\\eta_2 \\\\ -\\hat{s}_{21}\\eta_1 + \\eta_2 \\end{pmatrix} $$\nWe focus on the compensated intensity in channel $2$, $\\hat{x}_2$:\n$$ \\hat{x}_2 \\approx -\\epsilon_{21}\\mu - \\hat{s}_{21}\\eta_1 + \\eta_2 $$\n\n**1. Mean and Variance of $\\hat{x}_2$**\nThe noise components $\\eta_1$ and $\\eta_2$ have zero mean, $E[\\eta_1] = E[\\eta_2] = 0$. The expected value of $\\hat{x}_2$ is:\n$$ E[\\hat{x}_2] \\approx E[-\\epsilon_{21}\\mu - \\hat{s}_{21}\\eta_1 + \\eta_2] = -\\epsilon_{21}\\mu - \\hat{s}_{21}E[\\eta_1] + E[\\eta_2] = -\\epsilon_{21}\\mu $$\nThe variance of $\\hat{x}_2$ is calculated using the independence of $\\eta_1$ and $\\eta_2$, where $\\mathrm{Var}(\\eta_1) = \\mathrm{Var}(\\eta_2) = \\sigma^2$. The term $-\\epsilon_{21}\\mu$ is a constant and does not affect the variance.\n$$ \\mathrm{Var}(\\hat{x}_2) \\approx \\mathrm{Var}(-\\hat{s}_{21}\\eta_1 + \\eta_2) = (-\\hat{s}_{21})^2 \\mathrm{Var}(\\eta_1) + (1)^2 \\mathrm{Var}(\\eta_2) = \\hat{s}_{21}^2 \\sigma^2 + \\sigma^2 = \\sigma^2(1 + \\hat{s}_{21}^2) $$\nSince $|\\hat{s}_{21}|$ is small, $\\hat{s}_{21}^2$ is a second-order term and is neglected. Thus, to first order, the variance is:\n$$ \\mathrm{Var}(\\hat{x}_2) \\approx \\sigma^2 $$\nAs $\\hat{x}_2$ is a linear combination of Gaussian random variables, it is also Gaussian. Therefore, the approximate distribution of $\\hat{x}_2$ is:\n$$ \\hat{x}_2 \\sim \\mathcal{N}(-\\epsilon_{21}\\mu, \\sigma^2) $$\n\n**2. False Coexpression Probability**\nThe false positive probability $p_{\\mathrm{fp}}$ is approximated by $P(\\hat{x}_2 \\ge \\tau)$. To calculate this, we standardize the random variable $\\hat{x}_2$:\n$$ Z = \\frac{\\hat{x}_2 - E[\\hat{x}_2]}{\\sqrt{\\mathrm{Var}(\\hat{x}_2)}} = \\frac{\\hat{x}_2 - (-\\epsilon_{21}\\mu)}{\\sigma} \\sim \\mathcal{N}(0, 1) $$\nThe probability can be expressed in terms of the standard normal variable $Z$:\n$$ P(\\hat{x}_2 \\ge \\tau) = P\\left(\\frac{\\hat{x}_2 + \\epsilon_{21}\\mu}{\\sigma} \\ge \\frac{\\tau + \\epsilon_{21}\\mu}{\\sigma}\\right) = P\\left(Z \\ge \\frac{\\tau + \\epsilon_{21}\\mu}{\\sigma}\\right) $$\nUsing the standard normal cumulative distribution function $\\Phi(z) = P(Z \\le z)$, we know that $P(Z \\ge a) = 1 - \\Phi(a)$. Therefore:\n$$ p_{\\mathrm{fp}}(\\mu, \\tau, s_{12}, s_{21}, \\epsilon_{12}, \\epsilon_{21}, \\sigma) \\approx 1 - \\Phi\\left(\\frac{\\tau + \\epsilon_{21}\\mu}{\\sigma}\\right) $$\n\n**3. Gate Correction Procedure**\nWe wish to find a gate shift $\\Delta$ such that with a new gate $\\tau' = \\tau + \\Delta$, the false positive probability is equal to a target value $\\alpha$. Let this specific shift be $\\Delta^{\\ast}(\\alpha)$. The condition is:\n$$ P(\\hat{x}_2 \\ge \\tau + \\Delta^{\\ast}(\\alpha)) = \\alpha $$\nUsing the result from part 2, this translates to:\n$$ 1 - \\Phi\\left(\\frac{(\\tau + \\Delta^{\\ast}(\\alpha)) + \\epsilon_{21}\\mu}{\\sigma}\\right) = \\alpha $$\nRearranging the equation gives:\n$$ \\Phi\\left(\\frac{\\tau + \\Delta^{\\ast}(\\alpha) + \\epsilon_{21}\\mu}{\\sigma}\\right) = 1 - \\alpha $$\nTo solve for $\\Delta^{\\ast}(\\alpha)$, we apply the inverse CDF, or quantile function, $\\Phi^{-1}$:\n$$ \\frac{\\tau + \\Delta^{\\ast}(\\alpha) + \\epsilon_{21}\\mu}{\\sigma} = \\Phi^{-1}(1 - \\alpha) $$\nNow, we isolate $\\Delta^{\\ast}(\\alpha)$:\n$$ \\tau + \\Delta^{\\ast}(\\alpha) + \\epsilon_{21}\\mu = \\sigma \\Phi^{-1}(1 - \\alpha) $$\n$$ \\Delta^{\\ast}(\\alpha) = \\sigma \\Phi^{-1}(1 - \\alpha) - \\epsilon_{21}\\mu - \\tau $$", "answer": "$$\\boxed{\\sigma \\Phi^{-1}(1 - \\alpha) - \\epsilon_{21}\\mu - \\tau}$$", "id": "2892450"}, {"introduction": "A central challenge in systems immunology is to quantitatively compare the behavior of entire cell populations under different conditions, such as health versus disease. This advanced practice introduces entropic-regularized optimal transport, a powerful computational framework for learning the most likely \"flow\" of cells between two distinct states. You will derive the widely used Sinkhorn algorithm and apply it to a model of T cell activation, equipping you with a state-of-the-art tool to analyze cellular plasticity and treatment responses [@problem_id:2892437].", "problem": "Consider two cell populations measured with single-cell RNA sequencing (scRNA-seq), each summarized as a discrete probability distribution over coarse-grained cell states obtained from a common latent embedding learned by a variational autoencoder. You wish to couple these distributions using entropic-regularized optimal transport to infer a stochastic mapping between conditions while discouraging overly sharp couplings in light of biological stochasticity. Let the source distribution be $a \\in \\mathbb{R}_{+}^{n}$ and the target distribution be $b \\in \\mathbb{R}_{+}^{m}$, with $\\sum_{i=1}^{n} a_{i} = 1$ and $\\sum_{j=1}^{m} b_{j} = 1$. Let $C \\in \\mathbb{R}^{n \\times m}$ be a nonnegative cost matrix derived from transcriptional dissimilarities between states, and let $\\varepsilon > 0$ be the entropic regularization strength.\n\nTask A (formulation): Using only the core definitions of probability distributions and the Kullbackâ€“Leibler Divergence (KLD), define the entropic-regularized optimal transport problem that seeks a coupling matrix $\\Pi \\in \\mathbb{R}_{+}^{n \\times m}$ minimizing transport cost plus entropy penalty subject to marginals $a$ and $b$. Your definition must be written as a constrained optimization problem in $\\Pi$ that depends on $C$, $a$, $b$, and $\\varepsilon$.\n\nTask B (derivation): Starting from your constrained optimization in Task A, perform a Lagrangian analysis and derive the fixed-point scaling equations that characterize the unique optimizer in the strictly positive case. Show how the optimal coupling admits the factorization $\\Pi^{\\star} = \\operatorname{diag}(u)\\,K\\,\\operatorname{diag}(v)$ with a Gibbs kernel $K$ determined by $C$ and $\\varepsilon$, and derive iterative update equations for the scaling vectors $u \\in \\mathbb{R}_{+}^{n}$ and $v \\in \\mathbb{R}_{+}^{m}$ that enforce the marginal constraints. Do not assume specific forms for $a$, $b$, or $C$ beyond positivity and feasibility.\n\nTask C (calculation): Now specialize to a minimal immunological scenario with $n = m = 2$ coarse-grained states representing resting and activated T cell phenotypes under two conditions. Suppose\n$$\na = \\begin{pmatrix} 0.6 \\\\ 0.4 \\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix},\n\\quad\nC = \\begin{pmatrix} 0 & \\ln(2) \\\\ \\ln(2) & 0 \\end{pmatrix},\n\\quad\n\\varepsilon = 1.\n$$\nCompute the $(1,2)$ entry of the optimal coupling $\\,\\Pi^{\\star}\\,$ obtained by the entropic-regularized optimal transport in Tasks A and B, using your derived scaling updates to convergence. Round your final numeric answer to four significant figures. State only the numeric value without units.", "solution": "**Task A: Formulation**\n\nThe classical optimal transport problem seeks to minimize the total transport cost $\\langle C, \\Pi \\rangle = \\sum_{i,j} C_{ij} \\Pi_{ij}$ subject to marginal constraints. Entropic regularization adds a penalty term to this objective, which encourages smoother, less deterministic couplings.\n\nLet $U(a,b)$ denote the transport polytope, which is the set of all matrices $\\Pi \\in \\mathbb{R}_{+}^{n \\times m}$ satisfying the marginal constraints:\n$$\nU(a,b) = \\left\\{ \\Pi \\in \\mathbb{R}_{+}^{n \\times m} \\mid \\sum_{j=1}^{m} \\Pi_{ij} = a_i, \\forall i; \\quad \\sum_{i=1}^{n} \\Pi_{ij} = b_j, \\forall j \\right\\}\n$$\nThe entropic-regularized optimal transport problem is equivalent to finding a coupling $\\Pi \\in U(a,b)$ that is closest, in the sense of Kullback-Leibler (KL) divergence, to a Gibbs kernel matrix $K \\in \\mathbb{R}_{+}^{n \\times m}$ defined as $K_{ij} = \\exp(-C_{ij}/\\varepsilon)$.\n\nThe constrained optimization problem is:\n$$\n\\min_{\\Pi \\in U(a,b)} \\text{KL}(\\Pi || K) = \\min_{\\Pi \\in U(a,b)} \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\Pi_{ij} \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right)\n$$\nExpanding this objective function reveals the connection to the transport cost and entropy:\n$$\n\\sum_{i,j} \\Pi_{ij} \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right) = \\sum_{i,j} \\Pi_{ij} \\ln(\\Pi_{ij}) - \\sum_{i,j} \\Pi_{ij} \\ln(K_{ij}) = \\sum_{i,j} \\Pi_{ij} \\ln(\\Pi_{ij}) + \\frac{1}{\\varepsilon} \\sum_{i,j} C_{ij} \\Pi_{ij}\n$$\nMinimizing this is equivalent to minimizing $\\frac{1}{\\varepsilon}\\langle C, \\Pi \\rangle - H(\\Pi)$, where $H(\\Pi) = -\\sum_{i,j} \\Pi_{ij} \\ln(\\Pi_{ij})$ is the entropy of the coupling. This matches the description of minimizing a \"transport cost plus entropy penalty\".\n\n**Task B: Derivation**\n\nWe derive the structure of the optimal coupling $\\Pi^{\\star}$ using Lagrange multipliers. The objective is $J(\\Pi) = \\text{KL}(\\Pi || K)$. The constraints are the marginals $\\sum_{j} \\Pi_{ij} = a_i$ and $\\sum_{i} \\Pi_{ij} = b_j$. Let $f_i$ and $g_j$ be the Lagrange multipliers. The Lagrangian $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(\\Pi, f, g) = \\sum_{i,j} \\Pi_{ij} \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right) - \\sum_{i=1}^{n} f_i \\left(\\sum_{j=1}^{m} \\Pi_{ij} - a_i\\right) - \\sum_{j=1}^{m} g_j \\left(\\sum_{i=1}^{n} \\Pi_{ij} - b_j\\right)\n$$\nSetting the partial derivative of $\\mathcal{L}$ with respect to $\\Pi_{ij}$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Pi_{ij}} = \\ln\\left(\\frac{\\Pi_{ij}}{K_{ij}}\\right) + 1 - f_i - g_j = 0\n$$\nSolving for $\\Pi_{ij}$:\n$$\n\\Pi_{ij} = K_{ij} \\exp(f_i + g_j - 1)\n$$\nLet us define two scaling vectors $u \\in \\mathbb{R}_{+}^{n}$ and $v \\in \\mathbb{R}_{+}^{m}$ such that $u_i = \\exp(f_i-1/2)$ and $v_j = \\exp(g_j-1/2)$. The optimal coupling $\\Pi^{\\star}$ must therefore have the structure:\n$$\n\\Pi^{\\star}_{ij} = u_i K_{ij} v_j\n$$\nThis is equivalent to the matrix form $\\Pi^{\\star} = \\operatorname{diag}(u)\\,K\\,\\operatorname{diag}(v)$. The scaling vectors $u$ and $v$ are determined by enforcing the marginal constraints:\n1.  Source marginal: $\\sum_{j=1}^{m} \\Pi^{\\star}_{ij} = u_i \\sum_{j=1}^{m} K_{ij} v_j = a_i \\implies u_i = \\frac{a_i}{\\sum_{j=1}^{m} K_{ij} v_j}$. In vector notation, $u = a \\oslash (Kv)$, where $\\oslash$ is element-wise division.\n2.  Target marginal: $\\sum_{i=1}^{n} \\Pi^{\\star}_{ij} = v_j \\sum_{i=1}^{n} u_i K_{ij} = b_j \\implies v_j = \\frac{b_j}{\\sum_{i=1}^{n} K_{ij} u_i}$. In vector notation, $v = b \\oslash (K^T u)$.\n\nThese coupled equations are solved iteratively using Sinkhorn's algorithm. Starting with $v^{(0)} = \\mathbf{1}_m$ (a vector of ones), one iterates for $k \\ge 1$ until convergence:\n$$\nu^{(k)} = a \\oslash (K v^{(k-1)})\n$$\n$$\nv^{(k)} = b \\oslash (K^T u^{(k)})\n$$\n\n**Task C: Calculation**\n\nGiven $n=m=2$, $\\varepsilon=1$, $a = (0.6, 0.4)^T$, $b = (0.5, 0.5)^T$, and $C = \\begin{pmatrix} 0 & \\ln(2) \\\\ \\ln(2) & 0 \\end{pmatrix}$.\nFirst, compute the Gibbs kernel $K = \\exp(-C/\\varepsilon) = \\exp(-C)$:\n$$\nK = \\begin{pmatrix} \\exp(-0) & \\exp(-\\ln(2)) \\\\ \\exp(-\\ln(2)) & \\exp(-0) \\end{pmatrix} = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}\n$$\nApply the Sinkhorn updates, initializing $v^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Iteration 1**:\n- Update $u$:\n$u_1^{(1)} = \\frac{0.6}{1(1) + 0.5(1)} = \\frac{0.6}{1.5} = 0.4$\n$u_2^{(1)} = \\frac{0.4}{0.5(1) + 1(1)} = \\frac{0.4}{1.5} = \\frac{4}{15} \\approx 0.2667$\n- Update $v$:\n$v_1^{(1)} = \\frac{0.5}{1(0.4) + 0.5(4/15)} = \\frac{0.5}{0.4 + 2/15} = \\frac{0.5}{8/15} = \\frac{15}{16} = 0.9375$\n$v_2^{(1)} = \\frac{0.5}{0.5(0.4) + 1(4/15)} = \\frac{0.5}{0.2 + 4/15} = \\frac{0.5}{7/15} = \\frac{15}{14} \\approx 1.0714$\n\nContinuing this process until convergence, the scaling vectors stabilize at approximately:\n$u^{\\star} \\approx \\begin{pmatrix} 0.408417 \\\\ 0.258714 \\end{pmatrix}$ and $v^{\\star} \\approx \\begin{pmatrix} 0.929977 \\\\ 1.079911 \\end{pmatrix}$.\n\nThe task is to compute the $(1,2)$ entry of the optimal coupling matrix, $\\Pi^{\\star}_{12}$:\n$$\n\\Pi^{\\star}_{12} = u^{\\star}_1 K_{12} v^{\\star}_2\n$$\nSubstituting the converged values and $K_{12}=0.5$:\n$$\n\\Pi^{\\star}_{12} \\approx (0.408417) \\times (0.5) \\times (1.079911) \\approx 0.22052825\n$$\nRounding to four significant figures, we get $0.2205$.", "answer": "$$\\boxed{0.2205}$$", "id": "2892437"}]}