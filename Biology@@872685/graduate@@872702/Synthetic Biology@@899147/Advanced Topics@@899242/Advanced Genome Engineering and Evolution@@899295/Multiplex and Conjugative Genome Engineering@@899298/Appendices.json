{"hands_on_practices": [{"introduction": "Large-scale genome redesign using Conjugative Assembly Genome Engineering (CAGE) requires partitioning a target genome into smaller, transferrable segments. This presents a critical experimental design choice: using fewer, larger segments reduces the number of assembly steps, but the success probability of each transfer decreases with segment length. This exercise [@problem_id:2752386] challenges you to find the optimal balance by formulating the problem as a constrained optimization task, using dynamic programming to devise a segmentation scheme that minimizes the total expected number of conjugation rounds and maximizes laboratory efficiency.", "problem": "A laboratory is planning a Conjugative Assembly Genome Engineering (CAGE) program to assemble a redesigned genome by transferring a set of contiguous DNA segments into a recipient strain. In each conjugation round, the attempt to transfer a single contiguous segment of length $\\ell$ (measured in kilobases, abbreviated as kb) succeeds with probability $p(\\ell)$, independently of all other rounds and segments. The laboratory can choose a segmentation scheme that partitions a target genome length $L$ (in kb) into a multiset of allowed segment sizes $S$ (in kb), under a constraint that the number of segments is at most $K_{\\max}$. The probability of successful transfer per round declines with segment length according to an exponential rule. The goal is to compute an optimal segmentation scheme that minimizes the total expected number of conjugation rounds needed to successfully transfer all segments sequentially into a single strain.\n\nUse the following foundational facts and definitions as the starting point:\n- By the Central Dogma of molecular biology, a DNA segment is a physical substrate whose manipulation probability does not depend on symbolic interpretation, but rather on process parameters; we therefore model each conjugation attempt as a Bernoulli trial with a length-dependent success probability.\n- For a Bernoulli process with per-trial success probability $p \\in (0,1]$, the expected number of trials until the first success (a geometric random variable) is $1/p$.\n- Assuming independence across segments and strictly sequential assembly into a single strain (i.e., segments do not overlap temporally and must be completed one after another), the total expected number of rounds is the sum of expected rounds for each segment.\n\nModel of length-dependent transfer probability:\n- Let $S$ be the set of allowed segment sizes (in kb), and let $\\ell_{\\min} = \\min S$.\n- Let $p_0 \\in (0,1]$ be the baseline success probability for a segment of length $\\ell_{\\min}$ in a single round.\n- Let $\\alpha > 0$ be a decay constant with units $\\text{kb}^{-1}$.\n- For any allowed segment length $\\ell \\in S$, define\n$$\np(\\ell) = \\min\\!\\left(1,\\; p_0 \\, e^{-\\alpha(\\ell - \\ell_{\\min})}\\right).\n$$\n- The expected number of rounds for a single segment of length $\\ell$ is then\n$$\n\\mathbb{E}[\\text{rounds} \\mid \\ell] = \\frac{1}{p(\\ell)}.\n$$\n\nOptimization problem:\n- Choose a multiset $\\{\\ell_1,\\ell_2,\\dots,\\ell_k\\}$ such that each $\\ell_i \\in S$, $\\sum_{i=1}^{k} \\ell_i = L$, and $1 \\leq k \\leq K_{\\max}$.\n- Minimize the total expected rounds\n$$\n\\sum_{i=1}^{k} \\frac{1}{p(\\ell_i)}.\n$$\n- If no exact partition of $L$ exists using elements of $S$ with at most $K_{\\max}$ segments, report no feasible solution.\n- Tie-breaking: if multiple segmentations achieve the same minimal total expected rounds (to within exact arithmetic under the model), select the one with the smallest $k$; if a tie remains, select the lexicographically smallest nondecreasing list $(\\ell_1\\leq \\ell_2 \\leq \\dots \\leq \\ell_k)$.\n\nPhysical units and output requirements:\n- All lengths must be expressed in kilobases (kb).\n- Probabilities are unitless; do not use the percentage symbol; represent any real-valued results in decimal form if needed.\n- Angles are not involved in this problem.\n\nYour task is to write a complete program that, for each test case below, computes the optimal segmentation scheme under the model and constraints. If a test case has no feasible segmentation, return the empty list. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a list of integers (segment sizes in kb) in nondecreasing order. For example, a valid output with three results could look like \"[[a,b],[c,d,e],[]]\".\n\nTest suite:\n- Case $1$ (general happy path):\n  - $L = 200$ kb\n  - $S = \\{20,40,60,80\\}$ kb\n  - $K_{\\max} = 5$\n  - $p_0 = 0.92$\n  - $\\alpha = 0.03$ $\\text{kb}^{-1}$\n- Case $2$ (infeasible under tight $K_{\\max}$):\n  - $L = 180$ kb\n  - $S = \\{20,40,60,80\\}$ kb\n  - $K_{\\max} = 2$\n  - $p_0 = 0.92$\n  - $\\alpha = 0.03$ $\\text{kb}^{-1}$\n- Case $3$ (boundary with a single segment):\n  - $L = 80$ kb\n  - $S = \\{20,40,60,80\\}$ kb\n  - $K_{\\max} = 1$\n  - $p_0 = 0.92$\n  - $\\alpha = 0.03$ $\\text{kb}^{-1}$\n- Case $4$ (alternative allowed sizes and trade-offs):\n  - $L = 240$ kb\n  - $S = \\{30,60,90\\}$ kb\n  - $K_{\\max} = 4$\n  - $p_0 = 0.85$\n  - $\\alpha = 0.02$ $\\text{kb}^{-1}$\n- Case $5$ (balanced interior optimum):\n  - $L = 150$ kb\n  - $S = \\{25,50,75\\}$ kb\n  - $K_{\\max} = 3$\n  - $p_0 = 0.90$\n  - $\\alpha = 0.025$ $\\text{kb}^{-1}$\n\nFinal output format:\n- Your program should produce a single line of output containing the segmentation lists for the cases in order as a Python-style list literal, e.g., \"[[...],[...],...]\", where each inner list contains integers representing segment sizes in kb in nondecreasing order. If no feasible segmentation exists for a case, output \"[] \" for that case.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of molecular biology and probability theory, is mathematically well-posed as a constrained optimization problem, and is articulated with objective, unambiguous terminology. All necessary data are provided. No internal contradictions or violations of fundamental principles are present.\n\nThe problem is to find an optimal partition of a target genome of total length $L$ into a multiset of segments $\\{\\ell_1, \\ell_2, \\dots, \\ell_k\\}$. Each segment length $\\ell_i$ must be chosen from a given set of allowed sizes $S$. The partition must satisfy two constraints: the sum of segment lengths must equal the target length, $\\sum_{i=1}^{k} \\ell_i = L$, and the total number of segments must not exceed a maximum, $k \\leq K_{\\max}$. The objective is to minimize the total expected number of conjugation rounds, which is given by the sum of the expected rounds for each segment:\n$$\n\\text{Minimize } \\sum_{i=1}^{k} \\mathbb{E}[\\text{rounds} \\mid \\ell_i] = \\sum_{i=1}^{k} \\frac{1}{p(\\ell_i)}\n$$\nwhere the success probability $p(\\ell)$ for a segment of length $\\ell$ is defined as\n$$\np(\\ell) = \\min\\!\\left(1,\\; p_0 \\, e^{-\\alpha(\\ell - \\ell_{\\min})}\\right)\n$$\nwith $\\ell_{\\min} = \\min S$.\n\nThis problem is a variation of the classic change-making problem and can be solved optimally using dynamic programming. The core idea is to iteratively build up solutions for increasing total lengths, from $1$ up to $L$.\n\nFirst, an important optimization can be made. Any valid segmentation is a sum of integers from the set $S$. Therefore, the total length $L$ must be an integer linear combination of the elements in $S$. This implies that $L$ must be a multiple of the greatest common divisor (GCD) of all segment sizes in $S$. If $L \\pmod{\\text{gcd}(S)} \\neq 0$, no feasible solution exists. This check can prune infeasible cases immediately. If this condition holds, we can scale down the problem by dividing $L$ and all elements of $S$ by their GCD, which reduces the size of the state space for the dynamic programming algorithm.\n\nLet the scaled target length be $L'$ and the set of scaled segment sizes be $S'$. We define a DP table, `dp`, of size $L'+1$. Each entry `dp[i]` will store the optimal solution for assembling a total scaled length of $i$. An \"optimal solution\" must encapsulate all information needed for the tie-breaking rules. Thus, `dp[i]` will be a tuple: `(total_cost, num_segments, segmentation_list)`.\n\nThe base case for the recursion is for a total length of $0$, which requires zero segments and incurs zero cost.\n$$\n\\text{dp}[0] = (0.0, 0, [])\n$$\nAll other entries $\\text{dp}[i]$ for $i > 0$ are initialized to a state representing infinity, e.g., $(\\infty, \\infty, [])$.\n\nThe DP table is filled iteratively for scaled lengths $i$ from $1$ to $L'$. For each length $i$, we consider forming it by adding a segment of size $s' \\in S'$ to a previously computed optimal solution for length $i-s'$. The original segment size is $s = s' \\cdot \\text{gcd}(S)$. The candidate solution derived from `dp[i-s']` and segment $s$ is:\n-   New cost: $\\text{dp}[i-s'].\\text{cost} + \\frac{1}{p(s)}$\n-   New segment count: $\\text{dp}[i-s'].\\text{count} + 1$\n-   New segmentation list: sorted list from appending $s$ to $\\text{dp}[i-s'].\\text{list}$\n\nThis candidate solution is valid only if its segment count does not exceed $K_{\\max}$. If valid, it is compared against the current best solution stored in `dp[i]`. The comparison strictly follows the specified tie-breaking hierarchy:\n1.  The solution with the lower total cost is preferred. A small tolerance $\\epsilon$ is used for floating-point comparisons.\n2.  If costs are equal, the solution with the smaller number of segments ($k$) is preferred.\n3.  If both cost and segment count are equal, the solution with the lexicographically smaller nondecreasing segmentation list is preferred.\n\nIf the candidate is better than the existing entry in `dp[i]`, `dp[i]` is updated with the new optimal solution tuple.\n\nAfter the DP table is fully computed up to $L'$, the entry `dp[L']` contains the optimal solution for the original problem. If the cost in `dp[L']` remains infinite, no feasible segmentation was found. Otherwise, the segmentation list stored in `dp[L']` is the final answer.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the CAGE segmentation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: General happy path\n        {'L': 200, 'S': [20, 40, 60, 80], 'K_max': 5, 'p0': 0.92, 'alpha': 0.03},\n        # Case 2: Infeasible under tight K_max\n        {'L': 180, 'S': [20, 40, 60, 80], 'K_max': 2, 'p0': 0.92, 'alpha': 0.03},\n        # Case 3: Boundary with a single segment\n        {'L': 80, 'S': [20, 40, 60, 80], 'K_max': 1, 'p0': 0.92, 'alpha': 0.03},\n        # Case 4: Alternative allowed sizes and trade-offs\n        {'L': 240, 'S': [30, 60, 90], 'K_max': 4, 'p0': 0.85, 'alpha': 0.02},\n        # Case 5: Balanced interior optimum\n        {'L': 150, 'S': [25, 50, 75], 'K_max': 3, 'p0': 0.90, 'alpha': 0.025},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = find_optimal_segmentation(**case)\n        results.append(result)\n\n    # Format the final output string to match \"[[...],[...],[]]\"\n    def format_list_of_lists(lol):\n        outer_parts = []\n        for inner_list in lol:\n            outer_parts.append(f\"[{','.join(map(str, inner_list))}]\")\n        return f\"[{','.join(outer_parts)}]\"\n\n    print(format_list_of_lists(results))\n\ndef find_optimal_segmentation(L, S, K_max, p0, alpha):\n    \"\"\"\n    Finds the optimal segmentation for a single case using dynamic programming.\n    \"\"\"\n    if not S or L == 0:\n        return []\n\n    # --- Pre-computation and Optimizations ---\n    # GCD optimization: if L is not divisible by gcd of S, no solution exists.\n    g = S[0]\n    for i in range(1, len(S)):\n        g = math.gcd(g, S[i])\n    if L % g != 0:\n        return []\n\n    L_scaled = L // g\n    S_sorted = sorted(S)\n    S_scaled = [s // g for s in S_sorted]\n\n    # Calculate transfer costs for each allowed segment size.\n    l_min = S_sorted[0]\n    \n    def get_prob(l):\n        return min(1.0, p0 * np.exp(-alpha * (l - l_min)))\n        \n    costs = {s: 1.0 / get_prob(s) for s in S_sorted}\n\n    # --- Dynamic Programming Setup ---\n    # dp[i] stores a tuple: (total_cost, num_segments, segmentation_list)\n    dp = [(float('inf'), float('inf'), []) for _ in range(L_scaled + 1)]\n    dp[0] = (0.0, 0, [])\n    \n    EPS = 1e-9 # Epsilon for floating-point comparisons\n\n    def is_better(candidate, best):\n        cand_cost, cand_k, cand_seg = candidate\n        best_cost, best_k, best_seg = best\n\n        if cand_cost < best_cost - EPS: return True\n        if cand_cost > best_cost + EPS: return False\n        \n        if cand_k < best_k: return True\n        if cand_k > best_k: return False\n        \n        if cand_seg < best_seg: return True\n        return False\n\n    # --- DP Computation ---\n    for i in range(1, L_scaled + 1):\n        for s_idx, s_scaled in enumerate(S_scaled):\n            s_original = S_sorted[s_idx]\n            if i >= s_scaled:\n                prev_cost, prev_k, prev_seg = dp[i - s_scaled]\n\n                if prev_k < K_max:\n                    new_cost = prev_cost + costs[s_original]\n                    new_k = prev_k + 1\n                    \n                    # To avoid re-sorting, create the sorted list only when needed for tie-breaking.\n                    # For simplicity and correctness given problem constraints, we form it here.\n                    new_seg = sorted(prev_seg + [s_original])\n                    candidate = (new_cost, new_k, new_seg)\n\n                    if is_better(candidate, dp[i]):\n                        dp[i] = candidate\n\n    # --- Final Result ---\n    final_cost, _, final_segmentation = dp[L_scaled]\n\n    if final_cost == float('inf'):\n        return []\n    else:\n        return final_segmentation\n\nsolve()\n```", "id": "2752386"}, {"introduction": "The success of any CAGE design, such as the one optimized in the previous exercise, depends on accurate models of DNA transfer. This practice [@problem_id:2752416] delves into the fundamental kinetics of bacterial conjugation, challenging you to estimate the core conjugation rate constant, $\\beta$, from simulated experimental data. By applying mass-action principles and Bayesian inference, you will learn to connect a biophysical model to observable colony counts, a crucial skill for calibrating and predicting the performance of genome engineering systems.", "problem": "You are calibrating a conjugation rate constant in the context of Multiplex Automated Genome Engineering (MAGE) and Conjugative Assembly Genome Engineering (CAGE). Conjugation in donor-recipient mating assays is modeled as a well-mixed, mass-action process over short time scales, where donors carry a mobile genetic element and transfer it to recipients, producing transconjugants. Assume the following fundamental base:\n- Mass-action kinetics for pairwise contact between donors and recipients, where the number of successful transfers per unit time is proportional to the product of donor and recipient densities, with proportionality constant equal to the conjugation rate constant.\n- Short assay durations such that donor density remains effectively constant.\n- The Central Dogma of molecular biology applies, but at the timescale considered, there is no growth; only transfer is modeled.\n- Colony-forming units counted from plated aliquots are well-approximated as Poisson samples from the well-mixed culture.\n- Independent replicate plates and independent sampling events are conditionally independent given the underlying population state.\n\nConsider the following definitions:\n- Let $D(t)$ be donor density in cells per mL, $R(t)$ be recipient density in cells per mL, and $T(t)$ be transconjugant density in cells per mL. At $t=0$, suppose $D(0)=D_0$, $R(0)=R_0$, and $T(0)=0$.\n- Assume $D(t) = D_0$ for all $t$ in the assay due to the short time window.\n- Under mass-action kinetics, the rates satisfy $\\,\\frac{dR}{dt} = -\\beta D(t) R(t)$ and $\\,\\frac{dT}{dt} = \\beta D(t) R(t)$, where the conjugation rate constant $\\beta$ has units of $\\mathrm{mL}\\,\\mathrm{cell}^{-1}\\,\\mathrm{hour}^{-1}$.\n- An observed plate $i$ is generated by plating a volume $v_i$ mL from the mating culture at time $t$, yielding an observed transconjugant colony count $c_i$. Conditional on $T(t)$, assume $c_i$ is a Poisson random variable with mean $\\mu_i = v_i \\, T(t)$.\n\nYour tasks:\n- Starting from the above definitions, derive the expression for $R(t)$ and $T(t)$ in terms of $D_0$, $R_0$, $t$, and $\\beta$.\n- Using the Poisson sampling model across independent plates $i=1,\\dots,n$, derive the likelihood of the data $\\{(v_i,c_i)\\}_{i=1}^n$ given $\\beta$.\n- Adopt a log-uniform prior on $\\beta$ over the bounded support $\\beta \\in [\\beta_{\\min}, \\beta_{\\max}]$, that is, a prior density proportional to $1/\\beta$ on that interval and zero elsewhere.\n- Compute the posterior over $\\beta$ on a log-spaced grid of $N$ points between $\\beta_{\\min}$ and $\\beta_{\\max}$. Numerically evaluate the posterior mode (maximum a posteriori estimate), the equal-tailed credible interval at mass $\\alpha$ (i.e., lower bound at cumulative mass $(1-\\alpha)/2$ and upper bound at cumulative mass $1-(1-\\alpha)/2$), and report the mode and interval bounds.\n\nUnits:\n- All densities are in cells per mL.\n- Time is in hours.\n- The conjugation rate $\\beta$ must be expressed in $\\mathrm{mL}\\,\\mathrm{cell}^{-1}\\,\\mathrm{hour}^{-1}$.\n- Report the final numerical answers in scientific notation with six digits after the decimal point (for example, $1.234567\\mathrm{e}{-12}$), in the unit $\\mathrm{mL}\\,\\mathrm{cell}^{-1}\\,\\mathrm{hour}^{-1}$.\n\nAlgorithmic constraints:\n- Posterior evaluation must be performed on a fixed grid of size $N$, logarithmically spaced between $\\beta_{\\min}$ and $\\beta_{\\max}$.\n- The posterior should be normalized numerically on this grid to produce a proper discrete distribution over the grid points.\n- Equal-tailed credible intervals should be computed from the discrete cumulative distribution.\n\nTest suite:\n- Use the following four test cases. In each case, use $\\beta_{\\min} = 10^{-14}$, $\\beta_{\\max} = 10^{-9}$, $N=12001$, and $\\alpha = 0.95$.\n    1. Happy-path, low conversion regime: $D_0 = 1.0\\times 10^{7}$, $R_0 = 1.0\\times 10^{8}$, $t = 1.0$, replicate platings with volumes $[\\,0.1,\\,0.1,\\,0.2\\,]$ and counts $[\\,95,\\,110,\\,210\\,]$.\n    2. Edge case, near-zero counts: $D_0 = 5.0\\times 10^{6}$, $R_0 = 1.0\\times 10^{8}$, $t = 0.5$, replicate platings with volumes $[\\,0.2,\\,0.2,\\,0.5\\,]$ and counts $[\\,0,\\,1,\\,0\\,]$.\n    3. Nonlinear regime, appreciable conversion: $D_0 = 5.0\\times 10^{7}$, $R_0 = 5.0\\times 10^{7}$, $t = 2.0$, replicate platings with volumes $[\\,0.01,\\,0.02\\,]$ and counts $[\\,18000,\\,36000\\,]$.\n    4. Boundary case, zero-time assay: $D_0 = 1.0\\times 10^{7}$, $R_0 = 1.0\\times 10^{8}$, $t = 0.0$, replicate platings with volumes $[\\,1.0\\,]$ and counts $[\\,0\\,]$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a triplet $[\\,\\hat{\\beta}_{\\mathrm{MAP}},\\,\\beta_{\\mathrm{L}},\\,\\beta_{\\mathrm{U}}\\,]$, where $\\hat{\\beta}_{\\mathrm{MAP}}$ is the posterior mode (maximum a posteriori), $\\beta_{\\mathrm{L}}$ is the equal-tailed lower bound at cumulative mass $(1-\\alpha)/2$, and $\\beta_{\\mathrm{U}}$ is the equal-tailed upper bound at cumulative mass $1-(1-\\alpha)/2$. Each number must be printed in scientific notation with six digits after the decimal point, in units of $\\mathrm{mL}\\,\\mathrm{cell}^{-1}\\,\\mathrm{hour}^{-1}$. The final single line should therefore look like\n  \"[[b1_map,b1_low,b1_up],[b2_map,b2_low,b2_up],[b3_map,b3_low,b3_up],[b4_map,b4_low,b4_up]]\"\nwith no spaces and with the triplets in the order of the test cases given above.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard biophysical model of bacterial conjugation coupled with a standard statistical inference framework. All necessary parameters and definitions are provided, and there are no internal contradictions or factual errors. The problem is valid and can be solved as follows.\n\nThe derivation and solution proceed in four steps: first, solving the system of ordinary differential equations for population dynamics; second, constructing the likelihood function from the Poisson sampling model; third, formulating the Bayesian posterior distribution; and fourth, outlining the numerical procedure for computing the posterior mode and credible interval.\n\n1. Derivation of Population Dynamics\nThe problem postulates a mass-action kinetics model for conjugation. The densities of recipient cells, $R(t)$, and transconjugant cells, $T(t)$, are governed by the following system of ordinary differential equations (ODEs):\n$$\n\\frac{dR}{dt} = -\\beta D(t) R(t)\n$$\n$$\n\\frac{dT}{dt} = \\beta D(t) R(t)\n$$\nwith initial conditions $R(0) = R_0$ and $T(0) = 0$. The donor density, $D(t)$, is assumed to be constant over the short assay duration, so $D(t) = D_0$ for all $t$.\n\nThe ODE for the recipient density becomes:\n$$\n\\frac{dR}{dt} = -(\\beta D_0) R(t)\n$$\nThis is a first-order, linear, separable differential equation. We can solve it by separating variables and integrating:\n$$\n\\int_{R_0}^{R(t)} \\frac{dR'}{R'} = - \\int_{0}^{t} \\beta D_0 \\, dt'\n$$\n$$\n\\ln(R(t)) - \\ln(R_0) = - \\beta D_0 t\n$$\n$$\n\\ln\\left(\\frac{R(t)}{R_0}\\right) = - \\beta D_0 t\n$$\nExponentiating both sides yields the solution for $R(t)$:\n$$\nR(t) = R_0 e^{-\\beta D_0 t}\n$$\nThe total number of initial recipient cells is conserved, partitioning into remaining recipients and newly formed transconjugants. Thus, $R(t) + T(t) = R_0 + T(0) = R_0$. From this conservation law, we derive the expression for $T(t)$:\n$$\nT(t) = R_0 - R(t) = R_0 - R_0 e^{-\\beta D_0 t}\n$$\n$$\nT(t) = R_0 (1 - e^{-\\beta D_0 t})\n$$\nThis is the analytical expression for the transconjugant density at time $t$.\n\n2. Likelihood Function\nThe experimental data consist of a set of $n$ independent plate counts, $\\{ (v_i, c_i) \\}_{i=1}^n$, where $v_i$ is the volume plated and $c_i$ is the observed number of transconjugant colonies. The model states that each count $c_i$ is drawn from a Poisson distribution with a mean $\\mu_i$ proportional to the transconjugant density $T(t)$ and the plated volume $v_i$:\n$$\nc_i \\sim \\text{Poisson}(\\mu_i) \\quad \\text{where} \\quad \\mu_i = v_i T(t)\n$$\nThe probability mass function of the Poisson distribution gives the probability of observing $c_i$ colonies given the mean $\\mu_i$:\n$$\nP(c_i | \\beta) = \\frac{\\mu_i^{c_i} e^{-\\mu_i}}{c_i!} = \\frac{(v_i T(t))^{c_i} e^{-v_i T(t)}}{c_i!}\n$$\nSince the plating events are independent, the total likelihood of the data given the parameter $\\beta$ is the product of the individual probabilities:\n$$\nL(\\beta) \\equiv P(\\{c_i\\} | \\beta) = \\prod_{i=1}^{n} P(c_i | \\beta) = \\prod_{i=1}^{n} \\frac{(v_i T(t))^{c_i} e^{-v_i T(t)}}{c_i!}\n$$\nFor numerical computation, it is more stable to work with the log-likelihood:\n$$\n\\ln L(\\beta) = \\sum_{i=1}^{n} \\left( c_i \\ln(v_i T(t)) - v_i T(t) - \\ln(c_i!) \\right)\n$$\nSubstituting the expression for $T(t)$:\n$$\n\\ln L(\\beta) = \\sum_{i=1}^{n} \\left( c_i \\ln(v_i R_0 (1 - e^{-\\beta D_0 t})) - v_i R_0 (1 - e^{-\\beta D_0 t}) - \\ln(c_i!) \\right)\n$$\n\n3. Posterior Distribution\nWe employ Bayes' theorem to find the posterior distribution of $\\beta$, which is proportional to the product of the likelihood and the prior distribution, $P(\\beta)$:\n$$\nP(\\beta | \\{c_i\\}) \\propto L(\\beta) P(\\beta)\n$$\nThe problem specifies a log-uniform prior on $\\beta$ over a bounded interval $[\\beta_{\\min}, \\beta_{\\max}]$, which corresponds to a prior density $P(\\beta) \\propto 1/\\beta$ for $\\beta \\in [\\beta_{\\min}, \\beta_{\\max}]$ and $P(\\beta) = 0$ otherwise.\nThe log-posterior is therefore:\n$$\n\\ln P(\\beta | \\{c_i\\}) = \\ln L(\\beta) + \\ln P(\\beta) + \\text{constant}\n$$\nDropping all terms that do not depend on $\\beta$:\n$$\n\\ln P(\\beta | \\{c_i\\}) \\propto -\\ln(\\beta) + \\sum_{i=1}^{n} \\left( c_i \\ln(T(t)) - v_i T(t) \\right)\n$$\nLet $C = \\sum_{i=1}^{n} c_i$ and $V = \\sum_{i=1}^{n} v_i$. The expression simplifies to:\n$$\n\\ln P(\\beta | \\{c_i\\}) \\propto -\\ln(\\beta) + C \\ln(R_0 (1 - e^{-\\beta D_0 t})) - V R_0 (1 - e^{-\\beta D_0 t})\n$$\nAgain, dropping the constant term $C \\ln R_0$:\n$$\n\\ln P(\\beta | \\{c_i\\}) \\propto -\\ln(\\beta) + C \\ln(1 - e^{-\\beta D_0 t}) - V R_0(1 - e^{-\\beta D_0 t})\n$$\nThis is the unnormalized log-posterior function that will be evaluated numerically. An important special case is $t=0$, which implies $T(t)=0$. If all observed counts $c_i$ are zero, as is expected, the likelihood term becomes $P(\\{c_i=0\\} | \\beta) = \\prod e^0 = 1$. The likelihood is constant, and the posterior is proportional to the prior, $P(\\beta | \\{c_i=0\\}) \\propto P(\\beta) \\propto 1/\\beta$.\n\n4. Numerical Procedure\nThe posterior distribution is computed on a discrete grid of $N$ points, $\\{\\beta_j\\}_{j=1}^N$, spaced logarithmically between $\\beta_{\\min}$ and $\\beta_{\\max}$.\n1.  **Grid Generation**: Create the grid of $\\beta_j$ values: $\\beta_j = 10^y$ where $y$ is linearly spaced from $\\log_{10}(\\beta_{\\min})$ to $\\log_{10}(\\beta_{\\max})$.\n2.  **Log-Posterior Evaluation**: For each $\\beta_j$ on the grid, calculate the unnormalized log-posterior value using the derived expression. For numerical stability, expresions like $1-e^{-x}$ and $\\ln(1-e^{-x})$ should be computed using functions such as `numpy.expm1` and `numpy.log1p`. Let $x_j = \\beta_j D_0 t$, the log-posterior is proportional to $-\\ln(\\beta_j) + C \\ln(1-e^{-x_j}) - V R_0(1-e^{-x_j})$.\n3.  **Normalization**: To obtain a probability mass function (PMF), exponentiate the log-posterior values and normalize them by their sum. To prevent numerical underflow or overflow, subtract the maximum log-posterior value before exponentiating: $p_j = \\exp(\\ln P(\\beta_j | \\text{data}) - \\max_k(\\ln P(\\beta_k | \\text{data})))$. The normalized PMF is $P_j = p_j / \\sum_k p_k$.\n4.  **Mode Estimation**: The maximum a posteriori (MAP) estimate, $\\hat{\\beta}_{\\mathrm{MAP}}$, is the grid point $\\beta_j$ corresponding to the maximum value of the normalized posterior PMF, $P_j$.\n5.  **Credible Interval**: The equal-tailed credible interval is found from the cumulative distribution function (CDF) of the discrete posterior, $F_k = \\sum_{j=1}^k P_j$. The lower bound, $\\beta_L$, is the smallest $\\beta_j$ such that $F_j \\ge (1-\\alpha)/2$. The upper bound, $\\beta_U$, is the smallest $\\beta_j$ such that $F_j \\ge 1-(1-\\alpha)/2$. These are found efficiently using a search on the sorted CDF array.\nFor the special case $t=0$ and all $c_i=0$, the posterior is $P(\\beta) \\propto 1/\\beta$. On a log-spaced grid, this corresponds to a uniform discrete posterior distribution, which simplifies the calculation of mode and credible interval.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_posterior(beta_grid, D0, R0, t, volumes, counts, beta_min, beta_max):\n    \"\"\"\n    Calculates the unnormalized posterior probability on a grid of beta values.\n    \"\"\"\n    C = np.sum(counts)\n    V = np.sum(volumes)\n\n    # Handle the special case where t=0.\n    # T(t)=0, so mu=0. Likelihood is 1 if all counts are 0, 0 otherwise.\n    # If likelihood is 1, posterior is proportional to prior (1/beta).\n    # On a log-spaced grid, a 1/beta distribution becomes uniform after discretization.\n    if t == 0:\n        if C == 0:\n            # Uniform posterior on the log-spaced grid\n            return np.ones_like(beta_grid)\n        else:\n            # Likelihood is zero for any count > 0 when t=0.\n            # Posterior is zero everywhere.\n            # This case is not in the test suite but is handled for completeness.\n            return np.zeros_like(beta_grid)\n\n    # Log-posterior calculation for the t > 0 case\n    # log_posterior ~ -log(beta) + C*log(1 - exp(-beta*D0*t)) - V*R0*(1 - exp(-beta*D0*t))\n    # Use numerically stable functions:\n    # np.expm1(x) = exp(x) - 1\n    # 1 - exp(-x) = -expm1(-x)\n    # log(1 - exp(-x)) = log(-expm1(-x))\n\n    x = beta_grid * D0 * t\n    \n    # Suppress warnings for log(0) which occurs outside the valid range\n    # and results in -inf, which is handled correctly.\n    with np.errstate(divide='ignore'):\n        log_prior = -np.log(beta_grid)\n    \n    # The term `1 - exp(-x)` could become zero for large x, leading to log(0).\n    one_minus_exp_term = -np.expm1(-x)\n    \n    # Avoid log(0) if one_minus_exp_term is exactly zero by adding a small epsilon.\n    # This happens when beta*D0*t is very large.\n    log_likelihood_term1 = C * np.log(one_minus_exp_term + np.finfo(float).eps)\n    log_likelihood_term2 = -V * R0 * one_minus_exp_term\n    \n    log_posterior = log_prior + log_likelihood_term1 + log_likelihood_term2\n\n    # Set posterior to -inf for beta outside the support [beta_min, beta_max]\n    # Although grid is already within this range, this is conceptually correct.\n    log_posterior[beta_grid < beta_min] = -np.inf\n    log_posterior[beta_grid > beta_max] = -np.inf\n\n    # Normalize to avoid underflow/overflow when exponentiating\n    log_posterior -= np.nanmax(log_posterior[np.isfinite(log_posterior)])\n\n    posterior = np.exp(log_posterior)\n    return posterior\n\ndef solve():\n    \"\"\"\n    Main solver function to process test cases and print results.\n    \"\"\"\n    # Test cases parameters\n    beta_min = 1e-14\n    beta_max = 1e-9\n    N = 12001\n    alpha = 0.95\n\n    test_cases = [\n        # 1. Happy-path, low conversion\n        {'D0': 1.0e7, 'R0': 1.0e8, 't': 1.0, 'v': [0.1, 0.1, 0.2], 'c': [95, 110, 210]},\n        # 2. Edge case, near-zero counts\n        {'D0': 5.0e6, 'R0': 1.0e8, 't': 0.5, 'v': [0.2, 0.2, 0.5], 'c': [0, 1, 0]},\n        # 3. Nonlinear regime, appreciable conversion\n        {'D0': 5.0e7, 'R0': 5.0e7, 't': 2.0, 'v': [0.01, 0.02], 'c': [18000, 36000]},\n        # 4. Boundary case, zero-time assay\n        {'D0': 1.0e7, 'R0': 1.0e8, 't': 0.0, 'v': [1.0], 'c': [0]},\n    ]\n    \n    all_results = []\n\n    # Logarithmically spaced grid for beta\n    beta_grid = np.logspace(np.log10(beta_min), np.log10(beta_max), N)\n\n    for case in test_cases:\n        D0 = case['D0']\n        R0 = case['R0']\n        t = case['t']\n        volumes = np.array(case['v'])\n        counts = np.array(case['c'])\n\n        # Calculate posterior\n        posterior = calculate_posterior(beta_grid, D0, R0, t, volumes, counts, beta_min, beta_max)\n        \n        # Normalize the posterior to get a probability mass function\n        posterior_pmf = posterior / np.sum(posterior)\n\n        # 1. Find the posterior mode (MAP)\n        map_index = np.argmax(posterior_pmf)\n        beta_map = beta_grid[map_index]\n\n        # 2. Calculate the credible interval\n        cdf = np.cumsum(posterior_pmf)\n        \n        # Find index for lower bound\n        lower_mass = (1.0 - alpha) / 2.0\n        lower_index = np.searchsorted(cdf, lower_mass, side='left')\n        beta_low = beta_grid[lower_index]\n\n        # Find index for upper bound\n        upper_mass = 1.0 - lower_mass\n        upper_index = np.searchsorted(cdf, upper_mass, side='left')\n        \n        # Ensure upper_index is within bounds\n        if upper_index == len(beta_grid):\n            upper_index = len(beta_grid) - 1\n            \n        beta_up = beta_grid[upper_index]\n\n        # Format results for the current case\n        case_results = [\n            f\"{beta_map:.6e}\",\n            f\"{beta_low:.6e}\",\n            f\"{beta_up:.6e}\"\n        ]\n        all_results.append(f\"[{','.join(case_results)}]\")\n\n    # Print a single line with all results, formatted as required.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "2752416"}, {"introduction": "The final phase of any genome engineering project is verification, which typically involves analyzing high-throughput sequencing data. However, when multiple genomic loci are edited simultaneously, sequencing reads can map ambiguously to several similar target sites. This exercise [@problem_id:2752515] introduces a powerful statistical method to resolve this uncertainty: the Expectation-Maximization (EM) algorithm, a cornerstone of modern bioinformatics analysis for assigning reads to their most likely origin and quantifying the abundance of each engineered variant.", "problem": "You are modeling the assignment of short sequencing reads generated during Multiplex Automated Genome Engineering (MAGE) or Conjugative Assembly Genome Engineering (CAGE) to a set of closely related edited loci. Each read is assumed to originate from exactly one edited locus, but similarity among loci and sequencing errors cause ambiguous mapping. You are given, for each read-locus pair, a nonnegative compatibility score that is proportional to the conditional probability of observing the read given the locus. You will estimate the mixture proportions of loci and quantify uncertainty in both read assignments and locus proportions, using an expectation-maximization approach grounded in first principles.\n\nFundamental base:\n- The Central Dogma of molecular biology provides the flow of information from DNA to RNA to protein; high-throughput sequencing reads arise from DNA, and mapping uncertainty follows from stochastic sequencing errors and sequence similarity.\n- Bayesâ€™ theorem and mixture models: if a sample arises from a mixture of sources with unknown proportions, the likelihood for independent observations is the product over observations of the mixture-weighted sum of component likelihoods.\n- Expectation-Maximization (EM) is a principled approach to maximum likelihood for latent-variable models, alternating between computing the expected latent responsibilities and maximizing parameters.\n- For quantifying parametric uncertainty in a multinomial mixture proportion vector, a Dirichlet prior yields a Dirichlet posterior, from which analytic variances are available.\n\nMathematical specification:\n- Let there be $R$ reads and $L$ edited loci. Let $Q \\in \\mathbb{R}_{\\ge 0}^{R \\times L}$ with entries $Q_{r\\ell}$ proportional to $p(x_r \\mid z_r = \\ell)$, where $x_r$ denotes read $r$ and $z_r$ is the latent locus identity. Let the locus mixture proportions be $p = (p_1,\\dots,p_L)$ with $p_\\ell \\ge 0$ and $\\sum_{\\ell=1}^L p_\\ell = 1$.\n- The log-likelihood of the data under the mixture model is\n$$\n\\mathcal{L}(p) = \\sum_{r=1}^R \\log\\left(\\sum_{\\ell=1}^L p_\\ell Q_{r\\ell}\\right).\n$$\n- Introduce responsibilities (posterior assignment probabilities) $\\gamma_{r\\ell}$ satisfying $\\gamma_{r\\ell} \\ge 0$ and $\\sum_{\\ell=1}^L \\gamma_{r\\ell} = 1$. The expectation step computes\n$$\n\\gamma_{r\\ell} \\propto p_\\ell Q_{r\\ell},\n$$\nwith the convention that if $\\sum_{\\ell=1}^L p_\\ell Q_{r\\ell} = 0$ (an uninformative read), then $\\gamma_{r\\cdot} = p$.\n- The maximization step updates $p$ by normalizing the soft counts:\n$$\np_\\ell \\leftarrow \\frac{1}{R}\\sum_{r=1}^R \\gamma_{r\\ell}.\n$$\n- For uncertainty quantification of $p$, use a symmetric Dirichlet prior with concentration vector $\\alpha_0 \\in \\mathbb{R}_{>0}^L$ and compute a Dirichlet posterior with parameters $a_\\ell = \\alpha_{0,\\ell} + \\sum_{r=1}^R \\gamma_{r\\ell}$ and total concentration $A = \\sum_{\\ell=1}^L a_\\ell$. The posterior variance of $p_\\ell$ is\n$$\n\\mathrm{Var}(p_\\ell) = \\frac{a_\\ell (A - a_\\ell)}{A^2 (A+1)}.\n$$\n- For uncertainty in read assignments, use the mean per-read entropy,\n$$\n\\bar{H} = \\frac{1}{R}\\sum_{r=1}^R \\left(-\\sum_{\\ell=1}^L \\gamma_{r\\ell} \\log \\gamma_{r\\ell}\\right),\n$$\nwith natural logarithm and the convention $0 \\log 0 = 0$.\n\nProgramming task:\n- Implement the Expectation-Maximization algorithm to find the maximum likelihood estimate of $p$ given $Q$, using the responsibility convention above for uninformative reads. Initialize $p$ uniformly, iterate until the absolute change in $\\mathcal{L}(p)$ is less than $10^{-12}$ or until $10000$ iterations, whichever comes first. Use the natural logarithm.\n- After convergence, compute posterior standard deviations $\\sigma_\\ell = \\sqrt{\\mathrm{Var}(p_\\ell)}$ using a symmetric Dirichlet prior with concentration vector $\\alpha_0$ provided per test case.\n- Compute the mean per-read entropy $\\bar{H}$.\n\nTest suite:\nFor each test case, you are given $Q$ and $\\alpha_0$.\n\n- Test case $1$ (happy path, three loci):\nLet $Q^{(1)} \\in \\mathbb{R}^{5 \\times 3}$ with rows\n(0.90, 0.05, 0.05),\n(0.80, 0.10, 0.10),\n(0.10, 0.80, 0.10),\n(0.05, 0.10, 0.85),\n(0.33, 0.33, 0.34).\nLet $\\alpha_0^{(1)} = (1.0, 1.0, 1.0)$.\n\n- Test case $2$ (non-identifiability/symmetry, two loci):\nLet $Q^{(2)} \\in \\mathbb{R}^{4 \\times 2}$ with rows\n(0.50, 0.50),\n(0.70, 0.70),\n(0.10, 0.10),\n(1.00, 1.00).\nLet $\\alpha_0^{(2)} = (1.0, 1.0)$.\n\n- Test case $3$ (zero-support locus, three loci):\nLet $Q^{(3)} \\in \\mathbb{R}^{4 \\times 3}$ with rows\n(0.60, 0.40, 0.00),\n(0.10, 0.90, 0.00),\n(0.50, 0.50, 0.00),\n(0.90, 0.10, 0.00).\nLet $\\alpha_0^{(3)} = (1.0, 1.0, 1.0)$.\n\n- Test case $4$ (four loci, varying ambiguity):\nLet $Q^{(4)} \\in \\mathbb{R}^{6 \\times 4}$ with rows\n(0.40, 0.30, 0.20, 0.10),\n(0.25, 0.25, 0.25, 0.25),\n(0.10, 0.20, 0.30, 0.40),\n(0.35, 0.30, 0.20, 0.15),\n(0.05, 0.05, 0.45, 0.45),\n(0.45, 0.10, 0.30, 0.15).\nLet $\\alpha_0^{(4)} = (1.0, 1.0, 1.0, 1.0)$.\n\nRequired final output format:\n- For each test case with $L$ loci, output a flat list of length $2L+1$ consisting of, in order: the $L$ estimated mixture proportions $(p_1,\\dots,p_L)$, the $L$ posterior standard deviations $(\\sigma_1,\\dots,\\sigma_L)$, and finally the scalar mean entropy $\\bar{H}$. All floats must be rounded to $6$ decimal places.\n- Aggregate the four per-case lists into a single list and print exactly one line containing this aggregate list as a comma-separated list enclosed in square brackets, for example: $[\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,]$.", "solution": "The problem requires the estimation of mixture proportions for a set of $L$ genomic loci from a dataset of $R$ sequencing reads, and the quantification of uncertainty in these estimates. The provided data is a compatibility matrix $Q \\in \\mathbb{R}_{\\ge 0}^{R \\times L}$, where the entry $Q_{r\\ell}$ is proportional to the conditional probability of observing read $r$ given it originates from locus $\\ell$, denoted $p(x_r | z_r = \\ell)$.\n\nThe underlying statistical framework is a finite mixture model. The unknown mixture proportions are $p = (p_1, \\dots, p_L)$, which must satisfy $p_\\ell \\ge 0$ for all $\\ell \\in \\{1, \\dots, L\\}$ and $\\sum_{\\ell=1}^L p_\\ell = 1$. The log-likelihood of the observed reads, assuming independence, is given by:\n$$\n\\mathcal{L}(p) = \\sum_{r=1}^R \\log\\left(\\sum_{\\ell=1}^L p_\\ell Q_{r\\ell}\\right)\n$$\nDirect maximization of $\\mathcal{L}(p)$ is complicated. A standard and principled approach for Maximum Likelihood Estimation (MLE) in such latent variable models is the Expectation-Maximization (EM) algorithm. The algorithm iteratively alternates between an Expectation (E) step and a Maximization (M) step until convergence.\n\nFirst, the parameters $p$ are initialized. A uniform distribution is chosen, such that $p_\\ell^{(0)} = 1/L$ for all $\\ell$.\n\nThe E-step computes the expected values of the latent variables $z_r$, which indicate the originating locus for each read $r$. This is equivalent to calculating the posterior probability, or \"responsibility,\" $\\gamma_{r\\ell}$ that locus $\\ell$ is responsible for read $r$, given the current parameter estimates $p$. Applying Bayes' theorem:\n$$\n\\gamma_{r\\ell} = P(z_r = \\ell | x_r, p) = \\frac{p(x_r | z_r = \\ell) P(z_r = \\ell)}{\\sum_{k=1}^L p(x_r | z_k = k) P(z_k = k)} = \\frac{p_\\ell Q_{r\\ell}}{\\sum_{k=1}^L p_k Q_{rk}}\n$$\nA special case arises if the denominator $\\sum_{k=1}^L p_k Q_{rk}$ is zero, which signifies that read $r$ is impossible under the current model. For such uninformative reads, the responsibility vector $\\gamma_{r\\cdot}$ is defined to be the current proportion vector $p$.\n\nThe M-step updates the parameter estimates $p$ to maximize the expected complete-data log-likelihood, using the responsibilities computed in the E-step. This yields a simple and intuitive update rule where each proportion $p_\\ell$ is set to the average responsibility for that locus across all reads:\n$$\np_\\ell^{\\text{(new)}} \\leftarrow \\frac{1}{R} \\sum_{r=1}^R \\gamma_{r\\ell}\n$$\nThe E and M steps are repeated until the absolute change in the log-likelihood $\\mathcal{L}(p)$ between successive iterations falls below a tolerance of $10^{-12}$, or a maximum of $10000$ iterations is reached.\n\nUpon convergence to the MLE $\\hat{p}$, uncertainty is quantified in two ways.\n\nFirst, the uncertainty in the parameter estimates $\\hat{p}$ is assessed using a Bayesian framework. A symmetric Dirichlet prior, $p \\sim \\mathrm{Dir}(\\alpha_0, \\dots, \\alpha_0)$, is placed on the proportion vector $p$. Due to the conjugacy of the Dirichlet and Multinomial distributions, the posterior distribution is also a Dirichlet distribution, $p | \\{x_r\\} \\sim \\mathrm{Dir}(a_1, \\dots, a_L)$, with parameters:\n$$\na_\\ell = \\alpha_{0,\\ell} + \\sum_{r=1}^R \\gamma_{r\\ell}\n$$\nwhere $\\gamma_{r\\ell}$ are the final responsibilities computed with the MLE $\\hat{p}$. The total concentration is $A = \\sum_{\\ell=1}^L a_\\ell$. The posterior variance for each component $p_\\ell$ is then analytically given by:\n$$\n\\mathrm{Var}(p_\\ell) = \\frac{a_\\ell (A - a_\\ell)}{A^2 (A+1)}\n$$\nThe reported uncertainty is the standard deviation $\\sigma_\\ell = \\sqrt{\\mathrm{Var}(p_\\ell)}$.\n\nSecond, the uncertainty in the assignment of individual reads to loci is quantified using the Shannon entropy. For each read $r$, the entropy of its responsibility vector $\\gamma_{r\\cdot}$ measures the ambiguity of its origin: $H(\\gamma_{r\\cdot}) = -\\sum_{\\ell=1}^L \\gamma_{r\\ell} \\log \\gamma_{r\\ell}$, where the natural logarithm is used and the convention $0 \\log 0 = 0$ is followed. The mean per-read entropy, $\\bar{H}$, provides a single measure of the overall assignment ambiguity for the entire dataset:\n$$\n\\bar{H} = \\frac{1}{R}\\sum_{r=1}^R H(\\gamma_{r\\cdot})\n$$\nThis complete procedure is implemented to process the provided test cases, yielding the estimated proportions, their posterior standard deviations, and the mean assignment entropy.", "answer": "```python\nimport numpy as np\nfrom scipy.special import xlogy\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"Q\": [\n                [0.90, 0.05, 0.05],\n                [0.80, 0.10, 0.10],\n                [0.10, 0.80, 0.10],\n                [0.05, 0.10, 0.85],\n                [0.33, 0.33, 0.34],\n            ],\n            \"alpha_0\": [1.0, 1.0, 1.0],\n        },\n        {\n            \"Q\": [\n                [0.50, 0.50],\n                [0.70, 0.70],\n                [0.10, 0.10],\n                [1.00, 1.00],\n            ],\n            \"alpha_0\": [1.0, 1.0],\n        },\n        {\n            \"Q\": [\n                [0.60, 0.40, 0.00],\n                [0.10, 0.90, 0.00],\n                [0.50, 0.50, 0.00],\n                [0.90, 0.10, 0.00],\n            ],\n            \"alpha_0\": [1.0, 1.0, 1.0],\n        },\n        {\n            \"Q\": [\n                [0.40, 0.30, 0.20, 0.10],\n                [0.25, 0.25, 0.25, 0.25],\n                [0.10, 0.20, 0.30, 0.40],\n                [0.35, 0.30, 0.20, 0.15],\n                [0.05, 0.05, 0.45, 0.45],\n                [0.45, 0.10, 0.30, 0.15],\n            ],\n            \"alpha_0\": [1.0, 1.0, 1.0, 1.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(case[\"Q\"], case[\"alpha_0\"])\n        results.append(result)\n\n    # Format the final output string as a list of lists.\n    # The string representation of each sublist is joined by a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _solve_case(Q_list, alpha_0_list):\n    \"\"\"\n    Solves a single test case for MAGE/CAGE analysis.\n    \"\"\"\n    # 1. Initialization\n    Q = np.array(Q_list, dtype=np.float64)\n    alpha_0 = np.array(alpha_0_list, dtype=np.float64)\n    R, L = Q.shape  # Number of reads and loci\n\n    p = np.full(L, 1.0 / L, dtype=np.float64)\n    \n    TOL = 1e-12\n    MAX_ITER = 10000\n\n    log_likelihood_old = -np.inf\n\n    # 2. Expectation-Maximization (EM) Loop\n    for _ in range(MAX_ITER):\n        # --- E-step: Compute responsibilities (gamma) ---\n        weighted_scores = Q @ p\n        numerator = p * Q  # Element-wise product, using broadcasting\n\n        # Denominator for Bayes' rule; has shape (R, 1) for broadcasting\n        denominator = weighted_scores.reshape(R, 1)\n        \n        gamma = np.zeros_like(Q)\n        \n        # Mask for reads with a non-zero marginal probability\n        non_zero_denom_mask = denominator.flatten() > 0\n        \n        # Standard case: update responsibilities where denominator is non-zero\n        gamma[non_zero_denom_mask, :] = numerator[non_zero_denom_mask, :] / denominator[non_zero_denom_mask]\n        \n        # Special convention: for uninformative reads (denominator is zero), gamma = p\n        zero_denom_mask = ~non_zero_denom_mask\n        if np.any(zero_denom_mask):\n            gamma[zero_denom_mask, :] = p\n        \n        # --- M-step: Update mixture proportions (p) ---\n        soft_counts = np.sum(gamma, axis=0)\n        p = soft_counts / R\n\n        # --- Convergence Check ---\n        # Recalculate weighted scores with new p\n        weighted_scores = Q @ p\n        \n        # Filter out zero scores to avoid log(0) -> -inf\n        valid_scores = weighted_scores[weighted_scores > 0]\n        \n        if len(valid_scores) < R:\n            log_likelihood_new = -np.inf\n        else:\n            log_likelihood_new = np.sum(np.log(valid_scores))\n\n        if abs(log_likelihood_new - log_likelihood_old) < TOL:\n            break\n        \n        log_likelihood_old = log_likelihood_new\n\n    # 3. Post-convergence computations using the final p\n    final_p = p\n    \n    # Re-compute final responsibilities with the MLE p\n    weighted_scores = Q @ final_p\n    numerator = final_p * Q\n    denominator = weighted_scores.reshape(R, 1)\n    final_gamma = np.zeros_like(Q)\n    non_zero_denom_mask = denominator.flatten() > 0\n    final_gamma[non_zero_denom_mask, :] = numerator[non_zero_denom_mask, :] / denominator[non_zero_denom_mask]\n    zero_denom_mask = ~non_zero_denom_mask\n    if np.any(zero_denom_mask):\n        final_gamma[zero_denom_mask, :] = final_p\n    \n    # --- Uncertainty in p (Posterior Standard Deviation) ---\n    final_soft_counts = np.sum(final_gamma, axis=0)\n    a = alpha_0 + final_soft_counts\n    A = np.sum(a)\n    \n    # Handle case where A+1 could be zero, though unlikely with alpha_0 > 0\n    if A == 0 or (A + 1) == 0:\n        var_p = np.zeros(L)\n    else:    \n        var_p = (a * (A - a)) / (A**2 * (A + 1))\n    \n    # Ensure variance is non-negative before taking sqrt\n    std_dev_p = np.sqrt(np.maximum(0, var_p))\n\n    # --- Uncertainty in assignments (Mean Per-Read Entropy) ---\n    # xlogy(x, y) computes x*log(y) and handles x=0 correctly\n    entropy_per_read = -np.sum(xlogy(final_gamma, final_gamma), axis=1)\n    H_bar = np.mean(entropy_per_read)\n\n    # 4. Format and return results\n    full_result = np.concatenate([final_p, std_dev_p, [H_bar]])\n    \n    # Round all final values to 6 decimal places\n    rounded_result = [round(x, 6) for x in full_result]\n    \n    return rounded_result\n\nsolve()\n\n```", "id": "2752515"}]}