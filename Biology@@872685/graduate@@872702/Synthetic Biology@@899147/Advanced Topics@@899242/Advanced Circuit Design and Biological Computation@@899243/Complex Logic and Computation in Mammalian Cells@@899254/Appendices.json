{"hands_on_practices": [{"introduction": "The input-output characteristics of synthetic gene circuits are often described by the Hill function, a cornerstone model in quantitative biology. Before we can confidently design complex systems, we must first be able to characterize their components by fitting such models to experimental data. This practice ([@problem_id:2723242]) guides you through the fundamental task of estimating the Hill coefficient from synthetic fluorescence data using nonlinear least squares and, crucially, quantifying the uncertainty in that estimate.", "problem": "You are given a modeling task arising from the analysis of inducible promoters used to implement complex logic and computation in mammalian cells. In these systems, the steady-state reporter fluorescence under an input concentration $u$ is commonly modeled by a Hill-type input-output function that has been well tested in gene regulation studies. Assume the following conditions: (i) the steady-state mapping is instantaneous relative to the sampling interval (so dynamic transients can be neglected for the purposes of parameter estimation from the provided samples), (ii) the response follows a Hill function with known gain and half-activation parameters, and (iii) the measurement noise is independent and identically distributed Gaussian. Your task is to infer the Hill coefficient $n$ and compute a two-sided $95\\%$ confidence interval for $n$ using nonlinear least squares for several datasets.\n\nFundamental base to be used: the Central Dogma of molecular biology establishes that gene expression can be modeled as a function of transcriptional input, and promoter occupancy models yield Hill-type steady-state response functions; under independent identically distributed Gaussian noise, maximum likelihood estimation reduces to nonlinear least squares; asymptotic confidence intervals for parameters estimated by nonlinear least squares can be computed from the Jacobian-based covariance approximation and Student's t-distribution.\n\nModel definition. For each data point $i \\in \\{1,\\dots,N\\}$ with input $u_i$, the observed fluorescence $y_i$ is modeled as\n$$\ny_i \\;=\\; f(u_i;n) + \\varepsilon_i, \\quad f(u;n) \\;=\\; \\alpha \\,\\frac{u^{\\,n}}{K^{\\,n} + u^{\\,n}},\n$$\nwhere $\\alpha$ and $K$ are known positive constants, $n$ is the unknown Hill coefficient to be estimated, and the noise satisfies $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ with an unknown variance $\\sigma^2$ common to all observations. All computations are unitless for this exercise.\n\nEstimation objective. Given one dataset $(u_i,y_i)_{i=1}^N$ together with known $\\alpha$ and $K$, estimate $\\hat{n}$ by minimizing the sum of squared residuals\n$$\nS(n) \\;=\\; \\sum_{i=1}^N \\left(y_i - f(u_i;n)\\right)^2.\n$$\nLet $r_i(n) = y_i - f(u_i;n)$ denote the residuals at $n$, and let $J$ denote the Jacobian of the residual vector with respect to $n$ evaluated at $\\hat{n}$, i.e., the $N \\times 1$ vector with entries\n$$\nJ_i \\;=\\; \\frac{\\partial r_i}{\\partial n}\\bigg|_{n=\\hat{n}} \\;=\\; -\\,\\frac{\\partial f(u_i;n)}{\\partial n}\\bigg|_{n=\\hat{n}}.\n$$\nDefine the residual sum of squares at the estimate as $\\mathrm{SSR} = \\sum_{i=1}^N r_i(\\hat{n})^2$, the degrees of freedom as $\\nu = N - p$ with $p=1$, and the unbiased residual variance estimate as $s^2 = \\mathrm{SSR}/\\nu$. Under standard regularity conditions, the approximate variance of $\\hat{n}$ is given by\n$$\n\\mathrm{Var}(\\hat{n}) \\;\\approx\\; s^2\\,\\left(J^\\top J\\right)^{-1},\n$$\nand a two-sided $95\\%$ confidence interval is\n$$\n\\hat{n} \\,\\pm\\, t_{0.975,\\nu}\\,\\sqrt{\\mathrm{Var}(\\hat{n})},\n$$\nwhere $t_{0.975,\\nu}$ is the $0.975$ quantile of the Student distribution with $\\nu$ degrees of freedom.\n\nAnalytic derivative. You must implement the analytic derivative $\\partial f/\\partial n$ to construct $J$:\n$$\n\\frac{\\partial f(u;n)}{\\partial n} \\;=\\; \\alpha \\,\\frac{u^{\\,n} K^{\\,n}\\,\\ln\\!\\big(u/K\\big)}{\\left(K^{\\,n} + u^{\\,n}\\right)^{2}},\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nTest suite. For each test case below, you are provided known $\\alpha$, known $K$, a vector of inputs $u_1,\\dots,u_N$, a latent true Hill coefficient $n_{\\mathrm{true}}$ (used only to synthesize $y_i$; your algorithm must not use $n_{\\mathrm{true}}$), and an additive noise sequence $\\varepsilon_1,\\dots,\\varepsilon_N$. The observed data are defined by $y_i = f(u_i;n_{\\mathrm{true}}) + \\varepsilon_i$. You must compute the estimate $\\hat{n}$ and the two-sided $95\\%$ confidence interval $[\\mathrm{lo},\\mathrm{hi}]$ for each dataset.\n\n- Case A (well-conditioned across a broad input range):\n  - $\\alpha = 1000.0$, $K = 50.0$, $n_{\\mathrm{true}} = 2.5$.\n  - Inputs $u = [5.0,10.0,20.0,30.0,50.0,80.0,120.0,200.0]$.\n  - Noise $\\varepsilon = [10.0,-5.0,8.0,-12.0,15.0,-7.0,9.0,-6.0]$.\n- Case B (inputs mostly below half-activation, challenging identifiability):\n  - $\\alpha = 800.0$, $K = 100.0$, $n_{\\mathrm{true}} = 3.0$.\n  - Inputs $u = [1.0,2.0,5.0,10.0,20.0]$.\n  - Noise $\\varepsilon = [0.5,-0.7,0.9,-1.1,1.3]$.\n- Case C (includes a point at $u=K$ yielding a locally uninformative sample for $n$):\n  - $\\alpha = 1200.0$, $K = 50.0$, $n_{\\mathrm{true}} = 2.0$.\n  - Inputs $u = [40.0,45.0,50.0,55.0,60.0,80.0]$.\n  - Noise $\\varepsilon = [-5.0,7.0,0.0,-8.0,6.0,-4.0]$.\n- Case D (high dynamic range, moderate cooperativity):\n  - $\\alpha = 1500.0$, $K = 30.0$, $n_{\\mathrm{true}} = 1.2$.\n  - Inputs $u = [2.0,5.0,10.0,20.0,30.0,60.0,120.0,300.0]$.\n  - Noise $\\varepsilon = [-2.0,3.0,-1.5,2.5,-3.0,4.0,-2.5,3.5]$.\n\nImplementation requirements.\n- For each case, construct $y_i$ exactly as $y_i = \\alpha \\, u_i^{\\,n_{\\mathrm{true}}}/(K^{\\,n_{\\mathrm{true}}} + u_i^{\\,n_{\\mathrm{true}}}) + \\varepsilon_i$ using the provided values.\n- Estimate $\\hat{n}$ by minimizing $S(n)$ via nonlinear least squares, treating $n$ as a single scalar decision variable. Use the analytic residual Jacobian based on the derivative above.\n- Compute the two-sided $95\\%$ confidence interval using the covariance approximation $s^2 (J^\\top J)^{-1}$ and the Student quantile $t_{0.975,\\nu}$ with $\\nu=N-1$.\n- To ensure numerical realism, constrain the search to $n \\in [0.1,6.0]$.\n- If $J^\\top J$ is numerically zero (e.g., below machine epsilon), return the confidence interval bounds as $\\mathrm{NaN}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of three floating-point numbers $[\\hat{n},\\mathrm{lo},\\mathrm{hi}]$. Round every printed floating-point value to exactly $6$ decimal places. For example, the overall format must be like $[[\\hat{n}_A,\\mathrm{lo}_A,\\mathrm{hi}_A],[\\hat{n}_B,\\mathrm{lo}_B,\\mathrm{hi}_B],\\dots]$ with no spaces anywhere in the line.", "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in established principles of molecular biology and statistical inference, well-posed with a clear objective and sufficient data, and formulated with objective and precise language. The task is to perform parameter estimation for a Hill-type model from synthetic data, which is a standard and meaningful problem in quantitative biology. We proceed with the solution.\n\nThe core of the problem is to estimate the Hill coefficient, denoted by $n$, from a set of noisy measurements. The relationship between the input concentration $u$ and the measured output $y$ is given by the Hill function with additive Gaussian noise. For each of the $N$ data points in a given dataset, the model is:\n$$\ny_i = f(u_i; n) + \\varepsilon_i, \\quad \\text{for } i = 1, \\dots, N\n$$\nwhere the noise terms $\\varepsilon_i$ are independent and identically distributed draws from a normal distribution with mean $0$ and unknown variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. The function $f(u; n)$ is the deterministic Hill model:\n$$\nf(u; n) = \\alpha \\frac{u^n}{K^n + u^n}\n$$\nHere, the parameters $\\alpha$ (maximum response) and $K$ (half-activation constant) are known. The only unknown parameter to be estimated is the Hill coefficient $n$.\n\nThe estimation is performed by the method of nonlinear least squares (NLLS). This method seeks the value of $n$, denoted $\\hat{n}$, that minimizes the sum of squared residuals $S(n)$:\n$$\n\\hat{n} = \\arg\\min_{n} S(n) = \\arg\\min_{n} \\sum_{i=1}^{N} \\left( y_i - f(u_i; n) \\right)^2\n$$\nUnder the assumption of i.i.d. Gaussian noise, the NLLS estimate is equivalent to the maximum likelihood estimate. To find $\\hat{n}$, a numerical optimization algorithm is employed. We utilize the Levenberg-Marquardt algorithm, as implemented in `scipy.optimize.least_squares`. This algorithm iteratively refines an initial guess for $n$ to minimize the sum of squares. The search for $n$ is constrained to the physically realistic interval $[0.1, 6.0]$.\n\nTo accelerate convergence and ensure accuracy, we provide the optimizer with the analytic Jacobian of the residual vector. The residual vector is $r(n)$ with components $r_i(n) = y_i - f(u_i; n)$. The Jacobian is an $N \\times 1$ matrix $J$ whose entries are the derivatives of the residuals with respect to the single parameter $n$:\n$$\nJ_i(n) = \\frac{\\partial r_i(n)}{\\partial n} = - \\frac{\\partial f(u_i; n)}{\\partial n}\n$$\nThe problem provides the explicit formula for this derivative:\n$$\n\\frac{\\partial f(u; n)}{\\partial n} = \\alpha \\frac{u^n K^n \\ln(u/K)}{(K^n + u^n)^2}\n$$\nThis exact Jacobian is computed at each iteration of the optimization.\n\nOnce the optimal estimate $\\hat{n}$ is found, we proceed to compute a two-sided $95\\%$ confidence interval for it. This interval quantifies the uncertainty in our estimate. The theoretical basis for this calculation is the asymptotic distribution of the NLLS estimator. The approximate variance of $\\hat{n}$ is given by:\n$$\n\\mathrm{Var}(\\hat{n}) \\approx s^2 \\left( J^\\top J \\right)^{-1}\n$$\nHere, $J$ is the Jacobian evaluated at the final estimate $\\hat{n}$. The term $s^2$ is the unbiased estimate of the noise variance $\\sigma^2$, calculated from the residual sum of squares (SSR) at the solution:\n$$\ns^2 = \\frac{\\mathrm{SSR}}{\\nu} = \\frac{1}{N-p} \\sum_{i=1}^{N} \\left( y_i - f(u_i; \\hat{n}) \\right)^2\n$$\nThe degrees of freedom are $\\nu = N - p$, where $N$ is the number of data points and $p$ is the number of estimated parameters. In this problem, we estimate only one parameter, $n$, so $p=1$ and $\\nu=N-1$.\n\nThe term $J^\\top J$ is a $1 \\times 1$ matrix (a scalar) representing the squared Euclidean norm of the Jacobian vector at the solution. If $J^\\top J$ is numerically zero (for instance, if all $u_i = K$), the variance is undefined, and a confidence interval cannot be computed.\n\nThe confidence interval is then constructed using the quantiles of the Student's $t$-distribution, which is appropriate when the sample size is small and the population variance $\\sigma^2$ is estimated by $s^2$. The two-sided $95\\%$ confidence interval for $n$ is:\n$$\n\\left[ \\hat{n} - t_{0.975, \\nu} \\sqrt{\\mathrm{Var}(\\hat{n})}, \\quad \\hat{n} + t_{0.975, \\nu} \\sqrt{\\mathrm{Var}(\\hat{n})} \\right]\n$$\nwhere $t_{0.975, \\nu}$ is the upper $0.025$ critical value of the Student's $t$-distribution with $\\nu$ degrees of freedom.\n\nThe procedure is implemented as a Python script. For each test case, we first synthesize the observation data $y_i$ using the provided true parameters $n_{\\text{true}}$. Then, we apply the NLLS optimization to estimate $\\hat{n}$ from $(u_i,y_i)$ and subsequently compute the confidence interval as described above. The results, including the estimate $\\hat{n}$ and the lower and upper bounds of the confidence interval, are collected and formatted according to the specified output requirements.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import least_squares\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to solve the parameter estimation problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"alpha\": 1000.0, \"K\": 50.0, \"n_true\": 2.5,\n            \"u\": np.array([5.0, 10.0, 20.0, 30.0, 50.0, 80.0, 120.0, 200.0]),\n            \"eps\": np.array([10.0, -5.0, 8.0, -12.0, 15.0, -7.0, 9.0, -6.0]),\n        },\n        {\n            \"alpha\": 800.0, \"K\": 100.0, \"n_true\": 3.0,\n            \"u\": np.array([1.0, 2.0, 5.0, 10.0, 20.0]),\n            \"eps\": np.array([0.5, -0.7, 0.9, -1.1, 1.3]),\n        },\n        {\n            \"alpha\": 1200.0, \"K\": 50.0, \"n_true\": 2.0,\n            \"u\": np.array([40.0, 45.0, 50.0, 55.0, 60.0, 80.0]),\n            \"eps\": np.array([-5.0, 7.0, 0.0, -8.0, 6.0, -4.0]),\n        },\n        {\n            \"alpha\": 1500.0, \"K\": 30.0, \"n_true\": 1.2,\n            \"u\": np.array([2.0, 5.0, 10.0, 20.0, 30.0, 60.0, 120.0, 300.0]),\n            \"eps\": np.array([-2.0, 3.0, -1.5, 2.5, -3.0, 4.0, -2.5, 3.5]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = estimate_hill_params(case)\n        results.append(result)\n\n    # Format the output as specified: [[n1,lo1,hi1],[n2,lo2,hi2],...] with no spaces.\n    formatted_results = []\n    for res_tuple in results:\n        n_hat, lo, hi = res_tuple\n        if np.isnan(lo):\n            formatted_entry = f\"[{n_hat:.6f},nan,nan]\"\n        else:\n            formatted_entry = f\"[{n_hat:.6f},{lo:.6f},{hi:.6f}]\"\n        formatted_results.append(formatted_entry)\n\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef estimate_hill_params(case):\n    \"\"\"\n    Performs NLLS estimation and computes confidence interval for a single case.\n    \"\"\"\n    alpha = case[\"alpha\"]\n    K = case[\"K\"]\n    n_true = case[\"n_true\"]\n    u = case[\"u\"]\n    eps = case[\"eps\"]\n\n    # --- 1. Define model and its derivative ---\n    def hill_model(n, u_vals, alpha_val, K_val):\n        # Numerically stable version for small u\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = u_vals / K_val\n            ratio_n = ratio**n\n        return alpha_val * ratio_n / (1.0 + ratio_n)\n\n    def hill_jacobian(n, u_vals, alpha_val, K_val):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = u_vals / K_val\n            # Handle u=K case where log(1)=0\n            log_ratio = np.log(ratio, where=ratio > 0)\n            log_ratio[ratio = 0] = -np.inf\n\n            Kn = K_val**n\n            un = u_vals**n\n            \n            numerator = alpha_val * un * Kn * log_ratio\n            denominator = (Kn + un)**2\n            # Handle potential division by zero if Kn + un is zero\n            deriv = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n        return deriv\n\n    # --- 2. Synthesize data ---\n    y_obs = hill_model(n_true, u, alpha, K) + eps\n\n    # --- 3. Define residual and Jacobian functions for the optimizer ---\n    def residuals(n_vec):\n        n = n_vec[0]\n        return y_obs - hill_model(n, u, alpha, K)\n\n    def residuals_jacobian(n_vec):\n        n = n_vec[0]\n        # Jacobian of residual r = y - f(n) is -df/dn\n        jac = -hill_jacobian(n, u, alpha, K)\n        return jac.reshape(-1, 1) # Reshape to N x 1 matrix\n\n    # --- 4. Perform Nonlinear Least Squares ---\n    n_initial = [2.0]  # Initial guess for n\n    bounds = (0.1, 6.0)\n    result = least_squares(residuals, n_initial, jac=residuals_jacobian, bounds=bounds, method='trf')\n    \n    n_hat = result.x[0]\n\n    # --- 5. Compute Confidence Interval ---\n    N = len(u)\n    p = 1  # Number of parameters\n    nu = N - p  # Degrees of freedom\n\n    J = result.jac # This is already the Jacobian of residuals at the solution\n    JtJ = J.T @ J\n\n    # Check for singularity\n    if JtJ[0, 0]  np.finfo(float).eps:\n        return [n_hat, np.nan, np.nan]\n\n    SSR = np.sum(result.fun**2)\n    s2 = SSR / nu  # Unbiased estimate of error variance\n    \n    try:\n        var_n_hat = s2 * np.linalg.inv(JtJ)\n        se_n_hat = np.sqrt(var_n_hat[0, 0])\n    except (np.linalg.LinAlgError, ValueError):\n        return [n_hat, np.nan, np.nan]\n\n    t_crit = t.ppf(0.975, df=nu)\n    ci_half_width = t_crit * se_n_hat\n\n    ci_lo = n_hat - ci_half_width\n    ci_hi = n_hat + ci_half_width\n\n    return [n_hat, ci_lo, ci_hi]\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2723242"}, {"introduction": "Synthetic circuits do not operate in a vacuum; they are embedded within the complex and resource-limited environment of a living cell. This exercise ([@problem_id:2723261]) delves into the critical issue of circuit robustness by modeling how a synthetic AND gate's performance is affected by fluctuations in the cell's metabolic state, represented as a shared pool of resources for transcription and translation. By deriving and computing key performance metrics, you will learn to quantify the resilience of a circuit's logic function to its cellular context.", "problem": "Consider a synthetic gene circuit in mammalian cells that computes an AND logic on two transcription factor (TF) inputs. Use the Central Dogma (deoxyribonucleic acid to ribonucleic acid to protein) and mass-action Ordinary Differential Equations (ODEs) with Hill-type promoter activation as the fundamental base. The cell’s metabolic state provides a dimensionless resource availability scalar $\\rho \\in \\mathbb{R}_{0}$ that multiplicatively scales both transcription and translation rates for all genes in the circuit.\n\nThe circuit consists of two TFs, $A$ and $B$, and a reporter protein $P$. The reporter is transcribed into reporter messenger ribonucleic acid (mRNA) $m$ and then translated into protein $p$. Assume the following:\n\n- TF production is regulated upstream by external inducers and can be summarized as constant effective synthesis rates $s_A$ and $s_B$ that already encode inducer effects. Both $A$ and $B$ obey linear production and first-order degradation dynamics with resource scaling on production.\n- The reporter promoter integrates TFs via an AND logic implemented as the product of two Hill activation functions. Reporter transcription and translation are both resource-scaled. All degradation processes are first-order and are not resource-scaled.\n\nModel the dynamics with the following ODEs, where all variables and parameters are nonnegative and dimensionless unless explicitly stated:\n\n- TFs:\n$$\n\\frac{dA}{dt} = \\rho\\, s_A - \\gamma_A A, \\quad \\frac{dB}{dt} = \\rho\\, s_B - \\gamma_B B.\n$$\n\n- Reporter:\n$$\n\\frac{dm}{dt} = \\rho\\, k_{\\mathrm{tx}}\\, f(A,B) - \\gamma_m m, \\quad \\frac{dp}{dt} = \\rho\\, k_{\\mathrm{tl}}\\, m - \\gamma_p p.\n$$\n\nThe AND gate transcriptional input is\n$$\nf(A,B) = H_A(A)\\, H_B(B),\n$$\nwith Hill activations\n$$\nH_A(A) = \\frac{A^{n_A}}{K_A^{n_A} + A^{n_A}}, \\quad H_B(B) = \\frac{B^{n_B}}{K_B^{n_B} + B^{n_B}}.\n$$\n\nTask. Starting from these definitions alone, derive the steady-state reporter protein level $p^\\ast(\\rho)$ and the local logarithmic sensitivity (elasticity) of the steady-state output to $\\rho$,\n$$\nE(\\rho) \\equiv \\frac{d \\log p^\\ast(\\rho)}{d \\log \\rho}.\n$$\nUse first principles and the chain rule; do not assume any “shortcut” formulas beyond the ODEs and the Hill functions stated above. In addition, to quantify robustness of the logic computation itself (separately from the trivial overall scaling of the output stage), define the logic robustness metric over a specified resource interval,\n$$\nJ_{\\mathrm{logic}} \\equiv \\max_{\\rho \\in [\\rho_{\\min}, \\rho_{\\max}]} \\left| \\frac{f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)}{f\\big(A^\\ast(\\rho_0), B^\\ast(\\rho_0)\\big)} - 1 \\right|,\n$$\nwhere $A^\\ast(\\rho)$ and $B^\\ast(\\rho)$ are the TF steady states.\n\nFor numerical evaluation, use the following shared constants across all test cases:\n- $k_{\\mathrm{tx}} = 50.0$, $k_{\\mathrm{tl}} = 5.0$, $\\gamma_m = 1.0$, $\\gamma_p = 0.2$,\n- $\\rho_0 = 1.0$, $\\rho_{\\min} = 0.3$, $\\rho_{\\max} = 3.0$,\n- sampling on $\\rho$ must be performed on a logarithmic grid with $N_\\rho = 10001$ points (inclusive of endpoints),\n- robustness thresholds $E_{\\mathrm{thr}} = 3.2$ and $J_{\\mathrm{thr}} = 0.2$.\n\nDefine a robustness flag as\n$$\n\\mathrm{robust} = \\begin{cases}\n1  \\text{if } E(\\rho_0) \\le E_{\\mathrm{thr}} \\text{ and } J_{\\mathrm{logic}} \\le J_{\\mathrm{thr}},\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n\nTest suite. Evaluate the following five parameter sets, each provided as $(n_A, K_A, s_A, \\gamma_A, n_B, K_B, s_B, \\gamma_B)$:\n\n- Case $1$: $(2.0, 1.0, 10.0, 5.0, 2.0, 1.5, 12.0, 6.0)$,\n- Case $2$: $(3.0, 2.0, 100.0, 5.0, 2.0, 2.0, 90.0, 3.0)$,\n- Case $3$: $(2.0, 1.0, 1.0, 5.0, 2.0, 1.0, 1.0, 5.0)$,\n- Case $4$: $(1.0, 5.0, 20.0, 1.0, 4.0, 1.0, 1.0, 2.0)$,\n- Case $5$: $(2.0, 3.0, 3.0, 1.0, 2.0, 2.0, 2.0, 1.0)$.\n\nYour program must, for each test case, compute:\n- the elasticity $E(\\rho_0)$,\n- the logic robustness $J_{\\mathrm{logic}}$,\n- the robustness flag as an integer $0$ or $1$.\n\nFinal output format. Your program should produce a single line of output containing a Python-style list of lists\n$$\n\\big[\\,[E_1, J_1, R_1], [E_2, J_2, R_2], \\dots, [E_5, J_5, R_5]\\,\\big],\n$$\nwhere $E_i$ and $J_i$ are floats rounded to six decimal places, and $R_i$ is an integer. No other text should be printed. All computations are dimensionless; no physical units are required and none should be included in the output.", "solution": "The problem posed is a well-defined exercise in the analysis of ordinary differential equation (ODE) models, a standard practice in systems biology. It is scientifically grounded, mathematically consistent, and presents a non-trivial but solvable task. We shall therefore proceed with its solution. The objective is to derive the steady-state protein concentration $p^\\ast$ as a function of the resource availability scalar $\\rho$, derive the corresponding logarithmic sensitivity $E(\\rho)$, and then perform numerical evaluations for specified parameter sets.\n\nFirst, we determine the steady-state concentrations of all species by setting their time derivatives to zero. The system of ODEs is:\n$$\n\\frac{dA}{dt} = \\rho\\, s_A - \\gamma_A A\n$$\n$$\n\\frac{dB}{dt} = \\rho\\, s_B - \\gamma_B B\n$$\n$$\n\\frac{dm}{dt} = \\rho\\, k_{\\mathrm{tx}}\\, f(A,B) - \\gamma_m m\n$$\n$$\n\\frac{dp}{dt} = \\rho\\, k_{\\mathrm{tl}}\\, m - \\gamma_p p\n$$\nAt steady state, denoted by a superscript asterisk ($^\\ast$), we have $\\frac{dA}{dt} = \\frac{dB}{dt} = \\frac{dm}{dt} = \\frac{dp}{dt} = 0$.\n\nFor transcription factor $A$:\n$$\n\\rho\\, s_A - \\gamma_A A^\\ast = 0 \\implies A^\\ast(\\rho) = \\frac{\\rho\\, s_A}{\\gamma_A}\n$$\nSimilarly, for transcription factor $B$:\n$$\n\\rho\\, s_B - \\gamma_B B^\\ast = 0 \\implies B^\\ast(\\rho) = \\frac{\\rho\\, s_B}{\\gamma_B}\n$$\nThese show that the steady-state concentrations of the input TFs scale linearly with the resource scalar $\\rho$.\n\nFor the reporter mRNA $m$:\n$$\n\\rho\\, k_{\\mathrm{tx}}\\, f(A^\\ast, B^\\ast) - \\gamma_m m^\\ast = 0 \\implies m^\\ast(\\rho) = \\frac{\\rho\\, k_{\\mathrm{tx}}}{\\gamma_m} f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)\n$$\nFor the reporter protein $p$:\n$$\n\\rho\\, k_{\\mathrm{tl}}\\, m^\\ast - \\gamma_p p^\\ast = 0 \\implies p^\\ast(\\rho) = \\frac{\\rho\\, k_{\\mathrm{tl}}}{\\gamma_p} m^\\ast(\\rho)\n$$\nSubstituting the expression for $m^\\ast(\\rho)$ into the equation for $p^\\ast(\\rho)$ yields the full expression for the steady-state reporter protein level:\n$$\np^\\ast(\\rho) = \\frac{\\rho\\, k_{\\mathrm{tl}}}{\\gamma_p} \\left( \\frac{\\rho\\, k_{\\mathrm{tx}}}{\\gamma_m} f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big) \\right) = \\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{\\gamma_p \\gamma_m} \\rho^2 f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)\n$$\nwhere $f(A,B) = H_A(A) H_B(B)$ and $A^\\ast(\\rho)$, $B^\\ast(\\rho)$ are the linear functions of $\\rho$ derived above. The explicit form is:\n$$\np^\\ast(\\rho) = \\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{\\gamma_p \\gamma_m} \\rho^2 \\left( \\frac{(A^\\ast(\\rho))^{n_A}}{K_A^{n_A} + (A^\\ast(\\rho))^{n_A}} \\right) \\left( \\frac{(B^\\ast(\\rho))^{n_B}}{K_B^{n_B} + (B^\\ast(\\rho))^{n_B}} \\right)\n$$\n\nNext, we derive the local logarithmic sensitivity, or elasticity, $E(\\rho)$, defined as:\n$$\nE(\\rho) \\equiv \\frac{d \\log p^\\ast(\\rho)}{d \\log \\rho}\n$$\nTo facilitate differentiation, we take the natural logarithm of $p^\\ast(\\rho)$:\n$$\n\\log p^\\ast(\\rho) = \\log\\left(\\frac{k_{\\mathrm{tl}} k_{\\mathrm{tx}}}{\\gamma_p \\gamma_m}\\right) + 2\\log\\rho + \\log(H_A(A^\\ast(\\rho))) + \\log(H_B(B^\\ast(\\rho)))\n$$\nDifferentiating with respect to $\\log\\rho$ term by term:\n$$\nE(\\rho) = \\frac{d}{d\\log\\rho}\\left( \\log(\\text{const.}) \\right) + \\frac{d(2\\log\\rho)}{d\\log\\rho} + \\frac{d\\log H_A(A^\\ast(\\rho))}{d\\log\\rho} + \\frac{d\\log H_B(B^\\ast(\\rho))}{d\\log\\rho}\n$$\nThe derivative of the constant is $0$, and $\\frac{d(2\\log\\rho)}{d\\log\\rho} = 2$. For the Hill function terms, we use the chain rule:\n$$\n\\frac{d\\log H_A(A^\\ast(\\rho))}{d\\log\\rho} = \\frac{d\\log H_A}{d\\log A^\\ast} \\cdot \\frac{d\\log A^\\ast}{d\\log\\rho}\n$$\nWe need the derivative $\\frac{d\\log A^\\ast}{d\\log\\rho}$. Since $A^\\ast(\\rho) = (\\frac{s_A}{\\gamma_A})\\rho$, we have $\\log A^\\ast = \\log(\\frac{s_A}{\\gamma_A}) + \\log\\rho$. Thus, $\\frac{d\\log A^\\ast}{d\\log\\rho} = 1$. The same holds for $B^\\ast$.\nThe problem reduces to computing the elasticity of the Hill function with respect to its input. Let $H(x) = \\frac{x^n}{K^n + x^n}$.\n$$\n\\log H(x) = n\\log x - \\log(K^n + x^n)\n$$\nDifferentiating with respect to $\\log x$:\n$$\n\\frac{d\\log H(x)}{d\\log x} = n - \\frac{d\\log(K^n + x^n)}{d \\log x} = n - \\frac{x}{K^n+x^n}\\frac{d(K^n+x^n)}{dx} = n - \\frac{x}{K^n+x^n}(nx^{n-1})\n$$\n$$\n\\frac{d\\log H(x)}{d\\log x} = n - \\frac{nx^n}{K^n + x^n} = n\\left(1 - \\frac{x^n}{K^n+x^n}\\right) = n\\left(\\frac{K^n}{K^n+x^n}\\right) = n(1 - H(x))\n$$\nApplying this result to our expression for $E(\\rho)$:\n$$\nE(\\rho) = 2 + \\frac{d\\log H_A}{d\\log A^\\ast} + \\frac{d\\log H_B}{d\\log B^\\ast} = 2 + n_A\\left(1 - H_A(A^\\ast(\\rho))\\right) + n_B\\left(1 - H_B(B^\\ast(\\rho))\\right)\n$$\nThis is the final analytical expression for the elasticity. It is evaluated at $\\rho = \\rho_0 = 1.0$, where $A^\\ast(\\rho_0) = s_A/\\gamma_A$ and $B^\\ast(\\rho_0) = s_B/\\gamma_B$.\n\nThe logic robustness metric $J_{\\mathrm{logic}}$ is defined as:\n$$\nJ_{\\mathrm{logic}} \\equiv \\max_{\\rho \\in [\\rho_{\\min}, \\rho_{\\max}]} \\left| \\frac{f\\big(A^\\ast(\\rho), B^\\ast(\\rho)\\big)}{f\\big(A^\\ast(\\rho_0), B^\\ast(\\rho_0)\\big)} - 1 \\right|\n$$\nThis quantity does not have a simple closed-form expression and must be computed numerically. The procedure is as follows:\n1.  Generate a logarithmic grid of $N_\\rho = 10001$ points for $\\rho$ in the interval $[\\rho_{\\min}, \\rho_{\\max}]$.\n2.  Compute the reference value $f_0 = f\\big(A^\\ast(\\rho_0), B^\\ast(\\rho_0)\\big)$.\n3.  For each point $\\rho_i$ in the grid, compute $A^\\ast(\\rho_i)$ and $B^\\ast(\\rho_i)$, then calculate $f_i = f\\big(A^\\ast(\\rho_i), B^\\ast(\\rho_i)\\big)$.\n4.  Compute the value $|f_i/f_0 - 1|$ for all points in the grid.\n5.  $J_{\\mathrm{logic}}$ is the maximum of these values.\n\nFinally, the robustness flag is determined by comparing the computed $E(\\rho_0)$ and $J_{\\mathrm{logic}}$ against their respective thresholds, $E_{\\mathrm{thr}} = 3.2$ and $J_{\\mathrm{thr}} = 0.2$.\n\nThese steps will be implemented for each test case to generate the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the synthetic gene circuit problem: calculates elasticity,\n    logic robustness, and a robustness flag for five parameter sets.\n    \"\"\"\n    # Shared constants\n    k_tx = 50.0\n    k_tl = 5.0\n    gamma_m = 1.0\n    gamma_p = 0.2\n    rho_0 = 1.0\n    rho_min = 0.3\n    rho_max = 3.0\n    N_rho = 10001\n    E_thr = 3.2\n    J_thr = 0.2\n\n    # Test suite: (n_A, K_A, s_A, gamma_A, n_B, K_B, s_B, gamma_B)\n    test_cases = [\n        (2.0, 1.0, 10.0, 5.0, 2.0, 1.5, 12.0, 6.0),\n        (3.0, 2.0, 100.0, 5.0, 2.0, 2.0, 90.0, 3.0),\n        (2.0, 1.0, 1.0, 5.0, 2.0, 1.0, 1.0, 5.0),\n        (1.0, 5.0, 20.0, 1.0, 4.0, 1.0, 1.0, 2.0),\n        (2.0, 3.0, 3.0, 1.0, 2.0, 2.0, 2.0, 1.0),\n    ]\n\n    results = []\n\n    def hill_function(x, n, K):\n        \"\"\"Computes the Hill activation function.\"\"\"\n        # This form is robust against potential overflow for very large x.\n        # However, direct computation is safe for the given parameters.\n        # return 1.0 / (1.0 + (K / x)**n)\n        x_n = x**n\n        return x_n / (K**n + x_n)\n\n    # Generate the rho grid for J_logic calculation\n    rho_grid = np.logspace(np.log10(rho_min), np.log10(rho_max), N_rho)\n\n    for case in test_cases:\n        nA, KA, sA, gammaA, nB, KB, sB, gammaB = case\n\n        # 1. Calculate Elasticity E(rho_0)\n        A_star_0 = sA / gammaA\n        B_star_0 = sB / gammaB\n\n        H_A_0 = hill_function(A_star_0, nA, KA)\n        H_B_0 = hill_function(B_star_0, nB, KB)\n        \n        # E(rho) = 2 + nA*(1 - H_A(A*(rho))) + nB*(1 - H_B(B*(rho)))\n        E_rho0 = 2.0 + nA * (1.0 - H_A_0) + nB * (1.0 - H_B_0)\n\n        # 2. Calculate Logic Robustness J_logic\n        f_0 = H_A_0 * H_B_0\n        \n        A_star_rho = rho_grid * sA / gammaA\n        B_star_rho = rho_grid * sB / gammaB\n\n        f_rho = hill_function(A_star_rho, nA, KA) * hill_function(B_star_rho, nB, KB)\n\n        # Avoid division by zero, although f_0 should be positive.\n        if f_0 > 1e-12:\n            deviation = np.abs(f_rho / f_0 - 1.0)\n            J_logic = np.max(deviation)\n        else:\n            # If f_0 is effectively zero, any non-zero f_rho indicates infinite relative change.\n            # This case is unlikely here but handled for robustness.\n            J_logic = np.inf\n        \n        # 3. Determine the robustness flag\n        is_robust = 1 if (E_rho0 = E_thr and J_logic = J_thr) else 0\n\n        # Store results for this case\n        results.append([E_rho0, J_logic, is_robust])\n\n    # Format the final output string as specified\n    output_parts = []\n    for res in results:\n        E_str = f\"{res[0]:.6f}\"\n        J_str = f\"{res[1]:.6f}\"\n        R_str = str(res[2])\n        output_parts.append(f\"[{E_str},{J_str},{R_str}]\")\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "2723261"}, {"introduction": "The inherent stochasticity of gene expression presents a major challenge for engineering reliable cellular computers, where unintended outputs can have serious consequences. To build circuits for sensitive applications like therapeutics, we need formal methods to guarantee safe operation. This advanced practice ([@problem_id:2723266]) introduces a powerful technique for safety verification by modeling a logic circuit as a Piecewise Deterministic Markov Process (PDMP) and using reachability analysis to prove that the system cannot access unsafe states.", "problem": "Consider a minimal, mechanistic model of a two-input logic circuit operating in mammalian cells, represented as a Piecewise Deterministic Markov Process (PDMP). The PDMP has a discrete promoter state and a continuous concentration for each gene product. There are two upstream transcriptional units, denoted by genes A and B, and one downstream output gene O implementing an AND-like combinatorial promoter. The discrete promoter state for each upstream unit is either ON or OFF, denoted by $s_A \\in \\{0,1\\}$ and $s_B \\in \\{0,1\\}$. The continuous state is $x = (x_A, x_B, x_O) \\in \\mathbb{R}_{\\ge 0}^3$, where $x_A$, $x_B$, and $x_O$ are concentrations (arbitrary consistent units).\n\nFundamental model components:\n- Central Dogma of Molecular Biology (CDMB): Transcription translates promoter occupancy into synthesis; translation and degradation determine protein levels.\n- Between promoter state jumps, $x$ evolves deterministically according to linear first-order kinetics for $x_A$ and $x_B$, and a Hill-type input for $x_O$.\n- Promoter jumps are memoryless (Markov) with exponentially distributed waiting times conditioned on the current input and state.\n\nModel assumptions and definitions:\n1. Inputs $u = (u_A, u_B)$ are fixed over the time horizon and binary, with $u_A, u_B \\in \\{0,1\\}$.\n2. Promoter jump intensities are constrained as follows:\n   - If $u_j = 0$ for $j \\in \\{A,B\\}$, then the ON-transition intensity is zero, so $s_j(t) = 1$ is not reachable from $s_j(t) = 0$; OFF is absorbing for that gene. The OFF-to-ON jump has intensity $0$, and the ON-to-OFF jump has strictly positive intensity (so $s_j=1$ cannot persist unless initially set, which it is not).\n   - If $u_j = 1$, then both ON and OFF are reachable, with strictly positive intensities for both transitions.\n   These constraints imply that, for each $j \\in \\{A,B\\}$, the set of reachable synthesis regimes is $\\{k_{j,\\mathrm{off}}\\}$ if $u_j = 0$ and $\\{k_{j,\\mathrm{off}}, k_{j,\\mathrm{on}}\\}$ if $u_j = 1$.\n3. Deterministic flows between jumps are:\n   - For $j \\in \\{A,B\\}$, the concentration $x_j$ satisfies\n     $$ \\frac{dx_j}{dt} = s_j \\, k_{j,\\mathrm{on}} + (1 - s_j)\\, k_{j,\\mathrm{off}} - d_j \\, x_j, $$\n     where $k_{j,\\mathrm{on}}  0$, $k_{j,\\mathrm{off}} \\ge 0$ and $d_j  0$.\n   - The output concentration $x_O$ satisfies\n     $$ \\frac{dx_O}{dt} = k_O \\, H(x_A;K,n) \\, H(x_B;K,n) - d_O \\, x_O, $$\n     where $k_O  0$, $d_O  0$, the Hill function is\n     $$ H(x;K,n) = \\frac{x^n}{K^n + x^n}, $$\n     with $K  0$ (effective dissociation constant) and $n \\ge 1$ (Hill coefficient).\n4. Initial conditions at time $t=0$ are\n   $$ s_A(0) = 0, \\quad s_B(0) = 0, \\quad x_A(0) = 0, \\quad x_B(0) = 0, \\quad x_O(0) = 0. $$\n5. Time horizon is $[0,T]$ for a given $T  0$.\n\nReachability objective:\n- The reachable set at time $t$ is the set of concentrations $x(t)$ that can be attained by at least one PDMP trajectory with positive probability, given the above constraints.\n- Construct a sound over-approximation of the reachable set for $x_A$ and $x_B$ by using the comparison principle: for each $j \\in \\{A,B\\}$, given the set of reachable synthesis rates $\\mathcal{A}_j \\subseteq \\{k_{j,\\mathrm{off}}, k_{j,\\mathrm{on}}\\}$ implied by the input $u_j$, the dynamics of $x_j$ are bounded by the differential inclusion\n  $$ \\frac{dx_j}{dt} \\in [\\min \\mathcal{A}_j - d_j x_j, \\; \\max \\mathcal{A}_j - d_j x_j]. $$\n  Use this to produce lower and upper bounds, $x_j^{\\mathrm{min}}(t)$ and $x_j^{\\mathrm{max}}(t)$, for all $t \\in [0,T]$.\n- Use the monotonicity of the Hill function in each argument to bound the output production rate and then propagate an interval $[x_O^{\\mathrm{min}}(t), x_O^{\\mathrm{max}}(t)]$ over time via the linear first-order differential inequality\n  $$ \\frac{dx_O}{dt} \\in [a_O^{\\mathrm{min}}(t) - d_O x_O,\\; a_O^{\\mathrm{max}}(t) - d_O x_O], $$\n  where\n  $$ a_O^{\\mathrm{min}}(t) = k_O \\, H\\!\\big(x_A^{\\mathrm{min}}(t);K,n\\big)\\, H\\!\\big(x_B^{\\mathrm{min}}(t);K,n\\big), \\quad\n     a_O^{\\mathrm{max}}(t) = k_O \\, H\\!\\big(x_A^{\\mathrm{max}}(t);K,n\\big)\\, H\\!\\big(x_B^{\\mathrm{max}}(t);K,n\\big). $$\n  Implement the bound propagation numerically by time-discretizing the horizon $[0,T]$ into uniform steps of size $\\Delta t  0$ and applying the exact affine solution over each step assuming $a_O$ is held at its respective bound within the step.\n\nSafety specification:\n- Define the unsafe set for a given input $u$ as spurious high output when the AND condition is not met:\n  $$ \\mathcal{U}_{\\mathrm{unsafe}}(u) = \\begin{cases}\n     \\{ x \\in \\mathbb{R}_{\\ge 0}^3 \\mid x_O \\ge \\theta_H \\},  \\text{if } u \\neq (1,1), \\\\\n     \\varnothing,  \\text{if } u = (1,1),\n  \\end{cases} $$\n  where $\\theta_H  0$ is a given activation threshold.\n- Determine whether the over-approximated reachable set of the PDMP on $[0,T]$ intersects $\\mathcal{U}_{\\mathrm{unsafe}}(u)$. Concretely, for $u \\neq (1,1)$, report True if there exists $t \\in [0,T]$ such that $x_O^{\\mathrm{max}}(t) \\ge \\theta_H$, and False otherwise. For $u = (1,1)$, by definition the unsafe set is empty and the answer is False.\n\nNumerical implementation requirements:\n- Use a fixed time step $\\Delta t$ to discretize $[0,T]$.\n- For $j \\in \\{A,B\\}$, propagate $x_j^{\\mathrm{min}}$ and $x_j^{\\mathrm{max}}$ using the exact solution of the linear subproblems over each step given constant synthesis $a \\in \\{\\min \\mathcal{A}_j, \\max \\mathcal{A}_j\\}$ on that step.\n- For the output, at each step compute $a_O^{\\mathrm{min}}$ and $a_O^{\\mathrm{max}}$ from the current bounds, then propagate $x_O^{\\mathrm{min}}$ and $x_O^{\\mathrm{max}}$ over the step using the exact affine update for constant input over the step.\n\nTest suite:\nFor each test case $i \\in \\{1,2,3,4\\}$, you are given $(u_A,u_B)$ and parameters. Unless otherwise specified, use the following defaults:\n- $k_{A,\\mathrm{on}} = k_{B,\\mathrm{on}} = 100$, $k_{A,\\mathrm{off}} = k_{B,\\mathrm{off}} = 1$, $d_A = d_B = 1$,\n- $k_O = 200$, $d_O = 1$, $K = 50$, $n = 2$,\n- $T = 10$, $\\Delta t = 0.01$, $\\theta_H = 60$.\nAll numbers are dimensionless for computational purposes but represent consistent physical units (for example, concentrations in arbitrary units and time in hours).\n- Test 1 (both inputs low, nominal parameters): $u = (0,0)$, use defaults.\n- Test 2 (asymmetric input, nominal parameters): $u = (1,0)$, use defaults.\n- Test 3 (asymmetric input, ultrasensitive output promoter): $u = (1,0)$, set $K = 0.5$; other parameters default.\n- Test 4 (both inputs low, high leak in upstream units): $u = (0,0)$, set $k_{A,\\mathrm{off}} = k_{B,\\mathrm{off}} = 60$; other parameters default.\n\nRequired output:\n- For each test case, output a boolean indicating whether the unsafe set is reachable (True if reachable, False otherwise), according to the over-approximation described above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[True,False,True,False]”). No extra characters or lines are permitted.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the principles of molecular biology and dynamical systems, is mathematically well-posed, and all necessary parameters and conditions are provided for a unique computational solution. The task is to perform a reachability analysis for a hybrid model of a synthetic gene circuit and verify a safety property.\n\nThe core of the problem lies in constructing a sound over-approximation of the reachable set of concentrations for a Piecewise Deterministic Markov Process (PDMP) model. This is achieved by propagating interval bounds for the state variables over time. The propagation relies on the comparison principle for differential equations and the monotonicity of the functions involved.\n\nThe methodology proceeds in three main steps: propagating the bounds for the input gene products ($x_A$, $x_B$), propagating the bounds for the output gene product ($x_O$), and performing the safety check at each time step.\n\nFirst, we determine the bounds for the concentrations of the input gene products, $x_A$ and $x_B$. For each gene $j \\in \\{A, B\\}$, the dynamics are given by the ordinary differential equation (ODE):\n$$ \\frac{dx_j}{dt} = \\alpha_j - d_j x_j $$\nwhere the synthesis rate $\\alpha_j$ depends on the promoter state $s_j \\in \\{0, 1\\}$, taking values from the set $\\{k_{j,\\mathrm{off}}, k_{j,\\mathrm{on}}\\}$. The problem specifies that the reachable set for $x_j$ can be over-approximated by solving a differential inclusion. The set of possible synthesis rates, $\\mathcal{A}_j$, is determined by the external input $u_j \\in \\{0,1\\}$:\n- If $u_j = 0$, the promoter is locked in the OFF state ($s_j=0$), so $\\mathcal{A}_j = \\{k_{j,\\mathrm{off}}\\}$.\n- If $u_j = 1$, the promoter can switch between OFF and ON states, so $\\mathcal{A}_j = \\{k_{j,\\mathrm{off}}, k_{j,\\mathrm{on}}\\}$.\n\nThis leads to two linear ODEs for the lower bound $x_j^{\\mathrm{min}}(t)$ and upper bound $x_j^{\\mathrm{max}}(t)$:\n$$ \\frac{dx_j^{\\mathrm{min}}}{dt} = \\min(\\mathcal{A}_j) - d_j x_j^{\\mathrm{min}} $$\n$$ \\frac{dx_j^{\\mathrm{max}}}{dt} = \\max(\\mathcal{A}_j) - d_j x_j^{\\mathrm{max}} $$\nwith initial conditions $x_j^{\\mathrm{min}}(0) = x_j^{\\mathrm{max}}(0) = 0$.\nThe solution to an ODE of the form $\\frac{dy}{dt} = a - d \\cdot y$ over a time interval $[t_k, t_{k+1}]$ of duration $\\Delta t = t_{k+1} - t_k$, with constant $a$ and $d$, is:\n$$ y(t_{k+1}) = y(t_k) e^{-d \\Delta t} + \\frac{a}{d} (1 - e^{-d \\Delta t}) $$\nThis formula is used to update $x_j^{\\mathrm{min}}$ and $x_j^{\\mathrm{max}}$ at each discrete time step, by setting $a = \\min(\\mathcal{A}_j)$ and $a = \\max(\\mathcal{A}_j)$, respectively.\n\nSecond, we propagate the bounds for the output concentration, $x_O$. The dynamics of $x_O$ are nonlinear:\n$$ \\frac{dx_O}{dt} = k_O \\, H(x_A;K,n) \\, H(x_B;K,n) - d_O \\, x_O $$\nwhere the Hill function $H(x;K,n) = \\frac{x^n}{K^n + x^n}$ is monotonically increasing with $x$ for $x \\ge 0$. This monotonicity allows us to bound the synthesis rate for $x_O$ at any time $t$ using the bounds on $x_A(t)$ and $x_B(t)$:\n$$ \\frac{dx_O}{dt} \\in [a_O^{\\mathrm{min}}(t) - d_O x_O, \\; a_O^{\\mathrm{max}}(t) - d_O x_O] $$\nwhere\n$$ a_O^{\\mathrm{min}}(t) = k_O \\, H(x_A^{\\mathrm{min}}(t);K,n) \\, H(x_B^{\\mathrm{min}}(t);K,n) $$\n$$ a_O^{\\mathrm{max}}(t) = k_O \\, H(x_A^{\\mathrm{max}}(t);K,n) \\, H(x_B^{\\mathrm{max}}(t);K,n) $$\nFor the numerical integration over a time step $[t_k, t_{k+1}]$, we approximate the time-varying synthesis rates $a_O^{\\mathrm{min}}(t)$ and $a_O^{\\mathrm{max}}(t)$ by holding them constant at their values at the start of the interval, $t_k$. Let $a_{O,k}^{\\mathrm{min}} = a_O^{\\mathrm{min}}(t_k)$ and $a_{O,k}^{\\mathrm{max}} = a_O^{\\mathrm{max}}(t_k)$. The updates for the bounds $x_O^{\\mathrm{min}}$ and $x_O^{\\mathrm{max}}$ from $t_k$ to $t_{k+1}$ are then given by the same affine solution:\n$$ x_O^{\\mathrm{min}}(t_{k+1}) = x_O^{\\mathrm{min}}(t_k) e^{-d_O \\Delta t} + \\frac{a_{O,k}^{\\mathrm{min}}}{d_O} (1 - e^{-d_O \\Delta t}) $$\n$$ x_O^{\\mathrm{max}}(t_{k+1}) = x_O^{\\mathrm{max}}(t_k) e^{-d_O \\Delta t} + \\frac{a_{O,k}^{\\mathrm{max}}}{d_O} (1 - e^{-d_O \\Delta t}) $$\n\nThird, the safety verification is performed. The unsafe set for an input $u \\neq (1,1)$ is defined by $\\{ x \\mid x_O \\ge \\theta_H \\}$. The system is deemed unsafe if the over-approximated reachable set intersects this unsafe set. This is checked by verifying if $x_O^{\\mathrm{max}}(t_k) \\ge \\theta_H$ at any time step $t_k$ in the horizon $[0, T]$. For $u = (1,1)$, the unsafe set is empty, so the system is safe by definition.\n\nThe overall algorithm is as follows:\n1. For each test case, define the parameters and input $u$.\n2. If $u = (1,1)$, the result is `False`.\n3. Otherwise, initialize all concentration bounds to $0$.\n4. Determine the synthesis rate bounds for genes A and B based on $u$.\n5. Discretize the time horizon $[0, T]$ into steps of size $\\Delta t$.\n6. In a loop over the time steps:\n   a. Update the bounds $[x_A^{\\mathrm{min}}, x_A^{\\mathrm{max}}]$ and $[x_B^{\\mathrm{min}}, x_B^{\\mathrm{max}}]$.\n   b. Calculate the output synthesis bounds $[a_O^{\\mathrm{min}}, a_O^{\\mathrm{max}}]$ using the updated input concentration bounds.\n   c. Update the output bounds $[x_O^{\\mathrm{min}}, x_O^{\\mathrm{max}}]$.\n   d. Check if the safety condition is violated: $x_O^{\\mathrm{max}} \\ge \\theta_H$. If true, the system is unsafe for this case; terminate and report `True`.\n7. If the loop completes without any violation, the system is considered safe; report `False`.\n\nThis procedure is implemented below to determine the safety status for each of the four specified test cases.", "answer": "```python\nimport numpy as np\n\ndef _run_single_case(params):\n    \"\"\"\n    Runs a single simulation for a given set of parameters to check for safety violations.\n    \"\"\"\n    u = params['u']\n    k_A_on = params['k_A_on']\n    k_A_off = params['k_A_off']\n    d_A = params['d_A']\n    k_B_on = params['k_B_on']\n    k_B_off = params['k_B_off']\n    d_B = params['d_B']\n    k_O = params['k_O']\n    d_O = params['d_O']\n    K = params['K']\n    n = params['n']\n    T = params['T']\n    dt = params['dt']\n    theta_H = params['theta_H']\n\n    # Unpack input vector\n    u_A, u_B = u\n\n    # For u=(1,1), the unsafe set is empty, so it's always safe.\n    if u_A == 1 and u_B == 1:\n        return False\n\n    # Determine synthesis rate bounds based on inputs\n    k_A_min = k_A_off\n    k_A_max = k_A_on if u_A == 1 else k_A_off\n    \n    k_B_min = k_B_off\n    k_B_max = k_B_on if u_B == 1 else k_B_off\n\n    # Initial conditions\n    x_A_min, x_A_max = 0.0, 0.0\n    x_B_min, x_B_max = 0.0, 0.0\n    x_O_min, x_O_max = 0.0, 0.0\n\n    # Pre-calculate constant terms for the affine update rule\n    exp_A = np.exp(-d_A * dt)\n    exp_B = np.exp(-d_B * dt)\n    exp_O = np.exp(-d_O * dt)\n\n    term_A_min = (k_A_min / d_A) * (1 - exp_A)\n    term_A_max = (k_A_max / d_A) * (1 - exp_A)\n    term_B_min = (k_B_min / d_B) * (1 - exp_B)\n    term_B_max = (k_B_max / d_B) * (1 - exp_B)\n\n    # Time discretization\n    num_steps = int(T / dt)\n\n    def hill_function(x, K_val, n_val):\n        # A small epsilon to prevent division by zero if K=0, though K>0 is assumed.\n        x_n = x**n_val\n        K_n = K_val**n_val\n        return x_n / (K_n + x_n)\n\n    for _ in range(num_steps):\n        # Update bounds for x_A and x_B\n        x_A_min = x_A_min * exp_A + term_A_min\n        x_A_max = x_A_max * exp_A + term_A_max\n        x_B_min = x_B_min * exp_B + term_B_min\n        x_B_max = x_B_max * exp_B + term_B_max\n        \n        # Calculate bounds for output synthesis rate a_O\n        a_O_min = k_O * hill_function(x_A_min, K, n) * hill_function(x_B_min, K, n)\n        a_O_max = k_O * hill_function(x_A_max, K, n) * hill_function(x_B_max, K, n)\n\n        # Update bounds for x_O\n        term_O_min = (a_O_min / d_O) * (1 - exp_O)\n        term_O_max = (a_O_max / d_O) * (1 - exp_O)\n        x_O_min = x_O_min * exp_O + term_O_min\n        x_O_max = x_O_max * exp_O + term_O_max\n\n        # Check for safety violation\n        if x_O_max >= theta_H:\n            return True\n\n    # If loop completes without violation, it's safe\n    return False\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the gene circuit safety verification problem.\n    \"\"\"\n    default_params = {\n        'k_A_on': 100.0, 'k_A_off': 1.0, 'd_A': 1.0,\n        'k_B_on': 100.0, 'k_B_off': 1.0, 'd_B': 1.0,\n        'k_O': 200.0, 'd_O': 1.0, 'K': 50.0, 'n': 2.0,\n        'T': 10.0, 'dt': 0.01, 'theta_H': 60.0\n    }\n\n    test_cases = [\n        # Test 1: u=(0,0), nominal parameters\n        {'u': (0, 0)},\n        # Test 2: u=(1,0), nominal parameters\n        {'u': (1, 0)},\n        # Test 3: u=(1,0), ultrasensitive output promoter (low K)\n        {'u': (1, 0), 'K': 0.5},\n        # Test 4: u=(0,0), high leak in upstream units\n        {'u': (0, 0), 'k_A_off': 60.0, 'k_B_off': 60.0}\n    ]\n\n    results = []\n    for overrides in test_cases:\n        params = default_params.copy()\n        params.update(overrides)\n        result = _run_single_case(params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results)).replace('True', 'True').replace('False', 'False')}]\")\n\nsolve()\n```", "id": "2723266"}]}