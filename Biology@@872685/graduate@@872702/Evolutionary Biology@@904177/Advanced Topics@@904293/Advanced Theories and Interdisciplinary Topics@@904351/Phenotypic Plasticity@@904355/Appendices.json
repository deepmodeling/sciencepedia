{"hands_on_practices": [{"introduction": "The linear reaction norm is a foundational tool for quantifying phenotypic plasticity. In this model, the phenotype ($z$) is expressed as a linear function of the environment ($E$), described by an intercept ($a$) and a slope ($b$). This exercise will guide you through deriving the estimates for these parameters from experimental data using the method of least squares [@problem_id:2741824]. Mastering this fundamental technique is the first step toward understanding how to measure plasticity and interpret its components: the baseline phenotype and the environmental sensitivity.", "problem": "A single genotype is assayed for a quantitative trait under two thermal environments, and the phenotype is modeled by a linear reaction norm. Let the environment variable be defined as temperature deviation in degrees Celsius from a baseline of $25$ degrees Celsius, so that $E = 0$ corresponds to $25$ degrees Celsius. The observed data for this genotype are: at environment $E_1 = -5$ (i.e., $20$ degrees Celsius), the phenotype is $z_1 = 6.4$ milligrams, and at environment $E_2 = +5$ (i.e., $30$ degrees Celsius), the phenotype is $z_2 = 5.2$ milligrams. Assume the linear reaction norm\n$$\nz(E) = a + b E + \\varepsilon,\n$$\nwhere $a$ and $b$ are genotype-specific parameters and $\\varepsilon$ is a residual with mean zero. Starting from the definitions of linear regression and the least squares principle, derive the least-squares estimates $\\hat{a}$ and $\\hat{b}$ using these two observations, compute their numerical values, and interpret the biological meaning of $\\hat{a}$ and $\\hat{b}$ in terms of phenotypic plasticity. Express $\\hat{a}$ in milligrams and $\\hat{b}$ in milligrams per degree Celsius. Report the final numerical estimates for $a$ and $b$ in that order, using exact decimal values with no rounding.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n- Model for phenotype: linear reaction norm, $z(E) = a + b E + \\varepsilon$.\n- Environmental variable $E$: Temperature deviation in degrees Celsius from a baseline of $25$ degrees Celsius. $E=0$ corresponds to $25^\\circ\\text{C}$.\n- Observation 1: At $E_1 = -5$, the phenotype is $z_1 = 6.4$ milligrams.\n- Observation 2: At $E_2 = 5$, the phenotype is $z_2 = 5.2$ milligrams.\n- Parameters $a$ and $b$ are genotype-specific.\n- Residual $\\varepsilon$ has a mean of zero.\n- Task: Derive least-squares estimates $\\hat{a}$ and $\\hat{b}$ from first principles, compute their values, and interpret their biological meaning.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, utilizing the standard linear reaction norm model from quantitative genetics to describe phenotypic plasticity. It is well-posed; with two data points, there exists a unique linear function that fits them perfectly, and the method of least squares will yield this unique solution. The given data are complete and consistent for determining the two parameters of the linear model. The values are physically realistic. The problem is objective and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full derivation and solution will be provided.\n\nThe principle of least squares dictates that the estimates for the parameters $a$ and $b$, denoted $\\hat{a}$ and $\\hat{b}$, are the values that minimize the sum of squared residuals, $S$. The residual for an observation $(E_i, z_i)$ is $\\varepsilon_i = z_i - (a + b E_i)$. The sum of squared residuals for the two given observations is:\n$$\nS(a, b) = \\sum_{i=1}^{2} \\left[z_i - (a + b E_i)\\right]^2 = \\left[z_1 - (a + b E_1)\\right]^2 + \\left[z_2 - (a + b E_2)\\right]^2\n$$\nTo find the minimum of this function, we must compute its partial derivatives with respect to $a$ and $b$ and set them to zero. This yields a system of two linear equations known as the normal equations.\n\nFirst, the partial derivative with respect to $a$:\n$$\n\\frac{\\partial S}{\\partial a} = \\frac{\\partial}{\\partial a} \\left( [z_1 - a - b E_1]^2 + [z_2 - a - b E_2]^2 \\right) = 2(z_1 - a - b E_1)(-1) + 2(z_2 - a - b E_2)(-1)\n$$\nSetting $\\frac{\\partial S}{\\partial a} = 0$:\n$$\n-2(z_1 - \\hat{a} - \\hat{b} E_1) - 2(z_2 - \\hat{a} - \\hat{b} E_2) = 0\n$$\n$$\n(z_1 + z_2) - 2\\hat{a} - \\hat{b}(E_1 + E_2) = 0\n$$\n$$\n2\\hat{a} + \\hat{b}(E_1 + E_2) = z_1 + z_2 \\quad \\quad (1)\n$$\n\nSecond, the partial derivative with respect to $b$:\n$$\n\\frac{\\partial S}{\\partial b} = \\frac{\\partial}{\\partial b} \\left( [z_1 - a - b E_1]^2 + [z_2 - a - b E_2]^2 \\right) = 2(z_1 - a - b E_1)(-E_1) + 2(z_2 - a - b E_2)(-E_2)\n$$\nSetting $\\frac{\\partial S}{\\partial b} = 0$:\n$$\n-2E_1(z_1 - \\hat{a} - \\hat{b} E_1) - 2E_2(z_2 - \\hat{a} - \\hat{b} E_2) = 0\n$$\n$$\n-E_1z_1 + \\hat{a}E_1 + \\hat{b}E_1^2 - E_2z_2 + \\hat{a}E_2 + \\hat{b}E_2^2 = 0\n$$\n$$\n\\hat{a}(E_1 + E_2) + \\hat{b}(E_1^2 + E_2^2) = E_1z_1 + E_2z_2 \\quad \\quad (2)\n$$\n\nNow we solve the system of normal equations (1) and (2) for $\\hat{a}$ and $\\hat{b}$. We substitute the given environmental values: $E_1 = -5$ and $E_2 = 5$.\nA significant simplification arises from the fact that the environments are symmetric about zero:\n$$\nE_1 + E_2 = -5 + 5 = 0\n$$\nSubstituting this into equation (1):\n$$\n2\\hat{a} + \\hat{b}(0) = z_1 + z_2 \\implies 2\\hat{a} = z_1 + z_2\n$$\nThis gives the estimator for the intercept:\n$$\n\\hat{a} = \\frac{z_1 + z_2}{2}\n$$\nThe estimator $\\hat{a}$ is simply the arithmetic mean of the observed phenotypes. This is a direct consequence of the experimental design where the mean of the environmental variables, $\\bar{E} = (E_1+E_2)/2$, is zero.\n\nNext, we substitute $E_1+E_2=0$ into equation (2):\n$$\n\\hat{a}(0) + \\hat{b}(E_1^2 + E_2^2) = E_1z_1 + E_2z_2 \\implies \\hat{b}(E_1^2 + E_2^2) = E_1z_1 + E_2z_2\n$$\nThis gives the estimator for the slope:\n$$\n\\hat{b} = \\frac{E_1z_1 + E_2z_2}{E_1^2 + E_2^2}\n$$\nIt is a useful check to show this is equivalent to the standard formula for the slope of a line passing through two points, $\\frac{z_2 - z_1}{E_2 - E_1}$. Since $E_1 = -E_2$, let $E_2 = E_0 = 5$. Then $E_1 = -E_0 = -5$.\n$$\n\\hat{b} = \\frac{(-E_0)z_1 + E_0 z_2}{(-E_0)^2 + E_0^2} = \\frac{E_0(z_2 - z_1)}{2E_0^2} = \\frac{z_2 - z_1}{2E_0}\n$$\nAnd since $E_2 - E_1 = E_0 - (-E_0) = 2E_0$, the formula is indeed identical. The derivation from the least squares principle is thus confirmed.\n\nNow we compute the numerical values using the given data: $z_1 = 6.4$ and $z_2 = 5.2$.\n$$\n\\hat{a} = \\frac{6.4 + 5.2}{2} = \\frac{11.6}{2} = 5.8\n$$\n$$\n\\hat{b} = \\frac{z_2 - z_1}{E_2 - E_1} = \\frac{5.2 - 6.4}{5 - (-5)} = \\frac{-1.2}{10} = -0.12\n$$\n\nThe biological interpretation of these parameters is as follows:\n- $\\hat{a}$: This is the intercept of the reaction norm, which is the predicted phenotypic value when the environmental variable $E$ is zero. In this problem, $E=0$ corresponds to the baseline temperature of $25$ degrees Celsius. Therefore, $\\hat{a} = 5.8$ milligrams is the estimated mass of the genotype when it develops at $25^\\circ \\text{C}$. It represents the phenotype in the central or reference environment of the study.\n\n- $\\hat{b}$: This is the slope of the reaction norm. It quantifies the sensitivity of the phenotype to changes in the environment, which is the definition of phenotypic plasticity. The value $\\hat{b} = -0.12$ milligrams per degree Celsius indicates that for every $1^\\circ \\text{C}$ increase in temperature from the baseline, the phenotype (mass) is predicted to decrease by $0.12$ milligrams. The fact that $\\hat{b}$ is non-zero demonstrates that the genotype is phenotypically plastic for this trait. The negative sign specifies the direction of this plasticity—a decrease in mass with an increase in temperature, a common pattern known as the temperature-size rule in many ectotherms.", "answer": "$$\n\\boxed{\\begin{pmatrix} 5.8  -0.12 \\end{pmatrix}}\n$$", "id": "2741824"}, {"introduction": "Once we can describe a reaction norm, the next question is how it evolves. Natural selection acts directly on the phenotypes expressed in different environments, but this induces indirect selection on the underlying genetic parameters that define the reaction norm. In this practice problem, you will explore this crucial link by deriving the directional selection gradients acting on the intercept ($a$) and slope ($b$) of a linear reaction norm when the population is under stabilizing selection toward a moving optimum [@problem_id:2741888]. This will reveal how the interplay between the organism's response and the environment's demands drives the evolution of plasticity.", "problem": "Consider a population expressing a linear reaction norm for a scalar phenotype across a scalar environment. The expressed phenotype in environment $E$ is $z(E)=a+bE$, where $a$ is the reaction-norm intercept and $b$ is the reaction-norm slope. Viability selection is stabilizing and centered on an environmentally dependent optimum phenotype $z^{*}(E)=\\alpha+\\beta E$. Absolute fitness in environment $E$ is Gaussian stabilizing selection around the optimum with width parameter $V_{s}0$, so that\n$$\nW(E; a,b)=W_{0}\\exp\\!\\left(-\\frac{\\left[z(E)-z^{*}(E)\\right]^{2}}{2V_{s}}\\right),\n$$\nwhere $W_{0}0$ is a constant baseline fitness.\n\nAssume that within a generation, each individual encounters a single environmental value $E$ drawn from a distribution that is independent of genotype, with finite mean $\\mu_{E}$ and variance $\\sigma_{E}^{2}$. Let the Malthusian (log) fitness be $m(E; a,b)=\\ln W(E; a,b)$. Define the directional selection gradient on the reaction-norm parameters as the gradient of expected Malthusian fitness with respect to the parameters, evaluated at the current $(a,b)$:\n$$\n\\nabla_{(a,b)}\\,\\mathbb{E}\\!\\left[m(E; a,b)\\right]=\\left(\\frac{\\partial}{\\partial a}\\mathbb{E}[m(E; a,b)],\\ \\frac{\\partial}{\\partial b}\\mathbb{E}[m(E; a,b)]\\right),\n$$\nwhere the expectation is with respect to the environmental distribution.\n\nDerive a closed-form expression for this two-component directional selection gradient in terms of $a$, $b$, $\\alpha$, $\\beta$, $\\mu_{E}$, $\\sigma_{E}^{2}$, and $V_{s}$. Express your final result as a single $1\\times 2$ row matrix. No numerical evaluation is required and no units are associated with the answer.", "solution": "The problem as stated is scientifically grounded, well-posed, and free of contradictions or ambiguities. It represents a standard calculation in the field of evolutionary quantitative genetics. We may therefore proceed with the derivation.\n\nThe objective is to compute the directional selection gradient on the reaction-norm parameters $(a, b)$, which is defined as the gradient of the expected Malthusian fitness:\n$$\n\\nabla_{(a,b)}\\,\\mathbb{E}\\!\\left[m(E; a,b)\\right]=\\left(\\frac{\\partial}{\\partial a}\\mathbb{E}[m(E; a,b)],\\ \\frac{\\partial}{\\partial b}\\mathbb{E}[m(E; a,b)]\\right)\n$$\nThe Malthusian fitness, $m(E; a, b)$, is the natural logarithm of the absolute fitness $W(E; a, b)$. First, we write the expression for $m(E; a, b)$.\nGiven the absolute fitness function:\n$$\nW(E; a,b)=W_{0}\\exp\\!\\left(-\\frac{\\left[z(E)-z^{*}(E)\\right]^{2}}{2V_{s}}\\right)\n$$\nThe Malthusian fitness is:\n$$\nm(E; a,b) = \\ln\\left(W(E; a,b)\\right) = \\ln\\left(W_{0}\\exp\\!\\left(-\\frac{\\left[z(E)-z^{*}(E)\\right]^{2}}{2V_{s}}\\right)\\right)\n$$\n$$\nm(E; a,b) = \\ln(W_{0}) - \\frac{\\left[z(E)-z^{*}(E)\\right]^{2}}{2V_{s}}\n$$\nThe deviation from the optimum phenotype is given by substituting the expressions for the reaction norm, $z(E) = a+bE$, and the optimal phenotype, $z^{*}(E) = \\alpha+\\beta E$:\n$$\nz(E) - z^{*}(E) = (a+bE) - (\\alpha+\\beta E) = (a-\\alpha) + (b-\\beta)E\n$$\nSubstituting this into the Malthusian fitness expression gives:\n$$\nm(E; a,b) = \\ln(W_{0}) - \\frac{1}{2V_{s}}\\left[(a-\\alpha) + (b-\\beta)E\\right]^{2}\n$$\nTo find the selection gradient, we must compute the partial derivatives of the expected Malthusian fitness, $\\mathbb{E}[m(E; a, b)]$, with respect to $a$ and $b$. Because the expectation is a linear operator and the partial derivatives with respect to $a$ and $b$ are also linear operators, we can interchange the order of expectation and differentiation. This simplifies the procedure. We first find the partial derivatives of $m(E; a, b)$ with respect to $a$ and $b$, and then take the expectation of the resulting expressions over the distribution of the environment $E$.\n\nThe partial derivative of $m(E; a, b)$ with respect to $a$ is:\n$$\n\\frac{\\partial m}{\\partial a} = \\frac{\\partial}{\\partial a}\\left(\\ln(W_{0}) - \\frac{1}{2V_{s}}\\left[(a-\\alpha) + (b-\\beta)E\\right]^{2}\\right)\n$$\nUsing the chain rule:\n$$\n\\frac{\\partial m}{\\partial a} = - \\frac{1}{2V_{s}} \\cdot 2 \\left[(a-\\alpha) + (b-\\beta)E\\right] \\cdot \\frac{\\partial}{\\partial a}\\left(a-\\alpha\\right)\n$$\n$$\n\\frac{\\partial m}{\\partial a} = - \\frac{1}{V_{s}} \\left[(a-\\alpha) + (b-\\beta)E\\right]\n$$\nThe partial derivative of $m(E; a, b)$ with respect to $b$ is:\n$$\n\\frac{\\partial m}{\\partial b} = \\frac{\\partial}{\\partial b}\\left(\\ln(W_{0}) - \\frac{1}{2V_{s}}\\left[(a-\\alpha) + (b-\\beta)E\\right]^{2}\\right)\n$$\nUsing the chain rule:\n$$\n\\frac{\\partial m}{\\partial b} = - \\frac{1}{2V_{s}} \\cdot 2 \\left[(a-\\alpha) + (b-\\beta)E\\right] \\cdot \\frac{\\partial}{\\partial b}\\left((b-\\beta)E\\right)\n$$\n$$\n\\frac{\\partial m}{\\partial b} = - \\frac{1}{V_{s}} \\left[(a-\\alpha) + (b-\\beta)E\\right] \\cdot E\n$$\n$$\n\\frac{\\partial m}{\\partial b} = - \\frac{1}{V_{s}} \\left[(a-\\alpha)E + (b-\\beta)E^{2}\\right]\n$$\nNow, we compute the expectation of these partial derivatives. The terms $a, b, \\alpha, \\beta, V_s$ are constants with respect to the environmental distribution. We are given $\\mathbb{E}[E] = \\mu_{E}$ and $\\text{Var}(E) = \\sigma_{E}^{2}$, which implies $\\mathbb{E}[E^2] = \\text{Var}(E) + (\\mathbb{E}[E])^{2} = \\sigma_{E}^{2} + \\mu_{E}^{2}$.\n\nThe first component of the selection gradient is:\n$$\n\\frac{\\partial}{\\partial a}\\mathbb{E}[m(E; a,b)] = \\mathbb{E}\\left[\\frac{\\partial m}{\\partial a}\\right] = \\mathbb{E}\\left[- \\frac{1}{V_{s}} \\left((a-\\alpha) + (b-\\beta)E\\right)\\right]\n$$\n$$\n= - \\frac{1}{V_{s}} \\left(\\mathbb{E}[a-\\alpha] + \\mathbb{E}[(b-\\beta)E]\\right) = - \\frac{1}{V_{s}} \\left((a-\\alpha) + (b-\\beta)\\mathbb{E}[E]\\right)\n$$\n$$\n= - \\frac{1}{V_{s}} \\left[(a-\\alpha) + (b-\\beta)\\mu_{E}\\right]\n$$\nThe second component of the selection gradient is:\n$$\n\\frac{\\partial}{\\partial b}\\mathbb{E}[m(E; a,b)] = \\mathbb{E}\\left[\\frac{\\partial m}{\\partial b}\\right] = \\mathbb{E}\\left[- \\frac{1}{V_{s}} \\left((a-\\alpha)E + (b-\\beta)E^{2}\\right)\\right]\n$$\n$$\n= - \\frac{1}{V_{s}} \\left(\\mathbb{E}[(a-\\alpha)E] + \\mathbb{E}[(b-\\beta)E^{2}]\\right) = - \\frac{1}{V_{s}} \\left((a-\\alpha)\\mathbb{E}[E] + (b-\\beta)\\mathbb{E}[E^{2}]\\right)\n$$\n$$\n= - \\frac{1}{V_{s}} \\left[(a-\\alpha)\\mu_{E} + (b-\\beta)(\\sigma_{E}^{2} + \\mu_{E}^{2})\\right]\n$$\nThis expression for the second component can be rearranged to reveal its relationship with the first component:\n$$\n- \\frac{1}{V_{s}} \\left[(a-\\alpha)\\mu_{E} + (b-\\beta)\\mu_{E}^{2} + (b-\\beta)\\sigma_{E}^{2}\\right] = - \\frac{1}{V_{s}} \\left[\\mu_{E}\\left((a-\\alpha) + (b-\\beta)\\mu_{E}\\right) + (b-\\beta)\\sigma_{E}^{2}\\right]\n$$\nThe directional selection gradient is the $1 \\times 2$ row matrix containing these two components:\n$$\n\\nabla_{(a,b)}\\,\\mathbb{E}[m] = \\begin{pmatrix} - \\frac{1}{V_{s}} \\left[(a-\\alpha) + (b-\\beta)\\mu_{E}\\right]  - \\frac{1}{V_{s}} \\left[(a-\\alpha)\\mu_{E} + (b-\\beta)(\\sigma_{E}^{2} + \\mu_{E}^{2})\\right] \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} - \\frac{1}{V_{s}} \\left[ (a - \\alpha) + (b - \\beta)\\mu_{E} \\right]  - \\frac{1}{V_{s}} \\left[ (a - \\alpha)\\mu_{E} + (b - \\beta)(\\sigma_{E}^{2} + \\mu_{E}^{2}) \\right] \\end{pmatrix}}\n$$", "id": "2741888"}, {"introduction": "While linear models are useful, many biological reaction norms are nonlinear, and expressing plasticity can be costly. This advanced exercise challenges you to model the evolution of a more flexible, quadratic reaction norm, which includes a curvature parameter ($c$) in addition to the intercept and slope [@problem_id:2741861]. By deriving the evolutionary equilibrium for all three parameters under stabilizing selection and explicit fitness costs, you will see how the statistical moments of the environmental distribution—not just its mean—play a critical role in shaping the optimal level of plasticity and the curvature of the response.", "problem": "You are asked to formalize and analyze a model of phenotypic plasticity where a genotype expresses a nonlinear reaction norm across an environmental gradient. The expressed phenotype in environment $e$ is given by the quadratic reaction norm $z(e) = a + b e + c e^2$, where $a$, $b$, and $c$ are the intercept, slope, and curvature, respectively. The environment-dependent optimal phenotype is $\\phi(e) = \\phi_0 + \\phi_1 e + \\phi_2 e^2$. Viability selection acts through stabilizing selection on the deviation between $z(e)$ and $\\phi(e)$ with width $\\sigma_s$, and there are costs associated with maintaining $a$, $b$, and $c$.\n\nFundamental base and assumptions to use:\n- Natural selection increases mean Malthusian fitness. Assume genotypes are rare enough and mutational steps are small enough that, on the relevant timescale, the resident reaction norm parameters $(a,b,c)$ evolve by maximizing the expected log fitness $\\mathbb{E}_e[\\ln W(e)]$.\n- The instantaneous fitness in environment $e$ is \n$$\nW(e) = \\exp\\!\\left(-\\frac{(z(e) - \\phi(e))^2}{2\\sigma_s^2}\\right)\\,\\exp\\!\\left(-\\kappa_a a^2 - \\kappa_b b^2 - \\kappa_c c^2\\right),\n$$\nwhere $\\sigma_s  0$ and $\\kappa_a,\\kappa_b,\\kappa_c \\ge 0$.\n- The environmental variable $e$ is a real-valued random variable drawn independently in each generation from a fixed distribution with finite raw moments up to order $4$, i.e., $m_k = \\mathbb{E}[e^k]$ exists for $k \\in \\{1,2,3,4\\}$.\n\nYour tasks:\n- Starting only from the definitions above and the principle that evolution under the stated assumptions maximizes $\\mathbb{E}_e[\\ln W(e)]$, derive the first-order conditions that characterize stationary values $(a^\\star,b^\\star,c^\\star)$ in terms of the environmental moments $m_1, m_2, m_3, m_4$, the selection width $\\sigma_s$, the cost coefficients $\\kappa_a,\\kappa_b,\\kappa_c$, and the optimal phenotype coefficients $\\phi_0,\\phi_1,\\phi_2$.\n- Using those first-order conditions, design an algorithm that, given $(m_1,\\dots,m_4)$, $(\\phi_0,\\phi_1,\\phi_2)$, $(\\kappa_a,\\kappa_b,\\kappa_c)$, and $\\sigma_s$, returns the stationary curvature $c^\\star$.\n\nThe answer must be a complete, runnable program that evaluates $c^\\star$ for each of the following test cases. There are no physical units; all quantities are dimensionless. Angles do not appear. Your program must implement the exact environmental raw moments $m_k = \\mathbb{E}[e^k]$ for the specified distributions.\n\n- Test case $1$ (happy path):\n  - Environment: Gaussian with mean $0$ and variance $1$, denoted $\\mathcal{N}(0,1)$.\n  - Parameters: $(\\phi_0,\\phi_1,\\phi_2) = (0, 1, 0.2)$, $(\\kappa_a,\\kappa_b,\\kappa_c) = (0.01, 0.01, 0.01)$, $\\sigma_s = 0.5$.\n- Test case $2$ (different spread and tails):\n  - Environment: Uniform on $[-1,1]$.\n  - Parameters: $(\\phi_0,\\phi_1,\\phi_2) = (0, 1, 0.2)$, $(\\kappa_a,\\kappa_b,\\kappa_c) = (0.01, 0.01, 0.01)$, $\\sigma_s = 0.5$.\n- Test case $3$ (bimodality):\n  - Environment: Symmetric bimodal mixture $\\tfrac{1}{2}\\mathcal{N}(-1, 0.2^2) + \\tfrac{1}{2}\\mathcal{N}(1, 0.2^2)$.\n  - Parameters: $(\\phi_0,\\phi_1,\\phi_2) = (0, 0, 0.2)$, $(\\kappa_a,\\kappa_b,\\kappa_c) = (0.01, 0.01, 0.01)$, $\\sigma_s = 0.5$.\n- Test case $4$ (boundary condition: extremely narrow environment):\n  - Environment: Gaussian with mean $0$ and variance $0.1^2$, i.e., $\\mathcal{N}(0, 0.01)$.\n  - Parameters: $(\\phi_0,\\phi_1,\\phi_2) = (0, 1, 0.2)$, $(\\kappa_a,\\kappa_b,\\kappa_c) = (0.01, 0.01, 0.01)$, $\\sigma_s = 0.5$.\n- Test case $5$ (boundary condition: very high curvature cost with opposite-sign optimum curvature):\n  - Environment: Gaussian with mean $0$ and variance $1$, denoted $\\mathcal{N}(0,1)$.\n  - Parameters: $(\\phi_0,\\phi_1,\\phi_2) = (0, 1, -0.3)$, $(\\kappa_a,\\kappa_b,\\kappa_c) = (0.01, 0.01, 5.0)$, $\\sigma_s = 0.5$.\n\nEnvironmental raw moments to be used by your program:\n- If $e \\sim \\mathcal{N}(\\mu,\\sigma^2)$, then $m_1 = \\mu$, $m_2 = \\mu^2 + \\sigma^2$, $m_3 = \\mu^3 + 3\\mu\\sigma^2$, $m_4 = \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4$.\n- If $e \\sim \\text{Uniform}[L,U]$, then $m_k = \\dfrac{U^{k+1} - L^{k+1}}{(k+1)(U-L)}$ for $k \\in \\{1,2,3,4\\}$.\n- If $e$ is the symmetric mixture $\\tfrac{1}{2}\\mathcal{N}(-\\mu,\\sigma^2) + \\tfrac{1}{2}\\mathcal{N}(\\mu,\\sigma^2)$, then $m_1 = 0$, $m_2 = \\mu^2 + \\sigma^2$, $m_3 = 0$, $m_4 = \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4$.\n\nFinal output specification:\n- For each test case in the order given above, compute the stationary curvature $c^\\star$ as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[x_1,x_2,x_3,x_4,x_5]$, where each $x_i$ is $c^\\star$ for test case $i$, rounded to exactly $6$ decimal places.", "solution": "The problem statement is valid. It is scientifically grounded in the theory of phenotypic plasticity, well-posed, objective, and provides all necessary information for a unique solution. I will proceed with the derivation and solution.\n\nThe task is to find the stationary values $(a^\\star, b^\\star, c^\\star)$ of the reaction norm parameters that maximize the expected log-fitness, $\\mathbb{E}_e[\\ln W(e)]$.\n\nThe fitness function in environment $e$ is given by:\n$$W(e) = \\exp\\!\\left(-\\frac{(z(e) - \\phi(e))^2}{2\\sigma_s^2}\\right)\\,\\exp\\!\\left(-\\kappa_a a^2 - \\kappa_b b^2 - \\kappa_c c^2\\right)$$\nwhere the reaction norm is $z(e) = a + b e + c e^2$ and the optimal phenotype is $\\phi(e) = \\phi_0 + \\phi_1 e + \\phi_2 e^2$. The parameters $\\kappa_a, \\kappa_b, \\kappa_c \\ge 0$ represent costs of plasticity and $\\sigma_s  0$ defines the strength of stabilizing selection.\n\nThe logarithm of the fitness is:\n$$ \\ln W(e) = -\\frac{(z(e) - \\phi(e))^2}{2\\sigma_s^2} - \\kappa_a a^2 - \\kappa_b b^2 - \\kappa_c c^2 $$\nWe must maximize the expectation of this quantity over the environmental distribution of $e$, denoted by $F(a, b, c) = \\mathbb{E}_e[\\ln W(e)]$. By linearity of expectation:\n$$ F(a,b,c) = \\mathbb{E}_e\\left[ -\\frac{(z(e) - \\phi(e))^2}{2\\sigma_s^2} \\right] - \\mathbb{E}_e\\left[\\kappa_a a^2 + \\kappa_b b^2 + \\kappa_c c^2\\right] $$\nThe cost parameters and reaction norm parameters are constant with respect to $e$, so the expectation of the cost term is the term itself.\n$$ F(a,b,c) = -\\frac{1}{2\\sigma_s^2} \\mathbb{E}_e\\left[ (z(e) - \\phi(e))^2 \\right] - \\kappa_a a^2 - \\kappa_b b^2 - \\kappa_c c^2 $$\nLet the deviation between the phenotype and the optimum be $\\Delta(e) = z(e) - \\phi(e)$. Substituting the polynomials for $z(e)$ and $\\phi(e)$, we get:\n$$ \\Delta(e) = (a - \\phi_0) + (b - \\phi_1)e + (c - \\phi_2)e^2 $$\nWe expand the squared deviation term:\n$$ \\Delta(e)^2 = \\left((a - \\phi_0) + (b - \\phi_1)e + (c - \\phi_2)e^2\\right)^2 $$\n$$ \\Delta(e)^2 = (a - \\phi_0)^2 + (b - \\phi_1)^2 e^2 + (c - \\phi_2)^2 e^4 + 2(a - \\phi_0)(b - \\phi_1)e + 2(a - \\phi_0)(c - \\phi_2)e^2 + 2(b - \\phi_1)(c - \\phi_2)e^3 $$\nNow, we take the expectation of $\\Delta(e)^2$ over the distribution of $e$. Using the definition of raw moments $m_k = \\mathbb{E}[e^k]$, and noting that $m_0 = \\mathbb{E}[e^0] = 1$:\n$$ \\mathbb{E}_e[\\Delta(e)^2] = (a-\\phi_0)^2 m_0 + (b-\\phi_1)^2 m_2 + (c-\\phi_2)^2 m_4 + 2(a-\\phi_0)(b-\\phi_1)m_1 + 2(a-\\phi_0)(c-\\phi_2)m_2 + 2(b-\\phi_1)(c-\\phi_2)m_3 $$\nSubstituting this back into the expression for $F(a,b,c)$:\n$$ F(a,b,c) = -\\frac{1}{2\\sigma_s^2} \\left[ (a-\\phi_0)^2 + \\dots \\right] - \\kappa_a a^2 - \\kappa_b b^2 - \\kappa_c c^2 $$\nTo find the stationary point $(a^\\star, b^\\star, c^\\star)$, we set the partial derivatives of $F(a, b, c)$ with respect to $a$, $b$, and $c$ to zero.\n\nThe partial derivative with respect to $a$ is:\n$$ \\frac{\\partial F}{\\partial a} = -\\frac{1}{2\\sigma_s^2} \\left[ 2(a-\\phi_0) + 2(b-\\phi_1)m_1 + 2(c-\\phi_2)m_2 \\right] - 2\\kappa_a a = 0 $$\nMultiplying by $-\\sigma_s^2$ and rearranging for the stationary values yields the first first-order condition:\n$$ (a^\\star-\\phi_0) + (b^\\star-\\phi_1)m_1 + (c^\\star-\\phi_2)m_2 + 2\\sigma_s^2 \\kappa_a a^\\star = 0 $$\n$$ a^\\star(1 + 2\\sigma_s^2 \\kappa_a) + b^\\star m_1 + c^\\star m_2 = \\phi_0 + \\phi_1 m_1 + \\phi_2 m_2 $$\n\nThe partial derivative with respect to $b$ is:\n$$ \\frac{\\partial F}{\\partial b} = -\\frac{1}{2\\sigma_s^2} \\left[ 2(b-\\phi_1)m_2 + 2(a-\\phi_0)m_1 + 2(c-\\phi_2)m_3 \\right] - 2\\kappa_b b = 0 $$\nRearranging gives the second condition:\n$$ (a^\\star-\\phi_0)m_1 + (b^\\star-\\phi_1)m_2 + (c^\\star-\\phi_2)m_3 + 2\\sigma_s^2 \\kappa_b b^\\star = 0 $$\n$$ a^\\star m_1 + b^\\star(m_2 + 2\\sigma_s^2 \\kappa_b) + c^\\star m_3 = \\phi_0 m_1 + \\phi_1 m_2 + \\phi_2 m_3 $$\n\nThe partial derivative with respect to $c$ is:\n$$ \\frac{\\partial F}{\\partial c} = -\\frac{1}{2\\sigma_s^2} \\left[ 2(c-\\phi_2)m_4 + 2(a-\\phi_0)m_2 + 2(b-\\phi_1)m_3 \\right] - 2\\kappa_c c = 0 $$\nRearranging gives the third condition:\n$$ (a^\\star-\\phi_0)m_2 + (b^\\star-\\phi_1)m_3 + (c^\\star-\\phi_2)m_4 + 2\\sigma_s^2 \\kappa_c c^\\star = 0 $$\n$$ a^\\star m_2 + b^\\star m_3 + c^\\star(m_4 + 2\\sigma_s^2 \\kappa_c) = \\phi_0 m_2 + \\phi_1 m_3 + \\phi_2 m_4 $$\n\nThese three linear equations for $(a^\\star, b^\\star, c^\\star)$ can be written in matrix form $\\mathbf{M} \\mathbf{x} = \\mathbf{v}$, where $\\mathbf{x} = [a^\\star, b^\\star, c^\\star]^T$. The coefficient matrix $\\mathbf{M}$ is:\n$$\n\\mathbf{M} = \\begin{pmatrix}\n1 + 2\\sigma_s^2 \\kappa_a  m_1  m_2 \\\\\nm_1  m_2 + 2\\sigma_s^2 \\kappa_b  m_3 \\\\\nm_2  m_3  m_4 + 2\\sigma_s^2 \\kappa_c\n\\end{pmatrix}\n$$\nThe vector $\\mathbf{v}$ on the right-hand side is:\n$$\n\\mathbf{v} = \\begin{pmatrix}\n\\phi_0 + \\phi_1 m_1 + \\phi_2 m_2 \\\\\n\\phi_0 m_1 + \\phi_1 m_2 + \\phi_2 m_3 \\\\\n\\phi_0 m_2 + \\phi_1 m_3 + \\phi_2 m_4\n\\end{pmatrix}\n$$\nThe matrix $\\mathbf{M}$ is symmetric. Since the moment matrix is a Gram matrix and thus positive semi-definite, and the cost matrix is diagonal with non-negative entries (strictly positive for the test cases), $\\mathbf{M}$ is positive definite and thus invertible, guaranteeing a unique solution.\n\nThe algorithm to find $c^\\star$ is:\n$1$. For each test case, compute the environmental moments $m_1, m_2, m_3, m_4$.\n$2$. Construct the $3 \\times 3$ matrix $\\mathbf{M}$ and the $3 \\times 1$ vector $\\mathbf{v}$ using the derived formulas.\n$3$. Solve the linear system $\\mathbf{M} \\mathbf{x} = \\mathbf{v}$ for the vector of stationary parameters $\\mathbf{x} = [a^\\star, b^\\star, c^\\star]^T$.\n$4$. The result is the third component of the solution vector, $c^\\star$.\nThis constitutes the complete theoretical basis for the computational solution.", "answer": "```python\nimport numpy as np\n\ndef get_gaussian_moments(mu, sigma_sq):\n    \"\"\"\n    Calculates the first four raw moments for a Gaussian distribution \n    N(mu, sigma_sq).\n    \"\"\"\n    m1 = mu\n    m2 = mu**2 + sigma_sq\n    m3 = mu**3 + 3 * mu * sigma_sq\n    m4 = mu**4 + 6 * mu**2 * sigma_sq + 3 * sigma_sq**2\n    return m1, m2, m3, m4\n\ndef get_uniform_moments(L, U):\n    \"\"\"\n    Calculates the first four raw moments for a Uniform distribution on [L, U].\n    \"\"\"\n    moments = []\n    for k in range(1, 5):\n        if abs(U - L)  1e-12: # Handles the case of a point mass\n            mk = L**k\n        else:\n            mk = (U**(k + 1) - L**(k + 1)) / ((k + 1) * (U - L))\n        moments.append(mk)\n    return tuple(moments)\n\ndef get_bimodal_gaussian_moments(mu, sigma_sq):\n    \"\"\"\n    Calculates the first four raw moments for a symmetric bimodal Gaussian \n    mixture distribution 0.5*N(-mu, sigma_sq) + 0.5*N(mu, sigma_sq).\n    \"\"\"\n    m1 = 0.0\n    m2 = mu**2 + sigma_sq\n    m3 = 0.0\n    m4 = mu**4 + 6 * mu**2 * sigma_sq + 3 * sigma_sq**2\n    return m1, m2, m3, m4\n\ndef solve_for_c_star(phi, kappa, sigma_s, moments):\n    \"\"\"\n    Solves the linear system for the stationary reaction norm parameters \n    and returns the curvature c_star.\n    \"\"\"\n    phi0, phi1, phi2 = phi\n    kappa_a, kappa_b, kappa_c = kappa\n    m1, m2, m3, m4 = moments\n    m0 = 1.0\n    \n    sigma_s_sq = sigma_s**2\n\n    # Construct the matrix M based on the first-order conditions\n    M = np.array([\n        [m0 + 2 * sigma_s_sq * kappa_a, m1, m2],\n        [m1, m2 + 2 * sigma_s_sq * kappa_b, m3],\n        [m2, m3, m4 + 2 * sigma_s_sq * kappa_c]\n    ], dtype=np.float64)\n\n    # Construct the vector v based on the first-order conditions\n    v = np.array([\n        phi0 * m0 + phi1 * m1 + phi2 * m2,\n        phi0 * m1 + phi1 * m2 + phi2 * m3,\n        phi0 * m2 + phi1 * m3 + phi2 * m4\n    ], dtype=np.float64)\n\n    # Solve the system M * x = v for x = [a_star, b_star, c_star]\n    try:\n        x = np.linalg.solve(M, v)\n        c_star = x[2]\n        return c_star\n    except np.linalg.LinAlgError:\n        # This case is not expected given the problem's constraints\n        return np.nan\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        {'phi': (0.0, 1.0, 0.2), 'kappa': (0.01, 0.01, 0.01), 'sigma_s': 0.5, 'env': ('gaussian', {'mu': 0.0, 'sigma_sq': 1.0})},\n        # Test case 2 (different spread and tails)\n        {'phi': (0.0, 1.0, 0.2), 'kappa': (0.01, 0.01, 0.01), 'sigma_s': 0.5, 'env': ('uniform', {'L': -1.0, 'U': 1.0})},\n        # Test case 3 (bimodality)\n        {'phi': (0.0, 0.0, 0.2), 'kappa': (0.01, 0.01, 0.01), 'sigma_s': 0.5, 'env': ('bimodal', {'mu': 1.0, 'sigma_sq': 0.2**2})},\n        # Test case 4 (boundary condition: extremely narrow environment)\n        {'phi': (0.0, 1.0, 0.2), 'kappa': (0.01, 0.01, 0.01), 'sigma_s': 0.5, 'env': ('gaussian', {'mu': 0.0, 'sigma_sq': 0.1**2})},\n        # Test case 5 (boundary condition: very high curvature cost)\n        {'phi': (0.0, 1.0, -0.3), 'kappa': (0.01, 0.01, 5.0), 'sigma_s': 0.5, 'env': ('gaussian', {'mu': 0.0, 'sigma_sq': 1.0})},\n    ]\n\n    results = []\n    for case in test_cases:\n        env_type, env_params = case['env']\n        \n        if env_type == 'gaussian':\n            moments = get_gaussian_moments(**env_params)\n        elif env_type == 'uniform':\n            moments = get_uniform_moments(**env_params)\n        elif env_type == 'bimodal':\n            moments = get_bimodal_gaussian_moments(**env_params)\n        else:\n            # Should not be reached with the defined test cases\n            raise ValueError(f\"Unknown environment type: {env_type}\")\n            \n        c_star = solve_for_c_star(case['phi'], case['kappa'], case['sigma_s'], moments)\n        results.append(c_star)\n        \n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2741861"}]}