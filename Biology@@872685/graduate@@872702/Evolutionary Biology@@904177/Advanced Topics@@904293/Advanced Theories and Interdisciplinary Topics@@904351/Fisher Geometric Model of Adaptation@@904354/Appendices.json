{"hands_on_practices": [{"introduction": "To understand the process of adaptation, we must first characterize the raw material on which selection acts: the fitness effects of new mutations. The Fisher Geometric Model (FGM) provides a simple yet powerful framework for this by mapping a mutation's random effect in a high-dimensional phenotype space to its consequence for fitness. This first exercise invites you to derive the fundamental statistical properties—the mean and variance—of the distribution of fitness effects (DFE) predicted by the model [@problem_id:2713201]. Completing this calculation reveals two core predictions of the FGM: that random mutations are, on average, deleterious, and that the variability of their effects depends critically on how well-adapted the population already is.", "problem": "Consider Fisher's geometric model (FGM) of adaptation in which phenotype is an $n$-dimensional vector $\\mathbf{z} \\in \\mathbb{R}^{n}$, the optimal phenotype is at the origin, and Malthusian fitness is quadratic in the distance to the optimum. Without loss of generality, adopt a curvature scaling such that Malthusian fitness is given by $m(\\mathbf{z}) = - \\frac{1}{2}\\|\\mathbf{z}\\|^{2}$. Let the resident genotype be at a distance $r = \\|\\mathbf{z}\\|$ from the optimum. A new mutation adds a phenotypic step $\\Delta$ that is isotropic (direction uniformly distributed on the unit sphere in $\\mathbb{R}^{n}$) and of fixed magnitude $\\|\\Delta\\| = \\sqrt{\\mu_{2}}$, where $\\mu_{2}$ denotes the second moment of the step-size distribution. Define the selection coefficient $s$ of a single mutation as the change in Malthusian fitness, $s = m(\\mathbf{z} + \\Delta) - m(\\mathbf{z})$.\n\nCompute the mean and the variance of $s$ over the isotropic distribution of mutational directions, expressed as functions of $r$, $n$, and $\\mu_{2}$. Express your final result as a row matrix $\\big(\\mathbb{E}[s]\\ \\ \\mathrm{Var}(s)\\big)$ with no units. No numerical rounding is required; provide an exact analytical expression.", "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-contained. It is a standard problem in theoretical evolutionary biology concerning Fisher's geometric model. We shall proceed with the derivation.\n\nThe selection coefficient, $s$, is defined as the change in Malthusian fitness resulting from a mutation $\\Delta$. The Malthusian fitness function is given as $m(\\mathbf{z}) = - \\frac{1}{2}\\|\\mathbf{z}\\|^{2}$. The resident phenotype is $\\mathbf{z}$, and the mutant phenotype is $\\mathbf{z} + \\Delta$. Thus, the selection coefficient is:\n$$\ns = m(\\mathbf{z} + \\Delta) - m(\\mathbf{z})\n$$\nSubstituting the fitness function, we have:\n$$\ns = - \\frac{1}{2}\\|\\mathbf{z} + \\Delta\\|^{2} - \\left( - \\frac{1}{2}\\|\\mathbf{z}\\|^{2} \\right)\n$$\nWe expand the squared norm term $\\|\\mathbf{z} + \\Delta\\|^{2} = (\\mathbf{z} + \\Delta) \\cdot (\\mathbf{z} + \\Delta) = \\|\\mathbf{z}\\|^{2} + 2(\\mathbf{z} \\cdot \\Delta) + \\|\\Delta\\|^{2}$.\n$$\ns = - \\frac{1}{2} \\left( \\|\\mathbf{z}\\|^{2} + 2(\\mathbf{z} \\cdot \\Delta) + \\|\\Delta\\|^{2} \\right) + \\frac{1}{2}\\|\\mathbf{z}\\|^{2}\n$$\n$$\ns = - \\frac{1}{2}\\|\\mathbf{z}\\|^{2} - (\\mathbf{z} \\cdot \\Delta) - \\frac{1}{2}\\|\\Delta\\|^{2} + \\frac{1}{2}\\|\\mathbf{z}\\|^{2}\n$$\nThis simplifies to:\n$$\ns = -(\\mathbf{z} \\cdot \\Delta) - \\frac{1}{2}\\|\\Delta\\|^{2}\n$$\nThe problem states that the magnitude of the mutational step is fixed, $\\|\\Delta\\| = \\sqrt{\\mu_{2}}$, which implies $\\|\\Delta\\|^{2} = \\mu_{2}$. The resident phenotype is at a distance $r$ from the optimum, so $\\|\\mathbf{z}\\| = r$. The expression for $s$ becomes:\n$$\ns = -(\\mathbf{z} \\cdot \\Delta) - \\frac{1}{2}\\mu_{2}\n$$\nWe are asked to compute the mean, $\\mathbb{E}[s]$, and the variance, $\\mathrm{Var}(s)$, of the selection coefficient over the isotropic distribution of mutational directions. The vector $\\mathbf{z}$ is fixed for this calculation, while $\\Delta$ is the random variable.\n\nFirst, we compute the mean of $s$. By the linearity of the expectation operator:\n$$\n\\mathbb{E}[s] = \\mathbb{E}\\left[-(\\mathbf{z} \\cdot \\Delta) - \\frac{1}{2}\\mu_{2}\\right] = -\\mathbb{E}[\\mathbf{z} \\cdot \\Delta] - \\mathbb{E}\\left[\\frac{1}{2}\\mu_{2}\\right]\n$$\nSince $\\mathbf{z}$ is a fixed vector, we can write $\\mathbb{E}[\\mathbf{z} \\cdot \\Delta] = \\mathbf{z} \\cdot \\mathbb{E}[\\Delta]$. The mutational step $\\Delta$ has a fixed magnitude $\\sqrt{\\mu_{2}}$ and a direction that is uniformly distributed on the unit sphere in $\\mathbb{R}^{n}$. Let $\\mathbf{u}$ be a random vector uniformly distributed on the unit sphere $S^{n-1}$. Then $\\Delta = \\sqrt{\\mu_{2}} \\mathbf{u}$. The expectation of $\\Delta$ is:\n$$\n\\mathbb{E}[\\Delta] = \\mathbb{E}[\\sqrt{\\mu_{2}}\\mathbf{u}] = \\sqrt{\\mu_{2}}\\mathbb{E}[\\mathbf{u}]\n$$\nDue to the symmetry of the uniform distribution on the sphere, the expected value of the vector $\\mathbf{u}$ is the zero vector, $\\mathbb{E}[\\mathbf{u}] = \\mathbf{0}$. Therefore, $\\mathbb{E}[\\Delta] = \\mathbf{0}$.\nIt follows that $\\mathbb{E}[\\mathbf{z} \\cdot \\Delta] = \\mathbf{z} \\cdot \\mathbf{0} = 0$. The term $\\frac{1}{2}\\mu_{2}$ is a constant. Thus, the mean selection coefficient is:\n$$\n\\mathbb{E}[s] = -0 - \\frac{1}{2}\\mu_{2} = - \\frac{1}{2}\\mu_{2}\n$$\nNext, we compute the variance of $s$. The variance is defined as $\\mathrm{Var}(s) = \\mathbb{E}[s^2] - (\\mathbb{E}[s])^2$. Alternatively, we use the property that for any random variable $X$ and constants $a, b$, $\\mathrm{Var}(aX+b) = a^2\\mathrm{Var}(X)$.\n$$\n\\mathrm{Var}(s) = \\mathrm{Var}\\left(-(\\mathbf{z} \\cdot \\Delta) - \\frac{1}{2}\\mu_{2}\\right) = \\mathrm{Var}(-(\\mathbf{z} \\cdot \\Delta)) = (-1)^{2} \\mathrm{Var}(\\mathbf{z} \\cdot \\Delta) = \\mathrm{Var}(\\mathbf{z} \\cdot \\Delta)\n$$\nThe variance of the random variable $\\mathbf{z} \\cdot \\Delta$ is given by:\n$$\n\\mathrm{Var}(\\mathbf{z} \\cdot \\Delta) = \\mathbb{E}\\left[(\\mathbf{z} \\cdot \\Delta)^2\\right] - \\left(\\mathbb{E}[\\mathbf{z} \\cdot \\Delta]\\right)^2\n$$\nAs we have already established that $\\mathbb{E}[\\mathbf{z} \\cdot \\Delta] = 0$, the variance simplifies to the second moment:\n$$\n\\mathrm{Var}(s) = \\mathbb{E}\\left[(\\mathbf{z} \\cdot \\Delta)^2\\right]\n$$\nTo compute this expectation, we consider the quadratic form $(\\mathbf{z} \\cdot \\Delta)^2 = (\\mathbf{z}^T \\Delta)^2 = (\\mathbf{z}^T \\Delta)(\\Delta^T \\mathbf{z}) = \\mathbf{z}^T (\\Delta \\Delta^T) \\mathbf{z}$.\nTaking the expectation:\n$$\n\\mathbb{E}\\left[(\\mathbf{z} \\cdot \\Delta)^2\\right] = \\mathbb{E}\\left[\\mathbf{z}^T (\\Delta \\Delta^T) \\mathbf{z}\\right] = \\mathbf{z}^T \\mathbb{E}[\\Delta \\Delta^T] \\mathbf{z}\n$$\nThe matrix $\\mathbb{E}[\\Delta \\Delta^T]$ is the second moment matrix of the vector $\\Delta$. Since $\\Delta$ is isotropic, this matrix must be proportional to the identity matrix $\\mathbf{I}$, so $\\mathbb{E}[\\Delta \\Delta^T] = c\\mathbf{I}$ for some scalar $c$. We can determine $c$ by taking the trace of the matrix:\n$$\n\\mathrm{Tr}(\\mathbb{E}[\\Delta \\Delta^T]) = \\mathbb{E}[\\mathrm{Tr}(\\Delta \\Delta^T)]\n$$\nThe trace of $\\Delta \\Delta^T$ is $\\mathrm{Tr}(\\Delta \\Delta^T) = \\Delta^T \\Delta = \\|\\Delta\\|^2 = \\mu_{2}$.\nThus, $\\mathbb{E}[\\mathrm{Tr}(\\Delta \\Delta^T)] = \\mathbb{E}[\\mu_{2}] = \\mu_{2}$, since $\\mu_{2}$ is a constant.\nThe trace of $c\\mathbf{I}$ is $\\mathrm{Tr}(c\\mathbf{I}) = nc$.\nEquating the two expressions for the trace gives $nc = \\mu_{2}$, which implies $c = \\frac{\\mu_{2}}{n}$.\nTherefore, $\\mathbb{E}[\\Delta \\Delta^T] = \\frac{\\mu_{2}}{n} \\mathbf{I}$.\nSubstituting this back into the expression for the variance:\n$$\n\\mathrm{Var}(s) = \\mathbf{z}^T \\left(\\frac{\\mu_{2}}{n} \\mathbf{I}\\right) \\mathbf{z} = \\frac{\\mu_{2}}{n} (\\mathbf{z}^T \\mathbf{z}) = \\frac{\\mu_{2}}{n} \\|\\mathbf{z}\\|^2\n$$\nGiven that $\\|\\mathbf{z}\\| = r$, we finally obtain the variance of the selection coefficient:\n$$\n\\mathrm{Var}(s) = \\frac{\\mu_{2}r^2}{n}\n$$\nSo, the mean and variance of $s$ are $\\mathbb{E}[s] = -\\frac{1}{2}\\mu_{2}$ and $\\mathrm{Var}(s) = \\frac{\\mu_{2}r^2}{n}$. The final result is presented as a row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{\\mu_{2}}{2}  \\frac{r^{2}\\mu_{2}}{n} \\end{pmatrix}}$$", "id": "2713201"}, {"introduction": "Once we have a grasp of the distribution of fitness effects, we can begin to ask more sophisticated evolutionary questions, such as how natural selection might shape the properties of mutation itself. This practice challenges you to apply the FGM to determine the optimal size of a mutation, specifically the size that maximizes the expected gain in fitness per mutation [@problem_id:2713211]. The solution requires you to move beyond the average effect of all mutations and focus only on the beneficial ones, a task that involves careful integration and optimization. This exercise is invaluable for learning how the model's geometric constraints lead to concrete, testable predictions about evolutionary parameters and for practicing powerful analytical techniques like scaling analysis.", "problem": "Consider Fisher’s geometric model of adaptation in which an organism’s phenotype is represented by an $n$-dimensional trait vector $\\mathbf{z} \\in \\mathbb{R}^{n}$, with a single phenotypic optimum at the origin. Assume Malthusian fitness (log-fitness) is quadratic, $f(\\mathbf{z}) = -\\|\\mathbf{z}\\|^{2}/2$, so that the selection coefficient of a mutation with phenotypic effect $\\mathbf{m}$ is $s = f(\\mathbf{z}+\\mathbf{m}) - f(\\mathbf{z})$. Let the current phenotype be at distance $r = \\|\\mathbf{z}\\|$ from the optimum, and consider mutations of fixed magnitude $a = \\|\\mathbf{m}\\|$ with direction uniformly random on the $(n-1)$-sphere. For a fixed $a$, define the expected fitness gain per random mutation as $E[s \\mid a] P_{b}(a)$, where $P_{b}(a)$ is the probability the mutation is beneficial and $E[s \\mid a]$ is the mean selection coefficient conditional on being beneficial. \n\n1. Starting from the definitions above and basic facts about uniform random directions on the $(n-1)$-sphere, derive an integral expression for $E[s \\mid a] P_{b}(a)$ in terms of $n$, $r$, and $a$ by integrating over the cosine of the angle between $\\mathbf{m}$ and $\\mathbf{z}$. Use only the facts that $s = -\\mathbf{z}\\!\\cdot\\!\\mathbf{m} - a^{2}/2$, that $\\mathbf{z}\\!\\cdot\\!\\mathbf{m} = r a \\cos\\psi$ for the angle $\\psi$ between $\\mathbf{z}$ and $\\mathbf{m}$, and that the marginal density of $u = \\cos\\psi$ for a uniformly random direction in $\\mathbb{R}^{n}$ is proportional to $(1-u^{2})^{(n-3)/2}$ on $[-1,1]$. Do not assume any target formula for $E[s \\mid a] P_{b}(a)$.\n\n2. Show from your integral expression that the maximizer $a^{\\ast}$ of $E[s \\mid a] P_{b}(a)$ must scale linearly with $r$, i.e., $a^{\\ast} = c_{n}\\, r$ for some coefficient $c_{n}$ that depends only on $n$.\n\n3. Specialize to $n = 3$. Compute $a^{\\ast}$ exactly as a function of $r$ by solving the first-order condition for the maximum. Provide your final result for $a^{\\ast}$ as a closed-form expression in terms of $r$. No numerical approximation is required. There are no physical units. The final answer must be a single analytic expression. If you choose to check any intermediate numerical values, round such checks to four significant figures, but the final answer must be exact.\n\nBriefly explain within your derivation how the optimizer’s dependence on $r$ arises in general and why it is linear.", "solution": "The problem as stated is scientifically sound and mathematically well-posed. It presents a standard derivation and analysis within the framework of Fisher's geometric model of adaptation, a foundational concept in evolutionary biology. All necessary definitions and mathematical facts are provided, and the problem is free of contradictions or ambiguities. Thus, we proceed with the solution.\n\nThe problem asks for a three-part analysis of the expected fitness gain from mutations in Fisher's geometric model. The Malthusian fitness is given by $f(\\mathbf{z}) = -\\|\\mathbf{z}\\|^{2}/2$, where $\\mathbf{z} \\in \\mathbb{R}^{n}$ is the phenotype and the optimum is at the origin. A mutation of phenotypic effect $\\mathbf{m}$ changes the phenotype from $\\mathbf{z}$ to $\\mathbf{z}+\\mathbf{m}$. The selection coefficient of such a mutation is:\n$$ s = f(\\mathbf{z}+\\mathbf{m}) - f(\\mathbf{z}) = -\\frac{1}{2}\\|\\mathbf{z}+\\mathbf{m}\\|^{2} - \\left(-\\frac{1}{2}\\|\\mathbf{z}\\|^{2}\\right) $$\n$$ s = -\\frac{1}{2}(\\mathbf{z}\\cdot\\mathbf{z} + 2\\mathbf{z}\\cdot\\mathbf{m} + \\mathbf{m}\\cdot\\mathbf{m}) + \\frac{1}{2}\\mathbf{z}\\cdot\\mathbf{z} = -\\mathbf{z}\\cdot\\mathbf{m} - \\frac{1}{2}\\|\\mathbf{m}\\|^{2} $$\nLet $r = \\|\\mathbf{z}\\|$ be the distance of the current phenotype from the optimum, and $a = \\|\\mathbf{m}\\|$ be the magnitude of the mutation. Let $\\psi$ be the angle between the vectors $\\mathbf{z}$ and $\\mathbf{m}$. The dot product can be written as $\\mathbf{z}\\cdot\\mathbf{m} = \\|\\mathbf{z}\\|\\|\\mathbf{m}\\|\\cos\\psi = ra\\cos\\psi$. The selection coefficient is then a function of the angle $\\psi$:\n$$ s(\\psi) = -ra\\cos\\psi - \\frac{a^{2}}{2} $$\nThe direction of the mutation $\\mathbf{m}$ is assumed to be uniformly random on the $(n-1)$-sphere. This implies a specific probability distribution for $u = \\cos\\psi$. The problem provides the marginal probability density function (PDF) for $u$ on the interval $[-1, 1]$:\n$$ p(u) = C_{n} (1-u^{2})^{(n-3)/2} $$\nwhere $C_{n} = \\frac{\\Gamma(n/2)}{\\sqrt{\\pi}\\Gamma((n-1)/2)}$ is the normalization constant.\n\nThe quantity of interest is the expected fitness gain, defined as $E[s \\mid a] P_{b}(a)$. This is the expectation of the selection coefficient, conditional on it being positive, multiplied by the probability of it being positive. This product is equivalent to the expectation of $\\max(0, s)$, which we denote as $G(a, r, n)$.\n$$ G(a, r, n) = E[\\max(0, s)] = \\int_{-1}^{1} \\max(0, s(u)) p(u) du $$\n\nA mutation is beneficial if $s(u)  0$. Using the expression for $s$ in terms of $u$:\n$$ -rau - \\frac{a^{2}}{2}  0 \\implies -rau  \\frac{a^{2}}{2} $$\nSince $r  0$ and $a  0$, this inequality simplifies to:\n$$ u  -\\frac{a}{2r} $$\nFor beneficial mutations to be possible, the upper bound for $u$ must be greater than the lower bound of its domain, i.e., $-a/(2r)  -1$, which implies $a  2r$. If $a \\geq 2r$, no mutation can be beneficial, and the expected gain is $0$.\n\n**1. Derivation of the integral expression**\n\nWe can now write the expression for $G(a, r, n)$ by integrating $s(u) p(u)$ over the range of $u$ where the mutation is beneficial.\n$$ G(a, r, n) = \\int_{-1}^{-a/(2r)} s(u) p(u) du $$\nSubstituting the expressions for $s(u)$ and $p(u)$:\n$$ G(a, r, n) = \\int_{-1}^{-a/(2r)} \\left(-rau - \\frac{a^{2}}{2}\\right) C_{n} (1-u^{2})^{(n-3)/2} du $$\nFactoring out constants and substituting the expression for $C_n$, we obtain the required integral expression for the expected fitness gain:\n$$ G(a, r, n) = \\frac{\\Gamma(n/2)}{\\sqrt{\\pi}\\Gamma(\\frac{n-1}{2})} \\int_{-1}^{-a/(2r)} \\left(-rau - \\frac{a^{2}}{2}\\right) (1-u^{2})^{\\frac{n-3}{2}} du $$\nThis integral is defined for $0 \\le a  2r$. For $a \\ge 2r$, $G(a, r, n) = 0$.\n\n**2. Scaling of the optimal mutation size $a^{\\ast}$**\n\nTo find the optimal mutation size $a^{\\ast}$ that maximizes $G(a, r, n)$, we must analyze how $G$ depends on $a$ and $r$. Let us introduce a dimensionless variable $x = a/r$, which represents the mutation size relative to the distance from the optimum. We can rewrite the expression for $G$ by substituting $a = xr$:\n$$ G(xr, r, n) = C_{n} \\int_{-1}^{-(xr)/(2r)} \\left(-r(xr)u - \\frac{(xr)^{2}}{2}\\right) (1-u^{2})^{(n-3)/2} du $$\n$$ G(xr, r, n) = C_{n} r^{2} \\int_{-1}^{-x/2} \\left(-xu - \\frac{x^{2}}{2}\\right) (1-u^{2})^{(n-3)/2} du $$\nWe can define a function $H(x, n)$ that depends only on the relative mutation size $x$ and the dimension $n$:\n$$ H(x, n) = C_{n} \\int_{-1}^{-x/2} \\left(-xu - \\frac{x^{2}}{2}\\right) (1-u^{2})^{(n-3)/2} du $$\nThen, the expected gain can be expressed as $G(a, r, n) = r^{2} H(a/r, n)$.\n\nTo find the optimal mutation size $a^{\\ast}$, we set the partial derivative of $G$ with respect to $a$ to zero:\n$$ \\frac{\\partial G}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ r^{2} H(a/r, n) \\right] = r^{2} \\cdot \\frac{d H(x, n)}{d x} \\cdot \\frac{\\partial x}{\\partial a} \\quad \\text{where } x = a/r $$\nSince $\\partial x/\\partial a = 1/r$, this becomes:\n$$ \\frac{\\partial G}{\\partial a} = r^{2} \\cdot H'(x, n) \\cdot \\frac{1}{r} = r H'(a/r, n) $$\nFor $r  0$, the condition $\\frac{\\partial G}{\\partial a} = 0$ is equivalent to $H'(a/r, n) = 0$. The solution to this equation, let's call it $x^{\\ast}$, will only depend on the parameter $n$, since $H'$ is only a function of $x$ and $n$.\n$$ x^{\\ast} = c_{n} $$\nSubstituting back $x = a/r$, we find that the optimal value $a^{\\ast}$ satisfies:\n$$ \\frac{a^{\\ast}}{r} = c_{n} \\implies a^{\\ast} = c_{n} r $$\nThis proves that the optimal mutation size $a^{\\ast}$ scales linearly with the distance to the optimum $r$. The reason for this linear relationship is the self-similar, or scale-invariant, geometry of the quadratic fitness landscape. The shape of the fitness surface around a point at distance $r$ is identical to the shape around a point at distance $\\lambda r$ after scaling all spatial dimensions by a factor of $\\lambda$. The optimization problem for mutation size is therefore geometrically identical at all distances from the optimum, provided that the mutation size is scaled by the same factor. This means the optimal ratio of mutation size to distance, $a^{\\ast}/r$, must be a constant that depends only on the dimensionality $n$ of the space, not on the distance $r$ itself.\n\n**3. Calculation of $a^{\\ast}$ for $n=3$**\n\nFor the three-dimensional case, $n=3$, the exponent in the PDF of $u$ becomes $(3-3)/2 = 0$.\nThe PDF simplifies to $p(u) = C_{3}(1-u^{2})^{0} = C_{3}$. To find the constant, we normalize:\n$$ \\int_{-1}^{1} C_{3} du = 2C_{3} = 1 \\implies C_{3} = \\frac{1}{2} $$\nSo for $n=3$, the cosine of the angle is uniformly distributed on $[-1, 1]$. The expected gain is:\n$$ G(a, r, 3) = \\int_{-1}^{-a/(2r)} \\left(-rau - \\frac{a^{2}}{2}\\right) \\frac{1}{2} du $$\nThis integral is elementary:\n$$ G(a, r, 3) = \\frac{1}{2} \\left[ -ra\\frac{u^{2}}{2} - \\frac{a^{2}}{2}u \\right]_{-1}^{-a/(2r)} $$\n$$ G(a, r, 3) = \\frac{1}{4} \\left[ -rau^{2} - a^{2}u \\right]_{-1}^{-a/(2r)} $$\nEvaluating at the limits:\n$$ G(a, r, 3) = \\frac{1}{4} \\left( \\left[-ra\\left(-\\frac{a}{2r}\\right)^{2} - a^{2}\\left(-\\frac{a}{2r}\\right)\\right] - \\left[-ra(-1)^{2} - a^{2}(-1)\\right] \\right) $$\n$$ G(a, r, 3) = \\frac{1}{4} \\left( \\left[-ra\\frac{a^{2}}{4r^{2}} + \\frac{a^{3}}{2r}\\right] - \\left[-ra + a^{2}\\right] \\right) $$\n$$ G(a, r, 3) = \\frac{1}{4} \\left( -\\frac{a^{3}}{4r} + \\frac{2a^{3}}{4r} + ra - a^{2} \\right) = \\frac{1}{4} \\left( \\frac{a^{3}}{4r} - a^{2} + ra \\right) $$\nTo find the optimal $a$, we differentiate with respect to $a$ and set the result to zero:\n$$ \\frac{\\partial G}{\\partial a} = \\frac{1}{4} \\left( \\frac{3a^{2}}{4r} - 2a + r \\right) = 0 $$\n$$ \\frac{3a^{2}}{4r} - 2a + r = 0 $$\nMultiplying by $4r$ gives a quadratic equation for $a$:\n$$ 3a^{2} - 8ra + 4r^{2} = 0 $$\nUsing the quadratic formula, $a = \\frac{-b \\pm \\sqrt{b^{2}-4ac}}{2a_{q}}$ (where $a_{q}=3$, $b=-8r$, $c=4r^2$):\n$$ a = \\frac{8r \\pm \\sqrt{(-8r)^{2} - 4(3)(4r^{2})}}{2(3)} = \\frac{8r \\pm \\sqrt{64r^{2} - 48r^{2}}}{6} $$\n$$ a = \\frac{8r \\pm \\sqrt{16r^{2}}}{6} = \\frac{8r \\pm 4r}{6} $$\nThis yields two possible solutions for $a^{\\ast}$:\n$$ a_{1} = \\frac{8r + 4r}{6} = \\frac{12r}{6} = 2r $$\n$$ a_{2} = \\frac{8r - 4r}{6} = \\frac{4r}{6} = \\frac{2}{3}r $$\nTo determine which is the maximum, we check the second derivative:\n$$ \\frac{\\partial^{2} G}{\\partial a^{2}} = \\frac{1}{4} \\left( \\frac{6a}{4r} - 2 \\right) = \\frac{1}{4} \\left( \\frac{3a}{2r} - 2 \\right) $$\nAt $a_{1} = 2r$, the second derivative is $\\frac{1}{4}(\\frac{3(2r)}{2r} - 2) = \\frac{1}{4}(3-2) = \\frac{1}{4}  0$, indicating a local minimum.\nAt $a_{2} = \\frac{2}{3}r$, the second derivative is $\\frac{1}{4}(\\frac{3(2r/3)}{2r} - 2) = \\frac{1}{4}(1-2) = -\\frac{1}{4}  0$, indicating a local maximum.\nThe valid domain for $a$ is $[0, 2r]$. We found that $G(0,r,3)=0$ and $G(2r,r,3)=0$. Since $G(a,r,3)  0$ for $a \\in (0, 2r)$, the local maximum at $a = \\frac{2}{3}r$ is the global maximum in this interval.\nTherefore, the optimal mutation size for $n=3$ is $a^{\\ast} = \\frac{2}{3}r$.", "answer": "$$\\boxed{\\frac{2}{3}r}$$", "id": "2713211"}, {"introduction": "A theoretical model's true power is realized when it can be used to interpret empirical observations. This final hands-on practice bridges the gap between the abstract geometry of the FGM and the analysis of real-world data. You will develop a computational method to infer the unobservable parameters of the model—namely, the phenotypic complexity $n$ and the population's distance from the fitness optimum $r$—from a given set of selection coefficients [@problem_id:2713195]. By implementing a maximum likelihood estimation procedure, you will see how the theoretical DFE can serve as a generative model, providing a statistically rigorous way to connect theory with data. This exercise offers practical experience in a cornerstone technique of modern, quantitative evolutionary biology.", "problem": "You are asked to implement a likelihood-based inference procedure for Fisher’s geometric model of adaptation using the Fisher’s geometric distribution of fitness effects as the generative model. Work from first principles by modeling phenotype as a point in a high-dimensional Euclidean space and fitness as a smooth, spherically symmetric function of distance to the optimum, and by assuming isotropic, small-effect mutational steps. Then derive the induced distribution of fitness effects of new mutations and use it to compute the likelihood for observed fitness effects.\n\nAssume the following fundamental setup and definitions:\n- The phenotype is an $n$-dimensional vector with the optimum at the origin. The resident genotype has phenotypic displacement of Euclidean norm $r \\ge 0$ from the optimum.\n- A new mutation adds an isotropic effect vector $\\mathbf{X}$ whose coordinates are independent and identically distributed Gaussian random variables with mean $0$ and variance $1$. Thus, $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$.\n- Fitness is a smooth, spherically symmetric function of squared distance to the optimum, with curvature fixed to set the selection coefficient scale. Use the small-effect approximation to relate the selection coefficient $s$ of a new mutation to the change in squared distance $\\Delta$ by $s = - \\Delta$, where\n$$\n\\Delta = \\lVert \\mathbf{z} + \\mathbf{X} \\rVert^2 - \\lVert \\mathbf{z} \\rVert^2 = \\lVert \\mathbf{X} \\rVert^2 + 2 \\, r \\, X_1,\n$$\nafter rotating coordinates so that $\\mathbf{z}$ lies along the first axis with $\\lVert \\mathbf{z} \\rVert = r$ and $X_1$ is the first coordinate of $\\mathbf{X}$.\n- Define the noncentrality parameter $\\delta = r^2$ and the random variable\n$$\nT = \\sum_{i=1}^n (X_i + \\mu_i)^2,\n$$\nwith $\\mu_1 = r$ and $\\mu_i = 0$ for $i  1$. Then $T$ has a noncentral chi-square distribution with $n$ degrees of freedom and noncentrality $\\delta$, denoted $T \\sim \\chi^2_n(\\delta)$. One has the identity\n$$\n\\Delta = T - \\delta, \\quad \\text{so} \\quad s = - \\Delta = \\delta - T.\n$$\n- Therefore, for any candidate pair $(r, n)$ with $\\delta = r^2$, each observed selection coefficient $s$ determines $T = \\delta - s$. Since $T$ has support on $[0, \\infty)$, any observation with $T  0$ has zero likelihood under that candidate.\n\nYour program must implement the following likelihood-based inference task:\n- For each observed dataset of selection coefficients $\\{s_i\\}_{i=1}^m$, treat the data as independent and identically distributed draws from the Fisher’s geometric distribution of fitness effects induced by the noncentral chi-square $T \\sim \\chi^2_n(\\delta)$ via $s = \\delta - T$.\n- The likelihood for a candidate $(r, n)$ is the product over $i$ of the noncentral chi-square density evaluated at $T_i = \\delta - s_i$:\n$$\n\\mathcal{L}(r, n \\mid \\{s_i\\}) = \\prod_{i=1}^m f_{\\chi^2_n(\\delta)}(T_i), \\quad \\delta = r^2,\n$$\nwith the convention that if any $T_i  0$ then $\\mathcal{L}(r, n \\mid \\{s_i\\}) = 0$.\n- Compute the maximum likelihood pair $(\\hat{r}, \\hat{n})$ over a fixed search grid defined below. When multiple pairs achieve the same maximum likelihood up to numerical tolerance, break ties by choosing the smallest $\\hat{r}$ and then the smallest $\\hat{n}$.\n\nUse the following fixed candidate grids:\n- Dimension grid: $n \\in \\{1, 2, 3, 4, 5, 6\\}$.\n- Distance grid: $r \\in \\{0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0\\}$.\n\nTest suite:\n- Dataset A: $\\{s_i\\} = \\{-1.0, -3.0, -5.0\\}$.\n- Dataset B: $\\{s_i\\} = \\{3.8, -2.0, -6.0\\}$.\n- Dataset C: $\\{s_i\\} = \\{0.8, -2.0, -6.0\\}$.\n\nFor each dataset, your program must:\n- Evaluate the log-likelihood $\\log \\mathcal{L}(r, n \\mid \\{s_i\\})$ over all grid pairs, treating any zero density as contributing a log-likelihood of negative infinity and hence excluding that pair.\n- Return the maximizing pair $(\\hat{r}, \\hat{n})$ with the specified tie-breaking rule.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each dataset’s result is represented as a two-element list $[\\hat{r}, \\hat{n}]$. For example, the overall output must have the exact form\n$[[\\hat{r}_A,\\hat{n}_A],[\\hat{r}_B,\\hat{n}_B],[\\hat{r}_C,\\hat{n}_C]]$\nwith no spaces inserted.", "solution": "We begin from the core geometric and probabilistic structure of Fisher’s geometric model. Phenotypes are points in an $n$-dimensional Euclidean space, and fitness decreases smoothly with squared distance from the optimum. Let the resident genotype be at a displacement vector $\\mathbf{z}$ of Euclidean norm $r = \\lVert \\mathbf{z} \\rVert$. A new mutation adds an isotropic effect vector $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$ under the assumption of independent, identically distributed, mean-zero Gaussian effects along each trait axis.\n\nThe distance-based fitness function can be locally approximated in the small-effect limit by a quadratic form whose curvature sets the scale. Fix this curvature so that the selection coefficient $s$ equals the negative of the change in squared distance, $s = -\\Delta$. This is a standard and convenient normalization that makes $s$ dimensionless and directly comparable across parameterizations of the curvature.\n\nRotating coordinates so that $\\mathbf{z}$ lies along the first coordinate axis, $\\mathbf{z} = (r, 0, \\ldots, 0)$, the change in squared distance after adding $\\mathbf{X}$ is\n$$\n\\Delta = \\lVert \\mathbf{z} + \\mathbf{X} \\rVert^2 - \\lVert \\mathbf{z} \\rVert^2\n= (r + X_1)^2 + \\sum_{i=2}^n X_i^2 - r^2 = X_1^2 + 2 r X_1 + \\sum_{i=2}^n X_i^2.\n$$\nDefine the shifted variables $Y_1 = X_1 + r$ and $Y_i = X_i$ for $i \\ge 2$. Then\n$$\nT \\equiv \\sum_{i=1}^n Y_i^2 = (X_1 + r)^2 + \\sum_{i=2}^n X_i^2.\n$$\nBy standard results on sums of squares of independent Gaussian random variables with nonzero means, $T$ follows a noncentral chi-square distribution with $n$ degrees of freedom and noncentrality parameter $\\delta = r^2$, denoted $T \\sim \\chi^2_n(\\delta)$. Moreover,\n$$\n\\Delta = T - \\delta \\quad \\Longrightarrow \\quad s = -\\Delta = \\delta - T.\n$$\nThus, conditional on $(r, n)$, the selection coefficient $s$ is a shifted and sign-flipped version of $T$, with a one-to-one transformation given by $T = \\delta - s$ and Jacobian magnitude $\\lvert \\mathrm{d}T / \\mathrm{d}s \\rvert = 1$. Because $T$ has support on $[0, \\infty)$, the support of $s$ under $(r, n)$ is $(-\\infty, \\delta]$, that is, $s \\le r^2$.\n\nGiven an observed set $\\{s_i\\}_{i=1}^m$, independence across observations implies that the likelihood for a candidate pair $(r, n)$ is the product of the corresponding densities evaluated at $T_i = \\delta - s_i$:\n$$\n\\mathcal{L}(r, n \\mid \\{s_i\\}) = \\prod_{i=1}^m f_{\\chi^2_n(\\delta)}(T_i),\n$$\nwith the convention that any $T_i  0$ yields zero likelihood. Equivalently, the log-likelihood is\n$$\n\\log \\mathcal{L}(r, n \\mid \\{s_i\\}) = \\sum_{i=1}^m \\log f_{\\chi^2_n(\\delta)}(\\delta - s_i),\n$$\nwith the convention that if any density is zero then the log-likelihood is negative infinity. The Jacobian of the transformation from $s$ to $T$ is $1$ in magnitude and contributes no additional term to the log-likelihood.\n\nThis establishes an explicit, likelihood-based connection between parameters $(r, n)$ and the observable selection coefficients $\\{s_i\\}$. To perform inference we adopt a discrete grid for computational tractability and identifiability:\n- The distance grid is $r \\in \\{0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0\\}$, implying $\\delta = r^2 \\in \\{0.0, 0.25, 1.0, 2.25, 4.0, 6.25, 9.0\\}$.\n- The dimension grid is $n \\in \\{1, 2, 3, 4, 5, 6\\}$.\n\nFor each dataset we compute the log-likelihood over this grid and select the maximizing pair $(\\hat{r}, \\hat{n})$. Because continuous densities can result in numerically indistinguishable maxima, we impose a deterministic tie-breaking rule: among pairs with log-likelihood within a small tolerance, choose the smallest $\\hat{r}$, and if still tied, choose the smallest $\\hat{n}$.\n\nEdge cases and constraints are naturally handled:\n- If any observation has $s_i  r^2$ for a candidate $r$, then $T_i = \\delta - s_i  0$ and that candidate has zero likelihood.\n- The presence of any beneficial observation $s_i  0$ immediately eliminates $r = 0$ because the support at $r = 0$ is $s \\le 0$.\n\nAlgorithmic steps for each dataset:\n- For each $r$ in the distance grid:\n  - Compute $\\delta = r^2$.\n  - Transform each $s_i$ to $T_i = \\delta - s_i$; if any $T_i  0$, assign log-likelihood negative infinity and skip.\n  - Otherwise, sum the logarithms of the noncentral chi-square densities $f_{\\chi^2_n(\\delta)}(T_i)$ over $i$ for each $n$ in the dimension grid.\n- Select $(\\hat{r}, \\hat{n})$ maximizing the log-likelihood with the specified tie-breaking.\n\nWe now apply this to the designated test suite:\n- Dataset A: $\\{-1.0, -3.0, -5.0\\}$. All entries are non-positive, so $r = 0$ is not excluded by support and will be competitive; the shape across $n$ values determines $\\hat{n}$.\n- Dataset B: $\\{3.8, -2.0, -6.0\\}$. Since $s_{\\max} = 3.8$, all $r$ with $r^2  3.8$ (namely, $r \\in \\{0.0, 0.5, 1.0, 1.5\\}$) are eliminated by support; this concentrates likelihood on larger $r$, with $n$ determined by density shape at the transformed $T$ values.\n- Dataset C: $\\{0.8, -2.0, -6.0\\}$. Here $s_{\\max} = 0.8$ eliminates only $r$ with $r^2  0.8$, i.e., $r \\in \\{0.0, 0.5\\}$, again leaving likelihood concentrated on allowed $r$.\n\nThe program computes the log-likelihood using the noncentral chi-square probability density function and returns the maximizing $(\\hat{r}, \\hat{n})$ for each dataset in the specified single-line list format.", "answer": "```python\n# Fisher's geometric model: likelihood-based inference of r and n from observed DFEs\n# Execution environment: Python 3.12, numpy 1.23.5, scipy 1.11.4\nimport numpy as np\nfrom scipy.stats import ncx2\n\ndef log_likelihood_for_dataset(s_vals, r, n, tol=1e-300):\n    \"\"\"\n    Compute the log-likelihood for one dataset under candidate (r, n).\n    Transformation: s = delta - T = T = delta - s, where delta = r^2.\n    If any T  0, the likelihood is zero (log-likelihood = -inf).\n    \"\"\"\n    delta = r * r\n    T = delta - np.array(s_vals, dtype=float)\n    if np.any(T  0):\n        return -np.inf\n    # Evaluate pdf; if any zero or negative due to underflow, treat as zero-likelihood\n    pdf_vals = ncx2.pdf(T, df=int(n), nc=delta)\n    if np.any(pdf_vals = 0.0) or np.any(~np.isfinite(pdf_vals)):\n        return -np.inf\n    return float(np.sum(np.log(pdf_vals)))\n\ndef infer_mle_pair(s_vals, r_grid, n_grid, tie_tol=1e-12):\n    \"\"\"\n    Grid search for MLE (r, n) maximizing the log-likelihood for the given dataset.\n    Tie-breaking: smallest r, then smallest n, when log-likelihoods are within tie_tol.\n    \"\"\"\n    best_ll = -np.inf\n    best_r = None\n    best_n = None\n    for r in r_grid:\n        delta = r * r\n        # Quick support check: if any s_i  delta then T_i  0, zero likelihood for this r (for all n)\n        if np.any(np.array(s_vals)  delta):\n            continue\n        for n in n_grid:\n            ll = log_likelihood_for_dataset(s_vals, r, n)\n            if ll  best_ll + tie_tol:\n                best_ll = ll\n                best_r = r\n                best_n = int(n)\n            elif abs(ll - best_ll) = tie_tol:\n                # Tie-break by smallest r, then smallest n\n                if best_r is None or r  best_r or (r == best_r and int(n)  best_n):\n                    best_r = r\n                    best_n = int(n)\n    # In degenerate case where all candidates have -inf (should not happen for provided datasets),\n    # fall back to smallest r, n.\n    if best_r is None:\n        best_r = float(r_grid[0])\n        best_n = int(n_grid[0])\n    return [float(best_r), int(best_n)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Dataset A, B, C:\n    datasets = [\n        [-1.0, -3.0, -5.0],   # A\n        [3.8, -2.0, -6.0],    # B\n        [0.8, -2.0, -6.0],    # C\n    ]\n    # Candidate grids\n    r_grid = np.array([0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0], dtype=float)\n    n_grid = np.array([1, 2, 3, 4, 5, 6], dtype=int)\n\n    results = []\n    for s_vals in datasets:\n        mle_pair = infer_mle_pair(s_vals, r_grid, n_grid)\n        results.append(mle_pair)\n\n    # Format as a single line with no spaces, as specified.\n    # Example: [[2.0,2],[0.0,3],[1.0,5]]\n    out = \"[\" + \",\".join(\"[\" + f\"{r:.1f}\".rstrip('0').rstrip('.') + (\".0\" if abs(r - round(r))  1e-12 else \"\") + f\",{n}]\" \n                         for r, n in results) + \"]\"\n    # The formatting above ensures floats like 2.0, 0.0, 1.0 keep one decimal.\n    # However, to strictly ensure one decimal place as in examples, enforce formatting explicitly:\n    out_fixed = \"[\" + \",\".join(\"[\" + f\"{r:.1f}\" + f\",{n}]\" for r, n in results) + \"]\"\n    print(out_fixed)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2713195"}]}