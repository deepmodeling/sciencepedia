{"hands_on_practices": [{"introduction": "Before we can understand the effects of complex evolutionary forces like selection or learning biases, we must first establish a baseline. This practice explores the fundamental dynamics of cultural drift, the process of random frequency change that acts as a 'null model' for cultural evolution. By deriving the fixation probability of a neutral variant in a finite population [@problem_id:2699390], you will uncover one of the most elegant and foundational results in evolutionary theory and gain a deeper appreciation for the power of stochastic chance in shaping cultural patterns.", "problem": "Consider a finite population of constant size $N$ evolving under the Wright–Fisher copying model for cultural transmission with no innovation. Time is discrete, indexed by $t \\in \\{0,1,2,\\dots\\}$, and each generation $t+1$ is formed by sampling $N$ cultural parents with replacement from generation $t$, each draw choosing a parent uniformly from the $N$ individuals in generation $t$. There are two cultural variants, denoted $A$ and $B$. Let $X_t \\in \\{0,1,\\dots,N\\}$ be the number of $A$-bearers in generation $t$, and let $x_t = X_t/N$ be the corresponding frequency. Assume neutrality and no innovation: in every draw the parent is chosen independently and uniformly, so that conditional on $X_t=i$, the distribution of $X_{t+1}$ is $\\mathrm{Binomial}(N, i/N)$.\n\nDefine the fixation probability $\\phi(i;N)$ to be the probability that variant $A$ eventually reaches frequency $1$ (i.e., $X_t$ hits the absorbing state $N$) starting from $X_0=i$. Equivalently, define $\\phi(x_0;N)$ with $x_0=i/N$.\n\nUsing only the fundamental definitions above, derive a closed-form expression for the fixation probability $\\phi(x_0;N)$ as a function of the initial frequency $x_0 \\in [0,1]$ and the population size $N$. Express your final answer as a single analytic expression. No rounding is required, and no units are involved.", "solution": "The problem asks for the fixation probability of a neutral cultural variant in a finite population of size $N$ evolving under the Wright–Fisher model. Let $X_t$ be the number of individuals carrying variant $A$ at generation $t$, and let $x_t = X_t/N$ be the frequency of this variant. The process starts with an initial count $X_0=i$, corresponding to an initial frequency $x_0 = i/N$. We are tasked with finding the probability that the variant $A$ eventually becomes fixed, meaning its frequency reaches $1$. This probability is denoted by $\\phi(x_0;N)$.\n\nThe state space of the process is $\\{0, 1, \\dots, N\\}$. The states $0$ and $N$ are absorbing states, corresponding to the loss and fixation of variant $A$, respectively. The process evolves according to the transition rule that, conditional on $X_t=k$, the number of $A$-bearers in the next generation, $X_{t+1}$, follows a binomial distribution:\n$$ X_{t+1} | (X_t = k) \\sim \\mathrm{Binomial}(N, k/N) $$\n\nA direct and rigorous method to solve this problem is to analyze the properties of the frequency process $\\{x_t\\}_{t \\geq 0}$. We will demonstrate that under the specified neutral model, the frequency of the variant is a martingale. A process $\\{Y_t\\}$ is a martingale if $\\mathbb{E}[Y_{t+1} | Y_t, Y_{t-1}, \\dots, Y_0] = Y_t$.\n\nLet us compute the conditional expectation of $x_{t+1}$ given the state at time $t$. Suppose at time $t$, the state is $X_t = i$. The frequency is $x_t = i/N$. The expected number of $A$-bearers at time $t+1$ is the expectation of a $\\mathrm{Binomial}(N, p=i/N)$ random variable. The expectation of a binomial distribution $\\mathrm{Binomial}(n,p)$ is $np$. Therefore,\n$$ \\mathbb{E}[X_{t+1} | X_t = i] = N \\cdot \\left(\\frac{i}{N}\\right) = i $$\nNow we can find the expected frequency at time $t+1$:\n$$ \\mathbb{E}[x_{t+1} | X_t = i] = \\mathbb{E}\\left[\\frac{X_{t+1}}{N} \\Big| X_t = i\\right] = \\frac{1}{N} \\mathbb{E}[X_{t+1} | X_t = i] = \\frac{i}{N} $$\nSince we have defined $x_t = i/N$, this shows that $\\mathbb{E}[x_{t+1} | x_t] = x_t$. This equality confirms that the frequency process $\\{x_t\\}$ is a martingale.\n\nThe process stops when it reaches one of the absorbing boundaries, either $x_t=0$ (loss) or $x_t=1$ (fixation). Let $T$ be the stopping time of this process, defined as:\n$$ T = \\inf\\{t \\geq 0 : x_t = 0 \\text{ or } x_t = 1\\} $$\nFor a finite population $N$, eventual absorption into one of these states is guaranteed, meaning the stopping time $T$ is almost surely finite. The martingale $\\{x_t\\}$ is bounded, as its values are always within the interval $[0,1]$.\n\nThese conditions—a bounded martingale and an almost surely finite stopping time—allow us to apply the Optional Stopping Theorem. The theorem states that the expected value of the martingale at the stopping time is equal to its initial value:\n$$ \\mathbb{E}[x_T] = x_0 $$\nThe value of the process at the stopping time, $x_T$, is a random variable that can only take one of two values: $1$ if the variant $A$ fixes, or $0$ if it is lost. The probability of fixation is precisely what we aim to find, $\\phi(x_0;N)$. Thus,\n$$ P(x_T = 1) = \\phi(x_0;N) $$\nAnd the probability of loss is:\n$$ P(x_T = 0) = 1 - \\phi(x_0;N) $$\nThe expectation $\\mathbb{E}[x_T]$ can be calculated from its definition:\n$$ \\mathbb{E}[x_T] = (1) \\cdot P(x_T = 1) + (0) \\cdot P(x_T = 0) = 1 \\cdot \\phi(x_0;N) + 0 \\cdot (1 - \\phi(x_0;N)) = \\phi(x_0;N) $$\nBy equating the two expressions for $\\mathbb{E}[x_T]$, we obtain the final result:\n$$ \\phi(x_0;N) = x_0 $$\nThis result demonstrates a fundamental principle of population genetics: in the absence of selection, mutation, or migration, the probability that a neutral allele or cultural variant will eventually become fixed in the population is equal to its initial frequency. The result is independent of the population size $N$. Given the initial frequency $x_0 = i/N$, the fixation probability is simply $i/N$.", "answer": "$$\\boxed{x_{0}}$$", "id": "2699390"}, {"introduction": "While cultural drift tends to remove variation from a population, observation tells us that cultures are rich with diverse variants. This is because drift is constantly opposed by forces that introduce novelty, such as innovation or error in transmission. This exercise [@problem_id:2699366] challenges you to model the long-term outcome of this dynamic balance. By deriving the stationary distribution of cultural variants under drift and symmetric innovation, you will learn how to characterize the statistical equilibrium of diversity that emerges from these opposing processes.", "problem": "Consider a large, well-mixed, haploid cultural population of constant size $N$ evolving in discrete, non-overlapping generations under a Wright–Fisher sampling scheme. Each new individual chooses a random role model uniformly from the previous generation and adopts that role model’s cultural variant (unbiased copying). With probability $\\mu$ per individual per generation, an innovation event occurs that overwrites the copied variant with a new variant drawn uniformly from a finite set of $k$ possible variants; that is, the innovation is parent-independent and symmetric over the $k$ variants.\n\nLet $\\mathbf{X}(t) = (X_{1}(t), \\dots, X_{k}(t))$ denote the frequency vector of the $k$ variants at generation $t$, with $\\sum_{i=1}^{k} X_{i}(t) = 1$. In the diffusion limit as $N \\to \\infty$ and $\\mu \\to 0$ with the compound parameter $\\theta \\equiv 2 N \\mu$ held constant, $\\mathbf{X}(t)$ is well-approximated by a continuous diffusion on the $(k-1)$-dimensional simplex. Under this approximation and in the absence of selection, the drift and diffusion coefficients are those of the neutral Wright–Fisher diffusion with parent-independent mutation:\n- The drift for component $i$ is $m_{i}(\\mathbf{x}) = \\mu \\left(\\frac{1}{k} - x_{i} \\right)$.\n- The diffusion covariance is $V_{ij}(\\mathbf{x}) = \\frac{1}{N} \\left( x_{i} \\delta_{ij} - x_{i} x_{j} \\right)$, where $\\delta_{ij}$ is the Kronecker delta.\n\nDerive, up to normalization and then normalize, the stationary probability density $p^{\\ast}(\\mathbf{x})$ over the simplex $\\{\\mathbf{x} \\in \\mathbb{R}^{k}_{\\ge 0}: \\sum_{i=1}^{k} x_{i} = 1\\}$ for this diffusion process. Express your final answer as a single closed-form analytic expression in terms of $k$, $N$, and $\\mu$, valid on the interior of the simplex, and normalized to integrate to $1$. Your final expression must be fully simplified in terms of Gamma functions. Provide the expression for $p^{\\ast}(\\mathbf{x})$; no numerical evaluation is required.", "solution": "The problem statement will first be subjected to a rigorous validation procedure.\n\nStep 1: Extract Givens.\n- Population: Large, well-mixed, haploid cultural population of constant size $N$.\n- Generations: Discrete, non-overlapping.\n- Sampling scheme: Wright–Fisher.\n- Cultural transmission: Unbiased copying from a uniformly chosen random role model.\n- Innovation: Occurs with probability $\\mu$ per individual per generation. The copied variant is overwritten with a new variant drawn uniformly from a set of $k$ possible variants. Innovation is parent-independent and symmetric.\n- State vector: $\\mathbf{X}(t) = (X_{1}(t), \\dots, X_{k}(t))$, frequencies of the $k$ variants. $\\sum_{i=1}^{k} X_{i}(t) = 1$.\n- Diffusion limit: $N \\to \\infty$, $\\mu \\to 0$, with $\\theta \\equiv 2 N \\mu$ constant.\n- Drift coefficient: $m_{i}(\\mathbf{x}) = \\mu \\left(\\frac{1}{k} - x_{i} \\right)$.\n- Diffusion covariance: $V_{ij}(\\mathbf{x}) = \\frac{1}{N} \\left( x_{i} \\delta_{ij} - x_{i} x_{j} \\right)$, where $\\delta_{ij}$ is the Kronecker delta.\n- Objective: Derive the normalized stationary probability density $p^{\\ast}(\\mathbf{x})$ over the simplex $\\{\\mathbf{x} \\in \\mathbb{R}^{k}_{\\ge 0}: \\sum_{i=1}^{k} x_{i} = 1\\}$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded. It describes a standard Wright–Fisher model with parent-independent mutation, a cornerstone of theoretical population genetics and widely applied to cultural evolution. The drift and diffusion coefficients are the correct, well-established forms for this process in the diffusion limit. The problem is well-posed, as it asks for the stationary distribution of a well-defined ergodic diffusion process, for which a unique solution exists and is known. The problem is objective, using precise mathematical language with no subjective or ambiguous terms. All necessary information is provided, and there are no internal contradictions. It does not violate any criteria for validity.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be derived.\n\nThe problem asks for the stationary probability distribution, $p^{\\ast}(\\mathbf{x})$, of a multivariate diffusion process described by the given drift vector $\\mathbf{m}(\\mathbf{x})$ and diffusion matrix $\\mathbf{V}(\\mathbf{x})$. This is a classic problem in population genetics, first solved by Sewall Wright. The process is the $k$-allele neutral Wright–Fisher model with symmetric, parent-independent mutation.\n\nThe stationary distribution $p^{\\ast}(\\mathbf{x})$ is the time-independent solution to the Fokker-Planck (or forward Kolmogorov) equation. For this class of models, it is known that the stationary distribution is a Dirichlet distribution. The general form of the stationary distribution for the neutral $k$-allele Wright-Fisher model is given by Wright's formula:\n$$ p^{\\ast}(\\mathbf{x}) \\propto \\prod_{i=1}^{k} x_{i}^{2Nu_{i} - 1} $$\nwhere $u_{i}$ is the mutation rate to allele $i$.\n\nWe must determine the effective mutation rate $u_{i}$ from the problem description. The innovation process acts as mutation. An innovation event occurs with probability $\\mu$ per individual. When this happens, a new variant is chosen uniformly from the set of $k$ possibilities. Therefore, the probability that an innovation results in a specific variant $i$ is $1/k$. The total rate at which any given lineage mutates to become variant $i$ is the product of the innovation rate and the probability of choosing variant $i$, that is:\n$$ u_{i} = \\mu \\cdot \\frac{1}{k} = \\frac{\\mu}{k} $$\nThis rate is the same for all variants $i=1, \\dots, k$, due to the symmetric nature of the innovation process.\n\nNow we substitute this effective mutation rate into Wright's formula. The exponents become:\n$$ 2Nu_{i} - 1 = 2N\\left(\\frac{\\mu}{k}\\right) - 1 = \\frac{2N\\mu}{k} - 1 $$\nThe problem defines the compound parameter $\\theta = 2N\\mu$. We can write the exponent as $\\frac{\\theta}{k} - 1$. Since this is the same for all $i$, the unnormalized stationary distribution is:\n$$ p^{\\ast}(\\mathbf{x}) \\propto \\prod_{i=1}^{k} x_{i}^{\\frac{2N\\mu}{k} - 1} $$\nThis expression defines a symmetric Dirichlet distribution with parameters $\\alpha_{i} = \\frac{2N\\mu}{k}$ for all $i=1, \\dots, k$.\n\nTo complete the solution, we must normalize this probability density function. The normalization constant, $C$, for a Dirichlet distribution with parameters $(\\alpha_1, \\dots, \\alpha_k)$ is given by the multivariate Beta function:\n$$ C = \\frac{\\Gamma\\left(\\sum_{i=1}^{k} \\alpha_{i}\\right)}{\\prod_{i=1}^{k} \\Gamma(\\alpha_{i})} $$\nIn our case, all $\\alpha_{i}$ are equal to $\\alpha = \\frac{2N\\mu}{k}$.\nThe sum of the parameters is:\n$$ \\sum_{i=1}^{k} \\alpha_{i} = \\sum_{i=1}^{k} \\frac{2N\\mu}{k} = k \\cdot \\frac{2N\\mu}{k} = 2N\\mu $$\nThe product of the Gamma functions of the parameters is:\n$$ \\prod_{i=1}^{k} \\Gamma(\\alpha_{i}) = \\prod_{i=1}^{k} \\Gamma\\left(\\frac{2N\\mu}{k}\\right) = \\left[\\Gamma\\left(\\frac{2N\\mu}{k}\\right)\\right]^k $$\nTherefore, the normalization constant is:\n$$ C = \\frac{\\Gamma(2N\\mu)}{\\left[\\Gamma\\left(\\frac{2N\\mu}{k}\\right)\\right]^k} $$\nCombining the normalization constant with the unnormalized density, we obtain the final expression for the stationary probability density $p^{\\ast}(\\mathbf{x})$ over the simplex:\n$$ p^{\\ast}(\\mathbf{x}) = \\frac{\\Gamma(2N\\mu)}{\\left[\\Gamma\\left(\\frac{2N\\mu}{k}\\right)\\right]^k} \\prod_{i=1}^{k} x_{i}^{\\frac{2N\\mu}{k} - 1} $$\nThis expression is valid for $\\mathbf{x}$ in the interior of the simplex (i.e., $x_i > 0$ for all $i$) and is properly normalized such that its integral over the simplex is equal to $1$.", "answer": "$$\\boxed{\\frac{\\Gamma(2N\\mu)}{\\left[\\Gamma\\left(\\frac{2N\\mu}{k}\\right)\\right]^k} \\prod_{i=1}^{k} x_{i}^{\\frac{2N\\mu}{k} - 1}}$$", "id": "2699366"}, {"introduction": "Theoretical models provide the conceptual framework for understanding cultural evolution, but their real power is revealed when they are connected to empirical data. This final practice moves from abstract theory to hands-on data analysis, providing a powerful accounting tool—the Price equation—to dissect observed cultural change into its constituent components. By implementing this framework on a concrete dataset [@problem_id:2699362], you will learn to quantitatively partition the change in a population's average trait into a component due to cultural selection (differential influence) and a component due to transmission biases (systematic errors in learning).", "problem": "You are given individual-level cultural data for a single generational step in a population with a set of cultural models (parents) and a set of learners. Each learner chooses one model to learn from and acquires a possibly modified trait value. Your task is to compute two components of population-level cultural change that partition the change in the mean trait value into a component attributable to cultural selection (differential influence of models) and a component attributable to transmission bias (systematic within-line changes during learning).\n\nFundamental base and definitions:\n- Let there be $N$ models indexed by $i \\in \\{1,\\dots,N\\}$, each with trait value $z_i \\in \\mathbb{R}$. Let the mean model trait be $\\bar{z} = \\frac{1}{N} \\sum_{i=1}^N z_i$.\n- Let there be $M$ learners indexed by $j \\in \\{1,\\dots,M\\}$. Each learner $j$ chooses exactly one model $m(j) \\in \\{1,\\dots,N\\}$ and acquires trait $z'_j \\in \\mathbb{R}$ after learning.\n- The cultural influence (number of learners) of model $i$ is $w_i = \\left| \\{ j : m(j) = i \\} \\right|$. The mean influence is $\\bar{w} = \\frac{1}{N} \\sum_{i=1}^N w_i$. Note that $\\sum_{i=1}^N w_i = M$ implies $\\bar{w} = M / N$.\n- Define the per-model learner mean $\\mu_i$ as follows: if $w_i > 0$, then $\\mu_i = \\frac{1}{w_i} \\sum_{j : m(j) = i} z'_j$; if $w_i = 0$, adopt the convention $\\mu_i = z_i$ so that $\\mu_i - z_i = 0$ contributes nothing to the transmission bias.\n\nYour program must compute, for each provided dataset:\n1. The cultural selection covariance term $S$, defined as the covariance between relative influence and model trait,\n$$\nS \\equiv \\operatorname{Cov}\\!\\left(\\frac{w_i}{\\bar{w}}, z_i\\right) \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}} - 1\\right)\\left(z_i - \\bar{z}\\right).\n$$\n2. The transmission bias term $T$, defined as the expected within-line change weighted by relative influence,\n$$\nT \\equiv \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right)\\left(\\mu_i - z_i\\right).\n$$\n3. The total change in the mean trait across the generational step,\n$$\n\\Delta \\bar{z} \\equiv \\left(\\frac{1}{M}\\sum_{j=1}^M z'_j\\right) - \\bar{z}.\n$$\n4. An interpretation code indicating which component has the larger magnitude. Let $\\varepsilon = 10^{-9}$. Define\n$$\nd \\equiv \\left|S\\right| - \\left|T\\right|.\n$$\nReturn an integer code per dataset as follows: if $d > \\varepsilon$, return $1$ (selection magnitude exceeds transmission); if $d < -\\varepsilon$, return $-1$ (transmission magnitude exceeds selection); otherwise return $0$ (magnitudes are comparable within tolerance).\n\nInput specification for each dataset:\n- $N$ and the model trait vector $\\mathbf{z} = [z_1,\\dots,z_N]$.\n- The learner-to-model index vector $\\mathbf{m} = [m(1),\\dots,m(M)]$ with zero-based indices in $\\{0,\\dots,N-1\\}$.\n- The learner trait vector after learning $\\mathbf{z}' = [z'_1,\\dots,z'_M]$.\nAll quantities are dimensionless real numbers.\n\nTest suite:\nProvide results for the following four datasets. Indices in $\\mathbf{m}$ are zero-based.\n\n- Dataset A:\n  - $N = 4$.\n  - $\\mathbf{z} = [0.0, 1.0, 2.0, 3.0]$.\n  - $\\mathbf{m} = [0, 1, 1, 2, 2, 3, 3, 3, 3, 3]$.\n  - $\\mathbf{z}' = [0.2, 1.0, 1.0, 1.9, 1.9, 2.8, 2.8, 2.8, 2.9, 2.9]$.\n\n- Dataset B:\n  - $N = 3$.\n  - $\\mathbf{z} = [1.0, 2.0, 3.0]$.\n  - $\\mathbf{m} = [0, 0, 1, 1, 2, 2]$.\n  - $\\mathbf{z}' = [1.3, 1.3, 2.0, 2.0, 2.8, 2.8]$.\n\n- Dataset C:\n  - $N = 3$.\n  - $\\mathbf{z} = [0.0, 1.0, 3.0]$.\n  - $\\mathbf{m} = [0, 1, 1, 2, 2, 2]$.\n  - $\\mathbf{z}' = [0.0, 1.0, 1.0, 3.0, 3.0, 3.0]$.\n\n- Dataset D:\n  - $N = 4$.\n  - $\\mathbf{z} = [1.0, 2.0, 3.0, 4.0]$.\n  - $\\mathbf{m} = [1, 2, 3, 3]$.\n  - $\\mathbf{z}' = [1.9, 3.2, 3.7, 3.9]$.\n\nOutput specification:\n- For each dataset, output a list of the form $[S, T, \\Delta\\bar{z}, \\text{code}]$ where $S$, $T$, and $\\Delta\\bar{z}$ are floats rounded to six decimal places, and $\\text{code}$ is an integer as defined above.\n- Your program should produce a single line of output containing the results for all datasets as a comma-separated list of these lists enclosed in square brackets. For example, the output format must be\n$[[S_A,T_A,\\Delta\\bar{z}_A,\\text{code}_A],[S_B,T_B,\\Delta\\bar{z}_B,\\text{code}_B],[S_C,T_C,\\Delta\\bar{z}_C,\\text{code}_C],[S_D,T_D,\\Delta\\bar{z}_D,\\text{code}_D]]$\nwith the required rounding applied.", "solution": "The problem as stated has been subjected to rigorous validation. It is deemed to be a valid, well-posed, and scientifically grounded exercise. The definitions and computations requested are a direct application of the Price equation framework, a cornerstone of evolutionary analysis, adapted for the study of cultural evolution. All terms are defined with mathematical precision, the data provided are self-contained, and the objectives are unambiguous. There are no logical contradictions or factual errors. We shall proceed with the derivation of the solution.\n\nThe core task is to partition the total change in the mean population trait value, $\\Delta\\bar{z}$, into two components: a selection component, $S$, and a transmission bias component, $T$. The problem provides definitions for these three quantities. A fundamental check of consistency is to demonstrate that the total change is, in fact, the sum of the two components, i.e., $\\Delta\\bar{z} = S + T$. This identity forms the basis of our analysis.\n\nLet us begin with the definitions provided. The mean trait of the $N$ models is $\\bar{z} = \\frac{1}{N} \\sum_{i=1}^N z_i$. The mean trait of the $M$ learners in the next generation is $\\bar{z}' = \\frac{1}{M}\\sum_{j=1}^M z'_j$. The total change is $\\Delta\\bar{z} = \\bar{z}' - \\bar{z}$.\n\nWe can express $\\bar{z}'$ by grouping learners according to their chosen model $m(j)$. The number of learners who chose model $i$ is its influence, $w_i$. The sum of the trait values of learners who chose model $i$ is $\\sum_{j : m(j) = i} z'_j$. By definition, the per-model learner mean is $\\mu_i = \\frac{1}{w_i} \\sum_{j : m(j) = i} z'_j$ for $w_i > 0$. Therefore, $\\sum_{j : m(j) = i} z'_j = w_i \\mu_i$. This relation holds even for $w_i=0$, as both sides are zero. The total sum of learner traits is $\\sum_{j=1}^M z'_j = \\sum_{i=1}^N \\sum_{j : m(j) = i} z'_j = \\sum_{i=1}^N w_i \\mu_i$.\n\nSubstituting this into the expression for $\\bar{z}'$:\n$$\n\\bar{z}' = \\frac{1}{M} \\sum_{i=1}^N w_i \\mu_i\n$$\nThe mean influence is $\\bar{w} = \\sum_{i=1}^N w_i / N = M/N$. Thus, $M = N\\bar{w}$. Substituting this for $M$:\n$$\n\\bar{z}' = \\frac{1}{N\\bar{w}} \\sum_{i=1}^N w_i \\mu_i = \\frac{1}{N} \\sum_{i=1}^N \\frac{w_i}{\\bar{w}} \\mu_i\n$$\nNow, we can write the total change $\\Delta\\bar{z}$ as:\n$$\n\\Delta\\bar{z} = \\bar{z}' - \\bar{z} = \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) \\mu_i - \\bar{z}\n$$\nTo partition this change, we introduce the term $\\frac{1}{N} \\sum_{i=1}^N (\\frac{w_i}{\\bar{w}}) z_i$ by adding and subtracting it:\n$$\n\\Delta\\bar{z} = \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) \\mu_i - \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) z_i \\right) + \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) z_i - \\bar{z} \\right)\n$$\nLet us analyze the two parenthesized terms. The first term can be written as:\n$$\n\\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}}\\right) (\\mu_i - z_i)\n$$\nThis is precisely the definition of the transmission bias term, $T$. It represents the average change in trait value from model to learner, weighted by the relative influence of each model. The convention $\\mu_i = z_i$ when $w_i = 0$ ensures that models with no learners contribute nothing to this term.\n\nThe second term can be recognized as the covariance between relative influence $\\frac{w_i}{\\bar{w}}$ and model trait $z_i$. The expectation of a variable $X_i$ over the population of models is $E[X] = \\frac{1}{N}\\sum_{i=1}^N X_i$. The covariance is $\\operatorname{Cov}(X, Y) = E[(X-E[X])(Y-E[Y])]$.\nLet $X_i = \\frac{w_i}{\\bar{w}}$ and $Y_i = z_i$. Then $E[Y] = \\bar{z}$. The expectation of $X_i$ is $E[X] = \\frac{1}{N}\\sum_{i=1}^N \\frac{w_i}{\\bar{w}} = \\frac{1}{N\\bar{w}}\\sum_{i=1}^N w_i = \\frac{M}{N(M/N)} = 1$.\nThe second term is $E[XY] - \\bar{z} = E[XY] - E[X]E[Y]$, which is $\\operatorname{Cov}(X,Y)$. Expanding this gives:\n$$\n\\operatorname{Cov}\\left(\\frac{w_i}{\\bar{w}}, z_i\\right) = \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{w_i}{\\bar{w}} - 1\\right) (z_i - \\bar{z})\n$$\nThis is precisely the definition of the cultural selection term, $S$. It measures the statistical association between a model's trait value and its cultural influence.\n\nThus, we have demonstrated the identity $\\Delta\\bar{z} = S + T$. The total change is perfectly partitioned. The computational procedure is as follows:\n\nFor each dataset ($N$, $\\mathbf{z}$, $\\mathbf{m}$, $\\mathbf{z}'$):\n1.  Compute the mean model trait $\\bar{z} = \\frac{1}{N}\\sum z_i$.\n2.  Determine the number of learners $M$ from the length of $\\mathbf{m}$.\n3.  Calculate the influence vector $\\mathbf{w}$, where $w_i$ is the count of model index $i$ in $\\mathbf{m}$. The indices in $\\mathbf{m}$ are given as zero-based, $\\{0, \\dots, N-1\\}$.\n4.  Compute the mean influence $\\bar{w} = M/N$.\n5.  Determine the per-model learner mean vector $\\boldsymbol{\\mu}$. For each model $i$, if $w_i > 0$, $\\mu_i$ is the mean of $z'_j$ for all learners $j$ who chose model $i$. If $w_i = 0$, $\\mu_i$ is set to $z_i$.\n6.  Calculate the selection term $S = \\frac{1}{N}\\sum_{i=0}^{N-1} (\\frac{w_i}{\\bar{w}} - 1)(z_i - \\bar{z})$.\n7.  Calculate the transmission bias term $T = \\frac{1}{N}\\sum_{i=0}^{N-1} (\\frac{w_i}{\\bar{w}})(\\mu_i - z_i)$.\n8.  Calculate the total change $\\Delta\\bar{z} = (\\frac{1}{M}\\sum z'_j) - \\bar{z}$. As a verification, one must confirm that $S + T$ is approximately equal to $\\Delta\\bar{z}$.\n9.  Compute the magnitude difference $d = |S| - |T|$ and determine the interpretation code based on the given tolerance $\\varepsilon = 10^{-9}$. If $d > \\varepsilon$, code is $1$; if $d < -\\varepsilon$, code is $-1$; otherwise, code is $0$.\n10. Format the results as a list $[S, T, \\Delta\\bar{z}, \\text{code}]$, with floating-point numbers rounded to six decimal places.\n\nThis algorithm will be implemented to process the provided test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cultural evolution problem for all given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"N\": 4,\n            \"z_models\": [0.0, 1.0, 2.0, 3.0],\n            \"m_learners\": [0, 1, 1, 2, 2, 3, 3, 3, 3, 3],\n            \"z_prime_learners\": [0.2, 1.0, 1.0, 1.9, 1.9, 2.8, 2.8, 2.8, 2.9, 2.9],\n        },\n        # Dataset B\n        {\n            \"N\": 3,\n            \"z_models\": [1.0, 2.0, 3.0],\n            \"m_learners\": [0, 0, 1, 1, 2, 2],\n            \"z_prime_learners\": [1.3, 1.3, 2.0, 2.0, 2.8, 2.8],\n        },\n        # Dataset C\n        {\n            \"N\": 3,\n            \"z_models\": [0.0, 1.0, 3.0],\n            \"m_learners\": [0, 1, 1, 2, 2, 2],\n            \"z_prime_learners\": [0.0, 1.0, 1.0, 3.0, 3.0, 3.0],\n        },\n        # Dataset D\n        {\n            \"N\": 4,\n            \"z_models\": [1.0, 2.0, 3.0, 4.0],\n            \"m_learners\": [1, 2, 3, 3],\n            \"z_prime_learners\": [1.9, 3.2, 3.7, 3.9],\n        },\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        s, t, delta_z, code = calculate_components(\n            case[\"N\"],\n            case[\"z_models\"],\n            case[\"m_learners\"],\n            case[\"z_prime_learners\"]\n        )\n        \n        # Format the output for the current case\n        s_str = f\"{s:.6f}\"\n        t_str = f\"{t:.6f}\"\n        delta_z_str = f\"{delta_z:.6f}\"\n        \n        result_str = f\"[{s_str},{t_str},{delta_z_str},{code}]\"\n        results_str_list.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(results_str_list)}]]\")\n\ndef calculate_components(N_models, z_models_list, m_learners_list, z_prime_learners_list):\n    \"\"\"\n    Computes S, T, delta_z, and the interpretation code for a single dataset.\n    \"\"\"\n    \n    # Convert lists to numpy arrays for vectorized operations\n    z_models = np.array(z_models_list, dtype=float)\n    m_learners = np.array(m_learners_list, dtype=int)\n    z_prime_learners = np.array(z_prime_learners_list, dtype=float)\n    \n    # Number of learners\n    M_learners = len(m_learners)\n\n    # 1. Compute model-level quantities\n    z_bar = np.mean(z_models)\n\n    # 2. Compute learner-level and linking quantities\n    # Influence of each model (number of learners choosing each model)\n    w = np.bincount(m_learners, minlength=N_models)\n    \n    # Mean influence\n    if N_models > 0:\n        w_bar = M_learners / N_models\n    else:\n        w_bar = 0\n\n    # 3. Calculate per-model learner mean mu\n    mu = np.zeros(N_models, dtype=float)\n    for i in range(N_models):\n        if w[i] > 0:\n            learners_of_model_i = z_prime_learners[m_learners == i]\n            mu[i] = np.mean(learners_of_model_i)\n        else:\n            # Convention: if w_i = 0, mu_i = z_i\n            mu[i] = z_models[i]\n\n    # Handle case where all models have zero influence\n    if w_bar == 0:\n        rel_w = np.zeros(N_models, dtype=float)\n    else:\n        rel_w = w / w_bar\n\n    # 4. Calculate S (Selection)\n    # S = (1/N) * sum((w_i/w_bar - 1) * (z_i - z_bar))\n    s_term_per_model = (rel_w - 1) * (z_models - z_bar)\n    S = np.mean(s_term_per_model)\n\n    # 5. Calculate T (Transmission)\n    # T = (1/N) * sum((w_i/w_bar) * (mu_i - z_i))\n    t_term_per_model = rel_w * (mu - z_models)\n    T = np.mean(t_term_per_model)\n\n    # 6. Calculate Delta z_bar (Total Change)\n    if M_learners > 0:\n        z_prime_bar = np.mean(z_prime_learners)\n    else:\n        z_prime_bar = z_bar # No learners, no change\n    delta_z_bar = z_prime_bar - z_bar\n\n    # 7. Calculate the interpretation code\n    epsilon = 1e-9\n    d = abs(S) - abs(T)\n    if d > epsilon:\n        code = 1\n    elif d  -epsilon:\n        code = -1\n    else:\n        code = 0\n        \n    return S, T, delta_z_bar, code\n\n# Execute the main function\nsolve()\n```", "id": "2699362"}]}