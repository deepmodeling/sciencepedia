{"hands_on_practices": [{"introduction": "A central goal in experimental evolution is to precisely measure the strength of selection acting on a mutation. This practice [@problem_id:2712517] walks you through the fundamental derivation of the selection coefficient, $s$, from a competitive fitness assay, a workhorse method in the field. By starting from a model of exponential growth and incorporating the statistical nature of measurement, you will derive not only the estimator for $s$ but also its variance, a critical step for hypothesis testing and understanding the limits of your experimental precision.", "problem": "In a batch-culture experimental evolution assay, two asexual strains, denoted by $A$ and $B$, compete for $t$ generations under constant conditions. Assume that each strain has a constant Malthusian fitness per generation, denoted by $m_A$ and $m_B$, respectively, and that the deterministic population dynamics follow exponential growth so that the expected abundances satisfy $N_A(t) = N_A(0)\\exp(m_A t)$ and $N_B(t) = N_B(0)\\exp(m_B t)$. Define the selection coefficient per generation as $s = m_A - m_B$. At time $0$ and time $t$ you measure the strainsâ€™ relative abundances using flow cytometry on large, well-mixed aliquots, producing counts $(A_0,B_0)$ from a total of $N_0 = A_0 + B_0$ events at time $0$, and $(A_t,B_t)$ from a total of $N_t = A_t + B_t$ events at time $t$. Model the cytometry sampling at each time point as an independent binomial draw from the true underlying frequency $p_\\tau = N_A(\\tau)/[N_A(\\tau)+N_B(\\tau)]$ with sample size $N_\\tau$, that is, $A_\\tau \\sim \\mathrm{Binomial}(N_\\tau, p_\\tau)$ for $\\tau \\in \\{0,t\\}$ with $B_\\tau = N_\\tau - A_\\tau$.\n\nStarting from these assumptions and the definition of the selection coefficient as a difference in Malthusian parameters, derive:\n- a closed-form estimator $\\hat{s}$ expressed in terms of the observed counts $(A_0,B_0,A_t,B_t)$ and the known number of generations $t$, and\n- a first-order delta-method approximation to $\\mathrm{Var}(\\hat{s})$ that accounts for sampling variance in the cytometry counts at times $0$ and $t$.\n\nState any regularity assumptions you invoke, and use a plug-in approach for any unknown nuisance parameters. Provide your final answer as exact analytic expressions. Express $s$ in per-generation units. Do not include units inside the final boxed answer. If you choose to present multiple expressions, present them as a single row vector.", "solution": "The problem requires the derivation of an estimator for the selection coefficient, $\\hat{s}$, and a first-order approximation for its variance, $\\mathrm{Var}(\\hat{s})$, from a model of competitive growth and binomial sampling.\n\nWe begin by validating the problem statement.\n**Step 1: Extract Givens**\n- Strains: $A$ and $B$, asexual.\n- Time of competition: $t$ generations.\n- Malthusian fitness parameters: $m_A$, $m_B$.\n- Population dynamics: $N_A(t) = N_A(0)\\exp(m_A t)$ and $N_B(t) = N_B(0)\\exp(m_B t)$.\n- Selection coefficient: $s = m_A - m_B$.\n- Observed counts: $(A_0, B_0)$ from a sample of size $N_0 = A_0 + B_0$ at time $\\tau=0$; $(A_t, B_t)$ from a sample of size $N_t = A_t + B_t$ at time $\\tau=t$.\n- Sampling model: $A_\\tau \\sim \\mathrm{Binomial}(N_\\tau, p_\\tau)$ for $\\tau \\in \\{0,t\\}$, where $p_\\tau = N_A(\\tau) / (N_A(\\tau) + N_B(\\tau))$ is the true frequency of strain $A$ at time $\\tau$. The two samples are independent.\n- Task: Derive a closed-form estimator $\\hat{s}$ in terms of $(A_0, B_0, A_t, B_t, t)$ and an approximation for $\\mathrm{Var}(\\hat{s})$ using the delta method and a plug-in approach for unknown parameters.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing standard models from population genetics and experimental evolution (exponential growth, Malthusian fitness, binomial sampling). It is well-posed, providing sufficient information for a unique solution. The language is objective and precise. The assumptions, while simplifications of reality, are standard and reasonable in the context of many microbial evolution experiments. The problem is not trivial and requires a rigorous application of statistical theory. Therefore, the problem is deemed valid.\n\n**Step 3: Proceed with Solution**\n\n**Derivation of the Estimator $\\hat{s}$**\n\nThe population dynamics for the two strains are given by:\n$$N_A(t) = N_A(0)\\exp(m_A t)$$\n$$N_B(t) = N_B(0)\\exp(m_B t)$$\nWe consider the ratio of the abundances of the two strains at time $t$:\n$$ \\frac{N_A(t)}{N_B(t)} = \\frac{N_A(0)\\exp(m_A t)}{N_B(0)\\exp(m_B t)} = \\frac{N_A(0)}{N_B(0)} \\exp\\left( (m_A - m_B)t \\right) $$\nUsing the definition of the selection coefficient, $s = m_A - m_B$, we have:\n$$ \\frac{N_A(t)}{N_B(t)} = \\frac{N_A(0)}{N_B(0)} \\exp(st) $$\nTo find an expression for $s$, we rearrange the equation and take the natural logarithm:\n$$ \\exp(st) = \\frac{N_A(t)/N_B(t)}{N_A(0)/N_B(0)} $$\n$$ st = \\ln\\left( \\frac{N_A(t)/N_B(t)}{N_A(0)/N_B(0)} \\right) = \\ln\\left(\\frac{N_A(t)}{N_B(t)}\\right) - \\ln\\left(\\frac{N_A(0)}{N_B(0)}\\right) $$\nThus, the selection coefficient is given by:\n$$ s = \\frac{1}{t} \\left[ \\ln\\left(\\frac{N_A(t)}{N_B(t)}\\right) - \\ln\\left(\\frac{N_A(0)}{N_B(0)}\\right) \\right] $$\nThis equation relates $s$ to the true, unobserved population ratios. We must construct an estimator from the observed counts. The true ratio at time $\\tau$ can be expressed in terms of the true frequency $p_\\tau = N_A(\\tau) / (N_A(\\tau) + N_B(\\tau))$ as $N_A(\\tau)/N_B(\\tau) = p_\\tau/(1-p_\\tau)$. We estimate these true frequencies using the sample frequencies from the cytometry data:\n$$ \\hat{p}_0 = \\frac{A_0}{N_0} = \\frac{A_0}{A_0+B_0} \\quad \\text{and} \\quad \\hat{p}_t = \\frac{A_t}{N_t} = \\frac{A_t}{A_t+B_t} $$\nThe corresponding estimated ratios are:\n$$ \\frac{\\hat{p}_0}{1-\\hat{p}_0} = \\frac{A_0/N_0}{B_0/N_0} = \\frac{A_0}{B_0} \\quad \\text{and} \\quad \\frac{\\hat{p}_t}{1-\\hat{p}_t} = \\frac{A_t/N_t}{B_t/N_t} = \\frac{A_t}{B_t} $$\nUsing the plug-in principle, we substitute these estimates into the equation for $s$ to obtain the estimator $\\hat{s}$:\n$$ \\hat{s} = \\frac{1}{t} \\left[ \\ln\\left(\\frac{A_t}{B_t}\\right) - \\ln\\left(\\frac{A_0}{B_0}\\right) \\right] $$\nThis can be written more compactly as:\n$$ \\hat{s} = \\frac{1}{t} \\ln\\left( \\frac{A_t B_0}{A_0 B_t} \\right) $$\nA regularity assumption for this estimator is that all counts $A_0, B_0, A_t, B_t$ must be strictly positive for the logarithms to be defined.\n\n**Derivation of the Variance Approximation $\\mathrm{Var}(\\hat{s})$**\n\nWe use the delta method to approximate the variance of $\\hat{s}$. The estimator $\\hat{s}$ is a function of the random variables $A_0$ and $A_t$ (since $B_0=N_0-A_0$ and $B_t=N_t-A_t$, where $N_0$ and $N_t$ are fixed sample sizes). Let us define the function $g(x,y)$:\n$$ g(x,y) = \\frac{1}{t} \\left[ \\ln\\left(\\frac{y}{N_t-y}\\right) - \\ln\\left(\\frac{x}{N_0-x}\\right) \\right] $$\nSo, $\\hat{s} = g(A_0, A_t)$. The random variables $A_0$ and $A_t$ are independent by the problem statement. According to the delta method, the variance of $\\hat{s}$ is approximated by:\n$$ \\mathrm{Var}(\\hat{s}) \\approx \\left( \\frac{\\partial g}{\\partial x} \\right)^2 \\mathrm{Var}(A_0) + \\left( \\frac{\\partial g}{\\partial y} \\right)^2 \\mathrm{Var}(A_t) $$\nwhere the partial derivatives are evaluated at the expected values of $A_0$ and $A_t$, which are $\\mathrm{E}[A_0] = N_0 p_0$ and $\\mathrm{E}[A_t] = N_t p_t$.\n\nFirst, we compute the partial derivatives of $g(x,y) = \\frac{1}{t} [ \\ln(y) - \\ln(N_t-y) - \\ln(x) + \\ln(N_0-x) ]$:\n$$ \\frac{\\partial g}{\\partial x} = \\frac{1}{t} \\left[ -\\frac{1}{x} + \\frac{1}{N_0-x} \\right] = \\frac{1}{t} \\frac{-N_0+x-x}{x(N_0-x)} = -\\frac{N_0}{t x (N_0-x)} $$\n$$ \\frac{\\partial g}{\\partial y} = \\frac{1}{t} \\left[ \\frac{1}{y} - \\frac{-1}{N_t-y} \\right] = \\frac{1}{t} \\frac{N_t-y+y}{y(N_t-y)} = \\frac{N_t}{t y (N_t-y)} $$\nNext, we evaluate these derivatives at $x=\\mathrm{E}[A_0]=N_0 p_0$ and $y=\\mathrm{E}[A_t]=N_t p_t$:\n$$ \\left. \\frac{\\partial g}{\\partial x} \\right|_{x=N_0 p_0} = -\\frac{N_0}{t (N_0 p_0) (N_0 - N_0 p_0)} = -\\frac{1}{t N_0 p_0 (1-p_0)} $$\n$$ \\left. \\frac{\\partial g}{\\partial y} \\right|_{y=N_t p_t} = \\frac{N_t}{t (N_t p_t) (N_t - N_t p_t)} = \\frac{1}{t N_t p_t (1-p_t)} $$\nThe variances of the binomial random variables are $\\mathrm{Var}(A_0) = N_0 p_0 (1-p_0)$ and $\\mathrm{Var}(A_t) = N_t p_t (1-p_t)$.\nSubstituting these into the delta method formula:\n$$ \\mathrm{Var}(\\hat{s}) \\approx \\left( -\\frac{1}{t N_0 p_0 (1-p_0)} \\right)^2 [N_0 p_0 (1-p_0)] + \\left( \\frac{1}{t N_t p_t (1-p_t)} \\right)^2 [N_t p_t (1-p_t)] $$\n$$ \\mathrm{Var}(\\hat{s}) \\approx \\frac{1}{t^2 N_0 p_0 (1-p_0)} + \\frac{1}{t^2 N_t p_t (1-p_t)} = \\frac{1}{t^2} \\left[ \\frac{1}{N_0 p_0 (1-p_0)} + \\frac{1}{N_t p_t (1-p_t)} \\right] $$\nThis approximation depends on the unknown true frequencies $p_0$ and $p_t$. As instructed, we use a plug-in approach, replacing the true frequencies with their sample estimates $\\hat{p}_0 = A_0/N_0$ and $\\hat{p}_t = A_t/N_t$. This yields an estimator for the variance, which we also denote by $\\mathrm{Var}(\\hat{s})$ for simplicity.\n$$ \\mathrm{Var}(\\hat{s}) \\approx \\frac{1}{t^2} \\left[ \\frac{1}{N_0 \\hat{p}_0 (1-\\hat{p}_0)} + \\frac{1}{N_t \\hat{p}_t (1-\\hat{p}_t)} \\right] $$\nLet's simplify the terms in the bracket:\n$$ \\frac{1}{N_0 \\hat{p}_0 (1-\\hat{p}_0)} = \\frac{1}{N_0 (A_0/N_0) (B_0/N_0)} = \\frac{N_0}{A_0 B_0} = \\frac{A_0+B_0}{A_0 B_0} = \\frac{1}{A_0} + \\frac{1}{B_0} $$\nSimilarly, for the term at time $t$:\n$$ \\frac{1}{N_t \\hat{p}_t (1-\\hat{p}_t)} = \\frac{1}{A_t} + \\frac{1}{B_t} $$\nSubstituting these back, we obtain the final expression for the variance approximation, expressed in terms of the observed counts:\n$$ \\mathrm{Var}(\\hat{s}) \\approx \\frac{1}{t^2} \\left( \\frac{1}{A_0} + \\frac{1}{B_0} + \\frac{1}{A_t} + \\frac{1}{B_t} \\right) $$\nThis expression represents the estimated variance of our estimator $\\hat{s}$.\nThe two required expressions are thus derived.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{t} \\ln\\left(\\frac{A_t B_0}{A_0 B_t}\\right) & \\frac{1}{t^2} \\left( \\frac{1}{A_0} + \\frac{1}{B_0} + \\frac{1}{A_t} + \\frac{1}{B_t} \\right) \\end{pmatrix} } $$", "id": "2712517"}, {"introduction": "The effective population size, $N_e$, governs the strength of genetic drift and is a cornerstone of population genetics, but its estimation in real experiments can be subtle. This exercise [@problem_id:2712475] explores how fluctuations in daily bottleneck sizes, a common occurrence due to pipetting inaccuracies, can lead to a systematic overestimation of $N_e$ if the wrong averaging method is used. By applying a Taylor series approximation, you will quantify this bias and gain a deeper appreciation for why the harmonic mean is the appropriate measure for effective size in the face of varying population sizes.", "problem": "In a daily serial transfer experiment with an asexual microbe, each day $t$ a bottleneck of size $N_{b}(t)$ is imposed by pipetting, followed by a large expansion phase that does not contribute substantially to genetic drift relative to the bottleneck event. Assume that the daily bottleneck sizes $N_{b}(t)$ are independent and identically distributed with finite mean $\\mu \\equiv \\mathbb{E}[N_{b}]$ and variance $\\sigma^{2} \\equiv \\operatorname{Var}(N_{b})$, and that the squared coefficient of variation $c_{v}^{2} \\equiv \\sigma^{2}/\\mu^{2}$ is small. Over $T$ discrete daily transfers with $T \\gg 1$ and nonoverlapping generations, it is a well-tested fact that the inbreeding effective population size (denoted $N_{e}$) for time-varying census sizes is the harmonic mean across time of the daily census sizes.\n\nAn investigator erroneously estimates $N_{e}$ using the arithmetic mean $\\widehat{N}_{e}^{\\,\\text{arith}} \\equiv \\frac{1}{T}\\sum_{t=1}^{T} N_{b}(t)$, whereas the correct estimate under these assumptions is the harmonic mean $\\widehat{N}_{e}^{\\,\\text{harm}} \\equiv \\left(\\frac{1}{T}\\sum_{t=1}^{T} \\frac{1}{N_{b}(t)}\\right)^{-1}$. In the limit $T \\to \\infty$, these converge almost surely to $\\mu$ and $\\left(\\mathbb{E}[1/N_{b}]\\right)^{-1}$ respectively by the law of large numbers.\n\nUsing only the above fundamentals and a second-order expansion in $c_{v}^{2}$, derive the leading-order relative bias of the arithmetic-mean estimator compared to the true effective size,\n$$\n\\beta \\equiv \\frac{\\widehat{N}_{e}^{\\,\\text{arith}} - \\widehat{N}_{e}^{\\,\\text{harm}}}{\\widehat{N}_{e}^{\\,\\text{harm}}},\n$$\nexpressed as a closed-form function of $\\mu$ and $\\sigma^{2}$. Provide your final answer as a simplified analytic expression. No numerical rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard scenario in population genetics and asks for a derivation based on established mathematical principles. The problem is therefore valid, and we may proceed with the solution.\n\nThe problem asks for the relative bias $\\beta$ of the arithmetic-mean estimator $\\widehat{N}_{e}^{\\,\\text{arith}}$ with respect to the correct harmonic-mean estimator $\\widehat{N}_{e}^{\\,\\text{harm}}$ for the effective population size. The definition of the bias is given as:\n$$\n\\beta = \\frac{\\widehat{N}_{e}^{\\,\\text{arith}} - \\widehat{N}_{e}^{\\,\\text{harm}}}{\\widehat{N}_{e}^{\\,\\text{harm}}}\n$$\nThe problem considers the limit of a large number of transfers, $T \\to \\infty$. In this limit, by the Law of Large Numbers, the sample means converge almost surely to the true expectations.\nThe arithmetic mean of the bottleneck sizes converges to the expectation of the bottleneck size:\n$$\n\\widehat{N}_{e}^{\\,\\text{arith}} = \\frac{1}{T}\\sum_{t=1}^{T} N_{b}(t) \\xrightarrow[T\\to\\infty]{a.s.} \\mathbb{E}[N_{b}] = \\mu\n$$\nThe harmonic mean estimator is the reciprocal of the arithmetic mean of the reciprocals. Thus, it converges to the reciprocal of the expectation of the reciprocal a bottleneck size:\n$$\n\\widehat{N}_{e}^{\\,\\text{harm}} = \\left(\\frac{1}{T}\\sum_{t=1}^{T} \\frac{1}{N_{b}(t)}\\right)^{-1} \\xrightarrow[T\\to\\infty]{a.s.} \\left(\\mathbb{E}\\left[\\frac{1}{N_{b}}\\right]\\right)^{-1}\n$$\nSubstituting these asymptotic values into the expression for the relative bias $\\beta$, we obtain:\n$$\n\\beta = \\frac{\\mu - \\left(\\mathbb{E}\\left[\\frac{1}{N_{b}}\\right]\\right)^{-1}}{\\left(\\mathbb{E}\\left[\\frac{1}{N_{b}}\\right]\\right)^{-1}} = \\mu \\cdot \\mathbb{E}\\left[\\frac{1}{N_{b}}\\right] - 1\n$$\nThe central task is to find an approximation for the term $\\mathbb{E}[1/N_{b}]$. The problem specifies using a second-order expansion, justified by the assumption that the squared coefficient of variation $c_{v}^{2} = \\sigma^{2}/\\mu^{2}$ is small. This implies that the random variable $N_{b}$ does not deviate substantially from its mean $\\mu$.\n\nWe use a Taylor series expansion for the function $f(x) = 1/x$ around the point $x = \\mu$. The general form of the expansion up to the second-order term is:\n$$\nf(x) \\approx f(\\mu) + f'(\\mu)(x - \\mu) + \\frac{f''(\\mu)}{2!}(x - \\mu)^{2}\n$$\nWe evaluate the function and its first two derivatives at $x = \\mu$:\n-   $f(x) = x^{-1} \\implies f(\\mu) = \\mu^{-1} = \\frac{1}{\\mu}$\n-   $f'(x) = -x^{-2} \\implies f'(\\mu) = -\\mu^{-2} = -\\frac{1}{\\mu^{2}}$\n-   $f''(x) = 2x^{-3} \\implies f''(\\mu) = 2\\mu^{-3} = \\frac{2}{\\mu^{3}}$\n\nSubstituting these into the expansion for $f(N_{b})$ where $N_b$ is the random variable:\n$$\n\\frac{1}{N_{b}} \\approx \\frac{1}{\\mu} - \\frac{1}{\\mu^{2}}(N_{b} - \\mu) + \\frac{1}{2!} \\left(\\frac{2}{\\mu^{3}}\\right) (N_{b} - \\mu)^{2}\n$$\n$$\n\\frac{1}{N_{b}} \\approx \\frac{1}{\\mu} - \\frac{N_{b} - \\mu}{\\mu^{2}} + \\frac{(N_{b} - \\mu)^{2}}{\\mu^{3}}\n$$\nTo find the expectation $\\mathbb{E}[1/N_{b}]$, we take the expectation of both sides of this approximation. By the linearity of the expectation operator:\n$$\n\\mathbb{E}\\left[\\frac{1}{N_{b}}\\right] \\approx \\mathbb{E}\\left[\\frac{1}{\\mu} - \\frac{N_{b} - \\mu}{\\mu^{2}} + \\frac{(N_{b} - \\mu)^{2}}{\\mu^{3}}\\right]\n$$\n$$\n\\mathbb{E}\\left[\\frac{1}{N_{b}}\\right] \\approx \\frac{1}{\\mu} - \\frac{1}{\\mu^{2}}\\mathbb{E}[N_{b} - \\mu] + \\frac{1}{\\mu^{3}}\\mathbb{E}[(N_{b} - \\mu)^{2}]\n$$\nBy definition, the expectation of the deviation from the mean is zero, and the expectation of the squared deviation from the mean is the variance:\n-   $\\mathbb{E}[N_{b} - \\mu] = \\mathbb{E}[N_{b}] - \\mu = \\mu - \\mu = 0$\n-   $\\mathbb{E}[(N_{b} - \\mu)^{2}] = \\operatorname{Var}(N_{b}) = \\sigma^{2}$\n\nSubstituting these results into the expression for $\\mathbb{E}[1/N_{b}]$ yields the second-order approximation:\n$$\n\\mathbb{E}\\left[\\frac{1}{N_{b}}\\right] \\approx \\frac{1}{\\mu} - \\frac{0}{\\mu^{2}} + \\frac{\\sigma^{2}}{\\mu^{3}} = \\frac{1}{\\mu} + \\frac{\\sigma^{2}}{\\mu^{3}}\n$$\nNow, we substitute this approximation back into the expression for the relative bias $\\beta$:\n$$\n\\beta \\approx \\mu \\left(\\frac{1}{\\mu} + \\frac{\\sigma^{2}}{\\mu^{3}}\\right) - 1\n$$\nDistributing the factor of $\\mu$:\n$$\n\\beta \\approx \\left(\\mu \\cdot \\frac{1}{\\mu}\\right) + \\left(\\mu \\cdot \\frac{\\sigma^{2}}{\\mu^{3}}\\right) - 1\n$$\n$$\n\\beta \\approx 1 + \\frac{\\sigma^{2}}{\\mu^{2}} - 1\n$$\nSimplifying this expression gives the leading-order relative bias:\n$$\n\\beta \\approx \\frac{\\sigma^{2}}{\\mu^{2}}\n$$\nThis result is the squared coefficient of variation, $c_{v}^{2}$, which is consistent with the problem's premise that this quantity is small, thereby justifying the truncation of the Taylor series. The final answer is expressed in terms of the given parameters $\\mu$ and $\\sigma^{2}$.", "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{\\mu^{2}}}\n$$", "id": "2712475"}, {"introduction": "Modern experimental evolution heavily relies on deep sequencing to track allele frequency changes, but the necessary step of Polymerase Chain Reaction (PCR) amplification can introduce significant distortions. This practice [@problem_id:2712500] tackles the \"jackpotting\" effect, where stochastic amplification in early PCR cycles creates overdispersion in read counts, leading to inflated variance in frequency estimates. You will derive this variance inflation factor and see analytically how Unique Molecular Identifiers (UMIs) can correct for this artifact, restoring the statistical power of your sequencing data.", "problem": "In an experimental evolution study, you sequence a mixed population to estimate the frequency of a focal genetic variant. Deoxyribonucleic acid (DNA) molecules are first amplified by Polymerase Chain Reaction (PCR), and then sequenced deeply. Because of stochastic early-cycle amplification variation (PCR jackpotting), different input template molecules produce highly variable numbers of amplified copies. You consider correcting this with Unique Molecular Identifiers (UMIs), which collapse PCR duplicates to the level of distinct input templates.\n\nAssume the following generative model. There are $N_{0}$ input template molecules, indexed by $i \\in \\{1,\\dots,N_{0}\\}$. Each template is either the focal variant ($X_{i}=1$) or the reference ($X_{i}=0$), where $X_{i} \\sim \\mathrm{Bernoulli}(p)$ independently, with unknown true variant fraction $p \\in (0,1)$. Conditional on $X_{i}$, each template $i$ produces a random number $K_{i}$ of PCR-amplified molecules; jackpotting is modeled by taking the $K_{i}$ to be independent and identically distributed with finite mean $\\mu_{K}=\\mathbb{E}[K_{i}]$ and variance $\\sigma_{K}^{2}=\\mathrm{Var}(K_{i})$, and independent of the $\\{X_{i}\\}$. Let $S=\\sum_{i=1}^{N_{0}} X_{i} K_{i}$ be the total number of amplified variant molecules and $T=\\sum_{i=1}^{N_{0}} K_{i}$ be the total number of amplified molecules.\n\nYou sequence to saturating depth so that sampling noise from sequencing is negligible relative to upstream variation, i.e., the naive read-based estimator of variant frequency is $\\hat{q}=S/T$ in the limit of very large read depth. With UMI-based deduplication under saturating coverage (every input molecule tagged and observed at least once), the corrected estimator is the fraction of distinct input templates carrying the variant, $\\hat{p}_{\\mathrm{UMI}}=N_{0}^{-1}\\sum_{i=1}^{N_{0}} X_{i}$.\n\nUsing only basic probability definitions, independence assumptions, and first-order Taylor (delta method) approximation justified for large $N_{0}$, derive the leading-order variance of $\\hat{q}$ under PCR jackpotting and the corresponding variance of $\\hat{p}_{\\mathrm{UMI}}$ under perfect UMI deduplication. Define the overdispersion reduction factor as the ratio\n$$\n\\mathcal{R} \\equiv \\frac{\\mathrm{Var}(\\hat{q})}{\\mathrm{Var}(\\hat{p}_{\\mathrm{UMI}})} \\, .\n$$\nAssuming $N_{0}$ is large and read depth is saturating as described, what is the closed-form analytic expression for $\\mathcal{R}$ in terms of $\\mu_{K}$ and $\\sigma_{K}^{2}$ only? Provide your final expression for $\\mathcal{R}$; no numerical evaluation is required and no units are needed.", "solution": "The problem statement must first be rigorously validated.\n\nStep 1: Extract Givens.\n- $N_{0}$: number of input template molecules.\n- $i \\in \\{1, \\dots, N_{0}\\}$: index for molecules.\n- $X_{i}$: indicator variable for a focal variant, $X_{i}=1$ if variant, $X_{i}=0$ if reference.\n- $X_{i} \\sim \\mathrm{Bernoulli}(p)$ independently for all $i$.\n- $p \\in (0,1)$: true fraction of the focal variant.\n- $K_{i}$: random number of PCR-amplified molecules from template $i$.\n- $\\{K_{i}\\}$ are independent and identically distributed (i.i.d.).\n- $\\mu_{K} = \\mathbb{E}[K_{i}]$: finite mean of amplification count.\n- $\\sigma_{K}^{2} = \\mathrm{Var}(K_{i})$: finite variance of amplification count.\n- $\\{K_{i}\\}$ and $\\{X_{j}\\}$ are mutually independent for all $i, j$.\n- $S = \\sum_{i=1}^{N_{0}} X_{i} K_{i}$: total number of amplified variant molecules.\n- $T = \\sum_{i=1}^{N_{0}} K_{i}$: total number of amplified molecules.\n- $\\hat{q} = S/T$: naive read-based estimator of variant frequency.\n- $\\hat{p}_{\\mathrm{UMI}} = N_{0}^{-1}\\sum_{i=1}^{N_{0}} X_{i}$: UMI-corrected estimator.\n- Assumption: $N_{0}$ is large.\n- Assumption: Sequencing depth is saturating, so sampling from the amplified pool is not a source of error.\n- Task: Derive $\\mathrm{Var}(\\hat{q})$ using the first-order Taylor (delta method) approximation.\n- Task: Derive $\\mathrm{Var}(\\hat{p}_{\\mathrm{UMI}})$.\n- Task: Compute the ratio $\\mathcal{R} \\equiv \\frac{\\mathrm{Var}(\\hat{q})}{\\mathrm{Var}(\\hat{p}_{\\mathrm{UMI}})}$ in terms of $\\mu_{K}$ and $\\sigma_{K}^{2}$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, describing a standard and well-understood issue in quantitative sequencing (PCR jackpotting) and a common correction method (UMIs). The probabilistic model is a conventional and appropriate formalization of this process. The problem is well-posed, providing all necessary definitions, assumptions (large $N_{0}$, independence), and a clear objective. It is not underspecified, contradictory, or based on subjective claims. The derivation requires standard, non-trivial application of probability theory and approximation methods. The problem is valid.\n\nStep 3: Verdict and Action.\nThe problem is valid. I will proceed with the derivation.\n\nFirst, we derive the variance of the UMI-based estimator, $\\hat{p}_{\\mathrm{UMI}}$. The estimator is the sample mean of $N_{0}$ i.i.d. Bernoulli random variables, $X_{i}$.\nThe variance of a Bernoulli$(p)$ random variable is $p(1-p)$.\n$$\n\\mathrm{Var}(\\hat{p}_{\\mathrm{UMI}}) = \\mathrm{Var}\\left(\\frac{1}{N_{0}}\\sum_{i=1}^{N_{0}} X_{i}\\right)\n$$\nBecause the $X_{i}$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}(\\hat{p}_{\\mathrm{UMI}}) = \\frac{1}{N_{0}^{2}} \\sum_{i=1}^{N_{0}} \\mathrm{Var}(X_{i}) = \\frac{1}{N_{0}^{2}} \\cdot N_{0} \\cdot p(1-p) = \\frac{p(1-p)}{N_{0}}\n$$\n\nNext, we derive the variance of the naive estimator, $\\hat{q} = S/T$. We use the first-order Taylor expansion (delta method) for the ratio of two random variables, which for large $N_{0}$ is a valid approximation. Let $g(S, T) = S/T$. The variance of $\\hat{q}$ is approximated by:\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\left(\\frac{\\partial g}{\\partial S}\\right)^2 \\mathrm{Var}(S) + \\left(\\frac{\\partial g}{\\partial T}\\right)^2 \\mathrm{Var}(T) + 2\\left(\\frac{\\partial g}{\\partial S}\\right)\\left(\\frac{\\partial g}{\\partial T}\\right) \\mathrm{Cov}(S, T)\n$$\nThe partial derivatives are evaluated at the expected values of $S$ and $T$.\n\nWe must first compute the expectations, variances, and covariance of $S$ and $T$.\nThe expectation of $T$ is:\n$$\n\\mathbb{E}[T] = \\mathbb{E}\\left[\\sum_{i=1}^{N_{0}} K_{i}\\right] = \\sum_{i=1}^{N_{0}} \\mathbb{E}[K_{i}] = N_{0}\\mu_{K}\n$$\nThe expectation of $S$ is:\n$$\n\\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{i=1}^{N_{0}} X_{i}K_{i}\\right] = \\sum_{i=1}^{N_{0}} \\mathbb{E}[X_{i}K_{i}]\n$$\nDue to independence of $X_{i}$ and $K_{i}$, $\\mathbb{E}[X_{i}K_{i}] = \\mathbb{E}[X_{i}]\\mathbb{E}[K_{i}] = p\\mu_{K}$.\n$$\n\\mathbb{E}[S] = \\sum_{i=1}^{N_{0}} p\\mu_{K} = N_{0}p\\mu_{K}\n$$\nThe expected value of the ratio is approximately $\\mathbb{E}[\\hat{q}] \\approx \\mathbb{E}[S]/\\mathbb{E}[T] = (N_{0}p\\mu_{K})/(N_{0}\\mu_{K}) = p$.\n\nThe partial derivatives of $g(S,T)=S/T$ are:\n$$\n\\frac{\\partial g}{\\partial S} = \\frac{1}{T} \\quad \\implies \\quad \\left. \\frac{\\partial g}{\\partial S} \\right|_{\\mathbb{E}[S], \\mathbb{E}[T]} = \\frac{1}{N_{0}\\mu_{K}}\n$$\n$$\n\\frac{\\partial g}{\\partial T} = -\\frac{S}{T^{2}} \\quad \\implies \\quad \\left. \\frac{\\partial g}{\\partial T} \\right|_{\\mathbb{E}[S], \\mathbb{E}[T]} = -\\frac{N_{0}p\\mu_{K}}{(N_{0}\\mu_{K})^{2}} = -\\frac{p}{N_{0}\\mu_{K}}\n$$\n\nNow we find the variances and covariance.\nThe variance of $T$: since the $K_{i}$ are i.i.d.,\n$$\n\\mathrm{Var}(T) = \\mathrm{Var}\\left(\\sum_{i=1}^{N_{0}} K_{i}\\right) = \\sum_{i=1}^{N_{0}} \\mathrm{Var}(K_{i}) = N_{0}\\sigma_{K}^{2}\n$$\nThe variance of $S$: let $Y_{i} = X_{i}K_{i}$. Since these terms are i.i.d., $\\mathrm{Var}(S) = \\sum \\mathrm{Var}(Y_{i}) = N_{0}\\mathrm{Var}(Y_{1})$. We use the law of total variance: $\\mathrm{Var}(Y_{i}) = \\mathbb{E}[\\mathrm{Var}(Y_{i}|X_{i})] + \\mathrm{Var}(\\mathbb{E}[Y_{i}|X_{i}])$.\n$$\n\\mathbb{E}[Y_{i}|X_{i}] = \\mathbb{E}[X_{i}K_{i}|X_{i}] = X_{i}\\mathbb{E}[K_{i}] = X_{i}\\mu_{K}\n$$\n$$\n\\mathrm{Var}(Y_{i}|X_{i}) = \\mathrm{Var}(X_{i}K_{i}|X_{i}) = X_{i}^{2}\\mathrm{Var}(K_{i}) = X_{i}\\sigma_{K}^{2} \\quad (\\text{since } X_{i}^{2}=X_{i} \\text{ for Bernoulli})\n$$\nSo, $\\mathbb{E}[\\mathrm{Var}(Y_{i}|X_{i})] = \\mathbb{E}[X_{i}\\sigma_{K}^{2}] = \\sigma_{K}^{2}\\mathbb{E}[X_{i}] = p\\sigma_{K}^{2}$.\nAnd, $\\mathrm{Var}(\\mathbb{E}[Y_{i}|X_{i}]) = \\mathrm{Var}(X_{i}\\mu_{K}) = \\mu_{K}^{2}\\mathrm{Var}(X_{i}) = \\mu_{K}^{2}p(1-p)$.\n$$\n\\mathrm{Var}(Y_{i}) = p\\sigma_{K}^{2} + \\mu_{K}^{2}p(1-p)\n$$\nTherefore, the variance of $S$ is:\n$$\n\\mathrm{Var}(S) = N_{0}\\left(p\\sigma_{K}^{2} + \\mu_{K}^{2}p(1-p)\\right)\n$$\nThe covariance of $S$ and $T$:\n$$\n\\mathrm{Cov}(S, T) = \\mathrm{Cov}\\left(\\sum_{i=1}^{N_{0}} X_{i}K_{i}, \\sum_{j=1}^{N_{0}} K_{j}\\right) = \\sum_{i=1}^{N_{0}}\\sum_{j=1}^{N_{0}} \\mathrm{Cov}(X_{i}K_{i}, K_{j})\n$$\nFor $i \\neq j$, $X_{i}K_{i}$ and $K_{j}$ are independent, so $\\mathrm{Cov}(X_{i}K_{i}, K_{j}) = 0$. We only need to consider terms where $i=j$.\n$$\n\\mathrm{Cov}(S, T) = \\sum_{i=1}^{N_{0}} \\mathrm{Cov}(X_{i}K_{i}, K_{i})\n$$\nWe compute $\\mathrm{Cov}(X_{i}K_{i}, K_{i}) = \\mathbb{E}[X_{i}K_{i}^{2}] - \\mathbb{E}[X_{i}K_{i}]\\mathbb{E}[K_{i}]$.\nBy independence, $\\mathbb{E}[X_{i}K_{i}^{2}] = \\mathbb{E}[X_{i}]\\mathbb{E}[K_{i}^{2}] = p(\\sigma_{K}^{2} + \\mu_{K}^{2})$.\nAnd $\\mathbb{E}[X_{i}K_{i}]\\mathbb{E}[K_{i}] = (p\\mu_{K})\\mu_{K} = p\\mu_{K}^{2}$.\nSo, $\\mathrm{Cov}(X_{i}K_{i}, K_{i}) = p(\\sigma_{K}^{2} + \\mu_{K}^{2}) - p\\mu_{K}^{2} = p\\sigma_{K}^{2}$.\n$$\n\\mathrm{Cov}(S, T) = \\sum_{i=1}^{N_{0}} p\\sigma_{K}^{2} = N_{0}p\\sigma_{K}^{2}\n$$\n\nNow substitute all components into the delta method formula for $\\mathrm{Var}(\\hat{q})$:\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\left(\\frac{1}{N_{0}\\mu_{K}}\\right)^{2}\\mathrm{Var}(S) + \\left(-\\frac{p}{N_{0}\\mu_{K}}\\right)^{2}\\mathrm{Var}(T) + 2\\left(\\frac{1}{N_{0}\\mu_{K}}\\right)\\left(-\\frac{p}{N_{0}\\mu_{K}}\\right)\\mathrm{Cov}(S, T)\n$$\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\frac{1}{(N_{0}\\mu_{K})^{2}} \\left[ \\mathrm{Var}(S) + p^{2}\\mathrm{Var}(T) - 2p\\mathrm{Cov}(S, T) \\right]\n$$\nSubstitute the expressions for the variances and covariance:\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\frac{1}{N_{0}^{2}\\mu_{K}^{2}} \\left[ N_{0}(p\\sigma_{K}^{2} + \\mu_{K}^{2}p(1-p)) + p^{2}(N_{0}\\sigma_{K}^{2}) - 2p(N_{0}p\\sigma_{K}^{2}) \\right]\n$$\nFactor out $N_{0}$ from brackets:\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\frac{N_{0}}{N_{0}^{2}\\mu_{K}^{2}} \\left[ (p\\sigma_{K}^{2} + p(1-p)\\mu_{K}^{2}) + p^{2}\\sigma_{K}^{2} - 2p^{2}\\sigma_{K}^{2} \\right]\n$$\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\frac{1}{N_{0}\\mu_{K}^{2}} \\left[ p\\sigma_{K}^{2} - p^{2}\\sigma_{K}^{2} + p(1-p)\\mu_{K}^{2} \\right]\n$$\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\frac{1}{N_{0}\\mu_{K}^{2}} \\left[ p(1-p)\\sigma_{K}^{2} + p(1-p)\\mu_{K}^{2} \\right]\n$$\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\frac{p(1-p)}{N_{0}\\mu_{K}^{2}} \\left( \\sigma_{K}^{2} + \\mu_{K}^{2} \\right)\n$$\n$$\n\\mathrm{Var}(\\hat{q}) \\approx \\frac{p(1-p)}{N_{0}} \\left( 1 + \\frac{\\sigma_{K}^{2}}{\\mu_{K}^{2}} \\right)\n$$\n\nFinally, we compute the overdispersion reduction factor $\\mathcal{R}$:\n$$\n\\mathcal{R} = \\frac{\\mathrm{Var}(\\hat{q})}{\\mathrm{Var}(\\hat{p}_{\\mathrm{UMI}})} \\approx \\frac{\\frac{p(1-p)}{N_{0}} \\left( 1 + \\frac{\\sigma_{K}^{2}}{\\mu_{K}^{2}} \\right)}{\\frac{p(1-p)}{N_{0}}}\n$$\n$$\n\\mathcal{R} \\approx 1 + \\frac{\\sigma_{K}^{2}}{\\mu_{K}^{2}}\n$$\nThis is the required closed-form expression in terms of $\\mu_{K}$ and $\\sigma_{K}^{2}$. It represents the inflation of variance due to the stochasticity of PCR amplification, quantified by the squared coefficient of variation of the number of amplified copies.", "answer": "$$\\boxed{1 + \\frac{\\sigma_{K}^{2}}{\\mu_{K}^{2}}}$$", "id": "2712500"}]}