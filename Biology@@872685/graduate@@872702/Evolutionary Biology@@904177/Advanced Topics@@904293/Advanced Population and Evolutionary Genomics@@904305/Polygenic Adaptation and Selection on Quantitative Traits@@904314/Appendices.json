{"hands_on_practices": [{"introduction": "In nature, traits rarely evolve in isolation. Due to pleiotropy and genetic linkage, traits are often genetically correlated, and selection acting on one can drag another along with it. This practice provides a concrete application of the multivariate breeder's equation, $\\Delta\\overline{\\mathbf{z}} = \\mathbf{G}\\mathbf{P}^{-1}\\mathbf{S}$, to calculate this \"correlated response\" [@problem_id:2744363]. By working through this calculation, you will solidify your understanding of how the additive genetic ($\\mathbf{G}$) and phenotypic ($\\mathbf{P}$) covariance matrices dictate the direction and magnitude of multivariate evolution.", "problem": "A large, randomly mating population expresses $2$ quantitative traits, $z_{1}$ and $z_{2}$, influenced by many loci with additive effects, such that the infinitesimal model of quantitative genetics is appropriate and the additive genetic covariance matrix is stable over one generation. Assume no transmission bias, no dominance or epistasis affecting the mean, and that environmental effects are independent of genotype.\n\nAt the start of generation $t$, the phenotypic variance-covariance matrix is\n$$\n\\mathbf{P} =\n\\begin{pmatrix}\n1.2  0.4 \\\\\n0.4  2.0\n\\end{pmatrix},\n$$\nand the additive genetic variance-covariance matrix is\n$$\n\\mathbf{G} =\n\\begin{pmatrix}\n0.6  0.3 \\\\\n0.3  1.0\n\\end{pmatrix}.\n$$\n\nIn generation $t$, artificial selection is imposed primarily on trait $z_{1}$. The realized phenotypic selection differential vector (the difference in mean phenotype between selected parents and the population before selection) is measured as\n$$\n\\mathbf{S} =\n\\begin{pmatrix}\n0.3 \\\\\n0.0\n\\end{pmatrix}.\n$$\n\nStarting from first principles in quantitative genetics (definitions of the selection differential, selection gradient, and additive genetic covariance), derive the expected change in the mean trait vector from generation $t$ to $t+1$ under standard assumptions, and compute the correlated response in trait $z_{2}$ after one generation (i.e., the second component of the response vector). Express your final answer as a pure number (trait units), and round your answer to four significant figures.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- A large, randomly mating population.\n- $2$ quantitative traits, $z_{1}$ and $z_{2}$, influenced by many loci with additive effects (infinitesimal model).\n- The additive genetic covariance matrix $\\mathbf{G}$ is stable over one generation.\n- No transmission bias, no dominance or epistasis affecting the mean.\n- Environmental effects are independent of genotype.\n- Phenotypic variance-covariance matrix at generation $t$:\n$$\n\\mathbf{P} =\n\\begin{pmatrix}\n1.2  0.4 \\\\\n0.4  2.0\n\\end{pmatrix}\n$$\n- Additive genetic variance-covariance matrix at generation $t$:\n$$\n\\mathbf{G} =\n\\begin{pmatrix}\n0.6  0.3 \\\\\n0.3  1.0\n\\end{pmatrix}\n$$\n- Realized phenotypic selection differential vector in generation $t$:\n$$\n\\mathbf{S} =\n\\begin{pmatrix}\n0.3 \\\\\n0.0\n\\end{pmatrix}\n$$\n- Task: Derive the expected change in the mean trait vector from generation $t$ to $t+1$ and compute the correlated response in trait $z_{2}$.\n\nValidation Verdict:\nThe problem is valid. It is a standard exercise in multivariate quantitative genetics, applying the Lande-Arnold framework. The problem is scientifically grounded, well-posed, and objective. All provided matrices ($\\mathbf{P}$ and $\\mathbf{G}$) are symmetric. Their determinants are $\\det(\\mathbf{P}) = (1.2)(2.0) - (0.4)^{2} = 2.24  0$ and $\\det(\\mathbf{G}) = (0.6)(1.0) - (0.3)^{2} = 0.51  0$, and their traces are positive, confirming they are positive definite and thus valid variance-covariance matrices. The environmental covariance matrix, $\\mathbf{E} = \\mathbf{P} - \\mathbf{G}$, is also positive definite, ensuring scientific plausibility. All necessary information is provided for a unique solution.\n\nSolution:\nThe expected evolutionary change in the vector of mean phenotypes, $\\Delta\\bar{\\mathbf{z}}$, from one generation to the next is given by the multivariate breeder's equation, also known as Lande's equation. This equation relates the response to selection to the causal forces of selection and the underlying genetic architecture of the traits.\n\nThe derivation begins from the definition of the selection gradient, $\\boldsymbol{\\beta}$. The selection gradient measures the direct force of directional selection acting on each trait, correcting for phenotypic correlations among traits. It is defined as the gradient of relative fitness with respect to the trait values, and under the assumption of a multivariate normal phenotype distribution, it can be calculated from the phenotypic variance-covariance matrix $\\mathbf{P}$ and the selection differential vector $\\mathbf{S}$ as:\n$$\n\\boldsymbol{\\beta} = \\mathbf{P}^{-1}\\mathbf{S}\n$$\nHere, $\\mathbf{S}$ represents the change in the mean phenotype within a generation due to selection (i.e., the difference between the mean of selected parents and the mean of the entire population before selection).\n\nThe response to selection, $\\Delta\\bar{\\mathbf{z}}$, is the change in the mean phenotype across one generation. This response depends on the heritable variation available for selection to act upon. In the multivariate context, this is captured by the additive genetic variance-covariance matrix, $\\mathbf{G}$. The response is predicted by:\n$$\n\\Delta\\bar{\\mathbf{z}} = \\mathbf{G}\\boldsymbol{\\beta}\n$$\nSubstituting the expression for $\\boldsymbol{\\beta}$ yields the full equation for the response to selection:\n$$\n\\Delta\\bar{\\mathbf{z}} = \\mathbf{G}\\mathbf{P}^{-1}\\mathbf{S}\n$$\nThe vector $\\Delta\\bar{\\mathbf{z}}$ has components $[\\Delta\\bar{z}_{1}, \\Delta\\bar{z}_{2}]^T$. The first component, $\\Delta\\bar{z}_{1}$, is the direct response to selection on trait $z_{1}$. The second component, $\\Delta\\bar{z}_{2}$, is the correlated response in trait $z_{2}$, which occurs because of genetic covariance between the two traits, even though there is no direct selection differential on $z_{2}$ (i.e., $S_{2}=0$).\n\nThe solution requires calculating this vector. First, we compute the inverse of the matrix $\\mathbf{P}$. For a $2 \\times 2$ matrix $\\mathbf{A} = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is $\\mathbf{A}^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nThe determinant of $\\mathbf{P}$ is:\n$$\n\\det(\\mathbf{P}) = (1.2)(2.0) - (0.4)(0.4) = 2.4 - 0.16 = 2.24\n$$\nThus, the inverse is:\n$$\n\\mathbf{P}^{-1} = \\frac{1}{2.24}\n\\begin{pmatrix}\n2.0  -0.4 \\\\\n-0.4  1.2\n\\end{pmatrix}\n$$\nNow, we can compute the full expression for $\\Delta\\bar{\\mathbf{z}}$:\n$$\n\\Delta\\bar{\\mathbf{z}} =\n\\begin{pmatrix}\n0.6  0.3 \\\\\n0.3  1.0\n\\end{pmatrix}\n\\left( \\frac{1}{2.24}\n\\begin{pmatrix}\n2.0  -0.4 \\\\\n-0.4  1.2\n\\end{pmatrix} \\right)\n\\begin{pmatrix}\n0.3 \\\\\n0.0\n\\end{pmatrix}\n$$\nWe can first compute the product $\\mathbf{P}^{-1}\\mathbf{S}$:\n$$\n\\mathbf{P}^{-1}\\mathbf{S} = \\frac{1}{2.24}\n\\begin{pmatrix}\n2.0  -0.4 \\\\\n-0.4  1.2\n\\end{pmatrix}\n\\begin{pmatrix}\n0.3 \\\\\n0.0\n\\end{pmatrix}\n= \\frac{1}{2.24}\n\\begin{pmatrix}\n(2.0)(0.3) + (-0.4)(0.0) \\\\\n(-0.4)(0.3) + (1.2)(0.0)\n\\end{pmatrix}\n= \\frac{1}{2.24}\n\\begin{pmatrix}\n0.6 \\\\\n-0.12\n\\end{pmatrix}\n$$\nThis vector is the selection gradient $\\boldsymbol{\\beta}$. Now, we multiply by $\\mathbf{G}$:\n$$\n\\Delta\\bar{\\mathbf{z}} = \\mathbf{G}\\boldsymbol{\\beta} =\n\\begin{pmatrix}\n0.6  0.3 \\\\\n0.3  1.0\n\\end{pmatrix}\n\\frac{1}{2.24}\n\\begin{pmatrix}\n0.6 \\\\\n-0.12\n\\end{pmatrix}\n= \\frac{1}{2.24}\n\\begin{pmatrix}\n(0.6)(0.6) + (0.3)(-0.12) \\\\\n(0.3)(0.6) + (1.0)(-0.12)\n\\end{pmatrix}\n$$\n$$\n\\Delta\\bar{\\mathbf{z}} = \\frac{1}{2.24}\n\\begin{pmatrix}\n0.36 - 0.036 \\\\\n0.18 - 0.12\n\\end{pmatrix}\n= \\frac{1}{2.24}\n\\begin{pmatrix}\n0.324 \\\\\n0.06\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.324 / 2.24 \\\\\n0.06 / 2.24\n\\end{pmatrix}\n$$\nThe problem asks for the correlated response in trait $z_{2}$, which is the second component of the vector $\\Delta\\bar{\\mathbf{z}}$:\n$$\n\\Delta\\bar{z}_{2} = \\frac{0.06}{2.24}\n$$\nTo obtain a numerical value, we perform the division:\n$$\n\\Delta\\bar{z}_{2} = \\frac{6}{224} = \\frac{3}{112} \\approx 0.0267857...\n$$\nRounding to four significant figures gives $0.02679$.\nThe positive sign indicates that despite no direct selection on trait $z_{2}$, its mean is expected to increase due to the positive selection on trait $z_{1}$ and the positive genetic covariance between the traits ($G_{12}=0.3$).", "answer": "$$\\boxed{0.02679}$$", "id": "2744363"}, {"introduction": "Building on the single-generation prediction, this practice moves into the realm of long-term evolutionary dynamics. Here, you will model a population's adaptive \"chase\" of an environmental optimum that is constantly changing over time [@problem_id:2744372]. By implementing the classic Lande-Arnold framework, you will simulate the evolutionary trajectory of mean traits, allowing you to directly observe phenomena like adaptive lag and understand how the interplay between the fitness landscape ($\\boldsymbol{\\Omega}$) and genetic architecture ($\\mathbf{G}$) governs the pace and limits of adaptation.", "problem": "You are to write a complete program that computes the deterministic evolutionary response of the mean of a quantitative polygenic trait vector under stabilizing selection with a potentially moving optimum, using the principle that directional selection arises from the gradient of log fitness with respect to the mean trait and the standard quantitative-genetic response to selection under linkage equilibrium.\n\nStarting base:\n- Use the following foundational elements only:\n  1. The definition of Malthusian fitness as the natural logarithm of absolute fitness, i.e., if absolute fitness is $W(\\mathbf{z})$, then Malthusian fitness is $m(\\mathbf{z}) = \\ln W(\\mathbf{z})$.\n  2. The definition of the selection gradient as the gradient of Malthusian fitness with respect to the trait vector, i.e., $\\boldsymbol{\\beta}(\\mathbf{z}) = \\nabla_{\\mathbf{z}} m(\\mathbf{z})$.\n  3. The statement that the change in trait mean over one generation under standard quantitative-genetic assumptions (weak selection, additivity, linkage equilibrium (LE), and approximately constant additive genetic variance-covariance) is proportional to the selection gradient via the additive genetic variance-covariance matrix. You must derive the explicit proportionality from these foundations.\n  4. A widely used and empirically grounded form of stabilizing selection on a multivariate quantitative trait with trait vector $\\mathbf{z} \\in \\mathbb{R}^n$ given by an absolute fitness function of the form\n     $$W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)\\right),$$\n     where $\\boldsymbol{\\theta}_t \\in \\mathbb{R}^n$ is the optimum at generation $t$ and $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive-definite with entries specifying the strength and correlational structure of stabilizing selection.\n- You must use the above elements as the fundamental base to derive a deterministic, discrete-time update for the mean trait vector $\\overline{\\mathbf{z}}_t$ over $T$ generations, given a constant additive genetic variance-covariance matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times n}$, a linear optimum path $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$ for a fixed velocity $\\mathbf{v} \\in \\mathbb{R}^n$, and initial mean $\\overline{\\mathbf{z}}_0$.\n\nTask:\n- Derive the explicit deterministic update for $\\overline{\\mathbf{z}}_{t+1}$ in terms of $\\overline{\\mathbf{z}}_t$, $\\boldsymbol{\\theta}_t$, $\\boldsymbol{\\Omega}$, and $\\mathbf{G}$ by combining the above base elements. Implement that update and iterate it for $T$ generations to obtain $\\overline{\\mathbf{z}}_T$.\n- You may assume $\\boldsymbol{\\Omega}$ is symmetric positive-definite for all test cases, so $\\boldsymbol{\\Omega}^{-1}$ exists. You may not assume $\\mathbf{G}$ is invertible.\n\nInput and test suite specification:\n- There is no external input; hard-code the following test suite in your program. For each test case $i$, you are given trait dimension $n$, initial mean $\\overline{\\mathbf{z}}_0^{(i)}$, additive genetic variance-covariance $\\mathbf{G}^{(i)}$, stabilizing selection matrix $\\boldsymbol{\\Omega}^{(i)}$, initial optimum $\\boldsymbol{\\theta}_0^{(i)}$, optimum velocity $\\mathbf{v}^{(i)}$, and number of generations $T^{(i)}$. For each test case, compute $\\overline{\\mathbf{z}}_{T^{(i)}}^{(i)}$.\n\nTest cases:\n1. One-dimensional, stationary optimum:\n   - $n = 1$\n   - $\\overline{\\mathbf{z}}_0^{(1)} = (0.0)$\n   - $\\mathbf{G}^{(1)} = \\begin{bmatrix} 0.2 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(1)} = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(1)} = (1.5)$\n   - $\\mathbf{v}^{(1)} = (0.0)$\n   - $T^{(1)} = 5$\n\n2. Two-dimensional, correlated $\\mathbf{G}$, stationary optimum:\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(2)} = (0.0, 0.0)$\n   - $\\mathbf{G}^{(2)} = \\begin{bmatrix} 0.1  0.05 \\\\ 0.05  0.2 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(2)} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.5 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(2)} = (1.0, -0.5)$\n   - $\\mathbf{v}^{(2)} = (0.0, 0.0)$\n   - $T^{(2)} = 10$\n\n3. One-dimensional, zero additive variance (no response):\n   - $n = 1$\n   - $\\overline{\\mathbf{z}}_0^{(3)} = (0.0)$\n   - $\\mathbf{G}^{(3)} = \\begin{bmatrix} 0.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(3)} = \\begin{bmatrix} 0.5 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(3)} = (1.0)$\n   - $\\mathbf{v}^{(3)} = (0.0)$\n   - $T^{(3)} = 7$\n\n4. Two-dimensional, weak stabilizing selection:\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(4)} = (0.2, -0.2)$\n   - $\\mathbf{G}^{(4)} = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(4)} = \\begin{bmatrix} 100.0  0.0 \\\\ 0.0  100.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(4)} = (1.0, 1.0)$\n   - $\\mathbf{v}^{(4)} = (0.0, 0.0)$\n   - $T^{(4)} = 10$\n\n5. Two-dimensional, moving optimum with correlational stabilizing selection:\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(5)} = (0.0, 0.0)$\n   - $\\mathbf{G}^{(5)} = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(5)} = \\begin{bmatrix} 1.0  0.2 \\\\ 0.2  1.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(5)} = (0.0, 0.0)$\n   - $\\mathbf{v}^{(5)} = (0.1, -0.05)$\n   - $T^{(5)} = 20$\n\nOutput requirements:\n- For each test case $i$, compute $\\overline{\\mathbf{z}}_{T^{(i)}}^{(i)}$.\n- Round every component of every resulting vector to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of vectors, each vector enclosed in square brackets, and the whole collection enclosed in square brackets. For example, the formatting should be like $[\\,[a_{1,1},a_{1,2}],\\,[a_{2,1},a_{2,2}]\\,]$ but with the actual numbers instead of symbols, with no spaces.\n- There are no physical units involved.\n\nScientific realism constraints:\n- Assume weak selection and additivity with approximately constant $\\mathbf{G}$ over the simulated time, and linkage equilibrium (LE). Ensure that $\\boldsymbol{\\Omega}$ is symmetric positive-definite in all computations so that $\\boldsymbol{\\Omega}^{-1}$ exists.\n\nYour program must be self-contained and must not read any external input.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in evolutionary quantitative genetics. It asks for the derivation and implementation of a discrete-time model for the evolution of a multivariate polygenic trait under stabilizing selection with a linearly moving optimum. The problem statement provides all necessary components and adheres to established principles of the field. It is therefore deemed valid.\n\nThe objective is to derive the deterministic, discrete-time update equation for the mean trait vector, $\\overline{\\mathbf{z}}_t$, and to iterate this equation for a specified number of generations, $T$, to find the final mean trait vector, $\\overline{\\mathbf{z}}_T$. The derivation will proceed from the foundational elements provided.\n\nFirst, we are given the absolute fitness function for an individual with trait vector $\\mathbf{z} \\in \\mathbb{R}^n$ at generation $t$:\n$$W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)\\right)$$\nHere, $\\boldsymbol{\\theta}_t$ is the optimal trait vector at generation $t$, and $\\boldsymbol{\\Omega}$ is a symmetric positive-definite matrix that describes the strength of stabilizing selection. A larger $\\boldsymbol{\\Omega}$ corresponds to weaker selection.\n\nThe Malthusian fitness, $m(\\mathbf{z})$, is the natural logarithm of the absolute fitness, $W(\\mathbf{z})$.\n$$m(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\ln W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = -\\frac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)$$\nThis function is a quadratic surface, which simplifies subsequent analysis.\n\nNext, we define the selection gradient, which measures the directional selective force on a trait. The gradient of Malthusian fitness with respect to the trait vector $\\mathbf{z}$ is:\n$$\\boldsymbol{\\beta}(\\mathbf{z}) = \\nabla_{\\mathbf{z}} m(\\mathbf{z}; \\boldsymbol{\\theta}_t)$$\nUsing the standard rule for differentiating a quadratic form, $\\nabla_{\\mathbf{x}} (\\mathbf{x}-\\mathbf{c})^{\\mathsf{T}}\\mathbf{A}(\\mathbf{x}-\\mathbf{c}) = 2\\mathbf{A}(\\mathbf{x}-\\mathbf{c})$ for a symmetric matrix $\\mathbf{A}$, and noting that $\\boldsymbol{\\Omega}^{-1}$ is symmetric as $\\boldsymbol{\\Omega}$ is symmetric, we obtain:\n$$\\boldsymbol{\\beta}(\\mathbf{z}) = -\\frac{1}{2} \\cdot 2\\boldsymbol{\\Omega}^{-1}(\\mathbf{z}-\\boldsymbol{\\theta}_t) = -\\boldsymbol{\\Omega}^{-1}(\\mathbf{z}-\\boldsymbol{\\theta}_t)$$\n\nThe evolutionary response of the population mean trait, $\\Delta \\overline{\\mathbf{z}}_t = \\overline{\\mathbf{z}}_{t+1} - \\overline{\\mathbf{z}}_t$, is governed by the multivariate breeder's equation. This fundamental result of quantitative genetics states that the change in the mean is the product of the additive genetic variance-covariance matrix, $\\mathbf{G}$, and the selection gradient acting on the population mean, $\\boldsymbol{\\beta}_t$:\n$$\\Delta \\overline{\\mathbf{z}}_t = \\mathbf{G} \\boldsymbol{\\beta}_t$$\nThe selection gradient on the mean, $\\boldsymbol{\\beta}_t$, is formally the gradient of the mean log fitness with respect to the mean trait, $\\boldsymbol{\\beta}_t = \\nabla_{\\overline{\\mathbf{z}}} \\overline{m(\\mathbf{z})}$. Under the standard assumption of a symmetric (e.g., multivariate normal) distribution of phenotypes around the mean $\\overline{\\mathbf{z}}_t$, and for the quadratic fitness surface given, this population-level gradient is precisely the individual-level gradient evaluated at the population mean trait value:\n$$\\boldsymbol{\\beta}_t = \\boldsymbol{\\beta}(\\overline{\\mathbf{z}}_t) = -\\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\nThis approximation is central to the Lande-Arnold framework and is implicitly required by the problem's structure, which omits the full phenotypic covariance matrix.\n\nBy substituting this expression for $\\boldsymbol{\\beta}_t$ into the breeder's equation, we find the change in the mean trait vector over one generation:\n$$\\Delta \\overline{\\mathbf{z}}_t = \\mathbf{G} \\left( -\\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t) \\right) = -\\mathbf{G} \\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\n\nFrom this, we derive the explicit, deterministic update rule for the mean trait vector from generation $t$ to $t+1$:\n$$\\overline{\\mathbf{z}}_{t+1} = \\overline{\\mathbf{z}}_t + \\Delta \\overline{\\mathbf{z}}_t = \\overline{\\mathbf{z}}_t - \\mathbf{G} \\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\n\nThe problem specifies a linearly changing optimum:\n$$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$$\nwhere $\\boldsymbol{\\theta}_0$ is the initial optimum, $\\mathbf{v}$ is the constant velocity of the optimum's movement, and $t$ is the generation number, starting from $t=0$.\n\nThe complete iterative algorithm is as follows:\n1. Initialize the mean trait vector at generation $t=0$: $\\overline{\\mathbf{z}} \\leftarrow \\overline{\\mathbf{z}}_0$.\n2. For each generation $t$ from $0$ to $T-1$:\n   a. Calculate the current optimum: $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$.\n   b. Calculate the difference vector between the mean and the optimum: $\\mathbf{d}_t = \\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t$.\n   c. Calculate the evolutionary response: $\\Delta \\overline{\\mathbf{z}}_t = - \\mathbf{G} \\boldsymbol{\\Omega}^{-1} \\mathbf{d}_t$.\n   d. Update the mean trait vector: $\\overline{\\mathbf{z}}_{t+1} = \\overline{\\mathbf{z}}_t + \\Delta \\overline{\\mathbf{z}}_t$.\n3. The final result after $T$ generations is the vector $\\overline{\\mathbf{z}}_T$.\n\nThis procedure will be implemented for each test case provided. The implementation involves basic matrix-vector operations: matrix inversion for $\\boldsymbol{\\Omega}$, and matrix-vector multiplication. The problem guarantees that $\\boldsymbol{\\Omega}$ is invertible. The additive genetic matrix $\\mathbf{G}$ may be singular, which is handled correctly by the derived equation, as no inversion of $\\mathbf{G}$ is required. For example, if $\\mathbf{G}$ is the zero matrix, $\\Delta \\overline{\\mathbf{z}}_t$ is zero, and the population mean does not evolve, as expected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the deterministic evolutionary response of a quantitative polygenic\n    trait vector under stabilizing selection with a potentially moving optimum.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: 1D, stationary optimum\n        {\n            \"n\": 1,\n            \"z0\": np.array([0.0]),\n            \"G\": np.array([[0.2]]),\n            \"Omega\": np.array([[1.0]]),\n            \"theta0\": np.array([1.5]),\n            \"v\": np.array([0.0]),\n            \"T\": 5\n        },\n        # Case 2: 2D, correlated G, stationary optimum\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.0, 0.0]),\n            \"G\": np.array([[0.1, 0.05], [0.05, 0.2]]),\n            \"Omega\": np.array([[1.0, 0.0], [0.0, 1.5]]),\n            \"theta0\": np.array([1.0, -0.5]),\n            \"v\": np.array([0.0, 0.0]),\n            \"T\": 10\n        },\n        # Case 3: 1D, zero additive variance\n        {\n            \"n\": 1,\n            \"z0\": np.array([0.0]),\n            \"G\": np.array([[0.0]]),\n            \"Omega\": np.array([[0.5]]),\n            \"theta0\": np.array([1.0]),\n            \"v\": np.array([0.0]),\n            \"T\": 7\n        },\n        # Case 4: 2D, weak stabilizing selection\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.2, -0.2]),\n            \"G\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"Omega\": np.array([[100.0, 0.0], [0.0, 100.0]]),\n            \"theta0\": np.array([1.0, 1.0]),\n            \"v\": np.array([0.0, 0.0]),\n            \"T\": 10\n        },\n        # Case 5: 2D, moving optimum, correlational selection\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.0, 0.0]),\n            \"G\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"Omega\": np.array([[1.0, 0.2], [0.2, 1.0]]),\n            \"theta0\": np.array([0.0, 0.0]),\n            \"v\": np.array([0.1, -0.05]),\n            \"T\": 20\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract parameters for the current test case\n        z_mean = case[\"z0\"]\n        G = case[\"G\"]\n        Omega = case[\"Omega\"]\n        theta0 = case[\"theta0\"]\n        v = case[\"v\"]\n        T = case[\"T\"]\n        \n        # Pre-compute the inverse of Omega and the product G * Omega_inv\n        # The problem statement guarantees Omega is invertible.\n        Omega_inv = np.linalg.inv(Omega)\n        G_Omega_inv = G @ Omega_inv\n\n        # Iterate the discrete-time update equation for T generations\n        for t in range(T):\n            # Calculate the optimum at the current generation t\n            theta_t = theta0 + t * v\n            \n            # Calculate the evolutionary response delta_z\n            # delta_z = -G * Omega_inv * (z_mean - theta_t)\n            delta_z = -G_Omega_inv @ (z_mean - theta_t)\n            \n            # Update the mean trait vector\n            z_mean = z_mean + delta_z\n            \n        # Round the final vector components to 6 decimal places\n        z_T = np.round(z_mean, 6)\n        \n        # Format the result vector as a string '[c1,c2,...]'\n        # Using .__str__() to avoid trailing zeros and ensure correct format for 0.0\n        result_str = f\"[{','.join(f'{x:.6f}'.rstrip('0').rstrip('.') if x != 0 else '0.0' for x in z_T)}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2744372"}, {"introduction": "Our previous explorations of adaptation operated under the convenient assumption that the additive genetic variance driving evolution remains constant. This final practice confronts a more complex reality by modeling the Bulmer effect, a phenomenon where directional selection itself predictably alters genetic variance [@problem_id:2744369]. By deriving and iterating the recursion for additive variance, you will see how selection creates negative linkage disequilibrium, thus consuming the very \"fuel\" for future adaptation and revealing a critical feedback loop that shapes long-term evolutionary potential.", "problem": "You are to formalize and compute the reduction of additive genetic variance due to selection-induced linkage disequilibrium (linkage disequilibrium (LD))—the Bulmer effect—under the infinitesimal model for a quantitative trait. Work in discrete generations with the following assumptions, which you must use as the fundamental base for the derivation and algorithm:\n\n- The phenotype $P$ is the sum $P = G + E$ of an additive breeding value $G$ and an independent environmental deviation $E$, with $\\mathrm{Var}(G) = V_A$ and $\\mathrm{Var}(E) = V_E$. Assume the population mean of $P$ is initially $0$ by centering.\n- The population is infinitely large, mating is random, loci are unlinked (free recombination), there is no dominance or epistasis, and there is no mutation. The infinitesimal model holds: $G$ is approximately normally distributed, and allele-frequency changes per locus are negligible so that the genic variance $V_g$ (the additive variance that would be present if LD were $0$) can be treated as constant over generations.\n- Selection is truncation selection on $P$, retaining the top $\\pi$ fraction each generation. For a normally distributed trait, conditional moments of a truncated normal random variable are well tested: if $Z \\sim \\mathcal{N}(0,1)$ and we condition on $Z  z_\\pi$ with $\\pi = 1 - \\Phi(z_\\pi)$, then $\\mathbb{E}[Z \\mid Z  z_\\pi] = \\lambda$ and $\\mathrm{Var}(Z \\mid Z  z_\\pi) = 1 + z_\\pi \\lambda - \\lambda^2$, where $\\lambda = \\phi(z_\\pi)/\\pi$, with $\\phi$ and $\\Phi$ the standard normal probability density function and cumulative distribution function, respectively.\n\nYour goals:\n\n- Derive, from first principles and the laws of total expectation and variance, an expression for the additive variance among selected parents, $V_A^{(S)}$, as a function of $V_A$, $V_E$, and $\\pi$. Your derivation must start from the bivariate normal relation between $G$ and $P$ and must not assume any target formulas beyond the stated statistical facts about truncation of a normal distribution.\n- Using random mating and free recombination in the unlinked limit, express the additive variance in the offspring generation, $V_A^{\\text{next}}$, in terms of the genic variance $V_g$ and the selected-parent additive variance $V_A^{(S)}$. Justify, from the decomposition of additive variance into genic variance plus contributions from LD and the effect of recombination on LD, why $V_A^{\\text{next}}$ lies strictly between $V_g$ and $V_A^{(S)}$ when selection induces nonzero LD.\n- Design and implement an algorithm that, given $V_A(0)$, $V_E$, $\\pi$, and an integer number of generations $T$, iterates the variance recursion to compute the ratio $\\rho(T) = V_A(T)/V_g$.\n\nScientific realism constraints:\n\n- Treat $V_g$ as constant and equal to $V_A(0)$ under the infinitesimal model.\n- Use the exact truncated normal moments stated above to compute the selected-phenotype variance each generation.\n- Ensure that your implementation handles the boundary case $\\pi = 1$ (no selection). In this case, the correct limit is $\\rho(T) = 1$ for all $T$.\n\nInput is not read from the user. Instead, your program must run the following test suite hard-coded within the program and output the results as specified:\n\nTest suite (each test case is a tuple $(V_A(0), V_E, \\pi, T)$):\n\n- Case A (moderate selection, intermediate heritability): $(1.0, 1.0, 0.1, 5)$.\n- Case B (no environmental variance, strong heritability): $(1.0, 0.0, 0.2, 10)$.\n- Case C (no selection boundary): $(0.5, 1.5, 1.0, 50)$.\n- Case D (very strong selection, low heritability, near-equilibrium): $(1.0, 3.0, 0.01, 100)$.\n\nAll real numbers must be handled in dimensionless form; no physical units are involved. Your program should compute, for each case, the single floating-point value $\\rho(T) = V_A(T)/V_g$, and round it to $6$ decimal places. Angles are not used. Percentages must be represented as decimal fractions; the parameter $\\pi$ is already provided in this form.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example, $[\\rho_A,\\rho_B,\\rho_C,\\rho_D]$, with each entry rounded to $6$ decimal places.", "solution": "The problem statement is a well-posed and scientifically grounded formulation of the Bulmer effect under the infinitesimal model of quantitative genetics. It provides a complete set of assumptions and parameters required for a unique solution. The problem is valid, and a solution is derived and implemented as follows.\n\nThe core of the problem lies in iterating a recursion for the additive genetic variance $V_A$ over discrete generations. This requires deriving the change in $V_A$ within one generation, which involves two steps: the effect of selection on the parental generation, and the effect of random mating and recombination on the offspring generation.\n\nFirst, we derive the additive genetic variance among selected parents, $V_A^{(S)}$.\nThe phenotype $P$ is the sum of the breeding value $G$ and an independent environmental deviation $E$, so $P = G + E$. We are given that $G \\sim \\mathcal{N}(0, V_A)$ and $E \\sim \\mathcal{N}(0, V_E)$. Their independence implies that the joint distribution of $(G, P)$ is a bivariate normal distribution with mean vector $(0, 0)$ and covariance matrix:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathrm{Var}(G)  \\mathrm{Cov}(G, P) \\\\ \\mathrm{Cov(G, P)}  \\mathrm{Var}(P) \\end{pmatrix} = \\begin{pmatrix} V_A  V_A \\\\ V_A  V_A + V_E \\end{pmatrix}\n$$\nwhere $\\mathrm{Cov}(G, P) = \\mathrm{Cov}(G, G+E) = \\mathrm{Var}(G) + \\mathrm{Cov}(G, E) = V_A$. The total phenotypic variance is $V_P = V_A + V_E$.\n\nThe conditional distribution of $G$ given a specific phenotype value $P=p$ is normal. Standard results for a bivariate normal distribution give the conditional mean and variance:\n$$\n\\mathbb{E}[G \\mid P=p] = \\frac{\\mathrm{Cov}(G, P)}{\\mathrm{Var}(P)} p = \\frac{V_A}{V_P} p = h^2 p\n$$\n$$\n\\mathrm{Var}(G \\mid P=p) = \\mathrm{Var}(G) - \\frac{\\mathrm{Cov}(G, P)^2}{\\mathrm{Var}(P)} = V_A - \\frac{V_A^2}{V_P} = V_A \\left(1 - \\frac{V_A}{V_P}\\right) = V_A (1 - h^2)\n$$\nwhere $h^2 = V_A / V_P$ is the narrow-sense heritability. Note that this conditional variance is independent of the value of $p$.\n\nWe seek the variance of $G$ among selected individuals, which are those with phenotype $P$ exceeding a truncation threshold $P_t$, i.e., $V_A^{(S)} = \\mathrm{Var}(G \\mid P  P_t)$. We apply the law of total variance:\n$$\nV_A^{(S)} = \\mathrm{Var}(G \\mid P  P_t) = \\mathbb{E}[\\mathrm{Var}(G \\mid P) \\mid P  P_t] + \\mathrm{Var}(\\mathbb{E}[G \\mid P] \\mid P  P_t)\n$$\nThe first term is the expectation of a constant:\n$$\n\\mathbb{E}[\\mathrm{Var}(G \\mid P) \\mid P  P_t] = \\mathbb{E}[V_A(1 - h^2) \\mid P  P_t] = V_A(1 - h^2)\n$$\nThe second term is the variance of the conditional expectation:\n$$\n\\mathrm{Var}(\\mathbb{E}[G \\mid P] \\mid P  P_t) = \\mathrm{Var}(h^2 P \\mid P  P_t) = (h^2)^2 \\mathrm{Var}(P \\mid P  P_t) = (h^2)^2 V_P^{(S)}\n$$\nwhere $V_P^{(S)}$ is the phenotypic variance in the selected group. To find $V_P^{(S)}$, we standardize $P$ to $Z = P / \\sqrt{V_P}$, where $Z \\sim \\mathcal{N}(0, 1)$. The selection condition $P  P_t$ becomes $Z  z_\\pi$, where $z_\\pi = P_t / \\sqrt{V_P}$. The problem provides the variance of a truncated standard normal variable: $\\mathrm{Var}(Z \\mid Z  z_\\pi) = 1 + z_\\pi \\lambda - \\lambda^2$, with $\\lambda = \\phi(z_\\pi)/\\pi$.\nThus, the variance of the truncated phenotype is $V_P^{(S)} = \\mathrm{Var}(\\sqrt{V_P} Z \\mid Z  z_\\pi) = V_P \\mathrm{Var}(Z \\mid Z  z_\\pi) = V_P (1 + z_\\pi \\lambda - \\lambda^2)$.\nSubstituting back, we get:\n$$\nV_A^{(S)} = V_A(1 - h^2) + (h^2)^2 V_P (1 + z_\\pi \\lambda - \\lambda^2)\n$$\nReplacing $h^2 = V_A/V_P$ and simplifying:\n$$\nV_A^{(S)} = V_A\\left(1 - \\frac{V_A}{V_P}\\right) + \\left(\\frac{V_A}{V_P}\\right)^2 V_P (1 + z_\\pi \\lambda - \\lambda^2) = V_A - \\frac{V_A^2}{V_P} + \\frac{V_A^2}{V_P}(1 + z_\\pi \\lambda - \\lambda^2)\n$$\n$$\nV_A^{(S)} = V_A + \\frac{V_A^2}{V_P}(z_\\pi \\lambda - \\lambda^2) = V_A\\left(1 - \\frac{V_A}{V_P}(\\lambda^2 - z_\\pi \\lambda)\\right)\n$$\nLet us define the selection intensity coefficient $k = \\lambda(\\lambda - z_\\pi)$. The expression simplifies to the well-known result:\n$$\nV_A^{(S)} = V_A(1 - k h^2)\n$$\nFor any non-trivial selection ($0  \\pi  1$), one can show $k0$, implying $V_A^{(S)}  V_A$. Selection reduces additive genetic variance by inducing negative linkage disequilibrium.\n\nSecond, we derive the additive variance in the next generation, $V_A^{\\text{next}}$. The selected parents mate randomly. The breeding value of an offspring is the average of its parents' breeding values, plus a Mendelian sampling term $\\delta$: $G_{\\text{offspring}} = \\frac{1}{2}(G_{\\text{father}} + G_{\\text{mother}}) + \\delta$.\nThe variance is $V_A^{\\text{next}} = \\mathrm{Var}(G_{\\text{offspring}}) = \\mathrm{Var}(\\frac{1}{2}(G_{\\text{father}} + G_{\\text{mother}})) + \\mathrm{Var}(\\delta)$. Since parents are chosen randomly from the selected group, $\\mathrm{Var}(G_{\\text{father}}) = \\mathrm{Var}(G_{\\text{mother}}) = V_A^{(S)}$. Their covariance is $0$.\n$$\nV_A^{\\text{next}} = \\frac{1}{4}(\\mathrm{Var}(G_{\\text{father}}) + \\mathrm{Var}(G_{\\text{mother}})) + \\mathrm{Var}(\\delta) = \\frac{1}{2}V_A^{(S)} + \\mathrm{Var}(\\delta)\n$$\nThe Mendelian sampling variance, $\\mathrm{Var}(\\delta)$, arises from segregation of alleles. With free recombination (unlinked loci), all linkage disequilibrium induced by selection is dissipated in one generation. The variance from segregation depends on the underlying allele frequencies, which under the infinitesimal model do not change. This variance is equal to half the genic variance, $\\mathrm{Var}(\\delta) = \\frac{1}{2}V_g$. The genic variance $V_g$ is the additive variance that would exist if the population were in linkage equilibrium; under the problem's assumptions, it is constant and equal to $V_A(0)$.\nThis yields the final recurrence relation:\n$$\nV_A^{\\text{next}} = \\frac{1}{2}V_A^{(S)} + \\frac{1}{2}V_g\n$$\nThis expression shows that $V_A^{\\text{next}}$ is the arithmetic mean of the post-selection variance $V_A^{(S)}$ and the constant genic variance $V_g$. Consequently, for any case where $V_A^{(S)} \\neq V_g$ (which is true for any non-trivial selection), $V_A^{\\text{next}}$ must lie strictly between $V_A^{(S)}$ and $V_g$. Selection drives $V_A$ down towards an equilibrium value below $V_g$, while recombination partially restores it by breaking down the negative linkage disequilibrium.\n\nThe computational algorithm proceeds as follows:\n1.  Initialize $V_A = V_A(0)$ and $V_g = V_A(0)$.\n2.  If $\\pi=1$, there is no selection, $k=0$, $V_A^{(S)}=V_A$. The recursion becomes $V_A^{\\text{next}} = \\frac{1}{2}V_A + \\frac{1}{2}V_g$. Since $V_A(0)=V_g$, $V_A$ remains equal to $V_g$ for all generations. Thus, $\\rho(T)=1$.\n3.  If $\\pi1$, calculate the constants $z_\\pi$, $\\lambda$, and $k$.\n4.  Iterate for $T$ generations. In each generation $t$:\n    a. Calculate $V_P(t) = V_A(t) + V_E$ and $h^2(t) = V_A(t) / V_P(t)$.\n    b. Calculate $V_A^{(S)}(t) = V_A(t)(1 - k \\cdot h^2(t))$.\n    c. Calculate $V_A(t+1) = \\frac{1}{2}V_A^{(S)}(t) + \\frac{1}{2}V_g$.\n5.  After $T$ iterations, compute the final ratio $\\rho(T) = V_A(T) / V_g$.\nThis procedure is implemented to solve the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the reduction of additive genetic variance due to the Bulmer effect.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate selection, intermediate heritability)\n        (1.0, 1.0, 0.1, 5),\n        # Case B (no environmental variance, strong heritability)\n        (1.0, 0.0, 0.2, 10),\n        # Case C (no selection boundary)\n        (0.5, 1.5, 1.0, 50),\n        # Case D (very strong selection, low heritability, near-equilibrium)\n        (1.0, 3.0, 0.01, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        Va0, Ve, pi, T = case\n        \n        # Under the infinitesimal model, genic variance is constant and equals\n        # the initial additive variance before selection begins.\n        Vg = Va0\n        \n        # Handle the boundary case of no selection (pi = 1).\n        if pi == 1.0:\n            # With no selection, V_A remains at V_g.\n            # The recursion V_A(t+1) = 0.5 * V_A(t) + 0.5 * V_g with V_A(0) = V_g\n            # yields V_A(t) = V_g for all t.\n            # Therefore, the ratio V_A(T) / V_g is always 1.\n            rho_T = 1.0\n            results.append(f\"{rho_T:.6f}\")\n            continue\n\n        # Pre-calculate selection-related constants that do not change over generations.\n        # z_pi is the truncation point on a standard normal distribution.\n        z_pi = norm.ppf(1.0 - pi)\n        # lambda_val is the mean of the truncated standard normal distribution.\n        # Note: 'lambda' is a reserved keyword in Python.\n        lambda_val = norm.pdf(z_pi) / pi\n        # k is the selection intensity coefficient related to variance reduction.\n        k = lambda_val * (lambda_val - z_pi)\n        \n        # Initialize the additive genetic variance for the first generation.\n        Va_t = Va0\n        \n        # Iterate the recursion for T generations.\n        for _ in range(T):\n            # Phenotypic variance in the current generation.\n            Vp_t = Va_t + Ve\n            \n            # Heritability in the current generation.\n            # We assume Vp_t  0, which is true for the given test cases.\n            h2_t = Va_t / Vp_t if Vp_t  0 else 0.0\n            \n            # Additive variance among selected parents (after selection).\n            Va_s = Va_t * (1.0 - k * h2_t)\n            \n            # Additive variance in the next generation (Bulmer's recursion).\n            # This accounts for recombination breaking down linkage disequilibrium.\n            Va_next = 0.5 * Va_s + 0.5 * Vg\n            \n            # Update the variance for the next iteration.\n            Va_t = Va_next\n            \n        # The final ratio is rho(T) = V_A(T) / V_g.\n        rho_T = Va_t / Vg\n        results.append(f\"{rho_T:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2744369"}]}