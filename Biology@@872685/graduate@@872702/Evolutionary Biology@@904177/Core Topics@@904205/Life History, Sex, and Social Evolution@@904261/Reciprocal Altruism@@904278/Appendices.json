{"hands_on_practices": [{"introduction": "The foundation of reciprocal altruism rests on a simple yet powerful calculation: the cost of a helpful act must be outweighed by the expected benefit of future reciprocation. This first exercise moves beyond qualitative arguments to the quantitative core of this principle. By calculating the expected net fitness change for individuals in a probabilistic exchange, you will solidify your understanding of the fundamental condition required for cooperation to be a profitable strategy [@problem_id:1877294].", "problem": "In studies of animal behavior, reciprocal altruism is a model used to explain cooperative acts between unrelated individuals. Consider a simplified model of blood-sharing in a colony of vampire bats, where fitness gains and losses are quantified in abstract 'energy units'. The cost to a donor bat for sharing a blood meal is denoted by $c$. The benefit to the recipient, which is often near starvation, is denoted by $b$.\n\nNow, consider a single interaction sequence between two unrelated bats, Bat A and Bat B. Bat A is known to be a 'cooperator' and initiates the sequence by sharing its meal with Bat B. Bat B, however, does not always reciprocate. For any given act of kindness it receives, the probability that Bat B will reciprocate in a future encounter is $p$. Assume the act of reciprocation, if it occurs, involves the same cost and benefit values.\n\nGiven the following parameters:\n- Cost of donation, $c = 8$ energy units\n- Benefit of reception, $b = 30$ energy units\n- Probability of reciprocation, $p = 0.75$\n\nCalculate the expected net fitness change for Bat A and the expected net fitness change for Bat B from this entire interaction sequence, which is initiated by Bat A. Express your answer as a pair of numerical values, in energy units, representing the expected net change for Bat A followed by the expected net change for Bat B.", "solution": "The problem asks for the expected net fitness change for each of the two bats, Bat A and Bat B. The expected value of a quantity that depends on probabilistic outcomes is calculated by summing the products of each possible outcome's value and its probability of occurrence.\n\nFirst, let's analyze the expected net fitness change for Bat A. The interaction sequence involves two parts for Bat A: the initial definitive cost and the potential future benefit.\n1.  Bat A initiates the sharing. This is a certain event. The cost to Bat A is $c$, so its fitness changes by $-c$.\n2.  Bat B may or may not reciprocate later. The benefit to Bat A depends on Bat B's action.\n    -   With probability $p$, Bat B reciprocates, and Bat A receives a benefit of $b$.\n    -   With probability $(1-p)$, Bat B does not reciprocate, and Bat A receives no benefit (a change of 0).\nThe expected benefit for Bat A from this probabilistic event is the sum of (value of outcome $\\times$ probability of outcome):\n$$E[\\text{Benefit for A}] = (b \\times p) + (0 \\times (1-p)) = pb$$\nThe total expected net fitness change for Bat A, denoted $E_A$, is the sum of the initial cost and the expected future benefit:\n$$E_A = -c + pb$$\nSubstituting the given values:\n$$E_A = -8 + (0.75)(30) = -8 + 22.5 = 14.5$$\nSo, the expected net fitness change for Bat A is 14.5 energy units.\n\nNext, let's analyze the expected net fitness change for Bat B. The interaction sequence also involves two parts for Bat B: the initial definitive benefit and the potential future cost.\n1.  Bat B receives the shared meal from Bat A. This is a certain event. The benefit to Bat B is $b$, so its fitness changes by $+b$.\n2.  Bat B may or may not reciprocate later. The cost to Bat B depends on its own action.\n    -   With probability $p$, Bat B reciprocates, incurring a cost of $c$. The fitness change associated with this cost is $-c$.\n    -   With probability $(1-p)$, Bat B does not reciprocate, incurring no cost (a change of 0).\nThe expected fitness change for Bat B from its own potential action is:\n$$E[\\text{Cost for B}] = (-c \\times p) + (0 \\times (1-p)) = -pc$$\nThe total expected net fitness change for Bat B, denoted $E_B$, is the sum of the initial benefit and the expected future cost:\n$$E_B = b - pc$$\nSubstituting the given values:\n$$E_B = 30 - (0.75)(8) = 30 - 6 = 24$$\nSo, the expected net fitness change for Bat B is 24 energy units.\n\nThe final answer is the pair of expected net fitness changes for Bat A and Bat B, respectively.", "answer": "$$\\boxed{\\begin{pmatrix} 14.5  24 \\end{pmatrix}}$$", "id": "1877294"}, {"introduction": "For cooperation to evolve, a cooperative strategy must not only be beneficial when established but must also be able to invade a population of non-cooperators. This practice delves into the crucial concept of invasion dynamics using the classic Tit-for-Tat strategy. You will explore how factors beyond simple cost and benefit, such as the probability of future encounters ($w$) and non-random social structures ($k$), create the ecological conditions necessary for reciprocal altruism to gain an evolutionary foothold [@problem_id:1877275].", "problem": "In evolutionary game theory, the emergence of cooperative behavior in populations of self-interested individuals is a central puzzle. The structure of the game being played is critical. Consider a scenario modeled by an iterated game where in each round, two players can either Cooperate (C) or Defect (D). The payoffs for a player are given by the following matrix, where the player's move is on the row and the opponent's move is on the column:\n- Payoff for (C, C): $R$ (Reward)\n- Payoff for (C, D): $S$ (Sucker)\n- Payoff for (D, C): $T$ (Temptation)\n- Payoff for (D, D): $P$ (Punishment)\n\nThe game is a Prisoner's Dilemma if $T  R  P  S$. After each round, there is a constant probability $w$ that another round of the game will be played between the same two individuals. The total expected payoff is the sum of payoffs over all rounds.\n\nConsider a large population composed almost entirely of individuals playing the \"Always Defect\" (ALLD) strategy. A small group of mutants playing the \"Tit-for-Tat\" (TFT) strategy is introduced. A TFT player cooperates on the first move and thereafter copies its opponent's move from the previous round.\n\nDue to social structuring or kinship, the mutant TFT players may interact with each other more frequently than expected by random chance. Let $k$ be the coefficient of assortment, defined as the probability that an interaction for a TFT individual is with another TFT individual. Consequently, a TFT player interacts with a random member of the vast ALLD population with probability $1-k$.\n\nFor cooperation to emerge, the TFT strategy must be able to invade the ALLD population. This requires the expected payoff for a TFT individual to be greater than the expected payoff for an ALLD individual.\n\nGiven the Prisoner's Dilemma payoffs $T=4$, $R=2$, $P=1$, and $S=0$, and a probability of future interaction $w=0.5$, calculate the minimum coefficient of assortment, $k_{crit}$, required for the TFT strategy to invade the ALLD population. Express your answer as a single fraction.", "solution": "In each iterated match, the probability that round $t+1$ occurs is $w^{t}$, so the expected total payoff equals the sum over rounds of the stage payoff weighted by $w^{t}$. Using the geometric series $\\sum_{t=0}^{\\infty} w^{t} = \\frac{1}{1-w}$ for $0w1$, compute the expected payoffs for the relevant pairings:\n\n- TFT vs TFT: both play $C$ forever, so every round yields $R$:\n$$\nV_{\\text{TFT},\\text{TFT}}=\\sum_{t=0}^{\\infty} w^{t} R=\\frac{R}{1-w}.\n$$\n\n- TFT vs ALLD: the sequence is $(C,D)$ in round $1$ (payoff $S$ to TFT), then $(D,D)$ thereafter (payoff $P$ to TFT). Hence\n$$\nV_{\\text{TFT},\\text{ALLD}}=S+\\sum_{t=1}^{\\infty} w^{t} P=S+\\frac{wP}{1-w}.\n$$\n\n- ALLD vs ALLD: every round is $(D,D)$, yielding\n$$\nV_{\\text{ALLD},\\text{ALLD}}=\\sum_{t=0}^{\\infty} w^{t} P=\\frac{P}{1-w}.\n$$\n\nWith assortment $k$, a rare TFT individual meets another TFT with probability $k$ and an ALLD with probability $1-k$, so its expected payoff is\n$$\n\\Pi_{\\text{TFT}}=k\\,V_{\\text{TFT},\\text{TFT}}+(1-k)\\,V_{\\text{TFT},\\text{ALLD}}.\n$$\nA resident ALLD in an ALLD population (mutants rare) meets ALLD almost surely, so\n$$\n\\Pi_{\\text{ALLD}}=V_{\\text{ALLD},\\text{ALLD}}.\n$$\nThe invasion condition is $\\Pi_{\\text{TFT}}\\Pi_{\\text{ALLD}}$, i.e.\n$$\nk\\frac{R}{1-w}+(1-k)\\left(S+\\frac{wP}{1-w}\\right)\\frac{P}{1-w}.\n$$\nMultiplying both sides by $(1-w)$ and rearranging,\n$$\nkR+(1-k)\\left(S(1-w)+wP\\right)P,\n$$\n$$\nk\\left[R-\\left(S(1-w)+wP\\right)\\right]P-\\left(S(1-w)+wP\\right).\n$$\nThus the critical assortment threshold is\n$$\nk_{\\text{crit}}=\\frac{P-\\left(S(1-w)+wP\\right)}{R-\\left(S(1-w)+wP\\right)}=\\frac{(1-w)(P-S)}{(R-S)-w(P-S)}.\n$$\nSubstitute $R=2$, $P=1$, $S=0$, $w=0.5$:\n$$\nk_{\\text{crit}}=\\frac{(1-0.5)(1-0)}{(2-0)-0.5(1-0)}=\\frac{0.5}{2-0.5}=\\frac{1}{3}.\n$$\nTherefore, the minimum coefficient of assortment required is the fraction $\\frac{1}{3}$.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1877275"}, {"introduction": "Real-world interactions are rarely perfect; misperceptions and errors can disrupt cooperation. A robust cooperative strategy must be able to recover from such mistakes. This advanced problem challenges you to model an error-prone world using Markov chains, a powerful tool for analyzing state-dependent stochastic processes. By comparing the long-run payoffs of two iconic strategies, Tit-for-Tat (TFT) and Win-Stay, Lose-Shift (WSLS), you will gain insight into why some strategies are more resilient than others in the face of uncertainty [@problem_id:2527576].", "problem": "Consider an infinite-horizon repeated Prisoner’s Dilemma (PD) in the donation-game form, a canonical model in ecology for cooperation among unrelated individuals. In each round, each player chooses either cooperation ($C$) or defection ($D$). Cooperation confers a benefit $b0$ to the partner at a personal cost $c0$. The one-shot PD payoffs to the row player are therefore: mutual cooperation ($CC$) yields $R=b-c$, unilateral cooperation when the partner defects ($CD$) yields $S=-c$, unilateral defection when the partner cooperates ($DC$) yields $T=b$, and mutual defection ($DD$) yields $P=0$. Assume $bc0$, so that $TRPS$ as required for a PD.\n\nPlayers implement one of two memory-one strategies that depend only on the previous realized outcome:\n- Tit-for-Tat (TFT): cooperate if and only if the partner cooperated in the previous round.\n- Win-Stay, Lose-Shift (WSLS): repeat the previous move if the previous payoff was a “win” ($R$ or $T$), and switch moves if the previous payoff was a “lose” ($P$ or $S$).\n\nImplementation errors occur independently: in each move, a player’s intended action is flipped ($C \\leftrightarrow D$) with probability $\\epsilon \\in (0,1)$, independently across players and rounds. For any fixed pair of strategies, the dynamics over the four realized joint-action states $\\{CC, CD, DC, DD\\}$ form a time-homogeneous Markov chain with a unique stationary distribution because the chain is finite, irreducible, and aperiodic for $0\\epsilon1$.\n\nDefine the long-run average payoff per round for a strategy pair as the expected one-shot payoff under the stationary distribution over $\\{CC, CD, DC, DD\\}$. Say that WSLS “outperforms” TFT if the long-run average payoff in WSLS self-play exceeds that in TFT self-play.\n\nStarting only from the above definitions and the law of total probability for Markov chains, derive symbolically the stationary distributions for TFT-vs-TFT and WSLS-vs-WSLS under noise level $\\epsilon$, compute their respective long-run average payoffs, and determine the unique critical error probability $\\epsilon^{\\ast}$ at which WSLS self-play and TFT self-play have equal long-run average payoffs. Your final answer must be the closed-form value of $\\epsilon^{\\ast}$. No rounding is required, and the answer is unitless.", "solution": "The problem requires the derivation of the critical error probability $\\epsilon^{\\ast}$ at which the long-run average payoff for Tit-for-Tat (TFT) self-play, $V_{TFT}$, equals the long-run average payoff for Win-Stay, Lose-Shift (WSLS) self-play, $V_{WSLS}$. The game is a repeated Prisoner's Dilemma with payoffs $R=b-c$, $S=-c$, $T=b$, and $P=0$, where $bc0$. The state of the system is the joint action of the two players in the previous round, from the set $\\{CC, CD, DC, DD\\}$. An intended action is flipped with probability $\\epsilon \\in (0,1)$.\n\nFirst, we analyze the TFT-vs-TFT interaction. A TFT player cooperates if and only if its partner cooperated in the previous round. The intended actions $(I_1, I_2)$ for the next round given the current state $(A_1, A_2)$ are:\n- From $CC$: Both players saw cooperation, so both intend to cooperate. Intended state is $(C,C)$.\n- From $CD$: Player $1$ saw defection, Player $2$ saw cooperation. Intended state is $(D,C)$.\n- From $DC$: Player $1$ saw cooperation, Player $2$ saw defection. Intended state is $(C,D)$.\n- From $DD$: Both players saw defection, so both intend to defect. Intended state is $(D,D)$.\n\nAn intended cooperation ($I_C$) becomes an actual cooperation ($C$) with probability $1-\\epsilon$ and an actual defection ($D$) with probability $\\epsilon$. An intended defection ($I_D$) becomes an actual $D$ with probability $1-\\epsilon$ and an actual $C$ with probability $\\epsilon$. The transition matrix $M_{TFT}$ for the Markov chain over states $\\{CC, CD, DC, DD\\}$ is therefore:\n$$\nM_{TFT} =\n\\begin{pmatrix}\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2 \\\\\n\\epsilon(1-\\epsilon)  \\epsilon^2  (1-\\epsilon)^2  \\epsilon(1-\\epsilon) \\\\\n\\epsilon(1-\\epsilon)  (1-\\epsilon)^2  \\epsilon^2  \\epsilon(1-\\epsilon) \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2\n\\end{pmatrix}\n$$\nThe sum of each column in $M_{TFT}$ is $1$, meaning the matrix is doubly stochastic. For an irreducible and aperiodic finite Markov chain, a doubly stochastic transition matrix implies that the unique stationary distribution is the uniform distribution. Let $\\pi_{TFT} = (p_{CC}, p_{CD}, p_{DC}, p_{DD})$ be this distribution. Thus, $p_{CC} = p_{CD} = p_{DC} = p_{DD} = \\frac{1}{4}$.\nThe long-run average payoff for a TFT player is the expected payoff under this stationary distribution:\n$$V_{TFT} = p_{CC}R + p_{CD}S + p_{DC}T + p_{DD}P$$\n$$V_{TFT} = \\frac{1}{4} (b-c) + \\frac{1}{4} (-c) + \\frac{1}{4} (b) + \\frac{1}{4} (0) = \\frac{1}{4}(2b - 2c) = \\frac{b-c}{2}$$\n\nNext, we analyze the WSLS-vs-WSLS interaction. A WSLS player repeats its previous move after a \"win\" (payoff $R$ or $T$) and switches its move after a \"lose\" (payoff $P$ or $S$). The intended actions are:\n- From $CC$: Both players received $R$ (a win). Both repeat their move ($C$). Intended state is $(C,C)$.\n- From $CD$: Player $1$ received $S$ (a lose), Player $2$ received $T$ (a win). Player $1$ switches from $C$ to $D$, Player $2$ repeats $D$. Intended state is $(D,D)$.\n- From $DC$: Player $1$ received $T$ (a win), Player $2$ received $S$ (a lose). Player $1$ repeats $D$, Player $2$ switches from $C$ to $D$. Intended state is $(D,D)$.\n- From $DD$: Both received $P$ (a lose). Both switch their move from $D$ to $C$. Intended state is $(C,C)$.\n\nThe transition matrix $M_{WSLS}$ is constructed based on these intended moves:\n$$\nM_{WSLS} =\n\\begin{pmatrix}\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2 \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2 \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2 \\\\\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2\n\\end{pmatrix}\n$$\nLet the stationary distribution be $\\pi_{WSLS} = (q_{CC}, q_{CD}, q_{DC}, q_{DD})$. Due to symmetry, $q_{CD}=q_{DC}$. The stationarity equations $\\pi_{WSLS} M_{WSLS} = \\pi_{WSLS}$ yield for state $CD$:\n$$q_{CD} = q_{CC}\\epsilon(1-\\epsilon) + q_{CD}\\epsilon(1-\\epsilon) + q_{DC}\\epsilon(1-\\epsilon) + q_{DD}\\epsilon(1-\\epsilon)$$\n$$q_{CD} = (q_{CC} + q_{CD} + q_{DC} + q_{DD})\\epsilon(1-\\epsilon)$$\nSince the sum of probabilities is $1$, we find $q_{CD} = \\epsilon(1-\\epsilon)$. Therefore, $q_{DC} = \\epsilon(1-\\epsilon)$.\nThe normalization condition is $q_{CC} + q_{CD} + q_{DC} + q_{DD} = 1$, which gives $q_{CC} + q_{DD} = 1 - 2\\epsilon(1-\\epsilon)$.\nNow consider the flow between the set of symmetric states $S_{sym}=\\{CC, DD\\}$ and asymmetric states $S_{asym}=\\{CD, DC\\}$. Let $X=q_{CC}+q_{DD}$ and $Y=q_{CD}+q_{DC}=2\\epsilon(1-\\epsilon)$.\nThe total probability of transitioning from $S_{sym}$ to $S_{asym}$ is $P(S_{asym}|S_{sym}) = \\epsilon(1-\\epsilon) + \\epsilon(1-\\epsilon) = 2\\epsilon(1-\\epsilon)$.\nThe total probability of transitioning from $S_{asym}$ to $S_{asym}$ is $P(S_{asym}|S_{asym}) = \\epsilon(1-\\epsilon) + \\epsilon(1-\\epsilon) = 2\\epsilon(1-\\epsilon)$. Uh oh, that is incorrect.\nThe probability of going from state $CD$ (intent $(D,D)$) to $CD$ is $\\epsilon(1-\\epsilon)$, to $DC$ is $\\epsilon(1-\\epsilon)$. Sum is $2\\epsilon(1-\\epsilon)$. So this part is correct. Let's use the stationarity equation for $q_{CC}$.\n$$q_{CC} = q_{CC}(1-\\epsilon)^2 + q_{CD}\\epsilon^2 + q_{DC}\\epsilon^2 + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC} = q_{CC}(1-\\epsilon)^2 + 2\\epsilon(1-\\epsilon)\\epsilon^2 + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC}(1 - (1-\\epsilon)^2) = 2\\epsilon^3(1-\\epsilon) + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC}\\epsilon(2-\\epsilon) = 2\\epsilon^3(1-\\epsilon) + (1 - 2\\epsilon(1-\\epsilon) - q_{CC})(1-\\epsilon)^2$$\nSolving this algebraic equation is cumbersome. A more elegant method uses state aggregates. Let $X=q_{CC}+q_{DD}$ and $Y=q_{CD}+q_{DC}$. At stationarity, the probability of being in state $CC$ is:\n$$q_{CC} = (q_{CC}+q_{DD})(1-\\epsilon)^2 + (q_{CD}+q_{DC})\\epsilon^2 = X(1-\\epsilon)^2 + Y\\epsilon^2$$\nAnd for $DD$:\n$$q_{DD} = (q_{CC}+q_{DD})\\epsilon^2 + (q_{CD}+q_{DC})(1-\\epsilon)^2 = X\\epsilon^2 + Y(1-\\epsilon)^2$$\nSumming these gives $q_{CC}+q_{DD} = X = (X+Y)((1-\\epsilon)^2+\\epsilon^2) = 1-2\\epsilon+2\\epsilon^2$. Then $Y=1-X=2\\epsilon-2\\epsilon^2$. This confirms $q_{CD}=q_{DC}=\\epsilon(1-\\epsilon)$.\nSubtracting the equations gives:\n$$q_{CC}-q_{DD} = (X-Y)((1-\\epsilon)^2-\\epsilon^2) = (X-Y)(1-2\\epsilon)$$\nSubstituting $X=1-2\\epsilon+2\\epsilon^2$ and $Y=2\\epsilon-2\\epsilon^2$ gives $X-Y = 1-4\\epsilon+4\\epsilon^2 = (1-2\\epsilon)^2$.\nSo, $q_{CC}-q_{DD} = (1-2\\epsilon)^2(1-2\\epsilon) = (1-2\\epsilon)^3$.\nWe have a system of two linear equations for $q_{CC}$ and $q_{DD}$:\n$q_{CC}+q_{DD} = 1-2\\epsilon+2\\epsilon^2$\n$q_{CC}-q_{DD} = (1-2\\epsilon)^3 = 1-6\\epsilon+12\\epsilon^2-8\\epsilon^3$\nAdding them: $2q_{CC} = 2-8\\epsilon+14\\epsilon^2-8\\epsilon^3 \\implies q_{CC} = 1-4\\epsilon+7\\epsilon^2-4\\epsilon^3$.\nThe long-run average payoff for WSLS is:\n$$V_{WSLS} = q_{CC}R + q_{CD}S + q_{DC}T + q_{DD}P$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(-c) + \\epsilon(1-\\epsilon)(b) + q_{DD}(0)$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(b-c) = (q_{CC} + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1-4\\epsilon+7\\epsilon^2-4\\epsilon^3 + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n\nFinally, we find the critical error probability $\\epsilon^{\\ast}$ by equating the payoffs:\n$$V_{TFT} = V_{WSLS}$$\n$$\\frac{b-c}{2} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\nSince $bc$, $b-c0$, so we can divide by it:\n$$\\frac{1}{2} = 1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n$$0 = \\frac{1}{2} - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\nMultiplying by $-2$:\n$$0 = 8\\epsilon^3 - 12\\epsilon^2 + 6\\epsilon - 1$$\nThis is the expansion of a cube:\n$$0 = (2\\epsilon - 1)^3$$\nThe unique real solution in the interval $(0,1)$ is found by setting the base to zero:\n$$2\\epsilon^{\\ast} - 1 = 0 \\implies \\epsilon^{\\ast} = \\frac{1}{2}$$\nThis is the critical error probability at which the long-run average payoffs of TFT and WSLS self-play are identical.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2527576"}]}