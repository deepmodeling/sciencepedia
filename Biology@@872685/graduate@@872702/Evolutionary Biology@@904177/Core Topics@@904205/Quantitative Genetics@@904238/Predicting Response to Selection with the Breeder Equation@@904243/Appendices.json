{"hands_on_practices": [{"introduction": "This problem is a foundational exercise designed to solidify your understanding of the breeder's equation, $R = h^2 S$. You will start from first principles—the regression of an individual's breeding value on its phenotype—to derive this cornerstone of quantitative genetics. This practice will also sharpen your intuition for how measurement scales and standardization affect the components of the equation, a crucial skill for interpreting results from different studies [@problem_id:2745780].", "problem": "A single, metric quantitative trait $X$ (e.g., a linear morphological measurement) is recorded in millimeters in a large, randomly mating population with purely additive gene action and no dominance, epistasis, or genotype-by-environment interaction. The pre-selection phenotypic variance is $V_{P} = 9$ mm$^{2}$, and the pre-selection additive genetic variance is $V_{A} = 4$ mm$^{2}$. A bout of viability selection occurs such that the mean phenotype of the selected parents is higher than the pre-selection population mean by $1.8$ mm. Investigators also analyze the standardized trait $Z = (X - \\mu)/\\sigma$, where $\\mu$ and $\\sigma$ denote the pre-selection trait mean and standard deviation in the same focal generation. With this standardization, the observed selection differential on $Z$ is $0.6$.\n\nStarting only from the definitions of narrow-sense heritability $h^{2}$ as $V_{A}/V_{P}$ and the linear regression relationship between breeding value and phenotype that follows from additivity, derive the one-generation evolutionary response of the trait mean to the described within-generation selection. Then compute its numerical value and convert it to centimeters. Verify conceptually why computing the response in standardized units and then back-transforming yields the same answer as computing directly in millimeters, and explain how a change of measurement units (millimeters to centimeters) affects variances, differentials, and the response.\n\nProvide a single number: the predicted change in the trait mean after one generation expressed in centimeters. Round your answer to four significant figures.", "solution": "The problem will first be validated for scientific soundness, consistency, and completeness. Only if it is deemed valid will a solution be provided.\n\n**Problem Validation**\n\nStep 1: Extract Givens\nThe problem provides the following explicit information:\n-   A quantitative trait $X$ is measured in millimeters ($mm$).\n-   The population is large and randomly mating.\n-   Genetic effects are purely additive, with no dominance, epistasis, or genotype-by-environment interaction.\n-   Pre-selection phenotypic variance: $V_{P} = 9 \\text{ mm}^{2}$.\n-   Pre-selection additive genetic variance: $V_{A} = 4 \\text{ mm}^{2}$.\n-   Selection differential on trait $X$: $S = \\mu_{\\text{selected}} - \\mu_{\\text{population}} = 1.8 \\text{ mm}$.\n-   Standardized trait: $Z = (X - \\mu)/\\sigma$, where $\\sigma = \\sqrt{V_P}$.\n-   Selection differential on standardized trait $Z$: $i = S_Z = 0.6$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is evaluated against standard criteria for validity.\n1.  **Scientific Grounding**: The problem is based on the breeder's equation, a fundamental concept in quantitative genetics and evolutionary biology. All terms ($V_P$, $V_A$, $h^2$, $S$) are standard and correctly defined within this framework. The simplifying assumptions (additivity, etc.) are common in introductory models.\n2.  **Consistency**: We must check for internal consistency. The phenotypic standard deviation is $\\sigma_P = \\sqrt{V_P} = \\sqrt{9 \\text{ mm}^2} = 3 \\text{ mm}$. The standardized selection differential, or selection intensity, is defined as $i = S / \\sigma_P$. Using the provided values, we compute $i = \\frac{1.8 \\text{ mm}}{3 \\text{ mm}} = 0.6$. This calculated value precisely matches the given standardized selection differential of $0.6$. The inputs are therefore consistent and redundant, not contradictory. Furthermore, the additive genetic variance ($V_A = 4 \\text{ mm}^2$) is a component of and less than the phenotypic variance ($V_P = 9 \\text{ mm}^2$), as is required ($V_P = V_A + V_E$, where $V_E$ is environmental variance, must be non-negative).\n3.  **Well-Posedness**: The problem is well-posed. It provides all necessary information to calculate the requested quantity, the evolutionary response ($R$), and asks for a clear derivation and conceptual explanation. A unique, stable solution exists.\n\nVerdict: The problem is valid. It is scientifically sound, internally consistent, and well-posed. We proceed to the solution.\n\n**Derivation and Solution**\n\nThe objective is to derive the one-generation evolutionary response, $R$, which is the change in the population mean phenotype due to selection.\n$R = \\bar{P}_{\\text{offspring}} - \\bar{P}_{\\text{parental}}$.\n\nUnder the assumption that offspring inherit traits from parents and that non-genetic factors do not create a covariance between parents and offspring, the mean phenotype of the offspring generation is equal to the mean breeding value of the selected parents from the previous generation.\n$\\bar{P}_{\\text{offspring}} = \\bar{A}_{\\text{selected parents}}$.\n\nBy definition, the mean breeding value of the entire unselected parental population is zero when trait values are expressed as deviations from the population mean, so $\\bar{A}_{\\text{parental}} = 0$. The response is thus equal to the mean breeding value of the selected parents:\n$R = \\bar{A}_{\\text{selected parents}}$.\n\nThe problem requires a derivation from the linear regression of breeding value ($A$) on phenotype ($P$). The slope of this regression, $b_{AP}$, is given by:\n$$b_{AP} = \\frac{\\text{Cov}(A, P)}{V_P}$$\nThe phenotype $P$ is modeled as the sum of the breeding value $A$ and a residual term $E$, which encompasses environmental effects and non-additive genetic effects. The problem states there are no non-additive effects, so $P = A + E$. Assuming no genotype-by-environment correlation ($\\text{Cov}(A,E)=0$), the covariance term is:\n$$\\text{Cov}(A, P) = \\text{Cov}(A, A+E) = \\text{Cov}(A,A) + \\text{Cov}(A,E) = V_A + 0 = V_A$$\nSubstituting this into the expression for the slope gives:\n$$b_{AP} = \\frac{V_A}{V_P}$$\nThis ratio is the definition of narrow-sense heritability, $h^2$. Thus, $b_{AP} = h^2$.\n\nThe expected breeding value of an individual, given its phenotype $P$, can be predicted from this regression line:\n$E[A | P] = \\bar{A} + b_{AP} (P - \\bar{P})$.\nWorking with deviations from the mean where $\\bar{A}=0$ and $\\bar{P}=0$, this simplifies to:\n$E[A | P] = h^2 P$.\n\nThe mean breeding value of the selected parents, $\\bar{A}_{\\text{selected parents}}$, is the average of these expected breeding values over all selected individuals:\n$\\bar{A}_{\\text{selected parents}} = E[h^2 P_{\\text{selected}}] = h^2 \\bar{P}_{\\text{selected parents}}$.\nThe selection differential, $S$, is the difference between the mean phenotype of selected parents and the mean phenotype of the entire parental generation: $S = \\bar{P}_{\\text{selected parents}} - \\bar{P}_{\\text{parental}}$. With $\\bar{P}_{\\text{parental}}=0$, this is simply $S = \\bar{P}_{\\text{selected parents}}$.\nSubstituting $S$ into the equation for $\\bar{A}_{\\text{selected parents}}$ yields:\n$$\\bar{A}_{\\text{selected parents}} = h^2 S$$\nSince we established that $R = \\bar{A}_{\\text{selected parents}}$, we arrive at the breeder's equation:\n$$R = h^2 S$$\nThis completes the required derivation.\n\nNow, we compute the numerical value for the response. First, we calculate the narrow-sense heritability $h^2$:\n$$h^2 = \\frac{V_A}{V_P} = \\frac{4 \\text{ mm}^2}{9 \\text{ mm}^2} = \\frac{4}{9}$$\nThe selection differential $S$ is given as $1.8 \\text{ mm}$. The response $R$ is:\n$$R = h^2 S = \\left(\\frac{4}{9}\\right) \\times 1.8 \\text{ mm} = \\frac{7.2}{9} \\text{ mm} = 0.8 \\text{ mm}$$\n\n**Conceptual Verification and Unit Transformation**\n\nFirst, we verify this result using the standardized trait $Z$. The response in standardized units, $R_Z$, is given by $R_Z = h_Z^2 S_Z$. The selection differential on $Z$ is given as $S_Z = i = 0.6$. The heritability of the standardized trait, $h_Z^2$, is $h_Z^2 = V_{A,Z} / V_Z$. The variance of a standardized variable is $V_Z = 1$. The additive genetic variance for $Z$ is $V_{A,Z} = V_A / \\sigma_P^2 = V_A / V_P = h^2$. Thus, $h_Z^2 = h^2 / 1 = h^2$. Heritability is a dimensionless ratio and is invariant to linear scaling of the trait.\nThe standardized response is:\n$$R_Z = h^2 i = \\left(\\frac{4}{9}\\right) \\times 0.6 = \\frac{2.4}{9} = \\frac{4}{15}$$\nThe response $R$ is a change in the mean, $\\Delta \\mu_X$. The standardized response $R_Z$ is the change in the mean of $Z$, which is $\\Delta (\\frac{X-\\mu_X}{\\sigma_X}) = \\frac{\\Delta \\mu_X}{\\sigma_X} = \\frac{R}{\\sigma_P}$.\nTo convert $R_Z$ back to original units, we multiply by the phenotypic standard deviation, $\\sigma_P = 3 \\text{ mm}$:\n$$R = R_Z \\times \\sigma_P = \\left(\\frac{4}{15}\\right) \\times 3 \\text{ mm} = \\frac{12}{15} \\text{ mm} = \\frac{4}{5} \\text{ mm} = 0.8 \\text{ mm}$$\nThe result is identical, confirming the conceptual framework. Calculating in standardized units and then back-transforming is equivalent to direct calculation because $R = \\sigma_P R_Z = \\sigma_P (h^2 i) = \\sigma_P h^2 (S/\\sigma_P) = h^2 S$.\n\nNext, we analyze the effect of changing units from millimeters ($mm$) to centimeters ($cm$). The conversion factor is $1 \\text{ cm} = 10 \\text{ mm}$. A measurement in $cm$ is $1/10$ of its value in $mm$.\n-   **Variances** ($V_A, V_P$): As these have units of $length^2$, they scale by $(1/10)^2 = 1/100$.\n    $V_{P,cm} = 9/100 = 0.09 \\text{ cm}^2$.\n    $V_{A,cm} = 4/100 = 0.04 \\text{ cm}^2$.\n-   **Heritability** ($h^2$): As a ratio of variances, it is dimensionless and invariant to a change in units.\n    $h^2 = \\frac{0.04 \\text{ cm}^2}{0.09 \\text{ cm}^2} = \\frac{4}{9}$.\n-   **Differentials and Responses** ($S, R$): As these have units of $length$, they scale by $1/10$.\n    $S_{cm} = 1.8 / 10 = 0.18 \\text{ cm}$.\n    $R_{cm} = h^2 S_{cm} = \\left(\\frac{4}{9}\\right) \\times 0.18 \\text{ cm} = 4 \\times 0.02 \\text{ cm} = 0.08 \\text{ cm}$.\nThis is consistent with converting our initially calculated response: $R = 0.8 \\text{ mm} = 0.08 \\text{ cm}$.\n\n**Final Answer Calculation**\n\nThe problem asks for the predicted change in the trait mean after one generation, expressed in centimeters, rounded to four significant figures.\nThe calculated response is exactly $0.08 \\text{ cm}$. To express this number with four significant figures, we must add trailing zeros. The significant figures are 8, 0, 0, 0.\nThe final numerical answer is $0.0800 \\text{ cm}$.", "answer": "$$\\boxed{0.0800}$$", "id": "2745780"}, {"introduction": "Evolutionary dynamics are rarely as simple as the basic breeder's equation assumes. This exercise challenges you to extend the model to a more complex and realistic scenario involving non-random mating and population structure [@problem_id:2745763]. By deriving the response to selection when only one sex is directly selected but mating is phenotypically assortative, you will learn how social behaviors and demography can modulate the efficiency of selection.", "problem": "A large, randomly mating diploid population expresses a quantitative trait, measured in centimeters, determined by purely additive genetic effects and independent environmental effects. Let individual phenotype be the sum of breeding value and environmental deviation, so that $P = G + E$, with $\\operatorname{Var}(G)$ additive and $\\operatorname{Var}(E)$ environmental. The outbred base-population additive genetic variance is $V_{A0}$, and the environmental variance is $V_{E}$. The current population exhibits an inbreeding coefficient $F$ due to its recent demographic history. Assume that for purely additive gene action, inbreeding scales the additive genetic variance by the factor $(1+F)$ while leaving $V_{E}$ unchanged, so that the current additive genetic variance is $V_{A}(F) = (1+F)\\,V_{A0}$ and the current phenotypic variance is $V_{P} = V_{A}(F) + V_{E}$. Assume no dominance, no epistasis, no shared environmental covariance between mates or across generations, and that phenotypes are approximately jointly normally distributed across potential mating pairs due to many small-effect loci and environmental influences.\n\nA truncation selection regime is imposed on males only, yielding a male selection differential $S_{\\mathrm{m}}$ (the mean of selected males’ phenotypes minus the population mean). Females are not directly selected. However, mating is phenotypically assortative: the correlation between the phenotypes of mated male–female pairs is $\\rho$. Assume equal phenotypic variances in males and females and monogamous mating between the selected males and their female partners.\n\nUsing only the assumptions above, derive from first principles the expected one-generation change in the offspring generation’s mean phenotype (the response), accounting for both inbreeding and phenotypic assortative mating. Then evaluate your result for the following parameter values: $V_{A0} = 2.4$ cm$^{2}$, $V_{E} = 1.6$ cm$^{2}$, $F = 0.10$, $\\rho = 0.30$, and $S_{\\mathrm{m}} = 1.20$ cm. Express your final answer in centimeters and round your answer to four significant figures.", "solution": "The problem statement is subjected to validation and is found to be scientifically grounded, well-posed, and objective. It presents a standard, albeit multipart, problem in quantitative genetics that is solvable using established principles. All necessary parameters and assumptions are provided, and there are no internal contradictions or factual inaccuracies. The problem is therefore deemed valid.\n\nThe task is to derive the expected one-generation response to selection, denoted $R$, from first principles, and then to evaluate it for a given set of parameters. The response $R$ is defined as the change in the mean phenotype of the offspring generation relative to the parental generation mean. Without loss of generality, we can set the mean phenotype of the parental generation to zero, so that $R = \\bar{P}_{\\text{offspring}}$.\n\nThe phenotype $P$ of an individual is given by the sum of its breeding value $G$ and an environmental deviation $E$, such that $P = G + E$. The expected mean phenotype of the offspring generation is thus $\\bar{P}_{\\text{offspring}} = \\operatorname{E}[G_{\\text{offspring}} + E_{\\text{offspring}}] = \\bar{G}_{\\text{offspring}} + \\bar{E}_{\\text{offspring}}$. By the problem's assumption that environmental effects are independent with a mean of zero, we have $\\bar{E}_{\\text{offspring}} = 0$. Therefore, the response to selection is equal to the mean breeding value of the offspring generation:\n$$R = \\bar{G}_{\\text{offspring}}$$\nFor a diploid, randomly mating population under a purely additive genetic model, the breeding value of an offspring is the average of its parents' breeding values. Thus, the mean breeding value of the offspring generation is the average of the mean breeding values of the group of fathers and the group of mothers that produced them:\n$$\\bar{G}_{\\text{offspring}} = \\frac{1}{2}(\\bar{G}_{\\text{fathers}} + \\bar{G}_{\\text{mothers}})$$\nOur objective now is to determine the mean breeding values of the selected fathers and their mated partners, the mothers. The mean breeding value of a selected group of individuals is related to their mean phenotype through the regression of breeding value on phenotype. The conditional expectation of an individual's breeding value $G$ given its phenotype $P$ is $E[G | P=p] = \\mu_{G} + b_{GP}(p - \\mu_{P})$. The regression coefficient $b_{GP}$ is given by:\n$$b_{GP} = \\frac{\\operatorname{Cov}(G, P)}{\\operatorname{Var}(P)}$$\nGiven $P = G + E$ and that $G$ and $E$ are uncorrelated, $\\operatorname{Cov}(G, P) = \\operatorname{Cov}(G, G+E) = \\operatorname{Var}(G) + \\operatorname{Cov}(G,E) = \\operatorname{Var}(G)$. The variance of breeding values is the additive genetic variance, which in the current inbred population is $V_{A}(F)$. The phenotypic variance is $V_{P}$. Therefore, the regression coefficient is the narrow-sense heritability in the current population, $h^2(F) = \\frac{V_{A}(F)}{V_{P}}$.\n\nAssuming the population means $\\mu_G$ and $\\mu_P$ are zero, the mean breeding value of a selected group is $\\bar{G} = h^2(F) \\bar{P}$, where $\\bar{P}$ is the mean phenotype of that group.\n\nFor the males, truncation selection is applied, resulting in a selection differential $S_{\\mathrm{m}}$. This is defined as the mean phenotype of the selected males minus the population mean, so $\\bar{P}_{\\text{fathers}} = S_{\\mathrm{m}}$. The mean breeding value of the fathers is therefore:\n$$\\bar{G}_{\\text{fathers}} = h^2(F) S_{\\mathrm{m}}$$\nThe females are not directly selected but mate assortatively with the selected males, with a phenotypic correlation of $\\rho$. Given the assumption of joint normality of phenotypes among potential pairs, the expected phenotype of a female, $P_{\\mathrm{f}}$, given her male partner's phenotype $P_{\\mathrm{m}}$, is described by the linear regression $E[P_{\\mathrm{f}} | P_{\\mathrm{m}}=p_{\\mathrm{m}}] = \\mu_{P} + \\rho \\frac{\\sigma_{P_{\\mathrm{f}}}}{\\sigma_{P_{\\mathrm{m}}}} (p_{\\mathrm{m}} - \\mu_{P})$. The problem states that phenotypic variances are equal in both sexes ($\\sigma_{P_{\\mathrm{f}}} = \\sigma_{P_{\\mathrm{m}}}$). With $\\mu_P=0$, this simplifies to $E[P_{\\mathrm{f}} | P_{\\mathrm{m}}=p_{\\mathrm{m}}] = \\rho p_{\\mathrm{m}}$.\n\nThe mean phenotype of the group of mothers is the expectation of this conditional expectation, averaged over the phenotypes of the selected males.\n$$\\bar{P}_{\\text{mothers}} = \\operatorname{E}[E[P_{\\mathrm{f}} | P_{\\mathrm{m}}]]_{\\text{selected males}} = \\operatorname{E}[\\rho P_{\\mathrm{m}}]_{\\text{selected males}} = \\rho \\operatorname{E}[P_{\\mathrm{m}}]_{\\text{selected males}} = \\rho S_{\\mathrm{m}}$$\nThis represents a correlated selection differential on the females, induced by assortative mating with selected males. The mean breeding value of the mothers is subsequently:\n$$\\bar{G}_{\\text{mothers}} = h^2(F) \\bar{P}_{\\text{mothers}} = h^2(F) \\rho S_{\\mathrm{m}}$$\nWe now combine the results for the parental groups to find the response:\n$$R = \\frac{1}{2}(\\bar{G}_{\\text{fathers}} + \\bar{G}_{\\text{mothers}}) = \\frac{1}{2}(h^2(F) S_{\\mathrm{m}} + h^2(F) \\rho S_{\\mathrm{m}})$$\nFactoring this expression gives the final equation for the response to selection under these specified conditions:\n$$R = \\frac{1}{2} (1+\\rho) h^2(F) S_{\\mathrm{m}}$$\nThis equation must now be evaluated using the provided parameter values.\nGiven: $V_{A0} = 2.4\\,\\text{cm}^2$, $V_{E} = 1.6\\,\\text{cm}^2$, $F = 0.10$, $\\rho = 0.30$, and $S_{\\mathrm{m}} = 1.20\\,\\text{cm}$.\n\nFirst, we calculate the variances and heritability for the current population.\nThe additive genetic variance in the inbred population is:\n$$V_{A}(F) = (1+F)V_{A0} = (1+0.10) \\times 2.4\\,\\text{cm}^2 = 1.1 \\times 2.4\\,\\text{cm}^2 = 2.64\\,\\text{cm}^2$$\nThe total phenotypic variance is:\n$$V_{P} = V_{A}(F) + V_{E} = 2.64\\,\\text{cm}^2 + 1.6\\,\\text{cm}^2 = 4.24\\,\\text{cm}^2$$\nThe narrow-sense heritability in the current population is:\n$$h^2(F) = \\frac{V_{A}(F)}{V_{P}} = \\frac{2.64}{4.24} = \\frac{66}{106} = \\frac{33}{53}$$\nNow, we substitute these values into the derived expression for the response $R$:\n$$R = \\frac{1}{2} (1 + 0.30) \\left(\\frac{33}{53}\\right) (1.20)\\,\\text{cm}$$\n$$R = \\frac{1}{2} (1.3) \\left(\\frac{33}{53}\\right) (1.2)\\,\\text{cm}$$\n$$R = (0.5 \\times 1.3 \\times 1.2) \\left(\\frac{33}{53}\\right)\\,\\text{cm}$$\n$$R = 0.78 \\times \\frac{33}{53}\\,\\text{cm} = \\frac{25.74}{53}\\,\\text{cm}$$\n$$R \\approx 0.485660377... \\,\\text{cm}$$\nRounding the result to four significant figures as required yields $0.4857\\,\\text{cm}$.", "answer": "$$ \\boxed{0.4857} $$", "id": "2745763"}, {"introduction": "A theoretical prediction is only as good as its ability to match empirical observation. This advanced practice bridges the gap between theory and data by tasking you with developing a framework for the empirical validation of the breeder's equation, both in its univariate and multivariate forms [@problem_id:2745772]. You will use statistical tools like the delta method and Mahalanobis distance to quantify uncertainty and rigorously test whether an observed evolutionary change is consistent with predictions from quantitative genetic theory.", "problem": "You will design and implement a program to check empirical validation of evolutionary response predictions under directional selection using general derivations from foundational principles in quantitative genetics. The aim is to operationalize model checking by comparing predicted responses derived from the additive genetic structure and selection to observed trait changes, while explicitly accounting for statistical uncertainty.\n\nStart from the following fundamental base:\n- The Price equation states that, for a trait with population mean $\\bar{z}$ and individual phenotype $z_i$ and absolute fitness $w_i$, the change in mean trait across one generation satisfies\n$$\n\\Delta \\bar{z} = \\frac{\\operatorname{Cov}(w,z)}{\\bar{w}} + \\frac{1}{\\bar{w}} \\mathbb{E}[w \\Delta z_i],\n$$\nwhere $\\bar{w}$ is mean fitness. Under standard assumptions of additive genetic inheritance with no within-family transmission bias in the trait, the second term is negligible for the additive genetic contribution to the response across a single generation, and the genetic response is driven by the covariance of fitness and breeding value.\n- The phenotypic variance $V_P$ decomposes into additive genetic variance $V_A$ and environmental variance $V_E$ as\n$$\nV_P = V_A + V_E,\n$$\nand the narrow-sense heritability is $h^2 = V_A / V_P$.\n- Let $\\mathbf{G}$ be the additive genetic variance-covariance matrix for a vector of traits, and let the selection gradient $\\boldsymbol{\\beta}$ be defined by the slope of relative fitness with respect to the trait vector under linear directional selection. The response to selection is a function of $\\mathbf{G}$ and $\\boldsymbol{\\beta}$.\n- The selection differential $S$ for a single trait equals the difference between the mean of selected parents and the mean of the entire population.\n\nYour tasks:\n1) From these bases, derive the expression linking the expected one-generation genetic response in the trait mean to the additive genetic variance (or variance-covariance matrix) and the selection gradient. Specialize it to both the univariate and multivariate cases, and relate the selection differential and selection gradient.\n2) Using a first-order delta method, derive an expression for the sampling variance of the predicted response in the univariate case when both the narrow-sense heritability $h^2$ and the selection differential $S$ are estimated with known sampling variances and covariance. Use a Normal approximation for the predictive distribution of the genetic response when applying the model check in univariate cases.\n3) In the multivariate case, assume that the additive genetic variance-covariance matrix $\\mathbf{G}$ is known without error, while the selection gradient $\\boldsymbol{\\beta}$ is estimated with a known sampling covariance matrix $\\mathbf{B}.$ Derive the sampling covariance of the predicted response vector using a first-order approximation. Show how to compute a generalized squared Mahalanobis discrepancy for the observed response relative to the prediction, and determine an acceptance criterion using a Chi-square distribution at confidence level $0.95$. Address how to proceed if the predictive covariance matrix is singular, including both the appropriate statistical degrees of freedom and the necessary condition for consistency in the null space.\n\nAll quantities are standardized to phenotypic standard deviation units of the focal population, so no physical unit is required. Express all numerical outputs as decimals. When any confidence level is used, write it as a decimal (for example, $0.95$).\n\nAlgorithmic requirements for model checking:\n- For each univariate test case, compute the predicted response and its sampling variance via the delta method derived above. Use a two-sided Normal acceptance region with confidence level $0.95$. If the sampling variance is exactly zero, accept only if the observed and predicted responses are equal up to numerical tolerance.\n- For each multivariate test case, compute the predicted response vector and its sampling covariance. Compute the generalized squared Mahalanobis distance using the inverse of the predictive covariance. If the predictive covariance is singular, replace the inverse with the Moore–Penrose pseudoinverse, compute the rank as the number of strictly positive eigenvalues, and test using a Chi-square distribution with degrees of freedom equal to that rank. Additionally, if the discrepancy vector has any nonzero component in the null space of the predictive covariance (beyond numerical tolerance), then reject consistency outright, because the model attributes zero sampling uncertainty to that component.\n\nTest suite:\nImplement the above for the following five cases. All numbers are in standardized trait units. For univariate cases, the acceptance criterion is the two-sided Normal interval at confidence level $0.95$. For multivariate cases, the acceptance criterion is a Chi-square threshold at confidence level $0.95$ with degrees of freedom equal to the rank of the predictive covariance.\n- Case 1 (univariate, typical): $h^2 = 0.5$, $S = 0.2$, $\\operatorname{Var}(\\hat{h}^2) = 0.01$, $\\operatorname{Var}(\\hat{S}) = 0.0025$, $\\operatorname{Cov}(\\hat{h}^2,\\hat{S}) = 0.0$, observed response $R_{\\text{obs}} = 0.11$.\n- Case 2 (univariate, boundary heritability): $h^2 = 0.0$, $S = 0.6$, $\\operatorname{Var}(\\hat{h}^2) = 0.0004$, $\\operatorname{Var}(\\hat{S}) = 0.01$, $\\operatorname{Cov}(\\hat{h}^2,\\hat{S}) = 0.0$, observed response $R_{\\text{obs}} = 0.02$.\n- Case 3 (univariate, model violation via environmental shift): $h^2 = 0.4$, $S = 0.1$, $\\operatorname{Var}(\\hat{h}^2) = 0.0025$, $\\operatorname{Var}(\\hat{S}) = 0.0004$, $\\operatorname{Cov}(\\hat{h}^2,\\hat{S}) = 0.0$, observed response $R_{\\text{obs}} = 0.09$.\n- Case 4 (multivariate, $2$ traits, full-rank predictive covariance): \n  $\\mathbf{G} = \\begin{bmatrix} 0.6  0.2 \\\\ 0.2  0.5 \\end{bmatrix}$,\n  $\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} 0.1 \\\\ -0.05 \\end{bmatrix}$,\n  $\\mathbf{B} = \\begin{bmatrix} 0.0009  0.0002 \\\\ 0.0002  0.0004 \\end{bmatrix}$,\n  observed response $\\mathbf{R}_{\\text{obs}} = \\begin{bmatrix} 0.058 \\\\ -0.012 \\end{bmatrix}$.\n- Case 5 (multivariate, singular predictive covariance and null-space discrepancy should reject):\n  $\\mathbf{G} = \\begin{bmatrix} 0.6  0.2 \\\\ 0.2  0.5 \\end{bmatrix}$,\n  $\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} 0.1 \\\\ -0.05 \\end{bmatrix}$,\n  $\\mathbf{B} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0009 \\end{bmatrix}$,\n  observed response $\\mathbf{R}_{\\text{obs}} = \\mathbf{R}_{\\text{pred}} + 0.01 \\mathbf{v}$ where $\\mathbf{R}_{\\text{pred}}$ is the predicted response for this case and $\\mathbf{v} = [0.5, -0.2]^\\top / \\|[0.5, -0.2]^\\top\\|$, i.e., a pure null-space deviation for the predictive covariance.\n\nProgram requirements:\n- Implement the derivations to compute predicted responses and their uncertainty according to the above assumptions, then perform model checks for the five cases in order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry a boolean indicating whether the observed response is consistent with the prediction under the model at confidence level $0.95$ in the sense defined above. For example, a valid output might look like $[\\text{True},\\text{False},\\dots]$ for five cases.", "solution": "The genetic response to selection can be derived from first principles using the Price equation and the additive genetic model. Consider first a single trait with individual phenotype $z$ and breeding value $a$. Under additive inheritance with no within-family transmission bias across one generation, the change in the mean breeding value equals the covariance of relative fitness and the breeding value. With linear directional selection, relative fitness can be approximated by $1 + \\beta z$ in standardized units, where $\\beta$ is the selection gradient. The covariance term that drives the genetic response is then $\\operatorname{Cov}(w, a)/\\bar{w}$; under weak selection we take $\\bar{w} \\approx 1$ and $\\operatorname{Cov}(w, a) \\approx \\beta\\,\\operatorname{Cov}(z, a).$ Because under additivity $\\operatorname{Cov}(z, a) = V_A$, the expected response in the mean breeding value over one generation is\n$$\nR = V_A \\beta.\n$$\nUsing $V_P = V_A + V_E$ and $h^2 = V_A / V_P$, the selection differential $S$ for a single trait satisfies $S = V_P \\beta$ by the definition of the selection gradient as the slope of relative fitness with respect to the trait. Combining these relations yields\n$$\nR = V_A \\beta = \\frac{V_A}{V_P} V_P \\beta = h^2 S,\n$$\nwhich is the univariate form of the breeder’s equation.\n\nFor a vector of traits $\\mathbf{z}$ with additive genetic variance-covariance matrix $\\mathbf{G}$ under linear directional selection with gradient vector $\\boldsymbol{\\beta}$, the same logic generalizes the response to\n$$\n\\mathbf{R} = \\mathbf{G}\\boldsymbol{\\beta},\n$$\nand the selection differential vector is $\\mathbf{S} = \\mathbf{P}\\boldsymbol{\\beta}$ where $\\mathbf{P}$ is the phenotypic variance-covariance matrix. These are standard consequences of the Price equation under additivity and linear selection.\n\nUnivariate uncertainty via the delta method:\nLet $\\hat{h}^2$ and $\\hat{S}$ be estimators of $h^2$ and $S$ with sampling variances $\\operatorname{Var}(\\hat{h}^2)$ and $\\operatorname{Var}(\\hat{S})$ and covariance $\\operatorname{Cov}(\\hat{h}^2,\\hat{S})$. The predicted response estimator is $\\hat{R} = \\hat{h}^2 \\hat{S}.$ A first-order delta method approximation to its variance is obtained by linearizing $g(h^2, S) = h^2 S$ around the true values. The gradient is\n$$\n\\nabla g = \\begin{bmatrix} \\partial g / \\partial h^2 \\\\ \\partial g / \\partial S \\end{bmatrix} = \\begin{bmatrix} S \\\\ h^2 \\end{bmatrix}.\n$$\nThus the variance is approximated by\n$$\n\\operatorname{Var}(\\hat{R}) \\approx \\begin{bmatrix} S  h^2 \\end{bmatrix}\n\\begin{bmatrix} \\operatorname{Var}(\\hat{h}^2)  \\operatorname{Cov}(\\hat{h}^2,\\hat{S}) \\\\ \\operatorname{Cov}(\\hat{h}^2,\\hat{S})  \\operatorname{Var}(\\hat{S}) \\end{bmatrix}\n\\begin{bmatrix} S \\\\ h^2 \\end{bmatrix}\n= S^2 \\operatorname{Var}(\\hat{h}^2) + (h^2)^2 \\operatorname{Var}(\\hat{S}) + 2 h^2 S \\operatorname{Cov}(\\hat{h}^2,\\hat{S}).\n$$\nUnder a Normal approximation $\\hat{R} \\sim \\mathcal{N}(R, \\operatorname{Var}(\\hat{R}))$, a two-sided acceptance region at confidence level $0.95$ for comparing an observed response $R_{\\text{obs}}$ to the predicted mean $R$ can be implemented as\n$$\n| R_{\\text{obs}} - R | \\le z_{0.975} \\sqrt{\\operatorname{Var}(\\hat{R})},\n$$\nwith $z_{0.975} \\approx 1.96$. If $\\operatorname{Var}(\\hat{R}) = 0$, then the only consistent observation under the model is $R_{\\text{obs}} = R$ up to negligible numerical tolerance.\n\nMultivariate uncertainty and Mahalanobis check:\nAssume $\\mathbf{G}$ is known and the selection gradient is estimated as $\\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\mathbf{B})$. The predicted response estimator is $\\hat{\\mathbf{R}} = \\mathbf{G} \\hat{\\boldsymbol{\\beta}}$, and by linearity its sampling covariance is\n$$\n\\mathbf{\\Sigma}_R = \\operatorname{Cov}(\\hat{\\mathbf{R}}) = \\mathbf{G} \\mathbf{B} \\mathbf{G}^\\top.\n$$\nGiven an observed response vector $\\mathbf{R}_{\\text{obs}}$ and predicted mean $\\mathbf{R} = \\mathbf{G}\\boldsymbol{\\beta}$, define the discrepancy $\\boldsymbol{\\delta} = \\mathbf{R}_{\\text{obs}} - \\mathbf{R}.$ When $\\mathbf{\\Sigma}_R$ is full rank, the squared Mahalanobis discrepancy\n$$\nD^2 = \\boldsymbol{\\delta}^\\top \\mathbf{\\Sigma}_R^{-1} \\boldsymbol{\\delta}\n$$\nshould be compared to the $0.95$ quantile of the Chi-square distribution with degrees of freedom $k$ equal to the number of traits. Consistency is accepted if $D^2 \\le \\chi^2_{0.95;k}.$\n\nIf $\\mathbf{\\Sigma}_R$ is singular, write its eigendecomposition $\\mathbf{\\Sigma}_R = \\mathbf{Q}\\,\\operatorname{diag}(\\lambda_1,\\dots,\\lambda_k)\\,\\mathbf{Q}^\\top$ with eigenvalues $\\lambda_i \\ge 0$. Let $\\mathcal{S}$ be the span of eigenvectors with strictly positive eigenvalues, of dimension $r = \\operatorname{rank}(\\mathbf{\\Sigma}_R).$ The model implies zero sampling uncertainty outside $\\mathcal{S}$. Therefore a necessary condition for consistency is that the projection of $\\boldsymbol{\\delta}$ onto the orthogonal complement $\\mathcal{S}^\\perp$ is zero (within numerical tolerance). If this condition fails, reject consistency. If it holds, compute\n$$\nD^2 = \\sum_{i:\\,\\lambda_i0} \\frac{(\\mathbf{q}_i^\\top \\boldsymbol{\\delta})^2}{\\lambda_i},\n$$\nand compare to $\\chi^2_{0.95;r}.$ This is equivalent to using the Moore–Penrose pseudoinverse $\\mathbf{\\Sigma}_R^+$ and the appropriate degrees of freedom.\n\nWe now apply these principles to the test suite.\n\nCase 1 (univariate):\nParameters are $h^2 = 0.5$, $S = 0.2$, $\\operatorname{Var}(\\hat{h}^2) = 0.01$, $\\operatorname{Var}(\\hat{S}) = 0.0025$, $\\operatorname{Cov} = 0.0$, $R_{\\text{obs}} = 0.11$.\nPrediction $R = h^2 S = 0.5 \\times 0.2 = 0.1$.\nVariance $\\operatorname{Var}(\\hat{R}) = 0.2^2 \\cdot 0.01 + 0.5^2 \\cdot 0.0025 + 2 \\cdot 0.5 \\cdot 0.2 \\cdot 0 = 0.001025$.\nStandard error $\\sqrt{0.001025} \\approx 0.032016$, margin $1.96 \\times 0.032016 \\approx 0.06275$.\nAbsolute discrepancy $|0.11 - 0.1| = 0.01$ is within the margin, so accept.\n\nCase 2 (univariate boundary):\n$h^2 = 0.0$, $S = 0.6$, $\\operatorname{Var}(\\hat{h}^2) = 0.0004$, $\\operatorname{Var}(\\hat{S}) = 0.01$, $\\operatorname{Cov} = 0$, $R_{\\text{obs}} = 0.02$.\nPrediction $R = 0.0$.\nVariance $\\operatorname{Var}(\\hat{R}) = 0.6^2 \\cdot 0.0004 + 0^2 \\cdot 0.01 + 0 = 0.000144$.\nStandard error $\\sqrt{0.000144} = 0.012$, margin $1.96 \\times 0.012 \\approx 0.02352$.\nAbsolute discrepancy $|0.02 - 0.0| = 0.02$ is within the margin, so accept.\n\nCase 3 (univariate violation):\n$h^2 = 0.4$, $S = 0.1$, $\\operatorname{Var}(\\hat{h}^2) = 0.0025$, $\\operatorname{Var}(\\hat{S}) = 0.0004$, $\\operatorname{Cov} = 0$, $R_{\\text{obs}} = 0.09$.\nPrediction $R = 0.04$.\nVariance $\\operatorname{Var}(\\hat{R}) = 0.1^2 \\cdot 0.0025 + 0.4^2 \\cdot 0.0004 = 0.000025 + 0.000064 = 0.000089$.\nStandard error $\\sqrt{0.000089} \\approx 0.009433$, margin $1.96 \\times 0.009433 \\approx 0.01849$.\nAbsolute discrepancy $|0.09 - 0.04| = 0.05$ exceeds the margin, so reject.\n\nCase 4 (multivariate full rank):\n$\\mathbf{G} = \\begin{bmatrix} 0.6  0.2 \\\\ 0.2  0.5 \\end{bmatrix}$, $\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} 0.1 \\\\ -0.05 \\end{bmatrix}$, $\\mathbf{B} = \\begin{bmatrix} 0.0009  0.0002 \\\\ 0.0002  0.0004 \\end{bmatrix}$, $\\mathbf{R}_{\\text{obs}} = \\begin{bmatrix} 0.058 \\\\ -0.012 \\end{bmatrix}$.\nPrediction $\\mathbf{R} = \\mathbf{G}\\,\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} 0.05 \\\\ -0.005 \\end{bmatrix}$.\nPredictive covariance $\\mathbf{\\Sigma}_R = \\mathbf{G}\\,\\mathbf{B}\\,\\mathbf{G}^\\top$ equals numerically $\\begin{bmatrix} 0.000388  0.000216 \\\\ 0.000216  0.000176 \\end{bmatrix}$.\nDiscrepancy $\\boldsymbol{\\delta} = \\begin{bmatrix} 0.008 \\\\ -0.007 \\end{bmatrix}$.\nThe squared Mahalanobis distance $D^2 \\approx 2.52$ and $\\chi^2_{0.95;2} \\approx 5.991$, so accept.\n\nCase 5 (multivariate singular):\n$\\mathbf{G} = \\begin{bmatrix} 0.6  0.2 \\\\ 0.2  0.5 \\end{bmatrix}$, $\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} 0.1 \\\\ -0.05 \\end{bmatrix}$, $\\mathbf{B} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0009 \\end{bmatrix}$.\nPrediction $\\mathbf{R} = \\begin{bmatrix} 0.05 \\\\ -0.005 \\end{bmatrix}$ as before. The predictive covariance is $\\mathbf{\\Sigma}_R = \\mathbf{G}\\,\\mathbf{B}\\,\\mathbf{G}^\\top = 0.0009 \\,\\mathbf{g}\\,\\mathbf{g}^\\top$ with $\\mathbf{g} = \\begin{bmatrix} 0.2 \\\\ 0.5 \\end{bmatrix}$, hence rank $1$. The observed response is specified as $\\mathbf{R}_{\\text{obs}} = \\mathbf{R} + 0.01 \\mathbf{v}$ with $\\mathbf{v} = [0.5, -0.2]^\\top / \\|[0.5, -0.2]^\\top\\|$. Because $\\mathbf{v}^\\top \\mathbf{g} = 0$, this discrepancy lies entirely in the null space of $\\mathbf{\\Sigma}_R$, violating the necessary condition for consistency. Therefore reject.\n\nCollecting the boolean outcomes in order yields $[\\text{True},\\text{True},\\text{False},\\text{True},\\text{False}]$.\n\nThe program implements these computations exactly, using a Normal acceptance region for univariate cases and a Chi-square acceptance region for multivariate cases with appropriate handling of singular predictive covariance via eigendecomposition, pseudoinverse, rank determination, and null-space consistency checking. The output is a single line with the list of five booleans in the specified order and format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef univariate_check(h2, S, var_h2, var_S, cov_h2_S, R_obs, z975=1.96, tol=1e-12):\n    \"\"\"\n    Compute predicted response R = h2 * S and its delta-method variance,\n    then perform a two-sided Normal 0.95 acceptance check for R_obs.\n    \"\"\"\n    R_pred = h2 * S\n    var_R = (S**2) * var_h2 + (h2**2) * var_S + 2.0 * h2 * S * cov_h2_S\n    if var_R  0:\n        # Numerical guard: variance cannot be negative\n        var_R = 0.0\n    if var_R == 0.0:\n        return abs(R_obs - R_pred) = tol\n    se = np.sqrt(var_R)\n    margin = z975 * se\n    return abs(R_obs - R_pred) = margin\n\ndef multivariate_check(G, beta, Bcov, R_obs, chi2_quantiles={1: 3.841458820694124, 2: 5.991464547107979}, tol_eig=1e-12, tol_null=1e-12):\n    \"\"\"\n    Compute predicted response R = G @ beta and predictive covariance Sigma = G @ Bcov @ G.T.\n    Perform model check using squared Mahalanobis distance and chi-square threshold at 0.95 level.\n    Handle singular Sigma using eigendecomposition:\n      - Reject if discrepancy has any component in the null space beyond tol_null.\n      - Otherwise compute D^2 on the support and compare to chi-square with df = rank(Sigma).\n    \"\"\"\n    G = np.asarray(G, dtype=float)\n    beta = np.asarray(beta, dtype=float).reshape(-1)\n    Bcov = np.asarray(Bcov, dtype=float)\n    R_pred = G @ beta\n    Sigma = G @ Bcov @ G.T\n    delta = np.asarray(R_obs, dtype=float).reshape(-1) - R_pred\n\n    # Eigendecomposition for rank and handling singularity\n    # Use symmetric eigendecomposition\n    w, Q = np.linalg.eigh(Sigma)\n    # Identify positive eigenvalues\n    pos_idx = w  tol_eig\n    lam_pos = w[pos_idx]\n    Q_pos = Q[:, pos_idx]\n    rank = lam_pos.size\n\n    if rank == 0:\n        # No predictive uncertainty at all; accept only if exact match within tolerance\n        return np.linalg.norm(delta) = tol_null\n\n    # Project discrepancy onto support and null space\n    delta_support = Q_pos @ (Q_pos.T @ delta)\n    delta_null = delta - delta_support\n\n    # If there is any component in null space, reject\n    if np.linalg.norm(delta_null)  tol_null:\n        return False\n\n    # Compute D^2 on the support\n    coeffs = Q_pos.T @ delta  # coordinates in eigenbasis on support\n    D2 = np.sum((coeffs**2) / lam_pos)\n\n    # Chi-square threshold at 0.95 for df = rank\n    if rank not in chi2_quantiles:\n        # For ranks beyond provided keys, approximate via Wilson-Hilferty or add more keys as needed.\n        # Here, we conservatively compute using numpy percentile on simulated chi2 if needed,\n        # but to keep deterministic without RNG, fallback to SciPy formula is not allowed; so raise.\n        # Given our test suite uses only ranks 1 or 2, this branch will not be used.\n        raise ValueError(\"Chi-square quantile for df={} not provided.\".format(rank))\n    threshold = chi2_quantiles[rank]\n    return D2 = threshold\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1: univariate typical\n    case1 = (\"uni\", {\n        \"h2\": 0.5,\n        \"S\": 0.2,\n        \"var_h2\": 0.01,\n        \"var_S\": 0.0025,\n        \"cov_h2_S\": 0.0,\n        \"R_obs\": 0.11\n    })\n\n    # Case 2: univariate boundary heritability\n    case2 = (\"uni\", {\n        \"h2\": 0.0,\n        \"S\": 0.6,\n        \"var_h2\": 0.0004,\n        \"var_S\": 0.01,\n        \"cov_h2_S\": 0.0,\n        \"R_obs\": 0.02\n    })\n\n    # Case 3: univariate model violation via environmental shift\n    case3 = (\"uni\", {\n        \"h2\": 0.4,\n        \"S\": 0.1,\n        \"var_h2\": 0.0025,\n        \"var_S\": 0.0004,\n        \"cov_h2_S\": 0.0,\n        \"R_obs\": 0.09\n    })\n\n    # Case 4: multivariate 2 traits, full-rank predictive covariance\n    G4 = np.array([[0.6, 0.2],\n                   [0.2, 0.5]], dtype=float)\n    beta4 = np.array([0.1, -0.05], dtype=float)\n    B4 = np.array([[0.0009, 0.0002],\n                   [0.0002, 0.0004]], dtype=float)\n    # Predicted R for case 4 (not needed to be precomputed, but for constructing R_obs we follow problem)\n    R4_pred = G4 @ beta4\n    R4_obs = np.array([0.058, -0.012], dtype=float)\n    case4 = (\"multi\", {\n        \"G\": G4,\n        \"beta\": beta4,\n        \"Bcov\": B4,\n        \"R_obs\": R4_obs\n    })\n\n    # Case 5: multivariate singular predictive covariance, null-space discrepancy\n    G5 = np.array([[0.6, 0.2],\n                   [0.2, 0.5]], dtype=float)\n    beta5 = np.array([0.1, -0.05], dtype=float)\n    B5 = np.array([[0.0, 0.0],\n                   [0.0, 0.0009]], dtype=float)\n    R5_pred = G5 @ beta5\n    v = np.array([0.5, -0.2], dtype=float)\n    v = v / np.linalg.norm(v)\n    R5_obs = R5_pred + 0.01 * v\n    case5 = (\"multi\", {\n        \"G\": G5,\n        \"beta\": beta5,\n        \"Bcov\": B5,\n        \"R_obs\": R5_obs\n    })\n\n    test_cases = [case1, case2, case3, case4, case5]\n\n    results = []\n    for kind, params in test_cases:\n        if kind == \"uni\":\n            res = univariate_check(\n                h2=params[\"h2\"],\n                S=params[\"S\"],\n                var_h2=params[\"var_h2\"],\n                var_S=params[\"var_S\"],\n                cov_h2_S=params[\"cov_h2_S\"],\n                R_obs=params[\"R_obs\"]\n            )\n            results.append(res)\n        elif kind == \"multi\":\n            res = multivariate_check(\n                G=params[\"G\"],\n                beta=params[\"beta\"],\n                Bcov=params[\"Bcov\"],\n                R_obs=params[\"R_obs\"]\n            )\n            results.append(res)\n        else:\n            # Unknown case type; mark as False\n            results.append(False)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2745772"}]}