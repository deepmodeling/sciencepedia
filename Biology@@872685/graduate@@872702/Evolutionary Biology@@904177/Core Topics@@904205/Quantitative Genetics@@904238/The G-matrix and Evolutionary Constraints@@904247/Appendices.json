{"hands_on_practices": [{"introduction": "The evolutionary response to selection is rarely a simple march in the direction of optimality. The existing pattern of genetic variation, encapsulated in the G-matrix, can channel evolution along certain paths more easily than others. This exercise [@problem_id:2758130] explores this fundamental principle by asking you to find the \"line of least resistance\"—the direction in trait space with the greatest potential for evolutionary change. By solving it, you will connect the mathematical concept of a matrix's largest eigenvalue, $\\lambda_{\\text{max}}$, to the concrete biological phenomenon of maximal evolvability.", "problem": "An organism expresses $3$ quantitative traits. The additive genetic variance-covariance matrix (G-matrix) has been estimated as\n$$\nG \\;=\\; \\begin{pmatrix}\n2  1  1 \\\\\n1  2  1 \\\\\n1  1  2\n\\end{pmatrix}.\n$$\nAssume a short-term, weak-selection regime where the per-generation change in the mean trait vector is given by a linear response to directional selection mediated by the additive genetic variance-covariance structure. The phenotypic selection gradient is constrained to have fixed Euclidean norm $b>0$, but its direction $\\hat{s}$ can be freely chosen subject to $\\|\\hat{s}\\|=1$.\n\nDefine the directional response per unit selection strength along $\\hat{s}$ in one generation as the scalar projection of the response onto $\\hat{s}$ divided by $b$. For the given $G$, determine the maximum possible value of this directional response per unit selection strength over all unit directions $\\hat{s}$.\n\nExpress your final answer as an exact real number. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- An organism expresses $3$ quantitative traits.\n- The additive genetic variance-covariance matrix (G-matrix) is $G = \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix}$.\n- The selection regime is short-term and weak.\n- The per-generation change in the mean trait vector is $\\Delta \\bar{z} = G\\beta$.\n- The phenotypic selection gradient is $\\beta$, constrained to have fixed Euclidean norm $b0$ and a freely chosen direction $\\hat{s}$ such that $\\|\\hat{s}\\|=1$.\n- The quantity to be maximized is the \"directional response per unit selection strength along $\\hat{s}$ in one generation\", defined as the scalar projection of the response onto $\\hat{s}$ divided by $b$.\n\nValidation verdict:\nThe problem is scientifically grounded. The governing equation, $\\Delta \\bar{z} = G \\beta$, is the standard multivariate breeder's equation, a fundamental concept in evolutionary quantitative genetics. The given $G$ matrix is symmetric. To be a valid covariance matrix, it must also be positive definite. The eigenvalues of $G$, which we will compute, are $4$, $1$, and $1$. As all are positive, $G$ is positive definite. The problem is well-posed, as it asks for the maximum of a continuous function (a quadratic form) over a compact set (the unit sphere), for which a maximum is guaranteed to exist. The problem is objective and contains all necessary information for a unique solution. The problem is declared valid.\n\nSolution:\nThe change in the mean trait vector, $\\Delta \\bar{z}$, under directional selection is given by the multivariate breeder's equation:\n$$ \\Delta \\bar{z} = G \\beta $$\nThe selection gradient, $\\beta$, is specified as having a magnitude $b  0$ and a direction given by the unit vector $\\hat{s}$:\n$$ \\beta = b \\hat{s} $$\nwhere $\\hat{s}$ is a column vector in $\\mathbb{R}^3$ satisfying $\\|\\hat{s}\\| = \\sqrt{\\hat{s}^T \\hat{s}} = 1$.\n\nSubstituting the expression for $\\beta$ into the breeder's equation gives the response vector:\n$$ \\Delta \\bar{z} = G (b \\hat{s}) = b (G \\hat{s}) $$\nThe problem defines the \"directional response per unit selection strength along $\\hat{s}$\" as the scalar projection of the response vector $\\Delta \\bar{z}$ onto the direction of selection $\\hat{s}$, all divided by the magnitude of selection $b$.\n\nThe scalar projection of a vector $\\vec{u}$ onto a unit vector $\\vec{v}$ is given by their dot product, $\\vec{u} \\cdot \\vec{v}$. In matrix notation, for column vectors $u$ and $v$ where $v$ is a unit vector, this is $v^T u$.\nThus, the scalar projection of $\\Delta \\bar{z}$ onto $\\hat{s}$ is $\\hat{s}^T (\\Delta \\bar{z})$.\nThe quantity to be maximized, let us call it $R(\\hat{s})$, is therefore:\n$$ R(\\hat{s}) = \\frac{\\hat{s}^T (\\Delta \\bar{z})}{b} $$\nSubstituting the expression for $\\Delta \\bar{z}$:\n$$ R(\\hat{s}) = \\frac{\\hat{s}^T (b G \\hat{s})}{b} = \\hat{s}^T G \\hat{s} $$\nThis quantity, $\\hat{s}^T G \\hat{s}$, represents the additive genetic variance in the direction of the vector $\\hat{s}$, also known as the evolvability in direction $\\hat{s}$.\n\nThe problem is now reduced to finding the maximum value of the function $R(\\hat{s}) = \\hat{s}^T G \\hat{s}$ subject to the constraint that $\\hat{s}$ is a unit vector, $\\|\\hat{s}\\| = 1$ (or $\\hat{s}^T \\hat{s} = 1$).\nThe expression $\\frac{\\hat{s}^T G \\hat{s}}{\\hat{s}^T \\hat{s}}$ is the Rayleigh quotient of the matrix $G$. Since $\\hat{s}^T \\hat{s} = 1$, we are maximizing the Rayleigh quotient itself.\nFor any real symmetric matrix, the maximum value of its Rayleigh quotient is its largest eigenvalue, $\\lambda_{\\text{max}}$. This maximum is achieved when $\\hat{s}$ is the unit eigenvector corresponding to $\\lambda_{\\text{max}}$.\n\nThe task is to find the eigenvalues of the matrix $G$:\n$$ G \\;=\\; \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix} $$\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(G - \\lambda I) = 0$, where $I$ is the $3 \\times 3$ identity matrix.\n$$ \\det \\begin{pmatrix} 2-\\lambda  1  1 \\\\ 1  2-\\lambda  1 \\\\ 1  1  2-\\lambda \\end{pmatrix} = 0 $$\nA more efficient method is to recognize the structure of $G$. We can express $G$ as the sum of two simpler matrices:\n$$ G = I + J $$\nwhere $I = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}$ is the identity matrix, and $J = \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}$ is the all-ones matrix.\n\nThe eigenvalues of the rank-$1$ matrix $J$ of size $n \\times n$ are $n$ (with multiplicity $1$) and $0$ (with multiplicity $n-1$). For $n=3$, the eigenvalues of $J$ are $\\{3, 0, 0\\}$.\nLet $v$ be an eigenvector of $J$ with eigenvalue $\\mu_J$. Then $Jv = \\mu_J v$.\nConsider the action of $G$ on this eigenvector $v$:\n$$ Gv = (I+J)v = Iv + Jv = v + \\mu_J v = (1+\\mu_J)v $$\nThis shows that $v$ is also an eigenvector of $G$, with eigenvalue $\\lambda_G = 1 + \\mu_J$.\nTherefore, the eigenvalues of $G$ can be found by adding $1$ to each eigenvalue of $J$.\nThe eigenvalues of $G$ are:\n$$ \\lambda_1 = 1 + 3 = 4 $$\n$$ \\lambda_2 = 1 + 0 = 1 $$\n$$ \\lambda_3 = 1 + 0 = 1 $$\nThe set of eigenvalues for $G$ is $\\{4, 1, 1\\}$.\n\nThe maximum value of the directional response per unit selection strength, $R(\\hat{s}) = \\hat{s}^T G \\hat{s}$, is the largest eigenvalue of $G$.\n$$ \\max_{\\|\\hat{s}\\|=1} R(\\hat{s}) = \\lambda_{\\text{max}}(G) = 4 $$\nThis maximum response occurs when the direction of selection $\\hat{s}$ is aligned with the eigenvector corresponding to the eigenvalue $\\lambda=4$. This eigenvector is proportional to $(1, 1, 1)^T$, representing selection for an equal increase in all three traits.\n\nThe maximum possible value is an exact real number.", "answer": "$$\n\\boxed{4}\n$$", "id": "2758130"}, {"introduction": "While some directions in trait space offer high evolvability, others may offer very little, or even none at all. This exercise [@problem_id:2758135] presents a scenario of absolute constraint, where developmental modularity has resulted in a rank-deficient G-matrix. By calculating the response to selection, $\\Delta \\bar{\\mathbf{z}}$, you will see firsthand how the population's trajectory is confined to a specific genetic subspace, making evolution impossible in any direction outside of it, regardless of the strength or direction of selection.", "problem": "A population expresses $5$ quantitative traits under purely additive genetic control. Developmental modularity implies that genetic variation is confined to two latent modules, represented by column vectors $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$ in trait space, so that the genetic variance-covariance (G) matrix equals\n$$\n\\mathbf{G} \\;=\\; \\mathbf{v}_{1}\\mathbf{v}_{1}^{\\top} \\;+\\; \\mathbf{v}_{2}\\mathbf{v}_{2}^{\\top}.\n$$\nThe two module vectors are\n$$\n\\mathbf{v}_{1} \\;=\\; \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\qquad\n\\mathbf{v}_{2} \\;=\\; \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}.\n$$\nDirectional selection acts on these traits with a constant phenotypic selection gradient\n$$\n\\boldsymbol{\\beta} \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.3 \\\\ 0.0 \\\\ 0.4 \\end{pmatrix}.\n$$\nAssume standard multivariate quantitative genetics conditions for short-term evolutionary response under weak selection and constant additive genetic architecture. Determine the Euclidean norm of the expected one-generation change in the mean trait vector. Round your answer to four significant figures and report it as a pure number with no units.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary data and conditions for a unique solution are provided. The problem is a direct application of the multivariate breeder's equation from quantitative genetics, a cornerstone of evolutionary theory. It is therefore valid for analysis.\n\nThe expected one-generation change in the mean trait vector, denoted as $\\Delta \\bar{\\mathbf{z}}$, is governed by the multivariate breeder's equation:\n$$\n\\Delta \\bar{\\mathbf{z}} = \\mathbf{G}\\boldsymbol{\\beta}\n$$\nwhere $\\mathbf{G}$ is the additive genetic variance-covariance matrix and $\\boldsymbol{\\beta}$ is the selection gradient.\n\nThe problem specifies that the population has $5$ traits, and the $\\mathbf{G}$-matrix is structured by two developmental modules, $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$, such that:\n$$\n\\mathbf{G} = \\mathbf{v}_{1}\\mathbf{v}_{1}^{\\top} + \\mathbf{v}_{2}\\mathbf{v}_{2}^{\\top}\n$$\nThe module vectors and the selection gradient are given as:\n$$\n\\mathbf{v}_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad\n\\mathbf{v}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.3 \\\\ 0.0 \\\\ 0.4 \\end{pmatrix}\n$$\nTo find the evolutionary response $\\Delta \\bar{\\mathbf{z}}$, we substitute the expression for $\\mathbf{G}$ into the breeder's equation:\n$$\n\\Delta \\bar{\\mathbf{z}} = (\\mathbf{v}_{1}\\mathbf{v}_{1}^{\\top} + \\mathbf{v}_{2}\\mathbf{v}_{2}^{\\top})\\boldsymbol{\\beta}\n$$\nBy the associativity of matrix multiplication, this can be rewritten as:\n$$\n\\Delta \\bar{\\mathbf{z}} = \\mathbf{v}_{1}(\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\beta}) + \\mathbf{v}_{2}(\\mathbf{v}_{2}^{\\top}\\boldsymbol{\\beta})\n$$\nThis form is computationally more efficient as it avoids the explicit construction of the $5 \\times 5$ matrix $\\mathbf{G}$. It also reveals that the response to selection is constrained to lie within the subspace spanned by the module vectors $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$.\n\nFirst, we compute the scalar projections of the selection gradient $\\boldsymbol{\\beta}$ onto the module vectors $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$.\n\nThe projection onto $\\mathbf{v}_{1}$ is:\n$$\n\\mathbf{v}_{1}^{\\top}\\boldsymbol{\\beta} = \\begin{pmatrix} 2  0  1  -1  0 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.3 \\\\ 0.0 \\\\ 0.4 \\end{pmatrix} = (2)(0.5) + (0)(-0.2) + (1)(0.3) + (-1)(0.0) + (0)(0.4) = 1.0 + 0 + 0.3 - 0 + 0 = 1.3\n$$\nThe projection onto $\\mathbf{v}_{2}$ is:\n$$\n\\mathbf{v}_{2}^{\\top}\\boldsymbol{\\beta} = \\begin{pmatrix} 0  1  1  0  -1 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.3 \\\\ 0.0 \\\\ 0.4 \\end{pmatrix} = (0)(0.5) + (1)(-0.2) + (1)(0.3) + (0)(0.0) + (-1)(0.4) = 0 - 0.2 + 0.3 - 0 - 0.4 = -0.3\n$$\nNow, we construct the response vector $\\Delta \\bar{\\mathbf{z}}$ as a linear combination of $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$ with these scalar coefficients:\n$$\n\\Delta \\bar{\\mathbf{z}} = \\mathbf{v}_{1}(1.3) + \\mathbf{v}_{2}(-0.3) = 1.3 \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix} - 0.3 \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2.6 \\\\ 0 \\\\ 1.3 \\\\ -1.3 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -0.3 \\\\ -0.3 \\\\ 0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 2.6 \\\\ -0.3 \\\\ 1.0 \\\\ -1.3 \\\\ 0.3 \\end{pmatrix}\n$$\nThe final step is to calculate the Euclidean norm of the response vector $\\Delta \\bar{\\mathbf{z}}$, denoted as $\\|\\Delta \\bar{\\mathbf{z}}\\|$. The norm is the square root of the sum of the squares of its components:\n$$\n\\|\\Delta \\bar{\\mathbf{z}}\\| = \\sqrt{(2.6)^{2} + (-0.3)^{2} + (1.0)^{2} + (-1.3)^{2} + (0.3)^{2}}\n$$\nWe calculate the squares of the components:\n$$\n(2.6)^{2} = 6.76 \\\\\n(-0.3)^{2} = 0.09 \\\\\n(1.0)^{2} = 1.00 \\\\\n(-1.3)^{2} = 1.69 \\\\\n(0.3)^{2} = 0.09\n$$\nSumming these values:\n$$\n\\|\\Delta \\bar{\\mathbf{z}}\\|^{2} = 6.76 + 0.09 + 1.00 + 1.69 + 0.09 = 9.63\n$$\nThe Euclidean norm is therefore:\n$$\n\\|\\Delta \\bar{\\mathbf{z}}\\| = \\sqrt{9.63} \\approx 3.10322416\n$$\nRounding the result to four significant figures, as requested, yields $3.103$.", "answer": "$$\\boxed{3.103}$$", "id": "2758135"}, {"introduction": "To move from theoretical understanding to empirical analysis, we need a robust toolkit for quantifying the properties of the G-matrix and its interaction with selection. This comprehensive computational exercise [@problem_id:2758140] challenges you to build such a toolkit. You will implement a suite of essential metrics, including directional evolvability, the magnitude of the evolutionary response vector $\\Delta z$, and the angle of deflection between the selection gradient $\\beta$ and the response, providing a powerful framework for dissecting real-world evolutionary dynamics.", "problem": "Design and implement a program that, given a symmetric additive genetic variance-covariance matrix $G$ and a directional selection gradient vector $\\beta$, quantifies evolvability and constraint under the multivariate breeder’s framework in evolutionary quantitative genetics. The fundamental base you must use is the well-established Lande multivariate breeder’s equation, which states that the expected per-generation mean trait response is the product of the additive genetic variance-covariance matrix and the selection gradient. Specifically, the starting point is: the expected response in mean traits is the vector $\\Delta z$ obtained from $G$ and $\\beta$, and the additive genetic variance-covariance (abbreviated as $G$) is symmetric and positive semidefinite. You must derive all requested quantities from these foundations and general facts about covariance, linear algebra, and vector geometry, without invoking any pre-specified shortcut formulas.\n\nYour program must compute, for each test case, the following five quantities that quantify evolvability and constraint:\n\n1. The evolvability along the direction of selection, defined conceptually as the additive genetic variance available in the unit direction parallel to $\\beta$. This is the variance of the projection of additive genetic values onto the unit vector parallel to $\\beta$.\n\n2. The conditional evolvability in the direction of selection, defined conceptually as the reciprocal of the curvature obtained when mapping required selection to achieve unit response along the unit direction of $\\beta$ under the additive genetic covariance, equivalently tied to the minimal selection magnitude needed to obtain a unit response along that direction. If $G$ is not invertible, use the Moore–Penrose pseudoinverse so that the quantity is well-defined whenever the direction is not entirely in the null space of $G$.\n\n3. The magnitude of the response vector $\\|\\Delta z\\|_2$ under the Lande multivariate breeder’s equation. This must be computed from $G$ and $\\beta$ alone.\n\n4. The angle, in radians, between the response vector $\\Delta z$ and the selection gradient vector $\\beta$. The angle must be computed using the Euclidean inner product and Euclidean norms.\n\n5. The squared cosine of the alignment between the unit selection direction and the leading eigenvector of $G$ (the eigenvector corresponding to the largest eigenvalue). This quantifies how aligned selection is with the direction of maximum additive genetic variance.\n\nAll computations must be expressed in pure mathematical and algorithmic terms. The angle must be reported in radians. Report all floating-point outputs rounded to $6$ decimal places.\n\nYour program must process the following test suite. Each case provides $G$ and $\\beta$. In each case, $G$ is symmetric and positive semidefinite and $\\beta \\neq 0$.\n\n- Case $1$ (happy path, $3 \\times 3$):\n  $$\n  G = \\begin{bmatrix}\n  0.8  0.2  0.1 \\\\\n  0.2  0.5  0.05 \\\\\n  0.1  0.05  0.3\n  \\end{bmatrix},\\quad\n  \\beta = \\begin{bmatrix} 0.4 \\\\ -0.2 \\\\ 0.1 \\end{bmatrix}.\n  $$\n\n- Case $2$ (near-singular axis, $3 \\times 3$):\n  $$\n  G = \\begin{bmatrix}\n  10^{-6}  0  0 \\\\\n  0  0.4  0.2 \\\\\n  0  0.2  0.2\n  \\end{bmatrix},\\quad\n  \\beta = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n  $$\n\n- Case $3$ (diagonal structure, $2 \\times 2$):\n  $$\n  G = \\begin{bmatrix}\n  0.5  0 \\\\\n  0  0.1\n  \\end{bmatrix},\\quad\n  \\beta = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n  $$\n\n- Case $4$ (strong covariance with a dominant subspace, $3 \\times 3$):\n  $$\n  G = \\begin{bmatrix}\n  1.0  0.9  0 \\\\\n  0.9  1.0  0 \\\\\n  0  0  0.1\n  \\end{bmatrix},\\quad\n  \\beta = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n  $$\n\nYour program must output a single line containing a list with one entry per test case, preserving the case order given above. Each entry must itself be a list of five floating-point numbers, in the following order:\n\n$[$ evolvability along selection, conditional evolvability, response magnitude $\\|\\Delta z\\|_2$, angle in radians between $\\Delta z$ and $\\beta$, squared cosine alignment with the leading eigenvector $]$,\n\nall rounded to $6$ decimal places. The final output must be a single line in the exact format (no spaces):\n\n$[ [x_{11},x_{12},x_{_{13}},x_{14},x_{15}], [x_{21},x_{22},x_{23},x_{24},x_{25}], [x_{31},x_{32},x_{33},x_{34},x_{35}], [x_{41},x_{42},x_{43},x_{44},x_{45}] ]$\n\nas a comma-separated list enclosed in square brackets. Replace each $x_{ij}$ with your computed and rounded float for case $i$ and quantity $j$.", "solution": "The problem requires the calculation of five quantitative measures of evolvability and constraint based on the foundational Lande multivariate breeder's equation, $\\Delta z = G \\beta$. Here, $\\Delta z$ is the vector of evolutionary responses in mean trait values, $G$ is the symmetric, positive semidefinite additive genetic variance-covariance matrix, and $\\beta$ is the vector of directional selection gradients. The derivation of each measure must be based on first principles of linear algebra and statistics, as applied to evolutionary theory.\n\nWe are given the matrix $G$ and the vector $\\beta$ for several test cases. All matrices $G$ are stated to be symmetric and positive semidefinite, and $\\beta \\neq 0$.\n\nFirst, let us establish the unit vector in the direction of selection, which is central to several of the required quantities. This vector is denoted by $\\hat{\\beta}$ and is calculated as:\n$$ \\hat{\\beta} = \\frac{\\beta}{\\|\\beta\\|_2} $$\nwhere $\\|\\beta\\|_2 = \\sqrt{\\beta^T \\beta}$ is the Euclidean norm of $\\beta$.\n\nWe will now derive the expressions for each of the five requested quantities.\n\n**1. Evolvability Along the Direction of Selection**\n\nThis quantity is conceptually defined as the additive genetic variance in the direction of selection. Let $a$ be the random vector of additive genetic values for the traits, with covariance matrix $\\text{cov}(a) = G$. The projection of $a$ onto the unit direction of selection $\\hat{\\beta}$ is a scalar random variable, $p_{a}$, given by the dot product:\n$$ p_a = a^T \\hat{\\beta} $$\nThe variance of this projected value is what we seek. Using the standard formula for the variance of a linear transformation of a random vector, $V(C^T X) = C^T \\text{cov}(X) C$, we can substitute $X=a$ and $C=\\hat{\\beta}$:\n$$ V(p_a) = \\hat{\\beta}^T \\text{cov}(a) \\hat{\\beta} = \\hat{\\beta}^T G \\hat{\\beta} $$\nSubstituting the expression for $\\hat{\\beta}$, we obtain the formula for the evolvability along the direction of selection, which we denote as $e(\\beta)$:\n$$ e(\\beta) = \\left( \\frac{\\beta}{\\|\\beta\\|_2} \\right)^T G \\left( \\frac{\\beta}{\\|\\beta\\|_2} \\right) = \\frac{\\beta^T G \\beta}{\\beta^T \\beta} $$\nThis expression is the Rayleigh quotient of the matrix $G$ for the vector $\\beta$.\n\n**2. Conditional Evolvability in the Direction of Selection**\n\nThis quantity is described as being related to the \"minimal selection magnitude needed to obtain a unit response along that direction.\" A standard interpretation in quantitative genetics defines this in terms of the selection cost. To achieve a specific response vector $\\delta$, the required selection gradient is $\\beta_{req} = G^+ \\delta$, where $G^+$ is the Moore-Penrose pseudoinverse of $G$. To achieve a response purely in the direction of $\\hat{\\beta}$ with unit magnitude, we set $\\delta = \\hat{\\beta}$. The \"cost\" of this selection can be measured by the variance in fitness it induces, which is given by $\\beta_{req}^T G \\beta_{req}$.\nLet us calculate this cost:\n$$ \\text{Cost} = (G^+ \\hat{\\beta})^T G (G^+ \\hat{\\beta}) = \\hat{\\beta}^T (G^+)^T G G^+ \\hat{\\beta} $$\nUsing the property $(G^+)^T = (G^T)^+ = G^+$ (since $G$ is symmetric) and $G^+ G G^+ = G^+$, this simplifies to:\n$$ \\text{Cost} = \\hat{\\beta}^T G^+ \\hat{\\beta} $$\nThe conditional evolvability, denoted $c(\\beta)$, is defined as the reciprocal of this cost.\n$$ c(\\beta) = \\frac{1}{\\hat{\\beta}^T G^+ \\hat{\\beta}} = \\frac{1}{\\left( \\frac{\\beta}{\\|\\beta\\|_2} \\right)^T G^+ \\left( \\frac{\\beta}{\\|\\beta\\|_2} \\right)} = \\frac{\\beta^T \\beta}{\\beta^T G^+ \\beta} $$\nThis formulation is robust and well-defined even when $G$ is singular, as per the problem's requirement to use the pseudoinverse.\n\n**3. Magnitude of the Response Vector, $\\|\\Delta z\\|_2$**\n\nThis is a direct application of the Lande equation. First, we compute the response vector $\\Delta z$:\n$$ \\Delta z = G \\beta $$\nThen, we compute its Euclidean norm:\n$$ \\|\\Delta z\\|_2 = \\sqrt{ (\\Delta z)^T (\\Delta z) } = \\sqrt{ (G\\beta)^T (G\\beta) } = \\sqrt{ \\beta^T G^T G \\beta } $$\nSince $G$ is symmetric ($G^T = G$), this is equivalent to $\\sqrt{\\beta^T G^2 \\beta}$. For computation, it is most direct to first calculate the vector $\\Delta z = G\\beta$ and then find its norm.\n\n**4. Angle Between the Response Vector $\\Delta z$ and the Selection Gradient $\\beta$**\n\nThe angle $\\theta$ between two non-zero vectors $u$ and $v$ is given by the formula derived from the Euclidean inner product:\n$$ \\cos(\\theta) = \\frac{u^T v}{\\|u\\|_2 \\|v\\|_2} $$\nIn our case, $u = \\Delta z = G\\beta$ and $v = \\beta$. Therefore, the angle is:\n$$ \\theta = \\arccos\\left( \\frac{(G\\beta)^T \\beta}{\\|G\\beta\\|_2 \\|\\beta\\|_2} \\right) = \\arccos\\left( \\frac{\\beta^T G \\beta}{\\|G\\beta\\|_2 \\|\\beta\\|_2} \\right) $$\nThe value of $\\theta$ quantifies the degree to which the evolutionary response aligns with the direction of selection. A value of $\\theta = 0$ indicates perfect alignment, while $\\theta > 0$ indicates that genetic correlations (off-diagonal elements of $G$) cause the population to evolve in a direction different from the one favored by selection. Since $G$ is positive semidefinite, $\\beta^T G \\beta \\geq 0$, so the angle $\\theta$ will be in the range $[0, \\pi/2]$ radians.\n\n**5. Squared Cosine of the Alignment with the Leading Eigenvector**\n\nThis metric assesses how well the direction of selection aligns with the direction of greatest genetic variance. The eigenvectors of $G$ represent the principal axes of genetic variation, and the corresponding eigenvalues represent the amount of variance along those axes.\nThe steps to compute this are:\n1.  Perform an eigendecomposition of the symmetric matrix $G$ to find its eigenvalues $\\lambda_i$ and corresponding eigenvectors $v_i$.\n2.  Identify the leading eigenvector, $v_{max}$, which is the eigenvector corresponding to the largest eigenvalue, $\\lambda_{max}$. This vector represents the direction of maximum additive genetic variance ($g_{max}$). We assume it is normalized to unit length, i.e., $\\|v_{max}\\|_2 = 1$.\n3.  Normalize the selection gradient vector to get the unit selection direction, $\\hat{\\beta} = \\beta / \\|\\beta\\|_2$.\n4.  The cosine of the angle $\\phi$ between $v_{max}$ and $\\hat{\\beta}$ is their dot product: $\\cos(\\phi) = v_{max}^T \\hat{\\beta}$.\n5.  The requested quantity is the square of this cosine:\n$$ (\\cos(\\phi))^2 = (v_{max}^T \\hat{\\beta})^2 $$\nThis value, ranging from $0$ to $1$, measures the alignment. A value of $1$ signifies that selection acts along the direction of maximum genetic variance, while a value of $0$ signifies that it acts along a direction orthogonal to it.\n\nThese five derived formulas form the basis for the computational implementation. Each will be calculated for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        {\n            \"G\": np.array([\n                [0.8, 0.2, 0.1],\n                [0.2, 0.5, 0.05],\n                [0.1, 0.05, 0.3]\n            ]),\n            \"beta\": np.array([0.4, -0.2, 0.1])\n        },\n        {\n            \"G\": np.array([\n                [1e-6, 0, 0],\n                [0, 0.4, 0.2],\n                [0, 0.2, 0.2]\n            ]),\n            \"beta\": np.array([1, 0, 0])\n        },\n        {\n            \"G\": np.array([\n                [0.5, 0],\n                [0, 0.1]\n            ]),\n            \"beta\": np.array([1, 1])\n        },\n        {\n            \"G\": np.array([\n                [1.0, 0.9, 0],\n                [0.9, 1.0, 0],\n                [0, 0, 0.1]\n            ]),\n            \"beta\": np.array([0, 0, 1])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        G = case[\"G\"]\n        beta = case[\"beta\"]\n        \n        # --- 1. Evolvability along the direction of selection ---\n        # Formula: (beta.T @ G @ beta) / (beta.T @ beta)\n        beta_T_beta = beta.T @ beta\n        beta_T_G_beta = beta.T @ G @ beta\n        evolvability = beta_T_G_beta / beta_T_beta\n\n        # --- 2. Conditional evolvability in the direction of selection ---\n        # Formula: (beta.T @ beta) / (beta.T @ G+ @ beta)\n        G_pinv = np.linalg.pinv(G)\n        beta_T_G_pinv_beta = beta.T @ G_pinv @ beta\n        # Handle the case where beta is in the null space of G+\n        if np.isclose(beta_T_G_pinv_beta, 0):\n            conditional_evolvability = np.inf\n        else:\n            conditional_evolvability = beta_T_beta / beta_T_G_pinv_beta\n\n        # --- 3. Magnitude of the response vector ---\n        # Formula: ||G @ beta||\n        delta_z = G @ beta\n        response_magnitude = np.linalg.norm(delta_z)\n\n        # --- 4. Angle between response vector and selection gradient ---\n        # Formula: arccos((delta_z.T @ beta) / (||delta_z|| * ||beta||))\n        beta_norm = np.linalg.norm(beta)\n        # Note: (G @ beta).T @ beta is the same as beta.T @ G @ beta since G is symmetric\n        cos_theta_numerator = beta_T_G_beta\n        cos_theta_denominator = response_magnitude * beta_norm\n        \n        # Clip to handle potential floating point inaccuracies leading to values just outside [-1, 1]\n        if np.isclose(cos_theta_denominator, 0):\n            # If beta or delta_z is a zero vector, angle is undefined.\n            # Problem statement says beta != 0. If delta_z is 0, angle is 0.\n            angle = 0.0\n        else:\n            cos_theta = np.clip(cos_theta_numerator / cos_theta_denominator, -1.0, 1.0)\n            angle = np.arccos(cos_theta)\n        \n        # --- 5. Squared cosine of alignment with the leading eigenvector ---\n        # Formula: (v_max.T @ beta_hat)^2\n        # Use eigh for symmetric matrices; it returns eigenvalues in ascending order.\n        eigenvalues, eigenvectors = np.linalg.eigh(G)\n        leading_eigenvector = eigenvectors[:, -1]\n        beta_unit = beta / beta_norm\n        dot_product_alignment = leading_eigenvector.T @ beta_unit\n        sq_cos_alignment = dot_product_alignment**2\n\n        # Collect and round results\n        case_results = [\n            evolvability,\n            conditional_evolvability,\n            response_magnitude,\n            angle,\n            sq_cos_alignment\n        ]\n        \n        results.append(case_results)\n\n    # Format the final output string according to the spec (no spaces)\n    output_parts = []\n    for res_list in results:\n        # Round each value to 6 decimal places before converting to string\n        inner_str = f\"[{','.join([f'{round(val, 6):.6f}' for val in res_list])}]\"\n        output_parts.append(inner_str)\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n\n```", "id": "2758140"}]}