{"hands_on_practices": [{"introduction": "To truly understand the $d_N/d_S$ ratio, we must begin with its fundamental calculation from sequence alignments. This exercise guides you through the essential steps of estimating synonymous and nonsynonymous substitution rates from scratch, moving beyond simple difference counting to incorporate a more realistic evolutionary model. By manually applying the Nei–Gojobori (NG86) method for site counting and the Kimura-80 (K80) model to correct for multiple hits and transition/transversion bias, you will gain a deep, mechanical understanding of how raw genetic data is transformed into a meaningful evolutionary metric [@problem_id:2757616].", "problem": "You are given a pairwise codon alignment between two orthologous genes evolved under the Standard Genetic Code. Your task is to estimate the ratio of nonsynonymous to synonymous substitutions per site, $\\omega = d_N/d_S$, using the Nei–Gojobori (1986) framework (NG86) extended with the Kimura (1980) two-parameter model (K80) to correct separately for transition and transversion multiple hits. Specifically, compute the Kimura two-parameter (K80) corrected synonymous distance $d_S$ and nonsynonymous distance $d_N$, and then compute the ratio $\\omega = d_N/d_S$. Use the following assumptions and data:\n\n- Use the Standard Genetic Code for identifying synonymous and nonsynonymous changes.\n- Treat transitions and transversions as defined by purine–purine or pyrimidine–pyrimidine changes (transitions) versus purine–pyrimidine changes (transversions).\n- Compute the number of synonymous sites and nonsynonymous sites per codon following the Nei–Gojobori definition: for each codon position, count the number of single-nucleotide changes out of $3$ that would be synonymous, sum across the three positions to obtain the codon’s synonymous site count, and set the nonsynonymous site count for the codon to $3$ minus that value. Do this separately for each sequence and then take the average across the two sequences to obtain the total synonymous sites $S$ and nonsynonymous sites $N$ for the alignment.\n- Count observed differences between the two sequences codon-by-codon. This alignment is constructed so that each differing codon pair differs at exactly one nucleotide (no codon pair has multiple nucleotide differences), making the classification of each difference unambiguous.\n- For the K80 correction, within the synonymous class use the observed proportion of synonymous transitions $P_S$ and synonymous transversions $Q_S$ (each normalized by $S$) and within the nonsynonymous class use the observed proportion of nonsynonymous transitions $P_N$ and nonsynonymous transversions $Q_N$ (each normalized by $N$). Then compute $$d_S \\text{ from } P_S, Q_S \\quad \\text{and} \\quad d_N \\text{ from } P_N, Q_N \\text{ using K80.}$$\n- Finally compute $\\omega = d_N/d_S$.\n- Round your final answer to $4$ significant figures. Report $\\omega$ as a dimensionless quantity.\n\nThe codon alignment (sequence $1$ versus sequence $2$) of length $10$ codons is:\n\n$1.$ $\\text{GCT}$ vs $\\text{GCC}$\n\n$2.$ $\\text{GGT}$ vs $\\text{GGA}$\n\n$3.$ $\\text{ACT}$ vs $\\text{ATT}$\n\n$4.$ $\\text{CCT}$ vs $\\text{CAT}$\n\n$5.$ $\\text{GTT}$ vs $\\text{GAT}$\n\n$6.$ $\\text{TTT}$ vs $\\text{TTC}$\n\n$7.$ $\\text{TCC}$ vs $\\text{TCT}$\n\n$8.$ $\\text{GCA}$ vs $\\text{GTA}$\n\n$9.$ $\\text{ACC}$ vs $\\text{ACA}$\n\n$10.$ $\\text{GGC}$ vs $\\text{AGC}$\n\nWhat is the value of $\\omega = d_N/d_S$ under this NG86+K80 procedure? Round your answer to $4$ significant figures.", "solution": "The problem is scientifically and mathematically valid. It requires the computation of the ratio of nonsynonymous to synonymous substitution rates, $\\omega = d_N/d_S$, for a given codon alignment using the Nei–Gojobori (1986) method for site counting and the Kimura two-parameter (K80) model for multiple-hit correction.\n\nThe procedure is executed in five steps:\n1.  Calculation of the number of synonymous sites ($S$) and nonsynonymous sites ($N$).\n2.  Classification and enumeration of observed substitution types.\n3.  Calculation of the proportions of transitional and transversional substitutions for both synonymous and nonsynonymous sites.\n4.  Application of the K80 correction to compute the synonymous distance ($d_S$) and nonsynonymous distance ($d_N$).\n5.  Calculation of the final ratio $\\omega = d_N/d_S$.\n\n**Step 1: Calculation of Synonymous ($S$) and Nonsynonymous ($N$) sites**\n\nThe number of synonymous sites ($s$) for a given codon is calculated as $s = \\sum_{i=1}^{3} f_i$, where $f_i$ is the fraction of single-nucleotide changes at position $i$ that are synonymous, based on the Standard Genetic Code. The number of nonsynonymous sites is $n = 3 - s$. Total sites $S$ and $N$ for the alignment are the averages of the site counts from the two sequences. The alignment has a length of $10$ codons, totaling $30$ nucleotide sites.\n\nFor sequence $1$:\n- Codons GCT, GGT, ACT, CCT, GTT, TCC, GCA, ACC, GGC ($9$ codons) each have $s=1$ and $n=2$. These are $4$-fold degenerate at the third position.\n- Codon TTT has $s=1/3$ and $n=8/3$. It is $2$-fold degenerate for Phe (TTC syn, TTA/TTG non-syn).\nTotal synonymous sites for sequence $1$: $S_1 = 9 \\times 1 + 1/3 = 28/3$.\nTotal nonsynonymous sites for sequence $1$: $N_1 = 30 - S_1 = 30 - 28/3 = 62/3$.\n\nFor sequence $2$:\n- Codons GCC, GGA, TCT, GTA, ACA ($5$ codons) each have $s=1$ and $n=2$.\n- Codon ATT has $s=2/3$ and $n=7/3$.\n- Codons CAT, GAT, TTC, AGC ($4$ codons) each have $s=1/3$ and $n=8/3$.\nTotal synonymous sites for sequence $2$: $S_2 = 5 \\times 1 + 1 \\times (2/3) + 4 \\times (1/3) = 5 + 6/3 = 7$.\nTotal nonsynonymous sites for sequence $2$: $N_2 = 30 - S_2 = 30 - 7 = 23$.\n\nThe average number of sites for the alignment are:\n$$ S = \\frac{S_1 + S_2}{2} = \\frac{28/3 + 7}{2} = \\frac{(28+21)/3}{2} = \\frac{49/3}{2} = \\frac{49}{6} $$\n$$ N = \\frac{N_1 + N_2}{2} = \\frac{62/3 + 23}{2} = \\frac{(62+69)/3}{2} = \\frac{131/3}{2} = \\frac{131}{6} $$\nCheck: $S+N = 49/6 + 131/6 = 180/6 = 30$. This is correct.\n\n**Step 2: Classification and Enumeration of Observed Substitutions**\n\nEach of the $10$ codon pairs with differences is classified as a synonymous or nonsynonymous change, and as a transition (Ti) or transversion (Tv).\n1.  GCT(Ala) $\\rightarrow$ GCC(Ala): Synonymous, T$\\rightarrow$C (Ti).\n2.  GGT(Gly) $\\rightarrow$ GGA(Gly): Synonymous, T$\\rightarrow$A (Tv).\n3.  ACT(Thr) $\\rightarrow$ ATT(Ile): Nonsynonymous, C$\\rightarrow$T (Ti).\n4.  CCT(Pro) $\\rightarrow$ CAT(His): Nonsynonymous, C$\\rightarrow$A (Tv).\n5.  GTT(Val) $\\rightarrow$ GAT(Asp): Nonsynonymous, T$\\rightarrow$A (Tv).\n6.  TTT(Phe) $\\rightarrow$ TTC(Phe): Synonymous, T$\\rightarrow$C (Ti).\n7.  TCC(Ser) $\\rightarrow$ TCT(Ser): Synonymous, C$\\rightarrow$T (Ti).\n8.  GCA(Ala) $\\rightarrow$ GTA(Val): Nonsynonymous, C$\\rightarrow$T (Ti).\n9.  ACC(Thr) $\\rightarrow$ ACA(Thr): Synonymous, C$\\rightarrow$A (Tv).\n10. GGC(Gly) $\\rightarrow$ AGC(Ser): Nonsynonymous, G$\\rightarrow$A (Ti).\n\nSummary of observed differences:\n- Number of synonymous transitions ($S_{ti}$): $3$\n- Number of synonymous transversions ($S_{tv}$): $2$\n- Number of nonsynonymous transitions ($N_{ti}$): $3$\n- Number of nonsynonymous transversions ($N_{tv}$): $2$\n\n**Step 3: Calculation of Proportions ($P, Q$)**\n\nThe observed proportions of transitions ($P$) and transversions ($Q$) are calculated for synonymous and nonsynonymous sites separately.\nFor synonymous sites:\n$$ P_S = \\frac{S_{ti}}{S} = \\frac{3}{49/6} = \\frac{18}{49} $$\n$$ Q_S = \\frac{S_{tv}}{S} = \\frac{2}{49/6} = \\frac{12}{49} $$\nFor nonsynonymous sites:\n$$ P_N = \\frac{N_{ti}}{N} = \\frac{3}{131/6} = \\frac{18}{131} $$\n$$ Q_N = \\frac{N_{tv}}{N} = \\frac{2}{131/6} = \\frac{12}{131} $$\n\n**Step 4: K80 Correction for $d_S$ and $d_N$**\n\nThe Kimura two-parameter distance is given by $d = -\\frac{1}{2} \\ln(1 - 2P - Q) - \\frac{1}{4} \\ln(1 - 2Q)$.\n\nFor synonymous distance, $d_S$:\nThe terms inside the logarithms are:\n$$ 1 - 2P_S - Q_S = 1 - 2\\left(\\frac{18}{49}\\right) - \\frac{12}{49} = 1 - \\frac{36}{49} - \\frac{12}{49} = \\frac{49 - 48}{49} = \\frac{1}{49} $$\n$$ 1 - 2Q_S = 1 - 2\\left(\\frac{12}{49}\\right) = 1 - \\frac{24}{49} = \\frac{25}{49} $$\nSo, the synonymous distance is:\n$$ d_S = -\\frac{1}{2} \\ln\\left(\\frac{1}{49}\\right) - \\frac{1}{4} \\ln\\left(\\frac{25}{49}\\right) = \\frac{1}{2} \\ln(49) - \\frac{1}{4} (\\ln(25) - \\ln(49)) $$\n$$ d_S = \\ln(7) - \\frac{1}{4} (2\\ln(5) - 2\\ln(7)) = \\ln(7) - \\frac{1}{2}\\ln(5) + \\frac{1}{2}\\ln(7) = \\frac{3}{2}\\ln(7) - \\frac{1}{2}\\ln(5) $$\nNumerically, $d_S \\approx 2.114146$.\n\nFor nonsynonymous distance, $d_N$:\nThe terms inside the logarithms are:\n$$ 1 - 2P_N - Q_N = 1 - 2\\left(\\frac{18}{131}\\right) - \\frac{12}{131} = 1 - \\frac{36}{131} - \\frac{12}{131} = \\frac{131 - 48}{131} = \\frac{83}{131} $$\n$$ 1 - 2Q_N = 1 - 2\\left(\\frac{12}{131}\\right) = 1 - \\frac{24}{131} = \\frac{107}{131} $$\nSo, the nonsynonymous distance is:\n$$ d_N = -\\frac{1}{2} \\ln\\left(\\frac{83}{131}\\right) - \\frac{1}{4} \\ln\\left(\\frac{107}{131}\\right) $$\n$$ d_N = -\\frac{1}{2}(\\ln(83)-\\ln(131)) - \\frac{1}{4}(\\ln(107)-\\ln(131)) = \\frac{3}{4}\\ln(131) - \\frac{1}{2}\\ln(83) - \\frac{1}{4}\\ln(107) $$\nNumerically, $d_N \\approx 0.278770$.\n\n**Step 5: Calculation of $\\omega = d_N/d_S$**\n\nFinally, the ratio $\\omega$ is computed:\n$$ \\omega = \\frac{d_N}{d_S} = \\frac{\\frac{3}{4}\\ln(131) - \\frac{1}{2}\\ln(83) - \\frac{1}{4}\\ln(107)}{\\frac{3}{2}\\ln(7) - \\frac{1}{2}\\ln(5)} $$\n$$ \\omega \\approx \\frac{0.278770}{2.114146} \\approx 0.13185966 $$\nRounding to $4$ significant figures gives $\\omega = 0.1319$.", "answer": "$$\\boxed{0.1319}$$", "id": "2757616"}, {"introduction": "While manual calculation is instructive, modern evolutionary analyses rely on sophisticated statistical models evaluated by maximum likelihood. This practice transitions from direct calculation to the critical task of model comparison using the Likelihood Ratio Test (LRT), a cornerstone of statistical hypothesis testing in biology. You will work with the outputs of two standard codon models, M7 (allowing only purifying selection) and M8 (adding a class for positive selection), to determine if the evidence for positive selection is statistically significant, paying close attention to the correct calculation of degrees of freedom [@problem_id:2757644].", "problem": "You are given a scenario in which an alignment of coding sequences and a fixed phylogenetic tree are analyzed under two nested codon substitution models used to test for positive selection via the ratio of nonsynonymous to synonymous substitution rates, denoted by $dN/dS$ (commonly written as $\\omega$). Model $M7$ assumes that site-specific $\\omega$ values are drawn from a Beta distribution confined to the interval $(0,1)$, capturing neutral and purifying selection. Model $M8$ augments $M7$ by adding an extra site class that allows $\\omega \\ge 1$, enabling a fraction of sites to experience positive selection. The Likelihood Ratio Test (LRT) compares the maximized likelihoods of these models to assess evidence for positive selection.\n\nFor each test case, you are provided with the maximized log-likelihood under $M7$, denoted $\\ell_{7}$, and under $M8$, denoted $\\ell_{8}$. You are also given the model-specific counts of additional free parameters unique to the site model component: $k_{7}$ for $M7$ and $k_{8}$ for $M8$. Parameters shared identically by both models (such as branch lengths and shared nucleotide or codon exchangeability parameters, if any) are not included in $k_{7}$ or $k_{8}$ and thus cancel in the comparison. In addition, you are given a nonnegative integer $b$ representing the number of boundary constraints effectively active at the $M8$ maximum likelihood estimate, which reduces the effective degrees of freedom when the $M8$ estimate lies on the null boundary (e.g., $\\omega=1$ or a zero mixing proportion). Treat the effective degrees of freedom as the difference in free parameters adjusted by the boundary reduction.\n\nYour tasks for each test case are:\n- Compute the Likelihood Ratio Test statistic using the provided maximized log-likelihoods.\n- Compute the effective degrees of freedom using the provided parameter counts and the boundary reduction.\n- If the computed test statistic is negative due to numerical imprecision, set it to $0.0$.\n- Report the test statistic rounded to exactly $4$ decimal places and the degrees of freedom as an integer.\n\nTest suite:\n1. Case A: $\\ell_{7} = -1234.56$, $\\ell_{8} = -1227.78$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\n2. Case B: $\\ell_{7} = -987.65$, $\\ell_{8} = -987.60$, $k_{7} = 2$, $k_{8} = 4$, $b = 1$.\n3. Case C: $\\ell_{7} = -1500.00$, $\\ell_{8} = -1500.00$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\n4. Case D: $\\ell_{7} = -200.1234$, $\\ell_{8} = -200.1235$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, with no spaces, in the form\n$[[t_{A},df_{A}],[t_{B},df_{B}],[t_{C},df_{C}],[t_{D},df_{D}]]$\nwhere $t_{X}$ is the rounded test statistic for case $X$ and $df_{X}$ is the corresponding integer degrees of freedom. For example, a valid output might look like $[[1.2345,2],[0.0000,1],[0.0000,2],[0.5678,2]]$.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n**Step 1: Extract Givens**\nThe following data and definitions are provided verbatim:\n- Context: An alignment of coding sequences and a phylogenetic tree are analyzed under two nested codon substitution models to test for positive selection via the ratio of nonsynonymous to synonymous substitution rates, denoted $dN/dS$ or $\\omega$.\n- Model $M7$: Assumes that site-specific $\\omega$ values are drawn from a Beta distribution confined to the interval $(0,1)$.\n- Model $M8$: Augments $M7$ by adding an extra site class that allows $\\omega \\ge 1$.\n- $\\ell_{7}$: The maximized log-likelihood under model $M7$.\n- $\\ell_{8}$: The maximized log-likelihood under model $M8$.\n- $k_{7}$: The count of additional free parameters unique to the site model component of $M7$.\n- $k_{8}$: The count of additional free parameters unique to the site model component of $M8$.\n- $b$: A nonnegative integer representing the number of boundary constraints effectively active at the $M8$ maximum likelihood estimate.\n- Test Cases:\n  1. Case A: $\\ell_{7} = -1234.56$, $\\ell_{8} = -1227.78$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\n  2. Case B: $\\ell_{7} = -987.65$, $\\ell_{8} = -987.60$, $k_{7} = 2$, $k_{8} = 4$, $b = 1$.\n  3. Case C: $\\ell_{7} = -1500.00$, $\\ell_{8} = -1500.00$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\n  4. Case D: $\\ell_{7} = -200.1234$, $\\ell_{8} = -200.1235$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It correctly describes the standard Likelihood Ratio Test (LRT) for comparing nested models in evolutionary biology, specifically the sites models $M7$ and $M8$ implemented in programs like PAML. The premise that model $M7$ (null hypothesis: neutral and purifying selection) is nested within model $M8$ (alternative hypothesis: allows for positive selection) is accurate. The parameters provided ($\\ell_{7}$, $\\ell_{8}$, $k_{7}$, $k_{8}$, and $b$) are precisely the necessary inputs for performing the LRT. The inclusion of the boundary constraint parameter, $b$, correctly addresses a known statistical complexity where the null parameter value lies on the boundary of the parameter space, leading to a chi-bar-square null distribution. The problem is self-contained, free of contradictions, and its terms are defined with sufficient precision for a unique solution.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be constructed.\n\nThe fundamental task is to compute the Likelihood Ratio Test statistic, which we will denote as $t$, and the effective degrees of freedom, $df_{eff}$, for each specified case.\n\nThe core principle is the Likelihood Ratio Test, which is applicable for comparing the goodness-of-fit of two nested statistical models. The null model, $M_{0}$, must be a special case of the alternative (or general) model, $M_{1}$. This is true for models $M7$ and $M8$, where $M7$ serves as the null hypothesis. The test statistic $t$ is defined as twice the difference between the maximized log-likelihood of the alternative model and the null model.\n$$t = 2 \\times (\\ell_{1} - \\ell_{0})$$\nIn the context of this problem, the formula is:\n$$t = 2 \\times (\\ell_{8} - \\ell_{7})$$\nTheoretically, since $M7$ is a constrained version of $M8$, the maximized likelihood for $M8$ cannot be less than that for $M7$. Thus, $\\ell_{8} \\ge \\ell_{7}$. The problem correctly specifies that if numerical imprecision leads to a negative value for $t$, it should be treated as $0$, which reflects no improvement in model fit.\n\nThe degrees of freedom, $df$, for an LRT statistic that follows a chi-square distribution are the difference in the number of free parameters between the two models.\n$$df = k_{1} - k_{0}$$\nFor the given problem, this would initially be $k_{8} - k_{7}$. However, the problem provides a factor, $b$, to account for boundary constraints. Such constraints reduce the effective degrees of freedom. The effective degrees of freedom, $df_{eff}$, are therefore calculated as:\n$$df_{eff} = (k_{8} - k_{7}) - b$$\n\nThese principles are now applied to the test suite.\n\n**Case A:**\nGiven: $\\ell_{7} = -1234.56$, $\\ell_{8} = -1227.78$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\nThe test statistic $t_{A}$ is computed as:\n$$t_{A} = 2 \\times (-1227.78 - (-1234.56)) = 2 \\times (6.78) = 13.56$$\nRounded to $4$ decimal places, $t_{A} = 13.5600$.\nThe degrees of freedom $df_{A}$ are:\n$$df_{A} = (4 - 2) - 0 = 2$$\nResult: $[13.5600, 2]$.\n\n**Case B:**\nGiven: $\\ell_{7} = -987.65$, $\\ell_{8} = -987.60$, $k_{7} = 2$, $k_{8} = 4$, $b = 1$.\nThe test statistic $t_{B}$ is:\n$$t_{B} = 2 \\times (-987.60 - (-987.65)) = 2 \\times (0.05) = 0.1$$\nRounded to $4$ decimal places, $t_{B} = 0.1000$.\nThe degrees of freedom $df_{B}$ are:\n$$df_{B} = (4 - 2) - 1 = 1$$\nResult: $[0.1000, 1]$.\n\n**Case C:**\nGiven: $\\ell_{7} = -1500.00$, $\\ell_{8} = -1500.00$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\nThe test statistic $t_{C}$ is:\n$$t_{C} = 2 \\times (-1500.00 - (-1500.00)) = 2 \\times (0) = 0$$\nThe value is exact, so $t_{C} = 0.0000$.\nThe degrees of freedom $df_{C}$ are:\n$$df_{C} = (4 - 2) - 0 = 2$$\nResult: $[0.0000, 2]$.\n\n**Case D:**\nGiven: $\\ell_{7} = -200.1234$, $\\ell_{8} = -200.1235$, $k_{7} = 2$, $k_{8} = 4$, $b = 0$.\nInitial computation of the test statistic $t_{D}$ yields:\n$$t_{raw} = 2 \\times (-200.1235 - (-200.1234)) = 2 \\times (-0.0001) = -0.0002$$\nAs this value is negative, it must be set to $0.0$ according to the problem's rule.\n$$t_{D} = 0.0$$\nRounded to $4$ decimal places, $t_{D} = 0.0000$.\nThe degrees of freedom $df_{D}$ are:\n$$df_{D} = (4 - 2) - 0 = 2$$\nResult: $[0.0000, 2]$.\n\nThe collective results are compiled into the required final format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# No external libraries are needed for this problem.\n\ndef solve():\n    \"\"\"\n    Calculates the Likelihood Ratio Test statistic and degrees of freedom\n    for comparing nested models of molecular evolution M7 and M8.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (l_7, l_8, k_7, k_8, b)\n    test_cases = [\n        # Case A: l_7, l_8, k_7, k_8, b\n        (-1234.56, -1227.78, 2, 4, 0),\n        # Case B: l_7, l_8, k_7, k_8, b\n        (-987.65, -987.60, 2, 4, 1),\n        # Case C: l_7, l_8, k_7, k_8, b\n        (-1500.00, -1500.00, 2, 4, 0),\n        # Case D: l_7, l_8, k_7, k_8, b\n        (-200.1234, -200.1235, 2, 4, 0),\n    ]\n\n    results_str = []\n    for case in test_cases:\n        l7, l8, k7, k8, b = case\n\n        # Task 1: Compute the Likelihood Ratio Test statistic.\n        # The statistic is 2 * (log-likelihood_alternative - log-likelihood_null).\n        # Model M8 is the alternative, M7 is the null.\n        test_statistic = 2 * (l8 - l7)\n        \n        # Task 3: If the computed test statistic is negative, set it to 0.0.\n        # This handles numerical artifacts where l8 is slightly less than l7.\n        if test_statistic  0:\n            test_statistic = 0.0\n\n        # Task 2: Compute the effective degrees of freedom.\n        # df = (parameters_alternative - parameters_null) - boundary_constraints.\n        degrees_of_freedom = (k8 - k7) - b\n        \n        # Task 4: Report the test statistic rounded to 4 decimal places\n        # and the degrees of freedom as an integer.\n        # Format the result for the current case as a string \"[t,df]\".\n        case_result_str = f\"[{test_statistic:.4f},{degrees_of_freedom}]\"\n        results_str.append(case_result_str)\n\n    # Final print statement in the exact required format: [[t_A,df_A],[t_B,df_B],...]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "2757644"}, {"introduction": "Detecting that a gene has experienced positive selection is a major discovery, but the ultimate goal is often to identify the specific amino acid sites that were the targets. This advanced practice simulates the final step of a branch-site analysis, where you will use the output from a fitted model to pinpoint individual codons under selection. By implementing an empirical Bayes procedure to calculate posterior probabilities and applying a False Discovery Rate (FDR) control method, you will learn the robust statistical techniques required to move from a gene-level conclusion to a precise, site-specific inference [@problem_id:2757614].", "problem": "You are given a fixed phylogeny with a specified foreground branch and a codon alignment that has already been analyzed under a standard branch-site codon model using maximum likelihood. The branch-site model partitions sites into three latent classes: class $0$ (purifying selection with $0  \\omega_0  1$ on all branches), class $1$ (neutral evolution with $\\omega_1 = 1$ on all branches), and class $2$ (positive selection on the foreground branch with $\\omega_2  1$, and either $0  \\omega_0  1$ or $\\omega_1 = 1$ on background branches). The fitted model yields, for each site $i$, the log-likelihood contributions $L_{i,k} = \\log f(\\text{data at site } i \\mid \\text{class } k, \\widehat{\\theta})$ for $k \\in \\{0,1,2\\}$ under the estimated parameter vector $\\widehat{\\theta}$, and a mixture prior over classes $\\pi = (\\pi_0,\\pi_1,\\pi_2)$ with $\\pi_k \\ge 0$ and $\\sum_k \\pi_k = 1$. Your task is to implement the following steps:\n\n1. Use Bayes' theorem to compute the empirical Bayes posterior probability that each site $i$ is under positive selection on the foreground branch, namely $P(C=2 \\mid \\text{data at site } i)$, using the class priors $\\pi_k$ and the per-site, per-class log-likelihoods $L_{i,k}$. For numerical stability, perform the computation in log-space using a stable log-sum-exp computation.\n\n2. Define, for each site $i$, the local false discovery rate as $\\ell_i = P(\\text{null} \\mid \\text{data at site } i) = 1 - P(C=2 \\mid \\text{data at site } i)$, where the null hypothesis is the union of classes $0$ and $1$ (no positive selection on the foreground branch). To control the expected false discovery rate at a target level $\\alpha$, use the Bayesian false discovery rate rule: sort the sites by $\\ell_i$ in ascending order, compute the cumulative means $\\bar{\\ell}_k = \\frac{1}{k} \\sum_{j=1}^k \\ell_{(j)}$ over the first $k$ ordered sites $(j)$, and select the largest $k^\\star$ such that $\\bar{\\ell}_{k^\\star} \\le \\alpha$. Declare the corresponding $k^\\star$ sites as discoveries. If no $\\bar{\\ell}_k$ satisfies the inequality, make zero discoveries.\n\n3. Report, for each test case, the list of posterior probabilities $P(C=2 \\mid \\text{data at site } i)$ across sites (rounded to six decimals) and the list of discovered site indices using zero-based indexing.\n\nFoundational base that you may invoke in your reasoning and implementation:\n- The definition of the ratio of nonsynonymous to synonymous substitution rates ($\\omega = d_N/d_S$) and its interpretation for purifying selection ($\\omega  1$), neutrality ($\\omega = 1$), and positive selection ($\\omega  1$).\n- The likelihood principle and Bayes' theorem for computing posterior probabilities from prior weights and likelihoods.\n- The definition of the false discovery rate as the expected proportion of false discoveries among discoveries.\n\nImplement an algorithm that, given the priors $\\pi_k$, the per-site log-likelihoods $L_{i,k}$, and the target level $\\alpha$, computes the posterior probabilities and applies the Bayesian false discovery rate control as described.\n\nTest Suite:\nProvide a program that solves the three cases below. In each case, you are given the class prior vector $\\pi$, the target Bayesian false discovery rate level $\\alpha$, and the per-site, per-class log-likelihood matrix $L$ (with rows indexing sites and columns indexing classes in the order $k = 0, 1, 2$).\n\n- Case A (happy path, one clear discovery):\n  $$\\pi = [0.7, 0.25, 0.05], \\quad \\alpha = 0.1,$$\n  $$L = \\begin{bmatrix}\n  -5.0  -4.8  0.5 \\\\\n  -2.2  -1.5  -3.0 \\\\\n  -1.0  -1.1  -1.2 \\\\\n  -6.0  -5.8  -3.9 \\\\\n  -0.2  -0.5  -2.5\n  \\end{bmatrix}.$$\n\n- Case B (boundary condition, equal evidence across classes):\n  $$\\pi = [0.49, 0.49, 0.02], \\quad \\alpha = 0.05,$$\n  $$L = \\begin{bmatrix}\n  -2.0  -2.0  -2.0 \\\\\n  -2.0  -2.0  -2.0 \\\\\n  -2.0  -2.0  -2.0 \\\\\n  -2.0  -2.0  -2.0\n  \\end{bmatrix}.$$\n\n- Case C (edge case, one extremely strong discovery, others null or ambiguous):\n  $$\\pi = [0.6, 0.35, 0.05], \\quad \\alpha = 0.1,$$\n  $$L = \\begin{bmatrix}\n  -10.0  -10.0  -0.1 \\\\\n  -0.1  -0.2  -5.0 \\\\\n  -1.0  -0.8  -4.0 \\\\\n  -5.0  -5.0  -5.0 \\\\\n  -3.0  -2.8  -1.9 \\\\\n  -0.5  -0.7  -2.2\n  \\end{bmatrix}.$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list with two elements: the first is the list of posterior probabilities $P(C=2 \\mid \\text{data at site } i)$ per site rounded to six decimals, and the second is the list of discovered site indices (zero-based) under the Bayesian false discovery rate rule at the specified $\\alpha$. For example, your output must have the structure\n$$[\\,[\\text{posteriors\\_A},\\ \\text{discoveries\\_A}],\\ [\\text{posteriors\\_B},\\ \\text{discoveries\\_B}],\\ [\\text{posteriors\\_C},\\ \\text{discoveries\\_C}]\\,].$$", "solution": "The problem requires the implementation of a standard statistical procedure used in computational biology to identify sites in a protein-coding gene that are under positive selection. The procedure consists of two main parts: first, the calculation of posterior probabilities of site classes using an empirical Bayes method, and second, the application of a false discovery rate control procedure to identify a set of sites with strong evidence for positive selection.\n\nThe foundation of the method is Bayes' theorem. For each site $i$ in the alignment, we wish to compute the posterior probability of it belonging to the positive selection class, class $k=2$, given the observed data at that site, $D_i$. Let $C_i$ be the random variable for the class assignment of site $i$. The posterior probability is given by:\n$$ P(C_i=2 \\mid D_i) = \\frac{f(D_i \\mid C_i=2) P(C_i=2)}{\\sum_{j=0}^{2} f(D_i \\mid C_i=j) P(C_i=j)} $$\nThe problem provides the necessary components: the prior probabilities of the classes, $P(C_i=k) = \\pi_k$, and the per-site, per-class log-likelihoods, $L_{i,k} = \\log f(D_i \\mid C_i=k, \\widehat{\\theta})$, where $\\widehat{\\theta}$ represents the maximum likelihood estimates of the model parameters. Substituting these into the formula, we have:\n$$ P(C_i=2 \\mid D_i) = \\frac{\\pi_2 \\exp(L_{i,2})}{\\sum_{j=0}^{2} \\pi_j \\exp(L_{i,j})} $$\nDirect computation of this expression is numerically unstable due to potential underflow or overflow from the exponential function acting on log-likelihood values, which can be large negative numbers. To ensure stability, all calculations must be performed in logarithmic space. Let us define the log of the joint probability of site data and class assignment as $J_{i,k} = \\log(\\pi_k) + L_{i,k}$. The denominator is the marginal likelihood of the data at site $i$, $f(D_i)$, whose logarithm, $M_i$, can be expressed as:\n$$ M_i = \\log \\left( \\sum_{j=0}^{2} \\exp(J_{i,j}) \\right) $$\nThis is a log-sum-exp operation. To compute it robustly, we use the identity $\\log(\\sum_j \\exp(x_j)) = m + \\log(\\sum_j \\exp(x_j - m))$, where $m = \\max_j(x_j)$. For each site $i$, we find $m_i = \\max_{k \\in \\{0,1,2\\}} (J_{i,k})$ and compute the log marginal likelihood as:\n$$ M_i = m_i + \\log \\left( \\sum_{j=0}^{2} \\exp(J_{i,j} - m_i) \\right) $$\nThe posterior probability for site $i$ to be in class $2$ can then be calculated as:\n$$ P(C_i=2 \\mid D_i) = \\exp(\\log P(C_i=2 \\mid D_i)) = \\exp(J_{i,2} - M_i) $$\nThis procedure is applied to every site $i$ to obtain a vector of posterior probabilities.\n\nThe second part of the task is to identify a subset of sites for which the claim of positive selection can be made, while controlling the false discovery rate (FDR). The problem specifies the use of a Bayesian FDR procedure. First, we define the local false discovery rate, $\\ell_i$, for each site $i$ as the posterior probability of the null hypothesis. The null hypothesis is that site $i$ is not under positive selection, meaning it belongs to either class $0$ or class $1$. Therefore:\n$$ \\ell_i = P(C_i \\in \\{0,1\\} \\mid D_i) = P(C_i=0 \\mid D_i) + P(C_i=1 \\mid D_i) = 1 - P(C_i=2 \\mid D_i) $$\nTo control the overall FDR at a target level $\\alpha$, we follow these steps:\n$1$. The $N$ sites in the alignment are sorted according to their local false discovery rates $\\ell_i$ in ascending order. Let the sorted values be $\\ell_{(1)} \\le \\ell_{(2)} \\le \\dots \\le \\ell_{(N)}$, corresponding to site indices $i_{(1)}, i_{(2)}, \\dots, i_{(N)}$.\n$2$. For each rank $k$ from $1$ to $N$, we compute the cumulative mean of the sorted local FDRs:\n$$ \\bar{\\ell}_k = \\frac{1}{k} \\sum_{j=1}^{k} \\ell_{(j)} $$\nThis value, $\\bar{\\ell}_k$, represents the expected proportion of false discoveries if we were to declare the top $k$ sites (those with the smallest $\\ell_i$ values) as being under positive selection.\n$3$. We find the largest integer $k^\\star$ for which the expected FDR does not exceed the target level $\\alpha$, i.e., $\\bar{\\ell}_{k^\\star} \\le \\alpha$.\n$4$. If such a $k^\\star \\ge 1$ exists, the sites corresponding to the first $k^\\star$ sorted local FDRs, $\\{i_{(1)}, i_{(2)}, \\dots, i_{(k^\\star)}\\}$, are declared as discoveries. If no $k$ satisfies the condition, then $k^\\star = 0$, and no discoveries are made.\n\nThe final algorithm proceeds by first computing the posterior probabilities $P(C_i=2 \\mid D_i)$ for all sites, then using these to compute the local FDRs $\\ell_i$. These are then used in the FDR control procedure to find the set of discovered sites. The results, comprising the list of all posterior probabilities and the list of discovered site indices, are then reported for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef analyze_case(pi, L, alpha):\n    \"\"\"\n    Analyzes a single test case for positive selection.\n\n    Args:\n        pi (np.ndarray): Prior probabilities for the site classes.\n        L (np.ndarray): Per-site, per-class log-likelihood matrix.\n        alpha (float): Target Bayesian false discovery rate.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of posterior probabilities P(C=2 | data) for each site,\n                 rounded to six decimals.\n              2. A list of zero-based indices of discovered sites, sorted.\n    \"\"\"\n    # Defensive check for empty inputs\n    if L.shape[0] == 0:\n        return [[], []]\n        \n    # Step 1: Compute posterior probabilities using a stable log-sum-exp\n    log_pi = np.log(pi)\n    \n    # J_ik = log(pi_k) + L_ik (joint log probability)\n    J = L + log_pi  # Broadcasting adds log_pi to each row of L\n    \n    # m_i = max_k(J_ik)\n    m = np.max(J, axis=1, keepdims=True)\n    # M_i = m_i + log(sum_k exp(J_ik - m_i)) (log marginal likelihood)\n    log_marginal_lik = m + np.log(np.sum(np.exp(J - m), axis=1, keepdims=True))\n\n    # log P(C=2 | data_i) = J_i2 - M_i\n    log_post_prob_k2 = J[:, 2] - log_marginal_lik.flatten()\n    post_prob_k2 = np.exp(log_post_prob_k2)\n    \n    posteriors_rounded = np.round(post_prob_k2, 6).tolist()\n    \n    # Step 2: Bayesian False Discovery Rate (FDR) control\n    # l_i = 1 - P(C=2 | data_i) (local false discovery rate)\n    local_fdr = 1 - post_prob_k2\n    \n    # Sort sites by local_fdr in ascending order\n    sorted_indices = np.argsort(local_fdr)\n    sorted_lfdr = local_fdr[sorted_indices]\n    \n    # Compute cumulative means of sorted local FDRs\n    num_sites = len(local_fdr)\n    k_vals = np.arange(1, num_sites + 1)\n    cumulative_means = np.cumsum(sorted_lfdr) / k_vals\n    \n    # Find the largest k such that cumulative_mean[k-1] = alpha\n    valid_k_indices = np.where(cumulative_means = alpha)[0]\n    \n    if len(valid_k_indices) == 0:\n        k_star = 0\n    else:\n        # np.where returns 0-based indices. The k value is index + 1.\n        k_star = valid_k_indices[-1] + 1\n        \n    # Get the indices of discovered sites\n    discovered_site_indices = sorted_indices[:k_star]\n    \n    # Report discovered indices sorted for consistency\n    discoveries = sorted(discovered_site_indices.tolist())\n    \n    return [posteriors_rounded, discoveries]\n\ndef solve():\n    \"\"\"\n    Wrapper function to define test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"pi\": np.array([0.7, 0.25, 0.05]),\n            \"alpha\": 0.1,\n            \"L\": np.array([\n                [-5.0, -4.8, 0.5],\n                [-2.2, -1.5, -3.0],\n                [-1.0, -1.1, -1.2],\n                [-6.0, -5.8, -3.9],\n                [-0.2, -0.5, -2.5]\n            ])\n        },\n        {\n            \"name\": \"Case B\",\n            \"pi\": np.array([0.49, 0.49, 0.02]),\n            \"alpha\": 0.05,\n            \"L\": np.array([\n                [-2.0, -2.0, -2.0],\n                [-2.0, -2.0, -2.0],\n                [-2.0, -2.0, -2.0],\n                [-2.0, -2.0, -2.0]\n            ])\n        },\n        {\n            \"name\": \"Case C\",\n            \"pi\": np.array([0.6, 0.35, 0.05]),\n            \"alpha\": 0.1,\n            \"L\": np.array([\n                [-10.0, -10.0, -0.1],\n                [-0.1, -0.2, -5.0],\n                [-1.0, -0.8, -4.0],\n                [-5.0, -5.0, -5.0],\n                [-3.0, -2.8, -1.9],\n                [-0.5, -0.7, -2.2]\n            ])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = analyze_case(case[\"pi\"], case[\"L\"], case[\"alpha\"])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list is what is needed.\n    # The map(str, ...) converts each result list (e.g., [[...],[...]])\n    # to its string form, and join combines them with commas.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2757614"}]}