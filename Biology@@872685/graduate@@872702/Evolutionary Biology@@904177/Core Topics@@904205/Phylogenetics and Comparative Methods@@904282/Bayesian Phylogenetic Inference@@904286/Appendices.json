{"hands_on_practices": [{"introduction": "The foundation of any phylogenetic likelihood calculation is a model of how sequences evolve along the branches of a tree. This process is typically described using a continuous-time Markov chain (CTMC). This first exercise ([@problem_id:2694200]) will guide you through deriving the transition probabilities for the classic Jukes-Cantor (JC69) model, starting from its basic definition. Mastering this derivation provides a deep understanding of how branch lengths and substitution rates translate into the probabilities that form the basis of the likelihood function.", "problem": "In Bayesian phylogenetic inference, branch likelihoods are computed from nucleotide substitution probabilities along branches. Consider the Jukes–Cantor (JC69) nucleotide substitution model, which assumes a time-homogeneous continuous-time Markov chain (CTMC) on the alphabet $\\{A, C, G, T\\}$ with equal equilibrium frequencies $\\pi_{i} = \\frac{1}{4}$ and equal instantaneous substitution rates between any two distinct nucleotides. Let the CTMC generator (rate) matrix be $Q$, with entries $Q_{ij} = \\frac{\\mu}{3}$ for $i \\neq j$ and $Q_{ii} = -\\mu$, so that the mean instantaneous substitution rate is $\\mu$. The transition probability matrix along a branch of length $t$ is $P(t) = \\exp(Qt)$, and a branch length $t$ is defined by the standard scaling that the expected number of substitutions per site along the branch equals $t$. Under this scaling, the mean substitution rate equals $1$.\n\nStarting only from the CTMC definitions above and the scaling convention that makes the expected substitutions per site equal to the branch length $t$, derive closed-form expressions for the JC69 transition probabilities $P_{ii}(t)$ and $P_{ij}(t)$ for $i \\neq j$, and then compute the numerical value of $P_{ii}(0.1)$. Report the final numerical value of $P_{ii}(0.1)$ rounded to four significant figures.", "solution": "The problem statement is subjected to validation and is found to be scientifically grounded, well-posed, and internally consistent. It describes a standard derivation in molecular evolution. We shall proceed with the solution.\n\nThe problem requires the derivation of the transition probabilities for the Jukes-Cantor (JC69) model of nucleotide substitution, starting from the definition of its generator matrix $Q$. The state space is the set of four nucleotides $\\{A, C, G, T\\}$. The generator matrix $Q$ for a continuous-time Markov chain is given with entries $Q_{ij} = \\frac{\\mu}{3}$ for any two distinct states $i, j$ and $Q_{ii} = -\\mu$, where $\\mu$ is the mean instantaneous substitution rate.\n\nThe model assumes uniform equilibrium frequencies, $\\pi_i = \\frac{1}{4}$ for each nucleotide $i$. The overall mean substitution rate, averaged across all states at equilibrium, is defined as $r = -\\sum_{i} \\pi_i Q_{ii}$. Substituting the given values, we find $r = \\sum_{i} \\pi_i \\mu = \\sum_{i=1}^{4} \\frac{1}{4} \\mu = \\mu$.\n\nThe problem specifies a scaling convention where this mean substitution rate equals $1$. Therefore, we must have $\\mu=1$. This convention also ensures that the branch length $t$ is equal to the expected number of substitutions per site, as the expected number is given by the product of the mean rate and time, which is $r \\times t = 1 \\times t = t$.\n\nWith $\\mu=1$, the $4 \\times 4$ generator matrix $Q$ is:\n$$\nQ = \\begin{pmatrix}\n-1 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n\\frac{1}{3} & -1 & \\frac{1}{3} & \\frac{1}{3} \\\\\n\\frac{1}{3} & \\frac{1}{3} & -1 & \\frac{1}{3} \\\\\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & -1\n\\end{pmatrix}\n$$\nThe transition probability matrix $P(t)$ over a branch of length $t$ is given by the matrix exponential $P(t) = \\exp(Qt)$. To compute this, we can express $Q$ as a linear combination of the identity matrix $I$ and the matrix of all ones, $J$. A direct inspection shows that $Q = \\frac{1}{3}J - \\frac{4}{3}I$.\nLet us verify this decomposition. For an off-diagonal element ($i \\neq j$), the $(i,j)$-th entry is $\\frac{1}{3}J_{ij} - \\frac{4}{3}I_{ij} = \\frac{1}{3}(1) - \\frac{4}{3}(0) = \\frac{1}{3}$. For a diagonal element ($i=j$), the $(i,i)$-th entry is $\\frac{1}{3}J_{ii} - \\frac{4}{3}I_{ii} = \\frac{1}{3}(1) - \\frac{4}{3}(1) = -\\frac{3}{3} = -1$. The decomposition is correct.\n\nSince the matrices $I$ and $J$ commute, we can write $\\exp(Qt) = \\exp((\\frac{1}{3}J - \\frac{4}{3}I)t) = \\exp(-\\frac{4}{3}tI) \\exp(\\frac{t}{3}J)$.\nThe first term is simply $\\exp(-\\frac{4}{3}t)I$.\nFor the second term, we use the Taylor series expansion of the exponential function, $\\exp(X) = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!}$. We require the powers of $J$. For a $4 \\times 4$ matrix $J$, we have the property $J^k = 4^{k-1}J$ for $k \\ge 1$.\n$$\n\\exp\\left(\\frac{t}{3}J\\right) = I + \\sum_{k=1}^{\\infty} \\frac{(\\frac{t}{3}J)^k}{k!} = I + \\sum_{k=1}^{\\infty} \\frac{(\\frac{t}{3})^k J^k}{k!} = I + J \\sum_{k=1}^{\\infty} \\frac{t^k 4^{k-1}}{3^k k!}\n$$\n$$\n= I + \\frac{J}{4} \\sum_{k=1}^{\\infty} \\frac{(4t/3)^k}{k!} = I + \\frac{J}{4} \\left( \\sum_{k=0}^{\\infty} \\frac{(4t/3)^k}{k!} - 1 \\right) = I + \\frac{J}{4} \\left( \\exp\\left(\\frac{4t}{3}\\right) - 1 \\right)\n$$\nNow, we combine the terms to find $P(t)$:\n$$\nP(t) = \\exp\\left(-\\frac{4t}{3}\\right)I \\cdot \\left[ I + \\frac{J}{4}\\left(\\exp\\left(\\frac{4t}{3}\\right) - 1\\right) \\right] = \\exp\\left(-\\frac{4t}{3}\\right)I + \\frac{J}{4}\\left(1 - \\exp\\left(-\\frac{4t}{3}\\right)\\right)\n$$\nFrom this matrix equation, we derive the expressions for the individual transition probabilities.\nThe probability of remaining in the same state, $P_{ii}(t)$, is a diagonal element of $P(t)$.\n$$\nP_{ii}(t) = \\left( \\exp\\left(-\\frac{4t}{3}\\right)I \\right)_{ii} + \\left( \\frac{J}{4}\\left(1 - \\exp\\left(-\\frac{4t}{3}\\right)\\right) \\right)_{ii}\n$$\n$$\nP_{ii}(t) = \\exp\\left(-\\frac{4t}{3}\\right) \\cdot 1 + \\frac{1}{4}\\left(1 - \\exp\\left(-\\frac{4t}{3}\\right)\\right) \\cdot 1 = \\frac{1}{4} + \\frac{3}{4}\\exp\\left(-\\frac{4t}{3}\\right)\n$$\nThe probability of transitioning to a different specific state, $P_{ij}(t)$ for $i \\neq j$, is an off-diagonal element of $P(t)$.\n$$\nP_{ij}(t) = \\left( \\exp\\left(-\\frac{4t}{3}\\right)I \\right)_{ij} + \\left( \\frac{J}{4}\\left(1 - \\exp\\left(-\\frac{4t}{3}\\right)\\right) \\right)_{ij}\n$$\n$$\nP_{ij}(t) = \\exp\\left(-\\frac{4t}{3}\\right) \\cdot 0 + \\frac{1}{4}\\left(1 - \\exp\\left(-\\frac{4t}{3}\\right)\\right) \\cdot 1 = \\frac{1}{4}\\left(1 - \\exp\\left(-\\frac{4t}{3}\\right)\\right)\n$$\nThese are the required closed-form expressions.\n\nFinally, we must compute the numerical value of $P_{ii}(0.1)$, rounded to four significant figures. We substitute $t=0.1$ into our derived formula for $P_{ii}(t)$:\n$$\nP_{ii}(0.1) = \\frac{1}{4} + \\frac{3}{4}\\exp\\left(-\\frac{4 \\times 0.1}{3}\\right) = 0.25 + 0.75\\exp\\left(-\\frac{0.4}{3}\\right) = 0.25 + 0.75\\exp\\left(-\\frac{2}{15}\\right)\n$$\nThe exponent is $-\\frac{2}{15} \\approx -0.13333$.\nCalculating the value:\n$$\nP_{ii}(0.1) \\approx 0.25 + 0.75 \\times (0.8751850) = 0.25 + 0.65638875 = 0.90638875\n$$\nRounding to four significant figures gives $0.9064$.", "answer": "$$\\boxed{0.9064}$$", "id": "2694200"}, {"introduction": "Bayesian inference does not find a single 'best' tree but instead samples from a posterior distribution of trees and parameters. This exploration of a vast and complex 'tree space' is powered by Markov chain Monte Carlo (MCMC) algorithms. The core of MCMC is the decision to move to a new state, which is governed by the Metropolis-Hastings acceptance probability, as you will calculate in this exercise ([@problem_id:2694143]). This calculation is central to understanding how MCMC efficiently navigates the posterior landscape, moving towards regions of higher probability while still being able to escape local peaks.", "problem": "In Bayesian phylogenetic inference, we target the posterior distribution over trees, where the posterior density for a tree state $x$ is denoted by $\\pi(x)$ and is proportional to the product of the likelihood of the sequence data under $x$ and the prior on trees and model parameters. A Metropolis–Hastings move is used to propose a new tree state $y$ from $x$ according to a proposal distribution with transition probability $q(x \\to y)$, and the move is then accepted with a probability designed to leave the target posterior invariant.\n\nSuppose we are performing Markov chain Monte Carlo on the posterior over rooted phylogenetic trees for a clade, and a single-tree move is proposed that does not change the dimensionality of the parameterization (so no Jacobian factor is required). You are given the following unnormalized posterior densities and proposal probabilities for the current state $x$ and the proposed state $y$:\n- $\\pi(x) = \\exp(-100)$,\n- $\\pi(y) = \\exp(-98)$,\n- $q(x \\to y) = 0.3$,\n- $q(y \\to x) = 0.6$.\n\nCompute the Metropolis–Hastings acceptance probability for this move. Express the final answer as a single real number between $0$ and $1$. No rounding is required.", "solution": "The problem asks for the computation of the Metropolis-Hastings acceptance probability for a proposed move in a Markov chain Monte Carlo (MCMC) simulation targeting a posterior distribution over phylogenetic trees.\n\nThe problem is first validated for correctness and solvability.\n\n**Step 1: Extract Givens**\n- Unnormalized posterior density of the current state $x$: $\\pi(x) = \\exp(-100)$.\n- Unnormalized posterior density of the proposed state $y$: $\\pi(y) = \\exp(-98)$.\n- Forward proposal probability from $x$ to $y$: $q(x \\to y) = 0.3$.\n- Reverse proposal probability from $y$ to $x$: $q(y \\to x) = 0.6$.\n- The move does not change parameter dimensionality, so the Jacobian determinant is $1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it describes a standard application of the Metropolis-Hastings algorithm, a cornerstone of MCMC methods used extensively in Bayesian statistics and computational biology. The problem is well-posed, providing all necessary quantities for the calculation. The definitions and values are objective, precise, and consistent. The problem is a straightforward, non-trivial test of understanding of the MCMC acceptance criterion. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be derived.\n\nThe Metropolis-Hastings algorithm ensures that the generated Markov chain has the target posterior distribution $\\pi$ as its stationary distribution. This is achieved by designing an acceptance probability $\\alpha(x \\to y)$ for a proposed move from state $x$ to state $y$. The general formula for this acceptance probability is:\n$$\n\\alpha(x \\to y) = \\min \\left( 1, \\frac{\\pi(y) q(y \\to x)}{\\pi(x) q(x \\to y)} |J| \\right)\n$$\nwhere $\\pi(\\cdot)$ is the target density, $q(\\cdot \\to \\cdot)$ is the proposal density, and $|J|$ is the Jacobian determinant for a transformation between parameter spaces. The problem states that the move does not change the dimensionality of the parameterization, which implies the transformation is within the same space and $|J| = 1$. Thus, the formula simplifies to:\n$$\n\\alpha(x \\to y) = \\min \\left( 1, \\frac{\\pi(y) q(y \\to x)}{\\pi(x) q(x \\to y)} \\right)\n$$\nThe posterior densities $\\pi(x)$ and $\\pi(y)$ need not be normalized, as the normalization constant would cancel out in the ratio.\n\nLet us define the acceptance ratio, $R$, as the second term inside the minimum function:\n$$\nR = \\frac{\\pi(y) q(y \\to x)}{\\pi(x) q(x \\to y)}\n$$\nWe substitute the given values into this expression:\n- $\\pi(x) = \\exp(-100)$\n- $\\pi(y) = \\exp(-98)$\n- $q(x \\to y) = 0.3$\n- $q(y \\to x) = 0.6$\n\nThe calculation proceeds as follows:\n$$\nR = \\frac{\\exp(-98) \\cdot 0.6}{\\exp(-100) \\cdot 0.3}\n$$\nWe can separate the ratio of posterior densities and the ratio of proposal probabilities:\n$$\nR = \\left( \\frac{\\exp(-98)}{\\exp(-100)} \\right) \\cdot \\left( \\frac{0.6}{0.3} \\right)\n$$\nThe ratio of the exponential terms is calculated using the property of exponents $\\frac{\\exp(a)}{\\exp(b)} = \\exp(a-b)$:\n$$\n\\frac{\\exp(-98)}{\\exp(-100)} = \\exp(-98 - (-100)) = \\exp(100 - 98) = \\exp(2)\n$$\nThe ratio of the proposal probabilities is:\n$$\n\\frac{0.6}{0.3} = 2\n$$\nCombining these results, the acceptance ratio $R$ is:\n$$\nR = \\exp(2) \\cdot 2 = 2e^2\n$$\nThe acceptance probability is therefore:\n$$\n\\alpha(x \\to y) = \\min(1, 2e^2)\n$$\nTo evaluate this expression, we must determine if $2e^2$ is greater than or less than $1$. The base of the natural logarithm, $e$, is a transcendental number with an approximate value of $2.71828$. Since $e > 1$, its square $e^2$ is also greater than $1$. Specifically, $e^2 \\approx (2.718)^2 \\approx 7.389$.\nThus, the value of $R$ is approximately $2 \\times 7.389 = 14.778$.\nSince $2e^2 > 1$, the minimum of $1$ and $2e^2$ is $1$.\n$$\n\\alpha(x \\to y) = 1\n$$\nThis means the proposed move from state $x$ to state $y$ will always be accepted. This is expected, as the proposed state $y$ has a higher posterior probability ($\\pi(y) > \\pi(x)$), and the Hastings correction term $\\frac{q(y \\to x)}{q(x \\to y)} = 2$ also favors the move.", "answer": "$$\\boxed{1}$$", "id": "2694143"}, {"introduction": "A powerful feature of the Bayesian framework is its ability to formally compare competing scientific hypotheses, which are often represented as different statistical models. The Bayes factor is the primary tool for this model comparison, quantifying the weight of evidence provided by the data for one model versus another. In this final practice problem ([@problem_id:2694154]), you will compute a Bayes factor from the marginal likelihoods of two competing models. This demonstrates how Bayesian inference provides a principled way to perform model selection in phylogenetics, such as choosing the most appropriate model of nucleotide substitution.", "problem": "In a Bayesian phylogenetic comparison of two substitution models for a deoxyribonucleic acid alignment across multiple taxa, suppose you consider model $M_1$ versus model $M_2$. You have estimated the marginal likelihoods using a valid thermodynamic path sampling method, and obtained the natural logarithms of these marginal likelihoods as $\\ln p(D \\mid M_1) = -3200$ and $\\ln p(D \\mid M_2) = -3210$, where $D$ denotes the observed alignment. Assume that prior model probabilities are proper and strictly positive. \n\nStarting only from Bayes’ theorem and the definition of the marginal likelihood as an integral over model-specific parameters, derive an expression for the log Bayes factor $\\ln \\mathrm{BF}_{12}$ in terms of $\\ln p(D \\mid M_1)$ and $\\ln p(D \\mid M_2)$, where $\\mathrm{BF}_{12}$ compares $M_1$ to $M_2$. Then compute the numerical value of $\\ln \\mathrm{BF}_{12}$ using the values above. \n\nProvide your final answer as a single real number. No rounding is necessary. Use the natural logarithm throughout.", "solution": "The problem is valid as it is scientifically grounded in Bayesian statistics, well-posed, objective, and self-contained. We are asked to derive the expression for the log Bayes factor from fundamental principles and then compute its value.\n\nLet us begin with Bayes' theorem applied to model selection. For a given model $M_i$ from a set of models and observed data $D$, the posterior probability of the model is given by:\n$$\np(M_i \\mid D) = \\frac{p(D \\mid M_i) p(M_i)}{p(D)}\n$$\nHere, $p(M_i \\mid D)$ is the posterior probability of model $M_i$, $p(D \\mid M_i)$ is the marginal likelihood of the data given the model, $p(M_i)$ is the prior probability of the model, and $p(D)$ is the marginal likelihood of the data, which serves as a normalization constant across all considered models.\n\nThe problem requires us to compare two models, $M_1$ and $M_2$. The standard way to do this in a Bayesian context is to compute the posterior odds ratio, which compares the posterior probabilities of the two models:\n$$\n\\frac{p(M_1 \\mid D)}{p(M_2 \\mid D)}\n$$\nSubstituting the expression from Bayes' theorem for each model, we obtain:\n$$\n\\frac{p(M_1 \\mid D)}{p(M_2 \\mid D)} = \\frac{\\frac{p(D \\mid M_1) p(M_1)}{p(D)}}{\\frac{p(D \\mid M_2) p(M_2)}{p(D)}}\n$$\nThe normalization constant $p(D)$ cancels, leading to:\n$$\n\\frac{p(M_1 \\mid D)}{p(M_2 \\mid D)} = \\frac{p(D \\mid M_1)}{p(D \\mid M_2)} \\times \\frac{p(M_1)}{p(M_2)}\n$$\nThis equation shows that the posterior odds ratio is the product of two terms: the Bayes factor and the prior odds ratio. The Bayes factor, denoted $\\mathrm{BF}_{12}$ for the comparison of $M_1$ against $M_2$, is defined as the ratio of the marginal likelihoods:\n$$\n\\mathrm{BF}_{12} = \\frac{p(D \\mid M_1)}{p(D \\mid M_2)}\n$$\nThe problem statement also requires starting from the definition of the marginal likelihood as an integral. The marginal likelihood $p(D \\mid M_i)$ is obtained by integrating the likelihood function $p(D \\mid \\theta_i, M_i)$ over the entire parameter space of the model, weighted by the prior distribution of the parameters $p(\\theta_i \\mid M_i)$:\n$$\np(D \\mid M_i) = \\int p(D \\mid \\theta_i, M_i) p(\\theta_i \\mid M_i) \\, d\\theta_i\n$$\nwhere $\\theta_i$ represents the set of all parameters for model $M_i$. The quantities given in the problem, $\\ln p(D \\mid M_1)$ and $\\ln p(D \\mid M_2)$, are the natural logarithms of these integrals, which are estimated via methods such as thermodynamic path sampling.\n\nOur task is to find an expression for the natural logarithm of the Bayes factor, $\\ln \\mathrm{BF}_{12}$. Starting from the definition of $\\mathrm{BF}_{12}$:\n$$\n\\ln \\mathrm{BF}_{12} = \\ln \\left( \\frac{p(D \\mid M_1)}{p(D \\mid M_2)} \\right)\n$$\nUsing the property of logarithms that $\\ln(a/b) = \\ln(a) - \\ln(b)$, we derive the expression for the log Bayes factor in terms of the log marginal likelihoods:\n$$\n\\ln \\mathrm{BF}_{12} = \\ln p(D \\mid M_1) - \\ln p(D \\mid M_2)\n$$\nThis is the required derived expression. It shows that the log Bayes factor is simply the difference between the log marginal likelihoods of the two competing models.\n\nNow, we compute the numerical value using the givens from the problem statement:\n- The natural logarithm of the marginal likelihood for model $M_1$: $\\ln p(D \\mid M_1) = -3200$.\n- The natural logarithm of the marginal likelihood for model $M_2$: $\\ln p(D \\mid M_2) = -3210$.\n\nSubstituting these values into our derived expression:\n$$\n\\ln \\mathrm{BF}_{12} = (-3200) - (-3210)\n$$\n$$\n\\ln \\mathrm{BF}_{12} = -3200 + 3210\n$$\n$$\n\\ln \\mathrm{BF}_{12} = 10\n$$\nThe resulting log Bayes factor is $10$. This positive value indicates that the data provide evidence in favor of model $M_1$ over model $M_2$.", "answer": "$$\\boxed{10}$$", "id": "2694154"}]}