{"hands_on_practices": [{"introduction": "Bootstrap support values are a cornerstone of phylogenetic analysis, yet it's easy to forget they are statistical estimates derived from a finite number of replicates. This exercise provides a crucial hands-on demonstration of how sampling error can affect our conclusions, especially when the number of bootstrap replicates is low. By calculating the probability of observing a high support value by chance alone, you will gain a more critical perspective on interpreting these numbers and appreciate the importance of using a sufficient number of replicates [@problem_id:2692789].", "problem": "A phylogenetic analyst uses the nonparametric bootstrap to assess clade support. For a focal clade, suppose there is a true inclusion probability $p$ that the clade appears in a single resampled alignment (i.e., the clade is recovered in a bootstrap tree). The analyst conducts $n$ independent bootstrap replicates and classifies the clade as “strongly supported” if the observed bootstrap proportion $\\hat{p}$ satisfies $\\hat{p} \\ge t$, where $t$ is a fixed support threshold used for downstream hypothesis testing about clades. Assume independent and identically distributed resamples so that the indicator of clade recovery in each replicate is a Bernoulli trial with success probability $p$.\n\nConsider the effect of using too few replicates by taking $n = 20$, a threshold $t = 0.95$, and a true inclusion probability $p = 0.90$ (so that, in truth, the clade does not meet the strong-support threshold). Under these assumptions, compute the probability that sampling error alone causes a misclassification of the clade as “strongly supported,” i.e., that the observed $\\hat{p}$ crosses the threshold despite $p < t$.\n\nExpress your final result as a decimal and round your answer to four significant figures.", "solution": "The problem statement has been analyzed and is determined to be valid. It is a well-posed statistical question grounded in the principles of phylogenetic inference and probability theory. It is self-contained, objective, and free from scientific or logical flaws. We shall proceed with the solution.\n\nThe problem requires us to calculate the probability of a specific type of error in phylogenetic bootstrapping analysis. We are given a set of parameters:\n- The number of independent bootstrap replicates, $n = 20$.\n- The true inclusion probability for a focal clade, $p = 0.90$.\n- The threshold for classifying a clade as \"strongly supported,\" $t = 0.95$.\n\nThe recovery of the clade in each of the $n$ bootstrap replicates is modeled as an independent Bernoulli trial with a success probability of $p$. Let $X$ be the random variable representing the total number of times the clade is recovered across the $n$ replicates. The random variable $X$ therefore follows a Binomial distribution with parameters $n$ and $p$, which we denote as $X \\sim \\text{Bin}(n, p)$.\n\nThe observed bootstrap proportion, $\\hat{p}$, is defined as the number of successes $X$ divided by the total number of trials $n$. Thus, $$ \\hat{p} = \\frac{X}{n} $$\n\nA misclassification occurs if the clade is classified as \"strongly supported\" despite its true inclusion probability $p$ being less than the threshold $t$. The condition for this misclassification is $\\hat{p} \\ge t$. We must calculate the probability of this event, i.e., $P(\\hat{p} \\ge t)$.\n\nWe can express this condition in terms of the random variable $X$:\n$$ \\frac{X}{n} \\ge t $$\n$$ X \\ge n \\cdot t $$\nSubstituting the given values $n = 20$ and $t = 0.95$:\n$$ X \\ge 20 \\cdot 0.95 $$\n$$ X \\ge 19 $$\nSince $X$ must be an integer (as it represents a count), this inequality means that a misclassification occurs if $X = 19$ or $X = 20$. We are therefore required to compute $P(X \\ge 19) = P(X=19) + P(X=20)$.\n\nThe probability mass function (PMF) for a Binomial distribution is given by:\n$$ P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nWe will use this function to calculate the probabilities for $k=19$ and $k=20$.\n\nFor $k=19$:\n$$ P(X=19) = \\binom{20}{19} (0.90)^{19} (1-0.90)^{20-19} $$\n$$ P(X=19) = \\frac{20!}{19!1!} (0.90)^{19} (0.10)^1 $$\n$$ P(X=19) = 20 \\cdot (0.90)^{19} \\cdot 0.10 $$\n$$ P(X=19) = 2 \\cdot (0.90)^{19} $$\n\nFor $k=20$:\n$$ P(X=20) = \\binom{20}{20} (0.90)^{20} (1-0.90)^{20-20} $$\n$$ P(X=20) = \\frac{20!}{20!0!} (0.90)^{20} (0.10)^0 $$\n$$ P(X=20) = 1 \\cdot (0.90)^{20} \\cdot 1 $$\n$$ P(X=20) = (0.90)^{20} $$\n\nThe total probability of misclassification is the sum of these two probabilities:\n$$ P(X \\ge 19) = P(X=19) + P(X=20) = 2 \\cdot (0.90)^{19} + (0.90)^{20} $$\nWe can factor this expression for calculation:\n$$ P(X \\ge 19) = (0.90)^{19} \\cdot (2 + 0.90) = 2.9 \\cdot (0.90)^{19} $$\nNow, we compute the numerical value:\n$$ (0.90)^{19} \\approx 0.135085171681387 $$\n$$ P(X=19) \\approx 2 \\cdot 0.135085171681387 = 0.270170343362774 $$\n$$ (0.90)^{20} \\approx 0.1215766545132483 $$\n$$ P(X \\ge 19) \\approx 0.270170343362774 + 0.121576654590569 = 0.391746997953343 $$\n\nThe problem requires the answer to be rounded to four significant figures. The calculated probability is $0.39174699...$. The fourth significant digit is $7$. The following digit is $4$, which is less than $5$, so we round down.\n\nThe probability of misclassification is approximately $0.3917$. This result demonstrates that using an insufficient number of bootstrap replicates can lead to a substantial probability of erroneously concluding that a clade is strongly supported.", "answer": "$$\\boxed{0.3917}$$", "id": "2692789"}, {"introduction": "Recognizing that a bootstrap proportion is a point estimate naturally leads to the next question: how precise is that estimate? This practice moves beyond simple point values by introducing the concept of a confidence interval for the true, long-run bootstrap frequency. You will derive and apply the Wilson score interval, a robust statistical method, to quantify the uncertainty around an observed support value, enabling a more nuanced and statistically sound basis for accepting or questioning a clade [@problem_id:2692757].", "problem": "In phylogenetic inference, nonparametric bootstrap replicates are commonly used to assess the support for a candidate split (clade). Consider a target internal branch in a maximum likelihood (ML) tree. You generated $R = 800$ nonparametric bootstrap replicates, and the split appears in $k = 584$ of them. Assume that each bootstrap replicate can be modeled as an independent Bernoulli trial indicating presence ($1$) or absence ($0$) of the split, with an unknown true split frequency $p$ that is the long-run bootstrap recovery probability for this branch.\n\nStarting from the binomial model and the definition of the score test for a binomial proportion, derive the Wilson score confidence interval for $p$ at $95\\%$ confidence and use it to compute the interval for the given $k$ and $R$. Then, interpret the interval in the context of decision-making under the following conservative policy: declare the branch “moderately supported” only if the lower bound of the $95\\%$ confidence interval (CI) is at least $0.70$. Round both numerical bounds of the interval to four significant figures. Report the two bounds as numerical values without units.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in applied statistics within the field of evolutionary biology. Therefore, it is deemed valid, and a solution will be provided.\n\nThe problem requires the derivation and computation of a $95\\%$ Wilson score confidence interval for a binomial proportion $p$. The number of times a specific split is observed, $k$, in $R$ bootstrap replicates is modeled as a random variable following a binomial distribution, $k \\sim \\text{Bin}(R, p)$, where $p$ is the true underlying probability of observing the split in a single replicate.\n\nThe derivation of the Wilson score interval begins by inverting the score test for the null hypothesis $H_0: p = p_0$. The score test statistic is based on the log-likelihood function for the binomial model:\n$$ \\ell(p; k) = \\ln\\binom{R}{k} + k \\ln(p) + (R-k) \\ln(1-p) $$\nThe score function, $U(p)$, is the first derivative of the log-likelihood with respect to $p$:\n$$ U(p) = \\frac{\\partial \\ell}{\\partial p} = \\frac{k}{p} - \\frac{R-k}{1-p} = \\frac{k(1-p) - (R-k)p}{p(1-p)} = \\frac{k - Rp}{p(1-p)} $$\nThe Fisher information, $I(p)$, is the negative expected value of the second derivative of the log-likelihood:\n$$ I(p) = -E\\left[\\frac{\\partial^2 \\ell}{\\partial p^2}\\right] = -E\\left[-\\frac{k}{p^2} - \\frac{R-k}{(1-p)^2}\\right] = \\frac{E[k]}{p^2} + \\frac{E[R-k]}{(1-p)^2} $$\nSince $E[k] = Rp$, we have:\n$$ I(p) = \\frac{Rp}{p^2} + \\frac{R-Rp}{(1-p)^2} = \\frac{R}{p} + \\frac{R}{1-p} = \\frac{R}{p(1-p)} $$\nThe score test statistic, which is approximately standard normal under $H_0$, is given by:\n$$ Z = \\frac{U(p_0)}{\\sqrt{I(p_0)}} = \\frac{(k - Rp_0) / (p_0(1-p_0))}{\\sqrt{R / (p_0(1-p_0))}} = \\frac{k - Rp_0}{\\sqrt{R p_0(1-p_0)}} $$\nLet $\\hat{p} = k/R$ be the sample proportion. The test statistic can be rewritten as:\n$$ Z = \\frac{R(\\hat{p} - p_0)}{\\sqrt{R p_0(1-p_0)}} = \\frac{\\sqrt{R}(\\hat{p} - p_0)}{\\sqrt{p_0(1-p_0)}} $$\nA $(1-\\alpha)$ confidence interval for $p$ is the set of all values $p_0$ for which we do not reject the null hypothesis $H_0: p = p_0$ at significance level $\\alpha$. This condition is $|Z| \\leq z_{\\alpha/2}$, where $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution. We solve the inequality $Z^2 \\leq z_{\\alpha/2}^2$:\n$$ \\frac{R(\\hat{p}-p)^2}{p(1-p)} \\leq z_{\\alpha/2}^2 $$\nThis inequality must be solved for $p$. Rearranging gives a quadratic inequality:\n$$ R(\\hat{p}^2 - 2\\hat{p}p + p^2) \\leq z_{\\alpha/2}^2 (p - p^2) $$\n$$ (R + z_{\\alpha/2}^2)p^2 - (2R\\hat{p} + z_{\\alpha/2}^2)p + R\\hat{p}^2 \\leq 0 $$\nThe roots of the corresponding quadratic equation $(R + z_{\\alpha/2}^2)p^2 - (2R\\hat{p} + z_{\\alpha/2}^2)p + R\\hat{p}^2 = 0$ provide the bounds of the confidence interval. Using the quadratic formula $p = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$, with $A = R + z_{\\alpha/2}^2$, $B = -(2R\\hat{p} + z_{\\alpha/2}^2)$, and $C = R\\hat{p}^2$, we obtain the interval bounds:\n$$ p = \\frac{2R\\hat{p} + z_{\\alpha/2}^2 \\pm \\sqrt{(2R\\hat{p} + z_{\\alpha/2}^2)^2 - 4(R + z_{\\alpha/2}^2)(R\\hat{p}^2)}}{2(R + z_{\\alpha/2}^2)} $$\nSimplifying the term under the square root yields:\n$$ p = \\frac{2R\\hat{p} + z_{\\alpha/2}^2 \\pm z_{\\alpha/2}\\sqrt{4R\\hat{p}(1-\\hat{p}) + z_{\\alpha/2}^2}}{2(R + z_{\\alpha/2}^2)} $$\nDividing the numerator and denominator by $2R$, we arrive at the standard form for the Wilson score interval:\n$$ p = \\frac{\\hat{p} + \\frac{z_{\\alpha/2}^2}{2R} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{R} + \\frac{z_{\\alpha/2}^2}{4R^2}}}{1 + \\frac{z_{\\alpha/2}^2}{R}} $$\nWe are given $R=800$ bootstrap replicates and $k=584$ successes. The sample proportion is $\\hat{p} = \\frac{584}{800} = 0.73$.\nFor a $95\\%$ confidence interval, $\\alpha = 0.05$, so $\\alpha/2 = 0.025$. The corresponding standard normal quantile is $z_{0.025} \\approx 1.96$. We use this value in our calculations. Then $z_{\\alpha/2}^2 \\approx (1.96)^2 = 3.8416$.\n\nWe substitute these values into the derived formula:\nThe denominator is $1 + \\frac{z_{\\alpha/2}^2}{R} = 1 + \\frac{3.8416}{800} = 1 + 0.004802 = 1.004802$.\nThe first term in the numerator is $\\hat{p} + \\frac{z_{\\alpha/2}^2}{2R} = 0.73 + \\frac{3.8416}{1600} = 0.73 + 0.002401 = 0.732401$.\nThe term under the square root in the numerator is $\\frac{\\hat{p}(1-\\hat{p})}{R} + \\frac{z_{\\alpha/2}^2}{4R^2} = \\frac{0.73 \\times 0.27}{800} + \\frac{3.8416}{4 \\times 800^2} = \\frac{0.1971}{800} + \\frac{3.8416}{2560000} \\approx 0.000246375 + 0.0000015 = 0.000247875$.\nThe second term in the numerator is $z_{\\alpha/2}\\sqrt{0.000247875} \\approx 1.96 \\times 0.01574404 \\approx 0.0308583$.\n\nThe interval bounds are thus:\n$$ p \\approx \\frac{0.732401 \\pm 0.0308583}{1.004802} $$\nThe lower bound is:\n$$ p_L \\approx \\frac{0.732401 - 0.0308583}{1.004802} = \\frac{0.7015427}{1.004802} \\approx 0.698189 $$\nThe upper bound is:\n$$ p_U \\approx \\frac{0.732401 + 0.0308583}{1.004802} = \\frac{0.7632593}{1.004802} \\approx 0.759610 $$\nRounding these bounds to four significant figures gives $p_L \\approx 0.6982$ and $p_U \\approx 0.7596$.\nThe $95\\%$ Wilson score confidence interval for the true split frequency $p$ is approximately $[0.6982, 0.7596]$.\n\nThe problem requires an interpretation of this result based on a conservative policy: a branch is declared \"moderately supported\" only if the lower bound of its $95\\%$ CI is at least $0.70$.\nOur calculated lower bound is $p_L \\approx 0.6982$. We must check if $p_L \\ge 0.70$.\nSince $0.6982  0.70$, the condition is not satisfied.\nTherefore, according to the specified policy, the statistical support for this branch is insufficient to be classified as \"moderately supported\". The true frequency $p$ could plausibly be less than $70\\%$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.6982  0.7596 \\end{pmatrix} } $$", "id": "2692757"}, {"introduction": "In modern phylogenetics, researchers have access to a diverse toolkit of branch support methods, from the standard bootstrap to faster approximations like UFBoot and aLRT. While fast, these methods have different statistical behaviors that must be understood to be used correctly for hypothesis testing. This advanced exercise challenges you to act as a methodologist, deriving and implementing a calibration framework to ensure that different support measures can be compared on an equal footing by controlling the Type I error rate, the probability of falsely supporting a non-existent branch [@problem_id:2692773].", "problem": "You are asked to formalize and implement a benchmarking calibration plan for assessing branch support in phylogenies using three commonly used measures: Shimodaira-Hasegawa-like approximate Likelihood Ratio Test (SH-like aLRT), Ultrafast Bootstrap (UFBoot), and standard nonparametric bootstrap. The primary objective is to calibrate decision thresholds for these support measures so that the probability of falsely supporting a nonexistent branch (Type I error) is controlled at a target level.\n\nAssume the following stylized and scientifically plausible generative model that abstracts common practice in phylogenetic support assessment while remaining mathematically tractable.\n\n1. For a fixed internal branch (split) under the null hypothesis that the branch is absent (that is, the true topology does not contain this split), consider site-wise, independent and identically distributed log-likelihood ratio increments $\\{\\delta_i\\}_{i=1}^{S}$ comparing the two nearest-neighbor resolutions around that internal branch. Under the null, assume $\\mathbb{E}[\\delta_i] = 0$ and $\\mathrm{Var}(\\delta_i) = \\sigma^2 \\in (0,\\infty)$, with $S \\in \\mathbb{N}$ sites. Denote the aggregate test signal by $\\Delta = \\sum_{i=1}^{S} \\delta_i$.\n\n2. For standard nonparametric bootstrap support, define the conditional support $U_{\\mathrm{BS}}$ for the focal resolution as the conditional probability (over bootstrap resampling with replacement of $S$ sites) that the bootstrap sum $\\Delta^\\star$ is positive, given the observed dataset. Under the Central Limit Theorem assumptions and as $S$ grows, the conditional distribution of $\\Delta^\\star$ given the observed data can be approximated by a normal distribution centered at the observed sum with a variance determined by the empirical variance of the $\\delta_i$.\n\n3. For Ultrafast Bootstrap (UFBoot), model its resampling variance relative to standard bootstrap by a positive scaling factor $\\lambda \\in (0,\\infty)$, meaning the conditional distribution of its resampled sum is approximately normal but with variance multiplied by $\\lambda$ compared to standard bootstrap. Define $U_{\\mathrm{UF}}$ similarly to $U_{\\mathrm{BS}}$, but with the variance scaling encoded by $\\lambda$.\n\n4. For SH-like approximate Likelihood Ratio Test (aLRT), assume the local test statistic for the branch is $T = 2 \\Delta \\ell$, twice the difference in maximum log-likelihood between the best and the second-best nearest-neighbor resolutions. Under a regular interior null, $T$ is asymptotically distributed as $\\chi^2$ with $1$ degree of freedom. Under a boundary null (for example, a branch length at the boundary), approximate $T$ as a mixture $q \\cdot \\delta_0 + (1-q)\\cdot \\chi^2_1$ for a fixed $q \\in [0,1)$, where $\\delta_0$ denotes a point mass at $0$. Suppose the method reports a support $U_{\\mathrm{SH}}$ obtained by applying the cumulative distribution function of $\\chi^2_1$ to $T$.\n\n5. Define Type I error control as follows: for a specified target level $\\alpha \\in (0,1)$, choose a threshold $\\tau$ for each support metric $U \\in \\{U_{\\mathrm{BS}}, U_{\\mathrm{UF}}, U_{\\mathrm{SH}}\\}$ such that, under the null data-generating model and over repeated datasets, $\\mathbb{P}(U \\ge \\tau) \\le \\alpha$.\n\nTask. Starting from these definitions and approximations, derive mathematically valid threshold functions that produce thresholds $\\tau_{\\mathrm{BS}}(\\alpha)$, $\\tau_{\\mathrm{UF}}(\\alpha,\\lambda)$, and $\\tau_{\\mathrm{SH}}(\\alpha,q)$ ensuring Type I error is controlled at level $\\alpha$ under the stated null models. Your program must implement the derived formulas and compute the thresholds for the following test suite. All thresholds must be expressed as decimals in $[0,1]$ (not percentages).\n\nTest suite. For each tuple $(\\alpha,\\lambda,q)$ below, compute a list $[\\tau_{\\mathrm{BS}}(\\alpha), \\tau_{\\mathrm{UF}}(\\alpha,\\lambda), \\tau_{\\mathrm{SH}}(\\alpha,q)]$:\n\n- Case A (general, interior null for SH-like aLRT): $(\\alpha,\\lambda,q) = (0.05, 1.0, 0.0)$.\n- Case B (reduced resampling variance for UFBoot): $(\\alpha,\\lambda,q) = (0.05, 0.6, 0.0)$.\n- Case C (boundary null for SH-like aLRT and increased variance for UFBoot): $(\\alpha,\\lambda,q) = (0.05, 1.4, 0.5)$.\n- Case D (small Type I error target): $(\\alpha,\\lambda,q) = (0.01, 0.6, 0.0)$.\n- Case E (intermediate boundary mixture): $(\\alpha,\\lambda,q) = (0.10, 0.8, 0.2)$.\n- Case F (coarse control with strong variance inflation and boundary null): $(\\alpha,\\lambda,q) = (0.25, 2.0, 0.5)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of three decimal numbers for one test case, in the same order as above. For example: \"[[x11,x12,x13],[x21,x22,x23],...]\" where each $x_{ij}$ is a float. For determinism, round each threshold to exactly six digits after the decimal point.\n\nConstraints and notes.\n\n- You must not return or print percentages; thresholds must be returned as decimals in $[0,1]$.\n- Assume $\\lambda  0$ and $q \\in [0,1)$ in all cases. If a requested $\\alpha$ violates feasibility for a given $q$ under your derivation, choose the smallest threshold in $[0,1]$ that still controls the Type I error under your model.\n- Your program must be self-contained and must not accept any input. Hard-code the test suite and print the single-line output as specified.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It is scientifically grounded, well-posed, and provides a complete, consistent formulation for a tractable theoretical analysis of phylogenetic support value calibration. We shall therefore proceed to the derivation and implementation of the solution.\n\nThe objective is to derive decision thresholds $\\tau$ for three branch support measures—standard nonparametric bootstrap ($U_{\\mathrm{BS}}$), Ultrafast Bootstrap ($U_{\\mathrm{UF}}$), and Shimodaira-Hasegawa-like approximate Likelihood Ratio Test ($U_{\\mathrm{SH}}$)—to control the Type I error rate at a specified level $\\alpha$. The condition to be satisfied is $\\mathbb{P}(U \\ge \\tau) \\le \\alpha$, where $U$ represents one of the support measures and the probability is taken over the distribution of datasets generated under the null hypothesis that the focal branch is absent.\n\nUnder the null hypothesis, the aggregate log-likelihood ratio signal $\\Delta = \\sum_{i=1}^{S} \\delta_i$ is a sum of independent and identically distributed random variables with $\\mathbb{E}[\\delta_i] = 0$ and $\\mathrm{Var}(\\delta_i) = \\sigma^2$. By the Central Limit Theorem, for a large number of sites $S$, the distribution of $\\Delta$ is well-approximated by a normal distribution: $\\Delta \\sim \\mathcal{N}(0, S\\sigma^2)$. It is convenient to work with the standardized signal, $X = \\frac{\\Delta}{\\sqrt{S}\\sigma}$, which follows a standard normal distribution, $X \\sim \\mathcal{N}(0, 1)$, under the null. The support values are functions of the observed data, and thus functions of the random variable $X$.\n\n**1. Threshold for Standard Nonparametric Bootstrap ($U_{\\mathrm{BS}}$)**\n\nThe standard bootstrap support $U_{\\mathrm{BS}}$ is the conditional probability that the bootstrap sum $\\Delta^\\star$ is positive, given the observed data $\\Delta_{obs}$. The distribution of $\\Delta^\\star$ is approximately $\\mathcal{N}(\\Delta_{obs}, S\\sigma^2)$. The support value is thus:\n$$\nU_{\\mathrm{BS}} = \\mathbb{P}(\\Delta^\\star  0 | \\Delta_{obs}) = \\mathbb{P}\\left(\\frac{\\Delta^\\star - \\Delta_{obs}}{\\sqrt{S}\\sigma}  \\frac{-\\Delta_{obs}}{\\sqrt{S}\\sigma}\\right)\n$$\nLetting $Z = (\\Delta^\\star - \\Delta_{obs})/(\\sqrt{S}\\sigma) \\sim \\mathcal{N}(0, 1)$, we have:\n$$\nU_{\\mathrm{BS}} = \\mathbb{P}\\left(Z  -\\frac{\\Delta_{obs}}{\\sqrt{S}\\sigma}\\right) = 1 - \\Phi\\left(-\\frac{\\Delta_{obs}}{\\sqrt{S}\\sigma}\\right) = \\Phi\\left(\\frac{\\Delta_{obs}}{\\sqrt{S}\\sigma}\\right)\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. As a function of the standardized signal $X = \\Delta/(\\sqrt{S}\\sigma)$, the support value is the random variable $U_{\\mathrm{BS}}(X) = \\Phi(X)$.\n\nWe seek a threshold $\\tau_{\\mathrm{BS}}$ such that $\\mathbb{P}(U_{\\mathrm{BS}} \\ge \\tau_{\\mathrm{BS}}) \\le \\alpha$.\n$$\n\\mathbb{P}(\\Phi(X) \\ge \\tau_{\\mathrm{BS}}) \\le \\alpha\n$$\nSince $\\Phi$ is monotonically increasing, this is equivalent to:\n$$\n\\mathbb{P}(X \\ge \\Phi^{-1}(\\tau_{\\mathrm{BS}})) \\le \\alpha\n$$\nFor $X \\sim \\mathcal{N}(0, 1)$, we know that $\\mathbb{P}(X \\ge z_{1-\\alpha}) = \\alpha$, where $z_{1-\\alpha} = \\Phi^{-1}(1-\\alpha)$ is the $(1-\\alpha)$-quantile of the standard normal distribution. To satisfy the inequality as an equality (which gives the least stringent threshold), we must set:\n$$\n\\Phi^{-1}(\\tau_{\\mathrm{BS}}) = z_{1-\\alpha} = \\Phi^{-1}(1-\\alpha)\n$$\nThis directly yields the threshold:\n$$\n\\tau_{\\mathrm{BS}}(\\alpha) = 1 - \\alpha\n$$\n\n**2. Threshold for Ultrafast Bootstrap ($U_{\\mathrm{UF}}$)**\n\nThe Ultrafast Bootstrap model is analogous, but with the variance of the resampled sum scaled by a factor $\\lambda  0$. The conditional distribution of the UFBoot sum is $\\mathcal{N}(\\Delta_{obs}, \\lambda S \\sigma^2)$. The support value $U_{\\mathrm{UF}}$ is:\n$$\nU_{\\mathrm{UF}} = \\mathbb{P}\\left(\\frac{\\Delta^{\\star\\star} - \\Delta_{obs}}{\\sqrt{\\lambda S}\\sigma}  \\frac{-\\Delta_{obs}}{\\sqrt{\\lambda S}\\sigma}\\right) = \\Phi\\left(\\frac{\\Delta_{obs}}{\\sqrt{\\lambda S}\\sigma}\\right)\n$$\nExpressed as a function of the standardized signal $X = \\Delta/(\\sqrt{S}\\sigma)$, the support is $U_{\\mathrm{UF}}(X) = \\Phi(X/\\sqrt{\\lambda})$. We seek $\\tau_{\\mathrm{UF}}$ such that $\\mathbb{P}(U_{\\mathrm{UF}} \\ge \\tau_{\\mathrm{UF}}) \\le \\alpha$.\n$$\n\\mathbb{P}\\left(\\Phi\\left(\\frac{X}{\\sqrt{\\lambda}}\\right) \\ge \\tau_{\\mathrm{UF}}\\right) \\le \\alpha \\implies \\mathbb{P}\\left(\\frac{X}{\\sqrt{\\lambda}} \\ge \\Phi^{-1}(\\tau_{\\mathrm{UF}})\\right) \\le \\alpha \\implies \\mathbb{P}\\left(X \\ge \\sqrt{\\lambda}\\Phi^{-1}(\\tau_{\\mathrm{UF}})\\right) \\le \\alpha\n$$\nTo calibrate the error to $\\alpha$, we set:\n$$\n\\sqrt{\\lambda}\\Phi^{-1}(\\tau_{\\mathrm{UF}}) = z_{1-\\alpha} = \\Phi^{-1}(1-\\alpha)\n$$\nSolving for $\\tau_{\\mathrm{UF}}$ gives:\n$$\n\\Phi^{-1}(\\tau_{\\mathrm{UF}}) = \\frac{\\Phi^{-1}(1-\\alpha)}{\\sqrt{\\lambda}} \\implies \\tau_{\\mathrm{UF}}(\\alpha, \\lambda) = \\Phi\\left(\\frac{\\Phi^{-1}(1-\\alpha)}{\\sqrt{\\lambda}}\\right)\n$$\n\n**3. Threshold for SH-like aLRT ($U_{\\mathrm{SH}}$)**\n\nThe support $U_{\\mathrm{SH}}$ is given by $F_{\\chi^2_1}(T)$, where $T$ is the test statistic and $F_{\\chi^2_1}$ is the CDF of a chi-squared distribution with $1$ degree of freedom. Under the null, the distribution of $T$ is a mixture: $T \\sim q \\cdot \\delta_0 + (1-q) \\cdot \\chi^2_1$ for $q \\in [0, 1)$, where $\\delta_0$ is a point mass at $0$.\n\nWe seek $\\tau_{\\mathrm{SH}}$ such that $\\mathbb{P}(U_{\\mathrm{SH}} \\ge \\tau_{\\mathrm{SH}}) \\le \\alpha$.\n$$\n\\mathbb{P}(F_{\\chi^2_1}(T) \\ge \\tau_{\\mathrm{SH}}) \\le \\alpha\n$$\nSince $F_{\\chi^2_1}$ is strictly increasing for non-negative arguments, this is equivalent to finding the threshold $t_\\tau = F_{\\chi^2_1}^{-1}(\\tau_{\\mathrm{SH}})$ and solving $\\mathbb{P}(T \\ge t_\\tau) \\le \\alpha$. The probability with respect to the mixture distribution is:\n$$\n\\mathbb{P}(T \\ge t_\\tau) = q \\cdot \\mathbb{P}(T \\ge t_\\tau| T \\sim \\delta_0) + (1-q) \\cdot \\mathbb{P}(T \\ge t_\\tau| T \\sim \\chi^2_1)\n$$\nAssuming $t_\\tau > 0$, the first term is $0$. The second term is $(1-q) \\cdot (1 - F_{\\chi^2_1}(t_\\tau))$. So we need:\n$$\n(1-q)(1-F_{\\chi^2_1}(t_\\tau)) \\le \\alpha\n$$\nSubstituting $t_\\tau = F_{\\chi^2_1}^{-1}(\\tau_{\\mathrm{SH}})$, we get:\n$$\n(1-q)(1-\\tau_{\\mathrm{SH}}) \\le \\alpha \\implies 1-\\tau_{\\mathrm{SH}} \\le \\frac{\\alpha}{1-q} \\implies \\tau_{\\mathrm{SH}} \\ge 1 - \\frac{\\alpha}{1-q}\n$$\nTo control the Type I error, we must choose a threshold at least this large. The smallest valid threshold (the least conservative choice) is $\\tau_{\\mathrm{SH}} = 1 - \\frac{\\alpha}{1-q}$. This formula is valid provided the threshold is non-negative, which requires $1 - \\frac{\\alpha}{1-q} \\ge 0$, or $\\alpha \\le 1-q$. If $\\alpha > 1-q$, the required threshold would be negative, which is not possible. In such a case, even a threshold of $\\tau_{\\mathrm{SH}}=0$ controls the Type I error. For $\\tau_{\\mathrm{SH}}=0$, the error is $\\mathbb{P}(U_{\\mathrm{SH}} \\ge 0)$. Since $U_{\\mathrm{SH}}$ is a CDF value, it is always non-negative, so $\\mathbb{P}(U_{\\mathrm{SH}} \\ge 0) = 1$ only if T can be negative, which is not the case for an LRT. Let's re-evaluate the actual error at $\\tau=0$. The error is $(1-q)(1-0)=1-q$. If $\\alpha > 1-q$, choosing $\\tau=0$ yields an error of $1-q$, which is indeed $\\le \\alpha$. Thus, the minimal valid threshold is $0$.\nCombining these cases, the general formula is:\n$$\n\\tau_{\\mathrm{SH}}(\\alpha, q) = \\max\\left(0, 1 - \\frac{\\alpha}{1-q}\\right)\n$$\n\n**Summary of Derived Thresholds:**\n1.  $\\tau_{\\mathrm{BS}}(\\alpha) = 1 - \\alpha$\n2.  $\\tau_{\\mathrm{UF}}(\\alpha, \\lambda) = \\Phi\\left(\\frac{\\Phi^{-1}(1-\\alpha)}{\\sqrt{\\lambda}}\\right)$\n3.  $\\tau_{\\mathrm{SH}}(\\alpha, q) = \\max\\left(0, 1 - \\frac{\\alpha}{1-q}\\right)$\n\nThese formulas will now be implemented to compute the thresholds for the provided test suite.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes calibrated decision thresholds for phylogenetic branch support\n    measures based on derived formulas.\n    \"\"\"\n    \n    # Test suite of parameters (alpha, lambda, q).\n    test_cases = [\n        # Case A: (alpha, lambda, q) = (0.05, 1.0, 0.0)\n        (0.05, 1.0, 0.0),\n        # Case B: (alpha, lambda, q) = (0.05, 0.6, 0.0)\n        (0.05, 0.6, 0.0),\n        # Case C: (alpha, lambda, q) = (0.05, 1.4, 0.5)\n        (0.05, 1.4, 0.5),\n        # Case D: (alpha, lambda, q) = (0.01, 0.6, 0.0)\n        (0.01, 0.6, 0.0),\n        # Case E: (alpha, lambda, q) = (0.10, 0.8, 0.2)\n        (0.10, 0.8, 0.2),\n        # Case F: (alpha, lambda, q) = (0.25, 2.0, 0.5)\n        (0.25, 2.0, 0.5),\n    ]\n\n    all_results = []\n    for alpha, lam, q in test_cases:\n        # 1. Standard Bootstrap threshold (tau_BS)\n        tau_bs = 1.0 - alpha\n\n        # 2. Ultrafast Bootstrap threshold (tau_UF)\n        # alpha is in (0,1), so 1-alpha is in (0,1). norm.ppf is well-defined.\n        # lam > 0 as per problem description.\n        z_1_minus_alpha = norm.ppf(1.0 - alpha)\n        tau_uf = norm.cdf(z_1_minus_alpha / np.sqrt(lam))\n\n        # 3. SH-like aLRT threshold (tau_SH)\n        # q is in [0,1) as per problem description, so 1-q is not zero.\n        tau_sh = max(0.0, 1.0 - alpha / (1.0 - q))\n        \n        all_results.append([tau_bs, tau_uf, tau_sh])\n\n    # Format the final output string as a list of lists of floats,\n    # with each float rounded to exactly six decimal places.\n    outer_list_str = []\n    for result_set in all_results:\n        inner_list_str = f\"[{','.join([f'{x:.6f}' for x in result_set])}]\"\n        outer_list_str.append(inner_list_str)\n    \n    final_output = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2692773"}]}