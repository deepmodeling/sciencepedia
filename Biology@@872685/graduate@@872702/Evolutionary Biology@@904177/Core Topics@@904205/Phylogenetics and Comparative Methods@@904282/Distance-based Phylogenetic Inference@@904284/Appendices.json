{"hands_on_practices": [{"introduction": "This first exercise is a foundational comparison between two classic distance-based methods: UPGMA and Neighbor-Joining (NJ). By working through a small, carefully chosen example by hand, you will gain direct insight into how their different underlying assumptions—the molecular clock for UPGMA and additivity for NJ—lead to different tree topologies. This practice is essential for understanding why NJ is often preferred for data where evolutionary rates may vary across lineages and for appreciating how an algorithm's success is tied to the properties of the input data [@problem_id:2840492].", "problem": "Consider four taxa $\\{A,B,C,D\\}$ with a symmetric pairwise distance matrix $D$ (interpretable as expected substitutions per site):\n$$\nD \\;=\\; \\begin{pmatrix}\n0 & 0.17 & 0.26 & 0.29\\\\\n0.17 & 0 & 0.27 & 0.22\\\\\n0.26 & 0.27 & 0 & 0.39\\\\\n0.29 & 0.22 & 0.39 & 0\n\\end{pmatrix}.\n$$\nThis matrix is nearly but not exactly ultrametric in the sense of the three-point condition. Starting only from core definitions in phylogenetic systematics and distance-based tree reconstruction, do the following:\n\n- Using the Unweighted Pair Group Method with Arithmetic Mean (UPGMA), construct the rooted ultrametric tree for $\\{A,B,C,D\\}$, including all intermediate merge heights and terminal branch lengths implied by the algorithm, and compute the sum of squared errors $\\mathrm{SSE}_{\\mathrm{UPGMA}}$ between the path distances in this UPGMA tree and the entries of $D$.\n\n- Using neighbor-joining (NJ), construct the unrooted additive tree for $\\{A,B,C,D\\}$, including the identification of the first joined pair, the interim reduced distances, and the final branch lengths, and compute the sum of squared errors $\\mathrm{SSE}_{\\mathrm{NJ}}$ between the path distances in this NJ tree and the entries of $D$.\n\n- Explain, using the definitions of ultrametricity and the algorithmic criteria of UPGMA and NJ, why UPGMA fails to recover the correct grouping on these data while NJ succeeds.\n\nFinally, compute the difference\n$$\n\\Delta \\;=\\; \\mathrm{SSE}_{\\mathrm{UPGMA}} \\;-\\; \\mathrm{SSE}_{\\mathrm{NJ}}.\n$$\nReport $\\Delta$ as a real number. Round your answer to four significant figures. Do not include any units.", "solution": "The problem requires the construction and evaluation of phylogenetic trees for four taxa $\\{A, B, C, D\\}$ using two distinct distance-based methods, UPGMA and Neighbor-Joining, based on a provided pairwise distance matrix $D$. First, we validate the problem. The givens are the set of taxa and the symmetric distance matrix $D$:\n$$\nD = \\begin{pmatrix}\n0 & 0.17 & 0.26 & 0.29 \\\\\n0.17 & 0 & 0.27 & 0.22 \\\\\n0.26 & 0.27 & 0 & 0.39 \\\\\n0.29 & 0.22 & 0.39 & 0\n\\end{pmatrix}\n$$\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in phylogenetic reconstruction. The data and methods are standard in the field of general biology, specifically systematics. The problem is valid. We proceed with the solution.\n\nFirst, we analyze the properties of the matrix $D$. The matrix is not ultrametric. For example, for the triplet $\\{A, C, D\\}$, the distances are $d(A,C)=0.26$, $d(A,D)=0.29$, and $d(C,D)=0.39$. The two largest distances, $0.39$ and $0.29$, are not equal, violating the three-point condition.\nHowever, the matrix is perfectly additive. For the four taxa, we check the four-point condition. Let the sums of opposing pairs of distances be $S_1 = d(A,B)+d(C,D)$, $S_2 = d(A,C)+d(B,D)$, and $S_3 = d(A,D)+d(B,C)$.\n$S_1 = 0.17 + 0.39 = 0.56$\n$S_2 = 0.26 + 0.22 = 0.48$\n$S_3 = 0.29 + 0.27 = 0.56$\nSince $S_1 = S_3 > S_2$, the matrix is additive and corresponds to the unrooted tree topology $((A,C),(B,D))$.\n\n**Unweighted Pair Group Method with Arithmetic Mean (UPGMA)**\n\nUPGMA is a hierarchical clustering algorithm that assumes a molecular clock, i.e., ultrametric data.\n**Iteration 1:** The initial clusters are $\\{A\\}, \\{B\\}, \\{C\\}, \\{D\\}$. The smallest distance in $D$ is $d(A,B) = 0.17$. We merge $A$ and $B$ into a new cluster $(AB)$. The height of the node connecting them is $h_{(AB)} = \\frac{d(A,B)}{2} = \\frac{0.17}{2} = 0.085$.\nThe distances from this new cluster to $C$ and $D$ are computed:\n$d((AB), C) = \\frac{d(A,C) + d(B,C)}{2} = \\frac{0.26 + 0.27}{2} = 0.265$\n$d((AB), D) = \\frac{d(A,D) + d(B,D)}{2} = \\frac{0.29 + 0.22}{2} = 0.255$\nThe new distance matrix is:\n$$\nD_1 = \\begin{array}{c|ccc}\n & (AB) & C & D \\\\ \\hline\n(AB) & 0 & 0.265 & 0.255 \\\\\nC & 0.265 & 0 & 0.39 \\\\\nD & 0.255 & 0.39 & 0\n\\end{array}\n$$\n**Iteration 2:** The smallest distance in $D_1$ is $d((AB), D) = 0.255$. We merge cluster $(AB)$ with $D$ to form $((AB)D)$. The height of this new node is $h_{((AB)D)} = \\frac{d((AB),D)}{2} = \\frac{0.255}{2} = 0.1275$.\n\n**Iteration 3:** The remaining clusters are $((AB)D)$ and $C$. The distance is:\n$d(((AB)D), C) = \\frac{d(A,C) + d(B,C) + d(D,C)}{3} = \\frac{0.26 + 0.27 + 0.39}{3} = \\frac{0.92}{3}$.\nWe merge them. The height of the root node is $h_{root} = \\frac{1}{2} d(((AB)D), C) = \\frac{1}{2} \\frac{0.92}{3} = \\frac{0.46}{3}$.\n\nThe resulting UPGMA tree has the topology $((A,B),D),C$. The path distances in this tree, $D_{\\mathrm{UPGMA}}$, are given by twice the height of the most recent common ancestor.\n$d_{\\mathrm{UPGMA}}(A,B) = 2 h_{(AB)} = 2(0.085) = 0.17$\n$d_{\\mathrm{UPGMA}}(A,D) = d_{\\mathrm{UPGMA}}(B,D) = 2 h_{((AB)D)} = 2(0.1275) = 0.255$\n$d_{\\mathrm{UPGMA}}(A,C) = d_{\\mathrm{UPGMA}}(B,C) = d_{\\mathrm{UPGMA}}(D,C) = 2 h_{root} = 2 (\\frac{0.46}{3}) = \\frac{0.92}{3}$\n\nThe sum of squared errors is $\\mathrm{SSE}_{\\mathrm{UPGMA}} = \\sum_{i<j} (D_{ij} - (D_{\\mathrm{UPGMA}})_{ij})^2$.\n$\\mathrm{SSE}_{\\mathrm{UPGMA}} = (0.17 - 0.17)^2 + (0.26 - \\frac{0.92}{3})^2 + (0.29 - 0.255)^2 + (0.27 - \\frac{0.92}{3})^2 + (0.22 - 0.255)^2 + (0.39 - \\frac{0.92}{3})^2$\n$= 0 + (\\frac{0.78 - 0.92}{3})^2 + (0.035)^2 + (\\frac{0.81 - 0.92}{3})^2 + (-0.035)^2 + (\\frac{1.17 - 0.92}{3})^2$\n$= (\\frac{-0.14}{3})^2 + (0.035)^2 + (\\frac{-0.11}{3})^2 + (-0.035)^2 + (\\frac{0.25}{3})^2$\n$= \\frac{0.0196}{9} + 0.001225 + \\frac{0.0121}{9} + 0.001225 + \\frac{0.0625}{9}$\n$= \\frac{0.0196+0.0121+0.0625}{9} + 2(0.001225) = \\frac{0.0942}{9} + 0.00245 = 0.010466... + 0.00245 = 0.0129166...$\nUsing fractions for precision: $\\mathrm{SSE}_{\\mathrm{UPGMA}} = (\\frac{-7}{150})^2 + (\\frac{7}{200})^2 + (\\frac{-11}{300})^2 + (\\frac{-7}{200})^2 + (\\frac{1}{12})^2 = \\frac{49}{22500} + \\frac{49}{40000} + \\frac{121}{90000} + \\frac{49}{40000} + \\frac{1}{144} = \\frac{31}{2400}$.\n\n**Neighbor-Joining (NJ)**\n\nNJ is designed to reconstruct additive trees.\n**Iteration 1:** For $N=4$ taxa, we compute $r_i = \\sum_{k \\ne i} d(i,k)$.\n$r_A=0.17+0.26+0.29=0.72$\n$r_B=0.17+0.27+0.22=0.66$\n$r_C=0.26+0.27+0.39=0.92$\n$r_D=0.29+0.22+0.39=0.90$\nThe selection criterion is to minimize $Q_{ij} = (N-2)d(i,j) - r_i - r_j = 2d(i,j) - r_i - r_j$.\n$Q_{AC} = 2(0.26) - 0.72 - 0.92 = 0.52 - 1.64 = -1.12$\n$Q_{BD} = 2(0.22) - 0.66 - 0.90 = 0.44 - 1.56 = -1.12$\nThe other $Q$ values are all $-1.04$. The minimum value is $-1.12$, corresponding to pairs $(A,C)$ and $(B,D)$. This is consistent with the four-point condition. We choose to join $(A,C)$ first; this is the first identified pair. A new node $U$ is created.\nThe branch lengths to the new node $U$ are:\n$v_A = d(A,U) = \\frac{1}{2}d(A,C) + \\frac{1}{2(N-2)}(r_A-r_C) = \\frac{0.26}{2} + \\frac{0.72-0.92}{4} = 0.13 - 0.05 = 0.08$.\n$v_C = d(C,U) = d(A,C) - v_A = 0.26 - 0.08 = 0.18$.\nThe interim reduced distances to the new node $U$ from $B$ and $D$ are:\n$d(B,U) = \\frac{1}{2}(d(A,B)+d(C,B)-d(A,C)) = \\frac{1}{2}(0.17+0.27-0.26) = 0.09$.\n$d(D,U) = \\frac{1}{2}(d(A,D)+d(C,D)-d(A,C)) = \\frac{1}{2}(0.29+0.39-0.26) = 0.21$.\n\n**Iteration 2:** The remaining taxa are $\\{B,D,U\\}$, and their distances are $d(B,D)=0.22$, $d(B,U)=0.09$, $d(D,U)=0.21$. This defines the final tree, with a central node $V$ connecting $B,D,U$. The final three branch lengths are:\n$v_B = d(B,V) = \\frac{1}{2}(d(B,D)+d(B,U)-d(D,U)) = \\frac{1}{2}(0.22+0.09-0.21) = 0.05$.\n$v_D = d(D,V) = \\frac{1}{2}(d(B,D)+d(D,U)-d(B,U)) = \\frac{1}{2}(0.22+0.21-0.09) = 0.17$.\n$v_{UV} = d(U,V) = \\frac{1}{2}(d(B,U)+d(D,U)-d(B,D)) = \\frac{1}{2}(0.09+0.21-0.22) = 0.04$.\nThe final branch lengths are: $v_A=0.08$, $v_C=0.18$, $v_B=0.05$, $v_D=0.17$, and the internal branch length is $v_{UV}=0.04$. The topology is $((A,C),(B,D))$.\nThe path distances in the NJ tree, $D_{\\mathrm{NJ}}$, perfectly match the original matrix $D$:\n$d_{\\mathrm{NJ}}(A,C) = v_A+v_C = 0.08+0.18 = 0.26$.\n$d_{\\mathrm{NJ}}(B,D) = v_B+v_D = 0.05+0.17 = 0.22$.\n$d_{\\mathrm{NJ}}(A,B) = v_A+v_{UV}+v_B = 0.08+0.04+0.05 = 0.17$.\n$d_{\\mathrm{NJ}}(C,D) = v_C+v_{UV}+v_D = 0.18+0.04+0.17 = 0.39$.\nSince $D_{\\mathrm{NJ}} = D$, the sum of squared errors is $\\mathrm{SSE}_{\\mathrm{NJ}} = 0$.\n\n**Explanation of UPGMA Failure and NJ Success**\n\nUPGMA fails because it operates under the molecular clock assumption, which requires the distance matrix to be ultrametric. The algorithm's criterion is to greedily merge the pair of clusters with the minimum distance at each step. For the given matrix $D$, the globally smallest distance is $d(A,B)=0.17$. UPGMA thus incorrectly pairs $A$ and $B$, forcing the topology $((A,B),...)$, which contradicts the true additive structure revealed by the four-point condition. The underlying data is not ultrametric, as shown by violation of the three-point condition, meaning evolutionary rates have varied among lineages.\n\nNJ succeeds because its objective is to find the tree that best fits an additive distance matrix. The data provided is perfectly additive, satisfying the four-point condition for the topology $((A,C),(B,D))$. NJ's selection criterion, minimizing the $Q_{ij}$ values, is mathematically constructed to identify true neighbors in an additive tree. Consequently, it correctly identifies the pairs $(A,C)$ and $(B,D)$ as neighbors and reconstructs the correct tree topology and branch lengths, resulting in a perfect fit to the data ($\\mathrm{SSE}_{\\mathrm{NJ}}=0$). NJ does not assume a molecular clock and accommodates lineage-specific rate variation.\n\n**Final Calculation**\n\nThe difference in the sum of squared errors is:\n$\\Delta = \\mathrm{SSE}_{\\mathrm{UPGMA}} - \\mathrm{SSE}_{\\mathrm{NJ}} = \\frac{31}{2400} - 0 = \\frac{31}{2400} \\approx 0.0129166...$\nRounding to four significant figures, we get $\\Delta = 0.01292$.", "answer": "$$\\boxed{0.01292}$$", "id": "2840492"}, {"introduction": "Moving from manual calculation to computational implementation is a crucial step in mastering phylogenetic methods. This exercise requires you to build the Neighbor-Joining algorithm from first principles, along with the Robinson-Foulds metric for comparing tree topologies [@problem_id:2701760]. By coding the algorithm yourself and running a series of controlled simulations, you will develop a much deeper understanding of its mechanics and gain practical experience in evaluating its performance and sensitivity to noise, a ubiquitous challenge in real-world phylogenetics.", "problem": "You are asked to implement a complete, self-contained simulation to assess how the neighbor-joining method responds to additive noise in pairwise distances. The task concerns distance-based phylogenetic inference using the neighbor-joining algorithm at an advanced graduate level. The simulation must be deterministic, require no input, and print a single line containing the aggregate results for a fixed test suite.\n\nThe fundamental base for this problem consists of the following scientifically established definitions and facts:\n\n- A distance on a finite set of taxa is additive if there exists an unrooted tree with nonnegative edge lengths such that the distance between any two taxa equals the sum of edge lengths along the unique path connecting the taxa.\n- The neighbor-joining algorithm reconstructs an unrooted tree from a dissimilarity map by iteratively joining a pair of taxa or clusters in a way that minimizes estimated total tree length under the balanced minimum evolution principle. The method is consistent on additive distances.\n- The Robinson–Foulds (RF) distance between two unrooted trees on the same leaf set is defined as the cardinality of the symmetric difference of their sets of nontrivial splits (bipartitions), where a split is nontrivial if both sides have size at least $2$.\n\nYour program must:\n\n1. Construct true additive distance matrices from explicitly specified unrooted trees with edge lengths.\n2. Add specified additive noise to certain cases to obtain noised distance matrices.\n3. Reconstruct trees from these matrices using neighbor-joining.\n4. Compute the unrooted Robinson–Foulds distance between each reconstructed tree and the corresponding true tree.\n5. Print a single line containing the list of RF distances for the test suite in the required format.\n\nMathematical and logical details to use:\n\n- A phylogenetic tree is modeled as an undirected weighted tree. The distance between two leaves is the sum of edge lengths along the unique path connecting them.\n- The neighbor-joining algorithm must be implemented from first principles without shortcuts. Any ties in the choice of the pair to join must be broken deterministically by choosing the lexicographically smallest pair of current node labels.\n- All branch lengths must be treated as real numbers. If due to floating-point rounding a computed branch length is slightly negative, it must be clamped to $0$ to maintain nonnegativity.\n- The Robinson–Foulds distance counts only nontrivial splits. For an unrooted tree on $n$ leaves, a split is represented canonically by the subset of leaves on the smaller side.\n\nTest suite specification:\n\nCase $1$ (baseline, quartet, no noise):\n- Leaves: $\\{A,B,C,D\\}$.\n- True unrooted tree with edges:\n  - $A$–$X$ length $0.30$, $B$–$X$ length $0.35$, $X$–$Z$ length $0.25$,\n  - $C$–$Y$ length $0.31$, $D$–$Y$ length $0.33$, $Y$–$Z$ length $0.27$.\n- No noise is added.\n\nCase $2$ (five leaves, no noise):\n- Leaves: $\\{A,B,C,D,E\\}$.\n- True unrooted tree with edges:\n  - $A$–$X$ length $0.12$, $B$–$X$ length $0.13$, $X$–$Z$ length $0.20$,\n  - $C$–$Z$ length $0.11$,\n  - $D$–$Y$ length $0.10$, $E$–$Y$ length $0.09$, $Y$–$Z$ length $0.18$.\n- No noise is added.\n\nCase $3$ (five leaves, small symmetric noise):\n- Leaves and true tree identical to Case $2$.\n- Additive noise matrix $\\Delta$ specified for unordered pairs $(i,j)$ with $i<j$ in the order $(A,B)$, $(A,C)$, $(A,D)$, $(A,E)$, $(B,C)$, $(B,D)$, $(B,E)$, $(C,D)$, $(C,E)$, $(D,E)$:\n  - $\\Delta(A,B)=+0.003$,\n  - $\\Delta(A,C)=+0.004$,\n  - $\\Delta(A,D)=-0.002$,\n  - $\\Delta(A,E)=+0.001$,\n  - $\\Delta(B,C)=+0.002$,\n  - $\\Delta(B,D)=-0.001$,\n  - $\\Delta(B,E)=+0.000$,\n  - $\\Delta(C,D)=-0.002$,\n  - $\\Delta(C,E)=+0.001$,\n  - $\\Delta(D,E)=-0.003$.\n- The diagonal entries are $0$, and $\\Delta$ is symmetric, so $\\Delta(j,i)=\\Delta(i,j)$.\n\nCase $4$ (six leaves, structured large noise causing a topology change):\n- Leaves: $\\{A,B,C,D,E,F\\}$.\n- True unrooted tree with three cherries joined at a central node:\n  - $A$–$X$ length $0.10$, $B$–$X$ length $0.11$, $X$–$Z$ length $0.20$,\n  - $C$–$Y$ length $0.095$, $D$–$Y$ length $0.105$, $Y$–$Z$ length $0.19$,\n  - $E$–$W$ length $0.09$, $F$–$W$ length $0.11$, $W$–$Z$ length $0.21$.\n- Define an alternative unrooted tree on the same leaves with cherries $(A,C)$, $(B,D)$, and $(E,F)$ joined at a central node:\n  - $A$–$X'$ length $0.10$, $C$–$X'$ length $0.095$, $X'$–$Z'$ length $0.20$,\n  - $B$–$Y'$ length $0.11$, $D$–$Y'$ length $0.105$, $Y'$–$Z'$ length $0.19$,\n  - $E$–$W'$ length $0.09$, $F$–$W'$ length $0.11$, $W'$–$Z'$ length $0.21$.\n- Let $D_{\\mathrm{true}}$ and $D_{\\mathrm{alt}}$ denote the pairwise leaf distance matrices induced by the true and alternative trees, respectively. The noise matrix for this case is defined as $\\Delta = D_{\\mathrm{alt}} - D_{\\mathrm{true}}$. The noised matrix is $D_{\\mathrm{noised}} = D_{\\mathrm{true}} + \\Delta = D_{\\mathrm{alt}}$.\n\nCase $5$ (boundary, three leaves, small noise, no nontrivial split):\n- Leaves: $\\{A,B,C\\}$.\n- True unrooted tree with a single internal node:\n  - $A$–$Z$ length $0.20$, $B$–$Z$ length $0.30$, $C$–$Z$ length $0.25$.\n- Additive noise matrix $\\Delta$ on unordered pairs $(A,B)$, $(A,C)$, $(B,C)$ given by:\n  - $\\Delta(A,B)=+0.010$,\n  - $\\Delta(A,C)=-0.010$,\n  - $\\Delta(B,C)=+0.000$.\n- The diagonal entries are $0$, and $\\Delta$ is symmetric.\n\nComputation and output requirements:\n\n- For each case $k \\in \\{1,2,3,4,5\\}$:\n  - Compute the true additive matrix $D_{\\mathrm{true}}^{(k)}$ from the specified tree.\n  - If a noise matrix $\\Delta^{(k)}$ is specified, set $D^{(k)} = D_{\\mathrm{true}}^{(k)} + \\Delta^{(k)}$, otherwise set $D^{(k)} = D_{\\mathrm{true}}^{(k)}$.\n  - Reconstruct an unrooted tree $\\widehat{T}^{(k)}$ from $D^{(k)}$ using neighbor-joining.\n  - Compute the Robinson–Foulds distance $R^{(k)}$ between $\\widehat{T}^{(k)}$ and the specified true tree $T^{(k)}$ on the same leaf set, counting only nontrivial splits.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the five cases: for example, $\\left[\\;R^{(1)},R^{(2)},R^{(3)},R^{(4)},R^{(5)}\\;\\right]$.\n\nAll numbers in this problem are unitless real values. Angles do not appear. Percentages do not appear. The required final output type is a list of integers. The program must be completely deterministic and require no external input files or user interaction. The implementation must be self-contained and runnable as is. The neighbor-joining procedure and the Robinson–Foulds computation must be implemented by you in full detail; use of library routines for these is not permitted.", "solution": "The problem presented is a valid, well-defined computational task within the field of evolutionary biology. It requires the implementation and application of the neighbor-joining algorithm and the Robinson-Foulds distance metric to analyze the effect of noise on phylogenetic reconstruction. The problem is scientifically grounded in established principles of phylogenetics, provides all necessary data and constraints for a deterministic solution, and is structured as a standard simulation study. I shall proceed to provide a complete solution.\n\nThe core of the problem lies in three main computational components:\n1.  Calculation of an additive distance matrix from a given tree topology and edge lengths.\n2.  Implementation of the neighbor-joining (NJ) algorithm to reconstruct a tree from a distance matrix.\n3.  Implementation of the Robinson-Foulds (RF) distance to compare the topology of the reconstructed tree with the true tree.\n\nWe will systematically address each of the five test cases by applying these components.\n\nFirst, we define the fundamental algorithms.\n\n**1. Additive Distance Matrix Calculation**\nGiven an unrooted phylogenetic tree with weighted edges (branch lengths), the distance $d(i, j)$ between any two leaves (taxa) $i$ and $j$ is the sum of the weights of the edges on the unique path connecting them. This can be computed by running a graph traversal algorithm, such as a Breadth-First Search (BFS) or Depth-First Search (DFS), starting from each leaf to find the path distance to all other leaves.\n\n**2. The Neighbor-Joining Algorithm**\nThe neighbor-joining algorithm is an iterative clustering method that reconstructs an unrooted tree from a distance matrix $D$. The algorithm proceeds as follows:\n\nLet $L$ be the set of active nodes (initially the leaves). The algorithm iterates until $|L|=2$. In each step:\na. For the current set of nodes $L$ of size $r = |L|$, compute the \"Q-matrix\" where for any two distinct nodes $i, j \\in L$:\n$$ Q_{ij} = (r-2)D_{ij} - \\sum_{k \\in L} D_{ik} - \\sum_{k \\in L} D_{jk} $$\nb. Find the pair of distinct nodes $(i, j)$ for which $Q_{ij}$ is minimal. Ties are broken by selecting the lexicographically smallest pair of node labels.\nc. Create a new node $u$ that represents the cluster $(i, j)$. The branch lengths from $i$ and $j$ to $u$ are calculated as:\n$$ d(i, u) = \\frac{1}{2} D_{ij} + \\frac{1}{2(r-2)} \\left( \\sum_{k \\in L} D_{ik} - \\sum_{k \\in L} D_{jk} \\right) $$\n$$ d(j, u) = D_{ij} - d(i, u) $$\nAny computed branch length that is slightly negative due to floating-point imprecision is clamped to $0$.\nd. A new distance matrix $D'$ is formed for the updated set of nodes $L' = (L \\setminus \\{i, j\\}) \\cup \\{u\\}$. The distance from the new node $u$ to any other node $k \\in L \\setminus \\{i, j\\}$ is:\n$$ D'_{uk} = \\frac{1}{2} (D_{ik} + D_{jk} - D_{ij}) $$\ne. The set of active nodes becomes $L'$. The process repeats.\n\nWhen only two nodes, say $a$ and $b$, remain, they are connected by an edge of length $D_{ab}$. This completes the unrooted tree.\n\n**3. Robinson–Foulds (RF) Distance**\nThe RF distance measures the topological dissimilarity between two unrooted trees on the same set of leaves. It is based on the concept of splits.\n- An internal edge in an unrooted tree partitions the set of leaves into two disjoint subsets, called a split.\n- A split is non-trivial if both subsets contain at least two leaves. An unrooted tree with $n$ leaves has at most $n-3$ non-trivial splits.\n- For two trees, $T_1$ and $T_2$, let $S(T_1)$ and $S(T_2)$ be their respective sets of non-trivial splits. Splits are represented canonically (e.g., by the frozenset of leaves in the smaller partition) to allow for comparison.\n- The RF distance is the size of the symmetric difference of these two sets:\n$$ RF(T_1, T_2) = | S(T_1) \\Delta S(T_2) | = |(S(T_1) \\setminus S(T_2)) \\cup (S(T_2) \\setminus S(T_1))| $$\n\nWe now apply this methodology to each case. Let the leaf labels be converted to a sorted list for deterministic indexing and tie-breaking: e.g., $\\{A, B, C, D\\} \\rightarrow ['A', 'B', 'C', 'D']$.\n\n**Case 1: Quartet, no noise**\n- True Tree $T^{(1)}$: Unrooted tree with topology $((A,B),(C,D))$. The leaves $\\{A,B\\}$ form a cherry, as do $\\{C,D\\}$, and these two pairs are joined via a central edge.\n- The distance matrix $D^{(1)}$ is calculated from this tree's edge lengths. Since it is a perfect additive matrix, the neighbor-joining algorithm is guaranteed to recover the correct topology.\n- True Splits $S(T^{(1)})$: The single internal edge splits the leaves into $\\{A,B\\}$ and $\\{C,D\\}$. This is a non-trivial split, represented as $\\{A,B\\}$. Thus, $S(T^{(1)}) = \\{\\{A,B\\}\\}$.\n- Reconstructed Tree $\\widehat{T}^{(1)}$: NJ on $D^{(1)}$ yields a tree with the same topology.\n- Reconstructed Splits $S(\\widehat{T}^{(1)})$: The set of splits is identical, $S(\\widehat{T}^{(1)}) = \\{\\{A,B\\}\\}$.\n- RF Distance $R^{(1)}$: $|S(T^{(1)}) \\Delta S(\\widehat{T}^{(1)})| = |\\emptyset| = 0$.\n\n**Case 2: Five leaves, no noise**\n- True Tree $T^{(2)}$: Topology corresponds to $((A,B),C,(D,E))$.\n- The distance matrix $D^{(2)}$ is additive.\n- True Splits $S(T^{(2)})$: The two internal edges correspond to splits $\\{A,B\\}|\\{C,D,E\\}$ and $\\{D,E\\}|\\{A,B,C\\}$. Both are non-trivial. Canonically, $S(T^{(2)}) = \\{\\{A,B\\}, \\{D,E\\}\\}$.\n- Reconstructed Tree $\\widehat{T}^{(2)}$: NJ on the additive matrix $D^{(2)}$ reconstructs the true topology.\n- Reconstructed Splits $S(\\widehat{T}^{(2)})$: $S(\\widehat{T}^{(2)}) = \\{\\{A,B\\}, \\{D,E\\}\\}$.\n- RF Distance $R^{(2)}$: $|S(T^{(2)}) \\Delta S(\\widehat{T}^{(2)})| = |\\emptyset| = 0$.\n\n**Case 3: Five leaves, small symmetric noise**\n- True Tree $T^{(3)}$: Same as Case 2, so $S(T^{(3)}) = \\{\\{A,B\\}, \\{D,E\\}\\}$.\n- Distance Matrix $D^{(3)}$: The additive matrix from Case 2 is perturbed by a small noise matrix $\\Delta$. $D^{(3)} = D_{\\mathrm{true}}^{(2)} + \\Delta^{(3)}$. This matrix is no longer perfectly additive.\n- Reconstructed Tree $\\widehat{T}^{(3)}$: We must run the NJ algorithm.\n  - Step 1 ($r=5$): The pair $(A,B)$ minimizes the Q-matrix. They are joined into a new node $U_1$.\n  - Step 2 ($r=4$): The active nodes are $\\{C,D,E,U_1\\}$. We find a tie for the minimum Q-value between pairs $(U_1,C)$ and $(D,E)$. According to the deterministic tie-breaking rule, the lexicographically smallest pair of node labels is chosen. The pair of labels `('C', 'U1')` is lexicographically smaller than `('D', 'E')`. Thus, NJ joins $C$ with the cluster $U_1=(A,B)$.\n  - This results in a reconstructed topology of $(((A,B),C),D,E)$.\n- Reconstructed Splits $S(\\widehat{T}^{(3)})$: The tree $(((A,B),C),D,E)$ has only one internal edge that produces a non-trivial split: $\\{A,B\\}|\\{C,D,E\\}$. Thus, $S(\\widehat{T}^{(3)}) = \\{\\{A,B\\}\\}$.\n- RF Distance $R^{(3)}$: $S(T^{(3)}) \\Delta S(\\widehat{T}^{(3)}) = \\{\\{A,B\\}, \\{D,E\\}\\} \\Delta \\{\\{A,B\\}\\} = \\{\\{D,E\\}\\}$. The size of this set is $1$. So, $R^{(3)} = 1$.\n\n**Case 4: Six leaves, structured large noise**\n- True Tree $T^{(4)}$: Topology is $(((A,B),(C,D)),(E,F))$.\n- True Splits $S(T^{(4)})$: The splits are $\\{A,B\\}|\\{C,D,E,F\\}$, $\\{C,D\\}|\\{A,B,E,F\\}$, and $\\{E,F\\}|\\{A,B,C,D\\}$. Canonically, $S(T^{(4)}) = \\{\\{A,B\\}, \\{C,D\\}, \\{E,F\\}\\}$.\n- Distance Matrix $D^{(4)}$: The matrix used for reconstruction is not from $T^{(4)}$ but from an alternative tree, $T_{\\mathrm{alt}}$, with topology $(((A,C),(B,D)),(E,F))$. The noise is structured such that $D^{(4)} = D_{\\mathrm{alt}}$.\n- Reconstructed Tree $\\widehat{T}^{(4)}$: Since $D^{(4)}$ is an additive matrix corresponding to $T_{\\mathrm{alt}}$, NJ will correctly reconstruct the topology of $T_{\\mathrm{alt}}$.\n- Reconstructed Splits $S(\\widehat{T}^{(4)})$: The splits of $T_{\\mathrm{alt}}$ are $\\{A,C\\}|\\{B,D,E,F\\}$, $\\{B,D\\}|\\{A,C,E,F\\}$, and $\\{E,F\\}|\\{A,B,C,D\\}$. Canonically, $S(\\widehat{T}^{(4)}) = \\{\\{A,C\\}, \\{B,D\\}, \\{E,F\\}\\}$.\n- RF Distance $R^{(4)}$: We compare $S(T^{(4)})$ and $S(\\widehat{T}^{(4)})$. The common split is $\\{E,F\\}$. The symmetric difference is $\\{\\{A,B\\}, \\{C,D\\}\\} \\cup \\{\\{A,C\\}, \\{B,D\\}\\}$. The size of this set is $4$. Thus, $R^{(4)} = 4$.\n\n**Case 5: Three leaves, small noise**\n- True Tree $T^{(5)}$: A star tree on three leaves $\\{A,B,C\\}$.\n- An unrooted tree on $3$ leaves has no internal edges, and thus no non-trivial splits (a non-trivial split requires at least $2$ leaves in each partition).\n- True Splits $S(T^{(5)}) = \\emptyset$.\n- Reconstructed Tree $\\widehat{T}^{(5)}$: Any distance matrix on $3$ leaves will produce a tree with the same star topology when processed by NJ. The branch lengths may change, but the topology is invariant.\n- Reconstructed Splits $S(\\widehat{T}^{(5)}) = \\emptyset$.\n- RF Distance $R^{(5)}$: $| \\emptyset \\Delta \\emptyset | = 0$.\n\n**Final Results**\nThe calculated Robinson-Foulds distances for the five cases are:\n$R^{(1)} = 0$\n$R^{(2)} = 0$\n$R^{(3)} = 1$\n$R^{(4)} = 4$\n$R^{(5)} = 0$\n\nThe final output is the list $[0,0,1,4,0]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import defaultdict\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the simulation for all test cases.\n    \"\"\"\n\n    def get_pairwise_distances_from_tree(leaves, tree_edges):\n        \"\"\"\n        Computes the pairwise distance matrix for leaves from a tree definition.\n        \n        Args:\n            leaves (list): A list of leaf labels.\n            tree_edges (list): A list of tuples (node1, node2, weight) representing edges.\n\n        Returns:\n            A tuple (dist_matrix, label_to_idx, idx_to_label)\n        \"\"\"\n        adj = defaultdict(list)\n        nodes = set(leaves)\n        for u, v, w in tree_edges:\n            adj[u].append((v, w))\n            adj[v].append((u, w))\n            nodes.add(u)\n            nodes.add(v)\n        \n        label_to_idx = {label: i for i, label in enumerate(leaves)}\n        idx_to_label = {i: label for i, label in enumerate(leaves)}\n        n = len(leaves)\n        dist_matrix = np.zeros((n, n))\n\n        for i in range(n):\n            start_node = idx_to_label[i]\n            q = [(start_node, 0)]\n            visited = {start_node}\n            distances = {node: float('inf') for node in nodes}\n            distances[start_node] = 0\n\n            head = 0\n            while head < len(q):\n                curr, dist = q[head]\n                head += 1\n                \n                for neighbor, weight in adj[curr]:\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        new_dist = dist + weight\n                        distances[neighbor] = new_dist\n                        q.append((neighbor, new_dist))\n\n            for j in range(i, n):\n                end_node = idx_to_label[j]\n                dist = distances[end_node]\n                dist_matrix[i, j] = dist_matrix[j, i] = dist\n        \n        return dist_matrix, label_to_idx, idx_to_label\n\n    def neighbor_joining(dist_matrix_in, initial_labels):\n        \"\"\"\n        Implements the Neighbor-Joining algorithm from first principles.\n\n        Args:\n            dist_matrix_in (np.ndarray): The input distance matrix.\n            initial_labels (list): The list of leaf labels corresponding to the matrix.\n\n        Returns:\n            A defaultdict representing the adjacency list of the reconstructed tree.\n        \"\"\"\n        D = dist_matrix_in.copy()\n        labels = list(initial_labels)\n        adj = defaultdict(list)\n        n_nodes = len(labels)\n        \n        next_new_node_id = 0\n\n        while len(labels) > 2:\n            r = len(labels)\n            \n            # Compute Q-matrix\n            Q = np.zeros_like(D)\n            sums = D.sum(axis=1)\n            for i in range(r):\n                for j in range(i + 1, r):\n                    Q[i, j] = Q[j, i] = (r - 2) * D[i, j] - sums[i] - sums[j]\n            np.fill_diagonal(Q, float('inf'))\n            \n            # Find pair (i, j) that minimizes Q\n            min_q = Q.min()\n            min_pairs = []\n            for i in range(r):\n                for j in range(i+1, r):\n                    if np.isclose(Q[i,j], min_q):\n                        # Canonicalize pair by label for tie-breaking\n                        l1, l2 = sorted((labels[i], labels[j]))\n                        min_pairs.append(((l1, l2), (i, j)))\n\n            min_pairs.sort()\n            _, (i, j) = min_pairs[0]\n            if i > j:\n                i, j = j, i # Ensure i < j\n\n            # Create new node u\n            u_label = f\"U_{next_new_node_id}\"\n            next_new_node_id += 1\n\n            # Calculate branch lengths\n            dist_ij = D[i, j]\n            len_iu = 0.5 * dist_ij + (sums[i] - sums[j]) / (2 * (r - 2))\n            len_ju = dist_ij - len_iu\n            \n            # Clamp to 0\n            len_iu = max(0, len_iu)\n            len_ju = max(0, len_ju)\n\n            l_i, l_j = labels[i], labels[j]\n            adj[u_label].extend([(l_i, len_iu), (l_j, len_ju)])\n            adj[l_i].append((u_label, len_iu))\n            adj[l_j].append((u_label, len_ju))\n\n            # Update distance matrix\n            new_D = np.zeros((r - 1, r - 1))\n            new_labels = []\n            \n            # Calculate distances from new node u to others k\n            dist_u_k = {}\n            new_idx_map = {} \n            c = 0\n            for k in range(r):\n                if k not in (i, j):\n                    dist_u_k[k] = 0.5 * (D[i, k] + D[j, k] - dist_ij)\n                    new_labels.append(labels[k])\n                    new_idx_map[k] = c\n                    c += 1\n            \n            # Fill new matrix\n            for k1_orig in range(r):\n                if k1_orig in (i, j): continue\n                for k2_orig in range(k1_orig + 1, r):\n                    if k2_orig in (i,j): continue\n                    k1_new, k2_new = new_idx_map[k1_orig], new_idx_map[k2_orig]\n                    new_D[k1_new, k2_new] = new_D[k2_new, k1_new] = D[k1_orig, k2_orig]\n            \n            u_new_idx = r-2\n            for k_orig, dist_val in dist_u_k.items():\n                k_new = new_idx_map[k_orig]\n                new_D[u_new_idx, k_new] = new_D[k_new, u_new_idx] = dist_val\n            \n            new_labels.append(u_label)\n            \n            D = new_D\n            labels = new_labels\n\n        # Connect the last two nodes\n        l1, l2 = labels[0], labels[1]\n        dist = D[0, 1]\n        adj[l1].append((l2, dist))\n        adj[l2].append((l1, dist))\n\n        return adj\n\n    def get_nontrivial_splits(tree_adj, leaves):\n        \"\"\"\n        Computes the set of non-trivial splits for a given tree.\n\n        Args:\n            tree_adj (defaultdict): Adjacency list of the tree.\n            leaves (list): The list of leaf labels.\n\n        Returns:\n            A set of frozensets representing the canonical splits.\n        \"\"\"\n        splits = set()\n        num_leaves = len(leaves)\n        \n        internal_nodes = {node for node in tree_adj if node not in leaves}\n        if not internal_nodes: return set()\n        \n        internal_edges = set()\n        for u in internal_nodes:\n            for v, _ in tree_adj[u]:\n                if v in internal_nodes:\n                    internal_edges.add(frozenset([u,v]))\n\n        for edge in internal_edges:\n            u, v = tuple(edge)\n            \n            # Get one side of the split by traversing from u without crossing to v\n            q = [u]\n            visited = {u, v}\n            partition1_leaves = set()\n            \n            head = 0\n            while head < len(q):\n                curr = q[head]\n                head += 1\n                if curr in leaves:\n                    partition1_leaves.add(curr)\n                \n                for neighbor, _ in tree_adj[curr]:\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        q.append(neighbor)\n            \n            if 2 <= len(partition1_leaves) <= num_leaves - 2:\n                # Canonical representation: the smaller partition\n                partition2_leaves = set(leaves) - partition1_leaves\n                if len(partition1_leaves) < len(partition2_leaves):\n                    splits.add(frozenset(partition1_leaves))\n                elif len(partition2_leaves) < len(partition1_leaves):\n                    splits.add(frozenset(partition2_leaves))\n                else: # same size, use lexicographical order\n                    if sorted(list(partition1_leaves)) < sorted(list(partition2_leaves)):\n                        splits.add(frozenset(partition1_leaves))\n                    else:\n                        splits.add(frozenset(partition2_leaves))\n\n        return splits\n\n    def robinson_foulds_distance(tree1_adj, tree2_adj, leaves):\n        \"\"\"Computes the Robinson-Foulds distance between two trees.\"\"\"\n        splits1 = get_nontrivial_splits(tree1_adj, leaves)\n        splits2 = get_nontrivial_splits(tree2_adj, leaves)\n        return len(splits1.symmetric_difference(splits2))\n        \n    # --- Test Cases ---\n    \n    test_cases = []\n\n    # Case 1\n    leaves1 = ['A', 'B', 'C', 'D']\n    edges1 = [('A','X',0.30), ('B','X',0.35), ('X','Z',0.25), \n              ('C','Y',0.31), ('D','Y',0.33), ('Y','Z',0.27)]\n    noise1 = np.zeros((4,4))\n    test_cases.append({'leaves': leaves1, 'edges': edges1, 'noise': noise1})\n    \n    # Case 2\n    leaves2 = ['A', 'B', 'C', 'D', 'E']\n    edges2 = [('A','X',0.12), ('B','X',0.13), ('X','Z',0.20), ('C','Z',0.11),\n              ('D','Y',0.10), ('E','Y',0.09), ('Y','Z',0.18)]\n    noise2 = np.zeros((5,5))\n    test_cases.append({'leaves': leaves2, 'edges': edges2, 'noise': noise2})\n\n    # Case 3\n    noise3_vals = [+0.003, +0.004, -0.002, +0.001, +0.002, -0.001, +0.000, -0.002, +0.001, -0.003]\n    noise3 = np.zeros((5,5))\n    k=0\n    for i in range(5):\n        for j in range(i+1, 5):\n            noise3[i, j] = noise3[j, i] = noise3_vals[k]\n            k+=1\n    test_cases.append({'leaves': leaves2, 'edges': edges2, 'noise': noise3})\n\n    # Case 4\n    leaves4 = ['A', 'B', 'C', 'D', 'E', 'F']\n    edges_true4 = [('A','X',0.10), ('B','X',0.11), ('X','Z',0.20),\n                   ('C','Y',0.095),('D','Y',0.105),('Y','Z',0.19),\n                   ('E','W',0.09), ('F','W',0.11), ('W','Z',0.21)]\n    edges_alt4 = [('A','Xp',0.10),('C','Xp',0.095),('Xp','Zp',0.20),\n                  ('B','Yp',0.11),('D','Yp',0.105),('Yp','Zp',0.19),\n                  ('E','Wp',0.09),('F','Wp',0.11),('Wp','Zp',0.21)]\n    dist_true4, _, _ = get_pairwise_distances_from_tree(leaves4, edges_true4)\n    dist_alt4, _, _ = get_pairwise_distances_from_tree(leaves4, edges_alt4)\n    noise4 = dist_alt4 - dist_true4\n    test_cases.append({'leaves': leaves4, 'edges': edges_true4, 'noise': noise4})\n\n    # Case 5\n    leaves5 = ['A', 'B', 'C']\n    edges5 = [('A','Z',0.20), ('B','Z',0.30), ('C','Z',0.25)]\n    noise5 = np.zeros((3,3))\n    noise5[0,1] = noise5[1,0] = +0.010\n    noise5[0,2] = noise5[2,0] = -0.010\n    noise5[1,2] = noise5[2,1] = +0.000\n    test_cases.append({'leaves': leaves5, 'edges': edges5, 'noise': noise5})\n\n    # --- Processing ---\n\n    results = []\n    for case in test_cases:\n        leaves = case['leaves']\n        true_edges = case['edges']\n        noise = case['noise']\n        \n        # Build true tree adj list\n        true_tree_adj = defaultdict(list)\n        for u, v, w in true_edges:\n            true_tree_adj[u].append((v, w))\n            true_tree_adj[v].append((u, w))\n            \n        # Get true distance matrix and add noise\n        dist_true, _, _ = get_pairwise_distances_from_tree(leaves, true_edges)\n        dist_noised = dist_true + noise\n        \n        # Reconstruct tree using NJ\n        reconstructed_tree_adj = neighbor_joining(dist_noised, sorted(leaves))\n        \n        # Compute RF distance\n        rf_dist = robinson_foulds_distance(true_tree_adj, reconstructed_tree_adj, sorted(leaves))\n        results.append(rf_dist)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2701760"}, {"introduction": "This final practice integrates the concepts of distance modeling and tree reconstruction into a complete, realistic bioinformatics workflow. You will implement a cross-validation framework to objectively select the best-fitting nucleotide substitution model ($p$-distance, JC69, or K80) for a given sequence alignment [@problem_id:2701738]. This advanced exercise demonstrates how to move beyond simply applying an algorithm to a given distance matrix, instead addressing the critical preliminary step of how to derive meaningful distances from raw sequence data and statistically validate your modeling choices.", "problem": "Design and implement a complete program that evaluates, by cross-validation, which of several nucleotide substitution distance models provides distances that are most additive on a tree inferred by the Neighbor-Joining (NJ) algorithm. The central objective is to formalize the following logic: if a distance model and its estimated distances are compatible with a tree-like evolutionary history under a homogeneous and stationary substitution process, then training-set distances computed under that model should yield an NJ tree whose edge-length sums generalize to predict test-set distances for the same taxa. Your program must, for each dataset in the test suite below, select the model with the smallest cross-validated squared prediction error and report the selected model indices.\n\nStart from the following foundational base:\n- An alignment of deoxyribonucleic acid (DNA) sequences for $n$ taxa provides, for each taxon pair $(i,j)$ and site $s$, a pair of nucleotides in $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$. The empirical proportion of differences $p_{ij}$ between taxa $i$ and $j$ over a chosen set of sites is the fraction of sites where their nucleotides differ.\n- The Neighbor-Joining (NJ) algorithm is a distance-based tree reconstruction method that, given a symmetric pairwise distance matrix $D=\\left(d_{ij}\\right)$ with $d_{ii}=0$, iteratively agglomerates a pair of taxa or clusters to minimize a specific criterion derived from the principle of additive distances on trees. The method outputs an unrooted tree topology on the leaf set together with edge lengths that are additive with respect to the input distances in the ideal case.\n- Under a homogeneous and stationary continuous-time Markov chain model of substitutions on $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$, the Jukes–Cantor 1969 (JC69) model and the Kimura two-parameter 1980 (K80) model provide corrected distances as deterministic functions of empirical quantities computed from aligned sites. The K80 model distinguishes transitions (purine $\\leftrightarrow$ purine, pyrimidine $\\leftrightarrow$ pyrimidine) from transversions (purine $\\leftrightarrow$ pyrimidine). The uncorrected $p$-distance is the raw fraction of differing sites.\n\nYou must implement the following procedure for each dataset and for each distance model $M \\in \\{\\text{p-distance, Jukes-Cantor 1969 (JC69), Kimura two-parameter 1980 (K80)}\\}$:\n- Partition the $L$ alignment columns into two disjoint folds by index parity using $1$-based indexing: the odd-indexed sites $S_{\\text{odd}}=\\{s \\in \\{1,\\dots,L\\}: s \\bmod 2 = 1\\}$ and the even-indexed sites $S_{\\text{even}}=\\{s \\in \\{1,\\dots,L\\}: s \\bmod 2 = 0\\}$.\n- For each fold $f \\in \\{\\text{odd},\\text{even}\\}$:\n  1. Compute the pairwise training distance matrix $D^{\\text{train}}_{M,f}=\\left(d^{\\text{train}}_{M,f}(i,j)\\right)$ using only sites in $S_f$.\n  2. Reconstruct an unrooted NJ tree $T_f$ on the $n$ taxa from $D^{\\text{train}}_{M,f}$.\n  3. Estimate nonnegative edge lengths by solving the following linear least-squares problem without constraints on signs: let $E_f$ be the set of edges of $T_f$, and for each leaf pair $(i,j)$, let $P_{ij} \\subseteq E_f$ be the set of edges on the unique simple path between $i$ and $j$ in $T_f$. Find edge-lengths $\\{x_e: e \\in E_f\\}$ that minimize\n     $$\\sum_{1 \\le i < j \\le n} \\left(\\sum_{e \\in P_{ij}} x_e - d^{\\text{train}}_{M,f}(i,j)\\right)^2.$$\n     Denote an optimizer by $\\hat{x}^{(f)}=\\{\\hat{x}^{(f)}_e: e \\in E_f\\}$.\n  4. For the test fold $\\bar{f}$, predict distances for each leaf pair $(i,j)$ by tree path-lengths\n     $$\\hat{d}^{\\text{test}}_{M,f}(i,j) = \\sum_{e \\in P_{ij}} \\hat{x}^{(f)}_e,$$\n     and compute the empirical test distances $d^{\\text{test}}_{M,\\bar{f}}(i,j)$ from the sites in $S_{\\bar{f}}$.\n  5. Compute the fold error\n     $$\\mathrm{SSE}_{M,f} = \\sum_{1 \\le i < j \\le n} \\left(\\hat{d}^{\\text{test}}_{M,f}(i,j) - d^{\\text{test}}_{M,\\bar{f}}(i,j)\\right)^2.$$\n- Aggregate the two folds to obtain the cross-validated error\n  $$\\mathrm{Err}(M) = \\mathrm{SSE}_{M,\\text{odd}} + \\mathrm{SSE}_{M,\\text{even}}.$$\n\nFor each dataset, select the model index $m^\\star \\in \\{0,1,2\\}$ that minimizes $\\mathrm{Err}(M)$ with the tie-breaking rule $0 \\prec 1 \\prec 2$ (i.e., prefer p-distance over JC69 over K80 in case of equal errors). The final program output must be a single line containing the selected indices for all datasets as a comma-separated list enclosed in square brackets.\n\nDistance models to implement from first principles:\n- p-distance: for a taxon pair $(i,j)$ on a site set $S$, let $L_S$ be the number of sites considered and $m_S(i,j)$ be the number of sites at which $i$ and $j$ differ; then $d_p(i,j)=m_S(i,j)/L_S$.\n- Jukes-Cantor 1969 (JC69): a function $d_{\\mathrm{JC69}}(i,j)$ of $p_{ij}$ derived under equal base frequencies and equal substitution rates.\n- Kimura two-parameter 1980 (K80): a function $d_{\\mathrm{K80}}(i,j)$ of the proportions of transitions $P_{ij}$ and transversions $Q_{ij}$.\n\nSequence generation rule for the test suite:\n- Each dataset is specified by a base sequence of length $L$ over $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ and, for each taxon, a finite list of desired point changes of the form $(\\ell, \\nu)$ with position $\\ell \\in \\{1,\\dots,L\\}$ and replacement nucleotide $\\nu \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$. To obtain a taxon’s sequence from the base, scan the list in order and, at each $(\\ell,\\nu)$, set position $\\ell$ to $\\nu$; if the current nucleotide already equals $\\nu$, replace it instead by the next nucleotide under the cyclic order $\\mathrm{A} \\to \\mathrm{C} \\to \\mathrm{G} \\to \\mathrm{T} \\to \\mathrm{A}$, applying the cycle repeatedly until it differs. This deterministic tie-handling ensures that every listed change produces a mutation event.\n\nTest suite:\n- Dataset $1$:\n  - Base sequence (length $L=60$): $\\mathrm{ATGCTACGATCGTACGATCGATCGTACGTAGCTAGCTAGGCTTACGATCGATCGTACGTA}$.\n  - Taxa and their changes:\n    - Taxon $1$: $(10,\\mathrm{G}),(20,\\mathrm{T}),(30,\\mathrm{A}),(40,\\mathrm{C})$.\n    - Taxon $2$: $(10,\\mathrm{G}),(20,\\mathrm{T}),(31,\\mathrm{G}),(41,\\mathrm{T})$.\n    - Taxon $3$: $(11,\\mathrm{C}),(21,\\mathrm{A}),(32,\\mathrm{C}),(42,\\mathrm{A})$.\n    - Taxon $4$: $(11,\\mathrm{C}),(21,\\mathrm{A}),(12,\\mathrm{A}),(22,\\mathrm{G}),(33,\\mathrm{T}),(43,\\mathrm{G})$.\n    - Taxon $5$: $(11,\\mathrm{C}),(21,\\mathrm{A}),(12,\\mathrm{A}),(22,\\mathrm{G}),(34,\\mathrm{A}),(44,\\mathrm{C})$.\n- Dataset $2$:\n  - Base sequence (length $L=60$): the $15$-fold repetition of $\\mathrm{ACGT}$.\n  - Taxa and their changes:\n    - Taxon $1$: no changes.\n    - Taxon $2$: $(7,\\mathrm{G})$.\n    - Taxon $3$: $(7,\\mathrm{G}),(13,\\mathrm{A})$.\n    - Taxon $4$: no changes.\n    - Taxon $5$: $(59,\\mathrm{T})$.\n- Dataset $3$:\n  - Base sequence (length $L=60$): the $15$-fold repetition of $\\mathrm{ACGT}$.\n  - Taxa and their changes (favoring transitions for several taxa):\n    - Taxon $1$: $(1,\\mathrm{G}),(5,\\mathrm{G}),(9,\\mathrm{G}),(13,\\mathrm{G}),(17,\\mathrm{G}),(21,\\mathrm{G}),(25,\\mathrm{G}),(29,\\mathrm{G}),(2,\\mathrm{T}),(6,\\mathrm{T}),(10,\\mathrm{T}),(14,\\mathrm{T})$.\n    - Taxon $2$: same as Taxon $1$ plus $(33,\\mathrm{G}),(37,\\mathrm{G}),(41,\\mathrm{G}),(45,\\mathrm{G})$.\n    - Taxon $3$: $(3,\\mathrm{A}),(4,\\mathrm{C}),(11,\\mathrm{A}),(12,\\mathrm{C})$.\n    - Taxon $4$: transversion-heavy additions $(7,\\mathrm{C}),(8,\\mathrm{G}),(16,\\mathrm{G})$.\n    - Taxon $5$: $(2,\\mathrm{T}),(18,\\mathrm{T}),(34,\\mathrm{T}),(50,\\mathrm{T}),(1,\\mathrm{G}),(17,\\mathrm{G})$.\n\nImplementation requirements:\n- Implement distance computation for all three models on an arbitrary site set $S$, NJ reconstruction from a full symmetric distance matrix with $0$ diagonal, least-squares edge-length estimation on the NJ topology using all $\\binom{n}{2}$ leaf pairs, and the fold-based cross-validation as declared above.\n- Angles are not involved; there are no physical units. All reported numbers are unitless.\n- For each dataset, output the selected model index $m^\\star \\in \\{0,1,2\\}$ as an integer, using the convention $0=\\text{p-distance}, 1=\\text{JC69}, 2=\\text{K80}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$).\n\nThe test suite is fully specified by the three datasets above; you must embed these data in your program and produce the final line described. No user input is permitted, and there must be no randomness in fold creation or any other component of the computation.", "solution": "The problem requires the design and implementation of a computational framework to perform model selection among three phylogenetic distance measures—p-distance, Jukes–Cantor 1969 (JC69), and Kimura 1980 (K80)—based on a two-fold cross-validation scheme. The core principle being tested is the additivity of distances on a phylogenetic tree. An optimal model should produce distances from one subset of data (the training set) that, when used to infer a tree and its edge lengths, can accurately predict the distances computed from a disjoint subset of data (the test set). The model that minimizes the cross-validated sum of squared prediction errors is selected.\n\nThe complete procedure is deterministic and involves several standard algorithms from computational biology and numerical analysis, which are detailed below.\n\n### Step 1: Sequence Data Generation\n\nFor each test case, a set of $n$ aligned DNA sequences of length $L$ is generated algorithmically from a given base sequence and a list of specified mutations for each taxon. The mutations for a given taxon are applied sequentially to the base sequence. A special rule handles redundant mutations: if a requested change $(\\ell, \\nu)$ at position $\\ell$ would not alter the nucleotide $\\nu_{current}$ (i.e., $\\nu = \\nu_{current}$), the nucleotide is instead replaced by the next one in the cyclic order $\\mathrm{A} \\to \\mathrm{C} \\to \\mathrm{G} \\to \\mathrm{T} \\to \\mathrm{A}$. This ensures every specified operation results in a substitution.\n\n### Step 2: Phylogenetic Distance Models\n\nFor any pair of taxa $(i,j)$ and a given set of alignment sites $S$, three distance metrics are computed. Let $L_S = |S|$ be the number of sites in the set.\n\n- **p-distance ($M=0$)**: This is the observed proportion of differing sites. Let $N_d(i, j, S)$ be the number of sites in $S$ where the sequences of taxa $i$ and $j$ differ. The p-distance is:\n$$d_p(i, j) = \\frac{N_d(i, j, S)}{L_S}$$\n\n- **Jukes–Cantor 1969 (JC69) distance ($M=1$)**: This model corrects for multiple substitutions at the same site under the assumption of equal base frequencies and equal substitution rates. The distance is a function of the p-distance, $p = d_p(i, j)$:\n$$d_{\\text{JC69}}(i, j) = -\\frac{3}{4} \\ln\\left(1 - \\frac{4}{3}p\\right)$$\nThis distance is defined only for $p < 3/4$. If $p \\ge 3/4$, the number of substitutions is considered to have saturated, and the distance is taken to be infinite.\n\n- **Kimura two-parameter 1980 (K80) distance ($M=2$)**: This model is more complex, distinguishing between transitions (substitutions between purines, {A,G}, or between pyrimidines, {C,T}) and transversions (substitutions between a purine and a pyrimidine). Let $P$ be the proportion of sites with transitional differences and $Q$ be the proportion of sites with transversional differences. The K80 distance is:\n$$d_{\\text{K80}}(i, j) = -\\frac{1}{2}\\ln(1 - 2P - Q) - \\frac{1}{4}\\ln(1 - 2Q)$$\nThis distance is defined only when the arguments of both logarithms are positive, i.e., $1 - 2P - Q > 0$ and $1 - 2Q > 0$. If either condition is violated, the distance is considered infinite.\n\n### Step 3: Neighbor-Joining (NJ) Algorithm\n\nThe Neighbor-Joining (NJ) algorithm is a bottom-up clustering method that reconstructs an unrooted tree topology from a symmetric distance matrix $D = (d_{ij})$. It iteratively joins pairs of taxa (or clusters) that are close to each other while being far from the rest. The criterion for selecting the pair $(i, j)$ to join is the minimization of the $Q$-matrix, defined for a set of $k$ active clusters as:\n$$Q_{ij} = (k-2)d_{ij} - \\sum_{m=1}^{k} d_{im} - \\sum_{m=1}^{k} d_{jm}$$\nOnce a pair $(i, j)$ is selected, they are connected to a newly created internal node $u$. The distance matrix is then updated by removing $i$ and $j$ and adding $u$, with distances from $u$ to any other cluster $m$ calculated as $d_{um} = \\frac{1}{2}(d_{im} + d_{jm} - d_{ij})$. This process is repeated until only two clusters remain, which are then joined to form the final edge, completing the unrooted tree topology. For $n$ taxa, the resulting tree has $n$ leaves, $n-2$ internal nodes, and $2n-3$ edges.\n\n### Step 4: Edge Length Estimation via Linear Least Squares\n\nGiven a tree topology $T_f$ from the NJ algorithm on a training distance matrix $D^{\\text{train}}$, the lengths of the edges in $T_f$ are estimated. For an ideal additive tree, the distance between any two leaves $i$ and $j$ is the sum of the lengths of the edges on the unique path $P_{ij}$ connecting them. We seek edge lengths $\\mathbf{x} = \\{x_e\\}_{e \\in E_f}$ that best fit the training distances in a least-squares sense. This is formulated as a linear regression problem:\n$$\\underset{\\mathbf{x}}{\\text{minimize}} \\sum_{1 \\le i < j \\le n} \\left(\\left(\\sum_{e \\in P_{ij}} x_e\\right) - d^{\\text{train}}(i, j)\\right)^2$$\nThis can be written in matrix form as $\\underset{\\mathbf{x}}{\\text{minimize}} \\|A\\mathbf{x} - \\mathbf{b}\\|^2$, where:\n- $\\mathbf{x}$ is the vector of $2n-3$ unknown edge lengths.\n- $\\mathbf{b}$ is the vector of $\\binom{n}{2}$ training distances $d^{\\text{train}}(i, j)$.\n- $A$ is the $\\binom{n}{2} \\times (2n-3)$ design matrix, where $A_{k,e} = 1$ if edge $e$ is on the path corresponding to the $k$-th pair of leaves, and $0$ otherwise.\n\nThis unconstrained linear least-squares problem is solved for $\\hat{\\mathbf{x}}$. Though biologically edge lengths are non-negative, the problem explicitly instructs to solve without sign constraints, which simplifies the optimization.\n\n### Step 5: Cross-Validation and Model Selection\n\nA two-fold cross-validation procedure is employed to evaluate each model. The $L$ alignment sites are partitioned into odd-indexed sites $S_{\\text{odd}}$ and even-indexed sites $S_{\\text{even}}$.\n\nThe procedure is symmetric for two folds:\n1.  **Fold 1**: Train on $S_{\\text{odd}}$, test on $S_{\\text{even}}$.\n    -   Compute the training distance matrix $D^{\\text{train}}_{M, \\text{odd}}$ using sites in $S_{\\text{odd}}$.\n    -   Infer the NJ tree $T_{\\text{odd}}$ from $D^{\\text{train}}_{M, \\text{odd}}$.\n    -   Estimate edge lengths $\\hat{\\mathbf{x}}^{(\\text{odd})}$ on $T_{\\text{odd}}$.\n    -   Predict test distances $\\hat{d}^{\\text{test}}_{M, \\text{odd}}(i, j)$ for all pairs $(i,j)$ by summing the estimated edge lengths along paths in $T_{\\text{odd}}$.\n    -   Compute the \"true\" test distances $d^{\\text{test}}_{M, \\text{even}}(i, j)$ using sites in $S_{\\text{even}}$.\n    -   Calculate the sum of squared errors: $\\mathrm{SSE}_{M, \\text{odd}} = \\sum_{1 \\le i < j \\le n} (\\hat{d}^{\\text{test}}_{M, \\text{odd}}(i, j) - d^{\\text{test}}_{M, \\text{even}}(i, j))^2$.\n\n2.  **Fold 2**: Train on $S_{\\text{even}}$, test on $S_{\\text{odd}}$. This is executed analogously to Fold 1, yielding $\\mathrm{SSE}_{M, \\text{even}}$.\n\nThe total error for a model $M$ is the sum of errors from both folds:\n$$\\mathrm{Err}(M) = \\mathrm{SSE}_{M, \\text{odd}} + \\mathrm{SSE}_{M, \\text{even}}$$\nIf any distance calculation for a training set results in an infinite value, the model is considered unsuitable for that fold, and its error contribution is treated as infinite.\n\nFor each dataset, the model index $m^\\star \\in \\{0, 1, 2\\}$ with the minimum $\\mathrm{Err}(M)$ is chosen. Ties are broken by preferring simpler models: p-distance ($0$) over JC69 ($1$), and JC69 over K80 ($2$).", "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef solve():\n    \"\"\"\n    Main function to run the cross-validation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"base_seq\": \"ATGCTACGATCGTACGATCGATCGTACGTAGCTAGCTAGGCTTACGATCGATCGTACGTA\",\n            \"taxa_changes\": {\n                \"Taxon 1\": [(10, 'G'), (20, 'T'), (30, 'A'), (40, 'C')],\n                \"Taxon 2\": [(10, 'G'), (20, 'T'), (31, 'G'), (41, 'T')],\n                \"Taxon 3\": [(11, 'C'), (21, 'A'), (32, 'C'), (42, 'A')],\n                \"Taxon 4\": [(11, 'C'), (21, 'A'), (12, 'A'), (22, 'G'), (33, 'T'), (43, 'G')],\n                \"Taxon 5\": [(11, 'C'), (21, 'A'), (12, 'A'), (22, 'G'), (34, 'A'), (44, 'C')],\n            }\n        },\n        {\n            \"base_seq\": \"ACGT\" * 15,\n            \"taxa_changes\": {\n                \"Taxon 1\": [],\n                \"Taxon 2\": [(7, 'G')],\n                \"Taxon 3\": [(7, 'G'), (13, 'A')],\n                \"Taxon 4\": [],\n                \"Taxon 5\": [(59, 'T')],\n            }\n        },\n        {\n            \"base_seq\": \"ACGT\" * 15,\n            \"taxa_changes\": {\n                \"Taxon 1\": [(1, 'G'), (5, 'G'), (9, 'G'), (13, 'G'), (17, 'G'), (21, 'G'), (25, 'G'), (29, 'G'),\n                            (2, 'T'), (6, 'T'), (10, 'T'), (14, 'T')],\n                \"Taxon 2\": [(1, 'G'), (5, 'G'), (9, 'G'), (13, 'G'), (17, 'G'), (21, 'G'), (25, 'G'), (29, 'G'),\n                            (2, 'T'), (6, 'T'), (10, 'T'), (14, 'T'),\n                            (33, 'G'), (37, 'G'), (41, 'G'), (45, 'G')],\n                \"Taxon 3\": [(3, 'A'), (4, 'C'), (11, 'A'), (12, 'C')],\n                \"Taxon 4\": [(7, 'C'), (8, 'G'), (16, 'G')],\n                \"Taxon 5\": [(2, 'T'), (18, 'T'), (34, 'T'), (50, 'T'),\n                            (1, 'G'), (17, 'G')],\n            }\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        sequences = _generate_sequences(case[\"base_seq\"], case[\"taxa_changes\"])\n        num_taxa = len(sequences)\n        num_sites = len(case[\"base_seq\"])\n        \n        sites_odd = list(range(0, num_sites, 2))\n        sites_even = list(range(1, num_sites, 2))\n        folds = [(sites_odd, sites_even), (sites_even, sites_odd)]\n        \n        model_errors = []\n        for model_idx in range(3):  # 0: p-dist, 1: JC69, 2: K80\n            total_sse = 0.0\n            \n            for train_sites, test_sites in folds:\n                D_train = _get_dist_matrix(sequences, train_sites, model_idx)\n                \n                if np.any(np.isinf(D_train)):\n                    total_sse = np.inf\n                    break\n\n                topology, nodes = _neighbor_joining(D_train, num_taxa)\n                edge_lengths = _estimate_edge_lengths(topology, nodes, D_train, num_taxa)\n                D_pred = _predict_distances(topology, nodes, edge_lengths, num_taxa)\n                D_test = _get_dist_matrix(sequences, test_sites, model_idx)\n                \n                sse = np.sum((D_pred - D_test)**2)\n                total_sse += sse\n\n            model_errors.append(total_sse)\n\n        best_model_idx = np.argmin(model_errors)\n        final_results.append(best_model_idx)\n        \n    print(f\"[{','.join(map(str, final_results))}]\")\n\n\ndef _generate_sequences(base_seq, taxa_changes):\n    \"\"\"Generates sequences from a base sequence and a list of changes.\"\"\"\n    seq_list = []\n    ordered_taxa = sorted(taxa_changes.keys())\n    \n    for taxon_name in ordered_taxa:\n        seq = list(base_seq)\n        changes = taxa_changes[taxon_name]\n        \n        cycle_map = {'A': 'C', 'C': 'G', 'G': 'T', 'T': 'A'}\n        \n        for pos, new_nuc in changes:\n            idx = pos - 1\n            current_nuc = seq[idx]\n            \n            if current_nuc == new_nuc:\n                mutated_nuc = cycle_map[current_nuc]\n                # The problem statement implies repeated application if needed.\n                while mutated_nuc == current_nuc:\n                    mutated_nuc = cycle_map[mutated_nuc]\n                seq[idx] = mutated_nuc\n            else:\n                seq[idx] = new_nuc\n        seq_list.append(\"\".join(seq))\n        \n    return seq_list\n\n\ndef _get_dist_matrix(sequences, sites, model_idx):\n    \"\"\"Computes a distance matrix for a given model.\"\"\"\n    n = len(sequences)\n    D = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            if model_idx == 0:\n                dist = _p_distance(sequences[i], sequences[j], sites)\n            elif model_idx == 1:\n                dist = _jc69_distance(sequences[i], sequences[j], sites)\n            elif model_idx == 2:\n                dist = _k80_distance(sequences[i], sequences[j], sites)\n            D[i, j] = D[j, i] = dist\n    return D\n\ndef _p_distance(seq1, seq2, sites):\n    \"\"\"Calculates p-distance.\"\"\"\n    diffs = sum(1 for s in sites if seq1[s] != seq2[s])\n    return diffs / len(sites) if sites else 0.0\n\ndef _jc69_distance(seq1, seq2, sites):\n    \"\"\"Calculates Jukes-Cantor 1969 distance.\"\"\"\n    p = _p_distance(seq1, seq2, sites)\n    if p >= 0.75:\n        return np.inf\n    return -0.75 * np.log(1.0 - (4.0 / 3.0) * p)\n\ndef _k80_distance(seq1, seq2, sites):\n    \"\"\"Calculates Kimura 2-parameter 1980 distance.\"\"\"\n    purines = {'A', 'G'}\n    transitions, transversions = 0, 0\n    for s in sites:\n        n1, n2 = seq1[s], seq2[s]\n        if n1 != n2:\n            is_n1_purine = n1 in purines\n            is_n2_purine = n2 in purines\n            if is_n1_purine == is_n2_purine:\n                transitions += 1\n            else:\n                transversions += 1\n    \n    if not sites:\n        return 0.0\n\n    P = transitions / len(sites)\n    Q = transversions / len(sites)\n    \n    arg1 = 1.0 - 2.0 * P - Q\n    arg2 = 1.0 - 2.0 * Q\n    \n    if arg1 <= 0 or arg2 <= 0:\n        return np.inf\n        \n    return -0.5 * np.log(arg1) - 0.25 * np.log(arg2)\n\ndef _neighbor_joining(D_matrix, num_taxa):\n    \"\"\"Performs Neighbor-Joining algorithm.\"\"\"\n    if num_taxa <= 2:\n        return [tuple(range(num_taxa))], list(range(num_taxa))\n\n    D = D_matrix.copy()\n    clusters = list(range(num_taxa))\n    new_node_idx = num_taxa\n    topology = []\n    \n    while len(clusters) > 2:\n        n = len(clusters)\n        \n        # Calculate Q-matrix\n        total_dist = D.sum(axis=0)\n        Q = (n - 2) * D - total_dist[:, np.newaxis] - total_dist[np.newaxis, :]\n        np.fill_diagonal(Q, np.inf)\n        \n        min_q_val = np.min(Q)\n        if np.isinf(min_q_val): # Disconnected components\n            # This can happen if distances are huge. Break arbitrary.\n            pair = np.unravel_index(np.argmin(D), D.shape)\n        else:\n            pair = np.unravel_index(np.argmin(Q), Q.shape)\n        i, j = pair\n        \n        # Original node labels from clusters list\n        c_i, c_j = clusters[i], clusters[j]\n        \n        # Add new edges to topology\n        topology.append((c_i, new_node_idx))\n        topology.append((c_j, new_node_idx))\n        \n        # Calculate distances from new node to others\n        dist_i = D[i, :]\n        dist_j = D[j, :]\n        dist_ij = D[i, j]\n        new_dist = 0.5 * (dist_i + dist_j - dist_ij)\n        \n        # Update distance matrix\n        D = np.delete(D, [i, j], axis=0)\n        D = np.delete(D, [i, j], axis=1)\n        \n        # Add new row and column for the new cluster\n        D_new = np.zeros((n-1, n-1))\n        D_new[:-1, :-1] = D\n        D_new[-1, :-1] = new_dist[np.arange(D.shape[0])]\n        D_new[:-1, -1] = new_dist[np.arange(D.shape[0])]\n        D = D_new\n        \n        # Update cluster list\n        clusters.pop(max(i, j))\n        clusters.pop(min(i, j))\n        clusters.append(new_node_idx)\n        \n        new_node_idx += 1\n        \n    topology.append(tuple(clusters))\n    \n    all_nodes = list(range(num_taxa)) + list(range(num_taxa, new_node_idx))\n    return topology, all_nodes\n\ndef _find_path(graph, start, end):\n    \"\"\"Finds the unique path between two nodes in a tree using BFS.\"\"\"\n    queue = [(start, [start])]\n    visited = {start}\n    while queue:\n        (vertex, path) = queue.pop(0)\n        for neighbor in graph[vertex]:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                new_path = list(path)\n                new_path.append(neighbor)\n                if neighbor == end:\n                    return new_path\n                queue.append((neighbor, new_path))\n    return []\n\ndef _estimate_edge_lengths(topology, nodes, D_train, num_taxa):\n    \"\"\"Estimates edge lengths using linear least squares.\"\"\"\n    num_leaves = num_taxa\n    \n    # Build graph from topology\n    graph = defaultdict(list)\n    for u, v in topology:\n        graph[u].append(v)\n        graph[v].append(u)\n\n    edges = sorted([tuple(sorted(edge)) for edge in topology])\n    edge_to_idx = {edge: i for i, edge in enumerate(edges)}\n    num_edges = len(edges)\n\n    leaf_pairs = [(i, j) for i in range(num_leaves) for j in range(i + 1, num_leaves)]\n    num_pairs = len(leaf_pairs)\n    \n    A = np.zeros((num_pairs, num_edges))\n    b = np.zeros(num_pairs)\n\n    for pair_idx, (leaf1, leaf2) in enumerate(leaf_pairs):\n        path_nodes = _find_path(graph, leaf1, leaf2)\n        b[pair_idx] = D_train[leaf1, leaf2]\n        \n        for k in range(len(path_nodes) - 1):\n            edge = tuple(sorted((path_nodes[k], path_nodes[k+1])))\n            if edge in edge_to_idx:\n                edge_idx = edge_to_idx[edge]\n                A[pair_idx, edge_idx] = 1\n\n    x, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n    \n    return {edge: x[i] for i, edge in enumerate(edges)}\n\ndef _predict_distances(topology, nodes, edge_lengths, num_taxa):\n    \"\"\"Predicts pairwise distances from a tree with edge lengths.\"\"\"\n    graph = defaultdict(list)\n    for u, v in topology:\n        graph[u].append(v)\n        graph[v].append(u)\n    \n    D_pred = np.zeros((num_taxa, num_taxa))\n    \n    for i in range(num_taxa):\n        for j in range(i + 1, num_taxa):\n            path_nodes = _find_path(graph, i, j)\n            path_dist = 0.0\n            for k in range(len(path_nodes) - 1):\n                edge = tuple(sorted((path_nodes[k], path_nodes[k+1])))\n                path_dist += edge_lengths.get(edge, 0.0)\n            D_pred[i, j] = D_pred[j, i] = path_dist\n            \n    return D_pred\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2701738"}]}