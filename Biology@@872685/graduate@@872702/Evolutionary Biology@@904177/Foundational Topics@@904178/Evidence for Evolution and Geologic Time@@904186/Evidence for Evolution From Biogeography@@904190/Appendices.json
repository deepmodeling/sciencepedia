{"hands_on_practices": [{"introduction": "The formation of volcanic archipelagos provides a natural laboratory for studying evolution in action. A key prediction, known as the progression rule, is that colonization and divergence events should proceed from older to younger islands, mirroring their geological formation. This exercise [@problem_id:2705037] provides a hands-on opportunity to quantitatively test this foundational hypothesis using Kendall's rank correlation coefficient, $\\tau$. By analyzing hypothetical but realistic data on island ages and clade colonization times, you will develop skills in applying robust non-parametric statistics to evaluate a core prediction of island biogeography.", "problem": "A volcanic hotspot archipelago has formed a linear chain of islands with well-constrained emergence ages. Under the progression rule of hotspot archipelagos in island biogeography, colonization or in situ divergence within a clade should proceed from older to younger islands, producing a monotonic association between island geological ages and colonization ages inferred from phylogenetic divergence times. Consider a focal endemic plant clade for which the colonization ages on each island have been estimated using a relaxed-clock phylogeny calibrated with fossils external to the archipelago.\n\nYou are given the following paired data for $n=8$ islands, where each pair lists the island’s emergence age in million years ago (million years ago (Ma)) and the estimated colonization age in Ma for the clade on that island. Each colonization event post-dates island emergence.\n\n- Island $1$: $(5.6,\\ 4.7)$\n- Island $2$: $(4.9,\\ 4.3)$\n- Island $3$: $(4.1,\\ 3.0)$\n- Island $4$: $(3.4,\\ 3.1)$\n- Island $5$: $(2.6,\\ 1.5)$\n- Island $6$: $(1.9,\\ 1.7)$\n- Island $7$: $(1.1,\\ 0.9)$\n- Island $8$: $(0.6,\\ 0.4)$\n\nAssume there are no ties in either variable and treat each island as an independent observational unit for the purposes of rank correlation. Using only rank information, compute Kendall’s $\\tau$ rank correlation between island ages and colonization ages to assess the progression rule. Then perform a two-sided significance test for Kendall’s $\\tau$ at significance level $\\alpha=0.05$ under the null hypothesis of no association, using the standard null variance for Kendall’s statistic without continuity correction.\n\nReport the value of Kendall’s $\\tau$ as your final answer, rounded to four significant figures. No units are required for the final answer.", "solution": "The problem presents a set of paired data for island emergence ages and clade colonization ages and asks for a statistical evaluation of the \"progression rule\" hypothesis using Kendall's rank correlation. The problem is scientifically grounded in island biogeography and presents a well-posed statistical question. It is free of contradictions, ambiguities, and factual errors. Therefore, it is a valid problem, and a solution will be furnished.\n\nThe objective is to compute Kendall's rank correlation coefficient, $\\tau$, and to test its statistical significance. Kendall's $\\tau$ measures the ordinal association between two measured quantities. For a set of $n$ pairs of observations $(x_i, y_i)$, Kendall's $\\tau$ is defined as:\n$$\n\\tau = \\frac{N_c - N_d}{\\frac{1}{2} n(n-1)}\n$$\nwhere $N_c$ is the number of concordant pairs and $N_d$ is the number of discordant pairs. The denominator is the total number of pairs of observations. A pair of observations, $(x_i, y_i)$ and $(x_j, y_j)$, is concordant if the ranks of both elements agree (i.e., if $x_i  x_j$ and $y_i  y_j$, or if $x_i  x_j$ and $y_i  y_j$). It is discordant if the ranks disagree (i.e., if $x_i  x_j$ and $y_i  y_j$, or if $x_i  x_j$ and $y_i  y_j$). The problem states there are no ties, simplifying the calculation.\n\nThe given data consists of $n=8$ pairs, with island emergence age as variable $X$ and colonization age as variable $Y$, both in millions of years ago (Ma). The pairs are: $(5.6, 4.7)$, $(4.9, 4.3)$, $(4.1, 3.0)$, $(3.4, 3.1)$, $(2.6, 1.5)$, $(1.9, 1.7)$, $(1.1, 0.9)$, and $(0.6, 0.4)$.\n\nTo compute $N_c$ and $N_d$, we first order the data according to one variable, say $X$. The data are already provided in descending order of island age $X$.\n$X: 5.6  4.9  4.1  3.4  2.6  1.9  1.1  0.6$\nThe corresponding sequence of $Y$ values is:\n$Y: 4.7, 4.3, 3.0, 3.1, 1.5, 1.7, 0.9, 0.4$\n\nNow, we count for each $y_i$, the number of subsequent values $y_j$ (where $ji$) that are smaller (concordant) or larger (discordant).\n\n1.  For $y_1 = 4.7$: The subsequent values are $\\{4.3, 3.0, 3.1, 1.5, 1.7, 0.9, 0.4\\}$. All $7$ values are smaller.\n    Concordant count = $7$. Discordant count = $0$.\n\n2.  For $y_2 = 4.3$: The subsequent values are $\\{3.0, 3.1, 1.5, 1.7, 0.9, 0.4\\}$. All $6$ values are smaller.\n    Concordant count = $6$. Discordant count = $0$.\n\n3.  For $y_3 = 3.0$: The subsequent values are $\\{3.1, 1.5, 1.7, 0.9, 0.4\\}$. Values smaller are $\\{1.5, 1.7, 0.9, 0.4\\}$ ($4$ values). Value larger is $\\{3.1\\}$ ($1$ value).\n    Concordant count = $4$. Discordant count = $1$.\n\n4.  For $y_4 = 3.1$: The subsequent values are $\\{1.5, 1.7, 0.9, 0.4\\}$. All $4$ values are smaller.\n    Concordant count = $4$. Discordant count = $0$.\n\n5.  For $y_5 = 1.5$: The subsequent values are $\\{1.7, 0.9, 0.4\\}$. Values smaller are $\\{0.9, 0.4\\}$ ($2$ values). Value larger is $\\{1.7\\}$ ($1$ value).\n    Concordant count = $2$. Discordant count = $1$.\n\n6.  For $y_6 = 1.7$: The subsequent values are $\\{0.9, 0.4\\}$. Both $2$ values are smaller.\n    Concordant count = $2$. Discordant count = $0$.\n\n7.  For $y_7 = 0.9$: The subsequent value is $\\{0.4\\}$. This $1$ value is smaller.\n    Concordant count = $1$. Discordant count = $0$.\n\nThe total number of concordant pairs is the sum of the individual counts:\n$$\nN_c = 7 + 6 + 4 + 4 + 2 + 2 + 1 = 26\n$$\nThe total number of discordant pairs is:\n$$\nN_d = 0 + 0 + 1 + 0 + 1 + 0 + 0 = 2\n$$\nThe total number of pairs is $\\binom{n}{2} = \\binom{8}{2} = \\frac{8 \\times 7}{2} = 28$.\nAs a check, $N_c + N_d = 26 + 2 = 28$, which is correct.\n\nNow, we compute Kendall's $\\tau$:\n$$\n\\tau = \\frac{N_c - N_d}{N_c + N_d} = \\frac{26 - 2}{26 + 2} = \\frac{24}{28} = \\frac{6}{7}\n$$\nAs a decimal, $\\tau \\approx 0.8571428...$. Rounded to four significant figures, $\\tau = 0.8571$.\n\nNext, we perform the two-sided significance test.\nThe null hypothesis $H_0$ is that there is no association between the variables, i.e., $\\tau = 0$.\nThe alternative hypothesis $H_1$ is that there is an association, i.e., $\\tau \\neq 0$.\nThe significance level is $\\alpha = 0.05$.\n\nFor sample sizes $n  10$, the distribution of the statistic $S = N_c - N_d$ under $H_0$ is approximately normal. We will use this approximation as implicitly instructed.\nUnder $H_0$, the expected value of $S$ is $E[S] = 0$.\nThe variance of $S$ (for the case with no ties) is given by:\n$$\n\\sigma_S^2 = \\text{Var}(S) = \\frac{n(n-1)(2n+5)}{18}\n$$\nFor $n=8$, we calculate the variance:\n$$\n\\sigma_S^2 = \\frac{8(8-1)(2(8)+5)}{18} = \\frac{8 \\times 7 \\times (16+5)}{18} = \\frac{8 \\times 7 \\times 21}{18} = \\frac{1176}{18} = \\frac{196}{3}\n$$\nThe standard deviation is:\n$$\n\\sigma_S = \\sqrt{\\frac{196}{3}} = \\frac{14}{\\sqrt{3}} \\approx 8.083\n$$\nThe test statistic $Z$ is calculated without continuity correction, as specified:\n$$\nZ = \\frac{S - E[S]}{\\sigma_S} = \\frac{S}{\\sigma_S}\n$$\nOur observed value of $S$ is $N_c - N_d = 24$.\n$$\nZ = \\frac{24}{14/\\sqrt{3}} = \\frac{24\\sqrt{3}}{14} = \\frac{12\\sqrt{3}}{7} \\approx 2.969\n$$\nFor a two-sided test at $\\alpha = 0.05$, the critical values are $Z_{\\alpha/2} = \\pm 1.96$.\nSince our observed test statistic $|Z| \\approx 2.969$ is greater than the critical value $1.96$, we reject the null hypothesis $H_0$. The p-value for this test is $p = 2 \\times P(Z \\ge 2.969) \\approx 0.003$, which is much less than $\\alpha=0.05$.\nThe conclusion is that there is a statistically significant positive association between island age and colonization age, which is consistent with the progression rule.\n\nThe problem asks only for the value of Kendall's $\\tau$.\n$$\n\\tau = \\frac{6}{7} \\approx 0.8571\n$$", "answer": "$$\n\\boxed{0.8571}\n$$", "id": "2705037"}, {"introduction": "Moving beyond discrete islands, modern biogeography increasingly models species distributions across complex, continuous landscapes. In this context, simple Euclidean distance is a poor predictor of connectivity; instead, we use the concept of \"effective distance,\" which integrates the costs of traversing heterogeneous terrain. This practice [@problem_id:2705035] challenges you to compute effective distances from gridded resistance layers and then use Maximum Likelihood Estimation to fit a colonization model. This exercise combines graph theory algorithms with statistical inference to quantify the impact of landscape features on dispersal, a central task in landscape genetics and conservation biology.", "problem": "You are given discrete resistance landscapes and observed colonization outcomes between a single source site and multiple destination sites. The scientific premise is that colonization probability decays with effective distance, a biogeographic quantity that accounts for the cumulative cost of movement across a heterogeneous landscape. Assume the following generative model: for each source-destination pair indexed by $i$, the colonization outcome $Y_i \\in \\{0,1\\}$ is a Bernoulli trial with success probability $p_i$, and the probability decays with effective distance as $p_i = \\exp(-\\beta \\,\\mathrm{ED}_i)$, where $\\mathrm{ED}_i \\ge 0$ is the effective distance and $\\beta \\ge 0$ is an unknown parameter. Your task is to estimate $\\beta$ by maximum likelihood given the data below.\n\nFundamental base to use:\n- A colonization outcome $Y_i$ is modeled as a Bernoulli random variable with parameter $p_i$, and independent across $i$.\n- The joint likelihood of independent Bernoulli observations is the product of individual likelihoods.\n- The effective distance $\\mathrm{ED}$ between two cells on a raster is defined as the minimum cumulative movement cost over an $8$-connected grid, where the movement cost of stepping from one cell to a neighboring cell is the average of their cell costs multiplied by the Euclidean step length.\n- A cell’s cost is a nonnegative linear combination of resistance layers: if there are $K$ layers with cell values $L_{k}(r,c)$ and nonnegative weights $w_k$, then the total cell cost is $C(r,c) = \\sum_{k=1}^{K} w_k \\, L_k(r,c)$.\n\nEffective distance computation details:\n- The grid is indexed by $(r,c)$ with $r$ as zero-based row and $c$ as zero-based column.\n- Movement is allowed to the $8$ immediate neighbors. For orthogonal moves the step length is $1$, for diagonal moves the step length is $\\sqrt{2}$.\n- The step cost from cell $(r,c)$ to neighbor $(r',c')$ is $\\frac{C(r,c)+C(r',c')}{2} \\times d$, where $d$ is the Euclidean distance between the centers of the two cells, which is either $1$ or $\\sqrt{2}$.\n\nEstimation details:\n- Constrain $\\beta$ to the closed interval $[0,\\beta_{\\max}]$, with $\\beta_{\\max} = 10$. If the unconstrained maximum likelihood estimate lies outside, return the boundary value.\n- For degenerate datasets, if all $Y_i = 1$, return $\\beta = 0$. If all $Y_i = 0$, return $\\beta = \\beta_{\\max}$.\n- Your program must compute $\\mathrm{ED}_i$ values from the provided resistance layers using the rule above and then estimate $\\beta$ by maximizing the likelihood under the model $p_i = \\exp(-\\beta \\,\\mathrm{ED}_i)$.\n\nTest suite:\nImplement your solution on the following three test cases. In all cases, answer by reporting the maximum likelihood estimate of $\\beta$ as a float rounded to $6$ decimal places.\n\n- Test case A:\n  - Grid size: $5 \\times 5$.\n  - Layers ($L_1$, $L_2$) given as matrices with entries:\n    - $L_1 = \\begin{bmatrix}\n    1  1  2  3  4\\\\\n    1  2  2  3  4\\\\\n    1  2  3  3  4\\\\\n    2  2  3  4  5\\\\\n    3  3  4  5  5\n    \\end{bmatrix}$,\n    $L_2 = \\begin{bmatrix}\n    3  3  3  3  3\\\\\n    3  2  2  2  3\\\\\n    3  2  1  2  3\\\\\n    3  2  2  2  3\\\\\n    3  3  3  3  3\n    \\end{bmatrix}$.\n  - Weights: $w_1 = 0.6$, $w_2 = 0.4$.\n  - Source: $(0,0)$.\n  - Destinations: $(0,4)$, $(2,2)$, $(4,4)$, $(4,0)$, $(1,3)$.\n  - Observed outcomes $Y$: $[0,1,0,0,1]$.\n\n- Test case B:\n  - Grid size: $4 \\times 4$.\n  - Layers ($L_1$, $L_2$):\n    - $L_1 = \\begin{bmatrix}\n    2  2  3  4\\\\\n    2  3  4  5\\\\\n    3  4  5  6\\\\\n    4  5  6  7\n    \\end{bmatrix}$,\n    $L_2 = \\begin{bmatrix}\n    1  1  1  1\\\\\n    1  2  2  1\\\\\n    1  2  3  1\\\\\n    1  1  1  1\n    \\end{bmatrix}$.\n  - Weights: $w_1 = 0.5$, $w_2 = 0.5$.\n  - Source: $(0,0)$.\n  - Destinations: $(3,3)$, $(2,1)$, $(1,2)$.\n  - Observed outcomes $Y$: $[0,0,0]$.\n\n- Test case C:\n  - Grid size: $3 \\times 3$.\n  - Layers ($L_1$, $L_2$):\n    - $L_1 = \\begin{bmatrix}\n    1  1  1\\\\\n    1  1  1\\\\\n    1  1  1\n    \\end{bmatrix}$,\n    $L_2 = \\begin{bmatrix}\n    1  2  1\\\\\n    2  1  2\\\\\n    1  2  1\n    \\end{bmatrix}$.\n  - Weights: $w_1 = 1.0$, $w_2 = 0.0$.\n  - Source: $(1,1)$.\n  - Destinations: $(0,0)$, $(0,2)$, $(2,0)$, $(2,2)$.\n  - Observed outcomes $Y$: $[1,1,1,1]$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the three estimates $[\\hat{\\beta}_A,\\hat{\\beta}_B,\\hat{\\beta}_C]$ as a comma-separated list enclosed in square brackets, with each estimate rounded to $6$ decimal places. For example, an output line could look like $[0.731000,10.000000,0.000000]$.\n- No additional text should be printed.", "solution": "The problem statement is parsed and validated. It is found to be scientifically grounded, well-posed, and objective. The problem is a standard application of maximum likelihood estimation in landscape genetics, a subfield of evolutionary biology. All necessary definitions, data, and constraints are provided, and no contradictions are present. The problem is therefore deemed **valid**.\n\nThe solution to this problem requires two main computational steps: first, the calculation of effective distances ($ED$) from a source to multiple destinations on a resistance landscape; second, the estimation of the decay parameter $\\beta$ using maximum likelihood.\n\n**1. Effective Distance Calculation**\n\nThe effective distance is defined as the minimum cumulative cost of movement between two locations. This is a classic shortest path problem on a graph. The grid cells $(r,c)$ represent the nodes of the graph. An edge exists between any two cells that are adjacent in an $8$-connected sense (including diagonals).\n\nThe cost of traversing an edge from cell $u$ with coordinates $(r,c)$ to a neighboring cell $v$ with coordinates $(r',c')$ is given by the formula:\n$$ \\text{cost}(u,v) = \\frac{C(r,c) + C(r',c')}{2} \\times d_{uv} $$\nwhere $d_{uv}$ is the Euclidean distance between the cell centers. For orthogonal neighbors, $d_{uv}=1$. For diagonal neighbors, $d_{uv}=\\sqrt{2}$. The term $C(r,c)$ represents the total cost or resistance of a cell, which must be computed first. It is a weighted sum of $K$ different resistance layers $L_k$:\n$$ C(r,c) = \\sum_{k=1}^{K} w_k L_k(r,c) $$\nwhere $w_k$ are non-negative weights.\n\nTo find the minimum cumulative cost from the single source cell to all other cells in the grid, we employ Dijkstra's algorithm. The algorithm starts with the source cell having a distance of $0$ and all other cells having an infinite distance. It iteratively explores the grid, always extending the path from the unvisited cell with the smallest known distance, and updates the distances to its neighbors if a shorter path is found. A priority queue is used to efficiently manage the set of unvisited cells. After the algorithm terminates, we have the effective distance from the source to every other cell, from which we extract the values $\\mathrm{ED}_i$ for the specified destination sites.\n\n**2. Maximum Likelihood Estimation of $\\beta$**\n\nThe colonization outcome for each destination site $i$, denoted by $Y_i \\in \\{0, 1\\}$, is modeled as an independent Bernoulli trial with success probability $p_i$. This probability is assumed to decay exponentially with effective distance $\\mathrm{ED}_i$:\n$$ p_i = \\exp(-\\beta \\,\\mathrm{ED}_i) $$\nwhere $\\beta \\ge 0$ is the parameter to be estimated.\n\nThe likelihood of a single observation $(Y_i, \\mathrm{ED}_i)$ is given by the Bernoulli probability mass function:\n$$ L_i(\\beta | Y_i, \\mathrm{ED}_i) = p_i^{Y_i} (1 - p_i)^{1 - Y_i} $$\nDue to the independence of observations, the total likelihood for the entire dataset is the product of individual likelihoods, $L(\\beta) = \\prod_{i} L_i(\\beta)$. It is computationally more convenient to work with the log-likelihood function, $\\ell(\\beta) = \\log L(\\beta)$:\n$$ \\ell(\\beta) = \\sum_{i=1}^{N} \\left[ Y_i \\log(p_i) + (1-Y_i) \\log(1-p_i) \\right] $$\nSubstituting the model for $p_i$, we get:\n$$ \\ell(\\beta) = \\sum_{i=1}^{N} \\left[ Y_i \\log(\\exp(-\\beta \\,\\mathrm{ED}_i)) + (1-Y_i) \\log(1 - \\exp(-\\beta \\,\\mathrm{ED}_i)) \\right] $$\n$$ \\ell(\\beta) = -\\beta \\sum_{i=1}^{N} Y_i \\mathrm{ED}_i + \\sum_{i=1}^{N} (1-Y_i) \\log(1 - \\exp(-\\beta \\,\\mathrm{ED}_i)) $$\nThe maximum likelihood estimate (MLE) $\\hat{\\beta}$ is the value of $\\beta$ that maximizes this function. The second derivative of $\\ell(\\beta)$ with respect to $\\beta$ is non-positive, which ensures that the log-likelihood function is concave and has a unique maximum.\n\nA closed-form solution for $\\hat{\\beta}$ does not exist, so a numerical optimization method must be used. We seek to find the maximum of $\\ell(\\beta)$ within the constrained interval $\\beta \\in [0, \\beta_{\\max}]$, where $\\beta_{\\max}=10$. This is equivalent to minimizing the negative log-likelihood, $-\\ell(\\beta)$. For numerical stability, especially for small values of the argument $x = \\beta \\mathrm{ED}_i$, the term $\\log(1 - \\exp(-x))$ is computed as $\\log(\\mathrm{expm1}(x)) - x$.\n\nThe problem specifies handling of degenerate cases:\n- If all observed outcomes $Y_i$ are $1$ (colonization succeeded everywhere), the MLE is $\\hat{\\beta}=0$. This corresponds to a situation where distance poses no barrier.\n- If all $Y_i$ are $0$ (colonization failed everywhere), the MLE is taken as the upper bound, $\\hat{\\beta}=\\beta_{\\max}=10$. This reflects the strongest possible distance effect allowed by the model constraints.\n\nFor the general case with mixed outcomes, a numerical solver, such as `scipy.optimize.minimize_scalar` with the `bounded` method, is employed to find the value of $\\beta$ that minimizes the negative log-likelihood within the specified interval $[0, 10]$.", "answer": "```python\nimport numpy as np\nimport heapq\nfrom scipy.optimize import minimize_scalar\n\ndef solve_case(grid_size, L1, L2, w1, w2, source, destinations, outcomes):\n    \"\"\"\n    Computes the MLE for beta for a single test case.\n    \"\"\"\n    outcomes = np.array(outcomes)\n    \n    # Handle degenerate cases as per the problem statement\n    if np.all(outcomes == 1):\n        return 0.0\n    if np.all(outcomes == 0):\n        return 10.0\n\n    # 1. Compute the cell cost matrix C\n    L1, L2 = np.array(L1), np.array(L2)\n    C = w1 * L1 + w2 * L2\n    rows, cols = grid_size\n\n    # 2. Run Dijkstra's algorithm to find effective distances\n    dists = np.full((rows, cols), np.inf)\n    dists[source] = 0\n    pq = [(0, source)]  # (distance, (r, c))\n\n    while pq:\n        d, (r, c) = heapq.heappop(pq)\n\n        if d > dists[r, c]:\n            continue\n\n        for dr in [-1, 0, 1]:\n            for dc in [-1, 0, 1]:\n                if dr == 0 and dc == 0:\n                    continue\n\n                nr, nc = r + dr, c + dc\n\n                if 0 = nr  rows and 0 = nc  cols:\n                    step_len = np.sqrt(dr**2 + dc**2)\n                    avg_cost = (C[r, c] + C[nr, nc]) / 2\n                    step_cost = avg_cost * step_len\n                    \n                    new_dist = dists[r, c] + step_cost\n                    if new_dist  dists[nr, nc]:\n                        dists[nr, nc] = new_dist\n                        heapq.heappush(pq, (new_dist, (nr, nc)))\n\n    # 3. Extract effective distances for specified destinations\n    EDs = np.array([dists[dest] for dest in destinations])\n\n    # 4. Define the negative log-likelihood function\n    def neg_log_likelihood(beta, ed_vec, y_vec):\n        # A very small beta is numerically equivalent to beta=0 for this model\n        if beta  1e-12:\n            # If beta=0, p=1. Likelihood is 0 if any y=0. Log-L is -inf.\n            if np.any(y_vec == 0):\n                return np.inf\n            # If all y=1, Log-L = -beta * sum(ed). -Log-L is beta*sum(ed)\n            else:\n                return beta * np.sum(ed_vec)\n        \n        # The negative log-likelihood function to be minimized is:\n        # -ll(beta) = beta * sum(ED_i) - sum_{i where Y_i=0} log(exp(beta*ED_i) - 1)\n        z = beta * ed_vec\n        \n        term1 = np.sum(z)\n        \n        z_fail = z[y_vec == 0]\n        \n        # log(expm1(x)) is numerically stable for log(exp(x)-1)\n        # np.expm1 returns nan for negative inputs, but z_fail should be non-negative\n        if np.any(z_fail = 0): # Should not happen if EDs are > 0\n            return np.inf\n\n        term2 = np.sum(np.log(np.expm1(z_fail)))\n        \n        return term1 - term2\n\n    # 5. Minimize the negative log-likelihood to find beta\n    result = minimize_scalar(\n        neg_log_likelihood,\n        args=(EDs, outcomes),\n        bounds=(0, 10),\n        method='bounded'\n    )\n    \n    return result.x\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {\n            \"grid_size\": (5, 5),\n            \"L1\": [\n                [1, 1, 2, 3, 4], [1, 2, 2, 3, 4], [1, 2, 3, 3, 4],\n                [2, 2, 3, 4, 5], [3, 3, 4, 5, 5]\n            ],\n            \"L2\": [\n                [3, 3, 3, 3, 3], [3, 2, 2, 2, 3], [3, 2, 1, 2, 3],\n                [3, 2, 2, 2, 3], [3, 3, 3, 3, 3]\n            ],\n            \"w1\": 0.6, \"w2\": 0.4,\n            \"source\": (0, 0),\n            \"destinations\": [(0, 4), (2, 2), (4, 4), (4, 0), (1, 3)],\n            \"outcomes\": [0, 1, 0, 0, 1]\n        },\n        # Test case B\n        {\n            \"grid_size\": (4, 4),\n            \"L1\": [\n                [2, 2, 3, 4], [2, 3, 4, 5], \n                [3, 4, 5, 6], [4, 5, 6, 7]\n            ],\n            \"L2\": [\n                [1, 1, 1, 1], [1, 2, 2, 1], \n                [1, 2, 3, 1], [1, 1, 1, 1]\n            ],\n            \"w1\": 0.5, \"w2\": 0.5,\n            \"source\": (0, 0),\n            \"destinations\": [(3, 3), (2, 1), (1, 2)],\n            \"outcomes\": [0, 0, 0]\n        },\n        # Test case C\n        {\n            \"grid_size\": (3, 3),\n            \"L1\": [\n                [1, 1, 1], [1, 1, 1], [1, 1, 1]\n            ],\n            \"L2\": [\n                [1, 2, 1], [2, 1, 2], [1, 2, 1]\n            ],\n            \"w1\": 1.0, \"w2\": 0.0,\n            \"source\": (1, 1),\n            \"destinations\": [(0, 0), (0, 2), (2, 0), (2, 2)],\n            \"outcomes\": [1, 1, 1, 1]\n        }\n    ]\n\n    results = []\n    for case_data in test_cases:\n        beta_hat = solve_case(**case_data)\n        results.append(f\"{beta_hat:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2705035"}, {"introduction": "A central challenge in historical biogeography is selecting an appropriate process-based model without overfitting the data. While complex models with additional parameters may offer a better fit to observed patterns, they risk being ad hoc explanations rather than robust hypotheses. This exercise [@problem_id:2705112] confronts this problem directly by comparing the standard Dispersal-Extinction-Cladogenesis (DEC) model to an augmented version that includes founder-event speciation (DEC+J). You will implement a leave-one-out cross-validation procedure to learn how to rigorously assess whether adding model complexity is justified by a genuine improvement in out-of-sample predictive power.", "problem": "You are given a simplified, testable formulation of a model comparison problem arising in historical biogeography where the Dispersal-Extinction-Cladogenesis (DEC) model is compared with an augmented DEC with founder-event speciation (DEC+J). The scientific context is the use of biogeographic event patterns to evaluate evidence for evolution through dispersal and speciation processes across clades. The aim is to assess whether augmenting DEC with a founder-event parameter is ad hoc by evaluating out-of-sample predictive performance on held-out clades.\n\nFundamental base and assumptions:\n- The data for each clade are summarized by counts of cladogenetic events: for clade index $i \\in \\{1,\\dots, n\\}$, let $N_i$ denote the total number of cladogenetic events that were assessed, and $K_i$ denote the number of those that exhibit the founder-event signature (that is, a jump dispersal associated with speciation).\n- Under exchangeability of clades and the likelihood principle, model comparison can be performed using leave-one-clade-out predictive densities.\n- Under DEC, the true founder-event probability is exactly $p_J = 0$, hence the predictive probability for $K_i$ is $1$ if $K_i = 0$ and $0$ if $K_i  0$.\n- Under DEC+J, the founder-event probability $p_J$ is a parameter with a Beta prior $p_J \\sim \\mathrm{Beta}(\\alpha,\\beta)$, with hyperparameters $\\alpha  0$ and $\\beta  0$ chosen to encode the well-tested empirical expectation that founder events are rare. The leave-one-out posterior given the training clades is $p_J \\mid \\text{train} \\sim \\mathrm{Beta}(\\alpha + \\sum_{j \\neq i} K_j, \\beta + \\sum_{j \\neq i} (N_j - K_j))$. The predictive distribution for the held-out clade’s founder-event count $K_i$ given $N_i$ is the Beta-Binomial distribution with probability mass function\n$$\n\\Pr(K_i \\mid N_i, \\text{train}) \\;=\\; \\binom{N_i}{K_i} \\frac{B\\!\\left(\\alpha_{\\text{post}} + K_i,\\, \\beta_{\\text{post}} + N_i - K_i\\right)}{B\\!\\left(\\alpha_{\\text{post}},\\, \\beta_{\\text{post}}\\right)},\n$$\nwhere $B(\\cdot,\\cdot)$ is the Beta function and $\\alpha_{\\text{post}} = \\alpha + \\sum_{j \\neq i} K_j$, $\\beta_{\\text{post}} = \\beta + \\sum_{j \\neq i} (N_j - K_j)$.\n- The comparison metric is the mean leave-one-clade-out log predictive density (average across clades of the log of the predictive probability of the held-out clade’s $K_i$ under each model). Adding $J$ is deemed not ad hoc if DEC+J strictly improves this average over DEC, that is, if the difference\n$$\n\\Delta \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\log \\Pr_{\\text{DEC+J}}(K_i \\mid N_i, \\text{train}_{-i}) \\;-\\; \\frac{1}{n} \\sum_{i=1}^n \\log \\Pr_{\\text{DEC}}(K_i \\mid N_i)\n$$\nis strictly greater than $0$.\n\nTask:\n- Implement a program that, for each dataset in the test suite below, computes the leave-one-clade-out average log predictive density under DEC and DEC+J using the Beta-Binomial predictive for DEC+J and the structural-zero predictive for DEC, and then returns a boolean indicating whether $\\Delta  0$.\n- Use hyperparameters $\\alpha = 1$ and $\\beta = 19$ for the $\\mathrm{Beta}(\\alpha,\\beta)$ prior to encode a prior mean $\\mathbb{E}[p_J] = \\alpha/(\\alpha+\\beta) = 1/20$, reflecting rare founder events consistent with many island biogeography systems.\n- All logarithms must be natural logarithms.\n\nTest suite:\n- Dataset $\\mathcal{D}_1$ (happy path case where DEC fails because some clades show founder events):\n  - $[(N_1,K_1),\\dots,(N_6,K_6)] = [(12,2),(9,0),(15,1),(8,0),(10,0),(11,1)]$.\n- Dataset $\\mathcal{D}_2$ (control with no founder events observed):\n  - $[(N_1,K_1),\\dots,(N_5,K_5)] = [(12,0),(9,0),(15,0),(8,0),(10,0)]$.\n- Dataset $\\mathcal{D}_3$ (boundary case with a single founder event among few clades):\n  - $[(N_1,K_1),\\dots,(N_5,K_5)] = [(5,0),(7,0),(6,0),(8,0),(10,1)]$.\n\nQuantities to compute:\n- For each dataset, compute the boolean result $R \\in \\{\\text{True},\\text{False}\\}$ where $R = \\text{True}$ if $\\Delta  0$ and $R = \\text{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\mathcal{D}_1,\\mathcal{D}_2,\\mathcal{D}_3]$. For example, if the three booleans are True, False, True, the output must be exactly \"[True,False,True]\".\n- No physical units are involved; all reported values are dimensionless real numbers and booleans.", "solution": "The problem as stated is subjected to validation and is found to be well-posed, scientifically grounded, and internally consistent. It presents a standard, albeit simplified, application of Bayesian model comparison using leave-one-out cross-validation in the context of historical biogeography. All necessary data, models, and evaluation criteria are provided. We may therefore proceed with a rigorous analytical solution.\n\nThe objective is to determine for each dataset whether the model DEC+J offers a strictly better out-of-sample predictive fit than the nested DEC model. This is quantified by the sign of the difference in their mean leave-one-clade-out log predictive densities, denoted by $\\Delta$. A positive $\\Delta$ indicates that the inclusion of the founder-event speciation parameter, $p_J$, is justified by the data. The decision rule is to evaluate if $\\Delta  0$, where\n$$\n\\Delta = L_{\\text{DEC+J}} - L_{\\text{DEC}} = \\frac{1}{n} \\sum_{i=1}^n \\log \\Pr_{\\text{DEC+J}}(K_i \\mid N_i, \\text{train}_{-i}) - \\frac{1}{n} \\sum_{i=1}^n \\log \\Pr_{\\text{DEC}}(K_i \\mid N_i).\n$$\n\nWe will compute the two terms, $L_{\\text{DEC}}$ and $L_{\\text{DEC+J}}$, for each dataset.\n\n**1. Log Predictive Density for the DEC Model**\n\nThe DEC model is structurally defined by the constraint that the founder-event probability $p_J$ is exactly $0$. Consequently, the probability of observing $K_i$ founder events is:\n$$\n\\Pr_{\\text{DEC}}(K_i \\mid N_i) = \\begin{cases} 1  \\text{if } K_i = 0 \\\\ 0  \\text{if } K_i  0 \\end{cases}\n$$\nThe corresponding natural logarithm of this predictive probability is:\n$$\n\\log \\Pr_{\\text{DEC}}(K_i \\mid N_i) = \\begin{cases} \\log(1) = 0  \\text{if } K_i = 0 \\\\ \\log(0) = -\\infty  \\text{if } K_i  0 \\end{cases}\n$$\nThe mean log predictive density, $L_{\\text{DEC}}$, is the average of these values over all $n$ clades.\n- If for all clades $i$, $K_i=0$, then every term in the sum is $0$, and $L_{\\text{DEC}} = 0$.\n- If there exists even one clade $i$ for which $K_i  0$, the sum will contain at least one term of $-\\infty$, rendering the entire sum and thus the average $L_{\\text{DEC}} = -\\infty$.\n\n**2. Log Predictive Density for the DEC+J Model**\n\nFor the DEC+J model, we perform leave-one-out cross-validation. For each clade $i$ (the hold-out set), the remaining $n-1$ clades constitute the training set, denoted $\\text{train}_{-i}$.\n\nFirst, we compute the total counts of founder events and non-founder events across the entire dataset: $S_K^{\\text{total}} = \\sum_{j=1}^n K_j$ and $S_{N-K}^{\\text{total}} = \\sum_{j=1}^n (N_j - K_j)$.\n\nFor each hold-out clade $i$, the training set counts are $S_K^{\\text{train}} = S_K^{\\text{total}} - K_i$ and $S_{N-K}^{\\text{train}} = S_{N-K}^{\\text{total}} - (N_i - K_i)$. The posterior hyperparameters for $p_J$ are then updated from the prior $\\mathrm{Beta}(\\alpha, \\beta)$ using the training data:\n$$\n\\alpha_{\\text{post}} = \\alpha + S_K^{\\text{train}}\n$$\n$$\n\\beta_{\\text{post}} = \\beta + S_{N-K}^{\\text{train}}\n$$\nThe predictive probability of observing $K_i$ founder events in the hold-out clade, given $N_i$ total events, follows a Beta-Binomial distribution. We compute its natural logarithm:\n$$\n\\log \\Pr_{\\text{DEC+J}}(K_i \\mid N_i, \\text{train}_{-i}) = \\log\\left[ \\binom{N_i}{K_i} \\frac{B(\\alpha_{\\text{post}} + K_i, \\beta_{\\text{post}} + N_i - K_i)}{B(\\alpha_{\\text{post}}, \\beta_{\\text{post}})} \\right]\n$$\nFor numerical stability, this is computed using log-gamma functions (via the log-beta function, $\\text{lbeta}$) and the logarithm of the binomial coefficient:\n$$\n\\log \\Pr_i = \\log\\binom{N_i}{K_i} + \\mathrm{lbeta}(\\alpha_{\\text{post}} + K_i, \\beta_{\\text{post}} + N_i - K_i) - \\mathrm{lbeta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}})\n$$\nwhere $\\mathrm{lbeta}(a, b) = \\log B(a, b)$. Since the hyperparameters $\\alpha, \\beta$ are strictly positive ($1$ and $19$) and counts $K_j, N_j-K_j$ are non-negative, the arguments to the Beta function are always positive. Thus, the predictive probability is always a finite, non-zero number, and its logarithm, $\\log \\Pr_i$, is a finite real number.\n\nThe mean log predictive density for DEC+J is the average of these log-probabilities:\n$$\nL_{\\text{DEC+J}} = \\frac{1}{n} \\sum_{i=1}^n \\log \\Pr_i\n$$\nThis quantity will always be a finite real number.\n\n**3. Evaluation of the Condition $\\Delta  0$**\n\nWe must now evaluate the condition $L_{\\text{DEC+J}}  L_{\\text{DEC}}$.\n\n**Case 1: At least one $K_i  0$.**\nIn this scenario, $L_{\\text{DEC}} = -\\infty$. Since $L_{\\text{DEC+J}}$ is a finite value, the inequality becomes:\n$$\nL_{\\text{DEC+J}}  -\\infty\n$$\nThis is always true. Thus, if any founder events are observed in the dataset, DEC+J is strictly preferred. The result is **True**.\n\n**Case 2: All $K_i = 0$.**\nIn this scenario, $L_{\\text{DEC}} = 0$. The inequality becomes:\n$$\nL_{\\text{DEC+J}}  0\n$$\nLet us analyze $L_{\\text{DEC+J}}$. If all $K_j=0$, then for any hold-out clade $i$, we also have $K_i=0$. The training set has $S_K^{\\text{train}} = 0$. The posterior parameters become $\\alpha_{\\text{post}} = \\alpha$ and $\\beta_{\\text{post}} = \\beta + S_{N-K}^{\\text{train}}$. The log predictive probability for clade $i$ is:\n$$\n\\log \\Pr_i = \\log\\binom{N_i}{0} + \\mathrm{lbeta}(\\alpha, \\beta_{\\text{post}} + N_i) - \\mathrm{lbeta}(\\alpha, \\beta_{\\text{post}})\n$$\nSince $\\binom{N_i}{0}=1$, its logarithm is $0$. The expression simplifies to:\n$$\n\\log \\Pr_i = \\log B(\\alpha, \\beta_{\\text{post}} + N_i) - \\log B(\\alpha, \\beta_{\\text{post}})\n$$\nGiven that $N_i  0$, we have $\\beta_{\\text{post}} + N_i  \\beta_{\\text{post}}$. The Beta function $B(a,b)$ is a decreasing function of $b$ for fixed $a0$. Therefore, $B(\\alpha, \\beta_{\\text{post}} + N_i)  B(\\alpha, \\beta_{\\text{post}})$. Consequently, $\\log \\Pr_i  0$ for every clade $i$.\nThe average of these negative terms, $L_{\\text{DEC+J}}$, must also be negative. The condition $L_{\\text{DEC+J}}  0$ is therefore false. The result is **False**.\n\n**Conclusion**\nThe analysis rigorously establishes a simple decision rule: the condition $\\Delta  0$ is met if and only if the dataset contains at least one observed founder event ($K_i  0$). The implementation will perform the full calculation as a matter of formal diligence, but this derived rule provides an independent verification of the outcome.\n\n-   **Dataset $\\mathcal{D}_1$**: Contains clades with $K=2$, $K=1$. Result must be **True**.\n-   **Dataset $\\mathcal{D}_2$**: All clades have $K=0$. Result must be **False**.\n-   **Dataset $\\mathcal{D}_3$**: Contains a clade with $K=1$. Result must be **True**.\n\nThe expected output is therefore `[True,False,True]`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import betaln\nfrom scipy.special import comb as scipy_comb\nfrom math import log\n\ndef compute_delta_positive(data, alpha, beta):\n    \"\"\"\n    Computes if the mean log predictive density of DEC+J is strictly greater\n    than that of DEC using leave-one-clade-out cross-validation.\n\n    This corresponds to checking if Delta  0.\n    \"\"\"\n    n = len(data)\n    if n == 0:\n        # Undefined for empty dataset, but problem context implies n  0.\n        return False\n\n    # Calculate total sums over the entire dataset once to optimize the loop.\n    total_K = sum(k for _, k in data)\n    total_N = sum(n_val for n_val, _ in data)\n    total_N_minus_K = total_N - total_K\n\n    # 1. Compute the average log predictive density for the DEC model.\n    # If any K_i  0, Pr(K_i | DEC) = 0, so log(Pr) = -inf. The average is -inf.\n    # If all K_i = 0, Pr(K_i | DEC) = 1, so log(Pr) = 0. The average is 0.\n    avg_logpd_dec = -np.inf if total_K > 0 else 0.0\n\n    # 2. Compute the average log predictive density for the DEC+J model.\n    sum_logpd_decj = 0.0\n    for i in range(n):\n        # The i-th clade is the hold-out set.\n        N_i, K_i = data[i]\n\n        # The remaining n-1 clades are the training set.\n        # Calculate sums for the training set by subtracting the hold-out clade's contribution.\n        sum_K_train = total_K - K_i\n        sum_N_minus_K_train = total_N_minus_K - (N_i - K_i)\n\n        # Update prior to get posterior hyperparameters based on the training set.\n        alpha_post = alpha + sum_K_train\n        beta_post = beta + sum_N_minus_K_train\n\n        # Calculate the log of the beta-binomial predictive probability for the hold-out clade.\n        # log Pr(K_i|...) = log(C(N_i, K_i)) + betaln(a_post+K_i, b_post+N_i-K_i) - betaln(a_post, b_post)\n        \n        # Using scipy.special.comb with exact=False to get a float for log calculation.\n        # comb(N, K) is 0 if K > N or K  0.\n        c = scipy_comb(N_i, K_i, exact=False)\n        \n        if c == 0:\n            # This case corresponds to Pr=0, so log(Pr) = -inf.\n            # It should not happen with valid problem data where 0 = K = N.\n            log_prob_i = -np.inf\n        else:\n            log_binom_coeff = log(c)\n            # Use scipy's betaln for the log of the Beta function.\n            log_beta_numerator = betaln(alpha_post + K_i, beta_post + N_i - K_i)\n            log_beta_denominator = betaln(alpha_post, beta_post)\n            log_prob_i = log_binom_coeff + log_beta_numerator - log_beta_denominator\n        \n        sum_logpd_decj += log_prob_i\n\n    avg_logpd_decj = sum_logpd_decj / n\n    \n    # 3. The condition is Delta > 0, which is equivalent to avg_logpd_decj > avg_logpd_dec.\n    return avg_logpd_decj > avg_logpd_dec\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the test suite and print the results.\n    \"\"\"\n    # Define the hyperparameters for the Beta prior on p_J.\n    alpha = 1.0\n    beta = 19.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset D1: Some founder events observed.\n        [(12, 2), (9, 0), (15, 1), (8, 0), (10, 0), (11, 1)],\n        # Dataset D2: No founder events observed.\n        [(12, 0), (9, 0), (15, 0), (8, 0), (10, 0)],\n        # Dataset D3: A single founder event observed.\n        [(5, 0), (7, 0), (6, 0), (8, 0), (10, 1)],\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_delta_positive(case, alpha, beta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) correctly converts Python booleans (True, False)\n    # into the required strings \"True\" and \"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution.\nsolve()\n```", "id": "2705112"}]}