{"hands_on_practices": [{"introduction": "A core cellular mechanism of addiction is long-lasting synaptic plasticity in the reward circuitry. This practice allows you to model how drug-induced changes in the AMPA/NMDA receptor ratio, a hallmark of this plasticity, alter the fundamental properties of synaptic transmission. By deriving these changes and computing an experimental measure known as the rectification index, you will gain a deeper, quantitative understanding of how synaptic efficacy is regulated. [@problem_id:2605797]", "problem": "A nucleus accumbens medium spiny neuron exhibits drug-evoked synaptic plasticity that alters receptor composition at glutamatergic synapses. Consider the macroscopic excitatory postsynaptic current (EPSC) generated by co-activation of alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid receptor (AMPAR) and N-methyl-D-aspartate receptor (NMDAR) conductances by an impulse-like glutamate transient. Model the AMPAR component as $$I_{\\mathrm{A}}(t,V)=g_{\\mathrm{A},0}\\,s_{\\mathrm{A}}(t)\\,(V-E_{\\mathrm{A}}),$$ where $$s_{\\mathrm{A}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{A}}}\\right),\\quad E_{\\mathrm{A}}=0\\,\\mathrm{mV},$$ and the NMDAR component as $$I_{\\mathrm{N}}(t,V)=g_{\\mathrm{N},0}\\,s_{\\mathrm{N}}(t)\\,B(V)\\,(V-E_{\\mathrm{N}}),$$ where $$s_{\\mathrm{N}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{N}}}\\right),\\quad E_{\\mathrm{N}}=0\\,\\mathrm{mV},$$ and the magnesium block factor is the well-tested form $$B(V)=\\frac{1}{1+\\dfrac{[\\mathrm{Mg}^{2+}]}{3.57}\\,\\exp(-0.062\\,V)},$$ with $$[\\mathrm{Mg}^{2+}]=1\\,\\mathrm{mM}.$$ The total EPSC is $$I_{\\mathrm{tot}}(t,V)=I_{\\mathrm{A}}(t,V)+I_{\\mathrm{N}}(t,V).$$ Let the decay time constants be $$\\tau_{\\mathrm{A}}=5\\,\\mathrm{ms},\\quad \\tau_{\\mathrm{N}}=100\\,\\mathrm{ms}.$$\n\nDefine the AMPA/NMDA ratio $$R$$ operationally as the ratio of the absolute value of the peak AMPAR-mediated current at $$V=-70\\,\\mathrm{mV}$$ to the absolute value of the NMDAR-mediated current at $$V=+40\\,\\mathrm{mV}$$ evaluated at $$t=50\\,\\mathrm{ms}$$ after stimulation (each component pharmacologically isolated for measurement). In a baseline condition, $$R_{\\mathrm{pre}}=0.40,$$ and after an intervention that promotes insertion of calcium-permeable AMPARs (CP-AMPARs), $$R_{\\mathrm{post}}=1.20.$$\n\nTask 1. Starting from the definitions above and using only general physical principles such as Ohm’s law for ionic currents and exponential relaxation for channel kinetics, derive an expression for how increasing $$R$$ from $$R_{\\mathrm{pre}}$$ to $$R_{\\mathrm{post}}$$ changes (i) the peak amplitude of $$I_{\\mathrm{tot}}(t,V)$$ at $$V=-70\\,\\mathrm{mV}$$ and (ii) the effective weighted decay constant at $$V=-70\\,\\mathrm{mV}$$ defined by the amplitude-weighted mean $$\\tau_{\\mathrm{w}}(V)$$ of the two exponential components at the moment of peak, $$\\tau_{\\mathrm{w}}(V)=\\dfrac{A_{\\mathrm{A}}(V)\\,\\tau_{\\mathrm{A}}+A_{\\mathrm{N}}(V)\\,\\tau_{\\mathrm{N}}}{A_{\\mathrm{A}}(V)+A_{\\mathrm{N}}(V)},$$ where $$A_{\\mathrm{A}}(V)$$ and $$A_{\\mathrm{N}}(V)$$ are the instantaneous amplitudes of the AMPAR and NMDAR components at $$t=0^{+}.$$ Express your results in terms of $$R,$$ $$\\tau_{\\mathrm{A}},$$ $$\\tau_{\\mathrm{N}},$$ $$B(V),$$ and driving forces. State any approximations you make and justify them physiologically.\n\nTask 2. After the intervention, you pharmacologically isolate AMPAR-mediated currents and measure the following steady-state current–voltage data points: at $$V=-70\\,\\mathrm{mV},\\ I=-180\\,\\mathrm{pA};$$ at $$V=-40\\,\\mathrm{mV},\\ I=-120\\,\\mathrm{pA};$$ at $$V=0\\,\\mathrm{mV},\\ I=0\\,\\mathrm{pA};$$ and at $$V=+40\\,\\mathrm{mV},\\ I=+25\\,\\mathrm{pA}.$$ Assume the AMPAR reversal potential is $$E_{\\mathrm{A}}=0\\,\\mathrm{mV}.$$ Define the rectification index as the ratio of absolute chord conductances at $$V=-70\\,\\mathrm{mV}$$ and $$V=+40\\,\\mathrm{mV},$$ $$\\mathrm{RI}=\\dfrac{|I(-70)|/|{-}70-E_{\\mathrm{A}}|}{|I(+40)|/|{+}40-E_{\\mathrm{A}}|}.$$ Compute $$\\mathrm{RI}$$ from the given data.\n\nReport only the value of $$\\mathrm{RI}$$ as your final answer, unitless, rounded to four significant figures.", "solution": "The problem statement is subjected to rigorous validation before a solution is attempted.\n\nStep 1: Extract Givens.\nThe provided information is as follows:\nAMPAR current: $I_{\\mathrm{A}}(t,V)=g_{\\mathrm{A},0}\\,s_{\\mathrm{A}}(t)\\,(V-E_{\\mathrm{A}})$\nAMPAR kinetics: $s_{\\mathrm{A}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{A}}}\\right)$\nAMPAR reversal potential: $E_{\\mathrm{A}}=0\\,\\mathrm{mV}$\nAMPAR decay time constant: $\\tau_{\\mathrm{A}}=5\\,\\mathrm{ms}$\n\nNMDAR current: $I_{\\mathrm{N}}(t,V)=g_{\\mathrm{N},0}\\,s_{\\mathrm{N}}(t)\\,B(V)\\,(V-E_{\\mathrm{N}})$\nNMDAR kinetics: $s_{\\mathrm{N}}(t)=\\exp\\!\\left(-\\frac{t}{\\tau_{\\mathrm{N}}}\\right)$\nNMDAR reversal potential: $E_{\\mathrm{N}}=0\\,\\mathrm{mV}$\nNMDAR decay time constant: $\\tau_{\\mathrm{N}}=100\\,\\mathrm{ms}$\nMagnesium block factor: $B(V)=\\frac{1}{1+\\dfrac{[\\mathrm{Mg}^{2+}]}{3.57}\\,\\exp(-0.062\\,V)}$\nMagnesium concentration: $[\\mathrm{Mg}^{2+}]=1\\,\\mathrm{mM}$\nThe constant $3.57$ has units of $\\mathrm{mM}$, and $0.062$ has units of $\\mathrm{mV}^{-1}$.\n\nTotal current: $I_{\\mathrm{tot}}(t,V)=I_{\\mathrm{A}}(t,V)+I_{\\mathrm{N}}(t,V)$\n\nAMPA/NMDA Ratio definition: $R = \\frac{|I_{\\mathrm{A, peak}} \\text{ at } V=-70\\,\\mathrm{mV}|}{|I_{\\mathrm{N}} \\text{ at } V=+40\\,\\mathrm{mV}, t=50\\,\\mathrm{ms}|}$\nBaseline ratio: $R_{\\mathrm{pre}}=0.40$\nPost-intervention ratio: $R_{\\mathrm{post}}=1.20$\n\nTask 1 definitions:\nPeak amplitude of $I_{\\mathrm{tot}}(t,V)$ at $V=-70\\,\\mathrm{mV}$.\nEffective weighted decay constant: $\\tau_{\\mathrm{w}}(V)=\\dfrac{A_{\\mathrm{A}}(V)\\,\\tau_{\\mathrm{A}}+A_{\\mathrm{N}}(V)\\,\\tau_{\\mathrm{N}}}{A_{\\mathrm{A}}(V)+A_{\\mathrm{N}}(V)}$, where $A_{\\mathrm{A}}(V)$ and $A_{\\mathrm{N}}(V)$ are instantaneous amplitudes at $t=0^{+}$.\n\nTask 2 data (post-intervention, pharmacologically isolated AMPAR current):\nAt $V=-70\\,\\mathrm{mV}$, $I=-180\\,\\mathrm{pA}$\nAt $V=-40\\,\\mathrm{mV}$, $I=-120\\,\\mathrm{pA}$\nAt $V=0\\,\\mathrm{mV}$, $I=0\\,\\mathrm{pA}$\nAt $V=+40\\,\\mathrm{mV}$, $I=+25\\,\\mathrm{pA}$\n\nTask 2 definition:\nRectification Index: $\\mathrm{RI}=\\dfrac{|I(-70)|/|{-}70-E_{\\mathrm{A}}|}{|I(+40)|/|{+}40-E_{\\mathrm{A}}|}$\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded. The models for AMPAR and NMDAR currents, including the voltage-dependent magnesium block, are standard, empirically-verified representations used in computational neuroscience and synaptic physiology. The parameter values ($\\tau_{\\mathrm{A}}$, $\\tau_{\\mathrm{N}}$, $[\\mathrm{Mg}^{2+}]$) are physiologically realistic. The concept of the AMPA/NMDA ratio and its modulation by synaptic plasticity, including the insertion of CP-AMPARs which exhibit inward rectification, is a cornerstone of modern neuroscience. The problem is well-posed, providing all necessary definitions and data for both the derivation in Task 1 and the calculation in Task 2. The tasks are objective and free of ambiguity. The problem is self-contained and internally consistent.\n\nStep 3: Verdict and Action.\nThe problem is valid. A complete solution will now be provided.\n\nThe solution is presented in two parts, corresponding to the two tasks.\n\nTask 1: Derivation of Changes in EPSC Properties\n\nThe central assumption, based on the problem's description of an \"intervention that promotes insertion of calcium-permeable AMPARs,\" is that the change in the AMPA/NMDA ratio $R$ is due to a change in the maximal AMPAR conductance, $g_{\\mathrm{A},0}$, while the maximal NMDAR conductance, $g_{\\mathrm{N},0}$, remains constant.\n\nFirst, we formalize the relationship between the experimental ratio $R$ and the biophysical parameters. By definition:\n$$R = \\frac{|I_{\\mathrm{A, peak}}(-70\\,\\mathrm{mV})|}{|I_{\\mathrm{N}}(t=50\\,\\mathrm{ms}, V=+40\\,\\mathrm{mV})|}$$\nThe peak AMPAR current occurs at $t=0^{+}$ due to the exponential decay model.\n$$|I_{\\mathrm{A, peak}}(-70\\,\\mathrm{mV})| = |I_{\\mathrm{A}}(0, -70)| = |g_{\\mathrm{A},0} \\exp(-0) (-70 - 0)| = 70\\,g_{\\mathrm{A},0}$$\nThe NMDAR current at the specified point is:\n$$|I_{\\mathrm{N}}(50, 40)| = |g_{\\mathrm{N},0} \\exp(-50/\\tau_{\\mathrm{N}}) B(40) (40 - 0)| = 40\\,g_{\\mathrm{N},0} B(40) \\exp(-50/\\tau_{\\mathrm{N}})$$\nSubstituting these into the definition of $R$:\n$$R = \\frac{70\\,g_{\\mathrm{A},0}}{40\\,g_{\\mathrm{N},0} B(40) \\exp(-50/\\tau_{\\mathrm{N}})}$$\nWe can define a ratio of the peak conductances $\\gamma = g_{\\mathrm{A},0} / g_{\\mathrm{N},0}$. The equation above shows that $\\gamma$ is directly proportional to $R$.\n$$\\gamma = \\frac{g_{\\mathrm{A},0}}{g_{\\mathrm{N},0}} = R \\cdot \\left( \\frac{40 B(40) \\exp(-50/\\tau_{\\mathrm{N}})}{70} \\right) = \\frac{R}{K}$$\nwhere $K$ is a constant defined as:\n$$K = \\frac{70}{40 B(40) \\exp(-50/\\tau_{\\mathrm{N}})} = \\frac{7}{4 B(40) \\exp(-1/2)}$$\n\n(i) Change in the peak amplitude of $I_{\\mathrm{tot}}(t,V)$ at $V=-70\\,\\mathrm{mV}$.\nThe total current is $I_{\\mathrm{tot}}(t, -70) = I_{\\mathrm{A}}(t, -70) + I_{\\mathrm{N}}(t, -70)$. Both components are described by decaying exponentials starting at $t=0$ and have the same sign (inward current). Therefore, the peak magnitude of the total current occurs at $t=0^{+}$.\n$$I_{\\mathrm{peak}} = I_{\\mathrm{tot}}(0, -70) = g_{\\mathrm{A},0}(-70) + g_{\\mathrm{N},0} B(-70) (-70)$$\nThe peak amplitude is the absolute value:\n$$A_{\\mathrm{tot}}(-70) = |I_{\\mathrm{peak}}| = 70 (g_{\\mathrm{A},0} + g_{\\mathrm{N},0} B(-70))$$\nWe express this amplitude using the conductance ratio $\\gamma = g_{\\mathrm{A},0}/g_{\\mathrm{N},0}$:\n$$A_{\\mathrm{tot}}(-70) = 70 g_{\\mathrm{N},0} (\\gamma + B(-70))$$\nSubstituting $\\gamma = R/K$:\n$$A_{\\mathrm{tot}}(-70) = 70 g_{\\mathrm{N},0} \\left(\\frac{R}{K} + B(-70)\\right)$$\nTo quantify how the amplitude changes as $R$ goes from $R_{\\mathrm{pre}}$ to $R_{\\mathrm{post}}$, we compute the ratio of the post- and pre-intervention amplitudes. Since $g_{\\mathrm{N},0}$ is constant, it cancels out.\n$$\\frac{A_{\\mathrm{tot, post}}}{A_{\\mathrm{tot, pre}}} = \\frac{70 g_{\\mathrm{N},0} \\left(\\frac{R_{\\mathrm{post}}}{K} + B(-70)\\right)}{70 g_{\\mathrm{N},0} \\left(\\frac{R_{\\mathrm{pre}}}{K} + B(-70)\\right)} = \\frac{R_{\\mathrm{post}} + K B(-70)}{R_{\\mathrm{pre}} + K B(-70)}$$\nThis expression shows how the total peak current amplitude at $V=-70\\,\\mathrm{mV}$ scales with the AMPA/NMDA ratio $R$.\n\n(ii) Change in the effective weighted decay constant $\\tau_{\\mathrm{w}}(-70)$.\nThe definition is $\\tau_{\\mathrm{w}}(V)=\\dfrac{A_{\\mathrm{A}}(V)\\,\\tau_{\\mathrm{A}}+A_{\\mathrm{N}}(V)\\,\\tau_{\\mathrm{N}}}{A_{\\mathrm{A}}(V)+A_{\\mathrm{N}}(V)}$.\nAt $V=-70\\,\\mathrm{mV}$, the component amplitudes at $t=0^{+}$ are:\n$$A_{\\mathrm{A}}(-70) = |I_{\\mathrm{A}}(0, -70)| = 70 g_{\\mathrm{A},0}$$\n$$A_{\\mathrm{N}}(-70) = |I_{\\mathrm{N}}(0, -70)| = 70 g_{\\mathrm{N},0} B(-70)$$\nSubstituting these into the definition of $\\tau_{\\mathrm{w}}$:\n$$\\tau_{\\mathrm{w}}(-70) = \\frac{(70 g_{\\mathrm{A},0})\\tau_{\\mathrm{A}} + (70 g_{\\mathrm{N},0} B(-70))\\tau_{\\mathrm{N}}}{(70 g_{\\mathrm{A},0}) + (70 g_{\\mathrm{N},0} B(-70))}$$\nDividing the numerator and denominator by $70 g_{\\mathrm{N},0}$:\n$$\\tau_{\\mathrm{w}}(-70) = \\frac{(g_{\\mathrm{A},0}/g_{\\mathrm{N},0})\\tau_{\\mathrm{A}} + B(-70)\\tau_{\\mathrm{N}}}{(g_{\\mathrm{A},0}/g_{\\mathrm{N},0}) + B(-70)} = \\frac{\\gamma \\tau_{\\mathrm{A}} + B(-70)\\tau_{\\mathrm{N}}}{\\gamma + B(-70)}$$\nSubstituting $\\gamma = R/K$, we obtain the expression for the weighted time constant as a function of $R$:\n$$\\tau_{\\mathrm{w}}(R) = \\frac{\\frac{R}{K} \\tau_{\\mathrm{A}} + B(-70)\\tau_{\\mathrm{N}}}{\\frac{R}{K} + B(-70)} = \\frac{R \\tau_{\\mathrm{A}} + K B(-70)\\tau_{\\mathrm{N}}}{R + K B(-70)}$$\nThis expression describes how the effective decay kinetics change with $R$. Since $\\tau_{\\mathrm{A}} < \\tau_{\\mathrm{N}}$, an increase in $R$ (and thus $\\gamma$) increases the weight of the faster AMPA component, leading to a decrease in the overall weighted time constant $\\tau_{\\mathrm{w}}$.\n\nTask 2: Calculation of the Rectification Index\n\nThe Rectification Index ($\\mathrm{RI}$) is computed for the post-intervention AMPAR-mediated current using the provided data and definition. The reversal potential is $E_{\\mathrm{A}}=0\\,\\mathrm{mV}$. The definition is:\n$$\\mathrm{RI}=\\frac{\\text{chord conductance at } -70\\,\\mathrm{mV}}{\\text{chord conductance at } +40\\,\\mathrm{mV}} = \\frac{|I(-70)|/|-70 - E_{\\mathrm{A}}|}{|I(+40)|/|+40 - E_{\\mathrm{A}}|}$$\nThe data provided for the post-intervention state are:\n$I(-70) = -180\\,\\mathrm{pA}$\n$I(+40) = +25\\,\\mathrm{pA}$\n\nSubstituting these values into the formula:\n$$\\mathrm{RI} = \\frac{|-180\\,\\mathrm{pA}| / |-70 - 0\\,\\mathrm{mV}|}{|+25\\,\\mathrm{pA}| / |+40 - 0\\,\\mathrm{mV}|}$$\nThe units of $\\mathrm{pA/mV}$ (conductance) cancel out.\n$$\\mathrm{RI} = \\frac{180/70}{25/40} = \\frac{18/7}{5/8}$$\n$$\\mathrm{RI} = \\frac{18}{7} \\times \\frac{8}{5} = \\frac{144}{35}$$\nConverting to a decimal value:\n$$\\mathrm{RI} = 4.1142857...$$\nRounding to four significant figures as required by the problem statement:\n$$\\mathrm{RI} \\approx 4.114$$\nThis value, being significantly greater than $1$, indicates strong inward rectification, consistent with the described insertion of calcium-permeable AMPARs which typically lack the GluA2 subunit.", "answer": "$$\\boxed{4.114}$$", "id": "2605797"}, {"introduction": "Understanding how pharmacotherapies for addiction work requires linking receptor-level actions to systems-level effects on reward. This exercise explores the counter-intuitive dual action of partial agonists, which can either produce a mild reward signal or blunt the effects of more powerful drugs. By applying principles of competitive binding and intrinsic efficacy, you will predict how a partial agonist modulates the net reward prediction error, providing insight into its therapeutic mechanism. [@problem_id:2605751]", "problem": "An investigator models how partial agonists used in addiction treatment can produce functional antagonism in a receptor system that is already driven toward a high-efficacy state by a full agonist. Consider a single receptor population that integrates competitive binding from two ligands and transduces occupancy into a downstream signal that scales the probability of reinforcement via phasic dopamine signals in the mesolimbic pathway. The investigator makes the following assumptions, consistent with receptor theory and reward learning: (i) competitive binding obeys the law of mass action; (ii) each ligand’s contribution to the net receptor activation scales with its intrinsic efficacy; (iii) the net receptor activation is monotonically related to the amplitude of cue- and outcome-evoked phasic dopamine, which serves as the biological substrate of reward prediction error (RPE), and positive RPE strengthens reinforcement whereas negative RPE weakens it.\n\nTwo ligands interact with the same receptors: a full agonist, $F$, and a partial agonist, $P$. The full agonist has equilibrium dissociation constant $K_A^F = 100\\,\\mathrm{nM}$ and intrinsic efficacy $\\varepsilon_F = 1.0$ (normalized), while the partial agonist has $K_A^P = 5\\,\\mathrm{nM}$ and $\\varepsilon_P = 0.4$. The system’s maximal transduction capacity is not saturated by receptor number alone (that is, there is no infinite receptor reserve), and within the operating range considered here, net receptor activation can be treated as approximately proportional to the efficacy-weighted fractional occupancy of each ligand under competitive equilibrium.\n\nThe investigator studies two contexts:\n\n1) High-efficacy environment (co-administration): An animal trained to self-administer a dose producing an effective concentration of full agonist $F$ of $C_F = 300\\,\\mathrm{nM}$ (with stable expectation anchored to the previously experienced outcome under $F$ alone) is now given a mixture containing $C_F = 300\\,\\mathrm{nM}$ of $F$ plus $C_P = 50\\,\\mathrm{nM}$ of the partial agonist $P$.\n\n2) Low-efficacy environment (substitution): In a separate session without the full agonist (e.g., during abstinence), the same animal receives only $P$ at $C_P = 50\\,\\mathrm{nM}$, with low expectation of drug effect at the moment of delivery.\n\nUsing only the standard consequences of the law of mass action and receptor efficacy concepts, and the link between net receptor activation and the sign of the reward prediction error, select the option that most accurately explains how a partial agonist can act as a functional antagonist in the high-efficacy environment and predicts its impact on reinforcement across the two contexts described.\n\nA) In the high-efficacy environment, $P$ competitively displaces $F$ from receptors due to higher affinity, replacing some high-efficacy occupancy with lower-efficacy occupancy and thereby reducing net receptor activation relative to $F$ alone; this yields a negative reward prediction error and weakens reinforcement. In the low-efficacy environment, $P$ alone produces moderate receptor activation, a small positive reward prediction error, and modest reinforcement.\n\nB) Partial agonists, by definition, cannot antagonize; they always add to the effect of a full agonist regardless of concentrations, thus increasing net receptor activation and producing stronger reinforcement in both environments.\n\nC) In the high-efficacy environment, $P$ enhances reinforcement by preventing receptor desensitization, increasing cue-evoked dopamine above that produced by $F$ alone; in the low-efficacy environment, $P$ has no effect because reinforcement requires a full agonist.\n\nD) The impact of $P$ on reinforcement depends only on $\\varepsilon_P$ and not on $K_A^P$ or the prevailing concentration of $F$; partial agonists neither displace full agonists nor change reward prediction error in a concentration-dependent manner, so reinforcement is unchanged in both environments.", "solution": "The problem statement is subjected to rigorous validation before any attempt at a solution.\n\n### Step 1: Extract Givens\n- **System**: A single receptor population.\n- **Ligands**: A full agonist, $F$, and a partial agonist, $P$, competitively bind to the receptors.\n- **Signal Transduction**: Receptor activation is transduced into a signal affecting phasic dopamine release, which functions as a Reward Prediction Error (RPE).\n- **Assumptions**:\n    - (i) Competitive binding follows the law of mass action.\n    - (ii) Each ligand's contribution to net receptor activation scales with its intrinsic efficacy ($\\varepsilon$).\n    - (iii) Net receptor activation is monotonically related to phasic dopamine amplitude (RPE). Positive RPE strengthens reinforcement; negative RPE weakens it.\n- **Parameters for Full Agonist ($F$)**:\n    - Equilibrium dissociation constant: $K_A^F = 100\\,\\mathrm{nM}$\n    - Intrinsic efficacy: $\\varepsilon_F = 1.0$\n- **Parameters for Partial Agonist ($P$)**:\n    - Equilibrium dissociation constant: $K_A^P = 5\\,\\mathrm{nM}$\n    - Intrinsic efficacy: $\\varepsilon_P = 0.4$\n- **System Constraint**: The system does not have an infinite receptor reserve. Net receptor activation is approximately proportional to the efficacy-weighted fractional occupancy of each ligand.\n- **Context 1: High-Efficacy Environment (Co-administration)**\n    - An animal trained with full agonist $F$ alone, establishing a stable expectation.\n    - A mixture is administered containing:\n        - Concentration of $F$: $C_F = 300\\,\\mathrm{nM}$\n        - Concentration of $P$: $C_P = 50\\,\\mathrm{nM}$\n- **Context 2: Low-Efficacy Environment (Substitution)**\n    - The animal receives only partial agonist $P$.\n    - Concentration of $P$: $C_P = 50\\,\\mathrm{nM}$\n    - Expectation: Low expectation of drug effect.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n\n- **Scientific Grounding**: The problem is constructed upon fundamental and well-established principles of pharmacology, specifically receptor theory (competitive binding, law of mass action, fractional occupancy, intrinsic efficacy) and a cornerstone concept in neuroscience (the role of dopamine in reward prediction error). The scenario of a partial agonist acting as a functional antagonist in the presence of a full agonist is a classic example used in teaching pharmacology. The parameters and concentrations are pharmacologically plausible. The problem is scientifically sound.\n- **Well-Posedness**: The problem provides all necessary quantitative data ($K_A$ values, concentrations, efficacies) and a clear theoretical framework to derive a unique solution. The question is precisely formulated, asking for an explanation of the partial agonist's dual role and its impact on reinforcement, which can be determined from the provided model.\n- **Objectivity**: The problem is stated in objective, technical language, free from ambiguity or subjective assertions.\n\nThe problem statement demonstrates no flaws. It is not scientifically unsound, non-formalizable, incomplete, contradictory, unrealistic, or ill-posed.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation\n\nThe net receptor activation, which we denote as $E_{net}$, is given as being proportional to the efficacy-weighted sum of fractional occupancies of the ligands. We can write this relationship as:\n$$E_{net} = \\varepsilon_F f_F + \\varepsilon_P f_P$$\nwhere $f_F$ and $f_P$ are the fractional occupancies of the receptors by the full agonist $F$ and partial agonist $P$, respectively. According to the law of mass action for competitive binding, these are given by:\n$$f_F = \\frac{[F]/K_A^F}{1 + [F]/K_A^F + [P]/K_A^P}$$\n$$f_P = \\frac{[P]/K_A^P}{1 + [F]/K_A^F + [P]/K_A^P}$$\nHere, $[F]$ and $[P]$ are the concentrations of the ligands.\n\nWe analyze the three relevant states: the baseline state that sets expectation, the high-efficacy context, and the low-efficacy context.\n\n**1. Baseline State (Expectation)**\nThe animal is trained with the full agonist $F$ alone at a concentration of $C_F = 300\\,\\mathrm{nM}$. This sets the expected level of receptor activation. In this state, $[F] = 300\\,\\mathrm{nM}$ and $[P] = 0$.\nThe fractional occupancy by $F$ is:\n$$f_{F, baseline} = \\frac{[F]/K_A^F}{1 + [F]/K_A^F} = \\frac{300\\,\\mathrm{nM} / 100\\,\\mathrm{nM}}{1 + 300\\,\\mathrm{nM} / 100\\,\\mathrm{nM}} = \\frac{3}{1 + 3} = \\frac{3}{4} = 0.75$$\nThe baseline receptor activation, $E_{baseline}$, is:\n$$E_{baseline} = \\varepsilon_F f_{F, baseline} = 1.0 \\times 0.75 = 0.75$$\nThis value of $0.75$ represents the expected outcome.\n\n**2. Context 1: High-Efficacy Environment (Co-administration)**\nThe animal receives a mixture of $F$ and $P$. The concentrations are $[F] = 300\\,\\mathrm{nM}$ and $[P] = 50\\,\\mathrm{nM}$. We first calculate the normalized concentration terms:\n$$[F]/K_A^F = 300\\,\\mathrm{nM} / 100\\,\\mathrm{nM} = 3$$\n$$[P]/K_A^P = 50\\,\\mathrm{nM} / 5\\,\\mathrm{nM} = 10$$\nThe common denominator for the fractional occupancy equations is $1 + 3 + 10 = 14$.\nThe fractional occupancies are:\n$$f_F = \\frac{3}{14}$$\n$$f_P = \\frac{10}{14} = \\frac{5}{7}$$\nThe net receptor activation in this context, $E_1$, is:\n$$E_1 = \\varepsilon_F f_F + \\varepsilon_P f_P = (1.0) \\left(\\frac{3}{14}\\right) + (0.4) \\left(\\frac{10}{14}\\right) = \\frac{3}{14} + \\frac{4}{14} = \\frac{7}{14} = 0.5$$\nComparing the actual outcome $E_1 = 0.5$ to the expected outcome $E_{baseline} = 0.75$, we find that $E_1 < E_{baseline}$. This corresponds to a negative Reward Prediction Error (RPE). According to assumption (iii), a negative RPE weakens reinforcement.\nThe mechanism is that the partial agonist $P$, due to its high affinity (low $K_A^P = 5\\,\\mathrm{nM}$), effectively competes with and displaces the full agonist $F$ (with lower affinity $K_A^F = 100\\,\\mathrm{nM}$). Since the displaced high-efficacy ligand ($\\varepsilon_F = 1.0$) is replaced by a low-efficacy ligand ($\\varepsilon_P = 0.4$), the overall receptor activation decreases. This is the definition of functional antagonism.\n\n**3. Context 2: Low-Efficacy Environment (Substitution)**\nThe animal receives only the partial agonist $P$ at a concentration of $[P] = 50\\,\\mathrm{nM}$. In this state, $[F] = 0$. The problem states a low expectation of drug effect, which we model as an expected activation $E_{expected} \\approx 0$.\nThe fractional occupancy by $P$ is:\n$$f_{P, alone} = \\frac{[P]/K_A^P}{1 + [P]/K_A^P} = \\frac{50\\,\\mathrm{nM} / 5\\,\\mathrm{nM}}{1 + 50\\,\\mathrm{nM} / 5\\,\\mathrm{nM}} = \\frac{10}{1 + 10} = \\frac{10}{11}$$\nThe net receptor activation in this context, $E_2$, is:\n$$E_2 = \\varepsilon_P f_{P, alone} = 0.4 \\times \\frac{10}{11} = \\frac{4}{11} \\approx 0.364$$\nComparing the actual outcome $E_2 \\approx 0.364$ to the expected outcome $E_{expected} \\approx 0$, we find that $E_2 > E_{expected}$. This corresponds to a positive Reward Prediction Error (RPE). According to assumption (iii), a positive RPE strengthens reinforcement. In this context, the partial agonist acts, by itself, as an agonist, producing moderate receptor activation and modest reinforcement.\n\n### Option-by-Option Analysis\n\n**A) In the high-efficacy environment, $P$ competitively displaces $F$ from receptors due to higher affinity, replacing some high-efficacy occupancy with lower-efficacy occupancy and thereby reducing net receptor activation relative to $F$ alone; this yields a negative reward prediction error and weakens reinforcement. In the low-efficacy environment, $P$ alone produces moderate receptor activation, a small positive reward prediction error, and modest reinforcement.**\nThis statement is a precise qualitative summary of the quantitative derivation performed above. The partial agonist $P$ has a higher affinity ($K_A^P=5\\,\\mathrm{nM}$ vs $K_A^F=100\\,\\mathrm{nM}$) and displaces $F$. This replacement lowers the net activation from $0.75$ to $0.5$, creating a negative RPE and weakening reinforcement. In the absence of $F$, $P$ itself produces an activation of $\\approx 0.364$ from a baseline of nearly zero, creating a positive RPE and modest reinforcement. The description is flawlessly accurate based on the provided model.\n**Verdict: Correct.**\n\n**B) Partial agonists, by definition, cannot antagonize; they always add to the effect of a full agonist regardless of concentrations, thus increasing net receptor activation and producing stronger reinforcement in both environments.**\nThis statement is pharmacologically false. A partial agonist acts as an antagonist if it reduces the net effect produced by a co-administered full agonist. Our calculation explicitly shows a reduction in net activation ($0.75 \\rightarrow 0.5$). The premise that partial agonists \"cannot antagonize\" is an incorrect generalization.\n**Verdict: Incorrect.**\n\n**C) In the high-efficacy environment, $P$ enhances reinforcement by preventing receptor desensitization, increasing cue-evoked dopamine above that produced by $F$ alone; in the low-efficacy environment, $P$ has no effect because reinforcement requires a full agonist.**\nThis option introduces the concept of receptor desensitization, which is not mentioned in the problem statement or its assumptions. We are restricted to solving the problem with the provided information. Furthermore, our calculation shows a *decrease*, not an increase, in net activation. The claim that $P$ has \"no effect\" in the low-efficacy environment is also false; as an agonist (albeit partial), it produces a significant effect ($E_2 \\approx 0.364$).\n**Verdict: Incorrect.**\n\n**D) The impact of $P$ on reinforcement depends only on $\\varepsilon_P$ and not on $K_A^P$ or the prevailing concentration of $F$; partial agonists neither displace full agonists nor change reward prediction error in a concentration-dependent manner, so reinforcement is unchanged in both environments.**\nThis statement contradicts the law of mass action, a fundamental principle given in the problem statement (assumption i). The fractional occupancy, and therefore the net effect, is explicitly a function of all ligand concentrations and their respective dissociation constants ($K_A$). The displacement of $F$ by $P$ is a direct result of competitive binding, which is concentration- and affinity-dependent. The entire analysis shows that the RPE and reinforcement are indeed changed in both contexts.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2605751"}, {"introduction": "This hands-on coding exercise challenges you to implement a foundational temporal difference (TD) learning model from scratch. By simulating a simple cue-reward task, you will witness the hallmark \"transfer of response\" where the dopamine-like prediction error signal shifts from the reward to the cue as the agent learns the association. This provides a tangible, algorithmic understanding of how value is learned and assigned in the brain, a process central to the development of addiction. [@problem_id:2605752]", "problem": "Write a program that models phasic dopaminergic responses during a deterministic cue-reward task with variable cue-to-reward delays, in order to demonstrate transfer of the response from reward delivery to cue onset as value estimates converge. Use a discrete-time, episodic setting with the following fundamental bases and assumptions, which are widely accepted in reinforcement learning models of mesolimbic dopamine system function in the physiology of reward circuitry and addiction:\n\n- The phasic dopaminergic signal is proportional to the instantaneous discrepancy between successive estimates of the expected discounted sum of future primary rewards plus any received primary reward at that moment.\n- Trials are independent episodes. Each trial consists of a brief predictive cue at time step $0$ followed by a fixed delay of $D$ seconds before delivery of a nonnegative primary reward of magnitude $R$. Time is discretized into steps of $1$ second, so the delay in steps equals $D$.\n- The agent maintains a table of scalar state values for each within-trial time step after the cue and before reward. The pre-cue baseline state has a fixed value of $0$ and is not updated. After reward delivery, the terminal state has a value of $0$.\n- The agent uses a constant learning rate $\\alpha$ and a constant discount factor $\\gamma$ across trials. Within each trial, the agent updates the value at each within-trial state sequentially by moving a fraction $\\alpha$ of the discrepancy described above toward reducing that discrepancy, using one-step bootstrapping. Across trials, these estimates converge for deterministic tasks.\n- Define the model-predicted phasic dopaminergic response at cue onset in a given trial as the discrepancy computed on the state transition from the pre-cue baseline (fixed value $0$) to the first post-cue state. Define the model-predicted phasic dopaminergic response at reward delivery as the discrepancy computed on the transition from the last pre-reward state to the terminal state upon primary reward delivery. If the delay is $0$ seconds, treat the episode as a single baseline-to-terminal transition at the time of cue-reward coincidence; in that boundary case, report the same discrepancy as both cue-aligned and reward-aligned responses.\n\nImplement the simulation exactly as specified above and compute, for each parameter set in the test suite below, the following four quantities:\n- the cue-aligned response on trial $1$,\n- the reward-aligned response on trial $1$,\n- the cue-aligned response on the last trial,\n- the reward-aligned response on the last trial.\n\nAll responses are unitless (arbitrary units). Round each reported float to three decimals. The time step is $1$ second; delays $D$ are provided in seconds. There are no angles.\n\nTest suite (each tuple is $(\\gamma,\\alpha,R,D,T)$):\n- Case A (happy path): $\\gamma=0.95$, $\\alpha=0.20$, $R=1.0$, $D=3$, $T=200$.\n- Case B (long delay, slow learning): $\\gamma=0.99$, $\\alpha=0.10$, $R=1.0$, $D=10$, $T=400$.\n- Case C (immediate reward boundary): $\\gamma=0.95$, $\\alpha=0.30$, $R=1.0$, $D=0$, $T=100$.\n- Case D (heavy discounting): $\\gamma=0.50$, $\\alpha=0.20$, $R=1.0$, $D=4$, $T=200$.\n\nYour program should produce a single line of output containing the results for all cases as a comma-separated list of lists, with no spaces, in this exact format:\n`[[cue1_caseA,reward1_caseA,cuelast_caseA,rewardlast_caseA],[cue1_caseB,reward1_caseB,cuelast_caseB,rewardlast_caseB],[cue1_caseC,reward1_caseC,cuelast_caseC,rewardlast_caseC],[cue1_caseD,reward1_caseD,cuelast_caseD,rewardlast_caseD]]`", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Model Principle:** The phasic dopaminergic signal is proportional to the temporal difference (TD) error: the discrepancy between successive estimates of expected future reward, plus any immediate reward.\n- **Task Structure:** Discrete-time, episodic. Each trial involves a cue at time step $0$, a delay of $D$ seconds, and a primary reward $R \\ge 0$ at time step $D$. Time steps are $1$ second.\n- **State Values:**\n    - Pre-cue baseline state value is fixed at $V_{pre-cue} = 0$.\n    - Terminal state value (post-reward) is fixed at $V_{terminal} = 0$.\n    - Values for within-trial states $s_0, s_1, \\dots, s_{D-1}$ are learned. Let their values be $V_0, V_1, \\dots, V_{D-1}$.\n    - Initial values for all learnable states are $0$.\n- **Learning Parameters:** Constant learning rate $\\alpha$, constant discount factor $\\gamma$.\n- **Update Rule:** Value updates use one-step bootstrapping: $V(s) \\leftarrow V(s) + \\alpha \\cdot (\\text{discrepancy})$. Updates are sequential for each state within a trial. Based on standard TD learning, this is interpreted as a batch update at the end of each trial, where for each state $s_t$, $V_t$ is updated using the TD error $\\delta_t$.\n- **Response Definitions:**\n    - Cue-aligned response: TD error on the transition from pre-cue state to the first post-cue state ($s_0$).\n    - Reward-aligned response: TD error on the transition from the last pre-reward state ($s_{D-1}$) to the terminal state.\n- **Boundary Case ($D=0$):**\n    - The task is a single transition from pre-cue baseline to the terminal state.\n    - The reward $R$ is delivered at cue time.\n    - The cue-aligned and reward-aligned responses are identical and equal to the TD error of this single transition.\n- **Input Parameters:** A set of tuples $(\\gamma, \\alpha, R, D, T)$, where $T$ is the total number of trials.\n- **Required Output:** For each parameter set, a list of four quantities: [cue response trial $1$, reward response trial $1$, cue response trial $T$, reward response trial $T$], with each value rounded to three decimals.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the validation criteria.\n\n- **Scientifically Grounded:** The problem is a direct implementation of the temporal difference (TD) model of dopamine function, a foundational concept in computational neuroscience and reinforcement learning. This model, proposed by Schultz, Dayan, and Montague, posits that phasic dopamine activity encodes a reward prediction error (RPE), which drives learning. The specified mechanics are a standard formulation of TD(0) learning in an episodic, deterministic Pavlovian conditioning paradigm. It is scientifically sound.\n- **Well-Posed:** The problem is well-posed. The dynamics of the system (the value updates) are clearly defined by a set of difference equations. The initial conditions are specified ($V_t=0$ for all learnable states at trial $1$). The parameters are provided. For a deterministic task with $\\gamma < 1$ and a constant positive learning rate, the value estimates are guaranteed to converge to a unique, stable solution. The quantities to be computed are precisely defined.\n- **Objective:** The problem is stated in objective, mathematical language, free of subjective or ambiguous terminology.\n- **Completeness and Consistency:** The problem provides all necessary information: the model equations, initial/boundary conditions, learning parameters, and a full description of the simulation procedure. The special case for $D=0$ is explicitly and consistently defined. There are no contradictions.\n- **Feasibility:** The parameter values given in the test suite are physically and computationally reasonable. The simulation is computationally tractable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a canonical exercise in computational modeling of neural reward systems. A solution will be provided.\n\n**Principle-Based Solution**\n\nThe problem requires simulating a Temporal Difference (TD) learning agent to model phasic dopamine responses. The core scientific principle is that dopamine neurons signal a reward prediction error (RPE), which is the difference between an actual outcome and its prediction. This RPE is used to update predictions for future events.\n\n**1. Formalization of the TD Model**\nLet $V_t^{(k)}$ be the value of being in state $s_t$ at the beginning of trial $k$, where $t \\in \\{0, 1, \\dots, D-1\\}$ corresponds to the time in seconds elapsed since the cue. The initial state values are $V_t^{(1)} = 0$ for all $t$. The pre-cue and terminal state values are fixed: $V_{pre-cue} = 0$ and $V_{terminal} = 0$.\n\nThe one-step TD error, $\\delta_t$, for a transition from state $s_t$ to $s_{t+1}$ with immediate reward $r_{t+1}$ is:\n$$ \\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t) $$\nIn this specific task, reward is only delivered at the end. Let the values at the beginning of a trial $k$ be the vector $\\mathbf{V}^{(k)} = [V_0^{(k)}, V_1^{(k)}, \\dots, V_{D-1}^{(k)}]$.\n\nThe TD errors for each state transition within trial $k$ are:\n- For transitions between delay states, $s_t \\to s_{t+1}$ where $t \\in \\{0, 1, \\dots, D-2\\}$:\n$$ \\delta_t^{(k)} = (r=0) + \\gamma V_{t+1}^{(k)} - V_t^{(k)} $$\n- For the final transition from the last delay state $s_{D-1}$ to the terminal state:\n$$ \\delta_{D-1}^{(k)} = R + \\gamma V_{terminal} - V_{D-1}^{(k)} = R - V_{D-1}^{(k)} $$\n\nThese errors are then used to update the state values for the next trial, $k+1$:\n$$ V_t^{(k+1)} = V_t^{(k)} + \\alpha \\delta_t^{(k)} \\quad \\text{for } t \\in \\{0, 1, \\dots, D-1\\} $$\n\n**2. Phasic Dopaminergic Responses**\nThe problem defines the model's \"dopamine responses\" as specific TD errors.\n- **Cue-aligned response:** This is the TD error on the transition from the fixed-value pre-cue state ($V_{pre-cue}=0$) to the first post-cue state $s_0$.\n$$ \\delta_{cue}^{(k)} = (r=0) + \\gamma V_0^{(k)} - V_{pre-cue} = \\gamma V_0^{(k)} $$\n- **Reward-aligned response:** This is the TD error on the transition from the last pre-reward state $s_{D-1}$ to the terminal state, which is simply $\\delta_{D-1}^{(k)}$.\n$$ \\delta_{reward}^{(k)} = R + \\gamma V_{terminal} - V_{D-1}^{(k)} = R - V_{D-1}^{(k)} $$\n\n**3. Convergence Analysis**\nAs the number of trials $k \\to \\infty$, the values converge to a steady state $\\mathbf{V}^*$ where the updates are zero ($\\delta_t^* = 0$).\nFor $t \\in \\{0, 1, \\dots, D-2\\}$, $\\delta_t^* = \\gamma V_{t+1}^* - V_t^* = 0 \\implies V_t^* = \\gamma V_{t+1}^*$.\nFor $t = D-1$, $\\delta_{D-1}^* = R - V_{D-1}^* = 0 \\implies V_{D-1}^* = R$.\nBy backward induction, the converged value for any state $s_t$ is the discounted value of the future reward $R$:\n$$ V_t^* = \\gamma^{D-1-t} R $$\nThe converged responses are thus:\n- $\\delta_{cue}^* = \\gamma V_0^* = \\gamma (\\gamma^{D-1}R) = \\gamma^D R$\n- $\\delta_{reward}^* = R - V_{D-1}^* = R - R = 0$\nThis result demonstrates the characteristic transfer of the prediction error signal from the time of reward to the time of the earliest predictive cue.\n\n**4. Boundary Case ($D=0$)**\nWhen the delay $D=0$, the cue and reward are simultaneous. The event is a single transition from the pre-cue state to the terminal state. There are no intermediate states $s_t$ to learn values for. The TD error for this single transition is:\n$$ \\delta = R + \\gamma V_{terminal} - V_{pre-cue} = R + \\gamma(0) - 0 = R $$\nThe problem stipulates that this single error value, $R$, is reported for both the cue- and reward-aligned responses for all trials, as no learning occurs.\n\n**5. Algorithm Implementation**\nThe simulation is implemented as follows:\n1. For each test case $(\\gamma, \\alpha, R, D, T)$, first check if $D=0$. If so, the result is $[R, R, R, R]$.\n2. If $D > 0$, initialize a NumPy array `V` of size $D$ with zeros. This represents the state values $V_t^{(1)}$.\n3. Calculate the trial $1$ responses: $\\delta_{cue}^{(1)} = \\gamma V_0^{(1)}$ and $\\delta_{reward}^{(1)} = R - V_{D-1}^{(1)}$.\n4. Loop for $T-1$ iterations to simulate the updates from trial $1$ to trial $T-1$. In each iteration:\n    a. Calculate a vector of TD errors, `td_errors`, using the current value vector `V`.\n    b. Update the value vector: `V = V + alpha * td_errors`.\n5. After the loop, `V` contains the values for the start of trial $T$, i.e., $V_t^{(T)}$.\n6. Calculate the last trial responses: $\\delta_{cue}^{(T)} = \\gamma V_0^{(T)}$ and $\\delta_{reward}^{(T)} = R - V_{D-1}^{(T)}$.\n7. Collect the four computed responses, round them to three decimal places, and format the output as specified.\nThis procedure correctly computes the evolution of the value function and associated prediction errors across trials.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (0.95, 0.20, 1.0, 3, 200),  # Case A\n        (0.99, 0.10, 1.0, 10, 400), # Case B\n        (0.95, 0.30, 1.0, 0, 100),  # Case C\n        (0.50, 0.20, 1.0, 4, 200),  # Case D\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = simulate_td_learning(*params)\n        formatted_result = f\"[{','.join(f'{x:.3f}' for x in result)}]\"\n        all_results.append(formatted_result)\n\n    print(f\"[{','.join(all_results)}]\")\n\n\ndef simulate_td_learning(gamma, alpha, R, D, T):\n    \"\"\"\n    Simulates a deterministic cue-reward task using a temporal difference (TD) model.\n\n    Args:\n        gamma (float): Discount factor.\n        alpha (float): Learning rate.\n        R (float): Reward magnitude.\n        D (int): Delay in time steps (seconds) from cue to reward.\n        T (int): Total number of trials.\n\n    Returns:\n        list: A list of four floats: [cue_response_trial_1, reward_response_trial_1,\n               cue_response_last_trial, reward_response_last_trial].\n    \"\"\"\n    # Handle the boundary case where D=0 (immediate reward)\n    if D == 0:\n        # No learning occurs as there are no intermediate states.\n        # The TD error is R for the single transition from pre-cue to terminal.\n        # This error is reported for both cue and reward response on all trials.\n        return [R, R, R, R]\n\n    # Initialize state values V_t for t=0..D-1 for the first trial (k=1).\n    # V_t^(1) = 0 for all t.\n    V = np.zeros(D, dtype=float)\n\n    # --- Calculate responses for Trial 1 ---\n    # Cue response is the TD error for pre-cue -> s_0 transition: delta_cue = gamma * V_0 - V_precue\n    cue_response_trial_1 = gamma * V[0]\n    # Reward response is the TD error for s_{D-1} -> terminal transition: delta_reward = R - V_{D-1}\n    reward_response_trial_1 = R - V[D-1]\n\n    # --- Simulate T trials to find values at the start of the last trial (T) ---\n    # We need to perform T-1 updates to get from the values for trial 1 to trial T.\n    for _ in range(T - 1):\n        td_errors = np.zeros(D, dtype=float)\n\n        # Calculate TD errors for transitions s_t -> s_{t+1} for t=0..D-2\n        # delta_t = gamma * V_{t+1} - V_t\n        if D > 1:\n            td_errors[:-1] = gamma * V[1:] - V[:-1]\n\n        # Calculate TD error for transition s_{D-1} -> terminal\n        # delta_{D-1} = R - V_{D-1}\n        td_errors[-1] = R - V[-1]\n\n        # Update all state values for the next trial\n        # V_t^(k+1) = V_t^(k) + alpha * delta_t^(k)\n        V += alpha * td_errors\n\n    # At this point, V contains the values for the start of trial T.\n    \n    # --- Calculate responses for the last trial (T) ---\n    cue_response_last_trial = gamma * V[0]\n    reward_response_last_trial = R - V[-1]\n\n    return [\n        cue_response_trial_1,\n        reward_response_trial_1,\n        cue_response_last_trial,\n        reward_response_last_trial,\n    ]\n\nsolve()\n```", "id": "2605752"}]}