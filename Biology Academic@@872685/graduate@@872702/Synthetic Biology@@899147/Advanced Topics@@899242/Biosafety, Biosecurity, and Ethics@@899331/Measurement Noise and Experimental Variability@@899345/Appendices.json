{"hands_on_practices": [{"introduction": "Before we analyze specific datasets, it's crucial to understand the theoretical limits of measurement precision. The Cramér-Rao Lower Bound (CRLB) provides this fundamental benchmark, establishing the minimum possible variance for any unbiased estimator of a parameter. This practice guides you through the first-principles derivation of the CRLB for several noise models common in single-cell biology, giving you a powerful tool to evaluate the efficiency of your experimental designs and estimation strategies [@problem_id:2749353].", "problem": "A synthetic biology lab quantifies a constitutively expressed reporter in single cells using three distinct assays, each with a different dominant noise structure. Let the true per-cell mean reporter abundance be an unknown scalar parameter $\\theta \\in (0,\\infty)$ that is constant across cells in a homogeneous culture. You observe $N$ independent and identically distributed measurements $Y_1,\\dots,Y_N$, where the single-sample noise model depends on the assay. Assume all nuisance parameters mentioned below are known constants. All quantities are dimensionless.\n\nAssay models for a single observation $Y$ are as follows, each motivated by a standard noise mechanism in single-cell measurements:\n- Model A (counting with shot noise): $Y \\sim \\mathrm{Poisson}(\\theta)$.\n- Model B (overdispersed counting due to cell-to-cell variability): $Y \\sim \\mathrm{NegativeBinomial}(\\text{size}=k,\\text{mean}=\\theta)$, with fixed $k \\in (0,\\infty)$ and probability mass function\n  $$\\mathbb{P}(Y=y\\mid \\theta,k) \\;=\\; \\frac{\\Gamma(y+k)}{\\Gamma(k)\\,y!}\\,\\left(\\frac{k}{k+\\theta}\\right)^k\\left(\\frac{\\theta}{k+\\theta}\\right)^y,$$\n  which has variance $\\mathrm{Var}(Y)=\\theta+\\theta^2/k$ for fixed $k$.\n- Model C (fluorescence with additive background and photon shot noise): $Y \\sim \\mathcal{N}(\\mu,\\sigma^2(\\theta))$ with $\\mu=\\theta$ and $\\sigma^2(\\theta)=\\sigma^2+\\beta \\theta$, where $\\sigma>0$ and $\\beta\\ge 0$ are known constants.\n\nTask:\n1. Starting from the definitions of likelihood and Fisher information, derive the single-sample Fisher information $I_1(\\theta)$ for $\\theta$ under each model. Use only foundational definitions: the log-likelihood $\\,\\ell(\\theta\\mid y)=\\log p(y\\mid \\theta)\\,$, the Fisher information $\\,I_1(\\theta)=\\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\theta}\\ell(\\theta\\mid Y)\\right)^2\\right]\\,$ under the true model, and independence of samples. Do not use pre-quoted results for $I_1(\\theta)$; derive from the given distributions.\n2. Use the fact that $N$ independent and identically distributed samples add Fisher information to compute the Cramér–Rao lower bound for any unbiased estimator $\\widehat{\\theta}$ as\n   $$\\mathrm{CRLB}(\\theta;N) \\;=\\; \\frac{1}{N\\,I_1(\\theta)}.$$\n\nImplement a program that, for the test suite below, computes the Cramér–Rao lower bound for $\\theta$ under the specified model and parameters. Your program must directly implement your derived $I_1(\\theta)$ for each model without numerical integration or simulation.\n\nTest suite (each case is a tuple $(\\text{model}, N, \\theta, \\text{extra parameters})$):\n- Case $1$: Model A, $N=100$, $\\theta=12.5$.\n- Case $2$: Model A, $N=5$, $\\theta=0.2$.\n- Case $3$: Model B, $N=100$, $\\theta=12.5$, $k=20$.\n- Case $4$: Model B, $N=100$, $\\theta=12.5$, $k=10^6$.\n- Case $5$: Model C, $N=50$, $\\theta=1000$, $\\sigma=100$, $\\beta=1$.\n- Case $6$: Model C, $N=50$, $\\theta=1000$, $\\sigma=100$, $\\beta=0$.\n- Case $7$: Model C, $N=10$, $\\theta=0$, $\\sigma=20$, $\\beta=2$.\n- Case $8$: Model B, $N=30$, $\\theta=5$, $k=0.5$.\n- Case $9$: Model A, $N=1$, $\\theta=0.001$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $9$, with each value rounded to $8$ decimal places. For example: `[result_1,result_2,...,result_9]`.\n\nNotes:\n- Use $N\\in \\mathbb{N}$, $\\theta>0$ unless explicitly given $\\theta=0$ in a test case, in which case interpret the limiting Fisher information appropriately from your derivation.\n- Angles are not involved. No physical units are required.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in established statistical theory and its application to biological measurement, is well-posed with all necessary information provided, and is stated in objective, formal language. We may proceed to the derivation and solution.\n\nThe task is to derive the single-sample Fisher Information, $I_1(\\theta)$, for three distinct statistical models, and then compute the Cramér-Rao Lower Bound (CRLB) for an estimator of the parameter $\\theta$ from $N$ independent samples. The CRLB is given by $\\mathrm{CRLB}(\\theta; N) = [N I_1(\\theta)]^{-1}$. The Fisher Information is defined as $I_1(\\theta) = \\mathbb{E}[(\\frac{\\partial}{\\partial \\theta}\\ell(\\theta\\mid Y))^2]$, where $\\ell(\\theta \\mid y) = \\log p(y \\mid \\theta)$ is the log-likelihood for a single sample $Y$.\n\n**Model A: Poisson counting noise**\n\nFor a single observation $Y \\sim \\mathrm{Poisson}(\\theta)$, the probability mass function (PMF) is:\n$$p(y \\mid \\theta) = \\frac{e^{-\\theta}\\theta^y}{y!}$$\nThe log-likelihood $\\ell(\\theta \\mid y)$ is:\n$$\\ell(\\theta \\mid y) = \\log\\left(\\frac{e^{-\\theta}\\theta^y}{y!}\\right) = -\\theta + y\\log\\theta - \\log(y!)$$\nThe first derivative of the log-likelihood with respect to $\\theta$, known as the score function, is:\n$$\\frac{\\partial}{\\partial\\theta}\\ell(\\theta \\mid y) = -1 + \\frac{y}{\\theta} = \\frac{y-\\theta}{\\theta}$$\nThe Fisher information is the expected value of the square of the score:\n$$I_1(\\theta) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial\\theta}\\ell(\\theta \\mid Y)\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\frac{Y-\\theta}{\\theta}\\right)^2\\right] = \\frac{1}{\\theta^2}\\mathbb{E}\\left[(Y-\\theta)^2\\right]$$\nThe term $\\mathbb{E}[(Y-\\theta)^2]$ is the definition of the variance of $Y$, $\\mathrm{Var}(Y)$. For a Poisson distribution with parameter $\\theta$, the variance is equal to the mean, so $\\mathrm{Var}(Y) = \\theta$.\nSubstituting this into the equation for $I_1(\\theta)$:\n$$I_1(\\theta) = \\frac{\\mathrm{Var}(Y)}{\\theta^2} = \\frac{\\theta}{\\theta^2} = \\frac{1}{\\theta}$$\n\n**Model B: Negative Binomial overdispersed counting**\n\nFor a single observation $Y \\sim \\mathrm{NegativeBinomial}(\\text{size}=k, \\text{mean}=\\theta)$, the PMF is:\n$$\\mathbb{P}(Y=y \\mid \\theta,k) = \\frac{\\Gamma(y+k)}{\\Gamma(k)\\,y!}\\left(\\frac{k}{k+\\theta}\\right)^k\\left(\\frac{\\theta}{k+\\theta}\\right)^y$$\nThe log-likelihood $\\ell(\\theta \\mid y)$ is:\n$$\\ell(\\theta \\mid y) = \\log\\left(\\frac{\\Gamma(y+k)}{\\Gamma(k)\\,y!}\\right) + k\\log\\left(\\frac{k}{k+\\theta}\\right) + y\\log\\left(\\frac{\\theta}{k+\\theta}\\right)$$\n$$\\ell(\\theta \\mid y) = C(y,k) + k\\log(k) - k\\log(k+\\theta) + y\\log\\theta - y\\log(k+\\theta)$$\nwhere $C(y,k)$ contains terms not dependent on $\\theta$. Differentiating with respect to $\\theta$:\n$$\\frac{\\partial}{\\partial\\theta}\\ell(\\theta \\mid y) = -\\frac{k}{k+\\theta} + \\frac{y}{\\theta} - \\frac{y}{k+\\theta} = \\frac{y}{\\theta} - \\frac{y+k}{k+\\theta} = \\frac{y(k+\\theta) - \\theta(y+k)}{\\theta(k+\\theta)} = \\frac{k(y-\\theta)}{\\theta(k+\\theta)}$$\nThe Fisher information is the expectation of the squared score:\n$$I_1(\\theta) = \\mathbb{E}\\left[\\left(\\frac{k(Y-\\theta)}{\\theta(k+\\theta)}\\right)^2\\right] = \\frac{k^2}{\\theta^2(k+\\theta)^2}\\mathbb{E}\\left[(Y-\\theta)^2\\right] = \\frac{k^2}{\\theta^2(k+\\theta)^2}\\mathrm{Var}(Y)$$\nThe problem statement provides the variance for this model: $\\mathrm{Var}(Y) = \\theta + \\theta^2/k$. Substituting this expression:\n$$I_1(\\theta) = \\frac{k^2}{\\theta^2(k+\\theta)^2}\\left(\\theta + \\frac{\\theta^2}{k}\\right) = \\frac{k^2}{\\theta^2(k+\\theta)^2}\\left(\\frac{k\\theta+\\theta^2}{k}\\right) = \\frac{k^2}{\\theta^2(k+\\theta)^2}\\frac{\\theta(k+\\theta)}{k}$$\n$$I_1(\\theta) = \\frac{k}{\\theta(k+\\theta)}$$\n\n**Model C: Normal distribution with structured variance**\n\nFor a single observation $Y \\sim \\mathcal{N}(\\mu=\\theta, \\sigma^2(\\theta)=\\sigma^2+\\beta\\theta)$, the probability density function (PDF) is:\n$$p(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2+\\beta\\theta)}} \\exp\\left(-\\frac{(y-\\theta)^2}{2(\\sigma^2+\\beta\\theta)}\\right)$$\nLet $V(\\theta) = \\sigma^2+\\beta\\theta$. The log-likelihood is:\n$$\\ell(\\theta \\mid y) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(V(\\theta)) - \\frac{(y-\\theta)^2}{2V(\\theta)}$$\nThe score is:\n$$\\frac{\\partial}{\\partial\\theta}\\ell(\\theta \\mid y) = -\\frac{V'(\\theta)}{2V(\\theta)} - \\frac{2(y-\\theta)(-1)V(\\theta) - (y-\\theta)^2 V'(\\theta)}{2V(\\theta)^2} = \\frac{y-\\theta}{V(\\theta)} + \\frac{V'(\\theta)}{2V(\\theta)^2}\\big((y-\\theta)^2 - V(\\theta)\\big)$$\nHere, $V'(\\theta) = \\beta$. Let $Z = Y-\\theta$ and $V = V(\\theta)$. The score is $\\frac{Z}{V} + \\frac{\\beta}{2V^2}(Z^2-V)$. We compute $I_1(\\theta) = \\mathbb{E}[(\\text{score})^2]$.\n$$I_1(\\theta) = \\mathbb{E}\\left[ \\left(\\frac{Z}{V} + \\frac{\\beta}{2V^2}(Z^2-V)\\right)^2 \\right] = \\mathbb{E}\\left[ \\frac{Z^2}{V^2} + \\frac{\\beta Z(Z^2-V)}{V^3} + \\frac{\\beta^2}{4V^4}(Z^2-V)^2 \\right]$$\nFor $Y \\sim \\mathcal{N}(\\theta,V)$, we have $Z \\sim \\mathcal{N}(0,V)$. The central moments are $\\mathbb{E}[Z]=0$, $\\mathbb{E}[Z^2]=V$, $\\mathbb{E}[Z^3]=0$, and $\\mathbb{E}[Z^4]=3V^2$. The cross-term's expectation $\\mathbb{E}[Z(Z^2-V)]$ is $\\mathbb{E}[Z^3] - V\\mathbb{E}[Z] = 0-0=0$. Thus:\n$$I_1(\\theta) = \\frac{\\mathbb{E}[Z^2]}{V^2} + \\frac{\\beta^2}{4V^4}\\mathbb{E}[(Z^2-V)^2] = \\frac{V}{V^2} + \\frac{\\beta^2}{4V^4}(\\mathbb{E}[Z^4] - 2V\\mathbb{E}[Z^2] + V^2)$$\n$$I_1(\\theta) = \\frac{1}{V} + \\frac{\\beta^2}{4V^4}(3V^2 - 2V \\cdot V + V^2) = \\frac{1}{V} + \\frac{\\beta^2(2V^2)}{4V^4} = \\frac{1}{V} + \\frac{\\beta^2}{2V^2}$$\nSubstituting $V(\\theta) = \\sigma^2+\\beta\\theta$:\n$$I_1(\\theta) = \\frac{1}{\\sigma^2+\\beta\\theta} + \\frac{\\beta^2}{2(\\sigma^2+\\beta\\theta)^2}$$\nFor the case $\\theta=0$ (Test Case $7$), since $\\sigma > 0$, the expression for $I_1(\\theta)$ is continuous at $\\theta=0$. We can directly substitute:\n$$I_1(0) = \\frac{1}{\\sigma^2} + \\frac{\\beta^2}{2\\sigma^4} = \\frac{2\\sigma^2+\\beta^2}{2\\sigma^4}$$\n\n**Cramér-Rao Lower Bound Formulae**\n\nThe CRLB for an unbiased estimator $\\widehat{\\theta}$ from $N$ i.i.d. samples is $\\mathrm{CRLB}(\\theta; N) = \\frac{1}{N I_1(\\theta)}$.\n-   **Model A:** $\\mathrm{CRLB}_A(\\theta; N) = \\frac{1}{N (1/\\theta)} = \\frac{\\theta}{N}$\n-   **Model B:** $\\mathrm{CRLB}_B(\\theta; N, k) = \\frac{1}{N \\frac{k}{\\theta(k+\\theta)}} = \\frac{\\theta(k+\\theta)}{Nk}$\n-   **Model C:** $\\mathrm{CRLB}_C(\\theta; N, \\sigma, \\beta) = \\frac{1}{N \\left(\\frac{1}{\\sigma^2+\\beta\\theta} + \\frac{\\beta^2}{2(\\sigma^2+\\beta\\theta)^2}\\right)} = \\frac{2(\\sigma^2+\\beta\\theta)^2}{N(2(\\sigma^2+\\beta\\theta)+\\beta^2)}$\n\nThese formulae will now be implemented to compute the results for the test suite.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Cramér-Rao Lower Bound (CRLB) for an estimator of parameter theta\n    under three different statistical models of measurement noise.\n    The solution is based on the analytical derivation of the Fisher Information for each model.\n    \"\"\"\n\n    # Test suite from the problem statement.\n    # Each case is a tuple: (model_char, N, theta, extra_params_tuple)\n    test_cases = [\n        ('A', 100, 12.5, None),\n        ('A', 5, 0.2, None),\n        ('B', 100, 12.5, (20,)),\n        ('B', 100, 12.5, (10**6,)),\n        ('C', 50, 1000, (100, 1)),\n        ('C', 50, 1000, (100, 0)),\n        ('C', 10, 0, (20, 2)),\n        ('B', 30, 5, (0.5,)),\n        ('A', 1, 0.001, None),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        model, N, theta, extra_params = case\n        crlb = 0.0\n\n        if model == 'A':\n            # CRLB for Poisson model: theta / N\n            # I_1(theta) = 1/theta\n            crlb = theta / N\n        \n        elif model == 'B':\n            # CRLB for Negative Binomial model: theta * (k + theta) / (N * k)\n            # I_1(theta) = k / (theta * (k + theta))\n            k = extra_params[0]\n            crlb = theta * (k + theta) / (N * k)\n\n        elif model == 'C':\n            # CRLB for Normal model with variance = sigma^2 + beta*theta\n            # I_1(theta) = 1/(sigma^2 + beta*theta) + beta^2 / (2*(sigma^2 + beta*theta)^2)\n            sigma, beta = extra_params\n            \n            # The formula is robust for theta=0 as long as sigma > 0.\n            var_theta = sigma**2 + beta * theta\n            \n            # Handle potential division by zero if var_theta is zero, though problem constraints prevent this.\n            if var_theta == 0:\n                # This case is not expected based on problem constraints (sigma > 0, theta >= 0, beta >= 0)\n                # except potentially theta=0, sigma=0 which is invalid.\n                # In a physical system, variance is never exactly zero.\n                # If it occurs, the Fisher Information would be infinite, and CRLB zero.\n                crlb = 0.0\n            else:\n                numerator = 2 * var_theta**2\n                denominator = N * (2 * var_theta + beta**2)\n                if denominator == 0:\n                    # Also not expected. Would imply infinite CRLB.\n                    crlb = np.inf\n                else:\n                    crlb = numerator / denominator\n\n        results.append(crlb)\n\n    # Format the output string as required: comma-separated values rounded to 8 decimal places, in brackets.\n    output_str = f\"[{','.join([f'{r:.8f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2749353"}, {"introduction": "Many key quantities in synthetic biology, such as fold-change or promoter activity, are not measured directly but are calculated from multiple raw inputs, each with its own uncertainty. This exercise demonstrates how to use the delta method, a technique based on Taylor expansion, to propagate these uncertainties through a calculation. By doing so, you will compute the total variance of a derived quantity and construct an \"uncertainty budget\" to pinpoint the dominant sources of experimental noise [@problem_id:2749348].", "problem": "Consider a high-throughput fluorescence assay in synthetic biology where a promoter’s activity is reported as a calibrated fold-change relative to a reference. The reported quantity is the function $Q$ of four jointly distributed random variables $X = (I_s, I_r, B, k)$ defined by\n$$\nQ = \\frac{k\\,(I_s - B)}{I_r - B},\n$$\nwhere $I_s$ is the instrument-reported mean intensity for the sample, $I_r$ is the instrument-reported mean intensity for a reference, $B$ is a background offset measured in the same run, and $k$ is a calibration factor mapping arbitrary units to a normalized scale. Assume that $(I_s, I_r, B, k)$ is well approximated as jointly Gaussian near its mean $\\mu = (\\mu_s, \\mu_r, \\mu_b, k_0)$ with standard deviations $\\sigma = (\\sigma_s, \\sigma_r, \\sigma_b, \\sigma_k)$. Assume that $I_s$ and $I_r$ have correlation coefficient $\\rho$ and that $B$ and $k$ are independent of $(I_s, I_r)$ and of each other. You may assume $\\mu_r > \\mu_b$ and $\\mu_s > \\mu_b$ for all test cases so that the denominator is positive at the linearization point.\n\nTask 1 (derivation): Starting only from first principles of variance and covariance definitions, linearization by first-order Taylor expansion, and the definition of covariance propagation for linear transformations, derive a first-order approximation to $\\mathrm{Var}(Q)$ in terms of the gradient of $Q$ evaluated at the mean $\\mu$ and the covariance matrix of $X$. Explicitly express the gradient components as partial derivatives of $Q$ with respect to $I_s$, $I_r$, $B$, and $k$, evaluated at $(\\mu_s, \\mu_r, \\mu_b, k_0)$. Do not invoke any pre-packaged \"shortcut\" formulas; instead, build the result from the definitions of expectation, variance, covariance, and the first-order Taylor approximation.\n\nTask 2 (uncertainty budget): Using the linearized variance from Task 1, express the total approximate variance as a sum of contributions attributable to the marginal variances of $I_s$, $I_r$, $B$, and $k$, together with the contribution from the covariance between $I_s$ and $I_r$. Define the fractional contribution of each term as that term divided by the total approximate variance. Note that the covariance contribution can be negative; report it as a signed decimal. Define the standard uncertainty $u_Q$ as the positive square root of the total approximate variance.\n\nTask 3 (computation and reporting): For each parameter set in the test suite below, compute $u_Q$ and the fractional contributions of the terms corresponding to $I_s$, $I_r$, $B$, $k$, and the covariance between $I_s$ and $I_r$. For each case, output a list of six floating-point numbers in the order $[u_Q, f_{I_s}, f_{I_r}, f_B, f_k, f_{\\mathrm{cov}}]$, where each $f_{\\cdot}$ is the fractional contribution defined in Task 2. Round each value to six decimal places.\n\nTest suite (each case is given as $(\\mu_s, \\sigma_s, \\mu_r, \\sigma_r, \\mu_b, \\sigma_b, k_0, \\sigma_k, \\rho)$):\n- Case A: $(10000, 300, 8000, 250, 500, 50, 1.0, 0.02, 0.6)$.\n- Case B (small denominator difference): $(700, 30, 600, 30, 500, 20, 1.0, 0.05, 0.6)$.\n- Case C (no correlation, exact background): $(9500, 400, 9000, 400, 0, 0, 0.8, 0.01, 0.0)$.\n- Case D (strong positive correlation): $(12000, 600, 11000, 600, 400, 80, 1.2, 0.03, 0.95)$.\n\nFinal output format: Your program should produce a single line of output containing a Python-like list of four sublists, one per case, in the order A, B, C, D. Each sublist must be of the form `[u_Q, f_Is, f_Ir, f_B, f_k, f_cov]` with each entry rounded to six decimal places. For example, an output with two hypothetical cases could look like `[[x_11, x_12, x_13, x_14, x_15, x_16],[x_21, x_22, x_23, x_24, x_25, x_26]]` where each `x_ij` is a decimal rounded to six places. No physical units are required; report pure numbers. Angles are not involved. Percentages must not be used; fractional contributions must be decimals.", "solution": "We begin from first principles. Let $X = (I_s, I_r, B, k)^{\\top}$ be a random vector with mean $\\mu = (\\mu_s, \\mu_r, \\mu_b, k_0)^{\\top}$ and covariance matrix $\\Sigma$. Consider the scalar function\n$$\nQ(X) = \\frac{k\\,(I_s - B)}{I_r - B}.\n$$\nBy the first-order Taylor expansion (linearization) of $Q$ around the mean $\\mu$, we have\n$$\nQ(X) \\approx Q(\\mu) + \\nabla Q(\\mu)^{\\top} (X - \\mu),\n$$\nwhere $\\nabla Q(\\mu)$ is the gradient of $Q$ evaluated at $\\mu$. Taking variance on both sides and using the definition of variance and covariance for linear transformations, namely that for a constant vector $a$ and random vector $X$ with covariance $\\Sigma$, $\\mathrm{Var}(a^{\\top}X) = a^{\\top}\\Sigma a$, we obtain the first-order (delta method) approximation\n$$\n\\mathrm{Var}(Q) \\approx \\nabla Q(\\mu)^{\\top} \\,\\Sigma\\, \\nabla Q(\\mu).\n$$\n\nIt remains to compute the gradient components. Define $N = I_s - B$ and $D = I_r - B$, so that $Q = k\\,N/D$. Using the quotient rule and the chain rule, with partial derivatives taken holding other variables fixed, we obtain\n$$\n\\frac{\\partial Q}{\\partial k} = \\frac{N}{D},\n\\qquad\n\\frac{\\partial Q}{\\partial I_s} = \\frac{k}{D},\n\\qquad\n\\frac{\\partial Q}{\\partial I_r} = -\\,\\frac{k\\,N}{D^2},\n\\qquad\n\\frac{\\partial Q}{\\partial B} = k\\,\\frac{N - D}{D^2}.\n$$\nEvaluating at the mean point $(\\mu_s, \\mu_r, \\mu_b, k_0)$ gives $N_0 = \\mu_s - \\mu_b$, $D_0 = \\mu_r - \\mu_b$, and the gradient vector\n$$\ng = \\nabla Q(\\mu) =\n\\begin{bmatrix}\n\\frac{\\partial Q}{\\partial I_s} \\\\\n\\frac{\\partial Q}{\\partial I_r} \\\\\n\\frac{\\partial Q}{\\partial B} \\\\\n\\frac{\\partial Q}{\\partial k}\n\\end{bmatrix}_{\\mu}\n=\n\\begin{bmatrix}\n\\frac{k_0}{D_0} \\\\\n-\\,\\frac{k_0\\,N_0}{D_0^2} \\\\\nk_0\\,\\frac{N_0 - D_0}{D_0^2} \\\\\n\\frac{N_0}{D_0}\n\\end{bmatrix}.\n$$\n\nUnder the stated assumptions, the covariance matrix $\\Sigma$ has the following nonzero entries: $\\mathrm{Var}(I_s) = \\sigma_s^2$, $\\mathrm{Var}(I_r) = \\sigma_r^2$, $\\mathrm{Var}(B) = \\sigma_b^2$, $\\mathrm{Var}(k) = \\sigma_k^2$, and $\\mathrm{Cov}(I_s, I_r) = \\rho\\,\\sigma_s\\,\\sigma_r$. All covariances involving $B$ or $k$ with other variables are zero, and $\\mathrm{Cov}(I_s, I_s) = \\sigma_s^2$, $\\mathrm{Cov}(I_r, I_r) = \\sigma_r^2$, $\\mathrm{Cov}(B, B) = \\sigma_b^2$, $\\mathrm{Cov}(k, k) = \\sigma_k^2$ by definition.\n\nTherefore, substituting into the quadratic form yields\n$$\n\\mathrm{Var}(Q) \\approx\n\\left(\\frac{\\partial Q}{\\partial I_s}\\right)^{2}\\sigma_s^{2}\n+\n\\left(\\frac{\\partial Q}{\\partial I_r}\\right)^{2}\\sigma_r^{2}\n+\n\\left(\\frac{\\partial Q}{\\partial B}\\right)^{2}\\sigma_b^{2}\n+\n\\left(\\frac{\\partial Q}{\\partial k}\\right)^{2}\\sigma_k^{2}\n+\n2\\left(\\frac{\\partial Q}{\\partial I_s}\\right)\\left(\\frac{\\partial Q}{\\partial I_r}\\right)\\mathrm{Cov}(I_s, I_r).\n$$\nThis expression is a direct application of the bilinear form $g^{\\top}\\Sigma g$ and the specified sparsity of $\\Sigma$. The standard uncertainty is then defined as\n$$\nu_Q = \\sqrt{\\mathrm{Var}(Q)}.\n$$\n\nFor the uncertainty budget, define the following additive contributions:\n$$\nT_{I_s} = \\left(\\frac{\\partial Q}{\\partial I_s}\\right)^{2}\\sigma_s^{2},\\quad\nT_{I_r} = \\left(\\frac{\\partial Q}{\\partial I_r}\\right)^{2}\\sigma_r^{2},\\quad\nT_{B} = \\left(\\frac{\\partial Q}{\\partial B}\\right)^{2}\\sigma_b^{2},\\quad\nT_{k} = \\left(\\frac{\\partial Q}{\\partial k}\\right)^{2}\\sigma_k^{2},\\quad\nT_{\\mathrm{cov}} = 2\\left(\\frac{\\partial Q}{\\partial I_s}\\right)\\left(\\frac{\\partial Q}{\\partial I_r}\\right)\\rho\\,\\sigma_s\\,\\sigma_r.\n$$\nThe total is\n$$\nT_{\\mathrm{tot}} = T_{I_s} + T_{I_r} + T_{B} + T_{k} + T_{\\mathrm{cov}},\n$$\nand $u_Q = \\sqrt{T_{\\mathrm{tot}}}$ provided $T_{\\mathrm{tot}} > 0$. The fractional contributions are then defined as\n$$\nf_{I_s} = \\frac{T_{I_s}}{T_{\\mathrm{tot}}},\\quad\nf_{I_r} = \\frac{T_{I_r}}{T_{\\mathrm{tot}}},\\quad\nf_{B} = \\frac{T_{B}}{T_{\\mathrm{tot}}},\\quad\nf_{k} = \\frac{T_{k}}{T_{\\mathrm{tot}}},\\quad\nf_{\\mathrm{cov}} = \\frac{T_{\\mathrm{cov}}}{T_{\\mathrm{tot}}}.\n$$\nNote that $f_{\\mathrm{cov}}$ can be negative, especially when $\\rho > 0$ because $\\frac{\\partial Q}{\\partial I_s} > 0$ and $\\frac{\\partial Q}{\\partial I_r} < 0$ for $N_0, D_0, k_0 > 0$, making $T_{\\mathrm{cov}}$ negative and potentially reducing the total variance.\n\nAlgorithmic steps for each case:\n1. Compute $N_0 = \\mu_s - \\mu_b$ and $D_0 = \\mu_r - \\mu_b$.\n2. Compute the gradient components at $\\mu$: $\\frac{\\partial Q}{\\partial I_s} = k_0/D_0$, $\\frac{\\partial Q}{\\partial I_r} = -k_0 N_0/D_0^2$, $\\frac{\\partial Q}{\\partial B} = k_0 (N_0 - D_0)/D_0^2$, $\\frac{\\partial Q}{\\partial k} = N_0/D_0$.\n3. Compute the five contributions $T_{I_s}$, $T_{I_r}$, $T_{B}$, $T_{k}$, $T_{\\mathrm{cov}}$ using the given standard deviations and $\\rho$.\n4. Sum to get $T_{\\mathrm{tot}}$ and then $u_Q = \\sqrt{T_{\\mathrm{tot}}}$, and compute the fractional contributions $f_{\\cdot}$ by dividing each $T_{\\cdot}$ by $T_{\\mathrm{tot}}$.\n5. Round $u_Q$ and all $f_{\\cdot}$ to six decimal places and report them in the specified order.\n\nApplying this procedure to the provided test suite yields the required output. The calculation is numerically stable provided $D_0$ is not too small and the parameter sets satisfy $\\mu_r > \\mu_b$ and $\\mu_s > \\mu_b$, which is guaranteed by the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_uncertainty_and_budget(mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho):\n    # Compute N0 and D0\n    N0 = mu_s - mu_b\n    D0 = mu_r - mu_b\n\n    # Gradient components at the mean\n    dQ_dIs = k0 / D0\n    dQ_dIr = -k0 * N0 / (D0 ** 2)\n    dQ_dB  = k0 * (N0 - D0) / (D0 ** 2)\n    dQ_dk  = N0 / D0\n\n    # Variances and covariance\n    var_Is = sigma_s ** 2\n    var_Ir = sigma_r ** 2\n    var_B  = sigma_b ** 2\n    var_k  = sigma_k ** 2\n    cov_IsIr = rho * sigma_s * sigma_r\n\n    # Contributions\n    T_Is  = (dQ_dIs ** 2) * var_Is\n    T_Ir  = (dQ_dIr ** 2) * var_Ir\n    T_B   = (dQ_dB  ** 2) * var_B\n    T_k   = (dQ_dk  ** 2) * var_k\n    T_cov = 2.0 * dQ_dIs * dQ_dIr * cov_IsIr\n\n    T_tot = T_Is + T_Ir + T_B + T_k + T_cov\n\n    # Numerical guard: total variance should be non-negative; clamp tiny negatives to zero due to rounding\n    if T_tot < 0 and abs(T_tot) < 1e-15:\n        T_tot = 0.0\n    if T_tot <= 0:\n        # In pathological cases (not expected in the test suite), return zeros\n        u_Q = 0.0\n        f_Is = f_Ir = f_B = f_k = f_cov = 0.0\n    else:\n        u_Q = np.sqrt(T_tot)\n        f_Is  = T_Is  / T_tot\n        f_Ir  = T_Ir  / T_tot\n        f_B   = T_B   / T_tot\n        f_k   = T_k   / T_tot\n        f_cov = T_cov / T_tot\n\n    return u_Q, f_Is, f_Ir, f_B, f_k, f_cov\n\ndef fmt6(x: float) -> str:\n    s = f\"{x:.6f}\"\n    # Normalize negative zero to zero\n    if s == \"-0.000000\":\n        s = \"0.000000\"\n    return s\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho)\n    test_cases = [\n        (10000.0, 300.0, 8000.0, 250.0, 500.0, 50.0, 1.0, 0.02, 0.6),   # Case A\n        (700.0,   30.0,  600.0,  30.0,  500.0, 20.0, 1.0, 0.05, 0.6),   # Case B\n        (9500.0,  400.0, 9000.0, 400.0, 0.0,   0.0,  0.8, 0.01, 0.0),   # Case C\n        (12000.0, 600.0, 11000.0,600.0, 400.0, 80.0, 1.2, 0.03, 0.95),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho = case\n        u_Q, f_Is, f_Ir, f_B, f_k, f_cov = compute_uncertainty_and_budget(\n            mu_s, sigma_s, mu_r, sigma_r, mu_b, sigma_b, k0, sigma_k, rho\n        )\n        # Round and format to six decimals as strings\n        formatted = [\n            fmt6(u_Q),\n            fmt6(f_Is),\n            fmt6(f_Ir),\n            fmt6(f_B),\n            fmt6(f_k),\n            fmt6(f_cov),\n        ]\n        results.append(f\"[{','.join(formatted)}]\")\n\n    # Final print statement in the exact required format: single line with list of sublists.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2749348"}, {"introduction": "Real-world single-cell data rarely conforms to simple noise models; it often exhibits complex features like overdispersion or an excess of zero counts that a basic Poisson model cannot capture. This hands-on practice addresses this challenge by having you implement and compare three essential statistical models—Poisson, Negative Binomial, and Zero-Inflated Poisson. You will use Maximum Likelihood Estimation and the Akaike Information Criterion to select the most appropriate model for realistic single-cell mRNA count datasets, a critical step for accurate downstream analysis [@problem_id:2749367].", "problem": "You are analyzing single-cell messenger ribonucleic acid (mRNA) count measurements from a synthetic gene circuit. Each dataset consists of independent and identically distributed nonnegative integer counts from cells measured under a fixed condition. Measurement noise and biological variability can be modeled using established discrete distributions: a Poisson model for intrinsic birth–death noise, a Negative Binomial (NB) model for overdispersion arising from cell-to-cell variability, and a Zero-Inflated Poisson (ZIP) model for structural zeros arising from gene-inactive subpopulations or measurement dropout. Starting only from core probabilistic definitions (probability mass functions, independence of replicates, and mixture modeling), and well-tested facts about model selection using likelihood-based criteria, you must construct maximum likelihood estimators, compare models, and report a scalar summary of noise.\n\nFundamental base to use:\n- Treat the count for each cell as an independent realization of a nonnegative integer-valued random variable.\n- The Poisson model assumes a single rate parameter $\\lambda$.\n- The Negative Binomial (NB) model assumes a mean–overdispersion representation with mean $\\mu$ and a positive dispersion (size) parameter $r$ such that the variance exceeds the mean.\n- The Zero-Inflated Poisson (ZIP) model is a two-component mixture with a structural-zero probability $\\pi$ and a Poisson component with rate $\\lambda$, producing zeros either structurally or from the Poisson component.\n- Use Maximum Likelihood Estimation (MLE) to estimate parameters for each model from the observed counts.\n- Use Akaike Information Criterion (AIC), defined in terms of the maximized log-likelihood and the number of free parameters $k$, to select the model with the smallest AIC for each dataset.\n- After selecting a model, summarize noise by the coefficient of variation squared $\\mathrm{CV}^2$, defined as $\\mathrm{Var}(X)/[\\mathbb{E}(X)]^2$, computed under the selected model at its MLE.\n\nConstraints and implementation requirements:\n- Treat all datasets as unitless counts. No physical units are involved.\n- Angles are not used.\n- All final numeric outputs must be rounded to $6$ decimal places.\n- For each dataset, produce a pair $[m, c]$ where $m$ is an integer code for the selected model and $c$ is the rounded $\\mathrm{CV}^2$:\n  - $m = 0$ for Poisson,\n  - $m = 1$ for Negative Binomial,\n  - $m = 2$ for Zero-Inflated Poisson.\n- Your program should produce a single line of output containing a comma-separated list of these per-dataset results, enclosed in square brackets, for example: `[[m_1,c_1],[m_2,c_2],...]`.\n\nTest suite (datasets):\n- Dataset $1$ (typical intrinsic noise): $[8,12,9,11,10,7,13,10,9,11,8,12,10,9,11]$.\n- Dataset $2$ (overdispersion without strong zero inflation): $[0,3,2,25,18,7,0,15,12,30,4,6,21,9,11,17,5]$.\n- Dataset $3$ (pronounced zero inflation): $[0,0,0,0,0,0,0,0,0,0,0,0,6,4,7,3,5,8,2,6]$.\n- Dataset $4$ (boundary case with rare expression): $[0,0,0,0,0,1,0,0,0,1]$.\n\nFinal output format:\n- Your program must print a single line containing a list of $4$ lists, one per dataset, in the exact order above. Each inner list must be $[m,c]$ with $m$ as defined above and $c$ equal to the selected model’s $\\mathrm{CV}^2$ rounded to $6$ decimals. For example, a valid output would look like `[[0,0.123456],[1,3.141593],[2,0.271828],[2,42.000000]]`.", "solution": "The problem requires a rigorous statistical analysis of single-cell mRNA count data. The objective is to identify the most appropriate statistical model for each of four datasets from a candidate set of three—Poisson, Negative Binomial (NB), and Zero-Inflated Poisson (ZIP)—and to quantify the noise of the underlying process using the selected model. This task is a standard application of maximum likelihood estimation (MLE) and information-theoretic model selection. The entire procedure will be derived from fundamental principles of probability theory and statistics.\n\nLet a dataset of counts be denoted by $D = \\{x_1, x_2, \\ldots, x_n\\}$, representing $n$ independent and identically distributed (i.i.d.) observations of a random variable $X$.\n\n**1. Model Definitions and Likelihood Functions**\n\nThe analysis is predicated on the construction of the likelihood function $L(\\theta | D)$, which is the joint probability of observing the data $D$ given the model parameters $\\theta$. Due to the i.i.d. assumption, the likelihood is the product of the individual probability mass functions (PMFs): $L(\\theta | D) = \\prod_{i=1}^{n} P(X=x_i | \\theta)$. For computational stability and mathematical convenience, we work with the log-likelihood, $\\ln L(\\theta | D) = \\sum_{i=1}^{n} \\ln P(X=x_i | \\theta)$.\n\n*   **Poisson Model**: This model is defined by a single rate parameter $\\lambda > 0$, representing both the mean and variance of the distribution. The PMF is:\n    $$ P(X=k | \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\n    The log-likelihood function for the dataset $D$ under the Poisson model is:\n    $$ \\ln L(\\lambda | D) = \\sum_{i=1}^{n} (x_i \\ln \\lambda - \\lambda - \\ln(x_i!)) = (\\ln \\lambda) \\sum_{i=1}^{n} x_i - n\\lambda - \\sum_{i=1}^{n} \\ln(x_i!) $$\n\n*   **Negative Binomial (NB) Model**: The NB model accommodates overdispersion (variance greater than the mean). It is parameterized here by its mean $\\mu > 0$ and a positive dispersion parameter $r > 0$. The variance is given by $\\mathrm{Var}(X) = \\mu + \\mu^2/r$. The PMF is derived from the standard $(r, p)$ parameterization where $p = r/(r+\\mu)$:\n    $$ P(X=k | \\mu, r) = \\frac{\\Gamma(k+r)}{k!\\Gamma(r)} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^k $$\n    where $\\Gamma(\\cdot)$ is the gamma function. The log-likelihood is:\n    $$ \\ln L(\\mu, r | D) = \\sum_{i=1}^{n} \\left[ \\ln\\Gamma(x_i+r) - \\ln\\Gamma(r) - \\ln(x_i!) + r\\ln\\left(\\frac{r}{r+\\mu}\\right) + x_i\\ln\\left(\\frac{\\mu}{r+\\mu}\\right) \\right] $$\n\n*   **Zero-Inflated Poisson (ZIP) Model**: This is a two-component mixture model used for datasets with an excess of zero counts. It is parameterized by a Poisson rate $\\lambda > 0$ and a zero-inflation probability $0 \\le \\pi < 1$. A zero is observed with probability $\\pi$ from a \"structural zero\" process, or with probability $(1-\\pi)e^{-\\lambda}$ from the Poisson process.\n    The PMF is:\n    $$\n    P(X=k | \\pi, \\lambda) = \n    \\begin{cases} \n    \\pi + (1-\\pi)e^{-\\lambda} & \\text{if } k=0 \\\\\n    (1-\\pi)\\frac{\\lambda^k e^{-\\lambda}}{k!} & \\text{if } k > 0\n    \\end{cases}\n    $$\n    Let $n_0$ be the number of zero counts and $D_{>0}$ be the non-zero counts in the dataset. The log-likelihood is:\n    $$ \\ln L(\\pi, \\lambda | D) = n_0 \\ln(\\pi + (1-\\pi)e^{-\\lambda}) + \\sum_{x_i \\in D_{>0}} \\ln\\left((1-\\pi)\\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\right) $$\n\n**2. Maximum Likelihood Estimation (MLE)**\n\nThe principle of MLE directs us to find the parameter values $\\hat{\\theta}_{MLE}$ that maximize the log-likelihood function $\\ln L(\\theta | D)$.\n\n*   For the **Poisson** model, the MLE for $\\lambda$ is found by setting the derivative of the log-likelihood to zero, which yields a closed-form solution:\n    $$ \\frac{\\partial \\ln L}{\\partial \\lambda} = \\frac{1}{\\lambda}\\sum_{i=1}^{n}x_i - n = 0 \\implies \\hat{\\lambda}_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}x_i = \\bar{x} $$\n    The MLE is the sample mean.\n\n*   For the **NB** and **ZIP** models, no general closed-form solutions for the MLEs exist. Therefore, we must resort to numerical methods to maximize the log-likelihood function subject to parameter constraints ($\\mu, r, \\lambda > 0$ and $0 \\le \\pi < 1$). We will minimize the negative log-likelihood using a quasi-Newton algorithm, specifically L-BFGS-B, which efficiently handles box constraints.\n\n**3. Model Selection**\n\nTo compare models with different numbers of parameters, we use the Akaike Information Criterion (AIC). AIC provides a relative measure of model quality, balancing the goodness of fit (maximized log-likelihood) against model complexity (number of free parameters, $k$). The model with the lowest AIC is preferred.\n$$ \\mathrm{AIC} = 2k - 2\\ln L(\\hat{\\theta}_{MLE} | D) $$\nThe number of free parameters for each model is specified as:\n*   Poisson: $k=1$ (parameter $\\lambda$)\n*   Negative Binomial: $k=2$ (parameters $\\mu, r$)\n*   Zero-Inflated Poisson: $k=2$ (parameters $\\pi, \\lambda$)\n\n**4. Noise Quantification: Coefficient of Variation Squared ($\\mathrm{CV}^2$)**\n\nAfter selecting the best-fitting model, the biological noise is summarized by the squared coefficient of variation, defined as $\\mathrm{CV}^2 = \\mathrm{Var}(X) / [\\mathbb{E}(X)]^2$. The expectation $\\mathbb{E}(X)$ and variance $\\mathrm{Var}(X)$ are computed using the MLE parameters of the chosen model.\n\n*   **Poisson**: $\\mathbb{E}(X) = \\lambda$, $\\mathrm{Var}(X) = \\lambda$.\n    $$ \\mathrm{CV}^2 = \\frac{\\lambda}{\\lambda^2} = \\frac{1}{\\lambda} $$\n*   **Negative Binomial**: $\\mathbb{E}(X) = \\mu$, $\\mathrm{Var}(X) = \\mu + \\mu^2/r$.\n    $$ \\mathrm{CV}^2 = \\frac{\\mu + \\mu^2/r}{\\mu^2} = \\frac{1}{\\mu} + \\frac{1}{r} $$\n*   **Zero-Inflated Poisson**: $\\mathbb{E}(X) = (1-\\pi)\\lambda$, $\\mathrm{Var}(X) = (1-\\pi)\\lambda(1+\\pi\\lambda)$.\n    $$ \\mathrm{CV}^2 = \\frac{(1-\\pi)\\lambda(1+\\pi\\lambda)}{((1-\\pi)\\lambda)^2} = \\frac{1+\\pi\\lambda}{(1-\\pi)\\lambda} $$\n\n**5. Computational Implementation**\n\nThe described procedure is implemented in a Python program. For each dataset, functions are defined to compute the negative log-likelihood for each of the three models. For the NB and ZIP models, `scipy.optimize.minimize` is used to find the parameter values that minimize this function. After obtaining the MLE parameters for all models, their corresponding AIC values are calculated and compared to select the optimal model. Finally, the $\\mathrm{CV}^2$ is computed using the parameters of the winning model. The final results are formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy import optimize\nfrom scipy import special\n\ndef solve():\n    \"\"\"\n    Main solver function to process all datasets and print the final results.\n    \"\"\"\n    test_cases = [\n        [8, 12, 9, 11, 10, 7, 13, 10, 9, 11, 8, 12, 10, 9, 11],\n        [0, 3, 2, 25, 18, 7, 0, 15, 12, 30, 4, 6, 21, 9, 11, 17, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 4, 7, 3, 5, 8, 2, 6],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n    ]\n\n    results = []\n    for data_list in test_cases:\n        data = np.array(data_list)\n        result = analyze_dataset(data)\n        results.append(result)\n\n    # Format output as a string representation of a list of lists.\n    # Ex: [[0,0.1],[1,0.56789]]\n    output_str = \"[\" + \",\".join([f\"[{m},{c}]\" for m, c in results]) + \"]\"\n    print(output_str)\n\ndef analyze_dataset(data):\n    \"\"\"\n    Analyzes a single dataset: fits all three models, selects the best one via AIC,\n    and computes its CV^2.\n    \"\"\"\n    # Small epsilon to avoid log(0) issues in optimization bounds.\n    epsilon = 1e-9\n\n    # --- Poisson Model ---\n    # Closed-form MLE\n    lambda_mle = np.mean(data)\n    if lambda_mle == 0: # All-zero data case\n        logL_pois = 0\n        cv2_pois = 0.0\n    else:\n        logL_pois = np.sum(\n            data * np.log(lambda_mle) - lambda_mle - special.gammaln(data + 1)\n        )\n        cv2_pois = 1.0 / lambda_mle\n    k_pois = 1\n    aic_pois = 2 * k_pois - 2 * logL_pois\n    \n    # --- Negative Binomial Model ---\n    def neg_logL_nb(params, data):\n        mu, r = params\n        # Use scipy's nbinom logpmf for numerical stability\n        # p = r / (r + mu)\n        # Using log-transformed p to prevent underflow inside logpmf\n        log_p = np.log(r) - np.log(r + mu)\n        # Scipy's nbinom takes n=r and p.\n        logpmf_vals = special.log_ndtr(-log_p * r)\n        # The above is not scipy.stats.nbinom.logpmf but a different function.\n        # Direct implementation is more transparent.\n        if r <= 0 or mu <= 0: return np.inf\n        \n        # log(Gamma(k+r)) - log(k!) - log(Gamma(r))\n        # gammaln(data+r) - gammaln(data+1) - gammaln(r)\n        term1 = special.gammaln(data + r) - special.gammaln(data + 1) - special.gammaln(r)\n        term2 = r * np.log(r) - r * np.log(r + mu)\n        term3 = data * np.log(mu) - data * np.log(r + mu)\n        \n        logL = np.sum(term1 + term2 + term3)\n        return -logL\n\n    # Initial guess using method of moments\n    mean_data = np.mean(data)\n    var_data = np.var(data, ddof=1) if len(data) > 1 else 0\n    mu_init = mean_data if mean_data > 0 else epsilon\n    if var_data > mean_data:\n        r_init = mean_data**2 / (var_data - mean_data)\n    else:\n        # Under-dispersed, NB approaches Poisson, so r is large\n        r_init = 1000.0\n    \n    res_nb = optimize.minimize(\n        neg_logL_nb,\n        x0=[mu_init, r_init],\n        args=(data,),\n        method='L-BFGS-B',\n        bounds=[(epsilon, None), (epsilon, None)]\n    )\n\n    mu_mle_nb, r_mle_nb = res_nb.x\n    logL_nb = -res_nb.fun\n    k_nb = 2\n    aic_nb = 2 * k_nb - 2 * logL_nb\n    cv2_nb = (1.0 / mu_mle_nb) + (1.0 / r_mle_nb)\n\n    # --- Zero-Inflated Poisson Model ---\n    def neg_logL_zip(params, data, n0, non_zeros):\n        pi, lam = params\n        if not (0 <= pi < 1 and lam > 0): return np.inf\n        \n        logL_zeros = n0 * np.log(pi + (1 - pi) * np.exp(-lam))\n        \n        n_pos = len(non_zeros)\n        if n_pos > 0:\n            logL_non_zeros = n_pos * np.log(1 - pi) + \\\n                             np.sum(non_zeros) * np.log(lam) - \\\n                             n_pos * lam - \\\n                             np.sum(special.gammaln(non_zeros + 1))\n        else: # Case of all zeros\n            logL_non_zeros = 0\n            \n        logL = logL_zeros + logL_non_zeros\n        return -logL\n\n    n0 = np.sum(data == 0)\n    non_zeros = data[data > 0]\n\n    # Initial guesses\n    pi_init = n0 / len(data)\n    if len(non_zeros) > 0:\n        lam_init = np.mean(non_zeros)\n    else:\n        lam_init = epsilon # all zeros case\n    \n    # Ensure pi_init is not 1 to avoid log(0)\n    if pi_init >= 1.0:\n        pi_init = 1.0 - epsilon\n\n    res_zip = optimize.minimize(\n        neg_logL_zip,\n        x0=[pi_init, lam_init],\n        args=(data, n0, non_zeros),\n        method='L-BFGS-B',\n        bounds=[(0, 1.0 - epsilon), (epsilon, None)]\n    )\n\n    pi_mle_zip, lam_mle_zip = res_zip.x\n    logL_zip = -res_zip.fun\n    k_zip = 2\n    aic_zip = 2 * k_zip - 2 * logL_zip\n    \n    mean_zip = (1 - pi_mle_zip) * lam_mle_zip\n    if mean_zip > 0 :\n        cv2_zip = ((1 - pi_mle_zip) * lam_mle_zip * (1 + pi_mle_zip * lam_mle_zip)) / mean_zip**2\n    else:\n        cv2_zip = 0.0\n\n    # Model selection\n    models = {\n        'poisson': (0, aic_pois, cv2_pois),\n        'nb': (1, aic_nb, cv2_nb),\n        'zip': (2, aic_zip, cv2_zip)\n    }\n\n    # Handle cases where optimization might fail for NB/ZIP, giving high AIC.\n    # Poisson is always stable.\n    best_model_name = 'poisson'\n    min_aic = aic_pois\n\n    if not res_nb.success: aic_nb = np.inf\n    if not res_zip.success: aic_zip = np.inf\n    \n    if aic_nb < min_aic:\n        min_aic = aic_nb\n        best_model_name = 'nb'\n\n    if aic_zip < min_aic:\n        min_aic = aic_zip\n        best_model_name = 'zip'\n\n    model_code, _, cv2_val = models[best_model_name]\n    \n    return [model_code, round(cv2_val, 6)]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2749367"}]}