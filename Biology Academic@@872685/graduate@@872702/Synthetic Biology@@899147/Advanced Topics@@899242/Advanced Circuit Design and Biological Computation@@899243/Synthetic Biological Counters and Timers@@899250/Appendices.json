{"hands_on_practices": [{"introduction": "A fundamental approach to building a synthetic timer involves the accumulation of a protein to a specific threshold. This exercise explores the core dynamics of such a system, modeled by constitutive expression and first-order degradation [@problem_id:2777873]. By deriving the time required to reach a threshold $P_{\\mathrm{th}}$, you will practice translating a simple ordinary differential equation into a key performance metric and investigate how this timing is affected by changes in the protein degradation rate $\\gamma$. This analysis introduces the concept of logarithmic sensitivity, a crucial tool for quantifying the tunability and robustness of synthetic circuits.", "problem": "A synthetic timer protein is expressed constitutively at a constant translation rate $k$ (molecules per unit time) and is degraded by first-order proteolysis at rate constant $\\gamma$ (per unit time). The proteolysis is controlled by a tunable protease tag, such that strengthening the tag changes the degradation rate constant from $\\gamma$ to $\\gamma'$. Assume that messenger ribonucleic acid (mRNA) is not limiting and that the protein dynamics can be modeled as a single-species birth–death process whose mean behavior is governed by the mass-action ordinary differential equation $dP/dt = k - \\gamma P$ with initial condition $P(0) = 0$, where $P(t)$ is the mean protein copy number at time $t$. A cell-intrinsic event is triggered when the protein level first reaches a preset threshold $P_{\\mathrm{th}}$, with $0  \\gamma P_{\\mathrm{th}}  k$ and $0  \\gamma' P_{\\mathrm{th}}  k$ so that the threshold is reachable in finite time in both conditions. In the regime of large copy number and weak intrinsic noise, take the mean first-passage time to threshold to be well approximated by the deterministic threshold-crossing time.\n\nStarting from the above fundamentals and assumptions, derive an analytic expression for the mean first-passage time $T(\\gamma)$ to reach $P_{\\mathrm{th}}$ under degradation rate $\\gamma$, and use it to compute the fractional change in mean first-passage time when the tag strength is altered from $\\gamma$ to $\\gamma'$, defined as $(T(\\gamma') - T(\\gamma))/T(\\gamma)$. Then, compute the logarithmic sensitivity $S = \\partial \\ln T / \\partial \\ln \\gamma$ evaluated at degradation rate $\\gamma$, holding $k$ and $P_{\\mathrm{th}}$ fixed.\n\nExpress both the fractional change and the sensitivity as closed-form analytic expressions in terms of $k$, $\\gamma$, $\\gamma'$, and $P_{\\mathrm{th}}$. No numerical evaluation or rounding is required.", "solution": "The problem statement must first be subjected to rigorous validation.\n\n**Step 1: Extract Givens**\n-   Governing ordinary differential equation (ODE) for the mean protein copy number $P(t)$: $\\frac{dP}{dt} = k - \\gamma P$.\n-   Initial condition: $P(0) = 0$.\n-   Constant translation rate: $k$.\n-   First-order degradation rate constant: $\\gamma$.\n-   Modified degradation rate constant: $\\gamma'$.\n-   Threshold protein copy number: $P_{\\mathrm{th}}$.\n-   Constraints: $0  \\gamma P_{\\mathrm{th}}  k$ and $0  \\gamma' P_{\\mathrm{th}}  k$.\n-   Approximation: The mean first-passage time to threshold is taken to be the deterministic threshold-crossing time.\n-   First objective: Derive an analytic expression for the mean first-passage time, $T(\\gamma)$.\n-   Second objective: Compute the fractional change in mean first-passage time, $(T(\\gamma') - T(\\gamma))/T(\\gamma)$.\n-   Third objective: Compute the logarithmic sensitivity, $S = \\partial \\ln T / \\partial \\ln \\gamma$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is subjected to validation against the required criteria.\n-   **Scientifically Grounded**: The problem is based on the standard mass-action kinetics model for protein expression and degradation ($dP/dt = \\text{production} - \\text{degradation}$), a fundamental concept in systems and synthetic biology. All terms and concepts are well-established. This criterion is satisfied.\n-   **Well-Posed**: The problem provides a linear first-order ODE with a specified initial condition, which guarantees a unique solution for $P(t)$. The constraints $0  \\gamma P_{\\mathrm{th}}  k$ ensure that the steady-state protein level, $P_{\\mathrm{ss}} = k/\\gamma$, is greater than the threshold $P_{\\mathrm{th}}$, making the threshold reachable in finite time. The objectives are clearly defined mathematical operations. This criterion is satisfied.\n-   **Objective**: The problem is stated using precise, unambiguous scientific language. There are no subjective or opinion-based claims. This criterion is satisfied.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a well-defined problem in mathematical biology.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be derived.\n\n**Derivation**\n\nThe analysis begins with the solution of the governing ordinary differential equation for the mean protein copy number, $P(t)$.\n$$\n\\frac{dP}{dt} = k - \\gamma P, \\quad P(0) = 0\n$$\nThis is a first-order linear inhomogeneous differential equation. It can be solved by separation of variables.\n$$\n\\frac{dP}{k - \\gamma P} = dt\n$$\nIntegrate both sides from the initial condition $(t'=0, P'=0)$ to a later state $(t'=t, P'=P(t))$:\n$$\n\\int_{0}^{P(t)} \\frac{dP'}{k - \\gamma P'} = \\int_{0}^{t} dt'\n$$\nThe left-hand side integral is:\n$$\n\\int_{0}^{P(t)} \\frac{dP'}{k - \\gamma P'} = \\left[ -\\frac{1}{\\gamma} \\ln(k - \\gamma P') \\right]_{0}^{P(t)} = -\\frac{1}{\\gamma} \\left[ \\ln(k - \\gamma P(t)) - \\ln(k) \\right] = -\\frac{1}{\\gamma} \\ln\\left(\\frac{k - \\gamma P(t)}{k}\\right)\n$$\nThe right-hand side integral is simply $t$. Equating the two gives:\n$$\nt = -\\frac{1}{\\gamma} \\ln\\left(1 - \\frac{\\gamma P(t)}{k}\\right)\n$$\nTo find the protein concentration as a function of time, $P(t)$, we solve for $P(t)$:\n$$\n-\\gamma t = \\ln\\left(1 - \\frac{\\gamma P(t)}{k}\\right)\n$$\n$$\n\\exp(-\\gamma t) = 1 - \\frac{\\gamma P(t)}{k}\n$$\n$$\n\\frac{\\gamma P(t)}{k} = 1 - \\exp(-\\gamma t)\n$$\n$$\nP(t) = \\frac{k}{\\gamma} \\left(1 - \\exp(-\\gamma t)\\right)\n$$\nThe problem defines the mean first-passage time, denoted as $T(\\gamma)$, as the deterministic time at which the protein level reaches the threshold $P_{\\mathrm{th}}$. We find this time by setting $P(T(\\gamma)) = P_{\\mathrm{th}}$ and solving for $T(\\gamma)$.\n$$\nP_{\\mathrm{th}} = \\frac{k}{\\gamma} \\left(1 - \\exp(-\\gamma T(\\gamma))\\right)\n$$\nRearranging to solve for $T(\\gamma)$:\n$$\n\\frac{\\gamma P_{\\mathrm{th}}}{k} = 1 - \\exp(-\\gamma T(\\gamma))\n$$\n$$\n\\exp(-\\gamma T(\\gamma)) = 1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\n$$\n$$\n-\\gamma T(\\gamma) = \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)\n$$\nThis yields the expression for the mean first-passage time:\n$$\nT(\\gamma) = -\\frac{1}{\\gamma} \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)\n$$\nThe constraint $0  \\gamma P_{\\mathrm{th}}  k$ ensures that $0  1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}  1$, so the logarithm is well-defined and negative, resulting in a positive time $T(\\gamma)$.\n\nNext, we compute the fractional change in time, $(T(\\gamma') - T(\\gamma))/T(\\gamma)$, which can be expressed as $\\frac{T(\\gamma')}{T(\\gamma)} - 1$. Using the derived expression for $T(\\gamma)$, we write $T(\\gamma')$ as:\n$$\nT(\\gamma') = -\\frac{1}{\\gamma'} \\ln\\left(1 - \\frac{\\gamma' P_{\\mathrm{th}}}{k}\\right)\n$$\nThe ratio $\\frac{T(\\gamma')}{T(\\gamma)}$ is:\n$$\n\\frac{T(\\gamma')}{T(\\gamma)} = \\frac{-\\frac{1}{\\gamma'} \\ln\\left(1 - \\frac{\\gamma' P_{\\mathrm{th}}}{k}\\right)}{-\\frac{1}{\\gamma} \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)} = \\frac{\\gamma}{\\gamma'} \\frac{\\ln\\left(1 - \\frac{\\gamma' P_{\\mathrm{th}}}{k}\\right)}{\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)}\n$$\nThe fractional change is thus:\n$$\n\\frac{T(\\gamma') - T(\\gamma)}{T(\\gamma)} = \\frac{\\gamma}{\\gamma'} \\frac{\\ln\\left(1 - \\frac{\\gamma' P_{\\mathrm{th}}}{k}\\right)}{\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)} - 1\n$$\n\nFinally, we compute the logarithmic sensitivity $S = \\partial \\ln T / \\partial \\ln \\gamma$. Using the identity for logarithmic derivatives, $S = \\frac{\\gamma}{T} \\frac{\\partial T}{\\partial \\gamma}$. We must first compute the derivative $\\frac{\\partial T}{\\partial \\gamma}$.\n$$\nT(\\gamma) = -\\frac{1}{\\gamma} \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)\n$$\nUsing the product rule for differentiation, $(uv)' = u'v + uv'$, with $u = -\\gamma^{-1}$ and $v = \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)$:\n$$\n\\frac{\\partial u}{\\partial \\gamma} = \\gamma^{-2}\n$$\n$$\n\\frac{\\partial v}{\\partial \\gamma} = \\frac{1}{1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}} \\cdot \\left(-\\frac{P_{\\mathrm{th}}}{k}\\right) = \\frac{-P_{\\mathrm{th}}}{k - \\gamma P_{\\mathrm{th}}}\n$$\nApplying the product rule:\n$$\n\\frac{\\partial T}{\\partial \\gamma} = \\left(\\frac{1}{\\gamma^2}\\right) \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right) + \\left(-\\frac{1}{\\gamma}\\right) \\left(\\frac{-P_{\\mathrm{th}}}{k - \\gamma P_{\\mathrm{th}}}\\right)\n$$\n$$\n\\frac{\\partial T}{\\partial \\gamma} = \\frac{1}{\\gamma^2} \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right) + \\frac{P_{\\mathrm{th}}}{\\gamma(k - \\gamma P_{\\mathrm{th}})}\n$$\nNow, substitute this into the expression for sensitivity:\n$$\nS = \\frac{\\gamma}{T(\\gamma)} \\left( \\frac{1}{\\gamma^2} \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right) + \\frac{P_{\\mathrm{th}}}{\\gamma(k - \\gamma P_{\\mathrm{th}})} \\right)\n$$\nSubstitute the expression for $T(\\gamma)$:\n$$\nS = \\frac{\\gamma}{-\\frac{1}{\\gamma} \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)} \\left( \\frac{1}{\\gamma^2} \\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right) + \\frac{P_{\\mathrm{th}}}{\\gamma(k - \\gamma P_{\\mathrm{th}})} \\right)\n$$\n$$\nS = \\frac{-\\gamma^2}{\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)} \\left( \\frac{\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)}{\\gamma^2} + \\frac{P_{\\mathrm{th}}}{\\gamma(k - \\gamma P_{\\mathrm{th}})} \\right)\n$$\nDistributing the leading term:\n$$\nS = -1 - \\frac{\\gamma^2 P_{\\mathrm{th}}}{\\gamma(k - \\gamma P_{\\mathrm{th}})\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)}\n$$\nSimplifying this final expression for sensitivity $S$:\n$$\nS = -1 - \\frac{\\gamma P_{\\mathrm{th}}}{(k - \\gamma P_{\\mathrm{th}})\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)}\n$$\nThe two required expressions have been derived.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\gamma}{\\gamma'} \\frac{\\ln\\left(1 - \\frac{\\gamma' P_{\\mathrm{th}}}{k}\\right)}{\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)} - 1  -1 - \\frac{\\gamma P_{\\mathrm{th}}}{(k - \\gamma P_{\\mathrm{th}})\\ln\\left(1 - \\frac{\\gamma P_{\\mathrm{th}}}{k}\\right)} \\end{pmatrix}}\n$$", "id": "2777873"}, {"introduction": "While simple deterministic models provide intuition, the performance of real biological timers is ultimately limited by stochastic noise. This practice introduces the Allan deviation, $\\sigma_{y}(\\tau)$, a rigorous metric from the field of frequency standards used to characterize clock stability [@problem_id:2777922]. You will apply this powerful tool to compare two distinct timer architectures—a limit-cycle oscillator and a leaky integrator—to understand how different circuit topologies filter upstream noise. This exercise provides critical insight into how rational design choices can enhance the precision and reliability of synthetic timekeeping devices.", "problem": "A synthetic biological system is used as a time base for event scheduling. Two distinct timer architectures are implemented and driven by the same upstream stochastic fluctuations arising from gene expression noise that modulate the instantaneous rate of the clock.\n\nArchitecture 1 (oscillator-based timer): The timing signal is produced by a stable limit-cycle oscillator whose phase derivative is the nominal angular frequency multiplied by one plus the fractional frequency fluctuation. Let the fractional frequency fluctuation be a zero-mean stationary white Gaussian process $y(t)$ with autocorrelation $\\mathbb{E}[y(t) y(t')] = N_{y} \\, \\delta(t - t')$, where $\\delta(\\cdot)$ is the Dirac delta function and $N_{y}  0$ is a constant. The Allan deviation is defined via the two-sample Allan variance of fractional frequency,\n$$\n\\sigma_{y}^{2}(\\tau) \\equiv \\frac{1}{2 \\tau^{2}} \\, \\mathbb{E}\\!\\left[\\left(\\int_{0}^{\\tau} y(t) \\, dt \\;-\\; \\int_{\\tau}^{2 \\tau} y(t) \\, dt\\right)^{2}\\right],\n$$\nfor averaging time $\\tau  0$.\n\nArchitecture 2 (leaky integrator timer): The same upstream fluctuation $y(t)$ is post-processed by a first-order linear time-invariant (LTI) low-pass filter with time constant $\\tau_{c}  0$ to smooth the instantaneous fractional frequency. The filtered fractional frequency $y_{\\mathrm{LI}}(t)$ obeys\n$$\n\\frac{d y_{\\mathrm{LI}}}{dt} \\;=\\; -\\frac{1}{\\tau_{c}} \\, y_{\\mathrm{LI}} \\;+\\; \\frac{1}{\\tau_{c}} \\, y(t),\n$$\nwhich is a unity direct current (DC) gain low-pass filter. The Allan deviation of the leaky integrator timer is defined by replacing $y(t)$ with $y_{\\mathrm{LI}}(t)$ in the same two-sample Allan variance definition above.\n\nStarting only from the definitions given, the whiteness of $y(t)$, and linear systems theory for first-order filters, derive closed-form expressions for the Allan deviations $\\sigma_{y,\\mathrm{OSC}}(\\tau)$ and $\\sigma_{y,\\mathrm{LI}}(\\tau)$ of the oscillator-based and leaky integrator timers, respectively. Then, provide the closed-form analytical expression for the ratio\n$$\nR(\\tau,\\tau_{c}) \\equiv \\frac{\\sigma_{y,\\mathrm{LI}}(\\tau)}{\\sigma_{y,\\mathrm{OSC}}(\\tau)}.\n$$\nYour final answer must be a single closed-form expression for $R(\\tau,\\tau_{c})$ in terms of $\\tau$ and $\\tau_{c}$, containing no unevaluated integrals. No numerical evaluation is required, and no units are to be reported (Allan deviation is dimensionless).", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n-   Architecture 1 (Oscillator-based): The fractional frequency fluctuation is a zero-mean stationary white Gaussian process $y(t)$ with autocorrelation $\\mathbb{E}[y(t) y(t')] = N_{y} \\, \\delta(t - t')$, where $N_{y}  0$.\n-   Architecture 2 (Leaky Integrator): The fluctuation $y(t)$ is filtered to produce $y_{\\mathrm{LI}}(t)$ according to the differential equation $\\frac{d y_{\\mathrm{LI}}}{dt} = -\\frac{1}{\\tau_{c}} y_{\\mathrm{LI}} + \\frac{1}{\\tau_{c}} y(t)$, with time constant $\\tau_{c}  0$.\n-   Allan Variance Definition: The two-sample Allan variance for a generic fractional frequency process $z(t)$ is given by $\\sigma_{z}^{2}(\\tau) = \\frac{1}{2 \\tau^{2}} \\mathbb{E}[(\\int_{0}^{\\tau} z(t) \\, dt - \\int_{\\tau}^{2 \\tau} z(t) \\, dt)^{2}]$ for averaging time $\\tau  0$.\n-   Allan Deviations: $\\sigma_{y,\\mathrm{OSC}}(\\tau)$ is the Allan deviation for $y(t)$, and $\\sigma_{y,\\mathrm{LI}}(\\tau)$ is the Allan deviation for $y_{\\mathrm{LI}}(t)$.\n-   Objective: Derive expressions for $\\sigma_{y,\\mathrm{OSC}}(\\tau)$ and $\\sigma_{y,\\mathrm{LI}}(\\tau)$, and find the ratio $R(\\tau,\\tau_{c}) = \\frac{\\sigma_{y,\\mathrm{LI}}(\\tau)}{\\sigma_{y,\\mathrm{OSC}}(\\tau)}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It utilizes standard models from stochastic processes and linear systems theory (white noise, LTI filters, Allan variance) applied to a relevant problem in synthetic biology. The givens are self-contained, consistent, and sufficient for deriving a unique solution. The model is an idealization but does not violate fundamental principles. All terms are clearly defined.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be derived.\n\n**Derivation of $\\sigma_{y,\\mathrm{OSC}}(\\tau)$**\n\nThe Allan variance for the oscillator is based on the unfiltered noise $y(t)$. We denote it as $\\sigma_{y,\\mathrm{OSC}}^2(\\tau)$.\n$$\n\\sigma_{y,\\mathrm{OSC}}^{2}(\\tau) = \\frac{1}{2 \\tau^{2}} \\, \\mathbb{E}\\!\\left[\\left(\\int_{0}^{\\tau} y(t) \\, dt \\;-\\; \\int_{\\tau}^{2 \\tau} y(t) \\, dt\\right)^{2}\\right]\n$$\nLet the random variable inside the expectation be $X_{\\mathrm{OSC}} = \\int_{0}^{\\tau} y(t) \\, dt - \\int_{\\tau}^{2 \\tau} y(t') \\, dt'$. We need to compute $\\mathbb{E}[X_{\\mathrm{OSC}}^2]$.\n$$\n\\mathbb{E}[X_{\\mathrm{OSC}}^2] = \\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y(t) \\, dt\\right)^2\\right] - 2\\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y(t) \\, dt\\right)\\left(\\int_{\\tau}^{2 \\tau} y(t') \\, dt'\\right)\\right] + \\mathbb{E}\\left[\\left(\\int_{\\tau}^{2 \\tau} y(t') \\, dt'\\right)^2\\right]\n$$\nWe evaluate each term using the autocorrelation property $\\mathbb{E}[y(t) y(t')] = N_{y} \\delta(t - t')$.\nThe first term is:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y(t) \\, dt\\right)^2\\right] = \\mathbb{E}\\left[\\int_{0}^{\\tau} \\int_{0}^{\\tau} y(t) y(t') \\, dt \\, dt'\\right] = \\int_{0}^{\\tau} \\int_{0}^{\\tau} \\mathbb{E}[y(t) y(t')] \\, dt \\, dt'\n$$\n$$\n= \\int_{0}^{\\tau} \\int_{0}^{\\tau} N_{y} \\delta(t - t') \\, dt \\, dt' = N_y \\int_{0}^{\\tau} dt = N_y \\tau\n$$\nThe third term is evaluated over a time interval of the same duration $\\tau$, so by time-stationarity:\n$$\n\\mathbb{E}\\left[\\left(\\int_{\\tau}^{2 \\tau} y(t) \\, dt\\right)^2\\right] = \\int_{\\tau}^{2 \\tau} \\int_{\\tau}^{2 \\tau} N_{y} \\delta(t - t') \\, dt \\, dt' = N_y \\int_{\\tau}^{2 \\tau} dt = N_y \\tau\n$$\nThe middle (cross) term is:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y(t) \\, dt\\right)\\left(\\int_{\\tau}^{2 \\tau} y(t') \\, dt'\\right)\\right] = \\int_{0}^{\\tau} \\int_{\\tau}^{2 \\tau} \\mathbb{E}[y(t) y(t')] \\, dt \\, dt' = \\int_{0}^{\\tau} \\int_{\\tau}^{2 \\tau} N_{y} \\delta(t - t') \\, dt \\, dt'\n$$\nThe integration domain is the rectangle $[0, \\tau] \\times [\\tau, 2\\tau]$ in the $(t,t')$-plane. The line $t=t'$ only intersects this domain at the single point $(\\tau, \\tau)$. The value of a two-dimensional integral over a single point is zero. Therefore, the cross term is $0$.\nCombining the terms:\n$$\n\\mathbb{E}[X_{\\mathrm{OSC}}^2] = N_y \\tau - 0 + N_y \\tau = 2 N_y \\tau\n$$\nSubstituting this into the Allan variance formula:\n$$\n\\sigma_{y,\\mathrm{OSC}}^{2}(\\tau) = \\frac{1}{2 \\tau^{2}} (2 N_y \\tau) = \\frac{N_y}{\\tau}\n$$\nThe Allan deviation for the oscillator is:\n$$\n\\sigma_{y,\\mathrm{OSC}}(\\tau) = \\sqrt{\\frac{N_y}{\\tau}}\n$$\n\n**Derivation of $\\sigma_{y,\\mathrm{LI}}(\\tau)$**\n\nFirst, we determine the properties of the filtered noise $y_{\\mathrm{LI}}(t)$. The process $y_{\\mathrm{LI}}(t)$ is the output of an LTI filter with input $y(t)$. The filter's ODE is $\\tau_c \\dot{y}_{\\mathrm{LI}} + y_{\\mathrm{LI}} = y$. The impulse response $h(t)$ for this filter is $h(t) = \\frac{1}{\\tau_c} \\exp(-t/\\tau_c)$ for $t \\ge 0$ and $h(t)=0$ for $t0$.\nThe output $y_{\\mathrm{LI}}(t)$ is the convolution of the input $y(t)$ with $h(t)$. Since $y(t)$ is a stationary process, $y_{\\mathrm{LI}}(t)$ is also stationary. Its autocorrelation function, $R_{\\mathrm{LI}}(s) = \\mathbb{E}[y_{\\mathrm{LI}}(t) y_{\\mathrm{LI}}(t+s)]$, is given by the convolution of the input autocorrelation with the filter's autocorrelation, but for a white noise input with $R_y(s) = N_y \\delta(s)$, this simplifies.\nThe autocorrelation of the output of a filter with impulse response $h(t)$ to a white noise input with $\\mathbb{E}[y(t)y(t')]=N_y\\delta(t-t')$ is $R_{\\mathrm{LI}}(s) = N_y \\int_{-\\infty}^{\\infty} h(u)h(u+s)du$. For $s \\ge 0$:\n$$\nR_{\\mathrm{LI}}(s) = N_y \\int_{0}^{\\infty} \\left(\\frac{1}{\\tau_c} \\exp(-u/\\tau_c)\\right) \\left(\\frac{1}{\\tau_c} \\exp(-(u+s)/\\tau_c)\\right) du\n$$\n$$\nR_{\\mathrm{LI}}(s) = \\frac{N_y}{\\tau_c^2} \\exp(-s/\\tau_c) \\int_{0}^{\\infty} \\exp(-2u/\\tau_c) du = \\frac{N_y}{\\tau_c^2} \\exp(-s/\\tau_c) \\left[-\\frac{\\tau_c}{2} \\exp(-2u/\\tau_c)\\right]_{0}^{\\infty} = \\frac{N_y}{\\tau_c^2} \\exp(-s/\\tau_c) \\frac{\\tau_c}{2}\n$$\nSince $R_{\\mathrm{LI}}(s)$ must be an even function, we use $|s|$:\n$$\nR_{\\mathrm{LI}}(s) = \\frac{N_y}{2\\tau_c} \\exp(-|s|/\\tau_c)\n$$\nNow we compute the Allan variance $\\sigma_{y,\\mathrm{LI}}^2(\\tau)$. Let $X_{\\mathrm{LI}} = \\int_{0}^{\\tau} y_{\\mathrm{LI}}(t) \\, dt - \\int_{\\tau}^{2 \\tau} y_{\\mathrm{LI}}(t) \\, dt$. We need $\\mathbb{E}[X_{\\mathrm{LI}}^2]$.\n$$\n\\mathbb{E}[X_{\\mathrm{LI}}^2] = \\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y_{\\mathrm{LI}}(t) dt\\right)^2\\right] - 2\\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y_{\\mathrm{LI}}(t) dt\\right)\\left(\\int_{\\tau}^{2 \\tau} y_{\\mathrm{LI}}(t') dt'\\right)\\right] + \\mathbb{E}\\left[\\left(\\int_{\\tau}^{2 \\tau} y_{\\mathrm{LI}}(t') dt'\\right)^2\\right]\n$$\nLet's evaluate the first term:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y_{\\mathrm{LI}}(t) dt\\right)^2\\right] = \\int_{0}^{\\tau} \\int_{0}^{\\tau} R_{\\mathrm{LI}}(t-t') dt dt' = \\int_{0}^{\\tau} \\int_{0}^{\\tau} \\frac{N_y}{2\\tau_c} \\exp(-|t-t'|/\\tau_c) dt dt'\n$$\nExploiting symmetry, this is $2 \\int_{0}^{\\tau} dt \\int_{0}^{t} dt' \\frac{N_y}{2\\tau_c} \\exp(-(t-t')/\\tau_c)$:\n$$\n= \\frac{N_y}{\\tau_c} \\int_{0}^{\\tau} \\exp(-t/\\tau_c) \\left[\\tau_c \\exp(t'/\\tau_c)\\right]_{0}^{t} dt = \\frac{N_y}{\\tau_c} \\int_{0}^{\\tau} \\exp(-t/\\tau_c) (\\tau_c \\exp(t/\\tau_c) - \\tau_c) dt\n$$\n$$\n= N_y \\int_{0}^{\\tau} (1 - \\exp(-t/\\tau_c)) dt = N_y \\left[t + \\tau_c \\exp(-t/\\tau_c)\\right]_{0}^{\\tau} = N_y (\\tau + \\tau_c \\exp(-\\tau/\\tau_c) - \\tau_c)\n$$\nDue to stationarity, the third term is identical.\nNow the cross term. For $t \\in [0, \\tau]$ and $t' \\in [\\tau, 2\\tau]$, we have $t't$ so $|t-t'|=t'-t$.\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{\\tau} y_{\\mathrm{LI}}(t) dt\\right)\\left(\\int_{\\tau}^{2 \\tau} y_{\\mathrm{LI}}(t') dt'\\right)\\right] = \\int_{0}^{\\tau} dt \\int_{\\tau}^{2 \\tau} dt' \\frac{N_y}{2\\tau_c} \\exp(-(t'-t)/\\tau_c)\n$$\n$$\n= \\frac{N_y}{2\\tau_c} \\left(\\int_{0}^{\\tau} \\exp(t/\\tau_c) dt\\right)\\left(\\int_{\\tau}^{2 \\tau} \\exp(-t'/\\tau_c) dt'\\right)\n$$\n$$\n= \\frac{N_y}{2\\tau_c} \\left[\\tau_c \\exp(t/\\tau_c)\\right]_{0}^{\\tau} \\left[-\\tau_c \\exp(-t'/\\tau_c)\\right]_{\\tau}^{2 \\tau}\n$$\n$$\n= \\frac{N_y}{2\\tau_c} \\left(\\tau_c (\\exp(\\tau/\\tau_c) - 1)\\right) \\left(-\\tau_c (\\exp(-2\\tau/\\tau_c) - \\exp(-\\tau/\\tau_c))\\right)\n$$\n$$\n= \\frac{N_y \\tau_c}{2} (\\exp(\\tau/\\tau_c) - 1) (\\exp(-\\tau/\\tau_c) - \\exp(-2\\tau/\\tau_c)) = \\frac{N_y \\tau_c}{2} (1 - \\exp(-\\tau/\\tau_c) - \\exp(-\\tau/\\tau_c) + \\exp(-2\\tau/\\tau_c))\n$$\n$$\n= \\frac{N_y \\tau_c}{2} (1 - \\exp(-\\tau/\\tau_c))^2\n$$\nCombining all parts for $\\mathbb{E}[X_{\\mathrm{LI}}^2]$:\n$$\n\\mathbb{E}[X_{\\mathrm{LI}}^2] = 2 N_y (\\tau - \\tau_c(1 - \\exp(-\\tau/\\tau_c))) - 2 \\frac{N_y \\tau_c}{2} (1 - \\exp(-\\tau/\\tau_c))^2\n$$\n$$\n= 2N_y\\tau - 2N_y\\tau_c + 2N_y\\tau_c\\exp(-\\tau/\\tau_c) - N_y\\tau_c(1 - 2\\exp(-\\tau/\\tau_c) + \\exp(-2\\tau/\\tau_c))\n$$\n$$\n= 2N_y\\tau - 3N_y\\tau_c + 4N_y\\tau_c\\exp(-\\tau/\\tau_c) - N_y\\tau_c\\exp(-2\\tau/\\tau_c)\n$$\n$$\n= N_y \\left( 2\\tau - 3\\tau_c + 4\\tau_c \\exp(-\\tau/\\tau_c) - \\tau_c \\exp(-2\\tau/\\tau_c) \\right)\n$$\nThe Allan variance for the leaky integrator is:\n$$\n\\sigma_{y,\\mathrm{LI}}^{2}(\\tau) = \\frac{1}{2 \\tau^{2}} \\mathbb{E}[X_{\\mathrm{LI}}^2] = \\frac{N_y}{2 \\tau^{2}} \\left( 2\\tau - 3\\tau_c + 4\\tau_c \\exp(-\\tau/\\tau_c) - \\tau_c \\exp(-2\\tau/\\tau_c) \\right)\n$$\nThe Allan deviation is the square root of this expression.\n\n**Derivation of the ratio $R(\\tau,\\tau_{c})$**\n\nThe ratio of the Allan deviations is $R(\\tau,\\tau_{c}) = \\frac{\\sigma_{y,\\mathrm{LI}}(\\tau)}{\\sigma_{y,\\mathrm{OSC}}(\\tau)} = \\sqrt{\\frac{\\sigma_{y,\\mathrm{LI}}^2(\\tau)}{\\sigma_{y,\\mathrm{OSC}}^2(\\tau)}}$.\n$$\n\\frac{\\sigma_{y,\\mathrm{LI}}^2(\\tau)}{\\sigma_{y,\\mathrm{OSC}}^2(\\tau)} = \\frac{\\frac{N_y}{2 \\tau^{2}} \\left( 2\\tau - 3\\tau_c + 4\\tau_c \\exp(-\\tau/\\tau_c) - \\tau_c \\exp(-2\\tau/\\tau_c) \\right)}{\\frac{N_y}{\\tau}}\n$$\n$$\n= \\frac{\\tau}{N_y} \\frac{N_y}{2 \\tau^{2}} \\left( 2\\tau - 3\\tau_c + 4\\tau_c \\exp(-\\tau/\\tau_c) - \\tau_c \\exp(-2\\tau/\\tau_c) \\right)\n$$\n$$\n= \\frac{1}{2 \\tau} \\left( 2\\tau - 3\\tau_c + 4\\tau_c \\exp(-\\frac{\\tau}{\\tau_c}) - \\frac{\\tau_c}{2\\tau} \\exp(-\\frac{2\\tau}{\\tau_c}) \\right)\n$$\n$$\n= 1 - \\frac{3\\tau_c}{2\\tau} + \\frac{2\\tau_c}{\\tau} \\exp(-\\frac{\\tau}{\\tau_c}) - \\frac{\\tau_c}{2\\tau} \\exp(-\\frac{2\\tau}{\\tau_c})\n$$\nTaking the square root gives the final ratio $R(\\tau,\\tau_{c})$.", "answer": "$$\n\\boxed{\\sqrt{1 - \\frac{3\\tau_c}{2\\tau} + \\frac{2\\tau_c}{\\tau} \\exp(-\\frac{\\tau}{\\tau_c}) - \\frac{\\tau_c}{2\\tau} \\exp(-\\frac{2\\tau}{\\tau_c})}}\n$$", "id": "2777922"}, {"introduction": "Building predictive models of synthetic circuits often involves grappling with numerous parameters that are difficult to measure precisely. This computational exercise delves into the concept of parameter 'sloppiness,' a common feature of systems biology models where predictive power is controlled by a few 'stiff' parameter combinations while being insensitive to others [@problem_id:2777825]. You will implement a numerical analysis based on the Fisher Information Matrix to perform an eigen-decomposition of parameter sensitivities. This practice provides hands-on experience with a state-of-the-art technique for identifying which aspects of a model are well-constrained by data, guiding both experimental design and model reduction.", "problem": "You are given a minimal mechanistic timer model for a synthetic gene expression cascade consisting of messenger RNA and protein. The deterministic dynamics follow first-order ordinary differential equations (ordinary differential equation (ODE)) driven by constant transcription beginning at time $t = 0$:\n- Messenger RNA concentration $M(t)$ evolves according to $dM/dt = \\alpha - \\delta_m M$, with initial condition $M(0) = 0$.\n- Protein concentration $P(t)$ evolves according to $dP/dt = \\beta M - \\delta_p P$, with initial condition $P(0) = 0$.\n\nAssume the observable output is $y(t) = P(t)$ sampled at known time points. Under an independent Gaussian noise model with known standard deviation $\\sigma$, the log-likelihood of parameters $\\theta = (\\alpha, \\beta, \\delta_m, \\delta_p)$ given data $\\{(t_i, y_i)\\}_{i=1}^n$ is\n$$\n\\ell(\\theta) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\big(y_i - P(t_i;\\theta)\\big)^2.\n$$\n\nFor this problem, the data are generated noiselessly from the model at a specified ground-truth parameter vector $\\theta^\\star$, that is, $y_i = P(t_i;\\theta^\\star)$ for all $i$. You are to evaluate parameter sloppiness by computing the Gauss–Newton approximation to the Hessian of the negative log-likelihood at $\\theta^\\star$,\n$$\nH(\\theta^\\star) \\approx \\frac{1}{\\sigma^2} J(\\theta^\\star)^\\top J(\\theta^\\star),\n$$\nwhere $J_{ik}(\\theta) = \\frac{\\partial P(t_i;\\theta)}{\\partial \\theta_k}$ is the sensitivity (Jacobian) matrix of the model predictions with respect to the parameters. Then perform eigenanalysis of $H(\\theta^\\star)$ to quantify stiffness and sloppiness.\n\nModel solution. The closed-form solution for $P(t)$ is required for numerical stability. For $\\delta_m \\neq \\delta_p$,\n$$\nP(t) = \\frac{\\alpha\\beta}{\\delta_m}\\left(\\frac{1 - e^{-\\delta_p t}}{\\delta_p} - \\frac{e^{-\\delta_p t} - e^{-\\delta_m t}}{\\delta_m - \\delta_p}\\right).\n$$\nFor the special case $\\delta_m = \\delta_p = \\delta$,\n$$\nP(t) = \\frac{\\alpha\\beta}{\\delta}\\left(\\frac{1 - e^{-\\delta t}}{\\delta} - t e^{-\\delta t}\\right).\n$$\n\nNumerical specification. Use central finite differences to approximate the Jacobian entries at $\\theta^\\star$:\n$$\n\\frac{\\partial P(t_i;\\theta)}{\\partial \\theta_k}\\bigg|_{\\theta=\\theta^\\star} \\approx \\frac{P\\big(t_i; \\theta^\\star + h_k e_k\\big) - P\\big(t_i; \\theta^\\star - h_k e_k\\big)}{2 h_k},\n$$\nwhere $e_k$ is the $k$-th standard basis vector, and $h_k = \\varepsilon \\max(1, |\\theta^\\star_k|)$ with $\\varepsilon = 10^{-6}$. Construct $H(\\theta^\\star)$ from $J(\\theta^\\star)$ as above. Compute its eigenvalues $\\{\\lambda_j\\}_{j=1}^4$ (there are $4$ parameters), sort them in descending order, and define the condition number\n$$\n\\kappa = \\frac{\\max_j \\lambda_j}{\\max\\{\\min_j \\lambda_j, \\lambda_{\\text{floor}}\\}},\n$$\nwith $\\lambda_{\\text{floor}} = 10^{-12}$ to avoid division by zero. Define a relative stiffness threshold $\\tau = 10^{-3}$ and classify an eigen-direction as stiff if its eigenvalue satisfies $\\lambda_j \\ge \\tau \\cdot \\max_k \\lambda_k$, otherwise classify it as sloppy. Count the number of stiff directions.\n\nYour task. For each test case below:\n- Build $J(\\theta^\\star)$ at the specified $\\theta^\\star$ and time grid using the central-difference rule above with $\\varepsilon = 10^{-6}$.\n- Set $\\sigma = 1$.\n- Compute the approximate Hessian $H(\\theta^\\star)$, its eigenvalues, the base-$10$ logarithm of the condition number $\\log_{10}\\kappa$, and the integer count of stiff directions under the threshold $\\tau = 10^{-3}$.\n- Report for each test case a pair of values: the float $\\log_{10}\\kappa$ rounded to three decimals, followed by the integer count of stiff directions.\n\nTest suite. Use the following three deterministic test cases. All times are in minutes, but your outputs are dimensionless; no physical units are required in the output.\n- Case $1$ (well-sampled, distinct rates): $\\theta^\\star = (\\alpha, \\beta, \\delta_m, \\delta_p) = (100, 1.5, 0.2, 0.05)$; time grid $t_i$ evenly spaced from $t = 0$ to $t = 240$ inclusive in steps of $2$, giving $121$ points; $\\sigma = 1$.\n- Case $2$ (nearly equal decay rates): $\\theta^\\star = (100, 1.5, 0.1, 0.100001)$; time grid $t_i$ from $t = 0$ to $t = 240$ in steps of $2$; $\\sigma = 1$.\n- Case $3$ (short window, slow dynamics): $\\theta^\\star = (100, 1.5, 0.02, 0.01)$; time grid $t_i$ from $t = 0$ to $t = 20$ in steps of $0.5$, giving $41$ points; $\\sigma = 1$.\n\nAlgorithmic constraints and numerical tolerances.\n- Use the closed-form $P(t)$ above with a branch that handles $\\delta_m$ close to $\\delta_p$ by switching to the equal-rate formula whenever $|\\delta_m - \\delta_p| \\le 10^{-12}$.\n- Use the central-difference steps $h_k = \\varepsilon \\max(1, |\\theta^\\star_k|)$ with $\\varepsilon = 10^{-6}$ for all $k \\in \\{1,2,3,4\\}$.\n- Clamp numerically tiny negative eigenvalues of $H(\\theta^\\star)$ to $0$ before computing $\\kappa$ and stiffness counts: replace $\\lambda_j$ by $\\max(\\lambda_j, 0)$.\n- Use $\\lambda_{\\text{floor}} = 10^{-12}$ and $\\tau = 10^{-3}$ as specified.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of the test cases. For each test case, output the two values flattened in sequence: first the rounded $\\log_{10}\\kappa$ (three decimals), then the integer count of stiff directions. For example, the output format should be exactly like\n\"[x1,s1,x2,s2,x3,s3]\"\nwhere $xk$ are floats with three decimals and $sk$ are integers for cases $k = 1,2,3$ respectively.", "solution": "The problem presented is a well-posed exercise in numerical parameter sensitivity analysis, a fundamental concept in systems biology and chemical kinetics. We are tasked with evaluating the \"sloppiness\" of a simple two-stage gene expression model. The problem is valid, scientifically grounded, and provides all necessary information for its solution. We shall proceed with the analysis.\n\nThe model describes the concentration of messenger RNA, $M(t)$, and protein, $P(t)$, governed by the following system of linear, first-order ordinary differential equations with constant coefficients:\n$$\n\\frac{dM}{dt} = \\alpha - \\delta_m M(t), \\quad M(0) = 0\n$$\n$$\n\\frac{dP}{dt} = \\beta M(t) - \\delta_p P(t), \\quad P(0) = 0\n$$\nThe parameter vector is $\\theta = (\\alpha, \\beta, \\delta_m, \\delta_p)$, representing the transcription rate, translation rate, mRNA degradation rate, and protein degradation rate, respectively.\n\nOur first task is to obtain the analytical solution for the protein concentration $P(t)$, as numerical integration would be both inefficient and prone to accumulating error. The equation for $M(t)$ is a standard first-order linear ODE, which solves to:\n$$\nM(t) = \\frac{\\alpha}{\\delta_m}(1 - e^{-\\delta_m t})\n$$\nSubstituting this into the equation for $P(t)$ gives:\n$$\n\\frac{dP}{dt} + \\delta_p P(t) = \\beta \\frac{\\alpha}{\\delta_m}(1 - e^{-\\delta_m t})\n$$\nThis is another linear first-order ODE. Solving it using an integrating factor $e^{\\delta_p t}$ yields the solution for $P(t)$. The form of the solution depends on whether the degradation rates are distinct or equal.\n\nCase 1: $\\delta_m \\neq \\delta_p$. The solution is provided as:\n$$\nP(t; \\theta) = \\frac{\\alpha\\beta}{\\delta_m}\\left(\\frac{1 - e^{-\\delta_p t}}{\\delta_p} - \\frac{e^{-\\delta_p t} - e^{-\\delta_m t}}{\\delta_m - \\delta_p}\\right)\n$$\nCase 2: $\\delta_m = \\delta_p = \\delta$. This is a resonant case. The solution is:\n$$\nP(t; \\theta) = \\frac{\\alpha\\beta}{\\delta}\\left(\\frac{1 - e^{-\\delta t}}{\\delta} - t e^{-\\delta t}\\right)\n$$\nFor numerical stability, we must use the second formula when $|\\delta_m - \\delta_p|$ is very small. The problem specifies a threshold of $10^{-12}$ for this switch.\n\nThe core of the analysis is to compute the Gauss-Newton approximation of the Hessian of the negative log-likelihood. Given noiseless data, the negative log-likelihood, up to a constant, is proportional to the sum of squared errors. Its Hessian at the true parameters $\\theta^\\star$ is approximated by Fisher Information Matrix, which for a Gaussian noise model is:\n$$\nH(\\theta^\\star) \\approx \\frac{1}{\\sigma^2} J(\\theta^\\star)^\\top J(\\theta^\\star)\n$$\nHere, $J(\\theta^\\star)$ is the Jacobian or sensitivity matrix, where each entry $J_{ik}$ is the partial derivative of the model prediction at time $t_i$ with respect to the parameter $\\theta_k$:\n$$\nJ_{ik}(\\theta^\\star) = \\frac{\\partial P(t_i; \\theta)}{\\partial \\theta_k}\\bigg|_{\\theta=\\theta^\\star}\n$$\nThe dimensions of $J$ are $n \\times p$, where $n$ is the number of time points and $p=4$ is the number of parameters.\n\nWe will not find the analytical derivatives, as they are complex and prone to implementation error. Instead, we use the specified numerical approximation via central finite differences:\n$$\nJ_{ik}(\\theta^\\star) \\approx \\frac{P\\big(t_i; \\theta^\\star + h_k e_k\\big) - P\\big(t_i; \\theta^\\star - h_k e_k\\big)}{2 h_k}\n$$\nwhere $e_k$ is the $k$-th standard basis vector and $h_k$ is a small perturbation for the $k$-th parameter, defined as $h_k = \\varepsilon \\max(1, |\\theta^\\star_k|)$ with $\\varepsilon = 10^{-6}$.\n\nThe computational procedure for each test case is as follows:\n1.  Define the parameter vector $\\theta^\\star = (\\alpha, \\beta, \\delta_m, \\delta_p)$ and generate the time grid $\\{t_i\\}$.\n2.  Construct the $n \\times 4$ Jacobian matrix $J$. For each parameter $\\theta_k, k \\in \\{1, 2, 3, 4\\}$:\n    a. Calculate the step size $h_k = 10^{-6} \\max(1, |\\theta^\\star_k|)$.\n    b. Define the perturbed parameter vectors $\\theta^+ = \\theta^\\star + h_k e_k$ and $\\theta^- = \\theta^\\star - h_k e_k$.\n    c. Evaluate $P(t_i; \\theta^+)$ and $P(t_i; \\theta^-)$ for all $t_i$ in the grid.\n    d. Compute the $k$-th column of $J$ using the central difference formula.\n3.  Given $\\sigma = 1$, compute the $4 \\times 4$ Hessian approximation $H = J^\\top J$.\n4.  Compute the eigenvalues of the symmetric matrix $H$, yielding $\\{\\lambda_j\\}_{j=1}^4$.\n5.  Due to numerical precision limits, some very small eigenvalues might be slightly negative. Clamp these to zero: $\\lambda'_j = \\max(\\lambda_j, 0)$.\n6.  Sort the eigenvalues in descending order: $\\lambda'_1 \\ge \\lambda'_2 \\ge \\lambda'_3 \\ge \\lambda'_4$.\n7.  Calculate the condition number $\\kappa = \\frac{\\lambda'_1}{\\max(\\lambda'_4, \\lambda_{\\text{floor}})}$, where the floor is given as $\\lambda_{\\text{floor}} = 10^{-12}$. Then compute $\\log_{10}\\kappa$.\n8.  Determine the number of \"stiff\" eigendirections. An eigenvalue $\\lambda'_j$ corresponds to a stiff direction if it is not drastically smaller than the largest eigenvalue. The criterion is $\\lambda'_j \\ge \\tau \\cdot \\lambda'_1$, with a relative threshold of $\\tau = 10^{-3}$. We count how many eigenvalues satisfy this. The largest eigenvalue always satisfies this, so the count is at least $1$.\n9.  Collect the required outputs: the rounded value of $\\log_{10}\\kappa$ and the integer count of stiff directions.\n\nThis procedure will be applied to the three specified test cases, and the results will be formatted as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    # Define a function for the protein concentration P(t)\n    def protein_concentration(t, theta):\n        \"\"\"\n        Calculates the protein concentration P(t) given parameters theta.\n        theta = (alpha, beta, delta_m, delta_p)\n        \"\"\"\n        alpha, beta, delta_m, delta_p = theta\n        \n        # Handle cases where degradation rates are zero or negative, which are non-physical\n        # and could cause division by zero.\n        if delta_m = 0 or delta_p = 0:\n            # Return an array of zeros if t is an array, else a single zero.\n            return np.zeros_like(t) if isinstance(t, np.ndarray) else 0.0\n\n        # Check for numerical equality of degradation rates\n        if np.abs(delta_m - delta_p) = 1e-12:\n            # Use the formula for delta_m = delta_p = delta\n            delta = delta_m\n            term1 = (1 - np.exp(-delta * t)) / delta\n            term2 = t * np.exp(-delta * t)\n            P = (alpha * beta / delta) * (term1 - term2)\n        else:\n            # Use the formula for delta_m != delta_p\n            term1 = (1 - np.exp(-delta_p * t)) / delta_p\n            term2 = (np.exp(-delta_p * t) - np.exp(-delta_m * t)) / (delta_m - delta_p)\n            P = (alpha * beta / delta_m) * (term1 - term2)\n        \n        return P\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: well-sampled, distinct rates\n        {'theta_star': (100, 1.5, 0.2, 0.05), 't_start': 0, 't_stop': 240, 't_step': 2},\n        # Case 2: nearly equal decay rates\n        {'theta_star': (100, 1.5, 0.1, 0.100001), 't_start': 0, 't_stop': 240, 't_step': 2},\n        # Case 3: short window, slow dynamics\n        {'theta_star': (100, 1.5, 0.02, 0.01), 't_start': 0, 't_stop': 20, 't_step': 0.5},\n    ]\n\n    # Constants and hyperparameters\n    epsilon = 1e-6\n    sigma = 1.0\n    lambda_floor = 1e-12\n    stiffness_threshold = 1e-3\n    num_params = 4\n\n    results = []\n\n    for case in test_cases:\n        theta_star = np.array(case['theta_star'])\n        t_grid = np.arange(case['t_start'], case['t_stop'] + case['t_step']/2, case['t_step'])\n        \n        n_times = len(t_grid)\n        jacobian = np.zeros((n_times, num_params))\n\n        # Compute Jacobian matrix using central finite differences\n        for k in range(num_params):\n            h_k = epsilon * np.max([1.0, np.abs(theta_star[k])])\n            \n            theta_plus = np.copy(theta_star)\n            theta_plus[k] += h_k\n            \n            theta_minus = np.copy(theta_star)\n            theta_minus[k] -= h_k\n            \n            p_plus = protein_concentration(t_grid, theta_plus)\n            p_minus = protein_concentration(t_grid, theta_minus)\n            \n            jacobian[:, k] = (p_plus - p_minus) / (2 * h_k)\n            \n        # Compute Gauss-Newton Hessian approximation\n        hessian = (1 / sigma**2) * (jacobian.T @ jacobian)\n        \n        # Compute eigenvalues. Use eigvalsh for symmetric matrices.\n        eigenvalues = np.linalg.eigvalsh(hessian)\n        \n        # Clamp numerically tiny negative eigenvalues to 0\n        eigenvalues = np.maximum(eigenvalues, 0)\n        \n        # Sort eigenvalues in descending order\n        eigenvalues = np.sort(eigenvalues)[::-1]\n        \n        # Calculate condition number and its log10\n        lambda_max = eigenvalues[0]\n        lambda_min = eigenvalues[-1]\n\n        # Handle case where max eigenvalue is zero or negative (should not happen with clamping)\n        if lambda_max = 0:\n            log10_kappa = -np.inf # effectively log10(0) or undefined\n        else:\n            kappa = lambda_max / np.max([lambda_min, lambda_floor])\n            log10_kappa = np.log10(kappa)\n\n        # Count stiff directions\n        if lambda_max  0:\n            stiff_count = np.sum(eigenvalues = stiffness_threshold * lambda_max)\n        else:\n            stiff_count = 0 # If all eigenvalues are 0\n        \n        results.append(f\"{log10_kappa:.3f}\")\n        results.append(str(stiff_count))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2777825"}]}