{"hands_on_practices": [{"introduction": "At the heart of any quantitative model for lineage tracing is a mathematical description of the mutation or editing process itself. We often model these discrete molecular events, which occur randomly over time, using the framework of a Continuous-Time Markov Chain (CTMC). This exercise [@problem_id:2752006] provides foundational practice in deriving the transition probabilities for an irreversible two-state process, a skill essential for building or interpreting nearly any model of molecular evolution or event recording.", "problem": "In a deoxyribonucleic acid (DNA) molecular event recorder used for cellular lineage tracing, each target site is initially unedited and can undergo an irreversible edit when an adenine base editor is active. Empirical evidence indicates that edits at a single site occur as independent Poisson events with a constant hazard rate. Consider a single genomic site whose state space is $\\{\\text{unedited}, \\text{edited}\\}$. Assume that once in the $\\text{edited}$ state, the site remains there indefinitely (no reversion), and that the instantaneous rate of transition from $\\text{unedited}$ to $\\text{edited}$ is a constant $\\mu$ with units of $\\text{time}^{-1}$. Let $t$ denote elapsed time in the same time units as $\\mu$.\n\nStarting only from the following foundational principles:\n- The waiting time for a Poisson event with constant hazard $\\mu$ is exponentially distributed with survival function $\\exp(-\\mu t)$.\n- A time-homogeneous continuous-time Markov chain (CTMC) satisfies the Markov property and the Kolmogorov equations with a generator matrix whose off-diagonal entries are nonnegative rates and whose rows sum to zero.\n\nModel this per-site editing as a two-state CTMC and derive, from first principles, the $2 \\times 2$ transition probability matrix $P(t)$ at time $t$, in the state order $\\{\\text{unedited}, \\text{edited}\\}$. Your final answer must be a single closed-form analytical expression in terms of $\\mu$ and $t$. Since the entries of $P(t)$ are probabilities, no physical units are required in the final answer. Do not round your answer.", "solution": "We are asked to derive the transition probability matrix $P(t)$ for a two-state time-homogeneous continuous-time Markov chain (CTMC) representing an irreversible editing process at a single molecular recorder site. The states are $\\{\\text{unedited}, \\text{edited}\\}$, which we will index in that order for rows and columns of $P(t)$.\n\nBy the problem statement, the process is a Poisson event process with constant hazard $\\mu$ for the transition from $\\text{unedited}$ to $\\text{edited}$, and no reverse transition. In CTMC language, the generator matrix $Q$ must have nonnegative off-diagonal rates and rows summing to zero. Therefore, with state order $\\{\\text{unedited}, \\text{edited}\\}$, we have\n$$\nQ \\;=\\; \\begin{pmatrix}\n-\\mu  \\mu \\\\\n0  0\n\\end{pmatrix}.\n$$\nThis reflects that the exit rate from $\\text{unedited}$ is $\\mu$, all of which goes to $\\text{edited}$, and $\\text{edited}$ is an absorbing state.\n\nThere are two complementary routes to obtain $P(t)$ from first principles: (i) use the exponential waiting-time characterization of the Poisson process to write and solve the Kolmogorov forward equations for the relevant transition probabilities, or (ii) use the matrix exponential $P(t) = \\exp(Q t)$ justified by the Kolmogorov equations for time-homogeneous CTMCs. We proceed with route (i) to remain close to the biochemical interpretation, and then verify consistency with route (ii).\n\nLet $p_{ij}(t)$ denote the transition probability from initial state $i$ to state $j$ at time $t$, with $i,j \\in \\{\\text{unedited}, \\text{edited}\\}$. We seek the $2 \\times 2$ matrix $P(t)$ with entries $p_{ij}(t)$.\n\nFrom the exponential survival function of the waiting time for the first edit event with rate $\\mu$, the probability that a site remains unedited up to time $t$ starting from unedited is\n$$\np_{\\text{unedited},\\text{unedited}}(t) \\;=\\; \\exp(-\\mu t).\n$$\nBecause there is no reversion, if the process starts in the edited state, it stays edited forever, so\n$$\np_{\\text{edited},\\text{edited}}(t) \\;=\\; 1,\n\\qquad\np_{\\text{edited},\\text{unedited}}(t) \\;=\\; 0.\n$$\nThe remaining entry $p_{\\text{unedited},\\text{edited}}(t)$ can be obtained by conservation of probability in the first row, since the only two possibilities at time $t$ starting from unedited are to be unedited or edited:\n$$\np_{\\text{unedited},\\text{edited}}(t) \\;=\\; 1 - p_{\\text{unedited},\\text{unedited}}(t) \\;=\\; 1 - \\exp(-\\mu t).\n$$\n\nCollecting these entries in the prescribed state order $\\{\\text{unedited}, \\text{edited}\\}$ for rows and columns, we obtain\n$$\nP(t) \\;=\\; \\begin{pmatrix}\np_{\\text{unedited},\\text{unedited}}(t)  p_{\\text{unedited},\\text{edited}}(t) \\\\\np_{\\text{edited},\\text{unedited}}(t)  p_{\\text{edited},\\text{edited}}(t)\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\exp(-\\mu t)  1 - \\exp(-\\mu t) \\\\\n0  1\n\\end{pmatrix}.\n$$\n\nFor completeness, we verify that this solution satisfies the Kolmogorov forward equation $ \\frac{d}{dt} P(t) = P(t) Q$, with $P(0)=I$, where $I$ is the identity matrix. First, note $P(0) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$ as required. Next, compute the time derivative:\n$$\n\\frac{d}{dt} P(t) \\;=\\; \\begin{pmatrix}\n-\\mu \\exp(-\\mu t)  \\mu \\exp(-\\mu t) \\\\\n0  0\n\\end{pmatrix}.\n$$\nCompute $P(t) Q$:\n$$\nP(t) Q \\;=\\; \n\\begin{pmatrix}\n\\exp(-\\mu t)  1 - \\exp(-\\mu t) \\\\\n0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n-\\mu  \\mu \\\\\n0  0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-\\mu \\exp(-\\mu t)  \\mu \\exp(-\\mu t) \\\\\n0  0\n\\end{pmatrix}.\n$$\nThus $\\frac{d}{dt} P(t) = P(t) Q$, confirming consistency with the CTMC framework. By uniqueness of solutions to this linear system with initial condition $P(0)=I$, the derived $P(t)$ is the transition probability matrix.\n\nTherefore, the transition matrix over time $t$ with rate $\\mu$ is\n$$\n\\begin{pmatrix}\n\\exp(-\\mu t)  1 - \\exp(-\\mu t) \\\\\n0  1\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\exp(-\\mu t)  1-\\exp(-\\mu t) \\\\ 0  1\\end{pmatrix}}$$", "id": "2752006"}, {"introduction": "A robust model of the mutation process is only useful if we can connect it to experimental data. The principled framework of maximum likelihood estimation (MLE) allows us to determine the model parameters that best explain our observations. This practice [@problem_id:2752064] builds directly on the CTMC model by demonstrating how to calculate the likelihood of observing a particular pattern of edited states on a given lineage tree, and then using this to find the most likely mutation rate.", "problem": "A laboratory uses a Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-based molecular event recorder to trace cellular lineages. The recorder consists of $L$ independent genomic barcode sites that are initially unedited at the root cell at time $t=0$. Each site is modeled as a two-state continuous-time process with states $\\{0,1\\}$, where $0$ denotes unedited and $1$ denotes irreversibly edited. Along any lineage branch of duration $t$, the transition $0 \\to 1$ occurs according to a Poisson process with constant rate $\\mu$ per site per unit time, and back-mutation $1 \\to 0$ is impossible. The cell lineage forms a rooted tree topology $\\mathcal{T}$ with known branch durations. All sampled leaves are observed at the same final time. The observed data are the leaf states (edited or unedited) for each site.\n\nStarting only from the definitions of a Poisson process and a continuous-time Markov chain, and assuming site independence and identical rates across sites:\n\n- Derive the single-site likelihood of the observed leaf states given the topology $\\mathcal{T}$ and rate $\\mu$, by summing over unobserved ancestral states on internal nodes. Then write the full likelihood for $L$ independent sites.\n\n- Now specialize to the case where $\\mathcal{T}$ is a rooted star topology with $m$ leaves, each branch having the same duration $t$. In this case, for each site, leaves evolve independently from the known root state $0$. Suppose $m=4$, $t=20$ h, and $L=1000$. Across all $mL$ leaf-site pairs, you observed a total of $K=1230$ edited states ($1$) and $mL-K$ unedited states ($0$). Compute the maximum likelihood estimate $\\hat{\\mu}$ for $\\mu$ for this star topology dataset. Express the final rate in h$^{-1}$, and round your answer to four significant figures.", "solution": "The problem is divided into two parts. First, the derivation of the general likelihood function for a molecular recorder on a tree. Second, the calculation of the maximum likelihood estimate for the recording rate $\\mu$ in a specific experimental scenario.\n\nPart 1: Derivation of the Likelihood Function\n\nThe state of a single barcode site is modeled as a two-state, continuous-time Markov chain with states $\\{0, 1\\}$, representing unedited and edited, respectively. The transition from state $0$ to state $1$ occurs with a constant rate $\\mu$. The reverse transition $1 \\to 0$ is forbidden, making state $1$ an absorbing state. The rate matrix, or generator, $Q$ for this process is:\n$$\nQ = \\begin{pmatrix} -\\mu  \\mu \\\\ 0  0 \\end{pmatrix}\n$$\nThe probability transition matrix $P(t)$ over a time interval $t$ is given by the matrix exponential $P(t) = \\exp(Qt)$. The elements $P_{ij}(t)$ give the probability of being in state $j$ at time $t$ given the state was $i$ at time $0$. The solution to the master equation $\\frac{d P(t)}{dt} = P(t)Q$ gives these probabilities. Specifically, for the probability of remaining in state $0$, $P_{00}(t)$:\n$$\n\\frac{d P_{00}(t)}{dt} = -\\mu P_{00}(t)\n$$\nWith the initial condition $P_{00}(0) = 1$, the solution is:\n$$\nP_{00}(t) = \\exp(-\\mu t)\n$$\nThis is the probability of zero editing events occurring in a time interval $t$, consistent with a Poisson process. Since there are only two states, the probability of transitioning from $0$ to $1$ is:\n$$\nP_{01}(t) = 1 - P_{00}(t) = 1 - \\exp(-\\mu t)\n$$\nBecause state $1$ is absorbing, we have:\n$$\nP_{11}(t) = 1\n$$\n$$\nP_{10}(t) = 0\n$$\nNow, consider a single site on a given rooted tree topology $\\mathcal{T}$ with known branch durations. The root cell at time $t=0$ has the site in state $0$. Let the set of leaf nodes be $\\mathcal{L}$ and the set of internal nodes be $\\mathcal{I}$ (including the root, $r$). The observed data for this site is the set of states $D = \\{s_v | v \\in \\mathcal{L}\\}$, where $s_v \\in \\{0, 1\\}$.\n\nThe likelihood of observing the data $D$ given the rate $\\mu$ is the probability $P(D | \\mu, \\mathcal{T})$. This is found by summing over all possible state assignments to the unobserved internal nodes (excluding the root, whose state is fixed at $0$).\nLet $s_{\\mathcal{I} \\setminus \\{r\\}}$ be a specific assignment of states to the internal nodes other than the root. The likelihood for a single site is:\n$$\nL_{\\text{site}}(\\mu) = P(D | s_r=0, \\mu, \\mathcal{T}) = \\sum_{s_{\\mathcal{I} \\setminus \\{r\\}}} P(D, s_{\\mathcal{I} \\setminus \\{r\\}} | s_r=0, \\mu, \\mathcal{T})\n$$\nThe joint probability for a full assignment of states to all nodes in the tree is the product of transition probabilities along each branch. Let $u$ be the parent of node $v$, and $t_{uv}$ be the duration of the branch connecting them.\n$$\nP(D, s_{\\mathcal{I} \\setminus \\{r\\}} | s_r=0, \\mu, \\mathcal{T}) = \\prod_{(u,v) \\in \\text{edges}} P_{s_u s_v}(t_{uv})\n$$\nThe sum can be computed efficiently using a dynamic programming approach, commonly known as Felsenstein's pruning algorithm. One defines a conditional likelihood $L_v(i)$ for each node $v$ and each state $i \\in \\{0, 1\\}$, representing the probability of the observed states in the subtree descending from $v$, given that $v$ is in state $i$. The computation proceeds from the leaves to the root. The final likelihood for the site is $L_v(0)$ evaluated at the root node $v=r$.\n\nSince the $L$ barcode sites are independent and have an identical rate $\\mu$, the total likelihood for the observed states across all sites is the product of the individual site likelihoods. Let $D_j$ be the data for site $j$.\n$$\nL_{\\text{total}}(\\mu) = \\prod_{j=1}^{L} L_{\\text{site}, j}(\\mu)\n$$\nwhere $L_{\\text{site}, j}(\\mu)$ is the likelihood for site $j$ calculated as described above.\n\nPart 2: Maximum Likelihood Estimation for a Star Topology\n\nThe problem now specializes to a rooted star topology with $m$ leaves, where each leaf is connected to the root by a branch of duration $t$. The root state is known to be $0$ for all sites. This structure greatly simplifies the likelihood calculation. Given the root state is $0$, the states of the $m$ leaves evolve independently from each other along their respective branches.\n\nFor any given site at any given leaf, the state is the result of a process of duration $t$ starting from state $0$. The probability of this leaf-site being unedited (state $0$) is:\n$$\np_0 = P(\\text{state}=0) = P_{00}(t) = \\exp(-\\mu t)\n$$\nThe probability of this leaf-site being edited (state $1$) is:\n$$\np_1 = P(\\text{state}=1) = P_{01}(t) = 1 - \\exp(-\\mu t)\n$$\nWe have a total of $N = m \\times L$ leaf-site pairs. Each of these constitutes an independent Bernoulli trial with a success probability of $p_1$ (where success is defined as being in the edited state $1$).\nThe data are summarized as observing $K$ total edited states ($1$) and $N-K$ unedited states ($0$). The likelihood of observing $K$ successes in $N$ independent trials follows a binomial distribution:\n$$\nL(\\mu) = \\binom{N}{K} (p_1)^K (p_0)^{N-K}\n$$\nSubstituting the expressions for $p_0$ and $p_1$:\n$$\nL(\\mu) = \\binom{N}{K} (1 - \\exp(-\\mu t))^K (\\exp(-\\mu t))^{N-K}\n$$\nTo find the maximum likelihood estimate (MLE) $\\hat{\\mu}$, we maximize $L(\\mu)$ with respect to $\\mu$. It is more convenient to maximize the log-likelihood, $\\ln L(\\mu)$:\n$$\n\\ln L(\\mu) = \\ln\\binom{N}{K} + K \\ln(1 - \\exp(-\\mu t)) + (N-K) \\ln(\\exp(-\\mu t))\n$$\n$$\n\\ln L(\\mu) = \\ln\\binom{N}{K} + K \\ln(1 - \\exp(-\\mu t)) - (N-K)\\mu t\n$$\nWe differentiate with respect to $\\mu$ and set the result to zero:\n$$\n\\frac{d}{d\\mu} \\ln L(\\mu) = K \\left( \\frac{1}{1 - \\exp(-\\mu t)} \\right) ( - \\exp(-\\mu t) ) (-t) - (N-K)t = 0\n$$\n$$\n\\frac{K t \\exp(-\\mu t)}{1 - \\exp(-\\mu t)} - (N-K)t = 0\n$$\nAssuming $t \\neq 0$, we can divide by $t$:\n$$\n\\frac{K \\exp(-\\mu t)}{1 - \\exp(-\\mu t)} = N-K\n$$\n$$\nK \\exp(-\\mu t) = (N-K)(1 - \\exp(-\\mu t))\n$$\n$$\nK \\exp(-\\mu t) = N - K - (N-K)\\exp(-\\mu t)\n$$\n$$\n(K + N - K) \\exp(-\\mu t) = N - K\n$$\n$$\nN \\exp(-\\mu t) = N - K\n$$\n$$\n\\exp(-\\mu t) = \\frac{N-K}{N} = 1 - \\frac{K}{N}\n$$\nSolving for $\\mu$ gives the MLE, $\\hat{\\mu}$:\n$$\n-\\hat{\\mu} t = \\ln\\left(1 - \\frac{K}{N}\\right)\n$$\n$$\n\\hat{\\mu} = -\\frac{1}{t} \\ln\\left(1 - \\frac{K}{N}\\right)\n$$\nNow we substitute the given numerical values:\n$m = 4$\n$L = 1000$\n$t = 20$ h\n$K = 1230$\nThe total number of observations is $N = m \\times L = 4 \\times 1000 = 4000$.\n\nPlugging these into the formula for $\\hat{\\mu}$:\n$$\n\\hat{\\mu} = -\\frac{1}{20} \\ln\\left(1 - \\frac{1230}{4000}\\right)\n$$\n$$\n\\hat{\\mu} = -\\frac{1}{20} \\ln\\left(1 - 0.3075\\right)\n$$\n$$\n\\hat{\\mu} = -\\frac{1}{20} \\ln(0.6925)\n$$\nCalculating the value:\n$$\n\\hat{\\mu} \\approx -\\frac{1}{20} (-0.3674608) \\approx 0.01837304 \\, \\text{h}^{-1}\n$$\nRounding to four significant figures as required:\n$$\n\\hat{\\mu} \\approx 0.01837 \\, \\text{h}^{-1}\n$$", "answer": "$$\n\\boxed{0.01837}\n$$", "id": "2752064"}, {"introduction": "Real-world data from lineage recorders is rarely perfect; it is often noisy and may arise from a complex mixture of distinct cell populations or clones. The Expectation-Maximization (EM) algorithm is a powerful statistical tool for performing inference in models with latent (unobserved) variables, such as the true ancestral states and the clone identity of each cell. This advanced coding exercise [@problem_id:2752035] challenges you to implement the EM algorithm to de-noise barcode data and infer ancestral states, providing critical experience in developing the robust computational methods required for modern lineage analysis.", "problem": "You are given a generative model for synthetic barcodes used in molecular event recording and cellular lineage tracing. An experiment yields single-cell barcodes that are strings of binary states across independent sites, but the observed data are affected by site-wise dropout and sequencing error. Assume the following fundamental base and definitions hold:\n\n- Each of $M$ observed cells originates from exactly one of $C$ unknown ancestral clones. Each ancestral clone $c \\in \\{1,\\dots,C\\}$ is characterized by a latent, deterministic ancestral barcode of length $L$, denoted by a vector $a_{c} \\in \\{0,1\\}^{L}$.\n- A cell $i \\in \\{1,\\dots,M\\}$ is generated by first sampling its clone identity $z_{i} \\in \\{1,\\dots,C\\}$ from a categorical distribution with mixture weights $\\pi = (\\pi_{1},\\dots,\\pi_{C})$, where $\\pi_{c} \\ge 0$ and $\\sum_{c=1}^{C} \\pi_{c} = 1$.\n- Conditional on clone $z_{i}=c$, the true per-site state equals the corresponding ancestral state $a_{c,s}$ deterministically for each site $s \\in \\{1,\\dots,L\\}$.\n- Observation noise is applied independently per site: with dropout probability $\\delta \\in (0,1)$ the observation is missing (denoted as a special missing symbol), and otherwise a sequencing error flips the site with probability $\\varepsilon \\in (0, \\tfrac{1}{2})$, leaving it unchanged with probability $1-\\varepsilon$. All sites and cells are independent conditioned on $z_{i}$ and $a_{c}$.\n\nYour task is to formulate and implement an expectation-maximization (EM) algorithm to infer the most probable ancestral states from observed data under this model. The algorithm must maximize the likelihood with respect to the unknown mixture weights $\\pi$ and the unknown ancestral barcodes $\\{a_{c}\\}_{c=1}^{C}$, treating the clone identities $\\{z_{i}\\}_{i=1}^{M}$ as latent variables. The probabilistic structure and independence assumptions above serve as the only permitted starting point.\n\nThe observed data for each cell is a length-$L$ vector in $\\{-1,0,1\\}^{L}$ where $-1$ encodes a missing site due to dropout, and $0$ or $1$ are observed states. Assume that $\\delta$ and $\\varepsilon$ are known. The aim is purely algorithmic and statistical; there are no physical units or angles to report.\n\nDesign requirements:\n\n- The algorithm must:\n  - Use a complete-data likelihood consistent with the model, derive the expectation (E) step to compute posterior responsibilities for clone assignments, and derive the maximization (M) step to update $\\pi$ and the ancestral barcodes $\\{a_{c}\\}$.\n  - Properly handle missing observations by treating them as missing completely at random, without imputing them in the likelihood, and without assuming any dependence on $\\{a_{c}\\}$ or $\\pi$.\n  - Be robust to the label-switching indeterminacy inherent to mixture models by evaluating the inferred ancestral barcodes up to a permutation of clone labels when compared to ground truth.\n  - Use a numerically stable implementation of the E-step using log-probabilities.\n\n- Output specification:\n  - For each test case, after running EM to convergence, compute the minimal total Hamming distance between the inferred ancestral barcodes and the ground-truth ancestral barcodes, minimized over all permutations of clone labels. For a pair of barcodes of equal length, the Hamming distance is the number of positions at which the corresponding bits differ.\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3]\"), where each $r_{k}$ is the minimal total Hamming distance (an integer) for test case $k$.\n\nTest suite:\n\nFor each test case $k$, you must generate synthetic data according to the model as specified below, using the provided random seed to ensure reproducibility. Then apply your EM implementation and produce the required integer output.\n\n- Test case $1$ (happy path):\n  - $C = 2$, $L = 12$, $M = 200$, $\\delta = 0.05$, $\\varepsilon = 0.01$, $\\pi = (0.6, 0.4)$, random seed $= 123$.\n  - Ground-truth ancestral barcodes:\n    - $a_{1} = (1,1,0,0,1,1,0,0,1,1,0,0)$,\n    - $a_{2} = (0,0,1,1,0,0,1,1,0,0,1,1)$.\n\n- Test case $2$ (increased error and three clones):\n  - $C = 3$, $L = 10$, $M = 300$, $\\delta = 0.10$, $\\varepsilon = 0.05$, $\\pi = (0.5, 0.3, 0.2)$, random seed $= 456$.\n  - Ground-truth ancestral barcodes:\n    - $a_{1} = (1,0,1,0,1,0,1,0,1,0)$,\n    - $a_{2} = (0,1,0,1,0,1,0,1,0,1)$,\n    - $a_{3} = (1,1,1,0,0,0,1,1,0,0)$.\n\n- Test case $3$ (high dropout boundary):\n  - $C = 2$, $L = 8$, $M = 250$, $\\delta = 0.50$, $\\varepsilon = 0.01$, $\\pi = (0.5, 0.5)$, random seed $= 789$.\n  - Ground-truth ancestral barcodes:\n    - $a_{1} = (1,1,1,1,0,0,0,0)$,\n    - $a_{2} = (0,0,0,0,1,1,1,1)$.\n\n- Test case $4$ (single-clone edge case):\n  - $C = 1$, $L = 14$, $M = 150$, $\\delta = 0.20$, $\\varepsilon = 0.02$, $\\pi = (1.0)$, random seed $= 321$.\n  - Ground-truth ancestral barcode:\n    - $a_{1} = (1,0,1,1,0,0,1,0,1,0,1,1,0,1)$.\n\n- Test case $5$ (closely related clones):\n  - $C = 2$, $L = 16$, $M = 400$, $\\delta = 0.15$, $\\varepsilon = 0.02$, $\\pi = (0.7, 0.3)$, random seed $= 654$.\n  - Ground-truth ancestral barcodes:\n    - $a_{1} = (1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)$,\n    - $a_{2} = (1,1,0,1,1,1,0,1,1,1,1,1,0,1,1,1)$.\n\nAlgorithmic and numerical requirements:\n\n- Implement EM with multiple random initializations and select the solution with the highest observed-data log-likelihood for each test case. Use a maximum of $50$ iterations per initialization, a tolerance of $10^{-6}$ for convergence in the log-likelihood, and at least $5$ random restarts with distinct initial ancestral barcodes per test case.\n- Use a numerically stable log-sum-exp computation in the E-step responsibilities.\n- When updating an ancestral barcode site for a clone in the M-step, use the update that maximizes the expected complete-data log-likelihood given the current responsibilities; ties must leave the site unchanged from the previous iteration.\n- If a site has no observed values for any cells assigned (probabilistically) to a clone, leave that site unchanged during that iteration.\n\nFinal output format specification:\n\n- Your program should produce a single line of output containing a list of $5$ integers corresponding to the test cases in order, formatted exactly as \"[r1,r2,r3,r4,r5]\".", "solution": "The problem is valid as it presents a well-posed statistical inference task grounded in established principles of mixture models and their application in computational biology. The task is to derive and implement an Expectation-Maximization (EM) algorithm to find the maximum likelihood estimates for the parameters of a generative model for synthetic barcodes. The following derivation provides a complete solution.\n\nLet the observed data be $X = \\{x_i\\}_{i=1}^{M}$, where each $x_i \\in \\{-1, 0, 1\\}^L$ is a barcode for cell $i$. The latent variables are the clone assignments $Z = \\{z_i\\}_{i=1}^{M}$, where $z_i \\in \\{1, \\dots, C\\}$. The parameters to be estimated are $\\theta = (\\pi, \\{a_c\\}_{c=1}^C)$, where $\\pi$ is the vector of mixture proportions and $\\{a_c\\}$ are the ancestral barcodes. The dropout probability $\\delta$ and sequencing error probability $\\varepsilon$ are known constants.\n\nThe complete-data log-likelihood $\\mathcal{L}_c(\\theta; X, Z) = \\log P(X, Z | \\theta)$ is given by:\n$$\n\\mathcal{L}_c(\\theta; X, Z) = \\sum_{i=1}^{M} \\log P(x_i, z_i | \\theta) = \\sum_{i=1}^{M} \\left( \\log P(x_i | z_i, \\theta) + \\log P(z_i | \\theta) \\right)\n$$\nUsing a one-hot encoding for the latent variable $z_i$, where $\\mathbb{I}(z_i = c)$ is $1$ if cell $i$ belongs to clone $c$ and $0$ otherwise, we can write the log-likelihood as:\n$$\n\\mathcal{L}_c(\\theta; X, Z) = \\sum_{i=1}^{M} \\sum_{c=1}^{C} \\mathbb{I}(z_i = c) \\left( \\log \\pi_c + \\sum_{s=1}^{L} \\log P(x_{i,s} | a_{c,s}) \\right)\n$$\nThe probability of observing a state $x_{i,s}$ at a single site $s$ given the ancestral state $a_{c,s}$ is defined by the noise model:\n$$\nP(x_{i,s} | a_{c,s}) = \\begin{cases}\n\\delta  \\text{if } x_{i,s} = -1 \\text{ (dropout)} \\\\\n(1-\\delta)(1-\\varepsilon)  \\text{if } x_{i,s} = a_{c,s} \\text{ (observed, no error)} \\\\\n(1-\\delta)\\varepsilon  \\text{if } x_{i,s} \\neq a_{c,s} \\text{ and } x_{i,s} \\neq -1 \\text{ (observed, with error)}\n\\end{cases}\n$$\nThe EM algorithm iteratively finds a local maximum of the observed-data log-likelihood $\\mathcal{L}(\\theta; X) = \\log P(X|\\theta)$ by alternating between an E-step and an M-step.\n\n**E-Step: Expectation**\nIn the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $Z$ given the observed data $X$ and the current parameter estimates $\\theta^{(t)} = (\\pi^{(t)}, \\{a_c^{(t)}\\})$. This requires computing the posterior probabilities, or responsibilities, $r_{ic}^{(t)}$, for each cell $i$ belonging to each clone $c$.\n$$\nr_{ic}^{(t)} \\equiv P(z_i = c | x_i, \\theta^{(t)}) = \\frac{P(x_i | z_i=c, \\theta^{(t)}) P(z_i=c | \\theta^{(t)})}{\\sum_{j=1}^{C} P(x_i | z_i=j, \\theta^{(t)}) P(z_i=j | \\theta^{(t)})}\n$$\nThe terms in the formula are:\n$$\nP(z_i=c | \\theta^{(t)}) = \\pi_c^{(t)}\n$$\n$$\nP(x_i | z_i=c, \\theta^{(t)}) = \\prod_{s=1}^{L} P(x_{i,s} | a_{c,s}^{(t)})\n$$\nFor numerical stability, computations are performed in log-space. Let $\\log P_{ic}^{(t)}$ be the log of the joint probability of observing $x_i$ and it originating from clone $c$:\n$$\n\\log P_{ic}^{(t)} = \\log \\pi_c^{(t)} + \\sum_{s=1}^{L} \\log P(x_{i,s} | a_{c,s}^{(t)})\n$$\nThe responsibilities are then calculated using a numerically stable softmax (log-sum-exp trick):\n$$\nr_{ic}^{(t)} = \\frac{\\exp(\\log P_{ic}^{(t)})}{\\sum_{j=1}^{C} \\exp(\\log P_{ij}^{(t)})}\n$$\nThe expected complete-data log-likelihood, known as the $Q$-function, is:\n$$\nQ(\\theta | \\theta^{(t)}) = E_{Z|X,\\theta^{(t)}}[\\mathcal{L}_c(\\theta; X, Z)] = \\sum_{i=1}^{M} \\sum_{c=1}^{C} r_{ic}^{(t)} \\left( \\log \\pi_c + \\sum_{s=1}^{L} \\log P(x_{i,s} | a_{c,s}) \\right)\n$$\n\n**M-Step: Maximization**\nIn the M-step, we find the parameters $\\theta^{(t+1)}$ that maximize the $Q$-function. We can maximize with respect to $\\pi$ and $\\{a_c\\}$ independently.\n\nMaximizing with respect to $\\pi$: We maximize $\\sum_{i,c} r_{ic}^{(t)} \\log \\pi_c$ subject to the constraint $\\sum_c \\pi_c = 1$. The standard Lagrange multiplier approach yields the update rule:\n$$\n\\pi_c^{(t+1)} = \\frac{\\sum_{i=1}^{M} r_{ic}^{(t)}}{M}\n$$\n\nMaximizing with respect to $\\{a_{c,s}\\}$: The update for each ancestral bit $a_{c,s}$ is independent of the others. We need to choose $a_{c,s} \\in \\{0, 1\\}$ to maximize $\\sum_{i=1}^{M} r_{ic}^{(t)} \\log P(x_{i,s} | a_{c,s})$. The term $\\log P(x_{i,s} | a_{c,s})$ is constant with respect to $a_{c,s}$ for missing sites ($x_{i,s} = -1$). For observed sites, we compare the objective for $a_{c,s}=0$ versus $a_{c,s}=1$. The value of $a_{c,s}^{(t+1)}$ is chosen to be $k \\in \\{0, 1\\}$ which maximizes:\n$$\n\\sum_{i: x_{i,s} \\in \\{0,1\\}} r_{ic}^{(t)} \\left( \\mathbb{I}(x_{i,s}=k) \\log(1-\\varepsilon) + \\mathbb{I}(x_{i,s} \\neq k) \\log\\varepsilon \\right)\n$$\nLet a weighted sum of responsibilities be defined as $W_{c,s,v} = \\sum_{i: x_{i,s}=v} r_{ic}^{(t)}$ for $v \\in \\{0, 1\\}$. The objective for $k=0$ is $W_{c,s,0}\\log(1-\\varepsilon) + W_{c,s,1}\\log\\varepsilon$, and for $k=1$ it is $W_{c,s,1}\\log(1-\\varepsilon) + W_{c,s,0}\\log\\varepsilon$. Since $\\varepsilon \\in (0, 1/2)$, $\\log(1-\\varepsilon)  \\log\\varepsilon$. The optimal choice for $a_{c,s}$ is $1$ if $W_{c,s,1}  W_{c,s,0}$ and $0$ if $W_{c,s,0}  W_{c,s,1}$. This corresponds to a weighted majority vote. If $W_{c,s,1} = W_{c,s,0}$ or if there are no observations for this site and clone, the state remains unchanged as per the problem specification.\n$$\na_{c,s}^{(t+1)} = \\begin{cases}\n1  \\text{if } W_{c,s,1}  W_{c,s,0} \\\\\n0  \\text{if } W_{c,s,0}  W_{c,s,1} \\\\\na_{c,s}^{(t)}  \\text{otherwise}\n\\end{cases}\n$$\n\n**Algorithm and Evaluation**\nThe EM algorithm is iterated until the change in the observed-data log-likelihood, $\\mathcal{L}(\\theta; X) = \\sum_{i=1}^{M} \\log \\left( \\sum_{c=1}^{C} \\exp(\\log P_{ic}) \\right)$, falls below a tolerance. Due to the risk of converging to a local maximum, the process is repeated with multiple random initializations for the ancestral barcodes $\\{a_c\\}$, and the solution yielding the highest final log-likelihood is selected.\n\nTo evaluate the inferred barcodes $\\{a_c^{\\text{inferred}}\\}$ against the ground-truth $\\{a_c^{\\text{true}}\\}$, we must account for label-switching indeterminacy. The minimal total Hamming distance is computed by finding the optimal permutation $\\sigma$ of clone labels that minimizes the sum of distances:\n$$\n\\text{min\\_dist} = \\min_{\\sigma \\in S_C} \\sum_{c=1}^{C} \\text{Hamming}(a_c^{\\text{inferred}}, a_{\\sigma(c)}^{\\text{true}})\n$$\nwhere $S_C$ is the set of all permutations of $\\{1, \\dots, C\\}$. This is an assignment problem, which for small $C$ can be solved by iterating through all $C!$ permutations.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nimport itertools\n\ndef generate_data(M, L, C, pi, a_true, delta, epsilon, seed):\n    \"\"\"Generates synthetic barcode data according to the specified model.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Sample clone identities for M cells\n    z_true = rng.choice(C, size=M, p=pi)\n    \n    # 2. Get true barcodes for each cell\n    true_barcodes = a_true[z_true]\n    \n    # 3. Apply noise model\n    X = np.zeros_like(true_barcodes, dtype=int)\n    \n    # Dropout\n    dropout_mask = rng.random(size=(M, L))  delta\n    \n    # Sequencing error\n    error_mask = rng.random(size=(M, L))  epsilon\n    \n    # Apply error to non-dropped-out sites\n    flipped_barcodes = 1 - true_barcodes\n    X = np.where(error_mask, flipped_barcodes, true_barcodes)\n    \n    # Apply dropout\n    X[dropout_mask] = -1\n    \n    return X\n\ndef em_algorithm(X, C, L, M, delta, epsilon, n_restarts, max_iter, tol):\n    \"\"\"\n    Performs EM to infer ancestral barcodes and mixture weights.\n    \"\"\"\n    best_log_likelihood = -np.inf\n    best_a = None\n    best_pi = None\n\n    log_delta = np.log(delta)\n    log_1_minus_delta = np.log(1 - delta)\n    log_epsilon = np.log(epsilon)\n    log_1_minus_epsilon = np.log(1 - epsilon)\n\n    for restart in range(n_restarts):\n        # 1. Initialize parameters\n        init_rng = np.random.default_rng(restart)\n        pi_t = np.ones(C) / C\n        \n        # Generate C distinct random barcodes\n        a_t = np.zeros((C, L), dtype=int)\n        generated_barcodes = set()\n        n_generated = 0\n        while n_generated  C:\n            barcode = tuple(init_rng.integers(0, 2, size=L))\n            if barcode not in generated_barcodes:\n                generated_barcodes.add(barcode)\n                a_t[n_generated, :] = barcode\n                n_generated += 1\n        \n        log_likelihood_old = -np.inf\n\n        for _ in range(max_iter):\n            # 2. E-Step\n            log_prob_x_given_a = np.zeros((M, C))\n            for c in range(C):\n                a_c = a_t[c, :]\n                is_eq = (X == a_c)\n                is_neq = (X != a_c)  (X != -1)\n                is_missing = (X == -1)\n                \n                log_p_matrix = (is_missing * log_delta +\n                                is_eq * (log_1_minus_delta + log_1_minus_epsilon) +\n                                is_neq * (log_1_minus_delta + log_epsilon))\n                log_prob_x_given_a[:, c] = log_p_matrix.sum(axis=1)\n\n            log_joint = log_prob_x_given_a + np.log(pi_t)\n\n            log_joint_max = np.max(log_joint, axis=1, keepdims=True)\n            log_joint_shifted = log_joint - log_joint_max\n            exp_log_joint_shifted = np.exp(log_joint_shifted)\n            \n            responsibilities = exp_log_joint_shifted / np.sum(exp_log_joint_shifted, axis=1, keepdims=True)\n            \n            log_likelihood_new = np.sum(log_joint_max.flatten() + np.log(np.sum(exp_log_joint_shifted, axis=1)))\n\n            if np.abs(log_likelihood_new - log_likelihood_old)  tol:\n                break\n            log_likelihood_old = log_likelihood_new\n\n            # 3. M-Step\n            pi_t_new = np.sum(responsibilities, axis=0) / M\n\n            a_t_new = a_t.copy()\n            for s in range(L):\n                W_s0 = np.zeros(C)\n                W_s1 = np.zeros(C)\n                \n                obs_0_indices = np.where(X[:, s] == 0)[0]\n                if obs_0_indices.size  0:\n                    W_s0 = np.sum(responsibilities[obs_0_indices, :], axis=0)\n                \n                obs_1_indices = np.where(X[:, s] == 1)[0]\n                if obs_1_indices.size  0:\n                    W_s1 = np.sum(responsibilities[obs_1_indices, :], axis=0)\n                \n                a_t_new[W_s1  W_s0, s] = 1\n                a_t_new[W_s0  W_s1, s] = 0\n            \n            # Add a small regularization term to pi to avoid log(0)\n            pi_t = pi_t_new + 1e-9\n            pi_t /= pi_t.sum()\n\n            a_t = a_t_new\n\n        if log_likelihood_new  best_log_likelihood:\n            best_log_likelihood = log_likelihood_new\n            best_a = a_t\n            best_pi = pi_t\n            \n    return best_a, best_pi\n\ndef calculate_min_hamming_distance(a_inferred, a_true):\n    \"\"\"Calculates the minimum total Hamming distance over all label permutations.\"\"\"\n    C = a_inferred.shape[0]\n    if C == 0:\n        return 0\n    \n    cost_matrix = np.zeros((C, C))\n    for i in range(C):\n        for j in range(C):\n            cost_matrix[i, j] = np.sum(a_inferred[i] != a_true[j])\n            \n    # For small C, iterating through permutations is also feasible.\n    # SciPy's linear_sum_assignment is a general and efficient solution.\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    min_dist = cost_matrix[row_ind, col_ind].sum()\n    \n    return int(min_dist)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # (C, L, M, delta, epsilon, pi, a_true, seed)\n        (2, 12, 200, 0.05, 0.01, np.array([0.6, 0.4]), np.array([[1,1,0,0,1,1,0,0,1,1,0,0],[0,0,1,1,0,0,1,1,0,0,1,1]]), 123),\n        (3, 10, 300, 0.10, 0.05, np.array([0.5, 0.3, 0.2]), np.array([[1,0,1,0,1,0,1,0,1,0],[0,1,0,1,0,1,0,1,0,1],[1,1,1,0,0,0,1,1,0,0]]), 456),\n        (2, 8, 250, 0.50, 0.01, np.array([0.5, 0.5]), np.array([[1,1,1,1,0,0,0,0],[0,0,0,0,1,1,1,1]]), 789),\n        (1, 14, 150, 0.20, 0.02, np.array([1.0]), np.array([[1,0,1,1,0,0,1,0,1,0,1,1,0,1]]), 321),\n        (2, 16, 400, 0.15, 0.02, np.array([0.7, 0.3]), np.array([[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[1,1,0,1,1,1,0,1,1,1,1,1,0,1,1,1]]), 654)\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        C, L, M, delta, epsilon, pi, a_true, seed = case\n        \n        # 1. Generate data\n        X = generate_data(M, L, C, pi, a_true, delta, epsilon, seed)\n        \n        # 2. Run EM algorithm\n        a_inferred, _ = em_algorithm(X, C, L, M, delta, epsilon, n_restarts=5, max_iter=50, tol=1e-6)\n        \n        # 3. Calculate minimal Hamming distance\n        result = calculate_min_hamming_distance(a_inferred, a_true)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2752035"}]}