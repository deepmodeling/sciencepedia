{"hands_on_practices": [{"introduction": "The first step in analyzing multivariate selection is to distinguish between the total change observed in traits and the direct forces of selection acting upon them. Phenotypic correlations among traits can cause a trait to change simply because it is correlated with another trait that is the true target of selection. This exercise [@problem_id:2737204] provides a foundational practice in using the Lande-Arnold framework to calculate the directional selection gradient vector, $\\boldsymbol{\\beta}$, which measures direct selection, from the observable selection differential, $\\mathbf{S}$.", "problem": "A natural population of a vertebrate species is monitored for three quantitative traits: body size ($z_{1}$), display rate ($z_{2}$), and limb length ($z_{3}$). Let the phenotypic trait vector be $\\mathbf{z} = (z_{1}, z_{2}, z_{3})^{\\top}$. In a single breeding season, relative fitness is measured as individual reproductive success divided by the population mean reproductive success. The following quantities are estimated from a large, random sample of adults:\n\n- The phenotypic covariance matrix of traits (across all adults before selection):\n$$\n\\mathbf{P} \\;=\\;\n\\begin{pmatrix}\n1.2  0.3  -0.1 \\\\\n0.3  0.8  0.2 \\\\\n-0.1  0.2  0.5\n\\end{pmatrix}.\n$$\n\n- The vector of selection differentials (the difference in trait means between the selected parents and the pre-selection population means), expressed in the same trait units:\n$$\n\\mathbf{S} \\;=\\; \n\\begin{pmatrix}\n0.12 \\\\\n0.04 \\\\\n-0.02\n\\end{pmatrix}.\n$$\n\nWork within the Lande–Arnold framework for multivariate selection, taking as fundamental base the Price equation definition of the selection differential as a covariance, the definition of directional selection gradients as the partial regression coefficients of relative fitness on traits, and the first-order (linear) approximation to the fitness surface around the mean phenotype. Assume that higher-order (quadratic) terms, including correlational selection on trait combinations, are negligible for the purpose of mapping selection differentials to directional gradients over the observed trait range.\n\nUsing only these bases and assumptions, derive the relationship that identifies the directional selection gradient vector $\\boldsymbol{\\beta}$ from $\\mathbf{P}$ and $\\mathbf{S}$, then compute the numerical value of $\\boldsymbol{\\beta}$ for the data above. Finally, interpret the elements of $\\boldsymbol{\\beta}$ in terms of predicted immediate phenotypic change, making explicit the distinction between the within-generation change captured by $\\mathbf{S}$ and the per-generation evolutionary response under additive genetics.\n\nRound each element of your vector answer for $\\boldsymbol{\\beta}$ to four significant figures. Report the answer as a row vector with no units.", "solution": "The problem requires the derivation of the directional selection gradient vector, $\\boldsymbol{\\beta}$, from the phenotypic covariance matrix, $\\mathbf{P}$, and the selection differential vector, $\\mathbf{S}$, within the Lande-Arnold framework. Following this derivation, a numerical calculation of $\\boldsymbol{\\beta}$ is required, along with an interpretation of its components.\n\nFirst, we derive the fundamental relationship between $\\mathbf{S}$, $\\mathbf{P}$, and $\\boldsymbol{\\beta}$. The problem states that the analysis is based on a first-order (linear) approximation of the fitness surface, $w(\\mathbf{z})$, around the mean phenotype, $\\bar{\\mathbf{z}}$. This approximation is given by:\n$$w(\\mathbf{z}) \\approx w(\\bar{\\mathbf{z}}) + (\\mathbf{z} - \\bar{\\mathbf{z}})^{\\top} \\boldsymbol{\\beta}$$\nwhere $\\boldsymbol{\\beta}$ is the vector of directional selection gradients. The components of $\\boldsymbol{\\beta}$, denoted $\\beta_i$, are the partial derivatives of relative fitness with respect to the trait values, evaluated at the population mean: $\\beta_i = \\frac{\\partial w}{\\partial z_i} \\Big|_{\\mathbf{z}=\\bar{\\mathbf{z}}}$. Relative fitness $w$ is defined such that its population mean is $\\bar{w} = E[w(\\mathbf{z})] = 1$. Evaluating the expectation of the linear fitness approximation, we find $E[w(\\mathbf{z})] = w(\\bar{\\mathbf{z}})$, thus $w(\\bar{\\mathbf{z}}) \\approx 1$. The fitness function is then:\n$$w(\\mathbf{z}) \\approx 1 + (\\mathbf{z} - \\bar{\\mathbf{z}})^{\\top} \\boldsymbol{\\beta}$$\nThe selection differential, $\\mathbf{S}$, is defined by the Price equation as the covariance between the trait vector $\\mathbf{z}$ and relative fitness $w$.\n$$\\mathbf{S} = \\text{cov}(\\mathbf{z}, w) = E\\left[(\\mathbf{z} - \\bar{\\mathbf{z}})(w - \\bar{w})\\right]$$\nSubstituting $\\bar{w} = 1$ and the linear approximation for $w(\\mathbf{z})$:\n$$\\mathbf{S} \\approx E\\left[(\\mathbf{z} - \\bar{\\mathbf{z}})(1 + (\\mathbf{z} - \\bar{\\mathbf{z}})^{\\top} \\boldsymbol{\\beta} - 1)\\right]$$\n$$\\mathbf{S} \\approx E\\left[(\\mathbf{z} - \\bar{\\mathbf{z}})((\\mathbf{z} - \\bar{\\mathbf{z}})^{\\top} \\boldsymbol{\\beta})\\right]$$\nSince $(\\mathbf{z} - \\bar{\\mathbf{z}})^{\\top} \\boldsymbol{\\beta}$ is a scalar, we can rearrange the expression. We use the property that for a column vector $\\mathbf{v}$ and a matrix $\\mathbf{A}$, $E[\\mathbf{v}\\mathbf{v}^\\top \\mathbf{A}] = E[\\mathbf{v}\\mathbf{v}^\\top]\\mathbf{A}$. Here $\\mathbf{v} = (\\mathbf{z}-\\bar{\\mathbf{z}})$ and $\\mathbf{A} = \\boldsymbol{\\beta}$. This leads to:\n$$\\mathbf{S} \\approx E\\left[(\\mathbf{z} - \\bar{\\mathbf{z}})(\\mathbf{z} - \\bar{\\mathbf{z}})^{\\top}\\right] \\boldsymbol{\\beta}$$\nThe term $E\\left[(\\mathbf{z} - \\bar{\\mathbf{z}})(\\mathbf{z} - \\bar{\\mathbf{z}})^{\\top}\\right]$ is the definition of the phenotypic variance-covariance matrix, $\\mathbf{P}$. Thus, we arrive at the central equation of multivariate selection:\n$$\\mathbf{S} = \\mathbf{P} \\boldsymbol{\\beta}$$\nThis is the required relationship.\n\nNext, we compute the numerical value of $\\boldsymbol{\\beta}$. To do so, we must solve the matrix equation for $\\boldsymbol{\\beta}$ by pre-multiplying by the inverse of the phenotypic covariance matrix, $\\mathbf{P}^{-1}$:\n$$\\boldsymbol{\\beta} = \\mathbf{P}^{-1} \\mathbf{S}$$\nThe given matrices are:\n$$\n\\mathbf{P} \\;=\\;\n\\begin{pmatrix}\n1.2  0.3  -0.1 \\\\\n0.3  0.8  0.2 \\\\\n-0.1  0.2  0.5\n\\end{pmatrix}\n\\quad \\text{and} \\quad\n\\mathbf{S} \\;=\\; \n\\begin{pmatrix}\n0.12 \\\\\n0.04 \\\\\n-0.02\n\\end{pmatrix}\n$$\nThe inverse matrix is $\\mathbf{P}^{-1} = \\frac{1}{\\det(\\mathbf{P})} \\text{adj}(\\mathbf{P})$. First, we calculate the determinant of $\\mathbf{P}$:\n$$\\det(\\mathbf{P}) = 1.2(0.8 \\cdot 0.5 - 0.2 \\cdot 0.2) - 0.3(0.3 \\cdot 0.5 - 0.2 \\cdot (-0.1)) + (-0.1)(0.3 \\cdot 0.2 - 0.8 \\cdot (-0.1))$$\n$$\\det(\\mathbf{P}) = 1.2(0.36) - 0.3(0.17) - 0.1(0.14) = 0.432 - 0.051 - 0.014 = 0.367$$\nThe adjugate of $\\mathbf{P}$ is the transpose of its cofactor matrix. For a symmetric matrix such as $\\mathbf{P}$, the cofactor matrix is also symmetric, so the adjugate is equal to the cofactor matrix. The cofactors $C_{ij}$ are:\n$C_{11} = (0.8)(0.5) - (0.2)(0.2) = 0.36$\n$C_{12} = -((0.3)(0.5) - (0.2)(-0.1)) = -0.17$\n$C_{13} = (0.3)(0.2) - (0.8)(-0.1) = 0.14$\n$C_{21} = C_{12} = -0.17$\n$C_{22} = (1.2)(0.5) - (-0.1)(-0.1) = 0.59$\n$C_{23} = -((1.2)(0.2) - (0.3)(-0.1)) = -0.27$\n$C_{31} = C_{13} = 0.14$\n$C_{32} = C_{23} = -0.27$\n$C_{33} = (1.2)(0.8) - (0.3)(0.3) = 0.87$\nThus, the inverse matrix is:\n$$\\mathbf{P}^{-1} = \\frac{1}{0.367} \\begin{pmatrix} 0.36  -0.17  0.14 \\\\ -0.17  0.59  -0.27 \\\\ 0.14  -0.27  0.87 \\end{pmatrix}$$\nNow we calculate $\\boldsymbol{\\beta} = \\mathbf{P}^{-1} \\mathbf{S}$:\n$$\\boldsymbol{\\beta} = \\frac{1}{0.367} \\begin{pmatrix} 0.36  -0.17  0.14 \\\\ -0.17  0.59  -0.27 \\\\ 0.14  -0.27  0.87 \\end{pmatrix} \\begin{pmatrix} 0.12 \\\\ 0.04 \\\\ -0.02 \\end{pmatrix}$$\n$$\\boldsymbol{\\beta} = \\frac{1}{0.367} \\begin{pmatrix} (0.36)(0.12) + (-0.17)(0.04) + (0.14)(-0.02) \\\\ (-0.17)(0.12) + (0.59)(0.04) + (-0.27)(-0.02) \\\\ (0.14)(0.12) + (-0.27)(0.04) + (0.87)(-0.02) \\end{pmatrix}$$\n$$\\boldsymbol{\\beta} = \\frac{1}{0.367} \\begin{pmatrix} 0.0432 - 0.0068 - 0.0028 \\\\ -0.0204 + 0.0236 + 0.0054 \\\\ 0.0168 - 0.0108 - 0.0174 \\end{pmatrix} = \\frac{1}{0.367} \\begin{pmatrix} 0.0336 \\\\ 0.0086 \\\\ -0.0114 \\end{pmatrix}$$\nPerforming the divisions and rounding to four significant figures:\n$$\\boldsymbol{\\beta} = \\begin{pmatrix} 0.091553... \\\\ 0.023433... \\\\ -0.031062... \\end{pmatrix} \\approx \\begin{pmatrix} 0.09155 \\\\ 0.02343 \\\\ -0.03106 \\end{pmatrix}$$\n\nFinally, we interpret the results. The elements of the selection gradient vector $\\boldsymbol{\\beta}$ quantify the strength of direct natural selection on each trait, independent of the effects of phenotypic correlations among traits.\n- $\\beta_1 \\approx 0.09155$: This positive value indicates direct selection favoring an increase in body size ($z_1$). Holding the other traits constant, larger body size is associated with higher relative fitness.\n- $\\beta_2 \\approx 0.02343$: This positive value indicates direct selection favoring an increase in display rate ($z_2$). After controlling for body size and limb length, a higher rate of display is associated with higher relative fitness.\n- $\\beta_3 \\approx -0.03106$: This negative value indicates direct selection favoring a decrease in limb length ($z_3$). Holding the other traits constant, shorter limbs are associated with higher relative fitness.\n\nIt is critical to distinguish the selection gradient $\\boldsymbol{\\beta}$ from the selection differential $\\mathbf{S}$. The selection differential vector $\\mathbf{S} = (0.12, 0.04, -0.02)^{\\top}$ measures the total change in the mean of each trait within a generation due to selection. This total change is a composite of direct selection on the trait itself and indirect selection arising from that trait's phenotypic correlation with other traits under selection. For example, the total change in mean body size, $S_1 = 0.12$, is larger than what would be predicted from direct selection alone ($P_{11} \\beta_1 = 1.2 \\times 0.09155 \\approx 0.1099$), because of additional indirect selection through correlations with display rate ($P_{12} \\beta_2 = 0.3 \\times 0.02343 \\approx 0.0070$) and limb length ($P_{13} \\beta_3 = -0.1 \\times -0.03106 \\approx 0.0031$). The selection gradient $\\boldsymbol{\\beta}$ thus provides a clearer picture of the direct \"targets\" of selection.\n\nFurthermore, the immediate phenotypic change within a generation, $\\mathbf{S}$, is not equivalent to the evolutionary response across generations. The predicted trans-generational change in the population mean phenotype, $\\Delta\\bar{\\mathbf{z}}$, is given by the multivariate breeder's equation, $\\Delta\\bar{\\mathbf{z}} = \\mathbf{G}\\boldsymbol{\\beta}$, where $\\mathbf{G}$ is the additive genetic covariance matrix. Evolutionary response depends on the direct selection pressures ($\\boldsymbol{\\beta}$) acting on heritable variation and covariation ($\\mathbf{G}$), not on the total phenotypic selection differential $\\mathbf{S}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.09155  0.02343  -0.03106\n\\end{pmatrix}\n}\n$$", "id": "2737204"}, {"introduction": "While directional gradients describe the slopes of the fitness landscape, its overall shape—its peaks, valleys, and ridges—is determined by quadratic and correlational selection. This form of selection, described by the $\\boldsymbol{\\Gamma}$ matrix, acts on the variance of traits and the covariance between them. This exercise [@problem_id:2737188] will guide you through the process of analyzing the fitness surface's curvature by finding its canonical axes, allowing you to classify selection as stabilizing or disruptive along these key directions.", "problem": "In a population of organisms measured for two standardized morphological traits, denoted by the column vector $\\mathbf{z} = (z_{1}, z_{2})^{\\mathsf{T}}$, suppose the log-relative fitness surface near the mean phenotype is well approximated by a second-order Taylor expansion of the form\n$$\n\\ln w(\\mathbf{z}) \\approx \\alpha + \\boldsymbol{\\beta}^{\\mathsf{T}} \\mathbf{z} + \\frac{1}{2}\\,\\mathbf{z}^{\\mathsf{T}} \\boldsymbol{\\Gamma}\\, \\mathbf{z},\n$$\nwhere $\\alpha$ is a constant, $\\boldsymbol{\\beta}$ is the vector of directional selection gradients, and $\\boldsymbol{\\Gamma}$ is the symmetric matrix of quadratic and correlational selection gradients (the Hessian of $\\ln w$ with respect to $\\mathbf{z}$). The canonical axes of the fitness surface are defined as the orthonormal eigenvectors of $\\boldsymbol{\\Gamma}$; along these axes, the curvature of the surface is given by the corresponding eigenvalues, and the axes are obtained by a rotation of the original trait coordinate system.\n\nAssume that traits are standardized (mean $0$, variance $1$) and that empirical estimation has yielded the following symmetric matrix of quadratic and correlational selection gradients:\n$$\n\\boldsymbol{\\Gamma} \\;=\\; \\begin{pmatrix}\n-0.40  0.30 \\\\\n0.30  0.10\n\\end{pmatrix}.\n$$\nStarting from the definition of the canonical axes as the eigenvectors of $\\boldsymbol{\\Gamma}$ and the interpretation of curvature via second derivatives, derive the rotation that diagonalizes $\\boldsymbol{\\Gamma}$, compute the rotation angle $\\theta$ (in radians) of the first canonical axis relative to the $z_{1}$-axis, where the first canonical axis is defined as the eigenvector associated with the larger eigenvalue, and determine the eigenvalues to classify selection along each canonical axis as stabilizing (negative curvature) or disruptive (positive curvature). Report the rotation angle $\\theta$ and the ordered pair of eigenvalues $(\\lambda_{1} \\ge \\lambda_{2})$ as a single $1 \\times 3$ row matrix, with the angle in radians. Round all three reported quantities to four significant figures.", "solution": "The problem as stated is scientifically sound, well-posed, objective, and self-contained. It is a standard application of linear algebra to the Lande-Arnold framework for analyzing multivariate selection. The provided data are sufficient and consistent. Therefore, the problem is valid and we proceed to its solution.\n\nThe task is to find the canonical axes and curvatures of the log-relative fitness surface, which are given by the eigenvectors and eigenvalues, respectively, of the matrix of quadratic selection gradients $\\boldsymbol{\\Gamma}$.\n\nThe given matrix is:\n$$\n\\boldsymbol{\\Gamma} = \\begin{pmatrix}\n-0.40  0.30 \\\\\n0.30  0.10\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are found by solving the characteristic equation $\\det(\\boldsymbol{\\Gamma} - \\lambda \\mathbf{I}) = 0$, where $\\mathbf{I}$ is the $2 \\times 2$ identity matrix.\n$$\n\\det \\begin{pmatrix} -0.40 - \\lambda  0.30 \\\\ 0.30  0.10 - \\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant gives the characteristic polynomial:\n$$\n(-0.40 - \\lambda)(0.10 - \\lambda) - (0.30)(0.30) = 0\n$$\n$$\n-0.04 - 0.10\\lambda + 0.40\\lambda + \\lambda^2 - 0.09 = 0\n$$\n$$\n\\lambda^2 + 0.30\\lambda - 0.13 = 0\n$$\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=0.30$, and $c=-0.13$.\n$$\n\\lambda = \\frac{-0.30 \\pm \\sqrt{(0.30)^2 - 4(1)(-0.13)}}{2(1)}\n$$\n$$\n\\lambda = \\frac{-0.30 \\pm \\sqrt{0.09 + 0.52}}{2}\n$$\n$$\n\\lambda = \\frac{-0.30 \\pm \\sqrt{0.61}}{2}\n$$\nThe problem defines the first canonical axis as being associated with the larger eigenvalue, $\\lambda_{1}$. Thus, we have:\n$$\n\\lambda_{1} = \\frac{-0.30 + \\sqrt{0.61}}{2} \\approx \\frac{-0.30 + 0.781025}{2} \\approx 0.2405125\n$$\n$$\n\\lambda_{2} = \\frac{-0.30 - \\sqrt{0.61}}{2} \\approx \\frac{-0.30 - 0.781025}{2} \\approx -0.5405125\n$$\nThe eigenvalues represent the curvature of the fitness surface along the canonical axes. A positive eigenvalue indicates a local minimum in that direction (disruptive selection), while a negative eigenvalue indicates a local maximum (stabilizing selection).\nHere, $\\lambda_{1} > 0$, so selection is disruptive along the first canonical axis (a fitness valley).\nAnd $\\lambda_{2}  0$, so selection is stabilizing along the second canonical axis (a fitness ridge).\n\nNext, we find the eigenvector $\\mathbf{v}_{1} = \\begin{pmatrix} v_{11} \\\\ v_{12} \\end{pmatrix}$ corresponding to the larger eigenvalue $\\lambda_{1}$. This eigenvector defines the orientation of the first canonical axis. It is found by solving the system $(\\boldsymbol{\\Gamma} - \\lambda_{1} \\mathbf{I})\\mathbf{v}_{1} = \\mathbf{0}$.\n$$\n\\begin{pmatrix} -0.40 - \\lambda_{1}  0.30 \\\\ 0.30  0.10 - \\lambda_{1} \\end{pmatrix} \\begin{pmatrix} v_{11} \\\\ v_{12} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we obtain the relationship:\n$$\n(-0.40 - \\lambda_{1})v_{11} + 0.30 v_{12} = 0\n$$\n$$\nv_{12} = \\frac{0.40 + \\lambda_{1}}{0.30} v_{11}\n$$\nThe rotation angle $\\theta$ of this axis relative to the $z_{1}$-axis is given by $\\tan(\\theta) = \\frac{v_{12}}{v_{11}}$.\n$$\n\\tan(\\theta) = \\frac{0.40 + \\lambda_{1}}{0.30}\n$$\nSubstituting the expression for $\\lambda_1$:\n$$\n\\tan(\\theta) = \\frac{0.40 + \\frac{-0.30 + \\sqrt{0.61}}{2}}{0.30} = \\frac{\\frac{0.80 - 0.30 + \\sqrt{0.61}}{2}}{0.30} = \\frac{0.50 + \\sqrt{0.61}}{0.60}\n$$\nNumerically, this value is:\n$$\n\\tan(\\theta) \\approx \\frac{0.50 + 0.781025}{0.60} \\approx \\frac{1.281025}{0.60} \\approx 2.13504\n$$\nThe angle $\\theta$ is the arctangent of this value. Since both the numerator and denominator are positive, the eigenvector lies in the first quadrant, and the principal value of the arctangent is the correct angle.\n$$\n\\theta = \\arctan\\left(\\frac{0.50 + \\sqrt{0.61}}{0.60}\\right) \\approx \\arctan(2.13504) \\approx 1.13279 \\text{ radians}\n$$\nThe problem requires the results to be rounded to four significant figures.\nThe rotation angle is $\\theta \\approx 1.133$.\nThe first eigenvalue is $\\lambda_{1} \\approx 0.2405$.\nThe second eigenvalue is $\\lambda_{2} \\approx -0.5405$.\n\nThe final result is the ordered tuple $(\\theta, \\lambda_{1}, \\lambda_{2})$ presented as a $1 \\times 3$ row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.133  0.2405  -0.5405 \\end{pmatrix}}\n$$", "id": "2737188"}, {"introduction": "Measuring selection is only half the story; the ultimate goal is to predict the evolutionary response. This requires knowing not only the forces of selection ($\\boldsymbol{\\beta}$) but also the patterns of heritable variation and covariation ($\\mathbf{G}$). This practice [@problem_id:2737209] centers on the multivariate breeder's equation, $\\Delta\\bar{\\mathbf{z}} = \\mathbf{G}\\boldsymbol{\\beta}$, to demonstrate how the genetic architecture of traits can cause the evolutionary trajectory to deviate from the direction of selection, a fundamental concept of evolutionary constraint and evolvability.", "problem": "A population expresses three standardized quantitative traits, denoted by the mean trait vector $\\bar{\\mathbf{z}} \\in \\mathbb{R}^{3}$. Assume that phenotypic covariances equal additive genetic covariances across the traits and that the additive genetic variance-covariance matrix $\\mathbf{G}$ is symmetric and positive definite. Natural selection acts on the traits such that the linear selection gradient (the vector of partial regression coefficients of relative fitness on traits) is $\\boldsymbol{\\beta} \\in \\mathbb{R}^{3}$. Work within the infinitesimal model of quantitative genetics under weak selection, and assume that the multivariate breeder’s equation holds across a single generation.\n\nConsider the following empirically motivated matrices and vectors:\n- The additive genetic variance-covariance matrix\n$$\n\\mathbf{G} \\;=\\; \\begin{pmatrix}\n3  2  0\\\\\n2  3  0\\\\\n0  0  2\n\\end{pmatrix},\n$$\nwith entries measured on the scale of the standardized traits.\n- The linear selection gradient\n$$\n\\boldsymbol{\\beta} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\n\nTasks:\n1. Using a well-tested result from multivariate quantitative genetics that relates the change in mean phenotype to the additive genetic variance-covariance matrix and the linear selection gradient, derive the one-generation selection response vector $\\Delta \\bar{\\mathbf{z}}$ and compute its components explicitly.\n2. Compute the eigensystem of $\\mathbf{G}$ by finding its eigenvalues and an orthonormal set of eigenvectors. Decompose the response $\\Delta \\bar{\\mathbf{z}}$ as a linear combination of these orthonormal eigenvectors, and write down the coefficients of this expansion. Briefly interpret how the off-diagonal structure of $\\mathbf{G}$ reflects correlational selection history and how it shapes the alignment between the response and the principal axes of $\\mathbf{G}$.\n3. Evolvability in the direction of selection is defined here as the scalar $e \\equiv \\boldsymbol{\\beta}^{\\mathsf T}\\mathbf{G}\\,\\boldsymbol{\\beta}$. Compute $e$ exactly.\n\nReport only the final scalar $e$ as your answer. Express your final answer in exact form (no rounding) and treat it as unitless.", "solution": "The problem as stated is scientifically grounded, well-posed, and internally consistent. It constitutes a standard application of multivariate quantitative genetics theory. All necessary data are provided, and the questions are unambiguous. We may proceed directly to the solution.\n\nThe problem requires the completion of three tasks, culminating in the calculation of a scalar quantity termed evolvability. We will address each task in sequence.\n\nFirst, we must derive the one-generation selection response vector, denoted $\\Delta \\bar{\\mathbf{z}}$. The problem specifies the use of the multivariate breeder’s equation, a fundamental result in evolutionary quantitative genetics, which takes the form:\n$$ \\Delta \\bar{\\mathbf{z}} = \\mathbf{G}\\boldsymbol{\\beta} $$\nwhere $\\mathbf{G}$ is the additive genetic variance-covariance matrix and $\\boldsymbol{\\beta}$ is the linear selection gradient. We are given the following matrices:\n$$\n\\mathbf{G} \\;=\\; \\begin{pmatrix}\n3  2  0\\\\\n2  3  0\\\\\n0  0  2\n\\end{pmatrix},\n\\quad\n\\boldsymbol{\\beta} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n$$\nSubstituting these into the breeder's equation and performing the matrix-vector multiplication, we obtain:\n$$\n\\Delta \\bar{\\mathbf{z}} = \\begin{pmatrix}\n3  2  0\\\\\n2  3  0\\\\\n0  0  2\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix} =\n\\begin{pmatrix}\n3(1) + 2(-1) + 0(2) \\\\\n2(1) + 3(-1) + 0(2) \\\\\n0(1) + 0(-1) + 2(2)\n\\end{pmatrix} =\n\\begin{pmatrix}\n3 - 2 \\\\\n2 - 3 \\\\\n4\n\\end{pmatrix} =\n\\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix}.\n$$\nThis vector, $\\Delta \\bar{\\mathbf{z}}$, represents the predicted change in the mean values of the $3$ traits after one generation of selection.\n\nSecond, we are tasked with computing the eigensystem of $\\mathbf{G}$ and decomposing the response vector $\\Delta \\bar{\\mathbf{z}}$ in this basis. The eigenvalues $\\lambda$ of $\\mathbf{G}$ are the roots of the characteristic equation $\\det(\\mathbf{G} - \\lambda\\mathbf{I}) = 0$. The block-diagonal structure of $\\mathbf{G}$ simplifies this calculation. One eigenvalue corresponds to the third trait, which is genetically uncorrelated with the first two; from the matrix, it is immediately evident that $\\lambda_3 = 2$. The other two eigenvalues are found from the upper-left $2 \\times 2$ submatrix:\n$$\n\\det \\begin{pmatrix} 3-\\lambda  2 \\\\ 2  3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 4 = \\lambda^2 - 6\\lambda + 5 = (\\lambda-5)(\\lambda-1) = 0.\n$$\nThis gives the eigenvalues $\\lambda_1 = 5$ and $\\lambda_2 = 1$. The complete set of eigenvalues is thus $\\{5, 1, 2\\}$. We now find the corresponding orthonormal eigenvectors.\nFor $\\lambda_1 = 5$, solving $(\\mathbf{G} - 5\\mathbf{I})\\mathbf{v} = \\mathbf{0}$ gives the normalized eigenvector $\\mathbf{v}_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nFor $\\lambda_2 = 1$, solving $(\\mathbf{G} - 1\\mathbf{I})\\mathbf{v} = \\mathbf{0}$ gives the normalized eigenvector $\\mathbf{v}_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\nFor $\\lambda_3 = 2$, the eigenvector corresponds to the third trait axis, giving $\\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nTo decompose $\\Delta \\bar{\\mathbf{z}}$ as $\\Delta \\bar{\\mathbf{z}} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + c_3\\mathbf{v}_3$, we find the coefficients $c_i = (\\Delta \\bar{\\mathbf{z}})^{\\mathsf T}\\mathbf{v}_i$ via projection:\n$$ c_1 = \\begin{pmatrix} 1  -1  4 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}}(1-1) = 0 $$\n$$ c_2 = \\begin{pmatrix} 1  -1  4 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}}(1+1) = \\sqrt{2} $$\n$$ c_3 = \\begin{pmatrix} 1  -1  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = 4 $$\nThe coefficients of the expansion are $c_1=0$, $c_2=\\sqrt{2}$, and $c_3=4$. The off-diagonal term $G_{12}=2$ indicates a positive genetic correlation between traits $1$ and $2$. This forces the principal axes of variation, $\\mathbf{v}_1$ and $\\mathbf{v}_2$, to be mixtures of these traits. The response $\\Delta \\bar{\\mathbf{z}}$ is biased by the genetic architecture; while selection favors changes in all traits, the response has no component along $\\mathbf{v}_1$ (the direction of greatest genetic variance, $\\lambda_1=5$) because the selection gradient $\\boldsymbol{\\beta}$ is orthogonal to this direction. This demonstrates how genetic correlations can channel evolutionary change away from the direction of selection.\n\nThird, we compute the evolvability, $e$, defined as $e \\equiv \\boldsymbol{\\beta}^{\\mathsf T}\\mathbf{G}\\,\\boldsymbol{\\beta}$. This quantity is a quadratic form that can be computed by substituting the result from our first step, $\\mathbf{G}\\boldsymbol{\\beta}=\\Delta \\bar{\\mathbf{z}}$:\n$$ e = \\boldsymbol{\\beta}^{\\mathsf T} ( \\mathbf{G}\\boldsymbol{\\beta} ) = \\boldsymbol{\\beta}^{\\mathsf T} \\Delta \\bar{\\mathbf{z}} $$\nUsing the given vector $\\boldsymbol{\\beta}$ and the calculated vector $\\Delta \\bar{\\mathbf{z}}$:\n$$ e = \\begin{pmatrix} 1  -1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 4 \\end{pmatrix} $$\nThe scalar product is:\n$$ e = (1)(1) + (-1)(-1) + (2)(4) = 1 + 1 + 8 = 10 $$\nThe evolvability in the direction of selection is exactly $10$. This is the final value requested.", "answer": "$$\\boxed{10}$$", "id": "2737209"}]}