{"hands_on_practices": [{"introduction": "Understanding the handicap principle begins with a firm grasp of the underlying mathematical models. This first practice focuses on deriving a separating equilibrium from first principles, as pioneered by Alan Grafen. By working through this exercise [@problem_id:2726702], you will see how an honest signal, where signal intensity $s$ directly reflects an individual's quality $q$, can emerge as a stable outcome when signaling is costly, and the marginal costs of signaling are lower for higher-quality individuals.", "problem": "In the Grafen-style indicator model of signaling in evolutionary biology, individual quality is represented by a continuous trait $q \\in [\\underline{q}, \\overline{q}]$ with $\\underline{q} > 0$. An individual of quality $q$ chooses a nonnegative signal $s \\in \\mathbb{R}_{+}$ to influence a receiver’s response. The individual’s fitness payoff is $U(s,q) = B(s) - c(s,q)$, where the benefit is linear in the signal, $B(s) = a s$ with $a > 0$, and the signaling cost decreases with quality and is convex in the signal, $c(s,q) = k s^{2}/q$ with $k > 0$. Assume differentiability and interior choices of $s$ for all $q \\in [\\underline{q}, \\overline{q}]$.\n\nConsider a separating equilibrium in which types choose distinct signals according to a differentiable function $s^{*}(q)$ that is strictly increasing in $q$ and satisfies incentive compatibility: for all $q$ and all $\\tilde{q}$ in $[\\underline{q}, \\overline{q}]$, $U(s^{*}(q), q) \\geq U(s^{*}(\\tilde{q}), q)$. Starting only from these primitives and the definitions of incentive compatibility and separating equilibrium in signaling games, derive the closed-form expression for the separating equilibrium signal $s^{*}(q)$ that maximizes $U(s,q)$ subject to incentive compatibility. Express your final answer as a function of $a$, $k$, and $q$. No rounding is required, and no units are involved.", "solution": "The problem is valid. It presents a well-defined mathematical problem in the context of evolutionary signaling theory, based on established models and principles. All necessary information is provided, the terms are clear, and the setup is scientifically and mathematically consistent.\n\nThe objective is to derive the separating equilibrium signal function, $s^{*}(q)$, for an individual of quality $q \\in [\\underline{q}, \\overline{q}]$. The individual's fitness payoff is given by $U(s,q) = B(s) - c(s,q)$, where the benefit is $B(s) = as$ and the cost is $c(s,q) = \\frac{k s^{2}}{q}$. The parameters $a$ and $k$ are positive constants. The signal choice is $s \\in \\mathbb{R}_{+}$.\n\nA separating equilibrium is characterized by an injective signal function $s^{*}(q)$ where different types $q$ choose different signals. The problem states that $s^{*}(q)$ is differentiable and strictly increasing, which means $\\frac{ds^{*}}{dq} > 0$. The fundamental condition for such an equilibrium is incentive compatibility (IC), which states that an individual of any type $q$ must achieve a higher (or at least equal) fitness by sending its true signal $s^{*}(q)$ compared to sending any other signal $s^{*}(\\tilde{q})$ corresponding to a different type $\\tilde{q}$. Mathematically:\n$$ U(s^{*}(q), q) \\geq U(s^{*}(\\tilde{q}), q) \\quad \\forall q, \\tilde{q} \\in [\\underline{q}, \\overline{q}] $$\nThis condition implies that for any given type $q$, the function $f(\\tilde{q}) = U(s^{*}(\\tilde{q}), q)$ must have a global maximum at $\\tilde{q} = q$. Since we are given that $s^{*}(q)$ is differentiable and choices are interior, we can analyze the local behavior of this function using calculus. The first-order necessary condition for a maximum at $\\tilde{q} = q$ is that the derivative of $f(\\tilde{q})$ with respect to $\\tilde{q}$, evaluated at $\\tilde{q} = q$, must be zero.\n$$ \\frac{d}{d\\tilde{q}} U(s^{*}(\\tilde{q}), q) \\Big|_{\\tilde{q}=q} = 0 $$\nWe apply the chain rule to differentiate $U(s^{*}(\\tilde{q}), q)$ with respect to $\\tilde{q}$:\n$$ \\frac{d}{d\\tilde{q}} U(s^{*}(\\tilde{q}), q) = \\frac{\\partial U(s,q)}{\\partial s}\\Bigg|_{s=s^{*}(\\tilde{q})} \\cdot \\frac{ds^{*}(\\tilde{q})}{d\\tilde{q}} $$\nEvaluating this derivative at $\\tilde{q} = q$ gives the first-order condition:\n$$ \\frac{\\partial U(s,q)}{\\partial s}\\Bigg|_{s=s^{*}(q)} \\cdot \\frac{ds^{*}(q)}{dq} = 0 $$\nThe problem specifies a separating equilibrium where $s^{*}(q)$ is strictly increasing. This implies that the derivative $\\frac{ds^{*}(q)}{dq}$ is strictly positive. Since the product of two terms is zero and one term is strictly positive, the other term must be zero. This gives the crucial local incentive compatibility condition:\n$$ \\frac{\\partial U(s,q)}{\\partial s}\\Bigg|_{s=s^{*}(q)} = 0 $$\nThis condition states that in equilibrium, each type $q$ must choose a signal level $s^*(q)$ that locally maximizes its own fitness function. We now compute this partial derivative for the given fitness function $U(s,q) = as - \\frac{ks^{2}}{q}$:\n$$ \\frac{\\partial U}{\\partial s} = \\frac{\\partial}{\\partial s} \\left( as - \\frac{ks^{2}}{q} \\right) = a - \\frac{2ks}{q} $$\nSetting this derivative to zero at the equilibrium signal $s = s^{*}(q)$:\n$$ a - \\frac{2ks^{*}(q)}{q} = 0 $$\nThis is an algebraic equation for the function $s^{*}(q)$. We solve for $s^{*}(q)$:\n$$ a = \\frac{2ks^{*}(q)}{q} $$\n$$ s^{*}(q) = \\frac{aq}{2k} $$\nThis expression gives the candidate separating equilibrium signal as a function of quality $q$ and the a priori parameters $a$ and $k$. We must verify that this solution is consistent with the initial assumptions. The derivative with respect to $q$ is $\\frac{ds^{*}}{dq} = \\frac{a}{2k}$. Since $a > 0$ and $k > 0$, this derivative is positive, confirming that the signal function is strictly increasing, which is required for a separating equilibrium. Furthermore, the global incentive compatibility is also satisfied. The utility function $U(s,q)$ for a fixed $q$ is a downward-opening parabola in $s$, with its unique maximum at $s = \\frac{aq}{2k}$. Therefore, $U\\left(\\frac{aq}{2k}, q\\right) \\ge U(s, q)$ for any other signal $s$, including any $s = s^{*}(\\tilde{q}) = \\frac{a\\tilde{q}}{2k}$. Thus, the incentive compatibility condition holds globally.\n\nThe derived function $s^{*}(q) = \\frac{aq}{2k}$ is the unique differentiable separating equilibrium for this model.", "answer": "$$\\boxed{\\frac{aq}{2k}}$$", "id": "2726702"}, {"introduction": "While foundational models often assume perfect information transmission, real biological communication is almost always subject to noise. This exercise [@problem_id:2726628] challenges you to extend the basic signaling framework by incorporating perceptual error, where a receiver observes a noisy version of the true signal. You will derive the modified incentive compatibility conditions, revealing how the strategic calculations of signalers must adapt when the link between their signal and the receiver's perception is no longer deterministic.", "problem": "Consider a costly signaling scenario consistent with the handicap principle. A population consists of two unobserved quality types, high-quality and low-quality, denoted by $q \\in \\{H,L\\}$, occurring with equal frequency. An individual of type $q$ chooses a signal level $s \\in \\mathbb{R}$ prior to being evaluated by a receiver. The cost of producing signal $s$ for type $q$ is $c_{q}(s) = \\frac{a_{q}}{2}s^{2}$ with $a_{L} > a_{H} > 0$. If accepted by the receiver, the individual obtains benefit $v > 0$; otherwise, the benefit is $0$. The receiver does not observe $s$ directly, but instead observes a noisy perception $s' = s + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ and is independent of type and action. The receiver adopts a threshold decision rule based on $s'$ that maximizes the expected fraction of correct classifications under equal priors and homoscedastic Gaussian noise, accepting if and only if $s' \\geq T$ for some threshold $T \\in \\mathbb{R}$.\n\nSuppose a candidate separating profile $(s_{H},s_{L})$ with $s_{H} > s_{L}$ is under consideration. Using only fundamental definitions from signaling theory (expected fitness as benefit minus cost) and the statistical properties of the Gaussian distribution, derive the modified (noise-adjusted) incentive compatibility conditions for separation in terms of $(s_{H},s_{L},\\sigma,a_{H},a_{L},v)$. Then, treating those conditions as constraints, determine the exact symbolic expression for the minimal benefit $v$ that just makes the high-quality type’s incentive compatibility constraint bind at this separating profile. Your final answer must be a single closed-form expression in terms of $(s_{H},s_{L},\\sigma,a_{H})$. No numerical approximation is required and no units are involved.", "solution": "The problem requires a derivation of noise-adjusted incentive compatibility conditions for a signaling game and the subsequent calculation of a minimal benefit value. A rigorous validation of the problem statement is the mandatory first step.\n\nProblem Validation:\n\nStep 1: Extract Givens\n-   Population quality types: $q \\in \\{H, L\\}$.\n-   Frequency of types: $P(H) = P(L) = \\frac{1}{2}$.\n-   Signal level chosen by individual: $s \\in \\mathbb{R}$.\n-   Cost function for type $q$: $c_{q}(s) = \\frac{a_{q}}{2}s^{2}$.\n-   Cost parameters relation: $a_{L} > a_{H} > 0$.\n-   Benefit upon acceptance: $v > 0$.\n-   Benefit upon rejection: $0$.\n-   Receiver's observed signal: $s' = s + \\epsilon$.\n-   Noise distribution: $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$.\n-   Receiver's decision rule: Accept if $s' \\geq T$ for some threshold $T \\in \\mathbb{R}$.\n-   Receiver's objective: Maximize the expected fraction of correct classifications.\n-   Candidate separating profile: $(s_{H}, s_{L})$ with $s_{H} > s_{L}$.\n\nStep 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is a standard, well-established model in evolutionary biology and economics, specifically within signaling theory. It formalizes the handicap principle under conditions of noisy perception. The assumptions, including quadratic costs and Gaussian noise, are canonical in the literature. It is scientifically sound.\n-   **Well-Posed:** The problem is clearly defined. It provides all necessary components: two types, a continuous action space, state-dependent costs, benefits, an information structure (noisy channel), and an explicit objective for the receiver. The tasks are specific and lead to a unique mathematical derivation.\n-   **Objective:** The problem is phrased using precise, unambiguous mathematical terminology. There is no subjectivity or opinion.\n\nThe problem does not violate any of the invalidity criteria. It is not scientifically unsound, non-formalizable, incomplete, unrealistic for a theoretical model, ill-posed, or trivial.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nSolution Derivation:\n\nThe solution proceeds in four stages:\n1.  Determine the receiver's optimal decision threshold $T$.\n2.  Define the fitness function for senders.\n3.  Derive the noise-adjusted incentive compatibility (IC) conditions.\n4.  Solve for the minimal benefit $v$ that makes the high-quality type's IC constraint bind.\n\nFirst, we determine the optimal threshold $T$ for the receiver. The receiver's goal is to maximize the probability of correct classification. Given equal priors $P(H) = P(L) = \\frac{1}{2}$, and a separating profile where type $H$ sends $s_H$ and type $L$ sends $s_L$, the probability of a correct classification is:\n$$ P(\\text{correct}) = P(\\text{classify H} | \\text{is H})P(H) + P(\\text{classify L} | \\text{is L})P(L) $$\n$$ P(\\text{correct}) = \\frac{1}{2} P(s' \\geq T | s=s_H) + \\frac{1}{2} P(s'  T | s=s_L) $$\nIf the sender of type $q$ chooses signal $s_q$, the receiver's observation $s'$ is a random variable with distribution $\\mathcal{N}(s_q, \\sigma^2)$. Let $f_H(x)$ and $f_L(x)$ be the probability density functions (PDFs) for the observed signal $s'$ conditional on the sender being type $H$ or $L$, respectively. The probability of correct classification is:\n$$ P(\\text{correct}) = \\frac{1}{2} \\int_{T}^{\\infty} f_H(x) dx + \\frac{1}{2} \\int_{-\\infty}^{T} f_L(x) dx $$\nTo maximize this with respect to $T$, we differentiate and set the derivative to zero:\n$$ \\frac{d P(\\text{correct})}{dT} = \\frac{1}{2} \\left( -f_H(T) + f_L(T) \\right) = 0 $$\nThis implies $f_H(T) = f_L(T)$. Substituting the Gaussian PDFs:\n$$ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(T-s_H)^2}{2\\sigma^2}\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(T-s_L)^2}{2\\sigma^2}\\right) $$\nTaking the natural logarithm of both sides and simplifying yields $(T-s_H)^2 = (T-s_L)^2$. Since $s_H \\neq s_L$, the only non-trivial solution is $T-s_H = -(T-s_L)$, which solves to:\n$$ T = \\frac{s_H + s_L}{2} $$\nThe optimal threshold is the midpoint between the two signals.\n\nSecond, we define the fitness $W_q(s)$ for a type $q$ individual choosing signal $s$. Fitness is expected benefit minus cost. The benefit $v$ is obtained only upon acceptance.\n$$ W_q(s) = v \\cdot P(\\text{acceptance} | s) - c_q(s) $$\nThe probability of acceptance for a signal $s$ is $P(s' \\geq T) = P(s+\\epsilon \\geq T) = P(\\epsilon \\geq T-s)$. Let $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$. Since $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, we can write $\\epsilon = \\sigma Z$ where $Z \\sim \\mathcal{N}(0,1)$.\n$$ P(\\text{acceptance} | s) = P\\left(Z \\geq \\frac{T-s}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{T-s}{\\sigma}\\right) $$\nSubstituting $T = \\frac{s_H + s_L}{2}$ and $c_q(s) = \\frac{a_q}{2}s^2$, the fitness function becomes:\n$$ W_q(s) = v \\left[ 1 - \\Phi\\left(\\frac{s_H + s_L - 2s}{2\\sigma}\\right) \\right] - \\frac{a_q}{2}s^2 $$\n\nThird, we derive the incentive compatibility (IC) conditions for the separating profile $(s_H, s_L)$. Each type must prefer its own signal to mimicking the other.\nThe IC condition for the high-quality type (IC-H) is $W_H(s_H) \\geq W_H(s_L)$:\n$$ v \\left(1 - \\Phi\\left(\\frac{s_L - s_H}{2\\sigma}\\right)\\right) - \\frac{a_H}{2}s_H^2 \\geq v \\left(1 - \\Phi\\left(\\frac{s_H - s_L}{2\\sigma}\\right)\\right) - \\frac{a_H}{2}s_L^2 $$\nLet $z = \\frac{s_H - s_L}{2\\sigma}$. Since $s_H > s_L$, $z > 0$. The inequality is:\n$$ v \\left(1 - \\Phi(-z)\\right) - \\frac{a_H}{2}s_H^2 \\geq v \\left(1 - \\Phi(z)\\right) - \\frac{a_H}{2}s_L^2 $$\nUsing the property $\\Phi(-z) = 1 - \\Phi(z)$:\n$$ v \\Phi(z) - \\frac{a_H}{2}s_H^2 \\geq v(1 - \\Phi(z)) - \\frac{a_H}{2}s_L^2 $$\n$$ v (2\\Phi(z) - 1) \\geq \\frac{a_H}{2}(s_H^2 - s_L^2) \\quad (\\text{IC-H})$$\n\nThe IC condition for the low-quality type (IC-L) is $W_L(s_L) \\geq W_L(s_H)$:\n$$ v \\left(1 - \\Phi(z)\\right) - \\frac{a_L}{2}s_L^2 \\geq v \\left(1 - \\Phi(-z)\\right) - \\frac{a_L}{2}s_H^2 $$\n$$ v (1 - \\Phi(z)) - \\frac{a_L}{2}s_L^2 \\geq v \\Phi(z) - \\frac{a_L}{2}s_H^2 $$\n$$ \\frac{a_L}{2}(s_H^2 - s_L^2) \\geq v (2\\Phi(z) - 1) \\quad (\\text{IC-L})$$\nThese are the two noise-adjusted incentive compatibility conditions.\n\nFourth, we find the minimal benefit $v$ that makes the high-quality type's IC constraint bind. A binding constraint is one where the inequality holds with equality. We set the IC-H condition to be an equality:\n$$ v \\left(2\\Phi\\left(\\frac{s_H - s_L}{2\\sigma}\\right) - 1\\right) = \\frac{a_H}{2}(s_H^2 - s_L^2) $$\nSince $s_H > s_L$ and $\\sigma > 0$, the argument of $\\Phi$ is positive, so $\\Phi(\\cdot) > \\frac{1}{2}$, and the term $(2\\Phi(\\cdot) - 1)$ is strictly positive. We can divide by it to solve for $v$:\n$$ v = \\frac{\\frac{a_H}{2}(s_H^2 - s_L^2)}{2\\Phi\\left(\\frac{s_H - s_L}{2\\sigma}\\right) - 1} $$\nThis expression can be written as:\n$$ v = \\frac{a_H(s_H^2 - s_L^2)}{2 \\left(2\\Phi\\left(\\frac{s_H - s_L}{2\\sigma}\\right) - 1\\right)} $$\nThis is the minimal benefit $v$ required for the high-quality type to be willing to bear the cost of the higher signal $s_H$ rather than mimic the low-quality type by signaling $s_L$, given the noisy observation channel. This is the desired final expression in terms of the specified parameters.", "answer": "$$\n\\boxed{\\frac{a_{H}(s_{H}^{2} - s_{L}^{2})}{2 \\left(2\\Phi\\left(\\frac{s_{H} - s_{L}}{2\\sigma}\\right) - 1\\right)}}\n$$", "id": "2726628"}, {"introduction": "The ultimate test of a theoretical model is its ability to explain empirical data. This final practice moves from theory to application, guiding you through a common task in modern evolutionary biology: model comparison [@problem_id:2726630]. You will use the Akaike Information Criterion (AIC) to determine whether a 'separating' model (where signals are informative) or a 'pooling' model (where they are not) provides a better fit to hypothetical datasets, developing crucial skills in computational and statistical analysis for testing evolutionary hypotheses.", "problem": "Consider a population in which individuals of latent genetic quality emit a costly signal. Under the indicator-mechanism perspective relevant to the handicap principle, high-quality individuals can afford higher signals. We observe data pairs $(s_i, q_i)$ for $i \\in \\{1,\\dots,n\\}$, where $s_i \\in \\mathbb{R}$ is a scalar signal magnitude and $q_i \\in \\{0,1\\}$ is an observed quality label ($q_i = 1$ for high quality, $q_i = 0$ for low quality). You are to perform likelihood-based model comparison between two signaling models:\n\n- Separating model: signals are informative of quality, modeled as conditionally Gaussian with quality-specific means and a shared variance. Formally, $s_i \\mid q_i = j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$ with $j \\in \\{0,1\\}$.\n- Pooling model: signals are uninformative of quality, modeled as a single Gaussian distribution irrespective of quality. Formally, $s_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ independently of $q_i$.\n\nYour task is to implement maximum likelihood estimation and compare the models using Akaike Information Criterion (AIC).\n\nFundamental definitions to be used:\n- The Gaussian (normal) density for $x \\in \\mathbb{R}$ is $f(x \\mid \\mu, \\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\!\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right)$ for $\\sigma^2 > 0$.\n- The log-likelihood for independent observations is the sum of the log-densities.\n- The maximum likelihood estimators (derived by setting derivatives of the log-likelihood to zero) under the pooling model are the sample mean $\\hat{\\mu} = \\dfrac{1}{n} \\sum_{i=1}^n s_i$ and the variance $\\hat{\\sigma}^2 = \\dfrac{1}{n} \\sum_{i=1}^n (s_i - \\hat{\\mu})^2$. Under the separating model, the means are group-specific sample means $\\hat{\\mu}_j = \\dfrac{1}{n_j} \\sum_{i: q_i = j} s_i$ for $j \\in \\{0,1\\}$, where $n_j = \\sum_{i=1}^n \\mathbb{I}[q_i = j]$, and the shared variance is $\\hat{\\sigma}^2 = \\dfrac{1}{n} \\sum_{j \\in \\{0,1\\}} \\sum_{i: q_i = j} (s_i - \\hat{\\mu}_j)^2$.\n- The maximized log-likelihood for a Gaussian model with unknown mean(s) and variance (evaluated at the maximum likelihood estimates) can be expressed as\n$$\n\\log L = -\\dfrac{n}{2}\\left( \\log(2\\pi) + 1 + \\log(\\hat{\\sigma}^2) \\right),\n$$\nwhere $\\hat{\\sigma}^2$ is the corresponding maximum likelihood variance ($\\dfrac{1}{n}$ times the sum of squared residuals).\n- The Akaike Information Criterion is $\\mathrm{AIC} = 2k - 2 \\log L$, where $k$ is the number of free parameters. For the pooling model, $k = 2$ (one mean and one variance). For the separating model with two means and one shared variance, $k = 3$.\n\nImportant implementation details:\n- If one quality class is absent (i.e., $n_j = 0$ for some $j$), treat the separating model as identical to the pooling model for that dataset with $k = 2$ and $\\hat{\\mu}$ equal to the overall sample mean, because the absent-class mean is not identifiable.\n- To ensure numerical stability, if a computed $\\hat{\\sigma}^2$ equals $0$, replace it by $\\varepsilon = 10^{-12}$ before evaluating the log-likelihood.\n- When comparing AIC values, if they are equal within an absolute tolerance $\\tau = 10^{-9}$, break ties in favor of the pooling model.\n\nInput specification for testing:\nYou will hard-code the following three datasets (each is a pair of aligned arrays $(\\mathbf{s}, \\mathbf{q})$) directly into your program.\n\n- Dataset $\\#1$ (a case favoring separating): $n = 10$, with aligned observations\n  - $\\mathbf{q} = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$,\n  - $\\mathbf{s} = [\\,9.8,\\,10.5,\\,10.2,\\,11.0,\\,9.7,\\,4.8,\\,5.1,\\,5.5,\\,6.0,\\,4.9\\,]$.\n- Dataset $\\#2$ (a case favoring pooling): $n = 10$, with aligned observations\n  - $\\mathbf{q} = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$,\n  - $\\mathbf{s} = [\\,6.8,\\,7.1,\\,7.0,\\,7.3,\\,6.9,\\,7.2,\\,6.7,\\,7.4,\\,6.8,\\,7.0\\,]$.\n- Dataset $\\#3$ (an imbalanced edge case): $n = 10$, with aligned observations\n  - $\\mathbf{q} = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,0\\,]$,\n  - $\\mathbf{s} = [\\,7.9,\\,8.1,\\,8.2,\\,8.0,\\,7.8,\\,8.3,\\,8.1,\\,8.0,\\,7.7,\\,8.05\\,]$.\n\nTask:\n- For each dataset, compute the maximum likelihood estimates under both models, compute the corresponding maximized log-likelihoods and AIC values, and decide which model better fits the data by the AIC rule (lower AIC is better; break ties within tolerance $\\tau$ in favor of pooling).\n- Express the decision for each dataset as an integer: output $1$ if the separating model is better and $0$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the decisions for the three datasets as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$. Each result must be an integer ($0$ or $1$).", "solution": "The problem requires a quantitative comparison of two statistical models for signaling behavior in a biological population, based on the principle of maximum likelihood and the Akaike Information Criterion (AIC). The problem is self-contained, scientifically grounded in statistical theory, and well-posed. We shall proceed with the solution.\n\nThe core of the task is to evaluate, for each provided dataset, which of two models provides a more parsimonious fit to the observed data pairs $(s_i, q_i)$, where $s_i$ is a continuous signal and $q_i$ is a binary quality label.\n\nThe two competing models are:\n1.  **Pooling Model**: This model assumes the signal $s_i$ is uninformative of quality $q_i$. The signals are modeled as independent draws from a single Gaussian distribution, $s_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$. This model has $k_{\\text{pool}} = 2$ free parameters: the mean $\\mu$ and the variance $\\sigma^2$.\n\n2.  **Separating Model**: This model, consistent with the handicap principle, assumes the signal is informative. Signals are drawn from one of two Gaussian distributions, conditional on quality, $s_i \\mid q_i=j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$ for $j \\in \\{0, 1\\}$. Critically, the variance $\\sigma^2$ is assumed to be shared across the two quality groups. This model has $k_{\\text{sep}} = 3$ free parameters: the two means $\\mu_0$ and $\\mu_1$, and the shared variance $\\sigma^2$.\n\nFor each model and dataset, we first find the maximum likelihood estimates (MLEs) for its parameters. The problem provides the standard closed-form expressions for these MLEs.\n\nFor the **pooling model**, given a sample of $n$ signals $\\{s_1, \\dots, s_n\\}$:\n-   The MLE for the mean is the overall sample mean:\n    $$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} s_i $$\n-   The MLE for the variance is the sample variance (with $n$ in the denominator):\n    $$ \\hat{\\sigma}^2_{\\text{pool}} = \\frac{1}{n} \\sum_{i=1}^{n} (s_i - \\hat{\\mu})^2 $$\n\nFor the **separating model**, we first partition the data into two subsamples based on the quality label $q_i$. Let $n_j = \\sum_{i=1}^{n} \\mathbb{I}[q_i=j]$ be the number of individuals in group $j \\in \\{0, 1\\}$.\n-   The MLEs for the group-specific means are the subsample means:\n    $$ \\hat{\\mu}_j = \\frac{1}{n_j} \\sum_{i: q_i=j} s_i \\quad \\text{for } j \\in \\{0, 1\\} $$\n-   The MLE for the shared variance is the pooled variance, calculated as the total sum of squared residuals divided by the total sample size $n$:\n    $$ \\hat{\\sigma}^2_{\\text{sep}} = \\frac{1}{n} \\left( \\sum_{i: q_i=0} (s_i - \\hat{\\mu}_0)^2 + \\sum_{i: q_i=1} (s_i - \\hat{\\mu}_1)^2 \\right) $$\n\nOnce the MLE for the variance, $\\hat{\\sigma}^2$, is computed for a given model, the maximized log-likelihood, $\\log L$, is calculated using the provided formula, which applies to Gaussian models where both mean(s) and variance are estimated:\n$$ \\log L = -\\frac{n}{2} \\left( \\log(2\\pi) + 1 + \\log(\\hat{\\sigma}^2) \\right) $$\nA numerical stability precaution is taken: if any calculated $\\hat{\\sigma}^2$ is zero, it is replaced with $\\varepsilon = 10^{-12}$ before taking the logarithm.\n\nWith the maximized log-likelihood $\\log L$ and the number of parameters $k$ for each model, we compute the AIC:\n$$ \\mathrm{AIC} = 2k - 2 \\log L $$\nThe model with the lower AIC value is considered a better fit to the data, as it provides a better balance between goodness-of-fit (high $\\log L$) and model complexity (low $k$).\n\nA crucial edge case arises if a dataset contains observations from only one quality class (i.e., $n_0 = 0$ or $n_1 = 0$). In this situation, the parameters for the absent class ($\\mu_j$) are not identifiable. The separating model cannot be distinguished from the pooling model. As per the problem specification, we treat the separating model as identical to the pooling model: we assign it the same log-likelihood, and notably, the same number of parameters, $k_{\\text{sep}} = 2$.\n\nThe final decision rule is as follows: the separating model is chosen (output $1$) if its AIC is strictly smaller than the pooling model's AIC, considering a numerical tolerance $\\tau = 10^{-9}$. That is, if $\\mathrm{AIC}_{\\text{sep}}  \\mathrm{AIC}_{\\text{pool}} - \\tau$. Otherwise, the pooling model is chosen (output $0$), which includes the case of near-equal AIC values.\n\nThe algorithm proceeds as follows for each dataset:\n1.  Compute the parameters $k_{\\text{pool}}$, $\\hat{\\sigma}^2_{\\text{pool}}$, $\\log L_{\\text{pool}}$, and $\\mathrm{AIC}_{\\text{pool}}$ for the pooling model.\n2.  Check for the class imbalance edge case. If one class is absent, set $k_{\\text{sep}} = k_{\\text{pool}}$ and $\\mathrm{AIC}_{\\text{sep}} = \\mathrm{AIC}_{\\text{pool}}$.\n3.  If both classes are present, compute the parameters $k_{\\text{sep}}$, $\\hat{\\sigma}^2_{\\text{sep}}$, $\\log L_{\\text{sep}}$, and $\\mathrm{AIC}_{\\text{sep}}$ for the separating model.\n4.  Compare $\\mathrm{AIC}_{\\text{sep}}$ and $\\mathrm{AIC}_{\\text{pool}}$ using the specified tolerance and decision rule to determine the result ($0$ or $1$).\nThis procedure is applied to all three datasets to produce the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for the three specified datasets.\n    \"\"\"\n    \n    # Define the datasets as per the problem statement.\n    test_cases = [\n        # Dataset #1: Expected to favor the separating model.\n        {\n            \"s\": np.array([9.8, 10.5, 10.2, 11.0, 9.7, 4.8, 5.1, 5.5, 6.0, 4.9]),\n            \"q\": np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n        },\n        # Dataset #2: Expected to favor the pooling model.\n        {\n            \"s\": np.array([6.8, 7.1, 7.0, 7.3, 6.9, 7.2, 6.7, 7.4, 6.8, 7.0]),\n            \"q\": np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n        },\n        # Dataset #3: Imbalanced edge case.\n        {\n            \"s\": np.array([7.9, 8.1, 8.2, 8.0, 7.8, 8.3, 8.1, 8.0, 7.7, 8.05]),\n            \"q\": np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        s_data = case[\"s\"]\n        q_data = case[\"q\"]\n        result = _compare_models(s_data, q_data)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _calculate_aic(n, k, sigma2_hat):\n    \"\"\"\n    Calculates the Akaike Information Criterion (AIC).\n\n    Args:\n        n (int): Number of observations.\n        k (int): Number of free parameters in the model.\n        sigma2_hat (float): Maximum likelihood estimate of the variance.\n\n    Returns:\n        float: The calculated AIC value.\n    \"\"\"\n    # Numerical stability constant as specified.\n    epsilon = 1e-12\n    if sigma2_hat == 0.0:\n        sigma2_hat = epsilon\n\n    # Maximized log-likelihood for a Gaussian model.\n    # logL = -n/2 * (log(2*pi) + 1 + log(sigma^2_hat))\n    logL = -n / 2.0 * (np.log(2 * np.pi) + 1.0 + np.log(sigma2_hat))\n    \n    # AIC = 2k - 2*logL\n    aic = 2 * k - 2 * logL\n    return aic\n\ndef _compare_models(s, q):\n    \"\"\"\n    Performs AIC-based model comparison between a pooling and a separating model.\n\n    Args:\n        s (np.ndarray): Array of signal magnitudes.\n        q (np.ndarray): Array of quality labels (0 or 1).\n\n    Returns:\n        int: 1 if the separating model is better, 0 otherwise.\n    \"\"\"\n    n = len(s)\n    tau = 1e-9  # Tie-breaking tolerance\n\n    # --- Pooling Model ---\n    # Parameters: mu, sigma^2 (k=2)\n    k_pool = 2\n    # MLE for variance is the population variance (ddof=0).\n    sigma2_hat_pool = np.var(s)\n    aic_pool = _calculate_aic(n, k_pool, sigma2_hat_pool)\n\n    # --- Separating Model ---\n    # Parameters: mu_0, mu_1, sigma^2 (k=3)\n    s_group0 = s[q == 0]\n    s_group1 = s[q == 1]\n    \n    n0 = len(s_group0)\n    n1 = len(s_group1)\n\n    # Handle edge case where one class is absent.\n    if n0 == 0 or n1 == 0:\n        # Separating model is identical to pooling model.\n        k_sep = 2\n        aic_sep = _calculate_aic(n, k_sep, sigma2_hat_pool)\n    else:\n        # Standard case with both classes present.\n        k_sep = 3\n        \n        # MLEs for group means.\n        mu0_hat = np.mean(s_group0)\n        mu1_hat = np.mean(s_group1)\n        \n        # Sum of squared residuals for each group.\n        ssr0 = np.sum((s_group0 - mu0_hat)**2)\n        ssr1 = np.sum((s_group1 - mu1_hat)**2)\n        \n        # MLE for shared variance.\n        sigma2_hat_sep = (ssr0 + ssr1) / n\n        aic_sep = _calculate_aic(n, k_sep, sigma2_hat_sep)\n\n    # --- Comparison ---\n    # Choose separating model if its AIC is strictly lower, considering tolerance.\n    if aic_sep  aic_pool - tau:\n        return 1\n    else: # Favor pooling model in case of tie or if it's better.\n        return 0\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2726630"}]}