{"hands_on_practices": [{"introduction": "A primary goal in scanning genomes for selection is to characterize the nature of an adaptive event. A selective sweep dramatically reshapes the haplotype frequency spectrum, but different modes of selection leave distinct signatures. This exercise provides practice in calculating fundamental haplotype-based statistics and using them to interpret the evidence for a classic hard sweep versus a soft sweep, where selection acts on standing variation [@problem_id:2750177].", "problem": "A genomic window in a panmictic population has been sequenced at high coverage, and phased haplotypes have been called for a sample of $n=200$ chromosomes. Within this window, the three most frequent haplotypes and the remainder of the haplotype spectrum are as follows (counts in chromosomes):\n- Haplotype $h_{A}$: $80$\n- Haplotype $h_{B}$: $60$\n- Haplotype $h_{C}$: $30$\n- All other observed haplotypes: $30$ distinct singletons (each with count $1$)\n\nStarting only from the fundamental definition that haplotype homozygosity in a window is the probability that two independent random draws (with replacement) from the sample yield the same haplotype, and that merging classes corresponds to summing their frequencies before computing this probability, do the following:\n- Derive an expression for $H_{1}$ in terms of the sample haplotype frequencies $\\{p_{i}\\}$.\n- Derive an expression for $H_{12}$ by first merging the two most frequent haplotypes into a single class and then computing homozygosity as defined above.\n- Derive an expression for $H_{2}/H_{1}$, where $H_{2}$ is the contribution to $H_{1}$ from all haplotypes except the single most frequent one.\n\nThen compute $H_{1}$, $H_{12}$, and $H_{2}/H_{1}$ for the data given above. Express your final numerical answers as exact fractions. Do not round. Provide your final answer as a row matrix $\\left(H_{1},\\,H_{12},\\,H_{2}/H_{1}\\right)$ in that order.\n\nFinally, based on these values and without appealing to any external thresholds, explain whether the evidence in this window is more consistent with a recent hard sweep or a soft sweep, justifying your reasoning from first principles about how hard and soft sweeps affect the haplotype frequency spectrum. Your interpretation will not be graded numerically, but it must be logically consistent with the computed values.", "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. All necessary data and definitions are provided, and there are no internal contradictions. The total number of chromosomes from the provided counts is $80 + 60 + 30 + 30 \\times 1 = 200$, which matches the given sample size $n=200$. Therefore, I shall proceed with the derivation and calculation.\n\nThe problem requires the derivation and calculation of three quantities related to haplotype homozygosity from a sample of $n=200$ chromosomes. The observed counts are: $c_{A} = 80$ for haplotype $h_{A}$, $c_{B} = 60$ for $h_{B}$, $c_{C} = 30$ for $h_{C}$, and $c_{s_j} = 1$ for each of $30$ distinct singleton haplotypes, $j=1, \\dots, 30$.\n\nThe corresponding sample frequencies are:\n$p_{A} = \\frac{c_{A}}{n} = \\frac{80}{200} = \\frac{2}{5}$\n$p_{B} = \\frac{c_{B}}{n} = \\frac{60}{200} = \\frac{3}{10}$\n$p_{C} = \\frac{c_{C}}{n} = \\frac{30}{200} = \\frac{3}{20}$\n$p_{s_j} = \\frac{c_{s_j}}{n} = \\frac{1}{200}$ for $j=1, \\dots, 30$.\nThe set of all distinct haplotypes is $\\{h_i\\}$, with frequencies $\\{p_i\\}$.\n\nFirst, we derive an expression for the total haplotype homozygosity, $H_{1}$. By definition, this is the probability that two independent random draws from the sample yield the same haplotype. Let the set of distinct haplotypes be indexed by $i$. The probability of drawing haplotype $h_i$ is its frequency, $p_i$. Since the two draws are independent, the probability of drawing $h_i$ twice in a row is $p_i \\times p_i = p_i^2$. The events of drawing each specific haplotype pair (e.g., two $h_A$'s, or two $h_B$'s) are mutually exclusive. Therefore, the total probability of drawing any identical pair is the sum of the probabilities for all possible haplotypes.\n$$H_{1} = \\sum_{i} p_i^2$$\nThis is the general expression for haplotype homozygosity.\n\nUsing the given data, we compute $H_{1}$:\n$$H_{1} = p_{A}^2 + p_{B}^2 + p_{C}^2 + \\sum_{j=1}^{30} p_{s_j}^2$$\n$$H_{1} = \\left(\\frac{80}{200}\\right)^2 + \\left(\\frac{60}{200}\\right)^2 + \\left(\\frac{30}{200}\\right)^2 + 30 \\times \\left(\\frac{1}{200}\\right)^2$$\nIt is more direct to work with the counts and the total sample size squared in the denominator:\n$$H_{1} = \\frac{\\sum_{i} c_i^2}{n^2} = \\frac{80^2 + 60^2 + 30^2 + 30 \\times 1^2}{200^2}$$\n$$H_{1} = \\frac{6400 + 3600 + 900 + 30}{40000} = \\frac{10930}{40000} = \\frac{1093}{4000}$$\n\nSecond, we derive an expression for $H_{12}$. This quantity is the homozygosity computed after merging the two most frequent haplotypes, $h_A$ and $h_B$, into a single class. Let this new merged class be $h_{AB}$. Its frequency, $p_{AB}$, is the sum of the frequencies of its constituent classes: $p_{AB} = p_A + p_B$. The set of haplotypes is now $\\{h_{AB}, h_C, h_{s_1}, \\dots, h_{s_{30}}\\}$. The homozygosity $H_{12}$ is the sum of squared frequencies for this new set.\n$$H_{12} = p_{AB}^2 + p_C^2 + \\sum_{j=1}^{30} p_{s_j}^2 = (p_A + p_B)^2 + p_C^2 + \\sum_{j=1}^{30} p_{s_j}^2$$\nUsing the data, the count of the merged class is $c_{AB} = c_A + c_B = 80 + 60 = 140$.\n$$H_{12} = \\frac{c_{AB}^2 + c_C^2 + \\sum_{j=1}^{30} c_{s_j}^2}{n^2} = \\frac{(80+60)^2 + 30^2 + 30 \\times 1^2}{200^2}$$\n$$H_{12} = \\frac{140^2 + 30^2 + 30}{40000} = \\frac{19600 + 900 + 30}{40000} = \\frac{20530}{40000} = \\frac{2053}{4000}$$\n\nThird, we derive an expression for the ratio $H_{2}/H_{1}$. $H_{2}$ is defined as the contribution to $H_{1}$ from all haplotypes except the single most frequent one. Let the haplotypes be ordered by frequency, $p_1 \\ge p_2 \\ge \\dots$. The most frequent haplotype is $h_1$ with frequency $p_1$ (in our case, $h_A$ with frequency $p_A$).\nTotal homozygosity is $H_{1} = p_1^2 + p_2^2 + \\dots = \\sum_{i} p_i^2$.\nThe contribution from all other haplotypes is $H_{2} = p_2^2 + p_3^2 + \\dots = \\sum_{i>1} p_i^2$.\nThus, $H_2 = H_1 - p_1^2$. The ratio is:\n$$\\frac{H_{2}}{H_{1}} = \\frac{\\sum_{i>1} p_i^2}{\\sum_{i} p_i^2} = \\frac{H_1 - p_1^2}{H_1} = 1 - \\frac{p_1^2}{H_1}$$\nUsing our data, the most frequent haplotype is $h_A$ with frequency $p_A = 2/5$.\nWe already computed $H_{1} = \\frac{1093}{4000}$.\n$H_{2}$ is the homozygosity contributed by $h_B$, $h_C$, and the singletons:\n$$H_{2} = p_{B}^2 + p_{C}^2 + \\sum_{j=1}^{30} p_{s_j}^2 = \\frac{60^2 + 30^2 + 30 \\times 1^2}{200^2}$$\n$$H_{2} = \\frac{3600 + 900 + 30}{40000} = \\frac{4530}{40000} = \\frac{453}{4000}$$\nThe ratio is therefore:\n$$\\frac{H_{2}}{H_{1}} = \\frac{453/4000}{1093/4000} = \\frac{453}{1093}$$\nThe numerator is $453 = 3 \\times 151$. The denominator $1093$ is not divisible by $3$ (sum of digits is $13$) nor is it divisible by the prime $151$. Thus, the fraction is irreducible.\n\nFinally, we must interpret these results in the context of a hard versus a soft selective sweep.\nA hard sweep occurs when a single, new beneficial mutation arises on a specific haplotype and sweeps to high frequency. This process purges genetic variation at linked sites. The expected signature is one extremely high-frequency haplotype (the sweep carrier) and a collection of very rare, divergent haplotypes. This leads to high overall homozygosity ($H_1$), but this homozygosity is almost entirely due to the dominant haplotype. Consequently, the ratio $H_2/H_1$ should be very close to $0$.\nA soft sweep occurs when selection acts on pre-existing variation (standing variation) or on recurrent mutations, causing multiple haplotypes carrying the beneficial allele(s) to increase in frequency simultaneously. The expected signature is the presence of two or more haplotypes at intermediate to high frequencies. This also results in elevated homozygosity ($H_1$), but a significant fraction of it is contributed by haplotypes other than the most frequent one. Consequently, $H_2/H_1$ will be substantially greater than $0$.\n\nIn our sample, the haplotype frequencies are $p_A = 0.4$, $p_B = 0.3$, and $p_C = 0.15$. The presence of two very common haplotypes, and a third at substantial frequency, is not characteristic of a classic hard sweep.\nOur computed values quantify this observation:\n1. $H_1 = \\frac{1093}{4000} \\approx 0.273$. This value is elevated compared to neutral expectations in many species, consistent with a recent sweep.\n2. $H_2/H_1 = \\frac{453}{1093} \\approx 0.414$. This value is large. It indicates that over $41\\%$ of the total homozygosity is contributed by haplotypes other than the most frequent one. This strongly contradicts the expectation for a hard sweep, where this ratio would be near zero.\n3. $H_{12} = \\frac{2053}{4000} \\approx 0.513$. This value is substantially higher than $H_1$. The large jump in homozygosity upon merging the two most frequent haplotypes ($h_A$ and $h_B$) underscores that both are major contributors to the haplotype frequency spectrum. Their combined frequency is $0.4 + 0.3 = 0.7$. If they were a single haplotype, the pattern would more closely resemble a hard sweep. The fact that they are distinct demonstrates that multiple haplotypes rose in frequency.\n\nConclusion: The presence of at least two high-frequency haplotypes ($h_A$ and $h_B$), quantitatively reflected in a high $H_2/H_1$ ratio and a large increase from $H_1$ to $H_{12}$, provides strong evidence that this genomic region has experienced a soft selective sweep, not a hard sweep.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1093}{4000} & \\frac{2053}{4000} & \\frac{453}{1093} \\end{pmatrix}}$$", "id": "2750177"}, {"introduction": "Beyond inspecting individual statistics, a powerful approach for detecting selection is to formally compare how well different evolutionary models explain the observed data. This practice demonstrates how to use the site frequency spectrum (SFS) from a genomic window to compute a composite likelihood ratio, a cornerstone of modern statistical genomics. By completing this exercise, you will learn how to quantitatively weigh the evidence for a selective sweep model against a neutral baseline model [@problem_id:2750222].", "problem": "A haploid sample of size $n_{\\text{sample}} = 20$ chromosomes is collected from a single genomic window and the unfolded site frequency spectrum (SFS; derived allele counts polarized with an outgroup) is summarized into $K = 5$ disjoint derived-allele-frequency bins that partition the possible non-fixed frequencies:\n- Bin $1$: singleton sites (derived count $=1$),\n- Bin $2$: doubleton sites (derived count $=2$),\n- Bin $3$: low-intermediate sites (derived count $=3$ to $5$),\n- Bin $4$: intermediate sites (derived count $=6$ to $10$),\n- Bin $5$: high-frequency derived sites (derived count $=11$ to $19$).\n\nThe observed counts of segregating sites in each bin for this window are:\n- $n_1 = 100$,\n- $n_2 = 25$,\n- $n_3 = 35$,\n- $n_4 = 20$,\n- $n_5 = 20$.\n\nTwo alternative models assign categorical probabilities over these bins:\n- Neutral baseline model probabilities $q = (q_1,q_2,q_3,q_4,q_5) = (0.35, 0.20, 0.25, 0.15, 0.05)$,\n- Selective sweep model probabilities $p = (p_1,p_2,p_3,p_4,p_5) = (0.45, 0.15, 0.15, 0.10, 0.15)$,\n\nwhere in each model the probabilities sum to $1$. Assume independence of sites within the window conditional on the model (composite likelihood approximation), and that the counts arise from categorical sampling consistent with these bin probabilities.\n\nStarting from first principles of likelihood for categorical counts, derive an explicit expression for the natural-log composite likelihood ratio comparing the sweep model to the neutral model for this window, and then evaluate it numerically using the values above. Clearly state the inputs and the output of the calculation in your derivation. Round your final numeric answer to four significant figures. The answer is dimensionless; do not include any units.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard application of statistical model comparison using a likelihood ratio test, a fundamental technique in computational biology and statistics. All necessary data and parameters are provided, and there are no internal contradictions or logical flaws. We may proceed with the solution.\n\nThe task is to derive and evaluate the natural-log composite likelihood ratio $(\\lambda)$ comparing a selective sweep model ($H_p$) to a neutral baseline model ($H_q$). The inputs are the observed counts of segregating sites in $K$ predefined frequency bins and the categorical probabilities associated with each bin under the two competing models.\n\nLet the vector of observed counts be $n = (n_1, n_2, \\dots, n_K)$, where $n_k$ is the number of sites in bin $k$. For this problem, we have $K=5$ and the counts are $n = (100, 25, 35, 20, 20)$.\nThe total number of observed segregating sites is $N = \\sum_{k=1}^{K} n_k$.\n\nThe two models are defined by their respective probability vectors for a site falling into one of the $K$ bins:\n- Neutral model: $q = (q_1, q_2, \\dots, q_K) = (0.35, 0.20, 0.25, 0.15, 0.05)$\n- Sweep model: $p = (p_1, p_2, \\dots, p_K) = (0.45, 0.15, 0.15, 0.10, 0.15)$\n\nThe problem states to assume independence of sites conditional on the model. This implies that the probability of observing the specific set of counts $n$ is given by the multinomial probability mass function. The likelihood of the data $n$ under a generic model with probability vector $\\theta = (\\theta_1, \\dots, \\theta_K)$ is:\n$$ L(\\theta|n) = \\frac{N!}{n_1! n_2! \\dots n_K!} \\prod_{k=1}^{K} \\theta_k^{n_k} $$\nThe term $\\frac{N!}{n_1! n_2! \\dots n_K!}$ is the multinomial coefficient, which is a constant with respect to the model parameters $\\theta$.\n\nThe likelihood of the data under the sweep model ($H_p$) is:\n$$ L(p|n) = C \\cdot \\prod_{k=1}^{K} p_k^{n_k} $$\nwhere $C = \\frac{N!}{\\prod_{k=1}^{K} n_k!}$.\n\nThe likelihood of the data under the neutral model ($H_q$) is:\n$$ L(q|n) = C \\cdot \\prod_{k=1}^{K} q_k^{n_k} $$\n\nThe likelihood ratio, $\\Lambda$, is the ratio of the likelihood of the alternative model (sweep) to the likelihood of the null model (neutral):\n$$ \\Lambda = \\frac{L(p|n)}{L(q|n)} = \\frac{C \\cdot \\prod_{k=1}^{K} p_k^{n_k}}{C \\cdot \\prod_{k=1}^{K} q_k^{n_k}} $$\nThe constant $C$ cancels, which simplifies the expression significantly. This cancellation is a key feature of the likelihood ratio test for nested or non-nested models on the same data.\n$$ \\Lambda = \\frac{\\prod_{k=1}^{K} p_k^{n_k}}{\\prod_{k=1}^{K} q_k^{n_k}} = \\prod_{k=1}^{K} \\left(\\frac{p_k}{q_k}\\right)^{n_k} $$\nThe problem requires the natural-log composite likelihood ratio, $\\lambda$. This is obtained by taking the natural logarithm of $\\Lambda$.\n$$ \\lambda = \\ln(\\Lambda) = \\ln\\left(\\prod_{k=1}^{K} \\left(\\frac{p_k}{q_k}\\right)^{n_k}\\right) $$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(x^y) = y\\ln(x)$, we derive the explicit expression for $\\lambda$:\n$$ \\lambda = \\sum_{k=1}^{K} \\ln\\left(\\left(\\frac{p_k}{q_k}\\right)^{n_k}\\right) = \\sum_{k=1}^{K} n_k \\ln\\left(\\frac{p_k}{q_k}\\right) $$\nThis is the general expression for the log-likelihood ratio for binned categorical data.\n\nNow, we substitute the provided numerical values into this expression. The inputs are:\n- Counts: $n_1=100$, $n_2=25$, $n_3=35$, $n_4=20$, $n_5=20$.\n- Probabilities $p$: $p_1=0.45$, $p_2=0.15$, $p_3=0.15$, $p_4=0.10$, $p_5=0.15$.\n- Probabilities $q$: $q_1=0.35$, $q_2=0.20$, $q_3=0.25$, $q_4=0.15$, $q_5=0.05$.\n\nThe calculation proceeds as follows:\n$$ \\lambda = n_1 \\ln\\left(\\frac{p_1}{q_1}\\right) + n_2 \\ln\\left(\\frac{p_2}{q_2}\\right) + n_3 \\ln\\left(\\frac{p_3}{q_3}\\right) + n_4 \\ln\\left(\\frac{p_4}{q_4}\\right) + n_5 \\ln\\left(\\frac{p_5}{q_5}\\right) $$\n$$ \\lambda = 100 \\ln\\left(\\frac{0.45}{0.35}\\right) + 25 \\ln\\left(\\frac{0.15}{0.20}\\right) + 35 \\ln\\left(\\frac{0.15}{0.25}\\right) + 20 \\ln\\left(\\frac{0.10}{0.15}\\right) + 20 \\ln\\left(\\frac{0.15}{0.05}\\right) $$\nWe evaluate each term:\n$$ 100 \\ln\\left(\\frac{9}{7}\\right) \\approx 100 \\times 0.2513147 = 25.13147 $$\n$$ 25 \\ln\\left(\\frac{3}{4}\\right) \\approx 25 \\times (-0.2876821) = -7.19205 $$\n$$ 35 \\ln\\left(\\frac{3}{5}\\right) \\approx 35 \\times (-0.5108256) = -17.87890 $$\n$$ 20 \\ln\\left(\\frac{2}{3}\\right) \\approx 20 \\times (-0.4054651) = -8.10930 $$\n$$ 20 \\ln(3) \\approx 20 \\times 1.0986123 = 21.97225 $$\nSumming these individual terms gives the final value for $\\lambda$:\n$$ \\lambda \\approx 25.13147 - 7.19205 - 17.87890 - 8.10930 + 21.97225 $$\n$$ \\lambda \\approx 13.92347 $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\lambda \\approx 13.92 $$\nThe positive value of the log-likelihood ratio indicates that the observed data are more likely under the selective sweep model than under the neutral model.", "answer": "$$\\boxed{13.92}$$", "id": "2750222"}, {"introduction": "Many of the most powerful methods for detecting selection integrate multiple sources of information, such as long-range haplotype integrity and differentiation between populations. This hands-on programming challenge guides you through the implementation of the Cross-Population Extended Haplotype Homozygosity (XP-EHH) statistic, a widely used tool for identifying population-specific selective sweeps. This exercise will build your computational skills by translating a formal definition into a working algorithm to analyze haplotype data [@problem_id:2750224].", "problem": "You are given phased haplotype data for two populations and a set of genetic marker positions along a one-dimensional chromosome. Your task is to compute, for each test case, the Cross-Population Extended Haplotype Homozygosity (XP-EHH) for a specified focal (core) marker and to interpret whether there is evidence for a population-specific selective sweep at that locus. All distances must be treated as genetic distances in centiMorgans (cM), and all integrals must thus be expressed in the unit of cM. The final numeric statistic must be a real number without any unit. Your program must aggregate the results across all test cases and print them in the specified final output format.\n\nDefinitions to be used:\n- Extended Haplotype Homozygosity (EHH): For a population with $N$ chromosomes, and at a given side of the core marker $c$, let the window include all markers from the core to a marker $j$ on that side. Each chromosome yields a multi-locus haplotype string for the markers in that window (inclusive of the core). Let there be $K$ distinct haplotype strings in that window with frequencies $p_1, p_2, \\dots, p_K$ across the $N$ chromosomes, where $\\sum_{k=1}^{K} p_k = 1$. The extended haplotype homozygosity at marker $j$ is\n$$\nEHH(j) = \\sum_{k=1}^{K} p_k^2.\n$$\n- For the right side, the window is from $c$ to $j$ with $j \\ge c$, and the genetic distance is $d_j^{(R)} = x_j - x_c$, where $x_j$ is the position of marker $j$ in cM. For the left side, the window is from $j$ to $c$ with $j \\le c$, and the genetic distance is $d_j^{(L)} = x_c - x_j$. In both sides include the core itself as the point with distance $0$ and $EHH(c)$ computed on the single-site window.\n- Integrated Haplotype Homozygosity (iHH) for a population is defined here as the sum of the trapezoidal integrals of the $EHH$ curve on the left and on the right, each starting at distance $0$ from the core and extending to the most distant provided marker on that side. If a side has sampled distances $0 = d_0^{(s)} < d_1^{(s)} < \\dots < d_{M_s-1}^{(s)}$ with corresponding $EHH$ values $E_0^{(s)}, E_1^{(s)}, \\dots, E_{M_s-1}^{(s)}$, then the side’s integral is\n$$\n\\sum_{m=1}^{M_s-1} \\frac{d_m^{(s)} - d_{m-1}^{(s)}}{2} \\left(E_m^{(s)} + E_{m-1}^{(s)}\\right),\n$$\nand the population’s $iHH$ is the sum of the left and right side integrals. The $iHH$ has units of cM.\n- Cross-Population Extended Haplotype Homozygosity (XP-EHH) between population $A$ and population $B$ at the core is defined as the natural logarithm of the ratio of the two integrated values with a small regularization added to each to avoid division by zero:\n$$\nX = \\ln\\!\\left(\\frac{iHH_A + \\varepsilon}{iHH_B + \\varepsilon}\\right),\n$$\nwhere $\\varepsilon = 10^{-8}$ cM.\n- Interpretation rule: Given a threshold $T > 0$, define a decision code $D$ by\n$$\nD = \\begin{cases}\n1 & \\text{if } X > T \\quad (\\text{evidence for a sweep in population } A),\\\\\n-1 & \\text{if } X < -T \\quad (\\text{evidence for a sweep in population } B),\\\\\n0 & \\text{otherwise} \\quad (\\text{no decision}).\n\\end{cases}\n$$\n\nInput to your program is fixed by the test suite embedded within the code. There is no external input. Each test case consists of:\n- A haplotype matrix for population $A$ of shape $N_A \\times M$ containing only $0$ or $1$, where $N_A$ is the number of chromosomes and $M$ is the number of markers.\n- A haplotype matrix for population $B$ of shape $N_B \\times M$ defined analogously.\n- A strictly increasing array of marker positions $x_0, x_1, \\dots, x_{M-1}$ in cM.\n- An integer index $c$ with $0 \\le c \\le M-1$ indicating the core marker.\n- A strictly positive decision threshold $T$.\n\nYour required outputs for each test case are:\n- The XP-EHH value $X$ rounded to $6$ decimal places (as a real number).\n- The decision code $D$ as an integer in $\\{-1, 0, 1\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must itself be a two-element list of the form $[X,D]$ for a corresponding test case. For example, a valid aggregate output could look like $[[0.123456,1],[0.000000,0],[-0.654321,-1]]$.\n\nTest suite:\n- Case $1$ (happy path, clear elevation in population $A$):\n  - Population $A$ haplotypes ($N_A = 8$, $M = 5$):\n    - $[0,1,1,1,1]$\n    - $[0,1,1,1,1]$\n    - $[0,1,1,1,1]$\n    - $[0,1,1,1,1]$\n    - $[1,1,1,1,1]$\n    - $[1,1,1,1,1]$\n    - $[1,1,1,1,1]$\n    - $[1,1,1,1,1]$\n  - Population $B$ haplotypes ($N_B = 8$, $M = 5$):\n    - $[0,0,1,0,1]$\n    - $[1,0,1,1,0]$\n    - $[0,1,1,0,0]$\n    - $[1,1,1,1,1]$\n    - $[0,0,0,0,0]$\n    - $[1,0,0,1,1]$\n    - $[0,1,0,1,0]$\n    - $[1,1,0,0,1]$\n  - Positions in cM: $[0.0, 0.1, 0.2, 0.3, 0.5]$\n  - Core index: $c = 2$\n  - Threshold: $T = 1.0$\n- Case $2$ (boundary, identical populations, expect near zero):\n  - Population $A$ haplotypes: same as Case $1$ population $A$\n  - Population $B$ haplotypes: same as Case $1$ population $A$\n  - Positions in cM: $[0.0, 0.1, 0.2, 0.3, 0.5]$\n  - Core index: $c = 2$\n  - Threshold: $T = 0.1$\n- Case $3$ (opposite elevation, population $B$ elevated):\n  - Population $A$ haplotypes: same as Case $1$ population $B$\n  - Population $B$ haplotypes: same as Case $1$ population $A$\n  - Positions in cM: $[0.0, 0.1, 0.2, 0.3, 0.5]$\n  - Core index: $c = 2$\n  - Threshold: $T = 1.0$\n- Case $4$ (small sample size, minimal markers):\n  - Population $A$ haplotypes ($N_A = 2$, $M = 3$):\n    - $[1,1,1]$\n    - $[1,1,1]$\n  - Population $B$ haplotypes ($N_B = 2$, $M = 3$):\n    - $[0,1,0]$\n    - $[1,0,1]$\n  - Positions in cM: $[1.0, 1.5, 2.0]$\n  - Core index: $c = 1$\n  - Threshold: $T = 0.6$\n\nImplementation notes:\n- You must implement the definitions exactly as stated. In particular, define $EHH(j)$ using the multi-locus haplotype frequencies within the window that runs from the core to $j$ on the chosen side, include the core point on both sides, and compute the $iHH$ as the sum of the two side integrals via the trapezoidal rule stated above.\n- Use $\\varepsilon = 10^{-8}$ cM for numerical stability only in the final logarithm as specified.\n- Report $X$ rounded to $6$ decimal places and $D$ as an integer.\n- Final output format: a single line printing a bracketed, comma-separated list of per-case results, each itself a two-element list $[X,D]$, for the cases in the order listed above.", "solution": "The problem requires the computation of the Cross-Population Extended Haplotype Homozygosity ($XP-EHH$) statistic, a measure used in population genetics to detect recent positive selection at a specific genetic locus. The core principle is that a selective sweep rapidly increases the frequency of a beneficial allele and, through genetic linkage, the frequency of the haplotype on which it resides. This results in an unusually long, high-frequency haplotype compared to what is expected under neutral evolution. The $XP-EHH$ statistic compares the extent of this haplotype homozygosity between two populations to identify population-specific sweeps. A significantly higher integrated haplotype homozygosity in one population suggests that a selective sweep has occurred in that population but not the other.\n\nThe computational procedure is implemented by adhering to the precise definitions provided in the problem statement. The overall process can be decomposed into four main stages:\n\n1.  **Extended Haplotype Homozygosity ($EHH$) Calculation**:\n    For a given population and a core marker at index $c$, we analyze haplotypes on the left and right sides of the core. For a specific marker $j$ on one side, we consider the multi-locus haplotype spanning all markers from the core $c$ to $j$ (inclusive). Let the number of chromosomes be $N$. We identify all distinct haplotypes $k=1, \\dots, K$ in this window and their corresponding frequencies $p_k$. The $EHH$ at marker $j$ is the sum of the squared frequencies:\n    $$\n    EHH(j) = \\sum_{k=1}^{K} p_k^2\n    $$\n    This value represents the probability that two randomly chosen chromosomes from the population carry the same extended haplotype from the core to marker $j$. An $EHH$ of $1$ signifies that all chromosomes share the same haplotype in that window.\n\n2.  **Integrated Haplotype Homozygosity ($iHH$) Calculation**:\n    The $EHH$ for a given core allele typically decreases as the distance from the core marker increases, because recombination events break down long-range haplotypes over time. We quantify the overall extent of homozygosity by integrating the $EHH$ value with respect to genetic distance.\n    For each side (left, $s=L$, and right, $s=R$), we have a set of marker positions, which are converted to genetic distances from the core: $d_j^{(R)} = x_j - x_c$ for $j \\ge c$, and $d_j^{(L)} = x_c - x_j$ for $j \\le c$. Let the sorted unique distances on a side be $0 = d_0^{(s)} < d_1^{(s)} < \\dots < d_{M_s-1}^{(s)}$ with corresponding $EHH$ values $E_0^{(s)}, E_1^{(s)}, \\dots, E_{M_s-1}^{(s)}$. The area under the $EHH$ curve is calculated using the trapezoidal rule as specified:\n    $$\n    \\text{Integral}^{(s)} = \\sum_{m=1}^{M_s-1} \\frac{d_m^{(s)} - d_{m-1}^{(s)}}{2} \\left(E_m^{(s)} + E_{m-1}^{(s)}\\right)\n    $$\n    The total Integrated Haplotype Homozygosity ($iHH$) for a population is the sum of these integrals from the left and right sides. This value has units of centiMorgans (cM).\n\n3.  **Cross-Population $XP-EHH$ Statistic ($X$) Calculation**:\n    To compare the two populations, A and B, we compute the natural logarithm of the ratio of their respective $iHH$ values. A small regularization constant, $\\varepsilon = 10^{-8}$, is added to both numerator and denominator to ensure numerical stability in cases where an $iHH$ value might be zero. The $XP-EHH$ statistic, $X$, is thus defined as:\n    $$\n    X = \\ln\\!\\left(\\frac{iHH_A + \\varepsilon}{iHH_B + \\varepsilon}\\right)\n    $$\n    A large positive value of $X$ indicates that $iHH_A \\gg iHH_B$, suggesting a selective sweep in population A. A large negative value indicates $iHH_B \\gg iHH_A$, suggesting a sweep in population B. A value near zero suggests similar haplotype homozygosity profiles in both populations.\n\n4.  **Decision Rule**:\n    Finally, a decision is made based on the value of $X$ relative to a given positive threshold $T$. The decision code $D$ is assigned as follows:\n    $$\n    D = \\begin{cases}\n    1 & \\text{if } X > T & (\\text{evidence for a sweep in population } A)\\\\\n    -1 & \\text{if } X < -T & (\\text{evidence for a sweep in population } B)\\\\\n    0 & \\text{otherwise} & (\\text{no decision})\n    \\end{cases}\n    $$\nThe implementation encapsulates these steps. A helper function computes the $iHH$ for a single population. This function iterates through markers on the left and right of the core, computes $EHH$ at each point by counting unique haplotypes in the expanding window, and then uses numerical integration (`numpy.trapz`) to find the areas. The main function calls this helper for both populations, calculates $X$, applies the decision rule to find $D$, and formats the output as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the XP-EHH problem for the given test suite.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"pop_A_haps\": np.array([\n                [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]\n            ]),\n            \"pop_B_haps\": np.array([\n                [0, 0, 1, 0, 1], [1, 0, 1, 1, 0], [0, 1, 1, 0, 0], [1, 1, 1, 1, 1],\n                [0, 0, 0, 0, 0], [1, 0, 0, 1, 1], [0, 1, 0, 1, 0], [1, 1, 0, 0, 1]\n            ]),\n            \"positions\": np.array([0.0, 0.1, 0.2, 0.3, 0.5]),\n            \"core_idx\": 2,\n            \"threshold\": 1.0\n        },\n        {\n            \"pop_A_haps\": np.array([\n                [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]\n            ]),\n            \"pop_B_haps\": np.array([\n                [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]\n            ]),\n            \"positions\": np.array([0.0, 0.1, 0.2, 0.3, 0.5]),\n            \"core_idx\": 2,\n            \"threshold\": 0.1\n        },\n        {\n            \"pop_A_haps\": np.array([\n                [0, 0, 1, 0, 1], [1, 0, 1, 1, 0], [0, 1, 1, 0, 0], [1, 1, 1, 1, 1],\n                [0, 0, 0, 0, 0], [1, 0, 0, 1, 1], [0, 1, 0, 1, 0], [1, 1, 0, 0, 1]\n            ]),\n            \"pop_B_haps\": np.array([\n                [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1], [0, 1, 1, 1, 1],\n                [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]\n            ]),\n            \"positions\": np.array([0.0, 0.1, 0.2, 0.3, 0.5]),\n            \"core_idx\": 2,\n            \"threshold\": 1.0\n        },\n        {\n            \"pop_A_haps\": np.array([\n                [1, 1, 1], [1, 1, 1]\n            ]),\n            \"pop_B_haps\": np.array([\n                [0, 1, 0], [1, 0, 1]\n            ]),\n            \"positions\": np.array([1.0, 1.5, 2.0]),\n            \"core_idx\": 1,\n            \"threshold\": 0.6\n        }\n    ]\n\n    def _calculate_ehh(hap_slice):\n        \"\"\"Helper to calculate EHH for a set of haplotypes (a slice of the main matrix).\"\"\"\n        n_chrom = hap_slice.shape[0]\n        if n_chrom == 0:\n            return 1.0\n        \n        _, counts = np.unique(hap_slice, axis=0, return_counts=True)\n        frequencies = counts / n_chrom\n        ehh = np.sum(frequencies**2)\n        return ehh\n\n    def _calculate_ihh(haplotypes, positions, core_idx):\n        \"\"\"Calculates iHH for a single population.\"\"\"\n        n_markers = haplotypes.shape[1]\n        core_pos = positions[core_idx]\n\n        # --- Right side calculation ---\n        right_indices = range(core_idx, n_markers)\n        right_distances = np.array([positions[j] - core_pos for j in right_indices])\n        right_ehh_values = np.array([_calculate_ehh(haplotypes[:, core_idx:j+1]) for j in right_indices])\n        \n        integral_right = 0.0\n        if len(right_distances) > 1:\n            integral_right = np.trapz(right_ehh_values, right_distances)\n\n        # --- Left side calculation ---\n        left_indices = range(core_idx, -1, -1)\n        left_distances = np.array([core_pos - positions[j] for j in left_indices])\n        left_ehh_values = np.array([_calculate_ehh(haplotypes[:, j:core_idx+1]) for j in left_indices])\n        \n        integral_left = 0.0\n        if len(left_distances) > 1:\n            integral_left = np.trapz(left_ehh_values, left_distances)\n\n        return integral_left + integral_right\n\n    results = []\n    epsilon = 1e-8\n\n    for case in test_cases:\n        hap_A = case[\"pop_A_haps\"]\n        hap_B = case[\"pop_B_haps\"]\n        positions = case[\"positions\"]\n        c = case[\"core_idx\"]\n        T = case[\"threshold\"]\n\n        ihh_A = _calculate_ihh(hap_A, positions, c)\n        ihh_B = _calculate_ihh(hap_B, positions, c)\n\n        X = np.log((ihh_A + epsilon) / (ihh_B + epsilon))\n\n        if X > T:\n            D = 1\n        elif X < -T:\n            D = -1\n        else:\n            D = 0\n        \n        results.append(f\"[{X:.6f},{D}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2750224"}]}