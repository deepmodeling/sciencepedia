{"hands_on_practices": [{"introduction": "Estimating population parameters from finite samples is a cornerstone of evolutionary biology, but it comes with statistical challenges. A naive 'plug-in' approach, where sample frequencies are used directly in a formula, can lead to biased results. This exercise [@problem_id:2690203] guides you through a formal derivation to quantify the bias in a common heterozygosity estimator and construct a corrected, unbiased version, providing a crucial lesson in statistical rigor.", "problem": "A single locus has $G$ distinguishable genotypes in a large, panmictic population with true genotype frequencies $\\{f_{g}\\}_{g=1}^{G}$, where $\\sum_{g=1}^{G} f_{g} = 1$ and $f_{g} \\ge 0$ for all $g$. The population genotype heterozygosity is defined as $H \\equiv 1 - \\sum_{g=1}^{G} f_{g}^{2}$, the probability that two independently drawn individuals have different genotypes. A sample of size $N \\ge 2$ individuals is drawn independently from the population, yielding counts $\\{n_{g}\\}_{g=1}^{G}$ with $\\sum_{g=1}^{G} n_{g} = N$ and empirical frequencies $\\hat{f}_{g} \\equiv n_{g}/N$. Consider the plug-in estimator $\\hat{H} \\equiv 1 - \\sum_{g=1}^{G} \\hat{f}_{g}^{2}$.\n\nStarting only from the definitions of expectation and independence under multinomial sampling, derive the exact bias $\\operatorname{Bias}(\\hat{H}) \\equiv \\mathbb{E}[\\hat{H}] - H$ as a function of $N$ and the true frequencies $\\{f_{g}\\}$. Then construct an unbiased estimator $\\tilde{H}$ for finite $N$ that is a function of the observed empirical frequencies $\\{\\hat{f}_{g}\\}$ and $N$, and verify its unbiasedness from first principles.\n\nYour final answer must be a single symbolic expression containing both the bias and the unbiased estimator as a single row matrix, with the first entry equal to the bias and the second entry equal to the unbiased estimator, expressed in terms of $N$, $\\{f_{g}\\}$, and $\\{\\hat{f}_{g}\\}$. Do not include any words in the final answer. If you choose to present $\\tilde{H}$ in multiple equivalent forms, give the version expressed in terms of $N$ and $\\{\\hat{f}_{g}\\}$. No numerical rounding is required.", "solution": "The problem statement has been evaluated and is determined to be valid. It is a well-posed, scientifically grounded problem in statistical population genetics, free of ambiguity or contradiction. We may proceed with the solution.\n\nThe problem requires the derivation of the bias of a plug-in estimator for genotype heterozygosity and the construction of a corresponding unbiased estimator. Let us proceed from first principles.\n\nThe sample counts $\\{n_{g}\\}_{g=1}^{G}$ for a sample of size $N$ drawn with replacement from a population with genotype frequencies $\\{f_{g}\\}_{g=1}^{G}$ follow a multinomial distribution, $(n_1, \\dots, n_G) \\sim \\operatorname{Multinomial}(N; f_1, \\dots, f_G)$. For any specific genotype $g$, the count $n_g$ follows a binomial distribution, $n_g \\sim \\operatorname{Binomial}(N, f_g)$. The moments of $n_g$ are therefore known:\nThe expectation is $\\mathbb{E}[n_g] = N f_g$.\nThe variance is $\\operatorname{Var}(n_g) = N f_g (1 - f_g)$.\n\nThe variance is also defined as $\\operatorname{Var}(n_g) = \\mathbb{E}[n_g^2] - (\\mathbb{E}[n_g])^2$. From this, we can find the expectation of $n_g^2$:\n$$ \\mathbb{E}[n_g^2] = \\operatorname{Var}(n_g) + (\\mathbb{E}[n_g])^2 = N f_g (1 - f_g) + (N f_g)^2 = N f_g - N f_g^2 + N^2 f_g^2 $$\n\nThe plug-in estimator for heterozygosity is $\\hat{H} = 1 - \\sum_{g=1}^{G} \\hat{f}_g^2$, where $\\hat{f}_g = n_g/N$. We compute its expectation, $\\mathbb{E}[\\hat{H}]$.\n$$ \\mathbb{E}[\\hat{H}] = \\mathbb{E}\\left[1 - \\sum_{g=1}^{G} \\left(\\frac{n_g}{N}\\right)^2\\right] = 1 - \\frac{1}{N^2} \\mathbb{E}\\left[\\sum_{g=1}^{G} n_g^2\\right] $$\nBy linearity of expectation, this becomes:\n$$ \\mathbb{E}[\\hat{H}] = 1 - \\frac{1}{N^2} \\sum_{g=1}^{G} \\mathbb{E}[n_g^2] $$\nSubstituting the expression for $\\mathbb{E}[n_g^2]$ derived above:\n$$ \\sum_{g=1}^{G} \\mathbb{E}[n_g^2] = \\sum_{g=1}^{G} (N f_g - N f_g^2 + N^2 f_g^2) = N \\sum_{g=1}^{G} f_g - N \\sum_{g=1}^{G} f_g^2 + N^2 \\sum_{g=1}^{G} f_g^2 $$\nGiven the constraint $\\sum_{g=1}^{G} f_g = 1$, this simplifies to:\n$$ \\sum_{g=1}^{G} \\mathbb{E}[n_g^2] = N(1) - N \\sum_{g=1}^{G} f_g^2 + N^2 \\sum_{g=1}^{G} f_g^2 = N + (N^2 - N) \\sum_{g=1}^{G} f_g^2 $$\nNow, we substitute this back into the expression for $\\mathbb{E}[\\hat{H}]$:\n$$ \\mathbb{E}[\\hat{H}] = 1 - \\frac{1}{N^2} \\left[ N + (N^2 - N) \\sum_{g=1}^{G} f_g^2 \\right] = 1 - \\frac{N}{N^2} - \\frac{N^2 - N}{N^2} \\sum_{g=1}^{G} f_g^2 $$\n$$ \\mathbb{E}[\\hat{H}] = 1 - \\frac{1}{N} - \\frac{N-1}{N} \\sum_{g=1}^{G} f_g^2 $$\nThe true heterozygosity is $H = 1 - \\sum_{g=1}^{G} f_g^2$, which implies $\\sum_{g=1}^{G} f_g^2 = 1 - H$. Substituting this:\n$$ \\mathbb{E}[\\hat{H}] = 1 - \\frac{1}{N} - \\frac{N-1}{N} (1 - H) = 1 - \\frac{1}{N} - \\left(\\frac{N-1}{N} - \\frac{N-1}{N}H\\right) $$\n$$ \\mathbb{E}[\\hat{H}] = \\frac{N - 1 - (N-1)}{N} + \\frac{N-1}{N}H = \\frac{N-1-N+1}{N} + \\frac{N-1}{N}H = \\frac{N-1}{N}H $$\nThe expectation of the estimator is $\\mathbb{E}[\\hat{H}] = \\frac{N-1}{N}H$.\n\nThe bias of the estimator $\\hat{H}$ is defined as $\\operatorname{Bias}(\\hat{H}) = \\mathbb{E}[\\hat{H}] - H$.\n$$ \\operatorname{Bias}(\\hat{H}) = \\frac{N-1}{N}H - H = \\left(\\frac{N-1}{N} - 1\\right)H = -\\frac{1}{N}H $$\nAs a function of $N$ and the true frequencies $\\{f_g\\}$, the bias is:\n$$ \\operatorname{Bias}(\\hat{H}) = -\\frac{1}{N}\\left(1 - \\sum_{g=1}^{G} f_g^2\\right) $$\nThe estimator $\\hat{H}$ is biased, systematically underestimating the true heterozygosity $H$.\n\nTo construct an unbiased estimator $\\tilde{H}$, we must correct for this bias. From the relation $\\mathbb{E}[\\hat{H}] = \\frac{N-1}{N}H$, it is clear that we can define an unbiased estimator $\\tilde{H}$ by scaling $\\hat{H}$. Let\n$$ \\tilde{H} = \\frac{N}{N-1}\\hat{H} $$\nThe expectation of this new estimator is:\n$$ \\mathbb{E}[\\tilde{H}] = \\mathbb{E}\\left[\\frac{N}{N-1}\\hat{H}\\right] = \\frac{N}{N-1}\\mathbb{E}[\\hat{H}] = \\frac{N}{N-1}\\left(\\frac{N-1}{N}H\\right) = H $$\nThus, $\\tilde{H}$ is an unbiased estimator for $H$. Expressed as a function of the empirical frequencies $\\{\\hat{f}_g\\}$ and sample size $N$, it is:\n$$ \\tilde{H} = \\frac{N}{N-1}\\left(1 - \\sum_{g=1}^{G} \\hat{f}_g^2\\right) $$\n\nFinally, we verify its unbiasedness from first principles, as requested.\n$$ \\tilde{H} = \\frac{N}{N-1}\\left(1 - \\sum_{g=1}^{G} \\frac{n_g^2}{N^2}\\right) = \\frac{N}{N-1} - \\frac{N}{N-1} \\frac{1}{N^2} \\sum_{g=1}^{G} n_g^2 = \\frac{N}{N-1} - \\frac{1}{N(N-1)}\\sum_{g=1}^{G} n_g^2 $$\nTaking the expectation:\n$$ \\mathbb{E}[\\tilde{H}] = \\frac{N}{N-1} - \\frac{1}{N(N-1)}\\mathbb{E}\\left[\\sum_{g=1}^{G} n_g^2\\right] = \\frac{N}{N-1} - \\frac{1}{N(N-1)}\\sum_{g=1}^{G}\\mathbb{E}[n_g^2] $$\nUsing our prior result $\\sum_{g=1}^{G} \\mathbb{E}[n_g^2] = N + N(N-1) \\sum_{g=1}^{G} f_g^2$:\n$$ \\mathbb{E}[\\tilde{H}] = \\frac{N}{N-1} - \\frac{1}{N(N-1)}\\left[N + N(N-1)\\sum_{g=1}^{G} f_g^2\\right] $$\n$$ \\mathbb{E}[\\tilde{H}] = \\frac{N}{N-1} - \\frac{N}{N(N-1)} - \\frac{N(N-1)}{N(N-1)}\\sum_{g=1}^{G} f_g^2 $$\n$$ \\mathbb{E}[\\tilde{H}] = \\frac{N}{N-1} - \\frac{1}{N-1} - \\sum_{g=1}^{G} f_g^2 = \\frac{N-1}{N-1} - \\sum_{g=1}^{G} f_g^2 $$\n$$ \\mathbb{E}[\\tilde{H}] = 1 - \\sum_{g=1}^{G} f_g^2 = H $$\nThe verification is complete. The estimator $\\tilde{H}$ is indeed unbiased for any finite sample size $N \\ge 2$.", "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{1}{N} \\left(1 - \\sum_{g=1}^{G} f_{g}^{2}\\right)  \\frac{N}{N-1} \\left(1 - \\sum_{g=1}^{G} \\hat{f}_{g}^{2}\\right) \\end{pmatrix}}$$", "id": "2690203"}, {"introduction": "A key challenge in population genetics is distinguishing between biological processes and technical artifacts that can produce similar patterns in data. An observed deficit of heterozygotes, for example, could be due to inbreeding or to systematic genotyping errors. In this practice [@problem_id:2690173], you will mathematically explore the conditions under which these two distinct mechanisms become statistically indistinguishable, highlighting the critical concept of model identifiability.", "problem": "A diploid population is sampled at a single biallelic locus with alleles $A$ and $a$. Let the population allele frequency of $A$ be $p \\in (0,1)$ and write $q=1-p$. Under random union of gametes without inbreeding or error, genotype frequencies at zygote formation follow Hardy–Weinberg equilibrium: $AA$ at $p^2$, $Aa$ at $2pq$, and $aa$ at $q^2$. Consider two distinct mechanisms, each of which can generate an observed heterozygote deficit relative to this baseline.\n\nMechanism I (biological): Inbreeding with inbreeding coefficient $F \\in [0,1]$, where $F$ is defined as the probability that the two alleles at a randomly chosen locus in a randomly chosen individual are identical by descent (autozygous). Use the identity-by-descent decomposition implied by this definition to derive the genotype frequency distribution under Mechanism I as a function of $p$, $q$, and $F$.\n\nMechanism II (technical): Genotyping error with no inbreeding. True genotypes are formed by random union of gametes, but each true heterozygote $Aa$ is reported as a homozygote with probability $e \\in [0,1]$, and when such a misclassification occurs it is $AA$ or $aa$ with equal probability; true homozygotes are always reported correctly. Use this misclassification process to derive the observed genotype frequency distribution under Mechanism II as a function of $p$, $q$, and $e$.\n\nAssume no other deviations from random mating or other error processes. Based solely on genotype frequency data from a single locus (with unknown $p$), analyze the identifiability of inbreeding versus genotyping error. Which of the following statements is correct?\n\nA. For all $p \\in (0,1)$, if heterozygote misclassification is symmetric and affects only heterozygotes at rate $e$, then the observed genotype frequency distribution under Mechanism II is identical to that under Mechanism I when $e=F$; thus, from single-locus genotype frequencies alone, the two mechanisms are indistinguishable under this condition.\n\nB. A lack of identifiability occurs only at $p=\\tfrac{1}{2}$; for $p \\neq \\tfrac{1}{2}$, comparisons of $AA$ versus $aa$ deficits allow one to distinguish inbreeding from symmetric heterozygote misclassification.\n\nC. If heterozygote misclassification is symmetric in the sense that $Aa$ is misclassified to $AA$ with probability proportional to $p$ and to $aa$ with probability proportional to $q$, then for any fixed $e \\in (0,1)$ and $F \\in (0,1)$ there exists an equality of genotype frequency distributions for all $p$.\n\nD. With data from multiple unlinked loci sharing a common inbreeding coefficient $F$ but with locus-specific symmetric misclassification rates $e_\\ell$, the mechanisms remain indistinguishable so long as the mean of the $e_\\ell$ equals $F$.\n\nE. Even under symmetric heterozygote misclassification at rate $e$ affecting only heterozygotes, the allele frequency inferred from observed genotype counts is biased unless homozygotes are also misclassified as heterozygotes at rate $e$; therefore, equality with inbreeding cannot hold without additional homozygote errors.", "solution": "Begin from Hardy–Weinberg equilibrium as the baseline for random union of gametes without inbreeding or error. At a biallelic locus with allele frequency $p$ for $A$ and $q=1-p$ for $a$, the zygote genotype frequencies are $P(AA)=p^2$, $P(Aa)=2pq$, $P(aa)=q^2$.\n\nMechanism I (inbreeding with inbreeding coefficient $F$): By definition, $F$ is the probability that the two alleles within a diploid individual at the focal locus are identical by descent. Decompose the population into two classes: with probability $F$, an individual is autozygous; with probability $1-F$, an individual is allozygous (two alleles are not identical by descent) and hence formed by random union of gametes. In the autozygous class, individuals must be homozygous, and conditional on allele frequency $p$, the probability of being $AA$ is $p$ and $aa$ is $q$; in the allozygous class, genotype frequencies follow Hardy–Weinberg equilibrium. Therefore,\n$$\n\\begin{aligned}\nP_{\\text{I}}(AA) = F\\cdot p + (1-F)\\cdot p^2 \\;=\\; p^2 + F\\,p(1-p) \\;=\\; p^2 + F\\,pq,\\\\\nP_{\\text{I}}(Aa) = F\\cdot 0 + (1-F)\\cdot 2pq \\;=\\; 2pq(1-F),\\\\\nP_{\\text{I}}(aa) = F\\cdot q + (1-F)\\cdot q^2 \\;=\\; q^2 + F\\,pq.\n\\end{aligned}\n$$\n\nMechanism II (symmetric heterozygote misclassification at rate $e$ with no inbreeding): True genotypes are $AA$ at $p^2$, $Aa$ at $2pq$, and $aa$ at $q^2$. The error process acts only on true heterozygotes $Aa$, which are reported as homozygotes with probability $e$, going to $AA$ with probability $\\tfrac{e}{2}$ and to $aa$ with probability $\\tfrac{e}{2}$; otherwise, with probability $1-e$, a true heterozygote is reported correctly as $Aa$. True homozygotes are always called correctly. Consequently, the observed genotype frequencies are\n$$\n\\begin{aligned}\nP_{\\text{II}}(AA) = p^2 \\;+\\; \\tfrac{e}{2}\\cdot 2pq \\;=\\; p^2 + e\\,pq,\\\\\nP_{\\text{II}}(Aa) = (1-e)\\cdot 2pq \\;=\\; 2pq(1-e),\\\\\nP_{\\text{II}}(aa) = q^2 \\;+\\; \\tfrac{e}{2}\\cdot 2pq \\;=\\; q^2 + e\\,pq.\n\\end{aligned}\n$$\n\nIdentifiability analysis: Comparing $P_{\\text{I}}$ and $P_{\\text{II}}$, we see immediately that for all $p \\in (0,1)$,\n$$\nP_{\\text{I}}(AA)=P_{\\text{II}}(AA),\\quad P_{\\text{I}}(Aa)=P_{\\text{II}}(Aa),\\quad P_{\\text{I}}(aa)=P_{\\text{II}}(aa)\n$$\nif and only if $F=e$. Under that equality, the two mechanisms produce exactly the same observed genotype frequency distribution at every $p$, hence they are statistically unidentifiable from single-locus genotype frequency data alone. Note also that under Mechanism II, the allele frequency inferred from observed genotype counts is\n$$\n\\hat p_{\\text{obs}} \\;=\\; P_{\\text{II}}(AA) + \\tfrac{1}{2} P_{\\text{II}}(Aa) \\;=\\; \\big(p^2 + e\\,pq\\big) + \\tfrac{1}{2}\\cdot 2pq(1-e) \\;=\\; p^2 + pq \\;=\\; p,\n$$\nso the symmetric heterozygote-only misclassification preserves the allele frequency estimate, matching the allele frequency under inbreeding as well.\n\nWe now evaluate each option:\n\nA. This states precisely the condition derived above: with symmetric misclassification affecting only heterozygotes at rate $e$, the Mechanism II genotype frequencies match those under inbreeding with coefficient $F$ when $e=F$, for all $p \\in (0,1)$. Therefore, the two mechanisms are indistinguishable from single-locus genotype frequency data alone under this condition. Verdict — Correct.\n\nB. This claims identifiability failure occurs only at $p=\\tfrac{1}{2}$. The derivation shows the equality of distributions holds for every $p \\in (0,1)$ when $e=F$; there is no special role for $p=\\tfrac{1}{2}$ in this symmetric model. Verdict — Incorrect.\n\nC. Here, “symmetric” is redefined to mean the misclassification of $Aa$ to $AA$ occurs with probability proportional to $p$ and to $aa$ with probability proportional to $q$. In that model, if a fraction $e$ of heterozygotes are misclassified, then $P_{\\text{II}}(AA)=p^2 + 2ep^2 q$ and $P_{\\text{II}}(aa)=q^2 + 2e p q^2$, while $P_{\\text{II}}(Aa)=2pq(1-e)$. To match inbreeding, we would require $p^2 + 2e p^2 q = p^2 + F pq$ and $q^2 + 2e p q^2 = q^2 + F pq$ for all $p$. The first equality implies $2e p = F$ for all $p$, which cannot hold unless $F=0$ and $e=0$. Thus, for fixed nonzero $e$ and $F$, distributions do not match for all $p$. Verdict — Incorrect.\n\nD. With multiple loci sharing a common $F$, Mechanism I yields heterozygote frequency $2p_\\ell q_\\ell (1-F)$ at each locus $\\ell$. Mechanism II with locus-specific $e_\\ell$ yields $2p_\\ell q_\\ell (1-e_\\ell)$ at locus $\\ell$. Equality of genotype distributions at each locus requires $e_\\ell=F$ for every $\\ell$. Matching only the mean of $e_\\ell$ to $F$ is insufficient, because deviations at any locus would produce mismatched genotype frequencies at that locus, detectable with adequate sample size. Verdict — Incorrect.\n\nE. The symmetric heterozygote-only misclassification preserves the allele frequency estimate, as shown by $\\hat p_{\\text{obs}}=p$. There is no requirement to misclassify homozygotes as heterozygotes to preserve allele frequency or to achieve equality with inbreeding; equality holds with $e=F$ even when homozygotes are always called correctly. Verdict — Incorrect.\n\nTherefore, only option A is correct under the stated assumptions, and it identifies the precise condition under which heterozygote deficits cannot distinguish inbreeding from genotyping error at a single locus.", "answer": "$$\\boxed{A}$$", "id": "2690173"}, {"introduction": "Real-world populations are often structured, with allele frequencies varying across different groups or demes. Hierarchical Bayesian models provide a powerful framework for analyzing such data, allowing us to estimate both local and global parameters while 'borrowing strength' across samples. This advanced practice [@problem_id:2690165] involves constructing a complete Beta-Binomial hierarchical model, deriving the relevant posteriors, and implementing a numerical solution to synthesize information across multiple populations.", "problem": "You are modeling a single biallelic locus across multiple subpopulations (demes) in a metapopulation. Let the allele of interest be denoted by allele $A$. In deme $d$, let the true allele frequency be $\\theta_d \\in (0,1)$ and let the observed allele count be $x_d$ out of $n_d$ sampled gene copies (assuming haploid counts or diploid counts aggregated into gene copy counts), so that $x_d \\in \\{0,1,\\dots,n_d\\}$. The data across demes are conditionally independent given their deme-specific frequencies. Between demes, the allele frequency is assumed to vary around a global mean due to population structure and drift.\n\nStarting from core definitions only, construct a hierarchical model with the following elements:\n\n- Local sampling model in each deme: conditional on $\\theta_d$, the observed count $x_d$ follows a Binomial distribution with parameters $n_d$ and $\\theta_d$.\n- Between-deme variation: conditional on the global mean allele frequency $p$ and a concentration parameter $\\kappa  0$, each $\\theta_d$ is independently drawn from a Beta distribution parameterized by mean $p$ and concentration $\\kappa$, i.e., $\\theta_d \\sim \\mathrm{Beta}(\\kappa p,\\kappa(1-p))$.\n- Global prior: the global mean allele frequency $p$ has a Beta prior with hyperparameters $\\alpha_0  0$ and $\\beta_0  0$, i.e., $p \\sim \\mathrm{Beta}(\\alpha_0,\\beta_0)$.\n\nYour tasks are:\n\n1. From the above model specification and core definitions of the Binomial and Beta distributions, derive the marginal likelihood of $x_d$ given $p$ and $\\kappa$ by integrating out $\\theta_d$. Then, derive the joint posterior density of $p$ given all observed counts $\\{(x_d,n_d)\\}_{d=1}^D$, up to a proportionality constant. Do not assume any shortcut formulas beyond the definitions of the Binomial and Beta distributions and the Beta function.\n2. Derive a closed-form expression for the conditional posterior of each $\\theta_d$ given $(p,x_d,n_d,\\kappa)$ and then derive a formula for the posterior mean $\\mathbb{E}[\\theta_d \\mid \\{(x_j,n_j)\\}_{j=1}^D,\\alpha_0,\\beta_0,\\kappa]$ in terms of $\\mathbb{E}[p \\mid \\{(x_j,n_j)\\}_{j=1}^D,\\alpha_0,\\beta_0,\\kappa]$.\n3. Implement a program that computes, for each test case below, the following outputs:\n   - The posterior mean of $p$, i.e., $\\mathbb{E}[p \\mid \\{(x_d,n_d)\\}_{d=1}^D,\\alpha_0,\\beta_0,\\kappa]$, obtained by numerically integrating the analytically derived unnormalized posterior density of $p$ over $p \\in (0,1)$.\n   - The posterior mean of each $\\theta_d$ using your derivation from task $2$.\n\nNumerical requirements and restrictions:\n\n- Your numerical computation of $\\mathbb{E}[p \\mid \\cdot]$ must be based on one-dimensional numerical integration over $p \\in (0,1)$ of the analytically derived unnormalized posterior density of $p$. You may use any stable change of variables or scaling for numerical stability, but do not use black-box sampling methods such as Markov Chain Monte Carlo (MCMC). You may use special functions for the Beta function in logarithmic form to ensure stability.\n- All outputs must be rounded to exactly $6$ decimal places.\n- All outputs must be pure numbers without any units and must be expressed as decimals (no percentage sign).\n- The final output format must be a single line containing a list of per-test-case lists. For each test case, output a list whose first element is the posterior mean of $p$ and whose subsequent elements are the posterior means of the $\\theta_d$ values in the same order as the input demes. The outer list must contain these per-case lists in the same order as provided below. The entire line must have no whitespace characters. For example, a valid format is $[[0.500000,0.500000,0.500000],[0.600000,0.590000,0.610000]]$.\n\nFoundational base you may use:\n\n- Definition of the Binomial probability mass: for $x \\in \\{0,1,\\dots,n\\}$, $n \\in \\mathbb{N}$, $\\theta \\in (0,1)$, $\\Pr(X=x \\mid n,\\theta) = \\binom{n}{x} \\theta^x (1-\\theta)^{n-x}$.\n- Definition of the Beta density: for $a0$, $b0$, $f(\\theta \\mid a,b) = \\dfrac{\\theta^{a-1} (1-\\theta)^{b-1}}{B(a,b)}$ on $\\theta \\in (0,1)$, where $B(a,b) = \\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ is the Beta function.\n- Independence assumptions specified above.\n- Basic properties of integrals and expectations.\n\nTest suite:\n\nProvide outputs for the following five test cases. Each test case is specified by the tuple $(\\alpha_0,\\beta_0,\\kappa,\\mathbf{x},\\mathbf{n})$, where $\\mathbf{x}$ and $\\mathbf{n}$ are aligned lists across demes.\n\n- Test case $1$: $(\\alpha_0,\\beta_0,\\kappa,\\mathbf{x},\\mathbf{n}) = (\\,\\,$$2$$,$$2$$,$$20$$,\\, [$$12$$,$$15$$,$$8$$],\\, [$$20$$,$$20$$,$$20$$]\\,)$.\n- Test case $2$: $(\\alpha_0,\\beta_0,\\kappa,\\mathbf{x},\\mathbf{n}) = (\\,\\,$$1$$,$$4$$,$$10$$,\\, [$$0$$,$$1$$,$$0$$],\\, [$$20$$,$$20$$,$$20$$]\\,)$.\n- Test case $3$: $(\\alpha_0,\\beta_0,\\kappa,\\mathbf{x},\\mathbf{n}) = (\\,\\,$$4$$,$$1$$,$$10$$,\\, [$$20$$,$$19$$,$$20$$],\\, [$$20$$,$$20$$,$$20$$]\\,)$.\n- Test case $4$: $(\\alpha_0,\\beta_0,\\kappa,\\mathbf{x},\\mathbf{n}) = (\\,\\,$$2$$,$$2$$,$$50$$,\\, [$$5$$,$$30$$,$$1$$,$$18$$],\\, [$$10$$,$$50$$,$$2$$,$$20$$]\\,)$.\n- Test case $5$: $(\\alpha_0,\\beta_0,\\kappa,\\mathbf{x},\\mathbf{n}) = (\\,\\,$$2$$,$$2$$,$$5$$,\\, [$$1$$],\\, [$$1$$]\\,)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, in the exact order of the test suite. For test case $d$, the inner list must be $[$ posterior mean of $p$, posterior mean of $\\theta_1$, posterior mean of $\\theta_2$, $\\dots$, posterior mean of $\\theta_D]$, each rounded to $6$ decimal places.", "solution": "The posed problem is scientifically sound, mathematically well-defined, and contains all necessary information for a unique solution. It is a standard problem in Bayesian hierarchical modeling applied to population genetics. The problem is valid. We proceed with the derivation and solution.\n\nThe hierarchical model is specified as follows:\n1.  **Likelihood**: For each deme $d \\in \\{1, \\dots, D\\}$, the observed allele count $x_d$ given the true deme-specific allele frequency $\\theta_d$ follows a Binomial distribution.\n    $$P(x_d \\mid n_d, \\theta_d) = \\binom{n_d}{x_d} \\theta_d^{x_d} (1-\\theta_d)^{n_d-x_d}$$\n2.  **Deme-level Prior**: The deme-specific allele frequencies $\\theta_d$ are drawn from a common Beta distribution, conditional on a global mean frequency $p$ and a concentration parameter $\\kappa$. The Beta distribution is parameterized by mean $p$ and concentration $\\kappa$, which corresponds to shape parameters $a = \\kappa p$ and $b = \\kappa(1-p)$.\n    $$P(\\theta_d \\mid p, \\kappa) = \\frac{\\theta_d^{\\kappa p-1} (1-\\theta_d)^{\\kappa(1-p)-1}}{B(\\kappa p, \\kappa(1-p))}$$\n    where $B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ is the Beta function.\n3.  **Global Hyperprior**: The global mean allele frequency $p$ follows a Beta distribution with hyperparameters $\\alpha_0$ and $\\beta_0$.\n    $$P(p \\mid \\alpha_0, \\beta_0) = \\frac{p^{\\alpha_0-1} (1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)}$$\n\nWe are given that the data across demes, $\\{(x_d, n_d)\\}_{d=1}^D$, are conditionally independent given their respective frequencies $\\theta_d$, and the frequencies $\\theta_d$ are conditionally independent given $p$ and $\\kappa$.\n\n### Part 1: Marginal Likelihood and Joint Posterior of $p$\n\nFirst, we derive the marginal likelihood of the observed count $x_d$ in a single deme $d$ by integrating out the unknown local frequency $\\theta_d$. This is the probability of observing $x_d$ given the global parameters $p$ and $\\kappa$.\n$$P(x_d \\mid n_d, p, \\kappa) = \\int_0^1 P(x_d \\mid n_d, \\theta_d) P(\\theta_d \\mid p, \\kappa) \\, d\\theta_d$$\nSubstituting the Binomial probability mass function and the Beta probability density function:\n$$P(x_d \\mid n_d, p, \\kappa) = \\int_0^1 \\left[ \\binom{n_d}{x_d} \\theta_d^{x_d} (1-\\theta_d)^{n_d-x_d} \\right] \\left[ \\frac{\\theta_d^{\\kappa p - 1} (1-\\theta_d)^{\\kappa(1-p) - 1}}{B(\\kappa p, \\kappa(1-p))} \\right] \\, d\\theta_d$$\nWe group terms that do not depend on $\\theta_d$ outside the integral:\n$$P(x_d \\mid n_d, p, \\kappa) = \\frac{\\binom{n_d}{x_d}}{B(\\kappa p, \\kappa(1-p))} \\int_0^1 \\theta_d^{x_d + \\kappa p - 1} (1-\\theta_d)^{n_d - x_d + \\kappa(1-p) - 1} \\, d\\theta_d$$\nThe integral has the form of a Beta function. Specifically, $\\int_0^1 t^{a-1}(1-t)^{b-1} dt = B(a,b)$. In our case, the equivalent parameters are $a = x_d + \\kappa p$ and $b = n_d - x_d + \\kappa(1-p)$. Thus, the integral evaluates to $B(x_d + \\kappa p, n_d - x_d + \\kappa(1-p))$.\nThe resulting marginal likelihood is the probability mass function of the Beta-Binomial distribution:\n$$P(x_d \\mid n_d, p, \\kappa) = \\binom{n_d}{x_d} \\frac{B(x_d + \\kappa p, n_d - x_d + \\kappa(1-p))}{B(\\kappa p, \\kappa(1-p))}$$\nNext, we derive the joint posterior density of the global mean frequency $p$, given all observed data and the fixed hyperparameters. Let $\\mathbf{x} = \\{x_1, \\dots, x_D\\}$ and $\\mathbf{n} = \\{n_1, \\dots, n_D\\}$.\nUsing Bayes' theorem, the posterior of $p$ is proportional to the product of the likelihood of the data given $p$ and the prior of $p$:\n$$P(p \\mid \\mathbf{x}, \\mathbf{n}, \\alpha_0, \\beta_0, \\kappa) \\propto P(\\mathbf{x} \\mid \\mathbf{n}, p, \\kappa) P(p \\mid \\alpha_0, \\beta_0)$$\nDue to the conditional independence of observations across demes given $p$ and $\\kappa$, the joint likelihood is the product of the marginal likelihoods for each deme:\n$$P(\\mathbf{x} \\mid \\mathbf{n}, p, \\kappa) = \\prod_{d=1}^D P(x_d \\mid n_d, p, \\kappa)$$\nCombining these, the posterior density for $p$ is:\n$$P(p \\mid \\cdot) \\propto \\left[ \\prod_{d=1}^D P(x_d \\mid n_d, p, \\kappa) \\right] P(p \\mid \\alpha_0, \\beta_0)$$\nSubstituting the derived expressions:\n$$P(p \\mid \\cdot) \\propto \\left[ \\prod_{d=1}^D \\binom{n_d}{x_d} \\frac{B(x_d + \\kappa p, n_d - x_d + \\kappa(1-p))}{B(\\kappa p, \\kappa(1-p))} \\right] \\left[ \\frac{p^{\\alpha_0-1} (1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)} \\right]$$\nFor the unnormalized posterior density, we can drop all terms that do not depend on $p$. These include the binomial coefficients $\\binom{n_d}{x_d}$ and the prior normalizing constant $B(\\alpha_0, \\beta_0)$.\nThe unnormalized posterior density of $p$ is therefore:\n$$f(p) \\propto p^{\\alpha_0-1} (1-p)^{\\beta_0-1} \\prod_{d=1}^D \\frac{B(x_d + \\kappa p, n_d - x_d + \\kappa(1-p))}{B(\\kappa p, \\kappa(1-p))}$$\n\n### Part 2: Conditional Posterior of $\\theta_d$ and its Mean\n\nFirst, we find the conditional posterior distribution of a single deme's frequency $\\theta_d$ given the global parameter $p$ and its local data $(x_d, n_d)$.\n$$P(\\theta_d \\mid p, x_d, n_d, \\kappa) \\propto P(x_d \\mid n_d, \\theta_d) P(\\theta_d \\mid p, \\kappa)$$\nThis relationship holds because, given $p$ and $\\kappa$, $\\theta_d$ is independent of data from other demes. Substituting the likelihood and prior:\n$$P(\\theta_d \\mid \\cdot) \\propto \\left[ \\theta_d^{x_d} (1-\\theta_d)^{n_d-x_d} \\right] \\left[ \\theta_d^{\\kappa p - 1} (1-\\theta_d)^{\\kappa(1-p) - 1} \\right]$$\nCombining the exponents:\n$$P(\\theta_d \\mid \\cdot) \\propto \\theta_d^{x_d + \\kappa p - 1} (1-\\theta_d)^{n_d - x_d + \\kappa(1-p) - 1}$$\nThis is the kernel of a Beta distribution, a result of the conjugacy between the Beta prior and the Binomial likelihood. The conditional posterior distribution is:\n$$\\theta_d \\mid p, x_d, n_d, \\kappa \\sim \\mathrm{Beta}(x_d + \\kappa p, n_d - x_d + \\kappa(1-p))$$\nNext, we derive the posterior mean of $\\theta_d$, which requires marginalizing over the uncertainty in $p$. Let $\\mathcal{D} = \\{(\\mathbf{x}, \\mathbf{n}), \\alpha_0, \\beta_0, \\kappa\\}$. We use the law of total expectation:\n$$\\mathbb{E}[\\theta_d \\mid \\mathcal{D}] = \\mathbb{E}_{p \\mid \\mathcal{D}} \\left[ \\mathbb{E}[\\theta_d \\mid p, \\mathcal{D}] \\right]$$\nThe inner expectation is the mean of the conditional posterior distribution $\\mathrm{Beta}(\\alpha, \\beta)$, which is $\\alpha/(\\alpha+\\beta)$.\n$$\\mathbb{E}[\\theta_d \\mid p, \\mathcal{D}] = \\mathbb{E}[\\theta_d \\mid p, x_d, n_d, \\kappa] = \\frac{x_d + \\kappa p}{(x_d + \\kappa p) + (n_d - x_d + \\kappa(1-p))} = \\frac{x_d + \\kappa p}{n_d + \\kappa}$$\nSubstituting this back into the outer expectation:\n$$\\mathbb{E}[\\theta_d \\mid \\mathcal{D}] = \\mathbb{E}_{p \\mid \\mathcal{D}} \\left[ \\frac{x_d + \\kappa p}{n_d + \\kappa} \\right]$$\nBy the linearity of expectation, we can write:\n$$\\mathbb{E}[\\theta_d \\mid \\mathcal{D}] = \\frac{1}{n_d + \\kappa} \\mathbb{E}_{p \\mid \\mathcal{D}}[x_d + \\kappa p] = \\frac{x_d + \\kappa \\, \\mathbb{E}[p \\mid \\mathcal{D}]}{n_d + \\kappa}$$\nThis elegant formula shows that the posterior mean of $\\theta_d$ is a weighted average of the local empirical frequency $\\frac{x_d}{n_d}$ and the posterior global mean frequency $\\mathbb{E}[p \\mid \\mathcal{D}]$. The parameter $\\kappa$ controls the strength of this \"shrinkage\" towards the global mean.\n\n### Part 3: Numerical Implementation\n\nThe analytical form of the posterior $P(p \\mid \\mathcal{D})$ is complex and does not belong to a standard distribution family. Therefore, its posterior mean, $\\mathbb{E}[p \\mid \\mathcal{D}]$, must be computed numerically.\n$$\\mathbb{E}[p \\mid \\mathcal{D}] = \\frac{\\int_0^1 p \\cdot f(p) \\, dp}{\\int_0^1 f(p) \\, dp}$$\nwhere $f(p)$ is the unnormalized posterior density derived in Part 1. To avoid numerical underflow/overflow issues with products of Beta functions, computations are performed in log-space. The log of the unnormalized posterior kernel is:\n$$\\log f(p) = (\\alpha_0-1)\\log p + (\\beta_0-1)\\log(1-p) + \\sum_{d=1}^D \\left[ \\log B(x_d + \\kappa p, n_d - x_d + \\kappa(1-p)) - \\log B(\\kappa p, \\kappa(1-p)) \\right]$$\nThis is computed efficiently using the `gammaln` function (log-gamma). For numerical stability during integration, the integrand is normalized by its maximum value. This is achieved by finding the mode of the posterior for $p$ via numerical optimization, computing the log-posterior maximum $L_{\\max}$, and then evaluating the integrals of $\\exp(\\log f(p) - L_{\\max})$ and $p \\cdot \\exp(\\log f(p) - L_{\\max})$. The ratio of these two integrals gives $\\mathbb{E}[p \\mid \\mathcal{D}]$. Once this value is found, the posterior means $\\mathbb{E}[\\theta_d \\mid \\mathcal{D}]$ are computed using the derived shrinkage formula from Part 2.", "answer": "[[0.582844,0.591422,0.664286,0.485714],[0.053862,0.017954,0.051268,0.017954],[0.949980,0.966653,0.916655,0.966653],[0.588691,0.573913,0.588691,0.575665,0.601416],[0.500000,0.666667]]", "id": "2690165"}]}