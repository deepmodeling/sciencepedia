{"hands_on_practices": [{"introduction": "Modern genomic methods like RAD-seq provide a wealth of data for species delimitation, but this comes with statistical challenges, most notably the physical linkage of loci. This exercise tackles the common problem of non-independence among SNPs by implementing a thinning strategy, a pragmatic approach to preparing data for analyses that assume unlinked loci. By engaging with the concept of the effective number of independent loci, you will learn to move beyond arbitrary data filtering and develop a statistically principled workflow for handling high-throughput sequence data [@problem_id:2752737].", "problem": "You are given Restriction site Associated DNA sequencing (RAD-seq) Single-Nucleotide Polymorphism (SNP) panels intended for species delimitation analysis, but the physical ordering of SNPs is unknown and thus the degree of Linkage Disequilibrium (LD) among adjacent SNPs is uncertain. To control for the inflation of variance in summary statistics that assume independent loci, you will implement a thinning strategy that retains every $t$-th SNP in index order (treating SNPs as an indexed sequence, not requiring physical position). Your design must control the effective number of independent loci and explicitly quantify the effect of thinning on variance estimates under a minimal, widely used correlation model.\n\nFundamental base and modeling assumptions:\n1. Each locus is modeled as a random variable with common variance $\\sigma^2$ and weak stationarity (constant mean and autocovariance that depends only on lag).\n2. The lag-$k$ correlation among adjacent loci in index order is modeled as $\\rho_k = r^k$, with $0 \\le r < 1$, i.e., a first-order autoregressive (AR(1)) correlation structure over the index. This serves as a conservative surrogate when physical distances are unknown, a common situation for RAD-seq contigs.\n3. For a stationary, weakly dependent sequence of length $n$ with lag correlations $\\rho_k$, the variance of the sample mean satisfies $\\mathrm{Var}(\\bar{X}) \\approx \\sigma^2 / n_{\\mathrm{eff}}$, where the effective sample size (effective number of independent loci) is\n$$\nn_{\\mathrm{eff}} \\approx \\frac{n}{1 + 2 \\sum_{k=1}^{\\infty} \\rho_k}.\n$$\nThis approximation is a standard result in statistics of correlated observations and is widely used for time series and genomic LD blocks.\n4. If you thin by stride $t \\in \\{1,2,\\dots\\}$, keeping the first SNP and then every $t$-th thereafter, the retained count is\n$$\nn' = \\left\\lceil \\frac{M}{t} \\right\\rceil,\n$$\nwhere $M$ is the total number of SNPs before thinning. Under the AR(1) index-correlation model, the correlation among retained loci has lag-$k$ correlation $\\rho_k' = (r^t)^k$, and thus\n$$\n\\sum_{k=1}^{\\infty} \\rho_k' = \\frac{r^t}{1 - r^t} \\quad \\text{for } 0 \\le r < 1.\n$$\nTherefore, the effective number of independent loci among the retained set is\n$$\nn_{\\mathrm{eff}}(t) \\approx n' \\cdot \\frac{1 - r^t}{1 + r^t}.\n$$\nThe variance inflation factor relative to independence for the retained set is\n$$\n\\mathrm{VIF}(t) = \\frac{n'}{n_{\\mathrm{eff}}(t)} = \\frac{1 + r^t}{1 - r^t}.\n$$\n\nYour programming task:\nGiven a test suite of tuples $(M, r, L_{\\mathrm{target}})$, where $M$ is the number of SNPs, $r$ is the adjacent-locus correlation parameter with $0 \\le r < 1$, and $L_{\\mathrm{target}}$ is the desired lower bound on the effective number of independent loci after thinning, design an algorithm to choose a thinning stride $t$ that maximizes thinning while meeting the effective-loci constraint:\n- If there exists at least one $t \\in \\{1,2,\\dots,M\\}$ such that $n_{\\mathrm{eff}}(t) \\ge L_{\\mathrm{target}}$, choose the largest such $t$ (i.e., the coarsest thinning that still meets the target).\n- If no such $t$ exists, report that the target is infeasible by setting the chosen stride to $-1$ and instead choose the $t$ that maximizes $n_{\\mathrm{eff}}(t)$ over $t \\in \\{1,2,\\dots,M\\}$; report this $t$ as the used stride together with its $n_{\\mathrm{eff}}$ and $\\mathrm{VIF}$.\n\nFor each test case, your program must output a list with four entries:\n- the chosen stride $t_{\\mathrm{choice}}$ (an integer, or $-1$ if the target is infeasible),\n- the used stride $t_{\\mathrm{used}}$ that determines the reported metrics (equals $t_{\\mathrm{choice}}$ if feasible, or the argmax of $n_{\\mathrm{eff}}(t)$ if infeasible),\n- the effective number of loci $n_{\\mathrm{eff}}(t_{\\mathrm{used}})$ rounded to three decimals,\n- the variance inflation factor $\\mathrm{VIF}(t_{\\mathrm{used}})$ rounded to three decimals.\n\nScientific realism and rationale:\n- Thinning cannot increase the total information beyond using all loci with proper modeling of LD, but it can reduce the variance inflation factor for analyses that assume independence, making variance estimates conservative and more robust in species delimitation pipelines that aggregate across loci.\n- For $r = 0$, you must treat $r^t = 0$, giving $n_{\\mathrm{eff}}(t) = n'$ and $\\mathrm{VIF}(t) = 1$.\n\nAngle units are not applicable. There are no physical units to report. All floats must be rounded to three decimals in the output.\n\nTest suite:\nUse exactly the following five cases as input, in this order:\n1. $(M, r, L_{\\mathrm{target}}) = (1000, 0.2, 300)$\n2. $(M, r, L_{\\mathrm{target}}) = (1000, 0.8, 100)$\n3. $(M, r, L_{\\mathrm{target}}) = (500, 0.0, 400)$\n4. $(M, r, L_{\\mathrm{target}}) = (75, 0.5, 20)$\n5. $(M, r, L_{\\mathrm{target}}) = (50, 0.95, 5)$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the case-specific list described above. For example: \n\"[[t_choice_case1,t_used_case1,neff_case1,VIF_case1],[t_choice_case2,t_used_case2,neff_case2,VIF_case2],...]\"\n\nNo user input is required; the program must hard-code the test suite above and print the results in the specified format.", "solution": "We are asked to design a thinning strategy for Restriction site Associated DNA sequencing (RAD-seq) Single-Nucleotide Polymorphism (SNP) panels with unknown physical linkage by working in a purely mathematical model and quantifying consequences for variance. The base is Linkage Disequilibrium (LD) modeled as an autoregressive correlation over an index ordering. We begin from fundamental, widely tested definitions.\n\nFundamental base:\n1. Let $\\{X_i\\}_{i=1}^n$ be weakly stationary with $\\mathbb{E}[X_i] = \\mu$, $\\mathrm{Var}(X_i) = \\sigma^2$, and lag-$k$ correlation $\\rho_k = \\mathrm{Corr}(X_i, X_{i+k})$. For the sample mean $\\bar{X}_n = n^{-1} \\sum_{i=1}^n X_i$, the variance under weak dependence satisfies\n$$\n\\mathrm{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n} \\left(1 + 2 \\sum_{k=1}^{n-1} \\left(1 - \\frac{k}{n}\\right) \\rho_k \\right).\n$$\nFor large $n$ and summable correlations, this is well-approximated by\n$$\n\\mathrm{Var}(\\bar{X}_n) \\approx \\frac{\\sigma^2}{n_{\\mathrm{eff}}}, \\quad n_{\\mathrm{eff}} \\approx \\frac{n}{1 + 2 \\sum_{k=1}^{\\infty} \\rho_k}.\n$$\nThis effective sample size captures the information-equivalent count of independent observations and is a standard tool in time series and genomics where correlations are present.\n\n2. Under an autoregressive of order one (AR(1)) index-correlation model with parameter $r \\in [0,1)$, we have $\\rho_k = r^k$. Then\n$$\n\\sum_{k=1}^{\\infty} \\rho_k = \\sum_{k=1}^{\\infty} r^k = \\frac{r}{1 - r},\n$$\nand therefore\n$$\nn_{\\mathrm{eff}} \\approx \\frac{n}{1 + 2 \\cdot \\frac{r}{1 - r}} = n \\cdot \\frac{1 - r}{1 + r}.\n$$\n\nThinning and its effect:\nWe thin by stride $t \\in \\{1,2,\\dots\\}$, keeping the first locus and every $t$-th thereafter. This yields a retained count\n$$\nn' = \\left\\lceil \\frac{M}{t} \\right\\rceil,\n$$\nwhere $M$ is the total number of loci before thinning. Since the retained subsequence is sampled at lag $t$ in the index order, its lag-$k$ correlation becomes\n$$\n\\rho_k' = \\mathrm{Corr}(X_i, X_{i+kt}) = r^{kt} = (r^t)^k,\n$$\nwhich is again AR(1) with parameter $r^t$. Consequently,\n$$\n\\sum_{k=1}^{\\infty} \\rho_k' = \\frac{r^t}{1 - r^t}, \\quad n_{\\mathrm{eff}}(t) \\approx n' \\cdot \\frac{1 - r^t}{1 + r^t}.\n$$\nThe variance inflation factor (VIF) for the retained set relative to independence (i.e., relative to variance $\\sigma^2/n'$) is\n$$\n\\mathrm{VIF}(t) = \\frac{n'}{n_{\\mathrm{eff}}(t)} = \\frac{1 + r^t}{1 - r^t}.\n$$\nWhen $r = 0$, we have $r^t = 0$ for all $t$, yielding $n_{\\mathrm{eff}}(t) = n'$ and $\\mathrm{VIF}(t) = 1$, as expected for independent loci.\n\nOptimization criterion:\nWe are asked to control the effective number of loci via thinning. Given a target $L_{\\mathrm{target}}$, the constraint is\n$$\nn_{\\mathrm{eff}}(t) = \\left\\lceil \\frac{M}{t} \\right\\rceil \\cdot \\frac{1 - r^t}{1 + r^t} \\ge L_{\\mathrm{target}}.\n$$\nIf feasible, we are to maximize $t$ subject to the constraint, i.e.,\n$$\nt_{\\mathrm{choice}} = \\max \\left\\{ t \\in \\{1,2,\\dots,M\\} : \\left\\lceil \\frac{M}{t} \\right\\rceil \\cdot \\frac{1 - r^t}{1 + r^t} \\ge L_{\\mathrm{target}} \\right\\}.\n$$\nIf the feasible set is empty, we must declare infeasibility by outputting $t_{\\mathrm{choice}} = -1$ and instead select\n$$\nt_{\\mathrm{used}} \\in \\arg\\max_{t \\in \\{1,2,\\dots,M\\}} \\left( \\left\\lceil \\frac{M}{t} \\right\\rceil \\cdot \\frac{1 - r^t}{1 + r^t} \\right).\n$$\nThe reported effective count and VIF are then computed at $t_{\\mathrm{used}}$.\n\nAlgorithmic steps:\n1. For each test case $(M, r, L_{\\mathrm{target}})$, iterate $t$ from $1$ to $M$:\n   - Compute $n' = \\left\\lceil \\frac{M}{t} \\right\\rceil$ using integer arithmetic as $n' = \\left\\lfloor \\frac{M + t - 1}{t} \\right\\rfloor$.\n   - Compute $r^t$; if $r = 0$, treat $r^t = 0$ directly.\n   - Compute the factor $f(t) = \\frac{1 - r^t}{1 + r^t}$ and $n_{\\mathrm{eff}}(t) = n' \\cdot f(t)$.\n   - Track the set of feasible $t$ with $n_{\\mathrm{eff}}(t) \\ge L_{\\mathrm{target}}$; if non-empty, take $t_{\\mathrm{choice}}$ to be its maximum.\n   - Track the $t$ that maximizes $n_{\\mathrm{eff}}(t)$ to provide $t_{\\mathrm{used}}$ in the infeasible case.\n2. If the feasible set is non-empty, set $t_{\\mathrm{used}} = t_{\\mathrm{choice}}$ and compute $\\mathrm{VIF}(t_{\\mathrm{used}}) = \\frac{1 + r^{t_{\\mathrm{used}}}}{1 - r^{t_{\\mathrm{used}}}}$; else, set $t_{\\mathrm{choice}} = -1$ and use the argmax $t_{\\mathrm{used}}$ and the corresponding $n_{\\mathrm{eff}}$ and $\\mathrm{VIF}$.\n3. Round $n_{\\mathrm{eff}}(t_{\\mathrm{used}})$ and $\\mathrm{VIF}(t_{\\mathrm{used}})$ to three decimals for reporting.\n\nScientific justification:\n- The AR(1) model for index correlation provides a conservative decay of LD with index lag, which is reasonable for unknown ordering in RAD-seq; while it is an approximation, it captures that thinning by stride $t$ reduces short-range correlation to $r^t$, improving the independence approximation of retained loci.\n- The effective number of independent loci $n_{\\mathrm{eff}}(t)$ directly controls the variance of locus-averaged summary statistics commonly used in species delimitation methods (e.g., site frequency-based statistics, coalescent summary statistics), via $\\mathrm{Var}(\\bar{X}) \\approx \\sigma^2 / n_{\\mathrm{eff}}(t)$.\n- The variance inflation factor $\\mathrm{VIF}(t)$ quantifies the residual correlation among retained loci; thinning decreases $\\mathrm{VIF}(t)$ monotonically in $t$ because $r^t$ decreases with $t$, ensuring more reliable variance estimates under independence assumptions.\n\nApplying to the specified test suite:\n- Case $(1000, 0.2, 300)$: For $t = 3$, $n' = \\lceil 1000/3 \\rceil = 334$, $r^t = 0.2^3 = 0.008$, $f(3) = \\frac{1 - 0.008}{1 + 0.008} \\approx 0.984127$, giving $n_{\\mathrm{eff}}(3) \\approx 328.26 \\ge 300$, while $t = 4$ yields $n_{\\mathrm{eff}}(4) \\approx 249.2 < 300$, so $t_{\\mathrm{choice}} = 3$ and $\\mathrm{VIF}(3) \\approx 1.016$.\n- Case $(1000, 0.8, 100)$: The maximal $t$ with $n_{\\mathrm{eff}}(t) \\ge 100$ is $t = 5$, where $n' = 200$, $r^5 = 0.32768$, $f(5) \\approx 0.506521$, $n_{\\mathrm{eff}}(5) \\approx 101.304$, and $\\mathrm{VIF}(5) \\approx 1.975$.\n- Case $(500, 0.0, 400)$: Independence, so $n_{\\mathrm{eff}}(t) = n'$. The largest $t$ such that $n' \\ge 400$ is $t = 1$ (since $t = 2$ gives $n' = 250$), hence $t_{\\mathrm{choice}} = 1$, $n_{\\mathrm{eff}} = 500$, $\\mathrm{VIF} = 1$.\n- Case $(75, 0.5, 20)$: $t = 2$ gives $n' = 38$, $r^2 = 0.25$, $f(2) = 0.6$, so $n_{\\mathrm{eff}}(2) = 22.8 \\ge 20$; $t = 3$ gives $n_{\\mathrm{eff}}(3) \\approx 19.444 < 20$, hence $t_{\\mathrm{choice}} = 2$ and $\\mathrm{VIF}(2) = 1.666\\ldots$.\n- Case $(50, 0.95, 5)$: The target is infeasible because $n_{\\mathrm{eff}}(t)$ never reaches $5$ for $t \\in \\{1,\\dots,50\\}$. We therefore set $t_{\\mathrm{choice}} = -1$ and choose $t_{\\mathrm{used}}$ to maximize $n_{\\mathrm{eff}}(t)$. A direct search shows the maximum near $t = 24$ with $n' = \\lceil 50/24 \\rceil = 3$, $r^{24} \\approx 0.29199$, $f(24) \\approx 0.5480$, giving $n_{\\mathrm{eff}}(24) \\approx 1.644$ and $\\mathrm{VIF}(24) \\approx 1.825$.\n\nThe program will implement these steps exactly, compute the optimal strides, and print the case-wise results in the specified format with three-decimal rounding.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef effective_loci_and_vif(M: int, r: float, t: int):\n    \"\"\"\n    Compute n_eff(t) and VIF(t) for given M, r, and thinning stride t.\n    n' = ceil(M / t)\n    r_t = r**t\n    n_eff(t) = n' * (1 - r_t) / (1 + r_t)\n    VIF(t) = (1 + r_t) / (1 - r_t)\n    Special case: if r == 0.0, then r_t = 0.0, so n_eff = n' and VIF = 1.0\n    \"\"\"\n    n_retained = (M + t - 1) // t  # ceil division\n    if r = 0.0:\n        r_t = 0.0\n    else:\n        r_t = r ** t\n    # Numerical safety: if r_t is extremely close to 1, cap within (0,1)\n    if r_t >= 1.0:\n        r_t = 1.0 - 1e-15\n    if r_t == 0.0:\n        n_eff = float(n_retained)\n        vif = 1.0\n    else:\n        factor = (1.0 - r_t) / (1.0 + r_t)\n        n_eff = n_retained * factor\n        vif = (1.0 + r_t) / (1.0 - r_t)\n    return n_eff, vif\n\ndef choose_thinning_stride(M: int, r: float, L_target: float):\n    \"\"\"\n    Choose the largest t in {1, ..., M} such that n_eff(t) >= L_target.\n    If no such t exists, return t_choice = -1 and t_used = argmax n_eff(t).\n    Returns (t_choice, t_used, n_eff_used, vif_used).\n    \"\"\"\n    feasible_t = None  # largest feasible t\n    best_t = 1\n    best_neff = -1.0\n\n    for t in range(1, M + 1):\n        n_eff, vif = effective_loci_and_vif(M, r, t)\n        # Track best n_eff across all t\n        if n_eff > best_neff + 1e-12:\n            best_neff = n_eff\n            best_t = t\n        # Check feasibility\n        if n_eff >= L_target - 1e-12:  # allow small tolerance\n            feasible_t = t  # since we iterate increasing t, this will end up as largest feasible\n\n    if feasible_t is not None:\n        # Use the largest feasible t\n        t_choice = feasible_t\n        t_used = feasible_t\n        n_eff_used, vif_used = effective_loci_and_vif(M, r, t_used)\n    else:\n        # Infeasible: use the t that maximizes n_eff\n        t_choice = -1\n        t_used = best_t\n        n_eff_used, vif_used = effective_loci_and_vif(M, r, t_used)\n\n    # Round as specified: three decimals\n    n_eff_used_rounded = round(n_eff_used + 1e-12, 3)\n    vif_used_rounded = round(vif_used + 1e-12, 3)\n    return [t_choice, t_used, n_eff_used_rounded, vif_used_rounded]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (M, r, L_target)\n    test_cases = [\n        (1000, 0.2, 300.0),\n        (1000, 0.8, 100.0),\n        (500, 0.0, 400.0),\n        (75, 0.5, 20.0),\n        (50, 0.95, 5.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        M, r, L_target = case\n        result = choose_thinning_stride(M, r, L_target)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Print as a single-line string representation of the list of lists.\n    # Ensure no extra spaces beyond commas to match the \"comma-separated list\" spirit.\n    def list_to_string(lst):\n        # Convert nested list of primitives to string without spaces after commas\n        if isinstance(lst, list):\n            return \"[\" + \",\".join(list_to_string(x) for x in lst) + \"]\"\n        else:\n            return str(lst)\n\n    print(list_to_string(results))\n\nsolve()\n```", "id": "2752737"}, {"introduction": "After applying a model-based method like the multispecies coalescent to test competing species hypotheses, we are often left with model evidence scores, such as log-marginal likelihoods. This practice focuses on the crucial step of interpreting these outputs to perform Bayesian model selection. You will calculate and interpret a Bayes factor, which provides a formal way to weigh the evidence from your data in favor of one delimitation model (e.g., splitting two lineages) over another (e.g., lumping them) [@problem_id:2752783].", "problem": "In a multispecies coalescent framework for species delimitation, two competing models are considered for a focal clade: model $\\mathcal{M}_{A}$ (lumping two candidate lineages) and model $\\mathcal{M}_{B}$ (splitting them). Using thermodynamic integration via stepping-stone sampling, you obtain the following model evidences reported as natural logarithms of the marginal likelihoods: $\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{A}) = -17890.432$ and $\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{B}) = -17882.932$, where $\\mathcal{D}$ denotes the multilocus sequence dataset. Assume the same priors and data processing across models so that these values are directly comparable as model evidences.\n\nUsing the foundational definitions of marginal likelihood and Bayes factor, compute the Bayes factor in favor of $\\mathcal{M}_{B}$ relative to $\\mathcal{M}_{A}$. Then, interpret the strength of evidence for $\\mathcal{M}_{B}$ using the following pre-specified scale (Kass–Raftery style, adapted for direct Bayes factors $\\mathrm{BF}$):\n\n- $1 \\le \\mathrm{BF}  3.2$: not worth more than a bare mention,\n- $3.2 \\le \\mathrm{BF}  10$: substantial,\n- $10 \\le \\mathrm{BF}  100$: strong,\n- $\\mathrm{BF} \\ge 100$: decisive.\n\nReport the Bayes factor as a pure number, rounded to four significant figures. Do not include any units. You may provide the interpretation in your reasoning, but the final reported answer must be only the Bayes factor value.", "solution": "The fundamental quantity for comparing two statistical models, $\\mathcal{M}_{A}$ and $\\mathcal{M}_{B}$, in a Bayesian framework is the Bayes factor. The Bayes factor, denoted $\\mathrm{BF}_{B,A}$, quantifies the evidence from the data $\\mathcal{D}$ in favor of model $\\mathcal{M}_{B}$ over model $\\mathcal{M}_{A}$. It is defined as the ratio of their marginal likelihoods:\n$$\n\\mathrm{BF}_{B,A} = \\frac{p(\\mathcal{D} \\mid \\mathcal{M}_{B})}{p(\\mathcal{D} \\mid \\mathcal{M}_{A})}\n$$\nThe marginal likelihood, $p(\\mathcal{D} \\mid \\mathcal{M})$, represents the probability of observing the data $\\mathcal{D}$ given the model $\\mathcal{M}$, integrated over the entire parameter space of the model. In practice, these values are often extremely small, so computations are performed using their natural logarithms, as provided in the problem statement.\n\nGiven are the log-marginal likelihoods:\n$$\n\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{A}) = -17890.432\n$$\n$$\n\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{B}) = -17882.932\n$$\nTo compute the Bayes factor from these logarithmic values, we utilize the property of logarithms that the logarithm of a ratio is the difference of the logarithms:\n$$\n\\ln(\\mathrm{BF}_{B,A}) = \\ln\\left(\\frac{p(\\mathcal{D} \\mid \\mathcal{M}_{B})}{p(\\mathcal{D} \\mid \\mathcal{M}_{A})}\\right) = \\ln p(\\mathcal{D} \\mid \\mathcal{M}_{B}) - \\ln p(\\mathcal{D} \\mid \\mathcal{M}_{A})\n$$\nThis quantity, $\\ln(\\mathrm{BF}_{B,A})$, is often referred to as the log-Bayes factor. Substituting the given values:\n$$\n\\ln(\\mathrm{BF}_{B,A}) = -17882.932 - (-17890.432) = -17882.932 + 17890.432 = 7.500\n$$\nTo obtain the Bayes factor $\\mathrm{BF}_{B,A}$ itself, we must exponentiate the log-Bayes factor:\n$$\n\\mathrm{BF}_{B,A} = \\exp(\\ln(\\mathrm{BF}_{B,A})) = \\exp(7.500)\n$$\nCalculating this value:\n$$\n\\mathrm{BF}_{B,A} \\approx 1808.0424\n$$\nThe problem requires this value to be rounded to four significant figures. The first four significant digits are $1$, $8$, $0$, and $8$. The following digit is $0$, so no rounding up is performed. The value is $1808$.\n\nThe final step is to interpret this result using the provided scale. The computed Bayes factor is $\\mathrm{BF}_{B,A} = 1808$. Since $1808 \\ge 100$, the evidence in favor of model $\\mathcal{M}_{B}$ (the split model) over model $\\mathcal{M}_{A}$ (the lump model) is classified as \"decisive\". This indicates very strong support for the hypothesis that the two candidate lineages represent distinct species.", "answer": "$$\\boxed{1808}$$", "id": "2752783"}, {"introduction": "A statistically strong result from any single method should always be met with healthy skepticism about its underlying assumptions. This exercise uses the popular Generalized Mixed Yule Coalescent (GMYC) model to cultivate this critical perspective, exploring how common issues like among-lineage rate heterogeneity and uneven sampling can conspire to produce misleading conclusions. By reasoning through these potential biases, you will develop the diagnostic mindset required to assess the robustness of species delimitation results and design more rigorous studies [@problem_id:2752763].", "problem": "You are investigating species delimitation with the Generalized Mixed Yule Coalescent (GMYC) model on an ultrametric phylogeny of a clade that is actually composed of two sister subclades, denoted $\\mathcal{A}$ and $\\mathcal{B}$. The true macroevolutionary and microevolutionary processes are as follows: forward-in-time speciation follows a Yule process with constant speciation rate $\\lambda$ that is identical in $\\mathcal{A}$ and $\\mathcal{B}$, and within-species genealogies follow the standard neutral Kingman coalescent with constant effective population size $N_e$ and panmixia in all species. Within each subclade, all species share the same effective population size $N_e$ and the same true substitution rate along lineages, but the substitution rate differs between subclades: the true per-lineage substitution rate in $\\mathcal{A}$ is $r_{\\mathcal{A}}$ and in $\\mathcal{B}}$ is $r_{\\mathcal{B}}$, with $r_{\\mathcal{A}} = 2 r_{\\mathcal{B}}$. For simplicity, assume that the true times of speciation and coalescent events are identically distributed across the two subclades because $\\lambda$ and $N_e$ are the same.\n\nSampling is incomplete and uneven. In subclade $\\mathcal{A}$, each species is represented by exactly $2$ sampled individuals ($n_{\\mathcal{A}} = 2$). In subclade $\\mathcal{B}$, each species is represented by exactly $10$ sampled individuals ($n_{\\mathcal{B}} = 10$). No unsampled species exist, but within-species sampling is incomplete in both subclades because true census sizes are much larger than $n_{\\mathcal{A}}$ and $n_{\\mathcal{B}}$.\n\nA single-locus alignment evolving under the Jukes–Cantor model was analyzed under a strict molecular clock to obtain an ultrametric tree for GMYC input, using a single global rate $\\hat{r}$ estimated from the entire tree. You may assume that, to first order, the time assigned to a node under a strict clock is $\\hat{t} = d/\\hat{r}$, where $d$ is the expected number of substitutions per site accumulated along the path, and that under the true model $d = r_i t$ with $r_i \\in \\{ r_{\\mathcal{A}}, r_{\\mathcal{B}} \\}$ the subclade-specific true rate and $t$ the true elapsed time. You may also use the following foundational results without proof: under the Yule process, when there are $k$ lineages backward in time, the waiting time to the previous speciation event is exponential with rate $k \\lambda$; under the Kingman coalescent within a panmictic population of constant size $N_e$, when there are $k$ lineages backward in time, the waiting time to the previous coalescent event is exponential with rate $k (k-1) / (2 N_e)$. The GMYC method attempts to detect a time threshold $t^\\ast$ at which the branching process in the ultrametric tree changes from a Yule regime to a coalescent regime by exploiting the different scaling of the event rate with $k$ in the two regimes.\n\nUsing only these principles and definitions, reason about how among-lineage rate heterogeneity and incomplete within-species sampling can bias GMYC delimitation and how to diagnose these issues from data. Consider the implications of the fact that the ultrametric times $\\hat{t}$ are inferred under a possibly misspecified strict clock ($\\hat{r} \\neq r_i$) and that the number of sampled within-species lineages $k$ differs drastically between subclades because $n_{\\mathcal{A}} \\ll n_{\\mathcal{B}}$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. If a strict clock with global rate $\\hat{r}$ is enforced on data generated with $r_{\\mathcal{A}} = 2 r_{\\mathcal{B}}$, then the node ages in $\\mathcal{A}$ are systematically rescaled by a factor $r_{\\mathcal{A}}/\\hat{r}$ and those in $\\mathcal{B}$ by $r_{\\mathcal{B}}/\\hat{r}$, so that, relative to their true values, nodes in the fast-rate subclade $\\mathcal{A}$ become older and nodes in the slow-rate subclade $\\mathcal{B}$ become younger. This can bias GMYC to over-split in $\\mathcal{A}$ (coalescent nodes misclassified as interspecific because they are pushed deeper) and to lump in $\\mathcal{B}$ (coalescent nodes compressed toward the present). A practical diagnostic is to compare GMYC results between strict- and relaxed-clock analyses and to test for a positive association across subclades between estimated clock rates and inferred species counts.\n\nB. Because the coalescent event rate near the present scales as $k(k-1)/(2 N_e)$ with the number of sampled lineages $k$, incomplete within-species sampling (e.g., $n_{\\mathcal{A}} = 2$) reduces the expected density of recent coalescent events, flattening the lineage-through-time plot near the present. This tends to bias the GMYC threshold $t^\\ast$ toward older times, thereby reducing over-splitting. A simple diagnostic is to rarefy the better-sampled subclade $\\mathcal{B}$ down to $n_{\\mathcal{B}} = 2$ and confirm that GMYC species counts are unchanged.\n\nC. GMYC uses only the tree topology and does not use branch lengths, so among-lineage rate heterogeneity in substitution rates cannot bias GMYC delimitation. Therefore, the most relevant diagnostic is to test for topological imbalance rather than to assess clock-model fit.\n\nD. A posterior predictive check that simulates sequence alignments under the fitted relaxed-clock model and a birth–death plus coalescent generative process with the observed sampling scheme, then re-estimates ultrametric trees and re-applies GMYC to each replicate, can flag bias from rate heterogeneity and sampling if the observed GMYC species count lies in the extreme tail of the posterior predictive distribution.\n\nE. Comparing GMYC to Poisson Tree Processes (PTP) on the same tree in units of substitutions per site is sufficient to diagnose among-lineage rate heterogeneity, because PTP is invariant to rate variation across lineages by construction and any discrepancy must therefore reflect GMYC bias.", "solution": "The core of the problem involves two potential sources of bias for the GMYC method: (1) among-lineage rate heterogeneity being modeled with a single-rate strict clock, and (2) uneven within-species sampling.\n\n**Analysis of Rate Heterogeneity Bias:**\nThe problem states that node ages are inferred under a strict clock as $\\hat{t} = d/\\hat{r}$. The true relationship is $d = r_i t$, where $t$ is the true time and $r_i$ is the true rate for a given lineage. Therefore, the inferred time $\\hat{t}$ is related to the true time $t$ by:\n$$ \\hat{t} = \\frac{r_i t}{\\hat{r}} = \\left( \\frac{r_i}{\\hat{r}} \\right) t $$\nThe estimated global rate $\\hat{r}$ will be an average of the true rates $r_{\\mathcal{A}}$ and $r_{\\mathcal{B}}$, weighted by the proportion of the tree in each subclade. Given $r_{\\mathcal{A}} = 2 r_{\\mathcal{B}}$, it is certain that $r_{\\mathcal{B}}  \\hat{r}  r_{\\mathcal{A}}$.\n- For lineages in subclade $\\mathcal{A}$ (the fast one), the scaling factor is $r_{\\mathcal{A}}/\\hat{r}  1$. Thus, their inferred node ages $\\hat{t}_{\\mathcal{A}}$ are artificially inflated relative to the true ages $t_{\\mathcal{A}}$.\n- For lineages in subclade $\\mathcal{B}$ (the slow one), the scaling factor is $r_{\\mathcal{B}}/\\hat{r}  1$. Thus, their inferred node ages $\\hat{t}_{\\mathcal{B}}$ are artificially compressed relative to the true ages $t_{\\mathcal{B}}$.\n\nThe GMYC method seeks a threshold $t^*$ to separate deeper speciation events (Yule process) from shallower coalescent events (Kingman coalescent).\n- In $\\mathcal{A}$, the inflation of node ages means that true coalescent events (within-species) are pushed deeper in time. These artificially old coalescent nodes may cross the threshold $t^*$ and be misclassified as speciation events. This leads to *over-splitting*.\n- In $\\mathcal{B}$, the compression of node ages means that true speciation events are pulled closer to the present. These artificially young speciation nodes may fall below the threshold $t^*$ and be misclassified as coalescent events. This leads to *lumping*.\n\n**Analysis of Sampling Bias:**\nThe rate of coalescent events for $k$ lineages is $\\frac{k(k-1)}{2N_e}$. The density of events near the present depends heavily on $k$.\n- In subclade $\\mathcal{A}$, with $n_{\\mathcal{A}} = 2$, each species starts with $k=2$ lineages. The rate of the single coalescent event is $\\frac{2(1)}{2N_e} = \\frac{1}{N_e}$.\n- In subclade $\\mathcal{B}$, with $n_{\\mathcal{B}} = 10$, each species starts with $k=10$ lineages. The initial coalescent rate is $\\frac{10(9)}{2N_e} = \\frac{45}{N_e}$, which is $45$ times faster than in $\\mathcal{A}$.\nThe dramatic difference in initial coalescent rates creates a much sharper \"upturn\" in the branching rate near the tips of the tree for subclade $\\mathcal{B}$ compared to $\\mathcal{A}$. GMYC relies on detecting this change in rate. Low sampling ($n_{\\mathcal{A}}=2$) provides a very weak signal of the coalescent process, making it appear more similar to the Yule process. This can bias the estimation of the threshold $t^*$. Published studies on GMYC indicate that low per-species sampling tends to cause the model to estimate an older threshold $t^*$, leading to lumping of true species.\n\nBased on this foundation, we evaluate each option:\n\n**A.** This statement is entirely consistent with the analysis above. The rescaling of node ages and its consequences for over-splitting in the fast clade ($\\mathcal{A}$) and lumping in the slow clade ($\\mathcal{B}$) are correctly described. The proposed diagnostics are sound. Using a relaxed-clock model would mitigate the branch-length distortion, so comparing its results to the strict-clock results is a valid check for robustness. The predicted positive correlation between estimated local rates and GMYC species counts is also a direct and testable consequence of the described bias. **This statement is correct.**\n\n**B.** The first part of the statement correctly describes the effect of low sampling on the density of coalescent events. The claim that this biases the threshold $t^*$ toward older times, causing lumping (a reduction in splitting), is also consistent with known behavior of the GMYC model. However, the proposed diagnostic is flawed. Rarefaction is a technique to check for sampling artifacts by equalizing sampling effort. If sampling density is a source of bias, then rarefying the densely sampled clade $\\mathcal{B}$ from $n_{\\mathcal{B}}=10$ down to $n_{\\mathcal{B}}=2$ should cause the GMYC results for that clade to change (specifically, it should induce lumping, reducing the species count). A diagnostic would be to check if the result *changes* in this predictable manner. The statement proposes a diagnostic where one must \"confirm that GMYC species counts are unchanged,\" which is the opposite of the expected outcome if sampling bias were present. Therefore, the diagnostic procedure is nonsensical. **This statement is incorrect.**\n\n**C.** This statement is fundamentally flawed. The GMYC method is explicitly based on the analysis of branch lengths (waiting times) in an ultrametric tree. It partitions the nodes of the tree based on their age, comparing the fit of a Yule process for nodes older than a threshold time $t^*$ and a coalescent process for nodes younger than $t^*$. Without branch lengths, the method cannot operate. Since GMYC is dependent on branch lengths, and rate heterogeneity distorts these branch lengths, rate heterogeneity is a major potential source of bias. Consequently, assessing the fit of the clock model is a critical diagnostic step. **This statement is incorrect.**\n\n**D.** This statement accurately describes a posterior predictive simulation (PPS), a powerful and standard statistical procedure for model assessment. The procedure involves simulating replicate datasets under a complex generative model that accounts for the suspected sources of error (here, rate heterogeneity via a relaxed-clock, and the specific sampling scheme). These simulated datasets are then analyzed with the same pipeline used for the original data. This generates a distribution of expected outcomes (e.g., number of GMYC species) under the model. If the observed outcome from the real data is an outlier with respect to this distribution, it indicates that the model fails to capture some essential aspect of the data-generating process. This is a valid and robust method for flagging biases. **This statement is correct.**\n\n**E.** This statement is based on a false premise. The Poisson Tree Processes (PTP) model works on a non-ultrametric tree where branch lengths are in units of substitutions per site. It is not \"invariant to rate variation\". In fact, rate variation directly affects its input data. A lineage with a high substitution rate will have longer branches in substitution units than a lineage with a low substitution rate, even over the same time period. PTP models the distribution of these substitution-length branches to infer species boundaries. Significant rate heterogeneity can therefore mislead PTP. Because PTP itself is sensitive to rate heterogeneity, a discrepancy between GMYC and PTP results is not an unambiguous signal of GMYC-specific bias; it could be due to PTP's own biases, or the different fundamental assumptions of the two models. **This statement is incorrect.**", "answer": "$$\\boxed{AD}$$", "id": "2752763"}]}