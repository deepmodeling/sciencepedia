{"hands_on_practices": [{"introduction": "Detecting historical gene flow is a cornerstone of modern evolutionary biology. This exercise introduces the D-statistic, a powerful method for identifying introgression by measuring asymmetry in allele sharing patterns among four taxa. You will not only compute this crucial statistic but also assess its statistical significance using the block-jackknife procedure, a robust technique that accounts for the non-independence of sites along a chromosome [@problem_id:2544491].", "problem": "A four-taxon test for adaptive introgression is conducted using genome-wide biallelic Single Nucleotide Polymorphisms (SNPs) arranged into non-overlapping physical blocks to account for linkage. The taxa are arranged as $\\left(P_{1},P_{2},P_{3};O\\right)$, where $O$ is an outgroup. Under the null hypothesis of no introgression and only Incomplete Lineage Sorting (ILS), the expected counts of the site-patterns $\\mathrm{ABBA}$ and $\\mathrm{BABA}$ are equal. The D-statistic quantifies asymmetry in these counts. The genome is partitioned into $B=50$ equal-length blocks for block-resampling.\n\nAcross the full dataset, the total counts are $\\mathrm{ABBA}=320$ and $\\mathrm{BABA}=280$. The delete-one block-jackknife (leaving out one block at a time) is performed to assess uncertainty. Let $\\hat{D}_{(-b)}$ denote the D-statistic computed from the data with block $b$ removed, and let $\\bar{D}_{\\mathrm{LOO}}=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{D}_{(-b)}$ be the mean of the leave-one-out estimates. From the $B=50$ leave-one-out replicates, the sum of squared deviations is observed to be\n$$\n\\sum_{b=1}^{B}\\left(\\hat{D}_{(-b)}-\\bar{D}_{\\mathrm{LOO}}\\right)^{2}=2.296\\times 10^{-4}.\n$$\n\nUsing only fundamental definitions underlying the $\\mathrm{ABBA}$–$\\mathrm{BABA}$ framework and the delete-one jackknife, do the following:\n- Derive the expression for the genome-wide D-statistic from the total counts and compute its value.\n- Starting from the definition of the delete-one jackknife variance estimator, derive the estimator in terms of $B$ and the leave-one-out deviations, then compute the jackknife standard error and the corresponding $Z$-score for testing the null hypothesis that $D=0$.\n\nReport your final answer as a row matrix $\\left[D,\\;Z\\right]$, where both entries are rounded to four significant figures and expressed as pure decimals (no units).", "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains all necessary information for a complete solution. It describes a standard application of the D-statistic (ABBA-BABA test) combined with a block jackknife procedure for variance estimation, which are established methods in population genomics. The provided data are consistent and plausible. We will therefore proceed with a full derivation and computation.\n\nFirst, we derive and compute the genome-wide D-statistic, denoted as $\\hat{D}$. This statistic quantifies the asymmetry between the counts of two alternative gene tree topologies, represented by $\\mathrm{ABBA}$ and $\\mathrm{BABA}$ site patterns. Let $n_{\\mathrm{ABBA}}$ and $n_{\\mathrm{BABA}}$ be the total counts of these patterns over the entire genome. The D-statistic is defined as the normalized difference of these counts:\n$$\n\\hat{D} = \\frac{n_{\\mathrm{ABBA}} - n_{\\mathrm{BABA}}}{n_{\\mathrm{ABBA}} + n_{\\mathrm{BABA}}}\n$$\nThe problem provides the total counts: $n_{\\mathrm{ABBA}} = 320$ and $n_{\\mathrm{BABA}} = 280$. Substituting these values into the formula yields:\n$$\n\\hat{D} = \\frac{320 - 280}{320 + 280} = \\frac{40}{600} = \\frac{1}{15}\n$$\nThe numerical value is $\\hat{D} \\approx 0.066667$.\n\nNext, we address the uncertainty estimation and hypothesis testing using the delete-one block jackknife. We are asked to derive the variance estimator. Let $\\hat{D}$ be the statistic computed from the full dataset of $B$ blocks, and let $\\hat{D}_{(-b)}$ be the statistic recomputed with block $b$ omitted. The jackknife pseudo-values, $J_b$, are defined for each block $b \\in \\{1, 2, ..., B\\}$ as:\n$$\nJ_b = B \\hat{D} - (B-1)\\hat{D}_{(-b)}\n$$\nThe jackknife variance estimator for $\\hat{D}$ is given by the sample variance of the pseudo-values divided by the number of blocks $B$:\n$$\n\\widehat{\\mathrm{Var}}_{J}(\\hat{D}) = \\frac{1}{B(B-1)} \\sum_{b=1}^{B} (J_b - \\bar{J})^2\n$$\nwhere $\\bar{J}$ is the mean of the pseudo-values, $\\bar{J} = \\frac{1}{B}\\sum_{b=1}^{B} J_b$. To proceed, we express this variance in terms of the leave-one-out estimates $\\hat{D}_{(-b)}$. First, we find the mean pseudo-value $\\bar{J}$:\n$$\n\\bar{J} = \\frac{1}{B} \\sum_{b=1}^{B} [B \\hat{D} - (B-1)\\hat{D}_{(-b)}] = B \\hat{D} - \\frac{B-1}{B} \\sum_{b=1}^{B} \\hat{D}_{(-b)} = B \\hat{D} - (B-1)\\bar{D}_{\\mathrm{LOO}}\n$$\nwhere $\\bar{D}_{\\mathrm{LOO}} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{D}_{(-b)}$ is the mean of the leave-one-out estimates. The deviation of a single pseudo-value from the mean is:\n$$\nJ_b - \\bar{J} = \\left[B \\hat{D} - (B-1)\\hat{D}_{(-b)}\\right] - \\left[B \\hat{D} - (B-1)\\bar{D}_{\\mathrm{LOO}}\\right] = -(B-1)(\\hat{D}_{(-b)} - \\bar{D}_{\\mathrm{LOO}})\n$$\nSubstituting this result into the variance formula:\n$$\n\\widehat{\\mathrm{Var}}_{J}(\\hat{D}) = \\frac{1}{B(B-1)} \\sum_{b=1}^{B} \\left[ -(B-1)(\\hat{D}_{(-b)} - \\bar{D}_{\\mathrm{LOO}}) \\right]^2 = \\frac{(B-1)^2}{B(B-1)} \\sum_{b=1}^{B} (\\hat{D}_{(-b)} - \\bar{D}_{\\mathrm{LOO}})^2\n$$\nThis simplifies to the desired expression for the delete-one jackknife variance estimator:\n$$\n\\widehat{\\mathrm{Var}}_{J}(\\hat{D}) = \\frac{B-1}{B} \\sum_{b=1}^{B} (\\hat{D}_{(-b)} - \\bar{D}_{\\mathrm{LOO}})^2\n$$\nWith this derived, we can now compute its value. We are given $B=50$ and the sum of squared deviations $\\sum_{b=1}^{B}\\left(\\hat{D}_{(-b)}-\\bar{D}_{\\mathrm{LOO}}\\right)^{2}=2.296\\times 10^{-4}$.\n$$\n\\widehat{\\mathrm{Var}}_{J}(\\hat{D}) = \\frac{50-1}{50} (2.296 \\times 10^{-4}) = \\frac{49}{50} (2.296 \\times 10^{-4}) = 0.98 \\times 2.296 \\times 10^{-4} = 2.25008 \\times 10^{-4}\n$$\nThe jackknife standard error, $\\mathrm{SE}_J(\\hat{D})$, is the square root of the variance:\n$$\n\\mathrm{SE}_J(\\hat{D}) = \\sqrt{\\widehat{\\mathrm{Var}}_{J}(\\hat{D})} = \\sqrt{2.25008 \\times 10^{-4}} \\approx 0.01500027\n$$\nFinally, we calculate the $Z$-score for testing the null hypothesis $H_0: D=0$. The $Z$-score is the ratio of the estimated statistic to its standard error:\n$$\nZ = \\frac{\\hat{D} - D_{H_0}}{\\mathrm{SE}_J(\\hat{D})} = \\frac{\\hat{D}}{\\mathrm{SE}_J(\\hat{D})}\n$$\nUsing the computed values for $\\hat{D}$ and $\\mathrm{SE}_J(\\hat{D})$:\n$$\nZ = \\frac{1/15}{0.01500027} \\approx \\frac{0.066666...}{0.01500027} \\approx 4.44439\n$$\nThe problem requires the final answer to be reported as a row matrix $[D, Z]$ with both values rounded to four significant figures.\n$D = 0.066666... \\approx 0.06667$\n$Z = 4.44439... \\approx 4.444$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.06667 & 4.444\n\\end{pmatrix}\n}\n$$", "id": "2544491"}, {"introduction": "Once an advantageous allele has introgressed into a new population, its subsequent fate is governed by natural selection. This problem challenges you to model this process from first principles, deriving the classic logistic growth trajectory for an allele under positive selection. Mastering this fundamental model provides crucial intuition for how quickly an adaptive trait can spread and the key parameters that influence its dynamics [@problem_id:2544560].", "problem": "An introgressed allele $A$ conferring high-altitude tolerance has entered two distinct taxa through adaptive introgression: a montane mammal and an alpine grass. After a brief hybridization pulse at time $t=0$, the allele is present in the recipient population at frequency $p_{0} \\in (0,1)$. Thereafter, the population is large, randomly mating, and closed (no migration), so that genetic drift and further gene flow are negligible. Assume that selection acts additively on Malthusian fitness in continuous time: each copy of allele $A$ contributes an additive increase $s>0$ to the per-capita growth rate, so that genotype Malthusian fitnesses are $r_{AA}=2s$, $r_{Aa}=s$, and $r_{aa}=0$. Assume Hardy–Weinberg genotype proportions at each instant due to random mating and that the continuous-time deterministic selection dynamics apply.\n\nUsing only these assumptions and the definition of the continuous-time selection (replicator) dynamics for genotype frequencies—namely that genotype frequency $x_{i}$ changes according to $\\dot{x}_{i}=x_{i}(r_{i}-\\bar{r})$, where $\\bar{r}$ is the mean Malthusian fitness—derive from first principles the expected allele frequency trajectory $p(t)$ for $t \\ge 0$ with initial condition $p(0)=p_{0}$. Then, define the time to half-fixation $t_{1/2}$ as the time at which $p(t)=\\frac{1}{2}$, and determine $t_{1/2}$ in terms of $s$ and $p_{0}$. Treat time $t$ in units of generations and $s$ as a per-generation Malthusian selection coefficient. Provide your final answers as closed-form analytic expressions for $p(t)$ and $t_{1/2}$. No numerical rounding is required.", "solution": "The task is to derive the trajectory of the allele frequency $p(t)$ from first principles. We are given the dynamics for genotype frequencies, so we must first relate the change in allele frequency, $\\dot{p}$, to the change in genotype frequencies.\n\nThe frequency of allele $A$, denoted by $p$, is related to the frequencies of genotypes $AA$ and $Aa$ (denoted $x_{AA}$ and $x_{Aa}$, respectively) by the relation:\n$$\np = x_{AA} + \\frac{1}{2}x_{Aa}\n$$\nThe problem assumes Hardy-Weinberg proportions hold at all times due to random mating. This means the genotype frequencies can be expressed in terms of the allele frequency $p$ and its alternative $q=1-p$:\n$$\nx_{AA} = p^2 \\quad , \\quad x_{Aa} = 2p(1-p) \\quad , \\quad x_{aa} = (1-p)^2\n$$\nThe mean Malthusian fitness of the population, $\\bar{r}$, is the average of the genotype fitnesses weighted by their frequencies:\n$$\n\\bar{r} = x_{AA}r_{AA} + x_{Aa}r_{Aa} + x_{aa}r_{aa}\n$$\nSubstituting the Hardy-Weinberg proportions and the given fitness values ($r_{AA}=2s$, $r_{Aa}=s$, $r_{aa}=0$):\n$$\n\\bar{r} = p^2(2s) + 2p(1-p)(s) + (1-p)^2(0) = 2sp^2 + 2sp - 2sp^2 = 2sp\n$$\nThis linear relationship between mean fitness and allele frequency is a characteristic property of additive selection.\n\nNow, we derive the differential equation for $p(t)$. We differentiate the expression for $p$ with respect to time $t$:\n$$\n\\dot{p} = \\frac{dp}{dt} = \\frac{d}{dt}\\left(x_{AA} + \\frac{1}{2}x_{Aa}\\right) = \\dot{x}_{AA} + \\frac{1}{2}\\dot{x}_{Aa}\n$$\nWe substitute the given replicator dynamics, $\\dot{x}_{i}=x_{i}(r_{i}-\\bar{r})$:\n$$\n\\dot{p} = x_{AA}(r_{AA} - \\bar{r}) + \\frac{1}{2}x_{Aa}(r_{Aa} - \\bar{r})\n$$\nNow, substitute the Hardy-Weinberg proportions and the expressions for fitnesses and mean fitness:\n$$\n\\dot{p} = p^2(2s - 2sp) + \\frac{1}{2}[2p(1-p)](s - 2sp)\n$$\nFactor out common terms:\n$$\n\\dot{p} = p^2(2s(1-p)) + p(1-p)(s(1-2p))\n$$\nWe can factor out $sp(1-p)$ from both terms:\n$$\n\\dot{p} = sp(1-p)[2p + (1-2p)]\n$$\nThe term in the square brackets simplifies to $1$:\n$$\n\\dot{p} = sp(1-p)\n$$\nThis is the logistic differential equation for the allele frequency under additive selection. To find $p(t)$, we must solve this equation with the initial condition $p(0)=p_0$. This is a separable equation:\n$$\n\\frac{dp}{p(1-p)} = s \\, dt\n$$\nWe integrate both sides. The left-hand side is integrated using partial fraction decomposition. We let $\\frac{1}{p(1-p)} = \\frac{C_1}{p} + \\frac{C_2}{1-p}$. This gives $1 = C_1(1-p) + C_2 p$, which yields $C_1=1$ and $C_2=1$.\n$$\n\\int \\left(\\frac{1}{p} + \\frac{1}{1-p}\\right) dp = \\int s \\, dt\n$$\nPerforming the integration gives:\n$$\n\\ln|p| - \\ln|1-p| = st + K\n$$\nwhere $K$ is the constant of integration. Since $p \\in (0,1)$, the absolute values are not necessary.\n$$\n\\ln\\left(\\frac{p}{1-p}\\right) = st + K\n$$\nThis quantity, $\\ln(p/(1-p))$, is the logit of the frequency $p$. To find $K$, we apply the initial condition $p(0)=p_0$:\n$$\n\\ln\\left(\\frac{p_0}{1-p_0}\\right) = s(0) + K \\implies K = \\ln\\left(\\frac{p_0}{1-p_0}\\right)\n$$\nSubstituting $K$ back into the equation:\n$$\n\\ln\\left(\\frac{p(t)}{1-p(t)}\\right) = st + \\ln\\left(\\frac{p_0}{1-p_0}\\right)\n$$\nTo solve for $p(t)$, we first exponentiate both sides:\n$$\n\\frac{p(t)}{1-p(t)} = \\exp\\left(st + \\ln\\left(\\frac{p_0}{1-p_0}\\right)\\right) = \\exp(st) \\exp\\left(\\ln\\left(\\frac{p_0}{1-p_0}\\right)\\right) = \\frac{p_0}{1-p_0}\\exp(st)\n$$\nLet this right-hand side be $Z(t)$. Then $p(t) = Z(t)(1-p(t)) = Z(t) - Z(t)p(t)$.\n$$\np(t)(1+Z(t)) = Z(t) \\implies p(t) = \\frac{Z(t)}{1+Z(t)}\n$$\nSubstituting the expression for $Z(t)$:\n$$\np(t) = \\frac{\\frac{p_0}{1-p_0}\\exp(st)}{1 + \\frac{p_0}{1-p_0}\\exp(st)}\n$$\nMultiplying the numerator and denominator by $(1-p_0)$ simplifies the expression to its final form:\n$$\np(t) = \\frac{p_0 \\exp(st)}{(1-p_0) + p_0 \\exp(st)}\n$$\nThis is the first required result.\n\nNext, we must find the time to half-fixation, $t_{1/2}$, defined as the time when $p(t_{1/2}) = \\frac{1}{2}$. We set our solution for $p(t)$ equal to $\\frac{1}{2}$:\n$$\n\\frac{1}{2} = \\frac{p_0 \\exp(st_{1/2})}{(1-p_0) + p_0 \\exp(st_{1/2})}\n$$\nCross-multiplying gives:\n$$\n(1-p_0) + p_0 \\exp(st_{1/2}) = 2 p_0 \\exp(st_{1/2})\n$$\nRearranging to solve for the exponential term:\n$$\n1-p_0 = 2 p_0 \\exp(st_{1/2}) - p_0 \\exp(st_{1/2}) = p_0 \\exp(st_{1/2})\n$$\n$$\n\\exp(st_{1/2}) = \\frac{1-p_0}{p_0}\n$$\nTaking the natural logarithm of both sides:\n$$\nst_{1/2} = \\ln\\left(\\frac{1-p_0}{p_0}\\right)\n$$\nFinally, solving for $t_{1/2}$:\n$$\nt_{1/2} = \\frac{1}{s}\\ln\\left(\\frac{1-p_0}{p_0}\\right)\n$$\nThis expression can also be written as $t_{1/2} = -\\frac{1}{s}\\ln\\left(\\frac{p_0}{1-p_0}\\right)$. Note that for $p_0  \\frac{1}{2}$, the argument of the logarithm is greater than $1$, so $t_{1/2} > 0$. If $p_0 = \\frac{1}{2}$, $t_{1/2} = 0$, as expected. This completes the derivation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{p_0 \\exp(st)}{(1-p_0) + p_0 \\exp(st)}  \\frac{1}{s}\\ln\\left(\\frac{1-p_0}{p_0}\\right)\n\\end{pmatrix}\n}\n$$", "id": "2544560"}, {"introduction": "While summary statistics can signal introgression, a more rigorous approach involves formally comparing explicit evolutionary models. This practice delves into statistical model selection, where you will contrast a pure-tree model (representing vertical descent only) with a network model that allows for introgression. By implementing Bayesian model comparison via Bayes factors and information-theoretic criteria like AIC and BIC, you will learn how to quantify the evidence for competing evolutionary histories [@problem_id:2544541].", "problem": "You are given independent counts of three site-pattern categories that summarize biallelic variation across four taxa arranged as three ingroups and one outgroup in a comparative zoology and botany context. The categories are labeled as follows: $n_{\\mathrm{ABBA}}$ records the count of derived allele sharing between the first and third ingroup species, $n_{\\mathrm{BABA}}$ records the count of derived allele sharing between the second and third ingroup species, and $n_{\\mathrm{OTHER}}$ aggregates all remaining patterns. Let $N = n_{\\mathrm{ABBA}} + n_{\\mathrm{BABA}} + n_{\\mathrm{OTHER}}$ be the total number of sites.\n\nWe want to compare two probabilistic models for these counts under the following modeling assumptions grounded in standard phylogenetic and statistical theory:\n\n- The counts arise from a multinomial distribution over three categories with category probabilities denoted by $p_{\\mathrm{ABBA}}$, $p_{\\mathrm{BABA}}$, and $p_{\\mathrm{OTHER}}$ that sum to $1$.\n- A pure tree model ($M_0$) imposes symmetry between the two discordant site-pattern categories consistent with incomplete lineage sorting, so that $p_{\\mathrm{ABBA}} = p_{\\mathrm{BABA}} = s$ and $p_{\\mathrm{OTHER}} = 1 - 2s$, with $s \\in [0, 1/2]$.\n- A single-reticulation network model ($M_1$) allows asymmetry between the two discordant categories attributable to adaptive introgression. It is parameterized as $p_{\\mathrm{ABBA}} = s(1 + \\rho)$, $p_{\\mathrm{BABA}} = s(1 - \\rho)$, and $p_{\\mathrm{OTHER}} = 1 - 2s$, with $s \\in [0, 1/2]$ and $\\rho \\in [-1, 1]$.\n- For Bayesian model comparison, use a prior on $s$ defined by $u = 2s \\sim \\mathrm{Beta}(\\alpha, \\beta)$ with $\\alpha = 1$ and $\\beta = 1$, and a prior on $\\rho$ that is uniform on the interval $[-1, 1]$. Under $M_0$ the asymmetry parameter is fixed at $\\rho = 0$.\n- For information-criterion model comparison, treat $M_0$ as having $k_0 = 1$ free parameter and $M_1$ as having $k_1 = 2$ free parameters.\n\nYour task is to implement a program that, for each test case, computes the following three quantities:\n\n1. The natural logarithm of the Bayes factor comparing the network model to the tree model, defined as $\\log \\mathrm{BF}_{10} = \\log \\left(\\dfrac{\\mathcal{Z}_1}{\\mathcal{Z}_0}\\right)$, where $\\mathcal{Z}_m$ denotes the marginal likelihood of model $M_m$ obtained by integrating the likelihood over the model’s parameters with the specified priors.\n2. The difference in the Bayesian Information Criterion (BIC) between the models, defined as $\\Delta \\mathrm{BIC} = \\mathrm{BIC}_1 - \\mathrm{BIC}_0$, where $\\mathrm{BIC}_m = k_m \\log N - 2 \\log \\hat{L}_m$ and $\\log \\hat{L}_m$ is the maximized log-likelihood under model $M_m$. The logarithm here is the natural logarithm, and $N$ is the total number of sites. A negative value of $\\Delta \\mathrm{BIC}$ favors the network model.\n3. The difference in the Akaike Information Criterion (AIC) between the models, defined as $\\Delta \\mathrm{AIC} = \\mathrm{AIC}_1 - \\mathrm{AIC}_0$, where $\\mathrm{AIC}_m = 2 k_m - 2 \\log \\hat{L}_m$. A negative value of $\\Delta \\mathrm{AIC}$ favors the network model.\n\nYou must start from the following fundamental base without shortcut formulas:\n\n- The multinomial model for counts with category probabilities that sum to $1$.\n- The definition of marginal likelihood as the integral of the likelihood times the prior over the parameter space.\n- The definition of Bayes factor as the ratio of marginal likelihoods.\n- The definitions of Maximum Likelihood Estimation (MLE), the Akaike Information Criterion (AIC), and the Bayesian Information Criterion (BIC).\n\nImplement a fully self-contained algorithm to evaluate these quantities exactly and stably for the given models and priors. Your implementation should be general and numerically robust for any nonnegative integer inputs $n_{\\mathrm{ABBA}}$, $n_{\\mathrm{BABA}}$, $n_{\\mathrm{OTHER}}$ that are jointly consistent.\n\nTest Suite:\nEvaluate your program on the following parameter sets, where each test case is a tuple $(n_{\\mathrm{ABBA}}, n_{\\mathrm{BABA}}, n_{\\mathrm{OTHER}})$:\n\n- Test case $1$: $(50, 48, 902)$\n- Test case $2$: $(90, 10, 900)$\n- Test case $3$: $(5, 0, 95)$\n- Test case $4$: $(1, 1, 1)$\n- Test case $5$: $(10, 10, 80)$\n\nFinal Output Format:\nYour program should produce a single line of output containing a Python-style list of results, one per test case, where each result is a list of three real numbers in the order $[\\log \\mathrm{BF}_{10}, \\Delta \\mathrm{BIC}, \\Delta \\mathrm{AIC}]$. For example: \"[[x11,x12,x13],[x21,x22,x23],...]\" where each $x_{ij}$ is a real number. No additional text should be printed.", "solution": "To compute the required metrics, we first derive the analytical expressions for the marginal likelihoods and the maximized log-likelihoods for both models, $M_0$ and $M_1$. Let the counts be denoted by $n_A = n_{\\mathrm{ABBA}}$, $n_B = n_{\\mathrm{BABA}}$, and $n_O = n_{\\mathrm{OTHER}}$. The total number of sites is $N = n_A + n_B + n_O$. The likelihood for the counts $(n_A, n_B, n_O)$ under a multinomial model is given by:\n$$L(p_A, p_B, p_O | n_A, n_B, n_O) = \\frac{N!}{n_A! n_B! n_O!} p_A^{n_A} p_B^{n_B} p_O^{n_O}$$\nThe multinomial coefficient is a constant with respect to the parameters and will be omitted in proportional likelihood expressions, as it cancels out in likelihood ratios and Bayes factors.\n\n**Analysis of Model $M_0$ (Pure Tree)**\n\nUnder $M_0$, the probabilities are $p_A = s$, $p_B = s$, and $p_O = 1 - 2s$. The likelihood is:\n$$L_0(s | \\text{data}) \\propto s^{n_A} s^{n_B} (1-2s)^{n_O} = s^{n_A+n_B} (1-2s)^{n_O}$$\nThe prior for $s$ is derived from $u = 2s \\sim \\mathrm{Beta}(1, 1)$, which is a uniform distribution on $[0, 1]$. The PDF is $f_U(u) = 1$ for $u \\in [0, 1]$. By change of variables, the PDF for $s$ is $p_S(s) = f_U(2s) |\\frac{d(2s)}{ds}| = 1 \\cdot 2 = 2$ for $s \\in [0, 1/2]$.\n\nThe marginal likelihood $\\mathcal{Z}_0$ is the integral of the likelihood times the prior over the parameter space:\n$$\\mathcal{Z}_0 = \\int_0^{1/2} L_0(s | \\text{data}) p(s) ds = \\int_0^{1/2} C \\cdot s^{n_A+n_B} (1-2s)^{n_O} \\cdot 2 \\, ds$$\nwhere $C = \\frac{N!}{n_A! n_B! n_O!}$. Let $u = 2s$, so $ds = du/2$. The integral becomes:\n$$\\mathcal{Z}_0 = C \\int_0^{1} \\left(\\frac{u}{2}\\right)^{n_A+n_B} (1-u)^{n_O} \\cdot 2 \\cdot \\frac{du}{2} = C \\left(\\frac{1}{2}\\right)^{n_A+n_B} \\int_0^{1} u^{n_A+n_B} (1-u)^{n_O} du$$\nThe integral is the Beta function $B(n_A+n_B+1, n_O+1)$.\n$$\\mathcal{Z}_0 = C \\left(\\frac{1}{2}\\right)^{n_A+n_B} B(n_A+n_B+1, n_O+1)$$\n\nFor BIC and AIC, we need the maximized log-likelihood. The log-likelihood kernel is $\\log L_0(s) = (n_A+n_B)\\log s + n_O \\log(1-2s)$. Setting the derivative with respect to $s$ to zero yields the MLE:\n$$\\frac{d \\log L_0}{ds} = \\frac{n_A+n_B}{s} - \\frac{2n_O}{1-2s} = 0 \\implies \\hat{s}_0 = \\frac{n_A+n_B}{2N}$$\nThe maximized log-likelihood (kernel) is $\\log \\hat{L}_0 = (n_A+n_B) \\log \\hat{s}_0 + n_O \\log(1-2\\hat{s}_0)$.\n\n**Analysis of Model $M_1$ (Network)**\n\nUnder $M_1$, $p_A = s(1+\\rho)$, $p_B = s(1-\\rho)$, $p_O = 1-2s$. The likelihood is:\n$$L_1(s, \\rho | \\text{data}) \\propto [s(1+\\rho)]^{n_A} [s(1-\\rho)]^{n_B} (1-2s)^{n_O} = s^{n_A+n_B} (1-2s)^{n_O} (1+\\rho)^{n_A} (1-\\rho)^{n_B}$$\nThe priors are $p(s)=2$ for $s \\in [0, 1/2]$ and $p(\\rho) = 1/2$ for $\\rho \\in [-1, 1]$. The joint prior is independent, $p(s, \\rho) = p(s)p(\\rho) = 2 \\cdot \\frac{1}{2} = 1$.\n\nThe marginal likelihood $\\mathcal{Z}_1$ is:\n$$\\mathcal{Z}_1 = \\int_{-1}^{1} \\int_{0}^{1/2} L_1(s, \\rho | \\text{data}) p(s, \\rho) ds d\\rho$$\nOmitting the constant C, the integrals are separable:\n$$\\mathcal{Z}_1 \\propto \\left( \\int_0^{1/2} s^{n_A+n_B} (1-2s)^{n_O} ds \\right) \\left( \\int_{-1}^{1} (1+\\rho)^{n_A} (1-\\rho)^{n_B} d\\rho \\right)$$\nThe first integral, using $u=2s$, is $\\left(\\frac{1}{2}\\right)^{n_A+n_B+1} B(n_A+n_B+1, n_O+1)$.\nThe second integral, using $v=(1+\\rho)/2$, is $2^{n_A+n_B+1} B(n_A+1, n_B+1)$.\nCombining these and re-introducing the constant $C$:\n$$\\mathcal{Z}_1 = C \\cdot B(n_A+n_B+1, n_O+1) \\cdot B(n_A+1, n_B+1)$$\n\nFor the MLE under $M_1$, we maximize $\\log L_1(s, \\rho)$. The parameters $s$ and $\\rho$ are separable in the likelihood function. Maximizing with respect to each independently gives:\n$$\\hat{s}_1 = \\frac{n_A+n_B}{2N} \\quad \\text{and} \\quad \\hat{\\rho}_1 = \\frac{n_A-n_B}{n_A+n_B}$$\nPlugging these into the probability definitions gives the unrestricted MLEs for a multinomial model: $\\hat{p}_A=n_A/N$, $\\hat{p}_B=n_B/N$, $\\hat{p}_O=n_O/N$.\nThe maximized log-likelihood (kernel) is $\\log \\hat{L}_1 = n_A \\log(n_A/N) + n_B \\log(n_B/N) + n_O \\log(n_O/N)$, where we adopt the convention $0 \\log 0 = 0$.\n\n**Final Formulas for Computation**\n\n1.  **Log Bayes Factor $\\log \\mathrm{BF}_{10}$**:\n    The Bayes factor is the ratio of marginal likelihoods. The constant $C$ cancels.\n    $$\\mathrm{BF}_{10} = \\frac{\\mathcal{Z}_1}{\\mathcal{Z}_0} = \\frac{B(n_A+n_B+1, n_O+1) \\cdot B(n_A+1, n_B+1)}{\\left(\\frac{1}{2}\\right)^{n_A+n_B} B(n_A+n_B+1, n_O+1)} = 2^{n_A+n_B} B(n_A+1, n_B+1)$$\n    In terms of logarithms, using $B(x,y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$:\n    $$\\log \\mathrm{BF}_{10} = (n_A+n_B)\\log 2 + \\log\\Gamma(n_A+1) + \\log\\Gamma(n_B+1) - \\log\\Gamma(n_A+n_B+2)$$\n\n2.  **$\\Delta \\mathrm{BIC}$**:\n    $$\\Delta \\mathrm{BIC} = \\mathrm{BIC}_1 - \\mathrm{BIC}_0 = (k_1 \\log N - 2\\log\\hat{L}_1) - (k_0 \\log N - 2\\log\\hat{L}_0)$$\n    With $k_1=2$ and $k_0=1$, this becomes $\\Delta \\mathrm{BIC} = \\log N - 2(\\log\\hat{L}_1 - \\log\\hat{L}_0)$.\n    The difference in maximized log-likelihood kernels is:\n    $$\\log\\hat{L}_1 - \\log\\hat{L}_0 = \\left(\\sum_{i \\in \\{A,B,O\\}} n_i \\log \\frac{n_i}{N}\\right) - \\left( (n_A+n_B)\\log\\frac{n_A+n_B}{2N} + n_O\\log\\frac{n_O}{N} \\right)$$\n    $$= n_A\\log n_A + n_B\\log n_B - (n_A+n_B)\\log(n_A+n_B) + (n_A+n_B)\\log 2$$\n    Let this difference be $\\Delta\\log L$.\n    $$\\Delta \\mathrm{BIC} = \\log N - 2 \\Delta\\log L$$\n\n3.  **$\\Delta \\mathrm{AIC}$**:\n    $$\\Delta \\mathrm{AIC} = \\mathrm{AIC}_1 - \\mathrm{AIC}_0 = (2k_1 - 2\\log\\hat{L}_1) - (2k_0 - 2\\log\\hat{L}_0)$$\n    $$\\Delta \\mathrm{AIC} = 2(k_1-k_0) - 2(\\log\\hat{L}_1 - \\log\\hat{L}_0) = 2 - 2 \\Delta\\log L$$\n\nThese formulas are numerically stable when implemented using log-gamma functions for the Bayes factor and careful handling of $n_i \\log n_i$ terms when $n_i=0$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for the given test suite.\n    \"\"\"\n\n    def calculate_metrics(n_abba, n_baba, n_other):\n        \"\"\"\n        Computes the three required model comparison metrics based on derived formulas.\n\n        Args:\n            n_abba (int): Count of ABBA site patterns.\n            n_baba (int): Count of BABA site patterns.\n            n_other (int): Count of other site patterns.\n\n        Returns:\n            list: A list containing [log_BF_10, delta_BIC, delta_AIC].\n        \"\"\"\n        # Ensure floating-point arithmetic for precision.\n        n_a = float(n_abba)\n        n_b = float(n_baba)\n        n_o = float(n_other)\n\n        n_ab = n_a + n_b\n        n_total = n_a + n_b + n_o\n\n        # --- 1. Log Bayes Factor (log BF_10) ---\n        # Derived formula: log(BF_10) = (n_a + n_b)*log(2) + log(B(n_a+1, n_b+1))\n        # where log(B(x,y)) is implemented using log-gamma functions:\n        # gammaln(x) + gammaln(y) - gammaln(x+y).\n        log_bf = (\n            n_ab * np.log(2.0)\n            + gammaln(n_a + 1.0)\n            + gammaln(n_b + 1.0)\n            - gammaln(n_ab + 2.0)\n        )\n\n        # --- 2. Delta Log-Likelihood (log L_hat_1 - log L_hat_0) ---\n        # Helper function for x*log(x) which handles the case x=0.\n        def xlogx(x):\n            if x = 1e-9:  # Handle x=0 case\n                return 0.0\n            return x * np.log(x)\n\n        # Derived formula for the difference in maximized log-likelihoods:\n        # delta_log_L = n_a*log(n_a) + n_b*log(n_b) - (n_a+n_b)*log(n_a+n_b) + (n_a+n_b)*log(2)\n        delta_log_L = (\n            xlogx(n_a)\n            + xlogx(n_b)\n            - xlogx(n_ab)\n            + n_ab * np.log(2.0)\n        )\n\n        # --- 3. Delta BIC ---\n        # Derived formula: delta_BIC = log(N) - 2 * delta_log_L\n        # where the penalty term is (k1-k0)*log(N) = (2-1)*log(N) = log(N).\n        if n_total = 1e-9: # Avoid log(0)\n             delta_bic = -2.0 * delta_log_L\n        else:\n             delta_bic = np.log(n_total) - 2.0 * delta_log_L\n\n        # --- 4. Delta AIC ---\n        # Derived formula: delta_AIC = 2 - 2 * delta_log_L\n        # where the penalty term is 2*(k1-k0) = 2*(2-1) = 2.\n        delta_aic = 2.0 - 2.0 * delta_log_L\n\n        return [log_bf, delta_bic, delta_aic]\n\n    # Test cases provided in the problem statement.\n    test_cases = [\n        (50, 48, 902),\n        (90, 10, 900),\n        (5, 0, 95),\n        (1, 1, 1),\n        (10, 10, 80),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_abba, n_baba, n_other = case\n        result = calculate_metrics(n_abba, n_baba, n_other)\n        results.append(result)\n\n    # The final output must be a single string representing a list of lists.\n    # e.g., \"[[x11,x12,x13],[x21,x22,x23],...]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2544541"}]}