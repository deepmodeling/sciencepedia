{"hands_on_practices": [{"introduction": "One of the most critical questions in evolutionary genetics is determining the ultimate fate of a new mutation. This practice guides you through the calculation of the fixation probability, a cornerstone of diffusion theory. By solving the backward Kolmogorov equation, you will derive Motoo Kimura's classic formula and apply it to a concrete scenario, gaining firsthand experience with how selection and genetic drift interact to shape an allele's chance of success [@problem_id:2700919].", "problem": "A diploid, randomly mating population of constant effective size $N_e$ follows a Wright–Fisher model with selection. Two alleles $A$ and $a$ segregate at a single locus. The relative viabilities of genotypes are $w_{aa}=1$, $w_{Aa}=1+h\\,s$, and $w_{AA}=1+s$, where $s>0$ and $0 \\leq h \\leq 1$ are constants. A single copy of allele $A$ arises in the population, so its initial frequency is $x_0=1/(2N_e)$. Assume selection is weak enough that the diffusion approximation is valid.\n\nUse the diffusion approximation with generator $L$ acting on sufficiently smooth functions $f$ of allele frequency $x$ by\n$$\nL f(x) \\equiv a(x)\\,f'(x) + \\frac{1}{2}\\,b(x)\\,f''(x),\n$$\nwhere the drift and diffusion coefficients per generation are the well-tested expressions\n$$\na(x)=s\\,x(1-x)\\big[h+(1-2h)\\,x\\big],\\qquad b(x)=\\frac{x(1-x)}{2N_e}.\n$$\nStarting from the backward Kolmogorov equation for the fixation probability $u(x)$, which satisfies $L u(x)=0$ with boundary conditions $u(0)=0$ and $u(1)=1$, derive $u(x)$ in closed form up to quadratures, then specialize to the additive case $h=\\tfrac{1}{2}$ to obtain an explicit expression. Evaluate the fixation probability $u(x_0)$ for parameters $N_e=5000$, $s=0.01$, $h=\\tfrac{1}{2}$, and $x_0=\\tfrac{1}{2N_e}$. Express your answer as a decimal and round your answer to five significant figures.", "solution": "The problem statement is validated against the required criteria.\nGivens are extracted as follows:\n- Population model: Diploid, randomly mating, constant effective size $N_e$, Wright–Fisher dynamics with selection.\n- Alleles: $A$ and $a$ at a single locus.\n- Relative viabilities: $w_{aa}=1$, $w_{Aa}=1+h\\,s$, $w_{AA}=1+s$.\n- Parameters: $s>0$, $0 \\leq h \\leq 1$.\n- Initial condition: Allele $A$ frequency is $x_0 = 1/(2N_e)$.\n- Approximation: Weak selection, diffusion approximation is valid.\n- Diffusion generator: $L f(x) = a(x)f'(x) + \\frac{1}{2}b(x)f''(x)$.\n- Drift coefficient: $a(x) = s\\,x(1-x)\\big[h+(1-2h)\\,x\\big]$.\n- Diffusion coefficient: $b(x) = \\frac{x(1-x)}{2N_e}$.\n- Fixation probability $u(x)$ satisfies the backward Kolmogorov equation $L u(x) = 0$ with boundary conditions $u(0)=0$ and $u(1)=1$.\n- Specific numerical parameters for evaluation: $N_e=5000$, $s=0.01$, $h=1/2$, $x_0=1/(2N_e)$.\n\nValidation verdict: The problem is scientifically grounded in established population genetics theory (Wright-Fisher model, diffusion approximation), is mathematically well-posed (a second-order ODE with two boundary conditions admitting a unique solution), and is objective and self-contained. All provided information is consistent and the parameters are realistic. The problem is valid. We proceed with the derivation.\n\nThe fixation probability $u(x)$ of allele $A$ starting from an initial frequency $x$ is given by the solution to the backward Kolmogorov equation $L u(x) = 0$, subject to the boundary conditions $u(0)=0$ (loss is certain if the allele is absent) and $u(1)=1$ (fixation is certain if the allele is the only one present).\nThe equation is:\n$$a(x) u'(x) + \\frac{1}{2}b(x) u''(x) = 0$$\nThis is a second-order linear ordinary differential equation. We can reduce its order by setting $v(x) = u'(x)$, which gives a first-order separable equation for $v(x)$:\n$$\\frac{1}{2}b(x) v'(x) = -a(x) v(x)$$\n$$\\frac{v'(x)}{v(x)} = -\\frac{2a(x)}{b(x)}$$\nWe define a function $\\Psi(x) = \\exp\\left(-\\int \\frac{2a(y)}{b(y)} dy\\right)$. Integrating the equation for $v(x)$ gives $\\ln(v(x)) = -\\int \\frac{2a(x)}{b(x)} dx + C_0$, which implies $v(x) = C_1 \\Psi(x)$ for some constant $C_1$.\nFirst, we compute the ratio $\\frac{2a(x)}{b(x)}$:\n$$\\frac{2a(x)}{b(x)} = \\frac{2 s\\,x(1-x)\\big[h+(1-2h)\\,x\\big]}{\\frac{x(1-x)}{2N_e}} = 4N_e s \\big[h+(1-2h)\\,x\\big]$$\nLet us integrate this term with respect to the allele frequency, which we denote by a dummy variable $y$:\n$$\\int \\frac{2a(y)}{b(y)} dy = \\int 4N_e s \\big[h+(1-2h)\\,y\\big] dy = 4N_e s \\left(hy + (1-2h)\\frac{y^2}{2}\\right)$$\nWe can set the constant of integration to zero as it will be absorbed into the multiplicative constant $C_1$.\nThus, $u'(x) = v(x) = C_1 \\exp\\left(-4N_e s \\left(hx + (1-2h)\\frac{x^2}{2}\\right)\\right)$.\nTo find $u(x)$, we integrate $u'(y)$ from $0$ to $x$:\n$$u(x) - u(0) = \\int_0^x u'(y) dy$$\nSince $u(0)=0$, we have:\n$$u(x) = C_1 \\int_0^x \\exp\\left(-4N_e s \\left(hy + (1-2h)\\frac{y^2}{2}\\right)\\right) dy$$\nTo determine the constant $C_1$, we apply the second boundary condition, $u(1)=1$:\n$$1 = u(1) = C_1 \\int_0^1 \\exp\\left(-4N_e s \\left(hy + (1-2h)\\frac{y^2}{2}\\right)\\right) dy$$\nThis gives $C_1 = \\left( \\int_0^1 \\exp\\left(-4N_e s \\left(hy + (1-2h)\\frac{y^2}{2}\\right)\\right) dy \\right)^{-1}$.\nSubstituting $C_1$ back, we obtain the general solution for the fixation probability in closed form up to quadratures, which is Kimura's celebrated formula:\n$$u(x) = \\frac{\\int_0^x \\exp\\left(-4N_e s \\left(hy + (1-2h)\\frac{y^2}{2}\\right)\\right) dy}{\\int_0^1 \\exp\\left(-4N_e s \\left(hy + (1-2h)\\frac{y^2}{2}\\right)\\right) dy}$$\n\nNext, we specialize to the additive case, where dominance coefficient $h=1/2$. For $h=1/2$, the term $(1-2h)$ becomes $1-2(1/2)=0$. The exponent in the integrands simplifies considerably:\n$$-4N_e s \\left(\\frac{1}{2}y + (0)\\frac{y^2}{2}\\right) = -2N_e s y$$\nThe expression for $u(x)$ becomes:\n$$u(x) = \\frac{\\int_0^x \\exp(-2N_e s y) dy}{\\int_0^1 \\exp(-2N_e s y) dy}$$\nThese are elementary integrals. Let $\\gamma = 2N_e s$.\nThe numerator is $\\int_0^x \\exp(-\\gamma y) dy = \\left[-\\frac{1}{\\gamma}\\exp(-\\gamma y)\\right]_0^x = -\\frac{1}{\\gamma}(\\exp(-\\gamma x) - 1) = \\frac{1 - \\exp(-\\gamma x)}{\\gamma}$.\nThe denominator is $\\int_0^1 \\exp(-\\gamma y) dy = \\left[-\\frac{1}{\\gamma}\\exp(-\\gamma y)\\right]_0^1 = -\\frac{1}{\\gamma}(\\exp(-\\gamma) - 1) = \\frac{1 - \\exp(-\\gamma)}{\\gamma}$.\nDividing the numerator by the denominator, the term $\\gamma$ cancels, and we obtain the explicit expression for the fixation probability in the additive case:\n$$u(x) = \\frac{1 - \\exp(-2N_e s x)}{1 - \\exp(-2N_e s)}$$\n\nFinally, we evaluate this expression for an initial frequency $x_0 = 1/(2N_e)$ and the given parameters $N_e=5000$ and $s=0.01$.\nSubstituting $x = x_0 = 1/(2N_e)$ into the formula for $u(x)$:\n$$u(x_0) = \\frac{1 - \\exp\\left(-2N_e s \\frac{1}{2N_e}\\right)}{1 - \\exp(-2N_e s)} = \\frac{1 - \\exp(-s)}{1 - \\exp(-2N_e s)}$$\nNow, we substitute the numerical values $s=0.01$ and $N_e=5000$:\n$$u(x_0) = \\frac{1 - \\exp(-0.01)}{1 - \\exp(-2 \\times 5000 \\times 0.01)} = \\frac{1 - \\exp(-0.01)}{1 - \\exp(-100)}$$\nWe calculate the values of the terms:\nThe numerator is $1 - \\exp(-0.01) \\approx 1 - 0.9900498337 = 0.0099501663$.\nThe term $\\exp(-100)$ in the denominator is exceedingly small, approximately $3.72 \\times 10^{-44}$.\nThus, the denominator $1 - \\exp(-100)$ is equal to $1$ for all practical purposes and up to many decimal places.\nThe fixation probability is therefore:\n$$u(x_0) \\approx 1 - \\exp(-0.01) \\approx 0.0099501663$$\nThe problem requires the answer to be rounded to five significant figures. The first non-zero digit is the first significant figure. The number is $0.0099501663...$. The first five significant figures are $9, 9, 5, 0, 1$. The sixth significant figure is $6$, which is greater than or equal to $5$, so we round up the fifth digit. The digit $1$ becomes $2$.\nThe rounded decimal value is $0.0099502$.", "answer": "$$\\boxed{0.0099502}$$", "id": "2700919"}, {"introduction": "Beyond the probability of fixation, the timescale of evolution is a quantity of major interest. This exercise delves into calculating the expected time until an allele is either lost or fixed, known as the mean time to absorption. You will also tackle the more subtle problem of finding the mean time to fixation for only those alleles that are destined to succeed, a calculation that requires the powerful technique of conditioning the diffusion process [@problem_id:2700930].", "problem": "Consider the Wright–Fisher (WF) diffusion approximation for a single biallelic locus in a randomly mating population of constant effective size $N$ with no mutation and no dominance. Let $X_{t} \\in [0,1]$ denote the allele frequency of allele $A$ at (continuous) time $t$ measured in generations. Under neutrality (no selection), the Kolmogorov backward operator acting on twice continuously differentiable test functions $f$ is\n$$\n\\mathcal{L} f(x) \\;=\\; \\frac{x(1-x)}{4N}\\, f^{\\prime\\prime}(x),\n$$\nwith absorbing boundaries at $x=0$ and $x=1$.\n\nStarting from this operator and fundamental definitions of absorption times for one-dimensional diffusions with absorbing boundaries, do the following:\n\n1) Derive the ordinary differential equation satisfied by the unconditional mean time to absorption $T(x)$, where $T(x)$ is the expected time for $X_{t}$, started at $X_{0}=x$, to hit either boundary $\\{0,1\\}$, and solve it subject to the appropriate boundary conditions.\n\n2) Let $u(x)$ be the fixation probability of allele $A$ when starting at $x$, and use the conditioning principle for Markov processes (Doob $h$-transform with $h=u$) to construct the backward operator of the process conditioned on eventual fixation at $x=1$. From this transformed operator, derive the differential equation for the mean time to fixation conditional on fixation, denoted $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x)$, and solve it with the appropriate boundary and regularity conditions to obtain a closed-form expression.\n\n3) Evaluate both expressions at $N = 1.0 \\times 10^{5}$ and initial allele frequency $x_{0} = 0.01$, and report the two numerical values (unconditional mean time to absorption, and mean time to fixation conditional on fixation) in units of generations. Round your answers to four significant figures.\n\nYour final answer must be a pair of numbers in the order $(T(x_{0}),\\, T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x_{0}))$.", "solution": "The problem as stated is subjected to validation against established principles of scientific and mathematical rigor.\n\nGivens are extracted verbatim:\n-   Model: Wright–Fisher (WF) diffusion approximation for a single biallelic locus.\n-   Population: Randomly mating, constant effective size $N$.\n-   Genetic parameters: No mutation, no dominance, no selection (neutrality).\n-   State variable: $X_{t} \\in [0,1]$ is the allele frequency at continuous time $t$ (in generations).\n-   Operator: The Kolmogorov backward operator is $\\mathcal{L} f(x) = \\frac{x(1-x)}{4N}\\, f^{\\prime\\prime}(x)$.\n-   Boundaries: Absorbing at $x=0$ and $x=1$.\n-   Quantities to derive:\n    1.  Unconditional mean time to absorption, $T(x)$.\n    2.  Mean time to fixation conditional on fixation, $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x)$.\n-   Numerical evaluation parameters: $N = 1.0 \\times 10^{5}$, initial frequency $x_{0} = 0.01$.\n\nValidation assessment:\nThe problem is scientifically grounded, being a classic problem in the field of population genetics based on the diffusion theory developed by Sewall Wright and Motoo Kimura. The provided Kolmogorov operator is correct for a neutral process when time is measured in generations. The problem is well-posed, objective, and contains sufficient information for a unique solution. The concepts of mean absorption time and conditional fixation time are standard and solvable using the provided operator and fundamental theory of stochastic processes. The problem is free of logical contradictions, factual errors, and ambiguity.\n\nConclusion of validation: The problem is valid. We proceed with the derivation and solution.\n\nPart 1: Unconditional mean time to absorption, $T(x)$\n\nThe unconditional mean time to absorption, $T(x)$, for a one-dimensional diffusion process started at $x$ is governed by the Dynkin formula, which leads to the ordinary differential equation (ODE):\n$$\n\\mathcal{L} T(x) = -1\n$$\nSubstituting the given operator $\\mathcal{L}$, we have:\n$$\n\\frac{x(1-x)}{4N} T^{\\prime\\prime}(x) = -1\n$$\nwhere $T^{\\prime\\prime}(x)$ is the second derivative of $T(x)$ with respect to $x$. This equation is valid for $x \\in (0,1)$.\nRearranging, we obtain:\n$$\nT^{\\prime\\prime}(x) = -\\frac{4N}{x(1-x)}\n$$\nThe boundaries at $x=0$ and $x=1$ are absorbing. If the process starts at an absorbing boundary, the time to absorption is $0$. Therefore, the boundary conditions are:\n$$\nT(0) = 0 \\quad \\text{and} \\quad T(1) = 0\n$$\nWe integrate $T^{\\prime\\prime}(x)$ once with respect to $x$. Using the partial fraction decomposition $\\frac{1}{x(1-x)} = \\frac{1}{x} + \\frac{1}{1-x}$:\n$$\nT^{\\prime}(x) = \\int -\\frac{4N}{x(1-x)} dx = -4N \\int \\left(\\frac{1}{x} + \\frac{1}{1-x}\\right) dx = -4N \\left(\\ln(x) - \\ln(1-x)\\right) + C_1\n$$\nIntegrating a second time:\n$$\nT(x) = \\int \\left(-4N(\\ln(x) - \\ln(1-x)) + C_1\\right) dx\n$$\nUsing the standard integral $\\int \\ln(u) du = u \\ln(u) - u$, we find:\n$$\nT(x) = -4N \\left[ (x\\ln(x) - x) - (-(1-x)\\ln(1-x) - x) \\right] + C_1 x + C_2\n$$\n$$\nT(x) = -4N \\left( x\\ln(x) + (1-x)\\ln(1-x) \\right) + C_1 x + C_2\n$$\nTo determine the constants $C_1$ and $C_2$, we apply the boundary conditions. We use the limits $\\lim_{u\\to 0^+} u\\ln(u) = 0$.\nAt $x=0$:\n$$\n\\lim_{x\\to 0^+} T(x) = -4N \\left( 0 + 1\\ln(1) \\right) + C_1(0) + C_2 = C_2\n$$\nFrom $T(0)=0$, we have $C_2=0$.\nAt $x=1$:\n$$\n\\lim_{x\\to 1^-} T(x) = -4N \\left( 1\\ln(1) + 0 \\right) + C_1(1) + C_2 = C_1 + C_2\n$$\nFrom $T(1)=0$ and $C_2=0$, we have $C_1=0$.\nThus, the unconditional mean time to absorption is:\n$$\nT(x) = -4N \\left( x \\ln(x) + (1-x)\\ln(1-x) \\right)\n$$\n\nPart 2: Mean time to fixation conditional on fixation, $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x)$\n\nFirst, we determine the fixation probability, $u(x)$, which satisfies the equation $\\mathcal{L} u(x) = 0$ with boundary conditions $u(0)=0$ (loss) and $u(1)=1$ (fixation).\n$$\n\\frac{x(1-x)}{4N} u^{\\prime\\prime}(x) = 0\n$$\nFor $x \\in (0,1)$, this implies $u^{\\prime\\prime}(x)=0$. Integrating twice yields $u(x) = C_1 x + C_2$.\nApplying the boundary conditions: $u(0)=0 \\implies C_2=0$, and $u(1)=1 \\implies C_1=1$.\nTherefore, the fixation probability is $u(x)=x$.\n\nTo find the dynamics conditional on fixation, we use the Doob $h$-transform with $h(x) = u(x) = x$. The backward operator for the conditioned process, $\\mathcal{L}^*$, acting on a test function $f(x)$, is given by:\n$$\n\\mathcal{L}^* f(x) = \\frac{1}{u(x)} \\mathcal{L} (u(x)f(x))\n$$\nLet's compute $\\mathcal{L}(x f(x))$:\nThe second derivative of $g(x) = x f(x)$ is $g^{\\prime\\prime}(x) = \\frac{d}{dx} (f(x) + xf^{\\prime}(x)) = 2f^{\\prime}(x) + xf^{\\prime\\prime}(x)$.\n$$\n\\mathcal{L}(x f(x)) = \\frac{x(1-x)}{4N} (2f^{\\prime}(x) + xf^{\\prime\\prime}(x))\n$$\nSubstituting this into the expression for $\\mathcal{L}^*$:\n$$\n\\mathcal{L}^* f(x) = \\frac{1}{x} \\left[ \\frac{x(1-x)}{4N} (2f^{\\prime}(x) + xf^{\\prime\\prime}(x)) \\right] = \\frac{x(1-x)}{4N} f^{\\prime\\prime}(x) + \\frac{1-x}{2N} f^{\\prime}(x)\n$$\nThe mean time to fixation, conditional on eventual fixation, $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x)$, satisfies $\\mathcal{L}^* T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x) = -1$.\n$$\n\\frac{x(1-x)}{4N} T_{\\mathrm{fix}\\mid \\mathrm{fix}}^{\\prime\\prime}(x) + \\frac{1-x}{2N} T_{\\mathrm{fix}\\mid \\mathrm{fix}}^{\\prime}(x) = -1\n$$\nFor $x \\in (0,1)$, we can multiply by $\\frac{4N}{1-x}$:\n$$\nx T_{\\mathrm{fix}\\mid \\mathrm{fix}}^{\\prime\\prime}(x) + 2 T_{\\mathrm{fix}\\mid \\mathrm{fix}}^{\\prime}(x) = -\\frac{4N}{1-x}\n$$\nThis is a first-order linear ODE for $v(x) = T_{\\mathrm{fix}\\mid \\mathrm{fix}}^{\\prime}(x)$. The left side can be written as $(x^2 v(x))' / x$. An integrating factor approach is more direct. Let $v(x) = T_{\\mathrm{fix}\\mid \\mathrm{fix}}'(x)$. The equation is $v' + \\frac{2}{x} v = -\\frac{4N}{x(1-x)}$. The integrating factor is $\\exp(\\int \\frac{2}{x} dx) = x^2$. Multiplying by $x^2$ gives:\n$$\nx^2 v'(x) + 2x v(x) = \\frac{d}{dx}(x^2 v(x)) = -\\frac{4Nx}{1-x} = 4N - \\frac{4N}{1-x}\n$$\nIntegrating with respect to $x$:\n$$\nx^2 v(x) = 4Nx + 4N \\ln(1-x) + C_1\n$$\n$$\nv(x) = T_{\\mathrm{fix}\\mid \\mathrm{fix}}^{\\prime}(x) = \\frac{4N}{x} + \\frac{4N\\ln(1-x)}{x^2} + \\frac{C_1}{x^2}\n$$\nThe conditioned process is absorbed at $x=1$ and the boundary at $x=0$ has become inaccessible (reflecting). The mean time $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x)$ must remain finite as $x \\to 0^+$. If $C_1 \\neq 0$, the term $C_1/x^2$ in the derivative would lead to a divergence of type $1/x$ in $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x)$ near $x=0$. Therefore, regularity requires $C_1=0$.\nSo, $T_{\\mathrm{fix}\\mid \\mathrm{fix}}^{\\prime}(x) = \\frac{4N}{x} + \\frac{4N\\ln(1-x)}{x^2}$.\nIntegrating to find $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x)$:\n$$\nT_{\\mathrm{fix}\\mid \\mathrm{fix}}(x) = \\int \\left(\\frac{4N}{x} + \\frac{4N\\ln(1-x)}{x^2}\\right) dx = 4N \\ln(x) + 4N \\int \\frac{\\ln(1-x)}{x^2} dx + C_2\n$$\nThe second integral is solved by parts: $\\int u dv = uv - \\int v du$. Let $u=\\ln(1-x)$ and $dv=x^{-2}dx$. Then $du = \\frac{-1}{1-x}dx$ and $v=-x^{-1}$.\n$$\n\\int \\frac{\\ln(1-x)}{x^2} dx = -\\frac{\\ln(1-x)}{x} - \\int \\frac{1}{x(1-x)} dx = -\\frac{\\ln(1-x)}{x} - \\ln(x) + \\ln(1-x)\n$$\nSubstituting this back:\n$$\nT_{\\mathrm{fix}\\mid \\mathrm{fix}}(x) = 4N \\ln(x) + 4N \\left( -\\frac{\\ln(1-x)}{x} - \\ln(x) + \\ln(1-x) \\right) + C_2\n$$\n$$\nT_{\\mathrm{fix}\\mid \\mathrm{fix}}(x) = -4N \\frac{\\ln(1-x)}{x} + 4N \\ln(1-x) + C_2 = -4N \\frac{1-x}{x} \\ln(1-x) + C_2\n$$\nThe boundary condition for the conditioned process is absorption at $x=1$, so $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(1)=0$.\nTaking the limit as $x \\to 1^-$:\n$$\n\\lim_{x\\to 1^-} T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x) = \\lim_{x\\to 1^-} \\left( -4N \\frac{1-x}{x} \\ln(1-x) \\right) + C_2 = 0 + C_2\n$$\nThis implies $C_2=0$.\nThe mean time to fixation, conditional on fixation, is:\n$$\nT_{\\mathrm{fix}\\mid \\mathrm{fix}}(x) = -4N \\frac{1-x}{x} \\ln(1-x)\n$$\n\nPart 3: Numerical Evaluation\n\nWe are given $N = 1.0 \\times 10^5$ and $x_0 = 0.01$.\n\n1) Unconditional mean time to absorption $T(x_0)$:\n$$\nT(0.01) = -4(1.0 \\times 10^5) \\left( 0.01 \\ln(0.01) + (1-0.01)\\ln(1-0.01) \\right)\n$$\n$$\nT(0.01) = -4.0 \\times 10^5 \\left( 0.01 \\ln(0.01) + 0.99 \\ln(0.99) \\right)\n$$\nUsing a calculator: $\\ln(0.01) \\approx -4.60517$ and $\\ln(0.99) \\approx -0.0100503$.\n$$\nT(0.01) \\approx -4.0 \\times 10^5 \\left( 0.01(-4.60517) + 0.99(-0.0100503) \\right)\n$$\n$$\nT(0.01) \\approx -4.0 \\times 10^5 \\left( -0.0460517 - 0.0099498 \\right) = -4.0 \\times 10^5(-0.0560015) \\approx 22400.6\n$$\nRounded to four significant figures, $T(x_0) = 2.240 \\times 10^4$ generations.\n\n2) Mean time to fixation conditional on fixation $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x_0)$:\n$$\nT_{\\mathrm{fix}\\mid \\mathrm{fix}}(0.01) = -4(1.0 \\times 10^5) \\frac{1-0.01}{0.01} \\ln(1-0.01)\n$$\n$$\nT_{\\mathrm{fix}\\mid \\mathrm{fix}}(0.01) = -4.0 \\times 10^5 \\frac{0.99}{0.01} \\ln(0.99) = -4.0 \\times 10^5 \\times 99 \\times \\ln(0.99)\n$$\n$$\nT_{\\mathrm{fix}\\mid \\mathrm{fix}}(0.01) \\approx -4.0 \\times 10^5 \\times 99 \\times (-0.0100503) \\approx 397993.3\n$$\nRounded to four significant figures, $T_{\\mathrm{fix}\\mid \\mathrm{fix}}(x_0) = 3.980 \\times 10^5$ generations.", "answer": "$$\n\\boxed{(2.240 \\times 10^4, 3.980 \\times 10^5)}\n$$", "id": "2700930"}, {"introduction": "The true power of theoretical models is realized when they are used to interpret empirical data. This advanced practice challenges you to bridge theory and statistical inference by using the stationary distribution of the Wright-Fisher diffusion to analyze a site frequency spectrum (SFS). You will implement a likelihood-based framework to estimate the strength of selection from genetic variation data, a core task in modern population genomics [@problem_id:2700942].", "problem": "You are asked to implement an inference engine based on the diffusion approximation for a bi-allelic locus evolving under the Wright–Fisher diffusion with genic selection and bidirectional mutation. Your task is to derive, implement, and evaluate a likelihood for the site frequency spectrum (SFS) using the stationary distribution of the diffusion process, and then compute a one-parameter maximum likelihood estimate (MLE) of the scaled selection parameter for a set of test cases.\n\nStart from the following fundamental base that is standard in population genetics:\n- Consider a Wright–Fisher diffusion for allele frequency $x \\in (0,1)$ of allele $\\mathsf{1}$ under genic selection coefficient $s$ and bidirectional mutation rates $u$ (from $\\mathsf{1}$ to $\\mathsf{0}$) and $v$ (from $\\mathsf{0}$ to $\\mathsf{1}$) in a population of effective size $N$. The infinitesimal mean and variance of the diffusion are $m(x) = s\\,x(1-x) + v(1-x) - u x$ and $V(x) = \\frac{x(1-x)}{2N}$, respectively.\n- The stationary density $\\pi(x)$ solves the stationary Fokker–Planck (FP) equation with zero probability flux at the boundaries. Use the standard general solution for a one-dimensional diffusion with drift $m(x)$ and variance $V(x)$ under zero-flux boundary conditions to obtain $\\pi(x)$.\n\nFrom this base, do all of the following:\n1) Derive the stationary density up to a normalizing constant, expressing it in terms of the scaled parameters $\\alpha = 4 N v$, $\\beta = 4 N u$, and $\\kappa = 4 N s$. Then, given a sample of size $n$ drawn from the population, derive the mixture representation for the sampling distribution over the count $i \\in \\{0,1,\\dots,n\\}$ of allele $\\mathsf{1}$ in the sample. The sampling probability $p_i(\\alpha,\\beta,\\kappa;n)$ must be expressed as an integral over $x \\in (0,1)$ of the binomial sampling kernel times the stationary density. Your derivation must be principled and must not assume the target formulas; it should start from the FP stationary solution form $p(x) \\propto \\frac{1}{V(x)} \\exp\\left(\\int^x \\frac{2 m(y)}{V(y)} \\, dy\\right)$ and proceed by algebraic manipulation. You may invoke well-tested special-function identities to evaluate integrals, but you must justify their applicability.\n2) Implement a numerically stable computation of the sampling probabilities $p_i(\\alpha,\\beta,\\kappa;n)$ for all $i \\in \\{0,1,\\dots,n\\}$ for given $(\\alpha,\\beta,\\kappa,n)$, either by:\n   - Using the identity for the integral $\\int_0^1 x^{a-1} (1-x)^{b-1} e^{\\kappa x} \\, dx$ in terms of the Beta function and Kummer’s confluent hypergeometric function, together with exact binomial coefficients, or\n   - High-accuracy quadrature that maintains at least $10^{-10}$ absolute accuracy across $x \\in (0,1)$ for all involved integrals.\n   In either approach, your implementation must avoid numerical underflow and overflow by operating in the logarithmic domain for the multiplicative terms, and by normalizing the unnormalized weights across $i$ to obtain a valid discrete probability vector that sums to $1$.\n3) Given observed SFS counts $X_0,\\dots,X_n$ with $\\sum_{i=0}^n X_i = L$ sites, define the log-likelihood $\\ell(\\kappa)$ as\n$$\n\\ell(\\kappa) \\;=\\; \\sum_{i=0}^{n} X_i \\,\\log p_i(\\alpha,\\beta,\\kappa;n),\n$$\nwhere $(\\alpha,\\beta,n)$ are known and fixed. Implement a robust one-dimensional numerical optimization that finds the maximizer $\\widehat{\\kappa}$ over the closed interval $[\\kappa_{\\min},\\kappa_{\\max}]$. You must handle arbitrary nonnegative integer counts, including zeros in some categories, and ensure the computed probability vector has strictly positive entries for all $i$ so that $\\ell(\\kappa)$ is well-defined. Report $\\widehat{\\kappa}$ rounded to six decimal places.\n\nTest Suite and Answer Specification:\n- Use the following three test cases. In each test case, the program must compute the maximum likelihood estimate $\\widehat{\\kappa}$ for the specified $(n,\\alpha,\\beta)$ and counts $(X_0,\\dots,X_n)$, with the search restricted to the interval $[\\kappa_{\\min},\\kappa_{\\max}]$.\n\n  - Test Case $1$:\n    - Sample size: $n = 10$.\n    - Scaled mutation parameters: $\\alpha = 1.0$, $\\beta = 1.0$.\n    - Selection search bounds: $\\kappa_{\\min} = -6.0$, $\\kappa_{\\max} = 6.0$.\n    - Observed counts vector $(X_0,\\dots,X_{10})$:\n      $[\\,10,\\,10,\\,10,\\,10,\\,10,\\,10,\\,10,\\,10,\\,10,\\,10,\\,10\\,]$.\n\n  - Test Case $2$:\n    - Sample size: $n = 10$.\n    - Scaled mutation parameters: $\\alpha = 1.0$, $\\beta = 1.0$.\n    - Selection search bounds: $\\kappa_{\\min} = -6.0$, $\\kappa_{\\max} = 6.0$.\n    - Observed counts vector $(X_0,\\dots,X_{10})$:\n      $[\\,2,\\,4,\\,6,\\,8,\\,10,\\,12,\\,14,\\,16,\\,18,\\,20,\\,22\\,]$.\n\n  - Test Case $3$:\n    - Sample size: $n = 20$.\n    - Scaled mutation parameters: $\\alpha = 0.5$, $\\beta = 2.0$.\n    - Selection search bounds: $\\kappa_{\\min} = -10.0$, $\\kappa_{\\max} = 10.0$.\n    - Observed counts vector $(X_0,\\dots,X_{20})$:\n      $[\\,42,\\,40,\\,38,\\,36,\\,34,\\,32,\\,30,\\,28,\\,26,\\,24,\\,22,\\,20,\\,18,\\,16,\\,14,\\,12,\\,10,\\,8,\\,6,\\,4,\\,2\\,]$.\n\n- Your program must implement the numerically stable computation described above and return, for each test case in order, the MLE $\\widehat{\\kappa}$ as a real number rounded to six decimal places.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example:\n  $[\\,\\widehat{\\kappa}_1,\\,\\widehat{\\kappa}_2,\\,\\widehat{\\kappa}_3\\,]$,\n  where each $\\widehat{\\kappa}_j$ is printed with exactly six digits after the decimal point.", "solution": "The problem presented is a well-posed and scientifically grounded task in computational population genetics. It requires the derivation of the stationary distribution for a Wright-Fisher diffusion, the subsequent calculation of the site frequency spectrum (SFS) likelihood, and the implementation of a maximum likelihood estimation (MLE) procedure for the scaled selection parameter $\\kappa$. We will proceed with a systematic derivation followed by a description of the numerical implementation.\n\nThe foundation of this analysis is the one-dimensional Fokker-Planck equation, which describes the evolution of the probability density function of the allele frequency, $p(x,t)$. The stationary distribution, $\\pi(x)$, is the solution to this equation as $t \\to \\infty$, where $\\frac{\\partial p}{\\partial t} = 0$. For a diffusion process with infinitesimal mean $m(x)$ and variance $V(x)$, the stationary solution under zero-flux boundary conditions at $x=0$ and $x=1$ is given by:\n$$\n\\pi(x) = \\frac{C}{V(x)} \\exp\\left( \\int \\frac{2 m(x')}{V(x')} \\, dx' \\right)\n$$\nwhere $C$ is a normalization constant ensuring that $\\int_0^1 \\pi(x) \\, dx = 1$.\n\nThe problem provides the drift and diffusion coefficients for a bi-allelic locus with genic selection and bidirectional mutation:\n$$\nm(x) = s\\,x(1-x) + v(1-x) - u x\n$$\n$$\nV(x) = \\frac{x(1-x)}{2N}\n$$\nHere, $x$ is the frequency of allele $\\mathsf{1}$, $s$ is the selection coefficient, $u$ and $v$ are the mutation rates from $\\mathsf{1} \\to \\mathsf{0}$ and $\\mathsf{0} \\to \\mathsf{1}$ respectively, and $N$ is the effective population size.\n\nFirst, we compute the integrand $\\frac{2 m(x')}{V(x')}$:\n$$\n\\frac{2 m(x')}{V(x')} = \\frac{2 \\left( s\\,x'(1-x') + v(1-x') - u x' \\right)}{\\frac{x'(1-x')}{2N}} = 4N \\left( s + \\frac{v}{x'} - \\frac{u}{1-x'} \\right)\n$$\nIntroducing the standard scaled parameters $\\kappa = 4Ns$, $\\alpha = 4Nv$, and $\\beta = 4Nu$, the expression becomes:\n$$\n\\frac{2 m(x')}{V(x')} = \\kappa + \\frac{\\alpha}{x'} - \\frac{\\beta}{1-x'}\n$$\nNext, we perform the indefinite integration with respect to $x'$:\n$$\n\\int \\left( \\kappa + \\frac{\\alpha}{x'} - \\frac{\\beta}{1-x'} \\right) dx' = \\kappa x' + \\alpha \\ln(x') + \\beta \\ln(1-x') + \\text{const.}\n$$\nSubstituting this result back into the formula for $\\pi(x)$ and absorbing constants into the primary normalization constant $C'$, we obtain:\n$$\n\\pi(x) \\propto \\frac{1}{\\frac{x(1-x)}{2N}} \\exp\\left( \\kappa x + \\alpha \\ln x + \\beta \\ln(1-x) \\right)\n$$\n$$\n\\pi(x) \\propto \\frac{1}{x(1-x)} e^{\\kappa x} x^{\\alpha} (1-x)^{\\beta}\n$$\nThis simplifies to the well-known stationary distribution for this model:\n$$\n\\pi(x) = C^{-1} x^{\\alpha-1} (1-x)^{\\beta-1} e^{\\kappa x}\n$$\nwhere $C = \\int_0^1 x'^{\\alpha-1} (1-x')^{\\beta-1} e^{\\kappa x'} \\, dx'$ is the normalization constant.\n\nGiven a sample of size $n$ drawn from the population, the number of times allele $\\mathsf{1}$ is observed, denoted by $i$, follows a binomial distribution conditional on the population frequency $x$:\n$$\nP(i | x) = \\binom{n}{i} x^i (1-x)^{n-i}\n$$\nThe marginal probability of observing $i$ derived alleles, $p_i$, is obtained by integrating over all possible population frequencies $x$, weighted by the stationary distribution $\\pi(x)$:\n$$\np_i(\\alpha,\\beta,\\kappa;n) = \\int_0^1 P(i | x) \\pi(x) \\, dx = \\int_0^1 \\binom{n}{i} x^i (1-x)^{n-i} \\left( C^{-1} x^{\\alpha-1} (1-x)^{\\beta-1} e^{\\kappa x} \\right) \\, dx\n$$\nCombining terms, we arrive at the mixture representation for the sampling probability:\n$$\np_i = C^{-1} \\binom{n}{i} \\int_0^1 x^{i+\\alpha-1} (1-x)^{n-i+\\beta-1} e^{\\kappa x} \\, dx\n$$\nThe probabilities $p_i$ must sum to $1$. Instead of computing the normalization constant $C$ explicitly, which involves special functions, we can define unnormalized weights $w_i$ and normalize them numerically:\n$$\nw_i = \\binom{n}{i} \\int_0^1 x^{i+\\alpha-1} (1-x)^{n-i+\\beta-1} e^{\\kappa x} \\, dx\n$$\nThen, the probability is $p_i = w_i / \\sum_{j=0}^n w_j$. This approach is more numerically robust.\n\nFor the implementation, the integrals will be computed using high-accuracy numerical quadrature as specified. Direct computation of the terms $w_i$ is prone to numerical underflow or overflow. Therefore, all calculations are performed in the logarithmic domain.\nThe log of the unnormalized weight is:\n$$\n\\log w_i = \\log \\binom{n}{i} + \\log \\left( \\int_0^1 x^{i+\\alpha-1} (1-x)^{n-i+\\beta-1} e^{\\kappa x} \\, dx \\right)\n$$\nThe log-binomial coefficient, $\\log \\binom{n}{i}$, is computed using the `gammaln` function to maintain precision. The integral is computed using `scipy.integrate.quad` with a high accuracy tolerance. This method is robust to the integrable singularities that can occur at the boundaries $x=0$ or $x=1$ when $\\alpha<1$ or $\\beta<1$.\n\nOnce the vector of log-weights $[\\log w_0, \\dots, \\log w_n]$ is computed, the log-probabilities $\\log p_i$ are obtained via the log-sum-exp trick to prevent numerical instability during normalization:\n$$\n\\log p_i = \\log w_i - \\log\\left(\\sum_{j=0}^{n} e^{\\log w_j}\\right) = \\log w_i - \\left( L_{\\max} + \\log\\left(\\sum_{j=0}^{n} e^{\\log w_j - L_{\\max}}\\right) \\right)\n$$\nwhere $L_{\\max} = \\max_j(\\log w_j)$.\n\nThe log-likelihood of the observed SFS data $X = [X_0, \\dots, X_n]$ for a given $\\kappa$ is:\n$$\n\\ell(\\kappa) = \\sum_{i=0}^{n} X_i \\log p_i(\\alpha,\\beta,\\kappa;n)\n$$\nOur goal is to find the maximum likelihood estimate $\\widehat{\\kappa}$ that maximizes this function over the specified interval $[\\kappa_{\\min}, \\kappa_{\\max}]$. This is achieved by minimizing the negative log-likelihood, $-\\ell(\\kappa)$, using a numerical optimizer. For this one-dimensional bounded optimization problem, `scipy.optimize.minimize_scalar` with the 'bounded' method is the appropriate tool.\n\nThe final algorithm is as follows:\n1. For each test case, define an objective function `neg_log_likelihood(kappa)` which takes $\\kappa$ as input.\n2. Inside this function, compute the vector of log-probabilities $[\\log p_0, \\dots, \\log p_n]$ using the quadrature-based, log-space method described above.\n3. Calculate the negative log-likelihood $-\\sum_{i=0}^{n} X_i \\log p_i$ and return it.\n4. Pass this objective function to `scipy.optimize.minimize_scalar`, along with the bounds $[\\kappa_{\\min}, \\kappa_{\\max}]$.\n5. The optimizer returns the value $\\widehat{\\kappa}$ that minimizes the function, which corresponds to the MLE.\n6. Report the result rounded to six decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize_scalar\n\ndef _calculate_log_sfs(kappa: float, n: int, alpha: float, beta: float) -> np.ndarray:\n    \"\"\"\n    Computes the logarithm of the site frequency spectrum (SFS) probabilities, log(p_i).\n    \n    This function calculates the probability p_i of observing i derived alleles in a sample\n    of size n, under the stationary distribution of a Wright-Fisher diffusion with\n    selection (kappa), and bidirectional mutation (alpha, beta).\n\n    The calculation is performed in log-space to maintain numerical stability.\n    The integral part of the formula is computed using high-accuracy numerical quadrature.\n    \"\"\"\n    # Precompute log binomial coefficients for all i from 0 to n\n    log_binom_coeffs = gammaln(n + 1) - gammaln(np.arange(n + 1) + 1) - gammaln(n - np.arange(n + 1) + 1)\n\n    log_unnorm_probs = np.zeros(n + 1)\n\n    for i in range(n + 1):\n        # Define exponents for the integrand\n        a = i + alpha\n        b = n - i + beta\n        \n        # The integrand is x^(a-1) * (1-x)^(b-1) * exp(kappa*x).\n        # We define it inside the loop to capture the current values of a, b, and kappa.\n        def integrand(x: float) -> float:\n            # The exponents a-1 and b-1 can be negative if alpha or beta are < 1,\n            # creating integrable singularities at x=0 or x=1. `quad` handles this.\n            return x**(a - 1) * (1 - x)**(b - 1) * np.exp(kappa * x)\n        \n        # Perform numerical integration with high precision.\n        # The problem requires at least 1e-10 absolute accuracy.\n        integral_val, _ = quad(integrand, 0, 1, epsabs=1e-12, epsrel=1e-12)\n        \n        # Safeguard against numerical underflow where integral becomes non-positive.\n        if integral_val <= 0:\n            log_unnorm_probs[i] = -np.inf\n        else:\n            log_unnorm_probs[i] = log_binom_coeffs[i] + np.log(integral_val)\n\n    # Normalize the probabilities using the log-sum-exp trick for stability\n    max_log_prob = np.max(log_unnorm_probs)\n    \n    # If all probabilities underflow to -inf, this would cause an error.\n    # Return uniform log-probabilities as a failsafe.\n    if not np.isfinite(max_log_prob):\n        return np.full(n + 1, -np.log(n + 1))\n        \n    log_shifted_probs = log_unnorm_probs - max_log_prob\n    log_norm_const = max_log_prob + np.log(np.sum(np.exp(log_shifted_probs)))\n    \n    log_sfs = log_unnorm_probs - log_norm_const\n    return log_sfs\n\ndef make_neg_log_likelihood(counts: list[int], n: int, alpha: float, beta: float):\n    \"\"\"\n    Factory function that creates the negative log-likelihood function for a given dataset.\n    This allows us to pass a simple one-parameter function to the optimizer.\n    \"\"\"\n    counts_arr = np.array(counts, dtype=float)\n    \n    # Memoization cache to store results for previously computed kappa values\n    memo = {}\n\n    def neg_log_likelihood(kappa: float) -> float:\n        if kappa in memo:\n            return memo[kappa]\n\n        # Calculate log probabilities of the SFS for the given kappa\n        log_p = _calculate_log_sfs(kappa, n, alpha, beta)\n        \n        # Calculate the log-likelihood as the dot product of counts and log-probabilities.\n        # This correctly handles cases where X_i = 0.\n        log_likelihood = np.dot(counts_arr, log_p)\n        \n        # The optimizer minimizes, so we return the negative log-likelihood.\n        # Handle cases where log-likelihood is -inf (i.e., probability is zero)\n        if not np.isfinite(log_likelihood):\n            result = np.inf\n        else:\n            result = -log_likelihood\n        \n        memo[kappa] = result\n        return result\n        \n    return neg_log_likelihood\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and compute MLE for kappa.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 10, \"alpha\": 1.0, \"beta\": 1.0, \n            \"kappa_min\":-6.0, \"kappa_max\": 6.0,\n            \"counts\": [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n        },\n        {\n            \"n\": 10, \"alpha\": 1.0, \"beta\": 1.0,\n            \"kappa_min\": -6.0, \"kappa_max\": 6.0,\n            \"counts\": [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]\n        },\n        {\n            \"n\": 20, \"alpha\": 0.5, \"beta\": 2.0,\n            \"kappa_min\": -10.0, \"kappa_max\": 10.0,\n            \"counts\": [42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8, 6, 4, 2]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Create the objective function for the current test case\n        neg_log_lik = make_neg_log_likelihood(\n            counts=case[\"counts\"],\n            n=case[\"n\"],\n            alpha=case[\"alpha\"],\n            beta=case[\"beta\"]\n        )\n\n        # Find the minimum of the negative log-likelihood within the specified bounds\n        opt_result = minimize_scalar(\n            neg_log_lik,\n            bounds=(case[\"kappa_min\"], case[\"kappa_max\"]),\n            method='bounded'\n        )\n        \n        # The MLE kappa is the value that minimizes the function\n        kappa_mle = opt_result.x\n        results.append(kappa_mle)\n\n    # Format the final output as specified\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2700942"}]}