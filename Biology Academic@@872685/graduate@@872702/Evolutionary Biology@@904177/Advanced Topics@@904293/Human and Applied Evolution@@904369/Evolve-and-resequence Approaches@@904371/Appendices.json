{"hands_on_practices": [{"introduction": "A well-designed Evolve-and-Resequence (ER) experiment must have sufficient statistical power to detect the evolutionary signals it targets. This exercise guides you through a foundational power analysis, a critical first step in any experimental plan. By deriving the required number of replicate populations from first principles, you will learn to quantitatively balance the expected strength of selection against the confounding noise from genetic drift and sequencing error, ensuring your experiment is robust and efficient [@problem_id:2711888].", "problem": "An experimental evolution study uses an Evolve and Resequence (E&R) design with $R$ independent replicate populations, each of constant effective population size $N_{e}$, initiated from the same base population with a focal single-nucleotide polymorphism (SNP) at initial derived-allele frequency $p_{0}$. Each replicate evolves for $T$ discrete generations under genic selection favoring the derived allele with selection coefficient $s$ in a diploid Wright–Fisher population. At generation $0$ and generation $T$, pooled whole-population sequencing is performed with per-site coverage $C$ in each replicate, and the derived-allele frequency is estimated at both timepoints in each replicate from the sequencing reads. Assume that across timepoints and across replicates, sequencing read sampling is independent and follows a binomial model. \n\nYou will design a one-sided test of the null hypothesis $H_{0}: \\Delta = 0$ versus the alternative $H_{1}: \\Delta > 0$, where $\\Delta$ denotes the derived-allele frequency change between generation $T$ and generation $0$. Use a Wald $Z$-test at significance level $\\alpha$. Base your derivation on the following well-tested foundations:\n\n- The diffusion approximation for Wright–Fisher drift without selection gives the variance of allele frequency after $T$ generations as $\\operatorname{Var}(p_{T}\\mid p_{0}) \\approx p_{0}(1-p_{0})\\left(1-\\exp\\!\\left(-\\frac{T}{2 N_{e}}\\right)\\right)$, which you may use to approximate the among-replicate variance contribution from genetic drift to the change $\\Delta$.\n- Under weak selection, the expected deterministic trajectory for genic selection in continuous time satisfies $\\frac{d p}{d t} = s\\,p(1-p)$, whose solution implies the expected allele frequency after $T$ generations is $p_{T} \\approx \\frac{1}{1 + \\left(\\frac{1-p_{0}}{p_{0}}\\right)\\exp(-s T)}$.\n- Pool-sequencing at coverage $C$ yields an unbiased estimator of allele frequency at each timepoint with binomial sampling variance $p(1-p)/C$, independent across timepoints and replicates.\n\nAssume independence of drift and sequencing contributions and of all replicates, and approximate the sampling distribution of the replicate-mean change by a normal distribution. Derive from first principles an analytic expression for the minimal number of replicates $R$ required to achieve statistical power $1-\\beta$ under $H_{1}$, expressed in terms of $s$, $p_{0}$, $T$, $N_{e}$, $C$, $\\alpha$, and $\\beta$. Then, using the parameters $s = 0.02$, $p_{0} = 0.10$, $T = 30$, $N_{e} = 1000$, $C = 100$, $\\alpha = 0.05$, and target power $1-\\beta = 0.80$, compute the minimal integer $R$ that satisfies this requirement. State the final $R$ as an integer with no units. If intermediate numerical approximations are needed, carry them out to at least $4$ significant figures; the final $R$ must be the minimal integer meeting the criterion and does not require rounding.", "solution": "The problem requires the derivation of the minimal number of replicates, $R$, for an Evolve and Resequence experiment to achieve a desired statistical power. The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is therefore valid.\n\nWe begin by establishing the statistical framework. The hypothesis test concerns the mean change in derived-allele frequency, $\\Delta = p_T - p_0$. The null hypothesis is $H_0: \\Delta = 0$, and the one-sided alternative is $H_1: \\Delta > 0$. We use a Wald $Z$-test based on the mean estimated frequency change over $R$ replicates, denoted by $\\bar{\\hat{\\Delta}}$.\n\nLet $\\hat{p}_{i,0}$ and $\\hat{p}_{i,T}$ be the estimated allele frequencies in replicate $i$ at generation $0$ and $T$, respectively. The estimated change for replicate $i$ is $\\hat{\\Delta}_i = \\hat{p}_{i,T} - \\hat{p}_{i,0}$. The test statistic is based on the average of these estimates across all $R$ replicates:\n$$ \\bar{\\hat{\\Delta}} = \\frac{1}{R} \\sum_{i=1}^{R} \\hat{\\Delta}_i $$\nBy the central limit theorem, for sufficiently large $R$, the sampling distribution of $\\bar{\\hat{\\Delta}}$ is approximately normal.\n\nThe Wald Z-statistic is given by:\n$$ Z = \\frac{\\bar{\\hat{\\Delta}} - E[\\bar{\\hat{\\Delta}} | H_0]}{\\sqrt{\\operatorname{Var}(\\bar{\\hat{\\Delta}} | H_0)}} $$\nUnder the null hypothesis $H_0$, there is no selection ($s=0$), so the expected allele frequency does not change. Thus, $E[p_T|H_0] = p_0$, and the expected change is $E[\\bar{\\hat{\\Delta}} | H_0] = 0$.\n\nTo construct the test and perform the power analysis, we must determine the variance of $\\bar{\\hat{\\Delta}}$ under both the null ($H_0$) and the alternative ($H_1$) hypotheses. The variance of the mean estimate is $\\operatorname{Var}(\\bar{\\hat{\\Delta}}) = \\frac{1}{R} \\operatorname{Var}(\\hat{\\Delta}_i)$. We must first find the variance for a single replicate, $\\sigma_{\\hat{\\Delta}}^2 = \\operatorname{Var}(\\hat{\\Delta}_i)$.\n\nThe total variance in the estimated change $\\hat{\\Delta}_i$ arises from two independent processes: genetic drift over $T$ generations and binomial sampling of reads during sequencing at two time points. Since the sequencing estimates at generation $0$ and $T$ are independent, we have:\n$$ \\operatorname{Var}(\\hat{\\Delta}_i) = \\operatorname{Var}(\\hat{p}_{i,T} - \\hat{p}_{i,0}) = \\operatorname{Var}(\\hat{p}_{i,T}) + \\operatorname{Var}(\\hat{p}_{i,0}) $$\nThe initial frequency $p_0$ is fixed, so the variance of its estimator is purely from sequencing:\n$$ \\operatorname{Var}(\\hat{p}_{i,0}) = \\frac{p_0(1-p_0)}{C} $$\nThe frequency $p_{i,T}$ in replicate $i$ at generation $T$ is a random variable due to drift. We use the law of total variance to find $\\operatorname{Var}(\\hat{p}_{i,T})$:\n$$ \\operatorname{Var}(\\hat{p}_{i,T}) = E[\\operatorname{Var}(\\hat{p}_{i,T} | p_{i,T})] + \\operatorname{Var}(E[\\hat{p}_{i,T} | p_{i,T}]) $$\nGiven the true frequency $p_{i,T}$, the sequencing estimator is unbiased, $E[\\hat{p}_{i,T} | p_{i,T}] = p_{i,T}$, and its variance is $\\operatorname{Var}(\\hat{p}_{i,T} | p_{i,T}) = \\frac{p_{i,T}(1-p_{i,T})}{C}$.\nThe expression for $\\operatorname{Var}(\\hat{p}_{i,T})$ becomes:\n$$ \\operatorname{Var}(\\hat{p}_{i,T}) = E\\left[\\frac{p_{i,T}(1-p_{i,T})}{C}\\right] + \\operatorname{Var}(p_{i,T}) $$\nWe approximate the first term using the expected value of $p_{i,T}$. Let $p_T^{\\text{exp}} = E[p_{i,T}]$. Then $E\\left[\\frac{p_{i,T}(1-p_{i,T})}{C}\\right] \\approx \\frac{p_T^{\\text{exp}}(1-p_T^{\\text{exp}})}{C}$. The second term, $\\operatorname{Var}(p_{i,T})$, is the variance due to genetic drift, which the problem provides as $\\operatorname{Var}(p_T | p_0) \\approx p_0(1-p_0)\\left(1-\\exp\\left(-\\frac{T}{2N_e}\\right)\\right)$.\n\nThe total per-replicate variance is therefore:\n$$ \\sigma_{\\hat{\\Delta}}^2 = \\operatorname{Var}(\\hat{\\Delta}_i) \\approx p_0(1-p_0)\\left(1-\\exp\\left(-\\frac{T}{2N_e}\\right)\\right) + \\frac{p_T^{\\text{exp}}(1-p_T^{\\text{exp}})}{C} + \\frac{p_0(1-p_0)}{C} $$\nUnder $H_0$, $s=0$, so $p_T^{\\text{exp}} = p_0$. The variance under the null, $\\sigma_0^2$, simplifies to:\n$$ \\sigma_0^2 = p_0(1-p_0)\\left(1-\\exp\\left(-\\frac{T}{2N_e}\\right)\\right) + \\frac{2p_0(1-p_0)}{C} = p_0(1-p_0)\\left[1-\\exp\\left(-\\frac{T}{2N_e}\\right) + \\frac{2}{C}\\right] $$\nUnder $H_1$, with selection coefficient $s > 0$, the expected frequency is $p_T^{\\text{exp}} \\approx \\frac{1}{1 + \\left(\\frac{1-p_0}{p_0}\\right)\\exp(-sT)}$. The variance under the alternative, $\\sigma_1^2$, is:\n$$ \\sigma_1^2 = p_0(1-p_0)\\left(1-\\exp\\left(-\\frac{T}{2N_e}\\right)\\right) + \\frac{p_T^{\\text{exp}}(1-p_T^{\\text{exp}})}{C} + \\frac{p_0(1-p_0)}{C} $$\nThe expected change under the alternative is $\\mu_{\\Delta} = E[\\bar{\\hat{\\Delta}}|H_1] = p_T^{\\text{exp}} - p_0$.\n\nFor a one-sided test at significance level $\\alpha$, we reject $H_0$ if $Z > z_{\\alpha}$, where $z_{\\alpha}$ is the upper $\\alpha$-quantile of the standard normal distribution. This is equivalent to rejecting if $\\bar{\\hat{\\Delta}} > z_{\\alpha}\\sqrt{\\sigma_0^2/R}$.\n\nStatistical power, $1-\\beta$, is the probability of rejecting $H_0$ given $H_1$ is true:\n$$ 1-\\beta = P\\left(\\bar{\\hat{\\Delta}} > z_{\\alpha}\\sqrt{\\frac{\\sigma_0^2}{R}} \\;\\middle|\\; H_1\\right) $$\nUnder $H_1$, $\\bar{\\hat{\\Delta}} \\sim N(\\mu_{\\Delta}, \\sigma_1^2/R)$. We standardize the inequality:\n$$ 1-\\beta = P\\left( \\frac{\\bar{\\hat{\\Delta}} - \\mu_{\\Delta}}{\\sqrt{\\sigma_1^2/R}} > \\frac{z_{\\alpha}\\sqrt{\\sigma_0^2/R} - \\mu_{\\Delta}}{\\sqrt{\\sigma_1^2/R}} \\right) $$\nThe left side is a standard normal variable. For the probability to be $1-\\beta$, the threshold on the right must be $-z_{\\beta}$, where $z_{\\beta}$ is the upper $\\beta$-quantile.\n$$ -z_{\\beta} = \\frac{z_{\\alpha}\\sqrt{\\sigma_0^2} - \\mu_{\\Delta}\\sqrt{R}}{\\sqrt{\\sigma_1^2}} $$\nSolving for $\\sqrt{R}$:\n$$ \\mu_{\\Delta}\\sqrt{R} = z_{\\alpha}\\sqrt{\\sigma_0^2} + z_{\\beta}\\sqrt{\\sigma_1^2} $$\n$$ \\sqrt{R} = \\frac{z_{\\alpha}\\sqrt{\\sigma_0^2} + z_{\\beta}\\sqrt{\\sigma_1^2}}{\\mu_{\\Delta}} $$\nThis leads to the analytical expression for the required number of replicates:\n$$ R = \\left( \\frac{z_{\\alpha}\\sqrt{\\sigma_0^2} + z_{\\beta}\\sqrt{\\sigma_1^2}}{\\mu_{\\Delta}} \\right)^2 $$\n\nNow, we compute the numerical value for $R$ using the provided parameters:\n$s=0.02$, $p_0=0.10$, $T=30$, $N_e=1000$, $C=100$, $\\alpha=0.05$, $1-\\beta=0.80$.\nThe corresponding quantiles are $z_{\\alpha} = z_{0.05} \\approx 1.6449$ and $z_{\\beta} = z_{0.20} \\approx 0.8416$.\n\n1.  Calculate expected frequency and change under $H_1$:\n    $p_T^{\\text{exp}} = \\frac{1}{1 + \\left(\\frac{1-0.10}{0.10}\\right)\\exp(-0.02 \\times 30)} = \\frac{1}{1 + 9\\exp(-0.6)} \\approx \\frac{1}{1 + 9(0.54881)} \\approx 0.16837$.\n    $\\mu_{\\Delta} = p_T^{\\text{exp}} - p_0 \\approx 0.16837 - 0.10 = 0.06837$.\n\n2.  Calculate variance under $H_0$:\n    $\\frac{T}{2N_e} = \\frac{30}{2000} = 0.015$.\n    $\\sigma_0^2 = 0.1(0.9)\\left[1-\\exp(-0.015) + \\frac{2}{100}\\right] \\approx 0.09[1 - 0.98511 + 0.02] = 0.09[0.01489 + 0.02] = 0.09(0.03489) \\approx 0.0031401$.\n    $\\sqrt{\\sigma_0^2} \\approx 0.056037$.\n\n3.  Calculate variance under $H_1$:\n    Drift component: $0.1(0.9)(1-\\exp(-0.015)) \\approx 0.0013401$.\n    Sequencing component at time $0$: $\\frac{0.1(0.9)}{100} = 0.0009$.\n    Sequencing component at time $T$: $\\frac{p_T^{\\text{exp}}(1-p_T^{\\text{exp}})}{C} \\approx \\frac{0.16837(1-0.16837)}{100} \\approx \\frac{0.14002}{100} = 0.0014002$.\n    $\\sigma_1^2 \\approx 0.0013401 + 0.0014002 + 0.0009 = 0.0036403$.\n    $\\sqrt{\\sigma_1^2} \\approx 0.060335$.\n\n4.  Calculate $R$:\n    $R = \\left( \\frac{1.6449 \\times 0.056037 + 0.8416 \\times 0.060335}{0.06837} \\right)^2$\n    $R \\approx \\left( \\frac{0.092174 + 0.050786}{0.06837} \\right)^2 = \\left( \\frac{0.14296}{0.06837} \\right)^2 \\approx (2.0910)^2 \\approx 4.372$.\n\nSince the number of replicates must be an integer, we take the ceiling of this value to ensure the power requirement is met.\n$R = \\lceil 4.372 \\rceil = 5$.\nThe minimal integer number of replicates required is $5$.", "answer": "$$\n\\boxed{5}\n$$", "id": "2711888"}, {"introduction": "In pooled sequencing (Pool-Seq), the precision of our allele frequency estimates is limited by two distinct sampling stages: the creation of the DNA pool from a finite number of individuals and the sequencing of a finite number of reads from that pool. This practice challenges you to dissect these two sources of variance using the law of total variance. Understanding their relative contributions is essential for optimizing experimental resources, as it clarifies the point of diminishing returns for increasing sequencing coverage [@problem_id:2711938].", "problem": "In an evolve-and-resequence experiment using pooled sequencing (Pool-Seq), a population of $n$ diploid individuals contributes $2n$ chromosomes to a pool. At a focal locus with true allele frequency $p$, the pool is constructed by sampling $2n$ chromosomes independently from the population. Sequencing then generates $C$ independent reads overlapping the locus, each read sampling an allele from the pool without bias or duplication. Let $\\hat{p}$ denote the estimator of the allele frequency based on the fraction of reads carrying the focal allele. Assume that both stages (pool construction and sequencing) are well modeled as binomial sampling, that all reads are independent and identically distributed, and that there is no additional overdispersion.\n\nUsing only the following fundamental principles:\n- The binomial sampling model and its variance $p(1-p)$ scaled by the number of trials.\n- The law of total variance, $\\mathrm{Var}(Z) = \\mathbb{E}[\\mathrm{Var}(Z \\mid Y)] + \\mathrm{Var}(\\mathbb{E}[Z \\mid Y])$.\n\nDefine the contribution of sequencing noise to the variance of $\\hat{p}$ as the conditional sampling variance of reads averaged over the randomness of the pool, and define the total variance of $\\hat{p}$ as the sum of the variance from finite pool construction and the averaged sequencing variance. For $p = 0.3$ and $n = 500$, compute the minimal sequencing coverage $C$ such that the fraction of the total variance attributable to sequencing noise is strictly less than $0.2$. Report the minimal integer value of $C$ that satisfies this constraint.", "solution": "The problem requires us to determine the minimal sequencing coverage $C$ under specific conditions. We must first validate the problem statement.\n\n**Problem Validation**\n\nStep 1: Extract Givens\n- Population size: $n$ diploid individuals, equivalent to $2n$ chromosomes.\n- True allele frequency at a focal locus: $p$.\n- Pool construction: a sample of $2n$ chromosomes drawn independently.\n- Sequencing: $C$ independent reads drawn from the pool.\n- Estimator of allele frequency: $\\hat{p}$, the fraction of reads with the focal allele.\n- Sampling model: Binomial for both pool construction and sequencing.\n- Principle 1: Binomial variance is $p(1-p)$ scaled by the number of trials.\n- Principle 2: The law of total variance, $\\mathrm{Var}(Z) = \\mathbb{E}[\\mathrm{Var}(Z \\mid Y)] + \\mathrm{Var}(\\mathbb{E}[Z \\mid Y])$.\n- Definition of sequencing noise variance: The conditional sampling variance of reads averaged over the randomness of the pool.\n- Definition of total variance: Sum of variance from pool construction and the averaged sequencing variance.\n- Numerical values: $p = 0.3$, $n = 500$.\n- Constraint: The fraction of total variance from sequencing noise must be strictly less than $0.2$.\n- Objective: Find the minimal integer $C$ satisfying the constraint.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard and well-understood model of Pool-Seq experiments in population genetics. The use of binomial sampling and the law of total variance is the correct theoretical framework for this two-stage sampling process. The problem is well-posed, providing all necessary definitions, variables, and constraints to arrive at a unique mathematical solution. The language is objective and precise. The premises are not contradictory, incomplete, or factually unsound. The problem requires a non-trivial application of specified principles, making it a valid test of understanding.\n\nStep 3: Verdict\nThe problem is valid. We proceed to the solution.\n\n**Solution**\n\nLet $p$ be the true allele frequency in the initial population. A pool of DNA is created by sampling $2n$ chromosomes. Let $p_{pool}$ be the random variable representing the allele frequency in this pool. The number of chromosomes carrying the focal allele, $K_{pool}$, follows a binomial distribution, $K_{pool} \\sim \\mathrm{Binomial}(2n, p)$. The frequency in the pool is $p_{pool} = \\frac{K_{pool}}{2n}$.\n\nFrom this pool, $C$ reads are sequenced. Let $\\hat{p}$ be the estimator of the allele frequency based on these reads. Given the allele frequency $p_{pool}$ in the pool, the number of reads carrying the focal allele, $K_{reads}$, follows a binomial distribution, $K_{reads} \\sim \\mathrm{Binomial}(C, p_{pool})$. The estimator is $\\hat{p} = \\frac{K_{reads}}{C}$.\n\nWe use the law of total variance to find the total variance of the estimator $\\hat{p}$:\n$$\n\\mathrm{Var}(\\hat{p}) = \\mathbb{E}[\\mathrm{Var}(\\hat{p} \\mid p_{pool})] + \\mathrm{Var}(\\mathbb{E}[\\hat{p} \\mid p_{pool}])\n$$\nThe problem defines two components of this variance:\n1.  The variance from finite pool construction, which we will show corresponds to $\\mathrm{Var}(\\mathbb{E}[\\hat{p} \\mid p_{pool}])$, let's call this $V_{pool}$.\n2.  The contribution of sequencing noise, which is defined as $\\mathbb{E}[\\mathrm{Var}(\\hat{p} \\mid p_{pool})]$, let's call this $V_{seq}$.\n\nFirst, we analyze the term for pool construction variance, $V_{pool}$. We calculate the inner expectation:\n$$\n\\mathbb{E}[\\hat{p} \\mid p_{pool}] = \\mathbb{E}\\left[\\frac{K_{reads}}{C} \\mid p_{pool}\\right] = \\frac{1}{C}\\mathbb{E}[K_{reads} \\mid p_{pool}]\n$$\nGiven $K_{reads} \\sim \\mathrm{Binomial}(C, p_{pool})$, its expectation is $C p_{pool}$.\n$$\n\\mathbb{E}[\\hat{p} \\mid p_{pool}] = \\frac{1}{C}(C p_{pool}) = p_{pool}\n$$\nThe variance of this term is therefore the variance of the pool frequency itself:\n$$\nV_{pool} = \\mathrm{Var}(\\mathbb{E}[\\hat{p} \\mid p_{pool}]) = \\mathrm{Var}(p_{pool}) = \\mathrm{Var}\\left(\\frac{K_{pool}}{2n}\\right) = \\frac{1}{(2n)^2} \\mathrm{Var}(K_{pool})\n$$\nGiven $K_{pool} \\sim \\mathrm{Binomial}(2n, p)$, its variance is $2np(1-p)$.\n$$\nV_{pool} = \\frac{2np(1-p)}{(2n)^2} = \\frac{p(1-p)}{2n}\n$$\n\nNext, we analyze the sequencing noise term, $V_{seq}$. We calculate the inner conditional variance:\n$$\n\\mathrm{Var}(\\hat{p} \\mid p_{pool}) = \\mathrm{Var}\\left(\\frac{K_{reads}}{C} \\mid p_{pool}\\right) = \\frac{1}{C^2}\\mathrm{Var}(K_{reads} \\mid p_{pool})\n$$\nGiven $K_{reads} \\sim \\mathrm{Binomial}(C, p_{pool})$, its conditional variance is $C p_{pool}(1-p_{pool})$.\n$$\n\\mathrm{Var}(\\hat{p} \\mid p_{pool}) = \\frac{C p_{pool}(1-p_{pool})}{C^2} = \\frac{p_{pool}(1-p_{pool})}{C}\n$$\nNow, we take the expectation of this conditional variance over the distribution of $p_{pool}$:\n$$\nV_{seq} = \\mathbb{E}[\\mathrm{Var}(\\hat{p} \\mid p_{pool})] = \\mathbb{E}\\left[\\frac{p_{pool}(1-p_{pool})}{C}\\right] = \\frac{1}{C}\\mathbb{E}[p_{pool}(1-p_{pool})]\n$$\nTo evaluate $\\mathbb{E}[p_{pool}(1-p_{pool})]$, we first find the moments of $p_{pool}$.\nThe expectation of $p_{pool}$ is $\\mathbb{E}[p_{pool}] = \\mathbb{E}[\\frac{K_{pool}}{2n}] = \\frac{2np}{2n} = p$.\nThe second moment $\\mathbb{E}[p_{pool}^2]$ is found using its variance: $\\mathbb{E}[p_{pool}^2] = \\mathrm{Var}(p_{pool}) + (\\mathbb{E}[p_{pool}])^2 = \\frac{p(1-p)}{2n} + p^2$.\nSo,\n$$\n\\mathbb{E}[p_{pool}(1-p_{pool})] = \\mathbb{E}[p_{pool}] - \\mathbb{E}[p_{pool}^2] = p - \\left(\\frac{p(1-p)}{2n} + p^2\\right) = p - p^2 - \\frac{p(1-p)}{2n} = p(1-p)\\left(1 - \\frac{1}{2n}\\right)\n$$\nSubstituting this back into the expression for $V_{seq}$:\n$$\nV_{seq} = \\frac{p(1-p)}{C}\\left(1 - \\frac{1}{2n}\\right)\n$$\nThe total variance is $V_{total} = V_{seq} + V_{pool}$. The problem requires the fraction of the total variance attributable to sequencing noise to be strictly less than $0.2$:\n$$\n\\frac{V_{seq}}{V_{total}} = \\frac{V_{seq}}{V_{seq} + V_{pool}} < 0.2\n$$\nSubstituting the expressions for $V_{seq}$ and $V_{pool}$:\n$$\n\\frac{\\frac{p(1-p)}{C}\\left(1 - \\frac{1}{2n}\\right)}{\\frac{p(1-p)}{C}\\left(1 - \\frac{1}{2n}\\right) + \\frac{p(1-p)}{2n}} < 0.2\n$$\nThe term $p(1-p)$ is a common factor and is non-zero for $p = 0.3$. We can cancel it:\n$$\n\\frac{\\frac{1}{C}\\left(1 - \\frac{1}{2n}\\right)}{\\frac{1}{C}\\left(1 - \\frac{1}{2n}\\right) + \\frac{1}{2n}} < 0.2\n$$\nLet the numerator be $X = \\frac{1}{C}(1 - \\frac{1}{2n})$. The inequality is $\\frac{X}{X + \\frac{1}{2n}} < 0.2$. Since $n$ and $C$ are positive, all terms are positive. We can rearrange the inequality:\n$$\nX < 0.2 \\left(X + \\frac{1}{2n}\\right)\n$$\n$$\n0.8 X < \\frac{0.2}{2n}\n$$\n$$\n4X < \\frac{1}{2n}\n$$\nNow substitute back $X$:\n$$\n4 \\frac{1}{C}\\left(1 - \\frac{1}{2n}\\right) < \\frac{1}{2n}\n$$\n$$\n\\frac{4}{C}\\left(\\frac{2n-1}{2n}\\right) < \\frac{1}{2n}\n$$\nSince $C > 0$ and $n > 0$, we can multiply both sides by $2nC$ without changing the inequality direction:\n$$\n4(2n - 1) < C\n$$\nWe are given $n = 500$:\n$$\nC > 4(2 \\cdot 500 - 1)\n$$\n$$\nC > 4(1000 - 1)\n$$\n$$\nC > 4(999)\n$$\n$$\nC > 3996\n$$\nThe problem asks for the minimal integer value of $C$ that satisfies this strict inequality. The smallest integer greater than $3996$ is $3997$.", "answer": "$$\n\\boxed{3997}\n$$", "id": "2711938"}, {"introduction": "After collecting E&R data, a primary goal is often to estimate the strength of selection acting on beneficial mutations. A common method involves regressing allele frequency changes against a function of the allele frequency itself, but this approach harbors a subtle statistical pitfall. This exercise delves into the concept of errors-in-variables bias, where measurement error in our predictor variable leads to a systematic underestimation of the selection coefficient. By analytically deriving this bias, you will gain a deeper appreciation for the nuances of statistical inference with noisy data and the importance of using appropriate correction methods [@problem_id:2711971].", "problem": "In an evolve-and-resequence experiment, consider $L$ independent biallelic loci indexed by $i \\in \\{1,\\dots,L\\}$ in a population of constant Effective Population Size (EPS; $N_e$). Let the baseline allele frequency at locus $i$ be $p_i$, drawn independently from a symmetric Beta distribution $\\mathrm{Beta}(\\alpha,\\alpha)$ with parameter $\\alpha > 0$. Over a single generation, assume additive directional selection with a constant selection coefficient $s$ that is small enough for the deterministic expectation of the allele-frequency change to be $s\\,p_i(1-p_i)$ to first order. Genetic drift is present and modeled as Wright–Fisher sampling noise with zero mean conditional on $p_i$; its details are not required beyond zero-mean independence from observation errors and predictors. \n\nBoth timepoints are measured by pooled sequencing, producing observed allele frequencies $\\hat{p}_{0,i}$ at baseline and $\\hat{p}_{1,i}$ at the subsequent timepoint. Conditional on the true frequencies, observation (sampling) errors are independent across loci and timepoints with $\\mathbb{E}[e_{0,i} \\mid p_i] = 0$ and $\\mathrm{Var}(e_{0,i} \\mid p_i) = p_i(1-p_i)/n_0$, where $e_{0,i} = \\hat{p}_{0,i} - p_i$ and $n_0$ is the baseline read depth; similarly, $\\mathbb{E}[e_{1,i} \\mid p_i] = 0$ and $\\mathrm{Var}(e_{1,i} \\mid p_i) \\approx p_i(1-p_i)/n_1$ with $e_{1,i} = \\hat{p}_{1,i} - (p_i + s\\,p_i(1-p_i) + \\varepsilon_i)$, where $\\varepsilon_i$ is the zero-mean drift increment, independent of observation errors and $p_i$.\n\nAn analyst naively regresses the observed change $\\widehat{\\Delta p}_i = \\hat{p}_{1,i} - \\hat{p}_{0,i}$ on the baseline plug-in predictor $\\widehat{X}_i = \\hat{p}_{0,i}\\bigl(1-\\hat{p}_{0,i}\\bigr)$ using ordinary least squares, ignoring drift and observation error. Under a first-order delta-method approximation that drops terms of order $e_{0,i}^2$ and $e_{1,i}^2$, and treating $n_1$ as arbitrary but finite, derive the expected bias of the naive slope estimator relative to $s$, i.e., compute $\\mathrm{Bias} = \\mathbb{E}[\\hat{\\beta}_{\\mathrm{naive}}] - s$, where $\\hat{\\beta}_{\\mathrm{naive}} = \\mathrm{Cov}(\\widehat{X},\\widehat{\\Delta p})/\\mathrm{Var}(\\widehat{X})$ is computed across loci.\n\nAssume the symmetric prior $\\mathrm{Beta}(\\alpha,\\alpha)$ for $p_i$ and use only the assumptions stated above. Express the final bias as a closed-form function of $s$, $\\alpha$, and $n_0$. You may exploit symmetry of the $\\mathrm{Beta}(\\alpha,\\alpha)$ distribution about $p=\\tfrac{1}{2}$ where appropriate. \n\nProvide your final answer as a single, closed-form analytical expression for the bias in terms of $s$, $\\alpha$, and $n_0$. No numerical evaluation is required and no units apply. Do not report an inequality or an equation. No rounding is required.", "solution": "The problem requires the derivation of the bias of a naive ordinary least squares (OLS) estimator for a selection coefficient. The problem is well-posed, scientifically grounded in population genetics theory, and provides a complete set of assumptions. The problem is therefore valid.\n\nLet the true, unobserved allele frequency at locus $i$ be $p_i$. The prior distribution for $p_i$ is given as $p_i \\sim \\mathrm{Beta}(\\alpha, \\alpha)$. The true predictor variable is $X_i = p_i(1-p_i)$. The true change in allele frequency due to selection is $s X_i$.\n\nThe observed data are the allele frequencies at baseline, $\\hat{p}_{0,i}$, and at a subsequent timepoint, $\\hat{p}_{1,i}$. These are related to the true frequencies via error terms.\nAt baseline, the observation error is $e_{0,i} = \\hat{p}_{0,i} - p_i$, with $\\mathbb{E}[e_{0,i} \\mid p_i] = 0$ and $\\mathrm{Var}(e_{0,i} \\mid p_i) = \\frac{p_i(1-p_i)}{n_0}$.\nThe allele frequency at the next timepoint is affected by selection and genetic drift, $p_{1,i} = p_i + s p_i(1-p_i) + \\varepsilon_i$, where $\\varepsilon_i$ is the drift increment with $\\mathbb{E}[\\varepsilon_i \\mid p_i] = 0$.\nThe observation error at the second timepoint is $e_{1,i} = \\hat{p}_{1,i} - p_{1,i}$, with $\\mathbb{E}[e_{1,i} \\mid p_i] = 0$.\n\nThe naive analysis involves regressing the observed change $\\widehat{\\Delta p}_i = \\hat{p}_{1,i} - \\hat{p}_{0,i}$ on the plug-in predictor $\\widehat{X}_i = \\hat{p}_{0,i}(1-\\hat{p}_{0,i})$. The naive OLS estimator for the slope is given by $\\hat{\\beta}_{\\mathrm{naive}} = \\frac{\\mathrm{Cov}(\\widehat{X},\\widehat{\\Delta p})}{\\mathrm{Var}(\\widehat{X})}$. For a large number of loci $L$, the sample moments converge to their expectations over the generative distribution of $p_i$ and the error processes. Therefore, we compute the expected value of the estimator as the ratio of expected moments:\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\mathrm{naive}}] \\approx \\frac{\\mathrm{Cov}(\\widehat{X}_i, \\widehat{\\Delta p}_i)}{\\mathrm{Var}(\\widehat{X}_i)}\n$$\nwhere the covariance and variance are taken over the joint distribution of $p_i$ and the error terms.\n\nWe are instructed to use a first-order delta-method approximation. Let's expand the observed quantities.\nThe observed predictor is:\n$$\n\\widehat{X}_i = \\hat{p}_{0,i}(1-\\hat{p}_{0,i}) = (p_i + e_{0,i})(1 - p_i - e_{0,i}) = p_i(1-p_i) + (1-2p_i)e_{0,i} - e_{0,i}^2\n$$\nDropping the second-order term $e_{0,i}^2$ as instructed, we have the approximation:\n$$\n\\widehat{X}_i \\approx X_i + (1-2p_i)e_{0,i}\n$$\nThe observed change in allele frequency is:\n$$\n\\widehat{\\Delta p}_i = \\hat{p}_{1,i} - \\hat{p}_{0,i} = (p_{1,i} + e_{1,i}) - (p_i + e_{0,i}) = (p_i + s p_i(1-p_i) + \\varepsilon_i + e_{1,i}) - (p_i + e_{0,i})\n$$\n$$\n\\widehat{\\Delta p}_i = s X_i + \\varepsilon_i + e_{1,i} - e_{0,i}\n$$\nThis is an errors-in-variables problem where both the predictor and the response are measured with error, and these errors can be correlated. The \"error\" in the predictor is $U_i = (1-2p_i)e_{0,i}$ and the \"error\" in the response (relative to the true model $\\Delta p_i = sX_i$) is $Z_i = \\varepsilon_i + e_{1,i} - e_{0,i}$.\n\nWe compute the required variance and covariance terms using the law of total expectation/covariance. Let $\\mathbb{E}[\\cdot]$ denote expectation over all random variables, and $\\mathbb{E}_{p_i}[\\cdot]$ denote expectation over the distribution of $p_i$.\nFirst, the denominator, $\\mathrm{Var}(\\widehat{X}_i)$:\n$$\n\\mathrm{Var}(\\widehat{X}_i) = \\mathrm{Var}(X_i + (1-2p_i)e_{0,i}) = \\mathrm{Var}(X_i) + \\mathrm{Var}((1-2p_i)e_{0,i}) + 2\\mathrm{Cov}(X_i, (1-2p_i)e_{0,i})\n$$\nThe covariance term is zero: $\\mathrm{Cov}(X_i, (1-2p_i)e_{0,i}) = \\mathbb{E}[X_i (1-2p_i)e_{0,i}] = \\mathbb{E}_{p_i}[\\mathbb{E}[X_i (1-2p_i)e_{0,i} \\mid p_i]] = \\mathbb{E}_{p_i}[X_i (1-2p_i)\\mathbb{E}[e_{0,i} \\mid p_i]] = 0$.\nThe variance of the error term is:\n$$\n\\mathrm{Var}((1-2p_i)e_{0,i}) = \\mathbb{E}[((1-2p_i)e_{0,i})^2] = \\mathbb{E}_{p_i}[\\mathbb{E}[(1-2p_i)^2 e_{0,i}^2 \\mid p_i]]\n$$\n$$\n= \\mathbb{E}_{p_i}[(1-2p_i)^2 \\mathrm{Var}(e_{0,i} \\mid p_i)] = \\mathbb{E}_{p_i}\\left[(1-2p_i)^2 \\frac{p_i(1-p_i)}{n_0}\\right] = \\frac{1}{n_0}\\mathbb{E}[X_i (1-2p_i)^2]\n$$\nSo, the denominator is $\\mathrm{Var}(\\widehat{X}_i) = \\mathrm{Var}(X_i) + \\frac{1}{n_0}\\mathbb{E}[X_i (1-2p_i)^2]$.\n\nNext, the numerator, $\\mathrm{Cov}(\\widehat{X}_i, \\widehat{\\Delta p}_i)$:\n$$\n\\mathrm{Cov}(\\widehat{X}_i, \\widehat{\\Delta p}_i) = \\mathrm{Cov}(X_i + (1-2p_i)e_{0,i}, sX_i + \\varepsilon_i + e_{1,i} - e_{0,i})\n$$\nExpanding the covariance operator:\n$$\n= s\\mathrm{Var}(X_i) + \\mathrm{Cov}(X_i, \\varepsilon_i + e_{1,i} - e_{0,i}) + \\mathrm{Cov}((1-2p_i)e_{0,i}, sX_i) + \\mathrm{Cov}((1-2p_i)e_{0,i}, \\varepsilon_i + e_{1,i} - e_{0,i})\n$$\nThe second and third terms are zero by the same logic as before (iterated expectation). We evaluate the fourth term:\n$$\n\\mathrm{Cov}((1-2p_i)e_{0,i}, \\varepsilon_i + e_{1,i} - e_{0,i}) = \\mathbb{E}[(1-2p_i)e_{0,i}(\\varepsilon_i + e_{1,i} - e_{0,i})]\n$$\nTaking expectation conditional on $p_i$, and using the independence of $\\varepsilon_i, e_{0,i}, e_{1,i}$:\n$$\n= \\mathbb{E}_{p_i}\\left[(1-2p_i) \\mathbb{E}[e_{0,i}\\varepsilon_i + e_{0,i}e_{1,i} - e_{0,i}^2 \\mid p_i]\\right] = \\mathbb{E}_{p_i}\\left[(1-2p_i) (0 + 0 - \\mathrm{Var}(e_{0,i} \\mid p_i))\\right]\n$$\n$$\n= \\mathbb{E}_{p_i}\\left[-(1-2p_i)\\frac{p_i(1-p_i)}{n_0}\\right] = -\\frac{1}{n_0}\\mathbb{E}[X_i(1-2p_i)]\n$$\nSo, the numerator is $\\mathrm{Cov}(\\widehat{X}_i, \\widehat{\\Delta p}_i) = s\\mathrm{Var}(X_i) - \\frac{1}{n_0}\\mathbb{E}[X_i(1-2p_i)]$.\n\nNow we must evaluate the expectations with respect to $p_i \\sim \\mathrm{Beta}(\\alpha, \\alpha)$. Let $p$ denote the random variable. This distribution is symmetric about $p=\\frac{1}{2}$.\nThe function $f(p) = X(1-2p) = p(1-p)(1-2p)$ is antisymmetric about $p=\\frac{1}{2}$, because $f(1-p) = (1-p)p(1-2(1-p)) = p(1-p)(2p-1) = -f(p)$. The expectation of an antisymmetric function with respect to a symmetric distribution is zero. Thus, $\\mathbb{E}[X_i(1-2p_i)] = 0$.\nThis greatly simplifies the numerator to $s\\mathrm{Var}(X_i)$.\n\nThe expected estimator is therefore:\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\mathrm{naive}}] \\approx \\frac{s\\mathrm{Var}(X_i)}{\\mathrm{Var}(X_i) + \\frac{1}{n_0}\\mathbb{E}[X_i (1-2p_i)^2]} = s\\left(1 + \\frac{1}{n_0} \\frac{\\mathbb{E}[X_i(1-2p_i)^2]}{\\mathrm{Var}(X_i)}\\right)^{-1}\n$$\nTo evaluate the ratio, we need the moments of the $\\mathrm{Beta}(\\alpha, \\alpha)$ distribution. The $k$-th raw moment is $\\mathbb{E}[p^k] = \\frac{\\Gamma(2\\alpha)\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)\\Gamma(2\\alpha+k)}$.\nWe require $\\mathrm{Var}(X_i) = \\mathbb{E}[X_i^2] - (\\mathbb{E}[X_i])^2$ and $\\mathbb{E}[X_i(1-2p_i)^2]$.\n$X_i = p_i - p_i^2$. Through calculation of moments up to $p_i^4$:\n$\\mathbb{E}[p_i] = \\frac{1}{2}$, $\\mathbb{E}[p_i^2] = \\frac{\\alpha+1}{2(2\\alpha+1)}$, $\\mathbb{E}[p_i^3] = \\frac{\\alpha+2}{4(2\\alpha+1)}$, $\\mathbb{E}[p_i^4] = \\frac{(\\alpha+2)(\\alpha+3)}{4(2\\alpha+1)(2\\alpha+3)}$.\nFrom this, one finds:\n$$\n\\mathrm{Var}(X_i) = \\mathrm{Var}(p_i-p_i^2) = \\frac{\\alpha}{4(2\\alpha+1)^2(2\\alpha+3)}\n$$\nAnd for the other expectation:\n$$\n\\mathbb{E}[X_i(1-2p_i)^2] = \\mathbb{E}[p_i(1-p_i)(1-4p_i+4p_i^2)] = \\mathbb{E}[p_i - 5p_i^2 + 8p_i^3 - 4p_i^4]\n$$\nSubstituting the moments and simplifying yields:\n$$\n\\mathbb{E}[X_i(1-2p_i)^2] = \\frac{\\alpha}{2(2\\alpha+1)(2\\alpha+3)}\n$$\nNow we compute the ratio:\n$$\n\\frac{\\mathbb{E}[X_i(1-2p_i)^2]}{\\mathrm{Var}(X_i)} = \\frac{\\frac{\\alpha}{2(2\\alpha+1)(2\\alpha+3)}}{\\frac{\\alpha}{4(2\\alpha+1)^2(2\\alpha+3)}} = \\frac{4(2\\alpha+1)^2(2\\alpha+3)}{2(2\\alpha+1)(2\\alpha+3)} = 2(2\\alpha+1)\n$$\nSubstituting this back into the expression for the expected estimator:\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\mathrm{naive}}] \\approx s\\left(1 + \\frac{2(2\\alpha+1)}{n_0}\\right)^{-1} = s \\frac{n_0}{n_0 + 2(2\\alpha+1)}\n$$\nThe bias is defined as $\\mathrm{Bias} = \\mathbb{E}[\\hat{\\beta}_{\\mathrm{naive}}] - s$.\n$$\n\\mathrm{Bias} = s \\frac{n_0}{n_0 + 2(2\\alpha+1)} - s = s \\left( \\frac{n_0 - (n_0 + 2(2\\alpha+1))}{n_0 + 2(2\\alpha+1)} \\right)\n$$\n$$\n\\mathrm{Bias} = s \\frac{-2(2\\alpha+1)}{n_0 + 2(2\\alpha+1)} = -\\frac{2s(2\\alpha+1)}{n_0 + 4\\alpha + 2}\n$$\nThis is the final expression for the bias, which is negative as expected for attenuation bias from errors in the predictor variable.", "answer": "$$\\boxed{-\\frac{2s(2\\alpha+1)}{n_0 + 4\\alpha + 2}}$$", "id": "2711971"}]}