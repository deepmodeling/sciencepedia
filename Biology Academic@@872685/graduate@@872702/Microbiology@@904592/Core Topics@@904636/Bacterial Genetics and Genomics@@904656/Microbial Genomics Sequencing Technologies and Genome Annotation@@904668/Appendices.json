{"hands_on_practices": [{"introduction": "A successful microbial genomics project begins with a well-conceived experimental design. This exercise challenges you to act as a project lead, selecting the optimal sequencing strategy for a particularly difficult microbial genome characterized by long repeats and epigenetic modifications. By weighing the trade-offs between competing technologies and balancing objectives like assembly contiguity, accuracy, and cost, you will practice the real-world decision-making that underpins modern genomics research [@problem_id:2509722].", "problem": "A microbial isolate has a genome size of $G = 6\\times 10^{6}\\ \\mathrm{bp}$ with high guanine-cytosine content ($70\\%$), abundant long exact repeats with maximum length $L_r = 50\\times 10^{3}\\ \\mathrm{bp}$, and is known to carry both $N^6$-methyladenine ($6\\mathrm{mA}$) and $C^5$-methylcytosine ($5\\mathrm{mC}$). You must select a sequencing strategy that optimizes three objectives: assembly contiguity, consensus accuracy, and detection of epigenetic modifications. The constraints are a total budget of $B = \\$8{,}000$ and a single high-molecular-weight DNA extraction providing $M = 6\\,\\mu\\mathrm{g}$ input DNA.\n\nFormalize the decision as a constrained multi-objective optimization with a weighted utility\n$$\nS = w_c\\,f_c + w_a\\,f_a + w_e\\,f_e,\n$$\nwhere $w_c = 0.4$, $w_a = 0.3$, and $w_e = 0.3$. Here, $f_c$ should increase when repeats of length $L_r$ are spanned by reads (contiguity), $f_a$ should increase with expected consensus accuracy after appropriate polishing (accuracy), and $f_e$ should reflect the ability to detect both $6\\mathrm{mA}$ and $5\\mathrm{mC}$ on native DNA (epigenetics). The design must satisfy both the budget constraint and the DNA input constraint.\n\nUse only the following widely accepted, foundational facts to construct your analysis and choice:\n- Coverage is $C = D/G$, where $D$ is total sequenced bases and $G$ is genome size. For consensus from independent errors, the probability of an incorrect majority vote decays with increasing depth.\n- Resolving a repeat of length $L_r$ requires at least one read whose aligned span across the repeat plus unique flanks exceeds $L_r$; thus, the probability to resolve repeats increases with the fraction of reads longer than $L_r$ at sufficient coverage.\n- Polymerase-based amplification erases native base modifications; platforms that measure polymerase kinetics or ionic current on native DNA can detect certain modifications.\n- High guanine-cytosine content can reduce effective coverage for polymerase-amplified short-read sequencing due to amplification bias, while single-molecule native DNA sequencing tends to have less guanine-cytosine bias.\n\nYou may assume the following platform-level properties (per run) are achievable under standard best-practice protocols for microbial genomes:\n\n- Illumina Sequencing by Synthesis (SBS), paired-end $2\\times 150\\ \\mathrm{bp}$: minimum practical run cost $C_I = \\$1{,}000$ for $D_I \\approx 10\\ \\mathrm{Gb}$; input $0.2\\,\\mu\\mathrm{g}$. Polymerase amplification introduces guanine-cytosine bias (effective coverage reduced by about $30\\%$ at $70\\%$ guanine-cytosine). No direct base-modification detection. Whole-Genome Bisulfite Sequencing (WGBS) add-on for cytosine methylation costs $C_B = \\$800$ and reduces sequence complexity with additional guanine-cytosine-dependent coverage losses; WGBS does not detect $6\\mathrm{mA}$.\n- Pacific Biosciences Single Molecule, Real-Time (SMRT) Circular Consensus Sequencing (CCS; High-Fidelity, HiFi): one SMRT Cell costs $C_P = \\$1{,}800$, yields $D_P \\approx 20\\ \\mathrm{Gb}$ of HiFi reads with modal length $\\approx 15\\ \\mathrm{kb}$ (tail to $\\approx 25\\ \\mathrm{kb}$), input $5\\,\\mu\\mathrm{g}$. HiFi read accuracy $\\approx 99.9\\%$; polymerase kinetics detects $6\\mathrm{mA}$ robustly and $4\\mathrm{mC}$; $5\\mathrm{mC}$ detection is limited without specialized chemistry.\n- Oxford Nanopore Technologies (ONT) native DNA ligation library: standard protocol yields read length $N50 \\approx 30\\ \\mathrm{kb}$; an ultra-long protocol requires $\\ge 3\\,\\mu\\mathrm{g}$ high-molecular-weight DNA and yields read length $N50 \\approx 100\\ \\mathrm{kb}$. One MinION flow cell costs $C_O = \\$900$; standard yield $D_O \\approx 15\\ \\mathrm{Gb}$; in duplex (approximately $Q20$) mode yield is reduced to $D_{O,duplex} \\approx 8\\ \\mathrm{Gb}$. Native ionic current signal supports detection of both $5\\mathrm{mC}$ and $6\\mathrm{mA}$. Input per library $\\ge 1\\,\\mu\\mathrm{g}$; ultra-long protocols benefit from $\\ge 3\\,\\mu\\mathrm{g}$.\n- Optional Illumina polishing dataset: $C_{pol} = \\$500$ for $D_{pol} \\approx 5\\ \\mathrm{Gb}$, $0.2\\,\\mu\\mathrm{g}$ input.\n\nConsider the following candidate designs:\n\nA. Illumina-only: deep whole-genome sequencing (WGS) with Illumina SBS at $> 100\\times$ coverage of the $G = 6\\times 10^{6}\\ \\mathrm{bp}$ genome, plus WGBS for cytosine methylation profiling.\n\nB. Pacific Biosciences HiFi-only: one SMRT Cell of HiFi reads for de novo assembly and modification detection.\n\nC. Oxford Nanopore ultra-long native sequencing: two ONT MinION flow cells using the ultra-long protocol to target read length $N50 \\approx 100\\ \\mathrm{kb}$ and total native coverage $> 50\\times$, plus an Illumina polishing dataset.\n\nD. Pacific Biosciences Continuous Long-Read (CLR) mode long reads from one SMRT Cell (typical read length $N50 \\approx 30\\ \\mathrm{kb}$) for assembly, plus an Illumina polishing dataset.\n\nWhich design best maximizes the weighted utility $S$ under the stated constraints, given the high guanine-cytosine, methylated, repeat-rich genome? Choose the single best option.\n\nOptions:\n- A. Illumina-only WGS $+$/WGBS delivers the highest $f_a$ and, despite short reads, achieves high $f_c$ through deep coverage; WGBS provides complete epigenetic detection.\n- B. PacBio HiFi-only balances high $f_a$ and moderate $f_c$; polymerase kinetics provides comprehensive $6\\mathrm{mA}$ and $5\\mathrm{mC}$ detection.\n- C. ONT ultra-long $+$ Illumina polishing achieves near-maximal $f_c$, high $f_a$ after polishing, and full native $5\\mathrm{mC}$ and $6\\mathrm{mA}$ detection; costs and DNA input fit the constraints.\n- D. PacBio CLR $+$ Illumina polishing yields high $f_c$ due to $30\\ \\mathrm{kb}$ reads and high $f_a$ after polishing; methylation detection is equivalent to ONT.", "solution": "The task is to identify the optimal sequencing strategy for a microbial genome with specific challenging characteristics, by maximizing a weighted utility function under budget and DNA input constraints.\n\nFirst, validation of the problem statement.\n\n**Step 1: Extract Givens**\n- **Genome Parameters**:\n  - Size: $G = 6 \\times 10^6 \\ \\mathrm{bp}$\n  - GC-content: $70\\%$\n  - Maximum repeat length: $L_r = 50 \\times 10^3 \\ \\mathrm{bp}$\n  - Epigenetic modifications: $N^6$-methyladenine ($6\\mathrm{mA}$) and $C^5$-methylcytosine ($5\\mathrm{mC}$)\n- **Constraints**:\n  - Budget: $B \\le \\$8,000$\n  - DNA input: $M \\le 6\\,\\mu\\mathrm{g}$\n- **Utility Function**: $S = w_c f_c + w_a f_a + w_e f_e$\n  - Weights: $w_c = 0.4$ (contiguity), $w_a = 0.3$ (accuracy), $w_e = 0.3$ (epigenetics)\n- **Objective Functions** (qualitative):\n  - $f_c$: Increases with ability to span repeats of length $L_r$.\n  - $f_a$: Increases with consensus accuracy.\n  - $f_e$: Increases with ability to detect both $6\\mathrm{mA}$ and $5\\mathrm{mC}$.\n- **Platform Properties**:\n  - **Illumina SBS**: Cost $C_I = \\$1,000$; Yield $D_I \\approx 10 \\ \\mathrm{Gb}$; Input $0.2\\,\\mu\\mathrm{g}$; Paired-end $2 \\times 150 \\ \\mathrm{bp}$; $30\\%$ effective coverage loss at $70\\%$ GC; No direct modification detection.\n  - **WGBS Add-on**: Cost $C_B = \\$800$; Detects cytosine methylation; Does not detect $6\\mathrm{mA}$.\n  - **PacBio HiFi (CCS)**: Cost $C_P = \\$1,800$; Yield $D_P \\approx 20 \\ \\mathrm{Gb}$; Modal read length $\\approx 15 \\ \\mathrm{kb}$; Input $5\\,\\mu\\mathrm{g}$; Accuracy $\\approx 99.9\\%$; Detects $6\\mathrm{mA}$ robustly, limited $5\\mathrm{mC}$ detection.\n  - **ONT Native DNA**: Cost $C_O = \\$900$ (per flow cell); Yield $D_O \\approx 15 \\ \\mathrm{Gb}$; Input $\\ge 1\\,\\mu\\mathrm{g}$ (standard) or $\\ge 3\\,\\mu\\mathrm{g}$ (ultra-long); Read length N50 $\\approx 30 \\ \\mathrm{kb}$ (standard) or $\\approx 100 \\ \\mathrm{kb}$ (ultra-long); Detects both $5\\mathrm{mC}$ and $6\\mathrm{mA}$.\n  - **Illumina Polishing**: Cost $C_{pol} = \\$500$; Yield $D_{pol} \\approx 5 \\ \\mathrm{Gb}$; Input $0.2\\,\\mu\\mathrm{g}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, using realistic parameters for sequencing technologies and a challenging microbial genome. It is a well-posed, constrained optimization problem. The language is objective and technical. All necessary information is provided to compare the candidate designs on a relative basis. The reference to \"PacBio CLR mode\" in one of the proposed designs, while not explicitly defined in the properties list, can be inferred from context and general domain knowledge; its read length is given in the design description, which is sufficient for evaluation. The problem is thus valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will now proceed with the analysis of each candidate design.\n\n**Analysis of Candidate Designs**\n\nThe primary challenge for assembly contiguity ($f_c$) is the presence of long exact repeats with $L_r = 50 \\times 10^3 \\ \\mathrm{bp}$ ($50 \\ \\mathrm{kb}$). To resolve these repeats, individual sequencing reads must be longer than $50 \\ \\mathrm{kb}$. The primary challenge for epigenetics ($f_e$) is the requirement to detect both $6\\mathrm{mA}$ and $5\\mathrm{mC}$.\n\n**A. Illumina-only: WGS + WGBS**\n- **Constraints**: Cost = $C_I + C_B = \\$1,000 + \\$800 = \\$1,800$. DNA input $\\approx 0.2 + 0.2 = 0.4\\,\\mu\\mathrm{g}$. Both are within limits. The strategy is feasible.\n- **Contiguity ($f_c$)**: Illumina reads are $150 \\ \\mathrm{bp}$ long. They cannot span $50 \\ \\mathrm{kb}$ repeats. Assembly will be extremely fragmented. $f_c$ is minimal.\n- **Accuracy ($f_a$)**: Very high coverage ($>100\\times$) with low error rate reads yields exceptionally high consensus accuracy. $f_a$ is maximal.\n- **Epigenetics ($f_e$)**: WGBS detects $5\\mathrm{mC}$ but not $6\\mathrm{mA}$. The epigenetic characterization is incomplete.\n- **Summary**: This design fails catastrophically on contiguity and provides incomplete epigenetic data.\n\n**B. Pacific Biosciences HiFi-only**\n- **Constraints**: Cost = $C_P = \\$1,800$. DNA input = $5\\,\\mu\\mathrm{g}$. Both are within limits. The strategy is feasible.\n- **Contiguity ($f_c$)**: HiFi reads have a modal length of $\\approx 15 \\ \\mathrm{kb}$. These reads are too short to span the $50 \\ \\mathrm{kb}$ repeats. The assembly will be fragmented, although much better than Illumina-only.\n- **Accuracy ($f_a$)**: HiFi reads have $99.9\\%$ accuracy, and the high coverage ($D_P/G \\approx (20 \\times 10^9)/(6 \\times 10^6) \\approx 3333\\times$) ensures extremely high consensus accuracy, comparable to polished Illumina data. $f_a$ is maximal.\n- **Epigenetics ($f_e$)**: PacBio kinetics robustly detect $6\\mathrm{mA}$, but the problem states $5\\mathrm{mC}$ detection is \"limited\". The epigenetic characterization is incomplete.\n- **Summary**: This design fails to resolve the key repeats and provides incomplete epigenetic information.\n\n**C. Oxford Nanopore ultra-long + Illumina polishing**\n- **Constraints**: Cost = $2 \\times C_O + C_{pol} = 2 \\times \\$900 + \\$500 = \\$2,300$. DNA input for ultra-long protocol is $\\ge 3\\,\\mu\\mathrm{g}$. Polishing requires $0.2\\,\\mu\\mathrm{g}$. Total DNA is $\\approx 3.2\\,\\mu\\mathrm{g}$, assuming a single library prep for the two ONT runs. Both cost and DNA are well within limits. The strategy is feasible.\n- **Contiguity ($f_c$)**: The ultra-long protocol yields a read N50 of $\\approx 100 \\ \\mathrm{kb}$. This is the only technology listed that produces a substantial fraction of reads long enough to span the $50 \\ \\mathrm{kb}$ repeats. This strategy will produce a highly contiguous, potentially single-contig, assembly. $f_c$ is maximal.\n- **Accuracy ($f_a$)**: Raw ONT reads have a higher error rate, but polishing with a high-coverage Illumina dataset will raise the final consensus accuracy to a very high level, making it highly competitive with other options.\n- **Epigenetics ($f_e$)**: ONT directly sequences native DNA, and the problem states it supports detection of both $5\\mathrm{mC}$ and $6\\mathrm{mA}$. This is the only option that provides a complete epigenetic profile. $f_e$ is maximal.\n- **Summary**: This design is the only one to successfully address all three core objectives: maximal contiguity, very high accuracy (after polishing), and complete epigenetics.\n\n**D. Pacific Biosciences CLR + Illumina polishing**\n- **Constraints**: Cost = $C_P + C_{pol} = \\$1,800 + \\$500 = \\$2,300$. DNA input = $5\\,\\mu\\mathrm{g} + 0.2\\,\\mu\\mathrm{g} = 5.2\\,\\mu\\mathrm{g}$. Both are within limits. The strategy is feasible.\n- **Contiguity ($f_c$)**: CLR reads have an N50 of $\\approx 30 \\ \\mathrm{kb}$. These are insufficient to span the $50 \\ \\mathrm{kb}$ repeats. Contiguity will be better than HiFi but still inadequate.\n- **Accuracy ($f_a$)**: Raw CLR reads have high error rates, but like the ONT strategy, polishing with Illumina data will result in a high-accuracy final consensus.\n- **Epigenetics ($f_e$)**: Like PacBio HiFi, CLR data allows for $6\\mathrm{mA}$ detection but has limited ability for $5\\mathrm{mC}$ detection. The epigenetic analysis is incomplete.\n- **Summary**: This design fails on the primary contiguity challenge and provides incomplete epigenetic data.\n\n**Conclusion on Designs**\nDesign C is superior to all others. It is the only strategy that resolves the long repeats ($f_c$), provides complete epigenetic information ($f_e$), and achieves high consensus accuracy ($f_a$) via polishing, all while satisfying the given constraints. Design C uniquely maximizes all components of the utility function $S$.\n\n**Evaluation of Options**\n- **A. Illumina-only WGS + WGBS delivers the highest $f_a$ and, despite short reads, achieves high $f_c$ through deep coverage; WGBS provides complete epigenetic detection.** This claim is **Incorrect**. High coverage does not solve long repeats ($f_c$ is very low), and WGBS misses the $6\\mathrm{mA}$ modification.\n\n- **B. PacBio HiFi-only balances high $f_a$ and moderate $f_c$; polymerase kinetics provides comprehensive $6\\mathrm{mA}$ and $5\\mathrm{mC}$ detection.** This claim is **Incorrect**. PacBio HiFi does *not* provide comprehensive detection of $5\\mathrm{mC}$ according to the problem statement.\n\n- **C. ONT ultra-long + Illumina polishing achieves near-maximal $f_c$, high $f_a$ after polishing, and full native $5\\mathrm{mC}$ and $6\\mathrm{mA}$ detection; costs and DNA input fit the constraints.** This claim is **Correct**. As determined by the analysis, this strategy excels in all three objective areas (contiguity, accuracy, epigenetics) and satisfies all constraints.\n\n- **D. PacBio CLR + Illumina polishing yields high $f_c$ due to $30\\ \\mathrm{kb}$ reads and high $f_a$ after polishing; methylation detection is equivalent to ONT.** This claim is **Incorrect**. $f_c$ is not \"high\" as the $30 \\ \\mathrm{kb}$ reads do not span the $50 \\ \\mathrm{kb}$ repeats. Furthermore, PacBio methylation detection is not equivalent to ONT, as it is limited for $5\\mathrm{mC}$.\n\nTherefore, option C is the only logically sound and factually correct choice.", "answer": "$$\\boxed{C}$$", "id": "2509722"}, {"introduction": "After selecting a sequencing strategy and assembling the reads, the next critical step is to assess the quality of the resulting genome. This exercise delves into the interpretation of standard quality metrics, particularly Benchmarking Universal Single-Copy Orthologs (BUSCO), and the common pitfalls that arise. You will analyze scenarios where naive interpretations are misleading due to biological realities like reductive evolution or gene duplication, learning to synthesize multiple lines of evidence to accurately evaluate a genome's completeness [@problem_id:2509741].", "problem": "A genomics team assembled three microbial genomes from metagenomic datasets sequenced with long reads and polished with short reads. They used Benchmarking Universal Single-Copy Orthologs (BUSCO) to assess completeness and also computed read mapping statistics and $k$-mer-based completeness. Consider the following plausible summaries (all percentages are proportions of BUSCO markers in the chosen lineage, categories are Complete Single-copy, Complete Duplicated, Fragmented, Missing):\n\n- Assembly A: An intracellular insect endosymbiont candidate with small genome size $\\approx 0.7$ Mb and low guanine-cytosine (GC) content $\\approx 30\\%$, long-read depth $\\approx 100\\times$, contig N50 $\\approx 0.9$ Mb, read mapping breadth $> 99\\%$, and $k$-mer recall relative to the read set $> 99\\%$. BUSCO bacteria_odb10: $\\text{S } 58\\%, \\text{ D } 0\\%, \\text{ F } 6\\%, \\text{ M } 36\\%$. BUSCO gammaproteobacteria_odb10: $\\text{S } 62\\%, \\text{ D } 0\\%, \\text{ F } 5\\%, \\text{ M } 33\\%$.\n\n- Assembly B: A free-living soil Actinobacteria with large genome size $\\approx 8.5$ Mb, high GC content $\\approx 70\\%$, known gene family expansions, hybrid depth $\\approx 150\\times$, contig N50 $\\approx 2.5$ Mb, read mapping breadth $> 99\\%$, and $k$-mer recall $> 99\\%$. BUSCO bacteria_odb10: $\\text{S } 79\\%, \\text{ D } 17\\%, \\text{ F } 1\\%, \\text{ M } 3\\%$. BUSCO actinobacteria_odb10: $\\text{S } 83\\%, \\text{ D } 14\\%, \\text{ F } 1\\%, \\text{ M } 2\\%$.\n\n- Assembly C: A putative archaeal symbiont with genome size $\\approx 1.1$ Mb, moderate GC content, long-read depth $\\approx 60\\times$, contig N50 $\\approx 0.4$ Mb, read mapping breadth $\\approx 96\\%$, $k$-mer recall $\\approx 97\\%$. BUSCO bacteria_odb10: $\\text{S } 10\\%, \\text{ D } 0\\%, \\text{ F } 20\\%, \\text{ M } 70\\%$. BUSCO archaea_odb10: $\\text{S } 52\\%, \\text{ D } 1\\%, \\text{ F } 12\\%, \\text{ M } 35\\%$.\n\nBackground definitions and assumptions:\n- BUSCO searches Hidden Markov Model (HMM) profiles of lineage-specific, near-universal single-copy orthologs to estimate completeness and duplication.\n- Genome reduction in obligate endosymbionts frequently entails genuine loss or pseudogenization of genes that are otherwise universal in free-living lineages.\n- Assembly contiguity metrics (for example, N50) reflect sequence continuity, not necessarily gene content completeness.\n\nWhich of the following statements are best supported by first principles and the data above? Select all that apply.\n\nA. For Assembly A, the lower BUSCO completeness against bacteria_odb10 and gammaproteobacteria_odb10 despite near-total read $k$-mer recall and mapping breadth is consistent with genuine lineage-specific gene loss typical of obligate endosymbionts; here, BUSCO primarily measures conservation of single-copy markers rather than assembly completion. Complementary checks should include coding density, presence of ribosomal RNA (rRNA) operon and transfer RNA (tRNA) complements, and evidence of circularization.\n\nB. For Assembly B, the elevated duplicated BUSCO fraction necessarily indicates contamination, because single-copy orthologs cannot be biologically duplicated in a single genome; therefore, the assembly is contaminated and must be purged.\n\nC. For Assembly B, an elevated duplicated BUSCO fraction can arise from true paralogy or from mis-annotation of split genes; to distinguish duplication from contamination, one should examine coverage parity across duplicates, local synteny, and whether duplicated markers come from families with frequent lineage-specific expansions. Complementary metrics include lineage-aware contamination estimates, consistency of depth across single-copy markers, and short-read depth over the duplicated loci.\n\nD. For Assembly C, the bacteria_odb10 result should be preferred over archaea_odb10 when taxonomy is uncertain, because using a marker set that is harder to satisfy provides a safer lower bound on completeness.\n\nE. BUSCO percentages across different lineage datasets can be misleadingly different because marker set universality and composition vary with lineage; interpretations should condition on an appropriate lineage choice and be cross-validated with orthogonal, assembly-centric metrics such as $k$-mer recall, read mapping breadth, and the presence of informational genes (for example, ribosomal proteins and RNA polymerase subunits).\n\nF. In highly adenine-thymine (AT)-biased genomes, fragmented BUSCO calls may be inflated by gene-calling errors and frameshifts; re-running in protein mode against six-frame-translated contigs or with gene prediction tuned for AT-rich genomes can reduce false fragmentation and better reflect true content.\n\nG. High contig N50 by itself is sufficient evidence of completeness even when BUSCO completeness is low, because high contiguity implies high completeness.", "solution": "The problem statement will first be validated for scientific soundness, self-consistency, and objectivity.\n\n### Step 1: Extract Givens\n\n- **Assembly A**:\n    - Type: Intracellular insect endosymbiont candidate\n    - Genome size: $\\approx 0.7$ Mb\n    - Guanine-cytosine (GC) content: $\\approx 30\\%$\n    - Long-read depth: $\\approx 100\\times$\n    - Contig N50: $\\approx 0.9$ Mb\n    - Read mapping breadth: $> 99\\%$\n    - $k$-mer recall relative to read set: $> 99\\%$\n    - BUSCO bacteria_odb10: $\\text{S } 58\\%, \\text{ D } 0\\%, \\text{ F } 6\\%, \\text{ M } 36\\%$\n    - BUSCO gammaproteobacteria_odb10: $\\text{S } 62\\%, \\text{ D } 0\\%, \\text{ F } 5\\%, \\text{ M } 33\\%$\n\n- **Assembly B**:\n    - Type: Free-living soil Actinobacteria\n    - Genome size: $\\approx 8.5$ Mb\n    - GC content: $\\approx 70\\%$\n    - Characteristics: Known gene family expansions\n    - Hybrid depth: $\\approx 150\\times$\n    - Contig N50: $\\approx 2.5$ Mb\n    - Read mapping breadth: $> 99\\%$\n    - $k$-mer recall: $> 99\\%$\n    - BUSCO bacteria_odb10: $\\text{S } 79\\%, \\text{ D } 17\\%, \\text{ F } 1\\%, \\text{ M } 3\\%$\n    - BUSCO actinobacteria_odb10: $\\text{S } 83\\%, \\text{ D } 14\\%, \\text{ F } 1\\%, \\text{ M } 2\\%$\n\n- **Assembly C**:\n    - Type: Putative archaeal symbiont\n    - Genome size: $\\approx 1.1$ Mb\n    - GC content: Moderate\n    - Long-read depth: $\\approx 60\\times$\n    - Contig N50: $\\approx 0.4$ Mb\n    - Read mapping breadth: $\\approx 96\\%$\n    - $k$-mer recall: $\\approx 97\\%$\n    - BUSCO bacteria_odb10: $\\text{S } 10\\%, \\text{ D } 0\\%, \\text{ F } 20\\%, \\text{ M } 70\\%$\n    - BUSCO archaea_odb10: $\\text{S } 52\\%, \\text{ D } 1\\%, \\text{ F } 12\\%, \\text{ M } 35\\%$\n\n- **Background Definitions and Assumptions**:\n    - BUSCO (Benchmarking Universal Single-Copy Orthologs) searches Hidden Markov Model (HMM) profiles of lineage-specific, near-universal single-copy orthologs to estimate completeness and duplication.\n    - Genome reduction in obligate endosymbionts frequently entails genuine loss or pseudogenization of genes that are otherwise universal in free-living lineages.\n    - Assembly contiguity metrics (for example, N50) reflect sequence continuity, not necessarily gene content completeness.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement describes a realistic scenario in microbial metagenomics and genome assembly quality assessment.\n\n- **Scientifically Grounded**: The problem is based on established principles and standard tools in genomics. BUSCO, $k$-mer analysis, read mapping, N50, and GC content are all fundamental metrics for evaluating genome assemblies. The biological scenarios (endosymbiont with reduced genome, free-living bacterium with large genome, putative archaeon) are plausible and commonly encountered.\n- **Well-Posed**: The problem provides sufficient data to evaluate the provided statements. The question requires an interpretation of these data based on first principles of genomics, which are also partially provided. A unique and meaningful evaluation of each option is possible.\n- **Objective**: The problem is stated using quantitative data and established terminology. It is free of subjective or ambiguous language.\n\nThe problem statement is scientifically sound, well-posed, and objective. It contains no contradictions or missing information that would prevent a rigorous analysis.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A full solution will be derived.\n\n### Solution Derivation and Option Analysis\n\nThe task is to evaluate each statement based on the provided data and fundamental principles of microbial genomics and bioinformatics.\n\n**A. For Assembly A, the lower BUSCO completeness against bacteria_odb10 and gammaproteobacteria_odb10 despite near-total read k-mer recall and mapping breadth is consistent with genuine lineage-specific gene loss typical of obligate endosymbionts; here, BUSCO primarily measures conservation of single-copy markers rather than assembly completion. Complementary checks should include coding density, presence of ribosomal RNA (rRNA) operon and transfer RNA (tRNA) complements, and evidence of circularization.**\n\n- **Analysis**: Assembly A shows extremely high technical completeness: read mapping breadth $> 99\\%$ and $k$-mer recall $> 99\\%$. This means the assembly faithfully represents nearly all the sequencing data. However, the BUSCO completeness is low (S is $58\\%-62\\%$, M is $33\\%-36\\%$). This discrepancy is a classic hallmark of a highly reduced genome, which is characteristic of obligate intracellular symbionts. Such organisms undergo reductive evolution, losing many genes that are essential for free-living relatives but are unnecessary in the stable host environment. BUSCO markers, being near-universal in broad lineages, are often among the genes lost. Therefore, in this context, the low BUSCO score reflects biological reality (genuine gene loss) rather than technical incompleteness of the assembly. The statement correctly identifies this and proposes a sound set of complementary checks. A high coding density, a complete set of rRNA and tRNA genes, and a circular chromosome (suggested by an N50 of $\\approx 0.9$ Mb for a $\\approx 0.7$ Mb genome) would provide strong evidence for a complete, albeit reduced, genome.\n- **Verdict**: **Correct**.\n\n**B. For Assembly B, the elevated duplicated BUSCO fraction necessarily indicates contamination, because single-copy orthologs cannot be biologically duplicated in a single genome; therefore, the assembly is contaminated and must be purged.**\n\n- **Analysis**: This statement contains two absolute claims that are biologically and technically incorrect. First, an elevated duplicated BUSCO fraction does not *necessarily* indicate contamination. Other causes include true gene duplications (paralogs), which are common evolutionary events, or assembly artifacts where different alleles of a heterozygous locus are assembled into separate contigs. The problem notes that Assembly B is from an organism with \"known gene family expansions,\" which makes true paralogy a very likely explanation. Second, the claim that \"single-copy orthologs cannot be biologically duplicated\" is false. BUSCO genes are defined as being single-copy in *most* organisms within a lineage, not all. Gene duplication is a primary driver of evolutionary innovation.\n- **Verdict**: **Incorrect**.\n\n**C. For Assembly B, an elevated duplicated BUSCO fraction can arise from true paralogy or from mis-annotation of split genes; to distinguish duplication from contamination, one should examine coverage parity across duplicates, local synteny, and whether duplicated markers come from families with frequent lineage-specific expansions. Complementary metrics include lineage-aware contamination estimates, consistency of depth across single-copy markers, and short-read depth over the duplicated loci.**\n\n- **Analysis**: This statement provides a correct and nuanced analysis of the situation in Assembly B (high duplication, D is $14\\%-17\\%$). It correctly identifies multiple potential causes: true paralogy (biologically real duplicates), mis-annotation, or assembly artifacts (not explicitly stated but implied). Crucially, it lists the exact set of standard bioinformatic procedures used to distinguish these possibilities. True duplicates within the same genome should share similar sequencing depth and GC content with the rest of the genome and may reside in syntenic blocks. Contaminating contigs often have different depth and/or GC content. Lineage-aware tools (e.g., CheckM) and manual inspection of read depth are standard practice. This statement accurately reflects the best practices for this type of investigation.\n- **Verdict**: **Correct**.\n\n**D. For Assembly C, the bacteria_odb10 result should be preferred over archaea_odb10 when taxonomy is uncertain, because using a marker set that is harder to satisfy provides a safer lower bound on completeness.**\n\n- **Analysis**: This statement demonstrates a fundamental misunderstanding of the BUSCO methodology. BUSCO's accuracy is entirely dependent on the selection of the appropriate lineage-specific marker set. Assembly C is a \"putative archaeal symbiont\". Using a bacterial marker set (bacteria_odb10) for an archaeon is incorrect. The resulting score (S $10\\%$, M $70\\%$) is not a \"safer lower bound\" on completeness; it is a meaningless artifact of extreme phylogenetic distance. The markers are missing because the organism is an archaeon, not because the assembly is poor. The appropriate set is archaea_odb10, which gives a much more informative (though still low) completeness of S $52\\%$. The goal is to obtain the most accurate estimate, not the lowest possible number.\n- **Verdict**: **Incorrect**.\n\n**E. BUSCO percentages across different lineage datasets can be misleadingly different because marker set universality and composition vary with lineage; interpretations should condition on an appropriate lineage choice and be cross-validated with orthogonal, assembly-centric metrics such as k-mer recall, read mapping breadth, and the presence of informational genes (for example, ribosomal proteins and RNA polymerase subunits).**\n\n- **Analysis**: This statement articulates a critical principle of genome quality assessment. As shown by Assembly C, the choice of lineage is paramount for a meaningful BUSCO result. Furthermore, BUSCO measures only one aspect of completeness (gene content relative to a specific marker set). A robust conclusion requires integrating evidence from multiple, independent (orthogonal) metrics. Assembly-centric metrics like $k$-mer recall and read mapping breadth are independent of gene content and measure technical completeness. Verifying the presence and completeness of core informational genes (e.g., those encoding ribosomal proteins) provides another, more targeted, gene-centric check. The statement is a perfect summary of best practices.\n- **Verdict**: **Correct**.\n\n**F. In highly adenine-thymine (AT)-biased genomes, fragmented BUSCO calls may be inflated by gene-calling errors and frameshifts; re-running in protein mode against six-frame-translated contigs or with gene prediction tuned for AT-rich genomes can reduce false fragmentation and better reflect true content.**\n\n- **Analysis**: Assembly A has a low GC content of $\\approx 30\\%$, making it an AT-biased genome (AT content $\\approx 70\\%$). Gene prediction algorithms, which are a core part of the default BUSCO workflow, can perform sub-optimally in genomes with extreme GC content. This can lead to incorrect gene models, such as predicting a single complete gene as two or more fragments. This artifactually inflates the \"Fragmented\" (F) BUSCO count. The statement correctly identifies this potential issue and proposes standard, valid solutions: bypassing the internal gene predictor by running BUSCO in protein mode (on six-frame translations of the contigs) or using a gene predictor specifically tuned for or trained on AT-rich genomes.\n- **Verdict**: **Correct**.\n\n**G. High contig N50 by itself is sufficient evidence of completeness even when BUSCO completeness is low, because high contiguity implies high completeness.**\n\n- **Analysis**: This statement is false and directly contradicted by one of the provided background definitions: \"Assembly contiguity metrics (for example, N50) reflect sequence continuity, not necessarily gene content completeness.\" N50 is a statistic that measures the contiguity of an assembly (i.e., how much of the genome is in large pieces). An assembly can be highly contiguous (very high N50) yet incomplete, for example, if a large plasmid or a significant portion of the chromosome was not assembled. Assembly A exemplifies this perfectly: it has a very high N50 ($\\approx 0.9$ Mb for a $\\approx 0.7$ Mb genome) but low gene completeness as measured by BUSCO. Contiguity and completeness are different, albeit related, aspects of assembly quality.\n- **Verdict**: **Incorrect**.", "answer": "$$\\boxed{ACEF}$$", "id": "2509741"}, {"introduction": "The final step in genome analysis is annotationâ€”finding the genes and other functional elements. This exercise demystifies a core bioinformatics tool by challenging you to build a simple gene finder from the ground up. By implementing the Viterbi algorithm for a gene-finding Hidden Markov Model (HMM), you will gain a deep, mechanistic understanding of how computational annotation translates the statistical properties of a genome into biological meaning [@problem_id:2509739].", "problem": "You are to formalize a Hidden Markov Model (HMM) to decode the most probable single-gene structure in a microbial genomic fragment, under the assumption that a gene comprises a start codon, a coding region that preserves reading frame, and a stop codon, with noncoding sequence elsewhere. Use fundamental definitions of HMMs and the Central Dogma of molecular biology as the starting base: a gene is a contiguous region beginning with an initiation codon and ending with a termination codon in-frame, and an HMM defines a joint distribution over a latent state sequence and an observed sequence via initial, transition, and emission probabilities. From these bases, derive the dynamic programming procedure that maximizes the joint probability over all latent state sequences consistent with an observed deoxyribonucleic acid sequence, and then implement it.\n\nModel specification. Use a discrete-time HMM over positions indexed by $t \\in \\{1,\\dots,L\\}$ for sequence length $L$, with the following finite state set for gene-structure categories:\n- Noncoding $N$.\n- Start region represented as a three-state chain $S_1 \\rightarrow S_2 \\rightarrow S_3$ to align with the canonical start codon.\n- Coding region represented as a three-state cycle $C_1 \\rightarrow C_2 \\rightarrow C_3 \\rightarrow C_1$ to enforce frame.\n- Stop region represented as a three-state chain beginning at $T_1$ and branching to capture the three canonical stop codons: $T_1 \\rightarrow T_{2A}$ or $T_{2G}$, then $T_{2A} \\rightarrow T_{3A}$ or $T_{3G}$, and $T_{2G} \\rightarrow T_{3A}$, finally $T_{3A} \\rightarrow N$ and $T_{3G} \\rightarrow N$. The set of states is thus $\\{N,S_1,S_2,S_3,C_1,C_2,C_3,T_1,T_{2A},T_{2G},T_{3A},T_{3G}\\}$.\n\nThe observation alphabet is the set of nucleotides $\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$ at each position $t$. Use the following initial distribution, transition probabilities, and emission probabilities.\n\nInitial distribution $\\pi$: assign $\\pi(N)=0.999$, $\\pi(S_1)=0.001$, and $\\pi(s)=0$ for all other states $s$.\n\nTransitions $a_{ij}$ from state $i$ to state $j$ (all omitted transitions have probability $0$):\n- From $N$: to $N$ with probability $0.995$, to $S_1$ with probability $0.005$.\n- From $S_1$: to $S_2$ with probability $0.999$, to $N$ with probability $0.001$.\n- From $S_2$: to $S_3$ with probability $0.999$, to $N$ with probability $0.001$.\n- From $S_3$: to $C_1$ with probability $0.999$, to $N$ with probability $0.001$.\n- From $C_1$: to $C_2$ with probability $0.99$, to $T_1$ with probability $0.01$.\n- From $C_2$: to $C_3$ with probability $1.0$.\n- From $C_3$: to $C_1$ with probability $0.98$, to $T_1$ with probability $0.02$.\n- From $T_1$: to $T_{2A}$ with probability $0.5$, to $T_{2G}$ with probability $0.5$.\n- From $T_{2A}$: to $T_{3A}$ with probability $0.5$, to $T_{3G}$ with probability $0.5$.\n- From $T_{2G}$: to $T_{3A}$ with probability $1.0$.\n- From $T_{3A}$: to $N$ with probability $1.0$.\n- From $T_{3G}$: to $N$ with probability $1.0$.\n\nEmissions $b_s(x)$ for state $s$ and nucleotide $x \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$:\n- For $N$: $(\\text{A}:\\,0.3,\\ \\text{C}:\\,0.2,\\ \\text{G}:\\,0.2,\\ \\text{T}:\\,0.3)$.\n- For $S_1$: $(\\text{A}:\\,0.999,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.001/3,\\ \\text{T}:\\,0.001/3)$.\n- For $S_2$: $(\\text{A}:\\,0.001/3,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.001/3,\\ \\text{T}:\\,0.999)$.\n- For $S_3$: $(\\text{A}:\\,0.001/3,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.999,\\ \\text{T}:\\,0.001/3)$.\n- For $C_1$: $(\\text{A}:\\,0.22,\\ \\text{C}:\\,0.28,\\ \\text{G}:\\,0.30,\\ \\text{T}:\\,0.20)$.\n- For $C_2$: $(\\text{A}:\\,0.18,\\ \\text{C}:\\,0.32,\\ \\text{G}:\\,0.30,\\ \\text{T}:\\,0.20)$.\n- For $C_3$: $(\\text{A}:\\,0.20,\\ \\text{C}:\\,0.30,\\ \\text{G}:\\,0.30,\\ \\text{T}:\\,0.20)$.\n- For $T_1$: $(\\text{A}:\\,0.001/3,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.001/3,\\ \\text{T}:\\,0.999)$.\n- For $T_{2A}$: $(\\text{A}:\\,0.999,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.001/3,\\ \\text{T}:\\,0.001/3)$.\n- For $T_{2G}$: $(\\text{A}:\\,0.001/3,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.999,\\ \\text{T}:\\,0.001/3)$.\n- For $T_{3A}$: $(\\text{A}:\\,0.999,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.001/3,\\ \\text{T}:\\,0.001/3)$.\n- For $T_{3G}$: $(\\text{A}:\\,0.001/3,\\ \\text{C}:\\,0.001/3,\\ \\text{G}:\\,0.999,\\ \\text{T}:\\,0.001/3)$.\n\nTask. Starting from the formal HMM definition and independence assumptions, derive the dynamic programming recursion that yields the most probable state sequence given an observed nucleotide sequence (the Viterbi decoding), justify its correctness, and then implement it using natural logarithms to prevent underflow. After decoding, identify the most probable gene structure as follows: report the first occurrence of a complete start-chain $S_1,S_2,S_3$ followed downstream by a complete stop-chain ending in either $T_{3A}$ or $T_{3G}$; the gene start index is the position of $S_1$ and the gene stop index is the position of $T_{3A}$ or $T_{3G}$. If no complete start-plus-stop is found in order, report that no gene is present.\n\nOutput. For each test case, your program must output a list of three values: the start index (integer), the stop index (integer), and the natural logarithm of the Viterbi path probability (floating-point). Indices must be $0$-based and inclusive. If no gene is found, output the pair $-1,-1$ for the indices but still output the logarithmic probability of the overall best path.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the three values (for example, $[\\,[i_1,j_1,\\ell_1],\\,[i_2,j_2,\\ell_2]\\,]$).\n\nTest suite. Use the following four observed sequences (strings over $\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$), each to be decoded independently using the same HMM specified above:\n- Case $1$ (a typical sequence with a clear gene): \"CCGCTTCCGCCATGGCCGAACAGTGATTTAC\".\n- Case $2$ (a sequence with no start codon): \"TTTCCCGGTTTTGCGCGTCTTCTTTCCC\".\n- Case $3$ (multiple potential starts; only one in-frame stop relative to a later start): \"TTTATGGCCGAACTAACCATGGCCGAATAACCT\".\n- Case $4$ (a gene starting at the first position and ending at the last): \"ATGGCCGAATGA\".\n\nYour implementation must strictly adhere to the model above and the output format described. No external input is provided; all parameters and the test suite are embedded in the program.", "solution": "The problem requires the formalization and implementation of a Hidden Markov Model (HMM) to identify a single gene structure within a given deoxyribonucleic acid (DNA) sequence. This will be accomplished by deriving and implementing the Viterbi algorithm to find the most probable sequence of hidden states, which correspond to genomic features (noncoding, start codon, coding region, stop codon).\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **States ($S$)**: A set of $12$ states: $\\{N, S_1, S_2, S_3, C_1, C_2, C_3, T_1, T_{2A}, T_{2G}, T_{3A}, T_{3G}\\}$.\n*   **Observations ($O$)**: The set of nucleotides $\\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$.\n*   **Initial Probabilities ($\\pi$)**: $\\pi(N)=0.999$, $\\pi(S_1)=0.001$, and $\\pi(s)=0$ for all other states $s$.\n*   **Transition Probabilities ($a_{ij}$)**:\n    *   $N \\to N$: $0.995$; $N \\to S_1$: $0.005$.\n    *   $S_1 \\to S_2$: $0.999$; $S_1 \\to N$: $0.001$.\n    *   $S_2 \\to S_3$: $0.999$; $S_2 \\to N$: $0.001$.\n    *   $S_3 \\to C_1$: $0.999$; $S_3 \\to N$: $0.001$.\n    *   $C_1 \\to C_2$: $0.99$; $C_1 \\to T_1$: $0.01$.\n    *   $C_2 \\to C_3$: $1.0$.\n    *   $C_3 \\to C_1$: $0.98$; $C_3 \\to T_1$: $0.02$.\n    *   $T_1 \\to T_{2A}$: $0.5$; $T_1 \\to T_{2G}$: $0.5$.\n    *   $T_{2A} \\to T_{3A}$: $0.5$; $T_{2A} \\to T_{3G}$: $0.5$.\n    *   $T_{2G} \\to T_{3A}$: $1.0$.\n    *   $T_{3A} \\to N$: $1.0$.\n    *   $T_{3G} \\to N$: $1.0$.\n    *   All other $a_{ij} = 0$.\n*   **Emission Probabilities ($b_s(x)$)**:\n    *   $b_N(x) = (\\text{A}:0.3, \\text{C}:0.2, \\text{G}:0.2, \\text{T}:0.3)$.\n    *   $b_{S_1}(x) = (\\text{A}:0.999, ...)$.\n    *   $b_{S_2}(x) = (..., \\text{T}:0.999)$.\n    *   $b_{S_3}(x) = (..., \\text{G}:0.999)$.\n    *   $b_{C_1}(x) = (\\text{A}:0.22, \\text{C}:0.28, \\text{G}:0.30, \\text{T}:0.20)$.\n    *   $b_{C_2}(x) = (\\text{A}:0.18, \\text{C}:0.32, \\text{G}:0.30, \\text{T}:0.20)$.\n    *   $b_{C_3}(x) = (\\text{A}:0.20, \\text{C}:0.30, \\text{G}:0.30, \\text{T}:0.20)$.\n    *   $b_{T_1}(x) = (..., \\text{T}:0.999)$.\n    *   $b_{T_{2A}}(x) = (\\text{A}:0.999, ...)$.\n    *   $b_{T_{2G}}(x) = (..., \\text{G}:0.999)$.\n    *   $b_{T_{3A}}(x) = (\\text{A}:0.999, ...)$.\n    *   $b_{T_{3G}}(x) = (..., \\text{G}:0.999)$.\n    (non-specified probabilities for codon states are $0.001/3$).\n*   **Task**:\n    1.  Derive the Viterbi algorithm.\n    2.  Implement it using natural logarithms.\n    3.  Decode four given sequences.\n    4.  Identify the first gene structure (start index of $S_1$, end index of $T_{3A}$/$T_{3G}$).\n    5.  Output results in a specific list format: `[start_index, stop_index, log_probability]`.\n    6.  Use $0$-based indexing.\n    7.  If no gene is found, output `[-1, -1, log_probability]`.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n*   **Scientifically Grounded**: The problem is an application of HMMs to gene finding, a standard and well-established method in bioinformatics. The model structure, though simplified, correctly represents the key components of a gene (start codon 'ATG', coding C+G/A+T content, stop codons 'TAA', 'TAG', 'TGA') according to the Central Dogma. The parameters are specified and mathematically consistent (probabilities sum to $1.0$). The problem is scientifically sound.\n*   **Well-Posed**: The problem asks for the single most probable state sequence for a given observation sequence. This is a classic decoding problem for which a unique and stable solution is guaranteed by the Viterbi algorithm. All necessary parameters for the HMM are provided. The problem is well-posed.\n*   **Objective**: The problem statement is precise, quantitative, and free of subjective language.\n\nThe problem does not exhibit any of the defined flaws (e.g., scientific unsoundness, incompleteness, ambiguity). All parameters are provided and consistent.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be derived and implemented.\n\n### Derivation of the Viterbi Algorithm\n\nLet the HMM be defined by the parameters $\\lambda = (S, O, \\pi, A, B)$, where $S$ is the state space, $O$ is the observation alphabet, $\\pi$ is the initial state distribution, $A$ is the state transition probability matrix, and $B$ is the observation emission probability matrix. We are given an observed sequence of nucleotides $X = (x_1, x_2, \\dots, x_L)$ of length $L$. We seek the most probable sequence of hidden states $Q^* = (q_1^*, q_2^*, \\dots, q_L^*)$ that could have generated $X$. This is the sequence $Q$ that maximizes the joint probability $P(X, Q | \\lambda)$.\n\nThe joint probability of an observation sequence $X$ and a state sequence $Q = (q_1, \\dots, q_L)$ is given by:\n$$ P(X, Q | \\lambda) = \\pi_{q_1} b_{q_1}(x_1) \\prod_{t=2}^{L} a_{q_{t-1}, q_t} b_{q_t}(x_t) $$\n\nFinding the maximum over all possible $|S|^L$ state sequences is computationally intractable. We use dynamic programming, via the Viterbi algorithm, to solve this efficiently. The algorithm relies on the principle of optimality.\n\nLet $v_t(j)$ be the probability of the most likely state sequence of length $t$ that ends in state $j \\in S$, having generated the first $t$ observations $x_1, \\dots, x_t$.\n$$ v_t(j) = \\max_{q_1, \\dots, q_{t-1}} P(x_1, \\dots, x_t, q_1, \\dots, q_{t-1}, q_t=j | \\lambda) $$\n\nWe derive a recursive formula for $v_t(j)$.\n\n**1. Initialization ($t=1$)**:\nThe probability of being in state $j$ at time $t=1$ is the initial probability of that state multiplied by the probability of emitting the first observation $x_1$.\n$$ v_1(j) = \\pi_j \\cdot b_j(x_1) $$\n\n**2. Recursion ($t=2, \\dots, L$)**:\nTo compute $v_t(j)$, we consider all possible previous states $i \\in S$ at time $t-1$. The most probable path to state $j$ at time $t$ must be an extension of one of the most probable paths to some state $i$ at time $t-1$. We take the maximum over all possible predecessor states $i$:\n$$ v_t(j) = \\left( \\max_{i \\in S} \\{ v_{t-1}(i) \\cdot a_{ij} \\} \\right) \\cdot b_j(x_t) $$\nTo reconstruct the path, we must store the predecessor state that maximized this probability at each step. We define a backpointer $\\psi_t(j)$:\n$$ \\psi_t(j) = \\underset{i \\in S}{\\arg\\max} \\{ v_{t-1}(i) \\cdot a_{ij} \\} $$\n\n**3. Termination ($t=L$)**:\nAfter computing the values for all states up to time $L$, the probability of the single most likely path is the maximum value in the final column of our dynamic programming table:\n$$ P^* = \\max_{j \\in S} v_L(j) $$\nThe final state of this optimal path is:\n$$ q_L^* = \\underset{j \\in S}{\\arg\\max} v_L(j) $$\n\n**4. Path Backtracking**:\nStarting with $q_L^*$, we trace backward using the stored backpointers to find the complete state sequence:\n$$ q_{t-1}^* = \\psi_t(q_t^*) \\quad \\text{for } t = L, L-1, \\dots, 2 $$\n\nThis procedure gives the optimal state sequence $Q^* = (q_1^*, \\dots, q_L^*)$ and its probability $P^*$.\n\n**Numerical Stability using Logarithms**:\nThe repeated multiplication of probabilities (numbers between $0$ and $1$) can lead to numerical underflow. To prevent this, we work with log-probabilities. The recursion becomes:\n$$ \\log v_t(j) = \\left( \\max_{i \\in S} \\{ \\log v_{t-1}(i) + \\log a_{ij} \\} \\right) + \\log b_j(x_t) $$\nThe backpointer calculation is similarly transformed:\n$$ \\psi_t(j) = \\underset{i \\in S}{\\arg\\max} \\{ \\log v_{t-1}(i) + \\log a_{ij} \\} $$\nThe initial step is $\\log v_1(j) = \\log \\pi_j + \\log b_j(x_1)$, and the final log-probability is $\\log P^* = \\max_{j \\in S} \\log v_L(j)$. Probabilities of $0$ become $-\\infty$ in log-space, which are handled correctly by the `max` operation.\n\n**Gene Finding Post-processing**:\nOnce the optimal state sequence $Q^*$ is determined, we parse it to find the specified gene structure. We search for the first index $i$ corresponding to the start of a start codon chain ($q_i^*=S_1$, $q_{i+1}^*=S_2$, $q_{i+2}^*=S_3$). If found, we then search for the first subsequent index $j > i+2$ that marks the end of a stop codon chain ($q_j^*=T_{3A}$ or $q_j^*=T_{3G}$). The Viterbi algorithm ensures that the path segment between $q_{i+2}^*$ and $q_j^*$ is the most probable one, which, according to the provided transition matrix, will consist of coding states and intermediate stop codon states. If this start-stop pattern is found, the gene is reported with start index $i$ and stop index $j$. If no such complete pattern is found in the decoded path, no gene is reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the HMM gene-finding problem for a suite of test cases.\n    \"\"\"\n    # 1. DEFINE THE HIDDEN MARKOV MODEL\n    states = {\n        'N': 0, 'S1': 1, 'S2': 2, 'S3': 3, 'C1': 4, 'C2': 5, 'C3': 6,\n        'T1': 7, 'T2A': 8, 'T2G': 9, 'T3A': 10, 'T3G': 11\n    }\n    num_states = len(states)\n    state_names = {v: k for k, v in states.items()}\n\n    observations = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_observations = len(observations)\n\n    # Initial probabilities (pi)\n    pi = np.zeros(num_states)\n    pi[states['N']] = 0.999\n    pi[states['S1']] = 0.001\n\n    # Transition probabilities (A)\n    # A[i, j] is the probability of transitioning from state i to state j\n    A = np.zeros((num_states, num_states))\n    A[states['N'], states['N']] = 0.995\n    A[states['N'], states['S1']] = 0.005\n    A[states['S1'], states['S2']] = 0.999\n    A[states['S1'], states['N']] = 0.001\n    A[states['S2'], states['S3']] = 0.999\n    A[states['S2'], states['N']] = 0.001\n    A[states['S3'], states['C1']] = 0.999\n    A[states['S3'], states['N']] = 0.001\n    A[states['C1'], states['C2']] = 0.99\n    A[states['C1'], states['T1']] = 0.01\n    A[states['C2'], states['C3']] = 1.0\n    A[states['C3'], states['C1']] = 0.98\n    A[states['C3'], states['T1']] = 0.02\n    A[states['T1'], states['T2A']] = 0.5\n    A[states['T1'], states['T2G']] = 0.5\n    A[states['T2A'], states['T3A']] = 0.5\n    A[states['T2A'], states['T3G']] = 0.5\n    A[states['T2G'], states['T3A']] = 1.0\n    A[states['T3A'], states['N']] = 1.0\n    A[states['T3G'], states['N']] = 1.0\n\n    # Emission probabilities (B)\n    # B[k, o] is the probability of emitting observation o from state k\n    B = np.zeros((num_states, num_observations))\n    e = 0.001 / 3.0\n    B[states['N']] = [0.3, 0.2, 0.2, 0.3]\n    B[states['S1']] = [0.999, e, e, e]\n    B[states['S2']] = [e, e, e, 0.999]\n    B[states['S3']] = [e, e, 0.999, e]\n    B[states['C1']] = [0.22, 0.28, 0.30, 0.20]\n    B[states['C2']] = [0.18, 0.32, 0.30, 0.20]\n    B[states['C3']] = [0.20, 0.30, 0.30, 0.20]\n    B[states['T1']] = [e, e, e, 0.999]\n    B[states['T2A']] = [0.999, e, e, e]\n    B[states['T2G']] = [e, e, 0.999, e]\n    B[states['T3A']] = [0.999, e, e, e]\n    B[states['T3G']] = [e, e, 0.999, e]\n    \n    # Convert to log-probabilities to prevent underflow\n    with np.errstate(divide='ignore'):\n        log_pi = np.log(pi)\n        log_A = np.log(A)\n        log_B = np.log(B)\n\n    # 2. VITERBI ALGORITHM IMPLEMENTATION\n    def viterbi_decode(obs_seq_str):\n        obs_seq = [observations[obs] for obs in obs_seq_str]\n        L = len(obs_seq)\n        \n        # DP table for Viterbi log-probabilities\n        viterbi_log_prob = np.full((num_states, L), -np.inf)\n        # DP table for backpointers\n        backpointers = np.zeros((num_states, L), dtype=int)\n\n        # Initialization step (t=0)\n        first_obs = obs_seq[0]\n        viterbi_log_prob[:, 0] = log_pi + log_B[:, first_obs]\n\n        # Recursion step (t=1 to L-1)\n        for t in range(1, L):\n            obs_t = obs_seq[t]\n            for j in range(num_states):\n                # Calculate max probability of transitioning to state j\n                trans_probs = viterbi_log_prob[:, t - 1] + log_A[:, j]\n                best_prev_state = np.argmax(trans_probs)\n                max_log_prob = trans_probs[best_prev_state]\n\n                # Store log prob and backpointer\n                viterbi_log_prob[j, t] = max_log_prob + log_B[j, obs_t]\n                backpointers[j, t] = best_prev_state\n        \n        # Termination: Find the best path's log-probability and final state\n        best_path_log_prob = np.max(viterbi_log_prob[:, -1])\n        last_state = np.argmax(viterbi_log_prob[:, -1])\n\n        # Backtracking\n        path = [last_state]\n        for t in range(L - 1, 0, -1):\n            prev_state = backpointers[path[-1], t]\n            path.append(prev_state)\n        # The path is built backwards, so reverse it\n        path.reverse()\n\n        return path, best_path_log_prob\n\n    # 3. GENE FINDING (POST-PROCESSING)\n    def find_gene(path):\n        start_idx = -1\n        stop_idx = -1\n        \n        s1 = states['S1']\n        s2 = states['S2']\n        s3 = states['S3']\n        t3a = states['T3A']\n        t3g = states['T3G']\n\n        # Find the first occurrence of a start codon pattern\n        for i in range(len(path) - 2):\n            if path[i] == s1 and path[i+1] == s2 and path[i+2] == s3:\n                # Found a potential start, now look for a stop\n                start_idx = i\n                # Search for a stop codon pattern downstream\n                for j in range(i + 3, len(path)):\n                    if path[j] == t3a or path[j] == t3g:\n                        stop_idx = j\n                        return start_idx, stop_idx # Return first complete gene\n                # If a start is found but no stop, abandon this start\n                start_idx = -1\n\n        return start_idx, stop_idx\n\n    # 4. PROCESS TEST CASES\n    test_cases = [\n        \"CCGCTTCCGCCATGGCCGAACAGTGATTTAC\",\n        \"TTTCCCGGTTTTGCGCGTCTTCTTTCCC\",\n        \"TTTATGGCCGAACTAACCATGGCCGAATAACCT\",\n        \"ATGGCCGAATGA\"\n    ]\n    \n    results = []\n    for seq in test_cases:\n        decoded_path, log_prob = viterbi_decode(seq)\n        start, stop = find_gene(decoded_path)\n        results.append([start, stop, log_prob])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2509739"}]}