{"hands_on_practices": [{"introduction": "Before any analysis of pangenome structure or dynamics can begin, we must first accurately determine which genes are present in each bacterial isolate from its raw sequencing data. This foundational step is fraught with challenges like repetitive DNA from paralogous genes, sequencing errors, and uneven read coverage. This exercise [@problem_id:2476538] challenges you to think like a bioinformatician, evaluating the components of a modern analysis pipeline to design a strategy that best navigates the trade-offs between sensitivity and specificity.", "problem": "You are given paired-end short-read sequencing data from a newly isolated bacterial strain and a species pangenome represented as a compacted De Bruijn graph (cDBG; compacted De Bruijn graph) built at $k=31$. The reads are paired-end $150$ base pairs (bp), mean insert size $350$ bp with standard deviation $50$ bp, nominal genome-wide depth $D=40\\times$, and per-base error rate $\\epsilon=5\\times 10^{-3}$. Genes in the pangenome are annotated as graph paths. For each gene path, the set of path unitigs is known, and for each unitig, the fraction of $k$-mers unique to that path versus shared with paralogs/alleles is precomputed. You may assume the following foundational facts:\n- Sequencing read starts along the genome behave approximately as a Poisson process; per-base read coverage can be modeled as independent Poisson with mean $\\lambda \\approx D$ after accounting for mapping uniqueness.\n- For a gene truly present in the isolate, coverage on its unique $k$-mers is approximately Poisson with mean $\\lambda_{\\text{eff}} \\approx D \\cdot u$, where $u \\in [0,1]$ is the fraction of $k$-mers in the path that are unique to that path in the cDBG.\n- For a gene truly absent, coverage on its unique $k$-mers arises from sequencing errors and spurious mappings, which is negligible at $k=31$ under typical nucleotide composition, so the effective mean on unique $k$-mers satisfies $\\lambda_{\\text{err}} \\approx D \\cdot \\epsilon \\cdot p_k$ with $p_k \\approx 4^{-31}$, i.e., $\\lambda_{\\text{err}} \\approx 0$ for practical purposes.\n- Properly oriented paired-end reads provide path-consistency evidence by bridging consecutive unitigs along a path; the count of such pairs across a given junction can be approximated by a Poisson random variable with mean proportional to $D$ and the effective window width.\n\nYou will design a pipeline to call gene presence/absence in the new isolate by aligning/counting reads on the pangenome graph and then applying thresholds to minimize both false positives and false negatives across core and accessory genes, including paralogous families. You have access to the following per-gene statistics computed from the reads and the cDBG:\n- $B_u$: fraction of bases along the gene path that are covered at least once by reads mapping to unique $k$-mers on the path (unique-breadth).\n- $\\tilde{d}_u$: median coverage across unique unitigs on the path normalized by the genome-wide median unitig coverage.\n- $C$: path coherence, defined as the fraction of consecutive unitig junctions along the annotated path that are bridged by at least a threshold number of properly oriented read pairs with insert sizes consistent with the library.\n- $\\text{LLR}$: log-likelihood ratio comparing the hypothesis that reads originate from the annotated path versus the best competing alternative path through the paralogous neighborhood, computed from unique $k$-mer coverage on the respective paths assuming independent Poisson sampling.\n\nAssume typical gene-path lengths near $L \\approx 1000$ bp and a range of uniqueness fractions $u \\in [0.2,0.9]$. Your goal is to select a pipeline and numerical thresholds that, under these assumptions, minimize both types of errors while maintaining scientific realism for accessory genes that may be entirely missing or present at variable copy number. Which option is the best choice?\n\nA. Use a graph-aware $k$-mer counting mapper to the cDBG. For each gene path, compute $B_u$ over bases belonging to unique unitigs; require total unique span $L_u \\ge 200$ bp. Call a gene present if all of the following hold: $B_u \\ge 0.6$; $\\tilde{d}_u \\in [0.5,2.0]$; $C \\ge 0.7$ when counting at least $2$ properly oriented pairs per junction; and $\\text{LLR} \\ge 2.3$ (evidence at least $10\\!:\\!1$) against the best alternative path. If $u  0.15$ or $L_u  200$ bp, do not call present; report as uncertain unless non-unique depth strongly exceeds $\\tilde{d}_u > 2.5$ (suggesting copy-number expansion).\n\nB. Map reads to a linearized pangenome reference using a standard aligner and call a gene present if at least $50\\%$ of its total path length (unique and shared segments together) has any coverage ($\\ge 1\\times$) or if any single contiguous covered segment $\\ge 100$ bp exists; do not use paired-end coherence or uniqueness; no normalization by genome-wide depth.\n\nC. Use the cDBG and require stringent corroboration to prevent false positives: $B_u \\ge 0.95$ on unique segments with $L_u \\ge 300$ bp; $\\tilde{d}_u \\in [0.9,1.1]$; $C=1.0$ with at least $5$ properly oriented pairs at every junction; and $\\text{LLR} \\ge 6.9$ (evidence at least $1000\\!:\\!1$) against alternatives. Genes failing any criterion are called absent.\n\nD. Use a cDBG $k$-mer presence test on all $k$-mers along the gene path without separating unique from shared $k$-mers. Call a gene present if at least $30\\%$ of its path bases have coverage ($B \\ge 0.3$) and the mean normalized depth across the path (including shared unitigs) satisfies $\\bar{d} \\ge 0.3$; do not use paired-end coherence or likelihood ratios.\n\nSelect the single best option.", "solution": "We begin from first principles relevant to short-read coverage and pangenome graphs. Under random shotgun sequencing, read starts along the genome can be modeled as a Poisson process, so per-base coverage is approximately independent Poisson with mean $\\lambda \\approx D$ for mappable regions. In a compacted De Bruijn graph (cDBG), a gene present in the isolate will contribute reads to unitigs along the path that are unique to that gene (unique $k$-mers); coverage on these unique unitigs follows Poisson with mean $\\lambda_{\\text{eff}} \\approx D \\cdot u$, where $u$ is the fraction of $k$-mers along the path that are unique. For a gene absent from the genome, unique $k$-mers for that path have no true source; their apparent coverage would come only from sequencing errors or mis-mapping. The probability that a random error produces a specific exact $k$-mer match is $p_k \\approx 4^{-k}$; for $k=31$, $p_k$ is astronomically small, so the effective mean coverage on unique unitigs for absent genes is $\\lambda_{\\text{err}} \\approx D \\cdot \\epsilon \\cdot p_k \\approx 40 \\cdot 5\\times 10^{-3} \\cdot 4^{-31} \\approx 0$, which implies essentially zero unique coverage when absent.\n\nDefine the unique-breadth statistic $B_u$ as the fraction of bases along the path’s unique unitigs covered by at least one read. For a present gene, per-base coverage on unique unitigs is $\\text{Poisson}(\\lambda_{\\text{eff}})$, so the probability a base is covered at least once is $1 - e^{-\\lambda_{\\text{eff}}}$. Therefore, the expected $B_u$ for present genes is $\\mathbb{E}[B_u] \\approx 1 - e^{-D \\cdot u}$. With $D=40$ and $u \\in [0.2,0.9]$, even the worst case $u=0.2$ yields $\\lambda_{\\text{eff}}=8$ and $\\mathbb{E}[B_u] \\approx 1 - e^{-8} \\approx 0.9997$. With $u=0.3$, $\\lambda_{\\text{eff}}=12$ and $1 - e^{-12} \\approx 0.999994$. This shows that for present genes, $B_u$ will be extremely close to $1$ provided the unique span $L_u$ is nontrivial (e.g., at least a few hundred bp). For absent genes, on unique unitigs, $\\lambda_{\\text{err}} \\approx 0$ gives $\\mathbb{E}[B_u] \\approx 0$. Thus, $B_u$ on unique unitigs is a powerful discriminator, and thresholds in the moderate range (e.g., $B_u \\ge 0.6$ or higher) will give negligible false negatives and negligible false positives for genes with sufficient unique span.\n\nWe also consider normalized depth on unique unitigs, $\\tilde{d}_u$, defined as the median coverage across unique unitigs divided by the genome-wide median unitig coverage. For a present single-copy gene, $\\tilde{d}_u$ should concentrate near $1$, but biases from GC-content, local accessibility, and stochasticity can induce variation; allowing a band such as $\\tilde{d}_u \\in [0.5,2.0]$ accommodates these effects and maintains sensitivity for accessory genes with lower mappability, while still excluding spurious sparse hits. For absent genes, on unique unitigs $\\tilde{d}_u$ will be near $0$.\n\nFor path coherence, define $C$ as the fraction of consecutive unitig junctions bridged by at least $r$ properly oriented read pairs. Let the number of pairs across a junction be approximately $\\text{Poisson}(\\mu)$ with $\\mu \\propto D$ and an effective window width tied to the insert size distribution. With $D=40\\times$ and typical fragment lengths around $350$ bp, $\\mu$ will be large enough that the probability of having at least $2$ pairs across most junctions is high for present genes, whereas for absent genes, the probability of spurious pairs consistently bridging many junctions is negligible. Requiring $C \\ge 0.7$ with at least $2$ pairs per bridged junction is a balanced coherence check that greatly reduces false positives due to local repeats without incurring many false negatives from occasional junction undercoverage.\n\nFinally, to contend with paralogous neighborhoods, compute a log-likelihood ratio $\\text{LLR}$ comparing the annotated path to the best competing path via unique coverage, assuming Poisson sampling. Under a Poisson model for counts $x$ with mean $\\lambda$, log-likelihoods differ by $\\log \\mathcal{L} \\propto x \\log \\lambda - \\lambda$. A threshold $\\text{LLR} \\ge \\log 10 \\approx 2.3$ corresponds to a Bayes factor of at least $10\\!:\\!1$, which is a reasonable evidentiary standard to avoid swapping to a competing paralog path.\n\nWith these principles, we now evaluate each option.\n\nOption A analysis:\n- $B_u \\ge 0.6$ on unique unitigs with $L_u \\ge 200$ bp: For present genes with $u \\ge 0.2$, we have $\\lambda_{\\text{eff}} \\ge 8$ and $\\mathbb{E}[B_u] \\approx 0.9997$. The number of uncovered unique bases $U$ over $L_u$ is $\\text{Binomial}(L_u, e^{-\\lambda_{\\text{eff}}})$ with mean $L_u e^{-\\lambda_{\\text{eff}}}$. For $L_u=200$ bp and $\\lambda_{\\text{eff}}=8$, $\\mathbb{E}[U] \\approx 200 e^{-8} \\approx 0.067$. The probability that $U/L_u > 0.4$ (i.e., $B_u  0.6$) is exponentially small by a Chernoff bound. Thus, false negatives from the $B_u$ threshold are negligible for present genes with minimal unique span. For absent genes, $B_u \\approx 0$, so false positives are essentially zero.\n- $\\tilde{d}_u \\in [0.5,2.0]$: This band tolerates realistic coverage fluctuations and copy-number variation while excluding spurious low-depth noise; it prevents calling presence based on a few stray reads.\n- $C \\ge 0.7$ with at least $2$ pairs per bridged junction: For a gene path with, say, $J \\approx 5$ to $10$ junctions over $L \\approx 1000$ bp, present genes will usually have multiple pairs over most junctions, so achieving $C \\ge 0.7$ is highly probable. Absent genes are unlikely to produce coherent bridges across the majority of junctions by chance.\n- $\\text{LLR} \\ge 2.3$: This prevents misassignment to the wrong paralog when both paths have some shared coverage; a $10\\!:\\!1$ threshold balances sensitivity and specificity.\n- Handling $u  0.15$ or $L_u  200$ bp by returning uncertainty avoids overconfident calls in poorly discriminated contexts, reducing both false positives and negatives by not forcing binary calls when the model has little power.\n\nThis pipeline operationalizes the Poisson coverage model on unique sequence, paired-end coherence, and a Bayesian comparison to paralogs, with thresholds set to be permissive enough to maintain sensitivity across accessory genes while robustly filtering artifacts. It therefore minimizes both error types under the stated conditions.\n\nOption B analysis:\n- Using total path breadth including shared segments with a $50\\%$ threshold and accepting any single $\\ge 100$ bp covered segment, without uniqueness or coherence, induces high false positives in paralogous families. Shared unitigs from other paralogs or conserved domains will inflate breadth even when the target gene is absent. Without normalization, local depth deviations can cause spurious calls. This violates the unique-coverage discrimination implied by $\\lambda_{\\text{err}} \\approx 0$ only on unique $k$-mers. Verdict: Incorrect due to high false positive rate.\n\nOption C analysis:\n- $B_u \\ge 0.95$ and $\\tilde{d}_u \\in [0.9,1.1]$ with $C=1.0$ and $\\ge 5$ pairs at every junction, plus $\\text{LLR} \\ge 6.9$ ($1000\\!:\\!1$), is overly stringent. While such thresholds reduce false positives, they dramatically increase false negatives: stochastic undercoverage, GC bias, or fragmentation can easily yield $B_u  0.95$ or some junctions with fewer than $5$ pairs, especially for shorter unique spans or accessory genes with uneven coverage. The tight depth band $[0.9,1.1]$ is unrealistic for many bacterial genomes. Verdict: Incorrect due to high false negative rate.\n\nOption D analysis:\n- Aggregating across all $k$-mers (unique and shared) with a low breadth threshold $B \\ge 0.3$ and mean depth $\\bar{d} \\ge 0.3$ ignores the key discriminative value of unique $k$-mers and paralog competition. Shared sequence from other family members will frequently meet these lenient thresholds even when the gene is absent. No path coherence or likelihood ratio is used, so misassignment in complex neighborhoods is likely. Verdict: Incorrect due to high false positive rate.\n\nTherefore, Option A is the best choice under the stated assumptions and goals, as it directly exploits the Poisson behavior of coverage on unique sequence, incorporates paired-end structural evidence, and compares alternative paths with a calibrated log-likelihood ratio, while using thresholds that are tight enough to limit artifacts but broad enough to retain true accessory genes with realistic variability.", "answer": "$$\\boxed{A}$$", "id": "2476538"}, {"introduction": "With a presence-absence matrix in hand, a primary task is to classify genes into categories like the core, shell, or cloud genomes based on their observed frequency. However, any classification is inherently a statement about the sample, not necessarily the entire species. This problem [@problem_id:2476544] demonstrates not only how to apply these essential frequency-based definitions but also how to use a Bayesian framework to model our uncertainty and make robust predictions about gene frequency in future samples.", "problem": "A bacterial species is sampled across $50$ genomes with a binary presence–absence matrix for a single gene of interest. The gene is observed to be present in $44$ of the $50$ genomes. In the study, genes are categorized by their frequency across genomes as follows: strict core if present in $100\\%$ of genomes, soft core if present in at least $95\\%$ of genomes, shell if present in at least $15\\%$ but less than $95\\%$ of genomes, and cloud if present in less than $15\\%$ of genomes. Assume the following modeling framework: gene presence in each genome is a Bernoulli random variable with an unknown population frequency $p$, the genomes are exchangeable and independent given $p$, and the prior on $p$ is $\\operatorname{Beta}(1,1)$.\n\nUsing only the definitions above, determine the category of this gene at sample size $50$. Then suppose the sample size doubles to $100$ genomes while the empirical frequency remains the same as the current sample frequency. Determine the category under this doubled sample using the same thresholds.\n\nFinally, under the Bernoulli–Beta model, compute the posterior predictive expected number of genomes (out of $100$) in which the gene will be present, and report this number as your final answer. Round your answer to four significant figures.", "solution": "The problem will be validated before a solution is attempted.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Initial sample size: $N_1 = 50$ genomes.\n- Number of genomes with the gene present: $k_1 = 44$.\n- Gene category definitions based on presence frequency $f$:\n    - Strict core: $f = 100\\%$.\n    - Soft core: $f \\ge 95\\%$.\n    - Shell: $15\\% \\le f  95\\%$.\n    - Cloud: $f  15\\%$.\n- Modeling framework:\n    - Gene presence in a genome is a Bernoulli random variable with parameter $p$.\n    - Genomes are independent and identically distributed trials given $p$.\n    - Prior distribution on $p$ is $\\operatorname{Beta}(1, 1)$.\n- Scenario 2:\n    - New sample size: $N_2 = 100$ genomes.\n    - Empirical frequency is unchanged from the first sample.\n- Final Task:\n    - Compute the posterior predictive expected number of genomes with the gene present in a new sample of $100$ genomes, based on the initial sample data ($N_1=50$, $k_1=44$).\n    - Round the final answer to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded. The pangenome concept, with its categorization of genes into core, shell, and cloud, is a fundamental paradigm in modern microbial genomics. The use of a Bernoulli-Beta conjugate model to represent uncertainty in gene frequency is a standard and appropriate statistical method in bioinformatics. The problem is well-posed, providing all necessary definitions, data, and constraints required for a unique solution. The language is objective and precise. The problem does not violate any fundamental principles, is not based on false premises, and contains no contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be provided.\n\n**Solution**\n\nThe problem consists of three parts. The first two parts concern the classification of a gene based on its empirical frequency in a sample. The third part requires a Bayesian approach to calculate a posterior predictive expectation.\n\n**Part 1: Gene Category for Sample Size $N_1 = 50$**\n\nGiven a sample of $N_1 = 50$ genomes, the gene is observed in $k_1 = 44$ of them. The empirical frequency of the gene, denoted as $f_1$, is the ratio of the number of genomes carrying the gene to the total number of genomes sampled.\n$$f_1 = \\frac{k_1}{N_1} = \\frac{44}{50} = 0.88$$\nTo compare this with the given thresholds, we express the frequency as a percentage: $0.88 \\times 100\\% = 88\\%$. According to the definitions:\n- Strict core: requires $100\\%$ presence.\n- Soft core: requires presence in at least $95\\%$ of genomes.\n- Shell: requires presence in at least $15\\%$ but less than $95\\%$ of genomes.\n- Cloud: requires presence in less than $15\\%$ of genomes.\n\nThe observed frequency of $88\\%$ satisfies the condition $15\\% \\le 88\\%  95\\%$. Therefore, based on the sample of $50$ genomes, the gene is classified in the **shell** category.\n\n**Part 2: Gene Category for Sample Size $N_2 = 100$**\n\nThe problem states that for a new sample size of $N_2 = 100$, the empirical frequency remains the same as in the first sample. Thus, $f_2 = f_1 = 0.88$, which corresponds to $88\\%$. The number of genomes with the gene in this hypothetical sample would be $k_2 = f_2 \\times N_2 = 0.88 \\times 100 = 88$. The gene classification is based solely on this frequency. Since the frequency is unchanged, the classification also remains unchanged. The condition $15\\% \\le 88\\%  95\\%$ is still met. Therefore, for a sample size of $100$ with the same empirical frequency, the gene is still classified in the **shell** category.\n\n**Part 3: Posterior Predictive Expected Number**\n\nThis part of the problem requires the application of a Bayesian model. The unknown population frequency of the gene, $p$, is treated as a random variable.\nThe model is as follows:\n- The prior distribution for $p$ is specified as a Beta distribution with parameters $\\alpha_0 = 1$ and $\\beta_0 = 1$:\n  $$p \\sim \\operatorname{Beta}(\\alpha_0, \\beta_0) = \\operatorname{Beta}(1, 1)$$\n  This is a uniform distribution on the interval $[0, 1]$, representing maximal prior uncertainty about $p$.\n- The observed data consists of $k_1 = 44$ \"successes\" (gene presence) in $N_1 = 50$ independent Bernoulli trials. The likelihood of the data given $p$ is proportional to $p^{k_1}(1-p)^{N_1-k_1}$.\n\nThe Beta distribution is the conjugate prior for the Bernoulli/Binomial likelihood. Therefore, the posterior distribution of $p$ after observing the data is also a Beta distribution. The parameters of the posterior distribution, $\\alpha'$ and $\\beta'$, are updated as follows:\n$$\\alpha' = \\alpha_0 + k_1 = 1 + 44 = 45$$\n$$\\beta' = \\beta_0 + (N_1 - k_1) = 1 + (50 - 44) = 1 + 6 = 7$$\nSo, the posterior distribution for the gene frequency $p$ is:\n$$p | (k_1=44, N_1=50) \\sim \\operatorname{Beta}(45, 7)$$\n\nWe are asked to compute the posterior predictive expected number of genomes in which the gene will be present in a new sample of $M = 100$ genomes. Let $Y$ be the random variable representing this number. The expected value of $Y$, $E[Y]$, is given by the new sample size $M$ multiplied by the posterior expected value of $p$.\n\nThe expected value of a random variable following a Beta distribution $\\operatorname{Beta}(\\alpha, \\beta)$ is $E[p] = \\frac{\\alpha}{\\alpha + \\beta}$.\nFor our posterior distribution $\\operatorname{Beta}(45, 7)$, the posterior mean of $p$ is:\n$$E[p | \\text{data}] = \\frac{\\alpha'}{\\alpha' + \\beta'} = \\frac{45}{45 + 7} = \\frac{45}{52}$$\nThis value, approximately $0.8654$, represents our updated belief about the true frequency of the gene in the population.\n\nNow, we can compute the posterior predictive expected number of genomes with the gene in a new sample of size $M=100$:\n$$E[Y | \\text{data}] = M \\times E[p | \\text{data}] = 100 \\times \\frac{45}{52}$$\n$$E[Y | \\text{data}] = \\frac{4500}{52} = \\frac{1125}{13} \\approx 86.5384615...$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $8, 6, 5, 3$. The fifth digit is $8$, so we round up the third decimal place.\n$$E[Y | \\text{data}] \\approx 86.54$$\n\nThis is the final answer.", "answer": "$$\n\\boxed{86.54}\n$$", "id": "2476544"}, {"introduction": "The static picture of a pangenome is the result of dynamic evolutionary processes, with selection playing a key role in shaping the frequencies of accessory genes over time. This exercise [@problem_id:2476519] offers a hands-on opportunity to act as a population geneticist by fitting a logistic model to time-series prevalence data. By implementing this analysis, you will learn how to quantify the strength and direction of natural selection ($s$) acting on a gene from real-world-like observations, a central task in evolutionary microbiology.", "problem": "You are given time-series data describing the prevalence of a single accessory gene across a population of hosts at multiple time points. For each time point, you observe the number of hosts carrying the gene out of a total number of hosts sampled. Assume that the host population is large and well-mixed, and that the net selection on the gene across hosts can be approximated by a constant selection coefficient, denoted by $s$, over the sampling period. Under this assumption and a deterministic selection model, the prevalence $p(t)$ of the gene across hosts satisfies the ordinary differential equation $dp/dt = s\\,p(t)\\,(1 - p(t))$, whose solution implies a linear relationship in the log-odds scale: $\\mathrm{logit}(p(t)) = \\alpha + s\\,t$, where $\\alpha$ is an unknown intercept capturing the initial log-odds. The sampling model at each time $t_i$ is $Y_i \\sim \\mathrm{Binomial}(n_i, p(t_i))$, with independent observations across time points.\n\nYour task is to write a program that, for each provided test case, estimates the selection coefficient $s$ by Maximum Likelihood Estimation (MLE) under the Binomial Generalized Linear Model (GLM) with a logit link, where $\\mathrm{logit}(p(t)) = \\alpha + s\\,t$. Compute a two-sided Wald confidence interval for $s$ based on the observed Fisher information at the MLE. The confidence interval limits must be computed as $s \\pm z \\cdot \\mathrm{SE}(s)$ with $z$ equal to the standard normal $0.975$ quantile and $\\mathrm{SE}(s)$ obtained from the square root of the $(2,2)$ element of the inverse of the observed information matrix evaluated at the MLE. You must implement the estimation numerically using the Iteratively Reweighted Least Squares (IRLS) algorithm for the Binomial GLM. All quantities must be reported per unit time of $t$.\n\nUse the following test suite of three cases. In each case, $t$ is the vector of time points, $n$ is the vector of total hosts sampled at each time, and $y$ is the vector of observed carriers at each time. All integers and time values are exact.\n\n- Case A (growth, moderate prevalence rise):\n  - $t = [0,2,4,6,8]$\n  - $n = [200,200,200,200,200]$\n  - $y = [20,34,54,80,110]$\n\n- Case B (decline from high prevalence):\n  - $t = [0,1,2,3,4,5]$\n  - $n = [150,150,150,150,150,150]$\n  - $y = [120,113,106,98,89,80]$\n\n- Case C (near-neutral change at low prevalence):\n  - $t = [0,5,10,15,20]$\n  - $n = [500,500,500,500,500]$\n  - $y = [25,28,30,33,36]$\n\nScientific and statistical bases you must use:\n- Accessory gene prevalence across hosts changes deterministically under effective selection according to $dp/dt = s\\,p\\,(1-p)$, implying $\\mathrm{logit}(p(t)) = \\alpha + s\\,t$.\n- Sampling at each time follows the Binomial model $Y_i \\sim \\mathrm{Binomial}(n_i, p_i)$ with $p_i = \\mathrm{logit}^{-1}(\\alpha + s\\,t_i)$.\n- The Binomial GLM with logit link is estimated by IRLS, and the observed information matrix at convergence equals $X^\\top W X$, where $X$ is the design matrix with columns $[1, t]$ and $W$ is the diagonal matrix with entries $n_i\\,\\hat{p}_i\\,(1-\\hat{p}_i)$.\n\nAlgorithmic requirements:\n- Implement IRLS to estimate $(\\alpha, s)$ by solving $(X^\\top W X)\\,\\beta = X^\\top W z$ iteratively, with $\\beta = (\\alpha, s)^\\top$ and working response $z = \\eta + (y - n\\,\\hat{p})/(n\\,\\hat{p}(1-\\hat{p}))$, where $\\eta = X\\beta$ and $\\hat{p} = \\mathrm{logit}^{-1}(\\eta)$.\n- At convergence, compute the covariance matrix of $\\hat{\\beta}$ as $(X^\\top W X)^{-1}$ and report $\\hat{s}$ and its Wald interval using $z = 1.959963984540054$.\n- For numerical stability, ensure that each $\\hat{p}_i$ is bounded away from $0$ and $1$ during iterations by clipping to $[\\varepsilon, 1-\\varepsilon]$ with a small $\\varepsilon$, and also avoid division by zero by using the same clipping in the weights.\n\nOutput specification:\n- For each case, output the triple $[\\hat{s}, \\mathrm{lower}, \\mathrm{upper}]$, where $\\mathrm{lower}$ and $\\mathrm{upper}$ are the two-sided Wald $95$ percent confidence bounds for $s$.\n- Round all three numbers to $6$ decimal places in decimal form (not as fractions and not with a percent sign).\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of lists without spaces, in the form: $[[\\hat{s}_A,\\mathrm{lower}_A,\\mathrm{upper}_A],[\\hat{s}_B,\\mathrm{lower}_B,\\mathrm{upper}_B],[\\hat{s}_C,\\mathrm{lower}_C,\\mathrm{upper}_C]]$.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Model for gene prevalence dynamics:** The prevalence $p(t)$ follows the ordinary differential equation $dp/dt = s\\,p(t)\\,(1 - p(t))$.\n- **Linearized model:** The solution to the ODE implies a linear relationship on the log-odds scale: $\\mathrm{logit}(p(t)) = \\alpha + s\\,t$, where $\\alpha$ is an intercept and $s$ is the selection coefficient.\n- **Sampling model:** The number of observed gene carriers $Y_i$ at time $t_i$ from a sample of size $n_i$ follows a binomial distribution: $Y_i \\sim \\mathrm{Binomial}(n_i, p(t_i))$, with observations being independent across time points.\n- **Estimation framework:** The parameters $(\\alpha, s)$ are to be estimated via Maximum Likelihood Estimation (MLE) under a Binomial Generalized Linear Model (GLM) with a logit link function.\n- **Numerical algorithm:** The MLE must be found using the Iteratively Reweighted Least Squares (IRLS) algorithm.\n- **IRLS update equations:**\n    - The parameter vector $\\beta = (\\alpha, s)^\\top$ is updated by solving $(X^\\top W X)\\,\\beta = X^\\top W z$.\n    - The design matrix $X$ has columns $[1, t]$.\n    - The working response is $z = \\eta + (y - n\\,\\hat{p})/(n\\,\\hat{p}(1-\\hat{p}))$.\n    - The linear predictor is $\\eta = X\\beta$.\n    - The fitted probabilities are $\\hat{p} = \\mathrm{logit}^{-1}(\\eta)$.\n    - The weight matrix $W$ is a diagonal matrix with entries $n_i\\,\\hat{p}_i\\,(1-\\hat{p}_i)$.\n- **Confidence interval:** A two-sided Wald confidence interval for $s$ is to be computed as $s \\pm z \\cdot \\mathrm{SE}(s)$.\n    - The quantile $z$ is specified as $1.959963984540054$.\n    - The standard error $\\mathrm{SE}(s)$ is derived from the inverse of the observed Fisher information matrix, $I(\\hat{\\beta}) = X^\\top W X$, evaluated at the MLE. Specifically, $\\mathrm{SE}(s)$ is the square root of the $(2,2)$ element of $(I(\\hat{\\beta}))^{-1}$.\n- **Numerical stability:** Fitted probabilities $\\hat{p}_i$ must be clipped to $[\\varepsilon, 1-\\varepsilon]$ to prevent values of $0$ or $1$.\n- **Test Cases:**\n    - Case A: $t = [0,2,4,6,8]$, $n = [200,200,200,200,200]$, $y = [20,34,54,80,110]$.\n    - Case B: $t = [0,1,2,3,4,5]$, $n = [150,150,150,150,150,150]$, $y = [120,113,106,98,89,80]$.\n    - Case C: $t = [0,5,10,15,20]$, $n = [500,500,500,500,500]$, $y = [25,28,30,33,36]$.\n- **Output Specification:** For each case, report a list $[\\hat{s}, \\mathrm{lower}, \\mathrm{upper}]$ rounded to $6$ decimal places. The final output is a single line containing a list of these lists.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in population genetics and modern statistical methods. The logistic model for gene frequency change under selection is a cornerstone of the field. The use of a Binomial GLM with a logit link function (logistic regression) is the standard, correct statistical approach for analyzing such binary outcome data (presence/absence of a gene) as a function of a continuous predictor (time).\n- **Well-Posed:** The problem provides a complete specification. It defines the model, the data structure, the estimation algorithm (IRLS), the method for calculating confidence intervals (Wald interval based on observed Fisher information), and all necessary constants and data for the test cases. A unique and stable solution is expected.\n- **Objective:** The problem is stated with mathematical and statistical precision, devoid of any subjective or ambiguous language.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is scientifically sound, well-posed, objective, complete, and non-trivial.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\n\nThe objective is to estimate the selection coefficient $s$ from time-series data on gene prevalence. The problem specifies a Binomial Generalized Linear Model (GLM) with a logit link function.\n\nThe model is defined by:\n1.  **Systematic Component:** The linear predictor $\\eta_i$ for the $i$-th observation at time $t_i$ is given by $\\eta_i = \\alpha + s \\cdot t_i$. In matrix notation, for the entire dataset, $\\boldsymbol{\\eta} = X\\beta$, where $\\beta = [\\alpha, s]^\\top$ is the vector of parameters and $X$ is the design matrix. The design matrix has $N$ rows (for $N$ time points) and $2$ columns. The first column is a vector of ones (for the intercept $\\alpha$) and the second column is the vector of time points $\\mathbf{t}$.\n    $$\n    X = \\begin{pmatrix} 1  t_1 \\\\ 1  t_2 \\\\ \\vdots  \\vdots \\\\ 1  t_N \\end{pmatrix}, \\quad \\beta = \\begin{pmatrix} \\alpha \\\\ s \\end{pmatrix}\n    $$\n2.  **Link Function:** The linear predictor $\\eta_i$ is related to the expected prevalence $p_i$ via the logit link function: $\\eta_i = \\mathrm{logit}(p_i) = \\ln\\left(\\frac{p_i}{1-p_i}\\right)$. The inverse link function gives the probability: $p_i = \\mathrm{logit}^{-1}(\\eta_i) = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} = \\frac{1}{1+e^{-\\eta_i}}$.\n3.  **Random Component:** The observed counts $y_i$ are assumed to follow a Binomial distribution, $y_i \\sim \\mathrm{Binomial}(n_i, p_i)$, where $n_i$ is the sample size at time $t_i$.\n\nThe parameters $\\beta$ are estimated by maximizing the log-likelihood function. For a GLM, this is efficiently done using the Iteratively Reweighted Least Squares (IRLS) algorithm. IRLS is equivalent to the Newton-Raphson method for finding the root of the score function.\n\nThe IRLS algorithm proceeds as follows:\n1.  **Initialization:** Choose an initial estimate for the parameter vector, $\\beta^{(0)}$. A common choice is $\\beta^{(0)} = \\mathbf{0}$.\n2.  **Iteration:** For iteration $k=0, 1, 2, \\dots$ until convergence:\n    a.  Calculate the current linear predictor: $\\boldsymbol{\\eta}^{(k)} = X \\beta^{(k)}$.\n    b.  Calculate the current fitted probabilities (mean values): $\\mathbf{\\hat{p}}^{(k)} = (1 + \\exp(-\\boldsymbol{\\eta}^{(k)}))^{-1}$. For numerical stability, these probabilities are clipped to a small interval $[\\varepsilon, 1-\\varepsilon]$.\n    c.  Calculate the diagonal weight matrix $W^{(k)}$. The diagonal elements are $W_{ii}^{(k)} = n_i \\hat{p}_i^{(k)} (1-\\hat{p}_i^{(k)})$. This is the variance of the binomial response, $n_i p_i (1-p_i)$, evaluated at the current estimate $\\hat{p}_i^{(k)}$.\n    d.  Calculate the working response vector $\\mathbf{z}^{(k)}$. Its elements are $z_i^{(k)} = \\eta_i^{(k)} + \\frac{y_i - n_i \\hat{p}_i^{(k)}}{n_i \\hat{p}_i^{(k)} (1-\\hat{p}_i^{(k)})}$. This can be viewed as a first-order Taylor expansion of the link function around the current mean.\n    e.  Update the parameter estimate by solving the weighted least squares problem:\n        $$\n        \\beta^{(k+1)} = (X^\\top W^{(k)} X)^{-1} X^\\top W^{(k)} \\mathbf{z}^{(k)}\n        $$\n3.  **Convergence:** The iterations stop when the change in the parameter vector, e.g., $||\\beta^{(k+1)} - \\beta^{(k)}||_2$, falls below a predefined tolerance.\n\nUpon convergence, the final estimate is $\\hat{\\beta} = [\\hat{\\alpha}, \\hat{s}]^\\top$. The asymptotic covariance matrix of this estimator is estimated by the inverse of the observed Fisher information matrix, evaluated at the MLE:\n$$\n\\mathrm{Cov}(\\hat{\\beta}) = I(\\hat{\\beta})^{-1} = (X^\\top \\hat{W} X)^{-1}\n$$\nwhere $\\hat{W}$ is the final weight matrix at convergence.\n\nThe standard error of the selection coefficient estimate, $\\mathrm{SE}(\\hat{s})$, is the square root of the second diagonal element (the $(2,2)$ element) of this covariance matrix:\n$$\n\\mathrm{SE}(\\hat{s}) = \\sqrt{[\\mathrm{Cov}(\\hat{\\beta})]_{2,2}}\n$$\nFinally, a $(1-\\gamma) \\times 100\\%$ two-sided Wald confidence interval for $s$ is constructed as:\n$$\n\\hat{s} \\pm z_{1-\\gamma/2} \\cdot \\mathrm{SE}(\\hat{s})\n$$\nFor a $95\\%$ confidence interval, $\\gamma = 0.05$, and $z_{0.975} \\approx 1.96$. The problem provides the precise value $z = 1.959963984540054$.\n\nThe implementation will apply this algorithm to each of the three test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It estimates the selection coefficient 's' and its 95% Wald confidence interval\n    for three different scenarios of gene prevalence data.\n    \"\"\"\n\n    test_cases = [\n        # Case A (growth, moderate prevalence rise)\n        {\n            \"t\": np.array([0, 2, 4, 6, 8], dtype=float),\n            \"n\": np.array([200, 200, 200, 200, 200], dtype=float),\n            \"y\": np.array([20, 34, 54, 80, 110], dtype=float),\n        },\n        # Case B (decline from high prevalence)\n        {\n            \"t\": np.array([0, 1, 2, 3, 4, 5], dtype=float),\n            \"n\": np.array([150, 150, 150, 150, 150, 150], dtype=float),\n            \"y\": np.array([120, 113, 106, 98, 89, 80], dtype=float),\n        },\n        # Case C (near-neutral change at low prevalence)\n        {\n            \"t\": np.array([0, 5, 10, 15, 20], dtype=float),\n            \"n\": np.array([500, 500, 500, 500, 500], dtype=float),\n            \"y\": np.array([25, 28, 30, 33, 36], dtype=float),\n        },\n    ]\n\n    results = []\n    for case_data in test_cases:\n        s_hat, lower, upper = estimate_s_with_ci(\n            case_data[\"t\"], case_data[\"n\"], case_data[\"y\"]\n        )\n        results.append([round(val, 6) for val in [s_hat, lower, upper]])\n\n    # Format the final output string as specified\n    output_str = \"[\" + \",\".join([str(res) for res in results]) + \"]\"\n    output_str = output_str.replace(\" \", \"\") # Remove spaces as per format\n    print(output_str)\n\ndef estimate_s_with_ci(t, n, y):\n    \"\"\"\n    Estimates the selection coefficient (s) and its confidence interval using IRLS\n    for a Binomial GLM with a logit link.\n\n    Args:\n        t (np.ndarray): Vector of time points.\n        n (np.ndarray): Vector of total hosts sampled at each time.\n        y (np.ndarray): Vector of observed carriers at each time.\n\n    Returns:\n        tuple: A tuple containing the estimated selection coefficient (s_hat),\n               the lower confidence bound, and the upper confidence bound.\n    \"\"\"\n    # Constants for the algorithm\n    Z_SCORE = 1.959963984540054\n    EPSILON = 1e-8  # For numerical stability\n    TOLERANCE = 1e-9 # Convergence criterion\n    MAX_ITER = 50\n\n    # 1. Construct the design matrix X\n    X = np.stack([np.ones_like(t), t], axis=1)\n\n    # 2. Initialize the parameter vector beta = [alpha, s]\n    # A simple initialization for beta is a zero vector.\n    # An improved start can be from a linear regression on transformed proportions,\n    # but a zero vector is robust enough for this problem.\n    beta = np.zeros(X.shape[1])\n\n    # 3. Implement the Iteratively Reweighted Least Squares (IRLS) algorithm\n    for i in range(MAX_ITER):\n        # Calculate linear predictor eta\n        eta = X @ beta\n\n        # Calculate fitted probabilities p_hat using the inverse logit function\n        p_hat = 1.0 / (1.0 + np.exp(-eta))\n\n        # Clip p_hat to avoid p=0 or p=1 for numerical stability\n        p_hat = np.clip(p_hat, EPSILON, 1.0 - EPSILON)\n\n        # Calculate the diagonal of the weight matrix W\n        # W_ii = n_i * p_hat_i * (1 - p_hat_i)\n        w_diag = n * p_hat * (1.0 - p_hat)\n\n        # Calculate the working response z\n        # z_i = eta_i + (y_i - n_i * p_hat_i) / (n_i * p_hat_i * (1 - p_hat_i))\n        z = eta + (y - n * p_hat) / w_diag\n        \n        # Store old beta for convergence check\n        beta_old = beta\n\n        # Update beta by solving the weighted least squares step\n        # beta_new = inv(X.T W X) * X.T W z\n        # To avoid creating the full diagonal matrix W, we can use broadcasting.\n        X_T_W = X.T * w_diag # Broadcasting w_diag onto rows of X.T\n        X_T_W_X = X_T_W @ X\n        X_T_W_z = X_T_W @ z\n        \n        # Solve the linear system for the new beta\n        beta = np.linalg.solve(X_T_W_X, X_T_W_z)\n\n        # Check for convergence\n        if np.linalg.norm(beta - beta_old)  TOLERANCE:\n            break\n            \n    # 4. Extract the MLE for s\n    s_hat = beta[1]\n\n    # 5. Calculate the Wald confidence interval\n    # Re-calculate final weights and probabilities based on the converged beta\n    eta = X @ beta\n    p_hat = np.clip(1.0 / (1.0 + np.exp(-eta)), EPSILON, 1.0 - EPSILON)\n    w_diag = n * p_hat * (1.0 - p_hat)\n    \n    # Observed Fisher Information Matrix: I = X.T * W * X\n    info_matrix = X.T @ (np.diag(w_diag) @ X)\n\n    # Covariance matrix is the inverse of the information matrix\n    try:\n        cov_matrix = np.linalg.inv(info_matrix)\n    except np.linalg.LinAlgError:\n        # Handle cases where the matrix is singular, returning NaNs\n        return (np.nan, np.nan, np.nan)\n\n    # Standard error of s is the square root of the (2,2) element of the covariance matrix\n    se_s = np.sqrt(cov_matrix[1, 1])\n\n    # Calculate confidence interval limits\n    margin_of_error = Z_SCORE * se_s\n    lower_bound = s_hat - margin_of_error\n    upper_bound = s_hat + margin_of_error\n\n    return s_hat, lower_bound, upper_bound\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2476519"}]}