{"hands_on_practices": [{"introduction": "A foundational challenge in co-management is ensuring a shared understanding of the resource being managed. When Traditional Ecological Knowledge (TEK) relies on local classification systems, it's vital to assess the consistency of these categories among knowledge holders before they can be reliably incorporated into policy. This exercise provides a quantitative method for moving beyond anecdote, using Cohen's kappa ($\\kappa$) to measure the level of agreement between informants, corrected for the agreement that would be expected by chance alone [@problem_id:2540754].", "problem": "A coastal co-management program aims to incorporate Traditional Ecological Knowledge (TEK) into species-specific harvest guidelines. Two ($2$) experienced fishers from the same community independently identified the species depicted in a set of $N$ voucher photographs ($N = 80$), each photo containing one individual. The emic taxonomy used by the community recognizes $K = 4$ categories, denoted $S_1, S_2, S_3, S_4$. The $4 \\times 4$ contingency of identifications between the two informants (rows for Informant A, columns for Informant B, with category order $S_1, S_2, S_3, S_4$) is\n$$\n\\mathbf{N} \\;=\\;\n\\begin{pmatrix}\n23  2  5  0 \\\\\n1  12  5  0 \\\\\n1  6  10  0 \\\\\n0  0  0  15\n\\end{pmatrix}.\n$$\nUsing only fundamental definitions from inter-rater agreement, proceed as follows:\n- Define the observed agreement as the proportion of cases where the two informants give the same category, and define the expected agreement under the assumption that the informants assign categories independently according to their marginal category frequencies.\n- From these definitions, derive the expression for the agreement beyond chance as the observed-minus-expected agreement normalized by the headroom above chance.\n- Compute the resulting agreement statistic (Cohen’s kappa) for the given data, expressing your final quantitative answer as an exact fraction with no units.\n- Briefly explain, based on the magnitude of your computed value and the pattern in $\\mathbf{N}$, what this implies about the reliability of using these informants’ identifications for management decisions. If you are familiar with Cultural Consensus Analysis (CCA), state in one sentence whether it is applicable here and why, but do not compute it.\n\nExpress the final quantitative answer as an exact fraction. No units. No rounding.", "solution": "The problem provided requires the calculation and interpretation of an inter-rater agreement statistic, specifically Cohen's kappa, based on a contingency table of species identifications by two informants. The validity of the problem statement must first be assessed.\n\nProblem Validation:\nStep 1: Extract Givens.\n- Number of informants: $2$.\n- Total number of items (photographs): $N = 80$.\n- Number of categories: $K = 4$, denoted $S_1, S_2, S_3, S_4$.\n- Contingency table of identifications, $\\mathbf{N}$:\n$$\n\\mathbf{N} \\;=\\;\n\\begin{pmatrix}\n23  2  5  0 \\\\\n1  12  5  0 \\\\\n1  6  10  0 \\\\\n0  0  0  15\n\\end{pmatrix}\n$$\n- Task: Define observed agreement ($P_o$), define expected agreement ($P_e$), derive the formula for Cohen's kappa ($\\kappa$), compute $\\kappa$, and provide an interpretation.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, well-posed, and objective. It uses standard statistical methods (inter-rater reliability analysis) applied to a realistic scenario in ethnoecology. The data provided is complete and internally consistent. The sum of the entries in the matrix $\\mathbf{N}$ is $23 + 2 + 5 + 0 + 1 + 12 + 5 + 0 + 1 + 6 + 10 + 0 + 0 + 0 + 0 + 15 = 80$, which matches the given total number of items $N$. There are no scientific or logical flaws, ambiguities, or missing information. The problem is therefore valid.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be provided.\n\nSolution:\nLet $n_{ij}$ be the number of items assigned to category $S_i$ by Informant A and to category $S_j$ by Informant B. The total number of items is $N = \\sum_{i=1}^{K} \\sum_{j=1}^{K} n_{ij}$. Here, $K=4$ and $N=80$.\n\nFirst, we define the observed proportional agreement, $P_o$. This is the proportion of items for which both informants agree on the classification. Agreement occurs on the main diagonal of the contingency table $\\mathbf{N}$.\n$$\nP_o = \\frac{\\sum_{k=1}^{K} n_{kk}}{N}\n$$\nUsing the given data:\n$$\n\\sum_{k=1}^{4} n_{kk} = n_{11} + n_{22} + n_{33} + n_{44} = 23 + 12 + 10 + 15 = 60\n$$\nTherefore, the observed agreement is:\n$$\nP_o = \\frac{60}{80} = \\frac{3}{4}\n$$\nNext, we define the expected agreement, $P_e$. This is the proportional agreement that would be expected by chance, assuming the informants' ratings are independent. For each category $S_k$, we calculate the marginal frequencies of assignment for each informant.\nLet $n_{i\\cdot} = \\sum_{j=1}^{K} n_{ij}$ be the row total for category $S_i$ (total assignments by Informant A to $S_i$).\nLet $n_{\\cdot j} = \\sum_{i=1}^{K} n_{ij}$ be the column total for category $S_j$ (total assignments by Informant B to $S_j$).\n\nThe row sums are:\n$n_{1\\cdot} = 23 + 2 + 5 + 0 = 30$\n$n_{2\\cdot} = 1 + 12 + 5 + 0 = 18$\n$n_{3\\cdot} = 1 + 6 + 10 + 0 = 17$\n$n_{4\\cdot} = 0 + 0 + 0 + 15 = 15$\n\nThe column sums are:\n$n_{\\cdot 1} = 23 + 1 + 1 + 0 = 25$\n$n_{\\cdot 2} = 2 + 12 + 6 + 0 = 20$\n$n_{\\cdot 3} = 5 + 5 + 10 + 0 = 20$\n$n_{\\cdot 4} = 0 + 0 + 0 + 15 = 15$\n\nThe probability that Informant A classifies an item as $S_k$ is $p_{A,k} = \\frac{n_{k\\cdot}}{N}$. The probability that Informant B classifies an item as $S_k$ is $p_{B,k} = \\frac{n_{\\cdot k}}{N}$. Under independence, the probability that both classify an item as $S_k$ is $p_{A,k} \\times p_{B,k}$. The total expected agreement is the sum of these probabilities over all categories:\n$$\nP_e = \\sum_{k=1}^{K} p_{A,k} \\cdot p_{B,k} = \\sum_{k=1}^{K} \\frac{n_{k\\cdot}}{N} \\frac{n_{\\cdot k}}{N} = \\frac{1}{N^2} \\sum_{k=1}^{K} n_{k\\cdot} n_{\\cdot k}\n$$\nUsing the calculated marginals:\n$$\n\\sum_{k=1}^{4} n_{k\\cdot} n_{\\cdot k} = (30 \\times 25) + (18 \\times 20) + (17 \\times 20) + (15 \\times 15)\n$$\n$$\n\\sum_{k=1}^{4} n_{k\\cdot} n_{\\cdot k} = 750 + 360 + 340 + 225 = 1675\n$$\nNow, we compute $P_e$:\n$$\nP_e = \\frac{1675}{N^2} = \\frac{1675}{80^2} = \\frac{1675}{6400}\n$$\nTo simplify this fraction, we note both numerator and denominator are divisible by $25$.\n$1675 \\div 25 = 67$.\n$6400 \\div 25 = 256$.\nSo, $P_e = \\frac{67}{256}$.\n\nCohen's kappa, $\\kappa$, is defined as the agreement beyond chance, normalized by the maximum possible agreement beyond chance. The observed agreement is $P_o$. The agreement expected by chance is $P_e$. Thus, the agreement achieved beyond chance is $P_o - P_e$. The maximum possible agreement is $1$, so the maximum possible agreement beyond chance is $1 - P_e$. The statistic is therefore the ratio:\n$$\n\\kappa = \\frac{P_o - P_e}{1 - P_e}\n$$\nSubstituting our calculated values for $P_o$ and $P_e$:\n$$\n\\kappa = \\frac{\\frac{3}{4} - \\frac{67}{256}}{1 - \\frac{67}{256}}\n$$\nWe find a common denominator for the terms in the numerator and denominator.\nNumerator:\n$$\n\\frac{3}{4} - \\frac{67}{256} = \\frac{3 \\times 64}{256} - \\frac{67}{256} = \\frac{192 - 67}{256} = \\frac{125}{256}\n$$\nDenominator:\n$$\n1 - \\frac{67}{256} = \\frac{256}{256} - \\frac{67}{256} = \\frac{189}{256}\n$$\nNow, we compute the ratio:\n$$\n\\kappa = \\frac{\\frac{125}{256}}{\\frac{189}{256}} = \\frac{125}{189}\n$$\nThe prime factorization of the numerator is $125 = 5^3$. The prime factorization of the denominator is $189 = 3^3 \\times 7$. As there are no common factors, the fraction is irreducible.\n\nInterpretation:\nThe computed value of Cohen's kappa is $\\kappa = \\frac{125}{189} \\approx 0.661$. This value typically indicates substantial agreement between the two informants. The agreement is considerably better than what would be expected by random chance.\nAn examination of the matrix $\\mathbf{N}$ reveals the pattern of agreement and disagreement. There is perfect agreement for all items identified as category $S_4$ (i.e., $15$ agreements, $0$ disagreements), suggesting this category is distinct and reliably identified. The disagreements are concentrated among categories $S_1$, $S_2$, and $S_3$. For instance, when Informant A identifies a specimen as $S_3$, Informant B agrees only $10$ times but misidentifies it as $S_2$ for $6$ specimens and as $S_1$ for $1$ specimen. This indicates that while overall reliability is substantial, there is a specific lack of distinguishability within the subset $\\{S_1, S_2, S_3\\}$. For management purposes, using these informants' TEK would be very reliable for decisions concerning species $S_4$, but potentially unreliable for decisions requiring discrimination between $S_1$, $S_2$, and $S_3$.\n\nRegarding the applicability of Cultural Consensus Analysis (CCA): Cultural Consensus Analysis is not applicable here because its fundamental model requires a minimum of three informants to statistically estimate a consensus answer key and individual informant competencies.", "answer": "$$\\boxed{\\frac{125}{189}}$$", "id": "2540754"}, {"introduction": "Beyond static classification, TEK is rich with dynamic indicators used to predict environmental conditions and guide resource-use decisions. To integrate such knowledge into formal management, we need a framework that can translate a TEK-based observation into an optimal action. This practice applies Bayesian decision theory to determine an ideal decision threshold, demonstrating how to systematically balance the evidence provided by a TEK indicator against the specific costs of making a wrong decision [@problem_id:2540709].", "problem": "A rain-fed agroforestry community uses Traditional Ecological Knowledge (TEK) to anticipate next-day rainfall and allocate labor to rainwater harvesting. A commonly used TEK indicator is the nocturnal call rate of a sentinel bird species, modeled as a continuous random variable $X$ (calls per hour) measured during the evening survey. Historical co-observations yield the following seasonal characterization:\n- When next-day rainfall occurs (event $R$), $X$ is approximately normal with mean $\\mu_{R}$ and variance $\\sigma^{2}$.\n- When next-day rainfall does not occur (event $\\bar{R}$), $X$ is approximately normal with mean $\\mu_{\\bar{R}}$ and the same variance $\\sigma^{2}$.\n- Seasonal base rate (prior) of rain is $\\mathbb{P}(R)=p$ and $\\mathbb{P}(\\bar{R})=1-p$.\n\nThe community must decide each evening whether to mobilize labor for rainwater harvesting. Consider a threshold policy: predict rain and mobilize if $X \\ge \\tau$, otherwise do not mobilize. The consequences are represented by a cost structure:\n- False alarm (mobilize when $\\bar{R}$): cost $C_{F}$.\n- Miss (do not mobilize when $R$): cost $C_{M}$.\n- Correct decisions incur zero additional cost (baseline).\n\nAssume that the distributions and parameters are:\n- $\\mu_{R} = 30$ calls per hour, $\\mu_{\\bar{R}} = 15$ calls per hour.\n- $\\sigma = 6$ calls per hour.\n- $p = 0.3$.\n- $C_{F} = 1$ (arbitrary but consistent cost units), $C_{M} = 4$.\n\nUsing only the definitions of conditional probability, Bayes’ theorem, expected loss minimization, and the properties of the normal distribution, derive from first principles the unique threshold $\\tau^{\\star}$ that minimizes the expected misclassification cost under the threshold rule $X \\ge \\tau \\Rightarrow$ mobilize. Then evaluate $\\tau^{\\star}$ numerically for the given parameters. Express the final threshold in calls per hour and round your answer to four significant figures.", "solution": "The problem is a classic application of Bayesian decision theory to determine an optimal classification threshold. The goal is to find the threshold $\\tau^{\\star}$ on the observed variable $X$ that minimizes the total expected cost of misclassification.\n\nThe problem statement has been validated and is scientifically grounded, well-posed, and objective. It contains all necessary information and is free of contradictions or logical flaws. We may proceed to the solution.\n\nLet $X$ be the random variable representing the bird call rate. Let $R$ denote the event that rain occurs the next day, and $\\bar{R}$ be the event that no rain occurs. The decision rule is to predict rain if $X \\ge \\tau$ and predict no rain if $X  \\tau$.\n\nThere are two types of errors:\n$1$. A false alarm occurs when we predict rain ($X \\ge \\tau$) but no rain occurs (event $\\bar{R}$). The cost of this error is $C_{F}$.\n$2$. A miss occurs when we predict no rain ($X  \\tau$) but rain occurs (event $R$). The cost of this error is $C_{M}$.\n\nThe total expected cost, which we denote as $E[C(\\tau)]$, is the sum of the costs of these two errors, each weighted by its probability of occurrence.\n$$E[C(\\tau)] = C_{F} \\cdot \\mathbb{P}(X \\ge \\tau \\text{ and } \\bar{R}) + C_{M} \\cdot \\mathbb{P}(X  \\tau \\text{ and } R)$$\nUsing the definition of conditional probability, $\\mathbb{P}(A \\text{ and } B) = \\mathbb{P}(A|B)\\mathbb{P}(B)$, we can rewrite the expected cost as:\n$$E[C(\\tau)] = C_{F} \\cdot \\mathbb{P}(X \\ge \\tau | \\bar{R})\\mathbb{P}(\\bar{R}) + C_{M} \\cdot \\mathbb{P}(X  \\tau | R)\\mathbb{P}(R)$$\nThe problem gives the prior probabilities $\\mathbb{P}(R) = p$ and $\\mathbb{P}(\\bar{R}) = 1-p$. The conditional probabilities can be expressed as integrals of the corresponding probability density functions (PDFs). Let $f_{X|R}(x)$ be the PDF of $X$ given $R$, and $f_{X|\\bar{R}}(x)$ be the PDF of $X$ given $\\bar{R}$.\n$$E[C(\\tau)] = C_{F}(1-p) \\int_{\\tau}^{\\infty} f_{X|\\bar{R}}(x) \\, dx + C_{M}p \\int_{-\\infty}^{\\tau} f_{X|R}(x) \\, dx$$\nTo find the threshold $\\tau^{\\star}$ that minimizes $E[C(\\tau)]$, we must differentiate $E[C(\\tau)]$ with respect to $\\tau$ and set the derivative to zero. Using the Fundamental Theorem of Calculus, specifically Leibniz's integral rule, we have:\n$$\\frac{d}{d\\tau} \\int_{\\tau}^{\\infty} f(x) \\, dx = -f(\\tau)$$\n$$\\frac{d}{d\\tau} \\int_{-\\infty}^{\\tau} f(x) \\, dx = f(\\tau)$$\nApplying this to our expected cost function:\n$$\\frac{dE[C(\\tau)]}{d\\tau} = C_{F}(1-p) [-f_{X|\\bar{R}}(\\tau)] + C_{M}p [f_{X|R}(\\tau)]$$\nSetting the derivative to zero to find the optimal threshold $\\tau^{\\star}$:\n$$C_{M}p f_{X|R}(\\tau^{\\star}) - C_{F}(1-p) f_{X|\\bar{R}}(\\tau^{\\star}) = 0$$\n$$C_{M}p f_{X|R}(\\tau^{\\star}) = C_{F}(1-p) f_{X|\\bar{R}}(\\tau^{\\star})$$\nThis can be rearranged into a condition on the likelihood ratio:\n$$\\frac{f_{X|R}(\\tau^{\\star})}{f_{X|\\bar{R}}(\\tau^{\\star})} = \\frac{C_{F}(1-p)}{C_{M}p}$$\nThe problem states that the conditional distributions are normal with the same variance $\\sigma^2$:\n$$X|R \\sim \\mathcal{N}(\\mu_{R}, \\sigma^2) \\quad \\implies \\quad f_{X|R}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{R})^2}{2\\sigma^2}\\right)$$\n$$X|\\bar{R} \\sim \\mathcal{N}(\\mu_{\\bar{R}}, \\sigma^2) \\quad \\implies \\quad f_{X|\\bar{R}}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{\\bar{R}})^2}{2\\sigma^2}\\right)$$\nSubstituting these into the likelihood ratio equation, the pre-exponential factors cancel:\n$$\\frac{\\exp\\left(-\\frac{(\\tau^{\\star} - \\mu_{R})^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(\\tau^{\\star} - \\mu_{\\bar{R}})^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{(\\tau^{\\star} - \\mu_{\\bar{R}})^2 - (\\tau^{\\star} - \\mu_{R})^2}{2\\sigma^2}\\right) = \\frac{C_{F}(1-p)}{C_{M}p}$$\nWe take the natural logarithm of both sides to solve for $\\tau^{\\star}$:\n$$\\frac{(\\tau^{\\star} - \\mu_{\\bar{R}})^2 - (\\tau^{\\star} - \\mu_{R})^2}{2\\sigma^2} = \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\nThe numerator is a difference of squares, which we can expand:\n$$(\\tau^{\\star 2} - 2\\tau^{\\star}\\mu_{\\bar{R}} + \\mu_{\\bar{R}}^2) - (\\tau^{\\star 2} - 2\\tau^{\\star}\\mu_{R} + \\mu_{R}^2) = 2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) - (\\mu_{R}^2 - \\mu_{\\bar{R}}^2)$$\nFactoring $(\\mu_{R}^2 - \\mu_{\\bar{R}}^2)$ as $(\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}})$ gives:\n$$2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) - (\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}})$$\nSubstituting this back into the equation:\n$$\\frac{2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) - (\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}})}{2\\sigma^2} = \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\nAssuming $\\mu_{R} \\neq \\mu_{\\bar{R}}$, we can isolate $\\tau^{\\star}$:\n$$2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) = (\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}}) + 2\\sigma^2 \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\n$$\\tau^{\\star} = \\frac{\\mu_{R} + \\mu_{\\bar{R}}}{2} + \\frac{\\sigma^2}{\\mu_{R} - \\mu_{\\bar{R}}} \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\nThis is the general analytical expression for the optimal threshold.\n\nNow, we substitute the given numerical values:\n$\\mu_{R} = 30$\n$\\mu_{\\bar{R}} = 15$\n$\\sigma = 6 \\implies \\sigma^2 = 36$\n$p = 0.3 \\implies 1-p = 0.7$\n$C_{F} = 1$\n$C_{M} = 4$\n\nThe expression becomes:\n$$\\tau^{\\star} = \\frac{30 + 15}{2} + \\frac{36}{30 - 15} \\ln\\left(\\frac{1 \\cdot (1-0.3)}{4 \\cdot 0.3}\\right)$$\n$$\\tau^{\\star} = \\frac{45}{2} + \\frac{36}{15} \\ln\\left(\\frac{0.7}{1.2}\\right)$$\n$$\\tau^{\\star} = 22.5 + 2.4 \\ln\\left(\\frac{7}{12}\\right)$$\nWe now compute the numerical value:\n$$\\ln\\left(\\frac{7}{12}\\right) \\approx -0.5389965$$\n$$\\tau^{\\star} \\approx 22.5 + 2.4 \\cdot (-0.5389965)$$\n$$\\tau^{\\star} \\approx 22.5 - 1.2935916$$\n$$\\tau^{\\star} \\approx 21.2064084$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\tau^{\\star} \\approx 21.21$$\nThis value is physically sensible, as it lies between the two means $\\mu_{\\bar{R}} = 15$ and $\\mu_{R} = 30$. Because the expected cost of a miss, $C_M p = 4 \\cdot 0.3 = 1.2$, is greater than the expected cost of a false alarm, $C_F(1-p) = 1 \\cdot 0.7 = 0.7$, the threshold is shifted lower than the simple midpoint of $22.5$ to reduce the probability of the more costly \"miss\" error.", "answer": "$$\\boxed{21.21}$$", "id": "2540709"}, {"introduction": "TEK is not a monolith; it varies across communities, and expertise varies within them. A central task in regional-scale management is to synthesize these diverse knowledge sources in a statistically sound manner. This problem introduces hierarchical Bayesian modeling as a powerful framework to achieve this synthesis, allowing us to \"borrow strength\" across communities to improve estimates while simultaneously respecting their unique knowledge and accounting for varying levels of expertise. This hands-on coding exercise will guide you through building and implementing such a model from first principles [@problem_id:2540689].", "problem": "A conservation team seeks to synthesize Traditional Ecological Knowledge (TEK) across multiple communities to estimate a latent community-level belief about a single resource indicator. Each observation is a numeric TEK judgment and is accompanied by an expertise score that modifies the observation’s reliability. Consider a hierarchical probabilistic model with the following components.\n\n1. For each community indexed by $c \\in \\{1, \\dots, C\\}$ and for each observation $i \\in \\{1, \\dots, n_c\\}$, the TEK judgment $y_{c,i}$ is modeled as a conditionally independent draw from a Gaussian distribution with a community-specific mean and an expertise-weighted variance. The observation model is:\n$$\ny_{c,i} \\mid \\theta_c \\sim \\mathcal{N}\\!\\left(\\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right),\n$$\nwhere $e_{c,i}  0$ is the expertise weight for the $i$th informant in the $c$th community, and $\\sigma^2$ is known.\n\n2. The community-level latent means $\\theta_c$ are pooled through a population distribution with mean $\\mu$ and variance $\\tau^2$:\n$$\n\\theta_c \\mid \\mu, \\tau^2 \\sim \\mathcal{N}\\!\\left(\\mu, \\tau^2\\right).\n$$\n\n3. To prevent over-shrinkage of $\\theta_c$ toward $\\mu$ when communities are genuinely heterogeneous, place a weakly informative, heavy-tailed prior on the heterogeneity variance $\\tau^2$. Use the inverse-gamma distribution with parameters $(\\alpha_0, \\beta_0)$:\n$$\n\\tau^2 \\sim \\mathrm{InverseGamma}\\!\\left(\\alpha_0, \\beta_0\\right),\n$$\nwhere smaller values of $\\alpha_0$ and $\\beta_0$ yield heavier tails and consequently less over-shrinkage pressure.\n\n4. Place a diffuse Gaussian prior on the population mean:\n$$\n\\mu \\sim \\mathcal{N}\\!\\left(m_0, s_0^2\\right),\n$$\nwith known hyperparameters $m_0$ and $s_0^2$.\n\nFoundational base you may use: Bayes’ theorem, properties of the Gaussian distribution including conjugacy with known-variance Gaussian likelihoods, the definition and properties of the inverse-gamma distribution as a prior on a variance parameter, and the law of total probability. You must not introduce untested or ad hoc formulas beyond these bases.\n\nTask. Derive from first principles the full conditional distributions necessary for a correct Gibbs sampler for this hierarchical model with known $\\sigma^2$. Then, implement a numerically stable Gibbs sampler that:\n- initializes sensibly from the data,\n- uses $N = 20000$ total iterations with a burn-in of $B = 5000$,\n- uses a fixed pseudo-random generator seed of $s = 42$ for reproducibility,\n- returns, for each test case, the posterior means $\\mathbb{E}[\\mu \\mid \\mathrm{data}]$ and $\\mathbb{E}[\\tau \\mid \\mathrm{data}]$, where $\\tau = \\sqrt{\\tau^2}$.\n\nHyperparameters to use in all test cases: $m_0 = 0$, $s_0^2 = 10$, $\\alpha_0 = 0.5$, $\\beta_0 = 0.5$, and $\\sigma^2 = 1$.\n\nTest suite. For each test case below, the input data are given as ordered lists of communities, each community providing a pair of lists $(\\mathbf{y}_c, \\mathbf{e}_c)$ with the same length $n_c$. All numbers are pure numbers without physical units.\n\n- Test case $1$:\n  - Community $1$: $\\mathbf{y}_1 = \\left[\\, 1.20, 0.90, 1.10, 1.00 \\,\\right]$, $\\mathbf{e}_1 = \\left[\\, 1.00, 0.80, 1.20, 1.10 \\,\\right]$.\n  - Community $2$: $\\mathbf{y}_2 = \\left[\\, 0.20, 0.00, -0.10 \\,\\right]$, $\\mathbf{e}_2 = \\left[\\, 0.60, 0.50, 0.70 \\,\\right]$.\n  - Community $3$: $\\mathbf{y}_3 = \\left[\\, 1.80, 1.60, 1.90, 2.10, 1.70 \\,\\right]$, $\\mathbf{e}_3 = \\left[\\, 1.40, 1.30, 1.20, 1.50, 1.30 \\,\\right]$.\n\n- Test case $2$:\n  - Community $1$: $\\mathbf{y}_1 = \\left[\\, 0.55, 0.52, 0.58, 0.49, 0.53, 0.51, 0.56, 0.54 \\,\\right]$, $\\mathbf{e}_1 = \\left[\\, 2.50, 2.20, 2.80, 2.00, 2.40, 2.30, 2.60, 2.50 \\,\\right]$.\n  - Community $2$: $\\mathbf{y}_2 = \\left[\\, 0.90 \\,\\right]$, $\\mathbf{e}_2 = \\left[\\, 0.40 \\,\\right]$.\n  - Community $3$: $\\mathbf{y}_3 = \\left[\\, 0.10, 0.20 \\,\\right]$, $\\mathbf{e}_3 = \\left[\\, 0.70, 0.80 \\,\\right]$.\n  - Community $4$: $\\mathbf{y}_4 = \\left[\\, 0.50, 0.60, 0.40 \\,\\right]$, $\\mathbf{e}_4 = \\left[\\, 1.00, 0.90, 1.10 \\,\\right]$.\n\n- Test case $3$:\n  - Community $1$: $\\mathbf{y}_1 = \\left[\\, -1.50, -1.60, -1.40 \\,\\right]$, $\\mathbf{e}_1 = \\left[\\, 1.20, 1.10, 1.30 \\,\\right]$.\n  - Community $2$: $\\mathbf{y}_2 = \\left[\\, 1.50, 1.60, 1.40 \\,\\right]$, $\\mathbf{e}_2 = \\left[\\, 1.20, 1.10, 1.30 \\,\\right]$.\n\nRequired program behavior. Your program must construct the hierarchical model exactly as specified, derive and implement a correct Gibbs sampler based solely on the foundational base, and run it on the test suite with the hyperparameters and sampling settings given above. Your program must not read any input. For each test case, compute two floats: the posterior means $\\mathbb{E}[\\mu \\mid \\mathrm{data}]$ and $\\mathbb{E}[\\tau \\mid \\mathrm{data}]$. Aggregate the results across all test cases in order into a single list with entries ordered as $\\left[\\mathbb{E}[\\mu]_1, \\mathbb{E}[\\tau]_1, \\mathbb{E}[\\mu]_2, \\mathbb{E}[\\tau]_2, \\mathbb{E}[\\mu]_3, \\mathbb{E}[\\tau]_3\\right]$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\left[\\, r_1, r_2, r_3, r_4, r_5, r_6 \\,\\right]$). No additional text should be printed.", "solution": "The problem requires the derivation and implementation of a Gibbs sampler for a three-level hierarchical Bayesian model. The model is specified as follows:\n1.  Observation Model: $y_{c,i} \\mid \\theta_c \\sim \\mathcal{N}\\!\\left(\\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right)$\n2.  Community-Level Model: $\\theta_c \\mid \\mu, \\tau^2 \\sim \\mathcal{N}\\!\\left(\\mu, \\tau^2\\right)$\n3.  Priors:\n    -   $\\mu \\sim \\mathcal{N}\\!\\left(m_0, s_0^2\\right)$\n    -   $\\tau^2 \\sim \\mathrm{InverseGamma}\\!\\left(\\alpha_0, \\beta_0\\right)$\n\nThe variance parameter $\\sigma^2$ is known. The goal is to perform inference on the parameters $\\mu$, $\\tau^2$, and the set of community-level means $\\boldsymbol{\\theta} = \\{\\theta_c\\}_{c=1}^C$. A Gibbs sampler is an iterative Markov chain Monte Carlo (MCMC) algorithm that samples from the full conditional distribution of each parameter (or block of parameters) in turn. The full conditional distribution for a parameter is its distribution conditioned on all other parameters and the data.\n\nFirst, we write the joint posterior distribution of all unknown parameters, which is proportional to the product of the likelihood and the priors, by Bayes' theorem:\n$$\np(\\boldsymbol{\\theta}, \\mu, \\tau^2 \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\sigma^2) p(\\boldsymbol{\\theta} \\mid \\mu, \\tau^2) p(\\mu \\mid m_0, s_0^2) p(\\tau^2 \\mid \\alpha_0, \\beta_0)\n$$\nwhere $\\mathbf{y}$ represents all observations $\\{y_{c,i}\\}$. Explicitly, this is:\n$$\np(\\boldsymbol{\\theta}, \\mu, \\tau^2 \\mid \\mathbf{y}) \\propto \\left( \\prod_{c=1}^C \\prod_{i=1}^{n_c} \\mathcal{N}\\!\\left(y_{c,i} \\mid \\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right) \\right) \\left( \\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu \\mid m_0, s_0^2) \\mathrm{InverseGamma}(\\tau^2 \\mid \\alpha_0, \\beta_0)\n$$\nTo derive the full conditionals, we isolate the terms in the joint posterior that involve the parameter of interest and treat all other quantities as constants. The resulting expression is the kernel of the conditional distribution.\n\n**1. Full Conditional Distribution for $\\theta_c$**\n\nThe full conditional for a specific community mean, $\\theta_c$, depends on the observations from that community, $\\mathbf{y}_c$, and the higher-level parameters $\\mu$ and $\\tau^2$. The relevant terms are:\n$$\np(\\theta_c \\mid \\text{rest}) \\propto p(\\mathbf{y}_c \\mid \\theta_c) p(\\theta_c \\mid \\mu, \\tau^2) = \\left( \\prod_{i=1}^{n_c} \\mathcal{N}\\!\\left(y_{c,i} \\mid \\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right) \\right) \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2)\n$$\nThis is a product of Gaussian densities. The posterior for $\\theta_c$ will also be Gaussian due to conjugacy. To find its parameters, we examine the exponent, which is a quadratic function of $\\theta_c$:\n$$\n\\log p(\\theta_c \\mid \\text{rest}) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n_c} e_{c,i}(y_{c,i} - \\theta_c)^2 - \\frac{1}{2\\tau^2}(\\theta_c - \\mu)^2\n$$\nExpanding the squares and collecting terms in $\\theta_c^2$ and $\\theta_c$:\n$$\n\\propto -\\frac{1}{2} \\left[ \\theta_c^2 \\left(\\frac{\\sum_i e_{c,i}}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2\\theta_c \\left(\\frac{\\sum_i e_{c,i} y_{c,i}}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) \\right]\n$$\nThis is the kernel of a Gaussian log-density. By completing the square, we identify the posterior precision $V_c^{-1}$ and mean $M_c$. The posterior precision is the coefficient of $\\theta_c^2/2$, and the posterior mean is the coefficient of $\\theta_c$ divided by the precision.\n$$\nV_c^{-1} = \\frac{\\sum_{i=1}^{n_c} e_{c,i}}{\\sigma^2} + \\frac{1}{\\tau^2} \\implies V_c = \\left( \\frac{\\sum_{i=1}^{n_c} e_{c,i}}{\\sigma^2} + \\frac{1}{\\tau^2} \\right)^{-1}\n$$\n$$\nM_c = V_c \\left( \\frac{\\sum_{i=1}^{n_c} e_{c,i} y_{c,i}}{\\sigma^2} + \\frac{\\mu}{\\tau^2} \\right)\n$$\nThus, the full conditional for each $\\theta_c$ is:\n$$\n\\theta_c \\mid \\mathbf{y}, \\mu, \\tau^2 \\sim \\mathcal{N}(M_c, V_c)\n$$\n\n**2. Full Conditional Distribution for $\\mu$**\n\nThe full conditional for the population mean $\\mu$ depends on the community means $\\boldsymbol{\\theta}$, the between-community variance $\\tau^2$, and its prior hyperparameters. The data $\\mathbf{y}$ are conditionally independent of $\\mu$ given $\\boldsymbol{\\theta}$.\n$$\np(\\mu \\mid \\text{rest}) \\propto p(\\boldsymbol{\\theta} \\mid \\mu, \\tau^2) p(\\mu) = \\left( \\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu \\mid m_0, s_0^2)\n$$\nThis is again a standard Gaussian-Gaussian update, where the $\\theta_c$ values act as data points. The log-posterior is:\n$$\n\\log p(\\mu \\mid \\text{rest}) \\propto -\\frac{1}{2\\tau^2} \\sum_{c=1}^C (\\theta_c - \\mu)^2 - \\frac{1}{2s_0^2}(\\mu - m_0)^2\n$$\nCompleting the square for $\\mu$:\n$$\n\\propto -\\frac{1}{2} \\left[ \\mu^2 \\left(\\frac{C}{\\tau^2} + \\frac{1}{s_0^2}\\right) - 2\\mu \\left(\\frac{\\sum_c \\theta_c}{\\tau^2} + \\frac{m_0}{s_0^2}\\right) \\right]\n$$\nFrom this quadratic form, we identify the posterior precision $V_\\mu^{-1}$ and mean $M_\\mu$:\n$$\nV_\\mu^{-1} = \\frac{C}{\\tau^2} + \\frac{1}{s_0^2} \\implies V_\\mu = \\left( \\frac{C}{\\tau^2} + \\frac{1}{s_0^2} \\right)^{-1}\n$$\n$$\nM_\\mu = V_\\mu \\left( \\frac{\\sum_{c=1}^C \\theta_c}{\\tau^2} + \\frac{m_0}{s_0^2} \\right)\n$$\nThe full conditional for $\\mu$ is:\n$$\n\\mu \\mid \\boldsymbol{\\theta}, \\tau^2, \\mathbf{y} \\sim \\mathcal{N}(M_\\mu, V_\\mu)\n$$\n\n**3. Full Conditional Distribution for $\\tau^2$**\n\nThe full conditional for the between-community variance $\\tau^2$ depends on $\\boldsymbol{\\theta}$, $\\mu$, and its prior hyperparameters.\n$$\np(\\tau^2 \\mid \\text{rest}) \\propto p(\\boldsymbol{\\theta} \\mid \\mu, \\tau^2) p(\\tau^2) = \\left( \\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\right) \\mathrm{InverseGamma}(\\tau^2 \\mid \\alpha_0, \\beta_0)\n$$\nThe product of Gaussian densities contributes terms involving $\\tau^2$:\n$$\n\\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\propto (\\tau^2)^{-C/2} \\exp\\left(-\\frac{1}{2\\tau^2} \\sum_{c=1}^C (\\theta_c - \\mu)^2\\right)\n$$\nThe prior density for $\\tau^2$ is proportional to $(\\tau^2)^{-(\\alpha_0+1)} \\exp(-\\beta_0/\\tau^2)$. Combining these gives the kernel of the full conditional for $\\tau^2$:\n$$\np(\\tau^2 \\mid \\text{rest}) \\propto (\\tau^2)^{-C/2} \\exp\\left(-\\frac{\\sum_c (\\theta_c - \\mu)^2}{2\\tau^2}\\right) \\cdot (\\tau^2)^{-(\\alpha_0+1)} \\exp(-\\beta_0/\\tau^2)\n$$\n$$\np(\\tau^2 \\mid \\text{rest}) \\propto (\\tau^2)^{-(\\alpha_0 + C/2 + 1)} \\exp\\left(-\\frac{1}{\\tau^2} \\left[ \\beta_0 + \\frac{1}{2} \\sum_{c=1}^C (\\theta_c - \\mu)^2 \\right]\\right)\n$$\nThis is the kernel of an Inverse-Gamma distribution, $\\mathrm{InverseGamma}(\\alpha^*, \\beta^*)$. By comparison, we find the posterior shape and scale parameters:\n$$\n\\alpha^* = \\alpha_0 + \\frac{C}{2}\n$$\n$$\n\\beta^* = \\beta_0 + \\frac{1}{2} \\sum_{c=1}^C (\\theta_c - \\mu)^2\n$$\nThe full conditional for $\\tau^2$ is:\n$$\n\\tau^2 \\mid \\boldsymbol{\\theta}, \\mu, \\mathbf{y} \\sim \\mathrm{InverseGamma}(\\alpha^*, \\beta^*)\n$$\n\n**Gibbs Sampler Algorithm**\n\nThe Gibbs sampler proceeds by initializing the parameters and then iteratively sampling from their full conditional distributions.\n\n1.  **Initialization**: Set initial values for $\\mu^{(0)}$, $(\\tau^2)^{(0)}$, and $\\boldsymbol{\\theta}^{(0)}$. Sensible choices are to initialize $\\theta_c^{(0)}$ to the expertise-weighted mean of observations in community $c$, $\\mu^{(0)}$ to the average of the $\\theta_c^{(0)}$, and $(\\tau^2)^{(0)}$ to a constant like $1.0$.\n\n2.  **Iteration**: For $t = 1, \\dots, N$:\n    a. For each community $c = 1, \\dots, C$, sample $\\theta_c^{(t)} \\sim \\mathcal{N}(M_c, V_c)$, using $\\mu^{(t-1)}$ and $(\\tau^2)^{(t-1)}$ to compute $M_c, V_c$.\n    b. Sample $\\mu^{(t)} \\sim \\mathcal{N}(M_\\mu, V_\\mu)$, using $\\boldsymbol{\\theta}^{(t)}$ and $(\\tau^2)^{(t-1)}$ to compute $M_\\mu, V_\\mu$.\n    c. Sample $(\\tau^2)^{(t)} \\sim \\mathrm{InverseGamma}(\\alpha^*, \\beta^*)$, using $\\mu^{(t)}$ and $\\boldsymbol{\\theta}^{(t)}$ to compute $\\alpha^*, \\beta^*$.\n\n3.  **Posterior Inference**: After a burn-in period of $B$ iterations, the remaining $N-B$ samples are used to approximate the posterior distributions. The posterior mean of any parameter is estimated by the arithmetic mean of its samples from the post-burn-in chain. For $\\tau = \\sqrt{\\tau^2}$, we compute $\\tau^{(t)} = \\sqrt{(\\tau^2)^{(t)}}$ at each step and average these values.", "answer": "```python\nimport numpy as np\n\ndef run_gibbs_sampler(communities_data, sigma_sq, m0, s0_sq, alpha0, beta0, n_iter, n_burn, seed):\n    \"\"\"\n    Implements a Gibbs sampler for the specified hierarchical model.\n\n    Args:\n        communities_data (list): List of tuples, each containing (y_c, e_c) numpy arrays.\n        sigma_sq (float): known observation variance component.\n        m0 (float): Prior mean for mu.\n        s0_sq (float): Prior variance for mu.\n        alpha0 (float): Prior shape parameter for tau_sq.\n        beta0 (float): Prior scale parameter for tau_sq.\n        n_iter (int): Total number of Gibbs iterations.\n        n_burn (int): Number of burn-in iterations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing the posterior means of mu and tau.\n    \"\"\"\n    C = len(communities_data)\n    rng = np.random.default_rng(seed)\n\n    # Pre-compute sums for efficiency\n    sum_e_y = np.array([np.sum(d[0] * d[1]) for d in communities_data])\n    sum_e = np.array([np.sum(d[1]) for d in communities_data])\n    \n    # Sensible initializations\n    thetas = sum_e_y / sum_e\n    mu = np.mean(thetas)\n    tau_sq = 1.0\n\n    # Storage for posterior samples (post-burn-in)\n    n_samples = n_iter - n_burn\n    mu_samples = np.zeros(n_samples)\n    tau_samples = np.zeros(n_samples)\n\n    # Gibbs sampling loop\n    for i in range(n_iter):\n        # 1. Sample thetas\n        for c in range(C):\n            # Posterior for theta_c is Gaussian\n            # Precision from likelihood: sum_e[c] / sigma_sq\n            # Precision from prior: 1 / tau_sq\n            post_var_c = 1.0 / (sum_e[c] / sigma_sq + 1.0 / tau_sq)\n            post_mean_c = post_var_c * (sum_e_y[c] / sigma_sq + mu / tau_sq)\n            \n            thetas[c] = rng.normal(post_mean_c, np.sqrt(post_var_c))\n\n        # 2. Sample mu\n        # Posterior for mu is Gaussian\n        # Precision from likelihood on thetas: C / tau_sq\n        # Precision from prior: 1 / s0_sq\n        post_var_mu = 1.0 / (C / tau_sq + 1.0 / s0_sq)\n        post_mean_mu = post_var_mu * (np.sum(thetas) / tau_sq + m0 / s0_sq)\n        \n        mu = rng.normal(post_mean_mu, np.sqrt(post_var_mu))\n\n        # 3. Sample tau_sq\n        # Posterior for tau_sq is Inverse-Gamma\n        post_alpha = alpha0 + C / 2.0\n        post_beta = beta0 + 0.5 * np.sum((thetas - mu)**2)\n        \n        # Sample from Inverse-Gamma by sampling from Gamma and inverting\n        # Gamma(shape=alpha, scale=1/beta) corresponds to InverseGamma(shape=alpha, scale=beta)\n        gamma_sample = rng.gamma(shape=post_alpha, scale=1.0/post_beta)\n        tau_sq = 1.0 / gamma_sample\n\n        # Store samples after burn-in\n        if i >= n_burn:\n            idx = i - n_burn\n            mu_samples[idx] = mu\n            tau_samples[idx] = np.sqrt(tau_sq)\n\n    # Compute posterior means\n    post_mean_mu = np.mean(mu_samples)\n    post_mean_tau = np.mean(tau_samples)\n\n    return post_mean_mu, post_mean_tau\n\ndef solve():\n    \"\"\"\n    Main function to run the Gibbs sampler on all test cases and print results.\n    \"\"\"\n    # Hyperparameters and sampler settings\n    SIGMA_SQ = 1.0\n    M0 = 0.0\n    S0_SQ = 10.0\n    ALPHA0 = 0.5\n    BETA0 = 0.5\n    N_ITER = 20000\n    N_BURN = 5000\n    SEED = 42\n\n    # Test suite\n    test_cases = [\n        # Test case 1\n        [\n            (np.array([1.20, 0.90, 1.10, 1.00]), np.array([1.00, 0.80, 1.20, 1.10])),\n            (np.array([0.20, 0.00, -0.10]), np.array([0.60, 0.50, 0.70])),\n            (np.array([1.80, 1.60, 1.90, 2.10, 1.70]), np.array([1.40, 1.30, 1.20, 1.50, 1.30])),\n        ],\n        # Test case 2\n        [\n            (np.array([0.55, 0.52, 0.58, 0.49, 0.53, 0.51, 0.56, 0.54]), np.array([2.50, 2.20, 2.80, 2.00, 2.40, 2.30, 2.60, 2.50])),\n            (np.array([0.90]), np.array([0.40])),\n            (np.array([0.10, 0.20]), np.array([0.70, 0.80])),\n            (np.array([0.50, 0.60, 0.40]), np.array([1.00, 0.90, 1.10])),\n        ],\n        # Test case 3\n        [\n            (np.array([-1.50, -1.60, -1.40]), np.array([1.20, 1.10, 1.30])),\n            (np.array([1.50, 1.60, 1.40]), np.array([1.20, 1.10, 1.30])),\n        ],\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        mean_mu, mean_tau = run_gibbs_sampler(case_data, SIGMA_SQ, M0, S0_SQ, ALPHA0, BETA0, N_ITER, N_BURN, SEED)\n        all_results.extend([mean_mu, mean_tau])\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in all_results)}]\")\n\nsolve()\n```", "id": "2540689"}]}