{"hands_on_practices": [{"introduction": "This first practice establishes the foundation of mark-recapture analysis, transforming simple field counts into a rigorous population estimate. Starting with a classic two-sample dataset, you will derive and apply the fundamental Lincoln-Petersen ($\\hat{N}_{\\mathrm{LP}}$) and Chapman ($\\hat{N}_{\\mathrm{Ch}}$) estimators to calculate abundance [@problem_id:2523184]. This exercise reinforces the core logic of the method and demonstrates how to quantify the uncertainty surrounding these point estimates using the delta method, a widely used analytical approximation.", "problem": "A closed two-sample capture–recapture study of a single population is conducted under the following standard assumptions: the population is demographically and geographically closed between sampling occasions; marks are permanent, correctly recorded, and do not affect capture probability; and all individuals share a common capture probability on each occasion. On the first occasion, $M=100$ individuals are captured, marked, and released. On the second occasion, $n=120$ individuals are captured, of which $m=15$ are found to be marked from the first occasion.\n\nUnder the independence and closure assumptions, the number of marked individuals observed on the second occasion, $m$, has a hypergeometric distribution determined by the true abundance $N$, with mean $\\mathbb{E}[m]=n\\,M/N$ and variance $\\mathrm{Var}(m)=n\\,(M/N)\\,\\bigl(1-M/N\\bigr)\\,\\bigl((N-n)/(N-1)\\bigr)$.\n\nStarting from these foundational facts and the large-sample delta method for smooth transformations, do the following:\n\n1. Derive the classical method-of-moments Lincoln–Petersen abundance estimator $\\hat N_{\\mathrm{LP}}$ by equating $\\mathbb{E}[m]$ to its observed value, and compute its value for the observed data.\n2. Adopt the standard first-order bias-reduced alternative, the Chapman estimator $\\hat N_{\\mathrm{Ch}}$, and compute its value for the observed data.\n3. Using the delta method with the hypergeometric variance of $m$ and a plug-in for the unknown $N$ appropriate to each estimator, derive and compute approximate standard errors for $\\hat N_{\\mathrm{LP}}$ and $\\hat N_{\\mathrm{Ch}}$.\n4. Construct approximate $95\\%$ confidence intervals for each estimator using the normal approximation.\n\nReport your final numerical results in the following order as eight numbers: $\\hat N_{\\mathrm{LP}}$, the standard error of $\\hat N_{\\mathrm{LP}}$, the lower and upper endpoints of the $95\\%$ confidence interval for $\\hat N_{\\mathrm{LP}}$, then $\\hat N_{\\mathrm{Ch}}$, the standard error of $\\hat N_{\\mathrm{Ch}}$, and the lower and upper endpoints of the $95\\%$ confidence interval for $\\hat N_{\\mathrm{Ch}}$. Round all reported numerical quantities to three significant figures. Do not include any units in your final reported values.", "solution": "The problem presented is a standard exercise in population abundance estimation using mark-recapture data. We first validate the problem statement. The givens are: number of individuals marked in the first sample, $M=100$; size of the second sample, $n=120$; number of marked individuals recaptured in the second sample, $m=15$. The problem states the standard assumptions for a closed-population model and correctly identifies the distribution of $m$ as hypergeometric, providing the correct formulae for its mean and variance. The tasks are mathematically and statistically well-defined. The problem is therefore scientifically grounded, well-posed, and objective. It is valid for solution.\n\nWe will address the four specified tasks in sequence. All numerical results will be rounded to three significant figures as required.\n\n1. The Lincoln–Petersen Estimator ($\\hat{N}_{\\mathrm{LP}}$)\n\nThe method of moments equates the theoretical expectation of a statistic to its observed value. Here, the statistic is the number of recaptures, $m$. The expectation is given as $\\mathbb{E}[m] = \\frac{nM}{N}$. We set this equal to the observed value of $m$:\n$$m = \\frac{nM}{N}$$\nSolving for the unknown population size $N$ yields the Lincoln–Petersen estimator, $\\hat{N}_{\\mathrm{LP}}$:\n$$\\hat{N}_{\\mathrm{LP}} = \\frac{nM}{m}$$\nSubstituting the given data: $M=100$, $n=120$, and $m=15$:\n$$\\hat{N}_{\\mathrm{LP}} = \\frac{120 \\times 100}{15} = \\frac{12000}{15} = 800$$\nTo three significant figures, this is $8.00 \\times 10^{2}$.\n\n2. The Chapman Estimator ($\\hat{N}_{\\mathrm{Ch}}$)\n\nThe Chapman estimator is a bias-corrected modification of the Lincoln–Petersen estimator. Its formula, which we adopt as instructed, is:\n$$\\hat{N}_{\\mathrm{Ch}} = \\frac{(M+1)(n+1)}{m+1} - 1$$\nSubstituting the given data:\n$$\\hat{N}_{\\mathrm{Ch}} = \\frac{(100+1)(120+1)}{15+1} - 1 = \\frac{101 \\times 121}{16} - 1 = \\frac{12221}{16} - 1 = 763.8125 - 1 = 762.8125$$\nRounding to three significant figures, we get $\\hat{N}_{\\mathrm{Ch}} = 763$.\n\n3. Approximate Standard Errors\n\nWe derive the standard errors using the delta method. For an estimator $\\hat{N}$ that is a function of the random variable $m$, say $\\hat{N} = g(m)$, the approximate variance is given by:\n$$\\mathrm{Var}(\\hat{N}) \\approx \\left( \\frac{dg}{dm} \\right)^2 \\mathrm{Var}(m)$$\nThe problem provides the hypergeometric variance of $m$:\n$$\\mathrm{Var}(m) = n \\left(\\frac{M}{N}\\right) \\left(1-\\frac{M}{N}\\right) \\left(\\frac{N-n}{N-1}\\right)$$\nWe will evaluate this expression by plugging in the appropriate estimate $\\hat{N}$ for the unknown true value $N$. The standard error is the square root of this estimated variance.\n\nStandard Error of $\\hat{N}_{\\mathrm{LP}}$:\nThe estimator is $\\hat{N}_{\\mathrm{LP}} = g(m) = \\frac{nM}{m}$. The derivative is:\n$$\\frac{dg}{dm} = -\\frac{nM}{m^2}$$\nThe estimated variance, $\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}})$, is therefore:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}}) \\approx \\left(-\\frac{nM}{m^2}\\right)^2 \\widehat{\\mathrm{Var}}(m) = \\left(\\frac{\\hat{N}_{\\mathrm{LP}}}{m}\\right)^2 \\widehat{\\mathrm{Var}}(m)$$\nWe estimate $\\mathrm{Var}(m)$ by substituting $N = \\hat{N}_{\\mathrm{LP}} = 800$.\n$$\\widehat{\\mathrm{Var}}(m) = n \\left(\\frac{M}{\\hat{N}_{\\mathrm{LP}}}\\right) \\left(1-\\frac{M}{\\hat{N}_{\\mathrm{LP}}}\\right) \\left(\\frac{\\hat{N}_{\\mathrm{LP}}-n}{\\hat{N}_{\\mathrm{LP}}-1}\\right)$$\nUsing the relation $\\frac{M}{\\hat{N}_{\\mathrm{LP}}} = \\frac{m}{n}$, this simplifies the expression for $\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}})$:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}}) \\approx \\left(\\frac{\\hat{N}_{\\mathrm{LP}}}{m}\\right)^2 \\left[ n \\left(\\frac{m}{n}\\right) \\left(1-\\frac{m}{n}\\right) \\left(\\frac{\\hat{N}_{\\mathrm{LP}}-n}{\\hat{N}_{\\mathrm{LP}}-1}\\right) \\right] = \\frac{\\hat{N}_{\\mathrm{LP}}^2 (n-m)}{mn} \\left(\\frac{\\hat{N}_{\\mathrm{LP}}-n}{\\hat{N}_{\\mathrm{LP}}-1}\\right)$$\nSubstituting the numerical values:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{LP}}) \\approx \\frac{(800)^2 (120-15)}{(15)(120)} \\left(\\frac{800-120}{800-1}\\right) = \\frac{640000 \\times 105}{1800} \\left(\\frac{680}{799}\\right) \\approx 31771.5$$\nThe standard error is the square root:\n$$\\mathrm{SE}(\\hat{N}_{\\mathrm{LP}}) = \\sqrt{31771.5} \\approx 178.2456$$\nTo three significant figures, $\\mathrm{SE}(\\hat{N}_{\\mathrm{LP}}) = 178$.\n\nStandard Error of $\\hat{N}_{\\mathrm{Ch}}$:\nThe estimator is $\\hat{N}_{\\mathrm{Ch}} = g(m) = \\frac{(M+1)(n+1)}{m+1} - 1$. The derivative is:\n$$\\frac{dg}{dm} = -\\frac{(M+1)(n+1)}{(m+1)^2}$$\nFrom the definition of $\\hat{N}_{\\mathrm{Ch}}$, we have $\\hat{N}_{\\mathrm{Ch}} + 1 = \\frac{(M+1)(n+1)}{m+1}$, so $\\frac{dg}{dm} = -\\frac{\\hat{N}_{\\mathrm{Ch}}+1}{m+1}$.\nThe estimated variance is:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx \\left(-\\frac{\\hat{N}_{\\mathrm{Ch}}+1}{m+1}\\right)^2 \\widehat{\\mathrm{Var}}(m)$$\nwhere we now plug $N = \\hat{N}_{\\mathrm{Ch}} = 762.8125$ into the variance formula for $m$:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx \\left(\\frac{\\hat{N}_{\\mathrm{Ch}}+1}{m+1}\\right)^2 \\left[ n \\left(\\frac{M}{\\hat{N}_{\\mathrm{Ch}}}\\right) \\left(1-\\frac{M}{\\hat{N}_{\\mathrm{Ch}}}\\right) \\left(\\frac{\\hat{N}_{\\mathrm{Ch}}-n}{\\hat{N}_{\\mathrm{Ch}}-1}\\right) \\right]$$\nSubstituting numerical values:\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx \\left(\\frac{763.8125}{16}\\right)^2 \\left[ 120 \\left(\\frac{100}{762.8125}\\right) \\left(1-\\frac{100}{762.8125}\\right) \\left(\\frac{762.8125-120}{762.8125-1}\\right) \\right]$$\n$$\\widehat{\\mathrm{Var}}(\\hat{N}_{\\mathrm{Ch}}) \\approx (47.738...)^2 \\left[ 120 (0.13109...) (0.86890...) (0.84379...) \\right] \\approx (2278.94) (11.5286) \\approx 26274.5$$\nThe standard error is the square root:\n$$\\mathrm{SE}(\\hat{N}_{\\mathrm{Ch}}) = \\sqrt{26274.5} \\approx 162.094$$\nTo three significant figures, $\\mathrm{SE}(\\hat{N}_{\\mathrm{Ch}}) = 162$.\n\n4. Approximate $95\\%$ Confidence Intervals\n\nWe use the normal approximation to construct the confidence intervals. The general formula is $\\hat{N} \\pm z_{\\alpha/2} \\mathrm{SE}(\\hat{N})$. For a $95\\%$ confidence level, $\\alpha=0.05$, and the critical value is $z_{0.025} = 1.96$.\n\nConfidence Interval for $\\hat{N}_{\\mathrm{LP}}$:\n$$\\mathrm{CI}_{\\mathrm{LP}} = 800 \\pm 1.96 \\times 178.2456 = 800 \\pm 349.36$$\nLower bound: $800 - 349.36 = 450.64$. Rounded to three significant figures, this is $451$.\nUpper bound: $800 + 349.36 = 1149.36$. Rounded to three significant figures, this is $1.15 \\times 10^3$.\n\nConfidence Interval for $\\hat{N}_{\\mathrm{Ch}}$:\n$$\\mathrm{CI}_{\\mathrm{Ch}} = 762.8125 \\pm 1.96 \\times 162.094 = 762.8125 \\pm 317.704$$\nLower bound: $762.8125 - 317.704 = 445.1085$. Rounded to three significant figures, this is $445$.\nUpper bound: $762.8125 + 317.704 = 1080.5165$. Rounded to three significant figures, this is $1.08 \\times 10^3$.\n\nThe eight required numerical quantities are, in order and rounded to three significant figures:\n1. $\\hat{N}_{\\mathrm{LP}} = 8.00 \\times 10^2$\n2. $\\mathrm{SE}(\\hat{N}_{\\mathrm{LP}}) = 178$\n3. Lower CI for $\\hat{N}_{\\mathrm{LP}} = 451$\n4. Upper CI for $\\hat{N}_{\\mathrm{LP}} = 1.15 \\times 10^3$\n5. $\\hat{N}_{\\mathrm{Ch}} = 763$\n6. $\\mathrm{SE}(\\hat{N}_{\\mathrm{Ch}}) = 162$\n7. Lower CI for $\\hat{N}_{\\mathrm{Ch}} = 445$\n8. Upper CI for $\\hat{N}_{\\mathrm{Ch}} = 1.08 \\times 10^3$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n8.00 \\times 10^{2}  178  451  1.15 \\times 10^{3}  763  162  445  1.08 \\times 10^{3}\n\\end{pmatrix}\n}\n$$", "id": "2523184"}, {"introduction": "Ecological processes are rarely described perfectly by a single statistical model, creating uncertainty about which model structure is \"best\". This practice moves beyond single-model inference by introducing the powerful concept of model averaging, a key component of modern information-theoretic approaches [@problem_id:2523129]. You will use the Akaike Information Criterion (AIC) to weigh evidence among a set of candidate models, compute a robust model-averaged abundance estimate, and, crucially, calculate an unconditional variance that properly incorporates both within-model sampling error and between-model selection uncertainty.", "problem": "A closed population of stream salamanders was sampled using $T=5$ capture occasions within a short period where demographic closure is reasonable. The study recorded $n_{\\text{obs}}=73$ unique individuals. Three closed-capture models are considered: model $M_{0}$ (constant capture probability), model $M_{t}$ (time-varying capture probabilities), and model $M_{h}$ (individual heterogeneity in capture probabilities). For each model, a Maximum Likelihood (ML) analysis produced the following summaries, where $K$ is the total number of free parameters estimated by the model (including population size $N$), $\\ell$ is the maximized log-likelihood, $\\hat{N}_{i}$ is the model-specific point estimate for $N$, and $\\widehat{\\operatorname{Var}}_{i}$ is the model-based variance of $\\hat{N}_{i}$ under model $i \\in \\{0,t,h\\}$:\n\n- $M_{0}$: $K_{0}=2$, $\\ell_{0}=-210.3$, $\\hat{N}_{0}=92.4$, $\\widehat{\\operatorname{Var}}_{0}=56.7$.\n- $M_{t}$: $K_{t}=6$, $\\ell_{t}=-205.9$, $\\hat{N}_{t}=95.1$, $\\widehat{\\operatorname{Var}}_{t}=72.4$.\n- $M_{h}$: $K_{h}=4$, $\\ell_{h}=-205.1$, $\\hat{N}_{h}=98.7$, $\\widehat{\\operatorname{Var}}_{h}=64.5$.\n\nStarting from the definitions of the Akaike Information Criterion (AIC), its small-sample correction (AICc) using effective sample size $n_{\\text{obs}}$, and the law of total expectation and the law of total variance, do the following:\n\n1. Compute the small-sample corrected Akaike values (AICc) for $M_{0}$, $M_{t}$, and $M_{h}$, then derive the normalized Akaike weights that place a probability mass function over the model set $\\{M_{0},M_{t},M_{h}\\}$.\n2. Using the interpretation of these weights as an empirical model distribution under equal model priors, derive from first principles a model-averaged estimator for $N$ and compute its value.\n3. Using the law of total variance applied to the model-averaged estimator, derive an unconditional variance expression that accounts for both within-model estimation uncertainty and between-model selection uncertainty, and compute its numerical value.\n\nReport the unconditional variance from part 3 as a pure number. Round your answer to four significant figures.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extracted Givens**\n- Number of capture occasions: $T=5$.\n- Number of observed unique individuals (effective sample size): $n_{\\text{obs}}=73$.\n- Model set evaluated: $\\{M_{0}, M_{t}, M_{h}\\}$.\n- Model $M_{0}$ (constant capture probability):\n  - Number of parameters: $K_{0}=2$.\n  - Maximized log-likelihood: $\\ell_{0}=-210.3$.\n  - Population size estimate: $\\hat{N}_{0}=92.4$.\n  - Estimated variance of $\\hat{N}_{0}$: $\\widehat{\\operatorname{Var}}_{0}=56.7$.\n- Model $M_{t}$ (time-varying capture probability):\n  - Number of parameters: $K_{t}=6$.\n  - Maximized log-likelihood: $\\ell_{t}=-205.9$.\n  - Population size estimate: $\\hat{N}_{t}=95.1$.\n  - Estimated variance of $\\hat{N}_{t}$: $\\widehat{\\operatorname{Var}}_{t}=72.4$.\n- Model $M_{h}$ (individual heterogeneity in capture probability):\n  - Number of parameters: $K_{h}=4$.\n  - Maximized log-likelihood: $\\ell_{h}=-205.1$.\n  - Population size estimate: $\\hat{N}_{h}=98.7$.\n  - Estimated variance of $\\hat{N}_{h}$: $\\widehat{\\operatorname{Var}}_{h}=64.5$.\n\n**Step 2: Validation**\nThe problem is scientifically grounded, rooted in the established ecological and statistical practice of mark-recapture analysis and information-theoretic model selection. The models presented ($M_{0}$, $M_{t}$, $M_{h}$) constitute a standard set for closed-population studies. All necessary data are provided, the terminology is precise, and the objectives are quantifiable. The problem contains no scientific fallacies, contradictions, or ambiguities. It is a well-posed problem in quantitative ecology.\n\n**Step 3: Verdict**\nThe problem is valid. A solution will be formulated.\n\nThe solution proceeds in three parts as specified.\n\n**Part 1: AICc and Akaike Weights**\n\nThe small-sample corrected Akaike Information Criterion (AICc) is defined for a model with $K$ parameters and maximized log-likelihood $\\ell$, given a sample size of $n$, as:\n$$\n\\text{AICc} = -2\\ell + 2K + \\frac{2K(K+1)}{n-K-1}\n$$\nHere, the effective sample size is given as $n = n_{\\text{obs}} = 73$. We compute the AICc for each of the three models, $M_{i}$ where $i \\in \\{0, t, h\\}$.\n\nFor model $M_{0}$:\n$$\n\\text{AICc}_{0} = -2(-210.3) + 2(2) + \\frac{2(2)(2+1)}{73-2-1} = 420.6 + 4 + \\frac{12}{70} \\approx 424.7714\n$$\nFor model $M_{t}$:\n$$\n\\text{AICc}_{t} = -2(-205.9) + 2(6) + \\frac{2(6)(6+1)}{73-6-1} = 411.8 + 12 + \\frac{84}{66} \\approx 425.0727\n$$\nFor model $M_{h}$:\n$$\n\\text{AICc}_{h} = -2(-205.1) + 2(4) + \\frac{2(4)(4+1)}{73-4-1} = 410.2 + 8 + \\frac{40}{68} \\approx 418.7882\n$$\nThe model with the minimum AICc value is the best-supported model. We find $\\text{AICc}_{\\min} = \\text{AICc}_{h} \\approx 418.7882$.\n\nNext, we compute the delta AICc values, $\\Delta_i = \\text{AICc}_i - \\text{AICc}_{\\min}$:\n$$\n\\Delta_{0} = 424.7714 - 418.7882 = 5.9832\n$$\n$$\n\\Delta_{t} = 425.0727 - 418.7882 = 6.2845\n$$\n$$\n\\Delta_{h} = 418.7882 - 418.7882 = 0\n$$\nThe Akaike weight for model $i$, $w_i$, which represents the probability that model $i$ is the best model in the set, is given by:\n$$\nw_i = \\frac{\\exp(-\\frac{1}{2}\\Delta_i)}{\\sum_{j \\in \\{0,t,h\\}} \\exp(-\\frac{1}{2}\\Delta_j)}\n$$\nThe denominator is:\n$$\n\\sum_{j} \\exp(-\\frac{1}{2}\\Delta_j) = \\exp(-\\frac{5.9832}{2}) + \\exp(-\\frac{6.2845}{2}) + \\exp(-\\frac{0}{2}) \\approx 0.05021 + 0.04319 + 1 = 1.0934\n$$\nThe individual weights are:\n$$\nw_{0} = \\frac{0.05021}{1.0934} \\approx 0.04592\n$$\n$$\nw_{t} = \\frac{0.04319}{1.0934} \\approx 0.03950\n$$\n$$\nw_{h} = \\frac{1}{1.0934} \\approx 0.91458\n$$\n\n**Part 2: Model-Averaged Estimator for N**\n\nWe are tasked to derive the model-averaged estimator for $N$ from first principles using the law of total expectation. Let $\\hat{N}_a$ be the model-averaged estimator for the true population size $N$. Let $M$ be a discrete random variable representing the model choice from the set $\\{M_0, M_t, M_h\\}$, with probability mass function $P(M=M_i) = w_i$. The law of total expectation states that the expected value of a random variable is the expected value of its conditional expectation. Applied to our estimator $\\hat{N}_a$:\n$$\n\\hat{N}_a = E[\\hat{N}] = E_M[E[\\hat{N} | M]]\n$$\nThe inner expectation, $E[\\hat{N} | M=M_i]$, is the point estimate of $N$ given that model $M_i$ is true, which is simply $\\hat{N}_i$. The outer expectation is taken over the distribution of models, weighted by the Akaike weights $w_i$. This expands to:\n$$\n\\hat{N}_a = \\sum_{i \\in \\{0,t,h\\}} E[\\hat{N} | M=M_i] P(M=M_i) = \\sum_{i \\in \\{0,t,h\\}} \\hat{N}_i w_i\n$$\nUsing the calculated weights and provided estimates:\n$$\n\\hat{N}_a = (92.4)(0.04592) + (95.1)(0.03950) + (98.7)(0.91458)\n$$\n$$\n\\hat{N}_a \\approx 4.243 + 3.756 + 90.269 = 98.268\n$$\nUsing higher precision values for the weights: $w_0 \\approx 0.04592186$, $w_t \\approx 0.03950005$, $w_h \\approx 0.91457809$.\n$$\n\\hat{N}_a = (92.4)(0.04592186) + (95.1)(0.03950005) + (98.7)(0.91457809) \\approx 98.26849\n$$\n\n**Part 3: Unconditional Variance**\n\nWe must derive the unconditional variance of $\\hat{N}_a$ using the law of total variance. This law states that for random variables $X$ and $Y$, $\\operatorname{Var}(X) = E[\\operatorname{Var}(X|Y)] + \\operatorname{Var}(E[X|Y])$. Applying this to our estimator $\\hat{N}_a$ with conditioning on the model choice $M$:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{N}_a) = E_M[\\widehat{\\operatorname{Var}}(\\hat{N} | M)] + \\operatorname{Var}_M(E[\\hat{N} | M])\n$$\nThe first term, $E_M[\\widehat{\\operatorname{Var}}(\\hat{N} | M)]$, is the expected within-model variance. The conditional variance, $\\widehat{\\operatorname{Var}}(\\hat{N} | M=M_i)$, is the variance of the estimator $\\hat{N}_i$ computed under model $M_i$, which is the given $\\widehat{\\operatorname{Var}}_i$. The expectation is then the weighted average over all models:\n$$\nE_M[\\widehat{\\operatorname{Var}}(\\hat{N} | M)] = \\sum_{i} \\widehat{\\operatorname{Var}}_i w_i\n$$\nThe second term, $\\operatorname{Var}_M(E[\\hat{N} | M])$, is the variance of the conditional expectation, which represents the uncertainty due to model selection. The conditional expectation $E[\\hat{N} | M=M_i]$ is the point estimate $\\hat{N}_i$. We need the variance of a discrete random variable that takes values $\\hat{N}_i$ with probabilities $w_i$. The mean of this variable is $\\hat{N}_a$. The variance is thus:\n$$\n\\operatorname{Var}_M(E[\\hat{N} | M]) = \\sum_{i} (\\hat{N}_i - E[\\hat{N}])^2 w_i = \\sum_{i} (\\hat{N}_i - \\hat{N}_a)^2 w_i\n$$\nCombining both terms yields the estimator for the unconditional variance as proposed by Buckland, Burnham, and Augustin (1997):\n$$\n\\widehat{\\operatorname{Var}}(\\hat{N}_a) = \\sum_{i} w_i \\widehat{\\operatorname{Var}}_i + \\sum_{i} w_i (\\hat{N}_i - \\hat{N}_a)^2 = \\sum_{i} w_i \\left( \\widehat{\\operatorname{Var}}_i + (\\hat{N}_i - \\hat{N}_a)^2 \\right)\n$$\nWe now compute this value. First, the component terms $(\\hat{N}_i - \\hat{N}_a)^2$:\n- $(\\hat{N}_0 - \\hat{N}_a)^2 = (92.4 - 98.26849)^2 \\approx (-5.86849)^2 \\approx 34.43927$\n- $(\\hat{N}_t - \\hat{N}_a)^2 = (95.1 - 98.26849)^2 \\approx (-3.16849)^2 \\approx 10.03943$\n- $(\\hat{N}_h - \\hat{N}_a)^2 = (98.7 - 98.26849)^2 \\approx (0.43151)^2 \\approx 0.18620$\n\nThe first component of variance (within-model):\n$$\n\\sum_{i} w_i \\widehat{\\operatorname{Var}}_i = (0.04592186)(56.7) + (0.03950005)(72.4) + (0.91457809)(64.5) \\approx 2.60477 + 2.85980 + 58.99029 \\approx 64.45486\n$$\nThe second component of variance (between-model):\n$$\n\\sum_{i} w_i (\\hat{N}_i - \\hat{N}_a)^2 = (0.04592186)(34.43927) + (0.03950005)(10.03943) + (0.91457809)(0.18620) \\approx 1.58148 + 0.39656 + 0.17024 \\approx 2.14828\n$$\nThe total unconditional variance is the sum of these two components:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{N}_a) = 64.45486 + 2.14828 = 66.60314\n$$\nRounding to four significant figures, the result is $66.60$.", "answer": "$$\n\\boxed{66.60}\n$$", "id": "2523129"}, {"introduction": "While analytical approximations like the delta method are valuable, their accuracy depends on large-sample assumptions that may not be met with the sparse data common in ecological studies. This hands-on coding exercise introduces the parametric bootstrap, a flexible and powerful computer-intensive method for assessing estimator properties and constructing reliable confidence intervals without relying on asymptotic theory [@problem_id:2523164]. By simulating data directly from the underlying hypergeometric sampling model, you will gain practical experience with a cornerstone of modern statistical inference and learn to build a more robust understanding of uncertainty in population estimates.", "problem": "A closed population is sampled twice under the standard two-occasion capture–recapture setting. Let $N$ denote the unknown population size, $n_1$ the number of individuals captured and marked on occasion $1$, $n_2$ the number captured on occasion $2$, and $m$ the number among the second sample that were previously marked. Assume a closed population over the two occasions, individual and temporal homogeneity of capture probabilities, and independent captures across individuals and occasions. Under these assumptions and sampling without replacement, $m$ is a discrete random variable with a hypergeometric law conditioned on $N$, $n_1$, and $n_2$. The task is to construct a parametric bootstrap for the two-sample model to estimate the bias and a two-sided confidence interval at level $1-\\alpha$ for $N$, choosing an estimator that remains finite when $m=0$. Base your construction only on these foundational assumptions and the hypergeometric sampling structure; you must not invoke asymptotic normal approximations.\n\nYour program must implement the following components:\n\n- Choose a small-sample bias-reduced, finite-for-$m=0$ two-sample estimator $\\widehat{N}$ of $N$ as the plug-in parameter for the bootstrap data-generating model. Use only the two-occasion data $(n_1,n_2,m)$.\n\n- Parametric bootstrap mechanism: conditional on a plug-in value $N_0$ constructed from $\\widehat{N}$, simulate bootstrap recaptures $m^\\ast$ from the correct distribution implied by the foundational assumptions (i.e., the distribution arising from two independent without-replacement samples of sizes $n_1$ and $n_2$ from a population of size $N_0$). For each bootstrap replicate, compute the bootstrap estimate $\\widehat{N}^\\ast$ by applying the same estimator to $(n_1,n_2,m^\\ast)$.\n\n- Estimate the bootstrap bias as $\\widehat{\\text{bias}}=\\mathbb{E}^\\ast[\\widehat{N}^\\ast]-\\widehat{N}$, where $\\mathbb{E}^\\ast[\\cdot]$ denotes expectation under the bootstrap distribution. Construct a two-sided confidence interval for $N$ at level $1-\\alpha$ by inverting the empirical cumulative distribution function of $\\{\\widehat{N}^\\ast\\}$ via the percentile method at tail probabilities $\\alpha/2$ and $1-\\alpha/2$.\n\n- Justify within your algorithmic design the choice of a parametric bootstrap in light of the discreteness of $m$ and its support constraints that depend on $N$, $n_1$, and $n_2$.\n\nAll outputs in this problem are unitless counts (no physical units). Probabilities must be interpreted as decimals (for example, write $0.95$ instead of any percentage notation). Angles are not involved.\n\nTest suite and required output:\n\nImplement your program to compute, for each test case below, a list of four floats $[\\widehat{N},\\widehat{\\text{bias}},L,U]$, where $L$ and $U$ are the lower and upper percentile interval endpoints at level $1-\\alpha$, respectively. Use the following test suite:\n- Case A (typical): $(n_1,n_2,m,\\alpha,B,\\text{seed})=(120,150,30,0.05,10000,42)$.\n- Case B (near-boundary small overlap): $(n_1,n_2,m,\\alpha,B,\\text{seed})=(50,60,1,0.05,10000,12345)$.\n- Case C (large overlap): $(n_1,n_2,m,\\alpha,B,\\text{seed})=(200,180,150,0.05,10000,2024)$.\n\nIn each case, initialize the random number generator with the given integer seed before drawing any bootstrap samples, and use exactly $B$ independent bootstrap replicates.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of lists enclosed in square brackets, with each float rounded to $6$ decimal places, in the order $[\\text{Case A},\\text{Case B},\\text{Case C}]$. For example, the output format must be like\n$[[x_{A,1},x_{A,2},x_{A,3},x_{A,4}],[x_{B,1},x_{B,2},x_{B,3},x_{B,4}],[x_{C,1},x_{C,2},x_{C,3},x_{C,4}]]$,\nwhere each $x_{\\cdot,\\cdot}$ is a float written in standard decimal notation.", "solution": "The problem statement has been validated and is determined to be valid. It is a well-posed problem in quantitative ecology and statistics, grounded in established first principles of population estimation. It provides all necessary information and is free from scientific error, ambiguity, and contradiction. We may proceed with a solution.\n\nThe problem requires the construction of a parametric bootstrap procedure to estimate the bias and a confidence interval for the population size, $N$, in a two-sample capture-recapture study. The foundational assumptions are a closed population, homogeneous capture probabilities, and independent captures, which imply that the number of recaptured individuals, $m$, follows a hypergeometric distribution conditional on the population size $N$, the first sample size $n_1$, and the second sample size $n_2$.\n\nFirst, an appropriate estimator for $N$ must be selected. The problem specifies that the estimator must be bias-reduced for small samples and remain finite when the number of recaptures $m=0$. The classic Lincoln-Petersen estimator, $\\widehat{N}_{LP} = \\frac{n_1 n_2}{m}$, fails on both counts, as it is undefined for $m=0$ and is known to be biased. A suitable choice that satisfies the requirements is the Chapman estimator:\n$$\n\\widehat{N} = \\frac{(n_1+1)(n_2+1)}{m+1} - 1\n$$\nThis estimator is a well-established modification that is finite for all valid $m \\ge 0$ and possesses a smaller bias than $\\widehat{N}_{LP}$, particularly when $n_1+n_2$ is close to $N$. This estimator will be used for both the initial estimate and the bootstrap re-estimates.\n\nThe parametric bootstrap procedure is constructed as follows:\n\n1.  **Initial Point Estimate**: Given the observed data $(n_1, n_2, m)$, calculate the initial estimate of the population size using the Chapman estimator:\n    $$\n    \\widehat{N} = \\frac{(n_1+1)(n_2+1)}{m+1} - 1\n    $$\n2.  **Bootstrap Model Formulation**: The parametric bootstrap simulates data from a fully specified model where the unknown parameters are replaced by their estimates. Here, the only unknown parameter in the data-generating process for $m$ is $N$. We set the population size for our bootstrap simulation to $N_0 = \\text{round}(\\widehat{N})$. We must ensure $N_0 \\ge n_1$ and $N_0 \\ge n_2$. The Chapman estimator guarantees $\\widehat{N} \\ge \\max(n_1, n_2)-1$, so after rounding, $N_0$ is a valid population size for the observed samples.\n3.  **Bootstrap Replication**: For a specified number of replicates, $B$, repeat the following steps for $b = 1, \\dots, B$:\n    a. **Data Generation**: Generate a bootstrap sample of recaptures, $m^{\\ast}_b$, by drawing a single random variate from the hypergeometric distribution with parameters corresponding to a second sample of size $n_2$ from a population of size $N_0$ containing $n_1$ marked individuals. The probability mass function is:\n       $$\n       P(m^{\\ast} | N_0, n_1, n_2) = \\frac{\\binom{n_1}{m^{\\ast}} \\binom{N_0 - n_1}{n_2 - m^{\\ast}}}{\\binom{N_0}{n_2}}\n       $$\n       The valid support for $m^{\\ast}$ is $[\\max(0, n_1+n_2-N_0), \\min(n_1, n_2)]$.\n    b. **Re-estimation**: Using the bootstrap recapture count $m^{\\ast}_b$, calculate the bootstrap estimate of population size, $\\widehat{N}^{\\ast}_b$, by applying the same Chapman estimator:\n       $$\n       \\widehat{N}^{\\ast}_b = \\frac{(n_1+1)(n_2+1)}{m^{\\ast}_b+1} - 1\n       $$\n    This process yields a collection of $B$ bootstrap estimates, $\\{\\widehat{N}^{\\ast}_1, \\widehat{N}^{\\ast}_2, \\dots, \\widehat{N}^{\\ast}_B\\}$, which forms an empirical distribution for the estimator $\\widehat{N}$.\n\nThe use of a parametric bootstrap is justified and necessary in this context. The distribution of the pivotal quantity $m$ is discrete and its support critically depends on the unknown parameter $N$. Standard non-parametric resampling methods (e.g., resampling individuals) are not directly applicable or easily adapted to preserve the two-sample dependency structure. The parametric bootstrap, however, directly honors the fundamental theoretical model of the sampling process. By simulating from the hypergeometric distribution with a plug-in estimate for $N$, we correctly propagate the sampling variability inherent in the study design. This is particularly crucial when $m$ is small, as the resulting distribution of $\\widehat{N}$ is highly skewed, making methods that rely on asymptotic normality (which are explicitly forbidden) inaccurate. The bootstrap provides a more robust and reliable assessment of uncertainty.\n\nFinally, we compute the required quantities from the bootstrap distribution:\n\n-   **Bootstrap Bias**: The bias of the estimator is estimated by the difference between the mean of the bootstrap estimates and the original sample estimate:\n    $$\n    \\widehat{\\text{bias}} = \\mathbb{E}^\\ast[\\widehat{N}^\\ast] - \\widehat{N} = \\left(\\frac{1}{B} \\sum_{b=1}^{B} \\widehat{N}^{\\ast}_b\\right) - \\widehat{N}\n    $$\n-   **Percentile Confidence Interval**: For a confidence level of $1-\\alpha$, the two-sided percentile interval is constructed from the quantiles of the empirical bootstrap distribution. The lower bound, $L$, is the $100(\\alpha/2)$-th percentile, and the upper bound, $U$, is the $100(1-\\alpha/2)$-th percentile of the sorted bootstrap estimates $\\{\\widehat{N}^{\\ast}_{(b)}\\}$.\n\nThe algorithm will now be implemented to solve for the provided test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import hypergeom\n\ndef solve():\n    \"\"\"\n    Solves the parametric bootstrap problem for capture-recapture analysis.\n    \"\"\"\n    # Test suite: (n1, n2, m, alpha, B, seed)\n    test_cases = [\n        (120, 150, 30, 0.05, 10000, 42),      # Case A\n        (50, 60, 1, 0.05, 10000, 12345),   # Case B\n        (200, 180, 150, 0.05, 10000, 2024),  # Case C\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        n1, n2, m, alpha, B, seed = case\n\n        # Initialize the random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Compute the initial estimate of N using Chapman's estimator.\n        # This estimator is finite for m=0 and has reduced bias.\n        N_hat = ((n1 + 1) * (n2 + 1) / (m + 1)) - 1\n\n        # Step 2: Define the parameter for the bootstrap data-generating model.\n        # The population size is a discrete parameter.\n        N0 = int(round(N_hat))\n\n        # Basic check for hypergeometric distribution parameter validity.\n        # Chapman's estimator ensures N_hat = max(n1, n2) - 1, so N0 is valid.\n        if N0  n1 or N0  n2:\n            # This case should not be reached with a valid estimator.\n            # Handle as an error or skip, but here we assume validity.\n            pass\n\n        # Step 3: Parametric Bootstrap - generate B bootstrap samples.\n        # a. Simulate m* from the hypergeometric distribution.\n        # M: total number of objects (population size N0)\n        # n: total number of type I objects (marked individuals n1)\n        # N: number of draws (second sample size n2)\n        m_star_samples = hypergeom.rvs(M=N0, n=n1, N=n2, size=B, random_state=rng)\n        \n        # b. Compute N* for each bootstrap sample m*.\n        # The operation is vectorized for efficiency.\n        N_star_estimates = ((n1 + 1) * (n2 + 1) / (m_star_samples + 1)) - 1\n        \n        # Step 4: Compute bias and confidence interval from bootstrap distribution.\n        \n        # Estimate the bootstrap bias.\n        bias = np.mean(N_star_estimates) - N_hat\n        \n        # Construct the percentile confidence interval.\n        lower_percentile = 100 * (alpha / 2)\n        upper_percentile = 100 * (1 - alpha / 2)\n        \n        L = np.percentile(N_star_estimates, lower_percentile)\n        U = np.percentile(N_star_estimates, upper_percentile)\n        \n        # Assemble the results for the current case.\n        # [N_hat, bias, lower_bound, upper_bound]\n        case_results = [N_hat, bias, L, U]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    formatted_cases = [\n        f\"[{','.join(f'{val:.6f}' for val in res)}]\" for res in all_results\n    ]\n    final_output_string = f\"[{','.join(formatted_cases)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "2523164"}]}