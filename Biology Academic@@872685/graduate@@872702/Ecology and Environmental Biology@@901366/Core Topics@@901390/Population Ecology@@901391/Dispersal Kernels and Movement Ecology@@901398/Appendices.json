{"hands_on_practices": [{"introduction": "Dispersal kernels are the mathematical backbone of movement ecology, but their functional forms can vary dramatically. A crucial way to characterize and compare kernels is through their moments, which describe properties like the average dispersal distance (first moment) and its variance (second moment). This exercise explores the moments of the Laplace and Cauchy kernels, revealing fundamental differences in their capacity to model rare, long-distance dispersal events—a property known as having \"fat tails.\" [@problem_id:2480505]", "problem": "A population ecologist models one-dimensional seed dispersal distances along a straight riparian corridor using alternative dispersal kernels. Let $X$ denote the signed displacement (in meters) from a parent plant to the landing location of a seed sampled at random. Two widely used dispersal kernels are considered as probability density functions (PDFs) for $X$:\n\n- A Laplace kernel with rate parameter $\\lambda > 0$, given by $K_{L}(x) = \\frac{\\lambda}{2}\\exp(-\\lambda|x|)$ for all real $x$.\n- A Cauchy kernel with scale parameter $\\gamma > 0$, given by $K_{C}(x) = \\frac{1}{\\pi\\gamma}\\frac{1}{1+(x/\\gamma)^{2}}$ for all real $x$.\n\nFor a fixed nonnegative real $p$, define the $p$th absolute moment of a kernel $K$ as\n$$\n\\mu_{p}(K) \\equiv \\int_{-\\infty}^{\\infty} |x|^{p}\\,K(x)\\,\\mathrm{d}x.\n$$\n\nTasks:\n1. Derive a closed-form expression for $\\mu_{p}(K_{L})$ as a function of $p$ and $\\lambda$. You may express your answer using the Gamma function $\\Gamma(\\cdot)$.\n2. Determine the number $p^{\\star} \\equiv \\sup\\{p \\ge 0 : \\mu_{p}(K_{C})  \\infty\\}$ for the Cauchy kernel.\n\nReport your final answer as a two-entry row vector $\\big[\\mu_{p}(K_{L}),\\,p^{\\star}\\big]$. No rounding is required, and no units should be included in the final reported expression.", "solution": "The problem statement presented is a well-defined mathematical exercise in the context of ecological modeling. It consists of two independent tasks: the calculation of the $p$-th absolute moment for a Laplace distribution and the determination of the domain of convergence for the $p$-th absolute moment of a Cauchy distribution. The problem is scientifically grounded, uses standard mathematical definitions, and contains all necessary information for a unique solution. Therefore, it is deemed valid, and a full derivation is provided below.\n\nThe $p$-th absolute moment of a kernel $K(x)$ is defined as\n$$ \\mu_{p}(K) \\equiv \\int_{-\\infty}^{\\infty} |x|^{p}\\,K(x)\\,\\mathrm{d}x $$\nfor a given $p \\ge 0$.\n\nTask 1: Calculation of $\\mu_{p}(K_{L})$ for the Laplace kernel.\n\nThe Laplace kernel is given by $K_{L}(x) = \\frac{\\lambda}{2}\\exp(-\\lambda|x|)$ for $\\lambda > 0$. We substitute this into the definition of the moment:\n$$ \\mu_{p}(K_{L}) = \\int_{-\\infty}^{\\infty} |x|^{p} \\frac{\\lambda}{2}\\exp(-\\lambda|x|) \\,\\mathrm{d}x $$\nThe integrand, $f(x) = |x|^{p} \\frac{\\lambda}{2}\\exp(-\\lambda|x|)$, is an even function of $x$ since $|-x|^{p} = |x|^{p}$ and $|-x| = |x|$. Thus, the integral over $(-\\infty, \\infty)$ is twice the integral over $(0, \\infty)$:\n$$ \\mu_{p}(K_{L}) = 2 \\int_{0}^{\\infty} x^{p} \\frac{\\lambda}{2}\\exp(-\\lambda x) \\,\\mathrm{d}x = \\lambda \\int_{0}^{\\infty} x^{p} \\exp(-\\lambda x) \\,\\mathrm{d}x $$\nFor this integral to converge, we require $p > -1$. Since the problem specifies $p \\ge 0$, this condition is always satisfied.\nWe proceed by making a substitution to relate the integral to the standard definition of the Gamma function, $\\Gamma(z) = \\int_{0}^{\\infty} t^{z-1} \\exp(-t) \\,\\mathrm{d}t$.\nLet $t = \\lambda x$. Then $x = t/\\lambda$ and $\\mathrm{d}x = (1/\\lambda)\\mathrm{d}t$. The limits of integration remain from $0$ to $\\infty$.\nSubstituting these into the expression for $\\mu_{p}(K_{L})$ yields:\n$$ \\mu_{p}(K_{L}) = \\lambda \\int_{0}^{\\infty} \\left(\\frac{t}{\\lambda}\\right)^{p} \\exp(-t) \\left(\\frac{1}{\\lambda}\\right) \\,\\mathrm{d}t $$\n$$ \\mu_{p}(K_{L}) = \\lambda \\left(\\frac{1}{\\lambda^{p}}\\right) \\left(\\frac{1}{\\lambda}\\right) \\int_{0}^{\\infty} t^{p} \\exp(-t) \\,\\mathrm{d}t $$\n$$ \\mu_{p}(K_{L}) = \\frac{1}{\\lambda^{p}} \\int_{0}^{\\infty} t^{(p+1)-1} \\exp(-t) \\,\\mathrm{d}t $$\nThe integral on the right-hand side is the definition of the Gamma function $\\Gamma(p+1)$.\nTherefore, the closed-form expression for the $p$-th absolute moment of the Laplace kernel is:\n$$ \\mu_{p}(K_{L}) = \\frac{\\Gamma(p+1)}{\\lambda^{p}} $$\n\nTask 2: Determination of $p^{\\star}$ for the Cauchy kernel.\n\nThe Cauchy kernel is given by $K_{C}(x) = \\frac{1}{\\pi\\gamma}\\frac{1}{1+(x/\\gamma)^{2}}$ for $\\gamma > 0$. We are tasked with finding $p^{\\star} = \\sup\\{p \\ge 0 : \\mu_{p}(K_{C})  \\infty\\}$.\nFirst, we write the expression for $\\mu_{p}(K_{C})$:\n$$ \\mu_{p}(K_{C}) = \\int_{-\\infty}^{\\infty} |x|^{p} \\frac{1}{\\pi\\gamma}\\frac{1}{1+(x/\\gamma)^{2}} \\,\\mathrm{d}x $$\nThe integrand is again an even function, so we can simplify the integral:\n$$ \\mu_{p}(K_{C}) = \\frac{2}{\\pi\\gamma} \\int_{0}^{\\infty} \\frac{x^{p}}{1+(x/\\gamma)^{2}} \\,\\mathrm{d}x $$\nTo determine for which values of $p \\ge 0$ this integral is finite, we must analyze the behavior of the integrand near the limits of integration, $x \\to 0^{+}$ and $x \\to \\infty$.\nLet the integrand be $g(x) = \\frac{x^{p}}{1+(x/\\gamma)^{2}}$.\n\nNear $x=0$, the denominator $1+(x/\\gamma)^{2}$ approaches $1$. Thus, the integrand behaves as $x^{p}$:\n$$ g(x) \\sim x^{p} \\quad \\text{as } x \\to 0^{+} $$\nThe integral $\\int_{0}^{\\delta} x^{p} \\,\\mathrm{d}x$ for some small $\\delta  0$ converges if and only if $p  -1$. Since we are given $p \\ge 0$, the integral always converges near the lower limit $x=0$.\n\nNear $x=\\infty$, the term $1$ in the denominator becomes negligible compared to $(x/\\gamma)^{2}$. Thus, the integrand behaves as:\n$$ g(x) = \\frac{x^{p}}{1+x^{2}/\\gamma^{2}} \\sim \\frac{x^{p}}{x^{2}/\\gamma^{2}} = \\gamma^{2}x^{p-2} \\quad \\text{as } x \\to \\infty $$\nBy the limit comparison test for improper integrals, the integral $\\int_{M}^{\\infty} g(x) \\,\\mathrm{d}x$ for some large $M  0$ converges if and only if the integral $\\int_{M}^{\\infty} x^{p-2} \\,\\mathrm{d}x$ converges.\nThis latter integral converges if and only if the exponent is strictly less than $-1$:\n$$ p - 2  -1 \\implies p  1 $$\nFor the integral $\\mu_{p}(K_{C})$ to be finite, the conditions for convergence at both $x=0$ and $x=\\infty$ must be met. Combining $p \\ge 0$ (from the problem statement) and $p  1$ (from our convergence analysis), we find that the integral is finite for all $p$ in the interval $[0, 1)$.\nThe set of values for which the moment is finite is $\\{p \\in \\mathbb{R} \\mid 0 \\le p  1\\}$.\nThe problem asks for $p^{\\star}$, the supremum of this set.\n$$ p^{\\star} = \\sup [0, 1) = 1 $$\n\nThe two results are $\\mu_{p}(K_{L}) = \\frac{\\Gamma(p+1)}{\\lambda^{p}}$ and $p^{\\star} = 1$. These are to be reported as a two-entry row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\Gamma(p+1)}{\\lambda^{p}}  1\n\\end{pmatrix}\n}\n$$", "id": "2480505"}, {"introduction": "The mathematical properties of a dispersal kernel are not just abstract concepts; they have direct, predictable consequences for population dynamics. This practice problem builds a powerful bridge between a kernel's shape and a key ecological process: the speed of a biological invasion. By working through an integrodifference model with a Laplace kernel, you will see how the kernel's moment generating function (MGF) can be used to derive an exact formula for the minimal invasion speed, $c^*$. [@problem_id:2480511]", "problem": "Consider a $1$-dimensional integrodifference model for a population spreading in space,\n$$\nn_{t+1}(x) \\;=\\; \\int_{\\mathbb{R}} K(x-y)\\, f\\!\\left(n_t(y)\\right)\\, dy,\n$$\nwhere $t$ is discrete time, $x \\in \\mathbb{R}$ is space, $K(x)$ is a probability density describing dispersal, and $f$ is the density-regulated per-capita growth function. Assume that at low density the growth is linear with multiplier $R \\equiv f'(0)  1$ and that dispersal is given by the Laplace kernel\n$$\nK(x) \\;=\\; \\frac{1}{2b}\\,\\exp\\!\\left(-\\frac{|x|}{b}\\right)\n$$\nwith scale parameter $b0$. Let $M(s)$ denote the moment generating function (MGF) of $K$, defined by $M(s) \\equiv \\int_{\\mathbb{R}} \\exp(sx)\\,K(x)\\,dx$ for $|s|1/b$.\n\nStarting from the low-density linearization and an exponential ansatz for solutions, derive a variational characterization of the minimal asymptotic invasion speed $c^*$ in terms of $M(s)$. Then, using the Laplace kernel above, show that $M(s)$ can be written in closed form and determine an exact closed-form expression for $c^*$ in terms of $R$ and $b$. Your final expression should be given symbolically (no numerical evaluation), and you must express it in terms of elementary functions and, if needed, the Lambert $W$ function defined implicitly by $W(z)\\exp(W(z))=z$. Do not include units in your final answer. If multiple real branches of the Lambert $W$ function are mathematically possible, use the branch that yields a biologically admissible minimal speed for $R1$.\n\nProvide the final expression for $c^*$ as your answer. No rounding is required; report an exact closed-form expression.", "solution": "The problem requires the derivation of the minimal asymptotic invasion speed $c^*$ for a population governed by an integrodifference equation.\n\nThe integrodifference model is given by\n$$\nn_{t+1}(x) \\;=\\; \\int_{\\mathbb{R}} K(x-y)\\, f\\!\\left(n_t(y)\\right)\\, dy\n$$\nwhere $n_t(x)$ is the population density, $K(x)$ is the dispersal kernel, and $f(n)$ is the growth function.\n\nFirst, we validate the problem statement.\nThe givens are:\n1.  The model equation: $n_{t+1}(x) = \\int_{\\mathbb{R}} K(x-y) f(n_t(y)) dy$.\n2.  The growth rate at low density: $R \\equiv f'(0)  1$.\n3.  The dispersal kernel: $K(x) = \\frac{1}{2b} \\exp(-\\frac{|x|}{b})$ with $b0$.\n4.  The moment generating function (MGF) definition: $M(s) \\equiv \\int_{\\mathbb{R}} \\exp(sx) K(x) dx$ for $|s|1/b$.\n5.  Ancillary definition: The Lambert W function, $W(z)\\exp(W(z))=z$.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in mathematical ecology concerning reaction-diffusion and invasion dynamics. The premises are factually correct and internally consistent. Essential information is provided, and the problem is formalizable and verifiable. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe solution proceeds in three main parts:\n1.  Derivation of the variational characterization for the minimal speed $c^*$.\n2.  Calculation of the MGF for the specified Laplace kernel.\n3.  Solving the variational problem to find an explicit expression for $c^*$.\n\nStep 1: Variational Characterization of $c^*$\n\nAt the leading edge of the invasion front, the population density $n_t(x)$ is very low. We can linearize the growth function $f(n)$ around the unstable equilibrium $n=0$. A Taylor expansion gives $f(n) \\approx f(0) + f'(0)n$. Since $n=0$ is an equilibrium, we have $f(0)=0$. The problem defines $R \\equiv f'(0)$, so the approximation becomes $f(n) \\approx Rn$. The linearized integrodifference equation is:\n$$\nn_{t+1}(x) \\;=\\; R \\int_{\\mathbb{R}} K(x-y)\\, n_t(y)\\, dy\n$$\nWe seek traveling wave solutions of the form $n_t(x) = A \\exp(-s(x-ct))$, where $c$ is the wave speed and $s0$ is the exponential decay rate at the front. Substituting this ansatz into the linearized equation:\n\nThe left-hand side (LHS) is:\n$$\nn_{t+1}(x) = A \\exp(-s(x-c(t+1))) = A \\exp(-s(x-ct) + sc) = n_t(x) \\exp(sc)\n$$\nThe right-hand side (RHS) is:\n$$\nR \\int_{\\mathbb{R}} K(x-y)\\, A \\exp(-s(y-ct))\\, dy\n$$\nLet the dummy variable of integration be $z=x-y$, so $y=x-z$ and $dy = -dz$. The limits of integration are reversed.\n$$\nRHS = R \\int_{+\\infty}^{-\\infty} K(z)\\, A \\exp(-s(x-z-ct))\\, (-dz) = R A \\exp(-s(x-ct)) \\int_{-\\infty}^{+\\infty} K(z) \\exp(sz)\\, dz\n$$\nThe integral is the definition of the MGF, $M(s)$. Thus, the RHS becomes:\n$$\nRHS = R n_t(x) M(s)\n$$\nEquating the LHS and RHS and dividing by $n_t(x)$ yields the characteristic equation:\n$$\n\\exp(sc) = R M(s)\n$$\nSolving for the speed $c$ as a function of the decay rate $s$:\n$$\nsc = \\ln(R M(s)) \\implies c(s) = \\frac{\\ln(R) + \\ln(M(s))}{s}\n$$\nThe asymptotic speed of invasion $c^*$ is the minimum of these possible speeds over all valid decay rates $s0$. This provides the variational characterization:\n$$\nc^* = \\min_{s0} c(s) = \\min_{s0} \\left[ \\frac{\\ln(R) + \\ln(M(s))}{s} \\right]\n$$\n\nStep 2: Calculation of the MGF\n\nThe kernel is the Laplace distribution $K(x) = \\frac{1}{2b}\\exp(-\\frac{|x|}{b})$. We compute its MGF:\n$$\nM(s) = \\int_{-\\infty}^{\\infty} \\exp(sx) \\frac{1}{2b} \\exp\\left(-\\frac{|x|}{b}\\right) dx = \\frac{1}{2b} \\left[ \\int_{-\\infty}^0 \\exp(sx)\\exp\\left(\\frac{x}{b}\\right)dx + \\int_0^{\\infty} \\exp(sx)\\exp\\left(-\\frac{x}{b}\\right)dx \\right]\n$$\n$$\nM(s) = \\frac{1}{2b} \\left[ \\int_{-\\infty}^0 \\exp\\left(x\\left(s+\\frac{1}{b}\\right)\\right)dx + \\int_0^{\\infty} \\exp\\left(x\\left(s-\\frac{1}{b}\\right)\\right)dx \\right]\n$$\nThe integrals converge for $|s|  1/b$. Evaluating them:\n$$\nM(s) = \\frac{1}{2b} \\left[ \\left. \\frac{\\exp(x(s+1/b))}{s+1/b} \\right|_{-\\infty}^0 + \\left. \\frac{\\exp(x(s-1/b))}{s-1/b} \\right|_0^{\\infty} \\right] = \\frac{1}{2b} \\left[ \\frac{1}{s+1/b} - \\frac{1}{s-1/b} \\right]\n$$\n$$\nM(s) = \\frac{1}{2b} \\left[ \\frac{(s-1/b) - (s+1/b)}{(s+1/b)(s-1/b)} \\right] = \\frac{1}{2b} \\left[ \\frac{-2/b}{s^2-1/b^2} \\right] = \\frac{1/b^2}{1/b^2-s^2} = \\frac{1}{1 - b^2s^2}\n$$\nThe closed-form expression for the MGF is $M(s) = (1 - b^2s^2)^{-1}$.\n\nStep 3: Determining the Expression for $c^*$\n\nTo find the minimum of $c(s)$, we set its derivative with respect to $s$ to zero, i.e., $c'(s)=0$.\n$$\nc(s) = \\frac{\\ln(R) - \\ln(1-b^2s^2)}{s}\n$$\nThe condition $c'(s)=0$ leads to the equation:\n$$\ns \\frac{d}{ds}[-\\ln(1-b^2s^2)] - [-\\ln(1-b^2s^2)] = \\ln(R)\n$$\n$$\ns \\left( -\\frac{-2b^2s}{1-b^2s^2} \\right) + \\ln(1-b^2s^2) = \\ln(R) \\implies \\frac{2b^2s^2}{1-b^2s^2} + \\ln(1-b^2s^2) = \\ln(R)\n$$\nLet a new variable $Y = 1-b^2s^2$. Since $s0$ and $|s|1/b$, we have $0  Y  1$. Also, $b^2s^2=1-Y$. The equation transforms to:\n$$\n\\frac{2(1-Y)}{Y} + \\ln(Y) = \\ln(R) \\implies \\frac{2}{Y} - 2 + \\ln(Y) = \\ln(R)\n$$\nRearranging gives:\n$$\n\\ln(Y) + \\frac{2}{Y} = \\ln(R) + 2\n$$\nExponentiating both sides:\n$$\n\\exp(\\ln(Y) + 2/Y) = Y \\exp(2/Y) = \\exp(\\ln(R)+2) = R\\exp(2)\n$$\nTo match the form of the Lambert W function, let $W = -2/Y$. Then $Y = -2/W$. Substituting this into the equation:\n$$\n(-\\frac{2}{W}) \\exp(-W) = R\\exp(2) \\implies W\\exp(W) = \\frac{-2}{R\\exp(2)} = -2R^{-1}\\exp(-2)\n$$\nThis implies $W = W_k(-2R^{-1}\\exp(-2))$ for some branch $k$ of the Lambert W function.\n\nWe must choose the branch that yields a physically valid solution. We require $s$ to be real and positive, which implies $0Y1$. Since $Y=-2/W$, this condition becomes $0  -2/W  1$. This implies $W  -2$.\nThe argument of the W function is $z = -2R^{-1}\\exp(-2)$. Since $R1$, we have $0  2R^{-1}\\exp(-2)  2\\exp(-2)$. The value $-1/e$ is the branching point for real W solutions, and $-1/e \\approx -0.3679$, while $-2\\exp(-2) \\approx -0.2707$. The argument $z$ lies in the interval $(-2\\exp(-2), 0)$, which is within $[-1/e, 0)$. In this interval, there are two real branches:\n-   $W_0(z) \\in [-1, 0)$.\n-   $W_{-1}(z) \\in (-\\infty, -1]$.\nThe condition $W  -2$ can only be satisfied by the $W_{-1}$ branch. Specifically, since $z  -2\\exp(-2)$ and $W_{-1}$ is a decreasing function, $W_{-1}(z)  W_{-1}(-2\\exp(-2)) = -2$. Thus, we must use the $k=-1$ branch. Let $W^* = W_{-1}(-2R^{-1}\\exp(-2))$.\n\nWe now find the expression for $c^*$. The speed at the optimal $s$ (denoted $s^*$) is $c^*=c(s^*)$. An alternative expression for $c^*$ at the minimum is $c^* = M'(s^*)/M(s^*)$.\n$M'(s) = - (1-b^2s^2)^{-2} (-2b^2s) = \\frac{2b^2s}{(1-b^2s^2)^2}$.\n$$\nc^* = \\frac{M'(s^*)}{M(s^*)} = \\frac{2b^2s^*/(1-b^2(s^*)^2)^2}{1/(1-b^2(s^*)^2)} = \\frac{2b^2s^*}{1-b^2(s^*)^2}\n$$\nUsing $Y = 1-b^2s^2$ and $s^*=\\sqrt{1-Y}/b$ (since $s0$):\n$$\nc^* = \\frac{2b^2 (\\sqrt{1-Y}/b)}{Y} = \\frac{2b\\sqrt{1-Y}}{Y}\n$$\nSubstituting $Y = -2/W^*$:\n$$\nc^* = \\frac{2b\\sqrt{1 - (-2/W^*)}}{-2/W^*} = -bW^*\\sqrt{1+2/W^*} = -bW^*\\sqrt{\\frac{W^*+2}{W^*}}\n$$\nSince $W^*  -2$, both $W^*$ and $W^*+2$ are negative. Their ratio is positive. The square root is real and can be written as $\\sqrt{-(W^*+2)}/\\sqrt{-W^*}$.\n$$\nc^* = -b W^* \\frac{\\sqrt{-(W^*+2)}}{\\sqrt{-W^*}} = -b(-|W^*|) \\frac{\\sqrt{-(W^*+2)}}{\\sqrt{|W^*|}} = b|W^*|\\frac{\\sqrt{-(W^*+2)}}{\\sqrt{|W^*|}}\n$$\n$$\nc^* = b\\sqrt{|W^*|} \\sqrt{-(W^*+2)} = b\\sqrt{-W^*} \\sqrt{-(W^*+2)} = b\\sqrt{(-W^*)(-(W^*+2))}\n$$\n$$\nc^* = b\\sqrt{W^*(W^*+2)}\n$$\nSubstituting back the full expression for $W^*$, we get the final result.", "answer": "$$\\boxed{b \\sqrt{W_{-1}\\left(-2R^{-1}\\exp(-2)\\right) \\left( W_{-1}\\left(-2R^{-1}\\exp(-2)\\right) + 2 \\right)}}$$", "id": "2480511"}, {"introduction": "Moving from one-dimensional theory to empirical data brings new challenges, as dispersal in real landscapes is often anisotropic—that is, not uniform in all directions. This hands-on computational exercise guides you through the process of fitting a two-dimensional anisotropic Gaussian kernel to simulated displacement data. You will implement a complete statistical workflow, from estimating the kernel's parameters via maximum likelihood to formally testing the hypothesis of isotropy using a likelihood ratio test. [@problem_id:2480590]", "problem": "You are studying two-dimensional animal displacement data in movement ecology, where one-step displacements are modeled by a Gaussian dispersal kernel. Assume that each observed displacement vector is an independent draw from a multivariate normal distribution with unknown mean and unknown covariance. You are to fit an anisotropic Gaussian kernel by maximum likelihood and then test the hypothesis of isotropy.\n\nFundamental base and definitions to be used:\n- The multivariate normal density in dimension $d$ with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ is a well-tested model for dispersal steps. Its likelihood for independent observations follows from the product of densities and its log-likelihood is additive.\n- For independent samples from a multivariate normal distribution, the maximum likelihood estimators for the mean and covariance exist and are obtained by optimizing the log-likelihood under positive definiteness constraints. The Likelihood Ratio Test (LRT) uses the ratio of maximized likelihoods under a null hypothesis and an alternative hypothesis, and Wilks's theorem provides the asymptotic null distribution of the test statistic.\n\nMathematical setup:\n- You observe $n$ two-dimensional displacement vectors $\\{\\mathbf{x}_i\\}_{i=1}^n$ with $\\mathbf{x}_i \\in \\mathbb{R}^2$, assumed independent and identically distributed as $\\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n- The alternative hypothesis allows an unconstrained symmetric positive definite covariance $\\boldsymbol{\\Sigma}$, representing an anisotropic Gaussian dispersal kernel.\n- The null hypothesis of isotropy constrains the covariance to be a scalar multiple of the identity, $\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}_2$.\n\nTasks:\n1. Given simulated data sets (specified below), estimate the mean $\\boldsymbol{\\mu}$ and the covariance matrix $\\boldsymbol{\\Sigma}$ by maximum likelihood under the alternative. Separately, estimate $\\boldsymbol{\\mu}$ and $\\sigma^2$ by maximum likelihood under the null.\n2. Compute the Likelihood Ratio Test statistic $T = 2\\big(\\ell_{\\text{alt}} - \\ell_{\\text{null}}\\big)$, where $\\ell_{\\text{alt}}$ and $\\ell_{\\text{null}}$ denote the maximized log-likelihoods under the alternative and null models, respectively. Under the null hypothesis, and for large $n$, $T$ is approximately $\\chi^2_k$ distributed with $k$ equal to the difference in the number of free covariance parameters between the two models. In two dimensions, $k = 2$.\n3. Compute the $p$-value for each data set as the upper tail probability of the $\\chi^2_k$ distribution evaluated at the observed $T$.\n\nAngle unit requirement:\n- Any rotation angle introduced below is in radians.\n\nUnits:\n- There are no physical units to report. All outputs are unitless probabilities.\n\nTest suite (data-generating mechanisms to be used by your program; each data set is generated by a fixed random seed for reproducibility):\nAll data are two-dimensional ($d = 2$), with zero-mean truth $\\boldsymbol{\\mu} = \\mathbf{0}$, and sample sizes and covariance structures as follows. In each case, draw $n$ samples from $\\mathcal{N}_2(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\text{true}})$ using the specified seed.\n\n- Case A (isotropic, large sample): $n = 500$, seed $= 17$, $\\boldsymbol{\\Sigma}_{\\text{true}} = 4 \\mathbf{I}_2$.\n- Case B (axis-aligned anisotropy, large sample): $n = 500$, seed $= 29$, $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathrm{diag}(9, 1)$.\n- Case C (rotated anisotropy, large sample): $n = 500$, seed $= 97$. Let $\\theta = \\pi/6$ (in radians). Define the rotation matrix $\\mathbf{R}(\\theta) = \\begin{pmatrix}\\cos \\theta  -\\sin \\theta \\\\ \\sin \\theta  \\cos \\theta\\end{pmatrix}$ and the axis-aligned covariance $\\mathbf{D} = \\mathrm{diag}(4, 1)$. Set $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathbf{R}(\\theta)\\,\\mathbf{D}\\,\\mathbf{R}(\\theta)^\\top$.\n- Case D (mild anisotropy, moderate sample): $n = 200$, seed $= 1234$, $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathrm{diag}(1.2, 0.8)$.\n- Case E (isotropic, smaller sample): $n = 50$, seed $= 2021$, $\\boldsymbol{\\Sigma}_{\\text{true}} = \\mathbf{I}_2$.\n\nImplementation and output requirements:\n- Your program must internally generate the data sets exactly as specified using the seeds above.\n- Use only the principle-based maximum likelihood estimators derived from the multivariate normal model, not ad hoc heuristics.\n- For each case, compute the $p$-value for the isotropy test based on the $\\chi^2_2$ reference distribution applied to the Likelihood Ratio Test statistic $T$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be the five $p$-values corresponding to Cases A through E, each rounded to exactly six digits after the decimal point, in order A, B, C, D, E. For example, an output with placeholder values would look like \"[0.123456,0.000321,0.543210,0.010000,0.999999]\".", "solution": "The problem requires performing a statistical test for isotropy of a two-dimensional Gaussian dispersal kernel. We are given $n$ independent and identically distributed displacement vectors $\\{\\mathbf{x}_i\\}_{i=1}^n$, where each $\\mathbf{x}_i \\in \\mathbb{R}^2$ is assumed to be a sample from a multivariate normal distribution $\\mathcal{N}_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. The analysis will proceed by deriving the Maximum Likelihood Estimators (MLEs) for the model parameters under two competing hypotheses and then constructing a Likelihood Ratio Test (LRT).\n\nThe two hypotheses are as follows:\nThe null hypothesis, $H_0$, posits an isotropic dispersal kernel. This constrains the covariance matrix to be a scalar multiple of the identity matrix:\n$$ H_0: \\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}_2 $$\nwhere $\\sigma^2 > 0$ is a single variance parameter and $\\mathbf{I}_2$ is the $2 \\times 2$ identity matrix.\n\nThe alternative hypothesis, $H_A$, allows for anisotropy. This means the covariance matrix $\\boldsymbol{\\Sigma}$ is an unconstrained symmetric positive definite matrix:\n$$ H_A: \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_{11}  \\sigma_{12} \\\\ \\sigma_{21}  \\sigma_{22} \\end{pmatrix}, \\text{ with } \\sigma_{12} = \\sigma_{21} \\text{ and } \\boldsymbol{\\Sigma} \\succ 0 $$\n\nThe log-likelihood function for a sample of $n$ observations from $\\mathcal{N}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, with dimension $d=2$, is:\n$$ \\ell(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{n}{2}\\log|\\boldsymbol{\\Sigma}| - \\frac{1}{2}\\sum_{i=1}^n (\\mathbf{x}_i - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}) $$\n\nFirst, we find the MLEs under the alternative hypothesis, $H_A$. The parameters to estimate are $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$. Standard results from multivariate statistics show that the MLEs are the sample mean and the sample covariance matrix (with divisor $n$):\n$$ \\hat{\\boldsymbol{\\mu}}_{\\text{alt}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i $$\n$$ \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}} = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})^\\top $$\nSubstituting these estimators into the log-likelihood function, the summation term simplifies due to a property of the trace operator:\n$$ \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})^\\top \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}^{-1} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}}) = \\mathrm{tr}\\left(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}^{-1} \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}_{\\text{alt}})^\\top\\right) = \\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}^{-1} n \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) = n \\, \\mathrm{tr}(\\mathbf{I}_d) = nd $$\nThe maximized log-likelihood under $H_A$ is therefore:\n$$ \\ell_{\\text{alt}} = \\ell(\\hat{\\boldsymbol{\\mu}}_{\\text{alt}}, \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{n}{2}\\log|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}| - \\frac{nd}{2} $$\n\nNext, we find the MLEs under the null hypothesis, $H_0$. The parameters are $\\boldsymbol{\\mu}$ and $\\sigma^2$. The log-likelihood function under this constraint is:\n$$ \\ell(\\boldsymbol{\\mu}, \\sigma^2 | H_0) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{nd}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\|\\mathbf{x}_i - \\boldsymbol{\\mu}\\|^2_2 $$\nThe MLE for the mean is again the sample mean, $\\hat{\\boldsymbol{\\mu}}_{\\text{null}} = \\hat{\\boldsymbol{\\mu}}_{\\text{alt}}$. Let us denote it by $\\hat{\\boldsymbol{\\mu}}$. To find the MLE for $\\sigma^2$, we differentiate the log-likelihood with respect to $\\sigma^2$ and set the result to zero, yielding:\n$$ \\hat{\\sigma}^2_{\\text{null}} = \\frac{1}{nd} \\sum_{i=1}^n \\|\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}\\|^2_2 $$\nThe sum of squares can be related to the trace of the unconstrained sample covariance matrix:\n$$ \\sum_{i=1}^n \\|\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}\\|^2_2 = \\sum_{i=1}^n \\mathrm{tr}\\left((\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^\\top\\right) = \\mathrm{tr}\\left( \\sum_{i=1}^n (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}})^\\top \\right) = \\mathrm{tr}(n \\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) $$\nThus, the MLE for $\\sigma^2$ under $H_0$ is the average of the total variance across dimensions:\n$$ \\hat{\\sigma}^2_{\\text{null}} = \\frac{1}{d} \\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}) $$\nSubstituting these estimators into the null log-likelihood, the summation term becomes $\\frac{1}{2\\hat{\\sigma}^2_{\\text{null}}} (nd \\hat{\\sigma}^2_{\\text{null}}) = \\frac{nd}{2}$. The maximized log-likelihood under $H_0$ is:\n$$ \\ell_{\\text{null}} = \\ell(\\hat{\\boldsymbol{\\mu}}, \\hat{\\sigma}^2_{\\text{null}}) = -\\frac{nd}{2}\\log(2\\pi) - \\frac{nd}{2}\\log(\\hat{\\sigma}^2_{\\text{null}}) - \\frac{nd}{2} $$\n\nThe Likelihood Ratio Test statistic, $T$, is defined as twice the difference in the maximized log-likelihoods:\n$$ T = 2(\\ell_{\\text{alt}} - \\ell_{\\text{null}}) = 2 \\left( \\left(-\\frac{n}{2}\\log|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}|\\right) - \\left(-\\frac{nd}{2}\\log(\\hat{\\sigma}^2_{\\text{null}})\\right) \\right) $$\n$$ T = n \\left( d\\log(\\hat{\\sigma}^2_{\\text{null}}) - \\log|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}| \\right) = n \\log\\left( \\frac{(\\hat{\\sigma}^2_{\\text{null}})^d}{|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}|} \\right) $$\nSubstituting $\\hat{\\sigma}^2_{\\text{null}} = \\frac{1}{d}\\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}})$ and setting $d=2$:\n$$ T = n \\log\\left( \\frac{\\left(\\frac{1}{2}\\mathrm{tr}(\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}})\\right)^2}{|\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}|} \\right) $$\nThis statistic compares the squared arithmetic mean of the eigenvalues of $\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}$ with their geometric mean (the determinant).\n\nAccording to Wilks's theorem, for large $n$, $T$ is asymptotically distributed as a chi-squared random variable under the null hypothesis. The number of degrees of freedom, $k$, is the difference in the number of free parameters in the covariance matrix between the alternative and null models. For $d=2$, $H_A$ has $3$ free covariance parameters ($\\sigma_{11}, \\sigma_{22}, \\sigma_{12}$), while $H_0$ has $1$ ($\\sigma^2$). Therefore, $k = 3 - 1 = 2$.\nSo, $T \\sim \\chi^2_2$ under $H_0$.\n\nThe procedure for each test case is:\n$1$. Generate $n$ data points from the specified true distribution $\\mathcal{N}_2(\\mathbf{0}, \\boldsymbol{\\Sigma}_{\\text{true}})$.\n$2$. Compute the sample mean $\\hat{\\boldsymbol{\\mu}}$ and the MLE of the covariance matrix $\\hat{\\boldsymbol{\\Sigma}}_{\\text{alt}}$.\n$3$. Calculate the LRT statistic $T$ using the derived formula.\n$4$. Compute the $p$-value as the upper-tail probability of the $\\chi^2_2$ distribution, i.e., $P(X \\ge T)$ where $X \\sim \\chi^2_2$. This is calculated using the survival function of the chi-squared distribution.\nA small $p$-value (e.g., $ 0.05$) provides evidence to reject the null hypothesis of isotropy in favor of the anisotropic alternative.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs a Likelihood Ratio Test for isotropy on simulated 2D displacement data.\n\n    For each test case, the function:\n    1. Generates a dataset from a bivariate normal distribution.\n    2. Computes the maximum likelihood estimates for parameters under the\n       anisotropic (alternative) and isotropic (null) hypotheses.\n    3. Calculates the Likelihood Ratio Test statistic.\n    4. Computes the p-value using the asymptotic chi-squared distribution.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Case C: Define the rotation and covariance matrix for rotated anisotropy.\n    theta = np.pi / 6\n    R_theta = np.array([\n        [np.cos(theta), -np.sin(theta)],\n        [np.sin(theta), np.cos(theta)]\n    ])\n    D_C = np.diag([4, 1])\n    Sigma_C = R_theta @ D_C @ R_theta.T\n\n    test_cases = [\n        {'n': 500, 'seed': 17, 'Sigma_true': np.array([[4, 0], [0, 4]]), 'case': 'A'},\n        {'n': 500, 'seed': 29, 'Sigma_true': np.array([[9, 0], [0, 1]]), 'case': 'B'},\n        {'n': 500, 'seed': 97, 'Sigma_true': Sigma_C, 'case': 'C'},\n        {'n': 200, 'seed': 1234, 'Sigma_true': np.array([[1.2, 0], [0, 0.8]]), 'case': 'D'},\n        {'n': 50, 'seed': 2021, 'Sigma_true': np.eye(2), 'case': 'E'}\n    ]\n\n    p_values = []\n    \n    # Dimension of the data\n    d = 2\n\n    for case in test_cases:\n        n = case['n']\n        seed = case['seed']\n        Sigma_true = case['Sigma_true']\n\n        # 1. Generate n samples from N_2(0, Sigma_true)\n        rng = np.random.default_rng(seed)\n        mean_true = np.zeros(d)\n        data = rng.multivariate_normal(mean_true, Sigma_true, size=n)\n\n        # 2. Estimate parameters by maximum likelihood.\n        # Although the true mean is 0, the model assumes an unknown mean.\n        # MLE for mu is the sample mean for both hypotheses.\n        mu_hat = np.mean(data, axis=0)\n\n        # MLE for covariance under the alternative hypothesis (anisotropic)\n        # np.cov with bias=True divides by n, which is the MLE.\n        Sigma_hat_alt = np.cov(data, rowvar=False, bias=True)\n\n        # 3. Calculate the Likelihood Ratio Test statistic T.\n        # T = n * log( (tr(Sigma_hat)/d)^d / det(Sigma_hat) )\n        det_Sigma_hat = np.linalg.det(Sigma_hat_alt)\n        tr_Sigma_hat = np.trace(Sigma_hat_alt)\n\n        # Handle potential numerical issues if determinant is\n        # non-positive, although unlikely with generated data.\n        if det_Sigma_hat = 0:\n            # If Sigma_hat is not positive definite, the likelihood is not\n            # well-defined. This can happen with degenerate data.\n            # In such a case, the test is not applicable. For this problem,\n            # we can assume it will be positive.\n            # We can use a very large T to yield a p-value of 0.\n            T_statistic = np.inf\n        else:\n            # MLE for variance under the null hypothesis (isotropic)\n            sigma_sq_hat_null = tr_Sigma_hat / d\n            \n            # The argument to the logarithm is the ratio of the arithmetic mean\n            # to the geometric mean of the eigenvalues of Sigma_hat_alt, raised to power d.\n            # By AM-GM inequality, this is = 1.\n            log_argument = (sigma_sq_hat_null**d) / det_Sigma_hat\n            T_statistic = n * np.log(log_argument)\n        \n        # 4. Compute the p-value.\n        # The test statistic T is asymptotically chi-squared distributed with\n        # k = (d*(d+1)/2 - 1) degrees of freedom. For d=2, k=2.\n        k = d * (d + 1) // 2 - 1\n        p_value = chi2.sf(T_statistic, df=k)\n        \n        p_values.append(f\"{p_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(p_values)}]\")\n\nsolve()\n\n```", "id": "2480590"}]}