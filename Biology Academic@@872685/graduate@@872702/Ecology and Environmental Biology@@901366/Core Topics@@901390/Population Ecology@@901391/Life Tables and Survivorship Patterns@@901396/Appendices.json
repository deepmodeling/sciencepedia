{"hands_on_practices": [{"introduction": "A life table's primary function is to summarize age-specific mortality and survival, but its true power lies in deriving key population metrics. This first practice challenges you to connect the fundamental definition of an expected value to the survivorship column ($l_x$) of a discrete life table. By deriving and calculating the life expectancy at birth, $e(0)$, you will solidify your understanding of how raw survival probabilities translate into a crucial summary of an organism's life history [@problem_id:2503617].", "problem": "A closed cohort of organisms is observed in discrete time with unit-length intervals (the interval is the basic time-step). Let the random variable $T \\in \\{0,1,2,\\dots\\}$ denote the number of completed time-steps lived by a randomly chosen newborn from this cohort. The cohort survivorship sequence at integer ages is given (as proportions of the initial cohort) by $l_x$ at ages $x \\in \\{0,1,2,3,4,5,6,7\\}$, where, by construction in this discrete model, $l_x = \\Pr(T  x)$. The observed values are\n$$\nl_x = \\{1.0,\\, 0.9,\\, 0.72,\\, 0.54,\\, 0.43,\\, 0.30,\\, 0.10,\\, 0.00\\}.\n$$\nStarting from the fundamental definition of an expected value for a nonnegative integer-valued random variable, derive an expression for the life expectancy at birth $e(0)$ (the expected number of completed time-steps lived by a newborn) in terms of the survivorship sequence $\\{l_x\\}_{x \\ge 0}$ without assuming any within-interval mortality model. Then evaluate $e(0)$ numerically using the data above. Round your final numerical answer to $3$ significant figures. Express the result in time-steps in your interpretation, but report only the numerical value as your final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a unique solution. It is a standard problem in demography and actuarial science. The problem is valid. We will proceed with the derivation and calculation.\n\nThe task is to first derive an expression for the life expectancy at birth, denoted $e(0)$, for a discrete-time model and then to compute its value. The life expectancy $e(0)$ is the expected value of the random variable $T$, where $T \\in \\{0, 1, 2, \\ldots\\}$ represents the number of completed time-steps lived by a newborn.\n\nThe derivation must begin from the fundamental definition of the expected value of a non-negative, integer-valued random variable. This definition is:\n$$\ne(0) = E[T] = \\sum_{k=0}^{\\infty} k \\cdot \\Pr(T=k)\n$$\nSince the term for $k=0$ is zero, the sum can start from $k=1$:\n$$\nE[T] = \\sum_{k=1}^{\\infty} k \\cdot \\Pr(T=k)\n$$\nThe integer $k$ can be expressed as a sum of ones: $k = \\sum_{j=1}^{k} 1$. Substituting this into the expectation formula gives:\n$$\nE[T] = \\sum_{k=1}^{\\infty} \\left( \\sum_{j=1}^{k} 1 \\right) \\Pr(T=k)\n$$\nThis is a double summation over the region defined by the integers $(j, k)$ satisfying $1 \\le j \\le k$. We can interchange the order of summation. The condition $1 \\le j \\le k$ is equivalent to $j \\ge 1$ and $k \\ge j$. Summing first over $k$ for a fixed $j$, and then over all possible values of $j$, we get:\n$$\nE[T] = \\sum_{j=1}^{\\infty} \\sum_{k=j}^{\\infty} \\Pr(T=k)\n$$\nThe inner sum, $\\sum_{k=j}^{\\infty} \\Pr(T=k)$, represents the sum of probabilities of all outcomes where the number of completed time-steps is at least $j$. This is, by definition, the probability of the event $\\{T \\ge j\\}$:\n$$\n\\sum_{k=j}^{\\infty} \\Pr(T=k) = \\Pr(T \\ge j)\n$$\nSubstituting this back into the expression for $E[T]$ yields:\n$$\nE[T] = \\sum_{j=1}^{\\infty} \\Pr(T \\ge j)\n$$\nThe problem defines the survivorship sequence as $l_x = \\Pr(T  x)$. For an integer-valued random variable like $T$, the event $\\{T \\ge j\\}$ (living at least $j$ full time-steps) is identical to the event $\\{T  j-1\\}$ (living more than $j-1$ time-steps). Therefore:\n$$\n\\Pr(T \\ge j) = \\Pr(T  j-1) = l_{j-1}\n$$\nWe substitute this relationship into our expression for the expectation:\n$$\nE[T] = \\sum_{j=1}^{\\infty} l_{j-1}\n$$\nTo make the notation consistent with the provided data index $x$, we perform a change of index in the summation. Let $x = j-1$. As the index $j$ ranges from $1$ to $\\infty$, the index $x$ ranges from $0$ to $\\infty$. The final derived expression for life expectancy at birth is:\n$$\ne(0) = \\sum_{x=0}^{\\infty} l_x\n$$\nThis derivation, known as the tail-sum formula, starts from the fundamental definition of expectation and relies on no assumptions about mortality patterns within the time intervals. This completes the first part of the problem.\n\nFor the second part, we must evaluate $e(0)$ numerically using the provided data for $l_x$:\n$$\nl_x = \\{1.0, 0.9, 0.72, 0.54, 0.43, 0.30, 0.10, 0.00\\}\n$$\nfor $x = 0, 1, 2, 3, 4, 5, 6, 7$ respectively.\nThe given data includes $l_7 = \\Pr(T  7) = 0.00$. This implies that the probability of surviving more than $7$ time-steps is zero. Consequently, for all integers $x \\ge 7$, $l_x=0$. This makes the infinite sum for $e(0)$ a finite sum:\n$$\ne(0) = \\sum_{x=0}^{\\infty} l_x = l_0 + l_1 + l_2 + l_3 + l_4 + l_5 + l_6 + \\sum_{x=7}^{\\infty} l_x\n$$\nSince all terms for $x \\ge 7$ are zero, the sum is:\n$$\ne(0) = l_0 + l_1 + l_2 + l_3 + l_4 + l_5 + l_6\n$$\nSubstituting the numerical values:\n$$\ne(0) = 1.0 + 0.9 + 0.72 + 0.54 + 0.43 + 0.30 + 0.10\n$$\nPerforming the summation:\n$$\ne(0) = 1.9 + 0.72 + 0.54 + 0.43 + 0.30 + 0.10\n$$\n$$\ne(0) = 2.62 + 0.54 + 0.43 + 0.30 + 0.10\n$$\n$$\ne(0) = 3.16 + 0.43 + 0.30 + 0.10\n$$\n$$\ne(0) = 3.59 + 0.30 + 0.10\n$$\n$$\ne(0) = 3.89 + 0.10\n$$\n$$\ne(0) = 3.99\n$$\nThe calculated life expectancy at birth is $3.99$ time-steps. The problem requires the answer to be rounded to $3$ significant figures. The value $3.99$ already has exactly three significant figures.", "answer": "$$\n\\boxed{3.99}\n$$", "id": "2503617"}, {"introduction": "The source of your data fundamentally shapes the conclusions you can draw. This practice explores a critical distinction between cohort life tables and static (cross-sectional) life tables, a common source of data in human demography and ecology. By deriving the mathematical relationship between the two under stable population growth, you will uncover the systematic bias that arises in a growing population and learn why a static life table does not represent the true survivorship experience of any single cohort [@problem_id:2503649].", "problem": "A demographer studies a species with discrete, non-overlapping annual age classes $x \\in \\{0,1,2,\\dots,\\omega\\}$ in a large, density-independent, environmentally constant population that has converged to a stable age distribution with constant finite rate of increase $\\lambda0$ (so that the per-time-step population size multiplies by $\\lambda$). Let the cohort survivorship schedule $l_x$ be the probability that a newborn survives to exact age $x$, with $l_0=1$. The demographer constructs a static (cross-sectional) life table at census time $t$ by counting the number $n_x(t)$ of individuals of each age $x$, and then estimates survivorship by normalizing counts so that $\\hat{l}_0^{\\mathrm{static}}=1$ and, for $x0$, $\\hat{l}_x^{\\mathrm{static}} \\propto n_x(t)$.\n\nStarting only from the definitions of cohort survivorship $l_x$, stable exponential growth with rate $\\lambda$, and the fact that the number of age-$x$ individuals at time $t$ equals the number of births that occurred at time $t-x$ times the probability of surviving from birth to age $x$, derive how $\\hat{l}_x^{\\mathrm{static}}$ relates to $l_x$ and $\\lambda$. Use your derivation to determine the direction of bias in $\\hat{l}_x^{\\mathrm{static}}$ when $\\lambda1$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. In a stable population with constant $\\lambda1$, a static life table that estimates $\\hat{l}_x^{\\mathrm{static}}$ by normalizing $n_x(t)$ so that $\\hat{l}_0^{\\mathrm{static}}=1$ yields $\\hat{l}_x^{\\mathrm{static}}=l_x\\,\\lambda^{-x}$; consequently, it underestimates $l_x$ for all $x0$.\n\nB. When $\\lambda1$, a static life table overestimates $l_x$ at larger ages $x$ because older individuals are overrepresented in the cross-section relative to a single cohort.\n\nC. If $\\lambda=1$, the static estimate is unbiased for all ages $x$, that is, $\\hat{l}_x^{\\mathrm{static}}=l_x$.\n\nD. For any $\\lambda\\neq 1$, the ratio $\\hat{l}_x^{\\mathrm{static}}/l_x$ is a constant independent of age $x$.\n\nE. The sign (direction) of the bias for $\\lambda1$ depends on the age-specific fertility schedule $b_x$; with sufficiently increasing $b_x$, the static life table can be unbiased even if $\\lambda1$.", "solution": "The problem statement will first be subjected to rigorous validation.\n\nStep 1: Extract Givens\n- Age classes are discrete, non-overlapping, and annual, denoted by $x \\in \\{0, 1, 2, \\dots, \\omega\\}$.\n- The population is large, density-independent, and in a constant environment.\n- The population has converged to a stable age distribution.\n- The population experiences a constant finite rate of increase $\\lambda  0$.\n- Cohort survivorship, $l_x$, is the probability that a newborn survives to exact age $x$, with the normalization $l_0 = 1$.\n- A static (cross-sectional) life table is constructed at census time $t$ by counting the number of individuals of each age, $n_x(t)$.\n- The static survivorship estimate, $\\hat{l}_x^{\\mathrm{static}}$, is defined by normalizing counts such that $\\hat{l}_0^{\\mathrm{static}} = 1$ and for $x  0$, $\\hat{l}_x^{\\mathrm{static}} \\propto n_x(t)$.\n- A key premise is given: \"the number of age-$x$ individuals at time $t$ equals the number of births that occurred at time $t-x$ times the probability of surviving from birth to age $x$.\"\n\nStep 2: Validate Using Extracted Givens\nThe problem is a standard exercise in mathematical demography, concerned with the relationship between cohort and static life tables.\n- **Scientifically Grounded:** The concepts of stable age distribution, survivorship schedules ($l_x$), and the finite rate of increase ($\\lambda$) are fundamental pillars of population ecology and demography, established by Lotka, Volterra, and others. The premises are sound.\n- **Well-Posed:** The problem provides sufficient definitions and a clear directive to derive a specific relationship and analyze its implications. The question is unambiguous and a unique solution can be derived from the premises.\n- **Objective:** The language is formal, precise, and devoid of subjectivity.\nThe problem statement is internally consistent, complete for the task at hand, and grounded in established scientific theory. There are no identifiable flaws.\n\nStep 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation and evaluation.\n\nDerivation of the Relationship\nLet $B(t)$ represent the number of births (individuals of age $0$ before mortality) at the beginning of time interval $t$. Since the population is growing at a constant finite rate $\\lambda$, the number of births must also increase exponentially at this rate. We can write the number of births at time $t$ in terms of the number of births at some reference time, say $t=0$, as:\n$$B(t) = B(0)\\lambda^t$$\nThe number of individuals of exact age $x$ at a census time $t$, denoted $n_x(t)$, consists of those individuals who were born at time $t-x$ and have survived to age $x$. The probability of surviving from birth to age $x$ is given by the cohort survivorship, $l_x$. Therefore, as stated in the problem:\n$$n_x(t) = B(t-x) \\cdot l_x$$\nWe can express $B(t-x)$ in terms of the number of births at the current time, $B(t)$:\n$$B(t-x) = B(0)\\lambda^{t-x} = B(0)\\lambda^t \\lambda^{-x} = B(t) \\lambda^{-x}$$\nSubstituting this into the expression for $n_x(t)$, we get:\n$$n_x(t) = B(t) \\lambda^{-x} l_x$$\nThe problem states that the static life table survivorship, $\\hat{l}_x^{\\mathrm{static}}$, is estimated by normalizing the age counts $n_x(t)$ such that $\\hat{l}_0^{\\mathrm{static}} = 1$. The general form of this normalization is $\\hat{l}_x^{\\mathrm{static}} = \\frac{n_x(t)}{n_0(t)}$. Let us verify this satisfies the conditions.\nFor $x=0$, we have:\n$$\\hat{l}_0^{\\mathrm{static}} = \\frac{n_0(t)}{n_0(t)} = 1$$\nThis matches the condition $\\hat{l}_0^{\\mathrm{static}} = 1$. For $x0$, this formulation means $\\hat{l}_x^{\\mathrm{static}} \\propto n_x(t)$ with a constant of proportionality $1/n_0(t)$, which is constant with respect to $x$ for a given census time $t$. So, this is the correct formulation.\n\nNow we substitute our derived expression for $n_x(t)$:\n$$\\hat{l}_x^{\\mathrm{static}} = \\frac{n_x(t)}{n_0(t)} = \\frac{B(t) \\lambda^{-x} l_x}{B(t) \\lambda^{-0} l_0}$$\nGiven that $l_0 = 1$ and $\\lambda^0 = 1$, the denominator is simply $B(t)$.\n$$\\hat{l}_x^{\\mathrm{static}} = \\frac{B(t) \\lambda^{-x} l_x}{B(t)} = l_x \\lambda^{-x}$$\nThis is the fundamental relationship between the static life table estimate and the true cohort survivorship in a population with a stable age distribution growing at rate $\\lambda$.\n\nAnalysis of Bias for $\\lambda  1$\nThe bias is the difference between the estimate and the true value, or can be analyzed via their ratio. The ratio is:\n$$\\frac{\\hat{l}_x^{\\mathrm{static}}}{l_x} = \\lambda^{-x}$$\nIf $\\lambda  1$, then for any age $x  0$, we have $\\lambda^x  1$. Consequently, $\\lambda^{-x} = \\frac{1}{\\lambda^x}  1$.\nThis implies that $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x}  l_x$ for all $x  0$.\nTherefore, for a growing population ($\\lambda  1$), the static life table systematically **underestimates** the true cohort survivorship for all positive ages. The intuition is that a cross-section of a growing population over-represents the younger age classes (which come from larger, more recent birth cohorts) relative to the older age classes (which come from smaller, past birth cohorts). When normalizing by the number of individuals at age $0$ (the largest group), the relative numbers in all older age classes appear smaller, leading to an apparent survivorship that is lower than the true cohort survivorship.\n\nEvaluation of Options\n\nA. In a stable population with constant $\\lambda1$, a static life table that estimates $\\hat{l}_x^{\\mathrm{static}}$ by normalizing $n_x(t)$ so that $\\hat{l}_0^{\\mathrm{static}}=1$ yields $\\hat{l}_x^{\\mathrm{static}}=l_x\\,\\lambda^{-x}$; consequently, it underestimates $l_x$ for all $x0$.\nThis statement is a direct consequence of our derivation. The formula $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x}$ is what we derived. For $\\lambda  1$ and $x  0$, $\\lambda^{-x}  1$, which means $\\hat{l}_x^{\\mathrm{static}}  l_x$, an underestimation. This statement is entirely correct.\n**Verdict: Correct**\n\nB. When $\\lambda1$, a static life table overestimates $l_x$ at larger ages $x$ because older individuals are overrepresented in the cross-section relative to a single cohort.\nThis is incorrect on two counts. First, as derived, for $\\lambda  1$ the static method *underestimates* $l_x$, it does not overestimate it. Second, the reasoning is false. In a growing population, it is the younger individuals from larger, recent cohorts who are overrepresented, not the older individuals from smaller, past cohorts.\n**Verdict: Incorrect**\n\nC. If $\\lambda=1$, the static estimate is unbiased for all ages $x$, that is, $\\hat{l}_x^{\\mathrm{static}}=l_x$.\nWe use our derived general formula $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x}$ and substitute $\\lambda=1$:\n$$\\hat{l}_x^{\\mathrm{static}} = l_x (1)^{-x} = l_x \\cdot 1 = l_x$$\nIn a stationary population ($\\lambda=1$), the static estimate is identical to the cohort survivorship. It is unbiased. This is because the size of birth cohorts is constant over time, so the age structure is directly proportional to the survivorship curve.\n**Verdict: Correct**\n\nD. For any $\\lambda\\neq 1$, the ratio $\\hat{l}_x^{\\mathrm{static}}/l_x$ is a constant independent of age $x$.\nThe ratio is $\\frac{\\hat{l}_x^{\\mathrm{static}}}{l_x} = \\lambda^{-x}$. As $x$ increases, this ratio changes (it decreases for $\\lambda  1$ and increases for $0  \\lambda  1$). It is a function of age $x$ and is not a constant. The bias is age-dependent.\n**Verdict: Incorrect**\n\nE. The sign (direction) of the bias for $\\lambda1$ depends on the age-specific fertility schedule $b_x$; with sufficiently increasing $b_x$, the static life table can be unbiased even if $\\lambda1$.\nThe age-specific fertility schedule $b_x$ and survivorship $l_x$ together determine the value of $\\lambda$ through the Euler-Lotka characteristic equation ($1 = \\sum_x \\lambda^{-x} l_x b_x$). However, once the value of $\\lambda  1$ is established and the population has reached a stable age distribution, the relationship we derived, $\\hat{l}_x^{\\mathrm{static}} = l_x \\lambda^{-x}$, holds irrespective of the specific $b_x$ schedule that gave rise to that $\\lambda$. The bias is determined by $\\lambda$, not directly by $b_x$. For any $\\lambda  1$, there will be an underestimation. It is not possible for the estimate to be unbiased.\n**Verdict: Incorrect**", "answer": "$$\\boxed{AC}$$", "id": "2503649"}, {"introduction": "In modern ecological research, we often work directly with individual survival data, which may be incomplete due to right-censoring. This advanced practice moves beyond simple life table columns to the flexible framework of parametric survival modeling. You will implement the statistical engine—the likelihood function for censored data—to fit and compare classic survival models, using the Akaike Information Criterion ($AIC$) to select the most plausible description of the underlying survivorship pattern [@problem_id:2503639].", "problem": "You are given three right-censored cohort datasets of survival times measured in years, each representing a hypothetical species with different survivorship patterns. Your task is to implement maximum likelihood estimation for three parametric hazard models and perform model selection via the Akaike Information Criterion (AIC).\n\nFundamental base and definitions to be used:\n- The hazard function $h(t)$ is defined as $h(t) = \\lim_{\\Delta t \\to 0^{+}} \\dfrac{\\mathbb{P}(t \\le T  t + \\Delta t \\mid T \\ge t)}{\\Delta t}$, where $T$ is the non-negative random time-to-event.\n- The survival function $S(t)$ is $S(t) = \\mathbb{P}(T \\ge t)$ and satisfies $S(t) = \\exp\\!\\left(-\\int_{0}^{t} h(u)\\,du\\right)$.\n- The density function is $f(t) = -\\dfrac{d}{dt} S(t) = h(t)\\,S(t)$ for $t  0$.\n- For right-censored observations at time $c$, only the information that $T \\ge c$ is observed. You must incorporate this censoring correctly in the likelihood using the definitions above.\n\nModels to be fitted to each dataset:\n- Exponential hazard model with constant hazard $h(t)$.\n- Weibull hazard model with a monotone hazard depending on time $t$.\n- Gompertz hazard model with an exponentially changing hazard in $t$.\n\nFor each dataset, you must:\n1. Derive and implement the log-likelihood under right-censoring using only the definitions above.\n2. Maximize the log-likelihood for each model to obtain the maximum likelihood estimates of the model parameters. Use numerically stable parameterizations suitable for constrained parameters (for example, parameters restricted to be positive) and handle limiting cases correctly (for example, the limiting case that reconciles an exponentially changing hazard with a constant hazard).\n3. Compute the Akaike Information Criterion $AIC = 2p - 2\\widehat{\\ell}$, where $p$ is the number of free parameters in the model and $\\widehat{\\ell}$ is the maximized log-likelihood.\n4. Select the model with the minimum $AIC$ for each dataset.\n\nRight-censoring is present as indicated by an event indicator array. For each observation $i$, $d_i = 1$ indicates an observed event (death), and $d_i = 0$ indicates a right-censored time.\n\nUse the following test suite (all times are in years):\n- Test case $1$ (moderate sample, mixed events and censoring, approximately constant hazard):\n  - Times $t$: $[\\,0.2,\\,0.6,\\,1.0,\\,1.4,\\,2.1,\\,2.5,\\,3.0,\\,3.7,\\,4.2,\\,4.9,\\,5.5,\\,6.1,\\,6.8,\\,7.4,\\,8.0,\\,9.0\\,]$\n  - Events $d$: $[\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n- Test case $2$ (increasing hazard consistent with aging, more events at larger $t$):\n  - Times $t$: $[\\,0.5,\\,0.8,\\,1.0,\\,1.2,\\,1.6,\\,2.0,\\,2.4,\\,3.0,\\,3.6,\\,4.2,\\,5.0,\\,5.8\\,]$\n  - Events $d$: $[\\,0,\\,0,\\,0,\\,1,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n- Test case $3$ (edge case with heavy right-censoring at early ages and concentration of events at large $t$):\n  - Times $t$: $[\\,0.5,\\,1.0,\\,2.0,\\,3.5,\\,5.0,\\,7.0,\\,9.5,\\,12.0,\\,15.0,\\,18.0\\,]$\n  - Events $d$: $[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n\nImplementation and output requirements:\n- Your program must estimate parameters via maximum likelihood for each model and dataset, compute $AIC$ for each model, and select the model with minimum $AIC$.\n- Use the following model indexing convention in your final output: $0$ for Exponential, $1$ for Weibull, $2$ for Gompertz.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each entry is the selected model index for the corresponding test case.\n- Units: All times are in years. No other unit conversions are required. The final outputs are integers and therefore unitless.\n\nYour program must be a complete, runnable implementation that does not require any user input. It must use numerically stable methods, including appropriate parameter transformations for positivity constraints and careful handling of small-parameter limits. The final output must be a single line as specified. No other text should be printed.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Hazard function**: $h(t) = \\lim_{\\Delta t \\to 0^{+}} \\dfrac{\\mathbb{P}(t \\le T  t + \\Delta t \\mid T \\ge t)}{\\Delta t}$, where $T$ is a non-negative random time-to-event variable.\n- **Survival function**: $S(t) = \\mathbb{P}(T \\ge t)$ and $S(t) = \\exp\\!\\left(-\\int_{0}^{t} h(u)\\,du\\right)$.\n- **Probability density function**: $f(t) = h(t)\\,S(t)$ for $t  0$.\n- **Right-censoring**: An observation at time $c$ indicates that the event time $T$ satisfies $T \\ge c$. The data for each observation $i$ consists of a time $t_i$ and an event indicator $d_i$, where $d_i=1$ for an observed event and $d_i=0$ for right-censoring.\n- **Models for fitting**:\n    1.  Exponential hazard model (constant hazard).\n    2.  Weibull hazard model (monotone hazard).\n    3.  Gompertz hazard model (exponentially changing hazard).\n- **Tasks**:\n    1.  Derive and implement the log-likelihood function for right-censored data.\n    2.  Find Maximum Likelihood Estimates (MLE) for model parameters.\n    3.  Compute the Akaike Information Criterion (AIC): $AIC = 2p - 2\\widehat{\\ell}$, where $p$ is the number of free parameters and $\\widehat{\\ell}$ is the maximized log-likelihood.\n    4.  Select the model with the minimum AIC for each dataset.\n- **Data sets**:\n    - **Test case 1**:\n        - Times $t$: $[\\,0.2,\\,0.6,\\,1.0,\\,1.4,\\,2.1,\\,2.5,\\,3.0,\\,3.7,\\,4.2,\\,4.9,\\,5.5,\\,6.1,\\,6.8,\\,7.4,\\,8.0,\\,9.0\\,]$\n        - Events $d$: $[\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,0,\\,0\\,]$\n    - **Test case 2**:\n        - Times $t$: $[\\,0.5,\\,0.8,\\,1.0,\\,1.2,\\,1.6,\\,2.0,\\,2.4,\\,3.0,\\,3.6,\\,4.2,\\,5.0,\\,5.8\\,]$\n        - Events $d$: $[\\,0,\\,0,\\,0,\\,1,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n    - **Test case 3**:\n        - Times $t$: $[\\,0.5,\\,1.0,\\,2.0,\\,3.5,\\,5.0,\\,7.0,\\,9.5,\\,12.0,\\,15.0,\\,18.0\\,]$\n        - Events $d$: $[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n- **Output Requirements**: A single-line list of integer model indices ($0$: Exponential, $1$: Weibull, $2$: Gompertz) corresponding to the best model for each test case.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is based on established principles of survival analysis, a core topic in biostatistics and reliability engineering. The definitions of hazard, survival, and likelihood functions are standard. The models—Exponential, Weibull, and Gompertz—are canonical parametric survival models. The task of deriving the likelihood, performing MLE, and using AIC for model selection constitutes a complete and standard statistical procedure. The provided datasets are self-contained and numerically reasonable. There are no contradictions, ambiguities, or violations of scientific or mathematical principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution Derivation and Methodology\nThe core of the problem is the formulation and maximization of the likelihood function under right-censoring. For a sample of $N$ individuals, let the data for individual $i$ be $(t_i, d_i)$, where $t_i$ is the time of event or censoring, and $d_i$ is the event indicator ($d_i=1$ if the event is observed, $d_i=0$ if censored).\n\nThe likelihood contribution of an individual $i$ is the probability density function $f(t_i)$ if the event is observed, and the survival function $S(t_i)$ if the observation is censored. The total likelihood over the sample is the product of these contributions:\n$$ L(\\boldsymbol{\\theta}) = \\prod_{i=1}^{N} [f(t_i; \\boldsymbol{\\theta})]^{d_i} [S(t_i; \\boldsymbol{\\theta})]^{1-d_i} $$\nwhere $\\boldsymbol{\\theta}$ is the vector of model parameters. Using the relations $f(t) = h(t)S(t)$ and $S(t) = \\exp(-H(t))$, where $H(t) = \\int_0^t h(u)du$ is the cumulative hazard function, we can rewrite the likelihood. It is more convenient to work with the log-likelihood, $\\ell(\\boldsymbol{\\theta}) = \\log L(\\boldsymbol{\\theta})$:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log f(t_i; \\boldsymbol{\\theta}) + (1-d_i) \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log(h(t_i; \\boldsymbol{\\theta})S(t_i; \\boldsymbol{\\theta})) + (1-d_i) \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log h(t_i; \\boldsymbol{\\theta}) + d_i \\log S(t_i; \\boldsymbol{\\theta}) + (1-d_i) \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log h(t_i; \\boldsymbol{\\theta}) + \\log S(t_i; \\boldsymbol{\\theta}) \\right] $$\nSubstituting $\\log S(t) = -H(t)$, we arrive at the final form used for computation:\n$$ \\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ d_i \\log h(t_i; \\boldsymbol{\\theta}) - H(t_i; \\boldsymbol{\\theta}) \\right] $$\nWe now define the specific forms of $h(t)$ and $H(t)$ for each model and discuss their numerical implementation. Maximum likelihood estimates $\\widehat{\\boldsymbol{\\theta}}$ are found by numerically minimizing the negative log-likelihood, $-\\ell(\\boldsymbol{\\theta})$.\n\n**1. Exponential Model ($p=1$)**\n- Hazard function: $h(t; \\lambda) = \\lambda$, with $\\lambda  0$.\n- Cumulative hazard: $H(t; \\lambda) = \\int_0^t \\lambda \\,du = \\lambda t$.\n- Log-likelihood: $\\ell(\\lambda) = \\sum_{i=1}^{N} [d_i \\log(\\lambda) - \\lambda t_i]$.\n- Parameterization: To enforce the constraint $\\lambda  0$, we re-parameterize using $\\lambda = e^{\\alpha}$. The objective function is then minimized with respect to the unconstrained parameter $\\alpha \\in \\mathbb{R}$.\n\n**2. Weibull Model ($p=2$)**\n- Hazard function: $h(t; k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{t}{\\lambda}\\right)^{k-1}$, with shape $k  0$ and scale $\\lambda  0$.\n- Cumulative hazard: $H(t; k, \\lambda) = \\left(\\frac{t}{\\lambda}\\right)^k$.\n- Log-likelihood: $\\ell(k, \\lambda) = \\sum_{i=1}^{N} \\left[ d_i \\left(\\log(k) - k\\log(\\lambda) + (k-1)\\log(t_i)\\right) - \\left(\\frac{t_i}{\\lambda}\\right)^k \\right]$. All $t_i  0$ in the data, so $\\log(t_i)$ is well-defined.\n- Parameterization: We use $k = e^{\\alpha_1}$ and $\\lambda = e^{\\alpha_2}$ to enforce positivity, optimizing over unconstrained $\\alpha_1, \\alpha_2 \\in \\mathbb{R}$.\n\n**3. Gompertz Model ($p=2$)**\n- Hazard function: $h(t; a, b) = a e^{bt}$, with initial hazard $a  0$ and shape parameter $b \\in \\mathbb{R}$.\n- Cumulative hazard: For $b \\neq 0$, $H(t; a, b) = \\int_0^t a e^{bu} \\,du = \\frac{a}{b}(e^{bt} - 1)$.\n- Limiting case: As $b \\to 0$, $h(t) \\to a$ and $H(t) \\to at$. The Gompertz model smoothly converges to the Exponential model with rate $\\lambda = a$.\n- Log-likelihood: $\\ell(a, b) = \\sum_{i=1}^{N} \\left[ d_i (\\log(a) + b t_i) - \\frac{a}{b}(e^{bt_i} - 1) \\right]$.\n- Parameterization: We use $a = e^{\\alpha_1}$ and $b = \\alpha_2$, optimizing over $\\alpha_1, \\alpha_2 \\in \\mathbb{R}$. To ensure numerical stability for $b \\approx 0$, the term $\\frac{e^{bt_i}-1}{b}$ is computed using a function equivalent to `expm1(x)/x`, which correctly evaluates to its limit $t_i$ as $b t_i \\to 0$.\n\n**Model Selection**\nFor each model, after finding the MLEs $\\widehat{\\boldsymbol{\\theta}}$, we compute the maximized log-likelihood $\\widehat{\\ell} = \\ell(\\widehat{\\boldsymbol{\\theta}})$. The Akaike Information Criterion is then calculated:\n$$ AIC = 2p - 2\\widehat{\\ell} $$\nwhere $p$ is the number of parameters ($1$ for Exponential, $2$ for Weibull, $2$ for Gompertz). The model with the lowest AIC value is selected as the best-fitting model for the given dataset, balancing goodness-of-fit with model complexity.\n\nThe implementation will proceed by defining a negative log-likelihood function for each model, using the specified parameterizations. An optimization routine (`scipy.optimize.minimize`) will be used to find the parameters that minimize these functions for each dataset. Finally, the AIC values will be compared to select the optimal model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Fits Exponential, Weibull, and Gompertz survival models to three\n    right-censored datasets, selecting the best model for each dataset\n    using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0.2, 0.6, 1.0, 1.4, 2.1, 2.5, 3.0, 3.7, 4.2, 4.9, 5.5, 6.1, 6.8, 7.4, 8.0, 9.0]),\n            np.array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0])\n        ),\n        (\n            np.array([0.5, 0.8, 1.0, 1.2, 1.6, 2.0, 2.4, 3.0, 3.6, 4.2, 5.0, 5.8]),\n            np.array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1])\n        ),\n        (\n            np.array([0.5, 1.0, 2.0, 3.5, 5.0, 7.0, 9.5, 12.0, 15.0, 18.0]),\n            np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n        )\n    ]\n\n    results = []\n    \n    # Model indexing: 0: Exponential, 1: Weibull, 2: Gompertz\n    model_indices = {'exp': 0, 'weibull': 1, 'gompertz': 2}\n    \n    # --- Negative Log-Likelihood Functions ---\n\n    def neg_log_lik_exp(params, t, d):\n        \"\"\"Negative log-likelihood for the Exponential model.\"\"\"\n        log_lambda = params[0]\n        lambda_ = np.exp(log_lambda)\n        \n        log_h = log_lambda\n        H = lambda_ * t\n        \n        log_lik = np.sum(d * log_h - H)\n        return -log_lik\n\n    def neg_log_lik_weibull(params, t, d):\n        \"\"\"Negative log-likelihood for the Weibull model.\"\"\"\n        log_k, log_lambda = params\n        k = np.exp(log_k)\n        lambda_ = np.exp(log_lambda)\n        \n        # log(t) is safe as all t > 0 in the provided data.\n        log_t = np.log(t)\n        \n        log_h = log_k + (k - 1) * log_t - k * log_lambda\n        H = np.exp(k * (log_t - log_lambda))\n\n        log_lik = np.sum(d * log_h - H)\n        return -log_lik\n\n    def neg_log_lik_gompertz(params, t, d):\n        \"\"\"Negative log-likelihood for the Gompertz model.\"\"\"\n        log_a, b = params\n        a = np.exp(log_a)\n        \n        log_h = log_a + b * t\n        \n        # Numerically stable computation for H(t)\n        if np.abs(b)  1e-9:\n            H = a * t\n        else:\n            H = a * np.expm1(b * t) / b\n            \n        log_lik = np.sum(d * log_h - H)\n        return -log_lik\n\n    for t_data, d_data in test_cases:\n        \n        # --- Initial Guesses for Optimization ---\n        # A good initial guess is the MLE for the exponential model,\n        # which is a special case of both Weibull (k=1) and Gompertz (b=0).\n        sum_d = np.sum(d_data)\n        sum_t = np.sum(t_data)\n        \n        # Handle case where there are no events to avoid division by zero.\n        if sum_d == 0:\n            lambda_init = 1.0 / np.mean(t_data) if len(t_data) > 0 else 1.0\n        else:\n             lambda_init = sum_d / sum_t\n        \n        # [log(lambda)]\n        params0_exp = [np.log(lambda_init)]\n        # [log(k), log(lambda_weibull)], init k=1, lambda_weibull=1/lambda_exp\n        params0_weibull = [0.0, -np.log(lambda_init)]\n        # [log(a), b], init a=lambda_exp, b=0\n        params0_gompertz = [np.log(lambda_init), 0.0]\n\n        # --- Optimization ---\n        # Exponential\n        res_exp = minimize(neg_log_lik_exp, params0_exp, args=(t_data, d_data), method='Nelder-Mead')\n        # Weibull\n        res_weibull = minimize(neg_log_lik_weibull, params0_weibull, args=(t_data, d_data), method='Nelder-Mead')\n        # Gompertz\n        res_gompertz = minimize(neg_log_lik_gompertz, params0_gompertz, args=(t_data, d_data), method='Nelder-Mead')\n\n        # --- AIC Calculation ---\n        # AIC = 2*p - 2*logL, where logL = -min_val_of_neg_log_lik\n        p_exp = 1\n        aic_exp = 2 * p_exp + 2 * res_exp.fun\n\n        p_weibull = 2\n        aic_weibull = 2 * p_weibull + 2 * res_weibull.fun\n        \n        p_gompertz = 2\n        aic_gompertz = 2 * p_gompertz + 2 * res_gompertz.fun\n        \n        # --- Model Selection ---\n        aics = [aic_exp, aic_weibull, aic_gompertz]\n        best_model_index = np.argmin(aics)\n        results.append(best_model_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2503639"}]}