{"hands_on_practices": [{"introduction": "At the heart of all scattering-based structural biology techniques lies the principle of interference. How do waves scattered from individual atoms combine to produce the complex diffraction patterns we observe? This foundational exercise ([@problem_id:2571528]) strips the problem down to its essence by modeling a molecule as just two point scatterers. By deriving the scattering intensity for this simple dimer, you will gain a first-principles understanding of the structure factor, $S(\\mathbf{q})$, and see how the spatial arrangement of atoms is encoded in the interference pattern.", "problem": "A rigid dimer in solution is probed under monochromatic X-ray illumination in the kinematic (single-scattering) regime, as commonly assumed in X-ray crystallography and small-angle scattering analyses. Model the dimer as two point scatterers with isotropic, energy-dependent but locally constant atomic form factors $f_A$ and $f_B$ over the scattering vector range of interest. Let the point scatterers be located at positions $\\mathbf{r}_1=-\\mathbf{r}_0/2$ and $\\mathbf{r}_2=+\\mathbf{r}_0/2$, where $\\mathbf{r}_0$ is a fixed vector specifying the inter-scatterer displacement. The scattering vector is defined as $\\mathbf{q}=\\mathbf{k}_s-\\mathbf{k}_i$, where the incident and scattered wavevectors satisfy $|\\mathbf{k}_i|=|\\mathbf{k}_s|=2\\pi/\\lambda$ for wavelength $\\lambda$.\n\nStarting only from the definition of the structure factor $S(\\mathbf{q})$ as the coherent sum of contributions from all scatterers and the fact that the differential scattering intensity is proportional to $|S(\\mathbf{q})|^2$, derive a closed-form expression for the normalized intensity\n$$\nI_{\\mathrm{rel}}(\\mathbf{q}) \\equiv \\frac{|S(\\mathbf{q})|^2}{|S(\\mathbf{0})|^2}\n$$\nas a function of $f_A$, $f_B$, and $\\mathbf{q}\\cdot\\mathbf{r}_0$. Assume the far-field approximation and neglect multiple scattering and inelastic effects. Express your final answer as a single closed-form analytic expression for $I_{\\mathrm{rel}}(\\mathbf{q})$ in terms of $f_A$, $f_B$, and $\\mathbf{q}\\cdot\\mathbf{r}_0$. No numerical evaluation is required, and no rounding is needed. Do not include units in your final answer.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of kinematic scattering theory, is well-posed with sufficient information for a unique solution, and is formulated with objective, precise language. We shall proceed with the derivation.\n\nThe structure factor, $S(\\mathbf{q})$, for a system of $N$ scatterers located at positions $\\mathbf{r}_j$ with corresponding atomic form factors $f_j$ is defined as the coherent sum of the amplitudes of the scattered waves:\n$$\nS(\\mathbf{q}) = \\sum_{j=1}^{N} f_j e^{i \\mathbf{q} \\cdot \\mathbf{r}_j}\n$$\nwhere $i$ is the imaginary unit and $\\mathbf{q}$ is the scattering vector. The form factors $f_j$ are assumed to be constant over the range of $\\mathbf{q}$ considered.\n\nThe system under consideration is a rigid dimer, which consists of two point scatterers, $A$ and $B$.\nThe number of scatterers is $N=2$.\nThe form factors are $f_A$ and $f_B$.\nThe positions are given as $\\mathbf{r}_1 = -\\frac{\\mathbf{r}_0}{2}$ and $\\mathbf{r}_2 = +\\frac{\\mathbf{r}_0}{2}$.\n\nSubstituting these givens into the general definition of the structure factor yields:\n$$\nS(\\mathbf{q}) = f_A e^{i \\mathbf{q} \\cdot \\left(-\\frac{\\mathbf{r}_0}{2}\\right)} + f_B e^{i \\mathbf{q} \\cdot \\left(+\\frac{\\mathbf{r}_0}{2}\\right)}\n$$\nThis can be rewritten as:\n$$\nS(\\mathbf{q}) = f_A e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2}\n$$\nThe scattering intensity is proportional to the squared modulus of the structure factor, $|S(\\mathbf{q})|^2$. To compute this, we multiply $S(\\mathbf{q})$ by its complex conjugate, $S^*(\\mathbf{q})$:\n$$\n|S(\\mathbf{q})|^2 = S(\\mathbf{q})S^*(\\mathbf{q})\n$$\nAssuming the atomic form factors $f_A$ and $f_B$ are real-valued, which is standard for non-anomalous X-ray scattering, the complex conjugate is:\n$$\nS^*(\\mathbf{q}) = f_A e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2}\n$$\nThe product is then:\n$$\n|S(\\mathbf{q})|^2 = \\left( f_A e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} \\right) \\left( f_A e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} \\right)\n$$\nExpanding this expression gives four terms:\n$$\n|S(\\mathbf{q})|^2 = f_A^2 e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_A f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B f_A e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} + f_B^2 e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2} e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)/2}\n$$\nSimplifying the exponents:\n$$\n|S(\\mathbf{q})|^2 = f_A^2 e^{0} + f_A f_B e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)} + f_A f_B e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)} + f_B^2 e^{0}\n$$\n$$\n|S(\\mathbf{q})|^2 = f_A^2 + f_B^2 + f_A f_B \\left( e^{i (\\mathbf{q} \\cdot \\mathbf{r}_0)} + e^{-i (\\mathbf{q} \\cdot \\mathbf{r}_0)} \\right)\n$$\nUsing Euler's formula, which states $e^{i\\theta} + e^{-i\\theta} = 2\\cos(\\theta)$, with $\\theta = \\mathbf{q} \\cdot \\mathbf{r}_0$, we obtain the Debye scattering equation for two particles:\n$$\n|S(\\mathbf{q})|^2 = f_A^2 + f_B^2 + 2 f_A f_B \\cos(\\mathbf{q} \\cdot \\mathbf{r}_0)\n$$\nNext, we must find the normalization factor, $|S(\\mathbf{0})|^2$. This is the intensity at zero scattering vector, $\\mathbf{q}=\\mathbf{0}$, which corresponds to the forward scattering direction. We evaluate $S(\\mathbf{q})$ at $\\mathbf{q}=\\mathbf{0}$:\n$$\nS(\\mathbf{0}) = f_A e^{-i (\\mathbf{0} \\cdot \\mathbf{r}_0)/2} + f_B e^{i (\\mathbf{0} \\cdot \\mathbf{r}_0)/2} = f_A e^0 + f_B e^0 = f_A + f_B\n$$\nThe squared modulus is therefore:\n$$\n|S(\\mathbf{0})|^2 = (f_A + f_B)^2 = f_A^2 + 2 f_A f_B + f_B^2\n$$\nFinally, the normalized intensity $I_{\\mathrm{rel}}(\\mathbf{q})$ is the ratio of $|S(\\mathbf{q})|^2$ to $|S(\\mathbf{0})|^2$:\n$$\nI_{\\mathrm{rel}}(\\mathbf{q}) = \\frac{|S(\\mathbf{q})|^2}{|S(\\mathbf{0})|^2} = \\frac{f_A^2 + f_B^2 + 2 f_A f_B \\cos(\\mathbf{q} \\cdot \\mathbf{r}_0)}{(f_A + f_B)^2}\n$$\nThis final expression is a function of $f_A$, $f_B$, and the scalar product $\\mathbf{q} \\cdot \\mathbf{r}_0$, as required by the problem statement.", "answer": "$$\n\\boxed{\\frac{f_A^2 + f_B^2 + 2 f_A f_B \\cos(\\mathbf{q} \\cdot \\mathbf{r}_0)}{(f_A + f_B)^2}}\n$$", "id": "2571528"}, {"introduction": "From the continuous interference pattern of a single molecule in solution, we turn to the ordered array of a crystal. A crystal lattice samples the structure factor at discrete points in reciprocal space, known as reflections. This practice ([@problem_id:2571471]) challenges you to step into the shoes of an experimentalist and determine which of these reflections are actually observable given the physical constraints of an X-ray diffractometer. By applying Bragg's law and the concept of the limiting sphere, you will calculate the accessible portion of reciprocal space, a key step in planning a successful crystallography experiment.", "problem": "In a single-crystal X-ray crystallography data collection for an orthorhombic protein crystal, you are given a unit cell with edge lengths $a=5\\,\\text{\\AA}$, $b=6\\,\\text{\\AA}$, and $c=7\\,\\text{\\AA}$ and all interaxial angles equal to $90^{\\circ}$. The incident wavelength is $\\lambda=2.0\\,\\text{\\AA}$. The diffractometer and detector geometry permit a maximum scattering angle of $2\\theta_{\\max}=30^{\\circ}$, and the goniometer permits a maximum total rotation range of $\\phi_{\\max}=180^{\\circ}$ about a single axis. Assume that the $180^{\\circ}$ rotation range is sufficient to bring every reciprocal-lattice point that satisfies the Bragg condition within the detector’s $2\\theta$ acceptance into the diffracting condition at some sample orientation, and ignore detector gaps, beamstop shadow, and absorption. Also assume there are no systematic absences (primitive lattice) and neglect anomalous scattering, treating inversion-related reflections as equivalent for the purpose of accessibility.\n\nStarting from Bragg’s law $2d\\sin\\theta=\\lambda$ and the definition of the reciprocal lattice metric in an orthorhombic system, determine how many distinct reflections in the nonredundant set defined by Miller indices with $h\\ge 0$, $k\\ge 0$, $l\\ge 0$ (excluding $(0,0,0)$) are kinematically accessible under these constraints. Express your final answer as a single integer with no units. No rounding is required. Angles are in degrees, and lengths are in ångströms.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary parameters and simplifying assumptions are provided. The problem is therefore valid. We proceed to the solution.\n\nThe condition for constructive interference in X-ray diffraction is given by Bragg's law:\n$$2d_{hkl}\\sin\\theta = n\\lambda$$\nFor a given set of Miller indices $(h, k, l)$, the first-order reflection ($n=1$) is considered. Higher-order reflections $(nh, nk, nl)$ are treated as distinct reflections in their own right, so we can set $n=1$ without loss of generality. Thus, Bragg's law is written as:\n$$2d_{hkl}\\sin\\theta = \\lambda$$\nThis can be rearranged to relate the interplanar spacing $d_{hkl}$ to the scattering angle $\\theta$ and wavelength $\\lambda$:\n$$\\frac{1}{d_{hkl}} = \\frac{2\\sin\\theta}{\\lambda}$$\nThe term $\\frac{1}{d_{hkl}}$ is the magnitude of the reciprocal lattice vector $\\mathbf{d}^*_{hkl}$ corresponding to the crystal planes $(h,k,l)$. The experimental setup imposes a maximum scattering angle $2\\theta_{\\max} = 30^{\\circ}$, which means the maximum Bragg angle is $\\theta_{\\max}=15^{\\circ}$. This defines a sphere in reciprocal space, known as the limiting sphere, which contains all kinematically accessible reflections. The radius of this sphere, $R_{\\text{lim}}$, is the maximum possible magnitude of an observable reciprocal lattice vector:\n$$R_{\\text{lim}} = |\\mathbf{d}^*_{hkl}|_{\\max} = \\frac{2\\sin\\theta_{\\max}}{\\lambda}$$\nSubstituting the given values $\\lambda = 2.0\\,\\text{\\AA}$ and $\\theta_{\\max} = 15^{\\circ}$:\n$$R_{\\text{lim}} = \\frac{2\\sin(15^{\\circ})}{2.0} = \\sin(15^{\\circ})$$\nFor an orthorhombic crystal with unit cell parameters $a$, $b$, and $c$, the magnitude of the reciprocal lattice vector is given by:\n$$|\\mathbf{d}^*_{hkl}| = \\sqrt{\\frac{h^2}{a^2} + \\frac{k^2}{b^2} + \\frac{l^2}{c^2}}$$\nA reflection $(h,k,l)$ is accessible if its corresponding reciprocal lattice point lies within the limiting sphere. The condition is:\n$$|\\mathbf{d}^*_{hkl}| \\le R_{\\text{lim}}$$\nSquaring both sides gives the inequality we must solve:\n$$\\frac{h^2}{a^2} + \\frac{k^2}{b^2} + \\frac{l^2}{c^2} \\le R_{\\text{lim}}^2 = \\sin^2(15^{\\circ})$$\nWe are given $a=5\\,\\text{\\AA}$, $b=6\\,\\text{\\AA}$, and $c=7\\,\\text{\\AA}$. The inequality becomes:\n$$\\frac{h^2}{5^2} + \\frac{k^2}{6^2} + \\frac{l^2}{7^2} \\le \\sin^2(15^{\\circ})$$\nTo evaluate the right side, we use the angle subtraction formula for sine: $\\sin(A-B) = \\sin A \\cos B - \\cos A \\sin B$.\n$$\\sin(15^{\\circ}) = \\sin(45^{\\circ}-30^{\\circ}) = \\sin(45^{\\circ})\\cos(30^{\\circ}) - \\cos(45^{\\circ})\\sin(30^{\\circ})$$\n$$\\sin(15^{\\circ}) = \\left(\\frac{\\sqrt{2}}{2}\\right)\\left(\\frac{\\sqrt{3}}{2}\\right) - \\left(\\frac{\\sqrt{2}}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{\\sqrt{6}-\\sqrt{2}}{4}$$\nTherefore, the square of the radius is:\n$$\\sin^2(15^{\\circ}) = \\left(\\frac{\\sqrt{6}-\\sqrt{2}}{4}\\right)^2 = \\frac{6 - 2\\sqrt{12} + 2}{16} = \\frac{8 - 4\\sqrt{3}}{16} = \\frac{2-\\sqrt{3}}{4}$$\nNumerically, $\\sin^2(15^{\\circ}) \\approx 0.0669875$.\nThe problem requires us to count the number of integer triples $(h,k,l)$ satisfying $h \\ge 0$, $k \\ge 0$, $l \\ge 0$ (but not $(0,0,0)$) and the inequality:\n$$\\frac{h^2}{25} + \\frac{k^2}{36} + \\frac{l^2}{49} \\le \\frac{2-\\sqrt{3}}{4}$$\nTo constrain our search space, we find the maximum possible integer value for each index by setting the other two indices to zero.\nFor $h$: $\\frac{h^2}{25} \\le \\sin^2(15^{\\circ}) \\implies h \\le 5 \\sin(15^{\\circ}) = 5 \\left(\\frac{\\sqrt{6}-\\sqrt{2}}{4}\\right) \\approx 1.294$. The maximum integer $h$ is $1$.\nFor $k$: $\\frac{k^2}{36} \\le \\sin^2(15^{\\circ}) \\implies k \\le 6 \\sin(15^{\\circ}) = 6 \\left(\\frac{\\sqrt{6}-\\sqrt{2}}{4}\\right) \\approx 1.553$. The maximum integer $k$ is $1$.\nFor $l$: $\\frac{l^2}{49} \\le \\sin^2(15^{\\circ}) \\implies l \\le 7 \\sin(15^{\\circ}) = 7 \\left(\\frac{\\sqrt{6}-\\sqrt{2}}{4}\\right) \\approx 1.812$. The maximum integer $l$ is $1$.\nThus, we only need to test integer triples $(h,k,l)$ where each index is either $0$ or $1$. We exclude $(0,0,0)$. Let $S(h,k,l) = \\frac{h^2}{25} + \\frac{k^2}{36} + \\frac{l^2}{49}$ and we test against $R_{\\text{lim}}^2 \\approx 0.0669875$.\n\n1.  $(h,k,l) = (1,0,0)$: $S(1,0,0) = \\frac{1}{25} = 0.04$. Since $0.04  0.0669875$, this reflection is accessible.\n2.  $(h,k,l) = (0,1,0)$: $S(0,1,0) = \\frac{1}{36} \\approx 0.02778$. Since $0.02778  0.0669875$, this reflection is accessible.\n3.  $(h,k,l) = (0,0,1)$: $S(0,0,1) = \\frac{1}{49} \\approx 0.02041$. Since $0.02041  0.0669875$, this reflection is accessible.\n4.  $(h,k,l) = (1,1,0)$: $S(1,1,0) = \\frac{1}{25} + \\frac{1}{36} = \\frac{36+25}{900} = \\frac{61}{900} \\approx 0.06778$. Since $0.06778  0.0669875$, this reflection is not accessible.\n5.  $(h,k,l) = (1,0,1)$: $S(1,0,1) = \\frac{1}{25} + \\frac{1}{49} = \\frac{49+25}{1225} = \\frac{74}{1225} \\approx 0.06041$. Since $0.06041  0.0669875$, this reflection is accessible.\n6.  $(h,k,l) = (0,1,1)$: $S(0,1,1) = \\frac{1}{36} + \\frac{1}{49} = \\frac{49+36}{1764} = \\frac{85}{1764} \\approx 0.04819$. Since $0.04819  0.0669875$, this reflection is accessible.\n7.  $(h,k,l) = (1,1,1)$: $S(1,1,1) = \\frac{1}{25} + \\frac{1}{36} + \\frac{1}{49} \\approx 0.06778 + 0.02041 = 0.08819$. This is clearly greater than $0.0669875$, so this reflection is not accessible.\n\nThe distinct, non-redundant reflections that are kinematically accessible are $(1,0,0)$, $(0,1,0)$, $(0,0,1)$, $(1,0,1)$, and $(0,1,1)$.\nCounting these gives a total of $5$ reflections.", "answer": "$$\\boxed{5}$$", "id": "2571471"}, {"introduction": "Modern structural biology, particularly single-particle cryo-EM, operates at the frontier of signal and noise. Reconstructing a 3D map from thousands of extremely noisy 2D images presents a major statistical challenge: how can we be sure our final structure represents true biological features and not just cleverly modeled noise? This exercise ([@problem_id:2571522]) confronts the critical problem of overfitting and model bias. By reasoning from a simple signal-plus-noise model, you will learn to justify the 'gold-standard' half-set validation protocol, a cornerstone of rigor in modern cryo-EM.", "problem": "A single-particle cryogenic electron microscopy (cryo-EM) dataset of a mildly flexible, oligomeric enzyme is collected at near-physiological conditions. You plan a three-dimensional (3D) classification to separate conformational states before high-resolution refinement. The experimental conditions yield a per-particle Fourier domain signal-to-noise ratio (SNR) that decreases with spatial frequency, and in the mid-resolution shells of interest the SNR is approximately $0.05$.\n\nConsider the following general setup for 3D classification in single-particle analysis (SPA). Each particle image is modeled as an additive superposition of a projection of the underlying 3D signal and noise. When reconstructing maps from particle subsets, the reconstructed Fourier coefficients can be idealized as $X = S + N$, where $S$ is the unknown signal component and $N$ represents noise. Assume $N$ has zero mean and is uncorrelated with $S$. In practice, iterative algorithms (for example, expectation–maximization with regularized likelihood) simultaneously update class assignments, orientations, and maps, which risks overfitting by adapting to noise if model complexity outpaces data support.\n\nYou are considering two alternative protocols:\n\n- Protocol $\\mathrm{P1}$ (single-set classification): Use all particles in a single pool for iterative 3D classification. For resolution assessment, compute the correlation between the final map and a reference constructed from the same particle set and processing decisions.\n\n- Protocol $\\mathrm{P2}$ (gold-standard half-set classification): Randomly split particles into 2 statistically independent halves before any optimization. Carry out 3D classification and map reconstruction independently for each half. Assess resolution and feature reliability by correlating the two independent half-maps and by checking the reproducibility of class-specific features and occupancies across halves. Only after convergence, merge corresponding classes across halves for final maps.\n\nFrom first principles, answer the following. Use the additive model $X = S + N$ and the fact that, for statistically independent noise realizations $N$ and $\\tilde{N}$ with zero mean, cross-terms such as $\\langle S, \\tilde{N} \\rangle$ have expectation $0$, while shared noise terms can induce spurious correlation. Also use the principle of cross-validation: performance must be evaluated on data that were not used to fit the model parameters, in order to obtain an unbiased generalization estimate.\n\nWhich of the following statements are correct?\n\nA. In Protocol $\\mathrm{P1}$, iterative classification can couple reconstructions to the specific noise realization present in the particle set, so that resolution estimates based on correlating a map with a reference derived from the same data (i.e., sharing $N$) are upwardly biased. In Protocol $\\mathrm{P2}$, keeping half-sets independent ensures that, on average, only the shared signal $S$ contributes to the inter-half correlation, controlling overfitting-induced inflation.\n\nB. A sufficiently aggressive low-pass filter applied during classification is, by itself, sufficient to prevent overfitting and model bias, regardless of the number of classes, the SNR, and the number of particles.\n\nC. To test whether 3D classification features are driven by data rather than initial-model bias, one can rerun the entire gold-standard half-set classification with different random seeds and initial models, and then assess whether corresponding classes exhibit consistent features and occupancies across halves and repeats. Stability across independent halves and repeats is evidence against model bias.\n\nD. If particles are randomly split into two halves only after the final optimization in a single-set classification, then the inter-half correlation will still be a valid gold-standard estimate of resolution and feature reliability, because the split ensures statistical independence.\n\nE. Performing per-particle motion correction and contrast transfer function (CTF) refinement on each half-set while sharing alignment parameters across halves preserves the statistical independence needed for unbiased inter-half validation.\n\nSelect all that apply.\n\nImportant: Derive your reasoning from the additive signal-plus-noise model, statistical independence, and the definition of cross-validation. Avoid appeals to undocumented software-specific heuristics or black-box formulas that are not justified from these principles.", "solution": "The problem statement describes a common challenge in single-particle cryo-electron microscopy (cryo-EM): the analysis of conformational heterogeneity in a low signal-to-noise ratio (SNR) environment. The statement is scientifically sound, well-posed, and provides the necessary first principles for a rigorous evaluation. The experimental scenario is realistic; a per-particle SNR of $0.05$ is typical. The theoretical framework, modeling a particle image as an additive superposition of signal and noise ($X = S + N$), is a standard and effective idealization. The core task is to evaluate two data processing protocols, $\\mathrm{P1}$ (single-set) and $\\mathrm{P2}$ (gold-standard half-set), and related statements, based on the principles of statistical independence and cross-validation to control for overfitting. The problem is valid.\n\nThe fundamental principles to be applied are:\n1.  **Additive Model**: The measured data in Fourier space for a given projection, $X$, is the sum of the true signal, $S$, and a noise term, $N$. That is, $X = S + N$.\n2.  **Noise Properties**: The noise $N$ is a random variable with zero mean, $E[N] = 0$. The noise is uncorrelated with the signal, $E[S N] = E[S] E[N] = 0$. For two independent datasets, their noise realizations, $N_1$ and $N_2$, are statistically independent, meaning their covariance is zero, $E[N_1 N_2] = E[N_1] E[N_2] = 0$.\n3.  **Overfitting**: This occurs when a model with high complexity (e.g., a large number of 3D classes) is fit to a dataset with limited information (low SNR, insufficient number of particles). The model begins to fit the specific noise realization $N$ present in the data, rather than just the underlying signal $S$. This leads to the generation of spurious, non-reproducible features.\n4.  **Cross-Validation**: To obtain an unbiased estimate of a model's performance on new data (its generalization ability), the model must be evaluated on a dataset that was not used during the model's training or parameter fitting. This is the only robust way to detect overfitting.\n\nWith these principles, we shall evaluate each statement.\n\n**A. In Protocol $\\mathrm{P1}$, iterative classification can couple reconstructions to the specific noise realization present in the particle set, so that resolution estimates based on correlating a map with a reference derived from the same data (i.e., sharing $N$) are upwardly biased. In Protocol $\\mathrm{P2}$, keeping half-sets independent ensures that, on average, only the shared signal $S$ contributes to the inter-half correlation, controlling overfitting-induced inflation.**\n\nThis statement addresses the core issue of overfitting and its detection.\nIn Protocol $\\mathrm{P1}$, the entire dataset, which we can model as $\\{S+N\\}$, is used for classification and reconstruction. The final map, $M_{full}$, is a complex function of these data, $M_{full} = f(\\{S+N\\})$. If one assesses resolution by comparing this map to a reference, $R$, also constructed from the same data (e.g., from a subset of the same particles or using the same parameters), then $R = g(\\{S+N\\})$. Both the map and the reference are now functions of the *same* noise realization, $N$. When we compute their correlation, terms dependent on the self-correlation of the noise, $\\langle N', N' \\rangle$ (where $N'$ is the noise component in the final maps), will contribute. Since the algorithm has fit to this noise, these terms are non-zero and positive, leading to an artificially inflated correlation. This is not a true measure of signal resolution but rather a measure of how well the algorithm has memorized the noise.\n\nIn Protocol $\\mathrm{P2}$, the data is split *a priori* into two independent halves: set 1 with data $\\{S+N_1\\}$ and set 2 with data $\\{S+N_2\\}$. The noise realizations $N_1$ and $N_2$ are statistically independent. Two maps, $M_1 = f_1(\\{S+N_1\\})$ and $M_2 = f_2(\\{S+N_2\\})$, are reconstructed independently. When calculating their cross-correlation, we are evaluating the expectation of terms like $\\langle S+N_1, S+N_2 \\rangle$. Expanding this gives $\\langle S, S \\rangle + \\langle S, N_2 \\rangle + \\langle N_1, S \\rangle + \\langle N_1, N_2 \\rangle$. Due to the statistical independence and zero-mean properties of the noise, the expectation of the cross-terms is zero: $E[\\langle S, N_2 \\rangle] = 0$, $E[\\langle N_1, S \\rangle] = 0$, and $E[\\langle N_1, N_2 \\rangle] = 0$. Therefore, on average, the correlation is driven solely by the signal term, $\\langle S, S \\rangle$. This is the principle of gold-standard Fourier Shell Correlation (FSC), which provides an unbiased estimate of resolution by eliminating correlation arising from fitted noise. The statement accurately describes this fundamental concept.\n\nVerdict: **Correct**.\n\n**B. A sufficiently aggressive low-pass filter applied during classification is, by itself, sufficient to prevent overfitting and model bias, regardless of the number of classes, the SNR, and the number of particles.**\n\nThis statement proposes that low-pass filtering is a panacea for overfitting. A low-pass filter removes high-frequency components from the images. Since noise is often dominant at high spatial frequencies, this can indeed mitigate overfitting by preventing the algorithm from fitting high-resolution noise. However, the claim that this is *sufficient by itself* and *regardless* of other critical parameters is a gross overstatement and is incorrect.\nOverfitting is a function of the balance between data quality/quantity and model complexity. The key factors are the SNR, the number of particles, and the number of free parameters in the model (e.g., the number of 3D classes).\nIf the SNR is extremely low and the number of particles is small, but the number of classes sought is large, the algorithm can still overfit to the *low-frequency components of the noise*. An aggressive low-pass filter simply restricts the overfitting to lower resolution features, but it does not eliminate it. For example, if one tries to sort 10000 very noisy particles into 50 classes, the algorithm will likely produce 50 distinct maps whose differences are driven by patterned noise, even if the data is heavily filtered. True features may be blurred with noise-driven artifacts. The filter is a regularization tool, not a guarantee against overfitting. The problem must be well-posed in the first place, with sufficient data to support the model complexity.\n\nVerdict: **Incorrect**.\n\n**C. To test whether 3D classification features are driven by data rather than initial-model bias, one can rerun the entire gold-standard half-set classification with different random seeds and initial models, and then assess whether corresponding classes exhibit consistent features and occupancies across halves and repeats. Stability across independent halves and repeats is evidence against model bias.**\n\nThis statement addresses the problem of initial model bias. Iterative refinement algorithms, like those used in cryo-EM, are susceptible to converging to local minima. If the initial model possesses strong features that are not actually present in the data, the algorithm may fail to move away from this starting point and instead reinforce these spurious features using the noise in the data. This is distinct from overfitting noise from a random start, but is an equally dangerous source of artifacts.\nThe protocol described is a robust method to diagnose and refute this bias. Protocol $\\mathrm{P2}$ (gold-standard half-set classification) already validates features against overfitting to noise *within a single run*. By re-running the *entire* P2 procedure multiple times from different starting points (e.g., different ab-initio models, or a filtered sphere with different random seeds for assigning initial parameters), one tests the robustness of the result. If the classification is driven by the true signal, $S$, in the data, then independent runs should converge to the same set of conformational states. Consistency of the resulting classes, their structural features, and their relative populations (occupancies) across these independent repeats provides strong evidence that the result is a stable, global solution dictated by the data, not a fragile, local solution dictated by the starting bias.\n\nVerdict: **Correct**.\n\n**D. If particles are randomly split into two halves only after the final optimization in a single-set classification, then the inter-half correlation will still be a valid gold-standard estimate of resolution and feature reliability, because the split ensures statistical independence.**\n\nThis statement describes a flawed procedure that violates the principle of cross-validation. The \"gold-standard\" method requires that the two datasets being compared are independent throughout the entire modeling process.\nIn the described protocol, a single-set classification is performed first. This means all parameters—importantly, the orientation and class assignment for *every single particle*—are determined using the full dataset $\\{S+N\\}$. During this optimization, the algorithm has access to all particles, and the final parameters for particle $i$ are influenced by the data from particle $j$ (for all $i, j$). This means the specific noise realization $N$ across the entire dataset has been used to determine the final parameter set for all particles.\nIf one then splits the particles into two halves *after* this has been done, the parameter assignments for the two halves are not independent. They were derived from a common optimization process that saw both halves of the data. This creates an \"information leak.\" When two maps are reconstructed from these post-facto halves, they will exhibit inflated correlation because the noise was already partially \"fit\" in a shared manner during the initial, single-set classification. The split does not retroactively create the statistical independence required for a valid cross-validation. The split must be performed *before* any step of the optimization that is to be validated.\n\nVerdict: **Incorrect**.\n\n**E. Performing per-particle motion correction and contrast transfer function (CTF) refinement on each half-set while sharing alignment parameters across halves preserves the statistical independence needed for unbiased inter-half validation.**\n\nThis statement tests a nuanced understanding of \"statistical independence.\" In the gold-standard P2 protocol, any step that involves parameter optimization must be performed independently on each half-set.\nPer-particle motion correction and CTF refinement are indeed parameter optimization steps. They refine parameters (e.g., particle shifts, defocus, beam-tilt) to maximize some quality metric of the data. The statement proposes doing this on each half-set separately, which is correct, but *while sharing alignment parameters across halves*. The act of \"sharing\" parameters means that information from one half-set is used to determine the model for the other half-set. For instance, if a global alignment parameter or a parameter of a beam-tilt model is refined using particles from *both* half-sets simultaneously, then the noise $N_1$ from the first half influences the model applied to the second half (containing noise $N_2$), and vice-versa. This explicitly breaks the statistical independence between the processing of the two halves. To maintain independence, *all* parameter refinement for half-set $1$ must use *only* data from half-set $1$, and similarly for half-set $2$. Any communication or parameter sharing between the halves during optimization constitutes an information leak and invalidates the \"gold-standard\" nature of the cross-validation.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "2571522"}]}