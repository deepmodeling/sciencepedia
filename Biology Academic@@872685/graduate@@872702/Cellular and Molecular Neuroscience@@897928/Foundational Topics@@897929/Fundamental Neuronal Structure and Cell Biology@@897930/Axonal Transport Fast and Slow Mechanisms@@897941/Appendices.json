{"hands_on_practices": [{"introduction": "A central puzzle in axonal transport is how cytoskeletal and cytosolic proteins, which constitute the slow components, advance at rates of only a few millimeters per day. The underlying molecular motors, however, operate at speeds orders of magnitude faster. The 'stop-and-go' model resolves this by proposing that cargos undergo rapid, short bursts of movement interspersed with very long pauses. This exercise [@problem_id:2699394] challenges you to quantify this effect, calculating the effective transport speed to see if this simple but powerful model can account for experimentally observed rates of slow component b (SCb).", "problem": "In the stop-and-go model of slow axonal transport, cargos in the slow component b (SCb) move anterogradely in repeated cycles consisting of a motile burst followed by a pause. Consider a single cargo that, during each cycle, moves at constant burst velocity $v_b$ for a burst duration $\\tau_b$, and then experiences an inter-burst pause of duration $\\tau_p$ during which it does not advance appreciably along the axon. Assume that cycles repeat identically and indefinitely, that retrograde runs are negligible on the timescale considered, and that diffusive displacement during pauses is negligible compared to directed runs.\n\nStarting only from the definition of average velocity $v = \\frac{dx}{dt}$ applied over many identical cycles, derive the effective long-time transport speed $v_{\\mathrm{eff}}$ of the cargo in terms of $v_b$, $\\tau_b$, and $\\tau_p$. Then evaluate $v_{\\mathrm{eff}}$ for $v_b = 1\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}$, $\\tau_b = 1\\,\\mathrm{s}$, and $\\tau_p = 100\\,\\mathrm{s}$. Express the effective speed in $\\mathrm{mm}\\,\\mathrm{day}^{-1}$ and round your answer to four significant figures.\n\nFinally, based on published measurements that place SCb transport in the range from $2$ to $8\\,\\mathrm{mm}\\,\\mathrm{day}^{-1}$ in many mammalian neurons, state whether the computed $v_{\\mathrm{eff}}$ is within, below, or above this range. The comparison is explanatory and does not affect the requested numerical answer.", "solution": "The long-time average velocity is defined by $v = \\frac{\\Delta x}{\\Delta t}$. Over one full cycle consisting of a burst and a pause, the displacement is the distance traversed during the burst and the elapsed time is the sum of the burst and pause durations.\n\nLet one cycle be indexed by $i$. During the burst of duration $\\tau_b$, the cargo advances a distance $\\Delta x_i = v_b \\tau_b$. During the pause of duration $\\tau_p$, we assume negligible net advance, so the total time per cycle is $\\Delta t_i = \\tau_b + \\tau_p$. Over many identical cycles, the average velocity equals the displacement per cycle divided by the time per cycle, because the process is renewal with identical increments. Thus,\n$$\nv_{\\mathrm{eff}} \\equiv \\lim_{N \\to \\infty} \\frac{\\sum_{i=1}^{N} \\Delta x_i}{\\sum_{i=1}^{N} \\Delta t_i}\n= \\frac{v_b \\tau_b}{\\tau_b + \\tau_p}.\n$$\n\nThis expression can also be interpreted as the burst velocity $v_b$ multiplied by the duty fraction of time spent moving, $\\frac{\\tau_b}{\\tau_b + \\tau_p}$.\n\nNow substitute the given values $v_b = 1\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}$, $\\tau_b = 1\\,\\mathrm{s}$, and $\\tau_p = 100\\,\\mathrm{s}$:\n$$\nv_{\\mathrm{eff}} = \\frac{(1\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1})(1\\,\\mathrm{s})}{1\\,\\mathrm{s} + 100\\,\\mathrm{s}} = \\frac{1}{101}\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}.\n$$\n\nConvert $\\frac{1}{101}\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1}$ to $\\mathrm{mm}\\,\\mathrm{day}^{-1}$. There are $86400\\,\\mathrm{s}$ in one day and $1000\\,\\mu\\mathrm{m}$ in $1\\,\\mathrm{mm}$, so\n$$\nv_{\\mathrm{eff}} = \\frac{1}{101}\\,\\mu\\mathrm{m}\\,\\mathrm{s}^{-1} \\times \\frac{86400\\,\\mathrm{s}}{1\\,\\mathrm{day}} \\times \\frac{1\\,\\mathrm{mm}}{1000\\,\\mu\\mathrm{m}}\n= \\frac{86400}{101 \\times 1000}\\,\\mathrm{mm}\\,\\mathrm{day}^{-1}.\n$$\nSimplify:\n$$\nv_{\\mathrm{eff}} = \\frac{86400}{101000}\\,\\mathrm{mm}\\,\\mathrm{day}^{-1} = \\frac{864}{1010}\\,\\mathrm{mm}\\,\\mathrm{day}^{-1} \\approx 0.8554455\\,\\mathrm{mm}\\,\\mathrm{day}^{-1}.\n$$\nRounded to four significant figures and expressed in $\\mathrm{mm}\\,\\mathrm{day}^{-1}$, the effective speed is $0.8554\\,\\mathrm{mm}\\,\\mathrm{day}^{-1}$.\n\nFor comparison to published SCb values, many studies report SCb transport of soluble proteins in the range from $2$ to $8\\,\\mathrm{mm}\\,\\mathrm{day}^{-1}$. The computed $v_{\\mathrm{eff}} \\approx 0.8554\\,\\mathrm{mm}\\,\\mathrm{day}^{-1}$ lies below this commonly reported range, consistent with the notion that long pauses substantially depress the time-averaged speed despite fast instantaneous runs.", "answer": "$$\\boxed{0.8554}$$", "id": "2699394"}, {"introduction": "The movement of vesicular cargo is often not unidirectional but characterized by a dynamic back-and-forth motion, regulated by the coordinated or competing activities of anterograde (kinesin) and retrograde (dynein) motors. This practice [@problem_id:2699392] delves into the mechanistic basis of this bidirectionality using a probabilistic 'tug-of-war' model. By calculating the steady-state probability of motor engagement, you will determine the likelihood that the net force generated by stochastically bound motors favors retrograde motion, providing a quantitative link between single-molecule kinetics and organelle-level transport decisions.", "problem": "A cargo in an axon is simultaneously coupled to cytoskeletal tracks by dynein and kinesin motors whose attachments fluctuate due to stochastic binding and unbinding. Consider a cargo that has a pool of $n_d=3$ cytoplasmic dyneins and $n_k=2$ kinesins available to bind. Each motor independently switches between an unbound state and a bound state according to a two-state continuous-time Markov process: when unbound, a motor attaches to the cargo-track complex with rate $k_{\\mathrm{on}}$; when bound, it detaches with rate $k_{\\mathrm{off},d}$ for dynein and $k_{\\mathrm{off},k}$ for kinesin. Assume equal attachment rates $k_{\\mathrm{on}}=1\\,\\mathrm{s}^{-1}$ for both motor types, with dynein detachment rate $k_{\\mathrm{off},d}=2\\,\\mathrm{s}^{-1}$ and kinesin detachment rate $k_{\\mathrm{off},k}=1\\,\\mathrm{s}^{-1}$. The motors act in a simple tug-of-war with unequal stall forces: dynein has stall force $F_d=7\\,\\mathrm{pN}$ and kinesin has stall force $F_k=6\\,\\mathrm{pN}$. Neglect any load-dependence of the attachment and detachment rates and any interactions between motors beyond the force balance described below. Work in the steady-state of the attachment-detachment dynamics, and assume that the numbers of bound dyneins and bound kinesins are independent across motors and across types.\n\nAdopt a winner-take-all criterion for direction at an observation time: if the number of bound dyneins is $i_d$ and the number of bound kinesins is $i_k$, the cargo moves retrograde (toward the soma) if and only if $F_d\\,i_d>F_k\\,i_k$; if $F_d\\,i_d=F_k\\,i_k$, treat the cargo as not moving retrograde.\n\nStarting from the master equation for a two-state continuous-time Markov process and basic probability, derive the steady-state distribution of bound motors for each type and use it to compute the probability that the cargo is moving retrograde at an arbitrary observation time under the assumptions above. Express your final answer as an exact fraction with no units and do not round.", "solution": "The problem asks for the steady-state probability that a cargo moves retrograde, driven by competing dynein and kinesin motors. The analysis will proceed in three steps: first, determining the steady-state probability for a single motor to be bound; second, deriving the probability distributions for the number of bound dyneins and kinesins; and third, summing the probabilities of all states that result in retrograde motion.\n\nA single motor is modeled as a two-state continuous-time Markov process with an unbound state ($U$) and a bound state ($B$). The transitions are $U \\xrightarrow{k_{\\mathrm{on}}} B$ and $B \\xrightarrow{k_{\\mathrm{off}}} U$. Let $P_B(t)$ and $P_U(t)$ be the probabilities of a motor being in the bound and unbound states at time $t$, respectively. The master equation for the bound state is:\n$$ \\frac{dP_B}{dt} = k_{\\mathrm{on}} P_U - k_{\\mathrm{off}} P_B $$\nThe total probability is conserved, $P_U(t) + P_B(t) = 1$. In the steady state, the probabilities are constant, so $\\frac{dP_B}{dt} = 0$. Let $p_b$ and $p_u$ denote the steady-state probabilities. The master equation becomes:\n$$ 0 = k_{\\mathrm{on}} p_u - k_{\\mathrm{off}} p_b $$\nSubstituting $p_u = 1 - p_b$, we have $k_{\\mathrm{on}}(1 - p_b) = k_{\\mathrm{off}} p_b$. Solving for $p_b$ gives:\n$$ k_{\\mathrm{on}} = (k_{\\mathrm{on}} + k_{\\mathrm{off}}) p_b \\implies p_b = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off}}} $$\nWe apply this general formula to both dynein and kinesin motors using the provided rate constants: $k_{\\mathrm{on}} = 1\\,\\mathrm{s}^{-1}$ for both, $k_{\\mathrm{off},d} = 2\\,\\mathrm{s}^{-1}$ for dynein, and $k_{\\mathrm{off},k} = 1\\,\\mathrm{s}^{-1}$ for kinesin.\n\nThe steady-state probability for a single dynein motor to be bound is:\n$$ p_{b,d} = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off},d}} = \\frac{1}{1 + 2} = \\frac{1}{3} $$\nThe steady-state probability for a single kinesin motor to be bound is:\n$$ p_{b,k} = \\frac{k_{\\mathrm{on}}}{k_{\\mathrm{on}} + k_{\\mathrm{off},k}} = \\frac{1}{1 + 1} = \\frac{1}{2} $$\nThe problem states that there are $n_d = 3$ dyneins and $n_k = 2$ kinesins available, and their binding/unbinding events are independent. Therefore, the number of bound motors of each type, denoted by the random variables $I_d$ and $I_k$, follows a binomial distribution.\n\nThe probability of having $i_d$ dyneins bound out of $n_d=3$ is given by $P(I_d=i_d) = \\binom{n_d}{i_d} (p_{b,d})^{i_d} (1-p_{b,d})^{n_d-i_d}$. With $p_{b,d} = 1/3$, the probabilities are:\n$$ P(I_d=0) = \\binom{3}{0}\\left(\\frac{1}{3}\\right)^0\\left(\\frac{2}{3}\\right)^3 = 1 \\cdot 1 \\cdot \\frac{8}{27} = \\frac{8}{27} $$\n$$ P(I_d=1) = \\binom{3}{1}\\left(\\frac{1}{3}\\right)^1\\left(\\frac{2}{3}\\right)^2 = 3 \\cdot \\frac{1}{3} \\cdot \\frac{4}{9} = \\frac{12}{27} $$\n$$ P(I_d=2) = \\binom{3}{2}\\left(\\frac{1}{3}\\right)^2\\left(\\frac{2}{3}\\right)^1 = 3 \\cdot \\frac{1}{9} \\cdot \\frac{2}{3} = \\frac{6}{27} $$\n$$ P(I_d=3) = \\binom{3}{3}\\left(\\frac{1}{3}\\right)^3\\left(\\frac{2}{3}\\right)^0 = 1 \\cdot \\frac{1}{27} \\cdot 1 = \\frac{1}{27} $$\nThe probability of having $i_k$ kinesins bound out of $n_k=2$ is given by $P(I_k=i_k) = \\binom{n_k}{i_k} (p_{b,k})^{i_k} (1-p_{b,k})^{n_k-i_k}$. With $p_{b,k} = 1/2$, the probabilities are:\n$$ P(I_k=0) = \\binom{2}{0}\\left(\\frac{1}{2}\\right)^0\\left(\\frac{1}{2}\\right)^2 = 1 \\cdot 1 \\cdot \\frac{1}{4} = \\frac{1}{4} $$\n$$ P(I_k=1) = \\binom{2}{1}\\left(\\frac{1}{2}\\right)^1\\left(\\frac{1}{2}\\right)^1 = 2 \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{2}{4} $$\n$$ P(I_k=2) = \\binom{2}{2}\\left(\\frac{1}{2}\\right)^2\\left(\\frac{1}{2}\\right)^0 = 1 \\cdot \\frac{1}{4} \\cdot 1 = \\frac{1}{4} $$\nThe criterion for retrograde motion is that the total force from bound dyneins exceeds that from bound kinesins: $F_d \\cdot i_d > F_k \\cdot i_k$. With stall forces $F_d = 7\\,\\mathrm{pN}$ and $F_k = 6\\,\\mathrm{pN}$, the condition is $7i_d > 6i_k$.\n\nThe total probability of retrograde motion, $P_{\\text{retrograde}}$, is the sum of probabilities of all states $(i_d, i_k)$ that satisfy this inequality. Since the binding of dynein and kinesin populations is independent, the joint probability is $P(I_d=i_d, I_k=i_k) = P(I_d=i_d)P(I_k=i_k)$.\n$$ P_{\\text{retrograde}} = \\sum_{i_d=0}^{3} \\sum_{i_k=0}^{2} \\mathbb{I}(7i_d > 6i_k) P(I_d=i_d)P(I_k=i_k) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. It is more efficient to calculate the probability of the complementary event, non-retrograde motion, which occurs when $7i_d \\le 6i_k$. We enumerate the states $(i_d, i_k)$ that satisfy this condition:\n- If $i_d = 0$: $0 \\le 6i_k$ is true for all $i_k \\in \\{0, 1, 2\\}$. States: $(0,0), (0,1), (0,2)$.\n- If $i_d = 1$: $7 \\le 6i_k \\implies i_k \\ge 7/6$. So $i_k=2$. State: $(1,2)$.\n- If $i_d = 2$: $14 \\le 6i_k \\implies i_k \\ge 14/6 \\approx 2.33$. No possible $i_k$.\n- If $i_d = 3$: $21 \\le 6i_k \\implies i_k \\ge 21/6 = 3.5$. No possible $i_k$.\n\nThe set of states for non-retrograde motion is $\\{(0,0), (0,1), (0,2), (1,2)\\}$. The probability $P_{\\text{non-retrograde}}$ is:\n$$ P_{\\text{non-retrograde}} = P(I_d=0, I_k=0) + P(I_d=0, I_k=1) + P(I_d=0, I_k=2) + P(I_d=1, I_k=2) $$\nThis can be factored as:\n$$ P_{\\text{non-retrograde}} = P(I_d=0) \\left[ P(I_k=0) + P(I_k=1) + P(I_k=2) \\right] + P(I_d=1)P(I_k=2) $$\nSince $\\sum_{i_k=0}^{2} P(I_k=i_k) = 1$, this simplifies to:\n$$ P_{\\text{non-retrograde}} = P(I_d=0) + P(I_d=1)P(I_k=2) $$\nSubstituting the calculated probabilities:\n$$ P_{\\text{non-retrograde}} = \\frac{8}{27} + \\left(\\frac{12}{27}\\right)\\left(\\frac{1}{4}\\right) = \\frac{8}{27} + \\frac{3}{27} = \\frac{11}{27} $$\nThe probability of retrograde motion is $P_{\\text{retrograde}} = 1 - P_{\\text{non-retrograde}}$.\n$$ P_{\\text{retrograde}} = 1 - \\frac{11}{27} = \\frac{16}{27} $$\nTo verify, we can compute the sum directly. The states for retrograde motion are all other states: $(1,0), (1,1)$, $(2,0), (2,1), (2,2)$, $(3,0), (3,1), (3,2)$.\n$P(i_d=1) \\left[P(i_k=0)+P(i_k=1)\\right] = \\frac{12}{27}\\left(\\frac{1}{4}+\\frac{2}{4}\\right) = \\frac{12}{27} \\cdot \\frac{3}{4} = \\frac{9}{27}$.\n$P(i_d=2) \\left[P(i_k=0)+P(i_k=1)+P(i_k=2)\\right] = P(i_d=2) \\cdot 1 = \\frac{6}{27}$.\n$P(i_d=3) \\left[P(i_k=0)+P(i_k=1)+P(i_k=2)\\right] = P(i_d=3) \\cdot 1 = \\frac{1}{27}$.\nSumming these gives:\n$$ P_{\\text{retrograde}} = \\frac{9}{27} + \\frac{6}{27} + \\frac{1}{27} = \\frac{16}{27} $$\nBoth methods yield the same result. The calculation is consistent.", "answer": "$$\\boxed{\\frac{16}{27}}$$", "id": "2699392"}, {"introduction": "Live-cell imaging reveals a complex tapestry of cargo movements, where velocity distributions often reflect a mixture of different transport states, such as the slow, intermittent bursts of SCb and the sustained runs of fast axonal transport. To quantitatively analyze these processes, we must statistically disentangle their contributions from raw data. In this advanced computational practice [@problem_id:2699441], you will implement the Expectation-Maximization (EM) algorithm to fit a two-component mixture model to simulated velocity histograms, a powerful technique for estimating the fraction of cargo belonging to each transport population.", "problem": "You are given histograms of instantaneous axonal cargo velocities measured in micrometers per second, arising from two mechanistic populations in axonal transport: slow, intermittent bursts characteristic of Slow Component b (SCb) and sustained fast runs. A physically realistic and statistically appropriate assumption is that strictly positive velocities are distributed as a mixture of two log-normal populations corresponding to an SCb-like low-velocity component and a fast high-velocity component. The measured histograms display peaks near $0.05$ and $1.2$ micrometers per second.\n\nUsing only the following foundational principles and definitions, derive and implement a general-purpose estimator for the fraction of SCb-like bursts in the population:\n- Independence of observations allows factorization of the likelihood.\n- The mixture model definition states that the marginal probability density of an observation is a convex combination of component densities.\n- The log-normal probability density function with log-space parameters $(\\mu,\\sigma)$ for a positive velocity $v$ is $f_{\\mathrm{LN}}(v\\mid\\mu,\\sigma)=\\dfrac{1}{v\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\dfrac{(\\ln v-\\mu)^{2}}{2\\sigma^{2}}\\right)$ for $v&gt;0$.\n- The Expectation-Maximization (EM) principle for finite mixtures maximizes the likelihood by alternately computing responsibilities in the expectation step and maximizing expected complete-data log-likelihood in the maximization step; the maximization for Gaussian families in the natural parameter space reduces to weighted means and variances.\n\nYou must treat histogram bin centers as observations with integer weights equal to histogram counts. For a two-component mixture of log-normals with mixture weight $\\pi\\in(0,1)$ for the SCb-like component, log-space parameters $(\\mu_{1},\\sigma_{1})$ for SCb-like velocities and $(\\mu_{2},\\sigma_{2})$ for fast velocities, derive from first principles the EM updates in terms of the weighted data $(v_{i},w_{i})$, ensuring that the SCb-like component corresponds to the lower log-mean, that is, enforce $\\mu_{1}\\le \\mu_{2}$ at every iteration by swapping labels if necessary. Initialize with $\\mu_{1}=\\ln 0.05$, $\\mu_{2}=\\ln 1.2$, $\\sigma_{1}=\\sigma_{2}=0.5$, and $\\pi=0.5$. Use a stopping criterion based on the Euclidean norm of parameter changes falling below $10^{-8}$ or a maximum of $1000$ iterations, whichever occurs first. To avoid degenerate solutions, constrain $\\sigma_{1}$ and $\\sigma_{2}$ to be at least $0.05$.\n\nCompute and return the estimated SCb-like fraction $\\pi$ for each of the following test histograms. Each histogram shares the same bin centers $v_{i}$ (in micrometers per second) and differs in the integer counts $w_{i}$. The unit for velocities is micrometers per second, but you must report only the estimated fraction $\\pi$, which is dimensionless. Report each $\\pi$ as a decimal rounded to three places.\n\nBin centers $v_{i}$ (micrometers per second):\n$[0.02, 0.03, 0.04, 0.05, 0.065, 0.08, 0.10, 0.15, 0.25, 0.40, 0.60, 0.80, 1.00, 1.20, 1.40, 1.70, 2.00]$.\n\nTest suite of histograms (each is a list of counts aligned with the bin centers):\n- Case A (balanced mixture with two visible peaks near $0.05$ and $1.2$): $[3, 8, 15, 28, 22, 16, 12, 8, 6, 5, 5, 8, 14, 26, 18, 10, 6]$.\n- Case B (SCb-dominated; stronger low-velocity peak): $[6, 14, 28, 45, 30, 20, 14, 8, 5, 3, 2, 3, 4, 6, 4, 3, 2]$.\n- Case C (fast-dominated; stronger high-velocity peak): $[1, 2, 3, 6, 5, 4, 4, 5, 6, 8, 12, 16, 24, 38, 30, 20, 12]$.\n- Case D (sparser, noisier data; both peaks present but with few counts): $[0, 1, 2, 3, 2, 1, 1, 0, 1, 1, 2, 3, 5, 7, 5, 2, 1]$.\n\nYour program must:\n- Implement the expectation-maximization estimator for the two-component log-normal mixture as described, with weighted data based on the histogram counts, and with the SCb-like component defined as the lower-velocity component via $\\mu_{1}\\le\\mu_{2}$.\n- Apply the estimator to each of the four cases in the test suite.\n- Produce a single line of output containing the four estimated SCb-like fractions, rounded to three places, as a comma-separated list enclosed in square brackets, for example, $[0.412,0.735,0.113,0.508]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0.412,0.735,0.113,0.508]$).", "solution": "The problem requires the derivation and implementation of an Expectation-Maximization (EM) algorithm to estimate the mixing proportion, $\\pi$, of a two-component log-normal mixture model. This model describes the distribution of axonal cargo velocities, which are provided as histogram data. The data are represented as pairs of bin centers $(v_i)$ and counts $(w_i)$. The analysis must be derived from first principles.\n\nLet the two components of the mixture be denoted by $k=1$ (SCb-like, slow) and $k=2$ (fast). The parameters of the model are $\\theta = (\\pi, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2)$, where $\\pi$ is the mixture proportion for component $1$, and $(\\mu_k, \\sigma_k)$ are the mean and standard deviation of the logarithm of the velocity for component $k$. The probability density function (PDF) of the log-normal distribution is given as:\n$$ f_{\\mathrm{LN}}(v \\mid \\mu, \\sigma) = \\frac{1}{v\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln v - \\mu)^2}{2\\sigma^2}\\right) $$\nThe total probability density for an observation $v$ is a mixture of two such components:\n$$ p(v \\mid \\theta) = \\pi f_{\\mathrm{LN}}(v \\mid \\mu_1, \\sigma_1) + (1-\\pi) f_{\\mathrm{LN}}(v \\mid \\mu_2, \\sigma_2) $$\nThe data are provided as a histogram, which we treat as $M$ distinct velocity values $v_i$, each observed $w_i$ times. The total number of observations is $N = \\sum_{i=1}^M w_i$. The log-likelihood of the observed data is:\n$$ \\mathcal{L}(\\theta) = \\ln \\prod_{i=1}^M [p(v_i \\mid \\theta)]^{w_i} = \\sum_{i=1}^M w_i \\ln\\left[ \\pi f_{\\mathrm{LN}}(v_i \\mid \\mu_1, \\sigma_1) + (1-\\pi) f_{\\mathrm{LN}}(v_i \\mid \\mu_2, \\sigma_2) \\right] $$\nDirect maximization of $\\mathcal{L}(\\theta)$ is complicated by the sum inside the logarithm. The EM algorithm circumvents this by introducing latent variables. Let $z_{ik}$ be an indicator variable which is $1$ if an observation from bin $i$ belongs to component $k$, and $0$ otherwise.\n\nThe complete-data log-likelihood, assuming the values of the latent variables are known, is:\n$$ \\mathcal{L}_c(\\theta; v, w, z) = \\sum_{i=1}^M w_i \\sum_{k=1}^2 z_{ik} \\ln \\left[ \\pi_k f_{\\mathrm{LN}}(v_i \\mid \\mu_k, \\sigma_k) \\right] $$\n$$ = \\sum_{i=1}^M w_i \\sum_{k=1}^2 z_{ik} \\left( \\ln(\\pi_k) + \\ln(f_{\\mathrm{LN}}(v_i \\mid \\mu_k, \\sigma_k)) \\right) $$\nwhere $\\pi_1 = \\pi$ and $\\pi_2 = 1-\\pi$.\n\nThe EM algorithm is an iterative procedure consisting of two steps:\n\n**E-Step (Expectation):**\nIn this step, we compute the expected value of the complete-data log-likelihood, conditioned on the observed data $v$ and the current parameter estimates $\\theta^{(t)}$. This is equivalent to computing the posterior probability that an observation $v_i$ belongs to component $k$, known as the responsibility, $\\gamma_{ik}$.\n$$ \\gamma_{ik} = E[z_{ik} \\mid v_i, \\theta^{(t)}] = P(k \\mid v_i, \\theta^{(t)}) = \\frac{p(v_i \\mid k, \\theta^{(t)}) P(k \\mid \\theta^{(t)})}{p(v_i \\mid \\theta^{(t)})} $$\nSubstituting the model definitions:\n$$ \\gamma_{i1}^{(t+1)} = \\frac{\\pi^{(t)} f_{\\mathrm{LN}}(v_i \\mid \\mu_1^{(t)}, \\sigma_1^{(t)})}{\\pi^{(t)} f_{\\mathrm{LN}}(v_i \\mid \\mu_1^{(t)}, \\sigma_1^{(t)}) + (1-\\pi^{(t)}) f_{\\mathrm{LN}}(v_i \\mid \\mu_2^{(t)}, \\sigma_2^{(t)})} $$\nAnd $\\gamma_{i2}^{(t+1)} = 1 - \\gamma_{i1}^{(t+1)}$. The function to be maximized in the M-step, the $Q$-function, is the expectation of $\\mathcal{L}_c$ with these responsibilities:\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^M w_i \\sum_{k=1}^2 \\gamma_{ik}^{(t+1)} \\left[ \\ln(\\pi_k) + \\ln(f_{\\mathrm{LN}}(v_i \\mid \\mu_k, \\sigma_k)) \\right] $$\n\n**M-Step (Maximization):**\nIn this step, we find the parameters $\\theta^{(t+1)}$ that maximize the $Q$-function. We can optimize for $\\pi_k$ and $(\\mu_k, \\sigma_k)$ independently.\n\n1.  **Update for $\\pi_k$**: We maximize $\\sum_{i=1}^M w_i \\sum_{k=1}^2 \\gamma_{ik}^{(t+1)} \\ln(\\pi_k)$ subject to $\\sum_k \\pi_k = 1$. Using a Lagrange multiplier, this yields the intuitive result that the new mixing proportion for component $k$ is the average responsibility of that component over all observations:\n    $$ \\pi_k^{(t+1)} = \\frac{\\sum_{i=1}^M w_i \\gamma_{ik}^{(t+1)}}{\\sum_{i=1}^M w_i} = \\frac{\\sum_{i=1}^M w_i \\gamma_{ik}^{(t+1)}}{N} $$\n    For $k=1$, this gives the update for $\\pi$: $\\pi^{(t+1)} = \\frac{\\sum_i w_i \\gamma_{i1}^{(t+1)}}{N}$.\n\n2.  **Update for $\\mu_k$ and $\\sigma_k$**: We must maximize the term involving the log-normal PDF. It is simpler to work with the log-transformed velocities $x_i = \\ln(v_i)$. If $v$ follows a log-normal distribution with parameters $(\\mu, \\sigma)$, then $x = \\ln(v)$ follows a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. The term to maximize for each component $k$ is:\n    $$ \\sum_{i=1}^M w_i \\gamma_{ik}^{(t+1)} \\ln(f_{\\mathrm{LN}}(v_i \\mid \\mu_k, \\sigma_k)) = \\sum_{i=1}^M w_i \\gamma_{ik}^{(t+1)} \\left[ -\\ln(v_i) - \\ln(\\sigma_k) - \\frac{1}{2}\\ln(2\\pi) - \\frac{(\\ln(v_i) - \\mu_k)^2}{2\\sigma_k^2} \\right] $$\n    Maximizing this with respect to $\\mu_k$ and $\\sigma_k$ is equivalent to a weighted maximum likelihood estimation for a normal distribution with data points $x_i = \\ln(v_i)$ and weights $w_i \\gamma_{ik}^{(t+1)}$. The well-known solutions are the weighted mean and weighted standard deviation.\n    Let $W_k = \\sum_{i=1}^M w_i \\gamma_{ik}^{(t+1)}$ be the effective total weight for component $k$.\n    $$ \\mu_k^{(t+1)} = \\frac{1}{W_k} \\sum_{i=1}^M w_i \\gamma_{ik}^{(t+1)} \\ln(v_i) $$\n    $$ (\\sigma_k^{(t+1)})^2 = \\frac{1}{W_k} \\sum_{i=1}^M w_i \\gamma_{ik}^{(t+1)} (\\ln(v_i) - \\mu_k^{(t+1)})^2 $$\n\n**Algorithm Summary:**\n1.  **Initialization**: Set initial parameters $\\theta^{(0)} = (\\pi^{(0)}, \\mu_1^{(0)}, \\sigma_1^{(0)}, \\mu_2^{(0)}, \\sigma_2^{(0)})$ as given: $\\pi^{(0)}=0.5$, $\\mu_1^{(0)}=\\ln(0.05)$, $\\mu_2^{(0)}=\\ln(1.2)$, $\\sigma_1^{(0)}=0.5$, $\\sigma_2^{(0)}=0.5$.\n2.  **Iteration**: For $t=0, 1, 2, \\dots$ until convergence:\n    a.  **E-Step**: Compute responsibilities $\\gamma_{i1}^{(t+1)}$ and $\\gamma_{i2}^{(t+1)}$ for all $i=1, \\dots, M$ using the formulas derived above with parameters $\\theta^{(t)}$. For numerical stability, computations are best performed in log-space using the log-sum-exp trick to avoid underflow/overflow.\n    b.  **M-Step**: Update parameters to $\\theta^{(t+1)}$ using the weighted-data update rules derived above.\n    c.  **Constraints**: Apply the required constraints. Enforce $\\sigma_k^{(t+1)} \\ge 0.05$. To maintain the identity of the slow and fast components, enforce $\\mu_1^{(t+1)} \\le \\mu_2^{(t+1)}$. If this condition is violated, swap the parameters $(\\mu_1, \\sigma_1)$ with $(\\mu_2, \\sigma_2)$ and update $\\pi$ to $1-\\pi$.\n    d.  **Convergence Check**: Calculate the Euclidean norm of the change in the parameter vector, $\\| \\theta^{(t+1)} - \\theta^{(t)} \\|_2$. If the norm is less than $10^{-8}$, or if the number of iterations exceeds $1000$, terminate the algorithm.\n\nThe final estimated value of $\\pi$ is the result for each histogram case.", "answer": "[0.528,0.763,0.298,0.407]", "id": "2699441"}]}