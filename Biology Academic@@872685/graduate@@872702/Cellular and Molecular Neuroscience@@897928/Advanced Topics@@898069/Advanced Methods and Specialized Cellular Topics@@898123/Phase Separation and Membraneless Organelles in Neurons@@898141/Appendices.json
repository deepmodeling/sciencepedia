{"hands_on_practices": [{"introduction": "A core feature of liquid-liquid phase separation is the selective enrichment of specific biomolecules within the resulting condensates. This partitioning can be quantified by the partition coefficient, $K$, defined as the ratio of a component's concentration inside the dense phase to that in the surrounding dilute phase. This exercise [@problem_id:2737960] guides you through a realistic data analysis workflow to determine $K$ from fluorescence microscopy data, a fundamental skill for researchers studying the composition and thermodynamics of membraneless organelles.", "problem": "In cultured hippocampal neurons, an RNA-binding protein (RBP) fused to Green Fluorescent Protein (GFP) forms stress-induced condensates through liquid-liquid phase separation. Under fixed imaging conditions within the linear dynamic range of the detector, the background-corrected fluorescence intensity is proportional to the GFP-tagged RBP concentration. The partition coefficient is defined as the ratio of the absolute protein concentration inside the dense condensate phase to that in the surrounding dilute cytoplasm.\n\nYou obtain the following calibration data in buffer under the same imaging settings, together with a background measurement from a blank region of the field of view. All intensities are detector counts, and concentrations are in micromolar (µM).\n\n- Blank background intensity: $80$.\n- Known concentrations and corresponding raw mean intensities (field-averaged): at $1.0$ micromolar the raw intensity is $1030$; at $5.0$ micromolar the raw intensity is $4880$; at $10.0$ micromolar the raw intensity is $9680$.\n\nAssume that after subtracting the background, the calibration relationship between corrected intensity and concentration is linear and passes through the origin. Estimate the proportionality constant by least squares under this zero-intercept assumption.\n\nYou then image a neuron in which the same RBP-GFP has formed condensates. In this image, you measure:\n\n- Background from a cell-free region: $120$.\n- Raw mean intensity within a condensate: $13800$.\n- Raw mean intensity in nearby dilute cytoplasm: $1120$.\n\nUsing only the principles stated above, first correct all intensities for background appropriately, then calibrate the condensate and dilute-phase intensities to absolute concentrations, and finally compute the partition coefficient $K$ defined as the dense-phase concentration divided by the dilute-phase concentration. Report $K$ as a dimensionless number, rounded to four significant figures.", "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. We shall proceed directly to its solution.\n\nThe problem states that background-corrected fluorescence intensity, which we shall denote as $I_{\\text{corr}}$, is proportional to the protein concentration, $C$. This establishes a linear relationship passing through the origin:\n$$I_{\\text{corr}} = \\alpha C$$\nHere, $\\alpha$ is the proportionality constant that we must determine from the provided calibration data. The method specified is least squares for a model with a zero intercept. For a set of data points $(C_i, I_{\\text{corr},i})$, the least squares estimate for $\\alpha$ in the model $I_{\\text{corr},i} = \\alpha C_i$ is given by:\n$$\\alpha = \\frac{\\sum_{i} C_i I_{\\text{corr},i}}{\\sum_{i} C_i^2}$$\nFirst, we must process the calibration data to obtain the corrected intensities. The raw intensity values must be corrected by subtracting the specified background for the calibration measurement, $I_{\\text{bg,cal}} = 80$. The provided calibration data points $(C_i, I_{\\text{raw},i})$ are:\n- ($C_1 = 1.0$ µM, $I_{\\text{raw},1} = 1030$)\n- ($C_2 = 5.0$ µM, $I_{\\text{raw},2} = 4880$)\n- ($C_3 = 10.0$ µM, $I_{\\text{raw},3} = 9680$)\n\nThe corresponding background-corrected intensities are:\n- $I_{\\text{corr},1} = I_{\\text{raw},1} - I_{\\text{bg,cal}} = 1030 - 80 = 950$\n- $I_{\\text{corr},2} = I_{\\text{raw},2} - I_{\\text{bg,cal}} = 4880 - 80 = 4800$\n- $I_{\\text{corr},3} = I_{\\text{raw},3} - I_{\\text{bg,cal}} = 9680 - 80 = 9600$\n\nNow, we compute the necessary sums for the least squares calculation.\nThe sum of the products of concentration and corrected intensity is:\n$$\\sum_{i=1}^{3} C_i I_{\\text{corr},i} = (1.0)(950) + (5.0)(4800) + (10.0)(9600) = 950 + 24000 + 96000 = 120950$$\nThe sum of the squared concentrations is:\n$$\\sum_{i=1}^{3} C_i^2 = (1.0)^2 + (5.0)^2 + (10.0)^2 = 1.0 + 25.0 + 100.0 = 126.0$$\nThe proportionality constant $\\alpha$ is the ratio of these sums:\n$$\\alpha = \\frac{120950}{126.0}$$\nWe will leave this as an exact fraction to avoid premature rounding.\n\nNext, we analyze the experimental measurements from the neuron. The background in this experiment is given as $I_{\\text{bg,exp}} = 120$. The raw intensities are $I_{\\text{raw,dense}} = 13800$ for the condensate and $I_{\\text{raw,dilute}} = 1120$ for the dilute cytoplasm.\nWe correct these intensities using the appropriate background value:\n- Corrected dense phase intensity: $I_{\\text{corr,dense}} = I_{\\text{raw,dense}} - I_{\\text{bg,exp}} = 13800 - 120 = 13680$\n- Corrected dilute phase intensity: $I_{\\text{corr,dilute}} = I_{\\text{raw,dilute}} - I_{\\text{bg,exp}} = 1120 - 120 = 1000$\n\nThe concentrations in the dense ($C_{\\text{dense}}$) and dilute ($C_{\\text{dilute}}$) phases are found using the determined constant $\\alpha$ and the relation $C = I_{\\text{corr}}/\\alpha$:\n$$C_{\\text{dense}} = \\frac{I_{\\text{corr,dense}}}{\\alpha} = \\frac{13680}{\\frac{120950}{126.0}} = \\frac{13680 \\times 126.0}{120950}$$\n$$C_{\\text{dilute}} = \\frac{I_{\\text{corr,dilute}}}{\\alpha} = \\frac{1000}{\\frac{120950}{126.0}} = \\frac{1000 \\times 126.0}{120950}$$\nThe partition coefficient, $K$, is defined as the ratio of these concentrations:\n$$K = \\frac{C_{\\text{dense}}}{C_{\\text{dilute}}}$$\nSubstituting the expressions for the concentrations in terms of intensities, we see that the constant $\\alpha$ cancels out:\n$$K = \\frac{I_{\\text{corr,dense}}/\\alpha}{I_{\\text{corr,dilute}}/\\alpha} = \\frac{I_{\\text{corr,dense}}}{I_{\\text{corr,dilute}}}$$\nWe can now compute the value of $K$ directly from the ratio of the corrected intensities:\n$$K = \\frac{13680}{1000} = 13.68$$\nThe problem requests the result to be reported to four significant figures. The calculated value of $13.68$ is already in this form.", "answer": "$$\\boxed{13.68}$$", "id": "2737960"}, {"introduction": "Many neuronal condensates exhibit liquid-like material properties, a characteristic vividly demonstrated when two droplets fuse and relax into a single sphere. This process is governed by a competition between interfacial tension ($\\gamma$), which drives the system toward a minimal surface area, and the internal viscosity ($\\eta$), which resists flow. In this problem [@problem_id:2737980], you will use scaling analysis to derive the characteristic capillary relaxation timescale, $\\tau_{\\text{cap}}$, providing a powerful link between macroscopic observable dynamics and the underlying biophysical properties of the condensate.", "problem": "Neuronal ribonucleoprotein condensates formed by liquid-liquid phase separation (LLPS) behave as viscous liquid droplets in the cytoplasm. Consider the post-fusion shape relaxation of a nearly spherical droplet in a dendrite, where the dynamics are governed by the competition between capillarity and viscosity at low Reynolds number. Assume a Newtonian internal rheology and negligible inertia. Use the following foundational principles only: (i) the Laplace pressure scale associated with interfacial tension is set by curvature, so that the capillary driving pressure is on the order of $\\Delta p \\sim \\gamma / R$ for a droplet of radius $R$, and (ii) the viscous stress scale in a flow with velocity scale $U$ over length scale $R$ is $\\sigma_{v} \\sim \\eta U / R$, where $\\eta$ is the viscosity. Take the characteristic shape-relaxation timescale $\\tau$ to be set by $U \\sim R/\\tau$. \n\nA live-cell imaging experiment on a dendritic condensate reports a post-fusion exponential relaxation with an observed characteristic time $t_{\\mathrm{obs}} = 2.70\\,\\mathrm{s}$. Independent mechanical measurements on the same condensate composition yield an interfacial tension $\\gamma = 3.0 \\times 10^{-6}\\,\\mathrm{N\\,m^{-1}}$ and a viscosity $\\eta = 9.0\\,\\mathrm{Pa\\cdot s}$ at the imaging temperature. The droplet radius at the onset of relaxation is $R = 0.60\\,\\mu\\mathrm{m}$.\n\nStarting from the stated principles and no other formulas, derive an expression for the capillary relaxation timescale $\\tau_{\\mathrm{cap}}$ in terms of $\\gamma$, $\\eta$, and $R$ up to an order-unity prefactor, then compute its numerical value in seconds. Finally, compute the dimensionless ratio \n$$\\chi = \\frac{t_{\\mathrm{obs}}}{\\tau_{\\mathrm{cap}}}.$$\nRound your final reported value of $\\chi$ to three significant figures. Report only the value of $\\chi$ as your final answer.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the principles of low-Reynolds-number fluid dynamics applied to a well-established biological phenomenon. The problem is self-contained, well-posed, and all provided data are physically realistic and consistent. We may therefore proceed with the solution.\n\nThe problem asks for the derivation of the characteristic capillary relaxation timescale, $\\tau_{\\mathrm{cap}}$, for a viscous droplet based on fundamental principles of scaling. The dynamics are stated to be in a regime where inertia is negligible (low Reynolds number) and are governed by a competition between capillarity, which drives the system toward a spherical shape to minimize surface energy, and viscosity, which resists this deformation. This competition implies a balance between the capillary driving pressure and the viscous resisting stress.\n\nWe are given the following scaling relations:\n$1$. The capillary pressure scale, $\\Delta p$, associated with an interfacial tension $\\gamma$ and a radius of curvature $R$ is:\n$$ \\Delta p \\sim \\frac{\\gamma}{R} $$\n$2$. The viscous stress scale, $\\sigma_{v}$, in a flow with velocity scale $U$ and length scale $R$, for a fluid of viscosity $\\eta$ is:\n$$ \\sigma_v \\sim \\frac{\\eta U}{R} $$\nThe balance between these two effects, $\\Delta p \\sim \\sigma_v$, dictates the dynamics. Therefore, we equate the two scaling laws:\n$$ \\frac{\\gamma}{R} \\sim \\frac{\\eta U}{R} $$\nThe problem further provides a scaling relationship between the characteristic velocity $U$ and the relaxation timescale $\\tau$. For a deformation of size $R$ relaxing over a time $\\tau$, the velocity scale is naturally $U \\sim R / \\tau$. We designate the theoretical timescale derived from this analysis as $\\tau_{\\mathrm{cap}}$. Thus, we have:\n$$ U \\sim \\frac{R}{\\tau_{\\mathrm{cap}}} $$\nSubstituting this expression for $U$ into the pressure-stress balance equation yields:\n$$ \\frac{\\gamma}{R} \\sim \\frac{\\eta}{R} \\left( \\frac{R}{\\tau_{\\mathrm{cap}}} \\right) $$\nSimplifying this expression gives:\n$$ \\frac{\\gamma}{R} \\sim \\frac{\\eta}{\\tau_{\\mathrm{cap}}} $$\nWe can now rearrange this relation to solve for the capillary relaxation timescale $\\tau_{\\mathrm{cap}}$. This provides the required expression up to a dimensionless prefactor of order unity, as requested:\n$$ \\tau_{\\mathrm{cap}} \\sim \\frac{\\eta R}{\\gamma} $$\nFor the purpose of numerical estimation based on this scaling analysis, we will take the dimensionless prefactor to be $1$.\n$$ \\tau_{\\mathrm{cap}} = \\frac{\\eta R}{\\gamma} $$\nNext, we compute the numerical value of $\\tau_{\\mathrm{cap}}$ using the provided experimental parameters:\nInterfacial tension, $\\gamma = 3.0 \\times 10^{-6}\\,\\mathrm{N\\,m^{-1}}$.\nViscosity, $\\eta = 9.0\\,\\mathrm{Pa\\cdot s}$.\nDroplet radius, $R = 0.60\\,\\mu\\mathrm{m}$.\nFor dimensional consistency, we must express all quantities in base SI units. The radius must be converted from micrometers to meters:\n$$ R = 0.60\\,\\mu\\mathrm{m} = 0.60 \\times 10^{-6}\\,\\mathrm{m} $$\nNow, we substitute these values into our derived expression for $\\tau_{\\mathrm{cap}}$:\n$$ \\tau_{\\mathrm{cap}} = \\frac{(9.0\\,\\mathrm{Pa\\cdot s})(0.60 \\times 10^{-6}\\,\\mathrm{m})}{3.0 \\times 10^{-6}\\,\\mathrm{N\\,m^{-1}}} $$\nThe units are consistent, as $\\mathrm{Pa} = \\mathrm{N\\,m^{-2}}$, so $\\frac{(\\mathrm{Pa\\cdot s})(\\mathrm{m})}{\\mathrm{N\\,m^{-1}}} = \\frac{(\\mathrm{N\\,m^{-2}\\cdot s})(\\mathrm{m})}{\\mathrm{N\\,m^{-1}}} = \\frac{\\mathrm{N\\,m^{-1}\\cdot s}}{\\mathrm{N\\,m^{-1}}} = \\mathrm{s}$.\nPerforming the calculation:\n$$ \\tau_{\\mathrm{cap}} = \\frac{9.0 \\times 0.60 \\times 10^{-6}}{3.0 \\times 10^{-6}} \\,\\mathrm{s} = \\frac{5.4}{3.0}\\,\\mathrm{s} = 1.8\\,\\mathrm{s} $$\nFinally, we are asked to compute the dimensionless ratio $\\chi$, defined as the ratio of the observed relaxation time, $t_{\\mathrm{obs}}$, to our calculated capillary timescale, $\\tau_{\\mathrm{cap}}$.\n$$ \\chi = \\frac{t_{\\mathrm{obs}}}{\\tau_{\\mathrm{cap}}} $$\nWe are given $t_{\\mathrm{obs}} = 2.70\\,\\mathrm{s}$. Using our calculated value for $\\tau_{\\mathrm{cap}}$:\n$$ \\chi = \\frac{2.70\\,\\mathrm{s}}{1.8\\,\\mathrm{s}} = \\frac{270}{180} = \\frac{27}{18} = \\frac{3}{2} = 1.5 $$\nThe problem explicitly instructs to round the final reported value of $\\chi$ to three significant figures. The exact value of $1.5$ is written as $1.50$ to satisfy this requirement.", "answer": "$$\n\\boxed{1.50}\n$$", "id": "2737980"}, {"introduction": "While we can describe the bulk properties of condensates, their internal environments are far from simple, often comprising components with vastly different mobilities. Fluorescence Recovery After Photobleaching (FRAP) is an essential technique for dissecting these complex internal dynamics. This practice [@problem_id:2737978] introduces a computational approach to FRAP analysis, where you will use statistical model selection to distinguish between rapidly exchanging \"client\" molecules and slow-moving \"scaffold\" components, revealing the multi-layered kinetic organization within a condensate.", "problem": "You are given a computational task grounded in Fluorescence Recovery After Photobleaching (FRAP) of postsynaptic density (PSD) components in neurons, where the PSD behaves as a phase-separated condensate whose mesoscale material properties emerge from molecular exchange. A fundamental base for FRAP recovery modeling in well-mixed condensates is that recovery to a plateau can be described by first-order relaxation dynamics. For a single kinetic subpopulation that exchanges with a large reservoir, the recovery follows a single exponential relaxation. When two kinetically independent subpopulations coexist (for example, a scaffold-like subpopulation exchanging slowly and a client-like subpopulation exchanging rapidly), the recovery becomes the sum of two exponentials, each with its own amplitude and characteristic time constant. Your task is to implement model selection between single- and bi-exponential recovery functions for FRAP time courses and to interpret the fitted timescales in terms of scaffold–client interactions.\n\nUse the following definitions and constraints as the fundamental base:\n- Fluorescence recovery after an instantaneous bleach can be modeled as a relaxation back to a plateau due to exchange and diffusion, and for a single first-order process, the time-dependent recovery of normalized intensity is well described by a single exponential function. For two independent first-order processes, the recovery is the sum of two exponentials. Let the normalized intensity be $I(t)$, with plateau $C$ and amplitudes $A_i \\ge 0$ and time constants $\\tau_i  0$. Then:\n  - Single-exponential model (one dynamic subpopulation): $I(t) = C - A_1 e^{-t/\\tau_1}$.\n  - Bi-exponential model (two dynamic subpopulations): $I(t) = C - A_1 e^{-t/\\tau_1} - A_2 e^{-t/\\tau_2}$.\n- The corrected Akaike Information Criterion (AICc) for a model with $k$ free parameters fitted to $n$ data points with residual sum of squares $\\mathrm{RSS}$ is:\n  $$\\mathrm{AIC} = n \\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k,$$\n  $$\\mathrm{AICc} = \\mathrm{AIC} + \\frac{2k(k+1)}{n - k - 1},$$\n  which penalizes extra parameters when $n$ is not large.\n\nYour program must:\n- For each dataset, fit both the single-exponential and bi-exponential models using nonlinear least squares with parameter bounds $0.5 \\le C \\le 1.05$, $0 \\le A_i \\le 1.0$, and $0.5 \\le \\tau_i \\le 300$ (all $\\tau_i$ in seconds). Sort any bi-exponential fit so that the reported $\\tau_{\\text{fast}}$ is the smaller of the two fitted time constants and $\\tau_{\\text{slow}}$ is the larger.\n- Compute $\\mathrm{AICc}$ for both models and select the bi-exponential model only if it improves $\\mathrm{AICc}$ by at least $2$ units, i.e., select the bi-exponential if $\\mathrm{AICc}_{\\mathrm{bi}} \\le \\mathrm{AICc}_{\\mathrm{single}} - 2$. Otherwise, select the single-exponential model.\n- Interpret timescales as follows: if the bi-exponential model is selected, interpret the shorter time constant $\\tau_{\\text{fast}}$ as the client-like exchange and the longer $\\tau_{\\text{slow}}$ as the scaffold-like exchange. If the single-exponential model is selected, report the single characteristic time constant as an indeterminate mixed timescale.\n\nTest suite and data generation:\n- Use a shared time grid $t$ in seconds defined as all integers from $0$ to $150$ in steps of $5$, i.e., $t \\in \\{0, 5, 10, \\dots, 150\\}$, with units explicitly in seconds.\n- Synthesize four FRAP datasets by evaluating the specified model with the parameters below and adding independent Gaussian noise of zero mean and specified standard deviation (intensity units are arbitrary but normalized between $0$ and $1$). After noise addition, clip intensities to the interval $[0, 1]$. Use a fixed pseudorandom seed of $12345$ for noise generation to ensure reproducibility.\n  1. Dataset $1$ (single-exponential ground truth): $C = 0.90$, $A_1 = 0.70$, $\\tau_1 = 25.0$ (seconds), noise standard deviation $= 0.002$.\n  2. Dataset $2$ (bi-exponential, well-separated timescales): $C = 0.92$, $A_1 = 0.45$, $\\tau_1 = 5.0$ (seconds), $A_2 = 0.35$, $\\tau_2 = 60.0$ (seconds), noise standard deviation $= 0.003$.\n  3. Dataset $3$ (bi-exponential with nearly overlapping timescales): $C = 0.88$, $A_1 = 0.35$, $\\tau_1 = 15.0$ (seconds), $A_2 = 0.35$, $\\tau_2 = 17.0$ (seconds), noise standard deviation $= 0.002$.\n  4. Dataset $4$ (bi-exponential with immobile fraction implied by lower plateau): $C = 0.80$, $A_1 = 0.25$, $\\tau_1 = 3.0$ (seconds), $A_2 = 0.15$, $\\tau_2 = 40.0$ (seconds), noise standard deviation $= 0.004$.\n- In all cases, the intensity model is $I(t) = C - A_1 e^{-t/\\tau_1}$ for the single-exponential case and $I(t) = C - A_1 e^{-t/\\tau_1} - A_2 e^{-t/\\tau_2}$ for the bi-exponential case.\n\nAngle units do not apply. All reported time constants must be expressed in seconds.\n\nFinal output format:\n- For each dataset, output:\n  - If the single-exponential model is selected: a list of the form $[1, \\tau]$, where $1$ denotes the selected model and $\\tau$ is the fitted characteristic time constant in seconds, rounded to two decimals.\n  - If the bi-exponential model is selected: a list of the form $[2, \\tau_{\\text{fast}}, \\tau_{\\text{slow}}]$, where $2$ denotes the selected model, and $\\tau_{\\text{fast}}$ and $\\tau_{\\text{slow}}$ are the fitted time constants in seconds, rounded to two decimals, with $\\tau_{\\text{fast}} \\le \\tau_{\\text{slow}}$.\n- Your program should produce a single line of output containing the results for the four datasets as a comma-separated list of these lists enclosed in square brackets, for example $[[\\cdot],[\\cdot],[\\cdot],[\\cdot]]$.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the biophysics of phase-separated cellular structures, well-posed with a clear objective and constraints, and formulated with objective, unambiguous language. All necessary information for a unique, verifiable solution is provided.\n\nThe task is to perform model selection between single- and bi-exponential models for synthetic Fluorescence Recovery After Photobleaching (FRAP) data, simulating the molecular exchange dynamics within a postsynaptic density (PSD) condensate. The solution involves data synthesis, nonlinear curve fitting, and statistical model comparison using the corrected Akaike Information Criterion (AICc).\n\n**1. Theoretical Models of Fluorescence Recovery**\n\nThe recovery of fluorescence intensity, $I(t)$, after photobleaching is modeled as a relaxation process. The system evolves from a perturbed state (low fluorescence) towards a steady-state equilibrium characterized by a plateau intensity, $C$.\n\n- **Single-Exponential Model:** This model assumes a single, kinetically homogeneous population of molecules exchanging with a reservoir. The recovery dynamics follow a first-order process:\n  $$I(t) = C - A_1 e^{-t/\\tau_1}$$\n  Here, $C$ is the final recovery plateau, $A_1 \\ge 0$ is the amplitude of the recovering fraction, and $\\tau_1  0$ is the characteristic time constant of exchange. The initial intensity at time $t=0$ is $I(0) = C - A_1$. This model has $k=3$ free parameters: $\\{C, A_1, \\tau_1\\}$.\n\n- **Bi-Exponential Model:** This model describes a system with two independent, kinetically distinct subpopulations, such as a rapidly exchanging 'client' population and a more stably associated 'scaffold' population. The total recovery is the superposition of their individual first-order relaxations:\n  $$I(t) = C - A_1 e^{-t/\\tau_1} - A_2 e^{-t/\\tau_2}$$\n  This model involves two amplitudes, $A_1 \\ge 0$ and $A_2 \\ge 0$, and two time constants, $\\tau_1  0$ and $\\tau_2  0$, corresponding to the two subpopulations. The initial intensity is $I(0) = C - A_1 - A_2$. This more complex model has $k=5$ free parameters: $\\{C, A_1, \\tau_1, A_2, \\tau_2\\}$.\n\n**2. Data Synthesis**\n\nTo test the model selection algorithm, four synthetic datasets are generated according to the problem specification. A shared time vector, $t$, is defined as integers from $0$ to $150$ seconds with a step of $5$ seconds, resulting in $n=31$ data points. For each dataset:\n1. The ground-truth intensity curve is computed using the specified model (single- or bi-exponential) and its parameters.\n2. Independent Gaussian noise with a mean of $0$ and a specified standard deviation is added to each point on the curve. This process is made reproducible by seeding the pseudorandom number generator with a fixed value of $12345$.\n3. The final noisy intensity values are clipped to the physically meaningful range of $[0, 1]$.\n\n**3. Model Fitting and Selection**\n\nFor each synthetic dataset, both the single- and bi-exponential models are fitted to the data using the nonlinear least-squares method. This is achieved via the `curve_fit` function from the `scipy.optimize` library. The fitting process is constrained by the following physically reasonable parameter bounds: $C \\in [0.5, 1.05]$, $A_i \\in [0, 1.0]$, and $\\tau_i \\in [0.5, 300.0]$.\n\nThe core of the task lies in selecting the more appropriate model. A more complex model (bi-exponential) will almost always fit the data better, resulting in a lower residual sum of squares (RSS). To avoid overfitting, we employ a criterion that penalizes model complexity. The corrected Akaike Information Criterion (AICc) is used for this purpose, as it is well-suited for cases with a small sample size ($n$). The AICc is calculated as:\n$$\\mathrm{AICc} = n \\ln\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k + \\frac{2k(k+1)}{n - k - 1}$$\nwhere $n$ is the number of data points, $k$ is the number of free parameters in the model, and $\\mathrm{RSS}$ is the residual sum of squares from the fit.\n\nThe model selection rule is as follows: The bi-exponential model is chosen only if it offers a substantially better fit, defined by the condition $\\mathrm{AICc}_{\\mathrm{bi}} \\le \\mathrm{AICc}_{\\mathrm{single}} - 2$. A difference of at least $2$ units is considered significant evidence in favor of the more complex model. Otherwise, the principle of parsimony dictates that the simpler, single-exponential model is selected.\n\n**4. Implementation and Interpretation**\n\nThe algorithm is implemented as a Python script. For each dataset, the following steps are executed:\n1.  Fit the single-exponential model ($k=3$), calculate $\\mathrm{RSS}_{\\mathrm{single}}$, and compute $\\mathrm{AICc}_{\\mathrm{single}}$.\n2.  Fit the bi-exponential model ($k=5$), calculate $\\mathrm{RSS}_{\\mathrm{bi}}$, and compute $\\mathrm{AICc}_{\\mathrm{bi}}$.\n3.  Apply the selection rule: If $\\mathrm{AICc}_{\\mathrm{bi}} \\le \\mathrm{AICc}_{\\mathrm{single}} - 2$, select the bi-exponential model. Otherwise, select the single-exponential model.\n4.  Format the output based on the selected model. If the bi-exponential model is chosen, its two fitted time constants are sorted such that $\\tau_{\\text{fast}} \\le \\tau_{\\text{slow}}$. These are interpreted as the characteristic timescales for the fast client and slow scaffold components, respectively. If the single-exponential model is chosen, its single time constant is reported as an indeterminate mixture of the underlying dynamics.\n\nThe final output is a list containing the results for all four datasets, where each result is a list specifying the selected model ($1$ for single-exponential, $2$ for bi-exponential) and its relevant time constant(s) in seconds, rounded to two decimal places.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport json\n\ndef solve():\n    \"\"\"\n    Solves the FRAP model selection problem.\n    This involves generating synthetic FRAP data, fitting single- and bi-exponential\n    models, and using AICc for model selection.\n    \"\"\"\n\n    # --- 1. Define Models, AICc, and Constants ---\n\n    def single_exp(t, C, A1, tau1):\n        \"\"\"Single-exponential recovery model.\"\"\"\n        return C - A1 * np.exp(-t / tau1)\n\n    def bi_exp(t, C, A1, tau1, A2, tau2):\n        \"\"\"Bi-exponential recovery model.\"\"\"\n        return C - A1 * np.exp(-t / tau1) - A2 * np.exp(-t / tau2)\n\n    def calculate_aicc(n, k, rss):\n        \"\"\"Calculate the corrected Akaike Information Criterion.\"\"\"\n        if n - k - 1 = 0:\n            return np.inf  # Avoid division by zero or negative\n        aic = n * np.log(rss / n) + 2 * k\n        aicc = aic + (2 * k * (k + 1)) / (n - k - 1)\n        return aicc\n\n    # Shared parameters for all datasets\n    t_data = np.arange(0, 151, 5, dtype=float)\n    n_points = len(t_data)\n    rng = np.random.default_rng(12345)\n\n    # Parameter bounds for fitting\n    bounds_single = ([0.5, 0, 0.5], [1.05, 1.0, 300.0])\n    bounds_bi = ([0.5, 0, 0.5, 0, 0.5], [1.05, 1.0, 300.0, 1.0, 300.0])\n\n    # --- 2. Define Test Cases ---\n\n    test_cases = [\n        {\n            \"model\": \"single\",\n            \"params\": {\"C\": 0.90, \"A1\": 0.70, \"tau1\": 25.0},\n            \"noise_std\": 0.002,\n        },\n        {\n            \"model\": \"bi\",\n            \"params\": {\"C\": 0.92, \"A1\": 0.45, \"tau1\": 5.0, \"A2\": 0.35, \"tau2\": 60.0},\n            \"noise_std\": 0.003,\n        },\n        {\n            \"model\": \"bi\",\n            \"params\": {\"C\": 0.88, \"A1\": 0.35, \"tau1\": 15.0, \"A2\": 0.35, \"tau2\": 17.0},\n            \"noise_std\": 0.002,\n        },\n        {\n            \"model\": \"bi\",\n            \"params\": {\"C\": 0.80, \"A1\": 0.25, \"tau1\": 3.0, \"A2\": 0.15, \"tau2\": 40.0},\n            \"noise_std\": 0.004,\n        },\n    ]\n    \n    all_results = []\n\n    # --- 3. Process Each Test Case ---\n\n    for case in test_cases:\n        # Generate synthetic data\n        if case[\"model\"] == \"single\":\n            p = case[\"params\"]\n            y_true = single_exp(t_data, p[\"C\"], p[\"A1\"], p[\"tau1\"])\n        else: # bi-exponential\n            p = case[\"params\"]\n            y_true = bi_exp(t_data, p[\"C\"], p[\"A1\"], p[\"tau1\"], p[\"A2\"], p[\"tau2\"])\n        \n        noise = rng.normal(0, case[\"noise_std\"], size=n_points)\n        y_noisy = np.clip(y_true + noise, 0, 1)\n\n        # Fit single-exponential model\n        try:\n            popt_single, _ = curve_fit(single_exp, t_data, y_noisy, bounds=bounds_single, p0=[0.9, 0.5, 20])\n            y_fit_single = single_exp(t_data, *popt_single)\n            rss_single = np.sum((y_noisy - y_fit_single)**2)\n            aicc_single = calculate_aicc(n=n_points, k=3, rss=rss_single)\n        except RuntimeError:\n            aicc_single = np.inf\n\n        # Fit bi-exponential model\n        try:\n            popt_bi, _ = curve_fit(bi_exp, t_data, y_noisy, bounds=bounds_bi, p0=[0.9, 0.3, 10, 0.3, 50])\n            y_fit_bi = bi_exp(t_data, *popt_bi)\n            rss_bi = np.sum((y_noisy - y_fit_bi)**2)\n            aicc_bi = calculate_aicc(n=n_points, k=5, rss=rss_bi)\n        except RuntimeError:\n            aicc_bi = np.inf\n        \n        # Model selection and result formatting\n        if aicc_bi = aicc_single - 2:\n            # Bi-exponential model is selected\n            tau1_fit, tau2_fit = popt_bi[2], popt_bi[4]\n            tau_fast = min(tau1_fit, tau2_fit)\n            tau_slow = max(tau1_fit, tau2_fit)\n            result = [2, round(tau_fast, 2), round(tau_slow, 2)]\n        else:\n            # Single-exponential model is selected\n            tau_fit = popt_single[2]\n            result = [1, round(tau_fit, 2)]\n        \n        all_results.append(result)\n\n    # --- 4. Final Output ---\n    # Convert to JSON-style string without spaces for strict formatting\n    final_output_str = json.dumps(all_results, separators=(',', ':'))\n    print(final_output_str)\n\nsolve()\n```", "id": "2737978"}]}