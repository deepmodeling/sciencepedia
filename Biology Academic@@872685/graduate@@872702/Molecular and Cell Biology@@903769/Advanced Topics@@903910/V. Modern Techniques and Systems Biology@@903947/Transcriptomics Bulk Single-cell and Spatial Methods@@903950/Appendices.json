{"hands_on_practices": [{"introduction": "Raw read counts obtained from an RNA sequencing experiment are not directly comparable across genes. A core challenge is that longer transcripts, by virtue of their size, will generate more sequencing reads than shorter transcripts even if their true cellular abundances are identical. This practice [@problem_id:2967173] guides you through deriving and implementing Transcripts Per Million (TPM), a standard normalization method that corrects for this length bias and for sequencing depth, yielding a more accurate relative measure of gene expression.", "problem": "You are given a set of gene-level read count observations and corresponding effective lengths, arising from a standard bulk RNA sequencing protocol under uniform random fragmentation and sampling. Let there be $G$ genes indexed by $i \\in \\{0,1,\\dots,G-1\\}$. For each gene $i$, you are provided two quantities: an observed read count $c_{i} \\in \\mathbb{N}_{0}$ and a positive effective length $\\ell_{i} \\in \\mathbb{R}_{>0}$ measured in nucleotides. Assume the widely used model in transcriptomics that, for a fixed sequencing depth, the expected number of reads originating from a gene is proportional to both its underlying transcript abundance and its effective length. Under this model, removing length-dependent sampling bias requires an explicit length correction, followed by conversion to a relative abundance scale that sums to a fixed constant. Transcripts Per Million (TPM) is defined as a length-corrected relative abundance scale that sums to $10^{6}$ across genes. If the length correction yields a zero total across all genes, define the TPM vector to be the all-zero vector to avoid division by zero.\n\nYour tasks are:\n1) For each test case, derive an algorithm from first principles to compute a TPM vector $t \\in \\mathbb{R}_{\\ge 0}^{G}$ from $(c_{i}, \\ell_{i})_{i=0}^{G-1}$ under the modeling assumptions above, such that $\\sum_{i=0}^{G-1} t_{i} = 10^{6}$ whenever the total length-corrected signal is nonzero, and $t_{i} = 0$ for all $i$ otherwise. Do not assume any intermediate formulas other than the modeling statements given; reason from the proportional relationships implied by the sampling model and the definition that TPM is the length-corrected relative abundance scaled to sum to $10^{6}$.\n2) For each test case, compute two rank orderings of the genes as lists of zero-based indices:\n   a) The ordering by raw counts, from largest to smallest $c_{i}$.\n   b) The ordering by TPM, from largest to smallest $t_{i}$.\n   In both orderings, if values are equal for a tie, break ties by smaller index first.\n\nYour program must implement this algorithm and apply it to the following test suite. Each test case is fully specified by two lists of equal length: counts and effective lengths. All effective lengths are positive. The units of $\\ell_{i}$ are nucleotides, and the TPM scale is unitless.\n\nTest suite (each bullet gives counts then effective lengths for that case):\n- Case A:\n  - counts: [$500$, $1000$, $500$]\n  - effective lengths: [$1000$, $2000$, $500$]\n- Case B:\n  - counts: [$0$, $0$, $0$]\n  - effective lengths: [$1000$, $1000$, $1000$]\n- Case C:\n  - counts: [$100$, $101$, $102$, $103$]\n  - effective lengths: [$100$, $10000$, $100$, $100000$]\n- Case D:\n  - counts: [$10$, $10$, $5$, $5$]\n  - effective lengths: [$1000$, $1000$, $500$, $1000$]\n\nFinal output format:\n- Your program should produce a single line of output containing one top-level list with one entry per test case, in the order A, B, C, D.\n- For each test case, output a list with three elements:\n  1) the TPM vector $[t_{0}, t_{1}, \\dots, t_{G-1}]$ as floats in gene index order;\n  2) the rank ordering by raw counts as a list of zero-based indices in descending order with tie-breaking by smaller index;\n  3) the rank ordering by TPM as a list of zero-based indices in descending order with tie-breaking by smaller index.\n- The entire output must be formatted on a single line as a comma-separated list enclosed in square brackets, with no additional text. For example, the top-level structure must look like $[\\dots]$ where each per-case element is itself a list $[\\dots]$. No physical units should be printed. Express any fractions or proportions as decimal floats in the TPM vector. Angles are not applicable.\n\nYour program must be self-contained, take no input, and print exactly the specified single-line result for the given test suite.", "solution": "The problem as stated is valid. It is scientifically grounded in the standard principles of transcriptomic data analysis, is well-posed with sufficient information for a unique solution, and is formulated in objective, precise language. We shall proceed with a derivation from first principles.\n\nThe fundamental modeling assumption is that the expected read count for a gene, $E[c_i]$, is proportional to the product of its true transcript abundance, $A_i$, and its effective length, $\\ell_i$. This can be stated mathematically for a library of $G$ genes as:\n$$ E[c_i] = K \\cdot A_i \\cdot \\ell_i $$\nwhere $i \\in \\{0, 1, \\dots, G-1\\}$ is the gene index, $c_i \\in \\mathbb{N}_0$ is the observed read count, $\\ell_i \\in \\mathbb{R}_{>0}$ is the effective length, and $K$ is a library-wide normalization constant reflecting sequencing depth.\n\nOur objective is to estimate the relative abundance of each gene's transcripts. The observed count $c_i$ serves as our empirical estimate for the expectation $E[c_i]$. By rearranging the proportionality, we deduce that the true abundance $A_i$ is proportional to the read count divided by the effective length:\n$$ A_i \\propto \\frac{c_i}{\\ell_i} $$\nThis ratio, which we shall denote $s_i = c_i / \\ell_i$, represents a length-normalized measure of expression. It corrects for the sampling bias inherent in RNA sequencing, where longer transcripts are more likely to be fragmented and sequenced, thus producing more reads even at the same level of true cellular abundance. The quantity $s_i$ is therefore our best estimate of the abundance $A_i$, up to a common scaling factor across all genes.\n\nThe set of values $\\{s_0, s_1, \\dots, s_{G-1}\\}$ represents the relative expression levels of all genes. To convert these into true fractional abundances, we must normalize them by their sum. The total length-corrected signal in the library is the sum over all genes:\n$$ S_{total} = \\sum_{j=0}^{G-1} s_j = \\sum_{j=0}^{G-1} \\frac{c_j}{\\ell_j} $$\nThe fractional abundance of gene $i$, denoted $r_i$, is then the fraction of its length-corrected signal relative to the total:\n$$ r_i = \\frac{s_i}{S_{total}} = \\frac{c_i / \\ell_i}{\\sum_{j=0}^{G-1} (c_j / \\ell_j)} $$\nBy construction, these fractional abundances sum to unity: $\\sum_{i=0}^{G-1} r_i = 1$.\n\nThe problem defines Transcripts Per Million (TPM) as this fractional abundance scaled to a total of $10^6$. Therefore, the TPM value for gene $i$, denoted $t_i$, is obtained by multiplying its fractional abundance $r_i$ by $10^6$:\n$$ t_i = r_i \\cdot 10^6 = \\left( \\frac{c_i / \\ell_i}{\\sum_{j=0}^{G-1} (c_j / \\ell_j)} \\right) \\cdot 10^6 $$\nThis is the general formula for computing TPM from read counts and effective lengths.\n\nWe must also consider the specified edge case. If the total length-corrected signal $S_{total}$ is zero, the formula for $t_i$ involves division by zero and is therefore undefined. Since all counts $c_i \\ge 0$ and all lengths $\\ell_i > 0$, the sum $S_{total}$ can only be $0$ if all individual terms $c_i / \\ell_i$ are $0$, which implies that all counts $c_i$ must be $0$. In this scenario, the problem statement requires that the TPM for all genes must be $0$. Our algorithm must explicitly implement this condition.\n\nFor the second task, we must determine the rank ordering of genes based on two different metrics: raw counts, $c_i$, and the derived TPM values, $t_i$. The ordering must be from largest to smallest value. For any ties in these values, the problem specifies that the tie must be broken by giving precedence to the gene with the smaller index. This defines a lexicographical sort. For a set of values $v_i$ with indices $i$, the sorted order of indices is determined by sorting the pairs $(v_i, i)$ with the primary key $v_i$ in descending order and the secondary key $i$ in ascending order. This can be achieved algorithmically by sorting on the tuple $(-v_i, i)$.\n\nThe final algorithm is as follows:\n1.  For each gene $i$, calculate the length-normalized signal $s_i = c_i / \\ell_i$.\n2.  Calculate the total signal $S_{total} = \\sum_{i=0}^{G-1} s_i$.\n3.  If $S_{total} = 0$, set the TPM vector $t$ to be a vector of zeros. Otherwise, calculate $t_i = (s_i / S_{total}) \\cdot 10^6$ for all $i$.\n4.  Generate the list of gene indices $\\{0, 1, \\dots, G-1\\}$.\n5.  To obtain the raw count ranking, sort this list of indices based on the key $(-c_i, i)$.\n6.  To obtain the TPM ranking, sort this list of indices based on the key $(-t_i, i)$.\n\nThis completes the derivation from first principles as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the transcriptomics problem for the given test suite.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"counts\": [500, 1000, 500],\n            \"effective_lengths\": [1000, 2000, 500]\n        },\n        {\n            \"counts\": [0, 0, 0],\n            \"effective_lengths\": [1000, 1000, 1000]\n        },\n        {\n            \"counts\": [100, 101, 102, 103],\n            \"effective_lengths\": [100, 10000, 100, 100000]\n        },\n        {\n            \"counts\": [10, 10, 5, 5],\n            \"effective_lengths\": [1000, 1000, 500, 1000]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        counts = np.array(case[\"counts\"], dtype=float)\n        lengths = np.array(case[\"effective_lengths\"], dtype=float)\n        \n        # 1. Calculate TPM vector\n        \n        # Calculate rates (reads per nucleotide)\n        # np.divide handles division by zero, but lengths are guaranteed > 0.\n        rates = counts / lengths\n        \n        # Sum of rates\n        sum_of_rates = np.sum(rates)\n        \n        tpm_vector = []\n        if sum_of_rates == 0:\n            # Edge case: If total signal is zero, TPMs are all zero.\n            tpm_vector = np.zeros_like(counts, dtype=float)\n        else:\n            # TPM = (rate / sum_of_rates) * 1e6\n            tpm_vector = (rates / sum_of_rates) * 1e6\n\n        # 2. Compute rank orderings\n\n        num_genes = len(counts)\n        indices = list(range(num_genes))\n        \n        # Rank by raw counts (descending), tie-break by smaller index (ascending)\n        # Sorting by a tuple (-value, index) achieves this.\n        count_ranking = sorted(indices, key=lambda i: (-counts[i], i))\n        \n        # Rank by TPM (descending), tie-break by smaller index (ascending)\n        tpm_ranking = sorted(indices, key=lambda i: (-tpm_vector[i], i))\n\n        all_results.append([\n            tpm_vector.tolist(),\n            count_ranking,\n            tpm_ranking\n        ])\n    \n    # Format the final output string exactly as specified.\n    # The default str() representation for lists and floats is sufficient.\n    # .replace(\" \", \"\") is used to remove whitespace for a compact representation.\n    # The problem did not specify float precision, so default is used.\n    case_strings = []\n    for tpm_vec, count_rank, tpm_rank in all_results:\n        # Manually construct string to avoid unwanted whitespace from str(list)\n        tpm_str = f\"[{','.join(map(str, tpm_vec))}]\"\n        count_rank_str = f\"[{','.join(map(str, count_rank))}]\"\n        tpm_rank_str = f\"[{','.join(map(str, tpm_rank))}]\"\n        case_strings.append(f\"[{tpm_str},{count_rank_str},{tpm_rank_str}]\")\n        \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "2967173"}, {"introduction": "In droplet-based single-cell technologies, the goal of isolating individual cells is governed by stochastic physics. The loading of cells into droplets is a classic example of a Poisson process, where an ideal outcome (one cell per droplet) competes with undesired outcomes like empty droplets or, more problematically, multiplets. This hands-on exercise [@problem_id:2967132] challenges you to derive the mathematical relationship between cell loading concentration and the multiplet rate from the ground up, providing critical insight into the fundamental trade-offs that govern single-cell experimental design.", "problem": "In a droplet-based single-cell RNA sequencing (single-cell RNA-seq) workflow, a microfluidic generator forms monodisperse droplets of volume $V_d$ from a well-mixed cell suspension at concentration $c$ (cells per unit volume). Assume the following fundamental premises:\n- Cells are uniformly and independently distributed in space within the suspension at the moment of droplet formation.\n- Each cell is assigned to exactly one of $M$ exchangeable droplets uniformly at random, and the total number of cells in the input is $N$, with $N$ and $M$ both large.\n- The expected number of cells per droplet is $\\lambda$, and in the limit of large $N$ and $M$ at fixed $\\lambda$, the number of cells per droplet follows the Poisson distribution with mean $\\lambda$.\n\nStarting from these premises (and not from any pre-stated target formula), first derive a closed-form expression for the probability that a droplet is a multiplet (contains at least two cells) written solely in terms of $\\lambda$. Then connect $\\lambda$ to the loading concentration $c$ and droplet volume $V_d$ under the assumption of a homogeneous Poisson process in space, and use this to compute the multiplet probability for $c = 1.0 \\times 10^{5}\\ \\mathrm{cells/mL}$ and $V_d = 1.0\\ \\mathrm{nL}$. Express your final numerical answer as a decimal fraction, rounded to four significant figures.\n\nFinally, based on your derived expression, state whether the multiplet probability increases or decreases with loading concentration $c$, and justify your conclusion from first principles (no numerical value required for this part).", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- Droplet volume is $V_d$.\n- Cell suspension concentration is $c$.\n- Cells are uniformly and independently distributed.\n- Total number of cells is $N$, total number of droplets is $M$.\n- Each cell is assigned to one of $M$ exchangeable droplets uniformly at random.\n- $N$ and $M$ are large.\n- Expected number of cells per droplet is $\\lambda$.\n- The number of cells per droplet, $K$, follows a Poisson distribution with mean $\\lambda$ in the limit of large $N$ and $M$ at fixed $\\lambda$.\n- A multiplet is a droplet containing at least two cells ($K \\ge 2$).\n- The loading concentration is $c = 1.0 \\times 10^{5}\\ \\mathrm{cells/mL}$.\n- The droplet volume is $V_d = 1.0\\ \\mathrm{nL}$.\n- The final numerical answer must be rounded to four significant figures.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The assumption that cell encapsulation in droplets follows a Poisson distribution is a standard and well-validated model in the field of microfluidics and single-cell genomics. It is a direct consequence of counting rare, independent events (cell capture) in a fixed volume. The problem is well-posed, providing all necessary information for derivation and calculation. The language is objective and precise. The numerical values provided are realistic for typical droplet-based single-cell sequencing experiments. The problem does not violate any of the invalidity criteria.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe solution proceeds in three parts as requested.\n\nFirst, we derive the probability of a droplet being a multiplet in terms of the mean number of cells per droplet, $\\lambda$.\nLet $K$ be the random variable representing the number of cells in a single droplet. According to the problem statement, $K$ follows a Poisson distribution with mean $\\lambda$. The probability mass function (PMF) is given by:\n$$P(K=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$$\nfor $k \\in \\{0, 1, 2, \\dots\\}$.\n\nA multiplet is defined as a droplet containing at least two cells, which corresponds to the event $K \\ge 2$. The probability of this event, $P_{\\text{multiplet}}$, can be calculated as the complement of the event that a droplet contains fewer than two cells (i.e., zero or one cell).\n$$P_{\\text{multiplet}} = P(K \\ge 2) = 1 - P(K  2) = 1 - [P(K=0) + P(K=1)]$$\nWe calculate the probabilities for $K=0$ and $K=1$ using the Poisson PMF:\nThe probability of a droplet being empty ($k=0$):\n$$P(K=0) = \\frac{\\lambda^0 \\exp(-\\lambda)}{0!} = \\frac{1 \\cdot \\exp(-\\lambda)}{1} = \\exp(-\\lambda)$$\nThe probability of a droplet containing exactly one cell (a singlet, $k=1$):\n$$P(K=1) = \\frac{\\lambda^1 \\exp(-\\lambda)}{1!} = \\frac{\\lambda \\exp(-\\lambda)}{1} = \\lambda \\exp(-\\lambda)$$\nSubstituting these expressions back into the equation for the multiplet probability, we obtain the closed-form expression in terms of $\\lambda$:\n$$P_{\\text{multiplet}} = 1 - [\\exp(-\\lambda) + \\lambda \\exp(-\\lambda)] = 1 - (1+\\lambda)\\exp(-\\lambda)$$\n\nSecond, we connect $\\lambda$ to the physical parameters $c$ and $V_d$ and compute the numerical probability.\nThe problem states to assume a homogeneous Poisson process in space. For such a process, the expected number of events (cells) in a given volume is the product of the event density (concentration $c$) and the volume ($V_d$). Therefore, the mean $\\lambda$ is given by:\n$$\\lambda = c \\cdot V_d$$\nWe are given $c = 1.0 \\times 10^{5}\\ \\mathrm{cells/mL}$ and $V_d = 1.0\\ \\mathrm{nL}$. To ensure dimensional consistency, we must convert the units to be compatible. We convert nanoliters (nL) to milliliters (mL):\n$$1\\ \\mathrm{nL} = 10^{-6}\\ \\mathrm{mL}$$\nSo, $V_d = 1.0 \\times 10^{-6}\\ \\mathrm{mL}$.\nNow, we can calculate the dimensionless parameter $\\lambda$:\n$$\\lambda = (1.0 \\times 10^{5}\\ \\mathrm{cells/mL}) \\times (1.0 \\times 10^{-6}\\ \\mathrm{mL}) = 0.1$$\nThe expected number of cells per droplet is $\\lambda = 0.1$.\nWe substitute this value into our derived expression for the multiplet probability:\n$$P_{\\text{multiplet}} = 1 - (1 + 0.1) \\exp(-0.1) = 1 - 1.1 \\exp(-0.1)$$\nUsing the value $\\exp(-0.1) \\approx 0.904837418$:\n$$P_{\\text{multiplet}} \\approx 1 - 1.1 \\times 0.904837418 = 1 - 0.995321160$$\n$$P_{\\text{multiplet}} \\approx 0.004678840$$\nRounding to four significant figures as required, we get:\n$$P_{\\text{multiplet}} \\approx 0.004679$$\n\nThird, we determine if the multiplet probability increases or decreases with loading concentration $c$.\nThe multiplet probability is a function of $\\lambda$, $P_{\\text{multiplet}}(\\lambda) = 1 - (1+\\lambda)\\exp(-\\lambda)$. The mean $\\lambda$ is itself a function of concentration $c$, $\\lambda(c) = c \\cdot V_d$. Since the droplet volume $V_d$ is a positive constant, $\\lambda$ is directly and linearly promotional to $c$. To understand how $P_{\\text{multiplet}}$ changes with $c$, we can analyze its derivative with respect to $\\lambda$. By the chain rule, $\\frac{d P_{\\text{multiplet}}}{d c} = \\frac{d P_{\\text{multiplet}}}{d \\lambda} \\cdot \\frac{d\\lambda}{dc}$. Since $\\frac{d\\lambda}{dc} = V_d > 0$, the sign of $\\frac{d P_{\\text{multiplet}}}{d c}$ is the same as the sign of $\\frac{d P_{\\text{multiplet}}}{d \\lambda}$.\nLet us compute the derivative of $P_{\\text{multiplet}}(\\lambda)$ with respect to $\\lambda$:\n$$\\frac{d}{d\\lambda} P_{\\text{multiplet}}(\\lambda) = \\frac{d}{d\\lambda} [1 - (1+\\lambda)\\exp(-\\lambda)]$$\n$$= 0 - \\frac{d}{d\\lambda} [(1+\\lambda)\\exp(-\\lambda)]$$\nUsing the product rule for differentiation, $[f(\\lambda)g(\\lambda)]' = f'(\\lambda)g(\\lambda) + f(\\lambda)g'(\\lambda)$, with $f(\\lambda) = 1+\\lambda$ and $g(\\lambda) = \\exp(-\\lambda)$:\n$$\\frac{d}{d\\lambda} [(1+\\lambda)\\exp(-\\lambda)] = (\\frac{d}{d\\lambda}(1+\\lambda))\\exp(-\\lambda) + (1+\\lambda)(\\frac{d}{d\\lambda}\\exp(-\\lambda))$$\n$$= (1)\\exp(-\\lambda) + (1+\\lambda)(-\\exp(-\\lambda))$$\n$$= \\exp(-\\lambda) - \\exp(-\\lambda) - \\lambda\\exp(-\\lambda) = -\\lambda\\exp(-\\lambda)$$\nSubstituting this back into the derivative of $P_{\\text{multiplet}}$:\n$$\\frac{d}{d\\lambda} P_{\\text{multiplet}}(\\lambda) = -(-\\lambda\\exp(-\\lambda)) = \\lambda\\exp(-\\lambda)$$\nFor any physically realistic scenario, the concentration $c$ is non-negative, and the droplet volume $V_d$ is positive, so $\\lambda = c V_d \\ge 0$. The exponential term $\\exp(-\\lambda)$ is always positive. Therefore, for any $\\lambda > 0$ (which corresponds to $c > 0$), the derivative $\\frac{d}{d\\lambda} P_{\\text{multiplet}}(\\lambda)$ is strictly positive.\nA positive derivative indicates that the function $P_{\\text{multiplet}}(\\lambda)$ is a monotonically increasing function of $\\lambda$ for $\\lambda > 0$. Since $\\lambda$ is itself a monotonically increasing function of $c$, it follows that the multiplet probability increases as the loading concentration $c$ increases. This confirms the intuition that a higher density of cells in the suspension naturally leads to a higher chance of capturing multiple cells in a droplet of fixed volume.", "answer": "$$\\boxed{0.004679}$$", "id": "2967132"}, {"introduction": "A hallmark of transcriptomics is its scale: tens of thousands of genes are tested for differential expression simultaneously. This massive multiplicity of tests drastically increases the chance of finding \"significant\" results purely by chance. This practice [@problem_id:2967187] tackles this head-on by guiding you through the implementation of the Benjamini-Hochberg procedure, the standard method for controlling the False Discovery Rate (FDR). Mastering this technique is essential for distinguishing true biological signals from statistical noise in any high-throughput experiment.", "problem": "In high-throughput transcriptomics experiments such as bulk RNA sequencing (RNA-seq), single-cell RNA sequencing (scRNA-seq), and spatial transcriptomics, tens of thousands of genes are often tested for differential expression simultaneously. Under the Central Dogma of molecular biology and standard statistical hypothesis testing, a null hypothesis for a gene implies that its test statistic yields a p-value that is distributed as Uniform on the interval $[0,1]$ when the null is true and assumptions such as independence or positive dependence among tests approximately hold. The False Discovery Rate (FDR) is the expected value of the proportion of false rejections among all rejections. The Benjamini-Hochberg (BH) procedure is designed to control the FDR at a user-specified level $\\alpha$ in this multiple-testing context. The q-value for a test is the minimal FDR level at which the test would be called significant.\n\nTask: Write a complete program that, given a vector of p-values and a target FDR level $\\alpha$, applies a step-up decision rule derived from the definitions above to produce two outputs aligned to the original p-value order: (i) a Boolean rejection vector indicating which hypotheses are rejected at level $\\alpha$ and (ii) the q-values for all hypotheses, rounded to six decimal places. Use the foundational facts that under true nulls p-values are Uniform on $[0,1]$ and that controlling the ordering of p-values guides decision thresholds, but do not assume any particular shortcut formulas; you must derive the algorithmic steps from these principles.\n\nYour program must execute the following fixed test suite, where each test case consists of a list of p-values $p$ and a scalar $\\alpha$:\n\n- Test case A: $p = [\\,0.002,\\,0.040,\\,0.120,\\,0.5,\\,0.0005,\\,0.07,\\,0.9,\\,0.03,\\,0.2,\\,0.8\\,]$, $\\alpha = 0.1$.\n- Test case B: $p = [\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$, $\\alpha = 0.05$.\n- Test case C: $p = [\\,0.0,\\,0.2,\\,0.05,\\,0.04,\\,0.001,\\,0.1999999,\\,0.25,\\,0.2\\,]$, $\\alpha = 0.2$.\n- Test case D: $p = [\\,0.01,\\,0.02,\\,0.03,\\,0.2\\,]$, $\\alpha = 0.0$.\n- Test case E: $p = [\\,0.4,\\,0.6,\\,1.0\\,]$, $\\alpha = 1.0$.\n- Test case F: $p = [\\,0.0,\\,0.0,\\,0.05,\\,0.2,\\,0.5,\\,0.9\\,]$, $\\alpha = 0.05$.\n\nFor each test case, compute:\n- The integer number of discoveries $k$.\n- The Boolean list of rejections in the original order of $p$.\n- The list of q-values in the original order of $p$, each rounded to six decimal places.\n\nFinal output format: Your program should produce a single line of output containing a list whose elements correspond to the test cases A through F, in order. Each element must itself be a list with three entries: the integer $k$, the list of Booleans for rejections aligned to the original order, and the list of six-decimal rounded q-values aligned to the original order. For example, the outer structure must look like\n[ [k_A, [booleans_A], [qvals_A]], [k_B, [booleans_B], [qvals_B]], ..., [k_F, [booleans_F], [qvals_F]] ]\nwith no additional text. All q-values must be rounded to six decimal places. All Booleans must be expressed as either True or False. No physical units are involved in this problem. Angles are not involved. Percentages must not be used; any rates must be expressed as decimals in $[0,1]$.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in established statistical theory, well-posed with a clear objective, and free of contradictions or ambiguities. It presents a standard, non-trivial task in computational statistics that is central to the analysis of high-throughput biological data. We shall therefore proceed to derive and implement the required algorithm.\n\nThe task is to control the False Discovery Rate (FDR) in a multiple hypothesis testing scenario. We are given a set of $m$ p-values, $\\{p_1, p_2, \\ldots, p_m\\}$, and a target FDR level $\\alpha \\in [0, 1]$. We must derive an algorithm that outputs (i) a Boolean vector indicating which hypotheses to reject and (ii) a vector of q-values for each test.\n\nThe foundation of the method lies in the Benjamini-Hochberg (BH) procedure. Let us first order the p-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. Let $H_{(i)}$ be the null hypothesis corresponding to the p-value $p_{(i)}$. The BH procedure is a step-up procedure that finds the largest rank $k$ for which the p-value $p_{(k)}$ satisfies the condition:\n$$\np_{(k)} \\le \\frac{k}{m}\\alpha\n$$\nIf such a $k$ exists, the procedure rejects all null hypotheses $H_{(i)}$ for $i = 1, \\ldots, k$. If no such $k$ exists, no hypotheses are rejected. This procedure is proven to control the FDR at level $\\alpha$ under independence or positive dependence of the tests.\n\nThe problem requires the calculation of q-values. The q-value of a particular test is defined as the minimum FDR level at which that test would be deemed significant. Let us derive the formula for the q-value from this definition and the BH procedure.\n\nA test with an ordered p-value $p_{(i)}$ is called significant by the BH procedure at level $\\alpha$ if it is among the set of rejected hypotheses. This means the largest rank $k$ satisfying the BH condition must be at least $i$ (i.e., $k \\ge i$). For this to happen, there must exist some rank $j$ such that $i \\le j \\le m$ and $p_{(j)} \\le \\frac{j}{m}\\alpha$.\n\nWe can re-arrange this inequality to solve for $\\alpha$:\n$$\n\\alpha \\ge \\frac{m \\cdot p_{(j)}}{j}\n$$\nFor the test corresponding to $p_{(i)}$ to be significant, $\\alpha$ must be greater than or equal to the value $\\frac{m \\cdot p_{(j)}}{j}$ for at least one $j \\ge i$. The q-value is the *minimum* such $\\alpha$. Therefore, the q-value for the test with ordered p-value $p_{(i)}$, which we denote as $q_{(i)}$, is given by:\n$$\nq_{(i)} = \\min_{j=i, \\ldots, m} \\left\\{ \\frac{m \\cdot p_{(j)}}{j} \\right\\}\n$$\nThis formula ensures that the resulting q-values are monotonically non-decreasing with respect to the ordered p-values: $q_{(1)} \\le q_{(2)} \\le \\ldots \\le q_{(m)}$. This property can be seen by observing the recursive relationship:\n$$\nq_{(i)} = \\min\\left( \\frac{m \\cdot p_{(i)}}{i}, \\min_{j=i+1, \\ldots, m} \\left\\{ \\frac{m \\cdot p_{(j)}}{j} \\right\\} \\right) = \\min\\left( \\frac{m \\cdot p_{(i)}}{i}, q_{(i+1)} \\right)\n$$\nThis gives rise to an efficient computational strategy. We can first calculate the term $v_{(i)} = \\frac{m \\cdot p_{(i)}}{i}$ for all $i=1, \\ldots, m$. Then, we can compute the final q-values by iterating backwards from $i=m$:\n1. $q_{(m)} = v_{(m)} = \\frac{m \\cdot p_{(m)}}{m} = p_{(m)}$\n2. For $i=m-1, m-2, \\ldots, 1$, calculate $q_{(i)} = \\min(v_{(i)}, q_{(i+1)})$.\n\nOnce the q-value for each test is known, the rejection rule is straightforward. A hypothesis is rejected at a target FDR of $\\alpha$ if and only if its q-value is less than or equal to $\\alpha$.\n\nThe complete algorithm is as follows:\n\n1.  **Preparation**: Given the input vector of $m$ p-values, create a corresponding vector of original indices, $0, 1, \\ldots, m-1$.\n2.  **Sorting**: Sort the p-values in ascending order, $p_{(1)}, \\ldots, p_{(m)}$. Maintain the mapping from this sorted order back to the original indices.\n3.  **Q-value Calculation**:\n    a. Compute the ordered q-values, $q_{(i)}$, for $i = 1, \\ldots, m$. This is done by first calculating the intermediate values $v_{(i)} = \\frac{m \\cdot p_{(i)}}{i}$ for each rank $i$.\n    b. Then, enforce monotonicity by setting $q_{(m)} = v_{(m)}$ and iterating backwards for $i = m-1, \\ldots, 1$, setting $q_{(i)} = \\min(v_{(i)}, q_{(i+1)})$. The resulting q-values should be capped at $1.0$.\n4.  **Rejection Decision**: For each ordered test $i$, reject the null hypothesis if $q_{(i)} \\le \\alpha$. Count the total number of rejections, $k$.\n5.  **Un-sorting**: Using the saved original index mapping, reorder the calculated q-values and the Boolean rejection decisions to align with the original input p-value vector.\n6.  **Formatting**: Round the final q-values to six decimal places as required. The final output for each test case is the triplet: the integer count $k$, the list of Boolean rejections, and the list of rounded q-values.\n\nThis procedure correctly implements the required logic derived from the foundational principles of FDR control.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the formatted output.\n    \"\"\"\n\n    def fdr_procedure(p: list[float], alpha: float) - tuple[int, list[bool], list[float]]:\n        \"\"\"\n        Applies the Benjamini-Hochberg procedure to a list of p-values.\n\n        Args:\n            p: A list of p-values.\n            alpha: The target False Discovery Rate (FDR) level.\n\n        Returns:\n            A tuple containing:\n            - k (int): The number of rejected hypotheses.\n            - rejections (list[bool]): A boolean list indicating rejection for each\n              hypothesis in the original order.\n            - q_values (list[float]): The calculated q-values for each hypothesis\n              in the original order, rounded to six decimal places.\n        \"\"\"\n        p_values = np.array(p, dtype=np.float64)\n        m = len(p_values)\n\n        if m == 0:\n            return 0, [], []\n\n        # Sort p-values in ascending order while keeping track of their original indices.\n        # Using a stable sort ('mergesort') is good practice for tied p-values.\n        sorted_order = np.argsort(p_values, kind='mergesort')\n        p_sorted = p_values[sorted_order]\n\n        # Calculate q-values for the sorted p-values.\n        # The derivation q_i = min_{j=i} (m * p_j / j) can be computed efficiently\n        # with a single backward pass.\n        q_sorted = np.zeros(m)\n        \n        # The rank of a p-value is its 1-based index in the sorted list.\n        ranks = np.arange(1, m + 1)\n        # Raw BH-adjusted p-values (can be  1)\n        bh_adjusted_p = m * p_sorted / ranks\n\n        # Apply the monotonicity constraint by iterating backwards.\n        # q_m = p_m * m / m = p_m\n        # q_i = min(p_i * m / i, q_{i+1})\n        q_sorted[-1] = bh_adjusted_p[-1]\n        for i in range(m - 2, -1, -1):\n            q_sorted[i] = min(bh_adjusted_p[i], q_sorted[i+1])\n\n        # Q-values, like p-values, should not exceed 1.0.\n        q_sorted = np.minimum(q_sorted, 1.0)\n        \n        # Determine rejections by comparing q-values to the FDR level alpha.\n        rejections_sorted = q_sorted = alpha\n        k = int(np.sum(rejections_sorted))\n\n        # Reorder the q-values and rejection decisions to match the original input order.\n        q_original = np.zeros(m)\n        q_original[sorted_order] = q_sorted\n        \n        rejections_original = np.zeros(m, dtype=bool)\n        rejections_original[sorted_order] = rejections_sorted\n        \n        # Format the final output as per requirements.\n        final_q_values = [round(q, 6) for q in q_original]\n        final_rejections = rejections_original.tolist()\n\n        return k, final_rejections, final_q_values\n\n    # Fixed test suite as specified in the problem statement.\n    test_cases = [\n        # Test case A\n        ([0.002, 0.040, 0.120, 0.5, 0.0005, 0.07, 0.9, 0.03, 0.2, 0.8], 0.1),\n        # Test case B\n        ([1.0, 1.0, 1.0, 1.0, 1.0], 0.05),\n        # Test case C\n        ([0.0, 0.2, 0.05, 0.04, 0.001, 0.1999999, 0.25, 0.2], 0.2),\n        # Test case D\n        ([0.01, 0.02, 0.03, 0.2], 0.0),\n        # Test case E\n        ([0.4, 0.6, 1.0], 1.0),\n        # Test case F\n        ([0.0, 0.0, 0.05, 0.2, 0.5, 0.9], 0.05),\n    ]\n\n    all_results = []\n    for p_list, alpha_val in test_cases:\n        k, rejections, q_vals = fdr_procedure(p_list, alpha_val)\n        all_results.append([k, rejections, q_vals])\n    \n    # Print the final output in the specified single-line list-of-lists format.\n    # The default string representation of a list of lists is \"[k, [bools], [qs]]\".\n    # Joining these with commas and wrapping in \"[]\" produces the desired format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2967187"}]}