{"hands_on_practices": [{"introduction": "Before we can simulate or analyze a Neural ODE, we must first understand its basic structure. This exercise provides a concrete look at the \"neural network\" component that defines the system's dynamics. By calculating the total number of learnable parameters, you will gain a practical appreciation for the model's complexity and its capacity to capture the intricate behaviors of a biological network like a signaling pathway.", "problem": "In the field of systems biology, researchers aim to model the complex dynamics of biological networks. A modern approach involves using a Neural Ordinary Differential Equation (Neural ODE) to learn the governing equations from time-series data.\n\nConsider a simplified signaling pathway involving $N_p = 5$ distinct protein species. The state of this system at any time $t$ is described by a state vector $\\mathbf{y}(t) \\in \\mathbb{R}^{N_p}$, where the $i$-th component of $\\mathbf{y}(t)$ represents the concentration of the $i$-th protein. The dynamics are modeled by the autonomous ODE system $\\frac{d\\mathbf{y}}{dt} = f(\\mathbf{y}; \\theta)$, where the function $f$ is represented by a fully-connected feedforward neural network. This network takes the current state vector $\\mathbf{y}$ as its input and outputs its time derivative $\\frac{d\\mathbf{y}}{dt}$. The learnable parameters of the network are denoted by $\\theta$.\n\nThe architecture of this neural network is as follows:\n1.  An input layer that accepts the state vector $\\mathbf{y}$.\n2.  A single hidden layer containing $N_h = 32$ neurons.\n3.  An output layer that produces the derivative vector $\\frac{d\\mathbf{y}}{dt}$.\n\nEach layer is fully connected to the next without any skip connections. The hidden layer and the output layer each have their own bias vectors.\n\nCalculate the total number of learnable parameters (i.e., the sum of all weights and biases) in the neural network function $f$.", "solution": "The network implements a mapping from an input of dimension $N_{p}$ to a hidden layer of size $N_{h}$ and then to an output of dimension $N_{p}$. For a fully connected layer from size $a$ to size $b$, the number of weights is $ba$ and the number of biases is $b$.\n\nHidden layer parameters:\n- Weights from input to hidden: $N_{h} \\times N_{p}$.\n- Biases in hidden: $N_{h}$.\n\nOutput layer parameters:\n- Weights from hidden to output: $N_{p} \\times N_{h}$.\n- Biases in output: $N_{p}$.\n\nThus, the total number of learnable parameters is\n$$\n(N_{h}N_{p} + N_{h}) + (N_{p}N_{h} + N_{p}) = 2N_{p}N_{h} + N_{h} + N_{p}.\n$$\nSubstituting $N_{p} = 5$ and $N_{h} = 32$ gives\n$$\n2 \\cdot 5 \\cdot 32 + 32 + 5 = 320 + 37 = 357.\n$$", "answer": "$$\\boxed{357}$$", "id": "1453836"}, {"introduction": "Real biological systems are often subject to abrupt, discrete events, such as the sudden administration of a drug or a rapid environmental change. A robust model must be able to account for these instantaneous shifts. This problem demonstrates how to integrate such a \"jump discontinuity\" into the continuous dynamics of a Neural ODE, a fundamental technique for simulating realistic interventions and their consequences over time.", "problem": "In a simplified systems biology model, the concentration of a protein 'P', denoted by $z(t)$ in units of micromolars ($\\mu\\text{M}$), is modeled using a Neural Ordinary Differential Equation (Neural ODE). The dynamics of the system are governed by the equation:\n$$\n\\frac{dz(t)}{dt} = f_\\theta(z(t))\n$$\nwhere the function $f_\\theta$ is a very simple neural network layer represented by a linear transformation without a bias term: $f_\\theta(z) = w \\cdot z$. The parameter $w$ represents the net rate constant for the production and degradation of protein P.\n\nThe system starts at time $t=0$ with an initial concentration. At a specific later time, a researcher injects a compound into the cell culture that causes an instantaneous change in the concentration of protein P.\n\nGiven the following parameters:\n- Initial concentration at $t=0$: $z_0 = 10.0$ $\\mu\\text{M}$\n- Neural ODE parameter: $w = -0.50$ $s^{-1}$\n- Time of injection: $t_{inject} = 2.0$ $s$\n- Instantaneous increase in concentration due to injection: $\\Delta z = 50.0$ $\\mu\\text{M}$\n\nCalculate the concentration of protein P at the final time $t_{final} = 4.0$ $s$. Express your final answer in micromolars ($\\mu\\text{M}$), rounded to three significant figures.", "solution": "We are given the Neural ODE $\\frac{dz(t)}{dt} = w z(t)$ with a jump (instantaneous increase) $\\Delta z$ at $t = t_{\\text{inject}}$. For $0 \\leq t < t_{\\text{inject}}$, the solution of the linear ODE is obtained by separation of variables:\n$$\n\\frac{dz}{z} = w \\, dt \\quad \\Rightarrow \\quad \\ln|z| = w t + C \\quad \\Rightarrow \\quad z(t) = C \\exp(w t).\n$$\nImposing the initial condition $z(0) = z_{0}$ gives $C = z_{0}$, hence\n$$\nz(t) = z_{0} \\exp(w t), \\quad 0 \\leq t < t_{\\text{inject}}.\n$$\nThe left limit at injection is\n$$\nz(t_{\\text{inject}}^{-}) = z_{0} \\exp(w t_{\\text{inject}}).\n$$\nThe injection causes an instantaneous jump:\n$$\nz(t_{\\text{inject}}^{+}) = z(t_{\\text{inject}}^{-}) + \\Delta z = z_{0} \\exp(w t_{\\text{inject}}) + \\Delta z.\n$$\nFor $t \\geq t_{\\text{inject}}$, the dynamics again follow $\\frac{dz}{dt} = w z$ with initial condition $z(t_{\\text{inject}}^{+})$ at $t_{\\text{inject}}$, yielding\n$$\nz(t) = z(t_{\\text{inject}}^{+}) \\exp(w (t - t_{\\text{inject}})).\n$$\nEvaluating at $t = t_{\\text{final}}$ and combining terms gives the closed form\n$$\nz(t_{\\text{final}}) = \\left[z_{0} \\exp(w t_{\\text{inject}}) + \\Delta z \\right] \\exp(w (t_{\\text{final}} - t_{\\text{inject}})) = z_{0} \\exp(w t_{\\text{final}}) + \\Delta z \\exp(w (t_{\\text{final}} - t_{\\text{inject}})).\n$$\nNow substitute the given values $z_{0} = 10.0$, $w = -0.50$, $t_{\\text{inject}} = 2.0$, $t_{\\text{final}} = 4.0$, and $\\Delta z = 50.0$:\n$$\nz(4.0) = 10.0 \\exp(-2.0) + 50.0 \\exp(-1.0).\n$$\nNumerically,\n$$\n\\exp(-2.0) \\approx 0.1353352832, \\quad \\exp(-1.0) \\approx 0.3678794412,\n$$\nso\n$$\nz(4.0) \\approx 10.0 \\times 0.1353352832 + 50.0 \\times 0.3678794412 \\approx 1.353352832 + 18.39397206 \\approx 19.74732489.\n$$\nRounded to three significant figures, the concentration at $t_{\\text{final}} = 4.0$ s is $19.7$ $\\mu\\text{M}$.", "answer": "$$\\boxed{19.7}$$", "id": "1453781"}, {"introduction": "A key power of a trained Neural ODE model is its ability to perform *in silico* experiments that would be difficult or costly in a lab. This practice moves beyond simple simulation to predictive intervention, exploring how to correctly model a permanent gene knockout. The challenge here is not just to change an initial value, but to fundamentally alter the rules of the system's evolution, forcing you to think critically about how to impose lasting constraints on the learned dynamics.", "problem": "A systems biologist is modeling a gene regulatory network consisting of $N$ genes using a Neural Ordinary Differential Equation (Neural ODE). The state of the system at time $t$ is represented by a vector $\\mathbf{y}(t) \\in \\mathbb{R}^{N}$, where each component $y_i(t)$ corresponds to the concentration of the protein product of gene $i$. The temporal evolution of the system is governed by the differential equation:\n$$\n\\frac{d\\mathbf{y}}{dt} = f(\\mathbf{y}; \\theta)\n$$\nHere, $f: \\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ is a deep neural network with learned parameters $\\theta$, which approximates the true underlying vector field of the gene regulatory dynamics. The model has been successfully trained on experimental time-series data from the wild-type organism.\n\nThe biologist now wants to use this trained model to perform an *in silico* experiment simulating a permanent, complete gene knockout of a specific gene, say gene $k$ (where $1 \\leq k \\leq N$). A permanent knockout means that the gene is rendered completely non-functional, and its corresponding protein product can no longer be synthesized. The goal is to simulate the network's behavior starting from a state where the concentration of protein $k$ is zero and ensuring it remains zero for all subsequent time.\n\nWhich of the following approaches correctly modifies the simulation procedure to achieve this permanent knockout of gene $k$, using the already trained function $f(\\mathbf{y}; \\theta)$?\n\nA. Set the initial condition for the $k$-th component to zero, $y_k(t_0) = 0$, and leave the initial conditions for all other genes $y_j(t_0)$ ($j \\neq k$) unchanged. Then, solve the original ODE $\\frac{d\\mathbf{y}}{dt} = f(\\mathbf{y}; \\theta)$.\n\nB. During each evaluation of the vector field function $f(\\mathbf{y}; \\theta)$ within the ODE solver, modify the *input vector* by always setting its $k$-th component to zero before passing it to the neural network.\n\nC. Retrain the entire neural network $f$ from scratch, using a modified training dataset in which all measured values of the concentration $y_k$ have been artificially set to zero.\n\nD. Define a new vector field function $f'(\\mathbf{y}; \\theta)$ for the simulation. During each evaluation within the ODE solver, first compute the output of the original network, $\\mathbf{v} = f(\\mathbf{y}; \\theta)$, and then construct the output of $f'$ by setting its $k$-th component to zero, $v'_k = 0$, while keeping all other components the same, $v'_j = v_j$ for $j \\neq k$. Solve the new ODE $\\frac{d\\mathbf{y}}{dt} = f'(\\mathbf{y}; \\theta)$ with the initial condition $y_k(t_0) = 0$.\n\nE. Alter the architecture of the neural network by removing all connections leading to the $k$-th output neuron and setting its bias term to zero. Then, use this modified network to solve the ODE.", "solution": "We require a permanent knockout of component $k$, meaning the simulated trajectory must satisfy $y_{k}(t_{0})=0$ and remain on the invariant subspace $S=\\{\\mathbf{y}\\in\\mathbb{R}^{N}: y_{k}=0\\}$ for all subsequent time. A necessary and sufficient condition for $S$ to be invariant under the flow is that the $k$-th component of the vector field vanishes whenever $y_{k}=0$, i.e., $f_{k}(\\mathbf{y};\\theta)=0$ for all $\\mathbf{y}\\in S$. Since the trained $f$ is not guaranteed to satisfy this, the simulation procedure must be modified so that the effective vector field used by the ODE solver enforces\n$$\n\\frac{dy_{k}}{dt}=0 \\quad \\text{and} \\quad y_{k}(t_{0})=0,\n$$\nwhile allowing the remaining components to evolve according to the learned dynamics evaluated at states with $y_{k}=0$.\n\nEvaluate the options:\n\nA. Setting only $y_{k}(t_{0})=0$ and integrating $\\frac{d\\mathbf{y}}{dt}=f(\\mathbf{y};\\theta)$ does not ensure $y_{k}(t)=0$ thereafter, because in general $f_{k}(\\mathbf{y};\\theta)\\neq 0$ on $S$. Therefore $S$ need not be invariant, and $y_{k}$ can leave zero.\n\nB. Modifying the input to $f$ by projecting $\\mathbf{y}$ to $\\Pi(\\mathbf{y})$ with $\\Pi(\\mathbf{y})_{k}=0$ yields the effective ODE\n$$\n\\frac{d\\mathbf{y}}{dt}=f(\\Pi(\\mathbf{y});\\theta).\n$$\nThis still gives $\\frac{dy_{k}}{dt}=f_{k}(\\Pi(\\mathbf{y});\\theta)$, which generically is not identically zero. Hence $y_{k}$ will not be held at zero unless an additional constraint is imposed; this does not achieve a permanent knockout by itself.\n\nC. Retraining on data with $y_{k}$ set to zero is unnecessary and does not guarantee that $\\frac{dy_{k}}{dt}=0$ in simulation; moreover, it does not satisfy the requirement to use the already trained $f(\\mathbf{y};\\theta)$.\n\nD. Define a modified vector field $f'$ by masking only the $k$-th output:\n$$\n\\mathbf{v}=f(\\mathbf{y};\\theta), \\quad v'_{k}=0, \\quad v'_{j}=v_{j}\\ \\text{for}\\ j\\neq k,\n$$\nand solve\n$$\n\\frac{d\\mathbf{y}}{dt}=f'(\\mathbf{y};\\theta), \\quad y_{k}(t_{0})=0.\n$$\nThen $\\frac{dy_{k}}{dt}=0$ for all $t$, so $y_{k}(t)\\equiv 0$. For $j\\neq k$, $\\frac{dy_{j}}{dt}=f_{j}(\\mathbf{y};\\theta)$ evaluated at states with $y_{k}=0$ (since $y_{k}$ is held at zero), correctly propagating the absence of gene $k$ to the rest of the network. This uses the already trained $f$ and only modifies the simulation procedure.\n\nE. Forcing the $k$-th output to zero by altering the network architecture also makes $\\frac{dy_{k}}{dt}\\equiv 0$ and, with $y_{k}(t_{0})=0$, would keep $y_{k}(t)=0$. However, this does not use the already trained function $f(\\mathbf{y};\\theta)$ as-is; it changes the model architecture rather than the simulation procedure. The prompt asks for a modification to the simulation using the trained $f$, which $E$ does not satisfy.\n\nTherefore, only option D correctly and directly enforces a permanent knockout using the already trained $f(\\mathbf{y};\\theta)$ in the simulation.", "answer": "$$\\boxed{D}$$", "id": "1453843"}]}