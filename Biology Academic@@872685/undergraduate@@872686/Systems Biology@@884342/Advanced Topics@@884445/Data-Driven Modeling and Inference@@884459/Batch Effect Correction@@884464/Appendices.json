{"hands_on_practices": [{"introduction": "Before exploring sophisticated algorithms, it is essential to grasp the fundamental goal of batch correction: making data from different experimental runs comparable. This exercise provides a concrete, numerical introduction to this process using a simple mean-centering technique. By manually adjusting a small gene expression dataset, you will see firsthand how to align data from different batches to a common baseline, removing systematic shifts while preserving the underlying variation within each sample group [@problem_id:1418467].", "problem": "In a systems biology experiment designed to analyze the transcriptional response of cancer cells to a new drug, a researcher collects gene expression data. Due to logistical constraints, the experiment is conducted in two separate batches. After data acquisition, the researcher suspects a significant batch effect, where the measurement platform's calibration differed between the two runs.\n\nBelow is a table of the raw, unitless gene expression values for two genes, G1 and G2, across six cell samples. Samples S1, S2, and S3 belong to Batch 1, while samples S4, S5, and S6 belong to Batch 2.\n\n|       | S1 | S2 | S3 | S4 | S5 | S6 |\n| :---- | :- | :- | :- | :- | :- | :- |\n| **G1**| 5  | 7  | 9  | 12 | 15 | 18 |\n| **G2**| 22 | 30 | 26 | 35 | 41 | 38 |\n\nTo correct for the batch effect, the researcher decides to apply a gene-wise mean adjustment. This correction method adjusts the data for each gene individually, such that after correction, the mean expression value within each batch becomes equal to the overall mean expression value of that gene across all samples.\n\nCalculate the full corrected data matrix, with genes as rows and samples as columns. Present your final answer as a 2x6 matrix.", "solution": "The goal is to adjust the expression data for each gene so that the mean of the values within each batch is equal to the overall mean for that gene across all samples. We will perform this correction for each gene, G1 and G2, independently.\n\nLet $x_{g,s}$ be the expression value of gene $g$ in sample $s$. The adjusted value $x'_{g,s}$ for a sample in batch $b$ is given by the formula:\n$$x'_{g,s} = x_{g,s} + (\\text{Mean}_{g,\\text{overall}} - \\text{Mean}_{g,b})$$\nwhere $\\text{Mean}_{g,b}$ is the mean expression of gene $g$ in batch $b$, and $\\text{Mean}_{g,\\text{overall}}$ is the mean expression of gene $g$ across all samples.\n\n**Step 1: Correction for Gene G1**\n\nFirst, we calculate the required means for Gene G1.\nThe samples for G1 in Batch 1 are {5, 7, 9}. The mean for Batch 1 is:\n$$ \\text{Mean}_{\\text{G1, Batch 1}} = \\frac{5 + 7 + 9}{3} = \\frac{21}{3} = 7 $$\nThe samples for G1 in Batch 2 are {12, 15, 18}. The mean for Batch 2 is:\n$$ \\text{Mean}_{\\text{G1, Batch 2}} = \\frac{12 + 15 + 18}{3} = \\frac{45}{3} = 15 $$\nThe overall mean for G1 across all six samples is:\n$$ \\text{Mean}_{\\text{G1, overall}} = \\frac{5 + 7 + 9 + 12 + 15 + 18}{6} = \\frac{66}{6} = 11 $$\nNow, we calculate the adjustment factors for each batch.\nFor Batch 1, the adjustment factor is:\n$$ \\Delta_1 = \\text{Mean}_{\\text{G1, overall}} - \\text{Mean}_{\\text{G1, Batch 1}} = 11 - 7 = +4 $$\nFor Batch 2, the adjustment factor is:\n$$ \\Delta_2 = \\text{Mean}_{\\text{G1, overall}} - \\text{Mean}_{\\text{G1, Batch 2}} = 11 - 15 = -4 $$\nWe apply these adjustments to the original G1 data.\nFor Batch 1 (S1, S2, S3):\n$$ x'_{\\text{G1,S1}} = 5 + 4 = 9 $$\n$$ x'_{\\text{G1,S2}} = 7 + 4 = 11 $$\n$$ x'_{\\text{G1,S3}} = 9 + 4 = 13 $$\nFor Batch 2 (S4, S5, S6):\n$$ x'_{\\text{G1,S4}} = 12 - 4 = 8 $$\n$$ x'_{\\text{G1,S5}} = 15 - 4 = 11 $$\n$$ x'_{\\text{G1,S6}} = 18 - 4 = 14 $$\nThe corrected expression row for G1 is [9, 11, 13, 8, 11, 14].\n\n**Step 2: Correction for Gene G2**\n\nNext, we repeat the process for Gene G2.\nThe samples for G2 in Batch 1 are {22, 30, 26}. The mean for Batch 1 is:\n$$ \\text{Mean}_{\\text{G2, Batch 1}} = \\frac{22 + 30 + 26}{3} = \\frac{78}{3} = 26 $$\nThe samples for G2 in Batch 2 are {35, 41, 38}. The mean for Batch 2 is:\n$$ \\text{Mean}_{\\text{G2, Batch 2}} = \\frac{35 + 41 + 38}{3} = \\frac{114}{3} = 38 $$\nThe overall mean for G2 across all six samples is:\n$$ \\text{Mean}_{\\text{G2, overall}} = \\frac{22 + 30 + 26 + 35 + 41 + 38}{6} = \\frac{192}{6} = 32 $$\nNow, we calculate the adjustment factors for each batch for G2.\nFor Batch 1, the adjustment factor is:\n$$ \\Delta_1 = \\text{Mean}_{\\text{G2, overall}} - \\text{Mean}_{\\text{G2, Batch 1}} = 32 - 26 = +6 $$\nFor Batch 2, the adjustment factor is:\n$$ \\Delta_2 = \\text{Mean}_{\\text{G2, overall}} - \\text{Mean}_{\\text{G2, Batch 2}} = 32 - 38 = -6 $$\nWe apply these adjustments to the original G2 data.\nFor Batch 1 (S1, S2, S3):\n$$ x'_{\\text{G2,S1}} = 22 + 6 = 28 $$\n$$ x'_{\\text{G2,S2}} = 30 + 6 = 36 $$\n$$ x'_{\\text{G2,S3}} = 26 + 6 = 32 $$\nFor Batch 2 (S4, S5, S6):\n$$ x'_{\\text{G2,S4}} = 35 - 6 = 29 $$\n$$ x'_{\\text{G2,S5}} = 41 - 6 = 35 $$\n$$ x'_{\\text{G2,S6}} = 38 - 6 = 32 $$\nThe corrected expression row for G2 is [28, 36, 32, 29, 35, 32].\n\n**Step 3: Assemble the Final Matrix**\n\nWe now assemble the corrected rows for G1 and G2 into a final 2x6 matrix as requested. The first row corresponds to G1, and the second row corresponds to G2.\n$$\n\\text{Corrected Matrix} = \\begin{pmatrix}\n9 & 11 & 13 & 8 & 11 & 14 \\\\\n28 & 36 & 32 & 29 & 35 & 32\n\\end{pmatrix}\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 9 & 11 & 13 & 8 & 11 & 14 \\\\ 28 & 36 & 32 & 29 & 35 & 32 \\end{pmatrix}}$$", "id": "1418467"}, {"introduction": "The most powerful strategy against batch effects is not computational correction, but thoughtful experimental design. This practice shifts our focus from post-hoc data cleaning to proactive prevention, emphasizing the old adage that an ounce of prevention is worth a pound of cure. By evaluating different ways to arrange samples on experimental plates, you will learn how a balanced design can prevent the biological signal of interest from becoming tangled, or \"confounded,\" with technical artifacts from the start [@problem_id:1418479].", "problem": "A systems biology research group is conducting a proteomic study to identify protein expression differences between cancerous and healthy tissues. They have collected 20 tumor samples (T) and 20 healthy control samples (C). The analysis will be performed using a mass spectrometer that processes samples loaded onto 24-well plates. Due to the total of 40 samples, the experiment must be run in two separate batches, using two 24-well plates (Plate 1 and Plate 2).\n\nIt is known that non-biological, systematic variations can occur between different batches (e.g., due to minor differences in instrument calibration or reagent preparation on different days). This is known as a \"batch effect\". An experimental design is \"confounded\" if it is impossible to distinguish the biological effect of interest (cancer vs. healthy) from the technical batch effect.\n\nWhich of the following experimental designs for distributing the 40 samples across the two 24-well plates is optimal for minimizing the confounding between the treatment effect (T vs. C) and the batch effect (Plate 1 vs. Plate 2)?\n\nA. Place all 20 tumor samples (T) on Plate 1 and all 20 healthy control samples (C) on Plate 2.\n\nB. Completely randomize the assignment of the 40 samples to the 48 available wells on the two plates, leaving 8 wells empty at random.\n\nC. Place 10 tumor samples (T) and 10 healthy control samples (C) on Plate 1. Place the remaining 10 tumor samples (T) and 10 healthy control samples (C) on Plate 2. Leave 4 wells empty on each plate.\n\nD. Fill Plate 1 completely with 12 tumor samples (T) and 12 healthy control samples (C). Place the remaining 8 tumor samples (T) and 8 healthy control samples (C) on Plate 2.", "solution": "We formalize the notion of confounding between the biological treatment (tumor versus control) and the technical batch (Plate 1 versus Plate 2) using a linear model and the orthogonality of design columns.\n\nLet there be $N=40$ samples. Define, for sample $i$,\n- the treatment indicator $x_{i}=1$ if tumor (T) and $x_{i}=0$ if control (C),\n- the batch indicator $z_{i}=1$ if Plate 1 and $z_{i}=0$ if Plate 2.\n\nIn a linear model $y_{i}=\\mu+\\tau x_{i}+\\beta z_{i}+\\epsilon_{i}$, confounding between treatment and batch occurs if the design columns for $x$ and $z$ are not orthogonal. A practical diagnostic is the sample covariance between $x$ and $z$:\n$$\n\\operatorname{Cov}(x,z)\\ \\propto\\ \\bar{xz}-\\bar{x}\\,\\bar{z},\n$$\nwhere $\\bar{x}$, $\\bar{z}$ and $\\bar{xz}$ are the sample means of $x$, $z$ and $xz$, respectively. Absence of confounding corresponds to $\\bar{xz}=\\bar{x}\\,\\bar{z}$, i.e., orthogonality of treatment and batch in the design. In terms of counts, letting $n$ denote total samples, $n_{1}$ the number on Plate 1, and $t_{1}$ the number of tumor samples on Plate 1, we have\n$$\n\\bar{x}=\\frac{20}{40}=\\frac{1}{2},\\quad \\bar{z}=\\frac{n_{1}}{40},\\quad \\bar{xz}=\\frac{t_{1}}{40}.\n$$\nThus orthogonality is equivalent to\n$$\n\\frac{t_{1}}{40}=\\frac{1}{2}\\cdot\\frac{n_{1}}{40}\\quad\\Longleftrightarrow\\quad t_{1}=\\frac{n_{1}}{2},\n$$\nwhich means that each plate should have the same proportion of tumor and control as the overall study, namely one half tumor and one half control.\n\nWe now assess each option.\n\nA. All tumor on Plate 1 and all control on Plate 2. Here $n_{1}=20$ and $t_{1}=20$, so\n$$\n\\bar{xz}=\\frac{20}{40}=\\frac{1}{2},\\quad \\bar{x}\\bar{z}=\\frac{1}{2}\\cdot\\frac{20}{40}=\\frac{1}{4},\n$$\nhence $\\bar{xz}-\\bar{x}\\bar{z}\\neq 0$. This is complete confounding.\n\nB. Completely randomize across 48 wells with 8 empty at random. While the expected proportion of tumor per plate equals the overall proportion, randomization does not guarantee $t_{1}=n_{1}/2$ in the realized allocation. Therefore $\\bar{xz}-\\bar{x}\\bar{z}$ need not be zero, and confounding may occur by chance. This is not optimal when a deterministic balanced allocation is available.\n\nC. Put 10 tumor and 10 control on Plate 1, and 10 tumor and 10 control on Plate 2, with 4 empty wells per plate. Here $n_{1}=20$ and $t_{1}=10$, so\n$$\n\\bar{xz}=\\frac{10}{40}=\\frac{1}{4},\\quad \\bar{x}\\bar{z}=\\frac{1}{2}\\cdot\\frac{20}{40}=\\frac{1}{4},\n$$\nhence $\\bar{xz}-\\bar{x}\\bar{z}=0$. Treatment and batch are orthogonal, so there is no confounding. In addition, the batch sizes are equal, which improves precision of adjusted treatment estimates.\n\nD. Fill Plate 1 with 12 tumor and 12 control, and place 8 tumor and 8 control on Plate 2. Here $n_{1}=24$ and $t_{1}=12$, so\n$$\n\\bar{xz}=\\frac{12}{40}=\\frac{3}{10},\\quad \\bar{x}\\bar{z}=\\frac{1}{2}\\cdot\\frac{24}{40}=\\frac{3}{10},\n$$\nhence $\\bar{xz}-\\bar{x}\\bar{z}=0$. Treatment and batch are also orthogonal, so there is no confounding. However, the unequal batch sizes reduce efficiency relative to a design with equal numbers per batch when estimating treatment effects adjusted for batch.\n\nAmong the provided options, both C and D eliminate confounding by ensuring equal tumor-to-control proportion within each plate. Between these, C is optimal because it also balances the total number of samples per plate, which yields more efficient estimation and guards against plate-level differences unrelated to treatment while preserving zero confounding.", "answer": "$$\\boxed{C}$$", "id": "1418479"}, {"introduction": "What happens when an experimental design is fundamentally flawed, leading to a \"worst-case scenario\" of perfect confounding? This advanced problem explores such a situation, where the biological condition of interest is perfectly entangled with the experimental batch, making them statistically inseparable by standard methods. This exercise challenges you to think beyond conventional correction and consider how external biological knowledge, such as using \"negative control\" genes, can provide a principled way to salvage meaningful biological insights from an otherwise unusable dataset [@problem_id:2374330].", "problem": "You are given an omics data matrix with $p$ molecular features (e.g., genes) measured across $n$ samples from a Ribonucleic Acid sequencing (RNA-seq) experiment. Samples were processed in $2$ laboratory batches: Batch $1$ and Batch $2$. There are $2$ biological conditions: control and treatment. Due to the study design, the data are perfectly confounded: every sample in Batch $1$ is control, and every sample in Batch $2$ is treatment.\n\nAssume a per-feature additive model for log-scale measurements: for feature $i \\in \\{1,\\dots,p\\}$ and sample $j \\in \\{1,\\dots,n\\}$,\n$$\nx_{ij} \\;=\\; \\mu_i \\;+\\; \\beta_i\\, y_j \\;+\\; \\gamma_i\\, b_j \\;+\\; \\varepsilon_{ij},\n$$\nwhere $y_j \\in \\{0,1\\}$ indicates biological condition (control $0$, treatment $1$), $b_j \\in \\{1,2\\}$ indicates batch (Batch $1$ or Batch $2$), $\\mu_i$ is a baseline for feature $i$, $\\beta_i$ is the biological effect of condition on feature $i$, $\\gamma_i$ is the batch effect on feature $i$, and $\\varepsilon_{ij}$ is mean-zero noise. In this dataset, perfect confounding implies $y_j$ is a deterministic function of $b_j$ (all $b_j{=}1$ have $y_j{=}0$ and all $b_j{=}2$ have $y_j{=}1$).\n\nWhich strategy provides a principled way to salvage any interpretable information about the biological effect $\\beta_i$ under these constraints?\n\nA. Fit a standard linear model including both batch and condition as covariates and estimate $\\beta_i$ directly from the current data.\n\nB. Use Remove Unwanted Variation (RUV) with negative control features, such as known housekeeping genes or External RNA Control Consortium (ERCC) spike-in controls, to estimate unwanted factors attributable to batch and regress them out, then analyze residuals for condition effects.\n\nC. Compute Principal Component Analysis (PCA) on all features, regress out the top principal component assumed to represent batch, and then perform differential analysis.\n\nD. Apply Surrogate Variable Analysis (SVA) without control features to infer hidden batch factors directly from the current data and adjust for them.\n\nE. Discard the dataset entirely because perfect confounding makes it impossible to recover any information about $\\beta_i$ in any circumstance.", "solution": "The problem statement must first be validated for scientific soundness and logical consistency.\n\n### Step 1: Extract Givens\n- An omics data matrix with $p$ molecular features and $n$ samples from an RNA-seq experiment.\n- $2$ laboratory batches: Batch $1$ and Batch $2$.\n- $2$ biological conditions: control and treatment.\n- The data are perfectly confounded: all samples in Batch $1$ are control, and all samples in Batch $2$ are treatment.\n- An additive model for log-scale measurements is assumed for each feature $i \\in \\{1,\\dots,p\\}$ and sample $j \\in \\{1,\\dots,n\\}$:\n$$\nx_{ij} \\;=\\; \\mu_i \\;+\\; \\beta_i\\, y_j \\;+\\; \\gamma_i\\, b_j \\;+\\; \\varepsilon_{ij}\n$$\n- $x_{ij}$: log-scale measurement for feature $i$ in sample $j$.\n- $\\mu_i$: baseline expression level for feature $i$.\n- $\\beta_i$: the biological effect of the condition on feature $i$. This is the parameter of interest.\n- $y_j \\in \\{0,1\\}$: indicator for biological condition (control=$0$, treatment=$1$).\n- $b_j \\in \\{1,2\\}$: indicator for laboratory batch.\n- $\\gamma_i$: the batch effect on feature $i$.\n- $\\varepsilon_{ij}$: mean-zero random noise.\n- The confounding is specified as $y_j$ being a deterministic function of $b_j$. For all samples $j$, if $b_j=1$ then $y_j=0$, and if $b_j=2$ then $y_j=1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a classic and severe issue in experimental design known as perfect confounding, where the variable of interest (biological condition) is perfectly collinear with a nuisance variable (laboratory batch). The supplied linear model is a standard way to represent such effects in bioinformatics. The question asks for a principled strategy to estimate the biological effect $\\beta_i$ despite this confounding.\n\nTo analyze the model with rigor, we must properly encode the categorical variables $y_j$ and $b_j$. The coding for $y_j$ is already given as a $0/1$ dummy variable. For batch $b_j$, we can define a corresponding dummy variable $B_j$ such that $B_j=0$ for Batch $1$ and $B_j=1$ for Batch $2$. The relationship defined by the perfect confounding is thus $y_j = B_j$ for all samples $j$.\n\nLet us rewrite the model using this standard dummy variable formulation. The original model is $x_{ij} = \\mu_i + \\beta_i y_j + \\gamma_i b_j + \\varepsilon_{ij}$. Using $b_j \\in \\{1,2\\}$ directly as a numeric predictor is problematic as it imposes a linear relationship that may not be appropriate. A more standard formulation replaces the term $\\gamma_i b_j$ with a term involving the dummy variable $B_j$, for instance $\\gamma_i' B_j$. The model becomes:\n$$\nx_{ij} = \\alpha_i + \\beta_i y_j + \\gamma_i' B_j + \\varepsilon_{ij}\n$$\nwhere $\\alpha_i$ is the new intercept (representing the mean for the baseline group, i.e., control in Batch $1$). Given the perfect confounding, $y_j = B_j$. Substituting this into the model:\n$$\nx_{ij} = \\alpha_i + \\beta_i y_j + \\gamma_i' y_j + \\varepsilon_{ij} = \\alpha_i + (\\beta_i + \\gamma_i') y_j + \\varepsilon_{ij}\n$$\nThis equation demonstrates the core problem: the model is unidentifiable. From the observed data, one can only estimate the combined coefficient $\\theta_i = \\beta_i + \\gamma_i'$. It is mathematically impossible to separate the biological effect $\\beta_i$ from the batch effect $\\gamma_i'$ using only the information contained in the data matrix and the confounded design.\n\nThe problem is scientifically grounded, well-posed (it asks for a strategy to overcome a well-defined problem), and objective. It does not violate any of the invalidity criteria. The central question is what, if any, external information or assumptions can be leveraged to deconvolve $\\beta_i$ from $\\gamma_i'$.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed to derive the solution by evaluating each proposed strategy.\n\n### Solution Derivation\n\nThe goal is to estimate $\\beta_i$, the biological effect, which is confounded with the batch effect $\\gamma_i'$. As established, this requires information from a source external to the confounded variables themselves. We now evaluate each option.\n\n**A. Fit a standard linear model including both batch and condition as covariates and estimate $\\beta_i$ directly from the current data.**\nThis approach attempts to fit the model $x_{ij} = \\alpha_i + \\beta_i y_j + \\gamma_i' B_j + \\varepsilon_{ij}$. As shown in the validation step, the covariates $y_j$ and $B_j$ are perfectly collinear ($y_j = B_j$). The design matrix of this linear model is rank-deficient. Any standard statistical software will fail to produce unique estimates for both $\\beta_i$ and $\\gamma_i'$. Typically, the software will either return an error message indicating perfect collinearity or automatically drop one of the redundant variables. If, for example, $B_j$ is dropped, the model becomes $x_{ij} = \\alpha_i + \\beta_i^* y_j + \\varepsilon_{ij}$, where the estimated coefficient $\\beta_i^*$ is an estimate of the sum $(\\beta_i + \\gamma_i')$, not of $\\beta_i$ alone. Thus, this strategy cannot disentangle the effect of interest.\nVerdict: **Incorrect**.\n\n**B. Use Remove Unwanted Variation (RUV) with negative control features, such as known housekeeping genes or External RNA Control Consortium (ERCC) spike-in controls, to estimate unwanted factors attributable to batch and regress them out, then analyze residuals for condition effects.**\nThis strategy introduces crucial external information: the identity of so-called \"negative control\" features. These are features (e.g., genes) that are known *a priori* to be unaffected by the biological condition. For such a control feature $c$, the biological effect is $\\beta_c = 0$. The model for this feature simplifies to:\n$$\nx_{cj} = \\alpha_c + (0) \\cdot y_j + \\gamma_c' B_j + \\varepsilon_{cj} = \\alpha_c + \\gamma_c' B_j + \\varepsilon_{cj}\n$$\nSince $y_j = B_j$, the model can also be written in terms of $y_j$: $x_{cj} = \\alpha_c + \\gamma_c' y_j + \\varepsilon_{cj}$. For these control features, any observed variation between the two groups (control/Batch $1$ vs. treatment/Batch $2$) is, by assumption, due entirely to the batch effect $\\gamma_c'$ (and noise).\nThe RUV methodology leverages this assumption. It uses the measurements of these negative control features to estimate the latent factors of unwanted variation. These estimated factors serve as a proxy for the batch effect, uncontaminated by the biological signal of interest. Once these batch effect factors $\\hat{W}$ are estimated, they can be included as covariates in a new linear model for every feature $i$:\n$$\nx_{ij} = \\alpha_i' + \\beta_i y_j + \\sum_k \\delta_{ik} \\hat{W}_{kj} + \\varepsilon_{ij}'\n$$\nIn this adjusted model, the term $\\sum_k \\delta_{ik} \\hat{W}_{kj}$ accounts for the batch effect. Because the estimate of the batch effect $\\hat{W}$ was derived independently of the biological effect (using control features where $\\beta_c=0$), the coefficient $\\beta_i$ for the condition variable $y_j$ now provides an estimate of the biological effect, adjusted for batch. This is a principled and widely accepted method for resolving this type of confounding, provided that a reliable set of negative control features is available.\nVerdict: **Correct**.\n\n**C. Compute Principal Component Analysis (PCA) on all features, regress out the top principal component assumed to represent batch, and then perform differential analysis.**\nPCA is an unsupervised method that identifies the major axes of variation in a dataset. In this experimental design, the single largest source of variation is the division of samples into two groups: (Control, Batch $1$) and (Treatment, Batch $2$). The first principal component (PC1) will almost certainly capture this dominant axis of variation. However, this variation is a combination of the true biological differences and the technical batch effects. PCA has no information to distinguish one from the other. Therefore, PC1 will represent the confounded signal $(\\beta_i + \\gamma_i')$. If one \"regresses out\" PC1 from the data, one is removing both the unwanted batch effect and the biological signal of interest. Any subsequent differential analysis on the residuals will fail to find the biological effect, as it has already been removed. This approach is fundamentally flawed as it \"throws the baby out with the bathwater.\"\nVerdict: **Incorrect**.\n\n**D. Apply Surrogate Variable Analysis (SVA) without control features to infer hidden batch factors directly from the current data and adjust for them.**\nSVA is designed to estimate latent sources of variation when negative controls are unknown. It is a powerful tool for correcting for unmodeled batch effects in well-designed experiments. However, its mechanism relies on being able to distinguish the effects of the primary variable of interest from other, independent (orthogonal) sources of variation. In the case of perfect confounding, the batch effect is not an independent source of variation; it is perfectly collinear with the primary variable (condition). The SVA algorithm attempts to find variation in the data that is not explained by the primary variable. Since the batch effect is perfectly explained by (i.e. identical to) the primary variable, SVA has no mathematical basis to identify it as a separate, \"surrogate\" variable. It cannot deconvolve two perfectly collinear signals without external information. Therefore, SVA will either fail to identify any surrogate variables or produce a surrogate variable that is itself collinear with the condition variable, offering no solution to the identifiability problem.\nVerdict: **Incorrect**.\n\n**E. Discard the dataset entirely because perfect confounding makes it impossible to recover any information about $\\beta_i$ in any circumstance.**\nThis statement is an over-generalization and is factually incorrect. While perfect confounding makes it impossible to estimate $\\beta_i$ from the confounded data *alone*, it does not preclude any analysis under *any* circumstances. As demonstrated in the analysis of option B, the introduction of valid external information or assumptions (e.g., the existence and identity of negative control genes) provides a principled way to deconvolve the effects and estimate $\\beta_i$. The problem is not necessarily hopeless; it is merely difficult and requires careful application of methods that can incorporate such prior knowledge.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "2374330"}]}