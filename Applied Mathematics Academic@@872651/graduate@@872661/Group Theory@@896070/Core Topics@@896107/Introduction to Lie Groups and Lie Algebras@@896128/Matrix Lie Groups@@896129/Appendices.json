{"hands_on_practices": [{"introduction": "The foundation of any Lie algebra is its non-commutative product, the Lie bracket. For matrix Lie algebras, this operation is defined as the commutator, $[X, Y] = XY - YX$. This exercise provides direct computational practice with this fundamental operation, which encodes the infinitesimal non-commutativity of the corresponding Lie group elements [@problem_id:1629893].", "problem": "Consider the Lie algebra $\\mathfrak{sl}(2, \\mathbb{C})$, which is the set of all $2 \\times 2$ matrices with complex entries and a trace of zero. For any two matrices $X, Y \\in \\mathfrak{sl}(2, \\mathbb{C})$, the Lie bracket is defined as the matrix commutator $[X, Y] = XY - YX$.\n\nLet the matrices $A$ and $B$ be two specific elements of $\\mathfrak{sl}(2, \\mathbb{C})$, given by:\n$$ A = \\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix} $$\nwhere $i$ is the imaginary unit satisfying $i^{2} = -1$.\n\nYour task is to compute the Lie bracket $[A, B]$. Express your final answer as a $2 \\times 2$ matrix.", "solution": "We use the Lie bracket definition for matrices, $[A,B]=AB-BA$, with $A=\\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix}$ and $B=\\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix}$.\n\nFirst compute $AB$ by standard matrix multiplication:\n$$\nAB=\\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix}\\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix}\n=\\begin{pmatrix}\ni\\cdot 0 + 2\\cdot(-3) & i\\cdot 3 + 2\\cdot 0 \\\\\n1\\cdot 0 + (-i)\\cdot(-3) & 1\\cdot 3 + (-i)\\cdot 0\n\\end{pmatrix}\n=\\begin{pmatrix}\n-6 & 3i \\\\\n3i & 3\n\\end{pmatrix}.\n$$\n\nNext compute $BA$:\n$$\nBA=\\begin{pmatrix} 0 & 3 \\\\ -3 & 0 \\end{pmatrix}\\begin{pmatrix} i & 2 \\\\ 1 & -i \\end{pmatrix}\n=\\begin{pmatrix}\n0\\cdot i + 3\\cdot 1 & 0\\cdot 2 + 3\\cdot(-i) \\\\\n(-3)\\cdot i + 0\\cdot 1 & (-3)\\cdot 2 + 0\\cdot(-i)\n\\end{pmatrix}\n=\\begin{pmatrix}\n3 & -3i \\\\\n-3i & -6\n\\end{pmatrix}.\n$$\n\nTherefore,\n$$\n[A,B]=AB-BA=\\begin{pmatrix}\n-6-3 & 3i-(-3i) \\\\\n3i-(-3i) & 3-(-6)\n\\end{pmatrix}\n=\\begin{pmatrix}\n-9 & 6i \\\\\n6i & 9\n\\end{pmatrix}.\n$$\n\nThis result has trace $-9+9=0$, consistent with $[A,B]\\in\\mathfrak{sl}(2,\\mathbb{C})$.", "answer": "$$\\boxed{\\begin{pmatrix}-9 & 6i \\\\ 6i & 9\\end{pmatrix}}$$", "id": "1629893"}, {"introduction": "The exponential map provides the crucial bridge from the linear space of a Lie algebra to the curved manifold of its Lie group. It transforms infinitesimal generators (algebra elements) into finite transformations (group elements). This practice problem makes this abstract concept concrete by having you calculate the matrix exponential for a specific algebra element, revealing its direct correspondence to an intuitive geometric transformation in the plane [@problem_id:1629845].", "problem": "In the study of continuous symmetries, the matrix exponential provides a bridge from a Lie algebra, which is a vector space of infinitesimal generators, to a Lie group, which is a group of transformations. This problem explores the exponentiation of a matrix that generates both rotations and uniform scaling in a two-dimensional plane.\n\nConsider the Lie algebra consisting of all $2 \\times 2$ real matrices of the form:\n$$X = \\begin{pmatrix} \\alpha & \\beta \\\\ -\\beta & \\alpha \\end{pmatrix}$$\nwhere $\\alpha$ and $\\beta$ are arbitrary real numbers.\n\nYour task is to compute the matrix exponential $G = e^X$. The resulting matrix $G$ will be an element of the corresponding Lie group of similarity transformations in the plane. Express your final answer as a $2 \\times 2$ matrix whose entries are functions of $\\alpha$ and $\\beta$.", "solution": "The matrix exponential $e^X$ is defined by its Taylor series expansion:\n$$e^X = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!} = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\dots$$\nTo compute this for the given matrix $X$, we can decompose $X$ into two simpler, commuting parts. Let's write $X$ as a sum of a scalar matrix and a skew-symmetric matrix:\n$$X = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & \\alpha \\end{pmatrix} + \\begin{pmatrix} 0 & \\beta \\\\ -\\beta & 0 \\end{pmatrix}$$\nWe can factor out the scalar values $\\alpha$ and $\\beta$:\n$$X = \\alpha \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$$\nLet's define $A = \\alpha I$ and $B = \\beta J$, where $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ is the identity matrix and $J = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$. Thus, $X = A + B$.\n\nNext, we check if $A$ and $B$ commute.\n$$AB = (\\alpha I)(\\beta J) = \\alpha \\beta (IJ) = \\alpha \\beta J$$\n$$BA = (\\beta J)(\\alpha I) = \\beta \\alpha (JI) = \\alpha \\beta J$$\nSince $AB = BA$, the matrices commute. For commuting matrices, the exponential of their sum is the product of their exponentials: $e^{A+B} = e^A e^B$. We can compute $e^A$ and $e^B$ separately.\n\nFirst, let's compute $e^A = e^{\\alpha I}$:\n$$e^{\\alpha I} = \\sum_{k=0}^{\\infty} \\frac{(\\alpha I)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{\\alpha^k I^k}{k!}$$\nSince $I^k = I$ for any $k \\ge 1$ and $I^0=I$, we can factor out $I$:\n$$e^{\\alpha I} = I \\left( \\sum_{k=0}^{\\infty} \\frac{\\alpha^k}{k!} \\right) = e^{\\alpha} I = \\begin{pmatrix} e^{\\alpha} & 0 \\\\ 0 & e^{\\alpha} \\end{pmatrix}$$\n\nSecond, let's compute $e^B = e^{\\beta J}$. We need the powers of $J$:\n$J^0 = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$J^1 = J = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$\n$J^2 = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} = -I$\n$J^3 = J^2 J = (-I)J = -J$\n$J^4 = J^2 J^2 = (-I)(-I) = I$\nThe powers of $J$ are periodic with a period of 4, similar to the imaginary unit $i$ in complex numbers.\n\nNow we can write out the series for $e^{\\beta J}$:\n$$e^{\\beta J} = \\sum_{k=0}^{\\infty} \\frac{(\\beta J)^k}{k!} = \\frac{\\beta^0 J^0}{0!} + \\frac{\\beta^1 J^1}{1!} + \\frac{\\beta^2 J^2}{2!} + \\frac{\\beta^3 J^3}{3!} + \\dots$$\n$$e^{\\beta J} = I + \\beta J - \\frac{\\beta^2}{2!}I - \\frac{\\beta^3}{3!}J + \\frac{\\beta^4}{4!}I + \\dots$$\nLet's group the terms with $I$ and the terms with $J$:\n$$e^{\\beta J} = \\left(1 - \\frac{\\beta^2}{2!} + \\frac{\\beta^4}{4!} - \\dots \\right)I + \\left(\\beta - \\frac{\\beta^3}{3!} + \\frac{\\beta^5}{5!} - \\dots \\right)J$$\nWe recognize the Taylor series for cosine and sine:\n$$\\cos(\\beta) = 1 - \\frac{\\beta^2}{2!} + \\frac{\\beta^4}{4!} - \\dots$$\n$$\\sin(\\beta) = \\beta - \\frac{\\beta^3}{3!} + \\frac{\\beta^5}{5!} - \\dots$$\nSo, we can write:\n$$e^{\\beta J} = \\cos(\\beta) I + \\sin(\\beta) J$$\nSubstituting the matrices for $I$ and $J$:\n$$e^{\\beta J} = \\cos(\\beta) \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\sin(\\beta) \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(\\beta) & \\sin(\\beta) \\\\ -\\sin(\\beta) & \\cos(\\beta) \\end{pmatrix}$$\n\nFinally, we find $e^X$ by multiplying $e^A$ and $e^B$:\n$$e^X = e^A e^B = \\begin{pmatrix} e^{\\alpha} & 0 \\\\ 0 & e^{\\alpha} \\end{pmatrix} \\begin{pmatrix} \\cos(\\beta) & \\sin(\\beta) \\\\ -\\sin(\\beta) & \\cos(\\beta) \\end{pmatrix}$$\n$$e^X = \\begin{pmatrix} e^{\\alpha}\\cos(\\beta) & e^{\\alpha}\\sin(\\beta) \\\\ -e^{\\alpha}\\sin(\\beta) & e^{\\alpha}\\cos(\\beta) \\end{pmatrix}$$\nThis is the final matrix $G$. It represents a rotation by angle $\\beta$ and a uniform scaling by a factor of $e^{\\alpha}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\exp(\\alpha) \\cos(\\beta) & \\exp(\\alpha) \\sin(\\beta) \\\\ -\\exp(\\alpha) \\sin(\\beta) & \\exp(\\alpha) \\cos(\\beta) \\end{pmatrix}}$$", "id": "1629845"}, {"introduction": "Beyond understanding individual elements, a key goal is to characterize the global structure of a Lie group, starting with its dimensionâ€”the number of independent parameters needed to specify a group element. This exercise explores the dimension of the Special Unitary group $SU(n)$, a group of paramount importance in quantum physics. You will determine this dimension by analyzing the constraints defining the group, a process that highlights the powerful fact that the dimension of a Lie group is equal to the dimension of its Lie algebra [@problem_id:1629848].", "problem": "In quantum mechanics, the state of a system with $n$ distinct accessible states is described by a vector in an $n$-dimensional complex Hilbert space. Transformations that preserve the probabilistic nature of quantum states are represented by unitary matrices. A particularly important class of such transformations forms the Special Unitary group, denoted as $SU(n)$. These transformations are crucial in theories of fundamental forces, such as the Standard Model of particle physics.\n\nAn $n \\times n$ complex matrix $U$ is an element of the Special Unitary group $SU(n)$ if it satisfies two defining conditions:\n1.  It is unitary: $U^\\dagger U = I$, where $U^\\dagger$ is the conjugate transpose (Hermitian conjugate) of $U$, and $I$ is the $n \\times n$ identity matrix.\n2.  It has a unit determinant: $\\det(U) = 1$.\n\nThe set of all such matrices for a given $n$ forms a continuous group known as a Lie group. The \"dimension\" of this Lie group is the number of independent, continuous real parameters required to uniquely specify any matrix within the group.\n\nYour task is to determine the dimension of the Special Unitary group $SU(n)$ as a function of the integer $n$, where $n \\geq 2$. Express your answer as a closed-form analytic expression in terms of $n$.", "solution": "We identify the dimension of the Lie group by counting the real dimension of its Lie algebra. The Lie algebra of the unitary group is\n$$\n\\mathfrak{u}(n)=\\{X\\in M_{n}(\\mathbb{C}) \\mid X^{\\dagger}=-X\\},\n$$\nthe skew-Hermitian matrices. To count its real dimension, write $X=(x_{ij})$. The condition $X^{\\dagger}=-X$ implies $x_{jj}=-\\overline{x_{jj}}$ for each $j$, so every diagonal entry is purely imaginary: $x_{jj}=i a_{j}$ with $a_{j}\\in \\mathbb{R}$, contributing $n$ real parameters. For each pair $i<j$, $x_{ij}$ can be any complex number, say $x_{ij}=\\alpha_{ij}+i\\beta_{ij}$ with $\\alpha_{ij},\\beta_{ij}\\in \\mathbb{R}$, and then $x_{ji}=-\\overline{x_{ij}}=-\\alpha_{ij}+i\\beta_{ij}$ is determined. Thus each pair $(i,j)$ with $i<j$ contributes $2$ real parameters. There are $n(n-1)/2$ such pairs, contributing $n(n-1)$ real parameters. Therefore\n$$\n\\dim_{\\mathbb{R}}\\mathfrak{u}(n)=n+n(n-1)=n^{2}.\n$$\n\nThe special unitary group $SU(n)$ is the subgroup of $U(n)$ with determinant $1$. Its Lie algebra is\n$$\n\\mathfrak{su}(n)=\\{X\\in \\mathfrak{u}(n)\\mid \\operatorname{tr}X=0\\}.\n$$\nFor $X\\in \\mathfrak{u}(n)$ the trace is purely imaginary, $\\operatorname{tr}X=i\\sum_{j=1}^{n}a_{j}$. The condition $\\operatorname{tr}X=0$ imposes the single real linear constraint $\\sum_{j=1}^{n}a_{j}=0$ on the $n$ real diagonal parameters, reducing the dimension by $1$. Hence\n$$\n\\dim_{\\mathbb{R}}\\mathfrak{su}(n)=n^{2}-1.\n$$\nSince the dimension of a Lie group equals the real dimension of its Lie algebra, the dimension of $SU(n)$ is $n^{2}-1$.\n\nEquivalently, one can count constraints directly on matrices: an $n\\times n$ complex matrix has $2n^{2}$ real parameters; the unitarity condition $U^{\\dagger}U=I$ imposes $n^{2}$ independent real equations (matching the $n^{2}$-dimensional real space of Hermitian matrices), giving $\\dim U(n)=2n^{2}-n^{2}=n^{2}$; the determinant condition $\\det(U)=1$ removes one real degree of freedom (the overall $U(1)$ phase of $\\det(U)$), yielding $\\dim SU(n)=n^{2}-1$.\n\nThus, the dimension of $SU(n)$ as a function of $n$ is $n^{2}-1$.", "answer": "$$\\boxed{n^{2}-1}$$", "id": "1629848"}]}