{"hands_on_practices": [{"introduction": "This first exercise bridges the gap between an abstract stochastic process and the concrete state-space model required for filtering. You will translate the description of a signal corrupted by noise into the framework of the Kalman-Bucy filter, derive the resulting time-varying Riccati differential equation for the error variance, and solve it to see how estimation accuracy evolves over time. [@problem_id:701789] This practice is fundamental for learning how to model real-world problems and for understanding the transient behavior of the filter before it reaches a steady state.", "problem": "Let $X_t$ be a standard one-dimensional Brownian motion, which is a continuous-time stochastic process satisfying:\n1.  $X_0 = 0$.\n2.  For any $0 \\le s  t$, the increment $X_t - X_s$ is a Gaussian random variable with mean $0$ and variance $t-s$.\n3.  For any sequence of disjoint time intervals, the corresponding increments are independent random variables.\n\nThe process $X_t$ is not observed directly. Instead, we observe a related process $Y_t$ defined by:\n$$\nY_t = \\int_0^t X_s ds + B_t\n$$\nwhere $B_t$ is another standard one-dimensional Brownian motion, independent of $X_t$.\n\nLet $\\mathcal{F}_t^Y = \\sigma(\\{Y_s : 0 \\le s \\le t\\})$ be the natural filtration generated by the observation process $Y_t$. The optimal estimate of $X_t$ given the history of observations up to time $t$ is the conditional expectation $\\hat{X}_t = \\mathbb{E}[X_t | \\mathcal{F}_t^Y]$.\n\nThe filtering error variance at time $t$ is defined as $P(t) = \\mathbb{E}[(X_t - \\hat{X}_t)^2]$.\n\nFor a fixed time $T  0$, determine the filtering error variance $P(T)$.", "solution": "1.  To apply the Kalman-Bucy filter, we first formulate the problem in state-space form. Let the state be $x_t = X_t$. Its dynamic is $\\mathrm{d}x_t = \\mathrm{d}W_t$, where $W_t$ is a standard Brownian motion. This corresponds to the state equation $\\mathrm{d}x_t = A x_t \\mathrm{d}t + G \\mathrm{d}W_t$ with $A=0$, $G=1$, and thus process noise intensity $Q = GG^\\top = 1$. The observation process is $Y_t = \\int_0^t X_s \\mathrm{d}s + B_t$. In differential form, this is $\\mathrm{d}Y_t = X_t \\mathrm{d}t + \\mathrm{d}B_t$. This corresponds to the measurement equation $\\mathrm{d}y_t = C x_t \\mathrm{d}t + D \\mathrm{d}V_t$ with $C=1$, $D=1$, and thus measurement noise intensity $R = DD^\\top = 1$.\n\n2. The continuous-time scalar Riccati differential equation for the error variance $P(t)$ is:\n$$\\frac{\\mathrm{d}P}{\\mathrm{d}t} = 2AP + Q - \\frac{C^2 P^2}{R}$$\nSubstituting the parameters $A=0$, $Q=1$, $C=1$, and $R=1$, we get:\n$$\\frac{\\mathrm{d}P}{\\mathrm{d}t}=1-P^2$$\n3. The initial state is known, $X_0 = 0$, so the initial estimate is perfect, $\\hat{X}_0 = 0$. This means the initial error variance is $P(0)=0$. We solve the differential equation by separating variables:\n$$\\int\\frac{\\mathrm{d}P}{1-P^2}=\\int \\mathrm{d}t \\implies \\operatorname{arctanh}(P)=t+C_0$$\n4. Using the initial condition $P(0)=0$, we find $\\operatorname{arctanh}(0)=0+C_0$, which gives the integration constant $C_0=0$. Thus, the solution is:\n$$P(t)=\\tanh(t)$$\n5. Therefore, the filtering error variance at the fixed time $T$ is:\n$$P(T)=\\tanh(T)$$", "answer": "$$\\boxed{\\tanh(T)}$$", "id": "701789"}, {"introduction": "The steady-state error covariance, the solution to the algebraic Riccati equation, quantifies the filter's best possible performance. This practice explores how that performance depends on the system's underlying noise characteristics. By analyzing the effect of changing process noise versus measurement noise in a simplified setting [@problem_id:2748182], you will develop a deeper intuition for the trade-offs inherent in filtering and the nonlinear way in which the Riccati equation balances these factors.", "problem": "Consider the continuous-time linear time-invariant (LTI) stochastic system\n$$\\dot{x}(t)=A\\,x(t)+w(t),\\quad y(t)=C\\,x(t)+v(t),$$\nwith\n$$A=\\begin{pmatrix}10\\\\0-2\\end{pmatrix},\\quad C=\\begin{pmatrix}10\\\\01\\end{pmatrix},$$\nand white Gaussian process noise and measurement noise characterized by covariances\n$$Q=q\\,I_2,\\quad R=r\\,I_2,$$\nwith $q=1$ and $r=1$. Assume standard detectability and stabilizability conditions hold so that the steady-state error covariance $P\\succ 0$ of the Kalman-Bucy filter exists, where $P$ is the unique positive-definite solution to the steady-state estimation error covariance equation. Define the principal variances as the eigenvalues of $P$.\n\nTwo perturbations of the noise statistics are considered separately:\n- Doubling the measurement noise covariance to $R_{\\mathrm{d}}=2\\,R$ while keeping $Q$ unchanged.\n- Halving the process noise covariance to $Q_{\\mathrm{h}}=\\tfrac{1}{2}\\,Q$ while keeping $R$ unchanged.\n\nLet $P_{\\mathrm{d}}$ and $P_{\\mathrm{h}}$ denote the corresponding steady-state error covariance matrices under $R_{\\mathrm{d}}$ and $Q_{\\mathrm{h}}$, respectively. Using first principles of continuous-time state estimation and without invoking any shortcut results beyond core definitions, derive how these changes affect the principal variances and compute the scalar ratio\n$$\\Gamma=\\frac{\\det\\!\\big(P_{\\mathrm{d}}\\big)}{\\det\\!\\big(P_{\\mathrm{h}}\\big)}.$$\nExpress your final answer as a single real number without units. No rounding is required.", "solution": "We begin from the continuous-time Kalman-Bucy filter framework. For the stochastic system\n$$\\dot{x}(t)=A\\,x(t)+w(t),\\quad y(t)=C\\,x(t)+v(t),$$\nwith process noise covariance $Q$ and measurement noise covariance $R$, the steady-state estimation error covariance $P$ satisfies the continuous-time algebraic Riccati equation\n$$A\\,P+P\\,A^{\\top}+Q-P\\,C^{\\top}R^{-1}C\\,P=0.$$\nThis is a core, well-tested result in continuous-time state estimation.\n\nWith the given data,\n$$A=\\begin{pmatrix}10\\\\0-2\\end{pmatrix},\\quad C=I_2,\\quad Q=q\\,I_2,\\quad R=r\\,I_2,$$\nand $q=1$, $r=1$. Because $A$, $C$, $Q$, and $R$ are all diagonal (with $C=I_2$), the Riccati equation decouples into two independent scalar equations, one for each coordinate. Let $a_1=1$ and $a_2=-2$ denote the diagonal entries of $A$, and let $p_i$ denote the $i$-th principal variance (eigenvalue) of $P$. For a general scalar mode with drift $a$ and scalar covariances $q$ and $r$, the steady-state scalar Riccati equation reads\n$$2\\,a\\,p+q-\\frac{p^2}{r}=0.$$\nRewriting,\n$$\\frac{p^2}{r}-2\\,a\\,p-q=0,$$\nwhich is a quadratic equation in $p$ with solutions\n$$p=r\\left(a\\pm \\sqrt{a^2+\\frac{q}{r}}\\right).$$\nThe physically relevant (stabilizing) solution is selected by requiring that the estimation error dynamics matrix $A-K\\,C$ be Hurwitz, where the steady-state Kalman gain is $K=P\\,C^{\\top}R^{-1}=\\frac{p}{r}$. In scalar form, this requires $a-\\frac{p}{r}0$. Choosing the plus sign yields\n$$\\frac{p}{r}=a+\\sqrt{a^2+\\frac{q}{r}},$$\nso that\n$$a-\\frac{p}{r}=a-\\left(a+\\sqrt{a^2+\\frac{q}{r}}\\right)=-\\sqrt{a^2+\\frac{q}{r}}0,$$\nensuring stability. The minus sign would give $a-\\frac{p}{r}=+\\sqrt{a^2+\\frac{q}{r}}0$, which is not stabilizing. Therefore, for any scalar mode,\n$$p=r\\left(a+\\sqrt{a^2+\\frac{q}{r}}\\right).$$\n\nWe now apply this to the two perturbations.\n\n1) Doubling the measurement noise covariance: $R_{\\mathrm{d}}=2\\,r\\,I_2$ with $q$ unchanged. For a scalar mode with drift $a$, the corresponding principal variance $p_{\\mathrm{d}}$ satisfies\n$$p_{\\mathrm{d}}=2\\,r\\left(a+\\sqrt{a^2+\\frac{q}{2\\,r}}\\right).$$\n\n2) Halving the process noise covariance: $Q_{\\mathrm{h}}=\\tfrac{1}{2}\\,q\\,I_2$ with $r$ unchanged. For the same scalar mode with drift $a$, the corresponding principal variance $p_{\\mathrm{h}}$ satisfies\n$$p_{\\mathrm{h}}=r\\left(a+\\sqrt{a^2+\\frac{q/2}{r}}\\right)=r\\left(a+\\sqrt{a^2+\\frac{q}{2\\,r}}\\right).$$\n\nComparing these two expressions for an arbitrary scalar mode with the same $a$, we obtain the exact proportionality\n$$p_{\\mathrm{d}}=2\\,p_{\\mathrm{h}}.$$\nThis proportionality holds for each decoupled mode independently and therefore for both principal variances of the $2\\times 2$ covariance matrix. Hence, if $P_{\\mathrm{h}}$ has eigenvalues $p_{\\mathrm{h},1}$ and $p_{\\mathrm{h},2}$, then $P_{\\mathrm{d}}$ has eigenvalues $p_{\\mathrm{d},1}=2\\,p_{\\mathrm{h},1}$ and $p_{\\mathrm{d},2}=2\\,p_{\\mathrm{h},2}$. Consequently,\n$$\\det\\!\\big(P_{\\mathrm{d}}\\big)=(2\\,p_{\\mathrm{h},1})\\,(2\\,p_{\\mathrm{h},2})=4\\,p_{\\mathrm{h},1}\\,p_{\\mathrm{h},2}=4\\,\\det\\!\\big(P_{\\mathrm{h}}\\big).$$\nTherefore, the requested ratio is\n$$\\Gamma=\\frac{\\det\\!\\big(P_{\\mathrm{d}}\\big)}{\\det\\!\\big(P_{\\mathrm{h}}\\big)}=4.$$\n\nThis conclusion is independent of the specific values of $a_1$ and $a_2$ and thus applies to the given pair $A=\\mathrm{diag}(1,-2)$ and $C=I_2$ with $q=1$ and $r=1$.", "answer": "$$\\boxed{4}$$", "id": "2748182"}, {"introduction": "While analytical solutions to the Riccati equation are rare, powerful numerical methods are the standard in practice. This exercise introduces two cornerstone techniques for solving the continuous-time algebraic Riccati equation: the Hamiltonian matrix method and the principle of LQR-duality. By applying these methods to a concrete system [@problem_id:2913246], you will not only learn a practical computational workflow but also appreciate the profound and useful symmetry that connects optimal estimation and optimal control.", "problem": "Consider the continuous-time, linear time-invariant stochastic system driven by white process and measurement noises\n$$\n\\dot{x}(t) \\,=\\, A\\,x(t) \\,+\\, G\\,w(t), \\qquad y(t) \\,=\\, C\\,x(t) \\,+\\, v(t),\n$$\nwhere $x(t) \\in \\mathbb{R}^{2}$, $y(t) \\in \\mathbb{R}$, $w(t)$ and $v(t)$ are mutually independent zero-mean Gaussian white noises with power spectral density matrices $Q_{c}$ and $R$, respectively. Suppose\n$$\nA \\,=\\, \\begin{pmatrix} 0  1 \\\\ -2  -3 \\end{pmatrix}, \\quad\nG \\,=\\, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\nC \\,=\\, \\begin{pmatrix} 1  0 \\end{pmatrix}, \\quad\nQ_{c} \\,=\\, 2, \\quad R \\,=\\, 1.\n$$\nStarting only from the defining properties of the Kalman–Bucy filter for continuous-time systems and the steady-state assumption, derive the steady-state error covariance condition and then compute the stabilizing steady-state error covariance using a Hamiltonian Schur decomposition of the associated Hamiltonian matrix. In parallel, use the duality between continuous-time filtering and the Linear Quadratic Regulator (LQR) to obtain the same steady-state solution by solving the dual LQR Algebraic Riccati Equation (ARE). Let the resulting steady-state error covariance be\n$$\nP \\,=\\, \\begin{pmatrix} p_{11}  p_{12} \\\\ p_{12}  p_{22} \\end{pmatrix}.\n$$\nReport the value of $p_{11}$ as a single real number. Round your answer to four significant figures. No units are required.", "solution": "**Derivation of the Steady-State Error Covariance Condition**\n\nThe dynamics of the estimation error, $e(t) = x(t) - \\hat{x}(t)$, for a continuous-time Kalman–Bucy filter are given by $\\dot{e}(t) = (A-K(t)C)e(t) + Gw(t) - K(t)v(t)$, where $K(t)$ is the Kalman gain. The error covariance matrix $P(t) = \\mathbb{E}[e(t)e(t)^\\top]$ propagates according to the differential Riccati equation:\n$$\n\\dot{P}(t) = A P(t) + P(t) A^\\top - K(t)C P(t) - P(t) C^\\top K(t)^\\top + G Q_c G^\\top + K(t) R K(t)^\\top\n$$\nThe optimal Kalman gain that minimizes the trace of $P(t)$ is $K(t) = P(t) C^\\top R^{-1}$. Substituting this into the differential Riccati equation yields:\n$$\n\\dot{P}(t) = A P(t) + P(t) A^\\top - P(t) C^\\top R^{-1} C P(t) + G Q_c G^\\top\n$$\nAt steady-state, $\\dot{P}(t) = 0$, leading to the continuous-time Algebraic Riccati Equation (ARE) for the filter:\n$$\n0 = AP + PA^\\top - PC^\\top R^{-1} CP + GQ_cG^\\top\n$$\nThis is the required steady-state error covariance condition.\n\n**Solution via Hamiltonian Matrix Properties**\n\nA unique, positive definite steady-state solution for the error covariance $P$ is guaranteed to exist if the pair $(A, G\\sqrt{Q_c})$ is stabilizable and the pair $(A, C)$ is detectable. The controllability matrix is $\\begin{pmatrix} G\\sqrt{Q_c}  AG\\sqrt{Q_c} \\end{pmatrix} = \\begin{pmatrix} 0  \\sqrt{2} \\\\ \\sqrt{2}  -3\\sqrt{2} \\end{pmatrix}$, which has full rank. The observability matrix is $\\begin{pmatrix} C \\\\ CA \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, which also has full rank. Both conditions are met.\n\nThe solution to the filter ARE is related to the stable invariant subspace of the associated Hamiltonian matrix $H_f$:\n$$\nH_f = \\begin{pmatrix} A^\\top  -C^\\top R^{-1} C \\\\ -G Q_c G^\\top  -A \\end{pmatrix}\n$$\nFirst, we compute the component matrices:\n$$\nA^\\top = \\begin{pmatrix} 0  -2 \\\\ 1  -3 \\end{pmatrix}, \\quad -A = \\begin{pmatrix} 0  -1 \\\\ 2  3 \\end{pmatrix}\n$$\n$$\n-C^\\top R^{-1} C = -\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (1)^{-1} \\begin{pmatrix} 1  0 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  0 \\end{pmatrix}\n$$\n$$\n-G Q_c G^\\top = -\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} (2) \\begin{pmatrix} 0  1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  -2 \\end{pmatrix}\n$$\nThe Hamiltonian matrix is:\n$$\nH_f = \\begin{pmatrix} 0  -2  -1  0 \\\\ 1  -3  0  0 \\\\ 0  0  0  -1 \\\\ 0  -2  2  3 \\end{pmatrix}\n$$\nThe characteristic polynomial of $H_f$ is $\\det(\\lambda I - H_f) = \\lambda^4 - 5\\lambda^2 + 6 = 0$. Letting $y = \\lambda^2$, we have $y^2 - 5y + 6 = (y-2)(y-3) = 0$. The eigenvalues are therefore $\\lambda = \\pm\\sqrt{2}$ and $\\lambda = \\pm\\sqrt{3}$.\nThe stable eigenvalues of $H_f$ are those with negative real parts: $-\\sqrt{2}$ and $-\\sqrt{3}$. A fundamental property of the ARE is that the eigenvalues of the closed-loop dynamics matrix for the optimal filter, $A_{cl} = A - KC$, must be equal to the stable eigenvalues of the Hamiltonian matrix. The steady-state Kalman gain is $K = PC^\\top R^{-1}$.\n$$\nK = \\begin{pmatrix} p_{11}  p_{12} \\\\ p_{12}  p_{22} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (1)^{-1} = \\begin{pmatrix} p_{11} \\\\ p_{12} \\end{pmatrix}\n$$\nThe closed-loop matrix is:\n$$\nA_{cl} = A - KC = \\begin{pmatrix} 0  1 \\\\ -2  -3 \\end{pmatrix} - \\begin{pmatrix} p_{11} \\\\ p_{12} \\end{pmatrix} \\begin{pmatrix} 1  0 \\end{pmatrix} = \\begin{pmatrix} -p_{11}  1 \\\\ -2-p_{12}  -3 \\end{pmatrix}\n$$\nThe characteristic polynomial of $A_{cl}$ is:\n$$\n\\det(\\lambda I - A_{cl}) = \\det\\begin{pmatrix} \\lambda+p_{11}  -1 \\\\ 2+p_{12}  \\lambda+3 \\end{pmatrix} = (\\lambda+p_{11})(\\lambda+3) + (2+p_{12}) = \\lambda^2 + (3+p_{11})\\lambda + (3p_{11} + p_{12} + 2)\n$$\nThe desired characteristic polynomial with eigenvalues $-\\sqrt{2}$ and $-\\sqrt{3}$ is:\n$$\n(\\lambda + \\sqrt{2})(\\lambda + \\sqrt{3}) = \\lambda^2 + (\\sqrt{2}+\\sqrt{3})\\lambda + \\sqrt{6}\n$$\nBy comparing the coefficients of the two polynomials, we obtain a system of equations:\n1) $3+p_{11} = \\sqrt{2}+\\sqrt{3}$\n2) $3p_{11} + p_{12} + 2 = \\sqrt{6}$\n\nFrom equation (1), we solve for $p_{11}$:\n$$\np_{11} = \\sqrt{2} + \\sqrt{3} - 3\n$$\n\n**Solution via Duality with LQR**\n\nThe continuous-time ARE for the filter is dual to the ARE for a Linear Quadratic Regulator (LQR) problem. The standard LQR problem seeks to find a control law $u=-K_c x$ that minimizes $\\int_0^\\infty (x^\\top Q_{lqr} x + u^\\top R_{lqr} u) dt$ for a system $\\dot{x}=A_{lqr}x+B_{lqr}u$. The corresponding ARE for the controller cost matrix $S$ is $A_{lqr}^\\top S + S A_{lqr} - S B_{lqr} R_{lqr}^{-1} B_{lqr}^\\top S + Q_{lqr} = 0$.\nThe duality mapping is: $A_{lqr} \\leftrightarrow A^\\top$, $B_{lqr} \\leftrightarrow C^\\top$, $Q_{lqr} \\leftrightarrow G Q_c G^\\top$, and $R_{lqr} \\leftrightarrow R$. The solution $S$ of the LQR ARE is the solution $P$ of the filter ARE.\nApplying this mapping to the LQR ARE:\n$$\n(A^\\top)^\\top P + P A^\\top - P C^\\top R^{-1} (C^\\top)^\\top P + G Q_c G^\\top = 0\n$$\n$$\nA P + P A^\\top - P C^\\top R^{-1} C P + G Q_c G^\\top = 0\n$$\nThis is precisely the filter ARE. We can verify our solution using the scalar equations derived from this matrix ARE:\ni) $(1,1) \\text{ element: } 2p_{12} - p_{11}^2 = 0 \\implies 2p_{12} = p_{11}^2$\nii) $(1,2) \\text{ element: } p_{22} - 2p_{11} - 3p_{12} - p_{11}p_{12} = 0$\niii) $(2,2) \\text{ element: } -4p_{12} - 6p_{22} + 2 - p_{12}^2 = 0$\nThe results from the Hamiltonian pole-placement method must satisfy this system. From equation (2) of the pole-placement method, $p_{12} = \\sqrt{6} - 2 - 3p_{11}$. Substituting $p_{11} = \\sqrt{2}+\\sqrt{3}-3$:\n$$\np_{12} = \\sqrt{6} - 2 - 3(\\sqrt{2}+\\sqrt{3}-3) = 7 + \\sqrt{6} - 3\\sqrt{2} - 3\\sqrt{3}\n$$\nWe can verify that these values satisfy equation (i):\n$$\np_{11}^2 = (\\sqrt{2}+\\sqrt{3}-3)^2 = (\\sqrt{2})^2 + (\\sqrt{3})^2 + (-3)^2 + 2\\sqrt{2}\\sqrt{3} - 6\\sqrt{2} - 6\\sqrt{3} = 14 + 2\\sqrt{6} - 6\\sqrt{2} - 6\\sqrt{3}\n$$\n$$\n2p_{12} = 2(7 + \\sqrt{6} - 3\\sqrt{2} - 3\\sqrt{3}) = 14 + 2\\sqrt{6} - 6\\sqrt{2} - 6\\sqrt{3}\n$$\nThe equation $p_{11}^2 = 2p_{12}$ is satisfied, confirming the consistency of the methods.\n\n**Final Calculation**\n\nThe value of $p_{11}$ is given by $p_{11} = \\sqrt{2} + \\sqrt{3} - 3$.\nUsing the numerical values $\\sqrt{2} \\approx 1.41421356$ and $\\sqrt{3} \\approx 1.73205081$:\n$$\np_{11} \\approx 1.41421356 + 1.73205081 - 3 = 3.14626437 - 3 = 0.14626437\n$$\nRounding to four significant figures, we get $p_{11} \\approx 0.1463$.", "answer": "$$\n\\boxed{0.1463}\n$$", "id": "2913246"}]}