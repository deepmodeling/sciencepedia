{"hands_on_practices": [{"introduction": "This first practice serves as a foundational exercise in mean-square stability analysis. We directly compare the long-term behavior of the true second moments of two cornerstone SDEs—the Ornstein-Uhlenbeck process and geometric Brownian motion—with the moments of their numerical approximations. By deriving the exact moment recursions for the Milstein scheme, you will discover that numerical stability is not guaranteed and depends critically on the step size $h$, leading you to compute the explicit stability thresholds [@problem_id:2988113].", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard Brownian motion on a filtered probability space satisfying the usual conditions. Consider two Itô stochastic differential equations (SDEs) with globally Lipschitz and linearly growing coefficients:\n1) The Ornstein–Uhlenbeck (OU) process defined by\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\lambda X_{t}\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t},\n$$\nwith $\\lambda0$, $\\sigma0$, and initial condition $X_{0}$ satisfying $\\mathbb{E}[X_{0}^{2}]\\infty$.\n2) The geometric Brownian motion (GBM) defined by\n$$\n\\mathrm{d}Y_{t} \\;=\\; \\mu Y_{t}\\,\\mathrm{d}t \\;+\\; \\sigma Y_{t}\\,\\mathrm{d}W_{t},\n$$\nwith $\\mu \\in \\mathbb{R}$, $\\sigma0$, and initial condition $Y_{0}$ satisfying $\\mathbb{E}[Y_{0}^{2}]\\infty$.\n\nLet $\\{t_{n}\\}_{n \\geq 0}$ denote a uniform grid with step size $h0$, $t_{n}=nh$, and let $\\Delta W_{n}:=W_{t_{n+1}}-W_{t_{n}}$. Apply the one-step Milstein scheme to each SDE to obtain numerical approximations $\\{X_{n}\\}_{n \\geq 0}$ for the OU process and $\\{Y_{n}\\}_{n \\geq 0}$ for the GBM, with $X_{0}$ and $Y_{0}$ given.\n\nStarting from first principles of Itô calculus and the definition of the Milstein method, and using only well-tested facts about Gaussian increments (e.g., $\\mathbb{E}[\\Delta W_{n}]=0$, $\\mathbb{E}[(\\Delta W_{n})^{2}]=h$, $\\mathbb{E}[(\\Delta W_{n})^{3}]=0$, $\\mathbb{E}[(\\Delta W_{n})^{4}]=3h^{2}$), carry out the following:\n- Derive the exact second-moment evolution $\\mathbb{E}[X_{t}^{2}]$ for the OU process, and determine the stationary second-moment bound as $t \\to \\infty$. Derive the second-moment recursion for the Milstein discretization (which, for additive noise, coincides with the Euler–Maruyama scheme), establish the condition on $h$ under which $\\{\\mathbb{E}[X_{n}^{2}]\\}_{n \\geq 0}$ remains uniformly bounded, and determine the largest allowable step size $h_{\\star}^{\\mathrm{OU}}$ for which this uniform boundedness (mean-square dissipativity) holds.\n- Derive the exact second-moment evolution $\\mathbb{E}[Y_{t}^{2}]$ for the GBM and state the parameter regime in which the exact dynamics are mean-square dissipative. For the Milstein scheme applied to the GBM, derive the multiplicative second-moment amplification factor $r(h)$ defined by $\\mathbb{E}[Y_{n+1}^{2}]=r(h)\\,\\mathbb{E}[Y_{n}^{2}]$. From the mean-square dissipativity requirement $r(h)1$, determine the largest allowable step size $h_{\\star}^{\\mathrm{GBM}}$ as an explicit function of $\\mu$ and $\\sigma$.\n\nExpress your final answer as a single closed-form analytical expression containing both thresholds $h_{\\star}^{\\mathrm{OU}}$ and $h_{\\star}^{\\mathrm{GBM}}$ arranged as a row matrix using the $\\mathrm{pmatrix}$ environment. No numerical approximation or rounding is required, and no units are to be included in the final answer.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in the field of numerical analysis for stochastic differential equations. We proceed with the solution.\n\nThe solution is presented in two parts, one for the Ornstein–Uhlenbeck (OU) process and one for the geometric Brownian motion (GBM), as requested.\n\n### Part 1: Ornstein–Uhlenbeck Process\n\nThe SDE for the OU process is given by:\n$$\n\\mathrm{d}X_{t} = -\\lambda X_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $\\lambda  0$, $\\sigma  0$, and $\\mathbb{E}[X_{0}^{2}]  \\infty$.\n\n**Exact Second-Moment Evolution**\nTo find the evolution of the second moment, $\\mathbb{E}[X_{t}^{2}]$, we apply Itô's lemma to the function $f(x) = x^{2}$. The derivatives are $f'(x)=2x$ and $f''(x)=2$. Itô's lemma states that for a process $X_t$ and a twice-differentiable function $f$,\n$$\n\\mathrm{d}f(X_{t}) = f'(X_{t})\\,\\mathrm{d}X_{t} + \\frac{1}{2}f''(X_{t})\\,(\\mathrm{d}X_{t})^{2}.\n$$\nSubstituting the SDE for $\\mathrm{d}X_{t}$, we get:\n$$\n\\mathrm{d}(X_{t}^{2}) = 2X_{t}(-\\lambda X_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}) + \\frac{1}{2}(2)(-\\lambda X_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t})^{2}.\n$$\nAccording to Itô calculus rules, $(\\mathrm{d}t)^{2}=0$, $\\mathrm{d}t\\,\\mathrm{d}W_{t}=0$, and $(\\mathrm{d}W_{t})^{2}=\\mathrm{d}t$. Thus, the quadratic variation term is $(\\mathrm{d}X_{t})^{2} = \\sigma^{2}(\\mathrm{d}W_{t})^{2} = \\sigma^{2}\\mathrm{d}t$.\nThe expression for $\\mathrm{d}(X_{t}^{2})$ becomes:\n$$\n\\mathrm{d}(X_{t}^{2}) = -2\\lambda X_{t}^{2}\\,\\mathrm{d}t + 2\\sigma X_{t}\\,\\mathrm{d}W_{t} + \\sigma^{2}\\,\\mathrm{d}t.\n$$\nLet $m_{2}(t) = \\mathbb{E}[X_{t}^{2}]$. Taking the expectation of the integral form of the above equation, and noting that the expectation of the Itô integral term is zero (i.e., $\\mathbb{E}[\\int_{0}^{t} 2\\sigma X_{s}\\,\\mathrm{d}W_{s}]=0$ since $X_s$ is adapted), we obtain an ordinary differential equation (ODE) for $m_{2}(t)$:\n$$\n\\frac{\\mathrm{d}m_{2}(t)}{\\mathrm{d}t} = \\mathbb{E}[-2\\lambda X_{t}^{2} + \\sigma^{2}] = -2\\lambda m_{2}(t) + \\sigma^{2}.\n$$\nThis is a first-order linear ODE, $m'_{2}(t) + 2\\lambda m_{2}(t) = \\sigma^{2}$. The solution with initial condition $m_{2}(0) = \\mathbb{E}[X_{0}^{2}]$ is:\n$$\nm_{2}(t) = \\frac{\\sigma^{2}}{2\\lambda} + \\left(\\mathbb{E}[X_{0}^{2}] - \\frac{\\sigma^{2}}{2\\lambda}\\right)\\exp(-2\\lambda t).\n$$\nAs $t \\to \\infty$, since $\\lambda  0$, the exponential term decays to zero. The stationary second-moment bound is:\n$$\n\\lim_{t \\to \\infty} \\mathbb{E}[X_{t}^{2}] = \\frac{\\sigma^{2}}{2\\lambda}.\n$$\n\n**Milstein Discretization and Mean-Square Stability**\nThe general one-step Milstein scheme for an SDE $\\mathrm{d}X_{t} = a(X_{t})\\,\\mathrm{d}t + b(X_{t})\\,\\mathrm{d}W_{t}$ is:\n$$\nX_{n+1} = X_{n} + a(X_n)h + b(X_n)\\Delta W_n + \\frac{1}{2}b(X_n)b'(X_n)((\\Delta W_n)^2 - h).\n$$\nFor the OU process, we have $a(x) = -\\lambda x$ and $b(x) = \\sigma$. Since $b(x)$ is a constant, its derivative $b'(x) = 0$. Therefore, the Milstein scheme simplifies to the Euler–Maruyama scheme:\n$$\nX_{n+1} = X_{n} - \\lambda X_{n} h + \\sigma \\Delta W_{n} = (1 - \\lambda h)X_{n} + \\sigma \\Delta W_{n}.\n$$\nTo find the second-moment recursion, we square both sides:\n$$\nX_{n+1}^{2} = ((1 - \\lambda h)X_{n} + \\sigma \\Delta W_{n})^{2} = (1 - \\lambda h)^{2}X_{n}^{2} + 2\\sigma(1 - \\lambda h)X_{n}\\Delta W_{n} + \\sigma^{2}(\\Delta W_{n})^{2}.\n$$\nLet $\\mathcal{M}_{n} = \\mathbb{E}[X_{n}^{2}]$. We take the expectation, conditional on the information at time $t_{n}$, denoted by $\\mathcal{F}_{t_{n}}$. Since $X_n$ is $\\mathcal{F}_{t_{n}}$-measurable and $\\Delta W_n$ is independent of $\\mathcal{F}_{t_{n}}$, we use the provided facts $\\mathbb{E}[\\Delta W_{n}] = 0$ and $\\mathbb{E}[(\\Delta W_{n})^{2}] = h$:\n$$\n\\mathbb{E}[X_{n+1}^{2} | \\mathcal{F}_{t_{n}}] = (1 - \\lambda h)^{2}X_{n}^{2} + 2\\sigma(1 - \\lambda h)X_{n}\\mathbb{E}[\\Delta W_{n}] + \\sigma^{2}\\mathbb{E}[(\\Delta W_{n})^{2}] = (1 - \\lambda h)^{2}X_{n}^{2} + \\sigma^{2}h.\n$$\nTaking the full expectation gives the recursion for $\\mathcal{M}_{n}$:\n$$\n\\mathcal{M}_{n+1} = (1 - \\lambda h)^{2}\\mathcal{M}_{n} + \\sigma^{2}h.\n$$\nFor the sequence $\\{\\mathcal{M}_{n}\\}_{n \\ge 0}$ to remain uniformly bounded for any finite initial second moment $\\mathcal{M}_{0}$, the recurrence must be stable. This requires the magnitude of the amplification factor of the homogeneous part to be strictly less than $1$. If the magnitude were equal to $1$, $\\mathcal{M}_{n}$ would grow arithmetically, and thus would not be uniformly bounded. The condition is:\n$$\n|1 - \\lambda h|  1.\n$$\nThis inequality is equivalent to $-1  1 - \\lambda h  1$.\nThe right-hand side, $1 - \\lambda h  1$, implies $-\\lambda h  0$. Since $\\lambda  0$ and $h  0$, this is always satisfied.\nThe left-hand side, $-1  1 - \\lambda h$, implies $\\lambda h  2$.\nThus, the condition for uniform boundedness is $h  \\frac{2}{\\lambda}$. The largest allowable step size is the supremum of this set, which is:\n$$\nh_{\\star}^{\\mathrm{OU}} = \\frac{2}{\\lambda}.\n$$\n\n### Part 2: Geometric Brownian Motion\n\nThe SDE for GBM is given by:\n$$\n\\mathrm{d}Y_{t} = \\mu Y_{t}\\,\\mathrm{d}t + \\sigma Y_{t}\\,\\mathrm{d}W_{t}\n$$\nwhere $\\mu \\in \\mathbb{R}$, $\\sigma  0$, and $\\mathbb{E}[Y_{0}^{2}]  \\infty$.\n\n**Exact Second-Moment Evolution**\nWe again apply Itô's lemma to $f(y) = y^{2}$, so $f'(y)=2y$ and $f''(y)=2$.\n$$\n\\mathrm{d}(Y_{t}^{2}) = 2Y_{t}(\\mu Y_{t}\\,\\mathrm{d}t + \\sigma Y_{t}\\,\\mathrm{d}W_{t}) + \\frac{1}{2}(2)(\\mu Y_{t}\\,\\mathrm{d}t + \\sigma Y_{t}\\,\\mathrm{d}W_{t})^{2}.\n$$\nThe quadratic variation term is $(\\mathrm{d}Y_{t})^{2} = \\sigma^{2}Y_{t}^{2}(\\mathrm{d}W_{t})^{2} = \\sigma^{2}Y_{t}^{2}\\mathrm{d}t$.\nSubstituting this, we get:\n$$\n\\mathrm{d}(Y_{t}^{2}) = 2\\mu Y_{t}^{2}\\,\\mathrm{d}t + 2\\sigma Y_{t}^{2}\\,\\mathrm{d}W_{t} + \\sigma^{2}Y_{t}^{2}\\mathrm{d}t = (2\\mu + \\sigma^{2})Y_{t}^{2}\\,\\mathrm{d}t + 2\\sigma Y_{t}^{2}\\,\\mathrm{d}W_{t}.\n$$\nLet $m_{2}(t) = \\mathbb{E}[Y_{t}^{2}]$. Taking the expectation, the Itô integral term vanishes, and we obtain the ODE:\n$$\n\\frac{\\mathrm{d}m_{2}(t)}{\\mathrm{d}t} = (2\\mu + \\sigma^{2})m_{2}(t).\n$$\nThe solution is $m_{2}(t) = m_{2}(0)\\exp((2\\mu + \\sigma^{2})t)$. For the exact dynamics to be mean-square dissipative, the second moment must decay to zero as $t \\to \\infty$. This requires the exponent to be negative. The parameter regime is:\n$$\n2\\mu + \\sigma^{2}  0.\n$$\n\n**Milstein Discretization and Mean-Square Stability**\nFor GBM, we have $a(y) = \\mu y$ and $b(y) = \\sigma y$, so $b'(y) = \\sigma$. The Milstein scheme is:\n$$\n\\begin{aligned}\nY_{n+1} = Y_{n} + \\mu Y_{n}h + \\sigma Y_{n}\\Delta W_{n} + \\frac{1}{2}(\\sigma Y_{n})(\\sigma)((\\Delta W_{n})^{2} - h) \\\\\n= Y_{n}\\left[1 + \\mu h + \\sigma \\Delta W_{n} + \\frac{1}{2}\\sigma^{2}((\\Delta W_{n})^{2} - h)\\right].\n\\end{aligned}\n$$\nThe problem defines the multiplicative amplification factor $r(h)$ via $\\mathbb{E}[Y_{n+1}^{2}] = r(h)\\mathbb{E}[Y_{n}^{2}]$. By the tower property and independence of $\\Delta W_n$ from $\\mathcal{F}_{t_n}$, we have:\n$$\nr(h) = \\mathbb{E}\\left[\\left(1 + \\mu h - \\frac{1}{2}\\sigma^{2}h + \\sigma\\Delta W_{n} + \\frac{1}{2}\\sigma^{2}(\\Delta W_{n})^{2}\\right)^{2}\\right].\n$$\nLet $C = 1 + \\mu h - \\frac{1}{2}\\sigma^{2}h$ and $Z_{n} = \\Delta W_{n}$. We need to compute $\\mathbb{E}[(C + \\sigma Z_{n} + \\frac{1}{2}\\sigma^{2}Z_{n}^{2})^{2}]$.\nExpanding the square:\n$$\n(C + \\sigma Z_{n} + \\frac{1}{2}\\sigma^{2}Z_{n}^{2})^{2} = C^{2} + \\sigma^{2}Z_{n}^{2} + \\frac{1}{4}\\sigma^{4}Z_{n}^{4} + 2C\\sigma Z_{n} + C\\sigma^{2}Z_{n}^{2} + \\sigma^{3}Z_{n}^{3}.\n$$\nTaking the expectation and using the given moments $\\mathbb{E}[Z_n]=0$, $\\mathbb{E}[Z_n^2]=h$, $\\mathbb{E}[Z_n^3]=0$, $\\mathbb{E}[Z_n^4]=3h^2$:\n$$\n\\begin{aligned}\nr(h) = \\mathbb{E}[C^{2}] + \\sigma^{2}\\mathbb{E}[Z_{n}^{2}] + \\frac{1}{4}\\sigma^{4}\\mathbb{E}[Z_{n}^{4}] + 2C\\sigma\\mathbb{E}[Z_{n}] + C\\sigma^{2}\\mathbb{E}[Z_{n}^{2}] + \\sigma^{3}\\mathbb{E}[Z_{n}^{3}] \\\\\n= C^{2} + \\sigma^{2}h + \\frac{1}{4}\\sigma^{4}(3h^{2}) + 0 + C\\sigma^{2}h + 0 \\\\\n= C^{2} + (1+C)\\sigma^{2}h + \\frac{3}{4}\\sigma^{4}h^{2}.\n\\end{aligned}\n$$\nSubstituting $C = 1 + (\\mu - \\frac{1}{2}\\sigma^{2})h$:\n$C^{2} = (1 + (\\mu - \\frac{1}{2}\\sigma^{2})h)^{2} = 1 + 2(\\mu - \\frac{1}{2}\\sigma^{2})h + (\\mu - \\frac{1}{2}\\sigma^{2})^{2}h^{2}$.\n$1+C = 2 + (\\mu - \\frac{1}{2}\\sigma^{2})h$.\nSo, $(1+C)\\sigma^{2}h = 2\\sigma^{2}h + (\\mu - \\frac{1}{2}\\sigma^{2})\\sigma^{2}h^{2}$.\nSumming all terms for $r(h)$:\n$$\n\\begin{aligned}\nr(h) = \\left(1 + (2\\mu - \\sigma^{2})h + (\\mu^{2}-\\mu\\sigma^{2}+\\frac{1}{4}\\sigma^{4})h^{2}\\right) + \\left(2\\sigma^{2}h + (\\mu\\sigma^{2}-\\frac{1}{2}\\sigma^{4})h^{2}\\right) + \\frac{3}{4}\\sigma^{4}h^{2} \\\\\n= 1 + (2\\mu - \\sigma^{2} + 2\\sigma^{2})h + (\\mu^{2}-\\mu\\sigma^{2}+\\frac{1}{4}\\sigma^{4} + \\mu\\sigma^{2}-\\frac{1}{2}\\sigma^{4} + \\frac{3}{4}\\sigma^{4})h^{2} \\\\\n= 1 + (2\\mu + \\sigma^{2})h + (\\mu^{2} + \\frac{1}{2}\\sigma^{4})h^{2}.\n\\end{aligned}\n$$\nThe mean-square dissipativity requirement is $r(h)  1$:\n$$\n1 + (2\\mu + \\sigma^{2})h + (\\mu^{2} + \\frac{1}{2}\\sigma^{4})h^{2}  1.\n$$\nFor $h  0$, this simplifies to:\n$$\n(2\\mu + \\sigma^{2}) + (\\mu^{2} + \\frac{1}{2}\\sigma^{4})h  0.\n$$\nFor this inequality to have a solution for $h  0$, we must have the constant term negative, since the term with $h$ has a positive coefficient $(\\mu^{2} + \\frac{1}{2}\\sigma^{4})  0$. This implies $2\\mu + \\sigma^{2}  0$, which is exactly the dissipativity condition for the continuous SDE.\nUnder this condition, we solve for $h$:\n$$\n(\\mu^{2} + \\frac{1}{2}\\sigma^{4})h  -(2\\mu + \\sigma^{2}).\n$$\n$$\nh  \\frac{-(2\\mu + \\sigma^{2})}{\\mu^{2} + \\frac{1}{2}\\sigma^{4}}.\n$$\nThe largest allowable step size is the supremum of this interval:\n$$\nh_{\\star}^{\\mathrm{GBM}} = \\frac{-(2\\mu + \\sigma^{2})}{\\mu^{2} + \\frac{1}{2}\\sigma^{4}}.\n$$\nThis threshold is valid only in the parameter regime $2\\mu+\\sigma^2  0$. Otherwise, no step size $h0$ can ensure mean-square dissipativity for the Milstein method.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{\\lambda}  \\frac{-(2\\mu + \\sigma^2)}{\\mu^2 + \\frac{1}{2}\\sigma^4}\n\\end{pmatrix}\n}\n$$", "id": "2988113"}, {"introduction": "While direct moment calculation is insightful, it can be difficult for more complex systems. This practice introduces the powerful and widely applicable Lyapunov function method for proving stability and deriving moment bounds. By constructing a discrete Lyapunov drift inequality for the Euler–Maruyama approximation of an Ornstein-Uhlenbeck process, you will learn a systematic technique to guarantee that the moments of a numerical solution remain uniformly bounded over time [@problem_id:2988098].", "problem": "Consider the scalar Ornstein–Uhlenbeck (OU) stochastic differential equation (SDE) $$\\mathrm{d}X_{t} \\,=\\, -\\lambda X_{t} \\,\\mathrm{d}t \\;+\\; \\sigma \\,\\mathrm{d}W_{t},$$ where $W_{t}$ is a standard Wiener process, $\\lambda0$ is the mean-reversion rate, and $\\sigma0$ is the diffusion magnitude. Let $X_{0}$ be independent of $W_{t}$. The explicit Euler–Maruyama time discretization with step size $h0$ is given by $$X_{n+1} \\,=\\, X_{n} \\;-\\; \\lambda h\\, X_{n} \\;+\\; \\sigma \\sqrt{h}\\, \\xi_{n+1},$$ where $(\\xi_{n})_{n\\geq 1}$ are independent standard normal random variables and independent of $X_{0}$. Define the Lyapunov function $$V(x) \\,=\\, 1 \\;+\\; x^{2}.$$\n\nStarting from the fundamental definitions of conditional expectation and the properties of the Euler–Maruyama scheme and Gaussian noise, derive a discrete Lyapunov drift inequality of the form $$\\mathbb{E}\\!\\left[V\\!\\left(X_{n+1}\\right)\\,\\middle|\\,X_{n}\\right] \\,\\leq\\, \\rho\\, V\\!\\left(X_{n}\\right) \\;+\\; \\beta,$$ for explicit constants $\\rho\\in(0,1)$ and $\\beta0$ that depend only on $\\lambda$, $\\sigma$, and $h$. Then use this inequality and iterated expectations to obtain a uniform (in $n$) bound on the moments $\\mathbb{E}\\!\\left[V\\!\\left(X_{n}\\right)\\right]$ and identify the smallest constant $$C \\;=\\; \\sup_{n\\geq 0} \\,\\mathbb{E}\\!\\left[V\\!\\left(X_{n}\\right)\\right]$$ that your argument guarantees, expressed solely in terms of $\\lambda$, $\\sigma$, $h$, and the law of $X_{0}$.\n\nFinally, compute this constant for the specific parameter values $$\\lambda \\,=\\, 1.7, \\quad \\sigma \\,=\\, 1.3, \\quad h \\,=\\, 0.4, \\quad X_{0} \\sim \\mathcal{N}(m,s^{2}) \\text{ with } m \\,=\\, -0.5 \\text{ and } s^{2} \\,=\\, 0.8.$$ Round your final answer for $C$ to four significant figures. No units are required.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- The scalar Ornstein–Uhlenbeck (OU) stochastic differential equation (SDE) is given by $\\mathrm{d}X_{t} = -\\lambda X_{t} \\,\\mathrm{d}t + \\sigma \\,\\mathrm{d}W_{t}$.\n- Parameters are the mean-reversion rate $\\lambda0$ and the diffusion magnitude $\\sigma0$.\n- $W_{t}$ is a standard Wiener process.\n- The initial condition $X_{0}$ is independent of $W_{t}$.\n- The explicit Euler–Maruyama time discretization with step size $h0$ is $X_{n+1} = X_{n} - \\lambda h\\, X_{n} + \\sigma \\sqrt{h}\\, \\xi_{n+1}$.\n- $(\\xi_{n})_{n\\geq 1}$ are independent standard normal random variables, $\\xi_n \\sim \\mathcal{N}(0,1)$, and are independent of $X_{0}$.\n- The Lyapunov function is defined as $V(x) = 1 + x^{2}$.\n- The task is to derive a discrete Lyapunov drift inequality $\\mathbb{E}[V(X_{n+1})|X_{n}] \\leq \\rho V(X_{n}) + \\beta$ for constants $\\rho\\in(0,1)$ and $\\beta0$.\n- Subsequently, use this to find the smallest constant $C = \\sup_{n\\geq 0} \\mathbb{E}[V(X_{n})]$ guaranteed by the argument.\n- Finally, compute $C$ for the specific parameters $\\lambda = 1.7$, $\\sigma = 1.3$, $h = 0.4$, and $X_{0} \\sim \\mathcal{N}(m, s^2)$ with $m = -0.5$ and $s^2 = 0.8$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The Ornstein-Uhlenbeck process is a fundamental model in stochastic calculus. The Euler-Maruyama method is a standard numerical scheme for SDEs. The use of a Lyapunov function to analyze the stability of a numerical method is a well-established technique in stochastic numerical analysis. The problem is well-posed, objective, and internally consistent. It provides all necessary information to derive the inequality and compute the final constant. The problem asks for standard derivations and calculations within its field, with no scientific or factual unsoundness, ambiguity, or missing information. A meaningful, unique solution exists, provided the standard numerical stability condition on the step size $h$ is met.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe first step is to derive the discrete Lyapunov drift inequality. We start by computing the conditional expectation of $V(X_{n+1})$ given $X_{n}$.\nThe Lyapunov function is $V(x) = 1 + x^{2}$. Applying this to $X_{n+1}$ from the Euler-Maruyama scheme:\n$$V(X_{n+1}) = 1 + X_{n+1}^2 = 1 + \\left((1 - \\lambda h)X_{n} + \\sigma \\sqrt{h}\\, \\xi_{n+1}\\right)^{2}.$$\nWe expand the squared term:\n$$X_{n+1}^2 = (1 - \\lambda h)^{2}X_{n}^2 + 2(1 - \\lambda h)X_{n}\\sigma\\sqrt{h}\\,\\xi_{n+1} + \\sigma^{2}h\\,\\xi_{n+1}^2.$$\nNow, we take the conditional expectation with respect to the sigma-algebra generated by $X_n$. Since $X_n$ is known in this context and $\\xi_{n+1}$ is independent with $\\xi_{n+1} \\sim \\mathcal{N}(0,1)$, we have $\\mathbb{E}[\\xi_{n+1}|X_n]=\\mathbb{E}[\\xi_{n+1}]=0$ and $\\mathbb{E}[\\xi_{n+1}^2|X_n]=\\mathbb{E}[\\xi_{n+1}^2]=1$.\n\\begin{align*} \\mathbb{E}[V(X_{n+1})|X_{n}] = \\mathbb{E}\\left[1 + (1 - \\lambda h)^{2}X_{n}^2 + 2(1 - \\lambda h)X_{n}\\sigma\\sqrt{h}\\,\\xi_{n+1} + \\sigma^{2}h\\,\\xi_{n+1}^2 \\,\\middle|\\, X_{n}\\right] \\\\ = 1 + (1 - \\lambda h)^{2}X_{n}^2 + 2(1 - \\lambda h)X_{n}\\sigma\\sqrt{h}\\,\\mathbb{E}[\\xi_{n+1}] + \\sigma^{2}h\\,\\mathbb{E}[\\xi_{n+1}^2] \\\\ = 1 + (1 - \\lambda h)^{2}X_{n}^2 + \\sigma^{2}h. \\end{align*}\nTo obtain the desired inequality form, we express the result in terms of $V(X_n) = 1 + X_n^2$.\n\\begin{align*} \\mathbb{E}[V(X_{n+1})|X_{n}] = 1 + (1 - \\lambda h)^{2}(V(X_n) - 1) + \\sigma^{2}h \\\\ = (1 - \\lambda h)^{2}V(X_n) + 1 - (1 - \\lambda h)^{2} + \\sigma^{2}h \\\\ = (1 - \\lambda h)^{2}V(X_n) + 1 - (1 - 2\\lambda h + \\lambda^{2}h^{2}) + \\sigma^{2}h \\\\ = (1 - \\lambda h)^{2}V(X_n) + 2\\lambda h - \\lambda^{2}h^{2} + \\sigma^{2}h. \\end{align*}\nThis is a discrete Lyapunov drift inequality of the form $\\mathbb{E}[V(X_{n+1})|X_{n}] \\leq \\rho V(X_{n}) + \\beta$, where the inequality is an equality in this case. The constants are:\n$$\\rho = (1 - \\lambda h)^{2}$$\n$$\\beta = 2\\lambda h - \\lambda^{2}h^{2} + \\sigma^{2}h.$$\nFor the analysis to proceed, we require $\\rho \\in (0,1)$, which implies $|1 - \\lambda h|  1$. This yields $-1  1 - \\lambda h  1$, which simplifies to $0  \\lambda h  2$. Given $\\lambda0$ and $h0$, the condition is $h  2/\\lambda$. If this holds, $\\rho  1$. Also, $\\beta = \\lambda h(2 - \\lambda h) + \\sigma^{2}h  0$ since all terms are positive under this condition.\n\nNext, we derive the uniform moment bound. Taking the total expectation of the inequality and letting $v_n = \\mathbb{E}[V(X_n)]$:\n$$v_{n+1} = \\mathbb{E}[\\mathbb{E}[V(X_{n+1})|X_{n}]] \\leq \\mathbb{E}[\\rho V(X_n) + \\beta] = \\rho v_n + \\beta.$$\nThis is a linear recurrence relation. By iterating, we find:\n$$v_n \\leq \\rho^{n} v_0 + \\beta \\sum_{k=0}^{n-1} \\rho^k = \\rho^{n} v_0 + \\beta \\frac{1-\\rho^n}{1-\\rho}.$$\nLet $y^* = \\beta/(1-\\rho)$. The inequality can be rewritten as:\n$$v_n \\leq \\rho^n (v_0 - y^*) + y^*.$$\nWe seek $C = \\sup_{n\\geq 0} v_n$. We consider two cases for the initial value $v_0 = \\mathbb{E}[V(X_0)]$.\n1. If $v_0 \\leq y^*$: Then $v_0 - y^* \\leq 0$. Since $\\rho^n \\geq 0$, it follows that $\\rho^n(v_0 - y^*) \\leq 0$, so $v_n \\leq y^*$. The sequence is bounded above by $y^*$.\n2. If $v_0  y^*$: Then $v_0 - y^*  0$. Since $0  \\rho  1$, the function $\\rho^n$ is maximized at $n=0$ (where $\\rho^0=1$). Therefore, for any $n \\geq 0$, $v_n \\leq \\rho^n (v_0 - y^*) + y^* \\leq \\rho^0 (v_0 - y^*) + y^* = v_0$. The sequence is bounded above by its initial value $v_0$.\nCombining these cases, the uniform bound for all $n \\geq 0$ is the maximum of the two upper bounds:\n$$C = \\sup_{n\\geq 0} \\mathbb{E}[V(X_n)] = \\max(v_0, y^*) = \\max\\left(\\mathbb{E}[V(X_0)], \\frac{\\beta}{1-\\rho}\\right).$$\nLet's express the terms using the problem parameters.\nFirst, $v_0 = \\mathbb{E}[V(X_0)] = \\mathbb{E}[1 + X_0^2] = 1 + \\mathbb{E}[X_0^2]$. Since $X_0 \\sim \\mathcal{N}(m, s^2)$, its second moment is $\\mathbb{E}[X_0^2] = \\text{Var}(X_0) + (\\mathbb{E}[X_0])^2 = s^2 + m^2$. Thus, $v_0 = 1 + m^2 + s^2$.\n\nSecond, we calculate $y^* = \\beta / (1-\\rho)$:\n$$1-\\rho = 1 - (1 - \\lambda h)^2 = 1 - (1 - 2\\lambda h + \\lambda^{2}h^{2}) = 2\\lambda h - \\lambda^{2}h^{2} = \\lambda h(2 - \\lambda h).$$\n$$\\beta = 2\\lambda h - \\lambda^{2}h^{2} + \\sigma^{2}h = (1-\\rho) + \\sigma^{2}h.$$\n$$y^* = \\frac{\\beta}{1-\\rho} = \\frac{(1-\\rho) + \\sigma^{2}h}{1-\\rho} = 1 + \\frac{\\sigma^{2}h}{1-\\rho} = 1 + \\frac{\\sigma^{2}h}{\\lambda h(2 - \\lambda h)} = 1 + \\frac{\\sigma^{2}}{\\lambda(2 - \\lambda h)}.$$\nSo, the smallest constant guaranteed by this argument is:\n$$C = \\max\\left(1 + m^2 + s^2, \\; 1 + \\frac{\\sigma^2}{\\lambda(2 - \\lambda h)}\\right).$$\n\n### Final Calculation\nWe are given the parameter values:\n$\\lambda = 1.7$, $\\sigma = 1.3$, $h = 0.4$, $m = -0.5$, $s^2 = 0.8$.\n\nFirst, we verify the stability condition $0  \\lambda h  2$:\n$$\\lambda h = 1.7 \\times 0.4 = 0.68.$$\nSince $0  0.68  2$, the stability condition is satisfied, and our derivation is valid for these parameters.\n\nNow we compute the two terms in the maximum function.\nThe first term is the initial expected value:\n$$\\mathbb{E}[V(X_0)] = 1 + m^2 + s^2 = 1 + (-0.5)^2 + 0.8 = 1 + 0.25 + 0.8 = 2.05.$$\nThe second term is the asymptotic bound:\n$$1 + \\frac{\\sigma^2}{\\lambda(2 - \\lambda h)} = 1 + \\frac{1.3^2}{1.7(2 - 0.68)} = 1 + \\frac{1.69}{1.7(1.32)} = 1 + \\frac{1.69}{2.244}.$$\nCalculating the fraction:\n$$\\frac{1.69}{2.244} \\approx 0.75311943.$$\nSo, the second term is approximately $1 + 0.75311943 = 1.75311943$.\n\nFinally, we find the constant $C$:\n$$C = \\max(2.05, 1.75311943) = 2.05.$$\nRounding to four significant figures gives $2.050$.", "answer": "$$\\boxed{2.050}$$", "id": "2988098"}, {"introduction": "Real-world applications often involve systems of multiple interacting stochastic processes. This final practice extends our analysis from scalar SDEs to the vector case, demonstrating how linear algebra can be a powerful tool for simplification. You will use a change of basis to decouple a two-dimensional linear SDE system into familiar scalar equations, allowing you to determine the stability of the entire system by analyzing the spectral properties of its constituent matrices [@problem_id:2988062].", "problem": "Consider the linear stochastic differential equation (SDE) in $\\mathbb{R}^{2}$ driven by a standard one-dimensional Brownian motion $W_{t}$,\n$$\ndX_{t} = A X_{t}\\,dt + B X_{t}\\,dW_{t}, \\qquad X_{0} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},\n$$\nwhere $A,B \\in \\mathbb{R}^{2 \\times 2}$ are defined by an orthogonal rotation matrix $Q$ and diagonal matrices $D_{A}$ and $D_{B}$ as follows:\n$$\nQ \\coloneqq \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  -1 \\\\ 1  \\phantom{-}1 \\end{pmatrix}, \\quad D_{A} \\coloneqq \\begin{pmatrix} -3  0 \\\\ 0  -\\frac{1}{2} \\end{pmatrix}, \\quad D_{B} \\coloneqq \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}, \\quad A \\coloneqq Q D_{A} Q^{\\top}, \\quad B \\coloneqq Q D_{B} Q^{\\top}.\n$$\nTasks:\n- Using only core definitions from Itô calculus and linear algebra, verify that $A$ and $B$ commute and are simultaneously diagonalizable by the orthogonal matrix $Q$. Introduce the rotated process $Y_{t} \\coloneqq Q^{\\top} X_{t}$ and write the decoupled scalar SDEs solved by the components of $Y_{t}$.\n- Starting from Itô’s formula and without invoking any pre-assembled solution formulas, derive a closed ordinary differential equation for the second moment of each scalar component and use it to compute the exact expression of $\\mathbb{E}\\!\\left[|X_{t}|^{2}\\right]$ as a function of $t$.\n- For a general pair of real matrices $A$ and $B$ that commute and are simultaneously diagonalizable with real eigenvalues $\\{a_{i}\\}_{i=1}^{2}$ and $\\{b_{i}\\}_{i=1}^{2}$, respectively, characterize exponential mean-square stability in terms of a spectral condition on $\\{a_{i},b_{i}\\}$. Apply your condition to the present numerical data and state whether the system is exponentially mean-square stable.\nProvide as your final answer the explicit analytic expression for $\\mathbb{E}\\!\\left[|X_{t}|^{2}\\right]$ found in the second task. No rounding is required and no units are involved.", "solution": "### Problem Validation\n\n#### Step 1: Extract Givens\nThe problem provides the following information:\n- A linear stochastic differential equation (SDE) in $\\mathbb{R}^{2}$: $dX_{t} = A X_{t}\\,dt + B X_{t}\\,dW_{t}$\n- The process $W_{t}$ is a standard one-dimensional Brownian motion.\n- The initial condition is $X_{0} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\n- The matrices $A, B \\in \\mathbb{R}^{2 \\times 2}$ are defined via $A \\coloneqq Q D_{A} Q^{\\top}$ and $B \\coloneqq Q D_{B} Q^{\\top}$.\n- The constituent matrices are:\n$$\nQ \\coloneqq \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  -1 \\\\ 1  \\phantom{-}1 \\end{pmatrix}, \\quad D_{A} \\coloneqq \\begin{pmatrix} -3  0 \\\\ 0  -\\frac{1}{2} \\end{pmatrix}, \\quad D_{B} \\coloneqq \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}\n$$\n- Three tasks are specified:\n    1. Verify commutativity of $A$ and $B$, their simultaneous diagonalization by $Q$, and derive the decoupled SDEs for the rotated process $Y_{t} \\coloneqq Q^{\\top} X_{t}$.\n    2. Derive an ODE for the second moment of the scalar components and compute $\\mathbb{E}\\!\\left[|X_{t}|^{2}\\right]$.\n    3. Characterize exponential mean-square stability for a general commuting system and apply it to the given data.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly located within the standard theory of linear stochastic differential equations, Itô calculus, and linear algebra. All concepts are well-established.\n- **Well-Posed:** The problem is unambiguous, and the given information is sufficient to perform all requested tasks. The SDE is a standard linear SDE (geometric Brownian motion in vector form) with a well-defined unique solution. The definitions of all matrices are precise. A check of the matrix $Q$ confirms it is orthogonal:\n$$\nQ Q^{\\top} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  -1 \\\\ 1  1 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ -1  1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 1 \\cdot 1 + (-1) \\cdot (-1)  1 \\cdot 1 + (-1) \\cdot 1 \\\\ 1 \\cdot 1 + 1 \\cdot (-1)  1 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} = I\n$$\n- **Objective:** The language is formal and objective. The tasks are specific mathematical derivations and analyses.\n\nNo flaws are found. The problem is self-contained, consistent, and scientifically sound.\n\n#### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\n#### Task 1: Commutativity, Diagonalization, and Decoupled SDEs\n\nFirst, we verify that matrices $A$ and $B$ commute. Using their definitions:\n$$\nAB = (Q D_{A} Q^{\\top})(Q D_{B} Q^{\\top})\n$$\nSince $Q$ is an orthogonal matrix, $Q^{\\top}Q = I$. Substituting this into the expression gives:\n$$\nAB = Q D_{A} (Q^{\\top}Q) D_{B} Q^{\\top} = Q D_{A} I D_{B} Q^{\\top} = Q D_{A} D_{B} Q^{\\top}\n$$\nSimilarly, for the product $BA$:\n$$\nBA = (Q D_{B} Q^{\\top})(Q D_{A} Q^{\\top}) = Q D_{B} (Q^{\\top}Q) D_{A} Q^{\\top} = Q D_{B} D_{A} Q^{\\top}\n$$\nThe matrices $D_{A}$ and $D_{B}$ are diagonal, and diagonal matrices always commute:\n$$\nD_{A}D_{B} = \\begin{pmatrix} -3  0 \\\\ 0  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} -3  0 \\\\ 0  -1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} -3  0 \\\\ 0  -\\frac{1}{2} \\end{pmatrix} = D_{B}D_{A}\n$$\nTherefore, $AB = Q (D_{A}D_{B}) Q^{\\top} = Q (D_{B}D_{A}) Q^{\\top} = BA$. The matrices $A$ and $B$ commute.\n\nBy their very definition, $A = Q D_{A} Q^{\\top}$ and $B = Q D_{B} Q^{\\top}$, they are simultaneously diagonalized by the orthogonal matrix $Q$. Left-multiplying by $Q^{\\top}$ and right-multiplying by $Q$ yields $Q^{\\top}AQ = D_A$ and $Q^{\\top}BQ = D_B$. The columns of $Q$ are the common eigenvectors of $A$ and $B$.\n\nNow, we introduce the rotated process $Y_{t} \\coloneqq Q^{\\top} X_{t}$. Since $Q^{\\top}$ is a constant matrix, we apply Itô's lemma for a linear transformation, which is simply $dY_t = Q^{\\top} dX_t$:\n$$\ndY_{t} = Q^{\\top}(A X_{t}\\,dt + B X_{t}\\,dW_{t}) = (Q^{\\top} A X_{t})\\,dt + (Q^{\\top} B X_{t})\\,dW_{t}\n$$\nWe substitute $X_{t} = Q Y_{t}$ (since $Q^{-1} = Q^{\\top}$):\n$$\ndY_{t} = (Q^{\\top} A Q Y_{t})\\,dt + (Q^{\\top} B Q Y_{t})\\,dW_{t}\n$$\nUsing the diagonalization property, we have:\n$$\ndY_{t} = D_{A} Y_{t}\\,dt + D_{B} Y_{t}\\,dW_{t}\n$$\nLet $Y_{t} = \\begin{pmatrix} Y_{1,t} \\\\ Y_{2,t} \\end{pmatrix}$. The vector SDE becomes:\n$$\nd\\begin{pmatrix} Y_{1,t} \\\\ Y_{2,t} \\end{pmatrix} = \\begin{pmatrix} -3  0 \\\\ 0  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} Y_{1,t} \\\\ Y_{2,t} \\end{pmatrix} dt + \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} Y_{1,t} \\\\ Y_{2,t} \\end{pmatrix} dW_{t}\n$$\nThis decouples into two scalar SDEs:\n1.  $dY_{1,t} = -3 Y_{1,t}\\,dt + 1 \\cdot Y_{1,t}\\,dW_{t}$\n2.  $dY_{2,t} = -\\frac{1}{2} Y_{2,t}\\,dt + 2 Y_{2,t}\\,dW_{t}$\n\nThe initial condition for $Y_t$ is $Y_{0} = Q^{\\top}X_{0}$:\n$$\nY_{0} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 2-1 \\\\ -2-1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}\n$$\nSo, $Y_{1,0} = \\frac{1}{\\sqrt{2}}$ and $Y_{2,0} = -\\frac{3}{\\sqrt{2}}$.\n\n#### Task 2: Second Moment Calculation\n\nWe derive a general ODE for the second moment of a scalar SDE of the form $dZ_{t} = a Z_{t}\\,dt + b Z_{t}\\,dW_{t}$, where $a, b$ are constants. Let $f(z) = z^{2}$. By Itô's formula:\n$$\ndf(Z_{t}) = f'(Z_{t}) dZ_{t} + \\frac{1}{2} f''(Z_{t}) (dZ_{t})^{2}\n$$\nHere, $f'(z)=2z$ and $f''(z)=2$. The differential is:\n$$\ndZ_{t}^{2} = 2Z_{t} (a Z_{t}\\,dt + b Z_{t}\\,dW_{t}) + \\frac{1}{2}(2) (a Z_{t}\\,dt + b Z_{t}\\,dW_{t})^{2}\n$$\nUsing the Itô multiplication rules ($dt^{2}=0$, $dt\\,dW_{t}=0$, $dW_{t}^{2}=dt$), the quadratic term simplifies to $(dZ_{t})^{2} = (b Z_{t} dW_{t})^{2} = b^{2}Z_{t}^{2}dt$. Substituting this back:\n$$\ndZ_{t}^{2} = 2aZ_{t}^{2}\\,dt + 2bZ_{t}^{2}\\,dW_{t} + b^{2}Z_{t}^{2}\\,dt = (2a+b^{2})Z_{t}^{2}\\,dt + 2bZ_{t}^{2}\\,dW_{t}\n$$\nIntegrating from $0$ to $t$ and taking the expectation, we get:\n$$\n\\mathbb{E}[Z_{t}^{2}] - \\mathbb{E}[Z_{0}^{2}] = \\int_{0}^{t} (2a+b^{2})\\mathbb{E}[Z_{s}^{2}]\\,ds + \\int_{0}^{t} 2b\\mathbb{E}[Z_{s}^{2}\\,dW_{s}]\n$$\nThe expectation of the stochastic integral is zero. Thus, if $Z_0$ is deterministic, $\\mathbb{E}[Z_0^2] = Z_0^2$.\n$$\n\\mathbb{E}[Z_{t}^{2}] = Z_{0}^{2} + \\int_{0}^{t} (2a+b^{2})\\mathbb{E}[Z_{s}^{2}]\\,ds\n$$\nDifferentiating with respect to $t$, we obtain the ODE for the second moment $m(t) \\coloneqq \\mathbb{E}[Z_{t}^{2}]$:\n$$\n\\frac{d m(t)}{dt} = (2a+b^{2})m(t)\n$$\nThe solution is $m(t) = m(0) \\exp((2a+b^{2})t)$.\n\nNow, we apply this to our scalar components $Y_{1,t}$ and $Y_{2,t}$.\nFor $Y_{1,t}$: $a_{1}=-3$, $b_{1}=1$. The exponent is $2a_{1}+b_{1}^{2} = 2(-3) + 1^{2} = -5$. The initial second moment is $\\mathbb{E}[Y_{1,0}^{2}] = Y_{1,0}^{2} = (\\frac{1}{\\sqrt{2}})^{2} = \\frac{1}{2}$.\n$$\n\\mathbb{E}[Y_{1,t}^{2}] = \\frac{1}{2} \\exp(-5t)\n$$\nFor $Y_{2,t}$: $a_{2}=-\\frac{1}{2}$, $b_{2}=2$. The exponent is $2a_{2}+b_{2}^{2} = 2(-\\frac{1}{2}) + 2^{2} = -1+4=3$. The initial second moment is $\\mathbb{E}[Y_{2,0}^{2}] = Y_{2,0}^{2} = (-\\frac{3}{\\sqrt{2}})^{2} = \\frac{9}{2}$.\n$$\n\\mathbb{E}[Y_{2,t}^{2}] = \\frac{9}{2} \\exp(3t)\n$$\nThe squared Euclidean norm is invariant under orthogonal transformations: $|X_{t}|^{2} = X_{t}^{\\top}X_{t} = (QY_{t})^{\\top}(QY_{t}) = Y_{t}^{\\top}Q^{\\top}QY_{t} = Y_{t}^{\\top}IY_{t} = |Y_{t}|^{2} = Y_{1,t}^{2} + Y_{2,t}^{2}$.\nUsing the linearity of expectation:\n$$\n\\mathbb{E}[|X_{t}|^{2}] = \\mathbb{E}[Y_{1,t}^{2} + Y_{2,t}^{2}] = \\mathbb{E}[Y_{1,t}^{2}] + \\mathbb{E}[Y_{2,t}^{2}]\n$$\n$$\n\\mathbb{E}[|X_{t}|^{2}] = \\frac{1}{2} \\exp(-5t) + \\frac{9}{2} \\exp(3t)\n$$\n\n#### Task 3: Mean-Square Stability\n\nFor a general $n$-dimensional system $dX_{t} = A X_{t}\\,dt + B X_{t}\\,dW_{t}$ where $A$ and $B$ commute and are simultaneously diagonalizable by an orthogonal matrix, with real eigenvalues $\\{a_{i}\\}_{i=1}^{n}$ and $\\{b_{i}\\}_{i=1}^{n}$ respectively, the system decouples into $n$ scalar SDEs after transformation: $dY_{i,t} = a_{i}Y_{i,t}\\,dt + b_{i}Y_{i,t}\\,dW_{t}$.\n\nThe second moment of the solution norm is $\\mathbb{E}[|X_{t}|^{2}] = \\mathbb{E}[|Y_{t}|^{2}] = \\sum_{i=1}^{n} \\mathbb{E}[Y_{i,t}^{2}]$.\nFrom the previous derivation, $\\mathbb{E}[Y_{i,t}^{2}] = Y_{i,0}^{2}\\exp((2a_{i}+b_{i}^{2})t)$.\nThe system is exponentially mean-square stable if $\\lim_{t \\to \\infty} \\mathbb{E}[|X_{t}|^{2}] = 0$ for any initial condition $X_0 \\neq 0$ (which implies $Y_0 \\neq 0$). This requires that $\\mathbb{E}[Y_{i,t}^2] \\to 0$ for all $i$ such that $Y_{i,0} \\neq 0$.\nThe limit $\\exp((2a_{i}+b_{i}^{2})t) \\to 0$ as $t \\to \\infty$ if and only if the exponent is negative.\nThus, the condition for exponential mean-square stability is that the spectral condition $2a_{i} + b_{i}^{2}  0$ holds for all $i=1, \\dots, n$.\n\nApplying this condition to the given numerical data:\n- For the first eigenvalue pair ($i=1$): $a_{1}=-3$, $b_{1}=1$.\n  $2a_{1} + b_{1}^{2} = 2(-3) + 1^{2} = -6 + 1 = -5$. Since $-5  0$, this mode is stable.\n- For the second eigenvalue pair ($i=2$): $a_{2}=-\\frac{1}{2}$, $b_{2}=2$.\n  $2a_{2} + b_{2}^{2} = 2(-\\frac{1}{2}) + 2^{2} = -1 + 4 = 3$. Since $3  0$, this mode is unstable.\n\nSince the condition $2a_i + b_i^2  0$ is not satisfied for all eigenvalues, the system is **not** exponentially mean-square stable. This is consistent with the derived expression $\\mathbb{E}[|X_{t}|^2] = \\frac{1}{2}\\exp(-5t) + \\frac{9}{2}\\exp(3t)$, which grows exponentially as $t \\to \\infty$ due to the $\\exp(3t)$ term.", "answer": "$$\\boxed{\\frac{1}{2}\\exp(-5t) + \\frac{9}{2}\\exp(3t)}$$", "id": "2988062"}]}