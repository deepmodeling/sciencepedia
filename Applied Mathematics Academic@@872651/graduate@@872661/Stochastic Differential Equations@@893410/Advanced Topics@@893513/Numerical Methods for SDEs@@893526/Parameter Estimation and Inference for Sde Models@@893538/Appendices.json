{"hands_on_practices": [{"introduction": "Understanding the theoretical limits of parameter estimation is the cornerstone of statistical inference. This first exercise provides a foundational workout by asking you to derive the Fisher information matrix for the parameters $(\\kappa, \\mu, \\sigma^2)$ of an Ornstein-Uhlenbeck process from its exact discrete-time likelihood. By calculating this matrix [@problem_id:2989872], you will not only practice essential analytical skills but also gain deep insight into how information accumulates under various sampling regimes, setting the stage for understanding estimator efficiency.", "problem": "Consider the Ornstein–Uhlenbeck stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = \\kappa \\left( \\mu - X_t \\right) \\mathrm{d}t + \\sigma \\,\\mathrm{d}W_t,\n$$\nwith parameters $\\kappa \\gt 0$, $\\mu \\in \\mathbb{R}$, and $\\sigma \\gt 0$, where $W_t$ is a standard Wiener process. Suppose that the process is observed at $n$ discrete times with equal spacing $\\Delta \\gt 0$, yielding data $\\{X_0, X_{\\Delta}, X_{2\\Delta}, \\dots, X_{(n-1)\\Delta}\\}$. Assume the process is stationary and that the likelihood is computed conditionally on $X_0$ so that the log-likelihood is the sum of $(n-1)$ independent transition contributions. It is a well-known fact that the exact Markov transition distribution over time step $\\Delta$ is Gaussian with mean and variance given by\n$$\nm(x;\\kappa,\\mu,\\Delta) = \\mu + \\left( x - \\mu \\right) \\exp\\!\\left( -\\kappa \\Delta \\right),\n\\qquad\nv(\\kappa,\\sigma^2,\\Delta) = \\frac{\\sigma^2}{2\\kappa}\\left( 1 - \\exp\\!\\left( -2\\kappa \\Delta \\right) \\right).\n$$\nStarting from the definition of Fisher information as the expectation (under the true model) of the negative Hessian of the log-likelihood, derive the closed-form Fisher information matrix for the parameter vector $\\theta = (\\kappa, \\mu, \\sigma^2)$ based on the exact discrete-time likelihood from these $(n-1)$ independent transitions. Express your final result as an explicit $3 \\times 3$ matrix in terms of $(n,\\Delta,\\kappa,\\mu,\\sigma^2)$, with no unspecified constants.\n\nThen analyze the leading-order behavior of each entry of the per-transition Fisher information matrix as $\\Delta \\to 0$ with $n$ fixed, and discuss how the total Fisher information scales as $n \\to \\infty$ with $\\Delta$ fixed. Your reasoning must begin from the definition of Fisher information and fundamental properties of the Gaussian distribution. Do not assume any asymptotic results a priori; derive the small-$\\Delta$ expansions directly from series expansions of $\\exp(-\\kappa \\Delta)$.\n\nProvide your final answer as the exact Fisher information matrix for the $n$ observations in a single closed-form analytic expression. No numerical evaluation or rounding is required, and no units are involved.", "solution": "The problem asks for the Fisher information matrix (FIM) for the parameters $\\theta = (\\kappa, \\mu, \\sigma^2)$ of an Ornstein-Uhlenbeck process, observed at $n$ discrete time points with spacing $\\Delta$. The derivation is based on the exact conditional likelihood of $(n-1)$ transitions.\n\nThe Ornstein-Uhlenbeck SDE is given by:\n$$\n\\mathrm{d}X_t = \\kappa (\\mu - X_t) \\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\nThe process is observed at times $t_i = i\\Delta$ for $i=0, 1, \\dots, n-1$. The transition distribution from $X_i = X_{i\\Delta}$ to $X_{i+1} = X_{(i+1)\\Delta}$ is Gaussian, $X_{i+1} | X_i \\sim \\mathcal{N}(m_i, v)$, with mean and variance:\n$$\nm_i = \\mu + (X_i - \\mu) \\exp(-\\kappa \\Delta)\n$$\n$$\nv = \\frac{\\sigma^2}{2\\kappa} (1 - \\exp(-2\\kappa \\Delta))\n$$\nThe log-likelihood for a single transition, conditional on $X_i$, is:\n$$\n\\ell_i(\\theta) = -\\frac{1}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(v) - \\frac{(X_{i+1} - m_i)^2}{2v}\n$$\nThe total log-likelihood for the $(n-1)$ transitions, conditional on $X_0$, is $\\ell(\\theta) = \\sum_{i=0}^{n-2} \\ell_i(\\theta)$.\n\nFor a single transition, the FIM element for parameters $\\theta_j$ and $\\theta_k$ under a Gaussian likelihood is given by:\n$$\n\\mathcal{I}_{i,jk} = \\mathbb{E}\\left[ \\frac{\\partial \\ell_i}{\\partial \\theta_j} \\frac{\\partial \\ell_i}{\\partial \\theta_k} \\right] = \\frac{1}{v} \\mathbb{E}\\left[\\frac{\\partial m_i}{\\partial \\theta_j} \\frac{\\partial m_i}{\\partial \\theta_k}\\right] + \\frac{1}{2v^2} \\frac{\\partial v}{\\partial \\theta_j} \\frac{\\partial v}{\\partial \\theta_k}\n$$\nThe expectation is taken over the stationary distribution of the process. Due to stationarity, $\\mathcal{I}_i$ is the same for all $i$. Let's denote this per-transition FIM as $\\mathcal{I}_{\\text{single}}$. The total FIM for $(n-1)$ transitions is $\\mathcal{I}^{(n)} = (n-1)\\mathcal{I}_{\\text{single}}$.\n\nWe need the partial derivatives of $m_i$ and $v$ with respect to $\\theta = (\\kappa, \\mu, \\sigma^2)$. The process is assumed to be stationary, so $X_i$ is drawn from the stationary distribution $\\mathcal{N}(\\mu, \\frac{\\sigma^2}{2\\kappa})$. This implies $\\mathbb{E}[X_i - \\mu] = 0$ and $\\mathbb{E}[(X_i - \\mu)^2] = \\frac{\\sigma^2}{2\\kappa}$.\n\nThe required derivatives are:\n$$\n\\frac{\\partial m_i}{\\partial \\kappa} = -(X_i - \\mu) \\Delta \\exp(-\\kappa \\Delta) \\qquad \\frac{\\partial m_i}{\\partial \\mu} = 1 - \\exp(-\\kappa \\Delta) \\qquad \\frac{\\partial m_i}{\\partial \\sigma^2} = 0\n$$\n$$\n\\frac{\\partial v}{\\partial \\kappa} = \\frac{\\sigma^2}{2\\kappa^2} \\left( 2\\kappa\\Delta \\exp(-2\\kappa \\Delta) - (1 - \\exp(-2\\kappa \\Delta)) \\right) \\qquad \\frac{\\partial v}{\\partial \\mu} = 0 \\qquad \\frac{\\partial v}{\\partial \\sigma^2} = \\frac{1}{2\\kappa} (1 - \\exp(-2\\kappa \\Delta)) = \\frac{v}{\\sigma^2}\n$$\nNow we compute the elements of the per-transition FIM, $\\mathcal{I}_{\\text{single}}$.\n\n**Off-diagonal elements involving $\\mu$:** Since $\\partial v / \\partial \\mu = 0$ and $\\mathbb{E}[X_i - \\mu]=0$, any term involving $\\mathbb{E}[\\partial m_i/\\partial\\mu \\cdot \\partial m_i/\\partial\\kappa]$ will be zero. Also $\\partial m_i / \\partial \\sigma^2=0$.\nThis means $\\mathcal{I}_{\\kappa\\mu}=0$ and $\\mathcal{I}_{\\mu\\sigma^2}=0$, making the FIM block-diagonal.\n\n**Diagonal element for $\\mu$:**\n$$\n\\mathcal{I}_{\\mu\\mu} = \\frac{1}{v} \\mathbb{E}\\left[\\left(\\frac{\\partial m_i}{\\partial \\mu}\\right)^2\\right] = \\frac{(1 - \\exp(-\\kappa \\Delta))^2}{v} = \\frac{(1 - \\exp(-\\kappa \\Delta))^2}{\\frac{\\sigma^2}{2\\kappa}(1 - \\exp(-2\\kappa \\Delta))} = \\frac{2\\kappa(1 - \\exp(-\\kappa \\Delta))}{\\sigma^2(1 + \\exp(-\\kappa \\Delta))}\n$$\n**Diagonal element for $\\sigma^2$:**\n$$\n\\mathcal{I}_{\\sigma^2\\sigma^2} = \\frac{1}{2v^2}\\left(\\frac{\\partial v}{\\partial \\sigma^2}\\right)^2 = \\frac{1}{2v^2} \\left(\\frac{v}{\\sigma^2}\\right)^2 = \\frac{1}{2\\sigma^4}\n$$\n**Off-diagonal element for $(\\kappa, \\sigma^2)$:**\n$$\n\\mathcal{I}_{\\kappa\\sigma^2} = \\frac{1}{2v^2}\\frac{\\partial v}{\\partial \\kappa}\\frac{\\partial v}{\\partial \\sigma^2} = \\frac{1}{2v\\sigma^2} \\frac{\\partial v}{\\partial \\kappa} = \\frac{1}{2\\kappa\\sigma^2} \\left( \\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1} - 1 \\right)\n$$\n**Diagonal element for $\\kappa$:**\n$$\n\\mathcal{I}_{\\kappa\\kappa} = \\frac{1}{v} \\mathbb{E}\\left[\\left(\\frac{\\partial m_i}{\\partial \\kappa}\\right)^2\\right] + \\frac{1}{2v^2}\\left(\\frac{\\partial v}{\\partial \\kappa}\\right)^2\n$$\nThe first term is $\\frac{(\\Delta \\exp(-\\kappa\\Delta))^2}{v} \\mathbb{E}[(X_i-\\mu)^2] = \\frac{\\Delta^2 \\exp(-2\\kappa\\Delta)}{v} \\frac{\\sigma^2}{2\\kappa} = \\frac{\\Delta^2}{\\exp(2\\kappa\\Delta)-1}$.\nThe second term is $2\\sigma^4 (\\mathcal{I}_{\\kappa\\sigma^2})^2$.\nSo, $\\mathcal{I}_{\\kappa\\kappa} = \\frac{1}{2\\kappa^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right)^2 + \\frac{\\Delta^2}{\\exp(2\\kappa\\Delta)-1}$.\n\nThe total FIM for $(n-1)$ transitions is $\\mathcal{I}^{(n)} = (n-1)\\mathcal{I}_{\\text{single}}$.\n\n**Asymptotic Analysis:**\n\n1.  **Small $\\Delta$ behavior ($n$ fixed):** As $\\Delta \\to 0$, using the expansion $\\frac{x}{e^x-1} \\approx 1 - x/2$, we find $\\mathcal{I}_{\\mu\\mu} \\approx \\frac{\\kappa^2\\Delta}{\\sigma^2} = O(\\Delta)$, $\\mathcal{I}_{\\kappa\\kappa} \\approx \\frac{\\Delta}{2\\kappa} = O(\\Delta)$, and $\\mathcal{I}_{\\kappa\\sigma^2} \\approx -\\frac{\\Delta}{2\\sigma^2} = O(\\Delta)$. In contrast, $\\mathcal{I}_{\\sigma^2\\sigma^2} = \\frac{1}{2\\sigma^4} = O(1)$. This shows that for very high frequency sampling over a fixed interval, most information is about the diffusion parameter $\\sigma^2$, while information about drift parameters $\\kappa$ and $\\mu$ vanishes.\n\n2.  **Large $n$ behavior ($\\Delta$ fixed):** The total Fisher information is $\\mathcal{I}^{(n)} = (n-1)\\mathcal{I}_{\\text{single}}$. Since all components of $\\mathcal{I}_{\\text{single}}$ are positive constants for a fixed $\\Delta$, every entry of the total Fisher information matrix $\\mathcal{I}^{(n)}$ scales linearly with $(n-1)$. This implies that the variance of the maximum likelihood estimators for all three parameters will decrease proportionally to $1/(n-1)$, and thus all parameters are consistently estimable in the long-span regime.\n\nThe final explicit Fisher information matrix for the $(n-1)$ transitions is:\n$$\n\\mathcal{I}^{(n)} = (n-1) \\begin{pmatrix}\n\\frac{1}{2\\kappa^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right)^2 + \\frac{\\Delta^2}{\\exp(2\\kappa\\Delta)-1} & 0 & \\frac{1}{2\\kappa\\sigma^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right) \\\\\n0 & \\frac{2\\kappa(1 - \\exp(-\\kappa \\Delta))}{\\sigma^2(1 + \\exp(-\\kappa \\Delta))} & 0 \\\\\n\\frac{1}{2\\kappa\\sigma^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right) & 0 & \\frac{1}{2\\sigma^4}\n\\end{pmatrix}\n$$\nThis is the required closed-form matrix.", "answer": "$$\n\\boxed{\n(n-1)\n\\begin{pmatrix}\n\\frac{1}{2\\kappa^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right)^2 + \\frac{\\Delta^2}{\\exp(2\\kappa\\Delta)-1} & 0 & \\frac{1}{2\\kappa\\sigma^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right) \\\\\n0 & \\frac{2\\kappa(1 - \\exp(-\\kappa \\Delta))}{\\sigma^2(1 + \\exp(-\\kappa \\Delta))} & 0 \\\\\n\\frac{1}{2\\kappa\\sigma^2}\\left(\\frac{2\\kappa\\Delta}{\\exp(2\\kappa\\Delta)-1}-1\\right) & 0 & \\frac{1}{2\\sigma^4}\n\\end{pmatrix}\n}\n$$", "id": "2989872"}, {"introduction": "While exact likelihoods are powerful, they are often intractable for nonlinear SDEs, forcing practitioners to rely on approximations like the Euler-Maruyama scheme. This practice problem explores the consequences of such approximations by quantifying the systematic bias, which is a function of the time step $\\Delta$, that they introduce into parameter estimates for the Cox-Ingersoll-Ross model. By deriving the leading-order bias term [@problem_id:2989856], you will learn a crucial diagnostic and corrective technique based on the infinitesimal generator, bridging the gap between continuous-time theory and discrete-time data.", "problem": "Consider the scalar Cox–Ingersoll–Ross (CIR) diffusion, a nonlinear scalar diffusion in which the diffusion coefficient depends on the square root of the state, defined by the stochastic differential equation (SDE) for the process $X_{t}$,\n$$\n\\mathrm{d}X_{t} = \\kappa \\left( \\mu - X_{t} \\right) \\mathrm{d}t + \\sigma \\sqrt{X_{t}} \\,\\mathrm{d}W_{t},\n$$\nwhere $\\kappa>0$, $\\mu>0$, and $\\sigma>0$ are constants, and $W_{t}$ is a standard one-dimensional Wiener process (Brownian motion). Suppose $X_{t}$ is strictly stationary under parameter values satisfying the Feller condition and is observed at discrete times $t_{i} = i \\Delta$ for $i=0,1,\\dots,n$, with fixed sampling interval $\\Delta>0$.\n\nA common Euler discretization-based least-squares estimator for the mean-reversion parameter $\\kappa$ treats the increment $\\left( X_{t_{i+1}} - X_{t_{i}} \\right)/\\Delta$ as a noisy observation of the drift $\\kappa \\left( \\mu - X_{t_{i}} \\right)$ and is defined by\n$$\n\\hat{\\kappa}_{E} \\equiv \\frac{\\sum_{i=0}^{n-1} \\left( \\mu - X_{t_{i}} \\right) \\left( X_{t_{i+1}} - X_{t_{i}} \\right)}{\\Delta \\sum_{i=0}^{n-1} \\left( \\mu - X_{t_{i}} \\right)^{2}}.\n$$\nAssume the sample size $n$ is large and the process is ergodic so that sample averages may be replaced by expectations under the stationary distribution of $X_{t}$. Starting from the definition of the infinitesimal generator of the diffusion and fundamental semigroup expansions for conditional expectations over short time increments, quantify the leading-order bias introduced by the Euler discretization in $\\hat{\\kappa}_{E}$, and derive a first-order correction term in $\\Delta$ that, when added to $\\hat{\\kappa}_{E}$, yields an estimator with bias of order $o(\\Delta)$.\n\nYour final answer must be the closed-form analytic expression for this first-order correction term $C\\!\\left( \\kappa, \\Delta \\right)$ to be added to $\\hat{\\kappa}_{E}$ so that $\\mathbb{E}\\!\\left[ \\hat{\\kappa}_{E} + C\\!\\left( \\kappa, \\Delta \\right) \\right] = \\kappa + o(\\Delta)$. No rounding is required.", "solution": "The goal is to find the first-order correction term $C(\\kappa, \\Delta)$ for the estimator $\\hat{\\kappa}_E$. Under the assumption of large $n$ and ergodicity, we can replace sample averages with expectations under the stationary distribution $\\pi$:\n$$\n\\mathbb{E}[\\hat{\\kappa}_E] \\approx \\frac{\\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right) \\left( X_{t+\\Delta} - X_t \\right) \\right]}{\\Delta \\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right)^2 \\right]}\n$$\nThe expectation in the numerator can be evaluated using the law of total expectation:\n$$\n\\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right) \\mathbb{E}\\left[ X_{t+\\Delta} - X_t | X_t \\right] \\right]\n$$\nThe infinitesimal generator $\\mathcal{L}$ of the CIR process acts on a function $f(x)$ as:\n$$\n\\mathcal{L}f(x) = \\kappa(\\mu-x)f'(x) + \\frac{1}{2}\\sigma^2 x f''(x)\n$$\nUsing the semigroup expansion, the conditional expectation of an increment can be expressed in terms of the generator:\n$$\n\\mathbb{E}[X_{t+\\Delta} - X_t | X_t=x] = \\Delta\\mathcal{L}x + \\frac{\\Delta^2}{2}\\mathcal{L}^2x + O(\\Delta^3)\n$$\nWe need to compute $\\mathcal{L}x$ and $\\mathcal{L}^2x$.\n1.  For $f(x)=x$, we have $f'(x)=1$ and $f''(x)=0$. Applying the generator:\n    $$\n    \\mathcal{L}x = \\kappa(\\mu-x)(1) + \\frac{1}{2}\\sigma^2 x (0) = \\kappa(\\mu-x)\n    $$\n2.  To find $\\mathcal{L}^2x$, we apply the generator to the function $g(x) = \\mathcal{L}x = \\kappa(\\mu-x)$. The derivatives are $g'(x)=-\\kappa$ and $g''(x)=0$.\n    $$\n    \\mathcal{L}^2x = \\mathcal{L}g(x) = \\kappa(\\mu-x)g'(x) + \\frac{1}{2}\\sigma^2 x g''(x) = \\kappa(\\mu-x)(-\\kappa) + \\frac{1}{2}\\sigma^2 x (0) = -\\kappa^2(\\mu-x)\n    $$\nSubstituting these into the expansion for the conditional increment:\n$$\n\\mathbb{E}[X_{t+\\Delta} - X_t | X_t] = \\Delta\\kappa(\\mu-X_t) - \\frac{\\Delta^2}{2}\\kappa^2(\\mu-X_t) + O(\\Delta^3)\n$$\nNow, we substitute this back into the expression for the numerator's expectation:\n$$\n\\mathbb{E}_{\\pi}\\left[ (\\mu-X_t) \\left( \\Delta\\kappa(\\mu-X_t) - \\frac{\\Delta^2}{2}\\kappa^2(\\mu-X_t) + O(\\Delta^3) \\right) \\right]\n$$\n$$\n= \\Delta\\kappa\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right] - \\frac{\\Delta^2}{2}\\kappa^2\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right] + O(\\Delta^3)\n$$\nFinally, we assemble the full expression for $\\mathbb{E}[\\hat{\\kappa}_E]$:\n$$\n\\mathbb{E}[\\hat{\\kappa}_E] \\approx \\frac{\\Delta\\kappa\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right] - \\frac{\\Delta^2}{2}\\kappa^2\\mathbb{E}_{\\pi}\\left[(\\mu-X_t)^2\\right] + O(\\Delta^3)}{\\Delta \\mathbb{E}_{\\pi}\\left[ \\left( \\mu - X_t \\right)^2 \\right]}\n$$\nThe term $\\mathbb{E}_{\\pi}[(\\mu-X_t)^2]$ cancels out:\n$$\n\\mathbb{E}[\\hat{\\kappa}_E] \\approx \\frac{\\Delta\\kappa - \\frac{\\Delta^2}{2}\\kappa^2}{\\Delta} + O(\\Delta^2) = \\kappa - \\frac{\\Delta}{2}\\kappa^2 + O(\\Delta^2)\n$$\nThe leading-order bias of the estimator is the difference between its expectation and the true value $\\kappa$:\n$$\n\\text{Bias} = \\mathbb{E}[\\hat{\\kappa}_E] - \\kappa \\approx -\\frac{\\Delta}{2}\\kappa^2\n$$\nThe correction term $C(\\kappa, \\Delta)$ must cancel this leading-order bias:\n$$\nC(\\kappa, \\Delta) = -(\\text{Bias}) = \\frac{\\Delta}{2}\\kappa^2\n$$\nAdding this term results in a corrected estimator whose expectation is $\\kappa + O(\\Delta^2)$, which is $\\kappa + o(\\Delta)$ as required.", "answer": "$$\\boxed{\\frac{\\Delta \\kappa^2}{2}}$$", "id": "2989856"}, {"introduction": "Many real-world systems exhibit sudden, discontinuous movements that cannot be captured by a simple Brownian motion, necessitating the use of jump-diffusion models. This exercise tackles the challenge of estimating the drift parameter $b$ in the presence of such jumps, demonstrating how naive methods can lead to severely biased results. You will develop a robust, jump-filtered estimator and analyze its consistency [@problem_id:2989868], providing a practical introduction to the powerful thresholding techniques used in modern high-frequency statistics.", "problem": "Consider the discretely observed jump-diffusion Itô stochastic differential equation (SDE)\n$$\ndX_{t} = b\\,dt + \\sigma\\,dW_{t} + dJ_{t}, \\qquad t \\ge 0,\n$$\nwhere $b \\in \\mathbb{R}$ and $\\sigma > 0$ are unknown constants, $\\{W_{t}\\}_{t \\ge 0}$ is a standard Brownian motion, and $\\{J_{t}\\}_{t \\ge 0}$ is an independent compound Poisson process defined by\n$$\nJ_{t} = \\sum_{k=1}^{N_{t}} Y_{k}.\n$$\nHere $\\{N_{t}\\}_{t \\ge 0}$ is a Poisson process with intensity $\\lambda > 0$, and $\\{Y_{k}\\}_{k \\ge 1}$ are independent and identically distributed jump sizes with $\\mathbb{E}[\\,|Y_{1}|\\,] < \\infty$, $\\mathbb{E}[\\,Y_{1}\\,] = \\mu_{J}$, and $\\mathbb{P}(|Y_{1}| > 0) = 1$. All sources of randomness are mutually independent.\n\nYou observe $\\{X_{t_{i}}\\}_{i=0}^{n}$ at equidistant times $t_{i} = i \\Delta$ with mesh $\\Delta > 0$, total span $T = n\\Delta$, and the regime $n \\to \\infty$, $\\Delta \\to 0$, with $T = n\\Delta \\to \\infty$ and $n$ growing at most polynomially in $1/\\Delta$.\n\nDefine the naive drift estimator that ignores jumps by\n$$\n\\widehat{b}^{\\,N} = \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i}, \n\\qquad \\Delta X_{i} = X_{t_{i}} - X_{t_{i-1}}.\n$$\nStarting only from the defining properties of the Brownian motion, compound Poisson process, and their independent increments, determine the asymptotic bias of $\\widehat{b}^{\\,N}$ for estimating $b$ under the above regime.\n\nNext, define a jump-filtered estimator using a vanishing threshold \n$$\nu_{\\Delta} = c\\,\\Delta^{\\alpha}, \\qquad c > 0,\\;\\; \\alpha \\in (0, \\tfrac{1}{2}),\n$$\nby\n$$\n\\widehat{b}^{\\,JF} = \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i}\\,\\mathbf{1}\\!\\left\\{\\,|\\Delta X_{i}| \\le u_{\\Delta}\\,\\right\\}.\n$$\nUsing only the fundamental distributional properties of the increments, the scaling of Brownian increments, and the law of large numbers for arrays of independent (or conditionally independent) variables, show that $\\widehat{b}^{\\,JF}$ converges in probability to a deterministic limit as $n \\to \\infty$, $\\Delta \\to 0$, $T \\to \\infty$ under the stated growth regime, and identify that limit.\n\nReport your final result as a single row vector containing, in order, the asymptotic bias of $\\widehat{b}^{\\,N}$ and the probability limit of $\\widehat{b}^{\\,JF}$. No rounding is required and no units are needed.", "solution": "The problem asks for the asymptotic bias of a naive drift estimator and the probability limit of a jump-filtered estimator for a jump-diffusion process.\n\nThe SDE increment over an interval of length $\\Delta$ is:\n$$\n\\Delta X_i = X_{t_i} - X_{t_{i-1}} = b\\Delta + \\sigma(W_{t_i} - W_{t_{i-1}}) + (J_{t_i} - J_{t_{i-1}}) = b\\Delta + \\sigma\\Delta W_i + \\Delta J_i.\n$$\nThe increments $\\Delta W_i$ and $\\Delta J_i$ are independent. We have $\\mathbb{E}[\\Delta W_i] = 0$. By Wald's identity, the expectation of the compound Poisson increment is $\\mathbb{E}[\\Delta J_i] = \\mathbb{E}[\\text{number of jumps in } \\Delta] \\times \\mathbb{E}[\\text{jump size}] = (\\lambda\\Delta)\\mu_J$.\n\n**Part 1: Asymptotic bias of the naive estimator $\\widehat{b}^{\\,N}$**\n\nThe naive estimator is $\\widehat{b}^{\\,N} = \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i}$. By the law of large numbers, as $T \\to \\infty$, $\\widehat{b}^{\\,N} = (X_T - X_0)/T$ converges to the expected rate of change. We can find this by computing the expectation of the estimator:\n$$\n\\mathbb{E}[\\widehat{b}^{\\,N}] = \\frac{1}{n\\Delta} \\sum_{i=1}^{n} \\mathbb{E}[\\Delta X_{i}].\n$$\nThe increments are stationary, so $\\mathbb{E}[\\Delta X_i]$ is constant:\n$$\n\\mathbb{E}[\\Delta X_{i}] = \\mathbb{E}[b\\Delta + \\sigma\\Delta W_i + \\Delta J_i] = b\\Delta + \\sigma\\mathbb{E}[\\Delta W_i] + \\mathbb{E}[\\Delta J_i] = b\\Delta + 0 + \\lambda\\mu_J\\Delta.\n$$\nTherefore, the expectation of the estimator is:\n$$\n\\mathbb{E}[\\widehat{b}^{\\,N}] = \\frac{1}{n\\Delta} \\sum_{i=1}^{n} (b+\\lambda\\mu_J)\\Delta = b + \\lambda\\mu_J.\n$$\nThe bias is $\\mathbb{E}[\\widehat{b}^{\\,N}] - b = \\lambda\\mu_J$. Since the estimator converges in probability to $b + \\lambda\\mu_J$, this represents a non-vanishing asymptotic bias.\n\n**Part 2: Probability limit of the jump-filtered estimator $\\widehat{b}^{\\,JF}$**\n\nThe jump-filtered estimator is $\\widehat{b}^{\\,JF} = \\frac{1}{T}\\sum_{i=1}^{n} \\Delta X_{i}\\,\\mathbf{1}\\{|\\Delta X_{i}| \\le u_{\\Delta}\\}$ with threshold $u_{\\Delta} = c\\Delta^{\\alpha}$ for $\\alpha \\in (0, 1/2)$. This choice of $\\alpha$ ensures that the threshold separates purely diffusive increments from those containing jumps, as $\\Delta \\to 0$:\n-   A diffusive increment is of order $O_p(\\sqrt{\\Delta})$.\n-   A jump increment is of order $O_p(1)$.\n-   The threshold $u_\\Delta$ is of order $O(\\Delta^\\alpha)$, which is between $\\sqrt{\\Delta}$ and $1$.\n\nThus, for small $\\Delta$, the indicator function $\\mathbf{1}\\{|\\Delta X_{i}| \\le u_{\\Delta}\\}$ effectively selects only the increments without jumps.\nBy the law of large numbers for triangular arrays, $\\widehat{b}^{\\,JF}$ converges in probability to the limit of $\\frac{1}{\\Delta}\\mathbb{E}[\\Delta X_1 \\mathbf{1}\\{|\\Delta X_1|\\le u_\\Delta\\}]$.\nLet's analyze this expectation by conditioning on whether a jump occurs in the interval:\n$$\n\\mathbb{E}[\\Delta X_1 \\mathbf{1}_{\\{|\\Delta X_1|\\le u_\\Delta\\}}] = \\mathbb{E}[\\Delta X_1 \\mathbf{1}_{\\{|\\Delta X_1|\\le u_\\Delta\\}} | \\text{no jump}] P(\\text{no jump}) + \\mathbb{E}[\\Delta X_1 \\mathbf{1}_{\\{|\\Delta X_1|\\le u_\\Delta\\}} | \\text{jump}] P(\\text{jump}).\n$$\nThe probability of no jump is $P(\\Delta N_1=0) = e^{-\\lambda\\Delta} = 1 - \\lambda\\Delta + O(\\Delta^2)$.\nThe probability of at least one jump is $P(\\Delta N_1 \\ge 1) = 1 - e^{-\\lambda\\Delta} = \\lambda\\Delta + O(\\Delta^2)$.\n\n-   **Contribution from no-jump intervals:**\n    Given no jump, $\\Delta X_1 = b\\Delta + \\sigma\\Delta W_1$. Since $|\\Delta X_1| = O_p(\\sqrt{\\Delta})$, the condition $|\\Delta X_1| \\le u_\\Delta$ holds with probability approaching 1. The expectation is $\\mathbb{E}[(b\\Delta + \\sigma\\Delta W_1)\\mathbf{1}_{\\{|b\\Delta + \\sigma\\Delta W_1|\\le u_\\Delta\\}}]$. The indicator has a negligible effect on the expectation for small $\\Delta$, so this is approximately $\\mathbb{E}[b\\Delta + \\sigma\\Delta W_1] = b\\Delta$. The total contribution is $(b\\Delta + o(\\Delta)) \\times (1 - \\lambda\\Delta + \\dots) = b\\Delta + O(\\Delta^2)$.\n\n-   **Contribution from jump intervals:**\n    Given a jump, $|\\Delta X_1|$ is of order $O_p(1)$, so the condition $|\\Delta X_1| \\le u_\\Delta$ is violated with probability approaching 1. The term $\\mathbf{1}_{\\{|\\Delta X_1|\\le u_\\Delta\\}}$ is almost always zero. Therefore, the conditional expectation $\\mathbb{E}[\\Delta X_1 \\mathbf{1}_{\\{|\\Delta X_1|\\le u_\\Delta\\}} | \\text{jump}]$ approaches zero. A more detailed analysis shows this term is $o(\\Delta)$. The total contribution from jump intervals is $(\\lambda\\Delta + O(\\Delta^2)) \\times o(\\Delta) = o(\\Delta^2)$.\n\nCombining both contributions, we get:\n$$\n\\mathbb{E}[\\Delta X_1 \\mathbf{1}\\{|\\Delta X_1|\\le u_\\Delta\\}] = b\\Delta + O(\\Delta^2) + o(\\Delta^2) = b\\Delta + O(\\Delta^2).\n$$\nThe probability limit of the estimator is then:\n$$\n\\text{plim}\\, \\widehat{b}^{\\,JF} = \\lim_{\\Delta \\to 0} \\frac{\\mathbb{E}[\\Delta X_1 \\mathbf{1}\\{|\\Delta X_1|\\le u_\\Delta\\}]}{\\Delta} = \\lim_{\\Delta \\to 0} \\frac{b\\Delta + O(\\Delta^2)}{\\Delta} = b.\n$$\nThe jump-filtered estimator is consistent for the drift parameter $b$.\n\nThe final result is the row vector containing the asymptotic bias of the naive estimator and the probability limit of the filtered estimator.\n1.  Asymptotic bias of $\\widehat{b}^{\\,N}$: $\\lambda \\mu_J$.\n2.  Probability limit of $\\widehat{b}^{\\,JF}$: $b$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\lambda \\mu_{J} & b\n\\end{pmatrix}\n}\n$$", "id": "2989868"}]}