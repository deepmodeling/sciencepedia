{"hands_on_practices": [{"introduction": "Before embarking on any large-scale Monte Carlo simulation, a crucial first step is to estimate the required computational effort. This practice establishes the fundamental relationship between the desired statistical accuracy of an estimator and the number of sample paths needed. By working through the derivation [@problem_id:2988319], you will see how the inherent variability of the quantity being estimated, measured by its variance $V$, and the target root-mean-square error $\\varepsilon$ dictate the necessary sample size $N$. This exercise provides the foundational formula for planning Monte Carlo studies and understanding the trade-off between computational cost and precision, assuming for the moment that we can sample from the true distribution without any discretization error.", "problem": "Consider a one-dimensional Itô stochastic differential equation (SDE) given by $dX_{t}=\\mu(X_{t},t)\\,dt+\\sigma(X_{t},t)\\,dW_{t}$ with deterministic initial condition $X_{0}=x_{0}$, where $\\mu$ and $\\sigma$ satisfy standard conditions ensuring existence and uniqueness of strong solutions and finite second moments of functionals of $X_{t}$. Let $T0$ be a fixed terminal time and let $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ be a measurable function such that $\\mathbb{E}\\big[|\\varphi(X_{T})|^{2}\\big]\\infty$. The computational task is to approximate the expectation $\\mathbb{E}[\\varphi(X_{T})]$ by Monte Carlo sampling.\n\nAssume an idealized setting in which the sampling of $X_{T}$ is exact (i.e., neglect any discretization bias), and suppose one can draw $N$ independent samples $Y_{1},\\dots,Y_{N}$ with $Y_{i}=\\varphi(X_{T}^{(i)})$, where $X_{T}^{(i)}$ are independent copies of $X_{T}$. Let $V=\\mathrm{Var}(Y_{1})$ be known and finite, with $V0$. For the Monte Carlo estimator $\\widehat{m}_{N}=\\frac{1}{N}\\sum_{i=1}^{N}Y_{i}$, impose a target root-mean-square error (RMSE) threshold $\\varepsilon0$.\n\nStarting from the definitions of variance, independence, and mean-squared error, derive the minimal integer sample size $N$ required to ensure that the RMSE of $\\widehat{m}_{N}$ does not exceed $\\varepsilon$, under the assumption of zero bias. Then, quantify the sensitivity of the required sample size to variance misestimation by deriving the first-order sensitivity of the continuous relaxation $N(V)$ with respect to $V$ and the corresponding relative sensitivity at $V$.\n\nYour final answer must be a single closed-form analytic expression or a single row matrix containing your expressions. No numerical approximation is required.", "solution": "We begin with the goal of estimating the expectation $\\mathbb{E}[\\varphi(X_{T})]$ by the Monte Carlo estimator $\\widehat{m}_{N}=\\frac{1}{N}\\sum_{i=1}^{N}Y_{i}$, where $Y_{i}=\\varphi(X_{T}^{(i)})$ and $X_{T}^{(i)}$ are independent realizations of $X_{T}$. Under the stated assumptions, the samples $Y_{1},\\dots,Y_{N}$ are independent and identically distributed with finite variance $V=\\mathrm{Var}(Y_{1})$ and mean $m=\\mathbb{E}[Y_{1}]=\\mathbb{E}[\\varphi(X_{T})]$.\n\nThe mean-squared error (MSE) of $\\widehat{m}_{N}$ is defined as\n$$\n\\mathrm{MSE}(\\widehat{m}_{N})=\\mathbb{E}\\big[(\\widehat{m}_{N}-m)^{2}\\big].\n$$\nBy the bias–variance decomposition,\n$$\n\\mathrm{MSE}(\\widehat{m}_{N})=\\big(\\mathbb{E}[\\widehat{m}_{N}]-m\\big)^{2}+\\mathrm{Var}(\\widehat{m}_{N}).\n$$\nIn our idealized setting, there is no discretization bias and the estimator is unbiased, so $\\mathbb{E}[\\widehat{m}_{N}]=m$ and the bias term vanishes. Therefore,\n$$\n\\mathrm{MSE}(\\widehat{m}_{N})=\\mathrm{Var}(\\widehat{m}_{N}).\n$$\nUsing independence and identical distribution, the variance of the sample mean is\n$$\n\\mathrm{Var}(\\widehat{m}_{N})=\\mathrm{Var}\\!\\left(\\frac{1}{N}\\sum_{i=1}^{N}Y_{i}\\right)=\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\mathrm{Var}(Y_{i})=\\frac{1}{N^{2}}\\cdot N\\cdot V=\\frac{V}{N}.\n$$\nThe root-mean-square error (RMSE) is the square root of the MSE:\n$$\n\\mathrm{RMSE}(\\widehat{m}_{N})=\\sqrt{\\mathrm{MSE}(\\widehat{m}_{N})}=\\sqrt{\\frac{V}{N}}.\n$$\nImposing the target RMSE threshold $\\varepsilon>0$ means requiring\n$$\n\\sqrt{\\frac{V}{N}}\\leq \\varepsilon.\n$$\nSquaring both sides and solving for $N$,\n$$\n\\frac{V}{N}\\leq \\varepsilon^{2}\\quad\\Longleftrightarrow\\quad N\\geq \\frac{V}{\\varepsilon^{2}}.\n$$\nBecause $N$ must be an integer, the minimal integer sample size achieving the target is\n$$\nN^{\\star}=\\left\\lceil\\frac{V}{\\varepsilon^{2}}\\right\\rceil.\n$$\n\nWe now analyze sensitivity of the required sample size to variance misestimation. Consider the continuous relaxation $N(V)=\\frac{V}{\\varepsilon^{2}}$ (ignoring the ceiling for differential analysis). The first-order sensitivity of $N$ with respect to $V$ is the derivative\n$$\n\\frac{dN}{dV}=\\frac{d}{dV}\\left(\\frac{V}{\\varepsilon^{2}}\\right)=\\frac{1}{\\varepsilon^{2}}.\n$$\nTo quantify relative sensitivity, define the logarithmic derivative (elasticity)\n$$\nS_{\\mathrm{rel}}(V)=\\frac{dN/N}{dV/V}=\\frac{\\frac{dN}{dV}\\cdot \\frac{1}{N}}{\\frac{1}{V}}.\n$$\nSubstituting $N(V)=\\frac{V}{\\varepsilon^{2}}$ and $\\frac{dN}{dV}=\\frac{1}{\\varepsilon^{2}}$, we find\n$$\nS_{\\mathrm{rel}}(V)=\\frac{\\left(\\frac{1}{\\varepsilon^{2}}\\right)\\cdot \\left(\\frac{\\varepsilon^{2}}{V}\\right)}{\\frac{1}{V}}=1.\n$$\nThus, to first order, the relative error in the required (continuous) sample size equals the relative error in $V$: if $V$ is misestimated as $V(1+\\delta)$ with small $\\delta$, then $N$ is misestimated as $N(1+\\delta)$, up to the effect of the ceiling operator. In particular, underestimation of $V$ by a factor leads to an equal proportional underestimation of the required $N$, which risks violating the RMSE constraint $\\sqrt{V/N}\\leq \\varepsilon$; overestimation of $V$ proportionally over-provisions $N$ and is conservative.", "answer": "$$\\boxed{\\begin{pmatrix}\\left\\lceil \\dfrac{V}{\\varepsilon^{2}} \\right\\rceil  \\dfrac{1}{\\varepsilon^{2}}  1\\end{pmatrix}}$$", "id": "2988319"}, {"introduction": "While the first practice assumed perfect sampling, in reality, we almost always approximate the solution of an SDE using a numerical time-stepping scheme. This approximation introduces a systematic error known as discretization bias, which does not vanish by simply increasing the number of Monte Carlo paths. This exercise [@problem_id:2988356] moves from the statistical error of sampling to this systematic error of discretization. By analyzing the popular Euler-Maruyama scheme applied to the Geometric Brownian Motion model, you will explicitly calculate the \"weak bias\"—the difference between the expectation of the numerical solution and the true expectation—and see how it depends on the size of the time step $\\Delta t$.", "problem": "Let $\\{X_t\\}_{t \\in [0,T]}$ be the solution to the Geometric Brownian Motion (GBM) Stochastic Differential Equation (SDE)\n$$\ndX_t \\;=\\; \\mu\\,X_t\\,dt \\;+\\; \\sigma\\,X_t\\,dW_t, \\qquad X_0 \\;=\\; x \\;\\; 0,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma \\geq 0$ are constants and $\\{W_t\\}_{t \\geq 0}$ is a standard Brownian motion. Consider the Euler–Maruyama (EM) time discretization with a uniform time step $\\Delta t \\;=\\; T/N$ for some integer $N \\geq 1$, defined by the recursion\n$$\nX_{n+1}^{\\Delta t} \\;=\\; X_n^{\\Delta t} \\;+\\; \\mu\\,X_n^{\\Delta t}\\,\\Delta t \\;+\\; \\sigma\\,X_n^{\\Delta t}\\,\\Delta W_n, \\qquad X_0^{\\Delta t} \\;=\\; x,\n$$\nwhere $\\Delta W_n \\sim \\mathcal{N}(0,\\Delta t)$ are independent and identically distributed and independent of $X_0^{\\Delta t}$. A Monte Carlo estimator for $\\mathbb{E}[X_T]$ based on EM with $L$ independent simulated paths is\n$$\n\\widehat{M}_L(\\Delta t) \\;=\\; \\frac{1}{L}\\sum_{\\ell=1}^{L} X_{N}^{\\Delta t,(\\ell)},\n$$\nwhere $X_{N}^{\\Delta t,(\\ell)}$ denotes the terminal EM approximation at time $T$ for the $\\ell$-th path.\n\nStarting only from the SDE definition, the EM recursion, and the basic properties of Brownian motion increments, derive a closed-form expression for $\\mathbb{E}[X_{N}^{\\Delta t}]$ as a function of $\\mu$, $\\sigma$, $x$, $T$, and $\\Delta t$, and then derive $\\mathbb{E}[X_T]$ for the exact GBM solution. Use these results to compute the weak bias of the EM-based Monte Carlo estimator, defined by\n$$\n\\operatorname{bias}(\\Delta t) \\;=\\; \\mathbb{E}[X_{N}^{\\Delta t}] \\;-\\; \\mathbb{E}[X_T].\n$$\nProvide the final answer as a single closed-form analytic expression for $\\operatorname{bias}(\\Delta t)$ in terms of $\\mu$, $\\sigma$, $x$, $T$, and $\\Delta t$. No numerical values are required.", "solution": "The problem requires the derivation of the weak bias of the Euler-Maruyama (EM) method for the Geometric Brownian Motion (GBM) process. The weak bias is defined as $\\operatorname{bias}(\\Delta t) = \\mathbb{E}[X_{N}^{\\Delta t}] - \\mathbb{E}[X_T]$, where $X_{N}^{\\Delta t}$ is the numerical solution at time $T$ and $X_T$ is the exact solution. The derivation will proceed in three steps: first, we compute the expectation of the EM approximation, $\\mathbb{E}[X_{N}^{\\Delta t}]$; second, we compute the expectation of the exact solution, $\\mathbb{E}[X_T]$; and third, we take their difference.\n\nFirst, we determine the expectation of the numerical solution obtained from the Euler-Maruyama scheme. The recursion is given by:\n$$\nX_{n+1}^{\\Delta t} \\;=\\; X_n^{\\Delta t} \\;+\\; \\mu\\,X_n^{\\Delta t}\\,\\Delta t \\;+\\; \\sigma\\,X_n^{\\Delta t}\\,\\Delta W_n\n$$\nwhere $n = 0, 1, \\dots, N-1$. We can factor out $X_n^{\\Delta t}$ to get:\n$$\nX_{n+1}^{\\Delta t} \\;=\\; X_n^{\\Delta t}\\,(1 + \\mu\\,\\Delta t + \\sigma\\,\\Delta W_n)\n$$\nLet $\\mathcal{F}_{t_n}$ be the filtration generated by the Brownian motion up to time $t_n = n\\Delta t$. The value of $X_n^{\\Delta t}$ is known at time $t_n$, so it is $\\mathcal{F}_{t_n}$-measurable. The Brownian increment $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is independent of $\\mathcal{F}_{t_n}$. We compute the expectation of $X_{n+1}^{\\Delta t}$ by taking the conditional expectation with respect to $\\mathcal{F}_{t_n}$ and then applying the law of total expectation.\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}] \\;=\\; \\mathbb{E}[X_n^{\\Delta t}(1 + \\mu\\,\\Delta t + \\sigma\\,\\Delta W_n) | \\mathcal{F}_{t_n}]\n$$\nSince $X_n^{\\Delta t}$ is $\\mathcal{F}_{t_n}$-measurable, we can treat it as a constant within the conditional expectation:\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}] \\;=\\; X_n^{\\Delta t} \\,\\mathbb{E}[1 + \\mu\\,\\Delta t + \\sigma\\,\\Delta W_n | \\mathcal{F}_{t_n}]\n$$\nDue to the independence of $\\Delta W_n$ from $\\mathcal{F}_{t_n}$ and the property that $\\mathbb{E}[\\Delta W_n] = 0$, the expression simplifies:\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}] \\;=\\; X_n^{\\Delta t} \\,(1 + \\mu\\,\\Delta t + \\sigma\\,\\mathbb{E}[\\Delta W_n]) \\;=\\; X_n^{\\Delta t}\\,(1 + \\mu\\,\\Delta t)\n$$\nNow, taking the unconditional expectation of both sides using the law of total expectation, $\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y|\\mathcal{F}]]$, we get:\n$$\n\\mathbb{E}[X_{n+1}^{\\Delta t}] \\;=\\; \\mathbb{E}[\\mathbb{E}[X_{n+1}^{\\Delta t} | \\mathcal{F}_{t_n}]] \\;=\\; \\mathbb{E}[X_n^{\\Delta t}\\,(1 + \\mu\\,\\Delta t)] \\;=\\; (1 + \\mu\\,\\Delta t)\\,\\mathbb{E}[X_n^{\\Delta t}]\n$$\nThis is a recurrence relation for $\\mathbb{E}[X_n^{\\Delta t}]$. The initial condition is $\\mathbb{E}[X_0^{\\Delta t}] = \\mathbb{E}[x] = x$. We can solve this recurrence by iterating from $n=0$ to $N$:\n$$\n\\mathbb{E}[X_N^{\\Delta t}] \\;=\\; (1 + \\mu\\,\\Delta t)^N \\mathbb{E}[X_0^{\\Delta t}] \\;=\\; x(1 + \\mu\\,\\Delta t)^N\n$$\nSubstituting $N = T/\\Delta t$, we obtain the closed-form expression for the expectation of the EM approximation:\n$$\n\\mathbb{E}[X_N^{\\Delta t}] \\;=\\; x\\left(1 + \\mu\\,\\Delta t\\right)^{T/\\Delta t}\n$$\n\nSecond, we derive the expectation of the exact solution $X_T$. The SDE is given in differential form as $dX_t = \\mu X_t dt + \\sigma X_t dW_t$. Its integral form is:\n$$\nX_T \\;=\\; X_0 + \\int_0^T \\mu\\,X_t\\,dt + \\int_0^T \\sigma\\,X_t\\,dW_t\n$$\nTaking the expectation of both sides, we get:\n$$\n\\mathbb{E}[X_T] \\;=\\; \\mathbb{E}[X_0] + \\mathbb{E}\\left[\\int_0^T \\mu\\,X_t\\,dt\\right] + \\mathbb{E}\\left[\\int_0^T \\sigma\\,X_t\\,dW_t\\right]\n$$\nWe evaluate each term on the right-hand side. The initial condition gives $\\mathbb{E}[X_0] = x$. For the drift term, we can interchange the expectation and the Riemann integral by Fubini's theorem:\n$$\n\\mathbb{E}\\left[\\int_0^T \\mu\\,X_t\\,dt\\right] \\;=\\; \\mu \\int_0^T \\mathbb{E}[X_t]\\,dt\n$$\nFor the diffusion term, a fundamental property of the Itô integral is that its expectation is zero, provided the integrand is an adapted process satisfying certain integrability conditions, which hold for the solution of the GBM SDE. Thus:\n$$\n\\mathbb{E}\\left[\\int_0^T \\sigma\\,X_t\\,dW_t\\right] \\;=\\; 0\n$$\nSubstituting these back, and letting $m(t) = \\mathbb{E}[X_t]$, we obtain an integral equation for $m(T)$:\n$$\nm(T) \\;=\\; x + \\mu \\int_0^T m(t)\\,dt\n$$\nDifferentiating with respect to $T$ gives the ordinary differential equation (ODE):\n$$\n\\frac{dm(T)}{dT} \\;=\\; \\mu\\,m(T)\n$$\nThe initial condition is $m(0) = \\mathbb{E}[X_0] = x$. The solution to this ODE is:\n$$\nm(T) \\;=\\; x\\,\\exp(\\mu T)\n$$\nTherefore, the expectation of the exact solution is:\n$$\n\\mathbb{E}[X_T] \\;=\\; x\\,\\exp(\\mu T)\n$$\nNote that the expectations of both the numerical and the exact solutions are independent of the volatility parameter $\\sigma$.\n\nFinally, we compute the weak bias by taking the difference between the two expectations derived above:\n$$\n\\operatorname{bias}(\\Delta t) \\;=\\; \\mathbb{E}[X_{N}^{\\Delta t}] - \\mathbb{E}[X_T] \\;=\\; x\\left(1 + \\mu\\,\\Delta t\\right)^{T/\\Delta t} - x\\,\\exp(\\mu T)\n$$\nThis expression can be factored to yield the final form:\n$$\n\\operatorname{bias}(\\Delta t) \\;=\\; x\\left(\\left(1 + \\mu\\,\\Delta t\\right)^{T/\\Delta t} - \\exp(\\mu T)\\right)\n$$\nThis is the closed-form expression for the weak bias as a function of the specified parameters.", "answer": "$$\n\\boxed{x\\left(\\left(1 + \\mu\\Delta t\\right)^{\\frac{T}{\\Delta t}} - \\exp(\\mu T)\\right)}\n$$", "id": "2988356"}, {"introduction": "Our focus so far has been on estimating expectations of functionals that depend only on the SDE's value at a single point in time, $X_T$. However, many important applications, particularly in financial engineering with path-dependent options, involve functionals of the entire process history, such as the running maximum $M_T = \\max_{0 \\le t \\le T} X_t$. Discretely sampling a path can cause us to miss peaks that occur between our time steps, leading to a persistent underestimation bias. This advanced practice [@problem_id:2988361] delves into this subtle but critical issue, guiding you to quantify the expected size of a \"missed maximum\" within a single time step using the elegant theory of Brownian bridges.", "problem": "Consider the stochastic differential equation (SDE) $dX_t=\\sigma\\,dW_t$ with $X_0=0$ and constant volatility $\\sigma0$, where $W_t$ is a standard Brownian motion (BM). Let the running maximum be $M_T=\\max_{0\\le t\\le T}X_t$. A common Monte Carlo (MC) discretization strategy for estimating expectations of functionals that depend on $M_T$ samples the path at a uniform grid $0=h\\cdot 0h\\cdot 1\\dotsh\\cdot n=T$ and replaces $M_T$ by the discrete maximum $M_{T,h}=\\max_{0\\le k\\le n}X_{kh}$, which induces a negative bias due to missed subgrid maxima.\n\nA refinement is a Brownian bridge correction, which quantifies the missed maxima between grid points by using the conditional distribution of the supremum of the process on an interval $[kh,(k+1)h]$ given the endpoints $(X_{kh},X_{(k+1)h})$. In this problem, you will analyze the missed-maximum bias at the level of a single interval.\n\nAssume that over a single interval of length $h$, the grid endpoints both attain the current running maximum $m$ (that is, $X_{kh}=X_{(k+1)h}=m$ and $m=\\max\\{X_{kh},X_{(k+1)h}\\}$). Under this scenario, model the path on $[kh,(k+1)h]$ as a Brownian bridge with volatility $\\sigma$ pinned at $m$ at both endpoints, and calculate the conditional expectation of the overshoot of the continuous-time supremum above $m$ on this interval, namely\n$$\\mathbb{E}\\!\\left[\\sup_{kh\\le t\\le (k+1)h}X_t-m\\;\\middle|\\;X_{kh}=m,\\;X_{(k+1)h}=m\\right].$$\n\nDerive this quantity from first principles and express your final answer as a closed-form analytic expression in terms of $\\sigma$ and $h$. The final answer must be a single expression. No rounding is required.", "solution": "Let the quantity to be calculated be denoted by $\\mathcal{E}$.\n$$ \\mathcal{E} = \\mathbb{E}\\!\\left[\\sup_{kh\\le t\\le (k+1)h}X_t-m\\;\\middle|\\;X_{kh}=m,\\;X_{(k+1)h}=m\\right] $$\nThe problem is simplified by a change of variables. Let's define a new time variable $s = t - kh$, which runs from $0$ to $h$ as $t$ runs from $kh$ to $(k+1)h$. We also define a new process $Y_s$ by shifting the spatial origin:\n$$ Y_s = X_{kh+s} - m $$\nThe original conditioning event $X_{kh}=m$ and $X_{(k+1)h}=m$ translates to conditions on the new process $Y_s$:\n-   At $s=0$ (which corresponds to $t=kh$): $Y_0 = X_{kh} - m = m - m = 0$.\n-   At $s=h$ (which corresponds to $t=(k+1)h$): $Y_h = X_{(k+1)h} - m = m - m = 0$.\n\nThe SDE for $X_t$ is $dX_t = \\sigma dW_t$. The dynamics of $Y_s$ are given by $dY_s = d(X_{kh+s}) = \\sigma dW_{kh+s}$. Since $W_t$ is a standard Brownian motion, the process $W'_s = W_{kh+s} - W_{kh}$ is also a standard Brownian motion for $s \\ge 0$. Therefore, the dynamics of $Y_s$ can be written as $dY_s = \\sigma dW'_s$.\n\nThe process $Y_s$ for $s \\in [0, h]$ is thus a stochastic process that starts at $Y_0=0$, ends at $Y_h=0$, and follows the dynamics of a scaled Brownian motion. This is precisely the definition of a Brownian bridge with volatility $\\sigma$ over the time interval $[0, h]$.\n\nThe quantity of interest $\\mathcal{E}$ can be rewritten in terms of $Y_s$:\n$$ \\mathcal{E} = \\mathbb{E}\\!\\left[\\sup_{0\\le s\\le h}(Y_s + m) - m\\right] = \\mathbb{E}\\!\\left[\\sup_{0\\le s\\le h}Y_s\\right] $$\nSo, the problem reduces to finding the expected value of the supremum of a Brownian bridge $Y_s$ on the interval $[0,h]$ with volatility $\\sigma$.\n\nTo calculate this expectation, we can scale the process $Y_s$ to a standard Brownian bridge. A standard Brownian bridge, denoted $B_u$, is defined on the unit interval $u \\in [0,1]$ and has volatility $1$. The process $Y_s$ can be related to $B_u$ by scaling time and space.\n\nLet $s = uh$, where $u \\in [0,1]$. The process $Y_s = Y_{uh}$ can be expressed in terms of a standard Brownian bridge $B_u$ as:\n$$ Y_{uh} = \\sigma \\sqrt{h} B_u $$\nThis scaling relationship is a standard result. To briefly verify it, a Brownian bridge $Y_s$ from $0$ to $0$ on $[0,h]$ has a variance of $\\text{Var}[Y_s] = \\sigma^2 \\frac{s(h-s)}{h}$. The scaled process has variance $\\text{Var}[\\sigma\\sqrt{h}B_{s/h}] = (\\sigma\\sqrt{h})^2 \\text{Var}[B_{s/h}]$. Since $\\text{Var}[B_u] = u(1-u)$, we have $\\text{Var}[\\sigma\\sqrt{h}B_{s/h}] = \\sigma^2 h \\frac{s}{h}(1-\\frac{s}{h}) = \\sigma^2 s(1-\\frac{s}{h}) = \\sigma^2 \\frac{s(h-s)}{h}$, which matches.\n\nUsing this scaling, the supremum of $Y_s$ is related to the supremum of $B_u$:\n$$ \\sup_{0\\le s\\le h} Y_s = \\sup_{0\\le u\\le 1} Y_{uh} = \\sup_{0\\le u\\le 1} (\\sigma\\sqrt{h} B_u) = \\sigma\\sqrt{h} \\sup_{0\\le u\\le 1} B_u $$\nTaking the expectation of both sides:\n$$ \\mathcal{E} = \\mathbb{E}\\!\\left[\\sup_{0\\le s\\le h}Y_s\\right] = \\sigma\\sqrt{h} \\; \\mathbb{E}\\!\\left[\\sup_{0\\le u\\le 1} B_u\\right] $$\nThe final step is to calculate the expectation of the supremum of a standard Brownian bridge, $\\mathbb{E}[\\sup_{0\\le u\\le 1} B_u]$.\n\nLet $M_B = \\sup_{0\\le u\\le 1} B_u$. The cumulative distribution function (CDF) of $M_B$ is a well-known result from the theory of stochastic processes:\n$$ P(M_B \\le x) = 1 - \\exp(-2x^2) \\quad \\text{for } x \\ge 0 $$\nThe probability density function (PDF), $f_{M_B}(x)$, is found by differentiating the CDF with respect to $x$:\n$$ f_{M_B}(x) = \\frac{d}{dx} \\left(1 - \\exp(-2x^2)\\right) = 4x\\exp(-2x^2) \\quad \\text{for } x \\ge 0 $$\nThe expected value of $M_B$ is then calculated by integrating $x f_{M_B}(x)$ from $0$ to $\\infty$:\n$$ \\mathbb{E}[M_B] = \\int_0^\\infty x \\cdot f_{M_B}(x) \\, dx = \\int_0^\\infty x (4x\\exp(-2x^2)) \\, dx = 4 \\int_0^\\infty x^2\\exp(-2x^2) \\, dx $$\nThis is a standard Gaussian-type integral. We can solve it by substitution. Let $v = \\sqrt{2}x$. Then $x = v/\\sqrt{2}$ and $dx = dv/\\sqrt{2}$. The limits of integration remain $0$ and $\\infty$.\n\\begin{align*} \\mathbb{E}[M_B] = 4 \\int_0^\\infty \\left(\\frac{v}{\\sqrt{2}}\\right)^2 \\exp(-v^2) \\left(\\frac{dv}{\\sqrt{2}}\\right) \\\\ = 4 \\int_0^\\infty \\frac{v^2}{2} \\exp(-v^2) \\frac{dv}{\\sqrt{2}} \\\\ = \\frac{4}{2\\sqrt{2}} \\int_0^\\infty v^2\\exp(-v^2) \\, dv \\\\ = \\sqrt{2} \\int_0^\\infty v^2\\exp(-v^2) \\, dv \\end{align*}\nThe definite integral $\\int_0^\\infty v^2\\exp(-v^2) \\, dv$ is a known value related to the Gamma function, equal to $\\frac{\\sqrt{\\pi}}{4}$.\nSubstituting this value back:\n$$ \\mathbb{E}[M_B] = \\sqrt{2} \\cdot \\frac{\\sqrt{\\pi}}{4} = \\frac{\\sqrt{2\\pi}}{4} = \\frac{\\sqrt{\\pi}}{2\\sqrt{2}} = \\sqrt{\\frac{\\pi}{8}} $$\nNow, we substitute this result back into our expression for $\\mathcal{E}$:\n$$ \\mathcal{E} = \\sigma\\sqrt{h} \\cdot \\mathbb{E}[M_B] = \\sigma\\sqrt{h} \\sqrt{\\frac{\\pi}{8}} $$\nCombining the terms under the square root gives the final expression:\n$$ \\mathcal{E} = \\sigma \\sqrt{\\frac{\\pi h}{8}} $$\nThis is the conditional expectation of the overshoot of the supremum above the level $m$ on an interval of length $h$, given the process is pinned to $m$ at both ends.", "answer": "$$\\boxed{\\sigma \\sqrt{\\frac{\\pi h}{8}}}$$", "id": "2988361"}]}