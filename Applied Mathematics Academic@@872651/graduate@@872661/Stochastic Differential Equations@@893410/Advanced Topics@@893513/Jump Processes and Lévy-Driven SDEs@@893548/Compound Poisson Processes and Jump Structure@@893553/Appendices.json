{"hands_on_practices": [{"introduction": "Understanding the statistical properties of a compound Poisson process is the first step toward applying it in models. This exercise guides you through the derivation of the mean and variance of the process, which are essential for applications in areas like insurance risk and financial modeling. By applying the laws of total expectation and variance, you will practice a fundamental technique in stochastic analysis and uncover the elegant relationship between the jump intensity, jump size distribution, and the process's overall moments.", "problem": "Let $\\{N_t\\}_{t \\ge 0}$ be a Poisson process with rate $\\lambda  0$. Let $\\{J_i\\}_{i \\in \\mathbb{N}}$ be an independent and identically distributed sequence of real-valued jump sizes, independent of $\\{N_t\\}_{t \\ge 0}$. Define the compound Poisson process $X_t$ by\n$$\nX_t \\coloneqq \\sum_{i=1}^{N_t} J_i, \\quad t \\ge 0,\n$$\nwith the convention that the empty sum equals $0$ when $N_t = 0$. Using only fundamental definitions and properties of conditional expectation and variance, derive closed-form expressions for $\\mathbb{E}[X_t]$ and $\\operatorname{Var}(X_t)$ in terms of $\\lambda$, $t$, $\\mathbb{E}[J_1]$, and $\\mathbb{E}[J_1^2]$. State explicitly the weakest moment conditions on $J_1$ under which each of these expressions is finite. \n\nProvide your final answer as a single row matrix containing, in order, $\\mathbb{E}[X_t]$ and $\\operatorname{Var}(X_t)$, expressed in closed form.", "solution": "The problem is to derive the expectation and variance of a compound Poisson process $X_t = \\sum_{i=1}^{N_t} J_i$, where $\\{N_t\\}_{t \\ge 0}$ is a Poisson process with rate $\\lambda  0$, and $\\{J_i\\}_{i \\in \\mathbb{N}}$ are independent and identically distributed (i.i.d.) random variables, also independent of $\\{N_t\\}$. We are to use fundamental properties of conditional expectation and variance.\n\nWe recall the key properties of the Poisson process $\\{N_t\\}$:\n$1$. For any $t  0$, the random variable $N_t$ follows a Poisson distribution with parameter $\\lambda t$. Its probability mass function is $P(N_t=n) = \\frac{(\\lambda t)^n e^{-\\lambda t}}{n!}$ for $n \\in \\{0, 1, 2, \\dots\\}$.\n$2$. The expectation and variance are $\\mathbb{E}[N_t] = \\lambda t$ and $\\operatorname{Var}(N_t) = \\lambda t$.\n\nWe are also given that the jump sizes $\\{J_i\\}$ are i.i.d., so for any $i, k \\in \\mathbb{N}$, $\\mathbb{E}[J_i] = \\mathbb{E}[J_1]$ and $\\operatorname{Var}(J_i) = \\operatorname{Var}(J_1)$.\n\n**Derivation of the Expectation $\\mathbb{E}[X_t]$**\n\nWe use the Law of Total Expectation, which states that for two random variables $X$ and $Y$, $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X|Y]]$. We condition on the number of jumps $N_t$.\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[\\mathbb{E}[X_t | N_t]]\n$$\nFirst, we compute the inner conditional expectation, $\\mathbb{E}[X_t | N_t = n]$ for some integer $n \\ge 0$.\nIf $N_t=0$, then $X_t=0$ by convention, so $\\mathbb{E}[X_t | N_t=0] = \\mathbb{E}[0] = 0$.\nIf $N_t=n$ for $n  0$, then $X_t = \\sum_{i=1}^{n} J_i$.\n$$\n\\mathbb{E}[X_t | N_t = n] = \\mathbb{E}\\left[\\sum_{i=1}^{n} J_i \\bigg| N_t = n\\right]\n$$\nSince the sequence $\\{J_i\\}$ is independent of the process $\\{N_t\\}$, conditioning on $N_t=n$ does not affect the distribution of the $J_i$.\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^{n} J_i \\bigg| N_t = n\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} J_i\\right]\n$$\nBy linearity of expectation, and since the $J_i$ are identically distributed:\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^{n} J_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[J_i] = \\sum_{i=1}^{n} \\mathbb{E}[J_1] = n \\mathbb{E}[J_1]\n$$\nThis result also holds for $n=0$ if we define the empty sum to be $0$, as $0 \\cdot \\mathbb{E}[J_1] = 0$.\nSo, we can express the conditional expectation as a random variable depending on $N_t$:\n$$\n\\mathbb{E}[X_t | N_t] = N_t \\mathbb{E}[J_1]\n$$\nNow we take the expectation of this random variable with respect to the distribution of $N_t$:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[N_t \\mathbb{E}[J_1]]\n$$\nSince $\\mathbb{E}[J_1]$ is a constant, we can factor it out of the expectation:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[J_1] \\mathbb{E}[N_t]\n$$\nSubstituting $\\mathbb{E}[N_t] = \\lambda t$, we obtain the final expression for the expectation:\n$$\n\\mathbb{E}[X_t] = \\lambda t \\mathbb{E}[J_1]\n$$\nFor this expectation to be finite, we require $\\mathbb{E}[J_1]$ to be finite. The weakest moment condition on $J_1$ is that its first moment exists, i.e., $\\mathbb{E}[|J_1|]  \\infty$.\n\n**Derivation of the Variance $\\operatorname{Var}(X_t)$**\n\nWe use the Law of Total Variance, which states $\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X|Y)] + \\operatorname{Var}(\\mathbb{E}[X|Y])$. We again condition on $N_t$.\n$$\n\\operatorname{Var}(X_t) = \\mathbb{E}[\\operatorname{Var}(X_t | N_t)] + \\operatorname{Var}(\\mathbb{E}[X_t | N_t])\n$$\nWe compute each of the two terms separately.\n\nTerm 1: $\\mathbb{E}[\\operatorname{Var}(X_t | N_t)]$\nFirst, we compute the inner conditional variance, $\\operatorname{Var}(X_t | N_t=n)$.\nFor $N_t=n  0$, $X_t = \\sum_{i=1}^{n} J_i$.\n$$\n\\operatorname{Var}(X_t | N_t=n) = \\operatorname{Var}\\left(\\sum_{i=1}^{n} J_i \\bigg| N_t=n\\right)\n$$\nDue to the independence of $\\{J_i\\}$ and $\\{N_t\\}$, this is $\\operatorname{Var}(\\sum_{i=1}^{n} J_i)$. The events $\\{J_i \\le x_i\\}$ are also independent for different $i$. Therefore, the $J_i$ are mutually independent.\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} J_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(J_i)\n$$\nSince the $J_i$ are identically distributed, $\\operatorname{Var}(J_i) = \\operatorname{Var}(J_1)$ for all $i$.\n$$\n\\sum_{i=1}^{n} \\operatorname{Var}(J_i) = n \\operatorname{Var}(J_1)\n$$\nFor $n=0$, $X_t=0$, so $\\operatorname{Var}(X_t|N_t=0)=\\operatorname{Var}(0)=0$, which is consistent with $n \\operatorname{Var}(J_1)$ for $n=0$.\nThe conditional variance as a random variable is thus $\\operatorname{Var}(X_t | N_t) = N_t \\operatorname{Var}(J_1)$.\nTaking the expectation of this random variable:\n$$\n\\mathbb{E}[\\operatorname{Var}(X_t | N_t)] = \\mathbb{E}[N_t \\operatorname{Var}(J_1)] = \\operatorname{Var}(J_1) \\mathbb{E}[N_t] = \\lambda t \\operatorname{Var}(J_1)\n$$\n\nTerm 2: $\\operatorname{Var}(\\mathbb{E}[X_t | N_t])$\nFrom our derivation of the mean, we have $\\mathbb{E}[X_t | N_t] = N_t \\mathbb{E}[J_1]$.\nWe now compute the variance of this random variable.\n$$\n\\operatorname{Var}(\\mathbb{E}[X_t | N_t]) = \\operatorname{Var}(N_t \\mathbb{E}[J_1])\n$$\nUsing the property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ for a constant $a$, where here $a=\\mathbb{E}[J_1]$:\n$$\n\\operatorname{Var}(N_t \\mathbb{E}[J_1]) = (\\mathbb{E}[J_1])^2 \\operatorname{Var}(N_t)\n$$\nSubstituting $\\operatorname{Var}(N_t) = \\lambda t$:\n$$\n\\operatorname{Var}(\\mathbb{E}[X_t | N_t]) = (\\mathbb{E}[J_1])^2 \\lambda t\n$$\n\nCombining the two terms:\n$$\n\\operatorname{Var}(X_t) = \\lambda t \\operatorname{Var}(J_1) + \\lambda t (\\mathbb{E}[J_1])^2\n$$\nWe can factor out $\\lambda t$:\n$$\n\\operatorname{Var}(X_t) = \\lambda t (\\operatorname{Var}(J_1) + (\\mathbb{E}[J_1])^2)\n$$\nBy definition, $\\operatorname{Var}(J_1) = \\mathbb{E}[J_1^2] - (\\mathbb{E}[J_1])^2$. Substituting this into the expression:\n$$\n\\operatorname{Var}(X_t) = \\lambda t \\left( (\\mathbb{E}[J_1^2] - (\\mathbb{E}[J_1])^2) + (\\mathbb{E}[J_1])^2 \\right)\n$$\nThe terms $(\\mathbb{E}[J_1])^2$ cancel, leading to the simple result:\n$$\n\\operatorname{Var}(X_t) = \\lambda t \\mathbb{E}[J_1^2]\n$$\nFor this variance to be finite, both terms in the Law of Total Variance must be finite. This requires $\\operatorname{Var}(J_1)$ and $(\\mathbb{E}[J_1])^2$ to be finite. The finiteness of $\\operatorname{Var}(J_1) = \\mathbb{E}[J_1^2] - (\\mathbb{E}[J_1])^2$ requires $\\mathbb{E}[J_1^2]$ to be finite. If $\\mathbb{E}[J_1^2]  \\infty$, then by the Cauchy-Schwarz inequality or Jensen's inequality, $|\\mathbb{E}[J_1]| \\le \\mathbb{E}[|J_1|] \\le \\sqrt{\\mathbb{E}[J_1^2]}  \\infty$, so $\\mathbb{E}[J_1]$ is also finite. Thus, the weakest moment condition for the variance of $X_t$ to be finite is that the second moment of the jump size is finite, i.e., $\\mathbb{E}[J_1^2]  \\infty$.\n\nSummary of results:\n- Expectation: $\\mathbb{E}[X_t] = \\lambda t \\mathbb{E}[J_1]$\n- Variance: $\\operatorname{Var}(X_t) = \\lambda t \\mathbb{E}[J_1^2]$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\lambda t \\mathbb{E}[J_1]  \\lambda t \\mathbb{E}[J_1^2] \\end{pmatrix}}\n$$", "id": "2971257"}, {"introduction": "Having characterized the basic process, we now embed it within a dynamic system by defining a stochastic differential equation (SDE) with jumps. For such a model to be useful, we must first ensure it is well-posed—meaning a unique solution exists and behaves predictably. This practice challenges you to identify the standard global Lipschitz and linear growth conditions on the drift and jump coefficients that guarantee the existence and pathwise uniqueness of the solution, a cornerstone of SDE theory.", "problem": "Let $N(dt,dx)$ be a Poisson random measure (PRM) on $(0,\\infty)\\times\\mathbb{R}$ with finite intensity measure $\\lambda F(dx)\\,dt$, where $\\lambda\\in(0,\\infty)$ and $F$ is a probability measure on $\\mathbb{R}$. The associated jump process is a compound Poisson process with jump times $\\{T_k\\}_{k\\ge 1}$ and marks $\\{Y_k\\}_{k\\ge 1}$, where $(Y_k)_{k\\ge 1}$ are independent and identically distributed with law $F$ and independent of $(T_k)_{k\\ge 1}$. Consider the stochastic differential equation (SDE) with jumps\n$$\ndX_t \\;=\\; b(X_{t-})\\,dt \\;+\\; \\int_{\\mathbb{R}} g(X_{t-},x)\\,N(dt,dx), \n\\qquad X_0=x\\in\\mathbb{R},\n$$\nwhere $b:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ are Borel measurable.\n\nStarting from the definitions of a compound Poisson process and an SDE with jumps, and using standard existence and uniqueness principles for ordinary differential equations between jump times, select the set of global Lipschitz and linear growth assumptions on $b$ and $g$ that constitute a sufficient condition to guarantee existence of a strong solution on $[0,T]$ for every $T0$ and pathwise uniqueness.\n\nA. There exist constants $L_b,K_b,L_g,K_g\\in[0,\\infty)$ such that $|b(u)-b(v)|\\le L_b|u-v|$ and $|b(u)|\\le K_b(1+|u|)$ for all $u,v\\in\\mathbb{R}$, and $|g(u,x)-g(v,x)|\\le L_g|u-v|$ and $|g(u,x)|\\le K_g(1+|u|)$ for all $u,v,x\\in\\mathbb{R}$.\n\nB. The function $b$ is locally Lipschitz with $|b(u)|\\le K_b(1+|u|)$ for all $u\\in\\mathbb{R}$, while $g$ is globally Lipschitz in its first argument with a uniform constant $L_g$ and has linear growth $|g(u,x)|\\le K_g(1+|u|)$ for all $u,x\\in\\mathbb{R}$.\n\nC. There exist $L_b,K_b,L_g,K_g\\in[0,\\infty)$ and a parameter $q1$ such that $|b(u)-b(v)|\\le L_b|u-v|$ and $|b(u)|\\le K_b(1+|u|)$ for all $u,v\\in\\mathbb{R}$, and $|g(u,x)-g(v,x)|\\le L_g|u-v|$ for all $u,v,x\\in\\mathbb{R}$, but $|g(u,x)|\\le K_g(1+|u|^q)$ for all $u,x\\in\\mathbb{R}$.\n\nD. The function $b$ satisfies a one-sided Lipschitz (monotonicity) condition $(b(u)-b(v))(u-v)\\le L_b|u-v|^2$ and $|b(u)|\\le K_b(1+|u|)$ for all $u,v\\in\\mathbb{R}$, and $g$ is globally Lipschitz in its first argument with uniform constant $L_g$ and has linear growth $|g(u,x)|\\le K_g(1+|u|)$ for all $u,x\\in\\mathbb{R}$.\n\nYour answer should be the option that gives sufficient conditions ensuring existence of a strong solution and pathwise uniqueness for the SDE above under the finite-activity PRM $N(dt,dx)$ described.", "solution": "The problem asks to identify a set of sufficient conditions for the existence of a strong solution and pathwise uniqueness for the following stochastic differential equation (SDE) with jumps:\n$$\ndX_t = b(X_{t-}) \\, dt + \\int_{\\mathbb{R}} g(X_{t-}, x) \\, N(dt, dx), \\qquad X_0 = x_0 \\in \\mathbb{R}\n$$\nThe driving noise is a Poisson random measure (PRM) $N(dt, dx)$ with a finite intensity measure $\\lambda F(dx) \\, dt$, where $\\lambda \\in (0, \\infty)$ is a constant and $F$ is a probability measure on $\\mathbb{R}$.\n\nThe total intensity of the jumps is given by $\\int_{\\mathbb{R}} \\lambda F(dx) = \\lambda \\int_{\\mathbb{R}} F(dx) = \\lambda \\cdot 1 = \\lambda$. Since $\\lambda$ is finite, the jump process is a compound Poisson process. This means that in any finite time interval $[0, T]$, the number of jumps is finite almost surely. Let these jump times be $\\{T_k\\}_{k \\ge 1}$ and the corresponding jump sizes (marks) be $\\{Y_k\\}_{k \\ge 1}$. The integral form of the SDE can then be written as:\n$$\nX_t = X_0 + \\int_0^t b(X_{s-}) \\, ds + \\sum_{k=1}^{N_t} g(X_{T_k-}, Y_k)\n$$\nwhere $N_t = \\int_0^t \\int_{\\mathbb{R}} 1 \\, N(ds, dx)$ is a standard Poisson process with rate $\\lambda$.\n\nThe existence of a strong solution for all time $t \\in [0, T]$ for any $T  0$ and pathwise uniqueness are typically established under certain conditions on the coefficients $b$ and $g$. The standard approach relies on two pillars: ensuring the solution does not explode in finite time, and ensuring that any two solutions starting from the same initial condition and driven by the same noise process are identical.\n\n**1. Non-explosion (Global Existence)**\n\nTo ensure the solution exists for all time (i.e., does not explode), we need to control its growth. This is typically achieved by imposing linear growth conditions on the coefficients. Let us analyze the second moment of $X_t$. The SDE can be written using the compensated PRM, $\\tilde{N}(dt, dx) = N(dt, dx) - \\lambda F(dx) dt$, as:\n$$\ndX_t = \\left( b(X_{t-}) + \\lambda \\int_{\\mathbb{R}} g(X_{t-}, x) F(dx) \\right) dt + \\int_{\\mathbb{R}} g(X_{t-}, x) \\tilde{N}(dt, dx)\n$$\nLet $\\tilde{b}(u) = b(u) + \\lambda \\int_{\\mathbb{R}} g(u, x) F(dx)$. The SDE is $dX_t = \\tilde{b}(X_{t-}) dt + \\int_{\\mathbb{R}} g(X_{t-}, x) \\tilde{N}(dt, dx)$.\nA sufficient condition for non-explosion is that the coefficients satisfy a linear growth condition. That is, there exists a constant $K  \\infty$ such that for all $u \\in \\mathbb{R}$:\n$$\n|\\tilde{b}(u)|^2 + \\int_{\\mathbb{R}} |g(u, x)|^2 \\lambda F(dx) \\le K(1 + |u|^2)\n$$\nIf we assume linear growth on $b$ and $g$ individually, namely $|b(u)| \\le K_b(1+|u|)$ and $|g(u, x)| \\le K_g(1+|u|)$ for constants $K_b, K_g$, then:\n$$\n|\\tilde{b}(u)| \\le |b(u)| + \\lambda \\int_{\\mathbb{R}} |g(u, x)| F(dx) \\le K_b(1+|u|) + \\lambda K_g(1+|u|) \\int_{\\mathbb{R}} F(dx) = (K_b + \\lambda K_g)(1+|u|)\n$$\nSo $|\\tilde{b}(u)|^2 \\le (K_b + \\lambda K_g)^2 (1+|u|)^2 \\le 2(K_b + \\lambda K_g)^2 (1+|u|^2)$.\nAlso, $\\int_{\\mathbb{R}} |g(u, x)|^2 \\lambda F(dx) \\le \\int_{\\mathbb{R}} (K_g(1+|u|))^2 \\lambda F(dx) = \\lambda K_g^2 (1+|u|)^2 \\int_{\\mathbb{R}} F(dx) = \\lambda K_g^2 (1+|u|)^2 \\le 2\\lambda K_g^2(1+|u|^2)$.\nCombining these shows that the linear growth conditions on $b$ and $g$ are sufficient to ensure non-explosion.\n\n**2. Pathwise Uniqueness**\n\nPathwise uniqueness is typically proven using a global Lipschitz condition on the coefficients. Let $X_t$ and $X'_t$ be two solutions with the same initial condition $X_0 = X'_0$ and driven by the same PRM. Let $D_t = X_t - X'_t$. We want to show $D_t = 0$ for all $t \\ge 0$.\nA sufficient condition for uniqueness is that there exists a constant $L  \\infty$ such that for all $u, v \\in \\mathbb{R}$:\n$$\n|\\tilde{b}(u) - \\tilde{b}(v)|^2 + \\int_{\\mathbb{R}} |g(u, x) - g(v, x)|^2 \\lambda F(dx) \\le L|u-v|^2\n$$\nIf we assume global Lipschitz conditions on $b$ and $g$, i.e., $|b(u)-b(v)| \\le L_b|u-v|$ and $|g(u,x)-g(v,x)| \\le L_g|u-v|$ (uniformly in $x$), then:\n$$\n|\\tilde{b}(u) - \\tilde{b}(v)| \\le |b(u)-b(v)| + \\lambda \\int_{\\mathbb{R}} |g(u,x)-g(v,x)| F(dx) \\le L_b|u-v| + \\lambda L_g|u-v| = (L_b + \\lambda L_g)|u-v|\n$$\nSo $|\\tilde{b}(u) - \\tilde{b}(v)|^2 \\le (L_b + \\lambda L_g)^2 |u-v|^2$.\nAlso, $\\int_{\\mathbb{R}} |g(u,x)-g(v,x)|^2 \\lambda F(dx) \\le \\int_{\\mathbb{R}} (L_g|u-v|)^2 \\lambda F(dx) = \\lambda L_g^2 |u-v|^2$.\nCombining these shows that the global Lipschitz conditions on $b$ and $g$ are sufficient. Applying Itô's formula to $|D_t|^2$ and taking expectations leads to $E[|D_t|^2] \\le C \\int_0^t E[|D_s|^2] ds$, which by Gronwall's lemma implies $E[|D_t|^2]=0$, hence pathwise uniqueness.\n\nThese combined conditions—global Lipschitz and linear growth on the coefficients—are the classical sufficient conditions for the existence of a unique strong solution to SDEs with jumps.\n\nNow we evaluate each option:\n\n**A. There exist constants $L_b,K_b,L_g,K_g\\in[0,\\infty)$ such that $|b(u)-b(v)|\\le L_b|u-v|$ and $|b(u)|\\le K_b(1+|u|)$ for all $u,v\\in\\mathbb{R}$, and $|g(u,x)-g(v,x)|\\le L_g|u-v|$ and $|g(u,x)|\\le K_g(1+|u|)$ for all $u,v,x\\in\\mathbb{R}$.**\n\nThis option posits global Lipschitz and linear growth conditions for both the drift coefficient $b$ and the jump coefficient $g$. As demonstrated above, these are the standard sufficient conditions that guarantee the existence of a unique strong solution for all time.\nVerdict: **Correct**.\n\n**B. The function $b$ is locally Lipschitz with $|b(u)|\\le K_b(1+|u|)$ for all $u\\in\\mathbb{R}$, while $g$ is globally Lipschitz in its first argument with a uniform constant $L_g$ and has linear growth $|g(u,x)|\\le K_g(1+|u|)$ for all $u,x\\in\\mathbb{R}$.**\n\nThis option weakens the condition on $b$ from global to local Lipschitz. For the specific case of a finite activity process, one can construct the solution piece-wise between jumps. Uniqueness for the ODE part $\\dot{y}=b(y)$ between jumps requires only local Lipschitz on $b$. The linear growth condition prevents blow-up of the ODE paths. So these conditions are, in fact, also sufficient. However, the problem explicitly asks to \"select the set of *global* Lipschitz and linear growth assumptions\". This option includes a *local* Lipschitz assumption, which does not match the description. Option A provides a set of conditions that are all either global Lipschitz or linear growth.\nVerdict: **Incorrect**. While the conditions are sufficient, they do not match the description required by the problem statement as well as option A does.\n\n**C. There exist $L_b,K_b,L_g,K_g\\in[0,\\infty)$ and a parameter $q1$ such that $|b(u)-b(v)|\\le L_b|u-v|$ and $|b(u)|\\le K_b(1+|u|)$ for all $u,v\\in\\mathbb{R}$, and $|g(u,x)-g(v,x)|\\le L_g|u-v|$ for all $u,v,x\\in\\mathbb{R}$, but $|g(u,x)|\\le K_g(1+|u|^q)$ for all $u,x\\in\\mathbb{R}$.**\n\nThis option proposes a super-linear growth condition on the jump coefficient $g$, with $|g(u,x)|$ growing like $|u|^q$ for $q1$. Such a condition can lead to the moments of the solution exploding in finite time. For instance, the analysis of $E[|X_t|^p]$ would involve terms of order $E[|X_s|^{pq}]$, which prevents the use of Gronwall's inequality to establish bounds on moments. Consequently, one cannot guarantee that the solution will not explode in finite time. Therefore, these conditions are not sufficient to guarantee existence on $[0,T]$ for every $T0$.\nVerdict: **Incorrect**.\n\n**D. The function $b$ satisfies a one-sided Lipschitz (monotonicity) condition $(b(u)-b(v))(u-v)\\le L_b|u-v|^2$ and $|b(u)|\\le K_b(1+|u|)$ for all $u,v\\in\\mathbb{R}$, and $g$ is globally Lipschitz in its first argument with uniform constant $L_g$ and has linear growth $|g(u,x)|\\le K_g(1+|u|)$ for all $u,x\\in\\mathbb{R}$.**\n\nThis option replaces the global Lipschitz condition on $b$ with a one-sided Lipschitz condition. A global Lipschitz condition $|b(u)-b(v)| \\le L_b|u-v|$ implies a one-sided Lipschitz condition, since $(b(u)-b(v))(u-v) \\le |b(u)-b(v)||u-v| \\le L_b|u-v|^2$. However, the reverse is not true (e.g., $b(u) = -u^3$). The one-sided Lipschitz condition is indeed a well-known weaker condition that is also sufficient for uniqueness. As with option B, while the condition is sufficient, it does not fit the description \"global Lipschitz ... assumption\" as precisely as option A. The problem is asking for the canonical set of conditions taught in introductory texts on SDEs, which are global Lipschitz and linear growth.\nVerdict: **Incorrect**. The conditions are sufficient, but Option A is the one that consists purely of the types of assumptions specified in the problem statement.\n\nBased on this analysis, Option A is the most accurate choice. It describes the classical, standard sufficient conditions for existence and uniqueness and perfectly matches the phrasing of the question.", "answer": "$$\\boxed{A}$$", "id": "2971242"}, {"introduction": "To truly master jump-diffusion models, one must look deeper than the general existence theorems and understand the specific role of the jump component. This problem focuses on the stochastic integral with respect to the compensated Poisson random measure, which forms the heart of the jump dynamics. You will derive the specific linear growth condition on the jump coefficient that is necessary for the process to have finite second moments, connecting the model's stability directly to the square-integrability requirements of the underlying $L^2$ martingale theory.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a Poisson random measure $N(dt,dz)$ on $[0,\\infty)\\times E$ with compensator measure $\\nu(dz)\\,dt$, where $(E,\\mathcal{E})$ is a measurable mark space and $\\nu$ is a $\\sigma$-finite Lévy measure on $E$. Let $\\tilde N(dt,dz) \\coloneqq N(dt,dz)-\\nu(dz)\\,dt$ denote the compensated Poisson random measure. Consider the stochastic differential equation (SDE) with jumps\n$$\ndX_t \\;=\\; b\\!\\left(X_{t-}\\right)\\,dt \\;+\\; \\int_E \\sigma\\!\\left(X_{t-},z\\right)\\,\\tilde N(dt,dz),\\qquad X_0=x\\in\\mathbb{R}^d,\n$$\nwhere $b:\\mathbb{R}^d\\to\\mathbb{R}^d$ and $\\sigma:\\mathbb{R}^d\\times E\\to\\mathbb{R}^d$ are Borel functions. Assume $b$ is globally Lipschitz with linear growth so that the drift part is well-defined in $L^2$ on compact time intervals. The jump part is the stochastic integral with respect to the compensated Poisson random measure, which is defined for predictable integrands that are square-integrable with respect to the intensity measure.\n\nFrom first principles, the compensated Poisson integral $\\int_0^t\\int_E \\sigma\\!\\left(X_{s-},z\\right)\\,\\tilde N(ds,dz)$ is an $L^2$-martingale if and only if its predictable integrand is square-integrable against $\\nu(dz)\\,ds$ on finite time intervals. In the context of well-posedness of the SDE in $L^2$ (existence, uniqueness, and finite second moments), a structural growth condition on $\\sigma$ in the state variable is required to control this square integrability uniformly along the trajectory $X$.\n\nWhich of the following is the standard square-integrability linear growth condition on $\\sigma$ (in the state variable) that ensures the compensated Poisson integral is well-defined in $L^2$ and that supports $L^2$ well-posedness of the SDE on finite horizons?\n\nA. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^2\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nB. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nC. For all $y\\in\\mathbb{R}^d$,\n$$\n\\sup_{z\\in E}\\,\\big|\\sigma(y,z)\\big|^2\\;\\le\\; C\\big(1+|y|^2\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nD. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.\n\nE. For all $y\\in\\mathbb{R}^d$,\n$$\n\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^4\\big),\n$$\nfor some constant $C\\in(0,\\infty)$.", "solution": "The problem asks for the standard square-integrability linear growth condition on the jump coefficient $\\sigma$ that ensures the well-posedness in $L^2$ of the stochastic differential equation (SDE) on finite time horizons. The SDE is given by\n$$\ndX_t \\;=\\; b(X_{t-})\\,dt \\;+\\; \\int_E \\sigma(X_{t-},z)\\,\\tilde N(dt,dz), \\qquad X_0=x\\in\\mathbb{R}^d.\n$$\nWell-posedness in $L^2$ on a finite horizon $[0,T]$ implies the existence of a unique solution such that $\\mathbb{E}\\left[\\sup_{t \\in [0,T]} |X_t|^2\\right]  \\infty$. A key step in establishing this is to show that $\\mathbb{E}[|X_t|^2]$ is bounded for all $t \\in [0,T]$. We will derive the necessary condition on $\\sigma$ from first principles.\n\nThe integral form of the SDE is\n$$\nX_t = X_0 + \\int_0^t b(X_{s-}) \\,ds + \\int_0^t \\int_E \\sigma(X_{s-},z) \\, \\tilde N(ds,dz).\n$$\nLet $M_t = \\int_0^t \\int_E \\sigma(X_{s-},z) \\, \\tilde N(ds,dz)$. Taking the squared norm and then the expectation, we can use the inequality $|a+b+c|^2 \\le 3(|a|^2+|b|^2+|c|^2)$ to obtain:\n$$\n\\mathbb{E}[|X_t|^2] \\le 3\\mathbb{E}[|X_0|^2] + 3\\mathbb{E}\\left[\\left|\\int_0^t b(X_s)\\,ds\\right|^2\\right] + 3\\mathbb{E}[|M_t|^2].\n$$\nWe analyze each term on the right-hand side.\n$1$. The initial condition term is $3|x|^2$, a constant.\n\n$2$. The drift term: Using the Cauchy-Schwarz inequality (or Jensen's inequality for integrals),\n$$\n\\left|\\int_0^t b(X_s)\\,ds\\right|^2 \\le \\left(\\int_0^t |b(X_s)|\\,ds\\right)^2 \\le t \\int_0^t |b(X_s)|^2\\,ds.\n$$\nThe problem states that $b$ has linear growth, i.e., there exists a constant $K_b  0$ such that $|b(y)| \\le K_b(1+|y|)$ for all $y \\in \\mathbb{R}^d$. This implies $|b(y)|^2 \\le K_b^2(1+|y|)^2 \\le 2K_b^2(1+|y|^2)$. Taking expectations and integrating, for $t \\in [0, T]$,\n$$\n\\mathbb{E}\\left[\\left|\\int_0^t b(X_s)\\,ds\\right|^2\\right] \\le T \\int_0^t \\mathbb{E}[|b(X_s)|^2]\\,ds \\le 2TK_b^2 \\int_0^t (1 + \\mathbb{E}[|X_s|^2])\\,ds.\n$$\n\n$3$. The martingale term: The process $M_t$ is a right-continuous $L^2$-martingale provided its predictable integrand $\\sigma(X_{s-},z)$ is square-integrable. By the Itô isometry for stochastic integrals with respect to a compensated Poisson random measure, the second moment of $M_t$ equals the expectation of its predictable quadratic variation:\n$$\n\\mathbb{E}[|M_t|^2] = \\mathbb{E}[\\langle\\langle M \\rangle\\rangle_t] = \\mathbb{E}\\left[\\int_0^t \\int_E |\\sigma(X_{s-},z)|^2\\,\\nu(dz)\\,ds\\right].\n$$\nUsing Fubini's theorem (as the integrand is non-negative), this becomes\n$$\n\\mathbb{E}[|M_t|^2] = \\int_0^t \\mathbb{E}\\left[\\int_E |\\sigma(X_s,z)|^2\\,\\nu(dz)\\right]\\,ds,\n$$\nwhere we can replace $X_{s-}$ with $X_s$ inside the expectation because the set of times $s$ where $X_s \\ne X_{s-}$ has Lebesgue measure zero.\n\nTo control this term and close the argument, we need a condition on the inner integral, $\\int_E |\\sigma(y,z)|^2\\,\\nu(dz)$, as a function of $y=X_s$. The question asks for a \"linear growth condition\". In the context of second-moment analysis, this refers to a bound that is a linear function of $|y|^2$. Let us assume there exists a constant $C  0$ such that for all $y \\in \\mathbb{R}^d$,\n$$\n\\int_E |\\sigma(y,z)|^2\\,\\nu(dz) \\le C(1+|y|^2).\n$$\nWith this assumption, the martingale term is bounded as follows:\n$$\n\\mathbb{E}[|M_t|^2] \\le \\int_0^t \\mathbb{E}[C(1+|X_s|^2)]\\,ds = C \\int_0^t (1+\\mathbb{E}[|X_s|^2])\\,ds.\n$$\n\nCombining all the bounds, let $u(t) = \\mathbb{E}[|X_t|^2]$. For $t \\in [0,T]$,\n$$\nu(t) \\le 3|x|^2 + 3\\left(2TK_b^2 \\int_0^t (1+u(s))\\,ds\\right) + 3\\left(C \\int_0^t (1+u(s))\\,ds\\right)\n$$\n$$\nu(t) \\le 3|x|^2 + (6TK_b^2 + 3C)\\int_0^t (1+u(s))\\,ds.\n$$\nLet $K = 6TK_b^2 + 3C$. The inequality is $u(t) \\le 3|x|^2 + K\\int_0^t(1+u(s))\\,ds$. By Gronwall's lemma, this implies that $u(t)$ is bounded on $[0,T]$. Specifically, $1+u(t) \\le (1+3|x|^2)e^{Kt}$, which shows that $\\mathbb{E}[|X_t|^2]$ remains finite.\nThis confirms that the condition $\\int_E |\\sigma(y,z)|^2\\,\\nu(dz) \\le C(1+|y|^2)$ is the appropriate linear growth condition to ensure the finiteness of second moments.\n\nNow, we evaluate each option.\n\nA. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^2\\big)$, for some constant $C\\in(0,\\infty)$.\nThis is precisely the condition derived above. It is a \"square-integrability\" condition as it involves the integral of $|\\sigma|^2$ with respect to the Lévy measure $\\nu$. It is termed a \"linear growth condition\" in the context of second-moment analysis because the bound is a linear function of $|y|^2$. This is the standard condition in the literature for $L^2$ well-posedness of SDEs with jumps.\n**Verdict: Correct.**\n\nB. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition involves the first moment of $|\\sigma|$ with respect to $\\nu$. The Itô isometry for the second moment of the compensated jump integral requires control over $\\int |\\sigma|^2 d\\nu$, not $\\int |\\sigma| d\\nu$. This condition is insufficient for controlling the second moment of the martingale part and thus is not the standard condition for $L^2$ well-posedness. It is related to conditions for finiteness of the first moment, $\\mathbb{E}[|X_t|]$.\n**Verdict: Incorrect.**\n\nC. For all $y\\in\\mathbb{R}^d$, $\\sup_{z\\in E}\\,\\big|\\sigma(y,z)\\big|^2\\;\\le\\; C\\big(1+|y|^2\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition requires the jump sizes to be uniformly bounded in the mark variable $z$ (for a fixed state $y$). If the Lévy measure $\\nu$ is finite, this condition implies condition A, since $\\int_E |\\sigma(y,z)|^2 \\nu(dz) \\le (\\sup_z |\\sigma(y,z)|^2) \\nu(E)$. However, many important Lévy measures (e.g., for stable processes) are infinite ($\\nu(E)=\\infty$), so this condition is not general enough. It is far more restrictive than the standard requirement, which only demands that the integral is controlled, not the supremum of the integrand.\n**Verdict: Incorrect.**\n\nD. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition specifies a growth rate proportional to $|y|$, not $|y|^2$. Following our derivation, this would lead to an integral inequality of the form $u(t) \\le K_1 + K_2 \\int_0^t (1+\\sqrt{u(s)})\\,ds$, where $u(t)=\\mathbb{E}[|X_t|^2]$. This is not a standard linear integral inequality, and while one can show boundedness under this stronger condition (since for large $|y|$, $|y||y|^2$), it is not the \"linear growth\" condition that pairs naturally with the second moment. The standard terminology \"linear growth\" in this context refers to the bound being linear in $|y|^2$.\n**Verdict: Incorrect.**\n\nE. For all $y\\in\\mathbb{R}^d$, $\\int_E \\big|\\sigma(y,z)\\big|^2\\,\\nu(dz)\\;\\le\\; C\\big(1+|y|^4\\big)$, for some constant $C\\in(0,\\infty)$.\nThis condition allows for quartic growth. If we insert this into our derivation for the second moment, we obtain an inequality of the form $\\mathbb{E}[|X_t|^2] \\le K_1 + \\int_0^t K_2(1+ \\mathbb{E}[|X_s|^2] + \\mathbb{E}[|X_s|^4])\\,ds$. This inequality does not \"close\" because the bound on the second moment, $\\mathbb{E}[|X_t|^2]$, depends on the fourth moment, $\\mathbb{E}[|X_s|^4]$. This condition is too weak and generally does not guarantee that moments remain finite; it can lead to explosion in finite time.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2971245"}]}