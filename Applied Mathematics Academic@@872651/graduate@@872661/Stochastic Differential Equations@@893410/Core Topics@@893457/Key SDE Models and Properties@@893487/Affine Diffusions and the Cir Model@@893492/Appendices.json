{"hands_on_practices": [{"introduction": "A defining feature of affine diffusions, and the CIR model in particular, is their analytical tractability. At the heart of this tractability lies the ability to express the model's transition probability density in a closed form. This practice guides you through the foundational derivation of this density, starting from first principles, by connecting the CIR process to the squared norm of a multi-dimensional Ornstein-Uhlenbeck process. Mastering this derivation [@problem_id:2969020] is essential for understanding why exact simulation and closed-form pricing formulas are possible.", "problem": "Consider the Cox–Ingersoll–Ross (CIR) diffusion $X_t$ defined by the stochastic differential equation\n$$\n\\mathrm{d}X_t \\;=\\; \\kappa\\left(\\theta - X_t\\right)\\mathrm{d}t \\;+\\; \\sigma\\sqrt{X_t}\\,\\mathrm{d}W_t,\\qquad X_0=x0,\n$$\nwhere $\\kappa0$, $\\theta0$, and $\\sigma0$ are constants, and $W_t$ is a standard Brownian motion. Assume the Feller condition $2\\kappa\\theta\\sigma^2$ so that the origin is inaccessible and the state space is $(0,\\infty)$.\n\nStarting from first principles in the theory of affine diffusions and squared Bessel processes, use a transformation and time-change argument to identify the law of $X_t$ as a scaled noncentral chi-square random variable. Then, from the known probability density function (PDF) of the noncentral chi-square law, perform the appropriate change of variables to obtain the transition density $p(t,x,y)$ of $X_t$ conditional on $X_0=x$, for $t0$ and $y0$. Express your final answer in closed form in terms of the modified Bessel function of the first kind $I_{\\alpha}$, and write it as a single analytic expression in the parameters $\\kappa$, $\\theta$, $\\sigma$, $t$, $x$, and $y$.\n\nNo intermediate formulas for the target density are to be assumed; derive the representation from the foundational characterization of the squared Bessel process. Your final answer must be a single closed-form analytic expression. Do not round or approximate.", "solution": "The problem of deriving the transition density of the Cox-Ingersoll-Ross (CIR) process is valid. The problem is scientifically grounded in the theory of stochastic differential equations, well-posed, and objective. All necessary parameters and conditions are provided, and the task is a standard, albeit advanced, derivation in the field.\n\nThe CIR process $X_t$ is described by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_t = \\kappa(\\theta - X_t)\\mathrm{d}t + \\sigma\\sqrt{X_t}\\,\\mathrm{d}W_t, \\qquad X_0=x0\n$$\nwhere $\\kappa$, $\\theta$, and $\\sigma$ are positive constants, and $W_t$ is a standard one-dimensional Brownian motion. The Feller condition $2\\kappa\\theta  \\sigma^2$ is assumed to hold, which ensures that $X_t  0$ for all $t0$. Our goal is to derive the transition probability density function $p(t,x,y) = \\mathbb{P}(X_t \\in \\mathrm{d}y | X_0=x) / \\mathrm{d}y$ for $t0$ and $y0$.\n\nThe derivation proceeds in four main steps, as guided by the problem statement:\n1.  Establish a connection between the CIR process and the squared norm of a multi-dimensional Ornstein-Uhlenbeck (OU) process.\n2.  Determine the probability distribution of this squared norm process by analyzing the distribution of the underlying OU process.\n3.  Identify this distribution as a scaled noncentral chi-square distribution.\n4.  Perform a change of variables on the known probability density function (PDF) of the noncentral chi-square distribution to obtain the desired transition density $p(t,x,y)$.\n\n**Step 1: Connection between CIR and Squared OU Processes**\n\nLet $V_t$ be a $d$-dimensional vector OU process satisfying the SDE:\n$$\n\\mathrm{d}V_t = -\\frac{\\kappa}{2}V_t\\,\\mathrm{d}t + \\frac{\\sigma}{2}\\mathrm{d}W_t\n$$\nwhere $W_t$ is a $d$-dimensional standard Brownian motion with independent components $W_t^i$. Let the initial condition be $V_0 = v_0$.\n\nWe define a new process $Z_t = |V_t|^2 = \\sum_{i=1}^d (V_t^i)^2$. To find the SDE for $Z_t$, we first apply Itô's lemma to a single component squared, $(V_t^i)^2$:\n$$\n\\mathrm{d}(V_t^i)^2 = 2V_t^i\\,\\mathrm{d}V_t^i + \\mathrm{d}\\langle V^i, V^i \\rangle_t\n$$\n$$\n\\mathrm{d}(V_t^i)^2 = 2V_t^i\\left(-\\frac{\\kappa}{2}V_t^i\\,\\mathrm{d}t + \\frac{\\sigma}{2}\\mathrm{d}W_t^i\\right) + \\left(\\frac{\\sigma}{2}\\right)^2\\mathrm{d}t\n$$\n$$\n\\mathrm{d}(V_t^i)^2 = \\left(-\\kappa(V_t^i)^2 + \\frac{\\sigma^2}{4}\\right)\\mathrm{d}t + \\sigma V_t^i\\,\\mathrm{d}W_t^i\n$$\nSumming over all $d$ components:\n$$\n\\mathrm{d}Z_t = \\sum_{i=1}^d \\mathrm{d}(V_t^i)^2 = \\sum_{i=1}^d \\left(-\\kappa(V_t^i)^2 + \\frac{\\sigma^2}{4}\\right)\\mathrm{d}t + \\sigma\\sum_{i=1}^d V_t^i\\,\\mathrm{d}W_t^i\n$$\n$$\n\\mathrm{d}Z_t = \\left(-\\kappa \\sum_{i=1}^d (V_t^i)^2 + d\\frac{\\sigma^2}{4}\\right)\\mathrm{d}t + \\sigma\\sum_{i=1}^d V_t^i\\,\\mathrm{d}W_t^i\n$$\n$$\n\\mathrm{d}Z_t = \\left(d\\frac{\\sigma^2}{4} - \\kappa Z_t\\right)\\mathrm{d}t + \\sigma\\sum_{i=1}^d V_t^i\\,\\mathrm{d}W_t^i\n$$\nThe quadratic variation of the martingale term is:\n$$\n\\left\\langle \\sigma\\sum_{i=1}^d V_t^i\\,\\mathrm{d}W_t^i \\right\\rangle_t = \\sigma^2 \\sum_{i,j=1}^d V_t^i V_t^j \\mathrm{d}\\langle W^i, W^j \\rangle_t = \\sigma^2 \\sum_{i=1}^d (V_t^i)^2\\,\\mathrm{d}t = \\sigma^2 Z_t\\,\\mathrm{d}t\n$$\nBy Lévy's characterization theorem for Brownian motion, the term $\\sigma\\sum_{i=1}^d V_t^i\\,\\mathrm{d}W_t^i$ can be represented as $\\sigma\\sqrt{Z_t}\\,\\mathrm{d}\\tilde{W}_t$ for some new one-dimensional standard Brownian motion $\\tilde{W}_t$.\nThus, the SDE for $Z_t$ is:\n$$\n\\mathrm{d}Z_t = \\left(d\\frac{\\sigma^2}{4} - \\kappa Z_t\\right)\\mathrm{d}t + \\sigma\\sqrt{Z_t}\\,\\mathrm{d}\\tilde{W}_t\n$$\nThis SDE matches the form of the CIR SDE $\\mathrm{d}X_t = \\kappa(\\theta - X_t)\\mathrm{d}t + \\sigma\\sqrt{X_t}\\,\\mathrm{d}W_t$ if we equate the drift coefficients:\n$$\nd\\frac{\\sigma^2}{4} = \\kappa\\theta \\implies d = \\frac{4\\kappa\\theta}{\\sigma^2}\n$$\nThis establishes that the CIR process $X_t$ has the same law as the squared norm of a $d$-dimensional OU process, where the dimension $d$ is determined by the CIR parameters. The initial condition $X_0=x$ corresponds to $Z_0=x$, so we set $|v_0|^2 = x$. Note that $d$ is not restricted to be an integer. The Feller condition $2\\kappa\\theta  \\sigma^2$ is equivalent to $d2$.\n\n**Step 2: Distribution of the OU Process**\n\nThe solution to the $d$-dimensional OU process SDE is:\n$$\nV_t = V_0 \\exp\\left(-\\frac{\\kappa}{2}t\\right) + \\frac{\\sigma}{2}\\int_0^t \\exp\\left(-\\frac{\\kappa}{2}(t-s)\\right)\\mathrm{d}W_s\n$$\nSince $V_0=v_0$ is deterministic and the integral is a linear transformation of a Gaussian process, $V_t$ is a multivariate normally distributed random vector.\nThe mean vector is:\n$$\n\\mathbb{E}[V_t] = v_0 \\exp\\left(-\\frac{\\kappa}{2}t\\right)\n$$\nThe covariance matrix is $C_t = \\mathbb{E}[(V_t - \\mathbb{E}[V_t])(V_t - \\mathbb{E}[V_t])^T]$. Its components are:\n$$\n(C_t)_{ij} = \\mathbb{E}\\left[\\left(\\frac{\\sigma}{2}\\int_0^t \\exp\\left(-\\frac{\\kappa}{2}(t-s)\\right)\\mathrm{d}W_s^i\\right)\\left(\\frac{\\sigma}{2}\\int_0^t \\exp\\left(-\\frac{\\kappa}{2}(t-u)\\right)\\mathrm{d}W_u^j\\right)\\right]\n$$\nBy the Itô isometry, this is non-zero only for $i=j$:\n$$\n(C_t)_{ii} = \\frac{\\sigma^2}{4}\\int_0^t \\exp(-\\kappa(t-s))\\,\\mathrm{d}s = \\frac{\\sigma^2}{4}\\left[\\frac{\\exp(-\\kappa(t-s))}{\\kappa}\\right]_0^t = \\frac{\\sigma^2}{4\\kappa}(1 - \\exp(-\\kappa t))\n$$\nSo, the covariance matrix is $C_t = \\Sigma^2(t) I_d$, where $I_d$ is the $d \\times d$ identity matrix and the variance of each component is $\\Sigma^2(t) = \\frac{\\sigma^2}{4\\kappa}(1 - \\exp(-\\kappa t))$.\nTherefore, $V_t \\sim \\mathcal{N}\\left(v_0 \\exp(-\\frac{\\kappa}{2}t), \\Sigma^2(t)I_d\\right)$.\n\n**Step 3: Identification with a Scaled Noncentral Chi-Square Distribution**\n\nA noncentral chi-square random variable with $d$ degrees of freedom and non-centrality parameter $\\lambda$, denoted $\\chi'^2(d, \\lambda)$, is defined as the sum of squares of $d$ independent normal random variables, each with variance $1$ and with means $\\mu_i$ such that $\\sum_{i=1}^d \\mu_i^2 = \\lambda$.\n\nConsider the standardized random vector $Z_t = V_t/\\Sigma(t)$. It is normally distributed with mean $\\mathbb{E}[Z_t] = \\frac{v_0}{\\Sigma(t)}\\exp(-\\frac{\\kappa}{2} t)$ and covariance matrix $I_d$.\nThe sum of squares $|Z_t|^2 = |V_t|^2/\\Sigma^2(t) = X_t/\\Sigma^2(t)$ follows a noncentral chi-square distribution with:\n-   Degrees of freedom: $d = \\frac{4\\kappa\\theta}{\\sigma^2}$.\n-   Non-centrality parameter $\\lambda_t$:\n    $$\n    \\lambda_t = |\\mathbb{E}[Z_t]|^2 = \\frac{|v_0|^2}{\\Sigma^2(t)}\\exp(-\\kappa t) = \\frac{x \\exp(-\\kappa t)}{\\frac{\\sigma^2}{4\\kappa}(1-\\exp(-\\kappa t))} = \\frac{4\\kappa x \\exp(-\\kappa t)}{\\sigma^2(1-\\exp(-\\kappa t))}\n    $$\nThus, the random variable $X_t$ is distributed as a scaling factor times a noncentral chi-square variable:\n$$\nX_t \\sim \\Sigma^2(t) \\cdot \\chi'^2(d, \\lambda_t) = \\frac{\\sigma^2(1-\\exp(-\\kappa t))}{4\\kappa} \\cdot \\chi'^2\\left(\\frac{4\\kappa\\theta}{\\sigma^2}, \\frac{4\\kappa x \\exp(-\\kappa t)}{\\sigma^2(1-\\exp(-\\kappa t))}\\right)\n$$\n\n**Step 4: Derivation of the Transition Density**\n\nLet $Y \\sim \\chi'^2(d, \\lambda)$. Its PDF is given by:\n$$\nf_Y(y; d, \\lambda) = \\frac{1}{2}\\exp\\left(-\\frac{y+\\lambda}{2}\\right)\\left(\\frac{y}{\\lambda}\\right)^{\\frac{d-2}{4}} I_{\\frac{d}{2}-1}(\\sqrt{\\lambda y})\n$$\nwhere $I_\\alpha$ is the modified Bessel function of the first kind of order $\\alpha$.\n\nWe have the relationship $X_t = \\Sigma^2(t) Y$, so $Y = X_t / \\Sigma^2(t)$. Let $p(t,x,y_{val})$ denote the PDF of $X_t$ at value $y_{val}$. By the change of variables formula for PDFs:\n$$\np(t,x,y_{val}) = f_Y\\left(\\frac{y_{val}}{\\Sigma^2(t)}; d, \\lambda_t\\right) \\cdot \\left|\\frac{\\mathrm{d}Y}{\\mathrm{d}X_t}\\right| = f_Y\\left(\\frac{y_{val}}{\\Sigma^2(t)}; d, \\lambda_t\\right) \\cdot \\frac{1}{\\Sigma^2(t)}\n$$\nLet us define the following quantities for convenience:\n$$\nc_t = \\frac{4\\kappa}{\\sigma^2(1-\\exp(-\\kappa t))} = \\frac{1}{\\Sigma^2(t)}\n$$\n$$\nq = \\frac{d}{2}-1 = \\frac{2\\kappa\\theta}{\\sigma^2}-1\n$$\nNote that $\\lambda_t = c_t x \\exp(-\\kappa t)$.\nSubstituting into the formula for $p(t,x,y)$ (using $y$ for $y_{val}$):\n$$\np(t,x,y) = c_t \\cdot \\frac{1}{2}\\exp\\left(-\\frac{c_t y + \\lambda_t}{2}\\right)\\left(\\frac{c_t y}{\\lambda_t}\\right)^{\\frac{q}{2}} I_q\\left(\\sqrt{\\lambda_t c_t y}\\right)\n$$\nSubstitute $\\lambda_t = c_t x \\exp(-\\kappa t)$:\n$$\np(t,x,y) = \\frac{c_t}{2}\\exp\\left(-\\frac{c_t(y + x\\exp(-\\kappa t))}{2}\\right)\\left(\\frac{c_t y}{c_t x\\exp(-\\kappa t)}\\right)^{\\frac{q}{2}} I_q\\left(\\sqrt{c_t x\\exp(-\\kappa t) \\cdot c_t y}\\right)\n$$\nSimplifying the terms:\n-   Exponent: $-\\frac{c_t}{2}(y + x\\exp(-\\kappa t))$\n-   Ratio term: $\\left(\\frac{y}{x\\exp(-\\kappa t)}\\right)^{q/2}$\n-   Bessel function argument: $c_t \\sqrt{xy\\exp(-\\kappa t)}$\n\nSubstituting the full expressions for $c_t$ and $q$ gives the final transition density:\n$p(t,x,y) = \\frac{2\\kappa}{\\sigma^2(1-\\exp(-\\kappa t))} \\exp\\left(-\\frac{2\\kappa(y+x\\exp(-\\kappa t))}{\\sigma^2(1-\\exp(-\\kappa t))}\\right) \\left(\\frac{y}{x\\exp(-\\kappa t)}\\right)^{\\frac{1}{2}(\\frac{2\\kappa\\theta}{\\sigma^2}-1)} I_{\\frac{2\\kappa\\theta}{\\sigma^2}-1}\\left(\\frac{4\\kappa\\sqrt{xy\\exp(-\\kappa t)}}{\\sigma^2(1-\\exp(-\\kappa t))}\\right)$\n\nThis expression is the transition density of the CIR process.", "answer": "$$\n\\boxed{\\frac{2\\kappa}{\\sigma^2(1-\\exp(-\\kappa t))} \\exp\\left(-\\frac{2\\kappa(y+x\\exp(-\\kappa t))}{\\sigma^2(1-\\exp(-\\kappa t))}\\right) \\left(\\frac{y}{x\\exp(-\\kappa t)}\\right)^{\\frac{1}{2}\\left(\\frac{2\\kappa\\theta}{\\sigma^2}-1\\right)} I_{\\frac{2\\kappa\\theta}{\\sigma^2}-1}\\left(\\frac{4\\kappa\\sqrt{xy\\exp(-\\kappa t)}}{\\sigma^2(1-\\exp(-\\kappa t))}\\right)}\n$$", "id": "2969020"}, {"introduction": "With the exact transition law in hand, we can perform highly accurate Monte Carlo simulations without resorting to potentially unstable approximation schemes like Euler-Maruyama. This exercise applies this knowledge to a practical and insightful task: analyzing the ergodic properties of the CIR and Vasicek models. You will implement an exact sampler to simulate paths and empirically measure the rate of convergence to the theoretical stationary distribution [@problem_id:2429591], a key property for long-run financial and economic modeling.", "problem": "You are given two continuous-time short-rate models used in computational economics and finance, each defined as a one-dimensional Itô diffusion under the risk-neutral measure. The models are specified by their stochastic differential equations (SDEs), which are to be treated as model definitions.\n\nModel A (Vasicek, also called the Ornstein–Uhlenbeck short-rate model):\n$$\ndr_t = \\kappa \\left(\\theta - r_t\\right) \\, dt + \\sigma \\, dW_t.\n$$\n\nModel B (Cox–Ingersoll–Ross):\n$$\ndr_t = \\kappa \\left(\\theta - r_t\\right) \\, dt + \\sigma \\sqrt{r_t} \\, dW_t, \\quad r_t \\ge 0.\n$$\n\nHere, $r_t$ is the short rate at time $t$, $\\kappa  0$ is the mean-reversion speed, $\\theta$ is the long-run mean level, $\\sigma  0$ is the volatility parameter, and $W_t$ is a standard Brownian motion.\n\nTask:\n1) Using the stationary (time-independent) Kolmogorov forward (Fokker–Planck) equation for a one-dimensional Itô diffusion with drift $a(r)$ and diffusion coefficient $b(r)$, derive from first principles the stationary probability density function $p(r)$ for each model, including its natural support. Your derivations must start from the stationary forward equation and boundary conditions for zero probability current at natural boundaries. Express $p(r)$ in fully normalized form and also provide the corresponding cumulative distribution function. Do not assume any shortcut formulas for stationary densities; instead, derive them explicitly from the model SDEs.\n\n2) Design a Monte Carlo procedure that simulates each model and empirically measures how quickly the simulated marginal distribution of $r_t$ converges to the stationary distribution $p(r)$, starting from different initial conditions $r_0$. Your simulation must:\n- Use exact single-step transition sampling for both models at a fixed monitoring grid with uniform spacing $\\Delta t$, not an approximate Euler scheme.\n- For Model A (Vasicek), use the exact Gaussian transition implied by the linear SDE.\n- For Model B (Cox–Ingersoll–Ross), use the exact noncentral chi-square transition for $r_{t+\\Delta t}$ conditional on $r_t$.\n- At each monitoring time $t_k = k \\Delta t$, compute the Kolmogorov–Smirnov statistic $D_k$ between the empirical distribution of the simulated ensemble and the theoretical stationary distribution $p(r)$ derived in part $1$. The Kolmogorov–Smirnov statistic for a continuous target cumulative distribution function $F(r)$ and an empirical sample $\\{x_i\\}_{i=1}^n$ is\n$$\nD = \\sup_{r \\in \\mathbb{R}} \\left| F(r) - \\hat{F}_n(r) \\right|,\n$$\nwhere $\\hat{F}_n$ is the empirical cumulative distribution function of the sample.\n- Define the estimated mixing time as the smallest monitoring time $t_k$ such that $D_k \\le \\varepsilon$. If this condition is never met up to a specified horizon $T_{\\max}$, return $T_{\\max}$.\n\n3) Implement a complete, runnable program that:\n- Derives and uses the stationary distributions from part $1$.\n- Simulates independent paths under both models using exact single-step transitions as specified in part $2$.\n- Computes the mixing time for each test case below using the Kolmogorov–Smirnov statistic with threshold $\\varepsilon$.\n- Outputs the mixing times on a single line in the specified format.\n\nSimulation and reporting requirements:\n- Use $n = 10000$ independent paths for each test case.\n- Use a fixed monitoring step $\\Delta t = 0.05$ years and a maximum horizon $T_{\\max} = 10.0$ years.\n- Use a Kolmogorov–Smirnov threshold $\\varepsilon = 0.02$ (as a decimal, not a percentage).\n- Fix the random number generator seed to $12345$ for reproducibility.\n- Report every mixing time in years, rounded to $3$ decimals.\n\nTest suite (six cases):\n- Case $1$ (Vasicek): $\\kappa = 1.0$, $\\theta = 0.05$, $\\sigma = 0.02$, $r_0 = 0.00$.\n- Case $2$ (Vasicek): $\\kappa = 1.0$, $\\theta = 0.05$, $\\sigma = 0.02$, $r_0 = 0.20$.\n- Case $3$ (Vasicek): $\\kappa = 1.0$, $\\theta = 0.05$, $\\sigma = 0.02$, $r_0 = -0.05$.\n- Case $4$ (Cox–Ingersoll–Ross): $\\kappa = 1.2$, $\\theta = 0.04$, $\\sigma = 0.25$, $r_0 = 0.001$.\n- Case $5$ (Cox–Ingersoll–Ross): $\\kappa = 1.2$, $\\theta = 0.04$, $\\sigma = 0.25$, $r_0 = 0.04$.\n- Case $6$ (Cox–Ingersoll–Ross): $\\kappa = 1.2$, $\\theta = 0.04$, $\\sigma = 0.25$, $r_0 = 0.20$.\n\nFinal output format:\n- Your program should produce a single line containing a Python-style list of $6$ floats representing the mixing times for Cases $1$ through $6$, in order, rounded to $3$ decimals and expressed in years, for example:\n\"[t1,t2,t3,t4,t5,t6]\".\n\nNo user input should be read; all constants and test cases are fixed as above and must be embedded in the program. The program must rely only on standard libraries plus numerical libraries specified in the execution environment.", "solution": "The problem presented is a standard exercise in computational finance concerning the analysis of short-rate models. It is scientifically grounded, well-posed, and complete. I will proceed with the solution, which consists of three parts as requested: analytical derivation of stationary distributions, design of a Monte Carlo simulation for convergence analysis, and implementation of the complete procedure.\n\n**Part 1: Derivation of Stationary Distributions**\n\nA general one-dimensional Itô diffusion is described by the stochastic differential equation (SDE):\n$$\ndr_t = a(r_t) \\, dt + b(r_t) \\, dW_t\n$$\nwhere $a(r)$ is the drift function and $b(r)$ is the diffusion function. The time-independent or stationary probability density function (PDF), denoted by $p(r)$, must satisfy the stationary Kolmogorov forward (Fokker-Planck) equation. This equation expresses the conservation of probability. Integrating the full Fokker-Planck equation with respect to time and assuming a stationary state ($\\partial p / \\partial t = 0$) and zero probability current at the boundaries of the state space leads to the following ordinary differential equation:\n$$\na(r) p(r) - \\frac{1}{2} \\frac{d}{dr} \\left[ b(r)^2 p(r) \\right] = 0\n$$\nThis equation will be solved for each model to find its stationary PDF.\n\n**Model A: Vasicek**\nThe SDE is $dr_t = \\kappa (\\theta - r_t) \\, dt + \\sigma \\, dW_t$.\nThe drift and diffusion terms are:\n$$\na(r) = \\kappa (\\theta - r)\n$$\n$$\nb(r)^2 = \\sigma^2\n$$\nThe state space for the Vasicek model is $(-\\infty, \\infty)$. Substituting these into the stationary equation gives:\n$$\n\\kappa(\\theta - r) p(r) - \\frac{1}{2} \\frac{d}{dr} \\left[ \\sigma^2 p(r) \\right] = 0\n$$\n$$\n\\kappa(\\theta - r) p(r) = \\frac{\\sigma^2}{2} \\frac{dp(r)}{dr}\n$$\nThis is a first-order linear separable differential equation. We can separate variables:\n$$\n\\frac{dp}{p} = \\frac{2\\kappa}{\\sigma^2} (\\theta - r) \\, dr\n$$\nIntegrating both sides:\n$$\n\\int \\frac{dp}{p} = \\int \\frac{2\\kappa}{\\sigma^2} (\\theta - r) \\, dr \\implies \\ln p(r) = \\frac{2\\kappa}{\\sigma^2} \\left(\\theta r - \\frac{r^2}{2}\\right) + C_0\n$$\nwhere $C_0$ is the constant of integration. The PDF is thus:\n$$\np(r) = C \\exp\\left( \\frac{2\\kappa\\theta r - \\kappa r^2}{\\sigma^2} \\right)\n$$\nTo identify this distribution, we complete the square in the exponent:\n$$\n-\\frac{\\kappa}{\\sigma^2} (r^2 - 2\\theta r) = -\\frac{\\kappa}{\\sigma^2} \\left[ (r - \\theta)^2 - \\theta^2 \\right] = -\\frac{\\kappa(r - \\theta)^2}{\\sigma^2} + \\frac{\\kappa\\theta^2}{\\sigma^2}\n$$\nThe term $\\exp(\\kappa\\theta^2/\\sigma^2)$ is a constant that can be absorbed into the normalization constant $C$. The functional form is that of a Gaussian distribution:\n$$\np(r) = C' \\exp\\left( -\\frac{(r - \\theta)^2}{\\sigma^2 / \\kappa} \\right)\n$$\nBy comparing this to the standard form of a Normal PDF, $f(x) = (\\sqrt{2\\pi \\text{Var}})^{-1} \\exp(-(x-\\mu)^2/(2\\text{Var}))$, we identify the mean $\\mu = \\theta$ and variance $\\text{Var} = \\sigma^2 / (2\\kappa)$.\nThe fully normalized stationary PDF for the Vasicek model is:\n$$\np(r) = \\sqrt{\\frac{\\kappa}{\\pi\\sigma^2}} \\exp\\left( -\\frac{\\kappa(r-\\theta)^2}{\\sigma^2} \\right)\n$$\nThis is the PDF of a Normal distribution $\\mathcal{N}(\\theta, \\sigma^2/(2\\kappa))$. The corresponding cumulative distribution function (CDF) is:\n$$\nF(r) = \\Phi\\left( \\frac{r - \\theta}{\\sqrt{\\sigma^2/(2\\kappa)}} \\right)\n$$\nwhere $\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z e^{-x^2/2} dx$ is the standard Normal CDF.\n\n**Model B: Cox-Ingersoll-Ross (CIR)**\nThe SDE is $dr_t = \\kappa (\\theta - r_t) \\, dt + \\sigma \\sqrt{r_t} \\, dW_t$, for $r_t \\ge 0$.\nThe drift and squared diffusion terms are:\n$$\na(r) = \\kappa (\\theta - r)\n$$\n$$\nb(r)^2 = \\sigma^2 r\n$$\nThe state space is $[0, \\infty)$. The Feller condition $2\\kappa\\theta  \\sigma^2$ ensures that the origin $r=0$ is an inaccessible boundary. This condition is met by the problem parameters. The stationary Fokker-Planck equation is:\n$$\n\\kappa(\\theta - r) p(r) - \\frac{1}{2} \\frac{d}{dr} \\left[ \\sigma^2 r p(r) \\right] = 0\n$$\nExpanding the derivative and rearranging gives:\n$$\n\\frac{\\sigma^2}{2} r \\frac{dp(r)}{dr} = \\left[ \\kappa(\\theta - r) - \\frac{\\sigma^2}{2} \\right] p(r)\n$$\nSeparating variables:\n$$\n\\frac{dp}{p} = \\left[ \\frac{\\kappa\\theta - \\sigma^2/2}{(\\sigma^2/2)r} - \\frac{\\kappa r}{(\\sigma^2/2)r} \\right] \\, dr = \\left( \\frac{2\\kappa\\theta/\\sigma^2 - 1}{r} - \\frac{2\\kappa}{\\sigma^2} \\right) \\, dr\n$$\nIntegrating both sides:\n$$\n\\ln p(r) = \\left(\\frac{2\\kappa\\theta}{\\sigma^2} - 1\\right) \\ln r - \\frac{2\\kappa}{\\sigma^2} r + C_0\n$$\nThe PDF is therefore:\n$$\np(r) = C \\cdot r^{\\frac{2\\kappa\\theta}{\\sigma^2} - 1} \\exp\\left( -\\frac{2\\kappa}{\\sigma^2} r \\right)\n$$\nThis is the functional form of a Gamma distribution. The standard PDF for a Gamma distribution with shape $\\alpha$ and rate $\\beta$ is $f(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}$. By comparison, we identify the parameters:\n$$\n\\text{Shape: } \\alpha = \\frac{2\\kappa\\theta}{\\sigma^2}\n$$\n$$\n\\text{Rate: } \\beta = \\frac{2\\kappa}{\\sigma^2}\n$$\nThe normalized stationary PDF for the CIR model is:\n$$\np(r) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} r^{\\alpha - 1} e^{-\\beta r}\n$$\nThis is the PDF of a Gamma distribution $\\text{Gamma}(\\alpha, \\beta)$. The corresponding CDF is given by the regularized lower incomplete gamma function:\n$$\nF(r) = \\frac{\\gamma(\\alpha, \\beta r)}{\\Gamma(\\alpha)}\n$$\nwhere $\\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt$ is the lower incomplete gamma function.\n\n**Part 2: Simulation Methodology**\n\nThe convergence to the stationary distribution is measured using a Monte Carlo simulation. An ensemble of $n = 10000$ independent paths is simulated for each model starting from a given initial condition $r_0$. Instead of an approximate numerical scheme like Euler-Maruyama, we use the exact single-step transition distributions.\n\nAt each monitoring time $t_k = k \\Delta t$, the empirical distribution of the simulated rates $\\{r_{t_k}^{(i)}\\}_{i=1}^n$ is compared to the theoretical stationary distribution derived in Part 1. The distance between the distributions is quantified by the Kolmogorov-Smirnov (KS) statistic, $D_k$:\n$$\nD_k = \\sup_{r} | F(r) - \\hat{F}_{n,k}(r) |\n$$\nwhere $F(r)$ is the theoretical stationary CDF and $\\hat{F}_{n,k}(r)$ is the empirical CDF of the sample at time $t_k$. The estimated mixing time is the first time $t_k$ at which $D_k \\le \\varepsilon$ for a given threshold $\\varepsilon = 0.02$.\n\n**Exact Transition for Vasicek:**\nThe Vasicek SDE is a linear SDE (an Ornstein-Uhlenbeck process). Its exact solution implies that the distribution of $r_{t+\\Delta t}$ conditional on $r_t$ is Normal:\n$$\nr_{t+\\Delta t} | r_t \\sim \\mathcal{N}\\left( \\theta + (r_t - \\theta)e^{-\\kappa \\Delta t}, \\frac{\\sigma^2}{2\\kappa}(1 - e^{-2\\kappa \\Delta t}) \\right)\n$$\nTo simulate one step, we draw from this Normal distribution.\n\n**Exact Transition for Cox-Ingersoll-Ross:**\nThe distribution of $r_{t+\\Delta t}$ conditional on $r_t$ follows a scaled noncentral chi-square distribution. Specifically, $2c \\cdot r_{t+\\Delta t}$ follows a noncentral chi-square distribution $\\chi'^2(d, \\lambda)$ with:\n- Scaling factor: $c = \\frac{2\\kappa}{\\sigma^2(1 - e^{-\\kappa \\Delta t})}$\n- Degrees of freedom: $d = \\frac{4\\kappa\\theta}{\\sigma^2}$\n- Non-centrality parameter: $\\lambda = 2c \\cdot r_t e^{-\\kappa \\Delta t}$\nTherefore, $r_{t+\\Delta t} = \\frac{1}{2c} Y$, where $Y \\sim \\chi'^2(d, \\lambda)$. A sample from this distribution is generated for each path at each time step.\n\n**Part 3: Implementation Strategy**\n\nThe algorithm is implemented in Python using the `numpy` library for efficient vectorized computations and the `scipy` library for statistical functions.\n1.  Initialize a random number generator with a fixed seed for reproducibility.\n2.  For each test case, define model parameters $(\\kappa, \\theta, \\sigma, r_0)$ and the corresponding stationary CDF, $F(r)$.\n3.  Initialize an array of $n=10000$ paths with the value $r_0$.\n4.  Iterate through time steps $t_k = k \\Delta t$ up to $T_{\\max} = 10.0$.\n5.  In each step, update all paths simultaneously by drawing from the exact transition distributions using vectorized operations. For Vasicek, this involves `numpy.random.Generator.standard_normal`. For CIR, we use `scipy.stats.ncx2.rvs`.\n6.  At the end of each step, compute the KS statistic $D_k$. This involves sorting the simulated rates, evaluating the stationary CDF at these points, and finding the maximum deviation from the empirical CDF.\n7.  Compare $D_k$ with the threshold $\\varepsilon = 0.02$. If $D_k \\le \\varepsilon$, the mixing time is recorded as $t_k$ and the simulation for that case terminates.\n8.  If the condition is not met by $T_{\\max}$, the mixing time is reported as $T_{\\max}$.\n9.  The final results are collected and printed in the specified format. The use of vectorized operations over all paths is critical for computational efficiency.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, gamma, ncx2\n\ndef solve():\n    \"\"\"\n    Computes the mixing time for Vasicek and CIR short-rate models\n    based on convergence of the empirical distribution to the stationary one.\n    \"\"\"\n    # Define simulation and problem constants\n    N_PATHS = 10000\n    DT = 0.05\n    T_MAX = 10.0\n    EPSILON = 0.02\n    SEED = 12345\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'model': 'vasicek', 'kappa': 1.0, 'theta': 0.05, 'sigma': 0.02, 'r0': 0.00},\n        {'model': 'vasicek', 'kappa': 1.0, 'theta': 0.05, 'sigma': 0.02, 'r0': 0.20},\n        {'model': 'vasicek', 'kappa': 1.0, 'theta': 0.05, 'sigma': 0.02, 'r0': -0.05},\n        {'model': 'cir', 'kappa': 1.2, 'theta': 0.04, 'sigma': 0.25, 'r0': 0.001},\n        {'model': 'cir', 'kappa': 1.2, 'theta': 0.04, 'sigma': 0.25, 'r0': 0.04},\n        {'model': 'cir', 'kappa': 1.2, 'theta': 0.04, 'sigma': 0.25, 'r0': 0.20},\n    ]\n\n    rng = np.random.default_rng(SEED)\n    results = []\n\n    for case in test_cases:\n        kappa = case['kappa']\n        theta = case['theta']\n        sigma = case['sigma']\n        r0 = case['r0']\n\n        # Initialize paths\n        r_paths = np.full(N_PATHS, r0, dtype=np.float64)\n\n        if case['model'] == 'vasicek':\n            # Stationary distribution: Normal(mean, var)\n            stat_mean = theta\n            stat_var = sigma**2 / (2 * kappa)\n            stat_std = np.sqrt(stat_var)\n            stat_cdf = lambda x: norm.cdf(x, loc=stat_mean, scale=stat_std)\n\n            # Precompute transition parameters for exact sampling\n            trans_exp = np.exp(-kappa * DT)\n            trans_mean_term = theta * (1 - trans_exp)\n            trans_std = np.sqrt(sigma**2 / (2 * kappa) * (1 - trans_exp**2))\n\n        elif case['model'] == 'cir':\n            # Stationary distribution: Gamma(shape, scale)\n            # Feller condition 2*kappa*theta > sigma**2 must hold, and does for the test case.\n            alpha_stat = 2 * kappa * theta / sigma**2\n            scale_stat = sigma**2 / (2 * kappa) # scale = 1/rate\n            stat_cdf = lambda x: gamma.cdf(x, a=alpha_stat, scale=scale_stat)\n\n            # Precompute transition parameters for exact sampling (non-central chi-square)\n            c_trans = 2 * kappa / (sigma**2 * (1 - np.exp(-kappa * DT)))\n            df_trans = 4 * kappa * theta / sigma**2\n            exp_term_trans = np.exp(-kappa * DT)\n\n        mixing_time = T_MAX\n        time_grid = np.arange(DT, T_MAX + DT / 2, DT)\n\n        for t in time_grid:\n            # Simulate one step for all paths using the exact transition law\n            if case['model'] == 'vasicek':\n                mean_t = r_paths * trans_exp + trans_mean_term\n                z = rng.standard_normal(N_PATHS)\n                r_paths = mean_t + trans_std * z\n            else: # CIR\n                # Ensure non-negativity for the non-centrality parameter calculation\n                r_non_neg = np.maximum(r_paths, 0)\n                nc_param = 2 * c_trans * r_non_neg * exp_term_trans\n                chi2_samples = ncx2.rvs(df=df_trans, nc=nc_param, size=N_PATHS, random_state=rng)\n                r_paths = chi2_samples / (2 * c_trans)\n            \n            # Compute Kolmogorov-Smirnov statistic\n            r_sorted = np.sort(r_paths)\n            cdf_values = stat_cdf(r_sorted)\n            i_vals = np.arange(1, N_PATHS + 1) / N_PATHS\n            \n            d_plus = np.max(i_vals - cdf_values)\n            d_minus = np.max(cdf_values - (i_vals - 1/N_PATHS))\n            d_k = max(d_plus, d_minus)\n\n            if d_k = EPSILON:\n                mixing_time = t\n                break\n        \n        results.append(round(mixing_time, 3))\n\n    # Final print statement in the exact required format.\n    # The output format for floats is handled by rounding, map to str will preserve it.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2429591"}, {"introduction": "While simulating a model with known parameters is crucial, the inverse problem—estimating parameters from observed data—is paramount for any real-world application. This practice delves into the statistical inference of the CIR model using the Generalized Method of Moments (GMM). You will first derive the model's theoretical moment structure, including its mean, variance, and autocovariance, and then use these to construct a robust parameter estimator [@problem_id:2968996]. This exercise bridges the gap between the stochastic theory of the CIR process and its econometric application.", "problem": "Consider the Cox–Ingersoll–Ross (CIR) square-root diffusion, an example of an affine diffusion, defined by the stochastic differential equation\n$$\n\\mathrm{d}X_{t} \\;=\\; \\kappa \\big( \\theta - X_{t} \\big)\\,\\mathrm{d}t \\;+\\; \\sigma \\sqrt{X_{t}}\\,\\mathrm{d}W_{t},\n$$\nwhere $X_{t} \\ge 0$, $\\kappa  0$, $\\theta  0$, $\\sigma  0$, $\\{W_{t}\\}_{t \\ge 0}$ is a standard Brownian motion, and the Feller condition $2\\kappa\\theta \\ge \\sigma^{2}$ holds to ensure strict positivity. Suppose $X_{t}$ is observed at equally spaced times $t_{i} = i\\Delta$ for $i=0,1,\\dots,n$, with fixed sampling interval $\\Delta  0$, and that the process is strictly stationary and ergodic under the given parameters.\n\nYour tasks are:\n\n1. Starting from the stochastic differential equation and first principles (Itô’s formula and conditional expectation for Markov processes), derive the stationary first moment $\\mathbb{E}[X_{t}]$, the stationary variance $\\mathrm{Var}(X_{t})$, and the stationary autocovariance $\\mathrm{Cov}(X_{t}, X_{t+\\ell\\Delta})$ for any positive integer lag $\\ell$.\n\n2. Using the derived moment identities, design a method-of-moments estimator for the parameter vector $(\\kappa, \\theta, \\sigma)$ that leverages the stationary mean $\\mathbb{E}[X_{t}]$, the stationary variance $\\mathrm{Var}(X_{t})$, and multiple-lag autocovariances $\\mathrm{Cov}(X_{t}, X_{t+\\ell\\Delta})$ for $\\ell=1,2,\\dots,L$, where $L \\in \\mathbb{N}$ is fixed. Define the sample mean\n$$\n\\widehat{m}_{n} \\;=\\; \\frac{1}{n+1}\\sum_{i=0}^{n} X_{i\\Delta},\n$$\nthe centered sample variance\n$$\n\\widehat{v}_{n} \\;=\\; \\frac{1}{n+1}\\sum_{i=0}^{n} \\big(X_{i\\Delta} - \\widehat{m}_{n}\\big)^{2},\n$$\nand the lag-$\\ell$ centered sample autocovariance\n$$\n\\widehat{\\gamma}_{\\ell,n} \\;=\\; \\frac{1}{n-\\ell+1}\\sum_{i=0}^{n-\\ell} \\big(X_{(i+\\ell)\\Delta} - \\widehat{m}_{n}\\big)\\big(X_{i\\Delta} - \\widehat{m}_{n}\\big),\n$$\nfor $\\ell=1,2,\\dots,L$. Assume that $\\widehat{\\gamma}_{\\ell,n}  0$ for all $\\ell$ used. Construct explicit, closed-form estimators $(\\widehat{\\kappa}, \\widehat{\\theta}, \\widehat{\\sigma})$ in terms of $\\Delta$, $\\widehat{m}_{n}$, $\\widehat{v}_{n}$, and $\\{\\widehat{\\gamma}_{\\ell,n}\\}_{\\ell=1}^{L}$.\n\n3. Discuss identifiability of $(\\kappa, \\theta, \\sigma)$ from the chosen moments and justify why your estimator uniquely maps observed moments to parameters under the stated conditions.\n\n4. Within the Generalized Method of Moments (GMM) framework, define the moment function $g(\\beta)$ with $\\beta = (\\kappa, \\theta, \\sigma)^{\\top}$ that stacks the mean, variance, and $L$ lagged autocovariance-based conditions you employed. Let $D(\\beta)$ denote the Jacobian matrix of the corresponding theoretical moment map with respect to $\\beta$, and let $\\Sigma(\\beta)$ denote the long-run covariance matrix of the stacked sample moment vector. Let $I(\\beta;\\Delta)$ denote the per-observation Fisher information matrix for the discretely observed CIR model at sampling interval $\\Delta$ based on the exact transition density. Derive an analytic expression for the asymptotic relative efficiency, for the $\\kappa$ component, of your method-of-moments estimator relative to Maximum Likelihood Estimation (MLE), defined as the ratio of the asymptotic variance of the MLE to that of your GMM estimator under the optimal weighting $\\Sigma(\\beta)^{-1}$.\n\nProvide your final answer as a single closed-form analytic expression collecting $(\\widehat{\\kappa}, \\widehat{\\theta}, \\widehat{\\sigma})$ and the $\\kappa$-component asymptotic relative efficiency in one row matrix. No rounding is required.", "solution": "The problem is valid. It is a well-posed and scientifically grounded problem in stochastic calculus and statistical inference for diffusion processes. The tasks involve standard derivations and theoretical analysis of the Cox-Ingersoll-Ross (CIR) model, a cornerstone of financial mathematics. While part 4 is highly advanced and requires care in interpreting the required level of detail, it remains a feasible question about the formal structure of GMM and MLE theory.\n\n### Part 1: Stationary Moments of the CIR Process\n\nThe CIR process is described by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} \\;=\\; \\kappa \\big( \\theta - X_{t} \\big)\\,\\mathrm{d}t \\;+\\; \\sigma \\sqrt{X_{t}}\\,\\mathrm{d}W_{t}\n$$\nWe are given that the process is strictly stationary, which implies that its moments are constant over time.\n\n**Stationary First Moment $\\mathbb{E}[X_{t}]$**\n\nLet $\\mu(t) = \\mathbb{E}[X_{t}]$. Taking the expectation of the SDE, we have:\n$$\n\\mathrm{d}\\mathbb{E}[X_{t}] \\;=\\; \\mathbb{E}\\left[\\kappa \\big( \\theta - X_{t} \\big)\\right]\\mathrm{d}t \\;+\\; \\mathbb{E}\\left[\\sigma \\sqrt{X_{t}}\\,\\mathrm{d}W_{t}\\right]\n$$\nThe expectation of the Itô integral term is zero: $\\mathbb{E}[\\int_{0}^{t} \\sigma \\sqrt{X_{s}}\\,\\mathrm{d}W_{s}] = 0$. In differential form, $\\mathbb{E}[\\sigma \\sqrt{X_{t}}\\,\\mathrm{d}W_{t}] = 0$.\nThis gives an ordinary differential equation for the mean:\n$$\n\\frac{\\mathrm{d}\\mu(t)}{\\mathrm{d}t} \\;=\\; \\kappa \\big( \\theta - \\mu(t) \\big)\n$$\nUnder the stationarity assumption, the mean is constant, so $\\frac{\\mathrm{d}\\mu(t)}{\\mathrm{d}t} = 0$. Let the stationary mean be denoted by $\\mu$.\n$$\n0 \\;=\\; \\kappa \\big( \\theta - \\mu \\big)\n$$\nSince $\\kappa  0$, we must have:\n$$\n\\mu \\;=\\; \\theta\n$$\nSo, the stationary first moment is $\\mathbb{E}[X_t] = \\theta$.\n\n**Stationary Variance $\\mathrm{Var}(X_{t})$**\n\nLet $V(t) = \\mathrm{Var}(X_t) = \\mathbb{E}[(X_t - \\mu)^2]$. We use Itô's formula for the function $f(X_t) = (X_t - \\mu)^2 = (X_t - \\theta)^2$.\n$$\n\\mathrm{d}f(X_t) = f'(X_t)\\mathrm{d}X_t + \\frac{1}{2}f''(X_t)(\\mathrm{d}X_t)^2\n$$\nThe derivatives are $f'(x) = 2(x-\\theta)$ and $f''(x) = 2$. The quadratic variation is $(\\mathrm{d}X_t)^2 = \\sigma^2 X_t \\mathrm{d}t$.\nSubstituting these into the formula:\n$$\n\\mathrm{d}(X_t-\\theta)^2 \\;=\\; 2(X_t-\\theta)\\mathrm{d}X_t + \\frac{1}{2}(2)(\\sigma^2 X_t \\mathrm{d}t)\n$$\n$$\n\\mathrm{d}(X_t-\\theta)^2 \\;=\\; 2(X_t-\\theta)\\left[ \\kappa(\\theta-X_t)\\mathrm{d}t + \\sigma\\sqrt{X_t}\\mathrm{d}W_t \\right] + \\sigma^2 X_t \\mathrm{d}t\n$$\n$$\n\\mathrm{d}(X_t-\\theta)^2 \\;=\\; \\left[ -2\\kappa(X_t-\\theta)^2 + \\sigma^2 X_t \\right]\\mathrm{d}t + 2\\sigma(X_t-\\theta)\\sqrt{X_t}\\mathrm{d}W_t\n$$\nNow, take the expectation. Let $V(t) = \\mathbb{E}[(X_t-\\theta)^2]$.\n$$\n\\frac{\\mathrm{d}V(t)}{\\mathrm{d}t} \\;=\\; \\mathbb{E}\\left[-2\\kappa(X_t-\\theta)^2 + \\sigma^2 X_t\\right]\n$$\n$$\n\\frac{\\mathrm{d}V(t)}{\\mathrm{d}t} \\;=\\; -2\\kappa\\mathbb{E}[(X_t-\\theta)^2] + \\sigma^2\\mathbb{E}[X_t] \\;=\\; -2\\kappa V(t) + \\sigma^2 \\theta\n$$\nUnder stationarity, $\\frac{\\mathrm{d}V(t)}{\\mathrm{d}t} = 0$. Let the stationary variance be $V$.\n$$\n0 \\;=\\; -2\\kappa V + \\sigma^2\\theta\n$$\nSolving for $V$:\n$$\nV \\;=\\; \\frac{\\sigma^2\\theta}{2\\kappa}\n$$\nSo, the stationary variance is $\\mathrm{Var}(X_t) = \\frac{\\sigma^2\\theta}{2\\kappa}$.\n\n**Stationary Autocovariance $\\mathrm{Cov}(X_{t}, X_{t+s})$**\n\nThe autocovariance for a lag $s  0$ is $\\mathrm{Cov}(X_t, X_{t+s}) = \\mathbb{E}[(X_t - \\mu)(X_{t+s} - \\mu)]$. Using the law of iterated expectations:\n$$\n\\mathrm{Cov}(X_t, X_{t+s}) \\;=\\; \\mathbb{E}\\left[ (X_t - \\theta) \\mathbb{E}[X_{t+s} - \\theta | X_t] \\right]\n$$\nWe need the conditional expectation $\\mathbb{E}[X_{t+s}|X_t]$. The solution to the SDE for $X_t$ shows that its conditional mean follows the dynamics of the deterministic part of the SDE. Let $m(u) = \\mathbb{E}[X_u | X_t]$ for $u \\ge t$. Then $\\frac{\\mathrm{d}m(u)}{\\mathrm{d}u} = \\kappa(\\theta - m(u))$ with initial condition $m(t)=X_t$. The solution is:\n$$\nm(u) = \\theta + (X_t - \\theta)e^{-\\kappa(u-t)}\n$$\nSetting $u = t+s$, we have:\n$$\n\\mathbb{E}[X_{t+s}|X_t] = \\theta + (X_t - \\theta)e^{-\\kappa s}\n$$\nTherefore, $\\mathbb{E}[X_{t+s}-\\theta|X_t] = (X_t - \\theta)e^{-\\kappa s}$.\nSubstituting this back into the covariance formula:\n$$\n\\mathrm{Cov}(X_t, X_{t+s}) \\;=\\; \\mathbb{E}\\left[ (X_t - \\theta) (X_t - \\theta)e^{-\\kappa s} \\right]\n$$\n$$\n\\mathrm{Cov}(X_t, X_{t+s}) \\;=\\; e^{-\\kappa s} \\mathbb{E}[(X_t - \\theta)^2] \\;=\\; V e^{-\\kappa s}\n$$\nFor a discrete lag $\\ell\\Delta$, the stationary autocovariance is:\n$$\n\\mathrm{Cov}(X_t, X_{t+\\ell\\Delta}) \\;=\\; \\mathrm{Var}(X_t) e^{-\\kappa\\ell\\Delta} \\;=\\; \\frac{\\sigma^2\\theta}{2\\kappa} e^{-\\kappa\\ell\\Delta}\n$$\n\n### Part 2: Method-of-Moments Estimator\n\nWe need to construct explicit, closed-form estimators for $(\\kappa, \\theta, \\sigma)$ using the sample moments $\\widehat{m}_n$, $\\widehat{v}_n$, and $\\{\\widehat{\\gamma}_{\\ell,n}\\}_{\\ell=1}^{L}$. We have three parameters, and we can form more than three moment conditions. The requirement for a \"closed-form\" estimator suggests we should not use an iterative GMM procedure but rather an explicit algebraic solution. The relationship $\\ln(\\gamma_\\ell) = \\ln(V) - \\kappa\\ell\\Delta$ suggests a linear regression approach to estimate $\\kappa$ using all $L$ sample autocovariances.\n\n1.  **Estimator for $\\theta$**: The stationary mean is $\\mu=\\theta$. A natural estimator is the sample mean.\n    $$\n    \\widehat{\\theta} \\;=\\; \\widehat{m}_{n}\n    $$\n2.  **Estimator for $\\kappa$**: The theoretical autocovariance at lag $\\ell\\Delta$ is $\\gamma_\\ell = V e^{-\\kappa\\ell\\Delta}$. Taking the natural logarithm gives a linear relationship: $\\ln(\\gamma_\\ell) = \\ln(V) - (\\kappa\\Delta)\\ell$. We can estimate the slope $-\\kappa\\Delta$ by a least-squares regression of $y_\\ell = \\ln(\\widehat{\\gamma}_{\\ell,n})$ on $x_\\ell=\\ell$ for $\\ell=1, \\dots, L$. The slope estimator is $\\hat{\\beta_1} = \\frac{\\sum_{\\ell=1}^L(\\ell-\\bar{\\ell})(y_\\ell-\\bar{y})}{\\sum_{\\ell=1}^L(\\ell-\\bar{\\ell})^2}$.\n    This gives $-\\widehat{\\kappa}\\Delta = \\hat{\\beta_1}$, so $\\widehat{\\kappa} = -\\hat{\\beta_1}/\\Delta$.\n    We have $\\bar{\\ell} = \\frac{L+1}{2}$ and $\\sum_{\\ell=1}^L(\\ell-\\bar{\\ell})^2 = \\frac{L(L^2-1)}{12}$.\n    $$\n    \\widehat{\\kappa} \\;=\\; -\\frac{1}{\\Delta} \\frac{\\sum_{\\ell=1}^{L} (\\ell - \\frac{L+1}{2}) \\ln(\\widehat{\\gamma}_{\\ell,n})}{\\frac{L(L^2-1)}{12}} \\;=\\; -\\frac{12}{\\Delta L(L^2-1)} \\sum_{\\ell=1}^{L} \\left(\\ell - \\frac{L+1}{2}\\right) \\ln(\\widehat{\\gamma}_{\\ell,n})\n    $$\n3.  **Estimator for $\\sigma$**: From the stationary variance formula $V = \\frac{\\sigma^2\\theta}{2\\kappa}$, we can write $\\sigma^2 = \\frac{2\\kappa V}{\\theta}$. We can estimate $V$ with the sample variance $\\widehat{v}_n$, and use the estimators for $\\theta$ and $\\kappa$.\n    $$\n    \\widehat{\\sigma}^2 \\;=\\; \\frac{2\\widehat{\\kappa}\\widehat{v}_n}{\\widehat{\\theta}} \\;=\\; \\frac{2\\widehat{v}_n}{\\widehat{m}_n} \\widehat{\\kappa}\n    $$\n    This leads to:\n    $$\n    \\widehat{\\sigma} \\;=\\; \\sqrt{\\frac{2\\widehat{v}_n}{\\widehat{m}_n} \\widehat{\\kappa}} \\;=\\; \\sqrt{-\\frac{24\\widehat{v}_n}{\\widehat{m}_n\\Delta L(L^2-1)} \\sum_{\\ell=1}^{L} \\left(\\ell - \\frac{L+1}{2}\\right) \\ln(\\widehat{\\gamma}_{\\ell,n})}\n    $$\n    This requires the term under the square root to be positive, which holds if the sample autocovariances decay on average, making $\\widehat{\\kappa}  0$. The problem assumes $\\widehat{\\gamma}_{\\ell,n}  0$.\n\n### Part 3: Identifiability Discussion\n\nThe system of moments uniquely identifies the parameters $(\\kappa, \\theta, \\sigma)$.\n1.  The stationary mean $\\mu = \\mathbb{E}[X_t] = \\theta$ uniquely identifies $\\theta$.\n2.  The autocovariance function $\\gamma_s = V e^{-\\kappa s}$ has a decay rate $\\kappa$ that is uniquely determined by the ratio of autocovariances at any two different lags, e.g., $\\gamma_{s_1}/\\gamma_{s_2} = e^{-\\kappa(s_1-s_2)}$. Thus, the functional form of the autocovariance identifies $\\kappa$. Our regression-based estimator exploits this property.\n3.  The stationary variance is $V = \\mathrm{Var}(X_t)$. Once $\\theta$ and $\\kappa$ are identified, the relation $V = \\frac{\\sigma^2 \\theta}{2\\kappa}$ uniquely determines $\\sigma^2$ (and $\\sigma$ since $\\sigma0$).\nThe chosen set of moments (mean, variance, and autocovariances) is sufficient for unique identification of the parameter vector $\\beta = (\\kappa, \\theta, \\sigma)^{\\top}$. Our proposed estimator provides an explicit, unique mapping from the sample moments to the parameter estimates, assuming the sample moments satisfy basic properties (e.g., positive variance and autocovariances, and decay of autocovariances) that mirror their theoretical counterparts.\n\n### Part 4: Asymptotic Relative Efficiency\n\nWe analyze the optimal GMM estimator based on the vector of $L+2$ moments: the mean $\\mu$, variance $V$, and $L$ autocovariances $\\{\\gamma_\\ell\\}_{\\ell=1}^L$. Let $\\beta = (\\kappa, \\theta, \\sigma)^{\\top}$. The vector of theoretical moments is:\n$$\nM(\\beta) \\;=\\; \\begin{pmatrix} \\mu(\\beta) \\\\ V(\\beta) \\\\ \\gamma_1(\\beta) \\\\ \\dots \\\\ \\gamma_L(\\beta) \\end{pmatrix} \\;=\\; \\begin{pmatrix} \\theta \\\\ \\frac{\\sigma^2\\theta}{2\\kappa} \\\\ \\frac{\\sigma^2\\theta}{2\\kappa}e^{-\\kappa\\Delta} \\\\ \\vdots \\\\ \\frac{\\sigma^2\\theta}{2\\kappa}e^{-\\kappa L\\Delta} \\end{pmatrix}\n$$\nThe Jacobian of this moment map is a $(L+2) \\times 3$ matrix $D(\\beta) = \\nabla_\\beta M(\\beta)$. Its components are the partial derivatives of the moments with respect to $\\kappa, \\theta, \\sigma$.\nThe first row (derivatives of $\\mu(\\beta)=\\theta$): $(\\frac{\\partial\\theta}{\\partial\\kappa}, \\frac{\\partial\\theta}{\\partial\\theta}, \\frac{\\partial\\theta}{\\partial\\sigma}) = (0, 1, 0)$.\nThe second row (derivatives of $V(\\beta)=\\frac{\\sigma^2\\theta}{2\\kappa}$):\n$$\n\\left(\\frac{\\partial V}{\\partial \\kappa}, \\frac{\\partial V}{\\partial \\theta}, \\frac{\\partial V}{\\partial \\sigma}\\right) \\;=\\; \\left(-\\frac{\\sigma^2\\theta}{2\\kappa^2}, \\frac{\\sigma^2}{2\\kappa}, \\frac{2\\sigma\\theta}{2\\kappa}\\right) \\;=\\; \\left(-\\frac{V}{\\kappa}, \\frac{V}{\\theta}, \\frac{2V}{\\sigma}\\right)\n$$\nThe row $2+\\ell$ (for $\\ell=1, \\dots, L$) are derivatives of $\\gamma_\\ell(\\beta) = V e^{-\\kappa\\ell\\Delta}$:\n$$\n\\frac{\\partial \\gamma_\\ell}{\\partial \\kappa} = \\frac{\\partial V}{\\partial \\kappa}e^{-\\kappa\\ell\\Delta} - V\\ell\\Delta e^{-\\kappa\\ell\\Delta} = \\left(-\\frac{V}{\\kappa} - V\\ell\\Delta\\right)e^{-\\kappa\\ell\\Delta} = -\\gamma_\\ell\\left(\\frac{1}{\\kappa}+\\ell\\Delta\\right)\n$$\n$$\n\\frac{\\partial \\gamma_\\ell}{\\partial \\theta} = \\frac{\\partial V}{\\partial \\theta}e^{-\\kappa\\ell\\Delta} = \\frac{V}{\\theta}e^{-\\kappa\\ell\\Delta} = \\frac{\\gamma_\\ell}{\\theta}\n$$\n$$\n\\frac{\\partial \\gamma_\\ell}{\\partial \\sigma} = \\frac{\\partial V}{\\partial \\sigma}e^{-\\kappa\\ell\\Delta} = \\frac{2V}{\\sigma}e^{-\\kappa\\ell\\Delta} = \\frac{2\\gamma_\\ell}{\\sigma}\n$$\nThe asymptotic variance-covariance matrix of the optimal GMM estimator $\\widehat{\\beta}_{GMM}$ is given by:\n$$\n\\mathrm{avar}\\left(\\sqrt{n}(\\widehat{\\beta}_{GMM}-\\beta)\\right) \\;=\\; V_{GMM} \\;=\\; \\left(D(\\beta)^{\\top}\\Sigma(\\beta)^{-1}D(\\beta)\\right)^{-1}\n$$\nwhere $\\Sigma(\\beta)$ is the long-run covariance matrix of the vector of sample moment conditions, as defined in the problem statement.\nThe asymptotic variance-covariance matrix of the a-priori most efficient estimator, the MLE $\\widehat{\\beta}_{MLE}$, is the inverse of the aymptotic information matrix, which is the per-observation Fisher information matrix $I(\\beta;\\Delta)$ for discretely sampled data:\n$$\n\\mathrm{avar}\\left(\\sqrt{n}(\\widehat{\\beta}_{MLE}-\\beta)\\right) \\;=\\; V_{MLE} \\;=\\; I(\\beta;\\Delta)^{-1}\n$$\nThe asymptotic relative efficiency (ARE) of the GMM estimator for the parameter $\\kappa$ (the first component of $\\beta$) relative to the MLE is the ratio of their asymptotic variances:\n$$\n\\mathrm{ARE}_{\\kappa} \\;=\\; \\frac{(V_{MLE})_{11}}{(V_{GMM})_{11}}\n$$\nSubstituting the expressions for the variance matrices:\n$$\n\\mathrm{ARE}_{\\kappa} \\;=\\; \\frac{\\left(I(\\beta;\\Delta)^{-1}\\right)_{11}}{\\left(\\left(D(\\beta)^{\\top}\\Sigma(\\beta)^{-1}D(\\beta)\\right)^{-1}\\right)_{11}}\n$$\nThis expression is analytic in terms of the model parameters $\\beta=(\\kappa, \\theta, \\sigma)$, the sampling interval $\\Delta$, the number of lags $L$, and the matrices $D(\\beta)$, $\\Sigma(\\beta)$, and $I(\\beta;\\Delta)$. The Jacobian $D(\\beta)$ is derived above, while $\\Sigma(\\beta)$ and $I(\\beta;\\Delta)$ are treated as given, per the problem statement's structure, due to their analytical intractability.", "answer": "$$\n\\boxed{\\pmatrix{ -\\frac{12}{\\Delta L(L^2-1)} \\sum_{\\ell=1}^{L} \\left(\\ell - \\frac{L+1}{2}\\right) \\ln(\\widehat{\\gamma}_{\\ell,n})  \\widehat{m}_{n}  \\sqrt{-\\frac{24\\widehat{v}_n}{\\widehat{m}_n\\Delta L(L^2-1)} \\sum_{\\ell=1}^{L} \\left(\\ell - \\frac{L+1}{2}\\right) \\ln(\\widehat{\\gamma}_{\\ell,n})}  \\frac{\\left(I(\\beta;\\Delta)^{-1}\\right)_{11}}{\\left(\\left(D(\\beta)^{\\top}\\Sigma(\\beta)^{-1}D(\\beta)\\right)^{-1}\\right)_{11}} }}\n$$", "id": "2968996"}]}