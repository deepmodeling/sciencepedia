{"hands_on_practices": [{"introduction": "This first exercise serves as a direct application of the definition of the Itô integral for a simple predictable process. By constructing an integral whose value depends on an initial random event, you will see how the properties of Brownian motion and the law of total probability combine to determine the full probability distribution of the resulting stochastic integral. This practice solidifies the mechanics of the Itô integral and demonstrates how it transforms a simple process into a random variable with a specific, calculable distribution.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\ge 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a standard Brownian motion $W=\\{W_{t}\\}_{t \\ge 0}$ adapted to $(\\mathcal{F}_{t})_{t \\ge 0}$. Fix a time horizon $T > 0$. Let $m \\in \\mathbb{N}$ and let $\\{A_{1},\\dots,A_{m}\\}$ be a partition of $\\Omega$ with $A_{j} \\in \\mathcal{F}_{0}$ and $\\mathbb{P}(A_{j})=p_{j} \\in (0,1)$ for $j=1,\\dots,m$. Let $a_{1},\\dots,a_{m} \\in \\mathbb{R} \\setminus \\{0\\}$ be distinct constants, and define the simple predictable process $H=\\{H_{s}\\}_{0 \\le s \\le T}$ by\n$$\nH_{s}=\\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}\\,\\mathbf{1}_{(0,T]}(s).\n$$\nConsider the Itô integral of $H$ with respect to $W$ over $[0,T]$,\n$$\nI=\\int_{0}^{T} H_{s}\\,dW_{s}.\n$$\nStarting only from the definition of the Itô integral for simple predictable processes and the basic properties of Brownian motion, derive the unconditional probability density function $f_{I}(x)$ of $I$ by conditioning on the $\\sigma$-algebra generated by $\\{A_{1},\\dots,A_{m}\\}$. Express your final answer as a single closed-form analytic expression for $f_{I}(x)$ in terms of $p_{j}$, $a_{j}$, $T$, and $x$. No rounding is required, and no physical units are involved.", "solution": "The problem asks for the unconditional probability density function (PDF) of the Itô integral $I = \\int_{0}^{T} H_{s}\\,dW_{s}$, where $H_{s}$ is a simple predictable process.\n\n**Step 1: Problem Validation**\nThe problem is well-defined within the mathematical framework of stochastic calculus.\n*   **Givens**:\n    *   A filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\ge 0},\\mathbb{P})$.\n    *   A standard Brownian motion $W_{t}$ adapted to $(\\mathcal{F}_{t})_{t \\ge 0}$ with $W_0=0$.\n    *   A time horizon $T > 0$.\n    *   A partition of $\\Omega$, $\\{A_{1},\\dots,A_{m}\\}$, with $A_{j} \\in \\mathcal{F}_{0}$ and $\\mathbb{P}(A_{j})=p_{j} \\in (0,1)$.\n    *   Distinct non-zero constants $a_{1},\\dots,a_{m} \\in \\mathbb{R} \\setminus \\{0\\}$.\n    *   A simple predictable process $H_{s}=\\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}\\,\\mathbf{1}_{(0,T]}(s)$.\n    *   The Itô integral $I=\\int_{0}^{T} H_{s}\\,dW_{s}$.\n*   **Validation**: The problem is scientifically grounded, well-posed, and objective. The process $H_s$ is a standard simple predictable process as it can be written in the form $E \\cdot \\mathbf{1}_{(0,T]}(s)$, where $E = \\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}$ is an $\\mathcal{F}_0$-measurable random variable. The definition of the Itô integral is directly applicable. All conditions are consistent and sufficient for a unique solution. Therefore, the problem is valid.\n\n**Step 2: Derivation of the Itô Integral**\nThe process $H_s$ is a simple predictable process. A general simple process is of the form $\\phi_t = \\sum_{k=0}^{n-1} E_k \\mathbf{1}_{(t_k, t_{k+1}]}(t)$, where each $E_k$ is an $\\mathcal{F}_{t_k}$-measurable random variable. The Itô integral is defined as $\\int_0^T \\phi_t \\, dW_t = \\sum_{k=0}^{n-1} E_k (W_{t_{k+1}} - W_{t_k})$.\n\nIn this problem, the process $H_s$ corresponds to the case with a single time interval $(0, T]$. We can write $H_s = E \\cdot \\mathbf{1}_{(0,T]}(s)$, where the random variable $E$ is given by\n$$\nE = \\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}.\n$$\nSince each event $A_j$ is in the initial $\\sigma$-algebra $\\mathcal{F}_0$, the random variable $E$ is $\\mathcal{F}_0$-measurable.\n\nApplying the definition of the Itô integral for simple processes, we get:\n$$\nI = \\int_{0}^{T} H_{s}\\,dW_{s} = E \\cdot (W_T - W_0).\n$$\nBy the properties of standard Brownian motion, $W_0 = 0$. Thus, the integral simplifies to:\n$$\nI = E \\cdot W_T = \\left(\\sum_{j=1}^{m} a_{j}\\,\\mathbf{1}_{A_{j}}\\right) W_T.\n$$\n\n**Step 3: Derivation of the Probability Density Function**\nTo find the unconditional PDF of $I$, denoted $f_I(x)$, we will use the law of total probability, conditioning on the partition $\\{A_1, \\dots, A_m\\}$. The sets $\\{A_j\\}_{j=1}^m$ form a partition of $\\Omega$, so for any $x \\in \\mathbb{R}$, the PDF can be expressed as:\n$$\nf_I(x) = \\sum_{j=1}^{m} f_{I|A_j}(x) \\cdot \\mathbb{P}(A_j)\n$$\nwhere $f_{I|A_j}(x)$ is the conditional PDF of $I$ given the event $A_j$, and $\\mathbb{P}(A_j) = p_j$.\n\nLet us determine the conditional distribution of $I$ given $A_j$. When we condition on the event $A_j$, the indicator function $\\mathbf{1}_{A_j}$ becomes $1$, and $\\mathbf{1}_{A_k}$ becomes $0$ for all $k \\neq j$. Therefore, the random variable $E$ takes the constant value $a_j$:\n$$\nE \\big|_{A_j} = a_j.\n$$\nConsequently, the conditional random variable $I$ given $A_j$ is:\n$$\nI \\big|_{A_j} = a_j W_T.\n$$\nA standard Brownian motion $W_T$ at time $T$ follows a normal distribution with mean $0$ and variance $T$, i.e., $W_T \\sim \\mathcal{N}(0, T)$.\nThe random variable $E$ is $\\mathcal{F}_0$-measurable, while the increment $W_T = W_T - W_0$ is independent of the $\\sigma$-algebra $\\mathcal{F}_0$. Thus, $E$ and $W_T$ are independent random variables.\n\nFor a normally distributed random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and a constant $c$, the scaled variable $cX$ is distributed as $c X \\sim \\mathcal{N}(c\\mu, c^2\\sigma^2)$. Applying this property to $I \\big|_{A_j} = a_j W_T$, we have:\n$$\nI \\big|_{A_j} \\sim \\mathcal{N}(a_j \\cdot 0, a_j^2 T) = \\mathcal{N}(0, a_j^2 T).\n$$\nThe constants $a_j$ are non-zero, so the variance $a_j^2 T$ is strictly positive.\n\nThe PDF of a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$. For the conditional distribution of $I$ given $A_j$, the PDF is:\n$$\nf_{I|A_j}(x) = \\frac{1}{\\sqrt{2\\pi (a_j^2 T)}} \\exp\\left(-\\frac{(x-0)^2}{2 a_j^2 T}\\right) = \\frac{1}{\\sqrt{a_j^2} \\sqrt{2\\pi T}} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right).\n$$\nSince $\\sqrt{a_j^2} = |a_j|$, this becomes:\n$$\nf_{I|A_j}(x) = \\frac{1}{|a_j| \\sqrt{2\\pi T}} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right).\n$$\n\n**Step 4: Final Expression for the Unconditional PDF**\nFinally, we substitute the conditional PDFs and the probabilities $p_j$ into the law of total probability formula:\n$$\nf_I(x) = \\sum_{j=1}^{m} p_j \\cdot f_{I|A_j}(x) = \\sum_{j=1}^{m} p_j \\left( \\frac{1}{|a_j| \\sqrt{2\\pi T}} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right) \\right).\n$$\nWe can factor out the common term $\\frac{1}{\\sqrt{2\\pi T}}$:\n$$\nf_I(x) = \\frac{1}{\\sqrt{2\\pi T}} \\sum_{j=1}^{m} \\frac{p_j}{|a_j|} \\exp\\left(-\\frac{x^2}{2 a_j^2 T}\\right).\n$$\nThis is the PDF of a mixture of $m$ normal distributions, each with mean $0$ and variance $a_j^2 T$, weighted by the probabilities $p_j$. This provides the complete, closed-form analytic expression for the unconditional PDF of $I$.", "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2\\pi T}} \\sum_{j=1}^{m} \\frac{p_{j}}{|a_{j}|} \\exp\\left(-\\frac{x^{2}}{2 a_{j}^{2} T}\\right)}\n$$", "id": "2981993"}, {"introduction": "Stochastic calculus is built upon precise measure-theoretic foundations, and this exercise illuminates a crucial distinction: the difference between processes that are \"equal almost everywhere\" and those that are \"indistinguishable\". You will construct two simple predictable processes that differ on a set of time-measure zero and use the Itô isometry to prove that their respective Itô integrals are, in fact, identical as continuous-time processes. This highlights how the Itô integral is defined with respect to the underlying product measure, making it robust to modifications of the integrand on negligible sets.", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space equipped with the completed, right-continuous filtration $\\mathbb{F}=\\{\\mathcal{F}_{t}\\}_{t\\geq 0}$ generated by a standard Brownian motion $\\{W_{t}\\}_{t\\geq 0}$, with fixed horizon $T>0$. Consider a deterministic partition $0=t_{0}<t_{1}<\\cdots<t_{m}=T$ and random variables $K_{i}\\in L^{2}(\\Omega,\\mathcal{F}_{t_{i-1}},\\mathbb{P})$ for $i=1,\\dots,m$. Define the base simple process\n$$\nK(t,\\omega)\\;=\\;\\sum_{i=1}^{m}K_{i}(\\omega)\\,\\mathbf{1}_{(t_{i-1},t_{i}]}(t),\\qquad t\\in[0,T].\n$$\nLet $A\\in\\mathcal{F}_{0}$ satisfy $0<\\mathbb{P}(A)<1$, and set $Z(\\omega)=\\mathbf{1}_{A}(\\omega)$. Define two processes $H$ and $G$ on $[0,T]$ by\n$$\nH(t,\\omega)=\\begin{cases}\n0,&t=0,\\\\\nK(t,\\omega),&t\\in(0,T],\n\\end{cases}\n\\qquad\nG(t,\\omega)=\\begin{cases}\nZ(\\omega),&t=0,\\\\\nK(t,\\omega),&t\\in(0,T].\n\\end{cases}\n$$\nTasks:\n1. Verify, from first principles, that both $H$ and $G$ are simple predictable processes. Be explicit about the measurability with respect to $\\mathcal{F}_{t}$ and the predictability structure inherited from the step-function form on $(t_{i-1},t_{i}]$.\n2. Prove that $H$ and $G$ are equal almost everywhere in the product measure $\\mathbb{P}\\otimes dt$ on $\\Omega\\times[0,T]$, but that they are not indistinguishable, meaning that $\\mathbb{P}(H(t)=G(t)\\text{ for all }t\\in[0,T])<1$. Carefully justify each claim from the definitions of almost everywhere and indistinguishability.\n3. Using only the foundational definition of the Itô integral for simple processes with respect to Brownian motion and its core properties (linearity, additivity over disjoint time intervals, and $L^{2}$-isometry), explain why $\\left\\{\\int_{0}^{t}H(s,\\omega)\\,dW_{s}\\right\\}_{t\\in[0,T]}$ and $\\left\\{\\int_{0}^{t}G(s,\\omega)\\,dW_{s}\\right\\}_{t\\in[0,T]}$ define indistinguishable martingales.\n4. Compute the quantity\n$$\n\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T}\\big(H(s,\\omega)-G(s,\\omega)\\big)\\,dW_{s}\\right)^{2}\\right].\n$$\nProvide your final answer as a single, exact real number. No rounding is required.", "solution": "The problem will be addressed by sequentially completing the four specified tasks. The validation of the problem statement precedes the solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   A probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$.\n-   A completed, right-continuous filtration $\\mathbb{F}=\\{\\mathcal{F}_{t}\\}_{t\\geq 0}$ generated by a standard Brownian motion $\\{W_{t}\\}_{t\\geq 0}$.\n-   A fixed time horizon $T>0$.\n-   A deterministic partition $0=t_{0}<t_{1}<\\cdots<t_{m}=T$.\n-   Random variables $K_{i}\\in L^{2}(\\Omega,\\mathcal{F}_{t_{i-1}},\\mathbb{P})$ for $i=1,\\dots,m$.\n-   A base simple process $K(t,\\omega)\\;=\\;\\sum_{i=1}^{m}K_{i}(\\omega)\\,\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$ for $t\\in[0,T]$.\n-   An event $A\\in\\mathcal{F}_{0}$ with $0<\\mathbb{P}(A)<1$.\n-   A random variable $Z(\\omega)=\\mathbf{1}_{A}(\\omega)$.\n-   A process $H(t,\\omega)=\\begin{cases} 0,&t=0,\\\\ K(t,\\omega),&t\\in(0,T]. \\end{cases}$\n-   A process $G(t,\\omega)=\\begin{cases} Z(\\omega),&t=0,\\\\ K(t,\\omega),&t\\in(0,T]. \\end{cases}$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically well-defined and internally consistent. It is a standard problem in the theory of stochastic integration.\n-   **Scientifically Grounded:** The concepts (Brownian motion, filtration, Itô integral, simple processes, indistinguishability) are all standard and fundamental to stochastic calculus. The assumption that $\\mathcal{F}_0$ contains a non-trivial set $A$ is valid because the filtration is completed.\n-   **Well-Posed:** Each task is clearly stated and has a definite, unique answer derivable from the givens.\n-   **Objective:** The language is formal and unambiguous.\n-   **Completeness:** The problem provides all necessary information to perform the tasks, including the measurability and integrability conditions for the random variables $K_i$.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\n**Task 1: Verification that $H$ and $G$ are simple predictable processes.**\n\nA process $\\Phi$ is called a simple predictable process if it can be written in the form\n$$\n\\Phi(t,\\omega) = \\phi_0(\\omega)\\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^{m} \\phi_{i-1}(\\omega) \\mathbf{1}_{(t_{i-1},t_{i}]}(t)\n$$\nwhere each random variable $\\phi_{i-1}$ is $\\mathcal{F}_{t_{i-1}}$-measurable for $i=1, \\dots, m$, and $\\phi_0$ is $\\mathcal{F}_0$-measurable.\n\nLet's examine the process $H(t,\\omega)$. It is defined as:\n$$\nH(t,\\omega) = \\begin{cases} 0,& t=0 \\\\ K(t,\\omega),& t\\in(0,T] \\end{cases}\n$$\nwhere $K(t,\\omega) = \\sum_{i=1}^{m}K_{i}(\\omega)\\,\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$. We can rewrite $H(t,\\omega)$ in the standard form for a simple process:\n$$\nH(t,\\omega) = 0 \\cdot \\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^{m} K_i(\\omega)\\mathbf{1}_{(t_{i-1},t_{i}]}(t)\n$$\nTo verify that $H$ is a simple predictable process, we check the measurability conditions.\n- The coefficient of $\\mathbf{1}_{\\{0\\}}(t)$ is $\\phi_0(\\omega) = 0$. A constant is measurable with respect to any $\\sigma$-algebra, including $\\mathcal{F}_0$.\n- For $i=1,\\dots,m$, the coefficient of $\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$ is $\\phi_{i-1}(\\omega)=K_i(\\omega)$. By the problem statement, $K_i$ is $\\mathcal{F}_{t_{i-1}}$-measurable.\nSince both conditions are met, $H$ is a simple predictable process.\n\nNext, we examine the process $G(t,\\omega)$. It is defined as:\n$$\nG(t,\\omega) = \\begin{cases} Z(\\omega),& t=0 \\\\ K(t,\\omega),& t\\in(0,T] \\end{cases}\n$$\nwhere $Z(\\omega)=\\mathbf{1}_{A}(\\omega)$. We can rewrite $G(t,\\omega)$ as:\n$$\nG(t,\\omega) = Z(\\omega) \\cdot \\mathbf{1}_{\\{0\\}}(t) + \\sum_{i=1}^{m} K_i(\\omega)\\mathbf{1}_{(t_{i-1},t_{i}]}(t)\n$$\nTo verify that $G$ is a simple predictable process, we check the measurability conditions.\n- The coefficient of $\\mathbf{1}_{\\{0\\}}(t)$ is $Z(\\omega)$. We are given that $A \\in \\mathcal{F}_0$, which implies that the indicator function $Z(\\omega) = \\mathbf{1}_A(\\omega)$ is $\\mathcal{F}_0$-measurable.\n- For $i=1,\\dots,m$, the coefficient of $\\mathbf{1}_{(t_{i-1},t_{i}]}(t)$ is $K_i(\\omega)$, which is given to be $\\mathcal{F}_{t_{i-1}}$-measurable.\nSince both conditions are met, $G$ is also a simple predictable process.\n\n**Task 2: Prove $H=G$ almost everywhere but they are not indistinguishable.**\n\nTwo processes $\\Phi_1$ and $\\Phi_2$ are equal almost everywhere (a.e.) with respect to the product measure $\\mathbb{P}\\otimes dt$ on $\\Omega \\times [0,T]$ if the set $\\{(\\omega,t) \\in \\Omega \\times [0,T] \\mid \\Phi_1(t,\\omega) \\neq \\Phi_2(t,\\omega)\\}$ has measure zero.\nLet's identify the set where $H$ and $G$ differ.\n$$\nH(t,\\omega) - G(t,\\omega) = \\begin{cases} 0 - Z(\\omega),& t=0 \\\\ K(t,\\omega) - K(t,\\omega),& t\\in(0,T] \\end{cases} = \\begin{cases} -Z(\\omega),& t=0 \\\\ 0,& t\\in(0,T] \\end{cases}\n$$\nThe processes differ if and only if $t=0$ and $Z(\\omega) \\neq 0$. $Z(\\omega) = \\mathbf{1}_A(\\omega)$ is non-zero if and only if $\\omega \\in A$.\nThus, the set of points where they differ is $D = \\{(\\omega,t) \\mid H(t,\\omega) \\ne G(t,\\omega)\\} = A \\times \\{0\\}$.\nThe product measure of this set is $(\\mathbb{P}\\otimes dt)(D) = \\mathbb{P}(A) \\times dt(\\{0\\})$. The Lebesgue measure $dt$ of a singleton set like $\\{0\\}$ is $0$. Therefore,\n$$\n(\\mathbb{P}\\otimes dt)(D) = \\mathbb{P}(A) \\times 0 = 0\n$$\nThis proves that $H$ and $G$ are equal almost everywhere.\n\nTwo processes $\\Phi_1$ and $\\Phi_2$ are indistinguishable if $\\mathbb{P}(\\Phi_1(t) = \\Phi_2(t) \\text{ for all } t \\in [0,T]) = 1$. This is equivalent to showing that the set of paths on which they differ at any time has probability zero.\nLet's consider the event that $H$ and $G$ are identical for all time: $E = \\{\\omega \\in \\Omega \\mid H(t,\\omega) = G(t,\\omega) \\text{ for all } t \\in [0,T]\\}$.\nThe processes are identical for a given $\\omega$ if and only if they are identical at $t=0$, since they are identical for all $t \\in (0,T]$. The condition is $H(0,\\omega) = G(0,\\omega)$, which means $0 = Z(\\omega) = \\mathbf{1}_A(\\omega)$. This is true if and only if $\\omega \\notin A$, i.e., $\\omega \\in A^c$.\nThus, the event $E$ is identical to the event $A^c$.\nThe probability of this event is $\\mathbb{P}(E) = \\mathbb{P}(A^c) = 1 - \\mathbb{P}(A)$.\nWe are given that $0 < \\mathbb{P}(A) < 1$. This implies that $\\mathbb{P}(E) = 1 - \\mathbb{P}(A) < 1$.\nSince the probability of the paths being identical is less than $1$, the processes $H$ and $G$ are not indistinguishable.\n\n**Task 3: Explain why the Itô integrals of $H$ and $G$ are indistinguishable martingales.**\n\nLet $I_H(t) = \\int_{0}^{t}H(s,\\omega)\\,dW_{s}$ and $I_G(t) = \\int_{0}^{t}G(s,\\omega)\\,dW_{s}$.\nThe Itô integral of any simple predictable process belonging to the appropriate $L^2$ space is a continuous martingale. Since $H$ and $G$ are simple predictable processes and the coefficients $K_i$ are in $L^2(\\Omega, \\mathcal{F}_{t_{i-1}}, \\mathbb{P})$, their Itô integrals are well-defined continuous martingales.\n\nTo show that the processes $\\{I_H(t)\\}_{t \\in [0,T]}$ and $\\{I_G(t)\\}_{t \\in [0,T]}$ are indistinguishable, we must show that $\\mathbb{P}(I_H(t) = I_G(t) \\text{ for all } t\\in[0,T]) = 1$.\nConsider the difference process $D(t) = I_H(t) - I_G(t)$. By the linearity of the Itô integral,\n$$\nD(t) = \\int_{0}^{t} \\big(H(s,\\omega) - G(s,\\omega)\\big) \\,dW_{s}\n$$\n$D(t)$ is a continuous martingale with $D(0)=0$. We will show that $D(t)=0$ for all $t \\in [0,T]$ almost surely.\nA core property of the Itô integral is that if two integrands are equal almost everywhere with respect to $\\mathbb{P}\\otimes dt$, their Itô integrals are indistinguishable. We have already shown that $H$ and $G$ are equal a.e. in Task 2.\nTo demonstrate this from the core properties listed, we use the $L^2$-isometry. For any fixed $t \\in [0,T]$, the isometry states:\n$$\n\\mathbb{E}\\left[D(t)^2\\right] = \\mathbb{E}\\left[\\left(\\int_{0}^{t} \\big(H(s) - G(s)\\big) \\,dW_{s}\\right)^2\\right] = \\mathbb{E}\\left[\\int_{0}^{t} \\big(H(s) - G(s)\\big)^2 \\,ds\\right]\n$$\nBy Fubini's theorem (since the integrand is non-negative), we can swap the expectation and integral:\n$$\n\\mathbb{E}\\left[D(t)^2\\right] = \\int_{0}^{t} \\mathbb{E}\\left[\\big(H(s) - G(s)\\big)^2\\right] \\,ds\n$$\nAs established in Task 2, $H(s) - G(s) = 0$ for all $s \\in (0,T]$. At $s=0$, $H(0) - G(0) = -Z(\\omega)$. The integrand of the time integral, $\\mathbb{E}[(H(s)-G(s))^2]$, is $0$ for all $s \\in (0,t]$. The Lebesgue integral over $[0,t]$ is therefore $0$, as the value at a single point does not contribute to the integral.\n$$\n\\int_{0}^{t} \\mathbb{E}\\left[\\big(H(s) - G(s)\\big)^2\\right] \\,ds = \\int_{0}^{t} 0 \\,ds = 0\n$$\nSo, for any $t \\in [0,T]$, $\\mathbb{E}[D(t)^2] = 0$. This implies that $D(t) = 0$ almost surely for each fixed $t$.\nTo establish indistinguishability, we use the fact that $D(t)$ is a continuous process. A continuous process that is zero at every fixed time $t$ with probability $1$ is indistinguishable from the zero process.\nFormally, let $Q = [0,T] \\cap \\mathbb{Q}$ be the set of rational numbers in the interval. The event \"there exists $t \\in [0,T]$ such that $D(t) \\neq 0$\" is, by continuity, the same as \"there exists $q \\in Q$ such that $D(q) \\neq 0$\".\nLet $E_q = \\{\\omega \\mid D(q,\\omega) \\neq 0\\}$. We have shown $\\mathbb{P}(E_q) = 0$ for each $q$. The union $E = \\cup_{q \\in Q} E_q$ is a countable union of null sets, and thus is a null set: $\\mathbb{P}(E)=0$.\nThis means $\\mathbb{P}(D(q)=0 \\text{ for all } q \\in Q)=1$. By the continuity of the sample paths of $D(t)$, if a path is zero on the dense set $Q$, it must be zero for all $t \\in [0,T]$.\nTherefore, $\\mathbb{P}(D(t)=0 \\text{ for all } t \\in [0,T]) = 1$. This shows that the processes $\\{I_H(t)\\}$ and $\\{I_G(t)\\}$ are indistinguishable.\n\n**Task 4: Compute the quantity.**\n\nWe are asked to compute\n$$\n\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T}\\big(H(s,\\omega)-G(s,\\omega)\\big)\\,dW_{s}\\right)^{2}\\right]\n$$\nThis is precisely the expression for $\\mathbb{E}[D(T)^2]$ derived in Task 3. Using the $L^2$-isometry property of the Itô integral:\n$$\n\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T}\\big(H(s)-G(s)\\big)\\,dW_{s}\\right)^{2}\\right] = \\mathbb{E}\\!\\left[\\int_{0}^{T}\\big(H(s)-G(s)\\big)^{2}\\,ds\\right]\n$$\nWe analyze the integrand of the time integral, $(H(s)-G(s))^2$.\nFor any $s \\in (0, T]$, we have $H(s)=K(s)$ and $G(s)=K(s)$, so $(H(s)-G(s))^2 = 0$.\nAt $s=0$, we have $H(0)=0$ and $G(0)=Z(\\omega)$, so $(H(0)-G(0))^2 = (-Z(\\omega))^2 = Z(\\omega)^2 = (\\mathbf{1}_A(\\omega))^2 = \\mathbf{1}_A(\\omega)$.\nSo, for any given $\\omega \\in \\Omega$, the integrand as a function of time is:\n$$\nf(s) = \\big(H(s,\\omega)-G(s,\\omega)\\big)^{2} = \\begin{cases} \\mathbf{1}_A(\\omega), & s=0 \\\\ 0, & s \\in (0,T] \\end{cases}\n$$\nThe integral $\\int_{0}^{T} f(s) \\,ds$ is a Lebesgue integral with respect to time $s$. The integrand is non-zero only at the single point $s=0$. A set containing a single point has Lebesgue measure zero. Therefore, the integral is zero.\n$$\n\\int_{0}^{T}\\big(H(s,\\omega)-G(s,\\omega)\\big)^{2}\\,ds = 0 \\quad \\text{for every } \\omega \\in \\Omega.\n$$\nNow, we take the expectation of this result:\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{T}\\big(H(s)-G(s)\\big)^{2}\\,ds\\right] = \\mathbb{E}[0] = 0.\n$$\nThe value of the expression is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2982014"}, {"introduction": "This final practice connects the Itô integral to the powerful framework of functional analysis by using an orthonormal basis for the space of square-integrable functions, $L^2([0,T])$. By representing deterministic integrands using the Haar basis, you will explore how the Itô isometry maps this deterministic basis into a set of independent standard normal random variables. This perspective reveals that the Itô integral of any deterministic function is fundamentally a weighted sum of these elementary Gaussian \"building blocks,\" providing a deep structural understanding of the Wiener integral.", "problem": "Let $T>0$. For $n\\in \\mathbb{N}_0$ and $k\\in \\{0,1,\\dots,2^n-1\\}$, define the dyadic subintervals of $[0,T]$ by $I_{n,k}=[kT/2^n,(k+1)T/2^n)$, and let $I^{L}_{n,k}=[kT/2^n,(k+1/2)T/2^n)$ and $I^{R}_{n,k}=[(k+1/2)T/2^n,(k+1)T/2^n)$ denote its left and right halves. Consider the step functions\n$$\nh_{-1,0}(t):=T^{-1/2}\\,\\mathbf{1}_{[0,T]}(t),\\qquad\nh_{n,k}(t):=2^{n/2}T^{-1/2}\\big(\\mathbf{1}_{I^{L}_{n,k}}(t)-\\mathbf{1}_{I^{R}_{n,k}}(t)\\big).\n$$\nLet $W=(W_t)_{t\\in[0,T]}$ be a standard Brownian motion (also called a Wiener process) on a filtered probability space satisfying the usual conditions. For deterministic simple processes $f$, the Itô stochastic integral $\\int_0^T f(t)\\,dW_t$ is defined by linearity on step functions and extended by $L^2([0,T])$ continuity to all deterministic $f\\in L^2([0,T])$.\n\nUsing orthogonality, construct an orthonormal basis of $L^2([0,T])$ consisting of step functions and interpret the Itô integrals against these as coordinates of a Gaussian vector. Which of the following statements are correct?\n\nA. The collection $\\{h_{-1,0}\\}\\cup\\{h_{n,k}:n\\in\\mathbb{N}_0,\\;0\\le k\\le 2^n-1\\}$ is an orthonormal basis of $L^2([0,T])$ consisting of step functions, and its closed linear span is the whole $L^2([0,T])$.\n\nB. For any fixed finite subcollection $\\{h_{i}\\}_{i=1}^m$ from $\\{h_{-1,0}\\}\\cup\\{h_{n,k}\\}$, the random vector $\\big(\\int_0^T h_{1}(t)\\,dW_t,\\dots,\\int_0^T h_{m}(t)\\,dW_t\\big)$ is centered Gaussian with covariance matrix equal to the identity, so its components are independent standard normal random variables.\n\nC. For any deterministic $f\\in L^2([0,T])$ with expansion $f=\\sum_{n,k} a_{n,k} h_{n,k}$ in $L^2([0,T])$, one has $\\int_0^T f(t)\\,dW_t=\\sum_{n,k} a_{n,k}\\int_0^T h_{n,k}(t)\\,dW_t$ with convergence of the series in $L^2(\\Omega)$, and thus $\\int_0^T f(t)\\,dW_t$ is Gaussian with variance $\\|f\\|_{L^2([0,T])}^2$.\n\nD. The process $t\\mapsto W_t$ can be reconstructed pointwise from the coordinates $\\xi_{-1,0}:=\\int_0^T h_{-1,0}(t)\\,dW_t$ and $\\xi_{n,k}:=\\int_0^T h_{n,k}(t)\\,dW_t$ via the series identity $W_t=\\sum_{n,k}\\xi_{n,k}h_{n,k}(t)$ for all $t\\in[0,T]$.\n\nE. The mapping $I:L^2([0,T])\\to L^2(\\Omega)$ given by $I(f):=\\int_0^T f(t)\\,dW_t$ is a surjective isometry (that is, a unitary isomorphism) onto $L^2(\\Omega)$.", "solution": "The problem statement is a valid exercise in stochastic calculus, specifically concerning the properties of the Itô integral for deterministic integrands and their representation using an orthonormal basis. The definitions provided for the Haar functions and the Itô integral are standard and mathematically sound. The problem is well-posed and its components are based on established theorems in functional analysis and probability theory.\n\n### Analysis of the Statements\n\n**A. The collection $\\{h_{-1,0}\\}\\cup\\{h_{n,k}:n\\in\\mathbb{N}_0,\\;0\\le k\\le 2^n-1\\}$ is an orthonormal basis of $L^2([0,T])$ consisting of step functions, and its closed linear span is the whole $L^2([0,T])$.**\n\nThis statement concerns the properties of the Haar system on the interval $[0,T]$.\n1.  **Step Functions**: The function $h_{-1,0}(t) = T^{-1/2} \\mathbf{1}_{[0,T]}(t)$ is a constant multiple of an indicator function of an interval, hence it is a simple step function. Each function $h_{n,k}(t) = 2^{n/2}T^{-1/2}(\\mathbf{1}_{I^{L}_{n,k}}(t)-\\mathbf{1}_{I^{R}_{n,k}}(t))$ is a linear combination of two indicator functions of intervals, making it a simple step function. So, the collection consists of step functions.\n\n2.  **Orthonormality**: We must verify that the inner product $\\langle f, g \\rangle = \\int_0^T f(t)g(t)dt$ yields $\\delta_{ij}$.\n    -   Norm of $h_{-1,0}$:\n        $$ \\Vert h_{-1,0} \\Vert_{L^2}^2 = \\int_0^T (T^{-1/2})^2 dt = \\int_0^T T^{-1} dt = T^{-1} [t]_0^T = 1. $$\n    -   Norm of $h_{n,k}$: The supports of $\\mathbf{1}_{I^{L}_{n,k}}$ and $\\mathbf{1}_{I^{R}_{n,k}}$ are disjoint. The length of both $I^{L}_{n,k}$ and $I^{R}_{n,k}$ is $T/2^{n+1}$.\n        $$ \\Vert h_{n,k} \\Vert_{L^2}^2 = \\int_0^T (2^{n/2}T^{-1/2})^2 (\\mathbf{1}_{I^{L}_{n,k}}(t)-\\mathbf{1}_{I^{R}_{n,k}}(t))^2 dt = \\frac{2^n}{T} \\int_0^T (\\mathbf{1}_{I^{L}_{n,k}}(t)+\\mathbf{1}_{I^{R}_{n,k}}(t)) dt $$\n        $$ = \\frac{2^n}{T} \\left( \\text{length}(I^{L}_{n,k}) + \\text{length}(I^{R}_{n,k}) \\right) = \\frac{2^n}{T} \\left( \\frac{T}{2^{n+1}} + \\frac{T}{2^{n+1}} \\right) = \\frac{2^n}{T} \\frac{T}{2^n} = 1. $$\n    -   Orthogonality of $h_{-1,0}$ and $h_{n,k}$:\n        $$ \\langle h_{-1,0}, h_{n,k} \\rangle = \\int_0^T T^{-1/2} \\cdot 2^{n/2}T^{-1/2} (\\mathbf{1}_{I^{L}_{n,k}}(t)-\\mathbf{1}_{I^{R}_{n,k}}(t)) dt = \\frac{2^{n/2}}{T} \\left( \\int_{I^{L}_{n,k}} 1 dt - \\int_{I^{R}_{n,k}} 1 dt \\right) $$\n        $$ = \\frac{2^{n/2}}{T} \\left( \\frac{T}{2^{n+1}} - \\frac{T}{2^{n+1}} \\right) = 0. $$\n    -   Orthogonality of $h_{n,k}$ and $h_{m,j}$ for $(n,k) \\neq (m,j)$:\n        -   If $n=m$ and $k \\neq j$, the supports of $h_{n,k}$ and $h_{n,j}$ are the disjoint intervals $I_{n,k}$ and $I_{n,j}$, so their product is $0$ everywhere, and the integral is $0$.\n        -   If $n \\neq m$, assume $m < n$. The support of $h_{n,k}$ is $I_{n,k}$, which is contained in exactly one interval $I_{m,j}$ for some $j$. On this interval $I_{m,j}$, the function $h_{m,j}(t)$ is piecewise constant, taking one value on $I^L_{m,j}$ and another on $I^R_{m,j}$. The interval $I_{n,k}$ is entirely contained within either $I^L_{m,j}$ or $I^R_{m,j}$. Therefore, $h_{m,j}(t)$ is constant on the support of $h_{n,k}(t)$. Let this constant be $c$.\n        $$ \\langle h_{n,k}, h_{m,j} \\rangle = \\int_{I_{n,k}} h_{n,k}(t) h_{m,j}(t) dt = c \\int_{I_{n,k}} h_{n,k}(t) dt = 0, $$\n        since the integral of any Haar function (except the first one) over its support is $0$.\n    Thus, the collection is orthonormal.\n\n3.  **Basis Property**: It is a fundamental theorem of functional analysis that the Haar system (the set of functions defined in the problem) forms a complete orthonormal basis for $L^2([0,T])$. This means its closed linear span is the entire space $L^2([0,T])$.\n\nAll aspects of the statement are true.\nVerdict: **Correct**.\n\n**B. For any fixed finite subcollection $\\{h_{i}\\}_{i=1}^m$ from $\\{h_{-1,0}\\}\\cup\\{h_{n,k}\\}$, the random vector $\\big(\\int_0^T h_{1}(t)\\,dW_t,\\dots,\\int_0^T h_{m}(t)\\,dW_t\\big)$ is centered Gaussian with covariance matrix equal to the identity, so its components are independent standard normal random variables.**\n\nLet $X_i = \\int_0^T h_i(t) dW_t$. Since each $h_i(t)$ is a deterministic function in $L^2([0,T])$, the Itô integral $X_i$ is a Gaussian random variable. Any finite linear combination $\\sum_{i=1}^m c_i X_i = \\int_0^T (\\sum_{i=1}^m c_i h_i(t)) dW_t$ is also a Gaussian random variable. This implies that the vector $(X_1, \\dots, X_m)$ is a jointly Gaussian random vector.\n\n-   **Mean**: The expectation of an Itô integral with a deterministic integrand is zero.\n    $$ \\mathbb{E}[X_i] = \\mathbb{E}\\left[\\int_0^T h_i(t) dW_t\\right] = 0. $$\n    The vector is centered.\n\n-   **Covariance**: The covariance of two such integrals is given by the Itô isometry property:\n    $$ \\text{Cov}(X_i, X_j) = \\mathbb{E}[X_i X_j] = \\mathbb{E}\\left[\\left(\\int_0^T h_i(t) dW_t\\right)\\left(\\int_0^T h_j(t) dW_t\\right)\\right] = \\int_0^T h_i(t) h_j(t) dt = \\langle h_i, h_j \\rangle. $$\n    As established in the analysis of A, the collection $\\{h_i\\}_{i=1}^m$ is an orthonormal set, so $\\langle h_i, h_j \\rangle = \\delta_{ij}$ (the Kronecker delta). Therefore, the covariance matrix of the vector $(X_1, \\dots, X_m)$ is the identity matrix $I_m$.\n\n-   **Independence**: A jointly Gaussian random vector has independent components if and only if its covariance matrix is diagonal. In this case, the covariance matrix is the identity matrix, which is diagonal. Thus, the components $X_i$ are mutually independent. Each $X_i$ has mean $0$ and variance $\\text{Var}(X_i) = \\text{Cov}(X_i, X_i) = 1$. This means each $X_i$ is a standard normal random variable.\n\nThe statement is a direct consequence of the properties of the Itô integral and the orthonormality of the Haar functions.\nVerdict: **Correct**.\n\n**C. For any deterministic $f\\in L^2([0,T])$ with expansion $f=\\sum_{n,k} a_{n,k} h_{n,k}$ in $L^2([0,T])$, one has $\\int_0^T f(t)\\,dW_t=\\sum_{n,k} a_{n,k}\\int_0^T h_{n,k}(t)\\,dW_t$ with convergence of the series in $L^2(\\Omega)$, and thus $\\int_0^T f(t)\\,dW_t$ is Gaussian with variance $\\|f\\|_{L^2([0,T])}^2$.**\n\nLet $I(f) = \\int_0^T f(t) dW_t$. The mapping $I: L^2([0,T]) \\to L^2(\\Omega)$ is linear. The Itô isometry property states that $\\Vert I(f) \\Vert_{L^2(\\Omega)} = \\Vert f \\Vert_{L^2([0,T])}$. This implies $I$ is a continuous linear operator.\nThe statement $f = \\sum_{n,k} a_{n,k} h_{n,k}$ (where the sum is over the whole basis) means that the partial sums $f_N = \\sum_{i=1}^N a_i h_i$ converge to $f$ in the $L^2([0,T])$ norm as $N \\to \\infty$.\n\nBecause $I$ is a continuous linear operator, $f_N \\to f$ in $L^2([0,T])$ implies $I(f_N) \\to I(f)$ in $L^2(\\Omega)$.\nFor the finite sum $f_N$, linearity of the integral gives:\n$$ I(f_N) = I\\left(\\sum_{i=1}^N a_i h_i\\right) = \\sum_{i=1}^N a_i I(h_i) = \\sum_{i=1}^N a_i \\int_0^T h_i(t) dW_t. $$\nTaking the limit as $N \\to \\infty$ in $L^2(\\Omega)$, we get:\n$$ \\int_0^T f(t) dW_t = \\sum_{n,k} a_{n,k} \\int_0^T h_{n,k}(t) dW_t, $$\nwhere the series converges in $L^2(\\Omega)$. This confirms the first part of the statement.\n\nNow, for the distribution of $I(f)$: each partial sum $I(f_N)$ is a finite linear combination of jointly Gaussian random variables (from B), and is therefore itself a Gaussian random variable. The $L^2(\\Omega)$ limit of a sequence of Gaussian random variables is also Gaussian. Hence, $I(f)$ is a Gaussian random variable.\n-   Mean: $\\mathbb{E}[I(f)] = 0$.\n-   Variance: $\\text{Var}(I(f)) = \\mathbb{E}[I(f)^2] = \\Vert I(f) \\Vert_{L^2(\\Omega)}^2$. By the Itô isometry, this equals $\\Vert f \\Vert_{L^2([0,T])}^2$.\n\nSo, $\\int_0^T f(t) dW_t$ is a Gaussian random variable with mean $0$ and variance $\\Vert f \\Vert_{L^2([0,T])}^2$.\nVerdict: **Correct**.\n\n**D. The process $t\\mapsto W_t$ can be reconstructed pointwise from the coordinates $\\xi_{-1,0}:=\\int_0^T h_{-1,0}(t)\\,dW_t$ and $\\xi_{n,k}:=\\int_0^T h_{n,k}(t)\\,dW_t$ via the series identity $W_t=\\sum_{n,k}\\xi_{n,k}h_{n,k}(t)$ for all $t\\in[0,T]$.**\n\nThis statement proposes an expansion for the Brownian motion process $W_t$. We can write $W_t$ as an Itô integral: $W_t = \\int_0^T \\mathbf{1}_{[0,t]}(s) dW_s$.\nTo find the representation of $W_t$ in terms of the random coordinates $\\xi_{n,k} = \\int_0^T h_{n,k}(s) dW_s$, we first need to expand the deterministic function $g_t(s) = \\mathbf{1}_{[0,t]}(s)$ in the Haar basis $\\{h_{n,k}\\}$.\nThe expansion is $g_t(s) = \\sum_{n,k} \\langle g_t, h_{n,k} \\rangle_{L^2} h_{n,k}(s)$, where the coefficients are:\n$$ c_{n,k}(t) = \\langle g_t, h_{n,k} \\rangle_{L^2} = \\int_0^T \\mathbf{1}_{[0,t]}(s) h_{n,k}(s) ds = \\int_0^t h_{n,k}(s) ds. $$\nThese coefficients $c_{n,k}(t)$ are the Schauder functions, which are continuous and piecewise linear.\nNow, applying the Itô integral operator $I(\\cdot)$ to the expansion of $g_t(s)$ and using its linearity and continuity (as established in C), we obtain:\n$$ W_t = I(g_t) = I\\left(\\sum_{n,k} c_{n,k}(t) h_{n,k}\\right) = \\sum_{n,k} c_{n,k}(t) I(h_{n,k}) = \\sum_{n,k} \\left(\\int_0^t h_{n,k}(s) ds\\right) \\xi_{n,k}. $$\nThis is the correct Lévy-Ciesielski construction of Brownian motion. The statement in D suggests the expansion is $W_t = \\sum_{n,k} \\xi_{n,k} h_{n,k}(t)$. This is incorrect; it replaces the integrated Haar functions (Schauder functions) $\\int_0^t h_{n,k}(s)ds$ with the Haar functions $h_{n,k}(t)$ themselves.\nThe Haar functions $h_{n,k}(t)$ are discontinuous step functions. A sum of these, even with random coefficients, would not generally produce the continuous paths of a Brownian motion. The Schauder functions are continuous, and their series uniformly converges (almost surely) to the continuous paths of $W_t$.\nTherefore, the identity provided in the statement is mathematically incorrect.\nVerdict: **Incorrect**.\n\n**E. The mapping $I:L^2([0,T])\\to L^2(\\Omega)$ given by $I(f):=\\int_0^T f(t)\\,dW_t$ is a surjective isometry (that is, a unitary isomorphism) onto $L^2(\\Omega)$.**\n\n-   **Isometry**: As noted previously, the Itô isometry property $\\Vert I(f) \\Vert_{L^2(\\Omega)} = \\Vert f \\Vert_{L^2([0,T])}$ means that the map $I$ is indeed an isometry from $L^2([0,T])$ to its image in $L^2(\\Omega)$. An isometry is always injective.\n\n-   **Surjectivity**: The question is whether the image of $I$ is the entire space $L^2(\\Omega)$. The space $L^2(\\Omega)$ is assumed to be $L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$, where the filtration is generated by the Brownian motion $W$. Let's denote this space by $L^2(\\mathcal{F}_T^W)$.\nThe image of the map $I$ is the space of all random variables of the form $\\int_0^T f(t)dW_t$ for $f \\in L^2([0,T])$. This space is known as the first Wiener chaos, denoted $\\mathcal{H}_1$. As shown in the analysis of C, every element in $\\mathcal{H}_1$ is a Gaussian random variable.\nHowever, the space $L^2(\\mathcal{F}_T^W)$ contains many non-Gaussian random variables. For example, consider the random variable $Y = W_T^2 - T$. Its expectation is $\\mathbb{E}[W_T^2 - T] = T - T = 0$. It is in $L^2(\\mathcal{F}_T^W)$. However, $Y$ is not a Gaussian random variable; its distribution is that of a scaled and shifted chi-squared random variable. Since the image of $I$ contains only Gaussian variables (and the zero variable), $Y$ cannot be in the image of $I$.\nThe Wiener-Itô chaos decomposition theorem states that $L^2(\\mathcal{F}_T^W) = \\bigoplus_{q=0}^{\\infty} \\mathcal{H}_q$, where $\\mathcal{H}_q$ is the $q$-th Wiener chaos. The mapping $I$ has as its range only the first chaos space $\\mathcal{H}_1$. This is a proper subspace of the full $L^2$ space.\nTherefore, the mapping $I$ is not surjective.\n\nSince the mapping is not surjective, it cannot be a unitary isomorphism.\nVerdict: **Incorrect**.\n\n### Conclusion\n\nStatements A, B, and C are correct descriptions of the Haar basis and the Itô integral. Statements D and E contain mathematical errors regarding the series expansion of Brownian motion and the range of the Itô integral operator, respectively.", "answer": "$$\\boxed{ABC}$$", "id": "2981992"}]}