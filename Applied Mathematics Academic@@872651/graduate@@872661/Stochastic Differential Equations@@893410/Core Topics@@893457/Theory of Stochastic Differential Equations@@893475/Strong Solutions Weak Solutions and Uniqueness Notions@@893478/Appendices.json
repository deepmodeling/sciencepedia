{"hands_on_practices": [{"introduction": "This first practice serves as a foundational exercise, guiding you through the analysis of a well-behaved stochastic differential equation. By verifying the global Lipschitz and linear growth conditions, you will apply the fundamental theorem that guarantees the existence of a unique strong solution, the \"gold standard\" in SDE theory. This exercise solidifies your understanding of the solution's canonical semimartingale structure and the calculation of its quadratic variation, reinforcing the core properties that define Itô processes.", "problem": "Consider the $1$-dimensional stochastic differential equation (SDE)\n$$\ndX_t \\;=\\; b(X_t)\\,dt \\;+\\; \\sigma\\, dW_t,\\qquad X_0=x_0,\n$$\nwhere $W_t$ is a standard Brownian motion on a filtered probability space satisfying the usual conditions, $\\sigma\\neq 0$ is a constant, and\n$$\nb(x)\\;=\\;\\frac{x}{1+x^2}.\n$$\nAnswer the following in a logically structured manner, starting from foundational definitions and well-tested facts of stochastic calculus and stochastic differential equations. You must not use or assume any target formulas about quadratic variation or semimartingale decompositions beyond their definitions.\n\na) Using only the global Lipschitz and linear growth principles for drift and diffusion coefficients, establish strong existence and pathwise uniqueness for the SDE. Then explain how uniqueness in law (weak uniqueness) follows, and state the role of the Yamada–Watanabe theorem in relating these notions.\n\nb) Identify the canonical semimartingale decomposition of $X_t$ and justify that $X$ is a continuous semimartingale by appealing to definitions and standard properties of stochastic integrals and finite-variation processes.\n\nc) Compute the quadratic variation $[X]_t$ in closed form as a function of $t$ and the model parameters. You must present a single analytic expression. \n\nd) Use your computation of $[X]_t$ to verify the semimartingale structure you identified in part (b) by showing that the finite-variation component has zero quadratic variation and that the local martingale component alone contributes to $[X]_t$.\n\nProvide your final answer as the closed-form expression for $[X]_t$ from part (c). No rounding is required.", "solution": "### Solution\n\na) **Strong Existence and Pathwise Uniqueness**\n\nThe existence of a pathwise unique strong solution to the SDE $dX_t = b(X_t) dt + \\sigma(X_t) dW_t$ is guaranteed if the drift coefficient $b(x)$ and the diffusion coefficient $\\sigma(x)$ satisfy the global Lipschitz condition and the linear growth condition.\n\nThe drift coefficient is $b(x) = \\frac{x}{1+x^2}$ and the diffusion coefficient is $\\sigma(x) = \\sigma$, where $\\sigma$ is a non-zero constant.\n\n**1. Verification of Conditions for the Diffusion Coefficient, $\\sigma(x)=\\sigma$:**\n- **Global Lipschitz Condition:** For any $x, y \\in \\mathbb{R}$, we must find a constant $L_\\sigma \\ge 0$ such that $|\\sigma(x) - \\sigma(y)| \\le L_\\sigma|x-y|$. We have $|\\sigma - \\sigma| = 0$, so $0 \\le L_\\sigma|x-y|$ is true for any $L_\\sigma \\ge 0$. The condition is satisfied.\n- **Linear Growth Condition:** For any $x \\in \\mathbb{R}$, we must find a constant $K_\\sigma \\ge 0$ such that $|\\sigma(x)|^2 \\le K_\\sigma(1+x^2)$. We have $\\sigma^2 \\le K_\\sigma(1+x^2)$. Since $\\sigma$ is a constant and $1+x^2 \\ge 1$, we can choose $K_\\sigma = \\sigma^2$, and the inequality $\\sigma^2 \\le \\sigma^2(1+x^2)$ holds for all $x \\in \\mathbb{R}$. The condition is satisfied.\n\n**2. Verification of Conditions for the Drift Coefficient, $b(x)=\\frac{x}{1+x^2}$:**\n- **Global Lipschitz Condition:** We examine the derivative of $b(x)$ to find its maximum absolute value.\n$$\nb'(x) = \\frac{d}{dx}\\left(\\frac{x}{1+x^2}\\right) = \\frac{1 \\cdot (1+x^2) - x \\cdot (2x)}{(1+x^2)^2} = \\frac{1-x^2}{(1+x^2)^2}.\n$$\nTo find the maximum of $|b'(x)|$, we note that $b'(0) = 1$ and for $x\\to\\pm\\infty$, $b'(x) \\to 0$. A brief analysis shows that the maximum value of $|b'(x)|$ occurs at $x=0$, where $|b'(0)| = 1$. Therefore, $|b'(x)| \\le 1$ for all $x \\in \\mathbb{R}$. By the Mean Value Theorem, for any $x, y \\in \\mathbb{R}$, there exists a $c$ between $x$ and $y$ such that $|b(x)-b(y)| = |b'(c)||x-y|$. This implies $|b(x)-b(y)| \\le 1 \\cdot |x-y|$. Thus, $b(x)$ is globally Lipschitz with a Lipschitz constant $L_b=1$.\n- **Linear Growth Condition:** We need to find a constant $K_b \\ge 0$ such that $|b(x)|^2 \\le K_b(1+x^2)$. Since $|b(x)| = |\\frac{x}{1+x^2}| \\le \\frac{1}{2}$ for all $x$, we have $|b(x)|^2 \\le \\frac{1}{4}$. We can choose $K_b=1/4$. The inequality $\\frac{1}{4} \\le \\frac{1}{4}(1+x^2)$ is true for all $x \\in \\mathbb{R}$. Thus, the linear growth condition is satisfied.\n\nSince both coefficients $b(x)$ and $\\sigma(x)$ satisfy the global Lipschitz and linear growth conditions, the fundamental theorem on existence and uniqueness for SDEs guarantees that for any initial condition $X_0=x_0$, there exists a pathwise unique strong solution $(X_t)_{t \\ge 0}$.\n\n**Uniqueness in Law and the Yamada–Watanabe Theorem:**\nPathwise uniqueness is a stronger notion than uniqueness in law (also called weak uniqueness). If a solution is pathwise unique, it means that for a given Brownian motion $W_t$ and initial condition $x_0$, there is only one process $X_t$ that solves the SDE. This implies that the law (the probability distribution on the space of continuous functions) of the solution process is uniquely determined by the law of the initial condition. Therefore, pathwise uniqueness implies uniqueness in law.\n\nThe Yamada–Watanabe theorem formalizes the relationship between weak solutions, strong solutions, and uniqueness. It states that if weak existence and pathwise uniqueness hold, then strong existence follows. In our context, since standard theorems guarantee weak existence under linear growth conditions (which we have verified), and we have established pathwise uniqueness, the Yamada-Watanabe theorem confirms the existence of a strong solution.\n\nb) **Canonical Semimartingale Decomposition**\n\nThe SDE $dX_t = b(X_t)dt + \\sigma dW_t$ with $X_0=x_0$ can be written in its integral form:\n$$\nX_t = x_0 + \\int_0^t b(X_s) ds + \\int_0^t \\sigma dW_s.\n$$\nA continuous semimartingale is a stochastic process that can be decomposed as the sum of a continuous local martingale and a continuous process of finite variation. Let us define:\n- $A_t = \\int_0^t b(X_s) ds$\n- $M_t = \\int_0^t \\sigma dW_s$\n\nThen the solution can be written as $X_t = X_0 + A_t + M_t$. We must justify that this is a valid semimartingale decomposition.\n\n**1. The Finite-Variation Component, $A_t$:**\nThe process $A_t$ is a process of finite variation if its total variation on any finite time interval $[0, T]$ is almost surely finite. The total variation $V_T(A)$ is given by:\n$$\nV_T(A) = \\int_0^T |dA_s| = \\int_0^T |b(X_s)| ds.\n$$\nThe maximum absolute value of $b(x) = x/(1+x^2)$ occurs at $x=\\pm 1$, where $|b(x)| = 1/2$. Thus, $|b(x)| \\le 1/2$ for all $x \\in \\mathbb{R}$. Therefore,\n$$\nV_T(A) = \\int_0^T |b(X_s)| ds \\le \\int_0^T \\frac{1}{2} ds = \\frac{T}{2}.\n$$\nSince the total variation is almost surely finite (in fact, deterministically bounded) on any finite interval $[0,T]$, $A_t$ is a process of finite variation. Since the solution $X_s$ has continuous paths, the integrand $s \\mapsto b(X_s(\\omega))$ is a continuous function of $s$ for almost every $\\omega$. The integral of a continuous function is itself continuous. Thus, $A_t$ is a continuous process of finite variation.\n\n**2. The Local Martingale Component, $M_t$:**\nThe process $M_t = \\int_0^t \\sigma dW_s$ is a stochastic integral with respect to a standard Brownian motion $W_t$. The integrand is the constant $\\sigma$. A stochastic integral $\\int_0^t H_s dW_s$ is a continuous local martingale provided the integrand $H_s$ is a predictable process satisfying $\\int_0^t H_s^2 ds  \\infty$ almost surely for all $t \\ge 0$. Here, the integrand is $H_s = \\sigma$, which is a deterministic constant and hence a predictable process. The condition is\n$$\n\\int_0^t \\sigma^2 ds = \\sigma^2 t  \\infty\n$$\nfor all finite $t$. This condition is clearly met. In fact, since $\\mathbb{E}\\left[\\int_0^t \\sigma^2 ds\\right] = \\sigma^2 t  \\infty$, the Itô integral $M_t$ is not just a local martingale, but a true martingale (specifically, a scaled Brownian motion, $M_t = \\sigma W_t$). It is a fundamental property of the Itô integral that its sample paths are continuous.\n\n**Conclusion:**\nThe process $X_t$ can be decomposed as $X_t = X_0 + A_t + M_t$, where $X_0=x_0$ is the constant initial value, $A_t$ is a continuous process of finite variation, and $M_t$ is a continuous local martingale. By definition, this makes $X_t$ a continuous semimartingale. Its canonical decomposition is $X_t = X_0 + M_t + A_t$.\n\nc) **Computation of the Quadratic Variation $[X]_t$**\n\nThe quadratic variation of a continuous semimartingale $X_t$ is defined as the limit in probability of the sum of squared increments over a partition $\\Pi = \\{0=t_0  t_1  \\dots  t_n = t\\}$ of the interval $[0, t]$ as the mesh $|\\Pi| = \\max_i(t_i - t_{i-1})$ goes to zero.\n$$\n[X]_t = \\lim_{|\\Pi| \\to 0} \\sum_{i=1}^n (X_{t_i} - X_{t_{i-1}})^2.\n$$\nWe use the semimartingale decomposition $X_t = X_0 + A_t + M_t$. Since $X_0$ is a constant, $[X]_t = [X - X_0]_t = [A + M]_t$. Using the bilinearity of the quadratic covariation, which follows from its definition as a limit of sums:\n$$\n[X]_t = [A + M]_t = [A]_t + 2[A, M]_t + [M]_t.\n$$\nWe evaluate each term based on fundamental principles:\n- **$[A]_t$**: $A_t$ is a continuous process of finite variation. For such a process, its quadratic variation is zero. To see this from the definition, let $V_t(A)$ be the total variation of $A$ on $[0,t]$. Then $\\sum_{i=1}^n |A_{t_i} - A_{t_{i-1}}|^2 \\le \\max_i|A_{t_i} - A_{t_{i-1}}| \\sum_{i=1}^n |A_{t_i} - A_{t_{i-1}}| \\le \\max_i|A_{t_i} - A_{t_{i-1}}| \\cdot V_t(A)$. Since $A_t$ has continuous paths, $\\max_i|A_{t_i} - A_{t_{i-1}}| \\to 0$ as $|\\Pi| \\to 0$. Since $V_t(A)$ is finite, the entire expression converges to $0$. Thus, $[A]_t=0$.\n\n- **$[A, M]_t$**: This is the quadratic covariation between a continuous finite variation process $A_t$ and a continuous local martingale $M_t$. Its value is zero. This can be shown using the Cauchy-Schwarz inequality on the defining sum:\n$$\n\\left| \\sum_{i=1}^n (A_{t_i}-A_{t_{i-1}})(M_{t_i}-M_{t_{i-1}}) \\right| \\le \\left( \\sum_{i=1}^n (A_{t_i}-A_{t_{i-1}})^2 \\right)^{1/2} \\left( \\sum_{i=1}^n (M_{t_i}-M_{t_{i-1}})^2 \\right)^{1/2}.\n$$\nIn the limit as $|\\Pi| \\to 0$, this inequality becomes $|[A, M]_t| \\le ([A]_t)^{1/2} ([M]_t)^{1/2}$. Since $[A]_t=0$, it follows that $[A, M]_t=0$.\n\n- **$[M]_t$**: The local martingale component is $M_t = \\int_0^t \\sigma dW_s$. Since $\\sigma$ is a constant, this simplifies to $M_t = \\sigma W_t$. The quadratic variation is then:\n$$\n[M]_t = [\\sigma W]_t = \\sigma^2 [W]_t.\n$$\nThis follows directly from the definition, as $\\sum (\\sigma(W_{t_i}-W_{t_{i-1}}))^2 = \\sigma^2 \\sum (W_{t_i}-W_{t_{i-1}})^2$. A fundamental property of standard Brownian motion is that its quadratic variation is $[W]_t = t$. Therefore, $[M]_t = \\sigma^2 t$.\n\nCombining these results, the quadratic variation of $X_t$ is:\n$$\n[X]_t = [A]_t + 2[A, M]_t + [M]_t = 0 + 2(0) + \\sigma^2 t = \\sigma^2 t.\n$$\n\nd) **Verification of the Semimartingale Structure**\n\nIn part (b), we identified the canonical semimartingale decomposition of the solution process $X_t$ as $X_t = X_0 + M_t + A_t$, where $M_t = \\int_0^t \\sigma dW_s$ is the continuous local martingale part and $A_t = \\int_0^t b(X_s) ds$ is the continuous finite-variation part.\n\nWe now use our computation of $[X]_t$ to verify this structure.\n1.  The finite-variation component is $A_t = \\int_0^t b(X_s) ds$. As argued in part (c), any continuous process of finite variation has zero quadratic variation. Therefore, $[A]_t = 0$. This confirms that the finite-variation component, by its very nature, has zero quadratic variation.\n2.  The local martingale component is $M_t = \\int_0^t \\sigma dW_s$. As shown in part (c), its quadratic variation is $[M]_t = \\sigma^2 t$.\n3.  The general theory for the quadratic variation of a semimartingale decomposition states that $[X]_t = [A+M]_t = [A]_t + 2[A,M]_t + [M]_t$. As shown, both $[A]_t$ and $[A,M]_t$ are zero. Thus, the theory predicts $[X]_t = [M]_t = \\sigma^2 t$.\n\nOur independent calculation in part (c) yielded $[X]_t = \\sigma^2 t$. This result matches the quadratic variation of the local martingale component, $[M]_t$, alone. This directly verifies that the identified semimartingale structure is correct, in the sense that the quadratic variation of the process $X_t$ is entirely captured by its local martingale part, while the finite-variation part makes no contribution.", "answer": "$$\n\\boxed{\\sigma^2 t}\n$$", "id": "2999129"}, {"introduction": "Having established the conditions for uniqueness, this practice challenges you to explore the boundaries of the theory where those conditions fail. You will investigate a classic SDE where the diffusion coefficient is not Lipschitz at the origin, leading to the breakdown of pathwise uniqueness. This exercise is critical for understanding that the standard theorems provide sufficient, but not necessary, conditions, and it introduces the more subtle analysis required to determine uniqueness in these degenerate cases.", "problem": "Consider the one-dimensional Stochastic Differential Equation (SDE) $dX_{t}=|X_{t}|^{\\alpha}\\,dW_{t}$ with initial condition $X_{0}=0$, where $W$ is a standard Brownian motion and $\\alpha\\in(0,1)$. Use only fundamental definitions and well-tested facts, including time-change representations of diffusions, the occupation-times formula with Brownian local time, and standard boundary classification heuristics derived from speed measures, to analyze pathwise uniqueness. \n\nYour task is to determine the single critical threshold exponent $\\alpha_{c}$ such that:\n- for all $\\alpha\\geq \\alpha_{c}$, pathwise uniqueness holds for the SDE started at $X_{0}=0$, and\n- for all $\\alpha\\alpha_{c}$, pathwise uniqueness fails for the SDE started at $X_{0}=0$.\n\nYou must justify the threshold using a time-change construction of weak solutions: represent any weak solution as a time-changed Brownian motion $B$ via the right-continuous inverse of an additive functional of the form $\\Gamma(t)=\\int_{0}^{t}g(B_{s})\\,ds$ for an appropriate measurable function $g$, and use the occupation-times formula $\\int_{0}^{t}f(B_{s})\\,ds=\\int_{\\mathbb{R}}f(x)\\,L_{t}^{x}(B)\\,dx$ to determine precisely when $\\Gamma$ explodes upon Brownian hits of $0$. Explicitly identify the role of the singularity at $x=0$ of the function $g$ that arises in this representation, and show how divergence versus convergence of $\\int_{0}^{\\varepsilon}x^{-2\\alpha}\\,dx$ near $0$ governs the presence or absence of freedom to insert nontrivial holding times at $0$ in the time change, which corresponds to failure of pathwise uniqueness.\n\nReport the numerical value of the threshold $\\alpha_{c}$. No rounding is required.", "solution": "The problem asks for the critical threshold exponent $\\alpha_c$ for pathwise uniqueness of the one-dimensional stochastic differential equation (SDE)\n$$\ndX_t = |X_t|^{\\alpha} \\, dW_t\n$$\nwith initial condition $X_0 = 0$, where $\\alpha \\in (0, 1)$ and $W$ is a standard Brownian motion.\n\nThe SDE is of the form $dX_t = \\sigma(X_t) \\, dW_t$, with a drift coefficient of $b(x) = 0$ and a diffusion coefficient of $\\sigma(x) = |x|^{\\alpha}$. The initial state is $x_0 = 0$. Since the coefficient $\\sigma(x)$ is not Lipschitz continuous at $x=0$ for $\\alpha \\in (0,1)$, standard existence and uniqueness theorems do not apply. We must analyze the structure of the SDE more closely.\n\nA central result for pathwise uniqueness of such SDEs is the Engelbert-Schmidt zero-one law, which relates uniqueness to the local integrability of $\\sigma(x)^{-2}$ around the starting point. Pathwise uniqueness for solutions starting at $x_0$ holds if and only if the integral $\\int_{U} \\sigma(x)^{-2} \\, dx$ is infinite for every neighborhood $U$ of $x_0$. In our case, $x_0=0$, so we must analyze the integral of $\\sigma(x)^{-2} = (|x|^{\\alpha})^{-2} = |x|^{-2\\alpha}$ in a neighborhood of $0$.\n\nThe problem specifically asks for a justification using a time-change construction, which we now develop. A key insight is that any weak solution to this SDE, being a continuous local martingale, can be represented as a time-changed Brownian motion.\n\nLet $X = \\{X_t\\}_{t \\geq 0}$ be any weak solution to the SDE starting at $X_0 = 0$. The trivial process $X_t \\equiv 0$ for all $t \\geq 0$ is a valid solution, since both sides of the SDE are identically zero. Pathwise uniqueness fails if and only if there exists at least one non-trivial solution.\n\nLet's construct solutions from a standard one-dimensional Brownian motion $B = \\{B_s\\}_{s \\geq 0}$ starting at $B_0=0$. A weak solution $X$ can be constructed by time-changing $B$. Define a continuous, non-decreasing process $A_s$, called the \"clock\" or additive functional, by\n$$\nA_s = \\int_0^s \\sigma(B_u)^{-2} \\, du = \\int_0^s |B_u|^{-2\\alpha} \\, du\n$$\nLet $\\tau_t$ be the right-continuous inverse of $A_s$:\n$$\n\\tau_t = \\inf\\{s \\geq 0 : A_s > t\\}\n$$\nThen, the process $X_t = B_{\\tau_t}$ is a weak solution to the SDE $dX_t = \\sigma(X_t) \\, dW_t$.\n\nFor a non-trivial solution to exist, the time-change $\\tau_t$ must be finite and non-zero for $t > 0$. This requires the clock process $A_s$ to be finite almost surely for all $s > 0$. If $A_s$ were to diverge to infinity for an arbitrarily small $s > 0$, then $\\tau_t$ would be identically zero for all $t$, leading to $X_t = B_0 = 0$, the trivial solution.\n\nThe core of the analysis is to determine when $A_s$ is finite. As instructed, we use the occupation-times formula, which relates the integral of a function of a Brownian path to an integral over the local time of the Brownian motion:\n$$\nA_s = \\int_0^s |B_u|^{-2\\alpha} \\, du = \\int_{-\\infty}^{\\infty} |x|^{-2\\alpha} L_s^x(B) \\, dx\n$$\nwhere $L_s^x(B)$ is the local time of the Brownian motion $B$ at level $x$ up to time $s$. We are interested in whether this integral is finite or infinite. The integrand $|x|^{-2\\alpha}$ has a singularity at $x=0$. The local time $L_s^x(B)$ is a continuous function of the spatial variable $x$. For any $s > 0$, the Brownian path will have visited a neighborhood of $0$, and $L_s^0(B) > 0$ almost surely.\n\nThe convergence or divergence of the integral for $A_s$ is determined by the local integrability of the function $|x|^{-2\\alpha}$ around $x=0$. That is, the behavior of $A_s$ is dictated by the behavior of the integral\n$$\nI(\\varepsilon) = \\int_{-\\varepsilon}^{\\varepsilon} |x|^{-2\\alpha} \\, dx\n$$\nfor any $\\varepsilon > 0$. Due to symmetry, this is\n$$\nI(\\varepsilon) = 2 \\int_0^{\\varepsilon} x^{-2\\alpha} \\, dx\n$$\nThis is a standard p-integral. It converges if the exponent is greater than $-1$, and diverges otherwise.\nThe condition for convergence is $-2\\alpha > -1$, which simplifies to $2\\alpha  1$, or $\\alpha  1/2$.\nThe condition for divergence is $-2\\alpha \\leq -1$, which simplifies to $2\\alpha \\geq 1$, or $\\alpha \\geq 1/2$.\n\nLet's analyze the two cases:\n\nCase 1: $\\alpha \\geq 1/2$.\nIn this case, the integral $\\int_0^{\\varepsilon} x^{-2\\alpha} \\, dx$ diverges. The function $|x|^{-2\\alpha}$ is not locally integrable at $0$. For any $s > 0$, the Brownian motion $B$ almost surely spends a positive amount of time (in the sense of local time) in any neighborhood of $0$. The divergence of the integral implies that the additive functional $A_s = \\int_0^s |B_u|^{-2\\alpha} \\, du$ is almost surely infinite for any $s > 0$.\nIf $A_s = \\infty$ for all $s>0$, then its inverse $\\tau_t = \\inf\\{s : A_s > t\\}$ is $0$ for all $t \\geq 0$. The time-changed process is $X_t = B_{\\tau_t} = B_0 = 0$. This construction yields only the trivial solution. This aligns with the Engelbert-Schmidt criterion: since $\\sigma(x)^{-2}$ is not locally integrable at $0$, pathwise uniqueness holds. Any solution must be the trivial one. The \"cost\" for the process to leave the origin is infinite, which prevents the construction of non-trivial paths.\n\nCase 2: $\\alpha  1/2$.\nIn this case, the integral $\\int_0^{\\varepsilon} x^{-2\\alpha} \\, dx$ converges. The function $|x|^{-2\\alpha}$ is locally integrable at $0$. This ensures that the additive functional $A_s = \\int_0^s |B_u|^{-2\\alpha} \\, du$ is finite almost surely for all finite $s$.\nConsequently, the inverse $\\tau_t = A_t^{-1}$ is a well-defined, strictly increasing, and finite process for $t \\geq 0$. The constructed process $X_t = B_{\\tau_t}$ is therefore a non-trivial solution to the SDE.\nSince we have the trivial solution $X_t \\equiv 0$ and we have just constructed a non-trivial solution $X_t = B_{\\tau_t}$, there are at least two distinct solutions starting from $X_0 = 0$. Therefore, pathwise uniqueness fails.\nThe problem mentions \"freedom to insert nontrivial holding times at $0$\". This is precisely what happens when $\\alpha  1/2$. Because the singularity is integrable, the point $0$ is not \"sticky\" enough to enforce a unique exit behavior. One can construct a family of solutions $\\{X_t^c\\}_{c \\geq 0}$ defined by:\n$$\nX_t^c =\n\\begin{cases}\n0  \\text{if } t \\leq c \\\\\nB_{\\tau_{t-c}}  \\text{if } t > c\n\\end{cases}\n$$\nEach of these processes is a valid weak solution that remains at $0$ for a duration $c$ before evolving non-trivially. The existence of this uncountable family of solutions is a strong manifestation of the failure of pathwise uniqueness.\n\nSummary of the findings:\n- For $\\alpha \\geq 1/2$, pathwise uniqueness holds.\n- For $\\alpha  1/2$, pathwise uniqueness fails.\n\nThe problem asks for the critical threshold $\\alpha_c$ such that for all $\\alpha \\geq \\alpha_c$, uniqueness holds, and for all $\\alpha  \\alpha_c$, uniqueness fails. Based on our analysis, this critical threshold is precisely at the point where the integral of $|x|^{-2\\alpha}$ switches from divergent to convergent.\nThis transition occurs when $-2\\alpha = -1$, i.e., at $\\alpha = 1/2$.\n\nTherefore, the critical threshold exponent is $\\alpha_c=1/2$.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "2999079"}, {"introduction": "This final practice bridges the gap between abstract existence and uniqueness theory and its profound implications for numerical computation. You will analyze the convergence of the Euler-Maruyama scheme, a fundamental algorithm for simulating SDEs, demonstrating that the same Lipschitz conditions ensuring a unique strong solution are also key to proving that the numerical approximation converges strongly to it. This exercise highlights the practical importance of a well-posed theoretical framework, showing how it provides the foundation for reliable computational methods.", "problem": "Consider the stochastic differential equation on the finite time interval $[0,T]$ given by\n$$\n\\mathrm{d}X(t) = b\\big(X(t)\\big)\\,\\mathrm{d}t + \\sigma\\big(X(t)\\big)\\,\\mathrm{d}W(t),\\qquad X(0)=\\xi,\n$$\nwhere $W$ is an $m$-dimensional standard Brownian motion on a filtered probability space satisfying the usual conditions, $X$ is $\\mathbb{R}^{d}$-valued, and the measurable coefficients $b:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ and $\\sigma:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times m}$ are globally Lipschitz and of linear growth: there exists $L\\ge 1$ such that for all $x,y\\in\\mathbb{R}^{d}$,\n$$\n|b(x)-b(y)| + \\|\\sigma(x)-\\sigma(y)\\| \\le L |x-y|,\\qquad |b(x)|^{2} + \\|\\sigma(x)\\|^{2} \\le L^{2}\\big(1+|x|^{2}\\big),\n$$\nand the initial condition satisfies $\\mathbb{E}|\\xi|^{2}\\infty$. By standard results, pathwise uniqueness holds and there exists a unique strong solution $X$.\n\nLet $N\\in\\mathbb{N}$ and $h:=T/N$. Define the Euler–Maruyama approximation $(X^{h}_{k})_{k=0,\\dots,N}$ on the uniform grid $t_{k}:=kh$ by\n$$\nX_{k+1}^{h} = X_{k}^{h} + b\\big(X_{k}^{h}\\big)\\,h + \\sigma\\big(X_{k}^{h}\\big)\\,\\Delta W_{k},\\qquad \\Delta W_{k}:=W(t_{k+1})-W(t_{k}),\n$$\nwith $X_{0}^{h}=\\xi$. Define also the continuous-time Euler–Maruyama interpolation $\\overline{X}^{h}:[0,T]\\to\\mathbb{R}^{d}$ by\n$$\n\\overline{X}^{h}(t) := X_{k}^{h} + b\\big(X_{k}^{h}\\big)\\,(t-t_{k}) + \\sigma\\big(X_{k}^{h}\\big)\\,\\big(W(t)-W(t_{k})\\big),\\qquad t\\in[t_{k},t_{k+1}).\n$$\n\nStarting from fundamental definitions for strong and weak solutions and uniqueness notions in stochastic differential equations, and using only well-tested tools such as Itô’s formula, the Burkholder–Davis–Gundy (BDG) inequality, and Gronwall’s inequality, perform the following:\n\n1. Establish stability in the sense of uniform second-moment bounds that are independent of $h$, both for the exact strong solution and for the Euler–Maruyama approximation.\n\n2. Derive a bound of the form\n$$\n\\mathbb{E}\\Big[\\sup_{0\\le t\\le T}\\big|X(t)-\\overline{X}^{h}(t)\\big|^{2}\\Big] \\le C\\, h^{r},\n$$\nfor all sufficiently small $h\\in(0,1]$, where $C$ is a constant depending only on $T$, $L$, $d$, $m$, and $\\mathbb{E}|\\xi|^{2}$ but not on $h$.\n\nDetermine the largest exponent $r$ for which such a bound can be proved under the stated assumptions.\n\nProvide your final answer as a single number $r$. No rounding is required.", "solution": "### Part 1: Stability Analysis (Uniform Second-Moment Bounds)\n\nWe aim to find a constant $K$, independent of $h$, such that $\\mathbb{E}\\big[\\sup_{0\\le t\\le T}|X(t)|^{2}\\big] \\le K$ and $\\mathbb{E}\\big[\\sup_{0\\le t\\le T}|\\overline{X}^{h}(t)|^{2}\\big] \\le K$.\n\n**A. Stability of the Exact Solution $X(t)$**\n\nThe SDE can be written in integral form:\n$$\nX(t) = \\xi + \\int_{0}^{t} b(X(s))\\,\\mathrm{d}s + \\int_{0}^{t} \\sigma(X(s))\\,\\mathrm{d}W(s).\n$$\nUsing the inequality $(a+b+c)^{2} \\le 3(a^2+b^2+c^2)$, we have\n$$\n|X(t)|^{2} \\le 3|\\xi|^{2} + 3\\left|\\int_{0}^{t} b(X(s))\\,\\mathrm{d}s\\right|^{2} + 3\\left\\|\\int_{0}^{t} \\sigma(X(s))\\,\\mathrm{d}W(s)\\right\\|^{2}.\n$$\nTaking the supremum over $[0, t]$ and then the expectation:\n$$\n\\mathbb{E}\\left[\\sup_{0\\le u\\le t}|X(u)|^{2}\\right] \\le 3\\mathbb{E}|\\xi|^{2} + 3\\mathbb{E}\\left[\\sup_{0\\le u\\le t}\\left|\\int_{0}^{u} b(X(s))\\,\\mathrm{d}s\\right|^{2}\\right] + 3\\mathbb{E}\\left[\\sup_{0\\le u\\le t}\\left\\|\\int_{0}^{u} \\sigma(X(s))\\,\\mathrm{d}W(s)\\right\\|^{2}\\right].\n$$\nFor the drift term, applying the Cauchy-Schwarz inequality:\n$$\n\\left|\\int_{0}^{u} b(X(s))\\,\\mathrm{d}s\\right|^{2} \\le \\left(\\int_{0}^{u} |b(X(s))|\\,\\mathrm{d}s\\right)^{2} \\le u \\int_{0}^{u} |b(X(s))|^{2}\\,\\mathrm{d}s \\le t \\int_{0}^{t} |b(X(s))|^{2}\\,\\mathrm{d}s.\n$$\nFor the diffusion term, applying the Burkholder-Davis-Gundy (BDG) inequality for $p=2$, there is a constant $C_{BDG} > 0$ such that:\n$$\n\\mathbb{E}\\left[\\sup_{0\\le u\\le t}\\left\\|\\int_{0}^{u} \\sigma(X(s))\\,\\mathrm{d}W(s)\\right\\|^{2}\\right] \\le C_{BDG} \\mathbb{E}\\left[\\int_{0}^{t} \\|\\sigma(X(s))\\|^{2}\\,\\mathrm{d}s\\right].\n$$\nCombining these, letting $\\phi(t) := \\mathbb{E}\\left[\\sup_{0\\le u\\le t}|X(u)|^{2}\\right]$, we get:\n$$\n\\phi(t) \\le 3\\mathbb{E}|\\xi|^{2} + 3 T \\mathbb{E}\\left[\\int_{0}^{t} |b(X(s))|^{2}\\,\\mathrm{d}s\\right] + 3 C_{BDG} \\mathbb{E}\\left[\\int_{0}^{t} \\|\\sigma(X(s))\\|^{2}\\,\\mathrm{d}s\\right].\n$$\nUsing the linear growth condition $|b(x)|^{2} + \\|\\sigma(x)\\|^{2} \\le L^{2}(1+|x|^{2})$:\n$$\n\\phi(t) \\le 3\\mathbb{E}|\\xi|^{2} + 3(T+C_{BDG}) \\mathbb{E}\\left[\\int_{0}^{t} L^{2}(1+|X(s)|^{2})\\,\\mathrm{d}s\\right].\n$$\nBy Fubini's theorem and noting that $\\mathbb{E}|X(s)|^{2} \\le \\phi(s)$:\n$$\n\\phi(t) \\le 3\\mathbb{E}|\\xi|^{2} + 3L^{2}(T+C_{BDG}) \\int_{0}^{t} (1+\\mathbb{E}|X(s)|^{2})\\,\\mathrm{d}s \\le 3\\mathbb{E}|\\xi|^{2} + 3L^{2}(T+C_{BDG})T + 3L^{2}(T+C_{BDG})\\int_{0}^{t}\\phi(s)\\,\\mathrm{d}s.\n$$\nThis is of the form $\\phi(t) \\le A + B \\int_0^t \\phi(s)\\,\\mathrm{d}s$. By Gronwall's inequality, $\\phi(t) \\le A\\exp(Bt)$.\nThus, for any $t \\in [0,T]$, $\\mathbb{E}\\left[\\sup_{0\\le u\\le T}|X(u)|^{2}\\right]$ is bounded by a constant depending on $T, L, C_{BDG}, \\mathbb{E}|\\xi|^{2}$, but not on $h$.\n\n**B. Stability of the EM Approximation $\\overline{X}^{h}(t)$**\n\nLet $\\pi(t) = t_k$ for $t \\in [t_k, t_{k+1})$. The continuous-time EM approximation satisfies:\n$$\n\\overline{X}^{h}(t) = \\xi + \\int_{0}^{t} b(\\overline{X}^{h}(\\pi(s)))\\,\\mathrm{d}s + \\int_{0}^{t} \\sigma(\\overline{X}^{h}(\\pi(s)))\\,\\mathrm{d}W(s).\n$$\nThe argument is identical to the one for $X(t)$. Let $\\psi(t) := \\mathbb{E}\\left[\\sup_{0\\le u\\le t}|\\overline{X}^{h}(u)|^{2}\\right]$. We arrive at:\n$$\n\\psi(t) \\le 3\\mathbb{E}|\\xi|^{2} + 3L^{2}(T+C_{BDG}) \\int_{0}^{t} (1+\\mathbb{E}|\\overline{X}^{h}(\\pi(s))|^{2})\\,\\mathrm{d}s.\n$$\nSince $\\pi(s) \\le s$, we have $\\mathbb{E}|\\overline{X}^{h}(\\pi(s))|^{2} \\le \\mathbb{E}\\left[\\sup_{0\\le u\\le s}|\\overline{X}^{h}(u)|^{2}\\right] = \\psi(s)$.\nThe inequality becomes $\\psi(t) \\le A + B \\int_0^t \\psi(s)\\,\\mathrm{d}s$, and Gronwall's inequality again yields a uniform bound, independent of $h$.\nSo, there exists a constant $K$ (which can be chosen as the larger of the two bounds) such that for any $h>0$,\n$$\n\\sup_{0 \\le t \\le T}\\mathbb{E}|X(t)|^{2} \\le \\mathbb{E}\\left[\\sup_{0\\le t\\le T}|X(t)|^{2}\\right] \\le K \\quad \\text{and} \\quad \\sup_{0 \\le t \\le T}\\mathbb{E}|\\overline{X}^{h}(t)|^{2} \\le \\mathbb{E}\\left[\\sup_{0\\le t\\le T}|\\overline{X}^{h}(t)|^{2}\\right] \\le K.\n$$\n\n### Part 2: Convergence Rate Analysis\n\nLet $e(t) := X(t) - \\overline{X}^{h}(t)$. Since $X(0) = \\overline{X}^{h}(0) = \\xi$, we have $e(0)=0$. The error process is:\n$$\ne(t) = \\int_{0}^{t} \\left[ b(X(s)) - b(\\overline{X}^{h}(\\pi(s))) \\right]\\,\\mathrm{d}s + \\int_{0}^{t} \\left[ \\sigma(X(s)) - \\sigma(\\overline{X}^{h}(\\pi(s))) \\right]\\,\\mathrm{d}W(s).\n$$\nWe decompose the differences in the integrands:\n$$\nb(X(s)) - b(\\overline{X}^{h}(\\pi(s))) = \\left[ b(X(s)) - b(\\overline{X}^{h}(s)) \\right] + \\left[ b(\\overline{X}^{h}(s)) - b(\\overline{X}^{h}(\\pi(s))) \\right].\n$$\nA similar decomposition holds for $\\sigma$. Using $(a+b+c+d)^2 \\le 4(a^2+b^2+c^2+d^2)$, we bound $|e(t)|^2$ by four terms. Taking sup and expectation:\n\\begin{align*}\n\\mathbb{E}\\left[\\sup_{0\\le s\\le t}|e(s)|^2\\right] \\le 4\\mathbb{E}\\left[\\sup_{0\\le s\\le t}\\left|\\int_{0}^{s} (b(X(u)) - b(\\overline{X}^{h}(u)))\\,\\mathrm{d}u\\right|^{2}\\right]  \\quad \\text{(Term 1)} \\\\\n+ 4\\mathbb{E}\\left[\\sup_{0\\le s\\le t}\\left|\\int_{0}^{s} (b(\\overline{X}^{h}(u)) - b(\\overline{X}^{h}(\\pi(u))))\\,\\mathrm{d}u\\right|^{2}\\right]  \\quad \\text{(Term 2)} \\\\\n+ 4\\mathbb{E}\\left[\\sup_{0\\le s\\le t}\\left\\|\\int_{0}^{s} (\\sigma(X(u)) - \\sigma(\\overline{X}^{h}(u)))\\,\\mathrm{d}W(u)\\right\\|^{2}\\right]  \\quad \\text{(Term 3)} \\\\\n+ 4\\mathbb{E}\\left[\\sup_{0\\le s\\le t}\\left\\|\\int_{0}^{s} (\\sigma(\\overline{X}^{h}(u)) - \\sigma(\\overline{X}^{h}(\\pi(u))))\\,\\mathrm{d}W(u)\\right\\|^{2}\\right]  \\quad \\text{(Term 4)}\n\\end{align*}\nLet $\\phi_{\\text{err}}(t) := \\mathbb{E}\\left[\\sup_{0\\le s\\le t}|e(s)|^2\\right]$.\n\n**Main Terms (1 and 3):**\nUsing C-S, BDG, and the Lipschitz condition, these terms are bounded by an integral of the error itself.\nTerm 1: $\\le 4T\\mathbb{E}\\left[\\int_0^t |b(X(u))-b(\\overline{X}^{h}(u))|^2\\,\\mathrm{d}u\\right] \\le 4TL^2 \\int_0^t \\mathbb{E}|e(u)|^2\\,\\mathrm{d}u \\le 4TL^2 \\int_0^t \\phi_{\\text{err}}(u)\\,\\mathrm{d}u.$\nTerm 3: $\\le 4C_{BDG}\\mathbb{E}\\left[\\int_0^t \\|\\sigma(X(u))-\\sigma(\\overline{X}^{h}(u))\\|^2\\,\\mathrm{d}u\\right] \\le 4C_{BDG}L^2 \\int_0^t \\mathbb{E}|e(u)|^2\\,\\mathrm{d}u \\le 4C_{BDG}L^2 \\int_0^t \\phi_{\\text{err}}(u)\\,\\mathrm{d}u.$\nThese terms will be handled by Gronwall's inequality.\n\n**Remainder Terms (2 and 4):**\nThese terms determine the convergence rate. Both depend on the quantity $|\\overline{X}^{h}(u) - \\overline{X}^{h}(\\pi(u))|^2$. For $u \\in [t_k, t_{k+1})$:\n$$\n\\overline{X}^{h}(u) - \\overline{X}^{h}(\\pi(u)) = b(X_k^h) (u-t_k) + \\sigma(X_k^h) (W(u)-W(t_k)).\n$$\nTaking the squared norm and expectation, the cross-term vanishes because $X_k^h$ is $\\mathcal{F}_{t_k}$-measurable and $\\mathbb{E}[W(u)-W(t_k)|\\mathcal{F}_{t_k}]=0$.\n\\begin{align*}\n\\mathbb{E}|\\overline{X}^{h}(u) - \\overline{X}^{h}(\\pi(u))|^2 = \\mathbb{E}\\left[|b(X_k^h)|^2 (u-t_k)^2\\right] + \\mathbb{E}\\left[\\|\\sigma(X_k^h) (W(u)-W(t_k))\\|^2\\right] \\\\\n= (u-t_k)^2 \\mathbb{E}|b(X_k^h)|^2 + (u-t_k) \\mathbb{E}\\|\\sigma(X_k^h)\\|^2.\n\\end{align*}\nUsing linear growth and the stability bound $K$, $\\mathbb{E}|b(X_k^h)|^2 \\le L^2(1+K)$ and $\\mathbb{E}\\|\\sigma(X_k^h)\\|^2 \\le L^2(1+K)$. Let $K' = L^2(1+K)$.\n$$\n\\mathbb{E}|\\overline{X}^{h}(u) - \\overline{X}^{h}(\\pi(u))|^2 \\le K'(u-t_k)^2 + K'(u-t_k).\n$$\nNow we bound the integrals that appear in Terms 2 and 4.\n$$\n\\int_0^T \\mathbb{E}|\\overline{X}^{h}(u) - \\overline{X}^{h}(\\pi(u))|^2\\,\\mathrm{d}u = \\sum_{k=0}^{N-1} \\int_{t_k}^{t_{k+1}} \\mathbb{E}|\\overline{X}^{h}(u) - X_k^h|^2\\,\\mathrm{d}u.\n$$\nThe integral is bounded by:\n$$\n\\sum_{k=0}^{N-1} \\int_{t_k}^{t_{k+1}} [K'(u-t_k)^2 + K'(u-t_k)]\\,\\mathrm{d}u = K' \\sum_{k=0}^{N-1} \\left[\\frac{(u-t_k)^3}{3} + \\frac{(u-t_k)^2}{2}\\right]_{t_k}^{t_{k+1}}.\n$$\n$$\n= K' \\sum_{k=0}^{N-1} \\left(\\frac{h^3}{3} + \\frac{h^2}{2}\\right) = K' N \\left(\\frac{h^3}{3} + \\frac{h^2}{2}\\right) = K' \\frac{T}{h} \\left(\\frac{h^3}{3} + \\frac{h^2}{2}\\right) = K' T \\left(\\frac{h^2}{3} + \\frac{h}{2}\\right).\n$$\nThis expression is of order $O(h)$. Let's use this to bound terms 2 and 4.\nTerm 2: $\\le 4TL^2 \\int_0^T \\mathbb{E}|\\overline{X}^{h}(u)-\\overline{X}^{h}(\\pi(u))|^2\\,\\mathrm{d}u = 4TL^2 \\cdot K'T(\\frac{h^2}{3} + \\frac{h}{2}) = O(h)$.\nTerm 4: $\\le 4C_{BDG}L^2 \\int_0^T \\mathbb{E}|\\overline{X}^{h}(u)-\\overline{X}^{h}(\\pi(u))|^2\\,\\mathrm{d}u = 4C_{BDG}L^2 \\cdot K'T(\\frac{h^2}{3} + \\frac{h}{2}) = O(h)$.\n\n**Combining and Concluding**\nPutting all bounds together, we find there exist constants $C_1, C_2$ independent of $h$ such that:\n$$\n\\phi_{\\text{err}}(t) \\le C_1 \\int_0^t \\phi_{\\text{err}}(u)\\,\\mathrm{d}u + C_2 h.\n$$\nBy Gronwall's inequality, $\\phi_{\\text{err}}(t) \\le C_2 h \\exp(C_1 t)$. For $t=T$, we have:\n$$\n\\mathbb{E}\\left[\\sup_{0\\le t\\le T}\\big|X(t)-\\overline{X}^{h}(t)\\big|^{2}\\right] \\le C h,\n$$\nwhere $C = C_2 \\exp(C_1 T)$. This establishes the bound for an exponent $r=1$.\n\nTo show that $r=1$ is the largest possible exponent, we consider the source of the $O(h)$ term. It originates from the term $\\int_0^T (u-\\pi(u)) \\mathbb{E}\\|\\sigma(\\dots)\\|^2\\,\\mathrm{d}u$, which arises from the variance of the Brownian increments, $\\mathbb{E}[(\\mathrm{d}W)^2] = \\mathrm{d}t$. As long as $\\sigma$ is not identically zero, this term is genuinely of order $h$ and cannot be improved under the given general assumptions. Any proof of convergence with rate $r1$ would require this term to be of a higher order in $h$, which is impossible. Thus, the derived rate is sharp.\n\nThe largest exponent for which the bound can be proved is $r=1$.", "answer": "$$ \\boxed{1} $$", "id": "2999128"}]}