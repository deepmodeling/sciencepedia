{"hands_on_practices": [{"introduction": "Understanding abstract theorems often begins with concrete examples. This first exercise [@problem_id:2986765] grounds the Martingale Representation Theorem in a direct calculation. By computing the Itô integral of a simple indicator function, you will explicitly construct a martingale and verify its properties, linking the integral's value and distribution directly to the fundamental characteristics of Brownian motion.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ be a filtered probability space supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\ge 0}$ with its usual augmentation. Fix constants $T0$ and $0\\le ab\\infty$. Consider the elementary predictable process $H=(H_s)_{s\\ge 0}$ defined by $H_s=\\mathbf{1}_{(a,b]}(s)$, and the Itô stochastic integral\n$$\nX_T \\coloneqq \\int_0^T H_s\\,dW_s.\n$$\nStarting only from the definition of the Itô integral for elementary predictable processes and the defining properties of Brownian motion (independent, stationary, Gaussian increments with variance equal to the increment length) together with the Itô isometry, do the following:\n- Compute $X_T$ explicitly in terms of $W$ and the times $a,b,T$.\n- Identify the distribution of $X_T$, its expectation, and its variance, all expressed in closed form as functions of $a,b,T$.\n- Briefly justify why the process $X_t\\coloneqq\\int_0^t \\mathbf{1}_{(a,b]}(s)\\,dW_s$ is a martingale in the Brownian filtration and explain how this example is consistent with the martingale representation property in Brownian filtrations.\n\nDefine $x\\wedge y\\coloneqq \\min\\{x,y\\}$. Your final answer must present, in a single expression, the value of $\\int_0^T \\mathbf{1}_{(a,b]}(s)\\,dW_s$, its law written as a centered normal with its variance parameter, its expectation, and its variance, in that order. No numerical approximation is required.", "solution": "The problem statement is a well-posed exercise in the theory of stochastic integration with respect to Brownian motion. It is scientifically sound, self-contained, and objective. All terms are standard in the field of stochastic differential equations. The problem is therefore valid. We proceed with the solution.\n\nThe problem asks for several quantities related to the Itô stochastic integral $X_T \\coloneqq \\int_0^T H_s\\,dW_s$, where $H_s = \\mathbf{1}_{(a,b]}(s)$ is an elementary predictable process, and $W_t$ is a standard one-dimensional Brownian motion. The constants satisfy $T0$ and $0 \\le a  b  \\infty$. The definition $x\\wedge y\\coloneqq \\min\\{x,y\\}$ is used.\n\n**1. Explicit Computation of $X_T$**\n\nThe Itô integral is defined over the time interval $[0, T]$. The integrand $H_s = \\mathbf{1}_{(a,b]}(s)$ is non-zero only for $s \\in (a, b]$. Therefore, the integral is effectively over the intersection of these two intervals, which is $(a, b] \\cap [0, T]$. Given that $0 \\le a$, this intersection is the interval $(a, b \\wedge T]$. The integral is non-zero only if this interval is non-empty, which means $a  b \\wedge T$.\n\nWe can formalize this by considering three cases based on the relationship between $T$ and the interval $(a, b]$.\n\nCase 1: $0  T \\le a$.\nFor any $s \\in [0, T]$, we have $s \\le a$, so $s \\notin (a, b]$. This implies $H_s = \\mathbf{1}_{(a,b]}(s) = 0$ for all $s$ in the integration interval $[0, T]$. Thus,\n$$ X_T = \\int_0^T 0 \\, dW_s = 0. $$\n\nCase 2: $a  T \\le b$.\nThe integral can be split using the additivity property:\n$$ X_T = \\int_0^T \\mathbf{1}_{(a,b]}(s) \\, dW_s = \\int_0^a \\mathbf{1}_{(a,b]}(s) \\, dW_s + \\int_a^T \\mathbf{1}_{(a,b]}(s) \\, dW_s. $$\nOn the interval $[0, a]$, the integrand is $0$. On the interval $(a, T]$, we have $a  s \\le T \\le b$, so the integrand is $1$. The integral becomes:\n$$ X_T = \\int_0^a 0 \\, dW_s + \\int_a^T 1 \\, dW_s = 0 + (W_T - W_a) = W_T - W_a. $$\n\nCase 3: $T  b$.\nWe split the integral at points $a$ and $b$:\n$$ X_T = \\int_0^a 0 \\, dW_s + \\int_a^b 1 \\, dW_s + \\int_b^T 0 \\, dW_s = 0 + (W_b - W_a) + 0 = W_b - W_a. $$\n\nWe can unify these three cases into a single expression using the minimum notation $x \\wedge y$. Let us test the expression $W_{b \\wedge T} - W_{a \\wedge T}$.\n- If $T \\le a$, then $a \\wedge T = T$ and $b \\wedge T = T$. The expression is $W_T - W_T = 0$. This matches Case 1.\n- If $a  T \\le b$, then $a \\wedge T = a$ and $b \\wedge T = T$. The expression is $W_T - W_a$. This matches Case 2.\n- If $T  b$, then $a \\wedge T = a$ and $b \\wedge T = b$. The expression is $W_b - W_a$. This matches Case 3.\n\nThus, the explicit form of the stochastic integral is:\n$$ X_T = W_{b \\wedge T} - W_{a \\wedge T}. $$\n\n**2. Distribution, Expectation, and Variance of $X_T$**\n\nThe random variable $X_T = W_{b \\wedge T} - W_{a \\wedge T}$ is an increment of a standard Brownian motion. Let $t_1 = a \\wedge T$ and $t_2 = b \\wedge T$. Since $a  b$, it follows that $a \\wedge T \\le b \\wedge T$, so $t_1 \\le t_2$.\nAccording to the properties of standard Brownian motion, the increment $W_{t_2} - W_{t_1}$ is a normally distributed random variable with mean $0$ and variance $t_2 - t_1$.\nTherefore, the distribution of $X_T$ is Gaussian (normal):\n$$ X_T \\sim \\mathcal{N}(0, (b \\wedge T) - (a \\wedge T)). $$\n\nFrom this distribution, we can directly identify the expectation and variance:\n- Expectation: $\\mathbb{E}[X_T] = 0$.\n- Variance: $\\text{Var}(X_T) = (b \\wedge T) - (a \\wedge T)$.\n\nWe can verify the variance using the Itô isometry, which states that $\\mathbb{E}[(\\int_0^T H_s dW_s)^2] = \\mathbb{E}[\\int_0^T H_s^2 ds]$. Since $H_s$ is a deterministic process, this simplifies to $\\int_0^T H_s^2 ds$.\n$$ \\text{Var}(X_T) = \\mathbb{E}[X_T^2] - (\\mathbb{E}[X_T])^2 = \\int_0^T (\\mathbf{1}_{(a,b]}(s))^2 ds - 0^2 = \\int_0^T \\mathbf{1}_{(a,b]}(s) ds. $$\nThis integral is the Lebesgue measure of the interval $(a,b] \\cap [0,T]$, which is $(b \\wedge T) - (a \\wedge T)$. This confirms our result.\n\n**3. Martingale Property of $X_t$**\n\nLet $X_t = \\int_0^t \\mathbf{1}_{(a,b]}(s) \\, dW_s$. Based on our previous calculation, we have $X_t = W_{b \\wedge t} - W_{a \\wedge t}$. To show that $(X_t)_{t \\ge 0}$ is a martingale with respect to the Brownian filtration $(\\mathcal{F}_t)_{t \\ge 0}$, we must verify three conditions:\n1.  $X_t$ is $\\mathcal{F}_t$-measurable for all $t \\ge 0$.\n    Since $a \\wedge t \\le t$ and $b \\wedge t \\le t$, both $W_{a \\wedge t}$ and $W_{b \\wedge t}$ are $\\mathcal{F}_t$-measurable. Their difference, $X_t$, is therefore also $\\mathcal{F}_t$-measurable. The process is adapted.\n2.  $\\mathbb{E}[|X_t|]  \\infty$ for all $t \\ge 0$.\n    As shown above, $X_t \\sim \\mathcal{N}(0, (b \\wedge t) - (a \\wedge t))$. The absolute moments of a Gaussian random variable are all finite. Thus, $X_t$ is integrable.\n3.  For all $s  t$, $\\mathbb{E}[X_t | \\mathcal{F}_s] = X_s$.\n    We use the property that Brownian motion itself is a martingale, which implies $\\mathbb{E}[W_u | \\mathcal{F}_s] = W_{u \\wedge s}$ for any $u \\ge 0$.\n    \\begin{align*}\n    \\mathbb{E}[X_t | \\mathcal{F}_s] = \\mathbb{E}[W_{b \\wedge t} - W_{a \\wedge t} | \\mathcal{F}_s] \\\\\n    = \\mathbb{E}[W_{b \\wedge t} | \\mathcal{F}_s] - \\mathbb{E}[W_{a \\wedge t} | \\mathcal{F}_s] \\\\\n    = W_{(b \\wedge t) \\wedge s} - W_{(a \\wedge t) \\wedge s}\n    \\end{align*}\n    Using the associativity of the minimum operator and the fact that $s  t \\implies s \\wedge t = s$, we have:\n    $$ (b \\wedge t) \\wedge s = b \\wedge (t \\wedge s) = b \\wedge s $$\n    $$ (a \\wedge t) \\wedge s = a \\wedge (t \\wedge s) = a \\wedge s $$\n    Substituting these back, we get:\n    $$ \\mathbb{E}[X_t | \\mathcal{F}_s] = W_{b \\wedge s} - W_{a \\wedge s} = X_s. $$\nAll three conditions are met, hence $(X_t)_{t\\ge 0}$ is a martingale. This is consistent with the general theorem that Itô integrals of square-integrable predictable processes are martingales.\n\n**4. Consistency with Martingale Representation**\n\nThe martingale representation theorem states that any martingale $(M_t)_{t \\ge 0}$ relative to the Brownian filtration $(\\mathcal{F}_t)_{t \\ge 0}$ can be represented as a stochastic integral of the form $M_t = M_0 + \\int_0^t K_s \\, dW_s$ for some predictable process $K_s$.\n\nOur process $X_t = \\int_0^t \\mathbf{1}_{(a,b]}(s) \\, dW_s$ is constructed as such a stochastic integral (with $X_0=0$ and $K_s = \\mathbf{1}_{(a,b]}(s)$), and we have just proven it is a martingale. This provides a direct, constructive example of the principle underlying the representation theorem. We have built a martingale from a stochastic integral.\n\nConversely, a related version of the theorem (the Clark-Ocone formula) states that any $\\mathcal{F}_T$-measurable, square-integrable random variable $F$ can be written as $F = \\mathbb{E}[F] + \\int_0^T K_s \\, dW_s$. Our random variable is $X_T = W_{b \\wedge T} - W_{a \\wedge T}$. It is $\\mathcal{F}_T$-measurable and square-integrable. Its expectation is $0$. The theorem guarantees the existence of a representing integrand $K_s$. This problem explicitly provides it: $K_s = \\mathbf{1}_{(a,b]}(s)$. This example thus serves as a concrete illustration of the martingale representation property.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nW_{b \\wedge T} - W_{a \\wedge T}  \\mathcal{N}\\left(0, (b \\wedge T) - (a \\wedge T)\\right)  0  (b \\wedge T) - (a \\wedge T)\n\\end{pmatrix}\n}\n$$", "id": "2986765"}, {"introduction": "Having seen how a stochastic integral forms a martingale, we now tackle the inverse problem: finding the integrand for a given random variable. This practice [@problem_id:772770] introduces a cornerstone technique that involves constructing a martingale through conditional expectation and then applying Itô's formula to reveal its differential structure. Mastering this method is essential for explicitly determining the martingale representation of complex financial and physical models.", "problem": "Let $\\{W_t\\}_{t \\ge 0}$ be a standard one-dimensional Brownian motion on a probability space $(\\Omega, \\mathcal{F}, P)$, and let $\\{\\mathcal{F}_t\\}_{t \\ge 0}$ be the natural filtration generated by $W_t$, satisfying the usual conditions. The Martingale Representation Theorem states that any random variable $R$ that is square-integrable and measurable with respect to the sigma-algebra $\\mathcal{F}_T$ for some fixed time $T  0$ can be uniquely represented in the form:\n$$\nR = \\mathbb{E}[R] + \\int_0^T H_s dW_s\n$$\nwhere $H_s$ is a unique predictable process with respect to the filtration $\\{\\mathcal{F}_s\\}_{s \\ge 0}$ such that $\\int_0^T \\mathbb{E}[H_s^2] ds  \\infty$.\n\nConsider the random variable $R$ defined by the time-integral of the Brownian motion path up to time $T$:\n$$\nR = \\int_0^T W_u du\n$$\nThis random variable is $\\mathcal{F}_T$-measurable and square-integrable. Your task is to derive the explicit functional form of the integrand process $H_s$ for $s \\in [0, T]$ in the martingale representation of $R$.", "solution": "The goal is to find the predictable process $H_s$ in the martingale representation of the random variable $R = \\int_0^T W_u du$. The representation is given by:\n$$\nR = \\mathbb{E}[R] + \\int_0^T H_s dW_s\n$$\n\nFirst, we calculate the expected value of $R$. Since the expected value of a standard Brownian motion at any time $u$ is $\\mathbb{E}[W_u] = 0$, we have:\n$$\n\\mathbb{E}[R] = \\mathbb{E}\\left[\\int_0^T W_u du\\right] = \\int_0^T \\mathbb{E}[W_u] du = \\int_0^T 0 \\, du = 0\n$$\nSo the representation simplifies to $R = \\int_0^T H_s dW_s$.\n\nTo find $H_s$, we consider the martingale $M_t$ defined by the conditional expectation of $R$ with respect to the filtration $\\mathcal{F}_t$:\n$$\nM_t = \\mathbb{E}[R | \\mathcal{F}_t], \\quad t \\in [0, T]\n$$\nBy the definition of conditional expectation, $M_t$ is a martingale. We have $M_T = \\mathbb{E}[R | \\mathcal{F}_T] = R$ (since $R$ is $\\mathcal{F}_T$-measurable) and $M_0 = \\mathbb{E}[R | \\mathcal{F}_0] = \\mathbb{E}[R] = 0$.\n\nAccording to the Martingale Representation Theorem, any $\\{\\mathcal{F}_t\\}$-martingale can be represented as a stochastic integral with respect to the Brownian motion $W_t$. Thus, there exists a predictable process, which we can also denote by $H_s$, such that:\n$$\nM_t = M_0 + \\int_0^t H_s dW_s\n$$\nIn differential form, this is $dM_t = H_t dW_t$. Our strategy is to find an explicit expression for $M_t$ and then compute its stochastic differential to identify $H_t$.\n\nLet's express $M_t$ using the definition of $R$:\n$$\nM_t = \\mathbb{E}\\left[\\int_0^T W_u du \\bigg| \\mathcal{F}_t\\right]\n$$\nUsing the linearity of conditional expectation and the integral, we can write:\n$$\nM_t = \\int_0^T \\mathbb{E}[W_u | \\mathcal{F}_t] du\n$$\nWe split the integral at time $t$:\n$$\nM_t = \\int_0^t \\mathbb{E}[W_u | \\mathcal{F}_t] du + \\int_t^T \\mathbb{E}[W_u | \\mathcal{F}_t] du\n$$\nNow we evaluate the conditional expectation $\\mathbb{E}[W_u | \\mathcal{F}_t]$:\n1.  For $u \\le t$: $W_u$ is known at time $t$, so it is $\\mathcal{F}_t$-measurable. Thus, $\\mathbb{E}[W_u | \\mathcal{F}_t] = W_u$.\n2.  For $u  t$: We can write $W_u = W_t + (W_u - W_t)$. The increment $W_u - W_t$ is independent of the past information $\\mathcal{F}_t$ and has mean zero. Therefore:\n    $$\n    \\mathbb{E}[W_u | \\mathcal{F}_t] = \\mathbb{E}[W_t + (W_u - W_t) | \\mathcal{F}_t] = \\mathbb{E}[W_t | \\mathcal{F}_t] + \\mathbb{E}[W_u - W_t | \\mathcal{F}_t] = W_t + 0 = W_t\n    $$\n\nSubstituting these back into the expression for $M_t$:\n$$\nM_t = \\int_0^t W_u du + \\int_t^T W_t du\n$$\nThe second integral is with respect to $u$, and $W_t$ is constant in this context:\n$$\n\\int_t^T W_t du = W_t \\int_t^T du = W_t [u]_t^T = W_t (T-t)\n$$\nSo, the explicit expression for the martingale is:\n$$\nM_t = \\int_0^t W_u du + (T-t) W_t\n$$\nNow, we find the stochastic differential $dM_t$. Let's analyze the two terms separately.\nThe first term is $\\int_0^t W_u du$. Its differential is simply $W_t dt$.\nThe second term is $(T-t)W_t$. We use Itô's formula for a function $f(t, x) = (T-t)x$, with $x=W_t$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial t} = -x, \\quad \\frac{\\partial f}{\\partial x} = T-t, \\quad \\frac{\\partial^2 f}{\\partial x^2} = 0\n$$\nItô's formula for $df(t, W_t)$ is:\n$$\ndf = \\left(\\frac{\\partial f}{\\partial t} + \\mu_t \\frac{\\partial f}{\\partial x} + \\frac{1}{2} \\sigma_t^2 \\frac{\\partial^2 f}{\\partial x^2}\\right)dt + \\sigma_t \\frac{\\partial f}{\\partial x} dW_t\n$$\nFor $W_t$, we have $dW_t = 0 \\cdot dt + 1 \\cdot dW_t$, so $\\mu_t = 0$ and $\\sigma_t = 1$.\n$$\nd((T-t)W_t) = (-W_t + 0 \\cdot(T-t) + \\frac{1}{2}\\cdot 1^2 \\cdot 0)dt + 1 \\cdot (T-t) dW_t = -W_t dt + (T-t) dW_t\n$$\nCombining the differentials:\n$$\ndM_t = d\\left(\\int_0^t W_u du\\right) + d((T-t)W_t) = W_t dt + [-W_t dt + (T-t)dW_t]\n$$\nThe $dt$ terms cancel out:\n$$\ndM_t = (W_t - W_t)dt + (T-t)dW_t = (T-t)dW_t\n$$\nComparing this with the general form $dM_t = H_t dW_t$, we can identify the integrand process $H_t$:\n$$\nH_t = T-t\n$$\nThe question asks for the process $H_s$ for $s \\in [0, T]$, which is obtained by replacing the time variable $t$ with $s$.\n$$\nH_s = T-s\n$$", "answer": "$$ \\boxed{T-s} $$", "id": "772770"}, {"introduction": "The Martingale Representation Theorem is deeply connected to the orthogonal structure of the space of square-integrable random variables, described by the Wiener-Itô chaos expansion. This exercise [@problem_id:2986764] explores this link by asking you to approximate a fundamental object—the Doléans-Dade exponential—using its projection onto the simplest non-trivial chaos spaces. By calculating the mean-square error of this truncation, you will gain a tangible understanding of how the representation theorem facilitates approximation and quantifies the energy contained in higher-order complexities.", "problem": "Consider a complete probability space carrying a one-dimensional standard Brownian motion $W=\\{W_{t}\\}_{t\\in[0,T]}$ with its natural filtration $\\{\\mathcal{F}_{t}\\}_{t\\in[0,T]}$ satisfying the usual conditions. Let $T0$ and let $\\lambda\\in\\mathbb{R}$. Define the random variable\n$$\nX \\;=\\; \\exp\\!\\big(\\lambda W_{T} - \\tfrac{1}{2}\\lambda^{2}T\\big),\n$$\nwhich is square-integrable and $\\mathcal{F}_{T}$-measurable. The Wiener-Itô chaos decomposition relative to the Brownian filtration orthogonally splits $L^{2}(\\mathcal{F}_{T})$ into the direct sum of the zeroth chaos (constants), the first chaos (closures of stochastic integrals of deterministic functions against $W$), and higher-order chaoses.\n\nUsing only fundamental definitions and facts about Brownian motion and the Wiener-Itô chaos, approximate $X$ by its orthogonal projection onto the sum of the zeroth and first Wiener chaos (i.e., truncate the chaos expansion at order one). Express this approximation explicitly as an object of the form $a + \\int_{0}^{T}\\varphi(s)\\,\\mathrm{d}W_{s}$ with deterministic $\\varphi$. Then, quantify the mean-square approximation error by the tail of the chaos expansion. Your final answer should be the closed-form expression for\n$$\n\\mathbb{E}\\!\\left[\\big|X - X^{(1)}\\big|^{2}\\right],\n$$\nwhere $X^{(1)}$ denotes the zeroth-plus-first chaos truncation of $X$. No rounding is needed; present an exact analytic expression in terms of $\\lambda$ and $T$.", "solution": "The problem requires us to find the orthogonal projection of the random variable $X = \\exp(\\lambda W_{T} - \\frac{1}{2}\\lambda^{2}T)$ onto the subspace spanned by the zeroth and first Wiener chaos, and then to compute the mean-square error of this approximation. Let this projection be denoted by $X^{(1)}$. The space $L^{2}(\\mathcal{F}_{T})$ of square-integrable, $\\mathcal{F}_{T}$-measurable random variables forms a Hilbert space with the inner product $\\langle Y, Z \\rangle = \\mathbb{E}[YZ]$. The Wiener-Itô chaos decomposition provides an orthogonal basis for this space.\n\nThe projection $X^{(1)}$ is the element in the subspace of zeroth-plus-first chaos that is closest to $X$ in the $L^{2}$ norm. This subspace consists of all random variables of the form $c + \\int_{0}^{T} \\psi(s) \\, \\mathrm{d}W_s$, where $c$ is a constant and $\\psi \\in L^{2}([0,T])$ is a deterministic function. By the properties of orthogonal projections in a Hilbert space, $X^{(1)}$ is the sum of the projections of $X$ onto the zeroth chaos and the first chaos, respectively.\n\nThe projection of any random variable $Y \\in L^{2}(\\mathcal{F}_T)$ onto the zeroth chaos (the space of constants) is its expectation, $\\mathbb{E}[Y]$.\nThe projection of $Y$ onto the first chaos (the space of stochastic integrals of deterministic functions) is given by $\\int_{0}^{T} \\mathbb{E}[D_s Y] \\, \\mathrm{d}W_s$, where $D_s$ is the Malliavin derivative operator.\n\nLet us first determine $X^{(1)}$. The random variable $X$ is the value at time $T$ of the stochastic process $M_t = \\exp(\\lambda W_t - \\frac{1}{2}\\lambda^2 t)$. This process is a martingale, known as the Doléans-Dade exponential or stochastic exponential of $\\lambda W_t$. As a martingale starting at $M_0 = 1$, its expectation is constant, so $\\mathbb{E}[X] = \\mathbb{E}[M_T] = M_0 = 1$. Thus, the projection of $X$ onto the zeroth chaos is the constant $a=1$.\n\nNext, we find the projection onto the first chaos. We compute the Malliavin derivative of $X$. For a random variable of the form $g(W_T)$, its Malliavin derivative at time $s \\in [0,T]$ is $D_s g(W_T) = g'(W_T)$. In our case, $g(w) = \\exp(\\lambda w - \\frac{1}{2}\\lambda^2 T)$, so $g'(w) = \\lambda \\exp(\\lambda w - \\frac{1}{2}\\lambda^2 T) = \\lambda g(w)$.\nTherefore, for $s \\in [0,T]$, the Malliavin derivative of $X$ is\n$$\nD_s X = \\lambda X\n$$\nThe deterministic kernel $\\varphi(s)$ for the first-chaos projection is the expectation of this derivative:\n$$\n\\varphi(s) = \\mathbb{E}[D_s X] = \\mathbb{E}[\\lambda X] = \\lambda \\mathbb{E}[X] = \\lambda \\cdot 1 = \\lambda\n$$\nThe function $\\varphi(s) = \\lambda$ is a constant function over $[0,T]$. The projection of $X$ onto the first chaos is then\n$$\n\\int_{0}^{T} \\varphi(s) \\, \\mathrm{d}W_s = \\int_{0}^{T} \\lambda \\, \\mathrm{d}W_s = \\lambda \\int_{0}^{T} \\mathrm{d}W_s = \\lambda W_T\n$$\nCombining the zeroth and first chaos projections, we obtain the approximation $X^{(1)}$:\n$$\nX^{(1)} = 1 + \\lambda W_T\n$$\nThis is the explicit expression for the approximation in the required form $a + \\int_{0}^{T}\\varphi(s)\\,\\mathrm{d}W_{s}$ with $a=1$ and $\\varphi(s)=\\lambda$.\n\nNow, we must quantify the mean-square approximation error, $\\mathbb{E}[|X - X^{(1)}|^2]$. Since $X^{(1)}$ is the orthogonal projection of $X$ onto the subspace of first-order chaos, the error vector $X - X^{(1)}$ is orthogonal to the subspace, and thus orthogonal to $X^{(1)}$. By the Pythagorean theorem in the Hilbert space $L^2(\\mathcal{F}_T)$, we have:\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}[|X^{(1)} + (X - X^{(1)})|^2] = \\mathbb{E}[(X^{(1)})^2] + \\mathbb{E}[|X - X^{(1)}|^2]\n$$\nThis directly gives the error as:\n$$\n\\mathbb{E}[|X - X^{(1)}|^2] = \\mathbb{E}[X^2] - \\mathbb{E}[(X^{(1)})^2]\n$$\nWe proceed to compute the two terms on the right-hand side.\n\nFirst, we compute $\\mathbb{E}[X^2]$:\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}\\left[\\left(\\exp\\big(\\lambda W_{T} - \\tfrac{1}{2}\\lambda^{2}T\\big)\\right)^2\\right] = \\mathbb{E}\\left[\\exp\\big(2\\lambda W_{T} - \\lambda^{2}T\\big)\\right]\n$$\nWe can factor out the deterministic part:\n$$\n\\mathbb{E}[X^2] = \\exp(-\\lambda^2 T) \\mathbb{E}[\\exp(2\\lambda W_T)]\n$$\nThe random variable $W_T$ is normally distributed with mean $0$ and variance $T$, i.e., $W_T \\sim N(0, T)$. The moment-generating function of a normal random variable $Z \\sim N(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(sZ)] = \\exp(s\\mu + \\frac{1}{2}s^2\\sigma^2)$. Applying this for $W_T$ with $\\mu=0$, $\\sigma^2=T$, and $s=2\\lambda$:\n$$\n\\mathbb{E}[\\exp(2\\lambda W_T)] = \\exp\\left(2\\lambda \\cdot 0 + \\frac{1}{2}(2\\lambda)^2 T\\right) = \\exp\\left(\\frac{1}{2} \\cdot 4\\lambda^2 T\\right) = \\exp(2\\lambda^2 T)\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[X^2] = \\exp(-\\lambda^2 T) \\exp(2\\lambda^2 T) = \\exp(\\lambda^2 T)\n$$\n\nSecond, we compute $\\mathbb{E}[(X^{(1)})^2]$:\n$$\n\\mathbb{E}[(X^{(1)})^2] = \\mathbb{E}[(1 + \\lambda W_T)^2] = \\mathbb{E}[1 + 2\\lambda W_T + \\lambda^2 W_T^2]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[(X^{(1)})^2] = \\mathbb{E}[1] + 2\\lambda\\mathbb{E}[W_T] + \\lambda^2\\mathbb{E}[W_T^2]\n$$\nUsing the moments of $W_T \\sim N(0,T)$, we have $\\mathbb{E}[W_T] = 0$ and $\\text{Var}(W_T) = \\mathbb{E}[W_T^2] - (\\mathbb{E}[W_T])^2 = T$, which implies $\\mathbb{E}[W_T^2] = T$.\nSubstituting these values:\n$$\n\\mathbb{E}[(X^{(1)})^2] = 1 + 2\\lambda(0) + \\lambda^2(T) = 1 + \\lambda^2 T\n$$\n\nFinally, we compute the mean-square error by subtracting the second result from the first:\n$$\n\\mathbb{E}[|X - X^{(1)}|^2] = \\mathbb{E}[X^2] - \\mathbb{E}[(X^{(1)})^2] = \\exp(\\lambda^2 T) - (1 + \\lambda^2 T)\n$$\nThis expression represents the energy of $X$ contained in the higher-order chaos terms (from order $2$ upwards). The result is an exact analytical expression in terms of $\\lambda$ and $T$.", "answer": "$$\n\\boxed{\\exp(\\lambda^{2}T) - 1 - \\lambda^{2}T}\n$$", "id": "2986764"}]}