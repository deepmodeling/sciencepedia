{"hands_on_practices": [{"introduction": "The power of the Cameron-Martin-Girsanov theorem hinges on the properties of the Doléans-Dade exponential, the process used to change the probability measure. This first practice focuses on the fundamental requirement that this process, often denoted $Z_t$, is a true martingale. By verifying the conditions that ensure this property and applying the Optional Stopping Theorem, you will build a solid foundation for understanding why the change of measure is well-defined and preserves expectations as the theorem requires [@problem_id:3000259].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ satisfying the usual conditions, supporting a one-dimensional standard Brownian motion $W=(W_{t})_{t\\geq 0}$. Fix a finite horizon $T0$. Let $\\theta=(\\theta_{t})_{t\\in[0,T]}$ be a progressively measurable process such that $|\\theta_{t}|\\leq K$ almost surely for all $t\\in[0,T]$, for some constant $K\\infty$, and $\\int_{0}^{T}\\theta_{s}^{2}\\,\\mathrm{d}s\\infty$ almost surely. Define the Doléans-Dade exponential\n$$\nZ_{t} \\;=\\; \\exp\\!\\Bigg(\\,\\int_{0}^{t}\\theta_{s}\\,\\mathrm{d}W_{s}\\;-\\;\\frac{1}{2}\\int_{0}^{t}\\theta_{s}^{2}\\,\\mathrm{d}s\\,\\Bigg),\\qquad t\\in[0,T].\n$$\nLet $\\tau$ be an $(\\mathcal{F}_{t})$-stopping time satisfying $0\\leq \\tau\\leq T$ almost surely.\n\nStarting only from the definitions of Brownian motion, stochastic integrals, quadratic variation, and Itô calculus (including Itô's formula), and using well-tested sufficient conditions for true martingales such as Novikov's condition, carry out the following:\n\n- Establish that $(Z_{t})_{t\\in[0,T]}$ is a true martingale with $\\mathbb{E}[Z_{t}]=1$ for all $t\\in[0,T]$.\n- Justify that the stopped process $(Z_{t\\wedge \\tau})_{t\\in[0,T]}$ is a martingale.\n- Apply the optional stopping theorem to evaluate $\\mathbb{E}[Z_{\\tau}]$.\n\nYour final answer must be a single real number. No rounding is required. No physical units are involved.", "solution": "The problem requires us to first establish that the process $(Z_t)_{t\\in[0,T]}$ is a true martingale, and then to use this property along with the Optional Stopping Theorem to compute the expectation $\\mathbb{E}[Z_{\\tau}]$.\n\nThe process $Z_t$ is the Doléans-Dade exponential, defined as\n$$\nZ_{t} \\;=\\; \\exp\\!\\Bigg(\\,\\int_{0}^{t}\\theta_{s}\\,\\mathrm{d}W_{s}\\;-\\;\\frac{1}{2}\\int_{0}^{t}\\theta_{s}^{2}\\,\\mathrm{d}s\\,\\Bigg).\n$$\nLet us define the process $X_t$ as the argument of the exponential:\n$$\nX_{t} \\;=\\; \\int_{0}^{t}\\theta_{s}\\,\\mathrm{d}W_{s}\\;-\\;\\frac{1}{2}\\int_{0}^{t}\\theta_{s}^{2}\\,\\mathrm{d}s.\n$$\nIn differential form, this is $\\mathrm{d}X_t = \\theta_t\\,\\mathrm{d}W_t - \\frac{1}{2}\\theta_t^2\\,\\mathrm{d}t$. The process $Z_t$ can be written as $Z_t = f(X_t)$ where $f(x) = \\exp(x)$. We apply Itô's formula to find the stochastic differential of $Z_t$. The derivatives of $f(x)$ are $f'(x) = \\exp(x)$ and $f''(x) = \\exp(x)$.\n\nThe quadratic variation of $X_t$ is given by\n$$\n\\mathrm{d}\\langle X \\rangle_{t} = \\left(\\theta_t\\right)^2 \\mathrm{d}\\langle W \\rangle_{t} = \\theta_t^2\\,\\mathrm{d}t.\n$$\nAccording to Itô's formula, the differential of $Z_t = f(X_t)$ is:\n$$\n\\mathrm{d}Z_t = f'(X_t)\\,\\mathrm{d}X_t + \\frac{1}{2}f''(X_t)\\,\\mathrm{d}\\langle X \\rangle_t.\n$$\nSubstituting the expressions for the derivatives and differentials, we get\n$$\n\\mathrm{d}Z_t = \\exp(X_t)\\left(\\theta_t\\,\\mathrm{d}W_t - \\frac{1}{2}\\theta_t^2\\,\\mathrm{d}t\\right) + \\frac{1}{2}\\exp(X_t)\\left(\\theta_t^2\\,\\mathrm{d}t\\right).\n$$\nRecognizing that $\\exp(X_t) = Z_t$, this simplifies to\n$$\n\\mathrm{d}Z_t = Z_t \\theta_t\\,\\mathrm{d}W_t - \\frac{1}{2}Z_t \\theta_t^2\\,\\mathrm{d}t + \\frac{1}{2}Z_t \\theta_t^2\\,\\mathrm{d}t = Z_t \\theta_t\\,\\mathrm{d}W_t.\n$$\nIn integral form, this equation becomes\n$$\nZ_t = Z_0 + \\int_0^t Z_s \\theta_s\\,\\mathrm{d}W_s.\n$$\nThe initial value is $Z_0 = \\exp(0-0) = 1$. Thus,\n$$\nZ_t = 1 + \\int_0^t Z_s \\theta_s\\,\\mathrm{d}W_s.\n$$\nThe process $\\theta_t$ is progressively measurable and bounded, and $Z_t$ is adapted and has continuous paths. Therefore, the product $Z_t \\theta_t$ is a progressively measurable process satisfying the standard integrability conditions for the Itô integral. The stochastic integral $\\int_0^t Z_s \\theta_s\\,\\mathrm{d}W_s$ is a continuous local martingale. Since $Z_t$ is the sum of a constant and a continuous local martingale, $(Z_t)_{t\\in[0,T]}$ is itself a continuous local martingale.\n\nTo establish that $(Z_t)_{t\\in[0,T]}$ is a true martingale, we verify Novikov's condition. The process $Z_t$ is the stochastic exponential of the process $M_t = \\int_0^t \\theta_s\\,\\mathrm{d}W_s$, so $Z_t = \\mathcal{E}(M)_t$. Novikov's condition states that if $\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\langle M \\rangle_T\\right)\\right]  \\infty$, then $\\mathcal{E}(M)_t$ is a true martingale on $[0,T]$. The quadratic variation of $M_t$ is $\\langle M \\rangle_t = \\int_0^t \\theta_s^2\\,\\mathrm{d}s$. We must check if\n$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T \\theta_s^2\\,\\mathrm{d}s\\right)\\right]  \\infty.\n$$\nWe are given that the process $\\theta_t$ is bounded, i.e., there exists a constant $K  \\infty$ such that $|\\theta_t| \\leq K$ almost surely for all $t\\in[0,T]$. This implies $\\theta_t^2 \\leq K^2$ a.s. Therefore, the integral is bounded:\n$$\n\\int_0^T \\theta_s^2\\,\\mathrm{d}s \\leq \\int_0^T K^2\\,\\mathrm{d}s = K^2T \\quad \\text{a.s.}\n$$\nThis implies that the random variable inside the expectation is bounded:\n$$\n\\exp\\left(\\frac{1}{2}\\int_0^T \\theta_s^2\\,\\mathrm{d}s\\right) \\leq \\exp\\left(\\frac{1}{2}K^2T\\right) \\quad \\text{a.s.}\n$$\nSince $\\exp(\\frac{1}{2}K^2T)$ is a finite constant, the expectation is also finite:\n$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T \\theta_s^2\\,\\mathrm{d}s\\right)\\right] \\leq \\exp\\left(\\frac{1}{2}K^2T\\right)  \\infty.\n$$\nNovikov's condition is satisfied, which proves that $(Z_t)_{t\\in[0,T]}$ is a true martingale.\nFor any martingale, the expectation is constant over time. Therefore, for any $t \\in [0,T]$,\n$$\n\\mathbb{E}[Z_t] = \\mathbb{E}[Z_0] = \\mathbb{E}[1] = 1.\n$$\nThis completes the first part of the task.\n\nNext, we must justify that the stopped process $(Z_{t\\wedge \\tau})_{t\\in[0,T]}$ is a martingale. This is a standard result from martingale theory: if $(M_t)$ is a martingale with respect to a filtration $(\\mathcal{F}_t)$ and $\\tau$ is an $(\\mathcal{F}_t)$-stopping time, then the stopped process $(M_{t\\wedge \\tau})$ is also a martingale. Since we have established that $(Z_t)_{t\\in[0,T]}$ is a true martingale and $\\tau$ is an $(\\mathcal{F}_t)$-stopping time, we conclude that $(Z_{t\\wedge \\tau})_{t\\in[0,T]}$ is a martingale.\n\nFinally, we apply the Optional Stopping Theorem to evaluate $\\mathbb{E}[Z_{\\tau}]$. The theorem states that if $(M_t)$ is a martingale and $\\tau$ is a stopping time, then under certain conditions, $\\mathbb{E}[M_\\tau] = \\mathbb{E}[M_0]$. One of the sufficient conditions is that the stopping time $\\tau$ is bounded. The problem specifies that $\\tau$ is an $(\\mathcal{F}_t)$-stopping time satisfying $0 \\leq \\tau \\leq T$ almost surely. This means that $\\tau$ is a bounded stopping time.\nApplying the Optional Stopping Theorem to the martingale $(Z_t)_{t\\in[0,T]}$ and the bounded stopping time $\\tau$, we have:\n$$\n\\mathbb{E}[Z_\\tau] = \\mathbb{E}[Z_0].\n$$\nAs calculated before, $Z_0=1$. Therefore,\n$$\n\\mathbb{E}[Z_\\tau] = 1.\n$$\nThis could also be justified by noting that any continuous martingale on a finite closed interval $[0,T]$ is uniformly integrable. The Optional Stopping Theorem holds for uniformly integrable martingales and any stopping time $\\sigma$. Thus, for our stopping time $\\tau \\le T$, we have $\\mathbb{E}[Z_\\tau]=\\mathbb{E}[Z_0]=1$. The bounded stopping time argument is more direct.", "answer": "$$\\boxed{1}$$", "id": "3000259"}, {"introduction": "To build intuition for the continuous-time Cameron-Martin-Girsanov formula, it is incredibly helpful to examine its finite-dimensional counterpart. This practice explores how a drift shift on a Wiener process, when viewed at a discrete set of times, corresponds to a simple mean shift of a multivariate normal vector. By deriving the Radon–Nikodym density ratio in this concrete setting, you will see how the essential structure of the Girsanov formula emerges from basic principles of Gaussian distributions [@problem_id:3000310].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ support a standard one-dimensional Wiener process $W=(W_t)_{t\\ge 0}$. Fix deterministic times $0=t_0t_1\\dotst_n$ with $n\\in\\mathbb{N}$. Let $h:[0,t_n]\\to\\mathbb{R}$ be absolutely continuous with $h(0)=0$ and square-integrable derivative $h'\\in L^2([0,t_n])$, so that $h$ belongs to the Cameron–Martin space. Consider the finite-dimensional projections $X=(W_{t_1},\\dots,W_{t_n})$ and $Y=(W_{t_1}+h(t_1),\\dots,W_{t_n}+h(t_n))$.\n\nUsing only the fundamental facts that finite-dimensional distributions of a standard Wiener process are multivariate normal with mean $0$ and covariance $\\Sigma$ given by $\\Sigma_{ij}=\\min\\{t_i,t_j\\}$, and that shifting a multivariate normal vector by a deterministic mean vector changes only its mean while keeping the same covariance, do the following:\n\n1. Determine the distribution of $Y$ in terms of its mean vector and covariance matrix.\n2. Compute the Radon–Nikodym density ratio (i.e., the ratio of Lebesgue densities) of the law of $Y$ with respect to the law of $X$ at a generic point $x=(x_1,\\dots,x_n)\\in\\mathbb{R}^n$. Express your answer in closed form in terms of the time increments $\\Delta_k:=t_k-t_{k-1}$ and the discrete increments of $h$ given by $h(t_k)-h(t_{k-1})$. Your final answer should be a single analytic expression for this density ratio as a function of $x_1,\\dots,x_n$.\n\nNo numerical approximation is required; provide an exact symbolic expression. The final boxed answer should be the closed-form expression for the density ratio only.", "solution": "The solution proceeds in two parts as requested by the problem.\n\n**Part 1: Distribution of Y**\n\nLet $X = (W_{t_1}, \\dots, W_{t_n})$. According to the problem statement, $X$ follows a multivariate normal distribution. The mean vector is $\\mathbb{E}[X] = (\\mathbb{E}[W_{t_1}], \\dots, \\mathbb{E}[W_{t_n}]) = (0, \\dots, 0) = \\mathbf{0}$. The covariance matrix $\\Sigma$ has elements $\\Sigma_{ij} = \\text{Cov}(W_{t_i}, W_{t_j}) = \\min\\{t_i, t_j\\}$. Thus, the law of $X$ is $\\mathcal{N}(\\mathbf{0}, \\Sigma)$.\n\nThe vector $Y$ is defined as $Y = (W_{t_1}+h(t_1), \\dots, W_{t_n}+h(t_n))$. This can be written as $Y = X + \\mathbf{h}$, where $\\mathbf{h} = (h(t_1), \\dots, h(t_n))$ is a deterministic vector.\n\nUsing the provided premise that shifting a multivariate normal vector by a deterministic vector only shifts its mean, we can determine the distribution of $Y$.\nThe mean of $Y$ is $\\mathbb{E}[Y] = \\mathbb{E}[X + \\mathbf{h}] = \\mathbb{E}[X] + \\mathbf{h} = \\mathbf{0} + \\mathbf{h} = \\mathbf{h}$.\nThe covariance matrix of $Y$ is $\\text{Cov}(Y) = \\text{Cov}(X + \\mathbf{h}) = \\text{Cov}(X) = \\Sigma$.\n\nTherefore, the distribution of $Y$ is multivariate normal with mean vector $\\mathbf{h}$ and covariance matrix $\\Sigma$. We write this as $Y \\sim \\mathcal{N}(\\mathbf{h}, \\Sigma)$.\n\n**Part 2: Radon–Nikodym Density Ratio**\n\nThe Radon–Nikodym derivative of the law of $Y$ with respect to the law of $X$, evaluated at a point $x \\in \\mathbb{R}^n$, is the ratio of their probability density functions (PDFs), $\\frac{f_Y(x)}{f_X(x)}$.\n\nThe PDF of a general $n$-dimensional normal vector $Z \\sim \\mathcal{N}(\\mu, C)$ is given by\n$$f_Z(z) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(C)}} \\exp\\left(-\\frac{1}{2}(z-\\mu)^T C^{-1} (z-\\mu)\\right)$$\nFor $X \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$, the PDF is\n$$f_X(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2}x^T \\Sigma^{-1} x\\right)$$\nFor $Y \\sim \\mathcal{N}(\\mathbf{h}, \\Sigma)$, the PDF is\n$$f_Y(x) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2}(x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h})\\right)$$\nThe ratio is\n$$\\frac{f_Y(x)}{f_X(x)} = \\frac{\\exp\\left(-\\frac{1}{2}(x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h})\\right)}{\\exp\\left(-\\frac{1}{2}x^T \\Sigma^{-1} x\\right)} = \\exp\\left(-\\frac{1}{2} \\left[ (x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h}) - x^T \\Sigma^{-1} x \\right]\\right)$$\nExpanding the quadratic form in the exponent gives\n$$(x-\\mathbf{h})^T \\Sigma^{-1} (x-\\mathbf{h}) = x^T \\Sigma^{-1} x - 2x^T \\Sigma^{-1} \\mathbf{h} + \\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}$$\nwhere we used the symmetry of $\\Sigma^{-1}$.\nSubstituting this back into the exponent of the ratio, we get\n$$ \\text{exponent} = -\\frac{1}{2} [ (x^T \\Sigma^{-1} x - 2x^T \\Sigma^{-1} \\mathbf{h} + \\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}) - x^T \\Sigma^{-1} x ] = x^T \\Sigma^{-1} \\mathbf{h} - \\frac{1}{2}\\mathbf{h}^T \\Sigma^{-1} \\mathbf{h} $$\nThe density ratio is $\\exp\\left(x^T \\Sigma^{-1} \\mathbf{h} - \\frac{1}{2}\\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}\\right)$.\n\nTo express this in the required form, we must analyze $\\Sigma^{-1}$. Let's consider the vector of Wiener process increments $Z = (Z_1, \\dots, Z_n)$, where $Z_k = W_{t_k} - W_{t_{k-1}}$ for $k=1,\\dots,n$, and $t_0=0$. These increments are independent, and $Z_k \\sim \\mathcal{N}(0, t_k - t_{k-1})$. Let $\\Delta_k = t_k - t_{k-1}$. The covariance matrix of $Z$ is a diagonal matrix $D$ with $D_{kk} = \\Delta_k$.\n\nThe components of $X$ can be expressed as cumulative sums of the components of $Z$: $W_{t_k} = \\sum_{j=1}^k Z_j$. This is a linear transformation $X=AZ$, where $A$ is the $n \\times n$ lower-triangular matrix of ones:\n$$ A = \\begin{pmatrix} 1  0  \\dots  0 \\\\ 1  1  \\dots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 1  1  \\dots  1 \\end{pmatrix} $$\nThe covariance matrix of $X$ is $\\Sigma = \\text{Cov}(AZ) = A\\,\\text{Cov}(Z)\\,A^T = ADA^T$.\nThe inverse is $\\Sigma^{-1} = (A^T)^{-1} D^{-1} A^{-1}$. The matrix $A^{-1}$ represents the inverse operation of cumulative sum, which is taking differences:\n$$ A^{-1} = \\begin{pmatrix} 1  0  0  \\dots  0 \\\\ -1  1  0  \\dots  0 \\\\ 0  -1  1  \\dots  0 \\\\ \\vdots  \\ddots  \\ddots  \\ddots  \\vdots \\\\ 0  \\dots  0  -1  1 \\end{pmatrix} $$\nLet's apply this structure to the terms in the exponent. The first term is $x^T \\Sigma^{-1} \\mathbf{h} = x^T (A^T)^{-1} D^{-1} A^{-1} \\mathbf{h}$. This can be rewritten as $(A^{-1}x)^T D^{-1} (A^{-1}\\mathbf{h})$.\n\nLet $\\delta x = A^{-1}x$ and $\\delta h = A^{-1}\\mathbf{h}$. The components of these vectors are the discrete increments:\nLetting $x_0=0$, $(\\delta x)_k = x_k - x_{k-1}$ for $k=1,\\dots,n$.\nLetting $h(t_0)=h(0)=0$, $(\\delta h)_k = h(t_k) - h(t_{k-1})$ for $k=1,\\dots,n$.\nThe matrix $D^{-1}$ is diagonal with entries $(D^{-1})_{kk} = 1/\\Delta_k$.\nSo, the term $x^T \\Sigma^{-1} \\mathbf{h}$ becomes\n$$ (\\delta x)^T D^{-1} (\\delta h) = \\sum_{k=1}^n (\\delta x)_k \\frac{1}{\\Delta_k} (\\delta h)_k = \\sum_{k=1}^n \\frac{(x_k-x_{k-1})(h(t_k)-h(t_{k-1}))}{\\Delta_k} $$\nSimilarly, the term $\\mathbf{h}^T \\Sigma^{-1} \\mathbf{h}$ becomes\n$$ (\\delta h)^T D^{-1} (\\delta h) = \\sum_{k=1}^n (\\delta h)_k \\frac{1}{\\Delta_k} (\\delta h)_k = \\sum_{k=1}^n \\frac{(h(t_k)-h(t_{k-1}))^2}{\\Delta_k} $$\nCombining these results, the density ratio is given by the exponential of\n$$ \\sum_{k=1}^n \\frac{(x_k-x_{k-1})(h(t_k)-h(t_{k-1}))}{t_k-t_{k-1}} - \\frac{1}{2} \\sum_{k=1}^n \\frac{(h(t_k)-h(t_{k-1}))^2}{t_k-t_{k-1}} $$\nwhere we have set $x_0=0$ and used the given condition $h(t_0)=h(0)=0$. This expression is a function of $x=(x_1,\\dots,x_n)$ and is expressed in terms of the specified time and function increments.", "answer": "$$\\boxed{\\exp\\left(\\sum_{k=1}^{n} \\frac{(x_k - x_{k-1})(h(t_k) - h(t_{k-1}))}{t_k - t_{k-1}} - \\frac{1}{2} \\sum_{k=1}^{n} \\frac{(h(t_k) - h(t_{k-1}))^{2}}{t_k - t_{k-1}}\\right)}$$", "id": "3000310"}, {"introduction": "With a solid grasp of the Girsanov machinery, we can now apply it as a powerful problem-solving tool. This practice demonstrates a classic technique: transforming a complex expectation into a simpler one by changing the probability measure to eliminate stochastic terms. You will compute the Laplace transform for the integral of a Brownian motion, a non-trivial task, by judiciously selecting a drift that turns a challenging stochastic problem into a straightforward deterministic one [@problem_id:3000309].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$ satisfying the usual conditions, and let $(W_{t})_{t \\geq 0}$ be a standard Brownian motion. For a fixed time horizon $T0$, define the integrated Brownian motion $X_{T} := \\int_{0}^{T} W_{s} \\,\\mathrm{d}s$. Using only fundamental tools of stochastic calculus and the Cameron-Martin-Girsanov formula (absolutely continuous change of drift), derive the Laplace transform\n$$L(\\lambda) := \\mathbb{E}\\big[\\exp\\big(-\\lambda X_{T}\\big)\\big], \\quad \\lambda \\in \\mathbb{R},$$\nby explicitly carrying out a drift change based on a deterministic integrand and computing the resulting expectation under the new measure. Your derivation should start from the definitions of Brownian motion, Itô integration by parts, and the Radon–Nikodym derivative associated with an exponential local martingale, and should include any necessary integrability conditions to justify the measure change. Provide the final answer for $L(\\lambda)$ as a single closed-form analytic expression. No rounding is required.", "solution": "The problem asks for the derivation of the Laplace transform $L(\\lambda) = \\mathbb{E}\\big[\\exp\\big(-\\lambda X_{T}\\big)\\big]$ for the integrated Brownian motion $X_{T} := \\int_{0}^{T} W_{s} \\,\\mathrm{d}s$, where $(W_t)_{t \\geq 0}$ is a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$. The derivation must be performed using a change of measure based on the Cameron-Martin-Girsanov formula.\n\nLet $\\lambda \\in \\mathbb{R}$ be a fixed parameter. The quantity to compute is\n$$L(\\lambda) = \\mathbb{E}_{\\mathbb{P}}\\left[\\exp\\left(-\\lambda \\int_{0}^{T} W_{s} \\,\\mathrm{d}s\\right)\\right]$$\nThe core idea is to define a new probability measure $\\mathbb{Q}$ equivalent to $\\mathbb{P}$ such that the expectation becomes simpler to compute. The Cameron-Martin-Girsanov theorem provides the mechanism for this change of measure.\n\nLet $h(s)$ be a deterministic function on $[0, T]$ such that $\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s  \\infty$. We define a new measure $\\mathbb{Q}$ on $\\mathcal{F}_T$ via the Radon-Nikodym derivative:\n$$\\frac{\\mathrm{d}\\mathbb{Q}}{\\mathrm{d}\\mathbb{P}} \\bigg|_{\\mathcal{F}_T} = Z_T := \\exp\\left(\\int_{0}^{T} h(s) \\,\\mathrm{d}W_s - \\frac{1}{2}\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s\\right)$$\nThe process $Z_t = \\exp\\left(\\int_{0}^{t} h(s) \\,\\mathrm{d}W_s - \\frac{1}{2}\\int_{0}^{t} h(s)^2 \\,\\mathrm{d}s\\right)$ is the Doléans-Dade exponential $\\mathcal{E}\\left(\\int h(s)\\,\\mathrm{d}W_s\\right)_t$. Since $h(s)$ is deterministic and square-integrable on $[0,T]$, it satisfies the Novikov condition $\\mathbb{E}_{\\mathbb{P}}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T h(s)^2 ds\\right)\\right]  \\infty$, because the argument of the exponential is deterministic. This guarantees that $(Z_t)_{t \\in [0,T]}$ is a true martingale, not just a local one.\n\nAccording to Girsanov's theorem, under the measure $\\mathbb{Q}$, the process $W_t^{\\mathbb{Q}}$ defined by\n$$W_t^{\\mathbb{Q}} := W_t - \\int_{0}^{t} h(u) \\,\\mathrm{d}u$$\nis a standard Brownian motion. This implies $W_t = W_t^{\\mathbb{Q}} + \\int_{0}^{t} h(u) \\,\\mathrm{d}u$ and $\\mathrm{d}W_t = \\mathrm{d}W_t^{\\mathbb{Q}} + h(t)\\,\\mathrm{d}t$.\n\nBy the change of measure formula (also known as the abstract Bayes' rule), the expectation under $\\mathbb{P}$ can be written as an expectation under $\\mathbb{Q}$:\n$$L(\\lambda) = \\mathbb{E}_{\\mathbb{P}}\\left[\\exp\\left(-\\lambda \\int_{0}^{T} W_{s} \\,\\mathrm{d}s\\right)\\right] = \\mathbb{E}_{\\mathbb{Q}}\\left[Z_T^{-1} \\exp\\left(-\\lambda \\int_{0}^{T} W_{s} \\,\\mathrm{d}s\\right)\\right]$$\nFirst, we find the expression for $Z_T^{-1}$:\n$$Z_T^{-1} = \\exp\\left(-\\int_{0}^{T} h(s) \\,\\mathrm{d}W_s + \\frac{1}{2}\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s\\right)$$\nWe substitute $\\mathrm{d}W_s = \\mathrm{d}W_s^{\\mathbb{Q}} + h(s)\\,\\mathrm{d}s$ into the stochastic integral:\n$$-\\int_{0}^{T} h(s) \\,\\mathrm{d}W_s = -\\int_{0}^{T} h(s) (\\mathrm{d}W_s^{\\mathbb{Q}} + h(s)\\,\\mathrm{d}s) = -\\int_{0}^{T} h(s) \\,\\mathrm{d}W_s^{\\mathbb{Q}} - \\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s$$\nThus,\n$$Z_T^{-1} = \\exp\\left(-\\int_{0}^{T} h(s) \\,\\mathrm{d}W_s^{\\mathbb{Q}} - \\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s + \\frac{1}{2}\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s\\right) = \\exp\\left(-\\int_{0}^{T} h(s) \\,\\mathrm{d}W_s^{\\mathbb{Q}} - \\frac{1}{2}\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s\\right)$$\nNext, we express the term $\\int_0^T W_s \\,\\mathrm{d}s$ in terms of $W_s^{\\mathbb{Q}}$:\n$$\\int_{0}^{T} W_s \\,\\mathrm{d}s = \\int_{0}^{T} \\left(W_s^{\\mathbb{Q}} + \\int_{0}^{s} h(u) \\,\\mathrm{d}u\\right) \\mathrm{d}s = \\int_{0}^{T} W_s^{\\mathbb{Q}} \\,\\mathrm{d}s + \\int_{0}^{T} \\int_{0}^{s} h(u) \\,\\mathrm{d}u \\,\\mathrm{d}s$$\nCombining these results, the argument of the expectation $\\mathbb{E}_{\\mathbb{Q}}[\\cdot]$ becomes the exponential of a sum of terms:\n$$\\text{Exponent} = -\\lambda \\left(\\int_{0}^{T} W_s^{\\mathbb{Q}} \\,\\mathrm{d}s + \\int_{0}^{T} \\int_{0}^{s} h(u) \\,\\mathrm{d}u \\,\\mathrm{d}s\\right) - \\int_{0}^{T} h(s) \\,\\mathrm{d}W_s^{\\mathbb{Q}} - \\frac{1}{2}\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s$$\nWe separate this into a random part (containing integrals with respect to $W^{\\mathbb{Q}}$) and a deterministic part.\n$$\\text{Random Part} = -\\lambda \\int_{0}^{T} W_s^{\\mathbb{Q}} \\,\\mathrm{d}s - \\int_{0}^{T} h(s) \\,\\mathrm{d}W_s^{\\mathbb{Q}}$$\n$$\\text{Deterministic Part} = -\\lambda \\int_{0}^{T} \\int_{0}^{s} h(u) \\,\\mathrm{d}u \\,\\mathrm{d}s - \\frac{1}{2}\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s$$\nTo simplify the random part, we use Itô's integration by parts on the process $(T-t)W_t^{\\mathbb{Q}}$. Since $W_0^{\\mathbb{Q}}=0$:\n$$d((T-t)W_t^{\\mathbb{Q}}) = (T-t)\\,\\mathrm{d}W_t^{\\mathbb{Q}} - W_t^{\\mathbb{Q}}\\,\\mathrm{d}t$$\n$$\\int_0^T d((T-s)W_s^{\\mathbb{Q}}) = (T-T)W_T^{\\mathbb{Q}} - (T-0)W_0^{\\mathbb{Q}} = 0$$\n$$0 = \\int_{0}^{T} (T-s)\\,\\mathrm{d}W_s^{\\mathbb{Q}} - \\int_{0}^{T} W_s^{\\mathbb{Q}}\\,\\mathrm{d}s \\implies \\int_{0}^{T} W_s^{\\mathbb{Q}}\\,\\mathrm{d}s = \\int_{0}^{T} (T-s)\\,\\mathrm{d}W_s^{\\mathbb{Q}}$$\nSubstituting this identity into the random part gives:\n$$\\text{Random Part} = -\\lambda \\int_{0}^{T} (T-s)\\,\\mathrm{d}W_s^{\\mathbb{Q}} - \\int_{0}^{T} h(s)\\,\\mathrm{d}W_s^{\\mathbb{Q}} = -\\int_{0}^{T} \\big(\\lambda(T-s) + h(s)\\big)\\,\\mathrm{d}W_s^{\\mathbb{Q}}$$\nThe goal of the change of measure is to make the expectation calculation trivial. We can achieve this by choosing the drift process $h(s)$ such that the random part of the exponent becomes zero. We set the integrand of the stochastic integral to zero:\n$$\\lambda(T-s) + h(s) = 0 \\implies h(s) = -\\lambda(T-s)$$\nThis choice of $h(s)$ is deterministic and square-integrable on $[0,T]$, as required. With this $h(s)$, the random part vanishes, and the expectation becomes the exponential of the deterministic part:\n$$L(\\lambda) = \\mathbb{E}_{\\mathbb{Q}}\\left[\\exp(\\text{Deterministic Part})\\right] = \\exp(\\text{Deterministic Part})$$\nNow we must compute the deterministic part with $h(s) = -\\lambda(T-s)$. It consists of two terms.\nThe first term is:\n$$-\\frac{1}{2}\\int_{0}^{T} h(s)^2 \\,\\mathrm{d}s = -\\frac{1}{2}\\int_{0}^{T} (-\\lambda(T-s))^2 \\,\\mathrm{d}s = -\\frac{\\lambda^2}{2}\\int_{0}^{T} (T-s)^2 \\,\\mathrm{d}s$$\nLet $u=T-s$, so $\\mathrm{d}u = -\\mathrm{d}s$. The integral becomes $\\int_{T}^{0} u^2(-\\mathrm{d}u) = \\int_{0}^{T} u^2\\,\\mathrm{d}u = \\left[\\frac{u^3}{3}\\right]_0^T = \\frac{T^3}{3}$.\nSo, the first term is $-\\frac{\\lambda^2}{2} \\cdot \\frac{T^3}{3} = -\\frac{\\lambda^2 T^3}{6}$.\n\nThe second term is:\n$$-\\lambda \\int_{0}^{T} \\int_{0}^{s} h(u) \\,\\mathrm{d}u \\,\\mathrm{d}s = -\\lambda \\int_{0}^{T} \\left( \\int_{0}^{s} -\\lambda(T-u) \\,\\mathrm{d}u \\right) \\mathrm{d}s$$\nThe inner integral is:\n$$\\int_{0}^{s} -\\lambda(T-u) \\,\\mathrm{d}u = -\\lambda \\left[Tu - \\frac{u^2}{2}\\right]_0^s = -\\lambda \\left(Ts - \\frac{s^2}{2}\\right)$$\nSubstituting this into the outer integral:\n$$-\\lambda \\int_{0}^{T} -\\lambda \\left(Ts - \\frac{s^2}{2}\\right) \\mathrm{d}s = \\lambda^2 \\int_{0}^{T} \\left(Ts - \\frac{s^2}{2}\\right) \\mathrm{d}s = \\lambda^2 \\left[T\\frac{s^2}{2} - \\frac{s^3}{6}\\right]_0^T$$\n$$= \\lambda^2 \\left(\\frac{T^3}{2} - \\frac{T^3}{6}\\right) = \\lambda^2 \\left(\\frac{3T^3 - T^3}{6}\\right) = \\frac{\\lambda^2 T^3}{3}$$\nSo, the second term is $\\frac{\\lambda^2 T^3}{3}$.\n\nFinally, we sum the two parts of the deterministic exponent:\n$$\\text{Deterministic Part} = -\\frac{\\lambda^2 T^3}{6} + \\frac{\\lambda^2 T^3}{3} = \\frac{\\lambda^2 T^3}{6}$$\nTherefore, the Laplace transform is:\n$$L(\\lambda) = \\exp\\left(\\frac{\\lambda^2 T^3}{6}\\right)$$\nThis concludes the derivation as required.", "answer": "$$\n\\boxed{\\exp\\left(\\frac{\\lambda^2 T^3}{6}\\right)}\n$$", "id": "3000309"}]}