{"hands_on_practices": [{"introduction": "One of the most powerful tools in martingale theory is the ability to generate new submartingales or supermartingales from existing martingales. This practice explores a fundamental example of this principle by examining processes derived from a martingale, such as its running maximum. You will apply the conditional Jensen's inequality to show how a convex transformation of a martingale naturally gives rise to a submartingale, a concept that is central to many proofs and applications in stochastic analysis [@problem_id:1310340].", "problem": "In the study of stochastic processes, we often analyze the properties of sequences of random variables over time. Let $\\{M_n\\}_{n \\ge 0}$ be a discrete-time stochastic process adapted to a filtration $\\{\\mathcal{F}_n\\}_{n \\ge 0}$, where $\\mathcal{F}_n$ represents the information available up to time $n$. The process $\\{M_n\\}$ is called a **martingale** with respect to $\\{\\mathcal{F}_n\\}$ if it satisfies two conditions for all $n \\ge 0$:\n1. $\\mathbb{E}[|M_n|]  \\infty$ (it has a finite expected value).\n2. $\\mathbb{E}[M_{n+1} | \\mathcal{F}_n] = M_n$ (the best prediction of the next value, given the past, is the current value).\n\nA process $\\{X_n\\}$ is called a **submartingale** if $\\mathbb{E}[X_{n+1} | \\mathcal{F}_n] \\ge X_n$, and a **supermartingale** if $\\mathbb{E}[X_{n+1} | \\mathcal{F}_n] \\le X_n$. Assume all processes defined below have finite expected values.\n\nGiven an arbitrary martingale $\\{M_n\\}_{n \\ge 0}$, consider the following four new stochastic processes derived from it:\n- $X_n = \\max\\{M_0, M_1, \\dots, M_n\\}$ (the running maximum of the process)\n- $Y_n = \\min\\{M_0, M_1, \\dots, M_n\\}$ (the running minimum of the process)\n- $Z_n = \\exp(M_n)$\n- $W_n = M_n - M_{n-1}$ for $n \\ge 1$, with $W_0=M_0$.\n\nWhich of the following statements is always true for any martingale $\\{M_n\\}$?\n\nA. The process $\\{X_n\\}$ is a submartingale.\n\nB. The process $\\{Y_n\\}$ is a submartingale.\n\nC. The process $\\{Z_n\\}$ is a martingale.\n\nD. The process $\\{W_n\\}$ is a martingale.", "solution": "We are given a martingale $\\{M_{n}\\}_{n \\ge 0}$ with respect to $\\{\\mathcal{F}_{n}\\}$ and the derived processes $X_{n}=\\max\\{M_{0},\\dots,M_{n}\\}$, $Y_{n}=\\min\\{M_{0},\\dots,M_{n}\\}$, $Z_{n}=\\exp(M_{n})$, and $W_{n}=M_{n}-M_{n-1}$ for $n \\ge 1$ with $W_{0}=M_{0}$. We assume all these derived processes are integrable.\n\nFor statement A: We show that $\\{X_{n}\\}$ is a submartingale. Note that $X_{n}$ is $\\mathcal{F}_{n}$-measurable and\n$$\nX_{n+1}=\\max\\{X_{n},M_{n+1}\\}.\n$$\nFix $n$ and condition on $\\mathcal{F}_{n}$. Given $\\mathcal{F}_{n}$, $X_{n}$ is a constant (measurable), and the function $y \\mapsto \\max\\{X_{n},y\\}$ is convex in $y$ (it is the pointwise maximum of two affine functions). By Jensen's inequality and the martingale property $\\mathbb{E}[M_{n+1}\\mid \\mathcal{F}_{n}]=M_{n}$,\n$$\n\\mathbb{E}[X_{n+1}\\mid \\mathcal{F}_{n}]\n=\\mathbb{E}[\\max\\{X_{n},M_{n+1}\\}\\mid \\mathcal{F}_{n}]\n\\ge \\max\\{X_{n},\\mathbb{E}[M_{n+1}\\mid \\mathcal{F}_{n}]\\}\n=\\max\\{X_{n},M_{n}\\}\n=X_{n}.\n$$\nThus $\\mathbb{E}[X_{n+1}\\mid \\mathcal{F}_{n}] \\ge X_{n}$, so $\\{X_{n}\\}$ is a submartingale. Hence A is true.\n\nFor statement B: Consider $\\{Y_{n}\\}$ with $Y_{n}=\\min\\{M_{0},\\dots,M_{n}\\}$. We have\n$$\nY_{n+1}=\\min\\{Y_{n},M_{n+1}\\}.\n$$\nGiven $\\mathcal{F}_{n}$, $Y_{n}$ is constant and the function $y \\mapsto \\min\\{Y_{n},y\\}$ is concave in $y$. By Jensen's inequality for concave functions and the martingale property,\n$$\n\\mathbb{E}[Y_{n+1}\\mid \\mathcal{F}_{n}]\n=\\mathbb{E}[\\min\\{Y_{n},M_{n+1}\\}\\mid \\mathcal{F}_{n}]\n\\le \\min\\{Y_{n},\\mathbb{E}[M_{n+1}\\mid \\mathcal{F}_{n}]\\}\n=\\min\\{Y_{n},M_{n}\\}\n=Y_{n}.\n$$\nThus $\\mathbb{E}[Y_{n+1}\\mid \\mathcal{F}_{n}] \\le Y_{n}$, so $\\{Y_{n}\\}$ is a supermartingale in general, not a submartingale. Hence B is false.\n\nFor statement C: For $Z_{n}=\\exp(M_{n})$, since $\\exp$ is convex and $\\mathbb{E}[M_{n+1}\\mid \\mathcal{F}_{n}]=M_{n}$,\n$$\n\\mathbb{E}[Z_{n+1}\\mid \\mathcal{F}_{n}]\n=\\mathbb{E}[\\exp(M_{n+1})\\mid \\mathcal{F}_{n}]\n\\ge \\exp(\\mathbb{E}[M_{n+1}\\mid \\mathcal{F}_{n}])\n=\\exp(M_{n})\n=Z_{n}.\n$$\nTherefore $\\{Z_{n}\\}$ is a submartingale. It is a martingale only under additional conditions ensuring equality in Jensen, which do not hold for an arbitrary martingale. Hence C is false in general.\n\nFor statement D: For $n \\ge 1$, compute\n$$\n\\mathbb{E}[W_{n+1}\\mid \\mathcal{F}_{n}]\n=\\mathbb{E}[M_{n+1}-M_{n}\\mid \\mathcal{F}_{n}]\n=\\mathbb{E}[M_{n+1}\\mid \\mathcal{F}_{n}]-M_{n}\n=M_{n}-M_{n}\n=0.\n$$\nHowever, $W_{n}=M_{n}-M_{n-1}$ is generally not almost surely zero, so $\\mathbb{E}[W_{n+1}\\mid \\mathcal{F}_{n}] \\ne W_{n}$ in general. Thus $\\{W_{n}\\}$ is not a martingale. Hence D is false.\n\nConsequently, only statement A always holds.", "answer": "$$\\boxed{A}$$", "id": "1310340"}, {"introduction": "Building on the idea of transforming martingales, we can abstract the principle to a general class of functions. This moves our understanding from a specific example to a more powerful, general theorem. This problem asks you to analyze the composition of a supermartingale $X_n$ with a function $g$ that is both concave and non-decreasing, resulting in a new process $Y_n = g(X_n)$. Mastering this exercise solidifies your understanding of how functional properties interact with conditional expectations, enabling you to classify a wide range of transformed stochastic processes without recalculating from scratch each time [@problem_id:1295499].", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space equipped with a filtration $\\{\\mathcal{F}_n\\}_{n \\ge 0}$, which is a sequence of nested sigma-algebras. A discrete-time stochastic process $\\{X_n\\}_{n \\ge 0}$ is said to be adapted to this filtration if for each $n \\ge 0$, the random variable $X_n$ is $\\mathcal{F}_n$-measurable.\n\nSuppose $\\{X_n\\}_{n \\ge 0}$ is an adapted stochastic process that is a **supermartingale** with respect to the filtration $\\{\\mathcal{F}_n\\}_{n \\ge 0}$. By definition, this means that for all $n \\ge 0$:\n1. $X_n$ is integrable, i.e., $\\mathbb{E}[|X_n|]  \\infty$.\n2. $\\mathbb{E}[X_{n+1} | \\mathcal{F}_n] \\le X_n$.\n\nNow, consider a function $g: \\mathbb{R} \\to \\mathbb{R}$ that has the following two properties:\n1. $g$ is a **concave** function.\n2. $g$ is a **non-decreasing** function.\n\nA new stochastic process $\\{Y_n\\}_{n \\ge 0}$ is defined by the transformation $Y_n = g(X_n)$ for all $n \\ge 0$. We assume that this new process is also adapted to the filtration $\\{\\mathcal{F}_n\\}_{n \\ge 0}$ and that each $Y_n$ is integrable, i.e., $\\mathbb{E}[|Y_n|]  \\infty$.\n\nBased on the provided information, which of the following statements correctly classifies the process $\\{Y_n\\}_{n \\ge 0}$ in general?\n\nA. $\\{Y_n\\}_{n \\ge 0}$ is always a submartingale.\n\nB. $\\{Y_n\\}_{n \\ge 0}$ is always a supermartingale.\n\nC. $\\{Y_n\\}_{n \\ge 0}$ is always a martingale.\n\nD. $\\{Y_n\\}_{n \\ge 0}$ is a martingale if and only if $\\{X_n\\}_{n \\ge 0}$ is a martingale.\n\nE. The classification of $\\{Y_n\\}_{n \\ge 0}$ as a submartingale, supermartingale, or martingale cannot be determined without more information.", "solution": "We are given a discrete-time supermartingale $\\{X_{n}\\}_{n \\ge 0}$ with respect to $\\{\\mathcal{F}_{n}\\}_{n \\ge 0}$, so for all $n \\ge 0$,\n$$\n\\mathbb{E}[X_{n+1} \\mid \\mathcal{F}_{n}] \\le X_{n} \\quad \\text{a.s.}\n$$\nLet $g:\\mathbb{R} \\to \\mathbb{R}$ be concave and non-decreasing, and define $Y_{n} = g(X_{n})$. By assumption, each $Y_{n}$ is integrable and $\\mathcal{F}_{n}$-measurable.\n\nWe aim to classify $\\{Y_{n}\\}$ using conditional Jensen's inequality. For a concave function $g$ and an integrable random variable, conditional Jensen's inequality states\n$$\n\\mathbb{E}[g(X_{n+1}) \\mid \\mathcal{F}_{n}] \\le g\\big(\\mathbb{E}[X_{n+1} \\mid \\mathcal{F}_{n}]\\big) \\quad \\text{a.s.}\n$$\nApplying this to $X_{n+1}$ and using the supermartingale property of $X$ yields\n$$\n\\mathbb{E}[Y_{n+1} \\mid \\mathcal{F}_{n}] \\;=\\; \\mathbb{E}[g(X_{n+1}) \\mid \\mathcal{F}_{n}]\n\\;\\le\\; g\\big(\\mathbb{E}[X_{n+1} \\mid \\mathcal{F}_{n}]\\big)\n\\;\\le\\; g(X_{n})\n\\;=\\; Y_{n} \\quad \\text{a.s.,}\n$$\nwhere the second inequality follows from the fact that $g$ is non-decreasing and $\\mathbb{E}[X_{n+1} \\mid \\mathcal{F}_{n}] \\le X_{n}$.\n\nTherefore, $\\{Y_{n}\\}_{n \\ge 0}$ satisfies $\\mathbb{E}[Y_{n+1} \\mid \\mathcal{F}_{n}] \\le Y_{n}$ a.s. for all $n \\ge 0$, and, with integrability and adaptedness given, it is a supermartingale.\n\nHence the correct classification is that $\\{Y_{n}\\}$ is always a supermartingale.", "answer": "$$\\boxed{B}$$", "id": "1295499"}, {"introduction": "Martingale theory provides the foundation for some of the most important concentration inequalities in modern probability and statistics. This advanced practice explores the deep connection between martingales and the Azuma-Hoeffding inequality by analyzing a simple symmetric random walk, a canonical example of a martingale with bounded increments. By deriving the exact tail probability using combinatorial methods and large deviation theory, you will be able to quantitatively assess the tightness of the Azuma-Hoeffding bound, demonstrating its asymptotic optimality for small deviations and gaining insight into the power and precision of these theoretical tools [@problem_id:2972976].", "problem": "Let $\\{X_{k}\\}_{k=1}^{n}$ be independent Rademacher random variables, meaning $\\mathbb{P}(X_{k}=1)=\\mathbb{P}(X_{k}=-1)=\\frac{1}{2}$ for each $k$. Define the discrete-time process $M_{k}=\\sum_{i=1}^{k}X_{i}$ adapted to the natural filtration $\\mathcal{F}_{k}=\\sigma(X_{1},\\dots,X_{k})$. It is known that the Azuma–Hoeffding inequality for bounded-difference martingales asserts that if $\\{|M_{k}-M_{k-1}|\\leq c_{k}\\}$ almost surely for each $k$ and $t0$, then\n$$\n\\mathbb{P}\\big(M_{n}-M_{0}\\geq t\\big)\\leq \\exp\\!\\Big(-\\frac{t^{2}}{2\\sum_{k=1}^{n}c_{k}^{2}}\\Big).\n$$\nIn this setup, the increments satisfy $|M_{k}-M_{k-1}|=|X_{k}|=1$ almost surely.\n\nYou are asked to carry out a first-principles derivation using exact combinatorics and asymptotics to produce a quantitative comparison between the exact tail probability and the Azuma–Hoeffding bound, thereby assessing tightness up to constants.\n\nTasks:\n- Justify from first principles that $\\{M_{k}\\}_{k\\geq 0}$ is a discrete-time martingale and that $|M_{k}-M_{k-1}|=1$ almost surely.\n- For a fixed $a\\in(0,1)$, write the exact formula for the tail probability $\\mathbb{P}(M_{n}\\geq a n)$ in terms of binomial coefficients.\n- Using Stirling’s approximation for factorials, derive a large deviations rate function $I(a)$ such that\n$$\n\\mathbb{P}(M_{n}\\geq a n)=\\exp\\!\\big(-n\\,I(a)+o(n)\\big)\\quad\\text{as }n\\to\\infty.\n$$\n- Compare the exponential rate $\\exp(-n I(a))$ with the Azuma–Hoeffding upper bound $\\exp\\!\\big(-\\frac{a^{2}n}{2}\\big)$ in the regime of small deviations $a\\to 0$, and determine the limit\n$$\nL=\\lim_{a\\to 0}\\frac{I(a)}{a^{2}/2}.\n$$\n\nAnswer specification: Your final reported answer must be the single real number $L$ as defined above. No rounding is required.", "solution": "**Justification of Martingale Property and Bounded Differences**\nWe are given the process $M_{k} = \\sum_{i=1}^{k} X_{i}$ for $k \\geq 1$, with $M_{0}=0$, adapted to the filtration $\\mathcal{F}_{k} = \\sigma(X_{1}, \\dots, X_{k})$. To demonstrate that $\\{M_{k}\\}_{k \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_{k}\\}_{k \\geq 0}$, we must verify three conditions:\n$1$. $M_{k}$ is $\\mathcal{F}_{k}$-measurable for each $k \\geq 0$. This holds by definition, as $M_{k}$ is a function of the random variables $X_1, \\dots, X_k$ which generate $\\mathcal{F}_{k}$.\n$2$. $M_{k}$ is integrable, i.e., $\\mathbb{E}[|M_{k}|]  \\infty$. Using the triangle inequality, we have $\\mathbb{E}[|M_{k}|] = \\mathbb{E}[|\\sum_{i=1}^{k} X_{i}|] \\leq \\sum_{i=1}^{k} \\mathbb{E}[|X_{i}|]$. Since $X_{i}$ takes values in $\\{-1, 1\\}$, $|X_{i}|=1$ almost surely. Thus, $\\mathbb{E}[|X_{i}|]=1$, which gives $\\mathbb{E}[|M_{k}|] \\leq \\sum_{i=1}^{k} 1 = k  \\infty$.\n$3$. The core martingale property $\\mathbb{E}[M_{k} | \\mathcal{F}_{k-1}] = M_{k-1}$ for $k \\geq 1$.\n$$\n\\mathbb{E}[M_{k} | \\mathcal{F}_{k-1}] = \\mathbb{E}[M_{k-1} + X_{k} | \\mathcal{F}_{k-1}]\n$$\nBy linearity of conditional expectation,\n$$\n\\mathbb{E}[M_{k} | \\mathcal{F}_{k-1}] = \\mathbb{E}[M_{k-1} | \\mathcal{F}_{k-1}] + \\mathbb{E}[X_{k} | \\mathcal{F}_{k-1}]\n$$\nSince $M_{k-1}$ is $\\mathcal{F}_{k-1}$-measurable, $\\mathbb{E}[M_{k-1} | \\mathcal{F}_{k-1}] = M_{k-1}$. Since $X_{k}$ is independent of $\\mathcal{F}_{k-1} = \\sigma(X_{1}, \\dots, X_{k-1})$, its conditional expectation equals its unconditional expectation: $\\mathbb{E}[X_{k} | \\mathcal{F}_{k-1}] = \\mathbb{E}[X_{k}]$. The expectation of a Rademacher random variable is $\\mathbb{E}[X_{k}] = 1 \\cdot \\mathbb{P}(X_{k}=1) + (-1) \\cdot \\mathbb{P}(X_{k}=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0$.\nTherefore, $\\mathbb{E}[M_{k} | \\mathcal{F}_{k-1}] = M_{k-1} + 0 = M_{k-1}$.\nAll three conditions are satisfied, so $\\{M_k\\}_{k\\geq 0}$ is a discrete-time martingale.\nFor the increments, we have $M_{k}-M_{k-1} = (\\sum_{i=1}^{k} X_{i}) - (\\sum_{i=1}^{k-1} X_{i}) = X_{k}$ for $k \\ge 1$. As established, $|X_{k}|=1$ almost surely. Hence, $|M_{k}-M_{k-1}|=1$ almost surely.\n\n**Exact Formula for Tail Probability**\nLet $U_{n}$ be the number of variables in $\\{X_{1}, \\dots, X_{n}\\}$ that are equal to $1$. Consequently, $n-U_{n}$ variables are equal to $-1$. The sum $M_{n}$ can be expressed as:\n$$\nM_{n} = 1 \\cdot U_{n} + (-1) \\cdot (n - U_{n}) = 2U_{n} - n\n$$\nSince the $X_{k}$ are independent and identically distributed with $\\mathbb{P}(X_k=1)=1/2$, the random variable $U_{n}$ follows a binomial distribution, $U_{n} \\sim \\text{Bin}(n, 1/2)$. The probability mass function is $\\mathbb{P}(U_{n}=j) = \\binom{n}{j} (\\frac{1}{2})^{j} (\\frac{1}{2})^{n-j} = \\binom{n}{j} 2^{-n}$.\nThe event of interest is $M_{n} \\geq a n$. In terms of $U_{n}$, this is $2U_{n}-n \\geq an$, which simplifies to $U_{n} \\geq \\frac{n(1+a)}{2}$.\nThe exact tail probability is the sum of probabilities for all integer values of $j$ satisfying this condition:\n$$\n\\mathbb{P}(M_{n} \\geq a n) = \\mathbb{P}\\Big(U_{n} \\geq \\frac{n(1+a)}{2}\\Big) = \\sum_{j=\\lceil n(1+a)/2 \\rceil}^{n} \\mathbb{P}(U_{n}=j)\n$$\nSubstituting the binomial probability, we get:\n$$\n\\mathbb{P}(M_{n} \\geq a n) = 2^{-n} \\sum_{j=\\lceil n(1+a)/2 \\rceil}^{n} \\binom{n}{j}\n$$\n\n**Large Deviations Rate Function**\nWe seek the rate function $I(a)$ in the expression $\\mathbb{P}(M_{n}\\geq a n)=\\exp(-n\\,I(a)+o(n))$. This is a classic large deviation result. The probability of the sum is dominated by the probability of the most likely term in the sum, which for large $n$ corresponds to the lower limit of the summation. Let $p = j/n$. The event $M_{n} \\geq an$ corresponds to the empirical mean of \"+1\" outcomes being $U_n/n \\geq (1+a)/2$.\nAccording to Cramér's theorem, the rate function is the Legendre-Fenchel transform of the cumulant generating function of the underlying random variables. For i.i.d. Bernoulli trials $Y_i \\sim \\text{Bernoulli}(q)$, the rate function for the empirical mean $\\bar{Y}_n$ to be $p$ is given by the Kullback-Leibler divergence $I(p) = D_{KL}(p\\|q) = p\\ln(p/q) + (1-p)\\ln((1-p)/(1-q))$.\nIn our case, the variables are $Y_{i} = (X_{i}+1)/2 \\sim \\text{Bernoulli}(1/2)$, so $q=1/2$. The deviation is to $p=(1+a)/2$.\nThe rate function for the event $\\mathbb{P}(U_{n}/n \\geq (1+a)/2)$ is thus determined by the value $p=(1+a)/2$.\n$$\nI(a) = \\Big(\\frac{1+a}{2}\\Big)\\ln\\Big(\\frac{(1+a)/2}{1/2}\\Big) + \\Big(1-\\frac{1+a}{2}\\Big)\\ln\\Big(\\frac{1-(1+a)/2}{1/2}\\Big)\n$$\n$$\nI(a) = \\Big(\\frac{1+a}{2}\\Big)\\ln(1+a) + \\Big(\\frac{1-a}{2}\\Big)\\ln(1-a)\n$$\nThis is the desired large deviations rate function.\n\n**Comparison and Limit Calculation**\nThe Azuma-Hoeffding inequality for this process, with $c_{k}=1$ for all $k$, and $t=an$, gives the bound:\n$$\n\\mathbb{P}(M_{n}\\geq a n) \\leq \\exp\\left(-\\frac{(an)^2}{2\\sum_{k=1}^n 1^2}\\right) = \\exp\\left(-\\frac{a^2 n^2}{2n}\\right) = \\exp\\left(-n\\frac{a^2}{2}\\right)\n$$\nThis implies an upper bound on the rate of decay, suggesting a comparison of $I(a)$ with $a^2/2$. We now compute the specified limit $L = \\lim_{a\\to 0}\\frac{I(a)}{a^{2}/2}$.\nTo evaluate this limit, we find the Taylor series expansion of $I(a)$ around $a=0$. We use the series for $\\ln(1+x)$:\n$$\n\\ln(1+x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + O(x^5)\n$$\nFirst, we expand $(1+a)\\ln(1+a)$:\n$$\n(1+a)\\ln(1+a) = (1+a)\\Big(a - \\frac{a^2}{2} + \\frac{a^3}{3} - \\frac{a^4}{4} + O(a^5)\\Big) = a + \\frac{a^2}{2} - \\frac{a^3}{6} + \\frac{a^4}{12} + O(a^5)\n$$\nNext, we expand $(1-a)\\ln(1-a)$ by substituting $-a$ for $a$:\n$$\n(1-a)\\ln(1-a) = -a + \\frac{a^2}{2} + \\frac{a^3}{6} + \\frac{a^4}{12} + O(a^5)\n$$\nNow we combine these into the expression for $I(a)$:\n$$\nI(a) = \\frac{1}{2} \\Big[ \\Big(a + \\frac{a^2}{2} - \\frac{a^3}{6} + \\frac{a^4}{12}\\Big) + \\Big(-a + \\frac{a^2}{2} + \\frac{a^3}{6} + \\frac{a^4}{12}\\Big) + O(a^5) \\Big]\n$$\n$$\nI(a) = \\frac{1}{2} \\Big[ a^2 + \\frac{a^4}{6} + O(a^5) \\Big] = \\frac{a^2}{2} + \\frac{a^4}{12} + O(a^5)\n$$\nFinally, we compute the limit:\n$$\nL = \\lim_{a\\to 0}\\frac{I(a)}{a^{2}/2} = \\lim_{a\\to 0}\\frac{\\frac{a^2}{2} + \\frac{a^4}{12} + O(a^5)}{a^2/2} = \\lim_{a\\to 0}\\Big(1 + \\frac{a^2}{6} + O(a^3)\\Big) = 1\n$$\nThe limit is $1$. This demonstrates that for small deviations (small $a$), the Azuma-Hoeffding inequality is tight in its exponential rate, capturing the correct $a^2/2$ term. The factor of $1$ signifies asymptotic equivalence of the rates in this regime.", "answer": "$$\\boxed{1}$$", "id": "2972976"}]}