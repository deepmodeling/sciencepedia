{"hands_on_practices": [{"introduction": "Renewal processes are a cornerstone of stochastic modeling, capturing the essence of events that recur over time, such as machine failures or customer arrivals. The core principle is that the time between consecutive events follows a consistent, independent random pattern, causing the system to \"renew\" its probabilistic structure at each event. This foundational exercise [@problem_id:2998410] guides you from the ground up, starting with a rigorous definition and culminating in the derivation of the famous renewal equation, providing a solid basis for analyzing this important class of processes.", "problem": "Consider a counting process $\\{N(t): t \\ge 0\\}$ constructed from a sequence of independent and identically distributed (i.i.d.) nonnegative inter-arrival times $\\{X_{n}\\}_{n \\ge 1}$ with common distribution function $F$ and finite positive mean $\\mu \\in (0,\\infty)$. Let the renewal epochs be $S_{0} \\equiv 0$ and $S_{n} \\equiv \\sum_{k=1}^{n} X_{k}$ for $n \\ge 1$, and define the renewal function $m(t) \\equiv \\mathbb{E}[N(t)]$ for $t \\ge 0$. Your tasks are:\n\n- Using only first principles and core definitions from the theory of stochastic processes, give a rigorous definition of a renewal process in terms of $(X_{n})$, $(S_{n})$, and $N(t)$, and explain why $N(t) \\equiv \\max\\{n \\ge 0: S_{n} \\le t\\}$ is a well-defined stochastic process.\n\n- Starting from the law of total expectation and conditioning on the first inter-arrival time $X_{1}$, derive the fundamental renewal equation for the renewal function $m(t)$ in terms of the distribution function $F$. Work directly from the definition of $N(t)$ and the additivity of expectation; do not invoke any pre-packaged formulas from renewal theory.\n\n- Define and explain the classification of inter-arrival distributions into arithmetic (also called lattice) and nonarithmetic (nonlattice). Then classify each of the following inter-arrival distributions: deterministic with $X_{1}=a$ almost surely for some $a>0$, uniform with $X_{1}\\sim \\mathrm{Unif}([0,1])$, and exponential with $X_{1}\\sim \\mathrm{Exp}(\\lambda)$ for some $\\lambda>0$.\n\n- For the specific case $X_{1}\\sim \\mathrm{Unif}([0,1])$, compute the unilateral Laplace transform $M(s) \\equiv \\int_{0}^{\\infty} \\exp(-s t)\\, m(t)\\, dt$ of the renewal function. Express your final answer as a single closed-form analytic expression in $s$ that is valid for $\\Re(s)>0$. No rounding is required, and no units are involved.", "solution": "The problem asks for a multi-part analysis of a renewal process. We will address each task sequentially, building upon fundamental definitions and principles.\n\nFirst, we address the definition of a renewal process and the well-definedness of the counting process $N(t)$.\nA renewal process is a sequence of events occurring in time, where the time intervals between consecutive events are independent and identically distributed (i.i.d.) random variables. Formally, let $\\{X_{n}\\}_{n \\ge 1}$ be a sequence of i.i.d. nonnegative random variables representing the inter-arrival times. The problem specifies that their common mean $\\mu = \\mathbb{E}[X_n]$ is finite and positive, so $0 < \\mu < \\infty$. This implies that $\\mathbb{P}(X_n > 0) > 0$. We define the sequence of arrival times, or renewal epochs, $\\{S_n\\}_{n \\ge 0}$ by setting $S_0 \\equiv 0$ and $S_n \\equiv \\sum_{k=1}^{n} X_k$ for $n \\ge 1$. The counting process $\\{N(t) : t \\ge 0\\}$ counts the number of renewals that have occurred by time $t$. It is defined for each $t \\ge 0$ as $N(t) \\equiv \\max\\{n \\ge 0 : S_n \\le t\\}$.\n\nTo show that $N(t)$ is a well-defined stochastic process, we must verify that for any given outcome $\\omega$ in the sample space (which determines the entire sequence of inter-arrival times $\\{X_n(\\omega)\\}$) and for any fixed time $t \\ge 0$, the quantity $N(t)(\\omega)$ is a well-defined finite number. The definition of $N(t)$ involves taking the maximum of the set $A(t) = \\{n \\ge 0 : S_n \\le t\\}$.\n The set $A(t)$ is a set of nonnegative integers. It is non-empty because $S_0=0$, and for any $t \\ge 0$, $S_0 \\le t$, so $0 \\in A(t)$. For the maximum to exist, the set $A(t)$ must be bounded above. This is guaranteed by the condition that $\\mu = \\mathbb{E}[X_1] > 0$. By the Strong Law of Large Numbers, we have that $\\frac{S_n}{n} \\to \\mu$ as $n \\to \\infty$, almost surely. Since $\\mu > 0$, this implies that $S_n \\to \\infty$ as $n \\to \\infty$, almost surely. Therefore, for any finite time $t$, there can only be a finite number of renewal epochs $S_n$ that are less than or equal to $t$. This means the set $A(t)$ is almost surely finite. A finite, non-empty set of integers has a well-defined maximum. Thus, $N(t)$ is a well-defined random variable for each $t \\ge 0$, and $\\{N(t) : t \\ge 0\\}$ is a well-defined stochastic process.\n\nSecond, we derive the fundamental renewal equation for the renewal function $m(t) \\equiv \\mathbb{E}[N(t)]$. We start from the definition of $m(t)$ and apply the law of total expectation, conditioning on the time of the first arrival, $X_1$.\n$$m(t) = \\mathbb{E}[N(t)] = \\mathbb{E}[\\mathbb{E}[N(t) | X_1]]$$\nLet's analyze the inner conditional expectation, $\\mathbb{E}[N(t) | X_1 = x]$, for a given value $x \\ge 0$.\nThere are two cases:\nCase 1: $x > t$. If the first arrival occurs after time $t$, then no arrivals have occurred in the interval $[0, t]$. Therefore, $N(t) = 0$.\nCase 2: $x \\le t$. The first arrival occurs at time $x$. The process of arrivals effectively \"renews\" at this point. The number of subsequent arrivals in the time interval $(x, t]$ depends on the remaining inter-arrival times $\\{X_2, X_3, \\dots\\}$. The number of these subsequent arrivals is equivalent to the number of arrivals in a new, independent renewal process observed over a time duration of $t-x$. So, if $X_1=x\\le t$, we can write $N(t) = 1 + N'(t-x)$, where $N'(u)$ is the number of arrivals in a renewal process defined by inter-arrival times $X_2, X_3, \\dots$. Since the $X_n$ are i.i.d., the process $\\{N'(u)\\}$ has the same distribution as $\\{N(u)\\}$.\nTherefore, $\\mathbb{E}[N(t) | X_1 = x] = \\mathbb{E}[1 + N'(t-x)] = 1 + \\mathbb{E}[N(t-x)] = 1 + m(t-x)$.\n\nCombining these two cases, we have:\n$$\\mathbb{E}[N(t) | X_1 = x] = \\begin{cases} 1 + m(t-x) & \\text{if } 0 \\le x \\le t \\\\ 0 & \\text{if } x > t \\end{cases}$$\nNow we integrate this conditional expectation over all possible values of $X_1$ with respect to its distribution function $F$.\n$$m(t) = \\int_{0}^{\\infty} \\mathbb{E}[N(t) | X_1 = x] \\, dF(x)$$\nWe split the integral at $t$:\n$$m(t) = \\int_{0}^{t} (1 + m(t-x)) \\, dF(x) + \\int_{t}^{\\infty} 0 \\, dF(x)$$\n$$m(t) = \\int_{0}^{t} 1 \\, dF(x) + \\int_{0}^{t} m(t-x) \\, dF(x)$$\nThe first integral is $\\int_{0}^{t} dF(x) = \\mathbb{P}(X_1 \\le t)$, which is by definition the distribution function $F(t)$ (assuming $F(0)=\\mathbb{P}(X_1\\le 0)=0$ as $X_1$ is non-negative with positive mean). This gives the fundamental renewal equation:\n$$m(t) = F(t) + \\int_{0}^{t} m(t-x) \\, dF(x)$$\n\nThird, we define arithmetic and nonarithmetic distributions and classify the given examples.\nA random variable $X$ (and its distribution) is said to be **arithmetic** (or **lattice**) if there exists a number $d > 0$ such that the support of $X$ is a subset of the set $\\{0, \\pm d, \\pm 2d, \\dots\\}$. The largest such $d$ is called the span of the distribution. If a distribution is not arithmetic, it is called **nonarithmetic** (or **nonlattice**).\nWe classify the three distributions as follows:\n1.  **Deterministic**: $X_1 = a$ almost surely for some $a > 0$. The support of this distribution is the singleton set $\\{a\\}$. This set is a subset of $\\{ka\\}_{k \\in \\mathbb{Z}}$. Thus, the distribution is **arithmetic** with span $d=a$.\n2.  **Uniform**: $X_1 \\sim \\mathrm{Unif}([0,1])$. The support of this distribution is the continuous interval $[0,1]$. For any $d>0$, the set $\\{kd\\}_{k \\in \\mathbb{Z}}$ is a discrete set of points. An interval containing more than one point cannot be a subset of a discrete set. Therefore, the uniform distribution is **nonarithmetic**.\n3.  **Exponential**: $X_1 \\sim \\mathrm{Exp}(\\lambda)$ for $\\lambda > 0$. The support of this distribution is the interval $[0, \\infty)$. As with the uniform distribution, this continuous support cannot be contained within any lattice $\\{kd\\}_{k \\in \\mathbb{Z}}$. Thus, the exponential distribution is **nonarithmetic**.\n\nFourth, for the case $X_1 \\sim \\mathrm{Unif}([0,1])$, we compute the unilateral Laplace transform of the renewal function, $M(s) \\equiv \\int_{0}^{\\infty} \\exp(-s t)\\, m(t)\\, dt$.\nThe renewal equation $m(t) = F(t) + \\int_{0}^{t} m(t-x) \\, dF(x)$ is a convolution-type equation. Let $M(s) = \\mathcal{L}\\{m\\}(s)$. Let $\\hat{F}(s) = \\mathcal{L}\\{dF\\}(s) = \\mathbb{E}[\\exp(-sX_1)]$ be the Laplace-Stieltjes transform of $F$. The integral term is a convolution, so its Laplace transform is $M(s)\\hat{F}(s)$. The Laplace transform of the distribution function $F(t)$ is $\\mathcal{L}\\{F\\}(s) = \\int_0^\\infty \\exp(-st)F(t)dt$. Using integration by parts, and the fact that $F(0)=0$ and $F(t)\\exp(-st) \\to 0$ as $t \\to \\infty$ for $\\Re(s)>0$, we find $\\mathcal{L}\\{F\\}(s) = \\frac{1}{s} \\hat{F}(s)$.\nApplying the Laplace transform to the renewal equation yields:\n$$M(s) = \\frac{1}{s}\\hat{F}(s) + M(s)\\hat{F}(s)$$\nSolving for $M(s)$:\n$$M(s) (1 - \\hat{F}(s)) = \\frac{\\hat{F}(s)}{s}$$\n$$M(s) = \\frac{\\hat{F}(s)}{s(1 - \\hat{F}(s))}$$\nThis is a general formula for the Laplace transform of the renewal function. Now, we specialize to $X_1 \\sim \\mathrm{Unif}([0,1])$. The probability density function is $f(x) = 1$ for $x \\in [0,1]$ and $f(x)=0$ otherwise.\nWe compute $\\hat{F}(s)$:\n$$\\hat{F}(s) = \\mathbb{E}[\\exp(-sX_1)] = \\int_{0}^{\\infty} \\exp(-sx) f(x) \\, dx = \\int_{0}^{1} \\exp(-sx) \\cdot 1 \\, dx$$\n$$\\hat{F}(s) = \\left[ \\frac{\\exp(-sx)}{-s} \\right]_{0}^{1} = \\frac{\\exp(-s)}{-s} - \\frac{\\exp(0)}{-s} = \\frac{1 - \\exp(-s)}{s}$$\nSubstituting this expression for $\\hat{F}(s)$ into the formula for $M(s)$:\n$$M(s) = \\frac{\\frac{1 - \\exp(-s)}{s}}{s\\left(1 - \\frac{1 - \\exp(-s)}{s}\\right)}$$\nNow, we simplify the expression:\n$$M(s) = \\frac{\\frac{1 - \\exp(-s)}{s}}{s\\left(\\frac{s - (1 - \\exp(-s))}{s}\\right)} = \\frac{\\frac{1 - \\exp(-s)}{s}}{s - 1 + \\exp(-s)} = \\frac{1 - \\exp(-s)}{s(s - 1 + \\exp(-s))}$$\nThis expression is analytic for $\\Re(s)>0$, as the mean inter-arrival time is $\\mu = \\frac{1}{2} < \\infty$, and the denominator $s-1+\\exp(-s)$ has no zeros in the open right half-plane.", "answer": "$$\\boxed{\\frac{1 - \\exp(-s)}{s(s - 1 + \\exp(-s))}}$$", "id": "2998410"}, {"introduction": "Lévy processes represent a vast and fundamental family of stochastic processes characterized by stationary and independent increments, encompassing both Brownian motion and Poisson processes. The celebrated Lévy-Khintchine formula provides a complete classification of any such process through its characteristic \"Lévy triplet\": a drift, a Gaussian component, and a measure describing its jumps. This hands-on problem [@problem_id:2998400] offers a practical dive into this powerful classification framework, challenging you to construct the Lévy triplet for a compound Poisson process and investigate how its structure dictates crucial path properties like jump activity and variation.", "problem": "Let $N_{t}$ be a Poisson process with rate $\\lambda>0$, and let $\\{Y_{k}\\}_{k\\geq 1}$ be an independent and identically distributed sequence with $\\mathbb{P}(Y_{k}=1)=\\mathbb{P}(Y_{k}=-1)=\\frac{1}{2}$. Define the compound Poisson process $X^{(1)}_{t}=\\sum_{k=1}^{N_{t}}Y_{k}$. Consider the truncation function $h(x)=x\\,\\mathbf{1}_{\\{|x|\\leq 1\\}}$. \n\n(a) Using only the definitions of a Lévy process and a compound Poisson process, and the characterization of Lévy processes by their Lévy–Khintchine exponent with a truncation function, construct the Lévy measure associated with $X^{(1)}$ and determine its Lévy triplet under the truncation function $h$.\n\nNow, define an independent pure-jump Lévy process $X^{(0)}$ with Lévy measure given by a symmetric Lévy density near the origin:\n$$\n\\nu_{0}(\\mathrm{d}x)=\\rho\\,|x|^{-1-\\beta}\\,\\mathbf{1}_{\\{0<|x|<1\\}}\\,\\mathrm{d}x,\n$$\nwhere $\\rho>0$ and $\\beta\\in(0,1)$. Let the modified process be $X_{t}=X^{(1)}_{t}+X^{(0)}_{t}$.\n\n(b) Starting from the defining integrability conditions for a Lévy measure and the definition of total variation of a pure-jump Lévy process on finite time intervals, justify that $X$ has infinite jump activity and finite variation almost surely on finite time horizons. Then, determine the Lévy triplet of $X$ under the same truncation function $h$.\n\nProvide your final answer as the Lévy triplet of the modified process $X$ in a single row using the $\\mathrm{pmatrix}$ environment, with entries equal to the drift parameter, the Gaussian variance parameter, and the Lévy measure written explicitly as an analytic expression. No numerical evaluation is required.", "solution": "The problem is addressed in two parts as specified.\n\nPart (a): Analysis of the compound Poisson process $X^{(1)}_{t}$.\n\nA compound Poisson process $X^{(1)}_{t} = \\sum_{k=1}^{N_{t}} Y_{k}$, where $N_{t}$ is a Poisson process with rate $\\lambda > 0$ and $\\{Y_{k}\\}$ are i.i.d. random variables independent of $N_{t}$, is a Lévy process. Its Lévy measure $\\nu_{1}$ is given by $\\nu_{1}(B) = \\lambda \\mathbb{P}(Y_{1} \\in B)$ for any Borel set $B \\subset \\mathbb{R} \\setminus \\{0\\}$.\nGiven that $\\mathbb{P}(Y_{k}=1) = \\mathbb{P}(Y_{k}=-1) = \\frac{1}{2}$, the distribution of $Y_{1}$ is $\\frac{1}{2}\\delta_{1} + \\frac{1}{2}\\delta_{-1}$, where $\\delta_{c}$ is the Dirac measure concentrated at point $c$.\nThus, the Lévy measure for $X^{(1)}$ is:\n$$ \\nu_{1}(\\mathrm{d}x) = \\lambda \\left( \\frac{1}{2}\\delta_{1}(\\mathrm{d}x) + \\frac{1}{2}\\delta_{-1}(\\mathrm{d}x) \\right) = \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1})(\\mathrm{d}x) $$\nThe Lévy triplet of a process is denoted $(\\gamma, A, \\nu)$ with respect to a given truncation function $h(x)$. The characteristic exponent $\\Psi(u) = -\\log \\mathbb{E}[\\exp(iuX_{1})]$ is given by the Lévy–Khintchine formula:\n$$ \\Psi(u) = -i\\gamma u + \\frac{1}{2}A u^{2} - \\int_{\\mathbb{R}\\setminus\\{0\\}} \\left( \\exp(iux) - 1 - iuh(x) \\right) \\nu(\\mathrm{d}x) $$\nFor the process $X^{(1)}_{t}$, we can compute its characteristic exponent directly. The characteristic function of $Y_{1}$ is $\\phi_{Y}(u) = \\mathbb{E}[\\exp(iuY_{1})] = \\frac{1}{2}\\exp(iu) + \\frac{1}{2}\\exp(-iu) = \\cos(u)$.\nThe characteristic function of $X^{(1)}_{t}$ is $\\mathbb{E}[\\exp(iuX^{(1)}_{t})] = \\exp(\\lambda t (\\phi_{Y}(u)-1)) = \\exp(\\lambda t (\\cos(u)-1))$.\nThe characteristic exponent for $X^{(1)}_{1}$ is therefore $\\Psi_{1}(u) = -\\log[\\exp(\\lambda(\\cos(u)-1))] = \\lambda(1 - \\cos(u))$.\n\nNow we match this with the Lévy–Khintchine formula to find the triplet $(\\gamma_{1}, A_{1}, \\nu_{1})$ for $X^{(1)}$ with the truncation function $h(x)=x\\,\\mathbf{1}_{\\{|x|\\leq 1\\}}$.\nSince $X^{(1)}$ is a pure-jump process (it only changes value by discrete jumps), there is no continuous Gaussian component. Hence, $A_{1}=0$.\nThe integral term in the formula becomes:\n$$ I_{1}(u) = \\int_{\\mathbb{R}\\setminus\\{0\\}} \\left( \\exp(iux) - 1 - iuh(x) \\right) \\nu_{1}(\\mathrm{d}x) $$\nThe support of $\\nu_{1}$ is the set $\\{-1, 1\\}$. For $x \\in \\{-1, 1\\}$, we have $|x|=1$, so the condition $|x|\\leq 1$ in the definition of $h(x)$ is satisfied. This means $h(1)=1$ and $h(-1)=-1$. Therefore, $h(x)=x$ on the support of $\\nu_{1}$.\nThe integral is calculated as:\n$$ I_{1}(u) = \\int_{\\mathbb{R}\\setminus\\{0\\}} \\left( \\exp(iux) - 1 - iux \\right) \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1})(\\mathrm{d}x) $$\n$$ = \\frac{\\lambda}{2} \\left[ (\\exp(iu(1)) - 1 - iu(1)) + (\\exp(iu(-1)) - 1 - iu(-1)) \\right] $$\n$$ = \\frac{\\lambda}{2} \\left[ (\\exp(iu) - 1 - iu) + (\\exp(-iu) - 1 + iu) \\right] = \\frac{\\lambda}{2} \\left[ \\exp(iu) + \\exp(-iu) - 2 \\right] $$\n$$ = \\frac{\\lambda}{2} [2\\cos(u) - 2] = \\lambda(\\cos(u)-1) $$\nSubstituting this into the Lévy–Khintchine formula for $X^{(1)}$:\n$$ \\Psi_{1}(u) = -i\\gamma_{1} u - I_{1}(u) = -i\\gamma_{1} u - \\lambda(\\cos(u)-1) = -i\\gamma_{1} u + \\lambda(1-\\cos(u)) $$\nComparing this to the directly computed exponent $\\Psi_{1}(u) = \\lambda(1 - \\cos(u))$, we find that $-i\\gamma_{1} u = 0$ for all $u$, which implies $\\gamma_{1}=0$.\nThe Lévy triplet for $X^{(1)}_{t}$ with the given truncation function is $(\\gamma_{1}, A_{1}, \\nu_{1}) = (0, 0, \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1}))$.\n\nPart (b): Analysis of the modified process $X_{t} = X^{(1)}_{t} + X^{(0)}_{t}$.\n\nSince $X^{(1)}$ and $X^{(0)}$ are independent Lévy processes, their sum $X_{t}$ is also a Lévy process. Its Lévy triplet $(\\gamma, A, \\nu)$ is the sum of the individual triplets: $(\\gamma, A, \\nu) = (\\gamma_{1}+\\gamma_{0}, A_{1}+A_{0}, \\nu_{1}+\\nu_{0})$. The truncation function remains $h(x)=x\\,\\mathbf{1}_{\\{|x|\\leq 1\\}}$. The Lévy measure of $X_{t}$ is\n$$ \\nu(\\mathrm{d}x) = \\nu_{1}(\\mathrm{d}x) + \\nu_{0}(\\mathrm{d}x) = \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1})(\\mathrm{d}x) + \\rho\\,|x|^{-1-\\beta}\\,\\mathbf{1}_{\\{0<|x|<1\\}}\\,\\mathrm{d}x $$\nFirst, we justify the path properties of $X_{t}$.\nThe jump activity of a Lévy process is determined by the total mass of its Lévy measure, $\\int_{\\mathbb{R}\\setminus\\{0\\}} \\nu(\\mathrm{d}x)$. This represents the expected number of jumps per unit time.\n$$ \\int_{\\mathbb{R}\\setminus\\{0\\}} \\nu(\\mathrm{d}x) = \\int \\nu_{1}(\\mathrm{d}x) + \\int \\nu_{0}(\\mathrm{d}x) = \\frac{\\lambda}{2}(1+1) + \\int_{0<|x|<1} \\rho|x|^{-1-\\beta} \\mathrm{d}x $$\nThe second integral is:\n$$ \\int_{0<|x|<1} \\rho|x|^{-1-\\beta} \\mathrm{d}x = 2\\rho \\int_{0}^{1} x^{-1-\\beta} \\mathrm{d}x = 2\\rho \\left[ \\frac{x^{-\\beta}}{-\\beta} \\right]_{0}^{1} $$\nSince $\\beta \\in (0,1)$, the exponent $-\\beta$ is negative. As $x \\to 0^{+}$, $x^{-\\beta} \\to \\infty$, so the integral diverges. Thus, $\\int \\nu(\\mathrm{d}x) = \\infty$, which means the process $X_{t}$ has infinite jump activity.\n\nThe variation of a pure-jump Lévy process is finite on finite time intervals almost surely if and only if the condition $\\int_{\\mathbb{R}\\setminus\\{0\\}} \\min(1, |x|)\\,\\nu(\\mathrm{d}x) < \\infty$ holds. For $X_t$:\n$$ \\int_{\\mathbb{R}\\setminus\\{0\\}} \\min(1, |x|)\\,\\nu(\\mathrm{d}x) = \\int \\min(1, |x|)\\,\\nu_{1}(\\mathrm{d}x) + \\int \\min(1, |x|)\\,\\nu_{0}(\\mathrm{d}x) $$\nThe integral with respect to $\\nu_{1}$ is:\n$$ \\int \\min(1, |x|) \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1})(\\mathrm{d}x) = \\frac{\\lambda}{2}(\\min(1,1) + \\min(1,1)) = \\lambda $$\nThe integral with respect to $\\nu_{0}$ is, noting that on the support of $\\nu_{0}$ we have $|x|<1$ and thus $\\min(1,|x|)=|x|$:\n$$ \\int_{0<|x|<1} |x| \\cdot \\rho|x|^{-1-\\beta} \\mathrm{d}x = \\rho \\int_{0<|x|<1} |x|^{-\\beta} \\mathrm{d}x = 2\\rho \\int_{0}^{1} x^{-\\beta} \\mathrm{d}x $$\n$$ = 2\\rho \\left[ \\frac{x^{1-\\beta}}{1-\\beta} \\right]_{0}^{1} = 2\\rho \\left(\\frac{1}{1-\\beta} - 0\\right) = \\frac{2\\rho}{1-\\beta} $$\nSince $\\beta \\in (0,1)$, this integral is finite. The total integral is $\\lambda + \\frac{2\\rho}{1-\\beta}$, which is finite. Therefore, $X_{t}$ has paths of finite variation almost surely.\n\nNow we determine the Lévy triplet $(\\gamma, A, \\nu)$ of $X_{t}$. We need to find the triplet $(\\gamma_{0}, A_{0}, \\nu_{0})$ for $X^{(0)}_{t}$.\n$X^{(0)}$ is given as a pure-jump process, so its Gaussian component is $A_{0}=0$.\nThe Lévy measure $\\nu_{0}(\\mathrm{d}x) = \\rho\\,|x|^{-1-\\beta}\\,\\mathbf{1}_{\\{0<|x|<1\\}}\\,\\mathrm{d}x$ is symmetric, i.e., $\\nu_{0}(B) = \\nu_{0}(-B)$ for any Borel set $B$. A Lévy process with a symmetric Lévy measure is a symmetric process. This implies its characteristic function is real and even, so its characteristic exponent $\\Psi_{0}(u)$ must be real and even.\nLet's analyze $\\Psi_{0}(u) = -i\\gamma_{0} u + \\frac{1}{2}A_{0} u^{2} - \\int (\\exp(iux) - 1 - iuh(x)) \\nu_{0}(\\mathrm{d}x)$.\nWith $A_{0}=0$ and $h(x)$ being an odd function, we examine the imaginary part of $\\Psi_0(u)$. Let $\\Im(\\cdot)$ denote the imaginary part.\n$$ \\Im(\\Psi_{0}(u)) = -\\gamma_{0} u - \\int_{\\mathbb{R}\\setminus\\{0\\}} (\\sin(ux) - uh(x)) \\nu_{0}(\\mathrm{d}x) $$\nOn the support of $\\nu_{0}$, we have $|x|<1$, so $h(x)=x$. The integrand is $(\\sin(ux)-ux)$, which is an odd function of $x$. The measure density $|x|^{-1-\\beta}$ is an even function of $x$. The integral of an odd function against an even measure over a symmetric domain (here $(-1,0)\\cup(0,1)$) is zero.\nSo, $\\int (\\sin(ux) - uh(x)) \\nu_{0}(\\mathrm{d}x) = 0$.\nThis leaves $\\Im(\\Psi_{0}(u)) = -\\gamma_{0} u$. Since $\\Psi_{0}(u)$ must be real for a symmetric process, we must have $\\Im(\\Psi_{0}(u))=0$, which implies $-\\gamma_{0} u = 0$ for all $u$, and thus $\\gamma_{0}=0$.\nThe Lévy triplet for $X^{(0)}_{t}$ is $(\\gamma_{0}, A_{0}, \\nu_{0}) = (0, 0, \\rho\\,|x|^{-1-\\beta}\\,\\mathbf{1}_{\\{0<|x|<1\\}}\\,\\mathrm{d}x)$.\n\nFinally, we sum the triplets for $X^{(1)}$ and $X^{(0)}$ to get the triplet for $X_{t}$:\n$\\gamma = \\gamma_{1} + \\gamma_{0} = 0 + 0 = 0$.\n$A = A_{1} + A_{0} = 0 + 0 = 0$.\n$\\nu = \\nu_{1} + \\nu_{0}$, which is $\\nu(\\mathrm{d}x) = \\rho\\,|x|^{-1-\\beta}\\,\\mathbf{1}_{\\{0<|x|<1\\}}\\,\\mathrm{d}x + \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1})(\\mathrm{d}x)$.\n\nThe Lévy triplet of the modified process $X$ is $(0, 0, \\rho\\,|x|^{-1-\\beta}\\,\\mathbf{1}_{\\{0<|x|<1\\}}\\,\\mathrm{d}x + \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1}))$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 0 & \\rho\\,|x|^{-1-\\beta}\\,\\mathbf{1}_{\\{0<|x|<1\\}}\\,\\mathrm{d}x + \\frac{\\lambda}{2}(\\delta_{1} + \\delta_{-1})\n\\end{pmatrix}\n}\n$$", "id": "2998400"}, {"introduction": "A crucial aspect of classifying a stochastic process is understanding the geometric properties of its sample paths, such as continuity and smoothness. The Kolmogorov continuity theorem establishes a profound connection between the statistical moments of a process's increments and the regularity of its paths. This exercise [@problem_id:2998396] demonstrates this principle in action, asking you to leverage the specific properties of a Gaussian process to establish concrete bounds that guarantee a certain level of Hölder continuity, a skill essential for the theoretical analysis of continuous stochastic models.", "problem": "Consider a real-valued centered Gaussian process $\\{X_{t} : t \\in [0,1]\\}$ with stationary increments and continuous covariance, and suppose there exist constants $\\sigma > 0$ and $H \\in (0,1)$ such that for all $s,t \\in [0,1]$,\n$$\n\\operatorname{Var}(X_{t} - X_{s}) \\leq \\sigma^{2} |t - s|^{2H}.\n$$\nWork from first principles, beginning with the definitions of a modification and $\\gamma$-Hölder continuity of sample paths, and the classical moment characterization of centered Gaussian random variables. State a precise version of the Kolmogorov continuity theorem for $\\mathbb{R}$-valued processes indexed by a one-dimensional parameter. Then, for a fixed moment order $q > \\frac{1}{H}$, derive an explicit increment moment bound of the form\n$$\n\\mathbb{E}\\big(|X_{t} - X_{s}|^{\\alpha}\\big) \\leq C |t - s|^{1 + \\beta},\n$$\nvalid for all $s,t \\in [0,1]$, by identifying suitable exponents $\\alpha$ and $\\beta$ in terms of $H$ and $q$. Finally, determine the supremum Hölder index $\\gamma^{\\star}$ guaranteed by the Kolmogorov continuity theorem as a function of $H$ and $q$. Your final answer must be the single symbolic expression of the row vector $(\\alpha, \\beta, \\gamma^{\\star})$ in terms of $H$ and $q$.", "solution": "The solution requires starting from first principles. We will first state the necessary definitions and the Kolmogorov continuity theorem.\n\n**Step 1: Definitions and Preliminary Results**\n\n1.  **Modification of a Stochastic Process:** Let $\\{X_t\\}_{t \\in T}$ and $\\{Y_t\\}_{t \\in T}$ be two real-valued stochastic processes defined on the same probability space and indexed by the same set $T$. The process $\\{Y_t\\}$ is called a modification of $\\{X_t\\}$ if for every $t \\in T$, we have $P(X_t=Y_t) = 1$.\n\n2.  **$\\gamma$-Hölder Continuity:** A sample path of a process, which is a function $f: [0,1] \\to \\mathbb{R}$, is said to be globally $\\gamma$-Hölder continuous for an exponent $\\gamma \\in (0,1]$ if there exists a constant $K \\geq 0$ (which may depend on the sample path) such that for all $s, t \\in [0,1]$, the inequality $|f(t) - f(s)| \\leq K|t-s|^{\\gamma}$ holds.\n\n3.  **Moments of a Centered Gaussian Random Variable:** Let $Z$ be a centered Gaussian random variable, i.e., $Z \\sim \\mathcal{N}(0, v^2)$ where $v^2 = \\operatorname{Var}(Z)$. For any real number $p > 0$, the $p$-th absolute moment of $Z$ is given by\n    $$\n    \\mathbb{E}\\big[|Z|^{p}\\big] = M_p (v^2)^{p/2} = M_p \\left(\\sqrt{\\operatorname{Var}(Z)}\\right)^{p},\n    $$\n    where $M_p = \\mathbb{E}[|N|^p]$ is a constant depending only on $p$, with $N \\sim \\mathcal{N}(0,1)$ being a standard normal random variable. This constant is given by $M_p = \\frac{2^{p/2} \\Gamma(\\frac{p+1}{2})}{\\sqrt{\\pi}}$.\n\n**Step 2: The Kolmogorov Continuity Theorem**\n\nA standard version of the Kolmogorov continuity theorem for real-valued processes indexed by a one-dimensional parameter is as follows:\nLet $\\{X_t : t \\in [0,1]\\}$ be a real-valued stochastic process. Suppose there exist constants $K > 0$, $\\alpha > 0$, and $\\beta > 0$ such that for all $s, t \\in [0,1]$, the moment condition\n$$\n\\mathbb{E}\\big[|X_t - X_s|^{\\alpha}\\big] \\leq K |t-s|^{1+\\beta}\n$$\nis satisfied. Then there exists a modification $\\tilde{X}$ of $X$ whose sample paths are almost surely globally $\\gamma$-Hölder continuous for any exponent $\\gamma$ such that $0 < \\gamma < \\frac{\\beta}{\\alpha}$. The supremum of all such guaranteed Hölder exponents is $\\gamma^{\\star} = \\frac{\\beta}{\\alpha}$.\n\n**Step 3: Derivation of the Increment Moment Bound**\n\nWe are given a centered Gaussian process $\\{X_t : t \\in [0,1]\\}$. For any $s, t \\in [0,1]$, the increment $Y_{s,t} = X_t - X_s$ is a linear combination of Gaussian random variables, and thus is itself a Gaussian random variable. Since $X_t$ is centered for all $t$, we have $\\mathbb{E}[Y_{s,t}] = \\mathbb{E}[X_t] - \\mathbb{E}[X_s] = 0 - 0 = 0$. So, $Y_{s,t}$ is a centered Gaussian random variable.\n\nThe variance of this increment is $\\operatorname{Var}(Y_{s,t}) = \\operatorname{Var}(X_t - X_s)$. The problem provides the bound:\n$$\n\\operatorname{Var}(X_t - X_s) \\leq \\sigma^2 |t-s|^{2H}.\n$$\nWe want to find a moment bound of the form specified in the Kolmogorov theorem. We use the moment characterization for the centered Gaussian variable $Y_{s,t} = X_t - X_s$. For any $p > 0$:\n$$\n\\mathbb{E}\\big[|X_t - X_s|^p\\big] = M_p \\left(\\operatorname{Var}(X_t - X_s)\\right)^{p/2}.\n$$\nSubstituting the given variance bound, we obtain:\n$$\n\\mathbb{E}\\big[|X_t - X_s|^p\\big] \\leq M_p \\left( \\sigma^2 |t-s|^{2H} \\right)^{p/2} = M_p \\sigma^p |t-s|^{Hp}.\n$$\nThe problem asks us to find suitable exponents $\\alpha$ and $\\beta$ for a fixed moment order $q > \\frac{1}{H}$. We choose our moment order $p$ to be this given $q$. Let $p=q$. The moment inequality becomes:\n$$\n\\mathbb{E}\\big[|X_t - X_s|^q\\big] \\leq (M_q \\sigma^q) |t-s|^{Hq}.\n$$\nNow, we must match this to the form required by the Kolmogorov continuity theorem, $\\mathbb{E}[|X_t - X_s|^{\\alpha}] \\leq C|t-s|^{1+\\beta}$.\nBy comparing the two expressions, we can identify the parameters:\n1.  The moment exponent is $\\alpha = q$.\n2.  The constant is $C = M_q \\sigma^q$.\n3.  The exponent of the time difference term must match: $Hq = 1 + \\beta$.\n\nFrom $Hq = 1 + \\beta$, we can solve for $\\beta$:\n$$\n\\beta = Hq - 1.\n$$\nFor the Kolmogorov theorem to apply, we need $\\alpha > 0$ and $\\beta > 0$. Since $q > 1/H$ and $H>0$, we have $q>0$, so $\\alpha>0$. The condition $q > 1/H$ is equivalent to $Hq > 1$, which implies $\\beta = Hq - 1 > 0$. Thus, the conditions of the theorem are met with these choices of $\\alpha$ and $\\beta$.\n\nThe suitable exponents are $\\alpha = q$ and $\\beta = Hq-1$.\n\n**Step 4: Determination of the Supremum Hölder Index**\n\nHaving established the moment bound $\\mathbb{E}[|X_t - X_s|^q] \\leq C |t-s|^{1+(Hq-1)}$ with $\\alpha=q$ and $\\beta=Hq-1$, we can apply the conclusion of the Kolmogorov continuity theorem.\nThe theorem guarantees the existence of a modification of $\\{X_t\\}$ with almost surely $\\gamma$-Hölder continuous sample paths for any $\\gamma < \\frac{\\beta}{\\alpha}$.\nThe supremum Hölder index $\\gamma^{\\star}$ guaranteed by the theorem is therefore:\n$$\n\\gamma^{\\star} = \\frac{\\beta}{\\alpha}.\n$$\nSubstituting the expressions for $\\alpha$ and $\\beta$ we found:\n$$\n\\gamma^{\\star} = \\frac{Hq - 1}{q} = H - \\frac{1}{q}.\n$$\nThe condition $q > 1/H$ ensures that $\\gamma^{\\star} = H - 1/q > 0$. Since $H \\in (0,1)$, we have $\\gamma^{\\star} < H < 1$.\n\n**Step 5: Final Result**\n\nThe problem asks for the row vector $(\\alpha, \\beta, \\gamma^{\\star})$ as functions of $H$ and $q$. Based on our derivation:\n-   $\\alpha = q$\n-   $\\beta = Hq - 1$\n-   $\\gamma^{\\star} = H - \\frac{1}{q}$\n\nThe resulting row vector is $(q, Hq-1, H-\\frac{1}{q})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nq & Hq - 1 & H - \\frac{1}{q}\n\\end{pmatrix}\n}\n$$", "id": "2998396"}]}