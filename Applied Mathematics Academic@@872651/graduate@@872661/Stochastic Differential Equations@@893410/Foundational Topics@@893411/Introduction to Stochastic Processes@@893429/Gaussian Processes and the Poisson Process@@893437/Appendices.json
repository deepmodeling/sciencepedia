{"hands_on_practices": [{"introduction": "To truly grasp a stochastic process, it's invaluable to see it in action. This first practice moves from abstract definitions to concrete implementation by guiding you through the simulation of a homogeneous Poisson process. By leveraging the fundamental property that interarrival times are exponentially distributed, you will not only generate sample paths but also analyze the computational cost and verify the statistical properties of your simulation, bridging the gap between theory and practical application. [@problem_id:2978061]", "problem": "Design and implement a program that simulates a homogeneous Poisson process on the interval $[0,T]$ using exponential interarrival times. Your design must be grounded in first principles of stochastic processes and explicitly reason from the core definitions of the Poisson process and the exponential distribution, without invoking any unmotivated formulas. Then, analyze the computational complexity of your simulation algorithm in terms of the random number of arrivals it produces and its expectation as a function of the rate parameter. Finally, verify one distributional property of the simulated interarrival times via a nonparametric discrepancy measure.\n\nPrecise requirements:\n\n1. Fundamental basis and algorithmic design\n   - Start from the defining properties of a homogeneous Poisson process of rate $\\lambda$: the process has independent and stationary increments, the number of events in any interval of length $t$ is a Poisson-distributed random variable (equivalently derivable from the independent increments property and the small-interval characterization), and the interarrival times are independent and identically distributed exponential random variables of parameter $\\lambda$. Use the memorylessness of the exponential distribution to motivate simulating interarrival times sequentially.\n   - Construct an algorithm that draws independent exponential interarrival times with parameter $\\lambda$ and accumulates them until the running sum first exceeds $T$. The algorithm must return the list of all arrival times that lie within $[0,T]$. For $\\lambda \\le 0$ or $T \\le 0$, the algorithm must return an empty list.\n\n2. Computational complexity accounting\n   - Define the cost model where generating one exponential random variate has unit cost. Let $N_T$ denote the number of arrivals in $[0,T]$ and let $D_T$ denote the number of exponential variates drawn by your algorithm on a single run. Express $D_T$ in terms of the realized $N_T$ for $\\lambda0$ and $T0$, and specify its value for the boundary cases $\\lambda \\le 0$ or $T \\le 0$ by convention consistent with your implementation choice. Using linearity of expectation and the standard expectation of $N_T$, derive the expected cost $\\mathbb{E}[D_T]$ as a function of $\\lambda$ and $T$ for $\\lambda0$ and $T0$.\n\n3. Verification statistic for interarrival law\n   - Let the simulated interarrival times be $X_1,\\dots,X_n$ with $n=N_T$. Define the empirical one-sample Kolmogorov–Smirnov discrepancy\n     $$D_n \\equiv \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F(x) \\right|,$$\n     where $F_n$ is the empirical distribution function of $\\{X_i\\}$ and $F(x) = 1 - e^{-\\lambda x}$ is the exponential cumulative distribution function. Implement a computation of $D_n$ from first principles using the definitions of $F_n$ and $F$. If $n=0$, set $D_n=0$ by convention.\n\n4. Implementation constraints\n   - Use a pseudorandom number generator with a fixed seed $123456789$ to ensure reproducibility of the simulation outputs. Use independent draws across different test cases as they occur in a single program run.\n   - Use floating-point arithmetic for all real-valued outputs. If there are no arrivals, define the last arrival time to be $0$.\n\n5. Test suite\n   - Run your program on the following parameter pairs $(\\lambda,T)$:\n     - Case $1$: $\\lambda = 3.5$, $T = 2.0$.\n     - Case $2$: $\\lambda = 0.0$, $T = 5.0$.\n     - Case $3$: $\\lambda = 10.0$, $T = 0.0$.\n     - Case $4$: $\\lambda = 50.0$, $T = 0.1$.\n     - Case $5$: $\\lambda = 0.2$, $T = 100.0$.\n   - For each case, produce a list with the following five entries, in order:\n     - The realized number of arrivals $N_T$ (an integer).\n     - The realized number of exponential draws $D_T$ used by your algorithm under the unit-cost model (an integer).\n     - The Kolmogorov–Smirnov discrepancy $D_n$ between the empirical interarrival distribution and the exponential cumulative distribution function with parameter $\\lambda$ (a float).\n     - The last arrival time in $[0,T]$ or $0$ if there are no arrivals (a float).\n     - The theoretical expected number of arrivals $\\lambda T$ (a float).\n   - Thus the overall program output is a list of five such lists, one per test case.\n\n6. Output format\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each test case result itself is a bracketed comma-separated list with no spaces. For example, if there were two test cases resulting in $[1,2,0.0,0.1,3.14]$ and $[0,0,0.0,0.0,0.0]$, then the output line would be:\n     [[1,2,0.0,0.1,3.14],[0,0,0.0,0.0,0.0]]\n\nAll numerical quantities in this problem are to be treated as unitless real numbers; there are no physical units or angles involved. The final output must strictly follow the single-line format described above.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of stochastic processes, well-posed with clear and consistent requirements, and expressed in objective mathematical language. We proceed with a solution.\n\nThe solution is presented in three parts, as requested: the theoretical basis and algorithmic design, the computational complexity analysis, and the implementation of the verification statistic.\n\n**1. Fundamental Basis and Algorithmic Design**\n\nA homogeneous Poisson process, denoted by $\\{N_t\\}_{t \\ge 0}$, is a counting process that models the number of events occurring up to time $t$. For a process with a constant rate $\\lambda > 0$, it is defined by the following properties:\n1.  The process starts with zero events: $N_0 = 0$.\n2.  The increments are independent: for any sequence of times $0 \\le t_1  t_2  \\dots  t_k$, the random variables representing the number of events in disjoint intervals, $N_{t_2}-N_{t_1}, N_{t_3}-N_{t_2}, \\dots, N_{t_k}-N_{t_{k-1}}$, are mutually independent.\n3.  The increments are stationary: the distribution of the number of events in any interval, $N_{t+s} - N_s$, depends only on the length of the interval, $t$, and not on its starting point, $s$.\n\nFrom these axioms, it can be derived that the number of events in any interval of length $t$ follows a Poisson distribution with mean $\\lambda t$, i.e., $\\mathbb{P}(N_t=k) = \\frac{(\\lambda t)^k e^{-\\lambda t}}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nAn equivalent and powerful characterization of the Poisson process is through its interarrival times. Let $S_k$ be the time of the $k$-th event, with $S_0 = 0$. The interarrival times are defined as $X_k = S_k - S_{k-1}$ for $k \\ge 1$. A foundational result in stochastic process theory states that for a homogeneous Poisson process with rate $\\lambda$, the interarrival times $\\{X_k\\}_{k \\ge 1}$ are independent and identically distributed (i.i.d.) random variables following an exponential distribution with rate parameter $\\lambda$. The probability density function (PDF) of this distribution is $f(x) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$, and the cumulative distribution function (CDF) is $F(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n\nThis equivalence provides a direct method for simulating the event times of the process. We can generate a sequence of i.i.d. exponential random variates, which represent the times between consecutive events. The memorylessness property of the exponential distribution is fundamental here: $\\mathbb{P}(X > t+s | X > s) = \\mathbb{P}(X > t)$ for $s,t > 0$. This implies that the waiting time for the next event is independent of how long we have already waited, which is consistent with the independent increments property of the Poisson process.\n\nThe simulation algorithm for generating arrival times on the interval $[0, T]$ is as follows:\n1.  Initialize the current time $t_{\\text{current}} = 0$ and an empty list of arrival times, `arrivals`.\n2.  Handle the boundary cases: if the rate $\\lambda \\le 0$ or the time horizon $T \\le 0$, no events can occur. The process is empty, so we return an empty list.\n3.  For $\\lambda > 0$ and $T > 0$, enter a sequential generation loop:\n    a. Draw a random variate $\\Delta t$ from an exponential distribution with parameter $\\lambda$. A standard method to do this is via inverse transform sampling: if $U$ is a uniform random variate on $(0,1)$, then $\\Delta t = - \\frac{1}{\\lambda} \\ln(U)$ is an exponential($\\lambda$) variate.\n    b. Update the time of the next potential event: $t_{\\text{next}} = t_{\\text{current}} + \\Delta t$.\n    c. If $t_{\\text{next}} > T$, the next event occurs outside the observation window. The simulation for the interval $[0, T]$ is complete. We terminate the loop.\n    d. If $t_{\\text{next}} \\le T$, the event is within the interval. We record this event by adding $t_{\\text{next}}$ to our list of arrivals and update the current time: $t_{\\text{current}} = t_{\\text{next}}$. Then, we continue the loop to generate the next interarrival time.\n4.  The final output is the list of all recorded arrival times.\n\n**2. Computational Complexity Accounting**\n\nWe define the computational cost based on the number of random variates generated. Let one draw from the exponential distribution have a unit cost of $1$.\n\nLet $N_T$ be the random variable for the number of arrivals in the interval $[0, T]$. Let $D_T$ be the random variable for the total number of exponential variates drawn by the algorithm.\n\nFor the general case where $\\lambda > 0$ and $T > 0$:\nThe algorithm generates interarrival times $X_1, X_2, \\dots$ and stops when the cumulative sum first exceeds $T$. The arrival times are $S_k = \\sum_{i=1}^k X_i$.\nIf the realized number of arrivals in $[0, T]$ is $N_T = n$, this means that $S_n \\le T$ and $S_{n+1} > T$. To establish this, the algorithm must generate the interarrival times $X_1, \\dots, X_n, X_{n+1}$. The total number of draws is therefore $n+1$. Thus, for a given realization, the cost $D_T$ is related to the number of arrivals $N_T$ by the equation:\n$$D_T = N_T + 1$$\nThis relationship holds even when $N_T=0$, in which case $S_1 = X_1 > T$. The algorithm performs one draw ($D_T=1$) and finds zero arrivals, satisfying $1 = 0 + 1$.\n\nFor the boundary cases where $\\lambda \\le 0$ or $T \\le 0$, the algorithm is designed to immediately terminate without drawing any random variates. In these cases, the cost is deterministic: $D_T = 0$.\n\nTo find the expected cost $\\mathbb{E}[D_T]$ for $\\lambda > 0$ and $T > 0$, we use the linearity of expectation on the relationship $D_T = N_T + 1$:\n$$\\mathbb{E}[D_T] = \\mathbb{E}[N_T + 1] = \\mathbb{E}[N_T] + \\mathbb{E}[1]$$\nAs established from the definition of the homogeneous Poisson process, the random variable $N_T$ follows a Poisson distribution with parameter $\\lambda T$. The expectation of a Poisson($\\mu$) random variable is $\\mu$. Therefore, the expected number of arrivals is:\n$$\\mathbb{E}[N_T] = \\lambda T$$\nSubstituting this into the equation for the expected cost gives:\n$$\\mathbb{E}[D_T] = \\lambda T + 1$$\nFor the boundary cases, since $D_T$ is deterministically $0$, the expected cost is also $\\mathbb{E}[D_T] = 0$.\n\n**3. Verification Statistic for Interarrival Law**\n\nThe problem requires verifying the distribution of the simulated interarrival times using the one-sample Kolmogorov–Smirnov (KS) discrepancy. Let the realized interarrival times corresponding to the events within $[0, T]$ be $\\{X_1, \\dots, X_n\\}$, where $n=N_T$. Note that the last exponential variate drawn, $X_{n+1}$ (which causes the total time to exceed $T$), is not part of this set.\n\nThe KS discrepancy is defined as:\n$$D_n = \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F(x) \\right|$$\nHere, $F(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$ is the theoretical CDF of the exponential distribution with parameter $\\lambda$. $F_n(x)$ is the empirical distribution function (EDF) of the sample $\\{X_1, \\dots, X_n\\}$, defined as:\n$$F_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(X_i \\le x)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. If $n=0$, we define $D_n = 0$ by convention.\n\nTo compute $D_n$ from first principles for $n>0$, we first sort the observed interarrival times to get the order statistics $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$. The EDF $F_n(x)$ is a step function that jumps by $1/n$ at each $x_{(i)}$. Since the theoretical CDF $F(x)$ is continuous, the supremum of the absolute difference $|F_n(x) - F(x)|$ must occur at one of the jump points $x_{(i)}$. Specifically, we need to evaluate the difference just before and at each jump. This leads to the well-known computational formula:\n$$D_n = \\max_{i=1, \\dots, n} \\left\\{ \\frac{i}{n} - F(x_{(i)}), F(x_{(i)}) - \\frac{i-1}{n} \\right\\}$$\nThe implementation will calculate this maximum value over the $n$ sorted interarrival times. For each $i$ from $1$ to $n$, we compute the value of the theoretical CDF, $F(x_{(i)}) = 1 - e^{-\\lambda x_{(i)}}$, and then the two differences shown above. The largest of these $2n$ values is the desired statistic $D_n$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Poisson process simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        # (lambda, T)\n        (3.5, 2.0),\n        (0.0, 5.0),\n        (10.0, 0.0),\n        (50.0, 0.1),\n        (0.2, 100.0),\n    ]\n\n    # Initialize a single random number generator for reproducibility across all cases.\n    rng = np.random.default_rng(123456789)\n    \n    all_results = []\n    for lmbda, T in test_cases:\n        result = simulate_and_analyze(lmbda, T, rng)\n        all_results.append(result)\n\n    # Format the final output string exactly as required.\n    # e.g., [[1,2,0.1,0.2,0.3],[4,5,0.4,0.5,0.6]]\n    output_str = \"[\" + \",\".join(\n        \"[\" + \",\".join(map(str, res)) + \"]\" for res in all_results\n    ) + \"]\"\n    print(output_str)\n\ndef simulate_and_analyze(lmbda, T, rng):\n    \"\"\"\n    Simulates a homogeneous Poisson process and computes required statistics.\n\n    Args:\n        lmbda (float): The rate parameter lambda of the Poisson process.\n        T (float): The time horizon for the simulation interval [0, T].\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        list: A list containing [N_T, D_T, D_n, last_arrival_time, E_N_T].\n    \"\"\"\n    # Requirement: for lambda = 0 or T = 0, return an empty list of arrivals.\n    # The derived statistics are [0, 0, 0.0, 0.0, lambda*T].\n    if lmbda = 0 or T = 0:\n        expected_arrivals = lmbda * T if lmbda  0 and T  0 else 0.0\n        return [0, 0, 0.0, 0.0, expected_arrivals]\n    \n    # 1. Algorithmic Design: Simulate using exponential interarrivals.\n    arrivals = []\n    inter_arrivals = []\n    current_time = 0.0\n    draw_count = 0\n\n    while True:\n        # NumPy's exponential uses scale parameter beta = 1/lambda.\n        inter_arrival_time = rng.exponential(scale=1.0/lmbda)\n        draw_count += 1\n        current_time += inter_arrival_time\n\n        if current_time  T:\n            # The next arrival is after T, so we stop.\n            break\n        \n        arrivals.append(current_time)\n        inter_arrivals.append(inter_arrival_time)\n\n    # 2. Computational Complexity Accounting\n    N_T = len(arrivals)\n    # D_T is the total number of draws, which is N_T + 1 for the general case.\n    D_T = draw_count\n\n    # 3. Verification Statistic (Kolmogorov-Smirnov)\n    if N_T == 0:\n        D_n = 0.0\n    else:\n        # Calculate D_n from first principles.\n        n = N_T\n        # Sort the interarrival times to get the order statistics.\n        sorted_inter_arrivals = np.sort(inter_arrivals)\n        \n        max_discrepancy = 0.0\n        # Iterate through the sorted interarrivals to find the max discrepancy.\n        for i in range(n):\n            # Using 0-based index i for the i-th point (x_{i+1} in 1-based math)\n            x_i_sorted = sorted_inter_arrivals[i]\n            # Theoretical CDF value at the data point.\n            F_x_i = 1.0 - np.exp(-lmbda * x_i_sorted)\n            \n            # The supremum is found at the jump points. We check the difference\n            # from both sides of the jump.\n            # Using 1-based indexing for formula: D_n = max(i/n - F(x_i), F(x_i) - (i-1)/n)\n            # In 0-based index `i`, this corresponds to:\n            d1 = (i + 1) / n - F_x_i\n            d2 = F_x_i - i / n\n            \n            max_discrepancy = max(max_discrepancy, d1, d2)\n        D_n = max_discrepancy\n\n    # 4. Collect Final Statistics\n    last_arrival_time = arrivals[-1] if N_T  0 else 0.0\n    expected_arrivals = lmbda * T\n\n    return [N_T, D_T, float(D_n), float(last_arrival_time), float(expected_arrivals)]\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2978061"}, {"introduction": "We now turn our attention from the discrete jumps of the Poisson process to the continuous, erratic paths of the quintessential Gaussian process: standard Brownian motion. This exercise challenges you to analyze the extremal behavior of this process by deriving the distribution of its running maximum, $M_T = \\sup_{0 \\le t \\le T} W_t$. This practice is an excellent opportunity to master the elegant and powerful reflection principle, a symmetry argument that is a cornerstone of the study of Brownian motion and reveals profound properties about its path. [@problem_id:2978015]", "problem": "Let $\\{W_{t}\\}_{t \\ge 0}$ be a standard Brownian motion, that is, a zero-mean Gaussian process with continuous paths, stationary independent increments, $W_{0} = 0$, and variance $\\mathrm{Var}(W_{t}) = t$. For a fixed horizon $T > 0$, define the running maximum $M_{T} := \\sup_{0 \\le t \\le T} W_{t}$. Work from first principles of Gaussian processes, the reflection principle for Brownian motion, and basic properties of the normal distribution.\n\na) Using the reflection principle, derive the distribution function of $M_{T}$ for $a \\ge 0$ and then compute the expectation $\\mathbb{E}[M_{T}]$ in closed form as a function of $T$.\n\nb) Using the reflection principle together with the exponential Markov inequality and the moment generating function (MGF) of a Gaussian random variable, derive a nonasymptotic upper bound of the form $\\mathbb{P}(M_{T} \\ge a) \\le C \\exp\\!\\big(-a^{2}/(2T)\\big)$ that holds for all $a \\ge 0$. Identify the smallest constant $C$ that your method yields and provide the final bound explicitly as a function of $a$ and $T$.\n\nReport your final results as two expressions: first $\\mathbb{E}[M_{T}]$ and second your explicit tail bound $\\mathbb{P}(M_{T} \\ge a)$ upper bound. No numerical approximation is required.", "solution": "The problem asks for properties of the running maximum of a standard Brownian motion, $M_{T} = \\sup_{0 \\le t \\le T} W_{t}$. We will address the two parts of the problem sequentially.\n\na) Derivation of the distribution and expectation of $M_{T}$.\n\nThe first step is to find the cumulative distribution function (CDF) of $M_T$, denoted $F_{M_T}(a) = \\mathbb{P}(M_{T} \\le a)$. For $a  0$, since $W_0=0$ and the paths are continuous, the supremum $M_T$ must be non-negative, so $\\mathbb{P}(M_T \\le a) = 0$. We focus on the case $a \\ge 0$. It is more convenient to first compute the tail probability, $\\mathbb{P}(M_T  a)$. Since $M_T$ is a continuous random variable, this is the same as $\\mathbb{P}(M_T \\ge a)$.\n\nThe event $\\{M_T \\ge a\\}$ for $a > 0$ is equivalent to the event that the Brownian motion reaches the level $a$ at some time $t \\in [0, T]$. Let $\\tau_a = \\inf\\{t \\ge 0 : W_t = a\\}$ be the first-passage time to level $a$. Then $\\{M_T \\ge a\\} = \\{\\tau_a \\le T\\}$.\n\nWe use the reflection principle for Brownian motion. The event $\\{\\tau_a \\le T\\}$ can be partitioned into two disjoint events: $\\{\\tau_a \\le T \\text{ and } W_T \\ge a\\}$ and $\\{\\tau_a \\le T \\text{ and } W_T  a\\}$.\nBy the continuity of paths, the event $\\{W_T \\ge a\\}$ necessitates that the path must have crossed level $a$ at some point, so $\\{W_T \\ge a\\} \\subseteq \\{\\tau_a \\le T\\}$. Therefore, the first event is simply $\\{W_T \\ge a\\}$.\nSo, we have:\n$$\n\\mathbb{P}(M_T \\ge a) = \\mathbb{P}(\\tau_a \\le T) = \\mathbb{P}(W_T \\ge a) + \\mathbb{P}(\\tau_a \\le T, W_T  a)\n$$\nThe reflection principle states that, conditioned on hitting level $a$, the process is equally likely to end up above or below $a$ due to the symmetry of its increments. More formally, by the strong Markov property at time $\\tau_a$, the process $W'_{s} = W_{\\tau_a+s} - W_{\\tau_a}$ is a standard Brownian motion independent of the pre-$\\tau_a$ sigma-algebra. Since $W'_{s}$ is symmetric about $0$, for any time $u > 0$, $\\mathbb{P}(W'_{u}  0) = \\mathbb{P}(W'_{u}  0) = 1/2$. Applying this to our problem:\n$$\n\\mathbb{P}(\\tau_a \\le T, W_T  a) = \\mathbb{P}(\\tau_a \\le T, W_T - W_{\\tau_a}  0)\n$$\nBy symmetry of the increments of the process starting at $\\tau_a$:\n$$\n\\mathbb{P}(\\tau_a \\le T, W_T - W_{\\tau_a}  0) = \\mathbb{P}(\\tau_a \\le T, W_T - W_{\\tau_a}  0) = \\mathbb{P}(\\tau_a \\le T, W_T  a)\n$$\nAs established before, $\\{W_T > a\\}$ implies $\\{\\tau_a \\le T\\}$, so the event $\\{\\tau_a \\le T, W_T > a\\}$ is the same as $\\{W_T > a\\}$. Thus,\n$$\n\\mathbb{P}(\\tau_a \\le T, W_T  a) = \\mathbb{P}(W_T > a)\n$$\nSubstituting this back, and using that $\\mathbb{P}(W_T=a)=0$ for the continuous random variable $W_T$, we get:\n$$\n\\mathbb{P}(M_T \\ge a) = \\mathbb{P}(W_T \\ge a) + \\mathbb{P}(W_T > a) = 2 \\mathbb{P}(W_T \\ge a)\n$$\nThis result is valid for $a > 0$. For $a=0$, $\\mathbb{P}(M_T \\ge 0)=1$ and $2\\mathbb{P}(W_T \\ge 0) = 2(1/2) = 1$, so the formula holds for all $a \\ge 0$.\n\nA standard Brownian motion $W_t$ is a Gaussian process with $W_T \\sim \\mathcal{N}(0, T)$. Let $Z \\sim \\mathcal{N}(0, 1)$ be a standard normal random variable. Then $W_T$ has the same distribution as $\\sqrt{T}Z$.\n$$\n\\mathbb{P}(W_T \\ge a) = \\mathbb{P}(\\sqrt{T}Z \\ge a) = \\mathbb{P}(Z \\ge a/\\sqrt{T}) = 1 - \\Phi(a/\\sqrt{T})\n$$\nwhere $\\Phi(\\cdot)$ is the CDF of the standard normal distribution.\nThe tail probability of $M_T$ is therefore:\n$$\n\\mathbb{P}(M_T \\ge a) = 2(1 - \\Phi(a/\\sqrt{T})) \\quad \\text{for } a \\ge 0\n$$\nThe CDF of $M_T$ is $F_{M_T}(a) = \\mathbb{P}(M_T \\le a) = 1 - \\mathbb{P}(M_T > a) = 1 - \\mathbb{P}(M_T \\ge a)$.\n$$\nF_{M_T}(a) = 1 - 2(1 - \\Phi(a/\\sqrt{T})) = 2\\Phi(a/\\sqrt{T}) - 1, \\quad \\text{for } a \\ge 0\n$$\nTo compute the expectation $\\mathbb{E}[M_T]$, we first find the probability density function (PDF) $f_{M_T}(a)$ by differentiating the CDF for $a > 0$. Let $\\phi(\\cdot)$ be the PDF of the standard normal distribution.\n$$\nf_{M_T}(a) = \\frac{d}{da} F_{M_T}(a) = \\frac{d}{da} (2\\Phi(a/\\sqrt{T}) - 1) = 2\\phi(a/\\sqrt{T}) \\cdot \\frac{1}{\\sqrt{T}}\n$$\nSubstituting the expression for $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$:\n$$\nf_{M_T}(a) = \\frac{2}{\\sqrt{T}} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(a/\\sqrt{T})^2}{2}\\right) = \\frac{2}{\\sqrt{2\\pi T}} \\exp\\left(-\\frac{a^2}{2T}\\right), \\quad \\text{for } a \\ge 0\n$$\nand $f_{M_T}(a)=0$ for $a  0$. This is the PDF of a folded normal distribution.\nThe expectation is calculated by integrating $a \\cdot f_{M_T}(a)$ over its support:\n$$\n\\mathbb{E}[M_T] = \\int_0^\\infty a f_{M_T}(a) da = \\int_0^\\infty a \\frac{2}{\\sqrt{2\\pi T}} \\exp\\left(-\\frac{a^2}{2T}\\right) da\n$$\nWe perform a substitution. Let $u = a^2/(2T)$. Then $du = (2a)/(2T) da = a/T da$, which implies $a da = T du$. The limits of integration remain $0$ and $\\infty$.\n$$\n\\mathbb{E}[M_T] = \\frac{2}{\\sqrt{2\\pi T}} \\int_0^\\infty \\exp(-u) (T du) = \\frac{2T}{\\sqrt{2\\pi T}} \\int_0^\\infty \\exp(-u) du\n$$\nThe integral $\\int_0^\\infty \\exp(-u) du$ is the Gamma function $\\Gamma(1)$, which equals $1$.\n$$\n\\mathbb{E}[M_T] = \\frac{2T}{\\sqrt{2\\pi T}} = \\frac{2\\sqrt{T}}{\\sqrt{2\\pi}} = \\sqrt{\\frac{4T}{2\\pi}} = \\sqrt{\\frac{2T}{\\pi}}\n$$\n\nb) Derivation of a tail bound for $M_T$.\n\nWe are asked to derive an upper bound for $\\mathbb{P}(M_{T} \\ge a)$ of the form $C \\exp(-a^{2}/(2T))$ for some constant $C$. The derivation must use the reflection principle, the exponential Markov inequality, and the MGF of a Gaussian random variable.\n\nFrom part (a), the reflection principle gives the exact identity for $a \\ge 0$:\n$$\n\\mathbb{P}(M_T \\ge a) = 2\\mathbb{P}(W_T \\ge a)\n$$\nNow we derive an upper bound for the Gaussian tail probability $\\mathbb{P}(W_T \\ge a)$ using the specified tools.\nThe exponential Markov inequality (also known as a Chernoff bound) states that for a random variable $X$, any $a \\in \\mathbb{R}$, and any $\\lambda > 0$:\n$$\n\\mathbb{P}(X \\ge a) = \\mathbb{P}(\\exp(\\lambda X) \\ge \\exp(\\lambda a)) \\le \\frac{\\mathbb{E}[\\exp(\\lambda X)]}{\\exp(\\lambda a)} = \\exp(-\\lambda a) \\mathbb{E}[\\exp(\\lambda X)]\n$$\nWe apply this to $X = W_T$. The random variable $W_T$ follows a normal distribution $\\mathcal{N}(0, T)$. The moment generating function (MGF) of a general Gaussian variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_Y(\\lambda) = \\mathbb{E}[\\exp(\\lambda Y)] = \\exp(\\mu\\lambda + \\frac{1}{2}\\sigma^2\\lambda^2)$. For $W_T$, we have $\\mu=0$ and $\\sigma^2=T$, so its MGF is:\n$$\n\\mathbb{E}[\\exp(\\lambda W_T)] = \\exp\\left(\\frac{1}{2}T\\lambda^2\\right)\n$$\nSubstituting the MGF into the Chernoff bound for $W_T$:\n$$\n\\mathbb{P}(W_T \\ge a) \\le \\exp(-\\lambda a) \\exp\\left(\\frac{1}{2}T\\lambda^2\\right) = \\exp\\left(-\\lambda a + \\frac{1}{2}T\\lambda^2\\right)\n$$\nThis inequality holds for any $\\lambda > 0$. To obtain the tightest possible bound from this method, we minimize the expression in the exponent with respect to $\\lambda$. Let $g(\\lambda) = -\\lambda a + \\frac{1}{2}T\\lambda^2$. The minimum is found by setting the derivative to zero:\n$$\ng'(\\lambda) = -a + T\\lambda = 0 \\implies \\lambda = \\frac{a}{T}\n$$\nSince the problem is for $a \\ge 0$, we have $\\lambda \\ge 0$. We need $\\lambda > 0$, so this choice is valid for $a > 0$.\nSubstituting this optimal value of $\\lambda$ back into the bound:\n$$\n\\mathbb{P}(W_T \\ge a) \\le \\exp\\left(-\\left(\\frac{a}{T}\\right)a + \\frac{1}{2}T\\left(\\frac{a}{T}\\right)^2\\right) = \\exp\\left(-\\frac{a^2}{T} + \\frac{a^2}{2T}\\right) = \\exp\\left(-\\frac{a^2}{2T}\\right)\n$$\nThis provides the desired exponential bound for the tail of the Gaussian variable $W_T$.\n\nFinally, we combine this result with the identity from the reflection principle:\n$$\n\\mathbb{P}(M_T \\ge a) = 2 \\mathbb{P}(W_T \\ge a) \\le 2 \\exp\\left(-\\frac{a^2}{2T}\\right)\n$$\nThis bound is of the form $\\mathbb{P}(M_{T} \\ge a) \\le C \\exp(-a^{2}/(2T))$. The method described (reflection principle followed by Chernoff bound on the resulting Gaussian tail) directly yields the constant $C=2$. This is the smallest constant that this specific derivation produces. The resulting upper bound is valid for all $a \\ge 0$.\nThe final explicit bound is $\\mathbb{P}(M_T \\ge a) \\le 2 \\exp(-a^2/(2T))$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{\\frac{2T}{\\pi}}  \\mathbb{P}(M_{T} \\ge a) \\le 2 \\exp\\left(-\\frac{a^{2}}{2T}\\right)\n\\end{pmatrix}\n}\n$$", "id": "2978015"}, {"introduction": "Having explored each process individually, this final practice offers a direct comparison of their intrinsic characters. You will investigate the proportion of time that Brownian motion and a Poisson process spend above their starting point, a measure known as the occupation time. This analysis reveals a striking and fundamental difference in their path structures, contrasting the persistent, non-decreasing nature of the Poisson process with the recurrent behavior of Brownian motion, which leads to the famous and deeply counter-intuitive arcsine law. [@problem_id:2978056]", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion (BM) starting at $B_{0} = 0$, and let $\\{N_{t}\\}_{t \\geq 0}$ be a Poisson process (PP) with rate $\\lambda  0$ starting at $N_{0} = 0$. Fix a deterministic horizon $T  0$. Define the occupation proportion above zero for BM by\n$$\nA_{T} := \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{B_{t}  0\\}} \\,\\mathrm{d}t,\n$$\nand the occupation proportion strictly above zero for PP by\n$$\nO_{T} := \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{N_{t}  0\\}} \\,\\mathrm{d}t.\n$$\nStarting from the fundamental properties of BM (independent, stationary increments, continuity and symmetry) and PP (independent, stationary increments, exponentially distributed interarrival times), and using only well-tested facts such as the reflection principle for BM, the strong Markov property, Brownian scaling, and the Feynman–Kac formula for additive functionals, derive the distribution of $A_{T}$ and of $O_{T}$.\n\nSpecifically:\n\n- Compute the probability density function of $A_{T}$ on the interval $(0,1)$, expressed as a closed-form analytic expression.\n- Compute the cumulative distribution function of $O_{T}$ on $\\mathbb{R}$, expressed as a single analytic expression that is valid for all $x \\in \\mathbb{R}$, using indicator functions if needed to capture atoms and support constraints.\n\nProvide your final answer as two analytic expressions in a single row matrix, where the first entry is the density of $A_{T}$ and the second entry is the cumulative distribution function of $O_{T}$. No numerical approximation is required.", "solution": "The problem is valid. It consists of two well-defined subproblems from the theory of stochastic processes, both of which are scientifically grounded and have unique, meaningful solutions. The premises are factually correct and all terms are well-defined.\n\nWe will address the two parts of the problem sequentially.\n\nPart 1: The distribution of $A_{T}$\n\nThe random variable $A_{T}$ is defined as the proportion of time a standard one-dimensional Brownian motion $\\{B_{t}\\}_{t \\geq 0}$ spends above zero up to a fixed time $T > 0$:\n$$\nA_{T} := \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{B_{t} > 0\\}} \\,\\mathrm{d}t\n$$\nOur goal is to find the probability density function (PDF) of $A_{T}$.\n\nFirst, we utilize the Brownian scaling property. For any constant $c > 0$, the process $\\{ W_{t} \\}_{t \\ge 0}$ defined by $W_{t} = c^{-1/2} B_{ct}$ is also a standard Brownian motion. Let us choose $c=T$. The process $\\{ W_{u} \\}_{u \\ge 0}$ defined by $W_{u} = T^{-1/2} B_{Tu}$ is a standard Brownian motion. We can rewrite the integral defining $A_{T}$ by a change of variables $t = Tu$:\n$$\nA_{T} = \\frac{1}{T} \\int_{0}^{1} \\mathbf{1}_{\\{B_{Tu} > 0\\}} \\,T\\,\\mathrm{d}u = \\int_{0}^{1} \\mathbf{1}_{\\{B_{Tu} > 0\\}} \\,\\mathrm{d}u\n$$\nSince $T > 0$, the sign of $B_{Tu}$ is the same as the sign of $T^{-1/2} B_{Tu} = W_{u}$. Therefore,\n$$\nA_{T} = \\int_{0}^{1} \\mathbf{1}_{\\{W_{u} > 0\\}} \\,\\mathrm{d}u\n$$\nThe right-hand side is the definition of $A_{1}$ for the standard Brownian motion $W_{u}$. This shows that the distribution of $A_{T}$ is independent of $T$. We can therefore focus on finding the distribution of $A_{1} = \\int_{0}^{1} \\mathbf{1}_{\\{B_{t} > 0\\}} \\,\\mathrm{d}t$.\n\nThis is a classic result known as Lévy's first arcsine law. A key \"well-tested fact\" relates the distribution of the occupation time $A_{t}$ to the distribution of the last time the process hits zero before time $t$. Let $g_{t} = \\sup\\{s \\in [0,t] : B_s = 0\\}$ be the last zero of the Brownian motion on $[0,t]$. Lévy's arcsine law states that $A_{t}$ and $g_{t}/t$ have the same distribution. We will derive the distribution of $g_{1}$.\n\nThe cumulative distribution function (CDF) of $g_1$ for $s \\in [0,1]$ is $F_{g_1}(s) = P(g_1 \\le s)$. The event $\\{g_1 \\le s\\}$ is equivalent to the event that the Brownian motion does not return to zero in the time interval $(s, 1]$.\n$$\nP(g_1 \\le s) = P(B_u \\ne 0 \\text{ for all } u \\in (s, 1])\n$$\nWe condition on the value of $B_s$ and use the strong Markov property.\n$$\nP(g_1 \\le s) = \\int_{-\\infty}^{\\infty} P(B_s \\in dy) \\, P_y(\\tau_0 > 1-s)\n$$\nwhere $P_y(\\tau_0 > t')$ is the probability that a Brownian motion starting from $y$ does not hit zero before time $t'$. The density of $B_s$ is $p_s(y) = (2\\pi s)^{-1/2} \\exp(-y^2/(2s))$.\n\nUsing the reflection principle, the probability $P_y(\\tau_0 \\le t')$ for $y>0$ is equal to the probability that a standard Brownian motion starting at $0$ reaches level $y$ before time $t'$, which is $P_0(M_{t'} \\ge y)$, where $M_{t'} = \\sup_{u \\in [0, t']} B_u$. This probability is $2P_0(B_{t'} \\ge y)$. Thus,\n$$\nP_y(\\tau_0 \\le t') = 2\\int_{y}^{\\infty} \\frac{1}{\\sqrt{2\\pi t'}} \\exp(-z^2/(2t')) \\, dz\n$$\nThis implies that $P_y(\\tau_0 > t') = 1 - 2P_0(B_{t'} \\ge y) = P_0(|B_{t'}|  y)$. For any $y \\ne 0$, we have $P_y(\\tau_0 > t') = P_0(|B_{t'}||y|)$.\nLetting $t' = 1-s$, we have\n$$\nP_y(\\tau_0 > 1-s) = \\int_{-|y|}^{|y|} \\frac{1}{\\sqrt{2\\pi(1-s)}} \\exp\\left(-\\frac{z^2}{2(1-s)}\\right) \\, dz\n$$\nSubstituting this into the integral for $P(g_1 \\le s)$:\n$$\nP(g_1 \\le s) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi s}} \\exp\\left(-\\frac{y^2}{2s}\\right) \\left[ \\int_{-|y|}^{|y|} \\frac{1}{\\sqrt{2\\pi(1-s)}} \\exp\\left(-\\frac{z^2}{2(1-s)}\\right) \\, dz \\right] \\, dy\n$$\nLet $\\mathcal{N}_1$ and $\\mathcal{N}_2$ be two independent standard normal random variables. Let $y = \\sqrt{s}\\mathcal{N}_1$ and $z=\\sqrt{1-s}\\mathcal{N}_2$. The expression above is equivalent to the probability:\n$$\nP(|\\sqrt{1-s}\\mathcal{N}_2|  |\\sqrt{s}\\mathcal{N}_1|) = P\\left(|\\mathcal{N}_2|  |\\mathcal{N}_1| \\sqrt{\\frac{s}{1-s}}\\right)\n$$\nThis probability can be evaluated using a geometric argument. Consider the point $(\\mathcal{N}_1, \\mathcal{N}_2)$ in the plane. Its distribution is isotropic, meaning the angle of the vector $(\\mathcal{N}_1, \\mathcal{N}_2)$ is uniformly distributed on $[0, 2\\pi)$. The condition $|\\mathcal{N}_2|  |\\mathcal{N}_1| \\tan(\\theta)$, where $\\tan(\\theta) = \\sqrt{s/(1-s)}$ for $\\theta \\in [0, \\pi/2)$, defines a double cone region symmetric about the horizontal axis. The total angle of this region is $4\\theta$. The probability is the ratio of this angle to the total angle $2\\pi$.\n$$\nP(g_1 \\le s) = \\frac{4\\theta}{2\\pi} = \\frac{2\\theta}{\\pi}\n$$\nFrom $\\tan(\\theta) = \\sqrt{s/(1-s)}$, we find $\\tan^2(\\theta) = s/(1-s)$, which implies $\\sin^2(\\theta) = s$, so $\\theta = \\arcsin(\\sqrt{s})$.\nTherefore, the CDF of $g_1$ (and thus of $A_1$) is:\n$$\nF_{A_1}(x) = P(A_1 \\le x) = \\frac{2}{\\pi} \\arcsin(\\sqrt{x}) \\quad \\text{for } x \\in [0,1]\n$$\nTo find the PDF $f_{A_T}(x) = f_{A_1}(x)$ for $x \\in (0,1)$, we differentiate the CDF with respect to $x$:\n$$\nf_{A_1}(x) = \\frac{d}{dx} \\left( \\frac{2}{\\pi} \\arcsin(\\sqrt{x}) \\right) = \\frac{2}{\\pi} \\cdot \\frac{1}{\\sqrt{1-(\\sqrt{x})^2}} \\cdot \\frac{d}{dx}(\\sqrt{x}) = \\frac{2}{\\pi} \\frac{1}{\\sqrt{1-x}} \\frac{1}{2\\sqrt{x}}\n$$\nThis simplifies to the arcsine density:\n$$\nf_{A_T}(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}}\n$$\n\nPart 2: The distribution of $O_T$\n\nThe random variable $O_T$ is defined as the proportion of time a Poisson process $\\{N_t\\}_{t \\geq 0}$ with rate $\\lambda > 0$ spends strictly above zero:\n$$\nO_{T} := \\frac{1}{T} \\int_{0}^{T} \\mathbf{1}_{\\{N_{t} > 0\\}} \\,\\mathrm{d}t\n$$\nThe Poisson process starts at $N_0 = 0$. It remains at $0$ until the first event occurs. Let $\\tau_1$ be the time of the first event. $\\tau_1$ is an exponential random variable with rate $\\lambda$, so its CDF is $P(\\tau_1 \\le t) = 1 - \\exp(-\\lambda t)$ for $t \\ge 0$.\nThe indicator function $\\mathbf{1}_{\\{N_t > 0\\}}$ is $0$ for $t  \\tau_1$ and $1$ for $t \\ge \\tau_1$. We can thus rewrite the integral based on the value of $\\tau_1$:\n$$\n\\int_{0}^{T} \\mathbf{1}_{\\{N_{t} > 0\\}} \\,\\mathrm{d}t = \\int_{0}^{T} \\mathbf{1}_{\\{t \\ge \\tau_1\\}} \\,\\mathrm{d}t\n$$\nWe consider two cases for the value of $\\tau_1$:\n1.  If $\\tau_1 > T$, the first event occurs after time $T$. In this case, $N_t=0$ for all $t \\in [0,T]$. The integral is $0$, so $O_T = 0$. The probability of this event is $P(\\tau_1 > T) = \\exp(-\\lambda T)$.\n2.  If $\\tau_1 \\le T$, the first event occurs at or before time $T$. The integral is $\\int_{\\tau_1}^{T} 1 \\, \\mathrm{d}t = T - \\tau_1$. In this case, $O_T = \\frac{T - \\tau_1}{T} = 1 - \\frac{\\tau_1}{T}$.\n\nNow we derive the CDF, $F_{O_T}(x) = P(O_T \\le x)$, for all $x \\in \\mathbb{R}$.\n- Since $O_T \\ge 0$, for any $x  0$, $F_{O_T}(x) = 0$.\n- The distribution has a point mass (atom) at $x=0$. $P(O_T=0) = P(\\tau_1 > T) = \\exp(-\\lambda T)$. So, $F_{O_T}(0) = P(O_T \\le 0) = \\exp(-\\lambda T)$.\n- For $x \\in (0, 1)$, the event $\\{O_T \\le x\\}$ can occur if $O_T=0$ or if $0  O_T \\le x$.\n$$\nP(O_T \\le x) = P(O_T=0) + P(0  O_T \\le x)\n$$\nThe event $\\{0  O_T \\le x\\}$ is equivalent to $\\{ \\tau_1 \\le T \\text{ and } 1 - \\frac{\\tau_1}{T} \\le x \\}$. The second inequality implies $1-x \\le \\frac{\\tau_1}{T}$, or $\\tau_1 \\ge T(1-x)$.\nSo, $P(0  O_T \\le x) = P(T(1-x) \\le \\tau_1 \\le T)$.\nUsing the CDF of $\\tau_1$:\n$$\nP(T(1-x) \\le \\tau_1 \\le T) = P(\\tau_1 \\le T) - P(\\tau_1  T(1-x)) = (1 - \\exp(-\\lambda T)) - (1 - \\exp(-\\lambda T(1-x)))\n$$\n$$\nP(0  O_T \\le x) = \\exp(-\\lambda T(1-x)) - \\exp(-\\lambda T)\n$$\nTherefore, for $x \\in (0,1)$:\n$$\nF_{O_T}(x) = \\exp(-\\lambda T) + (\\exp(-\\lambda T(1-x)) - \\exp(-\\lambda T)) = \\exp(-\\lambda T(1-x))\n$$\n- Note that this expression is also valid for $x=0$, as $\\exp(-\\lambda T(1-0)) = \\exp(-\\lambda T)$. So, for $x \\in [0,1)$, $F_{O_T}(x) = \\exp(-\\lambda T(1-x))$.\n- For $x \\ge 1$: since $\\tau_1 \\ge 0$, we have $O_T = 1 - \\tau_1/T \\le 1$. In fact, since $\\tau_1$ is a continuous random variable, $P(\\tau_1=0)=0$, so $O_T  1$ almost surely. Thus, for any $x \\ge 1$, $P(O_T \\le x) = 1$.\n\nWe can combine these parts into a single expression valid for all $x \\in \\mathbb{R}$ using indicator functions:\n$$\nF_{O_T}(x) = \\exp(-\\lambda T(1-x)) \\mathbf{1}_{[0, 1)}(x) + \\mathbf{1}_{[1, \\infty)}(x)\n$$\nwhere $\\mathbf{1}_S(x)$ is the indicator function of the set $S$.\n\nFinal Answer Compilation:\nThe PDF of $A_T$ for $x \\in (0,1)$ is $\\frac{1}{\\pi \\sqrt{x(1-x)}}$.\nThe CDF of $O_T$ for $x \\in \\mathbb{R}$ is $\\exp(-\\lambda T(1-x)) \\mathbf{1}_{[0, 1)}(x) + \\mathbf{1}_{[1, \\infty)}(x)$.\nWe present these in a single row matrix as requested.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\pi \\sqrt{x(1-x)}}  \\exp(-\\lambda T(1-x)) \\mathbf{1}_{[0, 1)}(x) + \\mathbf{1}_{[1, \\infty)}(x) \\end{pmatrix}}\n$$", "id": "2978056"}]}