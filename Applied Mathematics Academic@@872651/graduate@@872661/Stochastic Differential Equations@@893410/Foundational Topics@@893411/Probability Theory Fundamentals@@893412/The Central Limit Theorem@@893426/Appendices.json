{"hands_on_practices": [{"introduction": "The classical Central Limit Theorem is a cornerstone of statistics, but its assumption of independent and identically distributed variables is often too restrictive for real-world applications like financial time series or climate data. This first practice extends the CLT to the realm of dependent data by examining a simple but fundamental time series model: the Moving-Average of order one (MA(1)). Your task is to compute the long-run variance, a crucial modification that accounts for the correlation between observations, providing a gateway to understanding asymptotics in dependent processes [@problem_id:852396].", "problem": "Consider a stationary Moving-Average process of order one, MA(1), defined by the equation:\n$$\nX_t = \\mu + \\varepsilon_t + \\theta \\varepsilon_{t-1}\n$$\nwhere $\\mu$ is the constant mean of the process, $\\theta$ is the moving-average parameter satisfying the stationarity condition $|\\theta|  \\infty$, and $\\{\\varepsilon_t\\}$ is a white noise process of innovations. The innovations are independent and identically distributed (i.i.d.) with mean $E[\\varepsilon_t] = 0$ and variance $\\text{Var}(\\varepsilon_t) = \\sigma^2_\\varepsilon$.\n\nThe sample mean of this process is given by $\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t$. For a stationary process under certain mixing conditions (which the MA(1) process satisfies), a Central Limit Theorem holds for the sample mean. It states that the distribution of the appropriately scaled sample mean converges to a normal distribution:\n$$\n\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2_{LR})\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution and $\\sigma^2_{LR}$ is the long-run variance of the process. The long-run variance is defined as the sum of all autocovariances:\n$$\n\\sigma^2_{LR} = \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k\n$$\nwhere $\\gamma_k = \\text{Cov}(X_t, X_{t-k})$ is the autocovariance at lag $k$.\n\nDerive the analytical expression for the long-run variance, $\\sigma^2_{LR}$, for this MA(1) process in terms of the parameters $\\theta$ and $\\sigma^2_\\varepsilon$.", "solution": "The goal is to compute the long-run variance $\\sigma^2_{LR}$ for the MA(1) process $X_t = \\mu + \\varepsilon_t + \\theta \\varepsilon_{t-1}$. The long-run variance is defined as $\\sigma^2_{LR} = \\sum_{k=-\\infty}^{\\infty} \\gamma_k$, where $\\gamma_k = \\text{Cov}(X_t, X_{t-k})$ is the autocovariance function. We can write this as $\\sigma^2_{LR} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k$ due to the symmetry property $\\gamma_k = \\gamma_{-k}$.\n\nFirst, let's compute the autocovariance function $\\gamma_k$. Since the mean $\\mu$ is a constant, it does not affect the covariance. We can work with the mean-zero process $Y_t = X_t - \\mu = \\varepsilon_t + \\theta \\varepsilon_{t-1}$. The autocovariance is then $\\gamma_k = \\text{Cov}(Y_t, Y_{t-k}) = E[Y_t Y_{t-k}]$.\n\n**Step 1: Compute the variance, $\\gamma_0$.**\nThe variance corresponds to the autocovariance at lag $k=0$.\n$$\n\\gamma_0 = \\text{Var}(X_t) = \\text{Var}(\\varepsilon_t + \\theta \\varepsilon_{t-1})\n$$\nSince $\\varepsilon_t$ and $\\varepsilon_{t-1}$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\gamma_0 = \\text{Var}(\\varepsilon_t) + \\text{Var}(\\theta \\varepsilon_{t-1}) = \\text{Var}(\\varepsilon_t) + \\theta^2 \\text{Var}(\\varepsilon_{t-1})\n$$\nGiven that $\\text{Var}(\\varepsilon_t) = \\sigma^2_\\varepsilon$ for all $t$:\n$$\n\\gamma_0 = \\sigma^2_\\varepsilon + \\theta^2 \\sigma^2_\\varepsilon = (1+\\theta^2)\\sigma^2_\\varepsilon\n$$\n\n**Step 2: Compute the autocovariance at lag 1, $\\gamma_1$.**\nFor $k=1$, we have:\n$$\n\\gamma_1 = \\text{Cov}(X_t, X_{t-1}) = E[(X_t-\\mu)(X_{t-1}-\\mu)] = E[(\\varepsilon_t + \\theta \\varepsilon_{t-1})(\\varepsilon_{t-1} + \\theta \\varepsilon_{t-2})]\n$$\nExpanding the product gives:\n$$\n\\gamma_1 = E[\\varepsilon_t \\varepsilon_{t-1} + \\theta \\varepsilon_t \\varepsilon_{t-2} + \\theta \\varepsilon_{t-1}^2 + \\theta^2 \\varepsilon_{t-1} \\varepsilon_{t-2}]\n$$\nUsing the linearity of expectation and the properties of the white noise process ($E[\\varepsilon_i]=0$ and $E[\\varepsilon_i \\varepsilon_j] = 0$ for $i \\neq j$, $E[\\varepsilon_i^2]=\\sigma_\\varepsilon^2$):\n$$\nE[\\varepsilon_t \\varepsilon_{t-1}] = 0\n$$\n$$\nE[\\varepsilon_t \\varepsilon_{t-2}] = 0\n$$\n$$\nE[\\varepsilon_{t-1}^2] = \\sigma^2_\\varepsilon\n$$\n$$\nE[\\varepsilon_{t-1} \\varepsilon_{t-2}] = 0\n$$\nSubstituting these into the expression for $\\gamma_1$:\n$$\n\\gamma_1 = 0 + \\theta(0) + \\theta(\\sigma^2_\\varepsilon) + \\theta^2(0) = \\theta \\sigma^2_\\varepsilon\n$$\n\n**Step 3: Compute the autocovariance for lags $k \\ge 2$, $\\gamma_k$.**\nFor any lag $k \\ge 2$:\n$$\n\\gamma_k = \\text{Cov}(X_t, X_{t-k}) = E[(\\varepsilon_t + \\theta \\varepsilon_{t-1})(\\varepsilon_{t-k} + \\theta \\varepsilon_{t-k-1})]\n$$\nThe innovation terms in the first factor are indexed by $t$ and $t-1$. The innovation terms in the second factor are indexed by $t-k$ and $t-k-1$. Since $k \\ge 2$, we have $t-1  t-k  t-k-1$. Thus, the set of time indices $\\{t, t-1\\}$ is disjoint from the set $\\{t-k, t-k-1\\}$. Because the innovations $\\varepsilon_t$ are independent across time, the expectation of any product of innovations with different time indices is zero.\n$$\n\\gamma_k = E[\\varepsilon_t \\varepsilon_{t-k} + \\theta \\varepsilon_t \\varepsilon_{t-k-1} + \\theta \\varepsilon_{t-1} \\varepsilon_{t-k} + \\theta^2 \\varepsilon_{t-1} \\varepsilon_{t-k-1}] = 0\n$$\nSo, $\\gamma_k = 0$ for all $k \\ge 2$.\n\n**Step 4: Calculate the long-run variance $\\sigma^2_{LR}$.**\nNow we can substitute the computed autocovariances into the formula for the long-run variance.\n$$\n\\sigma^2_{LR} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k = \\gamma_0 + 2(\\gamma_1 + \\gamma_2 + \\gamma_3 + \\dots)\n$$\nSubstituting the values we found:\n$$\n\\sigma^2_{LR} = (1+\\theta^2)\\sigma^2_\\varepsilon + 2(\\theta\\sigma^2_\\varepsilon + 0 + 0 + \\dots)\n$$\n$$\n\\sigma^2_{LR} = (1+\\theta^2)\\sigma^2_\\varepsilon + 2\\theta\\sigma^2_\\varepsilon\n$$\nFactoring out $\\sigma^2_\\varepsilon$:\n$$\n\\sigma^2_{LR} = (1 + \\theta^2 + 2\\theta)\\sigma^2_\\varepsilon\n$$\nRecognizing the perfect square trinomial $a^2+2ab+b^2 = (a+b)^2$:\n$$\n\\sigma^2_{LR} = (1+\\theta)^2 \\sigma^2_\\varepsilon\n$$\nThis is the final expression for the long-run variance.", "answer": "$$\n\\boxed{(1+\\theta)^2 \\sigma^2_\\varepsilon}\n$$", "id": "852396"}, {"introduction": "Building on the concept of long-run variance, we now turn to a process with a more persistent memory: the first-order Autoregressive (AR(1)) model. This exercise introduces another layer of realism by asking you to analyze the sample mean of a *squared* AR(1) process, a transformation relevant to the study of volatility. This requires not only summing an infinite series of autocovariances but also calculating higher-order moments of the process, deepening your skills in analyzing non-linear functions of dependent variables [@problem_id:852633].", "problem": "Consider a stationary, zero-mean, first-order autoregressive process, AR(1), defined by the stochastic difference equation:\n$$\nX_t = \\phi X_{t-1} + \\epsilon_t\n$$\nwhere $t$ is an integer index representing time. The innovations, $\\{ \\epsilon_t \\}$, are independent and identically distributed (i.i.d.) Gaussian random variables with mean zero and variance $\\sigma_\\epsilon^2$. The stationarity of the process is ensured by the condition $|\\phi|  1$.\n\nLet $Y_t = X_t^2$ be a new stochastic process derived from $X_t$. The sample mean of this new process is given by $\\bar{Y}_n = \\frac{1}{n} \\sum_{t=1}^n Y_t$. For large $n$, a Central Limit Theorem for stationary processes applies, stating that:\n$$\n\\sqrt{n} \\left( \\bar{Y}_n - E[Y_t] \\right) \\xrightarrow{d} N(0, S_Y)\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution, and $S_Y$ is the long-run variance of the process $\\{Y_t\\}$. The long-run variance is defined as the sum of all autocovariances:\n$$\nS_Y = \\sum_{k=-\\infty}^{\\infty} \\text{Cov}(Y_t, Y_{t+k})\n$$\nYour task is to compute the long-run variance $S_Y$ for the process $\\{Y_t\\}$. The final expression should be in terms of the AR(1) parameter $\\phi$ and the innovation variance $\\sigma_\\epsilon^2$.", "solution": "1. Define\n$$A=\\mathrm{Var}(X_t)=\\frac{\\sigma_\\epsilon^2}{1-\\phi^2},\\qquad \\gamma_k=\\mathrm{Cov}(X_t,X_{t+k})=\\phi^{|k|}A.$$\n\n2. For zero‐mean Gaussian variables,\n$$E[X_t^2X_{t+k}^2]=A^2+2\\gamma_k^2 \\quad\\Longrightarrow\\quad \\mathrm{Cov}(Y_t,Y_{t+k}) =E[X_t^2X_{t+k}^2]-A^2 =2\\gamma_k^2.$$\n\n3. The long‐run variance\n$$S_Y=\\sum_{k=-\\infty}^\\infty\\mathrm{Cov}(Y_t,Y_{t+k}) =\\mathrm{Var}(Y_t)+2\\sum_{k=1}^\\infty2\\gamma_k^2.$$\n\n4. Compute $\\mathrm{Var}(Y_t)=\\mathrm{Var}(X_t^2)=E[X_t^4]-A^2=3A^2-A^2=2A^2.$\n\n5. Sum the series:\n$$\\sum_{k=1}^\\infty\\gamma_k^2 =\\sum_{k=1}^\\infty(\\phi^{k}A)^2 =A^2\\frac{\\phi^2}{1-\\phi^2}.$$\n\n6. Substitute into $S_Y$:\n$$S_Y =2A^2+4\\cdot A^2\\frac{\\phi^2}{1-\\phi^2} =2A^2\\Bigl(1+\\frac{2\\phi^2}{1-\\phi^2}\\Bigr) =2A^2\\frac{1+\\phi^2}{1-\\phi^2}.$$\n\n7. Replace $A=\\sigma_\\epsilon^2/(1-\\phi^2)$:\n$$S_Y =2\\frac{\\sigma_\\epsilon^4}{(1-\\phi^2)^2} \\frac{1+\\phi^2}{1-\\phi^2} =\\frac{2(1+\\phi^2)\\,\\sigma_\\epsilon^4}{(1-\\phi^2)^3}.$$", "answer": "$$\\boxed{\\frac{2(1+\\phi^2)\\,\\sigma_\\epsilon^4}{(1-\\phi^2)^3}}$$", "id": "852633"}, {"introduction": "Our final practice takes the concept of the CLT to its continuous-time limit, a setting central to modern stochastic analysis and mathematical finance. Here, you will analyze an additive functional of the Ornstein-Uhlenbeck process, the continuous-time analogue of the AR(1) model. You will employ a powerful method that bypasses direct summation by solving a Poisson equation for the process's infinitesimal generator, connecting the CLT directly to Itō's calculus and the theory of Markov processes [@problem_id:3000487].", "problem": "Consider the one-dimensional Ornstein–Uhlenbeck diffusion defined by the stochastic differential equation (SDE) $dX_{t}=-\\theta X_{t}\\,dt+\\sigma\\,dW_{t}$, where $\\theta0$ and $\\sigma0$, and $W_{t}$ is a standard Brownian motion. Let the process be started in its invariant distribution. Denote the infinitesimal generator by $L$, acting on twice continuously differentiable functions $g$ by $Lg(x)=\\frac{\\sigma^{2}}{2}g''(x)-\\theta x g'(x)$. The invariant probability measure $\\pi$ of $X_{t}$ is Gaussian with mean $0$ and variance $\\sigma^{2}/(2\\theta)$.\n\nDefine the centered polynomial observable $f(x)=x^{2}-\\sigma^{2}/(2\\theta)$, and the additive functional $S_{T}=\\int_{0}^{T}f(X_{s})\\,ds$. Derive, from first principles, the asymptotic variance in the Central Limit Theorem (CLT) for the scaled additive functional $T^{-1/2}S_{T}$ by proceeding as follows:\n\n1. Solve the Poisson equation $Lg=-f$ for a polynomial solution $g$ and impose the centering condition $\\int g\\,d\\pi=0$.\n2. Use the semimartingale decomposition obtained from Itō’s formula and the generator to represent $S_{T}$ in terms of a local martingale and boundary terms. Identify the martingale term and its quadratic variation.\n3. Using the stationarity of $X_{t}$, compute the linear growth rate of the quadratic variation and thereby deduce the asymptotic variance of $T^{-1/2}S_{T}$.\n\nExpress the final asymptotic variance as a closed-form analytic expression in terms of $\\theta$ and $\\sigma$. No numerical approximation is required, and no units are involved. The final answer must be a single analytic expression.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem within the standard theory of stochastic differential equations. All necessary information and definitions are provided, and there are no internal contradictions or ambiguities. We shall proceed with the solution by following the prescribed three-step methodology.\n\nThe objective is to find the asymptotic variance of the scaled additive functional $T^{-1/2}S_{T}$, where $S_{T}=\\int_{0}^{T}f(X_{s})\\,ds$ and $f(x)=x^{2}-\\sigma^{2}/(2\\theta)$. The process $X_t$ follows the Ornstein-Uhlenbeck SDE $dX_{t}=-\\theta X_{t}\\,dt+\\sigma\\,dW_{t}$ and is assumed to be in its stationary state, described by the invariant measure $\\pi$, which is a Gaussian distribution with mean $0$ and variance $\\sigma^{2}/(2\\theta)$. The asymptotic variance, which we denote by $V_{asym}$, is given by the limit $\\lim_{T \\to \\infty} \\mathbb{E}[(T^{-1/2}S_{T})^2]$.\n\n**Step 1: Solve the Poisson equation $Lg=-f$ with a centering condition.**\n\nThe infinitesimal generator is $Lg(x)=\\frac{\\sigma^{2}}{2}g''(x)-\\theta x g'(x)$ and the observable is $f(x)=x^{2}-\\frac{\\sigma^{2}}{2\\theta}$. We are tasked with solving the Poisson equation $Lg = -f$, which is:\n$$\n\\frac{\\sigma^{2}}{2}g''(x)-\\theta x g'(x) = -\\left(x^{2}-\\frac{\\sigma^{2}}{2\\theta}\\right) = -x^{2} + \\frac{\\sigma^{2}}{2\\theta}\n$$\nGiven that the right-hand side is a polynomial of degree $2$, we seek a polynomial solution for $g(x)$. Let us assume $g(x)$ is a quadratic of the form $g(x) = ax^{2} + bx + c$. Its derivatives are $g'(x) = 2ax + b$ and $g''(x) = 2a$. Substituting these into the Poisson equation yields:\n$$\n\\frac{\\sigma^{2}}{2}(2a) - \\theta x (2ax + b) = -x^{2} + \\frac{\\sigma^{2}}{2\\theta}\n$$\n$$\n\\sigma^{2}a - 2a\\theta x^{2} - b\\theta x = -x^{2} + \\frac{\\sigma^{2}}{2\\theta}\n$$\nFor this equality to hold for all $x$, we must equate the coefficients of the powers of $x$:\n\\begin{itemize}\n    \\item Coefficient of $x^{2}$: $-2a\\theta = -1 \\implies a = \\frac{1}{2\\theta}$.\n    \\item Coefficient of $x^{1}$: $-b\\theta = 0 \\implies b = 0$.\n    \\item Constant term ($x^{0}$): $\\sigma^{2}a = \\frac{\\sigma^{2}}{2\\theta}$. Substituting our value for $a$, we get $\\sigma^{2}\\left(\\frac{1}{2\\theta}\\right) = \\frac{\\sigma^{2}}{2\\theta}$, which is consistent.\n\\end{itemize}\nThus, the general form of the polynomial solution is $g(x) = \\frac{1}{2\\theta}x^{2} + c$. We now impose the centering condition $\\int g(x) \\,d\\pi(x) = 0$, which is equivalent to $\\mathbb{E}_{\\pi}[g(X)] = 0$.\n$$\n\\mathbb{E}_{\\pi}\\left[\\frac{1}{2\\theta}X^{2} + c\\right] = 0\n$$\n$$\n\\frac{1}{2\\theta}\\mathbb{E}_{\\pi}[X^{2}] + c = 0\n$$\nThe measure $\\pi$ has mean $0$ and variance $\\frac{\\sigma^{2}}{2\\theta}$. Therefore, the second moment is $\\mathbb{E}_{\\pi}[X^{2}] = \\text{Var}_{\\pi}(X) + (\\mathbb{E}_{\\pi}[X])^{2} = \\frac{\\sigma^{2}}{2\\theta} + 0^{2} = \\frac{\\sigma^{2}}{2\\theta}$.\nSubstituting this value:\n$$\n\\frac{1}{2\\theta}\\left(\\frac{\\sigma^{2}}{2\\theta}\\right) + c = 0 \\implies \\frac{\\sigma^{2}}{4\\theta^{2}} + c = 0 \\implies c = -\\frac{\\sigma^{2}}{4\\theta^{2}}\n$$\nThe unique, centered polynomial solution to the Poisson equation is:\n$$\ng(x) = \\frac{1}{2\\theta}x^{2} - \\frac{\\sigma^{2}}{4\\theta^{2}}\n$$\n\n**Step 2: Semimartingale decomposition of $S_{T}$.**\n\nWe use Itō's formula for $g(X_t)$. The general formula is $dg(X_t) = Lg(X_t)dt + \\sigma g'(X_t)dW_t$. Since we have solved $Lg = -f$, we can write:\n$$\ndg(X_t) = -f(X_t)dt + \\sigma g'(X_t)dW_t\n$$\nRearranging this equation gives an expression for $f(X_t)dt$:\n$$\nf(X_t)dt = -dg(X_t) + \\sigma g'(X_t)dW_t\n$$\nIntegrating both sides from $s=0$ to $s=T$:\n$$\nS_{T} = \\int_{0}^{T}f(X_{s})ds = -\\int_{0}^{T}dg(X_s) + \\int_{0}^{T}\\sigma g'(X_s)dW_s\n$$\nThe first integral on the right-hand side is a simple integral of a differential, which evaluates to:\n$$\n\\int_{0}^{T}dg(X_s) = g(X_T) - g(X_0)\n$$\nThis gives the decomposition of $S_T$:\n$$\nS_{T} = g(X_0) - g(X_T) + \\int_{0}^{T}\\sigma g'(X_s)dW_s\n$$\nThis expression separates $S_T$ into a boundary term, $g(X_0) - g(X_T)$, and a stochastic integral term, which is a continuous local martingale.\nThe martingale term is $M_{T} = \\int_{0}^{T}\\sigma g'(X_s)dW_s$.\nThe quadratic variation of this Itō integral is given by:\n$$\n\\langle M \\rangle_{T} = \\int_{0}^{T} (\\sigma g'(X_s))^{2}ds = \\sigma^{2}\\int_{0}^{T}(g'(X_s))^{2}ds\n$$\n\n**Step 3: Compute the asymptotic variance.**\n\nTo find the asymptotic variance of $T^{-1/2}S_{T}$, we examine its components as $T \\to \\infty$:\n$$\nT^{-1/2}S_{T} = T^{-1/2}(g(X_0) - g(X_T)) + T^{-1/2}M_{T}\n$$\nSince the process $X_t$ is stationary, the distributions of $X_0$ and $X_T$ are both $\\pi$. The function $g(x)$ is a polynomial, so $g(X_0)$ and $g(X_T)$ have finite moments. The term $g(X_0) - g(X_T)$ is therefore a random variable with finite variance that does not grow with $T$. Consequently, the term $T^{-1/2}(g(X_0) - g(X_T))$ converges to $0$ in probability as $T \\to \\infty$.\nThe asymptotic distribution of $T^{-1/2}S_{T}$ is thus identical to that of $T^{-1/2}M_{T}$. By the Martingale Central Limit Theorem, $T^{-1/2}M_T$ converges in distribution to a normal distribution with mean $0$ and variance $V_{asym}$, provided that the normalized quadratic variation $T^{-1}\\langle M \\rangle_{T}$ converges in probability to a constant $V_{asym}$.\n$$\nV_{asym} = \\lim_{T \\to \\infty} \\frac{1}{T}\\langle M \\rangle_{T} = \\lim_{T \\to \\infty} \\frac{\\sigma^{2}}{T}\\int_{0}^{T}(g'(X_s))^{2}ds\n$$\nThe Ornstein-Uhlenbeck process is ergodic. Since the process is started in its invariant distribution, it is stationary. We can therefore apply the ergodic theorem, which states that the time average converges to the spatial average with respect to the invariant measure:\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T}(g'(X_s))^{2}ds = \\mathbb{E}_{\\pi}[(g'(X))^{2}] = \\int (g'(x))^{2}d\\pi(x)\n$$\nThe asymptotic variance is then:\n$$\nV_{asym} = \\sigma^{2}\\mathbb{E}_{\\pi}[(g'(X))^{2}]\n$$\nFrom Step 1, we have $g(x) = \\frac{1}{2\\theta}x^{2} - \\frac{\\sigma^{2}}{4\\theta^{2}}$, so its derivative is $g'(x) = \\frac{1}{\\theta}x$.\nSubstituting this into the expression for $V_{asym}$:\n$$\nV_{asym} = \\sigma^{2}\\mathbb{E}_{\\pi}\\left[\\left(\\frac{X}{\\theta}\\right)^{2}\\right] = \\frac{\\sigma^{2}}{\\theta^{2}}\\mathbb{E}_{\\pi}[X^{2}]\n$$\nAs established in Step 1, the second moment of the stationary distribution is $\\mathbb{E}_{\\pi}[X^{2}] = \\frac{\\sigma^{2}}{2\\theta}$.\nFinally, we substitute this value to obtain the asymptotic variance:\n$$\nV_{asym} = \\frac{\\sigma^{2}}{\\theta^{2}}\\left(\\frac{\\sigma^{2}}{2\\theta}\\right) = \\frac{\\sigma^{4}}{2\\theta^{3}}\n$$\nThis is the asymptotic variance in the Central Limit Theorem for the scaled additive functional $T^{-1/2}S_{T}$.", "answer": "$$\\boxed{\\frac{\\sigma^{4}}{2\\theta^{3}}}$$", "id": "3000487"}]}