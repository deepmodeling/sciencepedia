{"hands_on_practices": [{"introduction": "We begin by exploring the most direct link between orthogonal polynomials and the statistical measure they are associated with. This exercise focuses on the Krawtchouk ensemble, a discrete model where eigenvalues are confined to integer positions, and uses the fundamental connection between the coefficients of the three-term recurrence relation and the moments of the underlying probability distribution to determine a key parameter of the system [@problem_id:751148]. This practice highlights how the algebraic structure of orthogonal polynomials directly encodes the statistical properties of the spectrum.", "problem": "In the theory of random matrices, the Krawtchouk orthogonal polynomial ensemble describes a system where eigenvalues are located at discrete positions $x \\in \\{0, 1, \\dots, M\\}$. The probability of an eigenvalue at position $x$ is governed by the binomial distribution:\n$$\nw(x; M, p) = \\binom{M}{x} p^x (1-p)^{M-x}\n$$\nHere, $M$ is a positive integer defining the lattice size, and $p \\in [0, 1]$ is a probability parameter.\n\nThe monic orthogonal polynomials, $\\hat{K}_n(x)$, associated with this weight function satisfy a three-term recurrence relation. For $n \\ge 0$, this relation can be written as:\n$$\nx \\hat{K}_n(x) = \\hat{K}_{n+1}(x) + \\alpha_n \\hat{K}_n(x) + \\beta_n \\hat{K}_{n-1}(x)\n$$\nwith initial conditions $\\hat{K}_0(x)=1$ and $\\hat{K}_{-1}(x)=0$, and where $\\beta_0$ is conventionally set to equal the total mass of the distribution, $\\sum_{x=0}^M w(x)$. The coefficients $\\alpha_n$ and $\\beta_n$ are known as the recursion coefficients.\n\nIt is a fundamental property in the theory of orthogonal polynomials that the first recursion coefficient, $\\alpha_0$, is equal to the mean of the probability distribution $w(x)$, and the coefficient $\\beta_1$ is equal to its variance.\n\nFor a specific Krawtchouk ensemble, the first two non-trivial recursion coefficients are measured to be $\\alpha_0$ and $\\beta_1$. Assuming that the parameters $M$ and $p$ are such that $p \\in (0,1)$ and $\\alpha_0 \\neq 0$, determine the probability parameter $p$ as a function of $\\alpha_0$ and $\\beta_1$.", "solution": "1. For the binomial weight $w(x;M,p)$ one has \n$$\\alpha_0 = E[X] = Mp.$$\n2. The variance is \n$$\\beta_1 = \\mathrm{Var}(X) = Mp(1-p).$$\n3. Substituting $Mp=\\alpha_0$ into the variance gives\n$$\n\\beta_1 = \\alpha_0\\,(1-p)\n\\;\\Longrightarrow\\;\n1-p = \\frac{\\beta_1}{\\alpha_0}\n\\;\\Longrightarrow\\;\np = 1 - \\frac{\\beta_1}{\\alpha_0}. \n$$", "answer": "$$\\boxed{1 - \\frac{\\beta_1}{\\alpha_0}}$$", "id": "751148"}, {"introduction": "Next, we bridge the gap from abstract recurrence relations to concrete random matrices. A cornerstone of random matrix theory is that the eigenvalue problem of a dense matrix from the Gaussian Orthogonal Ensemble (GOE) can be simplified to that of a tridiagonal Jacobi matrix, which is the matrix representation of the recurrence relation [@problem_id:751082]. By analyzing the simplest non-trivial $2 \\times 2$ case, you will calculate the expected value of the smallest eigenvalue, providing tangible insight into how the probabilistic nature of the matrix entries governs the statistics of the spectrum.", "problem": "In random matrix theory, the Gaussian Orthogonal Ensemble (GOE) consists of real symmetric matrices $H$ of size $N \\times N$, whose probability distribution is given by\n$$\nP(H) \\propto \\exp\\left(-\\frac{1}{4\\sigma^2}\\text{Tr}(H^2)\\right),\n$$\nwhere $\\sigma$ is a positive real parameter related to the variance of the matrix elements. This distribution implies that the diagonal elements $H_{ii}$ are independent random variables drawn from a normal distribution $N(0, 2\\sigma^2)$, and the off-diagonal elements $H_{ij}$ for $i<j$ are independent random variables drawn from $N(0, \\sigma^2)$.\n\nA fundamental result states that the set of eigenvalues of any such matrix $H$ is statistically identical to the set of eigenvalues of a much simpler tridiagonal symmetric matrix $J$, also known as a Jacobi matrix. The non-zero elements of this $N \\times N$ matrix $J$ are $J_{ii} = a_i$ and $J_{i, i+1} = J_{i+1, i} = b_i$ for $i=1, \\dots, N-1$. The random variables $a_i$ and $b_i$ have the following distributions:\n1.  The diagonal elements $a_i$ for $i=1, \\dots, N$ are independent and identically distributed (i.i.d.) normal random variables, $a_i \\sim N(0, 2\\sigma^2)$.\n2.  The off-diagonal elements $b_i$ for $i=1, \\dots, N-1$ are independent positive random variables such that their squares, $b_i^2$, are distributed as $\\sigma^2 \\chi^2_{N-i}$, where $\\chi^2_k$ denotes the chi-squared distribution with $k$ degrees of freedom.\n\nConsider the simplest non-trivial case where $N=2$. The corresponding random Jacobi matrix is:\n$$\nJ = \\begin{pmatrix}\na_1 & b_1 \\\\\nb_1 & a_2\n\\end{pmatrix}\n$$\nwhere the elements $a_1, a_2$ are i.i.d. draws from $N(0, 2\\sigma^2)$, and $b_1^2$ is drawn from a $\\sigma^2 \\chi^2_1$ distribution.\n\nYour task is to calculate the expectation value of the smallest eigenvalue of this $2 \\times 2$ random matrix $J$. Express your answer in terms of the parameter $\\sigma$ and fundamental mathematical constants.", "solution": "1. Eigenvalues of \n$$J = \\begin{pmatrix}a_1 & b_1\\\\ b_1 & a_2\\end{pmatrix}$$\nare \n$$\\lambda_{\\pm} = \\frac{a_1 + a_2}{2} \\pm \\sqrt{\\Bigl(\\frac{a_1 - a_2}{2}\\Bigr)^2 + b_1^2}.$$\n2. Define \n$$X = \\frac{a_1 + a_2}{2},\\quad Y = \\frac{a_1 - a_2}{2},\\quad b_1 = \\sigma |Z|,$$\nwith $X,Y\\sim N(0,\\sigma^2)$ independent and $Z\\sim N(0,1)$ independent. Then the smaller eigenvalue is\n$$\\lambda_- = X - \\sqrt{Y^2 + \\sigma^2 Z^2}.$$\n3. By symmetry $\\mathbb{E}[X]=0$, so\n$$\\mathbb{E}[\\lambda_-] = -\\mathbb{E}\\bigl[\\sqrt{Y^2 + \\sigma^2 Z^2}\\bigr].$$\n4. Since $Y,\\;\\sigma Z$ are i.i.d.\\ $N(0,\\sigma^2)$, write $Y=\\sigma W_1$, $\\sigma Z=\\sigma W_2$ with $W_i\\sim N(0,1)$. Then\n$$\\mathbb{E}\\bigl[\\sqrt{Y^2 + \\sigma^2 Z^2}\\bigr]\n= \\sigma\\,\\mathbb{E}\\bigl[\\sqrt{W_1^2 + W_2^2}\\bigr]\n= \\sigma \\sqrt{\\frac{\\pi}{2}},$$\nbecause $\\sqrt{W_1^2+W_2^2}$ is Rayleigh with mean $\\sqrt{\\pi/2}$.\n5. Hence\n$$\\mathbb{E}[\\lambda_-] = -\\,\\sigma\\sqrt{\\frac{\\pi}{2}}.$$", "answer": "$$\\boxed{-\\sigma\\sqrt{\\frac{\\pi}{2}}}$$", "id": "751082"}, {"introduction": "Finally, we shift our focus from individual eigenvalues to collective properties of the entire spectrum. This practice delves into the Gaussian Unitary Ensemble (GUE) to calculate the expectation of a symmetric function of the eigenvalues, a quantity that often corresponds to interaction energies in modeled quantum systems [@problem_id:751057]. This exercise demonstrates a powerful method for computing higher-order statistical observables, which are crucial for understanding the collective behavior and correlations within the eigenvalue distribution.", "problem": "In the study of complex quantum systems, the statistical properties of energy levels can be modeled by the eigenvalues of random matrices. The Gaussian Unitary Ensemble (GUE) is a set of $N \\times N$ Hermitian matrices $H$ whose entries are independent complex Gaussian random variables (up to the Hermitian symmetry). The joint probability density function (JPDF) of the eigenvalues $\\{\\lambda_1, \\dots, \\lambda_N\\}$ of a GUE matrix is given by\n$$\nP(\\lambda_1, \\dots, \\lambda_N) = C_N \\prod_{1 \\le i < j \\le N} (\\lambda_i - \\lambda_j)^2 \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{k=1}^N \\lambda_k^2\\right),\n$$\nwhere $C_N$ is a normalization constant and $\\sigma^2$ is a parameter related to the variance of the matrix elements.\n\nThe statistical properties of the eigenvalues can be analyzed using $k$-point correlation functions $\\rho_k(\\lambda_1, \\dots, \\lambda_k)$. The expectation of a symmetric function $f(\\lambda_1, \\dots, \\lambda_N)$ can be computed from these correlation functions. For this problem, we are interested in the elementary symmetric polynomial of degree two, $e_2(\\lambda_1, \\dots, \\lambda_N) = \\sum_{1 \\le i < j \\le N} \\lambda_i \\lambda_j$. Its expectation value is given by\n$$\n\\mathbb{E}[e_2] = \\frac{1}{2} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} xy \\, \\rho_2(x,y) \\, dx \\, dy.\n$$\nA powerful result in random matrix theory states that the correlation functions can be expressed via a kernel built from orthonormal polynomials. For the GUE, the relevant weight function is $w(x) = \\exp(-x^2/(2\\sigma^2))$. Let $\\{\\psi_n(x)\\}_{n=0}^\\infty$ be the sequence of orthonormal polynomials with respect to this weight. The kernel is $K_N(x,y) = \\sum_{n=0}^{N-1} \\psi_n(x) \\psi_n(y)$. The two-point correlation function is given by the formula:\n$$\n\\rho_2(x,y) = K_N(x,x) K_N(y,y) - K_N(x,y)^2.\n$$\nThe orthonormal polynomials $\\psi_n(x)$ satisfy the three-term recurrence relation:\n$$\nx\\psi_n(x) = a_{n+1} \\psi_{n+1}(x) + a_n \\psi_{n-1}(x),\n$$\nwhere the coefficients for the weight $w(x) = \\exp(-x^2/(2\\sigma^2))$ are $a_n = \\sqrt{n\\sigma^2}$.\n\nUsing this framework, calculate the expected value of the sum of products of eigenvalues taken two at a time, $\\mathbb{E}\\left[\\sum_{1 \\le i < j \\le 3} \\lambda_i \\lambda_j\\right]$, for a $3 \\times 3$ GUE matrix (i.e., $N=3$). Express your answer in terms of the parameter $\\sigma$.", "solution": "1. Define the power sums \n$$p_1=\\sum_{i=1}^3\\lambda_i,\\qquad p_2=\\sum_{i=1}^3\\lambda_i^2,$$ \nand note the identity \n$$p_1^2=\\sum_i\\lambda_i^2+2\\sum_{i<j}\\lambda_i\\lambda_j\n=p_2+2e_2\\quad\\Longrightarrow\\quad e_2=\\frac{p_1^2-p_2}{2}.$$\n2. In matrix terms $p_1=\\text{tr}\\,H,\\ p_2=\\text{tr}\\,H^2$.  Under the GUE measure \n$$P(H)\\propto\\exp\\!\\Bigl(-\\frac1{2\\sigma^2}\\text{tr}\\,H^2\\Bigr)$$ \nthe independent entries satisfy \n$$\\mathbb{E}[H_{ii}^2]=\\sigma^2,\\quad\\mathbb{E}[|H_{ij}|^2]=\\sigma^2\\ (i\\ne j).$$\n3. Hence \n$$\\mathbb{E}[p_1]=0,\\quad \\mathbb{E}[p_1^2]=\\text{Var}(\\text{tr}\\,H)=3\\,\\sigma^2,$$ \nand \n$$\\mathbb{E}[p_2]=\\mathbb{E}[\\text{tr}\\,H^2]=\\sum_i\\mathbb{E}[H_{ii}^2]+2\\sum_{i<j}\\mathbb{E}[|H_{ij}|^2]\n=3\\sigma^2+2\\cdot3\\sigma^2=9\\sigma^2.$$\n4. Therefore \n$$\\mathbb{E}[e_2]\n=\\frac{\\mathbb{E}[p_1^2]-\\mathbb{E}[p_2]}{2}\n=\\frac{3\\sigma^2-9\\sigma^2}{2}\n=-3\\sigma^2.$$", "answer": "$$\\boxed{-3\\sigma^2}$$", "id": "751057"}]}