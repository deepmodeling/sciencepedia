{"hands_on_practices": [{"introduction": "Understanding the implications of stationary and independent increments is key to mastering the Poisson process. This exercise explores a classic scenario: if we know the total number of events that occurred over a period, what can we say about when they happened? This practice demonstrates how to use the core properties of the process to derive the conditional distribution of arrivals, a fundamental tool in queuing theory and event modeling. [@problem_id:1333460]", "problem": "The arrival of data packets at a network router is modeled as a Poisson process, denoted by $\\{N(t), t \\ge 0\\}$, with a constant average rate of $\\lambda$ packets per second. The router begins operation at time $t=0$. For scheduled maintenance, the router is shut down during the time interval $(t_1, t_2]$, where $0 < t_1 < t_2$. An observer monitors the router over the total time period from $t=0$ to $t=t_3$, where $t_3 > t_2$. During this entire period, it is found that a total of $n$ packets arrived during the time the router was operational.\n\nGiven this information, what is the probability that exactly $k$ of these $n$ packets arrived before the maintenance shutdown (i.e., in the interval $[0, t_1]$)?\n\nProvide your answer as a closed-form analytic expression in terms of the symbolic parameters $n$, $k$, $t_1$, $t_2$, and $t_3$.", "solution": "Let $\\{N(t), t \\ge 0\\}$ be a homogeneous Poisson process with rate $\\lambda$. The router is operational on the disjoint intervals $[0, t_{1}]$ and $(t_{2}, t_{3}]$. Define the random variables\n$$\nX = N(t_{1}) - N(0), \\quad Y = N(t_{3}) - N(t_{2}),\n$$\nwhich count arrivals during $[0, t_{1}]$ and $(t_{2}, t_{3}]$, respectively. By the independent increments property of the Poisson process, $X$ and $Y$ are independent. By the distribution of increments, \n$$\nX \\sim \\text{Poisson}(\\lambda t_{1}), \\quad Y \\sim \\text{Poisson}(\\lambda (t_{3} - t_{2})).\n$$\nThe observer reports that a total of $n$ packets arrived while the router was operational, so $X + Y = n$ is given. We seek $P(X = k \\mid X + Y = n)$ for $k \\in \\{0, 1, \\ldots, n\\}$.\n\nUsing the definition of conditional probability and independence,\n$$\nP(X = k \\mid X + Y = n) = \\frac{P(X = k, Y = n - k)}{P(X + Y = n)}.\n$$\nThe joint probability in the numerator factors as\n$$\nP(X = k, Y = n - k) = P(X = k)\\,P(Y = n - k) = \\exp(-\\lambda t_{1}) \\frac{(\\lambda t_{1})^{k}}{k!} \\cdot \\exp(-\\lambda (t_{3} - t_{2})) \\frac{(\\lambda (t_{3} - t_{2}))^{n - k}}{(n - k)!}.\n$$\nSince the sum of independent Poisson random variables is Poisson with the sum of their means, the denominator is\n$$\nP(X + Y = n) = \\exp(-\\lambda (t_{1} + t_{3} - t_{2})) \\frac{(\\lambda (t_{1} + t_{3} - t_{2}))^{n}}{n!}.\n$$\nTherefore,\n$$\nP(X = k \\mid X + Y = n) = \\frac{\\exp(-\\lambda t_{1}) \\exp(-\\lambda (t_{3} - t_{2})) (\\lambda t_{1})^{k} (\\lambda (t_{3} - t_{2}))^{n - k} \\frac{n!}{k!(n-k)!}}{\\exp(-\\lambda (t_{1} + t_{3} - t_{2})) (\\lambda (t_{1} + t_{3} - t_{2}))^{n}}.\n$$\nCanceling the exponential terms and powers of $\\lambda$ yields the binomial form\n$$\nP(X = k \\mid X + Y = n) = \\binom{n}{k} \\left(\\frac{t_{1}}{t_{1} + t_{3} - t_{2}}\\right)^{k} \\left(\\frac{t_{3} - t_{2}}{t_{1} + t_{3} - t_{2}}\\right)^{n - k}.\n$$\nThis gives the desired probability that exactly $k$ of the $n$ observed packets arrived before the shutdown.", "answer": "$$\\boxed{\\binom{n}{k}\\left(\\frac{t_{1}}{t_{1}+t_{3}-t_{2}}\\right)^{k}\\left(\\frac{t_{3}-t_{2}}{t_{1}+t_{3}-t_{2}}\\right)^{n-k}}$$", "id": "1333460"}, {"introduction": "We now turn to the continuous-time Wiener process, where the concept of independent increments leads to a rich internal structure. A common question is how knowledge of a process's future state affects our understanding of its past. This practice challenges you to think backward in time, calculating the probability of a past event given a future observation, and in doing so, reveals the covariance structure that underpins the entire process. [@problem_id:1333421]", "problem": "A nanoparticle is observed to undergo one-dimensional random motion along an axis, starting from the origin at time $t=0$. Its position at time $t$, denoted by $X(t)$, can be modeled by a standard Wiener process. For the purpose of this problem, the key properties of a standard Wiener process $W(t)$ are:\n1. $W(0) = 0$.\n2. For any $t > 0$, the random variable $W(t)$ is normally distributed with a mean of 0 and a variance of $t$.\n3. The process has independent increments, meaning that for any two non-overlapping time intervals $[t_1, t_2]$ and $[t_3, t_4]$, the random variables $W(t_2) - W(t_1)$ and $W(t_4) - W(t_3)$ are independent.\n\nAn experimenter measures the particle's position at time $t=4$ and finds it to be $X(4) = 2$. Given this observation, what is the probability that the particle's position was positive (i.e., $X(1) > 0$) at the earlier time of $t=1$?\n\nReport your answer as a decimal rounded to three significant figures.", "solution": "We model the position process as a standard Wiener process $W(t)$. Consider the bivariate normal vector $(W(1), W(4))$. By properties of Wiener processes:\n- $W(1) \\sim \\mathcal{N}(0,1)$ and $W(4) \\sim \\mathcal{N}(0,4)$ since $\\operatorname{Var}(W(t))=t$.\n- Using independent increments, write $W(4)=W(1)+\\big(W(4)-W(1)\\big)$ with $W(4)-W(1)$ independent of $W(1)$. Hence,\n$$\n\\operatorname{Cov}(W(1), W(4))=\\operatorname{Cov}\\big(W(1), W(1)\\big)+\\operatorname{Cov}\\big(W(1), W(4)-W(1)\\big)=\\operatorname{Var}(W(1)) + 0 = 1.\n$$\nTherefore, $(W(1), W(4))$ is jointly normal with covariance matrix\n$$\n\\Sigma=\\begin{pmatrix}\n1 & 1 \\\\\n1 & 4\n\\end{pmatrix}.\n$$\n\nFor jointly normal variables, the conditional distribution $W(1)\\,|\\,W(4)=2$ is normal with mean and variance given by\n$$\n\\mu_{1|4}=\\frac{\\operatorname{Cov}(W(1),W(4))}{\\operatorname{Var}(W(4))}\\cdot 2=\\frac{1}{4}\\cdot 2=\\frac{1}{2}, \\quad\n\\sigma_{1|4}^{2}=\\operatorname{Var}(W(1))-\\frac{\\operatorname{Cov}(W(1),W(4))^{2}}{\\operatorname{Var}(W(4))}=1-\\frac{1}{4}=\\frac{3}{4}.\n$$\nThus,\n$$\nW(1)\\,|\\,\\{W(4)=2\\} \\sim \\mathcal{N}\\!\\left(\\frac{1}{2}, \\frac{3}{4}\\right).\n$$\n\nWe seek\n$$\n\\mathbb{P}\\big(W(1)>0 \\,\\big|\\, W(4)=2\\big)=\\mathbb{P}\\!\\left(\\mathcal{N}\\!\\left(\\frac{1}{2},\\frac{3}{4}\\right)>0\\right).\n$$\nStandardizing with $Z\\sim \\mathcal{N}(0,1)$,\n$$\n\\mathbb{P}\\!\\left(\\mathcal{N}\\!\\left(\\frac{1}{2},\\frac{3}{4}\\right)>0\\right)\n=1-\\Phi\\!\\left(\\frac{0-\\frac{1}{2}}{\\sqrt{\\frac{3}{4}}}\\right)\n=\\Phi\\!\\left(\\frac{\\frac{1}{2}}{\\sqrt{\\frac{3}{4}}}\\right)\n=\\Phi\\!\\left(\\frac{1}{\\sqrt{3}}\\right),\n$$\nwhere $\\Phi(a)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{a}\\exp\\!\\left(-\\frac{u^{2}}{2}\\right)\\,du$ is the standard normal cumulative distribution function.\n\nNumerically, $\\frac{1}{\\sqrt{3}}\\approx 0.577350$, and hence\n$$\n\\mathbb{P}\\big(W(1)>0 \\,\\big|\\, W(4)=2\\big)=\\Phi\\!\\left(\\frac{1}{\\sqrt{3}}\\right)\\approx 0.718 \\quad \\text{(to three significant figures)}.\n$$", "answer": "$$\\boxed{0.718}$$", "id": "1333421"}, {"introduction": "Stochastic processes are powerful building blocks for creating more sophisticated models. In this final practice, we construct a \"subordinated\" process by using the random event times of a Poisson process as the time input for a Wiener process. This exercise demonstrates how to analyze such composite models using powerful tools like the law of total variance to uncover the properties, such as the variance of its increments, of the new process. [@problem_id:1333454]", "problem": "Consider a stochastic process $X(t)$ constructed by composing a standard Wiener process and an independent Poisson process. Let $W(t)$ for $t \\ge 0$ be a standard one-dimensional Wiener process, satisfying $W(0) = 0$, and for any $0 \\le u < v$, the increment $W(v) - W(u)$ is normally distributed with mean 0 and variance $v-u$. Let $N(t)$ for $t \\ge 0$ be an independent Poisson process with a constant rate $\\lambda > 0$, satisfying $N(0)=0$. For any $0 \\le u < v$, the increment $N(v) - N(u)$ follows a Poisson distribution with mean $\\lambda(v-u)$. The process $W(t)$ is independent of the process $N(t)$.\n\nA new \"subordinated\" process $X(t)$ is defined as $X(t) = W(N(t))$. For any fixed times $s$ and $t$ such that $0 \\le s < t$, determine the variance of the increment $X(t) - X(s)$.\n\nProvide your answer as a closed-form analytic expression in terms of $\\lambda$, $s$, and $t$.", "solution": "Define the increment $Y:=X(t)-X(s)=W(N(t))-W(N(s))$ for $0 \\le s < t$. Condition on the sigma-field $\\mathcal{G}:=\\sigma(N(s),N(t))$. By the independent increments of $W$ and the independence of $W$ and $N$, given $N(s)=n$ and $N(t)=\\ell$ (so that $\\ell \\ge n$), the increment can be written as $Y=W(\\ell)-W(n)$ with $W(\\ell)-W(n)\\sim \\mathcal{N}(0,\\ell-n)$. Therefore,\n$$\n\\mathbb{E}[Y \\mid \\mathcal{G}]=0, \\qquad \\operatorname{Var}(Y \\mid \\mathcal{G})=\\ell-n=N(t)-N(s).\n$$\nApply the law of total variance:\n$$\n\\operatorname{Var}(Y)=\\mathbb{E}\\!\\left[\\operatorname{Var}(Y \\mid \\mathcal{G})\\right]+\\operatorname{Var}\\!\\left(\\mathbb{E}[Y \\mid \\mathcal{G}]\\right)=\\mathbb{E}[N(t)-N(s)]+0.\n$$\nUsing the Poisson increment property, $N(t)-N(s)\\sim \\mathrm{Poisson}(\\lambda(t-s))$, hence $\\mathbb{E}[N(t)-N(s)]=\\lambda(t-s)$. Therefore,\n$$\n\\operatorname{Var}(X(t)-X(s))=\\lambda(t-s).\n$$", "answer": "$$\\boxed{\\lambda (t-s)}$$", "id": "1333454"}]}