{"hands_on_practices": [{"introduction": "Understanding the expected duration of a random process is a fundamental question in stochastic analysis. This first exercise provides a clean and elegant entry point by exploring the expected time for a random walk to exit a symmetric interval. By assuming no \"overshoot\"—that the walk stops exactly on the boundary—we can use the power of martingales and the Optional Stopping Theorem to derive a surprisingly simple and intuitive result, connecting the exit time directly to the boundary size and the variance of the steps [@problem_id:871105]. This practice is foundational for building intuition about stopping times.", "problem": "Consider a one-dimensional random walk $S_n$ starting at the origin, $S_0 = 0$. The walk is defined by the sum $S_n = \\sum_{i=1}^n X_i$, where the increments $X_i$ are independent and identically distributed (i.i.d.) random variables. These increments have a zero mean, $\\mathbb{E}[X_i] = 0$, and a finite, non-zero variance, $\\text{Var}(X_i) = \\mathbb{E}[X_i^2] = \\sigma^2$.\n\nLet $T$ be the first time the random walk exits the open symmetric interval $(-a, a)$, where $a$ is a positive constant. This stopping time is formally defined as $T = \\inf\\{n \\ge 1 : |S_n| \\ge a \\}$.\n\nTo simplify the analysis, we make the crucial assumption of **no overshoot**. This means that when the walk stops, its position is exactly on one of the boundaries, i.e., $S_T = a$ or $S_T = -a$. We also assume that the stopping time $T$ is finite with probability 1.\n\nUsing principles related to martingales and the optional stopping theorem (often connected to Wald's identity), derive an expression for the expected stopping time, $\\mathbb{E}[T]$, in terms of the boundary parameter $a$ and the increment variance $\\sigma^2$.", "solution": "1. Define the process  \n$$M_n = S_n^2 - n\\,\\sigma^2.$$  \nSince $\\{X_i\\}$ are i.i.d. with $\\mathbb{E}[X_i]=0$ and $\\text{Var}(X_i)=\\sigma^2$, we have  \n$$\\mathbb{E}[S_{n+1}^2\\mid\\mathcal{F}_n]\n=\\mathbb{E}[(S_n+X_{n+1})^2\\mid\\mathcal{F}_n]\n=S_n^2+2S_n\\mathbb{E}[X_{n+1}]+\\mathbb{E}[X_{n+1}^2]\n=S_n^2+\\sigma^2.$$  \nHence  \n$$\\mathbb{E}[M_{n+1}\\mid\\mathcal{F}_n]\n=\\mathbb{E}[S_{n+1}^2\\mid\\mathcal{F}_n]-(n+1)\\sigma^2\n=S_n^2+\\sigma^2-(n+1)\\sigma^2\n=M_n,$$  \nso $M_n$ is a martingale.\n\n2. By the optional stopping theorem at the finite stopping time $T$,  \n$$\\mathbb{E}[M_T]=\\mathbb{E}[M_0].$$  \nSince $M_0=S_0^2-0\\cdot\\sigma^2=0$, we get  \n$$\\mathbb{E}[S_T^2 - T\\,\\sigma^2]=0.$$\n\n3. Under the no-overshoot assumption, $|S_T|=a$, so $S_T^2=a^2$. Thus  \n$$a^2 - \\sigma^2\\,\\mathbb{E}[T]=0\n\\quad\\Longrightarrow\\quad\n\\mathbb{E}[T]=\\frac{a^2}{\\sigma^2}.$$", "answer": "$$\\boxed{\\frac{a^2}{\\sigma^2}}$$", "id": "871105"}, {"introduction": "Moving from idealized scenarios to more realistic applications, this problem models a common situation in reliability engineering: cumulative damage leading to failure. Here, we apply Wald's identity directly to find the expected number of \"surges\" a component can withstand [@problem_id:871116]. This exercise is particularly insightful as it introduces the concept of \"overshoot\"—the amount by which the process exceeds the threshold at the stopping time—and demonstrates how the memoryless property of the geometric distribution allows for its exact calculation.", "problem": "A sensitive electronic component is subject to a series of power surges. The damage from each surge is quantized, and the magnitude of the $i$-th surge is represented by a random variable $X_i$. These magnitudes are independent and identically distributed (i.i.d.) random variables.\n\nEach $X_i$ follows a geometric distribution with success parameter $p \\in (0, 1]$, described by the probability mass function:\n$$\nP(X = k) = (1-p)^{k-1}p, \\quad \\text{for } k \\in \\{1, 2, 3, \\dots\\}\n$$\nThis distribution models the number of trials required to achieve the first success in a sequence of independent Bernoulli trials.\n\nThe component experiences catastrophic failure when the cumulative damage, $S_n = \\sum_{i=1}^n X_i$, first reaches or exceeds a critical threshold $K$, where $K$ is a positive integer. Let $N$ be the number of surges that leads to the component's failure. Mathematically, $N$ is a stopping time defined as:\n$$\nN = \\min\\{n \\in \\{1, 2, \\dots\\} : S_n \\ge K\\}\n$$\n\nUsing the principles of sequential analysis, specifically Wald's identity, derive the expected number of surges, $\\mathbb{E}[N]$, that the component can withstand before failure. Express your answer in terms of $p$ and $K$.", "solution": "1. Relevant equations  \nWald’s identity for a stopping time $N$ and i.i.d. increments $X_i$ with finite mean:\n$$\\mathbb{E}[S_N]=\\mathbb{E}[N]\\,\\mathbb{E}[X],\\quad S_N=\\sum_{i=1}^N X_i.$$\nThe first passage sum satisfies\n$$S_N=K+R,$$\nwhere $R=S_N-K$ is the overshoot.\n\n2. Compute $\\mathbb{E}[X]$ and $\\mathbb{E}[R]$  \nFor $X\\sim\\mathrm{Geom}(p)$ on $\\{1,2,\\dots\\}$,\n$$\\mathbb{E}[X]=\\frac1p.$$\nBy the memoryless property, conditional on the last jump exceeding the gap to $K$, the overshoot $R$ is geometric on $\\{0,1,2,\\dots\\}$ with parameter $p$, so\n$$\\mathbb{E}[R]=\\frac{1-p}{p}.$$\n\n3. Combine via Wald’s identity  \n$$\\mathbb{E}[S_N]=\\mathbb{E}[K+R]=K+\\frac{1-p}{p},$$\nso\n$$\\mathbb{E}[N]\n=\\frac{\\mathbb{E}[S_N]}{\\mathbb{E}[X]}\n=\\biggl(K+\\frac{1-p}{p}\\biggr)\\,p\n=pK+1-p.$$", "answer": "$$\\boxed{pK+1-p}$$", "id": "871116"}, {"introduction": "This final practice challenges you to think like a data scientist by tackling an inverse problem. Instead of predicting the outcome of a random walk, you are given an observed outcome—the expected final position—and asked to infer a fundamental parameter of the process itself: its bias [@problem_id:871112]. This exercise leverages the classic Gambler's Ruin framework, which is intimately connected to martingales, to build a relationship between the stopping position and the underlying probabilities. Solving this problem will deepen your understanding of how the characteristics of a stopped process reveal crucial information about its internal dynamics.", "problem": "Consider a one-dimensional simple random walk $S_n = \\sum_{i=1}^n X_i$ that starts at $S_0=0$. The steps $X_i$ are independent and identically distributed random variables with probability mass function $P(X_i = +1) = p$ and $P(X_i = -1) = 1-p$, where $p \\in (0,1)$ is the bias parameter.\n\nThe walk is stopped at the first time $T$ it reaches either a lower boundary at $-a$ or an upper boundary at $a$, where $a$ is a positive integer. The stopping time is thus defined as $T = \\inf\\{n \\ge 1 : S_n = -a \\text{ or } S_n = a\\}$. We assume that the bias $p$ is not equal to $1/2$.\n\nThe expected position of the walk at this stopping time is given as $\\mathbb{E}[S_T] = \\mathcal{E}$. Note that for such a system to be physically realizable, it must be that $|\\mathcal{E}| < a$.\n\nDerive a closed-form expression for the bias parameter $p$ in terms of the boundary parameter $a$ and the expected stopping position $\\mathcal{E}$.", "solution": "1. Gambler’s ruin probabilities. Let $r=\\frac{1-p}{p}$. The probability of hitting $+a$ before $-a$ starting from $0$ is\n$$\nP_{+a}\n=\\frac{1-r^a}{1-r^{2a}}.\n$$\n\n2. Expected stopping position. Since $S_T=\\pm a$, we have\n$$\n\\mathbb{E}[S_T]\n=a\\,P_{+a}-a\\,(1-P_{+a})\n=a\\bigl(2P_{+a}-1\\bigr)\n=a\\,\\frac{1-r^a}{1+r^a}.\n$$\n\n3. Solve for $r$. Set $\\mathcal{E}=\\mathbb{E}[S_T]$ and define $X=\\mathcal{E}/a$. Then\n$$\nX=\\frac{1-r^a}{1+r^a}\n\\;\\Longrightarrow\\;\nr^a=\\frac{1-X}{1+X}\n\\;\\Longrightarrow\\;\nr=\\Bigl(\\tfrac{1-X}{1+X}\\Bigr)^{\\!1/a}.\n$$\n\n4. Express $p$. Since $r=(1-p)/p$, we get\n$$\np=\\frac{1}{1+r}\n=\\frac{1}{1+\\bigl(\\tfrac{1-X}{1+X}\\bigr)^{1/a}}\n=\\frac{1}{1+\\left(\\frac{1-\\mathcal{E}/a}{1+\\mathcal{E}/a}\\right)^{1/a}}.\n$$", "answer": "$$\\boxed{\\frac{1}{1+\\left(\\frac{1-\\mathcal{E}/a}{1+\\mathcal{E}/a}\\right)^{1/a}}}$$", "id": "871112"}]}