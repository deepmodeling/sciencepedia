{"hands_on_practices": [{"introduction": "Wide-Sense Stationarity (WSS) is a cornerstone concept in signal processing and time-series analysis, providing a practical framework for characterizing processes whose statistical properties are stable over time. This exercise challenges you to go beyond simple cases and investigate how properties of stationarity behave under composition. By determining if the product of two independent WSS processes is itself WSS, you will practice applying the fundamental definitions of constant mean and time-invariant autocorrelation, skills essential for modeling complex systems [@problem_id:1289251].", "problem": "In the modeling of complex systems, such as in signal processing or financial analysis, a new process is often generated by the product of two or more existing processes. Consider two stochastic processes, $X(t)$ and $Y(t)$, which are statistically independent of each other. Both $X(t)$ and $Y(t)$ are known to be Wide-Sense Stationary (WSS).\n\nA stochastic process $U(t)$ is defined as Wide-Sense Stationary (WSS) if it satisfies two conditions:\n1. Its mean function, $\\mu_U(t) = E[U(t)]$, is a constant for all time $t$.\n2. Its autocorrelation function, $R_{UU}(t_1, t_2) = E[U(t_1)U(t_2)]$, depends only on the time lag $\\tau = t_1 - t_2$.\n\nA new product process, $Z(t)$, is formed such that $Z(t) = X(t)Y(t)$.\n\nWhich of the following statements correctly describes the condition under which the product process $Z(t)$ is guaranteed to be Wide-Sense Stationary?\n\nA. $Z(t)$ is always WSS under the given conditions.\n\nB. $Z(t)$ is only WSS if at least one of the processes, $X(t)$ or $Y(t)$, has a zero mean.\n\nC. $Z(t)$ is only WSS if both processes, $X(t)$ and $Y(t)$, have zero mean.\n\nD. $Z(t)$ is only WSS if both $X(t)$ and $Y(t)$ are also Gaussian processes.\n\nE. The stationarity of $Z(t)$ cannot be determined from the information provided.", "solution": "Let $X(t)$ and $Y(t)$ be independent WSS processes, and define $Z(t)=X(t)Y(t)$. To check whether $Z(t)$ is WSS, we verify the two WSS conditions.\n\nMean: Because $X(t)$ and $Y(t)$ are independent for each $t$, we have\n$$\n\\mu_Z(t)=E[Z(t)]=E[X(t)Y(t)]=E[X(t)]\\,E[Y(t)].\n$$\nSince $X(t)$ and $Y(t)$ are WSS, their means are constants, say $E[X(t)]=\\mu_X$ and $E[Y(t)]=\\mu_Y$, hence\n$$\n\\mu_Z(t)=\\mu_X\\mu_Y,\n$$\nwhich is a constant. Thus the mean condition holds without any further assumptions.\n\nAutocorrelation: For any $t_1,t_2$,\n$$\nR_{ZZ}(t_1,t_2)=E[Z(t_1)Z(t_2)]=E[X(t_1)Y(t_1)X(t_2)Y(t_2)].\n$$\nIndependence of the processes implies that the vector $(X(t_1),X(t_2))$ is independent of the vector $(Y(t_1),Y(t_2))$. Therefore,\n$$\nE[X(t_1)X(t_2)Y(t_1)Y(t_2)]=E[X(t_1)X(t_2)]\\;E[Y(t_1)Y(t_2)]\n=R_{XX}(t_1,t_2)\\,R_{YY}(t_1,t_2).\n$$\nSince $X(t)$ and $Y(t)$ are WSS, $R_{XX}(t_1,t_2)=R_{XX}(t_1-t_2)$ and $R_{YY}(t_1,t_2)=R_{YY}(t_1-t_2)$, hence\n$$\nR_{ZZ}(t_1,t_2)=R_{XX}(t_1-t_2)\\,R_{YY}(t_1-t_2),\n$$\nwhich depends only on the lag $\\tau=t_1-t_2$. Thus the autocorrelation condition holds. The required moments exist because WSS guarantees finite second moments, and the product above is finite.\n\nTherefore, under the given conditions (independent WSS processes), $Z(t)$ is WSS without requiring zero means or Gaussianity.", "answer": "$$\\boxed{A}$$", "id": "1289251"}, {"introduction": "Processes with independent increments, like the Wiener process or Poisson process, are particularly tractable because their future changes do not depend on their past values. However, not all processes that seem simple possess this convenient property. This problem asks you to critically examine the number of customers in an $M/G/\\infty$ queueing system, a common model in operations research and teletraffic theory. You will discover that even when arrivals have independent increments, the system as a whole may not, forcing you to dissect the process's dynamics and understand how dependencies can arise [@problem_id:1289207].", "problem": "Consider an $M/G/\\infty$ queueing system, which models a scenario with an infinite number of servers. Customers arrive according to a Poisson process with a constant rate of $\\lambda > 0$. Upon arrival, a customer immediately enters service, as there is always a free server. The service times are independent and identically distributed random variables, drawn from a general probability distribution with a finite, non-zero mean $E[S]$. The service time distribution is independent of the arrival process.\n\nLet $X(t)$ for $t \\ge 0$ be the stochastic process representing the number of customers in the system at time $t$. Assume the system is empty at time $t=0$, i.e., $X(0) = 0$.\n\nWhich of the following statements correctly describes the property of independent increments for the process $X(t)$?\n\nA. The process $X(t)$ has independent increments because the underlying arrival process is a Poisson process, which has independent increments.\n\nB. The process $X(t)$ has independent increments, but only under the condition that the system is stable, which in this case means the mean service time is finite.\n\nC. The process $X(t)$ does not have independent increments because the number of departures in any time interval $(t_1, t_2)$ is dependent on the number of customers present at time $t_1$.\n\nD. The process $X(t)$ does not have independent increments because the process is not stationary, meaning the probability distribution of $X(t)$ changes as a function of $t$.\n\nE. The process $X(t)$ has independent increments only if the service times are exponentially distributed (i.e., the system is an $M/M/\\infty$ queue).", "solution": "Define the independent-increments property: a process $\\{X(t): t \\ge 0\\}$ has independent increments if for any disjoint intervals $(t_0, t_1], (t_1, t_2], \\ldots, (t_{k-1}, t_k]$, the random variables $X(t_1) - X(t_0), X(t_2) - X(t_1), \\ldots, X(t_k) - X(t_{k-1})$ are mutually independent.\n\nIn an $M/G/\\infty$ system, let arrivals be a Poisson process with rate $\\lambda$, and let $\\{S_i\\}$ be i.i.d. service times, independent of arrivals. Represent the system via a Poisson random measure of marked arrivals $\\{(A_i, S_i)\\}$ on $\\mathbb{R}_{+} \\times \\mathbb{R}_{+}$ with intensity measure $\\lambda \\, da \\, F_S(ds)$. Then the number in system at time $t$ is\n$$\nX(t) \\;=\\; \\sum_{i} \\mathbf{1}\\{A_i \\le t  A_i + S_i\\} \\;=\\; \\sum_{i} \\mathbf{1}\\{A_i \\le t\\}\\,\\mathbf{1}\\{S_i > t - A_i\\}.\n$$\nFix $0  t_1  t_2$ and write the increment as\n$$\n\\Delta X(t_1, t_2) \\;:=\\; X(t_2) - X(t_1) \\;=\\; N_{\\text{arr}}(t_1, t_2) \\;-\\; D(t_1, t_2),\n$$\nwhere $N_{\\text{arr}}(t_1, t_2)$ is the number of arrivals in $(t_1, t_2]$, and $D(t_1, t_2)$ is the number of departures in $(t_1, t_2]$. Because arrivals form a Poisson process, $N_{\\text{arr}}(t_1, t_2)$ is Poisson with mean $\\lambda (t_2 - t_1)$ and is independent of the history up to time $t_1$. However, $D(t_1, t_2)$ has a component due to customers already present at $t_1$, which necessarily depends on the state at $t_1$.\n\nDecompose departures as\n$$\nD(t_1, t_2) \\;=\\; D_{\\text{pre}}(t_1, t_2) \\;+\\; D_{\\text{post}}(t_1, t_2),\n$$\nwhere $D_{\\text{pre}}(t_1, t_2)$ counts the customers that were in service at $t_1$ and complete service by $t_2$, and $D_{\\text{post}}(t_1, t_2)$ counts those that both arrive after $t_1$ and depart by $t_2$. The term $D_{\\text{post}}(t_1, t_2)$ depends only on arrivals and service times after $t_1$ and is independent of the past. By contrast,\n$$\nD_{\\text{pre}}(t_1, t_2) \\;=\\; \\sum_{j=1}^{X(t_1)} \\mathbf{1}\\{R_j \\le t_2 - t_1\\},\n$$\nwhere $R_j$ is the residual service time at $t_1$ of the $j$th customer present then. Conditional on $X(t_1) = n$ and on the residuals $\\{R_j\\}_{j=1}^n$, this sum is a function of these $n$ residual times. In particular, conditional on $X(t_1) = n$, the distribution of $D_{\\text{pre}}(t_1, t_2)$ stochastically increases with $n$, so the distribution of $\\Delta X(t_1, t_2)$ depends on $X(t_1)$.\n\nA concrete special case underscores the dependence. If service times are exponential with rate $\\mu$ (the $M/M/\\infty$ case), then given $X(t_1) = n$, each of the $n$ customers present at $t_1$ departs in $(t_1, t_2]$ independently with probability $1 - \\exp(-\\mu (t_2 - t_1))$. Thus\n$$\nD_{\\text{pre}}(t_1, t_2) \\,\\big|\\, X(t_1) = n \\;\\sim\\; \\text{Binomial}(n,\\, 1 - \\exp(-\\mu (t_2 - t_1))),\n$$\nso the law of $\\Delta X(t_1, t_2)$ depends on $n$. Therefore $\\Delta X(t_1, t_2)$ is not independent of the sigma-field generated by the past up to $t_1$, and increments over disjoint intervals cannot be independent.\n\nThis directly contradicts options asserting independent increments. The failure has nothing to do with stability (the infinite-server system is always well-defined with finite $E[S]$, but independent increments would still fail) and nothing to do with stationarity (even if started in stationarity, increments remain dependent because departures in a future interval depend on the current population). Moreover, making service exponential does not fix this; it only makes $\\{X(t)\\}$ Markov, not a process with independent increments.\n\nHence, the correct statement is that $X(t)$ does not have independent increments because departures in an interval depend on the number in system at the beginning of that interval, which is option C.", "answer": "$$\\boxed{C}$$", "id": "1289207"}, {"introduction": "The concept of a martingale formalizes the idea of a \"fair game\" and is one of the most powerful tools in modern probability theory, with deep applications in quantitative finance. This practice provides a crucial hands-on test of your ability to identify martingales in a continuous-time context. By analyzing several processes derived from the standard Wiener process—the quintessential continuous-time random process—you will learn to verify the core martingale property, $E[X_t | \\mathcal{F}_s] = X_s$, and become familiar with foundational examples like the geometric Brownian motion martingale [@problem_id:1289227].", "problem": "In the context of quantitative finance and stochastic modeling, a key concept is that of a martingale, which models a fair game where the expected future value, given the present and past, is equal to the current value.\n\nLet $W_t$ be a standard Wiener process for $t \\ge 0$, characterized by $W_0=0$, continuous paths, and increments $W_t - W_s$ that are normally distributed with mean 0 and variance $t-s$ for $s  t$, and are independent of the process's history up to time $s$. Let $\\mathcal{F}_t = \\sigma(W_u : 0 \\le u \\le t)$ be the natural filtration generated by the Wiener process.\n\nA stochastic process $X_t$ is defined as a martingale with respect to the filtration $\\mathcal{F}_t$ if it satisfies three conditions:\n1.  $X_t$ is adapted to the filtration $\\mathcal{F}_t$ for all $t \\ge 0$.\n2.  The expectation of its absolute value is finite, i.e., $E[|X_t|]  \\infty$ for all $t \\ge 0$.\n3.  For any $s  t$, the conditional expectation of the future value given the history up to time $s$ is the current value, i.e., $E[X_t | \\mathcal{F}_s] = X_s$.\n\nConsider the following five stochastic processes, where $\\sigma$ is a non-zero real constant.\n\nA. $X_t^{(A)} = W_t$\nB. $X_t^{(B)} = W_t^2 - t$\nC. $X_t^{(C)} = W_t^2 + t$\nD. $X_t^{(D)} = \\exp(\\sigma W_t - \\frac{1}{2}\\sigma^2 t)$\nE. $X_t^{(E)} = \\sin(W_t)$\n\nWhich of these processes are martingales with respect to the natural filtration $\\mathcal{F}_t$? Select all valid options. Your answer should be a string of concatenated uppercase letters in alphabetical order (e.g., `AC`).", "solution": "We work with the natural filtration $\\mathcal{F}_t$ of a standard Wiener process $(W_t)_{t \\ge 0}$. Each candidate process is $\\mathcal{F}_t$-adapted and satisfies $E[|X_t|]  \\infty$ for all $t \\ge 0$ (since $W_t$ has Gaussian moments, $\\exp(\\sigma W_t - \\frac{1}{2}\\sigma^2 t)$ has unit expectation, and $|\\sin(W_t)| \\le 1$). Thus, we only need to verify the martingale property $E[X_t \\mid \\mathcal{F}_s] = X_s$ for $s  t$.\n\nA. $X_t^{(A)} = W_t$. Using independent increments,\n$$\nE[W_t \\mid \\mathcal{F}_s] = E[W_s + (W_t - W_s) \\mid \\mathcal{F}_s] = W_s + E[W_t - W_s \\mid \\mathcal{F}_s] = W_s.\n$$\nHence $X^{(A)}$ is a martingale.\n\nB. $X_t^{(B)} = W_t^2 - t$. Write $W_t = W_s + \\Delta$ with $\\Delta := W_t - W_s$ independent of $\\mathcal{F}_s$, $E[\\Delta] = 0$, $\\mathrm{Var}(\\Delta) = t - s$. Then\n$$\nW_t^2 = W_s^2 + 2 W_s \\Delta + \\Delta^2,\n$$\nso\n$$\nE[W_t^2 \\mid \\mathcal{F}_s] = W_s^2 + 2 W_s E[\\Delta \\mid \\mathcal{F}_s] + E[\\Delta^2 \\mid \\mathcal{F}_s] = W_s^2 + (t - s).\n$$\nTherefore,\n$$\nE[W_t^2 - t \\mid \\mathcal{F}_s] = W_s^2 + (t - s) - t = W_s^2 - s = X_s^{(B)}.\n$$\nHence $X^{(B)}$ is a martingale.\n\nC. $X_t^{(C)} = W_t^2 + t$. Using the same computation,\n$$\nE[W_t^2 + t \\mid \\mathcal{F}_s] = W_s^2 + (t - s) + t = W_s^2 + 2t - s \\neq W_s^2 + s = X_s^{(C)}\n$$\nfor $t > s$. Hence $X^{(C)}$ is not a martingale.\n\nD. $X_t^{(D)} = \\exp(\\sigma W_t - \\tfrac{1}{2}\\sigma^2 t)$. Let $\\Delta := W_t - W_s \\sim \\mathcal{N}(0, t - s)$ independent of $\\mathcal{F}_s$. Then\n$$\nE[\\exp(\\sigma W_t - \\tfrac{1}{2}\\sigma^2 t) \\mid \\mathcal{F}_s]\n= \\exp(\\sigma W_s - \\tfrac{1}{2}\\sigma^2 s) E[\\exp(\\sigma \\Delta - \\tfrac{1}{2}\\sigma^2 (t - s))].\n$$\nUsing the moment generating function of a centered normal variable, for $\\Delta \\sim \\mathcal{N}(0, t - s)$,\n$$\nE[\\exp(\\sigma \\Delta)] = \\exp(\\tfrac{1}{2}\\sigma^2(t - s)),\n$$\nso\n$$\nE[\\exp(\\sigma \\Delta - \\tfrac{1}{2}\\sigma^2(t - s))] = 1.\n$$\nThus,\n$$\nE[X_t^{(D)} \\mid \\mathcal{F}_s] = \\exp(\\sigma W_s - \\tfrac{1}{2}\\sigma^2 s) = X_s^{(D)}.\n$$\nHence $X^{(D)}$ is a martingale.\n\nE. $X_t^{(E)} = \\sin(W_t)$. Using the addition formula and $\\Delta := W_t - W_s$ independent of $\\mathcal{F}_s$,\n$$\n\\sin(W_t) = \\sin(W_s + \\Delta) = \\sin(W_s) \\cos(\\Delta) + \\cos(W_s) \\sin(\\Delta).\n$$\nWith $\\Delta \\sim \\mathcal{N}(0, t - s)$, its characteristic function gives\n$$\nE[\\exp(i \\Delta)] = \\exp(-\\tfrac{1}{2}(t - s)),\n$$\nso $E[\\cos(\\Delta)] = \\exp(-\\tfrac{1}{2}(t - s))$ and $E[\\sin(\\Delta)] = 0$. Therefore,\n$$\nE[\\sin(W_t) \\mid \\mathcal{F}_s] = \\sin(W_s) \\exp(-\\tfrac{1}{2}(t - s)) \\neq \\sin(W_s)\n$$\nfor $t > s$ unless $\\sin(W_s) = 0$ almost surely. Hence $X^{(E)}$ is not a martingale.\n\nConclusion: The martingales among the given processes are $X^{(A)}$, $X^{(B)}$, and $X^{(D)}$.", "answer": "$$\\boxed{ABD}$$", "id": "1289227"}]}