{"hands_on_practices": [{"introduction": "The analysis of any G/M/1 queue begins with finding a crucial parameter, $\\sigma$, which is the steady-state probability that an arriving customer must wait. This parameter is the unique root of a fundamental equation connecting the arrival process to the service rate. This first exercise [@problem_id:1338336] presents a concrete scenario with deterministic arrivals (a D/M/1 system) to provide hands-on practice in setting up and solving for this pivotal value.", "problem": "In an automated manufacturing facility, a specialized robotic arm is tasked with processing components. These components are delivered by a conveyor belt, arriving one at a time at precisely regular intervals of $T = 30$ seconds. The time required for the robotic arm to complete its task on a single component is a random variable that follows an exponential distribution with a mean processing time of $25$ seconds. If a component arrives while the robot is already busy, it is held in a queue with sufficient capacity.\n\nAssuming the system has been operating for a long time and has reached a statistical equilibrium, which of the following choices best represents the probability that a newly arriving component finds the robotic arm busy and therefore must wait in the queue?\n\nA. 0.582\n\nB. 0.691\n\nC. 0.833\n\nD. 0.500\n\nE. 0.167", "solution": "Let the interarrival time be deterministic with value $T$ and the service time be exponential with mean $1/\\mu$, where $\\mu$ is the service rate. Here $T=30$ and $\\mu=\\frac{1}{25}$, so $\\mu T=\\frac{6}{5}$, and the utilization is $\\rho=\\frac{1/T}{\\mu}=\\frac{25}{30}=\\frac{5}{6}1$, ensuring stability.\n\nFor a G/M/1 queue, the steady-state probability that an arrival finds the server busy (equivalently, that it must wait) equals $\\sigma$, where $\\sigma$ is the unique root in $[0,1)$ of the functional equation\n$$\n\\sigma=A^*(\\mu-\\mu \\sigma),\n$$\nwith $A^*$ the Laplaceâ€“Stieltjes transform of the interarrival-time distribution. For deterministic interarrival time $T$, $A^*(s)=\\exp(-sT)$, hence\n$$\n\\sigma=\\exp\\!\\big(-(\\mu-\\mu \\sigma)T\\big)=\\exp\\!\\big(-\\mu T(1-\\sigma)\\big).\n$$\nThis can be solved explicitly using the Lambert $W$ function. Rewrite as\n$$\n\\sigma\\,\\exp(-\\mu T \\sigma)=\\exp(-\\mu T),\n$$\nthen multiply both sides by $-\\mu T$ to obtain\n$$\n(-\\mu T \\sigma)\\,\\exp(-\\mu T \\sigma)=-\\mu T\\,\\exp(-\\mu T).\n$$\nThus\n$$\n-\\mu T \\sigma=W_{0}\\!\\big(-\\mu T\\,\\exp(-\\mu T)\\big),\n$$\nwhere the principal branch $W_{0}$ yields the solution in $(0,1)$, so\n$$\n\\sigma=-\\frac{1}{\\mu T}\\,W_{0}\\!\\big(-\\mu T\\,\\exp(-\\mu T)\\big).\n$$\nSubstituting $\\mu T=\\frac{6}{5}$ gives\n$$\n\\sigma=-\\frac{5}{6}\\,W_{0}\\!\\Big(-\\frac{6}{5}\\,\\exp\\!\\big(-\\tfrac{6}{5}\\big)\\Big).\n$$\nNumerically, this evaluates to $\\sigma\\approx 0.6863$. Among the provided options, this is closest to $0.691$, i.e., option B.\n\nTherefore, the probability that a newly arriving component finds the robotic arm busy (and must wait) is best represented by option B.", "answer": "$$\\boxed{B}$$", "id": "1338336"}, {"introduction": "Once the parameter $\\sigma$ is determined, it remarkably unlocks the entire stationary distribution of the queue length as seen by an arrival, which is geometric. This exercise [@problem_id:1338307] challenges you to take this foundational result as a given and use it to derive another key performance measure: the variance of the queue length. This practice will strengthen your skills in working with probability distributions and demonstrate how $\\sigma$ directly quantifies system variability.", "problem": "In a stable G/M/1 queuing system, customers arrive and are served by a single server. The service times are exponentially distributed. An important result for this system is that the number of customers an arriving customer finds in the system (including any customer currently being served) follows a specific probability distribution when the system is in steady state. Let this random variable be denoted by $N$. The probability mass function of $N$ is given by $P(N=k) = (1-\\sigma)\\sigma^k$ for $k=0, 1, 2, \\dots$, where $\\sigma$ is a constant between 0 and 1 that represents the unique root of the equation $z = A^*(\\mu(1-z))$ in the interval $(0, 1)$. Here, $A^*(s)$ is the Laplace-Stieltjes Transform (LST) of the inter-arrival time distribution and $1/\\mu$ is the mean service time. Given this information, derive a closed-form expression for the variance of the number of customers an arriving customer sees, $\\text{Var}(N)$. Your final answer should be expressed solely in terms of $\\sigma$.", "solution": "The problem asks for the variance of a random variable $N$ whose probability mass function (PMF) is given by $P(N=k) = (1-\\sigma)\\sigma^k$ for $k=0, 1, 2, \\dots$. This is a geometric distribution on the set of non-negative integers.\n\nThe variance of a random variable $N$ is defined as:\n$$ \\text{Var}(N) = E[N^2] - (E[N])^2 $$\nWe need to calculate the first moment (expected value) $E[N]$ and the second moment $E[N^2]$.\n\n**Step 1: Calculate the expected value $E[N]$**\nThe expected value is calculated by definition:\n$$ E[N] = \\sum_{k=0}^{\\infty} k P(N=k) = \\sum_{k=0}^{\\infty} k (1-\\sigma)\\sigma^k $$\nWe can factor out the constant $(1-\\sigma)$:\n$$ E[N] = (1-\\sigma) \\sum_{k=0}^{\\infty} k \\sigma^k $$\nThe $k=0$ term is zero, so the sum can start from $k=1$:\n$$ E[N] = (1-\\sigma) \\sum_{k=1}^{\\infty} k \\sigma^k $$\nTo evaluate this sum, we recall the formula for a geometric series for $|\\sigma|  1$:\n$$ \\sum_{k=0}^{\\infty} \\sigma^k = \\frac{1}{1-\\sigma} $$\nDifferentiating both sides with respect to $\\sigma$ gives:\n$$ \\frac{d}{d\\sigma} \\left( \\sum_{k=0}^{\\infty} \\sigma^k \\right) = \\sum_{k=1}^{\\infty} k \\sigma^{k-1} = \\frac{d}{d\\sigma} \\left( \\frac{1}{1-\\sigma} \\right) = \\frac{1}{(1-\\sigma)^2} $$\nMultiplying by $\\sigma$:\n$$ \\sum_{k=1}^{\\infty} k \\sigma^k = \\frac{\\sigma}{(1-\\sigma)^2} $$\nSubstituting this back into the expression for $E[N]$:\n$$ E[N] = (1-\\sigma) \\left( \\frac{\\sigma}{(1-\\sigma)^2} \\right) = \\frac{\\sigma}{1-\\sigma} $$\n\n**Step 2: Calculate the second moment $E[N^2]$**\nThe second moment is calculated by definition:\n$$ E[N^2] = \\sum_{k=0}^{\\infty} k^2 P(N=k) = (1-\\sigma) \\sum_{k=0}^{\\infty} k^2 \\sigma^k $$\nAgain, the sum can start from $k=1$:\n$$ E[N^2] = (1-\\sigma) \\sum_{k=1}^{\\infty} k^2 \\sigma^k $$\nTo evaluate this sum, we can use a similar differentiation trick. We start from the result of the first differentiation:\n$$ \\sum_{k=1}^{\\infty} k \\sigma^{k-1} = \\frac{1}{(1-\\sigma)^2} $$\nDifferentiating again with respect to $\\sigma$:\n$$ \\frac{d}{d\\sigma} \\left( \\sum_{k=1}^{\\infty} k \\sigma^{k-1} \\right) = \\sum_{k=2}^{\\infty} k(k-1)\\sigma^{k-2} = \\frac{d}{d\\sigma} \\left( \\frac{1}{(1-\\sigma)^2} \\right) = \\frac{2}{(1-\\sigma)^3} $$\nMultiplying by $\\sigma^2$:\n$$ \\sum_{k=2}^{\\infty} k(k-1)\\sigma^k = \\frac{2\\sigma^2}{(1-\\sigma)^3} $$\nWe can write $k^2 = k(k-1) + k$. Thus, the sum for $E[N^2]$ becomes:\n$$ \\sum_{k=1}^{\\infty} k^2 \\sigma^k = \\sum_{k=1}^{\\infty} (k(k-1) + k) \\sigma^k = \\sum_{k=1}^{\\infty} k(k-1)\\sigma^k + \\sum_{k=1}^{\\infty} k\\sigma^k $$\nThe term for $k=1$ in the first sum is zero, so its index can start from $k=2$. We have already found both of these sums:\n$$ \\sum_{k=1}^{\\infty} k^2 \\sigma^k = \\frac{2\\sigma^2}{(1-\\sigma)^3} + \\frac{\\sigma}{(1-\\sigma)^2} $$\nNow we substitute this back into the expression for $E[N^2]$:\n$$ E[N^2] = (1-\\sigma) \\left( \\frac{2\\sigma^2}{(1-\\sigma)^3} + \\frac{\\sigma}{(1-\\sigma)^2} \\right) $$\n$$ E[N^2] = \\frac{2\\sigma^2}{(1-\\sigma)^2} + \\frac{\\sigma}{1-\\sigma} $$\nTo combine these terms, we find a common denominator:\n$$ E[N^2] = \\frac{2\\sigma^2}{(1-\\sigma)^2} + \\frac{\\sigma(1-\\sigma)}{(1-\\sigma)^2} = \\frac{2\\sigma^2 + \\sigma - \\sigma^2}{(1-\\sigma)^2} = \\frac{\\sigma^2 + \\sigma}{(1-\\sigma)^2} $$\n\n**Step 3: Calculate the variance $\\text{Var}(N)$**\nNow we use the variance formula with our calculated moments:\n$$ \\text{Var}(N) = E[N^2] - (E[N])^2 $$\n$$ \\text{Var}(N) = \\frac{\\sigma^2 + \\sigma}{(1-\\sigma)^2} - \\left( \\frac{\\sigma}{1-\\sigma} \\right)^2 $$\n$$ \\text{Var}(N) = \\frac{\\sigma^2 + \\sigma}{(1-\\sigma)^2} - \\frac{\\sigma^2}{(1-\\sigma)^2} $$\n$$ \\text{Var}(N) = \\frac{(\\sigma^2 + \\sigma) - \\sigma^2}{(1-\\sigma)^2} = \\frac{\\sigma}{(1-\\sigma)^2} $$\nThis is the final expression for the variance of $N$ in terms of $\\sigma$.", "answer": "$$\\boxed{\\frac{\\sigma}{(1-\\sigma)^{2}}}$$", "id": "1338307"}, {"introduction": "The parameter $\\sigma$ not only describes static, long-run averages but also governs the dynamic, moment-to-moment behavior of the queue. A fascinating aspect of the G/M/1 model is the memory, or dependence, between the experiences of consecutive customers. This final practice [@problem_id:1338311] guides you to quantify this relationship by deriving the correlation between successive waiting times, revealing an elegant and insightful connection back to $\\sigma$.", "problem": "A specialized data processing center operates as a single-server queue. Data packets arrive with interarrival times drawn from a general probability distribution with a finite mean. The processing time for each packet is exponentially distributed with a rate parameter $\\mu$. This system is modeled as a G/M/1 queue.\n\nFor a stable G/M/1 queue, the stationary probability that an arriving packet finds the system busy (and therefore must wait in the queue) is given by a parameter $\\sigma \\in (0, 1)$. This parameter is the unique root within the interval $(0, 1)$ of the equation $\\sigma = A^*(\\mu(1-\\sigma))$, where $A^*(s)$ is the Laplace-Stieltjes transform of the interarrival time distribution.\n\nThe stationary waiting time in the queue, denoted by the random variable $W$, has a mixed distribution: there is a probability mass at $W=0$, and for $W  0$, the distribution is continuous. Specifically, $P(W=0) = 1-\\sigma$, and conditional on having to wait, the waiting time follows an exponential distribution with rate $\\mu(1-\\sigma)$.\n\nA key property of the waiting time process $\\{W_n\\}$ for consecutive packets $n$ and $n+1$ in this specific system is that the conditional expectation of the next waiting time given the current one is piecewise linear. For a customer who experiences a waiting time $W_n = w  0$, the expected waiting time for the next customer is given by:\n$$E[W_{n+1} | W_n = w] = \\sigma^2 w + E[W_{n+1} | W_n = 0]$$\nThis relation holds for any $w  0$.\n\nAssuming the queue is in its stationary state, derive a symbolic expression for the correlation coefficient, $\\text{Corr}(W_n, W_{n+1})$, between the waiting times of two consecutive packets. Express your final answer in terms of $\\sigma$.", "solution": "Let $W$ denote $W_{n}$ and $W'$ denote $W_{n+1}$ in the stationary regime. By the given property, for $w0$,\n$$\n\\mathbb{E}[W' \\mid W=w]=\\sigma^{2} w+\\mathbb{E}[W' \\mid W=0].\n$$\nDefine $a \\equiv \\mathbb{E}[W' \\mid W=0]$ and $b \\equiv \\sigma^{2}$. Since the right-hand side equals $a$ when $w=0$, the same affine relation holds at $w=0$, hence\n$$\n\\mathbb{E}[W' \\mid W]=a+bW \\quad \\text{almost surely}.\n$$\n\nUse the law of total covariance:\n$$\n\\operatorname{Cov}(W,W')=\\operatorname{Cov}\\bigl(W,\\mathbb{E}[W' \\mid W]\\bigr).\n$$\nSubstituting the affine form of the regression,\n$$\n\\operatorname{Cov}(W,W')=\\operatorname{Cov}(W,a+bW)=b\\,\\operatorname{Var}(W)=\\sigma^{2}\\operatorname{Var}(W).\n$$\n\nBy stationarity, $\\operatorname{Var}(W')=\\operatorname{Var}(W)$. Therefore, the correlation coefficient is\n$$\n\\operatorname{Corr}(W,W')=\\frac{\\operatorname{Cov}(W,W')}{\\sqrt{\\operatorname{Var}(W)\\operatorname{Var}(W')}}=\\frac{\\sigma^{2}\\operatorname{Var}(W)}{\\operatorname{Var}(W)}=\\sigma^{2}.\n$$\nThus,\n$$\n\\operatorname{Corr}(W_{n},W_{n+1})=\\sigma^{2}.\n$$", "answer": "$$\\boxed{\\sigma^{2}}$$", "id": "1338311"}]}