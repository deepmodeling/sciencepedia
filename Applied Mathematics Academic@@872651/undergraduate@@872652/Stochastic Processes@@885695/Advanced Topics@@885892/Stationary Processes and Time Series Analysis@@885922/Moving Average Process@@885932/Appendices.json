{"hands_on_practices": [{"introduction": "To begin understanding any time series model, we first need to characterize its fundamental properties. This exercise guides you through calculating the variance of a simple Moving Average process of order 1, or MA(1). By working through this calculation, you will see directly how the variance of the underlying white noise, $\\sigma^2$, and the model's structure combine to determine the overall variability of the observed process.", "problem": "In the analysis of time series data, a fundamental model is the Moving Average (MA) process. Consider a specific type of MA process of order 1, known as an MA(1) process, which is often used to model signals after they have been processed by a simple filter.\n\nLet a sequence of random variables $\\{\\varepsilon_t\\}$ represent a white noise process. This means that for any integer time index $t$, the random variables have a mean of zero ($E[\\varepsilon_t] = 0$) and a constant variance, $\\text{Var}(\\varepsilon_t) = \\sigma^2$. Furthermore, the random variables are uncorrelated over time, meaning the covariance between any two different elements is zero ($\\text{Cov}(\\varepsilon_t, \\varepsilon_s) = 0$ for $t \\neq s$).\n\nA new time series, $\\{X_t\\}$, is generated by applying a first-difference filter to the white noise process. The resulting process is defined by the equation:\n$$X_t = \\varepsilon_t - \\varepsilon_{t-1}$$\nGiven that the variance of the white noise process is $\\sigma^2 = 4$, calculate the variance of the process $X_t$.", "solution": "We are given a white noise process $\\{\\varepsilon_{t}\\}$ with $E[\\varepsilon_{t}] = 0$, $\\text{Var}(\\varepsilon_{t}) = \\sigma^{2}$, and $\\text{Cov}(\\varepsilon_{t}, \\varepsilon_{s}) = 0$ for $t \\neq s$. The process $X_{t}$ is defined by $X_{t} = \\varepsilon_{t} - \\varepsilon_{t-1}$.\n\nFirst, compute the mean of $X_{t}$ using linearity of expectation:\n$$\nE[X_{t}] = E[\\varepsilon_{t}] - E[\\varepsilon_{t-1}] = 0 - 0 = 0.\n$$\n\nNext, use the variance formula for a linear combination of random variables:\n$$\n\\text{Var}(aY + bZ) = a^{2}\\text{Var}(Y) + b^{2}\\text{Var}(Z) + 2ab\\,\\text{Cov}(Y,Z).\n$$\nSet $a = 1$, $b = -1$, $Y = \\varepsilon_{t}$, and $Z = \\varepsilon_{t-1}$ to obtain\n$$\n\\text{Var}(X_{t}) = \\text{Var}(\\varepsilon_{t} - \\varepsilon_{t-1}) = \\text{Var}(\\varepsilon_{t}) + \\text{Var}(\\varepsilon_{t-1}) - 2\\,\\text{Cov}(\\varepsilon_{t}, \\varepsilon_{t-1}).\n$$\nUsing stationarity of the white noise variance and the lack of temporal correlation,\n$$\n\\text{Var}(\\varepsilon_{t}) = \\text{Var}(\\varepsilon_{t-1}) = \\sigma^{2}, \\quad \\text{Cov}(\\varepsilon_{t}, \\varepsilon_{t-1}) = 0,\n$$\nso\n$$\n\\text{Var}(X_{t}) = \\sigma^{2} + \\sigma^{2} - 0 = 2\\sigma^{2}.\n$$\nWith the given $\\sigma^{2} = 4$, we conclude\n$$\n\\text{Var}(X_{t}) = 2 \\times 4 = 8.\n$$", "answer": "$$\\boxed{8}$$", "id": "1320203"}, {"introduction": "Beyond the variance, the key feature of a time series is how its values are related across different points in time. This practice moves from the MA(1) to a more complex MA(2) model to compute the autocovariance at lag 1, a measure of short-term memory. This calculation is crucial as it reveals the theoretical basis for the 'cutoff' property of the Moving Average process's autocorrelation function, which is essential for model identification.", "problem": "In a semiconductor manufacturing plant, the deviation from the target thickness of a deposited film is monitored over time. This deviation, $X_t$, is modeled by a stationary Moving Average (MA) process of order 2, also known as an MA(2) process. The model is given by the equation:\n$$X_t = \\varepsilon_t + \\theta_1\\varepsilon_{t-1} + \\theta_2\\varepsilon_{t-2}$$\nHere, $\\{\\varepsilon_t\\}$ is a white noise process, which means that the random variables $\\varepsilon_t$ are uncorrelated, each with a mean of $E[\\varepsilon_t] = 0$ and a constant variance of $E[\\varepsilon_t^2] = \\sigma^2$.\nFor a particular production line, the model parameters have been estimated as $\\theta_1 = -0.7$ and $\\theta_2 = 0.5$. The variance of the white noise is $\\sigma^2 = 2.25$.\n\nCalculate the autocovariance of the process at lag 1, denoted as $\\gamma(1)$. Round your final answer to three significant figures.", "solution": "The MA(2) process can be written as $X_{t}=\\sum_{j=0}^{2}\\theta_{j}\\varepsilon_{t-j}$ with $\\theta_{0}=1$, $\\theta_{1}=-0.7$, and $\\theta_{2}=0.5$. The white noise satisfies $E[\\varepsilon_{t}]=0$, $E[\\varepsilon_{t}^{2}]=\\sigma^{2}$, and $E[\\varepsilon_{t}\\varepsilon_{s}]=0$ for $t\\neq s$.\n\nThe autocovariance at lag $h$ is\n$$\n\\gamma(h)=\\operatorname{Cov}(X_{t},X_{t-h})=E[X_{t}X_{t-h}]\n=\\sum_{j=0}^{2}\\sum_{k=0}^{2}\\theta_{j}\\theta_{k}E[\\varepsilon_{t-j}\\varepsilon_{t-h-k}].\n$$\nUsing $E[\\varepsilon_{t-j}\\varepsilon_{t-h-k}]=\\sigma^{2}$ if $t-j=t-h-k$ (i.e., $j=h+k$) and $0$ otherwise, for $h=1$ the nonzero index pairs are $(j,k)=(1,0)$ and $(2,1)$. Hence\n$$\n\\gamma(1)=\\sigma^{2}\\left(\\theta_{1}\\theta_{0}+\\theta_{2}\\theta_{1}\\right)=\\sigma^{2}\\left(\\theta_{1}+\\theta_{1}\\theta_{2}\\right).\n$$\nSubstituting $\\theta_{1}=-0.7$, $\\theta_{2}=0.5$, and $\\sigma^{2}=2.25$,\n$$\n\\gamma(1)=2.25\\left(-0.7+(-0.7)\\cdot 0.5\\right)=2.25\\left(-0.7-0.35\\right)=2.25(-1.05)=-2.3625.\n$$\nRounding to three significant figures gives $-2.36$.", "answer": "$$\\boxed{-2.36}$$", "id": "1320236"}, {"introduction": "Having explored the theoretical properties of variance [@problem_id:1320203] and autocovariance [@problem_id:1320236], we now apply this knowledge to a practical scenario. This problem simulates a real-world task: identifying the order $q$ of an MA($q$) process by examining its sample autocorrelation function (ACF). You will learn to use a statistical rule of thumb to distinguish significant correlations from random noise, a fundamental skill in time series modeling.", "problem": "An electrical engineer is analyzing the noise component of a signal measured by a novel sensor. The engineer hypothesizes that the noise, after being centered by subtracting its mean, can be described by a stationary Moving Average (MA) process. To identify the order of this process, a time series of $n=225$ observations is collected. The sample autocorrelation function (ACF), denoted by $\\hat{\\rho}(k)$ for lag $k$, is computed from this data. The first few values are found to be:\n\n$\\hat{\\rho}(1) = 0.52$\n$\\hat{\\rho}(2) = -0.31$\n$\\hat{\\rho}(3) = 0.08$\n$\\hat{\\rho}(4) = -0.05$\n\nA standard method for identifying the order $q$ of an MA($q$) process is to find the lag after which the sample autocorrelations are no longer statistically significant. A common rule of thumb is to consider a sample autocorrelation $\\hat{\\rho}(k)$ to be statistically insignificant if its absolute value is less than $2/\\sqrt{n}$. Based on this information and statistical criterion, what is the most appropriate order $q$ for the MA model of the sensor noise?\n\nA. $q=0$\n\nB. $q=1$\n\nC. $q=2$\n\nD. $q=3$\n\nE. $q=4$", "solution": "We use the MA model identification rule that the theoretical autocorrelation function (ACF) of an MA($q$) process is exactly zero for all lags $kq$. In finite samples, we judge sample autocorrelations $\\hat{\\rho}(k)$ to be statistically insignificant if $|\\hat{\\rho}(k)|\\frac{2}{\\sqrt{n}}$, as given.\n\nGiven $n=225$, the significance threshold is\n$$\n\\frac{2}{\\sqrt{n}}=\\frac{2}{\\sqrt{225}}=\\frac{2}{15}.\n$$\nWe compare $|\\hat{\\rho}(k)|$ to $\\frac{2}{15}$ using exact rational forms:\n- For $k=1$: $|\\hat{\\rho}(1)|=0.52=\\frac{52}{100}=\\frac{13}{25}$. Compare $\\frac{13}{25}$ to $\\frac{2}{15}$:\n$$\n\\frac{13}{25}\\frac{2}{15}\\quad\\text{since}\\quad 13\\cdot 15=1952\\cdot 25=50.\n$$\nThus, lag $1$ is significant.\n\n- For $k=2$: $|\\hat{\\rho}(2)|=0.31=\\frac{31}{100}$. Compare $\\frac{31}{100}$ to $\\frac{2}{15}$:\n$$\n\\frac{31}{100}\\frac{2}{15}\\quad\\text{since}\\quad 31\\cdot 15=4652\\cdot 100=200.\n$$\nThus, lag $2$ is significant.\n\n- For $k=3$: $|\\hat{\\rho}(3)|=0.08=\\frac{8}{100}=\\frac{2}{25}$. Compare $\\frac{2}{25}$ to $\\frac{2}{15}$:\n$$\n\\frac{2}{25}\\frac{2}{15}\\quad\\text{since}\\quad 2\\cdot 15=302\\cdot 25=50.\n$$\nThus, lag $3$ is insignificant.\n\n- For $k=4$: $|\\hat{\\rho}(4)|=0.05=\\frac{5}{100}=\\frac{1}{20}$. Compare $\\frac{1}{20}$ to $\\frac{2}{15}$:\n$$\n\\frac{1}{20}\\frac{2}{15}\\quad\\text{since}\\quad 1\\cdot 15=152\\cdot 20=40.\n$$\nThus, lag $4$ is insignificant.\n\nTherefore, the sample ACF is significant at lags $1$ and $2$ and becomes insignificant from lag $3$ onward, indicating a cutoff after lag $2$. This pattern is characteristic of an MA($q$) process with $q=2$. Hence, the most appropriate order is $q=2$, corresponding to option C.", "answer": "$$\\boxed{C}$$", "id": "1320202"}]}