{"hands_on_practices": [{"introduction": "The concept of stationarity is fundamental to time series analysis, providing a crucial assumption for many models. A process is considered Wide-Sense Stationary (WSS) if its statistical properties, specifically its mean and autocovariance, do not change over time. This first practice [@problem_id:1311036] is a direct test of this definition, focusing on the most basic requirement: a constant mean. By analyzing a process with a time-varying mean, you'll see firsthand why this condition is a non-negotiable starting point for establishing stationarity.", "problem": "A discrete-time stochastic process is defined by $X_t$ for all integers $t$. The process is characterized by its mean function and its autocovariance function. The mean function is given by $\\mu_t = E[X_t] = A \\sin(\\frac{\\pi t}{2})$, where $A$ is a non-zero real constant. The autocovariance function is given by $C_X(t_1, t_2) = \\text{Cov}(X_{t_1}, X_{t_2}) = K \\alpha^{|t_1-t_2|}$, where $K$ is a positive constant and $\\alpha$ is a constant such that $0  |\\alpha|  1$.\n\nA process is defined as Wide-Sense Stationary (WSS) if it satisfies two conditions:\n1. Its mean function, $E[X_t]$, is a constant for all $t$.\n2. Its autocovariance function, $C_X(t_1, t_2)$, depends only on the time lag $\\tau = t_1 - t_2$.\n\nBased on this definition, which of the following statements about the process $X_t$ is true?\n\nA. The process $X_t$ is WSS because its mean function is periodic.\n\nB. The process $X_t$ is not WSS because its autocovariance function does not depend solely on the time lag $\\tau = t_1-t_2$.\n\nC. The process $X_t$ is WSS because its autocovariance function depends solely on the time lag $\\tau = t_1-t_2$.\n\nD. The process $X_t$ is not WSS because its mean function is not constant.\n\nE. Whether the process is WSS cannot be determined without knowing the specific value of $A$.", "solution": "A discrete-time process is wide-sense stationary (WSS) if and only if:\n1) The mean is constant in time: there exists a constant $m$ such that $E[X_t] = m$ for all integers $t$.\n2) The autocovariance depends only on the time lag $\\tau = t_1 - t_2$: there exists a function $R(\\tau)$ such that $C_X(t_1, t_2) = R(t_1 - t_2)$ for all integers $t_1, t_2$.\n\nGiven $E[X_t] = \\mu_t = A \\sin(\\frac{\\pi t}{2})$ with $A \\neq 0$, we check condition 1. Pick $t=0$ and $t=1$:\n$$\n\\mu_0 = A \\sin(0) = 0, \\quad \\mu_1 = A \\sin(\\frac{\\pi}{2}) = A.\n$$\nSince $A \\neq 0$, $\\mu_0 \\neq \\mu_1$, so the mean is not constant. Therefore, condition 1 fails.\n\nGiven $C_X(t_1, t_2) = K \\alpha^{|t_1 - t_2|}$ with $K>0$ and $0|\\alpha|1$, we observe that\n$$\nC_X(t_1, t_2) = K \\alpha^{|t_1 - t_2|} = R(\\tau) \\quad \\text{with} \\quad \\tau = t_1 - t_2,\n$$\nso condition 2 is satisfied since the autocovariance depends only on the lag.\n\nA process is WSS only if both conditions hold simultaneously. Here, condition 1 fails, hence the process is not WSS. Therefore, the correct statement is that the process is not WSS because its mean function is not constant.\n\nOption analysis:\n- A is false: periodic mean is not sufficient; it must be constant.\n- B is false: the autocovariance does depend only on the lag.\n- C is false: although the autocovariance is a function of lag, the mean is not constant.\n- D is true: the mean is not constant, so the process is not WSS.\n- E is false: $A \\neq 0$ is given, and the non-constancy does not depend on its specific value beyond being non-zero.", "answer": "$$\\boxed{D}$$", "id": "1311036"}, {"introduction": "Many stationary processes in fields like signal processing and economics are constructed by filtering a simple, unpredictable process called white noise. This exercise [@problem_id:1311045] delves into a common example, the Moving Average (MA) process, where the current value is a weighted sum of present and past noise terms. Calculating the autocovariance function will reveal a core feature of MA processes: their dependence on the past is limited to a finite window, a property often referred to as having a 'finite memory'.", "problem": "In digital signal processing, a common operation is to filter a noisy signal. Consider a simple discrete-time filter whose output, denoted by the stochastic process $\\{X_t\\}$, is a weighted average of the current and past two values of an input noise signal, $\\{Z_t\\}$. The input signal $\\{Z_t\\}$ is a white noise process, characterized by the following properties for any integer times $t$ and $s$:\n1. The expected value is zero: $E[Z_t] = 0$.\n2. The covariance is given by $E[Z_t Z_s] = \\sigma^2$ if $t=s$, and $E[Z_t Z_s] = 0$ if $t \\neq s$. Here, $\\sigma^2$ is a positive constant representing the variance of the noise.\n\nThe filter's output is defined by the relation:\n$$\nX_t = Z_t + \\frac{1}{2}Z_{t-1} + \\frac{1}{3}Z_{t-2}\n$$\n\nYour task is to characterize the time-correlation properties of the output signal. Specifically, calculate the values of the autocovariance function, $\\gamma_X(k) = \\text{Cov}(X_t, X_{t-k})$, for lags $k=0, 1, 2,$ and $3$.\n\nPresent your four answers in the form of a row matrix $(\\gamma_X(0), \\gamma_X(1), \\gamma_X(2), \\gamma_X(3))$. Your answers should be expressed in terms of $\\sigma^2$.", "solution": "We are given a white noise process $\\{Z_t\\}$ with $E[Z_t] = 0$ and $E[Z_t Z_s] = \\sigma^2$ if $t=s$, and $0$ otherwise. The output is the linear filter\n$$\nX_t = Z_t + \\frac{1}{2}Z_{t-1} + \\frac{1}{3}Z_{t-2}.\n$$\nSince $\\{Z_t\\}$ has zero mean and $X_t$ is a finite linear combination of $\\{Z_t\\}$, we have $E[X_t]=0$. The autocovariance at lag $k$ is\n$$\n\\gamma_X(k) = \\text{Cov}(X_t,X_{t-k}) = E[X_t X_{t-k}],\n$$\nbecause both $X_t$ and $X_{t-k}$ have mean zero. Using the white-noise orthogonality $E[Z_t Z_s] = 0$ for $t \\neq s$, only products involving the same $Z$ index contribute to the expectation.\n\nFor $k=0$, using $a=\\frac{1}{2}$ and $b=\\frac{1}{3}$,\n$$\n\\gamma_X(0) = E[(Z_t + a Z_{t-1} + b Z_{t-2})^2]\n= E[Z_t^2] + a^2 E[Z_{t-1}^2] + b^2 E[Z_{t-2}^2]\n= \\sigma^2(1 + \\frac{1}{4} + \\frac{1}{9})\n= \\frac{49}{36}\\sigma^2.\n$$\n\nFor $k=1$, write $X_{t-1} = Z_{t-1} + \\frac{1}{2}Z_{t-2} + \\frac{1}{3}Z_{t-3}$. Then\n$$\n\\gamma_X(1) = E[(Z_t + \\frac{1}{2}Z_{t-1} + \\frac{1}{3}Z_{t-2})(Z_{t-1} + \\frac{1}{2}Z_{t-2} + \\frac{1}{3}Z_{t-3})].\n$$\nAll cross terms with different time indices vanish. The nonzero contributions are\n$$\nE[\\frac{1}{2}Z_{t-1} \\cdot Z_{t-1}] + E[\\frac{1}{3}Z_{t-2} \\cdot \\frac{1}{2}Z_{t-2}]\n= \\frac{1}{2}\\sigma^2 + \\frac{1}{6}\\sigma^2\n= \\frac{2}{3}\\sigma^2.\n$$\n\nFor $k=2$, write $X_{t-2} = Z_{t-2} + \\frac{1}{2}Z_{t-3} + \\frac{1}{3}Z_{t-4}$. Then\n$$\n\\gamma_X(2) = E[(Z_t + \\frac{1}{2}Z_{t-1} + \\frac{1}{3}Z_{t-2})(Z_{t-2} + \\frac{1}{2}Z_{t-3} + \\frac{1}{3}Z_{t-4})].\n$$\nThe only overlapping index is $t-2$, giving\n$$\nE[\\frac{1}{3}Z_{t-2} \\cdot Z_{t-2}] = \\frac{1}{3}\\sigma^2.\n$$\n\nFor $k=3$, the index sets $\\{t, t-1, t-2\\}$ and $\\{t-3, t-4, t-5\\}$ do not overlap, so\n$$\n\\gamma_X(3) = 0.\n$$\n\nTherefore, the requested autocovariances are\n$$\n(\\gamma_X(0), \\gamma_X(1), \\gamma_X(2), \\gamma_X(3)) = (\\frac{49}{36}\\sigma^2, \\frac{2}{3}\\sigma^2, \\frac{1}{3}\\sigma^2, 0).\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{49}{36}\\sigma^2  \\frac{2}{3}\\sigma^2  \\frac{1}{3}\\sigma^2  0\\end{pmatrix}}$$", "id": "1311045"}, {"introduction": "In contrast to processes with finite memory, many real-world phenomena exhibit a dependence on their own past that gradually fades over time. The Autoregressive (AR) model captures this behavior by expressing the current value as a function of its previous values plus a noise term. This practice [@problem_id:1311073] challenges you to explore the dependence structure of a simple AR(1) process. By calculating its correlation coefficient, you will uncover the characteristic geometric decay of memory that defines this essential class of stationary processes.", "problem": "In the field of data center management, a simplified model is used to describe the daily temperature fluctuations. Let $X_t$ represent the deviation of the average temperature from its target set-point on day $t$. This fluctuation is modeled as a stationary Autoregressive (AR) process of order 1, given by the equation:\n$$X_t = \\phi X_{t-1} + Z_t$$\nHere, $\\phi$ is a constant parameter representing the system's \"thermal memory,\" and it satisfies the condition $|\\phi|  1$. The term $Z_t$ represents a white noise process due to random daily variations in server load and external conditions. The white noise process has a mean of zero ($E[Z_t] = 0$) and a constant variance of $Var(Z_t) = \\sigma_Z^2$. A key property of this model is that the noise term $Z_t$ is uncorrelated with all past temperature deviations, i.e., $Cov(X_s, Z_t) = 0$ for all $s  t$.\n\nAssuming the process $X_t$ is stationary, determine the theoretical correlation coefficient between the temperature deviation on a given day, $X_t$, and the deviation two days prior, $X_{t-2}$. Express your answer as a symbolic expression in terms of $\\phi$.", "solution": "We start from the AR(1) model $X_t = \\phi X_{t-1} + Z_t$ with $|\\phi|  1$, $E[Z_t]=0$, $Var(Z_t) = \\sigma_Z^2$, and $Z_t$ uncorrelated with all $X_s$ for $s  t$. Stationarity implies a constant mean $\\mu=E[X_t]$ satisfying $\\mu=\\phi\\mu+E[Z_t]$, hence $\\mu=0$.\n\nDefine the autocovariance function $\\gamma(k)=Cov(X_t, X_{t-k})=E[X_t X_{t-k}]$ for $k \\ge 0$ (using zero mean). Multiplying the model by $X_{t-1}$ and taking expectations yields\n$$\n\\gamma(1)=E[X_t X_{t-1}]=\\phi E[X_{t-1}^2]+E[Z_t X_{t-1}]=\\phi\\gamma(0),\n$$\nsince $Z_t$ is uncorrelated with $X_{t-1}$. More generally, for $k \\ge 1$,\n$$\n\\gamma(k)=E[X_t X_{t-k}]=\\phi E[X_{t-1}X_{t-k}]+E[Z_t X_{t-k}]=\\phi\\gamma(k-1),\n$$\nas $Z_t$ is uncorrelated with $X_{t-k}$ for $k \\ge 1$. By recursion,\n$$\n\\gamma(k)=\\phi^k\\gamma(0).\n$$\nThe autocorrelation function is $\\rho(k)=\\gamma(k)/\\gamma(0)$, so\n$$\n\\rho(k)=\\phi^k.\n$$\nTherefore, the correlation coefficient between $X_t$ and $X_{t-2}$ is $\\rho(2)=\\phi^2$. (For completeness, stationarity gives $\\gamma(0)=\\phi^2\\gamma(0)+\\sigma_Z^2$, hence $\\gamma(0)=\\sigma_Z^2/(1-\\phi^2)$, but this cancels in the correlation.)", "answer": "$$\\boxed{\\phi^2}$$", "id": "1311073"}]}