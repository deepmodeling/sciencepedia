{"hands_on_practices": [{"introduction": "A defining characteristic of Moving Average (MA) processes is their finite memory, meaning that the value of the process at a given time is only influenced by a finite number of recent random shocks. This practice allows you to explore this property directly by calculating the autocovariance, which quantifies the correlation between the process at different points in time. Understanding how to compute the autocovariance function is a fundamental skill for identifying the order of an MA model from observed data [@problem_id:1283025].", "problem": "In a simplified model for analyzing fluctuations in a communication channel, a discrete-time signal $\\{X_t\\}$ is generated by applying a digital filter to a white noise process $\\{Z_t\\}$. The process is described by the following equation:\n$$X_t = Z_t + 0.8 Z_{t-1} - 0.3 Z_{t-2}$$\nThe input $\\{Z_t\\}$ is a white noise process, meaning it consists of a sequence of independent and identically distributed random variables with a mean of $E[Z_t] = 0$ and a constant variance of $\\sigma_Z^2 = 4$ for all $t$.\n\nCalculate the value of the autocovariance of the signal $\\{X_t\\}$ at lag 2, which is defined as $\\gamma(2) = \\text{Cov}(X_t, X_{t-2})$.", "solution": "We write the finite impulse response representation $X_{t}=\\sum_{k=0}^{2}a_{k}Z_{t-k}$ with coefficients $a_{0}=1$, $a_{1}=0.8$, $a_{2}=-0.3$. Then $X_{t-2}=\\sum_{j=0}^{2}a_{j}Z_{t-2-j}$. Since $E[Z_{t}]=0$, we have $\\gamma(2)=\\text{Cov}(X_{t},X_{t-2})=E[X_{t}X_{t-2}]$.\n\nUsing bilinearity of expectation and independence of white noise, \n$$\n\\gamma(2)=E\\!\\left[\\left(\\sum_{k=0}^{2}a_{k}Z_{t-k}\\right)\\left(\\sum_{j=0}^{2}a_{j}Z_{t-2-j}\\right)\\right]\n=\\sum_{k=0}^{2}\\sum_{j=0}^{2}a_{k}a_{j}E[Z_{t-k}Z_{t-2-j}].\n$$\nFor white noise, $E[Z_{t-k}Z_{t-2-j}]=0$ unless $t-k=t-2-j$, i.e., $k=2+j$. With $k,j\\in\\{0,1,2\\}$, the only valid pair is $(k,j)=(2,0)$. Therefore,\n$$\n\\gamma(2)=a_{2}a_{0}E[Z_{t-2}^{2}]=a_{2}a_{0}\\sigma_{Z}^{2}.\n$$\nSubstituting $a_{2}=-0.3$, $a_{0}=1$, and $\\sigma_{Z}^{2}=4$ gives\n$$\n\\gamma(2)=(-0.3)(1)\\cdot 4=-1.2.\n$$", "answer": "$$\\boxed{-1.2}$$", "id": "1283025"}, {"introduction": "In contrast to the finite memory of MA models, Autoregressive (AR) processes exhibit a memory that, in theory, extends infinitely into the past, though its influence decays over time. This exercise demonstrates a simple and powerful relationship between the structure of an AR(1) model and its autocorrelation function. By relating the model's single parameter directly to its lag-1 autocorrelation, you will uncover a principle that forms the basis of the Yule-Walker equations, a cornerstone method for AR model estimation [@problem_id:1283011].", "problem": "A time series is believed to be adequately described by a stationary Autoregressive model of order 1, or AR(1). The model is given by the equation:\n$$X_t = \\phi X_{t-1} + Z_t$$\nwhere $X_t$ is the value of the series at time $t$, $\\phi$ is the autoregressive coefficient, and $Z_t$ is a white noise process with mean zero and constant variance $\\sigma_Z^2$. For the process to be stationary, it is required that $|\\phi|  1$.\n\nAn analyst examining the data computes the sample autocorrelation function. The lag-1 autocorrelation, denoted by $\\rho(1)$, is found to be 0.6. Based on this single piece of information, what is the value of the coefficient $\\phi$ for this AR(1) model?", "solution": "We start with the stationary AR(1) model\n$$\nX_{t}=\\phi X_{t-1}+Z_{t},\n$$\nwhere $\\{Z_{t}\\}$ is white noise with $\\mathbb{E}[Z_{t}]=0$ and $\\operatorname{Var}(Z_{t})=\\sigma_{Z}^{2}$, and stationarity requires $|\\phi|1$.\n\nDefine the autocovariance function $\\gamma(h)=\\operatorname{Cov}(X_{t},X_{t-h})$. Multiply the model by $X_{t-1}$ and take expectations:\n$$\n\\mathbb{E}[X_{t}X_{t-1}]=\\phi\\,\\mathbb{E}[X_{t-1}^{2}]+\\mathbb{E}[Z_{t}X_{t-1}].\n$$\nSince $Z_{t}$ is white noise and uncorrelated with past values, $\\mathbb{E}[Z_{t}X_{t-1}]=0$. Therefore,\n$$\n\\gamma(1)=\\phi\\,\\gamma(0).\n$$\nThe autocorrelation function is $\\rho(h)=\\gamma(h)/\\gamma(0)$, so for $h=1$,\n$$\n\\rho(1)=\\frac{\\gamma(1)}{\\gamma(0)}=\\phi.\n$$\nGiven the empirical information that $\\rho(1)=0.6$, the corresponding AR(1) coefficient consistent with the model is\n$$\n\\phi=\\rho(1)=0.6,\n$$\nwhich also satisfies the stationarity condition $|\\phi|1$.", "answer": "$$\\boxed{0.6}$$", "id": "1283011"}, {"introduction": "The distinction between AR and MA models can be elegantly bridged by understanding that one can often be represented as the other. This practice explores this duality by converting a stationary AR(2) process into its equivalent infinite Moving Average (MA($\\infty$)) form. This transformation is not just a theoretical curiosity; it is crucial for deriving forecasts and gaining deeper insight into the long-term behavior of autoregressive systems [@problem_id:1283037].", "problem": "Consider a stationary Autoregressive (AR) process of order 2, denoted AR(2), defined by the equation:\n$$X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + Z_t$$\nwhere $X_t$ is the value of the time series at time $t$, and $Z_t$ is a white noise process with mean zero and constant variance. For a specific process, the parameters are given as $\\phi_1 = 0.5$ and $\\phi_2 = 0.2$.\n\nThis stationary AR(2) process can be expressed as an infinite Moving Average (MA) process, denoted MA($\\infty$), of the form:\n$$X_t = \\sum_{j=0}^{\\infty} \\psi_j Z_{t-j}$$\nwhere the coefficients $\\psi_j$ are constants with the convention that $\\psi_0 = 1$.\n\nDetermine the exact values of the coefficients $\\psi_1$ and $\\psi_2$.", "solution": "An AR(2) process $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + Z_t$ has an MA($\\infty$) representation $X_t = \\sum_{j=0}^{\\infty} \\psi_j Z_{t-j}$ with $\\psi_0 = 1$, where the coefficients satisfy the recursion derived from the operator identity $(1 - \\phi_1 B - \\phi_2 B^{2}) \\sum_{j=0}^{\\infty} \\psi_j B^{j} = 1$. Equating coefficients gives, for $j \\geq 1$,\n$$\n\\psi_j = \\phi_1 \\psi_{j-1} + \\phi_2 \\psi_{j-2},\n$$\nwith boundary conditions $\\psi_0 = 1$ and $\\psi_{-1} = 0$.\n\nWith $\\phi_1 = \\frac{1}{2}$ and $\\phi_2 = \\frac{1}{5}$:\n- For $j=1$,\n$$\n\\psi_1 = \\phi_1 \\psi_0 + \\phi_2 \\psi_{-1} = \\frac{1}{2}\\cdot 1 + \\frac{1}{5}\\cdot 0 = \\frac{1}{2}.\n$$\n- For $j=2$,\n$$\n\\psi_2 = \\phi_1 \\psi_1 + \\phi_2 \\psi_0 = \\frac{1}{2}\\cdot \\frac{1}{2} + \\frac{1}{5}\\cdot 1 = \\frac{1}{4} + \\frac{1}{5} = \\frac{9}{20}.\n$$\n\nThus, $\\psi_1 = \\frac{1}{2}$ and $\\psi_2 = \\frac{9}{20}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  \\frac{9}{20}\\end{pmatrix}}$$", "id": "1283037"}]}