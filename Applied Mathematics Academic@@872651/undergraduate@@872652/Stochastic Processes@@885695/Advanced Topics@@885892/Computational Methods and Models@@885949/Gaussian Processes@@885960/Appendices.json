{"hands_on_practices": [{"introduction": "To truly understand what a Gaussian Process (GP) is, it helps to build one from the ground up. Instead of starting with an abstract definition, this exercise [@problem_id:1304141] shows how a GP can emerge from simpler, more familiar components—in this case, independent Gaussian random variables representing a random offset and drift. By deriving the mean and covariance functions from this generative model, you will gain a concrete intuition for how these two functions fully define the behavior of the entire process.", "problem": "A simple model for a sensor's reading with a linear drift over time $t$ is described by the stochastic process $X_t = At + B$. The term $B$ represents a random initial offset, and $A$ represents a random drift rate. Assume that $A$ and $B$ are independent random variables, each following a standard normal distribution, i.e., a normal distribution with a mean of 0 and a variance of 1. For any two distinct time points $t_1$ and $t_2$, the pair of readings $(X_{t_1}, X_{t_2})$ forms a random vector. Determine the covariance matrix of this random vector.", "solution": "Let the random vector be $\\mathbf{X} = (X_{t_1}, X_{t_2})$. The covariance matrix $\\boldsymbol{\\Sigma}$ of this vector is a $2 \\times 2$ matrix whose elements $\\boldsymbol{\\Sigma}_{ij}$ are given by the covariance between the $i$-th and $j$-th components of the vector.\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n\\text{Cov}(X_{t_1}, X_{t_1}) & \\text{Cov}(X_{t_1}, X_{t_2}) \\\\\n\\text{Cov}(X_{t_2}, X_{t_1}) & \\text{Cov}(X_{t_2}, X_{t_2})\n\\end{pmatrix} = \\begin{pmatrix}\n\\text{Var}(X_{t_1}) & \\text{Cov}(X_{t_1}, X_{t_2}) \\\\\n\\text{Cov}(X_{t_1}, X_{t_2}) & \\text{Var}(X_{t_2})\n\\end{pmatrix}\n$$\nThe definition of covariance between two random variables $U$ and $V$ is $\\text{Cov}(U, V) = E[UV] - E[U]E[V]$.\n\nFirst, we need to find the expected value (mean) of the process $X_t$. Using the linearity of expectation:\n$$\nE[X_t] = E[At + B] = E[A]t + E[B]\n$$\nThe problem states that $A$ and $B$ are standard normal random variables, $A \\sim N(0, 1)$ and $B \\sim N(0, 1)$. Thus, their expected values are $E[A] = 0$ and $E[B] = 0$.\n$$\nE[X_t] = (0)t + 0 = 0\n$$\nThe process has a mean of zero for all $t$.\n\nNow we can compute the general covariance function $\\text{Cov}(X_s, X_t)$ for any two time points $s$ and $t$.\n$$\n\\text{Cov}(X_s, X_t) = E[X_s X_t] - E[X_s]E[X_t]\n$$\nSince $E[X_s] = 0$ and $E[X_t] = 0$, the covariance simplifies to:\n$$\n\\text{Cov}(X_s, X_t) = E[X_s X_t]\n$$\nLet's compute this expectation:\n$$\nE[X_s X_t] = E[(As + B)(At + B)] = E[A^2 st + ABs + ABt + B^2]\n$$\nBy linearity of expectation, this becomes:\n$$\nE[X_s X_t] = st E[A^2] + (s+t)E[AB] + E[B^2]\n$$\nWe need to find $E[A^2]$, $E[B^2]$, and $E[AB]$.\nFor any random variable $Z$, its variance is given by $\\text{Var}(Z) = E[Z^2] - (E[Z])^2$. Since $A$ and $B$ are standard normal, $\\text{Var}(A) = 1$, $E[A] = 0$, $\\text{Var}(B) = 1$, and $E[B] = 0$.\nFor $A$:\n$1 = E[A^2] - (0)^2 \\implies E[A^2] = 1$.\nFor $B$:\n$1 = E[B^2] - (0)^2 \\implies E[B^2] = 1$.\nThe problem also states that $A$ and $B$ are independent. Therefore, the expectation of their product is the product of their expectations:\n$E[AB] = E[A]E[B] = (0)(0) = 0$.\n\nSubstituting these values back into the expression for the covariance:\n$$\n\\text{Cov}(X_s, X_t) = st(1) + (s+t)(0) + 1 = st + 1\n$$\nThis is the covariance function for the process $X_t$.\n\nNow we can find the specific elements of the covariance matrix for the vector $(X_{t_1}, X_{t_2})$.\nThe diagonal elements are the variances:\n$\\boldsymbol{\\Sigma}_{11} = \\text{Var}(X_{t_1}) = \\text{Cov}(X_{t_1}, X_{t_1}) = t_1 t_1 + 1 = t_1^2 + 1$.\n$\\boldsymbol{\\Sigma}_{22} = \\text{Var}(X_{t_2}) = \\text{Cov}(X_{t_2}, X_{t_2}) = t_2 t_2 + 1 = t_2^2 + 1$.\n\nThe off-diagonal elements are the covariances:\n$\\boldsymbol{\\Sigma}_{12} = \\boldsymbol{\\Sigma}_{21} = \\text{Cov}(X_{t_1}, X_{t_2}) = t_1 t_2 + 1$.\n\nAssembling these elements into the covariance matrix gives:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix}\nt_1^2 + 1 & t_1 t_2 + 1 \\\\\nt_1 t_2 + 1 & t_2^2 + 1\n\\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nt_1^{2} + 1 & t_1 t_2 + 1 \\\\\nt_1 t_2 + 1 & t_2^{2} + 1\n\\end{pmatrix}\n}\n$$", "id": "1304141"}, {"introduction": "The covariance function, or kernel, is the heart of a Gaussian Process, encoding our assumptions about the function we are modeling, such as its smoothness. In practice, we often choose from a family of well-studied kernels. This exercise [@problem_id:759044] invites you to work with the Matérn kernel, a versatile and widely used choice, to construct the prior covariance matrix for a discrete set of points. This matrix is a tangible representation of the GP's prior beliefs before any data is observed.", "problem": "A Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP is fully specified by its mean function $m(x)$ and covariance function (or kernel) $k(x, x')$. We consider a one-dimensional GP $f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$ with a zero mean function, $m(x)=0$.\n\nThe covariance is described by the Matérn family of kernels, which have a parameter $\\nu > 0$ that controls the smoothness of the resulting function. For the special case of $\\nu=1/2$, the Matérn kernel simplifies to the exponential kernel:\n$$\nk(x_i, x_j) = \\sigma^2 \\exp\\left(-\\frac{|x_i - x_j|}{\\ell}\\right)\n$$\nHere, $|x_i - x_j|$ is the Euclidean distance between points $x_i$ and $x_j$, $\\ell > 0$ is the length-scale parameter, and $\\sigma^2 > 0$ is the signal variance.\n\nConsider a set of three input points $X = \\{x_1, x_2, x_3\\} = \\{0, 1, 2\\}$. The prior distribution of the function values at these points, $\\mathbf{f} = [f(x_1), f(x_2), f(x_3)]^T$, is a multivariate Gaussian with zero mean and a $3 \\times 3$ covariance matrix $K$. The elements of this matrix are given by $K_{ij} = k(x_i, x_j)$.\n\nDerive the determinant of the prior covariance matrix $K$ for the given input points. Express your answer in terms of the parameters $\\sigma$ and $\\ell$.", "solution": "We have points $x_1=0,x_2=1,x_3=2$ and kernel \n$$k(x_i,x_j)=\\sigma^2\\exp\\!\\bigl(-|x_i-x_j|/\\ell\\bigr)\\,. $$ \nDefine \n$$a=\\exp\\!\\bigl(-1/\\ell\\bigr),\\quad a^2=\\exp\\!\\bigl(-2/\\ell\\bigr)\\,. $$\nThen the covariance matrix is\n$$K=\\sigma^2\n\\begin{pmatrix}\n1 & a & a^2\\\\\na & 1 & a\\\\\na^2 & a & 1\n\\end{pmatrix}\n=\\sigma^2A\\,. $$\nThus\n$$\\det K=\\sigma^6\\det A\\,. $$\nCompute $\\det A$ by expansion along the first row:\n\n$$\n\\det A\n=1\\cdot\\det\\begin{pmatrix}1&a\\\\a&1\\end{pmatrix}\n-a\\cdot\\det\\begin{pmatrix}a&a\\\\a^2&1\\end{pmatrix}\n+a^2\\cdot\\det\\begin{pmatrix}a&1\\\\a^2&a\\end{pmatrix}.\n$$\n\nEvaluate each minor:\n\n$$\n\\det\\begin{pmatrix}1&a\\\\a&1\\end{pmatrix}=1-a^2,\n\\quad\n\\det\\begin{pmatrix}a&a\\\\a^2&1\\end{pmatrix}=a - a^3,\n\\quad\n\\det\\begin{pmatrix}a&1\\\\a^2&a\\end{pmatrix}=a^2-a^2=0.\n$$\n\nHence\n\n$$\n\\det A=(1-a^2)-a(a-a^3)+a^2\\cdot0\n=1-a^2-a^2+a^4\n=(1-a^2)^2.\n$$\n\nSubstitute back:\n\n$$\n\\det K=\\sigma^6(1-a^2)^2\n=\\sigma^6\\bigl(1-e^{-2/\\ell}\\bigr)^2.\n$$", "answer": "$$\\boxed{\\sigma^6\\bigl(1-\\exp\\bigl(-\\tfrac{2}{\\ell}\\bigr)\\bigr)^2}$$", "id": "759044"}, {"introduction": "The real power of Gaussian Processes lies in updating our beliefs in light of new data—the core principle of Bayesian inference. After defining a GP and constructing its prior covariance, the next step is to see how observations influence our predictions. This problem [@problem_id:1304181] focuses on this crucial step by asking you to calculate the conditional variance. You will see firsthand how observing the value of the process at one point reduces the uncertainty about its value at another, which is the fundamental mechanism behind GP regression.", "problem": "A stochastic model is described by a zero-mean Gaussian Process (GP). A GP is a collection of random variables, any finite number of which have a joint Gaussian (normal) distribution. Consider two random variables, $X_1$ and $X_2$, drawn from this process. The random vector $\\mathbf{X} = \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}$ follows a multivariate normal distribution with a mean vector of $\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the covariance matrix:\n$$\n\\Sigma = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nSuppose the value of the random variable $X_1$ is observed to be $x_1$. What is the conditional variance of $X_2$ given this observation, denoted as $\\text{Var}(X_2 | X_1 = x_1)$? Express your answer as an exact value.", "solution": "We model $\\mathbf{X} = \\begin{pmatrix} X_{1} \\\\ X_{2} \\end{pmatrix}$ as jointly Gaussian with mean $\\mathbf{0}$ and covariance matrix\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_{22} \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix}.\n$$\nFor a bivariate normal, the conditional distribution of $X_{2}$ given $X_{1}=x_{1}$ is Gaussian with variance given by the Schur complement:\n$$\n\\operatorname{Var}(X_{2}\\mid X_{1}=x_{1}) = \\sigma_{22} - \\sigma_{21}\\sigma_{11}^{-1}\\sigma_{12}.\n$$\nSubstitute the entries from $\\Sigma$: $\\sigma_{11}=4$, $\\sigma_{22}=2$, and $\\sigma_{12}=\\sigma_{21}=1$. Since these are scalars, $\\sigma_{11}^{-1}=\\frac{1}{4}$, hence\n$$\n\\operatorname{Var}(X_{2}\\mid X_{1}=x_{1}) = 2 - 1 \\cdot \\frac{1}{4} \\cdot 1 = 2 - \\frac{1}{4} = \\frac{7}{4}.\n$$\nThis conditional variance does not depend on $x_{1}$.", "answer": "$$\\boxed{\\frac{7}{4}}$$", "id": "1304181"}]}