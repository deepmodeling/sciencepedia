{"hands_on_practices": [{"introduction": "The first step in implementing any Gibbs sampler is to correctly derive the full conditional distributions for each variable. This practice provides a foundational exercise in this crucial skill by considering a simple, non-rectangular target distribution. By working through the geometry of sampling from a uniform distribution on a triangle, you will solidify your understanding of how the support of the joint distribution dictates the form of the conditionals. [@problem_id:1338725]", "problem": "A statistician wants to generate random points $(X, Y)$ that are uniformly distributed within a specific two-dimensional region. This region is a triangle in the Cartesian plane defined by the vertices $(0,0)$, $(1,0)$, and $(0,1)$. The joint probability density function (PDF) for the random vector $(X,Y)$ is constant inside this triangle and zero outside. To generate these points, the statistician plans to use a Gibbs sampler, a Markov chain Monte Carlo algorithm that works by iteratively sampling from the full conditional distributions.\n\nThe core of implementing the Gibbs sampler requires the correct specification of these full conditional distributions, $p(x|y)$ and $p(y|x)$. Let $U(a,b)$ denote the continuous uniform distribution on the interval $[a,b]$.\n\nWhich of the following options correctly specifies the full conditional distributions for $X$ and $Y$ required for the Gibbs sampler?\n\nA. $X|Y=y \\sim U(0, 1-y)$ and $Y|X=x \\sim U(0, 1-x)$\n\nB. $X|Y=y \\sim U(0, 1)$ and $Y|X=x \\sim U(0, 1)$\n\nC. $X|Y=y \\sim U(0, y)$ and $Y|X=x \\sim U(0, x)$\n\nD. $X|Y=y \\sim U(0, 1-x)$ and $Y|X=x \\sim U(0, 1-y)$\n\nE. $X|Y=y \\sim U(y, 1)$ and $Y|X=x \\sim U(x, 1)$", "solution": "The problem asks for the full conditional distributions $p(x|y)$ and $p(y|x)$ for a point $(X,Y)$ uniformly distributed in the triangle with vertices $(0,0)$, $(1,0)$, and $(0,1)$.\n\nFirst, let's define the support of the distribution. The triangle is bounded by the lines $x=0$, $y=0$, and the line connecting $(1,0)$ and $(0,1)$, which is $x+y=1$. Therefore, the region of non-zero probability, let's call it $R$, is given by $R = \\{(x,y) | x \\ge 0, y \\ge 0, x+y \\le 1\\}$.\n\nThe distribution is uniform over this region. The area of the triangle is $A = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}$. The joint probability density function (PDF) $f(x,y)$ of $(X,Y)$ is the reciprocal of the area for points inside the triangle and zero otherwise.\n$$\nf(x,y) =\n\\begin{cases}\n\\frac{1}{A} = 2 & \\text{if } x \\ge 0, y \\ge 0, x+y \\le 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nTo find the conditional distributions, we use the formula $f(x|y) = \\frac{f(x,y)}{f_Y(y)}$ and $f(y|x) = \\frac{f(x,y)}{f_X(x)}$, where $f_X(x)$ and $f_Y(y)$ are the marginal PDFs.\n\nLet's first find the marginal PDF of $Y$, $f_Y(y)$. We integrate the joint PDF over all possible values of $x$. For a fixed value of $y \\in [0,1]$, the constraints on $x$ are $x \\ge 0$ and $x \\le 1-y$.\n$$\nf_Y(y) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dx = \\int_{0}^{1-y} 2 \\, dx\n$$\nThis integral is valid for $y \\in [0,1]$. For $y$ outside this interval, $f_Y(y)=0$.\n$$\nf_Y(y) = 2 [x]_0^{1-y} = 2(1-y) \\quad \\text{for } 0 \\le y \\le 1\n$$\nNow we can find the conditional PDF of $X$ given $Y=y$.\n$$\nf(x|y) = \\frac{f(x,y)}{f_Y(y)} = \\frac{2}{2(1-y)} = \\frac{1}{1-y}\n$$\nThis conditional PDF is defined over the range of $x$ for a given $y$, which is $0 \\le x \\le 1-y$. The function $f(x|y) = \\frac{1}{1-y}$ is constant with respect to $x$. This is the PDF of a uniform distribution on the interval $[0, 1-y]$. Thus, we write $X|Y=y \\sim U(0, 1-y)$.\n\nNext, we find the marginal PDF of $X$, $f_X(x)$. We integrate the joint PDF over all possible values of $y$. For a fixed value of $x \\in [0,1]$, the constraints on $y$ are $y \\ge 0$ and $y \\le 1-x$.\n$$\nf_X(x) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dy = \\int_{0}^{1-x} 2 \\, dy\n$$\nThis integral is valid for $x \\in [0,1]$. For $x$ outside this interval, $f_X(x)=0$.\n$$\nf_X(x) = 2 [y]_0^{1-x} = 2(1-x) \\quad \\text{for } 0 \\le x \\le 1\n$$\nNow we can find the conditional PDF of $Y$ given $X=x$.\n$$\nf(y|x) = \\frac{f(x,y)}{f_X(x)} = \\frac{2}{2(1-x)} = \\frac{1}{1-x}\n$$\nThis conditional PDF is defined over the range of $y$ for a given $x$, which is $0 \\le y \\le 1-x$. The function $f(y|x) = \\frac{1}{1-x}$ is constant with respect to $y$. This is the PDF of a uniform distribution on the interval $[0, 1-x]$. Thus, we write $Y|X=x \\sim U(0, 1-x)$.\n\nCombining our two findings, the full conditional distributions are:\n$X|Y=y \\sim U(0, 1-y)$\n$Y|X=x \\sim U(0, 1-x)$\n\nComparing this result with the given options, we see that it matches option A.", "answer": "$$\\boxed{A}$$", "id": "1338725"}, {"introduction": "A valid set of conditional distributions is necessary, but not sufficient, for a Gibbs sampler to work correctly. This next practice explores a fundamental failure mode known as reducibility, where the sampler is unable to explore the entire target distribution. The exercise presents a scenario where the joint distribution is confined to a line segment, resulting in deterministic conditionals that trap the sampler in its initial state. [@problem_id:1338719] This serves as a stark illustration of why the Markov chain must be irreducible for it to converge to the desired target distribution.", "problem": "A monitoring system for a specialized chemical reactor tracks two correlated temperature readings, $X$ and $Y$. Due to a strict physical constraint imposed by the reactor's design, the joint probability density function $p(x, y)$ for these two readings is non-zero only on the line segment defined by the equation $x + y = 10$, where $x$ and $y$ are both non-negative (i.e., $x \\ge 0$ and $y \\ge 0$). Along this accessible line segment, the probability distribution is uniform.\n\nA data scientist decides to use a Gibbs sampler to generate synthetic data points $(X, Y)$ that follow this distribution. The sampler is initialized at the point $(X_0, Y_0) = (4.5, 5.5)$. The sampling procedure for generating the next state $(X_{t}, Y_{t})$ from the current state $(X_{t-1}, Y_{t-1})$ for $t = 1, 2, 3, \\dots$ is as follows:\n1. Draw a new value for the first variable, $X_{t}$, from the conditional distribution $p(x | Y=Y_{t-1})$.\n2. Draw a new value for the second variable, $Y_{t}$, from the conditional distribution $p(y | X=X_{t})$.\n\nDetermine the state of the sampler, $(X_{100}, Y_{100})$, after 100 full iterations. Express your answer as a pair of numbers.", "solution": "The joint support is the line segment $S=\\{(x,y): x\\ge 0,\\ y\\ge 0,\\ x+y=10\\}$. Since $p(x,y)$ is uniform along this one-dimensional set, for any $y\\in[0,10]$ the conditional $p(x\\mid Y=y)$ concentrates all its mass on the unique $x$ satisfying $x+y=10$, i.e.,\n$$\np(x\\mid Y=y)=\\delta\\!\\left(x-(10-y)\\right),\n$$\nand similarly, for any $x\\in[0,10]$,\n$$\np(y\\mid X=x)=\\delta\\!\\left(y-(10-x)\\right).\n$$\nTherefore, the Gibbs updates are deterministic:\n$$\nX_{t}=10-Y_{t-1},\\qquad Y_{t}=10-X_{t}.\n$$\nStarting from $(X_{0},Y_{0})=(4.5,5.5)$, the first iteration yields\n$$\nX_{1}=10-5.5=4.5,\\qquad Y_{1}=10-4.5=5.5,\n$$\nso the state is unchanged. By induction, if $(X_{t},Y_{t})=(4.5,5.5)$, then\n$$\nX_{t+1}=10-5.5=4.5,\\qquad Y_{t+1}=10-4.5=5.5,\n$$\nhence the chain remains fixed at $(4.5,5.5)$ for all $t$. In particular,\n$$\n(X_{100},Y_{100})=(4.5,5.5).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 4.5 & 5.5 \\end{pmatrix}}$$", "id": "1338719"}, {"introduction": "Even an irreducible Gibbs sampler can be ineffective if it explores the state space too slowly, a problem known as poor mixing. This final practice delves into this more subtle issue, which often arises when sampling from multimodal distributions. Using a hypothetical \"myopic\" sampler on an annular region, this problem highlights the difficulty the sampler faces when trying to move between well-separated modes. [@problem_id:1338675] Understanding this behavior is critical for diagnosing convergence issues and assessing the efficiency of an MCMC algorithm in practical applications.", "problem": "A statistician is studying the behavior of different Markov Chain Monte Carlo (MCMC) methods. They are interested in sampling from a uniform probability distribution over a planar annulus defined by $A = \\{(x,y) \\in \\mathbb{R}^2 \\mid R_1^2 \\le x^2+y^2 \\le R_2^2\\}$, with inner radius $R_1=1.0$ and outer radius $R_2=2.0$.\n\nThey consider a \"Myopic Gibbs Sampler,\" which is a variant of the standard Gibbs sampler. A standard Gibbs sampler would draw from the full conditional probability distribution for each variable. The myopic version, however, behaves differently when the conditional distribution's support is bimodal (i.e., consists of two disjoint intervals).\n\nThe procedure for one full step of the Myopic Gibbs Sampler, starting from state $(x_t, y_t)$, is as follows:\n1.  **Update x:** The new value $x_{t+1}$ is sampled from the conditional distribution $f(x|y_t)$.\n    -   If the support of $f(x|y_t)$ is a single continuous interval, $x_{t+1}$ is sampled uniformly from this interval.\n    -   If the support consists of two disjoint intervals, $I_-$ (on the negative axis) and $I_+$ (on the positive axis), the sampler first identifies which interval contains values with the same sign as $x_t$. It then samples $x_{t+1}$ uniformly from that chosen interval only.\n2.  **Update y:** The new value $y_{t+1}$ is sampled from the conditional distribution $f(y|x_{t+1})$ using the same myopic logic: if the support is bimodal, it restricts sampling to the interval whose sign matches the sign of the previous value, $y_t$.\n\nAssume the sampler is currently in a state $(x_t, y_t)$ drawn from its stationary distribution, conditioned on being in the first quadrant (i.e., $x_t > 0$ and $y_t > 0$). Calculate the probability that, after one full step, the new state $(x_{t+1}, y_{t+1})$ is in the third quadrant (i.e., $x_{t+1} < 0$ and $y_{t+1} < 0$).\n\nProvide your answer as a single real number, rounded to three significant figures.", "solution": "We sample uniformly on the annulus $A=\\{(x,y): R_{1}^{2}\\le x^{2}+y^{2}\\le R_{2}^{2}\\}$ with $R_{1}=1$ and $R_{2}=2$, restricted initially to the first quadrant. For fixed $y$, the conditional support for $x$ is given by $R_{1}^{2}\\le x^{2}+y^{2}\\le R_{2}^{2}$, i.e., $x^{2}\\in [\\max\\{0, R_{1}^{2}-y^{2}\\},\\, R_{2}^{2}-y^{2}]$. Hence:\n- If $y^{2}\\le R_{1}^{2}$, the support for $x$ is bimodal: $x\\in [-\\sqrt{R_{2}^{2}-y^{2}}, -\\sqrt{R_{1}^{2}-y^{2}}]\\cup[\\sqrt{R_{1}^{2}-y^{2}}, \\sqrt{R_{2}^{2}-y^{2}}]$.\n- If $y^{2}>R_{1}^{2}$, it is unimodal: $x\\in [-\\sqrt{R_{2}^{2}-y^{2}}, \\sqrt{R_{2}^{2}-y^{2}}]$.\n\nStarting from $(x_{t}, y_{t})$ in the first quadrant, the myopic $x$-update behaves as follows:\n- If $y_{t}\\le R_{1}$, the conditional is bimodal and the sampler, matching the sign of $x_{t}>0$, draws $x_{t+1}$ from the positive interval only; thus $x_{t+1}>0$ with probability $1$.\n- If $y_{t}>R_{1}$, the conditional is unimodal and symmetric, so $x_{t+1}<0$ with probability $\\frac{1}{2}$.\n\nFor the subsequent $y$-update given $x_{t+1}$, the conditional support for $y$ is symmetric and unimodal if $x_{t+1}^{2}>R_{1}^{2}$ (i.e., $|x_{t+1}|>R_{1}$), and bimodal otherwise. Since $y_{t}>0$, the myopic rule forces $y_{t+1}>0$ whenever the $y$-conditional is bimodal. Therefore, to have $y_{t+1}<0$, we must have $|x_{t+1}|>R_{1}$, in which case $y$ is drawn uniformly from a symmetric interval and $y_{t+1}<0$ occurs with probability $\\frac{1}{2}$.\n\nFix $y=y_{t}>R_{1}$ and let $a=\\sqrt{R_{2}^{2}-y^{2}}$. Then $x_{t+1}$ is uniform on $[-a,a]$. Conditional on $x_{t+1}<0$ (probability $\\frac{1}{2}$), $x_{t+1}$ is uniform on $[-a,0]$. The event $|x_{t+1}|>R_{1}$ is $x_{t+1}\\in[-a,-R_{1}]$, which has probability $\\max\\{0,(a-R_{1})/a\\}$ given $x_{t+1}<0$. Thus, conditional on $y$,\n$$\n\\mathbb{P}\\big(x_{t+1}<0,\\ y_{t+1}<0 \\mid y_{t}=y\\big)\n=\n\\begin{cases}\n\\frac{1}{4}\\left(1-\\frac{R_{1}}{a}\\right), & a>R_{1} \\\\\n0, & a\\le R_{1}.\n\\end{cases}\n$$\nWith $R_{1}=1$, $R_{2}=2$, the inequality $a>R_{1}$ is $\\sqrt{4-y^{2}}>1$, i.e., $y<\\sqrt{3}$. Therefore, this conditional probability is nonzero only for $y\\in(R_{1},\\sqrt{3})=(1,\\sqrt{3})$ and equals $\\frac{1}{4}\\left(1-\\frac{1}{\\sqrt{4-y^{2}}}\\right)$ there.\n\nWe now average over the distribution of $y_{t}$ under the stationary density restricted to the first quadrant. For a uniform distribution over the first quadrant of the annulus, the marginal density of $y$ is proportional to the positive-$x$ length at level $y$. For $y\\in(R_{1},R_{2}]$, this length is $L(y)=\\sqrt{R_{2}^{2}-y^{2}}=a$. The area of the annulus in the first quadrant is $A_{Q1}=\\frac{1}{4}\\pi(R_{2}^{2}-R_{1}^{2})=\\frac{3\\pi}{4}$. Hence\n$$\nf_{Y}(y)=\\frac{L(y)}{A_{Q1}}=\\frac{\\sqrt{4-y^{2}}}{\\frac{3\\pi}{4}}=\\frac{4}{3\\pi}\\sqrt{4-y^{2}},\\quad y\\in(1,2],\n$$\nand the desired probability is\n$$\nP=\\int_{1}^{\\sqrt{3}} f_{Y}(y)\\cdot \\frac{1}{4}\\left(1-\\frac{1}{\\sqrt{4-y^{2}}}\\right)\\,dy\n=\\frac{1}{4A_{Q1}}\\int_{1}^{\\sqrt{3}}\\left(\\sqrt{4-y^{2}}-1\\right)\\,dy.\n$$\nUsing $A_{Q1}=\\frac{3\\pi}{4}$ gives\n$$\nP=\\frac{1}{3\\pi}\\int_{1}^{\\sqrt{3}}\\left(\\sqrt{4-y^{2}}-1\\right)\\,dy.\n$$\nEvaluate the integral using $\\int \\sqrt{a^{2}-y^{2}}\\,dy=\\frac{y}{2}\\sqrt{a^{2}-y^{2}}+\\frac{a^{2}}{2}\\arcsin\\!\\left(\\frac{y}{a}\\right)$ with $a=2$:\n$$\n\\int_{1}^{\\sqrt{3}}\\sqrt{4-y^{2}}\\,dy=\\left[\\frac{y}{2}\\sqrt{4-y^{2}}+2\\arcsin\\!\\left(\\frac{y}{2}\\right)\\right]_{1}^{\\sqrt{3}}\n=\\frac{\\pi}{3},\n$$\nand $\\int_{1}^{\\sqrt{3}}1\\,dy=\\sqrt{3}-1$. Therefore\n$$\nP=\\frac{1}{3\\pi}\\left(\\frac{\\pi}{3}-\\sqrt{3}+1\\right)\n=\\frac{1}{9}+\\frac{1-\\sqrt{3}}{3\\pi}.\n$$\nNumerically, this equals approximately $0.033438\\ldots$, which to three significant figures is $0.0334$.", "answer": "$$\\boxed{0.0334}$$", "id": "1338675"}]}