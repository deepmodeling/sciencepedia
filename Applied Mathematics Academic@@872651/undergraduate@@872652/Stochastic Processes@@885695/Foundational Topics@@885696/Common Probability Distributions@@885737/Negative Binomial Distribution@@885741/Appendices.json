{"hands_on_practices": [{"introduction": "Understanding the negative binomial distribution begins with its most intuitive property: the expected number of trials. This exercise provides a foundational check on your comprehension, asking you to work backward from a known average outcome to find the underlying probability of success, $p$. Mastering this relationship between the mean and the distribution's parameters is the first step toward applying it to real-world waiting-time problems [@problem_id:12870].", "problem": "Consider a sequence of independent Bernoulli trials where the outcome of each trial can be classified as either a \"success\" or a \"failure\". Let the probability of success on any given trial be $p$, where $0  p \\leq 1$.\n\nA random variable $X$ is said to follow a negative binomial distribution if it represents the total number of trials required to achieve a fixed number of successes, denoted by $r$. The expected value (or mean) of this random variable $X$, which signifies the average number of trials one would expect to perform to see exactly $r$ successes, is given by the formula:\n$$\nE[X] = \\frac{r}{p}\n$$\nSuppose an experiment is designed to run until $r=3$ successes are observed. Through theoretical analysis or prior observation, it is determined that the expected number of trials to complete this experiment is $15$.\n\nUsing the given formula for the expected value, derive the success probability $p$ of a single trial.", "solution": "The objective is to determine the probability of success, $p$, given the expected number of trials and the required number of successes.\n\n**1. Identify the given information:**\nThe problem provides the following values:\n- The required number of successes, $r = 3$.\n- The expected number of trials to achieve these successes, $E[X] = 15$.\n\n**2. State the relevant formula:**\nThe relationship between the expected number of trials $E[X]$, the number of successes $r$, and the probability of success $p$ is given by the formula for the mean of the negative binomial distribution:\n$$\nE[X] = \\frac{r}{p}\n$$\n\n**3. Substitute the known values into the formula:**\nWe substitute $E[X] = 15$ and $r = 3$ into the equation:\n$$\n15 = \\frac{3}{p}\n$$\n\n**4. Solve for the unknown variable $p$:**\nTo isolate $p$, we can first multiply both sides of the equation by $p$:\n$$\n15 \\cdot p = 3\n$$\nNext, we divide both sides by $15$ to solve for $p$:\n$$\np = \\frac{3}{15}\n$$\n\n**5. Simplify the expression:**\nThe fraction can be simplified by dividing both the numerator and the denominator by their greatest common divisor, which is 3:\n$$\np = \\frac{3 \\div 3}{15 \\div 3} = \\frac{1}{5}\n$$\nThus, the probability of success for a single trial is $\\frac{1}{5}$.", "answer": "$$\\boxed{\\frac{1}{5}}$$", "id": "12870"}, {"introduction": "In practice, we often don't know the parameters of a process but can estimate its mean and variance from observed data. This problem simulates such a scenario, challenging you to determine both the success probability $p$ and the target number of successes $r$ from these two key statistical moments [@problem_id:1939495]. This technique, known as the method of moments, is a vital tool for fitting theoretical distributions to empirical data. It is important to note that this problem uses a common variant of the negative binomial distribution, which counts the number of failures before achieving $r$ successes.", "problem": "A team of data scientists is modeling user engagement on a new social media platform. They are interested in the number of posts a user makes until they achieve a certain number of \"successful\" posts, defined as posts that receive a high level of community interaction.\n\nThe team models this process using a negative binomial distribution. Let the random variable $X$ represent the number of \"unsuccessful\" posts a user creates before achieving a target of $r$ successful posts. The probability of any given post being successful is $p$, and each post is an independent trial.\n\nFrom a large dataset of user histories, the team has calculated the sample mean and variance for the number of unsuccessful posts. The estimated mean is $E[X] = 10$ and the estimated variance is $\\text{Var}(X) = 20$.\n\nBased on these empirical estimates, determine the parameters of the negative binomial distribution that models this process: the target number of successful posts, $r$, and the probability of success for a single post, $p$. Your answer should be the ordered pair $(r, p)$.", "solution": "Let $X$ denote the number of failures before achieving $r$ successes in independent Bernoulli trials with success probability $p$. In this parameterization of the negative binomial distribution, $X$ can be written as the sum of $r$ independent geometric random variables (each counting failures before one success), each with mean $(1-p)/p$ and variance $(1-p)/p^{2}$. Therefore,\n$$\nE[X]=r\\frac{1-p}{p},\\qquad \\operatorname{Var}(X)=r\\frac{1-p}{p^{2}}.\n$$\nLet $\\mu=E[X]$ and $\\sigma^{2}=\\operatorname{Var}(X)$. Then\n$$\n\\frac{\\sigma^{2}}{\\mu}=\\frac{r(1-p)/p^{2}}{r(1-p)/p}=\\frac{1}{p}\\quad\\Longrightarrow\\quad p=\\frac{\\mu}{\\sigma^{2}}.\n$$\nNext, solve for $r$ from the mean:\n$$\n\\mu=r\\frac{1-p}{p}\\quad\\Longrightarrow\\quad r=\\frac{\\mu p}{1-p}.\n$$\nSubstitute $p=\\mu/\\sigma^{2}$:\n$$\nr=\\frac{\\mu\\left(\\frac{\\mu}{\\sigma^{2}}\\right)}{1-\\frac{\\mu}{\\sigma^{2}}}\n=\\frac{\\mu^{2}}{\\sigma^{2}-\\mu}.\n$$\nWith the empirical estimates $\\mu=10$ and $\\sigma^{2}=20$, we obtain\n$$\np=\\frac{10}{20}=\\frac{1}{2},\\qquad r=\\frac{10^{2}}{20-10}=\\frac{100}{10}=10.\n$$\nThus, the parameters are $(r,p)=(10,\\frac{1}{2})$.", "answer": "$$\\boxed{\\begin{pmatrix}10  \\frac{1}{2}\\end{pmatrix}}$$", "id": "1939495"}, {"introduction": "Moving beyond moment-based methods, we arrive at one of the most powerful techniques in statistical inference: Maximum Likelihood Estimation (MLE). This exercise guides you through the process of deriving the MLE for the success probability $p$ based on a set of experimental observations [@problem_id:1321150]. This practice is crucial as it demonstrates how to find the parameter value that makes your observed data most probable, a fundamental principle applied widely in data science and scientific research.", "problem": "A cybersecurity research team is evaluating a new algorithm for detecting malicious data packets within a network stream. This algorithm constitutes an Intrusion Detection System (IDS). In a controlled environment, data packets are streamed to the detection system one by one. Each packet has a constant and independent probability $p$ of being malicious and correctly identified by the algorithm, which is defined as a \"success\". If a packet is not a success for any reason (i.e., it is benign or it is a malicious packet that was missed), it is considered a \"failure\".\n\nTo test the algorithm's performance, the team runs $n$ independent experiments. In each experiment $i$ (for $i=1, \\dots, n$), the team counts the number of failures, $X_i$, that occur before observing exactly $r$ successes. The value of $r$ is a predetermined positive integer and is the same for all experiments.\n\nGiven the observed data from these $n$ experiments, $x_1, x_2, \\dots, x_n$, find the maximum likelihood estimator, $\\hat{p}$, for the success probability $p$. Express your answer as a symbolic expression in terms of $r$, $n$, and the total number of observed failures, $S = \\sum_{i=1}^{n} x_i$.", "solution": "We model each experiment as follows. Let $X_{i}$ denote the number of failures before the $r$-th success in experiment $i$, with constant success probability $p$ per trial, independently across trials and experiments. Then $X_{i}$ follows a negative binomial distribution (counting failures before $r$ successes) with probability mass function\n$$\n\\Pr(X_{i}=x_{i}\\,|\\,p)=\\binom{x_{i}+r-1}{x_{i}}(1-p)^{x_{i}}p^{r},\\quad x_{i}=0,1,2,\\dots.\n$$\nGiven $n$ independent experiments with observations $x_{1},\\dots,x_{n}$, the likelihood function is\n$$\nL(p)=\\prod_{i=1}^{n}\\binom{x_{i}+r-1}{x_{i}}(1-p)^{x_{i}}p^{r}.\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}$. The likelihood factors as\n$$\nL(p)=\\left[\\prod_{i=1}^{n}\\binom{x_{i}+r-1}{x_{i}}\\right](1-p)^{S}p^{nr}.\n$$\nSince the product of binomial coefficients does not depend on $p$, the log-likelihood is\n$$\n\\ell(p)=\\ln L(p)=\\text{constant}+S\\ln(1-p)+nr\\ln p.\n$$\nDifferentiate with respect to $p$ and set to zero to obtain the score equation:\n$$\n\\frac{d\\ell}{dp}=\\frac{nr}{p}-\\frac{S}{1-p}=0.\n$$\nSolving for $p$ yields\n$$\n\\frac{nr}{p}=\\frac{S}{1-p}\\quad\\Longrightarrow\\quad nr(1-p)=Sp\\quad\\Longrightarrow\\quad nr= p(nr+S)\\quad\\Longrightarrow\\quad \\hat{p}=\\frac{nr}{nr+S}.\n$$\nTo verify that this critical point is a maximum, compute the second derivative:\n$$\n\\frac{d^{2}\\ell}{dp^{2}}=-\\frac{nr}{p^{2}}-\\frac{S}{(1-p)^{2}}0\n$$\nfor $p\\in(0,1)$, which confirms a maximum. Therefore, the maximum likelihood estimator is\n$$\n\\hat{p}=\\frac{nr}{nr+S}.\n$$", "answer": "$$\\boxed{\\frac{nr}{nr+S}}$$", "id": "1321150"}]}