{"hands_on_practices": [{"introduction": "We begin our exploration in the familiar setting of finite-dimensional vector spaces, $\\mathbb{R}^n$. This exercise [@problem_id:1302454] provides a concrete application of Hölder's inequality—specifically, the Cauchy-Schwarz case—to establish a fundamental relationship between the 1-norm and the 2-norm. By proving this norm equivalence, you will develop a hands-on understanding of how the inequality can be used to compare different ways of measuring a vector's \"size,\" a concept essential in fields from machine learning to physics.", "problem": "In many areas of data science and machine learning, vectors in $\\mathbb{R}^n$ are used to represent objects or data points. The \"size\" or \"magnitude\" of these vectors is often measured using different norms. For a vector $\\mathbf{x} = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$, the $p$-norm is defined as $\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{1/p}$ for $p \\ge 1$. Two of the most commonly used norms are the 1-norm (or Manhattan norm), $\\|\\mathbf{x}\\|_1$, and the 2-norm (or Euclidean norm), $\\|\\mathbf{x}\\|_2$.\n\nIt is a fundamental result in analysis that for any finite dimension $n$, these two norms are equivalent. This means that there exist positive constants $c_n$ and $C_n$ (depending on the dimension $n$) such that $c_n \\|\\mathbf{x}\\|_2 \\le \\|\\mathbf{x}\\|_1 \\le C_n \\|\\mathbf{x}\\|_2$ for all $\\mathbf{x} \\in \\mathbb{R}^n$.\n\nYour task is to find the sharpest possible upper bound. Determine the smallest possible value for the constant $C_n$, expressed as a function of $n$, that makes the inequality $\\|\\mathbf{x}\\|_1 \\le C_n \\|\\mathbf{x}\\|_2$ true for all vectors $\\mathbf{x} \\in \\mathbb{R}^n$.", "solution": "We seek the smallest constant $C_{n}$ such that $\\|\\mathbf{x}\\|_{1} \\leq C_{n} \\|\\mathbf{x}\\|_{2}$ holds for all $\\mathbf{x} = (x_{1},\\dots,x_{n}) \\in \\mathbb{R}^{n}$. Consider the vectors $\\mathbf{u} = (|x_{1}|,\\dots,|x_{n}|)$ and $\\mathbf{1} = (1,\\dots,1)$. Then\n$$\n\\|\\mathbf{x}\\|_{1} = \\sum_{i=1}^{n} |x_{i}| = \\langle \\mathbf{u}, \\mathbf{1} \\rangle.\n$$\nBy the Cauchy–Schwarz inequality, which states that for any $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^{n}$ one has $\\langle \\mathbf{a}, \\mathbf{b} \\rangle \\leq \\|\\mathbf{a}\\|_{2} \\|\\mathbf{b}\\|_{2}$, we obtain\n$$\n\\|\\mathbf{x}\\|_{1} = \\langle \\mathbf{u}, \\mathbf{1} \\rangle \\leq \\|\\mathbf{u}\\|_{2} \\, \\|\\mathbf{1}\\|_{2}.\n$$\nNext compute each factor: since $|x_{i}|^{2} = x_{i}^{2}$, it follows that\n$$\n\\|\\mathbf{u}\\|_{2} = \\left( \\sum_{i=1}^{n} |x_{i}|^{2} \\right)^{1/2} = \\left( \\sum_{i=1}^{n} x_{i}^{2} \\right)^{1/2} = \\|\\mathbf{x}\\|_{2},\n$$\nand\n$$\n\\|\\mathbf{1}\\|_{2} = \\left( \\sum_{i=1}^{n} 1^{2} \\right)^{1/2} = \\left( n \\right)^{1/2} = \\sqrt{n}.\n$$\nSubstituting these into the inequality gives\n$$\n\\|\\mathbf{x}\\|_{1} \\leq \\sqrt{n} \\, \\|\\mathbf{x}\\|_{2} \\quad \\text{for all } \\mathbf{x} \\in \\mathbb{R}^{n}.\n$$\nTherefore $C_{n} = \\sqrt{n}$ is a valid upper bound.\n\nTo prove sharpness, consider $\\mathbf{x} = (t, t, \\dots, t)$ for an arbitrary real $t$. Then\n$$\n\\|\\mathbf{x}\\|_{1} = \\sum_{i=1}^{n} |t| = n |t|, \\qquad \\|\\mathbf{x}\\|_{2} = \\left( \\sum_{i=1}^{n} t^{2} \\right)^{1/2} = \\sqrt{n} \\, |t|.\n$$\nHence the ratio is\n$$\n\\frac{\\|\\mathbf{x}\\|_{1}}{\\|\\mathbf{x}\\|_{2}} = \\frac{n |t|}{\\sqrt{n} \\, |t|} = \\sqrt{n}.\n$$\nThis shows that no constant smaller than $\\sqrt{n}$ can satisfy the inequality for all $\\mathbf{x} \\in \\mathbb{R}^{n}$. Therefore the smallest possible constant is $C_{n} = \\sqrt{n}$.", "answer": "$$\\boxed{\\sqrt{n}}$$", "id": "1302454"}, {"introduction": "Having established the inequality in a discrete setting, we now transition to the more abstract world of function spaces. This practice [@problem_id:1421695] uses Hölder's inequality to uncover a deep structural property of $L^p$ spaces on finite measure domains: the inclusion $L^r \\subset L^p$ for $p \\lt r$. Mastering this application not only solidifies your command of the inequality but also provides critical insight into why the properties of function spaces depend so fundamentally on the size of the domain they are defined on.", "problem": "Let $(X, \\mathcal{M}, \\mu)$ be a measure space. For a real number $s \\ge 1$, the space $L^s(X, \\mu)$ consists of all measurable functions $f: X \\to \\mathbb{R}$ such that the $L^s$-norm, defined as $\\|f\\|_s = \\left(\\int_X |f|^s \\,d\\mu\\right)^{1/s}$, is finite. This problem explores the relationship between different $L^s$ spaces based on the nature of the measure space.\n\n**Part A:** Consider a finite measure space where the total measure is $\\mu(X) = M  0$. Let $p$ and $r$ be real numbers such that $1 \\le p  r  \\infty$. For any function $f \\in L^r(X, \\mu)$, it can be shown that $f$ is also an element of $L^p(X, \\mu)$. The relationship between their norms can be expressed as an inequality $\\|f\\|_p \\le C \\|f\\|_r$, which holds for every $f \\in L^r(X, \\mu)$. Find the expression for the smallest possible constant $C$ in terms of $M, p,$ and $r$.\n\n**Part B:** The inclusion $L^r(X, \\mu) \\subset L^p(X, \\mu)$ is not guaranteed for infinite measure spaces. Consider the space $([1, \\infty), \\mathcal{B}, \\lambda)$, where $\\mathcal{B}$ represents the Borel sigma-algebra on $[1, \\infty)$ and $\\lambda$ is the standard Lebesgue measure. Let $p=2$ and $r=4$. Find the specific value of the parameter $\\alpha  0$ for the function $f(x) = x^{-\\alpha}$ such that $f$ is in $L^4([1, \\infty), \\lambda)$ but not in $L^2([1, \\infty), \\lambda)$, and its $L^4$-norm satisfies $\\|f\\|_4 = \\sqrt{2}$.\n\nYour final answer should be a pair of values, providing the constant $C$ from Part A and the value of $\\alpha$ from Part B. Express $C$ as a symbolic expression and $\\alpha$ as an exact fraction.", "solution": "Part A: Let $\\mu(X)=M0$ and $1 \\le p  r  \\infty$. For $f \\in L^{r}(X,\\mu)$, apply Hölder's inequality with conjugate exponents $a=\\frac{r}{p}$ and $b=\\frac{r}{r-p}$, which satisfy $\\frac{1}{a}+\\frac{1}{b}=1$. Then\n$$\n\\int_{X} |f|^{p} \\, d\\mu = \\int_{X} |f|^{p} \\cdot 1 \\, d\\mu \\le \\left(\\int_{X} |f|^{pa} \\, d\\mu\\right)^{\\frac{1}{a}} \\left(\\int_{X} 1^{b} \\, d\\mu\\right)^{\\frac{1}{b}} = \\left(\\int_{X} |f|^{r} \\, d\\mu\\right)^{\\frac{p}{r}} M^{\\frac{r-p}{r}}.\n$$\nTaking both sides to the power $\\frac{1}{p}$ yields\n$$\n\\|f\\|_{p} \\le \\|f\\|_{r} \\, M^{\\frac{r-p}{pr}} = \\|f\\|_{r} \\, M^{\\frac{1}{p}-\\frac{1}{r}}.\n$$\nThus the inequality holds with $C = M^{\\frac{1}{p}-\\frac{1}{r}}$. To see this $C$ is minimal, take $f=c \\chi_{X}$ with $c \\neq 0$. Then\n$$\n\\|f\\|_{p} = |c| M^{\\frac{1}{p}}, \\quad \\|f\\|_{r} = |c| M^{\\frac{1}{r}},\n$$\nso\n$$\n\\frac{\\|f\\|_{p}}{\\|f\\|_{r}} = M^{\\frac{1}{p}-\\frac{1}{r}},\n$$\nshowing equality is achieved and the constant is sharp.\n\nPart B: Let $f(x)=x^{-\\alpha}$ on $[1,\\infty)$. For $q0$,\n$$\n\\|f\\|_{q}^{q} = \\int_{1}^{\\infty} x^{-\\alpha q} \\, dx = \\begin{cases}\n\\frac{1}{\\alpha q - 1},  \\alpha q  1,\\\\\n\\text{diverges},  \\alpha q \\le 1.\n\\end{cases}\n$$\nThus $f \\in L^{4}$ iff $\\alpha  \\frac{1}{4}$, and $f \\in L^{2}$ iff $\\alpha  \\frac{1}{2}$. We require $f \\in L^{4}$ but $f \\notin L^{2}$, so $\\alpha \\in \\left(\\frac{1}{4}, \\frac{1}{2}\\right]$. The $L^{4}$ norm is\n$$\n\\|f\\|_{4} = \\left(\\frac{1}{4\\alpha - 1}\\right)^{\\frac{1}{4}}.\n$$\nImpose $\\|f\\|_{4} = \\sqrt{2}$, i.e.\n$$\n\\left(\\frac{1}{4\\alpha - 1}\\right)^{\\frac{1}{4}} = 2^{\\frac{1}{2}} \\quad \\Longrightarrow \\quad \\frac{1}{4\\alpha - 1} = 4 \\quad \\Longrightarrow \\quad 4\\alpha - 1 = \\frac{1}{4} \\quad \\Longrightarrow \\quad \\alpha = \\frac{5}{16}.\n$$\nThis $\\alpha$ satisfies $\\frac{1}{4}  \\frac{5}{16} \\le \\frac{1}{2}$, so the conditions are met.\n\nTherefore, the smallest constant in Part A is $C = M^{\\frac{1}{p}-\\frac{1}{r}}$, and the required parameter in Part B is $\\alpha = \\frac{5}{16}$.", "answer": "$$\\boxed{\\begin{pmatrix} M^{\\frac{1}{p}-\\frac{1}{r}}  \\frac{5}{16} \\end{pmatrix}}$$", "id": "1421695"}, {"introduction": "The power of Hölder's inequality extends far beyond pure analysis, serving as a vital tool in probability and stochastic processes. In this practical application [@problem_id:1307029], you will use the Cauchy-Schwarz inequality for expectations to derive an upper bound for the interaction between two key random variables in a Poisson process. This exercise demonstrates how to translate an analytical inequality into a probabilistic context to gain tangible estimates about the behavior of random phenomena.", "problem": "A system experiences events according to a Poisson process with a constant rate of $\\lambda  0$. Let $T_1$ be the random variable representing the time at which the first event occurs. Let $N(T)$ be the random variable for the total number of events that have occurred up to a fixed, non-random time $T  0$. Your task is to find an upper bound for the expected value of the product of these two random variables, $E[T_1 N(T)]$. The bound should be derived using the second moments of $T_1$ and $N(T)$. Express your answer as a closed-form analytical expression in terms of $\\lambda$ and $T$.", "solution": "We use the Cauchy–Schwarz inequality for random variables: for any square-integrable random variables $X$ and $Y$,\n$$\n|E[XY]| \\leq \\sqrt{E[X^{2}]\\,E[Y^{2}]}.\n$$\nSince $T_{1} \\geq 0$ and $N(T) \\geq 0$, we have\n$$\nE[T_{1}N(T)] \\leq \\sqrt{E[T_{1}^{2}]\\,E[N(T)^{2}]}.\n$$\n\nWe now compute the required second moments.\n\n1) The first arrival time $T_{1}$ in a Poisson process with rate $\\lambda$ is exponential with parameter $\\lambda$, having density $f_{T_{1}}(t)=\\lambda \\exp(-\\lambda t)$ for $t \\geq 0$. Using the standard integral $\\int_{0}^{\\infty} t^{n} \\exp(-\\lambda t)\\,dt = \\frac{n!}{\\lambda^{n+1}}$ for $n \\in \\{0,1,2,\\dots\\}$, we obtain\n$$\nE[T_{1}^{2}] = \\int_{0}^{\\infty} t^{2}\\,\\lambda \\exp(-\\lambda t)\\,dt\n= \\lambda \\cdot \\frac{2!}{\\lambda^{3}} = \\frac{2}{\\lambda^{2}}.\n$$\n\n2) For a homogeneous Poisson process with rate $\\lambda$, the count $N(T)$ is Poisson with mean $\\lambda T$. Hence\n$$\nE[N(T)] = \\lambda T, \\qquad \\operatorname{Var}(N(T)) = \\lambda T,\n$$\nand therefore\n$$\nE[N(T)^{2}] = \\operatorname{Var}(N(T)) + (E[N(T)])^{2}\n= \\lambda T + \\lambda^{2} T^{2}.\n$$\n\nApplying Cauchy–Schwarz with these second moments,\n$$\nE[T_{1}N(T)] \\leq \\sqrt{E[T_{1}^{2}]\\,E[N(T)^{2}]}\n= \\sqrt{\\frac{2}{\\lambda^{2}}\\left(\\lambda T + \\lambda^{2} T^{2}\\right)}\n= \\sqrt{\\frac{2T}{\\lambda}\\,(1+\\lambda T)}.\n$$\n\nThis yields the desired closed-form upper bound in terms of $\\lambda$ and $T$.", "answer": "$$\\boxed{\\sqrt{\\frac{2T}{\\lambda}\\left(1+\\lambda T\\right)}}$$", "id": "1307029"}]}