{"hands_on_practices": [{"introduction": "Calculating the expectation of a transformed random variable is a foundational skill. This first exercise provides direct practice with the definition $E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx$, using the absolute value function, $g(x) = |x|$. Tackling this problem [@problem_id:1300766] will reinforce your ability to handle absolute values within integrals and leverage properties like function symmetry to simplify your calculations.", "problem": "A continuous random variable $X$ is described by a Laplace distribution. Its Probability Density Function (PDF) is given by\n$$f_X(x) = \\frac{\\lambda}{2} \\exp(-\\lambda |x|)$$\nfor all real numbers $x$, where $\\lambda$ is a positive constant.\n\nDetermine the expected absolute value of this random variable, which is denoted as $E[|X|]$. Your answer should be an expression in terms of $\\lambda$.", "solution": "We use the definition of expectation for a function of a continuous random variable: for any measurable function $g$, $E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_{X}(x) \\, dx$. Here, $g(x) = |x|$ and $f_{X}(x) = \\frac{\\lambda}{2} \\exp(-\\lambda |x|)$ with $\\lambda  0$. Therefore,\n$$\nE[|X|] = \\int_{-\\infty}^{\\infty} |x| \\cdot \\frac{\\lambda}{2} \\exp(-\\lambda |x|) \\, dx.\n$$\nThe integrand is an even function because both $|x|$ and $\\exp(-\\lambda |x|)$ are even. Using symmetry,\n$$\nE[|X|] = 2 \\int_{0}^{\\infty} x \\cdot \\frac{\\lambda}{2} \\exp(-\\lambda x) \\, dx = \\lambda \\int_{0}^{\\infty} x \\exp(-\\lambda x) \\, dx.\n$$\nEvaluate the remaining integral by integration by parts. Let $u = x$ and $dv = \\exp(-\\lambda x) \\, dx$, so $du = dx$ and $v = -\\frac{1}{\\lambda} \\exp(-\\lambda x)$. Then\n$$\n\\int_{0}^{\\infty} x \\exp(-\\lambda x) \\, dx = \\left[ -\\frac{x}{\\lambda} \\exp(-\\lambda x) \\right]_{0}^{\\infty} + \\frac{1}{\\lambda} \\int_{0}^{\\infty} \\exp(-\\lambda x) \\, dx.\n$$\nSince $\\lim_{x \\to \\infty} x \\exp(-\\lambda x) = 0$ for $\\lambda > 0$, the boundary term vanishes, and\n$$\n\\int_{0}^{\\infty} \\exp(-\\lambda x) \\, dx = \\frac{1}{\\lambda}.\n$$\nThus,\n$$\n\\int_{0}^{\\infty} x \\exp(-\\lambda x) \\, dx = \\frac{1}{\\lambda} \\cdot \\frac{1}{\\lambda} = \\frac{1}{\\lambda^{2}}.\n$$\nSubstituting back,\n$$\nE[|X|] = \\lambda \\cdot \\frac{1}{\\lambda^{2}} = \\frac{1}{\\lambda}.\n$$", "answer": "$$\\boxed{\\frac{1}{\\lambda}}$$", "id": "1300766"}, {"introduction": "Many real-world systems involve the interaction of multiple random components. This practice problem [@problem_id:1300795] moves from a single variable to two, asking for the expected distance between two randomly placed probes. Solving it requires setting up a double integral over the joint probability space, a core technique for analyzing systems with more than one source of randomness.", "problem": "Two autonomous sensor probes, Probe 1 and Probe 2, are deployed along a straight fiber optic cable of total length $L$. The final resting position of each probe is random. We model their positions, $X_1$ and $X_2$ respectively, as two independent random variables drawn from a continuous uniform distribution on the interval $[0, L]$.\n\nTo establish a communication link, the probes must orient their antennas towards each other. The power required for this initial \"handshake\" is proportional to the distance between them. A key design parameter for the probes' battery system is the expected value of this distance.\n\nCalculate the expected distance between the two probes. Your final answer should be a closed-form analytic expression in terms of $L$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent and identically distributed as $\\text{Uniform}(0,L)$. The expected distance is $\\mathbb{E}[|X_{1}-X_{2}|]$. For independent continuous variables, the expectation of a function $g(X_{1},X_{2})$ is the double integral of $g$ against the joint probability density. The joint density here is\n$$\nf_{X_{1},X_{2}}(x_{1},x_{2})=\\frac{1}{L^{2}} \\quad \\text{for } 0\\leq x_{1}\\leq L,\\; 0\\leq x_{2}\\leq L,\n$$\nand zero otherwise. Therefore,\n$$\n\\mathbb{E}[|X_{1}-X_{2}|]=\\int_{0}^{L}\\int_{0}^{L} |x_{1}-x_{2}| \\frac{1}{L^{2}} \\, dx_{2}\\, dx_{1}.\n$$\nBy symmetry of the integrand and the square domain, this equals\n$$\n\\mathbb{E}[|X_{1}-X_{2}|]=\\frac{2}{L^{2}} \\int_{0}^{L}\\int_{0}^{x_{1}} (x_{1}-x_{2}) \\, dx_{2}\\, dx_{1}.\n$$\nEvaluate the inner integral:\n$$\n\\int_{0}^{x_{1}} (x_{1}-x_{2}) \\, dx_{2}=\\left[x_{1}x_{2}-\\frac{1}{2}x_{2}^{2}\\right]_{0}^{x_{1}}=\\frac{1}{2}x_{1}^{2}.\n$$\nSubstitute back and integrate over $x_{1}$:\n$$\n\\mathbb{E}[|X_{1}-X_{2}|]=\\frac{2}{L^{2}} \\int_{0}^{L} \\frac{1}{2} x_{1}^{2} \\, dx_{1}=\\frac{1}{L^{2}} \\int_{0}^{L} x_{1}^{2} \\, dx_{1}=\\frac{1}{L^{2}} \\cdot \\frac{L^{3}}{3}=\\frac{L}{3}.\n$$\nThus, the expected distance between the probes is $\\frac{L}{3}$.", "answer": "$$\\boxed{\\frac{L}{3}}$$", "id": "1300795"}, {"introduction": "Theoretical probability distributions are powerful, but practical applications often involve physical or systemic constraints. This exercise [@problem_id:1300794] models a common scenario in engineering and data acquisition: sensor saturation, where measurements are capped at a maximum value. Calculating the expected reading of such a sensor demonstrates how to handle expectations for variables defined piecewise, a valuable technique for modeling real-world limitations.", "problem": "A digital sensor is designed to measure the intensity of a physical phenomenon. The true intensity, represented by a random variable $X$, follows an exponential distribution with a mean value of $\\lambda$. The probability density function for $X$ is given by $f_X(x) = \\frac{1}{\\lambda}\\exp(-\\frac{x}{\\lambda})$ for $x \\ge 0$, and $f_X(x) = 0$ for $x  0$.\n\nThe sensor has a saturation limit, meaning it cannot record values above a certain maximum threshold, $M$. If the true intensity $X$ is less than or equal to $M$, the sensor reading $Y$ is equal to $X$. If the true intensity $X$ is greater than $M$, the sensor reading $Y$ is capped at the value $M$.\n\nGiven that the mean true intensity $\\lambda$ is greater than the saturation limit $M$, determine the expected value of the sensor's reading, $E[Y]$. Express your answer as a single closed-form analytic expression in terms of $\\lambda$ and $M$.", "solution": "Let $Y=\\min(X,M)$. Using the law of the unconscious statistician for piecewise definitions,\n$$\nE[Y]=E[X\\mathbf{1}_{\\{X\\leq M\\}}]+M\\,P(XM)=\\int_{0}^{M}x f_{X}(x)\\,dx+M\\int_{M}^{\\infty}f_{X}(x)\\,dx.\n$$\nWith $f_{X}(x)=\\frac{1}{\\lambda}\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)$ for $x\\geq 0$, compute the first integral by parts. Let $u=x$ and $dv=\\frac{1}{\\lambda}\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)dx$, so $du=dx$ and $v=-\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)$. Then\n$$\n\\int_{0}^{M}x \\frac{1}{\\lambda}\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)dx\n=\\left[-x\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)\\right]_{0}^{M}\n+\\int_{0}^{M}\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)dx.\n$$\nEvaluate the remaining integral:\n$$\n\\int_{0}^{M}\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)dx\n=-\\lambda\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)\\Big|_{0}^{M}\n=\\lambda-\\lambda\\exp\\!\\left(-\\frac{M}{\\lambda}\\right).\n$$\nHence\n$$\n\\int_{0}^{M}x \\frac{1}{\\lambda}\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)dx\n=-M\\exp\\!\\left(-\\frac{M}{\\lambda}\\right)+\\lambda-\\lambda\\exp\\!\\left(-\\frac{M}{\\lambda}\\right)\n=\\lambda-(\\lambda+M)\\exp\\!\\left(-\\frac{M}{\\lambda}\\right).\n$$\nFor the tail probability,\n$$\n\\int_{M}^{\\infty}\\frac{1}{\\lambda}\\exp\\!\\left(-\\frac{x}{\\lambda}\\right)dx\n=\\exp\\!\\left(-\\frac{M}{\\lambda}\\right).\n$$\nCombine the parts:\n$$\nE[Y]=\\left[\\lambda-(\\lambda+M)\\exp\\!\\left(-\\frac{M}{\\lambda}\\right)\\right]+M\\exp\\!\\left(-\\frac{M}{\\lambda}\\right)\n=\\lambda-\\lambda\\exp\\!\\left(-\\frac{M}{\\lambda}\\right)\n=\\lambda\\left(1-\\exp\\!\\left(-\\frac{M}{\\lambda}\\right)\\right).\n$$\nThis closed-form expression is valid, and the assumption $\\lambdaM$ simply ensures a nonzero saturation probability.", "answer": "$$\\boxed{\\lambda\\left(1-\\exp\\left(-\\frac{M}{\\lambda}\\right)\\right)}$$", "id": "1300794"}]}