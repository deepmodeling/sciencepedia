{"hands_on_practices": [{"introduction": "Our first practice exercise grounds the abstract concept of conditional expectation in the familiar setting of coin tosses. By calculating the expected number of heads given that we didn't get all tails, we directly apply the definition of conditioning on a discrete event. This problem [@problem_id:1291494] provides a concrete example of how new information—in this case, ruling out one specific outcome—changes our prediction for the average result.", "problem": "A player participates in a simple carnival game. The game consists of tossing three distinct, fair coins simultaneously. The player's score for a round is determined by the number of heads that land face up. However, there's a special rule: if a round results in all three coins landing as tails, the round is considered a 'dud' and is immediately replayed. We are interested in the expected score for any round that is not a dud.\n\nLet $X$ be the random variable representing the number of heads in a single toss of the three coins. Calculate the expected value of $X$ given that the round is not a dud (i.e., given that at least one head is observed). Express your final answer as an exact fraction.", "solution": "Let $X$ denote the number of heads when three independent fair coins are tossed. Then $X$ has a binomial distribution with parameters $n=3$ and $p=\\frac{1}{2}$.\n\nDefine the event $A=\\{X0\\}$ (the round is not a dud). We seek $\\mathbb{E}[X \\mid A]$.\n\nFirst compute the unconditional expectation using indicator variables. Let $H_{i}$ be the indicator that the $i$-th coin shows heads. Then $X=\\sum_{i=1}^{3} H_{i}$, and by linearity of expectation,\n$$\n\\mathbb{E}[X]=\\sum_{i=1}^{3} \\mathbb{E}[H_{i}]=3 \\cdot \\frac{1}{2}=\\frac{3}{2}.\n$$\n\nNext compute $\\mathbb{P}(A)$. The complement $A^{c}$ is the event of all tails, which has probability\n$$\n\\mathbb{P}(A^{c})=\\left(\\frac{1}{2}\\right)^{3}=\\frac{1}{8},\n$$\nso\n$$\n\\mathbb{P}(A)=1-\\mathbb{P}(A^{c})=1-\\frac{1}{8}=\\frac{7}{8}.\n$$\n\nUse the conditional expectation identity\n$$\n\\mathbb{E}[X \\mid A]=\\frac{\\mathbb{E}[X \\mathbf{1}_{A}]}{\\mathbb{P}(A)}.\n$$\nSince $X=0$ on $A^{c}$, we have $X \\mathbf{1}_{A}=X$, hence\n$$\n\\mathbb{E}[X \\mid A]=\\frac{\\mathbb{E}[X]}{\\mathbb{P}(A)}=\\frac{\\frac{3}{2}}{\\frac{7}{8}}=\\frac{12}{7}.\n$$\n\nThis yields the expected score given the round is not a dud.", "answer": "$$\\boxed{\\frac{12}{7}}$$", "id": "1291494"}, {"introduction": "Next, we move from discrete events to the continuous domain, where conditional expectation often carries a strong geometric intuition. This problem asks for the expected value of one coordinate, $Y$, given the value of another, $X$, for a point chosen uniformly from a specific 2D region. Solving this [@problem_id:1905673] is akin to finding the center of mass of a vertical 'slice' of the region, illustrating how conditioning on $X=x$ restricts our focus to a smaller subset of the sample space.", "problem": "A point $(X, Y)$ is chosen uniformly at random from the region $R$ in the first quadrant of the Cartesian plane. The region $R$ is bounded by the curves $y = \\sqrt{x}$ and $y = x^3$. Find the conditional expectation of the random variable $Y$ given that the random variable $X$ takes on the value $x$, for $0  x  1$. Your answer should be a closed-form analytic expression in terms of $x$.", "solution": "The region $R$ in the first quadrant bounded by $y=\\sqrt{x}$ and $y=x^{3}$ has intersection points where $\\sqrt{x}=x^{3}$, which gives $x=x^{6}$, so $x=0$ or $x=1$. For $0x1$, the vertical cross-section at a fixed $x$ runs from $y_{\\ell}=x^{3}$ to $y_{u}=\\sqrt{x}$, with $y_{u}y_{\\ell}$.\n\nSince $(X,Y)$ is uniformly distributed over $R$, the joint density is\n$$\nf_{X,Y}(x,y)=\\frac{1}{|R|} \\quad \\text{for } (x,y)\\in R,\n$$\nwhere $|R|$ is the area of $R$. The marginal density of $X$ for $0x1$ is\n$$\nf_{X}(x)=\\int_{y=x^{3}}^{\\sqrt{x}} \\frac{1}{|R|}\\,dy=\\frac{\\sqrt{x}-x^{3}}{|R|}.\n$$\nHence, for $0x1$, the conditional density of $Y$ given $X=x$ is\n$$\nf_{Y\\mid X}(y\\mid x)=\\frac{f_{X,Y}(x,y)}{f_{X}(x)}=\\frac{1/|R|}{(\\sqrt{x}-x^{3})/|R|}=\\frac{1}{\\sqrt{x}-x^{3}}, \\quad x^{3}\\leq y\\leq \\sqrt{x}.\n$$\nThis means that given $X=x$, $Y$ is uniformly distributed on the interval $[x^3, \\sqrt{x}]$. The expectation of a uniform random variable on an interval $[a,b]$ is its midpoint $(a+b)/2$. Therefore,\n$$\n\\mathbb{E}[Y\\mid X=x]=\\frac{x^{3}+\\sqrt{x}}{2}.\n$$\nAlternatively, via integration:\n$$\n\\mathbb{E}[Y\\mid X=x]=\\int_{x^{3}}^{\\sqrt{x}} y\\,f_{Y\\mid X}(y\\mid x)\\,dy=\\int_{x^{3}}^{\\sqrt{x}} \\frac{y}{\\sqrt{x}-x^{3}}\\,dy.\n$$\nEvaluating the integral,\n$$\n\\mathbb{E}[Y\\mid X=x]=\\left.\\frac{y^{2}}{2(\\sqrt{x}-x^{3})}\\right|_{y=x^{3}}^{y=\\sqrt{x}}=\\frac{(\\sqrt{x})^{2}-(x^{3})^{2}}{2(\\sqrt{x}-x^{3})}=\\frac{x-x^{6}}{2(\\sqrt{x}-x^{3})}.\n$$\nFactor the numerator and denominator:\n$$\n\\frac{x(1-x^{5})}{2x^{1/2}(1-x^{5/2})} = \\frac{x^{1/2}(1-(x^{5/2})^2)}{2(1-x^{5/2})} = \\frac{x^{1/2}(1-x^{5/2})(1+x^{5/2})}{2(1-x^{5/2})} = \\frac{x^{1/2}(1+x^{5/2})}{2} = \\frac{x^{1/2}+x^3}{2}.\n$$\nThus, for $0x1$,\n$$\n\\mathbb{E}[Y\\mid X=x]=\\frac{\\sqrt{x}+x^{3}}{2}.\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{x}+x^{3}}{2}}$$", "id": "1905673"}, {"introduction": "This final practice demonstrates one of the most elegant and useful results in the study of random variables. We consider two independent Poisson processes and condition on their sum. The exercise [@problem_id:1905661] is to find the expected value of one process given the total, which reveals a surprising and fundamental connection between the Poisson and Binomial distributions. This principle is not just a theoretical curiosity; it is a vital tool in statistical modeling, particularly in fields like queuing theory and computational biology.", "problem": "In a data center, two independent servers, Server A and Server B, process incoming requests. The number of requests arriving at Server A in a given minute, denoted by the random variable $X_1$, follows a Poisson distribution with mean $\\lambda_1$. Similarly, the number of requests arriving at Server B in the same minute, denoted by $X_2$, is also a Poisson random variable, independent of $X_1$, with mean $\\lambda_2$.\n\nSuppose that at the end of a particular minute, a monitoring system reports that a total of $k$ requests arrived at the two servers combined, where $k$ is a non-negative integer. Given this information, what is the expected number of requests that arrived at Server A?\n\nYour task is to find a closed-form expression for the conditional expectation $E[X_1 | X_1 + X_2 = k]$.", "solution": "Let $X_{1} \\sim \\mathrm{Poisson}(\\lambda_{1})$ and $X_{2} \\sim \\mathrm{Poisson}(\\lambda_{2})$ be independent. The joint probability mass function is\n$$\n\\mathbb{P}(X_{1}=i, X_{2}=j)=\\mathbb{P}(X_{1}=i)\\mathbb{P}(X_{2}=j)=\\exp(-\\lambda_{1})\\frac{\\lambda_{1}^{i}}{i!}\\,\\exp(-\\lambda_{2})\\frac{\\lambda_{2}^{j}}{j!}=\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{\\lambda_{1}^{i}\\lambda_{2}^{j}}{i!\\,j!}.\n$$\nThe sum of two independent Poisson random variables is also a Poisson random variable: $X_1+X_2 \\sim \\mathrm{Poisson}(\\lambda_1+\\lambda_2)$. The probability mass function of the sum is\n$$\n\\mathbb{P}(X_{1}+X_{2}=k)=\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{(\\lambda_{1}+\\lambda_{2})^{k}}{k!}.\n$$\nFor $k \\in \\{0,1,2,\\dots\\}$ and $i \\in \\{0,1,\\dots,k\\}$, the conditional probability is\n$$\n\\mathbb{P}(X_{1}=i \\mid X_{1}+X_{2}=k)=\\frac{\\mathbb{P}(X_{1}=i, X_{1}+X_{2}=k)}{\\mathbb{P}(X_{1}+X_{2}=k)} = \\frac{\\mathbb{P}(X_{1}=i, X_{2}=k-i)}{\\mathbb{P}(X_{1}+X_{2}=k)}.\n$$\nSubstituting the PMFs:\n$$\n\\mathbb{P}(X_{1}=i \\mid X_{1}+X_{2}=k)=\\frac{\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{\\lambda_{1}^{i}\\lambda_{2}^{k-i}}{i!\\,(k-i)!}}{\\exp(-(\\lambda_{1}+\\lambda_{2}))\\frac{(\\lambda_{1}+\\lambda_{2})^{k}}{k!}}=\\frac{k!}{i!\\,(k-i)!} \\frac{\\lambda_{1}^{i}\\lambda_{2}^{k-i}}{(\\lambda_{1}+\\lambda_{2})^{k}}.\n$$\nThis can be rewritten as:\n$$\n\\mathbb{P}(X_{1}=i \\mid X_{1}+X_{2}=k)=\\binom{k}{i}\\left(\\frac{\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}\\right)^{i}\\left(\\frac{\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}}\\right)^{k-i}.\n$$\nThis is the probability mass function of a $\\mathrm{Binomial}\\left(k,p\\right)$ distribution with $p=\\frac{\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}$. \nThe expectation of a binomial random variable $Y \\sim \\mathrm{Binomial}(n,p)$ is $E[Y]=np$. Therefore,\n$$\nE[X_{1} \\mid X_{1}+X_{2}=k]=k \\cdot p = k\\frac{\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}.\n$$", "answer": "$$\\boxed{\\frac{k\\lambda_{1}}{\\lambda_{1}+\\lambda_{2}}}$$", "id": "1905661"}]}