{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of Physics-Informed Neural Networks, we will start with the most fundamental task: constructing the loss function. This exercise [@problem_id:2126324] demonstrates how a continuous physical problem, described by a partial differential equation and its boundary conditions, is translated into a discrete objective that a neural network can minimize. By building the loss for the classic Poisson's equation, you will gain a core understanding of how a PINN is trained to respect both the governing physics in the domain's interior and the prescribed constraints on its boundaries.", "problem": "A researcher is building a Physics-Informed Neural Network (PINN) to find an approximate solution for the electrostatic potential, $V(x,y)$, within a two-dimensional square region. The physical behavior of the potential is described by the Poisson equation:\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\nwhere $f(x,y)$ represents a given charge distribution density and $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator. The potential is defined over the domain $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$. The boundary of this domain, $\\partial D$, is held at a zero potential (grounded), which imposes the boundary condition $V(x,y) = 0$ for all $(x,y) \\in \\partial D$.\n\nThe PINN model, denoted by $\\hat{V}(x,y; \\theta)$, learns to approximate $V(x,y)$ by minimizing a loss function $L(\\theta)$ that incorporates the physics of the problem. Here, $\\theta$ represents all the trainable parameters of the neural network. The loss function is calculated using two sets of discrete points:\n1.  A set of $N_{pde}$ collocation points, $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$, located in the interior of the domain $D$.\n2.  A set of $N_{bc}$ boundary points, $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$, located on the boundary $\\partial D$.\n\nThe total loss function, $L(\\theta)$, is the sum of two mean squared error terms: one for the governing partial differential equation ($L_{pde}$) and one for the boundary conditions ($L_{bc}$).\n\nConstruct the mathematical expression for the total loss function $L(\\theta) = L_{pde} + L_{bc}$. Your expression should be in terms of the network's output $\\hat{V}$, its second partial derivatives, the function $f$, the given point sets, and their respective sizes $N_{pde}$ and $N_{bc}$.", "solution": "We begin from the governing Poisson equation and boundary condition:\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\nA Physics-Informed Neural Network approximates $V$ by $\\hat{V}(x,y;\\theta)$. The PDE residual at an interior collocation point $(x_{i},y_{i})\\in S_{pde}$ is defined by imposing the Poisson equation on $\\hat{V}$:\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nUsing the definition of the Laplacian in two dimensions, this is equivalently\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nThe mean squared error enforcing the PDE over $S_{pde}$ is then\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\nThe boundary condition $V=0$ on $\\partial D$ is enforced by penalizing the deviation of $\\hat{V}$ from zero at boundary points $(x_{j},y_{j})\\in S_{bc}$:\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\nTherefore, the total loss is the sum of the two mean squared error terms:\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "While enforcing the governing PDE is the cornerstone of a PINN, we can often imbue our models with deeper physical knowledge by enforcing known conservation laws. This practice [@problem_id:2126322] challenges you to augment the standard loss function for the wave equation with an additional term that penalizes any deviation from the principle of energy conservation. This technique is a powerful way to improve a model's long-term stability and physical realism, showcasing the true flexibility of the physics-informed approach.", "problem": "A Physics-Informed Neural Network (PINN) is a machine learning model used to find an approximate solution $\\hat{u}(x,t; \\theta)$ to a Partial Differential Equation (PDE), where $\\theta$ represents the trainable parameters of the network. The training process minimizes a loss function that penalizes deviations from known physical laws, initial conditions, and boundary conditions.\n\nConsider the one-dimensional linear wave equation on a spatio-temporal domain $(x,t) \\in [x_L, x_R] \\times [0, T]$:\n$$ \\frac{\\partial^2 u}{\\partial t^2} - c^2 \\frac{\\partial^2 u}{\\partial x^2} = 0 $$\nwith a constant wave speed $c$. The system is subject to the initial conditions $u(x,0) = g(x)$ and $\\frac{\\partial u}{\\partial t}(x,0) = h(x)$, and boundary conditions $u(x_L, t) = u_L(t)$ and $u(x_R, t) = u_R(t)$.\n\nA known property of the wave equation is the conservation of total energy, defined as:\n$$ E(t) = \\frac{1}{2} \\int_{x_L}^{x_R} \\left( \\left(\\frac{\\partial u}{\\partial t}\\right)^2 + c^2 \\left(\\frac{\\partial u}{\\partial x}\\right)^2 \\right) dx $$\nFor any valid solution, $E(t)$ must remain constant and equal to its initial value $E(0)$ for all $t \\in [0,T]$.\n\nYour task is to construct a modified total loss function, $\\mathcal{L}_{\\text{total}}$, that explicitly incorporates a penalty for any violation of this energy conservation law. The total loss is a weighted sum of four components:\n$$ \\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{E}}\\mathcal{L}_{\\text{E}} $$\nwhere the $\\lambda$ terms are positive weighting hyperparameters.\n\nDefine the components of the loss function based on the following sets of collocation points, formulating each component as a Mean Squared Error (MSE):\n- **PDE Loss $\\mathcal{L}_{\\text{PDE}}$**: Evaluated over a set of $N_{\\text{PDE}}$ points $\\mathcal{S}_{\\text{PDE}} = \\{(x_i, t_i)\\}_{i=1}^{N_{\\text{PDE}}}$ in the interior of the domain.\n- **Initial Condition Loss $\\mathcal{L}_{\\text{IC}}$**: Evaluated over a set of $N_{\\text{IC}}$ points $\\mathcal{S}_{\\text{IC}} = \\{(x_j, 0)\\}_{j=1}^{N_{\\text{IC}}}$ on the initial time line. The MSE should account for errors in both $u$ and its time derivative $u_t$.\n- **Boundary Condition Loss $\\mathcal{L}_{\\text{BC}}$**: Evaluated over a set of $N_{\\text{BC}}$ points $\\mathcal{S}_{\\text{BC}} = \\{(x_k, t_k)\\}_{k=1}^{N_{\\text{BC}}}$ on the spatial boundaries $x \\in \\{x_L, x_R\\}$. For a point $(x_k, t_k) \\in \\mathcal{S}_{\\text{BC}}$, the target boundary value is $u_{\\text{BC}}(x_k, t_k)$, which is equal to $u_L(t_k)$ if $x_k=x_L$ and $u_R(t_k)$ if $x_k=x_R$.\n- **Energy Conservation Loss $\\mathcal{L}_{\\text{E}}$**: This loss penalizes the deviation of the network's predicted energy at various times from the true initial energy. The energy integral is to be approximated using a midpoint Riemann sum over $N_E$ uniform subintervals of $[x_L, x_R]$, each of width $\\Delta x = (x_R - x_L)/N_E$. The spatial evaluation points for this sum are $\\{x_m = x_L + (m - 1/2)\\Delta x\\}_{m=1}^{N_E}$. The conservation is enforced at a set of $N_T$ time instances $\\mathcal{T}_E = \\{t_l\\}_{l=1}^{N_T}$ where $t_l > 0$.\n\nLet the neural network's approximation to the solution be $\\hat{u}(x,t; \\theta)$, and its partial derivatives with respect to $x$ and $t$ be denoted by $\\hat{u}_x$, $\\hat{u}_t$, $\\hat{u}_{xx}$, and $\\hat{u}_{tt}$. Assume the functions $g(x)$, $h(x)$, and the spatial derivative $g'(x)$ are known and can be evaluated at the required points. Construct the full expression for $\\mathcal{L}_{\\text{total}}$. Your final answer should be a single analytical expression in terms of the network outputs (e.g., $\\hat{u}(x,t)$), its derivatives, the given functions, the specified points, and the weighting factors.", "solution": "We seek a total loss that penalizes violations of the PDE, initial conditions, boundary conditions, and conservation of energy. For a PINN approximation $\\hat{u}(x,t;\\theta)$, define the PDE residual at interior collocation points $(x_{i},t_{i}) \\in \\mathcal{S}_{\\text{PDE}}$ by the wave equation:\n$$\nr(x_{i},t_{i};\\theta) = \\hat{u}_{tt}(x_{i},t_{i};\\theta) - c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta).\n$$\nThe PDE loss is the mean squared residual:\n$$\n\\mathcal{L}_{\\text{PDE}} = \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}} \\left(r(x_{i},t_{i};\\theta)\\right)^{2} = \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}} \\left(\\hat{u}_{tt}(x_{i},t_{i};\\theta) - c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta)\\right)^{2}.\n$$\nFor the initial conditions at $(x_{j},0) \\in \\mathcal{S}_{\\text{IC}}$, the target values are $u(x_{j},0)=g(x_{j})$ and $u_{t}(x_{j},0)=h(x_{j})$. The initial condition loss is the mean squared error over both $u$ and $u_{t}$:\n$$\n\\mathcal{L}_{\\text{IC}} = \\frac{1}{2N_{\\text{IC}}}\\sum_{j=1}^{N_{\\text{IC}}} \\left[\\left(\\hat{u}(x_{j},0;\\theta)-g(x_{j})\\right)^{2} + \\left(\\hat{u}_{t}(x_{j},0;\\theta)-h(x_{j})\\right)^{2}\\right].\n$$\nFor boundary conditions at $(x_{k},t_{k}) \\in \\mathcal{S}_{\\text{BC}}$ with $x_{k}\\in\\{x_{L},x_{R}\\}$, the target boundary value is $u_{\\text{BC}}(x_{k},t_{k})$ (equal to $u_{L}(t_{k})$ if $x_{k}=x_{L}$ and $u_{R}(t_{k})$ if $x_{k}=x_{R}$). The boundary loss is the mean squared discrepancy:\n$$\n\\mathcal{L}_{\\text{BC}} = \\frac{1}{N_{\\text{BC}}}\\sum_{k=1}^{N_{\\text{BC}}} \\left(\\hat{u}(x_{k},t_{k};\\theta) - u_{\\text{BC}}(x_{k},t_{k})\\right)^{2}.\n$$\nEnergy conservation requires $E(t)=E(0)$ for all $t$. We approximate the energy integral by a midpoint Riemann sum over $N_{E}$ uniform subintervals of $[x_{L},x_{R}]$ with spatial midpoints $x_{m}=x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}$ and width $\\Delta x=\\frac{x_{R}-x_{L}}{N_{E}}$. The network’s predicted energy at a time $t_{l}\\in\\mathcal{T}_{E}$ is\n$$\n\\widehat{E}(t_{l}) = \\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}} \\sum_{m=1}^{N_{E}}\\left(\\hat{u}_{t}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2} + c^{2}\\hat{u}_{x}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}\\right).\n$$\nThe true initial energy, computed from the known initial data using the same quadrature points, is\n$$\nE_{0} = \\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}} \\sum_{m=1}^{N_{E}}\\left(h\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)^{2} + c^{2}\\left(g'\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)\\right)^{2}\\right).\n$$\nThe energy conservation loss is the mean squared deviation of $\\widehat{E}(t_{l})$ from $E_{0}$ over the selected time instances:\n$\n\\mathcal{L}_{\\text{E}} = \\frac{1}{N_{T}}\\sum_{l=1}^{N_{T}}\\left(\\widehat{E}(t_{l}) - E_{0}\\right)^{2}.\n$\nCombining all four components with positive weights $\\lambda_{\\text{PDE}},\\lambda_{\\text{IC}},\\lambda_{\\text{BC}},\\lambda_{\\text{E}}$ yields the total loss:\n$$\n\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{E}}\\mathcal{L}_{\\text{E}}.\n$$\nSubstituting the explicit forms of all components gives a single analytical expression in terms of $\\hat{u}$, its derivatives, the given data, the collocation points, and the weights as required.", "answer": "$$\\boxed{\\lambda_{\\text{PDE}} \\frac{1}{N_{\\text{PDE}}}\\sum_{i=1}^{N_{\\text{PDE}}}\\left(\\hat{u}_{tt}(x_{i},t_{i};\\theta)-c^{2}\\hat{u}_{xx}(x_{i},t_{i};\\theta)\\right)^{2}+\\lambda_{\\text{IC}}\\frac{1}{2N_{\\text{IC}}}\\sum_{j=1}^{N_{\\text{IC}}}\\left[\\left(\\hat{u}(x_{j},0;\\theta)-g(x_{j})\\right)^{2}+\\left(\\hat{u}_{t}(x_{j},0;\\theta)-h(x_{j})\\right)^{2}\\right]+\\lambda_{\\text{BC}}\\frac{1}{N_{\\text{BC}}}\\sum_{k=1}^{N_{\\text{BC}}}\\left(\\hat{u}(x_{k},t_{k};\\theta)-u_{\\text{BC}}(x_{k},t_{k})\\right)^{2}+\\lambda_{\\text{E}}\\frac{1}{N_{T}}\\sum_{l=1}^{N_{T}}\\left(\\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}}\\sum_{m=1}^{N_{E}}\\left[\\hat{u}_{t}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}+c^{2}\\hat{u}_{x}\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}},t_{l};\\theta\\right)^{2}\\right]-\\frac{1}{2}\\frac{x_{R}-x_{L}}{N_{E}}\\sum_{m=1}^{N_{E}}\\left[h\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)^{2}+c^{2}\\left(g'\\!\\left(x_{L}+\\left(m-\\frac{1}{2}\\right)\\frac{x_{R}-x_{L}}{N_{E}}\\right)\\right)^{2}\\right]\\right)^{2}}$$", "id": "2126322"}, {"introduction": "Theory comes to life through implementation. This final hands-on problem [@problem_id:2427209] guides you through building a complete solver for the time-dependent Schrödinger equation, a cornerstone of quantum mechanics. Here, you will see a sophisticated application of the physics-informed paradigm where the physics—specifically, the dispersion relation—is used not only in the loss function but to architect the neural network itself. By constructing the model from basis functions that inherently satisfy the PDE, the learning task is elegantly reduced, demonstrating a powerful synergy between machine learning and classical spectral methods.", "problem": "You are to implement a complete, runnable program that constructs and solves a physics-informed neural network for the time-dependent Schrödinger equation using a complex-valued network. The partial differential equation is the time-dependent Schrödinger equation for a single particle with the free-particle Hamiltonian operator, starting from the fundamental relation that the Hamiltonian operator for a free particle of mass $m$ is $H = -\\frac{\\hbar^{2}}{2m}\\frac{\\partial^{2}}{\\partial x^{2}}$, and the governing equation is $i\\hbar \\frac{\\partial \\psi}{\\partial t} = H\\psi$. Work in nondimensionalized units in which $\\hbar = 1$ and $m = 1$, so the equation reduces to $i \\frac{\\partial \\psi}{\\partial t} = -\\frac{1}{2}\\frac{\\partial^{2} \\psi}{\\partial x^{2}}$. Impose periodic boundary conditions on a one-dimensional spatial domain $x \\in [0, 2\\pi]$ and consider times $t \\in [0, T]$ with $T > 0$. The initial condition is a plane wave $\\psi(x, 0) = e^{i k x}$ with a prescribed integer wavenumber $k$, and the exact solution for verification is $\\psi(x,t) = e^{i(kx - \\omega t)}$ with $\\omega = \\frac{1}{2}k^{2}$, which follows from substituting a plane wave into the Schrödinger equation and using the dispersion relation implied by the Hamiltonian operator.\n\nYour task is to derive, implement, and solve a physics-informed learning problem by building a complex-valued neural network of the following form. Let the network be a single-hidden-layer model with complex-valued activation functions and complex-valued parameters. Use hidden units with activation $\\phi(z) = e^{i z}$ where $z$ is an affine function of the inputs, so that a single hidden unit has the form $\\phi_{j}(x,t) = e^{i(k_{j} x - \\omega_{j} t)}$ with real $k_{j}$ and $\\omega_{j}$. The network output is the linear combination\n$$\n\\psi_{\\theta}(x,t) = \\sum_{j=1}^{M} \\alpha_{j} \\, e^{i(k_{j} x - \\omega_{j} t)},\n$$\nwhere $\\theta$ denotes the collection of complex output weights $\\{\\alpha_{j}\\}_{j=1}^{M}$ and fixed hidden parameters $\\{k_{j}, \\omega_{j}\\}_{j=1}^{M}$. Use the fundamental definition of the Hamiltonian operator and the chain rule to compute the residual of the Schrödinger equation for this model, which is\n$$\n\\mathcal{R}(x,t;\\theta) = i \\frac{\\partial \\psi_{\\theta}}{\\partial t}(x,t) + \\frac{1}{2} \\frac{\\partial^{2} \\psi_{\\theta}}{\\partial x^{2}}(x,t).\n$$\nStarting from the core definitions above, derive the residual for each hidden unit and show how the physics informs the architecture via the dispersion relation. Then, pose the physics-informed learning problem as a complex-valued linear least-squares system that simultaneously enforces: \n- the partial differential equation in the interior of the space-time domain by minimizing the squared magnitude of $\\mathcal{R}(x,t;\\theta)$ at collocation points,\n- the initial condition by minimizing $\\left|\\psi_{\\theta}(x, 0) - e^{i k x}\\right|^{2}$ at initial points $x \\in [0, 2\\pi]$,\n- periodic boundary conditions at the domain boundaries by enforcing $\\psi_{\\theta}(0, t) - \\psi_{\\theta}(2\\pi, t) = 0$ and $\\frac{\\partial \\psi_{\\theta}}{\\partial x}(0, t) - \\frac{\\partial \\psi_{\\theta}}{\\partial x}(2\\pi, t) = 0$ for $t \\in [0, T]$.\n\nTo ensure scientific realism and numerical stability, select the hidden parameters $\\{k_{j}\\}_{j=1}^{M}$ as integer wavenumbers to match the periodic domain, and use the physics-based dispersion relation $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$ so that each hidden unit exactly satisfies the Schrödinger equation, thereby making the residual identically zero for each hidden unit. The learning then reduces to determining the complex coefficients $\\{\\alpha_{j}\\}_{j=1}^{M}$ that best satisfy the initial condition and periodic boundary constraints in a least-squares sense. You must implement this as a single, complete program that solves for $\\{\\alpha_{j}\\}_{j=1}^{M}$ by complex least squares and then evaluates the learned solution.\n\nTest Suite and required outputs:\n- Use three test cases with the following parameter sets for the wavenumber $k$ and the final time $T$:\n  - Case $1$: $k = 0$, $T = 1$.\n  - Case $2$: $k = 1$, $T = \\frac{3}{4}$.\n  - Case $3$: $k = 2$, $T = \\frac{1}{2}$.\n- For each case, construct the feature set with $M$ complex units using integer wavenumbers $k_{j}$ uniformly covering $[-K, K]$ for some positive integer $K$ with $M = 2K+1$, and set $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$. Use a sufficient number of initial-condition sample points and periodic-boundary sample points to ensure a well-conditioned least-squares problem. The physics residual must be formulated, but for the chosen features it will evaluate to exactly zero for all collocation points due to the dispersion relation.\n- For each case, after solving for the coefficients, evaluate the learned solution on a uniform space-time grid on $[0, 2\\pi] \\times [0, T]$ and report the relative root-mean-square error \n$$\n\\varepsilon = \\left( \\frac{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|\\psi_{\\theta}(x_{i}, t_{n}) - e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}}{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}} \\right)^{1/2}.\n$$\nSince the exact solution has unit magnitude, the denominator simplifies to the number of evaluation points, but you must compute the expression as defined.\n- Your program must produce a single line of output containing the three errors as a comma-separated Python list enclosed in square brackets, in the order of the cases above, for example, $[\\varepsilon_{1},\\varepsilon_{2},\\varepsilon_{3}]$.\n\nAngle units are not applicable. No physical units are required because of the nondimensionalization. All numerical values in the output must be floating-point numbers. Your implementation must be self-contained and use only the specified libraries. The result for each test case is a single float.", "solution": "The task is to construct and solve a physics-informed neural network for the one-dimensional, time-dependent Schrödinger equation for a free particle. Before proceeding to a solution, a rigorous validation of the problem statement is required.\n\nFirst, I will extract the given information verbatim.\n- **Governing Equation**: The time-dependent Schrödinger equation in nondimensionalized units ($\\hbar=1$, $m=1$) is $i \\frac{\\partial \\psi}{\\partial t} = -\\frac{1}{2}\\frac{\\partial^{2} \\psi}{\\partial x^{2}}$.\n- **Domain**: Space $x \\in [0, 2\\pi]$ and time $t \\in [0, T]$ for $T > 0$.\n- **Boundary Conditions (BCs)**: Periodic boundary conditions are imposed. Specifically, $\\psi(0, t) = \\psi(2\\pi, t)$ and $\\frac{\\partial \\psi}{\\partial x}(0, t) = \\frac{\\partial \\psi}{\\partial x}(2\\pi, t)$.\n- **Initial Condition (IC)**: A plane wave $\\psi(x, 0) = e^{i k x}$ for a prescribed integer wavenumber $k$.\n- **Exact Solution for Verification**: $\\psi(x,t) = e^{i(kx - \\omega t)}$ with dispersion relation $\\omega = \\frac{1}{2}k^{2}$.\n- **Model Ansatz**: A complex-valued, single-hidden-layer network of the form $\\psi_{\\theta}(x,t) = \\sum_{j=1}^{M} \\alpha_{j} \\, e^{i(k_{j} x - \\omega_{j} t)}$, where parameters $\\theta$ are the complex coefficients $\\{\\alpha_j\\}_{j=1}^M$. The hidden parameters $\\{k_{j}, \\omega_{j}\\}_{j=1}^{M}$ are fixed.\n- **PDE Residual**: $\\mathcal{R}(x,t;\\theta) = i \\frac{\\partial \\psi_{\\theta}}{\\partial t}(x,t) + \\frac{1}{2} \\frac{\\partial^{2} \\psi_{\\theta}}{\\partial x^{2}}(x,t)$.\n- **Learning Objective**: Minimize a loss function comprising the PDE residual, the initial condition mismatch, and the boundary condition mismatch, formulated as a complex-valued linear least-squares problem.\n- **Physics-Informed Basis Selection**: The hidden parameters are chosen as integer wavenumbers $k_j$ and frequencies $\\omega_{j}$ satisfying the dispersion relation $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$.\n- **Test Cases**: $(k=0, T=1)$, $(k=1, T=3/4)$, $(k=2, T=1/2)$.\n- **Basis Set Construction**: The set of $M$ wavenumbers $\\{k_j\\}$ is chosen to be integers uniformly covering $[-K, K]$, where $M=2K+1$.\n- **Error Metric**: The relative root-mean-square error $\\varepsilon = \\left( \\frac{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|\\psi_{\\theta}(x_{i}, t_{n}) - e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}}{\\sum_{i=1}^{N_{x}}\\sum_{n=1}^{N_{t}} \\left|e^{i(k x_{i} - \\frac{1}{2}k^{2} t_{n})}\\right|^{2}} \\right)^{1/2}$.\n\nNext, I will validate the problem. The problem is **scientifically grounded** in the fundamental principles of quantum mechanics (Schrödinger equation) and computational mathematics (spectral methods, least-squares approximation). It is **well-posed**, providing a clear objective, a specific model, and sufficient constraints to determine the model parameters. The methodology, while presented under the modern term \"physics-informed neural network,\" is a well-established technique closely related to spectral methods, where basis functions are chosen to satisfy the governing differential equation. The problem is **objective** and all terms are defined precisely. The setup is not incomplete, contradictory, or ambiguous. It presents a concrete, solvable problem in computational physics. Thus, the problem is deemed **valid**.\n\nI will now provide the solution.\n\nThe governing partial differential equation is the free-particle Schrödinger equation in one spatial dimension:\n$$\ni \\frac{\\partial \\psi}{\\partial t} + \\frac{1}{2} \\frac{\\partial^{2} \\psi}{\\partial x^{2}} = 0\n$$\nThe proposed model, or ansatz, for the wave function $\\psi(x,t)$ is a linear combination of complex exponentials:\n$$\n\\psi_{\\theta}(x,t) = \\sum_{j=1}^{M} \\alpha_{j} \\, e^{i(k_{j} x - \\omega_{j} t)}\n$$\nHere, the parameters to be learned are the complex coefficients $\\theta = \\{\\alpha_j\\}_{j=1}^M$. The set of wavenumbers $\\{k_j\\}$ and frequencies $\\{\\omega_j\\}$ are fixed and define the basis functions of the model.\n\nThe core of the physics-informed approach is to enforce the governing equation. We formulate the PDE residual $\\mathcal{R}(x,t;\\theta)$ by substituting the ansatz $\\psi_{\\theta}$ into the Schrödinger equation. First, we compute the required partial derivatives:\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial t} = \\sum_{j=1}^{M} \\alpha_{j} (-i \\omega_{j}) e^{i(k_{j} x - \\omega_{j} t)}\n$$\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial x} = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{i(k_{j} x - \\omega_{j} t)}\n$$\n$$\n\\frac{\\partial^{2} \\psi_{\\theta}}{\\partial x^{2}} = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j})^{2} e^{i(k_{j} x - \\omega_{j} t)} = \\sum_{j=1}^{M} \\alpha_{j} (-k_{j}^{2}) e^{i(k_{j} x - \\omega_{j} t)}\n$$\nSubstituting these into the residual definition gives:\n$$\n\\mathcal{R}(x,t;\\theta) = i \\left( \\sum_{j=1}^{M} \\alpha_{j} (-i \\omega_{j}) e^{i(k_{j} x - \\omega_{j} t)} \\right) + \\frac{1}{2} \\left( \\sum_{j=1}^{M} \\alpha_{j} (-k_{j}^{2}) e^{i(k_{j} x - \\omega_{j} t)} \\right)\n$$\nBy linearity of the operator, we can write this as a sum:\n$$\n\\mathcal{R}(x,t;\\theta) = \\sum_{j=1}^{M} \\alpha_{j} \\left( \\omega_{j} - \\frac{1}{2} k_{j}^{2} \\right) e^{i(k_{j} x - \\omega_{j} t)}\n$$\nThe problem statement mandates a specific physics-informed choice for the basis functions: each pair $(k_j, \\omega_j)$ must satisfy the dispersion relation of a free particle, which is $\\omega_{j} = \\frac{1}{2}k_{j}^{2}$. With this choice, the term in the parentheses becomes $\\left( \\frac{1}{2}k_{j}^{2} - \\frac{1}{2}k_{j}^{2} \\right) = 0$. Consequently, the PDE residual is identically zero for any choice of coefficients $\\alpha_j$:\n$$\n\\mathcal{R}(x,t;\\theta) = 0\n$$\nThis is a critical simplification. The model is constructed such that any linear combination of its basis functions is an exact solution to the Schrödinger equation. The learning problem is thus reduced from a PDE-constrained optimization to one that must only satisfy the initial and boundary conditions.\n\nNext, we analyze the periodic boundary conditions. The domain is $x \\in [0, 2\\pi]$. The first condition is $\\psi_{\\theta}(0, t) = \\psi_{\\theta}(2\\pi, t)$. Evaluating the model at the boundaries:\n$$\n\\psi_{\\theta}(0, t) = \\sum_{j=1}^{M} \\alpha_{j} e^{-i \\omega_{j} t}\n$$\n$$\n\\psi_{\\theta}(2\\pi, t) = \\sum_{j=1}^{M} \\alpha_{j} e^{i(k_{j} 2\\pi - \\omega_{j} t)} = \\sum_{j=1}^{M} \\alpha_{j} e^{i k_{j} 2\\pi} e^{-i \\omega_{j} t}\n$$\nThe problem specifies that the wavenumbers $\\{k_j\\}$ are integers. For any integer $k_j$, Euler's formula gives $e^{i k_{j} 2\\pi} = \\cos(2\\pi k_j) + i \\sin(2\\pi k_j) = 1$. Therefore, $\\psi_{\\theta}(0, t) = \\psi_{\\theta}(2\\pi, t)$ is automatically satisfied for any choice of $\\alpha_j$.\n\nThe second boundary condition is on the spatial derivative, $\\frac{\\partial \\psi_{\\theta}}{\\partial x}(0, t) = \\frac{\\partial \\psi_{\\theta}}{\\partial x}(2\\pi, t)$. Evaluating the derivative:\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial x}(0, t) = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{-i \\omega_{j} t}\n$$\n$$\n\\frac{\\partial \\psi_{\\theta}}{\\partial x}(2\\pi, t) = \\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{i(k_{j} 2\\pi - \\omega_{j} t)} = \\left(\\sum_{j=1}^{M} \\alpha_{j} (i k_{j}) e^{-i \\omega_{j} t}\\right) e^{i k_j 2\\pi}\n$$\nAgain, since $k_j$ is an integer, $e^{i k_j 2\\pi} = 1$, and this boundary condition is also automatically satisfied.\n\nThe problem has now been reduced to satisfying only the initial condition, $\\psi(x, 0) = e^{i k x}$, in a least-squares sense. At time $t=0$, our model becomes:\n$$\n\\psi_{\\theta}(x, 0) = \\sum_{j=1}^{M} \\alpha_{j} e^{i k_{j} x}\n$$\nWe must find the complex coefficients $\\{\\alpha_j\\}$ that minimize the squared error between the model prediction and the true initial state over a set of $N_{IC}$ collocation points $\\{x_p\\}_{p=1}^{N_{IC}}$ in the domain $[0, 2\\pi]$. The objective is to minimize:\n$$\nL(\\theta) = \\sum_{p=1}^{N_{IC}} \\left| \\psi_{\\theta}(x_p, 0) - e^{i k x_p} \\right|^2 = \\sum_{p=1}^{N_{IC}} \\left| \\sum_{j=1}^{M} \\alpha_{j} e^{i k_{j} x_p} - e^{i k x_p} \\right|^2\n$$\nThis is a standard complex-valued linear least-squares problem of the form $\\| A\\mathbf{a} - \\mathbf{b} \\|_2^2$, where:\n- $\\mathbf{a}$ is the column vector of unknown coefficients $[\\alpha_1, \\alpha_2, \\ldots, \\alpha_M]^T$.\n- $A$ is an $N_{IC} \\times M$ matrix, known as the design matrix, with elements $A_{pj} = e^{i k_j x_p}$.\n- $\\mathbf{b}$ is a column vector of length $N_{IC}$ with elements $b_p = e^{i k x_p}$.\n\nTo solve this, we choose a set of basis wavenumbers $k_j \\in \\{-K, -K+1, \\ldots, K\\}$ for a total of $M=2K+1$ functions. The target initial condition has wavenumber $k$. As long as $k$ is included in our set $\\{k_j\\}$, the basis is capable of representing the initial condition perfectly. The least-squares solution should yield $\\alpha_j = 1$ for the basis function where $k_j=k$, and $\\alpha_j=0$ for all other $j$. The numerical solution obtained via `numpy.linalg.lstsq` will approximate this ideal result to high precision.\n\nThe implementation will proceed as follows:\n1. For each test case $(k, T)$, define the basis set $\\{k_j\\}$ and corresponding $\\{\\omega_j\\}$.\n2. Generate $N_{IC}$ collocation points $\\{x_p\\}$ for the initial condition.\n3. Construct the matrix $A$ and vector $\\mathbf{b}$ as defined above.\n4. Solve the system $A\\mathbf{a} \\approx \\mathbf{b}$ for the vector of coefficients $\\mathbf{a}$.\n5. Construct the learned solution $\\psi_{\\theta}$ over a fine space-time grid using the computed coefficients.\n6. Construct the exact solution $\\psi_{exact}$ on the same grid.\n7. Compute the relative root-mean-square error $\\varepsilon$ and report it.\nThis procedure constitutes a complete and correct solution to the posed problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves a physics-informed model for the time-dependent\n    Schrödinger equation using a complex-valued linear least-squares approach.\n    \"\"\"\n    \n    # Test cases: (k, T) where k is the initial wavenumber and T is the final time.\n    test_cases = [\n        (0, 1.0),\n        (1, 0.75),\n        (2, 0.5)\n    ]\n\n    results = []\n\n    # --- Model and Solver Configuration ---\n    # K defines the range of integer wavenumbers [-K, K] for the basis.\n    # M = 2*K + 1 is the total number of basis functions.\n    K = 10\n    M = 2 * K + 1\n\n    # Number of collocation points for the initial condition.\n    # Should be >= M for a well-posed least-squares problem.\n    N_IC = 101\n\n    # Number of points for the final error evaluation grid.\n    N_x_eval = 256\n    N_t_eval = 101\n    \n    # Define the basis wavenumbers {k_j}\n    k_j = np.arange(-K, K + 1)\n    \n    # Define the initial condition collocation points {x_p} on [0, 2*pi]\n    x_p = np.linspace(0, 2 * np.pi, N_IC, endpoint=False)\n\n    # Construct the design matrix A for the least-squares problem.\n    # The matrix A is independent of the test cases.\n    # A_pj = exp(i * k_j * x_p)\n    A = np.exp(1j * np.outer(x_p, k_j))\n\n    for k_target, T in test_cases:\n        # --- Step 1: Formulate and Solve the Least-Squares Problem ---\n\n        # The problem reduces to fitting the initial condition psi(x, 0) = exp(i*k*x).\n        # We solve A * alpha = b for the complex coefficients alpha.\n        \n        # Construct the target vector b for the initial condition.\n        # b_p = exp(i * k_target * x_p)\n        b = np.exp(1j * k_target * x_p)\n\n        # Solve the complex-valued linear least-squares system.\n        # This finds the coefficients 'alpha' that minimize ||A*alpha - b||^2.\n        alpha, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n\n        # --- Step 2: Evaluate the Learned Solution and Compute Error ---\n\n        # Create the space-time grid for evaluation.\n        x_eval = np.linspace(0, 2 * np.pi, N_x_eval)\n        t_eval = np.linspace(0, T, N_t_eval)\n        x_grid, t_grid = np.meshgrid(x_eval, t_eval)\n\n        # Calculate the frequencies omega_j from the dispersion relation.\n        omega_j = 0.5 * k_j**2\n\n        # Construct the learned solution psi_theta(x, t) on the evaluation grid.\n        # psi_theta is a linear combination of basis functions with the learned coefficients.\n        psi_theta = np.zeros_like(x_grid, dtype=np.complex128)\n        for j in range(M):\n            # Basis function: exp(i * (k_j * x - omega_j * t))\n            basis_func = np.exp(1j * (k_j[j] * x_grid - omega_j[j] * t_grid))\n            psi_theta += alpha[j] * basis_func\n\n        # Construct the exact solution on the evaluation grid.\n        omega_target = 0.5 * k_target**2\n        psi_exact = np.exp(1j * (k_target * x_grid - omega_target * t_grid))\n\n        # --- Step 3: Calculate the Relative Root-Mean-Square Error (RRMSE) ---\n        \n        # Numerator of the error formula\n        error_numerator = np.sum(np.abs(psi_theta - psi_exact)**2)\n\n        # Denominator of the error formula\n        error_denominator = np.sum(np.abs(psi_exact)**2)\n\n        # Calculate the relative RMSE.\n        # The denominator simplifies to N_x_eval * N_t_eval since |psi_exact| = 1,\n        # but we compute it as per the formula for correctness.\n        if error_denominator == 0:\n            # Handle the case of a zero denominator, though unlikely here.\n            relative_error = np.sqrt(error_numerator)\n        else:\n            relative_error = np.sqrt(error_numerator / error_denominator)\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2427209"}]}