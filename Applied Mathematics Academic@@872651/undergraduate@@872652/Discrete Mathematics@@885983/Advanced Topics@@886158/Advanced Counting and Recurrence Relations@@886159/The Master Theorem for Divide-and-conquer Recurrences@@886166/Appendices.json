{"hands_on_practices": [{"introduction": "The Master Theorem is a powerful tool for quickly assessing the efficiency of divide-and-conquer algorithms. This first exercise [@problem_id:1408675] presents a classic scenario: choosing between two competing algorithms based on their recurrence relations. By applying the theorem, you will learn to dissect the components of a recurrence—the number of subproblems, their size, and the combination cost—to make an informed decision about asymptotic performance.", "problem": "A distributed computing firm is developing a new service and is evaluating two competing algorithms, Algorithm A and Algorithm B, for a core processing task on a dataset of size $n$.\n\nAlgorithm A is a divide-and-conquer algorithm that splits the problem into 2 subproblems of size $n/2$, solves them recursively, and combines the results. The combination step takes a time proportional to $n^2$. Its running time, $T_A(n)$, can be described by the recurrence relation $T_A(n) = 2T_A(n/2) + n^2$.\n\nAlgorithm B is another divide-and-conquer algorithm that generates 7 subproblems, each on a dataset of size $n/3$, and solves them recursively. Its combining step also takes time proportional to $n^2$. Its running time, $T_B(n)$, is described by the recurrence $T_B(n) = 7T_B(n/3) + n^2$.\n\nFor the purpose of asymptotic analysis, you can ignore base cases and proportionality constants. Based on these recurrences, which of the following statements correctly describes the long-term efficiency relationship between the two algorithms?\n\nA. Algorithm A is asymptotically more efficient than Algorithm B. In mathematical terms, $T_A(n) \\in o(T_B(n))$.\n\nB. Algorithm B is asymptotically more efficient than Algorithm A. In mathematical terms, $T_B(n) \\in o(T_A(n))$.\n\nC. Both algorithms have the same asymptotic time complexity. In mathematical terms, $T_A(n) \\in \\Theta(T_B(n))$.\n\nD. The Master Theorem is not applicable to at least one of these recurrences, so their complexities cannot be determined using it.\n\nE. The asymptotic behavior of one or both algorithms cannot be determined from the information given.", "solution": "We analyze each recurrence using the Master Theorem. For a recurrence of the form $T(n)=a\\,T(n/b)+f(n)$, define $d=\\log_{b}(a)$ and compare $f(n)$ to $n^{d}$. If $f(n)=\\Omega\\!\\left(n^{d+\\epsilon}\\right)$ for some $\\epsilon0$ and the regularity condition $a\\,f(n/b)\\leq c\\,f(n)$ for some constant $c1$ holds for all sufficiently large $n$, then $T(n)=\\Theta\\!\\left(f(n)\\right)$.\n\nFor Algorithm A, we have $a=2$, $b=2$, and $f(n)=n^{2}$. Then\n$$\nd=\\log_{2}(2)=1,\\quad f(n)=n^{2}=\\Omega\\!\\left(n^{1+\\epsilon}\\right)\\ \\text{with}\\ \\epsilon=1.\n$$\nCheck the regularity condition:\n$$\na\\,f(n/b)=2\\left(\\frac{n}{2}\\right)^{2}=\\frac{n^{2}}{2}\\leq c\\,n^{2}\\ \\text{with}\\ c=\\frac{1}{2}1.\n$$\nTherefore, by the Master Theorem, \n$$\nT_{A}(n)=\\Theta\\!\\left(n^{2}\\right).\n$$\n\nFor Algorithm B, we have $a=7$, $b=3$, and $f(n)=n^{2}$. Then\n$$\nd=\\log_{3}(7),\\quad \\text{and since }2\\log_{3}(7),\\ f(n)=n^{2}=\\Omega\\!\\left(n^{\\log_{3}(7)+\\epsilon}\\right)\\ \\text{with}\\ \\epsilon=2-\\log_{3}(7)0.\n$$\nCheck the regularity condition:\n$$\na\\,f(n/b)=7\\left(\\frac{n}{3}\\right)^{2}=\\frac{7}{9}\\,n^{2}\\leq c\\,n^{2}\\ \\text{with}\\ c=\\frac{7}{9}1.\n$$\nTherefore, by the Master Theorem,\n$$\nT_{B}(n)=\\Theta\\!\\left(n^{2}\\right).\n$$\n\nSince both $T_{A}(n)$ and $T_{B}(n)$ are $\\Theta\\!\\left(n^{2}\\right)$, it follows that $T_{A}(n)\\in\\Theta\\!\\left(T_{B}(n)\\right)$. Hence the correct choice is C, and the Master Theorem is applicable to both recurrences.", "answer": "$$\\boxed{C}$$", "id": "1408675"}, {"introduction": "Beyond simply comparing distinct algorithms, a crucial skill is understanding the impact of optimization. This practice problem [@problem_id:1408700] simulates a common situation in algorithm development where a specific part of the process, the combination step, is improved. You will analyze the \"before\" and \"after\" scenarios, applying different cases of the Master Theorem to quantify the precise asymptotic speedup achieved through this optimization.", "problem": "A team of computational scientists is developing an algorithm to simulate the evolution of a complex N-body system, where $n$ is the number of bodies. The initial version of their algorithm uses a divide-and-conquer strategy. The problem of size $n$ is recursively broken down into 27 subproblems, each of size $n/3$. The work required to combine the results of these subproblems is dominated by a complex matrix-vector multiplication step, taking a time proportional to $n^4$. The total running time of this initial algorithm, denoted as $T_{old}(n)$, can thus be described by the recurrence relation $T_{old}(n) = 27 T_{old}(n/3) + f(n)$, where $f(n)$ is in $\\Theta(n^4)$.\n\nAfter a breakthrough in numerical methods, the team manages to optimize the combination step. They replace the original procedure with a more efficient one that takes time proportional to $n^3$. This leads to a new, optimized algorithm with a running time $T_{new}(n)$, described by the recurrence $T_{new}(n) = 27 T_{new}(n/3) + g(n)$, where $g(n)$ is in $\\Theta(n^3)$.\n\nYour task is to quantify the improvement achieved by this optimization. Calculate the asymptotic speedup, which is defined as the ratio of the old running time to the new running time, $S(n) = \\frac{T_{old}(n)}{T_{new}(n)}$. Express your answer for $S(n)$ using asymptotic $\\Theta$-notation.", "solution": "Consider a general divide-and-conquer recurrence of the form $T(n)=a\\,T(n/b)+f(n)$. Let $n^{\\log_{b}(a)}$ be the critical exponent.\n\nHere $a=27$ and $b=3$, so\n$$\nn^{\\log_{3}(27)}=n^{3}.\n$$\n\nFor the old algorithm, $T_{old}(n)=27\\,T_{old}(n/3)+f(n)$ with $f(n)\\in\\Theta(n^{4})$. Since $f(n)=\\Theta\\!\\left(n^{3+\\epsilon}\\right)$ with $\\epsilon=10$, and the regularity condition holds because\n$$\na\\,f(n/b)=27\\cdot\\Theta\\!\\left((n/3)^{4}\\right)=\\Theta\\!\\left(\\frac{n^{4}}{3}\\right)\\le c\\,\\Theta(n^{4})\n$$\nfor some constant $c1$, the Master Theorem (case 3) gives\n$$\nT_{old}(n)\\in\\Theta(n^{4}).\n$$\n\nFor the new algorithm, $T_{new}(n)=27\\,T_{new}(n/3)+g(n)$ with $g(n)\\in\\Theta(n^{3})=\\Theta\\!\\left(n^{\\log_{3}(27)}\\right)$. By the Master Theorem (case 2),\n$$\nT_{new}(n)\\in\\Theta\\!\\left(n^{\\log_{3}(27)}\\ln n\\right)=\\Theta\\!\\left(n^{3}\\ln n\\right).\n$$\n\nTherefore, the asymptotic speedup is\n$$\nS(n)=\\frac{T_{old}(n)}{T_{new}(n)}=\\Theta\\!\\left(\\frac{n^{4}}{n^{3}\\ln n}\\right)=\\Theta\\!\\left(\\frac{n}{\\ln n}\\right).\n$$", "answer": "$$\\boxed{\\Theta\\!\\left(\\frac{n}{\\ln n}\\right)}$$", "id": "1408700"}, {"introduction": "Sophisticated algorithms are often composed of other algorithms, creating nested layers of complexity. This final practice [@problem_id:1408677] challenges you to analyze such a hierarchical structure, where the combination cost of one divide-and-conquer algorithm is determined by a second, independent divide-and-conquer subroutine. This exercise demonstrates how to systematically apply the Master Theorem in a multi-step process, a skill essential for analyzing complex, real-world computational systems.", "problem": "An algorithm designer is developing a novel computational method called the Hierarchical Data Fusion (HDF) algorithm, intended for processing large-scale structured data. For an input of size $n$, the HDF algorithm operates as follows: it divides the data into two subproblems of size $n/2$, recursively calls itself on each subproblem, and then combines the results.\n\nThe combination step is computationally intensive and is handled by a separate subroutine, the Cross-Correlation Aligner (CCA). The running time of the CCA for an input of size $n$, denoted as $C(n)$, itself follows a divide-and-conquer strategy. The CCA algorithm divides its task into three subproblems of size $n/3$, recursively solves them, and then merges the results in a time that is linear with respect to the input size $n$.\n\nLet $T(n)$ be the total running time of the HDF algorithm on an input of size $n$, and let $C(n)$ be the running time of the CCA subroutine which constitutes the combination cost for HDF. The running times are described by the following recurrence relations for $n  1$, assuming that $T(1)$ and $C(1)$ are constant.\n\nThe HDF recurrence is:\n$T(n) = 2T(n/2) + C(n)$\n\nThe CCA recurrence is:\n$C(n) = 3C(n/3) + f(n)$, where $f(n) = \\Theta(n)$.\n\nDetermine the tightest asymptotic bound for the running time $T(n)$ of the HDF algorithm. Choose from the options below.\n\nA. $\\Theta(n^2)$\n\nB. $\\Theta(n \\log \\log n)$\n\nC. $\\Theta(n (\\log n)^2)$\n\nD. $\\Theta(n \\log n)$\n\nE. $\\Theta(n^{\\log_2 3})$", "solution": "We are given two recurrences. For $n1$,\n$$\nT(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+C(n),\\qquad C(n)=3\\,C\\!\\left(\\frac{n}{3}\\right)+f(n),\\quad f(n)=\\Theta(n),\n$$\nwith $T(1)$ and $C(1)$ constant. We first solve $C(n)$, then substitute into $T(n)$.\n\nTo solve $C(n)$, apply the Master Theorem with $a=3$, $b=3$, and $f(n)=\\Theta(n)$. We compute\n$$\nn^{\\log_{3}3}=n.\n$$\nThus $f(n)=\\Theta\\!\\big(n^{\\log_{3}3}\\big)$, which is Master Theorem case 2. Therefore,\n$$\nC(n)=\\Theta\\!\\big(n\\log_{3}n\\big).\n$$\nSince logarithms of different bases differ by a constant factor, $\\log_{3}n=\\frac{\\log_{2}n}{\\log_{2}3}$, we can equivalently write\n$$\nC(n)=\\Theta\\!\\big(n\\log_{2}n\\big).\n$$\n\nNow substitute this into the recurrence for $T(n)$:\n$$\nT(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+\\Theta\\!\\big(n\\log_{2}n\\big).\n$$\nHere $a=2$, $b=2$, so $n^{\\log_{2}2}=n$. The toll term satisfies\n$$\nf(n)=\\Theta\\!\\big(n\\log_{2}n\\big)=\\Theta\\!\\big(n^{\\log_{2}2}\\,(\\log_{2}n)^{1}\\big).\n$$\nThis matches the extended Master Theorem case where $f(n)=\\Theta\\!\\big(n^{\\log_{b}a}(\\log n)^{k}\\big)$ with $k=1$. Hence,\n$$\nT(n)=\\Theta\\!\\big(n^{\\log_{2}2}(\\log_{2}n)^{k+1}\\big)=\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big).\n$$\n\nFor completeness, a recursion-tree summation yields the same result. At level $i$ of the $T$-recursion ($0\\le i\\le \\log_{2}n-1$), there are $2^{i}$ nodes, each of size $n/2^{i}$, and each contributes a combine cost $C(n/2^{i})=\\Theta\\!\\big(\\frac{n}{2^{i}}\\log_{2}\\frac{n}{2^{i}}\\big)$. The total combine cost at level $i$ is\n$$\n2^{i}\\cdot \\Theta\\!\\left(\\frac{n}{2^{i}}\\log_{2}\\frac{n}{2^{i}}\\right)=\\Theta\\!\\big(n(\\log_{2}n-i)\\big).\n$$\nSumming over all levels with $L=\\log_{2}n$,\n$$\n\\sum_{i=0}^{L-1}\\Theta\\!\\big(n(\\log_{2}n-i)\\big)\n=\\Theta\\!\\left(n\\sum_{i=0}^{L-1}(L-i)\\right)\n=\\Theta\\!\\left(n\\cdot \\frac{L(L+1)}{2}\\right)\n=\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big).\n$$\nThe leaves contribute $\\Theta(n)$, which is asymptotically dominated by $\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big)$. Therefore,\n$$\nT(n)=\\Theta\\!\\big(n(\\log_{2}n)^{2}\\big)=\\Theta\\!\\big(n(\\log n)^{2}\\big).\n$$\n\nAmong the given options, this matches option C.", "answer": "$$\\boxed{C}$$", "id": "1408677"}]}