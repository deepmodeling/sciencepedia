{"hands_on_practices": [{"introduction": "The value function in optimal control represents the minimum possible cost we can achieve. Before we can tackle minimization, it is essential to understand how to calculate the cost for any given control strategy. This exercise [@problem_id:3080738] provides foundational practice by having you compute a cost functional for a simple, pre-defined control policy (in this case, doing nothing), which solidifies the link between the abstract value function and the concrete expectation of integrated and terminal costs.", "problem": "Consider the cost functional in stochastic optimal control defined for a control process $u=\\{u_{s}\\}_{s \\in [t,T]}$ by\n$$\nJ^{u}(t,x) \\;=\\; \\mathbb{E}\\!\\left[ \\int_{t}^{T} f(s, X_{s}^{t,x,u}, u_{s}) \\,\\mathrm{d}s \\;+\\; g\\!\\left(X_{T}^{t,x,u}\\right) \\right],\n$$\nwhere $f(s,x,u)$ is the running cost, $g(x)$ is the terminal cost, $\\mathbb{E}[\\cdot]$ denotes expectation, and $X_{s}^{t,x,u}$ is the state process starting from $X_{t}^{t,x,u}=x$. Assume the state process is the uncontrolled Ornstein–Uhlenbeck diffusion on $[0,T]$,\n$$\n\\mathrm{d}X_{s} \\;=\\; -\\alpha\\, X_{s}\\,\\mathrm{d}s \\;+\\; \\sigma\\, \\mathrm{d}W_{s}, \\qquad X_{0}=x,\n$$\nwhere $\\alpha0$, $\\sigma0$ are constants, and $W_{s}$ is a standard one-dimensional Brownian motion. Take the running cost and terminal cost to be\n$$\nf(s,x,u) = x^{2} + u^{2}, \\qquad g(x) = x^{2}.\n$$\nFor the identically zero control $u \\equiv 0$, compute the closed-form expression of $J^{u}(0,x)$ in terms of $\\alpha$, $\\sigma$, $T$, and $x$. Your final answer must be a single analytic expression. No rounding is required.", "solution": "The task is to compute the cost functional $J^{u}(0,x)$ for the specific control $u_s \\equiv 0$. The state process $X_s$ starts at time $t=0$ from state $x$. Since the SDE is uncontrolled, the dynamics are independent of $u$.\n\nThe cost functional with $u_s=0$, $t=0$, and $X_0=x$ becomes:\n$$\nJ^{0}(0,x) = \\mathbb{E}\\left[ \\int_{0}^{T} f(s, X_{s}, 0) \\,\\mathrm{d}s + g(X_{T}) \\right]\n$$\nSubstituting the given running and terminal cost functions, $f(s,x,0) = x^2+0^2=x^2$ and $g(x)=x^2$:\n$$\nJ^{0}(0,x) = \\mathbb{E}\\left[ \\int_{0}^{T} (X_{s})^{2} \\,\\mathrm{d}s + (X_{T})^{2} \\right]\n$$\nBy the linearity of expectation and Fubini-Tonelli theorem, we can interchange expectation and integration:\n$$\nJ^{0}(0,x) = \\int_{0}^{T} \\mathbb{E}[(X_{s})^{2}] \\,\\mathrm{d}s + \\mathbb{E}[(X_{T})^{2}]\n$$\nTo proceed, we must find the second moment of the Ornstein-Uhlenbeck process, $\\mathbb{E}[X_s^2]$. The solution to the SDE $\\mathrm{d}X_s = -\\alpha X_s \\mathrm{d}s + \\sigma \\mathrm{d}W_s$ with $X_0=x$ is:\n$$\nX_s = x \\exp(-\\alpha s) + \\sigma \\int_0^s \\exp(-\\alpha(s-\\tau)) \\mathrm{d}W_\\tau\n$$\nThe mean of $X_s$ is $\\mathbb{E}[X_s] = x \\exp(-\\alpha s)$, since the Itô integral has zero expectation. The variance of $X_s$ is found using Itô isometry:\n$$\n\\mathrm{Var}(X_s) = \\mathrm{Var}\\left(\\sigma \\int_0^s \\exp(-\\alpha(s-\\tau)) \\mathrm{d}W_\\tau\\right) = \\sigma^2 \\int_0^s \\exp(-2\\alpha(s-\\tau)) \\mathrm{d}\\tau\n$$\n$$\n\\mathrm{Var}(X_s) = \\sigma^2 \\exp(-2\\alpha s) \\int_0^s \\exp(2\\alpha\\tau) \\mathrm{d}\\tau = \\sigma^2 \\exp(-2\\alpha s) \\left[ \\frac{\\exp(2\\alpha\\tau)}{2\\alpha} \\right]_0^s = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s))\n$$\nThe second moment is given by $\\mathbb{E}[X_s^2] = \\mathrm{Var}(X_s) + (\\mathbb{E}[X_s])^2$:\n$$\n\\mathbb{E}[X_s^2] = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s)) + (x \\exp(-\\alpha s))^2 = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s)) + x^2 \\exp(-2\\alpha s)\n$$\nNow, we compute the integral term of the cost functional:\n$$\n\\int_{0}^{T} \\mathbb{E}[(X_{s})^{2}] \\,\\mathrm{d}s = \\int_{0}^{T} \\left( \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha s)) + x^2 \\exp(-2\\alpha s) \\right) \\mathrm{d}s\n$$\n$$\n= \\frac{\\sigma^2}{2\\alpha} \\int_{0}^{T} (1 - \\exp(-2\\alpha s)) \\mathrm{d}s + x^2 \\int_{0}^{T} \\exp(-2\\alpha s) \\mathrm{d}s\n$$\nThe first integral is $\\int_{0}^{T} (1 - \\exp(-2\\alpha s)) \\mathrm{d}s = T - \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}$. The second is $\\int_{0}^{T} \\exp(-2\\alpha s) \\mathrm{d}s = \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}$.\nSo the integral term is:\n$$\n\\frac{\\sigma^2}{2\\alpha} \\left( T - \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha} \\right) + x^2 \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha} = \\frac{\\sigma^2 T}{2\\alpha} - \\frac{\\sigma^2}{4\\alpha^2}(1 - \\exp(-2\\alpha T)) + \\frac{x^2}{2\\alpha}(1 - \\exp(-2\\alpha T))\n$$\nNext, the terminal cost term is $\\mathbb{E}[X_T^2]$, which is the second moment evaluated at $s=T$:\n$$\n\\mathbb{E}[X_T^2] = \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha T)) + x^2 \\exp(-2\\alpha T)\n$$\nFinally, we sum the two terms to find $J^0(0,x)$:\n$$\nJ^{0}(0,x) = \\left( \\frac{\\sigma^2 T}{2\\alpha} - \\frac{\\sigma^2}{4\\alpha^2}(1 - \\exp(-2\\alpha T)) + \\frac{x^2}{2\\alpha}(1 - \\exp(-2\\alpha T)) \\right) + \\left( \\frac{\\sigma^2}{2\\alpha} (1 - \\exp(-2\\alpha T)) + x^2 \\exp(-2\\alpha T) \\right)\n$$\nGrouping the terms containing $(1 - \\exp(-2\\alpha T))$:\n$$\nJ^{0}(0,x) = \\frac{\\sigma^2 T}{2\\alpha} + x^2 \\exp(-2\\alpha T) + (1-\\exp(-2\\alpha T))\\left(-\\frac{\\sigma^2}{4\\alpha^2} + \\frac{x^2}{2\\alpha} + \\frac{\\sigma^2}{2\\alpha}\\right)\n$$\n$$\nJ^{0}(0,x) = \\frac{\\sigma^2 T}{2\\alpha} + x^2 \\exp(-2\\alpha T) + (1-\\exp(-2\\alpha T))\\left(\\frac{2\\alpha x^2 - \\sigma^2 + 2\\alpha\\sigma^2}{4\\alpha^2}\\right)\n$$\nExpanding and collecting terms:\n$$\nJ^0(0,x) = \\frac{2\\alpha\\sigma^2 T}{4\\alpha^2} + \\frac{4\\alpha^2 x^2 \\exp(-2\\alpha T)}{4\\alpha^2} + \\frac{2\\alpha x^2 + \\sigma^2(2\\alpha-1)}{4\\alpha^2} - \\frac{(2\\alpha x^2 + \\sigma^2(2\\alpha-1))\\exp(-2\\alpha T)}{4\\alpha^2}\n$$\n$$\nJ^0(0,x) = \\frac{1}{4\\alpha^2} \\left[ 2\\alpha\\sigma^2 T + 2\\alpha x^2 + \\sigma^2(2\\alpha-1) + \\exp(-2\\alpha T)\\left(4\\alpha^2 x^2 - 2\\alpha x^2 - \\sigma^2(2\\alpha-1)\\right) \\right]\n$$\n$$\nJ^0(0,x) = \\frac{1}{4\\alpha^2}\\left[ 2\\alpha x^2 + 2\\alpha\\sigma^2 T + \\sigma^2(2\\alpha - 1) + \\left((4\\alpha^2-2\\alpha)x^2 - (2\\alpha-1)\\sigma^2\\right)\\exp(-2\\alpha T) \\right]\n$$\nFactoring $(2\\alpha-1)$ from the exponential term's coefficient:\n$$\nJ^0(0,x) = \\frac{1}{4\\alpha^2}\\left[ 2\\alpha x^2 + 2\\alpha\\sigma^2 T + \\sigma^2(2\\alpha - 1) + \\left(2\\alpha - 1\\right)\\left(2\\alpha x^2 - \\sigma^2\\right)\\exp(-2\\alpha T) \\right]\n$$\nThis is the closed-form expression for the cost functional.", "answer": "$$\n\\boxed{\\frac{1}{4\\alpha^2}\\left[ 2\\alpha x^2 + 2\\alpha\\sigma^2 T + \\sigma^2(2\\alpha - 1) + \\left(2\\alpha - 1\\right)\\left(2\\alpha x^2 - \\sigma^2\\right)\\exp(-2\\alpha T) \\right]}\n$$", "id": "3080738"}, {"introduction": "The Hamilton-Jacobi-Bellman equation is built upon the principle of optimality, which involves making the best possible decision at every instant. This \"best decision\" is found by minimizing the Hamiltonian, a function that combines the instantaneous running cost with the expected future cost rate, captured by the derivatives of the value function. This exercise [@problem_id:3080765] isolates this crucial minimization step, allowing you to derive the optimal control action as a function of the system's state and the value function's gradient.", "problem": "Consider the one-dimensional controlled Itô stochastic differential equation\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\nwith drift $b(x,u) = u$, constant diffusion coefficient $\\sigma  0$, and control process $u_{t}$ taking values in $\\mathbb{R}$. Let the running cost be\n$$\nf(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2},\n$$\nwith $r0$ and $q0$. Define the value function $V(t,x)$ for a finite horizon problem with a bounded and continuous terminal cost $g(x)$ by\n$$\nV(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right],\n$$\nwhere the infimum is over progressively measurable controls taking values in $\\mathbb{R}$. Using the dynamic programming principle and the definition of the Hamiltonian appearing in the Hamilton-Jacobi-Bellman (HJB) equation, the Hamiltonian at a point $(x,p,M)$ is given by the infimum over controls $u \\in \\mathbb{R}$ of the sum of the controlled generator applied to a $C^{2}$ test function and the running cost. In particular, if $p$ denotes the first spatial derivative and $M$ denotes the second spatial derivative, compute:\n- the minimized Hamiltonian as an explicit function of $x$, $p$, $M$, $\\sigma$, $r$, and $q$;\n- the corresponding minimizing state-feedback control as an explicit function of $p$ and $r$.\n\nProvide your final answer as a two-entry row matrix, with the first entry being the minimized Hamiltonian and the second entry being the minimizing feedback control. No numerical approximation is required, and no units are involved.", "solution": "The problem asks for the minimized Hamiltonian and the corresponding optimal control for a given stochastic control problem. The Hamiltonian, in the context of the Hamilton-Jacobi-Bellman (HJB) equation, is given by:\n$$\nH(x,p,M) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^{u}\\phi(x) + f(x,u) \\right\\}\n$$\nwhere $\\mathcal{L}^{u}$ is the generator of the controlled stochastic process, applied to a test function $\\phi(x)$ whose first and second spatial derivatives at $x$ are denoted by $p$ and $M$ respectively. $f(x,u)$ is the running cost.\n\nFirst, we determine the generator $\\mathcal{L}^{u}$. For a general one-dimensional Itô process given by $\\mathrm{d}X_{t} = \\mu(X_{t}, u_t)\\,\\mathrm{d}t + \\Sigma(X_{t}, u_t)\\,\\mathrm{d}W_{t}$, the generator applied to a $C^{2}$ function $\\phi(x)$ is $\\mathcal{L}^{u}\\phi(x) = \\mu(x,u) \\frac{\\partial \\phi}{\\partial x} + \\frac{1}{2} \\Sigma(x,u)^{2} \\frac{\\partial^{2} \\phi}{\\partial x^{2}}$.\n\nIn our specific problem, the drift is $\\mu(x,u) = b(x,u) = u$ and the diffusion is $\\Sigma(x,u) = \\sigma$. Using the notation $p = \\frac{\\partial \\phi}{\\partial x}$ and $M = \\frac{\\partial^2\\phi}{\\partial x^2}$, the generator is:\n$$\n\\mathcal{L}^{u}\\phi(x) = u \\cdot p + \\frac{1}{2}\\sigma^{2} \\cdot M\n$$\nNext, we add the running cost $f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$. The expression to be minimized with respect to the control $u \\in \\mathbb{R}$ is:\n$$\nJ(u) = \\mathcal{L}^{u}\\phi(x) + f(x,u) = \\left( u p + \\frac{1}{2}\\sigma^{2}M \\right) + \\left( \\frac{1}{2} r\\,u^{2} + q\\,x^{2} \\right)\n$$\nWe rearrange this expression as a function of $u$:\n$$\nJ(u) = \\frac{1}{2} r\\,u^{2} + p\\,u + \\left( q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right)\n$$\nThis is a quadratic function of $u$. Since the problem states that $r > 0$, the parabola opens upwards, and the function has a unique global minimum. To find the value of $u$ that minimizes $J(u)$, we compute the first derivative with respect to $u$ and set it to zero (First-Order Condition):\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}u} = \\frac{\\mathrm{d}}{\\mathrm{d}u} \\left( \\frac{1}{2} r\\,u^{2} + p\\,u + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right) = r\\,u + p\n$$\nSetting the derivative to zero gives the optimal control, which we denote as $u^{\\star}$:\n$$\nr\\,u^{\\star} + p = 0 \\implies u^{\\star} = -\\frac{p}{r}\n$$\nThis is the minimizing state-feedback control as a function of $p$ and $r$, the second part of the required answer.\n\nTo find the minimized Hamiltonian, $H(x,p,M)$, we substitute $u^{\\star}$ back into the expression for $J(u)$:\n$$\nH(x,p,M) = J(u^{\\star}) = \\frac{1}{2} r\\left(-\\frac{p}{r}\\right)^{2} + p\\left(-\\frac{p}{r}\\right) + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{1}{2} r\\left(\\frac{p^{2}}{r^{2}}\\right) - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{p^{2}}{2r} - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nCombining the terms involving $p^{2}$:\n$$\nH(x,p,M) = -\\frac{p^{2}}{2r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\nThis is the minimized Hamiltonian, the first part of the required answer.\n\nThe final answer is the row matrix containing the minimized Hamiltonian and the minimizing control.\nFirst entry: $H(x,p,M) = q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M$.\nSecond entry: $u^{\\star} = -\\frac{p}{r}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M  -\\frac{p}{r} \\end{pmatrix}}\n$$", "id": "3080765"}, {"introduction": "With an understanding of the cost functional and the Hamiltonian, we can now solve a complete stochastic control problem. This practice [@problem_id:3080756] guides you through using the HJB equation to find the optimal control law for the classic linear-quadratic regulator problem. By postulating a quadratic form for the value function—a common and powerful technique—you will see how the problem elegantly reduces to solving a Riccati differential equation, yielding the explicit optimal feedback control law.", "problem": "Consider the scalar controlled stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = \\big(a X_t + b\\,u_t\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\quad X_0 = x,\n$$\nwhere $W_t$ is a standard Brownian motion, and $u_t$ is a progressively measurable control taking values in $\\mathbb{R}$. For a fixed finite horizon $Tgt;0$, the performance criterion is\n$$\nJ^{u}(t,x) = \\mathbb{E}\\Bigg[\\int_{t}^{T} \\big(q\\,X_s^2 + r\\,u_s^2\\big)\\,\\mathrm{d}s + s_f\\,X_T^2\\ \\Big|\\ X_t = x\\Bigg].\n$$\nUsing the Dynamic Programming Principle (DPP) and Itô's formula as the foundational starting point, derive the Hamilton-Jacobi-Bellman (HJB) equation for the value function $V(t,x) = \\inf_{u} J^{u}(t,x)$, state the verification theorem in this setting, and from it derive the optimal feedback law $u^{*}(t,x)$.\n\nThen, specialize to the parameter values $a=0$, $b=1$, $\\sigmagt;0$ arbitrary, $q=1$, $r=1$, and terminal weight $s_f=1$, on the time interval $[0,T]$ with $Tgt;0$. Compute explicitly the optimal feedback control $u^{*}(t,x)$ and the resulting closed-loop drift $\\mu_{\\mathrm{cl}}(t,x)$ of the state SDE under $u^{*}$.\n\nYour final answer must consist of the pair $\\big(u^{*}(t,x),\\,\\mu_{\\mathrm{cl}}(t,x)\\big)$ as a single closed-form analytic expression. No numerical rounding is required. No physical units are involved.", "solution": "This problem requires solving a stochastic linear-quadratic regulator (LQR) problem using the Hamilton-Jacobi-Bellman (HJB) framework.\n\nFirst, we derive the HJB equation for the value function $V(t,x)$. By the principle of dynamic programming, if $V$ is sufficiently smooth, it must satisfy:\n$$\n\\sup_{u \\in \\mathbb{R}} \\left\\{ -\\frac{\\partial V}{\\partial t} - \\mathcal{L}^u V(t,x) - (q x^2 + r u^2) \\right\\} = 0\n$$\nwhere $\\mathcal{L}^u$ is the infinitesimal generator of the process. This is equivalent to:\n$$\n\\frac{\\partial V}{\\partial t} + \\inf_{u \\in \\mathbb{R}} \\left\\{ (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + r u^2 \\right\\} = 0\n$$\nThe expression inside the infimum is a quadratic function of $u$. To find the minimum, we take the derivative with respect to $u$ and set it to zero:\n$$\nb \\frac{\\partial V}{\\partial x} + 2ru = 0\n$$\nSince $r>0$, we can solve for the optimal feedback control $u^{*}(t,x)$:\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}\n$$\nSubstituting this back into the HJB equation yields the PDE for the value function:\n$$\n\\frac{\\partial V}{\\partial t} + a x\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 - \\frac{b^2}{4r}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nThis PDE must be solved subject to the terminal condition $V(T,x) = s_f x^2$.\n\nNow, we specialize to the parameters $a=0$, $b=1$, $q=1$, $r=1$, and $s_f=1$. The HJB equation becomes:\n$$\n\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + x^2 - \\frac{1}{4}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\nwith the terminal condition $V(T,x) = x^2$. Given the quadratic structure of the problem, we propose a quadratic solution for the value function:\n$$\nV(t,x) = P(t)x^2 + S(t)\n$$\nThe partial derivatives are: $\\frac{\\partial V}{\\partial t} = \\dot{P}(t)x^2 + \\dot{S}(t)$, $\\frac{\\partial V}{\\partial x} = 2P(t)x$, and $\\frac{\\partial^2 V}{\\partial x^2} = 2P(t)$.\nSubstituting these into the specialized HJB equation:\n$$\n(\\dot{P}(t)x^2 + \\dot{S}(t)) + \\frac{1}{2}\\sigma^2(2P(t)) + x^2 - \\frac{1}{4}(2P(t)x)^2 = 0\n$$\n$$\n\\dot{P}(t)x^2 + \\dot{S}(t) + \\sigma^2 P(t) + x^2 - P(t)^2 x^2 = 0\n$$\nFor this equation to hold for all $x$, the coefficients of the powers of $x^2$ and the constant terms must be zero independently. This gives a system of two ordinary differential equations (ODEs):\n1.  (Coefficient of $x^2$): $\\dot{P}(t) - P(t)^2 + 1 = 0 \\implies \\dot{P}(t) = P(t)^2 - 1$\n2.  (Constant term): $\\dot{S}(t) + \\sigma^2 P(t) = 0 \\implies \\dot{S}(t) = -\\sigma^2 P(t)$\n\nFrom the terminal condition $V(T,x) = 1 \\cdot x^2$, we deduce the terminal conditions for the ODEs: $P(T)=1$ and $S(T)=0$.\n\nWe solve the Riccati equation for $P(t)$. Observe that $P(t) = 1$ is a constant (equilibrium) solution since $\\dot{P} = 1^2 - 1 = 0$. Since this constant solution satisfies the terminal condition $P(T)=1$, by the uniqueness of solutions to ODEs, it is the unique solution: $P(t) = 1$ for all $t \\in [0,T]$.\n\nNow, we solve for $S(t)$ by substituting $P(t)=1$ into its ODE:\n$$\n\\dot{S}(t) = -\\sigma^2\n$$\nIntegrating with respect to $t$ gives $S(t) = -\\sigma^2 t + C$. Using the terminal condition $S(T)=0$:\n$$\nS(T) = -\\sigma^2 T + C = 0 \\implies C = \\sigma^2 T\n$$\nThus, $S(t) = \\sigma^2 T - \\sigma^2 t = \\sigma^2(T-t)$. The full value function is $V(t,x) = x^2 + \\sigma^2(T-t)$.\n\nThe optimal feedback control is:\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x} = -\\frac{1}{2(1)} (2P(t)x) = -P(t)x\n$$\nSince $P(t)=1$, we have:\n$$\nu^{*}(t,x) = -x\n$$\nThe closed-loop drift, $\\mu_{\\mathrm{cl}}(t,x)$, is found by substituting $u^*(t,x)$ into the drift of the SDE:\n$$\n\\mu_{\\mathrm{cl}}(t,x) = a x + b u^{*}(t,x) = (0)x + (1)(-x) = -x\n$$\nTherefore, the optimal control and the resulting closed-loop drift are both $-x$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -x  -x \\end{pmatrix}}\n$$", "id": "3080756"}]}