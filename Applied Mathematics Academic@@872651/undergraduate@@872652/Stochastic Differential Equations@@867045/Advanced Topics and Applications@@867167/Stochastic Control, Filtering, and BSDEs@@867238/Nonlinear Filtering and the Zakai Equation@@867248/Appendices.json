{"hands_on_practices": [{"introduction": "The Zakai equation governs the evolution of the unnormalized posterior density. Its structure elegantly separates the signal's intrinsic dynamics from the information gained through observation. This practice provides a direct, hands-on calculation of these two key components: the action of the signal's generator, $\\mathcal{L}\\phi(x)$, and the observation update term, $\\phi(x)h(x)$. Mastering this calculation [@problem_id:3068689] is the first step toward understanding how the filter processes information over time.", "problem": "Consider a one-dimensional signal-observation model used in nonlinear filtering. Let the signal process $\\{X_{t}\\}_{t \\geq 0}$ satisfy the stochastic differential equation $dX_{t} = a(X_{t})\\,dt + \\sigma(X_{t})\\,dW_{t}$, where $\\{W_{t}\\}_{t \\geq 0}$ is a standard Brownian motion. The observation process $\\{Y_{t}\\}_{t \\geq 0}$ is given by $dY_{t} = h(X_{t})\\,dt + dV_{t}$, where $\\{V_{t}\\}_{t \\geq 0}$ is an independent standard Brownian motion. Assume the drift, diffusion, and observation functions are\n$$\na(x) = \\alpha - \\kappa x,\\quad \\sigma(x) = \\sigma_{0}\\,(1 + x^{2}),\\quad h(x) = c\\,x^{2},\n$$\nwith constants $\\alpha  0$, $\\kappa  0$, $\\sigma_{0}  0$, and $c  0$. Let $\\mathcal{L}$ denote the infinitesimal generator of the signal process acting on sufficiently smooth test functions $\\phi$, defined by\n$$\n\\mathcal{L}\\phi(x) = a(x)\\,\\phi'(x) + \\frac{1}{2}\\,\\sigma(x)^{2}\\,\\phi''(x).\n$$\nIn the weak unnormalized filtering formulation known as the Zakai equation, the coefficient of $dt$ for a test function $\\phi$ is given by the unnormalized conditional expectation of $\\mathcal{L}\\phi$, while the coefficient of $dY_{t}$ is given by the unnormalized conditional expectation of $\\phi\\,h$. \n\nFor the concrete test function $\\phi(x) = x^{2}$, compute explicitly:\n- the function $\\mathcal{L}\\phi(x)$,\n- the function $\\phi(x)\\,h(x)$.\n\nProvide your final answer as a single row matrix containing these two expressions in the order $\\big[\\mathcal{L}\\phi(x),\\;\\phi(x)\\,h(x)\\big]$. No numerical evaluation or rounding is required, and no units are involved.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Signal process: $dX_{t} = a(X_{t})\\,dt + \\sigma(X_{t})\\,dW_{t}$\n- Observation process: $dY_{t} = h(X_{t})\\,dt + dV_{t}$\n- Drift function: $a(x) = \\alpha - \\kappa x$\n- Diffusion function: $\\sigma(x) = \\sigma_{0}\\,(1 + x^{2})$\n- Observation function: $h(x) = c\\,x^{2}$\n- Constants: $\\alpha  0$, $\\kappa  0$, $\\sigma_{0}  0$, $c  0$\n- Infinitesimal generator: $\\mathcal{L}\\phi(x) = a(x)\\,\\phi'(x) + \\frac{1}{2}\\,\\sigma(x)^{2}\\,\\phi''(x)$\n- Test function: $\\phi(x) = x^{2}$\n- Task: Compute the functions $\\mathcal{L}\\phi(x)$ and $\\phi(x)\\,h(x)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it is formulated within the established mathematical theory of stochastic differential equations and nonlinear filtering. The Ornstein-Uhlenbeck-type signal process with nonlinear diffusion and the measurement process are standard models in this field. The concept of an infinitesimal generator and its use in the Zakai equation are central to the theory. The problem is well-posed, providing all necessary definitions and functions for a direct computation. The language is objective and precise. The problem is thus deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe task is to compute two functions, $\\mathcal{L}\\phi(x)$ and $\\phi(x)\\,h(x)$, for the given test function $\\phi(x) = x^{2}$ and the specified system dynamics.\n\nFirst, we compute the first and second derivatives of the test function $\\phi(x) = x^{2}$.\nThe first derivative with respect to $x$ is:\n$$\n\\phi'(x) = \\frac{d}{dx}(x^{2}) = 2x\n$$\nThe second derivative with respect to $x$ is:\n$$\n\\phi''(x) = \\frac{d}{dx}(2x) = 2\n$$\n\nNext, we compute the function $\\mathcal{L}\\phi(x)$ using its definition:\n$$\n\\mathcal{L}\\phi(x) = a(x)\\,\\phi'(x) + \\frac{1}{2}\\,\\sigma(x)^{2}\\,\\phi''(x)\n$$\nWe substitute the given expressions for $a(x)$ and $\\sigma(x)$, and the computed derivatives $\\phi'(x)$ and $\\phi''(x)$:\n$$\n\\mathcal{L}\\phi(x) = (\\alpha - \\kappa x)(2x) + \\frac{1}{2}\\,\\left(\\sigma_{0}\\,(1 + x^{2})\\right)^{2}\\,(2)\n$$\nLet's simplify this expression term by term. The first term is:\n$$\n(\\alpha - \\kappa x)(2x) = 2\\alpha x - 2\\kappa x^{2}\n$$\nThe second term is:\n$$\n\\frac{1}{2}\\,\\left(\\sigma_{0}\\,(1 + x^{2})\\right)^{2}\\,(2) = \\sigma_{0}^{2}\\,(1 + x^{2})^{2}\n$$\nExpanding the squared binomial $(1 + x^{2})^{2}$:\n$$\n(1 + x^{2})^{2} = 1^{2} + 2(1)(x^{2}) + (x^{2})^{2} = 1 + 2x^{2} + x^{4}\n$$\nSo, the second term becomes:\n$$\n\\sigma_{0}^{2}\\,(1 + 2x^{2} + x^{4}) = \\sigma_{0}^{2} + 2\\sigma_{0}^{2}x^{2} + \\sigma_{0}^{2}x^{4}\n$$\nNow, we combine the simplified terms to find the full expression for $\\mathcal{L}\\phi(x)$:\n$$\n\\mathcal{L}\\phi(x) = (2\\alpha x - 2\\kappa x^{2}) + (\\sigma_{0}^{2} + 2\\sigma_{0}^{2}x^{2} + \\sigma_{0}^{2}x^{4})\n$$\nWe group the terms by powers of $x$:\n$$\n\\mathcal{L}\\phi(x) = \\sigma_{0}^{2}x^{4} + (2\\sigma_{0}^{2}x^{2} - 2\\kappa x^{2}) + 2\\alpha x + \\sigma_{0}^{2}\n$$\nFactoring out common terms, we arrive at the final expression for the first required function:\n$$\n\\mathcal{L}\\phi(x) = \\sigma_{0}^{2}x^{4} + 2(\\sigma_{0}^{2} - \\kappa)x^{2} + 2\\alpha x + \\sigma_{0}^{2}\n$$\n\nSecond, we compute the function $\\phi(x)\\,h(x)$. We substitute the given expressions for $\\phi(x)$ and $h(x)$:\n$$\n\\phi(x) = x^{2}\n$$\n$$\nh(x) = c\\,x^{2}\n$$\nThe product is:\n$$\n\\phi(x)\\,h(x) = (x^{2})(c\\,x^{2})\n$$\nSimplifying this expression gives the second required function:\n$$\n\\phi(x)\\,h(x) = c\\,x^{4}\n$$\nThe problem asks for the two computed expressions, $\\mathcal{L}\\phi(x)$ and $\\phi(x)\\,h(x)$, to be presented as a single row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sigma_{0}^{2}x^{4} + 2(\\sigma_{0}^{2} - \\kappa)x^{2} + 2\\alpha x + \\sigma_{0}^{2}  c x^{4} \\end{pmatrix}}\n$$", "id": "3068689"}, {"introduction": "While the Kalman filter provides a finite-dimensional solution for linear-Gaussian systems, most nonlinear problems are not so straightforward. A crucial question is whether the posterior distribution remains within a simple family of functions, such as the Gaussian family. This exercise [@problem_id:3068691] challenges you to test this hypothesis for a system with nonlinear drift, revealing why such \"closure\" fails and leading to the fundamental conclusion that the exact nonlinear filter is often infinite-dimensional.", "problem": "Consider a scalar signal process $X_t$ governed by the stochastic differential equation $dX_t = a(X_t)\\,dt + \\sigma\\,dW_t$, where $W_t$ is a standard Brownian motion, $\\sigma0$, and the drift is a cubic polynomial $a(x) = \\theta_1 x + \\theta_3 x^3$ with constants $\\theta_1,\\theta_3 \\in \\mathbb{R}$. The observation process is $dY_t = h(X_t)\\,dt + dV_t$, where $V_t$ is an independent standard Brownian motion and the observation function is linear, $h(x) = c x$, with known $c \\in \\mathbb{R}$. Assume the initial posterior (conditional) density of $X_0$ given $Y$ is Gaussian with mean $m_0$ and variance $s_00$.\n\nLet $\\rho_t(x)$ denote the unnormalized conditional density (posterior) of $X_t$ given observations up to time $t$, which evolves according to the Zakai equation (the unnormalized counterpart of the Kushner–Stratonovich dynamics). The Zakai equation for $\\rho_t$ reads $d\\rho_t(x) = \\mathcal{L}^{\\star}\\rho_t(x)\\,dt + h(x)\\rho_t(x)\\,dY_t$, where $\\mathcal{L}^{\\star}$ is the Fokker–Planck adjoint of the generator associated with $X_t$, given by $\\mathcal{L}^{\\star}\\rho = -\\frac{d}{dx}\\!\\left(a(x)\\rho\\right) + \\frac{1}{2}\\sigma^2\\frac{d^2\\rho}{dx^2}$.\n\nSuppose we test whether the unnormalized posterior remains within the exponential-quadratic family $\\rho_t(x) = \\exp\\!\\left(A_t x^2 + B_t x + C_t\\right)$ for some time-dependent parameters $A_t,B_t,C_t$. Using only the Zakai equation and the form of $\\mathcal{L}^{\\star}$, expand the deterministic ($dt$) part that results from substituting the exponential-quadratic ansatz into the Zakai equation and identify the coefficient of the highest-degree monomial in $x$ that appears in the $dt$-term at $t=0$ when the initial parameters $A_0,B_0,C_0$ are selected to match the given Gaussian posterior $N(m_0,s_0)$. Express your final answer as a single closed-form analytic expression in terms of $\\theta_3$ and $s_0$ only. No numerical approximation is required.", "solution": "The user wants me to solve a problem related to nonlinear filtering and the Zakai equation.\n\nHere's my plan:\n1.  **Validate the Problem:**\n    *   **Step 1: Extract Givens.** I'll list all the equations, definitions, and conditions provided in the problem statement.\n    *   **Step 2: Validate Using Extracted Givens.** I'll check for scientific soundness, well-posedness, objectivity, and other criteria. The problem is in the domain of stochastic differential equations and filtering theory. It involves standard models (SDE for the signal, linear observation) and the well-established Zakai equation. The question asks for a specific calculation based on a given ansatz. This seems like a standard, albeit complex, textbook-style problem.\n    *   **Step 3: Verdict and Action.** Assuming the problem is valid, I will proceed to the solution.\n\n2.  **Solve the Problem (if valid):**\n    *   **Goal:** Find the coefficient of the highest-degree monomial in `x` in the `dt` part of the Zakai equation, evaluated at `t=0`, given a specific ansatz.\n    *   **Ansatz:** The unnormalized posterior is assumed to be of the form `ρ_t(x) = exp(A_t x^2 + B_t x + C_t)`.\n    *   **Zakai Equation:** `dρ_t = L*ρ_t dt + h(x)ρ_t dY_t`.\n    *   **Fokker-Planck Adjoint:** `L*ρ = -d/dx(a(x)ρ) + (1/2)σ^2 d^2/dx^2(ρ)`.\n    *   **Signal Dynamics Drift:** `a(x) = θ_1 x + θ_3 x^3`.\n    *   **Observation Function:** `h(x) = c x`.\n\n    *   **Step-by-step Calculation:**\n        1.  I need to compute the `L*ρ_t` term using the given `a(x)` and the ansatz for `ρ_t(x)`.\n        2.  The problem asks me to focus *only* on the deterministic (`dt`) part. This means I need to calculate `L*ρ_t`. The term `h(x)ρ_t dY_t` is part of the stochastic update and can be ignored for this specific question.\n        3.  Let `ρ(x) = exp(A x^2 + B x + C)`. I'll drop the time subscript `t` for now to simplify notation.\n        4.  Calculate the derivatives of `ρ(x)`:\n            *   `dρ/dx = (2Ax + B) ρ(x)`\n            *   `d^2ρ/dx^2 = [2A + (2Ax + B)^2] ρ(x) = [2A + 4A^2x^2 + 4ABx + B^2] ρ(x)`\n        5.  Calculate the term `-d/dx(a(x)ρ)`:\n            *   `a(x)ρ = (θ_1 x + θ_3 x^3) ρ(x)`\n            *   `-d/dx(a(x)ρ) = - [ d/dx(a(x)) * ρ + a(x) * dρ/dx ]`\n            *   `d/dx(a(x)) = θ_1 + 3θ_3 x^2`\n            *   So, `-d/dx(a(x)ρ) = - [ (θ_1 + 3θ_3 x^2)ρ + (θ_1 x + θ_3 x^3)(2Ax + B)ρ ]`\n            *   Expanding the second part inside the brackets: `(θ_1 x + θ_3 x^3)(2Ax + B) = 2Aθ_1 x^2 + Bθ_1 x + 2Aθ_3 x^4 + Bθ_3 x^3`\n            *   Putting it together: `-d/dx(a(x)ρ) = - [ θ_1 + 3θ_3 x^2 + 2Aθ_1 x^2 + Bθ_1 x + 2Aθ_3 x^4 + Bθ_3 x^3 ] ρ(x)`\n            *   Collect terms by powers of `x`: `-d/dx(a(x)ρ) = - [ 2Aθ_3 x^4 + Bθ_3 x^3 + (3θ_3 + 2Aθ_1) x^2 + Bθ_1 x + θ_1 ] ρ(x)`\n        6.  Calculate the term `(1/2)σ^2 d^2ρ/dx^2`:\n            *   `(1/2)σ^2 d^2ρ/dx^2 = (1/2)σ^2 [4A^2x^2 + 4ABx + (2A + B^2)] ρ(x)`\n            *   `(1/2)σ^2 d^2ρ/dx^2 = [2σ^2A^2x^2 + 2σ^2ABx + (σ^2A + (1/2)σ^2B^2)] ρ(x)`\n        7.  Combine the two parts to get `L*ρ`:\n            *   `L*ρ = [-d/dx(a(x)ρ)] + [(1/2)σ^2 d^2ρ/dx^2]`\n            *   `L*ρ / ρ = - [ 2Aθ_3 x^4 + Bθ_3 x^3 + (3θ_3 + 2Aθ_1) x^2 + Bθ_1 x + θ_1 ] + [ 2σ^2A^2x^2 + 2σ^2ABx + (σ^2A + (1/2)σ^2B^2) ]`\n            *   `L*ρ = ( ... polynomial in x ... ) * ρ(x)`\n        8.  The problem is asking about the `dt` term, which is `L*ρ_t dt`. The expression inside the `dt` is `L*ρ_t`.\n        9.  `L*ρ_t(x) = P(x) * ρ_t(x)`, where `P(x)` is the polynomial I just calculated. The problem states \"expand the deterministic ($dt$) part ... and identify the coefficient of the highest-degree monomial in x\".\n        10. The instruction is slightly ambiguous. Does it mean the coefficient in the polynomial `P(x)` or the coefficient of the highest power of `x` in the entire expression `L*ρ_t(x)`?\n            Let's re-read: \"identify the coefficient of the highest-degree monomial in x that appears in the dt-term\". The `dt`-term is `L*ρ_t(x)`.\n            `L*ρ_t(x) = (-2A_tθ_3 x^4 + ...) * exp(A_t x^2 + ...)`.\n            This is not a polynomial. The term `x^k * exp(A x^2 + ...)` is not a monomial.\n            Let's reconsider the wording: \"expand the deterministic ($dt$) part that results from substituting the exponential-quadratic ansatz into the Zakai equation\".\n            The Zakai equation is `dρ_t = ...`.\n            Let's substitute `dρ_t` using the ansatz:\n            `ρ_t = exp(A_t x^2 + B_t x + C_t)`\n            `dρ_t = (x^2 dA_t + x dB_t + dC_t) ρ_t`\n            Equating the `dt` parts:\n            `(x^2 \\dot{A}_t + x \\dot{B}_t + \\dot{C}_t) ρ_t = L*ρ_t`\n            This implies `x^2 \\dot{A}_t + x \\dot{B}_t + \\dot{C}_t = L*ρ_t / ρ_t`.\n            The right hand side is the polynomial `P(x)` I calculated earlier.\n            `P(x) = -2A_tθ_3 x^4 - B_tθ_3 x^3 + (2σ^2A_t^2 - 3θ_3 - 2A_tθ_1)x^2 + (2σ^2A_tB_t - B_tθ_1)x + (σ^2A_t + (1/2)σ^2B_t^2 - θ_1)`.\n            So we have `x^2 \\dot{A}_t + x \\dot{B}_t + \\dot{C}_t = P(x)`.\n            The left side is a quadratic in `x`. The right side is a quartic in `x`.\n            For the equality to hold, the coefficients of `x^4` and `x^3` on the right must be zero. This is the essence of moment closure/Gaussian approximation methods. The ansatz `ρ_t(x) = exp(A_t x^2 + B_t x + C_t)` is generally not a solution, but an approximation. The method works by matching the coefficients of the lower order terms and setting the higher order ones to zero (or approximating them).\n            The problem seems to imply that the ansatz might not hold, as it asks to identify a coefficient. If the ansatz were an exact solution, the quartic and cubic terms would vanish, i.e., `-2A_tθ_3 = 0` and `-B_tθ_3 = 0`. Since `θ_3` is a general constant, this would force `A_t=0` and `B_t=0`, which is not a general Gaussian.\n\n            Let's re-read the problem again, very carefully. \"Suppose we test whether the unnormalized posterior remains within the exponential-quadratic family... Using only the Zakai equation and the form of L*, expand the deterministic (dt) part ... and identify the coefficient of the highest-degree monomial in x that appears in the dt-term\".\n\n            The \"dt-term\" is `L*ρ_t(x)`.\n            `L*ρ_t(x) = P(x) * ρ_t(x)`, where `P(x)` is the big polynomial.\n            The highest-degree monomial in `x` that appears in `L*ρ_t(x)` is not straightforward. The expression is `(-2A_tθ_3 x^4 + ...) * exp(A_t x^2 + ...)`. This is not a polynomial. This interpretation must be wrong.\n\n            Perhaps the question means \"find the coefficient of the highest-degree monomial in the polynomial `P(x) = L*ρ_t(x) / ρ_t(x)`\". This seems much more plausible. The expression `L*ρ_t / ρ_t` is indeed a polynomial.\n            Let's assume this is the intended meaning.\n\n            The polynomial is `P(x) = L*ρ_t(x) / ρ_t(x) = -2A_tθ_3 x^4 - B_tθ_3 x^3 + (2σ^2A_t^2 - 3θ_3 - 2A_tθ_1)x^2 + ...`\n            The highest-degree monomial in `x` is `x^4`.\n            Its coefficient is `-2A_tθ_3`.\n\n            Now, the question asks for this coefficient at `t=0`. So I need to find `-2A_0θ_3`.\n            The problem states that at `t=0`, the posterior is Gaussian with mean `m_0` and variance `s_0 > 0`.\n            The *posterior* `p_0(x)` is `N(m_0, s_0)`.\n            `p_0(x) = \\frac{1}{\\sqrt{2\\pi s_0}} \\exp\\left( - \\frac{(x-m_0)^2}{2s_0} \\right)`.\n            The unnormalized posterior `ρ_0(x)` is proportional to this.\n            `ρ_0(x) = K * p_0(x)` for some constant `K`.\n            So, `ρ_0(x) = \\frac{K}{\\sqrt{2\\pi s_0}} \\exp\\left( - \\frac{x^2 - 2m_0x + m_0^2}{2s_0} \\right)`.\n            `ρ_0(x) = \\exp\\left( -\\frac{1}{2s_0}x^2 + \\frac{m_0}{s_0}x - \\frac{m_0^2}{2s_0} + \\ln\\left(\\frac{K}{\\sqrt{2\\pi s_0}}\\right) \\right)`.\n            This matches the ansatz `ρ_0(x) = \\exp(A_0 x^2 + B_0 x + C_0)`.\n            By comparing the coefficients of the powers of `x` in the exponent, we can identify `A_0`, `B_0`, and `C_0`.\n            Coefficient of `x^2`: `A_0 = -1 / (2s_0)`.\n            Coefficient of `x`: `B_0 = m_0 / s_0`.\n            Constant term: `C_0 = -m_0^2 / (2s_0) + \\ln(...)`.\n\n            The highest-degree monomial in `P(x)` at `t=0` is `-2A_0θ_3 x^4`.\n            The coefficient is `-2A_0θ_3`.\n            I need to express this in terms of `θ_3` and `s_0`.\n            I have `A_0 = -1 / (2s_0)`.\n            So the coefficient is `-2 * (-1 / (2s_0)) * θ_3`.\n            This simplifies to `θ_3 / s_0`.\n\n            The problem statement is specific: \"Express your final answer as a single closed-form analytic expression in terms of $\\theta_3$ and $s_0$ only.\"\n            My answer `θ_3/s_0` fits this perfectly. It doesn't depend on `m_0`, `c`, `σ`, or `θ_1`, which makes sense because those terms don't contribute to the `x^4` term.\n            `B_0` depends on `m_0`, and `B_0` multiplies the `x^3` term coefficient.\n            `σ` and `θ_1` appear in the `x^2` term coefficient.\n            This confirms the dependencies seem right.\n\n    *   **Final structure of the solution:**\n        *   Start with validating the problem. It seems valid.\n        *   State the Zakai equation and the form of the operator `L*`.\n        *   State the ansatz `ρ_t(x) = exp(A_t x^2 + B_t x + C_t)`.\n        *   Explain that the question requires calculating the `dt` term, `L*ρ_t(x)`.\n        *   Clarify the interpretation of \"coefficient of the highest-degree monomial\". This means we should compute the polynomial `P(x) = L*ρ_t(x) / ρ_t(x)` and find the coefficient of its highest power of `x`.\n        *   Compute the necessary derivatives of `ρ_t(x)`.\n        *   Compute the two components of `L*ρ_t(x)`: `-(d/dx)(a(x)ρ)` and `(1/2)σ^2(d^2/dx^2)ρ`.\n        *   Form the polynomial `P(x)` and identify the highest degree term and its coefficient in terms of `A_t`, `B_t`, and the problem parameters.\n        *   Relate the initial parameters `A_0, B_0, C_0` to the given initial Gaussian posterior `N(m_0, s_0)`. This will give a value for `A_0$.\n        *   Substitute the value of `A_0` into the coefficient expression to get the final answer.\n        *   Ensure all mathematical entities are enclosed in LaTeX.\n\nThis plan seems solid. I will now proceed to write down the solution.\n\n**Pre-computation check:**\n`ρ = e^Q` where `Q = Ax^2+Bx+C`\n`ρ' = Q'ρ = (2Ax+B)ρ`\n`ρ'' = (Q'' + (Q')^2)ρ = (2A + (2Ax+B)^2)ρ`\n`L*ρ = - (aρ)' + \\frac{1}{2}\\sigma^2 ρ''`\n`L*ρ = - (a'ρ + aρ') + \\frac{1}{2}\\sigma^2 ρ''`\n`L*ρ / ρ = - (a' + aQ') + \\frac{1}{2}\\sigma^2 (Q'' + (Q')^2)`\n`a = \\theta_1 x + \\theta_3 x^3`, `a' = \\theta_1 + 3\\theta_3 x^2`\n`Q' = 2Ax+B`, `Q'' = 2A`\n`aQ' = (\\theta_1 x + \\theta_3 x^3)(2Ax+B) = 2A\\theta_1 x^2 + B\\theta_1 x + 2A\\theta_3 x^4 + B\\theta_3 x^3`\n`a' + aQ' = (\\theta_1 + 3\\theta_3 x^2) + (2A\\theta_1 x^2 + B\\theta_1 x + 2A\\theta_3 x^4 + B\\theta_3 x^3)`\n`= 2A\\theta_3 x^4 + B\\theta_3 x^3 + (3\\theta_3 + 2A\\theta_1)x^2 + B\\theta_1 x + \\theta_1`\nThe highest degree term here is `2A\\theta_3 x^4`.\nWith the minus sign in `L*ρ/ρ`, this becomes `-2A\\theta_3 x^4`.\nNow, let's check the other part: `\\frac{1}{2}\\sigma^2 (Q'' + (Q')^2) = \\frac{1}{2}\\sigma^2 (2A + (2Ax+B)^2)`\n`= \\frac{1}{2}\\sigma^2 (2A + 4A^2x^2 + 4ABx + B^2) = \\sigma^2A + 2\\sigma^2A^2x^2 + 2\\sigma^2ABx + \\frac{1}{2}\\sigma^2B^2`\nThe highest degree term here is `2\\sigma^2A^2x^2`.\nCombining all terms, the highest degree term in `L*ρ/ρ` comes solely from the `-(a' + aQ')` part and is indeed `-2A\\theta_3 x^4`.\nThe coefficient is `-2A\\theta_3`.\nAt `t=0`, this is `-2A_0\\theta_3`.\nInitial posterior density `p(x; m_0, s_0) = \\frac{1}{\\sqrt{2\\pi s_0}} \\exp(-\\frac{(x-m_0)^2}{2s_0})`.\nUnnormalized density `ρ_0(x) \\propto \\exp(-\\frac{x^2 - 2m_0 x + m_0^2}{2s_0}) = \\exp(-\\frac{1}{2s_0}x^2 + \\frac{m_0}{s_0}x - \\frac{m_0^2}{2s_0})`.\nComparing with `\\exp(A_0x^2+B_0x+C_0)`, we have `A_0 = -1/(2s_0)`.\nFinal coefficient = `-2 * (-\\frac{1}{2s_0}) * \\theta_3 = \\frac{\\theta_3}{s_0}`.\nThe logic and calculation are consistent and verified. I am confident in this result.\n\nThe problem is well-posed and scientifically grounded within the field of stochastic filtering. It is not incomplete, contradictory, or ambiguous. It presents a standard scenario of applying an approximation (Gaussian filter) to a non-linear system and asks to identify the term that breaks the closure. This is a very specific and targeted question.\nThe givens are:\n- `dX_t = a(X_t)dt + \\sigma dW_t`\n- `dW_t` is standard Brownian motion\n- `\\sigma > 0`\n- `a(x) = \\theta_1 x + \\theta_3 x^3`\n- `\\theta_1, \\theta_3 \\in \\mathbb{R}`\n- `dY_t = h(X_t)dt + dV_t`\n- `dV_t` is independent standard Brownian motion\n- `h(x) = cx`\n- `c \\in \\mathbb{R}`\n- Initial posterior density of `X_0` is Gaussian `N(m_0, s_0)` with `s_0 > 0`.\n- Unnormalized posterior `\\rho_t(x)` follows Zakai equation: `d\\rho_t(x) = \\mathcal{L}^{\\star}\\rho_t(x)\\,dt + h(x)\\rho_t(x)\\,dY_t`\n- `\\mathcal{L}^{\\star}\\rho = -\\frac{d}{dx}(a(x)\\rho) + \\frac{1}{2}\\sigma^2\\frac{d^2\\rho}{dx^2}`\n- Ansatz to test: `\\rho_t(x) = \\exp(A_t x^2 + B_t x + C_t)`\n\nThe problem seems valid. No flaws detected. I will write the full solution now.", "answer": "$$\n\\boxed{\\frac{\\theta_3}{s_0}}\n$$", "id": "3068691"}, {"introduction": "The infinite-dimensional nature of the nonlinear filtering problem, as hinted at in exercises like [@problem_id:3068691], necessitates the use of numerical approximations. This exercise bridges the gap between the continuous-time theory of the Zakai equation and the discrete-time algorithms used in practice. By constructing a simple numerical scheme based on the Euler-Maruyama method [@problem_id:3068661], you will not only approximate the solution but also analyze its local error, a critical skill in developing and evaluating any computational filter.", "problem": "Consider a scalar signal-observation model posed on a filtered probability space supporting two independent standard Wiener processes, where the signal satisfies the stochastic differential equation (SDE) $dX_t = a X_t \\, dt + \\sigma \\, dW_t$ with deterministic initial condition $X_0 = x_0$, and the observation satisfies $dY_t = dV_t$ under a reference probability measure constructed by a change of measure so that the observation process has drift removed. Let $h(x) = c x$ be the observation function, and let $\\tilde{\\pi}_t(\\varphi)$ denote the unnormalized conditional distribution of the signal applied to a test function $\\varphi$. For the test function $\\varphi(x) = 1$, define the unnormalized weight process $Z_t := \\tilde{\\pi}_t(1)$.\n\nOver a single time step $[t_k, t_{k+1}]$ of length $\\Delta t$, construct a discrete-time approximation to the Zakai update of $Z_t$ by:\n1. Predicting the signal at time $t_k$ with one step of the Euler–Maruyama method, yielding $\\hat{X}_k$ from $\\hat{X}_{k} = \\hat{X}_{k-1} + a \\hat{X}_{k-1} \\, \\Delta t + \\sigma \\, \\Delta W_{k-1}$, and setting a frozen observation coefficient $\\theta_k := h(\\hat{X}_k) = c \\hat{X}_k$ for the interval $[t_k, t_{k+1}]$.\n2. Approximating the unnormalized Zakai update for $Z_t$ on $[t_k, t_{k+1}]$ by the linear SDE $dZ_t = \\theta_k Z_t \\, dY_t$ with constant coefficient $\\theta_k$, and then applying a single explicit Euler step for $Z_t$ to produce $Z_{k+1}^{E} = Z_k + \\theta_k Z_k \\, \\Delta Y_k$, where $\\Delta Y_k := Y_{t_{k+1}} - Y_{t_k}$.\n\nUsing fundamental properties of Itô calculus and Gaussian moments, derive the leading-order expression, as $\\Delta t \\to 0$, for the conditional mean-square local discretization error $E\\!\\left[(Z_{t_{k+1}} - Z_{k+1}^{E})^{2} \\mid \\mathcal{F}_{t_k}\\right]$ in terms of $Z_k$, $\\theta_k$, and $\\Delta t$. Your final answer must be a single closed-form analytic expression. Do not provide an inequality; give the leading-order term only.", "solution": "The problem requires the derivation of the leading-order expression for the conditional mean-square local discretization error of a numerical approximation to a simplified Zakai-type equation.\n\nThe analysis is performed over a single time step of duration $\\Delta t = t_{k+1} - t_k$. We are asked to compute $E\\!\\left[(Z_{t_{k+1}} - Z_{k+1}^{E})^{2} \\mid \\mathcal{F}_{t_k}\\right]$, where the expectation is conditioned on the information available at time $t_k$. Under this conditioning, the initial value $Z_k = Z_{t_k}$ and the coefficient $\\theta_k$ are considered deterministic constants.\n\nFirst, we identify the stochastic differential equation (SDE) that defines the \"true\" process on the interval $[t_k, t_{k+1}]$, as specified in the problem statement. This is the SDE for which we are analyzing the discretization error. The problem states this is the linear SDE:\n$$ dZ_t = \\theta_k Z_t \\, dY_t $$\nwith the initial condition $Z_{t_k} = Z_k$. The observation process under the reference measure is given by $dY_t = dV_t$, where $V_t$ is a standard Wiener process. Thus, the SDE for $Z_t$ is:\n$$ dZ_t = \\theta_k Z_t \\, dV_t, \\quad t \\in [t_k, t_{k+1}] $$\n\nThis is a linear SDE, also known as a geometric Brownian motion with zero drift. We can find its exact solution. Let $f(t, z) = \\ln(z)$. By Itô's lemma, we have:\n$$ d(\\ln Z_t) = \\frac{\\partial f}{\\partial z} dZ_t + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial z^2} (dZ_t)^2 $$\nThe partial derivatives are $\\frac{\\partial f}{\\partial z} = \\frac{1}{Z_t}$ and $\\frac{\\partial^2 f}{\\partial z^2} = -\\frac{1}{Z_t^2}$. The quadratic variation of the process $Z_t$ is $(dZ_t)^2 = (\\theta_k Z_t dV_t)^2 = \\theta_k^2 Z_t^2 (dV_t)^2 = \\theta_k^2 Z_t^2 dt$.\nSubstituting these into Itô's lemma:\n$$ d(\\ln Z_t) = \\frac{1}{Z_t} (\\theta_k Z_t dV_t) + \\frac{1}{2} \\left(-\\frac{1}{Z_t^2}\\right) (\\theta_k^2 Z_t^2 dt) = \\theta_k dV_t - \\frac{1}{2} \\theta_k^2 dt $$\nIntegrating from $t_k$ to $t_{k+1}$:\n$$ \\int_{t_k}^{t_{k+1}} d(\\ln Z_t) = \\int_{t_k}^{t_{k+1}} \\theta_k dV_t - \\int_{t_k}^{t_{k+1}} \\frac{1}{2} \\theta_k^2 dt $$\n$$ \\ln(Z_{t_{k+1}}) - \\ln(Z_{t_k}) = \\theta_k (V_{t_{k+1}} - V_{t_k}) - \\frac{1}{2} \\theta_k^2 (t_{k+1} - t_k) $$\nLet $\\Delta V_k = V_{t_{k+1}} - V_{t_k}$ and $\\Delta t = t_{k+1} - t_k$. Exponentiating both sides gives the exact solution at time $t_{k+1}$:\n$$ Z_{t_{k+1}} = Z_k \\exp\\left(\\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t\\right) $$\n\nNext, we identify the discrete-time approximation $Z_{k+1}^{E}$. The problem specifies this is obtained by a single explicit Euler (Euler-Maruyama) step:\n$$ Z_{k+1}^{E} = Z_k + \\theta_k Z_k \\Delta Y_k = Z_k(1 + \\theta_k \\Delta V_k) $$\nwhere $\\Delta Y_k = \\Delta V_k$.\n\nThe local discretization error is the difference $\\mathcal{E} = Z_{t_{k+1}} - Z_{k+1}^{E}$:\n$$ \\mathcal{E} = Z_k \\exp\\left(\\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t\\right) - Z_k (1 + \\theta_k \\Delta V_k) $$\n\nTo find the leading-order term of the mean-square error, we first find the leading-order term of the error $\\mathcal{E}$ itself. We use a Taylor series expansion of the exponential function $\\exp(u) = 1 + u + \\frac{u^2}{2!} + O(u^3)$. Let $u = \\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t$. The Wiener increment $\\Delta V_k$ has a normal distribution with mean $0$ and variance $\\Delta t$, so $\\Delta V_k$ is of stochastic order $O(\\sqrt{\\Delta t})$.\nConsequently, the terms in $u$ are of order $O(\\sqrt{\\Delta t})$ and $O(\\Delta t)$, and we must expand to a sufficient order to capture the leading error.\n$$ \\exp\\left(\\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t\\right) = 1 + \\left(\\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t\\right) + \\frac{1}{2}\\left(\\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t\\right)^2 + \\ldots $$\n$$ = 1 + \\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t + \\frac{1}{2}\\left( (\\theta_k \\Delta V_k)^2 - \\theta_k^3 \\Delta V_k \\Delta t + \\ldots \\right) + \\ldots $$\nSubstituting this into the expression for the error $\\mathcal{E}$:\n$$ \\mathcal{E} = Z_k \\left[ \\left(1 + \\theta_k \\Delta V_k - \\frac{1}{2} \\theta_k^2 \\Delta t + \\frac{1}{2} \\theta_k^2 (\\Delta V_k)^2 + \\ldots \\right) - (1 + \\theta_k \\Delta V_k) \\right] $$\n$$ \\mathcal{E} = Z_k \\left[ \\frac{1}{2} \\theta_k^2 (\\Delta V_k)^2 - \\frac{1}{2} \\theta_k^2 \\Delta t + \\text{h.o.t.} \\right] $$\nwhere \"h.o.t.\" represents higher-order terms in $\\Delta t$. The leading-order term of the error is:\n$$ \\mathcal{E} \\approx \\frac{1}{2} Z_k \\theta_k^2 \\left( (\\Delta V_k)^2 - \\Delta t \\right) $$\nNote that this term is of stochastic order $O(\\Delta t)$, since $(\\Delta V_k)^2$ is $O(\\Delta t)$.\n\nWe are asked for the conditional mean-square error, $E[\\mathcal{E}^2 \\mid \\mathcal{F}_{t_k}]$. To find the leading-order term, we square the leading-order term of the error $\\mathcal{E}$:\n$$ \\mathcal{E}^2 \\approx \\left[ \\frac{1}{2} Z_k \\theta_k^2 \\left( (\\Delta V_k)^2 - \\Delta t \\right) \\right]^2 = \\frac{1}{4} Z_k^2 \\theta_k^4 \\left( (\\Delta V_k)^2 - \\Delta t \\right)^2 $$\nNow we take the conditional expectation. Since $Z_k$ and $\\theta_k$ are $\\mathcal{F}_{t_k}$-measurable, they act as constants inside the expectation. The increment $\\Delta V_k$ is independent of $\\mathcal{F}_{t_k}$.\n$$ E[\\mathcal{E}^2 \\mid \\mathcal{F}_{t_k}] \\approx \\frac{1}{4} Z_k^2 \\theta_k^4 E\\left[ \\left( (\\Delta V_k)^2 - \\Delta t \\right)^2 \\right] $$\nLet $N \\sim N(0, 1)$ be a standard normal random variable. We can write $\\Delta V_k = \\sqrt{\\Delta t} N$. Substituting this into the expectation:\n$$ E\\left[ \\left( (\\sqrt{\\Delta t} N)^2 - \\Delta t \\right)^2 \\right] = E\\left[ \\left( \\Delta t N^2 - \\Delta t \\right)^2 \\right] = E\\left[ (\\Delta t)^2 (N^2 - 1)^2 \\right] $$\n$$ = (\\Delta t)^2 E\\left[ N^4 - 2N^2 + 1 \\right] = (\\Delta t)^2 (E[N^4] - 2E[N^2] + 1) $$\nFor a standard normal random variable, the moments are $E[N^2] = 1$ and $E[N^4] = 3$.\n$$ E\\left[ \\left( (\\Delta V_k)^2 - \\Delta t \\right)^2 \\right] = (\\Delta t)^2 (3 - 2(1) + 1) = 2(\\Delta t)^2 $$\nSubstituting this result back into the expression for the mean-square error:\n$$ E[\\mathcal{E}^2 \\mid \\mathcal{F}_{t_k}] \\approx \\frac{1}{4} Z_k^2 \\theta_k^4 \\left( 2(\\Delta t)^2 \\right) = \\frac{1}{2} Z_k^2 \\theta_k^4 (\\Delta t)^2 $$\nThe higher-order terms in the expansion of $\\mathcal{E}$ would contribute terms of order $O((\\Delta t)^3)$ or higher to the mean-square error, so the derived expression is indeed the leading-order term.\n\nThus, the leading-order expression for the conditional mean-square local discretization error is $\\frac{1}{2} Z_k^2 \\theta_k^4 (\\Delta t)^2$.", "answer": "$$\n\\boxed{\\frac{1}{2} Z_k^2 \\theta_k^4 (\\Delta t)^2}\n$$", "id": "3068661"}]}