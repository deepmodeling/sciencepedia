{"hands_on_practices": [{"introduction": "To understand the long-term behavior of an Ornstein-Uhlenbeck process, we must first find its stationary distribution. This practice challenges you to derive the stationary probability density by solving the time-independent Fokker-Planck equation, which is a powerful tool for analyzing stochastic processes [@problem_id:3076418]. Mastering this derivation provides a foundational understanding of how the mean-reverting drift and random diffusion balance to create a stable, predictable equilibrium.", "problem": "Consider the scalar Ornstein–Uhlenbeck stochastic differential equation (SDE)\n$$\ndX_{t}=-\\theta\\left(X_{t}-\\mu\\right)\\,dt+\\sigma\\,dW_{t},\n$$\nwhere $\\theta0$, $\\mu\\in\\mathbb{R}$, $\\sigma0$, and $W_{t}$ is a Wiener process (standard Brownian motion). In stationarity, the probability density function $p(x)$ of $X_{t}$ solves the time-independent Fokker–Planck ordinary differential equation\n$$\n0=\\partial_{x}\\left\\{\\theta\\left(x-\\mu\\right)p(x)\\right\\}+\\frac{1}{2}\\sigma^{2}\\,p''(x),\n$$\non the real line, together with the requirements that $p(x)\\ge 0$, $\\int_{-\\infty}^{\\infty}p(x)\\,dx=1$, and that the stationary probability flux vanishes at $x\\to\\pm\\infty$ so that the solution is normalizable.\n\nStarting from this differential equation and the stated conditions, derive $p(x)$ and determine its normalization constant by imposing $\\int_{-\\infty}^{\\infty}p(x)\\,dx=1$. Express your final answer as a single closed-form expression for the stationary density $p(x)$ on $\\mathbb{R}$. No numerical approximation is required.", "solution": "The starting point for this derivation is the provided time-independent Fokker–Planck equation for the stationary probability density function $p(x)$ of an Ornstein–Uhlenbeck process:\n$$\n0=\\partial_{x}\\left\\{\\theta\\left(x-\\mu\\right)p(x)\\right\\}+\\frac{1}{2}\\sigma^{2}\\,p''(x)\n$$\nwhere $p''(x) = \\frac{d^2p}{dx^2}$. This equation can be rewritten by recognizing that the second term is also a derivative, $p''(x) = \\frac{d}{dx}\\left(p'(x)\\right)$. Factoring out the derivative operator $\\partial_x \\equiv \\frac{d}{dx}$ yields:\n$$\n0 = \\frac{d}{dx} \\left[ \\theta(x-\\mu)p(x) + \\frac{1}{2}\\sigma^2 \\frac{dp}{dx} \\right]\n$$\nThis equation states that the term inside the square brackets, which represents the stationary probability flux $J(x)$, has a zero spatial derivative. Therefore, the flux must be constant for all $x$:\n$$\nJ(x) = \\theta(x-\\mu)p(x) + \\frac{1}{2}\\sigma^2 \\frac{dp}{dx} = C\n$$\nwhere $C$ is a constant of integration.\n\nThe problem requires a normalizable solution, which implies that the probability density $p(x)$ and its derivative $p'(x)$ must vanish as $x \\to \\pm\\infty$. This is consistent with the provided condition that the stationary probability flux vanishes at infinity. Applying this boundary condition:\n$$\n\\lim_{x\\to\\pm\\infty} J(x) = \\lim_{x\\to\\pm\\infty} \\left( \\theta(x-\\mu)p(x) + \\frac{1}{2}\\sigma^2 \\frac{dp}{dx} \\right) = 0\n$$\nSince $J(x)$ must be equal to the constant $C$ for all $x$, it follows that the constant must be zero, $C=0$. This simplifies the differential equation to a first-order linear ordinary differential equation:\n$$\n\\theta(x-\\mu)p(x) + \\frac{1}{2}\\sigma^2 \\frac{dp}{dx} = 0\n$$\nThis equation is separable. We rearrange it to separate the variables $p$ and $x$:\n$$\n\\frac{1}{2}\\sigma^2 \\frac{dp}{dx} = -\\theta(x-\\mu)p(x)\n$$\n$$\n\\frac{dp}{p} = -\\frac{2\\theta}{\\sigma^2}(x-\\mu)dx\n$$\nWe can now integrate both sides. Let the constant of integration be denoted by $C_1$:\n$$\n\\int \\frac{dp}{p} = -\\frac{2\\theta}{\\sigma^2} \\int (x-\\mu)dx\n$$\n$$\n\\ln(p(x)) = -\\frac{2\\theta}{\\sigma^2} \\frac{(x-\\mu)^2}{2} + C_1\n$$\n$$\n\\ln(p(x)) = -\\frac{\\theta}{\\sigma^2}(x-\\mu)^2 + C_1\n$$\nTo find $p(x)$, we exponentiate both sides of the equation:\n$$\np(x) = \\exp\\left(-\\frac{\\theta(x-\\mu)^2}{\\sigma^2} + C_1\\right) = \\exp(C_1) \\exp\\left(-\\frac{\\theta(x-\\mu)^2}{\\sigma^2}\\right)\n$$\nLet $K = \\exp(C_1)$ be the normalization constant. The solution has the form of a Gaussian distribution:\n$$\np(x) = K \\exp\\left(-\\frac{\\theta(x-\\mu)^2}{\\sigma^2}\\right)\n$$\nTo determine the constant $K$, we apply the normalization condition $\\int_{-\\infty}^{\\infty}p(x)dx=1$:\n$$\n1 = \\int_{-\\infty}^{\\infty} K \\exp\\left(-\\frac{\\theta(x-\\mu)^2}{\\sigma^2}\\right) dx\n$$\n$$\n\\frac{1}{K} = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\theta}{\\sigma^2}(x-\\mu)^2\\right) dx\n$$\nThis integral is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-a(z-b)^2) dz = \\sqrt{\\frac{\\pi}{a}}$. In our case, the variables correspond to $z=x$, $b=\\mu$, and $a = \\frac{\\theta}{\\sigma^2}$. Therefore, the value of the integral is:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\theta}{\\sigma^2}(x-\\mu)^2\\right) dx = \\sqrt{\\frac{\\pi}{\\theta/\\sigma^2}} = \\sqrt{\\frac{\\pi\\sigma^2}{\\theta}}\n$$\nSubstituting this result back, we find the value of $K$:\n$$\n\\frac{1}{K} = \\sqrt{\\frac{\\pi\\sigma^2}{\\theta}} \\implies K = \\frac{1}{\\sqrt{\\frac{\\pi\\sigma^2}{\\theta}}} = \\sqrt{\\frac{\\theta}{\\pi\\sigma^2}}\n$$\nSubstituting the normalization constant $K$ into our expression for $p(x)$ yields the final form of the stationary probability density function:\n$$\np(x) = \\sqrt{\\frac{\\theta}{\\pi\\sigma^2}} \\exp\\left(-\\frac{\\theta(x-\\mu)^2}{\\sigma^2}\\right)\n$$\nThis is the probability density function of a normal distribution with mean $\\mu$ and variance $\\frac{\\sigma^2}{2\\theta}$.", "answer": "$$\n\\boxed{\\sqrt{\\frac{\\theta}{\\pi\\sigma^{2}}}\\exp\\left(-\\frac{\\theta\\left(x-\\mu\\right)^{2}}{\\sigma^{2}}\\right)}\n$$", "id": "3076418"}, {"introduction": "A common pitfall is to assume that a process in a stationary state is a sequence of independent random events. This exercise directly confronts that misconception by guiding you to calculate the autocovariance function of the stationary Ornstein-Uhlenbeck process [@problem_id:3076442]. By demonstrating that the correlation between the process at different times is non-zero, you will solidify your understanding that stationarity does not imply independence, a crucial concept in time-series analysis.", "problem": "Consider the Ornstein–Uhlenbeck process defined by the stochastic differential equation\n$$\ndX_t=-\\theta\\,(X_t-\\mu)\\,dt+\\sigma\\,dW_t,\n$$\nwhere $\\theta0$ and $\\sigma0$ are constants, $\\mu\\in\\mathbb{R}$ is the long-run mean, and $W_t$ is a standard Brownian motion. Assume the process is strictly stationary; that is, it has been running for all $t\\in\\mathbb{R}$ and its distribution does not depend on $t$.\n\nUsing only the stochastic differential equation and the definitions of mean, variance, and covariance, derive the stationary variance $\\operatorname{Var}(X_t)$ and the autocovariance function\n$$\nC(\\tau)=\\operatorname{Cov}(X_t,X_{t+\\tau})\n$$\nfor $\\tau\\in\\mathbb{R}$. Based on your derivation, which option correctly gives the autocovariance function and the corresponding autocorrelation function $\\rho(\\tau)=\\dfrac{C(\\tau)}{\\operatorname{Var}(X_t)}$, and thereby addresses the misconception that stationarity implies independence?\n\nA. $C(\\tau)=\\dfrac{\\sigma^2}{2\\theta}\\,e^{-\\theta|\\tau|}$ and $\\rho(\\tau)=e^{-\\theta|\\tau|}$, so the process is stationary but not independent across time.\n\nB. $C(\\tau)=0$ for all $\\tau\\neq 0$ and $\\rho(\\tau)=\\mathbf{1}\\{\\tau=0\\}$, so stationarity implies independence across time.\n\nC. $C(\\tau)=\\sigma^2\\,e^{-\\theta|\\tau|}$ and $\\rho(\\tau)=\\dfrac{\\sigma^2\\,e^{-\\theta|\\tau|}}{\\operatorname{Var}(X_t)}$, so the process is independent only when $\\theta=0$.\n\nD. $C(\\tau)=\\dfrac{\\sigma^2}{2\\theta}\\,e^{-2\\theta|\\tau|}$ and $\\rho(\\tau)=e^{-2\\theta|\\tau|}$, so correlation decays at rate $2\\theta$ and stationarity implies weak dependence that vanishes instantly.", "solution": "The problem statement is a standard exercise in the theory of stochastic differential equations concerning the properties of the Ornstein-Uhlenbeck process. The problem is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. Therefore, we proceed with the derivation.\n\nThe Ornstein-Uhlenbeck process is described by the stochastic differential equation (SDE):\n$$\ndX_t = -\\theta(X_t - \\mu)dt + \\sigma dW_t\n$$\nwith parameters $\\theta  0$, $\\sigma  0$, and $\\mu \\in \\mathbb{R}$. We are given that the process is strictly stationary.\n\n**Step 1: Stationary Mean and Centered Process**\n\nFirst, we determine the stationary mean of the process, $\\mathbb{E}[X_t]$. Since the process is stationary, its mean must be constant in time. Let $m = \\mathbb{E}[X_t]$. Taking the expectation of the SDE:\n$$\nd\\mathbb{E}[X_t] = \\mathbb{E}[-\\theta(X_t - \\mu)dt + \\sigma dW_t]\n$$\nUsing the linearity of expectation and the fact that $\\mathbb{E}[dW_t] = 0$:\n$$\nd(m) = -\\theta(\\mathbb{E}[X_t] - \\mu)dt = -\\theta(m - \\mu)dt\n$$\nSince $m$ is constant for a stationary process, its differential $d(m)$ is zero.\n$$\n0 = -\\theta(m - \\mu)dt\n$$\nGiven that $\\theta  0$ and this must hold for any $dt$, we conclude that $m - \\mu = 0$, so the stationary mean is $\\mathbb{E}[X_t] = \\mu$.\n\nTo simplify the calculation of variance and covariance, we define a mean-centered process $Y_t = X_t - \\mu$. The SDE for $Y_t$ is found by noting $dY_t = dX_t$ since $\\mu$ is a constant:\n$$\ndY_t = -\\theta Y_t dt + \\sigma dW_t\n$$\nThe statistical properties of $Y_t$ are directly related to those of $X_t$:\n-   Mean: $\\mathbb{E}[Y_t] = \\mathbb{E}[X_t - \\mu] = \\mu - \\mu = 0$.\n-   Variance: $\\operatorname{Var}(X_t) = \\operatorname{Var}(Y_t + \\mu) = \\operatorname{Var}(Y_t) = \\mathbb{E}[Y_t^2] - (\\mathbb{E}[Y_t])^2 = \\mathbb{E}[Y_t^2]$.\n-   Autocovariance: $C(\\tau) = \\operatorname{Cov}(X_t, X_{t+\\tau}) = \\operatorname{Cov}(Y_t + \\mu, Y_{t+\\tau} + \\mu) = \\operatorname{Cov}(Y_t, Y_{t+\\tau}) = \\mathbb{E}[Y_t Y_{t+\\tau}]$.\n\n**Step 2: Stationary Variance $\\operatorname{Var}(X_t)$**\n\nWe derive the stationary variance, $v = \\operatorname{Var}(X_t) = \\mathbb{E}[Y_t^2]$, using Itô's lemma for the function $f(y) = y^2$ applied to the process $Y_t$. We have $f'(y) = 2y$ and $f''(y) = 2$.\nThe differential for $Y_t^2$ is:\n$$\nd(Y_t^2) = f'(Y_t)dY_t + \\frac{1}{2}f''(Y_t)(dY_t)^2 = 2Y_t dY_t + (dY_t)^2\n$$\nThe term $(dY_t)^2$ is calculated using Itô calculus rules ($dt^2=0$, $dt dW_t=0$, $dW_t^2=dt$):\n$$\n(dY_t)^2 = (-\\theta Y_t dt + \\sigma dW_t)^2 = \\theta^2 Y_t^2 dt^2 - 2\\theta\\sigma Y_t dt dW_t + \\sigma^2 (dW_t)^2 = \\sigma^2 dt\n$$\nSubstituting $dY_t$ and $(dY_t)^2$ into the expression for $d(Y_t^2)$:\n$$\nd(Y_t^2) = 2Y_t(-\\theta Y_t dt + \\sigma dW_t) + \\sigma^2 dt = (-2\\theta Y_t^2 + \\sigma^2)dt + 2\\sigma Y_t dW_t\n$$\nNow, we take the expectation. Let $v(t) = \\mathbb{E}[Y_t^2]$.\n$$\nd\\mathbb{E}[Y_t^2] = \\mathbb{E}[(-2\\theta Y_t^2 + \\sigma^2)dt] + \\mathbb{E}[2\\sigma Y_t dW_t]\n$$\nThe expectation of the stochastic integral term is zero. Thus:\n$$\ndv(t) = (-2\\theta \\mathbb{E}[Y_t^2] + \\sigma^2)dt = (-2\\theta v(t) + \\sigma^2)dt\n$$\nFor a stationary process, the variance $v(t)=v$ is constant, so its derivative with respect to time is zero, $\\frac{dv}{dt}=0$.\n$$\n0 = -2\\theta v + \\sigma^2\n$$\nSolving for $v$, given $\\theta  0$, we find the stationary variance:\n$$\n\\operatorname{Var}(X_t) = v = \\frac{\\sigma^2}{2\\theta}\n$$\n\n**Step 3: Autocovariance Function $C(\\tau)$**\n\nTo find the autocovariance $C(\\tau) = \\mathbb{E}[Y_t Y_{t+\\tau}]$, we first solve the linear SDE for $Y_t$. For a time interval $[t, t+\\tau]$ with $\\tau0$, the solution of $dY_s = -\\theta Y_s ds + \\sigma dW_s$ is:\n$$\nY_{t+\\tau} = Y_t e^{-\\theta\\tau} + \\sigma \\int_t^{t+\\tau} e^{-\\theta(t+\\tau-s)} dW_s\n$$\nNow we compute $C(\\tau) = \\mathbb{E}[Y_t Y_{t+\\tau}]$:\n$$\nC(\\tau) = \\mathbb{E}\\left[Y_t \\left(Y_t e^{-\\theta\\tau} + \\sigma \\int_t^{t+\\tau} e^{-\\theta(t+\\tau-s)} dW_s\\right)\\right]\n$$\n$$\nC(\\tau) = e^{-\\theta\\tau} \\mathbb{E}[Y_t^2] + \\mathbb{E}\\left[Y_t \\cdot \\sigma \\int_t^{t+\\tau} e^{-\\theta(t+\\tau-s)} dW_s\\right]\n$$\nThe first term is $e^{-\\theta\\tau} \\operatorname{Var}(X_t) = e^{-\\theta\\tau} \\frac{\\sigma^2}{2\\theta}$.\nFor the second term, the process value $Y_t$ is determined by the history of the Brownian motion $\\{W_s\\}_{s \\leq t}$. The Itô integral $\\int_t^{t+\\tau} \\dots dW_s$ depends on the future increments $\\{W_s - W_t\\}_{s  t}$. By the properties of Brownian motion, these are independent. Therefore, $Y_t$ and the integral are independent.\n$$\n\\mathbb{E}\\left[Y_t \\cdot \\sigma \\int_t^{t+\\tau} e^{-\\theta(t+\\tau-s)} dW_s\\right] = \\mathbb{E}[Y_t] \\cdot \\mathbb{E}\\left[\\sigma \\int_t^{t+\\tau} e^{-\\theta(t+\\tau-s)} dW_s\\right]\n$$\nSince $\\mathbb{E}[Y_t] = 0$, this entire term is zero.\nThus, for $\\tau  0$:\n$$\nC(\\tau) = e^{-\\theta\\tau} \\frac{\\sigma^2}{2\\theta}\n$$\nSince stationarity implies $C(\\tau) = \\operatorname{Cov}(X_t, X_{t+\\tau}) = \\operatorname{Cov}(X_{t-\\tau}, X_t) = C(-\\tau)$, the function must be even. We can generalize for any $\\tau \\in \\mathbb{R}$ using the absolute value:\n$$\nC(\\tau) = \\frac{\\sigma^2}{2\\theta} e^{-\\theta|\\tau|}\n$$\n\n**Step 4: Autocorrelation Function $\\rho(\\tau)$**\n\nThe autocorrelation function is defined as $\\rho(\\tau) = \\frac{C(\\tau)}{\\operatorname{Var}(X_t)}$. Using our derived expressions:\n$$\n\\rho(\\tau) = \\frac{\\frac{\\sigma^2}{2\\theta} e^{-\\theta|\\tau|}}{\\frac{\\sigma^2}{2\\theta}} = e^{-\\theta|\\tau|}\n$$\n\n**Step 5: Evaluation of Options**\n\nNow we evaluate each option based on our derivations.\n\n**A. $C(\\tau)=\\dfrac{\\sigma^2}{2\\theta}\\,e^{-\\theta|\\tau|}$ and $\\rho(\\tau)=e^{-\\theta|\\tau|}$, so the process is stationary but not independent across time.**\n-   The formula for $C(\\tau)$ matches our derived result.\n-   The formula for $\\rho(\\tau)$ matches our derived result.\n-   The interpretation is correct. The process is stationary by assumption. For any $\\tau \\neq 0$, the autocorrelation $\\rho(\\tau) = e^{-\\theta|\\tau|}$ is non-zero (since $\\theta  0$). Non-zero correlation implies that the random variables $X_t$ and $X_{t+\\tau}$ are not statistically independent. This correctly highlights that stationarity does not imply independence.\n-   Verdict: **Correct**.\n\n**B. $C(\\tau)=0$ for all $\\tau\\neq 0$ and $\\rho(\\tau)=\\mathbf{1}\\{\\tau=0\\}$, so stationarity implies independence across time.**\n-   The formula for $C(\\tau)$ is incorrect. Our derived $C(\\tau)$ is non-zero for all $\\tau$. $C(\\tau)$ only approaches $0$ as $|\\tau| \\to \\infty$.\n-   The formula for $\\rho(\\tau)$ is also incorrect.\n-   The conclusion \"stationarity implies independence across time\" is a known misconception, and it is false for the OU process, which serves as a prime counterexample.\n-   Verdict: **Incorrect**.\n\n**C. $C(\\tau)=\\sigma^2\\,e^{-\\theta|\\tau|}$ and $\\rho(\\tau)=\\dfrac{\\sigma^2\\,e^{-\\theta|\\tau|}}{\\operatorname{Var}(X_t)}$, so the process is independent only when $\\theta=0$.**\n-   The formula for $C(\\tau)$ is missing the factor of $\\frac{1}{2\\theta}$.\n-   The conclusion is flawed. The problem statement requires $\\theta  0$. If one considers the limit $\\theta \\to 0$, the SDE becomes $dX_t = \\sigma dW_t$, which describes a Brownian motion. A standard Brownian motion is not stationary, as its variance is $\\sigma^2 t$. Thus, the premise of stationarity would be violated.\n-   Verdict: **Incorrect**.\n\n**D. $C(\\tau)=\\dfrac{\\sigma^2}{2\\theta}\\,e^{-2\\theta|\\tau|}$ and $\\rho(\\tau)=e^{-2\\theta|\\tau|}$, so correlation decays at rate $2\\theta$ and stationarity implies weak dependence that vanishes instantly.**\n-   The exponent in both $C(\\tau)$ and $\\rho(\\tau)$ is $-2\\theta|\\tau|$, whereas our derivation correctly shows it to be $-\\theta|\\tau|$. The rate of decay is $\\theta$, not $2\\theta$.\n-   The statement that dependence \"vanishes instantly\" is false. The correlation decays exponentially to zero as $|\\tau| \\to \\infty$, but is non-zero for any finite $\\tau$.\n-   Verdict: **Incorrect**.\n\nBased on the thorough analysis, only option A provides the correct formulas and a sound interpretation.", "answer": "$$\\boxed{A}$$", "id": "3076442"}, {"introduction": "Theoretical derivations provide the bedrock of our understanding, but verifying these results with simulated data builds intuition and practical skills. This hands-on coding challenge asks you to bridge the gap between theory and computation by simulating the Ornstein-Uhlenbeck process and testing if its long-run statistical properties match our analytical predictions [@problem_id:3076386]. You will implement an exact simulation scheme and use a formal statistical test to check if the variance observed in your data is consistent with the theoretically derived stationary variance, a fundamental practice in quantitative modeling.", "problem": "Consider the Ornstein–Uhlenbeck (OU) process defined by the stochastic differential equation (SDE) $$dX_t=-\\theta\\,(X_t-\\mu)\\,dt+\\sigma\\,dW_t,$$ where $W_t$ is a standard Brownian motion, and $\\theta0$, $\\mu\\in\\mathbb{R}$, and $\\sigma0$ are parameters. Assume that for sufficiently long time the process admits a stationary distribution. Your task is to design a program that, for each specified parameter set, performs the following steps grounded in first principles of stochastic differential equations and statistical inference for normal distributions.\n\n- Simulate the OU process using the exact discrete-time update implied by the linear SDE solution to obtain a long-run sequence of samples. Use a fixed pseudo-random seed $42$ for reproducibility.\n- Discard an initial burn-in segment to allow the process to reach stationarity. Then collect $N$ consecutive samples at a constant time step $\\Delta$.\n- Fit a Gaussian distribution to the collected samples by estimating its mean and variance from data. Use the maximum-likelihood estimate for the mean, and use the unbiased estimator for the variance (with divisor $N-1$).\n- Compute the theoretical stationary variance of the OU process from first principles and denote it by $v_{\\mathrm{th}}$.\n- Perform a two-sided check of consistency at significance level $\\alpha$ by using the well-tested fact that for samples from a normal distribution, the statistic $\\frac{(N-1)\\,s^2}{v}$ follows a chi-square distribution with $N-1$ degrees of freedom, where $s^2$ is the unbiased sample variance and $v$ is the true variance. Construct the confidence interval for the true variance using the chi-square quantiles and test whether $v_{\\mathrm{th}}$ lies within this interval. Return a boolean indicating whether the theoretical stationary variance is within sampling error of the fitted variance.\n\nUse the following test suite. Each test case is a tuple $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)$ where $\\theta$ is the mean-reversion rate, $\\mu$ is the long-run mean, $\\sigma$ is the diffusion scale, $\\Delta$ is the sampling time step, $B$ is the burn-in time, $N$ is the number of collected samples, and $x_0$ is the initial condition.\n- Test case $1$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(1.0,0.0,2.0,0.05,50.0,60000,5.0)$\n- Test case $2$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(0.1,3.0,1.5,0.1,200.0,100000,-10.0)$\n- Test case $3$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(5.0,-1.0,0.5,0.01,10.0,50000,0.0)$\n- Test case $4$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(2.0,0.0,0.01,0.1,20.0,120000,100.0)$\n\nSet the significance level to $\\alpha=0.05$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is a boolean indicating whether $v_{\\mathrm{th}}$ lies within the chi-square confidence interval for the variance derived from the fitted Gaussian for test case $i$.", "solution": "### Problem Validation\n\nThe problem statement is evaluated against the specified criteria.\n\n#### Step 1: Extract Givens\n\n-   **Stochastic Differential Equation (SDE)**: The Ornstein–Uhlenbeck (OU) process is defined by $dX_t=-\\theta\\,(X_t-\\mu)\\,dt+\\sigma\\,dW_t$.\n-   **Parameters**: $W_t$ is a standard Brownian motion. $\\theta0$, $\\mu\\in\\mathbb{R}$, and $\\sigma0$.\n-   **Assumption**: For a sufficiently long time, the process admits a stationary distribution.\n-   **Simulation Method**: Use the exact discrete-time update implied by the linear SDE solution. A fixed pseudo-random seed of $42$ must be used for reproducibility.\n-   **Sampling Procedure**: Discard an initial burn-in segment of time $B$. Collect $N$ consecutive samples at a constant time step $\\Delta$.\n-   **Statistical Fitting**: Fit a Gaussian distribution to the collected samples. The mean is the maximum-likelihood estimate. The variance is the unbiased estimator (with divisor $N-1$).\n-   **Theoretical Quantity**: The theoretical stationary variance is denoted by $v_{\\mathrm{th}}$.\n-   **Consistency Check**: Perform a two-sided test at significance level $\\alpha=0.05$. The test is based on the statistic $\\frac{(N-1)\\,s^2}{v}$, which follows a chi-square distribution with $N-1$ degrees of freedom, where $s^2$ is the unbiased sample variance and $v$ is the true variance. Construct a confidence interval for the true variance and test if $v_{\\mathrm{th}}$ lies within it.\n-   **Initial Condition**: The process starts at $x_0$.\n-   **Test Cases**: Each case is a tuple $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)$.\n    -   Test case $1$: $(1.0,0.0,2.0,0.05,50.0,60000,5.0)$\n    -   Test case $2$: $(0.1,3.0,1.5,0.1,200.0,100000,-10.0)$\n    -   Test case $3$: $(5.0,-1.0,0.5,0.01,10.0,50000,0.0)$\n    -   Test case $4$: $(\\theta,\\mu,\\sigma,\\Delta,B,N,x_0)=(2.0,0.0,0.01,0.1,20.0,120000,100.0)$.\n-   **Output**: A boolean indicating whether the theoretical stationary variance is within sampling error of the fitted variance for each test case.\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is based on the Ornstein-Uhlenbeck process, a cornerstone model in stochastic calculus with wide applications in science and engineering. The methods prescribed—exact simulation, parameter estimation via maximum likelihood and unbiased variance, and hypothesis testing using the chi-square distribution—are all standard, well-established, and rigorously defined within their respective fields of mathematics and statistics. The problem is free of pseudoscience or scientifically unsound premises.\n-   **Well-Posed**: The problem is fully specified. It provides all necessary parameters $(\\theta, \\mu, \\sigma)$, simulation controls $(\\Delta, B, N, x_0)$, statistical parameters ($\\alpha=0.05$, seed=$42$), and a clear, unambiguous objective. The procedure described leads to a unique, deterministic (given the seed) boolean outcome for each test case.\n-   **Objective**: The language is formal, precise, and devoid of any subjectivity, ambiguity, or opinion. All terms are standard in the field.\n\nThe problem does not exhibit any flaws. It is scientifically sound, fully specified, objective, and computationally feasible. The task is a standard exercise in computational statistics and stochastic process simulation.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Solution\n\nThe solution is designed by following the principles and steps outlined in the problem statement.\n\n#### 1. Theoretical Foundation of the Ornstein-Uhlenbeck Process\n\nThe Ornstein-Uhlenbeck (OU) process is described by the linear stochastic differential equation:\n$$dX_t = -\\theta(X_t - \\mu)dt + \\sigma dW_t$$\nwhere $\\theta  0$ is the rate of mean reversion, $\\mu$ is the long-term mean, and $\\sigma  0$ is the scale of the volatility. $W_t$ is a standard Wiener process (Brownian motion).\n\nThis SDE is solvable. Given an initial condition $X_s$ at time $s$, the value of the process at a later time $t  s$ is:\n$$X_t = \\mu + (X_s - \\mu)e^{-\\theta(t-s)} + \\sigma \\int_s^t e^{-\\theta(t-u)}dW_u$$\nThe integral term $\\int_s^t e^{-\\theta(t-u)}dW_u$ is an Itô integral, which represents a normally distributed random variable with mean $0$. By the Itô isometry, its variance is:\n$$\\mathbb{E}\\left[\\left(\\int_s^t e^{-\\theta(t-u)}dW_u\\right)^2\\right] = \\int_s^t e^{-2\\theta(t-u)}du = \\frac{1 - e^{-2\\theta(t-s)}}{2\\theta}$$\nTherefore, for a discrete time step $\\Delta = t-s$, the conditional distribution of $X_t$ given $X_s$ is Gaussian:\n$$X_t | X_s \\sim \\mathcal{N}\\left(\\mu + (X_s - \\mu)e^{-\\theta\\Delta}, \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta\\Delta})\\right)$$\nThis provides an exact update rule for simulating the process:\n$$X_{n+1} = \\mu + (X_n - \\mu)e^{-\\theta\\Delta} + \\sqrt{\\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta\\Delta})} Z_{n+1}$$\nwhere $Z_{n+1}$ are independent and identically distributed standard normal random variables, $Z_{n+1} \\sim \\mathcal{N}(0, 1)$. This formula will be used for the simulation.\n\nAs time $t \\to \\infty$, the initial condition's influence decays ($e^{-\\theta(t-s)} \\to 0$), and the process converges to a stationary distribution. The mean of the stationary distribution is $\\mu$, and the variance converges to:\n$$ v_{\\mathrm{th}} = \\lim_{t-s \\to \\infty} \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta(t-s)}) = \\frac{\\sigma^2}{2\\theta} $$\nThus, the stationary distribution of the OU process is a normal distribution $\\mathcal{N}(\\mu, v_{\\mathrm{th}})$. This theoretical variance $v_{\\mathrm{th}}$ will be the subject of our statistical test.\n\n#### 2. Statistical Validation Methodology\n\nThe program will execute the following sequence for each test case:\n\n1.  **Simulation**: We first simulate a trajectory of the OU process. To ensure the samples are drawn from the stationary distribution, we discard an initial segment of the simulation. The burn-in time is given as $B$. The number of simulation steps to discard is $N_B = \\lceil B/\\Delta \\rceil$. After this burn-in period, we simulate an additional $N$ steps, collecting the values $\\{X_1, X_2, \\dots, X_N\\}$. A fixed pseudo-random number generator seed of $42$ is used to ensure the sequence of generated random numbers is identical for each run, making the simulation reproducible.\n\n2.  **Variance Estimation**: From the collected $N$ samples, we estimate the variance of the stationary distribution. The problem specifies using the unbiased sample variance, $s^2$, defined as:\n    $$s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N}(X_i - \\bar{X})^2$$\n    where $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^{N}X_i$ is the sample mean.\n\n3.  **Consistency Check via Confidence Interval**: The core of the validation is to check if our estimated variance $s^2$ is statistically consistent with the theoretical variance $v_{\\mathrm{th}} = \\frac{\\sigma^2}{2\\theta}$. For $N$ samples drawn from a normal distribution with true variance $v$, the sampling distribution of the statistic $\\frac{(N-1)s^2}{v}$ is a chi-square distribution with $df = N-1$ degrees of freedom, i.e., $\\frac{(N-1)s^2}{v} \\sim \\chi^2_{N-1}$.\n\n    We can use this relationship to construct a $(1-\\alpha)$ confidence interval for the true variance $v$. With a significance level of $\\alpha=0.05$, we find the lower and upper critical values of the $\\chi^2_{N-1}$ distribution that cut off an area of $\\alpha/2 = 0.025$ in each tail. Let these be $c_{\\text{lower}} = \\chi^2_{0.025, N-1}$ and $c_{\\text{upper}} = \\chi^2_{0.975, N-1}$.\n    The confidence interval for $v$ is derived from the probability statement:\n    $$P\\left(c_{\\text{lower}} \\le \\frac{(N-1)s^2}{v} \\le c_{\\text{upper}}\\right) = 1-\\alpha$$\n    Inverting the inequalities for $v$ yields the confidence interval $[CI_{\\text{low}}, CI_{\\text{high}}]$:\n    $$CI_{\\text{low}} = \\frac{(N-1)s^2}{c_{\\text{upper}}}, \\quad CI_{\\text{high}} = \\frac{(N-1)s^2}{c_{\\text{lower}}}$$\n    The test is passed (returning `True`) if the theoretical variance $v_{\\mathrm{th}}$ falls within this interval, i.e., $CI_{\\text{low}} \\le v_{\\mathrm{th}} \\le CI_{\\text{high}}$. Otherwise, the test fails (returning `False`).\n\n#### 3. Algorithmic Implementation\n\nThe Python script implements this logic. A main function `solve()` iterates through the provided test cases. For each case, a helper function will:\n1.  Initialize a random number generator with the specified seed, $42$. This is done for each case to ensure independent and reproducible tests.\n2.  Unpack the parameters $(\\theta, \\mu, \\sigma, \\Delta, B, N, x_0)$.\n3.  Calculate the number of burn-in steps $N_B = \\text{int(np.ceil}(B/\\Delta)\\text{)}$.\n4.  Generate $N_B + N$ standard normal random variates required for the simulation.\n5.  Run the simulation using the exact update rule, first for $N_B$ steps (discarding results) and then for $N$ steps (storing results).\n6.  Compute the unbiased sample variance $s^2$ from the stored samples using `numpy.var(ddof=1)`.\n7.  Compute the theoretical variance $v_{\\mathrm{th}} = \\sigma^2 / (2\\theta)$.\n8.  Find the chi-square critical values $c_{\\text{lower}}$ and $c_{\\text{upper}}$ using `scipy.stats.chi2.ppf`.\n9.  Calculate the confidence interval bounds $CI_{\\text{low}}$ and $CI_{\\text{high}}$.\n10. Return the boolean result of the check $v_{\\mathrm{th}} \\in [CI_{\\text{low}}, CI_{\\text{high}}]$.\n\nThe final results are collected into a list and printed in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the Ornstein-Uhlenbeck process simulation and validation\n    for a suite of test cases.\n    \"\"\"\n    \n    # Each test case is a tuple:\n    # (theta, mu, sigma, delta_t, B, N, x0)\n    # theta: mean-reversion rate\n    # mu: long-run mean\n    # sigma: diffusion scale\n    # delta_t: sampling time step\n    # B: burn-in time\n    # N: number of collected samples\n    # x0: initial condition\n    test_cases = [\n        (1.0, 0.0, 2.0, 0.05, 50.0, 60000, 5.0),\n        (0.1, 3.0, 1.5, 0.1, 200.0, 100000, -10.0),\n        (5.0, -1.0, 0.5, 0.01, 10.0, 50000, 0.0),\n        (2.0, 0.0, 0.01, 0.1, 20.0, 120000, 100.0)\n    ]\n\n    alpha = 0.05\n    results = []\n\n    for case in test_cases:\n        result = run_ou_validation(case, alpha)\n        results.append(result)\n\n    # Format the output as a comma-separated list of booleans in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_ou_validation(case_params, alpha):\n    \"\"\"\n    Performs the OU process simulation and statistical validation for a single case.\n    \n    Args:\n        case_params (tuple): A tuple containing the parameters for the simulation.\n        alpha (float): The significance level for the consistency check.\n        \n    Returns:\n        bool: True if the theoretical variance is within the confidence interval\n              of the sample variance, False otherwise.\n    \"\"\"\n    theta, mu, sigma, delta_t, B, N, x0 = case_params\n\n    # Set a fixed seed for reproducibility for each test case.\n    rng = np.random.default_rng(42)\n\n    # 1. Simulate the OU process using the exact discrete-time update.\n    # The number of burn-in steps islceil(B / delta_t).\n    n_burn = int(math.ceil(B / delta_t))\n    total_steps = n_burn + N\n    \n    # Pre-calculate constants for the update rule for efficiency.\n    # X_{n+1} = mu + (X_n - mu)*exp(-theta*dt) + noise\n    # where noise is a Gaussian with mean 0 and variance V.\n    exp_term = math.exp(-theta * delta_t)\n    var_term = (sigma**2 / (2 * theta)) * (1 - math.exp(-2 * theta * delta_t))\n    std_dev_term = math.sqrt(var_term)\n\n    # Generate all required random numbers at once.\n    z_variates = rng.normal(size=total_steps)\n    \n    samples = np.zeros(N)\n    x_current = float(x0)\n\n    # Burn-in period\n    for i in range(n_burn):\n        x_current = mu + (x_current - mu) * exp_term + std_dev_term * z_variates[i]\n\n    # Sampling period\n    for i in range(N):\n        x_current = mu + (x_current - mu) * exp_term + std_dev_term * z_variates[n_burn + i]\n        samples[i] = x_current\n\n    # 2. Fit a Gaussian: compute the unbiased sample variance (s^2).\n    # ddof=1 provides the unbiased estimator for the variance.\n    sample_variance = np.var(samples, ddof=1)\n\n    # 3. Compute the theoretical stationary variance (v_th).\n    theoretical_variance = sigma**2 / (2 * theta)\n    \n    # 4. Perform the consistency check.\n    # The statistic (N-1)*s^2 / v follows a chi-square distribution with N-1 degrees of freedom.\n    # We construct a (1-alpha) confidence interval for the true variance v.\n    df = N - 1\n    \n    # Find the critical values of the chi-square distribution.\n    # ppf is the percent point function (inverse of cdf).\n    chi2_lower_critical = chi2.ppf(alpha / 2, df)\n    chi2_upper_critical = chi2.ppf(1 - alpha / 2, df)\n    \n    # Construct the confidence interval for the true variance.\n    # [ (df * s^2) / chi2_upper, (df * s^2) / chi2_lower ]\n    ci_lower_bound = (df * sample_variance) / chi2_upper_critical\n    ci_upper_bound = (df * sample_variance) / chi2_lower_critical\n    \n    # Check if the theoretical variance lies within the confidence interval.\n    is_consistent = (theoretical_variance >= ci_lower_bound) and \\\n                    (theoretical_variance = ci_upper_bound)\n    \n    return is_consistent\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3076386"}]}