{"hands_on_practices": [{"introduction": "The heart of the Heston model is its treatment of volatility not as a constant, but as a random process in itself. This exercise [@problem_id:3078433] dives into the dynamics of this variance process, which follows a Cox-Ingersoll-Ross (CIR) model. By deriving the conditional mean and variance, you will develop a deep intuition for key properties like mean-reversion and see firsthand how the process is expected to behave over time.", "problem": "Consider the variance process in the Heston stochastic volatility model, which is modeled by the Cox–Ingersoll–Ross (CIR) process. Let $\\{v_t\\}_{t \\geq 0}$ satisfy the stochastic differential equation (SDE)\n$$\n\\mathrm{d}v_t = \\kappa\\left(\\theta - v_t\\right)\\mathrm{d}t + \\xi \\sqrt{v_t}\\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is a standard Brownian motion, and parameters satisfy $\\kappa  0$, $\\theta  0$, $\\xi  0$. Assume an initial variance $v_0  0$ is given and treat conditioning on $v_0$ throughout. You may assume a unique strong solution exists and that the relevant conditional moments are finite for $t \\geq 0$.\n\nStarting from the SDE and fundamental principles of stochastic calculus and conditional expectation, derive closed-form expressions for the conditional mean $\\mathbb{E}[v_t \\mid v_0]$ and conditional variance $\\mathrm{Var}(v_t \\mid v_0)$ as functions of $t$, $\\kappa$, $\\theta$, $\\xi$, and $v_0$. Express your final answers as analytic formulas and simplify them fully. No numerical approximation or rounding is required.", "solution": "The problem asks for the conditional mean $\\mathbb{E}[v_t \\mid v_0]$ and conditional variance $\\mathrm{Var}(v_t \\mid v_0)$ of the variance process $\\{v_t\\}_{t \\geq 0}$ in the Heston model, which follows the Cox–Ingersoll–Ross (CIR) process given by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}v_t = \\kappa(\\theta - v_t)\\mathrm{d}t + \\xi \\sqrt{v_t}\\,\\mathrm{d}W_t\n$$\nThe problem is well-posed and scientifically grounded, providing all necessary information and assumptions to proceed with the derivations.\n\n**Part 1: Derivation of the Conditional Mean $\\mathbb{E}[v_t \\mid v_0]$**\n\nWe begin by taking the conditional expectation of the SDE. The integral form of the SDE is:\n$$\nv_t - v_0 = \\int_0^t \\kappa(\\theta - v_s)\\mathrm{d}s + \\int_0^t \\xi \\sqrt{v_s}\\,\\mathrm{d}W_s\n$$\nTaking the conditional expectation $\\mathbb{E}[\\cdot \\mid v_0]$ on both sides, we get:\n$$\n\\mathbb{E}[v_t \\mid v_0] - \\mathbb{E}[v_0 \\mid v_0] = \\mathbb{E}\\left[\\int_0^t \\kappa(\\theta - v_s)\\mathrm{d}s \\mid v_0\\right] + \\mathbb{E}\\left[\\int_0^t \\xi \\sqrt{v_s}\\,\\mathrm{d}W_s \\mid v_0\\right]\n$$\nLet $m(t) = \\mathbb{E}[v_t \\mid v_0]$. The initial condition is $m(0) = \\mathbb{E}[v_0 \\mid v_0] = v_0$, as $v_0$ is given. The expectation of the Itô integral term is zero, provided the integrand satisfies necessary regularity conditions, which are assumed.\n$$\n\\mathbb{E}\\left[\\int_0^t \\xi \\sqrt{v_s}\\,\\mathrm{d}W_s \\mid v_0\\right] = 0\n$$\nBy Fubini's theorem, we can interchange the expectation and the time integral:\n$$\nm(t) - v_0 = \\int_0^t \\kappa(\\theta - \\mathbb{E}[v_s \\mid v_0])\\mathrm{d}s = \\int_0^t \\kappa(\\theta - m(s))\\mathrm{d}s\n$$\nDifferentiating with respect to $t$ gives an ordinary differential equation (ODE) for $m(t)$:\n$$\n\\frac{\\mathrm{d}m(t)}{\\mathrm{d}t} = \\kappa(\\theta - m(t))\n$$\nThis is a first-order linear ODE, which can be rearranged as:\n$$\n\\frac{\\mathrm{d}m}{\\mathrm{d}t} + \\kappa m = \\kappa \\theta\n$$\nWe solve this ODE using an integrating factor $I(t) = \\exp\\left(\\int \\kappa \\, \\mathrm{d}t\\right) = \\exp(\\kappa t)$. Multiplying the ODE by $I(t)$:\n$$\n\\exp(\\kappa t)\\frac{\\mathrm{d}m}{\\mathrm{d}t} + \\kappa \\exp(\\kappa t) m = \\kappa \\theta \\exp(\\kappa t)\n$$\nThe left-hand side is the derivative of a product:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left(m(t)\\exp(\\kappa t)\\right) = \\kappa \\theta \\exp(\\kappa t)\n$$\nIntegrating from $0$ to $t$:\n$$\n\\int_0^t \\frac{\\mathrm{d}}{\\mathrm{d}s}\\left(m(s)\\exp(\\kappa s)\\right) \\mathrm{d}s = \\int_0^t \\kappa \\theta \\exp(\\kappa s) \\mathrm{d}s\n$$\n$$\nm(t)\\exp(\\kappa t) - m(0)\\exp(0) = \\kappa \\theta \\left[\\frac{1}{\\kappa}\\exp(\\kappa s)\\right]_0^t\n$$\nSubstituting $m(0) = v_0$:\n$$\nm(t)\\exp(\\kappa t) - v_0 = \\theta(\\exp(\\kappa t) - 1)\n$$\nSolving for $m(t)$:\n$$\nm(t) = v_0 \\exp(-\\kappa t) + \\theta(1 - \\exp(-\\kappa t))\n$$\nThis is the closed-form expression for the conditional mean $\\mathbb{E}[v_t \\mid v_0]$.\n\n**Part 2: Derivation of the Conditional Variance $\\mathrm{Var}(v_t \\mid v_0)$**\n\nThe conditional variance is defined as $\\mathrm{Var}(v_t \\mid v_0) = \\mathbb{E}[v_t^2 \\mid v_0] - (\\mathbb{E}[v_t \\mid v_0])^2$. A more direct approach than calculating $\\mathbb{E}[v_t^2 \\mid v_0]$ is to use the integral representation of $v_t$.\nFrom the ODE solution method, we can write $v_t$ by treating the stochastic term as a source. The SDE can be written as $\\mathrm{d}v_t + \\kappa v_t \\mathrm{d}t = \\kappa \\theta \\mathrm{d}t + \\xi \\sqrt{v_t} \\mathrm{d}W_t$. Using the integrating factor $\\exp(\\kappa t)$, we find:\n$$\n\\mathrm{d}(\\exp(\\kappa t) v_t) = \\kappa \\theta \\exp(\\kappa t) \\mathrm{d}t + \\xi \\exp(\\kappa t) \\sqrt{v_t} \\mathrm{d}W_t\n$$\nIntegrating from $0$ to $t$:\n$$\n\\exp(\\kappa t)v_t - v_0 = \\int_0^t \\kappa \\theta \\exp(\\kappa s) \\mathrm{d}s + \\int_0^t \\xi \\exp(\\kappa s) \\sqrt{v_s} \\mathrm{d}W_s\n$$\n$$\n\\exp(\\kappa t)v_t - v_0 = \\theta(\\exp(\\kappa t) - 1) + \\xi \\int_0^t \\exp(\\kappa s) \\sqrt{v_s} \\mathrm{d}W_s\n$$\nSolving for $v_t$:\n$$\nv_t = v_0 \\exp(-\\kappa t) + \\theta(1 - \\exp(-\\kappa t)) + \\xi \\exp(-\\kappa t) \\int_0^t \\exp(\\kappa s) \\sqrt{v_s} \\mathrm{d}W_s\n$$\nThe first two terms are exactly the conditional mean $m(t)=\\mathbb{E}[v_t \\mid v_0]$. Therefore:\n$$\nv_t - m(t) = \\xi \\exp(-\\kappa t) \\int_0^t \\exp(\\kappa s) \\sqrt{v_s} \\mathrm{d}W_s\n$$\nThe conditional variance is the expectation of the square of this difference:\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\mathbb{E}\\left[ (v_t - m(t))^2 \\mid v_0 \\right] = \\mathbb{E}\\left[ \\left(\\xi \\exp(-\\kappa t) \\int_0^t \\exp(\\kappa s) \\sqrt{v_s} \\mathrm{d}W_s\\right)^2 \\mid v_0 \\right]\n$$\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\xi^2 \\exp(-2\\kappa t) \\mathbb{E}\\left[ \\left(\\int_0^t \\exp(\\kappa s) \\sqrt{v_s} \\mathrm{d}W_s\\right)^2 \\mid v_0 \\right]\n$$\nBy the Itô isometry property for conditional expectations, $\\mathbb{E}[(\\int_0^t H_s \\mathrm{d}W_s)^2 \\mid \\mathcal{F}_0] = \\mathbb{E}[\\int_0^t H_s^2 \\mathrm{d}s \\mid \\mathcal{F}_0]$, where here $\\mathcal{F}_0$ represents conditioning on $v_0$.\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\xi^2 \\exp(-2\\kappa t) \\mathbb{E}\\left[ \\int_0^t (\\exp(\\kappa s) \\sqrt{v_s})^2 \\mathrm{d}s \\mid v_0 \\right]\n$$\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\xi^2 \\exp(-2\\kappa t) \\int_0^t \\exp(2\\kappa s) \\mathbb{E}[v_s \\mid v_0] \\mathrm{d}s\n$$\nWe substitute the expression for $m(s) = \\mathbb{E}[v_s \\mid v_0]$:\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\xi^2 \\exp(-2\\kappa t) \\int_0^t \\exp(2\\kappa s) \\left[v_0 \\exp(-\\kappa s) + \\theta(1 - \\exp(-\\kappa s))\\right] \\mathrm{d}s\n$$\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\xi^2 \\exp(-2\\kappa t) \\int_0^t \\left[v_0 \\exp(\\kappa s) + \\theta \\exp(2\\kappa s) - \\theta \\exp(\\kappa s)\\right] \\mathrm{d}s\n$$\nNow we perform the integration:\n$$\n\\int_0^t \\left[v_0 \\exp(\\kappa s) + \\theta \\exp(2\\kappa s) - \\theta \\exp(\\kappa s)\\right] \\mathrm{d}s = \\left[v_0 \\frac{\\exp(\\kappa s)}{\\kappa} + \\theta \\frac{\\exp(2\\kappa s)}{2\\kappa} - \\theta \\frac{\\exp(\\kappa s)}{\\kappa}\\right]_0^t\n$$\n$$\n= \\left(v_0 \\frac{\\exp(\\kappa t)}{\\kappa} + \\theta \\frac{\\exp(2\\kappa t)}{2\\kappa} - \\theta \\frac{\\exp(\\kappa t)}{\\kappa}\\right) - \\left(\\frac{v_0}{\\kappa} + \\frac{\\theta}{2\\kappa} - \\frac{\\theta}{\\kappa}\\right)\n$$\n$$\n= \\frac{v_0}{\\kappa}(\\exp(\\kappa t)-1) + \\frac{\\theta}{2\\kappa}(\\exp(2\\kappa t)-1) - \\frac{\\theta}{\\kappa}(\\exp(\\kappa t)-1)\n$$\nSubstituting this back into the expression for the variance:\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\xi^2 \\exp(-2\\kappa t) \\left[\\frac{v_0}{\\kappa}(\\exp(\\kappa t)-1) - \\frac{\\theta}{\\kappa}(\\exp(\\kappa t)-1) + \\frac{\\theta}{2\\kappa}(\\exp(2\\kappa t)-1)\\right]\n$$\nDistributing the $\\exp(-2\\kappa t)$ term:\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\frac{\\xi^2 v_0}{\\kappa}(\\exp(-\\kappa t) - \\exp(-2\\kappa t)) - \\frac{\\xi^2 \\theta}{\\kappa}(\\exp(-\\kappa t) - \\exp(-2\\kappa t)) + \\frac{\\xi^2 \\theta}{2\\kappa}(1 - \\exp(-2\\kappa t))\n$$\nFinally, we group terms involving $v_0$ and $\\theta$ for a standard presentation:\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\frac{\\xi^2 v_0}{\\kappa}(\\exp(-\\kappa t) - \\exp(-2\\kappa t)) + \\frac{\\xi^2 \\theta}{2\\kappa}\\left(-2(\\exp(-\\kappa t) - \\exp(-2\\kappa t)) + 1 - \\exp(-2\\kappa t)\\right)\n$$\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\frac{\\xi^2 v_0}{\\kappa}(\\exp(-\\kappa t) - \\exp(-2\\kappa t)) + \\frac{\\xi^2 \\theta}{2\\kappa}(1 - 2\\exp(-\\kappa t) + \\exp(-2\\kappa t))\n$$\nRecognizing that the last term is a perfect square:\n$$\n\\mathrm{Var}(v_t \\mid v_0) = \\frac{\\xi^2 v_0}{\\kappa}(\\exp(-\\kappa t) - \\exp(-2\\kappa t)) + \\frac{\\xi^2 \\theta}{2\\kappa}(1 - \\exp(-\\kappa t))^2\n$$\nThis is the closed-form expression for the conditional variance.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nv_0 \\exp(-\\kappa t) + \\theta(1 - \\exp(-\\kappa t))  \\frac{\\xi^2 v_0}{\\kappa}(\\exp(-\\kappa t) - \\exp(-2\\kappa t)) + \\frac{\\xi^2 \\theta}{2\\kappa}(1 - \\exp(-\\kappa t))^2\n\\end{pmatrix}\n}\n$$", "id": "3078433"}, {"introduction": "A powerful way to understand a new, complex model is to see how it relates to a simpler, foundational one. This practice [@problem_id:3078353] challenges you to connect the Heston model back to its famous predecessor, the Black–Scholes model. By deriving the general Heston pricing equation and then imposing simplifying assumptions, you will confirm that Heston is a true generalization and gain insight into the role of stochastic volatility.", "problem": "Consider the Heston stochastic volatility model under the risk-neutral measure for an equity price process. The model specifies the Stochastic Differential Equations (SDEs)\n$$\n\\mathrm{d}S_{t} = r\\,S_{t}\\,\\mathrm{d}t + \\sqrt{v_{t}}\\,S_{t}\\,\\mathrm{d}W^{(1)}_{t}, \\qquad\n\\mathrm{d}v_{t} = \\kappa\\left(\\theta - v_{t}\\right)\\mathrm{d}t + \\xi \\sqrt{v_{t}}\\,\\mathrm{d}W^{(2)}_{t},\n$$\nwhere $S_{t}$ is the asset price, $v_{t}$ is the instantaneous variance, $r$ is the constant risk-free rate, $\\kappa  0$ is the mean-reversion speed, $\\theta  0$ is the long-run variance level, $\\xi  0$ is the volatility of volatility, and $\\left(W^{(1)}_{t}, W^{(2)}_{t}\\right)$ are correlated Wiener processes with instantaneous correlation $\\rho$, i.e., $\\mathrm{d}\\langle W^{(1)},W^{(2)}\\rangle_{t} = \\rho\\,\\mathrm{d}t$.\n\nLet $C(S,v,t)$ denote the time-$t$ price of a European contingent claim with maturity $T$ and terminal payoff $\\Phi(S_{T})$, where $\\Phi$ is sufficiently smooth and of at most polynomial growth. Using the infinitesimal generator of the Markov process $\\left(S_{t},v_{t}\\right)$ and the Feynman–Kac theorem, derive the Partial Differential Equation (PDE) that $C(S,v,t)$ must satisfy under the risk-neutral measure. Then, impose the conditions $\\rho = 0$ and $v_{t} \\equiv v$ is a positive constant (i.e., $v$ does not evolve stochastically). Show from first principles that the resulting PDE coincides with the Black–Scholes PDE for a non-dividend-paying asset.\n\nAnswer specification:\n- Your derivation must start from the given SDEs and the definition of the infinitesimal generator, without assuming any pre-stated pricing PDE.\n- Present the reduced PDE clearly identified as the Black–Scholes PDE by appropriate comparison of coefficients.\n- For your final answer, give the effective Black–Scholes volatility parameter $\\sigma_{\\mathrm{eff}}$ as a single symbolic expression in terms of $v$. No numerical evaluation is required.", "solution": "We begin from the Heston model SDEs under the risk-neutral measure:\n$$\n\\mathrm{d}S_{t} = r\\,S_{t}\\,\\mathrm{d}t + \\sqrt{v_{t}}\\,S_{t}\\,\\mathrm{d}W^{(1)}_{t}, \\qquad\n\\mathrm{d}v_{t} = \\kappa\\left(\\theta - v_{t}\\right)\\mathrm{d}t + \\xi \\sqrt{v_{t}}\\,\\mathrm{d}W^{(2)}_{t},\n$$\nwith $\\mathrm{d}\\langle W^{(1)},W^{(2)}\\rangle_{t} = \\rho\\,\\mathrm{d}t$.\n\nThe state vector is $\\left(S_{t},v_{t}\\right)$. Let $f(S,v,t)$ be a sufficiently smooth test function. The infinitesimal generator $\\mathcal{L}$ of $\\left(S_{t},v_{t}\\right)$ acts on $f$ as\n$$\n\\mathcal{L} f = b_{S}\\,\\frac{\\partial f}{\\partial S} + b_{v}\\,\\frac{\\partial f}{\\partial v} + \\frac{1}{2}a_{SS}\\,\\frac{\\partial^{2} f}{\\partial S^{2}} + a_{Sv}\\,\\frac{\\partial^{2} f}{\\partial S\\,\\partial v} + \\frac{1}{2}a_{vv}\\,\\frac{\\partial^{2} f}{\\partial v^{2}},\n$$\nwhere $b_{S}$ and $b_{v}$ are the drift components and $a_{SS}$, $a_{Sv}$, $a_{vv}$ are entries of the diffusion covariance matrix associated with the SDEs. From the given SDEs,\n- The drift terms are\n$$\nb_{S} = r\\,S, \\qquad b_{v} = \\kappa\\left(\\theta - v\\right).\n$$\n- The diffusion coefficients (multiplying the Wiener processes) are\n$$\n\\text{for } S: \\quad \\sqrt{v}\\,S, \\qquad \\text{for } v: \\quad \\xi \\sqrt{v},\n$$\nwith instantaneous correlation $\\rho$. Therefore, the covariance entries are\n$$\na_{SS} = \\left(\\sqrt{v}\\,S\\right)^{2} = v\\,S^{2}, \\qquad\na_{vv} = \\left(\\xi \\sqrt{v}\\right)^{2} = \\xi^{2}\\,v, \\qquad\na_{Sv} = \\rho \\left(\\sqrt{v}\\,S\\right)\\left(\\xi \\sqrt{v}\\right) = \\rho\\,\\xi\\,v\\,S.\n$$\n\nThe Feynman–Kac theorem states that the arbitrage-free price $C(S,v,t)$ of a European contingent claim with terminal payoff $\\Phi(S_{T})$ satisfies the pricing PDE\n$$\n\\frac{\\partial C}{\\partial t} + \\mathcal{L} C - r\\,C = 0,\n$$\nsubject to the terminal condition $C(S,v,T) = \\Phi(S)$ and suitable growth/boundary conditions. Substituting the expressions for $b_{S}$, $b_{v}$, $a_{SS}$, $a_{Sv}$, and $a_{vv}$ into $\\mathcal{L}$, we obtain the Heston pricing PDE:\n$$\n\\frac{\\partial C}{\\partial t}\n+ r\\,S\\,\\frac{\\partial C}{\\partial S}\n+ \\kappa\\left(\\theta - v\\right)\\frac{\\partial C}{\\partial v}\n+ \\frac{1}{2}v\\,S^{2}\\,\\frac{\\partial^{2} C}{\\partial S^{2}}\n+ \\rho\\,\\xi\\,v\\,S\\,\\frac{\\partial^{2} C}{\\partial S\\,\\partial v}\n+ \\frac{1}{2}\\xi^{2}\\,v\\,\\frac{\\partial^{2} C}{\\partial v^{2}}\n- r\\,C\n= 0.\n$$\n\nNow impose the conditions $\\rho = 0$ and $v_{t} \\equiv v$ is a positive constant. Setting $\\rho = 0$ removes the mixed second derivative term:\n$$\n\\rho\\,\\xi\\,v\\,S\\,\\frac{\\partial^{2} C}{\\partial S\\,\\partial v} = 0.\n$$\nWhen $v$ is constant (i.e., does not evolve stochastically), the option price $C$ depends on $v$ only parametrically and not dynamically, so the derivatives with respect to $v$ vanish:\n$$\n\\frac{\\partial C}{\\partial v} = 0, \\qquad \\frac{\\partial^{2} C}{\\partial v^{2}} = 0.\n$$\nWith these simplifications, the pricing PDE reduces to\n$$\n\\frac{\\partial C}{\\partial t}\n+ r\\,S\\,\\frac{\\partial C}{\\partial S}\n+ \\frac{1}{2}v\\,S^{2}\\,\\frac{\\partial^{2} C}{\\partial S^{2}}\n- r\\,C\n= 0.\n$$\n\nThis is exactly the Black–Scholes Partial Differential Equation (PDE) for a non-dividend-paying asset, with constant volatility $\\sigma_{\\mathrm{eff}}$ satisfying\n$$\n\\frac{1}{2}\\sigma_{\\mathrm{eff}}^{2}\\,S^{2} = \\frac{1}{2}v\\,S^{2}.\n$$\nTherefore,\n$$\n\\sigma_{\\mathrm{eff}}^{2} = v \\quad \\Longrightarrow \\quad \\sigma_{\\mathrm{eff}} = \\sqrt{v}.\n$$\n\nThe final answer requested is the effective Black–Scholes volatility parameter in terms of $v$.", "answer": "$$\\boxed{\\sqrt{v}}$$", "id": "3078353"}, {"introduction": "Analytical solutions are not always available, making numerical simulation an essential tool for quantitative finance. This practical exercise [@problem_id:3078440] moves from theory to code, tasking you with implementing the engine that drives the Heston simulation: the generation of correlated random shocks. By using the Cholesky decomposition method, you will master a fundamental technique required to build a full Monte Carlo simulator for pricing and risk management.", "problem": "You are given the task of simulating the correlated Brownian motion increments required by the Heston stochastic volatility model using a Cholesky decomposition of the target covariance matrix. In the Heston model, the asset price and variance dynamics are driven by two Brownian motions with instantaneous correlation. Your goal is to implement a simulation that preserves a prescribed correlation using only the foundational facts about Brownian motion and multivariate normal distributions.\n\nStarting point (fundamental base):\n- Let $B_t^{(1)}$ and $B_t^{(2)}$ be Brownian motions with instantaneous correlation $\\rho \\in (-1,1)$. Over a time step of length $\\Delta t  0$, the increments $\\Delta W^{(1)} = B_{t+\\Delta t}^{(1)} - B_t^{(1)}$ and $\\Delta W^{(2)} = B_{t+\\Delta t}^{(2)} - B_t^{(2)}$ satisfy:\n  - $\\mathbb{E}[\\Delta W^{(1)}] = 0$, $\\mathbb{E}[\\Delta W^{(2)}] = 0$,\n  - $\\operatorname{Var}(\\Delta W^{(1)}) = \\Delta t$, $\\operatorname{Var}(\\Delta W^{(2)}) = \\Delta t$,\n  - $\\operatorname{Cov}(\\Delta W^{(1)}, \\Delta W^{(2)}) = \\rho \\, \\Delta t$.\n- Therefore the target covariance matrix for the vector $\\Delta W = (\\Delta W^{(1)}, \\Delta W^{(2)})^\\top$ is\n  $$\\Sigma = \\Delta t \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}.$$\n- If $Z = (Z_1,Z_2)^\\top$ has independent and identically distributed (i.i.d.) standard normal components, and $L$ is a lower-triangular matrix satisfying $L L^\\top = \\Sigma$ (a Cholesky factor), then $\\Delta W = L Z$ has mean zero and covariance $\\Sigma$.\n\nTask:\n- Implement a program that, for each test case, uses the Cholesky decomposition approach to generate $N$ samples of correlated Brownian increments $(\\Delta W^{(1)}, \\Delta W^{(2)})$ with the desired correlation $\\rho$ over a single time step of length $\\Delta t$.\n- For numerical stability and clarity at this level, derive and use the explicit lower-triangular Cholesky factor for the $2\\times 2$ matrix $\\Sigma$, and then generate the samples by transforming i.i.d. standard normal random variables. Your implementation must use only the stated fundamental properties, without invoking any black-box multivariate samplers.\n- Compute the empirical Pearson correlation coefficient between the simulated $\\Delta W^{(1)}$ and $\\Delta W^{(2)}$ samples. Let $\\widehat{\\rho}$ denote this empirical correlation.\n\nWhat to compute:\n- For each test case, output a boolean indicating whether the absolute error $|\\widehat{\\rho} - \\rho|$ is less than or equal to a given tolerance $\\varepsilon$. Use zero-mean centering when computing the empirical correlation.\n\nTest suite:\nUse the following test cases, where each tuple is $(\\rho, \\Delta t, N, \\varepsilon, \\text{seed})$:\n- $(0.0, 0.05, 200000, 0.01, 1)$\n- $(0.5, 0.01, 200000, 0.01, 2)$\n- $(-0.7, 0.02, 250000, 0.01, 3)$\n- $(0.99, 0.01, 300000, 0.02, 4)$\n\nDefinitions and constraints:\n- You must treat $(\\Delta W^{(1)}, \\Delta W^{(2)})$ as a single-step vector of correlated Gaussian increments constructed from i.i.d. standard normal variables via a Cholesky factor $L$ with $L L^\\top = \\Sigma$.\n- The simulation must be reproducible using the provided pseudorandom number generator seed in each test case.\n- Angles are not involved. No physical units are required beyond the time step $\\Delta t$, which is a dimensionless scalar here.\n- The program must be self-contained and deterministic given the seeds.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\text{True}]$.\n\nDeliverables:\n- A complete, runnable program that:\n  - For each test case, generates the correlated Brownian increments using a Cholesky factor,\n  - Computes the empirical correlation $\\widehat{\\rho}$,\n  - Compares $|\\widehat{\\rho} - \\rho|$ to the tolerance $\\varepsilon$,\n  - Outputs a single line with a list of booleans indicating pass or fail for each test case, in the exact order given above.", "solution": "The problem statement is found to be valid. It is scientifically grounded in the principles of stochastic calculus and multivariate statistics, well-posed with a clear objective and all necessary parameters, and objective in its language and requirements. There are no contradictions, ambiguities, or factual errors. The task is a standard and well-defined exercise in computational finance: simulating correlated random processes using Cholesky decomposition.\n\nThe core of the problem is to generate samples of a two-dimensional random vector $\\Delta W = (\\Delta W^{(1)}, \\Delta W^{(2)})^\\top$ that follows a multivariate normal distribution with mean zero and a specified covariance matrix $\\Sigma$. The given properties are:\n- $\\mathbb{E}[\\Delta W^{(1)}] = \\mathbb{E}[\\Delta W^{(2)}] = 0$\n- $\\operatorname{Var}(\\Delta W^{(1)}) = \\operatorname{Var}(\\Delta W^{(2)}) = \\Delta t$\n- $\\operatorname{Cov}(\\Delta W^{(1)}, \\Delta W^{(2)}) = \\rho \\Delta t$\n\nThese properties define the covariance matrix $\\Sigma$ for the vector $\\Delta W$:\n$$\n\\Sigma = \\mathbb{E}[\\Delta W \\Delta W^\\top] = \\begin{pmatrix} \\mathbb{E}[(\\Delta W^{(1)})^2]  \\mathbb{E}[\\Delta W^{(1)} \\Delta W^{(2)}] \\\\ \\mathbb{E}[\\Delta W^{(2)} \\Delta W^{(1)}]  \\mathbb{E}[(\\Delta W^{(2)})^2] \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}(\\Delta W^{(1)})  \\operatorname{Cov}(\\Delta W^{(1)}, \\Delta W^{(2)}) \\\\ \\operatorname{Cov}(\\Delta W^{(1)}, \\Delta W^{(2)})  \\operatorname{Var}(\\Delta W^{(2)}) \\end{pmatrix}\n$$\nSubstituting the given values, we have:\n$$\n\\Sigma = \\begin{pmatrix} \\Delta t  \\rho \\Delta t \\\\ \\rho \\Delta t  \\Delta t \\end{pmatrix} = \\Delta t \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\nThe method specified is to use a Cholesky decomposition of $\\Sigma$. This involves finding a lower-triangular matrix $L$ such that $L L^\\top = \\Sigma$. The desired correlated variates $\\Delta W$ can then be generated by transforming a vector $Z = (Z_1, Z_2)^\\top$ of independent standard normal variates (i.e., $Z_1, Z_2 \\sim \\mathcal{N}(0,1)$ and are i.i.d.) via the linear transformation $\\Delta W = L Z$. This works because the covariance of the transformed vector is $\\mathbb{E}[(L Z)(L Z)^\\top] = L \\, \\mathbb{E}[Z Z^\\top] \\, L^\\top = L I L^\\top = L L^\\top = \\Sigma$, where $I$ is the identity matrix.\n\nFirst, we derive the explicit form of the Cholesky factor $L$. Let $L$ be a generic $2 \\times 2$ lower-triangular matrix:\n$$\nL = \\begin{pmatrix} l_{11}  0 \\\\ l_{21}  l_{22} \\end{pmatrix}\n$$\nThe condition $L L^\\top = \\Sigma$ becomes:\n$$\n\\begin{pmatrix} l_{11}  0 \\\\ l_{21}  l_{22} \\end{pmatrix} \\begin{pmatrix} l_{11}  l_{21} \\\\ 0  l_{22} \\end{pmatrix} = \\begin{pmatrix} l_{11}^2  l_{11} l_{21} \\\\ l_{11} l_{21}  l_{21}^2 + l_{22}^2 \\end{pmatrix} = \\begin{pmatrix} \\Delta t  \\rho \\Delta t \\\\ \\rho \\Delta t  \\Delta t \\end{pmatrix}\n$$\nBy equating the matrix elements, we can solve for $l_{11}$, $l_{21}$, and $l_{22}$:\n1.  From the $(1,1)$ element: $l_{11}^2 = \\Delta t$. We take the positive root by convention, so $l_{11} = \\sqrt{\\Delta t}$.\n2.  From the $(2,1)$ element: $l_{11} l_{21} = \\rho \\Delta t$. Substituting $l_{11}$, we get $\\sqrt{\\Delta t} \\, l_{21} = \\rho \\Delta t$, which yields $l_{21} = \\rho \\sqrt{\\Delta t}$.\n3.  From the $(2,2)$ element: $l_{21}^2 + l_{22}^2 = \\Delta t$. Substituting $l_{21}$, we have $(\\rho \\sqrt{\\Delta t})^2 + l_{22}^2 = \\Delta t$, which simplifies to $\\rho^2 \\Delta t + l_{22}^2 = \\Delta t$. Solving for $l_{22}^2$ gives $l_{22}^2 = \\Delta t (1 - \\rho^2)$. Taking the positive root, $l_{22} = \\sqrt{\\Delta t(1 - \\rho^2)}$.\n\nThus, the Cholesky factor is:\n$$\nL = \\begin{pmatrix} \\sqrt{\\Delta t}  0 \\\\ \\rho \\sqrt{\\Delta t}  \\sqrt{\\Delta t(1 - \\rho^2)} \\end{pmatrix} = \\sqrt{\\Delta t} \\begin{pmatrix} 1  0 \\\\ \\rho  \\sqrt{1 - \\rho^2} \\end{pmatrix}\n$$\nThis matrix is well-defined for $\\rho \\in (-1, 1)$, as specified in the problem.\n\nThe simulation procedure for each test case $(\\rho, \\Delta t, N, \\varepsilon, \\text{seed})$ is as follows:\n1.  Initialize a pseudorandom number generator with the given `seed` for reproducibility.\n2.  Generate $N$ independent samples of two standard normal variables, $Z_1 = (Z_{1,1}, \\dots, Z_{1,N})^\\top$ and $Z_2 = (Z_{2,1}, \\dots, Z_{2,N})^\\top$.\n3.  Apply the transformation $\\Delta W = L Z$ to each pair of samples $(Z_{1,i}, Z_{2,i})$ to generate the correlated increments $(\\Delta W_i^{(1)}, \\Delta W_i^{(2)})$:\n    $$\n    \\Delta W_i^{(1)} = \\sqrt{\\Delta t} \\, Z_{1,i}\n    $$\n    $$\n    \\Delta W_i^{(2)} = \\rho \\sqrt{\\Delta t} \\, Z_{1,i} + \\sqrt{\\Delta t(1 - \\rho^2)} \\, Z_{2,i}\n    $$\n4.  After generating $N$ samples of $(\\Delta W^{(1)}, \\Delta W^{(2)})$, compute the empirical Pearson correlation coefficient, $\\widehat{\\rho}$. The formula for $\\widehat{\\rho}$ between two sample sets $X = \\{x_i\\}$ and $Y = \\{y_i\\}$ of size $N$ is:\n    $$\n    \\widehat{\\rho} = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^N (y_i - \\bar{y})^2}}\n    $$\n    where $\\bar{x}$ and $\\bar{y}$ are the sample means. This corresponds to using \"zero-mean centering\" as requested.\n5.  Finally, determine if the absolute error between the empirical correlation and the target correlation is within the specified tolerance: $|\\widehat{\\rho} - \\rho| \\leq \\varepsilon$. The result is a boolean value (`True` or `False`).\n\nThis procedure is repeated for all test cases, and the boolean results are collected into a list for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates correlated Brownian motion increments using Cholesky decomposition\n    and validates the empirical correlation against a tolerance.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (rho, dt, N, eps, seed)\n        (0.0, 0.05, 200000, 0.01, 1),\n        (0.5, 0.01, 200000, 0.01, 2),\n        (-0.7, 0.02, 250000, 0.01, 3),\n        (0.99, 0.01, 300000, 0.02, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, dt, N, eps, seed = case\n\n        # Set the seed for the pseudorandom number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate N samples of two independent standard normal random variables.\n        # Z is a (2, N) array where Z[0,:] are the Z_1 samples and Z[1,:] are the Z_2 samples.\n        Z = rng.standard_normal(size=(2, N))\n        Z1 = Z[0]\n        Z2 = Z[1]\n\n        #\n        # Construct the correlated Brownian increments dW1 and dW2.\n        # The transformation is based on the Cholesky factor L of the covariance matrix:\n        # L = sqrt(dt) * [[1, 0], [rho, sqrt(1-rho^2)]]\n        # dW = L @ Z\n        # dW1 = sqrt(dt) * Z1\n        # dW2 = sqrt(dt) * (rho * Z1 + sqrt(1 - rho^2) * Z2)\n        #\n\n        sqrt_dt = np.sqrt(dt)\n        sqrt_1_minus_rho_sq = np.sqrt(1 - rho**2)\n\n        # Generate the N samples of correlated increments.\n        dW1 = sqrt_dt * Z1\n        dW2 = sqrt_dt * (rho * Z1 + sqrt_1_minus_rho_sq * Z2)\n\n        # Compute the empirical Pearson correlation coefficient between dW1 and dW2.\n        # np.corrcoef calculates the correlation matrix. We need the off-diagonal element.\n        # The standard formula for Pearson correlation involves centering the data by\n        # subtracting the sample mean, as specified by \"zero-mean centering\".\n        # np.corrcoef handles this correctly.\n        corr_matrix = np.corrcoef(dW1, dW2)\n        rho_hat = corr_matrix[0, 1]\n\n        # Check if the absolute error is within the given tolerance.\n        is_passed = np.abs(rho_hat - rho) = eps\n        results.append(is_passed)\n\n    # Format the final output string as a comma-separated list of booleans\n    # enclosed in square brackets. Python's str() on a boolean gives \"True\" or \"False\".\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3078440"}]}