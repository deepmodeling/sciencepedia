{"hands_on_practices": [{"introduction": "The first step in understanding weak convergence is to quantify it in a concrete setting. This practice uses the Ornstein-Uhlenbeck process, a cornerstone model in finance and physics, to directly calculate the weak error of the Euler-Maruyama scheme when approximating the mean. By deriving the exact analytical solution and comparing it to the numerical approximation, you will perform a first-principles analysis that reveals the scheme's characteristic linear convergence rate, a foundational result in the numerical analysis of SDEs. [@problem_id:3083367]", "problem": "Consider the Ornstein–Uhlenbeck stochastic differential equation (SDE) driven by a standard Brownian motion (BM) on the interval $[0,T]$ with deterministic initial condition $X_{0}=x_{0}\\in\\mathbb{R}$:\n$dX_{t} = -\\lambda X_{t}\\,dt + \\sigma\\,dW_{t}$,\nwhere $\\lambda>0$, $\\sigma\\in\\mathbb{R}$, and $W_{t}$ is a standard Brownian motion. Let the observable be $\\varphi(x)=x$. \n\nYour tasks are:\n- Using only fundamental properties of Itô calculus and linearity of expectation, derive from first principles a closed-form expression for $m(T)=\\mathbb{E}[\\varphi(X_{T})]=\\mathbb{E}[X_{T}]$ by obtaining and solving the deterministic ordinary differential equation satisfied by $m(t)=\\mathbb{E}[X_{t}]$ with $m(0)=x_{0}$.\n- Consider the Euler–Maruyama (EM) method with uniform time step $h>0$ and grid $t_{k}=kh$ for $k=0,1,\\dots,N$, where $N=T/h\\in\\mathbb{N}$. The scheme is\n$X^{h}_{k+1}=X^{h}_{k}-\\lambda X^{h}_{k}\\,h+\\sigma\\,(W_{t_{k+1}}-W_{t_{k}})$,\nwith $X^{h}_{0}=x_{0}$. Compute $\\mathbb{E}[X^{h}_{N}]$ exactly in terms of $x_{0}$, $\\lambda$, $h$, and $N$.\n- Define the weak error on the observable $\\varphi$ at time $T$ by $e_{\\text{weak}}(h)=\\mathbb{E}[\\varphi(X_{T})]-\\mathbb{E}[\\varphi(X^{h}_{N})]$. Using your exact expressions, write $e_{\\text{weak}}(h)$ explicitly as a function of $x_{0}$, $\\lambda$, $T$, and $h$. Then determine the leading-order behavior of $e_{\\text{weak}}(h)$ as $h\\to 0$ and state the weak order on this observable, justifying your conclusion from the expansion you obtain.\n\nProvide, as your final answer, the exact closed-form expression for $e_{\\text{weak}}(h)$ in terms of $x_{0}$, $\\lambda$, $T$, and $h$. No rounding is required.", "solution": "The problem is validated as scientifically sound, well-posed, objective, and self-contained. The Ornstein–Uhlenbeck process and the Euler–Maruyama method are standard topics in stochastic differential equations and their numerical analysis. All provided information is consistent and sufficient to derive the requested quantities.\n\nThe solution is organized into three parts as requested by the problem statement.\n\n### Part 1: Mean of the Exact Solution\n\nWe are given the Ornstein–Uhlenbeck stochastic differential equation (SDE):\n$$dX_{t} = -\\lambda X_{t}\\,dt + \\sigma\\,dW_{t}$$\nwith a deterministic initial condition $X_{0} = x_{0}$, where $\\lambda > 0$, $\\sigma \\in \\mathbb{R}$, and $W_{t}$ is a standard Brownian motion. We are interested in the observable $\\varphi(x)=x$, so we need to compute $m(T) = \\mathbb{E}[\\varphi(X_{T})] = \\mathbb{E}[X_{T}]$.\n\nFirst, we write the SDE in its integral form:\n$$X_{t} = X_{0} + \\int_{0}^{t} (-\\lambda X_{s})\\,ds + \\int_{0}^{t} \\sigma\\,dW_{s}$$\nLet $m(t) = \\mathbb{E}[X_{t}]$. We take the expectation of both sides of the integral equation. By the linearity of the expectation operator, we have:\n$$m(t) = \\mathbb{E}[X_{t}] = \\mathbb{E}\\left[ X_{0} - \\lambda \\int_{0}^{t} X_{s}\\,ds + \\sigma \\int_{0}^{t} dW_{s} \\right]$$\n$$m(t) = \\mathbb{E}[X_{0}] - \\lambda \\mathbb{E}\\left[\\int_{0}^{t} X_{s}\\,ds\\right] + \\sigma \\mathbb{E}\\left[\\int_{0}^{t} dW_{s}\\right]$$\n\nWe evaluate each term:\n1.  The initial condition is deterministic, so $\\mathbb{E}[X_{0}] = \\mathbb{E}[x_{0}] = x_{0}$.\n2.  The expectation of the Itô integral with a deterministic integrand over a standard Brownian motion is zero. Since $X_s$ is adapted, the stochastic Fubini theorem allows us to interchange expectation and integration, and for the Itô integral, we have $\\mathbb{E}\\left[\\int_{0}^{t} dW_{s}\\right] = 0$. This is a fundamental property of Itô calculus, as the integral is a martingale starting at $0$.\n3.  Under mild conditions on the process $X_s$ (which are met here), we can apply the stochastic Fubini theorem to interchange the expectation and the Lebesgue integral: $\\mathbb{E}\\left[\\int_{0}^{t} X_{s}\\,ds\\right] = \\int_{0}^{t} \\mathbb{E}[X_{s}]\\,ds = \\int_{0}^{t} m(s)\\,ds$.\n\nSubstituting these back into the equation for $m(t)$, we obtain an integral equation for the mean:\n$$m(t) = x_{0} - \\lambda \\int_{0}^{t} m(s)\\,ds$$\nThis is a Volterra integral equation of the first kind. Differentiating both sides with respect to $t$ using the Fundamental Theorem of Calculus yields the ordinary differential equation (ODE) for $m(t)$:\n$$\\frac{d m(t)}{dt} = -\\lambda m(t)$$\nThe initial condition is obtained by setting $t=0$ in the integral equation: $m(0) = x_{0}$.\n\nThis is a first-order linear homogeneous ODE. The solution is:\n$$m(t) = m(0)\\exp(-\\lambda t) = x_{0}\\exp(-\\lambda t)$$\nAt the final time $T$, the mean of the exact solution is:\n$$m(T) = \\mathbb{E}[X_{T}] = x_{0}\\exp(-\\lambda T)$$\n\n### Part 2: Mean of the Numerical Solution\n\nThe Euler–Maruyama (EM) scheme with time step $h$ is given by:\n$$X^{h}_{k+1} = X^{h}_{k} - \\lambda X^{h}_{k}\\,h + \\sigma\\,(W_{t_{k+1}} - W_{t_{k}})$$\nwith the initial condition $X^{h}_{0}=x_{0}$. This can be rewritten as:\n$$X^{h}_{k+1} = (1 - \\lambda h)X^{h}_{k} + \\sigma\\,\\Delta W_{k}$$\nwhere $\\Delta W_{k} = W_{t_{k+1}} - W_{t_{k}}$ is the increment of the Brownian motion over the interval $[t_k, t_{k+1}]$.\n\nLet $\\mu_{k} = \\mathbb{E}[X^{h}_{k}]$. We take the expectation of the scheme:\n$$\\mathbb{E}[X^{h}_{k+1}] = \\mathbb{E}\\left[ (1 - \\lambda h)X^{h}_{k} + \\sigma\\,\\Delta W_{k} \\right]$$\nUsing the linearity of expectation:\n$$\\mu_{k+1} = (1 - \\lambda h)\\mathbb{E}[X^{h}_{k}] + \\sigma\\mathbb{E}[\\Delta W_{k}]$$\nThe increments of a standard Brownian motion $\\Delta W_{k}$ are independent and identically distributed normal random variables with mean $0$ and variance $h$. Thus, $\\mathbb{E}[\\Delta W_{k}] = 0$.\nThe recurrence relation for the mean of the numerical solution simplifies to:\n$$\\mu_{k+1} = (1 - \\lambda h)\\mu_{k}$$\nThe initial condition is $\\mu_{0} = \\mathbb{E}[X^{h}_{0}] = \\mathbb{E}[x_{0}] = x_{0}$.\n\nThis is a geometric progression. The solution at step $k$ is given by:\n$$\\mu_{k} = (1 - \\lambda h)^{k} \\mu_{0} = x_{0}(1 - \\lambda h)^{k}$$\nWe need the mean at the final time $T$, which corresponds to the $N$-th step, where $N=T/h$. Thus, for $k=N$:\n$$\\mathbb{E}[X^{h}_{N}] = x_{0}(1 - \\lambda h)^{N}$$\n\n### Part 3: Weak Error and Leading-Order Analysis\n\nThe weak error on the observable $\\varphi(x)=x$ at time $T$ is defined as:\n$$e_{\\text{weak}}(h) = \\mathbb{E}[\\varphi(X_{T})] - \\mathbb{E}[\\varphi(X^{h}_{N})] = \\mathbb{E}[X_{T}] - \\mathbb{E}[X^{h}_{N}]$$\nSubstituting the exact expressions derived in the previous parts:\n$$\\mathbb{E}[X_{T}] = x_{0}\\exp(-\\lambda T)$$\n$$\\mathbb{E}[X^{h}_{N}] = x_{0}(1 - \\lambda h)^{N}$$\nUsing the relation $N=T/h$, we can write $\\mathbb{E}[X^{h}_{N}]$ as a function of $h$ and $T$:\n$$\\mathbb{E}[X^{h}_{N}] = x_{0}(1 - \\lambda h)^{T/h}$$\nTherefore, the exact expression for the weak error is:\n$$e_{\\text{weak}}(h) = x_{0}\\exp(-\\lambda T) - x_{0}(1 - \\lambda h)^{T/h} = x_{0}\\left(\\exp(-\\lambda T) - (1 - \\lambda h)^{T/h}\\right)$$\n\nTo determine the leading-order behavior as $h \\to 0$, we find the Taylor expansion of the term $(1 - \\lambda h)^{T/h}$. We can write this term using the exponential function:\n$$(1 - \\lambda h)^{T/h} = \\exp\\left(\\frac{T}{h} \\ln(1 - \\lambda h)\\right)$$\nFor small $h$, we use the Taylor series for the natural logarithm, $\\ln(1-u) = -u - \\frac{u^2}{2} - O(u^3)$, setting $u = \\lambda h$:\n$$\\ln(1 - \\lambda h) = -\\lambda h - \\frac{(\\lambda h)^2}{2} - O(h^3)$$\nSubstituting this into the exponent:\n$$\\frac{T}{h}\\ln(1 - \\lambda h) = \\frac{T}{h}\\left(-\\lambda h - \\frac{\\lambda^2 h^2}{2} - O(h^3)\\right) = -\\lambda T - \\frac{\\lambda^2 T}{2}h - O(h^2)$$\nSo, the numerical mean expression becomes:\n$$(1 - \\lambda h)^{T/h} = \\exp\\left(-\\lambda T - \\frac{\\lambda^2 T}{2}h - O(h^2)\\right) = \\exp(-\\lambda T)\\exp\\left(-\\frac{\\lambda^2 T}{2}h - O(h^2)\\right)$$\nNow, we use the Taylor series for the exponential function, $\\exp(v) = 1 + v + O(v^2)$, with $v = -\\frac{\\lambda^2 T}{2}h$:\n$$\\exp\\left(-\\frac{\\lambda^2 T}{2}h - O(h^2)\\right) = 1 - \\frac{\\lambda^2 T}{2}h + O(h^2)$$\nPutting it all together:\n$$(1 - \\lambda h)^{T/h} = \\exp(-\\lambda T)\\left(1 - \\frac{\\lambda^2 T}{2}h + O(h^2)\\right) = \\exp(-\\lambda T) - \\frac{\\lambda^2 T}{2}\\exp(-\\lambda T)h + O(h^2)$$\nNow we substitute this expansion back into the weak error expression:\n$$e_{\\text{weak}}(h) = x_{0}\\left(\\exp(-\\lambda T) - \\left[\\exp(-\\lambda T) - \\frac{\\lambda^2 T}{2}\\exp(-\\lambda T)h + O(h^2)\\right]\\right)$$\n$$e_{\\text{weak}}(h) = x_{0}\\left(\\frac{\\lambda^2 T}{2}\\exp(-\\lambda T)h - O(h^2)\\right)$$\n$$e_{\\text{weak}}(h) = \\frac{x_{0}\\lambda^2 T}{2}\\exp(-\\lambda T)h + O(h^2)$$\nThe leading-order behavior of the weak error is proportional to $h$. Therefore, the weak order of convergence for the Euler-Maruyama method on this observable is $1$.", "answer": "$$\\boxed{x_{0}\\left(\\exp(-\\lambda T) - (1 - \\lambda h)^{T/h}\\right)}$$", "id": "3083367"}, {"introduction": "After establishing a baseline weak order for a smooth observable, it is crucial to investigate the limits of this concept. This exercise presents a thought-provoking scenario where the convergence order degrades for non-smooth test functions, such as those used to calculate probabilities. This practice highlights the critical fact that weak order is not an absolute property of a numerical scheme, but depends intimately on the regularity of the quantity being measured. [@problem_id:3083343]", "problem": "Consider the scalar stochastic differential equation $dX_{t} = dW_{t}$ with $X_{0} = 0$ over the time horizon $T = 1$, where $W_{t}$ is a standard Wiener process. Define a time step $h = 1/N$ with $N \\in \\mathbb{N}$ and consider the following weak approximation scheme for the terminal value:\n- The scheme is defined recursively by $Y_{k+1} = Y_{k} + \\xi_{k+1} \\sqrt{h}$ with $Y_{0} = 0$, where $\\{\\xi_{k}\\}_{k=1}^{N}$ are independent and identically distributed random variables taking values $\\pm 1$ with probability $1/2$ each, and independent of $X_{0}$.\n- Equivalently, $Y_{N} = \\sqrt{h} \\sum_{k=1}^{N} \\xi_{k}$.\n\nRecall the definition of weak convergence order $p$ on terminal functionals: a scheme is said to have weak order $p$ if there exists a constant $C$ independent of $h$ such that for all sufficiently small $h$ and all test functions $\\varphi$ in a specified class,\n$$\n\\left| \\mathbb{E}\\left[\\varphi\\left(Y_{N}\\right)\\right] - \\mathbb{E}\\left[\\varphi\\left(X_{1}\\right)\\right] \\right| \\le C h^{p}.\n$$\n\nYour tasks are:\n- Using only the properties of independent increments and Taylor expansion from real analysis, justify why this scheme achieves weak order $p=1$ for all twice continuously differentiable and bounded test functions $\\varphi \\in C_{b}^{2}(\\mathbb{R})$. Your reasoning must start from the definition of weak convergence order and rely only on foundational facts (such as central limit theorem refinements for smooth functions), not on prepackaged formulas specific to this scheme.\n- Now take the discontinuous test function $\\varphi(x) = \\mathbf{1}_{\\{x \\le 0\\}}$. Determine the largest real number $p^{\\star}$ such that\n$$\n\\left| \\mathbb{P}\\left(Y_{N} \\le 0\\right) - \\mathbb{P}\\left(X_{1} \\le 0\\right) \\right| = \\mathcal{O}\\left(h^{p^{\\star}}\\right) \\quad \\text{as } h \\to 0.\n$$\nProvide your derivation from first principles, beginning from the explicit law of $Y_{N}$ and the exact distribution of $X_{1}$, and invoking only standard, well-tested results from probability theory for sums of independent and identically distributed random variables. Report the final value of $p^{\\star}$ as a single real number. No units are required.", "solution": "This problem consists of two parts. The first part asks for a justification of the weak convergence order for a specific numerical scheme applied to a simple SDE, for smooth test functions. The second part asks for the determination of the convergence order for a specific discontinuous test function.\n\n**Part 1: Weak Convergence Order for Smooth Functions**\n\nThe stochastic differential equation (SDE) is $dX_{t} = dW_{t}$ with $X_{0} = 0$. The solution at time $T=1$ is $X_{1} = \\int_{0}^{1} dW_{t} = W_{1}$, which is a standard normal random variable, $X_{1} \\sim \\mathcal{N}(0, 1)$.\n\nThe numerical approximation for $X_{1}$ is given by $Y_{N}$, where $h=1/N$. The scheme is defined as $Y_{k+1} = Y_{k} + \\xi_{k+1}\\sqrt{h}$ with $Y_{0}=0$. The terminal value is $Y_{N} = \\sum_{k=1}^{N} \\xi_{k} \\sqrt{h}$. The random variables $\\{\\xi_{k}\\}_{k=1}^{N}$ are independent and identically distributed (i.i.d.), with $\\mathbb{P}(\\xi_{k}=1) = \\mathbb{P}(\\xi_{k}=-1) = 1/2$.\n\nWe can rewrite $Y_{N}$ as:\n$$\nY_{N} = \\sqrt{h} \\sum_{k=1}^{N} \\xi_{k} = \\frac{1}{\\sqrt{N}} \\sum_{k=1}^{N} \\xi_{k}\n$$\nThis shows that $Y_{N}$ is the standardized sum of the i.i.d. random variables $\\xi_{k}$. Let us compute the first few moments of $\\xi_{k}$:\n- Mean: $\\mathbb{E}[\\xi_{k}] = (1) \\cdot \\frac{1}{2} + (-1) \\cdot \\frac{1}{2} = 0$.\n- Variance: $\\mathbb{E}[\\xi_{k}^{2}] = (1)^{2} \\cdot \\frac{1}{2} + (-1)^{2} \\cdot \\frac{1}{2} = 1$.\n- Third moment: $\\mathbb{E}[\\xi_{k}^{3}] = (1)^{3} \\cdot \\frac{1}{2} + (-1)^{3} \\cdot \\frac{1}{2} = 0$.\n- Fourth moment: $\\mathbb{E}[\\xi_{k}^{4}] = (1)^{4} \\cdot \\frac{1}{2} + (-1)^{4} \\cdot \\frac{1}{2} = 1$.\n\nThe problem asks to justify that the weak order of convergence is $p=1$ for any test function $\\varphi \\in C_{b}^{2}(\\mathbb{R})$. This means showing that $|\\mathbb{E}[\\varphi(Y_{N})] - \\mathbb{E}[\\varphi(X_{1})]| \\le C h^{1}$ for some constant $C$ independent of $h$.\n\nThe quantity $Y_N = \\frac{1}{\\sqrt{N}}\\sum_{k=1}^N \\xi_k$ is a standardized sum of i.i.d. random variables satisfying the conditions of the Central Limit Theorem (CLT), which states that $Y_N$ converges in distribution to a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$, which has the same distribution as $X_1$. Our task is to determine the rate of this convergence for smooth test functions.\n\nThis is a classical result in probability theory, related to Edgeworth expansions and refinements of the CLT. A foundational result states that for a standardized sum $Z_N = \\frac{1}{\\sqrt{N}}\\sum_{k=1}^N \\xi_k$ of i.i.d. random variables with $\\mathbb{E}[\\xi_k]=0$ and $\\mathbb{E}[\\xi_k^2]=1$, the error in expectation for a sufficiently smooth test function $\\varphi$ can be bounded. Specifically, if $\\mathbb{E}[|\\xi_k|^3] < \\infty$ and $\\varphi \\in C_b^3(\\mathbb{R})$, the error is of order $O(1/\\sqrt{N})$.\n$$\n\\left| \\mathbb{E}\\left[\\varphi\\left(Y_{N}\\right)\\right] - \\mathbb{E}\\left[\\varphi\\left(X_{1}\\right)\\right] \\right| = \\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right) = \\mathcal{O}(\\sqrt{h})\n$$\nHowever, in our specific case, the random variables $\\xi_k$ are symmetric, which implies that their third moment is zero: $\\mathbb{E}[\\xi_k^3] = 0$. This special condition improves the rate of convergence.\n\nThe general one-term Edgeworth expansion for the expectation of a smooth function is given by:\n$$\n\\mathbb{E}[\\varphi(Y_N)] = \\mathbb{E}[\\varphi(Z)] + \\frac{\\mathbb{E}[\\xi^3]}{6\\sqrt{N}}\\mathbb{E}[\\varphi'''(Z)] + \\frac{\\mathbb{E}[\\xi^4]-3}{24N}\\mathbb{E}[\\varphi^{(4)}(Z)] + O(N^{-3/2})\n$$\nSince $\\mathbb{E}[\\xi^3]=0$, the term of order $1/\\sqrt{N}$ vanishes. The leading error term is then of order $1/N$. More rigorously, for i.i.d. random variables with zero mean, unit variance, and zero third moment, it is a standard result from the theory of CLT refinements that for any test function $\\varphi \\in C_{b}^{2}(\\mathbb{R})$, the convergence rate is of order $O(1/N)$. The bound is of the form:\n$$\n\\left| \\mathbb{E}\\left[\\varphi\\left(Y_{N}\\right)\\right] - \\mathbb{E}\\left[\\varphi\\left(X_{1}\\right)\\right] \\right| \\le \\frac{C_{\\varphi}}{N}\n$$\nwhere the constant $C_{\\varphi}$ depends on the test function $\\varphi$ (typically through bounds on its derivatives up to the second order and its modulus of continuity), but not on $N$.\n\nSince the time step $h$ is defined as $h=1/N$, we can express the error in terms of $h$:\n$$\n\\left| \\mathbb{E}\\left[\\varphi\\left(Y_{N}\\right)\\right] - \\mathbb{E}\\left[\\varphi\\left(X_{1}\\right)\\right] \\right| \\le C_{\\varphi} h\n$$\nThis inequality matches the definition of weak convergence order $p=1$. The justification relies on the fact that the underlying random variables $\\xi_k$ of the numerical scheme have a vanishing third moment, which is a stronger condition than required for the basic Euler-Maruyama scheme and leads to a higher order of convergence (or, in this case, satisfies the expected weak order $1$ with lower regularity requirements on $\\varphi$). The prompt's reference to \"$C_b^2$\" and \"central limit theorem refinements\" points directly to this line of reasoning.\n\n**Part 2: Convergence Order for a Discontinuous Test Function**\n\nWe now consider the discontinuous test function $\\varphi(x) = \\mathbf{1}_{\\{x \\le 0\\}}$, which is the indicator function for the set $(-\\infty, 0]$. The expectation $\\mathbb{E}[\\varphi(A)]$ is the probability $\\mathbb{P}(A \\le 0)$. We need to find the largest $p^{\\star}$ such that\n$$\n\\left| \\mathbb{P}\\left(Y_{N} \\le 0\\right) - \\mathbb{P}\\left(X_{1} \\le 0\\right) \\right| = \\mathcal{O}\\left(h^{p^{\\star}}\\right).\n$$\n\nFirst, we determine the exact probabilities.\nFor the exact solution $X_1 \\sim \\mathcal{N}(0,1)$, the probability density function is symmetric about $0$. Therefore,\n$$\n\\mathbb{P}(X_1 \\le 0) = \\frac{1}{2}.\n$$\n\nFor the numerical solution $Y_N = \\frac{1}{\\sqrt{N}} \\sum_{k=1}^{N} \\xi_{k}$, the condition $Y_N \\le 0$ is equivalent to $\\sum_{k=1}^{N} \\xi_{k} \\le 0$. Let $S_N = \\sum_{k=1}^{N} \\xi_{k}$. Let $K$ be the number of $\\xi_k$ that take the value $+1$. Then $N-K$ variables take the value $-1$. The sum is $S_N = 1 \\cdot K + (-1) \\cdot (N-K) = 2K-N$. The random variable $K$ follows a binomial distribution $K \\sim B(N, 1/2)$, since each $\\xi_k$ is chosen independently with probability $1/2$. The condition $S_N \\le 0$ becomes $2K-N \\le 0$, which is equivalent to $K \\le N/2$.\nSo we need to evaluate $\\mathbb{P}(K \\le N/2)$ for $K \\sim B(N, 1/2)$. We must analyze the cases where $N$ is even or odd.\n\nCase 1: $N$ is odd.\nLet $N=2m+1$ for some integer $m \\ge 0$. The condition is $K \\le (2m+1)/2 = m+1/2$. Since $K$ is integer-valued, this is equivalent to $K \\le m$. The binomial distribution $B(2m+1, 1/2)$ is symmetric about its mean $(2m+1)/2 = m+1/2$. This means $\\mathbb{P}(K=j) = \\mathbb{P}(K=2m+1-j)$.\nThen, $\\mathbb{P}(K \\le m) = \\sum_{j=0}^{m} \\mathbb{P}(K=j) = \\sum_{j=0}^{m} \\mathbb{P}(K=2m+1-j)$. Letting $l=2m+1-j$, as $j$ goes from $0$ to $m$, $l$ goes from $2m+1$ down to $m+1$. So $\\mathbb{P}(K \\le m) = \\sum_{l=m+1}^{2m+1} \\mathbb{P}(K=l) = \\mathbb{P}(K \\ge m+1)$.\nSince $\\mathbb{P}(K \\le m) + \\mathbb{P}(K \\ge m+1) = 1$, we must have $\\mathbb{P}(K \\le m) = 1/2$.\nFor odd $N$, the error is $|\\mathbb{P}(Y_N \\le 0) - \\mathbb{P}(X_1 \\le 0)| = |1/2 - 1/2| = 0$.\n\nCase 2: $N$ is even.\nLet $N=2m$ for some integer $m \\ge 1$. The condition is $K \\le (2m)/2 = m$. We need to compute $\\mathbb{P}(K \\le m)$ for $K \\sim B(2m, 1/2)$.\nThe total probability is $1 = \\sum_{j=0}^{2m} \\mathbb{P}(K=j) = \\mathbb{P}(K < m) + \\mathbb{P}(K=m) + \\mathbb{P}(K > m)$.\nBy symmetry of the $B(2m, 1/2)$ distribution about its mean $m$, we have $\\mathbb{P}(K < m) = \\mathbb{P}(K > m)$.\nThus, $1 = 2 \\mathbb{P}(K < m) + \\mathbb{P}(K=m)$, which gives $\\mathbb{P}(K < m) = \\frac{1 - \\mathbb{P}(K=m)}{2}$.\nThen, $\\mathbb{P}(K \\le m) = \\mathbb{P}(K < m) + \\mathbb{P}(K=m) = \\frac{1 - \\mathbb{P}(K=m)}{2} + \\mathbb{P}(K=m) = \\frac{1+\\mathbb{P}(K=m)}{2}$.\nThe error is $|\\mathbb{P}(Y_N \\le 0) - \\mathbb{P}(X_1 \\le 0)| = \\left| \\frac{1+\\mathbb{P}(K=m)}{2} - \\frac{1}{2} \\right| = \\frac{1}{2}\\mathbb{P}(K=m)$.\nHere, $\\mathbb{P}(K=m) = \\binom{2m}{m} \\left(\\frac{1}{2}\\right)^{2m}$.\n\nTo find the asymptotic order as $h \\to 0$ (which means $N \\to \\infty$, so $m \\to \\infty$), we use Stirling's approximation for the central binomial coefficient: $n! \\sim \\sqrt{2\\pi n} (n/e)^n$.\n$$\n\\binom{2m}{m} = \\frac{(2m)!}{(m!)^2} \\sim \\frac{\\sqrt{2\\pi (2m)} (2m/e)^{2m}}{(\\sqrt{2\\pi m} (m/e)^m)^2} = \\frac{\\sqrt{4\\pi m} (2m)^{2m} e^{-2m}}{2\\pi m (m^m)^2 e^{-2m}} = \\frac{2\\sqrt{\\pi m} \\cdot 4^m m^{2m}}{2\\pi m \\cdot m^{2m}} = \\frac{4^m}{\\sqrt{\\pi m}}.\n$$\nSo, $\\mathbb{P}(K=m) = \\binom{2m}{m} \\frac{1}{4^m} \\sim \\frac{4^m}{\\sqrt{\\pi m}} \\frac{1}{4^m} = \\frac{1}{\\sqrt{\\pi m}}$.\nThe error is $\\frac{1}{2}\\mathbb{P}(K=m) \\sim \\frac{1}{2\\sqrt{\\pi m}}$.\n\nWe relate this back to $h$. Since $N=2m$ and $h=1/N$, we have $m=N/2 = 1/(2h)$.\nThe error is of the order:\n$$\n\\frac{1}{2\\sqrt{\\pi (1/(2h))}} = \\frac{1}{2\\sqrt{\\pi/(2h)}} = \\frac{\\sqrt{2h}}{2\\sqrt{\\pi}} = \\frac{1}{\\sqrt{2\\pi}} h^{1/2}.\n$$\nThe error is $\\mathcal{O}(h^{1/2})$.\n\nThe convergence analysis requires us to find a single power $p^{\\star}$ that describes the error for all sufficiently small $h$ (i.e., for all large $N$). Although the error is $0$ for odd $N$, it is $\\mathcal{O}(h^{1/2})$ for even $N$. The overall rate is determined by the worst-case (slowest) decay, which is $\\mathcal{O}(h^{1/2})$. The definition $|\\text{error}| \\le C h^{p^{\\star}}$ must hold for all $N$ (even and odd) greater than some threshold. If we choose $p^{\\star}=1/2$, we can find a constant $C$ (e.g., $C=1/\\sqrt{2\\pi}$) that works for the subsequence of even $N$, and for odd $N$, $0 \\le C h^{1/2}$ is also true. A higher power like $p^{\\star} > 1/2$ would fail for the even $N$. Therefore, the largest such exponent is $1/2$.\n\nThis result is consistent with the Berry-Esseen theorem, which states that for i.i.d. variables with finite third moment, the maximum difference between the CDF of the standardized sum and the standard normal CDF is of order $O(1/\\sqrt{N}) = O(\\sqrt{h})$. Our calculation shows that this rate is achieved at the point $x=0$.\n\nThe largest real number $p^{\\star}$ is $1/2$.", "answer": "$$\\boxed{0.5}$$", "id": "3083343"}, {"introduction": "Finally, we bridge the gap between theoretical error analysis and the realities of computational simulation. This problem demonstrates how the weak convergence order governs the overall efficiency of a Monte Carlo simulation, forcing a trade-off between discretization error (from the time step $h$) and statistical error (from the number of samples $M$). Understanding this relationship is essential for designing efficient numerical experiments and managing computational budgets when solving SDEs. [@problem_id:3083399]", "problem": "Consider a one-dimensional stochastic differential equation (SDE) given by $dX_{t} = a(X_{t})\\,dt + b(X_{t})\\,dW_{t}$ with deterministic initial condition $X_{0} = x_{0}$ on a fixed time horizon $[0,T]$, where $a$ and $b$ are globally Lipschitz continuous functions and $(W_{t})_{t \\geq 0}$ is a standard Brownian motion. Let $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ be a test function with polynomial growth, and denote the quantity of interest by $\\mu := \\mathbb{E}[\\varphi(X_{T})]$.\n\nTo approximate $\\mu$, consider the Euler–Maruyama (EM) scheme with time step $h>0$ and $N := T/h \\in \\mathbb{N}$ steps:\n$$\nY_{0} := x_{0}, \\quad\nY_{n+1} := Y_{n} + a(Y_{n})\\,h + b(Y_{n})\\,\\Delta W_{n}, \\quad \\Delta W_{n} \\sim \\mathcal{N}(0,h), \\quad n=0,1,\\dots,N-1.\n$$\nDefine the Monte Carlo estimator based on $M \\in \\mathbb{N}$ independent and identically distributed paths $(Y_{N}^{(m)})_{m=1}^{M}$ of the EM terminal value:\n$$\n\\widehat{\\mu}_{M,h} := \\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big).\n$$\n\nAssume the following two properties hold uniformly for sufficiently small $h$:\n1. Weak order one for Euler–Maruyama: there exists a constant $C_{w}>0$ such that\n$$\n\\big|\\mathbb{E}[\\varphi(Y_{N})] - \\mathbb{E}[\\varphi(X_{T})]\\big| \\leq C_{w}\\,h.\n$$\n2. Unit variance bound for the payoff under discretization: there exists a constant $C_{v} \\leq 1$ such that\n$$\n\\mathrm{Var}\\big(\\varphi(Y_{N})\\big) \\leq C_{v} \\leq 1.\n$$\n\nUsing the mean squared error (MSE) of the estimator $\\widehat{\\mu}_{M,h}$, defined by\n$$\n\\mathrm{MSE}(M,h) := \\mathbb{E}\\Big[\\big(\\widehat{\\mu}_{M,h} - \\mu\\big)^{2}\\Big],\n$$\nderive the asymptotic scalings of the time step $h$ and the number of samples $M$ in terms of a user-prescribed accuracy $\\varepsilon \\in (0,1)$ such that the root-mean-square error $\\sqrt{\\mathrm{MSE}(M,h)}$ is bounded by $\\varepsilon$ up to multiplicative constants that are independent of $\\varepsilon$. Express your final answer as a single row matrix containing the leading-order scalings of $h$ and $M$ in terms of $\\varepsilon$.", "solution": "The problem statement is parsed and validated as follows.\n\n### Step 1: Extract Givens\n- **Stochastic Differential Equation (SDE):** $dX_{t} = a(X_{t})\\,dt + b(X_{t})\\,dW_{t}$\n- **Initial Condition:** $X_{0} = x_{0}$ (deterministic)\n- **Time Horizon:** $[0,T]$\n- **Coefficients:** $a$ and $b$ are globally Lipschitz continuous.\n- **Stochastic Process:** $(W_{t})_{t \\geq 0}$ is a standard Brownian motion.\n- **Test Function:** $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ with polynomial growth.\n- **Quantity of Interest:** $\\mu := \\mathbb{E}[\\varphi(X_{T})]$\n- **Euler–Maruyama (EM) Scheme:**\n  - Time step $h>0$ with $N := T/h \\in \\mathbb{N}$.\n  - $Y_{0} := x_{0}$\n  - $Y_{n+1} := Y_{n} + a(Y_{n})\\,h + b(Y_{n})\\,\\Delta W_{n}$ for $n=0,1,\\dots,N-1$.\n  - $\\Delta W_{n}$ are independent and identically distributed (i.i.d.) with $\\Delta W_{n} \\sim \\mathcal{N}(0,h)$.\n- **Monte Carlo Estimator:**\n  - $\\widehat{\\mu}_{M,h} := \\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big)$, where $M \\in \\mathbb{N}$ is the number of i.i.d. paths.\n- **Assumption 1 (Weak Order):** There exists a constant $C_{w}>0$ such that $\\big|\\mathbb{E}[\\varphi(Y_{N})] - \\mathbb{E}[\\varphi(X_{T})]\\big| \\leq C_{w}\\,h$.\n- **Assumption 2 (Variance Bound):** There exists a constant $C_{v} \\leq 1$ such that $\\mathrm{Var}\\big(\\varphi(Y_{N})\\big) \\leq C_{v}$.\n- **Error Metric:** Mean Squared Error (MSE), $\\mathrm{MSE}(M,h) := \\mathbb{E}\\Big[\\big(\\widehat{\\mu}_{M,h} - \\mu\\big)^{2}\\Big]$.\n- **Objective:** Derive the asymptotic scalings of $h$ and $M$ in terms of a prescribed accuracy $\\varepsilon \\in (0,1)$ such that $\\sqrt{\\mathrm{MSE}(M,h)} \\leq \\varepsilon$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically well-grounded in the field of numerical analysis for stochastic differential equations. It presents a standard task: analyzing the error of a Monte Carlo method combined with a time discretization scheme. The premises, including the form of the SDE, the properties of the coefficients, the Euler-Maruyama scheme, and the assumptions on weak order and variance, are all standard in the literature (e.g., in the works of Kloeden & Platen, or Glasserman). The problem is well-posed, objective, self-contained, and free from any scientific or logical flaws.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution Derivation\nThe goal is to find the asymptotic scalings of the time step $h$ and the number of Monte Carlo samples $M$ that ensure the root-mean-square error (RMSE) is bounded by a given tolerance $\\varepsilon$. The RMSE is defined as $\\sqrt{\\mathrm{MSE}(M,h)}$. The condition is $\\sqrt{\\mathrm{MSE}(M,h)} \\leq \\varepsilon$, which is equivalent to $\\mathrm{MSE}(M,h) \\leq \\varepsilon^{2}$.\n\nThe Mean Squared Error can be decomposed into the square of the bias and the variance of the estimator:\n$$\n\\mathrm{MSE}(M,h) = \\mathbb{E}\\Big[\\big(\\widehat{\\mu}_{M,h} - \\mu\\big)^{2}\\Big] = \\Big(\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] - \\mu\\Big)^{2} + \\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big).\n$$\n\nFirst, we analyze the bias term, $\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] - \\mu$. The expectation of the estimator $\\widehat{\\mu}_{M,h}$ is:\n$$\n\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] = \\mathbb{E}\\left[\\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big)\\right].\n$$\nBy linearity of expectation, and since the paths $(Y_{N}^{(m)})_{m=1}^{M}$ are independent and identically distributed, we have:\n$$\n\\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] = \\frac{1}{M}\\sum_{m=1}^{M} \\mathbb{E}\\left[\\varphi\\big(Y_{N}^{(m)}\\big)\\right] = \\frac{1}{M} \\cdot M \\cdot \\mathbb{E}\\left[\\varphi\\big(Y_{N}\\big)\\right] = \\mathbb{E}\\left[\\varphi\\big(Y_{N}\\big)\\right].\n$$\nThe bias is therefore the weak error of the Euler-Maruyama scheme:\n$$\n\\text{Bias} = \\mathbb{E}\\big[\\widehat{\\mu}_{M,h}\\big] - \\mu = \\mathbb{E}\\left[\\varphi\\big(Y_{N}\\big)\\right] - \\mathbb{E}\\left[\\varphi\\big(X_{T}\\big)\\right].\n$$\nUsing Assumption 1, the magnitude of the bias is bounded by:\n$$\n\\big|\\text{Bias}\\big| \\leq C_{w}h.\n$$\nThus, the squared bias is bounded by:\n$$\n(\\text{Bias})^{2} \\leq (C_{w}h)^{2} = C_{w}^{2}h^{2}.\n$$\n\nSecond, we analyze the variance term, $\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big)$.\n$$\n\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big) = \\mathrm{Var}\\left(\\frac{1}{M}\\sum_{m=1}^{M} \\varphi\\big(Y_{N}^{(m)}\\big)\\right).\n$$\nSince the paths are i.i.d., the random variables $\\varphi\\big(Y_{N}^{(m)}\\big)$ are also i.i.d. For a sum of i.i.d. random variables, the variance is:\n$$\n\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big) = \\frac{1}{M^{2}}\\sum_{m=1}^{M} \\mathrm{Var}\\left(\\varphi\\big(Y_{N}^{(m)}\\big)\\right) = \\frac{1}{M^{2}} \\cdot M \\cdot \\mathrm{Var}\\left(\\varphi\\big(Y_{N}\\big)\\right) = \\frac{1}{M} \\mathrm{Var}\\left(\\varphi\\big(Y_{N}\\big)\\right).\n$$\nUsing Assumption 2, the variance is bounded by:\n$$\n\\mathrm{Var}\\big(\\widehat{\\mu}_{M,h}\\big) \\leq \\frac{C_{v}}{M}.\n$$\n\nCombining the bounds for the squared bias and the variance, we get an upper bound for the MSE:\n$$\n\\mathrm{MSE}(M,h) \\leq C_{w}^{2}h^{2} + \\frac{C_{v}}{M}.\n$$\nTo satisfy the accuracy requirement $\\mathrm{MSE}(M,h) \\leq \\varepsilon^{2}$, we must have:\n$$\nC_{w}^{2}h^{2} + \\frac{C_{v}}{M} \\leq \\varepsilon^{2}.\n$$\nTo find the asymptotic scalings of $h$ and $M$ with respect to $\\varepsilon$, we need to choose $h$ and $M$ such that this inequality holds. A standard approach is to balance the two error components, the discretization error (from $h$) and the statistical error (from $M$), by making them both of the same order of magnitude as the target total squared error, $\\varepsilon^{2}$. We can require each term to be bounded by $\\frac{\\varepsilon^{2}}{2}$:\n$$\nC_{w}^{2}h^{2} \\leq \\frac{\\varepsilon^{2}}{2} \\quad \\text{and} \\quad \\frac{C_{v}}{M} \\leq \\frac{\\varepsilon^{2}}{2}.\n$$\nSumming these two inequalities guarantees that the total MSE is bounded by $\\varepsilon^{2}$.\n\nFrom the first inequality, we derive the scaling for $h$:\n$$\nh^{2} \\leq \\frac{\\varepsilon^{2}}{2C_{w}^{2}} \\implies h \\leq \\frac{\\varepsilon}{\\sqrt{2}C_{w}}.\n$$\nThis indicates that to achieve the desired accuracy, $h$ must be chosen to be proportional to $\\varepsilon$. The asymptotic scaling is $h = \\mathcal{O}(\\varepsilon)$.\n\nFrom the second inequality, we derive the scaling for $M$:\n$$\nM \\geq \\frac{2C_{v}}{\\varepsilon^{2}}.\n$$\nThis indicates that $M$ must be chosen to be proportional to $\\varepsilon^{-2}$. The asymptotic scaling is $M = \\mathcal{O}(\\varepsilon^{-2})$.\n\nThe leading-order scalings are the dominant dependencies on $\\varepsilon$, which are $\\varepsilon$ for $h$ and $\\varepsilon^{-2}$ for $M$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mathcal{O}(\\varepsilon) & \\mathcal{O}(\\varepsilon^{-2})\n\\end{pmatrix}\n}\n$$", "id": "3083399"}]}