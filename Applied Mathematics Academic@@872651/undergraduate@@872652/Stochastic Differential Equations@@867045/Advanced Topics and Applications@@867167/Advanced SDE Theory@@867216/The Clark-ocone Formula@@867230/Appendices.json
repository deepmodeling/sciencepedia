{"hands_on_practices": [{"introduction": "Mastering a new mathematical tool begins with applying it to a clear, concrete example. This first practice exercise provides just that, asking you to compute the Clark-Ocone integrand for the random variable $F = W_T^3$. By working through this problem [@problem_id:428234], you will practice the two core steps of the method: applying the Malliavin chain rule to find the derivative, and then computing the conditional expectation to find the predictable integrand.", "problem": "Let $(W_t)_{t \\ge 0}$ be a standard one-dimensional Wiener process (or Brownian motion) on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, and let $(\\mathcal{F}_t)_{t \\ge 0}$ be the natural filtration generated by $W$. The Malliavin derivative, denoted by $D$, is a generalization of the ordinary directional derivative to the space of random variables. For a random variable $F$ that is a functional of the Wiener path, its Malliavin derivative $D_t F$ for $t \\ge 0$ is a stochastic process that measures the sensitivity of $F$ to an infinitesimal perturbation of the path of $W$ at time $t$. A key property of this derivative is that for any $s \\ge 0$, $D_t W_s = \\mathbf{1}_{[0, s]}(t)$, where $\\mathbf{1}$ is the indicator function. The Malliavin derivative satisfies a chain rule: for a differentiable function $g: \\mathbb{R} \\to \\mathbb{R}$ and a suitable random variable $F$, $D_t g(F) = g'(F) D_t F$.\n\nThe Clark-Ocone formula provides a fundamental representation for any square-integrable, $\\mathcal{F}_T$-measurable random variable $F$ (that lies in the domain of the Malliavin derivative) in terms of a stochastic integral:\n$$F = \\mathbb{E}[F] + \\int_0^T \\Phi_s dW_s$$\nwhere the integrand process $(\\Phi_s)_{s \\in [0,T]}$ is the projection of the Malliavin derivative of $F$ onto the adapted processes, given by\n$$\\Phi_s = \\mathbb{E}[D_s F | \\mathcal{F}_s]$$\n\nConsider the random variable $F = W_T^3$ for a fixed time $T  0$. Your task is to compute the integrand process $(\\Phi_t)_{t \\in [0, T]}$ in the Clark-Ocone representation of $W_T^3$. Your answer should be an explicit expression for $\\Phi_t$ in terms of $t$, $T$, and the process $(W_t)_{t \\ge 0}$.", "solution": "1. We have the Malliavin derivative of the Wiener process:  \n   $$D_tW_s = \\mathbf{1}_{[0,s]}(t)\\,. $$  \n2. For $F = W_T^3$, the chain rule gives  \n   $$D_tF = D_t\\bigl(W_T^3\\bigr) = 3W_T^2\\,D_tW_T = 3W_T^2\\mathbf{1}_{[0,T]}(t) = 3W_T^2\\,,\\quad t\\in[0,T].$$  \n3. By the Clark–Ocone formula, the integrand is  \n   $$\\Phi_t = \\mathbb{E}[D_tF\\mid\\mathcal{F}_t] = 3\\,\\mathbb{E}\\bigl[W_T^2\\mid\\mathcal{F}_t\\bigr]\\,. $$  \n4. Decompose $W_T = W_t + (W_T - W_t)$. Since $W_T - W_t$ is independent of $\\mathcal{F}_t$ with mean 0 and variance $T-t$,  \n   $$\\mathbb{E}\\bigl[W_T^2\\mid\\mathcal{F}_t\\bigr]\n     = \\mathbb{E}\\bigl[(W_t + (W_T - W_t))^2\\mid\\mathcal{F}_t\\bigr]\n     = W_t^2 + 2W_t\\mathbb{E}[W_T - W_t]+\\mathbb{E}[(W_T - W_t)^2]\n     = W_t^2 + (T-t)\\,. $$\n5. Hence  \n   $$\\Phi_t = 3\\bigl(W_t^2 + T - t\\bigr)\\,. $$", "answer": "$$\\boxed{3\\bigl(W_t^2 + T - t\\bigr)}$$", "id": "428234"}, {"introduction": "Many important applications, particularly in mathematical finance, involve functionals that are not smoothly differentiable, such as an option payoff. This exercise [@problem_id:3000571] demonstrates how to handle such cases by applying the Clark-Ocone machinery to the functional $F = \\max(W_T, 0)$. You will learn the powerful technique of smooth approximation, which allows us to rigorously extend the formula's reach to these more complex and practical scenarios.", "problem": "Let $\\{W_{t}\\}_{t \\in [0,T]}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\in [0,T]},\\mathbb{P})$ satisfying the usual conditions. Consider the terminal functional $F = \\max(W_{T},0)$.\n\n1. Construct a family $\\{f_{\\epsilon}\\}_{\\epsilon0}$ of smooth functions $f_{\\epsilon}:\\mathbb{R}\\to\\mathbb{R}$ such that $f_{\\epsilon}(x) \\to x^{+}$ pointwise as $\\epsilon \\to 0$, and $f_{\\epsilon}$ has a bounded and continuous first derivative for each fixed $\\epsilon0$. Justify that $f_{\\epsilon}(W_{T}) \\to F$ in $L^{2}(\\Omega)$ as $\\epsilon \\to 0$.\n\n2. Using foundational definitions from the Malliavin calculus on the Wiener space (specifically, the Malliavin derivative of functionals of $W_{T}$ and the chain rule), compute the Malliavin derivative $D_{t}\\big(f_{\\epsilon}(W_{T})\\big)$ for $t \\in [0,T]$.\n\n3. Use independence of Brownian increments and standard properties of conditional expectation to express $\\mathbb{E}\\big[D_{t}\\big(f_{\\epsilon}(W_{T})\\big)\\,|\\,\\mathcal{F}_{t}\\big]$ in terms of $W_{t}$ and the law of $W_{T}-W_{t}$. Then analyze the limit as $\\epsilon \\to 0$ to obtain the predictable integrand $g_{t}$ that appears in the martingale representation of $F$ with respect to $\\{W_{t}\\}$.\n\nYour final task is to report the limiting analytic expression for $g_{t}$ obtained in step 3 (valid for $t \\in [0,T)$). The final answer must be a single closed-form expression.", "solution": "The problem requires finding the predictable integrand $g_{t}$ in the Clark-Ocone martingale representation of the functional $F = \\max(W_{T},0) = W_{T}^{+}$. The Clark-Ocone formula states that for a suitable functional $F$, its Itô representation is given by $F = \\mathbb{E}[F] + \\int_{0}^{T} g_{t} dW_{t}$, where the integrand is $g_{t} = \\mathbb{E}[D_{t}F | \\mathcal{F}_{t}]$. The problem guides us through a three-step procedure involving a smooth approximation of $F$.\n\nThe function $x \\mapsto \\max(x,0)$ is not differentiable, so we cannot directly apply the chain rule of Malliavin calculus. We proceed as directed.\n\n1.  Construction of a smooth approximation and $L^{2}$ convergence.\nThe function to be approximated is $x^{+} = \\max(x,0)$. We can express this as $x^{+} = \\frac{1}{2}(x + |x|)$. The non-smoothness comes from the absolute value function $|x| = \\sqrt{x^{2}}$. A standard way to smoothen this is to introduce a small positive parameter $\\epsilon$.\nWe define the family of functions $\\{f_{\\epsilon}\\}_{\\epsilon0}$ by\n$$f_{\\epsilon}(x) = \\frac{1}{2}(x + \\sqrt{x^{2} + \\epsilon^{2}})$$\nFor any fixed $\\epsilon  0$, the term under the square root is strictly positive, so $f_{\\epsilon}$ is infinitely differentiable ($C^{\\infty}$) on $\\mathbb{R}$. As $\\epsilon \\to 0$, we have $\\sqrt{x^{2} + \\epsilon^{2}} \\to \\sqrt{x^{2}} = |x|$, so $f_{\\epsilon}(x) \\to \\frac{1}{2}(x + |x|) = x^{+}$ for every $x \\in \\mathbb{R}$. This shows pointwise convergence.\nThe first derivative of $f_{\\epsilon}$ is:\n$$f_{\\epsilon}'(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(x + \\sqrt{x^{2} + \\epsilon^{2}}) \\right] = \\frac{1}{2}\\left(1 + \\frac{2x}{2\\sqrt{x^{2} + \\epsilon^{2}}}\\right) = \\frac{1}{2}\\left(1 + \\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}\\right)$$\nFor $\\epsilon  0$, $f_{\\epsilon}'(x)$ is a continuous function of $x$. To check if it is bounded, we analyze the term $\\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}$. Its absolute value is $\\left|\\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}\\right| = \\frac{|x|}{\\sqrt{x^{2} + \\epsilon^{2}}} \\le \\frac{|x|}{\\sqrt{x^{2}}} = 1$ for $x \\ne 0$. The inequality is strict for all $x$. Thus, $-1  \\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}  1$, which implies $0  1 + \\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}  2$. Therefore, $0  f_{\\epsilon}'(x)  1$. The derivative is continuous and bounded for each $\\epsilon  0$.\n\nTo show that $f_{\\epsilon}(W_{T}) \\to F$ in $L^{2}(\\Omega)$, we must show $\\mathbb{E}\\left[|f_{\\epsilon}(W_{T}) - W_{T}^{+}|^{2}\\right] \\to 0$ as $\\epsilon \\to 0$. Let's examine the difference $|f_{\\epsilon}(x) - x^{+}|$:\n$$|f_{\\epsilon}(x) - x^{+}| = \\left|\\frac{1}{2}(x + \\sqrt{x^{2} + \\epsilon^{2}}) - \\frac{1}{2}(x + |x|)\\right| = \\frac{1}{2}|\\sqrt{x^{2} + \\epsilon^{2}} - |x||$$\nMultiplying by the conjugate, we get:\n$$\\frac{1}{2} \\left| \\frac{(\\sqrt{x^{2} + \\epsilon^{2}} - |x|)(\\sqrt{x^{2} + \\epsilon^{2}} + |x|)}{\\sqrt{x^{2} + \\epsilon^{2}} + |x|} \\right| = \\frac{1}{2} \\frac{|x^{2} + \\epsilon^{2} - x^{2}|}{\\sqrt{x^{2} + \\epsilon^{2}} + |x|} = \\frac{\\epsilon^{2}}{2(\\sqrt{x^{2} + \\epsilon^{2}} + |x|)}$$\nSince $\\sqrt{x^{2} + \\epsilon^{2}} \\ge \\sqrt{\\epsilon^{2}} = \\epsilon$ and $|x| \\ge 0$, the denominator is at least $\\epsilon$.\n$$|f_{\\epsilon}(x) - x^{+}| \\le \\frac{\\epsilon^{2}}{2\\epsilon} = \\frac{\\epsilon}{2}$$\nThis bound is uniform in $x$. Thus, applied to $W_{T}$, we have $|f_{\\epsilon}(W_{T}) - W_{T}^{+}| \\le \\frac{\\epsilon}{2}$ for all $\\omega \\in \\Omega$.\nThe $L^{2}$ norm of the difference is then bounded by:\n$$\\mathbb{E}\\left[|f_{\\epsilon}(W_{T}) - W_{T}^{+}|^{2}\\right] \\le \\mathbb{E}\\left[\\left(\\frac{\\epsilon}{2}\\right)^{2}\\right] = \\frac{\\epsilon^{2}}{4}$$\nAs $\\epsilon \\to 0$, $\\frac{\\epsilon^{2}}{4} \\to 0$, which proves that $f_{\\epsilon}(W_{T}) \\to W_{T}^{+}$ in $L^{2}(\\Omega)$.\n\n2.  Computation of the Malliavin derivative.\nThe Malliavin derivative of a functional of the form $f(W_{T})$, where $f$ is continuously differentiable with a bounded derivative, is given by the chain rule: $D_{t}(f(W_{T})) = f'(W_{T}) D_{t}W_{T}$. The Malliavin derivative of $W_{T} = \\int_{0}^{T} 1 \\, dW_{s}$ is $D_{t}W_{T} = 1$ for $t \\in [0,T]$.\nApplying this to our approximation $F_{\\epsilon} = f_{\\epsilon}(W_{T})$, we get:\n$$D_{t}(f_{\\epsilon}(W_{T})) = f_{\\epsilon}'(W_{T})$$\nfor $t \\in [0,T]$. Substituting the expression for $f_{\\epsilon}'(x)$ from Step 1:\n$$D_{t}(f_{\\epsilon}(W_{T})) = \\frac{1}{2}\\left(1 + \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}}\\right)$$\n\n3.  Conditional expectation and limiting expression.\nThe integrand in the Clark-Ocone representation for $F$ is $g_{t} = \\lim_{\\epsilon \\to 0} \\mathbb{E}[D_{t}(f_{\\epsilon}(W_{T})) | \\mathcal{F}_{t}]$. Let's first compute the conditional expectation for fixed $\\epsilon$:\n$$g_{t}^{\\epsilon} = \\mathbb{E}[D_{t}(f_{\\epsilon}(W_{T})) | \\mathcal{F}_{t}] = \\mathbb{E}\\left[\\frac{1}{2}\\left(1 + \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}}\\right) \\bigg| \\mathcal{F}_{t}\\right]$$\nBy linearity of conditional expectation:\n$$g_{t}^{\\epsilon} = \\frac{1}{2} + \\frac{1}{2}\\mathbb{E}\\left[\\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}} \\bigg| \\mathcal{F}_{t}\\right]$$\nThe random variable inside the expectation, $h_{\\epsilon}(W_{T}) = \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}}$, is bounded in absolute value by $1$. As $\\epsilon \\to 0$, $h_{\\epsilon}(x) \\to \\text{sgn}(x)$ for $x \\ne 0$. Since $\\mathbb{P}(W_{T}=0)=0$, this convergence happens almost surely. By the bounded convergence theorem for conditional expectations, we can interchange the limit and the expectation:\n$$g_{t} = \\lim_{\\epsilon \\to 0} g_{t}^{\\epsilon} = \\frac{1}{2} + \\frac{1}{2}\\mathbb{E}\\left[\\lim_{\\epsilon \\to 0} \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}} \\bigg| \\mathcal{F}_{t}\\right] = \\frac{1}{2}\\left(1 + \\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}]\\right)$$\nNow we compute $\\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}]$. Note that $\\text{sgn}(x) = \\mathbf{1}_{x0} - \\mathbf{1}_{x0}$.\n$$\\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}] = \\mathbb{E}[\\mathbf{1}_{W_{T}0} | \\mathcal{F}_{t}] - \\mathbb{E}[\\mathbf{1}_{W_{T}0} | \\mathcal{F}_{t}] = \\mathbb{P}(W_{T}0 | \\mathcal{F}_{t}) - \\mathbb{P}(W_{T}0 | \\mathcal{F}_{t})$$\nWe decompose $W_{T}$ as $W_{T} = W_{t} + (W_{T} - W_{t})$. Given $\\mathcal{F}_{t}$, $W_{t}$ is a known value, and the increment $W_{T} - W_{t}$ is independent of $\\mathcal{F}_{t}$ and follows a normal distribution with mean $0$ and variance $T-t$. So, $W_{T} - W_{t} \\sim N(0, T-t)$.\nWe have, for $t  T$:\n$$\\mathbb{P}(W_{T}  0 | \\mathcal{F}_{t}) = \\mathbb{P}(W_{t} + (W_{T} - W_{t})  0 | \\mathcal{F}_{t}) = \\mathbb{P}(W_{T} - W_{t}  -W_{t})$$\nLet $Z \\sim N(0,1)$ be a standard normal variable. Then $W_{T} - W_{t}$ has the same distribution as $\\sqrt{T-t} Z$.\n$$\\mathbb{P}(\\sqrt{T-t}Z  -W_{t}) = \\mathbb{P}\\left(Z  -\\frac{W_{t}}{\\sqrt{T-t}}\\right)$$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} \\exp(-\\frac{y^{2}}{2}) dy$.\nThe probability is $1 - \\Phi\\left(-\\frac{W_{t}}{\\sqrt{T-t}}\\right)$. Using the symmetry property $\\Phi(-z) = 1 - \\Phi(z)$, we get:\n$$\\mathbb{P}(W_{T}  0 | \\mathcal{F}_{t}) = \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)$$\nSimilarly, $\\mathbb{P}(W_{T}  0 | \\mathcal{F}_{t}) = \\mathbb{P}(Z  -\\frac{W_{t}}{\\sqrt{T-t}}) = \\Phi(-\\frac{W_{t}}{\\sqrt{T-t}}) = 1 - \\Phi(\\frac{W_{t}}{\\sqrt{T-t}})$.\nTherefore,\n$$\\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}] = \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right) - \\left(1 - \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)\\right) = 2\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right) - 1$$\nSubstituting this back into the expression for $g_{t}$:\n$$g_{t} = \\frac{1}{2}\\left(1 + \\left(2\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right) - 1\\right)\\right) = \\frac{1}{2}\\left(2\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)\\right) = \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)$$\nThe expression is valid for $t \\in [0,T)$.\n\nThe final result for the predictable integrand is the CDF of the standard normal distribution evaluated at $\\frac{W_{t}}{\\sqrt{T-t}}$.", "answer": "$$\\boxed{\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)}$$", "id": "3000571"}, {"introduction": "This final practice moves from computation to a more theoretical verification that reveals a deep truth about the Clark-Ocone formula. We will consider a random variable $F$ that is defined as a general Itô integral and show that its Clark-Ocone integrand is simply the original integrand process [@problem_id:3079996]. This exercise solidifies the understanding of the Clark-Ocone formula as an explicit representation that, in a sense, 'inverts' the stochastic integral.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P}\\right)$ be a complete filtered probability space carrying a standard one-dimensional Brownian motion $W=\\{W_t:t\\in[0,T]\\}$, where $T0$ is fixed. Let $h=\\{h(t):t\\in[0,T]\\}$ be a real-valued predictable process such that $\\mathbb{E}\\left[\\int_0^T h(s)^2\\,ds\\right]\\infty$. Define the Itô integral\n$$\nF \\equiv \\int_0^T h(s)\\,dW_s,\n$$\nwhich is square-integrable and centered.\n\nStarting from the definition of the Malliavin derivative $D_t$ on smooth cylindrical functionals and using approximation by simple predictable processes together with the closability of $D$ in $L^2(\\Omega)$, compute $\\mathbb{E}\\!\\left[D_t F\\,\\middle|\\,\\mathcal{F}_t\\right]$ for $t\\in[0,T]$. Then, identify the unique predictable process $\\varphi=\\{\\varphi(t):t\\in[0,T]\\}$ that serves as the Clark-Ocone integrand for $F$, namely the process given by the conditional expectation of the Malliavin derivative at each time, and express it explicitly in terms of $h$.\n\nYour final answer must be a single closed-form expression for the Clark-Ocone integrand as a function of $t$. No rounding is required, and no units are applicable.", "solution": "1. The random variable $F = \\int_0^T h(s)\\,dW_s$ is an Itô integral with a predictable, square-integrable integrand $h$. In Malliavin calculus, this is equivalent to the Skorokhod integral, $F = \\delta(h)$.\n2. We use the duality formula, which connects the Malliavin derivative $D$ and the Skorokhod integral $\\delta$. For any $G \\in \\mathbb{D}^{1,2}$ and a suitable process $u$ in the domain of $\\delta$: \n   $$ \\mathbb{E}[G\\,\\delta(u)] = \\mathbb{E}\\left[\\int_0^T (D_t G) u(t) \\,dt\\right] $$\n3. Let $u$ be any square-integrable predictable process. The Itô isometry gives:\n   $$ \\mathbb{E}[F\\,\\delta(u)] = \\mathbb{E}\\left[ \\left(\\int_0^T h(s)\\,dW_s\\right) \\left(\\int_0^T u(s)\\,dW_s\\right) \\right] = \\mathbb{E}\\left[\\int_0^T h(t)u(t)\\,dt\\right] $$\n4. Applying the duality formula with $G=F$:\n   $$ \\mathbb{E}\\left[\\int_0^T h(t)u(t)\\,dt\\right] = \\mathbb{E}\\left[\\int_0^T (D_t F) u(t) \\,dt\\right] $$\n5. Using the stochastic Fubini theorem and the tower property of conditional expectation (since $u(t)$ is $\\mathcal{F}_t$-measurable):\n   $$ \\int_0^T \\mathbb{E}[h(t)u(t)]\\,dt = \\int_0^T \\mathbb{E}[(D_t F) u(t)]\\,dt = \\int_0^T \\mathbb{E}\\left[\\mathbb{E}[D_t F | \\mathcal{F}_t] u(t)\\right]\\,dt $$\n6. This equality for all valid $u$ implies that the integrands are equal for almost every $t$:\n   $$ \\mathbb{E}[h(t)u(t)] = \\mathbb{E}\\left[\\mathbb{E}[D_t F | \\mathcal{F}_t] u(t)\\right] $$\n7. Let $\\varphi(t) = \\mathbb{E}[D_t F | \\mathcal{F}_t]$. Then $\\mathbb{E}[(h(t) - \\varphi(t))u(t)] = 0$. Since this holds for any square-integrable predictable process $u$, we must have $h(t) = \\varphi(t)$ almost surely for almost every $t$.\n8. Therefore, the Clark-Ocone integrand is the original integrand process $h(t)$.", "answer": "$$\n\\boxed{h(t)}\n$$", "id": "3079996"}]}