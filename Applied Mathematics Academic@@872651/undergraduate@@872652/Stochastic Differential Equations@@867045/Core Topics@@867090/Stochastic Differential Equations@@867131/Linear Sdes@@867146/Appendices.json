{"hands_on_practices": [{"introduction": "The Geometric Brownian Motion (GBM) is arguably the most fundamental linear SDE, forming the bedrock of modern financial models such as the Black-Scholes option pricing formula. This exercise is a rite of passage in stochastic calculus, demonstrating a powerful solution technique where a direct approach is difficult. By applying Itô's Lemma to the logarithm of the process, you will transform the multiplicative SDE into a simple additive one, thereby uncovering the underlying log-normal distribution of the solution process [@problem_id:3064037].", "problem": "Consider a probability space equipped with a filtration satisfying the usual conditions, and let $W_{t}$ denote a standard one-dimensional Brownian motion (also called Wiener process). Let $S_{t}$ be an It\\^o process satisfying the linear stochastic differential equation (SDE)\n$$\ndS_{t} = \\mu\\,S_{t}\\,dt + \\sigma\\,S_{t}\\,dW_{t}, \\quad S_{0} > 0,\n$$\nwhere $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are constants. Using only foundational tools from It\\^o calculus and properties of standard Brownian motion, derive the explicit strong solution for $S_{t}$ as a function of $t$, $S_{0}$, $\\mu$, $\\sigma$, and $W_{t}$. Then, establish the distribution of $\\ln S_{t}$ by computing its mean and variance from first principles. Your derivation must begin from the It\\^o differential of a smooth transformation of $S_{t}$ and the defining property that $W_{t}$ has stationary independent increments with $W_{t} \\sim \\mathcal{N}(0,t)$. Do not invoke any pre-memorized closed-form solution; derive it explicitly.\n\nProvide your final answer as a single analytic expression in the row-matrix format, with three entries ordered as follows:\n1. The explicit solution for $S_{t}$.\n2. The mean of $\\ln S_{t}$.\n3. The variance of $\\ln S_{t}$.\n\nNo numerical rounding is required for this problem.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem central to the theory of stochastic differential equations. All necessary definitions and conditions are provided, and there are no contradictions or ambiguities.\n\nWe are tasked with finding the explicit strong solution to the linear stochastic differential equation (SDE) for a process $S_t$, and then determining the mean and variance of $\\ln S_t$. The SDE is given by:\n$$\ndS_{t} = \\mu S_{t} dt + \\sigma S_{t} dW_{t}\n$$\nwith an initial condition $S_{0} > 0$, where $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are constants, and $W_t$ is a standard one-dimensional Brownian motion.\n\nThe structure of the SDE suggests that a logarithmic transformation may simplify the equation. Let us define a new process $Y_t = f(S_t)$, where $f(x) = \\ln(x)$. The function $f(x)$ is twice continuously differentiable for $x > 0$. Since $S_0 > 0$, and the solution to this SDE is known to remain almost surely positive, this transformation is well-defined.\n\nWe apply Itô's lemma to find the differential $dY_t$. For a general Itô process $X_t$ and a twice-differentiable function $g(t, x)$, Itô's lemma states:\n$$\ndg(t, X_t) = \\frac{\\partial g}{\\partial t} dt + \\frac{\\partial g}{\\partial x} dX_t + \\frac{1}{2} \\frac{\\partial^2 g}{\\partial x^2} (dX_t)^2\n$$\nIn our case, the function $f$ does not explicitly depend on time $t$, so $\\frac{\\partial f}{\\partial t} = 0$. We have $X_t = S_t$ and $f(S_t) = \\ln(S_t)$. The required partial derivatives of $f(x)=\\ln(x)$ with respect to its argument $x$ are:\n$$\n\\frac{df}{dx} = f'(x) = \\frac{1}{x} \\implies \\frac{\\partial f}{\\partial S_t} = \\frac{1}{S_t}\n$$\n$$\n\\frac{d^2f}{dx^2} = f''(x) = -\\frac{1}{x^2} \\implies \\frac{\\partial^2 f}{\\partial S_t^2} = -\\frac{1}{S_t^2}\n$$\nNext, we determine the quadratic variation term $(dS_t)^2$. Using the Itô multiplication rules ($dt \\cdot dt = 0$, $dt \\cdot dW_t = 0$, and $dW_t \\cdot dW_t = dt$):\n$$\n(dS_t)^2 = (\\mu S_t dt + \\sigma S_t dW_t)^2 = (\\mu S_t dt)^2 + 2(\\mu S_t dt)(\\sigma S_t dW_t) + (\\sigma S_t dW_t)^2\n$$\n$$\n(dS_t)^2 = \\mu^2 S_t^2 (dt)^2 + 2\\mu\\sigma S_t^2 (dt \\cdot dW_t) + \\sigma^2 S_t^2 (dW_t)^2\n$$\n$$\n(dS_t)^2 = 0 + 0 + \\sigma^2 S_t^2 dt = \\sigma^2 S_t^2 dt\n$$\nNow, we substitute these components into Itô's lemma for $Y_t = f(S_t)$:\n$$\ndY_t = d(\\ln S_t) = \\left(\\frac{\\partial f}{\\partial S_t}\\right) dS_t + \\frac{1}{2} \\left(\\frac{\\partial^2 f}{\\partial S_t^2}\\right) (dS_t)^2\n$$\n$$\ndY_t = \\left(\\frac{1}{S_t}\\right) (\\mu S_t dt + \\sigma S_t dW_t) + \\frac{1}{2} \\left(-\\frac{1}{S_t^2}\\right) (\\sigma^2 S_t^2 dt)\n$$\nSimplifying the expression:\n$$\ndY_t = (\\mu dt + \\sigma dW_t) - \\frac{1}{2}\\sigma^2 dt\n$$\n$$\ndY_t = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)dt + \\sigma dW_t\n$$\nThis is a standard arithmetic Brownian motion, which can be solved by direct integration from $0$ to $t$:\n$$\n\\int_0^t dY_u = \\int_0^t \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)du + \\int_0^t \\sigma dW_u\n$$\n$$\nY_t - Y_0 = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma (W_t - W_0)\n$$\nBy definition, $W_0=0$ and $Y_t = \\ln(S_t)$, so $Y_0 = \\ln(S_0)$. Substituting these back gives:\n$$\n\\ln(S_t) - \\ln(S_0) = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\n$$\n\\ln(S_t) = \\ln(S_0) + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\nTo obtain the explicit solution for $S_t$, we exponentiate both sides:\n$$\nS_t = \\exp\\left( \\ln(S_0) + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\n$$\n$$\nS_t = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\n$$\nThis is the first required result: the explicit strong solution for $S_t$.\n\nNext, we establish the distribution of $\\ln S_t$ by computing its mean and variance. We have the expression for $\\ln S_t$:\n$$\n\\ln S_t = \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t\n$$\nThe problem states that $W_t$ is a standard Brownian motion with the property $W_t \\sim \\mathcal{N}(0, t)$. This means its expected value is $\\mathbb{E}[W_t] = 0$ and its variance is $\\text{Var}(W_t) = \\mathbb{E}[(W_t - \\mathbb{E}[W_t])^2] = \\mathbb{E}[W_t^2] = t$.\nThe expression for $\\ln S_t$ is a linear function of the normally distributed random variable $W_t$. The terms $\\ln S_0$, $\\mu$, $\\sigma$, and $t$ are deterministic constants with respect to the probability measure at time $t$. Therefore, $\\ln S_t$ is also a normally distributed random variable.\n\nTo find the mean of $\\ln S_t$, we take the expectation of the expression:\n$$\n\\mathbb{E}[\\ln S_t] = \\mathbb{E}\\left[ \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\ln S_t] = \\mathbb{E}[\\ln S_0] + \\mathbb{E}\\left[\\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t\\right] + \\mathbb{E}[\\sigma W_t]\n$$\nSince $\\ln S_0$, $\\mu$, $\\sigma$, and $t$ are non-random, and $\\mathbb{E}[W_t]=0$:\n$$\n\\mathbb{E}[\\ln S_t] = \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma \\mathbb{E}[W_t]\n$$\n$$\n\\mathbb{E}[\\ln S_t] = \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t\n$$\nThis is the second required result.\n\nTo find the variance of $\\ln S_t$, we use the property that for a random variable $X$ and constants $a$ and $b$, $\\text{Var}(aX+b) = a^2\\text{Var}(X)$. In our expression for $\\ln S_t$, the term $\\ln S_0 + (\\mu - \\frac{1}{2}\\sigma^2)t$ is a deterministic constant (analogous to $b$). The random part is $\\sigma W_t$ (analogous to $aX$).\n$$\n\\text{Var}(\\ln S_t) = \\text{Var}\\left( \\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right)\n$$\n$$\n\\text{Var}(\\ln S_t) = \\text{Var}(\\sigma W_t)\n$$\n$$\n\\text{Var}(\\ln S_t) = \\sigma^2 \\text{Var}(W_t)\n$$\nUsing the property that $\\text{Var}(W_t) = t$:\n$$\n\\text{Var}(\\ln S_t) = \\sigma^2 t\n$$\nThis is the third and final required result.\n\nIn summary, $\\ln S_t$ follows a normal distribution:\n$$\n\\ln S_t \\sim \\mathcal{N}\\left(\\ln S_0 + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t, \\sigma^2 t\\right)\n$$\nThis implies that $S_t$ follows a log-normal distribution.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nS_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t + \\sigma W_t \\right) & \\ln(S_0) + \\left(\\mu - \\frac{1}{2}\\sigma^2\\right)t & \\sigma^2 t\n\\end{pmatrix}\n}\n$$", "id": "3064037"}, {"introduction": "Beyond solving for a specific path, a primary goal in stochastic analysis is understanding the statistical properties of a system, particularly its long-term behavior. This practice explores the multivariate Ornstein-Uhlenbeck process, a cornerstone model for systems that exhibit mean-reversion. You will derive the differential Lyapunov equation that governs the evolution of the covariance matrix and solve for its stationary state, a crucial concept for analyzing the stability and steady-state fluctuations of systems in fields ranging from control engineering to physics [@problem_id:3063929].", "problem": "Consider the two-dimensional stochastic differential equation (SDE) for a state vector $X_t \\in \\mathbb{R}^{2}$ driven by a standard two-dimensional Wiener process (Brownian motion) $W_t \\in \\mathbb{R}^{2}$:\n$$\ndX_t = A X_t\\,dt + G\\,dW_t,\n$$\nwhere\n$$\nA = \\begin{pmatrix} -3 & 2 \\\\ -1 & -4 \\end{pmatrix}, \\qquad G = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix}.\n$$\nAssume the initial condition $X_0$ is independent of $W_t$, has finite second moment, and the matrix $A$ is stable in the sense that all its eigenvalues have strictly negative real parts. Starting from the foundational definitions of Itô stochastic calculus, the Itô integral representation of linear SDEs, and the product rule for Itô processes, derive the dynamical equation satisfied by the second-moment matrix $P(t) = \\mathbb{E}[X_t X_t^{\\top}]$ and use the stability of $A$ to justify the existence of a unique stationary covariance matrix $P$.\n\nFor the given matrices $A$ and $G$, compute the stationary covariance matrix $P$ and then determine the stationary variance of the scalar projection $Y_t = u^{\\top} X_t$ with $u = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. Express your final answer as an exact value. No rounding is required. The final answer must be a single real number.", "solution": "The problem requires the derivation of the differential equation for the second-moment matrix of a linear stochastic differential equation (SDE), the justification of a stationary solution, and the computation of this solution and a related quantity for specific system matrices. The validation process confirms the problem is scientifically grounded, well-posed, and objective. All provided data are consistent and sufficient for a unique solution. The stability of the matrix $A$, a key premise, is verified by computing its eigenvalues. The eigenvalues $\\lambda$ of $A = \\begin{pmatrix} -3 & 2 \\\\ -1 & -4 \\end{pmatrix}$ are solutions to the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} -3-\\lambda & 2 \\\\ -1 & -4-\\lambda \\end{pmatrix} = (-3-\\lambda)(-4-\\lambda) - (2)(-1) = \\lambda^2 + 7\\lambda + 12 + 2 = \\lambda^2 + 7\\lambda + 14 = 0.\n$$\nThe roots are $\\lambda = \\frac{-7 \\pm \\sqrt{49 - 4(1)(14)}}{2} = \\frac{-7 \\pm \\sqrt{-7}}{2} = -\\frac{7}{2} \\pm i\\frac{\\sqrt{7}}{2}$. The real parts of both eigenvalues are $-\\frac{7}{2}$, which is strictly negative, confirming that the matrix $A$ is stable (Hurwitz). The problem is valid.\n\nThe solution proceeds in four steps. First, we derive the general differential equation for the second-moment matrix $P(t) = \\mathbb{E}[X_t X_t^{\\top}]$. Second, we establish the existence of a unique stationary solution. Third, we compute this stationary solution for the given matrices. Fourth, we compute the required stationary variance.\n\nThe SDE is given by $dX_t = A X_t\\,dt + G\\,dW_t$. To find the dynamics of $P(t)$, we consider the differential of the matrix process $X_t X_t^{\\top}$. Using the matrix version of Itô's product rule, for a process $Z_t = X_t Y_t^{\\top}$, its differential is $dZ_t = dX_t Y_t^{\\top} + X_t dY_t^{\\top} + d\\langle X, Y \\rangle_t$. Here, we apply it with $Y_t = X_t$.\n$$\nd(X_t X_t^{\\top}) = dX_t X_t^{\\top} + X_t (dX_t)^{\\top} + d\\langle X_t, X_t \\rangle_t.\n$$\nSubstituting the SDE for $dX_t$:\n$$\ndX_t X_t^{\\top} = (A X_t\\,dt + G\\,dW_t) X_t^{\\top} = A X_t X_t^{\\top}\\,dt + G\\,dW_t X_t^{\\top}.\n$$\n$$\nX_t (dX_t)^{\\top} = X_t (A X_t\\,dt + G\\,dW_t)^{\\top} = X_t (X_t^{\\top} A^{\\top}\\,dt + dW_t^{\\top} G^{\\top}) = X_t X_t^{\\top} A^{\\top}\\,dt + X_t dW_t^{\\top} G^{\\top}.\n$$\nThe quadratic covariation term $d\\langle X_t, X_t \\rangle_t$ is given by the product of the diffusion terms:\n$$\nd\\langle X_t, X_t \\rangle_t = (G\\,dW_t)(G\\,dW_t)^{\\top} = G (dW_t dW_t^{\\top}) G^{\\top}.\n$$\nSince $W_t$ is a standard $2$-dimensional Wiener process, its components are independent standard Brownian motions, meaning $\\mathbb{E}[dW_i(t) dW_j(t)] = \\delta_{ij}\\,dt$. This implies that the matrix $dW_t dW_t^{\\top}$ behaves as $I\\,dt$, where $I$ is the identity matrix.\n$$\nd\\langle X_t, X_t \\rangle_t = G I G^{\\top}\\,dt = GG^{\\top}\\,dt.\n$$\nCombining these terms, the stochastic differential for $X_t X_t^{\\top}$ is:\n$$\nd(X_t X_t^{\\top}) = (A X_t X_t^{\\top} + X_t X_t^{\\top} A^{\\top} + GG^{\\top})\\,dt + G\\,dW_t X_t^{\\top} + X_t dW_t^{\\top} G^{\\top}.\n$$\nTo obtain the dynamic equation for $P(t) = \\mathbb{E}[X_t X_t^{\\top}]$, we take the expectation of the integral form of the above equation. The expectation of the stochastic integral terms (the Itô integrals) is zero because the integrand is non-anticipating with respect to the increment $dW_t$.\n$$\n\\mathbb{E}\\left[\\int_0^t (G\\,dW_s X_s^{\\top} + X_s dW_s^{\\top} G^{\\top})\\right] = 0.\n$$\nTherefore, taking the expectation of the entire equation yields:\n$$\n\\mathbb{E}[d(X_t X_t^{\\top})] = \\mathbb{E}[(A X_t X_t^{\\top} + X_t X_t^{\\top} A^{\\top} + GG^{\\top})\\,dt].\n$$\nUsing the linearity of expectation and that $d\\mathbb{E}[\\cdot] = \\mathbb{E}[d\\cdot]$, we have:\n$$\nd\\mathbb{E}[X_t X_t^{\\top}] = (A \\mathbb{E}[X_t X_t^{\\top}] + \\mathbb{E}[X_t X_t^{\\top}] A^{\\top} + GG^{\\top})\\,dt.\n$$\nThis gives the differential Lyapunov equation for the second-moment matrix $P(t)$:\n$$\n\\frac{d}{dt}P(t) = A P(t) + P(t) A^{\\top} + GG^{\\top}.\n$$\nA stationary solution $P$ exists if $\\lim_{t\\to\\infty} P(t)$ exists. In the stationary state, the time derivative is zero, so $\\frac{d}{dt}P(t) = 0$. The stationary second-moment matrix $P$ must therefore satisfy the continuous-time algebraic Lyapunov equation:\n$$\nA P + P A^{\\top} + GG^{\\top} = 0.\n$$\nAccording to a fundamental theorem of linear system theory, if the matrix $A$ is stable (all its eigenvalues have strictly negative real parts), then for any symmetric positive semi-definite matrix $Q$, the Lyapunov equation $AX + XA^{\\top} = -Q$ has a unique, symmetric, positive definite solution $X$. In our case, $Q = GG^{\\top}$, which is symmetric and positive semi-definite (and in this case positive definite, as $G$ is invertible). Since we have established that $A$ is stable, a unique symmetric positive definite stationary covariance matrix $P$ exists.\n\nWe now compute this stationary matrix $P$.\nFirst, calculate $GG^{\\top}$:\n$$\nGG^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}.\n$$\nThe Lyapunov equation is $A P + P A^{\\top} = -GG^{\\top}$. Let $P = \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix}$ due to symmetry.\n$$\n\\begin{pmatrix} -3 & 2 \\\\ -1 & -4 \\end{pmatrix} \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} + \\begin{pmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{pmatrix} \\begin{pmatrix} -3 & -1 \\\\ 2 & -4 \\end{pmatrix} = \\begin{pmatrix} -1 & -2 \\\\ -2 & -5 \\end{pmatrix}.\n$$\nThe sum $AP + PA^{\\top}$ is:\n$$\n\\begin{pmatrix} -3p_{11}+2p_{12} & -3p_{12}+2p_{22} \\\\ -p_{11}-4p_{12} & -p_{12}-4p_{22} \\end{pmatrix} + \\begin{pmatrix} -3p_{11}+2p_{12} & -p_{11}-4p_{12} \\\\ -3p_{12}+2p_{22} & -p_{12}-4p_{22} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -6p_{11}+4p_{12} & -p_{11}-7p_{12}+2p_{22} \\\\ -p_{11}-7p_{12}+2p_{22} & -2p_{12}-8p_{22} \\end{pmatrix}.\n$$\nEquating this matrix with $-GG^{\\top}$ yields a system of three linear equations:\n1.  $-6p_{11} + 4p_{12} = -1$\n2.  $-p_{11} - 7p_{12} + 2p_{22} = -2$\n3.  $-2p_{12} - 8p_{22} = -5$\n\nFrom equation (1), $p_{11} = \\frac{4p_{12}+1}{6} = \\frac{2}{3}p_{12} + \\frac{1}{6}$.\nFrom equation (3), $p_{22} = \\frac{5-2p_{12}}{8} = \\frac{5}{8}-\\frac{1}{4}p_{12}$.\nSubstituting these into equation (2):\n$$\n-\\left(\\frac{2}{3}p_{12} + \\frac{1}{6}\\right) - 7p_{12} + 2\\left(\\frac{5}{8}-\\frac{1}{4}p_{12}\\right) = -2\n$$\n$$\n-\\frac{2}{3}p_{12} - \\frac{1}{6} - 7p_{12} + \\frac{5}{4} - \\frac{1}{2}p_{12} = -2\n$$\n$$\n\\left(-\\frac{2}{3} - 7 - \\frac{1}{2}\\right)p_{12} = -2 + \\frac{1}{6} - \\frac{5}{4}\n$$\n$$\n\\left(-\\frac{4}{6} - \\frac{42}{6} - \\frac{3}{6}\\right)p_{12} = -\\frac{24}{12} + \\frac{2}{12} - \\frac{15}{12}\n$$\n$$\n-\\frac{49}{6}p_{12} = -\\frac{37}{12} \\implies p_{12} = \\frac{37}{12} \\cdot \\frac{6}{49} = \\frac{37}{98}.\n$$\nNow we find $p_{11}$ and $p_{22}$:\n$$\np_{11} = \\frac{2}{3}\\left(\\frac{37}{98}\\right) + \\frac{1}{6} = \\frac{37}{147} + \\frac{1}{6} = \\frac{74}{294} + \\frac{49}{294} = \\frac{123}{294} = \\frac{41}{98}.\n$$\n$$\np_{22} = \\frac{5}{8} - \\frac{1}{4}\\left(\\frac{37}{98}\\right) = \\frac{5}{8} - \\frac{37}{392} = \\frac{5 \\cdot 49}{392} - \\frac{37}{392} = \\frac{245-37}{392} = \\frac{208}{392} = \\frac{26}{49} = \\frac{52}{98}.\n$$\nThus, the stationary covariance matrix is $P = \\frac{1}{98}\\begin{pmatrix} 41 & 37 \\\\ 37 & 52 \\end{pmatrix}$.\n\nFinally, we compute the stationary variance of $Y_t = u^{\\top} X_t$ with $u = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe mean of the stationary process $X_t$ is $\\mathbb{E}[X] = 0$ since $A$ is stable. Thus, $\\mathbb{E}[Y_t] = u^{\\top}\\mathbb{E}[X_t] = 0$. The variance of $Y_t$ is its second moment:\n$$\n\\text{Var}(Y_t) = \\mathbb{E}[Y_t^2] = \\mathbb{E}[(u^{\\top}X_t)^2] = \\mathbb{E}[u^{\\top}X_t X_t^{\\top}u] = u^{\\top}\\mathbb{E}[X_t X_t^{\\top}]u = u^{\\top}Pu.\n$$\nSubstituting the values of $u$ and $P$:\n$$\n\\text{Var}(Y) = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\left( \\frac{1}{98}\\begin{pmatrix} 41 & 37 \\\\ 37 & 52 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 41 & 37 \\\\ 37 & 52 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} \\begin{pmatrix} 41-37 & 37-52 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} \\begin{pmatrix} 4 & -15 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{98} (4(1) + (-15)(-1)) = \\frac{1}{98}(4+15) = \\frac{19}{98}.\n$$\nThe stationary variance of $Y_t$ is an exact value.", "answer": "$$\\boxed{\\frac{19}{98}}$$", "id": "3063929"}, {"introduction": "When extending concepts from scalar to matrix SDEs, intuition borrowed from ordinary differential equations can be deceptive. This exercise serves as a critical cautionary tale about the subtleties of non-commutative matrix algebra within the framework of Itô calculus. By testing a proposed exponential solution for a matrix SDE, you will prove by direct calculation why it fails, pinpointing the discrepancy to the non-commutativity of the system matrices [@problem_id:3064013]. This practice is vital for developing a robust and careful approach to multidimensional stochastic systems.", "problem": "Consider the matrix-valued linear Itô stochastic differential equation (SDE) driven by a single real-valued standard Brownian motion $W_{t}$,\n$$\n\\mathrm{d}X_{t} = A X_{t}\\,\\mathrm{d}t + B X_{t}\\,\\mathrm{d}W_{t}, \\quad X_{0} = I,\n$$\nwhere $A$ and $B$ are constant $2 \\times 2$ real matrices and $I$ is the identity matrix. A commonly proposed exponential-form candidate solution for such linear SDEs is\n$$\nZ_{t} = \\exp\\!\\big((A - \\tfrac{1}{2} B^{2})\\,t\\big)\\,\\exp\\!\\big(B W_{t}\\big).\n$$\nIn deterministic ordinary differential equations, exponential solutions often rely on commutativity of operators, but in stochastic settings with Itô calculus, noncommutativity can spoil such formulas. Your task is to provide a concrete counterexample by explicit calculation.\n\nLet\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\qquad B = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}.\n$$\nThese matrices do not commute, and $B^{2} = 0$. Using only the fundamental definitions and rules of Itô calculus (Itô’s formula for functions of scalar semimartingales, and the product rule for stochastic differentials), compute the diffusion-term mismatch between the stochastic differential $\\mathrm{d}Z_{t}$ and the target SDE $\\mathrm{d}X_{t}$, defined as the matrix\n$$\nD_{t} \\equiv \\exp\\!\\big((A - \\tfrac{1}{2} B^{2})\\,t\\big)\\,B\\,\\exp\\!\\big(B W_{t}\\big) \\;-\\; B\\,\\exp\\!\\big((A - \\tfrac{1}{2} B^{2})\\,t\\big)\\,\\exp\\!\\big(B W_{t}\\big).\n$$\nIn other words, $D_{t}$ is the difference between the actual diffusion coefficient in $\\mathrm{d}Z_{t}$ and the desired diffusion coefficient $B Z_{t}$ in the SDE. Simplify $D_{t}$ fully using the given $A$ and $B$ so that your final expression depends only on $t$ and $W_{t}$.\n\nThe final answer must be a single closed-form matrix expression. No rounding is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-contained. All necessary matrices and definitions are provided, and there are no contradictions. The task is a concrete mathematical calculation within the established framework of stochastic differential equations and matrix algebra.\n\nThe objective is to compute the diffusion-term mismatch matrix $D_{t}$, which is defined as:\n$$\nD_{t} = \\exp\\left(\\left(A - \\frac{1}{2} B^{2}\\right)t\\right) B \\exp(B W_{t}) - B \\exp\\left(\\left(A - \\frac{1}{2} B^{2}\\right)t\\right) \\exp(B W_{t})\n$$\nThe problem provides the constant $2 \\times 2$ matrices $A$ and $B$:\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\qquad B = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\n$$\nFirst, we verify the provided condition $B^{2} = 0$.\n$$\nB^{2} = B B = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} (0)(0) + (0)(1) & (0)(0) + (0)(0) \\\\ (1)(0) + (0)(1) & (1)(0) + (0)(0) \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nSince $B^{2}$ is the zero matrix, the term $\\frac{1}{2} B^{2}$ vanishes. The expression for $D_{t}$ simplifies to:\n$$\nD_{t} = \\exp(At) B \\exp(B W_{t}) - B \\exp(At) \\exp(B W_{t})\n$$\nOur task is to evaluate this expression. This requires computing the matrix exponentials $\\exp(At)$ and $\\exp(B W_{t})$.\n\nStep 1: Compute $\\exp(At)$.\nThe matrix exponential is defined by its Taylor series: $\\exp(M) = \\sum_{k=0}^{\\infty} \\frac{M^{k}}{k!}$.\nFor $M = At$, we compute the powers of $A$:\n$$\nA^{2} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nSince $A^{2}$ is the zero matrix, all higher powers $A^{k}$ for $k \\ge 2$ are also zero matrices. The series for $\\exp(At)$ truncates:\n$$\n\\exp(At) = I + At + \\frac{(At)^{2}}{2!} + \\dots = I + At\n$$\nSubstituting the matrices for $I$ (the $2 \\times 2$ identity matrix) and $A$:\n$$\n\\exp(At) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + t \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix}\n$$\n\nStep 2: Compute $\\exp(B W_{t})$.\nSimilarly, for $M = B W_{t}$, we use the fact that $B^{2}$ is the zero matrix. Thus, $(B W_t)^k = B^k W_t^k = 0$ for $k \\ge 2$. The series for $\\exp(B W_{t})$ also truncates:\n$$\n\\exp(B W_{t}) = I + B W_{t} + \\frac{(B W_{t})^{2}}{2!} + \\dots = I + B W_{t}\n$$\nSubstituting the matrices for $I$ and $B$:\n$$\n\\exp(B W_{t}) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + W_{t} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ W_{t} & 1 \\end{pmatrix}\n$$\n\nStep 3: Calculate the two terms of $D_{t}$.\nLet's calculate the first term, $T_{1} = \\exp(At) B \\exp(B W_{t})$.\nFirst, we find the product $\\exp(At) B$:\n$$\n\\exp(At) B = \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (t)(1) & (1)(0) + (t)(0) \\\\ (0)(0) + (1)(1) & (0)(0) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} t & 0 \\\\ 1 & 0 \\end{pmatrix}\n$$\nNow, we multiply by $\\exp(B W_{t})$:\n$$\nT_{1} = (\\exp(At) B) \\exp(B W_{t}) = \\begin{pmatrix} t & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ W_{t} & 1 \\end{pmatrix} = \\begin{pmatrix} (t)(1) + (0)(W_{t}) & (t)(0) + (0)(1) \\\\ (1)(1) + (0)(W_{t}) & (1)(0) + (0)(1) \\end{pmatrix} = \\begin{pmatrix} t & 0 \\\\ 1 & 0 \\end{pmatrix}\n$$\nNext, let's calculate the second term, $T_{2} = B \\exp(At) \\exp(B W_{t})$.\nFirst, we find the product $\\exp(At) \\exp(B W_{t})$:\n$$\n\\exp(At) \\exp(B W_{t}) = \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ W_{t} & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (t)(W_{t}) & (1)(0) + (t)(1) \\\\ (0)(1) + (1)(W_{t}) & (0)(0) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 1 + tW_{t} & t \\\\ W_{t} & 1 \\end{pmatrix}\n$$\nNow, we pre-multiply by $B$:\n$$\nT_{2} = B (\\exp(At) \\exp(B W_{t})) = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 + tW_{t} & t \\\\ W_{t} & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 + tW_{t} & t \\end{pmatrix}\n$$\n\nStep 4: Compute $D_{t} = T_{1} - T_{2}$.\n$$\nD_{t} = \\begin{pmatrix} t & 0 \\\\ 1 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 1 + tW_{t} & t \\end{pmatrix} = \\begin{pmatrix} t - 0 & 0 - 0 \\\\ 1 - (1 + tW_{t}) & 0 - t \\end{pmatrix} = \\begin{pmatrix} t & 0 \\\\ -tW_{t} & -t \\end{pmatrix}\n$$\nThis is the final expression for the diffusion-term mismatch matrix $D_{t}$. The non-zero result demonstrates that the candidate exponential form $Z_t$ is not a solution to the given SDE, which is a consequence of the non-commutativity of the matrices $\\exp(At)$ and $B$.\nSpecifically, $[A,B] = AB - BA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\neq 0$.", "answer": "$$\n\\boxed{\\begin{pmatrix} t & 0 \\\\ -t W_{t} & -t \\end{pmatrix}}\n$$", "id": "3064013"}]}