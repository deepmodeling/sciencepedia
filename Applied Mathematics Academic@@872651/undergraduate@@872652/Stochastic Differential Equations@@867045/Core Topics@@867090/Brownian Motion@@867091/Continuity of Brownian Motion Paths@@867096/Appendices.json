{"hands_on_practices": [{"introduction": "Understanding the continuity of Brownian motion begins with its statistical properties. Before we can assert that individual paths are continuous, we can first check a weaker but fundamental condition known as mean-square continuity. This practice guides you through a foundational calculation, deriving the expected squared increment $E[|B_t - B_s|^2]$ directly from the covariance structure of a standard Brownian motion to rigorously establish its mean-square continuity. [@problem_id:3045665]", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion (SBM), that is, a mean-zero Gaussian process with covariance structure $\\mathbb{E}[B_{t} B_{s}] = \\min(t,s)$ for all $s,t \\geq 0$ and $B_{0} = 0$ almost surely. Using only these foundational properties and basic identities for variance and covariance of random variables, do the following:\n\n- Derive a closed-form expression for $\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big]$ in terms of $s$ and $t$.\n- Based on your expression, determine whether $\\{B_{t}\\}_{t \\geq 0}$ is mean-square (MS) continuous at an arbitrary fixed time $t_{0} \\geq 0$, and justify your conclusion from first principles.\n\nReport only the closed-form expression for $\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big]$ as your final answer. No rounding is required and no units are involved.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The solution proceeds in two parts as requested.\n\nFirst, we derive a closed-form expression for $\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big]$ using the provided properties of a standard Brownian motion (SBM) $\\{B_{t}\\}_{t \\geq 0}$. The SBM is defined as a mean-zero Gaussian process, so $\\mathbb{E}[B_{t}] = 0$ for all $t \\geq 0$. The covariance structure is given by $\\mathbb{E}[B_{t} B_{s}] = \\min(t,s)$.\n\nThe expression we seek is the second moment of the random variable $B_{t} - B_{s}$. We can expand the square inside the expectation:\n$$\n\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big] = \\mathbb{E}\\big[(B_{t} - B_{s})^{2}\\big]\n$$\nUsing the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}\\big[(B_{t} - B_{s})^{2}\\big] = \\mathbb{E}\\big[B_{t}^{2} - 2B_{t}B_{s} + B_{s}^{2}\\big] = \\mathbb{E}[B_{t}^{2}] - 2\\mathbb{E}[B_{t}B_{s}] + \\mathbb{E}[B_{s}^{2}]\n$$\nWe now evaluate each term using the given properties.\n\nThe term $\\mathbb{E}[B_{t}B_{s}]$ is the covariance, which is given as $\\min(t,s)$.\n$$\n\\mathbb{E}[B_{t}B_{s}] = \\min(t,s)\n$$\nThe terms $\\mathbb{E}[B_{t}^{2}]$ and $\\mathbb{E}[B_{s}^{2}]$ are the second moments of $B_{t}$ and $B_{s}$, respectively. Since the process is mean-zero, the second moment is equal to the variance. The variance of $B_{t}$ can be found from the covariance function:\n$$\n\\mathbb{E}[B_{t}^{2}] = \\text{Var}(B_{t}) = \\text{Cov}(B_t, B_t) = \\mathbb{E}[B_t B_t] = \\min(t,t) = t\n$$\nSimilarly, for $B_{s}$:\n$$\n\\mathbb{E}[B_{s}^{2}] = \\text{Var}(B_{s}) = \\text{Cov}(B_s, B_s) = \\mathbb{E}[B_s B_s] = \\min(s,s) = s\n$$\nSubstituting these results back into the expanded expression:\n$$\n\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big] = t - 2\\min(t,s) + s\n$$\nTo simplify this expression, we consider two cases for the non-negative times $s$ and $t$:\n\nCase 1: $t \\geq s$. In this case, $\\min(t,s) = s$. The expression becomes:\n$$\nt - 2s + s = t - s\n$$\nSince $t \\geq s$, we can write this as $|t-s|$.\n\nCase 2: $s > t$. In this case, $\\min(t,s) = t$. The expression becomes:\n$$\nt - 2t + s = s - t\n$$\nSince $s > t$, we can write this as $|s-t|$, which is equivalent to $|t-s|$.\n\nIn both cases, the result is the same. Therefore, the closed-form expression is:\n$$\n\\mathbb{E}\\big[|B_{t} - B_{s}|^{2}\\big] = |t - s|\n$$\n\nSecond, we use this result to determine if the process $\\{B_{t}\\}_{t \\geq 0}$ is mean-square (MS) continuous at an arbitrary fixed time $t_{0} \\geq 0$. A process $\\{X_{t}\\}$ is defined as MS continuous at $t_{0}$ if $\\lim_{t \\to t_{0}} \\mathbb{E}\\big[|X_{t} - X_{t_{0}}|^{2}\\big] = 0$.\n\nFor the Brownian motion process $\\{B_{t}\\}$, we need to evaluate the limit:\n$$\n\\lim_{t \\to t_{0}} \\mathbb{E}\\big[|B_{t} - B_{t_{0}}|^{2}\\big]\n$$\nUsing the expression derived in the first part, we substitute $|t - t_{0}|$ for the expectation:\n$$\n\\lim_{t \\to t_{0}} |t - t_{0}|\n$$\nThe function $f(t) = |t - t_{0}|$ is continuous for all $t \\in \\mathbb{R}$. Therefore, the limit can be evaluated by direct substitution of $t = t_{0}$:\n$$\n|t_{0} - t_{0}| = |0| = 0\n$$\nSince $\\lim_{t \\to t_{0}} \\mathbb{E}\\big[|B_{t} - B_{t_{0}}|^{2}\\big] = 0$ for any arbitrary $t_{0} \\geq 0$, we conclude that the standard Brownian motion process $\\{B_{t}\\}_{t \\geq 0}$ is mean-square continuous everywhere on its domain $[0, \\infty)$.", "answer": "$$\n\\boxed{|t-s|}\n$$", "id": "3045665"}, {"introduction": "While mean-square continuity gives us a statistical assurance, it does not guarantee that every sample path is continuous. A more profound understanding comes from seeing Brownian motion as the limit of scaled random walks, a cornerstone result known as Donsker's Invariance Principle. This exercise explores a key aspect of this convergence: it demonstrates that as the random walk approximation becomes finer, its largest jump size vanishes, providing a powerful argument for why the limiting process must have continuous sample paths. [@problem_id:3045659]", "problem": "Let $\\{X_{k}\\}_{k \\geq 1}$ be independent and identically distributed random variables with $\\mathbb{E}[X_{1}] = 0$, variance $\\mathrm{Var}(X_{1}) = \\sigma^{2} \\in (0,\\infty)$, and finite fourth ($4$) moment $\\mathbb{E}[|X_{1}|^{4}]  \\infty$. Define the rescaled càdlàg step process on $[0,1]$ by\n$$\nW_{n}(t) := \\frac{1}{\\sigma \\sqrt{n}} \\sum_{k=1}^{\\lfloor n t \\rfloor} X_{k}, \\quad t \\in [0,1].\n$$\nFor $t \\in (0,1]$, denote by $W_{n}(t^{-})$ the left limit at time $t$, and define the maximal jump size over $[0,1]$ by\n$$\nM_{n} := \\sup_{t \\in (0,1]} \\big| W_{n}(t) - W_{n}(t^{-}) \\big|.\n$$\nUsing only basic probability tools, specifically Boole’s inequality (union bound) and Markov’s inequality, derive a non-asymptotic bound on $\\mathbb{P}(M_{n}  \\epsilon)$ in terms of $\\epsilon$, $n$, $\\sigma$, and $\\mathbb{E}[|X_{1}|^{4}]$, for fixed $\\epsilon  0$. Then, compute the limit\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(M_{n}  \\epsilon).\n$$\nIn your reasoning, interpret how this bound motivates that any weak limit of $\\{W_{n}\\}_{n \\geq 1}$ has almost surely continuous sample paths, identifying the canonical limit process $B_{t}$ on $[0,1]$.\n\nYour final answer must be a single real number or a single closed-form analytic expression. No rounding is required.", "solution": "The problem requires us to derive a bound on the probability of the maximal jump size of a rescaled random walk, compute the limit of this probability, and interpret the result in the context of weak convergence to a continuous-time process.\n\nFirst, we validate the problem statement.\nThe givens are:\n1.  A sequence of independent and identically distributed (i.i.d.) random variables $\\{X_{k}\\}_{k \\geq 1}$.\n2.  $\\mathbb{E}[X_{1}] = 0$.\n3.  $\\mathrm{Var}(X_{1}) = \\sigma^{2}$ with $0  \\sigma^{2}  \\infty$.\n4.  $\\mathbb{E}[|X_{1}|^{4}]  \\infty$.\n5.  A sequence of càdlàg processes $W_{n}(t) := \\frac{1}{\\sigma \\sqrt{n}} \\sum_{k=1}^{\\lfloor n t \\rfloor} X_{k}$ for $t \\in [0,1]$.\n6.  The maximal jump size $M_{n} := \\sup_{t \\in (0,1]} \\big| W_{n}(t) - W_{n}(t^{-}) \\big|$.\n7.  The problem must be solved using only Boole's inequality and Markov's inequality.\n\nThe problem is scientifically grounded in the theory of stochastic processes, specifically concerning the construction of Brownian motion as a limit of random walks (Donsker's invariance principle). The definitions are precise and standard. The conditions on the random variables ($0$ mean, finite positive variance, finite fourth moment) are common assumptions for such theorems. The problem is well-posed, objective, and contains no scientific flaws. Therefore, the problem is valid, and we proceed with the solution.\n\nThe solution is divided into three parts: finding an expression for $M_{n}$, deriving the probability bound and computing the limit, and interpreting the result.\n\nPart 1: Analysis of the maximal jump size $M_{n}$.\nThe process $W_{n}(t)$ is a step function which is constant on intervals of the form $\\left[\\frac{k-1}{n}, \\frac{k}{n}\\right)$ for $k=1, \\dots, n$. Jumps can only occur at times $t = \\frac{k}{n}$ for $k=1, 2, \\dots, n$. For any $t$ not of this form, $W_n$ is continuous at $t$, so $W_{n}(t) - W_{n}(t^{-}) = 0$.\nLet's analyze the jump at $t = \\frac{k}{n}$ for some $k \\in \\{1, \\dots, n\\}$.\nThe value of the process at $t = \\frac{k}{n}$ is:\n$$\nW_{n}\\left(\\frac{k}{n}\\right) = \\frac{1}{\\sigma \\sqrt{n}} \\sum_{j=1}^{\\lfloor n \\cdot (k/n) \\rfloor} X_{j} = \\frac{1}{\\sigma \\sqrt{n}} \\sum_{j=1}^{k} X_{j}.\n$$\nThe left limit at $t = \\frac{k}{n}$ is the value of the process on the interval leading up to the jump:\n$$\nW_{n}\\left(\\left(\\frac{k}{n}\\right)^{-}\\right) = \\lim_{s \\to (k/n)^{-}} W_{n}(s) = \\frac{1}{\\sigma \\sqrt{n}} \\sum_{j=1}^{k-1} X_{j}.\n$$\nThe jump size at $t=\\frac{k}{n}$ is the absolute difference:\n$$\n\\left| W_{n}\\left(\\frac{k}{n}\\right) - W_{n}\\left(\\left(\\frac{k}{n}\\right)^{-}\\right) \\right| = \\left| \\frac{1}{\\sigma \\sqrt{n}} \\sum_{j=1}^{k} X_{j} - \\frac{1}{\\sigma \\sqrt{n}} \\sum_{j=1}^{k-1} X_{j} \\right| = \\left| \\frac{X_{k}}{\\sigma \\sqrt{n}} \\right|.\n$$\nThe maximal jump size $M_{n}$ is the supremum of these jump sizes over all possible jump points $t \\in (0,1]$.\n$$\nM_{n} = \\sup_{t \\in (0,1]} |W_{n}(t) - W_{n}(t^{-})| = \\max_{k=1, \\dots, n} \\left| \\frac{X_{k}}{\\sigma \\sqrt{n}} \\right| = \\frac{1}{\\sigma \\sqrt{n}} \\max_{k=1, \\dots, n} |X_{k}|.\n$$\n\nPart 2: Derivation of the probability bound and computation of the limit.\nWe want to bound $\\mathbb{P}(M_{n}  \\epsilon)$ for a fixed $\\epsilon  0$. Substituting the expression for $M_{n}$:\n$$\n\\mathbb{P}(M_{n}  \\epsilon) = \\mathbb{P}\\left(\\frac{1}{\\sigma \\sqrt{n}} \\max_{k=1, \\dots, n} |X_{k}|  \\epsilon\\right) = \\mathbb{P}\\left(\\max_{k=1, \\dots, n} |X_{k}|  \\epsilon \\sigma \\sqrt{n}\\right).\n$$\nThe event $\\{\\max_{k=1, \\dots, n} |X_{k}|  \\epsilon \\sigma \\sqrt{n}\\}$ is the union of the events $\\{|X_{k}|  \\epsilon \\sigma \\sqrt{n}\\}$ for $k=1, \\dots, n$.\n$$\n\\mathbb{P}(M_{n}  \\epsilon) = \\mathbb{P}\\left( \\bigcup_{k=1}^{n} \\{|X_{k}|  \\epsilon \\sigma \\sqrt{n}\\} \\right).\n$$\nUsing Boole's inequality (the union bound), which states that $\\mathbb{P}(\\cup A_k) \\leq \\sum \\mathbb{P}(A_k)$:\n$$\n\\mathbb{P}(M_{n}  \\epsilon) \\leq \\sum_{k=1}^{n} \\mathbb{P}\\left(|X_{k}|  \\epsilon \\sigma \\sqrt{n}\\right).\n$$\nSince the random variables $\\{X_{k}\\}_{k \\geq 1}$ are identically distributed, $\\mathbb{P}(|X_{k}|  c) = \\mathbb{P}(|X_{1}|  c)$ for any constant $c$ and any $k$. Therefore:\n$$\n\\mathbb{P}(M_{n}  \\epsilon) \\leq n \\cdot \\mathbb{P}\\left(|X_{1}|  \\epsilon \\sigma \\sqrt{n}\\right).\n$$\nNow, we use Markov's inequality to bound the term $\\mathbb{P}(|X_{1}|  \\epsilon \\sigma \\sqrt{n})$. Markov's inequality states that for a random variable $Y$ and a non-negative, non-decreasing function $\\phi$, $\\mathbb{P}(|Y| \\geq a) \\leq \\frac{\\mathbb{E}[\\phi(|Y|)]}{\\phi(a)}$ for $a0$ where $\\phi(a)0$. We are given that $\\mathbb{E}[|X_{1}|^4]  \\infty$. Let's choose $Y=X_1$ and $\\phi(x) = x^{4}$. This function is non-negative and non-decreasing for $x \\geq 0$.\nLet $a = \\epsilon \\sigma \\sqrt{n}$. For $\\epsilon0, \\sigma0, n \\ge 1$, we have $a0$.\n$$\n\\mathbb{P}\\left(|X_{1}|  \\epsilon \\sigma \\sqrt{n}\\right) \\leq \\frac{\\mathbb{E}[|X_{1}|^{4}]}{(\\epsilon \\sigma \\sqrt{n})^{4}} = \\frac{\\mathbb{E}[|X_{1}|^{4}]}{\\epsilon^{4} \\sigma^{4} n^{2}}.\n$$\nSubstituting this back into our inequality for $\\mathbb{P}(M_n  \\epsilon)$:\n$$\n\\mathbb{P}(M_{n}  \\epsilon) \\leq n \\cdot \\left(\\frac{\\mathbb{E}[|X_{1}|^{4}]}{\\epsilon^{4} \\sigma^{4} n^{2}}\\right) = \\frac{\\mathbb{E}[|X_{1}|^{4}]}{n \\epsilon^{4} \\sigma^{4}}.\n$$\nThis is the required non-asymptotic bound. Let $C = \\frac{\\mathbb{E}[|X_{1}|^{4}]}{\\epsilon^{4} \\sigma^{4}}$. Since $\\mathbb{E}[|X_{1}|^{4}]  \\infty$, $\\sigma  0$, and $\\epsilon  0$, $C$ is a finite positive constant. The bound is:\n$$\n\\mathbb{P}(M_{n}  \\epsilon) \\leq \\frac{C}{n}.\n$$\nNow we compute the limit as $n \\to \\infty$. Since probability is non-negative, we have:\n$$\n0 \\leq \\mathbb{P}(M_{n}  \\epsilon) \\leq \\frac{C}{n}.\n$$\nAs $n \\to \\infty$, the right-hand side $\\frac{C}{n}$ approaches $0$. By the Squeeze Theorem:\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(M_{n}  \\epsilon) = 0.\n$$\n\nPart 3: Interpretation.\nThe sequence of processes $\\{W_{n}\\}_{n \\geq 1}$ are random elements in the space $D([0,1])$ of càdlàg functions on $[0,1]$. Donsker's Invariance Principle states that, under the given conditions, $W_{n}$ converges weakly (in distribution) to a standard one-dimensional Brownian motion $B = \\{B_t\\}_{t\\in[0,1]}$.\nA defining property of Brownian motion is that its sample paths are almost surely continuous. A function is continuous if and only if it has no jumps. The result $\\lim_{n \\to \\infty} \\mathbb{P}(M_{n}  \\epsilon) = 0$ for any $\\epsilon  0$ means that the maximal jump size of $W_{n}$, $M_n$, converges to $0$ in probability.\nThis is a crucial condition for the weak limit of $\\{W_n\\}$ to have continuous paths. If the limiting process $B$ had a jump of size greater than $\\epsilon$ with some positive probability, then by the properties of weak convergence (specifically, the Portmanteau Theorem), we would expect $\\liminf_{n \\to \\infty} \\mathbb{P}(M_n  \\epsilon/2)  0$. Our result contradicts this, implying that the limit process cannot have jumps.\nMore formally, this result is a key ingredient in proving that the sequence of distributions of $\\{W_n\\}$ is C-tight, which means the sequence is tight and any weak limit point is supported on the space of continuous functions $C([0,1])$. The property that the maximal jump converges to zero in probability ensures that in the limit $n \\to \\infty$, the discrete approximations $W_n$ become uniformly \"smoother\" by eliminating large discontinuities. This intuitively motivates why the limiting object, Brownian motion $B_t$, must be a process with continuous trajectories. The calculation thus provides a rigorous justification for a property that is necessary for the convergence stated by Donsker's theorem to hold.", "answer": "$$\\boxed{0}$$", "id": "3045659"}, {"introduction": "Given that Brownian paths are continuous, how can we quantify their characteristic 'roughness'? This practice shifts our focus from proving continuity to analyzing its consequences for a single, realized path. By relating the error of a piecewise linear interpolation to the path's modulus of continuity, you will gain a tangible, geometric understanding of what uniform continuity implies and how it controls our ability to approximate these complex functions on a given interval. [@problem_id:3045678]", "problem": "Let $T0$ and let $\\{B_t\\}_{t\\in[0,T]}$ be a standard Brownian motion with almost surely continuous sample paths. For an integer $n\\ge 1$, consider the dyadic partition of $[0,T]$ given by $t_k^{(n)} := k\\,T/2^n$ for $k=0,1,\\dots,2^n$. Define the piecewise linear interpolation $L^{(n)}:[0,T]\\to\\mathbb{R}$ of the path $t\\mapsto B_t$ on this partition by\n$$\nL^{(n)}(t) := B_{t_k^{(n)}} + \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}}\\big(B_{t_{k+1}^{(n)}}-B_{t_k^{(n)}}\\big),\\quad t\\in[t_k^{(n)},t_{k+1}^{(n)}],\\;k=0,\\dots,2^n-1.\n$$\nDefine the uniform interpolation error\n$$\nE_n := \\sup_{t\\in[0,T]} \\big|B_t - L^{(n)}(t)\\big|.\n$$\nLet the modulus of continuity of the path $t\\mapsto B_t$ on $[0,T]$ be the function $\\omega_B:[0,T]\\to[0,\\infty)$ given by\n$$\n\\omega_B(\\delta) := \\sup\\big\\{|B_s-B_t|:\\, s,t\\in[0,T],\\,|s-t|\\le \\delta\\big\\}.\n$$\nUsing only the definitions above and basic properties of convex combinations, derive an explicit pathwise bound on $E_n$ in terms of $\\omega_B$ evaluated at the mesh size $T/2^n$. Provide your final bound as a single closed-form analytic expression in terms of $\\omega_B$, $T$, and $n$ (do not include inequality symbols in your final answer).", "solution": "The problem is to derive a pathwise bound on the uniform error $E_n$ of the piecewise linear interpolation of a Brownian motion path in terms of its modulus of continuity.\n\nFirst, let us establish the framework based on the provided definitions.\nLet $\\{B_t\\}_{t\\in[0,T]}$ be a standard Brownian motion on the interval $[0,T]$, with $T0$. We are given that its sample paths are almost surely continuous. We analyze a single such continuous path, which we denote by $t \\mapsto B_t$.\nThe interval $[0,T]$ is partitioned by the dyadic points $t_k^{(n)} = k\\,T/2^n$ for $k=0,1,\\dots,2^n$, where $n \\ge 1$ is an integer. The mesh size of this partition is uniform and given by $\\delta_n := t_{k+1}^{(n)} - t_k^{(n)} = T/2^n$.\n\nThe piecewise linear interpolation of the path, $L^{(n)}(t)$, is defined on each subinterval $[t_k^{(n)},t_{k+1}^{(n)}]$ as\n$$\nL^{(n)}(t) = B_{t_k^{(n)}} + \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}}\\left(B_{t_{k+1}^{(n)}}-B_{t_k^{(n)}}\\right).\n$$\nThis expression represents a convex combination of the values of the Brownian path at the endpoints of the subinterval. To see this, let $t \\in [t_k^{(n)},t_{k+1}^{(n)}]$ and define a parameter $\\lambda \\in [0,1]$ as\n$$\n\\lambda := \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}}.\n$$\nThen, $1-\\lambda = 1 - \\frac{t-t_k^{(n)}}{t_{k+1}^{(n)}-t_k^{(n)}} = \\frac{(t_{k+1}^{(n)}-t_k^{(n)}) - (t-t_k^{(n)})}{t_{k+1}^{(n)}-t_k^{(n)}} = \\frac{t_{k+1}^{(n)}-t}{t_{k+1}^{(n)}-t_k^{(n)}}$.\nSubstituting this into the definition of $L^{(n)}(t)$:\n$$\nL^{(n)}(t) = (1-\\lambda)B_{t_k^{(n)}} + \\lambda B_{t_{k+1}^{(n)}}.\n$$\nThis formulation explicitly uses the property of convex combinations as required by the problem statement.\n\nThe quantity to be bounded is the uniform interpolation error, defined as\n$$\nE_n = \\sup_{t\\in[0,T]} |B_t - L^{(n)}(t)|.\n$$\nSince the supremum is over the entire interval $[0,T]$, we can express it as the maximum of the suprema over the subintervals of the partition:\n$$\nE_n = \\sup_{k \\in \\{0,1,\\dots,2^n-1\\}} \\sup_{t \\in [t_k^{(n)}, t_{k+1}^{(n)}]} |B_t - L^{(n)}(t)|.\n$$\nLet us analyze the error $|B_t - L^{(n)}(t)|$ for an arbitrary $t \\in [t_k^{(n)}, t_{k+1}^{(n)}]$. Using the convex combination form of $L^{(n)}(t)$, we can rewrite the error term by introducing $B_t = 1 \\cdot B_t = ((1-\\lambda)+\\lambda)B_t$:\n$$\nB_t - L^{(n)}(t) = B_t - \\left( (1-\\lambda)B_{t_k^{(n)}} + \\lambda B_{t_{k+1}^{(n)}} \\right)\n$$\n$$\nB_t - L^{(n)}(t) = (1-\\lambda)B_t + \\lambda B_t - (1-\\lambda)B_{t_k^{(n)}} - \\lambda B_{t_{k+1}^{(n)}}\n$$\n$$\nB_t - L^{(n)}(t) = (1-\\lambda)(B_t - B_{t_k^{(n)}}) + \\lambda(B_t - B_{t_{k+1}^{(n)}}).\n$$\nThis expression for the error is a convex combination of the increments of the Brownian path from the point $t$ to the endpoints of the subinterval $[t_k^{(n)}, t_{k+1}^{(n)}]$.\n\nNow, we take the absolute value and apply the triangle inequality:\n$$\n|B_t - L^{(n)}(t)| = |(1-\\lambda)(B_t - B_{t_k^{(n)}}) + \\lambda(B_t - B_{t_{k+1}^{(n)}})| \\le |(1-\\lambda)(B_t - B_{t_k^{(n)}})| + |\\lambda(B_t - B_{t_{k+1}^{(n)}})|.\n$$\nSince $t \\in [t_k^{(n)},t_{k+1}^{(n)}]$, we have $0 \\le \\lambda \\le 1$, which implies $1-\\lambda \\ge 0$. Thus, we can write:\n$$\n|B_t - L^{(n)}(t)| \\le (1-\\lambda)|B_t - B_{t_k^{(n)}}| + \\lambda|B_t - B_{t_{k+1}^{(n)}}|.\n$$\nThe problem provides the definition for the path's modulus of continuity:\n$$\n\\omega_B(\\delta) := \\sup\\{|B_s - B_t| : s, t \\in [0,T], |s-t| \\le \\delta\\}.\n$$\nWe use this to bound the two difference terms.\nFor the first term, $|B_t - B_{t_k^{(n)}}|$, the time difference is $|t - t_k^{(n)}| = t - t_k^{(n)}$. Since $t \\le t_{k+1}^{(n)}$, we have $t - t_k^{(n)} \\le t_{k+1}^{(n)} - t_k^{(n)} = \\delta_n = T/2^n$. By the definition of the modulus of continuity, it follows that\n$$\n|B_t - B_{t_k^{(n)}}| \\le \\omega_B(t - t_k^{(n)}).\n$$\nThe modulus of continuity $\\omega_B(\\delta)$ is a non-decreasing function of $\\delta$, because as $\\delta$ increases, the supremum is taken over a larger set. Therefore, since $t-t_k^{(n)} \\le \\delta_n$, we have $\\omega_B(t-t_k^{(n)}) \\le \\omega_B(\\delta_n)$. This yields the bound:\n$$\n|B_t - B_{t_k^{(n)}}| \\le \\omega_B(T/2^n).\n$$\nFor the second term, $|B_t - B_{t_{k+1}^{(n)}}| = |B_{t_{k+1}^{(n)}} - B_t|$, the time difference is $|t_{k+1}^{(n)} - t| = t_{k+1}^{(n)} - t$. Since $t \\ge t_k^{(n)}$, we have $t_{k+1}^{(n)} - t \\le t_{k+1}^{(n)} - t_k^{(n)} = \\delta_n = T/2^n$. Applying the same reasoning:\n$$\n|B_t - B_{t_{k+1}^{(n)}}| \\le \\omega_B(t_{k+1}^{(n)} - t) \\le \\omega_B(T/2^n).\n$$\nSubstituting these two bounds back into the inequality for the error:\n$$\n|B_t - L^{(n)}(t)| \\le (1-\\lambda)\\omega_B(T/2^n) + \\lambda\\omega_B(T/2^n)\n$$\n$$\n|B_t - L^{(n)}(t)| \\le ((1-\\lambda) + \\lambda)\\omega_B(T/2^n) = \\omega_B(T/2^n).\n$$\nThis inequality holds for any $t \\in [t_k^{(n)},t_{k+1}^{(n)}]$. The derived bound, $\\omega_B(T/2^n)$, is independent of the specific point $t$ within the subinterval and is also independent of the subinterval index $k$.\nTherefore, this same bound must hold for the supremum of the error over the entire interval $[0,T]$.\n$$\nE_n = \\sup_{t\\in[0,T]} |B_t - L^{(n)}(t)| \\le \\omega_B(T/2^n).\n$$\nThe problem asks for an explicit pathwise bound on $E_n$. The expression $\\omega_B(T/2^n)$ is such a bound, derived as requested.", "answer": "$$\\boxed{\\omega_B\\left(\\frac{T}{2^n}\\right)}$$", "id": "3045678"}]}