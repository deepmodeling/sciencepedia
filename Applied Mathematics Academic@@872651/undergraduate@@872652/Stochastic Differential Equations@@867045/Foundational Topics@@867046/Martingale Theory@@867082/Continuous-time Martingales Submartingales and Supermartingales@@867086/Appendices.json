{"hands_on_practices": [{"introduction": "This first practice establishes a crucial link between Itô calculus and the martingale property. We will analyze the exponential martingale, a fundamental process in stochastic calculus and financial mathematics, verifying its martingale nature through both first principles and its stochastic differential equation (SDE). This exercise then explores how applying a convex function like $f(x)=x^2$ to a martingale does not preserve the property, but instead creates a submartingale, a direct consequence of Jensen's inequality in a dynamic setting [@problem_id:3045867].", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion adapted to its natural filtration on a probability space that satisfies the usual conditions. Define the process $\\{M_{t}\\}_{t \\geq 0}$ by $M_{t} := \\exp\\!\\left(B_{t} - \\frac{1}{2} t\\right)$. Using the fundamental definition of a continuous-time martingale and Itô's formula from stochastic differential equations (SDE), justify that $\\{M_{t}\\}_{t \\geq 0}$ is a martingale. Then, for the non-affine function $f(x) := x^{2}$, compute the exact value of $\\mathbb{E}[f(M_{t})]$ as a function of $t$, using only well-established properties of Brownian motion (such as the normal distribution and the moment generating function (MGF) of a Gaussian random variable). Your final answer must be a single closed-form analytic expression for $\\mathbb{E}[M_{t}^{2}]$ in terms of $t$. No rounding is required, and no physical units apply.", "solution": "The problem requires a two-part analysis of the process $\\{M_{t}\\}_{t \\geq 0}$ defined by $M_{t} := \\exp(B_{t} - \\frac{1}{2} t)$, where $\\{B_{t}\\}_{t \\geq 0}$ is a standard Brownian motion. First, we must justify that $\\{M_{t}\\}$ is a martingale. Second, we must compute the expectation $\\mathbb{E}[M_{t}^{2}]$.\n\nThe problem is evaluated to be valid as it is scientifically grounded in the theory of stochastic differential equations, well-posed, and objective. It is a standard problem in the field.\n\nPart 1: Justification of the Martingale Property\n\nAs requested, we will use both Itô's formula and the fundamental definition of a martingale. Itô's formula will show that $\\{M_{t}\\}$ has a stochastic differential equation (SDE) with a zero drift term, which establishes it as a local martingale. Then, we will verify the conditions of the fundamental definition to confirm it is a true martingale.\n\nLet $M_t = f(t, B_t)$ where $f(t, x) = \\exp(x - \\frac{1}{2}t)$. The partial derivatives of $f(t, x)$ are:\n$$\n\\frac{\\partial f}{\\partial t} = -\\frac{1}{2} \\exp\\left(x - \\frac{1}{2}t\\right) = -\\frac{1}{2} f(t, x)\n$$\n$$\n\\frac{\\partial f}{\\partial x} = \\exp\\left(x - \\frac{1}{2}t\\right) = f(t, x)\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\exp\\left(x - \\frac{1}{2}t\\right) = f(t, x)\n$$\nApplying Itô's formula to $M_t = f(t, B_t)$:\n$$\ndM_{t} = \\frac{\\partial f}{\\partial t}(t, B_t) dt + \\frac{\\partial f}{\\partial x}(t, B_t) dB_{t} + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2}(t, B_t) (dB_{t})^{2}\n$$\nSubstituting the partial derivatives and using the quadratic variation of Brownian motion, $(dB_{t})^{2} = dt$:\n$$\ndM_{t} = \\left(-\\frac{1}{2} M_{t}\\right) dt + (M_{t}) dB_{t} + \\frac{1}{2} (M_{t}) dt\n$$\n$$\ndM_{t} = \\left(-\\frac{1}{2} M_{t} + \\frac{1}{2} M_{t}\\right) dt + M_{t} dB_{t}\n$$\n$$\ndM_{t} = M_{t} dB_{t}\n$$\nThis is the SDE for the process $\\{M_{t}\\}$. In its integral form, since $M_0 = \\exp(B_0 - 0) = \\exp(0) = 1$, we have:\n$$\nM_{t} = 1 + \\int_{0}^{t} M_{s} dB_{s}\n$$\nA stochastic integral with respect to a Brownian motion is always a local martingale. Thus, $\\{M_{t}\\}$ is a local martingale.\n\nTo show it is a true martingale, we must verify the three conditions from its fundamental definition with respect to the natural filtration of the Brownian motion, $\\{\\mathcal{F}_{t}\\}_{t \\geq 0}$.\n\n1.  **Adaptedness:** The process $M_{t} = \\exp(B_{t} - \\frac{1}{2}t)$ is a measurable function of $B_{t}$ and $t$. Since $\\{B_{t}\\}$ is adapted to the filtration $\\{\\mathcal{F}_{t}\\}$, $M_{t}$ is also $\\mathcal{F}_{t}$-adapted for all $t \\geq 0$.\n\n2.  **Integrability:** We must show that $\\mathbb{E}[|M_{t}|]  \\infty$ for all $t \\geq 0$. Since $M_{t} = \\exp(B_{t} - \\frac{1}{2}t)$ is always positive, $|M_{t}| = M_{t}$. We compute its expectation:\n    $$\n    \\mathbb{E}[M_{t}] = \\mathbb{E}\\left[\\exp\\left(B_{t} - \\frac{1}{2}t\\right)\\right] = \\exp\\left(-\\frac{1}{2}t\\right) \\mathbb{E}[\\exp(B_{t})]\n    $$\n    The random variable $B_{t}$ follows a normal distribution, $B_{t} \\sim \\mathcal{N}(0, t)$. The moment generating function (MGF) of a general normal random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_{X}(k) = \\mathbb{E}[\\exp(kX)] = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$. Here, $X=B_t$, $\\mu=0$, $\\sigma^2=t$, and we evaluate the MGF at $k=1$.\n    $$\n    \\mathbb{E}[\\exp(B_{t})] = \\exp\\left(1 \\cdot 0 + \\frac{1}{2} \\cdot 1^2 \\cdot t\\right) = \\exp\\left(\\frac{1}{2}t\\right)\n    $$\n    Substituting this back into the expectation for $M_t$:\n    $$\n    \\mathbb{E}[M_{t}] = \\exp\\left(-\\frac{1}{2}t\\right) \\exp\\left(\\frac{1}{2}t\\right) = \\exp(0) = 1\n    $$\n    Since $\\mathbb{E}[|M_{t}|] = 1$, the integrability condition is satisfied.\n\n3.  **Conditional Expectation Property:** For any $s  t$, we must show that $\\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = M_{s}$.\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\mathbb{E}\\left[\\exp\\left(B_{t} - \\frac{1}{2}t\\right) \\Big| \\mathcal{F}_{s}\\right]\n    $$\n    We decompose the Brownian motion increment: $B_{t} = B_{s} + (B_{t} - B_{s})$.\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\mathbb{E}\\left[\\exp\\left(B_{s} + (B_{t} - B_{s}) - \\frac{1}{2}t\\right) \\Big| \\mathcal{F}_{s}\\right] = \\exp\\left(-\\frac{1}{2}t\\right) \\mathbb{E}\\left[\\exp(B_{s})\\exp(B_{t} - B_{s}) \\Big| \\mathcal{F}_{s}\\right]\n    $$\n    Since $B_{s}$ is $\\mathcal{F}_{s}$-measurable, $\\exp(B_{s})$ can be treated as a constant with respect to the conditional expectation. By the independence of increments of Brownian motion, the increment $(B_{t} - B_{s})$ is independent of the past filtration $\\mathcal{F}_{s}$.\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\exp\\left(-\\frac{1}{2}t\\right) \\exp(B_{s}) \\mathbb{E}[\\exp(B_{t} - B_{s})]\n    $$\n    The increment $B_{t} - B_{s}$ is normally distributed with mean $0$ and variance $t-s$, i.e., $B_{t} - B_{s} \\sim \\mathcal{N}(0, t-s)$. Using the MGF formula with $k=1$, $\\mu=0$, and $\\sigma^2=t-s$:\n    $$\n    \\mathbb{E}[\\exp(B_{t} - B_{s})] = \\exp\\left(1 \\cdot 0 + \\frac{1}{2} \\cdot 1^2 \\cdot (t-s)\\right) = \\exp\\left(\\frac{t-s}{2}\\right)\n    $$\n    Substituting this result back:\n    $$\n    \\mathbb{E}[M_{t} | \\mathcal{F}_{s}] = \\exp\\left(-\\frac{1}{2}t\\right) \\exp(B_{s}) \\exp\\left(\\frac{t-s}{2}\\right) = \\exp\\left(B_{s} - \\frac{t}{2} + \\frac{t-s}{2}\\right) = \\exp\\left(B_{s} - \\frac{s}{2}\\right) = M_{s}\n    $$\n    Since all three conditions are satisfied, $\\{M_{t}\\}_{t \\geq 0}$ is a true martingale. This process is a classic example of an exponential martingale, also known as a Doléans-Dade exponential.\n\nPart 2: Computation of $\\mathbb{E}[M_{t}^{2}]$\n\nWe are asked to compute $\\mathbb{E}[f(M_{t})]$ for $f(x)=x^2$, which is $\\mathbb{E}[M_t^2]$. We first express $M_t^2$ in terms of $B_t$ and $t$.\n$$\nM_{t}^{2} = \\left(\\exp\\left(B_{t} - \\frac{1}{2}t\\right)\\right)^{2} = \\exp\\left(2\\left(B_{t} - \\frac{1}{2}t\\right)\\right) = \\exp(2B_{t} - t)\n$$\nNow, we compute the expectation:\n$$\n\\mathbb{E}[M_{t}^{2}] = \\mathbb{E}[\\exp(2B_{t} - t)]\n$$\nSince $\\exp(-t)$ is a deterministic quantity, it can be factored out of the expectation:\n$$\n\\mathbb{E}[M_{t}^{2}] = \\exp(-t) \\mathbb{E}[\\exp(2B_{t})]\n$$\nTo evaluate $\\mathbb{E}[\\exp(2B_{t})]$, we again use the MGF of the normal distribution $B_{t} \\sim \\mathcal{N}(0, t)$. This time, we evaluate the MGF at $k=2$.\n$$\n\\mathbb{E}[\\exp(2B_{t})] = \\exp\\left(2 \\cdot 0 + \\frac{1}{2} \\cdot 2^2 \\cdot t\\right) = \\exp\\left(\\frac{1}{2} \\cdot 4 \\cdot t\\right) = \\exp(2t)\n$$\nSubstituting this back into the expression for $\\mathbb{E}[M_{t}^{2}]$:\n$$\n\\mathbb{E}[M_{t}^{2}] = \\exp(-t) \\cdot \\exp(2t) = \\exp(-t+2t) = \\exp(t)\n$$\nThus, the exact value of $\\mathbb{E}[M_t^2]$ is $\\exp(t)$.\nIt is noteworthy that since $\\mathbb{E}[M_t^2] = \\exp(t)$, the variance of $M_t$ is $\\text{Var}(M_t) = \\mathbb{E}[M_t^2] - (\\mathbb{E}[M_t])^2 = \\exp(t) - 1^2 = \\exp(t) - 1$.\nThe process $\\{M_t^2\\}_{t\\ge0}$ is an example of a submartingale, as $\\mathbb{E}[M_t^2]$ is an increasing function of $t$.", "answer": "$$\\boxed{\\exp(t)}$$", "id": "3045867"}, {"introduction": "The martingale convergence theorems are powerful results that describe the long-term behavior of stochastic processes, but their conditions must be respected. This practice constructs a simple submartingale, $X_t = \\max\\{B_t, 0\\}$, from a standard Brownian motion to demonstrate a scenario where convergence fails. By analyzing the long-term behavior of this process, you will identify exactly which condition of the submartingale convergence theorem is violated, providing a concrete understanding of why uniform integrability is essential for ensuring a process settles down [@problem_id:3045869].", "problem": "Consider a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\ge 0}, \\mathbb{P})$ supporting a standard Brownian motion $B = (B_t)_{t \\ge 0}$ with $B_0 = 0$ and its natural filtration. Define the process $X = (X_t)_{t \\ge 0}$ by $X_t = B_t^{+} = \\max\\{B_t, 0\\}$. \n\nUsing only core definitions and first principles, carry out the following steps:\n\n1. Prove that $(X_t)_{t \\ge 0}$ is a submartingale with respect to $(\\mathcal{F}_t)_{t \\ge 0}$.\n2. Explain why $(X_t)_{t \\ge 0}$ does not converge almost surely (almost surely (a.s.) means with probability one) as $t \\to \\infty$, and identify the specific integrability condition from the submartingale convergence theorem that fails for $(X_t)_{t \\ge 0}$. Justify your conclusions from foundational properties of Brownian motion and integrability.\n3. For any fixed $T  0$ and $b  0$, define the upcrossing count $U_T(0,b)$ of the interval $[0,b]$ by the process $(X_t)_{t \\in [0,T]}$ as the number of completed passages from below $0$ to above $b$ by the continuous path $t \\mapsto X_t$ up to time $T$. Derive, from first principles and standard inequalities for submartingales, a closed-form analytic expression for the upper bound on $\\mathbb{E}[U_T(0,b)]$ stated purely in terms of $T$ and $b$.\n\nExpress your final answer as a single simplified analytic expression. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded within the theory of stochastic processes. We will address the three parts in sequence.\n\nThe process $X = (X_t)_{t \\ge 0}$ is defined on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\ge 0}, \\mathbb{P})$ by $X_t = B_t^{+} = \\max\\{B_t, 0\\}$, where $B = (B_t)_{t \\ge 0}$ is a standard one-dimensional Brownian motion with $B_0 = 0$ and $(\\mathcal{F}_t)_{t \\ge 0}$ is its natural filtration.\n\n### 1. Proof that $(X_t)_{t \\ge 0}$ is a Submartingale\n\nA process $(X_t)_{t \\ge 0}$ is a submartingale with respect to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$ if it satisfies three conditions:\n(i) $X_t$ is $\\mathcal{F}_t$-measurable for all $t \\ge 0$.\n(ii) $\\mathbb{E}[|X_t|]  \\infty$ for all $t \\ge 0$.\n(iii) For any $s  t$, $\\mathbb{E}[X_t | \\mathcal{F}_s] \\ge X_s$ a.s. (almost surely).\n\nWe verify each condition for $X_t = \\max\\{B_t, 0\\}$.\n\n(i) **Measurability:** By definition, the process $B$ is adapted to its natural filtration $(\\mathcal{F}_t)_{t \\ge 0}$, meaning $B_t$ is an $\\mathcal{F}_t$-measurable random variable for every $t \\ge 0$. The function $f(x) = \\max\\{x, 0\\}$ is a continuous function from $\\mathbb{R}$ to $\\mathbb{R}$, and is therefore a Borel-measurable function. Since $X_t = f(B_t)$, and $B_t$ is $\\mathcal{F}_t$-measurable, $X_t$ is also $\\mathcal{F}_t$-measurable.\n\n(ii) **Integrability:** We must show that $\\mathbb{E}[|X_t|]  \\infty$ for all $t \\ge 0$. Since $X_t = \\max\\{B_t, 0\\}$ is always non-negative, $|X_t| = X_t$. We need to compute $\\mathbb{E}[X_t]$.\nThe random variable $B_t$ follows a normal distribution with mean $0$ and variance $t$, i.e., $B_t \\sim N(0, t)$. Its probability density function is $p(x,t) = \\frac{1}{\\sqrt{2\\pi t}} \\exp(-\\frac{x^2}{2t})$.\nThe expectation is given by:\n$$ \\mathbb{E}[X_t] = \\mathbb{E}[\\max\\{B_t, 0\\}] = \\int_{-\\infty}^{\\infty} \\max\\{x, 0\\} p(x,t) dx = \\int_{0}^{\\infty} x \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^2}{2t}\\right) dx $$\nWe perform a substitution. Let $u = \\frac{x^2}{2t}$, so $du = \\frac{2x}{2t}dx = \\frac{x}{t}dx$, which implies $x dx = t du$. The limits of integration remain from $0$ to $\\infty$.\n$$ \\mathbb{E}[X_t] = \\frac{1}{\\sqrt{2\\pi t}} \\int_0^{\\infty} \\exp(-u) (t du) = \\frac{t}{\\sqrt{2\\pi t}} \\int_0^{\\infty} \\exp(-u) du $$\nThe integral $\\int_0^{\\infty} \\exp(-u) du = [-\\exp(-u)]_0^{\\infty} = 0 - (-1) = 1$.\nThus,\n$$ \\mathbb{E}[X_t] = \\frac{t}{\\sqrt{2\\pi t}} = \\frac{\\sqrt{t}}{\\sqrt{2\\pi}} = \\sqrt{\\frac{t}{2\\pi}} $$\nFor any finite $t \\ge 0$, this value is finite. Thus, the integrability condition is satisfied.\n\n(iii) **Submartingale Property:** We need to show $\\mathbb{E}[X_t | \\mathcal{F}_s] \\ge X_s$ for $s  t$.\nWe use Jensen's inequality for conditional expectations, which states that for any convex function $f$ and integrable random variable $Y$, $\\mathbb{E}[f(Y) | \\mathcal{G}] \\ge f(\\mathbb{E}[Y | \\mathcal{G}])$ for a sub-$\\sigma$-algebra $\\mathcal{G}$.\nThe function $f(x) = \\max\\{x, 0\\}$ is convex. Applying Jensen's inequality with $Y = B_t$ and $\\mathcal{G} = \\mathcal{F}_s$:\n$$ \\mathbb{E}[X_t | \\mathcal{F}_s] = \\mathbb{E}[\\max\\{B_t, 0\\} | \\mathcal{F}_s] \\ge \\max\\{\\mathbb{E}[B_t | \\mathcal{F}_s], 0\\} $$\nSince standard Brownian motion $(B_t)_{t \\ge 0}$ is a martingale with respect to its natural filtration, we have $\\mathbb{E}[B_t | \\mathcal{F}_s] = B_s$ for $s  t$.\nSubstituting this into the inequality gives:\n$$ \\mathbb{E}[X_t | \\mathcal{F}_s] \\ge \\max\\{B_s, 0\\} $$\nBy definition, $\\max\\{B_s, 0\\} = X_s$. Therefore, we have shown that $\\mathbb{E}[X_t | \\mathcal{F}_s] \\ge X_s$ a.s.\n\nSince all three conditions are met, $(X_t)_{t \\ge 0}$ is a submartingale with respect to $(\\mathcal{F}_t)_{t \\ge 0}$.\n\n### 2. Non-Convergence and the Submartingale Convergence Theorem\n\nThe submartingale convergence theorem states that if $(Y_t)_{t \\ge 0}$ is a submartingale such that $\\sup_{t \\ge 0} \\mathbb{E}[Y_t]  \\infty$, then $Y_t$ converges a.s. to an integrable random variable $Y_\\infty$ as $t \\to \\infty$.\n\nFirst, we explain why $(X_t)_{t \\ge 0}$ does not converge almost surely as $t \\to \\infty$. The behavior of $X_t = \\max\\{B_t, 0\\}$ is dictated by the long-term behavior of the Brownian motion $B_t$. The Law of the Iterated Logarithm for Brownian motion states that:\n$$ \\limsup_{t \\to \\infty} \\frac{B_t}{\\sqrt{2t \\ln(\\ln t)}} = 1 \\quad \\text{a.s.} \\quad \\text{and} \\quad \\liminf_{t \\to \\infty} \\frac{B_t}{\\sqrt{2t \\ln(\\ln t)}} = -1 \\quad \\text{a.s.} $$\nFrom the $\\limsup$ result, it follows that $B_t$ will take arbitrarily large positive values as $t \\to \\infty$. This implies that $\\limsup_{t \\to \\infty} B_t = \\infty$ a.s. Consequently, for our process $X_t$:\n$$ \\limsup_{t \\to \\infty} X_t = \\limsup_{t \\to \\infty} \\max\\{B_t, 0\\} = \\max\\{\\limsup_{t \\to \\infty} B_t, 0\\} = \\infty \\quad \\text{a.s.} $$\nFrom the $\\liminf$ result, it follows that $B_t$ will take arbitrarily large negative values. More fundamentally, a one-dimensional Brownian motion is recurrent, meaning it returns to any neighborhood of $0$ infinitely often with probability $1$. In particular, $B_t$ will be less than or equal to $0$ for arbitrarily large values of $t$. For any such $t$, $X_t = \\max\\{B_t, 0\\} = 0$. This implies that:\n$$ \\liminf_{t \\to \\infty} X_t = 0 \\quad \\text{a.s.} $$\nSince $\\liminf_{t \\to \\infty} X_t \\neq \\limsup_{t \\to \\infty} X_t$, the limit $\\lim_{t \\to \\infty} X_t$ does not exist almost surely. The process does not converge.\n\nNext, we identify the failing condition of the submartingale convergence theorem. The theorem requires the submartingale to be bounded in $L^1$, i.e., $\\sup_{t \\ge 0} \\mathbb{E}[|X_t|]  \\infty$. As $|X_t| = X_t$, this condition is $\\sup_{t \\ge 0} \\mathbb{E}[X_t]  \\infty$.\nFrom our calculation in Part 1, we found that $\\mathbb{E}[X_t] = \\sqrt{\\frac{t}{2\\pi}}$.\nWe evaluate the supremum over $t \\ge 0$:\n$$ \\sup_{t \\ge 0} \\mathbb{E}[X_t] = \\sup_{t \\ge 0} \\sqrt{\\frac{t}{2\\pi}} = \\lim_{t \\to \\infty} \\sqrt{\\frac{t}{2\\pi}} = \\infty $$\nThe condition $\\sup_{t \\ge 0} \\mathbb{E}[X_t]  \\infty$ is not satisfied. This is the specific integrability condition from the submartingale convergence theorem that fails for the process $(X_t)_{t \\ge 0}$.\n\n### 3. Upper Bound on Expected Upcrossings\n\nWe need to derive an upper bound for $\\mathbb{E}[U_T(0,b)]$, the expected number of upcrossings of the interval $[0,b]$ by the process $(X_t)_{t \\in [0,T]}$, where $T0$ and $b0$. The problem specifies that an upcrossing is a passage from \"below $0$\" to \"above $b$\". For the process $X_t = \\max\\{B_t, 0\\}$, which is always non-negative, \"below $0$\" must be interpreted as being at the level $0$.\n\nThe derivation relies on Doob's Upcrossing Inequality, a foundational result for submartingales. For a continuous-time submartingale $(Y_t)_{t \\in [0,T]}$ and an interval $[a,b]$ with $a  b$, the expected number of upcrossings $U_T(a,b)$ is bounded by:\n$$ \\mathbb{E}[U_T(a,b)] \\le \\frac{\\mathbb{E}[(Y_T - a)^+] - \\mathbb{E}[(Y_0 - a)^+]}{b - a} $$\nwhere $y^+ = \\max\\{y,0\\}$.\n\nWe apply this standard inequality to our process $Y_t = X_t = \\max\\{B_t,0\\}$, which we have proven to be a submartingale. The interval is $[a,b] = [0,b]$.\nThe inequality becomes:\n$$ \\mathbb{E}[U_T(0,b)] \\le \\frac{\\mathbb{E}[(X_T - 0)^+] - \\mathbb{E}[(X_0 - 0)^+]}{b - 0} $$\nLet's evaluate the terms in the numerator:\n1.  The term $\\mathbb{E}[(X_T - 0)^+]$ simplifies to $\\mathbb{E}[X_T^+]$. Since $X_T \\ge 0$, we have $X_T^+ = X_T$. So this term is just $\\mathbb{E}[X_T]$. From our calculation in Part 1, with $t=T$, we have:\n    $$ \\mathbb{E}[X_T] = \\sqrt{\\frac{T}{2\\pi}} $$\n2.  The term $\\mathbb{E}[(X_0 - 0)^+]$ simplifies to $\\mathbb{E}[X_0]$. We are given $B_0 = 0$, so $X_0 = \\max\\{B_0, 0\\} = \\max\\{0, 0\\} = 0$. Therefore:\n    $$ \\mathbb{E}[X_0] = 0 $$\n\nSubstituting these results into the inequality:\n$$ \\mathbb{E}[U_T(0,b)] \\le \\frac{\\sqrt{\\frac{T}{2\\pi}} - 0}{b} = \\frac{\\sqrt{\\frac{T}{2\\pi}}}{b} $$\nSimplifying the expression gives the closed-form analytic upper bound:\n$$ \\mathbb{E}[U_T(0,b)] \\le \\frac{\\sqrt{T}}{b\\sqrt{2\\pi}} $$\nThis expression is the required upper bound on the expected number of upcrossings, stated purely in terms of $T$ and $b$. The final answer is this upper bound.", "answer": "$$ \\boxed{\\frac{\\sqrt{T}}{b\\sqrt{2\\pi}}} $$", "id": "3045869"}, {"introduction": "Perhaps one of the most powerful and subtly misunderstood results in the field is the Optional Sampling Theorem, which relates the expectation of a martingale at different random times. This exercise presents the canonical counterexample that every student of stochastic processes must master to avoid common errors. By examining a standard Brownian motion $W_t$ and the first time it reaches the level $1$, you will see firsthand how the theorem can fail for an unbounded stopping time and understand that uniform integrability is the key property that governs its applicability [@problem_id:3045841].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},\\mathbb{F},\\mathbb{P})$ satisfying the usual conditions, where $\\mathbb{F}=(\\mathcal{F}_t)_{t \\ge 0}$ is the natural filtration of a standard Brownian motion (Wiener process) $W=(W_t)_{t \\ge 0}$ with $W_0=0$. Recall the definitions: a process $M=(M_t)_{t \\ge 0}$ adapted to $\\mathbb{F}$ is a martingale if for all $s \\le t$, $M_t$ is integrable and $\\mathbb{E}[M_t \\mid \\mathcal{F}_s]=M_s$; it is a local martingale if there exists a sequence of stopping times $(\\sigma_n)_{n \\in \\mathbb{N}}$ increasing to $+\\infty$ almost surely such that the stopped process $M^{\\sigma_n}_t := M_{t \\wedge \\sigma_n}$ is a martingale for each $n$. A random time $\\tau$ is a stopping time if $\\{\\tau \\le t\\} \\in \\mathcal{F}_t$ for all $t \\ge 0$. The optional sampling theorem (also called optional stopping theorem) gives conditions under which sampling a martingale at a stopping time preserves expectations.\n\nConstruct the following example: let $\\tau := \\inf\\{t \\ge 0 : W_t = 1\\}$ and, for each $n \\in \\mathbb{N}$, let $\\tau_n := \\tau \\wedge n$. Consider the local martingale $M_t := W_t$ and the stopped random variables $W_{\\tau_n}$ and $W_\\tau$. It is known from the continuity of Brownian paths that $W_\\tau = 1$ almost surely and that $\\tau$ is almost surely finite. Analyze what happens to expectations under sampling at $\\tau_n$ and at $\\tau$, and explain the mechanism behind any failure that may arise when passing from $\\tau_n$ to $\\tau$.\n\nSelect all correct statements below.\n\nA. The optional sampling theorem guarantees $\\mathbb{E}[W_\\tau] = \\mathbb{E}[W_0]$ for any almost surely finite stopping time $\\tau$; therefore, in this construction, $\\mathbb{E}[W_\\tau]=0$.\n\nB. For each $n \\in \\mathbb{N}$, $\\mathbb{E}[W_{\\tau_n}] = \\mathbb{E}[W_0]$, but $\\mathbb{E}[W_{\\tau}] \\ne \\mathbb{E}[W_0]$; the failure at the limit occurs because the family $\\{W_{\\tau \\wedge n} : n \\in \\mathbb{N}\\}$ is not uniformly integrable.\n\nC. The random variables $W_{\\tau \\wedge n}$ are bounded in absolute value by $1$, so by dominated convergence $\\mathbb{E}[W_{\\tau \\wedge n}] \\to \\mathbb{E}[W_\\tau]$; therefore optional sampling cannot fail in this setting.\n\nD. If instead the stopping time were bounded, for example $\\tau' := \\tau \\wedge T$ with $T0$ deterministic, then $\\mathbb{E}[W_{\\tau'}] = \\mathbb{E}[W_0]$ would hold.", "solution": "The problem statement is scrutinized for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   A filtered probability space $(\\Omega,\\mathcal{F},\\mathbb{F},\\mathbb{P})$ satisfying the usual conditions.\n-   $\\mathbb{F}=(\\mathcal{F}_t)_{t \\ge 0}$ is the natural filtration of a standard Brownian motion (Wiener process) $W=(W_t)_{t \\ge 0}$.\n-   The initial value of the Brownian motion is $W_0=0$.\n-   A process $M=(M_t)_{t \\ge 0}$ adapted to $\\mathbb{F}$ is a martingale if for all $s \\le t$, $M_t$ is integrable and $\\mathbb{E}[M_t \\mid \\mathcal{F}_s]=M_s$.\n-   A process is a local martingale if there exists a sequence of stopping times $(\\sigma_n)_{n \\in \\mathbb{N}}$ increasing to $+\\infty$ almost surely such that the stopped process $M^{\\sigma_n}_t := M_{t \\wedge \\sigma_n}$ is a martingale for each $n$.\n-   A random time $\\tau$ is a stopping time if $\\{\\tau \\le t\\} \\in \\mathcal{F}_t$ for all $t \\ge 0$.\n-   The stopping time $\\tau$ is defined as $\\tau := \\inf\\{t \\ge 0 : W_t = 1\\}$.\n-   A sequence of stopping times is defined as $\\tau_n := \\tau \\wedge n$ for each $n \\in \\mathbb{N}$.\n-   The process under consideration is the local martingale $M_t := W_t$.\n-   It is given that $W_\\tau = 1$ almost surely.\n-   It is given that $\\tau$ is almost surely finite.\n-   The task is to analyze the expectations of the stopped random variables $W_{\\tau_n}$ and $W_\\tau$ and explain the mechanism of any failure when passing from $\\tau_n$ to $\\tau$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is firmly situated in the mathematical theory of stochastic processes, specifically concerning Brownian motion and martingale theory. All definitions (martingale, stopping time) and objects ($W_t$, $\\tau$) are standard and rigorously defined in this field. The given facts, such as $\\tau  \\infty$ almost surely for a one-dimensional Brownian motion, are well-established theorems.\n-   **Well-Posed:** The problem is well-posed. It presents a specific, canonical example used to illustrate the subtleties of the optional sampling theorem. The question is precise, asking for an analysis of expectations and an explanation for a potential failure of the theorem. A unique, correct analysis exists based on established theorems.\n-   **Objective:** The problem is stated in precise, objective mathematical language.\n-   **Conclusion:** The problem statement is free from scientific or factual unsoundness, is formalizable, is complete, is mathematically feasible, is well-posed, and is a non-trivial, standard pedagogical example. No invalidating flaws are present.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be derived.\n\n### Derivation\nThe process $W = (W_t)_{t \\ge 0}$ is a standard Brownian motion starting at $W_0 = 0$. By definition, $W_t$ is a martingale with respect to its natural filtration $\\mathbb{F}$. This implies $\\mathbb{E}[W_t] = \\mathbb{E}[W_0] = 0$ for all $t \\ge 0$.\n\nThe Optional Sampling Theorem (OST) provides conditions under which the property $\\mathbb{E}[M_T] = \\mathbb{E}[M_S]$ holds for stopping times $S \\le T$ and a martingale $M$. For a single stopping time $\\sigma$, this often means checking if $\\mathbb{E}[M_\\sigma] = \\mathbb{E}[M_0]$. Sufficient conditions for this equality include:\n1.  The stopping time $\\sigma$ is bounded, i.e., there exists a constant $C  \\infty$ such that $\\sigma \\le C$ almost surely.\n2.  $M$ is a uniformly integrable martingale.\n3.  For the specific stopping time $\\sigma$, the family of random variables $\\{M_{t \\wedge \\sigma}\\}_{t \\ge 0}$ is uniformly integrable.\n\nLet's analyze the given stopping times.\n\n**Analysis of $\\tau = \\inf\\{t \\ge 0 : W_t = 1\\}$:**\nThe problem states that $\\tau  \\infty$ almost surely and $W_\\tau = 1$ almost surely (by continuity of Brownian paths). We can compute the expectation of $W_\\tau$ directly:\n$$ \\mathbb{E}[W_\\tau] = \\mathbb{E}[1] = 1 $$\nWe also know that $\\mathbb{E}[W_0] = 0$. Since $\\mathbb{E}[W_\\tau] = 1 \\ne 0 = \\mathbb{E}[W_0]$, the conclusion of the Optional Sampling Theorem does not hold for the martingale $W_t$ and the stopping time $\\tau$. This implies that none of the sufficient conditions for the theorem are met. Specifically, $\\tau$ is not a bounded stopping time. It is also a known result that for a one-dimensional standard Brownian motion, the expectation of the first passage time to any level $a \\ne 0$ is infinite, so $\\mathbb{E}[\\tau] = \\infty$. The failure of the OST is ultimately due to a lack of uniform integrability, which we will analyze next.\n\n**Analysis of $\\tau_n = \\tau \\wedge n$:**\nFor any fixed $n \\in \\mathbb{N}$, the stopping time $\\tau_n$ is defined as the minimum of $\\tau$ and the constant $n$. Therefore, $\\tau_n \\le n$ almost surely. This means $\\tau_n$ is a bounded stopping time. According to the first sufficient condition listed above, the Optional Sampling Theorem holds for the martingale $W_t$ and any bounded stopping time.\nThus, for each $n \\in \\mathbb{N}$:\n$$ \\mathbb{E}[W_{\\tau_n}] = \\mathbb{E}[W_{\\tau \\wedge n}] = \\mathbb{E}[W_0] = 0 $$\n\n**The limiting behavior:**\nWe have a sequence of random variables {$W_{\\tau \\wedge n}$}$_{n \\in \\mathbb{N}}$. We've established:\n_1._ $\\mathbb{E}[W_{\\tau \\wedge n}] = 0$ for all $n \\in \\mathbb{N}$.\n_2._ As $n \\to \\infty$, $\\tau \\wedge n \\to \\tau$ almost surely, because $\\tau$ is almost surely finite.\n_3._ By the path continuity of Brownian motion, $W_{\\tau \\wedge n} \\to W_\\tau$ almost surely as $n \\to \\infty$.\n\nNow, let's examine the limit of the expectations:\n$$ \\lim_{n \\to \\infty} \\mathbb{E}[W_{\\tau \\wedge n}] = \\lim_{n \\to \\infty} 0 = 0 $$\nAnd the expectation of the limit:\n$$ \\mathbb{E}\\left[\\lim_{n \\to \\infty} W_{\\tau \\wedge n}\\right] = \\mathbb{E}[W_\\tau] = 1 $$\nSince $\\lim_{n \\to \\infty} \\mathbb{E}[W_{\\tau \\wedge n}] \\ne \\mathbb{E}[\\lim_{n \\to \\infty} W_{\\tau \\wedge n}]$, we are not permitted to interchange the limit and expectation operators. The Vitali Convergence Theorem states that for a sequence of random variables $X_n$ that converges in probability (and thus almost surely) to $X$, the convergence also holds in $L^1$ (i.e., $\\mathbb{E}[|X_n - X|] \\to 0$, which implies $\\mathbb{E}[X_n] \\to \\mathbb{E}[X]$) if and only if the sequence $\\{X_n\\}$ is uniformly integrable.\n\nIn our case, since $\\mathbb{E}[W_{\\tau \\wedge n}] \\not\\to \\mathbb{E}[W_\\tau]$, it must be that the sequence of random variables $\\{W_{\\tau \\wedge n}\\}_{n \\in \\mathbb{N}}$ is **not** uniformly integrable. This is the precise mathematical mechanism that explains the failure of the Optional Sampling Theorem in the limit.\n\n### Option-by-Option Analysis\n\n**A. The optional sampling theorem guarantees $\\mathbb{E}[W_\\tau] = \\mathbb{E}[W_0]$ for any almost surely finite stopping time $\\tau$; therefore, in this construction, $\\mathbb{E}[W_\\tau]=0$.**\nThis statement is incorrect. The premise \"The optional sampling theorem guarantees $\\mathbb{E}[W_\\tau] = \\mathbb{E}[W_0]$ for any almost surely finite stopping time $\\tau$\" is false. Almost sure finiteness is a necessary but not a sufficient condition. As demonstrated in this very problem, $\\tau$ is almost surely finite, yet $\\mathbb{E}[W_\\tau] = 1 \\ne 0 = \\mathbb{E}[W_0]$.\n**Verdict: Incorrect.**\n\n**B. For each $n \\in \\mathbb{N}$, $\\mathbb{E}[W_{\\tau_n}] = \\mathbb{E}[W_0]$, but $\\mathbb{E}[W_{\\tau}] \\ne \\mathbb{E}[W_0]$; the failure at the limit occurs because the family $\\{W_{\\tau \\wedge n} : n \\in \\mathbb{N}\\}$ is not uniformly integrable.**\nThis statement is composed of three claims. Let's verify each one.\n_1._ \"For each $n \\in \\mathbb{N}$, $\\mathbb{E}[W_{\\tau_n}] = \\mathbb{E}[W_0]$\": This is correct. As shown in the derivation, $\\tau_n = \\tau \\wedge n$ is a bounded stopping time, so the OST applies, yielding $\\mathbb{E}[W_{\\tau_n}] = 0$.\n_2._ \"but $\\mathbb{E}[W_{\\tau}] \\ne \\mathbb{E}[W_0]$\": This is correct. As shown, $\\mathbb{E}[W_\\tau] = 1$ while $\\mathbb{E}[W_0] = 0$.\n_3._ \"the failure at the limit occurs because the family $\\{W_{\\tau \\wedge n} : n \\in \\mathbb{N}\\}$ is not uniformly integrable\": This is the correct explanation. The non-convergence of the expectations for a sequence of random variables that converges almost surely is the definition of a lack of uniform integrability (via the Vitali Convergence Theorem). This is the fundamental reason the OST fails for $\\tau$.\nAll parts of the statement are correct.\n**Verdict: Correct.**\n\n**C. The random variables $W_{\\tau \\wedge n}$ are bounded in absolute value by $1$, so by dominated convergence $\\mathbb{E}[W_{\\tau \\wedge n}] \\to \\mathbb{E}[W_\\tau]$; therefore optional sampling cannot fail in this setting.**\nThe premise \"The random variables $W_{\\tau \\wedge n}$ are bounded in absolute value by $1$\" is false. The random variable is $W_{\\tau \\wedge n}$. If $\\tau  n$, then $W_{\\tau \\wedge n} = W_n$. On the event $\\{\\tau  n\\}$, the path $W_s$ for $s \\in [0, n]$ has stayed strictly below $1$. However, it is not bounded from below. A Brownian path can reach large negative values before time $n$ without having hit $1$. For instance, a path could go to $-10$ at time $t  n$ and then continue, with $\\tau  n$. In this case, $W_n$ might be a large negative number. Since the random variables are not uniformly bounded, the dominated convergence theorem cannot be applied on this basis, and the conclusion is invalid. We have already shown that optional sampling does fail.\n**Verdict: Incorrect.**\n\n**D. If instead the stopping time were bounded, for example $\\tau' := \\tau \\wedge T$ with $T0$ deterministic, then $\\mathbb{E}[W_{\\tau'}] = \\mathbb{E}[W_0]$ would hold.**\nThis statement is correct. It is a direct application of a standard version of the Optional Sampling Theorem. For any martingale $M_t$ and any bounded stopping time $\\sigma$ (i.e., $\\sigma \\le C$ a.s. for some constant $C$), $\\mathbb{E}[M_\\sigma] = \\mathbb{E}[M_0]$. The process $W_t$ is a martingale. The stopping time $\\tau' = \\tau \\wedge T$ for a deterministic $T  0$ is bounded by $T$. Therefore, the theorem applies directly, and the equality $\\mathbb{E}[W_{\\tau \\wedge T}] = \\mathbb{E}[W_0]$ must hold. This is precisely what we used to analyze $\\tau_n$ for each $n$.\n**Verdict: Correct.**", "answer": "$$\\boxed{BD}$$", "id": "3045841"}]}