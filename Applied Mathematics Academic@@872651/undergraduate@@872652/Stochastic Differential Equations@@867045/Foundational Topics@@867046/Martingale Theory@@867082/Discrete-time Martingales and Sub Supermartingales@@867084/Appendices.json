{"hands_on_practices": [{"introduction": "The increments of a martingale form what is known as a martingale difference sequence (MDS)—a series of \"innovations\" that have zero expected value given the past. This exercise [@problem_id:3049353] provides foundational practice by asking you to construct a process from a sequence of simple random variables and verify directly from the definitions that it forms an MDS. You will also compute the variance of the resulting martingale, highlighting how the uncorrelated nature of MDS terms simplifies the calculation.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a probability space supporting a sequence $\\left(\\xi_{k}\\right)_{k \\geq 1}$ of independent and identically distributed (i.i.d.) Rademacher random variables, that is, $\\mathbb{P}\\!\\left(\\xi_{k}=1\\right)=\\mathbb{P}\\!\\left(\\xi_{k}=-1\\right)=\\frac{1}{2}$ for every $k \\in \\mathbb{N}$. Fix a constant $r$ with $0r1$, and define the filtration $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$ by $\\mathcal{F}_{0}=\\{\\emptyset,\\Omega\\}$ and $\\mathcal{F}_{k}=\\sigma\\!\\left(\\xi_{1},\\dots,\\xi_{k}\\right)$ for $k \\geq 1$. Consider the sequence $\\left(D_{k}\\right)_{k \\geq 1}$ defined by $D_{k}=r^{k-1}\\xi_{k}$, and let the partial sums be $S_{n}=\\sum_{k=1}^{n}D_{k}$.\n\nUsing only fundamental definitions and properties of conditional expectation, martingale difference sequences, independence, and variance, do the following:\n\n- Show that $\\left(D_{k}\\right)_{k \\geq 1}$ is a martingale difference sequence (MDS) with respect to $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$.\n- Verify that the differences are bounded almost surely by a finite constant that does not depend on $k$.\n- Compute the variance $\\mathrm{Var}\\!\\left(S_{n}\\right)$ as a closed-form analytical expression in terms of $n$ and $r$.\n\nYour final answer must be a single closed-form expression for $\\mathrm{Var}\\!\\left(S_{n}\\right)$, with no units. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and complete. All provided definitions and constants are standard in the theory of stochastic processes. The tasks are specific, mathematically formalizable, and do not contain any contradictions or ambiguities. The conditions on the constant $r$ are consistent with the required calculations. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe problem asks for three things:\n1.  To show that the sequence $\\left(D_{k}\\right)_{k \\geq 1}$ is a martingale difference sequence (MDS) with respect to the filtration $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$.\n2.  To verify that a.s. $|D_k| \\le M$ for a constant $M$ independent of $k$.\n3.  To compute the variance $\\mathrm{Var}\\!\\left(S_{n}\\right)$.\n\nWe will address each part in sequence.\n\nA sequence of random variables $\\left(D_k\\right)_{k \\geq 1}$ is a martingale difference sequence with respect to a filtration $\\left(\\mathcal{F}_k\\right)_{k \\geq 0}$ if it satisfies three conditions for all $k \\ge 1$:\n(i) $D_k$ is $\\mathcal{F}_k$-measurable.\n(ii) $D_k$ is integrable, i.e., $\\mathbb{E}\\!\\left[|D_k|\\right]  \\infty$.\n(iii) The conditional expectation of $D_k$ given the past is zero, i.e., $\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = 0$.\n\nLet's verify these for $D_{k}=r^{k-1}\\xi_{k}$.\n\n(i) **Measurability**: The filtration is defined as $\\mathcal{F}_{k}=\\sigma\\!\\left(\\xi_{1},\\dots,\\xi_{k}\\right)$. The random variable $\\xi_k$ is, by definition, measurable with respect to the sigma-algebra it helps generate, $\\mathcal{F}_k$. Since $D_k$ is a constant multiple of $\\xi_k$ (namely, $D_k = f(\\xi_k)$ where $f(x)=r^{k-1}x$), and $f$ is a Borel-measurable function, $D_k$ is also $\\mathcal{F}_k$-measurable.\n\n(ii) **Integrability**: We compute the expectation of the absolute value of $D_k$:\n$$\n\\mathbb{E}\\!\\left[|D_k|\\right] = \\mathbb{E}\\!\\left[|r^{k-1}\\xi_{k}|\\right]\n$$\nSince $0  r  1$ and $k \\geq 1$, the term $r^{k-1}$ is a positive constant. The random variable $\\xi_k$ takes values in $\\{-1, 1\\}$, so $|\\xi_k|=1$ almost surely.\n$$\n\\mathbb{E}\\!\\left[|D_k|\\right] = \\mathbb{E}\\!\\left[r^{k-1}|\\xi_k|\\right] = r^{k-1}\\mathbb{E}\\!\\left[|\\xi_k|\\right] = r^{k-1}\\mathbb{E}\\!\\left[1\\right] = r^{k-1}\n$$\nSince $r^{k-1}$ is a finite real number for any $k \\geq 1$, the condition $\\mathbb{E}\\!\\left[|D_k|\\right]  \\infty$ is satisfied.\n\n(iii) **MDS Condition**: We must show that $\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = 0$.\n$$\n\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = \\mathbb{E}\\!\\left[r^{k-1}\\xi_k|\\mathcal{F}_{k-1}\\right]\n$$\nThe term $r^{k-1}$ is a deterministic constant, so it can be factored out of the conditional expectation:\n$$\n\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = r^{k-1}\\mathbb{E}\\!\\left[\\xi_k|\\mathcal{F}_{k-1}\\right]\n$$\nThe sequence $\\left(\\xi_i\\right)_{i \\geq 1}$ is independent. This means that $\\xi_k$ is independent of the set of random variables $\\{\\xi_1, \\dots, \\xi_{k-1}\\}$. Consequently, $\\xi_k$ is independent of the sigma-algebra generated by them, $\\mathcal{F}_{k-1}=\\sigma\\!\\left(\\xi_{1},\\dots,\\xi_{k-1}\\right)$. A fundamental property of conditional expectation is that if a random variable $X$ is independent of a sigma-algebra $\\mathcal{G}$, then $\\mathbb{E}\\!\\left[X|\\mathcal{G}\\right]=\\mathbb{E}\\!\\left[X\\right]$. Applying this property, we have:\n$$\n\\mathbb{E}\\!\\left[\\xi_k|\\mathcal{F}_{k-1}\\right] = \\mathbb{E}\\!\\left[\\xi_k\\right]\n$$\nThe expectation of a Rademacher random variable $\\xi_k$ is:\n$$\n\\mathbb{E}\\!\\left[\\xi_k\\right] = (-1) \\cdot \\mathbb{P}\\!\\left(\\xi_k=-1\\right) + (1) \\cdot \\mathbb{P}\\!\\left(\\xi_k=1\\right) = (-1)\\cdot\\frac{1}{2} + (1)\\cdot\\frac{1}{2} = 0\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right] = r^{k-1} \\cdot 0 = 0\n$$\nAll three conditions are satisfied. Thus, $\\left(D_{k}\\right)_{k \\geq 1}$ is a martingale difference sequence with respect to $\\left(\\mathcal{F}_{k}\\right)_{k \\geq 0}$.\n\nNext, we verify that the differences are bounded almost surely. We consider the absolute value of $D_k$:\n$$\n|D_k| = |r^{k-1}\\xi_k| = |r^{k-1}||\\xi_k|\n$$\nSince $0r1$, we have $|r^{k-1}| = r^{k-1}$. For $k \\geq 1$, we have $k-1 \\geq 0$, which implies $0  r^{k-1} \\leq r^0 = 1$. The variable $\\xi_k$ takes values in $\\{-1, 1\\}$, so $|\\xi_k| = 1$ almost surely.\nTherefore, for all $k \\geq 1$:\n$$\n|D_k| = r^{k-1} \\cdot 1 \\leq 1\n$$\nThis shows that the sequence is uniformly bounded almost surely by the constant $M=1$, which does not depend on $k$.\n\nFinally, we compute the variance of the partial sum $S_{n}=\\sum_{k=1}^{n}D_{k}$. The variance of a sum of random variables is the sum of their covariances:\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = \\mathrm{Var}\\!\\left(\\sum_{k=1}^{n}D_{k}\\right) = \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\mathrm{Cov}\\!\\left(D_i, D_j\\right)\n$$\nAn important property of a martingale difference sequence is that its elements are uncorrelated. To show this, let's compute the covariance for $i \\neq j$. Assume without loss of generality that $i  j$.\n$$\n\\mathrm{Cov}\\!\\left(D_i, D_j\\right) = \\mathbb{E}\\!\\left[D_i D_j\\right] - \\mathbb{E}\\!\\left[D_i\\right]\\mathbb{E}\\!\\left[D_j\\right]\n$$\nFirst, let's find the unconditional expectation of $D_k$:\n$$\n\\mathbb{E}\\!\\left[D_k\\right] = \\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[D_k|\\mathcal{F}_{k-1}\\right]\\right] = \\mathbb{E}\\!\\left[0\\right] = 0\n$$\nwhere we used the law of total expectation and the MDS property. Thus, $\\mathbb{E}\\!\\left[D_i\\right]=\\mathbb{E}\\!\\left[D_j\\right]=0$, and the covariance simplifies to:\n$$\n\\mathrm{Cov}\\!\\left(D_i, D_j\\right) = \\mathbb{E}\\!\\left[D_i D_j\\right]\n$$\nUsing the tower property of conditional expectation and the fact that $D_i$ is $\\mathcal{F}_{j-1}$-measurable for $ij$:\n$$\n\\mathbb{E}\\!\\left[D_i D_j\\right] = \\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[D_i D_j|\\mathcal{F}_{j-1}\\right]\\right] = \\mathbb{E}\\!\\left[D_i \\mathbb{E}\\!\\left[D_j|\\mathcal{F}_{j-1}\\right]\\right]\n$$\nSince $\\left(D_k\\right)$ is an MDS, we have $\\mathbb{E}\\!\\left[D_j|\\mathcal{F}_{j-1}\\right]=0$. Therefore:\n$$\n\\mathrm{Cov}\\!\\left(D_i, D_j\\right) = \\mathbb{E}\\!\\left[D_i \\cdot 0\\right] = 0 \\quad \\text{for } i \\neq j\n$$\nThe cross-terms in the variance sum are all zero. The variance of the sum simplifies to the sum of the variances:\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = \\sum_{k=1}^{n} \\mathrm{Var}\\!\\left(D_k\\right)\n$$\nNow, we compute $\\mathrm{Var}\\!\\left(D_k\\right)$:\n$$\n\\mathrm{Var}\\!\\left(D_k\\right) = \\mathbb{E}\\!\\left[D_k^2\\right] - \\left(\\mathbb{E}\\!\\left[D_k\\right]\\right)^2\n$$\nSince $\\mathbb{E}\\!\\left[D_k\\right]=0$, this is simply $\\mathrm{Var}\\!\\left(D_k\\right) = \\mathbb{E}\\!\\left[D_k^2\\right]$.\n$$\n\\mathbb{E}\\!\\left[D_k^2\\right] = \\mathbb{E}\\!\\left[\\left(r^{k-1}\\xi_k\\right)^2\\right] = \\mathbb{E}\\!\\left[\\left(r^{k-1}\\right)^2\\xi_k^2\\right] = r^{2(k-1)}\\mathbb{E}\\!\\left[\\xi_k^2\\right]\n$$\nFor a Rademacher random variable, $\\xi_k^2$ is always $1$ (since $(-1)^2=1$ and $1^2=1$). Thus, $\\mathbb{E}\\!\\left[\\xi_k^2\\right]=1$.\nSo, the variance of an individual term is:\n$$\n\\mathrm{Var}\\!\\left(D_k\\right) = r^{2(k-1)}\n$$\nFinally, we sum these variances to get $\\mathrm{Var}\\!\\left(S_{n}\\right)$:\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = \\sum_{k=1}^{n} r^{2(k-1)}\n$$\nThis is a finite geometric series with first term $a = r^{2(1-1)} = r^0 = 1$, common ratio $q = r^2$, and $n$ terms. The sum is given by the formula $a \\frac{1-q^n}{1-q}$.\n$$\n\\mathrm{Var}\\!\\left(S_{n}\\right) = 1 \\cdot \\frac{1 - (r^2)^n}{1-r^2} = \\frac{1 - r^{2n}}{1-r^2}\n$$\nThis is the closed-form analytical expression for the variance of $S_n$.", "answer": "$$\n\\boxed{\\frac{1-r^{2n}}{1-r^2}}\n$$", "id": "3049353"}, {"introduction": "While martingales model fair games, many stochastic processes exhibit a predictable drift, classifying them as submartingales (favorable games) or supermartingales (unfavorable games). The Doob decomposition is a fundamental theorem that uniquely splits any submartingale into a core martingale component and a predictable, increasing \"trend\" known as the compensator. This practice [@problem_id:3049361] guides you through applying this theorem to a discrete counting process, offering a concrete look at how to isolate a process's underlying martingale structure.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{n})_{n \\geq 0},\\mathbb{P})$. A discrete-time counting process is an adapted process $(N_{n})_{n \\geq 0}$ with $N_{0}=0$, taking values in $\\mathbb{Z}_{\\geq 0}$, and satisfying $N_{n}-N_{n-1} \\in \\{0,1\\}$ for all $n \\geq 1$. Assume that $(N_{n})_{n \\geq 0}$ has conditionally Bernoulli increments with time-varying success probability given by\n$$\n\\mathbb{P}\\!\\left(N_{k}-N_{k-1}=1 \\,\\middle|\\, \\mathcal{F}_{k-1}\\right) \\,=\\, \\frac{1}{k+2}, \\quad k \\geq 1,\n$$\nand that the filtration $(\\mathcal{F}_{n})_{n \\geq 0}$ is the natural filtration of $(N_{n})_{n \\geq 0}$.\n\nStarting only from the definitions of submartingale, martingale, predictable process in discrete time (meaning $\\mathcal{F}_{n-1}$-measurability at time $n$), and the Doob decomposition theorem for discrete-time submartingales, do the following:\n\n- Justify that $(N_{n})_{n \\geq 0}$ is a submartingale with respect to $(\\mathcal{F}_{n})_{n \\geq 0}$.\n- Use the Doob decomposition theorem to derive the unique predictable compensator $(A_{n})_{n \\geq 0}$ with $A_{0}=0$ such that $N_{n}-A_{n}$ is a martingale.\n- Verify from first principles that $(A_{n})_{n \\geq 0}$ is nondecreasing and predictable.\n\nReport your final answer as a single explicit expression for $A_{n}$ in terms of $n$ only. If you use special functions, define them clearly in your reasoning. Your final answer must be a single closed-form analytic expression and must not include an inequality or an equation.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Probability Space and Filtration**: A filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{n})_{n \\geq 0},\\mathbb{P})$.\n- **Process Definition**: $(N_{n})_{n \\geq 0}$ is a discrete-time counting process.\n- **Adaptation**: $(N_{n})_{n \\geq 0}$ is adapted to $(\\mathcal{F}_{n})_{n \\geq 0}$.\n- **Initial Condition**: $N_{0}=0$.\n- **State Space**: $N_{n} \\in \\mathbb{Z}_{\\geq 0}$ for all $n \\geq 0$.\n- **Increment Property**: $N_{n}-N_{n-1} \\in \\{0,1\\}$ for all $n \\geq 1$.\n- **Conditional Increment Law**: $\\mathbb{P}\\!\\left(N_{k}-N_{k-1}=1 \\,\\middle|\\, \\mathcal{F}_{k-1}\\right) \\,=\\, \\frac{1}{k+2}$ for $k \\geq 1$.\n- **Filtration Specification**: $(\\mathcal{F}_{n})_{n \\geq 0}$ is the natural filtration of $(N_{n})_{n \\geq 0}$, meaning $\\mathcal{F}_{n} = \\sigma(N_0, N_1, \\ldots, N_n)$.\n- **Tasks**:\n    1. Justify that $(N_{n})_{n \\geq 0}$ is a submartingale.\n    2. Use the Doob decomposition theorem to find the unique predictable compensator $(A_{n})_{n \\geq 0}$ with $A_0=0$ such that $N_n - A_n$ is a martingale.\n    3. Verify from first principles that $(A_{n})_{n \\geq 0}$ is nondecreasing and predictable.\n- **Required Output**: A single closed-form analytic expression for $A_n$ in terms of $n$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is formulated within the standard mathematical framework of stochastic processes, specifically the theory of discrete-time martingales. All concepts used, such as counting processes, natural filtrations, conditional expectations, and the Doob decomposition, are fundamental and rigorously defined in probability theory. The given conditional probability $\\frac{1}{k+2}$ is a well-defined probability for all $k \\ge 1$, as $0  \\frac{1}{k+2} \\le \\frac{1}{3}$. The premises are scientifically and mathematically sound.\n- **Well-Posedness**: The problem is self-contained, providing all necessary definitions and conditions. The objective is to apply a fundamental theorem (Doob decomposition) whose existence and uniqueness results guarantee a well-posed problem structure. The question is unambiguous and leads to a unique solution for the compensator $(A_n)$.\n- **Objectivity**: The problem is stated in precise, formal mathematical language, devoid of any subjective or opinion-based assertions.\n\n### Step 3: Verdict and Action\nThe problem is mathematically sound, well-posed, and objective. It is deemed **valid**. The solution process will now proceed.\n\n### Solution Derivation\n\nThe solution is structured according to the three tasks outlined in the problem statement.\n\n**1. Justification that $(N_{n})_{n \\geq 0}$ is a Submartingale**\n\nA discrete-time process $(X_n)_{n \\geq 0}$ is a submartingale with respect to the filtration $(\\mathcal{F}_n)_{n \\geq 0}$ if it satisfies three conditions:\ni. $(X_n)$ is adapted to $(\\mathcal{F}_n)$, i.e., $X_n$ is $\\mathcal{F}_n$-measurable for all $n \\geq 0$.\nii. $(X_n)$ is integrable, i.e., $\\mathbb{E}[|X_n|]  \\infty$ for all $n \\geq 0$.\niii. $\\mathbb{E}[X_n \\,|\\, \\mathcal{F}_{n-1}] \\geq X_{n-1}$ for all $n \\geq 1$.\n\nWe verify these conditions for the process $(N_n)_{n \\geq 0}$.\n\ni. **Adaptation**: The problem states that $(N_n)_{n \\geq 0}$ is an adapted process. This is also guaranteed by the fact that $(\\mathcal{F}_n)$ is the natural filtration of $(N_n)$.\n\nii. **Integrability**: For any $n \\geq 1$, we can write $N_n = N_0 + \\sum_{k=1}^n (N_k - N_{k-1})$. Since $N_0=0$ and $N_k - N_{k-1} \\in \\{0,1\\}$, it follows that $0 \\leq N_n \\leq n$. Since $N_n$ is a bounded random variable, its expectation is finite. Specifically, $\\mathbb{E}[|N_n|] = \\mathbb{E}[N_n] \\leq n  \\infty$ for all $n \\geq 0$.\n\niii. **Submartingale Inequality**: We must compute the conditional expectation $\\mathbb{E}[N_n \\,|\\, \\mathcal{F}_{n-1}]$ for $n \\geq 1$.\nUsing the tower property of conditional expectation is not needed here; we use linearity and the fact that $N_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable:\n$$ \\mathbb{E}[N_n \\,|\\, \\mathcal{F}_{n-1}] = \\mathbb{E}[N_{n-1} + (N_n - N_{n-1}) \\,|\\, \\mathcal{F}_{n-1}] = N_{n-1} + \\mathbb{E}[N_n - N_{n-1} \\,|\\, \\mathcal{F}_{n-1}] $$\nThe increment $N_n - N_{n-1}$ is a Bernoulli random variable. Its conditional expectation is:\n$$ \\mathbb{E}[N_n - N_{n-1} \\,|\\, \\mathcal{F}_{n-1}] = 1 \\cdot \\mathbb{P}(N_n - N_{n-1} = 1 \\,|\\, \\mathcal{F}_{n-1}) + 0 \\cdot \\mathbb{P}(N_n - N_{n-1} = 0 \\,|\\, \\mathcal{F}_{n-1}) $$\nUsing the given conditional probability,\n$$ \\mathbb{E}[N_n - N_{n-1} \\,|\\, \\mathcal{F}_{n-1}] = \\frac{1}{n+2} $$\nSubstituting this back, we get:\n$$ \\mathbb{E}[N_n \\,|\\, \\mathcal{F}_{n-1}] = N_{n-1} + \\frac{1}{n+2} $$\nSince $n \\geq 1$, we have $\\frac{1}{n+2}  0$, which implies:\n$$ \\mathbb{E}[N_n \\,|\\, \\mathcal{F}_{n-1}]  N_{n-1} $$\nAll three conditions are satisfied. Thus, $(N_n)_{n \\geq 0}$ is a (strict) submartingale with respect to its natural filtration.\n\n**2. Derivation of the Predictable Compensator $(A_n)_{n \\geq 0}$**\n\nThe Doob decomposition theorem for a discrete-time submartingale $(X_n)_{n \\geq 0}$ states that there exists a unique decomposition $X_n = M_n + A_n$ for all $n \\geq 0$, where:\n- $(M_n)_{n \\geq 0}$ is a martingale with respect to $(\\mathcal{F}_n)$.\n- $(A_n)_{n \\geq 0}$ is a predictable, non-decreasing process.\n- $A_0$ is set to a constant, typically $0$.\n\nThe predictable process $(A_n)$ is constructed via the recurrence relation:\n- $A_0 = 0$.\n- $A_n - A_{n-1} = \\mathbb{E}[X_n - X_{n-1} \\,|\\, \\mathcal{F}_{n-1}]$ for $n \\geq 1$.\n\nApplying this construction to the submartingale $(N_n)_{n \\geq 0}$:\n- $A_0 = 0$, as required.\n- For $n \\geq 1$, the increment is given by:\n$$ A_n - A_{n-1} = \\mathbb{E}[N_n - N_{n-1} \\,|\\, \\mathcal{F}_{n-1}] = \\frac{1}{n+2} $$\nTo find $A_n$, we sum the increments:\n$$ A_n = A_0 + \\sum_{k=1}^n (A_k - A_{k-1}) = 0 + \\sum_{k=1}^n \\frac{1}{k+2} $$\nThis sum can be expressed using the generalized harmonic numbers. The $m$-th harmonic number, a special function, is defined as $H_m = \\sum_{j=1}^m \\frac{1}{j}$.\nWe can re-index the summation for $A_n$ by letting $j=k+2$:\n$$ A_n = \\sum_{j=3}^{n+2} \\frac{1}{j} $$\nThis sum is equivalent to the difference of two harmonic numbers:\n$$ A_n = \\left(\\sum_{j=1}^{n+2} \\frac{1}{j}\\right) - \\left(\\sum_{j=1}^{2} \\frac{1}{j}\\right) = H_{n+2} - H_2 $$\nSince $H_2 = 1 + \\frac{1}{2} = \\frac{3}{2}$, we obtain the final expression for the compensator:\n$$ A_n = H_{n+2} - \\frac{3}{2} $$\nBy this construction, the process $M_n = N_n - A_n = N_n - (H_{n+2} - \\frac{3}{2})$ is a martingale.\n\n**3. Verification of Properties of $(A_n)_{n \\geq 0}$**\n\nWe verify from first principles that $(A_n)$ is predictable and nondecreasing.\n\n- **Predictability**: A process $(C_n)_{n \\geq 0}$ is predictable if $C_n$ is $\\mathcal{F}_{n-1}$-measurable for each $n \\geq 1$, and $C_0$ is $\\mathcal{F}_{-1}$-measurable (i.e., constant).\n    - For $n=0$, $A_0 = 0$, which is a constant.\n    - For $n \\geq 1$, $A_n = \\sum_{k=1}^n \\frac{1}{k+2}$. This value depends only on the non-random integer $n$. A deterministic quantity is measurable with respect to any sigma-algebra. In particular, for any $n \\geq 1$, $A_n$ is a constant and is therefore $\\mathcal{F}_{n-1}$-measurable.\n    - Thus, $(A_n)_{n \\geq 0}$ is a predictable process.\n\n- **Nondecreasing Nature**: A process $(C_n)_{n \\geq 0}$ is nondecreasing if $C_n \\geq C_{n-1}$ for all $n \\geq 1$.\n    - We examine the increment $A_n - A_{n-1}$. From our derivation, for $n \\geq 1$:\n    $$ A_n - A_{n-1} = \\frac{1}{n+2} $$\n    - Since $n \\geq 1$, the denominator $n+2$ is an integer greater than or equal to $3$. Therefore, $\\frac{1}{n+2}  0$ for all $n \\geq 1$.\n    - This implies $A_n  A_{n-1}$ for all $n \\geq 1$. The process $(A_n)$ is strictly increasing, which satisfies the nondecreasing condition.\n\nAll tasks have been completed, and the expression for $A_n$ has been derived and verified.", "answer": "$$\n\\boxed{H_{n+2} - \\frac{3}{2}}\n$$", "id": "3049361"}, {"introduction": "A key application of martingale theory lies in analyzing processes that are stopped at random times. The Optional Stopping Theorem (OST) is a powerful result that provides conditions under which the martingale property—that the expected future value is the current value—holds even at these stopping times. In this exercise [@problem_id:3049332], you will leverage the OST on two different martingales associated with a random walk to elegantly compute both the hitting probabilities and the expected duration of the walk, solving a classic problem known as the \"Gambler's Ruin.\"", "problem": "Consider a simple symmetric random walk $\\{S_{n}\\}_{n \\geq 0}$ on the integers defined by $S_{0} = 0$ and $S_{n} = \\sum_{k=1}^{n} X_{k}$ for $n \\geq 1$, where the increments $\\{X_{k}\\}_{k \\geq 1}$ are independent and identically distributed with $\\mathbb{P}(X_{k} = 1) = \\mathbb{P}(X_{k} = -1) = \\frac{1}{2}$. Let $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$ denote the natural filtration of $\\{S_{n}\\}_{n \\geq 0}$. We desire a stopping time $T$ such that the stopped value $S_{T}$ has the two-point distribution $\\mathbb{P}(S_{T} = 4) = \\frac{3}{7}$ and $\\mathbb{P}(S_{T} = -3) = \\frac{4}{7}$.\n\nConstruct the stopping time $T$ as the first hitting time of the set $\\{-3, 4\\}$, that is,\n$$\nT := \\inf\\{n \\geq 0 : S_{n} \\in \\{-3, 4\\}\\}.\n$$\nStarting from the core definitions of martingales and stopping times, and using only well-tested facts about conditional expectations and the optional stopping theorem under its standard sufficient conditions, do the following:\n\n1. Prove that $T$ is a stopping time with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n2. Verify that $\\{S_{n}\\}_{n \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n3. Justify the use of the optional stopping theorem for the bounded stopping times $T \\wedge n$ (where $a \\wedge b$ denotes the minimum of $a$ and $b$), and then, by an appropriate limiting argument, conclude that $\\mathbb{E}[S_{T}] = \\mathbb{E}[S_{0}]$.\n4. Using the conclusion in the previous step, derive the distribution of $S_{T}$ and confirm that $\\mathbb{P}(S_{T} = 4) = \\frac{3}{7}$ and $\\mathbb{P}(S_{T} = -3) = \\frac{4}{7}$.\n5. Consider the process $\\{M_{n}\\}_{n \\geq 0}$ defined by $M_{n} := S_{n}^{2} - n$. Prove that $\\{M_{n}\\}_{n \\geq 0}$ is a martingale, verify optional stopping for $T \\wedge n$, and use dominated and monotone convergence theorems to obtain a closed-form expression for $\\mathbb{E}[T]$.\n\nCompute the exact value of $\\mathbb{E}[T]$ for the constructed stopping time $T$. No rounding is required. Express your final answer as a single real number.", "solution": "The problem is analyzed and solved in five parts as requested.\n\n### Part 1: Prove that $T$ is a stopping time\nA random variable $T$ taking values in $\\{0, 1, 2, \\dots\\} \\cup \\{\\infty\\}$ is a stopping time with respect to the filtration $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$ if the event $\\{T \\leq n\\}$ is in $\\mathcal{F}_{n}$ for every $n \\geq 0$. The filtration is given as the natural filtration of the process $\\{S_{n}\\}$, i.e., $\\mathcal{F}_{n} = \\sigma(S_{0}, S_{1}, \\dots, S_{n})$.\n\nThe stopping time $T$ is defined as $T := \\inf\\{k \\geq 0 : S_{k} \\in \\{-3, 4\\}\\}$.\nThe event $\\{T \\leq n\\}$ can be written as the union of events that the random walk has hit the set $\\{-3, 4\\}$ at or before time $n$:\n$$\n\\{T \\leq n\\} = \\bigcup_{k=0}^{n} \\{S_{k} \\in \\{-3, 4\\}\\}.\n$$\nFor any $k \\in \\{0, 1, \\dots, n\\}$, the event $\\{S_{k} \\in \\{-3, 4\\}\\}$ is the union of two events, $\\{S_{k} = -3\\}$ and $\\{S_{k} = 4\\}$.\nSince $S_{k}$ is by definition an $\\mathcal{F}_{k}$-measurable random variable, the events $\\{S_{k} = -3\\}$ and $\\{S_{k} = 4\\}$ are both in $\\mathcal{F}_{k}$.\nThe filtration is non-decreasing, so $\\mathcal{F}_{k} \\subseteq \\mathcal{F}_{n}$ for any $k \\leq n$.\nTherefore, $\\{S_{k} = -3\\}$ and $\\{S_{k} = 4\\}$ are both in $\\mathcal{F}_{n}$ for any $k \\leq n$.\nThe finite union of sets in a $\\sigma$-algebra is also in that $\\sigma$-algebra. Thus, for each $k \\in \\{0, \\dots, n\\}$, the event $\\{S_{k} \\in \\{-3, 4\\}\\}$ is in $\\mathcal{F}_{n}$.\nConsequently, the event $\\{T \\leq n\\} = \\bigcup_{k=0}^{n} \\{S_{k} \\in \\{-3, 4\\}\\}$, being a finite union of $\\mathcal{F}_{n}$-measurable sets, is itself $\\mathcal{F}_{n}$-measurable. This holds for all $n \\geq 0$.\nHence, $T$ is a stopping time with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$.\n\n### Part 2: Verify that $\\{S_{n}\\}_{n \\geq 0}$ is a martingale\nTo show that $\\{S_{n}\\}_{n \\geq 0}$ is a martingale with respect to $\\{\\mathcal{F}_{n}\\}_{n \\geq 0}$, we must verify three conditions:\n1.  $S_{n}$ is $\\mathcal{F}_{n}$-measurable for each $n \\geq 0$. This is true by definition of the natural filtration $\\mathcal{F}_{n}$.\n2.  $\\mathbb{E}[|S_{n}|]  \\infty$ for each $n \\geq 0$. Since $S_{n} = \\sum_{k=1}^{n} X_{k}$, we have $|S_{n}| = |\\sum_{k=1}^{n} X_{k}| \\leq \\sum_{k=1}^{n} |X_{k}|$. As $|X_{k}|=1$ for all $k$, we get $|S_{n}| \\leq n$. Thus, $\\mathbb{E}[|S_{n}|] \\leq \\mathbb{E}[n] = n  \\infty$.\n3.  $\\mathbb{E}[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1}$ for all $n \\geq 1$.\nWe compute the conditional expectation:\n$$\n\\mathbb{E}[S_{n} | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n-1} + X_{n} | \\mathcal{F}_{n-1}].\n$$\nBy the linearity of conditional expectation, this becomes:\n$$\n\\mathbb{E}[S_{n-1} | \\mathcal{F}_{n-1}] + \\mathbb{E}[X_{n} | \\mathcal{F}_{n-1}].\n$$\nSince $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable, $\\mathbb{E}[S_{n-1} | \\mathcal{F}_{n-1}] = S_{n-1}$.\nThe increment $X_{n}$ is independent of the past increments $\\{X_{1}, \\dots, X_{n-1}\\}$, and thus independent of the $\\sigma$-algebra they generate, $\\mathcal{F}_{n-1} = \\sigma(S_0, ..., S_{n-1})$. Therefore, $\\mathbb{E}[X_{n} | \\mathcal{F}_{n-1}] = \\mathbb{E}[X_{n}]$.\nThe expectation of $X_{n}$ is:\n$$\n\\mathbb{E}[X_{n}] = (1) \\cdot \\mathbb{P}(X_{n}=1) + (-1) \\cdot \\mathbb{P}(X_{n}=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0.\n$$\nSubstituting these results back, we obtain:\n$$\n\\mathbb{E}[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1} + 0 = S_{n-1}.\n$$\nAll three conditions are satisfied, so $\\{S_{n}\\}_{n \\geq 0}$ is a martingale.\n\n### Part 3: Justify the use of the optional stopping theorem\nFor each $n \\geq 0$, define the stopped time $T_{n} = T \\wedge n = \\min(T, n)$. Since $T$ is a stopping time and $n$ is a constant, $T_{n}$ is also a stopping time. Furthermore, $T_{n}$ is bounded by $n$.\nThe optional stopping theorem for bounded stopping times states that if $\\{S_{k}\\}$ is a martingale and $\\tau$ is a bounded stopping time, then $\\mathbb{E}[S_{\\tau}] = \\mathbb{E}[S_{0}]$.\nApplying this to the bounded stopping time $T_{n}$, we have:\n$$\n\\mathbb{E}[S_{T \\wedge n}] = \\mathbb{E}[S_{0}].\n$$\nSince $S_{0}=0$, we have $\\mathbb{E}[S_{T \\wedge n}] = 0$ for all $n \\geq 0$.\n\nTo extend this to the unbounded stopping time $T$, we must justify taking the limit as $n \\to \\infty$. One of the sufficient conditions for the optional stopping theorem is that the stopped process is uniformly bounded. Let's check this condition for $S_{T \\wedge n}$.\nBy construction of $T$, for any $k  T$, the position of the walk $S_{k}$ is strictly between $-3$ and $4$, i.e., $-3  S_{k}  4$.\nIf the stopping occurs at or before time $n$ (i.e., $T \\leq n$), then $S_{T \\wedge n} = S_{T}$, which is either $-3$ or $4$.\nIf the stopping occurs after time $n$ (i.e., $T  n$), then $S_{T \\wedge n} = S_{n}$, and since $n  T$, we have $-3  S_{n}  4$.\nIn all cases, $|S_{T \\wedge n}| \\leq 4$ for all $n \\geq 0$ and all sample paths. The stopped process $\\{S_{T \\wedge n}\\}_{n \\geq 0}$ is uniformly bounded by the constant $4$.\nA one-dimensional simple symmetric random walk is recurrent, which implies that it will eventually visit any integer state. Thus, it will eventually hit the set $\\{-3, 4\\}$, which means $\\mathbb{P}(T  \\infty)=1$.\nAs $n \\to \\infty$, $T \\wedge n \\to T$ almost surely. Since $S$ is a function defined on the integers, this implies $S_{T \\wedge n} \\to S_{T}$ almost surely.\nThe sequence of random variables $\\{S_{T \\wedge n}\\}_{n \\geq 0}$ converges almost surely to $S_{T}$ and is uniformly bounded in absolute value by $4$. The constant random variable $Y=4$ is integrable. Therefore, by the Dominated Convergence Theorem, we can interchange the limit and the expectation:\n$$\n\\mathbb{E}[S_{T}] = \\mathbb{E}[\\lim_{n \\to \\infty} S_{T \\wedge n}] = \\lim_{n \\to \\infty} \\mathbb{E}[S_{T \\wedge n}].\n$$\nSince $\\mathbb{E}[S_{T \\wedge n}] = 0$ for all $n$, we conclude:\n$$\n\\mathbb{E}[S_{T}] = \\lim_{n \\to \\infty} 0 = 0.\n$$\nThus, $\\mathbb{E}[S_{T}] = \\mathbb{E}[S_{0}]$.\n\n### Part 4: Derive the distribution of $S_{T}$\nThe random variable $S_{T}$ takes values in the set $\\{-3, 4\\}$. Let $p = \\mathbb{P}(S_{T} = 4)$. Then, because $\\mathbb{P}(T  \\infty) = 1$, we have $\\mathbb{P}(S_{T} = -3) = 1-p$.\nThe expectation of $S_{T}$ can be calculated using its distribution:\n$$\n\\mathbb{E}[S_{T}] = (4) \\cdot \\mathbb{P}(S_{T} = 4) + (-3) \\cdot \\mathbb{P}(S_{T} = -3) = 4p - 3(1-p).\n$$\nFrom the previous step, we know $\\mathbb{E}[S_{T}] = 0$. So we set up the equation:\n$$\n4p - 3(1-p) = 0\n$$\n$$\n4p - 3 + 3p = 0\n$$\n$$\n7p = 3\n$$\n$$\np = \\frac{3}{7}.\n$$\nTherefore, $\\mathbb{P}(S_{T} = 4) = \\frac{3}{7}$, and $\\mathbb{P}(S_{T} = -3) = 1 - \\frac{3}{7} = \\frac{4}{7}$. This confirms the desired distribution.\n\n### Part 5: Compute $\\mathbb{E}[T]$\nFirst, we prove that $M_{n} = S_{n}^{2} - n$ is a martingale.\n1.  $M_{n}$ is $\\mathcal{F}_{n}$-measurable because $S_{n}$ is $\\mathcal{F}_{n}$-measurable and $n$ is a deterministic constant.\n2.  $\\mathbb{E}[|M_{n}|] = \\mathbb{E}[|S_{n}^{2} - n|] \\leq \\mathbb{E}[S_{n}^{2}] + n$. The variance of $X_{k}$ is $\\mathbb{E}[X_{k}^{2}] - (\\mathbb{E}[X_{k}])^{2} = 1 - 0^{2} = 1$. Since the $X_{k}$ are independent, $\\text{Var}(S_{n}) = \\sum_{k=1}^{n}\\text{Var}(X_{k}) = n$. Also $\\mathbb{E}[S_{n}]=0$. So, $\\mathbb{E}[S_{n}^{2}] = \\text{Var}(S_{n}) + (\\mathbb{E}[S_{n}])^{2} = n + 0 = n$. Thus, $\\mathbb{E}[|M_{n}|] \\leq n + n = 2n  \\infty$.\n3.  We check the martingale property: $\\mathbb{E}[M_{n} | \\mathcal{F}_{n-1}] = M_{n-1}$.\n$$\n\\mathbb{E}[M_{n} | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n}^{2} - n | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n}^{2} | \\mathcal{F}_{n-1}] - n.\n$$\nSubstitute $S_{n} = S_{n-1} + X_{n}$:\n$$\n\\mathbb{E}[S_{n}^{2} | \\mathcal{F}_{n-1}] = \\mathbb{E}[(S_{n-1} + X_{n})^{2} | \\mathcal{F}_{n-1}] = \\mathbb{E}[S_{n-1}^{2} + 2S_{n-1}X_{n} + X_{n}^{2} | \\mathcal{F}_{n-1}].\n$$\nUsing linearity and properties of conditional expectation:\n$$\n\\mathbb{E}[S_{n-1}^{2} | \\mathcal{F}_{n-1}] + 2\\mathbb{E}[S_{n-1}X_{n} | \\mathcal{F}_{n-1}] + \\mathbb{E}[X_{n}^{2} | \\mathcal{F}_{n-1}].\n$$\n- Since $S_{n-1}^{2}$ is $\\mathcal{F}_{n-1}$-measurable, $\\mathbb{E}[S_{n-1}^{2} | \\mathcal{F}_{n-1}] = S_{n-1}^{2}$.\n- Taking out what is known, $\\mathbb{E}[S_{n-1}X_{n} | \\mathcal{F}_{n-1}] = S_{n-1}\\mathbb{E}[X_{n} | \\mathcal{F}_{n-1}] = S_{n-1}\\mathbb{E}[X_{n}] = S_{n-1} \\cdot 0 = 0$.\n- Due to independence, $\\mathbb{E}[X_{n}^{2} | \\mathcal{F}_{n-1}] = \\mathbb{E}[X_{n}^{2}] = (1)^{2}\\frac{1}{2} + (-1)^{2}\\frac{1}{2} = 1$.\nCombining these results, we get $\\mathbb{E}[S_{n}^{2} | \\mathcal{F}_{n-1}] = S_{n-1}^{2} + 1$.\nSo, $\\mathbb{E}[M_{n} | \\mathcal{F}_{n-1}] = (S_{n-1}^{2} + 1) - n = S_{n-1}^{2} - (n-1) = M_{n-1}$.\nThus, $\\{M_{n}\\}$ is a martingale.\n\nNow, we apply the optional stopping theorem to $M_{n}$ and the bounded stopping time $T \\wedge n$.\n$$\n\\mathbb{E}[M_{T \\wedge n}] = \\mathbb{E}[M_{0}] = S_{0}^{2} - 0 = 0.\n$$\nThis means $\\mathbb{E}[S_{T \\wedge n}^{2} - (T \\wedge n)] = 0$, or $\\mathbb{E}[S_{T \\wedge n}^{2}] = \\mathbb{E}[T \\wedge n]$.\nWe need to take the limit as $n \\to \\infty$.\nFor the right-hand side, the sequence of random variables $\\{T \\wedge n\\}_{n \\geq 0}$ is non-negative and non-decreasing, converging point-wise to $T$. By the Monotone Convergence Theorem:\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[T \\wedge n] = \\mathbb{E}[\\lim_{n \\to \\infty} T \\wedge n] = \\mathbb{E}[T].\n$$\nFor the left-hand side, we have $S_{T \\wedge n}^{2} \\to S_{T}^{2}$ almost surely. As shown before, $|S_{T \\wedge n}| \\leq 4$, so $S_{T \\wedge n}^{2} \\leq 16$. Since the constant random variable $Y=16$ is integrable, we can use the Dominated Convergence Theorem:\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[S_{T \\wedge n}^{2}] = \\mathbb{E}[\\lim_{n \\to \\infty} S_{T \\wedge n}^{2}] = \\mathbb{E}[S_{T}^{2}].\n$$\nEquating the limits, we find $\\mathbb{E}[T] = \\mathbb{E}[S_{T}^{2}]$.\nWe can now compute $\\mathbb{E}[S_{T}^{2}]$ using the distribution of $S_{T}$ found in Part 4. The random variable $S_{T}^{2}$ takes the value $4^{2}=16$ with probability $\\frac{3}{7}$ and $(-3)^{2}=9$ with probability $\\frac{4}{7}$.\n$$\n\\mathbb{E}[S_{T}^{2}] = (16) \\cdot \\mathbb{P}(S_{T} = 4) + (9) \\cdot \\mathbb{P}(S_{T} = -3) = 16 \\cdot \\frac{3}{7} + 9 \\cdot \\frac{4}{7}.\n$$\n$$\n\\mathbb{E}[S_{T}^{2}] = \\frac{48}{7} + \\frac{36}{7} = \\frac{48+36}{7} = \\frac{84}{7} = 12.\n$$\nTherefore, $\\mathbb{E}[T] = 12$.", "answer": "$$\n\\boxed{12}\n$$", "id": "3049332"}]}