{"hands_on_practices": [{"introduction": "The first step in mastering any quantitative concept is to apply its definition to a concrete example. This practice provides a clear, foundational exercise in calculating covariance for discrete random variables [@problem_id:1614699]. By working directly with a given joint probability mass function, you will solidify your understanding of the core formula, $\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$, and the mechanics of computing marginal distributions and expectations.", "problem": "An engineer is analyzing the performance of a simple binary classification model for detecting faulty components on an assembly line. Let $X$ be a discrete random variable representing the true state of a component, where $X=1$ if the component is faulty and $X=0$ if it is not. Let $Y$ be a discrete random variable representing the model's prediction, where $Y=1$ if the model predicts the component is faulty and $Y=0$ if it predicts it is not.\n\nAfter observing a large number of components, the engineer has determined the joint probability mass function, $P(X=x, Y=y)$, for these two variables. The probabilities are as follows:\n\n-   $P(X=0, Y=0) = 0.65$\n-   $P(X=0, Y=1) = 0.05$\n-   $P(X=1, Y=0) = 0.10$\n-   $P(X=1, Y=1) = 0.20$\n\nCalculate the covariance, $\\text{Cov}(X, Y)$, between the true state of a component and the model's prediction.", "solution": "We use the definition of covariance for discrete random variables:\n$$\n\\text{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y].\n$$\nFrom the given joint pmf, convert the decimals to exact fractions:\n$$\nP(X=0,Y=0)=0.65=\\frac{13}{20},\\quad P(X=0,Y=1)=0.05=\\frac{1}{20},\\quad P(X=1,Y=0)=0.10=\\frac{1}{10},\\quad P(X=1,Y=1)=0.20=\\frac{1}{5}.\n$$\n\nFirst compute the marginal distributions. For $X$:\n$$\nP(X=1)=P(1,0)+P(1,1)=\\frac{1}{10}+\\frac{1}{5}=\\frac{3}{10},\\quad P(X=0)=1-P(X=1)=1-\\frac{3}{10}=\\frac{7}{10}.\n$$\nFor $Y$:\n$$\nP(Y=1)=P(0,1)+P(1,1)=\\frac{1}{20}+\\frac{1}{5}=\\frac{1}{4},\\quad P(Y=0)=1-P(Y=1)=1-\\frac{1}{4}=\\frac{3}{4}.\n$$\n\nNow compute the expectations. Since $X,Y\\in\\{0,1\\}$, $\\mathbb{E}[X]=P(X=1)$ and $\\mathbb{E}[Y]=P(Y=1)$:\n$$\n\\mathbb{E}[X]=\\frac{3}{10},\\qquad \\mathbb{E}[Y]=\\frac{1}{4}.\n$$\nCompute $\\mathbb{E}[XY]$ via the joint pmf:\n$$\n\\mathbb{E}[XY]=\\sum_{x\\in\\{0,1\\}}\\sum_{y\\in\\{0,1\\}}xy\\,P(X=x,Y=y)=1\\cdot 1\\cdot P(1,1)=\\frac{1}{5}.\n$$\n\nTherefore,\n$$\n\\text{Cov}(X,Y)=\\frac{1}{5}-\\left(\\frac{3}{10}\\right)\\left(\\frac{1}{4}\\right)=\\frac{1}{5}-\\frac{3}{40}=\\frac{8}{40}-\\frac{3}{40}=\\frac{5}{40}=\\frac{1}{8}.\n$$", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "1614699"}, {"introduction": "Covariance is a powerful tool for measuring the linear relationship between two variables, but it is crucial to understand its limitations. This thought-provoking problem explores a scenario where two random variables are clearly dependent, yet their covariance is zero [@problem_id:1614658]. By analyzing the effect of symmetry in a variable's distribution, you will gain a deeper insight into the fact that zero covariance does not imply independence, a common misconception that can lead to flawed conclusions.", "problem": "Consider a continuous random variable $X$ whose behavior is described by a probability density function (PDF), $f_X(u)$. The function $f_X(u)$ is known to be an even function, which means it satisfies the symmetry property $f_X(u) = f_X(-u)$ for all real numbers $u$. Let a second random variable $Y$ be defined as the absolute value of $X$, such that $Y = |X|$. You are given that all expected values required for the calculation of the covariance between $X$ and $Y$ are well-defined and finite.\n\nCalculate the covariance, $\\text{Cov}(X, Y)$. Express your final answer as a single real number.", "solution": "By definition, the covariance between $X$ and $Y$ is\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{E}[XY]-\\operatorname{E}[X]\\operatorname{E}[Y].\n$$\nSince $f_{X}(u)$ is even, the distribution of $X$ is symmetric about $0$. Therefore,\n$$\n\\operatorname{E}[X]=\\int_{-\\infty}^{\\infty} u\\,f_{X}(u)\\,du=0,\n$$\nbecause the integrand $u\\,f_{X}(u)$ is an odd function (product of odd $u$ and even $f_{X}(u)$), and the integral over a symmetric domain is zero.\n\nNext, with $Y=|X|$ we have $XY=X|X|$. Hence\n$$\n\\operatorname{E}[XY]=\\operatorname{E}[X|X|]=\\int_{-\\infty}^{\\infty} u|u|\\,f_{X}(u)\\,du.\n$$\nThe function $u|u|$ is odd, and $f_{X}(u)$ is even, so the integrand $u|u|\\,f_{X}(u)$ is odd. Therefore,\n$$\n\\int_{-\\infty}^{\\infty} u|u|\\,f_{X}(u)\\,du=0,\n$$\nagain by symmetry.\n\nPutting these together,\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{E}[XY]-\\operatorname{E}[X]\\operatorname{E}[Y]=0-0\\cdot \\operatorname{E}[|X|]=0.\n$$\nAll required expectations are finite by assumption, so the calculations are valid.", "answer": "$$\\boxed{0}$$", "id": "1614658"}, {"introduction": "We now extend the concept of covariance from static random variables to the dynamic realm of stochastic processes, a cornerstone of financial modeling and physics. This advanced practice introduces quadratic covariation, which describes how two processes, like the components of a multi-dimensional Brownian motion, vary together over time. Deriving the relationship between the quadratic covariation $[W^{i}, W^{j}]_{t}$ and the covariance matrix $\\Sigma$ is a fundamental skill for applying It√¥'s lemma and understanding correlated diffusions [@problem_id:3046943].", "problem": "Let $\\{B^{1}_{t},\\dots,B^{d}_{t}\\}_{t\\ge 0}$ be a $d$-dimensional standard Brownian motion with independent components, meaning each $B^{k}_{t}$ is a one-dimensional Brownian motion and for $k\\neq \\ell$ the processes $B^{k}$ and $B^{\\ell}$ are independent. Let $\\Sigma\\in\\mathbb{R}^{d\\times d}$ be a fixed symmetric positive semidefinite matrix. Consider the correlated $d$-dimensional Brownian motion $\\{W_{t}\\}_{t\\ge 0}$ defined by $W_{t}=C\\,B_{t}$, where $B_{t}=(B^{1}_{t},\\dots,B^{d}_{t})^{\\top}$ and $C\\in\\mathbb{R}^{d\\times d}$ is a deterministic matrix satisfying $C\\,C^{\\top}=\\Sigma$. In particular, the $i$-th component is $W^{i}_{t}=\\sum_{k=1}^{d}C_{ik}\\,B^{k}_{t}$.\n\nStarting from the definition of quadratic covariation for continuous semimartingales,\n$$[X,Y]_{t}=\\lim_{\\|\\mathcal{P}\\|\\to 0}\\sum_{m}(X_{t_{m+1}}-X_{t_{m}})(Y_{t_{m+1}}-Y_{t_{m}}),$$\nwhere the limit is in probability over partitions $\\mathcal{P}:\\,0=t_{0}t_{1}\\dotst_{n}=t$ with mesh $\\|\\mathcal{P}\\|=\\max_{m}(t_{m+1}-t_{m})$, and using only well-tested properties of standard Brownian motion and linear operations, derive an explicit expression for the quadratic covariation $[W^{i},W^{j}]_{t}$ in terms of $\\Sigma$ and $t$. Express your final answer as a closed-form analytic expression. No numerical approximation or rounding is required.", "solution": "We begin with the construction $W_{t}=C\\,B_{t}$, where $B_{t}=(B^{1}_{t},\\dots,B^{d}_{t})^{\\top}$ is a standard $d$-dimensional Brownian motion with independent components and $C\\in\\mathbb{R}^{d\\times d}$ is deterministic with $C\\,C^{\\top}=\\Sigma$. The $i$-th component is\n$$\nW^{i}_{t}=\\sum_{k=1}^{d}C_{ik}\\,B^{k}_{t}.\n$$\nThe quadratic covariation of two continuous semimartingales $X$ and $Y$ up to time $t$ is defined by\n$$\n[X,Y]_{t}=\\lim_{\\|\\mathcal{P}\\|\\to 0}\\sum_{m=0}^{n-1}\\big(X_{t_{m+1}}-X_{t_{m}}\\big)\\big(Y_{t_{m+1}}-Y_{t_{m}}\\big),\n$$\nwhere the limit is taken in probability as the mesh of the partition $\\mathcal{P}$ goes to zero. For Brownian motions, a well-tested property is that for standard independent components $B^{k}$ and $B^{\\ell}$,\n$$\n[B^{k},B^{\\ell}]_{t}=\\begin{cases}\nt,  k=\\ell,\\\\\n0,  k\\neq \\ell.\n\\end{cases}\n$$\nEquivalently, $[B^{k},B^{\\ell}]_{t}=\\delta_{k\\ell}\\,t$, where $\\delta_{k\\ell}$ is the Kronecker delta.\n\nWe compute $[W^{i},W^{j}]_{t}$ using the definition and linearity. For a partition $\\mathcal{P}:\\,0=t_{0}\\dotst_{n}=t$, set $\\Delta B^{k}_{m}=B^{k}_{t_{m+1}}-B^{k}_{t_{m}}$ and $\\Delta W^{i}_{m}=W^{i}_{t_{m+1}}-W^{i}_{t_{m}}$. Then\n$$\n\\Delta W^{i}_{m}=\\sum_{k=1}^{d}C_{ik}\\,\\Delta B^{k}_{m},\\qquad \\Delta W^{j}_{m}=\\sum_{\\ell=1}^{d}C_{j\\ell}\\,\\Delta B^{\\ell}_{m}.\n$$\nHence\n\\begin{align*}\n\\sum_{m=0}^{n-1}\\Delta W^{i}_{m}\\,\\Delta W^{j}_{m}\n=\\sum_{m=0}^{n-1}\\left(\\sum_{k=1}^{d}C_{ik}\\,\\Delta B^{k}_{m}\\right)\\left(\\sum_{\\ell=1}^{d}C_{j\\ell}\\,\\Delta B^{\\ell}_{m}\\right)\\\\\n=\\sum_{k=1}^{d}\\sum_{\\ell=1}^{d}C_{ik}C_{j\\ell}\\left(\\sum_{m=0}^{n-1}\\Delta B^{k}_{m}\\,\\Delta B^{\\ell}_{m}\\right).\n\\end{align*}\nTaking the limit in probability as $\\|\\mathcal{P}\\|\\to 0$, we use the well-tested property of standard Brownian motion increments to identify\n$$\n\\lim_{\\|\\mathcal{P}\\|\\to 0}\\sum_{m=0}^{n-1}\\Delta B^{k}_{m}\\,\\Delta B^{\\ell}_{m}=[B^{k},B^{\\ell}]_{t}=\\delta_{k\\ell}\\,t.\n$$\nTherefore,\n\\begin{align*}\n[W^{i},W^{j}]_{t}\n=\\sum_{k=1}^{d}\\sum_{\\ell=1}^{d}C_{ik}C_{j\\ell}\\,[B^{k},B^{\\ell}]_{t}\\\\\n=\\sum_{k=1}^{d}\\sum_{\\ell=1}^{d}C_{ik}C_{j\\ell}\\,\\delta_{k\\ell}\\,t\\\\\n=\\sum_{k=1}^{d}C_{ik}C_{jk}\\,t.\n\\end{align*}\nRecognizing the matrix product, the $(i,j)$ entry of $C\\,C^{\\top}$ is $(C\\,C^{\\top})_{ij}=\\sum_{k=1}^{d}C_{ik}C_{jk}$. By the assumption $C\\,C^{\\top}=\\Sigma$, we conclude\n$$\n[W^{i},W^{j}]_{t}=(C\\,C^{\\top})_{ij}\\,t=\\Sigma_{ij}\\,t.\n$$\nThis provides the desired closed-form expression for the quadratic covariation in terms of the covariance matrix $\\Sigma$ and time $t$.", "answer": "$$\\boxed{\\Sigma_{ij}\\,t}$$", "id": "3046943"}]}