{"hands_on_practices": [{"introduction": "To simulate stochastic processes with jumps, we first need a method to generate the jumps themselves. This exercise introduces a classic and elegant algorithm for sampling from a Poisson distribution using only simple uniform random numbers. By working through its justification, you will uncover the profound relationship between the Poisson, Exponential, and Uniform distributions, a cornerstone of stochastic simulation [@problem_id:3044284].", "problem": "A counting process with independent and stationary increments and with exponentially distributed inter-arrival times of unit rate is called a rate-one Poisson process. Let $X$ be a Poisson random variable with parameter $\\lambda>0$, denoted $X \\sim \\mathrm{Pois}(\\lambda)$, which has probability mass function $p(k)=\\exp(-\\lambda)\\lambda^{k}/k!$ for $k \\in \\{0,1,2,\\dots\\}$. The inverse transform sampling method for a discrete distribution with cumulative distribution function $F(k)=\\sum_{j=0}^{k}p(j)$ generates a sample by drawing one $U$ from the continuous Uniform on $(0,1)$ distribution (Uniform$(0,1)$), and returning the smallest integer $k$ such that $F(k)\\geq U$. An equivalent sequential-search inversion specialized to the Poisson distribution is the following procedure: draw independent $U_{1},U_{2},\\dots$ from Uniform$(0,1)$; form the running product $P_{n}=\\prod_{i=1}^{n}U_{i}$; stop at the first index $K$ such that $P_{K}\\leq \\exp(-\\lambda)$ and output $K-1$.\n\nStarting from fundamental definitions and facts about the Poisson process and the exponential distribution, do the following:\n\n- Justify that the sequential-search procedure described above indeed samples from $\\mathrm{Pois}(\\lambda)$.\n- Let $M_{\\lambda}$ denote the total number of independent Uniform$(0,1)$ draws consumed by the sequential-search procedure until it stops. Derive an exact closed-form expression for $\\mathbb{E}[M_{\\lambda}]$ as a function of $\\lambda$.\n- Using your exact expression, identify the leading-order behavior of $\\mathbb{E}[M_{\\lambda}]$ as $\\lambda \\to 0^{+}$ and as $\\lambda \\to \\infty$.\n\nYour final answer must be a single closed-form analytic expression for $\\mathbb{E}[M_{\\lambda}]$ in terms of $\\lambda$ (no units). Do not provide an inequality or an equation. Do not round; no numerical approximation is required.", "solution": "The problem is assessed to be valid. It is scientifically grounded in probability theory, well-posed with a clear objective, and all provided information is self-contained and consistent. We can therefore proceed with a full solution.\n\nThe problem asks for three distinct items: a justification of the sampling procedure, the derivation of an expectation, and an asymptotic analysis. We will address each in turn.\n\n### Justification of the Sampling Procedure\n\nThe algorithm generates a random integer $K-1$, where $K$ is the first index for which the product of $K$ independent $\\text{Uniform}(0,1)$ random variables falls below a threshold. Let the sequence of uniform random variables be $U_1, U_2, \\dots$, where each $U_i \\sim \\text{Uniform}(0,1)$ independently. The running product is $P_n = \\prod_{i=1}^{n} U_i$. The stopping time $K$ is defined as the smallest integer such that $P_K \\leq \\exp(-\\lambda)$. The output of the algorithm is the random variable $X = K-1$. We must show that $X \\sim \\mathrm{Pois}(\\lambda)$.\n\nThe condition $P_K \\leq \\exp(-\\lambda)$ can be transformed by taking the natural logarithm of both sides, which is a monotonic function.\n$$ \\ln\\left(\\prod_{i=1}^{K} U_i\\right) \\leq \\ln(\\exp(-\\lambda)) $$\n$$ \\sum_{i=1}^{K} \\ln(U_i) \\leq -\\lambda $$\nMultiplying by $-1$ reverses the inequality:\n$$ \\sum_{i=1}^{K} (-\\ln(U_i)) \\geq \\lambda $$\nLet us define a new set of random variables $E_i = -\\ln(U_i)$ for $i=1, 2, \\dots$. To find the distribution of $E_i$, we can compute its cumulative distribution function (CDF) for $x>0$:\n$$ F_{E_i}(x) = \\mathbb{P}(E_i \\leq x) = \\mathbb{P}(-\\ln(U_i) \\leq x) = \\mathbb{P}(\\ln(U_i) \\geq -x) = \\mathbb{P}(U_i \\geq \\exp(-x)) $$\nSince $U_i$ is a continuous random variable uniformly distributed on $(0,1)$, its CDF is $F_{U_i}(u)=u$ for $u \\in [0,1]$. Thus, $\\mathbb{P}(U_i \\geq y) = 1 - \\mathbb{P}(U_i  y) = 1 - y$ for $y \\in (0,1)$. For $x>0$, $\\exp(-x) \\in (0,1)$, so we have:\n$$ F_{E_i}(x) = 1 - \\exp(-x) $$\nThis is the CDF of an exponential distribution with rate parameter $1$. Therefore, the random variables $E_i$ are independent and identically distributed according to the $\\text{Exp}(1)$ distribution.\n\nLet $S_n = \\sum_{i=1}^{n} E_i$. The stopping condition for the algorithm can now be rephrased in terms of this sum. The algorithm stops at the first integer $K$ such that $S_K \\geq \\lambda$. The output is $X=K-1$.\nThe event $\\{X=k\\}$ for $k \\in \\{0, 1, 2, \\dots\\}$ is equivalent to the event $\\{K-1=k\\}$, or $\\{K=k+1\\}$. This means the algorithm stops at step $k+1$. This occurs if the stopping condition is not met at steps $1, 2, \\dots, k$, but is met at step $k+1$.\n$$ \\{K=k+1\\} = \\{S_1  \\lambda, S_2  \\lambda, \\dots, S_k  \\lambda, \\text{ and } S_{k+1} \\geq \\lambda\\} $$\nSince $E_i = -\\ln(U_i) > 0$ almost surely (as $\\mathbb{P}(U_i=1)=0$), the sequence of partial sums $S_n$ is strictly increasing. Thus, the conditions $S_1  \\lambda, \\dots, S_k  \\lambda$ simplify to the single condition $S_k  \\lambda$. The event becomes:\n$$ \\{K=k+1\\} = \\{S_k  \\lambda \\text{ and } S_{k+1} \\geq \\lambda\\} $$\nThe sum $S_n$ of $n$ i.i.d. $\\text{Exp}(1)$ random variables represents the time of the $n$-th arrival in a homogeneous Poisson process with a rate of $1$. Let $\\{N(t), t \\geq 0\\}$ be such a process. The random variable $N(t)$ counts the number of arrivals in the interval $[0, t]$ and follows a Poisson distribution with parameter $t$.\nThe event $\\{N(\\lambda) = k\\}$ signifies that there are exactly $k$ arrivals in the interval $[0, \\lambda]$. This means that the time of the $k$-th arrival, $S_k$, occurs before or at time $\\lambda$, and the time of the $(k+1)$-th arrival, $S_{k+1}$, occurs after time $\\lambda$. Formally, $\\{N(\\lambda) = k\\} = \\{S_k \\leq \\lambda  S_{k+1}\\}$.\nSince $S_k$ is a continuous random variable, $\\mathbb{P}(S_k = \\lambda) = 0$. Therefore, the event $\\{S_k  \\lambda \\text{ and } S_{k+1} \\geq \\lambda\\}$ is probabilistically equivalent to $\\{S_k \\leq \\lambda  S_{k+1}\\}$.\nThus, we have established the equivalence of the events:\n$$ \\mathbb{P}(X=k) = \\mathbb{P}(K=k+1) = \\mathbb{P}(S_k  \\lambda \\text{ and } S_{k+1} \\geq \\lambda) = \\mathbb{P}(N(\\lambda)=k) $$\nSince $N(\\lambda)$ follows a Poisson distribution with parameter $\\lambda$, its probability mass function is $\\mathbb{P}(N(\\lambda)=k) = \\frac{\\exp(-\\lambda)\\lambda^k}{k!}$.\nThis confirms that the random variable $X=K-1$ generated by the sequential-search procedure is indeed a sample from the $\\mathrm{Pois}(\\lambda)$ distribution.\n\n### Derivation of $\\mathbb{E}[M_{\\lambda}]$\n\nThe total number of uniform draws consumed is $M_{\\lambda} = K$. We seek to find the expected value of $K$, $\\mathbb{E}[K]$. For a random variable $K$ taking values in the positive integers $\\{1, 2, 3, \\dots\\}$, its expectation can be calculated as:\n$$ \\mathbb{E}[K] = \\sum_{j=1}^{\\infty} \\mathbb{P}(K \\geq j) $$\nThe event $\\{K \\geq j\\}$ means that the algorithm has not stopped by step $j-1$. This is equivalent to the condition $S_{j-1}  \\lambda$. For $j=1$, $S_0=0$, and $\\mathbb{P}(S_0\\lambda)=\\mathbb{P}(0\\lambda)=1$ since $\\lambda>0$.\nSo, we can write the expectation as:\n$$ \\mathbb{E}[K] = \\sum_{j=1}^{\\infty} \\mathbb{P}(S_{j-1}  \\lambda) $$\nBy changing the index of summation with $m = j-1$, we get:\n$$ \\mathbb{E}[K] = \\sum_{m=0}^{\\infty} \\mathbb{P}(S_m  \\lambda) $$\nThere is a fundamental identity that connects the CDF of a Gamma-distributed random variable ($S_m \\sim \\text{Gamma}(m,1)$) with the CDF of a Poisson-distributed random variable. The event $\\{S_m  \\lambda\\}$ (at least $m$ events needed to exceed time $\\lambda$) is equivalent to $\\{N(\\lambda) \\geq m\\}$ (at least $m$ events occurred by time $\\lambda$).\n$$ \\mathbb{P}(S_m  \\lambda) = \\mathbb{P}(N(\\lambda) \\geq m) $$\nSubstituting this into the expression for $\\mathbb{E}[K]$:\n$$ \\mathbb{E}[K] = \\sum_{m=0}^{\\infty} \\mathbb{P}(N(\\lambda) \\geq m) $$\nLet $Y$ be a non-negative integer-valued random variable. A standard formula for its expectation is $\\mathbb{E}[Y] = \\sum_{m=1}^{\\infty} \\mathbb{P}(Y \\geq m)$. We have a sum from $m=0$.\nNotice that $\\sum_{m=0}^{\\infty} \\mathbb{P}(Y \\geq m) = \\mathbb{P}(Y \\geq 0) + \\sum_{m=1}^{\\infty} \\mathbb{P}(Y \\geq m)$. Since $Y$ is non-negative, $\\mathbb{P}(Y \\geq 0) = 1$. Thus, $\\sum_{m=0}^{\\infty} \\mathbb{P}(Y \\geq m) = 1 + \\mathbb{E}[Y] = \\mathbb{E}[Y+1]$.\nApplying this to our case with $Y = N(\\lambda) \\sim \\mathrm{Pois}(\\lambda)$:\n$$ \\mathbb{E}[K] = \\sum_{m=0}^{\\infty} \\mathbb{P}(N(\\lambda) \\geq m) = \\mathbb{E}[N(\\lambda) + 1] = \\mathbb{E}[N(\\lambda)] + 1 $$\nThe expectation of a Poisson random variable with parameter $\\lambda$ is $\\lambda$.\n$$ \\mathbb{E}[N(\\lambda)] = \\lambda $$\nTherefore, the expected number of uniform draws is:\n$$ \\mathbb{E}[M_{\\lambda}] = \\mathbb{E}[K] = \\lambda + 1 $$\n\n### Asymptotic Behavior of $\\mathbb{E}[M_{\\lambda}]$\n\nThe exact closed-form expression for the expected number of draws is $\\mathbb{E}[M_{\\lambda}] = \\lambda + 1$. We can now analyze its behavior in the specified limits.\n\n1.  As $\\lambda \\to 0^{+}$:\n    The expression $\\lambda + 1$ is a simple linear function.\n    $$ \\lim_{\\lambda \\to 0^{+}} \\mathbb{E}[M_{\\lambda}] = \\lim_{\\lambda \\to 0^{+}} (\\lambda + 1) = 1 $$\n    The leading-order behavior is $1$. This is intuitively correct, as for very small $\\lambda$, the threshold $\\exp(-\\lambda)$ is close to $1$, so the first draw $U_1$ is highly likely to be smaller than the threshold, causing the algorithm to stop immediately with $K=1$.\n\n2.  As $\\lambda \\to \\infty$:\n    We examine the behavior for large $\\lambda$.\n    $$ \\mathbb{E}[M_{\\lambda}] = \\lambda + 1 $$\n    As $\\lambda$ becomes large, the constant term $1$ becomes negligible compared to $\\lambda$. The leading-order behavior is therefore $\\lambda$.\n    $$ \\mathbb{E}[M_{\\lambda}] \\sim \\lambda \\quad \\text{as} \\quad \\lambda \\to \\infty $$\n    This is consistent with the Elementary Renewal Theorem, which would predict $\\mathbb{E}[K] / \\lambda \\to 1/\\mathbb{E}[E_i] = 1$. Our exact result shows the convergence is of the form $(\\lambda+1)/\\lambda \\to 1$.\n\nThe problem requires a single closed-form expression for $\\mathbb{E}[M_{\\lambda}]$ as the final answer.", "answer": "$$\n\\boxed{\\lambda+1}\n$$", "id": "3044284"}, {"introduction": "Once we can generate jumps, a practical challenge in simulating jump-diffusion SDEs is choosing the time step for our numerical scheme. If the step size $h$ is too large, we might miss important dynamics or the simulation could become unstable. This practice demonstrates how to use the properties of the Poisson distribution to design an adaptive step-size controller, ensuring our simulation remains both efficient and accurate [@problem_id:3044306].", "problem": "Consider a one-dimensional Stochastic Differential Equation (SDE) with a pure-jump component whose jump intensity is state dependent, denoted by $\\lambda(x) \\ge 0$. You are implementing a jump-adapted Euler scheme over a single step $[t, t + h]$. Let $N_h$ denote the number of jumps that occur in this interval. Suppose that, based on local information at time $t$, you have computed an upper bound $\\overline{\\lambda} > 0$ that satisfies $\\lambda(X_s) \\le \\overline{\\lambda}$ for all $s \\in [t, t+h]$ on the step you contemplate taking. Your goal is to choose an adaptive step size $h$ so that the probability of observing more than one jump in the step is controlled by a user-specified tolerance $\\varepsilon \\in (0,1)$.\n\nStarting from first principles for counting processes and classical discrete distributions, use the following foundational facts:\n\n- If the jump intensity is bounded above by a constant $\\overline{\\lambda}$ on an interval of length $h$, then the jump count on that interval is stochastically dominated by a Poisson random variable with mean $\\mu = \\overline{\\lambda} h$.\n\n- Using only the definition of the Poisson distribution and standard inequalities for the exponential function, derive a bound for $\\mathbb{P}(N_h \\ge 2)$ in terms of $\\mu$ that holds for all $\\mu \\ge 0$.\n\nUse this bound to produce a conservative adaptive step-size prescription $h_{\\max}(\\varepsilon, \\overline{\\lambda})$ that guarantees $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$.\n\nProvide your final answer as a single closed-form symbolic expression for $h_{\\max}(\\varepsilon, \\overline{\\lambda})$ in terms of $\\varepsilon$ and $\\overline{\\lambda}$. Do not perform any numerical rounding. Express your final answer symbolically; no physical units are required in the final expression.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on standard principles of stochastic calculus and numerical analysis for stochastic differential equations. The premises are mathematically sound, and the objective is clearly defined with sufficient information for a unique solution to be derived. Therefore, the problem is deemed valid. We may proceed with the solution.\n\nThe objective is to derive a conservative adaptive step-size prescription, denoted $h_{\\max}(\\varepsilon, \\overline{\\lambda})$, which guarantees that the probability of observing two or more jumps in a time interval of length $h$ is no greater than a specified tolerance $\\varepsilon$. The condition to be satisfied is $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$, where $N_h$ is the number of jumps in the interval $[t, t+h]$.\n\nThe problem provides a foundational fact: if the state-dependent jump intensity $\\lambda(X_s)$ is bounded above by a constant $\\overline{\\lambda}$ for $s \\in [t, t+h]$, then the jump count $N_h$ in this interval is stochastically dominated by a Poisson random variable, which we shall call $M$, with mean parameter $\\mu = \\overline{\\lambda} h$. Stochastic dominance implies that for any integer $k \\ge 0$, the cumulative distribution functions satisfy $\\mathbb{P}(N_h \\le k) \\ge \\mathbb{P}(M \\le k)$, which is equivalent to $\\mathbb{P}(N_h  k) \\le \\mathbb{P}(M  k)$ or $\\mathbb{P}(N_h \\ge k) \\le \\mathbb{P}(M \\ge k)$.\n\nTo satisfy the target condition $\\mathbb{P}(N_h \\ge 2) \\le \\varepsilon$, it is therefore sufficient to enforce a more stringent condition on the dominating variable $M$:\n$$\n\\mathbb{P}(M \\ge 2) \\le \\varepsilon\n$$\n\nNext, we must derive a usable bound for $\\mathbb{P}(M \\ge 2)$ in terms of $\\mu$. The probability mass function of a Poisson random variable $M$ with mean $\\mu$ is given by $P(M=k) = \\frac{e^{-\\mu} \\mu^k}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$. The probability of observing two or more events is the complement of observing zero or one event:\n$$\n\\mathbb{P}(M \\ge 2) = 1 - \\mathbb{P}(M  2) = 1 - (\\mathbb{P}(M=0) + \\mathbb{P}(M=1))\n$$\nSubstituting the Poisson probabilities:\n$$\n\\mathbb{P}(M \\ge 2) = 1 - \\left( \\frac{e^{-\\mu} \\mu^0}{0!} + \\frac{e^{-\\mu} \\mu^1}{1!} \\right) = 1 - (e^{-\\mu} + \\mu e^{-\\mu}) = 1 - e^{-\\mu}(1+\\mu)\n$$\nThis expression is exact. However, the problem directs us to derive a simpler bound. We can express $\\mathbb{P}(M \\ge 2)$ as an infinite series:\n$$\n\\mathbb{P}(M \\ge 2) = \\sum_{k=2}^{\\infty} \\frac{e^{-\\mu} \\mu^k}{k!} = e^{-\\mu} \\left( \\frac{\\mu^2}{2!} + \\frac{\\mu^3}{3!} + \\frac{\\mu^4}{4!} + \\dots \\right)\n$$\nWe can establish an upper bound for this series. Factoring out the term $\\frac{\\mu^2}{2}$:\n$$\n\\mathbb{P}(M \\ge 2) = e^{-\\mu} \\frac{\\mu^2}{2} \\left( 1 + \\frac{\\mu}{3} + \\frac{\\mu^2}{3 \\cdot 4} + \\frac{\\mu^3}{3 \\cdot 4 \\cdot 5} + \\dots \\right) = e^{-\\mu} \\frac{\\mu^2}{2} \\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{(j+2)!}\n$$\nFor any integer $j \\ge 0$, we have the inequality $(j+2)! = (j+2)(j+1)j! \\ge 2 \\cdot j!$. Therefore, $\\frac{1}{(j+2)!} \\le \\frac{1}{2 \\cdot j!}$. Using this, we can bound the sum:\n$$\n\\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{(j+2)!} \\le \\sum_{j=0}^{\\infty} \\frac{2 \\mu^j}{2 j!} = \\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!}\n$$\nThe resulting sum on the right-hand side is the Taylor series expansion for the exponential function, $\\sum_{j=0}^{\\infty} \\frac{\\mu^j}{j!} = e^{\\mu}$.\nSubstituting this inequality back into our expression for $\\mathbb{P}(M \\ge 2)$, we obtain the bound:\n$$\n\\mathbb{P}(M \\ge 2) \\le e^{-\\mu} \\frac{\\mu^2}{2} (e^{\\mu}) = \\frac{\\mu^2}{2}\n$$\nThis inequality, $\\mathbb{P}(M \\ge 2) \\le \\frac{\\mu^2}{2}$, holds for all $\\mu \\ge 0$. To formally verify this, let $f(\\mu) = \\frac{\\mu^2}{2} - \\mathbb{P}(M \\ge 2) = \\frac{\\mu^2}{2} - (1 - (1+\\mu)e^{-\\mu})$. At $\\mu=0$, $f(0) = 0 - (1 - (1)e^0) = 0$. The derivative with respect to $\\mu$ is:\n$$\nf'(\\mu) = \\frac{d}{d\\mu} \\left( \\frac{\\mu^2}{2} - 1 + e^{-\\mu} + \\mu e^{-\\mu} \\right) = \\mu - e^{-\\mu} + (e^{-\\mu} - \\mu e^{-\\mu}) = \\mu - \\mu e^{-\\mu} = \\mu(1 - e^{-\\mu})\n$$\nFor all $\\mu  0$, we have $\\mu  0$ and $e^{-\\mu}  1$, which implies $(1 - e^{-\\mu})  0$. Thus, $f'(\\mu)  0$ for $\\mu  0$. Since $f(0)=0$ and the function is monotonically increasing for $\\mu \\ge 0$, we conclude that $f(\\mu) \\ge 0$ for all $\\mu \\ge 0$, confirming that $\\mathbb{P}(M \\ge 2) \\le \\frac{\\mu^2}{2}$ is a valid upper bound for all non-negative $\\mu$.\n\nWith this conservative bound, we can satisfy the control objective $\\mathbb{P}(M \\ge 2) \\le \\varepsilon$ by enforcing the stricter condition:\n$$\n\\frac{\\mu^2}{2} \\le \\varepsilon\n$$\nNow, substitute the definition of the mean, $\\mu = \\overline{\\lambda} h$:\n$$\n\\frac{(\\overline{\\lambda} h)^2}{2} \\le \\varepsilon\n$$\nWe solve this inequality for the step size $h$. Given that $\\overline{\\lambda}  0$ and $h \\ge 0$:\n$$\n(\\overline{\\lambda} h)^2 \\le 2\\varepsilon\n$$\n$$\n\\overline{\\lambda} h \\le \\sqrt{2\\varepsilon}\n$$\n$$\nh \\le \\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}\n$$\nThis inequality provides an upper limit on the step size $h$ that guarantees our probabilistic constraint is met. The problem asks for the maximum such step size, $h_{\\max}(\\varepsilon, \\overline{\\lambda})$, which is the upper boundary of this interval.\n$$\nh_{\\max}(\\varepsilon, \\overline{\\lambda}) = \\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}\n$$\nThis is the desired conservative adaptive step-size prescription.", "answer": "$$\n\\boxed{\\frac{\\sqrt{2\\varepsilon}}{\\overline{\\lambda}}}\n$$", "id": "3044306"}, {"introduction": "In many scientific and financial applications, the parameters of our stochastic models are not known in advance and must be estimated from data. This exercise tackles the fundamental problem of inferring the jump rate $\\lambda$ of a Poisson process from a series of observations. You will apply the powerful principles of Maximum Likelihood Estimation and the Cramér–Rao Lower Bound to find an optimal estimator and understand the theoretical limits of its precision [@problem_id:3044313].", "problem": "A homogeneous Poisson process with rate parameter $\\lambda \\in (0,\\infty)$ models the count of jump events driving a jump-diffusion component within a stochastic differential equation over disjoint unit-length time windows. You observe $n$ independent counts $X_{1}, X_{2}, \\dots, X_{n}$, each corresponding to the number of jumps in a single unit-length window. Assume $X_{i} \\sim \\text{Poisson}(\\lambda)$ independently for $i \\in \\{1,2,\\dots,n\\}$.\n\nStarting from core definitions in statistical estimation, including the likelihood function for the Poisson model, the score function as the derivative of the log-likelihood, and the Fisher information as the expected negative second derivative of the log-likelihood, derive the Cramér–Rao Lower Bound (CRLB) for the variance of any unbiased estimator of $\\lambda$ based on $X_{1},\\dots,X_{n}$. Then derive the maximum likelihood estimator (MLE) for $\\lambda$, determine its expectation and variance, and assess whether it attains the CRLB.\n\nYour final answer must be a single closed-form analytic expression for the CRLB as a function of $\\lambda$ and $n$. No rounding is required.", "solution": "The problem requires the derivation of the Cramér–Rao Lower Bound (CRLB) for the variance of any unbiased estimator of the rate parameter $\\lambda$ of a Poisson distribution, given a sample of $n$ independent and identically distributed (i.i.d.) observations. It also asks for the derivation of the Maximum Likelihood Estimator (MLE) of $\\lambda$ and an assessment of its efficiency.\n\nLet $X_{1}, X_{2}, \\dots, X_{n}$ be a set of $n$ independent random variables, where each $X_{i}$ follows a Poisson distribution with parameter $\\lambda  0$, denoted as $X_{i} \\sim \\text{Poisson}(\\lambda)$. The probability mass function (PMF) for a single observation $X_{i}$ is given by:\n$$P(X_{i} = k) = \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!}$$\nfor $k \\in \\{0, 1, 2, \\dots\\}$.\n\nFirst, we construct the likelihood function, $L(\\lambda)$, which is the joint probability of observing the sample $x_{1}, x_{2}, \\dots, x_{n}$. Due to the independence of the observations, the likelihood function is the product of the individual PMFs:\n$$L(\\lambda; x_{1}, \\dots, x_{n}) = \\prod_{i=1}^{n} P(X_{i} = x_{i}) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_{i}} \\exp(-\\lambda)}{x_{i}!}$$\n\nIt is computationally more convenient to work with the log-likelihood function, $\\ell(\\lambda) = \\ln(L(\\lambda))$:\n$$ \\ell(\\lambda) = \\ln\\left(\\prod_{i=1}^{n} \\frac{\\lambda^{x_{i}} \\exp(-\\lambda)}{x_{i}!}\\right) = \\sum_{i=1}^{n} \\ln\\left(\\frac{\\lambda^{x_{i}} \\exp(-\\lambda)}{x_{i}!}\\right) $$\n$$ \\ell(\\lambda) = \\sum_{i=1}^{n} (x_{i}\\ln(\\lambda) - \\lambda - \\ln(x_{i}!)) $$\n$$ \\ell(\\lambda) = \\ln(\\lambda)\\left(\\sum_{i=1}^{n} x_{i}\\right) - n\\lambda - \\sum_{i=1}^{n} \\ln(x_{i}!) $$\n\nThe score function, $S(\\lambda)$, is the first derivative of the log-likelihood function with respect to the parameter $\\lambda$:\n$$ S(\\lambda) = \\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ \\ln(\\lambda)\\left(\\sum_{i=1}^{n} x_{i}\\right) - n\\lambda - \\sum_{i=1}^{n} \\ln(x_{i}!) \\right] $$\n$$ S(\\lambda) = \\frac{1}{\\lambda}\\left(\\sum_{i=1}^{n} x_{i}\\right) - n $$\n\nNext, we calculate the Fisher Information, $I(\\lambda)$. The Fisher Information is defined as the negative of the expected value of the second derivative of the log-likelihood function. First, we compute the second derivative:\n$$ \\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2} = \\frac{\\partial S(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left[ \\frac{1}{\\lambda}\\left(\\sum_{i=1}^{n} x_{i}\\right) - n \\right] = -\\frac{1}{\\lambda^2}\\left(\\sum_{i=1}^{n} x_{i}\\right) $$\n\nNow, we take the expectation of this quantity. For this step, we treat the observations $x_{i}$ as random variables $X_{i}$. We use the fact that for a Poisson-distributed random variable $X_{i}$, its expectation is $\\mathbb{E}[X_{i}] = \\lambda$.\n$$ I(\\lambda) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right] = -\\mathbb{E}\\left[-\\frac{1}{\\lambda^2}\\left(\\sum_{i=1}^{n} X_{i}\\right)\\right] $$\n$$ I(\\lambda) = \\frac{1}{\\lambda^2} \\mathbb{E}\\left[\\sum_{i=1}^{n} X_{i}\\right] $$\nBy linearity of expectation, $\\mathbb{E}\\left[\\sum_{i=1}^{n} X_{i}\\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_{i}] = \\sum_{i=1}^{n} \\lambda = n\\lambda$.\nSubstituting this result back into the expression for the Fisher Information:\n$$ I(\\lambda) = \\frac{1}{\\lambda^2} (n\\lambda) = \\frac{n}{\\lambda} $$\n\nThe Cramér–Rao Lower Bound (CRLB) gives a lower bound on the variance of any unbiased estimator $\\hat{\\lambda}$ of $\\lambda$. The bound is the reciprocal of the Fisher Information:\n$$ \\text{Var}(\\hat{\\lambda}) \\ge \\frac{1}{I(\\lambda)} $$\nTherefore, the CRLB for $\\lambda$ is:\n$$ \\text{CRLB} = \\frac{1}{I(\\lambda)} = \\frac{1}{n/\\lambda} = \\frac{\\lambda}{n} $$\n\nNow, let's derive the Maximum Likelihood Estimator (MLE) for $\\lambda$, denoted $\\hat{\\lambda}_{\\text{MLE}}$. We find the MLE by setting the score function to zero and solving for $\\lambda$:\n$$ S(\\lambda) = \\frac{1}{\\lambda}\\left(\\sum_{i=1}^{n} x_{i}\\right) - n = 0 $$\n$$ \\frac{1}{\\lambda}\\sum_{i=1}^{n} x_{i} = n $$\n$$ \\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} = \\bar{x} $$\nThe MLE for $\\lambda$ is the sample mean of the observations. The second derivative of the log-likelihood, $-\\frac{1}{\\lambda^2}\\sum_{i=1}^{n} x_{i}$, is negative for $\\lambda0$ and non-zero counts, confirming that this is a maximum.\n\nTo assess whether the MLE attains the CRLB, we must first check if it is an unbiased estimator and then compute its variance.\nThe expectation of the MLE is:\n$$ \\mathbb{E}[\\hat{\\lambda}_{\\text{MLE}}] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[X_{i}] = \\frac{1}{n} \\sum_{i=1}^{n} \\lambda = \\frac{n\\lambda}{n} = \\lambda $$\nSince $\\mathbb{E}[\\hat{\\lambda}_{\\text{MLE}}] = \\lambda$, the MLE is an unbiased estimator of $\\lambda$.\n\nThe variance of the MLE is:\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right) $$\nSince the $X_i$ are independent, the variance of the sum is the sum of the variances:\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_{i}) $$\nFor a Poisson distribution, we have $\\text{Var}(X_i) = \\lambda$.\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\lambda = \\frac{n\\lambda}{n^2} = \\frac{\\lambda}{n} $$\n\nComparing the variance of the MLE with the CRLB:\n$$ \\text{Var}(\\hat{\\lambda}_{\\text{MLE}}) = \\frac{\\lambda}{n} = \\text{CRLB} $$\nSince the variance of the unbiased MLE is equal to the Cramér–Rao Lower Bound, the MLE $\\hat{\\lambda}_{\\text{MLE}} = \\bar{X}$ is an efficient estimator for $\\lambda$.\n\nThe problem asks for the closed-form analytic expression for the CRLB. As derived, this is $\\frac{\\lambda}{n}$.", "answer": "$$\\boxed{\\frac{\\lambda}{n}}$$", "id": "3044313"}]}