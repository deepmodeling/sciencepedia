{"hands_on_practices": [{"introduction": "Understanding the normal distribution begins with its fundamental properties. This exercise invites you to use the technique of standardization to derive the first four moments of any normal random variable, starting from the known moments of the standard normal distribution. By doing so, you will directly see how the iconic zero skewness and kurtosis of $3$ emerge, reinforcing the theoretical underpinnings of this ubiquitous distribution. [@problem_id:3068835]", "problem": "Consider the stochastic differential equation (SDE) $dX_{t} = a\\,dt + b\\,dW_{t}$, where $W_{t}$ is a standard Wiener process and $a$, $b$ are constants. Over a fixed time step $\\Delta t$, the Euler–Maruyama increment $\\Delta X := X_{t+\\Delta t} - X_{t}$ is normally distributed with mean $\\mu := a\\,\\Delta t$ and variance $\\sigma^{2} := b^{2}\\,\\Delta t$. Thus, we may write $\\Delta X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. Let $X$ denote a generic normal random variable with $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. Use standardization to compute the first four raw moments $\\mathbb{E}[X^{k}]$ for $k \\in \\{1,2,3,4\\}$ starting from the definitions of moments and well-tested facts about the standard normal distribution. Then, using the definitions of standardized skewness and kurtosis,\n- skewness $\\gamma_{1} := \\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{3}\\right]$,\n- kurtosis $\\kappa := \\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^{4}\\right]$,\nuse your computed moments to illustrate the zero skewness and the specific kurtosis of the normal distribution.\n\nReport your final results as the single row matrix $(\\mathbb{E}[X],\\ \\mathbb{E}[X^{2}],\\ \\mathbb{E}[X^{3}],\\ \\mathbb{E}[X^{4}],\\ \\gamma_{1},\\ \\kappa)$ in that order. No numerical rounding is required, and no physical units are involved.", "solution": "The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. We may therefore proceed with the derivation.\n\nThe problem requires the calculation of the first four raw moments, the skewness, and the kurtosis for a random variable $X$ with a normal distribution, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The core technique is standardization, which relates $X$ to a standard normal random variable $Z \\sim \\mathcal{N}(0, 1)$.\n\nThe relationship is given by the transformation:\n$$Z = \\frac{X - \\mu}{\\sigma}$$\nFrom this, we can express $X$ in terms of $Z$:\n$$X = \\mu + \\sigma Z$$\nTo compute the raw moments of $X$, $\\mathbb{E}[X^k]$, we will use the above expression for $X$ and the linearity of the expectation operator. This requires the moments of the standard normal variable $Z$. As per the problem's allowance to use \"well-tested facts about the standard normal distribution,\" we will use the following known moments of $Z$:\nThe odd moments of $Z$ are all zero due to the symmetry of the distribution around $0$.\n$$\\mathbb{E}[Z^n] = 0 \\quad \\text{for odd } n$$\nSpecifically, $\\mathbb{E}[Z^1] = \\mathbb{E}[Z] = 0$ and $\\mathbb{E}[Z^3] = 0$.\n\nThe even moments are given by the double factorial $(n-1)!! = (n-1)(n-3)\\cdots1$.\n$$\\mathbb{E}[Z^2] = (2-1)!! = 1$$\nThis is consistent with the fact that $Z$ has a variance of $1$, since $\\text{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2 = 1 - 0^2 = 1$.\n$$\\mathbb{E}[Z^4] = (4-1)!! = 3 \\cdot 1 = 3$$\n\nNow, we compute the first four raw moments of $X$.\n\nFirst raw moment ($k=1$):\n$$\\mathbb{E}[X] = \\mathbb{E}[\\mu + \\sigma Z] = \\mathbb{E}[\\mu] + \\sigma\\mathbb{E}[Z] = \\mu + \\sigma(0) = \\mu$$\n\nSecond raw moment ($k=2$):\nWe use the binomial expansion of $(\\mu + \\sigma Z)^2$.\n$$\\mathbb{E}[X^2] = \\mathbb{E}[(\\mu + \\sigma Z)^2] = \\mathbb{E}[\\mu^2 + 2\\mu\\sigma Z + \\sigma^2 Z^2]$$\nBy linearity of expectation:\n$$\\mathbb{E}[X^2] = \\mathbb{E}[\\mu^2] + 2\\mu\\sigma\\mathbb{E}[Z] + \\sigma^2\\mathbb{E}[Z^2] = \\mu^2 + 2\\mu\\sigma(0) + \\sigma^2(1) = \\mu^2 + \\sigma^2$$\n\nThird raw moment ($k=3$):\nWe use the binomial expansion of $(\\mu + \\sigma Z)^3$.\n$$\\mathbb{E}[X^3] = \\mathbb{E}[(\\mu + \\sigma Z)^3] = \\mathbb{E}[\\mu^3 + 3\\mu^2\\sigma Z + 3\\mu\\sigma^2 Z^2 + \\sigma^3 Z^3]$$\nBy linearity of expectation:\n$$\\mathbb{E}[X^3] = \\mathbb{E}[\\mu^3] + 3\\mu^2\\sigma\\mathbb{E}[Z] + 3\\mu\\sigma^2\\mathbb{E}[Z^2] + \\sigma^3\\mathbb{E}[Z^3]$$\n$$\\mathbb{E}[X^3] = \\mu^3 + 3\\mu^2\\sigma(0) + 3\\mu\\sigma^2(1) + \\sigma^3(0) = \\mu^3 + 3\\mu\\sigma^2$$\n\nFourth raw moment ($k=4$):\nWe use the binomial expansion of $(\\mu + \\sigma Z)^4$.\n$$\\mathbb{E}[X^4] = \\mathbb{E}[(\\mu + \\sigma Z)^4] = \\mathbb{E}[\\mu^4 + 4\\mu^3\\sigma Z + 6\\mu^2\\sigma^2 Z^2 + 4\\mu\\sigma^3 Z^3 + \\sigma^4 Z^4]$$\nBy linearity of expectation:\n$$\\mathbb{E}[X^4] = \\mathbb{E}[\\mu^4] + 4\\mu^3\\sigma\\mathbb{E}[Z] + 6\\mu^2\\sigma^2\\mathbb{E}[Z^2] + 4\\mu\\sigma^3\\mathbb{E}[Z^3] + \\sigma^4\\mathbb{E}[Z^4]$$\n$$\\mathbb{E}[X^4] = \\mu^4 + 4\\mu^3\\sigma(0) + 6\\mu^2\\sigma^2(1) + 4\\mu\\sigma^3(0) + \\sigma^4(3) = \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4$$\n\nNext, we compute the skewness and kurtosis as defined in the problem.\n\nSkewness ($\\gamma_1$):\nThe skewness is defined as the third standardized moment.\n$$\\gamma_1 := \\mathbb{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right]$$\nRecognizing that $\\frac{X-\\mu}{\\sigma} = Z$, this is simply the third moment of the standard normal distribution.\n$$\\gamma_1 = \\mathbb{E}[Z^3] = 0$$\nThe zero skewness reflects the symmetry of handcuffs normal distribution about its mean.\n\nKurtosis ($\\kappa$):\nThe kurtosis is defined as the fourth standardized moment.\n$$\\kappa := \\mathbb{E}\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^4\\right]$$\nSimilarly, this is the fourth moment of the standard normal distribution.\n$$\\kappa = \\mathbb{E}[Z^4] = 3$$\nThis value is a defining characteristic of the mesokurtic nature of the normal distribution.\n\nThe problem asks to illustrate the zero skewness and specific kurtosis using the computed moments. The definitions provided for $\\gamma_1$ and $\\kappa$ are directly in terms of the standardized variable $Z$, whose moments we have used throughout. The calculation of these values from the moments of $Z$ serves as the required illustration.\n\nFinally, we assemble the results into the specified single row matrix $(\\mathbb{E}[X], \\mathbb{E}[X^2], \\mathbb{E}[X^3], \\mathbb{E}[X^4], \\gamma_1, \\kappa)$.\nThe elements are:\n$\\mathbb{E}[X] = \\mu$\n$\\mathbb{E}[X^2] = \\mu^2 + \\sigma^2$\n$\\mathbb{E}[X^3] = \\mu^3 + 3\\mu\\sigma^2$\n$\\mathbb{E}[X^4] = \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4$\n$\\gamma_1 = 0$\n$\\kappa = 3$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\mu  \\mu^2 + \\sigma^2  \\mu^3 + 3\\mu\\sigma^2  \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4  0  3 \\end{pmatrix} } $$", "id": "3068835"}, {"introduction": "The normal distribution's importance in SDEs stems from its connection to Brownian motion, the fundamental source of randomness. This practice challenges you to derive the distribution of a random variable defined by an Itô integral, a core component in the solution of many SDEs. Applying the Itô isometry will reveal that the result is a normal distribution whose parameters you can determine, showcasing how Gaussian properties are preserved through stochastic integration. [@problem_id:3068837]", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion (also called a Wiener process) on a filtered probability space satisfying the usual conditions. Fix parameters $\\lambda0$ and $T0$, and consider the stochastic integral\n$$\nX_{T} \\equiv \\int_{0}^{T} \\exp(-\\lambda s)\\, \\mathrm{d}B_{s}.\n$$\nUsing only fundamental properties of Brownian motion (independent and stationary Gaussian increments), the definition of the Itô stochastic integral as an $L^{2}$ limit for deterministic square-integrable integrands, and the Itô isometry, derive from first principles the probability distribution of $X_{T}$ and compute its variance. Then standardize $X_{T}$ to obtain a random variable with the standard normal distribution, and identify its law. Finally, determine the characteristic function $\\varphi_{X_{T}}(u) = \\mathbb{E}[\\exp(\\mathrm{i}u X_{T})]$ in closed form as a function of $u \\in \\mathbb{R}$, $\\lambda$, and $T$.\n\nProvide your final answer as the closed-form expression for $\\varphi_{X_{T}}(u)$. No numerical approximation is required.", "solution": "The problem asks for a derivation from first principles. We begin by establishing the probability distribution of the random variable $X_T$.\n\nThe random variable $X_{T}$ is defined as the Itô integral of a deterministic function $f(s) = \\exp(-\\lambda s)$ with respect to a standard Brownian motion $B_s$:\n$$\nX_{T} = \\int_{0}^{T} \\exp(-\\lambda s)\\, \\mathrm{d}B_{s}\n$$\nBy definition, for a deterministic integrand, this integral is the limit in the $L^2(\\Omega, \\mathcal{F}, \\mathbb{P})$ sense of approximating sums. Let's consider a partition of the interval $[0, T]$ given by $0 = t_0  t_1  \\dots  t_n = T$. An approximating sum is given by:\n$$\nS_n = \\sum_{k=0}^{n-1} \\exp(-\\lambda t_k) (B_{t_{k+1}} - B_{t_k})\n$$\nFrom the fundamental properties of standard Brownian motion, the increments $\\Delta B_k = B_{t_{k+1}} - B_{t_k}$ are independent Gaussian random variables, each with a distribution $N(0, t_{k+1} - t_k)$. The sum $S_n$ is a linear combination of these independent Gaussian random variables. A well-known property of the Gaussian distribution is its stability under linear combination: any linear combination of independent Gaussian random variables is also a Gaussian random variable. Therefore, each approximating sum $S_n$ is a Gaussian random variable.\n\nThe integral $X_T$ is the $L^2$ limit of the sequence $\\{S_n\\}$ as the mesh of the partition, $\\max_k(t_{k+1}-t_k)$, goes to zero. Convergence in $L^2$ implies convergence in distribution. Since each term in the sequence of random variables $\\{S_n\\}$ is Gaussian, the limiting random variable $X_T$ must also be Gaussian.\n\nA Gaussian (normal) distribution is completely characterized by its mean and variance. We now compute these two moments for $X_T$.\n\n**1. Mean of $X_T$**\nThe mean of $X_T$ is given by its expectation. For an Itô integral with a deterministic integrand, the expectation can be taken inside the integral, or more formally, we can compute the expectation of the approximating sums:\n$$\n\\mathbb{E}[S_n] = \\mathbb{E}\\left[\\sum_{k=0}^{n-1} \\exp(-\\lambda t_k) (B_{t_{k+1}} - B_{t_k})\\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[S_n] = \\sum_{k=0}^{n-1} \\exp(-\\lambda t_k) \\mathbb{E}[B_{t_{k+1}} - B_{t_k}]\n$$\nSince $\\mathbb{E}[B_{t_{k+1}} - B_{t_k}] = 0$ for all $k$, we have $\\mathbb{E}[S_n] = 0$. As $S_n \\to X_T$ in $L^2$, which implies $S_n \\to X_T$ in $L^1$, the expectation of the limit is the limit of the expectations:\n$$\n\\mathbb{E}[X_T] = \\lim_{n \\to \\infty} \\mathbb{E}[S_n] = 0\n$$\n\n**2. Variance of $X_T$**\nThe variance of $X_T$ is given by $\\text{Var}(X_T) = \\mathbb{E}[X_T^2] - (\\mathbb{E}[X_T])^2$. Since the mean is $0$, the variance is simply the second moment: $\\text{Var}(X_T) = \\mathbb{E}[X_T^2]$.\n\nTo compute this, we use the Itô isometry, which for a deterministic, square-integrable integrand $f(s)$ states:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{T} f(s)\\, \\mathrm{d}B_{s}\\right)^2\\right] = \\int_{0}^{T} \\mathbb{E}[f(s)^2]\\, \\mathrm{d}s = \\int_{0}^{T} f(s)^2\\, \\mathrm{d}s\n$$\nIn our case, $f(s) = \\exp(-\\lambda s)$. Applying the isometry:\n$$\n\\text{Var}(X_T) = \\mathbb{E}[X_T^2] = \\int_{0}^{T} (\\exp(-\\lambda s))^2 \\, \\mathrm{d}s = \\int_{0}^{T} \\exp(-2\\lambda s) \\, \\mathrm{d}s\n$$\nWe evaluate this definite integral:\n$$\n\\int_{0}^{T} \\exp(-2\\lambda s) \\, \\mathrm{d}s = \\left[ \\frac{\\exp(-2\\lambda s)}{-2\\lambda} \\right]_{0}^{T} = \\frac{\\exp(-2\\lambda T)}{-2\\lambda} - \\frac{\\exp(0)}{-2\\lambda} = \\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}\n$$\nSo, the variance of $X_T$ is $\\text{Var}(X_T) = \\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}$.\n\nCombining these results, the probability distribution of $X_T$ is a normal distribution with mean $0$ and variance $\\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}$. We denote this as:\n$$\nX_T \\sim N\\left(0, \\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}\\right)\n$$\n\n**3. Standardization of $X_T$**\nTo standardize a random variable $Y$ with mean $\\mu$ and variance $\\sigma^2$, we define a new variable $Z = \\frac{Y-\\mu}{\\sigma}$. The resulting random variable $Z$ has mean $0$ and variance $1$.\nFor $X_T$, the mean is $\\mu = 0$ and the variance is $\\sigma^2 = \\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}$. The standard deviation is $\\sigma_{X_T} = \\sqrt{\\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}}$.\nThe standardized variable is:\n$$\nZ_T = \\frac{X_T - 0}{\\sigma_{X_T}} = \\frac{X_T}{\\sqrt{\\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}}}\n$$\nSince $X_T$ is a Gaussian random variable, any affine transformation of it is also Gaussian. Therefore, $Z_T$ is a Gaussian random variable with mean $\\mathbb{E}[Z_T] = \\frac{\\mathbb{E}[X_T]}{\\sigma_{X_T}} = 0$ and variance $\\text{Var}(Z_T) = \\frac{\\text{Var}(X_T)}{\\sigma_{X_T}^2} = 1$. The law of $Z_T$ is the standard normal distribution, $N(0, 1)$.\n\n**4. Characteristic Function of $X_T$**\nThe characteristic function of a random variable $Y$ is defined as $\\varphi_Y(u) = \\mathbb{E}[\\exp(\\mathrm{i}uY)]$ for $u \\in \\mathbb{R}$. For a general Gaussian random variable $Y \\sim N(\\mu, \\sigma^2)$, the characteristic function has the well-known form:\n$$\n\\varphi_Y(u) = \\exp\\left(\\mathrm{i}u\\mu - \\frac{1}{2}u^2\\sigma^2\\right)\n$$\nWe apply this formula to $X_T$, using its derived mean $\\mu = 0$ and variance $\\sigma^2 = \\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}$:\n$$\n\\varphi_{X_{T}}(u) = \\exp\\left(\\mathrm{i}u(0) - \\frac{1}{2}u^2\\left(\\frac{1 - \\exp(-2\\lambda T)}{2\\lambda}\\right)\\right)\n$$\nSimplifying the expression, we obtain the characteristic function of $X_T$ in closed form:\n$$\n\\varphi_{X_{T}}(u) = \\exp\\left( - \\frac{u^2(1 - \\exp(-2\\lambda T))}{4\\lambda} \\right)\n$$\nThis is the final expression required by the problem.", "answer": "$$\n\\boxed{\\exp\\left(-\\frac{u^{2}(1 - \\exp(-2\\lambda T))}{4\\lambda}\\right)}\n$$", "id": "3068837"}, {"introduction": "This exercise bridges the gap between the abstract theory of SDEs and their practical implementation in numerical simulations. By generating random increments of a Wiener process using standardization and empirically testing their statistical moments, you will confirm the theoretical scaling laws that govern Brownian motion. This provides hands-on experience with the foundational techniques used in computational finance, physics, and engineering to model stochastic systems. [@problem_id:3068854]", "problem": "Consider a Wiener process (also called Brownian motion) $\\{B_t\\}_{t \\geq 0}$, which by definition has independent increments and satisfies that for any fixed time step $\\Delta t  0$, the increment $\\Delta B := B_{t+\\Delta t} - B_t$ is normally distributed with mean $0$ and variance $\\Delta t$. In the numerical simulation of a stochastic differential equation (SDE), such as $dX_t = \\mu(X_t,t)\\,dt + \\sigma(X_t,t)\\,dB_t$, one needs to approximate increments $\\Delta B$ over small time steps $\\Delta t$. A standard approach uses the scaling property of the normal distribution: if $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable, then for any $\\sigma  0$, the scaled variable $\\sigma Z$ satisfies $\\sigma Z \\sim \\mathcal{N}(0,\\sigma^2)$. Therefore, the increment can be simulated by the transformation $\\Delta B = \\sqrt{\\Delta t}\\,Z$.\n\nTask: Starting from these foundational definitions and facts, you must do two things:\n- Derive, from first principles of the normal distribution and the definition of Brownian motion increments, why $\\Delta B = \\sqrt{\\Delta t}\\,Z$ has distribution $\\mathcal{N}(0,\\Delta t)$ when $Z \\sim \\mathcal{N}(0,1)$.\n- Empirically verify the convergence of moment estimates by simulating independent samples $\\{\\Delta B_i\\}_{i=1}^N$ using $\\Delta B_i = \\sqrt{\\Delta t}\\,Z_i$ with $Z_i \\sim \\mathcal{N}(0,1)$, and computing the sample mean $\\hat{m} = \\frac{1}{N}\\sum_{i=1}^N \\Delta B_i$ and the unbiased sample variance $\\hat{s}^2 = \\frac{1}{N-1}\\sum_{i=1}^N (\\Delta B_i - \\hat{m})^2$. Use probabilistic reasoning to establish the expected accuracy of these estimators, and then implement a test that decides whether the empirical estimates are consistent with the theoretical targets $0$ for the mean and $\\Delta t$ for the variance at a quantified level.\n\nAcceptance criterion for each test case: Let $\\hat{m}$ and $\\hat{s}^2$ be the empirical estimators. The theoretical variance of the sample mean for independent draws with variance $\\Delta t$ is $\\mathrm{Var}(\\hat{m}) = \\Delta t / N$, so the natural scale for fluctuations of $\\hat{m}$ is $\\sqrt{\\Delta t / N}$. Moreover, for normal data, the unbiased sample variance satisfies $(N-1)\\hat{s}^2 / \\Delta t \\sim \\chi^2_{N-1}$, which implies $\\mathbb{E}[\\hat{s}^2] = \\Delta t$ and $\\mathrm{Var}(\\hat{s}^2) = \\frac{2\\,\\Delta t^2}{N-1}$, so the natural scale for fluctuations of $\\hat{s}^2 - \\Delta t$ is $\\sqrt{\\frac{2\\,\\Delta t^2}{N-1}}$. For each test case, declare it a pass if both inequalities hold:\n$$|\\hat{m} - 0| \\leq c \\sqrt{\\frac{\\Delta t}{N}}, \\quad |\\hat{s}^2 - \\Delta t| \\leq c \\sqrt{\\frac{2\\,\\Delta t^2}{N-1}},$$\nwhere $c$ is a fixed constant multiplier (use $c = 3$). This quantifies the notion that empirical estimates lie within a few standard errors of their targets.\n\nYour program must:\n- Simulate the specified test cases using a reproducible random number generator.\n- For each case, compute $\\hat{m}$ and $\\hat{s}^2$ and check the acceptance criterion above with $c = 3$.\n- Produce a single line of output containing a list of boolean values indicating pass or fail for each test case, formatted as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case $1$: $(\\Delta t, N) = (0, 1000)$, which probes the boundary case where the increment has zero variance and is deterministically $0$.\n- Case $2$: $(\\Delta t, N) = (0.1, 5000)$, a small time step with moderate sample size.\n- Case $3$: $(\\Delta t, N) = (1.0, 100000)$, a unit time step with large sample size.\n- Case $4$: $(\\Delta t, N) = (10^{-4}, 200000)$, a very small time step testing numerical precision.\n- Case $5$: $(\\Delta t, N) = (2.5, 50000)$, a larger time step with moderate sample size.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$, where each $\\text{result}_i$ is either $\\text{True}$ or $\\text{False}$ corresponding to the pass/fail outcome of the $i$-th test case.", "solution": "The solution consists of two parts as requested: a theoretical derivation and an empirical verification.\n\n### Part 1: Theoretical Derivation\n\nThe derivation proceeds from the fundamental properties of expectation and variance for linear transformations of random variables. Let $Z$ be a random variable with mean $\\mathbb{E}[Z]$ and variance $\\mathrm{Var}(Z)$. For constants $a$ and $b$, the transformed variable $Y = aZ + b$ has mean $\\mathbb{E}[Y] = a\\mathbb{E}[Z] + b$ and variance $\\mathrm{Var}(Y) = a^2\\mathrm{Var}(Z)$. A critical property of the normal distribution is its closure under affine transformations: if $Z$ is normally distributed, so is $Y$.\n\nWe are given a standard normal random variable $Z \\sim \\mathcal{N}(0, 1)$, for which its expectation is $\\mathbb{E}[Z] = 0$ and its variance is $\\mathrm{Var}(Z) = 1$. The Brownian increment $\\Delta B$ is constructed as the random variable $\\Delta B = \\sqrt{\\Delta t}\\,Z$. This is a linear transformation with a scaling factor $a = \\sqrt{\\Delta t}$ and a shift $b = 0$.\n\nFirst, we compute the expectation of $\\Delta B$:\n$$\n\\mathbb{E}[\\Delta B] = \\mathbb{E}[\\sqrt{\\Delta t}\\,Z] = \\sqrt{\\Delta t}\\,\\mathbb{E}[Z]\n$$\nSubstituting the known value $\\mathbb{E}[Z] = 0$:\n$$\n\\mathbb{E}[\\Delta B] = \\sqrt{\\Delta t} \\cdot 0 = 0\n$$\nThis establishes that the mean of the increment is $0$.\n\nSecond, we compute the variance of $\\Delta B$:\n$$\n\\mathrm{Var}(\\Delta B) = \\mathrm{Var}(\\sqrt{\\Delta t}\\,Z) = (\\sqrt{\\Delta t})^2\\,\\mathrm{Var}(Z)\n$$\nSubstituting the known value $\\mathrm{Var}(Z) = 1$:\n$$\n\\mathrm{Var}(\\Delta B) = \\Delta t \\cdot 1 = \\Delta t\n$$\nThis establishes that the variance of the increment is $\\Delta t$.\n\nGiven that $Z$ is a normally distributed random variable, the linearly transformed variable $\\Delta B$ must also be normally distributed. Combining these findings—a normal distribution with a mean of $0$ and a variance of $\\Delta t$—we conclude that $\\Delta B$ follows the distribution $\\mathcal{N}(0, \\Delta t)$. This completes the derivation from first principles, confirming the scaling property used to simulate Wiener process increments.\n\n### Part 2: Empirical Verification\n\nFor the empirical verification, a numerical simulation is implemented according to the problem specification. The core of the implementation processes a suite of test cases, each defined by a pair of parameters $(\\Delta t, N)$, representing the time step size and the number of samples, respectively.\n\nThe procedure for each test case is as follows:\n$1$. A reproducible random number generator is used to generate $N$ independent samples, $\\{Z_i\\}_{i=1}^N$, from the standard normal distribution $\\mathcal{N}(0, 1)$.\n$2$. These standard samples are scaled to produce increments of a Wiener process, $\\{\\Delta B_i\\}_{i=1}^N$, using the theoretically-derived relationship $\\Delta B_i = \\sqrt{\\Delta t}\\,Z_i$.\n$3$. The sample mean, $\\hat{m}$, and the unbiased sample variance, $\\hat{s}^2$, of the generated increments are computed using the standard estimators:\n$$ \\hat{m} = \\frac{1}{N}\\sum_{i=1}^N \\Delta B_i $$\n$$ \\hat{s}^2 = \\frac{1}{N-1}\\sum_{i=1}^N (\\Delta B_i - \\hat{m})^2 $$\n$4$. These empirical statistics are then compared against their theoretical expected values, which are $\\mathbb{E}[\\hat{m}] = 0$ for the mean and $\\mathbb{E}[\\hat{s}^2] = \\Delta t$ for the variance.\n$5$. The acceptance criterion quantifies whether the empirical estimates are statistically consistent with the theoretical targets. A test case is considered a pass if the estimates lie within a specified number of standard errors of their expected values. The problem specifies a multiplier of $c=3$.\n$6$. The first condition checks the sample mean. The theoretical variance of the sample mean is $\\mathrm{Var}(\\hat{m}) = \\frac{\\Delta t}{N}$. The condition is:\n$$ |\\hat{m} - 0| \\leq c \\sqrt{\\frac{\\Delta t}{N}} $$\n$7$. The second condition checks the sample variance. For samples from a normal distribution, the variance of the unbiased sample variance estimator is $\\mathrm{Var}(\\hat{s}^2) = \\frac{2\\Delta t^2}{N-1}$. The condition is:\n$$ |\\hat{s}^2 - \\Delta t| \\leq c \\sqrt{\\frac{2\\Delta t^2}{N-1}} $$\n$8$. A test case is marked as `True` if both conditions are met, and `False` otherwise. For the boundary case where $\\Delta t = 0$, the increments $\\Delta B_i$ are all identically $0$, leading to $\\hat{m}=0$ and $\\hat{s}^2=0$. The acceptance thresholds also evaluate to $0$, so both conditions $|0| \\leq 0$ are satisfied, and the case passes. The implementation correctly handles this limiting case without special conditional logic, as the numerical computations correctly yield the zero values throughout.\n\nThe final output is a list of boolean values, one for each test case, indicating the outcome of the verification.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates Brownian motion increments and verifies their statistical properties.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (delta_t, N_samples)\n        (0.0, 1000),\n        (0.1, 5000),\n        (1.0, 100000),\n        (1e-4, 200000),\n        (2.5, 50000),\n    ]\n\n    # Acceptance criterion multiplier\n    c = 3.0\n\n    # Initialize a reproducible random number generator for consistent results.\n    # The seed value is arbitrary but fixed.\n    rng = np.random.default_rng(42)\n\n    results = []\n    for dt, N in test_cases:\n        # The case N = 1 is not in the test suite. If it were, the denominator\n        # N-1 in the variance of the sample variance would be problematic.\n        # We assume N > 1 as per the test suite.\n        \n        # 1. Generate N standard normal random variables Z ~ N(0,1).\n        Z = rng.standard_normal(size=N)\n        \n        # 2. Scale Z to obtain Brownian increments dB ~ N(0, dt).\n        dB = np.sqrt(dt) * Z\n        \n        # 3. Compute the sample mean and unbiased sample variance.\n        m_hat = np.mean(dB)\n        # Using ddof=1 ensures the denominator is N-1 for an unbiased estimate.\n        s2_hat = np.var(dB, ddof=1)\n        \n        # 4. Calculate the acceptance thresholds.\n        # The formulas are robust and handle the dt=0 case correctly, where\n        # thresholds become 0.\n        mean_threshold = c * np.sqrt(dt / N)\n        var_threshold = c * np.sqrt(2 * (dt**2) / (N - 1))\n        \n        # 5. Check if the empirical estimates are within the thresholds.\n        mean_check = np.abs(m_hat - 0) = mean_threshold\n        var_check = np.abs(s2_hat - dt) = var_threshold\n        \n        # A test case passes if and only if both conditions are met.\n        results.append(mean_check and var_check)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3068854"}]}