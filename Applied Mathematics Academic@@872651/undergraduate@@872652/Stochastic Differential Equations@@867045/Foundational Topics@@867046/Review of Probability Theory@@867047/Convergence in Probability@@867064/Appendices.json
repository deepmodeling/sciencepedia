{"hands_on_practices": [{"introduction": "Mastering convergence in probability begins with understanding how it behaves with fundamental operations. This first practice explores the convergence of a sum of two random sequences. It provides an excellent opportunity to combine the formal definition of convergence in probability with other key tools, like Chebyshev's inequality, to prove a foundational result about stochastic limits [@problem_id:1910723].", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ and $\\{Y_n\\}_{n=1}^{\\infty}$ be two sequences of random variables. The sequence $\\{X_n\\}$ is known to converge in probability to the constant 5. The sequence $\\{Y_n\\}$ is characterized by having a mean of $E[Y_n] = 0$ and a variance of $Var(Y_n) = \\frac{1}{\\sqrt{n}}$ for all positive integers $n$.\n\nA new sequence of random variables, $\\{Z_n\\}_{n=1}^{\\infty}$, is defined by the sum $Z_n = X_n + Y_n$.\n\nDetermine the numerical value to which the sequence $\\{Z_n\\}$ converges in probability.", "solution": "We are given that $X_{n} \\xrightarrow{p} 5$, that is, for every $\\varepsilon0$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}\\big(|X_{n}-5|\\varepsilon\\big)=0.\n$$\nFor $Y_{n}$, we have $\\mathbb{E}[Y_{n}]=0$ and $\\operatorname{Var}(Y_{n})=n^{-1/2}$ for all $n$. By Chebyshev’s inequality, for any $\\varepsilon0$,\n$$\n\\mathbb{P}\\big(|Y_{n}|\\varepsilon\\big)=\\mathbb{P}\\big(|Y_{n}-\\mathbb{E}[Y_{n}]|\\varepsilon\\big)\\leq \\frac{\\operatorname{Var}(Y_{n})}{\\varepsilon^{2}}=\\frac{n^{-1/2}}{\\varepsilon^{2}}\\xrightarrow[n\\to\\infty]{}0.\n$$\nHence $Y_{n}\\xrightarrow{p}0$.\n\nDefine $Z_{n}=X_{n}+Y_{n}$. For any $\\varepsilon0$, by the triangle inequality,\n$$\n|Z_{n}-5|=\\big|(X_{n}-5)+Y_{n}\\big|\\leq |X_{n}-5|+|Y_{n}|.\n$$\nTherefore, using the union bound,\n$$\n\\mathbb{P}\\big(|Z_{n}-5|\\varepsilon\\big)\\leq \\mathbb{P}\\big(|X_{n}-5|\\varepsilon/2\\big)+\\mathbb{P}\\big(|Y_{n}|\\varepsilon/2\\big)\\xrightarrow[n\\to\\infty]{}0,\n$$\nbecause the first term tends to zero by $X_{n}\\xrightarrow{p}5$ and the second term tends to zero by Chebyshev’s inequality shown above. Thus $Z_{n}\\xrightarrow{p}5$.\n\nTherefore, the numerical value to which $Z_{n}$ converges in probability is $5$.", "answer": "$$\\boxed{5}$$", "id": "1910723"}, {"introduction": "Convergence in probability is the cornerstone of proving that a statistical estimator is \"consistent\"—meaning it gets closer to the true value as more data becomes available. This problem situates the concept in a practical engineering scenario, demonstrating how the Weak Law of Large Numbers and the Continuous Mapping Theorem work together. You will see how these powerful theoretical results allow us to establish the consistency of an estimator for a device's efficiency [@problem_id:1910693].", "problem": "An engineer is characterizing a new type of thermoelectric generator. In a series of $n$ independent trials, two physical quantities are measured for each trial $i$: the heat flow across the generator, $X_i$, and the resulting electrical power output, $Y_i$.\n\nThe sequences of measurements, $\\{X_1, X_2, \\dots, X_n\\}$ and $\\{Y_1, Y_2, \\dots, Y_n\\}$, can be modeled as follows:\n- The heat flow measurements $\\{X_i\\}$ are independent and identically distributed (i.i.d.) random variables with a true mean heat flow $\\mathbb{E}[X_i] = \\mu_{Q}$ and a finite variance.\n- The power output measurements $\\{Y_i\\}$ are i.i.d. random variables with a true mean power output $\\mathbb{E}[Y_i] = \\mu_{P}$ and a finite variance.\n- It is known that the true mean heat flow is non-zero, i.e., $\\mu_{Q} \\neq 0$.\n\nTo estimate the generator's conversion efficiency, the engineer computes the sample means of the two sets of measurements after $n$ trials:\n$$\n\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\quad \\text{and} \\quad \\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\nThe engineer then defines an estimator for the efficiency, $\\eta_n$, as the ratio of the sample mean power output to the sample mean heat flow:\n$$\n\\eta_n = \\frac{\\bar{Y}_n}{\\bar{X}_n}\n$$\nDetermine the value to which the sequence of random variables $\\eta_n$ converges in probability as the number of trials $n$ approaches infinity. Express your answer as an analytic expression in terms of $\\mu_{Q}$ and $\\mu_{P}$.", "solution": "By assumption, the heat flow measurements $\\{X_i\\}$ are i.i.d. with finite variance and mean $\\mathbb{E}[X_i]=\\mu_{Q}$, and the power measurements $\\{Y_i\\}$ are i.i.d. with finite variance and mean $\\mathbb{E}[Y_i]=\\mu_{P}$. Define the sample means\n$$\n\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^{n}X_i,\\qquad \\bar{Y}_n=\\frac{1}{n}\\sum_{i=1}^{n}Y_i.\n$$\nBy the Weak Law of Large Numbers (WLLN), finite variance implies\n$$\n\\bar{X}_n \\xrightarrow{p} \\mu_{Q}\\quad\\text{and}\\quad \\bar{Y}_n \\xrightarrow{p} \\mu_{P}\\quad\\text{as }n\\to\\infty.\n$$\nFrom these, joint convergence in probability of the pair follows: for any $\\varepsilon0$,\n$$\n\\mathbb{P}\\!\\left(\\left\\|(\\bar{X}_n,\\bar{Y}_n)-(\\mu_{Q},\\mu_{P})\\right\\|\\varepsilon\\right)\n\\leq \\mathbb{P}\\!\\left(|\\bar{X}_n-\\mu_{Q}|\\frac{\\varepsilon}{2}\\right)+\\mathbb{P}\\!\\left(|\\bar{Y}_n-\\mu_{P}|\\frac{\\varepsilon}{2}\\right)\\to 0,\n$$\nso $(\\bar{X}_n,\\bar{Y}_n)\\xrightarrow{p}(\\mu_{Q},\\mu_{P})$. Because $\\mu_{Q}\\neq 0$, the mapping $g(x,y)=y/x$ is continuous at $(\\mu_{Q},\\mu_{P})$. By the Continuous Mapping Theorem,\n$$\n\\eta_n=\\frac{\\bar{Y}_n}{\\bar{X}_n}=g(\\bar{X}_n,\\bar{Y}_n)\\xrightarrow{p} g(\\mu_{Q},\\mu_{P})=\\frac{\\mu_{P}}{\\mu_{Q}}.\n$$\nFinally, the fact that $\\mu_{Q}\\neq 0$ also ensures that $\\mathbb{P}(|\\bar{X}_n|\\tfrac{|\\mu_{Q}|}{2})\\to 1$, so the ratio is well-defined with probability tending to one.\nTherefore, $\\eta_n$ converges in probability to $\\mu_{P}/\\mu_{Q}$.", "answer": "$$\\boxed{\\frac{\\mu_{P}}{\\mu_{Q}}}$$", "id": "1910693"}, {"introduction": "Just as crucial as knowing when a sequence converges is understanding the conditions under which it fails to do so. This exercise presents a sequence of estimators that, while seemingly reasonable, does not converge to the desired constant. By analyzing why this occurs [@problem_id:1910710], you will gain a deeper appreciation for the necessary conditions for convergence and the importance of ensuring that the influence of any single measurement diminishes over time.", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ that are independent and identically distributed (i.i.d.). Each random variable $X_i$ has a well-defined mean $E[X_i] = \\mu$ and a finite, non-zero variance $\\text{Var}(X_i) = \\sigma^2  0$.\n\nA new sequence of random variables, $\\{Y_n\\}_{n=1}^{\\infty}$, is constructed from the original sequence as follows:\n$$Y_n = \\frac{X_n + X_1}{2}$$\nfor each integer $n \\ge 1$.\n\nWhich of the following statements accurately describes the limiting behavior of the sequence $\\{Y_n\\}$ as $n \\to \\infty$?\n\nA. $Y_n$ converges in probability to the constant mean $\\mu$.\n\nB. $Y_n$ does not converge in probability to the constant mean $\\mu$.\n\nC. $Y_n$ converges in probability to the random variable $X_1$.\n\nD. Whether $Y_n$ converges in probability to $\\mu$ depends on the specific shape of the distribution of the $X_i$ (e.g., normal, uniform, etc.), and cannot be determined from the mean and variance alone.", "solution": "We are given i.i.d. random variables $\\{X_{n}\\}_{n=1}^{\\infty}$ with $E[X_{i}]=\\mu$ and $\\text{Var}(X_{i})=\\sigma^{2}0$. Define $Y_{n}=\\frac{X_{n}+X_{1}}{2}$.\n\nFirst, compute the mean and variance of $Y_{n}$. For any $n \\geq 1$,\n$$\nE[Y_{n}]=\\frac{E[X_{n}]+E[X_{1}]}{2}=\\frac{\\mu+\\mu}{2}=\\mu.\n$$\nFor $n \\geq 2$, $X_{n}$ is independent of $X_{1}$, so $\\text{Cov}(X_{n},X_{1})=0$, and\n$$\n\\text{Var}(Y_{n})=\\text{Var}\\!\\left(\\frac{X_{n}+X_{1}}{2}\\right)=\\frac{1}{4}\\left(\\text{Var}(X_{n})+\\text{Var}(X_{1})+2\\,\\text{Cov}(X_{n},X_{1})\\right)=\\frac{1}{4}( \\sigma^{2}+\\sigma^{2})=\\frac{\\sigma^{2}}{2}.\n$$\nHence, for all $n \\geq 2$, $Y_{n}$ has the same distribution with mean $\\mu$ and strictly positive variance $\\frac{\\sigma^{2}}{2}$. In particular, the law of $Y_{n}$ does not depend on $n$ for $n \\geq 2$ and is non-degenerate.\n\nTo test convergence in probability to $\\mu$, recall that $Y_{n} \\xrightarrow{p} \\mu$ would require that for every $\\epsilon0$,\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(|Y_{n}-\\mu|\\epsilon)=0.\n$$\nHowever, for $n \\geq 2$, the quantity $\\mathbb{P}(|Y_{n}-\\mu|\\epsilon)$ is constant in $n$ because the distribution of $Y_{n}$ is fixed. Since $\\text{Var}(Y_{n})=\\frac{\\sigma^{2}}{2}0$, $Y_{n}$ is not almost surely equal to $\\mu$, so there exists some $\\epsilon_{0}0$ such that\n$$\n\\mathbb{P}(|Y_{n}-\\mu|\\epsilon_{0})0\n$$\nfor all $n \\geq 2$. Therefore the limit cannot be zero, and $Y_{n}$ does not converge in probability to $\\mu$. This rules out A and confirms B.\n\nTo test convergence in probability to $X_{1}$, observe that for $n \\geq 2$,\n$$\nY_{n}-X_{1}=\\frac{X_{n}-X_{1}}{2}.\n$$\nHence, for any $\\epsilon0$,\n$$\n\\mathbb{P}(|Y_{n}-X_{1}|\\epsilon)=\\mathbb{P}(|X_{n}-X_{1}|2\\epsilon).\n$$\nDefine $Z_{n}=X_{n}-X_{1}$. For $n \\geq 2$, $Z_{n}$ has mean $0$ and variance\n$$\n\\text{Var}(Z_{n})=\\text{Var}(X_{n})+\\text{Var}(X_{1})-2\\,\\text{Cov}(X_{n},X_{1})=2\\sigma^{2}0,\n$$\nand its distribution does not depend on $n$. Since $Z_{n}$ is non-degenerate, there exists $\\epsilon_{1}0$ such that\n$$\n\\mathbb{P}(|Z_{n}|2\\epsilon_{1})0,\n$$\nso $\\mathbb{P}(|Y_{n}-X_{1}|\\epsilon_{1})0$ for all $n \\geq 2$. Therefore $\\mathbb{P}(|Y_{n}-X_{1}|\\epsilon)$ does not tend to $0$, and $Y_{n}$ does not converge in probability to $X_{1}$. This rules out C.\n\nThe conclusion that $Y_{n}$ does not converge in probability to $\\mu$ does not depend on the specific shape of the distribution, only on independence, identical distribution, and $\\sigma^{2}0$, so D is false.\n\nTherefore, the correct statement is that $Y_{n}$ does not converge in probability to $\\mu$.", "answer": "$$\\boxed{B}$$", "id": "1910710"}]}