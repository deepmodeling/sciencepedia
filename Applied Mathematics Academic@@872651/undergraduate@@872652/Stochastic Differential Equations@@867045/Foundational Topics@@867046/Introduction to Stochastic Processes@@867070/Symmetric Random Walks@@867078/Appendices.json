{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with a direct calculation involving the core properties of a simple symmetric random walk. This exercise requires you to compute the joint probability of the walk's position at two different points in time [@problem_id:1313996]. Mastering this type of calculation is essential as it reinforces the definition of the random walk and the crucial property of independent increments, which are the fundamental building blocks for analyzing more complex stochastic behavior.", "problem": "Let $X_i$ for $i \\ge 1$ be a sequence of independent and identically distributed random variables, where the probability of each outcome is $P(X_i = 1) = p$ and $P(X_i = -1) = 1-p$. A general one-dimensional random walk on the integers is defined by the process $S_n = \\sum_{i=1}^{n} X_i$ for $n \\ge 1$, with the starting position fixed at $S_0 = 0$.\n\nFor the specific case of a simple symmetric random walk, where $p = 1/2$, consider the positions of the walk at time $n=2$ and time $n=4$. Calculate the joint probability that the walk is at position 0 at time $n=2$ and at position 2 at time $n=4$, which is denoted by $P(S_2 = 0, S_4 = 2)$.\n\nExpress your answer as an exact fraction.", "solution": "We use the independence of increments of the simple symmetric random walk. By the Markov property and independent increments,\n$$\nP(S_{2}=0,\\,S_{4}=2)=P(S_{2}=0)\\,P(S_{4}=2\\,|\\,S_{2}=0)=P(S_{2}=0)\\,P(S_{4}-S_{2}=2).\n$$\nFirst, compute $P(S_{2}=0)$:\n$$\nS_{2}=X_{1}+X_{2},\\quad P(S_{2}=0)=P((X_{1},X_{2})=(1,-1)) + P((X_{1},X_{2})=(-1,1)).\n$$\nSince $P(X_{i}=1)=P(X_{i}=-1)=\\frac{1}{2}$ and the $X_{i}$ are independent,\n$$\nP(S_{2}=0)=2\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{2}.\n$$\nNext, compute $P(S_{4}-S_{2}=2)$:\n$$\nS_{4}-S_{2}=X_{3}+X_{4},\\quad P(S_{4}-S_{2}=2)=P(X_{3}=1,\\,X_{4}=1)=\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{4}.\n$$\nTherefore,\n$$\nP(S_{2}=0,\\,S_{4}=2)=\\frac{1}{2}\\cdot\\frac{1}{4}=\\frac{1}{8}.\n$$", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "1313996"}, {"introduction": "A classic question in the study of random walks is the \"Gambler's Ruin\" problem: what is the probability that the walk reaches one boundary before another? This practice guides you through deriving this absorption probability by setting up and solving a linear difference equation, a powerful and general technique [@problem_id:3079254]. By first solving the problem for a general biased walk and then examining the limit as the walk becomes symmetric, you will not only derive a famous result but also appreciate the special nature of the symmetric case.", "problem": "Let $\\{S_{n}\\}_{n\\geq 0}$ be a discrete-time nearest-neighbor random walk on $\\mathbb{Z}$ defined by $S_{0}=i$ and $S_{n}=\\sum_{k=1}^{n}X_{k}+i$, where $\\{X_{k}\\}_{k\\geq 1}$ are independent and identically distributed with $\\mathbb{P}(X_{k}=1)=p$ and $\\mathbb{P}(X_{k}=-1)=q$, where $p\\in(0,1)$, $q=1-p$, and $p\\neq \\frac{1}{2}$. Fix an integer $N\\geq 2$ and take an initial state $i\\in\\{1,2,\\dots,N-1\\}$. Define the hitting times $\\tau_{A}=\\inf\\{n\\geq 0:S_{n}\\in A\\}$ for $A\\subset\\mathbb{Z}$, and set $\\tau_{0}=\\tau_{\\{0\\}}$ and $\\tau_{N}=\\tau_{\\{N\\}}$.\n\nUsing only the definition of conditional expectation and the Markov property, derive an explicit closed-form expression for the function\n$$\nu(i)=\\mathbb{P}_{i}\\big(\\tau_{N}\\tau_{0}\\big)\n$$\nwhen $p\\neq q$. Then, take the symmetric limit $p\\to \\frac{1}{2}$ and determine the corresponding limiting expression. Provide your final answer as two expressions in the order: the general biased case for $p\\neq q$ and the symmetric limit as $p\\to \\frac{1}{2}$. No numerical approximation is required, and no physical units are involved.", "solution": "Let $u(i) = \\mathbb{P}_{i}(\\tau_{N}  \\tau_{0})$ be the probability that the random walk, starting from state $i$, hits state $N$ before hitting state $0$. We are interested in finding $u(i)$ for $i \\in \\{1, 2, \\dots, N-1\\}$.\n\nThe boundary conditions are determined by the definition of the event.\nIf the walk starts at $i=0$, it has hit state $0$ at time $n=0$. Thus, $\\tau_{0}=0$. Since $N \\ge 2$, it cannot be at state $N$ at time $n=0$, so $\\tau_{N}0$. The event $\\tau_{N}  \\tau_{0}$ is impossible. Therefore,\n$$u(0) = 0$$\nIf the walk starts at $i=N$, it has hit state $N$ at time $n=0$. Thus, $\\tau_{N}=0$. Since $i \\neq 0$, $\\tau_{0}0$. The event $\\tau_{N}  \\tau_{0}$ is certain. Therefore,\n$$u(N) = 1$$\n\nFor any interior state $i \\in \\{1, 2, \\dots, N-1\\}$, we can condition on the outcome of the first step, $S_{1}$. The first step is either to $i+1$ with probability $p$ or to $i-1$ with probability $q$. Using the law of total probability (or the definition of conditional expectation):\n$$u(i) = \\mathbb{P}_{i}(S_1=i+1) \\mathbb{P}_{i}(\\tau_N  \\tau_0 | S_1=i+1) + \\mathbb{P}_{i}(S_1=i-1) \\mathbb{P}_{i}(\\tau_N  \\tau_0 | S_1=i-1)$$\nBy definition, $\\mathbb{P}_{i}(S_1=i+1) = \\mathbb{P}(X_1=1) = p$ and $\\mathbb{P}_{i}(S_1=i-1) = \\mathbb{P}(X_1=-1) = q$.\n\nThe random walk has the Markov property, which means the future evolution of the process from state $S_1$ is independent of the path taken to reach $S_1$. Therefore, the probability of reaching $N$ before $0$ starting from $S_1=j$ is the same as the probability of reaching $N$ before $0$ if the process started at $j$ initially.\n$$\\mathbb{P}_{i}(\\tau_N  \\tau_0 | S_1=i+1) = \\mathbb{P}_{i+1}(\\tau_N\\tau_0) = u(i+1)$$\n$$\\mathbb{P}_{i}(\\tau_N  \\tau_0 | S_1=i-1) = \\mathbb{P}_{i-1}(\\tau_N\\tau_0) = u(i-1)$$\nSubstituting these into the equation for $u(i)$ yields a linear second-order homogeneous difference equation:\n$$u(i) = p \\cdot u(i+1) + q \\cdot u(i-1) \\quad \\text{for } i \\in \\{1, 2, \\dots, N-1\\}$$\n\nWe solve this equation subject to the boundary conditions $u(0)=0$ and $u(N)=1$.\nSince $p+q=1$, we can write $u(i) = (p+q)u(i)$. The equation becomes:\n$$p u(i) + q u(i) = p u(i+1) + q u(i-1)$$\nRearranging terms, we get:\n$$p(u(i+1) - u(i)) = q(u(i) - u(i-1))$$\nLet $\\Delta(i) = u(i) - u(i-1)$ be the difference. The relation is $\\Delta(i+1) = \\frac{q}{p} \\Delta(i)$. This shows that the differences form a geometric progression.\nThe general solution to the difference equation $u(i) = p u(i+1) + q u(i-1)$ can be found using its characteristic equation. Assuming a solution of the form $u(i) = r^i$, we substitute it into the rearranged equation $p u(i+1) - u(i) + q u(i-1) = 0$:\n$$p r^{i+1} - r^i + q r^{i-1} = 0$$\nDividing by $r^{i-1}$ (since $r=0$ is not a solution), we get the characteristic equation:\n$$p r^2 - r + q = 0$$\nThis quadratic equation can be factored as $(pr - q)(r - 1) = 0$.\nThe roots are $r_1=1$ and $r_2=\\frac{q}{p}$.\n\n**Case 1: Biased walk ($p \\neq q$)**\nSince $p \\neq q$, the roots $r_1=1$ and $r_2=\\frac{q}{p}$ are distinct. The general solution is a linear combination of the powers of the roots:\n$$u(i) = A \\cdot (r_1)^i + B \\cdot (r_2)^i = A + B \\left(\\frac{q}{p}\\right)^i$$\nWe determine the constants $A$ and $B$ using the boundary conditions $u(0)=0$ and $u(N)=1$.\nFrom $u(0)=0$:\n$$A + B \\left(\\frac{q}{p}\\right)^0 = A+B = 0 \\implies A = -B$$\nFrom $u(N)=1$:\n$$A + B \\left(\\frac{q}{p}\\right)^N = 1$$\nSubstituting $A=-B$ into the second equation:\n$$-B + B \\left(\\frac{q}{p}\\right)^N = 1 \\implies B\\left(\\left(\\frac{q}{p}\\right)^N - 1\\right) = 1$$\n$$B = \\frac{1}{\\left(\\frac{q}{p}\\right)^N - 1}$$\nAnd thus:\n$$A = -B = \\frac{-1}{\\left(\\frac{q}{p}\\right)^N - 1} = \\frac{1}{1 - \\left(\\frac{q}{p}\\right)^N}$$\nSubstituting $A$ and $B$ back into the general solution:\n$$u(i) = \\frac{1}{1 - \\left(\\frac{q}{p}\\right)^N} - \\frac{1}{\\left(\\frac{q}{p}\\right)^N - 1} \\left(\\frac{q}{p}\\right)^i = \\frac{1 - \\left(\\frac{q}{p}\\right)^i}{1 - \\left(\\frac{q}{p}\\right)^N}$$\nThis is the required expression for $u(i)$ when $p \\neq q$.\n\n**Case 2: Symmetric limit ($p \\to \\frac{1}{2}$)**\nWe are asked to find the limit of the expression for $u(i)$ as $p \\to \\frac{1}{2}$. As $p \\to \\frac{1}{2}$, we have $q = 1-p \\to \\frac{1}{2}$, and thus the ratio $\\frac{q}{p} \\to 1$.\nThe expression for $u(i)$ becomes an indeterminate form $\\frac{0}{0}$:\n$$\\lim_{p \\to 1/2} u(i) = \\lim_{\\frac{q}{p} \\to 1} \\frac{1 - \\left(\\frac{q}{p}\\right)^i}{1 - \\left(\\frac{q}{p}\\right)^N}$$\nLet $x = \\frac{q}{p}$. We evaluate the limit $\\lim_{x \\to 1} \\frac{1 - x^i}{1 - x^N}$ using L'HÃ´pital's Rule. We differentiate the numerator and the denominator with respect to $x$:\n$$\\frac{d}{dx}(1-x^i) = -i x^{i-1}$$\n$$\\frac{d}{dx}(1-x^N) = -N x^{N-1}$$\nThe limit is:\n$$\\lim_{x \\to 1} \\frac{-i x^{i-1}}{-N x^{N-1}} = \\frac{-i \\cdot 1^{i-1}}{-N \\cdot 1^{N-1}} = \\frac{-i}{-N} = \\frac{i}{N}$$\nSo, in the symmetric case, the probability is $u(i) = \\frac{i}{N}$.\n\nWe can verify this result by solving the difference equation directly for $p=q=\\frac{1}{2}$.\nThe characteristic equation $pr^2 - r + q = 0$ becomes $\\frac{1}{2}r^2 - r + \\frac{1}{2} = 0$, or $r^2 - 2r + 1 = 0$, which is $(r-1)^2=0$.\nThis equation has a repeated root $r_1=r_2=1$. In this case, the general solution to the difference equation is:\n$$u(i) = A \\cdot (1)^i + B \\cdot i \\cdot (1)^i = A + Bi$$\nUsing the boundary conditions $u(0)=0$ and $u(N)=1$:\nFrom $u(0)=0$:\n$$A + B \\cdot 0 = 0 \\implies A=0$$\nFrom $u(N)=1$:\n$$A + B \\cdot N = 1$$\nSubstituting $A=0$ gives $BN=1$, so $B=\\frac{1}{N}$.\nThe solution is therefore $u(i) = \\frac{1}{N} \\cdot i = \\frac{i}{N}$, which confirms the result obtained from the limit.\n\nThe two expressions requested are:\n1. For $p \\neq q$: $u(i) = \\frac{1 - (q/p)^i}{1 - (q/p)^N}$\n2. The symmetric limit ($p \\to 1/2$): $u(i) = \\frac{i}{N}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1 - \\left(\\frac{q}{p}\\right)^{i}}{1 - \\left(\\frac{q}{p}\\right)^{N}}  \\frac{i}{N}\n\\end{pmatrix}\n}\n$$", "id": "3079254"}, {"introduction": "Beyond knowing the probability of hitting a boundary, we often need to know how long it takes. This advanced exercise introduces one of the most elegant and powerful tools in stochastic analysis: the Optional Stopping Theorem for martingales [@problem_id:3079267]. You will demonstrate that the process $M_n = S_n^2 - n$ is a martingale and use it to derive the expected time for the random walk to be absorbed, providing a beautiful solution to a problem that is much more cumbersome to solve with direct combinatorial methods.", "problem": "Consider the discrete-time simple symmetric random walk on the integers. Let $N \\in \\mathbb{N}$ be fixed and let $S_{0} = i$ with $i \\in \\{0, 1, \\dots, N\\}$. For $n \\geq 1$, define $S_{n} = S_{n-1} + X_{n}$, where $(X_{n})_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = \\tfrac{1}{2}$. Define the hitting times\n$$\n\\tau_{0} = \\inf\\{n \\geq 0 : S_{n} = 0\\}, \\qquad \\tau_{N} = \\inf\\{n \\geq 0 : S_{n} = N\\},\n$$\nand the absorption time $\\tau = \\tau_{0} \\wedge \\tau_{N}$. Using only fundamental definitions of martingales and the optional stopping theorem (OST) for bounded stopping times, derive a closed-form expression for the expected absorption time $E_{i}[\\tau]$ as a function of $i$ and $N$. Your final answer must be a single analytic expression. Do not provide intermediate formulas that directly reveal the final expression; instead, justify each step from first principles. No rounding is required.", "solution": "The problem asks for the expected absorption time, denoted $E_{i}[\\tau]$, for a simple symmetric random walk on the integers, starting at $S_{0} = i$, with absorbing barriers at $0$ and $N$. The absorption time is $\\tau = \\inf\\{n \\geq 0 : S_{n} \\in \\{0, N\\}\\}$. The solution will be derived using martingale theory and the optional stopping theorem (OST).\n\nFirst, let us formalize the process. The random walk is defined by $S_{n} = S_{n-1} + X_{n}$ for $n \\geq 1$, with $S_{0}=i$. The steps $(X_{n})_{n \\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{n} = 1) = \\mathbb{P}(X_{n} = -1) = 1/2$. Let $\\mathcal{F}_{n} = \\sigma(X_{1}, \\dots, X_{n})$ be the natural filtration of the process, with $\\mathcal{F}_{0} = \\{\\emptyset, \\Omega\\}$. The expectation conditional on starting at $S_0=i$ is denoted by $E_i[\\cdot]$.\n\nThe Optional Stopping Theorem (OST) states that for a martingale $(M_{n})_{n \\geq 0}$ and a stopping time $T$, under certain conditions, $E[M_{T}] = E[M_{0}]$. The problem specifies using the OST for bounded stopping times. The stopping time $\\tau$ is not deterministically bounded. However, for a one-dimensional random walk on a finite interval, it is known that $\\tau$ is almost surely finite and $E_i[\\tau]  \\infty$. A rigorous application of the OST involves considering the bounded stopping times $\\tau_{k} = \\tau \\wedge k = \\min(\\tau, k)$ for $k \\in \\mathbb{N}$. Applying the OST to $\\tau_k$ gives $E[M_{\\tau_k}] = E[M_0]$. We can then take the limit as $k \\to \\infty$ and, provided appropriate convergence conditions (such as the Dominated Convergence Theorem or Monotone Convergence Theorem) are met, we can deduce that $E[M_{\\tau}] = E[M_0]$. We will proceed with this understanding.\n\nOur derivation requires two distinct martingales.\n\nFirst, we determine the probability of absorption at each boundary. Let $p_{i} = \\mathbb{P}_{i}(S_{\\tau} = N)$ be the probability that the walk is absorbed at $N$, given it starts at $i$. Consequently, $\\mathbb{P}_{i}(S_{\\tau} = 0) = 1 - p_{i}$. To find $p_{i}$, we consider the process $(S_{n})_{n \\geq 0}$ itself. We verify it is a martingale with respect to the filtration $(\\mathcal{F}_{n})_{n \\geq 0}$. For any $n \\geq 1$:\n$$\nE[S_{n} | \\mathcal{F}_{n-1}] = E[S_{n-1} + X_{n} | \\mathcal{F}_{n-1}]\n$$\nSince $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable and $X_{n}$ is independent of $\\mathcal{F}_{n-1}$:\n$$\nE[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1} + E[X_{n}]\n$$\nThe expectation of a single step is $E[X_{n}] = (1) \\cdot \\mathbb{P}(X_{n}=1) + (-1) \\cdot \\mathbb{P}(X_{n}=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0$.\nThus, $E[S_{n} | \\mathcal{F}_{n-1}] = S_{n-1}$, confirming that $(S_{n})_{n \\geq 0}$ is a martingale.\n\nThe increments of the martingale are bounded, $|S_{n} - S_{n-1}| = |X_{n}| = 1$. The stopping time $\\tau$ has a finite expectation. These are sufficient conditions for the OST to apply to the martingale $S_n$ and stopping time $\\tau$. Applying the OST:\n$$\nE_{i}[S_{\\tau}] = E_{i}[S_{0}] = i\n$$\nThe expected value of the position at absorption time $\\tau$ is given by:\n$$\nE_{i}[S_{\\tau}] = N \\cdot \\mathbb{P}_{i}(S_{\\tau} = N) + 0 \\cdot \\mathbb{P}_{i}(S_{\\tau} = 0) = N \\cdot p_{i}\n$$\nEquating the two expressions for $E_{i}[S_{\\tau}]$, we get $N \\cdot p_{i} = i$, which yields the absorption probability:\n$$\np_{i} = \\frac{i}{N}\n$$\n\nSecond, to find the expected time $E_{i}[\\tau]$, we need a martingale that incorporates the time index $n$. Let us consider the process $M_{n} = S_{n}^2 - n$. We verify if it is a martingale. For any $n \\geq 1$:\n$$\nE[M_{n} | \\mathcal{F}_{n-1}] = E[S_{n}^2 - n | \\mathcal{F}_{n-1}] = E[S_{n}^2 | \\mathcal{F}_{n-1}] - n\n$$\nWe expand $S_{n}^2 = (S_{n-1} + X_{n})^2 = S_{n-1}^2 + 2S_{n-1}X_{n} + X_{n}^2$.\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = E[S_{n-1}^2 + 2S_{n-1}X_{n} + X_{n}^2 | \\mathcal{F}_{n-1}]\n$$\nUsing the linearity of conditional expectation and the fact that $S_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable:\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = S_{n-1}^2 + 2S_{n-1}E[X_{n} | \\mathcal{F}_{n-1}] + E[X_{n}^2 | \\mathcal{F}_{n-1}]\n$$\nAs before, $E[X_{n} | \\mathcal{F}_{n-1}] = E[X_{n}] = 0$. For the term $E[X_{n}^2 | \\mathcal{F}_{n-1}]$, we have $E[X_{n}^2] = (1)^2 \\cdot \\frac{1}{2} + (-1)^2 \\cdot \\frac{1}{2} = 1$.\nSubstituting these results back:\n$$\nE[S_{n}^2 | \\mathcal{F}_{n-1}] = S_{n-1}^2 + 2S_{n-1}(0) + 1 = S_{n-1}^2 + 1\n$$\nNow we can complete the calculation for $E[M_{n} | \\mathcal{F}_{n-1}]$:\n$$\nE[M_{n} | \\mathcal{F}_{n-1}] = (S_{n-1}^2 + 1) - n = S_{n-1}^2 - (n-1) = M_{n-1}\n$$\nThis demonstrates that $(M_{n})_{n \\geq 0} = (S_{n}^2 - n)_{n \\geq 0}$ is indeed a martingale.\n\nNow we apply the OST to the martingale $M_{n}$ and the stopping time $\\tau$. The conditions for OST are met because $E_i[\\tau]  \\infty$ and the martingale increments are bounded over a finite time interval.\n$$\nE_{i}[M_{\\tau}] = E_{i}[M_{0}]\n$$\nThe initial value is $M_{0} = S_{0}^2 - 0 = i^2$.\nThe value at the stopping time is $M_{\\tau} = S_{\\tau}^2 - \\tau$. Its expectation is:\n$$\nE_{i}[M_{\\tau}] = E_{i}[S_{\\tau}^2 - \\tau] = E_{i}[S_{\\tau}^2] - E_{i}[\\tau]\n$$\nEquating the two expressions for the expectation gives:\n$$\nE_{i}[S_{\\tau}^2] - E_{i}[\\tau] = i^2\n$$\nWe can rearrange this to express the expected absorption time:\n$$\nE_{i}[\\tau] = E_{i}[S_{\\tau}^2] - i^2\n$$\nTo finalize the solution, we must compute $E_{i}[S_{\\tau}^2]$. The position at absorption, $S_{\\tau}$, can only be $0$ or $N$.\n$$\nE_{i}[S_{\\tau}^2] = (N^2) \\cdot \\mathbb{P}_{i}(S_{\\tau} = N) + (0^2) \\cdot \\mathbb{P}_{i}(S_{\\tau} = 0) = N^2 \\cdot p_{i}\n$$\nUsing our previously derived result for the absorption probability, $p_{i} = i/N$:\n$$\nE_{i}[S_{\\tau}^2] = N^2 \\cdot \\left(\\frac{i}{N}\\right) = Ni\n$$\nFinally, substituting this back into our expression for $E_{i}[\\tau]$:\n$$\nE_{i}[\\tau] = Ni - i^2\n$$\nThis can be factored to give the final closed-form expression for the expected absorption time as a function of the starting position $i$ and the boundary $N$.\n$$\nE_{i}[\\tau] = i(N-i)\n$$\nThis result is valid for any starting position $i \\in \\{0, 1, \\dots, N\\}$. If $i=0$ or $i=N$, the starting position is already at an absorbing boundary, so $\\tau=0$, and the formula correctly yields $0(N-0)=0$ and $N(N-N)=0$, respectively.", "answer": "$$\n\\boxed{i(N-i)}\n$$", "id": "3079267"}]}