{"hands_on_practices": [{"introduction": "Moving from abstract definitions to concrete calculations is crucial for mastering stochastic processes. This first exercise provides fundamental practice in this skill by asking you to derive the covariance kernel of fractional Brownian motion (fBm) from its core axiomatic properties. This practice is invaluable as it demonstrates how high-level concepts like self-similarity and stationary increments directly sculpt the mathematical form of the kernel, a foundational task in stochastic modeling [@problem_id:3047382].", "problem": "Consider a zero-mean Gaussian process called fractional Brownian motion (fBm), denoted by $B_{H}(t)$, with Hurst parameter $H \\in (0,1)$, indexed by time $t \\in \\mathbb{R}$. The process satisfies the following fundamental properties:\n\n- Stationary increments: for any $s,t \\in \\mathbb{R}$ and any shift $u \\in \\mathbb{R}$, the increment $B_{H}(t+u)-B_{H}(s+u)$ has the same distribution as $B_{H}(t)-B_{H}(s)$.\n- Self-similarity of index $H$: for any $c0$, the process $\\{B_{H}(ct): t \\in \\mathbb{R}\\}$ has the same finite-dimensional distributions as $\\{c^{H} B_{H}(t): t \\in \\mathbb{R}\\}$.\n- Normalization: $\\operatorname{Var}(B_{H}(1)) = 1$.\n\nStarting only from these properties and the core definitions of covariance and variance for Gaussian processes, derive the explicit covariance kernel $k(s,t) = \\mathbb{E}[B_{H}(s) B_{H}(t)]$ as a closed-form analytic expression in terms of $s$, $t$, and $H$. Provide your final expression for $k(s,t)$.", "solution": "The problem asks for the derivation of the covariance kernel $k(s,t) = \\mathbb{E}[B_{H}(s) B_{H}(t)]$ for a zero-mean fractional Brownian motion (fBm) $B_H(t)$, starting from its fundamental properties. Since the process is specified as having zero mean, $\\mathbb{E}[B_{H}(t)] = 0$ for all $t \\in \\mathbb{R}$.\n\nFor any two zero-mean random variables $X$ and $Y$, their covariance is $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY]$. We can relate this to their variances using the identity for the variance of their difference:\n$$ \\operatorname{Var}(X-Y) = \\mathbb{E}[(X-Y)^2] = \\mathbb{E}[X^2 - 2XY + Y^2] = \\mathbb{E}[X^2] + \\mathbb{E}[Y^2] - 2\\mathbb{E}[XY] $$\nSince $X$ and $Y$ are zero-mean, $\\operatorname{Var}(X) = \\mathbb{E}[X^2]$ and $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2]$. Substituting these gives:\n$$ \\operatorname{Var}(X-Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y) - 2\\mathbb{E}[XY] $$\nRearranging this equation to solve for $\\mathbb{E}[XY]$ yields a crucial identity:\n$$ \\mathbb{E}[XY] = \\frac{1}{2} \\left( \\operatorname{Var}(X) + \\operatorname{Var}(Y) - \\operatorname{Var}(X-Y) \\right) $$\nApplying this identity to the stochastic process $B_H(t)$ by setting $X=B_H(s)$ and $Y=B_H(t)$, we obtain the expression for the covariance kernel:\n$$ k(s,t) = \\mathbb{E}[B_H(s)B_H(t)] = \\frac{1}{2} \\left( \\operatorname{Var}(B_H(s)) + \\operatorname{Var}(B_H(t)) - \\operatorname{Var}(B_H(s) - B_H(t)) \\right) $$\nTo find the explicit form of $k(s,t)$, we must determine the terms $\\operatorname{Var}(B_H(t))$ and $\\operatorname{Var}(B_H(s) - B_H(t))$ using the properties provided in the problem statement.\n\nFirst, let us determine $\\operatorname{Var}(B_H(t))$. The property of self-similarity states that for any $c0$, the process $\\{B_H(ct): t \\in \\mathbb{R}\\}$ has the same finite-dimensional distributions as $\\{c^H B_H(t): t \\in \\mathbb{R}\\}$. For a single time point, this implies that the random variable $B_H(ct)$ has the same distribution as $c^H B_H(t)$.\nLet us first establish that $B_H(0)=0$ almost surely. From self-similarity, $B_H(c \\cdot 0)$ has the same distribution as $c^H B_H(0)$ for any $c0$. This simplifies to $B_H(0)$ having the same distribution as $c^H B_H(0)$. The variance must therefore satisfy $\\operatorname{Var}(B_H(0)) = \\operatorname{Var}(c^H B_H(0)) = (c^H)^2 \\operatorname{Var}(B_H(0)) = c^{2H} \\operatorname{Var}(B_H(0))$. Since $H \\in (0,1)$, the term $c^{2H}$ is not equal to $1$ for all $c0$. The only way for the equality $\\operatorname{Var}(B_H(0)) = c^{2H} \\operatorname{Var}(B_H(0))$ to hold is if $\\operatorname{Var}(B_H(0)) = 0$. Since $B_H(0)$ is a Gaussian random variable with zero mean and zero variance, it must be equal to $0$ almost surely.\n\nNow, we can find $\\operatorname{Var}(B_H(t))$ for any $t \\in \\mathbb{R}$.\nCase 1: $t0$. We can use the self-similarity property with $c=t$ and a base time of $1$. The random variable $B_H(t) = B_H(t \\cdot 1)$ has the same distribution as $t^H B_H(1)$. Therefore, their variances are equal:\n$$ \\operatorname{Var}(B_H(t)) = \\operatorname{Var}(t^H B_H(1)) $$\nUsing the variance property $\\operatorname{Var}(aZ) = a^2 \\operatorname{Var}(Z)$, we get:\n$$ \\operatorname{Var}(B_H(t)) = (t^H)^2 \\operatorname{Var}(B_H(1)) = t^{2H} \\operatorname{Var}(B_H(1)) $$\nUsing the normalization condition $\\operatorname{Var}(B_H(1)) = 1$, we find:\n$$ \\operatorname{Var}(B_H(t)) = t^{2H} \\quad \\text{for } t0 $$\nCase 2: $t0$. Let $t = -u$ where $u0$. We need to find $\\operatorname{Var}(B_H(-u))$. We use the property of stationary increments. The increment $B_H(0) - B_H(-u)$ has the same distribution as the increment $B_H(u) - B_H(0)$ (by shifting time by $u$). Since $B_H(0)=0$ a.s., this implies that $-B_H(-u)$ has the same distribution as $B_H(u)$. This means $B_H(-u)$ has the same distribution as $-B_H(u)$.\nTherefore, their variances must be equal:\n$$ \\operatorname{Var}(B_H(-u)) = \\operatorname{Var}(-B_H(u)) = (-1)^2 \\operatorname{Var}(B_H(u)) = \\operatorname{Var}(B_H(u)) $$\nSince $u0$, we use the result from Case 1: $\\operatorname{Var}(B_H(u)) = u^{2H}$.\nThus, for $t0$, $\\operatorname{Var}(B_H(t)) = \\operatorname{Var}(B_H(-u)) = u^{2H} = (-t)^{2H} = |t|^{2H}$.\nCombining cases, and noting that for $t=0$, $\\operatorname{Var}(B_H(0))=0$, we have for all $t \\in \\mathbb{R}$:\n$$ \\operatorname{Var}(B_H(t)) = |t|^{2H} $$\n\nSecond, let us determine $\\operatorname{Var}(B_H(s) - B_H(t))$. We use the property of stationary increments, which states that for any shift $u \\in \\mathbb{R}$, $B_H(s+u)-B_H(t+u)$ has the same distribution as $B_H(s)-B_H(t)$. Let's choose the shift $u=-t$. Then the increment $B_H(s)-B_H(t)$ has the same distribution as the increment $B_H(s-t) - B_H(t-t) = B_H(s-t) - B_H(0)$.\nSince $B_H(0)=0$ a.s., the distribution of $B_H(s)-B_H(t)$ is the same as the distribution of $B_H(s-t)$. Thus, their variances are equal:\n$$ \\operatorname{Var}(B_H(s) - B_H(t)) = \\operatorname{Var}(B_H(s-t)) $$\nUsing our previously derived formula for the variance, we get:\n$$ \\operatorname{Var}(B_H(s) - B_H(t)) = |s-t|^{2H} $$\n\nFinally, we substitute these variance expressions back into our formula for the covariance kernel $k(s,t)$:\n$$ k(s,t) = \\frac{1}{2} \\left( \\operatorname{Var}(B_H(s)) + \\operatorname{Var}(B_H(t)) - \\operatorname{Var}(B_H(s) - B_H(t)) \\right) $$\n$$ k(s,t) = \\frac{1}{2} \\left( |s|^{2H} + |t|^{2H} - |s-t|^{2H} \\right) $$\nThis expression is the closed-form analytic representation of the covariance kernel for fractional Brownian motion, derived solely from the provided axiomatic properties.", "answer": "$$\\boxed{\\frac{1}{2} \\left( |s|^{2H} + |t|^{2H} - |s-t|^{2H} \\right)}$$", "id": "3047382"}, {"introduction": "Many of the most useful stochastic processes in science and engineering are defined as solutions to stochastic differential equations (SDEs). This exercise explores the celebrated Ornstein-Uhlenbeck process and challenges you to connect its dynamics directly to the local properties of its covariance kernel. By analyzing the kernel's behavior for small time lags, you will uncover the intimate relationship between the SDE's diffusion coefficient and the process's path regularity, a deep insight into how dynamics shape statistics [@problem_id:3047384].", "problem": "Consider the strictly stationary Ornstein–Uhlenbeck process defined as the unique stationary solution to the linear stochastic differential equation\n$$\n\\mathrm{d}X_{t} \\,=\\, -\\alpha X_{t}\\,\\mathrm{d}t \\,+\\, \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $\\alpha0$, $\\sigma0$, and $\\{W_{t}\\}_{t\\in\\mathbb{R}}$ is a standard Wiener process (also called standard Brownian motion). Let the covariance kernel of the stationary process be\n$$\nC(h) \\,:=\\, \\operatorname{Cov}(X_{t}, X_{t+h}) \\,=\\, \\mathbb{E}[X_{t}\\,X_{t+h}],\n$$\nfor $h\\in\\mathbb{R}$, where stationarity implies that $C(h)$ depends only on the lag $h$ and the mean is zero.\n\nStarting from the fundamental definitions of covariance for stationary processes and Itô calculus (in particular, Itô’s formula and the Itô isometry), analyze the small-lag behavior of $C(h)$ near $h=0$ and determine the right-hand limit\n$$\nL \\,:=\\, \\lim_{h\\to 0^{+}} \\frac{\\operatorname{Var}(X_{t+h}-X_{t})}{h}.\n$$\nYour reasoning should clearly establish the necessary regularity at $h=0$ that justifies the limit and should derive any intermediate quantities from first principles, without assuming closed-form kernels in advance.\n\nProvide your final answer for $L$ as a closed-form analytic expression in terms of $\\alpha$ and $\\sigma$. No numerical approximation is required.", "solution": "The problem requires the evaluation of the limit $L \\,:=\\, \\lim_{h\\to 0^{+}} \\frac{\\operatorname{Var}(X_{t+h}-X_{t})}{h}$ for a stationary Ornstein-Uhlenbeck process $X_t$. We will derive this from the fundamental properties of the process and Itô calculus, without assuming the closed-form expression for the covariance kernel $C(h)$.\n\nFirst, we express the variance term $\\operatorname{Var}(X_{t+h}-X_{t})$ in terms of the covariance kernel $C(h) = \\operatorname{Cov}(X_{t}, X_{t+h})$.\nThe Ornstein-Uhlenbeck process is defined by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} \\,=\\, -\\alpha X_{t}\\,\\mathrm{d}t \\,+\\, \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $\\alpha0$ and $\\sigma0$. For the process to be strictly stationary, its statistical properties must be independent of time. This implies that the mean $\\mathbb{E}[X_t]$ must be a constant, say $\\mu$. Taking the expectation of the SDE, we get $\\mathrm{d}\\mathbb{E}[X_t] = -\\alpha \\mathbb{E}[X_t] \\mathrm{d}t$. For a constant mean, $\\mathrm{d}\\mu/\\mathrm{d}t = 0$, so we must have $0 = -\\alpha \\mu$. Since $\\alpha > 0$, this forces the mean to be $\\mu = \\mathbb{E}[X_t] = 0$.\n\nWith a zero mean, the variance of the increment is:\n$$\n\\operatorname{Var}(X_{t+h}-X_{t}) \\,=\\, \\mathbb{E}[(X_{t+h}-X_{t})^2] - (\\mathbb{E}[X_{t+h}-X_{t}])^2\n$$\nThe second term is $(\\mathbb{E}[X_{t+h}]-\\mathbb{E}[X_t])^2 = (0-0)^2 = 0$.\nThe first term can be expanded:\n$$\n\\mathbb{E}[(X_{t+h}-X_{t})^2] \\,=\\, \\mathbb{E}[X_{t+h}^2 - 2X_t X_{t+h} + X_t^2]\n$$\nBy the linearity of expectation, this becomes:\n$$\n\\mathbb{E}[X_{t+h}^2] - 2\\mathbb{E}[X_t X_{t+h}] + \\mathbb{E}[X_t^2]\n$$\nFor a stationary process with zero mean, the variance is constant, $\\operatorname{Var}(X_s) = \\mathbb{E}[X_s^2] - (\\mathbb{E}[X_s])^2 = \\mathbb{E}[X_s^2]$. The covariance is defined as $C(s) = \\operatorname{Cov}(X_t, X_{t+s}) = \\mathbb{E}[X_t X_{t+s}]$.\nBy stationarity, $\\mathbb{E}[X_{t+h}^2] = \\mathbb{E}[X_t^2] = \\operatorname{Var}(X_t) = C(0)$.\nTherefore, the variance of the increment is:\n$$\n\\operatorname{Var}(X_{t+h}-X_{t}) \\,=\\, C(0) - 2C(h) + C(0) \\,=\\, 2(C(0) - C(h))\n$$\nSubstituting this into the definition of $L$:\n$$\nL \\,=\\, \\lim_{h\\to 0^{+}} \\frac{2(C(0) - C(h))}{h} \\,=\\, -2 \\lim_{h\\to 0^{+}} \\frac{C(h) - C(0)}{h}\n$$\nThis limit is, by definition, $-2$ times the right-hand derivative of the covariance function at $h=0$, which we denote as $C'(0^{+})$. So, our task is to find $L = -2C'(0^{+})$. This requires deriving the behavior of $C(h)$ for small positive $h$.\n\nWe can derive an ordinary differential equation for $C(h)$ by integrating the SDE from $t$ to $t+h$:\n$$\nX_{t+h} \\,=\\, X_t + \\int_{t}^{t+h} (-\\alpha X_s \\mathrm{d}s + \\sigma \\mathrm{d}W_s)\n$$\nNow we compute $\\mathbb{E}[X_t X_{t+h}]$ for $h0$:\n$$\n\\mathbb{E}[X_t X_{t+h}] \\,=\\, \\mathbb{E}\\left[X_t \\left(X_t - \\alpha \\int_{t}^{t+h} X_s \\mathrm{d}s + \\sigma \\int_{t}^{t+h} \\mathrm{d}W_s\\right)\\right]\n$$\nUsing linearity of expectation:\n$$\nC(h) \\,=\\, \\mathbb{E}[X_t^2] - \\alpha \\mathbb{E}\\left[X_t \\int_{t}^{t+h} X_s \\mathrm{d}s\\right] + \\sigma \\mathbb{E}\\left[X_t \\int_{t}^{t+h} \\mathrm{d}W_s\\right]\n$$\nThe first term is $\\mathbb{E}[X_t^2] = C(0)$. The last term is zero because $X_t$ is adapted to the filtration $\\mathcal{F}_t$, while the increment of the Wiener process $\\int_{t}^{t+h} \\mathrm{d}W_s = W_{t+h} - W_t$ is independent of $\\mathcal{F}_t$ and has zero mean.\nThe middle term can be rewritten using Fubini's theorem for stochastic integrals:\n$$\n\\mathbb{E}\\left[X_t \\int_{t}^{t+h} X_s \\mathrm{d}s\\right] \\,=\\, \\int_{t}^{t+h} \\mathbb{E}[X_t X_s] \\mathrm{d}s \\,=\\, \\int_{t}^{t+h} C(s-t) \\mathrm{d}s\n$$\nChanging the integration variable to $u=s-t$, with $\\mathrm{d}u=\\mathrm{d}s$, the integral becomes $\\int_{0}^{h} C(u) \\mathrm{d}u$.\nThus, we have derived a Volterra integral equation for $C(h)$:\n$$\nC(h) \\,=\\, C(0) - \\alpha \\int_{0}^{h} C(u) \\mathrm{d}u, \\quad \\text{for } h0.\n$$\nThe Ornstein-Uhlenbeck process is known to be mean-square continuous, which implies that its covariance function $C(h)$ is continuous. Therefore, by the Fundamental Theorem of Calculus, we can differentiate the integral equation with respect to $h$:\n$$\nC'(h) \\,=\\, -\\alpha C(h), \\quad \\text{for } h0.\n$$\nNow we can evaluate the right-hand derivative at $h=0$:\n$$\nC'(0^{+}) \\,=\\, \\lim_{h\\to 0^{+}} C'(h) \\,=\\, \\lim_{h\\to 0^{+}} (-\\alpha C(h))\n$$\nSince $C(h)$ is continuous at $h=0$, $\\lim_{h\\to 0^{+}} C(h) = C(0)$.\n$$\nC'(0^{+}) \\,=\\, -\\alpha C(0)\n$$\nSubstituting this back into the expression for $L$:\n$$\nL \\,=\\, -2C'(0^{+}) \\,=\\, -2(-\\alpha C(0)) \\,=\\, 2\\alpha C(0)\n$$\nThe final step is to determine the stationary variance $C(0) = \\mathbb{E}[X_t^2]$ from first principles. We use Itô's formula for the function $f(x) = x^2$:\n$$\n\\mathrm{d}(X_t^2) \\,=\\, 2X_t \\mathrm{d}X_t + \\frac{1}{2}(2)(\\mathrm{d}X_t)^2\n$$\nSubstituting $\\mathrm{d}X_t = -\\alpha X_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t$ and using the Itô multiplication rule $(\\mathrm{d}t)^2=0$, $(\\mathrm{d}t)(\\mathrm{d}W_t)=0$, $(\\mathrm{d}W_t)^2=\\mathrm{d}t$, we find the quadratic variation term:\n$$\n(\\mathrm{d}X_t)^2 \\,=\\, (-\\alpha X_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t)^2 \\,=\\, \\sigma^2 (\\mathrm{d}W_t)^2 \\,=\\, \\sigma^2 \\mathrm{d}t\n$$\nSo, the Itô-Doeblin formula gives:\n$$\n\\mathrm{d}(X_t^2) \\,=\\, 2X_t(-\\alpha X_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t) + \\sigma^2 \\mathrm{d}t\n$$\n$$\n\\mathrm{d}(X_t^2) \\,=\\, (-2\\alpha X_t^2 + \\sigma^2)\\mathrm{d}t + 2\\sigma X_t \\mathrm{d}W_t\n$$\nTaking the expectation of this differential form:\n$$\n\\mathrm{d}\\mathbb{E}[X_t^2] \\,=\\, \\mathbb{E}[(-2\\alpha X_t^2 + \\sigma^2)\\mathrm{d}t + 2\\sigma X_t \\mathrm{d}W_t]\n$$\n$$\n\\frac{\\mathrm{d}\\mathbb{E}[X_t^2]}{\\mathrm{d}t} \\,=\\, -2\\alpha \\mathbb{E}[X_t^2] + \\sigma^2\n$$\nWe used the property that the expectation of the Itô integral term $\\mathbb{E}[\\int 2\\sigma X_s \\mathrm{d}W_s]$ is zero.\nFor a stationary process, the variance $\\mathbb{E}[X_t^2] = C(0)$ is constant with respect to time $t$. Therefore, its time derivative is zero:\n$$\n0 \\,=\\, -2\\alpha C(0) + \\sigma^2\n$$\nSolving for $C(0)$, we find the stationary variance:\n$$\nC(0) \\,=\\, \\frac{\\sigma^2}{2\\alpha}\n$$\nFinally, we substitute this expression for $C(0)$ into our formula for $L$:\n$$\nL \\,=\\, 2\\alpha C(0) \\,=\\, 2\\alpha \\left(\\frac{\\sigma^2}{2\\alpha}\\right) \\,=\\, \\sigma^2\n$$\nThe limit $L$ measures the local rate of increase of variance, which is determined by the magnitude of the diffusion term in the SDE, and is independent of the mean-reversion parameter $\\alpha$.", "answer": "$$\\boxed{\\sigma^{2}}$$", "id": "3047384"}, {"introduction": "The properties of a covariance kernel are not just theoretical curiosities; they provide powerful tools for analyzing the behavior of the process itself. This final practice applies this principle to a process constructed by smoothing white noise with a Gaussian filter. After deriving the process's covariance kernel, you will use its characteristics to investigate fundamental properties like mean-square continuity and differentiability [@problem_id:3047391]. This exercise reinforces the key idea that the smoothness of the covariance function at the origin dictates the smoothness of the stochastic process's sample paths.", "problem": "Let $\\{W(t)\\}_{t \\in \\mathbb{R}}$ be a two-sided standard Wiener process (also called Brownian motion), and fix parameters $A0$ and $\\lambda0$. Define the zero-mean stochastic process $\\{X(t)\\}_{t \\in \\mathbb{R}}$ by the stochastic convolution\n$$\nX(t) \\coloneqq \\int_{-\\infty}^{\\infty} g(t-s)\\,\\mathrm{d}W(s), \\quad \\text{with } g(u) \\coloneqq A \\exp(-\\lambda u^{2}).\n$$\nUsing only core definitions and well-tested facts about stochastic integrals with deterministic integrands, proceed as follows.\n\n1. Starting from the definition of covariance $\\operatorname{Cov}(X(s),X(t)) \\coloneqq \\mathbb{E}[X(s)X(t)]$, derive the covariance kernel $K(s,t)$ of $\\{X(t)\\}$ in closed form, as a function of $t-s$.\n\n2. Recall the definitions: $\\{X(t)\\}$ is mean-square continuous at $t_{0}$ if $\\lim_{h \\to 0} \\mathbb{E}\\!\\left[|X(t_{0}+h)-X(t_{0})|^{2}\\right]=0$, and mean-square differentiable at $t_{0}$ if there exists a random variable $Y(t_{0})$ such that\n$$\n\\lim_{h \\to 0} \\mathbb{E}\\!\\left[\\left|\\frac{X(t_{0}+h)-X(t_{0})}{h}-Y(t_{0})\\right|^{2}\\right]=0.\n$$\nUsing these definitions and properties of the kernel $g$, determine whether $\\{X(t)\\}$ is mean-square continuous and whether it is mean-square differentiable. Identify a candidate for the mean-square derivative process if it exists.\n\n3. Compute explicitly the variance $\\operatorname{Var}(X'(t))$ of the mean-square derivative process (when it exists) in terms of $A$ and $\\lambda$, giving a closed-form analytic expression.\n\nYour final answer must be the closed-form expression for $\\operatorname{Var}(X'(t))$. No numerical rounding is required.", "solution": "The process $\\{X(t)\\}_{t \\in \\mathbb{R}}$ is defined as\n$$\nX(t) = \\int_{-\\infty}^{\\infty} g(t-s)\\,\\mathrm{d}W(s)\n$$\nwhere $g(u) = A \\exp(-\\lambda u^{2})$ with $A0$ and $\\lambda0$. Since the integrand is deterministic, $X(t)$ is a Gaussian process. As $\\mathbb{E}[W(t)]=0$, the mean of $X(t)$ is\n$$\n\\mathbb{E}[X(t)] = \\mathbb{E}\\left[\\int_{-\\infty}^{\\infty} g(t-s)\\,\\mathrm{d}W(s)\\right] = \\int_{-\\infty}^{\\infty} g(t-s)\\,\\mathbb{E}[\\mathrm{d}W(s)] = 0\n$$\nwhich is consistent with the problem statement.\n\n### 1. Derivation of the Covariance Kernel $K(s,t)$\n\nThe covariance kernel is defined as $K(s,t) \\coloneqq \\operatorname{Cov}(X(s),X(t))$. Since the process is zero-mean, this simplifies to $K(s,t) = \\mathbb{E}[X(s)X(t)]$.\nWe substitute the definition of $X(s)$ and $X(t)$:\n$$\nK(s,t) = \\mathbb{E}\\left[ \\left(\\int_{-\\infty}^{\\infty} g(s-u)\\,\\mathrm{d}W(u)\\right) \\left(\\int_{-\\infty}^{\\infty} g(t-v)\\,\\mathrm{d}W(v)\\right) \\right]\n$$\nUsing the bilinearity of covariance and applying Itô's isometry for stochastic integrals with respect to a standard Wiener process, which states that $\\mathbb{E}\\left[\\left(\\int f(u)\\,\\mathrm{d}W(u)\\right)\\left(\\int h(v)\\,\\mathrm{d}W(v)\\right)\\right] = \\int f(u)h(u)\\,\\mathrm{d}u$, we get:\n$$\nK(s,t) = \\int_{-\\infty}^{\\infty} g(s-u)g(t-u)\\,\\mathrm{d}u\n$$\nSubstituting the expression for $g(u)$:\n$$\nK(s,t) = \\int_{-\\infty}^{\\infty} \\left(A \\exp(-\\lambda(s-u)^2)\\right) \\left(A \\exp(-\\lambda(t-u)^2)\\right) \\,\\mathrm{d}u = A^2 \\int_{-\\infty}^{\\infty} \\exp(-\\lambda[(s-u)^2 + (t-u)^2]) \\,\\mathrm{d}u\n$$\nWe analyze the exponent by completing the square with respect to the integration variable $u$:\n\\begin{align*}\n(s-u)^2 + (t-u)^2 = (s^2 - 2su + u^2) + (t^2 - 2tu + u^2) \\\\\n= 2u^2 - 2(s+t)u + (s^2+t^2) \\\\\n= 2\\left(u^2 - (s+t)u\\right) + s^2+t^2 \\\\\n= 2\\left(u - \\frac{s+t}{2}\\right)^2 - 2\\left(\\frac{s+t}{2}\\right)^2 + s^2+t^2 \\\\\n= 2\\left(u - \\frac{s+t}{2}\\right)^2 - \\frac{s^2+2st+t^2}{2} + \\frac{2s^2+2t^2}{2} \\\\\n= 2\\left(u - \\frac{s+t}{2}\\right)^2 + \\frac{s^2-2st+t^2}{2} \\\\\n= 2\\left(u - \\frac{s+t}{2}\\right)^2 + \\frac{(t-s)^2}{2}\n\\end{align*}\nSubstituting this back into the integral expression for $K(s,t)$:\n$$\nK(s,t) = A^2 \\int_{-\\infty}^{\\infty} \\exp\\left(-\\lambda\\left[2\\left(u - \\frac{s+t}{2}\\right)^2 + \\frac{(t-s)^2}{2}\\right]\\right) \\,\\mathrm{d}u\n$$\nThe term involving $(t-s)^2$ is constant with respect to $u$ and can be factored out of the integral:\n$$\nK(s,t) = A^2 \\exp\\left(-\\frac{\\lambda(t-s)^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-2\\lambda\\left(u - \\frac{s+t}{2}\\right)^2\\right) \\,\\mathrm{d}u\n$$\nThe integral is a standard Gaussian integral. Let $v = u - \\frac{s+t}{2}$, so $\\mathrm{d}v = \\mathrm{d}u$. The integral becomes:\n$$\n\\int_{-\\infty}^{\\infty} \\exp(-2\\lambda v^2) \\,\\mathrm{d}v\n$$\nUsing the known result $\\int_{-\\infty}^{\\infty} \\exp(-ax^2)\\,\\mathrm{d}x = \\sqrt{\\frac{\\pi}{a}}$ for $a0$, with $a = 2\\lambda$, the integral evaluates to $\\sqrt{\\frac{\\pi}{2\\lambda}}$.\nTherefore, the covariance kernel is:\n$$\nK(s,t) = A^2 \\sqrt{\\frac{\\pi}{2\\lambda}} \\exp\\left(-\\frac{\\lambda(t-s)^2}{2}\\right)\n$$\nThis kernel depends only on the difference $\\tau = t-s$, which indicates that $\\{X(t)\\}$ is a wide-sense stationary process. Let us define $K(\\tau) = K(t, t-\\tau)$, so $K(\\tau) = A^2 \\sqrt{\\frac{\\pi}{2\\lambda}} \\exp\\left(-\\frac{\\lambda\\tau^2}{2}\\right)$.\n\n### 2. Mean-Square Continuity and Differentiability\n\nFor a wide-sense stationary process, mean-square (m-s) continuity is equivalent to the continuity of its covariance function $K(\\tau)$ at $\\tau=0$. The function $K(\\tau)$ is a Gaussian function, which is infinitely differentiable and hence continuous for all $\\tau \\in \\mathbb{R}$. Specifically, $\\lim_{\\tau \\to 0} K(\\tau) = K(0) = A^2 \\sqrt{\\frac{\\pi}{2\\lambda}}$. Thus, the process $\\{X(t)\\}$ is mean-square continuous.\n\nA wide-sense stationary process is mean-square differentiable if and only if its covariance function $K(\\tau)$ is twice differentiable at $\\tau=0$. Let's compute the first and second derivatives of $K(\\tau)$ with respect to $\\tau$. For convenience, let $C = A^2 \\sqrt{\\frac{\\pi}{2\\lambda}}$.\n$$\nK(\\tau) = C \\exp\\left(-\\frac{\\lambda\\tau^2}{2}\\right)\n$$\nThe first derivative is:\n$$\nK'(\\tau) = \\frac{\\mathrm{d}K}{\\mathrm{d}\\tau} = C \\left(-\\frac{\\lambda}{2} \\cdot 2\\tau\\right) \\exp\\left(-\\frac{\\lambda\\tau^2}{2}\\right) = -C\\lambda\\tau \\exp\\left(-\\frac{\\lambda\\tau^2}{2}\\right)\n$$\nThe second derivative is:\n$$\nK''(\\tau) = \\frac{\\mathrm{d}^2K}{\\mathrm{d}\\tau^2} = -C\\lambda \\left[ \\exp\\left(-\\frac{\\lambda\\tau^2}{2}\\right) + \\tau (-\\lambda\\tau) \\exp\\left(-\\frac{\\lambda\\tau^2}{2}\\right) \\right] = -C\\lambda (1 - \\lambda\\tau^2) \\exp\\left(-\\frac{\\lambda\\tau^2}{2}\\right)\n$$\nSince $K''(\\tau)$ exists for all $\\tau$, it certainly exists at $\\tau=0$. Its value is $K''(0) = -C\\lambda (1 - 0) \\exp(0) = -C\\lambda$. The existence of a finite second derivative at the origin confirms that the process $\\{X(t)\\}$ is mean-square differentiable. The mean-square derivative process, denoted $\\{X'(t)\\}$, exists. It is given by $X'(t) = \\int_{-\\infty}^{\\infty} g'(t-s)\\,\\mathrm{d}W(s)$.\n\n### 3. Variance of the Mean-Square Derivative\n\nThe derivative process $\\{X'(t)\\}$ is also a zero-mean process, since differentiation is a linear operation and expectation can be interchanged with the mean-square limit:\n$$\n\\mathbb{E}[X'(t)] = \\mathbb{E}\\left[ \\underset{h\\to 0}{\\text{l.i.m.}} \\frac{X(t+h)-X(t)}{h} \\right] = \\underset{h\\to 0}{\\lim} \\frac{\\mathbb{E}[X(t+h)]-\\mathbb{E}[X(t)]}{h} = \\lim_{h\\to 0} \\frac{0-0}{h} = 0\n$$\nTherefore, its variance is $\\operatorname{Var}(X'(t)) = \\mathbb{E}[(X'(t))^2] = \\operatorname{Cov}(X'(t), X'(t))$.\nThe covariance kernel of the derivative process, $K_{X'X'}(s,t) = \\operatorname{Cov}(X'(s),X'(t))$, is related to the kernel of the original process by:\n$$\nK_{X'X'}(s,t) = \\frac{\\partial^2}{\\partial s \\partial t} K(s,t)\n$$\nSince $K(s,t) = K(t-s)$, we have $\\frac{\\partial}{\\partial t} K(t-s) = K'(t-s)$ and $\\frac{\\partial^2}{\\partial s \\partial t} K(t-s) = \\frac{\\partial}{\\partial s} [K'(t-s)] = K''(t-s) \\cdot (-1) = -K''(t-s)$.\nSo, $\\operatorname{Cov}(X'(s), X'(t)) = -K''(t-s)$.\nThe variance of $X'(t)$ is found by setting $s=t$:\n$$\n\\operatorname{Var}(X'(t)) = \\operatorname{Cov}(X'(t), X'(t)) = -K''(t-t) = -K''(0)\n$$\nWe have already calculated $K''(0) = -C\\lambda$.\n$$\n\\operatorname{Var}(X'(t)) = -(-C\\lambda) = C\\lambda\n$$\nSubstituting back the expression for $C = A^2 \\sqrt{\\frac{\\pi}{2\\lambda}}$:\n$$\n\\operatorname{Var}(X'(t)) = \\left(A^2 \\sqrt{\\frac{\\pi}{2\\lambda}}\\right) \\lambda = A^2 \\lambda \\sqrt{\\frac{\\pi}{2\\lambda}}\n$$\nWe can simplify this expression:\n$$\n\\operatorname{Var}(X'(t)) = A^2 \\sqrt{\\lambda^2 \\cdot \\frac{\\pi}{2\\lambda}} = A^2 \\sqrt{\\frac{\\pi\\lambda^2}{2\\lambda}} = A^2 \\sqrt{\\frac{\\pi\\lambda}{2}}\n$$\nThis is the closed-form analytic expression for the variance of the mean-square derivative process.", "answer": "$$\n\\boxed{A^2 \\sqrt{\\frac{\\pi\\lambda}{2}}}\n$$", "id": "3047391"}]}