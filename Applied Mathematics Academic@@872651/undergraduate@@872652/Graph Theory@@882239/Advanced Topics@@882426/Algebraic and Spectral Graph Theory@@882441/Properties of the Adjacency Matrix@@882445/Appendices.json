{"hands_on_practices": [{"introduction": "The adjacency matrix is the foundational tool for representing a graph algebraically. This first practice explores the most direct relationship: how the properties of a single vertex are encoded within the matrix. By considering an isolated vertex, which has no connections, you will see how this translates into a distinct and easily recognizable pattern, solidifying your understanding of how rows and columns correspond to vertices and their adjacencies. [@problem_id:1529032]", "problem": "Consider a simple, undirected graph $G$ with a set of $n$ vertices, $V = \\{v_1, v_2, \\dots, v_n\\}$. The structure of this graph is represented by its $n \\times n$ adjacency matrix $A$, where the entry $A_{ij}$ is 1 if an edge exists between vertex $v_i$ and vertex $v_j$, and 0 otherwise. Since the graph is simple, there are no edges from a vertex to itself, meaning all diagonal entries $A_{ii}$ are zero.\n\nSuppose that the graph $G$ contains at least one isolated vertex. An isolated vertex is defined as a vertex that is not connected to any other vertex in the graph. By relabeling, or permuting, the vertices, we can obtain a new adjacency matrix $A'$ that is a row and column permutation of the original matrix $A$.\n\nWhich of the following statements describes a structural feature that is guaranteed to be present in the adjacency matrix $A'$ after an appropriate permutation of the vertices?\n\nA. The matrix $A'$ will be a diagonal matrix.\n\nB. At least one row and its corresponding column in $A'$ will consist entirely of zeros.\n\nC. The trace of the matrix $A'$ (the sum of its diagonal elements) will be non-zero.\n\nD. The matrix $A'$ will be symmetric with respect to its anti-diagonal.\n\nE. The first row of the matrix $A'$ will consist entirely of ones, except for the diagonal entry $A'_{11}$.", "solution": "Let $G$ be a simple, undirected graph on vertex set $V=\\{v_{1},\\dots,v_{n}\\}$ with adjacency matrix $A$. For a simple graph, $A$ is symmetric and has a zero diagonal, i.e., $A_{ii}=0$ for all $i$.\n\nAn isolated vertex $v_{k}$ is one with no incident edges. By the definition of the adjacency matrix, this implies that the $k$-th row and $k$-th column of $A$ are entirely zero:\n$$\nA_{kj}=0 \\text{ for all } j \\in \\{1,\\dots,n\\}, \\quad A_{ik}=0 \\text{ for all } i \\in \\{1,\\dots,n\\}.\n$$\nRelabeling vertices corresponds to conjugation of $A$ by a permutation matrix $P$, yielding\n$$\nA' = P A P^{T}.\n$$\nSuch a permutation can place the isolated vertex at any desired position, for instance the first position, which guarantees that in $A'$ there is at least one row and its corresponding column consisting entirely of zeros.\n\nNow assess each option:\n- A. If $A'$ were diagonal and the graph is simple, then all diagonal entries are zero, so $A'$ would be the zero matrix, implying all vertices are isolated. The existence of at least one isolated vertex does not guarantee this. Hence false.\n- B. As shown, there exists a permutation placing an isolated vertex at some index, making the corresponding row and column all zeros. Hence true.\n- C. The trace of $A'$ equals the trace of $A$ and for a simple graph is $\\sum_{i=1}^{n} A_{ii}=0$, so it is not non-zero. Hence false.\n- D. Symmetry with respect to the anti-diagonal is not a general property of adjacency matrices of undirected graphs and is not ensured by any permutation. Hence false.\n- E. The first row being all ones except the diagonal entry would mean the first vertex is adjacent to every other vertex, which is not implied by the existence of an isolated vertex. Hence false.\n\nTherefore, the guaranteed structural feature after an appropriate permutation is that at least one row and its corresponding column are entirely zeros.", "answer": "$$\\boxed{B}$$", "id": "1529032"}, {"introduction": "Moving from single vertices to entire sets, we can uncover larger patterns in a graph's structure. Bipartite graphs, which are fundamental in modeling relationships between two distinct groups, provide a perfect example of this. This exercise challenges you to visualize how partitioning the vertices of a complete bipartite graph organizes its adjacency matrix into a clear block structure, a powerful concept for analyzing large and complex networks. [@problem_id:1529036]", "problem": "Consider a simple, undirected, complete bipartite graph, denoted as $K_{m,n}$. The vertex set of this graph, $V$, is partitioned into two disjoint sets, $U$ and $W$, such that $|U|=m$ and $|W|=n$, with $m, n \\ge 1$. In $K_{m,n}$, an edge exists between two vertices if and only if one vertex is in set $U$ and the other is in set $W$.\n\nThe adjacency matrix $A$ of this graph is an $(m+n) \\times (m+n)$ matrix. It is constructed by first ordering the $m$ vertices of set $U$, and then ordering the $n$ vertices of set $W$. This ordering partitions the matrix $A$ into a $2 \\times 2$ block matrix structure.\n\nLet $O_{r,c}$ denote the $r \\times c$ matrix of all zeros, and let $J_{r,c}$ denote the $r \\times c$ matrix of all ones.\n\nWhich of the following block matrices correctly represents the adjacency matrix $A$ for the graph $K_{m,n}$ with the specified vertex ordering?\n\nA.\n$$\n\\begin{pmatrix}\nO_{m,m} & J_{m,n} \\\\\nJ_{n,m} & O_{n,n}\n\\end{pmatrix}\n$$\n\nB.\n$$\n\\begin{pmatrix}\nJ_{m,m} & O_{m,n} \\\\\nO_{n,m} & J_{n,n}\n\\end{pmatrix}\n$$\n\nC.\n$$\n\\begin{pmatrix}\nJ_{m,m} & J_{m,n} \\\\\nJ_{n,m} & J_{n,n}\n\\end{pmatrix}\n$$\n\nD.\n$$\n\\begin{pmatrix}\nO_{m,m} & J_{m,n} \\\\\nO_{n,m} & O_{n,n}\n\\end{pmatrix}\n$$", "solution": "By definition of the adjacency matrix for a simple undirected graph, if the vertices are ordered with all $m$ vertices of $U$ first followed by the $n$ vertices of $W$, then $A_{ij}=1$ if and only if the $i$-th and $j$-th vertices are adjacent, and $A_{ij}=0$ otherwise. Since the graph is simple, there are no self-loops, so diagonal entries are zero.\n\nIn the complete bipartite graph $K_{m,n}$, edges exist only between $U$ and $W$ and not within $U$ or within $W$. Therefore:\n- The $U$–$U$ block (upper-left) is an $m \\times m$ zero matrix, namely $O_{m,m}$.\n- The $W$–$W$ block (lower-right) is an $n \\times n$ zero matrix, namely $O_{n,n}$.\n- The $U$–$W$ block (upper-right) has a $1$ for every pair $(u,w)$ with $u \\in U$ and $w \\in W$, hence it is the $m \\times n$ all-ones matrix $J_{m,n}$.\n- Because the graph is undirected, the adjacency matrix is symmetric, so the $W$–$U$ block (lower-left) is the transpose of the $U$–$W$ block and equals $J_{n,m}$.\n\nThus the correct block form is\n$$\n\\begin{pmatrix}\nO_{m,m} & J_{m,n} \\\\\nJ_{n,m} & O_{n,n}\n\\end{pmatrix},\n$$\nwhich matches option A.\n\nOptions B and C are incorrect because they place ones within the $U$–$U$ and $W$–$W$ blocks, implying edges inside each part, which do not exist in a bipartite graph. Option C also incorrectly includes ones on the diagonal, contradicting simplicity. Option D is incorrect because it makes the matrix non-symmetric by placing $J_{m,n}$ in the upper-right but $O_{n,m}$ in the lower-left, violating the undirected property.", "answer": "$$\\boxed{A}$$", "id": "1529036"}, {"introduction": "The connection between linear algebra and graph theory is remarkably powerful, allowing us to use algebraic properties to deduce graphical structures. This problem presents a fascinating puzzle: what does a graph look like if its adjacency matrix must also satisfy the strict definition of a permutation matrix? By carefully combining the constraints of a simple graph with the properties of a permutation matrix, you can uncover the precise structure of the entire graph, showcasing a beautiful instance of algebraic graph theory at work. [@problem_id:1529030]", "problem": "Let $G$ be a simple graph with $n$ vertices, where $n$ is an even integer such that $n \\ge 2$. A simple graph is an undirected graph containing no loops (edges from a vertex to itself) and no multiple edges between the same pair of vertices. The adjacency matrix $A$ of $G$ is an $n \\times n$ matrix where the entry $A_{ij}$ is 1 if there is an edge connecting vertex $i$ and vertex $j$, and 0 otherwise. A permutation matrix is a square binary matrix that has exactly one entry equal to 1 in each row and each column, and 0s elsewhere.\n\nSuppose that the adjacency matrix $A$ of the graph $G$ is also a permutation matrix. Which of the following options provides the most accurate description of the structure of graph $G$?\n\nA. A cycle graph on $n$ vertices, denoted $C_n$.\n\nB. The complete graph on $n$ vertices, denoted $K_n$.\n\nC. A disjoint union of $\\frac{n}{2}$ copies of the complete graph on two vertices, $K_2$.\n\nD. A path graph on $n$ vertices, denoted $P_n$.\n\nE. The empty graph on $n$ vertices (a graph with no edges).", "solution": "Because $G$ is a simple undirected graph, its adjacency matrix $A$ satisfies $A=A^{T}$ and $A_{ii}=0$ for all $i$. Because $A$ is also a permutation matrix, each row and each column contains exactly one entry equal to $1$, and all others are $0$.\n\nFor each vertex $i$, the degree satisfies\n$$\n\\deg(i)=\\sum_{j=1}^{n}A_{ij}=1,\n$$\nso every vertex has degree $1$. Therefore $G$ is a $1$-regular graph.\n\nA finite simple connected $1$-regular graph must be $K_{2}$. Indeed, if a connected component has a vertex $v$ adjacent to a unique neighbor $u$, then $u$ also has degree $1$ and hence is adjacent only to $v$, so no other vertices can be connected to this component. Thus every connected component is $K_{2}$.\n\nSince $n$ is even, the entire graph is a disjoint union of $\\frac{n}{2}$ copies of $K_{2}$.\n\nEquivalently, viewing $A$ as a permutation matrix, the conditions $A=A^{T}$ and $A_{ii}=0$ imply that the permutation is an involution without fixed points, i.e., a product of $\\frac{n}{2}$ disjoint transpositions, which again yields a disjoint union of $\\frac{n}{2}$ edges.\n\nHence the correct description is a disjoint union of $\\frac{n}{2}$ copies of $K_{2}$.", "answer": "$$\\boxed{C}$$", "id": "1529030"}]}