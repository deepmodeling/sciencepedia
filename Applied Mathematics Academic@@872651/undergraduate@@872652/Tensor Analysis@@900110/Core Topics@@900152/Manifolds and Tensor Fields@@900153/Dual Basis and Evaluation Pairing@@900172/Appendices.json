{"hands_on_practices": [{"introduction": "The concept of a dual basis is fundamental to understanding tensors and their transformations. This first practice exercise will ground the abstract definition of a dual basis in the familiar setting of $\\mathbb{R}^2$. You will apply the defining property of dual vectors, $\\omega^i(e_j) = \\delta^i_j$, to develop a concrete computational method for finding the components of a covector in the dual basis. [@problem_id:1508583]", "problem": "Consider the real vector space $V = \\mathbb{R}^2$. A basis for this space is given by the set of vectors $\\{e_1, e_2\\}$, where the components of these vectors with respect to the standard Cartesian basis are $e_1 = (3, 1)$ and $e_2 = (2, 2)$. Let $\\{\\omega^1, \\omega^2\\}$ be the basis for the dual space $V^*$ that is dual to the basis $\\{e_1, e_2\\}$. Determine the components of the covector $\\omega^1$ and express your answer as a row matrix.", "solution": "Let $e_{1}, e_{2} \\in \\mathbb{R}^{2}$ be given in the standard Cartesian basis by $e_{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $e_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$. Form the matrix $E$ whose columns are the basis vectors:\n$$\nE = \\begin{pmatrix} 3 & 2 \\\\ 1 & 2 \\end{pmatrix}.\n$$\nLet $\\omega^{1}, \\omega^{2}$ be the dual basis covectors, represented as row vectors, and collect them as the rows of a matrix $W$:\n$$\nW = \\begin{pmatrix} \\omega^{1} \\\\ \\omega^{2} \\end{pmatrix}.\n$$\nThe duality condition $\\omega^{i}(e_{j}) = \\delta^{i}_{j}$ is equivalent to the matrix equation\n$$\nW E = I.\n$$\nTherefore,\n$$\nW = E^{-1}.\n$$\nCompute $E^{-1}$. The determinant is\n$$\n\\det(E) = 3 \\cdot 2 - 2 \\cdot 1 = 4,\n$$\nso by the $2 \\times 2$ inverse formula,\n$$\nE^{-1} = \\frac{1}{4} \\begin{pmatrix} 2 & -2 \\\\ -1 & 3 \\end{pmatrix}.\n$$\nThus the first row, which gives the components of $\\omega^{1}$ in the standard basis, is\n$$\n\\omega^{1} = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix}.\n$$\nThis row vector satisfies $\\omega^{1}(e_{1}) = 1$ and $\\omega^{1}(e_{2}) = 0$, as required.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & -\\frac{1}{2}\\end{pmatrix}}$$", "id": "1508583"}, {"introduction": "Having established how to compute a dual basis in $\\mathbb{R}^2$, we now explore how these concepts generalize to more abstract vector spaces, such as spaces of functions. This problem shifts our perspective to the space of linear polynomials, $P_1(\\mathbb{R})$, treating polynomials themselves as vectors. Here, you will see that the action of a dual basis covector is to elegantly extract the component of a given vector with respect to its corresponding basis element. [@problem_id:1508565]", "problem": "Consider the vector space $V = P_1(\\mathbb{R})$, which is the space of all real polynomials of degree at most one. A basis for this space is given by the set $\\mathcal{B} = \\{e_1, e_2\\}$, where the basis vectors are the polynomials $e_1(x) = 1 + x$ and $e_2(x) = 1 - x$. Let $V^*$ be the dual space of $V$, and let $\\{\\omega^1, \\omega^2\\}$ be the dual basis corresponding to $\\mathcal{B}$. The dual basis is defined by the property $\\omega^i(e_j) = \\delta^i_j$, where $\\delta^i_j$ is the Kronecker delta.\n\nCalculate the value of the functional $\\omega^1$ acting on the polynomial $p(x) = 5 + 3x$.", "solution": "We represent $p(x)=5+3x$ in the basis $\\mathcal{B}=\\{e_{1},e_{2}\\}$, where $e_{1}(x)=1+x$ and $e_{2}(x)=1-x$. Let $a,b\\in\\mathbb{R}$ such that\n$$\np=a e_{1}+b e_{2}=a(1+x)+b(1-x)=(a+b)+(a-b)x.\n$$\nEquating coefficients with $5+3x$ gives the system\n$$\na+b=5,\\qquad a-b=3.\n$$\nAdding yields $2a=8$, hence $a=4$. Then $b=1$ from $a+b=5$.\n\nBy the defining property of the dual basis, for any $v=c^{1}e_{1}+c^{2}e_{2}$ we have $\\omega^{1}(v)=c^{1}$. Since $p=4e_{1}+1\\cdot e_{2}$, it follows that\n$$\n\\omega^{1}(p)=4.\n$$", "answer": "$$\\boxed{4}$$", "id": "1508565"}, {"introduction": "This final practice inverts the typical problem, challenging you to find the vector basis that is dual to a given basis of functionals. It reinforces the deep symmetry of the relationship between a vector space and its dual, and introduces the important idea that linear functionals can be defined in sophisticated ways, such as through integration. Solving this will solidify your understanding of the reciprocal relationship between a basis and its dual. [@problem_id:1508617]", "problem": "Consider the real vector space $V=P_1(\\mathbb{R})$, which consists of all polynomials of degree at most 1, having the general form $p(x) = ax+b$ for real coefficients $a$ and $b$. Let $V^*$ denote the dual space of $V$, which is the space of all linear functionals from $V$ to $\\mathbb{R}$.\n\nA specific basis for this dual space, denoted $\\{\\omega^1, \\omega^2\\}$, is defined by the action of its elements on any polynomial $p(x) \\in V$ as follows:\n$$ \\omega^1(p) = \\int_{-1}^{1} p(x) dx $$\n$$ \\omega^2(p) = \\int_{-1}^{1} x p(x) dx $$\nYour task is to determine the basis $\\{p_1(x), p_2(x)\\}$ of the original vector space $V$ that is dual to the given basis $\\{\\omega^1, \\omega^2\\}$.\n\nPresent your final answer as a row matrix containing the two polynomials in the order $[p_1(x) \\quad p_2(x)]$.", "solution": "Let the vector space be $V = P_1(\\mathbb{R})$ and the basis for its dual space be $\\{\\omega^1, \\omega^2\\}$. We are looking for the basis $\\{p_1(x), p_2(x)\\}$ in $V$ which is dual to $\\{\\omega^1, \\omega^2\\}$.\n\nBy the definition of a dual basis, the basis vectors $p_i \\in V$ and the dual basis vectors (functionals) $\\omega^j \\in V^*$ must satisfy the relation:\n$$ \\omega^j(p_i) = \\delta^j_i $$\nwhere $\\delta^j_i$ is the Kronecker delta, which is 1 if $i=j$ and 0 if $i \\neq j$.\n\nFor our 2-dimensional space, we have four conditions to satisfy:\n1. $\\omega^1(p_1) = 1$\n2. $\\omega^2(p_1) = 0$\n3. $\\omega^1(p_2) = 0$\n4. $\\omega^2(p_2) = 1$\n\nFirst, let's evaluate the action of the functionals on a general polynomial $p(x) = ax+b$ in $V$.\n\nFor $\\omega^1$:\n$$ \\omega^1(p) = \\int_{-1}^{1} (ax+b) dx = \\left[ \\frac{a}{2}x^2 + bx \\right]_{-1}^{1} $$\n$$ \\omega^1(p) = \\left(\\frac{a}{2}(1)^2 + b(1)\\right) - \\left(\\frac{a}{2}(-1)^2 + b(-1)\\right) = \\left(\\frac{a}{2}+b\\right) - \\left(\\frac{a}{2}-b\\right) = 2b $$\n\nFor $\\omega^2$:\n$$ \\omega^2(p) = \\int_{-1}^{1} x(ax+b) dx = \\int_{-1}^{1} (ax^2+bx) dx = \\left[ \\frac{a}{3}x^3 + \\frac{b}{2}x^2 \\right]_{-1}^{1} $$\n$$ \\omega^2(p) = \\left(\\frac{a}{3}(1)^3 + \\frac{b}{2}(1)^2\\right) - \\left(\\frac{a}{3}(-1)^3 + \\frac{b}{2}(-1)^2\\right) = \\left(\\frac{a}{3}+\\frac{b}{2}\\right) - \\left(-\\frac{a}{3}+\\frac{b}{2}\\right) = \\frac{2a}{3} $$\n\nNow we can find the polynomials $p_1(x)$ and $p_2(x)$.\n\nLet $p_1(x) = a_1x + b_1$. We apply the conditions for $p_1$:\n1. $\\omega^1(p_1) = 1 \\implies 2b_1 = 1 \\implies b_1 = \\frac{1}{2}$\n2. $\\omega^2(p_1) = 0 \\implies \\frac{2a_1}{3} = 0 \\implies a_1 = 0$\nThus, the first basis polynomial is $p_1(x) = 0 \\cdot x + \\frac{1}{2} = \\frac{1}{2}$.\n\nNext, let $p_2(x) = a_2x + b_2$. We apply the conditions for $p_2$:\n1. $\\omega^1(p_2) = 0 \\implies 2b_2 = 0 \\implies b_2 = 0$\n2. $\\omega^2(p_2) = 1 \\implies \\frac{2a_2}{3} = 1 \\implies a_2 = \\frac{3}{2}$\nThus, the second basis polynomial is $p_2(x) = \\frac{3}{2}x + 0 = \\frac{3}{2}x$.\n\nThe dual basis is $\\{p_1(x), p_2(x)\\} = \\{\\frac{1}{2}, \\frac{3}{2}x\\}$.\nThe problem asks for the answer to be presented as a row matrix $[p_1(x) \\quad p_2(x)]$. This gives us $[\\frac{1}{2} \\quad \\frac{3}{2}x]$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{3}{2}x \\end{pmatrix}}$$", "id": "1508617"}]}