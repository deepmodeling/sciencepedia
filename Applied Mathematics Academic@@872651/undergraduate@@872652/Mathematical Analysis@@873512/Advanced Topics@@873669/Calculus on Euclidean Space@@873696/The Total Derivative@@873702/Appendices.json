{"hands_on_practices": [{"introduction": "The total derivative of a vector-valued function is best understood through its matrix representation, the Jacobian matrix. This first exercise provides foundational practice in computing this matrix, which is a direct generalization of the derivative from single-variable calculus. By systematically calculating the partial derivatives of each component function, you will construct the matrix that locally approximates the function's behavior [@problem_id:37815].", "problem": "Consider a vector-valued function $f$ that maps points from a 3-dimensional space to a 2-dimensional space, i.e., $f: \\mathbb{R}^3 \\to \\mathbb{R}^2$. The total derivative of such a function at a point $(x, y, z)$ is represented by a matrix known as the Jacobian matrix, denoted as $J_f(x, y, z)$. This matrix is the best linear approximation of the function near that point.\n\nThe Jacobian matrix for a function $f(x,y,z) = (f_1(x,y,z), f_2(x,y,z))$ is a $2 \\times 3$ matrix given by:\n$$\nJ_f(x,y,z) = \\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} & \\frac{\\partial f_1}{\\partial z} \\\\\n\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} & \\frac{\\partial f_2}{\\partial z}\n\\end{pmatrix}\n$$\nwhere $f_1$ and $f_2$ are the component functions of $f$.\n\nGiven the function:\n$$\nf(x, y, z) = (x\\cos(y) + z^2, y e^{xz})\n$$\nDerive its Jacobian matrix, $J_f(x, y, z)$.", "solution": "We have $f(x,y,z)=(f_1,f_2)$ with \n$$f_1(x,y,z)=x\\cos(y)+z^2,\\quad f_2(x,y,z)=y\\,e^{xz}.$$\nBy definition,\n$$J_f(x,y,z)=\n\\begin{pmatrix}\n\\partial_x f_1 & \\partial_y f_1 & \\partial_z f_1\\\\\n\\partial_x f_2 & \\partial_y f_2 & \\partial_z f_2\n\\end{pmatrix}.$$\nCompute each entry:\n\n1. For $f_1$:\n   $$\\partial_x f_1=\\cos(y),\\quad \\partial_y f_1=-x\\sin(y),\\quad \\partial_z f_1=2z.$$\n\n2. For $f_2$:\n   $$\\partial_x f_2=y\\,\\partial_x(e^{xz})=y\\,(z e^{xz})=yz\\,e^{xz},$$\n   $$\\partial_y f_2=e^{xz},$$\n   $$\\partial_z f_2=y\\,\\partial_z(e^{xz})=y\\,(x e^{xz})=xy\\,e^{xz}.$$\n\nHence the Jacobian matrix is\n$$\nJ_f(x,y,z)=\n\\begin{pmatrix}\n\\cos(y) & -x\\sin(y) & 2z\\\\\nyz\\,e^{xz} & e^{xz} & xy\\,e^{xz}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\cos(y)&-x\\sin(y)&2z\\\\yz\\,e^{xz}&e^{xz}&xy\\,e^{xz}\\end{pmatrix}}$$", "id": "37815"}, {"introduction": "After learning how to compute the Jacobian matrix, the next step is to understand what it *does*. The total derivative at a point is not just a static array of numbers; it is a linear map that describes how the function transforms infinitesimal vectors. This practice [@problem_id:37812] makes this concept tangible by asking you to apply the total derivative at a specific point to a direction vector, directly illustrating its role as the best linear approximation of the function's change.", "problem": "Consider a function $f: \\mathbb{R}^2 \\to \\mathbb{R}^2$ defined by:\n$$\nf(x,y) = (f_1(x,y), f_2(x,y)) = (xy, x-y)\n$$\nThe total derivative of $f$ at a point $\\vec{p} = (x_0, y_0)$, denoted as $Df_{\\vec{p}}$, is a linear transformation that can be represented by the Jacobian matrix of $f$ evaluated at $\\vec{p}$, which we call $J_f(\\vec{p})$. The action of this total derivative on a vector $\\vec{v}$ is given by the matrix-vector product $Df_{\\vec{p}}(\\vec{v}) = J_f(\\vec{p}) \\vec{v}$.\n\nGiven the point $\\vec{p} = (2, 1)$ and the vector $\\vec{v} = (3, -1)$, compute the resulting vector from the action of the total derivative of $f$ at $\\vec{p}$ on the vector $\\vec{v}$. Express your final answer as a column vector.", "solution": "The problem asks for the computation of $Df_{\\vec{p}}(\\vec{v})$, where $f(x,y) = (xy, x-y)$, $\\vec{p} = (2, 1)$, and $\\vec{v} = (3, -1)$.\n\nThe total derivative's action on a vector is given by the product of the Jacobian matrix and the vector:\n$$\nDf_{\\vec{p}}(\\vec{v}) = J_f(\\vec{p}) \\vec{v}\n$$\n\nFirst, we must find the Jacobian matrix of the function $f(x,y)$. The components of $f$ are $f_1(x,y) = xy$ and $f_2(x,y) = x-y$. The Jacobian matrix, $J_f(x,y)$, is a $2 \\times 2$ matrix of the partial derivatives of $f$:\n$$\nJ_f(x,y) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{pmatrix}\n$$\n\nWe compute each partial derivative:\n- $\\frac{\\partial f_1}{\\partial x} = \\frac{\\partial}{\\partial x}(xy) = y$\n- $\\frac{\\partial f_1}{\\partial y} = \\frac{\\partial}{\\partial y}(xy) = x$\n- $\\frac{\\partial f_2}{\\partial x} = \\frac{\\partial}{\\partial x}(x-y) = 1$\n- $\\frac{\\partial f_2}{\\partial y} = \\frac{\\partial}{\\partial y}(x-y) = -1$\n\nSubstituting these into the Jacobian matrix gives:\n$$\nJ_f(x,y) = \\begin{pmatrix} y & x \\\\ 1 & -1 \\end{pmatrix}\n$$\n\nNext, we evaluate this matrix at the specified point $\\vec{p} = (x_0, y_0) = (2, 1)$:\n$$\nJ_f(2,1) = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix}\n$$\n\nNow, we can compute the action of the total derivative on the vector $\\vec{v} = (3, -1)$. We can write $\\vec{v}$ as a column vector $\\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$.\n$$\nDf_{(2,1)}(\\vec{v}) = J_f(2,1) \\vec{v} = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\n$$\n\nFinally, we perform the matrix-vector multiplication:\n$$\n\\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(3) + (2)(-1) \\\\ (1)(3) + (-1)(-1) \\end{pmatrix} = \\begin{pmatrix} 3 - 2 \\\\ 3 + 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\n$$\n\nThe resulting vector is $(1, 4)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}}\n$$", "id": "37812"}, {"introduction": "While many functions encountered in introductory calculus are differentiable everywhere, it is crucial to identify points where this property fails. This exercise [@problem_id:2330050] challenges you to analyze a function constructed by taking the maximum of two smooth functions, a common scenario in optimization and engineering. Differentiability can fail where the two underlying functions meet, and this problem guides you to discover the precise geometric locus of points where the function has a 'kink' or 'crease', reinforcing the idea that differentiability requires a seamless match of local linear approximations.", "problem": "Consider the function $h: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by the expression:\n$$h(x, y) = \\max(x^2+y^2, 2x+2y-1)$$\nwhere $\\max(a, b)$ denotes the greater of the two real numbers $a$ and $b$.\n\nThe set of all points $(x,y)$ in the Cartesian plane where this function $h$ is not differentiable forms a specific geometric shape. Which of the following options correctly describes this set?\n\nA. A circle of radius 1 centered at (1, 1).\n\nB. A circle of radius $\\sqrt{2}$ centered at (1, 1).\n\nC. The single point (1, 1).\n\nD. The line $y = x$.\n\nE. The empty set (the function is differentiable everywhere).\n\nF. A parabola.", "solution": "Let $f(x,y) = x^2+y^2$ and $g(x,y) = 2x+2y-1$. The function is then $h(x, y) = \\max(f(x,y), g(x,y))$.\n\nBoth $f(x,y)$ and $g(x,y)$ are polynomials in $x$ and $y$. As such, they are infinitely differentiable (smooth) everywhere on $\\mathbb{R}^2$.\n\nThe function $h(x,y)$ is defined piecewise.\n- In the region where $f(x,y) > g(x,y)$, we have $h(x,y) = f(x,y)$. Since $f$ is differentiable, $h$ is differentiable in this region.\n- In the region where $f(x,y) < g(x,y)$, we have $h(x,y) = g(x,y)$. Since $g$ is differentiable, $h$ is differentiable in this region.\n\nThe only points where differentiability might fail are on the boundary between these two regions, that is, the set of points where $f(x,y) = g(x,y)$. Let's identify this set.\n$$x^2+y^2 = 2x+2y-1$$\nWe can rearrange this equation to better understand its geometric form by completing the square for both $x$ and $y$:\n$$x^2 - 2x + y^2 - 2y = -1$$\n$$(x^2 - 2x + 1) + (y^2 - 2y + 1) = -1 + 1 + 1$$\n$$(x-1)^2 + (y-1)^2 = 1$$\nThis is the equation of a circle, let's call it $C$, with its center at $(1,1)$ and a radius of $1$.\n\nNow, we must investigate the differentiability of $h$ at the points on this circle $C$. A function of the form $\\max(f, g)$ is differentiable at a point $\\mathbf{x}_0$ where $f(\\mathbf{x}_0) = g(\\mathbf{x}_0)$ if and only if both $f$ and $g$ are differentiable at $\\mathbf{x}_0$ and their gradients are equal, i.e., $\\nabla f(\\mathbf{x}_0) = \\nabla g(\\mathbf{x}_0)$.\n\nWe have already established that $f$ and $g$ are differentiable everywhere. Let's compute their gradients.\nThe gradient of $f(x,y)$ is:\n$$\\nabla f(x,y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (2x, 2y)$$\nThe gradient of $g(x,y)$ is:\n$$\\nabla g(x,y) = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right) = (2, 2)$$\n\nFor $h$ to be differentiable at a point $(x,y)$ on the circle $C$, the gradients must be equal at that point:\n$$\\nabla f(x,y) = \\nabla g(x,y)$$\n$$(2x, 2y) = (2, 2)$$\nThis equality holds only if $2x=2$ and $2y=2$, which means $x=1$ and $y=1$. So, the only potential point of differentiability on the boundary curve is $(1,1)$.\n\nHowever, for this to be a differentiability point of $h$, this point must lie on the boundary itself. Let's check if the point $(1,1)$ lies on the circle $C$ defined by $(x-1)^2 + (y-1)^2 = 1$:\n$$(1-1)^2 + (1-1)^2 = 0^2 + 0^2 = 0$$\nSince $0 \\neq 1$, the point $(1,1)$ is not on the circle $C$.\n\nThis means that for every point $(x,y)$ on the circle $C$, we have $f(x,y)=g(x,y)$ but $\\nabla f(x,y) \\neq \\nabla g(x,y)$. Consequently, the function $h(x,y)$ is not differentiable at any point on the circle $C$.\n\nThe set of points where $h$ is not differentiable is precisely the circle $(x-1)^2 + (y-1)^2 = 1$, which is a circle of radius 1 centered at (1, 1). This corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "2330050"}]}