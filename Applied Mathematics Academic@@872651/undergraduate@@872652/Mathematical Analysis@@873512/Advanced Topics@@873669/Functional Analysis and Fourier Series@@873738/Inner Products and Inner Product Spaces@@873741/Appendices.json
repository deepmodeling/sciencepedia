{"hands_on_practices": [{"introduction": "Before we can use an inner product, we must be certain that it is a valid one. This exercise challenges you to act as a mathematical quality inspector, testing a proposed function against the fundamental axioms that define an inner product [@problem_id:1367513]. By identifying which rule is broken, you will gain a deeper appreciation for why properties like symmetry are not just abstract requirements but are essential for the geometric structure we wish to build.", "problem": "In linear algebra, an inner product on a real vector space $V$ is a function that defines a notion of \"multiplication\" between two vectors, resulting in a scalar. For any vectors $\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V$ and any real scalar $c$, this function, denoted $\\langle \\mathbf{u}, \\mathbf{v} \\rangle$, must satisfy the following four axioms:\n\n1.  **Additivity:** $\\langle \\mathbf{u} + \\mathbf{v}, \\mathbf{w} \\rangle = \\langle \\mathbf{u}, \\mathbf{w} \\rangle + \\langle \\mathbf{v}, \\mathbf{w} \\rangle$\n2.  **Homogeneity:** $\\langle c\\mathbf{u}, \\mathbf{v} \\rangle = c\\langle \\mathbf{u}, \\mathbf{v} \\rangle$\n3.  **Symmetry:** $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{v}, \\mathbf{u} \\rangle$\n4.  **Positivity:** $\\langle \\mathbf{u}, \\mathbf{u} \\rangle \\ge 0$, and $\\langle \\mathbf{u}, \\mathbf{u} \\rangle = 0$ if and only if $\\mathbf{u} = \\mathbf{0}$ (the zero vector).\n\nConsider the vector space $\\mathbb{R}^2$. Let $\\mathbf{u} = (u_1, u_2)$ and $\\mathbf{v} = (v_1, v_2)$ be two vectors in this space. An engineer proposes a new function for this space, defined as:\n$$\n\\langle \\mathbf{u}, \\mathbf{v} \\rangle = u_1 v_1 + u_1 v_2 + u_2 v_2\n$$\nThis function is intended to be used in a signal processing algorithm, but it will only work correctly if it is a valid inner product. Which of the four fundamental axioms for an inner product does this proposed function fail to satisfy?\n\nA. Additivity\n\nB. Homogeneity\n\nC. Symmetry\n\nD. Positivity\n\nE. The function satisfies all axioms and is a valid inner product.", "solution": "We check each axiom for the proposed function on $\\mathbb{R}^{2}$:\n$$\n\\langle \\mathbf{u}, \\mathbf{v} \\rangle = u_{1} v_{1} + u_{1} v_{2} + u_{2} v_{2}.\n$$\n\nAdditivity in the first slot: For any $\\mathbf{x}=(x_{1},x_{2})$, $\\mathbf{y}=(y_{1},y_{2})$, and $\\mathbf{w}=(w_{1},w_{2})$,\n$$\n\\langle \\mathbf{x}+\\mathbf{y}, \\mathbf{w} \\rangle = (x_{1}+y_{1})w_{1} + (x_{1}+y_{1})w_{2} + (x_{2}+y_{2})w_{2}\n$$\n$$\n= x_{1}w_{1} + x_{1}w_{2} + x_{2}w_{2} \\;+\\; y_{1}w_{1} + y_{1}w_{2} + y_{2}w_{2}\n= \\langle \\mathbf{x}, \\mathbf{w} \\rangle + \\langle \\mathbf{y}, \\mathbf{w} \\rangle.\n$$\nThus additivity holds.\n\nHomogeneity in the first slot: For any $c \\in \\mathbb{R}$ and $\\mathbf{x}, \\mathbf{w}$,\n$$\n\\langle c\\mathbf{x}, \\mathbf{w} \\rangle = (c x_{1}) w_{1} + (c x_{1}) w_{2} + (c x_{2}) w_{2}\n= c \\left( x_{1} w_{1} + x_{1} w_{2} + x_{2} w_{2} \\right)\n= c \\langle \\mathbf{x}, \\mathbf{w} \\rangle.\n$$\nThus homogeneity holds.\n\nSymmetry: Compute the difference for general $\\mathbf{u}, \\mathbf{v}$,\n$$\n\\langle \\mathbf{u}, \\mathbf{v} \\rangle - \\langle \\mathbf{v}, \\mathbf{u} \\rangle\n= (u_{1} v_{1} + u_{1} v_{2} + u_{2} v_{2}) - (v_{1} u_{1} + v_{1} u_{2} + v_{2} u_{2})\n= u_{1} v_{2} - v_{1} u_{2}.\n$$\nThis is not identically zero; for example, with $\\mathbf{u}=(1,0)$ and $\\mathbf{v}=(0,1)$, it equals $1$. Hence symmetry fails.\n\nPositivity: For any $\\mathbf{u}$,\n$$\n\\langle \\mathbf{u}, \\mathbf{u} \\rangle = u_{1}^{2} + u_{1} u_{2} + u_{2}^{2}\n= \\left(u_{1} + \\frac{1}{2} u_{2}\\right)^{2} + \\frac{3}{4} u_{2}^{2}.\n$$\nBoth terms on the right are nonnegative, and they sum to zero only when $u_{2}=0$ and $u_{1}=0$. Therefore $\\langle \\mathbf{u}, \\mathbf{u} \\rangle \\ge 0$ with equality if and only if $\\mathbf{u}=\\mathbf{0}$. Thus positivity holds.\n\nConclusion: The proposed function fails the symmetry axiom and hence is not a valid inner product.", "answer": "$$\\boxed{C}$$", "id": "1367513"}, {"introduction": "The power of linear algebra lies in its ability to generalize concepts like distance and angle. Here, we extend the familiar geometric idea of projection from Euclidean space to the vector space of matrices, using the Frobenius inner product [@problem_id:1866036]. This practice guides you through calculating the orthogonal projection of a matrix onto a subspace, which is equivalent to finding the \"closest\" matrix of a simpler form.", "problem": "Consider the vector space $V$ of all $2 \\times 2$ matrices with real entries, denoted by $M_{2}(\\mathbb{R})$. This space is endowed with an inner product, known as the Frobenius inner product, defined by $\\langle A, B \\rangle = \\text{tr}(A^T B)$ for any $A, B \\in V$. Here, $\\text{tr}(M)$ represents the trace of a matrix $M$, and $M^T$ represents its transpose.\n\nLet $W$ be the subspace of $V$ that contains all $2 \\times 2$ diagonal matrices.\n\nFind the orthogonal projection of the matrix $M = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$ onto the subspace $W$.", "solution": "We work in $V = M_{2}(\\mathbb{R})$ with the Frobenius inner product $\\langle A,B\\rangle = \\operatorname{tr}(A^{T}B)$, which equals $\\sum_{i,j} A_{ij}B_{ij}$. The subspace $W$ consists of all diagonal $2\\times 2$ matrices.\n\nFirst, characterize $W^{\\perp}$. A matrix $X$ is orthogonal to every diagonal matrix $D=\\operatorname{diag}(d_{1},d_{2})$ if and only if\n$$\n\\langle X,D\\rangle \\;=\\; \\operatorname{tr}(X^{T}D)\\;=\\;\\sum_{i,j} X_{ij}D_{ij}\\;=\\;X_{11}d_{1}+X_{22}d_{2}\\;=\\;0 \\quad \\text{for all } d_{1},d_{2}\\in\\mathbb{R}.\n$$\nThis holds if and only if $X_{11}=0$ and $X_{22}=0$. Hence $W^{\\perp}$ is the set of off-diagonal matrices, and every $A\\in V$ decomposes uniquely as\n$$\nA \\;=\\; \\operatorname{diag}(A) \\;+\\; \\bigl(A-\\operatorname{diag}(A)\\bigr), \\quad \\operatorname{diag}(A)\\in W,\\; A-\\operatorname{diag}(A)\\in W^{\\perp}.\n$$\nTherefore, the orthogonal projection onto $W$ is $P_{W}(A)=\\operatorname{diag}(A)$.\n\nAlternatively, using an orthonormal basis of $W$ with respect to the Frobenius inner product, take $E_{11}=\\begin{pmatrix}10\\\\00\\end{pmatrix}$ and $E_{22}=\\begin{pmatrix}00\\\\01\\end{pmatrix}$. They satisfy $\\langle E_{11},E_{11}\\rangle=1$, $\\langle E_{22},E_{22}\\rangle=1$, and $\\langle E_{11},E_{22}\\rangle=0$. Hence the projection is\n$$\nP_{W}(M) \\;=\\; \\langle M,E_{11}\\rangle E_{11} \\;+\\; \\langle M,E_{22}\\rangle E_{22}.\n$$\nFor $M=\\begin{pmatrix}12\\\\34\\end{pmatrix}$, compute $\\langle M,E_{11}\\rangle = \\operatorname{tr}(M^{T}E_{11})=M_{11}=1$ and $\\langle M,E_{22}\\rangle = \\operatorname{tr}(M^{T}E_{22})=M_{22}=4$. Therefore,\n$$\nP_{W}(M) \\;=\\; 1\\cdot E_{11} + 4\\cdot E_{22} \\;=\\; \\begin{pmatrix}10\\\\04\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}1  0 \\\\ 0  4\\end{pmatrix}}$$", "id": "1866036"}, {"introduction": "One of the most powerful applications of inner product spaces is in approximation theory. This problem demonstrates how to find the \"best\" polynomial approximation for a transcendental function like sine, a task central to numerical analysis and signal processing [@problem_id:2154976]. You will use the concept of orthogonal projection within a function space to minimize the mean-square error and discover the optimal polynomial fit.", "problem": "In many areas of signal processing and numerical analysis, it is useful to approximate a complex transcendental function with a simpler polynomial function over a specific interval. The \"best\" approximation is often defined in the least-squares sense, where one seeks to minimize the total squared error between the two functions.\n\nConsider the function $f(x) = \\sin(\\pi x)$ defined on the interval $[-1, 1]$. Your task is to find the polynomial $p(x)$ of degree at most two that provides the best approximation to $f(x)$ on this interval. The best-approximating polynomial $p(x)$ is the one that minimizes the mean-square error integral, given by:\n$$E = \\int_{-1}^{1} [f(x) - p(x)]^2 \\, dx$$\nExpress the resulting polynomial $p(x)$ as a function of $x$ and the constant $\\pi$.", "solution": "We approximate $f(x)=\\sin(\\pi x)$ on $[-1,1]$ by a polynomial $p(x)$ of degree at most two in the least-squares sense with respect to the inner product $\\langle g,h\\rangle=\\int_{-1}^{1} g(x)h(x)\\,dx$. The best approximation $p$ is the orthogonal projection of $f$ onto the subspace $\\mathcal{P}_{2}=\\operatorname{span}\\{1,x,x^{2}\\}$. Equivalently, the residual $r(x)=f(x)-p(x)$ must be orthogonal to each basis element:\n$$\n\\int_{-1}^{1}\\big(f(x)-p(x)\\big)\\,dx=0,\\quad\n\\int_{-1}^{1}\\big(f(x)-p(x)\\big)\\,x\\,dx=0,\\quad\n\\int_{-1}^{1}\\big(f(x)-p(x)\\big)\\,x^{2}\\,dx=0.\n$$\nLet $p(x)=a+bx+cx^{2}$. Then the normal equations are\n$$\n\\int_{-1}^{1} f(x)\\,dx-a\\int_{-1}^{1}1\\,dx-b\\int_{-1}^{1}x\\,dx-c\\int_{-1}^{1}x^{2}\\,dx=0,\n$$\n$$\n\\int_{-1}^{1} f(x)\\,x\\,dx-a\\int_{-1}^{1}x\\,dx-b\\int_{-1}^{1}x^{2}\\,dx-c\\int_{-1}^{1}x^{3}\\,dx=0,\n$$\n$$\n\\int_{-1}^{1} f(x)\\,x^{2}\\,dx-a\\int_{-1}^{1}x^{2}\\,dx-b\\int_{-1}^{1}x^{3}\\,dx-c\\int_{-1}^{1}x^{4}\\,dx=0.\n$$\nWe use parity and basic integrals on $[-1,1]$:\n- $f(x)=\\sin(\\pi x)$ is odd, so $\\int_{-1}^{1} f(x)\\,dx=0$ and $\\int_{-1}^{1} f(x)\\,x^{2}\\,dx=0$.\n- $\\int_{-1}^{1} x\\,dx=0$, $\\int_{-1}^{1} x^{3}\\,dx=0$.\n- $\\int_{-1}^{1} x^{2}\\,dx=\\frac{2}{3}$, $\\int_{-1}^{1} x^{4}\\,dx=\\frac{2}{5}$.\n- To compute $\\int_{-1}^{1} f(x)\\,x\\,dx=\\int_{-1}^{1} x\\sin(\\pi x)\\,dx$, integrate by parts with $u=x$, $dv=\\sin(\\pi x)\\,dx$, so $du=dx$ and $v=-\\cos(\\pi x)/\\pi$. Then\n$$\n\\int_{-1}^{1} x\\sin(\\pi x)\\,dx=\\left[-\\frac{x\\cos(\\pi x)}{\\pi}\\right]_{-1}^{1}+\\int_{-1}^{1}\\frac{\\cos(\\pi x)}{\\pi}\\,dx\n=\\left[-\\frac{x\\cos(\\pi x)}{\\pi}\\right]_{-1}^{1}+\\left[\\frac{\\sin(\\pi x)}{\\pi^{2}}\\right]_{-1}^{1}.\n$$\nUsing $\\cos(\\pi)=-1$, $\\cos(-\\pi)=-1$, and $\\sin(\\pm\\pi)=0$, we obtain\n$$\n\\int_{-1}^{1} x\\sin(\\pi x)\\,dx=\\left(\\frac{1}{\\pi}\\right)-\\left(-\\frac{1}{\\pi}\\right)=\\frac{2}{\\pi}.\n$$\nSubstituting these into the normal equations, we get\n$$\n0-2a-0-c\\cdot\\frac{2}{3}=0,\\qquad \\frac{2}{\\pi}-0-b\\cdot\\frac{2}{3}-0=0,\\qquad 0-a\\cdot\\frac{2}{3}-0-c\\cdot\\frac{2}{5}=0.\n$$\nThe first and third equations are\n$$\n2a+\\frac{2}{3}c=0,\\qquad \\frac{2}{3}a+\\frac{2}{5}c=0.\n$$\nSolving, subtracting appropriate multiples yields $c=0$, and then $a=0$. The second equation reduces to\n$$\n\\frac{2}{\\pi}-b\\cdot\\frac{2}{3}=0 \\quad\\Rightarrow\\quad b=\\frac{3}{\\pi}.\n$$\nTherefore, the best $L^{2}$ approximation of degree at most two is\n$$\np(x)=\\frac{3}{\\pi}\\,x.\n$$", "answer": "$$\\boxed{\\frac{3}{\\pi}x}$$", "id": "2154976"}]}