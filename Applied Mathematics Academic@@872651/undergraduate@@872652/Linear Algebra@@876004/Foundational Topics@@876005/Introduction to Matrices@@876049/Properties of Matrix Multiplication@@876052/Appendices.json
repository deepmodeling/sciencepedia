{"hands_on_practices": [{"introduction": "One of the first and most crucial properties distinguishing matrix algebra from scalar algebra is that matrix multiplication is not commutative; in general, $AB \\neq BA$. This exercise invites you to go beyond simply observing this fact and instead explore the specific conditions required for commutativity. By determining the general form of a matrix that commutes with a given shear matrix, you will gain a deeper understanding of the structural constraints that this property imposes [@problem_id:1384856].", "problem": "In linear algebra, the order of matrix multiplication generally matters, i.e., for two matrices $M$ and $N$, the product $MN$ is not always equal to $NM$. When the equality $MN=NM$ does hold, the matrices are said to commute.\n\nConsider the specific $2 \\times 2$ matrix $A$ with real entries, defined as:\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nDetermine the general form of a $2 \\times 2$ matrix $B$ with real entries that commutes with $A$. Express your answer as a single $2 \\times 2$ matrix whose entries may depend on arbitrary real parameters, which you should denote as $u$ and $v$.", "solution": "Let $A=\\begin{pmatrix}1 & 1 \\\\ 0 & 1\\end{pmatrix}$ and let a general real $2\\times 2$ matrix be $B=\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}$. We require $AB=BA$.\n\nCompute $AB$:\n$$\nAB=\\begin{pmatrix}1 & 1 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}\n=\\begin{pmatrix}a+c & b+d \\\\ c & d\\end{pmatrix}.\n$$\n\nCompute $BA$:\n$$\nBA=\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}\\begin{pmatrix}1 & 1 \\\\ 0 & 1\\end{pmatrix}\n=\\begin{pmatrix}a & a+b \\\\ c & c+d\\end{pmatrix}.\n$$\n\nSet $AB=BA$ and equate corresponding entries:\n$$\na+c=a \\implies c=0,\\quad b+d=a+b \\implies d=a,\\quad c=c,\\quad d=c+d \\implies c=0.\n$$\nThus the constraints are $c=0$ and $d=a$, with $a$ and $b$ arbitrary real numbers.\n\nRenaming the free parameters as requested, let $a=u$ and $b=v$. Therefore\n$$\nB=\\begin{pmatrix}u & v \\\\ 0 & u\\end{pmatrix}.\n$$\nThis is the general real $2\\times 2$ matrix that commutes with $A$.", "answer": "$$\\boxed{\\begin{pmatrix}u & v \\\\ 0 & u\\end{pmatrix}}$$", "id": "1384856"}, {"introduction": "The associative property of multiplication, where $(AB)C = A(BC)$, is a rule that often seems trivial. However, in the world of computational science, it has profound implications for efficiency. This practice problem demonstrates how the choice of grouping matrix operations can dramatically alter the computational cost, providing a tangible link between an abstract algebraic law and real-world performance [@problem_id:1384850].", "problem": "In many computational pipelines involving linear algebra, the order of operations can have a significant impact on performance, even when the final result is mathematically identical. Consider the calculation of a scalar value $s$ from the product of three matrices: a row vector $u$, a rectangular matrix $A$, and a column vector $v$, such that $s = uAv$.\n\nThe matrices have the following dimensions:\n- $u$ is a $1 \\times N$ matrix (a row vector).\n- $A$ is an $N \\times M$ matrix.\n- $v$ is an $M \\times 1$ matrix (a column vector).\n\nThe computational cost of multiplying a $p \\times q$ matrix by a $q \\times r$ matrix is defined as the total number of scalar multiplications required, which is given by the product $pqr$.\n\nDue to the associativity of matrix multiplication, we can compute $s$ in two ways:\n1. Method 1: $(uA)v$\n2. Method 2: $u(Av)$\n\nLet $C_1$ be the total computational cost (total number of scalar multiplications) for Method 1, and $C_2$ be the cost for Method 2.\n\nFor a specific application, the dimensions are given by $N = 200$ and $M=5$. Determine the ratio of the computational costs, $\\frac{C_2}{C_1}$. Give your answer as a single numerical value, rounded to three significant figures.", "solution": "We are given $u$ of size $1 \\times N$, $A$ of size $N \\times M$, and $v$ of size $M \\times 1$. The cost of multiplying a $p \\times q$ matrix by a $q \\times r$ matrix is $pqr$ scalar multiplications.\n\nMethod 1: $(uA)v$.\n- First compute $uA$: sizes $1 \\times N$ and $N \\times M$ yield cost $1 \\cdot N \\cdot M = NM$. The result is $1 \\times M$.\n- Then multiply by $v$: sizes $1 \\times M$ and $M \\times 1$ yield cost $1 \\cdot M \\cdot 1 = M$.\nThus, $C_{1} = NM + M = M(N+1)$.\n\nMethod 2: $u(Av)$.\n- First compute $Av$: sizes $N \\times M$ and $M \\times 1$ yield cost $N \\cdot M \\cdot 1 = NM$. The result is $N \\times 1$.\n- Then multiply by $u$: sizes $1 \\times N$ and $N \\times 1$ yield cost $1 \\cdot N \\cdot 1 = N$.\nThus, $C_{2} = NM + N = N(M+1)$.\n\nTherefore, the ratio is\n$$\n\\frac{C_{2}}{C_{1}} = \\frac{N(M+1)}{M(N+1)}.\n$$\nFor $N=200$ and $M=5$,\n$$\n\\frac{C_{2}}{C_{1}} = \\frac{200 \\cdot (5+1)}{5 \\cdot (200+1)} = \\frac{1200}{1005} = \\frac{80}{67} \\approx 1.19402985\\ldots\n$$\nRounded to three significant figures, this is $1.19$.", "answer": "$$\\boxed{1.19}$$", "id": "1384850"}, {"introduction": "Repeated multiplication of a matrix by itself, known as taking the power of a matrix, is a fundamental operation in modeling systems that evolve over discrete time steps. This exercise explores this concept in a simple yet powerful context: finding a general formula for the power of a diagonal matrix. Mastering this calculation is a key step toward understanding more advanced topics like matrix diagonalization and its application in solving linear dynamical systems [@problem_id:1384867].", "problem": "In the study of discrete dynamical systems, the state of a system after $k$ time steps, represented by a vector $\\mathbf{v}_k$, can often be determined from an initial state $\\mathbf{v}_0$ through the relation $\\mathbf{v}_k = M^k \\mathbf{v}_0$, where $M$ is the transition matrix of the system.\n\nConsider such a system whose state is described by three variables, and its evolution is governed by the diagonal transition matrix $D$:\n$$ D = \\begin{pmatrix} \\lambda & 0 & 0 \\\\ 0 & \\mu & 0 \\\\ 0 & 0 & \\nu \\end{pmatrix} $$\nHere, $\\lambda$, $\\mu$, and $\\nu$ are distinct, non-zero parameters representing the growth or decay rates for each variable.\n\nFor any positive integer $k$, find a general expression for the matrix $D^k$.", "solution": "We are given the diagonal matrix\n$$\nD=\\begin{pmatrix}\n\\lambda & 0 & 0\\\\\n0 & \\mu & 0\\\\\n0 & 0 & \\nu\n\\end{pmatrix},\n$$\nwith distinct, non-zero parameters $\\lambda$, $\\mu$, and $\\nu$. We are to find $D^{k}$ for a positive integer $k$.\n\nKey property: The product of two diagonal matrices is diagonal, and the diagonal entries multiply componentwise. Specifically, if $A=\\operatorname{diag}(a_{1},a_{2},a_{3})$ and $B=\\operatorname{diag}(b_{1},b_{2},b_{3})$, then for each $i\\in\\{1,2,3\\}$, the $(i,i)$-entry of $AB$ is\n$$\n(AB)_{ii}=\\sum_{j=1}^{3}a_{ij}b_{ji}=a_{ii}b_{ii}=a_{i}b_{i},\n$$\nand for $i\\neq j$, $(AB)_{ij}=0$ because at least one factor in each term of the sum is zero. Hence,\n$$\nAB=\\operatorname{diag}(a_{1}b_{1},a_{2}b_{2},a_{3}b_{3}).\n$$\n\nApply this iteratively to $D^{k}$. Compute the base case:\n$$\nD^{2}=DD=\\operatorname{diag}(\\lambda\\cdot\\lambda,\\mu\\cdot\\mu,\\nu\\cdot\\nu)=\\operatorname{diag}(\\lambda^{2},\\mu^{2},\\nu^{2}).\n$$\nAssume for some positive integer $k$ that\n$$\nD^{k}=\\operatorname{diag}(\\lambda^{k},\\mu^{k},\\nu^{k}).\n$$\nThen\n$$\nD^{k+1}=D^{k}D=\\operatorname{diag}(\\lambda^{k},\\mu^{k},\\nu^{k})\\operatorname{diag}(\\lambda,\\mu,\\nu)=\\operatorname{diag}(\\lambda^{k+1},\\mu^{k+1},\\nu^{k+1}).\n$$\nBy induction, for every positive integer $k$,\n$$\nD^{k}=\\begin{pmatrix}\n\\lambda^{k} & 0 & 0\\\\\n0 & \\mu^{k} & 0\\\\\n0 & 0 & \\nu^{k}\n\\end{pmatrix}.\n$$\nThis is the required general expression.", "answer": "$$\\boxed{\\begin{pmatrix}\\lambda^{k} & 0 & 0 \\\\ 0 & \\mu^{k} & 0 \\\\ 0 & 0 & \\nu^{k}\\end{pmatrix}}$$", "id": "1384867"}]}