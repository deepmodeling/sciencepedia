{"hands_on_practices": [{"introduction": "To understand the effect of a linear transformation, it's useful to measure its \"size\" or maximum amplification power. This is precisely what induced matrix norms do. This first practice focuses on calculating the three most common induced norms—the $1$-norm, $2$-norm, and $\\infty$-norm—for a simple diagonal scaling matrix, providing a concrete starting point for building your computational skills. [@problem_id:1376555]", "problem": "In two-dimensional computer graphics, a scaling transformation is often represented by a matrix. Consider a non-uniform scaling transformation in a 2D Cartesian coordinate system represented by the matrix $S$ given by:\n$$\nS = \\begin{pmatrix} 4.5  0 \\\\ 0  2.1 \\end{pmatrix}\n$$\nThis matrix scales the x-component of a vector by a factor of $4.5$ and the y-component by a factor of $2.1$.\n\nFor this scaling matrix $S$, compute its induced 1-norm $\\|S\\|_1$, its induced 2-norm (also known as the spectral norm) $\\|S\\|_2$, and its induced $\\infty$-norm $\\|S\\|_\\infty$. Select the option that correctly lists all three values.\n\nA. $\\|S\\|_1 = 6.6$, $\\|S\\|_2 = 4.966$, $\\|S\\|_\\infty = 6.6$\n\nB. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 4.966$, $\\|S\\|_\\infty = 4.5$\n\nC. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 4.5$, $\\|S\\|_\\infty = 4.5$\n\nD. $\\|S\\|_1 = 2.1$, $\\|S\\|_2 = 2.1$, $\\|S\\|_\\infty = 2.1$\n\nE. $\\|S\\|_1 = 4.5$, $\\|S\\|_2 = 2.1$, $\\|S\\|_\\infty = 4.5$", "solution": "We are given the diagonal matrix\n$$\nS=\\begin{pmatrix}4.5  0 \\\\ 0  2.1\\end{pmatrix}.\n$$\nFor the induced 1-norm, by definition,\n$$\n\\|S\\|_{1}=\\max_{j}\\sum_{i}|a_{ij}|,\n$$\nwhich is the maximum absolute column sum. The column sums are\n$$\n|4.5|+|0|=4.5,\\quad |0|+|2.1|=2.1,\n$$\nso\n$$\n\\|S\\|_{1}=4.5.\n$$\n\nFor the induced infinity-norm, by definition,\n$$\n\\|S\\|_{\\infty}=\\max_{i}\\sum_{j}|a_{ij}|,\n$$\nwhich is the maximum absolute row sum. The row sums are\n$$\n|4.5|+|0|=4.5,\\quad |0|+|2.1|=2.1,\n$$\nso\n$$\n\\|S\\|_{\\infty}=4.5.\n$$\n\nFor the induced 2-norm (spectral norm), by definition,\n$$\n\\|S\\|_{2}=\\sqrt{\\lambda_{\\max}(S^{\\mathsf{T}}S)}.\n$$\nSince $S$ is diagonal, we have\n$$\nS^{\\mathsf{T}}S=\\begin{pmatrix}4.5^{2}  0 \\\\ 0  2.1^{2}\\end{pmatrix},\n$$\nwhose eigenvalues are $4.5^{2}$ and $2.1^{2}$. Therefore,\n$$\n\\|S\\|_{2}=\\sqrt{\\max\\{4.5^{2},\\,2.1^{2}\\}}=4.5.\n$$\n\nThus,\n$$\n\\|S\\|_{1}=4.5,\\quad \\|S\\|_{2}=4.5,\\quad \\|S\\|_{\\infty}=4.5,\n$$\nwhich corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1376555"}, {"introduction": "While induced norms are powerful, they are not the only way to measure a matrix's magnitude. The Frobenius norm, for instance, is defined directly from the matrix's entries and is common in many applications. This exercise reveals a simple yet profound difference between these categories by examining the norm of the identity matrix, a test that definitively shows the Frobenius norm is not an induced norm. [@problem_id:2186712]", "problem": "In numerical analysis, matrix norms are essential for understanding the behavior of matrix operations. A matrix norm is a function $\\|\\cdot\\|$ that assigns a real number to each matrix, satisfying certain properties. One common type is the **induced norm** (or operator norm), which is defined in terms of a vector norm. For a given vector norm $\\|\\cdot\\|_v$, the corresponding induced matrix norm is defined as:\n$$ \\|A\\| = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_v}{\\|x\\|_v} $$\nA fundamental property that holds for *any* induced norm, regardless of the underlying vector norm, is that for the $n \\times n$ identity matrix $I_n$, the norm must be exactly 1, i.e., $\\|I_n\\| = 1$.\n\nAnother widely used matrix norm is the **Frobenius norm**, which is not an induced norm. For an $m \\times n$ matrix $A$, the Frobenius norm is defined as the square root of the sum of the absolute squares of its elements:\n$$ \\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2} $$\n\nYour task is to demonstrate this distinction with a simple example. Consider the $2 \\times 2$ identity matrix, $I_2$. Calculate its Frobenius norm, $\\|I_2\\|_F$, and select the option that correctly states the value and the logical conclusion that can be drawn from it.\n\nA. $\\|I_2\\|_F = 1$, which is consistent with the Frobenius norm being an induced norm.\n\nB. $\\|I_2\\|_F = \\sqrt{2}$, which proves the Frobenius norm is not an induced norm.\n\nC. $\\|I_2\\|_F = 2$, which proves the Frobenius norm is not an induced norm.\n\nD. $\\|I_2\\|_F = \\sqrt{2}$, which is consistent with the Frobenius norm being an induced norm.\n\nE. $\\|I_2\\|_F = 2$, which is consistent with the Frobenius norm being an induced norm.", "solution": "By definition, the Frobenius norm of an $m \\times n$ matrix $A = [a_{ij}]$ is\n$$\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}.\n$$\nFor the $2 \\times 2$ identity matrix $I_{2}$, its entries are $a_{11} = 1$, $a_{22} = 1$, and $a_{12} = a_{21} = 0$. Substituting these values into the Frobenius norm formula gives\n$$\n\\|I_{2}\\|_{F} = \\sqrt{|1|^{2} + |0|^{2} + |0|^{2} + |1|^{2}} = \\sqrt{2}.\n$$\nNow recall the defining property of any induced (operator) matrix norm corresponding to a vector norm $\\|\\cdot\\|_{v}$:\n$$\n\\|A\\| = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{v}}{\\|x\\|_{v}}.\n$$\nApplying this to $A = I_{n}$ yields\n$$\n\\|I_{n}\\| = \\sup_{x \\neq 0} \\frac{\\|I_{n}x\\|_{v}}{\\|x\\|_{v}} = \\sup_{x \\neq 0} \\frac{\\|x\\|_{v}}{\\|x\\|_{v}} = 1.\n$$\nTherefore, every induced matrix norm must satisfy $\\|I_{n}\\| = 1$. Since we have computed $\\|I_{2}\\|_{F} = \\sqrt{2} \\neq 1$, it follows that the Frobenius norm is not an induced norm (i.e., not an operator norm induced by any vector norm). Hence the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "2186712"}, {"introduction": "Matrix norms are not just theoretical curiosities; they are essential tools for solving practical problems in fields like control theory and machine learning. This final practice demonstrates this by framing a question of efficiency as an optimization problem. Here, you will find the \"smallest\" matrix—as measured by the Frobenius norm—that performs a required linear mapping, illustrating how norms help us find simple and elegant solutions to complex constraints. [@problem_id:2186715]", "problem": "In many applications, from control theory to machine learning, it is often desirable to find the \"simplest\" linear transformation that satisfies certain criteria. One way to quantify the complexity or \"size\" of a transformation represented by a matrix is using a matrix norm.\n\nThe Frobenius norm of an $m \\times n$ matrix $M$ with real entries $m_{ij}$ is defined as $\\|M\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n m_{ij}^2}$.\n\nLet two vectors in $\\mathbb{R}^2$ be given as $x_0 = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$ and $b_0 = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$.\n\nYour task is to find the unique $2 \\times 2$ matrix $A$ with real entries that has the minimum possible Frobenius norm among all matrices satisfying the condition $A x_0 = b_0$. Express your answer as a $2 \\times 2$ matrix whose entries are exact fractions in their simplest form.", "solution": "We seek a real $2 \\times 2$ matrix $A$ that minimizes the Frobenius norm subject to $A x_{0} = b_{0}$, where $x_{0} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$ and $b_{0} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$. Minimizing $\\|A\\|_{F}$ is equivalent to minimizing $\\|A\\|_{F}^{2}$, which is strictly convex in $A$, ensuring uniqueness when combined with linear constraints.\n\nUse Lagrange multipliers. Define the Lagrangian with multiplier $\\lambda \\in \\mathbb{R}^{2}$:\n$$\nL(A,\\lambda) = \\|A\\|_{F}^{2} + 2 \\lambda^{T}(A x_{0} - b_{0}).\n$$\nTaking the gradient with respect to $A$ and setting it to zero (using the Frobenius inner product) gives\n$$\n\\frac{\\partial L}{\\partial A} = 2A + 2 \\lambda x_{0}^{T} = 0 \\quad \\Longrightarrow \\quad A = - \\lambda x_{0}^{T}.\n$$\nImpose the constraint $A x_{0} = b_{0}$:\n$$\nA x_{0} = (-\\lambda x_{0}^{T}) x_{0} = -\\lambda (x_{0}^{T} x_{0}) = b_{0}.\n$$\nSince $x_{0} \\neq 0$, we solve for $\\lambda$:\n$$\n\\lambda = - \\frac{b_{0}}{x_{0}^{T} x_{0}}.\n$$\nHence the minimizer is\n$$\nA = \\frac{b_{0} x_{0}^{T}}{x_{0}^{T} x_{0}}.\n$$\nCompute $x_{0}^{T} x_{0} = 3^{2} + (-1)^{2} = 10$ and\n$$\nb_{0} x_{0}^{T} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\begin{pmatrix} 3  -1 \\end{pmatrix} = \\begin{pmatrix} 6  -2 \\\\ 15  -5 \\end{pmatrix}.\n$$\nTherefore,\n$$\nA = \\frac{1}{10} \\begin{pmatrix} 6  -2 \\\\ 15  -5 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5}  -\\frac{1}{5} \\\\ \\frac{3}{2}  -\\frac{1}{2} \\end{pmatrix}.\n$$\nThis $A$ satisfies $A x_{0} = b_{0}$ and is unique due to the strict convexity of $\\|A\\|_{F}^{2}$ and linearity of the constraint.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{5}  -\\frac{1}{5} \\\\ \\frac{3}{2}  -\\frac{1}{2}\\end{pmatrix}}$$", "id": "2186715"}]}