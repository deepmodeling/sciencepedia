{"hands_on_practices": [{"introduction": "Understanding the pseudoinverse begins with building strong intuition, and this first exercise provides a concrete, geometric starting point. We will analyze a simple linear transformation that is not invertible because it involves a projection, which discards information. By computing its pseudoinverse, you will see how this powerful tool \"best\" reverses the invertible part of the operation while correctly handling the information that was lost [@problem_id:1397327].", "problem": "Consider a linear transformation $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$. This transformation takes any vector $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ and performs the following two operations in sequence:\n1. It projects the vector $\\mathbf{v}$ orthogonally onto the $x$-axis.\n2. It scales the resulting projected vector by a non-zero real constant $\\alpha$.\n\nLet $A$ be the matrix that represents this linear transformation $T$ with respect to the standard basis in $\\mathbb{R}^2$.\n\nIn linear algebra, the Moore-Penrose pseudoinverse of a matrix $M$, denoted $M^+$, is a generalization of the matrix inverse. It is particularly useful for finding a 'best fit' (least squares) solution to a system of linear equations that lacks a unique solution.\n\nYour task is to find the Moore-Penrose pseudoinverse, $A^+$, of the matrix $A$. Your answer should be expressed in terms of the constant $\\alpha$.", "solution": "The orthogonal projection onto the $x$-axis maps $\\begin{pmatrix} x \\\\ y \\end{pmatrix}$ to $\\begin{pmatrix} x \\\\ 0 \\end{pmatrix}$. Its matrix in the standard basis is\n$$\nP=\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nScaling by a non-zero real constant $\\alpha$ yields the linear transformation with matrix\n$$\nA=\\alpha P=\\begin{pmatrix} \\alpha  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nTo find the Moore-Penrose pseudoinverse $A^{+}$, let\n$$\nX=\\begin{pmatrix} x_{11}  x_{12} \\\\ x_{21}  x_{22} \\end{pmatrix}\n$$\nand impose the Moore-Penrose conditions:\n$$\n\\text{(i) } A X A = A, \\quad \\text{(ii) } X A X = X, \\quad \\text{(iii) } (A X)^{T} = A X, \\quad \\text{(iv) } (X A)^{T} = X A.\n$$\nCompute\n$$\nA X = \\begin{pmatrix} \\alpha x_{11}  \\alpha x_{12} \\\\ 0  0 \\end{pmatrix}, \\quad\nA X A = \\begin{pmatrix} \\alpha^{2} x_{11}  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nCondition (i) gives $\\alpha^{2} x_{11} = \\alpha$, hence $x_{11} = \\frac{1}{\\alpha}$.\n\nNext,\n$$\nX A = \\begin{pmatrix} x_{11} \\alpha  0 \\\\ x_{21} \\alpha  0 \\end{pmatrix}, \\quad\nX A X = \\begin{pmatrix} \\alpha x_{11}^{2}  \\alpha x_{11} x_{12} \\\\ \\alpha x_{21} x_{11}  \\alpha x_{21} x_{12} \\end{pmatrix}.\n$$\nCondition (ii) gives $\\alpha x_{11}^{2} = x_{11}$ (satisfied by $x_{11}=\\frac{1}{\\alpha}$), $\\alpha x_{11} x_{12} = x_{12}$, $\\alpha x_{21} x_{11} = x_{21}$, and $x_{22} = \\alpha x_{21} x_{12}$.\n\nCondition (iii) requires $A X$ to be symmetric:\n$$\nA X = \\begin{pmatrix} \\alpha x_{11}  \\alpha x_{12} \\\\ 0  0 \\end{pmatrix} \\quad \\Rightarrow \\quad \\alpha x_{12} = 0 \\ \\Rightarrow \\ x_{12} = 0\n$$\nsince $\\alpha \\neq 0$.\n\nCondition (iv) requires $X A$ to be symmetric:\n$$\nX A = \\begin{pmatrix} x_{11} \\alpha  0 \\\\ x_{21} \\alpha  0 \\end{pmatrix} \\quad \\Rightarrow \\quad x_{21} \\alpha = 0 \\ \\Rightarrow \\ x_{21} = 0.\n$$\nWith $x_{12}=x_{21}=0$, the relation $x_{22}=\\alpha x_{21} x_{12}$ gives $x_{22}=0$. Therefore,\n$$\nA^{+}=X=\\begin{pmatrix} \\frac{1}{\\alpha}  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nEquivalently, since $A=\\alpha P$ with $P$ an orthogonal projection that satisfies $P^{+}=P$, we have $(\\alpha P)^{+}=\\frac{1}{\\alpha} P$, yielding the same result.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\alpha}  0 \\\\ 0  0\\end{pmatrix}}$$", "id": "1397327"}, {"introduction": "Moving from a specific example to a general case is a key step in mathematical mastery. This practice [@problem_id:1397317] guides you through deriving a powerful closed-form expression for the pseudoinverse of any rank-one matrix of the form $A = uv^T$. Such matrices are fundamental building blocks in linear algebra and its applications, most notably in the singular value decomposition (SVD).", "problem": "Let $u$ be a non-zero column vector in $\\mathbb{R}^m$ and $v$ be a non-zero column vector in $\\mathbb{R}^n$. Consider the $m \\times n$ matrix $A$ formed by the outer product of these vectors, given by $A = uv^T$.\n\nThe Moore-Penrose pseudoinverse of a matrix $A$, denoted $A^+$, is the unique matrix that satisfies the following four criteria:\n1. $A A^+ A = A$\n2. $A^+ A A^+ = A^+$\n3. $(A A^+)^T = A A^+$\n4. $(A^+ A)^T = A^+ A$\n\nYour task is to find a closed-form expression for the pseudoinverse $A^+$. Express your answer in terms of the vectors $u$ and $v$ and their transposes. The squared Euclidean norm of a vector $x$ is defined as $\\|x\\|^2 = x^T x$ and may be used in your final expression.", "solution": "The problem asks for the Moore-Penrose pseudoinverse, $A^+$, of a rank-one matrix $A = uv^T$. A systematic method to compute the pseudoinverse is to first find the Singular Value Decomposition (SVD) of the matrix $A$.\n\nIf the SVD of an $m \\times n$ matrix $A$ is given by $A = U\\Sigma V^T$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and $\\Sigma$ is an $m \\times n$ diagonal matrix of singular values, then the pseudoinverse $A^+$ is given by $A^+ = V\\Sigma^+ U^T$. Here, $\\Sigma^+$ is the $n \\times m$ matrix obtained by taking the transpose of $\\Sigma$ and then taking the reciprocal of each non-zero entry.\n\nLet's find the SVD of $A = uv^T$. First, we normalize the vectors $u$ and $v$. Let $\\hat{u} = \\frac{u}{\\|u\\|}$ and $\\hat{v} = \\frac{v}{\\|v\\|}$. Note that $\\hat{u}$ and $\\hat{v}$ are unit vectors, and the norms are $\\|u\\| = \\sqrt{u^T u}$ and $\\|v\\| = \\sqrt{v^T v}$.\n\nWe can rewrite the matrix $A$ in terms of these unit vectors:\n$A = uv^T = (\\|u\\|\\hat{u})(\\|v\\|\\hat{v})^T = (\\|u\\| \\|v\\|) \\hat{u} \\hat{v}^T$\n\nThis expression is already in the form of a singular value decomposition. For a rank-one matrix, there is only one non-zero singular value, $\\sigma_1$. Comparing with the general SVD form for a rank-one matrix, $A = \\sigma_1 u_1 v_1^T$, we can identify:\n- The single non-zero singular value: $\\sigma_1 = \\|u\\| \\|v\\|$.\n- The first column of $U$: $u_1 = \\hat{u}$.\n- The first column of $V$: $v_1 = \\hat{v}$.\n\nThe matrix $\\Sigma$ is an $m \\times n$ matrix with $\\sigma_1$ as its top-left element and zeros everywhere else:\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_1  0  \\dots  0 \\\\\n0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  \\dots  0\n\\end{pmatrix}\n$$\nTo construct $A^+$, we first need $\\Sigma^+$. This is an $n \\times m$ matrix where the only non-zero element is at the top-left, and its value is the reciprocal of $\\sigma_1$:\n$$\n\\Sigma^+ = \\begin{pmatrix}\n1/\\sigma_1  0  \\dots  0 \\\\\n0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  \\dots  0\n\\end{pmatrix}\n$$\nThe full matrices $U$ and $V$ can be formed by completing the orthonormal bases. Let $U = [u_1 \\ u_2 \\dots u_m]$ and $V = [v_1 \\ v_2 \\dots v_n]$. The pseudoinverse is then calculated as $A^+ = V\\Sigma^+ U^T$.\n\n$A^+ = V \\Sigma^+ U^T = \\begin{pmatrix} v_1  v_2  \\dots  v_n \\end{pmatrix} \\begin{pmatrix}\n1/\\sigma_1  0  \\dots \\\\\n0  0  \\dots \\\\\n\\vdots  \\vdots  \\ddots\n\\end{pmatrix} \\begin{pmatrix} u_1^T \\\\ u_2^T \\\\ \\vdots \\\\ u_m^T \\end{pmatrix}$\n\nPerforming the matrix multiplication, the product $\\Sigma^+ U^T$ yields an $n \\times m$ matrix where the first row is $(1/\\sigma_1)u_1^T$ and all other rows are zero. Then, multiplying by $V$ from the left picks out the first column of $V$, $v_1$, scaled by the first row of $\\Sigma^+ U^T$. This results in:\n$A^+ = v_1 \\left( \\frac{1}{\\sigma_1} u_1^T \\right) = \\frac{1}{\\sigma_1} v_1 u_1^T$.\n\nNow, we substitute back the expressions for $\\sigma_1$, $u_1$, and $v_1$:\n$\\sigma_1 = \\|u\\| \\|v\\|$\n$u_1 = \\hat{u} = \\frac{u}{\\|u\\|}$\n$v_1 = \\hat{v} = \\frac{v}{\\|v\\|}$\n\nSo,\n$A^+ = \\frac{1}{\\|u\\| \\|v\\|} \\left( \\frac{v}{\\|v\\|} \\right) \\left( \\frac{u}{\\|u\\|} \\right)^T = \\frac{1}{\\|u\\|^2 \\|v\\|^2} v u^T$.\n\nFinally, using the provided definition for the squared norm, $\\|x\\|^2 = x^T x$, we can write the final expression as:\n$A^+ = \\frac{1}{(u^T u)(v^T v)} v u^T$.", "answer": "$$\\boxed{\\frac{1}{(u^T u)(v^T v)} v u^T}$$", "id": "1397317"}, {"introduction": "The power of the pseudoinverse extends beyond static problems into the dynamic world of real-time data analysis. This final exercise [@problem_id:1397278] explores a more advanced, practical scenario: how to efficiently update a pseudoinverse when new data becomes available. This concept of recursive updating is crucial in fields like adaptive signal processing and online machine learning, where computational efficiency is paramount.", "problem": "In many real-time applications such as adaptive signal processing or online machine learning, it is necessary to update a linear model as new data becomes available. A common task is to solve a linear least-squares problem, which often involves computing the Moore-Penrose pseudoinverse of a data matrix. When the model is updated by adding a new feature, this corresponds to appending a new column to the data matrix.\n\nConsider a data matrix $A_{k-1}$ of size $m \\times (k-1)$. A new feature is added, resulting in a new $m \\times k$ matrix $A_k = [A_{k-1} \\,|\\, v]$, where $v$ is the $m \\times 1$ column vector corresponding to the new feature's data. To reduce computational cost, instead of re-calculating the pseudoinverse of $A_k$ from scratch, one can use a recursive update formula. It can be shown that the pseudoinverse of $A_k$ can be partitioned as:\n$$A_k^+ = \\begin{pmatrix} A_{k-1}^+ - d b^T \\\\ b^T \\end{pmatrix}$$\nwhere $d = A_{k-1}^+ v$ and $b$ is a vector of size $m \\times 1$.\n\nThe vector $b$ depends on the properties of the new column $v$. Let $c = (I - A_{k-1}A_{k-1}^+)v$, which represents the component of $v$ orthogonal to the column space of $A_{k-1}$. Assume the new feature is genuinely new and not a linear combination of the existing features, which mathematically implies that $v$ is not in the column space of $A_{k-1}$, and therefore $c \\neq 0$.\n\nYour task is to determine the vector $b$ under this condition. Express your answer as a symbolic expression in terms of the vector $c$.", "solution": "We denote $A := A_{k-1}\\in \\mathbb{R}^{m\\times (k-1)}$ and consider the augmented matrix $A_{k} = [A \\,|\\, v] \\in \\mathbb{R}^{m\\times k}$. The proposed update for the pseudoinverse is\n$$\nA_k^{+} = \\begin{pmatrix} A^{+} - d b^{T} \\\\ b^{T} \\end{pmatrix}, \\quad d := A^{+} v,\n$$\nwith $b \\in \\mathbb{R}^{m\\times 1}$ to be determined. Define the orthogonal projector onto the column space of $A$ by $P := A A^{+}$. The component of $v$ orthogonal to the column space of $A$ is\n$$\nc := (I - P) v = v - A A^{+} v = v - A d.\n$$\nUnder the assumption that the new feature is not in the column space of $A$, we have $c \\neq 0$.\n\nWe impose the Moore-Penrose conditions through the product $A_k A_k^{+}$. Compute\n$$\nA_k A_k^{+} = [A, v] \\begin{pmatrix} A^{+} - d b^{T} \\\\ b^{T} \\end{pmatrix} = A(A^{+} - d b^{T}) + v b^{T} = A A^{+} - A d b^{T} + v b^{T}.\n$$\nUsing $A d = A A^{+} v = P v$ and $v - A d = c$, we obtain\n$$\nA_k A_k^{+} = P + c b^{T}.\n$$\nThe matrix $A_k A_k^{+}$ must be the orthogonal projector onto the column space of $A_k$, which is $\\operatorname{col}(A_k) = \\operatorname{col}(A) \\oplus \\operatorname{span}\\{c\\}$ with $c \\perp \\operatorname{col}(A)$. Therefore $P + c b^{T}$ must be an orthogonal projector satisfying:\n- It acts as the identity on $c$: $(P + c b^{T}) c = c$. Since $P c = 0$, this gives $c (b^{T} c) = c$, hence\n$$\nb^{T} c = 1.\n$$\n- It leaves $\\operatorname{col}(A)$ invariant: $(P + c b^{T}) A = P A + c b^{T} A = A + c (b^{T} A)$. Orthogonality of $c$ to $\\operatorname{col}(A)$ implies $c^{T} A = 0^{T}$, so for symmetry (below) we will require $b$ to be parallel to $c$, which then gives $b^{T} A = 0^{T}$ and thus $(P + c b^{T}) A = A$.\n- It is symmetric: $(P + c b^{T})^{T} = P + b c^{T}$. For $P + c b^{T}$ to be symmetric we need $c b^{T} = b c^{T}$, which holds if and only if $b$ is proportional to $c$, say $b = \\alpha c$ for some scalar $\\alpha$.\n\nCombining $b = \\alpha c$ with $b^{T} c = 1$ yields\n$$\n\\alpha \\, c^{T} c = 1 \\quad \\Rightarrow \\quad \\alpha = \\frac{1}{c^{T} c}.\n$$\nTherefore\n$$\nb = \\frac{c}{c^{T} c}.\n$$\n\nFor completeness, verify $A_k^{+} A_k$ has the required properties. Compute\n$$\nA_k^{+} A_k = \\begin{pmatrix} A^{+} - d b^{T} \\\\ b^{T} \\end{pmatrix} [A, v]\n= \\begin{pmatrix}\nA^{+} A - d b^{T} A  A^{+} v - d b^{T} v \\\\\nb^{T} A  b^{T} v\n\\end{pmatrix}.\n$$\nWith $b = \\frac{c}{c^{T} c}$ and $c \\perp \\operatorname{col}(A)$, we have $b^{T} A = \\frac{c^{T} A}{c^{T} c} = 0^{T}$. Hence the top-left block simplifies to $A^{+} A$. Moreover,\n$$\nb^{T} v = \\frac{c^{T} v}{c^{T} c} = \\frac{c^{T} (A A^{+} v + c)}{c^{T} c} = \\frac{c^{T} c}{c^{T} c} = 1,\n$$\nso the top-right block becomes $A^{+} v - d \\cdot 1 = d - d = 0$. Therefore\n$$\nA_k^{+} A_k = \\begin{pmatrix} A^{+} A  0 \\\\ 0  1 \\end{pmatrix},\n$$\nwhich is symmetric and idempotent, fulfilling the Moore-Penrose conditions. Hence, under $c \\neq 0$, the required vector is\n$$\nb = \\frac{c}{c^{T} c}.\n$$", "answer": "$$\\boxed{\\frac{c}{c^{T} c}}$$", "id": "1397278"}]}