{"hands_on_practices": [{"introduction": "The SVD expresses any matrix as a sum of rank-one components, weighted by the singular values. This exercise [@problem_id:1399063] provides a hands-on look at the most fundamental case: a matrix that is already a single rank-one component. By constructing the SVD for an outer product of two vectors, you will see how the singular value and singular vectors directly relate to the lengths and directions of these constituent vectors, providing a concrete foundation for understanding the SVD's structure.", "problem": "Consider a matrix $A \\in \\mathbb{R}^{3 \\times 2}$ defined by the outer product $A = \\vec{u}\\vec{v}^T$, where the column vectors $\\vec{u}$ and $\\vec{v}$ are given by\n$$\n\\vec{u} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix} \\quad \\text{and} \\quad \\vec{v} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}.\n$$\nA Singular Value Decomposition (SVD) of $A$ is a factorization of the form $A = U\\Sigma V^T$, where $U$ is a $3 \\times 3$ orthogonal matrix, $V$ is a $2 \\times 2$ orthogonal matrix, and $\\Sigma$ is a $3 \\times 2$ rectangular diagonal matrix with non-negative real numbers (the singular values) on the diagonal, arranged in descending order.\n\nLet $\\vec{u}_1$ be the first column of $U$ and $\\vec{v}_1$ be the first column of $V$. Which of the following options correctly identifies the matrix $\\Sigma$ and the vectors $\\vec{u}_1$ and $\\vec{v}_1$?\n\nA. $\\Sigma = \\begin{pmatrix} 15  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\end{pmatrix}$\n\nB. $\\Sigma = \\begin{pmatrix} 9  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\end{pmatrix}$\n\nC. $\\Sigma = \\begin{pmatrix} 15  0  0 \\\\ 0  0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\\\ 0 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\end{pmatrix}$\n\nD. $\\Sigma = \\begin{pmatrix} 225  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\vec{u}_1 = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\quad \\vec{v}_1 = \\begin{pmatrix} 4/5 \\\\ 3/5 \\end{pmatrix}$", "solution": "We are given $A \\in \\mathbb{R}^{3 \\times 2}$ defined by $A=\\vec{u}\\vec{v}^{T}$ with $\\vec{u}=\\begin{pmatrix}1\\\\2\\\\2\\end{pmatrix}$ and $\\vec{v}=\\begin{pmatrix}4\\\\3\\end{pmatrix}$. Since $A$ is an outer product of two nonzero vectors, it has rank $1$. For a rank-$1$ matrix of the form $\\vec{u}\\vec{v}^{T}$, the singular value decomposition has exactly one nonzero singular value.\n\nA standard derivation uses\n$$\nA A^{T} = (\\vec{u}\\vec{v}^{T})(\\vec{u}\\vec{v}^{T})^{T} = \\vec{u}\\vec{v}^{T}\\vec{v}\\vec{u}^{T} = (\\vec{v}^{T}\\vec{v})\\,\\vec{u}\\vec{u}^{T}.\n$$\nThe matrix $\\vec{u}\\vec{u}^{T}$ has one nonzero eigenvalue $\\vec{u}^{T}\\vec{u}$ with eigenvector $\\vec{u}$. Therefore $A A^{T}$ has one nonzero eigenvalue\n$$\n\\lambda_{1} = (\\vec{v}^{T}\\vec{v})(\\vec{u}^{T}\\vec{u}) = \\|\\vec{v}\\|^{2}\\,\\|\\vec{u}\\|^{2},\n$$\nand the corresponding unit eigenvector is $\\vec{u}/\\|\\vec{u}\\|$. The unique nonzero singular value is $\\sigma_{1} = \\sqrt{\\lambda_{1}} = \\|\\vec{u}\\|\\,\\|\\vec{v}\\|$, and the first left singular vector is\n$$\n\\vec{u}_{1} = \\frac{\\vec{u}}{\\|\\vec{u}\\|}.\n$$\nSimilarly, from $A^{T}A = (\\vec{u}^{T}\\vec{u})\\,\\vec{v}\\vec{v}^{T}$, the first right singular vector is\n$$\n\\vec{v}_{1} = \\frac{\\vec{v}}{\\|\\vec{v}\\|}.\n$$\n\nCompute the norms:\n$$\n\\|\\vec{u}\\| = \\sqrt{1^{2}+2^{2}+2^{2}} = \\sqrt{9} = 3, \\quad \\|\\vec{v}\\| = \\sqrt{4^{2}+3^{2}} = \\sqrt{25} = 5.\n$$\nHence the unique nonzero singular value is\n$$\n\\sigma_{1} = \\|\\vec{u}\\|\\,\\|\\vec{v}\\| = 3 \\cdot 5 = 15.\n$$\nThe corresponding singular vectors are\n$$\n\\vec{u}_{1} = \\frac{1}{3}\\begin{pmatrix}1\\\\2\\\\2\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{3}\\\\\\frac{2}{3}\\\\\\frac{2}{3}\\end{pmatrix}, \n\\quad\n\\vec{v}_{1} = \\frac{1}{5}\\begin{pmatrix}4\\\\3\\end{pmatrix} = \\begin{pmatrix}\\frac{4}{5}\\\\\\frac{3}{5}\\end{pmatrix}.\n$$\nTherefore, the SVD has $\\Sigma$ as a $3 \\times 2$ diagonal rectangular matrix with the first diagonal entry equal to $15$ and the rest zero:\n$$\n\\Sigma = \\begin{pmatrix} 15  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nComparing with the given options, this matches option A.", "answer": "$$\\boxed{A}$$", "id": "1399063"}, {"introduction": "The structure of the Singular Value Decomposition reveals a beautiful symmetry between a matrix and its transpose. This practice [@problem_id:1399057] guides you to discover this relationship by deriving the SVD of $A^T$ directly from the SVD of $A$. This exercise highlights how the left and right singular vectors swap roles, reinforcing the idea that the SVD provides a simultaneous and complete picture of a matrix's fundamental subspaces.", "problem": "Let $A$ be a real matrix of size $m \\times n$. The Singular Value Decomposition (SVD) of $A$ is a factorization of the form $A = U \\Sigma V^T$, where:\n- $U$ is an $m \\times m$ orthogonal matrix whose columns are the left-singular vectors of $A$.\n- $\\Sigma$ is an $m \\times n$ rectangular diagonal matrix with the non-negative singular values of $A$ on its diagonal.\n- $V$ is an $n \\times n$ orthogonal matrix whose columns are the right-singular vectors of $A$.\n\nNow, consider the transpose of matrix $A$, denoted as $A^T$. Its SVD must also be expressible in a standard form, which we will write as $A^T = U' \\Sigma' (V')^T$. The matrix $U'$ must be an $n \\times n$ orthogonal matrix, $\\Sigma'$ must be an $n \\times m$ rectangular diagonal matrix, and $V'$ must be an $m \\times m$ orthogonal matrix.\n\nWhich of the following choices correctly identifies the matrices $U'$, $\\Sigma'$, and $V'$ in terms of the original matrices $U$, $\\Sigma$, and $V$?\n\nA. $U'=U, \\quad \\Sigma'=\\Sigma, \\quad V'=V$\n\nB. $U'=V, \\quad \\Sigma'=\\Sigma, \\quad V'=U$\n\nC. $U'=V, \\quad \\Sigma'=\\Sigma^T, \\quad V'=U$\n\nD. $U'=U, \\quad \\Sigma'=\\Sigma^T, \\quad V'=V$\n\nE. $U'=U^T, \\quad \\Sigma'=\\Sigma^T, \\quad V'=V^T$", "solution": "Start from the given singular value decomposition of $A$:\n$$\nA = U \\Sigma V^{T},\n$$\nwhere $U$ is $m \\times m$ orthogonal, $V$ is $n \\times n$ orthogonal, and $\\Sigma$ is $m \\times n$ rectangular diagonal with nonnegative diagonal entries (the singular values).\n\nTake the transpose of both sides and use the identity $(XYZ)^{T} = Z^{T} Y^{T} X^{T}$ together with $(V^{T})^{T} = V$:\n$$\nA^{T} = (U \\Sigma V^{T})^{T} = V \\Sigma^{T} U^{T}.\n$$\nWe want to express $A^{T}$ in SVD form as $A^{T} = U' \\Sigma' (V')^{T}$, where $U'$ is $n \\times n$ orthogonal, $\\Sigma'$ is $n \\times m$ rectangular diagonal with nonnegative diagonal entries, and $V'$ is $m \\times m$ orthogonal.\n\nComparing $A^{T} = V \\Sigma^{T} U^{T}$ with the target form $U' \\Sigma' (V')^{T}$, we can set\n$$\nU' = V, \\quad \\Sigma' = \\Sigma^{T}, \\quad V' = U.\n$$\nThese choices satisfy all requirements:\n- $U' = V$ is $n \\times n$ orthogonal, and $V' = U$ is $m \\times m$ orthogonal, because $U$ and $V$ are orthogonal.\n- $\\Sigma' = \\Sigma^{T}$ is $n \\times m$ rectangular diagonal with the same nonnegative diagonal entries as $\\Sigma$.\n\nAs an internal consistency check, the defining relation $U'^{T} A^{T} V' = \\Sigma'$ holds:\n$$\nU'^{T} A^{T} V' = V^{T} (V \\Sigma^{T} U^{T}) U = (V^{T} V) \\Sigma^{T} (U^{T} U) = \\Sigma^{T} = \\Sigma'.\n$$\nTherefore, the correct identification is $U' = V$, $\\Sigma' = \\Sigma^{T}$, and $V' = U$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1399057"}, {"introduction": "One of the most powerful applications of the SVD is in matrix approximation, which is central to fields like data compression and machine learning. This exercise [@problem_id:1399073] presents a classic problem: finding the smallest perturbation to an invertible matrix to make it singular. Solving this provides a profound insight into the meaning of singular values, particularly the smallest one, $\\sigma_n$, as a measure of the matrix's \"distance to singularity\".", "problem": "Let $A$ be a real, invertible $n \\times n$ matrix. The Singular Value Decomposition (SVD) of $A$ is given by $A = U\\Sigma V^T$, where $U$ and $V$ are $n \\times n$ orthogonal matrices, and $\\Sigma$ is a diagonal matrix. The columns of $U$ are the left singular vectors, denoted $\\{u_1, u_2, \\dots, u_n\\}$, and the columns of $V$ are the right singular vectors, denoted $\\{v_1, v_2, \\dots, v_n\\}$. The diagonal entries of $\\Sigma$ are the singular values of $A$, ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n  0$.\n\nConsider a rank-one matrix $E$ that, when added to $A$, makes the resulting matrix $A+E$ singular (i.e., not invertible). Your task is to find the specific rank-one matrix $E$ for which the Frobenius norm, $\\|E\\|_F$, is minimized. The Frobenius norm of a matrix $M$ is defined as $\\|M\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n M_{ij}^2}$.\n\nExpress the matrix $E$ that satisfies these conditions in terms of the singular values and singular vectors of $A$.", "solution": "Let $A = U \\Sigma V^{T}$ be the SVD of $A$, where $\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{n})$ with $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{n}  0$, and $U = [u_{1}\\ \\cdots\\ u_{n}]$, $V = [v_{1}\\ \\cdots\\ v_{n}]$ are orthogonal.\n\nWe seek a rank-one matrix $E$ such that $A+E$ is singular and $\\|E\\|_{F}$ is minimized. Use the orthogonal invariance of the Frobenius norm and rank: for $\\tilde{E} \\coloneqq U^{T} E V$, one has $\\|E\\|_{F} = \\|\\tilde{E}\\|_{F}$ and\n$$\nA + E \\text{ is singular } \\Longleftrightarrow \\Sigma + \\tilde{E} \\text{ is singular},\n$$\nbecause $A+E = U(\\Sigma + \\tilde{E})V^{T}$ and $U,V$ are orthogonal.\n\nTo produce singularity with a rank-one perturbation of smallest Frobenius norm, set\n$$\n\\tilde{E} = - \\sigma_{n} e_{n} e_{n}^{T},\n$$\nwhere $e_{n}$ is the $n$-th standard basis vector. Then $\\Sigma + \\tilde{E} = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{n-1},0)$ is singular, and $\\tilde{E}$ is rank one with\n$$\n\\|\\tilde{E}\\|_{F} = \\sigma_{n}\\|e_{n} e_{n}^{T}\\|_{F} = \\sigma_{n}.\n$$\nMapping back gives\n$$\nE = U \\tilde{E} V^{T} = -\\sigma_{n} U e_{n} e_{n}^{T} V^{T} = -\\sigma_{n} u_{n} v_{n}^{T}.\n$$\nThis $E$ indeed makes $A+E$ singular because\n$$\n(A+E) v_{n} = A v_{n} + E v_{n} = \\sigma_{n} u_{n} - \\sigma_{n} u_{n} = 0.\n$$\n\nTo prove minimality, let $s_{1} \\ge \\cdots \\ge s_{n} \\ge 0$ be the singular values of $A+E$. Since $A+E$ is singular, $s_{n} = 0$. By the Hoffmanâ€“Wielandt inequality,\n$$\n\\sum_{i=1}^{n} (\\sigma_{i} - s_{i})^{2} \\le \\|E\\|_{F}^{2}.\n$$\nThus\n$$\n\\|E\\|_{F}^{2} \\ge (\\sigma_{n} - 0)^{2} = \\sigma_{n}^{2},\n$$\nso $\\|E\\|_{F} \\ge \\sigma_{n}$. Our construction achieves $\\|E\\|_{F} = \\sigma_{n}$, hence it is optimal. Therefore, a minimizing rank-one perturbation is\n$$\nE = -\\sigma_{n} u_{n} v_{n}^{T}.\n$$\nIf $\\sigma_{n}$ has multiplicity greater than one, any choice of unit left and right singular vectors from the corresponding singular subspaces yields an optimal $E$ of the same form.", "answer": "$$\\boxed{-\\sigma_{n} u_{n} v_{n}^{T}}$$", "id": "1399073"}]}