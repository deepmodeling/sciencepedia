{
    "hands_on_practices": [
        {
            "introduction": "Lévy's continuity theorem builds on a fundamental one-to-one relationship between a probability distribution and its characteristic function. This first practice exercise  serves as a crucial warm-up, challenging you to work backward from a given limiting characteristic function to identify the underlying probability distribution. Mastering this 'reverse lookup' is essential for applying the theorem effectively.",
            "id": "1319208",
            "problem": "In the study of stochastic processes, the convergence of sequences of random variables is a fundamental concept. One powerful tool for analyzing this is the characteristic function of a random variable. The characteristic function of a random variable $X$ is defined as $\\phi_X(t) = E[\\exp(itX)]$, where $t$ is a real number and $i$ is the imaginary unit.\n\nConsider a sequence of random variables $X_1, X_2, X_3, \\dots$, denoted by $\\{X_n\\}_{n=1}^{\\infty}$. Let $\\phi_{X_n}(t)$ be the characteristic function of the random variable $X_n$. It is observed that for every real number $t$, the sequence of functions $\\phi_{X_n}(t)$ converges to a limiting function $\\phi(t)$ as $n \\to \\infty$. The limiting function is given by:\n$$ \\phi(t) = \\exp(-|t|) $$\n\nAccording to Lévy's continuity theorem, this convergence implies that the sequence of random variables $\\{X_n\\}$ converges in distribution to a random variable $X$, whose characteristic function is $\\phi(t)$.\n\nIdentify the probability distribution of the limiting random variable $X$.\n\nA. The standard Normal distribution, with probability density function $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$.\nB. The Laplace distribution with location parameter 0 and scale parameter 1, with probability density function $f(x) = \\frac{1}{2} \\exp(-|x|)$.\nC. The Cauchy distribution with location parameter 0 and scale parameter 1, with probability density function $f(x) = \\frac{1}{\\pi(1+x^2)}$.\nD. The Exponential distribution with rate parameter $\\lambda=1$, with probability density function $f(x) = \\exp(-x)$ for $x \\ge 0$.\nE. The Logistic distribution with location 0 and scale 1, with probability density function $f(x) = \\frac{\\exp(-x)}{(1+\\exp(-x))^2}$.",
            "solution": "We are given that for each real $t$, the characteristic functions $\\phi_{X_{n}}(t)$ converge pointwise to $\\phi(t)=\\exp(-|t|)$. By Lévy’s continuity theorem, the sequence $\\{X_{n}\\}$ converges in distribution to a random variable $X$ whose characteristic function is $\\phi_{X}(t)=\\exp(-|t|)$. Since characteristic functions determine distributions uniquely, it suffices to identify the distribution whose characteristic function equals $\\exp(-|t|)$.\n\nConsider a standard Cauchy random variable $Y$ with location parameter $0$ and scale parameter $1$, having probability density function\n$$\nf_{Y}(x)=\\frac{1}{\\pi}\\frac{1}{1+x^{2}}.\n$$\nIts characteristic function is\n$$\n\\phi_{Y}(t)=\\mathbb{E}[\\exp(i t Y)]=\\int_{-\\infty}^{\\infty}\\exp(i t x)\\frac{1}{\\pi}\\frac{1}{1+x^{2}}\\,dx.\n$$\nWrite $\\exp(i t x)=\\cos(t x)+i\\sin(t x)$; the sine term integrates to zero by oddness, so\n$$\n\\phi_{Y}(t)=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}\\frac{\\cos(t x)}{1+x^{2}}\\,dx.\n$$\nUsing the standard integral $\\int_{-\\infty}^{\\infty}\\frac{\\cos(t x)}{1+x^{2}}\\,dx=\\pi\\exp(-|t|)$ for real $t$, we obtain\n$$\n\\phi_{Y}(t)=\\exp(-|t|).\n$$\nTherefore, the limiting characteristic function $\\phi(t)=\\exp(-|t|)$ corresponds exactly to the standard Cauchy distribution with location $0$ and scale $1$. Among the given options, this is option C.\n\nFor completeness, we note that other listed distributions have different characteristic functions: Normal has $\\exp(-t^{2}/2)$, Laplace$(0,1)$ has $1/(1+t^{2})$, Exponential has $1/(1-i t)$ for rate $1$, and Logistic$(0,1)$ has a different form; none equal $\\exp(-|t|)$.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Building on the previous exercise, we now move to the forward application of the continuity theorem, which is central to proving many limit theorems in probability. This practice  demonstrates the analytical power of this approach by asking you to determine the limit of a sequence of complex-looking characteristic functions. You will use calculus techniques to find the limiting function and then apply the theorem to identify the resulting distribution.",
            "id": "824959",
            "problem": "Consider a sequence of probability measures $(\\mu_n)_{n \\in \\mathbb{N}}$ on the real line $\\mathbb{R}$. The characteristic function $\\phi_n(t)$ corresponding to each measure $\\mu_n$ is given by:\n$$\n\\phi_n(t) = \\left(\\cos\\left(\\frac{t}{n}\\right)\\right)^{n^2}\n$$\nThe characteristic function of a probability measure $\\mu$ on $\\mathbb{R}$ with a density function $f(x)$ is defined as $\\phi(t) = \\int_{-\\infty}^{\\infty} e^{itx} f(x) dx$.\n\nAccording to Lévy's continuity theorem, if the sequence of characteristic functions $\\phi_n(t)$ converges pointwise for all $t \\in \\mathbb{R}$ to a function $\\phi(t)$ that is continuous at $t=0$, then the sequence of measures $\\mu_n$ converges weakly to a measure $\\mu$ whose characteristic function is $\\phi(t)$.\n\nAssuming that the weak limit measure $\\mu$ has a probability density function, denoted by $f(x)$, derive the closed-form expression for $f(x)$.",
            "solution": "1. The characteristic function of $\\mu_n$ is\n$$\n\\phi_n(t)=\\Bigl(\\cos\\frac{t}{n}\\Bigr)^{n^2}.\n$$\n2. Take the logarithm and use the expansion $\\cos u=1-\\tfrac{u^2}{2}+O(u^4)$:\n$$\n\\ln\\phi_n(t)\n=n^2\\ln\\!\\Bigl(1-\\tfrac{t^2}{2n^2}+O(n^{-4})\\Bigr)\n=n^2\\Bigl(-\\tfrac{t^2}{2n^2}+O(n^{-4})\\Bigr)\n=-\\tfrac{t^2}{2}+O(n^{-2}).\n$$\n3. As $n\\to\\infty$,\n$$\n\\phi(t)=\\lim_{n\\to\\infty}\\phi_n(t)\n=\\exp\\Bigl(-\\tfrac{t^2}{2}\\Bigr).\n$$\n4. The inverse Fourier transform gives the density:\n$$\nf(x)\n=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-itx}\\,e^{-t^2/2}\\,dt\n=\\frac{1}{\\sqrt{2\\pi}}\\,e^{-x^2/2}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\bigl(-\\tfrac{x^2}{2}\\bigr)}$$"
        },
        {
            "introduction": "The elegance of the continuity theorem extends to analyzing more intricate probabilistic models, such as mixture distributions. This final practice  presents a fascinating scenario where a distribution is a blend of two different types of behavior, with the mixing proportions changing over time. By analyzing the characteristic functions, you can precisely determine the ultimate fate of the system, revealing insights that might not be immediately obvious.",
            "id": "1353086",
            "problem": "A team of physicists is developing a highly sensitive measurement device. The error in a single measurement, denoted by the random variable $X_n$, is modeled as a mixture of two distinct noise sources at stage $n$ of the device's development cycle ($n=1, 2, 3, \\dots$).\n\nWith a high probability of $1 - \\frac{1}{n}$, the error is dominated by thermal noise and follows a standard normal distribution, which has a probability density function (PDF) of $f_N(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$ for $-\\infty < x < \\infty$.\n\nHowever, with a small probability of $\\frac{1}{n}$, a rare quantum tunneling event introduces a significant outlier, and the error follows a standard Cauchy distribution. The PDF for this anomalous error is $f_C(x) = \\frac{1}{\\pi(1+x^2)}$ for $-\\infty < x < \\infty$.\n\nAs the team refines the device, the probability of the quantum tunneling event diminishes, i.e., as $n \\to \\infty$. The question is to determine the limiting statistical behavior of the measurement error. This is known as the convergence in distribution of the sequence of random variables $\\{X_n\\}$.\n\nWhich of the following describes the limiting distribution of $X_n$ as $n \\to \\infty$?\n\nA. A standard normal distribution.\nB. A standard Cauchy distribution.\nC. A degenerate distribution at $x=0$ (i.e., a distribution where the probability of being exactly zero is 1).\nD. The sequence of distributions does not converge to any valid probability distribution.\nE. A normal distribution with mean 0 and variance $1/2$.",
            "solution": "Introduce an indicator variable for the mixture mechanism. Let $B_{n}$ be Bernoulli with parameter $p_{n}=\\frac{1}{n}$, independent of $N$ and $C$, where $N$ has the standard normal distribution with cumulative distribution function $\\Phi(x)$ and $C$ has the standard Cauchy distribution with cumulative distribution function $F_{C}(x)$. Define\n$$\nX_{n}=(1-B_{n})N+B_{n}C.\n$$\nBy the law of total probability and conditioning on $B_{n}$, for any $x\\in\\mathbb{R}$ the cumulative distribution function $F_{X_{n}}(x)$ of $X_{n}$ satisfies\n$$\nF_{X_{n}}(x)=\\mathbb{P}(X_{n}\\le x)=\\mathbb{E}\\big[\\mathbb{P}(X_{n}\\le x\\mid B_{n})\\big]=(1-p_{n})\\Phi(x)+p_{n}F_{C}(x).\n$$\nSubstituting $p_{n}=\\frac{1}{n}$ gives\n$$\nF_{X_{n}}(x)=\\left(1-\\frac{1}{n}\\right)\\Phi(x)+\\frac{1}{n}F_{C}(x).\n$$\nFor each fixed $x\\in\\mathbb{R}$, both $\\Phi(x)$ and $F_{C}(x)$ are finite and continuous, and $\\frac{1}{n}\\to 0$ as $n\\to\\infty$. Therefore,\n$$\n\\lim_{n\\to\\infty}F_{X_{n}}(x)=\\Phi(x)\\quad\\text{for all }x\\in\\mathbb{R}.\n$$\nSince $\\Phi$ is the cumulative distribution function of the standard normal distribution, by the definition of convergence in distribution, $X_{n}\\xrightarrow{d}N(0,1)$ as $n\\to\\infty$. Hence the limiting distribution is the standard normal, which corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}