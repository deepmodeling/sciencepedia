{
    "hands_on_practices": [
        {
            "introduction": "This first practice illustrates one of the most intuitive statements of the Portmanteau Theorem. We will examine a sequence of random variables whose distributions become increasingly concentrated around a single point, $c$. By computing the limit of the expectation for a bounded, continuous function, you will directly verify a fundamental condition for weak convergence to a constant, a concept central to understanding consistency in estimators. ",
            "id": "1404905",
            "problem": "In a high-precision physics experiment, a sequence of measurements, denoted by the random variables $X_n$ for $n = 1, 2, 3, \\ldots$, are taken to determine a constant physical parameter $c$. The measurement device improves with each trial, and the uncertainty decreases. The $n$-th measurement $X_n$ is modeled as a random variable following a uniform distribution on the interval $[c - 1/n, c + 1/n]$.\n\nA theoretical model predicts that the expected value of a related quantity, $g(X_n)$, should converge to a specific value as the number of measurements becomes infinitely large. The function $g(x)$ is defined as $g(x) = A \\sin(\\omega x) + B \\cos(\\omega x)$, where $A$, $B$, $\\omega$, and $c$ are given real constants, with $\\omega \\neq 0$.\n\nYour task is to determine the limiting value of the expectation of $g(X_n)$ as $n$ approaches infinity. Find the value of the limit:\n$$L = \\lim_{n \\to \\infty} \\mathbb{E}[g(X_n)]$$\nExpress your answer as a single closed-form analytic expression in terms of the constants $A, B, \\omega$, and $c$.",
            "solution": "Let $X_{n} \\sim \\text{Uniform}[c - 1/n, c + 1/n]$. Its density is $f_{n}(x) = \\frac{n}{2}$ for $x \\in [c - 1/n, c + 1/n]$ and zero otherwise. By linearity of expectation,\n$$\n\\mathbb{E}[g(X_{n})] \\equiv \\mathbb{E}[A \\sin(\\omega X_{n}) + B \\cos(\\omega X_{n})] = A\\,\\mathbb{E}[\\sin(\\omega X_{n})] + B\\,\\mathbb{E}[\\cos(\\omega X_{n})].\n$$\nCompute each term via integration:\n$$\n\\mathbb{E}[\\sin(\\omega X_{n})] = \\frac{n}{2} \\int_{c - 1/n}^{c + 1/n} \\sin(\\omega x)\\,dx = \\frac{n}{2} \\cdot \\frac{1}{\\omega}\\left[-\\cos(\\omega x)\\right]_{x=c - 1/n}^{x=c + 1/n}.\n$$\nUsing the identity $\\cos u - \\cos v = -2 \\sin\\!\\left(\\frac{u+v}{2}\\right)\\sin\\!\\left(\\frac{u-v}{2}\\right)$ with $u = \\omega(c - 1/n)$ and $v = \\omega(c + 1/n)$, we obtain\n$$\n\\mathbb{E}[\\sin(\\omega X_{n})] = \\frac{n}{2} \\cdot \\frac{1}{\\omega}\\left(\\cos(\\omega(c - 1/n)) - \\cos(\\omega(c + 1/n))\\right) = \\frac{n}{\\omega}\\,\\sin(\\omega c)\\,\\sin\\!\\left(\\frac{\\omega}{n}\\right).\n$$\nSimilarly,\n$$\n\\mathbb{E}[\\cos(\\omega X_{n})] = \\frac{n}{2} \\int_{c - 1/n}^{c + 1/n} \\cos(\\omega x)\\,dx = \\frac{n}{2} \\cdot \\frac{1}{\\omega}\\left[\\sin(\\omega x)\\right]_{x=c - 1/n}^{x=c + 1/n}.\n$$\nUsing $\\sin v - \\sin u = 2 \\cos\\!\\left(\\frac{u+v}{2}\\right)\\sin\\!\\left(\\frac{v-u}{2}\\right)$ with $u = \\omega(c - 1/n)$ and $v = \\omega(c + 1/n)$, we obtain\n$$\n\\mathbb{E}[\\cos(\\omega X_{n})] = \\frac{n}{2} \\cdot \\frac{1}{\\omega}\\left(\\sin(\\omega(c + 1/n)) - \\sin(\\omega(c - 1/n))\\right) = \\frac{n}{\\omega}\\,\\cos(\\omega c)\\,\\sin\\!\\left(\\frac{\\omega}{n}\\right).\n$$\nTherefore,\n$$\n\\mathbb{E}[g(X_{n})] = \\frac{n}{\\omega}\\,\\sin\\!\\left(\\frac{\\omega}{n}\\right)\\,\\big(A \\sin(\\omega c) + B \\cos(\\omega c)\\big).\n$$\nSince $\\omega \\neq 0$, set $y_{n} = \\frac{\\omega}{n}$ so that $y_{n} \\to 0$ as $n \\to \\infty$, and use $\\lim_{y \\to 0} \\frac{\\sin y}{y} = 1$ to get\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}[g(X_{n})] = \\left(\\lim_{n \\to \\infty} \\frac{n}{\\omega}\\sin\\!\\left(\\frac{\\omega}{n}\\right)\\right)\\big(A \\sin(\\omega c) + B \\cos(\\omega c)\\big) = A \\sin(\\omega c) + B \\cos(\\omega c).\n$$\nThis also agrees with the heuristic that $X_{n}$ concentrates at $c$ and $g$ is continuous, so $\\mathbb{E}[g(X_{n})] \\to g(c)$.",
            "answer": "$$\\boxed{A \\sin(\\omega c)+B \\cos(\\omega c)}$$"
        },
        {
            "introduction": "Building on the previous example, this exercise explores a critical and often misunderstood nuance of weak convergence. A common misconception is that convergence in distribution implies the convergence of moments, such as the mean. This problem presents a classic counterexample where a sequence of random variables converges to a point mass at zero, yet its expected value converges to a different number entirely, showcasing why weak convergence is distinct from other modes of convergence. ",
            "id": "1404921",
            "problem": "Consider a sequence of discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each integer $n \\geq 1$, the random variable $X_n$ can take one of two values: $1/n$ or $n$. The probabilities are given by $P(X_n = 1/n) = 1 - 1/n$ and $P(X_n = n) = 1/n$. This sequence could represent a simplified model for the error in an iterative algorithm at step $n$, where there is a high probability of a small, decreasing error, but a small probability of a large, diverging error.\n\nWhich of the following statements accurately describes the limiting behavior of the sequence $X_n$ as $n \\to \\infty$?\n\nA. $X_n$ converges in distribution to a random variable with a standard normal distribution.\n\nB. $X_n$ converges in distribution to a random variable that is a point mass at 0.\n\nC. $X_n$ converges in distribution to a random variable that is a point mass at 1.\n\nD. The sequence of random variables $\\{X_n\\}$ does not converge in distribution.\n\nE. $X_n$ converges in distribution to a point mass at 0, because the expected value of $X_n$ converges to 0.",
            "solution": "To determine the limiting behavior of the sequence of random variables $\\{X_n\\}$, we will use the definition of convergence in distribution (also known as weak convergence). A sequence of random variables $X_n$ converges in distribution to a random variable $X$, denoted $X_n \\xrightarrow{d} X$, if the Cumulative Distribution Function (CDF) of $X_n$, $F_{X_n}(x)$, converges to the CDF of $X$, $F_X(x)$, at every point $x$ where $F_X(x)$ is continuous.\n\nFirst, let's form an intuition about the limit. As $n \\to \\infty$, the probability that $X_n = 1/n$ approaches 1, while the value $1/n$ approaches 0. The probability that $X_n = n$ approaches 0. This suggests that the entire probability mass is concentrating at the point 0. So, we propose that the limiting random variable $X$ is a point mass at 0, meaning $P(X=0)=1$.\n\nThe CDF of the proposed limiting random variable $X$ is:\n$$\nF_X(x) = P(X \\le x) =\n\\begin{cases}\n0 & \\text{if } x < 0 \\\\\n1 & \\text{if } x \\ge 0\n\\end{cases}\n$$\nThis CDF, $F_X(x)$, is continuous for all $x \\in \\mathbb{R}$ except at the point $x=0$, where it has a jump discontinuity. According to the definition of convergence in distribution, we need to check if $\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x)$ for all $x \\neq 0$.\n\nNext, let's find the CDF of $X_n$. Since $X_n$ can only take values $1/n$ and $n$, its CDF, $F_{X_n}(x) = P(X_n \\le x)$, is a step function:\n$$\nF_{X_n}(x) =\n\\begin{cases}\n0 & \\text{if } x < 1/n \\\\\n1 - 1/n & \\text{if } 1/n \\le x < n \\\\\n1 & \\text{if } x \\ge n\n\\end{cases}\n$$\nNow, we analyze the limit of $F_{X_n}(x)$ as $n \\to \\infty$ for all $x \\neq 0$.\n\nCase 1: $x < 0$.\nFor any $n \\geq 1$, we have $1/n > 0 > x$. Therefore, for any $x < 0$, $F_{X_n}(x) = 0$ for all $n$.\n$$ \\lim_{n \\to \\infty} F_{X_n}(x) = \\lim_{n \\to \\infty} 0 = 0 $$\nThis matches $F_X(x)$ for $x < 0$.\n\nCase 2: $x > 0$.\nFor any given $x > 0$, we can choose an integer $N$ large enough such that for all $n > N$, we have $1/n < x$. Specifically, this holds for all $n > 1/x$. Also, for $n > x$, we have $x < n$. Thus, for any $n > \\max(x, 1/x)$, the inequality $1/n \\le x < n$ is satisfied.\nFor such large $n$, the CDF of $X_n$ at point $x$ is $F_{X_n}(x) = 1 - 1/n$.\nTaking the limit as $n \\to \\infty$:\n$$ \\lim_{n \\to \\infty} F_{X_n}(x) = \\lim_{n \\to \\infty} (1 - 1/n) = 1 $$\nThis matches $F_X(x)$ for $x > 0$.\n\nSince $\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x)$ for all continuity points of $F_X$ (i.e., for all $x \\neq 0$), we conclude that $X_n$ converges in distribution to a point mass at 0. This means statement B is correct.\n\nNow let's evaluate the other options to be certain.\nA. The limit is a point mass, not a continuous distribution like the normal distribution. So, A is incorrect.\nC. The limit is a point mass at 0, not at 1. So, C is incorrect.\nD. We have just shown that the sequence does converge in distribution. So, D is incorrect.\nE. This statement claims that the convergence is due to the expectation of $X_n$ converging to 0. Let's calculate the expectation of $X_n$:\n$$ \\mathbb{E}[X_n] = \\sum_{k} k \\cdot P(X_n=k) = \\left(\\frac{1}{n}\\right) P\\left(X_n = \\frac{1}{n}\\right) + (n) P(X_n = n) $$\n$$ \\mathbb{E}[X_n] = \\left(\\frac{1}{n}\\right) \\left(1 - \\frac{1}{n}\\right) + (n) \\left(\\frac{1}{n}\\right) = \\frac{1}{n} - \\frac{1}{n^2} + 1 $$\nNow we take the limit of the expectation:\n$$ \\lim_{n \\to \\infty} \\mathbb{E}[X_n] = \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n} - \\frac{1}{n^2}\\right) = 1 + 0 - 0 = 1 $$\nThe expectation of $X_n$ converges to 1, not 0. Therefore, the reasoning provided in statement E is false. An accurate statement must have correct reasoning. Even though the first part of the statement (\"$X_n$ converges in distribution to a point mass at 0\") is true, the overall statement is not accurate because the justification is incorrect. This example famously illustrates that convergence in distribution does not imply convergence of moments (in this case, the mean).\n\nThus, statement B is the only accurate description of the limiting behavior.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Our final practice delves into a more subtle mechanism of weak convergence that connects to principles in Fourier analysis. You will analyze a sequence of probability densities that feature increasingly rapid oscillations. This exercise demonstrates how, when integrated against a smooth function like $g(x) = x^3$, these oscillations effectively \"average out\" to zero, causing the sequence of distributions to converge weakly to a simple uniform distributionâ€”a principle elegantly captured by the Riemann-Lebesgue lemma. ",
            "id": "1404893",
            "problem": "Consider a sequence of probability distributions $\\{P_n\\}_{n=1}^{\\infty}$ defined on the interval $[0,1]$. The probability density function (PDF) corresponding to each distribution $P_n$ is given by $f_n(x) = 1 + \\cos(2\\pi n x)$ for $x \\in [0,1]$. Let $X_n$ be a random variable that follows the distribution $P_n$. Your task is to calculate the limit of the third moment of $X_n$ as $n$ approaches infinity. In other words, compute the exact value of the following limit:\n$$ \\lim_{n \\to \\infty} \\mathbb{E}[X_n^3] $$",
            "solution": "The problem asks for the limit of the expectation of $X_n^3$ as $n \\to \\infty$. By definition, the expectation of a function of a continuous random variable is the integral of that function multiplied by the probability density function over its support.\n\nStep 1: Write the expression for the expectation $\\mathbb{E}[X_n^3]$.\nFor a random variable $X_n$ with PDF $f_n(x)$ on $[0,1]$, the third moment is given by:\n$$ \\mathbb{E}[X_n^3] = \\int_{0}^{1} x^3 f_n(x) \\,dx $$\nSubstituting the given PDF, $f_n(x) = 1 + \\cos(2\\pi n x)$, we get:\n$$ \\mathbb{E}[X_n^3] = \\int_{0}^{1} x^3 (1 + \\cos(2\\pi n x)) \\,dx $$\n\nStep 2: Split the integral into two parts.\nWe can separate the integral into two simpler integrals:\n$$ \\mathbb{E}[X_n^3] = \\int_{0}^{1} x^3 \\,dx + \\int_{0}^{1} x^3 \\cos(2\\pi n x) \\,dx $$\n\nStep 3: Evaluate the first integral.\nThe first integral is a standard polynomial integral:\n$$ \\int_{0}^{1} x^3 \\,dx = \\left[ \\frac{x^4}{4} \\right]_{0}^{1} = \\frac{1^4}{4} - \\frac{0^4}{4} = \\frac{1}{4} $$\n\nStep 4: Evaluate the limit of the second integral as $n \\to \\infty$.\nLet the second integral be $I_n$:\n$$ I_n = \\int_{0}^{1} x^3 \\cos(2\\pi n x) \\,dx $$\nWe need to find $\\lim_{n \\to \\infty} I_n$. We can use the Riemann-Lebesgue lemma, which states that if a function $g(x)$ is in $L^1([a,b])$, i.e., $\\int_a^b |g(x)| dx < \\infty$, then\n$$ \\lim_{\\lambda \\to \\infty} \\int_{a}^{b} g(x) \\cos(\\lambda x) \\,dx = 0 $$\nand\n$$ \\lim_{\\lambda \\to \\infty} \\int_{a}^{b} g(x) \\sin(\\lambda x) \\,dx = 0 $$\nIn our case, the function is $g(x) = x^3$ on the interval $[0,1]$. This function is continuous on a closed interval, so it is bounded and therefore integrable (i.e., in $L^1([0,1])$). We have $\\lambda = 2\\pi n$. As $n \\to \\infty$, $\\lambda$ also approaches infinity.\nApplying the Riemann-Lebesgue lemma, we directly get:\n$$ \\lim_{n \\to \\infty} I_n = \\lim_{n \\to \\infty} \\int_{0}^{1} x^3 \\cos(2\\pi n x) \\,dx = 0 $$\n\nFor completeness, we can also show this result using integration by parts. Let $k = 2\\pi n$.\n$$ I_n = \\int_0^1 x^3 \\cos(kx) dx = \\left[x^3 \\frac{\\sin(kx)}{k}\\right]_0^1 - \\int_0^1 3x^2 \\frac{\\sin(kx)}{k} dx $$\n$$ I_n = \\frac{\\sin(k)}{k} - \\frac{3}{k} \\int_0^1 x^2 \\sin(kx) dx $$\nIntegrating by parts again for the remaining integral:\n$$ \\int_0^1 x^2 \\sin(kx) dx = \\left[x^2 \\frac{-\\cos(kx)}{k}\\right]_0^1 - \\int_0^1 2x \\frac{-\\cos(kx)}{k} dx = -\\frac{\\cos(k)}{k} + \\frac{2}{k} \\int_0^1 x \\cos(kx) dx $$\nIntegrating by parts a third time:\n$$ \\int_0^1 x \\cos(kx) dx = \\left[x \\frac{\\sin(kx)}{k}\\right]_0^1 - \\int_0^1 \\frac{\\sin(kx)}{k} dx = \\frac{\\sin(k)}{k} - \\left[\\frac{-\\cos(kx)}{k^2}\\right]_0^1 = \\frac{\\sin(k)}{k} - \\left(\\frac{1-\\cos(k)}{k^2}\\right) $$\nSubstituting everything back, we get a complex expression for $I_n$ with terms containing powers of $k=2\\pi n$ in the denominator. As $n \\to \\infty$, $k \\to \\infty$, and all terms go to zero because the numerators are bounded by constants. This explicitly confirms that $\\lim_{n \\to \\infty} I_n = 0$.\n\nStep 5: Combine the results to find the final limit.\nWe are looking for $\\lim_{n \\to \\infty} \\mathbb{E}[X_n^3]$.\n$$ \\lim_{n \\to \\infty} \\mathbb{E}[X_n^3] = \\lim_{n \\to \\infty} \\left( \\int_{0}^{1} x^3 \\,dx + \\int_{0}^{1} x^3 \\cos(2\\pi n x) \\,dx \\right) $$\n$$ \\lim_{n \\to \\infty} \\mathbb{E}[X_n^3] = \\lim_{n \\to \\infty} \\left( \\frac{1}{4} + I_n \\right) = \\frac{1}{4} + \\lim_{n \\to \\infty} I_n = \\frac{1}{4} + 0 = \\frac{1}{4} $$\n\nThis result can be understood in the context of weak convergence. The sequence of measures $P_n$ with densities $f_n(x) = 1 + \\cos(2\\pi n x)$ converges weakly to the uniform measure on $[0,1]$, which has a constant PDF $f(x)=1$. The Portmanteau theorem implies that the expectation of any bounded continuous function with respect to $P_n$ converges to the expectation with respect to the limit measure. For $g(x)=x^3$, the limit is:\n$$ \\mathbb{E}[X^3] = \\int_0^1 x^3 f(x) dx = \\int_0^1 x^3 (1) dx = \\frac{1}{4} $$\nOur calculation confirms this.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        }
    ]
}