{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex proofs, it is crucial to have a firm grasp of the definition of almost sure convergence itself. This first practice serves as a conceptual stress test, exploring a sequence that is constructed to misbehave. By analyzing why this sequence fails to converge almost surely , we can reinforce our understanding of the core requirement: the set of outcomes for which the sequence converges must have a probability of 1.",
            "id": "1352867",
            "problem": "Let $Y$ be a random variable defined on a probability space $(\\Omega, \\mathcal{F}, P)$. We are given that the expectation of $Y$, denoted by $E[Y]$, is non-zero. Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined by\n$$X_n = \\cos(n\\pi) Y$$\nfor all positive integers $n$.\n\nWhich of the following statements about the almost sure convergence of the sequence $\\{X_n\\}$ is correct?\n\nA. The sequence $\\{X_n\\}$ converges almost surely to 0.\n\nB. The sequence $\\{X_n\\}$ converges almost surely to a non-zero constant $C$.\n\nC. The sequence $\\{X_n\\}$ converges almost surely to a non-constant random variable.\n\nD. The sequence $\\{X_n\\}$ does not converge almost surely.",
            "solution": "We start from the definition $X_{n}=\\cos(n\\pi)Y$. For integer $n$, we use the trigonometric identity\n$$\n\\cos(n\\pi)=(-1)^{n}.\n$$\nTherefore,\n$$\nX_{n}=(-1)^{n}Y.\n$$\n\nFix $\\omega\\in\\Omega$ and set $y=Y(\\omega)$. Then the pointwise sequence is\n$$\nX_{n}(\\omega)=(-1)^{n}y.\n$$\nWe analyze its pointwise convergence as $n\\to\\infty$:\n- If $y=0$, then $X_{n}(\\omega)=0$ for all $n$, so the sequence converges to $0$.\n- If $y\\neq 0$, consider the subsequences of even and odd indices:\n$$\nX_{2k}(\\omega)=y,\\quad X_{2k+1}(\\omega)=-y.\n$$\nThese subsequences have constant but different limits $y$ and $-y$, respectively, and since $y\\neq 0$, the full sequence does not converge.\n\nHence, the set on which $\\{X_{n}\\}$ converges is exactly $\\{\\omega:Y(\\omega)=0\\}=\\{Y=0\\}$. Therefore,\n$$\nP\\big(\\{ \\omega:\\{X_{n}(\\omega)\\} \\text{ converges} \\}\\big)=P(Y=0).\n$$\nFor almost sure convergence, we would need $P(Y=0)=1$, which would imply $Y=0$ almost surely and consequently $E[Y]=0$, contradicting the given assumption $E[Y]\\neq 0$. Therefore $P(Y=0)\\neq 1$, so the sequence does not converge almost surely.\n\nAmong the options, this corresponds to the statement that the sequence does not converge almost surely.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "With a solid understanding of the definition, we now turn to a fundamental technique for proving almost sure convergence: the Borel-Cantelli lemma. This practice demonstrates how to combine the lemma with a probability bound, derived from Chebyshev's inequality, to rigorously show convergence. You will analyze a dynamically weighted average and prove that it converges to 0 almost surely, a common task in the analysis of stochastic algorithms and time series .",
            "id": "1352849",
            "problem": "Consider an experimental apparatus in a physics lab designed to measure fluctuations around a stable equilibrium point, which is defined as a reference value of zero. The measurements, taken at discrete time intervals $k=1, 2, 3, \\ldots$, are denoted by the sequence of random variables $\\{X_k\\}$. These measurements are independent and identically distributed (i.i.d.). While the process is centered at zero, meaning the expected value of any single measurement is $E[X_k] = 0$, there is inherent random noise. This noise is characterized by a finite, non-zero variance, $\\operatorname{Var}(X_k) = \\sigma^2$.\n\nTo analyze the long-term behavior and suppress noise, a data scientist computes a dynamically weighted average, $S_n$, for the first $n$ measurements, defined as:\n$$ S_n = \\frac{1}{n^2} \\sum_{k=1}^n X_k $$\nDetermine the value that $S_n$ converges to almost surely as the number of measurements $n$ approaches infinity.",
            "solution": "Define $S_n=\\frac{1}{n^{2}}\\sum_{k=1}^{n}X_{k}$ with $\\{X_{k}\\}$ i.i.d., $E[X_{k}]=0$, and $\\operatorname{Var}(X_{k})=\\sigma^{2}\\in(0,\\infty)$. We analyze almost sure convergence of $S_{n}$.\n\nFirst compute the expectation using linearity:\n$$\nE[S_{n}]=\\frac{1}{n^{2}}\\sum_{k=1}^{n}E[X_{k}]=\\frac{1}{n^{2}}\\cdot n \\cdot 0=0.\n$$\n\nNext compute the variance using independence (variance of a sum of independent random variables is the sum of variances):\n$$\n\\operatorname{Var}(S_{n})=\\operatorname{Var}\\left(\\frac{1}{n^{2}}\\sum_{k=1}^{n}X_{k}\\right)=\\frac{1}{n^{4}}\\sum_{k=1}^{n}\\operatorname{Var}(X_{k})=\\frac{1}{n^{4}}\\cdot n \\sigma^{2}=\\frac{\\sigma^{2}}{n^{3}}.\n$$\n\nApply Chebyshev’s inequality for any $\\epsilon>0$:\n$$\nP(|S_{n}|>\\epsilon)\\leq \\frac{\\operatorname{Var}(S_{n})}{\\epsilon^{2}}=\\frac{\\sigma^{2}}{\\epsilon^{2}n^{3}}.\n$$\n\nSum over $n$ and use the $p$-series test with $p=3>1$:\n$$\n\\sum_{n=1}^{\\infty}P(|S_{n}|>\\epsilon)\\leq \\frac{\\sigma^{2}}{\\epsilon^{2}}\\sum_{n=1}^{\\infty}\\frac{1}{n^{3}}<\\infty.\n$$\n\nBy the first Borel–Cantelli lemma, $P(|S_{n}|>\\epsilon \\text{ i.o.})=0$ for every $\\epsilon>0$, which implies $S_{n}\\to 0$ almost surely.\n\nEquivalently, one may invoke the strong law of large numbers: $\\bar{X}_{n}=\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\to E[X_{1}]=0$ almost surely, and since $S_{n}=\\frac{1}{n}\\bar{X}_{n}$ with $\\frac{1}{n}\\to 0$, it follows that $S_{n}\\to 0$ almost surely.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Our final practice tackles a more advanced question: when does an infinite series of random variables converge? This exercise introduces the famous random harmonic series, where the signs of the terms of the harmonic series are chosen randomly. To determine its convergence, we must employ a more powerful tool, Kolmogorov's three-series theorem, which provides necessary and sufficient conditions for the almost sure convergence of a sum of independent random variables .",
            "id": "1352900",
            "problem": "Consider an infinite sequence of independent and identically distributed (i.i.d.) random variables, $\\xi_1, \\xi_2, \\xi_3, \\dots$. Each random variable $\\xi_k$ is a Rademacher random variable, meaning it takes the value $+1$ with probability $1/2$ and the value $-1$ with probability $1/2$.\n\nWe construct a random series $S$ defined as:\n$$S = \\sum_{k=1}^{\\infty} \\frac{\\xi_k}{k}$$\nThis series is often called the random harmonic series. Your task is to determine the convergence behavior of this series.\n\nWhich of the following statements about the series $S$ is true?\n\nA. The series converges for every possible sequence of outcomes for $(\\xi_k)_{k \\ge 1}$.\n\nB. The series converges almost surely.\n\nC. The series diverges to $+\\infty$ almost surely.\n\nD. The series diverges to $-\\infty$ almost surely.\n\nE. The series oscillates without a limit almost surely.\n\nF. Whether the series converges or diverges depends on the specific sequence of outcomes for $(\\xi_k)_{k \\ge 1}$, and the set of sequences for which it converges has a probability strictly between 0 and 1.",
            "solution": "Let $X_{k}=\\xi_{k}/k$. Then $(X_{k})_{k\\geq 1}$ are independent random variables. We compute the mean and variance of $X_{k}$:\n$$E[X_{k}]=\\frac{1}{k}E[\\xi_{k}]=0,$$\nsince $E[\\xi_{k}]=0$, and\n$$\\operatorname{Var}(X_{k})=E[X_{k}^{2}]-\\left(E[X_{k}]\\right)^{2}=E\\left[\\frac{\\xi_{k}^{2}}{k^{2}}\\right]=\\frac{1}{k^{2}},$$\nbecause $\\xi_{k}^{2}=1$ almost surely.\n\nWe apply the Kolmogorov three-series theorem. Define the truncation at level $1$. Since $|X_{k}|=\\frac{1}{k}\\leq 1$ for all $k$, we have:\n- $\\sum_{k=1}^{\\infty}P(|X_{k}|>1)=0,$\n- $\\sum_{k=1}^{\\infty}E[X_{k}\\mathbf{1}_{\\{|X_{k}|\\leq 1\\}}]=\\sum_{k=1}^{\\infty}E[X_{k}]=\\sum_{k=1}^{\\infty}0=0$ (hence convergent),\n- $\\sum_{k=1}^{\\infty}\\operatorname{Var}(X_{k}\\mathbf{1}_{\\{|X_{k}|\\leq 1\\}})=\\sum_{k=1}^{\\infty}\\operatorname{Var}(X_{k})=\\sum_{k=1}^{\\infty}\\frac{1}{k^{2}}<\\infty.$\n\nAll three series satisfy the hypotheses of the theorem. Therefore, the series $\\sum_{k=1}^{\\infty}X_{k}=\\sum_{k=1}^{\\infty}\\frac{\\xi_{k}}{k}$ converges almost surely.\n\nIt does not converge absolutely, since\n$$\\sum_{k=1}^{\\infty}\\left|\\frac{\\xi_{k}}{k}\\right|=\\sum_{k=1}^{\\infty}\\frac{1}{k}=\\infty,$$\nso the convergence is conditional. Consequently:\n- A is false because there exist sequences of signs (e.g., $\\xi_{k}\\equiv 1$) for which the series diverges.\n- C and D are false because the series converges almost surely to a finite limit.\n- E is false because it does not oscillate without a limit; it converges almost surely.\n- F is false because the event of convergence has probability $1$, not strictly between $0$ and $1$.\n\nThus, the correct statement is B.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}