{
    "hands_on_practices": [
        {
            "introduction": "Getting hands-on with an algorithm is the best way to understand it. This first practice problem guides you through a single, complete iteration of a Gibbs sampler, from one state $(x^{(0)}, y^{(0)})$ to the next $(x^{(1)}, y^{(1)})$. By manually generating a new state using specified conditional distributions and the inverse transform method, you will solidify your understanding of the core mechanism that drives this powerful MCMC technique. ",
            "id": "1920320",
            "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.",
            "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$.\nFor an Exponential rate $y$, the conditional CDF is\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$.\nFor a Poisson mean $x$, the pmf is\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\nSince $p(0 \\mid x)=0.7368<0.750$, continue to $k=1$:\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\nThen\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.3054 & 1\\end{pmatrix}}$$"
        },
        {
            "introduction": "While powerful, the Gibbs sampler is not without its pitfalls. This exercise explores one of the most common challenges: slow mixing when variables are highly correlated. By deterministically tracking the sampler's path in a bivariate normal distribution with high correlation $\\rho$, you will gain crucial intuition about the factors that affect the efficiency and convergence speed of MCMC simulations. ",
            "id": "1363745",
            "problem": "In a high-precision manufacturing process for optical components, two geometric parameters of a lens, $x_1$ and $x_2$, are critical for its performance. These parameters represent normalized deviations from the ideal design specifications. Due to the physics of the fabrication process, these parameters are not independent. An analysis of production data reveals that their joint probability distribution can be modeled by an unnormalized density function $f(x_1, x_2)$ given by:\n$$f(x_1, x_2) \\propto \\exp \\left( -\\frac{1}{2(1-\\rho^2)} (x_1^2 - 2\\rho x_1 x_2 + x_2^2) \\right)$$\nFor this specific process, the correlation coefficient is found to be $\\rho = 0.99$. The mode of the distribution is at $(0, 0)$, which corresponds to a perfect component.\n\nTo simulate the process variations, you are asked to use a Gibbs sampler. You start from an initial state $(x_1^{(0)}, x_2^{(0)}) = (-4.0, -4.1)$, which represents a component at the edge of the acceptable quality range.\n\nYour task is to determine the state of the sampler, $(x_1^{(2)}, x_2^{(2)})$, after two full iterations. A full iteration consists of updating $x_1$ first, and then updating $x_2$. To make the calculation deterministic, you must assume that at each sampling step, the new value drawn for a variable is equal to the mean of its conditional distribution.\n\nCalculate the coordinates of the state $(x_1^{(2)}, x_2^{(2)})$. Report both coordinates in your final answer, rounded to four significant figures.",
            "solution": "We recognize the given unnormalized joint density\n$$\nf(x_{1},x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(x_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}\\right)\\right)\n$$\nas the kernel of a bivariate normal distribution with mean vector $(0,0), unit variances, and correlation coefficient $\\rho$. To derive the full conditional distributions, complete the square in $x_{1}$:\n$$\nx_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}=(x_{1}-\\rho x_{2})^{2}+(1-\\rho^{2})x_{2}^{2}.\n$$\nHence, conditional on $x_{2}$,\n$$\nf(x_{1}\\mid x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}(x_{1}-\\rho x_{2})^{2}\\right),\n$$\nwhich is the kernel of a normal distribution with\n$$\nx_{1}\\mid x_{2} \\sim \\mathcal{N}\\!\\left(\\rho x_{2},\\,1-\\rho^{2}\\right).\n$$\nBy symmetry, we also have\n$$\nx_{2}\\mid x_{1} \\sim \\mathcal{N}\\!\\left(\\rho x_{1},\\,1-\\rho^{2}\\right).\n$$\n\nThe Gibbs sampler updates $x_{1}$ first using $x_{2}$, then updates $x_{2}$ using the new $x_{1}$. Under the deterministic rule that each draw equals the conditional mean, the updates are\n$$\nx_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)},\\qquad x_{2}^{(t+1)}=\\rho\\,x_{1}^{(t+1)}.\n$$\nCombining these,\n$$\nx_{2}^{(t+1)}=\\rho^{2}\\,x_{2}^{(t)},\\qquad x_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)}.\n$$\nStarting from $(x_{1}^{(0)},x_{2}^{(0)})=(-4.0,-4.1)$ and using $\\rho=0.99$, the first full iteration yields\n$$\nx_{1}^{(1)}=\\rho\\,x_{2}^{(0)}=0.99\\times(-4.1)=-4.059,\\qquad\nx_{2}^{(1)}=\\rho\\,x_{1}^{(1)}=0.99\\times(-4.059)=-4.01841.\n$$\nThe second full iteration then gives\n$$\nx_{1}^{(2)}=\\rho\\,x_{2}^{(1)}=0.99\\times(-4.01841)=-3.9782259,\\qquad\nx_{2}^{(2)}=\\rho\\,x_{1}^{(2)}=0.99\\times(-3.9782259)=-3.938443641.\n$$\nRounding each coordinate to four significant figures:\n$$\nx_{1}^{(2)}\\approx -3.978,\\qquad x_{2}^{(2)}\\approx -3.938.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-3.978 & -3.938\\end{pmatrix}}$$"
        },
        {
            "introduction": "For a Gibbs sampler to be reliable, it must be ergodic, meaning it can eventually reach any part of the target distribution from any starting point. This advanced exercise presents a carefully constructed thought experiment where this fundamental property breaks down, trapping the sampler within a subspace. Analyzing this failure case is essential for understanding the theoretical conditions that guarantee a Gibbs sampler will correctly explore the entire distribution. ",
            "id": "1920351",
            "problem": "A Gibbs sampler is designed to generate samples from a bivariate probability distribution $p(x, y)$. The process starts from an initial point $(x^{(0)}, y^{(0)})$ and iteratively generates a sequence of points $(x^{(t)}, y^{(t)})$ for $t=1, 2, \\dots$ by alternately drawing from the full conditional distributions: first $x^{(t)} \\sim p(x|y^{(t-1)})$ and then $y^{(t)} \\sim p(y|x^{(t)})$.\n\nThe full conditional distributions are defined in terms of a positive real constant $\\alpha > 0$. They are specified by their probability density functions (PDFs) as follows:\n\n1.  For any given $y > 0$, the conditional PDF of $x$ given $y$ is $p(x|y) = \\alpha \\exp(-\\alpha x)$ for $x > 0$, and $p(x|y) = 0$ for $x \\le 0$.\n2.  For any given $y < 0$, the conditional PDF of $x$ given $y$ is $p(x|y) = \\alpha \\exp(\\alpha x)$ for $x < 0$, and $p(x|y) = 0$ for $x \\ge 0$.\n3.  For any given $x > 0$, the conditional PDF of $y$ given $x$ is $p(y|x) = \\alpha \\exp(-\\alpha y)$ for $y > 0$, and $p(y|x) = 0$ for $y \\le 0$.\n4.  For any given $x < 0$, the conditional PDF of $y$ given $x$ is $p(y|x) = \\alpha \\exp(\\alpha y)$ for $y < 0$, and $p(y|x) = 0$ for $y \\ge 0$.\n5.  The conditional distributions are not defined for $x=0$ or $y=0$, as these values occur with zero probability.\n\nTwo independent Gibbs sampling chains, Chain A and Chain B, are run.\n- Chain A is initialized at $(x_A^{(0)}, y_A^{(0)}) = (1, 1)$. Let $E_A$ be the limiting expectation of the $x$-component of the samples, defined as $E_A = \\lim_{t \\to \\infty} \\mathbb{E}[x_A^{(t)}]$.\n- Chain B is initialized at $(x_B^{(0)}, y_B^{(0)}) = (-1, -1)$. Let $E_B$ be the limiting expectation of the $x$-component of the samples, defined as $E_B = \\lim_{t \\to \\infty} \\mathbb{E}[x_B^{(t)}]$.\n\nAssuming these limits exist, compute the value of the difference $E_A - E_B$ in terms of $\\alpha$.",
            "solution": "The specified Gibbs sampler alternates draws from conditionals that depend only on the sign of the conditioning variable. From the given definitions:\n- If $y>0$ then $x$ is drawn from $p(x|y)=\\alpha \\exp(-\\alpha x)$ on $\\{x>0\\}$; if $y<0$ then $x$ is drawn from $p(x|y)=\\alpha \\exp(\\alpha x)$ on $\\{x<0\\}$.\n- If $x>0$ then $y$ is drawn from $p(y|x)=\\alpha \\exp(-\\alpha y)$ on $\\{y>0\\}$; if $x<0$ then $y$ is drawn from $p(y|x)=\\alpha \\exp(\\alpha y)$ on $\\{y<0\\}$.\n\nTherefore, the sign of the coordinates is preserved almost surely once the process starts:\n- If $y^{(t-1)}>0$ then $x^{(t)}>0$ almost surely; then $x^{(t)}>0$ implies $y^{(t)}>0$ almost surely. By induction, starting from $(1,1)$, $(x_{A}^{(t)},y_{A}^{(t)})$ remains in $\\{x>0,y>0\\}$ for all $t\\geq 1$.\n- Similarly, starting from $(-1,-1)$, $(x_{B}^{(t)},y_{B}^{(t)})$ remains in $\\{x<0,y<0\\}$ for all $t\\geq 1$.\n\nFixing the sign class implies the $x$-conditional does not depend on the value of the conditioning variable, only on its sign. Hence, for all $t\\geq 1$:\n- Chain A: $x_{A}^{(t)}$ has PDF $f_{+}(x)=\\alpha \\exp(-\\alpha x)$ on $\\{x>0\\}$.\n- Chain B: $x_{B}^{(t)}$ has PDF $f_{-}(x)=\\alpha \\exp(\\alpha x)$ on $\\{x<0\\}$.\n\nThus the expectations are constant for all $t\\geq 1$ and equal to their stationary values. Compute these expectations explicitly:\n$$\n\\mathbb{E}[x_{A}^{(t)}]\n=\\int_{0}^{\\infty} x\\,\\alpha \\exp(-\\alpha x)\\,dx.\n$$\nIntegrating by parts with $u=x$, $dv=\\alpha \\exp(-\\alpha x)\\,dx$ gives $v=-\\exp(-\\alpha x)$ and\n$$\n\\int_{0}^{\\infty} x\\,\\alpha \\exp(-\\alpha x)\\,dx\n=\\left[-x\\,\\exp(-\\alpha x)\\right]_{0}^{\\infty}\n+\\int_{0}^{\\infty} \\exp(-\\alpha x)\\,dx\n=\\frac{1}{\\alpha}.\n$$\nTherefore $E_{A}=\\lim_{t\\to\\infty}\\mathbb{E}[x_{A}^{(t)}]=\\frac{1}{\\alpha}$.\n\nSimilarly,\n$$\n\\mathbb{E}[x_{B}^{(t)}]\n=\\int_{-\\infty}^{0} x\\,\\alpha \\exp(\\alpha x)\\,dx.\n$$\nWith the substitution $t=-x$ (so $dx=-dt$), this becomes\n$$\n\\int_{-\\infty}^{0} x\\,\\alpha \\exp(\\alpha x)\\,dx\n=-\\int_{0}^{\\infty} t\\,\\alpha \\exp(-\\alpha t)\\,dt\n=-\\frac{1}{\\alpha},\n$$\nhence $E_{B}=\\lim_{t\\to\\infty}\\mathbb{E}[x_{B}^{(t)}]=-\\,\\frac{1}{\\alpha}$.\n\nTherefore,\n$$\nE_{A}-E_{B}=\\frac{1}{\\alpha}-\\left(-\\frac{1}{\\alpha}\\right)=\\frac{2}{\\alpha}.\n$$",
            "answer": "$$\\boxed{\\frac{2}{\\alpha}}$$"
        }
    ]
}