{
    "hands_on_practices": [
        {
            "introduction": "The geometric interpretation of conditional expectation as an orthogonal projection is a cornerstone of modern probability theory. To begin exploring this concept, we must first verify that it behaves like the projections we know from linear algebra. This first practice problem  solidifies our understanding by demonstrating the fundamental property of linearity, which is crucial for all subsequent applications.",
            "id": "1350188",
            "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, and let $L^2(\\Omega, \\mathcal{F}, P)$ be the Hilbert space of square-integrable random variables on this space, equipped with the inner product $\\langle X, Y \\rangle = E[XY]$. Let $\\mathcal{G}$ be a sub-$\\sigma$-algebra of $\\mathcal{F}$. It is a known result that the conditional expectation operator, $E[\\cdot | \\mathcal{G}]$, acts as an orthogonal projection from $L^2(\\Omega, \\mathcal{F}, P)$ onto its closed subspace $L^2(\\Omega, \\mathcal{G}, P)$.\n\nLet's denote the orthogonal projection of a random variable $Z \\in L^2(\\Omega, \\mathcal{F}, P)$ onto the subspace $L^2(\\Omega, \\mathcal{G}, P)$ by $P_{\\mathcal{G}}(Z)$. From the above, we have the identity $P_{\\mathcal{G}}(Z) = E[Z | \\mathcal{G}]$.\n\nConsider two arbitrary random variables $X, Y \\in L^2(\\Omega, \\mathcal{F}, P)$ and an arbitrary non-zero real constant $c$. Which of the following expressions correctly represents the projection of the linear combination $X + cY$ onto the subspace $L^2(\\Omega, \\mathcal{G}, P)$?\n\nA. $P_{\\mathcal{G}}(X) + P_{\\mathcal{G}}(Y)$\n\nB. $P_{\\mathcal{G}}(X) + c P_{\\mathcal{G}}(Y)$\n\nC. $P_{\\mathcal{G}}(X) + c^2 P_{\\mathcal{G}}(Y)$\n\nD. $|c| P_{\\mathcal{G}}(X) + \\sqrt{1-c^2} P_{\\mathcal{G}}(Y)$\n\nE. $c P_{\\mathcal{G}}(X) + P_{\\mathcal{G}}(Y)$\n\nF. The expression depends on the correlation between $X$ and $Y$ and cannot be determined in general.",
            "solution": "We use the characterization of the orthogonal projection onto the closed subspace $L^{2}(\\Omega, \\mathcal{G}, P)$: for any $Z \\in L^{2}(\\Omega, \\mathcal{F}, P)$, its projection $U = P_{\\mathcal{G}}(Z) \\in L^{2}(\\Omega, \\mathcal{G}, P)$ is the unique element such that\n$$\nE\\big[(Z - U)H\\big] = 0 \\quad \\text{for all } H \\in L^{2}(\\Omega, \\mathcal{G}, P).\n$$\nEquivalently, since $P_{\\mathcal{G}}(Z) = E[Z \\mid \\mathcal{G}]$, the same orthogonality holds with $U = E[Z \\mid \\mathcal{G}]$.\n\nLet $U_{X} = P_{\\mathcal{G}}(X) = E[X \\mid \\mathcal{G}]$ and $U_{Y} = P_{\\mathcal{G}}(Y) = E[Y \\mid \\mathcal{G}]$. For any $H \\in L^{2}(\\Omega, \\mathcal{G}, P)$, by the defining orthogonality we have\n$$\nE\\big[(X - U_{X})H\\big] = 0, \\qquad E\\big[(Y - U_{Y})H\\big] = 0.\n$$\nFix an arbitrary real constant $c \\neq 0$ (the argument also holds for $c = 0$). Consider $Z = X + cY$ and the candidate\n$$\nU := U_{X} + c\\,U_{Y} \\in L^{2}(\\Omega, \\mathcal{G}, P).\n$$\nThen, for any $H \\in L^{2}(\\Omega, \\mathcal{G}, P)$,\n$$\nE\\big[(Z - U)H\\big] = E\\big[(X + cY - U_{X} - cU_{Y})H\\big]\n= E\\big[(X - U_{X})H\\big] + c\\,E\\big[(Y - U_{Y})H\\big] = 0 + c \\cdot 0 = 0.\n$$\nBy the uniqueness of the orthogonal projection, this implies $U = P_{\\mathcal{G}}(Z)$, hence\n$$\nP_{\\mathcal{G}}(X + cY) = P_{\\mathcal{G}}(X) + c\\,P_{\\mathcal{G}}(Y).\n$$\nEquivalently, by linearity of conditional expectation,\n$$\nE[X + cY \\mid \\mathcal{G}] = E[X \\mid \\mathcal{G}] + c\\,E[Y \\mid \\mathcal{G}],\n$$\nwhich matches option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "With the linearity of the projection operator established, we can now apply it to a practical optimization problem. In fields like signal processing and econometrics, a common task is to find the best possible linear estimate of a quantity of interest based on available data. This exercise  frames this task as projecting a \"signal\" random variable onto the subspace spanned by \"measurement\" random variables, allowing you to calculate the minimum mean squared error achievable.",
            "id": "1350189",
            "problem": "In signal processing, a common task is to estimate a signal of interest, $S$, from other accessible measurements. We can model the signal $S$ and two reference signals, $R_1$ and $R_2$, as random variables. For this problem, assume all signals have been processed to have a mean of zero. These random variables are elements of the space of square-integrable random variables, $L^2$, with the inner product given by $\\langle A, B \\rangle = E[AB]$.\n\nThe goal is to find the best linear estimate of $S$ using $R_1$ and $R_2$. The best linear estimate is a random variable $\\hat{S}$ of the form $\\hat{S} = a_1 R_1 + a_2 R_2$, where the real coefficients $a_1$ and $a_2$ are chosen to minimize the mean squared error, $E[(S - \\hat{S})^2]$. This optimal estimate $\\hat{S}$ corresponds to the orthogonal projection of $S$ onto the linear subspace spanned by $\\{R_1, R_2\\}$.\n\nSuppose the following statistical properties of the signals are known:\n- $E[S^2] = 16$\n- $E[R_1^2] = 4$\n- $E[R_2^2] = 9$\n- $E[SR_1] = 3$\n- $E[SR_2] = -1$\n- $E[R_1 R_2] = 2$\n\nCalculate the minimum possible value of the mean squared error, $E[(S - \\hat{S})^2]$.",
            "solution": "We work in the Hilbert space $L^{2}$ with inner product $\\langle A,B\\rangle=E[AB]$. The best linear estimate $\\hat{S}=a_{1}R_{1}+a_{2}R_{2}$ is the orthogonal projection of $S$ onto $\\operatorname{span}\\{R_{1},R_{2}\\}$, characterized by the orthogonality conditions $E[(S-\\hat{S})R_{i}]=0$ for $i=1,2$.\n\nDefine the coefficient vector $a=\\begin{pmatrix}a_{1}\\\\ a_{2}\\end{pmatrix}$, the covariance matrix\n$$\nR=\\begin{pmatrix}\nE[R_{1}^{2}] & E[R_{1}R_{2}]\\\\\nE[R_{1}R_{2}] & E[R_{2}^{2}]\n\\end{pmatrix}\n=\\begin{pmatrix}\n4 & 2\\\\\n2 & 9\n\\end{pmatrix},\n$$\nand the cross-covariance vector\n$$\nc=\\begin{pmatrix}\nE[SR_{1}]\\\\\nE[SR_{2}]\n\\end{pmatrix}\n=\\begin{pmatrix}\n3\\\\\n-1\n\\end{pmatrix}.\n$$\nThe mean squared error as a function of $a$ is\n$$\nE[(S-\\hat{S})^{2}]=E[S^{2}]-2a^{T}c+a^{T}Ra.\n$$\nMinimizing with respect to $a$ gives the normal equations $Ra=c$. Since $\\det(R)=4\\cdot 9-2^{2}=32>0$, $R$ is invertible and the unique minimizer is $a=R^{-1}c$. Substituting back into the error expression yields the minimum mean squared error\n$$\nE[(S-\\hat{S})^{2}]_{\\min}=E[S^{2}]-c^{T}R^{-1}c.\n$$\n\nCompute $R^{-1}$. For $R=\\begin{pmatrix}4&2\\\\2&9\\end{pmatrix}$,\n$$\nR^{-1}=\\frac{1}{32}\\begin{pmatrix}9 & -2\\\\ -2 & 4\\end{pmatrix}.\n$$\nThen\n$$\nR^{-1}c=\\frac{1}{32}\\begin{pmatrix}9 & -2\\\\ -2 & 4\\end{pmatrix}\\begin{pmatrix}3\\\\ -1\\end{pmatrix}\n=\\frac{1}{32}\\begin{pmatrix}27+2\\\\ -6-4\\end{pmatrix}\n=\\begin{pmatrix}\\frac{29}{32}\\\\ -\\frac{5}{16}\\end{pmatrix},\n$$\nand\n$$\nc^{T}R^{-1}c=\\begin{pmatrix}3 & -1\\end{pmatrix}\\begin{pmatrix}\\frac{29}{32}\\\\ -\\frac{5}{16}\\end{pmatrix}\n=\\frac{87}{32}+\\frac{5}{16}\n=\\frac{97}{32}.\n$$\nGiven $E[S^{2}]=16$, the minimum mean squared error is\n$$\nE[(S-\\hat{S})^{2}]_{\\min}=16-\\frac{97}{32}\n=\\frac{512}{32}-\\frac{97}{32}\n=\\frac{415}{32}.\n$$",
            "answer": "$$\\boxed{\\frac{415}{32}}$$"
        },
        {
            "introduction": "Real-world approximation problems often come with additional constraints beyond simply belonging to a particular subspace. This final practice problem  presents a more advanced challenge: finding the best approximation that is not only measurable with respect to a given sub-$\\sigma$-algebra but also satisfies a specific condition on its expected value. Solving this requires combining the projection principle with constrained optimization techniques, showcasing the versatility of the Hilbert space framework.",
            "id": "1350191",
            "problem": "Consider a probability space $(\\Omega, \\mathcal{F}, P)$ where $\\Omega = [0,1] \\times [0,1]$ is the unit square in the plane, $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $\\Omega$, and $P$ is the uniform probability measure (i.e., the Lebesgue measure on the square). A point in $\\Omega$ is denoted by $(\\omega_1, \\omega_2)$.\n\nA random variable $X$ is defined on this space by the function $X(\\omega_1, \\omega_2) = 6\\omega_1^2 + 12\\omega_2$.\n\nWe seek a random variable $Y$ that serves as the \"best approximation\" to $X$ under a set of constraints. The quality of approximation is measured by the mean squared error, $E[(X-Y)^2]$. Find the random variable $Y$ that minimizes this quantity subject to both of the following conditions:\n\n1.  The value of $Y$ at any point $(\\omega_1, \\omega_2)$ must depend only on the value of the first coordinate, $\\omega_1$. That is, $Y$ must be of the form $Y(\\omega_1, \\omega_2) = g(\\omega_1)$ for some function $g$.\n2.  The expected value of $Y$ must be exactly 5, i.e., $E[Y] = 5$.\n\nDetermine the expression for this optimal random variable $Y$ as a function of $\\omega_1$ and $\\omega_2$.",
            "solution": "We minimize the mean squared error subject to the constraints by projecting in the Hilbert space $L^{2}(\\Omega,\\mathcal{F},P)$. The class of admissible random variables is $\\{Y:Y(\\omega_{1},\\omega_{2})=g(\\omega_{1}),\\ \\mathbb{E}[Y]=5\\}$.\n\nSet up the Lagrangian functional for $g$ and multiplier $\\lambda$:\n$$\n\\mathcal{L}(g,\\lambda)=\\mathbb{E}\\big[(X-g(\\omega_{1}))^{2}\\big]+2\\lambda\\big(\\mathbb{E}[g(\\omega_{1})]-5\\big).\n$$\nFor any admissible perturbation $h(\\omega_{1})$, the first-order optimality condition is\n$$\n\\frac{d}{d\\epsilon}\\Big|_{\\epsilon=0}\\mathcal{L}(g+\\epsilon h,\\lambda)\n=-2\\mathbb{E}\\big[(X-g(\\omega_{1}))h(\\omega_{1})\\big]+2\\lambda\\,\\mathbb{E}[h(\\omega_{1})]=0,\n$$\nwhich is equivalent to\n$$\n\\mathbb{E}\\big[(g(\\omega_{1})-X+\\lambda)h(\\omega_{1})\\big]=0\\quad\\text{for all }h(\\omega_{1}).\n$$\nSince this holds for every $h$ measurable with respect to $\\omega_{1}$, it implies\n$$\n\\mathbb{E}\\big[g(\\omega_{1})-X+\\lambda\\mid \\omega_{1}\\big]=0\n\\quad\\Longrightarrow\\quad\ng(\\omega_{1})=\\mathbb{E}[X\\mid \\omega_{1}]-\\lambda.\n$$\nCompute the conditional expectation using independence and uniformity of $\\omega_{1},\\omega_{2}$:\n$$\n\\mathbb{E}[X\\mid \\omega_{1}]\n=\\mathbb{E}[6\\omega_{1}^{2}+12\\omega_{2}\\mid \\omega_{1}]\n=6\\omega_{1}^{2}+12\\,\\mathbb{E}[\\omega_{2}]\n=6\\omega_{1}^{2}+6,\n$$\nsince $\\mathbb{E}[\\omega_{2}]=\\frac{1}{2}$. Thus\n$$\ng(\\omega_{1})=6\\omega_{1}^{2}+6-\\lambda.\n$$\nImpose the mean constraint $\\mathbb{E}[g(\\omega_{1})]=5$:\n$$\n5=\\mathbb{E}[g(\\omega_{1})]=\\mathbb{E}[6\\omega_{1}^{2}+6]-\\lambda\n=6\\,\\mathbb{E}[\\omega_{1}^{2}]+6-\\lambda.\n$$\nWith $\\mathbb{E}[\\omega_{1}^{2}]=\\int_{0}^{1}x^{2}\\,dx=\\frac{1}{3}$, we get\n$$\n5=6\\cdot\\frac{1}{3}+6-\\lambda=2+6-\\lambda=8-\\lambda\\quad\\Longrightarrow\\quad \\lambda=3.\n$$\nTherefore the optimal random variable is\n$$\nY(\\omega_{1},\\omega_{2})=g(\\omega_{1})=6\\omega_{1}^{2}+6-3=6\\omega_{1}^{2}+3,\n$$\nwhich depends only on $\\omega_{1}$ and satisfies $\\mathbb{E}[Y]=6\\,\\mathbb{E}[\\omega_{1}^{2}]+3=2+3=5$. Since the objective is strictly convex and the feasible set is convex, this solution is the unique minimizer.",
            "answer": "$$\\boxed{6\\omega_{1}^{2}+3}$$"
        }
    ]
}