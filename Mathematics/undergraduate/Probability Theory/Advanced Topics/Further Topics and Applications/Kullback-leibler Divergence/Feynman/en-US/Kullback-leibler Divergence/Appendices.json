{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of Kullback-Leibler (KL) divergence, our first practice involves a direct calculation with simple, discrete probability distributions. We will compare a familiar fair six-sided die to a hypothetical loaded die to quantify how different their outcome distributions are. This exercise  provides a foundational experience in applying the KL divergence formula and interpreting its meaning as a measure of information gain or \"surprise\".",
            "id": "1370292",
            "problem": "In information theory, the Kullback-Leibler (KL) divergence is a measure of how one probability distribution, $P$, is different from a second, reference probability distribution, $Q$. For discrete probability distributions $P$ and $Q$ defined over the same set of outcomes $\\mathcal{X}$, the KL divergence from $Q$ to $P$, denoted $D_{KL}(P \\| Q)$, is given by the formula:\n$$\nD_{KL}(P \\| Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nConsider two six-sided dice. The first die is a standard fair die, and the probability distribution of its outcomes is denoted by $P$. The second die is a loaded die, and the probability distribution of its outcomes is denoted by $Q$. For this loaded die, the probability of rolling any specific even number is exactly twice the probability of rolling any specific odd number. The set of possible outcomes for both dice is $\\mathcal{X} = \\{1, 2, 3, 4, 5, 6\\}$.\n\nCalculate the value of the KL divergence $D_{KL}(P \\| Q)$. Provide your answer as a single closed-form analytic expression.",
            "solution": "We denote the fair die distribution by $P$ and the loaded die distribution by $Q$ over the outcome set $\\mathcal{X}=\\{1,2,3,4,5,6\\}$. By definition, $P(x)=\\frac{1}{6}$ for all $x\\in\\mathcal{X}$. For $Q$, let $p_{o}$ be the probability of any specific odd outcome and $p_{e}$ the probability of any specific even outcome. The statement says $p_{e}=2p_{o}$. Since there are three odd and three even outcomes, normalization gives $3p_{o}+3p_{e}=1$, hence $3p_{o}+3(2p_{o})=9p_{o}=1$, which implies $p_{o}=\\frac{1}{9}$ and $p_{e}=\\frac{2}{9}$.\n\nThe Kullback-Leibler divergence from $Q$ to $P$ is\n$$\nD_{KL}(P\\|Q)=\\sum_{x\\in\\mathcal{X}}P(x)\\ln\\left(\\frac{P(x)}{Q(x)}\\right).\n$$\nWe separate the sum into odd and even outcomes. For each odd $x$, $P(x)=\\frac{1}{6}$ and $Q(x)=\\frac{1}{9}$, so each odd term equals\n$$\n\\frac{1}{6}\\ln\\left(\\frac{\\frac{1}{6}}{\\frac{1}{9}}\\right)=\\frac{1}{6}\\ln\\left(\\frac{9}{6}\\right)=\\frac{1}{6}\\ln\\left(\\frac{3}{2}\\right).\n$$\nThere are three odd outcomes, contributing a total of\n$$\n3\\cdot\\frac{1}{6}\\ln\\left(\\frac{3}{2}\\right)=\\frac{1}{2}\\ln\\left(\\frac{3}{2}\\right).\n$$\nFor each even $x$, $P(x)=\\frac{1}{6}$ and $Q(x)=\\frac{2}{9}$, so each even term equals\n$$\n\\frac{1}{6}\\ln\\left(\\frac{\\frac{1}{6}}{\\frac{2}{9}}\\right)=\\frac{1}{6}\\ln\\left(\\frac{9}{12}\\right)=\\frac{1}{6}\\ln\\left(\\frac{3}{4}\\right).\n$$\nThere are three even outcomes, contributing a total of\n$$\n3\\cdot\\frac{1}{6}\\ln\\left(\\frac{3}{4}\\right)=\\frac{1}{2}\\ln\\left(\\frac{3}{4}\\right).\n$$\nSumming both contributions yields\n$$\nD_{KL}(P\\|Q)=\\frac{1}{2}\\ln\\left(\\frac{3}{2}\\right)+\\frac{1}{2}\\ln\\left(\\frac{3}{4}\\right)=\\frac{1}{2}\\ln\\left(\\frac{3}{2}\\cdot\\frac{3}{4}\\right)=\\frac{1}{2}\\ln\\left(\\frac{9}{8}\\right).\n$$\nThis is a closed-form analytic expression.",
            "answer": "$$\\boxed{\\frac{1}{2}\\ln\\left(\\frac{9}{8}\\right)}$$"
        },
        {
            "introduction": "A crucial characteristic of KL divergence is its asymmetry; the divergence from distribution $Q$ to $P$ is generally not equal to the divergence from $P$ to $Q$. This means it is not a true distance metric, a common point of confusion for beginners. This next practice  uses two different models for a biased coin to provide a clear, numerical demonstration of this fundamental property, reinforcing the importance of order when calculating $D_{KL}(P || Q)$.",
            "id": "1370270",
            "problem": "Two statisticians, Alice and Bob, are tasked with creating a probabilistic model for the outcome of a single toss of a potentially biased coin. The outcome can be either heads (H) or tails (T). Alice proposes a model, which we'll call distribution $P$, where the probability of heads is $p_A = 0.2$. Bob proposes a different model, distribution $Q$, where the probability of heads is $p_B = 0.7$.\n\nTo quantify the difference between these two models, they decide to use the Kullback-Leibler (KL) divergence, also known as relative entropy. For two discrete probability distributions $P$ and $Q$ defined on the same probability space $\\mathcal{X}$, the KL divergence from $Q$ to $P$ is given by the formula:\n$$\nD_{KL}(P \\| Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)\n$$\nThis value measures the information lost when $Q$ is used to approximate $P$.\n\nYour task is to calculate two values:\n1. The KL divergence from Bob's model ($Q$) to Alice's model ($P$), which is $D_{KL}(P \\| Q)$.\n2. The KL divergence from Alice's model ($P$) to Bob's model ($Q$), which is $D_{KL}(Q \\| P)$.\n\nCalculate both $D_{KL}(P \\| Q)$ and $D_{KL}(Q \\| P)$, in that order, and provide the results as a pair of numerical values. Round your final answers to four significant figures.",
            "solution": "We model a single coin toss with two outcomes $\\mathcal{X}=\\{H,T\\}$. Alice’s model $P$ has $P(H)=p_{A}=0.2$ and $P(T)=1-p_{A}=0.8$. Bob’s model $Q$ has $Q(H)=p_{B}=0.7$ and $Q(T)=1-p_{B}=0.3$.\n\nBy definition, for discrete distributions on the same space,\n$$\nD_{KL}(P \\| Q)=\\sum_{x\\in\\mathcal{X}} P(x)\\,\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right).\n$$\n\nFirst, compute $D_{KL}(P \\| Q)$:\n$$\nD_{KL}(P \\| Q)=P(H)\\ln\\!\\left(\\frac{P(H)}{Q(H)}\\right)+P(T)\\ln\\!\\left(\\frac{P(T)}{Q(T)}\\right).\n$$\nSubstitute $P(H)=\\frac{1}{5}$, $Q(H)=\\frac{7}{10}$, $P(T)=\\frac{4}{5}$, $Q(T)=\\frac{3}{10}$:\n$$\nD_{KL}(P \\| Q)=\\frac{1}{5}\\ln\\!\\left(\\frac{\\frac{1}{5}}{\\frac{7}{10}}\\right)+\\frac{4}{5}\\ln\\!\\left(\\frac{\\frac{4}{5}}{\\frac{3}{10}}\\right)\n=\\frac{1}{5}\\ln\\!\\left(\\frac{2}{7}\\right)+\\frac{4}{5}\\ln\\!\\left(\\frac{8}{3}\\right).\n$$\nNumerically,\n$$\n\\frac{1}{5}\\ln\\!\\left(\\frac{2}{7}\\right)\\approx 0.2\\times(-1.2527629685)\\approx -0.2505525937,\\quad\n\\frac{4}{5}\\ln\\!\\left(\\frac{8}{3}\\right)\\approx 0.8\\times 0.9808292530\\approx 0.7846634024,\n$$\nso\n$$\nD_{KL}(P \\| Q)\\approx -0.2505525937+0.7846634024\\approx 0.5341108087.\n$$\n\nNext, compute $D_{KL}(Q \\| P)$:\n$$\nD_{KL}(Q \\| P)=Q(H)\\ln\\!\\left(\\frac{Q(H)}{P(H)}\\right)+Q(T)\\ln\\!\\left(\\frac{Q(T)}{P(T)}\\right).\n$$\nSubstitute $Q(H)=\\frac{7}{10}$, $P(H)=\\frac{1}{5}$, $Q(T)=\\frac{3}{10}$, $P(T)=\\frac{4}{5}$:\n$$\nD_{KL}(Q \\| P)=\\frac{7}{10}\\ln\\!\\left(\\frac{\\frac{7}{10}}{\\frac{1}{5}}\\right)+\\frac{3}{10}\\ln\\!\\left(\\frac{\\frac{3}{10}}{\\frac{4}{5}}\\right)\n=\\frac{7}{10}\\ln\\!\\left(\\frac{7}{2}\\right)+\\frac{3}{10}\\ln\\!\\left(\\frac{3}{8}\\right).\n$$\nNumerically,\n$$\n\\frac{7}{10}\\ln\\!\\left(\\frac{7}{2}\\right)\\approx 0.7\\times 1.2527629685\\approx 0.8769340779,\\quad\n\\frac{3}{10}\\ln\\!\\left(\\frac{3}{8}\\right)\\approx 0.3\\times(-0.9808292530)\\approx -0.2942487759,\n$$\nso\n$$\nD_{KL}(Q \\| P)\\approx 0.8769340779-0.2942487759\\approx 0.5826853020.\n$$\n\nRounding both results to four significant figures gives $D_{KL}(P \\| Q)\\approx 0.5341$ and $D_{KL}(Q \\| P)\\approx 0.5827$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.5341 & 0.5827\\end{pmatrix}}$$"
        },
        {
            "introduction": "While the asymmetry of KL divergence is a defining feature, there are many applications where a symmetric measure of difference between distributions is needed. The Jensen-Shannon (JS) divergence provides an elegant solution by creating a true metric based on KL divergence. In this final practice , you will construct and calculate the JS divergence, seeing firsthand how foundational concepts can be adapted to create new, powerful statistical tools.",
            "id": "1370279",
            "problem": "In information theory and statistics, a key task is to measure the \"distance\" or divergence between two probability distributions. A common, though asymmetric, measure is the Kullback-Leibler (KL) divergence. For two discrete probability distributions $P$ and $Q$ defined over the same set of outcomes $\\mathcal{X}$, the KL divergence of $Q$ from $P$ is given by:\n$$D_{\\text{KL}}(P \\| Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right)$$\nTo create a true, symmetric metric, one can use the Jensen-Shannon (JS) divergence, which is defined in terms of the KL divergence. The JS divergence is given by:\n$$JSD(P \\| Q) = \\frac{1}{2} D_{\\text{KL}}(P \\| M) + \\frac{1}{2} D_{\\text{KL}}(Q \\| M)$$\nwhere $M$ is the mixture distribution $M = \\frac{1}{2}(P+Q)$.\n\nConsider two coins. Coin A is a fair coin, while Coin B is a severely biased coin. The probability distributions for the outcome of a single flip (either Heads, H, or Tails, T) for each coin are as follows:\n-   Distribution $P$ for Coin A: $P(\\text{H}) = 0.5$, $P(\\text{T}) = 0.5$.\n-   Distribution $Q$ for Coin B: $Q(\\text{H}) = 0.9$, $Q(\\text{T}) = 0.1$.\n\nCalculate the Jensen-Shannon divergence, $JSD(P \\| Q)$, between these two distributions. Use the natural logarithm (base $e$) for your calculations. Provide your answer as a numerical value, rounded to four significant figures.",
            "solution": "The problem asks for the Jensen-Shannon divergence (JSD) between two probability distributions, $P$ and $Q$. The distributions are defined over the outcomes {H, T} for a coin flip.\n\nThe distributions are given as:\n$P = \\{P(\\text{H})=0.5, P(\\text{T})=0.5\\}$\n$Q = \\{Q(\\text{H})=0.9, Q(\\text{T})=0.1\\}$\n\nThe formula for the Jensen-Shannon divergence is:\n$$JSD(P \\| Q) = \\frac{1}{2} D_{\\text{KL}}(P \\| M) + \\frac{1}{2} D_{\\text{KL}}(Q \\| M)$$\nwhere $M$ is the mixture distribution $M = \\frac{1}{2}(P+Q)$.\n\nFirst, we must find the mixture distribution $M$. Its probabilities for each outcome are the average of the probabilities from $P$ and $Q$.\n$M(\\text{H}) = \\frac{1}{2}(P(\\text{H}) + Q(\\text{H})) = \\frac{1}{2}(0.5 + 0.9) = \\frac{1.4}{2} = 0.7$\n$M(\\text{T}) = \\frac{1}{2}(P(\\text{T}) + Q(\\text{T})) = \\frac{1}{2}(0.5 + 0.1) = \\frac{0.6}{2} = 0.3$\nSo, the mixture distribution is $M = \\{M(\\text{H})=0.7, M(\\text{T})=0.3\\}$.\n\nNext, we calculate the two Kullback-Leibler (KL) divergence terms. The formula for KL divergence is:\n$$D_{\\text{KL}}(A \\| B) = \\sum_{x \\in \\{\\text{H, T}\\}} A(x) \\ln\\left(\\frac{A(x)}{B(x)}\\right)$$\nLet's compute $D_{\\text{KL}}(P \\| M)$:\n$$D_{\\text{KL}}(P \\| M) = P(\\text{H}) \\ln\\left(\\frac{P(\\text{H})}{M(\\text{H})}\\right) + P(\\text{T}) \\ln\\left(\\frac{P(\\text{T})}{M(\\text{T})}\\right)$$\n$$D_{\\text{KL}}(P \\| M) = 0.5 \\ln\\left(\\frac{0.5}{0.7}\\right) + 0.5 \\ln\\left(\\frac{0.5}{0.3}\\right)$$\n$$D_{\\text{KL}}(P \\| M) = 0.5 \\ln\\left(\\frac{5}{7}\\right) + 0.5 \\ln\\left(\\frac{5}{3}\\right)$$\nNumerically, this is:\n$D_{\\text{KL}}(P \\| M) \\approx 0.5 \\times (-0.3364722) + 0.5 \\times (0.5108256) \\approx -0.1682361 + 0.2554128 = 0.0871767$\n\nNow, let's compute $D_{\\text{KL}}(Q \\| M)$:\n$$D_{\\text{KL}}(Q \\| M) = Q(\\text{H}) \\ln\\left(\\frac{Q(\\text{H})}{M(\\text{H})}\\right) + Q(\\text{T}) \\ln\\left(\\frac{Q(\\text{T})}{M(\\text{T})}\\right)$$\n$$D_{\\text{KL}}(Q \\| M) = 0.9 \\ln\\left(\\frac{0.9}{0.7}\\right) + 0.1 \\ln\\left(\\frac{0.1}{0.3}\\right)$$\n$$D_{\\text{KL}}(Q \\| M) = 0.9 \\ln\\left(\\frac{9}{7}\\right) + 0.1 \\ln\\left(\\frac{1}{3}\\right)$$\nNumerically, this is:\n$D_{\\text{KL}}(Q \\| M) \\approx 0.9 \\times (0.2513144) + 0.1 \\times (-1.0986123) \\approx 0.2261830 - 0.1098612 = 0.1163218$\n\nFinally, we use these values to compute the JS divergence:\n$$JSD(P \\| Q) = \\frac{1}{2} (D_{\\text{KL}}(P \\| M) + D_{\\text{KL}}(Q \\| M))$$\n$$JSD(P \\| Q) \\approx \\frac{1}{2} (0.0871767 + 0.1163218)$$\n$$JSD(P \\| Q) \\approx \\frac{1}{2} (0.2034985)$$\n$$JSD(P \\| Q) \\approx 0.10174925$$\n\nThe problem asks for the answer rounded to four significant figures.\nThe first four significant figures are 1, 0, 1, 7. The fifth digit is 4, so we round down.\nThe final result is 0.1017.",
            "answer": "$$\\boxed{0.1017}$$"
        }
    ]
}