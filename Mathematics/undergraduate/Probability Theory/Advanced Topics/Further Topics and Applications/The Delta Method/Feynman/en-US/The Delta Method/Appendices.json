{
    "hands_on_practices": [
        {
            "introduction": "This first exercise serves as a foundational application of the Delta Method. We will explore how to find the approximate sampling distribution for a quantity derived from a sample meanâ€”specifically, the volume of a cube whose side length is estimated from data. This practice illustrates the core mechanism of propagating uncertainty from an estimator to a function of that estimator. ",
            "id": "1959853",
            "problem": "In a materials science laboratory, a novel process is used to synthesize cubic nano-crystals. The side length of a randomly chosen crystal is a random variable $X$ with a true mean $\\mu > 0$ and a finite, non-zero variance $\\sigma^2$. To estimate the properties of the crystals, a quality control engineer measures the side lengths of a random sample of $n$ crystals, denoted by $X_1, X_2, \\ldots, X_n$. The sample mean of the side lengths is calculated as $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nThe engineer is interested in the statistical behavior of the volume of a hypothetical nano-crystal whose side length is equal to the sample mean, which we define as $V_n = \\bar{X}_n^3$. According to large-sample theory, for a sufficiently large sample size $n$, the sampling distribution of $V_n$ can be approximated by a normal distribution.\n\nDetermine the variance of this approximate normal distribution for $V_n$. Express your answer as a symbolic expression in terms of $\\mu$, $\\sigma^2$, and $n$.",
            "solution": "Let $X_{1},\\ldots,X_{n}$ be independent and identically distributed with $\\mathbb{E}[X_{i}]=\\mu>0$ and $\\operatorname{Var}(X_{i})=\\sigma^{2}\\in(0,\\infty)$. The sample mean is $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ with $\\mathbb{E}[\\bar{X}_{n}]=\\mu$ and $\\operatorname{Var}(\\bar{X}_{n})=\\frac{\\sigma^{2}}{n}$.\n\nBy the central limit theorem, for large $n$,\n$$\n\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)\\xrightarrow{d}\\mathcal{N}(0,\\sigma^{2}).\n$$\nDefine $g(x)=x^{3}$ so that $V_{n}=g(\\bar{X}_{n})$. By the delta method,\n$$\n\\sqrt{n}\\left(g(\\bar{X}_{n})-g(\\mu)\\right)\\xrightarrow{d}\\mathcal{N}\\left(0,\\left(g'(\\mu)\\right)^{2}\\sigma^{2}\\right),\n$$\nwhich implies the large-sample approximation\n$$\n\\operatorname{Var}(V_{n})\\approx\\frac{\\left(g'(\\mu)\\right)^{2}\\sigma^{2}}{n}.\n$$\nCompute $g'(x)=3x^{2}$, hence $g'(\\mu)=3\\mu^{2}$ and $\\left(g'(\\mu)\\right)^{2}=9\\mu^{4}$. Therefore,\n$$\n\\operatorname{Var}(V_{n})\\approx\\frac{9\\mu^{4}\\sigma^{2}}{n}.\n$$",
            "answer": "$$\\boxed{\\frac{9\\mu^{4}\\sigma^{2}}{n}}$$"
        },
        {
            "introduction": "The standard, first-order Delta Method is powerful, but it has limitations. This practice investigates a special case where the derivative of our function at the point of interest is zero, rendering the first-order approximation uninformative. You will learn how a higher-order approximation reveals the correct, non-degenerate limiting distribution, a crucial skill for handling more complex statistical models. ",
            "id": "1959855",
            "problem": "In a study of a stochastic binary process, a sequence of $n$ independent trials is conducted. Each trial results in either a \"success\" (coded as 1) or a \"failure\" (coded as 0). The true probability of a success in any given trial is known to be exactly $p = 1/2$. Let $\\hat{p}_n$ be the sample proportion of successes observed in the $n$ trials.\n\nA statistic of interest is the sample variance of this Bernoulli process, which is given by $\\hat{S}_n^2 = \\hat{p}_n(1-\\hat{p}_n)$. The corresponding true variance is $\\sigma^2 = p(1-p) = 1/4$. We wish to understand the asymptotic behavior of this sample variance estimator.\n\nConsider the centered and scaled statistic $T_n = n(\\hat{S}_n^2 - 1/4)$. Which of the following options correctly describes the limiting distribution of $T_n$ as $n \\to \\infty$?\nLet $Z$ denote a standard normal random variable, and let $\\chi^2_1$ denote a chi-squared random variable with one degree of freedom.\n\nA. A Normal distribution with mean 0 and variance $1/16$.\n\nB. The distribution of the random variable $-\\frac{1}{2}\\chi^2_1$.\n\nC. The distribution of the random variable $\\frac{1}{4}\\chi^2_1$.\n\nD. The distribution of the random variable $-\\frac{1}{4}\\chi^2_1$.\n\nE. The distribution of the random variable $-\\chi^2_1$.",
            "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. Bernoulli with $p=\\frac{1}{2}$, and let $\\hat{p}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. The sample variance estimator is $\\hat{S}_{n}^{2}=\\hat{p}_{n}(1-\\hat{p}_{n})$. The true variance is $\\sigma^{2}=p(1-p)=\\frac{1}{4}$.\n\nAlgebraically,\n$$\n\\hat{S}_{n}^{2}-\\frac{1}{4}\n=\\hat{p}_{n}(1-\\hat{p}_{n})-\\frac{1}{4}\n=\\hat{p}_{n}-\\hat{p}_{n}^{2}-\\frac{1}{4}\n=-(\\hat{p}_{n}-\\tfrac{1}{2})^{2},\n$$\nsince $(\\hat{p}_{n}-\\tfrac{1}{2})^{2}=\\hat{p}_{n}^{2}-\\hat{p}_{n}+\\tfrac{1}{4}$. Therefore,\n$$\nT_{n}=n\\bigl(\\hat{S}_{n}^{2}-\\tfrac{1}{4}\\bigr)\n=-\\,n\\bigl(\\hat{p}_{n}-\\tfrac{1}{2}\\bigr)^{2}\n=-\\bigl(\\sqrt{n}(\\hat{p}_{n}-\\tfrac{1}{2})\\bigr)^{2}.\n$$\n\nBy the Central Limit Theorem,\n$$\n\\sqrt{n}\\,(\\hat{p}_{n}-\\tfrac{1}{2})\\;\\xrightarrow{d}\\;Y,\\quad Y\\sim N\\bigl(0,\\tfrac{1}{4}\\bigr).\n$$\nBy the Continuous Mapping Theorem with the continuous function $g(y)=y^{2}$, we have\n$$\n\\bigl(\\sqrt{n}(\\hat{p}_{n}-\\tfrac{1}{2})\\bigr)^{2}\\;\\xrightarrow{d}\\;Y^{2}.\n$$\nIf $Z\\sim N(0,1)$ then $Y\\stackrel{d}{=}\\tfrac{1}{2}Z$, hence $Y^{2}\\stackrel{d}{=}\\tfrac{1}{4}Z^{2}=\\tfrac{1}{4}\\chi^{2}_{1}$. Consequently,\n$$\nT_{n}\\;\\xrightarrow{d}\\;-\\,Y^{2}\\;\\stackrel{d}{=}\\;-\\frac{1}{4}\\chi^{2}_{1}.\n$$\nComparing with the options, this is option D.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Real-world statistics are often functions of several estimated parameters, not just one. This practice introduces the Multivariate Delta Method, a powerful generalization that allows us to find the asymptotic distribution for statistics like the sample coefficient of variation. Mastering this technique is essential for analyzing complex estimators that depend on multiple sample moments simultaneously. ",
            "id": "1396699",
            "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed random variables from an exponential distribution with an unknown rate parameter $\\lambda > 0$. The probability density function is given by $f(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$.\n\nLet $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean and $S_n^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X}_n)^2$ be the sample variance. The sample coefficient of variation is defined as the ratio $C_n = S_n / \\bar{X}_n$.\n\nBy the Central Limit Theorem and the Delta Method, it can be shown that for some constant $c$, the distribution of $\\sqrt{n}(C_n - c)$ converges to a normal distribution with a mean of 0 and some variance $V$.\n\nYour task is to determine this asymptotic variance, $V$. To do so, first express the sample coefficient of variation as a function of the first two raw sample moments, $\\bar{X}_n$ and $\\overline{X^2}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i^2$. Then, apply the multivariate Delta Method to find the asymptotic variance.\n\nProvide the final answer as an exact numerical value.",
            "solution": "Let $m_{1}=\\bar{X}_{n}$ and $m_{2}=\\overline{X^{2}}_{n}$. The unbiased sample variance can be written as\n$$\nS_{n}^{2}=\\frac{n}{n-1}\\left(m_{2}-m_{1}^{2}\\right),\n$$\nso the sample coefficient of variation is\n$$\nC_{n}=\\frac{S_{n}}{m_{1}}=\\sqrt{\\frac{n}{n-1}}\\;\\sqrt{\\frac{m_{2}-m_{1}^{2}}{m_{1}^{2}}}=\\sqrt{\\frac{n}{n-1}}\\;h(m_{1},m_{2}),\n$$\nwhere\n$$\nh(m_{1},m_{2})=\\sqrt{\\frac{m_{2}}{m_{1}^{2}}-1}.\n$$\nSince $\\sqrt{n/(n-1)}\\to 1$, this prefactor does not affect the asymptotic variance. Thus we apply the multivariate Delta Method to $h(m_{1},m_{2})$.\n\nFor $X\\sim \\text{Exp}(\\lambda)$, the raw moments are $E[X]=\\mu_{1}=1/\\lambda$, $E[X^{2}]=\\mu_{2}=2/\\lambda^{2}$, $E[X^{3}]=6/\\lambda^{3}$, and $E[X^{4}]=24/\\lambda^{4}$. Hence, for the vector $(m_{1},m_{2})$,\n$$\n\\sqrt{n}\\Big((m_{1},m_{2})-(\\mu_{1},\\mu_{2})\\Big)\\;\\xrightarrow{d}\\;\\mathcal{N}\\big(0,\\Sigma\\big),\n$$\nwith\n$$\n\\Sigma=\\begin{pmatrix}\n\\operatorname{Var}(X) & \\operatorname{Cov}(X,X^{2})\\\\\n\\operatorname{Cov}(X,X^{2}) & \\operatorname{Var}(X^{2})\n\\end{pmatrix}\n=\\begin{pmatrix}\n\\frac{1}{\\lambda^{2}} & \\frac{4}{\\lambda^{3}}\\\\\n\\frac{4}{\\lambda^{3}} & \\frac{20}{\\lambda^{4}}\n\\end{pmatrix}.\n$$\n\nCompute the gradient of $h$:\nLet $u=m_{2}m_{1}^{-2}-1$, so $h=u^{1/2}$. Then\n$$\n\\frac{\\partial h}{\\partial m_{1}}=\\frac{1}{2}u^{-1/2}\\left(-\\frac{2m_{2}}{m_{1}^{3}}\\right)=-\\frac{m_{2}}{m_{1}^{3}\\sqrt{u}},\\qquad\n\\frac{\\partial h}{\\partial m_{2}}=\\frac{1}{2}u^{-1/2}\\left(\\frac{1}{m_{1}^{2}}\\right)=\\frac{1}{2m_{1}^{2}\\sqrt{u}}.\n$$\nAt $(\\mu_{1},\\mu_{2})=(\\frac{1}{\\lambda},\\frac{2}{\\lambda^{2}})$ we have\n$$\nu=\\frac{\\mu_{2}}{\\mu_{1}^{2}}-1=\\frac{2/\\lambda^{2}}{(1/\\lambda)^{2}}-1=2-1=1,\n$$\nso\n$$\n\\nabla h(\\mu_{1},\\mu_{2})=\\begin{pmatrix}-\\frac{\\mu_{2}}{\\mu_{1}^{3}}\\\\ \\frac{1}{2\\mu_{1}^{2}}\\end{pmatrix}\n=\\begin{pmatrix}-2\\lambda\\\\ \\frac{\\lambda^{2}}{2}\\end{pmatrix}.\n$$\nBy the multivariate Delta Method,\n$$\n\\sqrt{n}\\big(h(m_{1},m_{2})-h(\\mu_{1},\\mu_{2})\\big)\\;\\xrightarrow{d}\\;\\mathcal{N}\\big(0,V\\big),\\quad\nV=\\nabla h(\\mu)^{\\top}\\Sigma\\,\\nabla h(\\mu).\n$$\nCompute $V$:\n$$\nV=\\begin{pmatrix}-2\\lambda & \\frac{\\lambda^{2}}{2}\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{\\lambda^{2}} & \\frac{4}{\\lambda^{3}}\\\\\n\\frac{4}{\\lambda^{3}} & \\frac{20}{\\lambda^{4}}\n\\end{pmatrix}\n\\begin{pmatrix}-2\\lambda\\\\ \\frac{\\lambda^{2}}{2}\\end{pmatrix}.\n$$\nExpanding,\n$$\nV=(4\\lambda^{2})\\frac{1}{\\lambda^{2}}+2(-2\\lambda)\\left(\\frac{\\lambda^{2}}{2}\\right)\\frac{4}{\\lambda^{3}}+\\left(\\frac{\\lambda^{2}}{2}\\right)^{2}\\frac{20}{\\lambda^{4}}\n=4-8+5=1.\n$$\nMoreover, $h(\\mu_{1},\\mu_{2})=\\sqrt{\\frac{2/\\lambda^{2}}{(1/\\lambda)^{2}}-1}=1$, so $c=1$. The prefactor $\\sqrt{n/(n-1)}\\to 1$ does not change the asymptotic variance. Therefore, the asymptotic variance is\n$$\nV=1.\n$$",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}