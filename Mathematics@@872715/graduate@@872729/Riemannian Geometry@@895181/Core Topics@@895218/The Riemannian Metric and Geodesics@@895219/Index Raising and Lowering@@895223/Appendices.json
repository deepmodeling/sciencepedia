{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with a direct computational exercise in a familiar setting, $\\mathbb{R}^2$, but equipped with a non-Euclidean metric. This practice strips away geometric complexity to focus purely on the algebraic mechanics of the musical isomorphisms. By working through the steps of inverting the metric tensor and manually raising and lowering indices, you will gain concrete experience with how a metric dictates the duality between vectors and covectors at the level of their components [@problem_id:2980489].", "problem": "Let $\\mathbb{R}^{2}$ be equipped with the Riemannian metric $g$ whose matrix of components in the standard basis $\\{e_{1},e_{2}\\}$ is\n$$\nG=\\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix},\n$$\nso that for vectors $v,w\\in\\mathbb{R}^{2}$ with column coordinate vectors (in the standard basis) $[v]$ and $[w]$, the inner product is $g(v,w)=[v]^{\\mathsf{T}}\\,G\\,[w]$. The musical isomorphisms are the linear isomorphisms $g^{\\flat}:\\mathbb{R}^{2}\\to(\\mathbb{R}^{2})^{\\ast}$ and $g^{\\sharp}:(\\mathbb{R}^{2})^{\\ast}\\to\\mathbb{R}^{2}$ defined by $g^{\\flat}(v)(\\cdot)=g(v,\\cdot)$ and $g^{\\sharp}(\\alpha)$ the unique vector satisfying $g(g^{\\sharp}(\\alpha),\\cdot)=\\alpha(\\cdot)$. Work from these definitions.\n\nTasks:\n1. Construct an explicit basis $\\mathcal{B}=\\{b_{1},b_{2}\\}$ of $\\mathbb{R}^{2}$ that is not $g$-orthonormal, and justify that it is not $g$-orthonormal using only the definition of $g$.\n2. Compute the inverse matrix $G^{-1}$ of the metric in the standard basis from first principles.\n3. Let $v\\in\\mathbb{R}^{2}$ be the vector with contravariant components $v^{1}=1$ and $v^{2}=-2$ in the standard basis, and let $\\alpha\\in(\\mathbb{R}^{2})^{\\ast}$ be the covector with covariant components $\\alpha_{1}=3$ and $\\alpha_{2}=1$ in the dual of the standard basis. Using only the definitions of the musical isomorphisms, compute the lowered components $v_{i}$ of $g^{\\flat}(v)$ and the raised components $\\alpha^{i}$ of $g^{\\sharp}(\\alpha)$, both with respect to the standard bases.\n\nReport your final results concatenated in the order: the four entries of $G^{-1}$ listed row-wise, then the two lowered components $v_{i}$ (listed as $v_{1}$, then $v_{2}$), then the two raised components $\\alpha^{i}$ (listed as $\\alpha^{1}$, then $\\alpha^{2}$). Express the final answer as a single row matrix. No rounding is required; give exact values.", "solution": "The problem is well-posed and internally consistent, grounded in the standard principles of Riemannian geometry. All necessary definitions and data are provided. We proceed with the solution.\n\nThe Riemannian metric $g$ on $\\mathbb{R}^{2}$ is given by the matrix of its components in the standard basis $\\{e_{1}, e_{2}\\}$, which is $G = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$. The inner product of two vectors $v, w \\in \\mathbb{R}^{2}$ with coordinate vectors $[v]$ and $[w]$ is $g(v,w)=[v]^{\\mathsf{T}}G[w]$.\n\nFirst, we construct a basis $\\mathcal{B}=\\{b_{1}, b_{2}\\}$ that is not $g$-orthonormal. We can test the standard basis $\\mathcal{B}=\\{e_{1}, e_{2}\\}$ itself. A basis is $g$-orthonormal if $g(b_{i}, b_{j})=\\delta_{ij}$, the Kronecker delta. We compute the inner products of the standard basis vectors using the given definition of $g$. The coordinate vectors for $e_{1}$ and $e_{2}$ are $[e_{1}] = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $[e_{2}] = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe inner product of $e_{1}$ with itself is:\n$$g(e_{1}, e_{1}) = [e_{1}]^{\\mathsf{T}}G[e_{1}] = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2(1) + 1(0) = 2.$$\nThe inner product of $e_{1}$ with $e_{2}$ is:\n$$g(e_{1}, e_{2}) = [e_{1}]^{\\mathsf{T}}G[e_{2}] = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 2(0) + 1(1) = 1.$$\nSince $g(e_{1},e_{1})=2 \\neq 1$, the vector $e_{1}$ is not of unit length. Since $g(e_{1}, e_{2})=1 \\neq 0$, the vectors $e_{1}$ and $e_{2}$ are not orthogonal with respect to the metric $g$. Therefore, the standard basis $\\{e_{1}, e_{2}\\}$ is not $g$-orthonormal. This serves as the required example.\n\nSecond, we compute the inverse matrix $G^{-1}$ from first principles. This requires finding a matrix, which we denote $G^{-1} = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, such that $G G^{-1} = I$, where $I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$ is the identity matrix. The matrix equation is:\n$$ \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}. $$\nThis single matrix equation decomposes into two independent systems of linear equations. For the first column of $G^{-1}$:\n$$ \\begin{cases} 2a + c = 1 \\\\ a + 2c = 0 \\end{cases} $$\nFrom the second equation, $a = -2c$. Substituting this into the first equation gives $2(-2c) + c = 1$, which simplifies to $-3c = 1$, so $c = -\\frac{1}{3}$. Then $a = -2(-\\frac{1}{3}) = \\frac{2}{3}$.\nFor the second column of $G^{-1}$:\n$$ \\begin{cases} 2b + d = 0 \\\\ b + 2d = 1 \\end{cases} $$\nFrom the first equation, $d = -2b$. Substituting this into the second equation gives $b + 2(-2b) = 1$, which simplifies to $-3b = 1$, so $b = -\\frac{1}{3}$. Then $d = -2(-\\frac{1}{3}) = \\frac{2}{3}$.\nThus, the inverse matrix is $G^{-1} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$.\n\nThird, we compute the lowered and raised components.\nLet $v \\in \\mathbb{R}^{2}$ be the vector with contravariant components $v^{1}=1$ and $v^{2}=-2$ in the standard basis. So, $v = 1e_{1} - 2e_{2}$, and its coordinate vector is $[v] = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$. The lowered covector is $\\omega = g^{\\flat}(v)$, and its components $v_{i}$ in the dual basis $\\{e^{1}, e^{2}\\}$ are given by $v_{i} = \\omega(e_{i})$. Using the definition of the flat map, $g^{\\flat}(v)(\\cdot) = g(v, \\cdot)$, we have $v_{i} = g(v, e_{i})$.\nFor $i=1$:\n$$v_{1} = g(v, e_{1}) = [v]^{\\mathsf{T}}G[e_{1}] = \\begin{pmatrix} 1  -2 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1  -2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 1(2) + (-2)(1) = 0.$$\nFor $i=2$:\n$$v_{2} = g(v, e_{2}) = [v]^{\\mathsf{T}}G[e_{2}] = \\begin{pmatrix} 1  -2 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1  -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1(1) + (-2)(2) = -3.$$\nThe lowered components are $v_{1} = 0$ and $v_{2} = -3$.\n\nLet $\\alpha \\in (\\mathbb{R}^{2})^{\\ast}$ be the covector with covariant components $\\alpha_{1}=3$ and $\\alpha_{2}=1$. So, $\\alpha = 3e^{1} + 1e^{2}$. The raised vector is $u = g^{\\sharp}(\\alpha)$, and we want to find its components $\\alpha^{i}$ in the standard basis, so that $u = \\alpha^{1}e_{1} + \\alpha^{2}e_{2}$. The coordinate vector is $[u] = \\begin{pmatrix} \\alpha^{1} \\\\ \\alpha^{2} \\end{pmatrix}$.\nThe defining property of the sharp map is $g(u, w) = \\alpha(w)$ for any vector $w \\in \\mathbb{R}^{2}$. We test this property with the basis vectors $w=e_{1}$ and $w=e_{2}$.\nFor $w=e_{1}$:\n$$g(u, e_{1}) = [u]^{\\mathsf{T}}G[e_{1}] = \\begin{pmatrix} \\alpha^{1}  \\alpha^{2} \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2\\alpha^{1} + \\alpha^{2}.$$\nAlso, $\\alpha(e_{1}) = (3e^{1} + 1e^{2})(e_{1}) = 3e^{1}(e_{1}) + 1e^{2}(e_{1}) = 3(1) + 1(0) = 3$.\nThis gives the equation $2\\alpha^{1} + \\alpha^{2} = 3$.\nFor $w=e_{2}$:\n$$g(u, e_{2}) = [u]^{\\mathsf{T}}G[e_{2}] = \\begin{pmatrix} \\alpha^{1}  \\alpha^{2} \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\alpha^{1} + 2\\alpha^{2}.$$\nAlso, $\\alpha(e_{2}) = (3e^{1} + 1e^{2})(e_{2}) = 3e^{1}(e_{2}) + 1e^{2}(e_{2}) = 3(0) + 1(1) = 1$.\nThis gives the equation $\\alpha^{1} + 2\\alpha^{2} = 1$.\nWe now solve the system:\n$$ \\begin{cases} 2\\alpha^{1} + \\alpha^{2} = 3 \\\\ \\alpha^{1} + 2\\alpha^{2} = 1 \\end{cases} $$\nFrom the first equation, $\\alpha^{2} = 3 - 2\\alpha^{1}$. Substituting into the second: $\\alpha^{1} + 2(3-2\\alpha^{1})=1$, which gives $\\alpha^{1} + 6 - 4\\alpha^{1} = 1$, or $-3\\alpha^{1} = -5$. Thus, $\\alpha^{1} = \\frac{5}{3}$.\nThen, $\\alpha^{2} = 3 - 2(\\frac{5}{3}) = 3 - \\frac{10}{3} = \\frac{9-10}{3} = -\\frac{1}{3}$.\nThe raised components are $\\alpha^{1} = \\frac{5}{3}$ and $\\alpha^{2} = -\\frac{1}{3}$.\n\nThe final results, in the specified order, are the entries of $G^{-1}$ (row-wise), the components $v_{i}$, and the components $\\alpha^{i}$.\nEntries of $G^{-1}$: $\\frac{2}{3}, -\\frac{1}{3}, -\\frac{1}{3}, \\frac{2}{3}$.\nComponents $v_{i}$: $0, -3$.\nComponents $\\alpha^{i}$: $\\frac{5}{3}, -\\frac{1}{3}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3}  -\\frac{1}{3}  \\frac{2}{3}  0  -3  \\frac{5}{3}  -\\frac{1}{3} \\end{pmatrix}}\n$$", "id": "2980489"}, {"introduction": "Having mastered the mechanics in a flat space, we now turn to a classic example of a curved manifold: the unit $2$-sphere, $S^2$. This exercise requires you to first derive the components of the round metric from its definition as an induced metric from the ambient Euclidean space $\\mathbb{R}^3$. This practice is essential for understanding how geometry is encoded in the metric tensor and provides a tangible example of how index manipulation works on a genuinely curved surface [@problem_id:2980504].", "problem": "Let $S^{2}$ be the unit $2$-sphere embedded in $\\mathbb{R}^{3}$ with the standard Euclidean inner product. Consider the spherical coordinate chart $(\\theta,\\varphi)$ with $\\theta \\in (0,\\pi)$ and $\\varphi \\in (0,2\\pi)$ given by the smooth embedding $F:(0,\\pi)\\times(0,2\\pi)\\to \\mathbb{R}^{3}$ defined by $F(\\theta,\\varphi)=\\left(\\sin\\theta\\cos\\varphi,\\sin\\theta\\sin\\varphi,\\cos\\theta\\right)$. The round Riemannian metric $g$ on $S^{2}$ is the pullback of the Euclidean metric via $F$, so its coordinate components $g_{ij}$ are given by the fundamental definition $g_{ij}=\\langle \\partial_{i}F,\\partial_{j}F\\rangle$, where $\\langle \\cdot,\\cdot\\rangle$ denotes the Euclidean inner product and $\\partial_{i}$ denotes partial differentiation with respect to the coordinate $x^{i}$, with $x^{1}=\\theta$ and $x^{2}=\\varphi$.\n\nUsing only this definition of the induced metric, compute the inverse metric $g^{-1}=(g^{ij})$ in these coordinates by inverting the $2\\times 2$ matrix $(g_{ij})$, and then verify from first principles the index-raising identity $g^{ik}g_{kj}=\\delta^{i}{}_{j}$, where $\\delta^{i}{}_{j}$ is the Kronecker delta and the Einstein summation convention is adopted. Your final answer should be the explicit $2\\times 2$ matrix expression for $g^{-1}$ in terms of $\\theta$ and $\\varphi$. No numerical approximations are required.", "solution": "The problem is valid. It is a standard, well-posed exercise in elementary Riemannian geometry and is scientifically sound.\n\nOur objective is to compute the components of the induced metric tensor $g = (g_{ij})$ on the unit $2$-sphere $S^2$ in spherical coordinates, find its inverse $g^{-1} = (g^{ij})$, and verify the identity $g^{ik}g_{kj} = \\delta^{i}{}_{j}$.\n\nThe embedding of the sphere in $\\mathbb{R}^3$ is given by the map $F(\\theta,\\varphi) = (\\sin\\theta\\cos\\varphi, \\sin\\theta\\sin\\varphi, \\cos\\theta)$, where we associate the coordinates $x^1 = \\theta$ and $x^2 = \\varphi$.\n\nFirst, we compute the tangent vectors to the coordinate curves, $\\partial_{\\theta}F$ and $\\partial_{\\varphi}F$. These form a basis for the tangent space at each point on the sphere.\n$$\n\\partial_{\\theta}F = \\frac{\\partial F}{\\partial \\theta} = (\\cos\\theta\\cos\\varphi, \\cos\\theta\\sin\\varphi, -\\sin\\theta)\n$$\n$$\n\\partial_{\\varphi}F = \\frac{\\partial F}{\\partial \\varphi} = (-\\sin\\theta\\sin\\varphi, \\sin\\theta\\cos\\varphi, 0)\n$$\n\nThe components of the metric tensor, $g_{ij}$, are defined as the Euclidean inner products of these basis vectors: $g_{ij} = \\langle \\partial_{i}F, \\partial_{j}F \\rangle$. We compute each component:\n\n$1$. $g_{\\theta\\theta} = g_{11} = \\langle \\partial_{\\theta}F, \\partial_{\\theta}F \\rangle$\n$$\ng_{\\theta\\theta} = (\\cos\\theta\\cos\\varphi)^{2} + (\\cos\\theta\\sin\\varphi)^{2} + (-\\sin\\theta)^{2}\n$$\n$$\ng_{\\theta\\theta} = \\cos^{2}\\theta\\cos^{2}\\varphi + \\cos^{2}\\theta\\sin^{2}\\varphi + \\sin^{2}\\theta\n$$\n$$\ng_{\\theta\\theta} = \\cos^{2}\\theta(\\cos^{2}\\varphi + \\sin^{2}\\varphi) + \\sin^{2}\\theta\n$$\n$$\ng_{\\theta\\theta} = \\cos^{2}\\theta(1) + \\sin^{2}\\theta = 1\n$$\n\n$2$. $g_{\\theta\\varphi} = g_{12} = \\langle \\partial_{\\theta}F, \\partial_{\\varphi}F \\rangle$\n$$\ng_{\\theta\\varphi} = (\\cos\\theta\\cos\\varphi)(-\\sin\\theta\\sin\\varphi) + (\\cos\\theta\\sin\\varphi)(\\sin\\theta\\cos\\varphi) + (-\\sin\\theta)(0)\n$$\n$$\ng_{\\theta\\varphi} = -\\sin\\theta\\cos\\theta\\cos\\varphi\\sin\\varphi + \\sin\\theta\\cos\\theta\\sin\\varphi\\cos\\varphi = 0\n$$\nSince the metric tensor is symmetric, we have $g_{\\varphi\\theta} = g_{21} = g_{\\theta\\varphi} = 0$.\n\n$3$. $g_{\\varphi\\varphi} = g_{22} = \\langle \\partial_{\\varphi}F, \\partial_{\\varphi}F \\rangle$\n$$\ng_{\\varphi\\varphi} = (-\\sin\\theta\\sin\\varphi)^{2} + (\\sin\\theta\\cos\\varphi)^{2} + (0)^{2}\n$$\n$$\ng_{\\varphi\\varphi} = \\sin^{2}\\theta\\sin^{2}\\varphi + \\sin^{2}\\theta\\cos^{2}\\varphi\n$$\n$$\ng_{\\varphi\\varphi} = \\sin^{2}\\theta(\\sin^{2}\\varphi + \\cos^{2}\\varphi)\n$$\n$$\ng_{\\varphi\\varphi} = \\sin^{2}\\theta(1) = \\sin^{2}\\theta\n$$\n\nAssembling these components, we obtain the matrix representation of the metric tensor:\n$$\n(g_{ij}) = \\begin{pmatrix} g_{\\theta\\theta}  g_{\\theta\\varphi} \\\\ g_{\\varphi\\theta}  g_{\\varphi\\varphi} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\sin^{2}(\\theta) \\end{pmatrix}\n$$\n\nNext, we compute the inverse metric tensor $(g^{ij})$, which is the matrix inverse of $(g_{ij})$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is $A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nThe determinant of $(g_{ij})$ is $\\det(g_{ij}) = (1)(\\sin^{2}(\\theta)) - (0)(0) = \\sin^{2}(\\theta)$. Since the coordinate chart is defined for $\\theta \\in (0, \\pi)$, we have $\\sin\\theta  0$, and thus $\\det(g_{ij}) \\neq 0$.\nThe inverse matrix is:\n$$\n(g^{ij}) = \\frac{1}{\\sin^{2}(\\theta)} \\begin{pmatrix} \\sin^{2}(\\theta)  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{\\sin^{2}(\\theta)} \\end{pmatrix}\n$$\nThus, the components of the inverse metric are $g^{\\theta\\theta}=1$, $g^{\\theta\\varphi}=g^{\\varphi\\theta}=0$, and $g^{\\varphi\\varphi}=\\frac{1}{\\sin^{2}(\\theta)}$.\n\nFinally, we verify the identity $g^{ik}g_{kj} = \\delta^{i}{}_{j}$ from first principles. This corresponds to the matrix multiplication $(g^{ik})(g_{kj})$, which must yield the identity matrix $I$.\n$$\n(g^{ik})(g_{kj}) = \\begin{pmatrix} g^{\\theta\\theta}  g^{\\theta\\varphi} \\\\ g^{\\varphi\\theta}  g^{\\varphi\\varphi} \\end{pmatrix} \\begin{pmatrix} g_{\\theta\\theta}  g_{\\theta\\varphi} \\\\ g_{\\varphi\\theta}  g_{\\varphi\\varphi} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{\\sin^{2}(\\theta)} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\sin^{2}(\\theta) \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\n\\begin{pmatrix} (1)(1) + (0)(0)  (1)(0) + (0)(\\sin^{2}(\\theta)) \\\\ (0)(1) + (\\frac{1}{\\sin^{2}(\\theta)})(0)  (0)(0) + (\\frac{1}{\\sin^{2}(\\theta)})(\\sin^{2}(\\theta)) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThis resulting matrix is the identity matrix, which corresponds to the Kronecker delta $\\delta^{i}{}_{j}$ in index notation. Let's explicitly check the four component equations using the Einstein summation convention over the index $k \\in \\{\\theta, \\varphi\\}$:\nFor $(i,j) = (\\theta,\\theta)$: $g^{\\theta k}g_{k\\theta} = g^{\\theta\\theta}g_{\\theta\\theta} + g^{\\theta\\varphi}g_{\\varphi\\theta} = (1)(1) + (0)(0) = 1 = \\delta^{\\theta}{}_{\\theta}$.\nFor $(i,j) = (\\theta,\\varphi)$: $g^{\\theta k}g_{k\\varphi} = g^{\\theta\\theta}g_{\\theta\\varphi} + g^{\\theta\\varphi}g_{\\varphi\\varphi} = (1)(0) + (0)(\\sin^{2}(\\theta)) = 0 = \\delta^{\\theta}{}_{\\varphi}$.\nFor $(i,j) = (\\varphi,\\theta)$: $g^{\\varphi k}g_{k\\theta} = g^{\\varphi\\theta}g_{\\theta\\theta} + g^{\\varphi\\varphi}g_{\\varphi\\theta} = (0)(1) + (\\frac{1}{\\sin^{2}(\\theta)})(0) = 0 = \\delta^{\\varphi}{}_{\\theta}$.\nFor $(i,j) = (\\varphi,\\varphi)$: $g^{\\varphi k}g_{k\\varphi} = g^{\\varphi\\theta}g_{\\theta\\varphi} + g^{\\varphi\\varphi}g_{\\varphi\\varphi} = (0)(0) + (\\frac{1}{\\sin^{2}(\\theta)})(\\sin^{2}(\\theta)) = 1 = \\delta^{\\varphi}{}_{\\varphi}$.\nThe verification is complete. The problem asks for the explicit matrix expression for $g^{-1}$, which is $(g^{ij})$.\n$$\ng^{-1} = (g^{ij}) = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{\\sin^{2}(\\theta)} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{\\sin^{2}(\\theta)} \\end{pmatrix}}\n$$", "id": "2980504"}, {"introduction": "This final practice demonstrates the power of musical isomorphisms as a fundamental tool for defining other key objects in differential geometry. Working within an orthonormal frame—a concept crucial in both geometry and physics—you will see how the gradient and the Laplacian are constructed using index raising and lowering. This problem bridges the gap between abstract definitions and their application in constructing physical and geometric laws, showing that index manipulation is not just a notational convenience but a cornerstone of the entire theoretical edifice [@problem_id:2980514].", "problem": "Let $(M,g)$ be a $3$-dimensional Riemannian manifold and let $\\{e_{1},e_{2},e_{3}\\}$ be a local orthonormal frame near a point $p \\in M$ with dual coframe $\\{\\theta^{1},\\theta^{2},\\theta^{3}\\}$. Denote by $[\\cdot,\\cdot]$ the Lie bracket of vector fields and define the structure functions $c_{ij}{}^{k}$ by $[e_{i},e_{j}]=\\sum_{k}c_{ij}{}^{k}e_{k}$. At the point $p$, suppose the only nonzero $c_{ij}{}^{k}$ are given by\n$$\nc_{12}{}^{1}=1,\\quad c_{12}{}^{2}=-0.7,\\quad c_{12}{}^{3}=2,\\quad c_{23}{}^{1}=3,\\quad c_{23}{}^{2}=-1,\\quad c_{23}{}^{3}=1.3,\\quad c_{31}{}^{1}=-2.2,\\quad c_{31}{}^{2}=-4,\\quad c_{31}{}^{3}=0.5,\n$$\nand all other values follow by skew-symmetry $c_{ji}{}^{k}=-c_{ij}{}^{k}$.\n\nLet $f$ be a smooth function and $X$ a smooth vector field. At the point $p$, assume the first and second directional derivatives of $f$ along the orthonormal frame are\n$$\ne_{1}f=1,\\quad e_{2}f=-2,\\quad e_{3}f=4,\\qquad e_{1}e_{1}f=5,\\quad e_{2}e_{2}f=-3,\\quad e_{3}e_{3}f=2,\n$$\nand the components and componentwise directional derivatives of $X$ are\n$$\nX^{1}=2,\\quad X^{2}=-1,\\quad X^{3}=3,\\qquad e_{1}(X^{1})=0.4,\\quad e_{2}(X^{2})=-0.3,\\quad e_{3}(X^{3})=0.2,\n$$\nall evaluated at $p$. Here $X=\\sum_{i}X^{i}e_{i}$ and $e_{i}(X^{i})$ denotes the directional derivative of the scalar function $X^{i}$ along $e_{i}$.\n\nStarting only from the fundamental definitions in Riemannian geometry — the gradient defined by the metric-dual relation $df(Y)=\\langle \\mathrm{grad}\\,f,Y\\rangle$, the divergence defined as the trace of the covariant derivative $\\mathrm{div}\\,X=\\mathrm{tr}(Y\\mapsto \\nabla_{Y}X)$ with respect to the Levi-Civita connection (torsion-free and metric compatible), and the Laplacian defined by $\\Delta f=\\mathrm{div}(\\mathrm{grad}\\,f)$ — and using the Koszul formula to express connection coefficients in an orthonormal frame in terms of the structure functions $c_{ij}{}^{k}$, carry out the following at the point $p$:\n\n1. Derive the components of $\\mathrm{grad}\\,f$ with respect to $\\{e_{i}\\}$ and explicitly compute $\\mathrm{grad}\\,f(p)$.\n2. Derive an explicit expression for $\\mathrm{div}\\,X$ in terms of $e_{i}(X^{i})$ and the structure functions, and compute $\\mathrm{div}\\,X(p)$.\n3. Derive an explicit expression for $\\Delta f$ in terms of $e_{i}e_{i}f$ and the connection coefficients, and compute $\\Delta f(p)$ from the provided data.\n\nAdditionally, verify from first principles that in an orthonormal frame the musical isomorphisms $\\,\\flat$ and $\\,\\sharp$ reduce to the identity on components, in the sense that if $\\alpha=\\sum_{i}\\alpha_{i}\\theta^{i}$ then $\\alpha^{\\sharp}=\\sum_{i}\\alpha_{i}e_{i}$, and if $X=\\sum_{i}X^{i}e_{i}$ then $X^{\\flat}=\\sum_{i}X^{i}\\theta^{i}$.\n\nProvide your final numerical result for $\\Delta f(p)$ as the answer. No rounding is required. Express the final answer without units.", "solution": "The problem is assessed to be valid. It is a well-posed problem in Riemannian geometry that requires the application of fundamental definitions and formulas in a non-trivial context (an orthonormal, non-coordinate frame). The data provided are sufficient and consistent for the computations requested. The potential issue regarding the Jacobi identity for the given structure functions is moot, as the problem can be solved using only the values of these functions at a single point, without needing their derivatives.\n\nThe solution proceeds by first principles as requested.\n\n### Verification of Musical Isomorphisms in an Orthonormal Frame\n\nLet $\\{e_i\\}$ be an orthonormal frame, so that the metric tensor $g$ satisfies $g(e_i, e_j) = \\langle e_i, e_j \\rangle = \\delta_{ij}$. Let $\\{\\theta^i\\}$ be the dual coframe, defined by $\\theta^i(e_j) = \\delta^i_j$.\n\nThe flat operator $\\flat$ maps a vector field $X$ to a 1-form $X^\\flat$ such that for any vector field $Y$, $X^\\flat(Y) = \\langle X, Y \\rangle$.\nLet $X = \\sum_i X^i e_i$. We express $X^\\flat$ in the dual basis as $X^\\flat = \\sum_j (X^\\flat)_j \\theta^j$. The components $(X^\\flat)_j$ are found by evaluating the 1-form on the basis vectors $e_j$:\n$$ (X^\\flat)_j = X^\\flat(e_j) = \\langle X, e_j \\rangle = \\left\\langle \\sum_i X^i e_i, e_j \\right\\rangle = \\sum_i X^i \\langle e_i, e_j \\rangle = \\sum_i X^i \\delta_{ij} = X^j $$\nThus, $X^\\flat = \\sum_j X^j \\theta^j$. The components of the vector $X$ with respect to $\\{e_i\\}$ are identical to the components of the 1-form $X^\\flat$ with respect to $\\{\\theta^i\\}$.\n\nThe sharp operator $\\sharp$ maps a 1-form $\\alpha$ to a vector field $\\alpha^\\sharp$ such that for any vector field $Y$, $\\langle \\alpha^\\sharp, Y \\rangle = \\alpha(Y)$.\nLet $\\alpha = \\sum_i \\alpha_i \\theta^i$. We express $\\alpha^\\sharp$ in the frame basis as $\\alpha^\\sharp = \\sum_j (\\alpha^\\sharp)^j e_j$. The components $(\\alpha^\\sharp)^j$ are found by taking the inner product with the basis vectors $e_j$:\n$$ (\\alpha^\\sharp)^j = \\langle \\alpha^\\sharp, e_j \\rangle = \\alpha(e_j) = \\left( \\sum_i \\alpha_i \\theta^i \\right)(e_j) = \\sum_i \\alpha_i \\theta^i(e_j) = \\sum_i \\alpha_i \\delta^i_j = \\alpha_j $$\nThus, $\\alpha^\\sharp = \\sum_j \\alpha_j e_j$. The components of the 1-form $\\alpha$ with respect to $\\{\\theta^i\\}$ are identical to the components of the vector field $\\alpha^\\sharp$ with respect to $\\{e_i\\}$. This completes the required verification.\n\n### 1. Gradient of $f$\n\nThe gradient $\\mathrm{grad}\\,f$ is defined as the vector field metrically dual to the 1-form $df$, i.e., $\\mathrm{grad}\\,f = (df)^\\sharp$.\nThe components of the differential $df$ in the coframe basis $\\{\\theta^i\\}$ are given by $(df)_i = df(e_i) = e_i f$. So, $df = \\sum_i (e_i f) \\theta^i$.\nUsing the result of the musical isomorphism verification, the components of $\\mathrm{grad}\\,f = (df)^\\sharp$ in the basis $\\{e_i\\}$ are the same as the components of $df$ in the basis $\\{\\theta^i\\}$.\n$$ (\\mathrm{grad}\\,f)^i = e_i f $$\nTherefore, the gradient vector field is expressed as:\n$$ \\mathrm{grad}\\,f = \\sum_i (e_i f) e_i $$\nAt the point $p$, the given values are $e_1 f = 1$, $e_2 f = -2$, and $e_3 f = 4$. So,\n$$ \\mathrm{grad}\\,f(p) = 1 e_1 - 2 e_2 + 4 e_3 $$\n\n### 2. Divergence of $X$\n\nThe divergence is the trace of the covariant derivative operator, $\\mathrm{div}\\,X = \\mathrm{tr}(Y \\mapsto \\nabla_Y X)$. In an orthonormal frame $\\{e_i\\}$, this is computed as $\\mathrm{div}\\,X = \\sum_i \\langle \\nabla_{e_i} X, e_i \\rangle$.\nLet $X = \\sum_j X^j e_j$. The covariant derivative is:\n$$ \\nabla_{e_i} X = \\nabla_{e_i} \\left(\\sum_j X^j e_j\\right) = \\sum_j \\left( (e_i X^j) e_j + X^j (\\nabla_{e_i} e_j) \\right) $$\nUsing the connection coefficients $\\Gamma_{ij}^k$ in the frame $\\{e_i\\}$, defined by $\\nabla_{e_i} e_j = \\sum_k \\Gamma_{ij}^k e_k$, we get:\n$$ \\nabla_{e_i} X = \\sum_j \\left( (e_i X^j) e_j + X^j \\sum_k \\Gamma_{ij}^k e_k \\right) = \\sum_k \\left( e_i X^k + \\sum_j X^j \\Gamma_{ij}^k \\right) e_k $$\nTaking the inner product with $e_i$:\n$$ \\langle \\nabla_{e_i} X, e_i \\rangle = \\left\\langle \\sum_k \\left( e_i X^k + \\sum_j X^j \\Gamma_{ij}^k \\right) e_k, e_i \\right\\rangle = e_i X^i + \\sum_j X^j \\Gamma_{ij}^i $$\nSumming over $i$ yields the expression for the divergence:\n$$ \\mathrm{div}\\,X = \\sum_i \\left( e_i X^i + \\sum_j X^j \\Gamma_{ij}^i \\right) = \\sum_i e_i(X^i) + \\sum_{i,j} X^j \\Gamma_{ij}^i $$\nFor the Levi-Civita connection (torsion-free and metric-compatible) in an orthonormal frame, the Koszul formula gives the connection coefficients in terms of the structure functions $c_{ij}{}^k = \\langle [e_i,e_j], e_k\\rangle$:\n$$ \\Gamma_{ij}^k = \\frac{1}{2}\\left(c_{ij}{}^k - c_{jk}{}^i + c_{ki}{}^j\\right) $$\nThe divergence calculation requires the quantities $A_j = \\sum_i \\Gamma_{ij}^i$. Let's compute these:\n$A_1 = \\sum_i \\Gamma_{i1}^i = \\Gamma_{11}^1 + \\Gamma_{21}^2 + \\Gamma_{31}^3$.\n- $\\Gamma_{11}^1 = \\frac{1}{2}(c_{11}{}^1 - c_{11}{}^1 + c_{11}{}^1) = 0$.\n- $\\Gamma_{21}^2 = \\frac{1}{2}(c_{21}{}^2 - c_{12}{}^2 + c_{22}{}^1) = \\frac{1}{2}(-c_{12}{}^2 - c_{12}{}^2 + 0) = -c_{12}{}^2$.\n- $\\Gamma_{31}^3 = \\frac{1}{2}(c_{31}{}^3 - c_{13}{}^3 + c_{33}{}^1) = \\frac{1}{2}(c_{31}{}^3 - (-c_{31}{}^3) + 0) = c_{31}{}^3$.\nSo, $A_1 = -c_{12}{}^2 + c_{31}{}^3 = -(-0.7) + 0.5 = 1.2$.\n\n$A_2 = \\sum_i \\Gamma_{i2}^i = \\Gamma_{12}^1 + \\Gamma_{22}^2 + \\Gamma_{32}^3$.\n- $\\Gamma_{12}^1 = \\frac{1}{2}(c_{12}{}^1 - c_{21}{}^1 + c_{11}{}^2) = \\frac{1}{2}(c_{12}{}^1 - (-c_{12}{}^1) + 0) = c_{12}{}^1$.\n- $\\Gamma_{22}^2 = 0$.\n- $\\Gamma_{32}^3 = \\frac{1}{2}(c_{32}{}^3 - c_{23}{}^3 + c_{33}{}^2) = \\frac{1}{2}(-c_{23}{}^3 - c_{23}{}^3 + 0) = -c_{23}{}^3$.\nSo, $A_2 = c_{12}{}^1 - c_{23}{}^3 = 1 - 1.3 = -0.3$.\n\n$A_3 = \\sum_i \\Gamma_{i3}^i = \\Gamma_{13}^1 + \\Gamma_{23}^2 + \\Gamma_{33}^3$.\n- $\\Gamma_{13}^1 = \\frac{1}{2}(c_{13}{}^1 - c_{31}{}^1 + c_{11}{}^3) = \\frac{1}{2}(-c_{31}{}^1 - c_{31}{}^1 + 0) = -c_{31}{}^1$.\n- $\\Gamma_{23}^2 = \\frac{1}{2}(c_{23}{}^2 - c_{32}{}^2 + c_{22}{}^3) = \\frac{1}{2}(c_{23}{}^2 - (-c_{23}{}^2) + 0) = c_{23}{}^2$.\n- $\\Gamma_{33}^3 = 0$.\nSo, $A_3 = -c_{31}{}^1 + c_{23}{}^2 = -(-2.2) + (-1) = 1.2$.\n\nThe expression for divergence is $\\mathrm{div}\\,X = \\sum_i e_i(X^i) + \\sum_j X^j A_j$.\nAt point $p$, we have $e_1(X^1)=0.4$, $e_2(X^2)=-0.3$, $e_3(X^3)=0.2$, so $\\sum_i e_i(X^i) = 0.4 - 0.3 + 0.2 = 0.3$.\nAnd $X^1=2$, $X^2=-1$, $X^3=3$. The second term is:\n$\\sum_j X^j A_j = X^1 A_1 + X^2 A_2 + X^3 A_3 = (2)(1.2) + (-1)(-0.3) + (3)(1.2) = 2.4 + 0.3 + 3.6 = 6.3$.\nThus, $\\mathrm{div}\\,X(p) = 0.3 + 6.3 = 6.6$.\n\n### 3. Laplacian of $f$\n\nThe Laplacian is defined as $\\Delta f = \\mathrm{div}(\\mathrm{grad}\\,f)$. We can use the formula for divergence derived above with $X$ replaced by $\\mathrm{grad}\\,f$.\nThe components of $V = \\mathrm{grad}\\,f$ are $V^i = e_i f$.\nThe expression for the Laplacian is:\n$$ \\Delta f = \\mathrm{div}(V) = \\sum_i e_i(V^i) + \\sum_{i,j} V^j \\Gamma_{ij}^i = \\sum_i e_i(e_i f) + \\sum_j (e_j f) \\left(\\sum_i \\Gamma_{ij}^i\\right) $$\nIn terms of the quantities $A_j = \\sum_i \\Gamma_{ij}^i$ computed above, this is:\n$$ \\Delta f = \\sum_i e_i e_i f + \\sum_j (e_j f) A_j $$\nWe are given the following values at $p$:\n- Second derivatives: $e_1 e_1 f=5$, $e_2 e_2 f=-3$, $e_3 e_3 f=2$.\n- First derivatives: $e_1 f=1$, $e_2 f=-2$, $e_3 f=4$.\nThe first term is $\\sum_i e_i e_i f = 5 + (-3) + 2 = 4$.\nThe second term is $\\sum_j (e_j f) A_j = (e_1 f) A_1 + (e_2 f) A_2 + (e_3 f) A_3$.\nUsing the values of $A_j$ and $e_j f$:\n$$ \\sum_j (e_j f) A_j = (1)(1.2) + (-2)(-0.3) + (4)(1.2) = 1.2 + 0.6 + 4.8 = 6.6 $$\nFinally, the Laplacian at $p$ is the sum of these two terms:\n$$ \\Delta f(p) = 4 + 6.6 = 10.6 $$", "answer": "$$\\boxed{10.6}$$", "id": "2980514"}]}