{"hands_on_practices": [{"introduction": "Understanding what it means for two metric spaces to be \"almost\" isometric is the cornerstone of rigidity theory. The Gromov-Hausdorff ($d_{GH}$) distance provides the definitive language for this, but its definition can be abstract. This first exercise [@problem_id:3025614] makes the concept concrete by relating $d_{GH}$ to a more tangible quantity: the minimal \"distortion\" over all possible correspondences between two spaces. This practice solidifies the foundational concept of what it means for two spaces to be metrically close and provides a direct link between local properties, like the existence of an $\\varepsilon$-isometry, and the global $d_{GH}$ distance.", "problem": "Let $(X,d_{X})$ and $(Y,d_{Y})$ be compact metric spaces. The Gromov–Hausdorff (GH) distance $d_{GH}(X,Y)$ is defined as the infimum, over all isometric embeddings $i:X\\to Z$ and $j:Y\\to Z$ into a common metric space $(Z,d_{Z})$, of the Hausdorff distance $d_{H}(i(X),j(Y))$. A correspondence $R\\subset X\\times Y$ is a relation such that for every $x\\in X$ there exists $y\\in Y$ with $(x,y)\\in R$, and for every $y\\in Y$ there exists $x\\in X$ with $(x,y)\\in R$. The distortion of a correspondence $R$ is defined by\n$$\n\\mathrm{dis}(R):=\\sup\\left\\{|d_{X}(x,x')-d_{Y}(y,y')|:\\ (x,y)\\in R,\\ (x',y')\\in R\\right\\}.\n$$\nAn $\\varepsilon$-isometry $f:X\\to Y$ is a map satisfying two properties: for all $x,x'\\in X$,\n$$\n|d_{Y}(f(x),f(x'))-d_{X}(x,x')|\\leq \\varepsilon,\n$$\nand $f(X)$ is an $\\varepsilon$-net in $Y$, meaning that for every $y\\in Y$ there exists $x\\in X$ with $d_{Y}(f(x),y)\\leq \\varepsilon$.\n\nStarting from these fundamental definitions, determine the optimal constant $C$ such that for every pair of compact metric spaces $(X,d_{X})$ and $(Y,d_{Y})$,\n$$\nd_{GH}(X,Y)=C\\cdot\\inf_{R}\\mathrm{dis}(R),\n$$\nwhere the infimum is taken over all correspondences $R$ between $X$ and $Y$. Moreover, using the same foundational framework, justify quantitatively how the existence of an $\\varepsilon$-isometry implies almost rigidity of the metric structure by deriving a universal bound of $d_{GH}(X,Y)$ in terms of $\\varepsilon$.\n\nYour final answer must be the exact value of $C$ as a real number. No rounding is required, and no units apply. As part of your derivation, illustrate the sharpness of your constant using an explicit pair of two-point metric spaces with distinct interpoint distances.", "solution": "The problem asks for two main results derived from the provided definitions for compact metric spaces $(X,d_X)$ and $(Y,d_Y)$. First, we must find the optimal constant $C$ in the identity $d_{GH}(X,Y)=C\\cdot\\inf_{R}\\mathrm{dis}(R)$. Second, we must derive a bound on $d_{GH}(X,Y)$ in terms of $\\varepsilon$ given the existence of an $\\varepsilon$-isometry. We will show that the constant is $C=\\frac{1}{2}$.\n\nLet us denote $d_{corr}(X,Y) = \\inf_{R}\\mathrm{dis}(R)$, where the infimum is taken over all correspondences $R \\subset X \\times Y$. We will prove that $d_{GH}(X,Y) = \\frac{1}{2}d_{corr}(X,Y)$ by establishing two inequalities.\n\n**Part 1: Proof of $d_{GH}(X,Y) \\ge \\frac{1}{2}d_{corr}(X,Y)$**\n\nLet $d = d_{GH}(X,Y)$. By the definition of the Gromov-Hausdorff distance, for any $\\eta  0$, there exists a metric space $(Z, d_Z)$ and isometric embeddings $i:X \\to Z$ and $j:Y \\to Z$ such that the Hausdorff distance $d_H(i(X),j(Y))  d+\\eta$.\n\nLet $\\varepsilon = d+\\eta$. The condition $d_H(i(X),j(Y))  \\varepsilon$ implies two properties:\n1. For every point $p \\in i(X)$, there exists a point $q \\in j(Y)$ such that $d_Z(p,q)  \\varepsilon$.\n2. For every point $q \\in j(Y)$, there exists a point $p \\in i(X)$ such that $d_Z(p,q)  \\varepsilon$.\n\nWe use this to construct a correspondence $R_{\\varepsilon} \\subset X \\times Y$ defined as:\n$$R_{\\varepsilon} = \\{ (x,y) \\in X \\times Y \\mid d_Z(i(x), j(y))  \\varepsilon \\}$$\nWe must verify that $R_{\\varepsilon}$ is a correspondence.\n- For any $x \\in X$, its image is $i(x) \\in i(X)$. By property 1, there exists $q \\in j(Y)$ with $d_Z(i(x),q)  \\varepsilon$. Since $j$ is an embedding of $Y$, $q=j(y)$ for some $y \\in Y$. Thus, for any $x \\in X$, there is a $y \\in Y$ such that $(x,y) \\in R_{\\varepsilon}$.\n- For any $y \\in Y$, its image is $j(y) \\in j(Y)$. By property 2, there exists $p \\in i(X)$ with $d_Z(p,j(y))  \\varepsilon$. Since $i$ is an embedding of $X$, $p=i(x)$ for some $x \\in X$. Thus, for any $y \\in Y$, there is an $x \\in X$ such that $(x,y) \\in R_{\\varepsilon}$.\nSo, $R_{\\varepsilon}$ is a valid correspondence.\n\nNow we bound its distortion, $\\mathrm{dis}(R_{\\varepsilon})$. Let $(x,y) \\in R_{\\varepsilon}$ and $(x',y') \\in R_{\\varepsilon}$. By definition of $R_{\\varepsilon}$, we have $d_Z(i(x), j(y))  \\varepsilon$ and $d_Z(i(x'), j(y'))  \\varepsilon$.\nSince $i$ and $j$ are isometries, $d_X(x,x') = d_Z(i(x),i(x'))$ and $d_Y(y,y') = d_Z(j(y),j(y'))$.\nUsing the triangle inequality in $Z$:\n$$d_X(x,x') = d_Z(i(x), i(x')) \\le d_Z(i(x), j(y)) + d_Z(j(y), j(y')) + d_Z(j(y'), i(x'))$$\n$$d_X(x,x')  \\varepsilon + d_Y(y,y') + \\varepsilon = d_Y(y,y') + 2\\varepsilon$$\nThis gives $d_X(x,x') - d_Y(y,y')  2\\varepsilon$.\nSimilarly,\n$$d_Y(y,y') = d_Z(j(y), j(y')) \\le d_Z(j(y), i(x)) + d_Z(i(x), i(x')) + d_Z(i(x'), j(y'))$$\n$$d_Y(y,y')  \\varepsilon + d_X(x,x') + \\varepsilon = d_X(x,x') + 2\\varepsilon$$\nThis gives $d_Y(y,y') - d_X(x,x')  2\\varepsilon$.\nCombining these, we obtain $|d_X(x,x') - d_Y(y,y')|  2\\varepsilon$.\nSince this holds for any pair of pairs from $R_{\\varepsilon}$, we have $\\mathrm{dis}(R_{\\varepsilon}) = \\sup |d_X(x,x') - d_Y(y,y')| \\le 2\\varepsilon = 2(d+\\eta)$.\nThis implies that $d_{corr}(X,Y) = \\inf_{R} \\mathrm{dis}(R) \\le \\mathrm{dis}(R_{\\varepsilon}) \\le 2(d+\\eta)$.\nSince $\\eta  0$ was arbitrary, we can let $\\eta \\to 0$ to get $d_{corr}(X,Y) \\le 2d = 2d_{GH}(X,Y)$.\nThis establishes the first inequality: $d_{GH}(X,Y) \\ge \\frac{1}{2}d_{corr}(X,Y)$.\n\n**Part 2: Proof of $d_{GH}(X,Y) \\le \\frac{1}{2}d_{corr}(X,Y)$**\n\nLet $R$ be any correspondence between $X$ and $Y$. Let $\\delta = \\mathrm{dis}(R)$. We want to show that $d_{GH}(X,Y) \\le \\delta/2$. To do this, we construct a metric space $(Z, d_Z)$ that contains isometric copies of $X$ and $Y$ whose Hausdorff distance is at most $\\delta/2$.\n\nLet $Z$ be the disjoint union $X \\sqcup Y$. We define a function $d_Z: Z \\times Z \\to \\mathbb{R}_{\\ge 0}$ as follows:\n- If $z_1, z_2 \\in X$, $d_Z(z_1, z_2) = d_X(z_1, z_2)$.\n- If $z_1, z_2 \\in Y$, $d_Z(z_1, z_2) = d_Y(z_1, z_2)$.\n- If $x \\in X, y \\in Y$, $d_Z(x,y) = d_Z(y,x) = \\inf_{(x', y') \\in R} \\{ d_X(x, x') + \\frac{\\delta}{2} + d_Y(y', y) \\}$.\n\nWe show that $d_Z$ is a metric. Symmetry holds by definition. $d_Z(z,z) = 0$ is trivial. $d_Z(z_1,z_2)0$ for $z_1\\neq z_2$ is clear except for $d_Z(x,y)$, which is always $\\ge \\delta/2  0$ if $\\delta0$. The main task is to verify the triangle inequality, $d_Z(z_1, z_3) \\le d_Z(z_1, z_2) + d_Z(z_2, z_3)$. The only non-trivial cases are when $z_1, z_2, z_3$ are not all in $X$ or all in $Y$.\nConsider $x_1, x_2 \\in X$ and $y \\in Y$.\n$$d_Z(x_1, y) + d_Z(y, x_2) = \\inf_{(x', y') \\in R} \\{ d_X(x_1, x') + \\frac{\\delta}{2} + d_Y(y', y) \\} + \\inf_{(x'', y'') \\in R} \\{ d_Y(y, y'') + \\frac{\\delta}{2} + d_X(x'', x_2) \\}$$\n$$\\ge \\inf_{(x', y'), (x'', y'') \\in R} \\{ d_X(x_1, x') + d_X(x_2, x'') + d_Y(y', y) + d_Y(y, y'') + \\delta \\}$$\nUsing $d_Y(y',y'') \\le d_Y(y',y) + d_Y(y,y'')$ and the distortion property $|d_X(x',x'') - d_Y(y',y'')| \\le \\delta \\implies d_Y(y',y'') \\ge d_X(x',x'') - \\delta$:\n$$d_Z(x_1, y) + d_Z(y, x_2) \\ge \\inf_{(x', y'), (x'', y'') \\in R} \\{ d_X(x_1, x') + d_X(x_2, x'') + d_Y(y', y'') + \\delta \\}$$\n$$\\ge \\inf_{(x', y'), (x'', y'') \\in R} \\{ d_X(x_1, x') + d_X(x_2, x'') + d_X(x', x'') - \\delta + \\delta \\}$$\n$$\\ge \\inf_{(x', y'), (x'', y'') \\in R} \\{ d_X(x_1, x') + d_X(x', x'') + d_X(x'', x_2) \\} \\ge d_X(x_1, x_2)$$\nThe last step uses the triangle inequality for $d_X$. So $d_X(x_1, x_2) \\le d_Z(x_1, y) + d_Z(y, x_2)$. The other cases are symmetric. Thus, $d_Z$ is a metric, and the natural inclusions of $X$ and $Y$ into $Z$ are isometries.\n\nNow we compute the Hausdorff distance $d_H(X, Y)$ in $Z$.\n$d_H(X,Y) = \\max \\left( \\sup_{x \\in X} \\inf_{y \\in Y} d_Z(x,y), \\sup_{y \\in Y} \\inf_{x \\in X} d_Z(x,y) \\right)$.\nFor any $x \\in X$:\n$\\inf_{y \\in Y} d_Z(x,y) = \\inf_{y \\in Y} \\inf_{(x', y') \\in R} \\{ d_X(x, x') + \\frac{\\delta}{2} + d_Y(y', y) \\}$.\nWe can swap the infima over $y$ and $(x',y')$. For a fixed $(x',y')$, the infimum over $y$ is achieved at $y=y'$, yielding $d_X(x, x') + \\frac{\\delta}{2}$.\nSo $\\inf_{y \\in Y} d_Z(x,y) = \\inf_{(x', y') \\in R} \\{ d_X(x, x') + \\frac{\\delta}{2} \\}$.\nThis is always $\\ge \\delta/2$.\nSince $R$ is a correspondence, for any $x \\in X$, there exists some $y_x \\in Y$ such that $(x,y_x) \\in R$.\nThen $d_Z(x,y_x) = \\inf_{(x', y') \\in R} \\{ d_X(x, x') + \\frac{\\delta}{2} + d_Y(y', y_x) \\}$.\nChoosing $(x',y') = (x,y_x)$ gives an upper bound: $d_Z(x,y_x) \\le d_X(x,x) + \\frac{\\delta}{2} + d_Y(y_x, y_x) = \\frac{\\delta}{2}$.\nHence, for any $x \\in X$, $\\inf_{y \\in Y} d_Z(x,y) = \\frac{\\delta}{2}$.\nThis implies $\\sup_{x \\in X} \\inf_{y \\in Y} d_Z(x,y) = \\frac{\\delta}{2}$. By symmetry, $\\sup_{y \\in Y} \\inf_{x \\in X} d_Z(x,y) = \\frac{\\delta}{2}$.\nSo, $d_H(X,Y) = \\frac{\\delta}{2}$.\nBy the definition of the Gromov-Hausdorff distance, $d_{GH}(X,Y) \\le d_H(X,Y) = \\frac{\\delta}{2} = \\frac{1}{2}\\mathrm{dis}(R)$.\nSince this holds for any correspondence $R$, we have $d_{GH}(X,Y) \\le \\frac{1}{2}\\inf_{R} \\mathrm{dis}(R) = \\frac{1}{2}d_{corr}(X,Y)$.\n\nCombining the two inequalities, we conclude $d_{GH}(X,Y) = \\frac{1}{2}d_{corr}(X,Y)$. Thus, the optimal constant is $C=\\frac{1}{2}$.\n\n**Part 3: Sharpness of the constant**\n\nConsider two-point metric spaces. Let $X=\\{p_1, p_2\\}$ with $d_X(p_1, p_2)=a$ for some $a  0$. Let $Y=\\{q_1, q_2\\}$ with $d_Y(q_1, q_2)=b$ for some $b  0$.\nThe minimal correspondence is a bijection, e.g., $R = \\{(p_1, q_1), (p_2, q_2)\\}$. The only pair of pairs to check for distortion is $((p_1,q_1), (p_2,q_2))$, which gives $|d_X(p_1,p_2) - d_Y(q_1,q_2)| = |a-b|$. Any larger correspondence would also have to contain pairs like $((p_1,q_1), (p_1,q_2))$, giving distortion $|d_X(p_1,p_1) - d_Y(q_1,q_2)| = b$, which is generally larger. A careful analysis shows $\\inf_R \\mathrm{dis}(R) = |a-b|$.\nThe Gromov-Hausdorff distance between these two spaces is known to be $d_{GH}(X,Y) = \\frac{1}{2}|a-b|$.\nFor this example, $d_{GH}(X,Y) = \\frac{1}{2} \\inf_R \\mathrm{dis}(R)$ holds exactly, demonstrating that the constant $C=\\frac{1}{2}$ is sharp.\n\n**Part 4: Bound from $\\varepsilon$-isometry**\n\nGiven an $\\varepsilon$-isometry $f:X \\to Y$, we wish to bound $d_{GH}(X,Y)$. An $\\varepsilon$-isometry satisfies:\n1. $|d_Y(f(x), f(x')) - d_X(x, x')| \\le \\varepsilon$ for all $x,x' \\in X$.\n2. $f(X)$ is an $\\varepsilon$-net in $Y$: for all $y \\in Y$, there exists $x \\in X$ with $d_Y(y, f(x)) \\le \\varepsilon$.\n\nWe construct a correspondence $R \\subset X \\times Y$ as $R = \\{ (x, y) \\mid d_Y(f(x), y) \\le \\varepsilon \\}$.\nThis is a correspondence: for any $x \\in X$, $(x, f(x)) \\in R$ since $d_Y(f(x),f(x))=0 \\le \\varepsilon$. For any $y \\in Y$, the $\\varepsilon$-net property guarantees an $x \\in X$ with $d_Y(f(x), y) \\le \\varepsilon$, so $(x,y) \\in R$.\n\nNow we bound $\\mathrm{dis}(R)$. Let $(x,y) \\in R$ and $(x',y') \\in R$. So $d_Y(f(x), y) \\le \\varepsilon$ and $d_Y(f(x'), y') \\le \\varepsilon$.\nUsing the triangle inequality:\n$d_Y(y,y') \\le d_Y(y, f(x)) + d_Y(f(x), f(x')) + d_Y(f(x'), y') \\le \\varepsilon + d_Y(f(x), f(x')) + \\varepsilon$.\nFrom property 1, $d_Y(f(x), f(x')) \\le d_X(x,x')+\\varepsilon$.\nThus, $d_Y(y,y') \\le \\varepsilon + (d_X(x,x')+\\varepsilon) + \\varepsilon = d_X(x,x') + 3\\varepsilon$, which means $d_Y(y,y') - d_X(x,x') \\le 3\\varepsilon$.\nFor the other direction:\n$d_X(x,x') \\le d_Y(f(x),f(x')) + \\varepsilon$ from property 1.\n$d_Y(f(x),f(x')) \\le d_Y(f(x),y) + d_Y(y,y') + d_Y(y',f(x')) \\le \\varepsilon + d_Y(y,y') + \\varepsilon$.\nSo, $d_X(x,x') \\le (\\varepsilon + d_Y(y,y') + \\varepsilon) + \\varepsilon = d_Y(y,y') + 3\\varepsilon$, which means $d_X(x,x') - d_Y(y,y') \\le 3\\varepsilon$.\nCombining these, $|d_X(x,x') - d_Y(y,y')| \\le 3\\varepsilon$.\nSo, $\\mathrm{dis}(R) \\le 3\\varepsilon$.\n\nUsing our main result, we get a bound on the Gromov-Hausdorff distance:\n$$d_{GH}(X,Y) = \\frac{1}{2} d_{corr}(X,Y) \\le \\frac{1}{2}\\mathrm{dis}(R) \\le \\frac{1}{2}(3\\varepsilon) = \\frac{3}{2}\\varepsilon$$\nThis bound $d_{GH}(X,Y) \\le \\frac{3}{2}\\varepsilon$ provides a quantitative justification for the \"almost rigidity\" of the metric structure: if a space $X$ admits a map to $Y$ that is an $\\varepsilon$-isometry for a small $\\varepsilon$, then the two spaces are globally close in the Gromov-Hausdorff sense.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3025614"}, {"introduction": "Moving to the richer setting of Riemannian manifolds, we can ask how a condition like non-negative Ricci curvature enforces geometric structure. This practice [@problem_id:3025583] explores a key technique from Cheeger-Colding theory, where the \"excess function\" measures how far a region is from splitting into a product. You will use powerful analytic tools, such as the Caccioppoli inequality, to translate this small analytic deficit into a quantitative statement about the metric tensor itself, providing a glimpse into the powerful interplay between analysis and geometry.", "problem": "Let $(M^{n},g)$ be a complete $n$-dimensional Riemannian manifold with nonnegative Ricci curvature, and let $p,q \\in M$ be joined by a minimizing geodesic segment of length $L$ with midpoint $m$, so that $d(p,m) = d(q,m) = L/2$. Fix a radius $R > 0$ with $L \\geq 8R$, and consider the geodesic ball $B_R(m)$. Define the excess function $e : M \\to \\mathbb{R}$ by $e(x) = d(p,x) + d(x,q) - L$, and assume the small excess condition\n$$\n\\sup_{B_R(m)} e \\leq \\varepsilon R\n$$\nfor some $0  \\varepsilon \\ll 1$. Define the splitting-direction distance-difference function $u : M \\to \\mathbb{R}$ by\n$$\nu(x) = \\frac{d(p,x) - d(q,x)}{2}.\n$$\nUse only the following foundational inputs:\n- The almost everywhere identities $|\\nabla d(p,\\cdot)| = 1$, $|\\nabla d(q,\\cdot)| = 1$, and $\\nabla e = \\nabla d(p,\\cdot) + \\nabla d(q,\\cdot)$ on the regular set of the distance functions.\n- The weak subharmonicity of the excess on manifolds with nonnegative Ricci curvature (Abresch–Gromoll), namely that $e \\geq 0$ and, in the weak sense, $\\Delta e \\geq 0$ on $B_R(m) \\setminus (\\operatorname{Cut}(p) \\cup \\operatorname{Cut}(q))$.\n- The Caccioppoli inequality for nonnegative weakly subharmonic functions: for any Lipschitz cutoff $\\eta$ with compact support,\n$$\n\\int \\eta^{2} |\\nabla e|^{2} \\leq 4 \\int e^{2} |\\nabla \\eta|^{2}.\n$$\n- Bishop–Gromov volume comparison under nonnegative Ricci curvature: for $0  r \\leq R$, one has $\\operatorname{Vol}(B_R(m))/\\operatorname{Vol}(B_r(m)) \\leq (R/r)^{n}$.\n\nDerive, from first principles and the above facts, a quantitative control of the metric tensor component in the splitting direction by computing an explicit upper bound for the averaged deviation\n$$\nA := \\frac{1}{\\operatorname{Vol}(B_{R/2}(m))} \\int_{B_{R/2}(m)} \\big(1 - g(\\nabla u, \\nabla u)\\big).\n$$\nYour derivation must begin from the listed foundational inputs and justify each step. Express your final answer as a closed-form analytic expression depending only on $n$ and $\\varepsilon$ (and no other geometric quantities), by eliminating geometric-volume factors using Bishop–Gromov. No inequalities should appear as the final answer. Do not round; give the exact expression.", "solution": "The problem is valid. The premises are standard in the field of geometric analysis, specifically in the context of Cheeger-Colding theory on manifolds with Ricci curvature bounds. The problem is self-contained and well-posed. We proceed to derive the upper bound for $A$ from the given foundational inputs.\n\nOur objective is to find an explicit upper bound for the quantity\n$$\nA := \\frac{1}{\\operatorname{Vol}(B_{R/2}(m))} \\int_{B_{R/2}(m)} \\big(1 - g(\\nabla u, \\nabla u)\\big) \\, dV\n$$\nwhere $dV$ is the volume element of the metric $g$.\n\nFirst, we establish a relationship between the integrand $1 - g(\\nabla u, \\nabla u)$ and the excess function $e$. The splitting-direction function $u$ and the excess function $e$ are defined as:\n$$\nu(x) = \\frac{d(p,x) - d(q,x)}{2}\n$$\n$$\ne(x) = d(p,x) + d(q,x) - L\n$$\nLet us denote $d_p(x) = d(p,x)$ and $d_q(x) = d(q,x)$. The problem provides the almost-everywhere identities:\n$|\\nabla d_p| = 1$, $|\\nabla d_q| = 1$, and $\\nabla e = \\nabla d_p + \\nabla d_q$.\nFrom the definition of $u$, its gradient is $\\nabla u = \\frac{1}{2}(\\nabla d_p - \\nabla d_q)$.\n\nWe can compute the squared norms of $\\nabla u$ and $\\nabla e$:\n$$\ng(\\nabla u, \\nabla u) = |\\nabla u|^2 = \\frac{1}{4} |\\nabla d_p - \\nabla d_q|^2 = \\frac{1}{4} (|\\nabla d_p|^2 - 2g(\\nabla d_p, \\nabla d_q) + |\\nabla d_q|^2)\n$$\n$$\n|\\nabla e|^2 = |\\nabla d_p + \\nabla d_q|^2 = |\\nabla d_p|^2 + 2g(\\nabla d_p, \\nabla d_q) + |\\nabla d_q|^2\n$$\nUsing the given identities $|\\nabla d_p| = 1$ and $|\\nabla d_q| = 1$, these expressions simplify to:\n$$\n|\\nabla u|^2 = \\frac{1}{2}(1 - g(\\nabla d_p, \\nabla d_q))\n$$\n$$\n|\\nabla e|^2 = 2(1 + g(\\nabla d_p, \\nabla d_q))\n$$\nWe can solve both equations for $g(\\nabla d_p, \\nabla d_q)$:\nFrom the first, $g(\\nabla d_p, \\nabla d_q) = 1 - 2|\\nabla u|^2$.\nFrom the second, $g(\\nabla d_p, \\nabla d_q) = \\frac{1}{2}|\\nabla e|^2 - 1$.\nEquating these two expressions for $g(\\nabla d_p, \\nabla d_q)$ gives a crucial relation between $|\\nabla u|^2$ and $|\\nabla e|^2$:\n$$\n1 - 2|\\nabla u|^2 = \\frac{1}{2}|\\nabla e|^2 - 1 \\implies 2 - 2|\\nabla u|^2 = \\frac{1}{2}|\\nabla e|^2 \\implies 4(1 - |\\nabla u|^2) = |\\nabla e|^2\n$$\nThus, the integrand of $A$ can be expressed as:\n$$\n1 - g(\\nabla u, \\nabla u) = 1 - |\\nabla u|^2 = \\frac{1}{4}|\\nabla e|^2\n$$\nSubstituting this into the definition of $A$:\n$$\nA = \\frac{1}{4\\operatorname{Vol}(B_{R/2}(m))} \\int_{B_{R/2}(m)} |\\nabla e|^2 \\, dV\n$$\nNext, we bound the integral of $|\\nabla e|^2$ using the given Caccioppoli inequality. We need a suitable Lipschitz cutoff function $\\eta$. Let $\\eta : M \\to \\mathbb{R}$ be a function that is $1$ on $B_{R/2}(m)$, vanishes outside $B_R(m)$, and is linearly interpolated in between. Specifically, let $r(x) = d(x,m)$ be the distance from the center $m$. Define:\n$$\n\\eta(x) = \\begin{cases} 1  \\text{if } r(x) \\leq R/2 \\\\ 2 - \\frac{2r(x)}{R}  \\text{if } R/2  r(x)  R \\\\ 0  \\text{if } r(x) \\geq R \\end{cases}\n$$\nThis function has compact support in $B_R(m)$. For $x \\in B_{R/2}(m)$, $\\eta(x) = 1$, so $|\\nabla \\eta(x)| = 0$. For $x \\in M \\setminus B_R(m)$, $\\eta(x)=0$, so $|\\nabla \\eta(x)|=0$. On the annulus $B_R(m) \\setminus B_{R/2}(m)$, we have $\\nabla \\eta = -\\frac{2}{R} \\nabla r$. Since $|\\nabla r| = 1$ almost everywhere, we have $|\\nabla \\eta| = \\frac{2}{R}$ almost everywhere on this annulus. Thus, $|\\nabla \\eta|^2 = \\frac{4}{R^2}$ on the annulus and $0$ elsewhere.\n\nSince $\\eta=1$ on $B_{R/2}(m)$ and $\\eta \\leq 1$ everywhere, we can bound the integral for $A$:\n$$\n\\int_{B_{R/2}(m)} |\\nabla e|^2 \\, dV \\leq \\int_{B_R(m)} \\eta^2 |\\nabla e|^2 \\, dV\n$$\nThe problem states that $e$ is a nonnegative weakly subharmonic function, so we can apply the Caccioppoli inequality:\n$$\n\\int_{B_R(m)} \\eta^2 |\\nabla e|^2 \\, dV \\leq 4 \\int_{B_R(m)} e^2 |\\nabla \\eta|^2 \\, dV\n$$\nThe integral on the right-hand side is non-zero only where $|\\nabla \\eta| \\neq 0$, which is the annulus $B_R(m) \\setminus B_{R/2}(m)$.\n$$\n4 \\int_{B_R(m)} e^2 |\\nabla \\eta|^2 \\, dV = 4 \\int_{B_R(m) \\setminus B_{R/2}(m)} e^2 \\left(\\frac{4}{R^2}\\right) \\, dV = \\frac{16}{R^2} \\int_{B_R(m) \\setminus B_{R/2}(m)} e^2 \\, dV\n$$\nWe use the given small excess condition, $\\sup_{B_R(m)} e \\leq \\varepsilon R$. This implies $e(x)^2 \\leq (\\varepsilon R)^2 = \\varepsilon^2 R^2$ for all $x \\in B_R(m)$.\n$$\n\\frac{16}{R^2} \\int_{B_R(m) \\setminus B_{R/2}(m)} e^2 \\, dV \\leq \\frac{16}{R^2} \\int_{B_R(m) \\setminus B_{R/2}(m)} (\\varepsilon^2 R^2) \\, dV = 16 \\varepsilon^2 \\operatorname{Vol}(B_R(m) \\setminus B_{R/2}(m))\n$$\nSince $\\operatorname{Vol}(B_R(m) \\setminus B_{R/2}(m)) \\leq \\operatorname{Vol}(B_R(m))$, we can write the coarser but sufficient bound:\n$$\n\\int_{B_{R/2}(m)} |\\nabla e|^2 \\, dV \\leq 16 \\varepsilon^2 \\operatorname{Vol}(B_R(m))\n$$\nNow, substitute this bound back into the expression for $A$:\n$$\nA \\leq \\frac{1}{4\\operatorname{Vol}(B_{R/2}(m))} \\left(16 \\varepsilon^2 \\operatorname{Vol}(B_R(m))\\right) = 4 \\varepsilon^2 \\frac{\\operatorname{Vol}(B_R(m))}{\\operatorname{Vol}(B_{R/2}(m))}\n$$\nFinally, we eliminate the geometric volume ratio using the provided Bishop-Gromov volume comparison theorem for manifolds with nonnegative Ricci curvature. For $0  r \\leq R$, we have $\\operatorname{Vol}(B_R(m))/\\operatorname{Vol}(B_r(m)) \\leq (R/r)^n$. We apply this with $r = R/2$:\n$$\n\\frac{\\operatorname{Vol}(B_R(m))}{\\operatorname{Vol}(B_{R/2}(m))} \\leq \\left(\\frac{R}{R/2}\\right)^n = 2^n\n$$\nSubstituting this into our inequality for $A$:\n$$\nA \\leq 4 \\varepsilon^2 (2^n) = 2^{n+2} \\varepsilon^2\n$$\nThis provides the required quantitative control. The final explicit upper bound for $A$ depends only on the dimension $n$ and the parameter $\\varepsilon$.", "answer": "$$\n\\boxed{2^{n+2}\\varepsilon^{2}}\n$$", "id": "3025583"}, {"introduction": "A central goal of stability theory is to show that spaces which are \"close\" to a model space also share its important analytic properties, such as functional inequalities. However, \"closeness\" in the metric sense alone can be misleading. This problem [@problem_id:3025584] guides you through constructing a crucial counterexample to demonstrate why Gromov-Hausdorff convergence is insufficient, and why the notion of measured convergence is essential for the stability of structures like Sobolev inequalities. This insight is critical for understanding the modern framework of analysis on metric measure spaces.", "problem": "Let $\\{(X_i,d_i)\\}_{i\\in\\mathbb{N}}$ be a sequence of compact metric spaces and let $X_\\infty$ be a compact metric space. The Gromov–Hausdorff (GH) distance $d_{GH}$ between two compact metric spaces is defined via isometric embeddings into a common metric space and the Hausdorff distance of the embedded images. In the study of functional inequalities, the underlying measure is essential; the pointed measured Gromov–Hausdorff (pmGH) convergence additionally encodes weak convergence of Borel probability measures under the GH correspondences. Recall that weak convergence of measures $\\mu_i \\rightharpoonup \\mu_\\infty$ means that for every bounded continuous function $h:X_\\infty\\to\\mathbb{R}$, one has $\\int h\\,\\mathrm{d}\\mu_i \\to \\int h\\,\\mathrm{d}\\mu_\\infty$.\n\nConsider Sobolev-type inequalities on metric measure spaces, such as the $L^2$-Sobolev inequality on a compact metric measure space $(X,d,\\mu)$ of the form $\\|f\\|_{L^q(\\mu)} \\leq C\\,\\| \\nabla f\\|_{L^2(\\mu)}$ for some fixed exponent $q2$ and constant $C0$, where $|\\nabla f|$ denotes any admissible metric gradient representative arising from the Cheeger energy. Stability results in geometric analysis typically require pmGH convergence to pass such inequalities to limits.\n\nYour task is to identify a construction that exhibits the following phenomenon: the metric spaces converge in GH distance, namely $d_{GH}(X_i,X_\\infty)\\to 0$, but the associated measured structures fail to converge (in the weak sense) and, accordingly, no stability of Sobolev inequalities can be inferred from GH convergence alone. Select the option that both constructs an explicit sequence and rigorously explains, from first principles and core definitions, why GH convergence holds, why the measures fail to converge, and how this failure obstructs stability of Sobolev inequalities.\n\nOptions:\n\nA. Let $X_i=[0,1]$ with the Euclidean metric $d_i(x,y)=|x-y|$ for every $i\\in\\mathbb{N}$, and let $X_\\infty=[0,1]$ with the Euclidean metric. For a fixed parameter $\\alpha\\in(0,1)$, define $\\mu_i=(1-\\alpha)\\,\\lambda+\\alpha\\,\\delta_{x_i}$ where $\\lambda$ is the Lebesgue probability measure on $[0,1]$ and $\\delta_{x_i}$ is the Dirac mass at $x_i\\in[0,1]$. Choose $x_i$ to alternate between $0$ and $1$, i.e., $x_i=0$ for odd $i$ and $x_i=1$ for even $i$. Then $d_{GH}(X_i,X_\\infty)=0$ for all $i$, but $\\{\\mu_i\\}$ fails to converge weakly. Consequently, for the fixed Lipschitz function $g(x)=x$, the quantity $\\|g\\|_{L^q(\\mu_i)}$ oscillates while $\\|\\nabla g\\|_{L^2(\\mu_i)}$ is constant, showing that GH convergence alone does not yield stability of $L^2$-Sobolev inequalities and motivates pmGH stability assumptions.\n\nB. Let $X_i=[0,1]\\cup S_i$ where $S_i$ is a segment of length $\\varepsilon_i$ attached at $0$ (i.e., a metric wedge) with $\\varepsilon_i\\to 0$, and let $X_\\infty=[0,1]$ with its Euclidean metric. Endow $X_i$ with the normalized arc-length measure (Hausdorff measure) $\\mu_i$ on $[0,1]\\cup S_i$, and $X_\\infty$ with Lebesgue measure $\\lambda$. Then $d_{GH}(X_i,X_\\infty)\\to 0$, $\\mu_i\\rightharpoonup\\lambda$, and therefore the measured structures converge. This shows that GH convergence implies measured convergence and hence Sobolev inequalities are automatically stable.\n\nC. Let $X_i=[0,1]$ with the Euclidean metric and let $\\mu_i=(1-\\alpha)\\,\\lambda+\\alpha\\,\\delta_{x_0}$ for a fixed $x_0\\in(0,1)$ and fixed $\\alpha\\in(0,1)$. Take $X_\\infty=[0,1]$ with the Euclidean metric and $\\mu_\\infty=(1-\\alpha)\\,\\lambda+\\alpha\\,\\delta_{x_0}$. Then $d_{GH}(X_i,X_\\infty)=0$ and $\\mu_i\\rightharpoonup\\mu_\\infty$, illustrating that in this setting pmGH convergence holds automatically; hence stability of Sobolev inequalities follows from GH convergence.\n\nD. Let $X_i$ be the cycle graph on $i$ vertices with graph distance scaled so that the total length is $1$, equipped with the uniform probability measure on vertices, and let $X_\\infty=S^1$ the unit circle with arc-length metric and normalized arc-length measure. Then $d_{GH}(X_i,X_\\infty)\\to 0$ and the measures converge in the sense that graph measures pmGH-converge to arc-length measure. Therefore, GH convergence without explicit measure control suffices for Sobolev stability in this case.\n\nWhich option correctly constructs the required example and provides the correct justification?", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- A sequence of compact metric spaces: $\\{(X_i,d_i)\\}_{i\\in\\mathbb{N}}$.\n- A limit compact metric space: $X_\\infty$.\n- Gromov–Hausdorff (GH) distance: $d_{GH}$.\n- Pointed measured Gromov–Hausdorff (pmGH) convergence: Implies weak convergence of Borel probability measures $\\mu_i \\rightharpoonup \\mu_\\infty$ under GH correspondences.\n- Weak convergence of measures: For every bounded continuous function $h:X_\\infty\\to\\mathbb{R}$, $\\int h\\,\\mathrm{d}\\mu_i \\to \\int h\\,\\mathrm{d}\\mu_\\infty$.\n- Sobolev-type inequality: $\\|f\\|_{L^q(\\mu)} \\leq C\\,\\| \\nabla f\\|_{L^2(\\mu)}$ for some fixed exponent $q2$ and constant $C0$ on a compact metric measure space $(X,d,\\mu)$.\n- The task is to identify a construction where $d_{GH}(X_i,X_\\infty)\\to 0$, but the associated measures fail to converge weakly, thereby obstructing the stability of Sobolev inequalities.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on well-established concepts in geometric analysis and the theory of metric measure spaces, including Gromov-Hausdorff convergence, weak convergence of measures, and Sobolev inequalities on metric spaces (Cheeger-Colding theory). The premise that stability of functional inequalities often requires stronger convergence than just GH convergence (i.e., measured GH convergence) is a central theme in this field.\n- **Well-Posed:** The problem asks for the identification of a specific mathematical construction that illustrates a known and important phenomenon. The question is clear, and a definite answer exists.\n- **Objective:** The problem uses precise, standard mathematical terminology and is free of subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed question in geometric analysis that tests the understanding of the subtleties between different notions of convergence for metric measure spaces. I will proceed to analyze the options.\n\nThe core of the problem is to find a sequence of metric measure spaces $(X_i, d_i, \\mu_i)$ that demonstrates that Gromov-Hausdorff convergence of the metric spaces, $d_{GH}(X_i, X_\\infty) \\to 0$, is insufficient to guarantee the weak convergence of the measures $\\mu_i$. This failure of measure convergence then obstructs the stability of properties like Sobolev inequalities.\n\n### Option-by-Option Analysis\n\n**A. Let $X_i=[0,1]$ with the Euclidean metric $d_i(x,y)=|x-y|$ for every $i\\in\\mathbb{N}$, and let $X_\\infty=[0,1]$ with the Euclidean metric. For a fixed parameter $\\alpha\\in(0,1)$, define $\\mu_i=(1-\\alpha)\\,\\lambda+\\alpha\\,\\delta_{x_i}$ where $\\lambda$ is the Lebesgue probability measure on $[0,1]$ and $\\delta_{x_i}$ is the Dirac mass at $x_i\\in[0,1]$. Choose $x_i$ to alternate between $0$ and $1$, i.e., $x_i=0$ for odd $i$ and $x_i=1$ for even $i$. Then $d_{GH}(X_i,X_\\infty)=0$ for all $i$, but $\\{\\mu_i\\}$ fails to converge weakly. Consequently, for the fixed Lipschitz function $g(x)=x$, the quantity $\\|g\\|_{L^q(\\mu_i)}$ oscillates while $\\|\\nabla g\\|_{L^2(\\mu_i)}$ is constant, showing that GH convergence alone does not yield stability of $L^2$-Sobolev inequalities and motivates pmGH stability assumptions.**\n\n1.  **GH Convergence:** The spaces $X_i$ and $X_\\infty$ are all the interval $[0,1]$ with the standard Euclidean metric. They are isometric to each other. The identity map is an isometry between any $X_i$ and $X_\\infty$. Therefore, the Gromov-Hausdorff distance is $d_{GH}(X_i, X_\\infty) = 0$ for all $i \\in \\mathbb{N}$. This part of the statement is correct.\n\n2.  **Weak Convergence of Measures:** To check if the sequence of measures $\\{\\mu_i\\}_{i\\in\\mathbb{N}}$ converges weakly, we must test if the sequence of integrals $\\int h \\, d\\mu_i$ converges for *every* bounded continuous function $h:[0,1] \\to \\mathbb{R}$. Let's test this with the function $h(x)=x$. This function is continuous and bounded on $[0,1]$.\n    The integral is:\n    $$ \\int_0^1 h(x) \\, \\mathrm{d}\\mu_i = \\int_0^1 x \\, \\mathrm{d}((1-\\alpha)\\lambda + \\alpha\\delta_{x_i}) $$\n    $$ = (1-\\alpha) \\int_0^1 x \\, \\mathrm{d}\\lambda(x) + \\alpha \\int_0^1 x \\, \\mathrm{d}\\delta_{x_i}(x) $$\n    $$ = (1-\\alpha) \\left[\\frac{x^2}{2}\\right]_0^1 + \\alpha h(x_i) = \\frac{1-\\alpha}{2} + \\alpha x_i $$\n    According to the construction, $x_i$ alternates:\n    - For odd $i$, $x_i = 0$, so the integral is $\\frac{1-\\alpha}{2}$.\n    - For even $i$, $x_i = 1$, so the integral is $\\frac{1-\\alpha}{2} + \\alpha$.\n    Since $\\alpha \\in (0,1)$, we have $\\alpha \\neq 0$. The sequence of integrals $\\frac{1-\\alpha}{2}, \\frac{1-\\alpha}{2}+\\alpha, \\frac{1-\\alpha}{2}, \\frac{1-\\alpha}{2}+\\alpha, \\dots$ oscillates between two distinct values and thus does not converge. This proves that the sequence of measures $\\{\\mu_i\\}$ does not converge weakly.\n\n3.  **Stability of Sobolev Inequality:** The option analyzes the behavior of the terms in a Sobolev-type inequality for the function $g(x)=x$.\n    - The gradient term: For the function $g(x)=x$ on $[0,1]$, its metric gradient is $|\\nabla g|(x)=|g'(x)|=1$ for almost every $x$. The $L^2$-norm of the gradient is:\n      $$ \\|\\nabla g\\|_{L^2(\\mu_i)}^2 = \\int_0^1 |\\nabla g|^2 \\, \\mathrm{d}\\mu_i = \\int_0^1 1^2 \\, \\mathrm{d}\\mu_i = \\int_0^1 1 \\, \\mathrm{d}\\mu_i $$\n      Since each $\\mu_i$ is a probability measure (as it's a convex combination of two probability measures), $\\int_0^1 1 \\, \\mathrm{d}\\mu_i = 1$. Thus, $\\|\\nabla g\\|_{L^2(\\mu_i)} = 1$ for all $i$. The statement that this quantity is constant is correct.\n    - The function norm term: The $L^q$-norm of the function $g$ is:\n      $$ \\|g\\|_{L^q(\\mu_i)}^q = \\int_0^1 |g(x)|^q \\, \\mathrm{d}\\mu_i = \\int_0^1 x^q \\, \\mathrm{d}((1-\\alpha)\\lambda + \\alpha\\delta_{x_i}) $$\n      $$ = (1-\\alpha) \\int_0^1 x^q \\, \\mathrm{d}\\lambda(x) + \\alpha x_i^q = \\frac{1-\\alpha}{q+1} + \\alpha x_i^q $$\n      - For odd $i$, $x_i=0$, so $\\|g\\|_{L^q(\\mu_i)}^q = \\frac{1-\\alpha}{q+1}$.\n      - For even $i$, $x_i=1$, so $\\|g\\|_{L^q(\\mu_i)}^q = \\frac{1-\\alpha}{q+1} + \\alpha$.\n      The sequence $\\|g\\|_{L^q(\\mu_i)}$ oscillates and does not converge.\n\n4.  **Conclusion:** The ratio $\\|g\\|_{L^q(\\mu_i)} / \\|\\nabla g\\|_{L^2(\\mu_i)}$ oscillates. This means that any Sobolev constant $C_i$ satisfying the inequality for all functions on $(X_i, d_i, \\mu_i)$ cannot be expected to converge. The stability is broken. This construction perfectly illustrates the phenomenon requested in the problem. The reasoning is rigorous and based on first principles.\n\nVerdict: **Correct**.\n\n**B. Let $X_i=[0,1]\\cup S_i$ where $S_i$ is a segment of length $\\varepsilon_i$ attached at $0$ (i.e., a metric wedge) with $\\varepsilon_i\\to 0$, and let $X_\\infty=[0,1]$ with its Euclidean metric. Endow $X_i$ with the normalized arc-length measure (Hausdorff measure) $\\mu_i$ on $[0,1]\\cup S_i$, and $X_\\infty$ with Lebesgue measure $\\lambda$. Then $d_{GH}(X_i,X_\\infty)\\to 0$, $\\mu_i\\rightharpoonup\\lambda$, and therefore the measured structures converge. This shows that GH convergence implies measured convergence and hence Sobolev inequalities are automatically stable.**\n\nThis option constructs a scenario where both the spaces and the measures converge. As shown in the thought process, for any continuous function $h:[0,1] \\to \\mathbb{R}$, the integral $\\int_{X_i} (h \\circ \\pi_i) \\, \\mathrm{d}\\mu_i$ converges to $\\int_0^1 h \\, \\mathrm{d}\\lambda$, where $\\pi_i:X_i \\to [0,1]$ is the natural projection. Thus, pmGH convergence holds. The option's primary purpose is to provide a counterexample where measures *fail* to converge. By providing an example where they *do* converge, this option fails to fulfill the task. Furthermore, it draws the incorrect general conclusion that \"GH convergence implies measured convergence\", which is precisely the fallacy the problem is designed to refute.\n\nVerdict: **Incorrect**.\n\n**C. Let $X_i=[0,1]$ with the Euclidean metric and let $\\mu_i=(1-\\alpha)\\,\\lambda+\\alpha\\,\\delta_{x_0}$ for a fixed $x_0\\in(0,1)$ and fixed $\\alpha\\in(0,1)$. Take $X_\\infty=[0,1]$ with the Euclidean metric and $\\mu_\\infty=(1-\\alpha)\\,\\lambda+\\alpha\\,\\delta_{x_0}$. Then $d_{GH}(X_i,X_\\infty)=0$ and $\\mu_i\\rightharpoonup\\mu_\\infty$, illustrating that in this setting pmGH convergence holds automatically; hence stability of Sobolev inequalities follows from GH convergence.**\n\nIn this construction, both the spaces $X_i$ and the measures $\\mu_i$ form constant sequences. $X_i = [0,1]$ for all $i$, so $d_{GH}(X_i, X_\\infty) = 0$. The measure $\\mu_i = (1-\\alpha)\\,\\lambda+\\alpha\\,\\delta_{x_0}$ is the same for all $i$. A constant sequence of measures $\\mu_i = \\mu$ trivially converges weakly to $\\mu$. So, pmGH convergence holds. Like option B, this is an example of convergence, not a counterexample demonstrating failure. It does not address the core task of the problem. It also makes the false conclusion that stability follows from GH convergence in general.\n\nVerdict: **Incorrect**.\n\n**D. Let $X_i$ be the cycle graph on $i$ vertices with graph distance scaled so that the total length is $1$, equipped with the uniform probability measure on vertices, and let $X_\\infty=S^1$ the unit circle with arc-length metric and normalized arc-length measure. Then $d_{GH}(X_i,X_\\infty)\\to 0$ and the measures converge in the sense that graph measures pmGH-converge to arc-length measure. Therefore, GH convergence without explicit measure control suffices for Sobolev stability in this case.**\n\nThis option describes a well-known example of convergence in the pmGH sense. The sequence of cycle graphs $X_i$ with their scaled graph distance GH-converges to the circle $S^1$. The corresponding uniform measures on the vertices also converge weakly to the uniform arc-length measure on $S^1$. This is a standard example where both metric and measure structures converge together. As in options B and C, this fails to provide the required counterexample where measures *fail* to converge. The conclusion drawn, \"GH convergence without explicit measure control suffices for Sobolev stability in this case,\" is misleading. It's not just GH convergence, but the specific choice of measures that leads to stability. The option does not fulfill the problem's task.\n\nVerdict: **Incorrect**.\n\nIn summary, only option A correctly constructs a sequence of metric measure spaces where the metric spaces converge in the GH sense, but the measures fail to converge weakly, and correctly explains how this failure destabilizes the Sobolev inequality.", "answer": "$$\\boxed{A}$$", "id": "3025584"}]}