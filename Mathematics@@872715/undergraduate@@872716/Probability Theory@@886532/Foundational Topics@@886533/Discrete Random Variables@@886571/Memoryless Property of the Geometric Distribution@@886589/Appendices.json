{"hands_on_practices": [{"introduction": "The best way to solidify our understanding of the memoryless property is to apply it. This first exercise provides a classic scenario: modeling the lifetime of a component. By calculating the probability of failure in the immediate future, given that the component has already survived a number of trials, you'll see firsthand how the past history of successes becomes irrelevant for future predictions [@problem_id:11763]. This practice is fundamental to grasping why the geometric distribution is \"memoryless.\"", "problem": "Consider a sequence of independent Bernoulli trials, where the probability of \"failure\" in any given trial is $p$, and the probability of \"success\" (i.e., not failing) is $1-p$. Let the random variable $X$ denote the trial number on which the first failure occurs. The probability mass function (PMF) for $X$ is given by the geometric distribution:\n$$\nP(X=k) = (1-p)^{k-1} p, \\quad \\text{for } k = 1, 2, 3, \\dots\n$$\nThis represents the probability that there are $k-1$ consecutive successes followed by one failure.\n\nA component's lifetime is modeled by this random variable $X$. Given that the component has successfully survived the first $n=10$ trials, derive an expression for the probability that it will fail on either trial $n+1$ or trial $n+2$. Express your answer in terms of $p$.", "solution": "We seek $P(\\text{fail on trial }n+1\\text{ or }n+2\\mid Xn)$ with $n=10$.  By definition of conditional probability,\n$$\nP(X=n+1\\text{ or }n+2\\mid Xn)\n=\\frac{P((X=n+1\\text{ or }n+2)\\land Xn)}{P(Xn)}.\n$$\nSince $Xn$ implies $X\\ge n+1$, the numerator is \n$$\nP(X=n+1\\text{ or }n+2)\n=P(X=n+1)+P(X=n+2).\n$$\nUsing the PMF $P(X=k)=(1-p)^{k-1}p$, we have\n$$\nP(X=n+1)=(1-p)^n p,\\qquad\nP(X=n+2)=(1-p)^{n+1}p.\n$$\nAlso,\n$$\nP(Xn)=\\sum_{k=n+1}^{\\infty}(1-p)^{k-1}p\n=(1-p)^n\\sum_{j=0}^{\\infty}(1-p)^j\\,p\n=(1-p)^n.\n$$\nTherefore\n\n$$\nP(X=n+1\\text{ or }n+2\\mid Xn)\n=\\frac{(1-p)^n p+(1-p)^{n+1}p}{(1-p)^n}\n=p + (1-p)p\n=p\\bigl(1+1-p\\bigr)\n=p(2-p).\n$$\n\nAn alternative view uses the memoryless property of the geometric distribution: conditioned on surviving the first $n$ trials, the distribution of the excess lifetime is the same as the original, so\n\n$$\nP(\\text{fail at }n+1\\text{ or }n+2\\mid Xn)\n=P(X'=1\\text{ or }2)\n=p+(1-p)p\n=p(2-p).\n$$", "answer": "$$\\boxed{p(2-p)}$$", "id": "11763"}, {"introduction": "Having seen how memorylessness affects probabilities, we now explore its deeper implications for other statistical measures. This problem asks for the variance of the *additional* waiting time, given that a number of initial trials have already passed without success [@problem_id:1351951]. It's a powerful demonstration that the entire probability distribution of the remaining time is reset, not just the probability of the next event, meaning measures like expected value and variance remain constant regardless of the past.", "problem": "An experiment consists of repeatedly flipping a fair coin until a head first appears. Suppose it is observed that the first three flips of the coin all resulted in tails.\n\nGiven this observation, what is the variance of the number of *additional* flips required to obtain the first head?", "solution": "Let each flip be an independent Bernoulli trial with success probability $p=\\frac{1}{2}$ for a head. Let $X$ denote the total number of flips needed to obtain the first head. Then $X$ has the geometric distribution on $\\{1,2,\\ldots\\}$ with\n$$\n\\Pr(X=k)=(1-p)^{k-1}p \\quad \\text{for } k\\in \\{1,2,\\ldots\\}.\n$$\nThe observation that the first three flips are tails is the event $\\{X3\\}$. Let $Y$ denote the number of additional flips required to obtain the first head after these three tails. Then $Y=X-3$ conditional on $\\{X3\\}$. For $k\\in \\{1,2,\\ldots\\}$,\n$$\n\\Pr(Y=k \\mid X3)=\\Pr(X=3+k \\mid X3)=\\frac{\\Pr(X=3+k)}{\\Pr(X3)}.\n$$\nUsing the geometric pmf and tail,\n$$\n\\Pr(X=3+k)=(1-p)^{3+k-1}p,\\qquad \\Pr(X3)=(1-p)^{3},\n$$\nso\n$$\n\\Pr(Y=k \\mid X3)=\\frac{(1-p)^{3+k-1}p}{(1-p)^{3}}=(1-p)^{k-1}p.\n$$\nThus $Y \\mid (X3)$ is geometric with parameter $p$, by the memoryless property. The variance of a geometric random variable on $\\{1,2,\\ldots\\}$ with parameter $p$ is\n$$\n\\operatorname{Var}(Y)=\\frac{1-p}{p^{2}}.\n$$\nSubstituting $p=\\frac{1}{2}$ gives\n$$\n\\operatorname{Var}(Y)=\\frac{1-\\frac{1}{2}}{\\left(\\frac{1}{2}\\right)^{2}}=\\frac{\\frac{1}{2}}{\\frac{1}{4}}=2.\n$$\nTherefore, the variance of the number of additional flips required is $2$.", "answer": "$$\\boxed{2}$$", "id": "1351951"}, {"introduction": "The memoryless property has profound consequences when we consider a sequence of events. This problem shifts our focus from a single waiting time to the relationship between consecutive waiting times, such as the time to the first success and the additional time to the second [@problem_id:1922961]. By investigating their relationship, you will discover that the memory-resetting nature of the underlying process leads to the powerful and elegant conclusion that these waiting times are statistically independent.", "problem": "A research team is studying the spontaneous mutation of a specific gene in a bacterium culture. The experiment consists of a sequence of independent trials. In each trial, a single bacterium is isolated and tested for the mutation. The probability that any given bacterium has the mutation is a constant $p$, where $0  p  1$.\n\nLet the random variable $X_1$ denote the number of trials conducted up to and including the first observation of a mutated bacterium. Let the random variable $X_2$ denote the number of additional trials required after the first mutation was found, up to and including the second observation of a mutated bacterium.\n\nWhich of the following statements correctly describes the relationship between the random variables $X_1$ and $X_2$?\n\nA. $X_1$ and $X_2$ are independent.\n\nB. $X_1$ and $X_2$ are positively correlated, meaning that a larger value of $X_1$ tends to be associated with a larger value of $X_2$.\n\nC. $X_1$ and $X_2$ are negatively correlated, meaning that a larger value of $X_1$ tends to be associated with a smaller value of $X_2$.\n\nD. The relationship (independence or dependence) between $X_1$ and $X_2$ depends on the specific value of the probability $p$.\n\nE. $X_1$ and $X_2$ are dependent, but their covariance is zero.", "solution": "Let $\\{Y_{n}\\}_{n\\geq 1}$ be an independent and identically distributed sequence with $Y_{n}\\sim\\text{Bernoulli}(p)$, where $0p1$. A mutation corresponds to $Y_{n}=1$. Define\n$$\nX_{1}=\\min\\{n\\geq 1:Y_{n}=1\\},\n$$\nthe trial index of the first mutation, and\n$$\nX_{2}=\\min\\{m\\geq 1:Y_{X_{1}+m}=1\\},\n$$\nthe additional number of trials after the first mutation until the second mutation occurs. Equivalently, if $T_{k}$ denotes the index of the $k$th success, then $X_{1}=T_{1}$ and $X_{2}=T_{2}-T_{1}$.\n\nBy independence of the $Y_{n}$, the distribution of $X_{1}$ is geometric with parameter $p$ supported on $\\{1,2,\\dots\\}$:\n$$\n\\mathbb{P}(X_{1}=a)=(1-p)^{a-1}p,\\quad a\\in\\{1,2,\\dots\\}.\n$$\nAfter time $X_{1}$, the future trials $\\{Y_{X_{1}+1},Y_{X_{1}+2},\\dots\\}$ are independent of the past and have the same distribution as the original sequence. Hence $X_{2}$ is also geometric with parameter $p$ on $\\{1,2,\\dots\\}$:\n$$\n\\mathbb{P}(X_{2}=b)=(1-p)^{b-1}p,\\quad b\\in\\{1,2,\\dots\\}.\n$$\n\nTo establish independence, compute the joint distribution for $a,b\\in\\{1,2,\\dots\\}$:\n$$\n\\mathbb{P}(X_{1}=a,X_{2}=b)=\\mathbb{P}(Y_{1}=0,\\dots,Y_{a-1}=0,Y_{a}=1,Y_{a+1}=0,\\dots,Y_{a+b-1}=0,Y_{a+b}=1).\n$$\nBy independence of the $Y_{n}$,\n$$\n\\mathbb{P}(X_{1}=a,X_{2}=b)=(1-p)^{a-1}p\\cdot(1-p)^{b-1}p=\\big((1-p)^{a-1}p\\big)\\big((1-p)^{b-1}p\\big).\n$$\nTherefore\n$$\n\\mathbb{P}(X_{1}=a,X_{2}=b)=\\mathbb{P}(X_{1}=a)\\,\\mathbb{P}(X_{2}=b),\n$$\nwhich proves that $X_{1}$ and $X_{2}$ are independent.\n\nConsequently, the correct statement is that $X_{1}$ and $X_{2}$ are independent. Options asserting positive or negative correlation, dependence varying with $p$, or dependence with zero covariance are all incorrect in this model.", "answer": "$$\\boxed{A}$$", "id": "1922961"}]}