{"hands_on_practices": [{"introduction": "Understanding the covariance between two random variables is not just an abstract exercise; it's essential for practical applications where we combine different sources of data. This practice demonstrates how covariance naturally emerges when we analyze the variance of combined signals, such as the sum and difference of sensor outputs [@problem_id:1354744]. By working through this problem, you will see how the properties of variance provide an elegant and indirect method for determining the covariance, reinforcing the algebraic relationship between these two fundamental statistical measures.", "problem": "An engineer is calibrating a system with two noisy sensors, A and B. The dimensionless output signal from sensor A is modeled as a random variable $X$, and the output from sensor B is modeled as a random variable $Y$. To understand the system's combined behavior, two new signals are constructed: a sum signal, $S = X+Y$, and a difference signal, $D = X-Y$. After extensive testing, the variance of the sum signal is measured to be $\\text{Var}(S) = 10$, and the variance of the difference signal is measured to be $\\text{Var}(D) = 2$. Based on these measurements, determine the value of the covariance between the two sensor signals, $\\text{Cov}(X, Y)$.", "solution": "Let $X$ and $Y$ be random variables with finite second moments. Denote $\\operatorname{Var}(X)=\\sigma_{X}^{2}$, $\\operatorname{Var}(Y)=\\sigma_{Y}^{2}$, and $\\operatorname{Cov}(X,Y)=C$. Define $S=X+Y$ and $D=X-Y$. We use the variance properties:\n- For any random variables $U$ and $V$, $\\operatorname{Var}(U\\pm V)=\\operatorname{Var}(U)+\\operatorname{Var}(V)\\pm 2\\operatorname{Cov}(U,V)$.\nApplying this to $S$ and $D$ gives\n$$\n\\operatorname{Var}(S)=\\sigma_{X}^{2}+\\sigma_{Y}^{2}+2C=10,\n$$\n$$\n\\operatorname{Var}(D)=\\sigma_{X}^{2}+\\sigma_{Y}^{2}-2C=2.\n$$\nLet $A=\\sigma_{X}^{2}+\\sigma_{Y}^{2}$. Then the system becomes\n$$\nA+2C=10,\\quad A-2C=2.\n$$\nSubtracting the second equation from the first yields\n$$\n4C=8 \\quad \\Rightarrow \\quad C=2.\n$$\nThus, $\\operatorname{Cov}(X,Y)=2$.", "answer": "$$\\boxed{2}$$", "id": "1354744"}, {"introduction": "While we can calculate covariance, not just any value is permissible. A collection of variances and covariances, assembled into a covariance matrix, must satisfy certain mathematical constraints to be valid. This exercise explores the concept of positive semidefiniteness, a fundamental property of all covariance matrices, which ensures that the variance of any linear combination of the variables is non-negative [@problem_id:1354713]. This practice will help you understand the theoretical bounds on covariance and its relationship to the correlation coefficient.", "problem": "A data scientist is modeling the relationship between two random variables, $X_1$ and $X_2$, which represent daily measurements from two correlated sensors. The theoretical variances of these measurements are known to be $\\text{Var}(X_1) = 4$ and $\\text{Var}(X_2) = 9$. The covariance between the two sensors, $\\text{Cov}(X_1, X_2)$, is an unknown parameter denoted by $c$. The data scientist constructs the corresponding covariance matrix $\\Sigma$ for the random vector $\\mathbf{X} = [X_1, X_2]^T$ as:\n$$\n\\Sigma = \\begin{pmatrix} 4 & c \\\\ c & 9 \\end{pmatrix}\n$$\nFor this matrix to be a mathematically valid covariance matrix, the parameter $c$ must be restricted to a specific range of real numbers. Which of the following options represents the complete set of all possible values for $c$?\n\nA. $[0, 6]$\n\nB. $[-36, 36]$\n\nC. $[-6, 6]$\n\nD. $[-\\sqrt{6}, \\sqrt{6}]$\n\nE. $(-\\infty, \\infty)$", "solution": "A real covariance matrix must be symmetric positive semidefinite. For a symmetric $2 \\times 2$ matrix\n$$\n\\Sigma=\\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix},\n$$\npositive semidefiniteness is equivalent to the nonnegativity of all leading principal minors:\n$$\na \\geq 0,\\quad d \\geq 0,\\quad \\det(\\Sigma) \\geq 0.\n$$\nHere, $a=4$ and $d=9$ are already nonnegative. The determinant condition gives\n$$\n\\det(\\Sigma)=4 \\cdot 9 - c^{2}=36 - c^{2} \\geq 0,\n$$\nwhich implies\n$$\nc^{2} \\leq 36 \\quad \\Longrightarrow \\quad -6 \\leq c \\leq 6.\n$$\nEquivalently, by the Cauchyâ€“Schwarz inequality for random variables, $|\\text{Cov}(X_{1},X_{2})| \\leq \\sqrt{\\text{Var}(X_{1})\\text{Var}(X_{2})}=\\sqrt{4 \\cdot 9}=6$, yielding the same interval. Therefore, the complete set of all possible values for $c$ is $[-6,6]$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1354713"}, {"introduction": "A common pitfall in statistics is to assume that if two variables have zero covariance, they must be independent. This is not true, and understanding why is crucial for correct data interpretation. This problem presents a classic counterexample involving the Cartesian coordinates of a point moving on a circle, demonstrating a situation where two variables are clearly dependent, yet their covariance is zero [@problem_id:1354723]. This hands-on calculation will solidify your understanding that covariance measures only the linear component of a relationship between variables.", "problem": "A point particle is constrained to move along the circumference of a unit circle centered at the origin of a Cartesian coordinate system. The position of the particle is described by a single random variable, $\\Theta$, which represents the angle its position vector makes with the positive x-axis. The angle $\\Theta$ is uniformly distributed over the interval $[0, 2\\pi)$, with the angle measured in radians.\n\nLet the Cartesian coordinates of this particle be represented by the random variables $X$ and $Y$. Therefore, the relationships between the random variables are $X = \\cos(\\Theta)$ and $Y = \\sin(\\Theta)$.\n\nCalculate the covariance between the random variables $X$ and $Y$, denoted as $\\text{Cov}(X, Y)$. Provide a single numerical value as your answer.", "solution": "We use the definition of covariance, $\\text{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]$. With $\\Theta$ uniformly distributed on $[0,2\\pi)$, its density is $f_{\\Theta}(\\theta)=\\frac{1}{2\\pi}$ for $0\\leq \\theta<2\\pi$. The random variables are $X=\\cos(\\Theta)$ and $Y=\\sin(\\Theta)$.\n\nFirst compute the means. By definition of expectation for a function of a continuous random variable,\n$$\n\\mathbb{E}[X]=\\mathbb{E}[\\cos(\\Theta)]=\\int_{0}^{2\\pi}\\cos(\\theta)\\,\\frac{1}{2\\pi}\\,d\\theta=\\frac{1}{2\\pi}\\left[\\sin(\\theta)\\right]_{0}^{2\\pi}=0,\n$$\nand similarly,\n$$\n\\mathbb{E}[Y]=\\mathbb{E}[\\sin(\\Theta)]=\\int_{0}^{2\\pi}\\sin(\\theta)\\,\\frac{1}{2\\pi}\\,d\\theta=\\frac{1}{2\\pi}\\left[-\\cos(\\theta)\\right]_{0}^{2\\pi}=0.\n$$\n\nNext compute $\\mathbb{E}[XY]=\\mathbb{E}[\\cos(\\Theta)\\sin(\\Theta)]$. Using the identity $\\sin(2\\theta)=2\\sin(\\theta)\\cos(\\theta)$, we have\n$$\n\\mathbb{E}[XY]=\\int_{0}^{2\\pi}\\cos(\\theta)\\sin(\\theta)\\,\\frac{1}{2\\pi}\\,d\\theta=\\frac{1}{2\\pi}\\cdot\\frac{1}{2}\\int_{0}^{2\\pi}\\sin(2\\theta)\\,d\\theta.\n$$\nEvaluate the integral:\n$$\n\\int_{0}^{2\\pi}\\sin(2\\theta)\\,d\\theta=\\left[-\\frac{\\cos(2\\theta)}{2}\\right]_{0}^{2\\pi}=-\\frac{\\cos(4\\pi)-\\cos(0)}{2}=-\\frac{1-1}{2}=0,\n$$\nso\n$$\n\\mathbb{E}[XY]=0.\n$$\n\nTherefore,\n$$\n\\text{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]=0-0\\cdot 0=0.\n$$", "answer": "$$\\boxed{0}$$", "id": "1354723"}]}