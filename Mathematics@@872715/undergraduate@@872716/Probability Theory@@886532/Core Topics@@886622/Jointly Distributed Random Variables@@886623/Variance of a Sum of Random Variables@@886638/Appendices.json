{"hands_on_practices": [{"introduction": "We begin with a classic and intuitive example: rolling two dice. This problem serves as a practical application of calculating the variance for a sum of two *independent* random variables. It reinforces the foundational principle that when variables are independent, the variance of their sum is simply the sum of their individual variances, a key building block for more complex analyses.", "problem": "Consider two standard, fair, six-sided dice. Let the random variable $X_1$ represent the outcome of rolling the first die, and $X_2$ represent the outcome of rolling the second die. The possible outcomes for each die are the integers $\\{1, 2, 3, 4, 5, 6\\}$, with each outcome having an equal probability of occurrence. The two rolls are independent events.\n\nLet a new random variable $S$ be defined as the sum of the outcomes of the two dice, such that $S = X_1 + X_2$.\n\nYour task is to derive the variance of the sum $S$, denoted as $\\text{Var}(S)$. Base your derivation on the fundamental definitions of expectation and variance.\n\n**Definitions:**\n- The **expectation** (or expected value) of a discrete random variable $Y$ is given by $E[Y] = \\sum_i y_i P(Y=y_i)$, where $y_i$ are the possible values of $Y$.\n- The **variance** of a random variable $Y$ is given by $\\text{Var}(Y) = E[(Y - E[Y])^2]$, which can be simplified to the computational formula $\\text{Var}(Y) = E[Y^2] - (E[Y])^2$.", "solution": "We have two independent random variables $X_1,X_2$ each uniform on $\\{1,2,3,4,5,6\\}$.  By definition,\n$$E[X_i]=\\sum_{k=1}^6k\\cdot\\frac16=\\frac{1+2+3+4+5+6}6=\\frac{21}6=\\frac72.$$\nNext,\n$$E[X_i^2]=\\sum_{k=1}^6k^2\\cdot\\frac16=\\frac{1^2+2^2+3^2+4^2+5^2+6^2}6=\\frac{91}6.$$\nThus the variance of one die is\n$$\\text{Var}(X_i)=E[X_i^2]-\\bigl(E[X_i]\\bigr)^2\n=\\frac{91}6-\\Bigl(\\frac72\\Bigr)^2\n=\\frac{91}6-\\frac{49}4\n=\\frac{182-147}{12}\n=\\frac{35}{12}.$$\nSince $S=X_1+X_2$ and $X_1,X_2$ are independent,\n$$\\text{Var}(S)=\\text{Var}(X_1)+\\text{Var}(X_2)=2\\cdot\\frac{35}{12}=\\frac{35}{6}.$$", "answer": "$$\\boxed{\\frac{35}{6}}$$", "id": "18404"}, {"introduction": "In many real-world systems, variables are not independent; they influence one another. This exercise introduces the concept of *dependent* random variables, requiring the use of the full formula for the variance of a sum: $\\text{Var}(Z) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)$. By working through a discrete joint probability distribution [@problem_id:18394], you will see firsthand how the covariance term accounts for the relationship between the variables.", "problem": "Consider two discrete random variables, $X$ and $Y$. The variable $X$ can take values from the set $\\{0, a\\}$, and the variable $Y$ can take values from the set $\\{0, b\\}$, where $a$ and $b$ are distinct positive constants ($a > 0, b > 0, a \\neq b$). Their joint probability mass function, $P(X=x, Y=y)$, is defined by a parameter $p$ according to the table below.\n\n|           | $X=0$ | $X=a$ |\n|:---------:|:-----:|:-----:|\n|   **Y=0**   |  $p$  | $2p$  |\n|   **Y=b**   | $2p$  | $1-5p$|\n\nThe parameter $p$ is a real number such that $0  p  \\frac{1}{5}$, which ensures all probabilities are positive.\n\nDefine a new random variable $Z = X + Y$. Derive a symbolic expression for the variance of $Z$, denoted as $\\text{Var}(Z)$, in terms of the constants $a$, $b$, and $p$.", "solution": "We use the identity  \n$$\\text{Var}(Z)=\\text{Var}(X+Y)=\\text{Var}(X)+\\text{Var}(Y)+2\\text{Cov}(X,Y).$$  \nMarginally,  \n$$P(X=0)=p+2p=3p,\\quad P(X=a)=1-3p,$$  \n$$P(Y=0)=p+2p=3p,\\quad P(Y=b)=1-3p.$$  \nThus  \n$$E[X]=a(1-3p),\\quad E[Y]=b(1-3p).$$  \nCompute variances:  \n$$E[X^2]=a^2(1-3p),\\quad \\text{Var}(X)=E[X^2]-E[X]^2 = a^2(1-3p)-a^2(1-3p)^2 =3a^2p(1-3p),$$  \n$$\\text{Var}(Y)=3b^2p(1-3p).$$  \nFor the covariance,  \n$$E[XY]=ab\\,P(X=a,Y=b)=ab(1-5p),$$  \n$$\\text{Cov}(X,Y)=E[XY]-E[X]E[Y] \n=ab\\bigl[(1-5p)-(1-3p)^2\\bigr]\n=ab\\,p(1-9p).$$  \nHence  \n$$\\text{Var}(Z)=3p(1-3p)(a^2+b^2)+2ab\\,p(1-9p).$$", "answer": "$$\\boxed{3p(1-3p)(a^2 + b^2) + 2p(1-9p)ab}$$", "id": "18394"}, {"introduction": "To truly grasp the impact of covariance, it is helpful to examine an extreme case. This problem presents a thought experiment where one random variable is the exact opposite of another, creating perfect negative correlation. By calculating the variance of their sum, you will uncover a surprising but logical result that highlights how a strong negative relationship can dramatically reduce, or even eliminate, the variability of a combined system.", "problem": "Let $X$ be a random variable with a well-defined mean $E[X] = \\mu$ and a finite, non-zero variance $\\text{Var}(X) = \\sigma^2$. Let a second random variable $Y$ be defined as a linear function of $X$, specifically $Y = -X$.\n\nThe variance of a random variable $A$ is defined as $\\text{Var}(A) = E[(A - E[A])^2]$. The covariance between two random variables $A$ and $B$ is defined as $\\text{Cov}(A,B) = E[(A - E[A])(B - E[B])]$.\n\nThe variance for the sum of two random variables $X$ and $Y$ is given by the general formula:\n$$\n\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y)\n$$\n\nUsing these definitions and properties, derive the value of $\\text{Var}(X+Y)$.", "solution": "We have $Y=-X$.  First compute the mean and variance of $Y$:\n$$E[Y]=E[-X]=-E[X]=-\\mu,$$\n$$\\text{Var}(Y)=E\\bigl[(Y-E[Y])^2\\bigr]\n=E\\bigl[(-X+\\mu)^2\\bigr]\n=E\\bigl[(X-\\mu)^2\\bigr]\n=\\sigma^2.$$\nNext compute the covariance:\n$$\\text{Cov}(X,Y)=E\\bigl[(X-E[X])(Y-E[Y])\\bigr]\n=E\\bigl[(X-\\mu)(-X+\\mu)\\bigr]\n=-E\\bigl[(X-\\mu)^2\\bigr]\n=-\\sigma^2.$$\nNow apply the variance sum formula:\n$$\\text{Var}(X+Y)=\\text{Var}(X)+\\text{Var}(Y)+2\\,\\text{Cov}(X,Y)\n=\\sigma^2+\\sigma^2+2(-\\sigma^2)\n=0.$$", "answer": "$$\\boxed{0}$$", "id": "18400"}]}