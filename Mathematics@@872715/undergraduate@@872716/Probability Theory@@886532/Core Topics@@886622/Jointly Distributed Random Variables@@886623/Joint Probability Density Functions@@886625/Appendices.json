{"hands_on_practices": [{"introduction": "The first principle of working with any probability density function is ensuring its validity. A joint PDF must integrate to 1 over its entire domain of support, meaning the total probability of all possible outcomes is 1. This exercise [@problem_id:1926404] provides essential practice in applying this normalization condition. You will determine the constant of proportionality that makes the given function a true joint PDF by setting up and evaluating a double integral over a region defined by curves.", "problem": "A research team is studying the spatial distribution of a certain type of microorganism on a nutrient-rich substrate. The substrate is a flat plate, and the coordinates $(X, Y)$ of a microorganism are modeled as a pair of continuous random variables. The joint probability density function (PDF) for the location of a single microorganism is found to be $f(x, y) = c(x+y)$ for points $(x, y)$ in a specific region $R$, and $f(x, y) = 0$ otherwise. The region $R$ is defined by the area in the first quadrant of the xy-plane bounded by the curve $y=x^2$, the line $x=1$, and the x-axis (where $y=0$). For the PDF to be valid, it must be normalized over this region. Determine the value of the normalization constant $c$.", "solution": "For a joint probability density function to be valid, it must integrate to 1 over its support. The normalization condition is\n$$\n\\iint_{R} c(x+y)\\,dx\\,dy = 1.\n$$\nThe region $R$ is the set of points in the first quadrant bounded by $y=0$, $y=x^{2}$, and $x=1$, which can be written as\n$$\nR=\\{(x,y): 0 \\leq x \\leq 1,\\ 0 \\leq y \\leq x^{2}\\}.\n$$\nThus,\n$$\n\\iint_{R} c(x+y)\\,dx\\,dy = c \\int_{0}^{1} \\int_{0}^{x^{2}} (x+y)\\,dy\\,dx.\n$$\nEvaluate the inner integral:\n$$\n\\int_{0}^{x^{2}} (x+y)\\,dy = \\left[xy + \\frac{1}{2}y^{2}\\right]_{0}^{x^{2}} = x x^{2} + \\frac{1}{2} x^{4} = x^{3} + \\frac{1}{2} x^{4}.\n$$\nNow integrate with respect to $x$:\n$$\nc \\int_{0}^{1} \\left(x^{3} + \\frac{1}{2} x^{4}\\right)\\,dx = c \\left(\\left[\\frac{1}{4}x^{4}\\right]_{0}^{1} + \\left[\\frac{1}{10}x^{5}\\right]_{0}^{1}\\right) = c\\left(\\frac{1}{4} + \\frac{1}{10}\\right) = c \\cdot \\frac{7}{20}.\n$$\nSet this equal to 1 and solve for $c$:\n$$\nc \\cdot \\frac{7}{20} = 1 \\quad \\Rightarrow \\quad c = \\frac{20}{7}.\n$$", "answer": "$$\\boxed{\\frac{20}{7}}$$", "id": "1926404"}, {"introduction": "Once we have a valid joint PDF, we can use it to quantify the relationship between the random variables. Covariance is a fundamental measure that indicates the direction of the linear relationship between two variables. This practice problem [@problem_id:9607] guides you through the direct calculation of covariance, $\\operatorname{Cov}(X, Y) = E[XY] - E[X]E[Y]$. Mastering this calculation is key to understanding how variables interact within a system.", "problem": "Two continuous random variables, $X$ and $Y$, have a joint probability density function (JPDF) given by:\n$$\nf(x,y) = \\begin{cases} C  \\text{if } (x,y) \\text{ is in the triangular region } R \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe triangular region $R$ is defined by the vertices $(0,0)$, $(1,1)$, and $(0,1)$. The constant $C=2$.\n\nThe covariance between two random variables $X$ and $Y$ is defined as:\n$$\n\\operatorname{Cov}(X, Y) = E[XY] - E[X]E[Y]\n$$\nwhere $E[g(X,Y)]$ is the expected value of a function $g(X,Y)$, calculated by:\n$$\nE[g(X,Y)] = \\iint_R g(x,y) f(x,y) \\,dA\n$$\n\nDerive the value of the covariance, $\\operatorname{Cov}(X, Y)$.", "solution": "The joint density is \n$$f(x,y)=2,\\quad R=\\{(x,y):0\\le x\\le y\\le1\\}$$\nso that \n$$\\iint_Rf\\,dA\n=2\\int_{y=0}^1\\int_{x=0}^y1\\,dx\\,dy\n=2\\int_0^1y\\,dy\n=2\\cdot\\frac{1}{2}=1.$$\n\n1. Compute $E[X]$:\n$$E[X]\n=\\iint_Rx\\,f(x,y)\\,dA\n=2\\int_{y=0}^1\\int_{x=0}^yx\\,dx\\,dy\n=2\\int_0^1\\frac{y^2}{2}\\,dy\n=\\int_0^1y^2\\,dy\n=\\frac{1}{3}.$$\n\n2. Compute $E[Y]$:\n$$E[Y]\n=\\iint_Ry\\,f(x,y)\\,dA\n=2\\int_{y=0}^1\\int_{x=0}^yy\\,dx\\,dy\n=2\\int_0^1y\\,(y-0)\\,dy\n=2\\int_0^1y^2\\,dy\n=2\\cdot\\frac{1}{3}\n=\\frac{2}{3}.$$\n\n3. Compute $E[XY]$:\n$$E[XY]\n=\\iint_Rxy\\,f(x,y)\\,dA\n=2\\int_{y=0}^1\\int_{x=0}^yxy\\,dx\\,dy\n=2\\int_0^1y\\int_0^yx\\,dx\\,dy\n=2\\int_0^1y\\cdot\\frac{y^2}{2}\\,dy\n=\\int_0^1y^3\\,dy\n=\\frac{1}{4}.$$\n\n4. Therefore the covariance is\n$$\\operatorname{Cov}(X,Y)\n=E[XY]-E[X]\\,E[Y]\n=\\frac{1}{4}-\\frac{1}{3}\\cdot\\frac{2}{3}\n=\\frac{1}{4}-\\frac{2}{9}\n=\\frac{9-8}{36}\n=\\frac{1}{36}.$$", "answer": "$$\\boxed{\\frac{1}{36}}$$", "id": "9607"}, {"introduction": "While direct computation is crucial, a deeper understanding of probability comes from interpreting the structure of a distribution. The very definition of a joint PDF's support can reveal the nature of the dependency between variables without requiring extensive calculations. This thought experiment [@problem_id:1369429] challenges you to deduce the sign of the covariance by analyzing how the domain of one variable is constrained by the value of the other, connecting the geometry of the support region to the statistical concept of correlation.", "problem": "Let $X$ and $Y$ be two continuous random variables representing the lifetimes of two electronic components. Their joint probability density function (PDF) is given by\n$$ f(x,y) = \\exp(-y) \\quad \\text{for } 0  x  y  \\infty $$\nand $f(x,y) = 0$ otherwise. The structure of the support region $0  x  y  \\infty$ implies a specific relationship between the two random variables.\n\nWithout performing the full computation of all required expected values for the covariance formula, analyze the dependence between $X$ and $Y$. Based on this analysis, what can you conclude about the sign of their covariance, $\\operatorname{Cov}(X,Y)$?\n\nA. The covariance is positive.\n\nB. The covariance is negative.\n\nC. The covariance is zero.\n\nD. The sign of the covariance cannot be determined without the full computation.", "solution": "We are given the joint density $f(x,y)=\\exp(-y)$ on the support $0xy\\infty$ and $f(x,y)=0$ otherwise. First, compute the marginal density of $Y$:\n$$\nf_{Y}(y)=\\int_{0}^{y} f(x,y)\\,dx=\\int_{0}^{y} \\exp(-y)\\,dx = y \\exp(-y), \\quad y0.\n$$\nThen the conditional density of $X$ given $Y=y$ is\n$$\nf_{X|Y}(x|y)=\\frac{f(x,y)}{f_{Y}(y)}=\\frac{\\exp(-y)}{y \\exp(-y)}=\\frac{1}{y}, \\quad 0xy,\n$$\nwhich is the uniform distribution on $(0,y)$. Hence the conditional expectation is\n$$\n\\mathbb{E}[X \\mid Y=y]=\\int_{0}^{y} x \\cdot \\frac{1}{y}\\,dx=\\frac{y}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}[X \\mid Y]=\\frac{1}{2}Y,\n$$\nwhich is an increasing linear function of $Y$. Using the law of total covariance,\n$$\n\\operatorname{Cov}(X,Y)=\\mathbb{E}\\big[\\operatorname{Cov}(X,Y \\mid Y)\\big]+\\operatorname{Cov}\\big(\\mathbb{E}[X \\mid Y],Y\\big).\n$$\nSince conditioning on $Y$ makes $Y$ constant, $\\operatorname{Cov}(X,Y \\mid Y)=0$, so\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{Cov}\\left(\\frac{1}{2}Y,\\,Y\\right)=\\frac{1}{2}\\operatorname{Var}(Y).\n$$\nBecause $Y$ has a nondegenerate distribution with density $y\\exp(-y)$ on $(0,\\infty)$, we have $\\operatorname{Var}(Y)0$. Therefore,\n$$\n\\operatorname{Cov}(X,Y)=\\frac{1}{2}\\operatorname{Var}(Y)0,\n$$\nso the covariance is positive. This conclusion follows from the monotone (in fact linear) increase of $\\mathbb{E}[X \\mid Y]$ in $Y$, without needing the full computation of all expectations.", "answer": "$$\\boxed{A}$$", "id": "1369429"}]}