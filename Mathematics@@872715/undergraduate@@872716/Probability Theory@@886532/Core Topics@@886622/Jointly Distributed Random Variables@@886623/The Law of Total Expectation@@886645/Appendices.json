{"hands_on_practices": [{"introduction": "This first exercise provides a classic application of the law of total expectation in a two-stage experiment. We first determine a parameter in a random process (the number on a die) and then use that parameter in a second random process (flipping a coin that many times). By conditioning on the outcome of the first stage, we can break a seemingly complex problem into simpler, manageable parts, a fundamental strategy in probabilistic modeling [@problem_id:1400510].", "problem": "A two-stage probabilistic game is defined as follows. In the first stage, a single fair six-sided die is rolled. Let the integer outcome of this roll be denoted by $N$. In the second stage, a fair coin is flipped $N$ times. Let $H$ denote the total number of heads observed from these coin flips. A player's score for the game, $S$, is calculated as the square of the number of heads, i.e., $S = H^2$.\n\nWhat is the expected value of the score, $E[S]$?", "solution": "Let $N$ be the die outcome, uniformly distributed on $\\{1,2,3,4,5,6\\}$ with $\\Pr(N=n)=\\frac{1}{6}$. Conditional on $N=n$, the number of heads $H$ is distributed as a binomial random variable with parameters $(n, p)$ where $p=\\frac{1}{2}$, i.e., $H \\mid N=n \\sim \\mathrm{Binomial}(n, \\frac{1}{2})$. The score is $S=H^{2}$, so we want $E[S]=E[H^{2}]$.\n\nBy the law of total expectation,\n$$\nE[H^{2}]=E\\!\\left(E[H^{2}\\mid N]\\right).\n$$\nFor a binomial random variable $X\\sim \\mathrm{Binomial}(n,p)$, the identity\n$$\nE[X^{2}]=\\operatorname{Var}(X)+\\left(E[X]\\right)^{2}=np(1-p)+n^{2}p^{2}\n$$\nholds. Applying this with $p=\\frac{1}{2}$ gives, for each fixed $n$,\n$$\nE[H^{2}\\mid N=n]=n\\cdot \\frac{1}{2}\\left(1-\\frac{1}{2}\\right)+n^{2}\\left(\\frac{1}{2}\\right)^{2}=\\frac{n}{4}+\\frac{n^{2}}{4}=\\frac{n+n^{2}}{4}.\n$$\nTherefore,\n$$\nE[S]=E[H^{2}]=E\\!\\left(\\frac{N+N^{2}}{4}\\right)=\\frac{1}{4}\\left(E[N]+E[N^{2}]\\right).\n$$\nSince $N$ is uniform on $\\{1,2,3,4,5,6\\}$,\n$$\nE[N]=\\frac{1}{6}\\sum_{n=1}^{6} n=\\frac{1}{6}\\cdot \\frac{6\\cdot 7}{2}=\\frac{7}{2},\n$$\nand\n$$\nE[N^{2}]=\\frac{1}{6}\\sum_{n=1}^{6} n^{2}=\\frac{1}{6}\\cdot \\frac{6\\cdot 7\\cdot 13}{6}=\\frac{91}{6}.\n$$\nHence,\n$$\nE[S]=\\frac{1}{4}\\left(\\frac{7}{2}+\\frac{91}{6}\\right)=\\frac{1}{4}\\cdot \\frac{112}{6}=\\frac{112}{24}=\\frac{14}{3}.\n$$", "answer": "$$\\boxed{\\frac{14}{3}}$$", "id": "1400510"}, {"introduction": "Building upon the idea of conditioning, this problem explores a scenario common in many scientific fields: a process that continues for a random number of steps. Here, we investigate the final state of a particle whose \"active\" lifetime is uncertain. Applying the law of total expectation by conditioning on the random stopping time, $N$, provides an elegant path to the solution for $E[P_N^2]$ [@problem_id:1400560].", "problem": "Consider a simplified model for the fluorescence intermittency, or \"blinking,\" of a single Quantum Dot (QD). The internal state of the QD, which influences its emission properties, is described by a one-dimensional, dimensionless quantity called the \"polarization state,\" denoted by $P$. We start with a QD in a reference state, $P_0 = 0$, at time $t=0$.\n\nAt discrete time steps $t=1, 2, 3, \\ldots$, the polarization state undergoes a random fluctuation. The change in the polarization state at step $k$, denoted by $\\Delta P_k$, is a random variable. These fluctuations, $\\{\\Delta P_k\\}_{k=1,2,...}$, are independent and identically distributed with a mean of zero, $E[\\Delta P_k] = 0$, and a finite variance of $\\text{Var}(\\Delta P_k) = \\sigma^2$.\n\nThe QD remains in an \"active\" (blinking) state for a random number of time steps, $N$. At each step, after the fluctuation occurs, there is a constant probability, $p$, that the QD will transition into a permanent \"dark\" state and cease all activity. The process stops at the end of the step in which this transition occurs. The total number of active steps, $N$, is therefore a random variable. Assume the state transition check happens for the first time at the end of step 1.\n\nThe final polarization state of the QD, just before it goes permanently dark, is the sum of all fluctuations up to that point: $P_N = \\sum_{k=1}^{N} \\Delta P_k$.\n\nDetermine the expected value of the square of the final polarization state, $E[P_N^2]$. Express your answer as a closed-form analytic expression in terms of $\\sigma^2$ and $p$.", "solution": "Let $P_{N}=\\sum_{k=1}^{N}\\Delta P_{k}$ with $\\{\\Delta P_{k}\\}$ i.i.d., $E[\\Delta P_{k}]=0$, and $\\text{Var}(\\Delta P_{k})=\\sigma^{2}$. At each step, after the fluctuation occurs, the process stops with probability $p$, independently of the fluctuations, so $N$ is geometric with parameter $p$ on $\\{1,2,\\ldots\\}$:\n$$\n\\Pr(N=n)=(1-p)^{n-1}p,\\quad n\\in\\{1,2,\\ldots\\}.\n$$\nCondition on $N=n$. Expanding the square gives\n$$\nP_{n}^{2}=\\left(\\sum_{k=1}^{n}\\Delta P_{k}\\right)^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\Delta P_{i}\\Delta P_{j}.\n$$\nTaking expectation conditional on $N=n$ and using independence and zero mean,\n$$\nE[P_{n}^{2}\\mid N=n]=\\sum_{i=1}^{n}E[\\Delta P_{i}^{2}]+\\sum_{\\substack{i,j=1\\\\ i\\neq j}}^{n}E[\\Delta P_{i}\\Delta P_{j}]=n\\sigma^{2}+0=n\\sigma^{2}.\n$$\nBy the law of total expectation,\n$$\nE[P_{N}^{2}]=E\\big[E[P_{N}^{2}\\mid N]\\big]=\\sigma^{2}E[N].\n$$\nFor a geometric random variable with parameter $p$ supported on $\\{1,2,\\ldots\\}$,\n$$\nE[N]=\\sum_{n=1}^{\\infty}n(1-p)^{\\,n-1}p=p\\sum_{n=1}^{\\infty}n(1-p)^{\\,n-1}.\n$$\nUsing the identity $\\sum_{n=1}^{\\infty}nr^{n-1}=\\frac{1}{(1-r)^{2}}$ for $|r|1$ with $r=1-p$, we obtain\n$$\nE[N]=p\\cdot\\frac{1}{(1-(1-p))^{2}}=p\\cdot\\frac{1}{p^{2}}=\\frac{1}{p}.\n$$\nTherefore,\n$$\nE[P_{N}^{2}]=\\sigma^{2}E[N]=\\frac{\\sigma^{2}}{p}.\n$$", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{p}}$$", "id": "1400560"}, {"introduction": "This final practice problem showcases a particularly powerful and elegant use of the law of total expectation for analyzing sequential processes. By defining states that represent our progress toward observing a specific pattern, we can create a system of equations to find the expected time to success. This state-based conditioning method is a cornerstone for solving problems involving stopping times in areas like computer science and genetics [@problem_id:1400547].", "problem": "An engineered microbe is used in a bioreactor. In each cycle, it can be in one of two states, which we label Heads (H) or Tails (T). The state in any given cycle is independent of the previous cycles. The probability of the microbe being in the Heads state is $p$, where $0  p  1$, and consequently, the probability of it being in the Tails state is $1-p$. A monitoring process records the state of the microbe at the end of each cycle.\n\nWe are interested in the first time a specific sequence of states, Heads-Tails-Heads-Tails (HTHT), is observed. Assuming the process starts at cycle 1, what is the expected number of cycles required to observe the pattern HTHT for the first time?\n\nProvide your answer as a single closed-form analytic expression in terms of $p$.", "solution": "Let $p=\\mathbb{P}(H)$ and $q=1-p=\\mathbb{P}(T)$. We consider the pattern $P=$ HTHT of length $4$. Define states $S_{k}$ for $k\\in\\{0,1,2,3,4\\}$ where $S_{k}$ denotes that the current longest suffix of the observed sequence that matches a prefix of $P$ has length $k$; $S_{4}$ is absorbing (pattern first observed). Let $E_{k}$ be the expected additional number of cycles to reach $S_{4}$ starting from $S_{k}$. We want $E_{0}$.\n\nUsing the standard failure-function (longest prefix that is a suffix) logic for $P=$ HTHT, the transitions are:\n- From $S_{0}$: on $H$ go to $S_{1}$; on $T$ stay in $S_{0}$.\n- From $S_{1}$: on $H$ stay in $S_{1}$; on $T$ go to $S_{2}$.\n- From $S_{2}$: on $H$ go to $S_{3}$; on $T$ go to $S_{0}$.\n- From $S_{3}$: on $H$ go to $S_{1}$; on $T$ go to $S_{4}$.\n- $S_{4}$ is absorbing.\n\nThus the expectation equations are\n$$\nE_{0}=1+pE_{1}+qE_{0},\n$$\n$$\nE_{1}=1+pE_{1}+qE_{2},\n$$\n$$\nE_{2}=1+pE_{3}+qE_{0},\n$$\n$$\nE_{3}=1+pE_{1}+qE_{4},\\quad E_{4}=0.\n$$\n\nSimplify step by step. From the first equation,\n$$\nE_{0}-qE_{0}=1+pE_{1}\\;\\Rightarrow\\;pE_{0}=1+pE_{1}\\;\\Rightarrow\\;E_{0}=\\frac{1}{p}+E_{1}.\n$$\nFrom the second,\n$$\nE_{1}-pE_{1}=1+qE_{2}\\;\\Rightarrow\\;qE_{1}=1+qE_{2}\\;\\Rightarrow\\;E_{1}=\\frac{1}{q}+E_{2}.\n$$\nFrom the fourth,\n$$\nE_{3}=1+pE_{1}.\n$$\nSubstitute $E_{3}$ into the third:\n$$\nE_{2}=1+pE_{3}+qE_{0}=1+p(1+pE_{1})+qE_{0}=1+p+p^{2}E_{1}+qE_{0}.\n$$\nNow substitute $E_{0}=\\frac{1}{p}+E_{1}$ into this:\n$$\nE_{2}=1+p+p^{2}E_{1}+q\\left(\\frac{1}{p}+E_{1}\\right)=1+p+\\frac{q}{p}+(p^{2}+q)E_{1}.\n$$\nUse $E_{1}=\\frac{1}{q}+E_{2}$ to get\n$$\nE_{1}=\\frac{1}{q}+1+p+\\frac{q}{p}+(p^{2}+q)E_{1},\n$$\nso\n$$\nE_{1}\\bigl(1-p^{2}-q\\bigr)=\\frac{1}{q}+1+p+\\frac{q}{p}.\n$$\nSince $1-p^{2}-q=pq$, we have\n$$\npq\\,E_{1}=\\frac{1}{q}+1+p+\\frac{q}{p}.\n$$\nTherefore\n$$\nE_{1}=\\frac{\\frac{1}{q}+1+p+\\frac{q}{p}}{pq}.\n$$\nWe need $E_{0}=\\frac{1}{p}+E_{1}$, so\n$$\nE_{0}=\\frac{1}{p}+\\frac{\\frac{1}{q}+1+p+\\frac{q}{p}}{pq}.\n$$\nBring to a common denominator and simplify. Writing everything over $p^{2}q^{2}$,\n$$\nE_{0}=\\frac{p q^{2}}{p^{2}q^{2}}+\\frac{p+p q+p^{2}q+q^{2}}{p^{2}q^{2}}=\\frac{p q^{2}+p+p q+p^{2}q+q^{2}}{p^{2}q^{2}}.\n$$\nThe numerator simplifies as\n$$\np q^{2}+p+p q+p^{2}q+q^{2}=1+p-p^{2}=1+pq,\n$$\nusing $q=1-p$. Hence\n$$\nE_{0}=\\frac{1+pq}{p^{2}q^{2}}=\\frac{1+p-p^{2}}{p^{2}(1-p)^{2}}.\n$$\nThis is the expected number of cycles to first observe HTHT.", "answer": "$$\\boxed{\\frac{1+p-p^{2}}{p^{2}\\left(1-p\\right)^{2}}}$$", "id": "1400547"}]}