{"hands_on_practices": [{"introduction": "The Skorokhod Representation Theorem can seem abstract, but its core mechanism is quite concrete. This first practice makes the theorem tangible by walking you through the standard construction for a simple sequence of discrete random variables converging to a constant. By applying the quantile function (or inverse transform) method, you will see precisely how convergence in distribution is converted into the much stronger mode of almost sure convergence [@problem_id:1388075].", "problem": "Consider a sequence of discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each integer $n \\ge 1$, the random variable $X_n$ is defined by the probabilities $P(X_n = 1/n) = 1/2$ and $P(X_n = -1/n) = 1/2$. This sequence converges in distribution to a constant random variable $X=0$.\n\nSkorokhod's representation theorem provides a powerful method for converting convergence in distribution into almost sure convergence. It states that if a sequence of real-valued random variables $\\{X_n\\}$ converges in distribution to a random variable $X$, then there exists a single probability space $(\\Omega, \\mathcal{F}, P)$ and a new sequence of random variables $\\{Y_n\\}$ and a random variable $Y$ defined on this space with the following properties:\n1. For each $n$, $Y_n$ has the same distribution as $X_n$.\n2. $Y$ has the same distribution as $X$.\n3. The sequence $\\{Y_n\\}$ converges to $Y$ almost surely.\n\nYour task is to explicitly construct the sequence $\\{Y_n\\}$ for the given $\\{X_n\\}$. We will use the standard construction where the common probability space is the unit interval with the Lebesgue measure, i.e., $(\\Omega, \\mathcal{F}, P) = ([0,1], \\mathcal{B}([0,1]), \\lambda)$, where $\\mathcal{B}([0,1])$ is the Borel sigma-algebra and $\\lambda$ is the Lebesgue measure. On this space, each random variable $Y_n$ can be represented as a measurable function $Y_n(u)$ for $u \\in [0,1]$.\n\nDetermine the explicit functional form of $Y_n(u)$ for $u \\in (0,1]$. Your answer should be expressed as a piecewise function of $u$ and $n$.", "solution": "We are given the sequence $\\{X_{n}\\}_{n=1}^{\\infty}$ with $P(X_{n}=\\frac{1}{n})=\\frac{1}{2}$ and $P(X_{n}=-\\frac{1}{n})=\\frac{1}{2}$, and $X=0$ as the limit in distribution. To construct $\\{Y_{n}\\}$ on $([0,1],\\mathcal{B}([0,1]),\\lambda)$ so that $Y_{n}\\stackrel{d}{=}X_{n}$ for each $n$, $Y\\stackrel{d}{=}X$, and $Y_{n}\\to Y$ almost surely, we use the inverse transform method.\n\nLet $U$ denote the identity random variable on $([0,1],\\lambda)$, i.e., $U(u)=u$. For each $n$, let $F_{n}$ be the cumulative distribution function of $X_{n}$. Since $X_{n}$ has mass $\\frac{1}{2}$ at $-\\frac{1}{n}$ and $\\frac{1}{2}$ at $\\frac{1}{n}$, we have\n$$\nF_{n}(x)=\n\\begin{cases}\n0, & x<-\\frac{1}{n},\\\\\n\\frac{1}{2}, & -\\frac{1}{n}\\le x<\\frac{1}{n},\\\\\n1, & x\\ge \\frac{1}{n}.\n\\end{cases}\n$$\nDefine the right-continuous generalized inverse (quantile) function\n$$\nF_{n}^{-1}(u)=\\inf\\{x\\in\\mathbb{R}:F_{n}(x)\\ge u\\}, \\quad u\\in(0,1].\n$$\nFrom the form of $F_{n}$, we compute\n$$\nF_{n}^{-1}(u)=\n\\begin{cases}\n-\\frac{1}{n}, & 0<u\\le \\frac{1}{2},\\\\\n\\frac{1}{n}, & \\frac{1}{2}<u\\le 1.\n\\end{cases}\n$$\nSet $Y_{n}(u)=F_{n}^{-1}(u)$ for $u\\in(0,1]$. Then $Y_{n}$ has the same distribution as $X_{n}$ because $U$ is uniform on $(0,1]$ and the inverse transform preserves the distribution. Moreover, for every fixed $u\\in(0,1]\\setminus\\{\\frac{1}{2}\\}$, we have $|Y_{n}(u)|=\\frac{1}{n}\\to 0$, hence $Y_{n}(u)\\to 0$. Since the set $\\{\\frac{1}{2}\\}$ has Lebesgue measure zero, it follows that $Y_{n}\\to 0$ almost surely. Taking $Y(u)\\equiv 0$ yields $Y\\stackrel{d}{=}X$ and completes the Skorokhod representation.\n\nTherefore, the explicit functional form for $u\\in(0,1]$ is\n$$\nY_{n}(u)=\n\\begin{cases}\n-\\frac{1}{n}, & 0<u\\le \\frac{1}{2},\\\\\n\\frac{1}{n}, & \\frac{1}{2}<u\\le 1.\n\\end{cases}\n$$", "answer": "$$\\boxed{Y_{n}(u)=\\begin{cases}-\\frac{1}{n}, & 0<u\\le \\frac{1}{2},\\\\[4pt]\\frac{1}{n}, & \\frac{1}{2}<u\\le 1.\\end{cases}}$$", "id": "1388075"}, {"introduction": "A crucial part of Skorokhod's theorem is the creation of a *new* probability space. This practice explores why this is not just a technical detail but a fundamental necessity. You will analyze a specific sequence of random variables that converges in distribution but demonstrably fails to converge almost surely on its original space, highlighting the very problem that the theorem so elegantly solves [@problem_id:1388092].", "problem": "Consider the probability space $(\\Omega, \\mathcal{F}, P)$, where the sample space is the interval $\\Omega = [0, 1)$, the $\\sigma$-algebra $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $[0, 1)$, and the probability measure $P$ is the standard Lebesgue measure.\n\nWe define a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ on this space by the formula:\n$$X_n(\\omega) = \\cos(2^n \\pi \\omega)$$\nfor each $\\omega \\in [0, 1)$.\n\nFurther, let $X$ be a random variable, defined on some (possibly different) probability space, which has the arcsine distribution. The probability density function (PDF) of $X$ is given by:\n$$f_X(x) = \\begin{cases} \\frac{1}{\\pi \\sqrt{1-x^2}} & \\text{if } x \\in (-1, 1) \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\nWhich of the following statements correctly describes the convergence properties of the sequence of random variables $\\{X_n\\}$?\n\nA. The sequence $\\{X_n\\}$ converges almost surely to a constant value, but it does not converge in distribution to the random variable $X$.\n\nB. The sequence $\\{X_n\\}$ converges in distribution to the random variable $X$, but it does not converge almost surely.\n\nC. The sequence $\\{X_n\\}$ converges almost surely to a non-constant random variable, and it also converges in distribution to the random variable $X$.\n\nD. The sequence $\\{X_n\\}$ converges neither in distribution to $X$ nor almost surely.\n\nE. The sequence $\\{X_n\\}$ converges almost surely to a non-constant random variable, but it does not converge in distribution to the random variable $X$.", "solution": "We are given the probability space with $\\Omega=[0,1)$, $\\mathcal{F}$ the Borel $\\sigma$-algebra, and $P$ the Lebesgue measure. For each $n \\in \\mathbb{N}$,\n$$\nX_{n}(\\omega)=\\cos(2^{n}\\pi\\omega)=\\cos\\big(2\\pi\\cdot 2^{n-1}\\omega\\big).\n$$\nDefine the doubling map $T:[0,1)\\to[0,1)$ by $T(\\omega)=\\{2\\omega\\}$ (fractional part), and set $U_{n}(\\omega)=T^{n-1}(\\omega)=\\{2^{n-1}\\omega\\}$. Then\n$$\nX_{n}(\\omega)=\\cos\\big(2\\pi U_{n}(\\omega)\\big).\n$$\n\nFirst, we determine the distribution of $X_{n}$. The map $T$ is measure-preserving for Lebesgue measure, hence for each fixed $n$, $U_{n}$ is uniformly distributed on $[0,1)$ under $P$. Let $U$ be a $\\operatorname{Uniform}[0,1)$ random variable. Then $X_{n}\\stackrel{d}{=}\\cos(2\\pi U)$. Write $\\Theta=2\\pi U$, so $\\Theta$ is $\\operatorname{Uniform}[0,2\\pi)$. Let $W=\\cos\\Theta$. For $x\\in(-1,1)$, the preimages under $\\cos$ on $[0,2\\pi)$ are $\\theta_{1}(x)=\\arccos(x)$ and $\\theta_{2}(x)=2\\pi-\\arccos(x)$. Using the change-of-variables formula for densities with monotone branches, if $r_{\\Theta}(\\theta)=(2\\pi)^{-1}$ and $g(\\theta)=\\cos\\theta$ with $g'(\\theta)=-\\sin\\theta$, then\n$$\nf_{W}(x)=\\sum_{i=1}^{2}\\frac{r_{\\Theta}(\\theta_{i}(x))}{|g'(\\theta_{i}(x))|}=\\frac{1}{2\\pi}\\left(\\frac{1}{\\sqrt{1-x^{2}}}+\\frac{1}{\\sqrt{1-x^{2}}}\\right)=\\frac{1}{\\pi\\sqrt{1-x^{2}}}\n$$\nfor $x\\in(-1,1)$, and $f_{W}(x)=0$ otherwise. Thus for every $n$, $X_{n}$ has the arcsine distribution with density $f_{X}(x)=(\\pi\\sqrt{1-x^{2}})^{-1}$ on $(-1,1)$, which is exactly the law of $X$. Consequently,\n$$\nX_{n}\\stackrel{d}{=}X\\quad\\text{for all }n,\n$$\nand therefore $X_{n}\\xrightarrow{d}X$.\n\nNext, we analyze almost sure convergence. Write $X_{n}(\\omega)=\\cos(2\\pi U_{n}(\\omega))$ with $U_{n}=T^{n-1}(\\omega)$. The map $T$ is measure-preserving and ergodic with respect to Lebesgue measure. Fix $\\delta\\in(0,1/4)$ and define the sets\n$$\nA_{+}=[0,\\delta),\\qquad A_{-}=\\left(\\frac{1}{2}-\\delta,\\frac{1}{2}+\\delta\\right)\\cap[0,1).\n$$\nBoth have positive Lebesgue measure. By Birkhoffâ€™s ergodic theorem applied to the integrable indicator functions $1_{A_{+}}$ and $1_{A_{-}}$, for $P$-almost every $\\omega$,\n$$\n\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{n=1}^{N}1_{A_{\\pm}}(U_{n}(\\omega))=m(A_{\\pm})>0,\n$$\nwhere $m$ denotes Lebesgue measure. Hence, for almost every $\\omega$, the sets $\\{n:U_{n}(\\omega)\\in A_{+}\\}$ and $\\{n:U_{n}(\\omega)\\in A_{-}\\}$ are infinite.\n\nFor $u\\in A_{+}$, we have $2\\pi u\\in[0,2\\pi\\delta)$, so $|\\cos(2\\pi u)-1|\\leq\\max_{t\\in[0,2\\pi\\delta]}|1-\\cos t|$, which can be made arbitrarily small by taking $\\delta$ small. Thus along infinitely many $n$ with $U_{n}(\\omega)\\in A_{+}$, $X_{n}(\\omega)$ is arbitrarily close to $1$. For $u\\in A_{-}$, write $u=\\frac{1}{2}+v$ with $|v|<\\delta$, giving $\\cos(2\\pi u)=\\cos(\\pi+2\\pi v)=-\\cos(2\\pi v)$, hence $|\\cos(2\\pi u)+1|=|1-\\cos(2\\pi v)|\\leq\\max_{t\\in[0,2\\pi\\delta]}|1-\\cos t|$. Thus along infinitely many $n$ with $U_{n}(\\omega)\\in A_{-}$, $X_{n}(\\omega)$ is arbitrarily close to $-1$. Therefore, for almost every $\\omega$, there exist subsequences $n_{k}$ and $m_{k}$ such that $X_{n_{k}}(\\omega)\\to 1$ and $X_{m_{k}}(\\omega)\\to-1$, implying $X_{n}(\\omega)$ does not converge.\n\nSince the set of exceptional $\\omega$ where convergence might occur has Lebesgue measure zero (e.g., certain eventually periodic orbits), we conclude that $X_{n}$ does not converge almost surely, while it converges in distribution to $X$. Hence the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1388092"}, {"introduction": "The standard Skorokhod construction creates a \"coupling\" where the new random variables are all functions of a single underlying uniform random variable, which forces the almost sure convergence. But what is the trade-off for this powerful result? This exercise has you investigate the property of independence. Starting with a sequence of *independent* random variables, you will use the quantile construction and then prove that the resulting coupled sequence is, in fact, *dependent* [@problem_id:1388054].", "problem": "Consider a sequence of independent random variables $\\{X_n\\}_{n \\ge 1}$, where each $X_n$ follows a Bernoulli distribution with a success probability $p_n$. The parameter $p_n$ is given by the formula $p_n = \\frac{1}{2} - \\frac{1}{n+3}$ for $n \\ge 1$. This sequence of random variables converges in distribution to a random variable $X$.\n\nSkorokhod's representation theorem allows us to construct a different sequence of random variables $\\{Y_n\\}_{n \\ge 1}$ on a single common probability space. This is achieved using the quantile function method. Let $F_n$ be the Cumulative Distribution Function (CDF) for the random variable $X_n$. The corresponding quantile function is defined as $F_n^{-1}(u) = \\inf\\{x \\in \\mathbb{R} : F_n(x) \\ge u\\}$ for $u \\in (0,1)$. The new random variables are then defined as $Y_n = F_n^{-1}(U)$, where $U$ is a single random variable uniformly distributed on the interval $[0,1]$. By this construction, each $Y_n$ has the same distribution as the corresponding $X_n$.\n\nYour task is to analyze the relationship between the first two random variables in this new sequence, $Y_1$ and $Y_2$. Calculate the value of the expression $\\mathbb{E}[Y_1 Y_2] - \\mathbb{E}[Y_1]\\mathbb{E}[Y_2]$. Present your final answer as a fraction in simplest form.", "solution": "For a Bernoulli random variable with success probability $p_{n}$, the CDF is\n$$\nF_{n}(x)=\\begin{cases}\n0, & x<0,\\\\\n1-p_{n}, & 0\\le x<1,\\\\\n1, & x\\ge 1.\n\\end{cases}\n$$\nThe quantile function $F_{n}^{-1}(u)=\\inf\\{x:F_{n}(x)\\ge u\\}$ satisfies\n$$\nF_{n}^{-1}(u)=\\begin{cases}\n0, & 0<u\\le 1-p_{n},\\\\\n1, & 1-p_{n}<u<1.\n\\end{cases}\n$$\nHence, with $U\\sim \\text{Unif}[0,1]$, we have\n$$\nY_{n}=\\mathbf{1}_{\\{U>1-p_{n}\\}}.\n$$\nTherefore, for each $n$,\n$$\n\\mathbb{E}[Y_{n}]=\\mathbb{P}(U>1-p_{n})=p_{n}.\n$$\nIn particular,\n$$\np_{1}=\\frac{1}{2}-\\frac{1}{1+3}=\\frac{1}{4},\\qquad p_{2}=\\frac{1}{2}-\\frac{1}{2+3}=\\frac{3}{10},\n$$\nso\n$$\n\\mathbb{E}[Y_{1}]=\\frac{1}{4},\\qquad \\mathbb{E}[Y_{2}]=\\frac{3}{10}.\n$$\nFor the product,\n$$\nY_{1}Y_{2}=\\mathbf{1}_{\\{U>1-p_{1}\\}}\\mathbf{1}_{\\{U>1-p_{2}\\}}=\\mathbf{1}_{\\{U>\\max(1-p_{1},\\,1-p_{2})\\}},\n$$\nso\n$$\n\\mathbb{E}[Y_{1}Y_{2}]=\\mathbb{P}\\big(U>\\max(1-p_{1},\\,1-p_{2})\\big)\n=1-\\max(1-p_{1},\\,1-p_{2})=\\min(p_{1},p_{2}).\n$$\nSince $p_{1}=\\frac{1}{4}<\\frac{3}{10}=p_{2}$, we obtain\n$$\n\\mathbb{E}[Y_{1}Y_{2}]=\\frac{1}{4}.\n$$\nThus,\n$$\n\\mathbb{E}[Y_{1}Y_{2}]-\\mathbb{E}[Y_{1}]\\mathbb{E}[Y_{2}]\n=\\frac{1}{4}-\\left(\\frac{1}{4}\\right)\\left(\\frac{3}{10}\\right)\n=\\frac{1}{4}-\\frac{3}{40}\n=\\frac{7}{40}.\n$$", "answer": "$$\\boxed{\\frac{7}{40}}$$", "id": "1388054"}]}