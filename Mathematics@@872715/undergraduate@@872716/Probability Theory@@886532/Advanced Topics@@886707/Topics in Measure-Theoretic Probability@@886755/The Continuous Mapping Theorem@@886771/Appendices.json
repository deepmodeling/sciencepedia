{"hands_on_practices": [{"introduction": "This first exercise provides a practical application of the Continuous Mapping Theorem in the context of statistical estimation. We will analyze a hypothetical stability metric from a data center to see how the theorem guarantees the consistency of an estimator. This practice demonstrates a common and fundamental workflow: first, using a law of large numbers to show that a simple average converges to a true value, and then applying the CMT to prove that a continuous transformation of that average also converges [@problem_id:1395918].", "problem": "A data center models the number of incoming requests, $X_t$, over a time interval of duration $t$ minutes as a Poisson random variable with a mean of $t$. To monitor long-term server load stability, a system administrator defines a sequence of measurements. For each positive integer $n$, they observe the total number of requests, $X_n$, in an interval of $n$ minutes. Based on this observation, they compute an experimental stability metric, $M_n$, defined as the cube root of the average rate of requests over that interval. The average rate is calculated as the total number of requests $X_n$ divided by the duration of the interval, $n$.\n\nDetermine the value to which the stability metric $M_n$ converges in probability as $n$ tends to infinity.", "solution": "Let $\\{X_{t}: t \\geq 0\\}$ be such that for each $t$, $X_{t} \\sim \\operatorname{Poisson}(t)$. In particular, for each positive integer $n$, we have $X_{n} \\sim \\operatorname{Poisson}(n)$, so\n$$\n\\mathbb{E}[X_{n}] = n, \\qquad \\operatorname{Var}(X_{n}) = n.\n$$\nDefine the average rate $R_{n} = \\frac{X_{n}}{n}$. Then\n$$\n\\mathbb{E}[R_{n}] = \\frac{\\mathbb{E}[X_{n}]}{n} = 1, \\qquad \\operatorname{Var}(R_{n}) = \\frac{\\operatorname{Var}(X_{n})}{n^{2}} = \\frac{1}{n}.\n$$\nBy Chebyshev's inequality, for any $\\epsilon > 0$,\n$$\n\\mathbb{P}\\Big(|R_{n} - 1| > \\epsilon\\Big) \\leq \\frac{\\operatorname{Var}(R_{n})}{\\epsilon^{2}} = \\frac{1}{n \\epsilon^{2}} \\xrightarrow[n \\to \\infty]{} 0,\n$$\nso $R_{n} \\xrightarrow{P} 1$.\n\nThe stability metric is $M_{n} = \\left(\\frac{X_{n}}{n}\\right)^{1/3} = R_{n}^{1/3}$. The function $f(x) = x^{1/3}$ is continuous at $x = 1$. By the continuous mapping theorem, if $R_{n} \\xrightarrow{P} 1$, then\n$$\nM_{n} = f(R_{n}) \\xrightarrow{P} f(1) = 1.\n$$\nTherefore, the stability metric converges in probability to $1$.", "answer": "$$\\boxed{1}$$", "id": "1395918"}, {"introduction": "Moving beyond convergence to a single constant, this problem illustrates a more powerful application of the Continuous Mapping Theorem in conjunction with the Central Limit Theorem. You will determine the limiting *distribution* of a sequence of random variables, a critical task in statistical inference for constructing confidence intervals and performing hypothesis tests. This exercise shows how the CMT acts as a bridge, allowing us to derive the asymptotic distribution of a complex statistic from the known limiting distribution of a simpler one [@problem_id:1395931].", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of Independent and Identically Distributed (i.i.d.) random variables, each following a Uniform distribution on the interval $(0, 1)$. Let $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean.\n\nConsider a new sequence of random variables defined by\n$$ Y_n = \\exp\\left(\\sqrt{n}\\left(\\bar{X}_n - \\frac{1}{2}\\right)\\right) $$\nAs $n \\to \\infty$, the sequence of random variables $Y_n$ converges in distribution to a specific limiting probability distribution. Which of the following correctly describes this limiting distribution?\n\nA. A Normal distribution with mean 1 and variance $\\frac{1}{12}$.\n\nB. A Lognormal distribution with parameters $\\mu=0$ and $\\sigma^2=\\frac{1}{12}$.\n\nC. An Exponential distribution with rate parameter $\\lambda = \\sqrt{12}$.\n\nD. A Lognormal distribution with parameters $\\mu=\\frac{1}{2}$ and $\\sigma^2=\\frac{1}{12}$.\n\nE. A degenerate distribution at the constant value 1.", "solution": "Our goal is to find the limiting distribution of the random variable $Y_n = \\exp\\left(\\sqrt{n}\\left(\\bar{X}_n - \\frac{1}{2}\\right)\\right)$ as $n \\to \\infty$. The sequence $X_1, X_2, \\ldots$ consists of i.i.d. random variables from a Uniform(0, 1) distribution.\n\nFirst, we need to determine the mean and variance of a single random variable $X_i \\sim \\text{Uniform}(0, 1)$.\nThe mean (expected value) is given by:\n$$ E[X_i] = \\mu = \\frac{0+1}{2} = \\frac{1}{2} $$\nThe variance is given by:\n$$ V[X_i] = \\sigma^2 = \\frac{(1-0)^2}{12} = \\frac{1}{12} $$\n\nNext, we apply the Central Limit Theorem (CLT) to the sample mean $\\bar{X}_n$. The CLT states that for a sequence of i.i.d. random variables with finite mean $\\mu$ and finite variance $\\sigma^2$, the distribution of the standardized sample mean converges to a standard normal distribution. A more convenient form for our purpose is that the random variable $\\sqrt{n}(\\bar{X}_n - \\mu)$ converges in distribution to a normal distribution with mean 0 and variance $\\sigma^2$.\nSymbolically, as $n \\to \\infty$:\n$$ \\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2) $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution.\n\nSubstituting the values of $\\mu$ and $\\sigma^2$ we calculated for the Uniform(0, 1) distribution:\n$$ \\sqrt{n}\\left(\\bar{X}_n - \\frac{1}{2}\\right) \\xrightarrow{d} N\\left(0, \\frac{1}{12}\\right) $$\nLet's define a sequence of random variables $Z_n = \\sqrt{n}\\left(\\bar{X}_n - \\frac{1}{2}\\right)$. The CLT tells us that $Z_n$ converges in distribution to a random variable $Z \\sim N\\left(0, \\frac{1}{12}\\right)$.\n\nThe random variable we are interested in is $Y_n = \\exp(Z_n)$. To find the limiting distribution of $Y_n$, we can use the Continuous Mapping Theorem (CMT). The CMT states that if a sequence of random variables $Z_n$ converges in distribution to a random variable $Z$ (i.e., $Z_n \\xrightarrow{d} Z$), and if $g$ is a continuous function, then the sequence of random variables $g(Z_n)$ converges in distribution to the random variable $g(Z)$ (i.e., $g(Z_n) \\xrightarrow{d} g(Z)$).\n\nIn our case, the function is $g(x) = \\exp(x) = e^x$. This function is continuous for all real numbers $x$.\nApplying the CMT, we have:\n$$ Y_n = \\exp(Z_n) = g(Z_n) \\xrightarrow{d} g(Z) = \\exp(Z) $$\nwhere $Z \\sim N\\left(0, \\frac{1}{12}\\right)$.\n\nThe final step is to identify the distribution of $\\exp(Z)$. By definition, if a random variable $Z$ is normally distributed with mean $\\mu_Z$ and variance $\\sigma_Z^2$, i.e., $Z \\sim N(\\mu_Z, \\sigma_Z^2)$, then the random variable $W = \\exp(Z)$ follows a log-normal distribution. The parameters of this log-normal distribution are the mean and variance of the underlying normal random variable $Z$. This is often denoted as $W \\sim \\text{Lognormal}(\\mu_Z, \\sigma_Z^2)$.\n\nIn our problem, the limiting random variable $Z$ has a normal distribution with mean $\\mu_Z = 0$ and variance $\\sigma_Z^2 = \\frac{1}{12}$.\nTherefore, the limiting distribution of $Y_n$ is a Lognormal distribution with parameters $\\mu = 0$ and $\\sigma^2 = \\frac{1}{12}$.\n\nComparing this result with the given choices:\nA. A Normal distribution with mean 1 and variance $\\frac{1}{12}$. (Incorrect)\nB. A Lognormal distribution with parameters $\\mu=0$ and $\\sigma^2=\\frac{1}{12}$. (Correct)\nC. An Exponential distribution with rate parameter $\\lambda = \\sqrt{12}$. (Incorrect)\nD. A Lognormal distribution with parameters $\\mu=\\frac{1}{2}$ and $\\sigma^2=\\frac{1}{12}$. (Incorrect)\nE. A degenerate distribution at the constant value 1. (Incorrect)\n\nThe correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1395931"}, {"introduction": "This final practice expands our scope to the multivariate domain, showcasing the versatility of the Continuous Mapping Theorem when dealing with more than one sequence of random variables. By considering the limiting behavior of an angle derived from two independent sequences, you will see how the theorem handles mappings from a plane to a line. The problem also touches upon an important extension where the mapping function is not continuous everywhere, yet the theorem still holds, leading to a surprisingly elegant and simple limiting distribution [@problem_id:1395925].", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ and $\\{Y_n\\}_{n=1}^{\\infty}$ be two sequences of random variables, where for any fixed $n$, the variables $X_n$ and $Y_n$ are independent. Suppose that as $n \\to \\infty$, both sequences converge in distribution, denoted by $\\xrightarrow{d}$, to independent standard normal random variables. Specifically, $X_n \\xrightarrow{d} X$ and $Y_n \\xrightarrow{d} Y$, where $X \\sim \\mathcal{N}(0, 1)$ and $Y \\sim \\mathcal{N}(0, 1)$ are independent.\n\nConsider a new sequence of random variables $\\Theta_n$ defined by the transformation $\\Theta_n = \\arctan(Y_n / X_n)$, where the arctan function's range is the interval $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$ radians.\n\nWhich of the following options correctly describes the limiting distribution of $\\Theta_n$ as $n \\to \\infty$?\n\nA. A standard Normal distribution, $\\mathcal{N}(0,1)$.\n\nB. A standard Cauchy distribution, with a probability density function $f(\\theta) = \\frac{1}{\\pi(1+\\theta^2)}$ for $\\theta \\in (-\\infty, \\infty)$.\n\nC. A Uniform distribution on the interval $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$.\n\nD. A distribution with a probability density function $f(\\theta) = \\frac{1}{2}\\cos(\\theta)$ for $\\theta \\in (-\\frac{\\pi}{2}, \\frac{\\pi}{2})$.\n\nE. A triangular distribution on the interval $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$ with its peak at $\\theta=0$.", "solution": "We are given random variables $X_{n}$ and $Y_{n}$ such that for each $n$, $X_{n}$ and $Y_{n}$ are independent, and $X_{n} \\xrightarrow{d} X$, $Y_{n} \\xrightarrow{d} Y$ as $n \\to \\infty$, where $X \\sim \\mathcal{N}(0,1)$ and $Y \\sim \\mathcal{N}(0,1)$ are independent. Define $\\Theta_{n} = \\arctan\\!\\left(\\frac{Y_{n}}{X_{n}}\\right)$ with range $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$.\n\nStep 1: Joint convergence. Since for each $n$ the variables $X_{n}$ and $Y_{n}$ are independent, the joint characteristic function factorizes:\n$$\n\\varphi_{(X_{n},Y_{n})}(t,s) \\;=\\; \\varphi_{X_{n}}(t)\\,\\varphi_{Y_{n}}(s).\n$$\nBy the assumed marginal convergence, $\\varphi_{X_{n}}(t) \\to \\varphi_{X}(t)$ and $\\varphi_{Y_{n}}(s) \\to \\varphi_{Y}(s)$ pointwise, hence\n$$\n\\varphi_{(X_{n},Y_{n})}(t,s) \\;\\to\\; \\varphi_{X}(t)\\,\\varphi_{Y}(s) \\;=\\; \\varphi_{(X,Y)}(t,s),\n$$\nwhere the last equality uses the independence of $X$ and $Y$. Therefore $(X_{n},Y_{n}) \\xrightarrow{d} (X,Y)$.\n\nStep 2: Mapping to the ratio and to the angle. Consider the mapping $m:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $m(x,y)=y/x$ on the set $\\{x\\neq 0\\}$. The function $m$ is continuous on $\\{x\\neq 0\\}$ and its discontinuity set $\\{x=0\\}$ has probability zero under the limit law because $\\mathbb{P}(X=0)=0$ for $X \\sim \\mathcal{N}(0,1)$. By the continuous mapping theorem (version allowing discontinuities on a null set under the limit distribution),\n$$\n\\frac{Y_{n}}{X_{n}} \\;\\xrightarrow{d}\\; \\frac{Y}{X}.\n$$\nNow apply the continuous mapping theorem again with the continuous function $g:\\mathbb{R}\\to(-\\frac{\\pi}{2},\\frac{\\pi}{2})$ given by $g(t)=\\arctan(t)$ to obtain\n$$\n\\Theta_{n} \\;=\\; \\arctan\\!\\left(\\frac{Y_{n}}{X_{n}}\\right) \\;\\xrightarrow{d}\\; \\Theta \\;:=\\; \\arctan\\!\\left(\\frac{Y}{X}\\right).\n$$\n\nStep 3: Identify the law of the limit $\\Theta$. There are two equivalent standard routes.\n\nRoute A (via the Cauchy ratio): For independent $X,Y \\sim \\mathcal{N}(0,1)$, the ratio $T:=Y/X$ has the standard Cauchy distribution with density $f_{T}(t)=\\frac{1}{\\pi(1+t^{2})}$ for $t\\in\\mathbb{R}$. Since $\\Theta=\\arctan(T)$ and $\\arctan$ is a smooth bijection from $\\mathbb{R}$ to $(-\\frac{\\pi}{2},\\frac{\\pi}{2})$, the change-of-variables formula yields, for $\\theta \\in \\left(-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)$,\n$$\nf_{\\Theta}(\\theta) \\;=\\; f_{T}(\\tan\\theta)\\,\\left|\\frac{d}{d\\theta}\\tan\\theta\\right|\n\\;=\\; \\frac{1}{\\pi\\left(1+\\tan^{2}\\theta\\right)}\\,\\sec^{2}\\theta\n\\;=\\; \\frac{1}{\\pi},\n$$\nbecause $1+\\tan^{2}\\theta=\\sec^{2}\\theta$. Hence $\\Theta$ is uniform on $\\left(-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)$.\n\nRoute B (via rotational symmetry): The joint density of $(X,Y)$ is\n$$\nf_{X,Y}(x,y) \\;=\\; \\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{x^{2}+y^{2}}{2}\\right).\n$$\nIn polar coordinates $x=r\\cos\\phi$, $y=r\\sin\\phi$ with Jacobian $r$, one gets\n$$\nf_{R,\\Phi}(r,\\phi) \\;=\\; \\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{r^{2}}{2}\\right) r,\n$$\nwhich factorizes, implying $\\Phi$ is uniform on $(-\\pi,\\pi)$ and independent of $R$. The quantity $\\Theta=\\arctan(Y/X)$ equals the angle $\\Phi$ reduced modulo $\\pi$, hence is uniform on $(-\\frac{\\pi}{2},\\frac{\\pi}{2})$ with density $1/\\pi$.\n\nIn either route, the limit $\\Theta$ is $\\mathrm{Uniform}\\!\\left(-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)$. Therefore, by convergence in distribution established above, $\\Theta_{n} \\xrightarrow{d} \\mathrm{Uniform}\\!\\left(-\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1395925"}]}