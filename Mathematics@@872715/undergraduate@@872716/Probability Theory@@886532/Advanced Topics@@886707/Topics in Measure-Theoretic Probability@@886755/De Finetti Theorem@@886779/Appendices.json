{"hands_on_practices": [{"introduction": "De Finetti's theorem provides a powerful way to model sequences where the order of events doesn't matter. This first practice demonstrates the core mechanism: treating the unknown underlying probability as a random variable and averaging over all its possible values. By tackling a scenario with a uniform prior distribution, we can calculate a simple, unconditional probability and build intuition for how exchangeable sequences behave [@problem_id:1355478].", "problem": "Consider a sequence of experiments, where each experiment can result in either a success (represented by 1) or a failure (represented by 0). Let $X_1, X_2, X_3, \\dots$ be the sequence of random variables representing the outcomes of these experiments. This sequence is known to be *exchangeable*, which means that the joint probability distribution of any finite subset of these variables is unchanged by reordering them. For example, the probability of observing the sequence (1, 0, 1) is the same as observing (1, 1, 0) or (0, 1, 1).\n\nA key result in probability theory, de Finetti's theorem, implies that for such an infinite exchangeable sequence of binary outcomes, there exists a random variable $\\Theta$ taking values in $[0, 1]$ such that, conditional on $\\Theta = \\theta$, the random variables $X_1, X_2, \\dots$ are independent and identically distributed Bernoulli trials with a probability of success given by $P(X_i = 1 | \\Theta = \\theta) = \\theta$.\n\nSuppose that our initial uncertainty about the underlying success rate of the process is modeled by treating the parameter $\\Theta$ as a random variable drawn from a continuous uniform distribution on the interval $[0, 1]$.\n\nBased on this model, calculate the unconditional probability that the first experiment results in a success, $P(X_1 = 1)$.", "solution": "The problem asks for the unconditional probability $P(X_1 = 1)$. We are given information about the conditional probability of $X_1=1$ given the parameter $\\Theta$, and we are also given the probability distribution of $\\Theta$. This situation calls for the use of the Law of Total Probability for continuous random variables.\n\nLet $f_{\\Theta}(\\theta)$ be the probability density function (PDF) of the random variable $\\Theta$. The Law of Total Probability states that we can find the unconditional probability of an event by integrating the conditional probability of that event over all possible values of the conditioning variable, weighted by the PDF of that variable. Mathematically, this is expressed as:\n$$P(X_1 = 1) = \\int_{-\\infty}^{\\infty} P(X_1 = 1 | \\Theta = \\theta) f_{\\Theta}(\\theta) \\, d\\theta$$\n\nWe are given two key pieces of information:\n1. The conditional probability of success: $P(X_1 = 1 | \\Theta = \\theta) = \\theta$.\n2. The distribution of the parameter $\\Theta$: $\\Theta$ follows a continuous uniform distribution on the interval $[0, 1]$.\n\nFor a uniform distribution on $[a, b]$, the PDF is given by $f(x) = \\frac{1}{b-a}$ for $x \\in [a, b]$, and $f(x)=0$ otherwise. In our case, with the interval $[0, 1]$, the PDF of $\\Theta$ is:\n$$f_{\\Theta}(\\theta) = \\frac{1}{1-0} = 1 \\quad \\text{for } \\theta \\in [0, 1]$$\nAnd $f_{\\Theta}(\\theta) = 0$ for $\\theta$ outside this interval.\n\nNow we can substitute these into the integral from the Law of Total Probability. Since $f_{\\Theta}(\\theta)$ is zero outside the interval $[0, 1]$, the limits of integration can be changed from $(-\\infty, \\infty)$ to $(0, 1)$:\n$$P(X_1 = 1) = \\int_{0}^{1} P(X_1 = 1 | \\Theta = \\theta) f_{\\Theta}(\\theta) \\, d\\theta$$\n\nSubstituting the given expressions for the conditional probability and the PDF:\n$$P(X_1 = 1) = \\int_{0}^{1} (\\theta) \\cdot (1) \\, d\\theta$$\n$$P(X_1 = 1) = \\int_{0}^{1} \\theta \\, d\\theta$$\n\nNow, we evaluate this simple integral. The antiderivative of $\\theta$ with respect to $\\theta$ is $\\frac{\\theta^2}{2}$. We evaluate this from 0 to 1:\n$$P(X_1 = 1) = \\left[ \\frac{\\theta^2}{2} \\right]_{0}^{1}$$\n$$P(X_1 = 1) = \\frac{1^2}{2} - \\frac{0^2}{2}$$\n$$P(X_1 = 1) = \\frac{1}{2} - 0$$\n$$P(X_1 = 1) = \\frac{1}{2}$$\n\nThus, the unconditional probability that the first experiment is a success is $\\frac{1}{2}$. This result means that with no prior observations, and with a completely non-informative (uniform) prior on the success probability $\\theta$, success and failure are equally likely.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1355478"}, {"introduction": "A key application of de Finetti's theorem is in learning from observations. This exercise simulates a real-world scenario where initial uncertainty exists between two possibilities, and we must update our beliefs based on evidence. You will apply Bayes' rule to see how initial data affects the prediction of a future outcome, illustrating the dynamic nature of probabilistic inference [@problem_id:1355482].", "problem": "In a semiconductor fabrication plant, a batch of microprocessors is produced by one of two distinct manufacturing lines, Line A or Line B. It is not known which line produced the current batch. Based on historical production data, the prior probability that the batch comes from Line A is $\\frac{1}{4}$, and the probability that it comes from Line B is $\\frac{3}{4}$.\n\nThe quality of microprocessors from each line is characterized by a constant but different probability of passing a specific stress test. A microprocessor from Line A has a probability of $\\frac{1}{3}$ of passing the test, while one from Line B has a probability of $\\frac{2}{3}$ of passing. Within any given batch from a single line, the outcomes of the tests for different microprocessors are independent.\n\nAn engineer randomly selects three microprocessors from the batch for testing. The first microprocessor passes the test, and the second one fails. Let the random variable $X_i$ be 1 if the $i$-th microprocessor passes the test and 0 if it fails. Thus, the observed outcomes are $X_1 = 1$ and $X_2 = 0$.\n\nCalculate the probability that the third microprocessor will pass the test, given the outcomes of the first two tests. Express your answer as a single fraction in simplest form.", "solution": "Let the unknown manufacturing line be a latent variable $L \\in \\{A,B\\}$. The prior probabilities are $P(L=A)=\\frac{1}{4}$ and $P(L=B)=\\frac{3}{4}$. The pass probabilities are $p_{A}=\\frac{1}{3}$ and $p_{B}=\\frac{2}{3}$, and conditional on $L$, test outcomes are independent Bernoulli with the corresponding parameter.\n\nDenote the observed data by $D=\\{X_{1}=1, X_{2}=0\\}$. By conditional independence given $L$, the likelihoods are\n$$\nP(D \\mid L=A)=p_{A}\\left(1-p_{A}\\right)=\\frac{1}{3}\\cdot\\frac{2}{3}=\\frac{2}{9}, \\quad\nP(D \\mid L=B)=p_{B}\\left(1-p_{B}\\right)=\\frac{2}{3}\\cdot\\frac{1}{3}=\\frac{2}{9}.\n$$\nBy Bayesâ€™ rule,\n$$\nP(L=A \\mid D)=\\frac{P(D \\mid A)P(A)}{P(D \\mid A)P(A)+P(D \\mid B)P(B)}.\n$$\nSince $P(D \\mid A)=P(D \\mid B)$, the posterior equals the prior:\n$$\nP(L=A \\mid D)=\\frac{1}{4}, \\quad P(L=B \\mid D)=\\frac{3}{4}.\n$$\nThe predictive probability for the third test is obtained by the law of total probability:\n$$\nP(X_{3}=1 \\mid D)=P(L=A \\mid D)\\,p_{A}+P(L=B \\mid D)\\,p_{B}\n=\\frac{1}{4}\\cdot\\frac{1}{3}+\\frac{3}{4}\\cdot\\frac{2}{3}\n=\\frac{1}{12}+\\frac{1}{2}=\\frac{7}{12}.\n$$\nTherefore, the probability that the third microprocessor passes the test, given the outcomes of the first two, is $\\frac{7}{12}$.", "answer": "$$\\boxed{\\frac{7}{12}}$$", "id": "1355482"}, {"introduction": "While observations in an exchangeable sequence are independent *given* the underlying parameter, they are not unconditionally independent. This final practice explores the nature of this dependence by having you calculate the correlation between two observations. The result reveals a fundamental property of such sequences and highlights how uncertainty about a shared, unknown parameter links the outcomes together [@problem_id:1355469].", "problem": "A political scientist is studying public opinion on a new proposed law. The true proportion, $\\theta$, of the population that supports the law is unknown. To model this uncertainty, the scientist treats $\\theta$ as the outcome of a random variable $\\Theta$, which is uniformly distributed over the interval $[0, 1]$.\n\nThe scientist conducts a poll by randomly sampling individuals from the population. Let $X_i$ be a random variable representing the response of the $i$-th individual, where $X_i = 1$ if the individual supports the law and $X_i = 0$ otherwise. It is assumed that, for a given true proportion $\\theta$, the individual responses $X_1, X_2, \\ldots$ are independent and identically distributed Bernoulli trials with a probability of success (support) equal to $\\theta$.\n\nConsidering this model, what is the correlation coefficient, $\\mathrm{Corr}(X_i, X_j)$, between the responses of any two distinct individuals $i$ and $j$ (where $i \\neq j$)?", "solution": "We model the responses via a hierarchical mixture: let $\\Theta \\sim \\text{Uniform}(0,1)$, and conditional on $\\Theta=\\theta$, the variables $X_{1},X_{2},\\ldots$ are independent with $X_{i} \\sim \\text{Bernoulli}(\\theta)$. For $i \\neq j$, the correlation is\n$$\n\\mathrm{Corr}(X_{i},X_{j})=\\frac{\\mathrm{Cov}(X_{i},X_{j})}{\\sqrt{\\mathrm{Var}(X_{i})\\mathrm{Var}(X_{j})}}=\\frac{\\mathrm{Cov}(X_{1},X_{2})}{\\mathrm{Var}(X_{1})},\n$$\nby exchangeability.\n\nFirst compute $E[X_{1}]$ using the law of total expectation:\n$$\nE[X_{1}]=E\\!\\left[E[X_{1}\\mid \\Theta]\\right]=E[\\Theta]=\\int_{0}^{1}\\theta\\,d\\theta=\\frac{1}{2}.\n$$\nNext compute $E[X_{1}X_{2}]$ using conditional independence given $\\Theta$:\n$$\nE[X_{1}X_{2}]=E\\!\\left[E[X_{1}X_{2}\\mid \\Theta]\\right]=E\\!\\left[E[X_{1}\\mid \\Theta]\\,E[X_{2}\\mid \\Theta]\\right]=E[\\Theta^{2}]=\\int_{0}^{1}\\theta^{2}\\,d\\theta=\\frac{1}{3}.\n$$\nThus,\n$$\n\\mathrm{Cov}(X_{1},X_{2})=E[X_{1}X_{2}]-E[X_{1}]E[X_{2}]=\\frac{1}{3}-\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{12}.\n$$\n\nNow compute $\\mathrm{Var}(X_{1})$ using the law of total variance:\n$$\n\\mathrm{Var}(X_{1})=E\\!\\left[\\mathrm{Var}(X_{1}\\mid \\Theta)\\right]+\\mathrm{Var}\\!\\left(E[X_{1}\\mid \\Theta]\\right)=E[\\Theta(1-\\Theta)]+\\mathrm{Var}(\\Theta).\n$$\nWe have\n$$\nE[\\Theta(1-\\Theta)]=E[\\Theta]-E[\\Theta^{2}]=\\frac{1}{2}-\\frac{1}{3}=\\frac{1}{6},\n$$\nand\n$$\n\\mathrm{Var}(\\Theta)=E[\\Theta^{2}]-(E[\\Theta])^{2}=\\frac{1}{3}-\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{12}.\n$$\nTherefore,\n$$\n\\mathrm{Var}(X_{1})=\\frac{1}{6}+\\frac{1}{12}=\\frac{1}{4}.\n$$\n\nFinally, the correlation coefficient is\n$$\n\\mathrm{Corr}(X_{i},X_{j})=\\frac{\\frac{1}{12}}{\\frac{1}{4}}=\\frac{1}{3}.\n$$", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1355469"}]}