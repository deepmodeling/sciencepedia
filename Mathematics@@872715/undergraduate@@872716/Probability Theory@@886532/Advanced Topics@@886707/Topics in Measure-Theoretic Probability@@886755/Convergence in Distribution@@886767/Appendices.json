{"hands_on_practices": [{"introduction": "Before wielding powerful theorems, it is essential to grasp the foundational definition of convergence in distribution. This exercise challenges you to work directly with the cumulative distribution function (CDF) of a carefully constructed sequence of random variables to find its limit. By analyzing how the probability mass shifts as $n$ grows, you will gain a deeper intuition for how a sequence of distributions can converge to a single point, a concept that underpins many theoretical results [@problem_id:1353078].", "problem": "Consider a sequence of discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each positive integer $n$, the random variable $X_n$ can take one of two values. With probability $1 - \\frac{1}{n}$, the random variable $X_n$ takes the value $\\frac{1}{n}$. With the remaining probability $\\frac{1}{n}$, $X_n$ takes the value $n$.\n\nAs $n$ becomes very large, the probability distribution of $X_n$ converges to a limiting distribution. Which of the following options correctly describes this limiting distribution?\n\nA. A discrete distribution with a single point mass at 0.\nB. A discrete distribution with point masses at 0 and 1.\nC. A continuous uniform distribution on the interval [0, 1].\nD. The sequence does not converge to any distribution.\nE. A standard normal distribution.", "solution": "We are given a sequence of random variables $\\{X_{n}\\}_{n=1}^{\\infty}$ with\n$$\n\\mathbb{P}(X_{n}=\\tfrac{1}{n})=1-\\tfrac{1}{n}, \\qquad \\mathbb{P}(X_{n}=n)=\\tfrac{1}{n}.\n$$\nTo identify the limiting distribution in the sense of convergence in distribution, consider the distribution functions $F_{n}(x)=\\mathbb{P}(X_{n}\\le x)$. For fixed $n$, compute $F_{n}(x)$ piecewise:\n- If $x0$, then $F_{n}(x)=0$ because $X_{n}0$ almost surely.\n- If $0\\le x\\tfrac{1}{n}$, then $F_{n}(x)=0$ because both possible values of $X_{n}$ exceed $x$.\n- If $\\tfrac{1}{n}\\le xn$, then $F_{n}(x)=\\mathbb{P}(X_{n}=\\tfrac{1}{n})=1-\\tfrac{1}{n}$.\n- If $x\\ge n$, then $F_{n}(x)=1$.\n\nFix $x\\in \\mathbb{R}$ and take $n\\to\\infty$:\n- If $x0$, then $F_{n}(x)=0$ for all $n$, so $\\lim_{n\\to\\infty}F_{n}(x)=0$.\n- If $x0$, then for all sufficiently large $n$ we have $\\tfrac{1}{n}\\le xn$, hence $F_{n}(x)=1-\\tfrac{1}{n}\\to 1$ as $n\\to\\infty$.\n- At $x=0$, $F_{n}(0)=\\mathbb{P}(X_{n}\\le 0)=0$ for all $n$, so $\\lim_{n\\to\\infty}F_{n}(0)=0$; however, convergence in distribution only requires convergence at continuity points of the limit distribution function.\n\nDefine $F(x)$ by\n$$\nF(x)=\\begin{cases}\n0,  x0,\\\\\n1,  x0.\n\\end{cases}\n$$\nThis $F$ is the distribution function of a degenerate (point mass) distribution at $0$, with a jump at $x=0$. Since $\\lim_{n\\to\\infty}F_{n}(x)=F(x)$ for every continuity point $x$ of $F$ (that is, for all $x\\neq 0$), it follows by the definition of convergence in distribution that $X_{n}\\Rightarrow \\delta_{0}$, a discrete distribution with a single point mass at $0$.\n\nTherefore, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1353078"}, {"introduction": "Perhaps the most celebrated result concerning convergence in distribution is the Central Limit Theorem (CLT). It provides a powerful bridge between any distribution with finite variance and the ubiquitous normal distribution, explaining why the latter appears so frequently in nature and statistics. This practice problem demonstrates a classic application of the CLT to the sample mean of geometric random variables, illustrating how to standardize a variable and use the normal distribution to approximate probabilities for large sample sizes [@problem_id:1910214].", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed random variables, each following a Geometric distribution with a probability of success denoted by $p$. The probability mass function for each $X_i$ is given by $P(X_i=k) = (1-p)^{k-1}p$ for $k = 1, 2, 3, \\dots$. This represents the number of trials needed to get the first success.\n\nLet $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ be the sample mean of these variables. Consider the standardized random variable $Z_n$ defined as:\n$$Z_n = \\frac{\\sqrt{n}(\\bar{X}_n - 1/p)}{\\sqrt{\\frac{1-p}{p^2}}}$$\nGiven a success probability of $p=0.4$, calculate the value of the limit $\\lim_{n \\to \\infty} P(Z_n \\le 1.5)$.\n\nRound your final answer to four significant figures.", "solution": "The given $X_{i}$ are independent and identically distributed with the geometric distribution on $\\{1,2,\\dots\\}$, with success probability $p$. For this parameterization, the mean and variance are\n$$\n\\mu=\\mathbb{E}[X_{i}]=\\frac{1}{p}, \\qquad \\sigma^{2}=\\operatorname{Var}(X_{i})=\\frac{1-p}{p^{2}}.\n$$\nThe sample mean is $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. By the Lindebergâ€“Levy Central Limit Theorem (CLT), since the $X_{i}$ are iid with finite mean $\\mu$ and variance $\\sigma^{2}$, we have\n$$\n\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma}\\;\\xrightarrow{d}\\;N(0,1).\n$$\nThe standardized variable in the problem is\n$$\nZ_{n}=\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\frac{1}{p}\\right)}{\\sqrt{\\frac{1-p}{p^{2}}}},\n$$\nwhich matches the CLT normalization using $\\mu=\\frac{1}{p}$ and $\\sigma=\\sqrt{\\frac{1-p}{p^{2}}}$. Therefore,\n$$\nZ_{n}\\;\\xrightarrow{d}\\;N(0,1).\n$$\nBy the Continuous Mapping Theorem (equivalently, by convergence in distribution at continuity points of the limit CDF), for any real $a$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(Z_{n}\\le a)=\\Phi(a),\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. Taking $a=1.5$, and using $p=0.4$ as specified (which does not affect the standardized limit),\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(Z_{n}\\le 1.5)=\\Phi(1.5).\n$$\nEvaluating the standard normal CDF at $1.5$ and rounding to four significant figures gives\n$$\n\\Phi(1.5)\\approx 0.9332.\n$$", "answer": "$$\\boxed{0.9332}$$", "id": "1910214"}, {"introduction": "The principles of convergence in distribution extend beyond simple means to more complex statistics, such as the range of a sample. This advanced problem requires a synthesis of several key ideas, including order statistics, joint distributions, and the Continuous Mapping Theorem. By investigating the limiting behavior of the sample range, you will uncover the fascinating concept of asymptotic independence and see how the sum of two limiting exponential variables can result in a Gamma distribution, showcasing the rich variety of limiting distributions beyond the normal one [@problem_id:1910222].", "problem": "Let $U_1, U_2, \\dots, U_n$ be a random sample of size $n$ from the standard Uniform distribution on the interval $(0, 1)$. The order statistics of this sample are denoted by $U_{(1)} \\le U_{(2)} \\le \\dots \\le U_{(n)}$, where $U_{(1)}$ is the sample minimum and $U_{(n)}$ is the sample maximum. The sample range is defined as the random variable $R_n = U_{(n)} - U_{(1)}$.\n\nConsider a new random variable $W_n$ defined as $W_n = n(1 - R_n)$. As the sample size $n$ tends to infinity, the distribution of $W_n$ converges to a specific limiting distribution. Which of the following describes this limiting distribution?\n\nA. A Normal distribution with mean 1 and variance 1, $N(1, 1)$.\nB. An Exponential distribution with rate parameter $\\lambda = 1$.\nC. A Gamma distribution with shape parameter $\\alpha = 2$ and rate parameter $\\lambda = 1$.\nD. A Chi-squared distribution with 2 degrees of freedom, $\\chi^2(2)$.\nE. A degenerate distribution where all the probability mass is at the point 2.", "solution": "The problem asks for the limiting distribution of the random variable $W_n = n(1 - R_n)$, where $R_n = U_{(n)} - U_{(1)}$ is the sample range from a Uniform(0,1) sample of size $n$.\n\nFirst, we can rewrite $W_n$ in a more convenient form.\n$$W_n = n(1 - (U_{(n)} - U_{(1)})) = n(1 - U_{(n)} + U_{(1)})$$\nWe can separate this into two terms:\n$$W_n = n U_{(1)} + n(1 - U_{(n)})$$\nLet's define two new sequences of random variables: $X_n = n U_{(1)}$ and $Y_n = n(1 - U_{(n)})$. Then, $W_n = X_n + Y_n$. We will find the limiting distribution of $W_n$ by first finding the joint limiting distribution of $(X_n, Y_n)$ and then applying the Continuous Mapping Theorem.\n\nThe cumulative distribution function (CDF) of a standard Uniform(0,1) random variable $U$ is $F_U(u) = u$ for $u \\in (0, 1)$.\n\nLet's find the limiting distribution of $X_n = nU_{(1)}$. For any $x > 0$:\nThe CDF of $X_n$ is $F_{X_n}(x) = P(X_n \\le x) = P(nU_{(1)} \\le x) = P(U_{(1)} \\le x/n)$.\nIt is easier to work with the complementary event:\n$P(U_{(1)} > x/n) = P(U_1 > x/n, U_2 > x/n, \\dots, U_n > x/n)$.\nSince the $U_i$ are independent and identically distributed (i.i.d.), this is:\n$P(U_{(1)} > x/n) = [P(U_1 > x/n)]^n = (1 - P(U_1 \\le x/n))^n = (1 - F_U(x/n))^n = (1 - x/n)^n$.\nThis holds for $0  x/n  1$, which is true for large enough $n$.\nTaking the limit as $n \\to \\infty$:\n$\\lim_{n \\to \\infty} P(U_{(1)} > x/n) = \\lim_{n \\to \\infty} (1 - x/n)^n = \\exp(-x)$.\nTherefore, the limiting CDF of $X_n$ is:\n$F_X(x) = \\lim_{n \\to \\infty} F_{X_n}(x) = 1 - \\lim_{n \\to \\infty} P(U_{(1)} > x/n) = 1 - \\exp(-x)$ for $x > 0$.\nThis is the CDF of an Exponential distribution with rate parameter $\\lambda=1$. So, $X_n \\xrightarrow{d} X \\sim \\text{Exp}(1)$.\n\nNext, let's find the limiting distribution of $Y_n = n(1 - U_{(n)})$. For any $y > 0$:\nThe CDF of $Y_n$ is $F_{Y_n}(y) = P(Y_n \\le y) = P(n(1 - U_{(n)}) \\le y) = P(1 - U_{(n)} \\le y/n) = P(U_{(n)} \\ge 1-y/n)$.\nAgain, we consider the complementary event:\n$P(U_{(n)}  1 - y/n) = P(U_1  1-y/n, \\dots, U_n  1-y/n)$.\nSince the $U_i$ are i.i.d.:\n$P(U_{(n)}  1 - y/n) = [P(U_1  1-y/n)]^n = (F_U(1-y/n))^n = (1-y/n)^n$.\nThis holds for $0  1-y/n  1$, which is true for large enough $n$.\nTaking the limit as $n \\to \\infty$:\n$\\lim_{n \\to \\infty} P(U_{(n)}  1-y/n) = \\lim_{n \\to \\infty} (1 - y/n)^n = \\exp(-y)$.\nTherefore, the limiting CDF of $Y_n$ is:\n$F_Y(y) = \\lim_{n \\to \\infty} F_{Y_n}(y) = 1 - \\lim_{n \\to \\infty} P(U_{(n)}  1-y/n) = 1 - \\exp(-y)$ for $y > 0$.\nThis is also the CDF of an Exponential distribution with rate parameter $\\lambda=1$. So, $Y_n \\xrightarrow{d} Y \\sim \\text{Exp}(1)$.\n\nNow, we must determine if the limiting variables $X$ and $Y$ are independent. We do this by finding the limit of the joint CDF of $(X_n, Y_n)$. For $x > 0, y > 0$:\n$F_{X_n, Y_n}(x,y) = P(X_n \\le x, Y_n \\le y) = P(nU_{(1)} \\le x, n(1-U_{(n)}) \\le y) = P(U_{(1)} \\le x/n, U_{(n)} \\ge 1-y/n)$.\nThis is the probability that the sample minimum is at most $x/n$ and the sample maximum is at least $1-y/n$. It is easier to calculate the probability of the complement event, which is $\\{U_{(1)} > x/n\\} \\cup \\{U_{(n)}  1-y/n\\}$.\nUsing the principle of inclusion-exclusion:\n$P(\\{U_{(1)} > x/n\\} \\cup \\{U_{(n)}  1-y/n\\}) = P(U_{(1)} > x/n) + P(U_{(n)}  1-y/n) - P(U_{(1)} > x/n, U_{(n)}  1-y/n)$.\nWe already found $P(U_{(1)} > x/n) = (1-x/n)^n$ and $P(U_{(n)}  1-y/n) = (1-y/n)^n$.\nThe intersection event $P(U_{(1)} > x/n, U_{(n)}  1-y/n)$ means that all $U_i$ must fall in the interval $(x/n, 1-y/n)$. The length of this interval is $(1-y/n) - x/n = 1 - (x+y)/n$.\n$P(U_{(1)} > x/n, U_{(n)}  1-y/n) = P(\\text{all } U_i \\in (x/n, 1-y/n)) = [1 - (x+y)/n]^n$.\nFor this to be valid, we need $x/n  1-y/n$, which holds for large $n$.\nSo, $\\lim_{n \\to \\infty} P(\\{U_{(1)} > x/n\\} \\cup \\{U_{(n)}  1-y/n\\}) = \\exp(-x) + \\exp(-y) - \\exp(-(x+y))$.\nThe limiting joint CDF is:\n$F_{X,Y}(x,y) = \\lim_{n \\to \\infty} F_{X_n, Y_n}(x,y) = 1 - (\\exp(-x) + \\exp(-y) - \\exp(-(x+y)))$.\n$F_{X,Y}(x,y) = 1 - \\exp(-x) - \\exp(-y) + \\exp(-x)\\exp(-y) = (1-\\exp(-x))(1-\\exp(-y))$.\nThis is the product of the marginal CDFs, $F_X(x)F_Y(y)$. This proves that the limiting random variables $X$ and $Y$ are independent.\n\nSince $W_n = X_n + Y_n$, and $(X_n, Y_n) \\xrightarrow{d} (X,Y)$ where $X$ and $Y$ are i.i.d. $\\text{Exp}(1)$ random variables, by the Continuous Mapping Theorem, the distribution of $W_n$ converges to the distribution of $W = X+Y$.\nThe sum of two independent and identically distributed Exponential($\\lambda$) random variables follows a Gamma distribution with shape parameter $\\alpha=2$ and rate parameter $\\lambda$. In our case, $\\lambda=1$.\nTherefore, the limiting distribution of $W_n$ is a Gamma(2, 1) distribution.\n\nLet's check the options:\nA. Normal distribution - Incorrect.\nB. Exponential(1) distribution - Incorrect. This would be the limit if we only considered $X_n$ or $Y_n$.\nC. Gamma(2, 1) distribution - Correct.\nD. A Chi-squared distribution with 2 degrees of freedom, $\\chi^2(2)$, is equivalent to a Gamma distribution with shape $\\alpha=1$ and rate $\\lambda=1/2$. This is not a Gamma(2, 1). So, this is incorrect.\nE. Degenerate distribution - Incorrect. The limiting distribution is non-degenerate.", "answer": "$$\\boxed{C}$$", "id": "1910222"}]}