{"hands_on_practices": [{"introduction": "This first exercise provides a foundational look at convergence in probability. It demonstrates the intuitive principle that if a sequence of measurements becomes increasingly precise around a true value—meaning its variance shrinks to zero—then the measurements will converge to that value. We will use the powerful tool of Chebyshev's inequality to rigorously prove this fundamental concept. [@problem_id:1910709]", "problem": "An engineer is developing a new sensor to measure a specific physical property, whose true value is an unknown constant $c$. The engineer takes a series of measurements. Let $X_n$ be the random variable representing the outcome of the $n$-th measurement for $n=1, 2, 3, \\ldots$. Due to refinements in the experimental setup, the measurements are unbiased, meaning the expected value of any measurement is the true value, i.e., $E[X_n] = c$ for all $n \\geq 1$. The precision of the measurements improves with each attempt, and the variance of the $n$-th measurement is found to be $\\text{Var}(X_n) = \\frac{\\sigma^2}{n^2}$, where $\\sigma$ is a known positive constant representing a baseline measurement uncertainty.\n\nThe sequence of measurements $\\{X_n\\}$ converges in probability to a specific value. Determine this value.", "solution": "We want the limit in probability of $X_{n}$ as $n \\to \\infty$. By definition, $X_{n} \\to c$ in probability if for every $\\varepsilon > 0$,\n$$\n\\lim_{n \\to \\infty} P(|X_{n} - c| > \\varepsilon) = 0.\n$$\nSince the measurements are unbiased, $E[X_{n}] = c$ for all $n \\geq 1$. By Chebyshev's inequality, for any $\\varepsilon > 0$,\n$$\nP(|X_{n} - E[X_{n}]| \\geq \\varepsilon) \\leq \\frac{\\text{Var}(X_{n})}{\\varepsilon^{2}}.\n$$\nUsing $\\text{Var}(X_{n}) = \\frac{\\sigma^{2}}{n^{2}}$ and $E[X_{n}] = c$, we get\n$$\nP(|X_{n} - c| \\geq \\varepsilon) \\leq \\frac{\\sigma^{2}}{n^{2}\\varepsilon^{2}} \\xrightarrow[n \\to \\infty]{} 0.\n$$\nTherefore, by the definition of convergence in probability, $X_{n} \\xrightarrow{p} c$. Equivalently, one may observe that\n$$\nE\\left[(X_{n} - c)^{2}\\right] = \\text{Var}(X_{n}) = \\frac{\\sigma^{2}}{n^{2}} \\to 0,\n$$\nso $X_{n} \\to c$ in mean square, which implies convergence in probability to $c$.", "answer": "$$\\boxed{c}$$", "id": "1910709"}, {"introduction": "Building on the basics, this problem explores how convergence in probability behaves when we combine different random sequences. We are given two sequences, one converging to a constant and another converging to zero, and are asked to find the limit of their sum. This exercise illustrates a key algebraic property of convergence in probability, often related to Slutsky's Theorem, which is essential for analyzing more complex statistical models. [@problem_id:1910723]", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ and $\\{Y_n\\}_{n=1}^{\\infty}$ be two sequences of random variables. The sequence $\\{X_n\\}$ is known to converge in probability to the constant 5. The sequence $\\{Y_n\\}$ is characterized by having a mean of $E[Y_n] = 0$ and a variance of $\\text{Var}(Y_n) = \\frac{1}{\\sqrt{n}}$ for all positive integers $n$.\n\nA new sequence of random variables, $\\{Z_n\\}_{n=1}^{\\infty}$, is defined by the sum $Z_n = X_n + Y_n$.\n\nDetermine the numerical value to which the sequence $\\{Z_n\\}$ converges in probability.", "solution": "We are given that $X_{n} \\xrightarrow{p} 5$, that is, for every $\\varepsilon>0$,\n$$\n\\lim_{n\\to\\infty}P(|X_{n}-5|>\\varepsilon)=0.\n$$\nFor $Y_{n}$, we have $E[Y_{n}]=0$ and $\\text{Var}(Y_{n})=n^{-1/2}$ for all $n$. By Chebyshev’s inequality, for any $\\varepsilon>0$,\n$$\nP(|Y_{n}|>\\varepsilon)=P(|Y_{n}-E[Y_{n}]|>\\varepsilon)\\leq \\frac{\\text{Var}(Y_{n})}{\\varepsilon^{2}}=\\frac{n^{-1/2}}{\\varepsilon^{2}}\\xrightarrow[n\\to\\infty]{}0.\n$$\nHence $Y_{n}\\xrightarrow{p}0$.\n\nDefine $Z_{n}=X_{n}+Y_{n}$. For any $\\varepsilon>0$, by the triangle inequality,\n$$\n|Z_{n}-5|=\\big|(X_{n}-5)+Y_{n}\\big|\\leq |X_{n}-5|+|Y_{n}|.\n$$\nTherefore, using the union bound,\n$$\nP(|Z_{n}-5|>\\varepsilon)\\leq P(|X_{n}-5|>\\varepsilon/2)+P(|Y_{n}|>\\varepsilon/2)\\xrightarrow[n\\to\\infty]{}0,\n$$\nbecause the first term tends to zero by $X_{n}\\xrightarrow{p}5$ and the second term tends to zero by Chebyshev’s inequality shown above. Thus $Z_{n}\\xrightarrow{p}5$.\n\nTherefore, the numerical value to which $Z_{n}$ converges in probability is $5$.", "answer": "$$\\boxed{5}$$", "id": "1910723"}, {"introduction": "This final practice applies our understanding of convergence to the critical statistical concept of estimator consistency. By analyzing an estimator that ignores new data as the sample size grows, we can explore precisely why it fails to converge to the true population parameter. This counterexample provides a clear, practical lesson on what makes an estimator \"good\" and why convergence in probability is a benchmark for reliable statistical inference. [@problem_id:1910737]", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables from a population with a finite mean $E[X_i] = \\mu$ and a finite, non-zero variance $\\text{Var}(X_i) = \\sigma^2$.\n\nAn investigator proposes an estimator for the population mean $\\mu$, defined as $\\hat{\\mu}_n = X_1$. This estimator uses only the first observation, regardless of the total sample size $n$. We wish to analyze the consistency of this estimator.\n\nWhich of the following statements correctly describes the convergence property of the estimator $\\hat{\\mu}_n$ as the sample size $n$ approaches infinity?\n\nA. $\\hat{\\mu}_n$ converges in probability to $\\mu$ because it is an unbiased estimator of $\\mu$.\n\nB. $\\hat{\\mu}_n$ converges in probability to $\\mu$ since the Law of Large Numbers guarantees convergence for any estimator as the sample size $n$ grows infinitely large.\n\nC. $\\hat{\\mu}_n$ does not converge in probability to $\\mu$ because the probability of the estimator being more than a small distance away from $\\mu$ remains a fixed positive number as $n$ increases.\n\nD. $\\hat{\\mu}_n$ does not converge in probability to $\\mu$ because the estimator is biased, and biased estimators can never be consistent.\n\nE. Whether $\\hat{\\mu}_n$ converges in probability to $\\mu$ cannot be determined without knowing if the underlying distribution of the $X_i$ variables is normal.", "solution": "Let $\\{X_{i}\\}_{i=1}^{n}$ be i.i.d. with $E[X_{i}] = \\mu$ and $\\text{Var}(X_{i}) = \\sigma^{2}$ where $\\sigma^{2} \\in (0,\\infty)$. The proposed estimator is $\\hat{\\mu}_{n} = X_{1}$ for all $n$.\n\nBy definition, $\\hat{\\mu}_{n}$ is consistent for $\\mu$ if and only if for every $\\varepsilon > 0$,\n$$\n\\lim_{n \\to \\infty} P(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon) = 0.\n$$\nSince $\\hat{\\mu}_{n} = X_{1}$ for all $n$, we have for every $\\varepsilon > 0$ and every $n$,\n$$\nP(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon) = P(|X_{1} - \\mu| > \\varepsilon) =: p(\\varepsilon),\n$$\nwhich does not depend on $n$.\n\nWe now show $p(\\varepsilon)$ is strictly positive for at least one $\\varepsilon > 0$. Suppose, for contradiction, that $p(\\varepsilon) = 0$ for all $\\varepsilon > 0$. Then for each rational sequence $\\{\\varepsilon_{k}\\}$ with $\\varepsilon_{k} \\downarrow 0$,\n$$\nP(|X_{1} - \\mu| \\le \\varepsilon_{k}) = 1 \\quad \\text{for all } k,\n$$\nhence\n$$\nP\\left(\\bigcap_{k=1}^{\\infty} \\{|X_{1} - \\mu| \\le \\varepsilon_{k}\\}\\right) = 1,\n$$\nwhich implies $P(X_{1} = \\mu) = 1$, and therefore $\\text{Var}(X_{1}) = 0$, contradicting $\\sigma^{2} > 0$. Thus there exists $\\varepsilon_{0} > 0$ such that\n$$\np(\\varepsilon_{0}) = P(|X_{1} - \\mu| > \\varepsilon_{0}) > 0.\n$$\nConsequently,\n$$\n\\lim_{n \\to \\infty} P(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon_{0}) = \\lim_{n \\to \\infty} p(\\varepsilon_{0}) = p(\\varepsilon_{0}) > 0,\n$$\nso the consistency condition fails. Therefore $\\hat{\\mu}_{n}$ does not converge in probability to $\\mu$.\n\nOption-by-option assessment:\n- A is false: unbiasedness alone does not imply consistency.\n- B is false: the Law of Large Numbers applies to the sample mean that depends on $n$, not to $X_{1}$ which ignores $n$.\n- C is true: the error probability is a fixed positive constant for some small $\\varepsilon$, independent of $n$.\n- D is false: the estimator is unbiased, and in any case, bias does not preclude consistency in general.\n- E is false: normality is unnecessary; the argument relies only on $\\sigma^{2} > 0$.\n\nThus the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1910737"}]}