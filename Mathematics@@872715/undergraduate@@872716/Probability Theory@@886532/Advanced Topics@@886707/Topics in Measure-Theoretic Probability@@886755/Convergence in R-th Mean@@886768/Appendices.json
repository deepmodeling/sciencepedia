{"hands_on_practices": [{"introduction": "Understanding convergence starts with applying the definition to concrete examples. This first practice problem [@problem_id:1353612] presents a scenario with Poisson-distributed random variables, a common model for count data. By calculating the limit of the second moment, you will directly verify whether the sequence converges to zero in mean square, reinforcing the fundamental mechanics of this concept.", "problem": "A team of data scientists is monitoring the daily number of failed login attempts on a secure server. They model the number of failures on day $n$ (for $n=1, 2, 3, \\ldots$) as a random variable $X_n$. Based on security improvements over time, they hypothesize that the expected number of failures decreases. They choose a Poisson distribution for $X_n$ with a rate parameter $\\lambda_n = \\frac{1}{n}$.\n\nTo assess if the number of failures is effectively tending to zero over the long run, they decide to check for convergence in mean square. This involves evaluating if the expected squared difference between $X_n$ and the target value of 0 diminishes to nothing.\n\nCalculate the limit of the mean square difference between $X_n$ and 0. That is, compute the value of $L = \\lim_{n \\to \\infty} E[(X_n - 0)^2]$.", "solution": "We are given that $X_{n}\\sim \\text{Poisson}(\\lambda_{n})$ with $\\lambda_{n}=\\frac{1}{n}$ for $n\\in\\{1,2,3,\\ldots\\}$. The quantity of interest is the mean square difference from $0$, which equals the second moment:\n$$\nE\\big[(X_{n}-0)^{2}\\big]=E\\big[X_{n}^{2}\\big].\n$$\nFor any random variable, the identity $E[X^{2}]=\\operatorname{Var}(X)+\\big(E[X]\\big)^{2}$ holds. For a Poisson random variable with parameter $\\lambda$, it is a standard result that\n$$\nE[X]=\\lambda,\\qquad \\operatorname{Var}(X)=\\lambda.\n$$\nApplying this to $X_{n}$ with $\\lambda_{n}=\\frac{1}{n}$, we obtain\n$$\nE[X_{n}^{2}]=\\operatorname{Var}(X_{n})+\\big(E[X_{n}]\\big)^{2}=\\lambda_{n}+\\lambda_{n}^{2}=\\frac{1}{n}+\\frac{1}{n^{2}}.\n$$\nTherefore,\n$$\nL=\\lim_{n\\to\\infty}E\\big[(X_{n}-0)^{2}\\big]=\\lim_{n\\to\\infty}\\left(\\frac{1}{n}+\\frac{1}{n^{2}}\\right)=0.\n$$", "answer": "$$\\boxed{0}$$", "id": "1353612"}, {"introduction": "Not all modes of convergence are equivalent, as some are stronger than others. This exercise [@problem_id:1353588] is designed to illustrate the important distinction between convergence in mean ($L^1$) and convergence in quadratic mean ($L^2$). You will analyze a sequence that highlights a key theoretical point: convergence in mean does not necessarily imply convergence in quadratic mean.", "problem": "Let $U$ be a random variable that is uniformly distributed on the interval $(0, 1)$. Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined by\n$$X_n = \\sqrt{n} \\cdot I_{\\{U  1 - 1/n\\}}$$\nwhere $I_A$ is the indicator function for an event $A$, meaning $I_A$ takes the value 1 if the event $A$ occurs, and 0 otherwise.\n\nWe are interested in whether this sequence converges to the constant random variable $X=0$. We will consider two specific modes of convergence:\n\n1.  **Convergence in the mean** (or $L^1$ convergence): The sequence $\\{X_n\\}$ converges to $X$ in the mean if $\\lim_{n \\to \\infty} E[|X_n - X|] = 0$.\n2.  **Convergence in the quadratic mean** (or $L^2$ convergence): The sequence $\\{X_n\\}$ converges to $X$ in the quadratic mean if $\\lim_{n \\to \\infty} E[|X_n - X|^2] = 0$.\n\nWhich of the following statements is true regarding the convergence of the sequence $\\{X_n\\}$ to the zero random variable, $X=0$?\n\nA. $X_n$ converges to 0 in the mean and also in the quadratic mean.\n\nB. $X_n$ converges to 0 in the mean, but not in the quadratic mean.\n\nC. $X_n$ does not converge to 0 in the mean, but it does converge in the quadratic mean.\n\nD. $X_n$ converges to 0 neither in the mean nor in the quadratic mean.", "solution": "Let $A_{n}=\\{U1-\\frac{1}{n}\\}$. Since $U$ is uniformly distributed on $(0,1)$, the probability of an interval equals its length, hence\n$$\n\\mathbb{P}(A_{n})=\\mathbb{P}\\!\\left(U\\in\\left(1-\\frac{1}{n},1\\right)\\right)=1-\\left(1-\\frac{1}{n}\\right)=\\frac{1}{n}.\n$$\nBy definition, $X_{n}=\\sqrt{n}\\,I_{A_{n}}$. Using the fact that for any event $A$, $E[I_{A}]=\\mathbb{P}(A)$, we obtain the $L^{1}$ moment\n$$\nE|X_{n}|=E\\left[\\sqrt{n}\\,I_{A_{n}}\\right]=\\sqrt{n}\\,E[I_{A_{n}}]=\\sqrt{n}\\,\\mathbb{P}(A_{n})=\\sqrt{n}\\cdot\\frac{1}{n}=\\frac{1}{\\sqrt{n}}\\to 0,\n$$\nso $X_{n}\\to 0$ in the mean.\n\nFor quadratic mean, compute $E|X_{n}|^{2}=E[X_{n}^{2}]$. Since $X_{n}^{2}=n\\,I_{A_{n}}$, we have\n$$\nE[X_{n}^{2}]=E\\left[n\\,I_{A_{n}}\\right]=n\\,E[I_{A_{n}}]=n\\,\\mathbb{P}(A_{n})=n\\cdot\\frac{1}{n}=1,\n$$\nwhich does not converge to $0$. Hence $X_{n}$ does not converge to $0$ in the quadratic mean.\n\nTherefore, the correct statement is that $X_{n}$ converges to $0$ in the mean but not in the quadratic mean.", "answer": "$$\\boxed{B}$$", "id": "1353588"}, {"introduction": "Once we establish that a sequence converges, what can we say about functions of that sequence? This problem [@problem_id:1353601] explores the properties of mean square convergence by examining what happens when we square each random variable in a convergent sequence. This exercise demonstrates how strong properties of $L^2$ convergence, combined with tools like the Cauchy-Schwarz inequality, allow us to prove further results about related sequences.", "problem": "In probability theory, a sequence of random variables $\\{X_n\\}_{n \\ge 1}$ is said to converge in the $r$-th mean (or in $L^r$) to a random variable $X$ if $\\lim_{n \\to \\infty} E[|X_n - X|^r] = 0$, where $E[\\cdot]$ denotes the expectation operator. Convergence in mean square corresponds to $r=2$, and convergence in mean corresponds to $r=1$.\n\nConsider a sequence of random variables $\\{X_n\\}_{n \\ge 1}$ that converges in mean square to a non-zero real constant $c$. Which of the following statements regarding the sequence of squared random variables, $\\{X_n^2\\}_{n \\ge 1}$, is definitively true?\n\nA. The sequence $\\{X_n^2\\}$ converges in mean to $c^2$.\n\nB. The sequence $\\{X_n^2\\}$ converges in mean square to $c^4$.\n\nC. The sequence $\\{X_n^2\\}$ converges in mean to $2c$.\n\nD. The convergence of $\\{X_n^2\\}$ in mean cannot be determined without knowing the specific probability distributions of the $X_n$.\n\nE. The sequence $\\{X_n^2\\}$ does not converge in mean.", "solution": "We are given that $X_n \\to c$ in mean square, i.e.,\n$$\n\\lim_{n \\to \\infty} E\\big[(X_n - c)^{2}\\big] = 0.\n$$\nWe analyze the sequence of squares. Note the factorization\n$$\nX_n^{2} - c^{2} = (X_n - c)(X_n + c),\n$$\nso by the triangle inequality and the Cauchy–Schwarz inequality,\n$$\nE\\big[|X_n^{2} - c^{2}|\\big] = E\\big(|X_n - c||X_n + c|\\big) \\le \\big(E|X_n - c|^{2}\\big)^{1/2}\\big(E|X_n + c|^{2}\\big)^{1/2}.\n$$\nThe first factor tends to $0$ by the assumed $L^{2}$ convergence. It remains to show that the second factor is bounded uniformly in $n$.\n\nWe bound $E|X_n + c|^{2}$ as follows. Using $(a+b)^{2} \\le 2a^{2} + 2b^{2}$,\n$$\n|X_n + c|^{2} \\le 2|X_n|^{2} + 2c^{2} \\quad \\Rightarrow \\quad E|X_n + c|^{2} \\le 2E|X_n|^{2} + 2c^{2}.\n$$\nExpress $E|X_n|^{2}$ via $X_n - c$:\n$$\nE|X_n|^{2} = E\\big[(X_n - c + c)^{2}\\big] = E[(X_n - c)^{2}] + 2c\\,E[X_n - c] + c^{2}.\n$$\nBy Jensen or Cauchy–Schwarz,\n$$\n|E[X_n - c]| \\le E|X_n - c| \\le \\big(E[(X_n - c)^{2}]\\big)^{1/2}.\n$$\nHence,\n$$\nE|X_n|^{2} \\le E[(X_n - c)^{2}] + 2|c|\\,\\big(E[(X_n - c)^{2}]\\big)^{1/2} + c^{2}.\n$$\nCombining the bounds gives\n$$\nE|X_n + c|^{2} \\le 2E[(X_n - c)^{2}] + 4|c|\\,\\big(E[(X_n - c)^{2}]\\big)^{1/2} + 4c^{2}.\n$$\nSince $E[(X_n - c)^{2}] \\to 0$, the right-hand side is bounded for all sufficiently large $n$; taking the maximum over finitely many initial indices yields a finite constant $M$ such that $E|X_n + c|^{2} \\le M$ for all $n$. Therefore,\n$$\nE\\big[|X_n^{2} - c^{2}|\\big] \\le M^{1/2}\\,\\big(E|X_n - c|^{2}\\big)^{1/2} \\xrightarrow[n\\to\\infty]{} 0,\n$$\nwhich is exactly convergence in mean of $X_n^{2}$ to $c^{2}$. Hence statement A is true.\n\nStatement B is false because the natural limit of $X_n^{2}$ is $c^{2}$, not $c^{4}$. Statement C is false since $X_n^{2}$ does not converge to $2c$ in mean. Statement D is false because convergence in mean of $X_n^{2}$ to $c^{2}$ follows from the $L^{2}$ convergence of $X_n$ to $c$ without needing specific distributions. Statement E is false for the same reason.\n\nTherefore the definitively true statement is A.", "answer": "$$\\boxed{A}$$", "id": "1353601"}]}