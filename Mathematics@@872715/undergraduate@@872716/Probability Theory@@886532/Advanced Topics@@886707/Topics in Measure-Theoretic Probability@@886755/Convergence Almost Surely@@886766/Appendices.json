{"hands_on_practices": [{"introduction": "This first practice is a conceptual check-up, designed to solidify your understanding of what \"almost surely\" truly means. We will investigate a sequence that appears simple, but its convergence behavior is subtle, forcing us to go back to the formal definition of almost sure convergence [@problem_id:1352867] and analyze the set of outcomes for which the sequence converges.", "problem": "Let $Y$ be a random variable defined on a probability space $(\\Omega, \\mathcal{F}, P)$. We are given that the expectation of $Y$, denoted by $\\mathbb{E}[Y]$, is non-zero. Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined by\n$$X_n = \\cos(n\\pi) Y$$\nfor all positive integers $n$.\n\nWhich of the following statements about the almost sure convergence of the sequence $\\{X_n\\}$ is correct?\n\nA. The sequence $\\{X_n\\}$ converges almost surely to 0.\n\nB. The sequence $\\{X_n\\}$ converges almost surely to a non-zero constant $C$.\n\nC. The sequence $\\{X_n\\}$ converges almost surely to a non-constant random variable.\n\nD. The sequence $\\{X_n\\}$ does not converge almost surely.", "solution": "We start from the definition $X_{n}=\\cos(n\\pi)Y$. For integer $n$, we use the trigonometric identity\n$$\n\\cos(n\\pi)=(-1)^{n}.\n$$\nTherefore,\n$$\nX_{n}=(-1)^{n}Y.\n$$\n\nFix $\\omega\\in\\Omega$ and set $y=Y(\\omega)$. Then the pointwise sequence is\n$$\nX_{n}(\\omega)=(-1)^{n}y.\n$$\nWe analyze its pointwise convergence as $n\\to\\infty$:\n- If $y=0$, then $X_{n}(\\omega)=0$ for all $n$, so the sequence converges to $0$.\n- If $y\\neq 0$, consider the subsequences of even and odd indices:\n$$\nX_{2k}(\\omega)=y,\\quad X_{2k+1}(\\omega)=-y.\n$$\nThese subsequences have constant but different limits $y$ and $-y$, respectively, and since $y\\neq 0$, the full sequence does not converge.\n\nHence, the set on which $\\{X_{n}\\}$ converges is exactly $\\{\\omega:Y(\\omega)=0\\}=\\{Y=0\\}$. Therefore,\n$$\nP\\big(\\{ \\omega:\\{X_{n}(\\omega)\\} \\text{ converges} \\}\\big)=P(Y=0).\n$$\nFor almost sure convergence, we would need $P(Y=0)=1$, which would imply $Y=0$ almost surely and consequently $\\mathbb{E}[Y]=0$, contradicting the given assumption $\\mathbb{E}[Y]\\neq 0$. Therefore $P(Y=0)\\neq 1$, so the sequence does not converge almost surely.\n\nAmong the options, this corresponds to the statement that the sequence does not converge almost surely.", "answer": "$$\\boxed{D}$$", "id": "1352867"}, {"introduction": "With the definition clarified, we now turn to a core technique for proving almost sure convergence. This exercise demonstrates how to combine Chebyshev's inequality with the first Borel-Cantelli lemma, a powerful duo for establishing that a sequence of random variables eventually settles down near its limit [@problem_id:1352849]. This method is fundamental for proving many key results in probability theory, including the Strong Law of Large Numbers.", "problem": "Consider an experimental apparatus in a physics lab designed to measure fluctuations around a stable equilibrium point, which is defined as a reference value of zero. The measurements, taken at discrete time intervals $k=1, 2, 3, \\ldots$, are denoted by the sequence of random variables $\\{X_k\\}$. These measurements are independent and identically distributed (i.i.d.). While the process is centered at zero, meaning the expected value of any single measurement is $E[X_k] = 0$, there is inherent random noise. This noise is characterized by a finite, non-zero variance, $\\text{Var}(X_k) = \\sigma^2$.\n\nTo analyze the long-term behavior and suppress noise, a data scientist computes a dynamically weighted average, $S_n$, for the first $n$ measurements, defined as:\n$$ S_n = \\frac{1}{n^2} \\sum_{k=1}^n X_k $$\nDetermine the value that $S_n$ converges to almost surely as the number of measurements $n$ approaches infinity.", "solution": "Define $S_n=\\frac{1}{n^{2}}\\sum_{k=1}^{n}X_{k}$ with $\\{X_{k}\\}$ i.i.d., $E[X_{k}]=0$, and $\\operatorname{Var}(X_{k})=\\sigma^{2}\\in(0,\\infty)$. We analyze almost sure convergence of $S_{n}$.\n\nFirst compute the expectation using linearity:\n$$\nE[S_{n}]=\\frac{1}{n^{2}}\\sum_{k=1}^{n}E[X_{k}]=\\frac{1}{n^{2}}\\cdot n \\cdot 0=0.\n$$\n\nNext compute the variance using independence (variance of a sum of independent random variables is the sum of variances):\n$$\n\\operatorname{Var}\\!\\left(S_{n}\\right)=\\operatorname{Var}\\!\\left(\\frac{1}{n^{2}}\\sum_{k=1}^{n}X_{k}\\right)=\\frac{1}{n^{4}}\\sum_{k=1}^{n}\\operatorname{Var}(X_{k})=\\frac{1}{n^{4}}\\cdot n \\sigma^{2}=\\frac{\\sigma^{2}}{n^{3}}.\n$$\n\nApply Chebyshev’s inequality for any $\\epsilon0$:\n$$\n\\Pr\\!\\left(|S_{n}|\\epsilon\\right)\\leq \\frac{\\operatorname{Var}(S_{n})}{\\epsilon^{2}}=\\frac{\\sigma^{2}}{\\epsilon^{2}n^{3}}.\n$$\n\nSum over $n$ and use the $p$-series test with $p=31$:\n$$\n\\sum_{n=1}^{\\infty}\\Pr\\!\\left(|S_{n}|\\epsilon\\right)\\leq \\frac{\\sigma^{2}}{\\epsilon^{2}}\\sum_{n=1}^{\\infty}\\frac{1}{n^{3}}\\infty.\n$$\n\nBy the first Borel–Cantelli lemma, $\\Pr\\!\\left(|S_{n}|\\epsilon \\text{ i.o.}\\right)=0$ for every $\\epsilon0$, which implies $S_{n}\\to 0$ almost surely.\n\nEquivalently, one may invoke the strong law of large numbers: $\\bar{X}_{n}=\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\to E[X_{1}]=0$ almost surely, and since $S_{n}=\\frac{1}{n}\\bar{X}_{n}$ with $\\frac{1}{n}\\to 0$, it follows that $S_{n}\\to 0$ almost surely.", "answer": "$$\\boxed{0}$$", "id": "1352849"}, {"introduction": "Our final practice explores a more advanced topic: the convergence of an infinite series of random variables. We'll examine the famous \"random harmonic series\" [@problem_id:1352900], which behaves surprisingly differently from its deterministic counterpart. This problem provides an opportunity to apply the powerful Kolmogorov's three-series theorem, a definitive tool for determining the almost sure convergence of sums of independent random variables.", "problem": "Consider an infinite sequence of independent and identically distributed (i.i.d.) random variables, $\\xi_1, \\xi_2, \\xi_3, \\dots$. Each random variable $\\xi_k$ is a Rademacher random variable, meaning it takes the value $+1$ with probability $1/2$ and the value $-1$ with probability $1/2$.\n\nWe construct a random series $S$ defined as:\n$$S = \\sum_{k=1}^{\\infty} \\frac{\\xi_k}{k}$$\nThis series is often called the random harmonic series. Your task is to determine the convergence behavior of this series.\n\nWhich of the following statements about the series $S$ is true?\n\nA. The series converges for every possible sequence of outcomes for $(\\xi_k)_{k \\ge 1}$.\n\nB. The series converges almost surely.\n\nC. The series diverges to $+\\infty$ almost surely.\n\nD. The series diverges to $-\\infty$ almost surely.\n\nE. The series oscillates without a limit almost surely.\n\nF. Whether the series converges or diverges depends on the specific sequence of outcomes for $(\\xi_k)_{k \\ge 1}$, and the set of sequences for which it converges has a probability strictly between 0 and 1.", "solution": "Let $X_{k}=\\xi_{k}/k$. Then $(X_{k})_{k\\geq 1}$ are independent random variables. We compute the mean and variance of $X_{k}$:\n$$\\mathbb{E}[X_{k}]=\\frac{1}{k}\\mathbb{E}[\\xi_{k}]=0,$$\nsince $\\mathbb{E}[\\xi_{k}]=0$, and\n$$\\operatorname{Var}(X_{k})=\\mathbb{E}[X_{k}^{2}]-\\left(\\mathbb{E}[X_{k}]\\right)^{2}=\\mathbb{E}\\left[\\frac{\\xi_{k}^{2}}{k^{2}}\\right]=\\frac{1}{k^{2}},$$\nbecause $\\xi_{k}^{2}=1$ almost surely.\n\nWe apply the Kolmogorov three-series theorem. Define the truncation at level $1$. Since $|X_{k}|=\\frac{1}{k}\\leq 1$ for all $k$, we have:\n- $\\sum_{k=1}^{\\infty}\\mathbb{P}(|X_{k}|1)=0,$\n- $\\sum_{k=1}^{\\infty}\\mathbb{E}[X_{k}\\mathbf{1}_{\\{|X_{k}|\\leq 1\\}}]=\\sum_{k=1}^{\\infty}\\mathbb{E}[X_{k}]=\\sum_{k=1}^{\\infty}0=0$ (hence convergent),\n- $\\sum_{k=1}^{\\infty}\\operatorname{Var}(X_{k}\\mathbf{1}_{\\{|X_{k}|\\leq 1\\}})=\\sum_{k=1}^{\\infty}\\operatorname{Var}(X_{k})=\\sum_{k=1}^{\\infty}\\frac{1}{k^{2}}\\infty.$\n\nAll three series satisfy the hypotheses of the theorem. Therefore, the series $\\sum_{k=1}^{\\infty}X_{k}=\\sum_{k=1}^{\\infty}\\frac{\\xi_{k}}{k}$ converges almost surely.\n\nIt does not converge absolutely, since\n$$\\sum_{k=1}^{\\infty}\\left|\\frac{\\xi_{k}}{k}\\right|=\\sum_{k=1}^{\\infty}\\frac{1}{k}=\\infty,$$\nso the convergence is conditional. Consequently:\n- A is false because there exist sequences of signs (e.g., $\\xi_{k}\\equiv 1$) for which the series diverges.\n- C and D are false because the series converges almost surely to a finite limit.\n- E is false because it does not oscillate without a limit; it converges almost surely.\n- F is false because the event of convergence has probability $1$, not strictly between $0$ and $1$.\n\nThus, the correct statement is B.", "answer": "$$\\boxed{B}$$", "id": "1352900"}]}