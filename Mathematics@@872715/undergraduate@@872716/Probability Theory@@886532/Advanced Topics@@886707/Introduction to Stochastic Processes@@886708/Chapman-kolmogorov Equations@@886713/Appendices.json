{"hands_on_practices": [{"introduction": "The Chapman-Kolmogorov equations provide a powerful tool for predicting the future evolution of a stochastic system. This first exercise grounds the theory in a practical scenario: tracking a rental car's location over two days. By calculating the two-step transition probability, you will directly apply the core principle of summing over all possible intermediate states, building a foundational understanding of how probabilities evolve in a Markov chain. [@problem_id:1347924]", "problem": "A car rental company operates a fleet of vehicles between three major metropolitan areas: Aurelia (A), Borealis (B), and Cygnus (C). The company's data analytics team has modeled the daily movement of cars. Based on their model, the probabilities of a car's location on any given day depend only on its location the previous day.\n\n- A car in Aurelia on a given day has a 50% chance of staying in Aurelia, a 30% chance of being moved to Borealis, and a 20% chance of being moved to Cygnus the next day.\n- A car in Borealis has a 20% chance of being moved to Aurelia, a 60% chance of staying in Borealis, and a 20% chance of being moved to Cygnus the next day.\n- A car in Cygnus has a 10% chance of being moved to Aurelia, a 10% chance of being moved to Borealis, and an 80% chance of staying in Cygnus the next day.\n\nA customer rents a car from the Aurelia branch. Assuming the model is accurate, what is the probability that this specific car will be located in Cygnus after exactly two days? Express your answer as a decimal rounded to three significant figures.", "solution": "Let the carâ€™s location on day $t$ be $X_{t}\\in\\{A,B,C\\}$. By the Markov property and the law of total probability,\n$$\nP(X_{2}=C \\mid X_{0}=A)=\\sum_{y\\in\\{A,B,C\\}} P(X_{1}=y \\mid X_{0}=A)\\,P(X_{2}=C \\mid X_{1}=y).\n$$\nFrom the given transition probabilities,\n$$\nP(X_{1}=A \\mid X_{0}=A)=0.5,\\quad P(X_{2}=C \\mid X_{1}=A)=0.2,\n$$\n$$\nP(X_{1}=B \\mid X_{0}=A)=0.3,\\quad P(X_{2}=C \\mid X_{1}=B)=0.2,\n$$\n$$\nP(X_{1}=C \\mid X_{0}=A)=0.2,\\quad P(X_{2}=C \\mid X_{1}=C)=0.8.\n$$\nTherefore,\n$$\nP(X_{2}=C \\mid X_{0}=A)=(0.5)(0.2)+(0.3)(0.2)+(0.2)(0.8)=0.10+0.06+0.16=0.32.\n$$\nRounded to three significant figures, this is $0.320$.", "answer": "$$\\boxed{0.320}$$", "id": "1347924"}, {"introduction": "Beyond predicting the future, we can also use the Chapman-Kolmogorov framework to make inferences about the past. This practice presents a common scenario in diagnostics and monitoring: we know the state of a system at the beginning and end of an interval, and we want to know the probability of it having passed through a specific intermediate state. This problem requires you to combine the Chapman-Kolmogorov logic with conditional probability to derive a general solution in terms of system parameters, a key skill in theoretical modeling. [@problem_id:1347939]", "problem": "A monitoring system tracks the state of a critical server component at the end of each hour. The component can be in one of three states: State 1 (Optimal), State 2 (Degraded), or State 3 (Failed). The transitions between states form a discrete-time Markov chain. The one-step transition probability matrix, $P$, where $P_{ij} = P(X_{t+1}=j | X_t=i)$, is given by:\n$$\nP = \\begin{pmatrix}\n1 - 2\\alpha  2\\alpha  0 \\\\\n\\beta  1 - \\beta - \\gamma  \\gamma \\\\\n0  0  1\n\\end{pmatrix}\n$$\nHere, $X_t$ denotes the state of the component at hour $t$. The parameters $\\alpha, \\beta, \\gamma$ are constants that satisfy $0  2\\alpha  1$, $0  \\beta  1$, $0  \\gamma  1$, and $\\beta + \\gamma  1$.\n\nSuppose the component is observed to be in the Optimal state (State 1) at time $t=0$. Two hours later, at time $t=2$, it is observed to be in the Degraded state (State 2).\n\nGiven these observations, calculate the conditional probability that the component was in the Degraded state (State 2) at time $t=1$. Express your answer as a closed-form analytic expression in terms of $\\alpha$, $\\beta$, and $\\gamma$.", "solution": "We are asked to compute the conditional probability $P(X_{1}=2 \\mid X_{0}=1, X_{2}=2)$ for a time-homogeneous Markov chain with transition matrix $P$. By the definition of conditional probability and the Markov property (future depends on the present state only), we use\n$$\nP(X_{1}=j \\mid X_{0}=i, X_{2}=k) = \\frac{P(X_{1}=j, X_{2}=k \\mid X_{0}=i)}{P(X_{2}=k \\mid X_{0}=i)}.\n$$\nThe numerator factors by the Markov property as\n$$\nP(X_{1}=j, X_{2}=k \\mid X_{0}=i) = P(X_{1}=j \\mid X_{0}=i)\\,P(X_{2}=k \\mid X_{1}=j) = P_{i j} P_{j k},\n$$\nand the denominator, by the law of total probability and the Markov property, is\n$$\nP(X_{2}=k \\mid X_{0}=i) = \\sum_{m} P(X_{2}=k \\mid X_{1}=m)\\,P(X_{1}=m \\mid X_{0}=i) = \\sum_{m} P_{i m} P_{m k}.\n$$\nTherefore,\n$$\nP(X_{1}=j \\mid X_{0}=i, X_{2}=k) = \\frac{P_{i j} P_{j k}}{\\sum_{m} P_{i m} P_{m k}}.\n$$\nWe substitute $i=1$, $k=2$, and use the given $P$. The nonzero relevant entries are\n$$\nP_{1,1} = 1 - 2\\alpha,\\quad P_{1,2} = 2\\alpha,\\quad P_{2,2} = 1 - \\beta - \\gamma,\\quad P_{1,3} = 0.\n$$\nSince $P_{1,3}=0$, the sum in the denominator reduces to $m \\in \\{1,2\\}$. For $j=2$, the numerator is\n$$\nP_{1,2} P_{2,2} = (2\\alpha)(1 - \\beta - \\gamma).\n$$\nThe denominator is\n$$\n\\sum_{m} P_{1,m} P_{m,2} = P_{1,1} P_{1,2} + P_{1,2} P_{2,2} = (1 - 2\\alpha)(2\\alpha) + (2\\alpha)(1 - \\beta - \\gamma) = 2\\alpha \\big(2 - 2\\alpha - \\beta - \\gamma\\big).\n$$\nHence,\n$$\nP(X_{1}=2 \\mid X_{0}=1, X_{2}=2) = \\frac{(2\\alpha)(1 - \\beta - \\gamma)}{2\\alpha \\big(2 - 2\\alpha - \\beta - \\gamma\\big)} = \\frac{1 - \\beta - \\gamma}{2 - 2\\alpha - \\beta - \\gamma}.\n$$\nThis expression is well-defined under the given constraints $02\\alpha1$, $0\\beta1$, $0\\gamma1$, and $\\beta+\\gamma1$, which ensure the denominator is positive.", "answer": "$$\\boxed{\\frac{1-\\beta-\\gamma}{2-2\\alpha-\\beta-\\gamma}}$$", "id": "1347939"}, {"introduction": "A common simplification is to assume that transition probabilities are constant over time. However, many real-world systems are non-homogeneous, with rules that change based on external factors or time itself. This final practice challenges you to apply the Chapman-Kolmogorov principle to such a system, where different transition matrices govern the process at different time steps. This will solidify your understanding that the equations are fundamentally about composing sequential transitions, not just raising a single matrix to a power. [@problem_id:1347946]", "problem": "A simplified model for a specialized digital component describes its state at discrete time steps $t=0, 1, 2, \\dots$. The component can exist in one of three distinct states: 'low' (state 1), 'high' (state 2), or 'error' (state 3). The state transitions are governed by a non-homogeneous Markov chain process, influenced by two different, periodically applied external fields.\n\nThe one-step transition probability matrix from time $t=n$ to $t=n+1$ depends on whether $n$ is even or odd.\n- If the time step $n$ is even ($n=0, 2, 4, \\dots$), the transition probabilities are given by the matrix $Q$.\n- If the time step $n$ is odd ($n=1, 3, 5, \\dots$), the transition probabilities are given by the matrix $P$.\n\nThe matrices $P$ and $Q$ are defined as follows, where the entry in the $i$-th row and $j$-th column represents the probability of transitioning from state $i$ to state $j$:\n\n$$\nQ = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  0 \\\\ \\frac{1}{5}  \\frac{3}{5}  \\frac{1}{5} \\\\ \\frac{1}{10}  \\frac{1}{10}  \\frac{4}{5} \\end{pmatrix}, \\quad\nP = \\begin{pmatrix} \\frac{7}{10}  \\frac{2}{10}  \\frac{1}{10} \\\\ \\frac{1}{10}  \\frac{8}{10}  \\frac{1}{10} \\\\ \\frac{3}{10}  0  \\frac{7}{10} \\end{pmatrix}\n$$\n\nAssuming the component is in the 'low' state (state 1) at time $t=0$, what is the probability that it will be in the 'error' state (state 3) at time $t=2$? Express your answer as a single fraction in simplest form.", "solution": "We model the state distribution as a row vector. Let the initial distribution at $t=0$ be $p_{0}=(1,0,0)$ since the component starts in state $1$. The process uses $Q$ from $t=0$ to $t=1$ (even $n$) and $P$ from $t=1$ to $t=2$ (odd $n$), so\n$$\np_{2}=p_{0} Q P.\n$$\nThe desired probability is the component being in state $3$ at $t=2$, which is the third component of $p_{2}$:\n$$\n[p_{2}]_{3} = \\sum_{k=1}^{3} (p_{0} Q)_{k} P_{k3} = \\sum_{k=1}^{3} Q_{1k} P_{k3}.\n$$\nUsing the given entries,\n$$\n[p_{2}]_{3} = Q_{11} P_{13} + Q_{12} P_{23} + Q_{13} P_{33} = \\frac{1}{2}\\cdot \\frac{1}{10} + \\frac{1}{2}\\cdot \\frac{1}{10} + 0\\cdot \\frac{7}{10} = \\frac{1}{10}.\n$$\nThus, the probability the component is in state $3$ at time $t=2$ is $\\frac{1}{10}$.", "answer": "$$\\boxed{\\frac{1}{10}}$$", "id": "1347946"}]}