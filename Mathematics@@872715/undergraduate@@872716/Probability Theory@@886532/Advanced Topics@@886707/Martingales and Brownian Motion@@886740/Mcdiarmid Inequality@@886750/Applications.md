## Applications and Interdisciplinary Connections

Having established the formal statement of McDiarmid's inequality in the preceding chapter, we now turn our attention to its vast utility. The true power of a theoretical result in probability is measured by its ability to provide insight into complex phenomena across diverse scientific and engineering disciplines. McDiarmid's inequality is a premier example of such a tool, offering a robust method for proving that complicated random variables, which may not be simple sums of independent components, are nevertheless tightly concentrated around their mean.

This chapter will explore a curated selection of applications, demonstrating how the core principle—bounding the impact of changing a single independent variable—is deployed in various contexts. Our goal is not to re-derive the inequality but to build an intuitive understanding of its versatility. We will see how it provides crucial guarantees for the performance of computer algorithms, explains the structural properties of [random networks](@entry_id:263277), quantifies geometric features of random point sets, and even illuminates models in statistical physics and information theory. Through these examples, we will appreciate how a single, elegant idea can unify the analysis of many seemingly disparate problems.

### Computer Science and Combinatorics

Many fundamental problems in computer science involve analyzing the collective behavior of systems built from numerous independent random choices. McDiarmid's inequality has become an indispensable tool in this domain, particularly in the [analysis of algorithms](@entry_id:264228), data structures, and load-balancing schemes.

#### Load Balancing and Hashing

A canonical problem in [distributed computing](@entry_id:264044) is the "balls and bins" model, which serves as an abstraction for tasks like hashing data chunks into storage servers or balancing computational jobs across processors. Consider a scenario where $n$ items (e.g., jobs) are independently and uniformly assigned to one of $m$ bins (e.g., servers). The performance and stability of such a system often depend on how evenly the load is distributed.

One critical metric is the maximum load on any single server. Let this be the random variable $M$. We can view $M$ as a function of the $n$ independent server assignments. If we change the assignment of a single job, moving it from server A to server B, the load on server A decreases by one, the load on server B increases by one, and all other loads are unchanged. Consequently, the maximum load across all servers can change by at most 1. This establishes a bounded difference constant of $c_i = 1$ for all $n$ assignments. McDiarmid's inequality then provides a strong exponential bound on the probability that the maximum load exceeds its expectation, which is essential for provisioning server capacity and preventing overloads. [@problem_id:1372513]

A related metric is the load imbalance, defined as the difference between the maximum and minimum loads, $R = \max_i X_i - \min_i X_i$. Again, consider changing the assignment of a single job from server A to server B. The new maximum load can be at most one greater than the old maximum, while the new minimum load can be at most one less than the old minimum. This implies that the difference $R$ can increase by at most 2. A symmetric argument shows it can decrease by at most 2. Therefore, the function for load imbalance has a bounded difference of $c_i = 2$. This allows us to bound the probability of large deviations in system imbalance. [@problem_id:1372527]

Finally, we can also analyze the number of empty servers, $N_{\text{empty}}$. When a single job is reassigned from server A to B, the number of empty servers can change by at most 1. This occurs if server A had only that one job (becoming empty) or if server B was previously empty (becoming non-empty). In all cases, the change is bounded by 1, so $c_i = 1$. McDiarmid's inequality can thus provide concentration bounds for the number of unused resources in the system. [@problem_id:1372549]

#### Analysis of Complex Algorithms

McDiarmid's inequality is not limited to simple counting arguments. It can be applied to functions defined by complex algorithmic processes. One prominent example comes from bioinformatics and stringology: the length of the Longest Common Subsequence (LCS) of two random [binary strings](@entry_id:262113), $X$ and $Y$, of length $n$. The length $L(X,Y)$ is a function of the $2n$ independent bits comprising the strings. If we change a single bit in either string, what is the maximum possible change to the LCS length? A careful analysis shows that adding, deleting, or changing one character in a sequence can change the length of the LCS by at most 1. This remarkably simple property ($c_i = 1$) allows us to apply McDiarmid's inequality to prove that the length of the LCS of two random strings is highly concentrated around its expected value, a foundational result in the [probabilistic analysis](@entry_id:261281) of algorithms. [@problem_id:1372554]

Another combinatorial application is [counting inversions](@entry_id:637929) in a [random permutation](@entry_id:270972). If we generate a permutation of $\{1, \dots, n\}$ by taking the ranks of $n$ i.i.d. [continuous random variables](@entry_id:166541), the number of inversions is a function of these $n$ variables. An inversion is a pair $(i, j)$ with $i  j$ but $X_i > X_j$. If we change the value of a single variable, $X_k$, its rank relative to the other $n-1$ variables can change completely. It can go from being the smallest to the largest, or vice versa. This can affect its inversion status with respect to all $n-1$ other elements, leading to a bounded difference of $c_i = n-1$. While this bound is not a small constant, it is still powerful enough to establish a non-trivial concentration result. This example illustrates that the strength of the resulting [probability bound](@entry_id:273260) is highly dependent on the magnitude of the constants $c_i$. [@problem_id:1372543]

### Random Graphs and Network Models

Random graphs are mathematical models for real-world networks, from social networks to communication infrastructure. The properties of these graphs are often complex global features that emerge from simple local random rules. McDiarmid's inequality is a standard technique for showing that many such properties are stable and predictable.

In the classic Erdős-Rényi model $G(n,p)$, an edge exists between any pair of vertices with probability $p$, independent of all other edges. The [independent variables](@entry_id:267118) are the $\binom{n}{2}$ Bernoulli trials for the presence of each edge. Consider the number of [isolated vertices](@entry_id:269995), $X$. If we change the state of a single potential edge $(u,v)$ (i.e., add or remove it), this can only affect the degrees of vertices $u$ and $v$. The isolation status of any other vertex remains unchanged. At most, we can eliminate two [isolated vertices](@entry_id:269995) (by adding an edge between them) or create two (by removing the only edge connecting two vertices of degree one). Thus, the number of [isolated vertices](@entry_id:269995) changes by at most 2. With $c_i=2$ for all $\binom{n}{2}$ edge variables, McDiarmid's inequality shows that $X$ is sharply concentrated. [@problem_id:1372525]

A similar logic applies to vertex-centric models. Consider a graph where each vertex is independently and randomly assigned a property, such as a color from a set of choices. We can then study global properties that depend on these vertex states. For instance, in a graph where every vertex is colored black or white with equal probability, we can count the number of monochromatic edges (edges connecting two vertices of the same color). The independent variables are the color choices for each of the $n$ vertices. Changing the color of a single vertex $v$ can only affect the monochromatic status of the edges incident to it. If a vertex has degree $d(v)$, then changing its color can change the total count by at most $d(v)$. The bounded difference constant $c_v$ is therefore $d(v)$, and the sum in McDiarmid's inequality becomes $\sum_{v} d(v)^2$. For a graph with maximum degree $\Delta$, a simpler uniform bound is $c_v \le \Delta$. This demonstrates how the graph's structure directly influences the concentration of its properties. [@problem_id:1345097]

In a [random geometric graph](@entry_id:272724), $n$ vertices are placed randomly in a space (e.g., the unit square), and an edge connects two vertices if their distance is below a threshold $r$. Here, the independent variables are the positions of the $n$ vertices. If we consider the total number of edges, $N$, and change the position of a single vertex, we can potentially alter its connectivity to all $n-1$ other vertices. This means that up to $n-1$ edges can be created or destroyed. This gives a bounded difference of $c_i = n-1$, which, as in the inversions example, leads to a meaningful but weaker concentration result than in cases where $c_i$ is a small constant. [@problem_id:1372514]

### Computational and Random Geometry

Geometric functionals of random point sets are a natural domain for McDiarmid's inequality. The underlying geometry of the space often provides the tools, like the triangle inequality, needed to establish the bounded difference property.

A classic and celebrated application is to the length of the Traveling Salesperson Problem (TSP) tour for $n$ points chosen randomly in a unit square. Let $L_n$ be the length of the shortest tour. $L_n$ is an extremely complex function of the $n$ point locations. However, consider the effect of moving a single point $X_i$ to a new position $X_i'$. We can construct a new tour from the old optimal tour by removing the two edges connected to $X_i$ and adding two new edges connected to $X_i'$. By the triangle inequality, the increase in length is at most $2d(X_i, X_i')$, where $d(\cdot, \cdot)$ is the Euclidean distance. The maximum possible value for $d(X_i, X_i')$ in the unit square is $\sqrt{2}$. Thus, the length of the TSP tour changes by at most $2\sqrt{2}$ when a single point is moved. This provides a constant $c_i = 2\sqrt{2}$, yielding a strong concentration result for $L_n$. [@problem_id:1372515]

Simpler geometric functions can be analyzed similarly. The diameter of a set of $n$ random points in a unit disk is the maximum distance between any two points. This is a function of the $n$ independent point locations. If we move one point $X_k$ to $X_k'$, the new diameter can be shown, via the triangle inequality, to change by at most $d(X_k, X_k')$. The maximum value of this distance within a [unit disk](@entry_id:172324) is 2 (its geometric diameter). Therefore, the bounded difference constant is $c_i = 2$. [@problem_id:1372545] A similar argument can be made for the perimeter of the convex hull of random points, where repositioning a single point in the unit square can change the perimeter by at most $2\sqrt{2}$. [@problem_id:1372544]

The inequality also applies to [stochastic processes](@entry_id:141566) like [random walks](@entry_id:159635). For a [symmetric random walk](@entry_id:273558) of $n$ steps on the 2D integer lattice starting from the origin, the final displacement $D_n$ is the Euclidean distance of the final position from the origin. $D_n$ is a function of the $n$ independent random steps. If we change a single step vector $\xi_k$ to $\xi_k'$, the final position changes by $\xi_k' - \xi_k$. By the [reverse triangle inequality](@entry_id:146102), the change in the final displacement is bounded by the magnitude of this vector difference, $|D_n - D_n'| \le \|\xi_k' - \xi_k\|_2$. Since each step is a [unit vector](@entry_id:150575), the maximum value of this difference is 2. This gives $c_i = 2$ for each step, proving that the final displacement is tightly concentrated. [@problem_id:1372537]

### Advanced and Interdisciplinary Frontiers

The applicability of McDiarmid's inequality extends to highly specialized and advanced models at the frontiers of science.

#### Statistical Physics

In theoretical physics, the Sherrington-Kirkpatrick (SK) model is a foundational model of a spin glass, a disordered magnetic system. The system's state is described by a configuration of $n$ spins $\sigma \in \{-1, 1\}^n$, and its energy (Hamiltonian) depends on random interaction couplings $J_{ij}$ between every pair of spins. The [ground state energy](@entry_id:146823) $E_n$ is the minimum possible energy over all spin configurations for a given realization of the random couplings. $E_n$ is a random variable that is a function of the $\binom{n}{2}$ independent couplings $J_{ij}$. If we alter a single coupling $J_{k\ell}$, the Hamiltonian $H_n(\sigma)$ for any fixed $\sigma$ changes by at most $C/\sqrt{n}$ for some constant $C$, due to the $1/\sqrt{n}$ normalization factor in the model's definition. Since the minimum of a set of functions can't change by more than the maximum change of any individual function, the [ground state energy](@entry_id:146823) $E_n$ also has a bounded difference of $c_{ij} = C/\sqrt{n}$. This allows for a powerful concentration result on the ground state energy, a key quantity in the study of these complex systems. [@problem_id:1372528]

#### Information Theory

The inequality proves invaluable in information theory, especially when analyzing quantities that are implicitly defined. Consider compressing a random sequence of $n$ symbols using a Huffman code. A standard Huffman code is optimal for a known, fixed probability distribution. However, if the distribution is unknown, one can build a code based on the empirical frequencies of symbols in the sequence itself. The total compressed length $Z_n$ is then a function of the $n$ independent random symbols. This is a very complex function, as the codeword lengths themselves depend on the entire sequence. Despite this "self-referential" nature, it can be shown that changing a single symbol in the input sequence changes the total compressed length by at most an amount proportional to $\log n$. This establishes a bounded difference property (with $c_i \approx C \log n$) and allows McDiarmid's inequality to provide guarantees on the performance of such [adaptive compression](@entry_id:275787) schemes. [@problem_id:1372521]

#### Random Matrix Theory

Random matrices are fundamental objects with applications in physics, statistics, and engineering. Consider an $n \times n$ matrix where each entry is an independent Bernoulli random variable. A key property is its rank over a [finite field](@entry_id:150913) like $\mathbb{F}_2$. The rank is a highly complex, global property of the matrix. However, it is a basic result of linear algebra that changing a single entry of a matrix can change its rank by at most 1. This immediately establishes that the rank, as a function of the $n^2$ independent matrix entries, has a bounded difference of $c_i=1$. McDiarmid's inequality then directly implies that the rank of a large random binary matrix is extremely concentrated around its expected value, a result of significant importance in areas like coding theory and network analysis. [@problem_id:1372538]