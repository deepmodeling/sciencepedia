{"hands_on_practices": [{"introduction": "A crucial aspect of Kullback-Leibler divergence is that it is not a true mathematical distance, primarily because it is asymmetric. This practice provides a direct, hands-on demonstration of this property by calculating the divergence between two simple probabilistic models in both directions, revealing that the 'information lost' when approximating P with Q is not the same as when approximating Q with P [@problem_id:1370270].", "problem": "Two statisticians, Alice and Bob, are tasked with creating a probabilistic model for the outcome of a single toss of a potentially biased coin. The outcome can be either heads (H) or tails (T). Alice proposes a model, which we'll call distribution $P$, where the probability of heads is $p_A = 0.2$. Bob proposes a different model, distribution $Q$, where the probability of heads is $p_B = 0.7$.\n\nTo quantify the difference between these two models, they decide to use the Kullback-Leibler (KL) divergence, also known as relative entropy. For two discrete probability distributions $P$ and $Q$ defined on the same probability space $\\mathcal{X}$, the KL divergence from $Q$ to $P$ is given by the formula:\n$$\nD_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)\n$$\nThis value measures the information lost when $Q$ is used to approximate $P$.\n\nYour task is to calculate two values:\n1. The KL divergence from Bob's model ($Q$) to Alice's model ($P$), which is $D_{KL}(P || Q)$.\n2. The KL divergence from Alice's model ($P$) to Bob's model ($Q$), which is $D_{KL}(Q || P)$.\n\nCalculate both $D_{KL}(P || Q)$ and $D_{KL}(Q || P)$, in that order, and provide the results as a pair of numerical values. Round your final answers to four significant figures.", "solution": "We model a single coin toss with two outcomes $\\mathcal{X}=\\{H,T\\}$. Alice’s model $P$ has $P(H)=p_{A}=0.2$ and $P(T)=1-p_{A}=0.8$. Bob’s model $Q$ has $Q(H)=p_{B}=0.7$ and $Q(T)=1-p_{B}=0.3$.\n\nBy definition, for discrete distributions on the same space,\n$$\nD_{KL}(P \\,\\|\\, Q)=\\sum_{x\\in\\mathcal{X}} P(x)\\,\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right).\n$$\n\nFirst, compute $D_{KL}(P \\,\\|\\, Q)$:\n$$\nD_{KL}(P \\,\\|\\, Q)=P(H)\\ln\\!\\left(\\frac{P(H)}{Q(H)}\\right)+P(T)\\ln\\!\\left(\\frac{P(T)}{Q(T)}\\right).\n$$\nSubstitute $P(H)=\\frac{1}{5}$, $Q(H)=\\frac{7}{10}$, $P(T)=\\frac{4}{5}$, $Q(T)=\\frac{3}{10}$:\n$$\nD_{KL}(P \\,\\|\\, Q)=\\frac{1}{5}\\ln\\!\\left(\\frac{\\frac{1}{5}}{\\frac{7}{10}}\\right)+\\frac{4}{5}\\ln\\!\\left(\\frac{\\frac{4}{5}}{\\frac{3}{10}}\\right)\n=\\frac{1}{5}\\ln\\!\\left(\\frac{2}{7}\\right)+\\frac{4}{5}\\ln\\!\\left(\\frac{8}{3}\\right).\n$$\nNumerically,\n$$\n\\frac{1}{5}\\ln\\!\\left(\\frac{2}{7}\\right)\\approx 0.2\\times(-1.2527629685)\\approx -0.2505525937,\\quad\n\\frac{4}{5}\\ln\\!\\left(\\frac{8}{3}\\right)\\approx 0.8\\times 0.9808292530\\approx 0.7846634024,\n$$\nso\n$$\nD_{KL}(P \\,\\|\\, Q)\\approx -0.2505525937+0.7846634024\\approx 0.5341108087.\n$$\n\nNext, compute $D_{KL}(Q \\,\\|\\, P)$:\n$$\nD_{KL}(Q \\,\\|\\, P)=Q(H)\\ln\\!\\left(\\frac{Q(H)}{P(H)}\\right)+Q(T)\\ln\\!\\left(\\frac{Q(T)}{P(T)}\\right).\n$$\nSubstitute $Q(H)=\\frac{7}{10}$, $P(H)=\\frac{1}{5}$, $Q(T)=\\frac{3}{10}$, $P(T)=\\frac{4}{5}$:\n$$\nD_{KL}(Q \\,\\|\\, P)=\\frac{7}{10}\\ln\\!\\left(\\frac{\\frac{7}{10}}{\\frac{1}{5}}\\right)+\\frac{3}{10}\\ln\\!\\left(\\frac{\\frac{3}{10}}{\\frac{4}{5}}\\right)\n=\\frac{7}{10}\\ln\\!\\left(\\frac{7}{2}\\right)+\\frac{3}{10}\\ln\\!\\left(\\frac{3}{8}\\right).\n$$\nNumerically,\n$$\n\\frac{7}{10}\\ln\\!\\left(\\frac{7}{2}\\right)\\approx 0.7\\times 1.2527629685\\approx 0.8769340779,\\quad\n\\frac{3}{10}\\ln\\!\\left(\\frac{3}{8}\\right)\\approx 0.3\\times(-0.9808292530)\\approx -0.2942487759,\n$$\nso\n$$\nD_{KL}(Q \\,\\|\\, P)\\approx 0.8769340779-0.2942487759\\approx 0.5826853020.\n$$\n\nRounding both results to four significant figures gives $D_{KL}(P \\,\\|\\, Q)\\approx 0.5341$ and $D_{KL}(Q \\,\\|\\, P)\\approx 0.5827$.", "answer": "$$\\boxed{\\begin{pmatrix}0.5341 & 0.5827\\end{pmatrix}}$$", "id": "1370270"}, {"introduction": "Moving from discrete outcomes to continuous variables requires us to adapt our formula for KL divergence from a sum to an integral. This exercise will guide you through deriving the KL divergence between two exponential distributions, which are frequently used to model waiting times or lifetimes. The result is a general, closed-form expression that elegantly captures the divergence purely in terms of the models' parameters [@problem_id:1370289].", "problem": "In statistical modeling, it is often necessary to quantify the difference between a candidate model and a true, underlying probability distribution. The Kullback-Leibler (KL) divergence provides a measure of this difference.\n\nThe probability density function (PDF), $f(x; \\lambda)$, for a random variable $X$ following an exponential distribution with a rate parameter $\\lambda > 0$ is given by:\n$$\nf(x; \\lambda) = \\begin{cases} \\lambda \\exp(-\\lambda x) & \\text{if } x \\ge 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\n$$\nFor two continuous probability distributions $P$ and $Q$ with PDFs $p(x)$ and $q(x)$ respectively, the KL divergence from $Q$ to $P$, denoted $D_{KL}(P || Q)$, is defined as:\n$$\nD_{KL}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx\n$$\nThe integral is evaluated over the domain where $p(x) > 0$.\n\nConsider two models for the lifetime of a device. Model $P$ assumes the lifetime follows an exponential distribution with rate parameter $\\lambda_p$. Model $Q$ assumes the lifetime follows an exponential distribution with a different rate parameter $\\lambda_q$.\n\nYour task is to calculate the KL divergence $D_{KL}(P || Q)$. Provide a closed-form analytic expression for your answer in terms of $\\lambda_p$ and $\\lambda_q$.", "solution": "We are given two exponential distributions: model $P$ with rate parameter $\\lambda_{p} > 0$ and model $Q$ with rate parameter $\\lambda_{q} > 0$. Their probability density functions are\n$$\np(x) = \\begin{cases} \\lambda_{p} \\exp(-\\lambda_{p} x) & \\text{if } x \\ge 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}, \\quad\nq(x) = \\begin{cases} \\lambda_{q} \\exp(-\\lambda_{q} x) & \\text{if } x \\ge 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}.\n$$\nThe Kullback-Leibler divergence from $Q$ to $P$ is defined as\n$$\nD_{KL}(P \\,||\\, Q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\!\\left(\\frac{p(x)}{q(x)}\\right) \\, dx,\n$$\nand the integral is taken over the region where $p(x) > 0$. Here, $p(x) > 0$ only for $x \\ge 0$, and on this region $q(x) > 0$ as well because $\\lambda_{q} > 0$.\n\nFor $x \\ge 0$, compute the log-density ratio:\n$$\n\\frac{p(x)}{q(x)} = \\frac{\\lambda_{p} \\exp(-\\lambda_{p} x)}{\\lambda_{q} \\exp(-\\lambda_{q} x)} = \\frac{\\lambda_{p}}{\\lambda_{q}} \\exp\\!\\big(-( \\lambda_{p} - \\lambda_{q}) x\\big),\n$$\nso\n$$\n\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right) = \\ln\\!\\left(\\frac{\\lambda_{p}}{\\lambda_{q}}\\right) - (\\lambda_{p} - \\lambda_{q}) x = \\ln\\!\\left(\\frac{\\lambda_{p}}{\\lambda_{q}}\\right) + (\\lambda_{q} - \\lambda_{p}) x.\n$$\nTherefore,\n$$\nD_{KL}(P \\,||\\, Q) = \\int_{0}^{\\infty} \\lambda_{p} \\exp(-\\lambda_{p} x) \\left[ \\ln\\!\\left(\\frac{\\lambda_{p}}{\\lambda_{q}}\\right) + (\\lambda_{q} - \\lambda_{p}) x \\right] dx.\n$$\nSplit the integral into two parts:\n$$\nD_{KL}(P \\,||\\, Q) = \\ln\\!\\left(\\frac{\\lambda_{p}}{\\lambda_{q}}\\right) \\int_{0}^{\\infty} \\lambda_{p} \\exp(-\\lambda_{p} x) \\, dx + (\\lambda_{q} - \\lambda_{p}) \\int_{0}^{\\infty} \\lambda_{p} x \\exp(-\\lambda_{p} x) \\, dx.\n$$\nThe first integral equals $1$ because $p(x)$ is a probability density:\n$$\n\\int_{0}^{\\infty} \\lambda_{p} \\exp(-\\lambda_{p} x) \\, dx = 1.\n$$\nFor the second integral, perform the substitution $u = \\lambda_{p} x$, $dx = du/\\lambda_{p}$:\n$$\n\\int_{0}^{\\infty} \\lambda_{p} x \\exp(-\\lambda_{p} x) \\, dx = \\int_{0}^{\\infty} \\lambda_{p} \\left(\\frac{u}{\\lambda_{p}}\\right) \\exp(-u) \\frac{du}{\\lambda_{p}} = \\frac{1}{\\lambda_{p}} \\int_{0}^{\\infty} u \\exp(-u) \\, du = \\frac{1}{\\lambda_{p}},\n$$\nsince $\\int_{0}^{\\infty} u \\exp(-u) \\, du = 1$.\n\nPutting these results together,\n$$\nD_{KL}(P \\,||\\, Q) = \\ln\\!\\left(\\frac{\\lambda_{p}}{\\lambda_{q}}\\right) + (\\lambda_{q} - \\lambda_{p}) \\cdot \\frac{1}{\\lambda_{p}} = \\ln\\!\\left(\\frac{\\lambda_{p}}{\\lambda_{q}}\\right) + \\frac{\\lambda_{q}}{\\lambda_{p}} - 1.\n$$\nThis is a closed-form expression in terms of $\\lambda_{p}$ and $\\lambda_{q}$, and it is nonnegative with equality if and only if $\\lambda_{p} = \\lambda_{q}$.", "answer": "$$\\boxed{\\ln\\!\\left(\\frac{\\lambda_{p}}{\\lambda_{q}}\\right) + \\frac{\\lambda_{q}}{\\lambda_{p}} - 1}$$", "id": "1370289"}, {"introduction": "One of the most powerful applications of KL divergence is in approximating complex distributions with simpler ones, a cornerstone of modern machine learning. This advanced practice explores the minimization of the *reverse* KL divergence, $D_{KL}(Q||P)$, to find an optimal Gaussian approximation for a more complex, bimodal distribution. This exercise uniquely illustrates the 'mode-seeking' nature of this objective, where the approximation tends to focus on one of the peaks of the true distribution [@problem_id:1370260].", "problem": "In many areas of machine learning and statistical physics, it is often necessary to approximate a complex probability distribution $P(x)$ with a simpler, tractable one, such as a Gaussian distribution $Q(x)$. A common method for finding the best approximation is to minimize the Kullback-Leibler (KL) divergence between the two distributions.\n\nConsider a target probability distribution $P(x)$ for a single continuous variable $x \\in (-\\infty, \\infty)$ defined by the potential energy function $U(x) = \\alpha x^4 - \\beta x^2$, where $\\alpha$ and $\\beta$ are given positive real constants. The probability density function is given by $P(x) = \\frac{1}{Z} \\exp(-U(x))$, where $Z$ is a normalization constant. This distribution has two distinct modes (peaks) located symmetrically around $x=0$.\n\nWe wish to approximate $P(x)$ with a Gaussian distribution $Q(x) = \\mathcal{N}(x \\mid \\mu_Q, \\sigma_Q^2)$, which is defined as $Q(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_Q^2}} \\exp\\left(-\\frac{(x - \\mu_Q)^2}{2\\sigma_Q^2}\\right)$. The quality of the approximation is measured by the reverse KL divergence, $D_{KL}(Q||P)$, given by:\n$$\nD_{KL}(Q||P) = \\int_{-\\infty}^{\\infty} Q(x) \\ln\\left(\\frac{Q(x)}{P(x)}\\right) dx\n$$\nFor this problem, to make the optimization tractable, we assume the variance of the approximating Gaussian is fixed to a constant value $\\sigma_Q^2 = v_0$. The value of $v_0$ is chosen such that $v_0 < \\frac{\\beta}{6\\alpha}$. Your task is to find the optimal value of the mean $\\mu_Q$ that minimizes this KL divergence.\n\nFind a symbolic expression for the positive value of the optimal mean, $\\mu_Q^*$, in terms of the parameters $\\alpha$, $\\beta$, and $v_0$.", "solution": "We start from the definition of the reverse Kullback-Leibler divergence,\n$$\nD_{KL}(Q||P)=\\int_{-\\infty}^{\\infty} Q(x)\\ln\\left(\\frac{Q(x)}{P(x)}\\right)\\,dx= \\mathbb{E}_{Q}[\\ln Q(x)]-\\mathbb{E}_{Q}[\\ln P(x)].\n$$\nSince $P(x)=\\frac{1}{Z}\\exp(-U(x))$ with $U(x)=\\alpha x^{4}-\\beta x^{2}$, we have $\\ln P(x)=-U(x)-\\ln Z$, hence\n$$\nD_{KL}(Q||P)=\\mathbb{E}_{Q}[\\ln Q(x)]+\\mathbb{E}_{Q}[U(x)]+\\ln Z.\n$$\nThe normalizing constant $\\ln Z$ does not depend on the Gaussian parameters, and for $Q(x)=\\mathcal{N}(x\\mid \\mu_{Q},v_{0})$ with fixed variance $v_{0}$,\n$$\n\\mathbb{E}_{Q}[\\ln Q(x)]=-\\frac{1}{2}\\ln(2\\pi v_{0})-\\frac{1}{2}\\mathbb{E}_{Q}\\left[\\frac{(x-\\mu_{Q})^{2}}{v_{0}}\\right]=-\\frac{1}{2}\\ln(2\\pi v_{0})-\\frac{1}{2},\n$$\nwhich is independent of $\\mu_{Q}$. Therefore minimizing $D_{KL}(Q||P)$ over $\\mu_{Q}$ is equivalent to minimizing $\\mathbb{E}_{Q}[U(x)]$ over $\\mu_{Q}$:\n$$\n\\min_{\\mu_{Q}}\\ \\mathbb{E}_{Q}[U(x)]=\\min_{\\mu_{Q}}\\ \\left(\\alpha\\,\\mathbb{E}_{Q}[x^{4}]-\\beta\\,\\mathbb{E}_{Q}[x^{2}]\\right).\n$$\nFor $x\\sim \\mathcal{N}(\\mu_{Q},v_{0})$, write $x=\\mu_{Q}+\\varepsilon$ with $\\varepsilon\\sim \\mathcal{N}(0,v_{0})$. Using standard Gaussian moment formulas,\n$$\n\\mathbb{E}_{Q}[x^{2}]=\\mu_{Q}^{2}+v_{0},\\qquad \\mathbb{E}_{Q}[x^{4}]=\\mu_{Q}^{4}+6\\mu_{Q}^{2}v_{0}+3v_{0}^{2}.\n$$\nHence\n$$\n\\mathbb{E}_{Q}[U(x)]=\\alpha\\left(\\mu_{Q}^{4}+6\\mu_{Q}^{2}v_{0}+3v_{0}^{2}\\right)-\\beta\\left(\\mu_{Q}^{2}+v_{0}\\right).\n$$\nDiscarding terms independent of $\\mu_{Q}$, we minimize the quartic\n$$\nf(\\mu_{Q})=\\alpha \\mu_{Q}^{4}+(6\\alpha v_{0}-\\beta)\\mu_{Q}^{2}.\n$$\nDenote $c_{2}=6\\alpha v_{0}-\\beta$. The derivative is\n$$\nf'(\\mu_{Q})=4\\alpha \\mu_{Q}^{3}+2c_{2}\\mu_{Q}=2\\mu_{Q}\\left(2\\alpha \\mu_{Q}^{2}+c_{2}\\right).\n$$\nSetting $f'(\\mu_{Q})=0$ gives stationary points at $\\mu_{Q}=0$ and at\n$$\n\\mu_{Q}^{2}=-\\frac{c_{2}}{2\\alpha}=\\frac{\\beta-6\\alpha v_{0}}{2\\alpha}.\n$$\nUnder the given assumption $v_{0}<\\frac{\\beta}{6\\alpha}$, we have $\\beta-6\\alpha v_{0}>0$, so the nonzero stationary points are real. The second derivative is\n$$\nf''(\\mu_{Q})=12\\alpha \\mu_{Q}^{2}+2c_{2}.\n$$\nAt $\\mu_{Q}=0$, $f''(0)=2c_{2}<0$, so $\\mu_{Q}=0$ is a local maximum. At $\\mu_{Q}^{2}=(\\beta-6\\alpha v_{0})/(2\\alpha)$,\n$$\nf''=12\\alpha \\cdot \\frac{\\beta-6\\alpha v_{0}}{2\\alpha}+2c_{2}=6(\\beta-6\\alpha v_{0})+2(6\\alpha v_{0}-\\beta)=-4(6\\alpha v_{0}-\\beta)>0,\n$$\nso these are minima. By symmetry, there are two optimal means $\\pm \\sqrt{(\\beta-6\\alpha v_{0})/(2\\alpha)}$. The requested positive optimal mean is therefore\n$$\n\\mu_{Q}^{*}=\\sqrt{\\frac{\\beta-6\\alpha v_{0}}{2\\alpha}}.\n$$", "answer": "$$\\boxed{\\sqrt{\\frac{\\beta-6\\alpha v_{0}}{2\\alpha}}}$$", "id": "1370260"}]}