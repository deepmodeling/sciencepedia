{"hands_on_practices": [{"introduction": "It is a common and intuitive, yet often flawed, assumption that a Pearson correlation of zero implies independence between two variables. This practice problem [@problem_id:1353856] is designed to challenge that notion by exploring a classic scenario where two variables are perfectly dependent, yet their linear correlation is zero. By working through this example, you will develop a deeper appreciation for why more sophisticated tools like copulas are essential for capturing the full spectrum of dependence structures, especially non-monotonic relationships.", "problem": "A random variable $X$ follows a continuous uniform distribution on the interval $[-1, 1]$. A second random variable $Y$ is defined by the relationship $Y = X^2$. Your task is to analyze the dependence between $X$ and $Y$.\n\n1. Calculate the Pearson correlation coefficient, $\\rho(X, Y)$.\n2. The variables $X$ and $Y$ are perfectly dependent, yet their Pearson correlation coefficient may not fully capture this relationship. The complete dependence structure is described by their copula. Identify which of the following descriptions best characterizes the copula that links $X$ and $Y$.\n\nA. The copula represents perfect positive monotonic dependence, with its support on the main diagonal of the unit square.\n\nB. The copula represents perfect negative monotonic dependence, with its support on the anti-diagonal of the unit square.\n\nC. The copula is the independence copula, with its probability mass spread uniformly over the entire unit square.\n\nD. The copula represents a non-monotonic dependence, with its support concentrated on a V-shaped curve within the unit square.\n\nProvide your answer as a pair, consisting of the numerical value of the correlation coefficient and the letter corresponding to the correct option.", "solution": "Let $X$ be uniformly distributed on $[-1,1]$, so its density is $f_{X}(x)=\\frac{1}{2}$ for $x\\in[-1,1]$. Define $Y=X^{2}$. The Pearson correlation coefficient is $\\rho(X,Y)=\\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$.\n\nFirst compute the necessary moments. Since $X$ is symmetric about $0$,\n$$\n\\mathbb{E}[X]=\\int_{-1}^{1}x\\cdot\\frac{1}{2}\\,dx=\\frac{1}{2}\\left[\\frac{x^{2}}{2}\\right]_{-1}^{1}=0.\n$$\nAlso,\n$$\n\\mathbb{E}[X^{2}]=\\int_{-1}^{1}x^{2}\\cdot\\frac{1}{2}\\,dx=\\frac{1}{2}\\left[\\frac{x^{3}}{3}\\right]_{-1}^{1}=\\frac{1}{2}\\left(\\frac{1}{3}-\\left(-\\frac{1}{3}\\right)\\right)=\\frac{1}{3},\n$$\nso $\\mathbb{E}[Y]=\\mathbb{E}[X^{2}]=\\frac{1}{3}$. Next,\n$$\n\\mathbb{E}[X^{3}]=\\int_{-1}^{1}x^{3}\\cdot\\frac{1}{2}\\,dx=\\frac{1}{2}\\left[\\frac{x^{4}}{4}\\right]_{-1}^{1}=\\frac{1}{2}\\left(\\frac{1}{4}-\\frac{1}{4}\\right)=0,\n$$\nhence $\\mathbb{E}[XY]=\\mathbb{E}[X^{3}]=0$. Therefore,\n$$\n\\operatorname{Cov}(X,Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]=0-0\\cdot\\frac{1}{3}=0.\n$$\nFor completeness, compute the variances to confirm the denominator is positive. We have\n$$\n\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=\\frac{1}{3},\n$$\nand\n$$\n\\mathbb{E}[Y^{2}]=\\mathbb{E}[X^{4}]=\\int_{-1}^{1}x^{4}\\cdot\\frac{1}{2}\\,dx=\\frac{1}{2}\\left[\\frac{x^{5}}{5}\\right]_{-1}^{1}=\\frac{1}{2}\\left(\\frac{1}{5}-\\left(-\\frac{1}{5}\\right)\\right)=\\frac{1}{5},\n$$\nso\n$$\n\\operatorname{Var}(Y)=\\mathbb{E}[Y^{2}]-(\\mathbb{E}[Y])^{2}=\\frac{1}{5}-\\left(\\frac{1}{3}\\right)^{2}=\\frac{1}{5}-\\frac{1}{9}=\\frac{4}{45}>0.\n$$\nThus,\n$$\n\\rho(X,Y)=\\frac{0}{\\sqrt{\\frac{1}{3}\\cdot\\frac{4}{45}}}=0.\n$$\n\nAlthough $Y$ is a deterministic function of $X$ and they are perfectly dependent, the dependence is non-monotonic on $[-1,1]$, so the Pearson correlation is zero. To identify the copula, consider the probability integral transforms $U=F_{X}(X)$ and $V=F_{Y}(Y)$. For $x\\in[-1,1]$,\n$$\nF_{X}(x)=\\frac{x+1}{2},\\quad\\text{so}\\quad U=\\frac{X+1}{2}.\n$$\nFor $y\\in[0,1]$,\n$$\nF_{Y}(y)=\\mathbb{P}(X^{2}\\le y)=\\mathbb{P}(-\\sqrt{y}\\le X\\le \\sqrt{y})=\\int_{-\\sqrt{y}}^{\\sqrt{y}}\\frac{1}{2}\\,dx=\\sqrt{y},\n$$\nhence\n$$\nV=F_{Y}(Y)=\\sqrt{Y}=\\sqrt{X^{2}}=|X|.\n$$\nExpressing $X$ via $U$ gives $X=2U-1$, so the relation between the uniforms is\n$$\nV=|X|=|2U-1|.\n$$\nTherefore, the copula is singular with its support concentrated on the V-shaped curve $v=|2u-1|$ in the unit square. This is non-monotonic dependence and corresponds to option D.", "answer": "$$\\boxed{\\begin{pmatrix} 0 & D \\end{pmatrix}}$$", "id": "1353856"}, {"introduction": "Having established why copulas are necessary, we now turn to the fundamental mechanics of working with them. This exercise [@problem_id:1353902] provides hands-on practice in deriving a copula density function from its corresponding cumulative distribution function (CDF). This is a crucial skill, as the density is the key component in constructing likelihood functions for parameter estimation and model fitting.", "problem": "In statistical modeling, a copula is a multivariate cumulative distribution function for which the marginal probability distributions of each variable are uniform on the interval $[0, 1]$. One of the simplest families of copulas for modeling dependence between two random variables is the Farlie-Gumbel-Morgenstern (FGM) family.\n\nThe cumulative distribution function for a bivariate FGM copula is given by:\n$$C(u,v) = uv[1+\\alpha(1-u)(1-v)]$$\nwhere $u, v \\in [0, 1]$ and the dependence parameter $\\alpha$ is a real constant satisfying $|\\alpha| \\le 1$.\n\nThe copula density function, denoted by $c(u,v)$, describes the relative likelihood of a particular pair of values $(u,v)$. Find the analytical expression for the copula density function $c(u,v)$ corresponding to the FGM copula.", "solution": "The copula density function is the mixed second partial derivative of the copula distribution function:\n$$c(u,v)=\\frac{\\partial^{2}}{\\partial u \\partial v}C(u,v).$$\nGiven $C(u,v)=uv\\left[1+\\alpha(1-u)(1-v)\\right]$, first compute the partial derivative with respect to $u$ using the product rule:\n$$\\frac{\\partial C}{\\partial u}=v\\left[1+\\alpha(1-u)(1-v)\\right]+uv\\frac{\\partial}{\\partial u}\\left[1+\\alpha(1-u)(1-v)\\right].$$\nThe inner derivative is\n$$\\frac{\\partial}{\\partial u}\\left[1+\\alpha(1-u)(1-v)\\right]=\\alpha\\frac{\\partial}{\\partial u}\\left[(1-u)(1-v)\\right]=-\\alpha(1-v).$$\nThus,\n$$\\frac{\\partial C}{\\partial u}=v\\left[1+\\alpha(1-u)(1-v)\\right]-\\alpha uv(1-v).$$\nFactor and simplify:\n$$\\frac{\\partial C}{\\partial u}=v+\\alpha v(1-v)(1-2u).$$\nNow differentiate with respect to $v$:\n$$c(u,v)=\\frac{\\partial}{\\partial v}\\left[v+\\alpha v(1-v)(1-2u)\\right]=1+\\alpha(1-2u)\\frac{\\partial}{\\partial v}\\left[v-v^{2}\\right].$$\nSince $\\frac{\\partial}{\\partial v}(v-v^{2})=1-2v$, we obtain\n$$c(u,v)=1+\\alpha(1-2u)(1-2v).$$", "answer": "$$\\boxed{1+\\alpha(1-2u)(1-2v)}$$", "id": "1353902"}, {"introduction": "One of the great strengths of copula modeling is the ability to summarize complex dependence patterns using a single parameter. However, the interpretation of this parameter can be abstract. This exercise [@problem_id:1927387] provides a powerful link between theory and practice by showing how the dependence parameter $\\theta$ of the Clayton copula can be directly translated into the well-known Kendall's tau rank correlation coefficient, $\\tau$. This reveals a clear and intuitive meaning for the copula parameter.", "problem": "In the field of financial modeling and risk management, copulas are used to describe the dependence between random variables. An important class of copulas is the Archimedean family, which is constructed from a function called a generator.\n\nFor a continuous bivariate distribution whose dependence structure is described by an Archimedean copula with a generator $\\phi(t)$, the population version of Kendall's rank correlation coefficient, denoted $\\tau$, can be calculated directly from the generator using the formula:\n$$\n\\tau = 1 + 4 \\int_{0}^{1} \\frac{\\phi(t)}{\\phi'(t)} dt\n$$\nwhere $\\phi'(t)$ is the first derivative of $\\phi(t)$ with respect to $t$.\n\nConsider the Clayton family of copulas, which is a specific type of Archimedean copula. The generator for the Clayton family is given by:\n$$\n\\phi(t) = \\frac{1}{\\theta} (t^{-\\theta} - 1)\n$$\nwhere $\\theta > 0$ is a parameter that controls the strength of the dependence. A larger value of $\\theta$ implies stronger positive dependence.\n\nYour task is to derive a closed-form analytic expression for Kendall's tau ($\\tau$) for the Clayton copula family as a function of the parameter $\\theta$.", "solution": "We use the Archimedean-copula formula for Kendall's tau:\n$$\n\\tau=1+4\\int_{0}^{1}\\frac{\\phi(t)}{\\phi'(t)}\\,dt,\n$$\nwith the Clayton generator $\\phi(t)=\\frac{1}{\\theta}\\big(t^{-\\theta}-1\\big)$ for $\\theta>0$.\n\nFirst compute the derivative:\n$$\n\\phi'(t)=\\frac{1}{\\theta}\\big(-\\theta\\big)t^{-\\theta-1}=-t^{-(\\theta+1)}.\n$$\nThen the ratio simplifies to\n$$\n\\frac{\\phi(t)}{\\phi'(t)}=\\frac{\\frac{1}{\\theta}\\big(t^{-\\theta}-1\\big)}{-t^{-(\\theta+1)}}=\\frac{1}{\\theta}\\big(t^{\\theta+1}-t\\big).\n$$\nTherefore,\n$$\n\\int_{0}^{1}\\frac{\\phi(t)}{\\phi'(t)}\\,dt=\\frac{1}{\\theta}\\left(\\int_{0}^{1}t^{\\theta+1}\\,dt-\\int_{0}^{1}t\\,dt\\right)=\\frac{1}{\\theta}\\left(\\frac{1}{\\theta+2}-\\frac{1}{2}\\right).\n$$\nSubstituting back into the expression for $\\tau$,\n$$\n\\tau=1+\\frac{4}{\\theta}\\left(\\frac{1}{\\theta+2}-\\frac{1}{2}\\right)=1+\\frac{4}{\\theta}\\cdot\\frac{-\\theta}{2(\\theta+2)}=1-\\frac{2}{\\theta+2}=\\frac{\\theta}{\\theta+2}.\n$$\nThis holds for $\\theta>0$, which ensures integrability at $t=0$.", "answer": "$$\\boxed{\\frac{\\theta}{\\theta+2}}$$", "id": "1927387"}]}