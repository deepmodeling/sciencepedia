{"hands_on_practices": [{"introduction": "The geometric interpretation of conditional expectation as a projection is powerful because it allows us to use our spatial intuition. A key feature of projection operators in linear algebra is their linearity. This first exercise invites you to verify that the conditional expectation operator, $E[\\cdot | \\mathcal{G}]$, indeed behaves as a linear operator, solidifying the connection between this probabilistic concept and its geometric counterpart [@problem_id:1350188].", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, and let $L^2(\\Omega, \\mathcal{F}, P)$ be the Hilbert space of square-integrable random variables on this space, equipped with the inner product $\\langle X, Y \\rangle = E[XY]$. Let $\\mathcal{G}$ be a sub-$\\sigma$-algebra of $\\mathcal{F}$. It is a known result that the conditional expectation operator, $E[\\cdot | \\mathcal{G}]$, acts as an orthogonal projection from $L^2(\\Omega, \\mathcal{F}, P)$ onto its closed subspace $L^2(\\Omega, \\mathcal{G}, P)$.\n\nLet's denote the orthogonal projection of a random variable $Z \\in L^2(\\Omega, \\mathcal{F}, P)$ onto the subspace $L^2(\\Omega, \\mathcal{G}, P)$ by $P_{\\mathcal{G}}(Z)$. From the above, we have the identity $P_{\\mathcal{G}}(Z) = E[Z | \\mathcal{G}]$.\n\nConsider two arbitrary random variables $X, Y \\in L^2(\\Omega, \\mathcal-F, P)$ and an arbitrary non-zero real constant $c$. Which of the following expressions correctly represents the projection of the linear combination $X + cY$ onto the subspace $L^2(\\Omega, \\mathcal{G}, P)$?\n\nA. $P_{\\mathcal{G}}(X) + P_{\\mathcal{G}}(Y)$\n\nB. $P_{\\mathcal{G}}(X) + c P_{\\mathcal{G}}(Y)$\n\nC. $P_{\\mathcal{G}}(X) + c^2 P_{\\mathcal{G}}(Y)$\n\nD. $|c| P_{\\mathcal{G}}(X) + \\sqrt{1-c^2} P_{\\mathcal{G}}(Y)$\n\nE. $c P_{\\mathcal{G}}(X) + P_{\\mathcal{G}}(Y)$\n\nF. The expression depends on the correlation between $X$ and $Y$ and cannot be determined in general.", "solution": "We use the characterization of the orthogonal projection onto the closed subspace $L^{2}(\\Omega, \\mathcal{G}, P)$: for any $Z \\in L^{2}(\\Omega, \\mathcal{F}, P)$, its projection $U = P_{\\mathcal{G}}(Z) \\in L^{2}(\\Omega, \\mathcal{G}, P)$ is the unique element such that\n$$\nE\\big[(Z - U)H\\big] = 0 \\quad \\text{for all } H \\in L^{2}(\\Omega, \\mathcal{G}, P).\n$$\nEquivalently, since $P_{\\mathcal{G}}(Z) = E[Z \\mid \\mathcal{G}]$, the same orthogonality holds with $U = E[Z \\mid \\mathcal{G}]$.\n\nLet $U_{X} = P_{\\mathcal{G}}(X) = E[X \\mid \\mathcal{G}]$ and $U_{Y} = P_{\\mathcal{G}}(Y) = E[Y \\mid \\mathcal{G}]$. For any $H \\in L^{2}(\\Omega, \\mathcal{G}, P)$, by the defining orthogonality we have\n$$\nE\\big[(X - U_{X})H\\big] = 0, \\qquad E\\big[(Y - U_{Y})H\\big] = 0.\n$$\nFix an arbitrary real constant $c \\neq 0$ (the argument also holds for $c = 0$). Consider $Z = X + cY$ and the candidate\n$$\nU := U_{X} + c\\,U_{Y} \\in L^{2}(\\Omega, \\mathcal{G}, P).\n$$\nThen, for any $H \\in L^{2}(\\Omega, \\mathcal{G}, P)$,\n$$\nE\\big[(Z - U)H\\big] = E\\big[(X + cY - U_{X} - cU_{Y})H\\big]\n= E\\big[(X - U_{X})H\\big] + c\\,E\\big[(Y - U_{Y})H\\big] = 0 + c \\cdot 0 = 0.\n$$\nBy the uniqueness of the orthogonal projection, this implies $U = P_{\\mathcal{G}}(Z)$, hence\n$$\nP_{\\mathcal{G}}(X + cY) = P_{\\mathcal{G}}(X) + c\\,P_{\\mathcal{G}}(Y).\n$$\nEquivalently, by linearity of conditional expectation,\n$$\nE[X + cY \\mid \\mathcal{G}] = E[X \\mid \\mathcal{G}] + c\\,E[Y \\mid \\mathcal{G}],\n$$\nwhich matches option B.", "answer": "$$\\boxed{B}$$", "id": "1350188"}, {"introduction": "Moving from abstract properties to practical applications, one of the most important uses of orthogonal projection is finding the best possible approximation. In fields like signal processing or econometrics, we often want to estimate an unobservable quantity using a linear combination of observable signals. This problem demonstrates how projecting the target signal onto the subspace spanned by the reference signals provides the optimal estimate that minimizes the mean squared error [@problem_id:1350189].", "problem": "In signal processing, a common task is to estimate a signal of interest, $S$, from other accessible measurements. We can model the signal $S$ and two reference signals, $R_1$ and $R_2$, as random variables. For this problem, assume all signals have been processed to have a mean of zero. These random variables are elements of the space of square-integrable random variables, $L^2$, with the inner product given by $\\langle A, B \\rangle = E[AB]$.\n\nThe goal is to find the best linear estimate of $S$ using $R_1$ and $R_2$. The best linear estimate is a random variable $\\hat{S}$ of the form $\\hat{S} = a_1 R_1 + a_2 R_2$, where the real coefficients $a_1$ and $a_2$ are chosen to minimize the mean squared error, $E[(S - \\hat{S})^2]$. This optimal estimate $\\hat{S}$ corresponds to the orthogonal projection of $S$ onto the linear subspace spanned by $\\{R_1, R_2\\}$.\n\nSuppose the following statistical properties of the signals are known:\n- $E[S^2] = 16$\n- $E[R_1^2] = 4$\n- $E[R_2^2] = 9$\n- $E[SR_1] = 3$\n- $E[SR_2] = -1$\n- $E[R_1 R_2] = 2$\n\nCalculate the minimum possible value of the mean squared error, $E[(S - \\hat{S})^2]$.", "solution": "We work in the Hilbert space $L^{2}$ with inner product $\\langle A,B\\rangle=E[AB]$. The best linear estimate $\\hat{S}=a_{1}R_{1}+a_{2}R_{2}$ is the orthogonal projection of $S$ onto $\\operatorname{span}\\{R_{1},R_{2}\\}$, characterized by the orthogonality conditions $E[(S-\\hat{S})R_{i}]=0$ for $i=1,2$.\n\nDefine the coefficient vector $a=\\begin{pmatrix}a_{1}\\\\ a_{2}\\end{pmatrix}$, the covariance matrix\n$$\nR=\\begin{pmatrix}\nE[R_{1}^{2}]  E[R_{1}R_{2}]\\\\\nE[R_{1}R_{2}]  E[R_{2}^{2}]\n\\end{pmatrix}\n=\\begin{pmatrix}\n4  2\\\\\n2  9\n\\end{pmatrix},\n$$\nand the cross-covariance vector\n$$\nc=\\begin{pmatrix}\nE[SR_{1}]\\\\\nE[SR_{2}]\n\\end{pmatrix}\n=\\begin{pmatrix}\n3\\\\\n-1\n\\end{pmatrix}.\n$$\nThe mean squared error as a function of $a$ is\n$$\nE[(S-\\hat{S})^{2}]=E[S^{2}]-2a^{T}c+a^{T}Ra.\n$$\nMinimizing with respect to $a$ gives the normal equations $Ra=c$. Since $\\det(R)=4\\cdot 9-2^{2}=320$, $R$ is invertible and the unique minimizer is $a=R^{-1}c$. Substituting back into the error expression yields the minimum mean squared error\n$$\nE[(S-\\hat{S})^{2}]_{\\min}=E[S^{2}]-c^{T}R^{-1}c.\n$$\n\nCompute $R^{-1}$. For $R=\\begin{pmatrix}42\\\\29\\end{pmatrix}$,\n$$\nR^{-1}=\\frac{1}{32}\\begin{pmatrix}9  -2\\\\ -2  4\\end{pmatrix}.\n$$\nThen\n$$\nR^{-1}c=\\frac{1}{32}\\begin{pmatrix}9  -2\\\\ -2  4\\end{pmatrix}\\begin{pmatrix}3\\\\ -1\\end{pmatrix}\n=\\frac{1}{32}\\begin{pmatrix}27+2\\\\ -6-4\\end{pmatrix}\n=\\begin{pmatrix}\\frac{29}{32}\\\\ -\\frac{10}{32}\\end{pmatrix} = \\begin{pmatrix}\\frac{29}{32}\\\\ -\\frac{5}{16}\\end{pmatrix},\n$$\nand\n$$\nc^{T}R^{-1}c=\\begin{pmatrix}3  -1\\end{pmatrix}\\begin{pmatrix}\\frac{29}{32}\\\\ -\\frac{5}{16}\\end{pmatrix}\n=\\frac{87}{32}+\\frac{5}{16}\n=\\frac{87}{32}+\\frac{10}{32}\n=\\frac{97}{32}.\n$$\nGiven $E[S^{2}]=16$, the minimum mean squared error is\n$$\nE[(S-\\hat{S})^{2}]_{\\min}=16-\\frac{97}{32}\n=\\frac{512}{32}-\\frac{97}{32}\n=\\frac{415}{32}.\n$$", "answer": "$$\\boxed{\\frac{415}{32}}$$", "id": "1350189"}, {"introduction": "Once we have found the best approximation, a crucial follow-up question is to quantify how good that approximation is. The geometric framework gives us a natural way to do this by measuring the \"length\" of the error vector, which is the difference between the original random variable and its projection. This hands-on calculation will allow you to compute the $L^2$-norm of the approximation error, giving a tangible value to the quality of the conditional expectation as an estimate [@problem_id:1350216].", "problem": "Let $Z$ be a random variable following a standard normal distribution, $Z \\sim N(0, 1)$, defined on a probability space $(\\Omega, \\mathcal{F}, P)$. Consider the random variable $X$ defined as $X = (Z - c)^2$, where $c = \\sqrt{3/2}$.\n\nLet $\\mathcal{G}$ be the sub-$\\sigma$-algebra of $\\mathcal{F}$ generated by the absolute value of $Z$, that is, $\\mathcal{G} = \\sigma(|Z|)$. We are interested in finding the best approximation of the random variable $X$ within the space of all square-integrable, $\\mathcal{G}$-measurable random variables. Let this best approximation be denoted by $\\hat{X}$. The approximation error is the random variable $e = X - \\hat{X}$.\n\nYour objective is to calculate the $L^2$-norm of this error, which is defined as $\\|e\\|_{L^2} = \\sqrt{E[e^2]}$.\n\nProvide your final answer as a single real number, rounded to four significant figures.", "solution": "Let $Z\\sim N(0,1)$ and $X=(Z-c)^{2}$ with $c=\\sqrt{3/2}$. The space of square-integrable, $\\mathcal{G}$-measurable random variables is a closed subspace of $L^{2}$, and the best $L^{2}$ approximation of $X$ in this subspace is the orthogonal projection $\\hat{X}=E[X\\mid\\mathcal{G}]$, where $\\mathcal{G}=\\sigma(|Z|)$. The approximation error is $e=X-\\hat{X}$, and the orthogonality principle gives $E[e^{2}]=E[\\operatorname{Var}(X\\mid\\mathcal{G})]$ and also $E[e^{2}]=E[X^{2}]-E[\\hat{X}^{2}]$.\n\nSet $Y=|Z|$. Given $Y=a\\ge 0$, the conditional distribution satisfies $P(Z=a\\mid Y=a)=P(Z=-a\\mid Y=a)=\\tfrac{1}{2}$. Then\n$$\nE[X\\mid Y=a]=\\frac{1}{2}\\big((a-c)^{2}+(-a-c)^{2}\\big)=\\frac{1}{2}\\big((a-c)^{2}+(a+c)^{2}\\big)=a^{2}+c^{2}.\n$$\nTherefore $\\hat{X}=E[X\\mid\\mathcal{G}]=|Z|^{2}+c^{2}=Z^{2}+c^{2}$.\n\nCompute the error:\n$$\ne=X-\\hat{X}=(Z-c)^{2}-(Z^{2}+c^{2})=Z^{2}-2cZ+c^{2}-Z^{2}-c^{2}=-2cZ.\n$$\nHence\n$$\nE[e^{2}]=E[(-2cZ)^2]=4c^{2}E[Z^{2}]=4c^{2},\n$$\nsince $E[Z^{2}]=1$ for $Z\\sim N(0,1)$. Thus the $L^{2}$ norm is\n$$\n\\|e\\|_{L^{2}}=\\sqrt{E[e^{2}]}=2|c|=2c.\n$$\nWith $c=\\sqrt{3/2}$,\n$$\n\\|e\\|_{L^{2}}=2\\sqrt{\\frac{3}{2}}=\\sqrt{4 \\cdot \\frac{3}{2}}=\\sqrt{6}.\n$$\nNumerically, $\\sqrt{6}$ rounded to four significant figures is $2.449$.", "answer": "$$\\boxed{2.449}$$", "id": "1350216"}]}