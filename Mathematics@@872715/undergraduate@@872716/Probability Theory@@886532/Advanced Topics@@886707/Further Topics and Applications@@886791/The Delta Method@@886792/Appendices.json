{"hands_on_practices": [{"introduction": "To begin, let's explore a direct application of the Delta Method to understand its fundamental mechanics. This exercise demonstrates how to approximate the sampling distribution for a function of a sample mean, a common task in applied statistics. By estimating the variance of a nano-crystal's volume based on its side length [@problem_id:1959853], you will practice the core steps of defining a function, finding its derivative, and applying the main theorem to determine the asymptotic variance.", "problem": "In a materials science laboratory, a novel process is used to synthesize cubic nano-crystals. The side length of a randomly chosen crystal is a random variable $X$ with a true mean $\\mu > 0$ and a finite, non-zero variance $\\sigma^2$. To estimate the properties of the crystals, a quality control engineer measures the side lengths of a random sample of $n$ crystals, denoted by $X_1, X_2, \\ldots, X_n$. The sample mean of the side lengths is calculated as $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nThe engineer is interested in the statistical behavior of the volume of a hypothetical nano-crystal whose side length is equal to the sample mean, which we define as $V_n = \\bar{X}_n^3$. According to large-sample theory, for a sufficiently large sample size $n$, the sampling distribution of $V_n$ can be approximated by a normal distribution.\n\nDetermine the variance of this approximate normal distribution for $V_n$. Express your answer as a symbolic expression in terms of $\\mu$, $\\sigma^2$, and $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent and identically distributed with $\\mathbb{E}[X_{i}]=\\mu>0$ and $\\operatorname{Var}(X_{i})=\\sigma^{2}\\in(0,\\infty)$. The sample mean is $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ with $\\mathbb{E}[\\bar{X}_{n}]=\\mu$ and $\\operatorname{Var}(\\bar{X}_{n})=\\frac{\\sigma^{2}}{n}$.\n\nBy the central limit theorem, for large $n$,\n$$\n\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)\\xrightarrow{d}\\mathcal{N}(0,\\sigma^{2}).\n$$\nDefine $g(x)=x^{3}$ so that $V_{n}=g(\\bar{X}_{n})$. By the delta method,\n$$\n\\sqrt{n}\\left(g(\\bar{X}_{n})-g(\\mu)\\right)\\xrightarrow{d}\\mathcal{N}\\left(0,\\left(g'(\\mu)\\right)^{2}\\sigma^{2}\\right),\n$$\nwhich implies the large-sample approximation\n$$\n\\operatorname{Var}(V_{n})\\approx\\frac{\\left(g'(\\mu)\\right)^{2}\\sigma^{2}}{n}.\n$$\nCompute $g'(x)=3x^{2}$, hence $g'(\\mu)=3\\mu^{2}$ and $\\left(g'(\\mu)\\right)^{2}=9\\mu^{4}$. Therefore,\n$$\n\\operatorname{Var}(V_{n})\\approx\\frac{9\\mu^{4}\\sigma^{2}}{n}.\n$$", "answer": "$$\\boxed{\\frac{9\\mu^{4}\\sigma^{2}}{n}}$$", "id": "1959853"}, {"introduction": "Now we will examine a more subtle application: estimating the variance of a Bernoulli process. This practice is crucial because it highlights a potential pitfall of the standard, or first-order, Delta Method. As you will see [@problem_id:1959831], the resulting asymptotic variance, $(1-2p)^{2}p(1-p)$, depends on the true parameter $p$ and becomes zero in the special case where $p=0.5$, suggesting the approximation breaks down.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of independent and identically distributed random variables from a Bernoulli distribution with parameter $p$, where $0 < p < 1$. The sample proportion is defined as $\\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. This sample proportion is an unbiased estimator for the true proportion $p$.\n\nConsider the estimator for the variance of a single Bernoulli trial, defined as a function of the sample proportion: $V = g(\\hat{p}) = \\hat{p}(1-\\hat{p})$. The Central Limit Theorem implies that the distribution of $\\sqrt{n}(\\hat{p} - p)$ converges to a normal distribution as the sample size $n$ approaches infinity. As a consequence, the distribution of the centered and scaled estimator $\\sqrt{n}(V - p(1-p))$ also converges to a normal distribution with a mean of 0.\n\nDetermine the variance of this limiting normal distribution for $\\sqrt{n}(V - p(1-p))$. Express your answer as a closed-form analytic expression in terms of the parameter $p$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. Bernoulli$(p)$, with $0<p<1$. The sample proportion is $\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, so by the Central Limit Theorem,\n$$\n\\sqrt{n}\\,(\\hat{p}-p)\\;\\xrightarrow{d}\\;N\\bigl(0,\\;p(1-p)\\bigr).\n$$\nDefine $V=g(\\hat{p})$ with $g(x)=x(1-x)$. We seek the limiting variance of $\\sqrt{n}\\bigl(g(\\hat{p})-g(p)\\bigr)$.\n\nCompute derivatives of $g$:\n$$\ng'(x)=1-2x,\\qquad g''(x)=-2.\n$$\nApply a second-order Taylor expansion of $g$ at $p$:\n$$\ng(\\hat{p})-g(p)=g'(p)(\\hat{p}-p)+\\frac{1}{2}g''(\\tilde{p})\\,(\\hat{p}-p)^{2},\n$$\nfor some $\\tilde{p}$ between $\\hat{p}$ and $p$. Multiply by $\\sqrt{n}$:\n$$\n\\sqrt{n}\\bigl(g(\\hat{p})-g(p)\\bigr)=g'(p)\\sqrt{n}\\,(\\hat{p}-p)+\\frac{1}{2}g''(\\tilde{p})\\,\\sqrt{n}\\,(\\hat{p}-p)^{2}.\n$$\nSince $\\sqrt{n}(\\hat{p}-p)=O_{p}(1)$, we have $\\sqrt{n}(\\hat{p}-p)^{2}=\\frac{1}{\\sqrt{n}}\\bigl(\\sqrt{n}(\\hat{p}-p)\\bigr)^{2}=o_{p}(1)$, and $g''(\\tilde{p})$ is bounded. Hence the second term is $o_{p}(1)$. By Slutskyâ€™s theorem and the delta method,\n$$\n\\sqrt{n}\\bigl(g(\\hat{p})-g(p)\\bigr)\\;\\xrightarrow{d}\\;N\\Bigl(0,\\;\\bigl(g'(p)\\bigr)^{2}\\,p(1-p)\\Bigr).\n$$\nSubstituting $g'(p)=1-2p$ yields the asymptotic variance\n$$\n\\bigl(1-2p\\bigr)^{2}\\,p(1-p).\n$$\nNote that at $p=\\frac{1}{2}$ this variance equals $0$, corresponding to a degenerate limit under the $\\sqrt{n}$ scaling.", "answer": "$$\\boxed{(1-2p)^{2}p(1-p)}$$", "id": "1959831"}, {"introduction": "Building directly on the insight from the previous problem, this final exercise tackles the \"degenerate\" case where the first-order Delta Method yields an asymptotic variance of zero. Here we must use a higher-order approximation, known as the second-order Delta Method, to find the correct limiting distribution. This advanced practice [@problem_id:1959855] is essential for robust statistical analysis, revealing that the limiting distribution in such cases is not Normal but follows a Chi-squared distribution instead.", "problem": "In a study of a stochastic binary process, a sequence of $n$ independent trials is conducted. Each trial results in either a \"success\" (coded as 1) or a \"failure\" (coded as 0). The true probability of a success in any given trial is known to be exactly $p = 1/2$. Let $\\hat{p}_n$ be the sample proportion of successes observed in the $n$ trials.\n\nA statistic of interest is the sample variance of this Bernoulli process, which is given by $\\hat{S}_n^2 = \\hat{p}_n(1-\\hat{p}_n)$. The corresponding true variance is $\\sigma^2 = p(1-p) = 1/4$. We wish to understand the asymptotic behavior of this sample variance estimator.\n\nConsider the centered and scaled statistic $T_n = n(\\hat{S}_n^2 - 1/4)$. Which of the following options correctly describes the limiting distribution of $T_n$ as $n \\to \\infty$?\nLet $Z$ denote a standard normal random variable, and let $\\chi^2_1$ denote a chi-squared random variable with one degree of freedom.\n\nA. A Normal distribution with mean 0 and variance $1/16$.\n\nB. The distribution of the random variable $-\\frac{1}{2}\\chi^2_1$.\n\nC. The distribution of the random variable $\\frac{1}{4}\\chi^2_1$.\n\nD. The distribution of the random variable $-\\frac{1}{4}\\chi^2_1$.\n\nE. The distribution of the random variable $-\\chi^2_1$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. Bernoulli with $p=\\frac{1}{2}$, and let $\\hat{p}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. The sample variance estimator is $\\hat{S}_{n}^{2}=\\hat{p}_{n}(1-\\hat{p}_{n})$. The true variance is $\\sigma^{2}=p(1-p)=\\frac{1}{4}$.\n\nAlgebraically,\n$$\n\\hat{S}_{n}^{2}-\\frac{1}{4}\n=\\hat{p}_{n}(1-\\hat{p}_{n})-\\frac{1}{4}\n=\\hat{p}_{n}-\\hat{p}_{n}^{2}-\\frac{1}{4}\n=-(\\hat{p}_{n}-\\tfrac{1}{2})^{2},\n$$\nsince $(\\hat{p}_{n}-\\tfrac{1}{2})^{2}=\\hat{p}_{n}^{2}-\\hat{p}_{n}+\\tfrac{1}{4}$. Therefore,\n$$\nT_{n}=n\\bigl(\\hat{S}_{n}^{2}-\\tfrac{1}{4}\\bigr)\n=-\\,n\\bigl(\\hat{p}_{n}-\\tfrac{1}{2}\\bigr)^{2}\n=-\\bigl(\\sqrt{n}(\\hat{p}_{n}-\\tfrac{1}{2})\\bigr)^{2}.\n$$\n\nBy the Central Limit Theorem,\n$$\n\\sqrt{n}\\,(\\hat{p}_{n}-\\tfrac{1}{2})\\;\\xrightarrow{d}\\;Y,\\quad Y\\sim N\\bigl(0,\\tfrac{1}{4}\\bigr).\n$$\nBy the Continuous Mapping Theorem with the continuous function $g(y)=y^{2}$, we have\n$$\n\\bigl(\\sqrt{n}(\\hat{p}_{n}-\\tfrac{1}{2})\\bigr)^{2}\\;\\xrightarrow{d}\\;Y^{2}.\n$$\nIf $Z\\sim N(0,1)$ then $Y\\stackrel{d}{=}\\tfrac{1}{2}Z$, hence $Y^{2}\\stackrel{d}{=}\\tfrac{1}{4}Z^{2}=\\tfrac{1}{4}\\chi^{2}_{1}$. Consequently,\n$$\nT_{n}\\;\\xrightarrow{d}\\;-\\,Y^{2}\\;\\stackrel{d}{=}\\;-\\frac{1}{4}\\chi^{2}_{1}.\n$$\nComparing with the options, this is option D.", "answer": "$$\\boxed{D}$$", "id": "1959855"}]}