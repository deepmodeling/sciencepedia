{"hands_on_practices": [{"introduction": "This first exercise provides a classic application of Bayesian inference to model real-world event counts. We will explore how to update beliefs about a disease transmission rate, $\\lambda$, by combining prior knowledge (modeled by a Gamma distribution) with new data on weekly cases (modeled by a Poisson distribution). This problem is a perfect illustration of how conjugate priors simplify the process of deriving the posterior distribution, a foundational skill in Bayesian analysis [@problem_id:1379681].", "problem": "An epidemiologist is studying the transmission of a newly identified virus in a specific city. The number of new cases reported each week is assumed to follow a Poisson distribution with an unknown average rate parameter $\\lambda$. Based on preliminary studies of similar viruses in other regions, the epidemiologist's prior belief about $\\lambda$ is modeled by a Gamma distribution, $\\text{Gamma}(\\alpha, \\beta)$. The probability density function for this prior distribution is given by:\n$$p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$$\nfor $\\lambda  0$, where $\\Gamma(\\alpha)$ is the Gamma function. For this particular virus, the epidemiologist sets the prior parameters to be $\\alpha=3$ and $\\beta=2$.\n\nOver a period of four consecutive weeks, the following numbers of new cases are observed: $k_1=7, k_2=4, k_3=10, k_4=5$. Assuming that the number of cases in each week is an independent event, determine the full posterior probability density function for the rate parameter $\\lambda$ given the observed data. Express your answer as a function of $\\lambda$.", "solution": "Let the weekly counts be modeled as independent Poisson random variables given the rate parameter $\\lambda$, so for a single observation $k$ the likelihood is\n$$\np(k \\mid \\lambda)=\\frac{\\lambda^{k}\\exp(-\\lambda)}{k!}.\n$$\nFor $n$ independent weeks with observations $k_{1},\\dots,k_{n}$, the joint likelihood is\n$$\nL(\\lambda \\mid k_{1},\\dots,k_{n})=\\prod_{i=1}^{n}\\frac{\\lambda^{k_{i}}\\exp(-\\lambda)}{k_{i}!}=\\left(\\prod_{i=1}^{n}\\frac{1}{k_{i}!}\\right)\\lambda^{\\sum_{i=1}^{n}k_{i}}\\exp(-n\\lambda).\n$$\nThe prior for $\\lambda$ is $\\text{Gamma}(\\alpha,\\beta)$ with density\n$$\np(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta\\lambda),\\quad \\lambda0.\n$$\nBy Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior:\n$$\np(\\lambda \\mid k_{1},\\dots,k_{n})\\propto \\lambda^{\\sum_{i=1}^{n}k_{i}}\\exp(-n\\lambda)\\cdot \\lambda^{\\alpha-1}\\exp(-\\beta\\lambda)=\\lambda^{\\alpha-1+\\sum_{i=1}^{n}k_{i}}\\exp\\bigl(-(\\beta+n)\\lambda\\bigr).\n$$\nThis is the kernel of a Gamma distribution with updated parameters\n$$\n\\alpha^{\\prime}=\\alpha+\\sum_{i=1}^{n}k_{i},\\qquad \\beta^{\\prime}=\\beta+n.\n$$\nTherefore, the normalized posterior is\n$$\np(\\lambda \\mid k_{1},\\dots,k_{n})=\\frac{{\\beta^{\\prime}}^{\\alpha^{\\prime}}}{\\Gamma(\\alpha^{\\prime})}\\lambda^{\\alpha^{\\prime}-1}\\exp(-\\beta^{\\prime}\\lambda),\\quad \\lambda0.\n$$\nFor the given data, $\\alpha=3$, $\\beta=2$, $n=4$, and $\\sum_{i=1}^{4}k_{i}=7+4+10+5=26$, hence\n$$\n\\alpha^{\\prime}=3+26=29,\\qquad \\beta^{\\prime}=2+4=6,\n$$\nso the full posterior density is\n$$\np(\\lambda \\mid k_{1}=7,k_{2}=4,k_{3}=10,k_{4}=5)=\\frac{6^{29}}{\\Gamma(29)}\\lambda^{28}\\exp(-6\\lambda),\\quad \\lambda0.\n$$", "answer": "$$\\boxed{\\frac{6^{29}}{\\Gamma(29)}\\,\\lambda^{28}\\,\\exp(-6\\lambda)}$$", "id": "1379681"}, {"introduction": "Moving from rates to probabilities, this next problem challenges us to compare two different ways of estimating an unknown success probability, $\\theta$. We will calculate both the maximum likelihood estimate, a cornerstone of frequentist statistics, and the posterior mean, a common Bayesian estimate. By comparing these two values, we can gain a deeper appreciation for how a prior distribution, even a simple uniform one, influences our conclusions and provides a more regularized estimate than the raw data proportion [@problem_id:1379693].", "problem": "A materials scientist is studying a newly synthesized semiconductor material. The probability, denoted by $\\theta$, that an electron injected into the material successfully passes through a potential barrier is unknown. Based on the material's theoretical design, the scientist has no prior preference for any particular value of $\\theta$, and thus assumes a uniform prior probability distribution for $\\theta$ over the interval $[0, 1]$.\n\nIn an experiment, $N$ electrons are independently injected into the material, and it is observed that exactly $k$ of them successfully pass through the barrier, where $0  k  N$.\n\nTwo methods are proposed to estimate the value of $\\theta$ based on this experimental outcome.\n\n1.  **Method A**: Find the value of $\\theta$ that maximizes the probability of observing the experimental outcome of $k$ successes in $N$ trials. Let this estimate be $\\hat{\\theta}_{A}$.\n2.  **Method B**: First, determine the updated (posterior) probability distribution of $\\theta$ given the experimental data. Then, calculate the expected value of $\\theta$ with respect to this new distribution. Let this estimate be $\\hat{\\theta}_{B}$.\n\nDetermine the difference between these two estimates, $\\hat{\\theta}_{B} - \\hat{\\theta}_{A}$. Express your final answer as a single closed-form expression in terms of $N$ and $k$.", "solution": "We model the number of successes $k$ in $N$ independent trials with success probability $\\theta$ as a binomial outcome. The likelihood of observing exactly $k$ successes is\n$$\nL(\\theta)\\propto \\binom{N}{k}\\,\\theta^{k}\\left(1-\\theta\\right)^{N-k}.\n$$\nMethod A asks for the value that maximizes this likelihood. Since $\\binom{N}{k}$ does not depend on $\\theta$, we maximize $f(\\theta)=\\theta^{k}(1-\\theta)^{N-k}$ on $(0,1)$. Taking the derivative of $\\ln f(\\theta)=k\\ln\\theta+(N-k)\\ln(1-\\theta)$ and setting it to zero gives\n$$\n\\frac{d}{d\\theta}\\ln f(\\theta)=\\frac{k}{\\theta}-\\frac{N-k}{1-\\theta}=0\n\\;\\;\\Longrightarrow\\;\\;\nk(1-\\theta)=(N-k)\\theta\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{\\theta}_{A}=\\frac{k}{N},\n$$\nwhich is an interior maximum for $0kN$.\n\nFor Method B, with a uniform prior on $[0,1]$, the prior is $\\operatorname{Beta}(1,1)$. Given $k$ successes and $N-k$ failures, the posterior is\n$$\n\\theta\\mid k,N \\sim \\operatorname{Beta}(k+1,\\,N-k+1),\n$$\nsince the posterior density is proportional to $\\theta^{k}(1-\\theta)^{N-k}$. The posterior mean is\n$$\n\\hat{\\theta}_{B}=\\mathbb{E}[\\theta\\mid k,N]=\\frac{k+1}{(k+1)+(N-k+1)}=\\frac{k+1}{N+2}.\n$$\nTherefore, the difference is\n$$\n\\hat{\\theta}_{B}-\\hat{\\theta}_{A}\n=\\frac{k+1}{N+2}-\\frac{k}{N}\n=\\frac{N(k+1)-k(N+2)}{N(N+2)}\n=\\frac{Nk+N-kN-2k}{N(N+2)}\n=\\frac{N-2k}{N(N+2)}.\n$$", "answer": "$$\\boxed{\\frac{N-2k}{N(N+2)}}$$", "id": "1379693"}, {"introduction": "How do we update our beliefs when the data seems extreme? This final practice tackles that question by examining a scenario where a series of high-risk trials all result in failure. Instead of concluding the success probability is exactly zero, we will use Bayesian methods to construct a full posterior distribution for the probability $p$ and find its median. This exercise demonstrates the power of Bayesian inference to provide nuanced insights and avoid unreasonable conclusions, even with sparse or one-sided data [@problem_id:1923968].", "problem": "A new, high-risk surgical procedure is being developed for a condition that was previously considered untreatable. The true probability of success for this procedure, denoted by $p$, is unknown. Before conducting any clinical trials, medical researchers hold no preconceived bias about this probability, and thus consider every possible value of $p$ in the interval $[0, 1]$ to be equally likely.\n\nThe procedure is then attempted on $n$ different patients. In every one of these $n$ independent trials, the procedure fails to achieve a successful outcome.\n\nBased on this observed data of zero successes in $n$ trials, an updated belief about the success probability $p$ is formed. A medical statistician decides to summarize this updated belief by calculating a central value, $p_{med}$. This value is defined such that, given the observed data, it is equally probable for the true success rate $p$ to be less than or equal to $p_{med}$ as it is for it to be greater than $p_{med}$.\n\nFind a closed-form analytic expression for $p_{med}$ in terms of $n$.", "solution": "Let $p$ denote the success probability. The prior belief is uniform on $[0,1]$, which is the $\\operatorname{Beta}(1,1)$ prior with density $\\pi(p)=1$ for $p\\in[0,1]$.\n\nWe observe $n$ independent Bernoulli trials with $k=0$ successes. The likelihood is\n$$\nL(p\\mid \\text{data}) \\propto p^{k}(1-p)^{n-k}=(1-p)^{n}.\n$$\nBy Bayesâ€™ theorem, the posterior is proportional to prior times likelihood:\n$$\n\\pi(p\\mid \\text{data}) \\propto \\pi(p)L(p\\mid \\text{data}) \\propto (1-p)^{n}, \\quad 0\\leq p\\leq 1.\n$$\nThis is a $\\operatorname{Beta}(1,n+1)$ distribution. Normalizing explicitly,\n$$\n\\int_{0}^{1} (1-p)^{n}\\,dp=\\frac{1}{n+1},\n$$\nso the posterior density is\n$$\nf(p\\mid \\text{data})=(n+1)(1-p)^{n}, \\quad 0\\leq p\\leq 1.\n$$\nThe posterior cumulative distribution function is\n$$\nF(p)=\\int_{0}^{p} (n+1)(1-t)^{n}\\,dt.\n$$\nCompute the integral via $u=1-t$, $du=-dt$:\n$$\nF(p)=(n+1)\\int_{0}^{p} (1-t)^{n}\\,dt=(n+1)\\int_{1-p}^{1} u^{n}\\,du\n= (n+1)\\left[\\frac{u^{n+1}}{n+1}\\right]_{1-p}^{1}\n=1-(1-p)^{n+1}.\n$$\nThe posterior median $p_{med}$ satisfies $F(p_{med})=\\frac{1}{2}$, hence\n$$\n1-(1-p_{med})^{n+1}=\\frac{1}{2}\n\\;\\;\\Longrightarrow\\;\\;\n(1-p_{med})^{n+1}=\\frac{1}{2}.\n$$\nTaking the $(n+1)$-th root gives\n$$\n1-p_{med}=2^{-\\frac{1}{n+1}},\n$$\nso\n$$\np_{med}=1-2^{-\\frac{1}{n+1}}.\n$$\nThis expression is valid for all $n\\geq 0$ and, in particular, for $n\\geq 1$ as in the problem context.", "answer": "$$\\boxed{1-2^{-\\frac{1}{n+1}}}$$", "id": "1923968"}]}