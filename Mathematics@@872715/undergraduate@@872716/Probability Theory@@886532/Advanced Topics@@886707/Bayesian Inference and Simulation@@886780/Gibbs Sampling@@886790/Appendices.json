{"hands_on_practices": [{"introduction": "The most direct way to understand Gibbs sampling is to perform a single iteration by hand. This exercise breaks the process down into its fundamental steps: drawing a new value for each variable from its conditional distribution, given the current values of the other variables. By using the inverse transform sampling method on two different types of distributions, you will gain a concrete understanding of the mechanics that drive this powerful simulation technique. [@problem_id:1920320]", "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.", "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$.\nFor an Exponential rate $y$, the conditional CDF is\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$.\nFor a Poisson mean $x$, the pmf is\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\nSince $p(0 \\mid x)=0.73680.750$, continue to $k=1$:\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\nThen\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.3054  1\\end{pmatrix}}$$", "id": "1920320"}, {"introduction": "While powerful, a Gibbs sampler is not guaranteed to work correctly just because its conditional distributions are well-defined. This thought experiment explores a crucial theoretical requirement: ergodicity, which ensures the sampler can explore the entire state space. You will investigate a pathological case where the sampler becomes trapped in a subset of the distribution, demonstrating that the limiting results can incorrectly depend on the starting point. [@problem_id:1920351]", "problem": "A Gibbs sampler is designed to generate samples from a bivariate probability distribution $p(x, y)$. The process starts from an initial point $(x^{(0)}, y^{(0)})$ and iteratively generates a sequence of points $(x^{(t)}, y^{(t)})$ for $t=1, 2, \\dots$ by alternately drawing from the full conditional distributions: first $x^{(t)} \\sim p(x|y^{(t-1)})$ and then $y^{(t)} \\sim p(y|x^{(t)})$.\n\nThe full conditional distributions are defined in terms of a positive real constant $\\alpha > 0$. They are specified by their probability density functions (PDFs) as follows:\n\n1.  For any given $y > 0$, the conditional PDF of $x$ given $y$ is $p(x|y) = \\alpha \\exp(-\\alpha x)$ for $x > 0$, and $p(x|y) = 0$ for $x \\le 0$.\n2.  For any given $y  0$, the conditional PDF of $x$ given $y$ is $p(x|y) = \\alpha \\exp(\\alpha x)$ for $x  0$, and $p(x|y) = 0$ for $x \\ge 0$.\n3.  For any given $x > 0$, the conditional PDF of $y$ given $x$ is $p(y|x) = \\alpha \\exp(-\\alpha y)$ for $y > 0$, and $p(y|x) = 0$ for $y \\le 0$.\n4.  For any given $x  0$, the conditional PDF of $y$ given $x$ is $p(y|x) = \\alpha \\exp(\\alpha y)$ for $y  0$, and $p(y|x) = 0$ for $y \\ge 0$.\n5.  The conditional distributions are not defined for $x=0$ or $y=0$, as these values occur with zero probability.\n\nTwo independent Gibbs sampling chains, Chain A and Chain B, are run.\n- Chain A is initialized at $(x_A^{(0)}, y_A^{(0)}) = (1, 1)$. Let $E_A$ be the limiting expectation of the $x$-component of the samples, defined as $E_A = \\lim_{t \\to \\infty} \\mathbb{E}[x_A^{(t)}]$.\n- Chain B is initialized at $(x_B^{(0)}, y_B^{(0)}) = (-1, -1)$. Let $E_B$ be the limiting expectation of the $x$-component of the samples, defined as $E_B = \\lim_{t \\to \\infty} \\mathbb{E}[x_B^{(t)}]$.\n\nAssuming these limits exist, compute the value of the difference $E_A - E_B$ in terms of $\\alpha$.", "solution": "The specified Gibbs sampler alternates draws from conditionals that depend only on the sign of the conditioning variable. From the given definitions:\n- If $y>0$ then $x$ is drawn from $p(x|y)=\\alpha \\exp(-\\alpha x)$ on $\\{x>0\\}$; if $y0$ then $x$ is drawn from $p(x|y)=\\alpha \\exp(\\alpha x)$ on $\\{x0\\}$.\n- If $x>0$ then $y$ is drawn from $p(y|x)=\\alpha \\exp(-\\alpha y)$ on $\\{y>0\\}$; if $x0$ then $y$ is drawn from $p(y|x)=\\alpha \\exp(\\alpha y)$ on $\\{y0\\}$.\n\nTherefore, the sign of the coordinates is preserved almost surely once the process starts:\n- If $y^{(t-1)}>0$ then $x^{(t)}>0$ almost surely; then $x^{(t)}>0$ implies $y^{(t)}>0$ almost surely. By induction, starting from $(1,1)$, $(x_{A}^{(t)},y_{A}^{(t)})$ remains in $\\{x>0,y>0\\}$ for all $t\\geq 1$.\n- Similarly, starting from $(-1,-1)$, $(x_{B}^{(t)},y_{B}^{(t)})$ remains in $\\{x0,y0\\}$ for all $t\\geq 1$.\n\nFixing the sign class implies the $x$-conditional does not depend on the value of the conditioning variable, only on its sign. Hence, for all $t\\geq 1$:\n- Chain A: $x_{A}^{(t)}$ has PDF $f_{+}(x)=\\alpha \\exp(-\\alpha x)$ on $\\{x>0\\}$.\n- Chain B: $x_{B}^{(t)}$ has PDF $f_{-}(x)=\\alpha \\exp(\\alpha x)$ on $\\{x0\\}$.\n\nThus the expectations are constant for all $t\\geq 1$ and equal to their stationary values. Compute these expectations explicitly:\n$$\n\\mathbb{E}[x_{A}^{(t)}]\n=\\int_{0}^{\\infty} x\\,\\alpha \\exp(-\\alpha x)\\,dx.\n$$\nIntegrating by parts with $u=x$, $dv=\\alpha \\exp(-\\alpha x)\\,dx$ gives $v=-\\exp(-\\alpha x)$ and\n$$\n\\int_{0}^{\\infty} x\\,\\alpha \\exp(-\\alpha x)\\,dx\n=\\left[-x\\,\\exp(-\\alpha x)\\right]_{0}^{\\infty}\n+\\int_{0}^{\\infty} \\exp(-\\alpha x)\\,dx\n=\\frac{1}{\\alpha}.\n$$\nTherefore $E_{A}=\\lim_{t\\to\\infty}\\mathbb{E}[x_{A}^{(t)}]=\\frac{1}{\\alpha}$.\n\nSimilarly,\n$$\n\\mathbb{E}[x_{B}^{(t)}]\n=\\int_{-\\infty}^{0} x\\,\\alpha \\exp(\\alpha x)\\,dx.\n$$\nWith the substitution $t=-x$ (so $dx=-dt$), this becomes\n$$\n\\int_{-\\infty}^{0} x\\,\\alpha \\exp(\\alpha x)\\,dx\n=-\\int_{0}^{\\infty} t\\,\\alpha \\exp(-\\alpha t)\\,dt\n=-\\frac{1}{\\alpha},\n$$\nhence $E_{B}=\\lim_{t\\to\\infty}\\mathbb{E}[x_{B}^{(t)}]=-\\,\\frac{1}{\\alpha}$.\n\nTherefore,\n$$\nE_{A}-E_{B}=\\frac{1}{\\alpha}-\\left(-\\frac{1}{\\alpha}\\right)=\\frac{2}{\\alpha}.\n$$", "answer": "$$\\boxed{\\frac{2}{\\alpha}}$$", "id": "1920351"}, {"introduction": "A correctly specified sampler can still face practical challenges. One of the most common issues is slow mixing, where the sampler explores the target distribution at an impractically slow rate. This exercise uses a deterministic version of the Gibbs sampler to illustrate how high correlation between variables can drastically hinder movement through the state space, providing a clear intuition for a problem that frequently arises in practice. [@problem_id:1363745]", "problem": "In a high-precision manufacturing process for optical components, two geometric parameters of a lens, $x_1$ and $x_2$, are critical for its performance. These parameters represent normalized deviations from the ideal design specifications. Due to the physics of the fabrication process, these parameters are not independent. An analysis of production data reveals that their joint probability distribution can be modeled by an unnormalized density function $f(x_1, x_2)$ given by:\n$$f(x_1, x_2) \\propto \\exp \\left( -\\frac{1}{2(1-\\rho^2)} (x_1^2 - 2\\rho x_1 x_2 + x_2^2) \\right)$$\nFor this specific process, the correlation coefficient is found to be $\\rho = 0.99$. The mode of the distribution is at $(0, 0)$, which corresponds to a perfect component.\n\nTo simulate the process variations, you are asked to use a Gibbs sampler. You start from an initial state $(x_1^{(0)}, x_2^{(0)}) = (-4.0, -4.1)$, which represents a component at the edge of the acceptable quality range.\n\nYour task is to determine the state of the sampler, $(x_1^{(2)}, x_2^{(2)})$, after two full iterations. A full iteration consists of updating $x_1$ first, and then updating $x_2$. To make the calculation deterministic, you must assume that at each sampling step, the new value drawn for a variable is equal to the mean of its conditional distribution.\n\nCalculate the coordinates of the state $(x_1^{(2)}, x_2^{(2)})$. Report both coordinates in your final answer, rounded to four significant figures.", "solution": "We recognize the given unnormalized joint density\n$$\nf(x_{1},x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(x_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}\\right)\\right)\n$$\nas the kernel of a bivariate normal distribution with mean vector $(0,0)$, unit variances, and correlation coefficient $\\rho$. To derive the full conditional distributions, complete the square in $x_{1}$:\n$$\nx_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}=(x_{1}-\\rho x_{2})^{2}+(1-\\rho^{2})x_{2}^{2}.\n$$\nHence, conditional on $x_{2}$,\n$$\nf(x_{1}\\mid x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}(x_{1}-\\rho x_{2})^{2}\\right),\n$$\nwhich is the kernel of a normal distribution with\n$$\nx_{1}\\mid x_{2} \\sim \\mathcal{N}\\!\\left(\\rho x_{2},\\,1-\\rho^{2}\\right).\n$$\nBy symmetry, we also have\n$$\nx_{2}\\mid x_{1} \\sim \\mathcal{N}\\!\\left(\\rho x_{1},\\,1-\\rho^{2}\\right).\n$$\n\nThe Gibbs sampler updates $x_{1}$ first using $x_{2}$, then updates $x_{2}$ using the new $x_{1}$. Under the deterministic rule that each draw equals the conditional mean, the updates are\n$$\nx_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)},\\qquad x_{2}^{(t+1)}=\\rho\\,x_{1}^{(t+1)}.\n$$\nCombining these,\n$$\nx_{2}^{(t+1)}=\\rho^{2}\\,x_{2}^{(t)},\\qquad x_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)}.\n$$\nStarting from $(x_{1}^{(0)},x_{2}^{(0)})=(-4.0,-4.1)$ and using $\\rho=0.99$, the first full iteration yields\n$$\nx_{1}^{(1)}=\\rho\\,x_{2}^{(0)}=0.99\\times(-4.1)=-4.059,\\qquad\nx_{2}^{(1)}=\\rho\\,x_{1}^{(1)}=0.99\\times(-4.059)=-4.01841.\n$$\nThe second full iteration then gives\n$$\nx_{1}^{(2)}=\\rho\\,x_{2}^{(1)}=0.99\\times(-4.01841)=-3.9782259,\\qquad\nx_{2}^{(2)}=\\rho\\,x_{1}^{(2)}=0.99\\times(-3.9782259)=-3.938443641.\n$$\nRounding each coordinate to four significant figures:\n$$\nx_{1}^{(2)}\\approx -3.978,\\qquad x_{2}^{(2)}\\approx -3.938.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-3.978  -3.938\\end{pmatrix}}$$", "id": "1363745"}]}