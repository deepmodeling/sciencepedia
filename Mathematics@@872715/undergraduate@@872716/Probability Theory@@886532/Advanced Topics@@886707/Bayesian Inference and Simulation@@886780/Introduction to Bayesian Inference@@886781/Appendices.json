{"hands_on_practices": [{"introduction": "Bayesian inference is fundamentally about updating our beliefs in light of new evidence. This first exercise provides a classic and intuitive application: estimating a basketball player's free-throw success rate [@problem_id:1923983]. By combining a prior belief about the player's skill with observed data from a practice session, you will calculate an updated estimate of their ability, illustrating the core mechanics of the Beta-Binomial conjugate model.", "problem": "A sports analyst is evaluating a basketball player's free-throw shooting ability. The analyst models the player's intrinsic success probability for any given shot, denoted by $p$, as a random variable. The analyst's initial belief about $p$ is represented by a Beta distribution with parameters $\\alpha_0 = 4$ and $\\beta_0 = 2$.\n\nThe analyst then observes the player in a practice session. The player is instructed to keep shooting until they have made exactly $r=5$ successful free throws. The analyst records that the player took a total of $n=8$ shots to achieve these 5 successes.\n\nGiven this new experimental data, the analyst updates their model for $p$. The updated model for $p$ will also be a Beta distribution. Calculate the expected value of $p$ according to this updated model. Express your answer as a decimal rounded to three significant figures.", "solution": "We model $p$ with a Beta prior $p \\sim \\mathrm{Beta}(\\alpha_{0},\\beta_{0})$ where $\\alpha_{0}=4$ and $\\beta_{0}=2$. The Beta density is proportional to $p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}$ for $0p1$.\n\nThe data arise from a negative binomial experiment: the number of trials $n$ needed to achieve exactly $r$ successes. The likelihood for observing $n$ given $p$ and $r$ is, up to a factor not depending on $p$,\n$$\n\\mathcal{L}(p\\,|\\,n,r) \\propto p^{r}(1-p)^{n-r}.\n$$\nBy conjugacy, the posterior is proportional to the product of prior and likelihood:\n$$\n\\pi(p\\,|\\,\\text{data}) \\propto p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}\\cdot p^{r}(1-p)^{n-r}\n= p^{(\\alpha_{0}+r)-1}(1-p)^{(\\beta_{0}+n-r)-1},\n$$\nwhich is a Beta distribution,\n$$\np\\,|\\,\\text{data} \\sim \\mathrm{Beta}(\\alpha_{0}+r,\\ \\beta_{0}+n-r).\n$$\nThe posterior mean is\n$$\n\\mathbb{E}[p\\,|\\,\\text{data}] = \\frac{\\alpha_{0}+r}{\\alpha_{0}+r+\\beta_{0}+n-r}\n= \\frac{\\alpha_{0}+r}{\\alpha_{0}+\\beta_{0}+n}.\n$$\nSubstituting $\\alpha_{0}=4$, $\\beta_{0}=2$, $r=5$, and $n=8$ gives\n$$\n\\mathbb{E}[p\\,|\\,\\text{data}] = \\frac{4+5}{4+2+8} = \\frac{9}{14}.\n$$\nAs a decimal rounded to three significant figures,\n$$\n\\frac{9}{14} \\approx 0.643.\n$$", "answer": "$$\\boxed{0.643}$$", "id": "1923983"}, {"introduction": "Beyond providing a single \"best guess,\" a major strength of Bayesian analysis is its ability to rigorously quantify uncertainty. In this problem, we shift from simple proportions to estimating the error variance of a high-tech gyroscope, a critical parameter in engineering and navigation [@problem_id:1924015]. You will learn how to construct a credible interval, which provides a probabilistic range for the unknown standard deviation, moving your understanding from point estimation to interval estimation.", "problem": "A new type of microelectromechanical system (MEMS) gyroscope is being tested for use in satellite attitude control. The random drift error of the gyroscope is modeled as a normally distributed random variable with a mean of exactly zero, as any systematic bias is removed during calibration. The variance of this error, denoted by $\\sigma^2$, is unknown.\n\nBased on the physical principles of the sensor's noise sources, a subject matter expert suggests that a reasonable prior belief for the variance $\\sigma^2$ can be described by an Inverse-Gamma distribution with shape parameter $\\alpha_0 = 3$ and scale parameter $\\beta_0 = 0.02$. The Probability Density Function (PDF) for an Inverse-Gamma distribution, $\\text{Inv-Gamma}(\\alpha, \\beta)$, is given by $f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1} \\exp(-\\frac{\\beta}{x})$ for $x  0$.\n\nDuring testing, the following five independent measurements of the drift error (in degrees per hour) are recorded:\n$x_1 = -0.25$, $x_2 = 0.15$, $x_3 = 0.30$, $x_4 = -0.10$, $x_5 = 0.05$.\n\nYour task is to determine a 95% equal-tailed credible interval for the standard deviation of the drift error, $\\sigma$.\n\nFor your calculations, you may need the following quantiles of a chi-squared ($\\chi^2$) distribution. For a $\\chi^2$ distribution with 11 degrees of freedom, the 0.025 quantile is approximately 3.816 and the 0.975 quantile is approximately 21.920.\n\nCalculate the lower and upper bounds of the credible interval for $\\sigma$. Round your final answers for both bounds to three significant figures and express them in degrees per hour.", "solution": "Let $x_{1},\\dots,x_{n}$ be independent with $x_{i}\\mid\\sigma^{2}\\sim\\mathcal{N}(0,\\sigma^{2})$ and prior $\\sigma^{2}\\sim\\text{Inv-Gamma}(\\alpha_{0},\\beta_{0})$ with density $f(x\\mid\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{-\\alpha-1}\\exp\\!\\left(-\\frac{\\beta}{x}\\right)$ for $x0$. Conjugacy implies the posterior\n$$\n\\sigma^{2}\\mid x_{1:n}\\sim\\text{Inv-Gamma}\\!\\left(\\alpha_{n},\\beta_{n}\\right),\n$$\nwith\n$$\n\\alpha_{n}=\\alpha_{0}+\\frac{n}{2},\\qquad \\beta_{n}=\\beta_{0}+\\frac{1}{2}\\sum_{i=1}^{n}x_{i}^{2}.\n$$\nUsing $n=5$, $\\alpha_{0}=3$, $\\beta_{0}=0.02$, and the data $x_{1}=-0.25$, $x_{2}=0.15$, $x_{3}=0.30$, $x_{4}=-0.10$, $x_{5}=0.05$, compute\n$$\n\\sum_{i=1}^{5}x_{i}^{2}=(-0.25)^{2}+(0.15)^{2}+(0.30)^{2}+(-0.10)^{2}+(0.05)^{2}=0.1875,\n$$\nso\n$$\n\\alpha_{n}=3+\\frac{5}{2}=\\frac{11}{2},\\qquad \\beta_{n}=0.02+\\frac{1}{2}\\cdot 0.1875=0.11375.\n$$\nIf $\\sigma^{2}\\sim\\text{Inv-Gamma}(\\alpha,\\beta)$, then the transformation\n$$\nU=\\frac{2\\beta}{\\sigma^{2}}\n$$\nsatisfies $U\\sim\\chi^{2}_{2\\alpha}$. Therefore, for the posterior,\n$$\n\\frac{2\\beta_{n}}{\\sigma^{2}}\\sim\\chi^{2}_{2\\alpha_{n}}=\\chi^{2}_{11}.\n$$\nLet $q_{p}$ denote the $p$-quantile of $\\chi^{2}_{11}$. An equal-tailed $95$ percent credible interval for $\\sigma^{2}$ is obtained by inverting the tails:\n$$\n\\left[\\frac{2\\beta_{n}}{q_{0.975}},\\,\\frac{2\\beta_{n}}{q_{0.025}}\\right].\n$$\nUsing $q_{0.025}\\approx 3.816$ and $q_{0.975}\\approx 21.920$ with $2\\beta_{n}=0.2275$ gives\n$$\n\\text{lower bound for }\\sigma^{2}=\\frac{0.2275}{21.920}\\approx 0.01037865,\\qquad\n\\text{upper bound for }\\sigma^{2}=\\frac{0.2275}{3.816}\\approx 0.05961740.\n$$\nSince $\\sigma=\\sqrt{\\sigma^{2}}$ is a monotone transformation, the equal-tailed credible interval for $\\sigma$ is the square root of the above bounds:\n$$\n\\text{lower bound for }\\sigma=\\sqrt{0.01037865}\\approx 0.1019,\\qquad\n\\text{upper bound for }\\sigma=\\sqrt{0.05961740}\\approx 0.2442.\n$$\nRounding both to three significant figures yields $0.102$ and $0.244$, in degrees per hour.", "answer": "$$\\boxed{\\begin{pmatrix}0.102  0.244\\end{pmatrix}}$$", "id": "1924015"}, {"introduction": "While conjugate priors offer elegant mathematical solutions, many real-world problems involve models where the posterior distribution cannot be calculated analytically. This exercise introduces you to the world of computational statistics, which is essential for modern Bayesian practice [@problem_id:1924028]. You will tackle a scenario with a non-conjugate prior and lay the groundwork for a rejection sampling algorithm, a foundational technique for drawing samples from complex, non-standard distributions.", "problem": "A research team is studying the lifetime of a newly developed electronic component. They model the lifetime $T$, measured in hours, as an exponential random variable with an unknown rate parameter $\\lambda  0$. The probability density function (PDF) for a single observation $t$ is given by $p(t|\\lambda) = \\lambda \\exp(-\\lambda t)$. The team's prior belief about the parameter $\\lambda$ is described by a half-Cauchy distribution with a scale parameter of 1, whose PDF is $p(\\lambda) = \\frac{2}{\\pi(1+\\lambda^2)}$ for $\\lambda  0$.\n\nA single component is tested and is observed to fail at time $t_0$. To make inferences about $\\lambda$, the team needs to generate samples from the posterior distribution $p(\\lambda|t_0)$. Since this posterior distribution is not a standard one, they decide to use a rejection sampling algorithm. As the target function for sampling (a function proportional to the posterior density), they use $\\tilde{\\pi}(\\lambda) = \\frac{\\lambda \\exp(-\\lambda t_0)}{1+\\lambda^2}$. For their proposal distribution, they choose an exponential distribution with a rate parameter equal to the observed failure time, i.e., $g(\\lambda) = t_0 \\exp(-t_0 \\lambda)$.\n\nRejection sampling requires finding a constant $M$ such that $\\tilde{\\pi}(\\lambda) \\le M g(\\lambda)$ for all $\\lambda  0$. Determine the smallest possible value for this constant $M$. Express your answer as a closed-form analytic expression in terms of $t_0$.", "solution": "We need the smallest constant $M$ such that $\\tilde{\\pi}(\\lambda) \\le M g(\\lambda)$ for all $\\lambda0$. This is equivalent to\n$$\nM \\ge \\sup_{\\lambda0} \\frac{\\tilde{\\pi}(\\lambda)}{g(\\lambda)}.\n$$\nWith $\\tilde{\\pi}(\\lambda) = \\frac{\\lambda \\exp(-\\lambda t_{0})}{1+\\lambda^{2}}$ and $g(\\lambda) = t_{0} \\exp(-t_{0} \\lambda)$, the ratio simplifies as\n$$\n\\frac{\\tilde{\\pi}(\\lambda)}{g(\\lambda)} = \\frac{\\lambda \\exp(-\\lambda t_{0})}{(1+\\lambda^{2}) \\, t_{0} \\exp(-t_{0} \\lambda)} = \\frac{\\lambda}{t_{0} \\left(1+\\lambda^{2}\\right)}.\n$$\nTherefore,\n$$\nM_{\\min} = \\sup_{\\lambda0} \\frac{\\lambda}{t_{0} \\left(1+\\lambda^{2}\\right)} = \\frac{1}{t_{0}} \\sup_{\\lambda0} \\frac{\\lambda}{1+\\lambda^{2}}.\n$$\nDefine $h(\\lambda) = \\frac{\\lambda}{1+\\lambda^{2}}$ for $\\lambda0$. Differentiate to find the critical points:\n$$\nh'(\\lambda) = \\frac{(1+\\lambda^{2}) - 2\\lambda^{2}}{(1+\\lambda^{2})^{2}} = \\frac{1 - \\lambda^{2}}{(1+\\lambda^{2})^{2}}.\n$$\nSet $h'(\\lambda)=0$ to obtain $1-\\lambda^{2}=0$, hence the only positive critical point is $\\lambda=1$. Check monotonicity: $h'(\\lambda)0$ for $0\\lambda1$ and $h'(\\lambda)0$ for $\\lambda1$, so $\\lambda=1$ is the global maximizer on $(0,\\infty)$. Evaluate\n$$\n\\sup_{\\lambda0} h(\\lambda) = h(1) = \\frac{1}{1+1} = \\frac{1}{2}.\n$$\nThus,\n$$\nM_{\\min} = \\frac{1}{t_{0}} \\cdot \\frac{1}{2} = \\frac{1}{2 t_{0}}.\n$$\nThis constant satisfies $\\tilde{\\pi}(\\lambda) \\le M g(\\lambda)$ for all $\\lambda0$ and is the smallest possible.", "answer": "$$\\boxed{\\frac{1}{2 t_{0}}}$$", "id": "1924028"}]}