{"hands_on_practices": [{"introduction": "The Metropolis-Hastings algorithm is a cornerstone of MCMC methods. Its elegance lies in a simple probabilistic rule for deciding whether to move to a new proposed state or stay put, ensuring that over time, the visited states map out the desired target distribution. This first exercise provides fundamental practice in applying this rule by calculating the acceptance probability for a specific move, which is the core computational step of the algorithm. [@problem_id:1371728]", "problem": "A data scientist is implementing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a posterior probability distribution for a parameter $x$. The target distribution, $\\pi(x)$, is proportional to the exponential of the negative absolute value of the parameter, such that $\\pi(x) \\propto \\exp(-|x|)$.\n\nThe scientist uses the Metropolis algorithm with a symmetric proposal distribution $q(x'|x)$, where the probability of proposing a new state $x'$ given the current state $x$ is equal to the probability of proposing $x$ given $x'$ (i.e., $q(x'|x) = q(x|x')$).\n\nSuppose that at a certain step in the simulation, the current state of the chain is $x = 1.5$. The algorithm then proposes a move to a new candidate state $x' = 2.0$.\n\nCalculate the acceptance probability for this specific move. Your answer should be a dimensionless real number. Round your final answer to four significant figures.", "solution": "The Metropolis acceptance probability for a move from $x$ to $x'$ with a symmetric proposal $q(x'|x)=q(x|x')$ is\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\nGiven the target distribution $\\pi(x)\\propto \\exp(-|x|)$, the ratio simplifies to\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\nWith $x=1.5$ and $x'=2.0$, we have $|x|=1.5$ and $|x'|=2.0$, so\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\nTherefore,\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\nNumerically, $\\exp(-0.5)\\approx 0.6065$ when rounded to four significant figures.", "answer": "$$\\boxed{0.6065}$$", "id": "1371728"}, {"introduction": "While Metropolis-Hastings is broadly applicable, a highly efficient MCMC variant known as the Gibbs sampler is often used for multi-dimensional problems. Instead of proposing a move for all variables at once, Gibbs sampling updates one variable at a time by drawing from its full conditional distribution. The main task in building a Gibbs sampler is therefore to correctly derive these conditional distributions from the joint target density. This practice problem focuses on this essential skill, asking you to find the conditional distributions for a simple, geometrically-defined joint distribution. [@problem_id:1932854]", "problem": "A Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for generating a sequence of observations that approximate a target multivariate probability distribution, which is especially useful when direct sampling is difficult. A crucial step in implementing a Gibbs sampler is to derive the full conditional distribution for each variable in the model.\n\nConsider a bivariate random vector $(X, Y)$ with a joint probability density function (PDF), $p(x, y)$, that is uniform over a triangular region $\\mathcal{R}$ in the $xy$-plane. The region $\\mathcal{R}$ is defined by the inequalities $x>0$, $y>0$, and $x+y < 1$. The joint PDF is therefore:\n$$\np(x, y) =\n\\begin{cases}\n2 & \\text{if } (x,y) \\in \\mathcal{R} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nTo construct a Gibbs sampler for this distribution, we need the full conditional distributions, $p(x|y)$ and $p(y|x)$.\n\nWhich of the following options correctly identifies these two conditional distributions? Note that $\\text{Uniform}(a,b)$ denotes the uniform distribution on the interval $(a,b)$.\n\nA: $p(x|y)$ is Uniform$(0, 1-y)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, 1-x)$ for $x \\in (0,1)$.\n\nB: $p(x|y)$ is Uniform$(0, 1)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, 1)$ for $x \\in (0,1)$.\n\nC: $p(x|y)$ is Uniform$(0, y)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, x)$ for $x \\in (0,1)$.\n\nD: The conditional distributions are not uniform.\n\nE: $p(x|y)$ is Uniform$(y, 1)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(x, 1)$ for $x \\in (0,1)$.", "solution": "We are given a joint density $p(x,y)$ that is uniform over the region $\\mathcal{R}=\\{(x,y): x>0,\\ y>0,\\ x+y<1\\}$ with $p(x,y)=2$ on $\\mathcal{R}$ and $0$ otherwise. To obtain the full conditional distributions, we use the definition of conditionals via the marginals.\n\nFirst, compute the marginal density of $Y$. For a fixed $y$, the admissible $x$ values are those satisfying $0<x<1-y$, provided $0<y<1$. Therefore,\n$$\np_{Y}(y)=\\int_{-\\infty}^{\\infty} p(x,y)\\,dx=\\int_{0}^{1-y} 2\\,dx=2(1-y), \\quad \\text{for } 0<y<1,\n$$\nand $p_{Y}(y)=0$ otherwise.\n\nThen the conditional density of $X$ given $Y=y$ is\n$$\np_{X\\mid Y}(x\\mid y)=\\frac{p(x,y)}{p_{Y}(y)}=\\frac{2}{2(1-y)}=\\frac{1}{1-y},\n$$\nvalid exactly on the support where $p(x,y)>0$, namely $0<x<1-y$ (and $0<y<1$). This is the density of a Uniform distribution on $(0,1-y)$:\n$$\nX\\mid Y=y \\sim \\text{Uniform}(0,1-y), \\quad \\text{for } 0<y<1.\n$$\n\nBy symmetry, compute the marginal of $X$:\n$$\np_{X}(x)=\\int_{-\\infty}^{\\infty} p(x,y)\\,dy=\\int_{0}^{1-x} 2\\,dy=2(1-x), \\quad \\text{for } 0<x<1,\n$$\nand $p_{X}(x)=0$ otherwise.\n\nThen the conditional density of $Y$ given $X=x$ is\n$$\np_{Y\\mid X}(y\\mid x)=\\frac{p(x,y)}{p_{X}(x)}=\\frac{2}{2(1-x)}=\\frac{1}{1-x},\n$$\nvalid on $0<y<1-x$ (and $0<x<1$). This is Uniform on $(0,1-x)$:\n$$\nY\\mid X=x \\sim \\text{Uniform}(0,1-x), \\quad \\text{for } 0<x<1.\n$$\n\nTherefore, the correct option is that both conditionals are uniform on the respective truncated intervals determined by the constraint $x+y<1$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1932854"}, {"introduction": "A powerful MCMC sampler must be able to explore the entire landscape of the target distribution, a property known as irreducibility. If a sampler is confined to only a portion of the state space, it will fail to generate samples that accurately represent the true distribution. This problem presents a hypothetical scenario involving a flawed Gibbs sampler to illustrate the crucial concept of irreducibility. By analyzing why this sampler fails, you will gain a deeper understanding of the structural requirements for a valid MCMC algorithm and the importance of diagnosing its behavior. [@problem_id:1932797]", "problem": "A statistician is developing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a target probability distribution that is uniform over a two-dimensional annulus. The annulus, denoted by $A$, is defined as the set of points $(x, y)$ in the Cartesian plane such that $r_1^2 < x^2 + y^2 < r_2^2$, for two given positive constants $r_1$ and $r_2$ with $r_1 < r_2$.\n\nThe statistician decides to use a component-wise Gibbs sampler. At each step $t$, the sampler generates a new state $(x_{t+1}, y_{t+1})$ from the current state $(x_t, y_t)$ using the following two sub-steps:\n1.  Draw a new value $x_{t+1}$ from the full conditional distribution $p(x | y=y_t)$.\n2.  Draw a new value $y_{t+1}$ from the full conditional distribution $p(y | x=x_{t+1})$.\n\nHowever, the specific implementation of the sampler has a subtle flaw. When sampling from a conditional distribution whose support consists of two disjoint intervals (one positive and one negative), the sampler's behavior depends on the sign of the previous value for that coordinate. For example, to sample $x_{t+1}$ given $y_t$, the implementation checks the sign of the previous value, $x_t$. If $x_t > 0$, it will only sample a new $x_{t+1}$ from the positive region of the conditional support. If $x_t < 0$, it will only sample from the negative region. A similar rule based on the sign of $y_t$ is applied when sampling $y_{t+1}$. (Assume the sampler is never initialized at or transitions to a state where a coordinate is exactly zero).\n\nThe sampler is initialized at a point $(x_0, y_0)$ located in the first quadrant of the annulus (i.e., $x_0 > 0$ and $y_0 > 0$). After running the sampler for a very large number of iterations, which of the following statements best describes the resulting collection of sampled points?\n\nA. The samples will correctly approximate the uniform distribution over the entire annulus $A$.\n\nB. The samples will approximate the uniform distribution over the portion of the annulus in the first quadrant, i.e., the region defined by $A \\cap \\{ (x,y) \\mid x>0, y>0 \\}$.\n\nC. The samples will be confined to the first quadrant, but due to the axis-aligned moves, their distribution will be non-uniform and will concentrate near the boundaries $x=y$.\n\nD. The samples will converge to the uniform distribution over the upper half of the annulus, i.e., the region defined by $A \\cap \\{ (x,y) \\mid y>0 \\}$.\n\nE. The chain will eventually get stuck at a fixed point and stop generating new, distinct samples.", "solution": "The problem asks us to determine the long-term behavior of a flawed Gibbs sampler designed to sample from a uniform distribution on an annulus $A = \\{(x, y) \\mid r_1^2 < x^2 + y^2 < r_2^2\\}$.\n\nFirst, let's analyze the true conditional distributions required for a correct Gibbs sampler. The target probability density function (PDF) is $\\pi(x,y) = C$ for $(x,y) \\in A$ and $0$ otherwise, where $C = 1/(\\pi(r_2^2 - r_1^2))$ is the normalization constant.\n\nThe full conditional distribution for $x$ given $y$ is found by treating $\\pi(x,y)$ as a function of $x$ for a fixed $y$:\n$p(x|y) \\propto \\pi(x,y) \\propto \\mathbf{1}_{r_1^2 < x^2+y^2 < r_2^2}$, where $\\mathbf{1}$ is the indicator function.\nThis is equivalent to $p(x|y)$ being uniform on the set of $x$ values satisfying $r_1^2 - y^2 < x^2 < r_2^2 - y^2$.\n\nLet's analyze the support of this conditional distribution:\n- If $|y| \\ge r_2$, there is no $x$ that satisfies the inequality, so the support is empty.\n- If $r_1 \\le |y| < r_2$, the condition becomes $x^2 < r_2^2 - y^2$. The support for $x$ is a single connected interval: $(-\\sqrt{r_2^2 - y^2}, \\sqrt{r_2^2 - y^2})$.\n- If $|y| < r_1$, the condition is $\\sqrt{r_1^2 - y^2} < |x| < \\sqrt{r_2^2 - y^2}$. The support for $x$ is the union of two disjoint intervals: $(-\\sqrt{r_2^2 - y^2}, -\\sqrt{r_1^2 - y^2}) \\cup (\\sqrt{r_1^2 - y^2}, \\sqrt{r_2^2 - y^2})$.\n\nA symmetric analysis holds for the conditional distribution $p(y|x)$.\n\nNow, let's consider the flawed sampler. The flaw is that when the conditional support is disjoint, the sampler restricts itself to the component whose sign matches the sign of the previous value of that coordinate. The sampler starts at $(x_0, y_0)$ with $x_0 > 0$ and $y_0 > 0$.\n\nLet the state at iteration $t$ be $(x_t, y_t)$, and assume for induction that $x_t > 0$ and $y_t > 0$.\n1.  **Sample $x_{t+1}$ from $p(x | y_t)$:**\n    The sampler must draw a new $x_{t+1}$. The implementation flaw dictates that since $x_t > 0$, the sampler will only choose a value from the positive part of the support of $p(x|y_t)$. Whether the support is one interval or two, the positive part consists only of positive values for $x$. Therefore, the new sample $x_{t+1}$ must be positive. The state is now $(x_{t+1}, y_t)$ where $x_{t+1} > 0$ and $y_t > 0$.\n\n2.  **Sample $y_{t+1}$ from $p(y | x_{t+1})$:**\n    The sampler must draw a new $y_{t+1}$. Since the previous value $y_t$ was positive, the flawed implementation will only choose a value from the positive part of the support of $p(y|x_{t+1})$. Therefore, the new sample $y_{t+1}$ must be positive.\n\nThe final state for the iteration is $(x_{t+1}, y_{t+1})$, where both components are positive. By induction, if the sampler starts in the first quadrant ($x>0, y>0$), all subsequent samples will also be in the first quadrant.\n\nThis means the Markov chain is confined to the region $A_{++} = A \\cap \\{ (x,y) \\mid x>0, y>0 \\}$. Since the chain cannot transition from $A_{++}$ to any other part of the annulus (e.g., the part in the second quadrant), it is not irreducible on the full state space $A$. A non-irreducible chain cannot converge to the target distribution over the entire state space. Thus, Option A is incorrect.\n\nNow we must determine the behavior of the sampler *within* the reachable set $A_{++}$. Let's define a new target distribution $\\pi_{++}(x,y)$ that is uniform on $A_{++}$. For this new target, the conditional $p_{++}(x|y)$ is uniform on the set of $x>0$ such that $(x,y) \\in A_{++}$. This is precisely the distribution from which our flawed sampler draws when $x_t > 0$. Similarly, the conditional $p_{++}(y|x)$ is the distribution from which the flawed sampler draws when $y_t > 0$.\n\nTherefore, the flawed sampler, when restricted to the first quadrant, is actually a *correct* Gibbs sampler for the target distribution $\\pi_{++}(x,y)$ (uniform on $A_{++}$). Since this is a valid Gibbs sampler on a connected region, it will converge to its stationary distribution, which is $\\pi_{++}$. The collected samples will thus approximate a uniform distribution over the portion of the annulus in the first quadrant. This corresponds to Option B.\n\nLet's evaluate the other options:\n- C: The sampler performs uniform draws from its conditional distributions. There is no mechanism that would cause the samples to concentrate in a non-uniform way within the quadrant. The limiting distribution on the reachable set will be uniform.\n- D: The sampler is confined to the first quadrant, not the entire upper half of the annulus, because the sign of $x$ is also preserved. It cannot reach the part of the upper half where $x<0$.\n- E: The draws are probabilistic from continuous intervals, so the chain will not get stuck at a single fixed point. It will continue to explore the entire region $A_{++}$.\n\nTherefore, the only correct description of the sampler's long-term behavior is that it will explore and uniformly sample from the part of the annulus in the first quadrant.", "answer": "$$\\boxed{B}$$", "id": "1932797"}]}