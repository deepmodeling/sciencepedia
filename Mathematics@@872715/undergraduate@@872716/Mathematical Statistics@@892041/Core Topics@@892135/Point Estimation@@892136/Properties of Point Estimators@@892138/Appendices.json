{"hands_on_practices": [{"introduction": "One of the most fundamental properties we desire in an estimator is unbiasedness, which means that on average, the estimator hits the true parameter value. This exercise provides a hands-on check of this property for a proposed estimator of the maximum value of a uniform distribution. By calculating the expected value, you will determine if the estimator $\\hat{\\theta} = 2\\bar{X}$ provides a systematically accurate guess for $\\theta$ [@problem_id:1948686].", "problem": "In a materials science laboratory, a process is developed to create thin films whose thickness, $X$, is a random variable. The process is calibrated such that the thickness is uniformly distributed on the interval $[0, \\theta]$, where $\\theta$ is an unknown maximum possible thickness. To estimate this parameter $\\theta$, a researcher collects a random sample of $n$ films, with thicknesses denoted by $X_1, X_2, \\ldots, X_n$. An estimator for $\\theta$ is proposed as $\\hat{\\theta} = 2\\bar{X}$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ is the sample mean of the thicknesses.\n\nDetermine whether the proposed estimator $\\hat{\\theta}$ is biased or unbiased for the true parameter $\\theta$.\n\nA. Yes, the estimator is unbiased.\n\nB. No, the estimator has a positive bias.\n\nC. No, the estimator has a negative bias.\n\nD. The nature of the bias (positive, negative, or zero) depends on the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. with $X_{i}\\sim \\text{Uniform}(0,\\theta)$. The density is $f_{X}(x;\\theta)=\\frac{1}{\\theta}$ for $0\\leq x\\leq \\theta$, and $0$ otherwise. Compute the mean of a single observation:\n$$\n\\mathbb{E}[X_{i}]=\\int_{0}^{\\theta}x\\cdot \\frac{1}{\\theta}\\,dx=\\frac{1}{\\theta}\\left[\\frac{x^{2}}{2}\\right]_{0}^{\\theta}=\\frac{1}{\\theta}\\cdot \\frac{\\theta^{2}}{2}=\\frac{\\theta}{2}.\n$$\nBy linearity of expectation, the sample mean satisfies\n$$\n\\mathbb{E}[\\bar{X}]=\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}]=\\frac{1}{n}\\cdot n\\cdot \\frac{\\theta}{2}=\\frac{\\theta}{2}.\n$$\nFor the proposed estimator $\\hat{\\theta}=2\\bar{X}$,\n$$\n\\mathbb{E}[\\hat{\\theta}]=\\mathbb{E}[2\\bar{X}]=2\\,\\mathbb{E}[\\bar{X}]=2\\cdot \\frac{\\theta}{2}=\\theta.\n$$\nThe bias is defined as $\\text{Bias}(\\hat{\\theta})=\\mathbb{E}[\\hat{\\theta}]-\\theta$, hence\n$$\n\\text{Bias}(\\hat{\\theta})=\\theta-\\theta=0,\n$$\nso the estimator is unbiased. Therefore the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1948686"}, {"introduction": "When we have more than one unbiased estimator for the same parameter, how do we decide which one is better? This practice introduces the Mean Squared Error (MSE) as a crucial tool for comparing estimator precision, a concept known as efficiency. You will compare the sample mean and the sample variance as two different unbiased estimators for the rate parameter $\\lambda$ of a Poisson distribution, revealing which method provides a more reliable estimate [@problem_id:1948719].", "problem": "Two competing research groups are analyzing data on the number of photons detected from a distant, faint star over a series of short, equal-duration observation windows. Based on theoretical models of photon emission, the number of photons, $X$, detected in any given window is assumed to follow a Poisson distribution with an unknown mean rate $\\lambda$. Both groups have access to the same dataset, which consists of photon counts $X_1, X_2, \\ldots, X_n$ from $n$ independent observation windows, where the sample size $n > 1$.\n\nThe first group proposes to estimate the rate $\\lambda$ using the sample mean, which they denote as $\\hat{\\lambda}_1$:\n$$\n\\hat{\\lambda}_1 = \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\nThe second group, leveraging the property that the variance of a Poisson distribution is equal to its mean, proposes an alternative estimator using the sample variance, denoted as $\\hat{\\lambda}_2$:\n$$\n\\hat{\\lambda}_2 = S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2\n$$\nTo determine which estimator is more precise for this particular distribution, you are tasked with comparing their performance using the Mean Squared Error (MSE).\n\nCalculate the ratio $R = \\frac{\\text{MSE}(\\hat{\\lambda}_2)}{\\text{MSE}(\\hat{\\lambda}_1)}$. Your final answer should be a closed-form analytic expression in terms of the rate $\\lambda$ and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be independent with $X_{i}\\sim \\text{Poisson}(\\lambda)$. Denote the sample mean by $\\bar{X}$ and the sample variance by $S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$. For any estimator $\\hat{\\theta}$ of a parameter $\\theta$, the Mean Squared Error is\n$$\n\\text{MSE}(\\hat{\\theta})=\\mathbb{E}\\big[(\\hat{\\theta}-\\theta)^{2}\\big]=\\operatorname{Var}(\\hat{\\theta})+\\big(\\mathbb{E}[\\hat{\\theta}]-\\theta\\big)^{2}.\n$$\n\nFirst estimator $\\hat{\\lambda}_{1}=\\bar{X}$:\nSince $\\mathbb{E}[X_{i}]=\\lambda$ and $\\operatorname{Var}(X_{i})=\\lambda$, independence implies\n$$\n\\mathbb{E}[\\bar{X}]=\\lambda,\\qquad \\operatorname{Var}(\\bar{X})=\\frac{\\operatorname{Var}(X_{i})}{n}=\\frac{\\lambda}{n}.\n$$\nThus $\\hat{\\lambda}_{1}$ is unbiased and\n$$\n\\text{MSE}(\\hat{\\lambda}_{1})=\\operatorname{Var}(\\bar{X})=\\frac{\\lambda}{n}.\n$$\n\nSecond estimator $\\hat{\\lambda}_{2}=S^{2}$:\nFor any distribution with finite second moment, $S^{2}$ is an unbiased estimator of the population variance $\\sigma^{2}$, so $\\mathbb{E}[S^{2}]=\\sigma^{2}$. Here $\\sigma^{2}=\\operatorname{Var}(X_{i})=\\lambda$, hence $\\mathbb{E}[\\hat{\\lambda}_{2}]=\\lambda$, so $\\hat{\\lambda}_{2}$ is also unbiased and\n$$\n\\text{MSE}(\\hat{\\lambda}_{2})=\\operatorname{Var}(S^{2}).\n$$\nA standard formula for the variance of the unbiased sample variance in terms of the fourth central moment $\\mu_{4}=\\mathbb{E}[(X-\\mathbb{E}X)^{4}]$ and $\\sigma^{2}$ is\n$$\n\\operatorname{Var}(S^{2})=\\frac{1}{n}\\left(\\mu_{4}-\\frac{n-3}{n-1}\\sigma^{4}\\right).\n$$\nFor a Poisson$(\\lambda)$ distribution, all cumulants equal $\\lambda$, and the fourth central moment is $\\mu_{4}=\\kappa_{4}+3\\kappa_{2}^{2}=\\lambda+3\\lambda^{2}$, while $\\sigma^{2}=\\lambda$. Substituting,\n$$\n\\operatorname{Var}(S^{2})=\\frac{1}{n}\\left(\\lambda+3\\lambda^{2}-\\frac{n-3}{n-1}\\lambda^{2}\\right)\n=\\frac{1}{n}\\left(\\lambda+\\lambda^{2}\\left(3-\\frac{n-3}{n-1}\\right)\\right).\n$$\nCompute the bracketed term:\n$$\n3-\\frac{n-3}{n-1}=\\frac{3(n-1)-(n-3)}{n-1}=\\frac{2n}{n-1},\n$$\nso\n$$\n\\operatorname{Var}(S^{2})=\\frac{\\lambda}{n}+\\frac{2\\lambda^{2}}{n-1}.\n$$\nTherefore,\n$$\n\\text{MSE}(\\hat{\\lambda}_{2})=\\frac{\\lambda}{n}+\\frac{2\\lambda^{2}}{n-1},\\qquad \\text{MSE}(\\hat{\\lambda}_{1})=\\frac{\\lambda}{n}.\n$$\nThe required ratio is\n$$\nR=\\frac{\\text{MSE}(\\hat{\\lambda}_{2})}{\\text{MSE}(\\hat{\\lambda}_{1})}\n=\\frac{\\frac{\\lambda}{n}+\\frac{2\\lambda^{2}}{n-1}}{\\frac{\\lambda}{n}}\n=1+\\frac{2n\\lambda}{n-1}.\n$$", "answer": "$$\\boxed{1+\\frac{2 n \\lambda}{n-1}}$$", "id": "1948719"}, {"introduction": "Rather than choosing between two estimators, we can often create a superior estimator by combining them. This exercise demonstrates how to find the optimal combination of two independent, unbiased estimators to minimize the variance of the final result. This powerful technique of weighted averaging is fundamental in many scientific fields, and you will derive the optimal weights that give more influence to the more precise measurement [@problem_id:1948718].", "problem": "Two independent research laboratories, A and B, have developed methods to estimate an unknown physical constant, denoted by $\\mu$. Both methods provide unbiased estimators for $\\mu$. Let $\\hat{\\mu}_A$ be the estimator from Lab A and $\\hat{\\mu}_B$ be the estimator from Lab B. We are given that the estimators are independent. Due to different experimental techniques, the estimators have different variances, given by $\\text{Var}(\\hat{\\mu}_A) = \\sigma_A^2$ and $\\text{Var}(\\hat{\\mu}_B) = \\sigma_B^2$.\n\nA new combined estimator, $\\hat{\\mu}_C$, is proposed as a weighted average of the two:\n$$ \\hat{\\mu}_C = c \\hat{\\mu}_A + (1-c) \\hat{\\mu}_B $$\nwhere $c$ is a real constant.\n\nDetermine the value of $c$ that minimizes the variance of the combined estimator $\\hat{\\mu}_C$. Express your answer in terms of $\\sigma_A^2$ and $\\sigma_B^2$.", "solution": "We are given two independent unbiased estimators $\\hat{\\mu}_{A}$ and $\\hat{\\mu}_{B}$ with variances $\\text{Var}(\\hat{\\mu}_{A})=\\sigma_{A}^{2}$ and $\\text{Var}(\\hat{\\mu}_{B})=\\sigma_{B}^{2}$. Consider the combined estimator $\\hat{\\mu}_{C}=c\\hat{\\mu}_{A}+(1-c)\\hat{\\mu}_{B}$.\n\nFirst, verify unbiasedness of $\\hat{\\mu}_{C}$. Using linearity of expectation and unbiasedness of $\\hat{\\mu}_{A}$ and $\\hat{\\mu}_{B}$,\n$$\n\\mathbb{E}[\\hat{\\mu}_{C}]=c\\,\\mathbb{E}[\\hat{\\mu}_{A}]+(1-c)\\,\\mathbb{E}[\\hat{\\mu}_{B}]=c\\mu+(1-c)\\mu=\\mu,\n$$\nso $\\hat{\\mu}_{C}$ is unbiased for all real $c$.\n\nNext, compute the variance of $\\hat{\\mu}_{C}$. Using independence so that the covariance term is zero,\n$$\n\\text{Var}(\\hat{\\mu}_{C})=\\text{Var}\\big(c\\hat{\\mu}_{A}+(1-c)\\hat{\\mu}_{B}\\big)=c^{2}\\sigma_{A}^{2}+(1-c)^{2}\\sigma_{B}^{2}.\n$$\nDefine $V(c)=c^{2}\\sigma_{A}^{2}+(1-c)^{2}\\sigma_{B}^{2}$ and minimize it with respect to $c$. Differentiate:\n$$\nV'(c)=2c\\,\\sigma_{A}^{2}+2(c-1)\\,\\sigma_{B}^{2}.\n$$\nSet $V'(c)=0$:\n$$\n2c\\,\\sigma_{A}^{2}+2(c-1)\\,\\sigma_{B}^{2}=0 \\;\\;\\Longrightarrow\\;\\; c\\,\\sigma_{A}^{2}+c\\,\\sigma_{B}^{2}-\\sigma_{B}^{2}=0,\n$$\n$$\nc(\\sigma_{A}^{2}+\\sigma_{B}^{2})=\\sigma_{B}^{2} \\;\\;\\Longrightarrow\\;\\; c=\\frac{\\sigma_{B}^{2}}{\\sigma_{A}^{2}+\\sigma_{B}^{2}}.\n$$\nCheck that this critical point is a minimum via the second derivative:\n$$\nV''(c)=2\\sigma_{A}^{2}+2\\sigma_{B}^{2}>0,\n$$\nwhich confirms a unique minimum. Therefore, the variance-minimizing weight is $c=\\sigma_{B}^{2}/(\\sigma_{A}^{2}+\\sigma_{B}^{2})$.", "answer": "$$\\boxed{\\frac{\\sigma_{B}^{2}}{\\sigma_{A}^{2}+\\sigma_{B}^{2}}}$$", "id": "1948718"}]}