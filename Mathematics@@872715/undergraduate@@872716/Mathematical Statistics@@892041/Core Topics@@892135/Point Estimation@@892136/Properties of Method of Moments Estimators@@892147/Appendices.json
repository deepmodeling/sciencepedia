{"hands_on_practices": [{"introduction": "The first step in understanding the method of moments is to apply it to a concrete example. This exercise [@problem_id:1948410] provides a scenario involving the Pareto distribution, a model frequently used in fields like actuarial science and economics to describe phenomena with heavy tails. By working through this problem, you will practice the fundamental technique of equating a population moment, in this case the mean $\\mathbb{E}[X]$, with its sample equivalent, the sample mean $\\bar{X}$, to derive an estimator for the distribution's shape parameter $\\alpha$.", "problem": "An actuary is modeling the magnitude of large insurance claims. It is proposed that the size of a claim $X$, normalized to be greater than 1 (in units of a certain large monetary amount), follows a Pareto distribution. The probability density function (PDF) for a single claim is given by\n$$f(x; \\alpha) = \\alpha x^{-(\\alpha+1)}$$\nfor $x  1$, where $\\alpha  1$ is a shape parameter that determines the heaviness of the tail of the distribution.\n\nSuppose a random sample of $n$ such claims, $X_1, X_2, \\dots, X_n$, is collected. Using the method of moments, find the estimator for the parameter $\\alpha$. Express your answer in terms of the sample mean, which is denoted by $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.", "solution": "We are given a Pareto distribution with support $x1$ and density $f(x;\\alpha)=\\alpha x^{-(\\alpha+1)}$ for $\\alpha1$. The method of moments estimator equates the sample mean $\\bar{X}$ to the population mean $\\mathbb{E}[X]$.\n\nFirst, compute the population mean:\n$$\n\\mathbb{E}[X] = \\int_{1}^{\\infty} x \\cdot \\alpha x^{-(\\alpha+1)} \\, dx = \\alpha \\int_{1}^{\\infty} x^{-\\alpha} \\, dx.\n$$\nFor $\\alpha1$, the integral converges, and\n$$\n\\int_{1}^{\\infty} x^{-\\alpha} \\, dx = \\left[ \\frac{x^{1-\\alpha}}{1-\\alpha} \\right]_{1}^{\\infty} = 0 - \\frac{1}{1-\\alpha} = \\frac{1}{\\alpha-1}.\n$$\nTherefore,\n$$\n\\mathbb{E}[X] = \\alpha \\cdot \\frac{1}{\\alpha-1} = \\frac{\\alpha}{\\alpha-1}.\n$$\nBy the method of moments, set $\\bar{X} = \\mathbb{E}[X]$ and solve for $\\alpha$:\n$$\n\\bar{X} = \\frac{\\alpha}{\\alpha-1} \\quad \\Longrightarrow \\quad \\bar{X}(\\alpha-1) = \\alpha \\quad \\Longrightarrow \\quad \\alpha \\bar{X} - \\bar{X} = \\alpha,\n$$\n$$\n\\alpha(\\bar{X}-1) = \\bar{X} \\quad \\Longrightarrow \\quad \\alpha = \\frac{\\bar{X}}{\\bar{X}-1}.\n$$\nSince each $X_{i}1$, we have $\\bar{X}1$, ensuring the estimator is well-defined.", "answer": "$$\\boxed{\\frac{\\bar{X}}{\\bar{X}-1}}$$", "id": "1948410"}, {"introduction": "Once an estimator has been derived, a crucial follow-up question is whether it is a \"good\" estimator. One of the most fundamental properties we look for is unbiasedness, which means that the estimator's expected value is equal to the true parameter value. This practice [@problem_id:1948412] builds on the previous exercise by not only asking you to find the Method of Moments Estimator (MOME) for a parameter $\\theta$, but also to formally check if it is unbiased, thus introducing a key concept in evaluating estimator performance.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ from a distribution with a probability density function (PDF) given by\n$$\nf(x; \\theta) = \\begin{cases} \\exp(-(x-\\theta))  \\text{if } x  \\theta \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nwhere $\\theta$ is an unknown real parameter. Let $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ denote the sample mean.\n\nYour task is to find the Method of Moments Estimator (MOME) for the parameter $\\theta$ and to determine if this estimator is unbiased.\n\nWhich of the following statements is correct?\n\nA. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X} - 1$, and it is an unbiased estimator.\n\nB. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X} - 1$, and it is a biased estimator.\n\nC. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X}$, and it is an unbiased estimator.\n\nD. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X}$, and it is a biased estimator.\n\nE. The MOME for $\\theta$ is $\\hat{\\theta} = \\bar{X} + 1$, and it is a biased estimator.", "solution": "The given density is $f(x;\\theta)=\\exp(-(x-\\theta))$ for $x\\theta$ and $0$ otherwise. This is a location-shifted exponential distribution with rate $1$, so $Y=X-\\theta$ follows $\\operatorname{Exp}(1)$.\n\nCompute the population mean:\n$$\n\\mathbb{E}[X]=\\int_{\\theta}^{\\infty} x\\,\\exp\\bigl(-(x-\\theta)\\bigr)\\,dx.\n$$\nUse the substitution $y=x-\\theta$, so $x=y+\\theta$ and $dx=dy$, with limits $y\\in[0,\\infty)$:\n$$\n\\mathbb{E}[X]=\\int_{0}^{\\infty} (y+\\theta)\\,\\exp(-y)\\,dy=\\int_{0}^{\\infty} y\\,\\exp(-y)\\,dy+\\theta\\int_{0}^{\\infty}\\exp(-y)\\,dy=1+\\theta.\n$$\n\nBy the method of moments, equate the sample mean to the population mean:\n$$\n\\bar{X}=\\mathbb{E}[X]=\\theta+1 \\quad \\Rightarrow \\quad \\hat{\\theta}_{\\mathrm{MOM}}=\\bar{X}-1.\n$$\n\nTo assess unbiasedness, use linearity of expectation and $\\mathbb{E}[\\bar{X}]=\\mathbb{E}[X]=\\theta+1$:\n$$\n\\mathbb{E}[\\hat{\\theta}_{\\mathrm{MOM}}]=\\mathbb{E}[\\bar{X}-1]=\\mathbb{E}[\\bar{X}]-1=(\\theta+1)-1=\\theta.\n$$\nThus, $\\hat{\\theta}_{\\mathrm{MOM}}=\\bar{X}-1$ is unbiased.\n\nTherefore, the correct statement is that the MOME is $\\hat{\\theta}=\\bar{X}-1$ and it is an unbiased estimator.", "answer": "$$\\boxed{A}$$", "id": "1948412"}, {"introduction": "Just because an estimator is unbiased does not mean it is the best one possible. To compare different unbiased estimators, we often look at their variances, with a smaller variance indicating a more efficient and precise estimator. This problem [@problem_id:1948421] guides you through a comparison of the unbiased MOME for the parameter $\\theta$ of a uniform distribution against another unbiased estimator derived from the sample maximum, highlighting that the method of moments, while simple, does not always yield the most efficient result.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n  1$ drawn from a Uniform distribution on the interval $(0, \\theta)$, where $\\theta  0$ is an unknown parameter. Two different unbiased estimators for $\\theta$ are to be compared for their efficiency.\n\nThe first estimator, $\\tilde{\\theta}_1$, is the unbiased estimator for $\\theta$ derived using the Method of Moments Estimator (MOME).\n\nThe second estimator, $\\tilde{\\theta}_2$, is the unbiased estimator for $\\theta$ obtained by multiplying the sample maximum, $X_{(n)} = \\max(X_1, X_2, \\dots, X_n)$, by an appropriate constant.\n\nDetermine the ratio of the variance of the first estimator to the variance of the second estimator, $\\frac{\\text{Var}(\\tilde{\\theta}_1)}{\\text{Var}(\\tilde{\\theta}_2)}$. Express your final answer as a function of the sample size $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. from $U(0,\\theta)$ with $\\theta0$. For this distribution,\n$$\n\\mathbb{E}[X_{i}]=\\frac{\\theta}{2},\\qquad \\operatorname{Var}(X_{i})=\\frac{\\theta^{2}}{12}.\n$$\nThe Method of Moments estimator equates $\\bar{X}$ to $\\mathbb{E}[X_{i}]$, giving $\\tilde{\\theta}_{1}=2\\bar{X}$. Its unbiasedness follows from $\\mathbb{E}[\\bar{X}]=\\theta/2$, so $\\mathbb{E}[\\tilde{\\theta}_{1}]=\\theta$. Its variance is\n$$\n\\operatorname{Var}(\\tilde{\\theta}_{1})=\\operatorname{Var}(2\\bar{X})=4\\operatorname{Var}(\\bar{X})=4\\frac{\\operatorname{Var}(X_{i})}{n}=4\\frac{\\theta^{2}}{12n}=\\frac{\\theta^{2}}{3n}.\n$$\n\nLet $X_{(n)}=\\max(X_{1},\\dots,X_{n})$. Its cumulative distribution function is $F_{X_{(n)}}(x)=(x/\\theta)^{n}$ for $0x\\theta$, and density $f_{X_{(n)}}(x)=n x^{n-1}/\\theta^{n}$. Then\n$$\n\\mathbb{E}[X_{(n)}]=\\int_{0}^{\\theta} x \\frac{n x^{n-1}}{\\theta^{n}}\\,dx=\\frac{n}{\\theta^{n}}\\cdot\\frac{\\theta^{n+1}}{n+1}=\\frac{n\\theta}{n+1}.\n$$\nTherefore the unbiased multiple of $X_{(n)}$ is $\\tilde{\\theta}_{2}=cX_{(n)}$ with $c=(n+1)/n$. Next compute\n$$\n\\mathbb{E}[X_{(n)}^{2}]=\\int_{0}^{\\theta} x^{2} \\frac{n x^{n-1}}{\\theta^{n}}\\,dx=\\frac{n}{\\theta^{n}}\\cdot\\frac{\\theta^{n+2}}{n+2}=\\frac{n\\theta^{2}}{n+2},\n$$\nso\n$$\n\\operatorname{Var}(X_{(n)})=\\mathbb{E}[X_{(n)}^{2}]-\\mathbb{E}[X_{(n)}]^{2}\n=\\theta^{2}\\left(\\frac{n}{n+2}-\\frac{n^{2}}{(n+1)^{2}}\\right)\n=\\theta^{2}\\frac{n}{(n+1)^{2}(n+2)}.\n$$\nThus\n$$\n\\operatorname{Var}(\\tilde{\\theta}_{2})=c^{2}\\operatorname{Var}(X_{(n)})=\\frac{(n+1)^{2}}{n^{2}}\\cdot \\theta^{2}\\frac{n}{(n+1)^{2}(n+2)}=\\frac{\\theta^{2}}{n(n+2)}.\n$$\n\nThe desired ratio is\n$$\n\\frac{\\operatorname{Var}(\\tilde{\\theta}_{1})}{\\operatorname{Var}(\\tilde{\\theta}_{2})}\n=\\frac{\\theta^{2}/(3n)}{\\theta^{2}/(n(n+2))}=\\frac{n+2}{3}.\n$$", "answer": "$$\\boxed{\\frac{n+2}{3}}$$", "id": "1948421"}]}