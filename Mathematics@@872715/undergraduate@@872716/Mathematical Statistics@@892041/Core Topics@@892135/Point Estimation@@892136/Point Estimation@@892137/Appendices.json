{"hands_on_practices": [{"introduction": "The Method of Moments is one of the oldest and most intuitive techniques for constructing estimators. It works by equating theoretical population moments, which are functions of the unknown parameters, with their corresponding sample moments calculated from the data. This practice provides a straightforward application of this principle to derive an estimator for a parameter in a given probability distribution. [@problem_id:1944335]", "problem": "In a materials science experiment, the normalized fracture toughness, $X$, of a newly developed ceramic composite is found to follow a specific probability distribution. The probability density function (PDF) for a single measurement $X$ is given by\n$$f(x;\\theta) = (\\theta+1)x^{\\theta}$$\nfor $x \\in (0,1)$, where $\\theta > -1$ is an unknown parameter related to the material's microstructure.\n\nTo estimate this parameter, a set of $n$ independent measurements, $X_1, X_2, \\dots, X_n$, is collected from a random sample of the composite. An estimator for $\\theta$, let's call it $\\hat{\\theta}$, is constructed by equating the theoretical mean of the distribution, $E[X]$, to the sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n\nBased on this procedure, derive the expression for the estimator $\\hat{\\theta}$ in terms of the sample mean $\\bar{X}$.", "solution": "We are given a random variable $X$ with probability density function $f(x;\\theta)=(\\theta+1)x^{\\theta}$ for $x\\in(0,1)$ and $\\theta>-1$. The method of moments estimator equates the theoretical mean $E[X]$ to the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$.\n\nFirst, compute the mean:\n$$\nE[X]=\\int_{0}^{1}x f(x;\\theta)\\,dx=\\int_{0}^{1}x(\\theta+1)x^{\\theta}\\,dx=(\\theta+1)\\int_{0}^{1}x^{\\theta+1}\\,dx.\n$$\nSince $\\theta>-1$, the integral converges and\n$$\n\\int_{0}^{1}x^{\\theta+1}\\,dx=\\left[\\frac{x^{\\theta+2}}{\\theta+2}\\right]_{0}^{1}=\\frac{1}{\\theta+2},\n$$\nhence\n$$\nE[X]=\\frac{\\theta+1}{\\theta+2}.\n$$\n\nSet $E[X]=\\bar{X}$ and solve for $\\theta$:\n$$\n\\bar{X}=\\frac{\\theta+1}{\\theta+2}\\;\\Longrightarrow\\;\\bar{X}(\\theta+2)=\\theta+1,\n$$\n$$\n\\bar{X}\\theta+2\\bar{X}=\\theta+1,\n$$\n$$\n\\bar{X}\\theta-\\theta=1-2\\bar{X},\n$$\n$$\n\\theta(\\bar{X}-1)=1-2\\bar{X},\n$$\n$$\n\\theta=\\frac{1-2\\bar{X}}{\\bar{X}-1}=\\frac{2\\bar{X}-1}{1-\\bar{X}}.\n$$\n\nTherefore, the method of moments estimator is obtained by replacing $\\bar{X}$ with the sample mean:\n$$\n\\hat{\\theta}=\\frac{2\\bar{X}-1}{1-\\bar{X}}.\n$$", "answer": "$$\\boxed{\\frac{2\\bar{X}-1}{1-\\bar{X}}}$$", "id": "1944335"}, {"introduction": "Maximum Likelihood Estimation (MLE) is a cornerstone of modern statistics, offering a powerful and versatile method for parameter estimation. The core idea is to find the parameter value that maximizes the probability, or likelihood, of observing the collected sample. In this exercise, you will apply the standard calculus-based approach to find the MLE for the parameter of a Pareto distribution, a model often used in economics. [@problem_id:1944340]", "problem": "In development economics, the distribution of high-end incomes in many countries is often modeled using the Pareto distribution. An economist is studying the income distribution for individuals earning above a certain minimum threshold. The data is modeled by a random variable $X$ following a Pareto distribution with the probability density function (PDF) given by:\n$$f(x; \\alpha, c) = \\alpha c^{\\alpha} x^{-(\\alpha+1)}$$\nfor $x \\ge c$, and $f(x; \\alpha, c) = 0$ for $x < c$.\n\nIn this model, $c$ is a known positive constant representing the minimum income threshold, and $\\alpha > 0$ is the unknown Pareto index, which measures income inequality. A random sample of $n$ incomes, denoted by $X_1, X_2, \\dots, X_n$, is collected from this population.\n\nYour task is to find the Maximum Likelihood Estimator (MLE) for the parameter $\\alpha$. Express your answer as a symbolic expression in terms of $n$, $c$, and the sample observations $X_i$.", "solution": "We have an independent and identically distributed sample $X_{1},\\dots,X_{n}$ from the Pareto distribution with known threshold $c>0$ and unknown shape parameter $\\alpha>0$, with density\n$$\nf(x;\\alpha,c)=\\alpha c^{\\alpha}x^{-(\\alpha+1)}\\quad\\text{for }x\\ge c,\\quad f(x;\\alpha,c)=0\\text{ for }x<c.\n$$\nThe likelihood function for $\\alpha$ given the sample is\n$$\nL(\\alpha; x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\alpha c^{\\alpha}X_{i}^{-(\\alpha+1)}\\cdot \\mathbf{1}\\{X_{i}\\ge c\\}\n=\\alpha^{n}c^{n\\alpha}\\prod_{i=1}^{n}X_{i}^{-(\\alpha+1)}\\cdot \\mathbf{1}\\{X_{(1)}\\ge c\\},\n$$\nwhere $X_{(1)}=\\min_{1\\le i\\le n}X_{i}$. For samples satisfying $X_{(1)}\\ge c$, the log-likelihood is\n$$\n\\ell(\\alpha)=\\ln L(\\alpha)=n\\ln \\alpha+n\\alpha\\ln c-(\\alpha+1)\\sum_{i=1}^{n}\\ln X_{i}.\n$$\nDifferentiate with respect to $\\alpha$ and set to zero to obtain the first-order condition:\n$$\n\\frac{\\partial \\ell}{\\partial \\alpha}=\\frac{n}{\\alpha}+n\\ln c-\\sum_{i=1}^{n}\\ln X_{i}=0,\n$$\nwhich yields\n$$\n\\frac{n}{\\alpha}=\\sum_{i=1}^{n}\\ln X_{i}-n\\ln c=\\sum_{i=1}^{n}\\ln\\!\\left(\\frac{X_{i}}{c}\\right).\n$$\nSolving for $\\alpha$ gives the MLE\n$$\n\\hat{\\alpha}=\\frac{n}{\\sum_{i=1}^{n}\\ln\\!\\left(\\frac{X_{i}}{c}\\right)}.\n$$\nTo verify it is a maximizer, compute the second derivative:\n$$\n\\frac{\\partial^{2}\\ell}{\\partial \\alpha^{2}}=-\\frac{n}{\\alpha^{2}}<0\\quad\\text{for }\\alpha>0,\n$$\nso the critical point is indeed a global maximum on the feasible set. Note that if all $X_{i}=c$, then $\\sum_{i=1}^{n}\\ln(X_{i}/c)=0$ and the likelihood is increasing in $\\alpha$, corresponding to the MLE at $+\\infty$; for continuous data this occurs with probability zero. Otherwise, the expression above gives the MLE.", "answer": "$$\\boxed{\\frac{n}{\\sum_{i=1}^{n}\\ln\\!\\left(\\frac{X_{i}}{c}\\right)}}$$", "id": "1944340"}, {"introduction": "Once we have different potential estimators for a parameter, how do we decide which one is 'better'? The Mean Squared Error (MSE) provides a robust criterion for this comparison, as it captures both an estimator's bias and its variance. This hands-on problem asks you to calculate and compare the MSE for two different estimators of a population mean, illustrating how we can quantitatively evaluate their performance. [@problem_id:1944364]", "problem": "Two students, Alice and Bob, are tasked with estimating an unknown physical constant, denoted by $\\mu$. They use a measurement device that is known to produce readings that are normally distributed with mean $\\mu$ and a variance of 1. That is, a single measurement $X$ is a random variable with distribution $N(\\mu, 1)$. They decide to take two independent measurements, $X_1$ and $X_2$.\n\nAlice proposes an estimator for $\\mu$ which is the sample mean:\n$$ \\hat{\\mu}_A = \\frac{1}{2}X_1 + \\frac{1}{2}X_2 $$\n\nBob suggests a different weighted average, arguing that the second measurement might be more \"settled\":\n$$ \\hat{\\mu}_B = \\frac{1}{3}X_1 + \\frac{2}{3}X_2 $$\n\nTo compare the quality of these two estimators, they decide to use the Mean Squared Error (MSE), which for an estimator $\\hat{\\theta}$ of a parameter $\\theta$ is defined as $MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$.\n\nCalculate the ratio of the MSE of Bob's estimator to the MSE of Alice's estimator, $\\frac{MSE(\\hat{\\mu}_B)}{MSE(\\hat{\\mu}_A)}$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent with $X_{i} \\sim N(\\mu,1)$. For any estimator $\\hat{\\theta}$ of $\\theta$, the Mean Squared Error is\n$$\nMSE(\\hat{\\theta})=E\\!\\left[(\\hat{\\theta}-\\theta)^{2}\\right]=\\operatorname{Var}(\\hat{\\theta})+\\left(\\operatorname{Bias}(\\hat{\\theta})\\right)^{2}.\n$$\nCompute the bias for each estimator using linearity of expectation. For Alice,\n$$\nE[\\hat{\\mu}_{A}]=E\\!\\left[\\tfrac{1}{2}X_{1}+\\tfrac{1}{2}X_{2}\\right]=\\tfrac{1}{2}E[X_{1}]+\\tfrac{1}{2}E[X_{2}]=\\tfrac{1}{2}\\mu+\\tfrac{1}{2}\\mu=\\mu,\n$$\nso $\\operatorname{Bias}(\\hat{\\mu}_{A})=0$. For Bob,\n$$\nE[\\hat{\\mu}_{B}]=E\\!\\left[\\tfrac{1}{3}X_{1}+\\tfrac{2}{3}X_{2}\\right]=\\tfrac{1}{3}\\mu+\\tfrac{2}{3}\\mu=\\mu,\n$$\nso $\\operatorname{Bias}(\\hat{\\mu}_{B})=0$. Therefore, for both estimators $MSE=\\operatorname{Var}$.\n\nUse independence to compute the variance of a linear combination: for independent $X_{1},X_{2}$,\n$$\n\\operatorname{Var}(aX_{1}+bX_{2})=a^{2}\\operatorname{Var}(X_{1})+b^{2}\\operatorname{Var}(X_{2}),\n$$\nsince $\\operatorname{Cov}(X_{1},X_{2})=0$. With $\\operatorname{Var}(X_{i})=1$, we get\n$$\n\\operatorname{Var}(\\hat{\\mu}_{A})=\\left(\\tfrac{1}{2}\\right)^{2}\\cdot 1+\\left(\\tfrac{1}{2}\\right)^{2}\\cdot 1=\\tfrac{1}{4}+\\tfrac{1}{4}=\\tfrac{1}{2},\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{B})=\\left(\\tfrac{1}{3}\\right)^{2}\\cdot 1+\\left(\\tfrac{2}{3}\\right)^{2}\\cdot 1=\\tfrac{1}{9}+\\tfrac{4}{9}=\\tfrac{5}{9}.\n$$\nHence,\n$$\n\\frac{MSE(\\hat{\\mu}_{B})}{MSE(\\hat{\\mu}_{A})}=\\frac{\\operatorname{Var}(\\hat{\\mu}_{B})}{\\operatorname{Var}(\\hat{\\mu}_{A})}=\\frac{\\tfrac{5}{9}}{\\tfrac{1}{2}}=\\frac{10}{9}.\n$$", "answer": "$$\\boxed{\\frac{10}{9}}$$", "id": "1944364"}]}