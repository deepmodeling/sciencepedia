{"hands_on_practices": [{"introduction": "Choosing the right statistical tool is crucial, and the most familiar tool is not always the best. This exercise [@problem_id:1951462] explores this idea by comparing two different estimators for the maximum lifetime of a component assumed to follow a uniform distribution. We will contrast an estimator based on the familiar sample mean with one based on the sample maximum, demonstrating how relative efficiency gives us a clear, quantitative criterion to determine which estimator is superior for this specific task.", "problem": "A manufacturer is testing a new type of solid-state battery. The lifetime of a battery, denoted by the random variable $X$, is assumed to follow a uniform distribution on the interval $[0, \\theta]$, where $\\theta$ is the unknown maximum possible lifetime. To estimate $\\theta$, a random sample of $n$ batteries, $X_1, X_2, \\dots, X_n$, is tested until failure.\n\nTwo different estimators for $\\theta$ are proposed:\n1. The first estimator, $T_1$, is defined as twice the sample mean: $T_1 = 2\\bar{X}$, where $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$.\n2. The second estimator, $T_2$, is constructed by scaling the sample maximum, $X_{(n)} = \\max(X_1, X_2, \\dots, X_n)$, in such a way that $T_2$ is an unbiased estimator for $\\theta$.\n\nCalculate the relative efficiency of $T_1$ with respect to $T_2$. Express your answer as a function of the sample size $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. from the $\\mathrm{Uniform}(0,\\theta)$ distribution. Then the mean and variance of a single observation are\n$$\n\\mathbb{E}[X_{i}]=\\frac{\\theta}{2},\\qquad \\mathrm{Var}(X_{i})=\\frac{\\theta^{2}}{12}.\n$$\nFor the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, by linearity of expectation and independence,\n$$\n\\mathbb{E}[\\bar{X}]=\\frac{\\theta}{2},\\qquad \\mathrm{Var}(\\bar{X})=\\frac{1}{n}\\mathrm{Var}(X_{i})=\\frac{\\theta^{2}}{12n}.\n$$\nThe first estimator is $T_{1}=2\\bar{X}$, so\n$$\n\\mathbb{E}[T_{1}]=2\\,\\mathbb{E}[\\bar{X}]=\\theta,\\qquad \\mathrm{Var}(T_{1})=4\\,\\mathrm{Var}(\\bar{X})=\\frac{\\theta^{2}}{3n}.\n$$\n\nLet $X_{(n)}=\\max(X_{1},\\dots,X_{n})$ denote the sample maximum. For $X_{i}\\sim \\mathrm{Uniform}(0,\\theta)$, the scaled maximum $X_{(n)}/\\theta$ has the $\\mathrm{Beta}(n,1)$ distribution. Hence\n$$\n\\mathbb{E}[X_{(n)}]=\\theta\\,\\frac{n}{n+1},\\qquad \\mathrm{Var}\\!\\left(X_{(n)}\\right)=\\theta^{2}\\,\\frac{n}{(n+1)^{2}(n+2)}.\n$$\nTo make $T_{2}$ unbiased, define $T_{2}=c\\,X_{(n)}$ with $c$ chosen so that $\\mathbb{E}[T_{2}]=\\theta$. Using $\\mathbb{E}[X_{(n)}]=\\theta\\,\\frac{n}{n+1}$,\n$$\nc\\,\\theta\\,\\frac{n}{n+1}=\\theta\\;\\;\\Rightarrow\\;\\; c=\\frac{n+1}{n},\n$$\nso\n$$\nT_{2}=\\frac{n+1}{n}\\,X_{(n)},\\qquad \\mathrm{Var}(T_{2})=\\left(\\frac{n+1}{n}\\right)^{2}\\mathrm{Var}\\!\\left(X_{(n)}\\right)=\\left(\\frac{n+1}{n}\\right)^{2}\\theta^{2}\\frac{n}{(n+1)^{2}(n+2)}=\\frac{\\theta^{2}}{n(n+2)}.\n$$\n\nFor two unbiased estimators, the relative efficiency of $T_{1}$ with respect to $T_{2}$ is defined as\n$$\n\\mathrm{RE}(T_{1}\\text{ w.r.t. }T_{2})=\\frac{\\mathrm{Var}(T_{2})}{\\mathrm{Var}(T_{1})}.\n$$\nSubstituting the variances derived above gives\n$$\n\\mathrm{RE}(T_{1}\\text{ w.r.t. }T_{2})=\\frac{\\theta^{2}/\\bigl(n(n+2)\\bigr)}{\\theta^{2}/(3n)}=\\frac{3}{n+2}.\n$$\nThis expresses the relative efficiency as a function of $n$.", "answer": "$$\\boxed{\\frac{3}{n+2}}$$", "id": "1951462"}, {"introduction": "Often in practice, we have access to multiple sources of information, such as measurements from two different instruments. Instead of choosing one over the other, we can often achieve better results by combining them. This practice [@problem_id:1951437] delves into the powerful technique of creating an optimal linear combination of two estimators, taking into account not just their individual variances but also their correlation. You will learn how to find the ideal weights to minimize the variance of the combined estimator, thereby maximizing its efficiency.", "problem": "A materials scientist is developing a new class of semiconductor wafers and needs to precisely estimate the average impurity concentration, $\\theta$. Two different measurement systems are available. System A produces an unbiased estimate $T_1$ of $\\theta$, and System B produces another unbiased estimate $T_2$ of $\\theta$. From historical data, the variances of these estimators are known to be $\\text{Var}(T_1) = \\sigma_1^2$ and $\\text{Var}(T_2) = \\sigma_2^2$. Due to shared environmental factors affecting both systems, the estimates are not independent, and their correlation coefficient is $\\text{Corr}(T_1, T_2) = \\rho$, where $|\\rho|  1$.\n\nTo improve the estimation, a combined linear estimator is proposed: $T_C = w T_1 + (1-w) T_2$, where $w$ is a real-valued weight.\n\nYour task is to find the optimal configuration for this combined estimator. Determine the expression for the optimal weight, $w_{opt}$, that minimizes the variance of $T_C$. Then, find the expression for the relative efficiency of this optimal combined estimator, $T_{C,opt}$, with respect to the estimator $T_1$. The relative efficiency is defined as the ratio $\\text{Eff}(T_{C,opt}, T_1) = \\frac{\\text{Var}(T_1)}{\\text{Var}(T_{C,opt})}$.\n\nPresent your final answer as a row matrix containing two elements: the expression for $w_{opt}$ as the first element and the expression for the relative efficiency as the second element.", "solution": "The combined estimator is $T_{C}=w T_{1}+(1-w) T_{2}$. Since $T_{1}$ and $T_{2}$ are unbiased for $\\theta$, and the weights sum to one, $T_{C}$ is unbiased: $\\mathbb{E}[T_{C}]=w \\theta+(1-w)\\theta=\\theta$.\n\nThe variance of $T_{C}$ uses $\\text{Cov}(T_{1},T_{2})=\\rho \\sigma_{1}\\sigma_{2}$:\n$$\n\\text{Var}(T_{C})=\\text{Var}\\big(w T_{1}+(1-w) T_{2}\\big)\n=w^{2}\\sigma_{1}^{2}+(1-w)^{2}\\sigma_{2}^{2}+2 w(1-w)\\rho \\sigma_{1}\\sigma_{2}.\n$$\nDefine $V(w)=w^{2}\\sigma_{1}^{2}+(1-w)^{2}\\sigma_{2}^{2}+2 w(1-w)\\rho \\sigma_{1}\\sigma_{2}$. Differentiate and set to zero:\n$$\n\\frac{dV}{dw}=2 w \\sigma_{1}^{2}-2(1-w)\\sigma_{2}^{2}+2(1-2w)\\rho \\sigma_{1}\\sigma_{2}=0.\n$$\nThis simplifies to\n$$\nw\\big(\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}\\big)+\\big(\\rho \\sigma_{1}\\sigma_{2}-\\sigma_{2}^{2}\\big)=0,\n$$\nso the optimal weight is\n$$\nw_{\\text{opt}}=\\frac{\\sigma_{2}^{2}-\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}V}{dw^{2}}=2\\big(\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}\\big)=2\\,\\text{Var}(T_{1}-T_{2})0,\n$$\nsince $|\\rho|1$, ensuring a minimum.\n\nTo find the minimized variance, write $V(w)=a w^{2}+2 b w+c$ with\n$$\na=\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2},\\quad b=-(\\sigma_{2}^{2}-\\rho \\sigma_{1}\\sigma_{2}),\\quad c=\\sigma_{2}^{2}.\n$$\nThen\n$$\n\\text{Var}(T_{C,\\text{opt}})=V(w_{\\text{opt}})=c-\\frac{b^{2}}{a}\n=\\frac{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^{2})}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}.\n$$\nThe relative efficiency of $T_{C,\\text{opt}}$ with respect to $T_{1}$ is\n$$\n\\text{Eff}(T_{C,\\text{opt}},T_{1})=\\frac{\\text{Var}(T_{1})}{\\text{Var}(T_{C,\\text{opt}})}\n=\\frac{\\sigma_{1}^{2}}{\\dfrac{\\sigma_{1}^{2}\\sigma_{2}^{2}(1-\\rho^{2})}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}}\n=\\frac{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{2}^{2}(1-\\rho^{2})}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\dfrac{\\sigma_{2}^{2}-\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}  \\dfrac{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2\\rho \\sigma_{1}\\sigma_{2}}{\\sigma_{2}^{2}(1-\\rho^{2})}\\end{pmatrix}}$$", "id": "1951437"}, {"introduction": "Statistical efficiency is deeply connected to the concept of information. In complex models, we often care about one parameter while having to estimate others, known as nuisance parameters. This exercise [@problem_id:1951476] advances our understanding by using the Fisher Information Matrix to explore the cost of this uncertainty. By comparing the asymptotic efficiency of an estimator for a location parameter when a scale parameter is known versus unknown, you will quantify the precise \"information loss\" and gain insight into the fundamental limits of estimation.", "problem": "In statistical modeling, the efficiency of an estimator can be significantly affected by the presence of nuisance parameters. Consider a random variable $Y$ that follows a Gumbel distribution, which is an asymmetric location-scale family of distributions. The probability density function is parameterized by a location parameter $\\mu$ and a positive scale parameter $\\sigma$.\n\nFor a single observation $Y$, the Fisher Information Matrix (FIM) for the parameter vector $\\boldsymbol{\\theta} = (\\mu, \\sigma)^T$ is given by\n$$\nI(\\mu, \\sigma) = \\frac{1}{\\sigma^2}\n\\begin{pmatrix}\n1  1-\\gamma \\\\\n1-\\gamma  \\frac{\\pi^2}{6} + (1-\\gamma)^2\n\\end{pmatrix}\n$$\nwhere $\\gamma$ is the Euler-Mascheroni constant ($\\gamma \\approx 0.5772$). The off-diagonal terms are non-zero due to the asymmetry of the Gumbel distribution.\n\nWe are interested in estimating the location parameter $\\mu$. Compare the following two scenarios based on a large sample of $n$ independent and identically distributed observations $Y_1, \\dots, Y_n$:\n1.  **Known Scale:** The scale parameter $\\sigma$ is known to be some value $\\sigma_0$, and we only need to estimate $\\mu$.\n2.  **Unknown Scale:** Both $\\mu$ and $\\sigma$ are unknown and must be estimated simultaneously.\n\nThe performance of an estimator is measured by its asymptotic variance. The Asymptotic Relative Efficiency (ARE) of an estimator $\\hat{\\theta}_A$ with respect to another estimator $\\hat{\\theta}_B$ is defined as the ratio of their asymptotic variances, $\\text{ARE}(\\hat{\\theta}_A, \\hat{\\theta}_B) = \\frac{\\text{Asymptotic Var}(\\hat{\\theta}_B)}{\\text{Asymptotic Var}(\\hat{\\theta}_A)}$.\n\nCalculate the ARE of the Maximum Likelihood Estimator (MLE) for $\\mu$ in the \"Unknown Scale\" scenario with respect to the MLE for $\\mu$ in the \"Known Scale\" scenario. Express your answer as a closed-form analytic expression in terms of $\\pi$ and $\\gamma$.", "solution": "For a single observation, the Fisher Information Matrix (FIM) for $\\boldsymbol{\\theta} = (\\mu,\\sigma)^{T}$ is\n$$\nI_{1}(\\mu,\\sigma) = \\frac{1}{\\sigma^{2}}\n\\begin{pmatrix}\n1  1-\\gamma \\\\\n1-\\gamma  \\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}\n\\end{pmatrix},\n$$\nand for $n$ i.i.d. observations it is $I_{n}(\\mu,\\sigma) = n I_{1}(\\mu,\\sigma)$.\n\nAsymptotic normality of the MLE implies that, for large $n$, the asymptotic covariance matrix of the joint MLE is $(I_{n})^{-1} = \\frac{1}{n} I_{1}^{-1}$, so the asymptotic variance of $\\hat{\\mu}$ equals the $(1,1)$ entry of $\\frac{1}{n} I_{1}^{-1}$.\n\nKnown scale $\\sigma$:\nWhen $\\sigma$ is known, the relevant Fisher information for $\\mu$ is the scalar $I_{1,\\mu\\mu} = \\sigma^{-2}$. Therefore, for $n$ observations, the asymptotic variance of the MLE $\\hat{\\mu}$ is\n$$\n\\operatorname{Avar}_{\\text{known}}(\\hat{\\mu}) = \\frac{1}{n I_{1,\\mu\\mu}} = \\frac{\\sigma^{2}}{n}.\n$$\n\nUnknown scale $\\sigma$:\nWhen both $\\mu$ and $\\sigma$ are unknown, write the per-observation FIM as\n$$\nI_{1} = \\frac{1}{\\sigma^{2}} M,\\quad M =\n\\begin{pmatrix}\n1  a \\\\\na  b\n\\end{pmatrix},\\quad a = 1-\\gamma,\\quad b = \\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}.\n$$\nThen $I_{1}^{-1} = \\sigma^{2} M^{-1}$. For a $2\\times 2$ matrix, $M^{-1} = \\frac{1}{\\det(M)}\n\\begin{pmatrix}\nb  -a \\\\\n-a  1\n\\end{pmatrix}$, so the $(1,1)$ entry is $(M^{-1})_{11} = b/\\det(M)$. The determinant is\n$$\n\\det(M) = 1\\cdot b - a^{2} = \\left(\\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}\\right) - (1-\\gamma)^{2} = \\frac{\\pi^{2}}{6}.\n$$\nTherefore,\n$$\n(I_{1}^{-1})_{11} = \\sigma^{2} \\frac{b}{\\det(M)} = \\sigma^{2} \\frac{\\frac{\\pi^{2}}{6} + (1-\\gamma)^{2}}{\\frac{\\pi^{2}}{6}}\n= \\sigma^{2}\\left(1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}\\right).\n$$\nThus, for $n$ observations,\n$$\n\\operatorname{Avar}_{\\text{unknown}}(\\hat{\\mu}) = \\frac{1}{n} (I_{1}^{-1})_{11}\n= \\frac{\\sigma^{2}}{n}\\left(1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}\\right).\n$$\n\nAsymptotic Relative Efficiency:\nBy definition, for estimator $A$ (unknown scale) relative to estimator $B$ (known scale),\n$$\n\\text{ARE}(\\hat{\\mu}_{\\text{unknown}}, \\hat{\\mu}_{\\text{known}}) = \\frac{\\operatorname{Avar}_{\\text{known}}(\\hat{\\mu})}{\\operatorname{Avar}_{\\text{unknown}}(\\hat{\\mu})}\n= \\frac{\\sigma^{2}/n}{\\frac{\\sigma^{2}}{n}\\left(1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}\\right)}\n= \\frac{1}{1 + \\frac{6}{\\pi^{2}}(1-\\gamma)^{2}}\n= \\frac{\\pi^{2}}{\\pi^{2} + 6(1-\\gamma)^{2}}.\n$$\nThis gives the closed-form expression in terms of $\\pi$ and $\\gamma$.", "answer": "$$\\boxed{\\frac{\\pi^{2}}{\\pi^{2} + 6\\left(1-\\gamma\\right)^{2}}}$$", "id": "1951476"}]}