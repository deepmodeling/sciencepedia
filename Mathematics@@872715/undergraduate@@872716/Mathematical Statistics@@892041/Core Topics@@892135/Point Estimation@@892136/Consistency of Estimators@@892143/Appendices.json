{"hands_on_practices": [{"introduction": "A consistent estimator is one that gets arbitrarily close to the true parameter value as the sample size increases. This first exercise lets you test this core concept by examining several estimators that are simple modifications of the sample mean. By analyzing how small additive or multiplicative terms affect the estimator's long-run behavior, you will build a solid intuition for the formal definition of consistency [@problem_id:1909315].", "problem": "Let $X_1, X_2, \\dots, X_n$ be an independent and identically distributed (i.i.d.) random sample from a probability distribution with a finite mean $\\mu$ and a finite, positive variance $\\sigma^2  0$. Let $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean.\n\nAn estimator for a parameter is said to be consistent if it converges in probability to the true value of the parameter as the sample size $n$ approaches infinity.\n\nConsider the following four estimators for the population mean $\\mu$.\n\nA. $\\hat{\\mu}_A = \\bar{X}_n + \\frac{1}{\\sqrt{n}}$\n\nB. $\\hat{\\mu}_B = \\frac{n}{n+2} \\bar{X}_n$\n\nC. $\\hat{\\mu}_C = \\frac{1}{2}(\\bar{X}_n + X_1)$\n\nD. $\\hat{\\mu}_D = \\bar{X}_n + \\frac{\\cos(n\\pi)}{n}$\n\nWhich of the above estimators are consistent for $\\mu$? Select all that apply.", "solution": "By the weak law of large numbers, $\\bar{X}_{n} \\xrightarrow{p} \\mu$ since the $X_{i}$ are i.i.d. with finite mean $\\mu$ and finite positive variance $\\sigma^{2}0$. We analyze each estimator using this fact and Slutsky’s theorem.\n\nA. $\\hat{\\mu}_{A}=\\bar{X}_{n}+\\frac{1}{\\sqrt{n}}$. Write\n$$\n\\hat{\\mu}_{A}-\\mu=(\\bar{X}_{n}-\\mu)+\\frac{1}{\\sqrt{n}}.\n$$\nWe have $\\bar{X}_{n}-\\mu \\xrightarrow{p} 0$ and $\\frac{1}{\\sqrt{n}} \\to 0$ deterministically. By Slutsky’s theorem (sum of a sequence converging in probability to $0$ and a deterministic sequence converging to $0$), $\\hat{\\mu}_{A} \\xrightarrow{p} \\mu$. Hence $\\hat{\\mu}_{A}$ is consistent.\n\nB. $\\hat{\\mu}_{B}=\\frac{n}{n+2}\\bar{X}_{n}$. Note that $\\frac{n}{n+2} \\to 1$ deterministically. Then\n$$\n\\hat{\\mu}_{B}=\\frac{n}{n+2}\\bar{X}_{n} \\xrightarrow{p} 1 \\cdot \\mu=\\mu\n$$\nby Slutsky’s theorem (product of a sequence converging in probability and a deterministic sequence converging to a constant). More explicitly,\n$$\n\\hat{\\mu}_{B}-\\mu=\\frac{n}{n+2}(\\bar{X}_{n}-\\mu)+\\left(\\frac{n}{n+2}-1\\right)\\mu,\n$$\nwhere the first term converges in probability to $0$ and the second term converges deterministically to $0$. Thus $\\hat{\\mu}_{B}$ is consistent.\n\nC. $\\hat{\\mu}_{C}=\\frac{1}{2}(\\bar{X}_{n}+X_{1})$. Consider, for any $\\epsilon0$,\n$$\n\\left|\\hat{\\mu}_{C}-\\mu\\right|=\\left|\\frac{1}{2}(\\bar{X}_{n}-\\mu)+\\frac{1}{2}(X_{1}-\\mu)\\right|\\ge \\frac{1}{2}|X_{1}-\\mu|-\\frac{1}{2}|\\bar{X}_{n}-\\mu|.\n$$\nHence, on the event $\\{|X_{1}-\\mu|3\\epsilon,\\ |\\bar{X}_{n}-\\mu|\\epsilon\\}$, we have $\\left|\\hat{\\mu}_{C}-\\mu\\right|\\epsilon$. Therefore,\n$$\n\\mathbb{P}\\left(\\left|\\hat{\\mu}_{C}-\\mu\\right|\\epsilon\\right)\\ge \\mathbb{P}\\left(|X_{1}-\\mu|3\\epsilon,\\ |\\bar{X}_{n}-\\mu|\\epsilon\\right)\\ge \\mathbb{P}\\left(|X_{1}-\\mu|3\\epsilon\\right)-\\mathbb{P}\\left(|\\bar{X}_{n}-\\mu|\\ge \\epsilon\\right).\n$$\nBy the weak law, $\\mathbb{P}\\left(|\\bar{X}_{n}-\\mu|\\ge \\epsilon\\right)\\to 0$. Since $\\sigma^{2}0$, the distribution of $X_{1}$ is non-degenerate, so there exists some $\\epsilon0$ such that $\\mathbb{P}\\left(|X_{1}-\\mu|3\\epsilon\\right)0$. For such $\\epsilon$, the lower bound above stays bounded away from $0$, so $\\mathbb{P}\\left(\\left|\\hat{\\mu}_{C}-\\mu\\right|\\epsilon\\right)\\not\\to 0$. Thus $\\hat{\\mu}_{C}$ is not consistent.\n\nD. $\\hat{\\mu}_{D}=\\bar{X}_{n}+\\frac{\\cos(n\\pi)}{n}$. Note that $\\cos(n\\pi)=(-1)^{n}$, so $\\frac{\\cos(n\\pi)}{n}\\to 0$ deterministically. Then\n$$\n\\hat{\\mu}_{D}-\\mu=(\\bar{X}_{n}-\\mu)+\\frac{\\cos(n\\pi)}{n},\n$$\nwith the first term converging in probability to $0$ and the second term converging deterministically to $0$. By Slutsky’s theorem, $\\hat{\\mu}_{D} \\xrightarrow{p} \\mu$. Hence $\\hat{\\mu}_{D}$ is consistent.\n\nTherefore, the consistent estimators are A, B, and D.", "answer": "$$\\boxed{ABD}$$", "id": "1909315"}, {"introduction": "In statistical practice, many useful estimators, such as Maximum Likelihood Estimators (MLEs), are functions of sample statistics like the sample mean. This problem demonstrates a powerful and standard technique for proving their consistency: combining the Law of Large Numbers with the Continuous Mapping Theorem. Understanding this line of reasoning is essential for validating the properties of a wide range of estimators used across scientific disciplines [@problem_id:1909316].", "problem": "A materials scientist is studying the lifetime of a new type of fiber optic cable. The time to failure, $X$, for a segment of this cable is modeled by an Exponential distribution with a constant failure rate $\\lambda  0$. The probability density function (PDF) for the lifetime is given by $f(x; \\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$. To estimate the failure rate, the scientist tests a random sample of $n$ independent cable segments, measuring their lifetimes $X_1, X_2, \\ldots, X_n$.\n\nAn estimator for $\\lambda$ is proposed based on the sample mean lifetime, $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$. The proposed estimator is $\\hat{\\lambda}_n = \\frac{1}{\\bar{X}_n}$.\n\nThe scientist wants to establish that this is a good estimator in the sense that it becomes more accurate as the sample size $n$ increases. This property is known as consistency. An estimator $\\hat{\\theta}_n$ is said to be a consistent estimator for a parameter $\\theta$ if $\\hat{\\theta}_n$ converges in probability to $\\theta$ as $n \\to \\infty$.\n\nWhich of the following statements provides the correct reasoning to prove that $\\hat{\\lambda}_n = 1/\\bar{X}_n$ is a consistent estimator for $\\lambda$?\n\nA. The estimator is consistent because the Central Limit Theorem states that for large $n$, the sampling distribution of $\\hat{\\lambda}_n$ is approximately Normal with a mean of $\\lambda$.\n\nB. The estimator is consistent because the Law of Large Numbers states that the sample mean $\\bar{X}_n$ converges in probability to the true failure rate $\\lambda$, and therefore $\\hat{\\lambda}_n = 1/\\bar{X}_n$ must converge in probability to $1/\\lambda$.\n\nC. The estimator is consistent because it can be shown to be an unbiased estimator of $\\lambda$, meaning that its expected value $E[\\hat{\\lambda}_n]$ is exactly equal to $\\lambda$ for any sample size $n$.\n\nD. The estimator is consistent because its variance, $\\text{Var}(\\hat{\\lambda}_n)$, can be shown to approach zero as the sample size $n$ approaches infinity. While this fact is true, it is not, by itself, a complete justification for consistency.\n\nE. The estimator is consistent because the Law of Large Numbers ensures that the sample mean $\\bar{X}_n$ converges in probability to the true mean lifetime, which is $1/\\lambda$. Since the function $g(y) = 1/y$ is continuous for $y \\neq 0$, the Continuous Mapping Theorem then guarantees that $\\hat{\\lambda}_n = g(\\bar{X}_n)$ converges in probability to $g(1/\\lambda) = \\lambda$.", "solution": "To determine the correct justification for the consistency of the estimator $\\hat{\\lambda}_n = 1/\\bar{X}_n$, we must follow the formal definition of consistency and apply the relevant theorems from probability theory.\n\nAn estimator $\\hat{\\theta}_n$ is consistent for a parameter $\\theta$ if it converges in probability to $\\theta$ as the sample size $n$ approaches infinity. We write this as $\\hat{\\theta}_n \\xrightarrow{p} \\theta$. We need to show that $\\hat{\\lambda}_n \\xrightarrow{p} \\lambda$.\n\nFirst, let's identify the properties of the random sample $X_1, X_2, \\ldots, X_n$. The variables are independent and identically distributed (i.i.d.) from an Exponential distribution with rate $\\lambda$. The expected value (or mean) of an Exponentially distributed random variable with rate $\\lambda$ is $E[X] = 1/\\lambda$. The variance is $\\text{Var}(X) = 1/\\lambda^2$. Since the rate $\\lambda  0$, the mean $1/\\lambda$ is finite.\n\nThe core of the argument for consistency often involves the Law of Large Numbers. The Weak Law of Large Numbers (WLLN) states that for a sequence of i.i.d. random variables with a finite mean $\\mu$, the sample mean $\\bar{X}_n$ converges in probability to $\\mu$.\nIn our case, $\\mu = E[X] = 1/\\lambda$. Therefore, by the WLLN:\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow{p} E[X] = \\frac{1}{\\lambda} \\quad \\text{as } n \\to \\infty $$\n\nOur estimator is not $\\bar{X}_n$, but a function of it: $\\hat{\\lambda}_n = 1/\\bar{X}_n$. To handle the convergence of a function of a random variable, we use the Continuous Mapping Theorem (CMT). The CMT states that if a sequence of random variables $Y_n$ converges in probability to a constant $c$ (i.e., $Y_n \\xrightarrow{p} c$), and if $g$ is a function that is continuous at the point $c$, then the sequence of transformed random variables $g(Y_n)$ converges in probability to $g(c)$ (i.e., $g(Y_n) \\xrightarrow{p} g(c)$).\n\nIn this problem, our sequence of random variables is $Y_n = \\bar{X}_n$, and it converges in probability to the constant $c = 1/\\lambda$. Our function is $g(y) = 1/y$. This function is continuous for all $y \\neq 0$. Since $\\lambda  0$, the constant $c = 1/\\lambda$ is also greater than 0, so the function $g(y)$ is continuous at $c=1/\\lambda$.\n\nApplying the Continuous Mapping Theorem:\n$$ \\hat{\\lambda}_n = g(\\bar{X}_n) \\xrightarrow{p} g\\left(\\frac{1}{\\lambda}\\right) $$\nNow, we evaluate $g(1/\\lambda)$:\n$$ g\\left(\\frac{1}{\\lambda}\\right) = \\frac{1}{(1/\\lambda)} = \\lambda $$\nThus, we have shown that $\\hat{\\lambda}_n \\xrightarrow{p} \\lambda$, which is the definition of consistency.\n\nThis line of reasoning perfectly matches option E.\n\nLet's evaluate why the other options are incorrect:\n- **A:** The Central Limit Theorem describes convergence in *distribution* (to a Normal distribution), not convergence in *probability* (to a constant). Consistency is defined by convergence in probability. This reasoning is flawed.\n- **B:** This statement incorrectly claims that $\\bar{X}_n$ converges to $\\lambda$. The WLLN states that $\\bar{X}_n$ converges to the mean of the distribution, which is $E[X] = 1/\\lambda$, not $\\lambda$. This leads to the contradictory conclusion that $\\hat{\\lambda}_n$ converges to $1/\\lambda$.\n- **C:** This statement claims the estimator is unbiased. The property of being unbiased ($E[\\hat{\\theta}_n] = \\theta$) is distinct from consistency. While some unbiased estimators are consistent, this is not a general rule, and one property does not automatically imply the other. Furthermore, for the Exponential distribution, $E[\\hat{\\lambda}_n] = E[1/\\bar{X}_n] \\neq \\lambda$ (due to Jensen's inequality, since $1/y$ is a convex function, $E[1/\\bar{X}_n]  1/E[\\bar{X}_n] = \\lambda$), so the premise of this statement is false.\n- **D:** This statement mentions that $\\text{Var}(\\hat{\\lambda}_n) \\to 0$. A sufficient condition for consistency is that both the bias and the variance of the estimator approach zero as $n \\to \\infty$. Stating that only the variance approaches zero is incomplete. One must also show that the bias, $B(\\hat{\\lambda}_n) = E[\\hat{\\lambda}_n] - \\lambda$, goes to zero. While it is true that both conditions hold for this estimator, the statement that the variance approaching zero is a *complete justification by itself* is false. The reasoning in option E is more direct and fundamental.\n\nTherefore, the only correct and complete justification among the choices is E.", "answer": "$$\\boxed{E}$$", "id": "1909316"}, {"introduction": "It's important to distinguish between different desirable properties of an estimator. While unbiasedness (having an expected value equal to the true parameter) is a good feature, it does not guarantee consistency. This practice explores a fascinating case study of an estimator that is unbiased for any sample size yet fails to be consistent, clarifying the crucial difference between these two fundamental concepts [@problem_id:1909328].", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample of size $n$ drawn from an Exponential distribution with a probability density function given by $f(x; \\theta) = \\frac{1}{\\theta} \\exp(-\\frac{x}{\\theta})$ for $x  0$ and $\\theta  0$. The parameter $\\theta$ represents the mean of the distribution.\n\nConsider an estimator $T_n$ for the parameter $\\theta$, defined as $T_n = n X_{(1)}$, where $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$ is the smallest observation in the sample.\n\nWhich of the following statements accurately describes the properties of the estimator $T_n$ as the sample size $n$ approaches infinity?\n\nA. $T_n$ is a consistent estimator for $\\theta$.\nB. $T_n$ is not a consistent estimator for $\\theta$ because it is a biased estimator for any finite $n$.\nC. $T_n$ is not a consistent estimator for $\\theta$ because its variance does not approach zero as $n \\to \\infty$.\nD. $T_n$ is a consistent estimator for $\\theta$ because it is an unbiased estimator and its variance approaches zero as $n \\to \\infty$.\nE. $T_n$ is not a consistent estimator for $\\theta$, and it converges in probability to 0.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. with pdf $f(x;\\theta)=(1/\\theta)\\exp(-x/\\theta)$ for $x0$. The survival function is $P(X_{i}x)=\\exp(-x/\\theta)$ for $x\\geq 0$.\n\nFor the minimum $X_{(1)}=\\min(X_{1},\\ldots,X_{n})$, independence gives\n$$\nP(X_{(1)}x)=P(X_{1}x,\\ldots,X_{n}x)=\\left[P(X_{i}x)\\right]^{n}=\\exp\\!\\left(-\\frac{n x}{\\theta}\\right),\\quad x\\geq 0.\n$$\nThus $X_{(1)}$ is exponential with rate $n/\\theta$, so\n$$\n\\mathbb{E}[X_{(1)}]=\\frac{\\theta}{n},\\qquad \\operatorname{Var}(X_{(1)})=\\frac{\\theta^{2}}{n^{2}}.\n$$\nDefine $T_{n}=nX_{(1)}$. Then\n$$\n\\mathbb{E}[T_{n}]=n\\,\\mathbb{E}[X_{(1)}]=n\\cdot\\frac{\\theta}{n}=\\theta,\n$$\nso $T_{n}$ is unbiased for every $n$. Its variance is\n$$\n\\operatorname{Var}(T_{n})=n^{2}\\operatorname{Var}(X_{(1)})=n^{2}\\cdot\\frac{\\theta^{2}}{n^{2}}=\\theta^{2},\n$$\nwhich does not approach $0$ as $n\\to\\infty$.\n\nEquivalently, the distribution of $T_{n}$ is independent of $n$: for $t\\geq 0$,\n$$\nP(T_{n}t)=P(nX_{(1)}t)=P\\!\\left(X_{(1)}\\frac{t}{n}\\right)=\\exp\\!\\left(-\\frac{n}{\\theta}\\cdot\\frac{t}{n}\\right)=\\exp\\!\\left(-\\frac{t}{\\theta}\\right),\n$$\nso $T_{n}$ is exponential with rate $1/\\theta$, having mean $\\theta$ and variance $\\theta^{2}$ for all $n$. Therefore $T_{n}$ does not converge in probability to $\\theta$ (its variance does not vanish) and is not consistent. It also does not converge in probability to $0$ because its distribution is nondegenerate and constant in $n$.\n\nHence, the correct statement is that $T_{n}$ is not a consistent estimator because its variance does not approach zero as $n\\to\\infty$.", "answer": "$$\\boxed{C}$$", "id": "1909328"}]}