{"hands_on_practices": [{"introduction": "Every hypothesis test carries the risk of making an incorrect decision. One such risk is the Type I error, or a \"false alarm,\" which occurs when we reject a null hypothesis that is actually true. This exercise [@problem_id:1918510] provides a concrete scenario to calculate the probability of this error, known as the significance level or size ($\\alpha$) of the test. Mastering this calculation is a fundamental first step in understanding and evaluating the performance of any statistical test.", "problem": "A team of data scientists is evaluating the performance of a new machine learning model for binary classification. The standard model for this task has a well-established success rate of $p=0.5$. The team wants to test if their new model is superior. They formulate a null hypothesis, $H_0$, that their new model is no better than the standard one, so its success probability is $p=0.5$.\n\nTo test this hypothesis, they run the new model on a validation set of $15$ independent trials. Let the random variable $X$ represent the number of successful classifications in these $15$ trials. The team decides on the following decision rule: they will reject the null hypothesis $H_0$ if the number of observed successes $X$ is strictly greater than 10.\n\nGiven that the null hypothesis $H_0: p=0.5$ is true, calculate the exact significance level (the size) of this statistical test. Express your answer as a single, irreducible fraction.", "solution": "Under the null hypothesis $H_{0}: p=0.5$, the number of successes $X$ in $n=15$ independent Bernoulli trials follows a binomial distribution with parameters $n=15$ and $p=\\frac{1}{2}$:\n$$\nX \\sim \\text{Binomial}(15, \\tfrac{1}{2}).\n$$\nThe significance level (size) of the test is the probability, under $H_{0}$, of rejecting $H_{0}$. The decision rule is to reject $H_{0}$ if $X10$, hence the size $\\alpha$ is\n$$\n\\alpha = \\mathbb{P}_{H_{0}}(X10) = \\sum_{k=11}^{15} \\binom{15}{k} \\left(\\tfrac{1}{2}\\right)^{15}.\n$$\nCompute the required binomial coefficients:\n$$\n\\binom{15}{11} = \\binom{15}{4} = 1365,\\quad\n\\binom{15}{12} = \\binom{15}{3} = 455,\\quad\n\\binom{15}{13} = \\binom{15}{2} = 105,\\quad\n\\binom{15}{14} = \\binom{15}{1} = 15,\\quad\n\\binom{15}{15} = 1.\n$$\nThus,\n$$\n\\sum_{k=11}^{15} \\binom{15}{k} = 1365 + 455 + 105 + 15 + 1 = 1941.\n$$\nTherefore,\n$$\n\\alpha = \\frac{1941}{2^{15}} = \\frac{1941}{32768}.\n$$\nSince the denominator is a power of $2$ and the numerator $1941$ is odd, the fraction is irreducible.", "answer": "$$\\boxed{\\frac{1941}{32768}}$$", "id": "1918510"}, {"introduction": "Once we know how to quantify a test's error rate, the next logical question is how to design the *best* possible test. The Neyman-Pearson Lemma provides a powerful theoretical foundation, guiding us to construct a test that maximizes power for a given significance level when comparing two simple hypotheses. This practice problem [@problem_id:1918491] walks you through the application of this fundamental theorem to derive the optimal rejection region, directly connecting the theory of optimal testing to a practical application.", "problem": "A quality control engineer is assessing the lifetime of a new type of electronic component. The lifetime $X$, measured in years, is modeled by an Exponential distribution with a probability density function given by $f(x; \\lambda) = \\lambda \\exp(-\\lambda x)$ for $x  0$, where $\\lambda$ is the failure rate.\n\nThe engineer wants to test two competing hypotheses about the failure rate based on a single observation of the component's lifetime.\n- The null hypothesis, $H_0$, is that the component is from a standard batch with a failure rate of $\\lambda_0 = 2$.\n- The alternative hypothesis, $H_A$, is that the component is from a high-quality batch with a lower failure rate of $\\lambda_1 = 1$.\n\nTo make a decision, the engineer will use the most powerful test at a significance level (size) of $\\alpha = 0.05$. The rejection region for this test is a one-sided interval. Your task is to find the boundary of this region.\n\nDetermine the value of the critical threshold $k$ that defines this rejection region. Express your final answer as a numerical value in years, rounded to four significant figures.", "solution": "We have one observation $X$ from an Exponential distribution with density $f(x;\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x0$. We test the simple hypotheses $H_{0}:\\lambda=\\lambda_{0}=2$ versus $H_{A}:\\lambda=\\lambda_{1}=1$ at size $\\alpha=0.05$.\n\nBy the Neyman–Pearson lemma, the most powerful size-$\\alpha$ test rejects $H_{0}$ for large values of the likelihood ratio $\\Lambda(x) = \\frac{f(x;\\lambda_{1})}{f(x;\\lambda_{0})}$. Compute\n$$\n\\Lambda(x) = \\frac{\\lambda_{1} \\exp(-\\lambda_{1} x)}{\\lambda_{0} \\exp(-\\lambda_{0} x)} = \\frac{\\lambda_{1}}{\\lambda_{0}} \\exp\\big((\\lambda_{0}-\\lambda_{1})x\\big).\n$$\nWith $\\lambda_{0}=2$ and $\\lambda_{1}=1$, this becomes\n$$\n\\Lambda(x) = \\frac{1}{2} \\exp(x),\n$$\nwhich is strictly increasing in $x$. Therefore, the most powerful test rejects $H_{0}$ for large $x$, i.e., the rejection region is of the form $\\{x \\geq k\\}$ for some threshold $k$.\n\nTo achieve size $\\alpha$, choose $k$ so that under $H_{0}$,\n$$\n\\Pr_{\\lambda_{0}}(X \\geq k) = \\alpha.\n$$\nFor $X \\sim \\text{Exp}(\\lambda_{0})$, the survival function is $\\Pr(X \\geq k) = \\exp(-\\lambda_{0} k)$. Hence,\n$$\n\\exp(-\\lambda_{0} k) = \\alpha \\quad \\Longrightarrow \\quad k = -\\frac{1}{\\lambda_{0}} \\ln(\\alpha).\n$$\nSubstituting $\\lambda_{0}=2$ and $\\alpha=0.05$ gives\n$$\nk = -\\frac{1}{2} \\ln(0.05).\n$$\nNumerically, $\\ln(0.05) \\approx -2.995732273553991$, so\n$$\nk \\approx 1.4978661367769955,\n$$\nwhich rounded to four significant figures is $1.498$.\n\nThus, the rejection region is $\\{x \\geq k\\}$ with $k \\approx 1.498$ years.", "answer": "$$\\boxed{1.498}$$", "id": "1918491"}, {"introduction": "Constructing a test is only half the battle; interpreting its results correctly is just as crucial. A common and serious mistake is to misinterpret a \"failure to reject the null hypothesis\" as proof that the null hypothesis is true. This exercise [@problem_id:1918527] confronts this critical misconception head-on, asking you to pinpoint the correct statistical reasoning. It underscores the vital principle that an absence of evidence against the null hypothesis is not the same as evidence for it, often reflecting the test's power rather than the truth of the null.", "problem": "A quality control engineer at a company that manufactures high-precision electronic components is tasked with verifying the mean capacitance of a new batch of capacitors. The design specification requires the mean capacitance to be $\\mu_0 = 250.0$ picofarads (pF). The engineer plans to perform a two-tailed hypothesis test with a significance level of $\\alpha = 0.05$. The null and alternative hypotheses are set as:\n\n$H_0: \\mu = 250.0$ pF\n$H_a: \\mu \\neq 250.0$ pF\n\nThe engineer collects a random sample of $n = 40$ capacitors from the batch and measures their capacitance. The sample mean is found to be $\\bar{x} = 250.8$ pF, and the sample standard deviation is $s = 4.5$ pF. After performing the appropriate statistical test, the engineer calculates a p-value of approximately $0.26$. Since this p-value is greater than the significance level $\\alpha = 0.05$, the engineer fails to reject the null hypothesis.\n\nUpon seeing the report, a manager exclaims, \"Excellent! This result proves that the average capacitance of our new batch is exactly 250.0 pF, right on target.\"\n\nWhich of the following statements provides the best statistical explanation for why the manager's conclusion is incorrect?\n\nA. The test only indicates there is insufficient evidence to conclude the mean is different from 250.0 pF. The true mean could be slightly different, but the test may not have had enough statistical power to detect this difference.\n\nB. A Type I error might have occurred. This means the null hypothesis was incorrectly rejected, and the true mean is indeed 250.0 pF.\n\nC. The large p-value of 0.26 indicates that there is a 26% probability that the null hypothesis is true, which is not high enough to constitute a proof.\n\nD. The conclusion is based on a sample, not the entire population. To prove the mean is 250.0 pF, every single capacitor in the batch must be measured.\n\nE. The significance level of $\\alpha = 0.05$ was too large. A smaller significance level (e.g., $\\alpha = 0.01$) should have been used to provide definitive proof for the null hypothesis.", "solution": "We formalize the test the engineer performed. The hypotheses are $H_{0}:\\mu=\\mu_{0}$ versus $H_{a}:\\mu\\neq\\mu_{0}$ with $\\mu_{0}=250.0$. Using a sample of size $n$ with sample mean $\\bar{X}$ and sample standard deviation $S$, the usual test statistic for a two-sided one-sample mean test is\n$$\nT=\\frac{\\bar{X}-\\mu_{0}}{S/\\sqrt{n}}.\n$$\nThe two-sided p-value is\n$$\np=P\\left(|T^{\\ast}|\\geq |T_{\\text{obs}}|\\mid H_{0}\\right),\n$$\nwhere $T^{\\ast}$ denotes the sampling distribution of $T$ under $H_{0}$. The decision rule at level $\\alpha$ is: reject $H_{0}$ if $p\\leq \\alpha$; otherwise, fail to reject $H_{0}$.\n\nIn the report, $p\\alpha$, so the correct statistical conclusion is to fail to reject $H_{0}$. Crucially, this decision does not establish that $H_{0}$ is true; it only indicates that the observed data do not provide sufficient evidence against $H_{0}$ at the chosen $\\alpha$.\n\nTo see why the manager’s statement is incorrect, recall the concepts of Type I and Type II errors and power. A Type I error occurs when $H_{0}$ is rejected even though $H_{0}$ is true, with probability $P(\\text{Type I})=\\alpha$. That possibility is only relevant when the decision is to reject. Here, there was no rejection, so a Type I error cannot have occurred. A Type II error occurs when $H_{0}$ is not rejected even though $H_{0}$ is false, with probability $\\beta(\\mu)$, and the test’s power is $1-\\beta(\\mu)$. Power depends on the true effect size $\\delta=\\mu-\\mu_{0}$, the sample size $n$, the variability (through $S$), and the test form. If $|\\delta|$ is modest relative to the standard error $S/\\sqrt{n}$, the test may have low to moderate power, and thus it may fail to detect a real but small difference. Therefore, a nonrejection (a large p-value) is consistent both with $H_{0}$ being true and with $H_{0}$ being false by a small margin; it does not “prove” equality.\n\nWe now evaluate the options:\n- Option A correctly states that failing to reject $H_{0}$ indicates insufficient evidence of a difference and that the true mean could still differ slightly, potentially undetected due to limited power. This matches the correct interpretation of $p\\alpha$ and the role of power.\n- Option B invokes a Type I error, which would require a rejection that did not occur; thus it is inapplicable.\n- Option C misinterprets the p-value as $P(H_{0}\\mid\\text{data})$, which it is not. The p-value is $P(\\text{data or more extreme}\\mid H_{0})$, not a posterior probability of $H_{0}$.\n- Option D shifts to the idea of measuring every item. While a census could reveal the finite-batch mean, hypothesis testing does not require a census to justify nonproof of $H_{0}$; the key point is that nonrejection does not establish truth of $H_{0}$, regardless of census considerations.\n- Option E incorrectly suggests that a smaller $\\alpha$ can provide “definitive proof” for $H_{0}$. Hypothesis tests do not prove $H_{0}$; they can only fail to find evidence against it, and decreasing $\\alpha$ makes rejection harder, not proof stronger.\n\nTherefore, the best statistical explanation is precisely the one given in Option A.", "answer": "$$\\boxed{A}$$", "id": "1918527"}]}