{"hands_on_practices": [{"introduction": "The Karlin-Rubin theorem provides a powerful tool for constructing Uniformly Most Powerful (UMP) tests, especially for distributions in the one-parameter exponential family. The first step in this process is to identify the sufficient statistic and confirm the monotone likelihood ratio property. This practice [@problem_id:1966245] provides a hands-on exercise in this foundational skill using the Weibull distribution, a model frequently applied in reliability engineering and survival analysis.", "problem": "In industrial quality control and reliability engineering, the lifetime of certain electronic components is often modeled using the Weibull distribution. A random sample of $n$ such components is taken, and their lifetimes are measured, yielding the observations $X_1, X_2, \\ldots, X_n$. These lifetimes are assumed to be independent and identically distributed according to a Weibull distribution with a known, fixed shape parameter $k > 0$ and an unknown scale parameter $\\lambda > 0$.\n\nThe probability density function (PDF) for a single lifetime observation $X$ is given by:\n$$f(x; \\lambda, k) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right), \\quad \\text{for } x \\ge 0$$\nAn engineer wants to determine if the average lifetime of the components has increased. This corresponds to testing the null hypothesis $H_0: \\lambda \\le \\lambda_0$ against the alternative hypothesis $H_1: \\lambda > \\lambda_0$, where $\\lambda_0$ is a pre-specified performance benchmark.\n\nA Uniformly Most Powerful (UMP) test for this hypothesis problem exists. The rejection region for this test, at a given significance level $\\alpha$, can be expressed in the form $T(X_1, \\ldots, X_n) > c$, where $T$ is a test statistic derived from the sample and $c$ is a critical value.\n\nDetermine the test statistic $T(X_1, \\ldots, X_n)$. Your answer should be an expression in terms of the sample values $X_i$ and the known parameter $k$.", "solution": "We begin from the given Weibull density for a single observation with known shape parameter $k>0$ and scale parameter $\\lambda>0$:\n$$\nf(x;\\lambda,k)=\\frac{k}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{x}{\\lambda}\\right)^{k}\\right),\\quad x\\ge 0.\n$$\nFor independent observations $X_{1},\\ldots,X_{n}$, the joint likelihood is\n$$\nL(\\lambda; x_{1},\\ldots,x_{n})=\\prod_{i=1}^{n}\\frac{k}{\\lambda}\\left(\\frac{x_{i}}{\\lambda}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{x_{i}}{\\lambda}\\right)^{k}\\right).\n$$\nAlgebraically,\n$$\nL(\\lambda;x)=\\left(\\prod_{i=1}^{n}k x_{i}^{k-1}\\right)\\lambda^{-n}\\lambda^{-(k-1)n}\\exp\\!\\left(-\\sum_{i=1}^{n}\\frac{x_{i}^{k}}{\\lambda^{k}}\\right)\n=\\left(\\prod_{i=1}^{n}k x_{i}^{k-1}\\right)\\lambda^{-kn}\\exp\\!\\left(-\\sum_{i=1}^{n}\\frac{x_{i}^{k}}{\\lambda^{k}}\\right).\n$$\nIntroduce the reparameterization $\\theta=\\lambda^{k}$ with $\\theta>0$. Then $\\lambda^{-kn}=\\theta^{-n}$ and $\\sum x_{i}^{k}/\\lambda^{k}=\\sum x_{i}^{k}/\\theta$, so\n$$\nL(\\theta;x)=\\left(\\prod_{i=1}^{n}k x_{i}^{k-1}\\right)\\theta^{-n}\\exp\\!\\left(-\\frac{1}{\\theta}\\sum_{i=1}^{n}x_{i}^{k}\\right).\n$$\nThis exhibits a one-parameter exponential family in $\\theta$ with sufficient statistic\n$$\nS=\\sum_{i=1}^{n}x_{i}^{k}.\n$$\nFor testing $H_{0}:\\lambda\\le\\lambda_{0}$ versus $H_{1}:\\lambda\\lambda_{0}$, equivalently $H_{0}:\\theta\\le\\theta_{0}$ versus $H_{1}:\\theta\\theta_{0}$ where $\\theta_{0}=\\lambda_{0}^{k}$, the likelihood ratio for any $\\theta_{1}\\theta_{0}$ is\n$$\n\\frac{L(\\theta_{1};x)}{L(\\theta_{0};x)}=\\left(\\frac{\\theta_{0}}{\\theta_{1}}\\right)^{n}\\exp\\!\\left(-S\\left(\\frac{1}{\\theta_{1}}-\\frac{1}{\\theta_{0}}\\right)\\right),\n$$\nwhich is an increasing function of $S$ because $\\theta_{1}\\theta_{0}$ implies $\\frac{1}{\\theta_{1}}-\\frac{1}{\\theta_{0}}0$. Thus the family has a monotone likelihood ratio in $S$, and by the Karlinâ€“Rubin theorem, the uniformly most powerful test for the one-sided hypothesis rejects for large values of $S$.\n\nTherefore, the test statistic may be taken as\n$$\nT(X_{1},\\ldots,X_{n})=\\sum_{i=1}^{n}X_{i}^{k}.\n$$\nEquivalently, any positive scalar multiple such as $\\sum_{i=1}^{n}(X_{i}/\\lambda_{0})^{k}$ induces the same rejection ordering, but the canonical sufficient statistic is $\\sum_{i=1}^{n}X_{i}^{k}$.", "answer": "$$\\boxed{\\sum_{i=1}^{n}X_{i}^{k}}$$", "id": "1966245"}, {"introduction": "Once the correct test statistic for a UMP test has been identified, the next practical step is to define the precise rejection region for a given significance level $\\alpha$. This involves calculating a critical value that draws the line between rejecting and not rejecting the null hypothesis. This exercise [@problem_id:1966299] demonstrates this crucial process for a log-normal distribution, a common model in economics and environmental science, reinforcing the link between theoretical test construction and its numerical application.", "problem": "In a study of environmental science, the concentration of a certain heavy metal pollutant in a river is believed to follow a log-normal distribution. Let the random variable $X$ represent this concentration. The probability density function (PDF) of a log-normal random variable $X$ with parameters $\\mu$ and $\\sigma^2$ is given by:\n$$f(x; \\mu, \\sigma^2) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(x) - \\mu)^2}{2\\sigma^2}\\right), \\quad \\text{for } x > 0$$\nwhere $\\mu$ is the mean and $\\sigma^2$ is the variance of the variable's natural logarithm.\n\nFrom historical data, the variance parameter is known to be stable at $\\sigma_0^2 = 4.0$. However, recent industrial activity in the watershed is suspected to have increased the median concentration of the pollutant, which corresponds to an increase in the parameter $\\mu$. To test this suspicion, a random sample of $n=16$ water specimens is collected.\n\nWe want to perform a hypothesis test for the parameter $\\mu$ with a significance level of $\\alpha = 0.05$. The null and alternative hypotheses are set as:\n$$H_0: \\mu \\le 1.5$$\n$$H_1: \\mu > 1.5$$\n\nThe uniformly most powerful (UMP) test for this problem has a rejection region of the form $\\bar{Y} > c$, where $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} \\ln(X_i)$ and $c$ is a critical value. Determine the value of this critical value $c$.\n\nFor the standard normal distribution, you may use the approximation that a value of $z=1.645$ cuts off the top 5% of the distribution. Express your final answer as a number rounded to four significant figures.", "solution": "Let $X$ be log-normal with parameters $(\\mu,\\sigma^{2})$, so $Y=\\ln(X)$ is normal with $Y \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. For a random sample $X_{1},\\dots,X_{n}$, define $Y_{i}=\\ln(X_{i})$, then $Y_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(\\mu,\\sigma^{2})$. The sample mean $\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$ is therefore normal with\n$$\n\\bar{Y} \\sim \\mathcal{N}\\left(\\mu,\\frac{\\sigma^{2}}{n}\\right).\n$$\nWe test $H_{0}:\\mu \\le 1.5$ versus $H_{1}:\\mu1.5$ with known variance $\\sigma^{2}=\\sigma_{0}^{2}=4.0$ and $n=16$ at significance level $\\alpha=0.05$. For a one-sided test with known variance in the normal family, the uniformly most powerful test rejects for large values of $\\bar{Y}$, with critical value $c$ chosen so that under the least favorable case $\\mu=\\mu_{0}=1.5$,\n$$\n\\mathbb{P}_{\\mu_{0}}\\!\\left(\\bar{Y}c\\right)=\\alpha.\n$$\nStandardizing, let\n$$\nZ=\\frac{\\bar{Y}-\\mu_{0}}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1)\\quad \\text{under } \\mu=\\mu_{0},\n$$\nso the condition becomes\n$$\n\\mathbb{P}\\!\\left(Z\\frac{c-\\mu_{0}}{\\sigma/\\sqrt{n}}\\right)=\\alpha.\n$$\nEquivalently,\n$$\n\\frac{c-\\mu_{0}}{\\sigma/\\sqrt{n}}=z_{1-\\alpha},\n$$\nwhere $z_{1-\\alpha}$ is the $(1-\\alpha)$ quantile of the standard normal distribution. Thus,\n$$\nc=\\mu_{0}+z_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}}.\n$$\nUsing $\\mu_{0}=1.5$, $\\sigma=\\sqrt{4.0}=2$, $n=16$ so $\\sqrt{n}=4$, and the given approximation $z_{1-\\alpha}=1.645$ for $\\alpha=0.05$, we obtain\n$$\nc=1.5+1.645\\cdot \\frac{2}{4}=1.5+1.645\\cdot 0.5=1.5+0.8225=2.3225.\n$$\nRounding to four significant figures yields $c=2.323$.", "answer": "$$\\boxed{2.323}$$", "id": "1966299"}, {"introduction": "Many real-world statistical questions, such as those in A/B testing or clinical trials, involve comparing two populations. These situations often introduce nuisance parameters that complicate the hypothesis test. This problem [@problem_id:1966251] showcases an elegant and powerful technique for constructing a UMP test in such scenarios by conditioning on a sufficient statistic to eliminate the nuisance parameter, a cornerstone method in statistical inference.", "problem": "In the context of online advertising, a company runs an A/B test on two different ad campaigns, Campaign A and Campaign B, over a fixed, identical time period. The number of user clicks generated by each campaign can be modeled as an independent Poisson process. Let $X_A$ be the number of clicks from Campaign A, assumed to be a random variable from a Poisson distribution with mean $\\lambda_A$. Similarly, let $X_B$ be the number of clicks from Campaign B, from a Poisson distribution with mean $\\lambda_B$. The two random variables $X_A$ and $X_B$ are independent.\n\nThe company wants to determine if Campaign A is significantly more effective than Campaign B. This can be formulated as a hypothesis test on the ratio of the click rates, $\\theta = \\lambda_A / \\lambda_B$. The null hypothesis is that Campaign A is not more effective than Campaign B, $H_0: \\theta \\le 1$, and the alternative is that it is more effective, $H_1: \\theta > 1$.\n\nLet $x_A$ and $x_B$ be the observed number of clicks for each campaign, and let the total number of clicks be $s = x_A + x_B$. You are tasked with identifying the Uniformly Most Powerful (UMP) test of size $\\alpha$.\n\nWhich of the following describes the rejection region for the size-$\\alpha$ UMP test for $H_0: \\lambda_A/\\lambda_B \\le 1$ versus $H_1: \\lambda_A/\\lambda_B > 1$?\n\nA. Reject $H_0$ if $x_A > k_s$, where the critical value $k_s$ depends on the total count $s$ and is chosen to satisfy the size constraint based on a Binomial distribution.\n\nB. Reject $H_0$ if $s > k$, where the critical value $k$ is a constant chosen to satisfy the size constraint based on a Poisson distribution.\n\nC. Reject $H_0$ if the ratio $x_A / x_B > k$, where the critical value $k$ is a constant chosen to satisfy the size constraint based on an F-distribution.\n\nD. Reject $H_0$ if $x_A  k_s$, where the critical value $k_s$ depends on the total count $s$ and is chosen to satisfy the size constraint based on a Binomial distribution.\n\nE. Reject $H_0$ if $x_A - x_B > k$, where the critical value $k$ is a constant chosen to satisfy the size constraint based on a Normal distribution.", "solution": "Let $X_A \\sim \\text{Poisson}(\\lambda_A)$ and $X_B \\sim \\text{Poisson}(\\lambda_B)$ be independent random variables. The joint probability mass function (PMF) of $(X_A, X_B)$ for observed values $(x_A, x_B)$ is given by:\n$$ P(X_A=x_A, X_B=x_B; \\lambda_A, \\lambda_B) = \\frac{e^{-\\lambda_A}\\lambda_A^{x_A}}{x_A!} \\frac{e^{-\\lambda_B}\\lambda_B^{x_B}}{x_B!} $$\nThis can be written as:\n$$ f(x_A, x_B; \\lambda_A, \\lambda_B) = \\frac{1}{x_A! x_B!} \\exp\\left(x_A \\ln(\\lambda_A) + x_B \\ln(\\lambda_B) - (\\lambda_A+\\lambda_B)\\right) $$\nThis is a two-parameter exponential family with a two-dimensional sufficient statistic $(X_A, X_B)$. The hypothesis is on the parameter $\\theta = \\lambda_A / \\lambda_B$. This is a composite hypothesis with a nuisance parameter (e.g., $\\lambda_A + \\lambda_B$). To derive a UMP test, we can use the method of conditioning on the sufficient statistic for the nuisance parameter. Let $S = X_A + X_B$. The distribution of $S$ is Poisson with mean $\\lambda_A + \\lambda_B$, since the sum of independent Poisson random variables is also a Poisson random variable.\n\nLet's find the conditional distribution of $X_A$ given $S=s$. For $x_A = 0, 1, \\dots, s$:\n$$ P(X_A=x_A | S=s) = \\frac{P(X_A=x_A, X_A+X_B=s)}{P(X_A+X_B=s)} = \\frac{P(X_A=x_A, X_B=s-x_A)}{P(S=s)} $$\nUsing the independence of $X_A$ and $X_B$:\n$$ P(X_A=x_A | S=s) = \\frac{\\frac{e^{-\\lambda_A}\\lambda_A^{x_A}}{x_A!} \\frac{e^{-\\lambda_B}\\lambda_B^{s-x_A}}{(s-x_A)!}}{\\frac{e^{-(\\lambda_A+\\lambda_B)}(\\lambda_A+\\lambda_B)^s}{s!}} $$\n$$ = \\frac{s!}{x_A!(s-x_A)!} \\frac{e^{-(\\lambda_A+\\lambda_B)}\\lambda_A^{x_A}\\lambda_B^{s-x_A}}{e^{-(\\lambda_A+\\lambda_B)}(\\lambda_A+\\lambda_B)^s} $$\n$$ = \\binom{s}{x_A} \\frac{\\lambda_A^{x_A}\\lambda_B^{s-x_A}}{(\\lambda_A+\\lambda_B)^s} = \\binom{s}{x_A} \\left(\\frac{\\lambda_A}{\\lambda_A+\\lambda_B}\\right)^{x_A} \\left(\\frac{\\lambda_B}{\\lambda_A+\\lambda_B}\\right)^{s-x_A} $$\nThis is the PMF of a Binomial distribution, $X_A | (S=s) \\sim \\text{Binomial}(s, p)$, where the probability of success is $p = \\frac{\\lambda_A}{\\lambda_A+\\lambda_B}$.\n\nNow, we re-parameterize the hypothesis in terms of $p$. Let $\\theta = \\lambda_A / \\lambda_B$.\n$$ p = \\frac{\\lambda_A}{\\lambda_A+\\lambda_B} = \\frac{\\lambda_A/\\lambda_B}{(\\lambda_A/\\lambda_B) + 1} = \\frac{\\theta}{\\theta+1} $$\nThe function $g(\\theta) = \\frac{\\theta}{\\theta+1}$ is a strictly increasing function of $\\theta$ for $\\theta > 0$. Therefore, the original hypothesis test is equivalent to a test on $p$.\n$H_0: \\theta \\le 1 \\iff p \\le \\frac{1}{1+1} = \\frac{1}{2}$\n$H_1: \\theta > 1 \\iff p > \\frac{1}{2}$\n\nThe problem is now reduced to finding a UMP test for $H_0: p \\le 1/2$ versus $H_1: p > 1/2$ for a random variable $X_A$ which follows a Binomial$(s, p)$ distribution (conditional on the observed total $s$). The PMF of this distribution, $f(x_A; p) = \\binom{s}{x_A} p^{x_A} (1-p)^{s-x_A}$, belongs to a one-parameter exponential family with sufficient statistic $T(X_A) = X_A$. The likelihood ratio for any $p_2 > p_1$ is an increasing function of $X_A$.\n\nBy the Karlin-Rubin Theorem, a UMP test of size $\\alpha$ for this one-sided hypothesis exists and its rejection region is of the form $\\{x_A : x_A > k_s\\}$. The critical value $k_s$ is chosen for a given total count $s$ to satisfy the size constraint:\n$$ \\sup_{p \\le 1/2} P_p(X_A > k_s | S=s) = P_{p=1/2}(X_A > k_s | S=s) \\le \\alpha $$\nThe equality holds because the power function is increasing in $p$. So, for a given observed total $s=x_A+x_B$, the test rejects $H_0$ if the observed count $x_A$ is greater than some critical value $k_s$. This critical value is determined from the null distribution, which is $\\text{Binomial}(s, 1/2)$.\n\nThis matches option A.\n- Option B tests the total rate $\\lambda_A+\\lambda_B$, not the ratio.\n- Option C is an intuitive but incorrect test statistic form, and the F-distribution is used for ratios of variances of normal populations, not ratios of Poisson means.\n- Option D describes a test for the alternative $H_1: \\lambda_A/\\lambda_B  1$.\n- Option E uses a statistic which is also not the UMP one, and a Normal approximation is an approximation, not the exact UMP test.", "answer": "$$\\boxed{A}$$", "id": "1966251"}]}