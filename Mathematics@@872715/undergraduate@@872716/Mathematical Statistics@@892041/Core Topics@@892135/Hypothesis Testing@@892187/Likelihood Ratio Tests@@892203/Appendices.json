{"hands_on_practices": [{"introduction": "This first practice provides a foundational walkthrough of constructing a Likelihood Ratio Test. By working with the familiar Gamma distribution [@problem_id:1930657], you will apply the core principle of comparing the likelihood maximized under a specific null hypothesis to the likelihood maximized over the entire parameter space. This exercise is essential for building the procedural fluency needed to tackle more complex hypothesis testing scenarios.", "problem": "A telecommunications engineer is modeling the processing times of data packets at a network router. Based on the router's architecture, the time $X$ (in milliseconds) required to process a single packet is known to follow a Gamma distribution with a known shape parameter $\\alpha  0$ and an unknown rate parameter $\\beta  0$. The Probability Density Function (PDF) is given by:\n$$f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) \\quad \\text{for } x  0$$\nThe engineer has collected a random sample of $n$ packet processing times, $X_1, X_2, \\dots, X_n$, which can be considered independent and identically distributed. She wants to test whether the router is operating under a nominal load, which corresponds to a specific rate parameter $\\beta = \\beta_0$, against the alternative that it is not. The formal hypotheses are:\n$$H_0: \\beta = \\beta_0$$\n$$H_1: \\beta \\neq \\beta_0$$\nDerive the Likelihood Ratio Test (LRT) statistic, $\\lambda(x_1, \\dots, x_n)$, for this hypothesis test. Express your answer as a function of the sample size $n$, the known shape parameter $\\alpha$, the hypothesized rate $\\beta_0$, and the sample mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$.", "solution": "The joint likelihood for independent observations $X_{1},\\dots,X_{n}$ from the Gamma$(\\alpha,\\beta)$ distribution with known $\\alpha$ and unknown $\\beta$ is\n$$\nL(\\beta \\mid x_{1},\\dots,x_{n})=\\prod_{i=1}^{n}\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x_{i}^{\\alpha-1}\\exp(-\\beta x_{i})\n=\\frac{\\beta^{n\\alpha}}{\\Gamma(\\alpha)^{n}}\\left(\\prod_{i=1}^{n}x_{i}^{\\alpha-1}\\right)\\exp\\!\\left(-\\beta\\sum_{i=1}^{n}x_{i}\\right).\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}=n\\bar{x}$. The log-likelihood is\n$$\n\\ell(\\beta)=n\\alpha\\ln\\beta-n\\ln\\Gamma(\\alpha)+(\\alpha-1)\\sum_{i=1}^{n}\\ln x_{i}-\\beta S.\n$$\nMaximizing over $\\beta0$ gives the unrestricted MLE via\n$$\n\\frac{\\partial \\ell}{\\partial \\beta}=\\frac{n\\alpha}{\\beta}-S=0\\quad\\Rightarrow\\quad \\hat{\\beta}=\\frac{n\\alpha}{S}=\\frac{\\alpha}{\\bar{x}}.\n$$\nThe likelihood ratio statistic is\n$$\n\\lambda(x_{1},\\dots,x_{n})=\\frac{\\sup_{\\beta=\\beta_{0}}L(\\beta\\mid x)}{\\sup_{\\beta0}L(\\beta\\mid x)}=\\frac{L(\\beta_{0}\\mid x)}{L(\\hat{\\beta}\\mid x)}.\n$$\nUsing the likelihood form, terms not depending on $\\beta$ cancel, yielding\n$$\n\\lambda(x_{1},\\dots,x_{n})=\\left(\\frac{\\beta_{0}}{\\hat{\\beta}}\\right)^{n\\alpha}\\exp\\!\\left(-(\\beta_{0}-\\hat{\\beta})S\\right).\n$$\nSubstituting $\\hat{\\beta}=\\alpha/\\bar{x}$ and $S=n\\bar{x}$ gives\n$$\n\\lambda(x_{1},\\dots,x_{n})=\\left(\\frac{\\beta_{0}\\bar{x}}{\\alpha}\\right)^{n\\alpha}\\exp\\!\\left(n\\alpha-n\\beta_{0}\\bar{x}\\right).\n$$\nThis expresses the LRT statistic in terms of $n$, $\\alpha$, $\\beta_{0}$, and $\\bar{x}$.", "answer": "$$\\boxed{\\left(\\frac{\\beta_{0}\\bar{x}}{\\alpha}\\right)^{n\\alpha}\\exp\\!\\left(n\\alpha-n\\beta_{0}\\bar{x}\\right)}$$", "id": "1930657"}, {"introduction": "Statistical models are not always \"regular\" or smooth, and the Likelihood Ratio Test framework is robust enough to handle these important exceptions. This exercise [@problem_id:1930681] explores a test for the parameter of a Uniform distribution, where the parameter itself defines the support of the distribution. This scenario requires a different maximization technique and demonstrates how LRTs can lead to intuitive tests based on order statistics, such as the sample maximum.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a random sample from a Uniform distribution on the interval $(0, \\theta)$, where $\\theta  0$ is an unknown parameter. A statistician wants to test the null hypothesis $H_0: \\theta = \\theta_0$ against the two-sided alternative hypothesis $H_1: \\theta \\ne \\theta_0$ using a test with a significance level of $\\alpha$, where $\\theta_0$ is a specified positive constant and $\\alpha \\in (0, 1)$.\n\nThe rejection region for the corresponding Likelihood Ratio Test (LRT) can be shown to be of the form $X_{(n)} \\le k_1$ or $X_{(n)}  k_2$, where $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$ is the sample maximum. Under the null hypothesis, the probability of the event $X_{(n)}  \\theta_0$ is zero, so for the purpose of constructing the test, we can set $k_2 = \\theta_0$. Your task is to find the value of the other critical value, $k_1$.\n\nExpress $k_1$ as a closed-form analytic expression in terms of $n$, $\\theta_0$, and $\\alpha$.", "solution": "Let $M = X_{(n)} = \\max\\{X_{1},\\ldots,X_{n}\\}$. The likelihood for a sample $x=(x_{1},\\ldots,x_{n})$ from $\\operatorname{Unif}(0,\\theta)$ is\n$$\nL(\\theta\\mid x)=\\theta^{-n}\\,\\mathbf{1}\\{M\\le \\theta\\}.\n$$\nThe unrestricted supremum of the likelihood over $\\theta0$ occurs at $\\theta=M$, giving\n$$\n\\sup_{\\theta0}L(\\theta\\mid x)=M^{-n}.\n$$\nUnder $H_{0}:\\theta=\\theta_{0}$,\n$$\nL(\\theta_{0}\\mid x)=\\theta_{0}^{-n}\\,\\mathbf{1}\\{M\\le \\theta_{0}\\}.\n$$\nHence, the likelihood ratio statistic is\n$$\n\\lambda(x)=\\frac{L(\\theta_{0}\\mid x)}{\\sup_{\\theta0}L(\\theta\\mid x)}=\\left(\\frac{M}{\\theta_{0}}\\right)^{n}\\mathbf{1}\\{M\\le \\theta_{0}\\},\n$$\nwith $\\lambda(x)=0$ if $M\\theta_{0}$. The LRT rejects for small values of $\\lambda(x)$, which is equivalent to rejecting when $M$ is small (or when $M\\theta_{0}$). Therefore, the rejection region has the form $M\\le k_{1}$ or $M\\theta_{0}$.\n\nTo achieve significance level $\\alpha$, we require under $H_{0}$ that\n$$\n\\mathbb{P}_{\\theta_{0}}(M\\le k_{1})+\\mathbb{P}_{\\theta_{0}}(M\\theta_{0})=\\alpha.\n$$\nSince $\\mathbb{P}_{\\theta_{0}}(M\\theta_{0})=0$, this reduces to\n$$\n\\mathbb{P}_{\\theta_{0}}(M\\le k_{1})=\\alpha.\n$$\nFor $X_{i}\\sim \\operatorname{Unif}(0,\\theta_{0})$ i.i.d., the distribution of the maximum is\n$$\n\\mathbb{P}_{\\theta_{0}}(M\\le t)=\\left(\\frac{t}{\\theta_{0}}\\right)^{n},\\quad 0t\\theta_{0}.\n$$\nSetting this equal to $\\alpha$ and solving for $k_{1}$ gives\n$$\n\\left(\\frac{k_{1}}{\\theta_{0}}\\right)^{n}=\\alpha\n\\quad\\Longrightarrow\\quad\nk_{1}=\\theta_{0}\\,\\alpha^{\\frac{1}{n}}.\n$$", "answer": "$$\\boxed{\\theta_{0}\\alpha^{\\frac{1}{n}}}$$", "id": "1930681"}, {"introduction": "The power of the Likelihood Ratio Test truly shines when we move from single-sample problems to comparing multiple groups. This practice challenges you to derive the LRT for testing the equality of means across several normal populations, which forms the theoretical basis for the Analysis of Variance (ANOVA) [@problem_id:1930651]. You will see how the LRT principle elegantly yields a test statistic that partitions total variability, a cornerstone concept in applied statistics.", "problem": "An environmental protection agency is tasked with monitoring the concentration of a particular pollutant, Particulate Matter 2.5 (PM2.5), at a large industrial park. To do this, they deploy $k$ independent, identically manufactured air quality sensors at different locations. Let $X_{ij}$ be the $j$-th measurement from the $i$-th sensor, for $i=1, \\dots, k$ and $j=1, \\dots, n_i$. From the manufacturing specifications, it is known that the measurements from any sensor follow a normal distribution with a common, known variance $\\sigma^2$. However, due to potential drift or malfunction, the mean reading $\\mu_i$ for each sensor might differ.\n\nThe agency wants to test if all sensors are properly calibrated, meaning they have the same mean response. The formal hypothesis test is:\n$H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k$\nversus\n$H_a: \\text{at least one } \\mu_i \\text{ is different from the others.}$\n\nYour task is to determine the test statistic for the generalized likelihood ratio test. Let $\\lambda$ be the generalized likelihood ratio. For large sample sizes, the test is based on the statistic $-2\\ln\\lambda$.\n\nLet $\\bar{X}_i = \\frac{1}{n_i}\\sum_{j=1}^{n_i} X_{ij}$ be the sample mean for the $i$-th sensor, and let $\\bar{X}_{..} = \\frac{1}{\\sum_{i=1}^k n_i} \\sum_{i=1}^k \\sum_{j=1}^{n_i} X_{ij}$ be the grand mean of all measurements.\n\nWhich of the following expressions represents the statistic $-2\\ln\\lambda$?\n\nA. $\\frac{1}{\\sigma^2} \\sum_{i=1}^k n_i (\\bar{X}_i - \\bar{X}_{..})^2$\n\nB. $\\frac{1}{\\sigma^2} \\sum_{i=1}^k \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2$\n\nC. $\\frac{1}{\\sigma^2} \\sum_{i=1}^k (\\bar{X}_i - \\bar{X}_{..})^2$\n\nD. $\\frac{1}{\\sigma^2} \\sum_{i=1}^k n_i (\\bar{X}_i)^2$\n\nE. $\\frac{\\sum_{i=1}^k n_i (\\bar{X}_i - \\bar{X}_{..})^2}{\\sum_{i=1}^k \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2}$", "solution": "Let $N=\\sum_{i=1}^{k} n_{i}$. Under the model $X_{ij}\\sim \\mathcal{N}(\\mu_{i},\\sigma^{2})$ independently with known $\\sigma^{2}$, the likelihood for parameters $\\{\\mu_{i}\\}_{i=1}^{k}$ is\n$$\nL(\\{\\mu_{i}\\})=(2\\pi\\sigma^{2})^{-N/2}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\mu_{i})^{2}\\right).\n$$\nThe generalized likelihood ratio is $\\lambda=\\dfrac{\\sup_{H_{0}}L}{\\sup_{H_{1}}L}$, so\n$$\n\\ln\\lambda=\\ln\\sup_{H_{0}}L-\\ln\\sup_{H_{1}}L.\n$$\n\nUnder $H_{1}$, each $\\mu_{i}$ is free. Maximizing the likelihood is equivalent to minimizing $\\sum_{j=1}^{n_{i}}(X_{ij}-\\mu_{i})^{2}$ with respect to $\\mu_{i}$, yielding the MLE $\\hat{\\mu}_{i}=\\bar{X}_{i}$. Thus\n$$\n\\sup_{H_{1}}L=(2\\pi\\sigma^{2})^{-N/2}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})^{2}\\right).\n$$\n\nUnder $H_{0}$, $\\mu_{1}=\\cdots=\\mu_{k}=\\mu$ is common. Maximizing the likelihood is equivalent to minimizing $\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\mu)^{2}$ with respect to $\\mu$, yielding the MLE $\\hat{\\mu}=\\bar{X}_{..}$. Thus\n$$\n\\sup_{H_{0}}L=(2\\pi\\sigma^{2})^{-N/2}\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{..})^{2}\\right).\n$$\n\nTherefore,\n$$\n\\ln\\lambda=-\\frac{1}{2\\sigma^{2}}\\left[\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{..})^{2}-\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})^{2}\\right],\n$$\nand hence\n$$\n-2\\ln\\lambda=\\frac{1}{\\sigma^{2}}\\left[\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{..})^{2}-\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})^{2}\\right].\n$$\n\nUse the standard ANOVA decomposition for each $i$:\n$$\n\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{..})^{2}=\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})^{2}+2(\\bar{X}_{i}-\\bar{X}_{..})\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})+n_{i}(\\bar{X}_{i}-\\bar{X}_{..})^{2}.\n$$\nThe cross term vanishes because $\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})=0$, so\n$$\n\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{..})^{2}=\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})^{2}+n_{i}(\\bar{X}_{i}-\\bar{X}_{..})^{2}.\n$$\nSumming over $i$ gives\n$$\n\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{..})^{2}-\\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(X_{ij}-\\bar{X}_{i})^{2}=\\sum_{i=1}^{k}n_{i}(\\bar{X}_{i}-\\bar{X}_{..})^{2}.\n$$\nHence\n$$\n-2\\ln\\lambda=\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{k}n_{i}(\\bar{X}_{i}-\\bar{X}_{..})^{2},\n$$\nwhich corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1930651"}]}