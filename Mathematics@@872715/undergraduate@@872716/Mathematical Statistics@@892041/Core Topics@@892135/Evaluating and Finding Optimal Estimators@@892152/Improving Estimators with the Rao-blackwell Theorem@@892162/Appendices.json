{"hands_on_practices": [{"introduction": "This first practice provides a very intuitive application of the Rao-Blackwell theorem in the context of reliability engineering. We start with a simple, unbiased estimator for the mean lifetime of a component based only on the time of the first failure, $nX_{(1)}$. By conditioning this on the total time on test, $T = \\sum X_i$, which is a sufficient statistic, we will see how the theorem transforms this initial guess into the familiar and efficient sample mean [@problem_id:1922392].", "problem": "A quality control engineer is studying the lifetime of a specific electronic component. The lifetime, $X$, of any given component is modeled by an exponential distribution with an unknown mean lifetime of $\\theta$. The probability density function is given by $f(x; \\theta) = \\frac{1}{\\theta} \\exp(-x/\\theta)$ for $x > 0$.\n\nA random sample of $n$ new components, with lifetimes $X_1, X_2, \\dots, X_n$, is put on a life test. To obtain a quick estimate of the mean lifetime $\\theta$, an engineer proposes an initial estimator, $W$, based on the time of the first failure, which is denoted by the first order statistic $X_{(1)} = \\min(X_1, X_2, \\dots, X_n)$. The specific estimator is $W = nX_{(1)}$. It is known that $W$ is an unbiased estimator for $\\theta$.\n\nTo obtain a more precise estimate, the total observed lifetime for all components, $T = \\sum_{i=1}^n X_i$, is also calculated. Your task is to find an improved estimator for $\\theta$, which we will call $W^*$, by conditioning the initial estimator $W$ on the total lifetime $T$.\n\nDetermine the expression for the improved estimator $W^* = E[W | T]$. Express your answer in terms of $T$ and $n$.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. with density $f(x;\\theta)=\\frac{1}{\\theta}\\exp(-x/\\theta)$ for $x>0$. The joint density of $(X_{1},\\dots,X_{n})$ is\n$$\nf(x_{1},\\dots,x_{n};\\theta)=\\theta^{-n}\\exp\\!\\left(-\\frac{1}{\\theta}\\sum_{i=1}^{n}x_{i}\\right),\\quad x_{i}>0.\n$$\nLet $T=\\sum_{i=1}^{n}X_{i}$. Conditioning on $T=t$ fixes $\\sum x_{i}=t$, and the joint density on the simplex $\\{x_{i}\\ge 0,\\ \\sum x_{i}=t\\}$ becomes constant (since the factor $\\exp(-t/\\theta)$ is constant given $T=t$). Therefore, conditional on $T=t$, the vector of proportions $U_{i}=X_{i}/t$ is uniform on the simplex $\\{u_{i}\\ge 0,\\ \\sum u_{i}=1\\}$, i.e., $(U_{1},\\dots,U_{n})\\sim\\text{Dirichlet}(1,\\dots,1)$, and\n$$\nX_{(1)}=t\\,U_{(1)},\\quad U_{(1)}=\\min(U_{1},\\dots,U_{n}).\n$$\nWe seek $W^{*}=E[W\\mid T]=n\\,E[X_{(1)}\\mid T]=n\\,E[X_{(1)}\\mid T=t]$ with $t$ replaced by $T$ at the end. Thus we need $E[X_{(1)}\\mid T=t]=t\\,E[U_{(1)}]$.\n\nTo compute $E[U_{(1)}]$ for the uniform simplex, use the survival function. For $u\\in[0,1/n]$, the event $\\{U_{(1)}>u\\}$ is equivalent to $U_{i}\\ge u$ for all $i$, which, after the change of variables $V_{i}=U_{i}-u$, becomes the simplex $\\{V_{i}\\ge 0,\\ \\sum V_{i}=1-nu\\}$. The volume (and hence probability under the uniform distribution) scales as $(1-nu)^{n-1}$. For $u>1/n$, the event is empty. Therefore,\n$$\nP(U_{(1)}>u)=\n\\begin{cases}\n(1-nu)^{n-1}, & 0\\le u\\le \\frac{1}{n},\\\\\n0, & u> \\frac{1}{n}.\n\\end{cases}\n$$\nHence\n$$\nE[U_{(1)}]=\\int_{0}^{1}P(U_{(1)}>u)\\,du=\\int_{0}^{1/n}(1-nu)^{n-1}\\,du.\n$$\nWith $y=1-nu$, $du=-\\frac{1}{n}dy$, the bounds $u=0\\mapsto y=1$ and $u=1/n\\mapsto y=0$ give\n$$\nE[U_{(1)}]=\\frac{1}{n}\\int_{0}^{1}y^{n-1}\\,dy=\\frac{1}{n}\\cdot\\frac{1}{n}=\\frac{1}{n^{2}}.\n$$\nTherefore,\n$$\nE[X_{(1)}\\mid T=t]=t\\cdot\\frac{1}{n^{2}},\\quad\\text{so}\\quad W^{*}=n\\,E[X_{(1)}\\mid T]=n\\cdot\\frac{T}{n^{2}}=\\frac{T}{n}.\n$$\nThus the improved estimator is the sample mean $T/n$.", "answer": "$$\\boxed{\\frac{T}{n}}$$", "id": "1922392"}, {"introduction": "In many statistical applications, we are interested in estimating a function of a parameter rather than the parameter itself. This exercise tackles such a scenario, where the goal is to estimate $\\lambda^2$ for a Poisson distribution. You will start with an unbiased estimator $T = X_1(X_1 - 1)$ and improve it by conditioning on the sufficient statistic $S = \\sum X_i$, demonstrating a powerful technique that leverages the properties of conditional distributions [@problem_id:1922440].", "problem": "Let $X_1, X_2, \\ldots, X_n$ be an independent and identically distributed random sample of size $n > 1$ from a Poisson distribution with an unknown mean $\\lambda > 0$. We are interested in estimating the parameter $\\theta = \\lambda^2$.\n\nIt can be shown that the statistic $T = X_1(X_1 - 1)$ is an unbiased estimator for $\\theta$. However, this estimator is not optimal as it is based on only a single observation, $X_1$. To construct a more statistically efficient estimator, one can compute the conditional expectation of $T$ given the sufficient statistic for $\\lambda$. For a Poisson sample, the sufficient statistic is the sum of the observations, $S = \\sum_{i=1}^n X_i$.\n\nYour task is to calculate the improved estimator $T'$ defined as the conditional expectation of $T$ given $S$.\n$$T' = E[T | S]$$\nExpress your final answer as an analytic function of the sufficient statistic $S$ and the sample size $n$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be iid $\\operatorname{Poisson}(\\lambda)$ and $S=\\sum_{i=1}^{n}X_{i}$ the sufficient statistic for $\\lambda$. Define $T=X_{1}(X_{1}-1)$. The Rao-Blackwell improvement is $T'=\\mathbb{E}[T\\mid S]$.\n\nFor iid Poisson variables, conditional on $S=s$, the vector $(X_{1},\\ldots,X_{n})$ has a multinomial distribution with $s$ trials and equal cell probabilities $1/n$. In particular,\n$$\nX_{1}\\mid S=s \\sim \\operatorname{Binomial}\\!\\left(s,\\frac{1}{n}\\right).\n$$\nUsing the identity $\\mathbb{E}[X(X-1)]=\\operatorname{Var}(X)+\\{\\mathbb{E}[X]\\}^{2}-\\mathbb{E}[X]$, applied conditionally on $S=s$, we compute\n$$\n\\mathbb{E}\\!\\left[X_{1}(X_{1}-1)\\mid S=s\\right]\n=\\operatorname{Var}(X_{1}\\mid S=s)+\\left(\\mathbb{E}[X_{1}\\mid S=s]\\right)^{2}-\\mathbb{E}[X_{1}\\mid S=s].\n$$\nFor $X_{1}\\mid S=s\\sim \\operatorname{Binomial}(s,1/n)$, we have\n$$\n\\mathbb{E}[X_{1}\\mid S=s]=\\frac{s}{n},\\qquad \\operatorname{Var}(X_{1}\\mid S=s]=\\frac{s}{n}\\left(1-\\frac{1}{n}\\right)=\\frac{s(n-1)}{n^{2}}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[X_{1}(X_{1}-1)\\mid S=s\\right]\n=\\frac{s(n-1)}{n^{2}}+\\left(\\frac{s}{n}\\right)^{2}-\\frac{s}{n}\n=\\frac{s^{2}-s}{n^{2}}\n=\\frac{s(s-1)}{n^{2}}.\n$$\nSince this holds for every $s$, the improved estimator as a function of $S$ is\n$$\nT'=\\mathbb{E}[T\\mid S]=\\frac{S(S-1)}{n^{2}}.\n$$\nAs a check of unbiasedness, note that $S\\sim \\operatorname{Poisson}(n\\lambda)$, so $\\mathbb{E}[S(S-1)]=(n\\lambda)^{2}$ and hence $\\mathbb{E}[T']=\\lambda^{2}$, agreeing with the target $\\theta$.", "answer": "$$\\boxed{\\frac{S(S-1)}{n^{2}}}$$", "id": "1922440"}, {"introduction": "This problem expands our toolkit by exploring a case where the sufficient statistic is not the sum of the observations. For a uniform distribution $U(0, \\theta)$, the sample maximum $X_{(n)}$ holds all the necessary information about the unknown upper bound $\\theta$. This practice will guide you through the process of improving an estimator based on the sample minimum by conditioning on the sample maximum, highlighting the versatility of the Rao-Blackwell theorem [@problem_id:1922455].", "problem": "Let $X_1, \\dots, X_n$ be a random sample of size $n > 1$ from a continuous uniform distribution on the interval $(0, \\theta)$, where $\\theta > 0$ is an unknown parameter. The probability density function for this distribution is $f(x) = \\frac{1}{\\theta}$ for $0 < x < \\theta$, and $f(x)=0$ otherwise.\n\nLet $X_{(1)} = \\min(X_1, \\dots, X_n)$ be the sample minimum and $X_{(n)} = \\max(X_1, \\dots, X_n)$ be the sample maximum.\n\nAn initial, though suboptimal, unbiased estimator for $\\theta$ is given by $T = (n+1)X_{(1)}$. By leveraging the information contained in the sample maximum $X_{(n)}$, a new estimator, $T'$, can be constructed that is also unbiased for $\\theta$ but has a smaller variance.\n\nFind the expression for this improved estimator $T'$. Your answer should be a function of the sample size $n$ and the sample maximum $X_{(n)}$.", "solution": "We begin with the joint density of the sample from the Uniform distribution on $(0,\\theta)$:\n$$\nf(x_{1},\\dots,x_{n};\\theta)=\\theta^{-n}\\,\\mathbb{I}\\{0<x_{(1)}<x_{(n)}<\\theta\\},\n$$\nwhich factors as $h(x_{1},\\dots,x_{n})\\,g_{\\theta}(x_{(n)})$ with $g_{\\theta}(x_{(n)})=\\theta^{-n}\\,\\mathbb{I}\\{x_{(n)}<\\theta\\}$. By the factorization theorem, $X_{(n)}$ is a sufficient statistic for $\\theta$.\n\nGiven any unbiased estimator $T$, the Rao-Blackwell theorem states that $T'=\\mathbb{E}[T\\mid X_{(n)}]$ is also unbiased for $\\theta$ and has variance no larger than that of $T$. Here, $T=(n+1)X_{(1)}$ is an unbiased estimator of $\\theta$, so we set\n$$\nT'=\\mathbb{E}[(n+1)X_{(1)}\\mid X_{(n)}].\n$$\n\nTo compute this conditional expectation, fix $t\\in(0,\\theta)$ and condition on $X_{(n)}=t$. Given $X_{(n)}=t$, the remaining $n-1$ observations behave as if they were drawn i.i.d. from $\\text{Uniform}(0,t)$, and $X_{(1)}$ is the minimum of these $n-1$ values. The conditional density of $X_{(1)}$ given $X_{(n)}=t$ is therefore\n$$\nf_{X_{(1)}\\mid X_{(n)}=t}(y)=\\frac{n-1}{t}\\left(1-\\frac{y}{t}\\right)^{n-2},\\qquad 0<y<t.\n$$\nThus,\n$$\n\\mathbb{E}[X_{(1)}\\mid X_{(n)}=t]\n=\\int_{0}^{t}y\\,\\frac{n-1}{t}\\left(1-\\frac{y}{t}\\right)^{n-2}\\,dy.\n$$\nWith the substitution $u=y/t$, $dy=t\\,du$, this becomes\n$$\n\\mathbb{E}[X_{(1)}\\mid X_{(n)}=t]\n=(n-1)t\\int_{0}^{1}u(1-u)^{n-2}\\,du\n=(n-1)t\\,\\mathrm{B}(2,n-1),\n$$\nwhere $\\mathrm{B}(a,b)=\\int_{0}^{1}u^{a-1}(1-u)^{b-1}\\,du=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$. Hence,\n$$\n\\mathbb{E}[X_{(1)}\\mid X_{(n)}=t]\n=(n-1)t\\,\\frac{\\Gamma(2)\\Gamma(n-1)}{\\Gamma(n+1)}\n=(n-1)t\\,\\frac{1!\\,(n-2)!}{n!}\n=\\frac{t}{n}.\n$$\nTherefore,\n$$\nT'=(n+1)\\,\\mathbb{E}[X_{(1)}\\mid X_{(n)}]\n=(n+1)\\,\\frac{X_{(n)}}{n}\n=\\frac{n+1}{n}\\,X_{(n)}.\n$$\nUnbiasedness can be verified directly: the density of $X_{(n)}$ is $f_{X_{(n)}}(t)=\\frac{n}{\\theta^{n}}t^{n-1}$ for $0<t<\\theta$, so\n$$\n\\mathbb{E}[X_{(n)}]=\\int_{0}^{\\theta}t\\,\\frac{n}{\\theta^{n}}t^{n-1}\\,dt=\\frac{n}{n+1}\\,\\theta,\n$$\nand thus\n$$\n\\mathbb{E}[T']=\\frac{n+1}{n}\\,\\mathbb{E}[X_{(n)}]=\\frac{n+1}{n}\\cdot\\frac{n}{n+1}\\,\\theta=\\theta.\n$$\nBy Rao-Blackwell, $\\mathrm{Var}(T')\\le \\mathrm{Var}(T)$, and $T'$ uses the information in the sufficient statistic $X_{(n)}$ as required. Hence the improved estimator is\n$$\nT'=\\frac{n+1}{n}\\,X_{(n)}.\n$$", "answer": "$$\\boxed{\\frac{n+1}{n}\\,X_{(n)}}$$", "id": "1922455"}]}