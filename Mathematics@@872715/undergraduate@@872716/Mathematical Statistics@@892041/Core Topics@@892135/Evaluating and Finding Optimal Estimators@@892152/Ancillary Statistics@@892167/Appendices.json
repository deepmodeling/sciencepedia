{"hands_on_practices": [{"introduction": "Many statistical models involve a \"location parameter,\" which often represents an unknown systematic offset or bias. Intuitively, while the absolute values of our measurements depend on this offset, their internal configuration or spread should not. This exercise [@problem_id:1895670] provides a hands-on opportunity to formalize this intuition by showing that the sample range—the difference between the maximum and minimum observations—has a distribution that is completely independent of the location parameter $\\theta$ in a uniform distribution.", "problem": "A manufacturer of environmental sensors is testing a new device that measures atmospheric particle concentration. The device outputs an integer value. Due to a fixed but unknown systematic calibration error, the true concentration is shifted by an unknown integer offset, $\\theta$. For any given measurement, the sensor's output, $X$, is a discrete random variable uniformly distributed on the set of consecutive integers $S = \\{\\theta+1, \\theta+2, \\dots, \\theta+10\\}$. While the value of $\\theta$ is unknown, quality control has established that the sensor will always output one of these 10 specific integer values for a stable environment.\n\nAn engineer takes a random sample of three independent measurements, $X_1, X_2, X_3$, under stable environmental conditions. Let $M$ be the maximum value observed in the sample, and let $m$ be the minimum value observed. The engineer is interested in the sample range, defined as $R = M-m$.\n\nCalculate the expected value of this sample range, $E[R]$. Express your answer as an exact fraction in simplest form.", "solution": "Let $X_{1},X_{2},X_{3}$ be i.i.d. uniform on $S=\\{\\theta+1,\\dots,\\theta+10\\}$. Since adding a constant does not change a range, define $Y_{i}=X_{i}-\\theta$, which are i.i.d. uniform on $\\{1,\\dots,10\\}$. Then $R=M-m$ is unchanged, where $M=\\max(Y_{1},Y_{2},Y_{3})$ and $m=\\min(Y_{1},Y_{2},Y_{3})$.\n\nWe use linearity: $E[R]=E[M]-E[m]$. For i.i.d. discrete samples, by independence,\n$$\nP(M\\leq k)=\\left(\\frac{k}{10}\\right)^{3}\\quad\\text{for }k\\in\\{1,\\dots,10\\},\n$$\nso by the tail-sum formula for expectations of integer-valued variables,\n$$\nE[M]=\\sum_{k=1}^{10}P(M\\geq k)=\\sum_{k=1}^{10}\\left[1-\\left(\\frac{k-1}{10}\\right)^{3}\\right]\n=10-\\frac{1}{10^{3}}\\sum_{k=0}^{9}k^{3}.\n$$\nSimilarly,\n$$\nP(m\\geq k)=\\left(\\frac{10-k+1}{10}\\right)^{3},\\quad\nE[m]=\\sum_{k=1}^{10}P(m\\geq k)=\\frac{1}{10^{3}}\\sum_{j=1}^{10}j^{3},\n$$\nwhere we substituted $j=10-k+1$.\n\nUsing the identity $\\sum_{j=1}^{m}j^{3}=\\left(\\frac{m(m+1)}{2}\\right)^{2}$, we have\n$$\n\\sum_{j=1}^{10}j^{3}=\\left(\\frac{10\\cdot 11}{2}\\right)^{2}=55^{2}=3025,\\quad\n\\sum_{k=0}^{9}k^{3}=\\sum_{j=1}^{9}j^{3}=\\left(\\frac{9\\cdot 10}{2}\\right)^{2}=45^{2}=2025.\n$$\nTherefore,\n$$\nE[m]=\\frac{3025}{1000}=\\frac{121}{40},\\qquad\nE[M]=10-\\frac{2025}{1000}=10-\\frac{81}{40}=\\frac{319}{40}.\n$$\nHence,\n$$\nE[R]=E[M]-E[m]=\\frac{319}{40}-\\frac{121}{40}=\\frac{198}{40}=\\frac{99}{20}.\n$$", "answer": "$$\\boxed{\\frac{99}{20}}$$", "id": "1895670"}, {"introduction": "In contrast to location parameters that shift data, \"scale parameters\" stretch or shrink it, affecting the magnitude of observations. This concept is common in physical sciences where an unknown sensor range or signal strength scales the measurements. In this problem [@problem_id:1895664], we explore a geometric model where you will discover that even when the radial scale $\\theta$ is unknown, the angular coordinate $\\Phi$ provides information that is completely free of $\\theta$, making it a classic example of an ancillary statistic.", "problem": "A new type of short-range sensor is being tested. The sensor is designed to scan a fixed angular sector of width $\\alpha$ (in radians), where $0  \\alpha \\le 2\\pi$ is a known constant. Due to manufacturing variations, the maximum effective range of any given sensor unit, denoted by the parameter $\\theta  0$, is a positive but unknown constant. It is known from the sensor's design that when it detects a single object, the object's location is a random point drawn from a uniform distribution over the entire circular sector that the sensor can scan.\n\nSuppose a single object is detected. Its position is recorded in polar coordinates $(R, \\Phi)$, where $R$ is the radial distance from the sensor and $\\Phi$ is the angle relative to the start of the sector's sweep, with $0 \\le R \\le \\theta$ and $0 \\le \\Phi \\le \\alpha$. For this statistical model, the parameter of interest is the unknown maximum range $\\theta$. A statistic is a function of the observed data $(R, \\Phi)$ that does not itself depend on any unknown parameters. A statistic is said to be ancillary for the parameter $\\theta$ if its probability distribution is entirely independent of $\\theta$.\n\nWhich of the following statistics is ancillary for the parameter $\\theta$?\n\nA. $R$ (The radial coordinate)\n\nB. $\\Phi$ (The angular coordinate)\n\nC. $R^2$ (The squared radial coordinate)\n\nD. $R+\\Phi$\n\nE. $R\\Phi$", "solution": "The object is uniformly distributed over the circular sector with angle width $\\alpha$ and radius $\\theta$. The area of this sector is\n$$\n\\text{Area}=\\frac{1}{2}\\alpha \\theta^{2}.\n$$\nA uniform distribution over this region in Cartesian coordinates has constant density with respect to area measure equal to\n$$\nc=\\frac{1}{\\text{Area}}=\\frac{2}{\\alpha \\theta^{2}}.\n$$\nIn polar coordinates, the Jacobian of the transformation is $r$, so the joint density of $(R,\\Phi)$ with respect to $dr\\,d\\phi$ is\n$$\nf_{R,\\Phi}(r,\\phi\\mid \\theta)=c\\,r=\\frac{2r}{\\alpha \\theta^{2}}, \\quad 0\\le r\\le \\theta,\\; 0\\le \\phi\\le \\alpha.\n$$\nTo test ancillarity, compute the marginal distributions and check dependence on $\\theta$.\n\nFor $\\Phi$, integrate out $r$:\n$$\nf_{\\Phi}(\\phi\\mid \\theta)=\\int_{0}^{\\theta} \\frac{2r}{\\alpha \\theta^{2}}\\,dr=\\frac{2}{\\alpha \\theta^{2}}\\cdot \\frac{\\theta^{2}}{2}=\\frac{1}{\\alpha}, \\quad 0\\le \\phi\\le \\alpha.\n$$\nThis is uniform on $[0,\\alpha]$ and does not depend on $\\theta$, so $\\Phi$ is ancillary.\n\nFor $R$, integrate out $\\phi$:\n$$\nf_{R}(r\\mid \\theta)=\\int_{0}^{\\alpha} \\frac{2r}{\\alpha \\theta^{2}}\\,d\\phi=\\frac{2r}{\\theta^{2}}, \\quad 0\\le r\\le \\theta,\n$$\nwhich depends on $\\theta$ through both its form and support, so $R$ is not ancillary.\n\nFor $R^{2}$, let $T=R^{2}$ with $0\\le T\\le \\theta^{2}$. Using the change of variables $r=\\sqrt{t}$ and $\\frac{dr}{dt}=\\frac{1}{2\\sqrt{t}}$, the density is\n$$\nf_{T}(t\\mid \\theta)=f_{R}(\\sqrt{t}\\mid \\theta)\\cdot \\frac{1}{2\\sqrt{t}}=\\frac{2\\sqrt{t}}{\\theta^{2}}\\cdot \\frac{1}{2\\sqrt{t}}=\\frac{1}{\\theta^{2}}, \\quad 0\\le t\\le \\theta^{2}.\n$$\nAlthough constant on its support, it depends on $\\theta$ via the support $[0,\\theta^{2}]$, hence $R^{2}$ is not ancillary.\n\nFor $R+\\Phi$ and $R\\Phi$, since $\\Phi$ is independent of $R$ and $R$ scales with $\\theta$ (specifically, $R/\\theta$ has a fixed distribution on $[0,1]$), any statistic involving $R$ in a non-scale-invariant way will have a distribution that changes with $\\theta$. Therefore both $R+\\Phi$ and $R\\Phi$ are not ancillary.\n\nConsequently, among the given options, only $\\Phi$ is ancillary for $\\theta$.", "answer": "$$\\boxed{B}$$", "id": "1895664"}, {"introduction": "The concept of ancillarity extends beyond simple location and scale families, sometimes appearing in surprising contexts. This exercise [@problem_id:1895652] challenges you to go back to the fundamental definition of an ancillary statistic within a more complex model—a mixture of two distributions. By constructing a simple binary statistic and calculating its probability distribution, you will see how a statistic's distribution can remain constant even when the underlying parameter $\\theta$ changes, demonstrating the power and subtlety of this statistical principle.", "problem": "In statistical inference, a statistic is a function of the observable random variables in a sample. A statistic $T$ is said to be ancillary for a parameter $\\theta$ if the probability distribution of $T$ does not depend on the value of $\\theta$.\n\nConsider a single observation $X$ drawn from a probability distribution that is a mixture of two continuous uniform distributions. Specifically, with probability $0.5$, $X$ is drawn from a uniform distribution on the interval $(0, \\theta)$, and with probability $0.5$, $X$ is drawn from a uniform distribution on the interval $(0, 2\\theta)$. The parameter $\\theta$ is an unknown positive real number, so $\\theta  0$. Let $U(a, b)$ denote the continuous uniform distribution on the interval $(a, b)$. The probability density function of $X$ is thus given by $f(x|\\theta) = 0.5 \\cdot \\frac{1}{\\theta}I(0  x  \\theta) + 0.5 \\cdot \\frac{1}{2\\theta} I(0  x  2\\theta)$, where $I(\\cdot)$ is the indicator function.\n\nLet's define a statistic $S$ based on the observation $X$ as $S = I(X  \\theta)$. This statistic takes the value $1$ if the observation $X$ is greater than $\\theta$, and $0$ otherwise.\n\nIs the statistic $S$ ancillary for the parameter $\\theta$?\n\nA. Yes, the statistic is ancillary for $\\theta$.\n\nB. No, the statistic is not ancillary for $\\theta$ because its distribution depends on $\\theta$.\n\nC. The statistic is ancillary for $\\theta$ only if $\\theta$ is an integer.\n\nD. There is not enough information to determine if the statistic is ancillary.", "solution": "We need to determine whether the distribution of $S=I(X\\theta)$ depends on $\\theta$. By definition, $S=1$ if and only if $X\\theta$. Using the law of total probability over the mixture components,\n$$\n\\Pr(S=1\\mid \\theta)=\\Pr(X\\theta\\mid \\theta)=\\tfrac{1}{2}\\Pr\\bigl(X\\theta\\mid X\\sim U(0,\\theta)\\bigr)+\\tfrac{1}{2}\\Pr\\bigl(X\\theta\\mid X\\sim U(0,2\\theta)\\bigr).\n$$\nFor $X\\sim U(0,\\theta)$, the support is $(0,\\theta)$, so\n$$\n\\Pr\\bigl(X\\theta\\mid X\\sim U(0,\\theta)\\bigr)=0.\n$$\nFor $X\\sim U(0,2\\theta)$, with density $1/(2\\theta)$ on $(0,2\\theta)$,\n$$\n\\Pr\\bigl(X\\theta\\mid X\\sim U(0,2\\theta)\\bigr)=\\int_{\\theta}^{2\\theta}\\frac{1}{2\\theta}\\,dx=\\frac{2\\theta-\\theta}{2\\theta}=\\frac{1}{2}.\n$$\nTherefore,\n$$\n\\Pr(S=1\\mid \\theta)=\\tfrac{1}{2}\\cdot 0+\\tfrac{1}{2}\\cdot \\frac{1}{2}=\\frac{1}{4},\n$$\nwhich does not depend on $\\theta$. Hence $S$ has a Bernoulli distribution with parameter $1/4$ independent of $\\theta$, so $S$ is ancillary for $\\theta$. The correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1895652"}]}