{"hands_on_practices": [{"introduction": "The principle of sufficiency allows us to condense a dataset into a single statistic without losing information about the parameter of interest. This first practice provides a foundational exercise using the geometric distribution, a common model for discrete trials until success. By applying the Neyman-Fisher Factorization Theorem, we can identify a sufficient statistic for the success probability $p$, demonstrating the core technique of data reduction in a clear, discrete setting. [@problem_id:1957623]", "problem": "In a wireless communication system, a data packet is sent from a transmitter to a receiver. Due to noise in the channel, a sent packet may be corrupted. The system employs an Automatic Repeat reQuest (ARQ) protocol, where the transmitter repeatedly sends the packet until it receives a successful acknowledgment. Assume that each transmission attempt is an independent trial with a constant probability of success $p$, where $0 < p < 1$.\n\nLet a random variable $X$ represent the number of failed transmission attempts before the first successful transmission. Thus, the probability mass function of $X$ is given by $P(X=k) = (1-p)^k p$ for $k = 0, 1, 2, \\dots$.\n\nAn engineer collects data from $n$ independent successful packet transmissions to estimate the channel's success probability $p$. Let $X_1, X_2, \\dots, X_n$ be a random sample, where each $X_i$ represents the number of failures for the $i$-th successful transmission.\n\nWhich of the following statistics is a sufficient statistic for the parameter $p$?\n\nA. $\\frac{1}{n} \\sum_{i=1}^n X_i^2$\n\nB. $\\prod_{i=1}^n X_i$\n\nC. $\\sum_{i=1}^n X_i$\n\nD. $\\max(X_1, X_2, \\dots, X_n)$\n\nE. $X_1$", "solution": "Each $X_{i}$ is geometric with support $\\{0,1,2,\\dots\\}$ and pmf $f_{X}(x_{i};p)=p(1-p)^{x_{i}}$, where $0<p<1$. For a random sample $X_{1},\\dots,X_{n}$, the joint pmf is\n$$\nf(x_{1},\\dots,x_{n};p)=\\prod_{i=1}^{n}p(1-p)^{x_{i}}=p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}}.\n$$\nBy the Neyman–Fisher factorization theorem, a statistic $T(X_{1},\\dots,X_{n})$ is sufficient for $p$ if the joint pmf can be written as\n$$\nf(x_{1},\\dots,x_{n};p)=g\\big(T(x_{1},\\dots,x_{n});p\\big)\\,h(x_{1},\\dots,x_{n}),\n$$\nwhere $g$ depends on the data only through $T$ and $h$ does not depend on $p$. From the displayed factorization,\n$$\nf(x_{1},\\dots,x_{n};p)=\\underbrace{p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}}}_{g\\big(T;p\\big)}\\cdot\\underbrace{1}_{h(x)},\n$$\nwith $T=\\sum_{i=1}^{n}X_{i}$. Therefore, $T=\\sum_{i=1}^{n}X_{i}$ is sufficient for $p$.\n\nAmong the given options, only option C equals $\\sum_{i=1}^{n}X_{i}$. The other options are not functions of $T$ and are not sufficient for this one-parameter geometric family, whose minimal sufficient statistic is $\\sum_{i=1}^{n}X_{i}$.", "answer": "$$\\boxed{C}$$", "id": "1957623"}, {"introduction": "Moving from discrete events to continuous measurements, this practice explores sufficiency in the context of the Weibull distribution, a cornerstone of reliability engineering and survival analysis. This exercise challenges the intuition that the sample mean is always the key summary by showing how the model's structure dictates which function of the data is truly informative. You will use the factorization theorem to discover that the sum of squares captures all the necessary information about the scale parameter $\\lambda$. [@problem_id:1957586]", "problem": "Consider a random sample of $n$ electronic components, with their lifetimes denoted by the independent and identically distributed random variables $X_1, X_2, \\ldots, X_n$. The lifetime of each component is modeled by a Weibull distribution with a known shape parameter $k=2$ and an unknown scale parameter $\\lambda > 0$. The Probability Density Function (PDF) for a single observation $X_i=x$ is given by:\n$$f(x; \\lambda) = \\frac{2x}{\\lambda^2} \\exp\\left(-\\frac{x^2}{\\lambda^2}\\right), \\quad \\text{for } x > 0$$\nWhich of the following is a sufficient statistic for the parameter $\\lambda$?\n\nA. $\\sum_{i=1}^{n} X_i$\n\nB. $\\left( \\sum_{i=1}^{n} X_i \\right)^2$\n\nC. $\\sum_{i=1}^{n} X_i^2$\n\nD. $\\prod_{i=1}^{n} X_i$\n\nE. $\\max(X_1, X_2, \\ldots, X_n)$", "solution": "We are given independent and identically distributed random variables $X_{1},\\ldots,X_{n}$ with common density\n$$\nf(x;\\lambda)=\\frac{2x}{\\lambda^{2}}\\exp\\!\\left(-\\frac{x^{2}}{\\lambda^{2}}\\right), \\quad x>0,\n$$\nwhere the shape parameter is known to be $k=2$ and the scale parameter $\\lambda>0$ is unknown.\n\nBy independence, the joint density of the sample $\\boldsymbol{X}=(X_{1},\\ldots,X_{n})$ is\n$$\nL(\\lambda;\\boldsymbol{x})=\\prod_{i=1}^{n}f(x_{i};\\lambda)=\\prod_{i=1}^{n}\\left(\\frac{2x_{i}}{\\lambda^{2}}\\exp\\!\\left(-\\frac{x_{i}^{2}}{\\lambda^{2}}\\right)\\right), \\quad x_{i}>0.\n$$\nAlgebraically factor this product:\n$$\nL(\\lambda;\\boldsymbol{x})=\\left(\\prod_{i=1}^{n}\\frac{2x_{i}}{\\lambda^{2}}\\right)\\left(\\prod_{i=1}^{n}\\exp\\!\\left(-\\frac{x_{i}^{2}}{\\lambda^{2}}\\right)\\right)\n=2^{n}\\left(\\prod_{i=1}^{n}x_{i}\\right)\\lambda^{-2n}\\exp\\!\\left(-\\frac{1}{\\lambda^{2}}\\sum_{i=1}^{n}x_{i}^{2}\\right),\n$$\nvalid for $x_{i}>0$, a support that does not depend on $\\lambda$.\n\nBy the Neyman–Fisher factorization theorem, a statistic $T(\\boldsymbol{X})$ is sufficient for $\\lambda$ if the joint density can be written as\n$$\nL(\\lambda;\\boldsymbol{x})=g(T(\\boldsymbol{x}),\\lambda)\\,h(\\boldsymbol{x}),\n$$\nwith $h$ not depending on $\\lambda$. From the displayed factorization, choose\n$$\nT(\\boldsymbol{x})=\\sum_{i=1}^{n}x_{i}^{2}, \\quad g(T,\\lambda)=\\lambda^{-2n}\\exp\\!\\left(-\\frac{T}{\\lambda^{2}}\\right), \\quad h(\\boldsymbol{x})=2^{n}\\prod_{i=1}^{n}x_{i},\n$$\nwhich exhibits the required factorization. Therefore, $T(\\boldsymbol{X})=\\sum_{i=1}^{n}X_{i}^{2}$ is sufficient for $\\lambda$.\n\nAmong the given options, this corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1957586"}, {"introduction": "Our final practice tackles a different and important class of statistical models: those where the parameters define the boundaries of the data's possible values. Using the two-parameter uniform distribution, this problem reveals that order statistics—the minimum and maximum values—can become the crucial summary of the data. This exercise demonstrates why the pair $(X_{(1)}, X_{(n)})$ jointly forms a sufficient statistic for the interval's endpoints $(\\alpha, \\beta)$, a significant departure from the sum-based statistics seen in many exponential-family distributions. [@problem_id:1963665]", "problem": "In the field of digital signal processing, an analog-to-digital converter is sampling a continuous voltage signal. The signal's voltage level is known to be uniformly distributed over an unknown range $[\\alpha, \\beta]$, where $\\alpha$ and $\\beta$ are the minimum and maximum possible voltages, respectively ($\\alpha < \\beta$). To calibrate the system, a set of $n$ independent voltage measurements, denoted by $X_1, X_2, \\ldots, X_n$, are taken.\n\nA central goal in statistical inference is to find a *sufficient statistic*, which is a function of the data that captures all the relevant information about the unknown parameters. In this case, we are interested in a joint sufficient statistic for the parameter pair $(\\alpha, \\beta)$.\n\nLet $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$ be the minimum observed voltage and $X_{(n)} = \\max(X_1, X_2, \\ldots, X_n)$ be the maximum observed voltage in the sample. Let $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ be the sample mean and $S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$ be the sample variance.\n\nWhich of the following represents a joint sufficient statistic for the parameters $(\\alpha, \\beta)$?\n\nA. $(X_{(1)}, X_{(n)})$\n\nB. $(\\bar{X}, S^2)$\n\nC. $\\left(\\sum_{i=1}^n X_i, \\sum_{i=1}^n X_i^2\\right)$\n\nD. $X_{(n)}$\n\nE. The sample median", "solution": "Assume $X_{1},\\ldots,X_{n}$ are independent and identically distributed from the $\\text{Uniform}[\\alpha,\\beta]$ model, with density\n$$\nf(x\\mid \\alpha,\\beta)=\\frac{1}{\\beta-\\alpha}\\,\\mathbf{1}_{[\\alpha,\\beta]}(x),\n$$\nwhere $\\mathbf{1}_{[\\alpha,\\beta]}(x)$ is the indicator of $x\\in[\\alpha,\\beta]$.\n\nBy independence, the joint density (likelihood as a function of $(\\alpha,\\beta)$ given data $\\boldsymbol{x}=(x_{1},\\ldots,x_{n})$) is\n$$\nL(\\alpha,\\beta;\\boldsymbol{x})=\\prod_{i=1}^{n} f(x_{i}\\mid \\alpha,\\beta)\n=\\left(\\frac{1}{\\beta-\\alpha}\\right)^{n}\\prod_{i=1}^{n}\\mathbf{1}_{[\\alpha,\\beta]}(x_{i}).\n$$\nThe product of indicators equals the indicator that all observations lie in $[\\alpha,\\beta]$, which is equivalent to requiring $\\alpha\\le x_{(1)}$ and $x_{(n)}\\le \\beta$, where $x_{(1)}=\\min_{i}x_{i}$ and $x_{(n)}=\\max_{i}x_{i}$. Hence\n$$\nL(\\alpha,\\beta;\\boldsymbol{x})=\\left(\\frac{1}{\\beta-\\alpha}\\right)^{n}\\mathbf{1}_{\\{\\alpha\\le x_{(1)}\\}}\\mathbf{1}_{\\{x_{(n)}\\le \\beta\\}}\n=\\left(\\frac{1}{\\beta-\\alpha}\\right)^{n}\\mathbf{1}_{\\{\\alpha\\le x_{(1)}\\le x_{(n)}\\le \\beta\\}}.\n$$\nThis factors as\n$$\nL(\\alpha,\\beta;\\boldsymbol{x})=h(\\boldsymbol{x})\\,q_{\\alpha,\\beta}\\!\\left(x_{(1)},x_{(n)}\\right),\n$$\nwith $h(\\boldsymbol{x})=1$ and $q_{\\alpha,\\beta}(t_{1},t_{2})=(\\beta-\\alpha)^{-n}\\mathbf{1}_{\\{\\alpha\\le t_{1}\\le t_{2}\\le \\beta\\}}$. By the Fisher–Neyman factorization theorem, $T(\\boldsymbol{X})=(X_{(1)},X_{(n)})$ is sufficient for $(\\alpha,\\beta)$.\n\nTherefore, option A is a joint sufficient statistic. The other options are not sufficient: the likelihood depends on the data only through $(X_{(1)},X_{(n)})$, so any sufficient statistic must determine these extremes. Neither $(\\bar{X},S^{2})$ nor $\\left(\\sum_{i=1}^{n}X_{i},\\sum_{i=1}^{n}X_{i}^{2}\\right)$ nor $X_{(n)}$ alone nor the sample median determines both $X_{(1)}$ and $X_{(n)}$ in general, so they are not sufficient.", "answer": "$$\\boxed{A}$$", "id": "1963665"}]}