{"hands_on_practices": [{"introduction": "This first exercise provides a foundational application of Fisher information to a discrete scenario. By calculating the information for the parameter of a geometric distribution—which models the number of trials until a first success—you will practice the core mechanics of finding the log-likelihood and its derivatives for a discrete random variable. This problem [@problem_id:1918253] builds the essential computational skills needed for more complex models.", "problem": "Consider a sequence of independent Bernoulli trials, where the probability of success on any given trial is a constant $p$, with $0  p  1$. Let the random variable $X$ denote the total number of trials required to obtain the first success. The probability mass function for $X$ is given by:\n$$P(X=k; p) = (1-p)^{k-1}p, \\quad \\text{for } k = 1, 2, 3, \\dots$$\nThis distribution is known as the geometric distribution. The Fisher information, $I(p)$, quantifies the amount of information that a single observation from the random variable $X$ carries about the unknown parameter $p$.\n\nCalculate the Fisher information $I(p)$ based on a single observation of $X$. Your answer should be an expression in terms of $p$.", "solution": "For a single observation $X=k$ from the geometric distribution with pmf $P(X=k;p)=(1-p)^{k-1}p$, the log-likelihood is\n$$\n\\ell(p;k)=\\ln p+(k-1)\\ln(1-p).\n$$\nDifferentiate to obtain the score and its derivative:\n$$\n\\frac{\\partial \\ell}{\\partial p}=\\frac{1}{p}-\\frac{k-1}{1-p}, \\qquad \\frac{\\partial^{2}\\ell}{\\partial p^{2}}=-\\frac{1}{p^{2}}-\\frac{k-1}{(1-p)^{2}}.\n$$\nUnder standard regularity conditions, the Fisher information for one observation is\n$$\nI(p)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial p^{2}}\\right]=\\frac{1}{p^{2}}+\\frac{\\mathbb{E}[X-1]}{(1-p)^{2}}.\n$$\nCompute $\\mathbb{E}[X-1]$ using $\\mathbb{E}[X]$. Since\n$$\n\\mathbb{E}[X]=\\sum_{k=1}^{\\infty}k(1-p)^{k-1}p=p\\sum_{k=1}^{\\infty}k r^{\\,k-1} \\quad \\text{with } r=1-p,\n$$\nand the series identity $\\sum_{k=1}^{\\infty}k r^{\\,k-1}=\\frac{1}{(1-r)^{2}}$ for $|r|1$ gives\n$$\n\\mathbb{E}[X]=p\\cdot \\frac{1}{(1-(1-p))^{2}}=\\frac{1}{p},\n$$\nhence\n$$\n\\mathbb{E}[X-1]=\\frac{1}{p}-1=\\frac{1-p}{p}.\n$$\nSubstitute into $I(p)$:\n$$\nI(p)=\\frac{1}{p^{2}}+\\frac{\\frac{1-p}{p}}{(1-p)^{2}}=\\frac{1}{p^{2}}+\\frac{1}{p(1-p)}=\\frac{1-p+p}{p^{2}(1-p)}=\\frac{1}{p^{2}(1-p)}.\n$$\nThus, the Fisher information in a single geometric observation is $I(p)=\\frac{1}{p^{2}(1-p)}$.", "answer": "$$\\boxed{\\frac{1}{p^{2}(1-p)}}$$", "id": "1918253"}, {"introduction": "Moving from discrete to continuous data, this practice explores the ubiquitous normal distribution. In many scientific applications, understanding the variability or precision of a measurement is as important as its average value. This exercise [@problem_id:1918246] focuses on calculating the Fisher information for the variance parameter, $\\theta = \\sigma^2$, demonstrating how to quantify the information about a scale parameter in a continuous measurement.", "problem": "An advanced materials science lab is developing a novel nanoscale sensor to measure temperature. The sensor's output is an electrical signal, represented by a random variable $X$. According to the sensor's design model, a single measurement $X$ follows a normal distribution with a mean $\\mu_0$ that is a known, precisely calibrated constant, and a variance $\\theta$. This variance $\\theta$ is directly proportional to the absolute temperature that the sensor is exposed to, making it the parameter of interest. To quantify the theoretical limit of precision for estimating this temperature-dependent variance, the research team needs to calculate the Fisher information for $\\theta$ contained in a single measurement $X$.\n\nAssume a single observation $X$ is drawn from a normal distribution with known mean $\\mu_0$ and unknown variance $\\theta = \\sigma^2$. Find the Fisher information, $I(\\theta)$, for the parameter $\\theta$.", "solution": "We model a single observation $X$ under the normal density with known mean $\\mu_{0}$ and unknown variance $\\theta$:\n$$\nf(x;\\theta)=\\frac{1}{\\sqrt{2\\pi\\theta}}\\exp\\!\\left(-\\frac{(x-\\mu_{0})^{2}}{2\\theta}\\right).\n$$\nThe log-likelihood for a single observation $x$ is\n$$\n\\ell(\\theta;x)=\\ln f(x;\\theta)=-\\frac{1}{2}\\ln(2\\pi)-\\frac{1}{2}\\ln\\theta-\\frac{(x-\\mu_{0})^{2}}{2\\theta}.\n$$\nDifferentiate with respect to $\\theta$ to obtain the score:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta}=-\\frac{1}{2\\theta}+\\frac{(x-\\mu_{0})^{2}}{2\\theta^{2}}.\n$$\nDifferentiate again to obtain the observed information (negative Hessian):\n$$\n\\frac{\\partial^{2} \\ell}{\\partial \\theta^{2}}=\\frac{1}{2\\theta^{2}}-\\frac{(x-\\mu_{0})^{2}}{\\theta^{3}}.\n$$\nThe Fisher information is the negative expectation of the second derivative under the model:\n$$\nI(\\theta)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell}{\\partial \\theta^{2}}\\right]\n=-\\left(\\frac{1}{2\\theta^{2}}-\\frac{\\mathbb{E}\\!\\left[(X-\\mu_{0})^{2}\\right]}{\\theta^{3}}\\right).\n$$\nSince $X\\sim \\mathcal{N}(\\mu_{0},\\theta)$, we have $\\mathbb{E}\\!\\left[(X-\\mu_{0})^{2}\\right]=\\theta$. Substituting gives\n$$\nI(\\theta)=-\\left(\\frac{1}{2\\theta^{2}}-\\frac{\\theta}{\\theta^{3}}\\right)\n=-\\left(\\frac{1}{2\\theta^{2}}-\\frac{1}{\\theta^{2}}\\right)\n=\\frac{1}{2\\theta^{2}}.\n$$\nTherefore, the Fisher information in a single observation for the variance parameter $\\theta$ is $\\frac{1}{2\\theta^{2}}$.", "answer": "$$\\boxed{\\frac{1}{2\\theta^{2}}}$$", "id": "1918246"}, {"introduction": "This final practice delves into a more nuanced question: what happens to statistical information when we don't observe our data directly, but rather a transformation of it? By analyzing a variable derived from an exponentially distributed lifetime, you will explore how data transformation impacts Fisher information. The result of this exercise [@problem_id:1918235] reveals a deep and sometimes surprising principle about information preservation in statistics.", "problem": "Let $X$ be a random variable representing the lifetime of an electronic component. The lifetime $X$ follows an exponential distribution with a probability density function (PDF) given by:\n$$f_{X}(x; \\theta) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x > 0$$\nHere, $\\theta > 0$ is the unknown mean lifetime of the component.\n\nIn an experiment, instead of observing the lifetime $X$ directly, we are only able to measure a transformed quantity $Y$, which is related to $X$ by the equation $Y = \\sqrt{X}$.\n\nYour task is to determine the Fisher information for the parameter $\\theta$ that is contained in a single observation of the transformed variable $Y$.", "solution": "We start with the given distribution of $X$:\n$$\nf_{X}(x; \\theta) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad x>0,\\ \\theta>0.\n$$\nWe observe $Y = \\sqrt{X}$, which induces the transformation $x = y^{2}$ with Jacobian $\\left|\\frac{dx}{dy}\\right| = 2y$. By the change-of-variables formula, the density of $Y$ is\n$$\nf_{Y}(y; \\theta) = f_{X}(y^{2}; \\theta)\\,2y = \\frac{2y}{\\theta}\\,\\exp\\left(-\\frac{y^{2}}{\\theta}\\right), \\quad y>0.\n$$\nFor a single observation $y$, the log-likelihood is\n$$\n\\ell(\\theta; y) = \\ln(2y) - \\ln \\theta - \\frac{y^{2}}{\\theta}.\n$$\nDifferentiate with respect to $\\theta$ to obtain the score function:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{1}{\\theta} + \\frac{y^{2}}{\\theta^{2}}.\n$$\nDifferentiate again to get the observed information:\n$$\n\\frac{\\partial^{2} \\ell}{\\partial \\theta^{2}} = \\frac{1}{\\theta^{2}} - \\frac{2y^{2}}{\\theta^{3}}.\n$$\nThe Fisher information is the negative expectation of the second derivative:\n$$\nI_{Y}(\\theta) = -\\mathbb{E}_{\\theta}\\!\\left[\\frac{\\partial^{2} \\ell}{\\partial \\theta^{2}}\\right] = -\\left(\\frac{1}{\\theta^{2}} - \\frac{2\\,\\mathbb{E}_{\\theta}[Y^{2}]}{\\theta^{3}}\\right).\n$$\nSince $Y^{2}=X$ almost surely under the transformation, we have $\\mathbb{E}_{\\theta}[Y^{2}] = \\mathbb{E}_{\\theta}[X] = \\theta$. Substituting gives\n$$\nI_{Y}(\\theta) = -\\left(\\frac{1}{\\theta^{2}} - \\frac{2\\theta}{\\theta^{3}}\\right) = -\\left(\\frac{1}{\\theta^{2}} - \\frac{2}{\\theta^{2}}\\right) = \\frac{1}{\\theta^{2}}.\n$$\nThus, a single observation of $Y$ contains Fisher information $1/\\theta^{2}$ about $\\theta$.", "answer": "$$\\boxed{\\frac{1}{\\theta^{2}}}$$", "id": "1918235"}]}