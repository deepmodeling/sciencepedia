{"hands_on_practices": [{"introduction": "The journey into least squares estimation begins with understanding its core objective: minimizing the sum of squared errors. This first practice focuses on that fundamental step, asking you to translate the abstract principle into a concrete mathematical function for a small dataset. By constructing this objective function, you are setting up the very problem that the method of least squares is designed to solve [@problem_id:1935126].", "problem": "In statistical modeling, a simple linear regression model is often used to describe the relationship between a predictor variable $x$ and a response variable $y$. The model is given by the equation $y = \\beta_0 + \\beta_1 x$, where $\\beta_0$ represents the y-intercept and $\\beta_1$ represents the slope. The method of least squares provides a way to estimate the parameters $\\beta_0$ and $\\beta_1$ by finding the line that minimizes the sum of the squared differences between the observed responses and the responses predicted by the linear model.\n\nSuppose an experiment yields the following three data points $(x, y)$:\n$$ \\{(0, 1), (1, 3), (2, 4)\\} $$\nTo find the least squares regression line, one must minimize a function $S(\\beta_0, \\beta_1)$ representing the sum of squared residuals for this dataset.\n\nDetermine the explicit form of the function $S(\\beta_0, \\beta_1)$ in terms of the parameters $\\beta_0$ and $\\beta_1$.", "solution": "The least squares criterion minimizes the sum of squared residuals. For the linear model $y=\\beta_{0}+\\beta_{1}x$ and data points $(x_{i},y_{i})$, the residual for point $i$ is $r_{i}=y_{i}-(\\beta_{0}+\\beta_{1}x_{i})$, and the sum of squared residuals is\n$$\nS(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left[y_{i}-(\\beta_{0}+\\beta_{1}x_{i})\\right]^{2}.\n$$\nFor the given three points $(0,1)$, $(1,3)$, and $(2,4)$, this becomes\n$$\nS(\\beta_{0},\\beta_{1})=\\left[1-(\\beta_{0}+\\beta_{1}\\cdot 0)\\right]^{2}+\\left[3-(\\beta_{0}+\\beta_{1}\\cdot 1)\\right]^{2}+\\left[4-(\\beta_{0}+\\beta_{1}\\cdot 2)\\right]^{2}.\n$$\nSimplifying the arguments of the squares yields\n$$\nS(\\beta_{0},\\beta_{1})=(1-\\beta_{0})^{2}+(3-\\beta_{0}-\\beta_{1})^{2}+(4-\\beta_{0}-2\\beta_{1})^{2}.\n$$\nExpanding each term:\n$$\n(1-\\beta_{0})^{2}=\\beta_{0}^{2}-2\\beta_{0}+1,\n$$\n$$\n(3-\\beta_{0}-\\beta_{1})^{2}=\\beta_{0}^{2}+2\\beta_{0}\\beta_{1}+\\beta_{1}^{2}-6\\beta_{0}-6\\beta_{1}+9,\n$$\n$$\n(4-\\beta_{0}-2\\beta_{1})^{2}=\\beta_{0}^{2}+4\\beta_{0}\\beta_{1}+4\\beta_{1}^{2}-8\\beta_{0}-16\\beta_{1}+16.\n$$\nSumming and collecting like terms gives\n$$\nS(\\beta_{0},\\beta_{1})=3\\beta_{0}^{2}+6\\beta_{0}\\beta_{1}+5\\beta_{1}^{2}-16\\beta_{0}-22\\beta_{1}+26.\n$$", "answer": "$$\\boxed{3 \\beta_{0}^{2} + 6 \\beta_{0} \\beta_{1} + 5 \\beta_{1}^{2} - 16 \\beta_{0} - 22 \\beta_{1} + 26}$$", "id": "1935126"}, {"introduction": "After fitting a regression line, the resulting residuals are not just random leftovers; they possess crucial mathematical properties that confirm our model is well-fitted. This exercise delves into one of the most important of these properties: the relationship, or lack thereof, between the residuals and the original predictor variable. Understanding this concept is fundamental to regression diagnostics and validates that our model has captured all the linear information available [@problem_id:1935149].", "problem": "In the context of statistical modeling, consider a simple linear regression performed on a dataset consisting of $n$ pairs of observations $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$, where we assume that not all $x_i$ values are identical. The model is given by $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$. The method of least squares is used to find the estimators for the intercept, $\\hat{\\beta}_0$, and the slope, $\\hat{\\beta}_1$.\n\nFrom this initial regression, the fitted values are calculated as $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$. The corresponding residuals are then computed for each observation as $e_i = y_i - \\hat{y}_i$.\n\nNow, suppose you perform a second, new simple linear regression. In this new regression, you treat the calculated residuals $e_i$ as the new response variable and the original $x_i$ values as the predictor variable. This new model is of the form $e_i = \\gamma_0 + \\gamma_1 x_i + \\delta_i$.\n\nDetermine the values of the least squares estimators for the intercept, $\\hat{\\gamma}_0$, and the slope, $\\hat{\\gamma}_1$, for this second regression. Present your final answer as a row matrix containing two elements, where the first element is the value of $\\hat{\\gamma}_0$ and the second element is the value of $\\hat{\\gamma}_1$.", "solution": "Consider the first least squares problem with model $y_i=\\beta_{0}+\\beta_{1}x_i+\\epsilon_i$ and residuals $e_i=y_i-\\hat{y}_i=y_i-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_i$. The least squares estimators $(\\hat{\\beta}_{0},\\hat{\\beta}_{1})$ minimize\n$$\nS(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)^{2}.\n$$\nThe normal equations are obtained by setting the partial derivatives to zero:\n$$\n\\frac{\\partial S}{\\partial \\beta_{0}}=-2\\sum_{i=1}^{n}\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)=0,\\quad\n\\frac{\\partial S}{\\partial \\beta_{1}}=-2\\sum_{i=1}^{n}x_i\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)=0.\n$$\nEvaluated at $(\\hat{\\beta}_{0},\\hat{\\beta}_{1})$, these give\n$$\n\\sum_{i=1}^{n}e_i=0,\\qquad \\sum_{i=1}^{n}x_i e_i=0.\n$$\nHence the residuals from the first regression satisfy both zero-sum and orthogonality to $x$.\n\nNow consider the second regression with response $e_i$ and predictor $x_i$:\n$$\ne_i=\\gamma_{0}+\\gamma_{1}x_i+\\delta_i,\n$$\nwhose least squares estimators $(\\hat{\\gamma}_{0},\\hat{\\gamma}_{1})$ minimize\n$$\nT(\\gamma_{0},\\gamma_{1})=\\sum_{i=1}^{n}\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)^{2}.\n$$\nThe normal equations are\n$$\n\\frac{\\partial T}{\\partial \\gamma_{0}}=-2\\sum_{i=1}^{n}\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)=0,\\qquad\n\\frac{\\partial T}{\\partial \\gamma_{1}}=-2\\sum_{i=1}^{n}x_i\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)=0.\n$$\nLet $\\bar{e}=\\frac{1}{n}\\sum_{i=1}^{n}e_i$ and $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i$. Solving the normal equations in the standard way yields\n$$\n\\hat{\\gamma}_{1}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(e_i-\\bar{e})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}},\\qquad\n\\hat{\\gamma}_{0}=\\bar{e}-\\hat{\\gamma}_{1}\\bar{x}.\n$$\nFrom the first regression we have $\\sum_{i=1}^{n}e_i=0$, hence $\\bar{e}=0$, and $\\sum_{i=1}^{n}x_i e_i=0$. Therefore\n$$\n\\sum_{i=1}^{n}(x_i-\\bar{x})(e_i-\\bar{e})=\\sum_{i=1}^{n}x_i e_i - n\\bar{x}\\bar{e}=0- n\\bar{x}\\cdot 0=0.\n$$\nSince not all $x_i$ are equal, the denominator $\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}>0$. Thus\n$$\n\\hat{\\gamma}_{1}=0,\\qquad \\hat{\\gamma}_{0}=\\bar{e}-\\hat{\\gamma}_{1}\\bar{x}=0-0\\cdot \\bar{x}=0.\n$$\nBy uniqueness of the least squares solution when $\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}>0$, these are the least squares estimators for the second regression.", "answer": "$$\\boxed{\\begin{pmatrix}0 & 0\\end{pmatrix}}$$", "id": "1935149"}, {"introduction": "The true power of a statistical method lies in its adaptability to non-standard problems, which are common in scientific research where theoretical considerations often impose constraints on model parameters. This problem challenges you to move beyond standard formulas and apply the first principles of least squares to a model with a linear constraint on its coefficients. Successfully solving this demonstrates how to customize the least squares framework to fit the specific demands of a scientific hypothesis [@problem_id:1935134].", "problem": "An ecologist is studying the population density of a certain species of plankton, $Y$, in a lake. The ecologist hypothesizes that the density is linearly dependent on two key environmental factors: water temperature, $x_1$, and the concentration of a specific nutrient, $x_2$. Based on a preliminary ecosystem model, there is a theoretical reason to believe that the parameters governing the marginal effects of temperature and nutrient concentration are coupled.\n\nFor a sample of $n$ different locations in the lake, data $(Y_i, x_{i1}, x_{i2})$ are collected for $i = 1, \\dots, n$. The proposed statistical model is:\n$$ Y_i = \\beta_0 + \\theta x_{i1} + (1-\\theta)x_{i2} + \\epsilon_i $$\nHere, $Y_i$ is the plankton density (in cells per liter), $x_{i1}$ is the water temperature (in degrees Celsius), and $x_{i2}$ is the nutrient concentration (in micromoles per liter) at location $i$. The parameter $\\theta$ is a dimensionless constant that partitions the combined effect, $\\beta_0$ is the baseline density, and $\\epsilon_i$ are independent and identically distributed random errors with a mean of zero and a constant variance $\\sigma^2$.\n\nYour task is to find the least squares estimator for the parameter $\\theta$, denoted by $\\hat{\\theta}$. Express your answer as a single closed-form analytic expression in terms of the observed data $Y_i, x_{i1}, x_{i2}$ and their corresponding sample means $\\bar{Y}, \\bar{x}_1, \\bar{x}_2$. Let a sample mean be defined as $\\bar{v} = \\frac{1}{n} \\sum_{i=1}^{n} v_i$ for any variable $v$.", "solution": "We are given the model\n$$\nY_i=\\beta_0+\\theta x_{i1}+(1-\\theta)x_{i2}+\\epsilon_i,\n$$\nwhich can be rewritten as\n$$\nY_i - x_{i2}=\\beta_0+\\theta\\left(x_{i1}-x_{i2}\\right)+\\epsilon_i.\n$$\nDefine $b_i=Y_i-x_{i2}$ and $d_i=x_{i1}-x_{i2}$. Then the model is\n$$\nb_i=\\beta_0+\\theta d_i+\\epsilon_i.\n$$\nThe ordinary least squares estimator minimizes\n$$\nS(\\beta_0,\\theta)=\\sum_{i=1}^{n}\\left(b_i-\\beta_0-\\theta d_i\\right)^{2}.\n$$\nThe normal equations are obtained by setting the partial derivatives to zero:\n$$\n\\frac{\\partial S}{\\partial \\beta_0}=-2\\sum_{i=1}^{n}\\left(b_i-\\beta_0-\\theta d_i\\right)=0,\\qquad\n\\frac{\\partial S}{\\partial \\theta}=-2\\sum_{i=1}^{n}d_i\\left(b_i-\\beta_0-\\theta d_i\\right)=0.\n$$\nFrom the first equation,\n$$\n\\sum_{i=1}^{n} b_i - n\\beta_0 - \\theta \\sum_{i=1}^{n} d_i=0\n\\;\\;\\Rightarrow\\;\\;\n\\beta_0=\\bar{b}-\\theta \\bar{d},\n$$\nwhere $\\bar{b}=\\frac{1}{n}\\sum_{i=1}^{n}b_i=\\bar{Y}-\\bar{x}_2$ and $\\bar{d}=\\frac{1}{n}\\sum_{i=1}^{n}d_i=\\bar{x}_1-\\bar{x}_2$. Substitute $\\beta_0=\\bar{b}-\\theta \\bar{d}$ into the second normal equation:\n$$\n\\sum_{i=1}^{n} d_i\\left[b_i-(\\bar{b}-\\theta \\bar{d})-\\theta d_i\\right]=0\n\\;\\;\\Rightarrow\\;\\;\n\\sum_{i=1}^{n} d_i\\left[(b_i-\\bar{b})-\\theta(d_i-\\bar{d})\\right]=0.\n$$\nRearranging,\n$$\n\\sum_{i=1}^{n} d_i(b_i-\\bar{b})-\\theta \\sum_{i=1}^{n} d_i(d_i-\\bar{d})=0.\n$$\nUsing $d_i=d_i-\\bar{d}+\\bar{d}$ and the fact that $\\sum_{i=1}^{n}(b_i-\\bar{b})=0$ and $\\sum_{i=1}^{n}(d_i-\\bar{d})=0$, we obtain the centered form\n$$\n\\sum_{i=1}^{n} (d_i-\\bar{d})(b_i-\\bar{b})-\\theta \\sum_{i=1}^{n} (d_i-\\bar{d})^{2}=0.\n$$\nSolving for $\\theta$ yields\n$$\n\\hat{\\theta}=\\frac{\\sum_{i=1}^{n} (d_i-\\bar{d})(b_i-\\bar{b})}{\\sum_{i=1}^{n} (d_i-\\bar{d})^{2}}.\n$$\nSubstituting back $d_i=x_{i1}-x_{i2}$, $\\bar{d}=\\bar{x}_1-\\bar{x}_2$, $b_i=Y_i-x_{i2}$, and $\\bar{b}=\\bar{Y}-\\bar{x}_2$, we obtain the required closed-form expression in the observed variables and their means:\n$$\n\\hat{\\theta}=\\frac{\\sum_{i=1}^{n}\\left[(x_{i1}-x_{i2})-(\\bar{x}_1-\\bar{x}_2)\\right]\\left[(Y_i-x_{i2})-(\\bar{Y}-\\bar{x}_2)\\right]}{\\sum_{i=1}^{n}\\left[(x_{i1}-x_{i2})-(\\bar{x}_1-\\bar{x}_2)\\right]^{2}}.\n$$", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n}\\left[(x_{i1}-x_{i2})-(\\bar{x}_1-\\bar{x}_2)\\right]\\left[(Y_i-x_{i2})-(\\bar{Y}-\\bar{x}_2)\\right]}{\\sum_{i=1}^{n}\\left[(x_{i1}-x_{i2})-(\\bar{x}_1-\\bar{x}_2)\\right]^{2}}}$$", "id": "1935134"}]}