{"hands_on_practices": [{"introduction": "The coefficient of determination, $R^2$, provides a crucial measure of how well a regression model explains the variability in the dependent variable. It is fundamentally defined by partitioning the total variation in the data into two parts: the variation explained by the model and the unexplained residual variation. This first exercise [@problem_id:1904808] is a direct application of the core formula relating the total sum of squares ($SST$) to the regression sum of squares ($SSR$) and $R^2$, providing a solid foundation for understanding these components.", "problem": "An urban planning student is investigating the relationship between the population density of a city district and the average daily ridership on its public transit system. After collecting data from various districts, the student performs a simple linear regression analysis to model this relationship. The analysis reveals that the total sum of squares (SST), which represents the total variation in the daily ridership data around its mean, is 240. The model's coefficient of determination, $R^2$, is found to be 0.25. The coefficient of determination measures the proportion of the variance in the dependent variable (ridership) that is predictable from the independent variable (population density).\n\nCalculate the regression sum of squares (SSR), which quantifies the amount of variation in the ridership data explained by the regression model.", "solution": "Let $SST$ denote the total sum of squares, $SSR$ the regression sum of squares, and $SSE$ the error sum of squares. The standard decomposition is\n$$\nSST = SSR + SSE.\n$$\nThe coefficient of determination is defined by\n$$\nR^{2} = \\frac{SSR}{SST}.\n$$\nSolving for $SSR$ gives\n$$\nSSR = R^{2} \\cdot SST.\n$$\nSubstituting the given values $R^{2} = 0.25$ and $SST = 240$,\n$$\nSSR = 0.25 \\cdot 240 = \\frac{1}{4} \\cdot 240 = 60.\n$$\nThus, the regression sum of squares is $60$.", "answer": "$$\\boxed{60}$$", "id": "1904808"}, {"introduction": "While $R^2$ quantifies the strength of a linear relationship, it is closely related to another fundamental measure: the Pearson correlation coefficient, $r$. In simple linear regression, the relationship is elegantly simple: $R^2 = r^2$. This practice [@problem_id:1904864] explores this direct connection, emphasizing that while $R^2$ tells us the proportion of variance explained, we must look to other information, such as the slope of the regression line, to determine the direction (positive or negative) of the association.", "problem": "An agricultural scientist is investigating the relationship between the amount of a specific fertilizer applied to a crop and the subsequent crop yield. The scientist conducts an experiment, applying varying amounts of fertilizer (in kg/hectare) to different plots of land and measuring the yield (in tonnes/hectare) for each plot. A simple linear regression analysis is performed on the collected data, with fertilizer amount as the independent variable and crop yield as the dependent variable. The analysis reveals that the coefficient of determination, denoted as $R^2$, is 0.49. A key observation from the data is that higher amounts of fertilizer are associated with lower crop yields, suggesting a potential toxic effect at the concentrations used.\n\nBased on this information, what is the value of the sample correlation coefficient, $r$, between the amount of fertilizer applied and the crop yield? Provide your answer as a single numerical value.", "solution": "We are given a simple linear regression with a single predictor and an intercept. For such a model, the coefficient of determination and the sample Pearson correlation coefficient satisfy\n$$\nR^{2} = r^{2}.\n$$\nGiven $R^{2} = 0.49$, it follows that\n$$\n|r| = \\sqrt{R^{2}} = \\sqrt{0.49} = 0.7.\n$$\nThe sign of $r$ matches the sign of the slope of the regression line. The observation that higher fertilizer amounts are associated with lower yields implies a negative slope and hence a negative correlation. Therefore,\n$$\nr = -0.7.\n$$", "answer": "$$\\boxed{-0.7}$$", "id": "1904864"}, {"introduction": "One of the most common interpretations of $R^2$ is as a value between 0 and 1, representing the proportion of variance explained. However, this interpretation is only guaranteed for certain models, specifically those that include an intercept term. This thought-provoking exercise [@problem_id:1904857] delves into a scenario with a regression-through-the-origin model. It serves as a critical cautionary tale, demonstrating that applying the standard $R^2$ formula to such a model can result in a negative value, revealing that the model's predictions are even worse than simply using the mean of the dependent variable.", "problem": "An electrical engineer is characterizing a novel resistive component. According to theoretical models based on Ohm's Law, the voltage $V$ across the component should be directly proportional to the current $I$ passing through it, implying a relationship of the form $V = \\beta I$, where $\\beta$ is the resistance. This corresponds to a linear regression model forced to pass through the origin.\n\nTo test this, the engineer performs a small experiment and collects the following three data points, where current $I$ is measured in amperes (A) and voltage $V$ is measured in microvolts ($\\mu$V):\n$(I_1, V_1) = (10, 1)$\n$(I_2, V_2) = (11, 2)$\n$(I_3, V_3) = (12, 0)$\n\nTo assess the goodness-of-fit for the regression-through-the-origin model $V = \\beta I$, the engineer decides to compute the coefficient of determination, $R^2$. The standard definition for $R^2$ is used:\n$$R^2 = 1 - \\frac{SSE}{SST}$$\nwhere:\n- The Sum of Squared Errors (SSE) is defined as $SSE = \\sum_{i=1}^{n} (V_i - \\hat{V}_i)^2$, with $\\hat{V}_i$ being the voltage predicted by the model for the current $I_i$.\n- The Total Sum of Squares (SST) is defined as $SST = \\sum_{i=1}^{n} (V_i - \\bar{V})^2$, with $\\bar{V}$ being the sample mean of the observed voltages.\n\nFor a regression-through-the-origin model, the least squares estimate of the slope is given by $\\hat{\\beta} = \\frac{\\sum I_i V_i}{\\sum I_i^2}$.\n\nCalculate the value of the coefficient of determination, $R^2$, for this model and dataset. Round your final answer to three significant figures.", "solution": "We use the regression-through-the-origin model $V=\\beta I$. The least squares estimator is given by $\\hat{\\beta}=\\frac{\\sum I_{i}V_{i}}{\\sum I_{i}^{2}}$. For the data $(I_{1},V_{1})=(10,1)$, $(I_{2},V_{2})=(11,2)$, $(I_{3},V_{3})=(12,0)$, compute the required sums:\n$$\\sum I_{i}V_{i}=10\\cdot 1+11\\cdot 2+12\\cdot 0=32,$$\n$$\\sum I_{i}^{2}=10^{2}+11^{2}+12^{2}=100+121+144=365.$$\nThus,\n$$\\hat{\\beta}=\\frac{32}{365}.$$\nPredicted values are $\\hat{V}_{i}=\\hat{\\beta}I_{i}$, so\n$$\\hat{V}_{1}=\\frac{32}{365}\\cdot 10=\\frac{320}{365}=\\frac{64}{73},\\quad \\hat{V}_{2}=\\frac{32}{365}\\cdot 11=\\frac{352}{365},\\quad \\hat{V}_{3}=\\frac{32}{365}\\cdot 12=\\frac{384}{365}.$$\nResiduals are\n$$r_{1}=V_{1}-\\hat{V}_{1}=1-\\frac{64}{73}=\\frac{9}{73},\\quad r_{2}=2-\\frac{352}{365}=\\frac{378}{365},\\quad r_{3}=0-\\frac{384}{365}=-\\frac{384}{365}.$$\nHence the sum of squared errors is\n$$SSE=\\left(\\frac{9}{73}\\right)^{2}+\\left(\\frac{378}{365}\\right)^{2}+\\left(\\frac{384}{365}\\right)^{2}=\\frac{801}{365}.$$\nAlternatively, using the identity for regression through the origin,\n$$SSE=\\sum V_{i}^{2}-\\frac{\\left(\\sum I_{i}V_{i}\\right)^{2}}{\\sum I_{i}^{2}}=5-\\frac{32^{2}}{365}=\\frac{801}{365},$$\nwhich matches the direct calculation. The sample mean of the observed voltages is\n$$\\bar{V}=\\frac{1+2+0}{3}=1,$$\nso the total sum of squares is\n$$SST=\\sum\\left(V_{i}-\\bar{V}\\right)^{2}=(1-1)^{2}+(2-1)^{2}+(0-1)^{2}=0+1+1=2.$$\nTherefore,\n$$R^{2}=1-\\frac{SSE}{SST}=1-\\frac{\\frac{801}{365}}{2}=1-\\frac{801}{730}=-\\frac{71}{730}\\approx -0.0973,$$\nrounded to three significant figures.", "answer": "$$\\boxed{-0.0973}$$", "id": "1904857"}]}