{"hands_on_practices": [{"introduction": "This first practice takes us back to basics with the simple linear regression model forced through the origin. By working through a scenario grounded in physics, we can derive the least squares estimator from first principles and investigate its fundamental statistical properties like unbiasedness and variance. This exercise provides a solid foundation and highlights important distinctions from the more common model that includes an intercept. [@problem_id:1948111]", "problem": "An engineer is characterizing a new resistive material. According to physical theory, the voltage $V$ across the material should be directly proportional to the current $I$ passing through it, following the model $V = \\beta I$, where $\\beta$ is the unknown true resistance. To estimate $\\beta$, the engineer performs a series of $n$ experiments, setting the current to known, non-random values $x_1, x_2, \\ldots, x_n$ and measuring the corresponding voltages $Y_1, Y_2, \\ldots, Y_n$.\n\nThe relationship is described by the simple linear regression model forced through the origin:\n$$Y_i = \\beta x_i + \\epsilon_i \\quad \\text{for } i=1, \\ldots, n$$\nHere, $Y_i$ is the measured voltage for the set current $x_i$, and $\\epsilon_i$ represents the random measurement error. The errors are assumed to be independent with $E[\\epsilon_i] = 0$ and $\\text{Var}(\\epsilon_i) = \\sigma^2$ for some unknown constant $\\sigma^2 > 0$. It is also assumed that at least one of the applied currents $x_i$ is non-zero.\n\nThe engineer uses the method of least squares to obtain an estimator for the resistance, denoted by $\\hat{\\beta}$. Consider the following statements about the properties of this estimator $\\hat{\\beta}$ and its corresponding residuals $e_i = Y_i - \\hat{\\beta} x_i$.\n\nA. The formula for the estimator is $\\hat{\\beta} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_i}{x_i}$.\n\nB. The estimator $\\hat{\\beta}$ is an unbiased estimator of $\\beta$.\n\nC. The variance of the estimator is given by $\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2 \\sum_{i=1}^n x_i^2}{(\\sum_{i=1}^n x_i)^2}$.\n\nD. The sum of the residuals, $\\sum_{i=1}^n e_i$, is not guaranteed to be zero.\n\nE. The variance of the estimator is given by $\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n x_i^2}$.\n\nWhich of the above statements are correct?", "solution": "We begin by deriving the ordinary least squares estimator for the slope in the regression through the origin. The estimator $\\hat{\\beta}$ minimizes the sum of squared residuals\n$$\nS(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta x_{i}\\right)^{2}.\n$$\nDifferentiating with respect to $\\beta$ and setting the derivative to zero gives the normal equation\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta}=-2\\sum_{i=1}^{n}x_{i}\\left(Y_{i}-\\beta x_{i}\\right)=0,\n$$\nwhich simplifies to\n$$\n\\sum_{i=1}^{n}x_{i}Y_{i}-\\beta\\sum_{i=1}^{n}x_{i}^{2}=0.\n$$\nAssuming $\\sum_{i=1}^{n}x_{i}^{2}>0$ (guaranteed by at least one $x_{i}\\neq 0$), the least squares estimator is\n$$\n\\hat{\\beta}=\\frac{\\sum_{i=1}^{n}x_{i}Y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\nThis directly shows that statement A, $\\hat{\\beta}=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{Y_{i}}{x_{i}}$, is incorrect in general; moreover, it would be undefined if any $x_{i}=0$, while the correct least squares estimator remains well-defined as long as not all $x_{i}$ are zero.\n\nNext, we establish unbiasedness. Using the model $Y_{i}=\\beta x_{i}+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$ and treating $x_{i}$ as fixed,\n$$\nE[\\hat{\\beta}]=\\frac{\\sum_{i=1}^{n}x_{i}E[Y_{i}]}{\\sum_{i=1}^{n}x_{i}^{2}}=\\frac{\\sum_{i=1}^{n}x_{i}(\\beta x_{i})}{\\sum_{i=1}^{n}x_{i}^{2}}=\\beta,\n$$\nso statement B is correct.\n\nFor the variance, write\n$$\n\\hat{\\beta}=\\beta+\\frac{\\sum_{i=1}^{n}x_{i}\\epsilon_{i}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\nSince the $\\epsilon_{i}$ are independent with $\\text{Var}(\\epsilon_{i})=\\sigma^{2}$,\n$$\n\\text{Var}\\!\\left(\\sum_{i=1}^{n}x_{i}\\epsilon_{i}\\right)=\\sum_{i=1}^{n}x_{i}^{2}\\,\\text{Var}(\\epsilon_{i})=\\sigma^{2}\\sum_{i=1}^{n}x_{i}^{2}.\n$$\nTherefore,\n$$\n\\text{Var}(\\hat{\\beta})=\\frac{\\text{Var}\\!\\left(\\sum_{i=1}^{n}x_{i}\\epsilon_{i}\\right)}{\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)^{2}}=\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\nThus statement E is correct, and statement C, which gives a different expression, is incorrect.\n\nFinally, consider the residuals $e_{i}=Y_{i}-\\hat{\\beta}x_{i}$. The normal equation implies\n$$\n\\sum_{i=1}^{n}x_{i}e_{i}=\\sum_{i=1}^{n}x_{i}(Y_{i}-\\hat{\\beta}x_{i})=0.\n$$\nHowever, there is no constraint in the through-the-origin model that forces $\\sum_{i=1}^{n}e_{i}=0$. In fact,\n$$\n\\sum_{i=1}^{n}e_{i}=\\sum_{i=1}^{n}Y_{i}-\\hat{\\beta}\\sum_{i=1}^{n}x_{i},\n$$\nwhich is not generally zero. Therefore, statement D is correct.\n\nCollecting the conclusions: B, D, and E are correct; A and C are incorrect.", "answer": "$$\\boxed{BDE}$$", "id": "1948111"}, {"introduction": "Building on our foundational understanding, we now explore a crucial property of the standard regression model that includes an intercept. This practice investigates the effect of 'centering' the data—subtracting the mean from each variable—before fitting the model. You will discover that the slope estimator is invariant to this transformation, a result that provides deep insight into what the slope coefficient truly represents: the relationship between variables as they deviate from their respective averages. [@problem_id:1948117]", "problem": "In statistical analysis, a common preprocessing step is to center variables by subtracting their mean. Consider a dataset of $n$ pairs of observations, $\\{(x_i, Y_i)\\}_{i=1}^n$. A researcher wishes to model the relationship using a Simple Linear Regression (SLR) model of the form:\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\nwhere $\\beta_0$ and $\\beta_1$ are the intercept and slope parameters, respectively, and $\\epsilon_i$ are independent error terms with a mean of zero. The Ordinary Least Squares (OLS) estimator for the slope, denoted as $\\hat{\\beta}_1$, is given by the formula:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\nwhere $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$ are the sample means.\n\nBefore fitting the model, the researcher transforms the data by centering both the predictor and response variables. The new, centered variables are defined as:\n$$ x_i' = x_i - \\bar{x} $$\n$$ Y_i' = Y_i - \\bar{Y} $$\nA new SLR model is then proposed for these transformed variables:\n$$ Y_i' = \\gamma_0 + \\gamma_1 x_i' + \\delta_i $$\nLet $\\hat{\\gamma}_1$ be the OLS estimator for the slope in this new model using the centered data $(x_i', Y_i')$. Assuming that the predictor variable has some variation, i.e., $\\sum_{i=1}^n (x_i - \\bar{x})^2 > 0$, calculate the exact value of the ratio $\\frac{\\hat{\\gamma}_1}{\\hat{\\beta}_1}$.", "solution": "We start from the given OLS slope estimator in the original model:\n$$\n\\hat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}.\n$$\nDefine centered variables $x_{i}'=x_{i}-\\bar{x}$ and $Y_{i}'=Y_{i}-\\bar{Y}$. Their sample means satisfy\n$$\n\\bar{x}'=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}'=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})=0,\\qquad\n\\bar{Y}'=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}'=\\frac{1}{n}\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})=0.\n$$\nIn the centered SLR $Y_{i}'=\\gamma_{0}+\\gamma_{1}x_{i}'+\\delta_{i}$, the OLS slope estimator has the standard form\n$$\n\\hat{\\gamma}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}'-\\bar{x}')(Y_{i}'-\\bar{Y}')}{\\sum_{i=1}^{n}(x_{i}'-\\bar{x}')^{2}}.\n$$\nUsing $\\bar{x}'=0$ and $\\bar{Y}'=0$, this simplifies to\n$$\n\\hat{\\gamma}_{1}=\\frac{\\sum_{i=1}^{n}x_{i}'Y_{i}'}{\\sum_{i=1}^{n}(x_{i}')^{2}}.\n$$\nSubstituting $x_{i}'=x_{i}-\\bar{x}$ and $Y_{i}'=Y_{i}-\\bar{Y}$ gives\n$$\n\\hat{\\gamma}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}=\\hat{\\beta}_{1}.\n$$\nSince $\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}>0$ by assumption, both estimators are well-defined, and hence\n$$\n\\frac{\\hat{\\gamma}_{1}}{\\hat{\\beta}_{1}}=1.\n$$", "answer": "$$\\boxed{1}$$", "id": "1948117"}, {"introduction": "Our final practice generalizes the concept of removing an effect from a variable to the multiple regression setting. Here, we move beyond just accounting for the mean to accounting for the influence of other predictors. This exercise introduces the powerful Frisch-Waugh-Lovell theorem, demonstrating how a coefficient in a multiple regression model can be understood as the result of a regression on residuals. This provides the definitive interpretation for a coefficient's unique contribution. [@problem_id:1948175]", "problem": "A data scientist is modeling a server's Central Processing Unit (CPU) load. The model aims to predict the CPU load ($Y$, in percent) based on two predictor variables: the number of active user sessions ($X_1$) and the network bandwidth utilization ($X_2$, in Mbps). The multiple linear regression model is given by:\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\n$$\nwhere $\\epsilon_i$ are independent and identically distributed error terms with mean zero, and the subscript $i$ denotes the $i$-th observation from a sample of $n$ observations.\n\nThe scientist is particularly interested in quantifying the unique contribution of active user sessions ($X_1$) to the CPU load, after accounting for the influence of network bandwidth ($X_2$). To investigate this, they perform two preliminary Ordinary Least Squares (OLS) regressions.\n\nFirst, they regress the CPU load ($Y$) on an intercept and the network bandwidth ($X_2$). Let the residuals from this regression be denoted by the vector $r_Y$.\nSecond, they regress the number of active user sessions ($X_1$) on an intercept and the network bandwidth ($X_2$). Let the residuals from this second regression be denoted by the vector $r_{X_1}$.\n\nFrom these two sets of residuals, the following summary statistics are calculated:\n- The sum of squared residuals from the second regression: $\\sum_{i=1}^{n} (r_{X_1,i})^2 = 88.4$.\n- The sum of the products of the corresponding residuals from the two regressions: $\\sum_{i=1}^{n} r_{Y,i} \\, r_{X_1,i} = 153.7$.\n\nUsing only these summary statistics, determine the OLS estimate for the coefficient $\\beta_1$ in the full multiple regression model. Round your final answer to three significant figures.", "solution": "We aim to estimate $\\hat{\\beta}_{1}$ in the multiple regression of $Y$ on an intercept, $X_{1}$, and $X_{2}$. By the Frisch–Waugh–Lovell theorem, the OLS estimate $\\hat{\\beta}_{1}$ equals the slope from regressing the residuals of $Y$ on $[1, X_{2}]$ onto the residuals of $X_{1}$ on $[1, X_{2}]$. Denote $Z$ as the matrix with columns for the intercept and $X_{2}$, and define the residual-maker matrix $M_{Z} = I - Z(Z^{\\top}Z)^{-1}Z^{\\top}$. Then\n$$\nr_{Y} = M_{Z}Y, \\quad r_{X_{1}} = M_{Z}X_{1}.\n$$\nThe OLS coefficient from the regression of $r_{Y}$ on $r_{X_{1}}$ without an intercept minimizes\n$$\nS(\\alpha) = \\|r_{Y} - \\alpha r_{X_{1}}\\|^{2} = (r_{Y} - \\alpha r_{X_{1}})^{\\top}(r_{Y} - \\alpha r_{X_{1}}).\n$$\nDifferentiating and setting to zero gives\n$$\n\\frac{dS}{d\\alpha} = -2 r_{X_{1}}^{\\top} r_{Y} + 2 \\alpha \\, r_{X_{1}}^{\\top} r_{X_{1}} = 0 \\;\\;\\Rightarrow\\;\\; \\hat{\\alpha} = \\frac{r_{X_{1}}^{\\top} r_{Y}}{r_{X_{1}}^{\\top} r_{X_{1}}}.\n$$\nBy the Frisch–Waugh–Lovell theorem, $\\hat{\\beta}_{1} = \\hat{\\alpha}$. Using the provided summary statistics,\n$$\nr_{X_{1}}^{\\top} r_{X_{1}} = \\sum_{i=1}^{n} (r_{X_{1},i})^{2} = 88.4, \\quad r_{X_{1}}^{\\top} r_{Y} = \\sum_{i=1}^{n} r_{X_{1},i} r_{Y,i} = 153.7,\n$$\nso\n$$\n\\hat{\\beta}_{1} = \\frac{153.7}{88.4} \\approx 1.7386877828\\ldots\n$$\nRounded to three significant figures,\n$$\n\\hat{\\beta}_{1} \\approx 1.74.\n$$", "answer": "$$\\boxed{1.74}$$", "id": "1948175"}]}