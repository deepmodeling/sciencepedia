{"hands_on_practices": [{"introduction": "A key assumption of Linear Discriminant Analysis (LDA) is that all classes in the data share a common covariance matrix. This practice provides a foundational, hands-on exercise in estimating this shared matrix by calculating the pooled sample covariance matrix, $S_p$, from raw data points. Mastering this calculation is an essential first step in understanding the mechanics of building an LDA model from scratch [@problem_id:1914041].", "problem": "A data scientist at an e-commerce company is conducting a preliminary analysis for a potential Linear Discriminant Analysis (LDA) model to classify customers. The goal is to distinguish between two groups of customers: \"Premium Subscribers\" and \"Standard Users\". Two key metrics have been measured for small random samples from each group:\n- $X_1$: Average session duration in minutes.\n- $X_2$: Number of items purchased in the last month.\n\nThe data is collected as two-dimensional vectors $\\mathbf{x} = [X_1, X_2]^T$.\n\nThe sample for the \"Premium Subscribers\" group (Group 1) consists of $n_1=4$ customers:\n$$ \\mathbf{x}_1^{(1)} = \\begin{pmatrix} 15 \\\\ 8 \\end{pmatrix}, \\quad \\mathbf{x}_2^{(1)} = \\begin{pmatrix} 20 \\\\ 10 \\end{pmatrix}, \\quad \\mathbf{x}_3^{(1)} = \\begin{pmatrix} 18 \\\\ 6 \\end{pmatrix}, \\quad \\mathbf{x}_4^{(1)} = \\begin{pmatrix} 17 \\\\ 8 \\end{pmatrix} $$\n\nThe sample for the \"Standard Users\" group (Group 2) consists of $n_2=5$ customers:\n$$ \\mathbf{x}_1^{(2)} = \\begin{pmatrix} 8 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{x}_2^{(2)} = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix}, \\quad \\mathbf{x}_3^{(2)} = \\begin{pmatrix} 10 \\\\ 3 \\end{pmatrix}, \\quad \\mathbf{x}_4^{(2)} = \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{x}_5^{(2)} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix} $$\n\nA key assumption for standard LDA is that the covariance matrices of the different groups are equal. To proceed, the data scientist needs to compute a single estimate for this common covariance matrix, known as the pooled sample covariance matrix, $S_p$.\n\nCalculate the pooled sample covariance matrix $S_p$ based on the provided data.", "solution": "We estimate the common covariance via the pooled sample covariance matrix\n$$\nS_{p}=\\frac{(n_{1}-1)S_{1}+(n_{2}-1)S_{2}}{n_{1}+n_{2}-2},\n$$\nwhere, for group $k\\in\\{1,2\\}$ with size $n_{k}$, the sample covariance is\n$$\nS_{k}=\\frac{1}{n_{k}-1}\\sum_{i=1}^{n_{k}}\\left(\\mathbf{x}_{i}^{(k)}-\\bar{\\mathbf{x}}^{(k)}\\right)\\left(\\mathbf{x}_{i}^{(k)}-\\bar{\\mathbf{x}}^{(k)}\\right)^{T},\n$$\nand $\\bar{\\mathbf{x}}^{(k)}=\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}}\\mathbf{x}_{i}^{(k)}$.\n\nFirst compute the sample means.\n\nFor Group 1 ($n_{1}=4$):\n$$\n\\bar{\\mathbf{x}}^{(1)}=\\frac{1}{4}\\begin{pmatrix}15+20+18+17\\\\ 8+10+6+8\\end{pmatrix}=\\begin{pmatrix}17.5\\\\ 8\\end{pmatrix}.\n$$\nFor Group 2 ($n_{2}=5$):\n$$\n\\bar{\\mathbf{x}}^{(2)}=\\frac{1}{5}\\begin{pmatrix}8+12+10+6+9\\\\ 2+4+3+1+5\\end{pmatrix}=\\begin{pmatrix}9\\\\ 3\\end{pmatrix}.\n$$\n\nNext compute $S_{1}$. Let $\\mathbf{d}_{i}^{(1)}=\\mathbf{x}_{i}^{(1)}-\\bar{\\mathbf{x}}^{(1)}$.\nThe deviations and outer products are:\n$$\n\\mathbf{d}_{1}^{(1)}=\\begin{pmatrix}-2.5\\\\ 0\\end{pmatrix},\\quad \\mathbf{d}_{1}^{(1)}\\mathbf{d}_{1}^{(1)T}=\\begin{pmatrix}6.25&0\\\\ 0&0\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{2}^{(1)}=\\begin{pmatrix}2.5\\\\ 2\\end{pmatrix},\\quad \\mathbf{d}_{2}^{(1)}\\mathbf{d}_{2}^{(1)T}=\\begin{pmatrix}6.25&5\\\\ 5&4\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{3}^{(1)}=\\begin{pmatrix}0.5\\\\ -2\\end{pmatrix},\\quad \\mathbf{d}_{3}^{(1)}\\mathbf{d}_{3}^{(1)T}=\\begin{pmatrix}0.25&-1\\\\ -1&4\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{4}^{(1)}=\\begin{pmatrix}-0.5\\\\ 0\\end{pmatrix},\\quad \\mathbf{d}_{4}^{(1)}\\mathbf{d}_{4}^{(1)T}=\\begin{pmatrix}0.25&0\\\\ 0&0\\end{pmatrix}.\n$$\nSumming gives\n$$\n\\sum_{i=1}^{4}\\mathbf{d}_{i}^{(1)}\\mathbf{d}_{i}^{(1)T}=\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix},\n$$\nhence\n$$\nS_{1}=\\frac{1}{n_{1}-1}\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix}=\\frac{1}{3}\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix}.\n$$\n\nNow compute $S_{2}$. Let $\\mathbf{d}_{i}^{(2)}=\\mathbf{x}_{i}^{(2)}-\\bar{\\mathbf{x}}^{(2)}$.\nThe deviations and outer products are:\n$$\n\\mathbf{d}_{1}^{(2)}=\\begin{pmatrix}-1\\\\ -1\\end{pmatrix},\\ \\ \\mathbf{d}_{1}^{(2)}\\mathbf{d}_{1}^{(2)T}=\\begin{pmatrix}1&1\\\\ 1&1\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{2}^{(2)}=\\begin{pmatrix}3\\\\ 1\\end{pmatrix},\\ \\ \\mathbf{d}_{2}^{(2)}\\mathbf{d}_{2}^{(2)T}=\\begin{pmatrix}9&3\\\\ 3&1\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{3}^{(2)}=\\begin{pmatrix}1\\\\ 0\\end{pmatrix},\\ \\ \\mathbf{d}_{3}^{(2)}\\mathbf{d}_{3}^{(2)T}=\\begin{pmatrix}1&0\\\\ 0&0\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{4}^{(2)}=\\begin{pmatrix}-3\\\\ -2\\end{pmatrix},\\ \\ \\mathbf{d}_{4}^{(2)}\\mathbf{d}_{4}^{(2)T}=\\begin{pmatrix}9&6\\\\ 6&4\\end{pmatrix},\n$$\n$$\n\\mathbf{d}_{5}^{(2)}=\\begin{pmatrix}0\\\\ 2\\end{pmatrix},\\ \\ \\mathbf{d}_{5}^{(2)}\\mathbf{d}_{5}^{(2)T}=\\begin{pmatrix}0&0\\\\ 0&4\\end{pmatrix}.\n$$\nSumming gives\n$$\n\\sum_{i=1}^{5}\\mathbf{d}_{i}^{(2)}\\mathbf{d}_{i}^{(2)T}=\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix},\n$$\nhence\n$$\nS_{2}=\\frac{1}{n_{2}-1}\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix}=\\frac{1}{4}\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix}.\n$$\n\nFinally, the pooled covariance is\n$$\nS_{p}=\\frac{(n_{1}-1)S_{1}+(n_{2}-1)S_{2}}{n_{1}+n_{2}-2}=\\frac{\\begin{pmatrix}13&4\\\\ 4&8\\end{pmatrix}+\\begin{pmatrix}20&10\\\\ 10&10\\end{pmatrix}}{7}=\\frac{1}{7}\\begin{pmatrix}33&14\\\\ 14&18\\end{pmatrix}=\\begin{pmatrix}\\frac{33}{7}&2\\\\ 2&\\frac{18}{7}\\end{pmatrix}.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{33}{7} & 2 \\\\ 2 & \\frac{18}{7}\\end{pmatrix}}$$", "id": "1914041"}, {"introduction": "Once the statistical parameters of the groups are known—specifically their means and a common pooled covariance—the core of LDA is to define a clear rule for classification. This exercise guides you through the process of deriving the explicit equation for the linear decision boundary, which is the line that geometrically separates the two classes. By translating the statistical parameters into a concrete geometric rule, you will gain a tangible understanding of how an LDA classifier actually makes its decisions [@problem_id:1914104].", "problem": "A semiconductor manufacturing plant uses an automated optical inspection system to classify silicon wafers. The system measures two key geometric properties of a wafer: its bow ($x_1$) and its warp ($x_2$), both measured in micrometers. Based on historical data, wafers are categorized into two groups: 'Prime' (high quality) and 'Test' (lower quality, requires further testing).\n\nA statistical model, based on Linear Discriminant Analysis (LDA), is used to create a decision boundary to separate these two groups. The analysis assumes that the feature vectors for both groups, $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, are drawn from bivariate normal distributions with a common covariance matrix. It is also assumed that the prior probabilities of a wafer being 'Prime' or 'Test' are equal.\n\nThe historical data provides the following statistical summaries:\n- The mean vector for 'Prime' wafers is $\\boldsymbol{\\mu}_P = \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix}$.\n- The mean vector for 'Test' wafers is $\\boldsymbol{\\mu}_T = \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}$.\n- The pooled covariance matrix for both groups is $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}$.\n\nDetermine the linear discriminant function, $f(x_1, x_2)$, such that the decision boundary is given by the equation $f(x_1, x_2) = 0$. The function should be in the form $f(x_1, x_2) = a_1 x_1 + a_2 x_2 + a_0$, where the coefficients $a_1$, $a_2$, and $a_0$ are integers with no common factors, and $a_1$ is positive.", "solution": "For two classes with means $\\boldsymbol{\\mu}_{P}$ and $\\boldsymbol{\\mu}_{T}$, common covariance $\\boldsymbol{\\Sigma}$, and equal priors, Linear Discriminant Analysis yields class scores\n$$\n\\delta_{k}(\\mathbf{x})= \\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{k} - \\frac{1}{2}\\boldsymbol{\\mu}_{k}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{k} + \\ln \\pi_{k},\n$$\nwhere $\\pi_{k}$ is the prior. With $\\pi_{P}=\\pi_{T}$, the decision boundary is $\\delta_{P}(\\mathbf{x})=\\delta_{T}(\\mathbf{x})$, equivalently\n$$\n\\mathbf{x}^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T}) - \\frac{1}{2}\\left(\\boldsymbol{\\mu}_{P}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P} - \\boldsymbol{\\mu}_{T}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}\\right)=0.\n$$\nThus the linear discriminant function can be written as\n$$\nf(x_{1},x_{2})= \\mathbf{w}^{\\top}\\mathbf{x} + a_{0},\n$$\nwith $\\mathbf{w}=\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T})$ and $a_{0}= -\\frac{1}{2}\\left(\\boldsymbol{\\mu}_{P}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P} - \\boldsymbol{\\mu}_{T}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}\\right)$.\n\nGiven\n$$\n\\boldsymbol{\\mu}_{P}=\\begin{pmatrix}10\\\\ 5\\end{pmatrix},\\quad\n\\boldsymbol{\\mu}_{T}=\\begin{pmatrix}6\\\\ 9\\end{pmatrix},\\quad\n\\boldsymbol{\\Sigma}=\\begin{pmatrix}2&1\\\\ 1&3\\end{pmatrix},\n$$\ncompute $\\boldsymbol{\\Sigma}^{-1}$. Its determinant is $\\det(\\boldsymbol{\\Sigma})=2\\cdot 3 - 1\\cdot 1 = 5$, so\n$$\n\\boldsymbol{\\Sigma}^{-1}=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}.\n$$\nCompute the weight vector:\n$$\n\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T}=\\begin{pmatrix}4\\\\ -4\\end{pmatrix},\\quad\n\\mathbf{w}=\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_{P}-\\boldsymbol{\\mu}_{T})=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}\\begin{pmatrix}4\\\\ -4\\end{pmatrix}=\\frac{1}{5}\\begin{pmatrix}16\\\\ -12\\end{pmatrix}=\\begin{pmatrix}\\frac{16}{5}\\\\ -\\frac{12}{5}\\end{pmatrix}.\n$$\nCompute the constant term:\n$$\n\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P}=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}\\begin{pmatrix}10\\\\ 5\\end{pmatrix}=\\begin{pmatrix}5\\\\ 0\\end{pmatrix}\\Rightarrow \\boldsymbol{\\mu}_{P}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{P}=50,\n$$\n$$\n\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}=\\frac{1}{5}\\begin{pmatrix}3&-1\\\\ -1&2\\end{pmatrix}\\begin{pmatrix}6\\\\ 9\\end{pmatrix}=\\begin{pmatrix}\\frac{9}{5}\\\\ \\frac{12}{5}\\end{pmatrix}\\Rightarrow \\boldsymbol{\\mu}_{T}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_{T}=\\frac{162}{5}.\n$$\nTherefore,\n$$\na_{0}=-\\frac{1}{2}\\left(50-\\frac{162}{5}\\right)=-\\frac{1}{2}\\cdot \\frac{88}{5}=-\\frac{44}{5}.\n$$\nHence\n$$\nf(x_{1},x_{2})=\\frac{16}{5}x_{1}-\\frac{12}{5}x_{2}-\\frac{44}{5}.\n$$\nMultiplying by the positive factor $5$ and simplifying to coprime integer coefficients yields\n$$\nf(x_{1},x_{2})=16x_{1}-12x_{2}-44=4x_{1}-3x_{2}-11,\n$$\nwith $a_{1}=4>0$ and $\\gcd(4,3,11)=1$, as required.", "answer": "$$\\boxed{4 x_{1} - 3 x_{2} - 11}$$", "id": "1914104"}, {"introduction": "Linear Discriminant Analysis is powerful, but its assumption of equal covariance matrices is not always valid. This is where Quadratic Discriminant Analysis (QDA) offers a more flexible alternative. This final practice challenges you to explore what happens when we relax this assumption, allowing each class to have its own unique covariance structure, and to derive the resulting decision boundary, which is no longer linear. This exercise illuminates the fundamental difference between LDA and QDA and deepens your intuition for how the underlying data structure shapes the classification rule [@problem_id:1914090].", "problem": "In a study of bioluminescent organisms, an ecologist is trying to classify two species of fireflies, Species A and Species B, based on a two-dimensional feature vector $x = (x_1, x_2)^T$. The feature $x_1$ represents the duration of a light pulse, and $x_2$ represents its peak frequency, after a suitable normalization process.\n\nThe distributions of these features for each species are modeled as bivariate normal.\n- For Species A, the feature vector $X$ follows a normal distribution with mean vector $\\mu_1 = (0, 0)^T$ and covariance matrix $\\Sigma_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n- For Species B, the feature vector $X$ follows a normal distribution with mean vector $\\mu_2 = (\\mu, 0)^T$ and covariance matrix $\\Sigma_2 = \\begin{pmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{pmatrix}$.\n\nThe parameters are known constants satisfying $\\mu > 0$, $\\sigma_1 > 1$, and $\\sigma_2 > 1$. The ecologist uses Quadratic Discriminant Analysis (QDA) to build a classifier. It is assumed that the two species are equally common, so the prior probabilities are equal, $\\pi_1 = \\pi_2 = 0.5$.\n\nThe decision boundary is defined as the set of points $x$ in the feature space where the posterior probabilities of belonging to either class are equal, i.e., $P(\\text{Class A}|x) = P(\\text{Class B}|x)$. Given the setup, what is the geometric shape of this decision boundary in the $(x_1, x_2)$ plane?\n\nA. A straight line\n\nB. A circle\n\nC. An ellipse\n\nD. A parabola\n\nE. A hyperbola", "solution": "With equal priors, the QDA decision boundary is given by the set of points where the class-conditional densities are equal:\n$$\np_{1}(x)=p_{2}(x).\n$$\nFor a Gaussian density in dimension two,\n$$\n\\ln p(x)= -\\frac{1}{2}\\ln|2\\pi\\Sigma| - \\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu).\n$$\nSetting $\\ln p_{1}(x)=\\ln p_{2}(x)$ and cancelling the common term $-\\ln(2\\pi)$ yields\n$$\n-\\frac{1}{2}\\ln|\\Sigma_{1}| - \\frac{1}{2}(x-\\mu_{1})^{T}\\Sigma_{1}^{-1}(x-\\mu_{1}) = -\\frac{1}{2}\\ln|\\Sigma_{2}| - \\frac{1}{2}(x-\\mu_{2})^{T}\\Sigma_{2}^{-1}(x-\\mu_{2}).\n$$\nMultiplying by $-2$ gives\n$$\n\\ln|\\Sigma_{1}| + (x-\\mu_{1})^{T}\\Sigma_{1}^{-1}(x-\\mu_{1}) = \\ln|\\Sigma_{2}| + (x-\\mu_{2})^{T}\\Sigma_{2}^{-1}(x-\\mu_{2}).\n$$\nSubstitute $\\mu_{1}=(0,0)^{T}$, $\\Sigma_{1}=I$, $\\mu_{2}=(\\mu,0)^{T}$, and $\\Sigma_{2}=\\mathrm{diag}(\\sigma_{1}^{2},\\sigma_{2}^{2})$. Then $|\\Sigma_{1}|=1$, $\\Sigma_{1}^{-1}=I$, $|\\Sigma_{2}|=\\sigma_{1}^{2}\\sigma_{2}^{2}$, and $\\Sigma_{2}^{-1}=\\mathrm{diag}(\\sigma_{1}^{-2},\\sigma_{2}^{-2})$. Writing $x=(x_{1},x_{2})^{T}$, the boundary satisfies\n$$\nx_{1}^{2}+x_{2}^{2} = \\ln(\\sigma_{1}^{2}\\sigma_{2}^{2}) + \\frac{(x_{1}-\\mu)^{2}}{\\sigma_{1}^{2}} + \\frac{x_{2}^{2}}{\\sigma_{2}^{2}}.\n$$\nRearranging and expanding $(x_{1}-\\mu)^{2}=x_{1}^{2}-2\\mu x_{1}+\\mu^{2}$,\n$$\nx_{1}^{2}+x_{2}^{2} - \\frac{x_{1}^{2}-2\\mu x_{1}+\\mu^{2}}{\\sigma_{1}^{2}} - \\frac{x_{2}^{2}}{\\sigma_{2}^{2}} - \\ln(\\sigma_{1}^{2}\\sigma_{2}^{2}) = 0,\n$$\nwhich simplifies to the quadratic equation\n$$\n\\left(1-\\sigma_{1}^{-2}\\right)x_{1}^{2} + \\left(1-\\sigma_{2}^{-2}\\right)x_{2}^{2} + \\frac{2\\mu}{\\sigma_{1}^{2}}x_{1} - \\frac{\\mu^{2}}{\\sigma_{1}^{2}} - \\ln(\\sigma_{1}^{2}\\sigma_{2}^{2}) = 0.\n$$\nThere is no $x_{1}x_{2}$ term, so the conic is axis-aligned. Since $\\sigma_{1}>1$ and $\\sigma_{2}>1$, we have $1-\\sigma_{1}^{-2}>0$ and $1-\\sigma_{2}^{-2}>0$, so the quadratic form in $(x_{1},x_{2})$ is positive definite. Therefore, the decision boundary is an ellipse (possibly shifted), not a straight line, circle (except in the special case $\\sigma_{1}=\\sigma_{2}$), parabola, or hyperbola.", "answer": "$$\\boxed{C}$$", "id": "1914090"}]}