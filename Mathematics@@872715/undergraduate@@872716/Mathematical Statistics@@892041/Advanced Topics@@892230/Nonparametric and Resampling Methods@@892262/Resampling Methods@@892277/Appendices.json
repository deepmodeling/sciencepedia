{"hands_on_practices": [{"introduction": "Hypothesis testing is a cornerstone of statistical inference, but its classical forms often rely on strong assumptions about data distribution, such as normality. Permutation tests offer a powerful and intuitive non-parametric alternative that bypasses these assumptions by directly simulating the null hypothesis of exchangeability. This exercise [@problem_id:1951649] challenges you to apply this method to a practical scenario: comparing the manufacturing consistency of two processes using a robust statistic, the ratio of Median Absolute Deviations (MAD). By working through this problem, you will gain hands-on experience in calculating a test statistic and interpreting pre-computed permutation results to determine statistical significance.", "problem": "An industrial statistician is tasked with comparing the consistency of two different manufacturing processes for producing a specific type of resistor. Process A and Process B are used to create batches of resistors, and the resistance values are measured. The goal is to determine if Process B exhibits greater variability than Process A. Due to the potential for outliers and non-normal data, a robust, non-parametric approach is chosen.\n\nTwo independent random samples of resistance values (in Ohms) are collected:\n- Sample A (from Process A): $\\{10, 12, 13, 15\\}$\n- Sample B (from Process B): $\\{40, 48, 50, 53, 61\\}$\n\nThe statistician decides to use a test based on the ratio of the Median Absolute Deviations (MAD) of the two samples. The MAD of a sample $\\{x_1, x_2, \\dots, x_n\\}$ is defined as the median of the absolute differences between each data point and the sample's median:\n$$ \\text{MAD} = \\text{median} \\left( |x_i - \\text{median}(\\{x_j\\})| \\right) $$\nFor this analysis, any standard scaling constant for the MAD is ignored as it will cancel in the ratio. The test statistic is defined as $T = \\frac{\\text{MAD}(B)}{\\text{MAD}(A)}$.\n\nTo assess the significance of the observed test statistic, a computational permutation procedure was performed. The $4+5=9$ total data points were pooled together. From this combined set, 9,999 random permutations were generated. In each permutation, a new sample of size 4 (let's call it A') and a new sample of size 5 (B') were formed without replacement, and the statistic $T^* = \\frac{\\text{MAD}(B')}{\\text{MAD}(A')}$ was calculated.\n\nThe 30 largest values of $T^*$ from the 9,999 permutations, sorted in descending order, are:\n`3.15, 2.98, 2.81, 2.75, 2.66, 2.54, 2.49, 2.41, 2.35, 2.33, 2.28, 2.22, 2.19, 2.15, 2.11, 2.08, 2.05, 2.02, 1.99, 1.97, 1.94, 1.90, 1.88, 1.85, 1.83, 1.80, 1.77, 1.76, 1.75, 1.72`\n\nUsing the provided simulation results, calculate the one-sided p-value for testing the hypothesis that Process B has a greater scale (variability) than Process A. The p-value is to be computed as $p = \\frac{1+k}{1+N}$, where $N$ is the number of permutations and $k$ is the number of permuted statistics $T^*$ that are greater than or equal to the statistic observed from the original samples. Give your answer as a decimal rounded to four significant figures.", "solution": "We are testing whether Process B has greater variability than Process A using the ratio of the Median Absolute Deviations (MAD). The MAD of a sample $\\{x_{1},\\dots,x_{n}\\}$ is defined as\n$$\n\\text{MAD}=\\text{median}\\left(|x_{i}-\\text{median}(\\{x_{j}\\})|\\right).\n$$\nFirst compute the observed statistic $T=\\frac{\\text{MAD}(B)}{\\text{MAD}(A)}$.\n\nFor Sample A, $\\{10,12,13,15\\}$, the sample median is the average of the two middle order statistics:\n$$\n\\text{median}(A)=\\frac{12+13}{2}=12.5.\n$$\nThe absolute deviations from $12.5$ are\n$$\n|10-12.5|=2.5,\\quad |12-12.5|=0.5,\\quad |13-12.5|=0.5,\\quad |15-12.5|=2.5.\n$$\nSorting these gives $0.5,0.5,2.5,2.5$, so the MAD is the median of these four numbers, i.e., the average of the two middle values:\n$$\n\\text{MAD}(A)=\\frac{0.5+2.5}{2}=1.5.\n$$\n\nFor Sample B, $\\{40,48,50,53,61\\}$, the sample median is the middle order statistic:\n$$\n\\text{median}(B)=50.\n$$\nThe absolute deviations from $50$ are\n$$\n|40-50|=10,\\quad |48-50|=2,\\quad |50-50|=0,\\quad |53-50|=3,\\quad |61-50|=11.\n$$\nSorting these gives $0,2,3,10,11$, so the MAD is the middle value:\n$$\n\\text{MAD}(B)=3.\n$$\nTherefore, the observed test statistic is\n$$\nT=\\frac{\\text{MAD}(B)}{\\text{MAD}(A)}=\\frac{3}{1.5}=2.\n$$\n\nFrom the permutation procedure with $N=9999$ permutations, we must count $k$, the number of permuted statistics $T^{*}$ that are greater than or equal to the observed $T=2$. The $30$ largest $T^{*}$ values in descending order are given. Among these, the $18$th largest is $2.02\\geq 2$, while the $19$th largest is $1.992$. Since all remaining values are less than or equal to $1.99$, it follows that exactly $k=18$ permutations satisfy $T^{*}\\geq 2$.\n\nUsing the add-one p-value formula\n$$\np=\\frac{1+k}{1+N}=\\frac{1+18}{1+9999}=\\frac{19}{10000}=1.9\\times 10^{-3}.\n$$\nRounded to four significant figures, this is\n$$\n1.900\\times 10^{-3}.\n$$", "answer": "$$\\boxed{1.900 \\times 10^{-3}}$$", "id": "1951649"}, {"introduction": "While the standard bootstrap is a versatile tool, its validity hinges on the crucial assumption that the data are independent and identically distributed (i.i.d.). In many real-world settings, from environmental science to economics, data possess a hierarchical or clustered structure that violates this assumption. This conceptual exercise [@problem_id:1951652] requires you to think critically about resampling design, asking you to identify the correct bootstrap procedure for estimating a regression coefficient when data points are grouped into clusters. Mastering this concept is essential for applying resampling methods responsibly to complex, dependent data.", "problem": "An environmental scientist is investigating the effect of industrial discharge on mercury levels in fish. Data is collected from $M$ distinct river systems. In each river system $i$ (where $i=1, \\dots, M$), $n_i$ fish are sampled, and for each fish $j$ (where $j=1, \\dots, n_i$), two measurements are recorded: its mercury concentration, $Y_{ij}$, and its proximity to a specific industrial discharge point, $X_{ij}$. The total number of fish sampled is $N = \\sum_{i=1}^M n_i$.\n\nThe scientist proposes a simple linear regression model to describe the relationship:\n$$\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + \\epsilon_{ij}\n$$\nThe primary goal is to construct a 95% confidence interval for the coefficient $\\beta_1$. A key consideration is that observations from the same river system are likely not independent. Unobserved factors common to a river (e.g., local water chemistry, specific food chain characteristics) can induce correlation in the error terms. That is, for fish within the same river $i$, $\\text{Cov}(\\epsilon_{ij}, \\epsilon_{ik}) \\neq 0$ for $j \\neq k$. However, observations from different river systems are assumed to be independent.\n\nTo account for this data structure, several bootstrap-based procedures are considered for estimating the sampling distribution of the Ordinary Least Squares (OLS) estimator, $\\hat{\\beta_1}$. Which of the following statements provides the most accurate description of a valid procedure and the reasoning for its validity in this context?\n\nA. The Non-clustered (Naive) Bootstrap: Create a bootstrap sample by drawing $N$ individual fish with replacement from the full dataset of all fish, ignoring which river they came from. This procedure is valid because, for a large total number of fish $N$, the distribution of the OLS estimator $\\hat{\\beta_1}$ will be approximately normal by the Central Limit Theorem.\n\nB. The Clustered Bootstrap: Create a bootstrap sample by first drawing $M$ river systems with replacement from the original list of $M$ rivers. Then, for each selected river, include all of its associated fish samples in the new bootstrap dataset. This procedure is valid because it treats the river systems as the independent sampling units, thereby preserving the within-river correlation structure of the original data.\n\nC. The Parametric Bootstrap: First, fit the OLS model to the original data to obtain estimates $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ and the set of all $N$ residuals, $\\{e_{ij}\\}$. Then, create a bootstrap outcome $Y_{ij}^*$ for each observation by sampling a residual $e_{ij}^*$ with replacement from $\\{e_{ij}\\}$ and setting $Y_{ij}^* = \\hat{\\beta_0} + \\hat{\\beta_1} X_{ij} + e_{ij}^*$. This procedure is valid because it simulates new data from the fitted model while honoring the original predictor values.\n\nD. The Within-Cluster Resampling Bootstrap: Create a bootstrap sample by keeping the original set of $M$ rivers fixed. Then, for each river $i$, a new set of $n_i$ fish for that river is generated by resampling with replacement from the original $n_i$ fish in that same river. This procedure is valid because it correctly models the sampling variability that occurs within each individual river system.\n\nE. The Fixed-X Bootstrap: Create a bootstrap sample by first fitting the OLS model to get residuals $\\{e_{ij}\\}$. Then, for each fish, create a new outcome $Y_{ij}^* = \\hat{\\beta_0} + \\hat{\\beta_1} X_{ij} + \\delta_{ij}$, where $\\delta_{ij}$ is a random value drawn from a standard normal distribution, $\\mathcal{N}(0, \\hat{\\sigma}^2)$, with $\\hat{\\sigma}^2$ being the estimated variance of the residuals. This procedure is valid as long as the errors are homoscedastic and approximately normal.", "solution": "We are given clustered data: fish are nested within river systems. The linear model is\n$$\nY_{ij}=\\beta_{0}+\\beta_{1}X_{ij}+\\epsilon_{ij},\n$$\nwith independence across rivers and within-river correlation. Formally, for $i \\neq i'$,\n$$\n\\text{Cov}(\\epsilon_{ij},\\epsilon_{i'k})=0,\n$$\nwhile for the same river $i$ and distinct fish $j \\neq k$,\n$$\n\\text{Cov}(\\epsilon_{ij},\\epsilon_{ik}) \\neq 0.\n$$\nA standard way to express this is via a random-effects decomposition,\n$$\n\\epsilon_{ij}=u_{i}+v_{ij},\n$$\nwhere $u_{i}$ is a river-specific component and $v_{ij}$ is an idiosyncratic component, with $\\text{Var}(u_{i})=\\sigma_{u}^{2}$, $\\text{Var}(v_{ij})=\\sigma_{v}^{2}$, $\\text{Cov}(u_{i},v_{ij})=0$, implying\n$$\n\\text{Var}(\\epsilon_{ij})=\\sigma_{u}^{2}+\\sigma_{v}^{2},\\quad \\text{Cov}(\\epsilon_{ij},\\epsilon_{ik})=\\sigma_{u}^{2}\\quad \\text{for }j \\neq k.\n$$\nThe sampling units that are independent are the clusters $\\mathcal{C}_{i}=\\{(X_{ij},Y_{ij}):j=1,\\dots,n_{i}\\}$ across $i=1,\\dots,M$. For the bootstrap to be consistent for the sampling distribution of $\\hat{\\beta}_{1}$ under cluster dependence, the resampling scheme must mimic this dependence structure: resample the independent units (the rivers) and preserve the within-cluster joint distribution.\n\nEvaluation of the proposed procedures:\n- Option A (Naive individual-level bootstrap) draws $N$ fish i.i.d. from the pooled sample, ignoring river membership. This imposes an i.i.d. error structure across all resampled observations and does not reproduce the within-river covariance $\\text{Cov}(\\epsilon_{ij},\\epsilon_{ik})=\\sigma_{u}^{2}$. It therefore yields a bootstrap variance that is typically biased downward when $\\sigma_{u}^{2}0$. Appealing to a central limit theorem for large $N$ does not fix the bootstrap inconsistency; the bootstrap must replicate the correct dependence to estimate the variance of $\\hat{\\beta}_{1}$ under clustering.\n- Option B (Clustered bootstrap) samples rivers with replacement from $\\{1,\\dots,M\\}$ and includes all fish from each selected river, thus resampling $\\{\\mathcal{C}_{i}\\}$ as the i.i.d. units. This preserves the within-river correlation induced by $u_{i}$ and reflects the randomness in which rivers enter the sample. Under standard regularity (in particular, $M \\to \\infty$ with bounded or suitably controlled $n_{i}$), this procedure is asymptotically valid for the sampling distribution of $\\hat{\\beta}_{1}$ with clustered errors.\n- Option C (Residual bootstrap pooling all residuals) samples residuals i.i.d. from the pooled set $\\{e_{ij}\\}$ and constructs $Y_{ij}^{*}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}X_{ij}+e_{ij}^{*}$. This enforces i.i.d. errors in the bootstrap world and destroys the within-river covariance structure; it is therefore invalid for clustered dependence.\n- Option D (Within-cluster resampling with clusters fixed) resamples fish within each observed river, keeping the set of rivers fixed. This conditions on the particular realized cluster effects and does not reflect the between-river sampling variability that drives the sampling distribution of $\\hat{\\beta}_{1}$ when rivers are the independent units. It underestimates variance when cluster effects are random and is not generally valid for inference that targets the population of rivers.\n- Option E (Fixed-$X$ Gaussian parametric bootstrap with i.i.d. noise) draws $\\delta_{ij}\\sim N(0,\\hat{\\sigma}^{2})$ independently and sets $Y_{ij}^{*}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}X_{ij}+\\delta_{ij}$. This imposes homoscedastic i.i.d. errors and fails to encode the within-river covariance; even with normality, the correct parametric bootstrap would need to draw a multivariate normal error vector within each river with the estimated cluster covariance, not independent draws. As stated, it is invalid for clustered dependence.\n\nTherefore, the procedure that correctly treats rivers as the independent sampling units and preserves the within-river correlation is the clustered bootstrap described in option B, and its stated reasoning matches the required justification.", "answer": "$$\\boxed{B}$$", "id": "1951652"}, {"introduction": "Resampling methods provide excellent estimates of uncertainty, but how stable are these estimates themselves? A bootstrap-derived statistic, such as a confidence interval endpoint, is calculated from a random sample and is therefore also a random variable with its own variance. This practice [@problem_id:1951647] introduces the jackknife-after-bootstrap, a clever technique used to quantify this second-order uncertainty. You will calculate the variance of a bootstrap percentile, gaining insight into the stability of bootstrap outputs and learning how different resampling techniques can be powerfully combined.", "problem": "A data scientist is analyzing the stability of a new type of a volatile organic compound sensor. A small sample of five measurements of the sensor's response time to a standard stimulus is recorded, yielding the dataset $X = \\{10, 12, 15, 20, 25\\}$ in milliseconds. The scientist is interested in the variability of the sensor's response, which they decide to quantify using the Interquartile Range (IQR). For a sample of size $m$, sorted as $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(m)}$, the IQR is defined as the difference between the upper and lower quartiles: $Q_3 - Q_1$, where $Q_1 = x_{(\\lceil 0.25m \\rceil)}$ and $Q_3 = x_{(\\lceil 0.75m \\rceil)}$.\n\nTo assess the precision of their statistical estimates, the scientist uses a bootstrap procedure. They are particularly interested in the variability of the 90th percentile of the bootstrap distribution of the IQR, which serves as an estimate for the upper bound of a confidence interval. Let this statistic be denoted by $\\hat{\\phi}$.\n\nTo estimate the variance of $\\hat{\\phi}$, they employ the jackknife-after-bootstrap method. This involves creating $n$ jackknife samples by deleting one observation at a time from the original dataset $X$. For each jackknife sample $X_{(-i)}$, the entire bootstrap procedure is repeated to compute the corresponding 90th percentile of the IQR's bootstrap distribution, denoted as $\\hat{\\phi}_{(-i)}$.\n\nThe analyst has already performed these computationally intensive steps and has provided the following set of values for the statistic computed on each of the five jackknife samples:\n$$ \\{\\hat{\\phi}_{(-1)}, \\hat{\\phi}_{(-2)}, \\hat{\\phi}_{(-3)}, \\hat{\\phi}_{(-4)}, \\hat{\\phi}_{(-5)}\\} = \\{9.50, 11.5, 11.8, 6.00, 5.80\\} $$\nwhere $\\hat{\\phi}_{(-i)}$ is the statistic calculated from the dataset $X$ with the $i$-th element removed (e.g., $\\hat{\\phi}_{(-1)}$ is from the sample $\\{12, 15, 20, 25\\}$).\n\nUsing these provided values, calculate the jackknife-after-bootstrap estimate for the variance of $\\hat{\\phi}$. Round your final answer to three significant figures.", "solution": "We have $n=5$ jackknife replicates $\\{\\hat{\\phi}_{(-i)}\\}_{i=1}^{5}=\\{9.50, 11.5, 11.8, 6.00, 5.80\\}$. The jackknife-after-bootstrap variance estimator is\n$$\n\\widehat{\\operatorname{Var}}_{\\text{JAB}}(\\hat{\\phi})=\\frac{n-1}{n}\\sum_{i=1}^{n}\\left(\\hat{\\phi}_{(-i)}-\\bar{\\hat{\\phi}}_{(\\cdot)}\\right)^{2},\n$$\nwhere\n$$\n\\bar{\\hat{\\phi}}_{(\\cdot)}=\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\phi}_{(-i)}=\\frac{1}{5}(9.50+11.5+11.8+6.00+5.80)=\\frac{44.6}{5}=8.92.\n$$\nDefine $d_{i}=\\hat{\\phi}_{(-i)}-\\bar{\\hat{\\phi}}_{(\\cdot)}$, so\n$$\n\\sum_{i=1}^{5}d_{i}^{2}=(0.58)^{2}+(2.58)^{2}+(2.88)^{2}+(-2.92)^{2}+(-3.12)^{2}=33.548.\n$$\nThus,\n$$\n\\widehat{\\operatorname{Var}}_{\\text{JAB}}(\\hat{\\phi})=\\frac{4}{5}\\times 33.548=26.8384,\n$$\nwhich rounded to three significant figures is $26.8$.", "answer": "$$\\boxed{26.8}$$", "id": "1951647"}]}