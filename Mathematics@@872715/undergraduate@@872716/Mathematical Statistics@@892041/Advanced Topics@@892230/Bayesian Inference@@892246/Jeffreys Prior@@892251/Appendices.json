{"hands_on_practices": [{"introduction": "We begin with a foundational exercise to build your skills in deriving a Jeffreys prior for a single-parameter model. This practice focuses on the scale parameter, $\\sigma$, of a Rayleigh distribution, a common task in fields like signal processing. By working through the calculation of the Fisher information from the log-likelihood function [@problem_id:1925852], you will see how Jeffreys' rule formally justifies the classic non-informative prior $\\pi(\\sigma) \\propto 1/\\sigma$ for scale parameters, providing a solid basis for its widespread use.", "problem": "In the field of wireless communications, the amplitude of a received signal that has undergone multipath scattering is often modeled by a Rayleigh distribution. A key task for a receiver is to estimate the characteristics of the noise and signal environment. One approach is to use Bayesian methods, which require a prior distribution for the model's parameters.\n\nConsider a single measurement $x$ drawn from a Rayleigh distribution with a probability density function (PDF) given by:\n$$f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\nfor $x \\geq 0$. The parameter $\\sigma > 0$ is the scale parameter of the distribution.\n\nTo begin a Bayesian analysis without strong prior beliefs, one might use a non-informative prior. Jeffreys' prior is a popular choice, derived from the structure of the likelihood function. For a parameter $\\theta$, Jeffreys' prior $\\pi(\\theta)$ is defined to be proportional to the square root of the Fisher information, $I(\\theta)$:\n$$\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$$\nThe Fisher information for a single observation is given by the formula:\n$$I(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x; \\theta)\\right]$$\nwhere $E[\\cdot]$ denotes the expectation with respect to the distribution $f(x; \\theta)$.\n\nDetermine Jeffreys' prior for the scale parameter $\\sigma$ of the Rayleigh distribution. Your answer should be the function of $\\sigma$ to which the prior $\\pi(\\sigma)$ is proportional.", "solution": "We are given a single observation from a Rayleigh distribution with density\n$$\nf(x;\\sigma)=\\frac{x}{\\sigma^{2}}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right),\\quad x\\geq 0,\\ \\sigma>0.\n$$\nJeffreys' prior for a parameter is proportional to the square root of the Fisher information. For the scale parameter $\\sigma$, we compute the Fisher information using\n$$\nI(\\sigma)=-E\\left[\\frac{\\partial^{2}}{\\partial \\sigma^{2}}\\ln f(x;\\sigma)\\right].\n$$\nFirst, compute the log-likelihood for one observation:\n$$\n\\ln f(x;\\sigma)=\\ln x-2\\ln \\sigma-\\frac{x^{2}}{2\\sigma^{2}}.\n$$\nDifferentiate with respect to $\\sigma$:\n$$\n\\frac{\\partial}{\\partial \\sigma}\\ln f(x;\\sigma)=-\\frac{2}{\\sigma}+\\frac{x^{2}}{\\sigma^{3}}.\n$$\nDifferentiate again:\n$$\n\\frac{\\partial^{2}}{\\partial \\sigma^{2}}\\ln f(x;\\sigma)=\\frac{2}{\\sigma^{2}}-\\frac{3x^{2}}{\\sigma^{4}}.\n$$\nTherefore,\n$$\nI(\\sigma)=-E\\left[\\frac{2}{\\sigma^{2}}-\\frac{3x^{2}}{\\sigma^{4}}\\right]=-\\frac{2}{\\sigma^{2}}+\\frac{3\\,E[x^{2}]}{\\sigma^{4}}.\n$$\nWe now compute $E[x^{2}]$ under the Rayleigh$(\\sigma)$ distribution:\n$$\nE[x^{2}]=\\int_{0}^{\\infty}x^{2}\\,\\frac{x}{\\sigma^{2}}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)dx=\\frac{1}{\\sigma^{2}}\\int_{0}^{\\infty}x^{3}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)dx.\n$$\nUse the substitution $u=\\frac{x^{2}}{2\\sigma^{2}}$ so that $du=\\frac{x\\,dx}{\\sigma^{2}}$ and $x^{3}dx=2\\sigma^{4}u\\,du$. Then\n$$\nE[x^{2}]=\\frac{1}{\\sigma^{2}}\\int_{0}^{\\infty}2\\sigma^{4}u\\,\\exp(-u)\\,du=2\\sigma^{2}\\int_{0}^{\\infty}u\\,\\exp(-u)\\,du=2\\sigma^{2}.\n$$\nSubstitute $E[x^{2}]=2\\sigma^{2}$ into the Fisher information:\n$$\nI(\\sigma)=-\\frac{2}{\\sigma^{2}}+\\frac{3\\cdot 2\\sigma^{2}}{\\sigma^{4}}=\\frac{4}{\\sigma^{2}}.\n$$\nJeffreys' prior is proportional to $\\sqrt{I(\\sigma)}$, hence\n$$\n\\pi(\\sigma)\\propto \\sqrt{I(\\sigma)}=\\sqrt{\\frac{4}{\\sigma^{2}}}=\\frac{2}{\\sigma}\\propto \\frac{1}{\\sigma},\\quad \\sigma>0.\n$$", "answer": "$$\\boxed{\\frac{1}{\\sigma}}$$", "id": "1925852"}, {"introduction": "Next, we explore a more nuanced scenario to demonstrate the sensitivity of the Jeffreys prior to the data-generating process. This problem investigates the prior for the mean $\\mu$ of a normal distribution, but with the added constraint that observations are truncated, a common situation in experimental measurements. Calculating the prior in this context [@problem_id:1925861] reveals that the resulting prior is significantly more complex than for a standard normal model, highlighting that the prior is intrinsically linked to the full structure of the likelihood function.", "problem": "In Bayesian statistics, Jeffreys' prior provides a non-informative prior distribution for a parameter. Let's consider a scenario where a physical measurement is modeled by a normal distribution with an unknown mean $\\mu$ and a known, constant variance $\\sigma^2$. Due to the nature of the measurement apparatus, only values greater than zero can be observed. This scenario corresponds to sampling from a normal distribution that is left-truncated at zero.\n\nThe probability density function (PDF) for an observation $y$ from this truncated distribution is given by:\n$$ p(y|\\mu, \\sigma) = \\frac{\\frac{1}{\\sigma} \\phi\\left(\\frac{y-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sigma}\\right)}, \\quad \\text{for } y > 0 $$\nand $p(y|\\mu, \\sigma) = 0$ for $y \\le 0$. Here, $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{z^2}{2})$ is the PDF of the standard normal distribution, and $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$ is its Cumulative Distribution Function (CDF).\n\nYour task is to determine the Jeffreys' prior for the parameter $\\mu$. Provide the final expression for the prior, $\\pi_J(\\mu)$. Your answer should be an analytical expression in terms of $\\mu$, the known constant $\\sigma$, and the functions $\\phi(\\cdot)$ and $\\Phi(\\cdot)$. Any constants of proportionality should be omitted from the final answer.", "solution": "We consider a single observation from the left-truncated normal distribution at zero with known variance $\\sigma^{2}$ and unknown mean $\\mu$. The probability density is\n$$\np(y \\mid \\mu,\\sigma) = \\frac{\\frac{1}{\\sigma}\\,\\phi\\left(\\frac{y-\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sigma}\\right)}, \\quad y>0,\n$$\nand $p(y \\mid \\mu,\\sigma)=0$ for $y \\le 0$. The Jeffreys prior for $\\mu$ is given (up to a multiplicative constant independent of $\\mu$) by\n$$\n\\pi_{J}(\\mu) \\propto \\sqrt{I(\\mu)},\n$$\nwhere $I(\\mu)$ is the Fisher information for one observation.\n\nLet $z = \\frac{y-\\mu}{\\sigma}$ and $u = \\frac{\\mu}{\\sigma}$. The log-likelihood for one observation is\n$$\n\\ell(\\mu; y) = -\\ln \\sigma + \\ln \\phi(z) - \\ln \\Phi(u).\n$$\nDifferentiate with respect to $\\mu$. Using $\\frac{d}{dz}\\ln \\phi(z) = -z$, $\\frac{dz}{d\\mu} = -\\frac{1}{\\sigma}$, and $\\frac{d}{du}\\ln \\Phi(u) = \\frac{\\phi(u)}{\\Phi(u)}$, $\\frac{du}{d\\mu} = \\frac{1}{\\sigma}$, the score function is\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = \\frac{z}{\\sigma} - \\frac{1}{\\sigma}\\frac{\\phi(u)}{\\Phi(u)} = \\frac{1}{\\sigma^{2}}(y-\\mu) - \\frac{1}{\\sigma}\\lambda(u),\n$$\nwhere $\\lambda(u) = \\frac{\\phi(u)}{\\Phi(u)}$ is the inverse Mills ratio. Differentiating again with respect to $\\mu$,\n$$\n\\frac{\\partial^{2} \\ell}{\\partial \\mu^{2}} = -\\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma}\\frac{d}{d\\mu}\\lambda(u) = -\\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma^{2}}\\lambda'(u),\n$$\nsince $\\frac{d}{d\\mu}\\lambda(u) = \\lambda'(u)\\frac{1}{\\sigma}$. By the quotient rule and the identities $\\phi'(u) = -u\\phi(u)$ and $\\Phi'(u) = \\phi(u)$, we have\n$$\n\\lambda'(u) = \\frac{\\phi'(u)\\Phi(u) - \\phi(u)\\Phi'(u)}{\\Phi(u)^{2}} = -u\\frac{\\phi(u)}{\\Phi(u)} - \\left(\\frac{\\phi(u)}{\\Phi(u)}\\right)^{2} = -u\\lambda(u) - \\lambda(u)^{2}.\n$$\nTherefore,\n$$\n\\frac{\\partial^{2} \\ell}{\\partial \\mu^{2}} = -\\frac{1}{\\sigma^{2}}\\left[1 + \\lambda'(u)\\right] = -\\frac{1}{\\sigma^{2}}\\left[1 - u\\lambda(u) - \\lambda(u)^{2}\\right].\n$$\nThe Fisher information is the negative expectation of the second derivative, which here does not depend on $y$, so\n$$\nI(\\mu) = -E\\left[\\frac{\\partial^{2} \\ell}{\\partial \\mu^{2}}\\right] = \\frac{1}{\\sigma^{2}}\\left[1 - u\\lambda(u) - \\lambda(u)^{2}\\right], \\quad u = \\frac{\\mu}{\\sigma}, \\quad \\lambda(u) = \\frac{\\phi(u)}{\\Phi(u)}.\n$$\nThus the Jeffreys prior is\n$$\n\\pi_{J}(\\mu) \\propto \\sqrt{I(\\mu)} = \\frac{1}{\\sigma}\\sqrt{1 - \\frac{\\mu}{\\sigma}\\frac{\\phi\\left(\\frac{\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sigma}\\right)} - \\left(\\frac{\\phi\\left(\\frac{\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sigma}\\right)}\\right)^{2}}.\n$$\nSince any constant factor independent of $\\mu$ can be omitted, we drop the factor $\\frac{1}{\\sigma}$ and obtain the Jeffreys prior up to proportionality.", "answer": "$$\\boxed{\\sqrt{1 - \\frac{\\mu}{\\sigma}\\frac{\\phi\\left(\\frac{\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sigma}\\right)} - \\left(\\frac{\\phi\\left(\\frac{\\mu}{\\sigma}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sigma}\\right)}\\right)^{2}}}$$", "id": "1925861"}, {"introduction": "Finally, we extend our analysis to the more realistic case of multi-parameter models, which are prevalent in scientific research. This exercise introduces the multivariate Jeffreys prior, which requires calculating the Fisher Information *Matrix* and its determinant for the Gamma distribution's shape ($\\alpha$) and rate ($\\beta$) parameters. Mastering this procedure [@problem_id:1925872] is a crucial step toward applying objective Bayesian methods to complex, real-world problems involving multiple unknown quantities.", "problem": "In Bayesian statistics, an uninformative prior can be constructed using a method proposed by Harold Jeffreys. For a model with a parameter vector $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\dots, \\theta_k)$ and likelihood function $L(\\boldsymbol{\\theta}|x)$ based on data $x$, the multivariate Jeffreys' prior is defined as being proportional to the square root of the determinant of the Fisher Information Matrix, $\\mathcal{I}(\\boldsymbol{\\theta})$. That is,\n$$ \\pi_J(\\boldsymbol{\\theta}) \\propto \\sqrt{\\det(\\mathcal{I}(\\boldsymbol{\\theta}))} $$\nThe $(i,j)$-th element of the Fisher Information Matrix is given by\n$$ \\mathcal{I}_{ij}(\\boldsymbol{\\theta}) = -E\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\ln L(\\boldsymbol{\\theta}|X)\\right] $$\nwhere the expectation is taken with respect to the random variable $X$.\n\nConsider a random variable $X$ that follows a Gamma distribution, which is frequently used to model waiting times or rainfall amounts. Its Probability Density Function (PDF) is given by:\n$$ f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) \\quad \\text{for } x > 0 $$\nwhere $\\alpha > 0$ is the shape parameter, $\\beta > 0$ is the rate parameter, and $\\Gamma(\\alpha)$ is the Gamma function.\n\nFor a single observation $x$ from this distribution, determine the bivariate Jeffreys' prior, $\\pi_J(\\alpha, \\beta)$. Your task is to find the function of $\\alpha$ and $\\beta$ to which this prior is proportional.\n\nThe following definitions may be useful:\n- The digamma function: $\\psi(\\alpha) = \\frac{d}{d\\alpha} \\ln \\Gamma(\\alpha)$\n- The trigamma function: $\\psi_1(\\alpha) = \\frac{d}{d\\alpha} \\psi(\\alpha) = \\frac{d^2}{d\\alpha^2} \\ln \\Gamma(\\alpha)$\n\nProvide the final expression in terms of $\\alpha$, $\\beta$, and the trigamma function $\\psi_1(\\alpha)$.", "solution": "The problem asks for the expression to which the bivariate Jeffreys' prior for the parameters $(\\alpha, \\beta)$ of a Gamma distribution is proportional. According to the definition provided, $\\pi_J(\\alpha, \\beta) \\propto \\sqrt{\\det(\\mathcal{I}(\\alpha, \\beta))}$. We must first compute the Fisher Information Matrix, $\\mathcal{I}(\\alpha, \\beta)$.\n\n**Step 1: Find the log-likelihood function.**\nThe likelihood function for a single observation $x$ from a Gamma$(\\alpha, \\beta)$ distribution is the PDF itself:\n$$ L(\\alpha, \\beta|x) = f(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} \\exp(-\\beta x) $$\nThe log-likelihood, denoted $\\ell(\\alpha, \\beta|x)$, is:\n$$ \\ell(\\alpha, \\beta|x) = \\ln L(\\alpha, \\beta|x) = \\alpha \\ln \\beta - \\ln \\Gamma(\\alpha) + (\\alpha-1) \\ln x - \\beta x $$\n\n**Step 2: Calculate the second-order partial derivatives of the log-likelihood.**\nWe first compute the first-order partial derivatives with respect to $\\alpha$ and $\\beta$.\n$$ \\frac{\\partial \\ell}{\\partial \\alpha} = \\ln \\beta - \\frac{d}{d\\alpha}\\ln \\Gamma(\\alpha) + \\ln x = \\ln \\beta - \\psi(\\alpha) + \\ln x $$\n$$ \\frac{\\partial \\ell}{\\partial \\beta} = \\frac{\\alpha}{\\beta} - x $$\nNow, we compute the three unique second-order partial derivatives:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = \\frac{\\partial}{\\partial \\alpha} (\\ln \\beta - \\psi(\\alpha) + \\ln x) = -\\frac{d}{d\\alpha}\\psi(\\alpha) = -\\psi_1(\\alpha) $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta^2} = \\frac{\\partial}{\\partial \\beta} \\left(\\frac{\\alpha}{\\beta} - x\\right) = -\\frac{\\alpha}{\\beta^2} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\alpha} = \\frac{\\partial}{\\partial \\beta} \\left(\\frac{\\partial \\ell}{\\partial \\alpha}\\right) = \\frac{\\partial}{\\partial \\beta} (\\ln \\beta - \\psi(\\alpha) + \\ln x) = \\frac{1}{\\beta} $$\nBy Clairaut's theorem, $\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} = \\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\alpha}$, so we have all the necessary components.\n\n**Step 3: Compute the elements of the Fisher Information Matrix.**\nThe elements of the Fisher Information Matrix, $\\mathcal{I}(\\alpha, \\beta)$, are given by $\\mathcal{I}_{ij} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_i \\partial \\theta_j}\\right]$. In our case, the parameter vector is $\\boldsymbol{\\theta} = (\\alpha, \\beta)$.\n\n$$ \\mathcal{I}_{11} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\alpha^2}\\right] = -E[-\\psi_1(\\alpha)] $$\nSince $\\psi_1(\\alpha)$ is a function of $\\alpha$ only and does not depend on the random variable $X$, its expectation is itself.\n$$ \\mathcal{I}_{11} = \\psi_1(\\alpha) $$\nSimilarly for $\\mathcal{I}_{22}$:\n$$ \\mathcal{I}_{22} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta^2}\\right] = -E\\left[-\\frac{\\alpha}{\\beta^2}\\right] = \\frac{\\alpha}{\\beta^2} $$\nAnd for the off-diagonal elements $\\mathcal{I}_{12} = \\mathcal{I}_{21}$:\n$$ \\mathcal{I}_{12} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\alpha}\\right] = -E\\left[\\frac{1}{\\beta}\\right] = -\\frac{1}{\\beta} $$\n\n**Step 4: Construct the Fisher Information Matrix and compute its determinant.**\nThe Fisher Information Matrix is:\n$$ \\mathcal{I}(\\alpha, \\beta) = \\begin{pmatrix} \\mathcal{I}_{11} & \\mathcal{I}_{12} \\\\ \\mathcal{I}_{21} & \\mathcal{I}_{22} \\end{pmatrix} = \\begin{pmatrix} \\psi_1(\\alpha) & -1/\\beta \\\\ -1/\\beta & \\alpha/\\beta^2 \\end{pmatrix} $$\nThe determinant of this matrix is:\n$$ \\det(\\mathcal{I}(\\alpha, \\beta)) = (\\psi_1(\\alpha))\\left(\\frac{\\alpha}{\\beta^2}\\right) - \\left(-\\frac{1}{\\beta}\\right)\\left(-\\frac{1}{\\beta}\\right) $$\n$$ \\det(\\mathcal{I}(\\alpha, \\beta)) = \\frac{\\alpha \\psi_1(\\alpha)}{\\beta^2} - \\frac{1}{\\beta^2} = \\frac{\\alpha \\psi_1(\\alpha) - 1}{\\beta^2} $$\n\n**Step 5: Determine the Jeffreys' prior.**\nThe Jeffreys' prior is proportional to the square root of the determinant of the Fisher Information Matrix.\n$$ \\pi_J(\\alpha, \\beta) \\propto \\sqrt{\\det(\\mathcal{I}(\\alpha, \\beta))} = \\sqrt{\\frac{\\alpha \\psi_1(\\alpha) - 1}{\\beta^2}} $$\nSince $\\beta > 0$, we can simplify this expression:\n$$ \\pi_J(\\alpha, \\beta) \\propto \\frac{\\sqrt{\\alpha \\psi_1(\\alpha) - 1}}{\\beta} $$\nThis is the final expression to which the bivariate Jeffreys' prior is proportional. (Note: For $\\alpha > 0$, a known property of the trigamma function is $\\alpha \\psi_1(\\alpha) > 1$, so the term under the square root is always positive).", "answer": "$$\\boxed{\\frac{\\sqrt{\\alpha \\psi_1(\\alpha) - 1}}{\\beta}}$$", "id": "1925872"}]}