{"hands_on_practices": [{"introduction": "This first exercise demonstrates the foundational technique of empirical Bayes: estimating the parameters of a prior distribution directly from the data. We explore a scenario involving LED lifetimes, which are modeled with an Exponential distribution whose rate parameter varies. By deriving the marginal moments of the observed lifetimes, you will use the method of moments to learn the characteristics of the Gamma prior governing these rates [@problem_id:1915124]. This practice is a clear illustration of how we can let the data inform our prior beliefs.", "problem": "A materials scientist is studying the reliability of a newly developed type of high-intensity Light Emitting Diode (LED). The LEDs are manufactured in different production batches. The scientist models the lifetime $T$ of an LED from a given batch $i$ (in thousands of hours) as a random variable following an exponential distribution with a rate parameter $\\lambda_i$. The probability density function (PDF) for the lifetime $T$ is given by $f(t|\\lambda_i) = \\lambda_i \\exp(-\\lambda_i t)$ for $t > 0$.\n\nDue to minor variations in the manufacturing process, the rate parameter $\\lambda_i$ itself is considered a random variable that differs from batch to batch. This variability is modeled by assuming that $\\lambda_i$ is drawn from a Gamma distribution with a shape parameter $\\alpha > 0$ and a rate parameter $\\beta > 0$. The PDF for $\\lambda_i$ is given by $g(\\lambda|\\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$, where $\\Gamma(\\cdot)$ is the gamma function.\n\nThe scientist tests one LED from each of four different production batches and records their lifetimes. The observed lifetimes, in thousands of hours, are:\n$$ \\{20.0, 80.0, 200.0, 700.0\\} $$\nUsing the method of moments on the marginal distribution of the lifetimes, determine the estimates for the parameters $\\alpha$ and $\\beta$ of the Gamma distribution. Present your answers for the estimate of $\\alpha$ and the estimate of $\\beta$, in that order. Round your final numerical answers to four significant figures.", "solution": "The problem requires us to find the method-of-moments estimates for the parameters $\\alpha$ and $\\beta$ of a Gamma prior distribution, given data from an Exponential likelihood. This is a common empirical Bayes setup. The model is specified as:\n1.  Likelihood: $T_i | \\lambda_i \\sim \\text{Exponential}(\\lambda_i)$\n2.  Prior: $\\lambda_i \\sim \\text{Gamma}(\\alpha, \\beta)$\n\nThe method of moments involves equating the theoretical moments of the marginal distribution of $T_i$ to the sample moments calculated from the data. We need to find the first two theoretical moments, $E[T_i]$ and $E[T_i^2]$.\n\nFirst, let's find the first moment, $E[T_i]$. We use the law of total expectation: $E[T_i] = E[E[T_i | \\lambda_i]]$.\nFor an exponential distribution with rate $\\lambda_i$, the expectation is $E[T_i | \\lambda_i] = \\frac{1}{\\lambda_i}$.\nSo, we need to calculate $E[T_i] = E\\left[\\frac{1}{\\lambda_i}\\right]$, where the outer expectation is over the distribution of $\\lambda_i$, which is $\\text{Gamma}(\\alpha, \\beta)$.\n$$ E[\\lambda_i^{-1}] = \\int_0^\\infty \\lambda^{-1} g(\\lambda|\\alpha, \\beta) d\\lambda = \\int_0^\\infty \\lambda^{-1} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda) d\\lambda $$\n$$ E[\\lambda_i^{-1}] = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^\\infty \\lambda^{\\alpha-2} \\exp(-\\beta \\lambda) d\\lambda $$\nThe integral is the kernel of a Gamma distribution with shape parameter $\\alpha-1$ and rate parameter $\\beta$. The value of this integral is $\\frac{\\Gamma(\\alpha-1)}{\\beta^{\\alpha-1}}$. This requires $\\alpha-1 > 0$, or $\\alpha > 1$.\n$$ E[\\lambda_i^{-1}] = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha-1)}{\\beta^{\\alpha-1}} = \\frac{\\beta}{\\Gamma(\\alpha)/\\Gamma(\\alpha-1)} $$\nUsing the property $\\Gamma(z) = (z-1)\\Gamma(z-1)$, we have $\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1)$.\n$$ E[T_i] = E[\\lambda_i^{-1}] = \\frac{\\beta}{\\alpha-1} $$\n\nNext, we find the second moment, $E[T_i^2]$. We use the law of total expectation again: $E[T_i^2] = E[E[T_i^2 | \\lambda_i]]$.\nFor an exponential distribution, the second moment is $E[T_i^2 | \\lambda_i] = \\frac{2}{\\lambda_i^2}$.\nSo, we need to calculate $E[T_i^2] = E\\left[\\frac{2}{\\lambda_i^2}\\right] = 2 E[\\lambda_i^{-2}]$.\n$$ E[\\lambda_i^{-2}] = \\int_0^\\infty \\lambda^{-2} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda) d\\lambda = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^\\infty \\lambda^{\\alpha-3} \\exp(-\\beta \\lambda) d\\lambda $$\nThe integral is the kernel of a Gamma distribution with shape parameter $\\alpha-2$ and rate parameter $\\beta$. Its value is $\\frac{\\Gamma(\\alpha-2)}{\\beta^{\\alpha-2}}$, which requires $\\alpha-2 > 0$, or $\\alpha > 2$.\n$$ E[\\lambda_i^{-2}] = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha-2)}{\\beta^{\\alpha-2}} = \\frac{\\beta^2}{\\Gamma(\\alpha)/\\Gamma(\\alpha-2)} $$\nUsing the property $\\Gamma(\\alpha) = (\\alpha-1)(\\alpha-2)\\Gamma(\\alpha-2)$.\n$$ E[\\lambda_i^{-2}] = \\frac{\\beta^2}{(\\alpha-1)(\\alpha-2)} $$\nTherefore, the second moment of $T_i$ is:\n$$ E[T_i^2] = 2 E[\\lambda_i^{-2}] = \\frac{2\\beta^2}{(\\alpha-1)(\\alpha-2)} $$\n\nNow, we calculate the sample moments from the data $\\{20.0, 80.0, 200.0, 700.0\\}$. Let $n=4$.\nThe first sample moment (sample mean) is:\n$$ \\bar{t} = \\frac{1}{n} \\sum_{i=1}^n t_i = \\frac{20.0 + 80.0 + 200.0 + 700.0}{4} = \\frac{1000.0}{4} = 250.0 $$\nThe second sample moment is:\n$$ m_2 = \\frac{1}{n} \\sum_{i=1}^n t_i^2 = \\frac{20.0^2 + 80.0^2 + 200.0^2 + 700.0^2}{4} $$\n$$ m_2 = \\frac{400 + 6400 + 40000 + 490000}{4} = \\frac{536800}{4} = 134200 $$\n\nWe equate the theoretical moments to the sample moments to get a system of equations for the estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$:\n1.  $\\bar{t} = \\frac{\\hat{\\beta}}{\\hat{\\alpha}-1} \\implies 250.0 = \\frac{\\hat{\\beta}}{\\hat{\\alpha}-1}$\n2.  $m_2 = \\frac{2\\hat{\\beta}^2}{(\\hat{\\alpha}-1)(\\hat{\\alpha}-2)} \\implies 134200 = \\frac{2\\hat{\\beta}^2}{(\\hat{\\alpha}-1)(\\hat{\\alpha}-2)}$\n\nFrom equation (1), we solve for $\\hat{\\beta}$:\n$\\hat{\\beta} = 250.0(\\hat{\\alpha}-1)$.\n\nSubstitute this expression for $\\hat{\\beta}$ into equation (2):\n$$ 134200 = \\frac{2 [250.0(\\hat{\\alpha}-1)]^2}{(\\hat{\\alpha}-1)(\\hat{\\alpha}-2)} = \\frac{2 (250.0^2) (\\hat{\\alpha}-1)^2}{(\\hat{\\alpha}-1)(\\hat{\\alpha}-2)} $$\n$$ 134200 = \\frac{2 (62500) (\\hat{\\alpha}-1)}{\\hat{\\alpha}-2} = \\frac{125000 (\\hat{\\alpha}-1)}{\\hat{\\alpha}-2} $$\nNow, we solve for $\\hat{\\alpha}$:\n$$ 134200 (\\hat{\\alpha}-2) = 125000 (\\hat{\\alpha}-1) $$\n$$ 134200\\hat{\\alpha} - 268400 = 125000\\hat{\\alpha} - 125000 $$\n$$ (134200 - 125000)\\hat{\\alpha} = 268400 - 125000 $$\n$$ 9200\\hat{\\alpha} = 143400 $$\n$$ \\hat{\\alpha} = \\frac{143400}{9200} = 15.5869565... $$\nRounding to four significant figures, $\\hat{\\alpha} = 15.59$.\n\nNow we find $\\hat{\\beta}$ using the expression $\\hat{\\beta} = 250.0(\\hat{\\alpha}-1)$:\n$$ \\hat{\\beta} = 250.0 (15.5869565... - 1) = 250.0 (14.5869565...) = 3646.73913... $$\nRounding to four significant figures, $\\hat{\\beta} = 3647$.\n\nThe estimates are $\\hat{\\alpha} = 15.59$ and $\\hat{\\beta} = 3647$.", "answer": "$$\\boxed{\\begin{pmatrix} 15.59 & 3647 \\end{pmatrix}}$$", "id": "1915124"}, {"introduction": "Estimating a prior is a means to an end. The ultimate goal is to perform more robust inference on individual units by \"borrowing strength\" from the entire collection. This problem [@problem_id:1915179] takes you through the full empirical Bayes workflow using the common Beta-Binomial model for success rates. You will first estimate the parameters of the Beta prior from summary statistics and then use this empirical prior to calculate the posterior variance for a specific hospital, showcasing how the population-level information helps refine our uncertainty about a single case.", "problem": "A team of biostatisticians is analyzing the success rates of a novel surgical procedure performed at a large number of hospitals. For each hospital $i$, the number of successful surgeries, $X_i$, out of a total of $n_i$ surgeries is recorded. The team adopts a hierarchical model to account for variability among hospitals.\n\nThe model assumes that for each hospital $i$, the number of successes $X_i$ follows a Binomial distribution with parameters $n_i$ and $p_i$, where $p_i$ is the true success probability for that specific hospital. That is, $X_i | p_i \\sim \\text{Binomial}(n_i, p_i)$. Furthermore, it is assumed that the true success rates $p_i$ are themselves random variables drawn from a common underlying distribution, which is modeled as a Beta distribution with shape parameters $\\alpha$ and $\\beta$. That is, $p_i \\sim \\text{Beta}(\\alpha, \\beta)$.\n\nThe parameters $\\alpha$ and $\\beta$ of the prior distribution are unknown. To estimate them, the team uses an empirical Bayes approach based on the method of moments. A study was conducted where each hospital performed the same number of surgeries, so $n_i = n = 50$ for all $i$. From the data collected across all hospitals, the sample mean number of successes was calculated to be $\\bar{X} = 40.5$, and the sample variance was $S^2 = 75$.\n\nConsider a specific hospital, Hospital J, which reported $X_J = 45$ successes in its $n=50$ surgeries. Using the empirical Bayes methodology, first estimate the prior parameters $\\alpha$ and $\\beta$ from the summary statistics ($\\bar{X}$, $S^2$, $n$), and then determine the posterior variance of the success probability $p_J$ for Hospital J.\n\nCalculate this posterior variance and report your answer as a real number. Round your final answer to four significant figures.", "solution": "We model $X_{i} \\mid p_{i} \\sim \\text{Binomial}(n, p_{i})$ with $p_{i} \\sim \\text{Beta}(\\alpha, \\beta)$. Marginally, $X_{i}$ follows a Beta-Binomial distribution with\n$$\n\\mu := \\mathbb{E}[p_{i}] = \\frac{\\alpha}{\\alpha + \\beta}, \n\\quad\nA := \\alpha + \\beta,\n$$\nand for $X_{i}$,\n$$\n\\mathbb{E}[X_{i}] = n \\mu, \n\\quad \n\\operatorname{Var}(X_{i}) = n \\mu (1 - \\mu)\\frac{n + A}{A + 1}.\n$$\n\nMethod of moments uses the sample mean and variance of $X_{i}$ across hospitals. Given $n=50$, $\\bar{X} = 40.5$, and $S^{2} = 75$, we have\n$$\n\\hat{\\mu} = \\frac{\\bar{X}}{n} = \\frac{40.5}{50} = 0.81,\n\\quad\nn \\hat{\\mu}(1 - \\hat{\\mu}) = 50 \\cdot 0.81 \\cdot 0.19 = 7.695.\n$$\nSet the theoretical variance equal to the sample variance:\n$$\n75 = 7.695 \\cdot \\frac{50 + A}{A + 1}.\n$$\nDefine $r := \\frac{75}{7.695} = \\frac{75000}{7695} = \\frac{15000}{1539}$. Then\n$$\n\\frac{50 + A}{A + 1} = r \n\\implies\n50 + A = r(A + 1)\n\\implies\nA(1 - r) = r - 50,\n$$\nso\n$$\n\\hat{A} = \\frac{r - 50}{1 - r} = \\frac{50 - r}{r - 1}.\n$$\nUsing $r = \\frac{15000}{1539}$,\n$$\n\\hat{A} = \\frac{50 - \\frac{15000}{1539}}{\\frac{15000}{1539} - 1}\n= \\frac{50 \\cdot 1539 - 15000}{15000 - 1539}\n= \\frac{61950}{13461}\n= \\frac{20650}{4487}\n\\approx 4.60218.\n$$\nHence\n$$\n\\hat{\\alpha} = \\hat{\\mu}\\hat{A} = 0.81 \\cdot \\hat{A} = \\frac{81}{100}\\cdot\\frac{20650}{4487} = \\frac{33453}{8974} \\approx 3.72777,\n$$\n$$\n\\hat{\\beta} = (1 - \\hat{\\mu})\\hat{A} = 0.19 \\cdot \\hat{A} = \\frac{19}{100}\\cdot\\frac{20650}{4487} = \\frac{7847}{8974} \\approx 0.87441.\n$$\n\nFor Hospital J, with $X_{J} = 45$ and $n = 50$, the posterior for $p_{J}$ is\n$$\np_{J} \\mid X_{J} \\sim \\text{Beta}(\\hat{\\alpha} + X_{J}, \\hat{\\beta} + n - X_{J})\n= \\text{Beta}\\left(\\hat{\\alpha} + 45, \\hat{\\beta} + 5\\right).\n$$\nThus,\n$$\na_{\\text{post}} = \\hat{\\alpha} + 45 \\approx 48.72777,\n\\quad\nb_{\\text{post}} = \\hat{\\beta} + 5 \\approx 5.87441,\n\\quad\ns_{\\text{post}} = a_{\\text{post}} + b_{\\text{post}} \\approx 54.60218.\n$$\nThe variance of a $\\text{Beta}(a,b)$ distribution is\n$$\n\\operatorname{Var}(p) = \\frac{ab}{(a + b)^{2}(a + b + 1)}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(p_{J} \\mid X_{J})\n= \\frac{a_{\\text{post}}\\,b_{\\text{post}}}{s_{\\text{post}}^{2}(s_{\\text{post}} + 1)}\n\\approx \\frac{48.72777 \\times 5.87441}{(54.60218)^{2} \\times 55.60218}.\n$$\nNumerically,\n$$\na_{\\text{post}}\\,b_{\\text{post}} \\approx 286.24708,\n\\quad\ns_{\\text{post}}^{2}(s_{\\text{post}} + 1) \\approx 165772.23163,\n$$\nso\n$$\n\\operatorname{Var}(p_{J} \\mid X_{J}) \\approx \\frac{286.24708}{165772.23163} \\approx 0.00172675.\n$$\nRounded to four significant figures, the posterior variance is $0.001727$.", "answer": "$$\\boxed{0.001727}$$", "id": "1915179"}, {"introduction": "Real-world data analysis often comes with unexpected challenges, and empirical Bayes methods are no exception. This practice confronts a common and important issue that arises in the Normal-Normal hierarchical model: obtaining a negative estimate for a variance component like $\\tau^2$. By working through this exercise [@problem_id:1915166], you will not only calculate such an estimate but also learn the standard procedure for handling this seemingly nonsensical result and its proper statistical interpretation.", "problem": "A team of education researchers is evaluating a new teaching method across $J=4$ different school districts. For each district $j$, they obtain an estimate of the average student score improvement, denoted by $Y_j$. This estimate is known to have a specific sampling variance $\\sigma_j^2$ which depends on the number of students in that district's study. The researchers adopt an empirical Bayes approach, modeling the score improvements with a Normal-Normal hierarchical model.\n\nThe model is specified as follows:\n1.  The observed improvement in each district, $Y_j$, is an estimate of the true but unknown effectiveness $\\theta_j$ for that district. The sampling distribution is $Y_j | \\theta_j \\sim N(\\theta_j, \\sigma_j^2)$.\n2.  The true effectiveness levels $\\theta_j$ for the different districts are assumed to be drawn from a common population distribution, which is modeled by a normal prior: $\\theta_j \\sim N(\\mu, \\tau^2)$. Here, $\\mu$ represents the overall average effectiveness of the method, and $\\tau^2$ represents the true variance in effectiveness between districts.\n\nThe collected data are:\n-   Observed score improvements: $Y = \\{8.00, 9.00, 11.00, 12.00\\}$\n-   Known sampling variances: $\\sigma^2 = \\{5.00, 6.00, 7.00, 8.00\\}$\n\nUsing the method of moments on the observed data $\\{Y_j\\}$, an analyst is tasked with estimating the between-district variance hyperparameter, $\\tau^2$. Based on the result of this estimation, a conclusion must be drawn about the model.\n\nSelect the option that correctly states the initial method-of-moments estimate for $\\tau^2$ (rounded to three significant figures) and describes the standard interpretation or procedural step that follows in an empirical Bayes framework.\n\nA. The estimate is $\\hat{\\tau}^2 = -3.17$. This estimate is truncated to 0, leading to the conclusion that there is no evidence of true between-district variation in effectiveness.\n\nB. The estimate is $\\hat{\\tau}^2 = 3.33$. This indicates that the true between-district variance is positive but smaller than the average sampling uncertainty.\n\nC. The estimate is $\\hat{\\tau}^2 = -3.17$. A negative variance estimate invalidates the Normal prior assumption, and the analysis cannot proceed without choosing a different prior distribution.\n\nD. The estimate is $\\hat{\\tau}^2 = 6.50$. This suggests the between-district variance is equal to the average sampling variance, indicating a high level of uncertainty.\n\nE. The estimate is $\\hat{\\tau}^2 = 0$. This value is obtained because the sample variance of the observations is smaller than the average sampling variance, rendering the method of moments inapplicable.", "solution": "We start from the hierarchical normal model with known sampling variances. For each district $j$, the sampling model is $Y_{j} \\mid \\theta_{j} \\sim N(\\theta_{j}, \\sigma_{j}^{2})$ and the prior is $\\theta_{j} \\sim N(\\mu, \\tau^{2})$. By the law of total variance,\n$$\n\\operatorname{Var}(Y_{j}) \\;=\\; \\mathbb{E}\\!\\left[\\operatorname{Var}(Y_{j}\\mid \\theta_{j})\\right] \\;+\\; \\operatorname{Var}\\!\\left(\\mathbb{E}[Y_{j}\\mid \\theta_{j}]\\right) \\;=\\; \\sigma_{j}^{2} \\;+\\; \\tau^{2}.\n$$\nBecause the $\\sigma_{j}^{2}$ differ across districts, the method-of-moments equates the empirical variance of $\\{Y_{j}\\}$ to the average of the model variances:\n$$\n\\frac{1}{J}\\sum_{j=1}^{J}\\operatorname{Var}(Y_{j}) \\;=\\; \\tau^{2} \\;+\\; \\frac{1}{J}\\sum_{j=1}^{J}\\sigma_{j}^{2}.\n$$\nUsing the sample variance of the observations as the moment estimator for the left-hand side, the method-of-moments estimator is\n$$\n\\hat{\\tau}^{2} \\;=\\; s_{Y}^{2} \\;-\\; \\bar{\\sigma^{2}},\n$$\nwhere\n$$\n\\bar{\\sigma^{2}} \\;=\\; \\frac{1}{J}\\sum_{j=1}^{J}\\sigma_{j}^{2}, \\qquad s_{Y}^{2} \\;=\\; \\frac{1}{J-1}\\sum_{j=1}^{J}\\left(Y_{j}-\\bar{Y}\\right)^{2}, \\qquad \\bar{Y} \\;=\\; \\frac{1}{J}\\sum_{j=1}^{J}Y_{j}.\n$$\nCompute $\\bar{Y}$ and $s_{Y}^{2}$ from the data $Y=\\{8.00,9.00,11.00,12.00\\}$ with $J=4$. First,\n$$\n\\bar{Y} \\;=\\; \\frac{8+9+11+12}{4} \\;=\\; 10.\n$$\nThen the deviations are $-2,-1,1,2$ with squared deviations summing to $10$, so\n$$\ns_{Y}^{2} \\;=\\; \\frac{10}{4-1} \\;=\\; \\frac{10}{3}.\n$$\nThe average sampling variance is\n$$\n\\bar{\\sigma^{2}} \\;=\\; \\frac{5+6+7+8}{4} \\;=\\; \\frac{26}{4} \\;=\\; \\frac{13}{2}.\n$$\nTherefore,\n$$\n\\hat{\\tau}^{2} \\;=\\; \\frac{10}{3} \\;-\\; \\frac{13}{2} \\;=\\; \\frac{20-39}{6} \\;=\\; -\\frac{19}{6} \\;\\approx\\; -3.17,\n$$\nrounded to three significant figures. In empirical Bayes practice, variance components are constrained to be nonnegative, so a negative moment estimate is truncated at $0$. The standard interpretation is that there is no evidence of true between-district heterogeneity beyond sampling error, and one proceeds with $\\hat{\\tau}^{2}=0$.\n\nHence, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1915166"}]}