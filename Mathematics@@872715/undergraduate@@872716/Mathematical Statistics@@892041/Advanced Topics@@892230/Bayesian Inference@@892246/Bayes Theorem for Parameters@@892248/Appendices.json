{"hands_on_practices": [{"introduction": "Before we can update our beliefs with data, we must first have a way to quantify them. This process, known as prior elicitation, is a crucial first step in any Bayesian analysis. In this exercise, we will tackle a common real-world scenario where an expert provides their subjective assessment not as a full distribution, but through summary values like a median and a confidence interval. Your task is to translate this expert knowledge into a formal Beta distribution, a versatile tool for modeling uncertainty about a proportion [@problem_id:1898866].", "problem": "A team of astrophysicists is trying to estimate the proportion, $\\theta$, of a certain class of exoplanets that could host atmospheric biosignatures. To establish a prior distribution for this unknown proportion, they consult a senior expert in planetary science. The expert states that their subjective belief is that the median value for $\\theta$ is 0.5, and that there is a 50% probability that the true proportion $\\theta$ lies somewhere between 0.42 and 0.58.\n\nThe team decides to model this prior belief using a Beta distribution, Beta($\\alpha, \\beta$), which has a probability density function proportional to $x^{\\alpha-1}(1-x)^{\\beta-1}$ for $x \\in [0, 1]$ and parameters $\\alpha, \\beta > 0$. For this task, you may assume that for Beta distributions that are not heavily skewed and have sufficiently large parameters, the central quantiles can be approximated using a normal distribution. Specifically, the 25th and 75th percentiles can be approximated by $\\mu - z^* \\sigma$ and $\\mu + z^* \\sigma$, respectively, where $\\mu$ is the mean, $\\sigma$ is the standard deviation, and the constant $z^*$ is approximately $0.6745$.\n\nAssuming the expert's subjective belief is adequately described by this model and approximation, determine the parameters $\\alpha$ and $\\beta$ of the Beta distribution. Report your values for $\\alpha$ and $\\beta$, in that order, rounded to three significant figures.", "solution": "We model the prior as $\\theta \\sim \\text{Beta}(\\alpha,\\beta)$. For a Beta distribution, the mean and variance are\n$$\\mu=\\frac{\\alpha}{\\alpha+\\beta}, \\qquad \\sigma^{2}=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)}.$$\n\nThe expert states the median is $0.5$ and that the central $50$ percent of the distribution lies between $0.42$ and $0.58$. Under the given normal approximation for central quantiles, the first and third quartiles satisfy\n$$Q_{1}\\approx \\mu - z^{*}\\sigma, \\qquad Q_{3}\\approx \\mu + z^{*}\\sigma,$$\nwith $z^{*}\\approx 0.6745$. The interval $[0.42,\\,0.58]$ represents the interquartile range, so\n$$Q_{1}=0.42, \\qquad Q_{3}=0.58.$$\n\nAdding these and dividing by $2$ gives the mean:\n$$\\mu \\approx \\frac{Q_{1}+Q_{3}}{2}=\\frac{0.42+0.58}{2}=0.5.$$\nThus\n$$\\frac{\\alpha}{\\alpha+\\beta}=0.5 \\quad \\Longrightarrow \\quad \\alpha=\\beta.$$\nLet $\\alpha=\\beta=a$. Then the variance simplifies to\n$$\\sigma^{2}=\\frac{a^{2}}{(2a)^{2}(2a+1)}=\\frac{1}{4(2a+1)}, \\qquad \\sigma=\\frac{1}{2\\sqrt{2a+1}}.$$\n\nFrom $Q_{3}-\\mu\\approx z^{*}\\sigma$ we have\n$$0.58-0.50 = z^{*}\\sigma \\quad \\Longrightarrow \\quad \\sigma=\\frac{0.08}{z^{*}}.$$\nEquating this to the expression for $\\sigma$ in terms of $a$,\n$$\\frac{1}{2\\sqrt{2a+1}}=\\frac{0.08}{z^{*}} \\quad \\Longrightarrow \\quad \\sqrt{2a+1}=\\frac{z^{*}}{0.16} \\quad \\Longrightarrow \\quad 2a+1=\\left(\\frac{z^{*}}{0.16}\\right)^{2}.$$\nHence\n$$a=\\frac{1}{2}\\left[\\left(\\frac{z^{*}}{0.16}\\right)^{2}-1\\right].$$\n\nWith $z^{*}=0.6745$,\n$$\\frac{z^{*}}{0.16}=\\frac{0.6745}{0.16}=4.215625,\\quad \\left(\\frac{z^{*}}{0.16}\\right)^{2}=17.771494140625,$$\nso\n$$a=\\frac{17.771494140625-1}{2}=8.3857470703125.$$\n\nTherefore, $\\alpha=\\beta\\approx 8.3857470703125$, which rounded to three significant figures gives\n$$\\alpha\\approx 8.39,\\qquad \\beta\\approx 8.39.$$", "answer": "$$\\boxed{\\begin{pmatrix}8.39 & 8.39\\end{pmatrix}}$$", "id": "1898866"}, {"introduction": "At the heart of Bayesian inference lies the elegant process of updating our knowledge as we gather evidence. This exercise demonstrates this fundamental principle by combining a prior belief about a success probability, $p$, with new data. We will use a Beta distribution as our prior, which is a *conjugate prior* for the Geometric likelihood of the data, ensuring our updated belief (the posterior) is also a Beta distribution. This practice solidifies the core mechanical step of using Bayes' theorem to learn from data [@problem_id:1898877].", "problem": "A data scientist at an educational technology company is analyzing player performance on a new puzzle. The number of attempts, $K$, a player needs to achieve their first success is modeled by a Geometric distribution with probability mass function $P(K=k|p) = p(1-p)^{k-1}$ for $k=1, 2, 3, \\dots$. Here, $p$ is the unknown probability of solving the puzzle on any single attempt, which is assumed to be constant for a given player.\n\nThe scientist uses a Bayesian approach to update her beliefs about the parameter $p$. Her prior belief about $p$ is described by a Beta distribution with parameters $\\alpha_0 = 4$ and $\\beta_0 = 6$.\n\nA player from the target audience is recruited for testing and is observed to take $k_{obs} = 8$ attempts to solve the puzzle for the first time. Given this observation, calculate the scientist's updated estimate for the probability $p$. Specifically, compute the posterior mean of $p$.\n\nRound your final answer to four significant figures.", "solution": "We model the number of attempts until first success by the geometric likelihood\n$$\nP(K=k \\mid p)=p(1-p)^{k-1}, \\quad k=1,2,3,\\dots .\n$$\nThe prior for $p$ is $\\operatorname{Beta}(\\alpha_{0},\\beta_{0})$ with density proportional to\n$$\n\\pi(p)\\propto p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}.\n$$\nGiven a single observation $k_{\\text{obs}}$, the posterior by Bayes’ rule is proportional to the product of likelihood and prior:\n$$\n\\pi(p \\mid k_{\\text{obs}})\\propto \\left[p(1-p)^{k_{\\text{obs}}-1}\\right]\\cdot p^{\\alpha_{0}-1}(1-p)^{\\beta_{0}-1}\n= p^{\\alpha_{0}}(1-p)^{\\beta_{0}+k_{\\text{obs}}-2}.\n$$\nRecognizing the Beta kernel, the posterior is\n$$\np \\mid k_{\\text{obs}} \\sim \\operatorname{Beta}\\big(\\alpha_{0}+1,\\ \\beta_{0}+k_{\\text{obs}}-1\\big).\n$$\nFor a $\\operatorname{Beta}(\\alpha,\\beta)$ distribution, the posterior mean is\n$$\n\\mathbb{E}[p \\mid k_{\\text{obs}}]=\\frac{\\alpha}{\\alpha+\\beta}.\n$$\nSubstituting $\\alpha_{0}=4$, $\\beta_{0}=6$, and $k_{\\text{obs}}=8$, we obtain\n$$\n\\alpha=\\alpha_{0}+1=5,\\qquad \\beta=\\beta_{0}+k_{\\text{obs}}-1=13,\n$$\nso\n$$\n\\mathbb{E}[p \\mid k_{\\text{obs}}]=\\frac{5}{5+13}=\\frac{5}{18}.\n$$\nNumerically, $\\frac{5}{18}=0.277777\\ldots$, which rounded to four significant figures is $0.2778$.", "answer": "$$\\boxed{0.2778}$$", "id": "1898877"}, {"introduction": "Once we have our posterior distribution, which encapsulates all our knowledge about a parameter, we often need to summarize it with a single point estimate. This exercise introduces the Maximum A Posteriori (MAP) estimate, which represents the single most probable value for the parameter. You will see how the interplay between the prior belief and the likelihood function shapes the posterior, and more importantly, how constraints on the parameter space—a common feature in physical models—can directly determine the final estimate [@problem_id:1898906].", "problem": "A materials scientist is developing a new type of filament for 3D printing. The filament's breaking strength, measured in megapascals (MPa), is theorized to be uniformly distributed between 0 and a maximum strength $\\theta$. This maximum strength $\\theta$ is an unknown parameter that depends on the specific manufacturing batch.\n\nBased on material composition theory, the scientist's prior belief about $\\theta$ is described by an Inverse-Gamma distribution with shape parameter $\\alpha = 4$ and scale parameter $\\beta = 120$. The probability density function of the prior on $\\theta$ is $p(\\theta) \\propto \\theta^{-(\\alpha+1)} \\exp(-\\frac{\\beta}{\\theta})$ for $\\theta > 0$.\n\nTo estimate $\\theta$ for a new batch, a random sample of $n=6$ filaments is tested until they break. The recorded breaking strengths are:\n$S = \\{32.5, 41.2, 28.9, 35.1, 45.8, 38.0\\}$ MPa.\n\nAssuming the observed strengths are independent and identically distributed samples from a Uniform$(0, \\theta)$ distribution, determine the Maximum A Posteriori (MAP) estimate for the parameter $\\theta$.\n\nExpress your answer for the estimated maximum strength in MPa, rounded to four significant figures.", "solution": "The problem asks for the Maximum A Posteriori (MAP) estimate of the parameter $\\theta$. The MAP estimate is the value of $\\theta$ that maximizes the posterior distribution, $p(\\theta|\\mathbf{s})$, which is determined by Bayes' theorem: $p(\\theta|\\mathbf{s}) \\propto L(\\mathbf{s}|\\theta) p(\\theta)$, where $L(\\mathbf{s}|\\theta)$ is the likelihood function and $p(\\theta)$ is the prior distribution.\n\n**Step 1: Define the Likelihood Function**\nThe observations $s_1, \\ldots, s_n$ are independent and identically distributed (i.i.d.) samples from a Uniform$(0, \\theta)$ distribution. The probability density function (PDF) for a single observation $s_i$ is $f(s_i|\\theta) = \\frac{1}{\\theta}$ for $0 \\le s_i \\le \\theta$, and $0$ otherwise.\n\nThe likelihood function $L(\\mathbf{s}|\\theta)$ is the joint probability of observing the data $\\mathbf{s} = \\{s_1, \\ldots, s_n\\}$ given the parameter $\\theta$:\n$$L(\\mathbf{s}|\\theta) = \\prod_{i=1}^{n} f(s_i|\\theta) = \\prod_{i=1}^{n} \\left(\\frac{1}{\\theta}\\right) I(0 \\le s_i \\le \\theta)$$\nwhere $I(\\cdot)$ is the indicator function. The condition $0 \\le s_i \\le \\theta$ must hold for all $i$. This is equivalent to requiring that $\\theta$ must be greater than or equal to the maximum value in the sample. Let $s_{\\max} = \\max(s_1, \\ldots, s_n)$. The likelihood function can then be written as:\n$$L(\\mathbf{s}|\\theta) = \\left(\\frac{1}{\\theta}\\right)^n I(\\theta \\ge s_{\\max}) = \\theta^{-n} I(\\theta \\ge s_{\\max})$$\n\n**Step 2: Define the Prior Distribution**\nThe prior distribution for $\\theta$ is given as an Inverse-Gamma distribution, IG$(\\alpha, \\beta)$, with a PDF proportional to:\n$$p(\\theta) \\propto \\theta^{-(\\alpha+1)} \\exp\\left(-\\frac{\\beta}{\\theta}\\right) \\quad \\text{for } \\theta > 0$$\nThe given hyperparameters are $\\alpha = 4$ and $\\beta = 120$.\n\n**Step 3: Determine the Posterior Distribution**\nThe posterior distribution is proportional to the product of the likelihood and the prior:\n$$p(\\theta|\\mathbf{s}) \\propto L(\\mathbf{s}|\\theta) p(\\theta)$$\n$$p(\\theta|\\mathbf{s}) \\propto \\left( \\theta^{-n} I(\\theta \\ge s_{\\max}) \\right) \\left( \\theta^{-(\\alpha+1)} \\exp\\left(-\\frac{\\beta}{\\theta}\\right) \\right)$$\nCombining the terms and incorporating the indicator function into the domain of $\\theta$:\n$$p(\\theta|\\mathbf{s}) \\propto \\theta^{-n - (\\alpha+1)} \\exp\\left(-\\frac{\\beta}{\\theta}\\right) = \\theta^{-(n+\\alpha+1)} \\exp\\left(-\\frac{\\beta}{\\theta}\\right), \\quad \\text{for } \\theta \\ge s_{\\max}$$\n\n**Step 4: Find the MAP Estimate**\nThe MAP estimate is the value of $\\theta$ that maximizes the posterior distribution. It is often easier to maximize the natural logarithm of the posterior, as the logarithm is a monotonic function.\nLet $g(\\theta) = \\ln(p(\\theta|\\mathbf{s}))$. For $\\theta \\ge s_{\\max}$:\n$$g(\\theta) = \\text{const} - (n+\\alpha+1)\\ln(\\theta) - \\frac{\\beta}{\\theta}$$\nTo find the maximum, we take the derivative of $g(\\theta)$ with respect to $\\theta$ and set it to zero.\n$$\\frac{dg}{d\\theta} = -\\frac{n+\\alpha+1}{\\theta} + \\frac{\\beta}{\\theta^2}$$\nSetting the derivative to zero gives the critical point, which we denote as $\\theta^*$:\n$$-\\frac{n+\\alpha+1}{\\theta^*} + \\frac{\\beta}{(\\theta^*)^2} = 0 \\implies \\frac{\\beta}{(\\theta^*)^2} = \\frac{n+\\alpha+1}{\\theta^*} \\implies \\theta^* = \\frac{\\beta}{n+\\alpha+1}$$\nThis value $\\theta^*$ is the mode of the unconstrained posterior distribution. The second derivative, $\\frac{d^2g}{d\\theta^2} = \\frac{n+\\alpha+1}{\\theta^2} - \\frac{2\\beta}{\\theta^3}$, is negative at $\\theta=\\theta^*$, confirming it is a maximum.\n\nHowever, the posterior is only defined for $\\theta \\ge s_{\\max}$. We must find the maximum of the posterior function over this restricted domain.\nThe function $g(\\theta)$ is increasing for $\\theta  \\theta^*$ and decreasing for $\\theta > \\theta^*$. This leads to two cases:\n1. If $\\theta^* \\ge s_{\\max}$: The unconstrained maximum lies within the valid domain. The MAP estimate is therefore $\\theta_{\\text{MAP}} = \\theta^*$.\n2. If $\\theta^*  s_{\\max}$: The unconstrained maximum lies outside the valid domain. Over the domain $[\\,s_{\\max}, \\infty)$, the function is strictly decreasing. Thus, the maximum value occurs at the boundary, $\\theta_{\\text{MAP}} = s_{\\max}$.\n\nCombining these two cases, the MAP estimate is given by $\\theta_{\\text{MAP}} = \\max(s_{\\max}, \\theta^*)$.\n\n**Step 5: Calculate the Numerical Value**\nThe problem provides the following values:\n- $n = 6$\n- $\\alpha = 4$\n- $\\beta = 120$\n- Data $\\mathbf{s} = \\{32.5, 41.2, 28.9, 35.1, 45.8, 38.0\\}$ MPa\n\nFirst, we find the maximum value in the data sample:\n$$s_{\\max} = \\max\\{32.5, 41.2, 28.9, 35.1, 45.8, 38.0\\} = 45.8 \\text{ MPa}$$\n\nNext, we calculate the unconstrained mode $\\theta^*$:\n$$\\theta^* = \\frac{\\beta}{n+\\alpha+1} = \\frac{120}{6+4+1} = \\frac{120}{11} \\approx 10.909 \\text{ MPa}$$\n\nNow, we find the MAP estimate by comparing $s_{\\max}$ and $\\theta^*$:\n$$\\theta_{\\text{MAP}} = \\max(s_{\\max}, \\theta^*) = \\max(45.8, 10.909) = 45.8 \\text{ MPa}$$\n\nThe problem requires the answer to be rounded to four significant figures.\n$$\\theta_{\\text{MAP}} = 45.80$$", "answer": "$$\\boxed{45.80}$$", "id": "1898906"}]}