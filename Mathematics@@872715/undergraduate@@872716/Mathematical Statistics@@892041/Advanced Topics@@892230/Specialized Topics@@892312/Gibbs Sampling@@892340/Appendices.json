{"hands_on_practices": [{"introduction": "The theory of Gibbs sampling comes to life when you walk through the steps yourself. This first exercise provides a direct, hands-on opportunity to execute one full iteration of a Gibbs sampler. By using the inverse transform method to generate random variates from specified conditional distributions, you will practice the fundamental mechanics that form the core of this powerful MCMC technique. [@problem_id:1920320]", "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.", "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$.\nFor an Exponential rate $y$, the conditional CDF is\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$.\nFor a Poisson mean $x$, the pmf is\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\nSince $p(0 \\mid x)=0.7368<0.750$, continue to $k=1$:\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\nThen\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.3054 & 1\\end{pmatrix}}$$", "id": "1920320"}, {"introduction": "Once you understand the mechanics of a Gibbs sampler, the next step is to build intuition for its behavior in different scenarios. This thought experiment explores how the underlying structure of the target distribution, specifically the correlation between variables, dramatically affects the sampler's efficiency. By visualizing the sampler's path, you will learn to recognize the signs of poor mixing, a common challenge in MCMC applications where high correlation can slow convergence to a crawl. [@problem_id:1363771]", "problem": "A statistician is working with a Gibbs sampler, a type of Markov Chain Monte Carlo (MCMC) algorithm, to draw samples from a bivariate probability distribution. The general procedure for a bivariate distribution $p(x, y)$ is as follows:\n1.  Begin with an arbitrary starting point $(x_0, y_0)$.\n2.  For each iteration $t=1, 2, \\dots, N$:\n    a. Sample a new value $x_t$ from the full conditional distribution $p(x | y = y_{t-1})$.\n    b. Sample a new value $y_t$ from the full conditional distribution $p(y | x = x_t)$.\nThe resulting sequence of points $\\{(x_t, y_t)\\}_{t=1}^N$ forms a Markov chain whose stationary distribution is the target distribution $p(x, y)$.\n\nThe statistician is interested in two specific target distributions, both of which are bivariate normal distributions with means $\\mu_X = \\mu_Y = 0$ and standard deviations $\\sigma_X = \\sigma_Y = 1$. The two distributions differ only by their correlation coefficient, $\\rho$.\n-   **Distribution A**: A bivariate normal distribution with $\\rho_A = 0.1$.\n-   **Distribution B**: A bivariate normal distribution with $\\rho_B = 0.98$.\n\nThe statistician runs a Gibbs sampler for each distribution and generates two long sequences of sample points. When plotting the paths of these sequences in the $xy$-plane, two distinct qualitative behaviors are observed, which are described below.\n\n-   **Path Description 1**: The sequence of points moves freely throughout the sample space. The transition from one point $(x_{t-1}, y_{t-1})$ to the next $(x_t, y_t)$ can result in a large displacement, and the path rapidly covers the primary, nearly circular region of the distribution's probability mass.\n-   **Path Description 2**: The sequence of points exhibits highly autocorrelated, incremental movements. The path is characterized by a series of small, axis-aligned steps, creating a distinct staircase-like trajectory that slowly traverses the narrow, elliptical region of the distribution's probability mass.\n\nWhich of the following statements correctly matches the distributions to the observed sampler path descriptions?\n\nA. Distribution A corresponds to Path 2, and Distribution B corresponds to Path 1.\nB. Both distributions would generate paths resembling Path 1.\nC. Distribution A corresponds to Path 1, and Distribution B corresponds to Path 2.\nD. Both distributions would generate paths resembling Path 2.\nE. The path characteristic is independent of the correlation and depends only on the random seed used.", "solution": "The core of this problem is to understand how the correlation coefficient $\\rho$ of a bivariate normal distribution affects the behavior of a Gibbs sampler. The key lies in the properties of the conditional distributions used for sampling.\n\nFor a general bivariate normal distribution for $(X, Y)$ with means $(\\mu_X, \\mu_Y)$, standard deviations $(\\sigma_X, \\sigma_Y)$, and correlation $\\rho$, the full conditional distributions are also normal:\n1.  The distribution of $X$ conditional on $Y=y$ is $X | (Y=y) \\sim \\mathcal{N}\\left(\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y - \\mu_Y), \\sigma_X^2(1-\\rho^2)\\right)$.\n2.  The distribution of $Y$ conditional on $X=x$ is $Y | (X=x) \\sim \\mathcal{N}\\left(\\mu_Y + \\rho\\frac{\\sigma_Y}{\\sigma_X}(x - \\mu_X), \\sigma_Y^2(1-\\rho^2)\\right)$.\n\nIn our specific problem, the parameters are simplified: $\\mu_X = \\mu_Y = 0$ and $\\sigma_X = \\sigma_Y = 1$. Substituting these values into the general formulas, we get the conditional distributions for this specific case:\n1.  $X | (Y=y) \\sim \\mathcal{N}(\\rho y, 1-\\rho^2)$\n2.  $Y | (X=x) \\sim \\mathcal{N}(\\rho x, 1-\\rho^2)$\n\nThe Gibbs sampler at step $t$ samples $x_t$ from $p(x | y_{t-1})$ and then samples $y_t$ from $p(y | x_t)$. Let's analyze this process for the two given distributions.\n\n**Case 1: Distribution A with $\\rho_A = 0.1$ (low correlation)**\n\nFor this distribution, the conditional distributions are:\n-   $X | (Y=y) \\sim \\mathcal{N}(0.1y, 1 - 0.1^2) = \\mathcal{N}(0.1y, 0.99)$.\n-   $Y | (X=x) \\sim \\mathcal{N}(0.1x, 1 - 0.1^2) = \\mathcal{N}(0.1x, 0.99)$.\n\nThe mean of the conditional distribution for $x_t$ is $0.1 y_{t-1}$, which is very close to zero for typical values of $y_{t-1}$ (since the marginal variance is 1). The variance of this conditional distribution is $0.99$, which is large and close to the marginal variance of 1. This means that the new value $x_t$ is drawn from a wide distribution and is only weakly dependent on the previous value $y_{t-1}$. A similar logic applies when sampling $y_t$ from $p(y|x_t)$. Because the steps are drawn from wide distributions and are nearly independent of the previous state, the sampler can make large jumps and explore the state space efficiently. This behavior matches **Path Description 1**.\n\n**Case 2: Distribution B with $\\rho_B = 0.98$ (high correlation)**\n\nFor this distribution, the conditional distributions are:\n-   $X | (Y=y) \\sim \\mathcal{N}(0.98y, 1 - 0.98^2) = \\mathcal{N}(0.98y, 0.0396)$.\n-   $Y | (X=x) \\sim \\mathcal{N}(0.98x, 1 - 0.98^2) = \\mathcal{N}(0.98x, 0.0396)$.\n\nHere, the situation is drastically different. The mean of the conditional distribution for $x_t$ is $0.98y_{t-1}$. The variance is $0.0396$, which is very small. This implies that the new value $x_t$ will be drawn from a very narrow distribution centered very close to the value of $y_{t-1}$. That is, $x_t \\approx 0.98 y_{t-1}$.\nSubsequently, when sampling $y_t$, we draw from a narrow distribution centered at $0.98x_t$. So, $y_t \\approx 0.98 x_t$.\nCombining these, we see that $y_t \\approx 0.98 (0.98 y_{t-1}) \\approx 0.96 y_{t-1}$. This shows that consecutive values in the chain are highly correlated and change very slowly. The sampler is forced to take very small steps. Since the updates are done one variable at a time (first move parallel to the x-axis, then parallel to the y-axis), this process creates a \"staircase\" or \"zig-zag\" pattern as it slowly moves along the narrow high-probability region of the distribution (which lies along the line $y=x$ for high positive correlation). This behavior matches **Path Description 2**.\n\n**Conclusion**\n\n-   Distribution A (low correlation) leads to efficient exploration with large, less-correlated steps, matching Path 1.\n-   Distribution B (high correlation) leads to slow exploration with small, highly-correlated, axis-aligned steps, matching Path 2.\n\nTherefore, the correct matching is that Distribution A corresponds to Path 1, and Distribution B corresponds to Path 2. This corresponds to option C. Options A, B, D, and E are incorrect based on this analysis.", "answer": "$$\\boxed{C}$$", "id": "1363771"}, {"introduction": "Running a sampler is one thing; trusting its results is another. This final practice addresses the critical task of diagnosing convergence. By analyzing the output from two parallel chains started at different points, you will learn to spot a classic sign of sampler failure. This exercise demonstrates how visual inspection of trace plots is an indispensable first step in verifying whether your MCMC chains have successfully explored the entire target posterior distribution. [@problem_id:1920355]", "problem": "A data scientist is using a Gibbs sampler, a type of Markov Chain Monte Carlo (MCMC) algorithm, to draw samples from a complex, one-dimensional posterior probability distribution for a parameter $\\theta$. To diagnose the convergence of the sampler to the target distribution, they initialize two independent MCMC chains from very different starting points.\n\nAfter running both chains for 20,000 iterations, the data scientist examines the trace plots, which show the value of $\\theta$ at each iteration. The visual inspection of these plots is described as follows:\n\n- **Chain 1:** Initialized with $\\theta_0 = -15.0$. For all 20,000 iterations, the sampled values of $\\theta$ quickly moved to and then fluctuated within the approximate interval $[-8.5, -6.5]$, exhibiting behavior consistent with a stationary time series centered around -7.5.\n- **Chain 2:** Initialized with $\\theta_0 = 15.0$. For all 20,000 iterations, the sampled values of $\\theta$ quickly moved to and then fluctuated within the approximate interval $[6.5, 8.5]$, exhibiting behavior consistent with a stationary time series centered around 7.5.\n\nBased on this textual description of the trace plots, what is the most appropriate conclusion regarding the performance of the Gibbs sampler?\n\nA. The sampler has converged successfully. The posterior distribution for $\\theta$ is clearly bimodal with modes near -7.5 and 7.5.\nB. The two chains have not mixed. This is a strong indication that the sampler has failed to converge and explore the full parameter space.\nC. The sampler is working correctly, but the \"burn-in\" period is too short. Discarding the first 10,000 samples from each chain will ensure the remaining samples are from the true posterior.\nD. The posterior distribution for $\\theta$ is unimodal with a mean of 0, which is the average of the means of the two chains. The chains simply show the range of the distribution.\nE. The starting values were not far enough apart. A re-run with starting values of -100 and 100 is necessary to make any valid conclusion about convergence.", "solution": "We consider the definition of Markov chain Monte Carlo convergence to a target posterior distribution $\\pi(\\theta)$: for a Markov chain with transition kernel invariant under $\\pi(\\theta)$, convergence requires that the marginal distribution of $\\theta_{t}$ approaches $\\pi(\\theta)$ as $t \\to \\infty$, regardless of the initial state. In practice, a standard diagnostic is to run multiple overdispersed chains and check whether their trace plots overlap and whether the chains mix across the same regions of the parameter space.\n\nFrom the description, Chain 1 rapidly moves to and then remains within approximately $[-8.5,-6.5]$ with apparent stationarity centered near $-7.5$, while Chain 2 rapidly moves to and then remains within approximately $[6.5,8.5]$ with apparent stationarity centered near $7.5$. There is no crossing between these disjoint regions over $20{,}000$ iterations for either chain. This indicates that each chain is trapped in a different region of the parameter space and does not mix across regions.\n\nA standard quantitative check is the Gelmanâ€“Rubin potential scale reduction factor. With $m$ chains of length $n$, denote the chain means by $\\bar{\\theta}_{i\\cdot}$ and the overall mean by $\\bar{\\theta}_{\\cdot\\cdot}$. The between-chain variance is\n$$\nB=\\frac{n}{m-1}\\sum_{i=1}^{m}\\left(\\bar{\\theta}_{i\\cdot}-\\bar{\\theta}_{\\cdot\\cdot}\\right)^{2},\n$$\nand the within-chain variance is\n$$\nW=\\frac{1}{m}\\sum_{i=1}^{m}s_{i}^{2},\n$$\nwhere $s_{i}^{2}$ is the sample variance within chain $i$. The marginal variance estimate is\n$$\n\\hat{V}=\\frac{n-1}{n}W+\\frac{1}{n}B,\n$$\nand the potential scale reduction factor is\n$$\n\\hat{R}=\\sqrt{\\frac{\\hat{V}}{W}}.\n$$\nGiven that the two chains have means far apart with little within-chain overlap, $B$ is large relative to $W$, implying $\\hat{R}>1$, which indicates lack of convergence.\n\nRegarding the options:\n- Option A is not justified: although the target posterior might be bimodal, convergence requires that each chain explores the full support according to $\\pi(\\theta)$. Persistent separation without mode switching shows non-mixing and failure to explore both modes; one cannot claim successful convergence from disjoint occupancy.\n- Option C is incorrect: discarding early iterations (burn-in) does not solve the lack of transitions between regions when no transitions occur at any time.\n- Option D is incorrect: the assertion of a unimodal posterior with mean $0$ contradicts the disjoint, stationary behavior in separated regions; averaging chain means does not validate posterior characteristics.\n- Option E is incorrect: the starting values were already far apart; increasing that distance does not remedy non-mixing across modes.\n\nTherefore, the correct conclusion is that the two chains have not mixed, indicating failure of the sampler to converge and explore the full parameter space.", "answer": "$$\\boxed{B}$$", "id": "1920355"}]}