{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will start with one of the most fundamental models in statistics: estimating the mean of a Normal distribution. This exercise explores a crucial conceptual question: what happens to our minimum expected loss, the Bayes risk, when our prior beliefs about the parameter become increasingly vague? By examining the limit as the prior variance approaches infinity, you will gain insight into the role of the prior in Bayesian inference and see how Bayes risk behaves under conditions of minimal prior information [@problem_id:1898419].", "problem": "Consider a statistical estimation problem where we want to estimate an unknown parameter $\\mu$. Our prior belief about $\\mu$ is modeled by a normal distribution with a mean of 0 and a variance of $\\tau^2$, where $\\tau^2 > 0$. We denote this as $\\mu \\sim N(0, \\tau^2)$. We then observe a single data point $X$ from a normal distribution whose mean is the true value of $\\mu$ and whose variance is 1, which is denoted as $X | \\mu \\sim N(\\mu, 1)$.\n\nThe quality of an estimator, denoted $\\hat{\\mu}(X)$, is measured by the squared error loss function, $L(\\mu, \\hat{\\mu}) = (\\mu - \\hat{\\mu})^2$. The Bayes risk, which we will denote as $r(\\tau^2)$, is defined as the minimum possible expected loss, where the expectation is taken over the joint distribution of both $\\mu$ and $X$.\n\nDetermine the limiting value of the Bayes risk as the prior variance becomes infinitely large. That is, calculate the value of $\\lim_{\\tau^2 \\to \\infty} r(\\tau^2)$. Provide the final answer as a single numerical value.", "solution": "We are given a normal-normal model with prior $\\mu \\sim N(0,\\tau^{2})$ and likelihood $X \\mid \\mu \\sim N(\\mu,1)$. Under squared error loss $L(\\mu,\\hat{\\mu})=(\\mu-\\hat{\\mu})^{2}$, the Bayes estimator is the posterior mean:\n$$\n\\hat{\\mu}(X)=\\mathbb{E}[\\mu \\mid X].\n$$\nBy conjugacy, the posterior distribution is normal with precision equal to the sum of prior and likelihood precisions. Hence\n$$\n\\operatorname{Var}(\\mu \\mid X)=\\left(\\frac{1}{\\tau^{2}}+1\\right)^{-1}=\\frac{\\tau^{2}}{1+\\tau^{2}},\n$$\nand\n$$\n\\mathbb{E}[\\mu \\mid X]=\\operatorname{Var}(\\mu \\mid X)\\left(\\frac{0}{\\tau^{2}}+X\\right)=\\frac{\\tau^{2}}{1+\\tau^{2}}\\,X.\n$$\nThe Bayes risk is the minimum expected loss over estimators, which for squared error loss and the Bayes rule equals the expected posterior variance:\n$$\nr(\\tau^{2})=\\mathbb{E}\\big[(\\mu-\\hat{\\mu}(X))^{2}\\big]=\\mathbb{E}\\big[\\operatorname{Var}(\\mu \\mid X)\\big]=\\frac{\\tau^{2}}{1+\\tau^{2}},\n$$\nwhere the expectation is trivial because $\\operatorname{Var}(\\mu \\mid X)$ does not depend on $X$.\n\nTherefore, the limiting Bayes risk as $\\tau^{2}\\to\\infty$ is\n$$\n\\lim_{\\tau^{2}\\to\\infty} r(\\tau^{2})=\\lim_{\\tau^{2}\\to\\infty} \\frac{\\tau^{2}}{1+\\tau^{2}}=1.\n$$", "answer": "$$\\boxed{1}$$", "id": "1898419"}, {"introduction": "Now, let's apply our understanding to a practical scenario from reliability engineering, where the lifetime of components is often modeled using the Exponential distribution. This problem provides a complete, step-by-step exercise in calculating the Bayes risk for an unknown failure rate, $\\lambda$, using a conjugate Gamma prior. Working through this example will solidify your understanding of the entire computational workflow: from finding the posterior distribution and its variance to averaging this risk over the marginal distribution of the observed data [@problem_id:1898422].", "problem": "In reliability engineering, the lifetime of an electronic component is often modeled by an Exponential distribution. Suppose the time to failure, $X$, of a new type of component follows an Exponential distribution with an unknown rate parameter $\\lambda > 0$. The probability density function of the lifetime is given by $f(x|\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x > 0$.\n\nFrom past experience with similar components, we have prior beliefs about the failure rate $\\lambda$. We model this uncertainty using a Gamma prior distribution with a known shape parameter $\\alpha > 0$ and a known rate parameter $\\beta > 0$. The prior probability density function for $\\lambda$ is given by $\\pi(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$.\n\nWe will make a single observation of a component's lifetime, $X=x$, and use it to estimate $\\lambda$. The quality of our estimate, $\\hat{\\lambda}$, is evaluated using a squared error loss function, defined as $L(\\lambda, \\hat{\\lambda}) = (\\lambda - \\hat{\\lambda})^2$.\n\nYour task is to calculate the Bayes risk associated with estimating $\\lambda$ under these conditions. The Bayes risk is the minimum possible expected loss, averaged over both the data distribution and the prior distribution of the parameter.\n\nExpress your answer as a closed-form analytic expression in terms of the prior parameters $\\alpha$ and $\\beta$.", "solution": "We are given a single observation $X=x$ from an Exponential distribution with rate $\\lambda$, with prior $\\lambda \\sim \\text{Gamma}(\\alpha,\\beta)$ under the rate parameterization. The loss is squared error, $L(\\lambda,\\hat{\\lambda})=(\\lambda-\\hat{\\lambda})^{2}$. Under squared error loss, the Bayes estimator is the posterior mean, and the Bayes risk equals the expected posterior variance:\n$$\n\\mathcal{R}_{B}=\\mathbb{E}_{X}\\!\\left[\\operatorname{Var}(\\lambda\\mid X)\\right].\n$$\n\nFirst, compute the posterior. The likelihood is $f(x\\mid \\lambda)=\\lambda \\exp(-\\lambda x)$ for $x>0$, and the prior is $\\pi(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)$ for $\\lambda>0$. By conjugacy, the posterior is\n$$\n\\lambda \\mid x \\sim \\text{Gamma}(\\alpha+1,\\beta+x),\n$$\nso its variance is\n$$\n\\operatorname{Var}(\\lambda\\mid x)=\\frac{\\alpha+1}{(\\beta+x)^{2}}.\n$$\nHence,\n$$\n\\mathcal{R}_{B}=\\mathbb{E}_{X}\\!\\left[\\frac{\\alpha+1}{(\\beta+X)^{2}}\\right].\n$$\n\nNext, compute the prior predictive (marginal) density of $X$:\n$$\nf_{X}(x)=\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x)\\,\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)\\,d\\lambda\n=\\frac{\\beta^{\\alpha}\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)}\\frac{1}{(\\beta+x)^{\\alpha+1}}\n=\\frac{\\alpha \\beta^{\\alpha}}{(\\beta+x)^{\\alpha+1}},\n$$\nfor $x>0$. Therefore,\n$$\n\\mathcal{R}_{B}=(\\alpha+1)\\int_{0}^{\\infty}\\frac{1}{(\\beta+x)^{2}}\\cdot \\frac{\\alpha \\beta^{\\alpha}}{(\\beta+x)^{\\alpha+1}}\\,dx\n=\\alpha(\\alpha+1)\\beta^{\\alpha}\\int_{0}^{\\infty}(\\beta+x)^{-(\\alpha+3)}\\,dx.\n$$\nEvaluate the integral using $u=\\beta+x$:\n$$\n\\int_{0}^{\\infty}(\\beta+x)^{-(\\alpha+3)}\\,dx=\\int_{\\beta}^{\\infty}u^{-(\\alpha+3)}\\,du=\\frac{\\beta^{-(\\alpha+2)}}{\\alpha+2}.\n$$\nThus,\n$$\n\\mathcal{R}_{B}=\\alpha(\\alpha+1)\\beta^{\\alpha}\\cdot \\frac{\\beta^{-(\\alpha+2)}}{\\alpha+2}=\\frac{\\alpha(\\alpha+1)}{\\alpha+2}\\cdot \\frac{1}{\\beta^{2}}.\n$$\n\nTherefore, the Bayes risk under squared error loss with one Exponential observation and a $\\text{Gamma}(\\alpha,\\beta)$ prior is $\\frac{\\alpha(\\alpha+1)}{(\\alpha+2)\\beta^{2}}$.", "answer": "$$\\boxed{\\frac{\\alpha(\\alpha+1)}{(\\alpha+2)\\beta^{2}}}$$", "id": "1898422"}, {"introduction": "In many real-world applications, our choice of an estimate is limited by practical constraints. This final practice problem tackles such a scenario, where an engineer must choose an estimate for a success probability from a small, discrete set of possible values. Here, you will not only calculate the ideal Bayes risk but also find the risk of the best *constrained* estimator and compare the two. This exercise highlights the power of Bayes risk as a fundamental benchmark, allowing us to quantify the loss in performance incurred when we are forced to deviate from the theoretically optimal decision rule [@problem_id:1898413].", "problem": "An engineer is testing a manufacturing process that produces a specific type of semiconductor. The quality of each semiconductor can be modeled as an independent Bernoulli trial, where the probability of a semiconductor being non-defective is $p$. The engineer's prior belief about the value of $p$ is modeled by a Beta distribution with parameters $\\alpha=1$ and $\\beta=1$, i.e., $p \\sim \\text{Beta}(1, 1)$. To update this belief, the engineer tests a batch of $n=4$ semiconductors and observes the total number of non-defective items, $S$.\n\nThe engineer considers two different decision rules (estimators) for the parameter $p$, based on the observed data $S$. The performance of an estimator $\\delta$ is evaluated using the squared error loss function, $L(p, \\delta) = (p - \\delta)^2$.\n\nThe first estimator, $\\delta_B$, is the standard Bayes estimator for this model, which minimizes the posterior expected loss.\n\nThe second estimator, $\\delta_C$, is a constrained estimator. Due to system limitations, this estimator must choose its estimate for $p$ from the discrete set $D = \\{0, 0.25, 0.5, 0.75, 1\\}$. For a given number of successes $S=k$, the rule $\\delta_C$ selects the value from the set $D$ that minimizes the posterior expected squared error loss.\n\nThe overall performance of any estimator $\\delta$ is measured by its Bayes risk, $r(\\delta)$, which is the expected value of the loss function over both the prior distribution of $p$ and the sampling distribution of the data $S$.\n\nCalculate the difference in the Bayes risks of the two estimators, $r(\\delta_C) - r(\\delta_B)$. Express your answer as an exact fraction in simplest form.", "solution": "Let $p \\sim \\text{Beta}(\\alpha,\\beta)$ with $\\alpha=\\beta=1$ and $S \\mid p \\sim \\text{Binomial}(n,p)$ with $n=4$. Under squared error loss, the Bayes estimator is the posterior mean. Given $S=k$, the posterior is $\\text{Beta}(\\alpha+k,\\beta+n-k)=\\text{Beta}(k+1,5-k)$, so the Bayes estimator is\n$$\n\\delta_{B}(k)=\\mathbb{E}[p \\mid S=k]=\\frac{k+\\alpha}{\\alpha+\\beta+n}=\\frac{k+1}{6}.\n$$\nDenote $\\mu_{k}=\\delta_{B}(k)=\\frac{k+1}{6}$.\n\nFor any estimator $\\delta(S)$, the posterior expected squared loss decomposes as\n$$\n\\mathbb{E}\\big[(p-\\delta(S))^{2}\\mid S\\big]=\\operatorname{Var}(p\\mid S)+\\big(\\mathbb{E}[p\\mid S]-\\delta(S)\\big)^{2}\n=\\operatorname{Var}(p\\mid S)+\\big(\\mu_{S}-\\delta(S)\\big)^{2}.\n$$\nTherefore,\n$$\nr(\\delta_{C})-r(\\delta_{B})=\\mathbb{E}\\big[(\\mu_{S}-\\delta_{C}(S))^{2}\\big],\n$$\nwhere the expectation is over the prior predictive distribution of $S$.\n\nThe constrained estimator $\\delta_{C}(k)$ must lie in $D=\\{0,\\tfrac{1}{4},\\tfrac{1}{2},\\tfrac{3}{4},1\\}$ and minimizes the posterior expected loss, which by the quadratic form above is equivalent to choosing the element of $D$ closest to $\\mu_{k}$. Compute $\\mu_{k}$ for $k=0,1,2,3,4$:\n$$\n\\mu_{0}=\\frac{1}{6},\\quad \\mu_{1}=\\frac{1}{3},\\quad \\mu_{2}=\\frac{1}{2},\\quad \\mu_{3}=\\frac{2}{3},\\quad \\mu_{4}=\\frac{5}{6}.\n$$\nThe nearest elements in $D$ are\n$$\n\\delta_{C}(0)=\\frac{1}{4},\\quad \\delta_{C}(1)=\\frac{1}{4},\\quad \\delta_{C}(2)=\\frac{1}{2},\\quad \\delta_{C}(3)=\\frac{3}{4},\\quad \\delta_{C}(4)=\\frac{3}{4}.\n$$\nHence the squared deviations are\n$$\n(\\mu_{0}-\\delta_{C}(0))^{2}=\\left(\\frac{1}{6}-\\frac{1}{4}\\right)^{2}=\\frac{1}{144},\\quad\n(\\mu_{1}-\\delta_{C}(1))^{2}=\\left(\\frac{1}{3}-\\frac{1}{4}\\right)^{2}=\\frac{1}{144},\n$$\n$$\n(\\mu_{2}-\\delta_{C}(2))^{2}=0,\\quad\n(\\mu_{3}-\\delta_{C}(3))^{2}=\\left(\\frac{2}{3}-\\frac{3}{4}\\right)^{2}=\\frac{1}{144},\\quad\n(\\mu_{4}-\\delta_{C}(4))^{2}=\\left(\\frac{5}{6}-\\frac{3}{4}\\right)^{2}=\\frac{1}{144}.\n$$\n\nThe prior predictive distribution of $S$ under $\\text{Beta}(1,1)$ is\n$$\n\\mathbb{P}(S=k)=\\int_{0}^{1}\\binom{4}{k}p^{k}(1-p)^{4-k}\\,dp=\\binom{4}{k}B(k+1,5-k)=\\frac{1}{5},\n$$\nfor $k=0,1,2,3,4$, since $B(k+1,5-k)=\\frac{k!(4-k)!}{5!}$ and $\\binom{4}{k}\\frac{k!(4-k)!}{5!}=\\frac{1}{5}$.\n\nTherefore,\n$$\nr(\\delta_{C})-r(\\delta_{B})=\\sum_{k=0}^{4}\\mathbb{P}(S=k)\\,(\\mu_{k}-\\delta_{C}(k))^{2}\n=\\frac{1}{5}\\left(\\frac{1}{144}+\\frac{1}{144}+0+\\frac{1}{144}+\\frac{1}{144}\\right)\n=\\frac{4}{5}\\cdot\\frac{1}{144}\n=\\frac{1}{180}.\n$$", "answer": "$$\\boxed{\\frac{1}{180}}$$", "id": "1898413"}]}