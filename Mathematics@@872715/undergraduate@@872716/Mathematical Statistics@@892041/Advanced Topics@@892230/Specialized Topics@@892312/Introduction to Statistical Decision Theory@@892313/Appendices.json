{"hands_on_practices": [{"introduction": "The first step in evaluating any statistical procedure is to quantify its performance. In decision theory, the risk function serves as this primary measure, representing the expected loss of our decision rule. This exercise provides foundational practice in calculating the risk for a simple estimator, connecting the abstract definition to a concrete value and reinforcing the crucial concepts of bias and variance. [@problem_id:1924831]", "problem": "A specialized sensor is designed to measure a constant, but unknown, physical quantity denoted by the parameter $\\theta$. Due to its internal mechanics, any single measurement $X$ is subject to a random error such that the measurement's value is a random variable uniformly distributed over the interval $[\\theta, \\theta+1]$.\n\nAn engineer takes two independent measurements, $X_1$ and $X_2$, from this sensor. They propose an estimator, $\\delta(X_1, X_2)$, to estimate the true value of $\\theta$. The estimator is defined as the sample mean of the measurements minus a constant correction factor:\n$$\n\\delta(X_1, X_2) = \\frac{X_1 + X_2}{2} - \\frac{1}{2}\n$$\nThe quality of this estimator is evaluated using a squared error loss function, $L(\\theta, a) = (a - \\theta)^2$. The overall performance of an estimator is quantified by its risk, which is defined as the expected value of the loss function, $R(\\theta, \\delta) = E[L(\\theta, \\delta)]$.\n\nCalculate the risk associated with the estimator $\\delta(X_1, X_2)$. Express your answer as an exact fraction.", "solution": "We are given independent measurements $X_{1}$ and $X_{2}$ with $X_{i} \\sim \\mathrm{Unif}[\\theta,\\theta+1]$ and the estimator\n$$\n\\delta(X_{1},X_{2})=\\frac{X_{1}+X_{2}}{2}-\\frac{1}{2}.\n$$\nUnder squared error loss, the risk is the mean squared error:\n$$\nR(\\theta,\\delta)=\\mathbb{E}\\left[(\\delta-\\theta)^{2}\\right]=\\operatorname{Var}(\\delta)+\\left(\\mathbb{E}[\\delta]-\\theta\\right)^{2}.\n$$\nFor $X_{i} \\sim \\mathrm{Unif}[\\theta,\\theta+1]$, the mean and variance are\n$$\n\\mathbb{E}[X_{i}]=\\theta+\\frac{1}{2}, \\qquad \\operatorname{Var}(X_{i})=\\frac{1}{12}.\n$$\nCompute the expectation of $\\delta$:\n$$\n\\mathbb{E}[\\delta]=\\frac{\\mathbb{E}[X_{1}]+\\mathbb{E}[X_{2}]}{2}-\\frac{1}{2}=\\frac{2\\left(\\theta+\\frac{1}{2}\\right)}{2}-\\frac{1}{2}=\\theta,\n$$\nso the estimator is unbiased and the bias term is zero. Hence,\n$$\nR(\\theta,\\delta)=\\operatorname{Var}(\\delta).\n$$\nSince subtracting a constant does not change variance,\n$$\n\\operatorname{Var}(\\delta)=\\operatorname{Var}\\!\\left(\\frac{X_{1}+X_{2}}{2}\\right)=\\frac{1}{4}\\operatorname{Var}(X_{1}+X_{2}).\n$$\nUsing independence,\n$$\n\\operatorname{Var}(X_{1}+X_{2})=\\operatorname{Var}(X_{1})+\\operatorname{Var}(X_{2})=\\frac{1}{12}+\\frac{1}{12}=\\frac{1}{6}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(\\delta)=\\frac{1}{4}\\cdot\\frac{1}{6}=\\frac{1}{24}.\n$$\nThus the risk is constant in $\\theta$ and equals $\\frac{1}{24}$.", "answer": "$$\\boxed{\\frac{1}{24}}$$", "id": "1924831"}, {"introduction": "Choosing between different estimators is a central task in statistics. This practice problem moves beyond evaluating a single rule to comparing two different estimators for the same parameter. By analyzing their respective risk functions, you will gain insight into the classic bias-variance tradeoff and see how an unbiased estimator is not always the best choice. [@problem_id:1924850]", "problem": "A physicist is studying a rare particle decay process. The number of decays detected in a fixed time interval, $X$, is modeled by a Poisson distribution with an unknown mean rate $\\lambda  0$. To estimate $\\lambda$, the experiment is repeated $n$ times, yielding an independent and identically distributed (i.i.d.) random sample $X_1, X_2, \\ldots, X_n$.\n\nTwo estimators for $\\lambda$ are being considered. The first is the standard sample mean, defined as $\\delta_1(\\mathbf{X}) = \\bar{X}$, where $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. The second is a non-standard estimator proposed as $\\delta_2(\\mathbf{X}) = \\bar{X} + 1$.\n\nTo evaluate and compare these estimators, the physicist uses a squared error loss function, $L(\\lambda, a) = (\\lambda - a)^2$. The performance of an estimator $\\delta$ is quantified by its risk function, $R(\\lambda, \\delta)$, which is the expected value of the loss function.\n\nDetermine the ratio of the risk of the second estimator to the risk of the first estimator, $\\frac{R(\\lambda, \\delta_2)}{R(\\lambda, \\delta_1)}$. Your final answer should be a closed-form analytic expression in terms of the sample size $n$ and the true mean rate $\\lambda$.", "solution": "We model $X_{1},\\ldots,X_{n}$ as i.i.d. $\\text{Poisson}(\\lambda)$ with $\\lambda0$. For any estimator $\\delta$, under squared error loss $L(\\lambda,a)=(\\lambda-a)^{2}$, the risk is the mean squared error:\n$$\nR(\\lambda,\\delta)=\\mathbb{E}_{\\lambda}\\big[(\\lambda-\\delta)^{2}\\big]=\\operatorname{Var}_{\\lambda}(\\delta)+\\big(\\mathbb{E}_{\\lambda}[\\delta]-\\lambda\\big)^{2}.\n$$\n\nProperties of the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ when $X_{i}\\sim\\text{Poisson}(\\lambda)$:\n- $\\mathbb{E}_{\\lambda}[X_{i}]=\\lambda$ and $\\operatorname{Var}_{\\lambda}(X_{i})=\\lambda$.\n- By linearity and independence,\n$$\n\\mathbb{E}_{\\lambda}[\\bar{X}]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\lambda}[X_{i}]=\\lambda,\\quad\n\\operatorname{Var}_{\\lambda}(\\bar{X})=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\operatorname{Var}_{\\lambda}(X_{i})=\\frac{\\lambda}{n}.\n$$\n\nRisk of $\\delta_{1}(\\mathbf{X})=\\bar{X}$:\n- Bias is $\\mathbb{E}_{\\lambda}[\\bar{X}]-\\lambda=0$.\n- Hence\n$$\nR(\\lambda,\\delta_{1})=\\operatorname{Var}_{\\lambda}(\\bar{X})+0^{2}=\\frac{\\lambda}{n}.\n$$\n\nRisk of $\\delta_{2}(\\mathbf{X})=\\bar{X}+1$:\n- Expectation $\\mathbb{E}_{\\lambda}[\\delta_{2}]=\\mathbb{E}_{\\lambda}[\\bar{X}]+1=\\lambda+1$, so bias is $1$.\n- Variance $\\operatorname{Var}_{\\lambda}(\\delta_{2})=\\operatorname{Var}_{\\lambda}(\\bar{X})=\\frac{\\lambda}{n}$.\n- Therefore\n$$\nR(\\lambda,\\delta_{2})=\\operatorname{Var}_{\\lambda}(\\delta_{2})+\\big(\\mathbb{E}_{\\lambda}[\\delta_{2}]-\\lambda\\big)^{2}=\\frac{\\lambda}{n}+1.\n$$\n\nRatio of risks:\n$$\n\\frac{R(\\lambda,\\delta_{2})}{R(\\lambda,\\delta_{1})}=\\frac{\\frac{\\lambda}{n}+1}{\\frac{\\lambda}{n}}=\\frac{\\lambda+n}{\\lambda}=1+\\frac{n}{\\lambda}.\n$$", "answer": "$$\\boxed{1+\\frac{n}{\\lambda}}$$", "id": "1924850"}, {"introduction": "What should we do when one estimator is not uniformly better than another across all possible states of nature? This is where decision principles like the minimax criterion become essential. This exercise simulates a common scenario where you must choose between a rule with variable performance and one with constant risk, providing a clear, hands-on application of the minimax principle to make a rational choice under uncertainty. [@problem_id:1924864]", "problem": "In a statistical decision problem, an analyst must choose between two decision rules, $\\delta_1$ and $\\delta_2$, to make an inference about an unknown parameter $\\theta$. The parameter $\\theta$ is known to lie in the interval $[0, 1]$. The performance of any decision rule $\\delta$ is evaluated using a pre-defined loss function, which leads to a risk function $R(\\theta, \\delta)$ representing the expected loss for a given value of $\\theta$.\n\nThe risk functions for the two rules are given as:\n1. $R(\\theta, \\delta_1) = A \\theta (1 - \\theta)$\n2. $R(\\theta, \\delta_2) = c$\n\nHere, $A$ and $c$ are known positive constants.\n\nThe analyst subscribes to the minimax principle for selecting a decision rule. This principle dictates that one should choose the rule that minimizes the maximum possible risk over all possible values of the parameter $\\theta$.\n\nFor the rule $\\delta_2$ to be strictly preferred over the rule $\\delta_1$ under the minimax principle, the constant $c$ must be less than a certain threshold value that depends on $A$. Determine this threshold value.", "solution": "Under the minimax principle, for a decision rule $\\delta$ the criterion is to minimize the maximum risk $\\sup_{\\theta \\in [0,1]} R(\\theta,\\delta)$. For $\\delta_{2}$, the risk is constant, so\n$$\n\\sup_{\\theta \\in [0,1]} R(\\theta,\\delta_{2}) = \\sup_{\\theta \\in [0,1]} c = c.\n$$\nFor $\\delta_{1}$, compute the supremum of $A\\theta(1-\\theta)$ on $[0,1]$. Using completing the square,\n$$\n\\theta(1-\\theta) = -\\theta^{2} + \\theta = -\\left(\\theta - \\frac{1}{2}\\right)^{2} + \\frac{1}{4},\n$$\nwhich attains its maximum $\\frac{1}{4}$ at $\\theta = \\frac{1}{2}$. Therefore,\n$$\n\\sup_{\\theta \\in [0,1]} R(\\theta,\\delta_{1}) = A \\cdot \\frac{1}{4} = \\frac{A}{4}.\n$$\nBy the minimax principle, $\\delta_{2}$ is strictly preferred over $\\delta_{1}$ if its maximal risk is strictly smaller:\n$$\nc  \\frac{A}{4}.\n$$\nThus, the threshold value for $c$ is $\\frac{A}{4}$.", "answer": "$$\\boxed{\\frac{A}{4}}$$", "id": "1924864"}]}