{"hands_on_practices": [{"introduction": "To build a solid understanding of convergence in mean square, we begin with a foundational exercise. This problem requires a direct application of the definition, breaking down the mean squared error into its bias and variance components. Mastering this calculation is the first step toward analyzing the behavior of more complex sequences of random variables [@problem_id:1910454].", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$. The statistical properties of the $n$-th variable in the sequence are defined by its mean and variance, given by the expressions:\n$$E[X_n] = \\mu + \\frac{(-1)^n \\alpha}{n}$$\n$$\\text{Var}(X_n) = \\frac{\\sigma^2}{n^2}$$\nwhere $\\mu$, $\\alpha$, and $\\sigma$ are given positive real constants.\n\nDetermine if the sequence $X_n$ converges in mean square to a constant. If it converges, what is the value of this constant? Your answer should be an expression written in terms of the given parameters.", "solution": "Convergence in mean square to a constant $c$ means $\\lim_{n \\to \\infty} E[(X_{n}-c)^{2}] = 0$. For any random variable $X$ and constant $c$, the bias-variance decomposition gives\n$$\nE[(X-c)^{2}] = \\text{Var}(X) + \\left(E[X] - c\\right)^{2}.\n$$\nApplying this to $X_{n}$ with the given moments yields\n$$\nE[(X_{n}-c)^{2}] = \\frac{\\sigma^{2}}{n^{2}} + \\left(\\mu + \\frac{(-1)^{n}\\alpha}{n} - c\\right)^{2}.\n$$\nExpanding the square,\n$$\n\\left(\\mu + \\frac{(-1)^{n}\\alpha}{n} - c\\right)^{2} = (\\mu - c)^{2} + 2(\\mu - c)\\frac{(-1)^{n}\\alpha}{n} + \\frac{\\alpha^{2}}{n^{2}}.\n$$\nTherefore,\n$$\nE[(X_{n}-c)^{2}] = (\\mu - c)^{2} + 2(\\mu - c)\\frac{(-1)^{n}\\alpha}{n} + \\frac{\\alpha^{2} + \\sigma^{2}}{n^{2}}.\n$$\nTaking the limit as $n \\to \\infty$,\n$$\n\\lim_{n \\to \\infty} E[(X_{n}-c)^{2}] = (\\mu - c)^{2},\n$$\nsince the terms proportional to $\\frac{1}{n}$ and $\\frac{1}{n^{2}}$ vanish. For mean square convergence to a constant, this limit must equal $0$, which requires $(\\mu - c)^{2} = 0$, hence $c = \\mu$. Substituting $c = \\mu$ back gives\n$$\nE[(X_{n}-\\mu)^{2}] = \\frac{\\alpha^{2} + \\sigma^{2}}{n^{2}} \\to 0,\n$$\nso $X_{n}$ converges in mean square to $\\mu$.", "answer": "$$\\boxed{\\mu}$$", "id": "1910454"}, {"introduction": "This next practice challenges our intuition by presenting a sequence where the random variable can take on increasingly large values as $n$ grows. Does this guarantee divergence? By rigorously applying the definition of mean square convergence, you will discover that the outcome depends on a delicate balance between the magnitude of the values and their probabilities. This problem highlights why we must rely on the mathematical framework rather than surface-level intuition [@problem_id:1910471].", "problem": "Consider a sequence of independent, discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each integer $n \\ge 1$, the probability mass function for $X_n$ is defined as follows:\n$$\nP(X_n = n^2) = \\frac{1}{n^5}\n$$\n$$\nP(X_n = 0) = 1 - \\frac{1}{n^5}\n$$\nWe wish to determine if the sequence $X_n$ converges in mean square to the constant random variable $X=0$. Which of the following statements is correct?\n\nA. Yes, the sequence converges in mean square to 0 because the probability $P(X_n \\neq 0)$ approaches 0 as $n \\to \\infty$.\nB. Yes, the sequence converges in mean square to 0 because the limit of the mean squared error, $\\lim_{n \\to \\infty} E[(X_n - 0)^2]$, is 0.\nC. No, the sequence does not converge in mean square to 0 because one of the possible values for $X_n$, namely $n^2$, grows infinitely large as $n \\to \\infty$.\nD. No, the sequence does not converge in mean square to 0 because the expected value $E[X_n]$ does not converge to 0.", "solution": "Mean-square convergence to a constant $X=0$ is defined by\n$$\n\\lim_{n \\to \\infty} E\\big[(X_{n}-0)^{2}\\big]=0.\n$$\nUsing the law of the unconscious statistician for a discrete random variable,\n$$\nE[X_{n}^{2}]=\\sum_{x} x^{2}\\, P(X_{n}=x).\n$$\nGiven $P(X_{n}=n^{2})=\\frac{1}{n^{5}}$ and $P(X_{n}=0)=1-\\frac{1}{n^{5}}$, we compute\n$$\nE[X_{n}^{2}]=(n^{2})^{2}\\cdot \\frac{1}{n^{5}}+0^{2}\\cdot\\left(1-\\frac{1}{n^{5}}\\right)=\\frac{n^{4}}{n^{5}}=\\frac{1}{n}.\n$$\nTherefore,\n$$\n\\lim_{n \\to \\infty} E[(X_{n}-0)^{2}]=\\lim_{n \\to \\infty} \\frac{1}{n}=0,\n$$\nso $X_{n}$ converges in mean square to $0$. This directly validates statement B.\n\nTo assess the other options:\n- A: $P(X_{n}\\neq 0)=\\frac{1}{n^{5}} \\to 0$ shows convergence in probability to $0$, which does not in general imply mean-square convergence; thus the reasoning for mean-square convergence is not correct.\n- C: The fact that a possible value $n^{2}$ grows without bound does not preclude mean-square convergence; what matters is the decay of $E[X_{n}^{2}]$.\n- D: $E[X_{n}]=n^{2}\\cdot \\frac{1}{n^{5}}=\\frac{1}{n^{3}} \\to 0$, so the stated reason is false.\n\nHence, the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "1910471"}, {"introduction": "Finally, let's connect theory to statistical practice. This problem examines convergence in mean square in the context of evaluating an estimator for a population mean. You will analyze a simple but flawed estimator to see why it fails to converge, providing a clear counterexample that underscores the importance of mean square convergence as a criterion for an estimator's consistency. This concept is fundamental to knowing whether our statistical methods actually improve with more data [@problem_id:1910475].", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a population with a finite mean $\\mu$ and a finite, non-zero variance $\\sigma^2$.\n\nConsider a sequence of estimators for the population mean $\\mu$, defined as $T_n = X_1$ for all sample sizes $n \\ge 1$. This means that for any sample size $n$, our estimate for the mean is always the value of the first observation.\n\nDoes the sequence of estimators $T_n$ converge in mean square to $\\mu$?\n\nA. Yes, the sequence converges in mean square to $\\mu$.\nB. No, the sequence does not converge in mean square to $\\mu$.\nC. The sequence converges in mean square, but to a value different from $\\mu$.\nD. There is not enough information to determine convergence, as the specific probability distribution of the $X_i$ is not provided.", "solution": "We recall the definition: a sequence of random variables $T_{n}$ converges in mean square to a constant $\\mu$ if and only if\n$$\n\\lim_{n\\to\\infty} E\\!\\left[(T_{n}-\\mu)^{2}\\right]=0.\n$$\nHere, $T_{n}=X_{1}$ for all $n\\geq 1$. Therefore,\n$$\nE\\!\\left[(T_{n}-\\mu)^{2}\\right]=E\\!\\left[(X_{1}-\\mu)^{2}\\right]=\\text{Var}(X_{1})=\\sigma^{2},\n$$\nusing the definition $\\text{Var}(X_{1})=E[(X_{1}-\\mu)^{2}]$ and the given that the variance is finite and non-zero. Since this quantity is the constant $\\sigma^{2}>0$ for all $n$, it does not tend to $0$ as $n\\to\\infty$.\n\nHence, $T_{n}$ does not converge in mean square to $\\mu$. (Note: While $T_{n}$ trivially converges in mean square to $X_{1}$, the question asks specifically about convergence to $\\mu$, which fails because $\\sigma^{2}\\neq 0$.)", "answer": "$$\\boxed{B}$$", "id": "1910475"}]}