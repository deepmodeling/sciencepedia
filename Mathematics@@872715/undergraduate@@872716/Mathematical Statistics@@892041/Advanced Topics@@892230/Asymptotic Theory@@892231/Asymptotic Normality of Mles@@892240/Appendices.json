{"hands_on_practices": [{"introduction": "The precision of a Maximum Likelihood Estimator (MLE) is fundamentally linked to the amount of information the data provides about the unknown parameter. Fisher information, $I(\\theta)$, quantifies this concept, forming the bedrock for the asymptotic theory of MLEs. This exercise [@problem_id:1896729] walks you through the essential calculation of Fisher information for a single observation from an exponential distribution, a common model for waiting times.", "problem": "Consider a random variable $X$ that models the time until a specific event occurs, such as the decay of a radioactive particle or the failure of an electronic component. This time is assumed to follow an exponential distribution with a constant rate parameter $\\lambda  0$. The probability density function (PDF) for $X$ is given by:\n$$f(x; \\lambda) = \\lambda \\exp(-\\lambda x) \\quad \\text{for } x \\ge 0$$\nThe Fisher information, denoted as $I(\\lambda)$, quantifies the amount of information that a single observation $X$ provides about the unknown parameter $\\lambda$. It is a fundamental concept in the theory of statistical estimation.\n\nCalculate the Fisher information $I(\\lambda)$ for a single observation from this exponential distribution.", "solution": "We are given a single observation from an exponential distribution with rate parameter $\\lambda0$ and density $f(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\ge 0$. The Fisher information for a single observation is defined by either\n$$\nI(\\lambda)=\\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\lambda}\\ln f(X;\\lambda)\\right)^{2}\\right]\n$$\nor equivalently\n$$\nI(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial \\lambda^{2}}\\ln f(X;\\lambda)\\right].\n$$\nWe start from the log-likelihood for one observation:\n$$\n\\ell(\\lambda;X)=\\ln f(X;\\lambda)=\\ln \\lambda-\\lambda X.\n$$\nDifferentiate with respect to $\\lambda$ to obtain the score:\n$$\n\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda;X)=\\frac{1}{\\lambda}-X.\n$$\nHence,\n$$\nI(\\lambda)=\\mathbb{E}\\!\\left[\\left(\\frac{1}{\\lambda}-X\\right)^{2}\\right]=\\mathbb{E}\\!\\left[\\frac{1}{\\lambda^{2}}-\\frac{2}{\\lambda}X+X^{2}\\right]=\\frac{1}{\\lambda^{2}}-\\frac{2}{\\lambda}\\mathbb{E}[X]+\\mathbb{E}[X^{2}].\n$$\nWe compute the necessary moments under $X\\sim \\text{Exp}(\\lambda)$. Using the integral identity $\\int_{0}^{\\infty} x^{n}\\exp(-\\lambda x)\\,dx=\\frac{n!}{\\lambda^{n+1}}$ (obtained from the substitution $y=\\lambda x$ and the definition of the gamma function), we have\n$$\n\\mathbb{E}[X]=\\int_{0}^{\\infty} x \\,\\lambda \\exp(-\\lambda x)\\,dx=\\lambda \\int_{0}^{\\infty} x \\exp(-\\lambda x)\\,dx=\\lambda \\cdot \\frac{1!}{\\lambda^{2}}=\\frac{1}{\\lambda},\n$$\nand\n$$\n\\mathbb{E}[X^{2}]=\\int_{0}^{\\infty} x^{2}\\,\\lambda \\exp(-\\lambda x)\\,dx=\\lambda \\int_{0}^{\\infty} x^{2}\\exp(-\\lambda x)\\,dx=\\lambda \\cdot \\frac{2!}{\\lambda^{3}}=\\frac{2}{\\lambda^{2}}.\n$$\nSubstituting these into the expression for $I(\\lambda)$ gives\n$$\nI(\\lambda)=\\frac{1}{\\lambda^{2}}-\\frac{2}{\\lambda}\\cdot \\frac{1}{\\lambda}+\\frac{2}{\\lambda^{2}}=\\frac{1}{\\lambda^{2}}.\n$$\nEquivalently, differentiating the log-likelihood twice,\n$$\n\\frac{\\partial^{2}}{\\partial \\lambda^{2}}\\ell(\\lambda;X)=-\\frac{1}{\\lambda^{2}},\n$$\nand thus\n$$\nI(\\lambda)=-\\mathbb{E}\\!\\left[-\\frac{1}{\\lambda^{2}}\\right]=\\frac{1}{\\lambda^{2}},\n$$\nwhich agrees with the previous result.", "answer": "$$\\boxed{\\frac{1}{\\lambda^{2}}}$$", "id": "1896729"}, {"introduction": "The theory of asymptotic normality has profound practical implications for designing experiments and studies. One of the most important consequences is that the standard error of an MLE decreases predictably as the sample size, $n$, increases, specifically scaling with $1/\\sqrt{n}$. This exercise [@problem_id:1896698] provides a concrete scenario to explore this relationship, demonstrating how to determine the required increase in sample size to achieve a desired level of precision in your estimate.", "problem": "A team of financial analysts is modeling the frequency of rare market shocks using a statistical distribution governed by a single, unknown parameter $\\theta$. They collect a dataset of $n$ observations and compute an estimate for this parameter using the method of Maximum Likelihood Estimation (MLE). Let this estimator be denoted by $\\hat{\\theta}_n$. Based on this initial dataset, the standard error of their estimator, $\\text{SE}(\\hat{\\theta}_n)$, is calculated.\n\nFor their model to be sufficiently reliable for risk management, the team determines that the standard error of their estimate must be reduced by a factor of 4. To achieve this, they plan to collect additional data, thereby increasing their total sample size.\n\nAssuming that the necessary regularity conditions are met, such that the asymptotic properties of the MLE provide a valid approximation, by what factor must the total sample size be increased to reduce the standard error to one-quarter of its original value? Your answer should be a single number representing this factor.", "solution": "Let $I(\\theta)$ denote the Fisher information for a single observation. Under standard regularity conditions for MLEs, the asymptotic distribution is\n$$\n\\sqrt{n}\\left(\\hat{\\theta}_{n}-\\theta\\right)\\;\\overset{d}{\\to}\\;\\mathcal{N}\\left(0,\\,I(\\theta)^{-1}\\right),\n$$\nwhich implies the asymptotic variance and standard error\n$$\n\\operatorname{Var}(\\hat{\\theta}_{n})\\approx \\frac{1}{n\\,I(\\theta)},\\qquad \\text{SE}(\\hat{\\theta}_{n})\\approx \\frac{1}{\\sqrt{n\\,I(\\theta)}}.\n$$\nThus $\\text{SE}(\\hat{\\theta}_{n})$ scales as $n^{-1/2}$. Let the original sample size be $n$ and the new total sample size be $n'$. Then\n$$\n\\frac{\\text{SE}(\\hat{\\theta}_{n'})}{\\text{SE}(\\hat{\\theta}_{n})}\\approx \\sqrt{\\frac{n}{n'}}.\n$$\nRequiring the new standard error to be one-quarter of the original gives\n$$\n\\sqrt{\\frac{n}{n'}}=\\frac{1}{4}\\;\\;\\Longrightarrow\\;\\;\\frac{n}{n'}=\\frac{1}{16}\\;\\;\\Longrightarrow\\;\\;n'=16\\,n.\n$$\nTherefore, the total sample size must be increased by a factor of $16$.", "answer": "$$\\boxed{16}$$", "id": "1896698"}, {"introduction": "Often in statistical analysis, our interest lies not in the parameter $p$ itself, but in a transformation of it, such as the log-odds, $g(p) = \\ln(p/(1-p))$. The Delta Method is a vital tool that extends the principle of asymptotic normality to functions of MLEs, allowing us to approximate their distribution and variance. This practice problem [@problem_id:1896726] illustrates how to apply the Delta Method to find the asymptotic variance of the log-odds, a crucial transformation in fields like epidemiology and machine learning.", "problem": "In a large-scale clinical trial, the effectiveness of a new treatment is modeled as a Bernoulli process. Let $X_1, X_2, \\dots, X_n$ be a random sample of size $n$ from a Bernoulli distribution with parameter $p$, representing the probability of a successful treatment. The parameter $p$ is unknown, with $0  p  1$.\n\nThe standard estimator for $p$ is the sample proportion, $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n X_i$, which is the Maximum Likelihood Estimator (MLE). For a large sample size $n$, it is a known result that $\\hat{p}$ is approximately normally distributed with a mean of $p$ and a variance of $\\frac{p(1-p)}{n}$.\n\nStatisticians are often interested in the log-odds of success, defined as $L(p) = \\ln\\left(\\frac{p}{1-p}\\right)$, where $\\ln$ is the natural logarithm. The corresponding estimator is the log-odds of the sample proportion, $L(\\hat{p})$. Based on large-sample theory, this estimator $L(\\hat{p})$ is also approximately normally distributed.\n\nDetermine the variance of the approximate normal distribution for $L(\\hat{p})$. Your answer should be a closed-form analytic expression in terms of $p$ and $n$.", "solution": "We are given that, for large $n$, the sample proportion $\\hat{p}$ is approximately normal with mean $p$ and variance $\\frac{p(1-p)}{n}$:\n$$\n\\hat{p}\\approx \\mathcal{N}\\!\\left(p,\\ \\frac{p(1-p)}{n}\\right).\n$$\nWe seek the large-sample variance of $L(\\hat{p})$, where $L(p)=\\ln\\!\\left(\\frac{p}{1-p}\\right)$. Let $g(p)=L(p)$. By the Delta method, if $g$ is differentiable at $p$, then\n$$\n\\operatorname{Var}\\big(g(\\hat{p})\\big)\\approx \\big(g'(p)\\big)^{2}\\,\\operatorname{Var}(\\hat{p}).\n$$\nCompute $g'(p)$:\n$$\ng(p)=\\ln(p)-\\ln(1-p),\\quad g'(p)=\\frac{1}{p}+\\frac{1}{1-p}=\\frac{1}{p(1-p)}.\n$$\nSubstituting into the Delta method formula with $\\operatorname{Var}(\\hat{p})=\\frac{p(1-p)}{n}$ gives\n$$\n\\operatorname{Var}\\big(L(\\hat{p})\\big)\\approx \\left(\\frac{1}{p(1-p)}\\right)^{2}\\cdot \\frac{p(1-p)}{n}\n= \\frac{1}{n\\,p(1-p)}.\n$$\nTherefore, the asymptotic variance of the approximate normal distribution for $L(\\hat{p})$ is $\\frac{1}{n\\,p(1-p)}$.", "answer": "$$\\boxed{\\frac{1}{n\\,p(1-p)}}$$", "id": "1896726"}]}