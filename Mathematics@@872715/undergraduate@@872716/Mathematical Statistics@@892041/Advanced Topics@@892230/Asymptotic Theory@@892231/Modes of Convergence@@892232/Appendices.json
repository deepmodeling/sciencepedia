{"hands_on_practices": [{"introduction": "To begin our exploration, let's examine a sequence of random variables that converges in multiple, strong ways. This exercise provides a concrete example where a sequence converges in mean square, in probability, and almost surely, illustrating the ideal behavior described by foundational theorems in statistics. Understanding this \"well-behaved\" case builds a solid foundation before we explore more complex scenarios. [@problem_id:1319236]", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ be a sequence of independent random variables. For each positive integer $n$, the random variable $X_n$ is drawn from a Normal distribution with a mean of $\\frac{1}{n}$ and a variance of $\\frac{1}{n^2}$.\n\nConsider the following statements regarding the limiting behavior of this sequence as $n$ approaches infinity:\n\nI. The sequence converges in probability to 0.\nII. The sequence converges in mean square to 0.\nIII. The sequence converges almost surely to 0.\n\nWhich of the above statements is/are correct?\n\nA. I only\n\nB. II only\n\nC. I and II only\n\nD. I and III only\n\nE. II and III only\n\nF. I, II, and III\n\nG. None are correct", "solution": "Let $X_{n} \\sim \\mathcal{N}(\\mu_{n},\\sigma_{n}^{2})$ with $\\mu_{n}=\\frac{1}{n}$ and $\\sigma_{n}^{2}=\\frac{1}{n^{2}}$.\n\nTo check convergence in mean square to $0$, compute\n$$\n\\mathbb{E}\\big[(X_{n}-0)^{2}\\big]=\\operatorname{Var}(X_{n})+\\big(\\mathbb{E}[X_{n}]\\big)^{2}=\\frac{1}{n^{2}}+\\frac{1}{n^{2}}=\\frac{2}{n^{2}} \\xrightarrow[n\\to\\infty]{} 0.\n$$\nThus $X_{n} \\to 0$ in mean square, so Statement II is true.\n\nConvergence in mean square implies convergence in probability. Explicitly, for any $\\epsilon0$, by Chebyshev (or Markov) inequality,\n$$\n\\mathbb{P}(|X_{n}|\\epsilon)\\leq \\frac{\\mathbb{E}[X_{n}^{2}]}{\\epsilon^{2}}=\\frac{2}{\\epsilon^{2}n^{2}} \\xrightarrow[n\\to\\infty]{} 0,\n$$\nso $X_{n} \\to 0$ in probability, and Statement I is true.\n\nFor almost sure convergence, fix $\\epsilon0$ and define $Y_{n}=nX_{n}$. Then $Y_{n}\\sim \\mathcal{N}(1,1)$ for all $n$, and\n$$\n\\mathbb{P}(|X_{n}|\\epsilon)=\\mathbb{P}(|Y_{n}|n\\epsilon).\n$$\nLet $W\\sim \\mathcal{N}(0,1)$. For $t1$, $\\mathbb{P}(|Y_{n}|t)=\\mathbb{P}(|W+1|t)\\leq \\mathbb{P}(|W|t-1)\\leq 2\\exp\\big(-\\frac{(t-1)^{2}}{2}\\big)$. For $n$ large so that $n\\epsilon1$,\n$$\n\\mathbb{P}(|X_{n}|\\epsilon)\\leq 2\\exp\\big(-\\tfrac{(n\\epsilon-1)^{2}}{2}\\big).\n$$\nHence\n$$\n\\sum_{n=1}^{\\infty}\\mathbb{P}(|X_{n}|\\epsilon)\\infty\n$$\nby comparison with a Gaussian-decay series. By the first Borel-Cantelli lemma, $\\mathbb{P}(|X_{n}|\\epsilon \\text{ i.o.})=0$, so $X_{n}\\to 0$ almost surely. Therefore Statement III is true.\n\nAll three statements I, II, III are correct.", "answer": "$$\\boxed{F}$$", "id": "1319236"}, {"introduction": "The great theorems of probability, like the Law of Large Numbers, have specific conditions for a reason. This practice explores a famous and instructive counterexample—the Cauchy distribution—to demonstrate what happens when those conditions are not met. By analyzing the sample mean of Cauchy variables, you will discover a scenario where the law of large numbers fails, deepening your appreciation for the theorem's assumptions. [@problem_id:1319185]", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of $n$ independent and identically distributed (i.i.d.) random variables, where $n$ is a positive integer. Each random variable $X_i$ follows a standard Cauchy distribution, which is defined by the probability density function (PDF):\n$$f(x) = \\frac{1}{\\pi(1 + x^2)}, \\quad \\text{for } -\\infty  x  \\infty$$\nThe sample mean of these random variables is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\nIdentify the distribution of the sample mean $\\bar{X}_n$.\n\nA. A Normal distribution with mean 0 and variance $\\frac{1}{n}$.\n\nB. A Normal distribution with mean 0 and variance 1.\n\nC. A Cauchy distribution with location parameter 0 and scale parameter $n$.\n\nD. A standard Cauchy distribution (location parameter 0, scale parameter 1).\n\nE. A Student's t-distribution with $n-1$ degrees of freedom.\n\nF. The distribution is undefined because the mean of the Cauchy distribution does not exist.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. standard Cauchy with density $f(x)=\\frac{1}{\\pi(1+x^{2})}$. The characteristic function of a standard Cauchy random variable is\n$$\n\\phi_{X}(t)=\\int_{-\\infty}^{\\infty}\\exp(itx)\\frac{1}{\\pi(1+x^{2})}\\,dx=\\exp(-|t|).\n$$\nBy independence, the sum $S_{n}=\\sum_{i=1}^{n}X_{i}$ has characteristic function\n$$\n\\phi_{S_{n}}(t)=\\prod_{i=1}^{n}\\phi_{X}(t)=[\\exp(-|t|)]^{n}=\\exp(-n|t|).\n$$\nThis is the characteristic function of a Cauchy distribution with location $0$ and scale parameter $n$. For the sample mean $\\bar{X}_{n}=\\frac{S_{n}}{n}$, use the scaling property of characteristic functions: if $Y=aZ$, then $\\phi_{Y}(t)=\\phi_{Z}(at)$. Hence\n$$\n\\phi_{\\bar{X}_{n}}(t)=\\phi_{S_{n}}\\!\\left(\\frac{t}{n}\\right)=\\exp\\!\\left(-n\\left|\\frac{t}{n}\\right|\\right)=\\exp(-|t|),\n$$\nwhich is exactly the characteristic function of the standard Cauchy distribution (location $0$, scale $1$). Therefore, $\\bar{X}_{n}$ is standard Cauchy for every $n$. Although the mean of a Cauchy distribution does not exist, the distribution of the sample mean is well defined and equals the original standard Cauchy distribution, so option F is incorrect.\n\nThe correct choice is the standard Cauchy distribution, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1319185"}, {"introduction": "Theoretical concepts of convergence find their true power when applied to statistical inference. This final practice challenges you to combine different modes of convergence to analyze a complex statistic, using powerful tools like the Delta Method or Slutsky's Theorem. This exercise demonstrates a fundamental skill for deriving the asymptotic properties of estimators and test statistics in practical applications. [@problem_id:798678]", "problem": "Let $\\{X_i\\}_{i=1}^n$ be a sequence of independent and identically distributed (i.i.d.) random variables from a Normal distribution with mean $\\mu_X = 0$ and variance $\\sigma_X^2$.\nLet $\\{Y_i\\}_{i=1}^n$ be another sequence of i.i.d. random variables, independent of the $\\{X_i\\}$ sequence, drawn from an Exponential distribution with rate parameter $\\lambda$. The probability density function for the Exponential distribution is given by $f(y; \\lambda) = \\lambda e^{-\\lambda y}$ for $y \\ge 0$.\n\nLet $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ and $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^n Y_i$ denote the respective sample means.\nConsider the statistic $T_n$ defined as:\n$$\nT_n = \\sqrt{n} \\frac{\\bar{X}_n}{\\bar{Y}_n}\n$$\nThe statistic $T_n$ converges in distribution to a random variable $T$ as $n \\to \\infty$. Determine the variance of this limiting distribution, $\\text{Var}(T)$.", "solution": "We apply the multivariate Delta method to $(\\bar X_n,\\bar Y_n)$.\n\n1. CLT for the sample means:\n$$\n\\sqrt{n}\\begin{pmatrix}\\bar X_n-0\\\\\\bar Y_n-\\tfrac1\\lambda\\end{pmatrix}\n\\Rightarrow N\\!\\Bigl(\\begin{pmatrix}0\\\\0\\end{pmatrix},\n\\begin{pmatrix}\\sigma_X^20\\\\01/\\lambda^2\\end{pmatrix}\\Bigr).\n$$\n\n2. Define $g(a,b)=a/b$.  Then\n$$\nT_n = \\sqrt{n}\\bigl(g(\\bar X_n,\\bar Y_n)-g(0,1/\\lambda)\\bigr),\n\\quad g(0,1/\\lambda)=0.\n$$\n\n3. Compute the gradient at $(0,1/\\lambda)$:\n$$\n\\nabla g(a,b)=\\Bigl(\\frac{\\partial}{\\partial a},\\frac{\\partial}{\\partial b}\\Bigr)\n=\\Bigl(\\tfrac1b,\\,-\\tfrac{a}{b^2}\\Bigr),\n\\quad\\nabla g(0,1/\\lambda)=(\\lambda,0).\n$$\n\n4. By the Delta method,\n$$\nT_n\\Rightarrow N\\bigl(0,\\;(\\nabla g)^T\\Sigma\\,(\\nabla g)\\bigr),\n\\quad\\Sigma=\\begin{pmatrix}\\sigma_X^20\\\\01/\\lambda^2\\end{pmatrix}.\n$$\n\n5. Hence\n$$\n\\mathrm{Var}(T)\n=(\\lambda,0)\\begin{pmatrix}\\sigma_X^20\\\\01/\\lambda^2\\end{pmatrix}\n\\begin{pmatrix}\\lambda\\\\0\\end{pmatrix}\n=\\lambda^2\\,\\sigma_X^2.\n$$", "answer": "$$\\boxed{\\lambda^2\\sigma_X^2}$$", "id": "798678"}]}