{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice applies the Likelihood Ratio Test (LRT) to a familiar one-parameter model. In this exercise [@problem_id:1896229], we use data from a Geometric distribution to test a simple hypothesis about its success probability. This hands-on example will demonstrate the core principle of Wilks' theorem in a clear and direct setting, showing how the $-2 \\ln \\Lambda$ statistic converges to a chi-squared distribution.", "problem": "An experiment is conducted by repeatedly flipping a biased coin until the first \"Heads\" is observed. The random variable $X$ represents the total number of flips required. This process is modeled by a Geometric distribution with success probability $p$, where $p$ is the probability of getting \"Heads\" on a single flip. The Probability Mass Function (PMF) for $X$ is given by $P(X=k) = (1-p)^{k-1}p$ for $k=1, 2, 3, \\ldots$ and $0  p  1$.\n\nSuppose we have a large sample of $n$ independent and identically distributed (i.i.d.) observations, $X_1, X_2, \\ldots, X_n$, from this Geometric distribution. We wish to test the simple null hypothesis $H_0: p = p_0$ against the composite alternative hypothesis $H_1: p \\neq p_0$, where $p_0$ is a fixed, known value in the interval $(0, 1)$.\n\nThe Likelihood Ratio Test (LRT) statistic for this test is defined as:\n$$\n\\Lambda_n = \\frac{\\sup_{p \\in \\{p_0\\}} L(p|\\mathbf{x})}{\\sup_{p \\in (0,1)} L(p|\\mathbf{x})}\n$$\nwhere $L(p|\\mathbf{x})$ is the likelihood function for the observed sample $\\mathbf{x} = (x_1, \\ldots, x_n)$.\n\nAccording to a fundamental theorem in mathematical statistics regarding likelihood ratio tests, the distribution of the transformed statistic $-2\\ln\\Lambda_n$ converges to a specific well-known distribution as the sample size $n \\to \\infty$, provided the null hypothesis $H_0$ is true.\n\nWhat is this limiting (asymptotic) distribution of $-2\\ln\\Lambda_n$?\n\nA. A Normal distribution with mean 0 and variance 1, $N(0, 1)$.\n\nB. A Chi-squared distribution with 1 degree of freedom, $\\chi^2_1$.\n\nC. A Chi-squared distribution with $n-1$ degrees of freedom, $\\chi^2_{n-1}$.\n\nD. An F-distribution with $(1, n-1)$ degrees of freedom, $F_{1, n-1}$.\n\nE. A t-distribution with $n-1$ degrees of freedom, $t_{n-1}$.", "solution": "We observe i.i.d. $X_{1},\\ldots,X_{n}$ from a Geometric distribution with PMF $P(X=k)=(1-p)^{k-1}p$ for $k=1,2,\\ldots$ and $p\\in(0,1)$. The likelihood for a sample $\\mathbf{x}=(x_{1},\\ldots,x_{n})$ is\n$$\nL(p\\mid\\mathbf{x})=\\prod_{i=1}^{n}p(1-p)^{x_{i}-1}=p^{n}(1-p)^{\\sum_{i=1}^{n}x_{i}-n}.\n$$\nLet $S=\\sum_{i=1}^{n}x_{i}$. Then the log-likelihood is\n$$\n\\ell(p)=\\ln L(p\\mid\\mathbf{x})=n\\ln p+(S-n)\\ln(1-p).\n$$\nThe unrestricted MLE $\\hat{p}$ solves $\\frac{\\partial}{\\partial p}\\ell(p)=0$:\n$$\n\\frac{\\partial}{\\partial p}\\ell(p)=\\frac{n}{p}-\\frac{S-n}{1-p}=0\n\\;\\;\\Longrightarrow\\;\\; n(1-p)-p(S-n)=0\n\\;\\;\\Longrightarrow\\;\\; n-pS=0\n\\;\\;\\Longrightarrow\\;\\; \\hat{p}=\\frac{n}{S}.\n$$\nThe likelihood ratio statistic is\n$$\n\\Lambda_{n}=\\frac{\\sup_{p\\in\\{p_{0}\\}}L(p\\mid\\mathbf{x})}{\\sup_{p\\in(0,1)}L(p\\mid\\mathbf{x})}\n=\\frac{L(p_{0}\\mid\\mathbf{x})}{L(\\hat{p}\\mid\\mathbf{x})}.\n$$\nUnder the null hypothesis $H_{0}:p=p_{0}$ with $p_{0}\\in(0,1)$, this is a regular one-parameter problem in an open parameter space, and the Geometric family is a regular exponential family. Therefore, by Wilksâ€™ theorem, the distribution of $-2\\ln\\Lambda_{n}$ converges in distribution, as $n\\to\\infty$, to a chi-squared distribution with degrees of freedom equal to the difference in dimensionality between the unrestricted and restricted parameter spaces. Here, the unrestricted model has dimension $1$ and the null model has dimension $0$, so the difference is $1$. Hence,\n$$\n-2\\ln\\Lambda_{n}\\;\\xrightarrow{d}\\;\\chi^{2}_1\\quad\\text{under }H_{0}.\n$$\nTherefore, the correct choice is the chi-squared distribution with $1$ degree of freedom.", "answer": "$$\\boxed{B}$$", "id": "1896229"}, {"introduction": "Having mastered the basic application, we now extend the concept to the powerful framework of multiple linear regression. This problem [@problem_id:1896221] moves beyond testing a single parameter value to evaluating a linear relationship between two model coefficients. It serves as a practical illustration of how the LRT's power generalizes, reinforcing the crucial idea that the asymptotic chi-squared distribution's degrees of freedom correspond to the number of independent constraints defined by the null hypothesis.", "problem": "A data science team is analyzing the factors that influence user engagement on their platform. They propose a multiple linear regression model to predict the weekly hours a user spends on the service, $Y$. The model includes two explanatory variables: $x_1$, a score representing the user's exposure to a content-ranking algorithm named \"EngageMax\", and $x_2$, a score representing exposure to a different algorithm named \"ExploreNow\".\n\nThe model is given by:\n$$Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$$\nfor users $i = 1, \\dots, n$. The error terms $\\epsilon_i$ are assumed to be independent and identically distributed (i.i.d.) as normal random variables with a mean of 0 and an unknown variance $\\sigma^2 > 0$. The covariates $x_{i1}$ and $x_{i2}$ are treated as fixed, non-random quantities.\n\nThe team wants to determine if the two algorithms have an identical impact on user engagement. To do this, they formulate the null hypothesis $H_0: \\beta_1 = \\beta_2$. They construct the Likelihood Ratio Test (LRT) statistic, $T_n$, defined as:\n$$T_n = -2 \\ln\\left(\\frac{L_0}{L_1}\\right)$$\nwhere $L_0$ represents the maximized likelihood of the data under the null hypothesis $H_0$, and $L_1$ represents the maximized likelihood under the full, unrestricted model.\n\nAs the sample size $n$ tends to infinity, the test statistic $T_n$ converges to a specific probability distribution. Which of the following correctly describes this limiting distribution?\n\nA. A Chi-squared distribution with 1 degree of freedom.\n\nB. A Chi-squared distribution with 2 degrees of freedom.\n\nC. A Chi-squared distribution with 3 degrees of freedom.\n\nD. An F-distribution with $(1, n-3)$ degrees of freedom.\n\nE. A standard Normal distribution, $N(0,1)$.", "solution": "We have the normal linear regression model with fixed regressors:\n$$Y_{i} = \\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + \\epsilon_{i}, \\quad \\epsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} N(0,\\sigma^{2}), \\quad i=1,\\dots,n.$$\nWe test the null hypothesis $H_{0}:\\beta_{1}=\\beta_{2}$ using the likelihood ratio test with statistic\n$$T_{n} = -2 \\ln\\left(\\frac{L_{0}}{L_{1}}\\right),$$\nwhere $L_{0}$ is the maximized likelihood under $H_{0}$ and $L_{1}$ is the maximized likelihood under the full model.\n\nBy Wilks' theorem, under standard regularity conditions (which hold here for the normal linear model with fixed regressors and identifiable parameters), the asymptotic distribution under $H_{0}$ of the likelihood ratio statistic is\n$$T_{n} \\xrightarrow{d} \\chi^{2}_{k},$$\nwhere $k$ is the difference in the dimensions of the parameter spaces between the unrestricted and restricted models.\n\nCompute $k$:\n- Unrestricted parameter vector is $(\\beta_0, \\beta_1, \\beta_2, \\sigma^2)$, which has dimension $4$.\n- Under $H_{0}:\\beta_{1}=\\beta_{2}$, we can reparameterize as $(\\beta_0, \\beta, \\sigma^2)$ with $\\beta=\\beta_{1}=\\beta_{2}$, giving dimension $3$.\n\nThus the number of independent restrictions is $k=4-3=1$, so\n$$T_{n} \\xrightarrow{d} \\chi^{2}_{1}.$$\nThis corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1896221"}, {"introduction": "Our final practice explores the important boundaries of asymptotic theory by examining a \"non-regular\" case where Wilks' theorem does not apply in its standard form. The Uniform distribution, whose support depends on its parameters, violates the regularity conditions required for the typical $\\chi^2$ convergence. This challenging exercise [@problem_id:1896246] reveals that the LRT statistic can converge to a different limiting distribution, underscoring the critical need to check a model's underlying assumptions before applying a theorem.", "problem": "Let $X_1, X_2, \\dots, X_n$ be an independent and identically distributed random sample from a uniform distribution on the interval $[\\theta_1, \\theta_2]$, where $\\theta_1, \\theta_2 \\in \\mathbb{R}$ with $\\theta_1  \\theta_2$ are unknown parameters. Consider the hypothesis test with the null hypothesis $H_0: \\theta_2 - \\theta_1 = c$ against the alternative hypothesis $H_1: \\theta_2 - \\theta_1 \\neq c$, where $c$ is a known positive constant.\n\nLet $\\Lambda_n$ denote the Likelihood Ratio Test (LRT) statistic for this test, defined as the ratio of the maximum likelihood under the null hypothesis to the maximum likelihood over the entire parameter space. Determine the limiting distribution of the statistic $T_n = -2 \\ln \\Lambda_n$ as the sample size $n$ approaches infinity, under the assumption that the null hypothesis $H_0$ is true.\n\nSelect the correct description of the limiting distribution from the options below:\n\nA. A chi-squared distribution with 1 degree of freedom.\n\nB. A chi-squared distribution with 2 degrees of freedom.\n\nC. A standard normal distribution.\n\nD. A chi-squared distribution with 4 degrees of freedom.\n\nE. An exponential distribution with a rate parameter of 1.\n\nF. A gamma distribution with a shape parameter of 2 and a rate parameter of 1.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. $\\operatorname{Uniform}[\\theta_{1},\\theta_{2}]$ with unknown $\\theta_{1}\\theta_{2}$ and let $R_{n}=X_{(n)}-X_{(1)}$ denote the sample range.\n\nThe likelihood for $(\\theta_{1},\\theta_{2})$ is\n$$\nL(\\theta_{1},\\theta_{2})=(\\theta_{2}-\\theta_{1})^{-n}\\,\\mathbf{1}\\{\\theta_{1}\\leq X_{(1)},\\,\\theta_{2}\\geq X_{(n)}\\}.\n$$\nMaximizing over the full parameter space yields the unconstrained MLE $\\hat{\\theta}_{1}=X_{(1)}$, $\\hat{\\theta}_{2}=X_{(n)}$, with maximized likelihood\n$$\nL_{\\text{full}}=R_{n}^{-n}.\n$$\nUnder $H_{0}:\\theta_{2}-\\theta_{1}=c$, any interval of length $c$ that contains all observations has likelihood $c^{-n}$; such an interval exists if and only if $R_{n}\\leq c$. Under $H_{0}$ this holds with probability $1$, so the constrained maximum is\n$$\nL_{0}=c^{-n}.\n$$\nHence the likelihood ratio is\n$$\n\\Lambda_{n}=\\frac{L_{0}}{L_{\\text{full}}}=\\left(\\frac{R_{n}}{c}\\right)^{n},\n$$\nand therefore\n$$\nT_{n}=-2\\ln\\Lambda_{n}=2n\\ln\\left(\\frac{c}{R_{n}}\\right).\n$$\n\nTo obtain the limiting distribution of $T_{n}$ under $H_{0}$, reparameterize to $Y_{i}=(X_{i}-\\theta_{1})/c\\sim \\operatorname{Uniform}[0,1]$. Then\n$$\nR_{n}=c\\bigl(Y_{(n)}-Y_{(1)}\\bigr),\\qquad D_{n}=c-R_{n}=c\\bigl[(1-Y_{(n)})+Y_{(1)}\\bigr]\\geq 0.\n$$\nClassical extreme-value limits for the uniform distribution give\n$$\nnY_{(1)}\\xrightarrow{d}E_{1},\\qquad n(1-Y_{(n)})\\xrightarrow{d}E_{2},\n$$\nwhere $E_{1}$ and $E_{2}$ are independent $\\operatorname{Exp}(1)$ variables. Thus\n$$\n\\frac{nD_{n}}{c}=n\\bigl[(1-Y_{(n)})+Y_{(1)}\\bigr]\\xrightarrow{d}E_{1}+E_{2}\\equiv G,\n$$\nwith $G\\sim \\operatorname{Gamma}(2,1)$ (shape $2$, rate $1$).\n\nSince $D_{n}/c\\to 0$ in probability at rate $n^{-1}$, use the expansion\n$$\n\\ln\\left(\\frac{c}{R_{n}}\\right)=-\\ln\\left(1-\\frac{D_{n}}{c}\\right)=\\frac{D_{n}}{c}+o_{p}\\left(\\frac{D_{n}}{c}\\right).\n$$\nTherefore\n$$\nT_{n}=2n\\ln\\left(\\frac{c}{R_{n}}\\right)=2n\\frac{D_{n}}{c}+o_{p}(1)\\xrightarrow{d}2G.\n$$\nIf $G\\sim \\operatorname{Gamma}(2,1)$, then $2G\\sim \\operatorname{Gamma}\\bigl(2,\\tfrac{1}{2}\\bigr)$, which is a chi-squared distribution with $4$ degrees of freedom. Hence, under $H_{0}$,\n$$\nT_{n}\\xrightarrow{d}\\chi^{2}_{4}.\n$$\n\nThus the correct option is D.", "answer": "$$\\boxed{D}$$", "id": "1896246"}]}