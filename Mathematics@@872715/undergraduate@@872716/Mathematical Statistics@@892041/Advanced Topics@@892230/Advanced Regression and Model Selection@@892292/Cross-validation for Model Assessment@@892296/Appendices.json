{"hands_on_practices": [{"introduction": "Before we can evaluate any model, we must first correctly partition our data. This exercise focuses on the fundamental mechanics of K-fold cross-validation, specifically how to determine the sizes of the training and validation sets in each fold. Mastering this setup is the first step toward implementing a methodologically sound validation procedure. [@problem_id:1912440]", "problem": "A data scientist is working with a dataset containing $n=97$ independent observations. To assess the generalization performance of a machine learning model, they decide to employ a $k$-fold cross-validation procedure with $k=10$.\n\nIn this method, the entire dataset is first partitioned into $k$ non-overlapping subsets, called \"folds\". Since the total number of observations $n$ is not perfectly divisible by the number of folds $k$, the folds are created to be of as nearly equal size as possible. Specifically, some folds will contain $\\lfloor n/k \\rfloor$ observations, while the rest will contain $\\lceil n/k \\rceil$ observations.\n\nThe cross-validation process then consists of $k$ iterations. In each iteration, a different fold is held out as the validation set, and the remaining $k-1$ folds are combined to form the training set. Consequently, the size of the training set can vary slightly from one iteration to another.\n\nDetermine the minimum possible size of the training set that will be encountered during any of the 10 iterations of this procedure.", "solution": "We are given $n=97$ total observations and $k=10$ folds. In $k$-fold cross-validation, the dataset is partitioned into $k$ folds whose sizes are as equal as possible, meaning some folds have size $\\lfloor n/k \\rfloor$ and the rest have size $\\lceil n/k \\rceil$.\n\nCompute the base fold sizes:\n$$\n\\left\\lfloor \\frac{n}{k} \\right\\rfloor = \\left\\lfloor \\frac{97}{10} \\right\\rfloor = 9, \\quad \\left\\lceil \\frac{n}{k} \\right\\rceil = \\left\\lceil \\frac{97}{10} \\right\\rceil = 10.\n$$\nBy the division algorithm,\n$$\nn = k \\left\\lfloor \\frac{n}{k} \\right\\rfloor + r \\quad \\text{with} \\quad r = n - k \\left\\lfloor \\frac{n}{k} \\right\\rfloor.\n$$\nHere,\n$$\nr = 97 - 10 \\cdot 9 = 7.\n$$\nTherefore, there are $r=7$ folds of size $10$ and $k-r=3$ folds of size $9$.\n\nIn each iteration, the training set size equals the total number of observations minus the size of the held-out fold. Thus, if a fold of size $s$ is held out, the training size is\n$$\nT = n - s.\n$$\nThe minimum training size occurs when $s$ is as large as possible, i.e., when holding out a fold of size $\\lceil n/k \\rceil = 10$, giving\n$$\nT_{\\min} = n - \\left\\lceil \\frac{n}{k} \\right\\rceil = 97 - 10 = 87.\n$$\nFor completeness, if a fold of size $9$ were held out, the training size would be $97 - 9 = 88$, which is larger. Hence, the minimum possible training set size over the $10$ iterations is $87$.", "answer": "$$\\boxed{87}$$", "id": "1912440"}, {"introduction": "A primary application of cross-validation is to guide model selection, helping us choose the best model from a set of candidates. This practice moves from theory to application, introducing the \"one-standard-error rule,\" a valuable heuristic for balancing predictive accuracy with model simplicity. By working through this scenario, you will learn how to make a principled choice that avoids overfitting and favors more parsimonious models. [@problem_id:1912455]", "problem": "A team of biostatisticians is building a predictive model for disease risk using genomic data. They are using a method called Least Absolute Shrinkage and Selection Operator (LASSO) regression. The complexity of the LASSO model is controlled by a tuning parameter, $\\lambda$. A higher value of $\\lambda$ results in a simpler model with fewer predictor variables.\n\nTo choose the best value for $\\lambda$, the team performs K-fold cross-validation (CV). They test five different values for $\\lambda$ and, for each one, they compute the mean CV prediction error and the standard error of that mean. The results are summarized below:\n\n*   **Model A**: $\\lambda = 0.01$, Number of predictors = 12, Mean CV Error = 0.35, Standard Error = 0.05\n*   **Model B**: $\\lambda = 0.05$, Number of predictors = 9, Mean CV Error = 0.31, Standard Error = 0.04\n*   **Model C**: $\\lambda = 0.10$, Number of predictors = 6, Mean CV Error = 0.28, Standard Error = 0.03\n*   **Model D**: $\\lambda = 0.20$, Number of predictors = 4, Mean CV Error = 0.30, Standard Error = 0.04\n*   **Model E**: $\\lambda = 0.50$, Number of predictors = 2, Mean CV Error = 0.34, Standard Error = 0.06\n\nThe team decides to use the \"one-standard-error rule\" to select the final model. This rule favors simpler models to avoid overfitting. According to the one-standard-error rule, which model should be selected for the final analysis?\n\nA. Model A\nB. Model B\nC. Model C\nD. Model D\nE. Model E", "solution": "We apply the one-standard-error (1-SE) rule used with cross-validation. Let $\\hat{E}(\\lambda)$ denote the mean CV error and $\\operatorname{SE}(\\lambda)$ its standard error. First, identify the model with the minimal mean CV error:\n- The minimum mean CV error is $\\hat{E}(\\lambda^{\\ast})=0.28$ at Model C, with $\\operatorname{SE}(\\lambda^{\\ast})=0.03$.\n\nCompute the 1-SE threshold:\n$$\nT=\\hat{E}(\\lambda^{\\ast})+\\operatorname{SE}(\\lambda^{\\ast})=0.28+0.03=0.31.\n$$\n\nBy the 1-SE rule, select the simplest model (largest $\\lambda$, fewest predictors) whose mean CV error is at most $T$. Check each modelâ€™s mean CV error against $T=0.31$:\n- Model A: $0.35>T$ (exclude).\n- Model B: $0.31\\leq T$ (eligible).\n- Model C: $0.28\\leq T$ (eligible).\n- Model D: $0.30\\leq T$ (eligible).\n- Model E: $0.34>T$ (exclude).\n\nAmong eligible models $\\{B,C,D\\}$, choose the simplest (largest $\\lambda$ or fewest predictors). Their complexities are: B has $9$ predictors, C has $6$, D has $4$. The simplest among these is Model D. Therefore, by the 1-SE rule, Model D should be selected.", "answer": "$$\\boxed{D}$$", "id": "1912455"}, {"introduction": "Real-world datasets are rarely perfect and often require preprocessing steps like imputation for missing values. This practice addresses a critical and subtle pitfall in model evaluation: information leakage. You will analyze different procedures to understand why preprocessing must be treated as part of the model-fitting process and included *within* each fold of the cross-validation loop to obtain an unbiased estimate of generalization error. [@problem_id:1912459]", "problem": "A data scientist at a biomedical research firm is tasked with developing a predictive model to assess the risk of a specific metabolic syndrome. The dataset available, denoted as $D$, contains records for $N$ patients, with each record comprising $P$ features (biomarkers, clinical measurements) and a binary outcome (high-risk or low-risk). A significant fraction of the biomarker measurements are missing.\n\nThe chosen predictive model is a Support Vector Machine (SVM). To handle the missing data, the team decides to use k-Nearest Neighbors (k-NN) imputation. In this method, a missing value for a feature in a given patient's record is estimated by finding the $k$ most similar patients (the \"neighbors\") in the dataset based on the features that are not missing, and then aggregating the corresponding feature values from these neighbors (e.g., by taking the mean or median).\n\nTo get a reliable estimate of the SVM's performance on unseen data, the data scientist must use K-fold cross-validation. The key challenge is to properly integrate the k-NN imputation step into the cross-validation procedure to avoid \"information leakage,\" which would lead to an overly optimistic performance estimate.\n\nYou are asked to identify the one methodologically sound procedure among the following options. A \"procedure\" is considered sound if it ensures that the information from the validation fold does not influence the training of the model or any preceding data preparation steps (like imputation) in any way.\n\nLet the original dataset with missing values be $D$.\n\nA. **Procedure A**\n1.  Apply k-NN imputation to the entire dataset $D$ to create a complete dataset, $D_{imputed}$.\n2.  Partition $D_{imputed}$ into $K$ folds.\n3.  For each fold $i=1, \\dots, K$:\n    a. Use fold $i$ as the test set and the remaining $K-1$ folds as the training set.\n    b. Train the SVM model on the training set.\n    c. Evaluate the SVM model on the test set.\n4.  Compute the average performance metric across all $K$ evaluations.\n\nB. **Procedure B**\n1.  Partition the original dataset $D$ (with missing values) into $K$ folds.\n2.  For each fold $i=1, \\dots, K$:\n    a. Designate fold $i$ as the test set ($D_{test}$) and the remaining $K-1$ folds as the training set ($D_{train}$). Note that both sets still contain missing values.\n    b. Build an imputation model by learning the k-NN structure (i.e., identifying neighbors) from the training set $D_{train}$ only.\n    c. Use the imputation model from step (2b) to fill in missing values in both $D_{train}$ and $D_{test}$. For each sample in $D_{test}$ with a missing value, its neighbors are found exclusively within $D_{train}$. This creates $D'_{train}$ and $D'_{test}$.\n    d. Train the SVM model on the imputed training set, $D'_{train}$.\n    e. Evaluate the SVM model on the imputed test set, $D'_{test}$.\n3.  Compute the average performance metric across all $K$ evaluations.\n\nC. **Procedure C**\n1.  Partition the original dataset $D$ (with missing values) into $K$ folds.\n2.  For each fold $i=1, \\dots, K$:\n    a. Designate fold $i$ as the test set ($D_{test}$) and the remaining $K-1$ folds as the training set ($D_{train}$).\n    b. Apply k-NN imputation to $D_{train}$ (using only neighbors within $D_{train}$) to create $D'_{train}$.\n    c. Apply k-NN imputation to $D_{test}$ (using only neighbors within $D_{test}$) to create $D'_{test}$.\n    d. Train the SVM model on $D'_{train}$.\n    e. Evaluate the SVM model on $D'_{test}$.\n3.  Compute the average performance metric across all $K$ evaluations.\n\nD. **Procedure D**\n1.  Partition the original dataset $D$ into a single training set (80% of data) and a single test set (20% of data).\n2.  Apply k-NN imputation to the training set to create an imputed training set.\n3.  Apply k-NN imputation to the test set to create an imputed test set.\n4.  Train the SVM model on the imputed training set.\n5.  Evaluate the model on the imputed test set and report the performance.\n\nWhich of the above procedures correctly implements cross-validation with imputation to provide an unbiased estimate of model performance?", "solution": "Let $D=\\{(x_n, y_n)\\}_{n=1}^N$ denote the dataset with missing entries in $x_n \\in \\mathbb{R}^P$ and binary labels $y_n \\in \\{0,1\\}$. A valid cross-validation pipeline must ensure that no function fitted using the validation data influences any step of training or preprocessing for that fold.\n\nDefine an imputation operator $\\mathcal{I}_\\eta$ parameterized by $\\eta$, where for k-NN imputation $\\eta$ consists of the training feature set used to compute neighbor relations (and any preprocessing such as scaling fit on the training set). The correct fold-wise procedure must:\n- fit $\\eta$ only on $D_\\text{train}$, i.e., $\\hat{\\eta}=g(D_\\text{train})$,\n- produce imputed sets $\\tilde{D}_\\text{train}=\\mathcal{I}_{\\hat{\\eta}}(D_\\text{train})$ and $\\tilde{D}_\\text{test}=\\mathcal{I}_{\\hat{\\eta}}(D_\\text{test})$,\n- train the model $h$ on $\\tilde{D}_\\text{train}$ and evaluate on $\\tilde{D}_\\text{test}$.\n\nFor k-NN, the application $\\mathcal{I}_{\\hat{\\eta}}$ to any $x$ with a missing coordinate $j$ must find neighbors exclusively from $D_\\text{train}$ and aggregate their values in coordinate $j$ to impute $x_j$.\n\nEvaluate each option against this requirement:\n\n- Procedure A fits imputation on the entire $D$ before splitting. This sets $\\hat{\\eta}=g(D)$, which depends on all folds, including validation data, violating the constraint that $\\eta$ must be learned only from $D_\\text{train}$. This is information leakage.\n\n- Procedure B first partitions $D$, then for each fold $i$ learns the k-NN structure from $D_\\text{train}^{(i)}$ only, i.e., $\\hat{\\eta}_i=g(D_\\text{train}^{(i)})$, imputes $D_\\text{train}^{(i)}$ using neighbors within $D_\\text{train}^{(i)}$, and imputes $D_\\text{test}^{(i)}$ by finding neighbors exclusively within $D_\\text{train}^{(i)}$. Thus $\\tilde{D}_\\text{train}^{(i)}=\\mathcal{I}_{\\hat{\\eta}_i}(D_\\text{train}^{(i)})$ and $\\tilde{D}_\\text{test}^{(i)}=\\mathcal{I}_{\\hat{\\eta}_i}(D_\\text{test}^{(i)})$. No information from $D_\\text{test}^{(i)}$ informs $\\hat{\\eta}_i$, so there is no leakage. This matches the deployment scenario where new samples are imputed using the training repository.\n\n- Procedure C imputes $D_\\text{train}$ using only $D_\\text{train}$, but imputes $D_\\text{test}$ using only $D_\\text{test}$. This defines a different parameter $\\hat{\\eta}_\\text{test}^{(i)}=g(D_\\text{test}^{(i)})$ for the test transformation, which uses statistics/structure from the validation data that would not be available at deployment and yields optimistic evaluation. It violates the requirement that all preprocessing be fit on $D_\\text{train}$.\n\n- Procedure D is not K-fold cross-validation and further performs separate imputations on training and test, i.e., uses $\\hat{\\eta}_\\text{train}=g(D_\\text{train})$ and $\\hat{\\eta}_\\text{test}=g(D_\\text{test})$, which improperly uses test information for preprocessing and does not provide K-fold estimates.\n\nTherefore, the only methodologically sound cross-validation with imputation is Procedure B.", "answer": "$$\\boxed{B}$$", "id": "1912459"}]}