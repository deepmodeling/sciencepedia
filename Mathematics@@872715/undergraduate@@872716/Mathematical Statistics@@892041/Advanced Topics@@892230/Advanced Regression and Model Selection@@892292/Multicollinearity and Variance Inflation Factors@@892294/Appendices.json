{"hands_on_practices": [{"introduction": "The Variance Inflation Factor (VIF) is our primary diagnostic tool for quantifying multicollinearity. It measures how much the variance of an estimated regression coefficient is increased due to its linear relationship with other predictors. This first exercise provides practice in applying the fundamental formula for VIF, linking it directly to the coefficient of determination, $R_j^2$, from an auxiliary regression. [@problem_id:1938245]", "problem": "In the context of multiple linear regression, multicollinearity is a phenomenon where one predictor variable in a model can be linearly predicted from the others with a substantial degree of accuracy. A key metric used to quantify the severity of multicollinearity is the Variance Inflation Factor (VIF).\n\nAn analyst is studying the factors that influence the price of a certain commodity. They build a multiple linear regression model with several predictor variables. To assess multicollinearity, they focus on a specific predictor, $X_j$. The analyst performs an auxiliary regression where $X_j$ is treated as the response variable and all other predictor variables in the original model are used as its predictors. The coefficient of determination, $R_j^2$, from this auxiliary regression is found to be $0.96$.\n\nBased on this information, calculate the Variance Inflation Factor for the regression coefficient $\\hat{\\beta}_j$ associated with the predictor $X_j$. Provide your answer as a single number.", "solution": "In multiple linear regression, the Variance Inflation Factor for coefficient $\\hat{\\beta}_{j}$ associated with predictor $X_{j}$ is defined by\n$$\n\\text{VIF}_{j}=\\frac{1}{1-R_{j}^{2}},\n$$\nwhere $R_{j}^{2}$ is the coefficient of determination from the auxiliary regression of $X_{j}$ on all the other predictors. This follows from the variance formula\n$$\n\\operatorname{Var}(\\hat{\\beta}_{j})=\\frac{\\sigma^{2}}{S_{X_{j}}^{2}\\left(1-R_{j}^{2}\\right)},\n$$\nso the inflation relative to the case with no collinearity among predictors is the factor $\\left(1-R_{j}^{2}\\right)^{-1}$.\n\nGiven $R_{j}^{2}=0.96$, compute the denominator:\n$$\n1-R_{j}^{2}=1-0.96=0.04.\n$$\nTherefore,\n$$\n\\text{VIF}_{j}=\\frac{1}{0.04}=25.\n$$", "answer": "$$\\boxed{25}$$", "id": "1938245"}, {"introduction": "To build intuition, it's helpful to move from the general definition of VIF to a more concrete scenario. This practice examines the simplest case of multicollinearity, which occurs between just two predictor variables. You will see how the VIF formula simplifies in a two-predictor model, connecting it to the squared Pearson correlation coefficient, $r^2$, and making the impact of collinearity more tangible. [@problem_id:1938207]", "problem": "An econometrician is developing a linear regression model to predict the annual profit of a company. Among the potential predictors, two variables are of particular concern: $X_1$, the company's annual marketing expenditure, and $X_2$, the size of its sales team. The econometrician suspects that these two variables might be highly correlated, which could inflate the variance of the estimated regression coefficients. To investigate this, they compute the sample correlation coefficient between the marketing expenditure and the sales team size from their dataset, finding it to be $r = 0.960$.\n\nTo quantify the severity of this multicollinearity, calculate the Variance Inflation Factor (VIF) for either of these predictors. The VIF is a measure of how much the variance of an estimated regression coefficient is increased because of collinearity. Express your answer as a numerical value rounded to three significant figures.", "solution": "The variance inflation factor for coefficient of predictor $X_{j}$ is defined as $\\mathrm{VIF}_{j}=\\frac{1}{1-R_{j}^{2}}$, where $R_{j}^{2}$ is the coefficient of determination from regressing $X_{j}$ on all other predictors. With only two predictors $X_{1}$ and $X_{2}$, the regression of one on the other has $R_{1}^{2}=R_{2}^{2}=r^{2}$, where $r$ is the sample correlation between $X_{1}$ and $X_{2}$. Given $r=0.960$, we have\n$$\nR_{j}^{2}=r^{2}=(0.960)^{2}=0.9216,\n$$\nso\n$$\n\\mathrm{VIF}=\\frac{1}{1-R_{j}^{2}}=\\frac{1}{1-0.9216}=\\frac{1}{0.0784}=\\frac{625}{49}\\approx 12.7551020408.\n$$\nRounding to three significant figures gives $12.8$.", "answer": "$$\\boxed{12.8}$$", "id": "1938207"}, {"introduction": "While VIF quantifies the magnitude of variance inflation, a deeper statistical understanding requires exploring *why* this inflation occurs. This advanced exercise delves into the theoretical underpinnings of multicollinearity by examining the covariance structure of the coefficient estimators. By deriving the variance of a linear combination of two estimators, you will uncover how the off-diagonal elements of the $(X'X)^{-1}$ matrix—which are non-zero in the presence of collinearity—directly contribute to the variance and covariance of our estimates. [@problem_id:1938196]", "problem": "Consider the standard multiple linear regression model, described by the matrix equation $Y = X\\beta + \\epsilon$. Here, $Y$ is an $n \\times 1$ vector of observations for a response variable, $X$ is a full-rank $n \\times p$ design matrix of predictor variables (including an intercept term), $\\beta$ is a $p \\times 1$ vector of unknown coefficients, and $\\epsilon$ is an $n \\times 1$ vector of unobservable random errors. The model adheres to the standard Gauss-Markov assumptions, namely that the errors have an expected value of zero, $E[\\epsilon] = 0$, and a constant variance with no correlation between observations, such that the variance-covariance matrix of the errors is $\\text{Var}(\\epsilon) = \\sigma^2 I_n$, where $\\sigma^2$ is a positive constant and $I_n$ is the $n \\times n$ identity matrix.\n\nThe Ordinary Least Squares (OLS) estimator for the coefficient vector $\\beta$ is given by $\\hat{\\beta} = (X'X)^{-1}X'Y$. Let $\\hat{\\beta}_j$ and $\\hat{\\beta}_k$ be the estimators for the $j$-th and $k$-th coefficients in the vector $\\hat{\\beta}$, where $1 \\le j  k \\le p$.\n\nFor two arbitrary non-zero real constants, $c_j$ and $c_k$, a researcher forms a linear combination of these estimators, $L = c_j \\hat{\\beta}_j + c_k \\hat{\\beta}_k$.\n\nDerive a general expression for the variance of this linear combination, $\\text{Var}(L)$. Express your answer in terms of $\\sigma^2$, the constants $c_j$ and $c_k$, and the elements of the matrix $(X'X)^{-1}$. For your notation, let $v_{il}$ denote the element in the $i$-th row and $l$-th column of the matrix $(X'X)^{-1}$.", "solution": "We start from the multiple linear regression model $Y = X\\beta + \\epsilon$ with $E[\\epsilon] = 0$ and $\\text{Var}(\\epsilon) = \\sigma^{2} I_{n}$. The OLS estimator is $\\hat{\\beta} = (X'X)^{-1}X'Y$. Using the linearity of variance for linear transformations and the fact that $Y = X\\beta + \\epsilon$ with $\\text{Var}(Y) = \\text{Var}(\\epsilon) = \\sigma^{2} I_{n}$, we compute the covariance matrix of $\\hat{\\beta}$:\n$$\n\\text{Var}(\\hat{\\beta}) = (X'X)^{-1}X' \\,\\text{Var}(Y)\\, X (X'X)^{-1} = (X'X)^{-1}X' (\\sigma^{2} I_{n}) X (X'X)^{-1} = \\sigma^{2} (X'X)^{-1}.\n$$\nLet $a$ be the $p \\times 1$ vector with $a_{j} = c_{j}$, $a_{k} = c_{k}$, and $a_{i} = 0$ for $i \\notin \\{j,k\\}$. Then the linear combination is $L = c_{j} \\hat{\\beta}_{j} + c_{k} \\hat{\\beta}_{k} = a' \\hat{\\beta}$. The variance of a linear form is\n$$\n\\text{Var}(L) = \\text{Var}(a' \\hat{\\beta}) = a' \\,\\text{Var}(\\hat{\\beta})\\, a = \\sigma^{2} a' (X'X)^{-1} a.\n$$\nWriting $(X'X)^{-1} = \\{v_{il}\\}_{i,l=1}^{p}$ and expanding the quadratic form yields\n$$\n\\text{Var}(L) = \\sigma^{2} \\sum_{i=1}^{p} \\sum_{l=1}^{p} a_{i} v_{il} a_{l} = \\sigma^{2} \\left( c_{j}^{2} v_{jj} + c_{j} c_{k} v_{jk} + c_{k} c_{j} v_{kj} + c_{k}^{2} v_{kk} \\right).\n$$\nSince $(X'X)^{-1}$ is symmetric, we have $v_{jk} = v_{kj}$, so the expression simplifies to\n$$\n\\text{Var}(L) = \\sigma^{2} \\left( c_{j}^{2} v_{jj} + 2 c_{j} c_{k} v_{jk} + c_{k}^{2} v_{kk} \\right).\n$$\nThis expresses the variance of the linear combination in terms of $\\sigma^{2}$, $c_{j}$, $c_{k}$, and the elements $v_{il}$ of $(X'X)^{-1}$.", "answer": "$$\\boxed{\\sigma^{2}\\left(c_{j}^{2} v_{jj} + 2 c_{j} c_{k} v_{jk} + c_{k}^{2} v_{kk}\\right)}$$", "id": "1938196"}]}