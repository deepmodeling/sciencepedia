{"hands_on_practices": [{"introduction": "To truly understand ridge regression, we will start from first principles. This exercise guides you through minimizing the penalized objective function for a simple, single-predictor model to derive the estimator $\\hat{\\beta}_{\\text{ridge}}$. This fundamental practice solidifies your understanding of how the regularization term $\\lambda$ influences the final coefficient estimate. [@problem_id:1951876]", "problem": "In a machine learning context, we are tasked with fitting a simple linear model without an intercept, $y = \\beta x$, to a set of $n$ data points $(x_i, y_i)$. To prevent overfitting on a small dataset, we employ ridge regression. The ridge estimate for the coefficient $\\beta$ is the value that minimizes the penalized sum of squared errors, also known as the objective function $L(\\beta)$:\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\nwhere $\\lambda > 0$ is the regularization parameter that controls the amount of shrinkage.\n\nYour task is two-fold. First, by minimizing the objective function $L(\\beta)$, derive the general closed-form expression for the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ in terms of the data points $(x_i, y_i)$ and the parameter $\\lambda$.\n\nSecond, apply this derived expression to a specific dataset consisting of two points: $(x_1, y_1) = (1, 3)$ and $(x_2, y_2) = (2, 5)$. Calculate the numerical value of the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ using a regularization parameter of $\\lambda = 1$.\n\nProvide the final numerical value as an exact fraction.", "solution": "We minimize the penalized sum of squared errors for the no-intercept linear model $y=\\beta x$ with objective\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\nExpand the squared term and collect like powers of $\\beta$:\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\nDifferentiate with respect to $\\beta$ and set the derivative to zero (first-order optimality condition):\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\nSolve for $\\beta$:\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\nso the solution is the unique minimizer.\n\nApply this to $(x_{1},y_{1})=(1,3)$, $(x_{2},y_{2})=(2,5)$ with $\\lambda=1$:\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\nTherefore,\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "Building on the single-variable case, we now move to the more general and practical matrix formulation of ridge regression. This problem requires you to apply the standard formula $\\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y$ using pre-computed matrices. This exercise will sharpen your matrix algebra skills and familiarize you with the typical calculations involved in real-world applications with multiple predictors. [@problem_id:1951893]", "problem": "In the field of machine learning, ridge regression is a common technique used to regularize linear regression models. This is particularly useful for preventing overfitting and handling multicollinearity among predictor variables. The ridge regression estimator for the coefficient vector, $\\hat{\\beta}_{\\lambda}$, is given by the formula:\n$$ \\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y $$\nHere, $X$ is the design matrix, $y$ is the vector of observed outcomes, $I$ is the identity matrix of appropriate dimensions, and $\\lambda$ is a non-negative regularization parameter.\n\nSuppose that for a particular dataset with two predictor variables, the following quantities have been pre-computed:\n$$ X^T X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} \\quad \\text{and} \\quad X^T y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\nUsing a regularization parameter of $\\lambda = 5$, determine the ridge regression coefficient vector $\\hat{\\beta}_5$.", "solution": "The ridge regression estimator is defined by\n$$\n\\hat{\\beta}_{\\lambda} = (X^{T}X + \\lambda I)^{-1} X^{T} y.\n$$\nWith the given data,\n$$\nX^{T}X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix}, \\quad X^{T} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\nCompute the regularized matrix:\n$$\nX^{T}X + \\lambda I = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} + 5 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 15 & 5 \\\\ 5 & 15 \\end{pmatrix}.\n$$\nFor a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is given by\n$$\n\\left(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\n$$\nApplying this,\n$$\n\\det(X^{T}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\nso\n$$\n(X^{T}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix}.\n$$\nThen\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$", "id": "1951893"}, {"introduction": "This practice explores a powerful conceptual interpretation that connects ridge regression back to a more familiar method, Ordinary Least Squares (OLS). You will demonstrate that the ridge solution is mathematically equivalent to performing OLS on an \"augmented\" dataset. Completing this derivation provides a deep intuition for how the penalty term functions, essentially by adding artificial data points that pull the coefficients toward zero. [@problem_id:1951855]", "problem": "In the context of linear regression, the Ordinary Least Squares (OLS) method is a standard approach to estimate the coefficients of a linear model. For a model described by $y = X\\beta + \\epsilon$, where $y$ is an $n \\times 1$ vector of observed responses, $X$ is an $n \\times p$ full-rank matrix of predictors (the design matrix), $\\beta$ is a $p \\times 1$ vector of unknown coefficients, and $\\epsilon$ is a vector of errors, the OLS estimator that minimizes the sum of squared residuals is given by the formula:\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^T X)^{-1} X^T y\n$$\n\nNow, consider a scenario where we construct an \"augmented\" dataset. Let $\\lambda$ be a positive scalar constant. We form a new, augmented design matrix $X_{\\text{aug}}$ and a new response vector $y_{\\text{aug}}$ as follows:\n$$\nX_{\\text{aug}} = \\begin{pmatrix} X \\\\ \\sqrt{\\lambda}I \\end{pmatrix} \\quad \\text{and} \\quad y_{\\text{aug}} = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}\n$$\nHere, $I$ is the $p \\times p$ identity matrix, and $0$ is a $p \\times 1$ column vector of zeros. The matrix $X_{\\text{aug}}$ has dimensions $(n+p) \\times p$, and the vector $y_{\\text{aug}}$ has dimensions $(n+p) \\times 1$.\n\nYour task is to apply the standard OLS formula to this augmented system to find the corresponding coefficient estimator, which we will call $\\hat{\\beta}_{\\text{aug}}$. Express your final answer for $\\hat{\\beta}_{\\text{aug}}$ as a single symbolic expression in terms of $X$, $y$, and $\\lambda$.", "solution": "We start from the standard OLS estimator applied to the augmented system. For any design matrix $Z$ and response vector $w$, the OLS estimator is given by\n$$\n\\hat{\\beta}=(Z^{T}Z)^{-1}Z^{T}w\n$$\nprovided that $Z^{T}Z$ is invertible.\n\nHere, we set $Z=X_{\\text{aug}}$ and $w=y_{\\text{aug}}$, where\n$$\nX_{\\text{aug}}=\\begin{pmatrix}X\\\\ \\sqrt{\\lambda}I\\end{pmatrix}, \\quad y_{\\text{aug}}=\\begin{pmatrix}y\\\\ 0\\end{pmatrix}.\n$$\nWe compute the two components needed:\n\n1) The Gram matrix of the augmented design:\nUsing the block multiplication identity $\\begin{pmatrix}A\\\\ B\\end{pmatrix}^{T}\\begin{pmatrix}A\\\\ B\\end{pmatrix}=A^{T}A+B^{T}B$, we obtain\n$$\nX_{\\text{aug}}^{T}X_{\\text{aug}}=X^{T}X+(\\sqrt{\\lambda}I)^{T}(\\sqrt{\\lambda}I)=X^{T}X+\\lambda I,\n$$\nsince $I^{T}=I$ and scalars commute with matrix multiplication.\n\n2) The cross term with the augmented response:\nUsing $\\begin{pmatrix}A\\\\ B\\end{pmatrix}^{T}\\begin{pmatrix}c\\\\ d\\end{pmatrix}=A^{T}c+B^{T}d$, we have\n$$\nX_{\\text{aug}}^{T}y_{\\text{aug}}=X^{T}y+(\\sqrt{\\lambda}I)^{T}0=X^{T}y.\n$$\n\nTherefore, the OLS estimator for the augmented system is\n$$\n\\hat{\\beta}_{\\text{aug}}=\\left(X_{\\text{aug}}^{T}X_{\\text{aug}}\\right)^{-1}X_{\\text{aug}}^{T}y_{\\text{aug}}=\\left(X^{T}X+\\lambda I\\right)^{-1}X^{T}y.\n$$\nThis expression is well-defined for $\\lambda>0$ because $X^{T}X+\\lambda I$ is invertible under the given assumptions.", "answer": "$$\\boxed{(X^{T}X+\\lambda I)^{-1}X^{T}y}$$", "id": "1951855"}]}