{"hands_on_practices": [{"introduction": "Many real-world systems can be modeled by simple binary outcomes, like a sensor detecting a particle or not. This exercise explores how the statistical properties of such a system, specifically its variance, behave when the outcomes are mapped to different signal levels. By working through this problem [@problem_id:1365287], you will solidify your understanding of variance and see how it applies to linear transformations of a fundamental discrete random variable.", "problem": "A simplified model for a digital sensor is used to detect the presence of a specific subatomic particle within a fixed time interval. The sensor has two output signal levels. If no particle is detected, it outputs a baseline signal level of $S_0$. If one or more particles are detected (treated as a single \"detection event\"), it outputs an elevated signal level of $S_1$. The probability of a detection event occurring in any given time interval is $p$. Let the random variable $V$ represent the signal level output by the detector in a randomly chosen time interval.\n\nDetermine the variance of the random variable $V$. Express your answer as a symbolic expression in terms of $p$, $S_0$, and $S_1$.", "solution": "Let $V$ be the random variable representing the signal level from the detector. The problem states that $V$ can take on two possible values: $S_0$ (no detection) and $S_1$ (detection). The probabilities associated with these values are given:\n$P(V = S_1) = p$\n$P(V = S_0) = 1-p$\n\nTo find the variance of $V$, we can use the formula $\\text{Var}(V) = E[V^2] - (E[V])^2$.\n\nFirst, we calculate the expected value (mean) of $V$, denoted as $E[V]$:\n$$E[V] = \\sum_{v} v \\cdot P(V=v)$$\n$$E[V] = S_1 \\cdot P(V=S_1) + S_0 \\cdot P(V=S_0)$$\n$$E[V] = S_1 p + S_0(1-p)$$\n\nNext, we calculate the expected value of $V^2$, denoted as $E[V^2]$:\n$$E[V^2] = \\sum_{v} v^2 \\cdot P(V=v)$$\n$$E[V^2] = S_1^2 \\cdot P(V=S_1) + S_0^2 \\cdot P(V=S_0)$$\n$$E[V^2] = S_1^2 p + S_0^2(1-p)$$\n\nNow, we substitute these expressions into the variance formula:\n$$\\text{Var}(V) = E[V^2] - (E[V])^2$$\n$$\\text{Var}(V) = (S_1^2 p + S_0^2(1-p)) - (S_1 p + S_0(1-p))^2$$\n\nLet's expand the squared term:\n$$(S_1 p + S_0(1-p))^2 = (S_1 p)^2 + 2(S_1 p)(S_0(1-p)) + (S_0(1-p))^2$$\n$$= S_1^2 p^2 + 2 S_0 S_1 p(1-p) + S_0^2 (1-p)^2$$\n$$= S_1^2 p^2 + 2 S_0 S_1 (p-p^2) + S_0^2 (1-2p+p^2)$$\n$$= S_1^2 p^2 + 2 S_0 S_1 p - 2 S_0 S_1 p^2 + S_0^2 - 2 S_0^2 p + S_0^2 p^2$$\n\nNow, substitute this expanded form back into the variance equation:\n$$\\text{Var}(V) = (S_1^2 p + S_0^2 - S_0^2 p) - (S_1^2 p^2 + 2 S_0 S_1 p - 2 S_0 S_1 p^2 - S_0^2 + 2 S_0^2 p - S_0^2 p^2)$$\n\nWe can cancel the $S_0^2$ terms. Now, let's group the remaining terms by powers of $p$:\n$$\\text{Var}(V) = (S_1^2 - S_0^2 - 2 S_0 S_1 + 2 S_0^2)p + (-S_1^2 + 2 S_0 S_1 - S_0^2)p^2$$\n$$\\text{Var}(V) = (S_1^2 - 2S_0 S_1 + S_0^2)p - (S_1^2 - 2S_0 S_1 + S_0^2)p^2$$\n\nRecognize the common factor $(S_1^2 - 2S_0 S_1 + S_0^2) = (S_1 - S_0)^2$:\n$$\\text{Var}(V) = (S_1 - S_0)^2 p - (S_1 - S_0)^2 p^2$$\n\nFactor out the common term $(S_1 - S_0)^2$:\n$$\\text{Var}(V) = (S_1 - S_0)^2 (p - p^2)$$\n$$\\text{Var}(V) = (S_1 - S_0)^2 p(1-p)$$\n\nAlternatively, we can model the detection event using a Bernoulli random variable. Let $X$ be a random variable such that $X=1$ for a detection event and $X=0$ for no detection. Then $X \\sim \\text{Bernoulli}(p)$. The signal level $V$ can be expressed as a linear transformation of $X$:\n$$V = S_0 (1-X) + S_1 X = S_0 + (S_1 - S_0)X$$\nUsing the property of variance, $\\text{Var}(aX+b) = a^2 \\text{Var}(X)$, with $a = S_1 - S_0$ and $b=S_0$:\n$$\\text{Var}(V) = \\text{Var}(S_0 + (S_1 - S_0)X) = (S_1 - S_0)^2 \\text{Var}(X)$$\nThe variance of a Bernoulli random variable $X$ with parameter $p$ is known to be $\\text{Var}(X) = p(1-p)$.\nTo show this: $E[X] = 1 \\cdot p + 0 \\cdot (1-p) = p$. $E[X^2] = 1^2 \\cdot p + 0^2 \\cdot (1-p) = p$. Thus, $\\text{Var}(X) = E[X^2] - (E[X])^2 = p - p^2 = p(1-p)$.\nSubstituting this into our equation for $\\text{Var}(V)$:\n$$\\text{Var}(V) = (S_1 - S_0)^2 p(1-p)$$\nBoth methods yield the same result.", "answer": "$$\\boxed{(S_1 - S_0)^{2} p(1-p)}$$", "id": "1365287"}, {"introduction": "Building on the concept of variance, we now explore how to describe the relationship between two random variables that are combined. This problem [@problem_id:1365306] asks you to calculate the covariance between linear combinations of two independent signals. It's a great opportunity to practice using the powerful properties of covariance, such as bilinearity, which can dramatically simplify calculations compared to working with full joint distributions.", "problem": "In a simplified model for a digital control system, two independent random integer signals, $X_1$ and $X_2$, are generated.\n\nThe first signal, $X_1$, can take values from the set $\\{1, 2, 3\\}$. The probabilities for these outcomes are given by $P(X_1=1) = \\frac{1}{2}$, $P(X_1=2) = \\frac{1}{3}$, and $P(X_1=3) = \\frac{1}{6}$.\n\nThe second signal, $X_2$, is generated by a process equivalent to rolling a standard fair six-sided die, so it can take any integer value from $1$ to $6$ with equal probability.\n\nThese two signals are then combined to form two new system variables, $U$ and $V$, defined as follows:\n$U = X_1 + X_2$\n$V = 2X_1 - X_2$\n\nAssuming $X_1$ and $X_2$ are statistically independent, calculate the covariance, $\\text{Cov}(U, V)$. Express your answer as an exact fraction in simplest form.", "solution": "We use bilinearity and symmetry of covariance. With $U=X_{1}+X_{2}$ and $V=2X_{1}-X_{2}$,\n$$\n\\text{Cov}(U,V)=\\text{Cov}(X_{1}+X_{2},\\,2X_{1}-X_{2})=\\text{Cov}(X_{1},2X_{1})+\\text{Cov}(X_{1},-X_{2})+\\text{Cov}(X_{2},2X_{1})+\\text{Cov}(X_{2},-X_{2}).\n$$\nUsing $\\text{Cov}(aX,bY)=ab\\,\\text{Cov}(X,Y)$, $\\text{Cov}(X,X)=\\text{Var}(X)$, and independence of $X_{1}$ and $X_{2}$ which gives $\\text{Cov}(X_{1},X_{2})=0$, we obtain\n$$\n\\text{Cov}(U,V)=2\\,\\text{Var}(X_{1})-\\text{Var}(X_{2}).\n$$\n\nCompute $\\text{Var}(X_{1})$. First,\n$$\nE[X_{1}]=1\\cdot \\frac{1}{2}+2\\cdot \\frac{1}{3}+3\\cdot \\frac{1}{6}=\\frac{1}{2}+\\frac{2}{3}+\\frac{1}{2}=\\frac{5}{3},\n$$\n$$\nE[X_{1}^{2}]=1^{2}\\cdot \\frac{1}{2}+2^{2}\\cdot \\frac{1}{3}+3^{2}\\cdot \\frac{1}{6}=\\frac{1}{2}+\\frac{4}{3}+\\frac{9}{6}=\\frac{10}{3}.\n$$\nThus,\n$$\n\\text{Var}(X_{1})=E[X_{1}^{2}]-(E[X_{1}])^{2}=\\frac{10}{3}-\\left(\\frac{5}{3}\\right)^{2}=\\frac{10}{3}-\\frac{25}{9}=\\frac{5}{9}.\n$$\n\nCompute $\\text{Var}(X_{2})$. Since $X_{2}$ is uniform on $\\{1,2,3,4,5,6\\}$,\n$$\nE[X_{2}]=\\frac{1+2+3+4+5+6}{6}=\\frac{7}{2}, \\quad E[X_{2}^{2}]=\\frac{1^{2}+2^{2}+3^{2}+4^{2}+5^{2}+6^{2}}{6}=\\frac{91}{6}.\n$$\nTherefore,\n$$\n\\text{Var}(X_{2})=E[X_{2}^{2}]-(E[X_{2}])^{2}=\\frac{91}{6}-\\left(\\frac{7}{2}\\right)^{2}=\\frac{91}{6}-\\frac{49}{4}=\\frac{35}{12}.\n$$\n\nSubstitute into the covariance expression:\n$$\n\\text{Cov}(U,V)=2\\cdot \\frac{5}{9}-\\frac{35}{12}=\\frac{10}{9}-\\frac{35}{12}=\\frac{40}{36}-\\frac{105}{36}=-\\frac{65}{36}.\n$$", "answer": "$$\\boxed{-\\frac{65}{36}}$$", "id": "1365306"}, {"introduction": "In many engineering and scientific applications, we know the mean and variance of a process but not its exact probability distribution. This practice introduces Chebyshev's inequality, a powerful tool for finding a \"worst-case\" probability bound for how far a random variable is likely to be from its mean. By solving this problem [@problem_id:1913503], you will learn a crucial skill for risk assessment and quality assurance when faced with incomplete information.", "problem": "An engineering team is evaluating a new machine learning model for real-time prediction. The inference time, denoted by the random variable $X$, is measured in milliseconds and is inherently discrete due to the system's clock cycle. The exact probability distribution of $X$ is unknown and complex, as it depends on unpredictable dynamic factors like system load and cache behavior.\n\nFrom a large set of performance benchmarks, the team has reliably determined that the mean inference time is $\\mu = 10$ ms and the variance is $\\sigma^2 = 9$ ms$^2$.\n\nTo guarantee a certain Quality of Service (QoS), the team needs to establish a rigorous upper bound on the probability of experiencing unusually slow or fast inference times. Without making any assumptions about the specific probability distribution of $X$, what is the maximum possible value for the probability that a single inference time is 16 ms or greater, or 4 ms or less?\n\nExpress your answer as a simplified fraction.", "solution": "We are asked for the maximum possible value of the probability of the two-sided tail event\n$$\n\\{X \\leq 4\\} \\cup \\{X \\geq 16\\}.\n$$\nWith the given mean $\\mu=10$ and variance $\\sigma^{2}=9$, rewrite this event in terms of deviation from the mean:\n$$\n\\{X \\leq 4\\} \\cup \\{X \\geq 16\\}=\\{|X-\\mu| \\geq 6\\}.\n$$\nBy Chebyshev's inequality, which holds for any random variable with finite mean and variance and requires no further distributional assumptions, for any $t0$,\n$$\nP(|X-\\mu|\\geq t) \\leq \\frac{\\sigma^{2}}{t^{2}}.\n$$\nApplying this with $t=6$ and $\\sigma^{2}=9$ gives\n$$\nP(|X-\\mu|\\geq 6) \\leq \\frac{9}{6^{2}}=\\frac{9}{36}=\\frac{1}{4}.\n$$\nThus,\n$$\nP(X \\leq 4 \\text{ or } X \\geq 16) \\leq \\frac{1}{4}.\n$$\nTo see that this upper bound is the maximum possible value (i.e., it is tight without further assumptions), consider the extremal three-point distribution that achieves equality in Chebyshev's bound: place probability $1-\\frac{1}{k^{2}}$ at $\\mu$ and probability $\\frac{1}{2k^{2}}$ at each of $\\mu \\pm k\\sigma$, where $k=\\frac{t}{\\sigma}$. Here $t=6$ and $\\sigma=3$, so $k=2$. The distribution\n$$\nP(X=10)=\\frac{3}{4},\\quad P(X=4)=\\frac{1}{8},\\quad P(X=16)=\\frac{1}{8}\n$$\nhas mean $10$, variance $9$, and satisfies\n$$\nP(X \\leq 4 \\text{ or } X \\geq 16)=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\nTherefore, the maximum possible value of the required probability, without making any distributional assumptions beyond the given mean and variance, is $\\frac{1}{4}$.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1913503"}]}