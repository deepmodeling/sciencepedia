{"hands_on_practices": [{"introduction": "Not all random events are equally likely, and a probability mass function (PMF) must accurately capture this. This first exercise explores how to construct a valid PMF when the probabilities of outcomes are not uniform but instead follow a specific rule of proportionality [@problem_id:1380322]. This practice is a fundamental application of the axiom that all probabilities in a distribution must sum to one, allowing you to find the precise PMF from a set of relative weights.", "problem": "A game designer is creating a custom four-sided die, shaped as a tetrahedron with faces numbered 1, 2, 3, and 4. Due to its irregular weighting, the die is not fair. When rolled, the probability of any given face landing down is directly proportional to the number on that face. Let the random variable $X$ represent the numerical outcome of a single roll of this die.\n\nConsider a new random variable $Y$ defined by the transformation $Y = \\frac{1}{X}$. Calculate the expected value of $Y$.", "solution": "The die has faces labeled $1,2,3,4$ and the probability of face $k$ landing down is proportional to $k$. Let $P(X=k)=c k$ for $k \\in \\{1,2,3,4\\}$, where $c$ is a normalization constant determined by $\\sum_{k=1}^{4} P(X=k)=1$.\n\nCompute $c$:\n$$\n\\sum_{k=1}^{4} P(X=k)=\\sum_{k=1}^{4} c k=c(1+2+3+4)=c \\cdot 10=1 \\implies c=\\frac{1}{10}.\n$$\nThus, for $k \\in \\{1,2,3,4\\}$,\n$$\nP(X=k)=\\frac{k}{10}.\n$$\n\nDefine $Y=\\frac{1}{X}$. The expectation of the transformed variable $Y$ is given by\n$$\n\\mathbb{E}[Y]=\\mathbb{E}\\left[\\frac{1}{X}\\right]=\\sum_{k=1}^{4} \\frac{1}{k} P(X=k).\n$$\nSubstitute the probabilities:\n$$\n\\mathbb{E}\\left[\\frac{1}{X}\\right]=\\sum_{k=1}^{4} \\frac{1}{k} \\cdot \\frac{k}{10}=\\frac{1}{10} \\sum_{k=1}^{4} 1=\\frac{4}{10}=\\frac{2}{5}.\n$$", "answer": "$$\\boxed{\\frac{2}{5}}$$", "id": "1380322"}, {"introduction": "In many scientific and engineering contexts, we are interested in the aggregate result of several independent processes. This problem models the total number of activated genes as the sum of two independent random events, each following a simple on/off pattern [@problem_id:1380307]. By working through this scenario, you will master the essential technique for deriving the PMF of a sum of discrete random variables, a method applicable to countless real-world systems.", "problem": "In a simplified model used in genomics, the activation of two distinct genes, Gene A and Gene B, is studied. The activation events are considered to be statistically independent.\n\nThe probability of Gene A being activated is $\\frac{1}{2}$. We can model this with a random variable $X$, where $X=1$ if Gene A is activated and $X=0$ otherwise.\n\nThe probability of Gene B being activated is $\\frac{1}{3}$. This is modeled by a second random variable $Y$, where $Y=1$ if Gene B is activated and $Y=0$ otherwise.\n\nA biologist is interested in the total number of activated genes from this pair. Let this total be represented by the random variable $Z = X+Y$.\n\nDetermine the probability mass function (PMF) of $Z$, which is the set of probabilities $p_Z(k) = P(Z=k)$ for all possible non-negative integer values $k$. Present your final answer as a row matrix containing the probabilities in the order $(p_Z(0), p_Z(1), p_Z(2))$. The entries in the matrix must be expressed as exact fractions.", "solution": "Let $X$ and $Y$ be independent Bernoulli random variables with $P(X=1)=\\frac{1}{2}$ and $P(Y=1)=\\frac{1}{3}$. Then $P(X=0)=1-\\frac{1}{2}=\\frac{1}{2}$ and $P(Y=0)=1-\\frac{1}{3}=\\frac{2}{3}$. Define $Z=X+Y$, so $Z$ takes values in $\\{0,1,2\\}$.\n\nBy independence, for any $a,b \\in \\{0,1\\}$, $P(X=a,Y=b)=P(X=a)P(Y=b)$.\n\nCompute each mass:\n- For $k=0$: $p_{Z}(0)=P(Z=0)=P(X=0,Y=0)=P(X=0)P(Y=0)=\\frac{1}{2}\\cdot\\frac{2}{3}=\\frac{1}{3}$.\n- For $k=2$: $p_{Z}(2)=P(Z=2)=P(X=1,Y=1)=P(X=1)P(Y=1)=\\frac{1}{2}\\cdot\\frac{1}{3}=\\frac{1}{6}$.\n- For $k=1$: $p_{Z}(1)=P(Z=1)=P(X=1,Y=0)+P(X=0,Y=1)=P(X=1)P(Y=0)+P(X=0)P(Y=1)=\\frac{1}{2}\\cdot\\frac{2}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}=\\frac{1}{3}+\\frac{1}{6}=\\frac{1}{2}$.\n\nAs a check, $\\frac{1}{3}+\\frac{1}{2}+\\frac{1}{6}=1$. Therefore, the PMF in the order $(p_{Z}(0),p_{Z}(1),p_{Z}(2))$ is $\\left(\\frac{1}{3},\\frac{1}{2},\\frac{1}{6}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3} & \\frac{1}{2} & \\frac{1}{6} \\end{pmatrix}}$$", "id": "1380307"}, {"introduction": "Theoretical models must often be adapted to account for the limitations of real-world measurement. This problem introduces the practical concept of \"right-censoring,\" where a measuring device has a maximum value it can record, causing all higher values to be grouped together [@problem_id:1947374]. This advanced exercise shows how to modify a standard distribution—in this case, the Poisson—to accurately model such a scenario, teaching you how to handle the \"piling up\" of probability at a boundary.", "problem": "In the field of high-energy physics, particle detectors are used to count events, such as the number of particles produced in a collision or the number of radioactive decays in a sample over a specific time interval. Consider a simplified model of such a detector.\n\nThe true number of events occurring in one observation period, denoted by the random variable $X$, follows a Poisson distribution with a mean of $\\lambda$. The Probability Mass Function (PMF) for a Poisson distribution is given by $P(X=k) = \\frac{\\exp(-\\lambda)\\lambda^k}{k!}$ for non-negative integers $k$.\n\nHowever, this particular detector has a hardware limitation: it cannot count beyond a certain threshold. The detector can accurately record any number of events from 0 up to $M-1$, where $M$ is a known positive integer representing the saturation point. If the true number of events $X$ is $M$ or greater, the detector's display simply reads \"M\".\n\nLet $Y$ be the random variable representing the number of events recorded by the detector. Thus, $Y$ is defined as $Y = X$ if $X < M$, and $Y = M$ if $X \\geq M$. This process is known as right-censoring.\n\nYour task is to derive the complete mathematical expression for the Probability Mass Function (PMF) of the observed count $Y$, denoted as $p_Y(k)$. Your final expression should be a function of $k$, $\\lambda$, and $M$, and it must be valid for all non-negative integer values of $k$.", "solution": "Let $X$ be Poisson with mean parameter $\\lambda>0$, so its PMF is $P(X=k)=\\frac{\\exp(-\\lambda)\\lambda^{k}}{k!}$ for integers $k\\geq 0$. The observed count is right-censored at $M\\in\\mathbb{Z}_{>0}$, with $Y=X$ if $X<M$ and $Y=M$ if $X\\geq M$. Hence $Y$ takes values in $\\{0,1,\\dots,M\\}$.\n\nTo derive $p_{Y}(k)=P(Y=k)$, consider cases based on the definition of $Y$:\n1) For integers $k$ with $0\\leq k<M$, the event $\\{Y=k\\}$ is exactly $\\{X=k\\}$, because no censoring occurs below $M$. Therefore,\n$$\np_{Y}(k)=P(Y=k)=P(X=k)=\\frac{\\exp(-\\lambda)\\lambda^{k}}{k!}, \\quad 0\\leq k<M.\n$$\n\n2) For $k=M$, the event $\\{Y=M\\}$ occurs precisely when $X\\geq M$, because any true count $X\\geq M$ is recorded as $M$. Thus,\n$$\np_{Y}(M)=P(Y=M)=P(X\\geq M)=\\sum_{x=M}^{\\infty}P(X=x)=1-\\sum_{x=0}^{M-1}P(X=x).\n$$\nSubstituting the Poisson PMF and changing the dummy index to $j$ yields\n$$\np_{Y}(M)=1-\\sum_{j=0}^{M-1}\\frac{\\exp(-\\lambda)\\lambda^{j}}{j!}\n=1-\\exp(-\\lambda)\\sum_{j=0}^{M-1}\\frac{\\lambda^{j}}{j!}.\n$$\n\n3) For integers $k>M$, the detector cannot report such a value, so $P(Y=k)=0$.\n\nThese cases together define $p_{Y}(k)$ for all non-negative integers $k$ (and, by the last case, for all integers). As a consistency check,\n$$\n\\sum_{k=0}^{M-1}p_{Y}(k)+p_{Y}(M)\n=\\sum_{k=0}^{M-1}P(X=k)+P(X\\geq M)\n=1,\n$$\nso the PMF is properly normalized.\n\nTherefore, the PMF of $Y$ is\n$$\np_{Y}(k)=\n\\begin{cases}\n\\frac{\\exp(-\\lambda)\\lambda^{k}}{k!}, & 0\\leq k<M, \\\\[6pt]\n1-\\exp(-\\lambda)\\displaystyle\\sum_{j=0}^{M-1}\\frac{\\lambda^{j}}{j!}, & k=M, \\\\[10pt]\n0, & k>M.\n\\end{cases}\n$$", "answer": "$$\\boxed{p_{Y}(k)=\\begin{cases}\\frac{\\exp(-\\lambda)\\lambda^{k}}{k!}, & 0\\leq k<M,\\\\[6pt]1-\\exp(-\\lambda)\\displaystyle\\sum_{j=0}^{M-1}\\frac{\\lambda^{j}}{j!}, & k=M,\\\\[10pt]0, & k>M.\\end{cases}}$$", "id": "1947374"}]}