{"hands_on_practices": [{"introduction": "To begin our hands-on practice, we will explore a classic two-stage experiment involving a game of chance. This problem provides a clear and direct application of the Law of Total Variance, allowing you to see how the overall variability of an outcome can be decomposed into the variance arising from the uncertainty within each stage and the variance between the stages. Mastering this type of problem [@problem_id:1401034] is fundamental to understanding how to partition variance in hierarchical models.", "problem": "A player participates in a two-stage game of chance. In the first stage, they roll a single fair six-sided die. Let the outcome of the roll be denoted by the random variable $K$. In the second stage, the player draws a prize with a monetary value, represented by the random variable $X$. The distribution from which the prize value is drawn depends on the outcome of the die roll. Specifically, for a given die roll outcome $K=k$, the prize value $X$ is drawn from a distribution with a mean of $k$ and a variance of $k^2$.\n\nCalculate the overall variance, $\\mathrm{Var}(X)$, of the prize value. Express your answer as an exact fraction in simplest form.", "solution": "Let $K$ be the outcome of a fair six-sided die, so $K \\in \\{1,2,3,4,5,6\\}$ with equal probability. Given $K=k$, the conditional mean and variance of $X$ are $E[X \\mid K=k]=k$ and $\\mathrm{Var}(X \\mid K=k)=k^{2}$.\n\nBy the law of total variance,\n$$\n\\mathrm{Var}(X)=E\\!\\left[\\mathrm{Var}(X \\mid K)\\right]+\\mathrm{Var}\\!\\left(E[X \\mid K]\\right).\n$$\nUsing the given conditional moments, this becomes\n$$\n\\mathrm{Var}(X)=E[K^{2}]+\\mathrm{Var}(K).\n$$\n\nCompute the moments of $K$ for a fair die:\n$$\nE[K]=\\frac{1}{6}\\sum_{k=1}^{6}k=\\frac{21}{6}=\\frac{7}{2},\n$$\n$$\nE[K^{2}]=\\frac{1}{6}\\sum_{k=1}^{6}k^{2}=\\frac{91}{6}.\n$$\nThus,\n$$\n\\mathrm{Var}(K)=E[K^{2}]-(E[K])^{2}=\\frac{91}{6}-\\left(\\frac{7}{2}\\right)^{2}=\\frac{91}{6}-\\frac{49}{4}=\\frac{35}{12}.\n$$\nTherefore,\n$$\n\\mathrm{Var}(X)=E[K^{2}]+\\mathrm{Var}(K)=\\frac{91}{6}+\\frac{35}{12}=\\frac{182+35}{12}=\\frac{217}{12}.\n$$\nThis fraction is in simplest form.", "answer": "$$\\boxed{\\frac{217}{12}}$$", "id": "1401034"}, {"introduction": "Next, we will tackle a more abstract problem that offers deep conceptual insight into the components of total variance. By conditioning on a randomly selected function, you will see a scenario where the variance between the conditional expectations is zero, meaning all the variability comes from the average conditional variance. This exercise [@problem_id:1929471] is designed to sharpen your understanding of what the two terms in Eve's Law, $E[\\mathrm{Var}(Y \\mid X)]$ and $\\mathrm{Var}(E[Y \\mid X])$, truly represent.", "problem": "Consider a two-stage random process.\nIn the first stage, a function, let's call it $F$, is selected. With a probability of $1/2$, the function is $F(x) = \\sin(x)$. With the remaining probability of $1/2$, the function is $F(x) = \\cos(x)$.\nIn the second stage, an angle $\\Theta$ is drawn as a random variable from a continuous uniform distribution over the interval $[0, 2\\pi]$.\nA new random variable, $Y$, is then defined as the result of applying the chosen function $F$ to the random angle $\\Theta$, such that $Y = F(\\Theta)$.\n\nCalculate the total variance of the random variable $Y$.", "solution": "Let $S$ denote the random choice of function, with $\\mathbb{P}(S=\\sin)=\\mathbb{P}(S=\\cos)=\\frac{1}{2}$. Let $\\Theta$ be independent of $S$ and uniformly distributed on $[0,2\\pi]$. Define $Y=\\sin(\\Theta)$ if $S=\\sin$ and $Y=\\cos(\\Theta)$ if $S=\\cos$.\n\nBy the law of total variance,\n$$\n\\operatorname{Var}(Y)=\\mathbb{E}\\!\\left[\\operatorname{Var}(Y\\mid S)\\right]+\\operatorname{Var}\\!\\left(\\mathbb{E}[Y\\mid S]\\right).\n$$\n\nFirst compute the conditional means. For $S=\\sin$,\n$$\n\\mathbb{E}[Y\\mid S=\\sin]=\\mathbb{E}[\\sin(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\sin(x)\\,dx=0,\n$$\nand for $S=\\cos$,\n$$\n\\mathbb{E}[Y\\mid S=\\cos]=\\mathbb{E}[\\cos(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\cos(x)\\,dx=0.\n$$\nHence $\\mathbb{E}[Y\\mid S]=0$ almost surely, so\n$$\n\\operatorname{Var}\\!\\left(\\mathbb{E}[Y\\mid S]\\right)=0.\n$$\n\nNext compute the conditional variances. For $S=\\sin$,\n$$\n\\operatorname{Var}(Y\\mid S=\\sin)=\\mathbb{E}[\\sin^{2}(\\Theta)]-\\left(\\mathbb{E}[\\sin(\\Theta)]\\right)^{2}=\\mathbb{E}[\\sin^{2}(\\Theta)],\n$$\nand\n$$\n\\mathbb{E}[\\sin^{2}(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\sin^{2}(x)\\,dx=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\frac{1-\\cos(2x)}{2}\\,dx=\\frac{1}{2\\pi}\\left[\\frac{x}{2}-\\frac{\\sin(2x)}{4}\\right]_{0}^{2\\pi}=\\frac{1}{2}.\n$$\nSimilarly, for $S=\\cos$,\n$$\n\\operatorname{Var}(Y\\mid S=\\cos)=\\mathbb{E}[\\cos^{2}(\\Theta)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\cos^{2}(x)\\,dx=\\frac{1}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}\\!\\left[\\operatorname{Var}(Y\\mid S)\\right]=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{2}.\n$$\n\nCombining, we obtain\n$$\n\\operatorname{Var}(Y)=\\frac{1}{2}+0=\\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1929471"}, {"introduction": "Finally, we apply the Law of Total Variance to a common and powerful scenario in stochastic processes: a random sum of random variables. This situation arises in fields from finance to physics, where the number of contributing events is itself a random variable. This problem [@problem_id:1401012] showcases the elegance and utility of the law in simplifying what would otherwise be a much more complex calculation, providing a key tool for your advanced problem-solving toolkit.", "problem": "A computational server is programmed to run an iterative simulation. The simulation proceeds in discrete time steps. At each step $i$, the simulation generates a real-valued numerical update $X_i$. These updates, $X_1, X_2, X_3, \\dots$, are independent and identically distributed (i.i.d.) random variables, each with a mean $E[X_i] = \\mu$ and a variance $\\text{Var}(X_i) = \\sigma^2$. The mean $\\mu$ is non-zero, and the variance $\\sigma^2$ is finite and positive.\n\nThe simulation incorporates a probabilistic stopping rule. After each step, an independent check is performed to decide whether to terminate. The probability of termination after any given step is a constant $p$, where $p \\in (0, 1)$. The simulation is guaranteed to run for at least one step. Let $T$ be the random variable representing the total number of steps for which the simulation runs.\n\nThe final output of the simulation is the cumulative sum of all updates generated, denoted by $S_T = \\sum_{i=1}^{T} X_i$.\n\nDetermine a closed-form expression for the variance of this final output, $\\text{Var}(S_T)$, in terms of the parameters $\\mu$, $\\sigma^2$, and $p$.", "solution": "Let $\\{X_{i}\\}_{i \\geq 1}$ be i.i.d. with $\\mathbb{E}[X_{i}] = \\mu$ and $\\operatorname{Var}(X_{i}) = \\sigma^{2}$. Let $T$ be independent of the $X_{i}$ and be geometric with support $\\{1,2,\\dots\\}$ and parameter $p \\in (0,1)$, so $\\mathbb{P}(T=t) = p(1-p)^{t-1}$.\n\nWe seek $\\operatorname{Var}(S_{T})$ for $S_{T} = \\sum_{i=1}^{T} X_{i}$. By the law of total variance,\n$$\n\\operatorname{Var}(S_{T}) = \\mathbb{E}\\!\\left[\\operatorname{Var}(S_{T}\\mid T)\\right] + \\operatorname{Var}\\!\\left(\\mathbb{E}[S_{T}\\mid T]\\right).\n$$\nGiven $T=t$, $S_{T}$ is a sum of $t$ i.i.d. variables, so\n$$\n\\mathbb{E}[S_{T}\\mid T=t] = t\\mu,\\qquad \\operatorname{Var}(S_{T}\\mid T=t) = t\\sigma^{2}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(S_{T}) = \\sigma^{2}\\mathbb{E}[T] + \\mu^{2}\\operatorname{Var}(T).\n$$\n\nIt remains to compute $\\mathbb{E}[T]$ and $\\operatorname{Var}(T)$ for $T \\sim \\operatorname{Geom}(p)$ on $\\{1,2,\\dots\\}$. Let $q = 1 - p$. Then\n$$\n\\mathbb{E}[T] = \\sum_{t=1}^{\\infty} t\\, p q^{t-1} = p \\sum_{t=1}^{\\infty} t q^{t-1}.\n$$\nUsing the geometric series $\\sum_{t=0}^{\\infty} q^{t} = \\frac{1}{1-q}$ for $|q|<1$ and differentiating with respect to $q$ gives\n$$\n\\sum_{t=1}^{\\infty} t q^{t-1} = \\frac{1}{(1-q)^{2}},\n$$\nso\n$$\n\\mathbb{E}[T] = p \\cdot \\frac{1}{(1-q)^{2}} = p \\cdot \\frac{1}{p^{2}} = \\frac{1}{p}.\n$$\nNext,\n$$\n\\mathbb{E}[T^{2}] = \\sum_{t=1}^{\\infty} t^{2} p q^{t-1} = p \\sum_{t=1}^{\\infty} t^{2} q^{t-1}.\n$$\nFrom $g(q) = \\sum_{t=1}^{\\infty} t q^{t} = \\frac{q}{(1-q)^{2}}$, differentiating yields\n$$\n\\sum_{t=1}^{\\infty} t^{2} q^{t-1} = g'(q) = \\frac{1+q}{(1-q)^{3}},\n$$\nhence\n$$\n\\mathbb{E}[T^{2}] = p \\cdot \\frac{1+q}{(1-q)^{3}} = p \\cdot \\frac{2 - p}{p^{3}} = \\frac{2 - p}{p^{2}}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(T) = \\mathbb{E}[T^{2}] - (\\mathbb{E}[T])^{2} = \\frac{2 - p}{p^{2}} - \\frac{1}{p^{2}} = \\frac{1 - p}{p^{2}}.\n$$\nSubstituting into $\\operatorname{Var}(S_{T})$ gives\n$$\n\\operatorname{Var}(S_{T}) = \\sigma^{2}\\left(\\frac{1}{p}\\right) + \\mu^{2}\\left(\\frac{1 - p}{p^{2}}\\right) = \\frac{\\sigma^{2}}{p} + \\frac{\\mu^{2}(1 - p)}{p^{2}}.\n$$\nThis is the required closed-form expression.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{p}+\\frac{\\mu^{2}(1-p)}{p^{2}}}$$", "id": "1401012"}]}