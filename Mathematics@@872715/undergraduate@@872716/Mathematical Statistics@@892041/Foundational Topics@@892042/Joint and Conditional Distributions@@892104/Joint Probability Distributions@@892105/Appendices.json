{"hands_on_practices": [{"introduction": "Understanding joint probability distributions often begins with the discrete case, where we analyze a finite number of outcomes. This first exercise provides a practical entry point, modeling a common scenario in cybersecurity: filtering emails based on keywords. By working with a joint probability mass function presented in a simple table, you will practice how to extract probabilities for combined events, a fundamental skill for working with multiple random variables [@problem_id:1635049].", "problem": "A cybersecurity analyst is developing a simple spam filter. The filter models the content of an incoming email by tracking the presence of two specific keywords, \"special\" and \"offer\". Let $X$ be a binary random variable such that $X=1$ if the email contains the word \"special\" and $X=0$ otherwise. Similarly, let $Y$ be a binary random variable such that $Y=1$ if the email contains the word \"offer\" and $Y=0$ otherwise.\n\nBased on a large dataset of emails, the analyst has determined the joint probability distribution for these two variables, $P(X=x, Y=y)$, which is given in the table below:\n\n| | $X=0$ | $X=1$ |\n| :--- | :---: | :---: |\n| **$Y=0$** | 0.82 | 0.09 |\n| **$Y=1$** | 0.05 | 0.04 |\n\nCalculate the probability that a randomly selected email contains exactly one of these two keywords. Express your answer as a decimal.", "solution": "We are asked for the probability that exactly one of the two keywords appears. Define the event of interest as the exclusive-or event\n$$\nE=\\{(X=1,Y=0)\\}\\cup\\{(X=0,Y=1)\\}.\n$$\nThe two subevents are mutually exclusive, so by additivity of probability for disjoint events,\n$$\nP(E)=P(X=1,Y=0)+P(X=0,Y=1).\n$$\nFrom the given joint distribution table,\n$$\nP(X=1,Y=0)=0.09,\\quad P(X=0,Y=1)=0.05.\n$$\nTherefore,\n$$\nP(E)=0.09+0.05=0.14.\n$$\nThus, the probability that a randomly selected email contains exactly one of the two keywords is $0.14$.", "answer": "$$\\boxed{0.14}$$", "id": "1635049"}, {"introduction": "Moving from discrete to continuous variables, we replace sums with integrals. This problem illustrates how to work with a joint probability density function (PDF) for two continuous random variables, a scenario frequently encountered in fields like engineering and chemistry. You will calculate the probability of an event by integrating the joint PDF over a specific region in the plane, reinforcing your understanding of how probability is distributed over a continuous space [@problem_id:1926675].", "problem": "A chemical engineer is studying the composition of a new solvent mixture. The mixture consists of two active components, labeled A and B, dissolved in a base solvent. Let $X$ and $Y$ be the random variables representing the mass fractions of component A and component B, respectively, in a randomly selected sample. Due to the synthesis process, the joint probability density function (PDF) for the pair $(X, Y)$ is given by\n$$f(x,y) = \\begin{cases} 24xy  \\text{if } x > 0, y > 0, \\text{ and } x+y  1 \\\\ 0  \\text{otherwise} \\end{cases}$$\nA sample of the mixture is considered to be of \"standard grade\" if the mass fraction of component A is less than $\\frac{1}{2}$, and the mass fraction of component B is also less than $\\frac{1}{2}$.\n\nCalculate the probability that a randomly selected sample is of standard grade. Express your final answer as an exact fraction.", "solution": "We are given the joint PDF\n$$\nf(x,y)=\\begin{cases}\n24xy  \\text{if } x0,\\ y0,\\ x+y1,\\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\nA sample is of standard grade if $X\\frac{1}{2}$ and $Y\\frac{1}{2}$. The probability of standard grade is the integral of the joint PDF over the event $\\{0x\\frac{1}{2},\\ 0y\\frac{1}{2}\\}$ intersected with the support $\\{x+y1\\}$. On the square $[0,\\frac{1}{2}]\\times[0,\\frac{1}{2}]$, we have $x+y\\leq 1$, and the set where $x+y=1$ is a boundary set of measure zero. Therefore,\n$$\n\\mathbb{P}\\left(X\\frac{1}{2},\\,Y\\frac{1}{2}\\right)\n=\\int_{0}^{1/2}\\int_{0}^{1/2} 24xy\\,dy\\,dx.\n$$\nCompute the inner integral:\n$$\n\\int_{0}^{1/2} 24xy\\,dy\n=24x\\int_{0}^{1/2} y\\,dy\n=24x\\left[\\frac{y^{2}}{2}\\right]_{0}^{1/2}\n=24x\\cdot\\frac{1}{8}\n=3x.\n$$\nNow integrate with respect to $x$:\n$$\n\\int_{0}^{1/2} 3x\\,dx\n=3\\left[\\frac{x^{2}}{2}\\right]_{0}^{1/2}\n=3\\cdot\\frac{1}{8}\n=\\frac{3}{8}.\n$$\nThus, the probability that a randomly selected sample is of standard grade is $\\frac{3}{8}$.", "answer": "$$\\boxed{\\frac{3}{8}}$$", "id": "1926675"}, {"introduction": "A crucial property of joint distributions is the concept of independence, which dramatically simplifies the analysis of complex systems. This exercise challenges you to investigate whether two random variables are independent by examining the structure of their joint probability mass function. You will learn to recognize when a joint PMF can be factorized into the product of its marginals and use this property to efficiently calculate properties of a combined variable, such as its variance [@problem_id:1926671].", "problem": "Let $X$ and $Y$ be two discrete random variables whose sample spaces are the set of non-negative integers, i.e., $\\{0, 1, 2, ...\\}$. Their joint probability mass function (PMF) is given by\n$$P(X=i, Y=j) = c \\frac{\\lambda^i \\mu^j}{i! j!}$$\nfor $i=0, 1, 2, ...$ and $j=0, 1, 2, ...$, where $\\lambda$ and $\\mu$ are positive real constants, and $c$ is a normalization constant. Determine the variance of the random variable $Z = X+Y$. Express your answer as a closed-form analytic expression in terms of $\\lambda$ and $\\mu$.", "solution": "We first determine the normalization constant $c$ by imposing that the joint PMF sums to $1$:\n$$\n\\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty} P(X=i,Y=j)\n= c \\sum_{i=0}^{\\infty}\\frac{\\lambda^{i}}{i!}\\sum_{j=0}^{\\infty}\\frac{\\mu^{j}}{j!}\n= c\\,\\exp(\\lambda)\\exp(\\mu)\n= c\\,\\exp(\\lambda+\\mu)\n= 1,\n$$\nwhich gives\n$$\nc=\\exp\\!\\big(-( \\lambda+\\mu )\\big).\n$$\nThus the joint PMF factors as\n$$\nP(X=i,Y=j)=\\exp(-\\lambda)\\frac{\\lambda^{i}}{i!}\\cdot \\exp(-\\mu)\\frac{\\mu^{j}}{j!},\n$$\nso $X$ and $Y$ are independent with $X\\sim \\text{Poisson}(\\lambda)$ and $Y\\sim \\text{Poisson}(\\mu)$.\n\nSince $X$ and $Y$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(Z) = \\operatorname{Var}(X+Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y).\n$$\nFor a Poisson-distributed random variable with parameter $\\theta$, its variance is equal to its mean, which is $\\theta$. Therefore,\n$$\n\\operatorname{Var}(X) = \\lambda \\quad \\text{and} \\quad \\operatorname{Var}(Y) = \\mu.\n$$\nSubstituting these into the equation for the variance of the sum, we get:\n$$\n\\operatorname{Var}(Z) = \\lambda + \\mu.\n$$\nAlternatively, we can use the property that the sum of independent Poisson random variables is also a Poisson random variable, with a parameter equal to the sum of the individual parameters. Thus, $Z = X+Y \\sim \\text{Poisson}(\\lambda+\\mu)$. The variance of a Poisson($\\lambda+\\mu$) distribution is $\\lambda+\\mu$.", "answer": "$$\\boxed{\\lambda+\\mu}$$", "id": "1926671"}]}