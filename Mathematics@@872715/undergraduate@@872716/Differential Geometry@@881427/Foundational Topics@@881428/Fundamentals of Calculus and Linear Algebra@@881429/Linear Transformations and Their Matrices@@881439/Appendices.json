{"hands_on_practices": [{"introduction": "A powerful way to understand a linear transformation is to identify which vectors it maps to the zero vector. This set of vectors, called the kernel, reveals the \"null space\" of the transformation and provides deep geometric insight. This first exercise asks you to determine the kernel of an orthogonal projection, helping you connect the abstract algebraic definition to a concrete geometric object. [@problem_id:1651517]", "problem": "Consider a linear transformation $L: \\mathbb{R}^3 \\to \\mathbb{R}^3$ that maps any vector in three-dimensional space to its orthogonal projection onto the $xy$-plane. The kernel of this transformation, denoted $\\ker(L)$, is the set of all input vectors that are mapped to the zero vector by $L$. Which of the following options provides a complete and accurate geometric description of $\\ker(L)$?\n\nA. The origin point $(0, 0, 0)$ only.\n\nB. The set of all vectors lying on the $x$-axis.\n\nC. The set of all vectors lying on the $z$-axis.\n\nD. The set of all vectors lying in the $xy$-plane.\n\nE. The set of all vectors lying in the $yz$-plane.", "solution": "Let a general vector in $\\mathbb{R}^3$ be represented by $\\mathbf{v} = (x, y, z)$. The linear transformation $L$ projects this vector orthogonally onto the $xy$-plane. This means the transformation sets the $z$-component of the vector to zero, while leaving the $x$ and $y$ components unchanged. Therefore, the action of the transformation $L$ on the vector $\\mathbf{v}$ can be expressed as:\n$$L(\\mathbf{v}) = L(x, y, z) = (x, y, 0)$$\n\nThe kernel of a linear transformation $L$, denoted $\\ker(L)$, is the set of all vectors $\\mathbf{v}$ in the domain such that $L(\\mathbf{v}) = \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector in the codomain. In this case, the domain and codomain are both $\\mathbb{R}^3$, so the zero vector is $\\mathbf{0} = (0, 0, 0)$.\n\nTo find the kernel of $L$, we must solve the equation $L(\\mathbf{v}) = \\mathbf{0}$:\n$$L(x, y, z) = (0, 0, 0)$$\nUsing the definition of our transformation $L$, this becomes:\n$$(x, y, 0) = (0, 0, 0)$$\n\nFor this vector equation to be true, each corresponding component must be equal. This gives us a system of equations:\n1.  $x = 0$\n2.  $y = 0$\n3.  $0 = 0$\n\nThe first two equations state that the $x$ and $y$ components of any vector in the kernel must be zero. The third equation, $0=0$, is always true and places no constraint on the $z$-component. Therefore, the variable $z$ can be any real number.\n\nAny vector $\\mathbf{v}$ in the kernel of $L$ must have the form $\\mathbf{v} = (0, 0, z)$ where $z \\in \\mathbb{R}$. This is the set of all vectors that point along the $z$-axis, including the zero vector (when $z=0$). Geometrically, this set of vectors constitutes the entire $z$-axis.\n\nNow we evaluate the given options:\nA. The origin point $(0, 0, 0)$ only. This is incorrect because it only accounts for the case where $z=0$. The kernel includes infinitely many other vectors, such as $(0, 0, 1)$ and $(0, 0, -5)$.\nB. The set of all vectors lying on the $x$-axis. This corresponds to vectors of the form $(x, 0, 0)$, which is incorrect.\nC. The set of all vectors lying on the $z$-axis. This corresponds to vectors of the form $(0, 0, z)$, which matches our result.\nD. The set of all vectors lying in the $xy$-plane. This corresponds to vectors of the form $(x, y, 0)$. These are the vectors that are *unchanged* by the projection (the image of L), not the vectors mapped to zero.\nE. The set of all vectors lying in the $yz$-plane. This corresponds to vectors of the form $(0, y, z)$, which is incorrect.\n\nThus, the correct geometric description of the kernel of $L$ is the entire $z$-axis.", "answer": "$$\\boxed{C}$$", "id": "1651517"}, {"introduction": "In fields like computer graphics and robotics, complex operations are often achieved by chaining together simpler linear transformations. The algebraic equivalent of this composition is matrix multiplication. This practice guides you through finding the single matrix that represents a sequence of two distinct transformations—a projection followed by a rotation—a fundamental skill for building sophisticated geometric manipulations. [@problem_id:1651567]", "problem": "In a simplified 2D computer graphics pipeline, a shape is processed through a series of transformations. Consider two such linear transformations acting on vectors in the plane $\\mathbb{R}^2$. The first transformation, $P: \\mathbb{R}^2 \\to \\mathbb{R}^2$, is an orthogonal projection that maps any vector onto the x-axis. The second transformation, $R: \\mathbb{R}^2 \\to \\mathbb{R}^2$, is a counterclockwise rotation about the origin by an angle of $\\frac{\\pi}{4}$ radians.\n\nA composite transformation, $L: \\mathbb{R}^2 \\to \\mathbb{R}^2$, is formed by first applying the projection $P$ to a vector, and then applying the rotation $R$ to the resulting vector.\n\nDetermine the $2 \\times 2$ standard matrix representation of the composite linear transformation $L$ with respect to the standard basis of $\\mathbb{R}^2$.", "solution": "The problem asks for the standard matrix representation of a composite linear transformation $L$, which is defined as $L = R \\circ P$. This means for any vector $\\mathbf{v} \\in \\mathbb{R}^2$, $L(\\mathbf{v}) = R(P(\\mathbf{v}))$. The matrix of a composition of linear transformations is the product of their individual matrices. If $[P]$ is the matrix for the projection and $[R]$ is the matrix for the rotation, the matrix for the composite transformation $L$, denoted as $[L]$, is given by the product $[L] = [R][P]$.\n\nOur first step is to find the standard matrix for the projection $P$. The transformation $P$ projects any vector $(x, y)$ onto the x-axis. The result of this projection is the vector $(x, 0)$. We can find the matrix $[P]$ by observing how it transforms the standard basis vectors of $\\mathbb{R}^2$, which are $\\mathbf{e}_1 = (1, 0)$ and $\\mathbf{e}_2 = (0, 1)$.\nThe image of $\\mathbf{e}_1$ under $P$ is $P(\\mathbf{e}_1) = P(1, 0) = (1, 0)$. This forms the first column of the matrix $[P]$.\nThe image of $\\mathbf{e}_2$ under $P$ is $P(\\mathbf{e}_2) = P(0, 1) = (0, 0)$. This forms the second column of the matrix $[P]$.\nTherefore, the matrix for the projection $P$ is:\n$$\n[P] = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\n$$\n\nOur second step is to find the standard matrix for the rotation $R$. The transformation $R$ is a counterclockwise rotation about the origin by an angle $\\theta$. The general form of the matrix for such a rotation is:\n$$\n[R_\\theta] = \\begin{pmatrix} \\cos\\theta  -\\sin\\theta \\\\ \\sin\\theta  \\cos\\theta \\end{pmatrix}\n$$\nIn our case, the angle is $\\theta = \\frac{\\pi}{4}$ radians. We evaluate the trigonometric functions for this angle:\n$$\n\\cos\\left(\\frac{\\pi}{4}\\right) = \\frac{\\sqrt{2}}{2}\n$$\n$$\n\\sin\\left(\\frac{\\pi}{4}\\right) = \\frac{\\sqrt{2}}{2}\n$$\nSubstituting these values into the rotation matrix formula gives us the matrix for $R$:\n$$\n[R] = \\begin{pmatrix} \\frac{\\sqrt{2}}{2}  -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2}  \\frac{\\sqrt{2}}{2} \\end{pmatrix}\n$$\n\nThe final step is to compute the matrix for the composite transformation $L$ by multiplying the matrices $[R]$ and $[P]$ in the correct order, which is $[L] = [R][P]$.\n$$\n[L] = [R][P] = \\begin{pmatrix} \\frac{\\sqrt{2}}{2}  -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2}  \\frac{\\sqrt{2}}{2} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\n$$\nWe perform the matrix multiplication:\n$$\n[L] = \\begin{pmatrix} \\left(\\frac{\\sqrt{2}}{2}\\right)(1) + \\left(-\\frac{\\sqrt{2}}{2}\\right)(0)  \\left(\\frac{\\sqrt{2}}{2}\\right)(0) + \\left(-\\frac{\\sqrt{2}}{2}\\right)(0) \\\\ \\left(\\frac{\\sqrt{2}}{2}\\right)(1) + \\left(\\frac{\\sqrt{2}}{2}\\right)(0)  \\left(\\frac{\\sqrt{2}}{2}\\right)(0) + \\left(\\frac{\\sqrt{2}}{2}\\right)(0) \\end{pmatrix}\n$$\n$$\n[L] = \\begin{pmatrix} \\frac{\\sqrt{2}}{2}  0 \\\\ \\frac{\\sqrt{2}}{2}  0 \\end{pmatrix}\n$$\nThis is the standard matrix representation of the composite transformation $L$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{2}  0 \\\\ \\frac{\\sqrt{2}}{2}  0 \\end{pmatrix}}$$", "id": "1651567"}, {"introduction": "Orthogonal projection is a cornerstone of linear algebra, with applications ranging from data compression to approximation theory. While projecting onto a space spanned by an orthonormal basis is straightforward, we often start with a basis that is not orthogonal. This problem challenges you to construct the projection matrix onto a 2-dimensional subspace of $\\mathbb{R}^4$, a task which first requires you to construct an orthonormal basis using the Gram-Schmidt process. [@problem_id:1651562]", "problem": "Consider the 4-dimensional Euclidean space $\\mathbb{R}^4$ equipped with the standard dot product. A 2-dimensional plane (a subspace) $W$ is spanned by the two linearly independent vectors $v_1 = (1, 1, 0, 2)$ and $v_2 = (0, 1, 1, 1)$. Let $T: \\mathbb{R}^4 \\to \\mathbb{R}^4$ be the linear transformation that performs an orthogonal projection of any vector $x \\in \\mathbb{R}^4$ onto the subspace $W$.\n\nDetermine the $4 \\times 4$ matrix $[T]$ that represents this transformation with respect to the standard basis of $\\mathbb{R}^4$.", "solution": "The matrix $[T]$ that represents the orthogonal projection onto a subspace $W$ is most easily constructed using an orthonormal basis for $W$. The given vectors $v_1 = (1, 1, 0, 2)$ and $v_2 = (0, 1, 1, 1)$ span the subspace $W$, but they are not orthogonal. We can verify this by computing their dot product:\n$$v_1 \\cdot v_2 = (1)(0) + (1)(1) + (0)(1) + (2)(1) = 0 + 1 + 0 + 2 = 3 \\neq 0$$\nSince the vectors are not orthogonal, we must first use the Gram-Schmidt process to find an orthonormal basis $\\{w_1, w_2\\}$ for the subspace $W$.\n\nStep 1: Find the first orthonormal vector $w_1$.\nLet $u_1 = v_1 = (1, 1, 0, 2)$.\nThe norm of $u_1$ is $\\|u_1\\| = \\sqrt{1^2 + 1^2 + 0^2 + 2^2} = \\sqrt{1 + 1 + 0 + 4} = \\sqrt{6}$.\nThe first orthonormal vector is $w_1 = \\frac{u_1}{\\|u_1\\|} = \\frac{1}{\\sqrt{6}}(1, 1, 0, 2)$.\n\nStep 2: Find the second orthonormal vector $w_2$.\nFirst, we find a vector $u_2$ that is orthogonal to $u_1$ by subtracting the projection of $v_2$ onto $u_1$ from $v_2$:\n$$u_2 = v_2 - \\text{proj}_{u_1}(v_2) = v_2 - \\frac{v_2 \\cdot u_1}{\\|u_1\\|^2} u_1$$\nWe already calculated $v_2 \\cdot u_1 = v_2 \\cdot v_1 = 3$, and we know $\\|u_1\\|^2 = 6$.\n$$u_2 = (0, 1, 1, 1) - \\frac{3}{6}(1, 1, 0, 2) = (0, 1, 1, 1) - \\left(\\frac{1}{2}, \\frac{1}{2}, 0, 1\\right)$$\n$$u_2 = \\left(0 - \\frac{1}{2}, 1 - \\frac{1}{2}, 1 - 0, 1 - 1\\right) = \\left(-\\frac{1}{2}, \\frac{1}{2}, 1, 0\\right)$$\nNow, we normalize $u_2$ to get $w_2$. The norm of $u_2$ is:\n$$\\|u_2\\| = \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + 1^2 + 0^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4} + 1} = \\sqrt{\\frac{1}{2} + 1} = \\sqrt{\\frac{3}{2}} = \\frac{\\sqrt{6}}{2}$$\nThe second orthonormal vector is $w_2 = \\frac{u_2}{\\|u_2\\|} = \\frac{1}{\\sqrt{6}/2}\\left(-\\frac{1}{2}, \\frac{1}{2}, 1, 0\\right) = \\frac{2}{\\sqrt{6}}\\left(-\\frac{1}{2}, \\frac{1}{2}, 1, 0\\right) = \\frac{1}{\\sqrt{6}}(-1, 1, 2, 0)$.\nSo, an orthonormal basis for $W$ is $\\{w_1, w_2\\} = \\left\\{ \\frac{1}{\\sqrt{6}}(1, 1, 0, 2), \\frac{1}{\\sqrt{6}}(-1, 1, 2, 0) \\right\\}$.\n\nStep 3: Construct the projection matrix $[T]$.\nThe projection matrix for a projection onto the subspace spanned by an orthonormal set of vectors $\\{w_1, \\dots, w_k\\}$ is given by $[T] = \\sum_{i=1}^k w_i w_i^T$. In our case, $k=2$, so $[T] = w_1 w_1^T + w_2 w_2^T$. This is equivalent to forming a matrix $Q$ whose columns are the orthonormal basis vectors and computing $[T] = QQ^T$.\nLet $Q$ be the matrix with columns $w_1$ and $w_2$:\n$$Q = \\frac{1}{\\sqrt{6}}\\begin{pmatrix} 1  -1 \\\\ 1  1 \\\\ 0  2 \\\\ 2  0 \\end{pmatrix}$$\nThen its transpose is:\n$$Q^T = \\frac{1}{\\sqrt{6}}\\begin{pmatrix} 1  1  0  2 \\\\ -1  1  2  0 \\end{pmatrix}$$\nThe projection matrix is $[T] = QQ^T$:\n$$[T] = \\left(\\frac{1}{\\sqrt{6}}\\right)^2 \\begin{pmatrix} 1  -1 \\\\ 1  1 \\\\ 0  2 \\\\ 2  0 \\end{pmatrix} \\begin{pmatrix} 1  1  0  2 \\\\ -1  1  2  0 \\end{pmatrix}$$\n$$[T] = \\frac{1}{6} \\begin{pmatrix} (1)(1)+(-1)(-1)  (1)(1)+(-1)(1)  (1)(0)+(-1)(2)  (1)(2)+(-1)(0) \\\\ (1)(1)+(1)(-1)  (1)(1)+(1)(1)  (1)(0)+(1)(2)  (1)(2)+(1)(0) \\\\ (0)(1)+(2)(-1)  (0)(1)+(2)(1)  (0)(0)+(2)(2)  (0)(2)+(2)(0) \\\\ (2)(1)+(0)(-1)  (2)(1)+(0)(1)  (2)(0)+(0)(2)  (2)(2)+(0)(0) \\end{pmatrix}$$\n$$[T] = \\frac{1}{6} \\begin{pmatrix} 2  0  -2  2 \\\\ 0  2  2  2 \\\\ -2  2  4  0 \\\\ 2  2  0  4 \\end{pmatrix}$$\nFinally, we simplify by factoring out a 2 from the matrix entries:\n$$[T] = \\frac{2}{6} \\begin{pmatrix} 1  0  -1  1 \\\\ 0  1  1  1 \\\\ -1  1  2  0 \\\\ 1  1  0  2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 1  0  -1  1 \\\\ 0  1  1  1 \\\\ -1  1  2  0 \\\\ 1  1  0  2 \\end{pmatrix}$$\nExpressing this as a single matrix with fractional entries:\n$$[T] = \\begin{pmatrix} 1/3  0  -1/3  1/3 \\\\ 0  1/3  1/3  1/3 \\\\ -1/3  1/3  2/3  0 \\\\ 1/3  1/3  0  2/3 \\end{pmatrix}$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3}  0  -\\frac{1}{3}  \\frac{1}{3} \\\\ 0  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{1}{3}  \\frac{2}{3}  0 \\\\ \\frac{1}{3}  \\frac{1}{3}  0  \\frac{2}{3} \\end{pmatrix}}$$", "id": "1651562"}]}