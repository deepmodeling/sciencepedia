{"hands_on_practices": [{"introduction": "The Weierstrass theorem is an existence theorem, but one of its most elegant proofs, due to Sergei Bernstein, is constructive. It provides an explicit formula for a sequence of polynomials that converges uniformly to any given continuous function. This exercise will guide you through the fundamental mechanics of this construction by having you calculate a low-degree Bernstein polynomial for the smooth function $f(x) = \\exp(x)$. [@problem_id:2330472]", "problem": "In mathematical analysis, the Weierstrass approximation theorem guarantees that any continuous function defined on a closed, bounded interval can be uniformly approximated by a polynomial function. A constructive proof of this theorem can be achieved using Bernstein polynomials.\n\nThe $n$-th degree Bernstein polynomial for a function $f(x)$ continuous on the interval $[0, 1]$ is defined as:\n$$B_n(f; x) = \\sum_{k=0}^{n} f\\left(\\frac{k}{n}\\right) \\binom{n}{k} x^k (1-x)^{n-k}$$\nwhere $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ represents the binomial coefficient.\n\nFor the function $f(x) = \\exp(x)$ on the interval $[0, 1]$, determine its second-degree Bernstein polynomial, $B_2(f; x)$. Your final answer should be a polynomial in the variable $x$.", "solution": "We use the definition of the $n$-th degree Bernstein polynomial on $[0,1]$:\n$$B_{n}(f;x)=\\sum_{k=0}^{n} f\\left(\\frac{k}{n}\\right)\\binom{n}{k}x^{k}(1-x)^{n-k}.$$\nFor $n=2$ and $f(x)=\\exp(x)$, this gives\n$$B_{2}(f;x)=f(0)\\binom{2}{0}(1-x)^{2}+f\\left(\\frac{1}{2}\\right)\\binom{2}{1}x(1-x)+f(1)\\binom{2}{2}x^{2}.$$\nUsing $f(0)=\\exp(0)=1$, $f\\left(\\frac{1}{2}\\right)=\\exp\\left(\\frac{1}{2}\\right)$, and $f(1)=\\exp(1)$, we obtain\n$$B_{2}(f;x)=(1-x)^{2}+2\\exp\\left(\\frac{1}{2}\\right)x(1-x)+\\exp(1)x^{2}.$$\nExpanding,\n$$(1-x)^{2}=1-2x+x^{2},$$\n$$2\\exp\\left(\\frac{1}{2}\\right)x(1-x)=2\\exp\\left(\\frac{1}{2}\\right)x-2\\exp\\left(\\frac{1}{2}\\right)x^{2},$$\nso\n$$B_{2}(f;x)=1+\\left(2\\exp\\left(\\frac{1}{2}\\right)-2\\right)x+\\left(\\exp(1)-2\\exp\\left(\\frac{1}{2}\\right)+1\\right)x^{2}.$$\nThis is a polynomial in $x$ with coefficients expressed in terms of $\\exp\\left(\\frac{1}{2}\\right)$ and $\\exp(1)$.", "answer": "$$\\boxed{1+\\left(2\\exp\\left(\\frac{1}{2}\\right)-2\\right)x+\\left(\\exp(1)-2\\exp\\left(\\frac{1}{2}\\right)+1\\right)x^{2}}$$", "id": "2330472"}, {"introduction": "A deep understanding of a theorem often comes from exploring its boundaries. The Weierstrass Approximation Theorem guarantees approximation by polynomials with *real* coefficients, but what if we impose stricter constraints? This problem challenges you to investigate whether approximation is still possible when we restrict the polynomial coefficients to be integers, revealing a crucial subtlety of the theorem. [@problem_id:1904669]", "problem": "Let $C[0,2]$ be the space of all real-valued continuous functions on the interval $[0,2]$, equipped with the uniform norm, defined as $\\|f\\|_\\infty = \\max_{x \\in [0,2]} |f(x)|$. Let $\\mathcal{P}_{\\mathbb{Z}}$ denote the set of all polynomials with only integer coefficients (e.g., $P(x) = a_n x^n + \\dots + a_1 x + a_0$ where all $a_i$ are integers).\n\nWhile the Weierstrass Approximation Theorem guarantees that any continuous function on a closed interval can be uniformly approximated by polynomials with *real* coefficients, the situation changes when coefficients are restricted to be integers.\n\nConsider the linear function $f(x) = \\frac{x}{2}$ defined on the interval $[0,2]$. Find the smallest possible value for the maximum approximation error. That is, calculate the value of $\\delta = \\inf_{P \\in \\mathcal{P}_{\\mathbb{Z}}} \\|f(x) - P(x)\\|_\\infty$. Express your final answer as a single real number.", "solution": "Let $f(x)=\\frac{x}{2}$ on $[0,2]$ and define \n$$\n\\delta=\\inf_{P\\in\\mathcal{P}_{\\mathbb{Z}}}\\|f-P\\|_{\\infty}.\n$$\nFor any $P(x)=\\sum_{k=0}^{n}a_{k}x^{k}$ with all $a_{k}\\in\\mathbb{Z}$, we have\n$$\nP(0)=a_{0}\\in\\mathbb{Z},\\qquad P(2)=\\sum_{k=0}^{n}a_{k}2^{k}\\in\\mathbb{Z}.\n$$\nMoreover, reducing modulo $2$ gives\n$$\nP(2)\\equiv a_{0}\\equiv P(0)\\pmod{2},\n$$\nso $P(0)$ and $P(2)$ have the same parity.\n\nAssume, for contradiction, that there exists $P\\in\\mathcal{P}_{\\mathbb{Z}}$ with\n$$\n\\|f-P\\|_{\\infty}1.\n$$\nThen, evaluating at the endpoints,\n$$\n|P(0)-f(0)|=|P(0)|1\\quad\\Rightarrow\\quad P(0)=0,\n$$\nand\n$$\n|P(2)-f(2)|=|P(2)-1|1\\quad\\Rightarrow\\quad P(2)=1.\n$$\nBut this contradicts the parity condition $P(2)\\equiv P(0)\\pmod{2}$, since $1\\not\\equiv 0\\pmod{2}$. Therefore no such $P$ exists, and we conclude\n$$\n\\delta\\geq 1.\n$$\n\nTo show this bound is sharp, take the constant polynomial $P(x)\\equiv 0\\in\\mathcal{P}_{\\mathbb{Z}}$. Then\n$$\n\\|f-P\\|_{\\infty}=\\max_{x\\in[0,2]}\\left|\\frac{x}{2}-0\\right|=\\max_{x\\in[0,2]}\\frac{x}{2}=1,\n$$\nso\n$$\n\\delta\\leq 1.\n$$\nCombining both inequalities yields\n$$\n\\delta=1.\n$$", "answer": "$$\\boxed{1}$$", "id": "1904669"}, {"introduction": "The Weierstrass theorem guarantees that for any continuous function, a polynomial approximation exists that is 'good enough'. However, in practical applications, we often want the *best* possible approximation for a given polynomial degree. This advanced exercise moves from the question of existence to one of optimization, tasking you with finding the best uniform approximation to $f(x)=|x|$ from a specific family of quadratic polynomials. [@problem_id:597435]", "problem": "The Weierstrass Approximation Theorem guarantees that any continuous function on a closed and bounded interval can be uniformly approximated by polynomials. A more refined question is to find the *best* such approximation of a given degree or form. This problem concerns finding such a polynomial that best approximates the absolute value function in the uniform norm.\n\nThe uniform norm (or Chebyshev norm) of a continuous function $g$ on an interval $[c, d]$ is defined as $\\|g\\|_{\\infty} = \\sup_{x \\in [c, d]} |g(x)|$. A polynomial $p(x)$ is called the best uniform approximation to a function $f(x)$ from a set of polynomials $\\mathcal{P}$ if it minimizes the error $\\|f-p\\|_{\\infty}$ for all $p \\in \\mathcal{P}$.\n\nConsider the function $f(x) = |x|$ defined on the interval $[-1, 1]$. Let the set of approximating functions be the space of even quadratic polynomials $\\mathcal{P} = \\{p(x) = ax^2 + b \\mid a, b \\in \\mathbb{R}\\}$.\n\nDetermine the specific polynomial $p(x) \\in \\mathcal{P}$ that is the best uniform approximation to $f(x)$ on $[-1, 1]$.", "solution": "Let the function to be approximated be $f(x) = |x|$ on $[-1, 1]$. The approximating functions are of the form $p(x) = ax^2 + b$. We want to find the parameters $a$ and $b$ that minimize the maximum error, defined by the uniform norm of the error function $E(x) = f(x) - p(x)$.\n$$L = \\|f - p\\|_{\\infty} = \\sup_{x \\in [-1, 1]} ||x| - (ax^2 + b)|$$\n\nThe key to solving this problem is to use the properties of the functions involved and apply the Chebyshev Equioscillation Theorem.\n\n**Step 1: Exploit Symmetry**\nThe function $f(x)=|x|$ is an even function, i.e., $f(-x) = f(x)$. The approximating polynomial $p(x)=ax^2+b$ is also an even function. Therefore, the error function $E(x) = |x| - (ax^2+b)$ is also even. This symmetry implies that the maximum absolute error over the interval $[-1, 1]$ will be the same as the maximum absolute error over the interval $[0, 1]$.\n$$\\sup_{x \\in [-1, 1]} |E(x)| = \\sup_{x \\in [0, 1]} |E(x)|$$\nFor $x \\in [0, 1]$, $|x|=x$. The error function simplifies to:\n$$E(x) = x - (ax^2 + b) = -ax^2 + x - b$$\nSo, the problem reduces to finding the best uniform approximation of $g(x)=x$ on $[0,1]$ by a function from the set $\\{q(x)=ax^2+b\\}$.\n\n**Step 2: Apply the Chebyshev Equioscillation Theorem**\nThe set of approximating functions on $[0, 1]$ is $V = \\mathrm{span}\\{1, x^2\\}$. This is a 2-dimensional subspace of $C[0,1]$. Let's check if $V$ satisfies the Haar condition on $[0,1]$. A subspace of dimension $k$ satisfies the Haar condition if any non-zero function in it has at most $k-1$ roots. Here $k=2$. A non-zero function $v(x) = ax^2+b$ can have at most one root in $[0,1]$ (since $x^2 = -b/a$ has at most one non-negative solution for $x$). Thus, the Haar condition is satisfied.\n\nThe Chebyshev Equioscillation Theorem states that a function $p(x) \\in V$ is the best uniform approximation to a continuous function $f(x)$ on $[c,d]$ if and only if the error function $E(x) = f(x) - p(x)$ exhibits at least $k+1$ points of alternating sign where the maximum error is achieved.\nIn our case, $k=2$, so we need at least $2+1=3$ such points in $[0,1]$. Let these points be $0 \\le x_1  x_2  x_3 \\le 1$. Let $L = \\sup_{x \\in [0,1]} |E(x)|$. We must have:\n$$E(x_i) = \\sigma(-1)^i L, \\quad i=1, 2, 3$$\nfor some $\\sigma \\in \\{+1, -1\\}$.\n\nThe error function on $[0,1]$ is $E(x) = -ax^2+x-b$, a parabola. Its extrema on a bounded interval occur either at the endpoints of the interval or at a critical point where $E'(x)=0$.\n$$E'(x) = -2ax + 1$$\nSetting $E'(x)=0$ gives the vertex of the parabola at $x = \\frac{1}{2a}$. For this critical point to be an interior extremum, we must have $0  \\frac{1}{2a}  1$, which implies $a  \\frac{1}{2}$.\nThe three points of equioscillation must therefore be the endpoints and the vertex: $x_1 = 0$, $x_2 = \\frac{1}{2a}$, and $x_3=1$.\n\n**Step 3: Solve for the parameters $a$ and $b$**\nLet's set the error values at these points to be alternating, starting with a negative value (i.e., $\\sigma = -1$).\n1. $E(x_1) = E(0) = -b = -L$\n2. $E(x_2) = E(\\frac{1}{2a}) = -a(\\frac{1}{2a})^2 + \\frac{1}{2a} - b = -\\frac{1}{4a} + \\frac{1}{2a} - b = \\frac{1}{4a} - b = L$\n3. $E(x_3) = E(1) = -a+1-b = -L$\n\nWe now have a system of three equations for $a, b, L$:\nFrom (1), we have $b=L$.\nSubstitute $b=L$ into (2):\n$$\\frac{1}{4a} - L = L \\implies \\frac{1}{4a} = 2L \\implies 8aL=1$$\nSubstitute $b=L$ into (3):\n$$-a + 1 - L = -L \\implies -a+1=0 \\implies a=1$$\nNow, substitute $a=1$ into the equation $8aL=1$:\n$$8(1)L = 1 \\implies L = \\frac{1}{8}$$\nFinally, using $b=L$, we get:\n$$b = \\frac{1}{8}$$\n\n**Step 4: Verification**\nThe determined parameters are $a=1$ and $b=1/8$. This gives the polynomial $p(x) = x^2 + \\frac{1}{8}$.\nLet's verify our assumption $a  1/2$. Indeed, $a=1  1/2$, so the interior extremum is in the interval $(0,1)$ at $x = 1/(2a)=1/2$.\nThe error function on $[0,1]$ is $E(x) = -x^2 + x - \\frac{1}{8}$.\nThe equioscillation points are $0, 1/2, 1$.\n$E(0) = -\\frac{1}{8}$\n$E(1/2) = -(1/2)^2 + 1/2 - 1/8 = -1/4 + 1/2 - 1/8 = 1/4 - 1/8 = \\frac{1}{8}$\n$E(1) = -1^2+1-1/8 = -\\frac{1}{8}$\nThe error values are $-1/8, 1/8, -1/8$, which alternate and have maximum magnitude $L=1/8$.\nThe function $E(x) = -x^2 + x - 1/8 = -(x^2 - x + 1/8) = -((x-1/2)^2 - 1/4 + 1/8) = 1/8 - (x-1/2)^2$.\nOn $[0,1]$, the maximum of $E(x)$ is $1/8$ at $x=1/2$, and the minimum is at the boundaries $x=0,1$, where $E(0)=E(1)=-1/8$. Thus, $|E(x)| \\le 1/8$ for all $x \\in [0,1]$.\nThis confirms that $p(x) = x^2 + 1/8$ is indeed the best uniform approximation to $f(x)=|x|$ on $[-1,1]$ from the specified family of polynomials.", "answer": "$$\n\\boxed{x^2 + \\frac{1}{8}}\n$$", "id": "597435"}]}