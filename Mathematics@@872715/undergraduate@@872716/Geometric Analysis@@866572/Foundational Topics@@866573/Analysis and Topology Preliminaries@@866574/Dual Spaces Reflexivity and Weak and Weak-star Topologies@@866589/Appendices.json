{"hands_on_practices": [{"introduction": "The concept of a dual space can feel abstract at first. This practice grounds the theory in a concrete calculation, demonstrating how any linear functional in a finite-dimensional space can be uniquely expressed in a special basis called the dual basis. Mastering this computational tool [@problem_id:3046464] is the first step toward understanding the structure and utility of dual spaces.", "problem": "Let $X$ be the finite-dimensional normed space $X=\\mathbb{R}^{3}$ equipped with the norm $\\|x\\|=\\| (x_{1},x_{2},x_{3}) \\|=|x_{1}|+2|x_{2}|+3|x_{3}|$. Consider the ordered basis $(e_{1},e_{2},e_{3})$ of $X$ given by\n$$\ne_{1}=(1,1,0),\\quad e_{2}=(0,1,1),\\quad e_{3}=(2,-1,1).\n$$\nUsing only the core definitions of the dual space $X^{*}$ (the space of all continuous linear functionals on $X$), the canonical pairing between $X$ and $X^{*}$, and the definition of the dual basis $(e_{i}^{*})_{i=1}^{3}$ satisfying $e_{i}^{*}(e_{j})=\\delta_{ij}$, construct the dual basis $(e_{i}^{*})_{i=1}^{3}$ abstractly via its defining property. Then, for the linear functional $f\\in X^{*}$ defined by\n$$\nf(x)=a^{\\top}B\\,x,\\quad \\text{where } a=(1,-2,3) \\text{ and } B=\\begin{pmatrix}0  1  -1 \\\\ 2  0  1 \\\\ 1  1  0\\end{pmatrix},\n$$\ncompute the matrix representation of $f$ with respect to the dual basis $(e_{i}^{*})_{i=1}^{3}$, understood as the unique row vector of coefficients $(c_{1},c_{2},c_{3})$ such that $f=c_{1}e_{1}^{*}+c_{2}e_{2}^{*}+c_{3}e_{3}^{*}$. Express your final answer as a single row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required. You may assume standard facts of finite-dimensional normed spaces, including reflexivity and the coincidence of weak, weak-star, and norm topologies in finite dimension.", "solution": "The problem is first validated to ensure it is well-posed and scientifically sound.\n\n**Step 1: Extract Givens**\n- The vector space is $X = \\mathbb{R}^{3}$, a finite-dimensional normed space.\n- The norm is given by $\\|x\\| = \\|(x_{1},x_{2},x_{3})\\| = |x_{1}|+2|x_{2}|+3|x_{3}|$.\n- An ordered basis for $X$ is $(e_{1},e_{2},e_{3})$, where $e_{1}=(1,1,0)$, $e_{2}=(0,1,1)$, and $e_{3}=(2,-1,1)$.\n- The dual space is $X^{*}$, the space of all continuous linear functionals on $X$.\n- The dual basis is $(e_{1}^{*},e_{2}^{*},e_{3}^{*})$, defined by the property $e_{i}^{*}(e_{j})=\\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n- A linear functional $f \\in X^{*}$ is defined by $f(x)=a^{\\top}B\\,x$.\n- The vectors and matrix for the functional are $a=(1,-2,3)$ and $B=\\begin{pmatrix}0  1  -1 \\\\ 2  0  1 \\\\ 1  1  0\\end{pmatrix}$.\n- The objective is to find the unique row vector of coefficients $(c_{1},c_{2},c_{3})$ such that $f=c_{1}e_{1}^{*}+c_{2}e_{2}^{*}+c_{3}e_{3}^{*}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically sound and well-posed.\n- **Scientific Grounding**: The concepts of vector spaces, norms, dual spaces, and dual bases are fundamental topics in linear algebra and functional analysis. The given norm is a valid weighted $l_{1}$-norm. A quick check shows the basis vectors are linearly independent, as the determinant of the matrix formed by them is non-zero:\n$$\n\\det\\begin{pmatrix} 1  0  2 \\\\ 1  1  -1 \\\\ 0  1  1 \\end{pmatrix} = 1(1 - (-1)) - 0 + 2(1 - 0) = 2 + 2 = 4 \\neq 0.\n$$\nThe functional $f$ is a linear transformation on a finite-dimensional space, and is therefore continuous, so $f \\in X^{*}$.\n- **Well-Posed**: For any basis in a finite-dimensional vector space, a unique dual basis exists. Any vector (or functional, in the dual space) has a unique representation in terms of a basis. Therefore, a unique solution for the coefficients $(c_{1},c_{2},c_{3})$ exists.\n- **Completeness**: All necessary information to determine the coefficients is provided. The mention of the specific norm and topological concepts (weak, weak-star) sets the context but is not strictly necessary for the algebraic computation, as the problem notes that in finite dimensions these topologies coincide with the norm topology.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution**\nThe task is to find the coordinates $(c_{1},c_{2},c_{3})$ of the linear functional $f$ with respect to the dual basis $(e_{1}^{*}, e_{2}^{*}, e_{3}^{*})$. The representation is given by the equation:\n$$\nf = c_{1}e_{1}^{*} + c_{2}e_{2}^{*} + c_{3}e_{3}^{*} = \\sum_{i=1}^{3} c_{i}e_{i}^{*}\n$$\nThe core definition of the dual basis is that $e_{i}^{*}(e_{j}) = \\delta_{ij}$ for $i,j \\in \\{1,2,3\\}$. This property provides a direct method for computing the coefficients $c_{j}$. By applying the functional $f$ to one of the basis vectors, say $e_{j}$, we get:\n$$\nf(e_{j}) = \\left(\\sum_{i=1}^{3} c_{i}e_{i}^{*}\\right)(e_{j}) = \\sum_{i=1}^{3} c_{i}e_{i}^{*}(e_{j})\n$$\nUsing the defining property of the dual basis, $e_{i}^{*}(e_{j}) = \\delta_{ij}$, the sum simplifies:\n$$\nf(e_{j}) = \\sum_{i=1}^{3} c_{i}\\delta_{ij} = c_{j} \\cdot 1 + \\sum_{i \\neq j} c_{i} \\cdot 0 = c_{j}\n$$\nThus, the coefficients $c_{j}$ are found by simply evaluating the functional $f$ on the corresponding basis vectors $e_{j}$.\n\nFirst, let's find the explicit form of the functional $f(x)$. It is defined as $f(x) = a^{\\top}B\\,x$ for $x=(x_{1},x_{2},x_{3}) \\in \\mathbb{R}^{3}$. The vector $a$ is $(1,-2,3)$, so its transpose is $a^{\\top} = \\begin{pmatrix} 1  -2  3 \\end{pmatrix}$. We compute the product $a^{\\top}B$:\n$$\na^{\\top}B = \\begin{pmatrix} 1  -2  3 \\end{pmatrix}\n\\begin{pmatrix}\n0  1  -1 \\\\\n2  0  1 \\\\\n1  1  0\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix}\n(1)(0) + (-2)(2) + (3)(1)  (1)(1) + (-2)(0) + (3)(1)  (1)(-1) + (-2)(1) + (3)(0)\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix}\n0 - 4 + 3  1 + 0 + 3  -1 - 2 + 0\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix}\n-1  4  -3\n\\end{pmatrix}\n$$\nSo, the functional $f$ acts on a vector $x = (x_{1},x_{2},x_{3})$ as:\n$$\nf(x) = \\begin{pmatrix} -1  4  -3 \\end{pmatrix}\n\\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{pmatrix}\n= -x_{1} + 4x_{2} - 3x_{3}\n$$\nNow, we compute the coefficients $c_{1}, c_{2}, c_{3}$ by evaluating $f$ on the basis vectors $e_{1}, e_{2}, e_{3}$.\n\nFor $c_{1}$:\n$e_{1} = (1,1,0)$.\n$$\nc_{1} = f(e_{1}) = f(1,1,0) = -(1) + 4(1) - 3(0) = -1 + 4 = 3\n$$\n\nFor $c_{2}$:\n$e_{2} = (0,1,1)$.\n$$\nc_{2} = f(e_{2}) = f(0,1,1) = -(0) + 4(1) - 3(1) = 4 - 3 = 1\n$$\n\nFor $c_{3}$:\n$e_{3} = (2,-1,1)$.\n$$\nc_{3} = f(e_{3}) = f(2,-1,1) = -(2) + 4(-1) - 3(1) = -2 - 4 - 3 = -9\n$$\n\nThe coefficients are $c_{1}=3$, $c_{2}=1$, and $c_{3}=-9$. The problem asks for the matrix representation of $f$ with respect to the dual basis, which is the row vector of these coefficients.\nTherefore, the representation is $(3, 1, -9)$.\nThe functional $f$ can be written as $f = 3e_{1}^{*} + 1e_{2}^{*} - 9e_{3}^{*}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3  1  -9\n\\end{pmatrix}\n}\n$$", "id": "3046464"}, {"introduction": "Beyond algebraic structure, vector spaces are endowed with geometric structure defined by norms. This practice explores the fascinating relationship between the geometry of a space and its dual, showing that the choice of a norm on a space determines a corresponding dual norm on its dual space. By comparing the standard Euclidean norm with the \"taxicab\" norm [@problem_id:3046453], you will see firsthand how different geometries transform under the operation of duality.", "problem": "Let $X = \\mathbb{R}^{2}$ with the standard inner product $\\langle x, y \\rangle = x_{1} y_{1} + x_{2} y_{2}$. Define two norms on $X$ by\n$$\n\\|x\\|_{A} = \\sqrt{x_{1}^{2} + x_{2}^{2}} \\quad \\text{and} \\quad \\|x\\|_{B} = |x_{1}| + |x_{2}|.\n$$\nUsing only first principles (definitions of norms, dual norms, and the Cauchy–Schwarz inequality), do the following:\n\n1. Prove that $\\|\\cdot\\|_{A}$ and $\\|\\cdot\\|_{B}$ are equivalent. Determine the optimal equivalence constants $c_{\\mathrm{low}}$ and $c_{\\mathrm{up}}$ such that for all $x \\in X$,\n$$\nc_{\\mathrm{low}} \\,\\|x\\|_{A} \\le \\|x\\|_{B} \\le c_{\\mathrm{up}} \\,\\|x\\|_{A}.\n$$\nHere, “optimal” means that $c_{\\mathrm{low}}$ is the largest constant for which the left inequality holds for all $x$, and $c_{\\mathrm{up}}$ is the smallest constant for which the right inequality holds for all $x$.\n\n2. Identify the dual norms $\\|\\cdot\\|_{A^{*}}$ and $\\|\\cdot\\|_{B^{*}}$ on $X^{*} \\cong \\mathbb{R}^{2}$ via the inner product pairing, and show that these dual norms are not equal but are equivalent. Determine the optimal equivalence constants $d_{\\mathrm{low}}$ and $d_{\\mathrm{up}}$ such that for all $y \\in X^{*} \\cong \\mathbb{R}^{2}$,\n$$\nd_{\\mathrm{low}} \\,\\|y\\|_{A^{*}} \\le \\|y\\|_{B^{*}} \\le d_{\\mathrm{up}} \\,\\|y\\|_{A^{*}}.\n$$\n\nYou must justify each inequality from fundamental definitions, and verify optimality by constructing explicit vectors that saturate each bound. Express your final answer as a single row matrix containing the four optimal constants in the order\n$$\n\\big(c_{\\mathrm{low}},\\; c_{\\mathrm{up}},\\; d_{\\mathrm{low}},\\; d_{\\mathrm{up}}\\big).\n$$\nNo rounding is required.", "solution": "The problem is valid as it is scientifically grounded in standard functional analysis, well-posed, objective, and self-contained. All terms and norms are defined explicitly, and the tasks are mathematically precise. We proceed to the solution.\n\nLet $X = \\mathbb{R}^{2}$ be equipped with the standard inner product $\\langle x, y \\rangle = x_{1} y_{1} + x_{2} y_{2}$. We are given two norms on $X$: the Euclidean norm $\\|x\\|_{A} = \\sqrt{x_{1}^{2} + x_{2}^{2}}$ (which is the standard $L_2$-norm) and the taxicab norm $\\|x\\|_{B} = |x_{1}| + |x_{2}|$ (which is the standard $L_1$-norm).\n\n**Part 1: Equivalence of $\\|\\cdot\\|_{A}$ and $\\|\\cdot\\|_{B}$**\n\nTwo norms $\\|\\cdot\\|_{1}$ and $\\|\\cdot\\|_{2}$ on a vector space $V$ are equivalent if there exist positive constants $c_{1}$ and $c_{2}$ such that for all $v \\in V$, $c_{1}\\|v\\|_{1} \\le \\|v\\|_{2} \\le c_{2}\\|v\\|_{1}$. We are asked to find the optimal constants $c_{\\mathrm{low}}$ and $c_{\\mathrm{up}}$ for the inequality $c_{\\mathrm{low}} \\|x\\|_{A} \\le \\|x\\|_{B} \\le c_{\\mathrm{up}} \\|x\\|_{A}$. This is equivalent to finding the minimum and maximum of the ratio $\\frac{\\|x\\|_{B}}{\\|x\\|_{A}}$ over all non-zero $x \\in X$.\n\nFirst, let's establish the upper bound for $\\|x\\|_{B}$, which will determine $c_{\\mathrm{up}}$. Consider the square of $\\|x\\|_{B}$:\n$$\n(\\|x\\|_{B})^{2} = (|x_{1}| + |x_{2}|)^{2} = |x_{1}|^{2} + |x_{2}|^{2} + 2|x_{1}||x_{2}| = x_{1}^{2} + x_{2}^{2} + 2|x_{1}x_{2}|.\n$$\nWe know that for any real numbers $a$ and $b$, $2ab \\le a^{2} + b^{2}$. Applying this with $a = |x_{1}|$ and $b = |x_{2}|$, we get $2|x_{1}x_{2}| \\le x_{1}^{2} + x_{2}^{2}$.\nSubstituting this into the expression for $(\\|x\\|_{B})^{2}$:\n$$\n(\\|x\\|_{B})^{2} \\le (x_{1}^{2} + x_{2}^{2}) + (x_{1}^{2} + x_{2}^{2}) = 2(x_{1}^{2} + x_{2}^{2}) = 2(\\|x\\|_{A})^{2}.\n$$\nTaking the square root of both sides (since norms are non-negative) yields:\n$$\n\\|x\\|_{B} \\le \\sqrt{2} \\|x\\|_{A}.\n$$\nThis shows that an upper bound is $c_{\\mathrm{up}} \\le \\sqrt{2}$. To prove that $c_{\\mathrm{up}} = \\sqrt{2}$ is optimal, we must find a vector $x \\in X$ for which equality holds. Equality holds if and only if $2|x_{1}x_{2}| = x_{1}^{2} + x_{2}^{2}$, which is equivalent to $(|x_{1}| - |x_{2}|)^{2} = 0$, meaning $|x_{1}| = |x_{2}|$. Let's choose $x = (1, 1)$. For this vector:\n$\\|x\\|_{A} = \\sqrt{1^{2} + 1^{2}} = \\sqrt{2}$.\n$\\|x\\|_{B} = |1| + |1| = 2$.\nSubstituting into the inequality: $\\|x\\|_{B} = 2$ and $\\sqrt{2}\\|x\\|_{A} = \\sqrt{2}(\\sqrt{2}) = 2$.\nSince we have found a vector for which $\\|x\\|_{B} = \\sqrt{2} \\|x\\|_{A}$, the constant $c_{\\mathrm{up}} = \\sqrt{2}$ is the smallest possible upper bound, and is therefore optimal.\n\nNext, we establish the lower bound for $\\|x\\|_{B}$, which will determine $c_{\\mathrm{low}}$.\n$$\n(\\|x\\|_{A})^{2} = x_{1}^{2} + x_{2}^{2} = |x_{1}|^{2} + |x_{2}|^{2}.\n$$\nWe compare this to $(\\|x\\|_{B})^{2}$:\n$$\n(\\|x\\|_{B})^{2} = |x_{1}|^{2} + |x_{2}|^{2} + 2|x_{1}||x_{2}|.\n$$\nSince $2|x_{1}||x_{2}| \\ge 0$, we have $(\\|x\\|_{B})^{2} \\ge |x_{1}|^{2} + |x_{2}|^{2} = (\\|x\\|_{A})^{2}$.\nTaking the square root gives:\n$$\n\\|x\\|_{B} \\ge \\|x\\|_{A}.\n$$\nThis shows that a lower bound is $c_{\\mathrm{low}} \\ge 1$. To prove optimality, we need to find a vector $x \\in X$ where equality holds. Equality holds if and only if $2|x_{1}||x_{2}| = 0$, which means either $x_{1}=0$ or $x_{2}=0$. Let's choose $x = (1, 0)$. For this vector:\n$\\|x\\|_{A} = \\sqrt{1^{2} + 0^{2}} = 1$.\n$\\|x\\|_{B} = |1| + |0| = 1$.\nHere, $\\|x\\|_{B} = 1 \\cdot \\|x\\|_{A}$. This demonstrates that $c_{\\mathrm{low}} = 1$ is the largest possible lower bound, and is therefore optimal.\n\nSo, for Part 1, we have $c_{\\mathrm{low}} = 1$ and $c_{\\mathrm{up}} = \\sqrt{2}$.\n\n**Part 2: Dual Norms and their Equivalence**\n\nThe dual space $X^{*}$ is the space of all continuous linear functionals on $X$. Through the Riesz Representation Theorem, for any $f \\in X^{*}$, there exists a unique $y \\in X$ such that $f(x) = \\langle x, y \\rangle$ for all $x \\in X$. We identify $X^{*}$ with $X$ (which is $\\mathbb{R}^{2}$) via this correspondence. The dual norm of a functional, identified with the vector $y$, is defined as:\n$$\n\\|y\\|_{*} = \\sup_{x \\in X, \\|x\\| \\le 1} |\\langle x, y \\rangle|.\n$$\n\nFirst, we find the dual norm $\\|\\cdot\\|_{A^{*}}$. Here, the norm on $X$ is $\\|x\\|_{A} = \\sqrt{x_{1}^{2} + x_{2}^{2}}$.\n$$\n\\|y\\|_{A^{*}} = \\sup_{\\|x\\|_{A} \\le 1} |\\langle x, y \\rangle|.\n$$\nBy the Cauchy-Schwarz inequality, $|\\langle x, y \\rangle| \\le \\|x\\|_{A} \\|y\\|_{A}$.\nFor any $x$ with $\\|x\\|_{A} \\le 1$, we have $|\\langle x, y \\rangle| \\le 1 \\cdot \\|y\\|_{A} = \\|y\\|_{A}$. This shows that $\\|y\\|_{A^{*}} \\le \\|y\\|_{A}$.\nTo show equality, for a given non-zero $y$, choose $x = y/\\|y\\|_{A}$. This choice gives $\\|x\\|_{A} = \\|y/\\|y\\|_{A}\\|_{A} = (1/\\|y\\|_{A})\\|y\\|_{A} = 1$.\nFor this $x$, we have $\\langle x, y \\rangle = \\langle y/\\|y\\|_{A}, y \\rangle = \\frac{1}{\\|y\\|_{A}} \\langle y, y \\rangle = \\frac{\\|y\\|_{A}^{2}}{\\|y\\|_{A}} = \\|y\\|_{A}$.\nSince we have found an $x$ in the unit sphere for which $|\\langle x, y \\rangle|$ attains the value $\\|y\\|_{A}$, the supremum is $\\|y\\|_{A}$.\nTherefore, the dual norm is $\\|y\\|_{A^{*}} = \\|y\\|_{A} = \\sqrt{y_{1}^{2} + y_{2}^{2}}$. The Euclidean norm is self-dual.\n\nNext, we find the dual norm $\\|\\cdot\\|_{B^{*}}$. Here, the norm on $X$ is $\\|x\\|_{B} = |x_{1}| + |x_{2}|$.\n$$\n\\|y\\|_{B^{*}} = \\sup_{\\|x\\|_{B} \\le 1} |\\langle x, y \\rangle| = \\sup_{|x_{1}|+|x_{2}|\\le 1} |x_{1} y_{1} + x_{2} y_{2}|.\n$$\nThe unit ball $\\{x \\in X : \\|x\\|_{B} \\le 1\\}$ is a square with vertices at $(1, 0)$, $(0, 1)$, $(-1, 0)$, and $(0, -1)$. The expression $L(x) = x_{1} y_{1} + x_{2} y_{2}$ is a linear function of $x$. The supremum of a linear function over a compact, convex set (like this square) is attained at an extreme point (a vertex). We evaluate $|L(x)|$ at the vertices:\nAt $x = (1, 0)$, $|L(x)| = |y_{1}|$.\nAt $x = (0, 1)$, $|L(x)| = |y_{2}|$.\nAt $x = (-1, 0)$, $|L(x)| = |-y_{1}| = |y_{1}|$.\nAt $x = (0, -1)$, $|L(x)| = |-y_{2}| = |y_{2}|$.\nThe supremum is the maximum of these values. Thus, $\\|y\\|_{B^{*}} = \\max(|y_{1}|, |y_{2}|)$. This is the standard $L_\\infty$-norm.\n\nThe norms are not equal. For example, if $y=(1, 1)$, $\\|y\\|_{A^{*}} = \\sqrt{1^2+1^2} = \\sqrt{2}$, while $\\|y\\|_{B^{*}} = \\max(|1|, |1|) = 1$.\n\nNow we find the optimal equivalence constants $d_{\\mathrm{low}}$ and $d_{\\mathrm{up}}$ for the inequality $d_{\\mathrm{low}} \\|y\\|_{A^{*}} \\le \\|y\\|_{B^{*}} \\le d_{\\mathrm{up}} \\|y\\|_{A^{*}}$.\n\nFirst, the upper bound for $\\|y\\|_{B^{*}}$. We want to find the smallest $d_{\\mathrm{up}}$ such that $\\|y\\|_{B^{*}} \\le d_{\\mathrm{up}} \\|y\\|_{A^{*}}$.\nLet $M = \\max(|y_{1}|, |y_{2}|) = \\|y\\|_{B^{*}}$. Then $y_{1}^{2} \\le M^{2}$ and $y_{2}^{2} \\le M^{2}$.\nHowever, we know $y_{1}^{2} + y_{2}^{2} \\ge M^2$. For instance, if $|y_{1}| \\ge |y_{2}|$, then $M = |y_1|$, so $y_{1}^{2} + y_{2}^{2} \\ge y_1^2 = M^2$.\nSo, $(\\|y\\|_{A^{*}})^{2} = y_{1}^{2} + y_{2}^{2} \\ge (\\max(|y_{1}|, |y_{2}|))^{2} = (\\|y\\|_{B^{*}})^{2}$.\nTaking the square root gives $\\|y\\|_{A^{*}} \\ge \\|y\\|_{B^{*}}$, or $\\|y\\|_{B^{*}} \\le 1 \\cdot \\|y\\|_{A^{*}}$.\nThis shows $d_{\\mathrm{up}} \\le 1$. To show optimality, we seek a vector $y$ where equality holds. Equality requires one component to be zero. Let $y = (1, 0)$.\n$\\|y\\|_{A^{*}} = \\sqrt{1^{2} + 0^{2}} = 1$.\n$\\|y\\|_{B^{*}} = \\max(|1|, |0|) = 1$.\nFor this vector, $\\|y\\|_{B^{*}} = 1 \\cdot \\|y\\|_{A^{*}}$, so $d_{\\mathrm{up}} = 1$ is optimal.\n\nNext, the lower bound for $\\|y\\|_{B^{*}}$. We want to find the largest $d_{\\mathrm{low}}$ such that $d_{\\mathrm{low}} \\|y\\|_{A^{*}} \\le \\|y\\|_{B^{*}}$.\nAgain, let $M = \\max(|y_{1}|, |y_{2}|) = \\|y\\|_{B^{*}}$. We know $|y_{1}| \\le M$ and $|y_{2}| \\le M$.\nThen $(\\|y\\|_{A^{*}})^{2} = y_{1}^{2} + y_{2}^{2} \\le M^{2} + M^{2} = 2M^{2} = 2(\\|y\\|_{B^{*}})^{2}$.\nTaking the square root gives $\\|y\\|_{A^{*}} \\le \\sqrt{2}\\|y\\|_{B^{*}}$.\nRearranging, we get $\\frac{1}{\\sqrt{2}} \\|y\\|_{A^{*}} \\le \\|y\\|_{B^{*}}$.\nThis shows $d_{\\mathrm{low}} \\ge \\frac{1}{\\sqrt{2}}$. To show optimality, we seek a vector where equality holds. Equality requires $|y_{1}| = |y_{2}| = M$. Let $y = (1, 1)$.\n$\\|y\\|_{A^{*}} = \\sqrt{1^{2} + 1^{2}} = \\sqrt{2}$.\n$\\|y\\|_{B^{*}} = \\max(|1|, |1|) = 1$.\nFor this vector, $\\frac{1}{\\sqrt{2}} \\|y\\|_{A^{*}} = \\frac{1}{\\sqrt{2}}(\\sqrt{2}) = 1 = \\|y\\|_{B^{*}}$. So, $d_{\\mathrm{low}} = \\frac{1}{\\sqrt{2}}$ is optimal.\n\nThe four optimal constants are $c_{\\mathrm{low}} = 1$, $c_{\\mathrm{up}} = \\sqrt{2}$, $d_{\\mathrm{low}} = \\frac{1}{\\sqrt{2}}$, and $d_{\\mathrm{up}} = 1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  \\sqrt{2}  \\frac{1}{\\sqrt{2}}  1\n\\end{pmatrix}\n}\n$$", "id": "3046453"}, {"introduction": "In finite-dimensional spaces, all reasonable notions of convergence for a sequence are equivalent, but this simple picture breaks down dramatically in infinite dimensions. This exercise presents a classic example in the function space $L^p(\\mathbb{R})$ to illustrate the crucial distinction between strong (norm) convergence and weak convergence. You will analyze a sequence of functions that \"vanishes\" weakly by escaping to infinity, a phenomenon that is central to modern analysis and the calculus of variations [@problem_id:3046456].", "problem": "Let $p$ be a fixed real number with $1p\\infty$, and let $\\Omega=\\mathbb{R}$ with the Lebesgue measure. Consider the sequence $(u_n)_{n\\in\\mathbb{N}}$ in $L^p(\\mathbb{R})$ defined by $u_n(x)=\\chi_{[n,n+1]}(x)$ for each $n\\in\\mathbb{N}$, where $\\chi_{[n,n+1]}$ is the indicator function of the interval $[n,n+1]$. Work from the following foundational definitions and facts:\n- Weak convergence in a Banach space: $u_n\\rightharpoonup u$ if for every continuous linear functional $\\Lambda$ on the space, $\\Lambda(u_n)\\to\\Lambda(u)$.\n- Duality for $L^p$: the dual space of $L^p(\\mathbb{R})$ is $L^q(\\mathbb{R})$ with $q=\\frac{p}{p-1}$, and the dual pairing is given by $\\Lambda_v(u)=\\int_{\\mathbb{R}} u(x)v(x)\\,dx$ for $v\\in L^q(\\mathbb{R})$.\n- Hölder’s inequality: for $u\\in L^p(\\mathbb{R})$ and $v\\in L^q(\\mathbb{R})$, $\\left|\\int_{\\mathbb{R}} u(x)v(x)\\,dx\\right|\\leq \\|u\\|_{L^p(\\mathbb{R})}\\|v\\|_{L^q(\\mathbb{R})}$.\n- The dual representation of the $L^p$ norm: $\\|u\\|_{L^p(\\mathbb{R})}=\\sup\\left\\{\\int_{\\mathbb{R}} u(x)v(x)\\,dx:\\,v\\in L^q(\\mathbb{R}),\\,\\|v\\|_{L^q(\\mathbb{R})}\\leq 1\\right\\}$.\n\nTasks:\n1. Prove that $u_n\\rightharpoonup 0$ in $L^p(\\mathbb{R})$.\n2. Show that $\\|u_n\\|_{L^p(\\mathbb{R})}\\not\\to \\|0\\|_{L^p(\\mathbb{R})}$ by computing $\\|u_n\\|_{L^p(\\mathbb{R})}$ for each $n$.\n3. Using only the foundational definitions and facts stated above (and not assuming any theorems about weak lower semicontinuity beyond them), verify the weak lower semicontinuity of the $L^p$ norm in this example by computing the value of\n$$\n\\liminf_{n\\to\\infty}\\|u_n\\|_{L^p(\\mathbb{R})}-\\|0\\|_{L^p(\\mathbb{R})}.\n$$\n\nExpress your final answer as a single real number. No rounding is required.", "solution": "The problem is validated as self-contained, scientifically grounded in the theory of functional analysis, and well-posed. We proceed with the solution, addressing each of the three tasks in turn.\n\nFirst, we prove that the sequence $(u_n)_{n\\in\\mathbb{N}}$ converges weakly to $0$ in $L^p(\\mathbb{R})$, denoted $u_n \\rightharpoonup 0$. According to the provided definitions, this requires showing that for every continuous linear functional $\\Lambda$ on $L^p(\\mathbb{R})$, we have $\\Lambda(u_n) \\to 0$ as $n \\to \\infty$. The dual space of $L^p(\\mathbb{R})$ is identified with $L^q(\\mathbb{R})$, where $q = \\frac{p}{p-1}$, via the pairing $\\Lambda_v(u) = \\int_{\\mathbb{R}} u(x)v(x)\\,dx$ for any $v \\in L^q(\\mathbb{R})$. Thus, we must show that for any arbitrary function $v \\in L^q(\\mathbb{R})$, the sequence of real numbers $\\int_{\\mathbb{R}} u_n(x)v(x)\\,dx$ converges to $0$.\n\nSubstituting the definition of $u_n(x) = \\chi_{[n,n+1]}(x)$, the integral becomes:\n$$\n\\Lambda_v(u_n) = \\int_{\\mathbb{R}} \\chi_{[n,n+1]}(x)v(x)\\,dx = \\int_n^{n+1} v(x)\\,dx.\n$$\nWe can bound the absolute value of this integral using Hölder's inequality. We consider the functions $v(x)$ and the constant function $1$ on the interval $[n, n+1]$:\n$$\n\\left| \\int_n^{n+1} v(x) \\cdot 1 \\,dx \\right| \\leq \\left( \\int_n^{n+1} |v(x)|^q \\,dx \\right)^{\\frac{1}{q}} \\left( \\int_n^{n+1} 1^p \\,dx \\right)^{\\frac{1}{p}}.\n$$\nThe second term is $(\\int_n^{n+1} 1 \\,dx)^{\\frac{1}{p}} = ((n+1)-n)^{\\frac{1}{p}} = 1^{\\frac{1}{p}} = 1$. Therefore, the inequality simplifies to:\n$$\n\\left| \\int_n^{n+1} v(x)\\,dx \\right| \\leq \\left( \\int_n^{n+1} |v(x)|^q \\,dx \\right)^{\\frac{1}{q}}.\n$$\nSince $v \\in L^q(\\mathbb{R})$, the integral $\\int_{-\\infty}^{\\infty} |v(x)|^q \\,dx$ is finite. A necessary condition for the convergence of this improper integral is that the integral over the \"tail\" of the domain must approach zero. That is,\n$$\n\\lim_{n \\to \\infty} \\int_n^{\\infty} |v(x)|^q \\,dx = 0.\n$$\nBecause the interval $[n, n+1]$ is a subset of $[n, \\infty)$ and the integrand $|v(x)|^q$ is non-negative, we have:\n$$\n0 \\leq \\int_n^{n+1} |v(x)|^q \\,dx \\leq \\int_n^{\\infty} |v(x)|^q \\,dx.\n$$\nBy the Squeeze Theorem, as $n \\to \\infty$, we must have $\\lim_{n \\to \\infty} \\int_n^{n+1} |v(x)|^q \\,dx = 0$. Consequently,\n$$\n\\lim_{n \\to \\infty} \\left( \\int_n^{n+1} |v(x)|^q \\,dx \\right)^{\\frac{1}{q}} = 0.\n$$\nCombining this with our earlier inequality, $0 \\le |\\Lambda_v(u_n)| \\le (\\int_n^{n+1} |v(x)|^q \\,dx)^{\\frac{1}{q}}$, the Squeeze Theorem implies that $\\lim_{n \\to \\infty} \\Lambda_v(u_n) = 0$. Since this holds for an arbitrary $v \\in L^q(\\mathbb{R})$, we have proven that $u_n \\rightharpoonup 0$ in $L^p(\\mathbb{R})$.\n\nSecond, we address the convergence of the norms. We compute the $L^p$-norm of $u_n$:\n$$\n\\|u_n\\|_{L^p(\\mathbb{R})}^p = \\int_{\\mathbb{R}} |u_n(x)|^p \\,dx = \\int_{\\mathbb{R}} |\\chi_{[n,n+1]}(x)|^p \\,dx.\n$$\nSince $\\chi_{[n,n+1]}(x)$ takes values in $\\{0, 1\\}$, $|\\chi_{[n,n+1]}(x)|^p = \\chi_{[n,n+1]}(x)$. Thus,\n$$\n\\|u_n\\|_{L^p(\\mathbb{R})}^p = \\int_{\\mathbb{R}} \\chi_{[n,n+1]}(x) \\,dx = \\int_n^{n+1} 1 \\,dx = 1.\n$$\nTaking the $p$-th root, we find $\\|u_n\\|_{L^p(\\mathbb{R})} = 1^{\\frac{1}{p}} = 1$ for all $n \\in \\mathbb{N}$.\nThe norm of the weak limit, which is the zero function, is:\n$$\n\\|0\\|_{L^p(\\mathbb{R})} = \\left( \\int_{\\mathbb{R}} |0|^p \\,dx \\right)^{\\frac{1}{p}} = 0.\n$$\nThe limit of the sequence of norms is $\\lim_{n \\to \\infty} \\|u_n\\|_{L^p(\\mathbb{R})} = \\lim_{n \\to \\infty} 1 = 1$. Since $1 \\neq 0$, we have shown that $\\|u_n\\|_{L^p(\\mathbb{R})} \\not\\to \\|0\\|_{L^p(\\mathbb{R})}$. This demonstrates that weak convergence does not imply norm convergence (strong convergence).\n\nThird, we verify the weak lower semicontinuity of the $L^p$ norm for this example and compute the specified value. The property states that if $u_n \\rightharpoonup u$, then $\\|u\\|_{L^p(\\mathbb{R})} \\le \\liminf_{n\\to\\infty} \\|u_n\\|_{L^p(\\mathbb{R})}$. In our case, $u=0$, so we must verify that $\\|0\\|_{L^p(\\mathbb{R})} \\le \\liminf_{n\\to\\infty} \\|u_n\\|_{L^p(\\mathbb{R})}$.\nFrom the previous task, we know that the sequence of norms $(\\|u_n\\|_{L^p(\\mathbb{R})})_{n\\in\\mathbb{N}}$ is the constant sequence $(1, 1, 1, \\dots)$. The limit inferior of a constant sequence is the constant value itself:\n$$\n\\liminf_{n\\to\\infty} \\|u_n\\|_{L^p(\\mathbb{R})} = \\liminf_{n\\to\\infty} 1 = 1.\n$$\nWe also know that $\\|0\\|_{L^p(\\mathbb{R})} = 0$. The inequality $0 \\le 1$ holds true, which verifies the weak lower semicontinuity for this specific case.\nThe problem then asks for the value of the expression $\\liminf_{n\\to\\infty}\\|u_n\\|_{L^p(\\mathbb{R})} - \\|0\\|_{L^p(\\mathbb{R})}$. Substituting the computed values:\n$$\n\\liminf_{n\\to\\infty}\\|u_n\\|_{L^p(\\mathbb{R})} - \\|0\\|_{L^p(\\mathbb{R})} = 1 - 0 = 1.\n$$\nThis positive value represents the \"mass\" that is lost at infinity as the sequence converges weakly but not strongly.", "answer": "$$\n\\boxed{1}\n$$", "id": "3046456"}]}