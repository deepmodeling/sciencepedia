{"hands_on_practices": [{"introduction": "Mazur's Lemma provides a crucial link between weak and strong convergence. The standard orthonormal basis $\\{e_n\\}$ in $\\ell^2$ offers a classic example of a sequence that converges weakly to zero but fails to converge strongly. This exercise invites you to take the first step in seeing the lemma in action by computing the norm of the simple arithmetic mean of these basis vectors, demonstrating how averaging can induce convergence. [@problem_id:1869444]", "problem": "Consider the Hilbert space $\\ell^2$, which consists of all square-summable real sequences $x = (x_1, x_2, x_3, \\dots)$ such that the series $\\sum_{k=1}^\\infty x_k^2$ converges. The norm on this space is defined as $\\|x\\|_{\\ell^2} = \\left(\\sum_{k=1}^\\infty x_k^2\\right)^{1/2}$.\n\nLet $\\{e_n\\}_{n=1}^\\infty$ be the standard orthonormal basis for $\\ell^2$, where the vector $e_n$ is a sequence with a $1$ in the $n$-th position and zeros in all other positions.\n\nFor any positive integer $N$, we can construct a new vector, $y_N$, by taking the arithmetic mean of the first $N$ basis vectors:\n$$y_N = \\frac{1}{N} \\sum_{n=1}^N e_n$$\nThis vector represents a simple averaging process within the space. To understand the properties of this averaging, we are interested in the magnitude of the resulting vector.\n\nDetermine a closed-form expression for the squared norm, $\\|y_N\\|_{\\ell^2}^2$, as a function of $N$.", "solution": "The problem asks for the squared norm of the vector $y_N = \\frac{1}{N} \\sum_{n=1}^N e_n$ in the Hilbert space $\\ell^2$.\n\nThe squared norm of a vector $v$ in a Hilbert space is given by the inner product of the vector with itself: $\\|v\\|^2 = \\langle v, v \\rangle$. Applying this to our vector $y_N$:\n$$ \\|y_N\\|_{\\ell^2}^2 = \\left\\langle y_N, y_N \\right\\rangle $$\n\nSubstitute the definition of $y_N$ into the inner product:\n$$ \\|y_N\\|_{\\ell^2}^2 = \\left\\langle \\frac{1}{N} \\sum_{n=1}^N e_n, \\frac{1}{N} \\sum_{m=1}^N e_m \\right\\rangle $$\n\nUsing the properties of the inner product, we can pull the constant factors out. The scalar $\\frac{1}{N}$ from the first argument comes out as $\\frac{1}{N}$, and the scalar $\\frac{1}{N}$ from the second argument comes out as its complex conjugate, which is also $\\frac{1}{N}$ since it is a real number.\n$$ \\|y_N\\|_{\\ell^2}^2 = \\frac{1}{N^2} \\left\\langle \\sum_{n=1}^N e_n, \\sum_{m=1}^N e_m \\right\\rangle $$\n\nNext, we use the linearity of the inner product to expand the sums:\n$$ \\|y_N\\|_{\\ell^2}^2 = \\frac{1}{N^2} \\sum_{n=1}^N \\sum_{m=1}^N \\langle e_n, e_m \\rangle $$\n\nThe problem states that $\\{e_n\\}_{n=1}^\\infty$ is an orthonormal basis. By the definition of an orthonormal set, the inner product of any two basis vectors is given by the Kronecker delta:\n$$ \\langle e_n, e_m \\rangle = \\delta_{nm} = \\begin{cases} 1 & \\text{if } n=m \\\\ 0 & \\text{if } n \\neq m \\end{cases} $$\n\nSubstituting this into our expression, the inner sum over $m$ simplifies. For a fixed $n$, the term $\\langle e_n, e_m \\rangle$ is non-zero only when $m=n$.\n$$ \\|y_N\\|_{\\ell^2}^2 = \\frac{1}{N^2} \\sum_{n=1}^N \\left( \\sum_{m=1}^N \\delta_{nm} \\right) $$\nThe inner sum $\\sum_{m=1}^N \\delta_{nm}$ evaluates to $1$, because for each $n \\in \\{1, \\dots, N\\}$, there is exactly one value of $m$ in the same range such that $m=n$.\n\nSo the expression simplifies to:\n$$ \\|y_N\\|_{\\ell^2}^2 = \\frac{1}{N^2} \\sum_{n=1}^N 1 $$\n\nThe sum $\\sum_{n=1}^N 1$ is simply $N$.\n$$ \\|y_N\\|_{\\ell^2}^2 = \\frac{1}{N^2} \\cdot N = \\frac{1}{N} $$\n\nThus, the squared norm of $y_N$ is $\\frac{1}{N}$.\n\nAlternatively, one can use the Pythagorean theorem for orthogonal vectors. Since the vectors $\\{e_n\\}$ are mutually orthogonal, the squared norm of their sum is the sum of their squared norms.\nFirst, pull out the constant:\n$$ \\|y_N\\|_{\\ell^2}^2 = \\left\\| \\frac{1}{N} \\sum_{n=1}^N e_n \\right\\|^2 = \\frac{1}{N^2} \\left\\| \\sum_{n=1}^N e_n \\right\\|^2 $$\nBy the generalized Pythagorean theorem:\n$$ \\left\\| \\sum_{n=1}^N e_n \\right\\|^2 = \\sum_{n=1}^N \\|e_n\\|^2 $$\nSince the basis is orthonormal, $\\|e_n\\| = 1$ for all $n$. Therefore, $\\|e_n\\|^2 = 1^2 = 1$.\n$$ \\sum_{n=1}^N \\|e_n\\|^2 = \\sum_{n=1}^N 1 = N $$\nSubstituting this back, we get:\n$$ \\|y_N\\|_{\\ell^2}^2 = \\frac{1}{N^2} \\cdot N = \\frac{1}{N} $$\nBoth methods yield the same result.", "answer": "$$\\boxed{\\frac{1}{N}}$$", "id": "1869444"}, {"introduction": "Building on the idea of averaging, this practice challenges you to explore the flexibility in constructing the strongly convergent sequence promised by Mazur's Lemma. After first confirming that the standard basis in $\\ell^2$ weakly converges to the zero vector, you will investigate various weighted averaging schemes. This will help you appreciate that while the lemma guarantees existence, the specific form of the convex combinations can vary significantly. [@problem_id:1869483]", "problem": "Consider the Hilbert space $\\ell^2$ of square-summable real sequences $x=(x_k)_{k=1}^\\infty$ with the inner product defined as $\\langle x, y \\rangle = \\sum_{k=1}^\\infty x_k y_k$. Let $(e_n)_{n=1}^\\infty$ be the standard orthonormal basis for $\\ell^2$, where the sequence $e_n$ has a 1 in the $n$-th position and zeroes elsewhere.\n\nThe sequence of basis vectors $(e_n)$ converges weakly to a limit $w$. Mazur's lemma states that there must exist a sequence of convex combinations of $(e_n)$, let's call it $(y_N)_{N=1}^\\infty$, that converges strongly (in the $\\ell^2$ norm) to the same limit $w$. A term $y_N$ is a convex combination of the first $N$ vectors if it can be written as $y_N = \\sum_{n=1}^N \\alpha_{n,N} e_n$ where the coefficients $\\alpha_{n,N}$ are all non-negative and sum to one, i.e., $\\sum_{n=1}^N \\alpha_{n,N} = 1$.\n\nWhich of the following options provides a correct pair $(w, y_N)$, where $w$ is the weak limit of $(e_n)$ and $y_N$ is the general term of a sequence of convex combinations that converges strongly to $w$? Note that $0$ represents the zero vector in $\\ell^2$. Select all that apply.\n\nA. $w = 0$, $y_N = \\frac{1}{N} \\sum_{n=1}^N e_n$\n\nB. $w = 0$, $y_N = \\frac{2}{N(N+1)} \\sum_{n=1}^N n e_n$\n\nC. $w = 0$, $y_N = \\frac{1}{\\sqrt{N}} \\sum_{n=1}^N e_n$\n\nD. $w = e_1$, $y_N = e_N$\n\nE. $w = 0$, $y_N = \\frac{6}{N(N+1)(2N+1)} \\sum_{n=1}^N n^2 e_n$", "solution": "First determine the weak limit of the standard basis $(e_{n})$. By definition, $e_{n}\\rightharpoonup w$ means that for every $y=(y_{k})\\in \\ell^{2}$,\n$$\n\\langle e_{n},y\\rangle = \\sum_{k=1}^{\\infty} (e_{n})_{k} y_{k} = y_{n}\\to 0 \\quad \\text{as } n\\to\\infty,\n$$\nsince every $\\ell^{2}$ sequence satisfies $y_{n}\\to 0$. Hence the weak limit is $w=0$.\n\nNow check each option for two properties: (i) $y_{N}$ is a convex combination, i.e., $y_{N}=\\sum_{n=1}^{N}\\alpha_{n,N} e_{n}$ with $\\alpha_{n,N}\\ge 0$ and $\\sum_{n=1}^{N}\\alpha_{n,N}=1$, and (ii) $\\|y_{N}-w\\|_{2}\\to 0$ with $w=0$.\n\nA. Here $\\alpha_{n,N}=\\frac{1}{N}$ for $1\\le n\\le N$. Then $\\sum_{n=1}^{N}\\alpha_{n,N}=1$, so it is a convex combination. Its norm is\n$$\n\\|y_{N}\\|_{2}^{2}=\\left\\|\\frac{1}{N}\\sum_{n=1}^{N}e_{n}\\right\\|_{2}^{2}\n=\\frac{1}{N^{2}}\\sum_{n=1}^{N}\\|e_{n}\\|_{2}^{2}\n=\\frac{1}{N}\\to 0.\n$$\nThus $y_{N}\\to 0$ strongly. A is correct.\n\nB. Here $\\alpha_{n,N}=\\frac{2n}{N(N+1)}\\ge 0$. Using $\\sum_{n=1}^{N} n=\\frac{N(N+1)}{2}$,\n$$\n\\sum_{n=1}^{N}\\alpha_{n,N}=\\frac{2}{N(N+1)}\\sum_{n=1}^{N}n=1,\n$$\nso it is a convex combination. Using $\\sum_{n=1}^{N} n^{2}=\\frac{N(N+1)(2N+1)}{6}$,\n$$\n\\|y_{N}\\|_{2}^{2}\n=\\left\\|\\frac{2}{N(N+1)}\\sum_{n=1}^{N} n e_{n}\\right\\|_{2}^{2}\n=\\frac{4}{N^{2}(N+1)^{2}}\\sum_{n=1}^{N} n^{2}\n=\\frac{4}{N^{2}(N+1)^{2}}\\cdot \\frac{N(N+1)(2N+1)}{6}\n=\\frac{2(2N+1)}{3N(N+1)}\\to 0.\n$$\nThus $y_{N}\\to 0$ strongly. B is correct.\n\nC. Here $\\alpha_{n,N}=\\frac{1}{\\sqrt{N}}$, so $\\sum_{n=1}^{N}\\alpha_{n,N}=\\sqrt{N}\\ne 1$; it is not a convex combination. Moreover,\n$$\n\\|y_{N}\\|_{2}^{2}\n=\\left\\|\\frac{1}{\\sqrt{N}}\\sum_{n=1}^{N} e_{n}\\right\\|_{2}^{2}\n=\\frac{1}{N}\\sum_{n=1}^{N}\\|e_{n}\\|_{2}^{2}=1,\n$$\nso it does not converge to $0$. C is incorrect.\n\nD. The weak limit is $w=0$, not $w=e_{1}$. Also $y_{N}=e_{N}$ does not converge strongly to $e_{1}$ since $\\|e_{N}-e_{1}\\|_{2}^{2}=2$ for $N\\ge 2$. D is incorrect.\n\nE. Here $\\alpha_{n,N}=\\frac{6 n^{2}}{N(N+1)(2N+1)}\\ge 0$, and using $\\sum_{n=1}^{N} n^{2}=\\frac{N(N+1)(2N+1)}{6}$,\n$$\n\\sum_{n=1}^{N}\\alpha_{n,N}=1,\n$$\nso it is a convex combination. Using $\\sum_{n=1}^{N} n^{4}=\\frac{N(N+1)(2N+1)(3N^{2}+3N-1)}{30}$,\n$$\n\\|y_{N}\\|_{2}^{2}\n=\\left\\|\\frac{6}{N(N+1)(2N+1)}\\sum_{n=1}^{N} n^{2} e_{n}\\right\\|_{2}^{2}\n=\\frac{36}{N^{2}(N+1)^{2}(2N+1)^{2}}\\sum_{n=1}^{N} n^{4}\n=\\frac{36}{N^{2}(N+1)^{2}(2N+1)^{2}}\\cdot \\frac{N(N+1)(2N+1)(3N^{2}+3N-1)}{30}\n=\\frac{6}{5}\\cdot \\frac{3N^{2}+3N-1}{N(N+1)(2N+1)}\\to 0.\n$$\nThus $y_{N}\\to 0$ strongly. E is correct.\n\nTherefore the correct options are A, B, and E.", "answer": "$$\\boxed{ABE}$$", "id": "1869483"}, {"introduction": "Beyond constructing convergent sequences, Mazur's Lemma is a powerful theoretical tool for deduction. This problem asks you to think about the lemma's logical structure by considering its contrapositive. Given a sequence whose convex combinations are all bounded away from zero, you can draw a firm conclusion about whether the sequence can converge weakly to zero, illustrating how the lemma can be used to prove non-convergence. [@problem_id:1869440]", "problem": "Let $X$ be a real normed linear space. Consider a sequence of elements $\\{x_n\\}_{n=1}^{\\infty}$ in $X$. A finite convex combination of elements from this sequence is any vector $y$ of the form $y = \\sum_{i=1}^{N} \\alpha_i x_{m_i}$, where $N$ is a positive integer, $\\{m_i\\}_{i=1}^N$ are positive integers, each coefficient $\\alpha_i \\ge 0$, and $\\sum_{i=1}^{N} \\alpha_i = 1$.\n\nSuppose that for this particular sequence $\\{x_n\\}$, there exists a positive real number $\\delta > 0$ such that for any finite convex combination $y$ of elements from $\\{x_n\\}$, the inequality $\\|y\\| > \\delta$ holds.\n\nBased on this information, what can be definitively concluded about the weak convergence of the sequence $\\{x_n\\}$ to the zero vector $0$?\n\nA. The sequence $\\{x_n\\}$ must converge weakly to the zero vector.\n\nB. The sequence $\\{x_n\\}$ cannot converge weakly to the zero vector.\n\nC. The sequence $\\{x_n\\}$ may or may not converge weakly to the zero vector; the given information is insufficient to decide.\n\nD. The sequence $\\{x_n\\}$ must converge strongly to a non-zero vector.\n\nE. The sequence $\\{x_n\\}$ is not a Cauchy sequence.", "solution": "We are given a real normed linear space $X$ and a sequence $\\{x_{n}\\}_{n=1}^{\\infty}\\subset X$. A finite convex combination of elements of the sequence is any $y$ of the form\n$$\ny=\\sum_{i=1}^{N}\\alpha_{i}x_{m_{i}},\n$$\nwith $N\\in\\mathbb{N}$, $m_{i}\\in\\mathbb{N}$, $\\alpha_{i}\\ge 0$, and $\\sum_{i=1}^{N}\\alpha_{i}=1$. The assumption is that there exists $\\delta>0$ such that for every such $y$ one has\n$$\n\\|y\\|>\\delta.\n$$\n\nWe analyze the possibility that $\\{x_{n}\\}$ converges weakly to $0$. Recall the definition: $\\{x_{n}\\}$ converges weakly to $0$ if and only if for every $f\\in X^{\\ast}$,\n$$\nf(x_{n})\\to 0.\n$$\nWe will use the following standard result (Mazur's lemma/Mazur's theorem): If $x_{n}\\to x$ weakly in a normed linear space $X$, then there exists a sequence of finite convex combinations $y_{k}$ of elements from suitable tails of $\\{x_{n}\\}$ such that\n$$\n\\|y_{k}-x\\|\\to 0 \\quad \\text{as } k\\to\\infty.\n$$\nApply this with $x=0$. If $\\{x_{n}\\}$ converges weakly to $0$, then there exist finite convex combinations\n$$\ny_{k}=\\sum_{i=1}^{N_{k}}\\alpha_{i}^{(k)}x_{m_{i}^{(k)}}, \\quad \\alpha_{i}^{(k)}\\ge 0,\\quad \\sum_{i=1}^{N_{k}}\\alpha_{i}^{(k)}=1,\n$$\nsuch that\n$$\n\\|y_{k}\\|\\to 0 \\quad \\text{as } k\\to\\infty.\n$$\nBy the definition of limit in a normed space, this implies that for some $k_{0}\\in\\mathbb{N}$,\n$$\n\\|y_{k}\\|<\\frac{\\delta}{2}\\quad \\text{for all } k\\ge k_{0}.\n$$\nHowever, each $y_{k}$ is a finite convex combination of elements of $\\{x_{n}\\}$, so the given assumption enforces\n$$\n\\|y_{k}\\|>\\delta \\quad \\text{for all } k,\n$$\nwhich contradicts $\\|y_{k}\\|<\\frac{\\delta}{2}$ for $k\\ge k_{0}$. Therefore, the hypothesis that $\\{x_{n}\\}$ converges weakly to $0$ is impossible.\n\nIt follows definitively that the sequence $\\{x_{n}\\}$ cannot converge weakly to the zero vector. Hence the correct choice is B, while none of the other options are entailed by the hypothesis.", "answer": "$$\\boxed{B}$$", "id": "1869440"}]}