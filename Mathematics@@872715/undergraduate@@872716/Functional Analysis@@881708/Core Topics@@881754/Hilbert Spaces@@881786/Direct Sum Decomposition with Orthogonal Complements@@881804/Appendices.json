{"hands_on_practices": [{"introduction": "This first exercise provides a foundational understanding of orthogonal complements in a concrete setting. By applying the definition of the Frobenius inner product, you will determine the set of all matrices orthogonal to the subspace of skew-symmetric matrices. This practice [@problem_id:1858250] reveals a fundamental decomposition of the space of $2 \\times 2$ matrices, illustrating a key structural result where the space is the direct sum of symmetric and skew-symmetric matrices.", "problem": "Let $M_2(\\mathbb{R})$ be the vector space of all $2 \\times 2$ matrices with real entries. This space is equipped with the Frobenius inner product, defined for any two matrices $A, B \\in M_2(\\mathbb{R})$ as $\\langle A, B \\rangle_F = \\text{tr}(A^T B)$, where $\\text{tr}(X)$ denotes the trace of a matrix $X$ and $X^T$ denotes its transpose.\n\nConsider the subspace $W \\subset M_2(\\mathbb{R})$ consisting of all skew-symmetric matrices, that is, matrices $S$ such that $S^T = -S$.\n\nThe orthogonal complement of $W$, denoted by $W^\\perp$, is the set of all matrices $A \\in M_2(\\mathbb{R})$ such that $\\langle A, S \\rangle_F = 0$ for every matrix $S \\in W$. Which of the following correctly describes the subspace $W^\\perp$?\n\nA. The subspace of all symmetric matrices (i.e., matrices $A$ satisfying $A^T = A$).\n\nB. The subspace of all matrices with zero trace (i.e., matrices $A$ satisfying $\\text{tr}(A) = 0$).\n\nC. The subspace of all diagonal matrices.\n\nD. The subspace consisting only of the zero matrix.\n\nE. The subspace of all scalar matrices (i.e., matrices of the form $kI$ for some scalar $k \\in \\mathbb{R}$ and $I$ being the identity matrix).", "solution": "To determine the orthogonal complement $W^\\perp$ of the subspace $W$ of skew-symmetric matrices, we must identify all matrices $A \\in M_{2}(\\mathbb{R})$ that are orthogonal to every matrix $S \\in W$ with respect to the Frobenius inner product.\n\nFirst, let's characterize a general skew-symmetric matrix $S \\in W$. A matrix $S = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{21}  s_{22} \\end{pmatrix}$ is skew-symmetric if $S^T = -S$.\n$$\n\\begin{pmatrix} s_{11}  s_{21} \\\\ s_{12}  s_{22} \\end{pmatrix} = -\\begin{pmatrix} s_{11}  s_{12} \\\\ s_{21}  s_{22} \\end{pmatrix} = \\begin{pmatrix} -s_{11}  -s_{12} \\\\ -s_{21}  -s_{22} \\end{pmatrix}\n$$\nEquating the corresponding entries gives:\n$s_{11} = -s_{11} \\implies 2s_{11} = 0 \\implies s_{11} = 0$\n$s_{22} = -s_{22} \\implies 2s_{22} = 0 \\implies s_{22} = 0$\n$s_{21} = -s_{12}$\nSo, any skew-symmetric matrix $S \\in W$ must be of the form $S = \\begin{pmatrix} 0  s \\\\ -s  0 \\end{pmatrix}$ for some real number $s$.\n\nNext, let's consider a general matrix $A \\in M_{2}(\\mathbb{R})$, which can be written as $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$.\nBy definition, $A \\in W^\\perp$ if and only if $\\langle A, S \\rangle_F = 0$ for all $S \\in W$. Using the definition of the Frobenius inner product, this condition is $\\text{tr}(A^T S) = 0$.\n\nLet's compute the product $A^T S$:\n$$\nA^T S = \\begin{pmatrix} a  c \\\\ b  d \\end{pmatrix} \\begin{pmatrix} 0  s \\\\ -s  0 \\end{pmatrix} = \\begin{pmatrix} (a)(0) + (c)(-s)  (a)(s) + (c)(0) \\\\ (b)(0) + (d)(-s)  (b)(s) + (d)(0) \\end{pmatrix} = \\begin{pmatrix} -cs  as \\\\ -ds  bs \\end{pmatrix}\n$$\n\nNow, we compute the trace of this product matrix:\n$$\n\\text{tr}(A^T S) = -cs + bs = s(b - c)\n$$\n\nThe condition for $A$ to be in $W^\\perp$ is that $\\text{tr}(A^T S) = 0$ for all possible choices of $S \\in W$. This means that $s(b - c) = 0$ must hold for any real number $s$. If we choose any non-zero value for $s$ (e.g., $s=1$), the equation implies that we must have $b - c = 0$, which means $b = c$.\n\nTherefore, any matrix $A \\in W^\\perp$ must have the form:\n$$\nA = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix} = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}\n$$\nwhere $a, b, d$ can be any real numbers. This is precisely the definition of a symmetric matrix, since for such a matrix,\n$$\nA^T = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix}^T = \\begin{pmatrix} a  b \\\\ b  d \\end{pmatrix} = A\n$$\nThus, the orthogonal complement $W^\\perp$ is the subspace of all $2 \\times 2$ symmetric matrices.\n\nComparing this result with the given options:\nA. The subspace of all symmetric matrices ($A^T = A$). This matches our finding.\nB. The subspace of all matrices with zero trace ($\\text{tr}(A) = 0$). This would require $a+d=0$, which is not a necessary condition.\nC. The subspace of all diagonal matrices. This would require $b=c=0$, which is a subset of symmetric matrices but not the entire space.\nD. The subspace consisting only of the zero matrix. This is also a subset but not the entire space.\nE. The subspace of all scalar matrices. This would require $b=c=0$ and $a=d$, which is a smaller subset of symmetric matrices.\n\nThe correct description for $W^\\perp$ is the entire subspace of symmetric matrices.", "answer": "$$\\boxed{A}$$", "id": "1858250"}, {"introduction": "Building on the idea of orthogonal subspaces, this problem asks you to perform a concrete projection. You will calculate the closest symmetric matrix to a given non-symmetric matrix, effectively decomposing it into a component within the subspace of symmetric matrices and a component orthogonal to it [@problem_id:1858280]. This exercise makes the abstract projection theorem tangible by guiding you through the mechanics of finding an orthonormal basis and applying the projection formula.", "problem": "Consider the vector space $V = M_{2 \\times 2}(\\mathbb{R})$ of all real $2 \\times 2$ matrices. This space is equipped with the Frobenius inner product, defined for any two matrices $A, B \\in V$ as $\\langle A, B \\rangle = \\mathrm{tr}(A^T B)$, where $\\mathrm{tr}(\\cdot)$ denotes the trace of a matrix.\n\nLet $W$ be the subspace of $V$ consisting of all symmetric matrices (i.e., matrices $S$ such that $S^T = S$). By the projection theorem, any matrix $M \\in V$ can be uniquely written as a sum $M = M_W + M_{W^\\perp}$, where $M_W$ is the orthogonal projection of $M$ onto the subspace $W$, and $M_{W^\\perp}$ is the component in the orthogonal complement of $W$.\n\nGiven the matrix $A = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$, determine its orthogonal projection $A_W$ onto the subspace $W$.", "solution": "The problem asks for the orthogonal projection of the matrix $A = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$ onto the subspace $W$ of symmetric $2 \\times 2$ matrices, within the inner product space $V = M_{2 \\times 2}(\\mathbb{R})$ with the Frobenius inner product $\\langle A, B \\rangle = \\mathrm{tr}(A^T B)$.\n\nThe orthogonal projection $A_W$ of $A$ onto $W$ is given by the formula $A_W = \\sum_{i=1}^{k} \\langle A, u_i \\rangle u_i$, where $\\{u_1, u_2, \\dots, u_k\\}$ is an orthonormal basis for the subspace $W$.\n\nFirst, we need to find an orthonormal basis for $W$. A general symmetric $2 \\times 2$ matrix has the form $S = \\begin{pmatrix} a  b \\\\ b  c \\end{pmatrix}$, where $a, b, c \\in \\mathbb{R}$. We can write this as a linear combination of basis matrices:\n$S = a \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + c \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + b \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$.\nSo, a basis for $W$ is given by the set $\\{B_1, B_2, B_3\\}$, where\n$B_1 = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$, $B_2 = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}$, $B_3 = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$.\n\nNext, we check if this basis is orthogonal by computing the inner products between distinct basis vectors.\n$\\langle B_1, B_2 \\rangle = \\mathrm{tr}(B_1^T B_2) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}\\right) = 0$.\n$\\langle B_1, B_3 \\rangle = \\mathrm{tr}(B_1^T B_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}\\right) = 0$.\n$\\langle B_2, B_3 \\rangle = \\mathrm{tr}(B_2^T B_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}\\right) = 0$.\nThe basis is indeed orthogonal. Now we find the norm of each basis vector to normalize them. The squared norm is $\\|B\\|^2 = \\langle B, B \\rangle$.\n$\\|B_1\\|^2 = \\langle B_1, B_1 \\rangle = \\mathrm{tr}(B_1^T B_1) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}^2\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\\right) = 1$. So, $\\|B_1\\|=1$.\n$\\|B_2\\|^2 = \\langle B_2, B_2 \\rangle = \\mathrm{tr}(B_2^T B_2) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}^2\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}\\right) = 1$. So, $\\|B_2\\|=1$.\n$\\|B_3\\|^2 = \\langle B_3, B_3 \\rangle = \\mathrm{tr}(B_3^T B_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}^2\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\right) = 2$. So, $\\|B_3\\|=\\sqrt{2}$.\n\nAn orthonormal basis $\\{u_1, u_2, u_3\\}$ for $W$ is therefore:\n$u_1 = \\frac{B_1}{\\|B_1\\|} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\n$u_2 = \\frac{B_2}{\\|B_2\\|} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}$.\n$u_3 = \\frac{B_3}{\\|B_3\\|} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$.\n\nNow we compute the inner products of $A = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$ with these orthonormal basis vectors.\n$\\langle A, u_1 \\rangle = \\mathrm{tr}(A^T u_1) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  3 \\\\ 2  4 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  0 \\\\ 2  0 \\end{pmatrix}\\right) = 1$.\n$\\langle A, u_2 \\rangle = \\mathrm{tr}(A^T u_2) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  3 \\\\ 2  4 \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}\\right) = \\mathrm{tr}\\left(\\begin{pmatrix} 0  3 \\\\ 0  4 \\end{pmatrix}\\right) = 4$.\n$\\langle A, u_3 \\rangle = \\mathrm{tr}(A^T u_3) = \\mathrm{tr}\\left(\\begin{pmatrix} 1  3 \\\\ 2  4 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\right) = \\frac{1}{\\sqrt{2}}\\mathrm{tr}\\left(\\begin{pmatrix} 3  1 \\\\ 4  2 \\end{pmatrix}\\right) = \\frac{3+2}{\\sqrt{2}} = \\frac{5}{\\sqrt{2}}$.\n\nFinally, we construct the projection $A_W$:\n$A_W = \\langle A, u_1 \\rangle u_1 + \\langle A, u_2 \\rangle u_2 + \\langle A, u_3 \\rangle u_3$\n$A_W = (1) \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + (4) \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + \\left(\\frac{5}{\\sqrt{2}}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\right)$\n$A_W = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} + \\frac{5}{2}\\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$\n$A_W = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} + \\begin{pmatrix} 0  \\frac{5}{2} \\\\ \\frac{5}{2}  0 \\end{pmatrix}$\n$A_W = \\begin{pmatrix} 1  \\frac{5}{2} \\\\ \\frac{5}{2}  4 \\end{pmatrix}$.\n\nThis is the component of $A$ that lies in the subspace of symmetric matrices.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 1  \\frac{5}{2} \\\\ \\frac{5}{2}  4 \\end{pmatrix}\n}\n$$", "id": "1858280"}, {"introduction": "This final practice extends the concept of orthogonality to function spaces, a cornerstone of functional analysis. You will work within a space of polynomials equipped with an $L^2$ inner product, which is defined by an integral [@problem_id:1858246]. The task is to find a basis for the orthogonal complement of a given subspace, a process that requires setting up and solving equations derived from the inner product definition and demonstrating the versatility of projection concepts.", "problem": "Let $P_2([0,1])$ be the vector space of all polynomials of degree at most 2 with real coefficients, defined on the interval $[0,1]$. This space is equipped with the standard $L^2$ inner product, defined for any two polynomials $f(x)$ and $g(x)$ in $P_2([0,1])$ as:\n$$\n\\langle f, g \\rangle = \\int_0^1 f(x)g(x) \\,dx\n$$\nConsider the subspace $W$ of $P_2([0,1])$ consisting of all polynomials $p(x)$ such that $p(0) = 0$.\nThe orthogonal complement of $W$, denoted by $W^\\perp$, is the set of all polynomials in $P_2([0,1])$ that are orthogonal to every polynomial in $W$. Find the specific polynomial $q(x)$ that forms a basis for $W^\\perp$, subject to the condition that its coefficients are all integers with a greatest common divisor of 1, and its highest-degree non-zero coefficient (the leading coefficient) is positive.", "solution": "Let $P_2([0,1])$ be equipped with the inner product $\\langle f,g\\rangle=\\int_{0}^{1}f(x)g(x)\\,dx$. The subspace $W=\\{p\\in P_2([0,1]) : p(0)=0\\}$ consists of polynomials of the form $p(x)=ax + bx^2$, so $\\dim(W)=2$. Since $\\dim(P_2)=3$, the orthogonal complement $W^\\perp$ has dimension $1$.\n\nLet $q(x)=c_0+c_1x+c_2x^2 \\in W^\\perp$. Orthogonality to a basis of $W$, namely $\\{x, x^2\\}$, gives the conditions\n$$\n\\langle q,x\\rangle=\\int_{0}^{1}(c_0+c_1x+c_2x^2)x\\,dx=c_0\\int_{0}^{1}x\\,dx+c_1\\int_{0}^{1}x^2\\,dx+c_2\\int_{0}^{1}x^3\\,dx=0,\n$$\n$$\n\\langle q,x^2\\rangle=\\int_{0}^{1}(c_0+c_1x+c_2x^2)x^2\\,dx=c_0\\int_{0}^{1}x^2\\,dx+c_1\\int_{0}^{1}x^3\\,dx+c_2\\int_{0}^{1}x^4\\,dx=0.\n$$\nEvaluating the integrals yields the linear system\n$$\n\\frac{1}{2}c_0+\\frac{1}{3}c_1+\\frac{1}{4}c_2=0,\\qquad \\frac{1}{3}c_0+\\frac{1}{4}c_1+\\frac{1}{5}c_2=0.\n$$\nMultiplying by $60$ to clear denominators gives\n$$\n30c_0+20c_1+15c_2=0,\\qquad 20c_0+15c_1+12c_2=0.\n$$\nSubtracting $\\frac{3}{2}$ times the second equation from the first yields\n$$\n\\left(30c_0+20c_1+15c_2\\right)-\\frac{3}{2}\\left(20c_0+15c_1+12c_2\\right)=0 \\implies -\\frac{5}{2}c_1-3c_2=0,\n$$\nso $5c_1+6c_2=0$, hence $c_1=-(6/5)c_2$. Substituting into $30c_0+20c_1+15c_2=0$ gives\n$$\n30c_0+20\\left(-\\frac{6}{5}c_2\\right)+15c_2=0 \\implies 30c_0-24c_2+15c_2=0 \\implies 30c_0=9c_2 \\implies c_0=\\frac{3}{10}c_2.\n$$\nThus a general nonzero solution has proportional coefficients $(c_0, c_1, c_2)=\\left(\\frac{3}{10}c_2, -\\frac{6}{5}c_2, c_2\\right)$. Choosing $c_2=10$ yields integer coefficients $(3,-12,10)$ with greatest common divisor $1$ and positive leading coefficient. Therefore,\n$$\nq(x)=10x^2-12x+3\n$$\nspans $W^\\perp$. Verification:\n$$\n\\int_{0}^{1}(10x^2-12x+3)x\\,dx=\\left[\\frac{3x^2}{2}-4x^3+\\frac{10x^4}{4}\\right]_0^1 = \\frac{3}{2}-4+\\frac{5}{2}=0,\n$$\n$$\n\\int_{0}^{1}(10x^2-12x+3)x^2\\,dx=\\left[\\frac{3x^3}{3}- \\frac{12x^4}{4}+\\frac{10x^5}{5}\\right]_0^1=1-3+2=0,\n$$\nso $q(x)$ is orthogonal to both $x$ and $x^2$, and hence to all of $W$.", "answer": "$$\\boxed{10x^2-12x+3}$$", "id": "1858246"}]}