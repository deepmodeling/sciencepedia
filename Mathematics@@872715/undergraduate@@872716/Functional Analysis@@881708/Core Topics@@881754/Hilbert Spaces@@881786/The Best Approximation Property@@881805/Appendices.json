{"hands_on_practices": [{"introduction": "The best approximation theorem provides a powerful geometric framework for approximation problems. It states that in an inner product space, the best approximation to a vector $f$ from a subspace $W$ is its orthogonal projection onto $W$. This exercise, [@problem_id:1886631], provides a concrete starting point by asking you to find the best approximation of the function $f(t)=t^2$ from a simple one-dimensional subspace, making the geometric concept of orthogonality easy to visualize and compute.", "problem": "Consider the vector space of real-valued functions that are square-integrable on the interval $[0, 1]$, denoted as $L^2[0,1]$. This space is equipped with an inner product defined for any two functions $f$ and $g$ as $\\langle f, g \\rangle = \\int_0^1 f(t)g(t) \\, dt$. The distance between two functions $f$ and $g$ in this space is given by the norm of their difference, $\\|f-g\\| = \\sqrt{\\langle f-g, f-g \\rangle}$.\n\nLet $W$ be the subspace of $L^2[0,1]$ consisting of all linear polynomials that pass through the origin. That is, any polynomial $p(t)$ in $W$ has the form $p(t)=at+b$ and must satisfy the condition $p(0) = 0$.\n\nYour task is to find the specific function $p^*(t)$ within the subspace $W$ that serves as the best approximation to the function $f(t) = t^2$. The best approximation is defined as the function $p^*(t) \\in W$ that minimizes the distance $\\|f(t) - p(t)\\|$ for all possible choices of $p(t)$ in $W$.\n\nExpress your answer for $p^*(t)$ as a polynomial in the variable $t$.", "solution": "We work in the Hilbert space $L^{2}[0,1]$ with inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(t)g(t) \\, dt$. The subspace $W$ consists of all linear polynomials through the origin, so $W = \\{p(t) = a t : a \\in \\mathbb{R}\\} = \\operatorname{span}\\{t\\}$.\n\nIn an inner product space, the best approximation $p^{*} \\in W$ to $f$ is the orthogonal projection of $f$ onto $W$. Therefore, the residual $f - p^{*}$ is orthogonal to $W$. Writing $p^{*}(t) = a t$, the orthogonality condition is\n$$\n\\langle f - a t, t \\rangle = 0.\n$$\nHere $f(t) = t^{2}$, hence\n$$\n\\langle t^{2} - a t, t \\rangle = \\int_{0}^{1} \\left(t^{2} - a t\\right) t \\, dt = \\int_{0}^{1} t^{3} \\, dt - a \\int_{0}^{1} t^{2} \\, dt = 0.\n$$\nEvaluating the integrals,\n$$\n\\int_{0}^{1} t^{3} \\, dt = \\left[\\frac{t^{4}}{4}\\right]_{0}^{1} = \\frac{1}{4}, \\qquad \\int_{0}^{1} t^{2} \\, dt = \\left[\\frac{t^{3}}{3}\\right]_{0}^{1} = \\frac{1}{3}.\n$$\nThus,\n$$\n\\frac{1}{4} - a \\cdot \\frac{1}{3} = 0 \\quad \\Rightarrow \\quad a = \\frac{\\frac{1}{4}}{\\frac{1}{3}} = \\frac{3}{4}.\n$$\nTherefore, the best approximation is\n$$\np^{*}(t) = \\frac{3}{4}\\, t.\n$$", "answer": "$$\\boxed{\\frac{3}{4} t}$$", "id": "1886631"}, {"introduction": "The choice of norm is a critical decision in any approximation problem, as it defines what we mean by 'best'. While the $L^2$ norm is mathematically convenient due to its connection to inner products, the $L^1$ norm offers a robust alternative that is less sensitive to outliers. In this practice problem, [@problem_id:1886646], you will directly compare these two approaches by finding the best constant approximation to a function, revealing how changing the norm fundamentally changes the solution.", "problem": "In the study of approximation theory, a central problem is to find the \"best\" simple approximation to a more complex function. The notion of \"best\" depends on the norm used to measure the error.\n\nConsider the vector space $C[0,1]$ of continuous real-valued functions on the interval $[0,1]$. We wish to approximate the function $f(x) = x^2$ using a constant function $g(x) = c$.\n\nLet $c_1$ be the unique real constant that provides the best approximation to $f(x)$ in the $L^1$ sense. This means $c_1$ is the value of $c$ that minimizes the $L^1$ error, defined as:\n$$E_1(c) = \\int_{0}^{1} |f(x) - c| \\, dx$$\n\nLet $c_2$ be the unique real constant that provides the best approximation to $f(x)$ in the $L^2$ sense. This means $c_2$ is the value of $c$ that minimizes the $L^2$ error, defined as the square of the $L^2$ norm of the difference:\n$$E_2(c) = \\int_{0}^{1} (f(x) - c)^2 \\, dx$$\n\nDetermine the exact value of the ratio $\\frac{c_1}{c_2}$. Express your answer as a fraction in simplest form.", "solution": "We seek constants $c_{1}$ and $c_{2}$ minimizing the $L^{1}$ and $L^{2}$ errors for approximating $f(x)=x^{2}$ on $[0,1]$ by the constant function $g(x)=c$.\n\nFor the $L^{2}$ case, define\n$$\nE_{2}(c)=\\int_{0}^{1}(x^{2}-c)^{2}\\,dx=\\int_{0}^{1}\\left(x^{4}-2cx^{2}+c^{2}\\right)\\,dx.\n$$\nEvaluating the integrals gives\n$$\nE_{2}(c)=\\int_{0}^{1}x^{4}\\,dx-2c\\int_{0}^{1}x^{2}\\,dx+c^{2}\\int_{0}^{1}1\\,dx=\\frac{1}{5}-\\frac{2}{3}c+c^{2}.\n$$\nDifferentiate with respect to $c$ and set to zero:\n$$\n\\frac{dE_{2}}{dc}=-\\frac{2}{3}+2c=0 \\quad\\Longrightarrow\\quad c_{2}=\\frac{1}{3}.\n$$\nSince $E_{2}$ is a strictly convex quadratic in $c$, this critical point is the unique minimizer.\n\nFor the $L^{1}$ case, define\n$$\nE_{1}(c)=\\int_{0}^{1}|x^{2}-c|\\,dx.\n$$\nFor any $c$ at which $x^{2}\\neq c$ almost everywhere, the derivative is the integral of the pointwise derivative of $|x^{2}-c|$ with respect to $c$, namely\n$$\n\\frac{dE_{1}}{dc}=\\int_{0}^{1}\\operatorname{sgn}(c-x^{2})\\,dx=\\lambda\\{x\\in[0,1]:x^{2}c\\}-\\lambda\\{x\\in[0,1]:x^{2}c\\},\n$$\nwhere $\\lambda$ denotes Lebesgue measure. Since the total measure is $1$, this simplifies to\n$$\n\\frac{dE_{1}}{dc}=2\\,\\lambda\\{x\\in[0,1]:x^{2}c\\}-1.\n$$\nOn $[0,1]$, the condition $x^{2}c$ is equivalent to $x\\sqrt{c}$ for $c\\in[0,1]$, so\n$$\n\\frac{dE_{1}}{dc}=\n\\begin{cases}\n-1,  c0,\\\\\n2\\sqrt{c}-1,  0c1,\\\\\n1,  c1.\n\\end{cases}\n$$\nThus $E_{1}$ is strictly decreasing for $c0$, strictly increasing for $c1$, and in $(0,1)$ its critical point satisfies\n$$\n2\\sqrt{c}-1=0 \\quad\\Longrightarrow\\quad \\sqrt{c}=\\frac{1}{2}\\quad\\Longrightarrow\\quad c_{1}=\\frac{1}{4}.\n$$\nConvexity of $E_{1}$ in $c$ (as an integral of convex functions) ensures this critical point is the unique minimizer.\n\nTherefore,\n$$\n\\frac{c_{1}}{c_{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{3}}=\\frac{3}{4}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1886646"}, {"introduction": "The powerful orthogonal projection theorem guarantees a unique best approximation when the set of approximating functions is a closed, convex set, such as a subspace. But what happens when the set is not convex? This problem, [@problem_id:1886683], challenges you to find the best approximation of a function from a non-convex set, forcing a return to the fundamental definition of minimizing the distance and demonstrating a more general problem-solving approach.", "problem": "In the Hilbert space $L^2[0,1]$ of square-integrable real-valued functions on the unit interval, the squared distance between two functions $f$ and $g$ is given by $\\|f - g\\|^2 = \\int_0^1 (f(t) - g(t))^2 dt$.\n\nConsider the specific function $f(t) = t$. We aim to find its best approximation from a particular set $S$ of functions. The set $S$ comprises all characteristic functions of closed subintervals of $[0,1]$. A generic function $g_{a,b} \\in S$ is defined by parameters $a$ and $b$ satisfying $0 \\le a \\le b \\le 1$, such that $g_{a,b}(t) = \\mathbb{1}_{[a,b]}(t)$. The characteristic function $\\mathbb{1}_{[a,b]}(t)$ is equal to 1 for $t \\in [a,b]$ and 0 otherwise.\n\nYour task is to find the function in $S$ that is closest to $f(t)$. This is achieved by determining the values of the parameters $a$ and $b$ that minimize the squared distance $D(a,b) = \\|f - g_{a,b}\\|^2$.\n\nDetermine the exact rational values for the optimal starting point $a$, the optimal endpoint $b$, and the resulting minimum squared distance. The answer should be presented in the order `(a, b, \\text{minimum squared distance})`.", "solution": "We must minimize the squared distance\n$$\nD(a,b)=\\int_{0}^{1}\\left(t-\\mathbb{1}_{[a,b]}(t)\\right)^{2}\\,dt,\n$$\nwith $0\\leq a\\leq b\\leq 1$ and $f(t)=t$. Split the integral over $[0,a)$, $[a,b]$, and $(b,1]$:\n$$\nD(a,b)=\\int_{0}^{a}t^{2}\\,dt+\\int_{a}^{b}(t-1)^{2}\\,dt+\\int_{b}^{1}t^{2}\\,dt.\n$$\nCombine the first and third terms as $\\int_{0}^{1}t^{2}\\,dt$ and correct over $[a,b]$:\n$$\nD(a,b)=\\int_{0}^{1}t^{2}\\,dt+\\int_{a}^{b}\\left[(t-1)^{2}-t^{2}\\right]\\,dt.\n$$\nEvaluate each piece:\n$$\n\\int_{0}^{1}t^{2}\\,dt=\\frac{1}{3},\\qquad (t-1)^{2}-t^{2}=-2t+1,\n$$\nso\n$$\nD(a,b)=\\frac{1}{3}+\\int_{a}^{b}(-2t+1)\\,dt=\\frac{1}{3}+\\left[-t^{2}+t\\right]_{a}^{b}=\\frac{1}{3}-b^{2}+b+a^{2}-a.\n$$\nFor fixed $a$, regard $D$ as a function of $b\\in[a,1]$:\n$$\nD(a,b)=\\left(\\frac{1}{3}+a^{2}-a\\right)+(-b^{2}+b).\n$$\nThe term $-b^{2}+b$ is a concave quadratic in $b$, hence its minimum over the interval $[a,1]$ occurs at an endpoint. Evaluate at $b=a$ and $b=1$:\n$$\n-b^{2}+b\\big|_{b=a}=-a^{2}+a,\\qquad -b^{2}+b\\big|_{b=1}=0.\n$$\nSince $-a^{2}+a=a(1-a)\\geq 0$ for $a\\in[0,1]$, the minimum is $0$, attained at $b=1$ (also at $b=a$ only when $a\\in\\{0,1\\}$). Therefore, for any fixed $a$, the optimal choice is $b=1$, and the problem reduces to minimizing\n$$\nF(a)=D(a,1)=\\frac{1}{3}+a^{2}-a,\\qquad 0\\leq a\\leq 1.\n$$\nDifferentiate and set to zero:\n$$\nF'(a)=2a-1=0\\quad\\Rightarrow\\quad a=\\frac{1}{2},\\qquad F''(a)=20,\n$$\nso $a=\\frac{1}{2}$ is the unique minimizer on $[0,1]$. The minimum squared distance is\n$$\nF\\!\\left(\\frac{1}{2}\\right)=\\frac{1}{3}+\\frac{1}{4}-\\frac{1}{2}=\\frac{1}{12}.\n$$\nThus, the optimal interval is $[a,b]=\\left[\\frac{1}{2},1\\right]$, and the minimum squared distance is $\\frac{1}{12}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  1  \\frac{1}{12}\\end{pmatrix}}$$", "id": "1886683"}]}