{"hands_on_practices": [{"introduction": "The concept of an orthonormal basis is a direct generalization of the familiar Cartesian coordinate axes in three-dimensional space. This exercise provides a concrete opportunity to translate the geometric ideas of perpendicularity and unit length into the algebraic language of inner products. By starting with two orthonormal vectors in $\\mathbb{R}^3$, you will use the definitions of orthogonality and normalization to find the vector that completes the basis, reinforcing these fundamental concepts in a tangible setting [@problem_id:1874283].", "problem": "Consider the three-dimensional Euclidean space $\\mathbb{R}^3$, equipped with the standard dot product. Let two vectors, $v_1$ and $v_2$, be defined as:\n$$ v_1 = \\left( \\frac{1}{3}, \\frac{2}{3}, \\frac{2}{3} \\right) $$\n$$ v_2 = \\left( \\frac{2}{\\sqrt{5}}, -\\frac{1}{\\sqrt{5}}, 0 \\right) $$\nThese two vectors form part of an orthonormal set. Find the vector $v_3 = (x, y, z)$ that completes the set $\\{v_1, v_2, v_3\\}$ to form an orthonormal basis for $\\mathbb{R}^3$. To ensure a unique solution, it is required that the $z$-component of the vector $v_3$ be negative.", "solution": "The problem asks us to find a third vector, $v_3$, that completes an orthonormal basis for $\\mathbb{R}^3$ given two orthonormal vectors, $v_1$ and $v_2$.\n\nFirst, let's recall the definition of an orthonormal basis. A set of vectors $\\{v_1, v_2, ..., v_n\\}$ in an inner product space is orthonormal if the inner product of any two distinct vectors is zero (orthogonality) and the norm (length) of each vector is one (normalization). That is, $v_i \\cdot v_j = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nFor our problem, the set $\\{v_1, v_2, v_3\\}$ must satisfy:\n1. $v_1 \\cdot v_2 = 0$, $v_1 \\cdot v_3 = 0$, $v_2 \\cdot v_3 = 0$\n2. $||v_1|| = 1$, $||v_2|| = 1$, $||v_3|| = 1$\n\nLet's first verify that the given vectors $v_1$ and $v_2$ are indeed orthonormal.\nOrthogonality check:\n$$ v_1 \\cdot v_2 = \\left(\\frac{1}{3}\\right)\\left(\\frac{2}{\\sqrt{5}}\\right) + \\left(\\frac{2}{3}\\right)\\left(-\\frac{1}{\\sqrt{5}}\\right) + \\left(\\frac{2}{3}\\right)(0) = \\frac{2}{3\\sqrt{5}} - \\frac{2}{3\\sqrt{5}} + 0 = 0 $$\nSo, $v_1$ and $v_2$ are orthogonal.\n\nNormalization check for $v_1$:\n$$ ||v_1||^2 = v_1 \\cdot v_1 = \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{2}{3}\\right)^2 = \\frac{1}{9} + \\frac{4}{9} + \\frac{4}{9} = \\frac{9}{9} = 1 $$\nSo, $||v_1|| = 1$.\n\nNormalization check for $v_2$:\n$$ ||v_2||^2 = v_2 \\cdot v_2 = \\left(\\frac{2}{\\sqrt{5}}\\right)^2 + \\left(-\\frac{1}{\\sqrt{5}}\\right)^2 + (0)^2 = \\frac{4}{5} + \\frac{1}{5} + 0 = \\frac{5}{5} = 1 $$\nSo, $||v_2|| = 1$.\nThe given vectors are indeed orthonormal.\n\nNow we need to find a vector $v_3$ that is orthogonal to both $v_1$ and $v_2$. In $\\mathbb{R}^3$, a vector that is orthogonal to two other vectors is parallel to their cross product. Let's compute a candidate vector $v_3'$ using the cross product: $v_3' = v_1 \\times v_2$.\n$$ v_3' = \\begin{vmatrix} \\mathbf{i}  \\mathbf{j}  \\mathbf{k} \\\\ \\frac{1}{3}  \\frac{2}{3}  \\frac{2}{3} \\\\ \\frac{2}{\\sqrt{5}}  -\\frac{1}{\\sqrt{5}}  0 \\end{vmatrix} $$\nExpanding the determinant:\n$$ v_3' = \\mathbf{i} \\left( \\left(\\frac{2}{3}\\right)(0) - \\left(\\frac{2}{3}\\right)\\left(-\\frac{1}{\\sqrt{5}}\\right) \\right) - \\mathbf{j} \\left( \\left(\\frac{1}{3}\\right)(0) - \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{\\sqrt{5}}\\right) \\right) + \\mathbf{k} \\left( \\left(\\frac{1}{3}\\right)\\left(-\\frac{1}{\\sqrt{5}}\\right) - \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{\\sqrt{5}}\\right) \\right) $$\n$$ v_3' = \\mathbf{i} \\left( \\frac{2}{3\\sqrt{5}} \\right) - \\mathbf{j} \\left( -\\frac{4}{3\\sqrt{5}} \\right) + \\mathbf{k} \\left( -\\frac{1}{3\\sqrt{5}} - \\frac{4}{3\\sqrt{5}} \\right) $$\n$$ v_3' = \\left( \\frac{2}{3\\sqrt{5}}, \\frac{4}{3\\sqrt{5}}, -\\frac{5}{3\\sqrt{5}} \\right) $$\nBy construction, $v_3'$ is orthogonal to both $v_1$ and $v_2$. The final step is to ensure it has a norm of 1. A general property states that if $v_1$ and $v_2$ are orthogonal unit vectors, then their cross product $v_1 \\times v_2$ is also a unit vector. Let's verify this by calculating the norm of $v_3'$.\n$$ ||v_3'||^2 = \\left(\\frac{2}{3\\sqrt{5}}\\right)^2 + \\left(\\frac{4}{3\\sqrt{5}}\\right)^2 + \\left(-\\frac{5}{3\\sqrt{5}}\\right)^2 $$\n$$ ||v_3'||^2 = \\frac{4}{9 \\times 5} + \\frac{16}{9 \\times 5} + \\frac{25}{9 \\times 5} = \\frac{4+16+25}{45} = \\frac{45}{45} = 1 $$\nSince $||v_3'|| = 1$, the vector $v_3'$ is already normalized. So we can take $v_3 = v_3'$.\n\nThe problem requires the $z$-component of $v_3$ to be negative. The z-component of our calculated vector is $-\\frac{5}{3\\sqrt{5}}$, which is indeed negative. Thus, this is the unique vector satisfying the conditions. The other possibility, $-v_3'$, would have a positive $z$-component.\n\nSo, the required vector is:\n$$ v_3 = \\left( \\frac{2}{3\\sqrt{5}}, \\frac{4}{3\\sqrt{5}}, -\\frac{5}{3\\sqrt{5}} \\right) $$\nTo present the answer with rational denominators, we multiply the numerator and denominator of each component by $\\sqrt{5}$:\n$$ x = \\frac{2\\sqrt{5}}{3\\sqrt{5}\\sqrt{5}} = \\frac{2\\sqrt{5}}{15} $$\n$$ y = \\frac{4\\sqrt{5}}{3\\sqrt{5}\\sqrt{5}} = \\frac{4\\sqrt{5}}{15} $$\n$$ z = -\\frac{5\\sqrt{5}}{3\\sqrt{5}\\sqrt{5}} = -\\frac{5\\sqrt{5}}{15} = -\\frac{\\sqrt{5}}{3} $$\nTherefore, the final vector is $v_3 = \\left( \\frac{2\\sqrt{5}}{15}, \\frac{4\\sqrt{5}}{15}, -\\frac{\\sqrt{5}}{3} \\right)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2\\sqrt{5}}{15}  \\frac{4\\sqrt{5}}{15}  -\\frac{\\sqrt{5}}{3} \\end{pmatrix}}$$", "id": "1874283"}, {"introduction": "While knowing what an orthonormal set is is important, we also need a systematic method to construct one from a given set of vectors. The Gram-Schmidt process provides just such an algorithm, allowing us to convert a linearly independent set into an orthogonal one that spans the same subspace. This problem explores a crucial nuance by asking what happens when the initial set of vectors is linearly dependent, revealing the deep connection between the algorithm, orthogonality, and linear independence [@problem_id:1874299].", "problem": "Consider the set of three vectors $S = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$ in the Euclidean space $\\mathbb{R}^3$, where the vectors are defined as $\\mathbf{v}_1 = (1, 1, 0)$, $\\mathbf{v}_2 = (1, 0, 1)$, and $\\mathbf{v}_3 = (0, 1, -1)$. Apply the Gram-Schmidt orthogonalization process to this set of vectors, taken in the given order, to generate an orthogonal set $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\mathbf{u}_3\\}$. What is the final vector, $\\mathbf{u}_3$, in the resulting orthogonal set? Express your answer as a row vector.", "solution": "We use the standard Euclidean inner product. The Gram-Schmidt process constructs an orthogonal set $\\{\\mathbf{u}_{1},\\mathbf{u}_{2},\\mathbf{u}_{3}\\}$ from $\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\mathbf{v}_{3}\\}$ in the given order by\n$$\n\\mathbf{u}_{1}=\\mathbf{v}_{1},\\quad\n\\mathbf{u}_{2}=\\mathbf{v}_{2}-\\frac{\\mathbf{v}_{2}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}\\mathbf{u}_{1},\\quad\n\\mathbf{u}_{3}=\\mathbf{v}_{3}-\\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}\\mathbf{u}_{1}-\\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{2}}{\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}}\\mathbf{u}_{2}.\n$$\nGiven $\\mathbf{v}_{1}=(1,1,0)$, we have\n$$\n\\mathbf{u}_{1}=\\mathbf{v}_{1}=(1,1,0),\\quad \\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}=1^{2}+1^{2}+0^{2}=2.\n$$\nNext,\n$$\n\\mathbf{v}_{2}\\cdot\\mathbf{u}_{1}=(1,0,1)\\cdot(1,1,0)=1,\\quad\n\\frac{\\mathbf{v}_{2}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}=\\frac{1}{2},\n$$\nso\n$$\n\\mathbf{u}_{2}=\\mathbf{v}_{2}-\\frac{1}{2}\\mathbf{u}_{1}=(1,0,1)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)=\\left(\\frac{1}{2},-\\frac{1}{2},1\\right).\n$$\nCompute the quantities needed for $\\mathbf{u}_{3}$:\n$$\n\\mathbf{v}_{3}\\cdot\\mathbf{u}_{1}=(0,1,-1)\\cdot(1,1,0)=1,\\quad \\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{1}}{\\mathbf{u}_{1}\\cdot\\mathbf{u}_{1}}=\\frac{1}{2},\n$$\n$$\n\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}=\\left(\\frac{1}{2}\\right)^{2}+\\left(-\\frac{1}{2}\\right)^{2}+1^{2}=\\frac{1}{4}+\\frac{1}{4}+1=\\frac{3}{2},\n$$\n$$\n\\mathbf{v}_{3}\\cdot\\mathbf{u}_{2}=(0,1,-1)\\cdot\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)=0\\cdot\\frac{1}{2}+1\\cdot\\left(-\\frac{1}{2}\\right)+(-1)\\cdot 1=-\\frac{3}{2},\n$$\n$$\n\\frac{\\mathbf{v}_{3}\\cdot\\mathbf{u}_{2}}{\\mathbf{u}_{2}\\cdot\\mathbf{u}_{2}}=\\frac{-\\frac{3}{2}}{\\frac{3}{2}}=-1.\n$$\nTherefore,\n$$\n\\mathbf{u}_{3}=\\mathbf{v}_{3}-\\frac{1}{2}\\mathbf{u}_{1}-(-1)\\mathbf{u}_{2}\n=\\mathbf{v}_{3}-\\frac{1}{2}\\mathbf{u}_{1}+\\mathbf{u}_{2}.\n$$\nSubstituting the vectors,\n$$\n\\mathbf{u}_{3}=(0,1,-1)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)+\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)\n=\\left(0,0,0\\right).\n$$\nThus the final vector in the orthogonal set is the zero vector, reflecting the linear dependence $\\mathbf{v}_{3}=\\mathbf{v}_{1}-\\mathbf{v}_{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}0  0  0\\end{pmatrix}}$$", "id": "1874299"}, {"introduction": "The principles of orthogonality and projection extend powerfully from finite-dimensional vectors to infinite-dimensional function spaces. This practice moves us into the space of square-integrable functions, $L^2[0, 1]$, to explore one of the most fundamental applications of these ideas: approximation. By projecting a function onto a simple subspace, we are essentially finding its \"best fit\" within that subspace, a core concept that underpins Fourier analysis and signal processing [@problem_id:1874314].", "problem": "Consider the real inner product space $L^2[0, 1]$, which consists of all real-valued square-integrable functions on the interval $[0, 1]$. The inner product for any two functions $f$ and $g$ in this space is defined as:\n$$\n\\langle f, g \\rangle = \\int_0^1 f(x) g(x) \\, dx\n$$\nLet $W$ be the one-dimensional subspace of $L^2[0, 1]$ spanned by the function $u(x) = 1$. This subspace represents the set of all constant functions on $[0, 1]$.\n\nDetermine the orthogonal projection of the function $f(x) = \\exp(x)$ onto the subspace $W$. Your answer should be the expression that defines the resulting projected function.", "solution": "We project $f$ onto the one-dimensional subspace $W=\\operatorname{span}\\{u\\}$ with $u(x)=1$ using the orthogonal projection formula onto a span of a single vector:\n$$\nP_{W}f=\\frac{\\langle f,u\\rangle}{\\langle u,u\\rangle}\\,u.\n$$\nHere $f(x)=\\exp(x)$ and $u(x)=1$. Compute the inner products:\n$$\n\\langle f,u\\rangle=\\int_{0}^{1}\\exp(x)\\cdot 1\\,dx=\\int_{0}^{1}\\exp(x)\\,dx=\\left[\\exp(x)\\right]_{0}^{1}=\\exp(1)-\\exp(0)=\\exp(1)-1,\n$$\n$$\n\\langle u,u\\rangle=\\int_{0}^{1}1\\cdot 1\\,dx=\\int_{0}^{1}1\\,dx=\\left[x\\right]_{0}^{1}=1.\n$$\nTherefore the projection coefficient is\n$$\n\\frac{\\langle f,u\\rangle}{\\langle u,u\\rangle}=\\exp(1)-1,\n$$\nand the projected function is\n$$\nP_{W}f(x)=\\left(\\exp(1)-1\\right)u(x)=\\exp(1)-1.\n$$\nThus the orthogonal projection is the constant function equal to $\\exp(1)-1$ on $[0,1]$.", "answer": "$$\\boxed{\\exp(1)-1}$$", "id": "1874314"}]}