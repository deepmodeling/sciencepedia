{"hands_on_practices": [{"introduction": "The abstract definition of conditional expectation can be challenging to grasp without a concrete example. This exercise grounds the theory in a finite, discrete setting, allowing you to directly compute and compare the quantities in Jensen's inequality. By working through this simple model with the convex function $\\phi(x) = \\exp(x)$, you will verify the inequality $E[\\phi(X)|\\mathcal{G}] \\ge \\phi(E[X|\\mathcal{G}])$ step-by-step and build a solid intuition for how it operates.", "problem": "Consider a simple probabilistic model for a system with a finite number of states. The sample space is $\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4\\}$. The probability measure $P$ on the power set of $\\Omega$ is defined by the probabilities of the elementary events: $P(\\{\\omega_1\\}) = \\frac{1}{8}$, $P(\\{\\omega_2\\}) = \\frac{1}{8}$, $P(\\{\\omega_3\\}) = \\frac{1}{4}$, and $P(\\{\\omega_4\\}) = \\frac{1}{2}$.\n\nA real-valued random variable $X$ is defined on this space as follows: $X(\\omega_1) = 1$, $X(\\omega_2) = 2$, $X(\\omega_3) = 3$, and $X(\\omega_4) = 4$.\n\nInformation about the system is coarse-grained, and we only observe which of two disjoint events has occurred: $A_1 = \\{\\omega_1, \\omega_2\\}$ or $A_2 = \\{\\omega_3, \\omega_4\\}$. This observation structure is captured by the sub-$\\sigma$-algebra $\\mathcal{G} = \\{\\emptyset, A_1, A_2, \\Omega\\}$, which is generated by the partition $\\{A_1, A_2\\}$.\n\nWe are interested in two derived random variables. The first is $Y_1 = E[e^X|\\mathcal{G}]$, which is the conditional expectation of $e^X$ given $\\mathcal{G}$. The second is $Y_2 = e^{E[X|\\mathcal{G}]}$, which is the exponential of the conditional expectation of $X$ given $\\mathcal{G}$.\n\nDetermine the value of the random variable $D = Y_1 - Y_2$ for any outcome in the event set $A_2$.", "solution": "We have a finite probability space $\\Omega=\\{\\omega_{1},\\omega_{2},\\omega_{3},\\omega_{4}\\}$ with\n$$\nP(\\{\\omega_{1}\\})=\\frac{1}{8},\\quad P(\\{\\omega_{2}\\})=\\frac{1}{8},\\quad P(\\{\\omega_{3}\\})=\\frac{1}{4},\\quad P(\\{\\omega_{4}\\})=\\frac{1}{2}.\n$$\nDefine $A_{1}=\\{\\omega_{1},\\omega_{2}\\}$ and $A_{2}=\\{\\omega_{3},\\omega_{4}\\}$, so the sub-$\\sigma$-algebra $\\mathcal{G}$ is generated by $\\{A_{1},A_{2}\\}$. The random variable $X$ satisfies\n$$\nX(\\omega_{1})=1,\\quad X(\\omega_{2})=2,\\quad X(\\omega_{3})=3,\\quad X(\\omega_{4})=4.\n$$\nFirst compute the probabilities of the partition elements:\n$$\nP(A_{1})=P(\\{\\omega_{1}\\})+P(\\{\\omega_{2}\\})=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4},\\qquad P(A_{2})=P(\\{\\omega_{3}\\})+P(\\{\\omega_{4}\\})=\\frac{1}{4}+\\frac{1}{2}=\\frac{3}{4}.\n$$\nBy definition of conditional expectation on a finite partition, for any $\\omega\\in A_{2}$,\n$$\nY_{1}(\\omega)=E[\\exp(X)\\mid\\mathcal{G}](\\omega)=E[\\exp(X)\\mid A_{2}]\n=\\frac{\\sum_{\\omega'\\in A_{2}}\\exp(X(\\omega'))\\,P(\\{\\omega'\\})}{P(A_{2})}.\n$$\nSubstitute the values on $A_{2}$:\n$$\n\\sum_{\\omega'\\in A_{2}}\\exp(X(\\omega'))\\,P(\\{\\omega'\\})\n=\\exp(3)\\cdot\\frac{1}{4}+\\exp(4)\\cdot\\frac{1}{2}=\\frac{\\exp(3)}{4}+\\frac{\\exp(4)}{2}.\n$$\nTherefore,\n$$\nY_{1}(\\omega)=\\frac{\\frac{\\exp(3)}{4}+\\frac{\\exp(4)}{2}}{\\frac{3}{4}}\n=\\left(\\frac{\\exp(3)+2\\exp(4)}{4}\\right)\\cdot\\frac{4}{3}\n=\\frac{\\exp(3)+2\\exp(4)}{3},\\qquad \\omega\\in A_{2}.\n$$\nNext, for any $\\omega\\in A_{2}$,\n$$\nY_{2}(\\omega)=\\exp\\!\\left(E[X\\mid\\mathcal{G}](\\omega)\\right)=\\exp\\!\\left(E[X\\mid A_{2}]\\right)\n=\\exp\\!\\left(\\frac{\\sum_{\\omega'\\in A_{2}}X(\\omega')\\,P(\\{\\omega'\\})}{P(A_{2})}\\right).\n$$\nCompute the conditional mean of $X$ on $A_{2}$:\n$$\n\\sum_{\\omega'\\in A_{2}}X(\\omega')\\,P(\\{\\omega'\\})\n=3\\cdot\\frac{1}{4}+4\\cdot\\frac{1}{2}=\\frac{3}{4}+2=\\frac{11}{4},\n$$\nhence\n$$\nE[X\\mid A_{2}]=\\frac{\\frac{11}{4}}{\\frac{3}{4}}=\\frac{11}{3},\n$$\nand thus\n$$\nY_{2}(\\omega)=\\exp\\!\\left(\\frac{11}{3}\\right),\\qquad \\omega\\in A_{2}.\n$$\nFinally, the random variable $D=Y_{1}-Y_{2}$ is constant on $A_{2}$ and equals\n$$\nD(\\omega)=\\frac{\\exp(3)+2\\exp(4)}{3}-\\exp\\!\\left(\\frac{11}{3}\\right),\\qquad \\omega\\in A_{2}.\n$$\nAs a consistency check, since $\\exp$ is convex, Jensen's inequality implies $Y_{1}\\geq Y_{2}$ on $A_{2}$, so $D(\\omega)\\geq 0$.", "answer": "$$\\boxed{\\frac{\\exp(3)+2\\exp(4)}{3}-\\exp\\!\\left(\\frac{11}{3}\\right)}$$", "id": "1425915"}, {"introduction": "Building on the computational intuition from the previous exercise ([@problem_id:1425915]), we now establish a general and powerful principle. By applying the conditional Jensen's inequality to the strictly convex function $\\phi(x)=x^2$, you will derive the fundamental relationship between the conditional second moment and the square of the conditional mean. This result is not merely an example; it forms the basis for the definition of conditional variance, a crucial tool in statistics and probability theory.", "problem": "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, and let $X$ be a random variable defined on this space such that its second moment is finite, i.e., $E[X^2]  \\infty$. Let $\\mathcal{G}$ be a sub-$\\sigma$-algebra of $\\mathcal{F}$. We are interested in the relationship between two random variables derived from $X$: the conditional second moment, $E[X^2|\\mathcal{G}]$, and the square of the conditional mean, $(E[X|\\mathcal{G}])^2$.\n\nWhich of the following statements correctly describes the relationship that holds almost surely between these two quantities for any such $X$ and $\\mathcal{G}$?\n\nA. $E[X^2|\\mathcal{G}]  (E[X|\\mathcal{G}])^2$\n\nB. $E[X^2|\\mathcal{G}] \\le (E[X|\\mathcal{G}])^2$\n\nC. $E[X^2|\\mathcal{G}] = (E[X|\\mathcal{G}])^2$\n\nD. $E[X^2|\\mathcal{G}] \\ge (E[X|\\mathcal{G}])^2$\n\nE. The relationship cannot be determined without more information about $X$ and $\\mathcal{G}$.", "solution": "We use that $E[X^{2}]\\infty$ implies $X\\in L^{2}$, so $E[X\\mid\\mathcal{G}]$ exists and belongs to $L^{2}$, and conditional variance is well defined.\n\nConsider the conditional variance\n$$\n\\operatorname{Var}(X\\mid\\mathcal{G}) \\equiv E\\big[(X-E[X\\mid\\mathcal{G}])^{2}\\mid\\mathcal{G}\\big].\n$$\nBy definition and linearity of conditional expectation,\n$$\n\\operatorname{Var}(X\\mid\\mathcal{G})\n=E[X^{2}\\mid\\mathcal{G}]-2\\,E\\big[X\\,E[X\\mid\\mathcal{G}]\\mid\\mathcal{G}\\big]+E\\big[(E[X\\mid\\mathcal{G}])^{2}\\mid\\mathcal{G}\\big].\n$$\nUse the property that if $Y$ is $\\mathcal{G}$-measurable and integrable, then $E[XY\\mid\\mathcal{G}]=Y\\,E[X\\mid\\mathcal{G}]$. Taking $Y=E[X\\mid\\mathcal{G}]$, we obtain\n$$\nE\\big[X\\,E[X\\mid\\mathcal{G}]\\mid\\mathcal{G}\\big] = E[X\\mid\\mathcal{G}]\\,E[X\\mid\\mathcal{G}] = (E[X\\mid\\mathcal{G}])^{2}.\n$$\nSince $E[X\\mid\\mathcal{G}]$ is $\\mathcal{G}$-measurable, we also have\n$$\nE\\big[(E[X\\mid\\mathcal{G}])^{2}\\mid\\mathcal{G}\\big]=(E[X\\mid\\mathcal{G}])^{2}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X\\mid\\mathcal{G})=E[X^{2}\\mid\\mathcal{G}]-(E[X\\mid\\mathcal{G}])^{2}.\n$$\nBy definition, $\\operatorname{Var}(X\\mid\\mathcal{G})\\ge 0$ almost surely. Hence,\n$$\nE[X^{2}\\mid\\mathcal{G}] \\ge (E[X\\mid\\mathcal{G}])^{2}\\quad\\text{almost surely}.\n$$\nEquality holds almost surely if and only if $\\operatorname{Var}(X\\mid\\mathcal{G})=0$ almost surely, i.e., when $X=E[X\\mid\\mathcal{G}]$ almost surely (for instance, if $X$ is $\\mathcal{G}$-measurable). This shows that a strict inequality need not hold, equality may occur, but the inequality direction is always $\\ge$. Equivalently, this is an instance of conditional Jensen's inequality for the convex function $t\\mapsto t^{2}$:\n$$\n(E[X\\mid\\mathcal{G}])^{2} \\le E[X^{2}\\mid\\mathcal{G}] \\quad\\text{almost surely}.\n$$\nThus the correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1425924"}, {"introduction": "Theoretical tools are most valuable when they can solve tangible problems. This exercise demonstrates the practical utility of the concepts we've developed by tasking you with calculating the variance of a compound random variable, a model frequently used in actuarial science and physics. You will apply the law of total variance, a direct consequence of the relationship explored previously ([@problem_id:1425924]), to see how conditioning provides an elegant solution to an otherwise complex calculation.", "problem": "In the modeling of aggregate claims in insurance or shot noise in electronics, one often encounters compound random variables. Consider such a scenario defined within a probability space $(\\Omega, \\mathcal{F}, P)$. Let $N$ be a discrete random variable following a Poisson distribution with mean $\\lambda  0$. Let $\\{Y_i\\}_{i=1}^{\\infty}$ be a sequence of independent and identically distributed (i.i.d.) random variables, which are also independent of $N$. Each variable $Y_i$ has a mean $E[Y_i] = \\mu$ and a finite variance $\\text{Var}(Y_i) = \\sigma^2$.\n\nA compound random variable $X$ is constructed as the sum of a random number of these variables:\n$$X = \\sum_{i=1}^{N} Y_i$$\nBy convention, if the random variable $N$ takes the value $0$, the sum is considered empty and $X=0$.\n\nYour task is to determine the variance of the compound random variable $X$. Express your answer as a closed-form analytic expression in terms of the given parameters $\\lambda$, $\\mu$, and $\\sigma$.", "solution": "We use the law of total variance:\n$$\\operatorname{Var}(X)=\\mathbb{E}\\!\\left[\\operatorname{Var}(X\\mid N)\\right]+\\operatorname{Var}\\!\\left(\\mathbb{E}[X\\mid N]\\right).$$\nCondition on $N=n$. Since $X=\\sum_{i=1}^{n}Y_{i}$ is a sum of $n$ i.i.d. variables,\n$$\\mathbb{E}[X\\mid N=n]=n\\mu,\\qquad \\operatorname{Var}(X\\mid N=n)=n\\sigma^{2}.$$\nTherefore,\n$$\\mathbb{E}\\!\\left[\\operatorname{Var}(X\\mid N)\\right]=\\mathbb{E}[N]\\sigma^{2}=\\lambda\\sigma^{2},$$\nand\n$$\\operatorname{Var}\\!\\left(\\mathbb{E}[X\\mid N]\\right)=\\operatorname{Var}(N\\mu)=\\mu^{2}\\operatorname{Var}(N)=\\mu^{2}\\lambda,$$\nwhere we used $\\mathbb{E}[N]=\\lambda$ and $\\operatorname{Var}(N)=\\lambda$ for $N\\sim\\text{Poisson}(\\lambda)$. Summing these contributions yields\n$$\\operatorname{Var}(X)=\\lambda\\sigma^{2}+\\lambda\\mu^{2}=\\lambda\\left(\\sigma^{2}+\\mu^{2}\\right).$$", "answer": "$$\\boxed{\\lambda\\left(\\sigma^{2}+\\mu^{2}\\right)}$$", "id": "1425910"}]}