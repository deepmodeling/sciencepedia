{"hands_on_practices": [{"introduction": "The Weak Law of Large Numbers (WLLN) is a cornerstone of statistical inference, providing the theoretical justification for why we can use sample averages to estimate population parameters. This first exercise [@problem_id:864068] puts this principle into practice. We will examine a hypothetical scenario where an estimator, derived from the method of moments, is used to approximate an unknown parameter, and apply the WLLN to verify its consistency—a fundamental property indicating that the estimator approaches the true value as the sample size grows.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a continuous uniform distribution on the interval $[0, \\theta]$, where $\\theta  0$ is an unknown parameter. The probability density function of each $X_i$ is given by:\n$$\nf(x; \\theta) = \\begin{cases} \\frac{1}{\\theta}  \\text{if } 0 \\le x \\le \\theta \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe sample mean is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$. A common estimator for the parameter $\\theta$, derived from the method of moments, is $\\hat{\\theta}_n = 2\\bar{X}_n$.\n\nWe say that a sequence of random variables $Y_n$ converges in probability to a constant $c$ (denoted $Y_n \\xrightarrow{p} c$) if for every $\\epsilon  0$, the following condition holds:\n$$\n\\lim_{n \\to \\infty} P(|Y_n - c|  \\epsilon) = 0\n$$\nThe Weak Law of Large Numbers (WLLN) states that if $X_1, X_2, \\ldots$ are i.i.d. random variables with a finite expected value $E[X_i] = \\mu$, then their sample mean $\\bar{X}_n$ converges in probability to $\\mu$.\n\nUsing the WLLN and the properties of convergence in probability, determine the value $c$ such that the estimator $\\hat{\\theta}_n$ converges in probability to $c$.", "solution": "We seek the limit in probability of $\\hat\\theta_n=2\\bar X_n$ using the WLLN.  \n1. The expectation of each $X_i\\sim\\mathrm{Uniform}[0,\\theta]$ is  \n$$\nE[X_i]=\\int_{0}^{\\theta}x\\,\\frac{1}{\\theta}\\,dx\n=\\frac{1}{\\theta}\\,\\frac{\\theta^2}{2}\n=\\frac{\\theta}{2}.\n$$  \n2. By the WLLN,  \n$$\n\\bar X_n=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\xrightarrow{p}E[X_i]\n=\\frac{\\theta}{2}.\n$$  \n3. Hence  \n$$\n\\hat\\theta_n\n=2\\bar X_n\n\\xrightarrow{p}2\\cdot\\frac{\\theta}{2}\n=\\theta.\n$$  \nTherefore, $\\hat\\theta_n$ converges in probability to $\\theta\\,$.", "answer": "$$\\boxed{\\theta}$$", "id": "864068"}, {"introduction": "Beyond the simple sample mean, the WLLN allows us to understand the long-run behavior of other important sample statistics. This practice [@problem_id:864091] extends the core concept to the convergence of sample moments, specifically the second moment. By considering a new sequence of random variables formed by squaring the original variables, we can directly apply the WLLN to determine the limit of their average, a value crucial for understanding concepts like sample variance.", "problem": "Let $X_1, X_2, \\dots$ be a sequence of independent and identically distributed (i.i.d.) random variables, where each $X_i$ follows a Poisson distribution with a rate parameter $\\lambda  0$. The probability mass function for each $X_i$ is given by:\n$$\nP(X_i = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}\n$$\nConsider the sample mean of the squares of these random variables, defined as:\n$$\nM_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2\n$$\nThe Weak Law of Large Numbers states that as $n \\to \\infty$, the sample mean of a sequence of i.i.d. variables converges in probability to the expected value of the variable, provided the expectation is finite. In this case, $M_n$ converges in probability to a constant value, which we shall call $L$.\n\nDerive the value of $L$ as a function of the parameter $\\lambda$.", "solution": "We seek the probability limit of \n$$\nM_n = \\frac{1}{n}\\sum_{i=1}^n X_i^2\n$$\nas $n\\to\\infty$.\n\nBy the Weak Law of Large Numbers, $M_n$ converges in probability to \n$$\nE[X^2].\n$$\n\nWe use the identity\n$$\nE[X^2] = \\mathrm{Var}(X) + \\bigl(E[X]\\bigr)^2.\n$$\n\nFor $X\\sim\\mathrm{Poisson}(\\lambda)$, we have\n$$\nE[X] = \\lambda,\n\\qquad\n\\mathrm{Var}(X) = \\lambda.\n$$\n\nHence\n$$\nE[X^2] = \\lambda + \\lambda^2.\n$$", "answer": "$$\\boxed{\\lambda + \\lambda^2}$$", "id": "864091"}, {"introduction": "Convergence in probability has powerful implications, especially when we apply functions to random variables that are already converging. This exercise [@problem_id:863858] explores how the WLLN works in tandem with the Continuous Mapping Theorem, a vital result guaranteeing that convergence is preserved under continuous transformations. This problem demonstrates how the established convergence of a sample mean allows us to determine the limit of a more complex, non-linear function of that mean.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables, where each $X_i$ follows a continuous uniform distribution on the interval $(0, \\theta)$, with $\\theta  0$ being a fixed parameter. The sample mean is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\nConsider a new random variable $Y_n$, which is a rational function of the sample mean:\n$$Y_n = \\frac{(\\bar{X}_n)^2}{1 + \\bar{X}_n}$$\n\nUsing the Weak Law of Large Numbers and its associated properties for functions of random variables, derive the value $L$ to which $Y_n$ converges in probability as $n$ approaches infinity.", "solution": "1. For $X_i\\sim \\mathrm{Uniform}(0,\\theta)$, we have \n$$E[X_i]=\\frac{\\theta}{2},\\quad \\mathrm{Var}(X_i)=\\frac{\\theta^2}{12}.$$\nBy the Weak Law of Large Numbers,\n$$\\bar X_n=\\frac1n\\sum_{i=1}^nX_i\\;\\xrightarrow{P}\\;E[X_1]=\\frac{\\theta}{2}.$$\n2. Define the continuous function \n$$f(x)=\\frac{x^2}{1+x}.$$\nBy the Continuous Mapping Theorem,\n$$Y_n=f(\\bar X_n)\\;\\xrightarrow{P}\\;f\\bigl(E[X_1]\\bigr)\n=f\\Bigl(\\frac{\\theta}{2}\\Bigr).$$\n3. Compute the limit:\n$$f\\Bigl(\\frac{\\theta}{2}\\Bigr)\n=\\frac{\\bigl(\\frac{\\theta}{2}\\bigr)^2}{1+\\frac{\\theta}{2}}\n=\\frac{\\frac{\\theta^2}{4}}{\\frac{2+\\theta}{2}}\n=\\frac{\\theta^2}{2(\\theta+2)}.$$", "answer": "$$\\boxed{\\frac{\\theta^2}{2(\\theta+2)}}$$", "id": "863858"}]}