{"hands_on_practices": [{"introduction": "This first exercise provides a foundational look at conditional expectation by examining a simple, yet fundamental, scenario in probability: waiting for the first success in a series of trials. By conditioning on the outcome of the very first trial, you will practice computing the expected value on the basic building blocks (atoms) of the information set, which is a core skill for understanding more complex models. This problem [@problem_id:1410778] directly illustrates how our expectation of a future event changes based on initial information.", "problem": "Consider a sequence of independent Bernoulli trials where the probability of success on any given trial is a constant $p$, with $0  p  1$. Let the random variable $X$ denote the trial number on which the first success occurs. Let $Y_1$ be a random variable representing the outcome of the first trial, where $Y_1=1$ if the first trial is a success and $Y_1=0$ if it is a failure. Furthermore, let $\\mathcal{G} = \\sigma(Y_1)$ be the $\\sigma$-algebra generated by the random variable $Y_1$.\n\nDetermine the conditional expectation $E[X|\\mathcal{G}]$. Your answer should be a single closed-form analytic expression in terms of $p$ and $Y_1$.", "solution": "Let $\\{Y_{n}\\}_{n\\geq 1}$ be i.i.d. Bernoulli trials with success probability $p$, and let $X$ be the index of the first success, so $X$ has the geometric distribution on $\\{1,2,\\dots\\}$ with parameter $p$. The $\\sigma$-algebra $\\mathcal{G}=\\sigma(Y_{1})$ has two atoms $\\{Y_{1}=1\\}$ and $\\{Y_{1}=0\\}$, and $E[X|\\mathcal{G}]$ must be a function of $Y_{1}$.\n\nBy the definition of conditional expectation on atoms,\n$$\nE[X|\\mathcal{G}] = E[X\\,|\\,Y_{1}=1]\\cdot \\mathbf{1}_{\\{Y_{1}=1\\}} + E[X\\,|\\,Y_{1}=0]\\cdot \\mathbf{1}_{\\{Y_{1}=0\\}}.\n$$\nOn $\\{Y_{1}=1\\}$, the first trial is a success, hence $X=1$ almost surely, so\n$$\nE[X\\,|\\,Y_{1}=1]=1.\n$$\nOn $\\{Y_{1}=0\\}$, the first trial is a failure, and by independence and stationarity of the trials, the waiting time from trial $2$ onward is geometric$(p)$ and independent of $Y_{1}$. Thus\n$$\nX \\,\\big|\\, \\{Y_{1}=0\\} \\stackrel{d}{=} 1 + Z,\n$$\nwhere $Z$ is geometric$(p)$ on $\\{1,2,\\dots\\}$. Therefore,\n$$\nE[X\\,|\\,Y_{1}=0]=1+E[Z].\n$$\nTo compute $E[Z]$, use the series representation\n$$\nE[Z]=\\sum_{k=1}^{\\infty} k\\,(1-p)^{k-1} p = p \\sum_{k=1}^{\\infty} k r^{k-1} \\quad \\text{with } r=1-p,\n$$\nand the identity $\\sum_{k=1}^{\\infty} k r^{k-1} = \\frac{1}{(1-r)^{2}}$ for $|r|1$, which yields\n$$\nE[Z]= p \\cdot \\frac{1}{(1-(1-p))^{2}} = p \\cdot \\frac{1}{p^{2}} = \\frac{1}{p}.\n$$\nHence,\n$$\nE[X\\,|\\,Y_{1}=0]=1+\\frac{1}{p}.\n$$\n\nSince $Y_1 \\in \\{0,1\\}$, we can write the conditional expectation as a single function of $Y_{1}$:\n$$\nE[X|\\mathcal{G}] = 1\\cdot Y_{1} + \\left(1+\\frac{1}{p}\\right)(1-Y_{1}) = 1 + \\frac{1 - Y_{1}}{p}.\n$$\nThis is measurable with respect to $\\sigma(Y_{1})$ and matches the conditional expectations on the atoms, hence it is the desired conditional expectation.", "answer": "$$\\boxed{1+\\frac{1-Y_{1}}{p}}$$", "id": "1410778"}, {"introduction": "Building upon the idea of conditioning on a simple partition of the sample space, this problem transitions to a continuous setting. You will calculate the best estimate of a signal's intensity given only coarse \"high\" or \"low\" information, which corresponds to averaging the signal over specific intervals. This practice [@problem_id:1410818] is crucial for understanding how conditional expectation acts as a data-smoothing or estimation tool when complete information is unavailable.", "problem": "Let $X$ be a random variable representing the intensity of a signal, which is modeled to be continuously distributed on the interval $[0,1]$. The probability density function (PDF) of $X$ is given by $f(x) = 2x$ for $x \\in [0,1]$, and $f(x)=0$ otherwise.\n\nA simple detector processes this signal and provides only coarse information: it reports whether the signal intensity is \"low\" ($X \\leq 1/2$) or \"high\" ($X  1/2$). Let $A$ be the event that the signal is high, i.e., $A = \\{X  1/2\\}$. The information available after detection is represented by the $\\sigma$-algebra $\\mathcal{G} = \\sigma(A)$, which is the smallest $\\sigma$-algebra containing the event $A$.\n\nThe best estimate of the signal intensity $X$ given this coarse information is the conditional expectation $Y = E[X|\\mathcal{G}]$. This conditional expectation is a new random variable which is constant on the event $A$ and on its complement $A^c = \\{X \\leq 1/2\\}$.\n\nCalculate the numerical value that $Y$ takes on the event $A^c$ and the numerical value that $Y$ takes on the event $A$. Provide your answer as two exact fractions, with the first value corresponding to the event $A^c$ and the second value corresponding to the event $A$.", "solution": "We use the fact that $\\mathcal{G}=\\sigma(A)$ partitions the sample space into $A=\\{X1/2\\}$ and $A^{c}=\\{X\\leq 1/2\\}$. The conditional expectation $Y=E[X\\mid\\mathcal{G}]$ is constant on each atom of this partition, with\n$$\nY = E[X\\mid A]\\ \\text{on }A,\\qquad Y=E[X\\mid A^{c}]\\ \\text{on }A^{c}.\n$$\nBy definition of conditional expectation on an event,\n$$\nE[X\\mid A] = \\frac{E[X\\,\\mathbf{1}_{A}]}{P(A)},\\qquad E[X\\mid A^{c}] = \\frac{E[X\\,\\mathbf{1}_{A^{c}}]}{P(A^{c})}.\n$$\nThe PDF is $f(x)=2x$ for $x\\in[0,1]$. First compute the probabilities:\n$$\nP(A)=\\int_{1/2}^{1} 2x\\,dx = \\left[x^{2}\\right]_{1/2}^{1} = 1 - \\frac{1}{4} = \\frac{3}{4},\\qquad P(A^{c})=1-P(A)=\\frac{1}{4}.\n$$\nNext compute the numerators:\n$$\nE[X\\,\\mathbf{1}_{A}] = \\int_{1/2}^{1} x\\cdot 2x\\,dx = \\int_{1/2}^{1} 2x^{2}\\,dx = \\left[\\frac{2}{3}x^{3}\\right]_{1/2}^{1} = \\frac{2}{3}\\left(1 - \\frac{1}{8}\\right) = \\frac{7}{12},\n$$\n$$\nE[X\\,\\mathbf{1}_{A^{c}}] = \\int_{0}^{1/2} x\\cdot 2x\\,dx = \\int_{0}^{1/2} 2x^{2}\\,dx = \\left[\\frac{2}{3}x^{3}\\right]_{0}^{1/2} = \\frac{2}{3}\\cdot\\frac{1}{8} = \\frac{1}{12}.\n$$\nTherefore,\n$$\nE[X\\mid A] = \\frac{7/12}{3/4} = \\frac{7}{9},\\qquad E[X\\mid A^{c}] = \\frac{1/12}{1/4} = \\frac{1}{3}.\n$$\nThus $Y$ equals $\\frac{1}{3}$ on $A^{c}$ and $\\frac{7}{9}$ on $A$. As a consistency check, $E[Y] = \\frac{1}{3}\\cdot\\frac{1}{4} + \\frac{7}{9}\\cdot\\frac{3}{4} = \\frac{2}{3} = E[X]$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{3}  \\frac{7}{9}\\end{pmatrix}}$$", "id": "1410818"}, {"introduction": "This exercise generalizes the concept of conditioning from a simple two-event partition to conditioning on the information provided by a random variable that can take on multiple values. You will determine the expected maximum of two die rolls, given the outcome of the first roll. This practice [@problem_id:1410798] demonstrates the powerful idea that the conditional expectation $E[Z|\\sigma(X)]$ is itself a random variable that can be expressed as a function of $X$.", "problem": "Consider a probability space $(\\Omega, \\mathcal{F}, P)$ modeling the outcomes of two independent rolls of a fair six-sided die. Let the random variable $X: \\Omega \\to \\{1, 2, 3, 4, 5, 6\\}$ represent the outcome of the first roll, and $Y: \\Omega \\to \\{1, 2, 3, 4, 5, 6\\}$ represent the outcome of the second roll. The random variables $X$ and $Y$ are independent and identically distributed, with $P(X=k) = P(Y=k) = 1/6$ for each $k \\in \\{1, 2, 3, 4, 5, 6\\}$.\n\nLet $\\sigma(X)$ be the $\\sigma$-algebra generated by the random variable $X$. Determine the conditional expectation of the random variable $Z = \\max(X, Y)$ with respect to the $\\sigma$-algebra $\\sigma(X)$, denoted by $E[\\max(X,Y) | \\sigma(X)]$.\n\nExpress your answer as a closed-form analytic expression in terms of the random variable $X$.", "solution": "We seek the $\\sigma(X)$-conditional expectation of $Z=\\max(X,Y)$, where $X$ and $Y$ are independent and uniformly distributed on $\\{1,2,3,4,5,6\\}$. Because $\\sigma(X)$ is generated by $X$, the conditional expectation $E[\\max(X,Y)\\mid\\sigma(X)]$ must be an $X$-measurable function. By the definition of conditional expectation with respect to $\\sigma(X)$ and using independence, there exists a function $g$ such that\n$$\nE[\\max(X,Y)\\mid\\sigma(X)] = g(X), \\quad \\text{with } g(x) = E[\\max(x,Y)] \\text{ for each } x\\in\\{1,\\ldots,6\\}.\n$$\nFix $x\\in\\{1,\\ldots,6\\}$. Since $Y$ is uniform on $\\{1,\\ldots,6\\}$ and independent of $X$,\n$$\ng(x) = \\sum_{y=1}^{6} \\max(x,y) P(Y=y) = \\frac{1}{6}\\left(\\sum_{y=1}^{x} x \\;+\\; \\sum_{y=x+1}^{6} y\\right).\n$$\nThe first sum contributes $x$ repeated $x$ times, and the second sum runs over $yx$. Thus,\n$$\ng(x) = \\frac{1}{6}\\left( x\\cdot x \\;+\\; \\sum_{y=1}^{6} y \\;-\\; \\sum_{y=1}^{x} y \\right).\n$$\nUsing the identity $\\sum_{y=1}^{n} y = \\frac{n(n+1)}{2}$, we have $\\sum_{y=1}^{6} y = \\frac{6\\cdot 7}{2} = 21$ and $\\sum_{y=1}^{x} y = \\frac{x(x+1)}{2}$. Therefore,\n$$\ng(x) = \\frac{1}{6}\\left( x^{2} + 21 - \\frac{x(x+1)}{2} \\right)\n= \\frac{1}{6}\\left(21 + \\frac{x^{2}-x}{2}\\right)\n= \\frac{x^{2} - x + 42}{12}.\n$$\nHence,\n$$\nE[\\max(X,Y)\\mid \\sigma(X)] = g(X) = \\frac{X^{2} - X + 42}{12}.\n$$\nThis is an $X$-measurable (hence $\\sigma(X)$-measurable) function, and by construction satisfies the defining property of the conditional expectation.", "answer": "$$\\boxed{\\frac{X^{2}-X+42}{12}}$$", "id": "1410798"}]}