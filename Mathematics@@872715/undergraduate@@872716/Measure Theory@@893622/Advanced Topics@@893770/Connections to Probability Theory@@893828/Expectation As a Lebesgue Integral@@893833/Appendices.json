{"hands_on_practices": [{"introduction": "The first step in mastering a new mathematical tool is to see it in action on a familiar task. This exercise grounds the abstract definition of expectation as a Lebesgue integral, $E[g(X)] = \\int g(x) \\, dP(x)$, in the practical and fundamental statistical calculation of variance. By working through the computation for a simple continuous random variable, you will solidify the connection between the theoretical framework and the computational methods used in probability and statistics [@problem_id:1418566].", "problem": "Let $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}), P)$ be a probability space, where $\\mathcal{B}(\\mathbb{R})$ is the Borel sigma-algebra on the real line. The probability measure $P$ is absolutely continuous with respect to the Lebesgue measure $\\lambda$, and its Radon-Nikodym derivative (the probability density function) is given by:\n$$\nf(x) = \\frac{dP}{d\\lambda}(x) =\n\\begin{cases}\n2 - 2x  \\text{if } x \\in [0, 1] \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nConsider the random variable $X: (\\mathbb{R}, \\mathcal{B}(\\mathbb{R})) \\to (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$ defined by the identity function, $X(\\omega) = \\omega$. The expectation of a real-valued measurable function $g$ of the random variable $X$ is defined by the Lebesgue integral $E[g(X)] = \\int_{\\mathbb{R}} g(x) \\, dP(x)$.\n\nThe variance of the random variable $X$ is defined as $\\text{Var}(X) = E\\left[(X - E[X])^2\\right]$.\n\nCalculate the variance, $\\text{Var}(X)$, of this random variable. Your final answer should be a single numerical value expressed as an irreducible fraction.", "solution": "First verify that $f$ is a valid probability density on $\\mathbb{R}$. By definition, $f(x)=2-2x$ for $x\\in[0,1]$ and $f(x)=0$ otherwise. The total mass is\n$$\n\\int_{\\mathbb{R}} f(x)\\,dx=\\int_{0}^{1} (2-2x)\\,dx=\\left[2x - x^{2}\\right]_{0}^{1}=2-1=1,\n$$\nso $f$ is normalized.\n\nThe expectation of $X$ is given by\n$$\nE[X]=\\int_{\\mathbb{R}} x\\,dP(x)=\\int_{\\mathbb{R}} x f(x)\\,dx=\\int_{0}^{1} x(2-2x)\\,dx.\n$$\nCompute\n$$\n\\int_{0}^{1} x(2-2x)\\,dx=2\\int_{0}^{1} x\\,dx-2\\int_{0}^{1} x^{2}\\,dx=2\\left[\\frac{x^{2}}{2}\\right]_{0}^{1}-2\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1-\\frac{2}{3}=\\frac{1}{3}.\n$$\n\nNext compute the second moment:\n$$\nE[X^{2}]=\\int_{\\mathbb{R}} x^{2}\\,dP(x)=\\int_{0}^{1} x^{2}(2-2x)\\,dx=2\\int_{0}^{1} x^{2}\\,dx-2\\int_{0}^{1} x^{3}\\,dx.\n$$\nEvaluate\n$$\n2\\int_{0}^{1} x^{2}\\,dx-2\\int_{0}^{1} x^{3}\\,dx=2\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}-2\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{2}{3}-\\frac{1}{2}=\\frac{1}{6}.\n$$\n\nUsing the identity $\\text{Var}(X)=E[X^{2}]-(E[X])^{2}$, which follows from expanding $E[(X-E[X])^{2}]=E[X^{2}]-2E[X]E[X]+(E[X])^{2}$ and linearity of expectation, we obtain\n$$\n\\text{Var}(X)=\\frac{1}{6}-\\left(\\frac{1}{3}\\right)^{2}=\\frac{1}{6}-\\frac{1}{9}=\\frac{3}{18}-\\frac{2}{18}=\\frac{1}{18}.\n$$\nThus the variance is the irreducible fraction $\\frac{1}{18}$.", "answer": "$$\\boxed{\\frac{1}{18}}$$", "id": "1418566"}, {"introduction": "One of the most powerful aspects of measure theory is its ability to unify discrete and continuous concepts under a single framework. This problem offers a striking example of this principle by tackling the expectation of a function of a discrete binomial random variable using continuous integration. You will use a clever integral identity and then apply the Fubini-Tonelli theorem to justify swapping a summation and an integral, transforming a potentially complicated discrete sum into a straightforward calculus problem [@problem_id:744855].", "problem": "Let $X$ be a random variable following a Binomial distribution, $X \\sim \\text{Binomial}(n, p)$, with parameters $n \\in \\mathbb{Z}^+$ (the number of trials) and $p \\in (0, 1)$ (the probability of success). The probability mass function (PMF) for $X$ is given by:\n$$\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots, n\\}\n$$\nWe are interested in the expected value of a related random variable, $Y = \\frac{1}{X+1}$.\n\nYour task is to derive a closed-form expression for the expectation $E\\left[\\frac{1}{X+1}\\right]$ in terms of $n$ and $p$. The derivation must be performed by leveraging the following integral identity for the reciprocal of a positive integer $m$:\n$$\n\\frac{1}{m} = \\int_0^1 y^{m-1} dy\n$$\nYou must explicitly apply this identity and justify the subsequent interchange of summation and integration.", "solution": "We use the integral identity \n$$\n\\frac{1}{m} = \\int_0^1 y^{m-1}\\,dy\n$$\nwith $m = X+1$, so that\n$$\n\\frac{1}{X+1} = \\int_0^1 y^X\\,dy.\n$$\nTaking expectation and interchanging sum and integral (justified by nonnegativity), we get\n$$\nE\\bigl[\\tfrac{1}{X+1}\\bigr]\n= \\int_0^1 E[y^X]\\,dy\n= \\int_0^1 \\sum_{k=0}^n y^k \\binom{n}{k}p^k(1-p)^{n-k}\\,dy.\n$$\nInside the integral the binomial theorem gives\n$$\n\\sum_{k=0}^n \\binom{n}{k}(py)^k(1-p)^{n-k} = \\bigl(1-p + p\\,y\\bigr)^n,\n$$\nso\n$$\nE\\bigl[\\tfrac{1}{X+1}\\bigr]\n= \\int_0^1 \\bigl(1-p+p\\,y\\bigr)^n dy.\n$$\nWe evaluate the integral by the substitution $u = 1-p+p\\,y$, $du = p\\,dy$:\n$$\n\\int_0^1 (1-p+p\\,y)^n dy\n= \\frac{1}{p}\\int_{u=1-p}^{1} u^n du\n= \\frac{1}{p}\\,\\frac{1}{n+1}\\Bigl[1^{\\,n+1} - (1-p)^{\\,n+1}\\Bigr]\n= \\frac{1-(1-p)^{n+1}}{p\\,(n+1)}.\n$$", "answer": "$$\\boxed{\\frac{1-(1-p)^{n+1}}{p\\,(n+1)}}$$", "id": "744855"}, {"introduction": "The ability to interchange limits and integrals is not guaranteed, and understanding when it is permissible is a cornerstone of analysis. This practice presents a classic cautionary example that explores the consequences when the conditions for such an interchange are not met. By analyzing a sequence of random variables where $\\lim_{n \\to \\infty} E[X_n] \\neq E[\\lim_{n \\to \\infty} X_n]$, you will gain a deeper appreciation for the necessity of the conditions in powerful results like the Dominated Convergence Theorem [@problem_id:1360914].", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on the probability space $(\\Omega, \\mathcal{F}, P)$, where the sample space is the unit interval $\\Omega = [0, 1]$, the sigma-algebra $\\mathcal{F}$ is the Borel sigma-algebra on $[0,1]$, and the probability measure $P$ is the Lebesgue measure on $[0,1]$ (corresponding to a continuous uniform distribution).\n\nEach random variable in the sequence is defined by the function $X_n(\\omega) = n \\cdot I_{(0, 1/n)}(\\omega)$, where $I_A(\\omega)$ is the indicator function for the set $A$, which equals 1 if $\\omega \\in A$ and 0 otherwise.\n\nThis sequence can be interpreted as a simplified model for a series of high-intensity, short-duration phenomena. As $n$ increases, the magnitude of the outcome, $n$, grows, while the interval over which it occurs, $(0, 1/n)$, shrinks.\n\nYour task is to analyze the limiting behavior of the expectations of this sequence.\n1.  First, calculate the limit of the expected values, denoted as $L_E = \\lim_{n \\to \\infty} E[X_n]$.\n2.  Second, determine the pointwise limit of the sequence of random variables, which we'll call $X(\\omega) = \\lim_{n \\to \\infty} X_n(\\omega)$ for each $\\omega \\in [0, 1]$. Then, calculate the expectation of this limiting random variable, denoted as $E_L = E[X]$.\n\nProvide the values of $L_E$ and $E_L$ as an ordered pair $(L_E, E_L)$.", "solution": "We are given $X_{n}(\\omega)=n\\,I_{(0,1/n)}(\\omega)$ on $\\Omega=[0,1]$ with Lebesgue measure $P$. By definition of expectation for a nonnegative measurable function,\n$$\nE[X_{n}]=\\int_{0}^{1}X_{n}(\\omega)\\,d\\omega=\\int_{0}^{1}n\\,I_{(0,1/n)}(\\omega)\\,d\\omega.\n$$\nUsing linearity of the integral and the property $\\int I_{A}\\,dP=P(A)$,\n$$\nE[X_{n}]=n\\,P\\big((0,1/n)\\big)=n\\cdot\\frac{1}{n}=1.\n$$\nTherefore, the limit of the expectations is\n$$\nL_{E}=\\lim_{n\\to\\infty}E[X_{n}]=\\lim_{n\\to\\infty}1=1.\n$$\n\nNext, fix $\\omega\\in[0,1]$ and analyze the pointwise limit $X(\\omega)=\\lim_{n\\to\\infty}X_{n}(\\omega)$. If $\\omega=0$, then $I_{(0,1/n)}(0)=0$ for all $n$, hence $X_{n}(0)=0$ for all $n$ and $\\lim_{n\\to\\infty}X_{n}(0)=0$. If $\\omega0$, choose $N\\in\\mathbb{N}$ with $N\\frac{1}{\\omega}$, which implies that for all $n\\geq N$ we have $\\frac{1}{n}\\omega$, so $\\omega\\notin(0,1/n)$ and $I_{(0,1/n)}(\\omega)=0$. Thus $X_{n}(\\omega)=0$ for all sufficiently large $n$, and therefore $\\lim_{n\\to\\infty}X_{n}(\\omega)=0$. It follows that\n$$\nX(\\omega)=0\\quad\\text{for all }\\omega\\in[0,1].\n$$\nHence $X\\equiv 0$ almost everywhere, and its expectation is\n$$\nE_{L}=E[X]=\\int_{0}^{1}0\\,d\\omega=0.\n$$\n\nTherefore, the ordered pair is $(L_{E},E_{L})=(1,0)$.", "answer": "$$\\boxed{\\begin{pmatrix}1  0\\end{pmatrix}}$$", "id": "1360914"}]}