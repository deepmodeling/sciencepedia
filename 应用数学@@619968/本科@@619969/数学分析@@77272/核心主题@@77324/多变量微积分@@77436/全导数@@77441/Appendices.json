{"hands_on_practices": [{"introduction": "掌握全导数的第一步是学会计算其矩阵表示——雅可比矩阵。本练习旨在通过一个从三维空间到二维空间的向量值函数，让您直接实践雅可比矩阵的计算。这不仅是一项基本的计算技能，更是理解函数在某点附近如何进行线性变换的基础。[@problem_id:37815]", "problem": "考虑一个向量值函数 $f$，它将点从三维空间映射到二维空间，即 $f: \\mathbb{R}^3 \\to \\mathbb{R}^2$。这样一个函数在点 $(x, y, z)$ 处的全导数由一个称为雅可比矩阵的矩阵表示，记为 $J_f(x, y, z)$。该矩阵是函数在该点附近的最佳线性近似。\n\n对于函数 $f(x,y,z) = (f_1(x,y,z), f_2(x,y,z))$，其雅可比矩阵是一个 $2 \\times 3$ 矩阵，由下式给出：\n$$\nJ_f(x,y,z) = \\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} & \\frac{\\partial f_1}{\\partial z} \\\\\n\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} & \\frac{\\partial f_2}{\\partial z}\n\\end{pmatrix}\n$$\n其中 $f_1$ 和 $f_2$ 是 $f$ 的分量函数。\n\n已知函数：\n$$\nf(x, y, z) = (x\\cos(y) + z^2, y e^{xz})\n$$\n求其雅可比矩阵 $J_f(x, y, z)$。", "solution": "我们有 $f(x,y,z)=(f_1,f_2)$，其中\n$$f_1(x,y,z)=x\\cos(y)+z^2,\\quad f_2(x,y,z)=y\\,e^{xz}.$$\n根据定义，\n$$J_f(x,y,z)=\n\\begin{pmatrix}\n\\partial_x f_1 & \\partial_y f_1 & \\partial_z f_1\\\\\n\\partial_x f_2 & \\partial_y f_2 & \\partial_z f_2\n\\end{pmatrix}.$$\n计算各项：\n\n1. 对于 $f_1$:\n   $$\\partial_x f_1=\\cos(y),\\quad \\partial_y f_1=-x\\sin(y),\\quad \\partial_z f_1=2z.$$\n\n2. 对于 $f_2$:\n   $$\\partial_x f_2=y\\,\\partial_x(e^{xz})=y\\,(z e^{xz})=yz\\,e^{xz},$$\n   $$\\partial_y f_2=e^{xz},$$\n   $$\\partial_z f_2=y\\,\\partial_z(e^{xz})=y\\,(x e^{xz})=xy\\,e^{xz}.$$\n\n因此，雅可比矩阵为\n$$\nJ_f(x,y,z)=\n\\begin{pmatrix}\n\\cos(y) & -x\\sin(y) & 2z\\\\\nyz\\,e^{xz} & e^{xz} & xy\\,e^{xz}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\cos(y)&-x\\sin(y)&2z\\\\yz\\,e^{xz}&e^{xz}&xy\\,e^{xz}\\end{pmatrix}}$$", "id": "37815"}, {"introduction": "计算出雅可比矩阵后，更重要的是理解它的作用。全导数本质上是一个线性变换，它描述了函数在某点附近对微小变化的响应。本练习将雅可比矩阵从一个静态的偏导数数组，转变为一个作用于向量的动态算子，直观地展示了函数沿特定方向的变化是如何被其线性近似所捕捉的。[@problem_id:37812]", "problem": "考虑一个函数 $f: \\mathbb{R}^2 \\to \\mathbb{R}^2$，定义为：\n$$\nf(x,y) = (f_1(x,y), f_2(x,y)) = (xy, x-y)\n$$\n函数 $f$ 在点 $\\vec{p} = (x_0, y_0)$ 的全导数，记作 $Df_{\\vec{p}}$，是一个线性变换，可以由 $f$ 在点 $\\vec{p}$ 的雅可比矩阵表示，我们称之为 $J_f(\\vec{p})$。这个全导数对向量 $\\vec{v}$ 的作用由矩阵向量乘积 $Df_{\\vec{p}}(\\vec{v}) = J_f(\\vec{p}) \\vec{v}$ 给出。\n\n给定点 $\\vec{p} = (2, 1)$ 和向量 $\\vec{v} = (3, -1)$，计算函数 $f$ 在点 $\\vec{p}$ 的全导数作用于向量 $\\vec{v}$ 所得到的向量。请将最终答案表示为列向量。", "solution": "问题要求计算 $Df_{\\vec{p}}(\\vec{v})$，其中 $f(x,y) = (xy, x-y)$，$\\vec{p} = (2, 1)$，并且 $\\vec{v} = (3, -1)$。\n\n全导数对一个向量的作用由雅可比矩阵与该向量的乘积给出：\n$$\nDf_{\\vec{p}}(\\vec{v}) = J_f(\\vec{p}) \\vec{v}\n$$\n\n首先，我们必须求出函数 $f(x,y)$ 的雅可比矩阵。$f$ 的分量是 $f_1(x,y) = xy$ 和 $f_2(x,y) = x-y$。雅可比矩阵 $J_f(x,y)$ 是一个由 $f$ 的偏导数组成的 $2 \\times 2$ 矩阵：\n$$\nJ_f(x,y) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{pmatrix}\n$$\n\n我们计算每个偏导数：\n- $\\frac{\\partial f_1}{\\partial x} = \\frac{\\partial}{\\partial x}(xy) = y$\n- $\\frac{\\partial f_1}{\\partial y} = \\frac{\\partial}{\\partial y}(xy) = x$\n- $\\frac{\\partial f_2}{\\partial x} = \\frac{\\partial}{\\partial x}(x-y) = 1$\n- $\\frac{\\partial f_2}{\\partial y} = \\frac{\\partial}{\\partial y}(x-y) = -1$\n\n将这些偏导数代入雅可比矩阵，得到：\n$$\nJ_f(x,y) = \\begin{pmatrix} y & x \\\\ 1 & -1 \\end{pmatrix}\n$$\n\n接下来，我们在指定点 $\\vec{p} = (x_0, y_0) = (2, 1)$ 处计算该矩阵的值：\n$$\nJ_f(2,1) = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix}\n$$\n\n现在，我们可以计算全导数对向量 $\\vec{v} = (3, -1)$ 的作用。我们可以将 $\\vec{v}$ 写成列向量 $\\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}$。\n$$\nDf_{(2,1)}(\\vec{v}) = J_f(2,1) \\vec{v} = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}\n$$\n\n最后，我们进行矩阵向量乘法：\n$$\n\\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(3) + (2)(-1) \\\\ (1)(3) + (-1)(-1) \\end{pmatrix} = \\begin{pmatrix} 3 - 2 \\\\ 3 + 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}\n$$\n\n得到的向量是 $(1, 4)$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}}\n$$", "id": "37812"}, {"introduction": "链式法则是处理复合函数的强大工具，这在机器学习等领域中尤为常见，其中数据常常先经过变换再进行分析。本练习在一个更抽象的设置中应用链式法则，涉及到一个重要的线性变换——投影。通过这个练习，您将推导出一个简洁而优美的结果，深刻揭示了梯度如何通过线性变换进行传播，展现了全导数理论在现代科学中的应用价值。[@problem_id:2330045]", "problem": "在许多机器学习和信号处理应用中，高维数据向量在进行进一步分析之前，首先会被投影到一个低维子空间上。这种投影作为一种降维或特征提取的形式。\n\n考虑这样一个系统，其中输入数据向量 $\\mathbf{x} \\in \\mathbb{R}^n$ 受到一个由矩阵 $P \\in M_n(\\mathbb{R})$ 表示的线性变换的作用。矩阵 $P$ 是一个投影矩阵，这意味着它是幂等的，满足性质 $P^2 = P$。\n\n在此变换之后，一个可微的实值成本函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 被应用于结果向量。因此，整个操作由复合函数 $g(\\mathbf{x}) = f(P\\mathbf{x})$ 描述。\n\n对于一个标量函数 $h: \\mathbb{R}^n \\to \\mathbb{R}$，其在点 $\\mathbf{a}$ 处的梯度，记作 $\\nabla h(\\mathbf{a})$，定义为其偏导数的列向量，即 $\\nabla h(\\mathbf{a}) = \\left(\\frac{\\partial h}{\\partial x_1}(\\mathbf{a}), \\frac{\\partial h}{\\partial x_2}(\\mathbf{a}), \\dots, \\frac{\\partial h}{\\partial x_n}(\\mathbf{a})\\right)^T$。$h$ 在 $\\mathbf{a}$ 处的导数（或雅可比矩阵），记作 $Dh(\\mathbf{a})$，是 $1 \\times n$ 行向量 $(\\nabla h(\\mathbf{a}))^T$。\n\n确定函数 $g$ 在任意点 $\\mathbf{x} \\in \\mathbb{R}^n$ 处的梯度。你的最终答案应该是一个包含矩阵 $P$、其转置 $P^T$ 以及在适当点求值的 $f$ 的梯度的表达式。", "solution": "我们给定 $g(\\mathbf{x}) = f(P\\mathbf{x})$，其中 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是可微的，并且 $P\\in M_{n}(\\mathbb{R})$ 是线性的和幂等的。为了确定 $\\nabla g(\\mathbf{x})$，我们使用欧几里得空间之间可微映射的链式法则。\n\n首先，由于 $P$ 是线性的，它在任何点 $\\mathbf{x}$ 处的导数都是它本身这个常数矩阵：\n$$\nD(P\\mathbf{x}) = P.\n$$\n接下来，$f$ 在点 $\\mathbf{y}\\in\\mathbb{R}^{n}$ 处的导数（雅可比矩阵）是 $1\\times n$ 的行向量 $Df(\\mathbf{y}) = \\left(\\nabla f(\\mathbf{y})\\right)^{T}$。将链式法则应用于 $g(\\mathbf{x})=f(P\\mathbf{x})$ 得到\n$$\nDg(\\mathbf{x}) = Df(P\\mathbf{x})\\,D(P\\mathbf{x}) = Df(P\\mathbf{x})\\,P.\n$$\n根据给定的约定，梯度是雅可比矩阵的转置：\n$$\n\\nabla g(\\mathbf{x}) = \\left(Dg(\\mathbf{x})\\right)^{T} = \\left(Df(P\\mathbf{x})\\,P\\right)^{T} = P^{T}\\left(Df(P\\mathbf{x})\\right)^{T}.\n$$\n由于 $\\left(Df(P\\mathbf{x})\\right)^{T} = \\nabla f(P\\mathbf{x})$，我们得出结论\n$$\n\\nabla g(\\mathbf{x}) = P^{T}\\,\\nabla f(P\\mathbf{x}).\n$$\n此推导不需要幂等性 $P^{2}=P$；只用到了 $P$ 的线性和 $f$ 的可微性。", "answer": "$$\\boxed{P^{T}\\,\\nabla f(P\\mathbf{x})}$$", "id": "2330045"}]}