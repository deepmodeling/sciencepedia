{"hands_on_practices": [{"introduction": "主成分分析 (PCA) 的核心在于其数学基础。通过手动计算一个小型数据集，我们可以揭开 PCA 的神秘面纱，并真正掌握其工作原理。这个练习将指导你完成 PCA 的关键步骤——数据中心化、协方差矩阵的构建以及特征向量的求解，从而为理解更复杂的应用奠定坚实的数学基础 [@problem_id:2416060]。", "problem": "一项基因表达实验测量了基因 $G_1$、$G_2$ 和 $G_3$ 在样本 $S_1$、$S_2$、$S_3$ 和 $S_4$ 中的对数转换后的表达值。数据矩阵 $X \\in \\mathbb{R}^{4 \\times 3}$ 以样本为行，基因为列：\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\n将样本视为独立观测值，基因视为变量。使用主成分分析 (PCA)，通过以下步骤计算基因空间中的第一主成分载荷向量：\n- 对 $X$ 进行列中心化，\n- 构建基因间的样本协方差矩阵，其中样本数 $n=4$，分母为 $n-1$，并且\n- 取该协方差矩阵对应于最大特征值的单位范数特征向量。\n\n将载荷向量以 $1 \\times 3$ 行矩阵的形式给出，按 $(G_1, G_2, G_3)$ 的顺序排列，并选择符号以使其第一个非零元素为正。无需四舍五入。", "solution": "题目要求我们通过对基因间的样本协方差矩阵进行特征分解，来计算基因空间中的第一主成分载荷向量。令样本数为 $n=4$，基因数为 $p=3$。数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中行为样本，列为基因。\n\n第一步：列中心化。计算 $X$ 的列均值：\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\n从每列中减去这些均值，得到中心化矩阵 $Z$：\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\n第二步：基因间的样本协方差矩阵。使用分母 $n-1=3$，基因的样本协方差为\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\n注意到 $Z$ 的每一行都是 $(1,\\,1,\\,1)$ 的标量倍，所以三个中心化后的基因列是相同的。计算 $Z^{\\top}Z$ 时，注意到对于任意两列 $j$ 和 $k$，其 $(j,k)$ 元素等于 $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$。由于所有三列都相同，$Z^{\\top}Z$ 的每个元素都等于单个中心化列的平方和：\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\n因此，\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\n第三步：特征分解。令 $J \\in \\mathbb{R}^{3 \\times 3}$ 表示全一矩阵，即对所有 $j,k$ 都有 $J_{jk}=1$。根据基本原理可知，$J$ 的秩为 1，其特征值为 $3$ 和 $0$ (重数为 $2$)。对应于特征值 $3$ 的一个单位范数特征向量与 $(1,\\,1,\\,1)^{\\top}$ 成比例，具体为\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n由于 $S = \\frac{5}{3} J$，$S$ 的特征值为 $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ 和 $\\lambda_{2} = 0$，$\\lambda_{3} = 0$，其特征向量与 $J$ 的相同。因此，基因空间中的第一主成分载荷向量是与 $\\lambda_{1}=5$ 相关联的单位范数特征向量，即上面所示的 $v$。选择符号以使第一个非零元素为正，得到\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\n因此，按 $(G_1, G_2, G_3)$ 顺序排列并写成 $1 \\times 3$ 行矩阵形式的第一主成分载荷向量是\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2416060"}, {"introduction": "从理论转向实践时，我们必须认识到 PCA 对数据尺度的敏感性。这个练习通过一个计算任务，生动地展示了为什么在应用 PCA 之前进行数据标准化至关重要。你将通过编程模拟一个金融数据集，直观地比较原始数据与标准化数据分析结果的差异，从而理解不当的数据预处理会如何扭曲我们对数据主要变异方向的认知 [@problem_id:2421735]。", "problem": "要求您使用主成分分析（Principal Component Analysis, PCA）的基本原理，演示当变量以不同单位度量时，若未能对其进行标准化，会如何扭曲所估计的主方向和解释方差。您将在一个纯数学框架下，通过一个模拟典型金融变量（如价格和交易量）的合成数据生成过程来完成此项工作。您需要实现完整的分析流程，并报告量化诊断指标，以比较对原始数据执行 PCA 与对标准化数据执行 PCA 的结果。\n\n基本原理：\n- PCA 旨在寻找使样本方差最大化的正交方向。给定一个中心化的数据矩阵 $X \\in \\mathbb{R}^{T \\times n}$，其样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^\\top X$。主成分是 $\\Sigma$ 的特征向量，按其对应的特征值从大到小排序。\n- 标准化将每个变量 $x_j$ 转换为 $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$，其中 $\\bar{x}_j$ 是样本均值，$\\hat{\\sigma}_j$ 是样本标准差，从而使得每个标准化后的变量都具有单位样本方差。对标准化数据进行 PCA 等同于对样本相关系数矩阵进行 PCA。\n- 将对角缩放 $D = \\operatorname{diag}(s_1,\\dots,s_n)$ 应用于变量，即 $X \\mapsto X D$，会使协方差矩阵的元素乘以 $s_i s_j$，从而改变特征向量，除非所有的 $s_j$ 都相等。\n\n数据生成过程：\n- 对于每个测试用例 $k$，固定样本量 $T_k \\in \\mathbb{N}$、变量数量 $n_k \\in \\mathbb{N}$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、特异性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n- 对于 $t = 1,\\dots,T_k$，生成一个单一共同因子 $f_t \\sim \\mathcal{N}(0,1)$，并生成特异性噪声 $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$。所有这些因子和噪声在 $t$ 和 $j$ 上均相互独立。\n- 对于 $t=1,\\dots,T_k$ 和 $j=1,\\dots,n_k$，构建原始观测值 $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$。\n- 在计算任何协方差之前，通过减去各列的样本均值来对矩阵 $X$ 进行中心化。\n\n每个测试用例的计算任务：\n- 从中心化的原始数据 $X$ 计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$，并获取第一主成分的特征向量 $v_{\\text{raw}}$（单位范数）及其特征值 $\\lambda_{\\text{raw}}$。\n- 将 $X$ 的每一列进行标准化，使其样本方差为 1，从而得到矩阵 $Z$。计算 $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$（即样本相关系数矩阵），并获取第一主成分的特征向量 $v_{\\text{std}}$（单位范数）及其特征值 $\\lambda_{\\text{std}}$。\n- 计算角度 $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$；以弧度为单位报告 $\\theta$。\n- 计算解释方差份额的差异 $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$，该值必须以小数形式报告（而非百分比）。\n\n随机性与可复现性：\n- 在整个实验中使用固定的伪随机数生成器种子 $314159$，以确保结果的可复现性。\n\n测试套件：\n- 共有 $3$ 个测试用例。对于每个测试用例 $k$，使用以下参数 $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$：\n  - 用例 $1$（单位相似，两个变量）：\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$。\n  - 用例 $2$（单位不匹配，两个变量：其中一个因尺度而占主导地位）：\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$。\n  - 用例 $3$（单位不匹配，三个变量：一个巨大尺度，一个中等尺度，一个微小尺度）：\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$。\n\n每个测试用例的必需输出：\n- 一个包含两个浮点数的列表 $[\\theta, \\Delta]$，其中 $\\theta$ 是以弧度为单位的角度，$\\Delta$ 是解释方差份额的绝对差值。两个值都必须四舍五入到恰好 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由各用例列表组成的逗号分隔列表，并用方括号括起来，例如 $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$。每个浮点数都需四舍五入到恰好 $6$ 位小数，角度以弧度为单位。", "solution": "所提出的问题是计算统计学中一个有效且定义明确的练习，旨在具体演示主成分分析（Principal Component Analysis, PCA）对变量尺度的敏感性。该问题在科学上是合理的，它基于线性代数和统计学的基础原理，并且所有参数和程序都已足够清晰地指定，从而能够得出一个唯一的、可验证的解。我们将着手进行分析。\n\n其核心论点是，PCA作为一种方差最大化技术，不具有尺度不变性。当变量以迥异的单位度量时（例如，以美元计的股价与以百万股计的交易量），方差最大的变量将在机制上主导第一个主成分。这通常是所选单位造成的人为结果，而非真实潜在重要性的指标。标准化是标准的补救方法，它将所有变量转换到同一尺度（单位方差），从而使分析专注于数据的相关性结构，而非任意的度量尺度。\n\n我们首先将数据生成和分析流程进行形式化。\n\n**1. 数据生成过程**\n\n对于每个测试用例 k，我们给定样本量 $T_k$、变量数量 $n_k$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、特异性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n\n数据由一个单因子模型生成。对于每个时间点 $t=1, \\dots, T_k$，从标准正态分布中抽取一个共同潜因子 $f_t \\sim \\mathcal{N}(0, 1)$。对于每个变量 $j=1, \\dots, n_k$，从 $\\mathcal{N}(0, (u^{(k)}_j)^2)$ 中抽取一个特异性噪声项 $e_{t,j}$。所有的 $f_t$ 和 $e_{t,j}$ 均相互独立。\n\n在时间点 $t$ 变量 $j$ 的观测值构建如下：\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\n这就构成了一个数据矩阵 $X \\in \\mathbb{R}^{T_k \\times n_k}$，其列代表不同的变量。尺度因子 $s^{(k)}_j$ 代表了变量 $j$ 的任意度量单位。\n\n**2. 对原始数据进行 PCA（基于协方差的 PCA）**\n\nPCA 的第一步是通过减去按列计算的样本均值来对数据进行中心化。设 $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ 为第 $j$ 个变量的样本均值。中心化后的数据矩阵记为 $X_c$，其元素为 $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$。\n\n然后计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$：\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\n主成分是 $\\Sigma_{\\text{raw}}$ 的特征向量。我们对该矩阵进行特征分解：\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\n其中 $V$ 是正交特征向量组成的矩阵，$\\Lambda$ 是由相应特征值组成的对角矩阵。特征值按降序排列，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$。第一主成分是与最大特征值 $\\lambda_1$ 相关联的特征向量 $v_1$。在此问题中，我们将此特征向量记为 $v_{\\text{raw}}$，特征值记为 $\\lambda_{\\text{raw}}$。\n\n**3. 对标准化数据进行 PCA（基于相关系数的 PCA）**\n\n为消除任意尺度的影响，我们对数据进行标准化。对于原始数据矩阵 $X$ 的每一列 $j$，我们计算其样本标准差 $\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$。\n\n标准化的数据矩阵 $Z$ 由以下元素构成：\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\n根据构造， $Z$ 的每一列的样本均值为 $0$，样本方差为 $1$。\n\n然后对这个标准化数据 $Z$ 进行 PCA。相关的矩阵是 $Z$ 的样本协方差矩阵，我们记为 $\\Sigma_{\\text{std}}$：\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\n由于 $Z$ 的每一列都具有单位方差，$\\Sigma_{\\text{std}}$ 的对角元素全为 $1$，而非对角元素 $(i, j)$ 是原始变量 $x_i$ 和 $x_j$ 之间的样本相关系数。因此，$\\Sigma_{\\text{std}}$ 是 $X$ 的样本相关系数矩阵。\n\n我们对 $\\Sigma_{\\text{std}}$ 进行特征分解，以找到其最大特征值 $\\lambda_{\\text{std}}$ 和对应的特征向量 $v_{\\text{std}}$。\n\n**4. 诊断指标**\n\n为了量化因未进行标准化而造成的扭曲，我们计算两个指标：\n\n- **主成分之间的夹角**：主成分方向 $v_{\\text{raw}}$ 和 $v_{\\text{std}}$ 是 $\\mathbb{R}^{n_k}$ 中的单位向量。它们之间的夹角衡量了最大方差方向的偏移程度。由于特征向量的定义只到符号为止（即，如果 $v$ 是一个特征向量，$-v$ 也是），我们计算它们所张成的直线之间的锐角：\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  $\\theta=0$ 的值表示完全对齐，而一个大的角度（接近 $\\pi/2$）则表示严重错位。\n\n- **解释方差份额的差异**：第一主成分所解释的总方差比例由其特征值除以所有特征值的总和给出。特征值的总和等于矩阵的迹，即 $\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$，代表了数据中的总方差。我们计算解释方差份额的绝对差异：\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  请注意，对于标准化数据，$\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$，即变量的数量。一个大的 $\\Delta$ 值表明两种方法对第一主成分重要性的评估差异巨大。\n\n该流程将使用指定的参数和固定的随机种子对每个测试用例执行，以保证可复现性。预计结果将显示，用例 1（尺度相似）的扭曲最小，而用例 2 和 3（尺度迥异）的扭曲显著，从而验证了在实践中进行标准化的必要性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421735"}, {"introduction": "在处理大规模数据集时，传统 PCA 方法的计算成本可能高得令人望而却步。本练习旨在让你掌握一种适用于“大数据”场景的先进技术——幂迭代法。你将学习如何通过一种内存高效的流式算法，在无法显式构建协方差矩阵的情况下，有效地计算出数据的第一主成分，这对于处理现实世界中的海量数据至关重要 [@problem_id:2421742]。", "problem": "在计算经济学和金融学中，给定一个由单一潜在市场因子驱动的资产收益数据生成模型。令 $R \\in \\mathbb{R}^{n \\times d}$ 表示资产收益矩阵，其中 $n$ 是时间周期数，$d$ 是资产数量。行对应时间，列对应资产。数据按如下方式生成\n$$\nR_{t,\\cdot} \\;=\\; z_t \\, a^\\top \\;+\\; \\varepsilon_{t,\\cdot}, \\quad \\text{for } t \\in \\{1,\\dots,n\\},\n$$\n其中 $z_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的标量因子实现，$a \\in \\mathbb{R}^d$ 是一个固定的载荷向量，$\\varepsilon_{t,\\cdot} \\in \\mathbb{R}^d$ 的各分量独立同分布于 $\\mathcal{N}(0,\\sigma^2)$，且与 $\\{z_t\\}_{t=1}^n$ 相互独立。列中心化数据的样本协方差矩阵为\n$$\nS \\;=\\; \\frac{1}{n-1} \\, X^\\top X,\n$$\n其中 $X$ 是通过使用每列的经验均值对 $R$ 进行列中心化而形成的。\n\n目标是仅计算第一个主成分载荷向量，即求解以下方程的单位向量 $v^\\ast \\in \\mathbb{R}^d$\n$$\nS v^\\ast \\;=\\; \\lambda_1 \\, v^\\ast, \\quad \\|v^\\ast\\|_2 \\;=\\; 1,\n$$\n其中 $\\lambda_1$ 是 $S$ 的最大特征值。您必须假设显式地构造 $S$ 是不可行的。您只能通过以块（连续的行块）的形式流式传输其行来访问 $R$，并且可以对数据进行多遍处理。您必须使用从数据中计算出的经验均值来对列进行中心化。您的程序不得构造 $X^\\top X$，并且使用的内存必须与 $d$ 呈线性关系。\n\n为进行测试，您的程序必须使用上述模型在内部为三个独立的案例生成 $R$，并为每个案例计算两个量：\n1. 计算出的单位向量 $\\hat v$ 与真实载荷向量的单位范数版本 $\\bar a = a / \\|a\\|_2$ 之间的绝对余弦相似度，由下式给出\n$$\nc \\;=\\; \\left| \\hat v^\\top \\bar a \\right|.\n$$\n2. 第一个主成分的方差解释比，定义为\n$$\n\\rho \\;=\\; \\frac{\\hat \\lambda_1}{\\operatorname{tr}(S)},\n$$\n其中 $\\hat \\lambda_1$ 通过瑞利商估计\n$$\n\\hat \\lambda_1 \\;=\\; \\hat v^\\top S \\hat v \\;=\\; \\frac{1}{n-1} \\, \\| X \\hat v \\|_2^2,\n$$\n且 $\\operatorname{tr}(S)$ 是 $S$ 的迹，等价于\n$$\n\\operatorname{tr}(S) \\;=\\; \\frac{1}{n-1} \\, \\| X \\|_F^2.\n$$\n所有范数均为所指示的标准欧几里德范数或弗罗贝尼乌斯范数。\n\n您的程序必须使用下面确切的参数集，以流式方式（不存储完整矩阵）模拟数据。在每种情况下，向量 $a$ 都按指定的方式确定性地定义，并且在计算 $c$ 之前必须将其归一化为单位范数。\n\n案例 A（一般良态）：\n- $n = 5000$，$d = 50$，$\\sigma = 0.5$，随机种子 $s = 42$，块大小 $m = 250$。\n- 载荷 $a \\in \\mathbb{R}^{50}$ 定义为 $a_i = i$，其中 $i \\in \\{1,\\dots,50\\}$。\n\n案例 B（高维，$d \\gt n$）：\n- $n = 120$，$d = 600$，$\\sigma = 0.6$，随机种子 $s = 7$，块大小 $m = 64$。\n- 载荷 $a \\in \\mathbb{R}^{600}$ 定义为 $a_i = (-1)^i \\left(0.5 + \\frac{i}{d}\\right)$，其中 $i \\in \\{1,\\dots,600\\}$。\n\n案例 C（特征间隙小，收敛慢）：\n- $n = 4000$，$d = 100$，$\\sigma = 0.97$，随机种子 $s = 123$，块大小 $m = 200$。\n- 载荷 $a \\in \\mathbb{R}^{100}$ 定义为 $a_i = \\sin\\!\\left(\\frac{2\\pi i}{d}\\right) + 0.1 \\frac{i}{d}$，其中 $i \\in \\{1,\\dots,100\\}$，$\\pi$ 是圆周率。\n\n在所有情况下，随机种子 $s$ 控制 $\\{z_t\\}$ 和噪声 $\\{\\varepsilon_{t,\\cdot}\\}$ 的生成，并且必须使用它以确保每次对数据的处理都能复现相同的序列。您最多可以使用固定次数的迭代（选择合适的值）和数值容差，以便在方向稳定时提前终止迭代。如果在内部使用了角度，无需报告。不涉及物理单位。\n\n您的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表，每个内部列表按 A、B、C 的顺序包含两个浮点数 $[c,\\rho]$，并四舍五入到六位小数。例如，输出格式必须严格为\n$$\n\\big[ [c_A,\\rho_A], [c_B,\\rho_B], [c_C,\\rho_C] \\big],\n$$\n作为单行打印，例如\n$$\n\\texttt{[[0.999000,0.850000],[0.990000,0.300000],[0.950000,0.600000]]}.\n$$", "solution": "所提出的问题是计算金融学中一个良态且有科学依据的任务。它要求在内存和数据访问限制下计算样本协方差矩阵的第一个主成分，这是大规模统计分析中的一个标准问题。所有参数和条件都以数学精度进行了规定。因此，该问题是有效的，我将着手解决它。\n\n问题的核心是找到样本协方差矩阵 $S = \\frac{1}{n-1} X^\\top X$ 的最大特征值 $\\lambda_1$ 对应的单位范数特征向量 $v^\\ast$。这里，$X \\in \\mathbb{R}^{n \\times d}$ 是资产收益的数据矩阵，通过减去列向经验均值进行了中心化。关键的约束是，无论是完整的数据矩阵 $R \\in \\mathbb{R}^{n \\times d}$ 还是矩阵 $X^\\top X \\in \\mathbb{R}^{d \\times d}$ 都不能被显式地构造或存储在内存中。数据只能通过以连续块的形式流式传输行来访问。\n\n这种设定需要使用迭代算法。幂迭代法是寻找与矩阵最大模特征值相关联的特征向量的经典算法。该方法从一个初始随机向量 $v_0$ 开始，迭代地应用矩阵 $S$ 以生成一个收敛到主特征向量的向量序列：\n$$\nv_{k+1} = \\frac{S v_k}{\\|S v_k\\|_2}\n$$\n在迭代过程中，$S$ 定义中的常数因子 $\\frac{1}{n-1}$ 可以省略，因为向量在每一步都会被重新归一化。因此，核心计算是矩阵-向量乘积 $w_k = X^\\top X v_k$。\n\n为了在流式约束下实现这一点，我们必须在不显式构造 $X$ 或 $X^\\top X$ 的情况下执行计算。\n首先，数据必须被中心化。中心化后的数据矩阵是 $X = R - \\mathbf{1}\\mu^\\top$，其中 $\\mathbf{1}$ 是一个 $n \\times 1$ 的全一向量，$\\mu \\in \\mathbb{R}^d$ 是列均值向量。均值计算为 $\\mu = \\frac{1}{n} \\sum_{t=1}^n R_{t,\\cdot}$，这需要对数据矩阵 $R$ 进行一次完整的遍历。我们可以计算并存储向量 $\\mu$，因为其内存占用 $O(d)$ 是允许的。\n\n在已知 $\\mu$ 的情况下，乘积 $w_k = X^\\top (X v_k)$ 可以在每次迭代 $k$ 中通过对数据进行单次遍历来计算。该乘积可以表示为对 $X$ 的所有行 $x_{t,\\cdot}$ 的求和：\n$$\nw_k = X^\\top (X v_k) = \\sum_{t=1}^{n} x_{t,\\cdot}^\\top (x_{t,\\cdot} v_k)\n$$\n在遍历过程中，对于数据流中的每一行 $R_{t,\\cdot}$，我们首先计算中心化后的行 $x_{t,\\cdot} = R_{t,\\cdot} - \\mu$。然后，我们计算标量投影 $p_t = x_{t,\\cdot} v_k$。这个标量用于缩放向量 $x_{t,\\cdot}^\\top$，结果被加到一个累加向量中，在遍历结束时，该向量即为所求的向量 $w_k$。然后，特征向量的新估计值为 $v_{k+1} = w_k / \\|w_k\\|_2$。重复这个迭代过程，直到向量 $v_k$ 的变化变得可以忽略不计，例如，当对于某个很小的容差 $\\epsilon > 0$ 有 $1 - |v_k^\\top v_{k-1}| < \\epsilon$ 时。\n\n对于每一次遍历，$R$ 的数据生成过程必须是可复现的。这可以通过实现一个生成器函数来完成，该函数在每次遍历开始时都用特定的种子 $s$ 重新初始化其随机数生成器。\n\n在幂迭代收敛到一个稳定的特征向量 $\\hat{v}$ 后，问题要求计算两个指标：绝对余弦相似度 $c = |\\hat{v}^\\top \\bar{a}|$ 和方差解释比 $\\rho = \\hat{\\lambda}_1 / \\operatorname{tr}(S)$。\n真实的载荷向量 $a$ 是给定的，其单位范数版本 $\\bar{a}$ 计算为 $\\bar{a} = a / \\|a\\|_2$。$c$ 的值可以直接得出。\n\n方差解释比 $\\rho$ 需要计算 $\\hat{\\lambda}_1$ 和 $\\operatorname{tr}(S)$。它们由以下公式给出：\n$$\n\\hat{\\lambda}_1 = \\hat{v}^\\top S \\hat{v} = \\frac{1}{n-1} \\|X\\hat{v}\\|_2^2\n$$\n$$\n\\operatorname{tr}(S) = \\frac{1}{n-1} \\operatorname{tr}(X^\\top X) = \\frac{1}{n-1} \\|X\\|_F^2\n$$\n该比率简化为 $\\rho = \\frac{\\|X\\hat{v}\\|_2^2}{\\|X\\|_F^2}$。$\\|X\\hat{v}\\|_2^2 = \\sum_{t=1}^n (x_{t,\\cdot}\\hat{v})^2$ 和弗罗贝尼乌斯范数的平方 $\\|X\\|_F^2 = \\sum_{t=1}^n \\sum_{j=1}^d X_{t,j}^2 = \\sum_{t=1}^n \\|x_{t,\\cdot}\\|_2^2$ 都可以在对数据的最后一次遍历中高效地计算出来。在这次遍历中，我们初始化两个标量累加器为零，对于每个中心化后的行 $x_{t,\\cdot}$，我们将 $(x_{t,\\cdot}\\hat{v})^2$ 加到第一个累加器，将 $\\|x_{t,\\cdot}\\|_2^2$ 加到第二个累加器。\n\n完整的算法如下：\n1.  **均值计算（第一遍）**：对数据流进行一次遍历以计算均值向量 $\\mu$。\n2.  **幂迭代（多遍）**：初始化一个随机单位向量 $v$。进行固定次数的迭代或直到收敛为止，每次迭代都对数据进行一次遍历以计算 $w = X^\\top(Xv)$ 并更新 $v \\leftarrow w/\\|w\\|_2$。\n3.  **指标计算（最后一遍）**：收敛到 $\\hat{v}$ 后，进行最后一次遍历以计算 $\\|X\\hat{v}\\|_2^2$ 和 $\\|X\\|_F^2$。\n4.  **最终计算**：根据收集到的统计数据和真实的载荷向量 $a$ 计算 $c$ 和 $\\rho$。\n\n这种多遍、流式的方法遵守了所有问题约束，其内存使用与 $O(d)$ 成比例，并以可管理的块处理数据。", "answer": "```python\nimport numpy as np\n\ndef generate_R_chunks(n, d, a, sigma, seed, chunk_size):\n    \"\"\"\n    Generator for streaming the data matrix R in chunks.\n    A new np.random.RandomState is created to ensure reproducibility for each pass.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    current_pos = 0\n    while current_pos < n:\n        size = min(chunk_size, n - current_pos)\n        \n        # Generate z_t and epsilon_t for the whole chunk for efficiency\n        z = rng.normal(0, 1, size=(size, 1))\n        epsilon = rng.normal(0, sigma, size=(size, d))\n        \n        # R_chunk = z * a + epsilon\n        chunk = z * a.reshape(1, d) + epsilon\n        \n        yield chunk\n        current_pos += size\n\ndef solve_pca_streaming(n, d, a, sigma, seed, chunk_size, max_iter=100, tol=1e-9):\n    \"\"\"\n    Computes the first PC and related metrics using a streaming algorithm.\n    \"\"\"\n    # Pass 1: Compute column means\n    mean_vec = np.zeros(d)\n    data_stream_mean = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n    for chunk in data_stream_mean:\n        mean_vec += np.sum(chunk, axis=0)\n    mu = mean_vec / n\n\n    # Power Iteration to find the first principal component vector\n    v = np.random.rand(d)\n    v /= np.linalg.norm(v)\n\n    for _ in range(max_iter):\n        v_old = v\n        w = np.zeros(d)\n\n        # New pass for this iteration to compute w = X^T @ (X @ v)\n        data_stream_iter = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n        for chunk in data_stream_iter:\n            X_chunk = chunk - mu\n            p_chunk = X_chunk @ v\n            w += X_chunk.T @ p_chunk\n\n        norm_w = np.linalg.norm(w)\n        if norm_w == 0:\n            # Should not happen in this problem\n            break\n        \n        v = w / norm_w\n\n        # Check for convergence (sign-invariant)\n        if 1 - abs(v @ v_old) < tol:\n            break\n    \n    v_hat = v\n\n    # Final pass: Compute metrics (explained variance ratio)\n    sq_norm_Xv = 0.0\n    sq_frob_norm_X = 0.0\n    data_stream_metrics = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n    for chunk in data_stream_metrics:\n        X_chunk = chunk - mu\n        sq_norm_Xv += np.sum((X_chunk @ v_hat)**2)\n        sq_frob_norm_X += np.sum(X_chunk**2) \n\n    # Explained variance ratio\n    rho = sq_norm_Xv / sq_frob_norm_X if sq_frob_norm_X > 0 else 0.0\n\n    # Absolute cosine similarity\n    a_norm = np.linalg.norm(a)\n    if a_norm == 0:\n        a_bar = a # Or handle as an error\n    else:\n        a_bar = a / a_norm\n    c = abs(v_hat @ a_bar)\n\n    return c, rho\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Define test cases as per the problem statement\n    test_cases = [\n        # Case A: general well-conditioned\n        {'n': 5000, 'd': 50, 'sigma': 0.5, 's': 42, 'm': 250, \n         'a_def': lambda i, d_val: i},\n        # Case B: high-dimensional with d > n\n        {'n': 120, 'd': 600, 'sigma': 0.6, 's': 7, 'm': 64, \n         'a_def': lambda i, d_val: ((-1)**i) * (0.5 + i / d_val)},\n        # Case C: small eigengap, slower convergence\n        {'n': 4000, 'd': 100, 'sigma': 0.97, 's': 123, 'm': 200, \n         'a_def': lambda i, d_val: np.sin(2 * np.pi * i / d_val) + 0.1 * (i / d_val)}\n    ]\n    \n    results = []\n    # Use a fixed seed for initializing the random vector v in power iteration\n    # to ensure the entire program's output is deterministic.\n    np.random.seed(0)\n\n    for case in test_cases:\n        d = case['d']\n        indices = np.arange(1, d + 1)\n        a = case['a_def'](indices, d)\n        \n        c, rho = solve_pca_streaming(\n            n=case['n'], d=d, a=a, \n            sigma=case['sigma'], seed=case['s'], chunk_size=case['m']\n        )\n        results.append((c, rho))\n\n    # Format the output exactly as required\n    formatted_results = [f\"[{c:.6f},{rho:.6f}]\" for c, rho in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421742"}]}