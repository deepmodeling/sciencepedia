{"hands_on_practices": [{"introduction": "理解哈密顿-雅可比-贝尔曼（HJB）方程的核心在于掌握其核心机制：哈密顿量的最小化。本练习将引导您专注于这一关键步骤，即针对一个给定的随机控制问题，定义其哈密顿量，并通过最小化过程推导出最优控制的反馈形式。通过这个练习，您将清晰地看到最优控制策略是如何通过价值函数（及其导数）来表达的 [@problem_id:3080765]。", "problem": "考虑一维受控Itô随机微分方程\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\n漂移为 $b(x,u) = u$，扩散系数为常数 $\\sigma > 0$，控制过程 $u_{t}$ 在 $\\mathbb{R}$ 中取值。设运行成本为\n$$\nf(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2},\n$$\n其中 $r>0$ 且 $q>0$。对于一个具有有界连续终端成本 $g(x)$ 的有限时间域问题，将价值函数 $V(t,x)$ 定义为\n$$\nV(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right],\n$$\n其中下确界取遍所有在 $\\mathbb{R}$ 中取值的循序可测控制。使用动态规划原理以及Hamilton-Jacobi-Bellman (HJB)方程中出现的哈密顿量的定义，在点 $(x,p,M)$ 处的哈密顿量由施加于一个 $C^{2}$ 测试函数的受控生成元与运行成本之和关于控制 $u \\in \\mathbb{R}$ 的下确界给出。特别地，如果 $p$ 表示一阶空间导数，$M$ 表示二阶空间导数，请计算：\n- 最小化的哈密顿量，作为 $x$, $p$, $M$, $\\sigma$, $r$, 和 $q$ 的显式函数；\n- 相应的最小化状态反馈控制，作为 $p$ 和 $r$ 的显式函数。\n\n将你的最终答案以一个双元素行矩阵的形式给出，第一个元素是最小化的哈密顿量，第二个元素是最小化反馈控制。不需要数值近似，也不涉及单位。", "solution": "用户希望我解决一个在随机最优控制背景下，与Hamilton-Jacobi-Bellman (HJB)方程相关的问题。\n\n### 第1步：提取已知条件\n- 受控Itô随机微分方程（SDE）：$\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$。\n- 漂移函数：$b(x,u) = u$。\n- 扩散系数：$\\sigma > 0$ 是一个常数。\n- 控制过程 $u_{t}$ 在 $\\mathbb{R}$ 中取值。\n- 运行成本函数：$f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$。\n- 成本函数中的常数满足：$r>0$ 和 $q>0$。\n- 价值函数对于有限时间域 $[t, T]$ 定义为：$V(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right]$。\n- 终端成本 $g(x)$ 是一个有界连续函数。\n- 在点 $(x,p,M)$ 处的哈密顿量定义为，施加于一个 $C^{2}$ 测试函数的受控生成元与运行成本之和，关于控制 $u \\in \\mathbb{R}$ 的下确界。\n- 变量 $p$ 表示测试函数的一阶空间导数。\n- 变量 $M$ 表示测试函数的二阶空间导数。\n\n### 第2步：使用提取的已知条件进行验证\n对问题进行验证。\n\n- **科学依据**：该问题是随机线性二次调节器（LQR）问题的标准表述，这是最优控制理论的基石。Hamilton-Jacobi-Bellman (HJB)方程是通过动态规划解决此类问题的核心工具。所有概念在随机微分方程和控制理论领域都是标准且成熟的。\n- **适定性**：该问题是适定的。它要求计算两个具体且定义明确的量：最小化的哈密顿量和最优反馈控制。条件 $r>0$ 确保成本函数在控制变量 $u$ 上是严格凸的，这保证了最小值的存在性和唯一性。\n- **客观性**：该问题以精确、客观的数学语言陈述，没有任何主观性或歧义。\n\n该问题没有说明中列出的任何缺陷。它是科学合理的、适定的、客观的、完整的且可形式化的。\n\n### 第3步：结论与行动\n问题是**有效的**。将提供完整解答。\n\n### 解答\n该问题要求计算给定随机控制问题的最小化哈密顿量和相应的最优控制。在Hamilton-Jacobi-Bellman (HJB)方程的背景下，哈密顿量定义为：\n$$\nH(x,p,M) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^{u} + f(x,u) \\right\\}\n$$\n其中 $\\mathcal{L}^{u}$ 是受控随机过程的生成元，作用于一个测试函数（其一阶和二阶空间导数分别用 $p$ 和 $M$ 表示），而 $f(x,u)$ 是运行成本。\n\n首先，我们确定生成元 $\\mathcal{L}^{u}$。对于一个由 $\\mathrm{d}X_{t} = \\mu(t, X_{t})\\,\\mathrm{d}t + \\Sigma(t, X_{t})\\,\\mathrm{d}W_{t}$ 给出的一般一维Itô过程，作用于 $C^{2}$ 函数 $\\phi(x)$ 的生成元是：\n$$\n\\mathcal{L}\\phi(x) = \\mu(t,x) \\frac{\\partial \\phi}{\\partial x} + \\frac{1}{2} \\Sigma(t,x)^{2} \\frac{\\partial^{2} \\phi}{\\partial x^{2}}\n$$\n在我们的具体问题中，SDE是 $\\mathrm{d}X_{t} = u_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$。漂移是 $\\mu(t,x,u) = u$，扩散是 $\\Sigma(t,x,u) = \\sigma$。设 $\\phi(x)$ 是一个通用的 $C^2$ 测试函数。根据问题陈述，我们将其导数表示为 $ p = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}$ 和 $M = \\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}x^2}$。\n因此，受控过程的生成元 $\\mathcal{L}^{u}$ 为：\n$$\n\\mathcal{L}^{u} = u \\cdot p + \\frac{1}{2}\\sigma^{2} \\cdot M\n$$\n接下来，我们加上运行成本 $f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$。需要关于控制 $u \\in \\mathbb{R}$ 最小化的表达式是：\n$$\nJ(u) = \\mathcal{L}^{u} + f(x,u) = \\left( u p + \\frac{1}{2}\\sigma^{2}M \\right) + \\left( \\frac{1}{2} r\\,u^{2} + q\\,x^{2} \\right)\n$$\n我们可以将此表达式重排为关于 $u$ 的函数：\n$$\nJ(u) = \\frac{1}{2} r\\,u^{2} + p\\,u + \\left( q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right)\n$$\n这是一个关于 $u$ 的二次函数。由于问题陈述 $r>0$，该抛物线开口向上，函数具有唯一的全局最小值。为了找到使 $J(u)$ 最小化的 $u$ 值，我们计算关于 $u$ 的一阶导数并将其设为零。\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}u} = \\frac{\\mathrm{d}}{\\mathrm{d}u} \\left( \\frac{1}{2} r\\,u^{2} + p\\,u + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right) = r\\,u + p\n$$\n将导数设为零，得到最优控制，我们记为 $u^{\\star}$：\n$$\nr\\,u^{\\star} + p = 0 \\implies u^{\\star} = -\\frac{p}{r}\n$$\n这就是作为 $p$ 和 $r$ 的函数的最小化状态反馈控制。这是要求答案的第二部分。\n\n为了求得最小化的哈密顿量 $H(x,p,M)$，我们将 $u^{\\star}$ 代回 $J(u)$ 的表达式中：\n$$\nH(x,p,M) = J(u^{\\star}) = \\frac{1}{2} r\\left(-\\frac{p}{r}\\right)^{2} + p\\left(-\\frac{p}{r}\\right) + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{1}{2} r\\left(\\frac{p^{2}}{r^{2}}\\right) - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{p^{2}}{2r} - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n合并包含 $p^{2}$ 的项：\n$$\nH(x,p,M) = -\\frac{p^{2}}{2r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n这就是作为 $x$, $p$, $M$, $\\sigma$, $r$, 和 $q$ 的显式函数的最小化哈密顿量。这是要求答案的第一部分。\n\n最终答案以一个双元素行矩阵的形式呈现。\n第一个元素：最小化的哈密顿量 $H(x,p,M) = q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M$。\n第二个元素：最小化控制 $u^{\\star} = -\\frac{p}{r}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M  -\\frac{p}{r} \\end{pmatrix}}\n$$", "id": "3080765"}, {"introduction": "在掌握了如何将最优控制表示为价值函数的反馈形式后，下一步是求解价值函数本身满足的非线性偏微分方程。本练习将带领您解决一类非常重要的线性二次（LQ）问题，通过引入一个二次型的解的拟设（ansatz），将复杂的HJB偏微分方程转化为一个更易于处理的常微分方程——著名的黎卡提（Riccati）方程。这个过程展示了如何从HJB理论出发，为一类重要问题找到解析解 [@problem_id:3080756]。", "problem": "考虑标量受控随机微分方程 (SDE)\n$$\n\\mathrm{d}X_t = \\big(a X_t + b\\,u_t\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\quad X_0 = x,\n$$\n其中 $W_t$ 是标准布朗运动，$u_t$ 是取值于 $\\mathbb{R}$ 的循序可测控制。对于固定的有限时间域 $T0$，性能准则为\n$$\nJ^{u}(t,x) = \\mathbb{E}\\Bigg[\\int_{t}^{T} \\big(q\\,X_s^2 + r\\,u_s^2\\big)\\,\\mathrm{d}s + s_f\\,X_T^2\\ \\Big|\\ X_t = x\\Bigg].\n$$\n以动态规划原理 (DPP) 和伊藤公式为基础出发点，推导价值函数 $V(t,x) = \\inf_{u} J^{u}(t,x)$ 的 Hamilton-Jacobi-Bellman (HJB) 方程，陈述此设定下的验证定理，并由此推导最优反馈律 $u^{*}(t,x)$。\n\n然后，在时间区间 $[0,T]$（其中 $T0$）上，将参数特化为 $a=0$, $b=1$, $\\sigma0$ (任意), $q=1$, $r=1$ 以及终端权重 $s_f=1$。显式计算最优反馈控制 $u^{*}(t,x)$ 以及在 $u^{*}$ 下状态 SDE 的闭环漂移项 $\\mu_{\\mathrm{cl}}(t,x)$。\n\n你的最终答案必须是数对 $\\big(u^{*}(t,x),\\,\\mu_{\\mathrm{cl}}(t,x)\\big)$ 的单个闭式解析表达式。无需数值舍入。不涉及物理单位。", "solution": "在尝试求解之前，对问题进行验证。\n\n### 步骤 1：提取已知条件\n-   状态动态由标量受控随机微分方程 (SDE) 控制：\n    $$\n    \\mathrm{d}X_t = \\big(a X_t + b\\,u_t\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\quad X_0 = x\n    $$\n-   $W_t$ 是标准布朗运动。\n-   $u_t$ 是取值于 $\\mathbb{R}$ 的循序可测控制。\n-   时间域为固定的有限值 $T0$。\n-   需要最小化的性能准则为：\n    $$\n    J^{u}(t,x) = \\mathbb{E}\\Bigg[\\int_{t}^{T} \\big(q\\,X_s^2 + r\\,u_s^2\\big)\\,\\mathrm{d}s + s_f\\,X_T^2\\ \\Big|\\ X_t = x\\Bigg]\n    $$\n-   价值函数定义为 $V(t,x) = \\inf_{u} J^{u}(t,x)$。\n-   对于问题的第二部分，参数特化为：在时间区间 $[0,T]$ 上，$a=0$, $b=1$, $\\sigma0$ (任意), $q=1$, $r=1$ 以及终端权重 $s_f=1$。\n-   要求输出的是最优反馈控制 $u^{*}(t,x)$ 和由此产生的闭环漂移项 $\\mu_{\\mathrm{cl}}(t,x)$ 组成的数对。\n\n### 步骤 2：使用提取的已知条件进行验证\n所给问题是随机最优控制中的一个标准练习，具体来说，是一个随机系统的线性二次 (LQ) 控制问题。\n\n-   **科学依据**：该问题基于随机微分方程、伊藤微积分和动态规划（特别是 Hamilton-Jacobi-Bellman 框架）等成熟的数学理论。所有组成部分在高等控制理论和金融数学中都是标准的。\n-   **适定性**：该问题是适定的。具有系数 $q \\ge 0$, $r>0$ 和 $s_f \\ge 0$（此处指定为 $q=1$, $r=1$, $s_f=1$）的二次代价函数确保了最小化问题是凸的，并且通常有唯一解。问题提供了推导解所需的所有必要信息。\n-   **客观性**：问题以精确、客观的数学语言陈述，没有任何歧义或主观因素。\n\n### 步骤 3：结论与行动\n该问题是**有效的**，因为它具有科学依据、是适定的且客观的。我们可以继续求解。\n\n### Hamilton-Jacobi-Bellman 方程的推导\n\n推导始于动态规划原理 (DPP)。对于任意小的时间增量 $\\delta t  0$，价值函数 $V(t,x)$ 满足：\n$$\nV(t,x) = \\inf_{u \\in \\mathcal{U}[t,T]} \\mathbb{E}\\left[ \\int_{t}^{t+\\delta t} (qX_s^2 + r u_s^2)\\,\\mathrm{d}s + V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x \\right]\n$$\n其中 $\\mathcal{U}[t,T]$ 是在区间 $[t,T]$ 上的容许控制集合。对于无穷小区间 $[t, t+\\delta t]$，我们可以近似积分并假设控制 $u_s=u_t=u$ 是常数。\n$$\nV(t,x) \\approx \\inf_{u} \\mathbb{E}\\left[ (q x^2 + r u^2)\\delta t + V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x \\right]\n$$\n我们对过程 $V(t,X_t)$ 应用伊藤公式。假设 $V \\in C^{1,2}([0,T) \\times \\mathbb{R})$，其微分为：\n$$\n\\mathrm{d}V(t,X_t) = \\left( \\frac{\\partial V}{\\partial t} + (a X_t + b u_t)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\mathrm{d}t + \\sigma \\frac{\\partial V}{\\partial x} \\mathrm{d}W_t\n$$\n从 $t$ 到 $t+\\delta t$ 积分并取条件期望，随机积分项消失，因为 $\\mathbb{E}[\\int_{t}^{t+\\delta t} (\\dots) \\mathrm{d}W_s \\mid \\mathcal{F}_t] = 0$。\n$$\n\\mathbb{E}[V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x] - V(t,x) = \\mathbb{E}\\left[ \\int_{t}^{t+\\delta t} \\left( \\frac{\\partial V}{\\partial s} + (a X_s + b u_s)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\mathrm{d}s \\mid X_t=x \\right]\n$$\n对于小的 $\\delta t$，这变为：\n$$\n\\mathbb{E}[V(t+\\delta t, X_{t+\\delta t}) \\mid X_t=x] \\approx V(t,x) + \\left( \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\delta t\n$$\n将此代入 DPP 表达式：\n$$\nV(t,x) \\approx \\inf_{u} \\left\\{ (q x^2 + r u^2)\\delta t + V(t,x) + \\left( \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right)\\delta t \\right\\}\n$$\n两边减去 $V(t,x)$，除以 $\\delta t$，并取 $\\delta t \\to 0$ 的极限，我们得到 Hamilton-Jacobi-Bellman (HJB) 方程：\n$$\n0 = \\inf_{u} \\left\\{ \\frac{\\partial V}{\\partial t} + (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + r u^2 \\right\\}\n$$\n这等价于：\n$$\n-\\frac{\\partial V}{\\partial t} = \\inf_{u} \\left\\{ (a x + b u)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + r u^2 \\right\\}\n$$\n待最小化的项，我们称之为哈密顿量 $\\mathcal{H}(t,x,u,V_x)$，是关于 $u$ 的二次函数。通过将其关于 $u$ 的导数设为零来找到最小值：\n$$\n\\frac{\\partial \\mathcal{H}}{\\partial u} = b \\frac{\\partial V}{\\partial x} + 2ru = 0\n$$\n因为对于一个有意义的问题必须有 $r>0$（在本特例中 $r=1$），我们可以求解最优反馈控制 $u^{*}(t,x)$：\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}\n$$\n将其代回 HJB 方程，得到价值函数 $V(t,x)$ 的偏微分方程 (PDE)：\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 + b\\left(-\\frac{b}{2r}\\frac{\\partial V}{\\partial x}\\right)\\frac{\\partial V}{\\partial x} + r\\left(-\\frac{b}{2r}\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 - \\frac{b^2}{2r}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 + \\frac{rb^2}{4r^2}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\n这简化为 HJB 偏微分方程的最终形式：\n$$\n\\frac{\\partial V}{\\partial t} + (a x)\\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + q x^2 - \\frac{b^2}{4r}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\n这个偏微分方程必须从终端条件 $V(T,x) = s_f x^2$ 开始，在时间上向后求解。\n\n### 验证定理\n验证定理正式地将 HJB 方程的解与价值函数联系起来。在此背景下，它可以表述为：\n\n设 $W(t,x) \\in C^{1,2}([0,T] \\times \\mathbb{R})$ 是一个满足 HJB 偏微分方程的函数：\n$$\n\\frac{\\partial W}{\\partial t} + (a x)\\frac{\\partial W}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 W}{\\partial x^2} + q x^2 - \\frac{b^2}{4r}\\left(\\frac{\\partial W}{\\partial x}\\right)^2 = 0\n$$\n其终端条件为 $W(T,x) = s_f x^2$。设 $u^*(t,x) = -\\frac{b}{2r} \\frac{\\partial W}{\\partial x}(t,x)$。如果在此控制下，对于任意初始条件 $(t,x)$，状态 $X_t^*$ 的随机微分方程存在唯一的强解，并且代价函数满足一定的可积性条件，那么 $W(t,x)$ 就是真正的价值函数，即 $W(t,x) = V(t,x)$，并且 $u^*$ 是最优控制。\n\n### 特化与求解\n我们现在将参数特化为 $a=0$, $b=1$, $\\sigma>0$, $q=1$, $r=1$ 和 $s_f=1$。HJB 方程变为：\n$$\n\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + x^2 - \\frac{1}{4}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0\n$$\n终端条件为 $V(T,x) = x^2$。鉴于二次代价和线性动态，我们假设一个二次形式的解：\n$$\nV(t,x) = P(t)x^2 + S(t)\n$$\n偏导数是：\n$\\frac{\\partial V}{\\partial t} = \\dot{P}(t)x^2 + \\dot{S}(t)$，$\\frac{\\partial V}{\\partial x} = 2P(t)x$，$\\frac{\\partial^2 V}{\\partial x^2} = 2P(t)$。\n将这些代入 HJB 方程：\n$$\n(\\dot{P}(t)x^2 + \\dot{S}(t)) + \\frac{1}{2}\\sigma^2(2P(t)) + x^2 - \\frac{1}{4}(2P(t)x)^2 = 0\n$$\n$$\n\\dot{P}(t)x^2 + \\dot{S}(t) + \\sigma^2 P(t) + x^2 - P(t)^2 x^2 = 0\n$$\n按 $x^2$ 的幂次对各项进行分组：\n$$\n(\\dot{P}(t) - P(t)^2 + 1)x^2 + (\\dot{S}(t) + \\sigma^2 P(t)) = 0\n$$\n为使此式对所有 $x \\in \\mathbb{R}$ 成立，$x$ 的各次幂的系数必须为零。这产生了一个由两个常微分方程 (ODE) 组成的系统：\n1.  $\\dot{P}(t) = P(t)^2 - 1$ (Riccati 方程)\n2.  $\\dot{S}(t) = -\\sigma^2 P(t)$\n\n终端条件 $V(T,x) = 1 \\cdot x^2$ 意味着 $P(T)x^2 + S(T) = x^2$，这给出了常微分方程的终端条件：$P(T)=1$ 和 $S(T)=0$。\n\n我们来解 $P(t)$。常微分方程是 $\\frac{\\mathrm{d}P}{\\mathrm{d}t} = P^2-1$。我们观察到 $P_c=1$ 是一个常数（平衡）解，因为 $1^2-1=0$。由于终端条件是 $P(T)=1$，根据常微分方程解的唯一性，解必须是常数函数 $P(t) = 1$，对所有 $t \\in [0,T]$ 成立。\n\n现在，我们来解 $S(t)$。常微分方程是 $\\dot{S}(t) = -\\sigma^2 P(t)$。代入 $P(t)=1$：\n$$\n\\dot{S}(t) = -\\sigma^2\n$$\n对 $t$ 积分：\n$$\nS(t) = \\int -\\sigma^2 \\mathrm{d}t = -\\sigma^2 t + C\n$$\n使用终端条件 $S(T)=0$：\n$$\nS(T) = -\\sigma^2 T + C = 0 \\implies C = \\sigma^2 T\n$$\n因此，$S(t)$ 的解是 $S(t) = \\sigma^2 T - \\sigma^2 t = \\sigma^2 (T-t)$。\n\n价值函数是 $V(t,x) = x^2 + \\sigma^2(T-t)$。\n\n### 最优控制与闭环漂移项\n最优反馈控制由我们推导的一般公式给出：\n$$\nu^{*}(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}\n$$\n当 $b=1$, $r=1$ 且 $\\frac{\\partial V}{\\partial x} = 2P(t)x = 2(1)x = 2x$ 时，我们得到：\n$$\nu^{*}(t,x) = -\\frac{1}{2(1)} (2x) = -x\n$$\n闭环漂移项 $\\mu_{\\mathrm{cl}}(t,x)$ 是通过将 $u^*(t,x)$ 代入原始 SDE 的漂移项 $a x_t + b u_t$ 得到的。当 $a=0$ 且 $b=1$ 时：\n$$\n\\mu_{\\mathrm{cl}}(t,x) = a x + b u^{*}(t,x) = (0)x + (1)(-x) = -x\n$$\n得到的最优控制和闭环漂移项均为 $-x$。它们不依赖于时间 $t$。", "answer": "$$\n\\boxed{\\begin{pmatrix} -x  -x \\end{pmatrix}}\n$$", "id": "3080756"}, {"introduction": "虽然解析解能够提供深刻的洞察，但在实际应用中，绝大多数HJB方程都无法手动求解，必须依赖数值方法。这项高级练习旨在帮助您跨越理论与实践之间的鸿沟，通过从头开始编写一个策略迭代（policy iteration）算法来数值求解HJB方程。这不仅能加深您对动态规划原理的理解，还能让您掌握一种在计算机上解决复杂控制问题的强大工具 [@problem_id:3001638]。", "problem": "从第一性原理出发，实现一个完整的一维平稳折扣Hamilton-Jacobi-Bellman方程的策略迭代方案，该方程源于一个受控随机微分方程。受控状态过程由随机微分方程 $dX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t$ 指定，折扣率为 $\\rho  0$，其中 $a$、$b$、$\\sigma$ 和 $\\rho$ 是固定的实数参数，$u_t \\in \\mathbb{R}$ 是一个控制，$W_t$ 是一个标准维纳过程。运行成本为 $\\ell(x,u) = q x^2 + r u^2$，其中 $q  0$ 且 $r  0$。从初始状态 $x$ 出发，目标是在所有容许控制下，最小化折扣成本 $J^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt\\right]$。令 $V(x)$ 表示价值函数。\n\n您的任务是：\n- 推导 $V(x)$ 在有界区间 $\\left[-x_{\\max}, x_{\\max}\\right]$ 上满足的平稳折扣Hamilton-Jacobi-Bellman方程，其狄利克雷边界条件选择为与该问题的精确无限时域折扣线性二次解 $V^{\\mathrm{ref}}(x)$ 相匹配。已知参考解是二次形式 $V^{\\mathrm{ref}}(x) = P x^2 + C$，其中系数 $P$ 由Hamilton-Jacobi-Bellman方程所蕴含的代数Riccati关系确定，而 $C$ 的选择是为了满足常数扩散项的平稳折扣平衡。\n- 在 $\\left[-x_{\\max}, x_{\\max}\\right]$ 上用 $N$ 个网格点均匀离散化状态空间，网格间距为 $h = 2 x_{\\max}/(N-1)$。在内部节点处，用中心差分算子逼近一阶导数，用标准三点中心差分算子逼近二阶导数。施加狄利克雷边界条件 $V(-x_{\\max}) = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V(x_{\\max}) = V^{\\mathrm{ref}}(x_{\\max})$。\n- 将动作空间离散化为一个有限、对称的集合 $\\mathcal{U}_M = \\left\\{u_j\\right\\}_{j=1}^M$，包含 $M$ 个在 $\\left[-u_{\\max}, u_{\\max}\\right]$ 内均匀分布的值。其中 $u_{\\max}$ 选择为 $u_{\\max} = \\kappa \\left| b P x_{\\max} / r \\right|$，$\\kappa = 1.25$，以确保离散动作网格能覆盖整个计算域上的连续最优动作。\n- 实现策略迭代：\n  - 策略评估：对于每个内部网格点 $x_i$ 的一个固定离散策略 $u(x_i) \\in \\mathcal{U}_M$，求解由离散化的平稳折扣Hamilton-Jacobi-Bellman方程产生的线性系统，以获得网格上的价值向量。\n  - 策略改进：在每个内部网格点 $x_i$ 处，使用当前的离散梯度近似 $\\left(D_x V\\right)(x_i)$，计算离散哈密顿量最小化子 $\\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ r u^2 + b u \\left(D_x V\\right)(x_i) \\right\\}$，并更新策略。\n  - 终止条件：当价值向量和策略向量的上确界范数变化均小于容差 $\\varepsilon$，或当策略迭代次数达到最大值 $K_{\\max}$ 时，终止迭代。\n- 对于下面的每个测试案例，计算网格上的均匀误差 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$，其中 $V_{\\mathrm{num}}$ 是收敛后的数值价值函数，$V^{\\mathrm{ref}}$ 是精确的二次参考解。\n\n使用以下参数集测试套件，每个参数集以元组 $\\left(a, b, q, r, \\sigma, \\rho, x_{\\max}, N, M\\right)$ 的形式提供：\n- 测试 $1$：$\\left(-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61\\right)$。\n- 测试 $2$：$\\left(0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61\\right)$。\n- 测试 $3$：$\\left(0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61\\right)$。\n\n实现细节与要求：\n- 从内部节点上的零策略 $u \\equiv 0$ 开始策略迭代。\n- 使用容差 $\\varepsilon = 10^{-6}$ 和最大迭代次数 $K_{\\max} = 100$。\n- 所有计算都是无量纲的；不需要物理单位。\n- 您的程序必须是一个单一、完整的脚本，能够为所有三个测试执行整个计算过程，无需用户输入，并打印包含结果的单行输出。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起，按测试顺序排列，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$。其中每个条目是相应测试中在状态网格上计算出的 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$ 的浮点数值。", "solution": "用户提供的问题是随机最优控制领域中一个适定且科学合理的问题。它要求使用策略迭代方案对一个平稳折扣Hamilton-Jacobi-Bellman (HJB) 方程进行数值求解。该问题是自洽的，提供了所有必要的参数和方法规范。它基于成熟的数学理论，是计算金融和控制工程中的一个标准问题。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n### 1. Hamilton-Jacobi-Bellman方程与精确解\n\n状态过程由线性随机微分方程控制：\n$$\ndX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t\n$$\n其中 $a$、$b$ 和 $\\sigma$ 是实常数，$u_t \\in \\mathbb{R}$ 是控制，$W_t$ 是一个维纳过程。目标是最小化折扣成本泛函，折扣率为 $\\rho  0$，运行成本为二次形式 $\\ell(x, u) = q x^2 + r u^2$：\n$$\nJ^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt \\;\\middle|\\; X_0 = x \\right]\n$$\n价值函数 $V(x) = \\inf_u J^u(x)$ 满足平稳Hamilton-Jacobi-Bellman (HJB) 方程：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^u V(x) + \\ell(x, u) \\right\\}\n$$\n其中 $\\mathcal{L}^u$ 是过程 $X_t$ 的无穷小生成元，由 $\\mathcal{L}^u V(x) = (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x)$ 给出。代入生成元和运行成本的表达式，HJB方程变为：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x) + q x^2 + r u^2 \\right\\}\n$$\n通过将大括号内表达式关于 $u$ 的偏导数设为零，可以找到关于 $u$ 的下确界：\n$$\nb V'(x) + 2 r u = 0 \\implies u^*(x) = -\\frac{b}{2r} V'(x)\n$$\n将此最优控制 $u^*(x)$ 代回HJB方程，得到一个关于 $V(x)$ 的非线性常微分方程：\n$$\n\\rho V(x) = a x V'(x) - \\frac{b^2}{4r} (V'(x))^2 + \\frac{1}{2} \\sigma^2 V''(x) + q x^2\n$$\n对于这个线性二次问题，已知价值函数是状态的二次函数，$V(x) = P x^2 + C$。其导数为 $V'(x) = 2 P x$ 和 $V''(x) = 2 P$。将这些代入HJB方程得到：\n$$\n\\rho (P x^2 + C) = a x (2 P x) - \\frac{b^2}{4r} (2 P x)^2 + \\frac{1}{2} \\sigma^2 (2 P) + q x^2\n$$\n$$\n\\rho P x^2 + \\rho C = (2aP - \\frac{b^2}{r}P^2 + q) x^2 + \\sigma^2 P\n$$\n通过令 $x$ 的同次幂系数相等，我们得到两个关于未知常数 $P$ 和 $C$ 的代数方程。$x^2$ 的系数产生了连续时间代数Riccati方程 (ARE)：\n$$\n\\frac{b^2}{r} P^2 - (2a - \\rho) P - q = 0\n$$\n由于成本必须是正定的，我们要求 $P  0$。ARE 是一个关于 $P$ 的二次方程，它有一个唯一的正解，由下式给出：\n$$\nP = \\frac{(2a - \\rho) + \\sqrt{(2a - \\rho)^2 + 4(b^2/r)q}}{2(b^2/r)}\n$$\n令常数项相等，得到 $C$ 的方程：\n$$\n\\rho C = \\sigma^2 P \\implies C = \\frac{\\sigma^2 P}{\\rho}\n$$\n这定义了在无限域上的精确参考解 $V^{\\mathrm{ref}}(x) = Px^2 + C$。\n\n### 2. 离散化\n\n问题在一个有界域 $x \\in [-x_{\\max}, x_{\\max}]$ 上求解，该域被离散化为一个包含 $N$ 个点的均匀网格 $\\{x_i\\}_{i=0}^{N-1}$，间距为 $h = 2x_{\\max}/(N-1)$。令 $V_i$ 为 $V(x_i)$ 的数值近似。在内部网格点 $x_i$（其中 $i \\in \\{1, \\dots, N-2\\}$），导数使用中心有限差分进行近似：\n$$\nV'(x_i) \\approx \\frac{V_{i+1} - V_{i-1}}{2h}, \\qquad V''(x_i) \\approx \\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\n$$\n对于一个固定的策略 $u(x)$（离散表示为 $u_i = u(x_i)$），HJB方程在 $V$ 中是线性的。在每个内部点 $x_i$ 的离散形式为：\n$$\n\\rho V_i = (a x_i + b u_i) \\left(\\frac{V_{i+1} - V_{i-1}}{2h}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\\right) + q x_i^2 + r u_i^2\n$$\n整理各项，我们得到一个关于 $V_{i-1}, V_i, V_{i+1}$ 的线性方程：\n$$\n\\left( \\frac{a x_i + b u_i}{2h} - \\frac{\\sigma^2}{2h^2} \\right) V_{i-1} + \\left( \\rho + \\frac{\\sigma^2}{h^2} \\right) V_i - \\left( \\frac{a x_i + b u_i}{2h} + \\frac{\\sigma^2}{2h^2} \\right) V_{i+1} = q x_i^2 + r u_i^2\n$$\n边界条件为狄利克雷类型，固定为参考解的值：$V_0 = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V_{N-1} = V^{\\mathrm{ref}}(x_{\\max})$。\n\n### 3. 策略迭代算法\n\n策略迭代是一种迭代方法，交替执行两个步骤：策略评估和策略改进。\n\n**初始化**：算法从一个初始策略开始，该策略被设置为零策略，$u^{(0)}(x_i) = 0$，适用于所有内部网格点 $x_i$。初始价值函数 $V^{(0)}$ 初始化为零，边界值由参考解设定。\n\n**策略评估**：对于给定的策略 $u^{(k)}$，我们求解相应的价值函数 $V^{(k+1)}$。所有内部点 $i \\in \\{1, \\dots, N-2\\}$ 的离散HJB方程集合构成一个关于未知值 $\\{V_i^{(k+1)}\\}_{i=1}^{N-2}$ 的三对角线性方程组。这个形如 $\\mathbf{A}\\mathbf{V} = \\mathbf{d}$ 的系统使用标准线性代数求解器求解。矩阵 $\\mathbf{A}$ 和向量 $\\mathbf{d}$ 依赖于当前策略 $u^{(k)}$。\n\n**策略改进**：计算出新的价值函数 $V^{(k+1)}$ 后，通过在每个内部网格点 $x_i$ 最小化离散哈密顿量来更新策略为 $u^{(k+1)}$：\n$$\nu_i^{(k+1)} = \\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ (a x_i + b u) \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h} + \\frac{1}{2}\\sigma^2 (\\dots) + q x_i^2 + r u^2 \\right\\}\n$$\n这等价于在离散动作集 $\\mathcal{U}_M$ 上最小化 $r u^2 + b u \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。无约束最小化子为 $u_i^* = -\\frac{b}{2r} \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。然后，新的策略动作 $u_i^{(k+1)}$ 被选为离散动作集 $\\mathcal{U}_M$ 中最接近 $u_i^*$ 的元素。\n\n**终止条件**：评估和改进之间的迭代持续进行，直到策略和价值函数收敛。当价值向量的最大绝对变化 $\\|V^{(k+1)} - V^{(k)}\\|_{\\infty}$ 和策略向量的最大绝对变化 $\\|u^{(k+1)} - u^{(k)}\\|_{\\infty}$ 均低于指定的容差 $\\varepsilon = 10^{-6}$，或达到最大迭代次数 $K_{\\max} = 100$ 时，过程终止。\n\n### 4. 误差计算\n\n在策略迭代收敛到一个最终的数值解 $V_{\\mathrm{num}}$ 后，通过计算其与精确参考解 $V^{\\mathrm{ref}}$ 在计算网格上的误差的均匀范数来评估其准确性：\n$$\n\\text{误差} = \\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty} = \\max_{i \\in \\{0, \\dots, N-1\\}} | V_{\\mathrm{num}}(x_i) - V^{\\mathrm{ref}}(x_i) |\n$$\n为每个提供的测试案例计算此量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the policy iteration for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (a, b, q, r, sigma, rho, x_max, N, M)\n        (-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61),\n        (0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61),\n        (0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61),\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_policy_iteration(params)\n        results.append(error)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef run_policy_iteration(params):\n    \"\"\"\n    Solves the HJB equation for a single set of parameters using policy iteration.\n    \"\"\"\n    a, b, q, r, sigma, rho, x_max, N, M = params\n    \n    # --- Step A: Pre-computation and Setup ---\n\n    # 1. Solve the Algebraic Riccati Equation for P\n    # The ARE is (b^2/r)P^2 - (2a - rho)P - q = 0\n    A_ric = b**2 / r\n    B_ric = -(2.0*a - rho)\n    C_ric = -q\n    discriminant = B_ric**2 - 4.0 * A_ric * C_ric\n    # We need the positive root for P\n    P = (-B_ric + np.sqrt(discriminant)) / (2.0 * A_ric)\n\n    # 2. Calculate the constant C\n    C = (sigma**2 * P) / rho\n\n    # 3. Define the reference (exact) value function\n    def v_ref(x):\n        return P * x**2 + C\n\n    # 4. Set up the state and action grids\n    x_grid = np.linspace(-x_max, x_max, N)\n    h = x_grid[1] - x_grid[0]\n    v_ref_grid = v_ref(x_grid)\n\n    kappa = 1.25\n    u_max_val = kappa * abs(b * P * x_max / r)\n    \n    if M > 1 and u_max_val > 0:\n        u_grid = np.linspace(-u_max_val, u_max_val, M)\n    else:\n        u_grid = np.zeros(M)\n    u_step = u_grid[1] - u_grid[0] if M > 1 else 0\n\n    # 5. Set up iteration parameters\n    tol = 1e-6\n    max_iter = 100\n    \n    # --- Step B: Policy Iteration Loop ---\n\n    # Initialization\n    # Policy for interior points (size N-2)\n    current_policy = np.zeros(N - 2)\n    # Value function on the full grid (size N)\n    current_V = np.zeros(N)\n    # Set Dirichlet boundary conditions from the exact solution\n    current_V[0] = v_ref_grid[0]\n    current_V[-1] = v_ref_grid[-1]\n    \n    x_interior = x_grid[1:-1]\n    \n    for k in range(max_iter):\n        \n        # --- 1. Policy Evaluation ---\n        # Solve the linear system A * V_interior = d for V\n        \n        drift_coeff = a * x_interior + b * current_policy\n        \n        # Coefficients of the tridiagonal system\n        L = drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        D = rho + sigma**2 / h**2\n        U = -drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        \n        # Construct the (N-2) x (N-2) system matrix A\n        A = np.diag(D * np.ones(N-2)) + np.diag(U[:-1], k=1) + np.diag(L[1:], k=-1)\n        \n        # Construct the right-hand side vector d\n        d = q * x_interior**2 + r * current_policy**2\n        \n        # Adjust d for boundary conditions\n        d[0] -= L[0] * v_ref_grid[0]\n        d[-1] -= U[-1] * v_ref_grid[-1]\n        \n        # Solve for the new interior values of V\n        try:\n            V_interior_new = np.linalg.solve(A, d)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although not expected here\n            return np.inf\n\n        # Form the full new value function vector\n        V_new = np.concatenate(([v_ref_grid[0]], V_interior_new, [v_ref_grid[-1]]))\n        \n        # --- 2. Policy Improvement ---\n        # Update the policy by minimizing the Hamiltonian\n        \n        # Approximate V'(x) at interior points\n        V_prime_interior = (V_new[2:] - V_new[:-2]) / (2.0 * h)\n        \n        # Compute the unconstrained optimal control\n        u_star_interior = -b / (2.0 * r) * V_prime_interior\n        \n        # Find the closest control in the discrete action space\n        new_policy = np.zeros_like(current_policy)\n        if M > 1 and u_step > 0:\n            indices = np.round((u_star_interior - u_grid[0]) / u_step)\n            indices = np.clip(indices, 0, M - 1).astype(int)\n            new_policy = u_grid[indices]\n            \n        # --- 3. Termination Check ---\n        \n        val_change = np.max(np.abs(V_new - current_V))\n        pol_change = np.max(np.abs(new_policy - current_policy))\n        \n        # Update for the next iteration\n        current_V = V_new\n        current_policy = new_policy\n        \n        if val_change  tol and pol_change  tol:\n            break\n            \n    # --- Step C: Final Error Calculation ---\n    \n    # The converged numerical solution is the last computed value function\n    V_num = current_V\n    error = np.max(np.abs(V_num - v_ref_grid))\n    \n    return error\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3001638"}]}