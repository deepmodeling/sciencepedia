{"hands_on_practices": [{"introduction": "这项实践将引导你完成鲍姆-韦尔奇算法“期望步骤”中的一个关键计算：前向变量的递归计算。通过一个简化的金融市场模型，你将学习如何确定截至某一时间点的观测序列的概率，这是估计模型隐藏状态的基础。[@problem_id:1336466]", "problem": "一个简化的金融市场模型使用隐马尔可夫模型 (HMM) 进行描述。市场可能处于两种隐藏状态之一：$S = \\{\\text{Bull}, \\text{Bear}\\}$。在每天结束时，会记录一个可观测的市场变动，该变动可以是“Up”或“Down”。\n\nHMM 的组成部分定义如下：\n\n1.  **初始状态概率分布 ($\\pi$)**：市场在开始时处于各个状态的概率。\n    *   $P(\\text{state at day 1 is Bull}) = \\pi_{\\text{Bull}} = 0.6$\n    *   $P(\\text{state at day 1 is Bear}) = \\pi_{\\text{Bear}} = 0.4$\n\n2.  **状态转移概率矩阵 ($A$)**：在连续两天之间，从一个状态转移到另一个状态的概率。\n    *   $P(\\text{state}_{t+1}=\\text{Bull} | \\text{state}_t=\\text{Bull}) = a_{\\text{Bull},\\text{Bull}} = 0.7$\n    *   $P(\\text{state}_{t+1}=\\text{Bear} | \\text{state}_t=\\text{Bull}) = a_{\\text{Bull},\\text{Bear}} = 0.3$\n    *   $P(\\text{state}_{t+1}=\\text{Bull} | \\text{state}_t=\\text{Bear}) = a_{\\text{Bear},\\text{Bull}} = 0.4$\n    *   $P(\\text{state}_{t+1}=\\text{Bear} | \\text{state}_t=\\text{Bear}) = a_{\\text{Bear},\\text{Bear}} = 0.6$\n\n3.  **观测发射概率矩阵 ($B$)**：在给定当前隐藏状态的情况下，观测到特定市场变动的概率。\n    *   $P(\\text{observation}=\\text{Up} | \\text{state}=\\text{Bull}) = b_{\\text{Bull}}(\\text{Up}) = 0.8$\n    *   $P(\\text{observation}=\\text{Down} | \\text{state}=\\text{Bull}) = b_{\\text{Bull}}(\\text{Down}) = 0.2$\n    *   $P(\\text{observation}=\\text{Up} | \\text{state}=\\text{Bear}) = b_{\\text{Bear}}(\\text{Up}) = 0.3$\n    *   $P(\\text{observation}=\\text{Down} | \\text{state}=\\text{Bear}) = b_{\\text{Bear}}(\\text{Down}) = 0.7$\n\n一位分析师在两天内观测到以下市场变动序列：$Y = (\\text{Up}, \\text{Down})$。\n\n前向变量 $\\alpha_t(i)$ 定义为在时间 $t$ 观测到部分观测序列且在时间 $t$ 处于状态 $i$ 的联合概率。对于第一天，给定观测值为“Up”，前向变量已计算得出：\n*   $\\alpha_1(\\text{Bull}) = 0.480$\n*   $\\alpha_1(\\text{Bear}) = 0.120$\n\n使用所提供的信息，计算前向变量 $\\alpha_2(\\text{Bull})$ 的值。将最终答案四舍五入到四位有效数字。", "solution": "前向算法使用以下递归式\n$$\n\\alpha_{t}(i)=\\left[\\sum_{j\\in S}\\alpha_{t-1}(j)\\,a_{j,i}\\right]\\,b_{i}(o_{t}),\n$$\n其中 $o_{t}$ 是在时间 $t$ 的观测值。当 $t=2$ 时，观测值为 Down，我们要求解 $\\alpha_{2}(\\text{Bull})$。代入给定值，\n$$\n\\alpha_{2}(\\text{Bull})=\\left[\\alpha_{1}(\\text{Bull})\\,a_{\\text{Bull},\\text{Bull}}+\\alpha_{1}(\\text{Bear})\\,a_{\\text{Bear},\\text{Bull}}\\right]\\,b_{\\text{Bull}}(\\text{Down}).\n$$\n计算括号内的和：\n$$\n\\alpha_{1}(\\text{Bull})\\,a_{\\text{Bull},\\text{Bull}}+\\alpha_{1}(\\text{Bear})\\,a_{\\text{Bear},\\text{Bull}}=0.480\\cdot 0.7+0.120\\cdot 0.4=0.336+0.048=0.384.\n$$\n现在乘以在 Bull 状态下观测到 Down 的发射概率：\n$$\n\\alpha_{2}(\\text{Bull})=0.384\\cdot 0.2=0.0768.\n$$\n四舍五入到四位有效数字得到 $0.0768$。", "answer": "$$\\boxed{0.0768}$$", "id": "1336466"}, {"introduction": "理解了前向传递之后，我们现在转向学习过程的核心：最大化（M）步骤。这个练习将参数更新机制单独提炼出来，展示在“期望（E）步骤”中计算出的概率如何被用来优化模型。你将为一个文本生成隐马尔可夫模型更新一个发射概率，从而直接了解该算法如何从数据中学习。[@problem_id:1336492]", "problem": "一位计算语言学家正在使用隐马尔可夫模型（HMM）来分析一种简化语言的结构。该模型设计有两个隐藏状态：$S_V$（一个倾向于生成元音的状态）和 $S_C$（一个倾向于生成辅音的状态）。该语言学家使用一个观测到的字母序列 $O = o_1, o_2, \\ldots, o_T$，通过 Baum-Welch 算法来训练 HMM 的参数。\n\nBaum-Welch 算法是一个迭代过程。在完成第一次迭代的期望步骤（E-step）后，该语言学家计算了量 $\\gamma_t(i)$，它表示在给定整个观测序列和初始模型参数的情况下，在时间 $t$ 处于特定状态 $i$ 的概率。\n\n根据对元音状态 $S_V$ 的这些计算，得出了两个聚合值：\n1.  模型处于状态 $S_V$ 的总期望次数，即 $\\gamma_t(S_V)$ 在序列中所有时间步上的总和：$\\sum_{t=1}^{T} \\gamma_t(S_V) = 20.0$。\n2.  模型处于状态 $S_V$ 且观测到的字母为 'e' 的总期望次数，即在所有观测 $o_t$ 为字母 'e' 的时间步 $t$ 上 $\\gamma_t(S_V)$ 的总和：$\\sum_{t | o_t=\\text{'e'}} \\gamma_t(S_V) = 12.5$。\n\n基于这些值，执行 Baum-Welch 算法的最大化步骤（M-step），以找到从元音状态 $S_V$ 观测到字母 'e' 的更新后的发射概率。设这个更新后的概率为 $b'_{S_V}(\\text{'e'})$。\n\n计算 $b'_{S_V}(\\text{'e'})$ 的精确小数值。", "solution": "在 Baum-Welch 算法中，M-步骤通过状态 $i$ 发射符号 $v$ 的期望次数与访问状态 $i$ 的总期望次数之比来更新从状态 $i$ 观测到符号 $v$ 的发射概率。更新公式为\n$$\nb'_{i}(v)=\\frac{\\sum_{t: o_{t}=v}\\gamma_{t}(i)}{\\sum_{t=1}^{T}\\gamma_{t}(i)}.\n$$\n将此应用于元音状态 $S_{V}$ 和符号 $\\text{'e'}$，并使用提供的 E-步骤聚合值，\n$$\nb'_{S_{V}}(\\text{'e'})=\\frac{\\sum_{t: o_{t}=\\text{'e'}}\\gamma_{t}(S_{V})}{\\sum_{t=1}^{T}\\gamma_{t}(S_{V})}=\\frac{12.5}{20.0}.\n$$\n化简得，\n$$\n\\frac{12.5}{20.0}=\\frac{125}{200}=\\frac{5}{8}=0.625.\n$$\n因此，从 $S_{V}$ 观测到 $\\text{'e'}$ 的更新后的发射概率是 $0.625$。", "answer": "$$\\boxed{0.625}$$", "id": "1336492"}, {"introduction": "现实世界的数据往往是稀疏的，这可能导致标准的鲍姆-韦尔奇算法为未观测到的事件分配零概率，从而阻碍学习。这项综合性实践通过引入拉普拉斯平滑（一种构建稳健模型的关键技术）来解决这个问题。通过对一个小数据集进行完整的单次迭代，你将学会如何调整转移概率和发射概率，以有效处理数据稀疏性问题。[@problem_id:1336464]", "problem": "一个简化的隐马尔可夫模型（HMM）用于为一个发布每日天气摘要的自动化“情绪天气机器人”的行为建模。该机器人有两个隐藏的内部状态（其“情绪”），并能发布三种摘要类型中的一种。\n\n该HMM的参数定义如下：\n-   **隐藏状态 ($S$)**：一个包含 $N=2$ 个状态的集合，$S = \\{S_1, S_2\\}$，其中 $S_1$ 为‘开心’，$S_2$ 为‘伤心’。\n-   **观测符号 ($V$)**：一个包含 $M=3$ 个观测符号的集合，$V = \\{V_1, V_2, V_3\\}$，其中 $V_1$ 为‘晴天’，$V_2$ 为‘雨天’，$V_3$ 为‘阴天’。\n-   **初始状态概率 ($\\pi$)**：初始状态的概率分布。\n    $$ \\pi = [\\pi_i] = \\begin{pmatrix} P(q_1=S_1) \\\\ P(q_1=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} $$\n-   **状态转移概率矩阵 ($A$)**：从状态 $S_i$ 转移到状态 $S_j$ 的概率。\n    $$ A = [a_{ij}] = \\begin{pmatrix} P(q_t=S_1|q_{t-1}=S_1) & P(q_t=S_2|q_{t-1}=S_1) \\\\ P(q_t=S_1|q_{t-1}=S_2) & P(q_t=S_2|q_{t-1}=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.7 & 0.3 \\\\ 0.4 & 0.6 \\end{pmatrix} $$\n-   **发射概率矩阵 ($B$)**：在系统处于状态 $S_j$ 的条件下，观测到符号 $V_k$ 的概率。\n    $$ B = [b_j(k)] = \\begin{pmatrix} P(O_t=V_1|q_t=S_1) & P(O_t=V_2|q_t=S_1) & P(O_t=V_3|q_t=S_1) \\\\ P(O_t=V_1|q_t=S_2) & P(O_t=V_2|q_t=S_2) & P(O_t=V_3|q_t=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.8 & 0.1 & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\end{pmatrix} $$\n\n给定一个长度为 $T=3$ 的短观测序列：\n$$ O = (O_1, O_2, O_3) = (\\text{‘晴天’}, \\text{‘雨天’}, \\text{‘晴天’}) $$\n它对应于符号序列 $(V_1, V_2, V_1)$。\n\n你的任务是执行 Baum-Welch 算法的单次迭代，以找到重估后的模型参数 $\\bar{A}$ 和 $\\bar{B}$。在此任务中，你必须将拉普拉斯平滑（也称作加一平滑）整合到M步的重估公式中，以处理稀疏数据。平滑后的重估公式为：\n$$ \\bar{a}_{ij} = \\frac{(\\text{从 } S_i \\text{ 到 } S_j \\text{ 的期望转移次数}) + 1}{(\\text{从 } S_i \\text{ 出发的期望转移次数}) + N} $$\n$$ \\bar{b}_{j}(k) = \\frac{(\\text{在状态 } S_j \\text{ 下观测到 } V_k \\text{ 的期望次数}) + 1}{(\\text{处于状态 } S_j \\text{ 的期望次数}) + M} $$\n\n计算转移概率 $\\bar{a}_{12}$（从‘开心’转移到‘伤心’的概率）和发射概率 $\\bar{b}_{2}(3)$（在‘伤心’状态下观测到‘阴天’的概率）的更新值。\n\n分别给出 $\\bar{a}_{12}$ 和 $\\bar{b}_{2}(3)$ 的最终答案，四舍五入至三位有效数字。", "solution": "我们对给定的 HMM 和观测序列 $O=(V_{1},V_{2},V_{1})$，使用拉普拉斯平滑执行一次 Baum-Welch 迭代。\n\n前向变量 $\\alpha_{t}(i)$ 定义为 $\\alpha_{1}(i)=\\pi_{i} b_{i}(O_{1})$，且对于 $t\\geq 2$，有 $\\alpha_{t}(j)=\\left(\\sum_{i=1}^{N}\\alpha_{t-1}(i)a_{ij}\\right)b_{j}(O_{t})$。使用 $\\pi=(0.5,0.5)$、$A=\\begin{pmatrix}0.7&0.3\\\\0.4&0.6\\end{pmatrix}$、$B=\\begin{pmatrix}0.8&0.1&0.1\\\\0.2&0.6&0.2\\end{pmatrix}$ 和 $O=(V_{1},V_{2},V_{1})$，我们计算：\n$$\n\\alpha_{1}(1)=0.5\\cdot 0.8=0.4,\\quad \\alpha_{1}(2)=0.5\\cdot 0.2=0.1,\n$$\n$$\n\\alpha_{2}(1)=(0.4\\cdot 0.7+0.1\\cdot 0.4)\\cdot 0.1=0.032,\\quad \\alpha_{2}(2)=(0.4\\cdot 0.3+0.1\\cdot 0.6)\\cdot 0.6=0.108,\n$$\n$$\n\\alpha_{3}(1)=(0.032\\cdot 0.7+0.108\\cdot 0.4)\\cdot 0.8=0.05248,\\quad \\alpha_{3}(2)=(0.032\\cdot 0.3+0.108\\cdot 0.6)\\cdot 0.2=0.01488.\n$$\n因此 $P(O)=\\sum_{j}\\alpha_{3}(j)=0.06736$。\n\n后向变量 $\\beta_{t}(i)$ 满足 $\\beta_{3}(i)=1$，且对于 $t\\leq T-1$，有 $\\beta_{t}(i)=\\sum_{j=1}^{N}a_{ij} b_{j}(O_{t+1})\\beta_{t+1}(j)$。因此：\n$$\n\\beta_{2}(1)=0.7\\cdot 0.8+0.3\\cdot 0.2=0.62,\\quad \\beta_{2}(2)=0.4\\cdot 0.8+0.6\\cdot 0.2=0.44,\n$$\n$$\n\\beta_{1}(1)=0.7\\cdot 0.1\\cdot 0.62+0.3\\cdot 0.6\\cdot 0.44=0.1226,\\quad \\beta_{1}(2)=0.4\\cdot 0.1\\cdot 0.62+0.6\\cdot 0.6\\cdot 0.44=0.1832.\n$$\n\n状态后验概率 $\\gamma_{t}(i)=\\frac{\\alpha_{t}(i)\\beta_{t}(i)}{P(O)}$ 为：\n$$\n\\gamma_{1}(1)=\\frac{0.04904}{0.06736}=\\frac{613}{842},\\quad \\gamma_{1}(2)=\\frac{0.01832}{0.06736}=\\frac{229}{842},\n$$\n$$\n\\gamma_{2}(1)=\\frac{0.01984}{0.06736}=\\frac{124}{421},\\quad \\gamma_{2}(2)=\\frac{0.04752}{0.06736}=\\frac{297}{421},\n$$\n$$\n\\gamma_{3}(1)=\\frac{0.05248}{0.06736}=\\frac{328}{421},\\quad \\gamma_{3}(2)=\\frac{0.01488}{0.06736}=\\frac{93}{421}.\n$$\n\n成对后验概率 $\\xi_{t}(i,j)=\\frac{\\alpha_{t}(i)a_{ij}b_{j}(O_{t+1})\\beta_{t+1}(j)}{P(O)}$ 可得出从 $S_{1}$ 到 $S_{2}$ 的期望转移：\n$$\n\\xi_{1}(1,2)=\\frac{0.4\\cdot 0.3\\cdot 0.6\\cdot 0.44}{0.06736}=\\frac{198}{421},\\quad\n\\xi_{2}(1,2)=\\frac{0.032\\cdot 0.3\\cdot 0.2\\cdot 1}{0.06736}=\\frac{12}{421},\n$$\n因此 $S_{1}\\to S_{2}$ 转移的期望次数为\n$$\n\\sum_{t=1}^{2}\\xi_{t}(1,2)=\\frac{198}{421}+\\frac{12}{421}=\\frac{210}{421}.\n$$\n从 $S_{1}$ 出发的期望转移次数（即在 $t=1,2$ 时刻停留在 $S_{1}$ 的期望次数）为\n$$\n\\sum_{t=1}^{2}\\gamma_{t}(1)=\\frac{613}{842}+\\frac{124}{421}=\\frac{861}{842}.\n$$\n\n使用拉普拉斯平滑，对于 $N=2$，重估的转移概率为\n$$\n\\bar{a}_{12}=\\frac{\\left(\\sum_{t=1}^{2}\\xi_{t}(1,2)\\right)+1}{\\left(\\sum_{t=1}^{2}\\gamma_{t}(1)\\right)+N}\n=\\frac{\\frac{210}{421}+1}{\\frac{861}{842}+2}\n=\\frac{\\frac{631}{421}}{\\frac{2545}{842}}\n=\\frac{1262}{2545}\\approx 0.496068\\;\\Rightarrow\\;0.496\\text{ (三位有效数字)}.\n$$\n\n对于发射概率，在状态 $S_{2}$ 中观测到 $V_{3}$ 的期望次数为\n$$\n\\sum_{t=1}^{3}\\mathbb{I}\\{O_{t}=V_{3}\\}\\gamma_{t}(2)=0,\n$$\n而在状态 $S_{2}$ 中的总期望停留次数为\n$$\n\\sum_{t=1}^{3}\\gamma_{t}(2)=\\frac{229}{842}+\\frac{297}{421}+\\frac{93}{421}=\\frac{1009}{842}.\n$$\n使用拉普拉斯平滑且 $M=3$ 时，\n$$\n\\bar{b}_{2}(3)=\\frac{0+1}{\\left(\\sum_{t=1}^{3}\\gamma_{t}(2)\\right)+M}\n=\\frac{1}{\\frac{1009}{842}+3}\n=\\frac{842}{3535}\\approx 0.238190\\;\\Rightarrow\\;0.238\\text{ (三位有效数字)}.\n$$\n因此，更新后的值为 $\\bar{a}_{12}\\approx 0.496$ 和 $\\bar{b}_{2}(3)\\approx 0.238$。", "answer": "$$\\boxed{\\begin{pmatrix} 0.496 & 0.238 \\end{pmatrix}}$$", "id": "1336464"}]}