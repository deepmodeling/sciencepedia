{"hands_on_practices": [{"introduction": "掌握了马尔可夫性的核心定义后，首要的实践技能便是识别一个随机过程是否具备此性质。本练习将引导你分析两种由独立同分布随机变量序列生成的常见过程：累加和过程与累积最大值过程。通过运用马尔可夫性的“无记忆”定义，你将锻炼自己判断未来状态的条件分布是否仅依赖于当前状态的能力。[@problem_id:1342487]", "problem": "设 $X_1, X_2, X_3, \\dots$ 是一个独立同分布 (i.i.d.) 的连续随机变量序列。基于此序列，我们可以定义若干个离散时间随机过程。考虑对于 $n \\ge 1$ 的以下两个过程：\n\n1.  累加和过程 $\\{S_n\\}_{n \\ge 1}$，其中 $S_n = \\sum_{i=1}^{n} X_i$。\n2.  累积最大值过程 $\\{M_n\\}_{n \\ge 1}$，其中 $M_n = \\max\\{X_1, X_2, \\dots, X_n\\}$。\n\n如果一个随机过程 $\\{Z_n\\}$ 未来状态的条件概率分布在给定当前状态的情况下，仅取决于当前状态，而与它之前的事件序列无关，那么我们称该过程具有马尔可夫性质（或称为马尔可夫过程）。形式上，对于任意时间步 $n$ 和任意一组值 $z_1, \\dots, z_{n+1}$，以下等式必须成立：\n$P(Z_{n+1} \\le z_{n+1} | Z_n = z_n, Z_{n-1}=z_{n-1}, \\dots, Z_1=z_1) = P(Z_{n+1} \\le z_{n+1} | Z_n = z_n)$。\n\n关于这两个过程，以下哪个陈述是正确的？\n\nA. 只有过程 $\\{S_n\\}_{n \\ge 1}$ 是马尔可夫过程。\n\nB. 只有过程 $\\{M_n\\}_{n \\ge 1}$ 是马尔可夫过程。\n\nC. $\\{S_n\\}_{n \\ge 1}$ 和 $\\{M_n\\}_{n \\ge 1}$ 都是马尔可夫过程。\n\nD. $\\{S_n\\}_{n \\ge 1}$ 和 $\\{M_n\\}_{n \\ge 1}$ 都不是马尔可夫过程。", "solution": "设 $\\{X_{n}\\}_{n \\ge 1}$ 是独立同分布的连续随机变量，其累积分布函数为 $F$。定义 $S_{n}=\\sum_{i=1}^{n}X_{i}$ 和 $M_{n}=\\max\\{X_{1},\\dots,X_{n}\\}$。令 $\\mathcal{F}_{n}=\\sigma(X_{1},\\dots,X_{n})$。\n\n对于累加和过程 $\\{S_{n}\\}$，对任意实数 $y$，我们有\n$$\n\\begin{aligned}\nP(S_{n+1}\\le y\\mid \\mathcal{F}_{n})\n&=P(S_{n}+X_{n+1}\\le y\\mid \\mathcal{F}_{n})\\\\\n&=P(X_{n+1}\\le y-S_{n}\\mid \\mathcal{F}_{n})\\\\\n&=P(X_{n+1}\\le y-S_{n}) \\quad \\text{($X_{n+1}$ 与 $\\mathcal{F}_{n}$ 的独立性)}\\\\\n&=F(y-S_{n}).\n\\end{aligned}\n$$\n右侧只是 $S_{n}$ 的一个可测函数，所以\n$$\nP(S_{n+1}\\le y\\mid S_{n})=F(y-S_{n}).\n$$\n因此，在给定全部历史信息的条件下，$S_{n+1}$ 的条件分布只依赖于 $S_{n}$，所以 $\\{S_{n}\\}$ 是一个马尔可夫过程。\n\n对于累积最大值过程 $\\{M_{n}\\}$，注意到 $M_{n+1}=\\max(M_{n},X_{n+1})$。对任意实数 $y$，\n$$\n\\begin{aligned}\nP(M_{n+1}\\le y\\mid \\mathcal{F}_{n})\n&=P(\\max(M_{n},X_{n+1})\\le y\\mid \\mathcal{F}_{n})\\\\\n&=\\mathbf{1}_{\\{M_{n}\\le y\\}}\\,P(X_{n+1}\\le y\\mid \\mathcal{F}_{n})\\\\\n&=\\mathbf{1}_{\\{M_{n}\\le y\\}}\\,F(y) \\quad \\text{($X_{n+1}$ 与 $\\mathcal{F}_{n}$ 的独立性)}.\n\\end{aligned}\n$$\n等价地，对于 $m\\in \\mathbb{R}$，\n$$\nP(M_{n+1}\\le y\\mid M_{n}=m)=\n\\begin{cases}\n0, & y<m,\\\\\nF(y), & y\\ge m,\n\\end{cases}\n$$\n其结果只依赖于 $m=M_{n}$。因此 $\\{M_{n}\\}$ 也是一个马尔可夫过程。\n\n因此，$\\{S_{n}\\}$ 和 $\\{M_{n}\\}$ 都具有马尔可夫性质。", "answer": "$$\\boxed{C}$$", "id": "1342487"}, {"introduction": "在现实世界的许多系统中，过程的未来演化不仅依赖于当前状态，还受到过去状态的影响，这使得它们本身并非马尔可夫过程。本练习将介绍一种极为重要的建模技巧——状态增广（state augmentation）。你将学习如何通过巧妙地重新定义状态向量，将一个具有两期“记忆”的种群动态模型转化为一个标准的一阶马尔可夫过程，从而能够运用强大的马尔可夫分析工具。[@problem_id:1342473]", "problem": "一位生物学家正在研究一个物种，其种群大小（在第 $t$ 年表示为 $P_t$）表现出复杂的动态。研究发现，下一年的种群 $P_{t+1}$ 不仅取决于当年（第 $t$ 年）的种群 $P_t$，还取决于前一年（第 $t-1$ 年）的种群 $P_{t-1}$。这种“记忆”效应是由于资源可利用性以两年为周期进行恢复。该模型由以下线性递推关系描述：\n$$P_{t+1} = \\alpha P_t + \\beta P_{t-1} + W_t$$\n其中，$\\alpha$ 和 $\\beta$ 是常数系数，分别代表增长效应和延迟调节效应，而 $\\{W_t\\}$ 是一个独立同分布的随机变量序列，代表不可预测的环境波动，其均值为零。\n\n这个过程 $\\{P_t\\}$ 不是一个马尔可夫过程，因为预测种群的未来分布需要的信息不仅仅是当前状态 $P_t$。为了使用马尔可夫过程这一强大框架来分析该系统，我们必须首先在时间 $t$ 定义一个合适的状态向量 $S_t$，使得由此产生的向量过程 $\\{S_t\\}$ 具有马尔可夫性质。\n\n下列向量中（向量以列向量形式书写），哪一个构成了用于此目的的有效状态向量 $S_t$？\n\nA. $S_t = P_t$\n\nB. $S_t = \\begin{pmatrix} P_t \\\\ W_t \\end{pmatrix}$\n\nC. $S_t = \\begin{pmatrix} P_t \\\\ P_{t-1} \\end{pmatrix}$\n\nD. $S_t = \\begin{pmatrix} P_{t+1} \\\\ P_t \\end{pmatrix}$\n\nE. $S_t = P_t + P_{t-1}$", "solution": "我们已知线性递推关系\n$$P_{t+1}=\\alpha P_{t}+\\beta P_{t-1}+W_{t},$$\n其中 $\\{W_{t}\\}$ 是一个均值为零且独立于过去的独立同分布序列。一个向量过程 $\\{S_{t}\\}$ 是马尔可夫过程，如果对于任意 $t$，在给定整个过去的情况下，$S_{t+1}$ 的条件分布仅取决于 $S_{t}$；也就是说，\n$$\\Pr(S_{t+1}\\in A\\mid S_{t},S_{t-1},\\dots)=\\Pr(S_{t+1}\\in A\\mid S_{t})\\quad\\text{对于所有可测集 }A。$$\n\n考虑 $S_{t}=\\begin{pmatrix}P_{t}\\\\P_{t-1}\\end{pmatrix}$。那么\n$$S_{t+1}=\\begin{pmatrix}P_{t+1}\\\\P_{t}\\end{pmatrix}=\\begin{pmatrix}\\alpha & \\beta\\\\ 1 & 0\\end{pmatrix}\\begin{pmatrix}P_{t}\\\\P_{t-1}\\end{pmatrix}+\\begin{pmatrix}1\\\\0\\end{pmatrix}W_{t}。$$\n令 $A=\\begin{pmatrix}\\alpha & \\beta\\\\ 1 & 0\\end{pmatrix}$ 且 $B=\\begin{pmatrix}1\\\\0\\end{pmatrix}$。那么 $S_{t+1}=A S_{t}+B W_{t}$。因为 $W_{t}$ 独立于 $(S_{t},S_{t-1},\\dots)$ 并且具有固定的分布，所以在给定过去的情况下，$S_{t+1}$ 的条件分布仅通过确定性项 $A S_{t}$ 和独立的创新项 $B W_{t}$ 依赖于 $S_{t}$。因此，$\\{S_{t}\\}$ 是一阶马尔可夫过程。所以选项C是有效的。\n\n我们现在来论证为什么其他选项作为时间 $t$ 的状态向量是无效的。\n\nA. $S_{t}=P_{t}$ 是不充分的，因为\n$$P_{t+1}\\mid(P_{t},P_{t-1})=\\alpha P_{t}+\\beta P_{t-1}+W_{t}$$\n其条件均值依赖于 $P_{t-1}$，而 $P_{t-1}$ 不能由 $P_{t}$ 确定。因此，仅给定 $P_{t}$ 时 $P_{t+1}$ 的条件分布与给定全部过去历史时的条件分布不匹配，这违反了马尔可夫性质。\n\nB. $S_{t}=\\begin{pmatrix}P_{t}\\\\ W_{t}\\end{pmatrix}$ 也是不充分的。尽管 $P_{t+1}=\\alpha P_{t}+\\beta P_{t-1}+W_{t}$ 中包含了 $W_{t}$，但在给定 $(P_{t},W_{t})$ 的情况下，项 $\\beta P_{t-1}$ 仍然是未知的。因此，给定 $(P_{t},W_{t})$ 时 $P_{t+1}$ 的条件分布依赖于 $P_{t-1}$，所以该过程在此状态下不是马尔可夫过程。\n\nD. $S_{t}=\\begin{pmatrix}P_{t+1}\\\\P_{t}\\end{pmatrix}$ 使用了一个未来值 $P_{t+1}$ 作为时间 $t$ 状态的一部分，而这个值并不能由时间 $t$ 可用的信息确定。虽然如果允许使用未来信息，序列 $\\left(\\begin{pmatrix}P_{t+1}\\\\P_{t}\\end{pmatrix}\\right)_{t}$ 会满足关于 $t$ 的马尔可夫递推关系，但它不是一个由当前和过去信息构建的、在时间 $t$ 的有效状态向量，而后者是为系统演化进行马尔可夫建模所必需的。\n\nE. $S_{t}=P_{t}+P_{t-1}$ 将两个必需的分量聚合为一个标量，导致了信息丢失。因为\n$$P_{t+1}=\\alpha P_{t}+\\beta P_{t-1}+W_{t},$$\n仅知道 $P_{t}+P_{t-1}$ 的值并不能确定 $P_{t+1}$ 的条件分布，因为不同的 $(P_{t},P_{t-1})$ 对可以产生相同的和，但 $\\alpha P_{t}+\\beta P_{t-1}$ 的值却不同。\n\n因此，唯一能产生时间 $t$ 的马尔可夫状态向量的有效选择是 $S_{t}=\\begin{pmatrix}P_{t}\\\\P_{t-1}\\end{pmatrix}$。", "answer": "$$\\boxed{C}$$", "id": "1342473"}, {"introduction": "我们已经知道如何将非马尔可夫过程改造为马尔可夫过程，一个自然的问题是：对一个已知的马尔可夫过程进行变换，其马尔可夫性是否总能被保持？本练习通过一个具体的例子——对一个双态马尔可夫链进行乘积变换，来深入探讨这个问题。你需要找出在何种特定条件下，这个新的过程才能保持马尔可夫性，从而体会到信息压缩与“可集性”（lumpability）概念的精妙之处。[@problem_id:1342502]", "problem": "考虑一个离散时间随机过程 $\\{X_n\\}_{n \\ge 0}$，其状态空间为 $S_X = \\{-1, 1\\}$，该过程根据一个时间齐次马尔可夫链的规则演化。其转移概率由下式给出：\n$$P(X_n = 1 | X_{n-1} = -1) = p$$\n$$P(X_n = -1 | X_{n-1} = 1) = q$$\n其中 $p$ 和 $q$ 为常数，满足 $0 < p < 1$ 且 $0 < q < 1$。其余两个转移概率由从任一状态离开的概率之和必须为1这一事实所决定。\n\n基于 $\\{X_n\\}$，通过关系式 $Y_n = X_n X_{n-1}$ 定义了一个新的随机过程 $\\{Y_n\\}_{n \\ge 1}$。这个新过程的状态空间为 $S_Y = \\{-1, 1\\}$。\n\n在下列哪个条件下，过程 $\\{Y_n\\}_{n \\ge 1}$ 是一个马尔可夫链？\n\nA. 对所有有效的 $p$ 和 $q$，$\\{Y_n\\}$ 都是马尔可夫链。\n\nB. $\\{Y_n\\}$ 是马尔可夫链当且仅当 $p=q$。\n\nC. $\\{Y_n\\}$ 是马尔可夫链当且仅当 $p+q=1$。\n\nD. $\\{Y_n\\}$ 是马尔可夫链当且仅当 $p=q=\\frac{1}{2}$。\n\nE. 对任何有效的 $p$ 和 $q$，$\\{Y_n\\}$ 都不是马尔可夫链。", "solution": "设 $\\{X_{n}\\}_{n\\ge 0}$ 是一个在 $S_{X}=\\{-1,1\\}$ 上的双状态时间齐次马尔可夫链，其转移概率为\n$$\nP(X_{n}=1\\mid X_{n-1}=-1)=p,\\qquad P(X_{n}=-1\\mid X_{n-1}=1)=q,\n$$\n因此\n$$\nP(X_{n}=-1\\mid X_{n-1}=-1)=1-p,\\qquad P(X_{n}=1\\mid X_{n-1}=1)=1-q,\n$$\n其中 $0<p<1$ 且 $0<q<1$。\n\n定义 $Y_{n}=X_{n}X_{n-1}\\in\\{-1,1\\}$。那么，$Y_{n}=1$ 当且仅当 $X_{n}=X_{n-1}$（无变化），而 $Y_{n}=-1$ 当且仅当 $X_{n}=-X_{n-1}$（有变化）。\n\n引入配对过程 $Z_{n}=(X_{n-1},X_{n})$，其状态空间为\n$$\n\\{(-1,-1),\\,(-1,1),\\,(1,-1),\\,(1,1)\\}.\n$$\n因为 $X$ 是马尔可夫过程，所以 $Z$ 也是一个时间齐次马尔可夫链，其转移仅由第二个分量决定：从 $(i,j)$ 出发，下一个状态是 $(j,k)$，其中 $k=j$ 的概率为 $P(X_{n+1}=j\\mid X_{n}=j)$，$k=-j$ 的概率为 $P(X_{n+1}=-j\\mid X_{n}=j)$。具体来说，\n$$\nP\\big((i,j)\\to (j,j)\\big)=\n\\begin{cases}\n1-p,& j=-1,\\\\\n1-q,& j=1,\n\\end{cases}\n\\qquad\nP\\big((i,j)\\to (j,-j)\\big)=\n\\begin{cases}\np,& j=-1,\\\\\nq,& j=1.\n\\end{cases}\n$$\n\n过程 $Y_{n}$ 是 $Z_{n}$ 的一个函数，通过 $Y_{n}=f(Z_{n})=x_{n-1}x_{n}$ 建立联系。这导致 $Z$ 的状态空间被划分为两个块：\n$$\nA=\\{(-1,-1),(1,1)\\}\\quad\\text{，对应 }Y=1,\\qquad B=\\{(-1,1),(1,-1)\\}\\quad\\text{，对应 }Y=-1.\n$$\n根据强可集结性准则（strong lumpability criterion），聚合过程 $\\{Y_{n}\\}$ 是马尔可夫过程（对所有初始分布而言）当且仅当，对于同一块中的任意两个状态 $s,s'$，以及对于每个块 $C\\in\\{A,B\\}$，从 $s$ 到 $C$ 的总转移概率等于从 $s'$ 到 $C$ 的总转移概率。\n\n计算从每个 $Z$ 状态出发的块间转移概率：\n- 从 $(-1,-1)$（第二个分量为 $-1$）出发：转移到 $A$ 的概率为 $1-p$（到 $(-1,-1)$），转移到 $B$ 的概率为 $p$（到 $(-1,1)$）。\n- 从 $(1,1)$（第二个分量为 $1$）出发：转移到 $A$ 的概率为 $1-q$（到 $(1,1)$），转移到 $B$ 的概率为 $q$（到 $(1,-1)$）。\n因此，在块 $A$ 内部，出射概率相等要求\n$$\n1-p=1-q\\quad\\text{且}\\quad p=q,\n$$\n这等价于 $p=q$。\n\n同样地，对于块 $B$：\n- 从 $(-1,1)$（第二个分量为 $1$）出发：转移到 $A$ 的概率为 $1-q$，转移到 $B$ 的概率为 $q$。\n- 从 $(1,-1)$（第二个分量为 $-1$）出发：转移到 $A$ 的概率为 $1-p$，转移到 $B$ 的概率为 $p$。\n块 $B$ 内部的概率相等同样要求 $1-q=1-p$ 且 $q=p$，即 $p=q$。\n\n因此，过程 $\\{Y_{n}\\}$ 是马尔可夫过程当且仅当 $p=q$。当 $p=q=r$ 时，我们也可以直接读出 $Y$ 的转移概率：\n$$\nP(Y_{n+1}=-1\\mid Y_{n}=1)=r,\\quad P(Y_{n+1}=1\\mid Y_{n}=1)=1-r,\\quad P(Y_{n+1}=1\\mid Y_{n}=-1)=1-r,\\quad P(Y_{n+1}=-1\\mid Y_{n}=-1)=r,\n$$\n这证实了其马尔可夫性质。如果 $p\\neq q$，同一块内不同状态的出射概率不同，这违反了可集结性；因此在这种情况下 $\\{Y_{n}\\}$ 不是马尔可夫过程。\n\n因此，正确的条件是 $p=q$，对应于选项B。", "answer": "$$\\boxed{B}$$", "id": "1342502"}]}