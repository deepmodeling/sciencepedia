{"hands_on_practices": [{"introduction": "在平均场博弈中，一项核心技能是求解代表性个体的优化问题，此时我们视平均场项为已知。本练习将重点关注这一步骤，通过应用哈密顿-雅可比-贝尔曼（HJB）方程，为经典的线性二次（LQ）模型推导出最优反馈控制。这是构建平均场不动点问题的关键“控制”部分。[@problem_id:2987064]", "id": "2987064", "problem": "考虑一个由连续统相同智能体组成的线性二次平均场博弈。对于一个代表性智能体，其受控状态过程 $(X_t)_{t \\in [0,T]}$ 遵循如下随机微分方程\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t,\n$$\n其中 $a$、$b$、$c$ 和 $\\sigma$ 为实常数，$(W_t)_{t \\in [0,T]}$ 为标准布朗运动，$\\alpha_t$ 为智能体的控制，而 $m_t$ 表示总体均值 $m_t = \\mathbb{E}[X_t]$。该代表性智能体旨在最小化期望二次成本\n$$\nJ(\\alpha) = \\mathbb{E}\\Bigg[\\int_{0}^{T} \\left(q X_t^{2} + \\bar{q}\\left(X_t - m_t\\right)^{2} + r \\alpha_t^{2}\\right)\\mathrm{d}t + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2}\\Bigg],\n$$\n其中给定常数 $q>0$、$\\bar{q}\\geq 0$、$r>0$、$g\\geq 0$ 和 $\\bar{g}\\geq 0$。在求解代表性智能体问题时，假设平均场 $(m_t)_{t\\in[0,T]}$ 是一个给定的时间确定性函数。\n\n从动态规划原理和 Hamilton–Jacobi–Bellman (HJB) 方程出发，对值函数使用以下形式的拟设\n$$\nV(t,x) = \\tfrac{1}{2}P_t x^{2} + S_t m_t x + \\tfrac{1}{2}U_t m_t^{2} + \\phi_t,\n$$\n其中 $(P_t,S_t,U_t,\\phi_t)$ 是时间的确定性系数函数，其选择使得 $V$ 满足 HJB 方程及终端条件 $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$。通过最小化 HJB 方程中关于控制的哈密顿量，推导以函数 $P_t$ 和 $S_t$ 表示的反馈控制 $\\alpha_t$。您可以利用 HJB 方程中的系数匹配会产生关于 $(P_t,S_t)$ 的耦合 Riccati 系统这一事实，但您不应显式求解该系统。请将您的最终答案表示为关于 $b$、$r$、$P_t$、$S_t$、$X_t$ 和 $m_t$ 的单个闭式解析表达式。无需进行数值计算。", "solution": "该问题是有效的，因为它提出了一个平均场博弈论领域的标准、适定问题，具体来说是一个线性二次 (LQ) 模型。所有参数和目标都已明确定义，并且在随机最优控制的背景下，其前提在科学上是合理的。\n\n目标是为代表性智能体找到最优反馈控制 $\\alpha_t$。智能体的状态 $X_t$ 遵循随机微分方程 (SDE)：\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\n智能体寻求最小化一个二次成本泛函 $J(\\alpha)$。在平均场博弈框架中，第一步是求解代表性智能体的优化问题，假设平均场项 $m_t = \\mathbb{E}[X_t]$ 是一个已知的时间确定性函数。\n\n智能体问题的值函数定义为：\n$$\nV(t,x) = \\min_{\\alpha} \\mathbb{E}\\Bigg[\\int_{t}^{T} \\left(q X_s^{2} + \\bar{q}\\left(X_s - m_s\\right)^{2} + r \\alpha_s^{2}\\right)\\mathrm{d}s + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2} \\Bigg| X_t=x \\Bigg]\n$$\n该值函数 $V(t,x)$ 必须满足 Hamilton-Jacobi-Bellman (HJB) 偏微分方程。HJB 方程由下式给出：\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\left\\{ L(t,x,\\alpha,m_t) + \\mathcal{L}^\\alpha V(t,x) \\right\\}\n$$\n其中 $L(t,x,\\alpha,m_t)$ 是即期成本，而 $\\mathcal{L}^\\alpha$ 是在控制 $\\alpha$ 下状态过程 $X_t$ 的无穷小生成元。\n\n即期成本是：\n$$\nL(t,x,\\alpha,m_t) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2}\n$$\n应用于值函数 $V(t,x)$ 的无穷小生成元 $\\mathcal{L}^\\alpha$ 是：\n$$\n\\mathcal{L}^\\alpha V(t,x) = \\left(a x + b \\alpha + c m_t\\right) \\frac{\\partial V}{\\partial x}(t,x) + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2}(t,x)\n$$\nHJB 方程中最小化算子内部的项是哈密顿量 $\\mathcal{H}$，它依赖于状态 $x$、时间 $t$、控制 $\\alpha$ 以及值函数的偏导数 $V_x = \\frac{\\partial V}{\\partial x}$ 和 $V_{xx} = \\frac{\\partial^2 V}{\\partial x^2}$。\n$$\n\\mathcal{H}(t, x, \\alpha, V_x, V_{xx}) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2} + \\left(a x + b \\alpha + c m_t\\right) V_x + \\frac{1}{2}\\sigma^2 V_{xx}\n$$\n因此，HJB 方程可以写为：\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\mathcal{H}(t, x, \\alpha, V_x, V_{xx})\n$$\n为了找到最优控制 $\\alpha_t^*$，我们必须对于一个固定的状态 $(t,x)$ 最小化关于 $\\alpha$ 的哈密顿量。我们可以重写哈密顿量以分离出涉及 $\\alpha$ 的项：\n$$\n\\mathcal{H} = r \\alpha^{2} + b \\alpha V_x + \\left( q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + (a x + c m_t) V_x + \\frac{1}{2}\\sigma^2 V_{xx} \\right)\n$$\n这是一个关于 $\\alpha$ 的二次函数。由于参数 $r > 0$，该抛物线开口向上，其最小值可以通过将其关于 $\\alpha$ 的导数设为零来找到。\n$$\n\\frac{\\partial \\mathcal{H}}{\\partial \\alpha} = 2r \\alpha + b V_x = 0\n$$\n求解 $\\alpha$ 可得到以值函数的空间导数表示的最优控制，记为 $\\alpha^*$：\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}(t,x)\n$$\n问题为值函数提供了一个拟设：\n$$\nV(t,x) = \\frac{1}{2}P_t x^{2} + S_t m_t x + \\frac{1}{2}U_t m_t^{2} + \\phi_t\n$$\n其中 $P_t$、$S_t$、$U_t$ 和 $\\phi_t$ 是时间的确定性函数。我们计算该拟设关于 $x$ 的偏导数：\n$$\n\\frac{\\partial V}{\\partial x}(t,x) = P_t x + S_t m_t\n$$\n将此表达式代入最优控制 $\\alpha^*(t,x)$ 的公式中，得到：\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\left(P_t x + S_t m_t\\right)\n$$\n这是在时间 $t$ 对给定状态 $x$ 的反馈形式的最优控制。对于在时间 $t$ 状态为 $X_t$ 的代表性智能体，其最优控制 $\\alpha_t$ 是通过将泛型状态变量 $x$ 替换为随机变量 $X_t$ 得到的：\n$$\n\\alpha_t = -\\frac{b}{2r} \\left(P_t X_t + S_t m_t\\right)\n$$\n该表达式按照要求，以状态 $X_t$、平均场 $m_t$、问题参数 $b$ 和 $r$，以及值函数拟设中的系数函数 $P_t$ 和 $S_t$ 来提供了最优控制 $\\alpha_t$。终端条件 $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$ 提供了系数的终端值，例如 $P_T = 2(g+\\bar{g})$ 和 $S_T = -2\\bar{g}$，这些值将用于求解关于 $P_t$ 和 $S_t$ 的 Riccati 系统。然而，问题陈述并未要求求解此系统。", "answer": "$$\n\\boxed{-\\frac{b}{2r}(P_t X_t + S_t m_t)}\n$$"}, {"introduction": "找到一个平均场博弈均衡需要确保一致性：即由个体最优策略产生的群体分布，必须与个体制定策略时所依赖的群体分布相匹配。本练习在一个简单且易于处理的两状态模型中，完整地展示了寻找这种均衡的过程，将单个体的 HJB 方程与群体的平稳性条件联系起来。通过这个练习，你将亲手解出博弈的完整解，包括最优控制和稳态分布。[@problem_id:2987138]", "id": "2987138", "problem": "考虑一个连续时间平均场博弈 (MFG)，其中有连续统的同质代理人，其个体状态为两点集 $\\{0,1\\}$ 中的一个节点。位于节点 $i \\in \\{0,1\\}$ 的代理人选择一个转移率 $u_i \\in [0,\\bar{u}_i]$ 以跳跃到节点 $1-i$。在平稳马尔可夫控制 $u = (u_0,u_1)$ 下，群体根据一个受控连续时间马尔可夫链 (CTMC) 演化，其生成元 $\\mathcal{L}^{u}$ 对任意有界函数 $f$ 由 $\\mathcal{L}^{u} f(i) = u_i \\big(f(1-i) - f(i)\\big)$ 给出。\n\n代理人寻求最小化遍历（长期平均）成本。节点 $i$ 的瞬时运行成本是线性拥堵成本 $\\gamma_i m_i$ 加上线性控制成本 $\\alpha_i u_i$，其中 $m_i$ 是位于节点 $i$ 的群体平稳比例。参数为\n$$\n\\gamma_0 = 0.6,\\quad \\gamma_1 = 1.4,\\quad \\alpha_0 = -1,\\quad \\alpha_1 = 0.5,\\quad \\bar{u}_0 = 3,\\quad \\bar{u}_1 = 2.\n$$\n一个平均场 Nash 均衡是一个对 $(u^{\\ast},m^{\\ast})$，使得：\n- 对于平稳分布 $m^{\\ast} = (m_0^{\\ast},m_1^{\\ast})$，控制 $u^{\\ast}$ 最小化代理人的遍历 Hamilton–Jacobi–Bellman (HJB) 方程，其值函数为 $V = (V_0,V_1)$，遍历常数为 $\\lambda$。\n- 平稳分布 $m^{\\ast}$ 与由 $u^{\\ast}$ 驱动的 CTMC 一致，即它满足平稳性（全局平衡）条件。\n\n从这些定义和原理出发，推导此具有线性成本和有界转移率的两节点 CTMC 的遍历 HJB 最优性条件。然后，显式求解均衡转移率 $u_0^{\\ast}$ 和 $u_1^{\\ast}$ 以及平稳分布 $m_0^{\\ast}$ 和 $m_1^{\\ast}$。\n\n将最终答案表示为包含 $(u_0^{\\ast},\\,u_1^{\\ast},\\,m_0^{\\ast},\\,m_1^{\\ast})$ 的单行矩阵。无需四舍五入。", "solution": "该问题要求解一个两状态连续时间马尔可夫链 (CTMC) 模型的平均场 Nash 均衡。一个均衡是一个对 $(u^{\\ast}, m^{\\ast})$，由最优控制策略 $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$ 和一个一致的平稳群体分布 $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ 组成，它们满足两个主要条件：代理人最优性和群体一致性。\n\n**1. 代理人最优性：遍历 Hamilton-Jacobi-Bellman (HJB) 方程**\n\n对于单个代理人，群体分布 $m = (m_0, m_1)$ 被视为固定的。代理人的目标是选择一个控制策略 $u = (u_0, u_1)$ 来最小化其长期平均（遍历）成本。设 $V = (V_0, V_1)$ 是表示每个状态下未来总期望成本的值函数，$\\lambda$ 是常数遍历成本。最优性条件由遍历 Hamilton-Jacobi-Bellman (HJB) 方程给出。对于每个状态 $i \\in \\{0, 1\\}$，HJB 方程为：\n$$ \\lambda = \\min_{u_i \\in [0, \\bar{u}_i]} \\left\\{ \\text{状态 } i \\text{ 的瞬时成本} + \\mathcal{L}^{u} V(i) \\right\\} $$\n状态 $i$ 的瞬时成本是 $\\gamma_i m_i + \\alpha_i u_i$。CTMC 的生成元为 $\\mathcal{L}^{u} V(i) = u_i (V_{1-i} - V_i)$。将这些代入 HJB 系统，我们得到：\n$$ \\lambda = \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ \\gamma_0 m_0 + \\alpha_0 u_0 + u_0(V_1 - V_0) \\right\\} $$\n$$ \\lambda = \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ \\gamma_1 m_1 + \\alpha_1 u_1 + u_1(V_0 - V_1) \\right\\} $$\n为简化起见，我们定义相对值，或称为势，$P = V_1 - V_0$。HJB 方程可以重写为：\n$$ \\lambda = \\gamma_0 m_0 + \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ u_0 (\\alpha_0 + P) \\right\\} $$\n$$ \\lambda = \\gamma_1 m_1 + \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ u_1 (\\alpha_1 - P) \\right\\} $$\n由于控制 $u_i$ 在目标函数中是线性的，最优控制 $u_i^{\\ast}$ 由其系数的符号决定。这导致了“bang-bang”或奇异控制策略。\n最优控制 $u_0^{\\ast}$ 为：\n$$ u_0^{\\ast}(P) = \\begin{cases} \\bar{u}_0 & \\text{若 } \\alpha_0 + P < 0 \\\\ \\text{在 } [0, \\bar{u}_0] \\text{ 内的任意值} & \\text{若 } \\alpha_0 + P = 0 \\\\ 0 & \\text{若 } \\alpha_0 + P > 0 \\end{cases} $$\n最优控制 $u_1^{\\ast}$ 为：\n$$ u_1^{\\ast}(P) = \\begin{cases} \\bar{u}_1 & \\text{若 } \\alpha_1 - P < 0 \\\\ \\text{在 } [0, \\bar{u}_1] \\text{ 内的任意值} & \\text{若 } \\alpha_1 - P = 0 \\\\ 0 & \\text{若 } \\alpha_1 - P > 0 \\end{cases} $$\n\n**2. 群体一致性：平稳分布**\n\n平稳分布 $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ 必须与最优控制 $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$ 引致的动态一致。这由 CTMC 的平稳性（或全局平衡）方程体现，该方程表明从状态 $0$ 到 $1$ 的代理人流必须等于从状态 $1$ 到 $0$ 的流：\n$$ m_0^{\\ast} u_0^{\\ast} = m_1^{\\ast} u_1^{\\ast} $$\n结合群体比例之和必须为一的事实，$m_0^{\\ast} + m_1^{\\ast} = 1$，我们可以求解平稳分布。假设 $u_0^{\\ast} + u_1^{\\ast} > 0$，我们有：\n$$ m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} \\quad \\text{且} \\quad m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} $$\n\n**3. 求解平均场均衡**\n\n一个均衡是一个状态 $(u^{\\ast}, m^{\\ast}, P)$，它同时求解 HJB 方程和一致性条件。我们可以通过令两个 HJB 方程相等来消去 $\\lambda$ 以找到均衡：\n$$ \\gamma_0 m_0^{\\ast} + u_0^{\\ast}(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + u_1^{\\ast}(\\alpha_1 - P) $$\n给定参数为：$\\gamma_0 = 0.6$，$\\gamma_1 = 1.4$，$\\alpha_0 = -1$，$\\alpha_1 = 0.5$，$\\bar{u}_0 = 3$，$\\bar{u}_1 = 2$。\n决定控制策略的势 $P$ 的临界值是 $P = -\\alpha_0 = 1$ 和 $P = \\alpha_1 = 0.5$。这将对 $P$ 的搜索划分为几种情况。我们来分析均衡控制处于其最大值的情况，这发生在 $\\alpha_0 + P < 0$ 和 $\\alpha_1 - P < 0$ 时。这对应于区间 $P \\in (0.5, 1)$。\n\n在这个候选情况下，最优控制为：\n- $u_0^{\\ast} = \\bar{u}_0 = 3$ (因为 $P < 1 \\implies P - 1 < 0 \\implies \\alpha_0 + P < 0$)\n- $u_1^{\\ast} = \\bar{u}_1 = 2$ (因为 $P > 0.5 \\implies 0.5 - P < 0 \\implies \\alpha_1 - P < 0$)\n\n使用这些控制，一致的平稳分布是：\n- $m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{2}{3 + 2} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{3}{3 + 2} = \\frac{3}{5}$\n\n现在，我们必须验证这些值与关于 $P$ 的 HJB 等式一致。将 $u_0^{\\ast}, u_1^{\\ast}, m_0^{\\ast}, m_1^{\\ast}$ 代入方程：\n$$ \\gamma_0 m_0^{\\ast} + \\bar{u}_0(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + \\bar{u}_1(\\alpha_1 - P) $$\n代入数值：\n$$ (0.6)\\left(\\frac{2}{5}\\right) + 3(-1 + P) = (1.4)\\left(\\frac{3}{5}\\right) + 2(0.5 - P) $$\n$$ 0.24 - 3 + 3P = 0.84 + 1 - 2P $$\n$$ 3P - 2.76 = 1.84 - 2P $$\n$$ 5P = 1.84 + 2.76 $$\n$$ 5P = 4.6 $$\n$$ P = \\frac{4.6}{5} = 0.92 $$\n值 $P = 0.92$ 位于假设的区间 $(0.5, 1)$ 内，这证实了我们对控制策略的选择是正确的，并且解是自洽的。对其他情况（例如 $P < 0.5$ 或 $P > 1$）的穷尽分析会导致矛盾，从而证实该均衡是唯一的。\n\n因此，均衡转移率和平稳分布为：\n- $u_0^{\\ast} = 3$\n- $u_1^{\\ast} = 2$\n- $m_0^{\\ast} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{3}{5}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 3 & 2 & \\frac{2}{5} & \\frac{3}{5} \\end{pmatrix}\n}\n$$"}, {"introduction": "与中心化优化问题不同，博弈问题可能展现出多个纳什均衡。本练习通过深入比较一个平均场博弈（MFG）与其对应的“社会计划者”问题（平均场控制，MFC），揭示了博弈的去中心化竞争特性如何导致均衡的多重性，即使在对应的中心化问题有唯一解的情况下也是如此。这凸显了为何 Lasry-Lions 单调性条件在博弈论中如此重要。[@problem_id:2987096]", "id": "2987096", "problem": "考虑一个由大量相同个体组成的连续统，其状态由时间区间 $[0,T]$ 上的随机微分方程 $dX_t = u_t\\,dt + \\sigma\\,dW_t$ 决定，其中 $W_t$ 是标准布朗运动，$\\sigma>0$ 是扩散系数，$u_t$ 是具有有限二次能量的循序可测控制。初始状态 $X_0$ 是可积的，其均值为 $m_0 := \\mathbb{E}[X_0]$。令 $\\bar{m}_t := \\mathbb{E}[X_t]$ 表示在时间 $t$ 的群体均值。\n\n每个个体评估一个线性二次代价泛函\n$$\nJ(u;\\bar{m}_T) \\;=\\; \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt \\;+\\; \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right],\n$$\n其中参数 $r>0$，$\\delta>0$，耦合强度 $\\alpha\\in\\mathbb{R}$。耦合项依赖于终端群体均值 $\\bar{m}_T$。考虑两种表述：\n\n1. 平均场博弈 (MFG): 纳什均衡是一种在给定 $\\bar{m}_T$ 的情况下最小化 $J(u;\\bar{m}_T)$ 的控制，并且其引致的终端均值 $\\bar{m}_T$ 与由最优控制生成的群体分布相一致。\n\n2. 平均场控制 (社会计划者问题): 选择一个共同的控制来最小化群体的平均代价，这等价于最小化 $J(u;\\bar{m}_T)$，并理解 $\\bar{m}_T$ 是由所选控制决定的。\n\n从第一性原理出发（平均场博弈中纳什均衡的定义、线性二次随机控制的结构，以及如柯西-施瓦茨不等式等基本不等式），完成以下任务：\n\n(a) 推导在平均场博弈中，终端均值 $\\bar{m}_T$ 所满足的标量不动点方程，并用 $\\alpha$, $r$, $\\delta$, $T$, 和 $m_0$ 表示。\n\n(b) 证明即使当耦合项 $x\\mapsto \\frac{\\delta}{2}\\left(x - \\alpha\\,\\bar{m}_T\\right)^2$ 不满足 Lasry–Lions 单调性条件时，由于严格凸性，平均场控制问题仍然存在唯一的 dla 终端均值 $\\bar{m}_T$。提供一个简要论证，说明当 $\\alpha>0$ 时 Lasry–Lions 单调性条件不成立。\n\n(c) 考虑 $m_0 = 0$ 的特殊情况。确定精确的耦合强度 $\\alpha^{\\star}$，在该强度下，平均场博弈存在无限多个纳什均衡（即 (a) 中的不动点方程对 $\\bar{m}_T$ 有无限多个解），而平均场控制的最优解在相同数据下仍然是唯一的。将你的最终答案表示为 $\\alpha^{\\star}$ 的闭式解析表达式。", "solution": "用户提供了一个问题陈述，在尝试解答之前需要进行验证。\n\n### 步骤 1: 提取已知条件\n- 个体状态动态学: $dX_t = u_t\\,dt + \\sigma\\,dW_t$ 对于 $t \\in [0,T]$。\n- $W_t$ 是标准布朗运动。\n- $\\sigma > 0$ 是扩散系数。\n- $u_t$ 是具有有限二次能量的循序可测控制。\n- 初始状态 $X_0$ 是可积的，其均值为 $m_0 := \\mathbb{E}[X_0]$。\n- 群体均值: $\\bar{m}_t := \\mathbb{E}[X_t]$。\n- 个体代价泛函: $J(u;\\bar{m}_T) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$。\n- 参数: $r>0$, $\\delta>0$, $\\alpha\\in\\mathbb{R}$。\n- 平均场博弈(MFG): 在给定 $\\bar{m}_T$ 的情况下，最优控制产生一个一致的群体均值的纳什均衡。\n- 平均场控制(MFC): 选择一个共同控制以最小化平均代价 $J(u;\\bar{m}_T)$，其中 $\\bar{m}_T$ 由 $u$ 引致。\n\n### 步骤 2: 使用提取的已知条件进行验证\n该问题具有科学依据，是适定的，并且是客观的。\n- **科学合理性**: 该问题是一个标准的线性二次 (LQ) 平均场博弈，这是随机微分方程和控制理论中一个成熟的课题。SDE、代价泛函和参数约束都是经典的。\n- **适定性**: 该问题结构清晰，提供了推导所需量的所有必要信息。任务 (a)、(b) 和 (c) 是具体的，并能得出明确的数学答案。\n- **客观性**: 该问题以精确的数学语言陈述，没有歧义或主观论断。\n\n### 步骤 3: 结论与行动\n该问题是有效的。我现在将着手解答。\n\n这是一个线性二次 (LQ) 随机博弈，可以求得显式解。我们将按顺序处理每个部分。\n\n**(a) 平均场博弈不动点方程的推导**\n\n在 MFG 框架下，每个个体针对一个*给定的*、固定的终端群体均值 $\\bar{m}_T$ 解决一个最优控制问题。个体寻求最小化：\n$$J(u) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$$\n受动态学 $dX_t = u_t\\,dt + \\sigma\\,dW_t$ 约束。这是一个标准的 LQ 随机控制问题。我们使用针对 SDE 的庞特里亚金极大值原理。哈密顿量为：\n$$H(t, x, y, u) = \\frac{r}{2} u^2 + y u$$\n一阶最优性条件为 $\\frac{\\partial H}{\\partial u} = ru + y = 0$，这给出了最优控制的候选形式：\n$$u_t^* = -\\frac{1}{r} Y_t$$\n其中 $(Y_t, Z_t)$ 是由倒向随机微分方程 (BSDE) 控制的伴随过程：\n$$dY_t = -\\frac{\\partial H}{\\partial x}(t, X_t^*, Y_t, u_t^*)\\,dt + Z_t\\,dW_t = 0 \\cdot dt + Z_t dW_t = Z_t dW_t$$\n其终端条件由终端代价的导数给出：\n$$Y_T = \\frac{\\partial}{\\partial x} \\left[\\frac{\\delta}{2}(x - \\alpha\\bar{m}_T)^2\\right]\\bigg|_{x=X_T^*} = \\delta(X_T^* - \\alpha\\bar{m}_T)$$\n由于 $dY_t = Z_t dW_t$， $Y_t$ 是一个鞅，其期望值 $\\bar{y}_t := \\mathbb{E}[Y_t]$ 必须随时间保持不变。令 $\\bar{y} := \\mathbb{E}[Y_t]$ 对所有 $t \\in [0,T]$ 成立。\n对 $Y_T$ 的终端条件取期望：\n$$\\bar{y} = \\mathbb{E}[Y_T] = \\mathbb{E}[\\delta(X_T^* - \\alpha\\bar{m}_T)] = \\delta(\\mathbb{E}[X_T^*] - \\alpha\\bar{m}_T) = \\delta(\\bar{m}_T^* - \\alpha\\bar{m}_T)$$\n其中 $\\bar{m}_T^*$ 是由个体的最优控制引致的终端均值。\n\n接下来，我们求解均值状态 $\\bar{m}_t^* = \\mathbb{E}[X_t^*]$ 的动态学。对状态 SDE 取期望：\n$$d\\bar{m}_t^* = \\mathbb{E}[dX_t^*] = \\mathbb{E}[u_t^*]\\,dt + \\mathbb{E}[\\sigma\\,dW_t] = \\mathbb{E}[-\\frac{1}{r}Y_t]\\,dt = -\\frac{1}{r}\\bar{y}\\,dt$$\n从 $t=0$ 到 $t=T$ 积分：\n$$\\bar{m}_T^* - \\bar{m}_0 = \\int_0^T \\left(-\\frac{1}{r}\\bar{y}\\right) dt = -\\frac{T}{r}\\bar{y}$$\n已知 $\\bar{m}_0=m_0$，我们有 $\\bar{m}_T^* = m_0 - \\frac{T}{r}\\bar{y}$。\n\n至此，对于一个给定的预期均值 $\\bar{m}_T$，我们得到了两个关联结果均值 $\\bar{m}_T^*$ 和常数期望伴随值 $\\bar{y}$ 的方程：\n1. $\\bar{y} = \\delta(\\bar{m}_T^* - \\alpha\\bar{m}_T)$\n2. $\\bar{m}_T^* = m_0 - \\frac{T}{r}\\bar{y}$\n\nMFG 的一致性条件要求预期均值等于结果均值：$\\bar{m}_T^* = \\bar{m}_T$。将此代入方程中：\n1. $\\bar{y} = \\delta(\\bar{m}_T - \\alpha\\bar{m}_T) = \\delta(1-\\alpha)\\bar{m}_T$\n2. $\\bar{m}_T = m_0 - \\frac{T}{r}\\bar{y}$\n\n我们现在可以找到 $\\bar{m}_T$ 的不动点方程。将第一个方程中的 $\\bar{y}$ 代入第二个方程：\n$$\\bar{m}_T = m_0 - \\frac{T}{r} \\left( \\delta(1-\\alpha)\\bar{m}_T \\right)$$\n重新整理各项以求解 $\\bar{m}_T$：\n$$\\bar{m}_T + \\frac{\\delta T(1-\\alpha)}{r}\\bar{m}_T = m_0$$\n$$\\bar{m}_T \\left( 1 + \\frac{\\delta T(1-\\alpha)}{r} \\right) = m_0$$\n两边乘以 $r$：\n$$\\bar{m}_T \\left( r + \\delta T(1-\\alpha) \\right) = m_0 r$$\n这就是在平均场博弈中，终端均值 $\\bar{m}_T$ 所满足的标量不动点方程。\n\n**(b) Lasry–Lions 单调性与平均场控制的唯一性**\n\nLasry-Lions (LL) 单调性条件是 MFG 均衡唯一性的一个充分条件。对于本问题，耦合通过项 $\\frac{\\delta}{2}(X_T - \\alpha\\bar{m}_T)^2$ 实现。平均场施加在个体上的“力”可以被认为是代价函数相对于均值的导数，即 $-\\delta\\alpha(X_T - \\alpha\\bar{m}_T)$。当 $\\alpha>0$ 时，个体因接近 $\\alpha\\bar{m}_T$ 而受到惩罚，这可以解释为一种排斥性相互作用。这种排斥性相互作用通常会违反 LL 单调性条件。例如，如果 $\\alpha > 1$，从假定均值到结果均值的反馈回路的增益可能大于1，导致多重均衡。更正式地，从假定均值 $\\mu$到结果均值的映射是 $\\Phi(\\mu) = m_0 - \\frac{T\\delta(m_0-\\alpha\\mu)}{r+\\delta T}$。其导数 $\\Phi'(\\mu) = \\frac{\\alpha T \\delta}{r+\\delta T}$ 在 $\\alpha>0$ 时为正，这表明存在一个正反馈回路，这是非单调博弈的典型特征。因此，对于 $\\alpha>0$，系统具有非单调结构，LL 条件可能不成立，从而可能出现非唯一均衡。\n\n现在，考虑平均场控制（社会计划者）问题。计划者为所有个体选择一个共同的控制 $u_t$ 以最小化平均代价。由于控制是共同的，它必须是确定性的。一个代表性个体的状态是 $X_t = X_0 + \\int_0^t u_s\\,ds + \\sigma W_t$。均值状态是 $\\bar{m}_t = m_0 + \\int_0^t u_s\\,ds$。计划者的代价函数是：\n$$J_{MFC}(u) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$$\n让我们通过围绕均值展开来分析终端代价项：\n$$\\mathbb{E}[(X_T - \\alpha\\bar{m}_T)^2] = \\mathbb{E}[ ( (X_T - \\bar{m}_T) + (1-\\alpha)\\bar{m}_T )^2 ]$$\n$$= \\mathbb{E}[(X_T-\\bar{m}_T)^2] + 2(1-\\alpha)\\bar{m}_T\\mathbb{E}[X_T-\\bar{m}_T] + (1-\\alpha)^2\\bar{m}_T^2$$\n由于 $\\mathbb{E}[X_T-\\bar{m}_T]=0$，交叉项消失。第一项是 $X_T$ 的方差：\n$$\\text{Var}(X_T) = \\text{Var}(X_0 + \\text{const} + \\sigma W_T) = \\text{Var}(X_0) + \\sigma^2 T$$\n该方差与控制 $u_t$ 无关。因此，最小化 $J_{MFC}$ 等价于最小化：\n$$\\hat{J}_{MFC}(u) = \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta(1-\\alpha)^2}{2} \\bar{m}_T^2$$\n其中 $\\bar{m}_T = m_0 + \\int_0^T u_t\\,dt$。令 $U = \\int_0^T u_t\\,dt$。根据柯西-施瓦茨不等式，对于一个固定的 $U$ 值，当 $u_t$ 是常数时，即 $u_t = U/T$，积分 $\\int_0^T u_t^2\\,dt$ 取得最小值。在这种情况下，$\\int_0^T u_t^2\\,dt = (U/T)^2 T = U^2/T$。问题简化为最小化一个单标量变量 $U$ 的函数：\n$$F(U) = \\frac{r}{2T}U^2 + \\frac{\\delta(1-\\alpha)^2}{2}(m_0+U)^2$$\n这是关于 $U$ 的二次函数。为了检验最小值的唯一性，我们考察其二阶导数：\n$$F''(U) = \\frac{d^2 F}{d U^2} = \\frac{r}{T} + \\delta(1-\\alpha)^2$$\n由于 $r>0$，$T>0$，$\\delta>0$，且 $(1-\\alpha)^2 \\ge 0$，二阶导数 $F''(U)$ 对所有 $\\alpha \\in \\mathbb{R}$ 都是严格为正的。这证明了 $F(U)$ 是一个严格凸函数。一个严格凸函数至多有一个最小值点。由于 $F(U)$ 是一个开口向上的二次函数，它有一个唯一的全局最小值。因此，无论 $\\alpha$ 的值如何，平均场控制问题都存在唯一的最优控制和唯一的最优终端均值 $\\bar{m}_T$。\n\n**(c) MFG 中存在无限均衡的条件**\n\n我们考虑 $m_0=0$ 的特殊情况。(a) 部分的不动点方程变为：\n$$\\bar{m}_T \\left( r + \\delta T(1-\\alpha) \\right) = 0$$\n这个方程决定了 $\\bar{m}_T$ 可能的均衡值。\n- 如果系数 $r + \\delta T(1-\\alpha) \\neq 0$，唯一的解是 $\\bar{m}_T=0$。在这种情况下，MFG 有一个唯一的纳什均衡。\n- 如果系数 $r + \\delta T(1-\\alpha) = 0$，方程变为 $0 \\cdot \\bar{m}_T = 0$，这对任何 $\\bar{m}_T \\in \\mathbb{R}$ 都成立。在这种情况下，任何实数都是一个有效的均衡终端均值，这意味着存在无限多个纳什均衡。\n\n我们正在寻找导致第二种情况的耦合强度 $\\alpha^{\\star}$。我们将系数设为零：\n$$r + \\delta T(1-\\alpha^{\\star}) = 0$$\n$$r + \\delta T - \\delta T \\alpha^{\\star} = 0$$\n$$\\delta T \\alpha^{\\star} = r + \\delta T$$\n$$\\alpha^{\\star} = \\frac{r+\\delta T}{\\delta T} = 1 + \\frac{r}{\\delta T}$$\n在这个临界值 $\\alpha^{\\star}$ 下，平均场博弈存在无限多个均衡。正如在 (b) 部分所证明的，平均场控制问题的目标函数仍然是严格凸的，因为 $F''(U) = \\frac{r}{T} + \\delta(1-\\alpha^{\\star})^2 = \\frac{r}{T} + \\delta(-\\frac{r}{\\delta T})^2 = \\frac{r}{T} + \\frac{r^2}{\\delta T^2} > 0$。因此，即使在 $\\alpha = \\alpha^{\\star}$时，社会计划者问题仍然有唯一的解（即 $U=0$，导致 $\\bar{m}_T = 0$），这与博弈框架下均衡的多重性形成鲜明对比。", "answer": "$$\\boxed{1 + \\frac{r}{\\delta T}}$$"}]}