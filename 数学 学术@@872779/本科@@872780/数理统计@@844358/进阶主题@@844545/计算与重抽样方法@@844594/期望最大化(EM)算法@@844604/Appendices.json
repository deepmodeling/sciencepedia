{"hands_on_practices": [{"introduction": "理论是抽象的，但实践可以使其具体化。本练习将带领您完成期望最大化（EM）算法的单次完整迭代，处理一个经典的高斯混合模型（GMM）。通过为一个小型数据集手动计算 E-步 和 M-步，您将亲身体验 EM 算法如何以迭代方式“软分配”数据点并更新模型参数 [@problem_id:1960172]。", "problem": "一位研究人员正在分析一个测量数据集，该数据集被认为是从两个不同总体 A 和 B 的混合中抽样得到的。他们决定使用一个双组分高斯混合模型（GMM）对数据进行建模。单个数据点 $x$ 的概率密度函数（PDF）由下式给出：\n$$p(x | \\theta) = \\pi_A \\mathcal{N}(x | \\mu_A, \\sigma_A^2) + \\pi_B \\mathcal{N}(x | \\mu_B, \\sigma_B^2)$$\n其中 $\\pi_A$ 和 $\\pi_B$ 是混合比例（满足 $\\pi_A + \\pi_B = 1$），而 $\\mathcal{N}(x | \\mu, \\sigma^2)$ 是高斯概率密度函数：\n$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n\n数据集包含以下五个测量值：$\\{4.0, 4.5, 5.0, 8.0, 9.0\\}$。\n为简单起见，假设两个组分的方差是已知的，并固定为 $\\sigma_A^2 = \\sigma_B^2 = 1.0$。\n\n研究人员按如下方式初始化期望最大化（EM）算法的模型参数：\n- 初始均值：$\\mu_A^{(0)} = 4.2$ 和 $\\mu_B^{(0)} = 8.8$。\n- 初始混合比例：$\\pi_A^{(0)} = 0.5$ 和 $\\pi_B^{(0)} = 0.5$。\n\n你的任务是执行一次完整的 EM 算法迭代，包括一个期望步骤（E-step）和一个最大化步骤（M-step），以计算更新后的参数。\n\n计算第一个组分均值的更新值 $\\mu_A^{(1)}$。将你的答案报告为一个实数，四舍五入到四位有效数字。", "solution": "我们对双组分高斯混合模型应用一次期望最大化（EM）迭代，其方差固定为 $\\sigma_{A}^{2}=\\sigma_{B}^{2}=1$，初始参数为 $\\mu_{A}^{(0)}=4.2$，$\\mu_{B}^{(0)}=8.8$，$\\pi_{A}^{(0)}=\\pi_{B}^{(0)}=0.5$。\n\nE-步（责任）：\n对于每个数据点 $x_{i}$，组分 $A$ 的责任为\n$$\nr_{iA} \\equiv \\gamma_{iA} = \\frac{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1)}{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1) + \\pi_{B}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{B}^{(0)},1)}.\n$$\n由于 $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}$ 且方差相等，公共因子可以消去：\n$$\nr_{iA}=\\frac{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)}{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)+\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}=\\frac{1}{1+\\exp\\!\\left(\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}-(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}.\n$$\n对于数据集 $\\{4.0,4.5,5.0,8.0,9.0\\}$ 和 $(\\mu_{A}^{(0)},\\mu_{B}^{(0)})=(4.2,8.8)$：\n- 对于 $x_{1}=4.0$：$(x_{1}-\\mu_{A}^{(0)})^{2}=0.04$，$(x_{1}-\\mu_{B}^{(0)})^{2}=23.04$，所以\n$$\nr_{1A}=\\frac{1}{1+\\exp(-11.5)}\\approx 0.9999898625.\n$$\n- 对于 $x_{2}=4.5$：$(x_{2}-\\mu_{A}^{(0)})^{2}=0.09$，$(x_{2}-\\mu_{B}^{(0)})^{2}=18.49$，所以\n$$\nr_{2A}=\\frac{1}{1+\\exp(-9.2)}\\approx 0.9998989606.\n$$\n- 对于 $x_{3}=5.0$：$(x_{3}-\\mu_{A}^{(0)})^{2}=0.64$，$(x_{3}-\\mu_{B}^{(0)})^{2}=14.44$，所以\n$$\nr_{3A}=\\frac{1}{1+\\exp(-6.9)}\\approx 0.9989932309.\n$$\n- 对于 $x_{4}=8.0$：$(x_{4}-\\mu_{A}^{(0)})^{2}=14.44$，$(x_{4}-\\mu_{B}^{(0)})^{2}=0.64$，所以\n$$\nr_{4A}=\\frac{1}{1+\\exp(6.9)}=1-r_{3A}\\approx 0.0010067691.\n$$\n- 对于 $x_{5}=9.0$：$(x_{5}-\\mu_{A}^{(0)})^{2}=23.04$，$(x_{5}-\\mu_{B}^{(0)})^{2}=0.04$，所以\n$$\nr_{5A}=\\frac{1}{1+\\exp(11.5)}=1-r_{1A}\\approx 0.0000101375.\n$$\n注意，根据对称性，$r_{1A}+r_{5A}=1$ 且 $r_{3A}+r_{4A}=1$。\n\nM-步（更新组分 A 的均值）：\n在方差固定的情况下，更新后的均值是责任加权平均值：\n$$\n\\mu_{A}^{(1)}=\\frac{\\sum_{i=1}^{5} r_{iA} x_{i}}{\\sum_{i=1}^{5} r_{iA}}.\n$$\n使用对称性计算分母：\n$$\n\\sum_{i=1}^{5} r_{iA}=(r_{1A}+r_{5A})+(r_{3A}+r_{4A})+r_{2A}=2+r_{2A}\\approx 2.9998989606.\n$$\n计算分子；将对称的配对分组：\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i}=(r_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0) + (r_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0) + r_{2A}\\cdot 4.5.\n$$\n使用 $r_{5A}=1-r_{1A}$ 和 $r_{4A}=1-r_{3A}$：\n$$\nr_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0 = 9 - 5 r_{1A},\\quad\nr_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0 = 8 - 3 r_{3A}.\n$$\n因此\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i} = 17 - 5 r_{1A} - 3 r_{3A} + 4.5 r_{2A} \\approx 13.5026163178.\n$$\n所以，\n$$\n\\mu_{A}^{(1)}=\\frac{13.5026163178}{2.9998989606}\\approx 4.501023693.\n$$\n四舍五入到四位有效数字，更新后的均值为 $4.501$。", "answer": "$$\\boxed{4.501}$$", "id": "1960172"}, {"introduction": "EM 算法的强大之处在于其灵活性，能够适应各种模型约束。这个练习探讨了一个常见情景：混合模型中的多个成分共享某些参数。您将推导 M-步的更新规则，当两个高斯分布被约束为具有相同的均值时，这能加深您对 EM 算法核心优化原理的理解 [@problem_id:1960133]。", "problem": "在一个数据分析任务中，我们使用两个正态分布的混合来对一组一维观测值 $X = \\{x_1, x_2, \\dots, x_n\\}$ 进行建模。该模型施加了一个特定的约束：两个分布必须共享相同的均值 $\\mu$。因此，混合模型的两个分量由 $N(\\mu, \\sigma_1^2)$ 和 $N(\\mu, \\sigma_2^2)$ 给出，混合比例分别为 $1-\\pi$ 和 $\\pi$。该模型的完整参数集是 $\\theta = (\\pi, \\mu, \\sigma_1^2, \\sigma_2^2)$。\n\n期望最大化（EM）算法是一种迭代方法，用于寻找此类模型参数的最大似然估计。该算法在E步（期望）和M步（最大化）之间交替进行。M步包括相对于参数 $\\theta$ 最大化期望完全数据对数似然 $Q(\\theta | \\theta^{(t)})$。该函数定义为：\n\n$$Q(\\theta|\\theta^{(t)}) = \\sum_{i=1}^{n} \\left[ \\gamma_{i1}^{(t)} \\ln\\left((1-\\pi) \\phi(x_i; \\mu, \\sigma_1^2)\\right) + \\gamma_{i2}^{(t)} \\ln\\left(\\pi \\phi(x_i; \\mu, \\sigma_2^2)\\right) \\right]$$\n\n其中 $\\phi(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$ 是正态分布的概率密度函数。项 $\\gamma_{i1}^{(t)}$ 和 $\\gamma_{i2}^{(t)}$ 是在迭代 $t$ 的E步中计算的“责任”；它们分别表示在给定迭代 $t$ 的参数估计值的情况下，观测值 $x_i$ 属于分量1或分量2的后验概率。\n\n您的任务是推导公共均值参数 $\\mu$ 的更新表达式。这个更新后的估计值，表示为 $\\mu^{(t+1)}$，是使函数 $Q(\\theta|\\theta^{(t)})$ 最大化的 $\\mu$ 的值。为简化优化过程，在推导 $\\mu$ 的更新时，您应将方差参数 $\\sigma_1^2$ 和 $\\sigma_2^2$ 视为固定在它们在迭代 $t$ 的当前估计值 $(\\sigma_1^2)^{(t)}$ 和 $(\\sigma_2^2)^{(t)}$ 上。\n\n请以 $\\mu^{(t+1)}$ 的符号表达式形式给出您的最终答案。", "solution": "我们必须在保持 $(\\sigma_{1}^{2})^{(t)}$ 和 $(\\sigma_{2}^{2})^{(t)}$ 固定，并使用来自E步的责任 $\\gamma_{i1}^{(t)}$ 和 $\\gamma_{i2}^{(t)}$ 的情况下，关于公共均值 $\\mu$ 最大化 $Q(\\theta\\mid\\theta^{(t)})$。涉及 $\\pi$ 的项不依赖于 $\\mu$，因此在对 $\\mu$ 进行优化时可以舍去。使用 $\\ln\\phi(x;\\mu,\\sigma^{2})=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}$，$Q$ 中依赖于 $\\mu$ 的部分是\n$$\nQ_{\\mu}(\\mu)= -\\frac{1}{2}\\sum_{i=1}^{n}\\left[\\gamma_{i1}^{(t)}\\frac{(x_{i}-\\mu)^{2}}{(\\sigma_{1}^{2})^{(t)}}+\\gamma_{i2}^{(t)}\\frac{(x_{i}-\\mu)^{2}}{(\\sigma_{2}^{2})^{(t)}}\\right]+C,\n$$\n其中 $C$ 是一个不依赖于 $\\mu$ 的常数。定义权重 $w_{i1}=\\gamma_{i1}^{(t)}/(\\sigma_{1}^{2})^{(t)}$ 和 $w_{i2}=\\gamma_{i2}^{(t)}/(\\sigma_{2}^{2})^{(t)}$。那么\n$$\nQ_{\\mu}(\\mu)= -\\frac{1}{2}\\sum_{i=1}^{n}\\left[(w_{i1}+w_{i2})(x_{i}-\\mu)^{2}\\right]+C.\n$$\n对 $\\mu$ 求导并令其为零：\n$$\n\\frac{\\partial Q_{\\mu}}{\\partial \\mu}=\\sum_{i=1}^{n}(w_{i1}+w_{i2})(x_{i}-\\mu)=0\n\\quad\\Longrightarrow\\quad\n\\mu^{(t+1)}=\\frac{\\sum_{i=1}^{n}(w_{i1}+w_{i2})x_{i}}{\\sum_{i=1}^{n}(w_{i1}+w_{i2})}.\n$$\n将 $w_{i1}$ 和 $w_{i2}$ 代回，得到\n$$\n\\mu^{(t+1)}=\\frac{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}x_{i}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}x_{i}}{(\\sigma_{2}^{2})^{(t)}}\\right)}{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}}{(\\sigma_{2}^{2})^{(t)}}\\right)}.\n$$\n二阶导数为 $\\frac{\\partial^{2} Q_{\\mu}}{\\partial \\mu^{2}}=-\\sum_{i=1}^{n}(w_{i1}+w_{i2})  0$，因为方差是正的且 $\\gamma_{ik}^{(t)}\\geq 0$，这确认了该临界点是一个最大值点。", "answer": "$$\\boxed{\\mu^{(t+1)}=\\frac{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}x_{i}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}x_{i}}{(\\sigma_{2}^{2})^{(t)}}\\right)}{\\sum_{i=1}^{n}\\left(\\frac{\\gamma_{i1}^{(t)}}{(\\sigma_{1}^{2})^{(t)}}+\\frac{\\gamma_{i2}^{(t)}}{(\\sigma_{2}^{2})^{(t)}}\\right)}}$$", "id": "1960133"}, {"introduction": "除了最大似然估计（MLE），EM 框架还可以优雅地融入贝叶斯推断中，用于寻找最大后验（MAP）估计。在这个练习中，我们将先验知识（以 Gamma 分布的形式）引入泊松混合模型中。您的任务是修改 M-步以最大化后验概率而不是似然函数，从而展示 EM 算法在更广泛的统计推断领域中的应用 [@problem_id:1960196]。", "problem": "一位生物物理学家正在分析来自单分子荧光实验的数据。观测数据由在一系列连续时间间隔内记录的 $n$ 个独立的光子计数 $X_1, X_2, \\dots, X_n$ 组成。已知该分子会在两个不同的构象状态（状态1和状态2）之间随机切换。当处于状态 $k$ 时，光子计数服从一个泊松分布，其速率参数 $\\lambda_k$ 未知。任何一次测量来自状态1的概率是一个已知的常数 $\\pi$，来自状态2的概率是 $1-\\pi$。\n\n为了估计未知的速率 $\\lambda_1$ 和 $\\lambda_2$，使用了一种迭代算法。由于存在关于该系统的先验知识，因此采用了贝叶斯方法。目标是找到参数的最大后验 (MAP) 估计，而不仅仅是最大似然估计。速率参数的先验分布被选为伽马分布，因为它们是泊松似然的共轭先验。具体来说，$\\lambda_k$ 的先验是 $\\text{Gamma}(\\alpha_k, \\beta_k)$ 分布，其概率密度函数由 $p(\\lambda_k) \\propto \\lambda_k^{\\alpha_k-1} \\exp(-\\beta_k \\lambda_k)$ 给出（对于 $\\lambda_k  0$）。\n\n估计过程使用期望最大化 (EM) 算法的一个修改版本来执行。标准的 M 步（该步骤最大化期望完全数据对数似然）被一个最大化期望完全数据对数后验的步骤所取代。\n\n设 $\\theta^{(t)} = (\\lambda_1^{(t)}, \\lambda_2^{(t)})$ 为第 $t$ 次迭代时的参数估计。在期望步（E步）中，需要计算“责任”（responsibilities），即在给定数据和当前参数估计的条件下，观测值 $X_i$ 属于状态 $k$ 的后验概率。我们将状态1的责任记为 $\\gamma_{i,1}^{(t)} = P(\\text{State}=1 | X_i, \\theta^{(t)})$。\n\n你的任务是推导在修改后的最大化步（M步）中参数 $\\lambda_1$ 的更新方程。请用观测数据 $\\{X_i\\}_{i=1}^n$、责任 $\\{\\gamma_{i,1}^{(t)}\\}_{i=1}^n$ 以及 $\\lambda_1$ 先验分布的超参数 $\\alpha_1$ 和 $\\beta_1$ 来表示更新后的估计值 $\\lambda_1^{(t+1)}$。", "solution": "引入潜在指示变量 $z_{i,1} \\in \\{0,1\\}$，如果观测值 $X_{i}$ 来自状态1，则 $z_{i,1}=1$，否则 $z_{i,1}=0$。完全数据对数似然中涉及 $\\lambda_{1}$ 的项为\n$$\n\\sum_{i=1}^{n} z_{i,1} \\left( X_{i} \\ln \\lambda_{1} - \\lambda_{1} \\right) + \\text{const},\n$$\n因为对于泊松模型，$\\ln p(X_{i} \\mid \\lambda_{1}) = X_{i} \\ln \\lambda_{1} - \\lambda_{1} - \\ln X_{i}!.\n$\n$\\lambda_{1} \\sim \\text{Gamma}(\\alpha_{1},\\beta_{1})$（形状-速率参数化）的对数先验为\n$$\n(\\alpha_{1}-1)\\ln \\lambda_{1} - \\beta_{1} \\lambda_{1} + \\text{const}.\n$$\n在 E 步中，将 $z_{i,1}$ 替换为给定当前参数下的条件期望，即 $\\gamma_{i,1}^{(t)} = P(\\text{State}=1 \\mid X_{i}, \\theta^{(t)})$。于是，关于 $\\lambda_{1}$ 的期望完全数据对数后验为\n$$\nQ(\\lambda_{1}) = \\left( \\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i} \\right) \\ln \\lambda_{1} - \\left( \\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} \\right) \\lambda_{1} + \\text{const}.\n$$\n对 $\\lambda_{1}$ 求导，令其等于零，然后求解：\n$$\n\\frac{\\partial Q}{\\partial \\lambda_{1}} = \\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\lambda_{1}} - \\left( \\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} \\right) = 0\n$$\n这得到\n$$\n\\lambda_{1}^{(t+1)} = \\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\beta_{1} + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)}}.\n$$\n二阶导数为\n$$\n\\frac{\\partial^{2} Q}{\\partial \\lambda_{1}^{2}} = -\\frac{\\alpha_{1}-1 + \\sum_{i=1}^{n} \\gamma_{i,1}^{(t)} X_{i}}{\\lambda_{1}^{2}},\n$$\n只要分子为正，该二阶导数就为负，这在通常条件下确认了所求得的是一个最大值。", "answer": "$$\\boxed{\\frac{\\alpha_{1}-1+\\sum_{i=1}^{n}\\gamma_{i,1}^{(t)} X_{i}}{\\beta_{1}+\\sum_{i=1}^{n}\\gamma_{i,1}^{(t)}}}$$", "id": "1960196"}]}