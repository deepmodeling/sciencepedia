{"hands_on_practices": [{"introduction": "要理解一个新方法，将其与我们熟悉的概念联系起来往往很有帮助。Lasso 回归的目标函数包含两部分：残差平方和（RSS）和 $L_1$ 惩罚项。这个练习将探讨一个特殊情况：当惩罚参数 $\\lambda$ 设置为零时会发生什么 [@problem_id:1928607]。通过这个计算，你将发现 Lasso 回归与一种更基础的统计方法——普通最小二乘法（OLS）之间的直接联系，从而建立起对 Lasso 作为一种广义回归模型的直观认识。", "problem": "在统计建模中，最小绝对收缩和选择算子（LASSO）是一种回归分析方法，它同时进行变量选择和正则化，以提高其生成的统计模型的预测准确性和可解释性。对于一个有 $p$ 个预测变量的一般线性模型，其系数 $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_p)$ 的LASSO估计是最小化以下目标函数的值：\n$$\n\\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| = \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j\\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n$$\n其中 $(x_{i1}, \\dots, x_{ip}, y_i)$ 是 $i=1, \\dots, n$ 的观测数据，$\\beta_0$ 是截距，$\\lambda \\ge 0$ 是一个调整参数。\n\n一位数据科学家正在分析一个数据集，以拟合一个形式为 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ 的简单线性回归模型。他们决定使用LASSO框架来确定模型系数。然而，作为一个初步的探索性步骤，他们将调整参数 $\\lambda$ 精确地设置为零。该数据集包含以下四个 $(x_i, y_i)$ 数据点：\n$$\n(1, 2), (2, 3), (3, 5), (4, 6)\n$$\n计算在这种特定设置下获得的系数估计值 $\\hat{\\beta}_1$。将您的最终答案四舍五入到三位有效数字。", "solution": "设置 $\\lambda=0$ 会将LASSO目标函数简化为残差平方和，因此最小化估计量是普通最小二乘（OLS）解。对于带截距的简单线性回归，OLS斜率为\n$$\n\\hat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}},\n$$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。对于数据 $(1,2),(2,3),(3,5),(4,6)$，其中 $n=4$，\n$$\n\\bar{x}=\\frac{1+2+3+4}{4}=\\frac{10}{4}=\\frac{5}{2},\\quad \\bar{y}=\\frac{2+3+5+6}{4}=\\frac{16}{4}=4.\n$$\n计算分子：\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})(y_{i}-\\bar{y})=(1-\\tfrac{5}{2})(2-4)+(2-\\tfrac{5}{2})(3-4)+(3-\\tfrac{5}{2})(5-4)+(4-\\tfrac{5}{2})(6-4).\n$$\n计算各项乘积：\n$$\n(1-\\tfrac{5}{2})=-\\frac{3}{2},\\ (2-4)=-2\\ \\Rightarrow\\ \\left(-\\frac{3}{2}\\right)(-2)=3,\\\\\n(2-\\tfrac{5}{2})=-\\frac{1}{2},\\ (3-4)=-1\\ \\Rightarrow\\ \\left(-\\frac{1}{2}\\right)(-1)=\\frac{1}{2},\\\\\n(3-\\tfrac{5}{2})=\\frac{1}{2},\\ (5-4)=1\\ \\Rightarrow\\ \\left(\\frac{1}{2}\\right)(1)=\\frac{1}{2},\\\\\n(4-\\tfrac{5}{2})=\\frac{3}{2},\\ (6-4)=2\\ \\Rightarrow\\ \\left(\\frac{3}{2}\\right)(2)=3.\n$$\n求和得到\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})(y_{i}-\\bar{y})=3+\\frac{1}{2}+\\frac{1}{2}+3=7.\n$$\n计算分母：\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})^{2}=\\left(-\\frac{3}{2}\\right)^{2}+\\left(-\\frac{1}{2}\\right)^{2}+\\left(\\frac{1}{2}\\right)^{2}+\\left(\\frac{3}{2}\\right)^{2}=\\frac{9}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{9}{4}=\\frac{20}{4}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{1}=\\frac{7}{5}=1.4.\n$$\n四舍五入到三位有效数字得到 $1.40$。", "answer": "$$\\boxed{1.40}$$", "id": "1928607"}, {"introduction": "在考察了没有惩罚（$\\lambda=0$）的极端情况后，我们自然会想知道另一个极端会怎样。这个练习通过一个思想实验，探讨当惩罚参数 $\\lambda$ 趋向于无穷大时，Lasso 系数估计会发生什么变化 [@problem_id:1928648]。理解这种极限行为对于掌握 Lasso 的“收缩”效应至关重要，它揭示了 Lasso 是如何通过增强惩罚来强制实现模型稀疏性的。", "problem": "在线性模型 $y = \\beta_0 + \\sum_{j=1}^{p} x_j \\beta_j + \\epsilon$ 的背景下，最小绝对收缩和选择算子 (LASSO) 方法用于找到系数 $(\\beta_0, \\beta_1, \\dots, \\beta_p)$ 的估计值。这些估计值是使以下目标函数最小化的值：\n$$ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n这里，$(y_i, x_{i1}, \\dots, x_{ip})$ (对于 $i=1, \\dots, n$) 是观测到的数据点。项 $\\lambda \\ge 0$ 是一个控制惩罚强度的非负调整参数。请注意，截距项 $\\beta_0$ 不包含在惩罚项中。\n\n考虑当调整参数 $\\lambda$ 增大至一个非常大的值时（即，当 $\\lambda \\to \\infty$ 时），估计出的系数会发生什么变化。以下哪个陈述正确地描述了 LASSO 系数估计值的极限行为？\n\nA. 所有系数估计值，包括截距 $\\hat{\\beta}_0$，都被强制为零。\n\nB. 非截距系数估计值 ($\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) 被强制为零，而截距估计值 $\\hat{\\beta}_0$ 收敛于响应变量的样本均值 $\\bar{y}$。\n\nC. 所有系数估计值 ($\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) 收敛于其普通最小二乘 (OLS) 估计值。\n\nD. 至少有一个非截距系数估计值保持非零，而所有其他系数都被强制为零。\n\nE. 非截距系数估计值 ($\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) 的量级与 $\\lambda$ 成比例增长。", "solution": "我们考虑 LASSO 的目标函数\n$$\nQ_{\\lambda}(\\beta_{0},\\beta_{1},\\dots,\\beta_{p})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}+\\lambda\\sum_{j=1}^{p}|\\beta_{j}|,\n$$\n其中 $\\lambda\\ge 0$，且截距 $\\beta_{0}$ 未被惩罚。\n\n步骤 1：当 $\\lambda\\to\\infty$ 时非截距系数的行为。\n- 对于任何候选解 $(\\beta_{0},\\beta_{1},\\dots,\\beta_{p})$，如果对于某个 $j\\ge 1$ 存在至少一个 $\\beta_{j}\\neq 0$，则惩罚项满足 $\\sum_{j=1}^{p}|\\beta_{j}|>0$，因此当 $\\lambda\\to\\infty$ 时 $\\lambda\\sum_{j=1}^{p}|\\beta_{j}|\\to\\infty$。\n- 反之，考虑将所有非截距系数设为零的替代方案：$\\tilde{\\beta}_{j}=0$ (对于 $j\\ge 1$)，并选择 $\\tilde{\\beta}_{0}$ 以最小化残差平方和。对于这个替代方案，惩罚项对所有 $\\lambda$ 恒为零，且残差平方和是有限的。\n- 因此，对于足够大的 $\\lambda$，任何具有某个 $\\beta_{j}\\neq 0$ 的解的目标函数值都严格大于对所有 $j\\ge 1$ 都有 $\\beta_{j}=0$ 的解。因此，在极限 $\\lambda\\to\\infty$ 下，最小化器必须满足\n$$\n\\hat{\\beta}_{j}(\\lambda)\\to 0 \\quad \\text{对于所有 } j=1,\\dots,p.\n$$\n\n步骤 2：当 $\\beta_{1}=\\dots=\\beta_{p}=0$ 时截距的极限值。\n- 当 $j\\ge 1$ 时，$\\beta_{j}=0$，目标函数简化为最小化关于 $\\beta_{0}$ 的残差平方和：\n$$\nR(\\beta_{0})=\\sum_{i=1}^{n}(y_{i}-\\beta_{0})^{2}.\n$$\n- 求导并令其为零，得到一阶条件：\n$$\n\\frac{dR}{d\\beta_{0}}=-2\\sum_{i=1}^{n}(y_{i}-\\beta_{0})=0 \\quad \\Longrightarrow \\quad n\\beta_{0}=\\sum_{i=1}^{n}y_{i}.\n$$\n因此，\n$$\n\\beta_{0}=\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}.\n$$\n- 二阶导数为 $\\frac{d^{2}R}{d\\beta_{0}^{2}}=2n>0$，这证实了它是一个最小值。\n\n结论：当 $\\lambda\\to\\infty$ 时，非截距系数被驱动为零，而截距收敛到样本均值 $\\bar{y}$。这对应于选项 B。", "answer": "$$\\boxed{B}$$", "id": "1928648"}, {"introduction": "我们已经探索了调节参数 $\\lambda$ 的两个极端情况，现在是时候深入了解 Lasso 特征选择机制的核心了。在这个练习中，你将推导出一个精确的 $\\lambda$ 值，在该值下，一个预测变量的系数将首次被压缩至零 [@problem_id:1928602]。通过在一个简化的正交设定下解决这个问题，你将获得关于 Lasso 回归路径如何形成以及变量如何被依次从模型中剔除的定量理解。", "problem": "考虑一个包含 $n$ 个观测值的线性回归模型。该模型有一个响应向量 $\\mathbf{y}$ 和两个预测向量 $\\mathbf{x}_1$ 和 $\\mathbf{x}_2$。所有变量都假定已经中心化，均值为零，因此模型中不需要截距项。这两个预测向量经过处理，使得它们是正交的，并且它们的平方范数等于观测值的数量 $n$。具体来说，它们满足以下条件：\n$$ \\sum_{i=1}^n x_{i1}^2 = n $$\n$$ \\sum_{i=1}^n x_{i2}^2 = n $$\n$$ \\sum_{i=1}^n x_{i1} x_{i2} = 0 $$\n回归模型由 $y_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$ 给出。\n\n系数 $\\beta_1$ 和 $\\beta_2$ 使用最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO) 方法进行估计。LASSO 估计值，记作 $\\hat{\\beta}_1(\\lambda)$ 和 $\\hat{\\beta}_2(\\lambda)$，是在给定调优参数 $\\lambda > 0$ 时最小化目标函数的值：\n$$ L(\\beta_1, \\beta_2) = \\frac{1}{2} \\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (|\\beta_1| + |\\beta_2|) $$\n当调优参数 $\\lambda$ 从 0 开始增加时，估计系数的量值会连续地向零收缩。令 $S_1 = \\sum_{i=1}^n x_{i1} y_i$ 且 $S_2 = \\sum_{i=1}^n x_{i2} y_i$。假设我们处于 $|S_1| > |S_2| > 0$ 的情景中，这意味着对于小的非零 $\\lambda$ 值，$\\hat{\\beta}_1(\\lambda)$ 和 $\\hat{\\beta}_2(\\lambda)$ 都是非零的。\n\n确定调优参数 $\\lambda$ 的确切值，使得系数估计值 $\\hat{\\beta}_2(\\lambda)$ 首次等于零。用 $S_1$ 和/或 $S_2$ 表示你的答案。", "solution": "令 $X=[\\mathbf{x}_{1}\\ \\mathbf{x}_{2}]$，并注意给定条件意味着 $X^{\\top}X=n I_{2}$ 且 $X^{\\top}\\mathbf{y}=(S_{1},S_{2})^{\\top}$。LASSO 目标函数可以写为\n$$\nL(\\beta_{1},\\beta_{2})=\\frac{1}{2}\\|\\mathbf{y}-X\\beta\\|^{2}+\\lambda\\left(|\\beta_{1}|+|\\beta_{2}|\\right),\n$$\n使用 $X^{\\top}X=n I_{2}$ 和 $X^{\\top}\\mathbf{y}=(S_{1},S_{2})^{\\top}$ 将其展开为\n$$\nL(\\beta_{1},\\beta_{2})=\\frac{1}{2}\\mathbf{y}^{\\top}\\mathbf{y}-S_{1}\\beta_{1}-S_{2}\\beta_{2}+\\frac{n}{2}\\left(\\beta_{1}^{2}+\\beta_{2}^{2}\\right)+\\lambda\\left(|\\beta_{1}|+|\\beta_{2}|\\right).\n$$\n这个表达式在 $\\beta_{1}$ 和 $\\beta_{2}$ 上是可分的。对于 $j\\in\\{1,2\\}$，定义\n$$\nf_{j}(\\beta_{j})=\\frac{n}{2}\\beta_{j}^{2}-S_{j}\\beta_{j}+\\lambda|\\beta_{j}|.\n$$\n我们最小化 $f_{2}(\\beta_{2})$。使用次梯度最优性条件：\n- 如果 $\\beta_{2}\\neq 0$，令 $s_{2}=\\operatorname{sign}(\\beta_{2})$，则一阶条件为\n$$\nn\\beta_{2}-S_{2}+\\lambda s_{2}=0\\quad\\Longrightarrow\\quad \\beta_{2}=\\frac{S_{2}-\\lambda s_{2}}{n}.\n$$\n一致性要求 $s_{2}=\\operatorname{sign}(\\beta_{2})=\\operatorname{sign}(S_{2}-\\lambda s_{2})$，这在 $|S_{2}|>\\lambda$ 时恰好成立。在这种情况下，\n$$\n\\hat{\\beta}_{2}(\\lambda)=\\frac{\\operatorname{sign}(S_{2})\\left(|S_{2}|-\\lambda\\right)}{n}.\n$$\n- 如果 $\\beta_{2}=0$，次梯度条件为对于某个 $u\\in[-1,1]$ 有 $0\\in -S_{2}+\\lambda u$，这等价于 $|S_{2}|\\le \\lambda$。\n\n因此，当 $\\lambda$ 从 0 开始增加时，$\\hat{\\beta}_{2}(\\lambda)$ 在 $\\lambda$ 达到 $|S_{2}|$ 时恰好变为零。在假设 $|S_{1}|>|S_{2}|>0$ 下，$\\hat{\\beta}_{2}$ 是第一个在 $\\lambda=|S_{2}|$ 时变为零的系数。", "answer": "$$\\boxed{|S_{2}|}$$", "id": "1928602"}]}