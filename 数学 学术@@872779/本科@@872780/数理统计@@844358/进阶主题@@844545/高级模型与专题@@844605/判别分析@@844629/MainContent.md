## 引言
判别分析（Discriminant Analysis）是统计学中用于分类和[降维](@entry_id:142982)的一块基石。它的重要性不仅在于其作为一种强大的预测工具，更在于它为理解数据类别间的结构差异提供了一个清晰且可解释的数学框架。然而，许多从业者虽然会应用判别分析，却常常忽略其背后深刻的统计原理——从贝叶斯决策理论到[高斯分布](@entry_id:154414)假设——而这些恰恰是决定其成败和适用边界的关键。

本文旨在填补这一认知鸿沟。我们将系统性地剖析判别分析的内在逻辑与外在应用。在“原理与机制”一章中，我们将从[贝叶斯定理](@entry_id:151040)出发，推导出线性与二次判别分析的数学形式，并探讨其核心假设与局限性。接着，在“应用与跨学科联系”一章，我们将展示判别分析如何在从神经科学到核物理的广阔领域中解决实际问题。最后，“动手实践”部分将引导您将理论知识应用于具体的数据分析任务。

通过这三章的学习，您将不仅学会如何使用判别分析，更能深刻理解其为何有效，以及何时应当谨慎使用。让我们从判别分析最核心的统计原理与数学机制开始我们的探索之旅。

## 原理与机制

本章在前一章介绍判别分析基本概念的基础上，深入探讨其核心的统计原理和数学机制。我们将从贝叶斯决策理论的基础出发，推导出[线性判别分析](@entry_id:178689)（[LDA](@entry_id:138982)）和二次判别分析（QDA）的数学形式。此外，我们还将探讨这些模型背后的假设、它们之间的权衡，以及从不同视角（如费舍尔的降维思想）来理解判别分析的方法。

### [概率分类](@entry_id:637254)的基础：贝叶斯定理

判别分析的根本目标是构建一个规则，用于将新的观测值分配给预先定义的类别中的某一个。从概率论的视角来看，最理想的分类规则是将观测值 $\mathbf{x}$ 分配给[后验概率](@entry_id:153467) $P(Y=k|\mathbf{X}=\mathbf{x})$ 最大的那个类别 $k$。这个规则被称为**[贝叶斯最优分类器](@entry_id:164732)** (Bayes optimal classifier)，它能最小化总体的误分类率。

[后验概率](@entry_id:153467)本身不易直接建模，但我们可以借助**[贝叶斯定理](@entry_id:151040)** (Bayes' Theorem) 将其与更易于处理的量联系起来：

$$
P(Y=k|\mathbf{X}=\mathbf{x}) = \frac{P(\mathbf{X}=\mathbf{x}|Y=k) P(Y=k)}{P(\mathbf{X}=\mathbf{x})}
$$

在这个表达式中：
- $P(Y=k|\mathbf{X}=\mathbf{x})$ 是类别 $k$ 的**[后验概率](@entry_id:153467)** (posterior probability)，即在已知观测值为 $\mathbf{x}$ 的条件下，该观测属于类别 $k$ 的概率。
- $P(Y=k)$ 是类别 $k$ 的**[先验概率](@entry_id:275634)** (prior probability)，记作 $\pi_k$。它代表在进行任何观测之前，一个随机抽取的样本属于类别 $k$ 的概率。
- $P(\mathbf{X}=\mathbf{x}|Y=k)$ 是类别 $k$ 的**类[条件概率密度函数](@entry_id:190422)** (class-conditional probability density function)，记作 $f_k(\mathbf{x})$。它描述了在已知样本属于类别 $k$ 的前提下，观测值 $\mathbf{x}$ 的[概率分布](@entry_id:146404)。
- $P(\mathbf{X}=\mathbf{x})$ 是证据项 (evidence)，其计算方式为 $\sum_{j} P(\mathbf{X}=\mathbf{x}|Y=j) P(Y=j) = \sum_{j} f_j(\mathbf{x}) \pi_j$。

在[分类任务](@entry_id:635433)中，我们的目标是比较不同类别 $k$ 的[后验概率](@entry_id:153467)。由于分母 $P(\mathbf{X}=\mathbf{x})$ 对于所有类别都是相同的，因此最大化后验概率等价于最大化分子，即 $\pi_k f_k(\mathbf{x})$。

因此，贝叶斯最优分类规则可以简化为：将观测值 $\mathbf{x}$ 分配给使 $\pi_k f_k(\mathbf{x})$ 最大的类别 $k$。

为了具体说明这一点，我们来看一个假设性场景 [@problem_id:1914062]。假设我们需要将一个一维观测值 $x_0 = 1$ 分到两个类别中的一个。类别 1 的[先验概率](@entry_id:275634)为 $\pi_1 = 1/4$，其类条件密度 $f_1(x)$ 是一个均值为 $-1$、[方差](@entry_id:200758)为 $1$ 的正态分布。类别 2 的先验概率为 $\pi_2 = 3/4$，其类条件密度 $f_2(x)$ 是一个[位置参数](@entry_id:176482)为 $1$、[尺度参数](@entry_id:268705)为 $1$ 的[拉普拉斯分布](@entry_id:266437)。

对于 $x_0=1$：
- 类别 1 的密度为 $f_1(1) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(1-(-1))^2}{2}\right) = \frac{1}{\sqrt{2\pi}} \exp(-2)$。
- 类别 2 的密度为 $f_2(1) = \frac{1}{2} \exp(-|1-1|) = \frac{1}{2}$。

现在我们比较加权后的密度值：
- $\pi_1 f_1(1) = \frac{1}{4} \cdot \frac{\exp(-2)}{\sqrt{2\pi}} = \frac{\exp(-2)}{4\sqrt{2\pi}}$
- $\pi_2 f_2(1) = \frac{3}{4} \cdot \frac{1}{2} = \frac{3}{8}$

由于 $3/8$ 明显大于一个很小的数 $\frac{\exp(-2)}{4\sqrt{2\pi}}$，我们得出 $\pi_2 f_2(1) > \pi_1 f_1(1)$。因此，贝叶斯最优规则会将观测值 $x_0=1$ 分类到类别 2。这个例子清晰地展示了分类决策是如何同时依赖于先验知识（$\pi_k$）和数据在各个类别中的[似然](@entry_id:167119)（$f_k(\mathbf{x})$）的。

### [线性判别分析](@entry_id:178689) (LDA)

判别分析的核心思想是对类条件密度函数 $f_k(\mathbf{x})$ 的形式做出特定假设，从而得到具体可操作的分类规则。

#### [LDA](@entry_id:138982)的[生成模型](@entry_id:177561)

[线性判别分析](@entry_id:178689)（[LDA](@entry_id:138982)）是建立在一系列强假设之上的**生成模型** (generative model)。它之所以被称为“生成模型”，是因为它对数据的生成过程进行了建模：首先根据[先验概率](@entry_id:275634) $\pi_k$ "生成"一个类别，然后根据该类别的类条件分布 $f_k(\mathbf{x})$ "生成"一个观测值 $\mathbf{x}$ [@problem_id:1914108]。具体来说，[LDA](@entry_id:138982)的核心假设是：

1.  **高斯分布假设**：每个类别的类条件密度函数 $f_k(\mathbf{x})$ 都服从多元高斯（正态）[分布](@entry_id:182848)。
2.  **共同协[方差](@entry_id:200758)假设**：所有类别的高斯分布具有**相同的[协方差矩阵](@entry_id:139155)** $\Sigma$，但可以有不同的[均值向量](@entry_id:266544) $\mu_k$。

因此，在[LDA](@entry_id:138982)模型中，$f_k(\mathbf{x})$ 的形式为：
$$
f_k(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\mu_k)^T \Sigma^{-1} (\mathbf{x}-\mu_k)\right)
$$
其中 $p$ 是[特征向量](@entry_id:151813) $\mathbf{x}$ 的维度。

这种生成式的方法与**[判别模型](@entry_id:635697)** (discriminative model) 如逻辑回归形成鲜明对比。逻辑回归不关心特征 $\mathbf{X}$ 的[分布](@entry_id:182848) $f_k(\mathbf{x})$，而是直接对[后验概率](@entry_id:153467) $P(Y=k|\mathbf{X}=\mathbf{x})$ 的形式（例如，logistic函数）进行建模 [@problem_id:1914082]。[LDA](@entry_id:138982)的假设更强，但也因此在样本量较小时，如果假设成立或接近成立，模型会更稳定。

#### 线性[判别函数](@entry_id:637860)的推导

基于[LDA](@entry_id:138982)的假设，我们可以推导出具体的[判别函数](@entry_id:637860)。为了方便计算，我们通常对贝叶斯规则的判别量 $\pi_k f_k(\mathbf{x})$ 取对数，因为对数函数是单调递增的，最大化 $\pi_k f_k(\mathbf{x})$ 等价于最大化 $\ln(\pi_k f_k(\mathbf{x}))$。

$$
\ln(\pi_k f_k(\mathbf{x})) = \ln(\pi_k) + \ln(f_k(\mathbf{x}))
$$
代入[高斯密度函数](@entry_id:199706)：
$$
= \ln(\pi_k) - \frac{p}{2}\ln(2\pi) - \frac{1}{2}\ln|\Sigma| - \frac{1}{2}(\mathbf{x}-\mu_k)^T \Sigma^{-1} (\mathbf{x}-\mu_k)
$$
为了简化，我们通常会丢弃所有与类别 $k$ 无关的常数项（在这里是 $-\frac{p}{2}\ln(2\pi)$ 和 $-\frac{1}{2}\ln|\Sigma|$，因为协方差矩阵 $\Sigma$ 是共有的）。剩下的部分就构成了[LDA](@entry_id:138982)的**[判别函数](@entry_id:637860)** $\delta_k(\mathbf{x})$：
$$
\delta_k(\mathbf{x}) = \ln(\pi_k) - \frac{1}{2}(\mathbf{x}-\mu_k)^T \Sigma^{-1} (\mathbf{x}-\mu_k)
$$
展开二次型项 $(\mathbf{x}-\mu_k)^T \Sigma^{-1} (\mathbf{x}-\mu_k) = \mathbf{x}^T\Sigma^{-1}\mathbf{x} - 2\mathbf{x}^T\Sigma^{-1}\mu_k + \mu_k^T\Sigma^{-1}\mu_k$，我们可以将 $\delta_k(\mathbf{x})$ 写为：
$$
\delta_k(\mathbf{x}) = \ln(\pi_k) - \frac{1}{2}\mathbf{x}^T\Sigma^{-1}\mathbf{x} + \mathbf{x}^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k
$$
在比较不同类别 $k$ 的[判别函数](@entry_id:637860) $\delta_k(\mathbf{x})$ 时，二次项 $-\frac{1}{2}\mathbf{x}^T\Sigma^{-1}\mathbf{x}$ 再次成为一个公共项，可以被忽略。最终，我们得到了**线性[判别函数](@entry_id:637860)** (Linear Discriminant Function, LDF) 的最终形式：
$$
\delta_k(\mathbf{x}) = \mathbf{x}^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1} \mu_k + \ln(\pi_k)
$$
这个函数是 $\mathbf{x}$ 的线性函数，这也是“线性”判别分析名称的由来。分类规则是：将 $\mathbf{x}$ 分配给使得 $\delta_k(\mathbf{x})$ 值最大的类别 $k$。

例如，在一个二维[分类问题](@entry_id:637153)中，假设两个类别共享一个对角的[协方差矩阵](@entry_id:139155) $\Sigma = \begin{pmatrix} \sigma_1^2 & 0 \\ 0 & \sigma_2^2 \end{pmatrix}$，并且[先验概率](@entry_id:275634)相等。分类的[决策边界](@entry_id:146073)由 $\delta_1(\mathbf{x}) = \delta_2(\mathbf{x})$ 给出，这等价于一个[判别函数](@entry_id:637860) $L(\mathbf{x}) = \delta_1(\mathbf{x}) - \delta_2(\mathbf{x})$ 的符号。经过展开和化简，这个函数可以表示为 [@problem_id:1914045]：
$$
L(\mathbf{x}) = \frac{\mu_{11}-\mu_{21}}{\sigma_{1}^{2}}x_{1}+\frac{\mu_{12}-\mu_{22}}{\sigma_{2}^{2}}x_{2}-\frac{1}{2}\left(\frac{\mu_{11}^{2}-\mu_{21}^{2}}{\sigma_{1}^{2}}+\frac{\mu_{12}^{2}-\mu_{22}^{2}}{\sigma_{2}^{2}}\right)
$$
这清楚地表明，[判别函数](@entry_id:637860)是特征 $x_1$ 和 $x_2$ 的[线性组合](@entry_id:154743)，决策边界 $L(\mathbf{x})=0$ 是一条直线。

#### [决策边界](@entry_id:146073)与几何解释

LDA的决策边界是满足 $\delta_k(\mathbf{x}) = \delta_j(\mathbf{x})$ 的点集。由于 $\delta_k(\mathbf{x})$ 是 $\mathbf{x}$ 的线性函数，这个边界是一个[超平面](@entry_id:268044)。

考虑一个简单的一维双类别问题 [@problem_id:1914058]。假设类别1和类别2的观测值服从均值分别为 $\mu_1, \mu_2$、共同[方差](@entry_id:200758)为 $\sigma^2$ 的[正态分布](@entry_id:154414)，先验概率为 $\pi_1, \pi_2$。决策边界是一个阈值点 $c$，使得 $\pi_1 f_1(c) = \pi_2 f_2(c)$。通过代入正[态密度](@entry_id:147894)函数并取对数求解，我们得到这个阈值为：
$$
c = \frac{\mu_1+\mu_2}{2} + \frac{\sigma^2}{\mu_2-\mu_1}\ln\left(\frac{\pi_1}{\pi_2}\right)
$$
这个公式非常直观：决策边界始于两个均值的中点 $\frac{\mu_1+\mu_2}{2}$，然后根据[方差](@entry_id:200758)和先验概率的比率进行调整。如果先验概率相等 ($\pi_1=\pi_2$)，则 $\ln(1)=0$，边界就是中点。如果某个类别（比如类别1）的先验概率更高，$\ln(\pi_1/\pi_2) > 0$，边界就会向另一个类别（类别2）的均值方向移动，从而使得将新观测划分为类别1的区域更大。

[LDA](@entry_id:138982)还有一个优美的几何解释，这与**[马氏距离](@entry_id:269828)** (Mahalanobis distance) 有关。点 $\mathbf{x}$ 到均值 $\mu_k$ 的[马氏距离](@entry_id:269828)平方定义为：
$$
D_M^2(\mathbf{x}, \mu_k) = (\mathbf{x}-\mu_k)^T \Sigma^{-1} (\mathbf{x}-\mu_k)
$$
与欧氏距离不同，[马氏距离](@entry_id:269828)考虑了[数据协方差](@entry_id:748192)结构，它是在一个“拉伸”和“旋转”过的空间中测量的距离，使得[协方差矩阵](@entry_id:139155)变为单位矩阵。

可以证明，[LDA](@entry_id:138982)的[判别函数](@entry_id:637860) $\delta_k(\mathbf{x})$ 与[马氏距离](@entry_id:269828)有直接关系。将 $\delta_k(\mathbf{x})$ 重新整理，可以得到：
$$
\delta_k(\mathbf{x}) = -\frac{1}{2} D_M^2(\mathbf{x}, \mu_k) + \ln(\pi_k) + \text{常数}
$$
其中常数项与 $\mathbf{x}$ 无关。这个关系揭示了一个重要的事实 [@problem_id:1914107]：
- 如果所有类别的**先验概率相等** ($\pi_k$ 都相同)，那么最大化 $\delta_k(\mathbf{x})$ 就等价于**最小化[马氏距离](@entry_id:269828)** $D_M^2(\mathbf{x}, \mu_k)$。
- 换句话说，在这种情况下，[LDA](@entry_id:138982)分类器就是将一个新观测点分配给距离其类别中心（均值）最近的类别，这里的“距离”是由[马氏距离](@entry_id:269828)定义的。

### 二次判别分析 (QDA)

LDA的“共同协[方差](@entry_id:200758)”假设在实践中可能过于严格。当不同类别的数据不仅均值不同，其[分布](@entry_id:182848)的形状和方向（即协[方差](@entry_id:200758)）也不同时，LDA的性能可能会下降。

#### 放宽共同协[方差](@entry_id:200758)假设

二次判别分析（QDA）放宽了这一假设。QDA仍然假设每个类别的类条件密度是高斯分布，但它允许**每个类别 $k$ 拥有其自身的[协方差矩阵](@entry_id:139155) $\Sigma_k$**。

这个看似微小的改动，却对模型的性质产生了深远的影响。

#### 二次[判别函数](@entry_id:637860)

我们再次从对数化的贝叶斯规则出发，但这次使用类特定的协方差矩阵 $\Sigma_k$：
$$
\ln(\pi_k f_k(\mathbf{x})) = \ln(\pi_k) - \frac{p}{2}\ln(2\pi) - \frac{1}{2}\ln|\Sigma_k| - \frac{1}{2}(\mathbf{x}-\mu_k)^T \Sigma_k^{-1} (\mathbf{x}-\mu_k)
$$
在比较不同类别的[判别函数](@entry_id:637860)时，项 $-\frac{1}{2}\ln|\Sigma_k|$ 不再是公共常数，因为它依赖于 $k$。更重要的是，当我们展开二次型项 $(\mathbf{x}-\mu_k)^T \Sigma_k^{-1} (\mathbf{x}-\mu_k)$ 时，其中的 $\mathbf{x}^T \Sigma_k^{-1} \mathbf{x}$ 项因为 $\Sigma_k$ 随 $k$ 变化而无法被消去。

因此，QDA的[判别函数](@entry_id:637860)（在舍弃公共常数 $-\frac{p}{2}\ln(2\pi)$ 后）为 [@problem_id:1914078]：
$$
\delta_k(\mathbf{x}) = -\frac{1}{2}\ln|\Sigma_k| - \frac{1}{2}(\mathbf{x}-\mu_k)^T \Sigma_k^{-1} (\mathbf{x}-\mu_k) + \ln(\pi_k)
$$
展开后，$\delta_k(\mathbf{x})$ 包含 $\mathbf{x}$ 的常数项、线性项和二次项（具体为 $-\frac{1}{2}\mathbf{x}^T \Sigma_k^{-1} \mathbf{x}$）。正是这个二次项的存在，使得该函数成为 $\mathbf{x}$ 的**二次函数**，这也是模型名称的由来。

由于[判别函数](@entry_id:637860)是二次的，QDA的[决策边界](@entry_id:146073) $\delta_k(\mathbf{x}) = \delta_j(\mathbf{x})$ 是由[二次方程](@entry_id:163234)定义的，其形状是[二次曲面](@entry_id:264390)，在二维空间中通常是圆、椭圆、抛物线或双曲线。

### LDA 与 QDA 的比较：偏见-[方差](@entry_id:200758)权衡

选择[LDA](@entry_id:138982)还是QDA，本质上是一个**偏见-[方差](@entry_id:200758)权衡** (bias-variance trade-off) 的问题。

- **LDA**：模型更简单，假设更强（所有类别共享一个协方差矩阵）。
  - **偏见 (Bias)**：如果真实情况是各类别协[方差](@entry_id:200758)不同，LDA的线性边界假设就是不正确的，这会引入较高的偏见。模型无法捕捉到真实的、更复杂的[决策边界](@entry_id:146073)。
  - **[方差](@entry_id:200758) (Variance)**：由于模型需要估计的参数较少（一个共同的[协方差矩阵](@entry_id:139155)和 $K$ 个均值），其[参数估计](@entry_id:139349)值的[方差](@entry_id:200758)较低。这意味着模型对训练数据的微小变化不那么敏感，更不容易[过拟合](@entry_id:139093)。

- **QDA**：模型更复杂，假设更弱（每个类别有自己的[协方差矩阵](@entry_id:139155)）。
  - **偏见**：模型更灵活，能够拟合二次边界，因此偏见较低。如果真实的[决策边界](@entry_id:146073)确实是二次的，QDA能够很好地捕捉到它。
  - **[方差](@entry_id:200758)**：模型需要估计更多的参数（$K$ 个[协方差矩阵](@entry_id:139155)和 $K$ 个均值）。协方差矩阵的参数数量随特征维度 $p$ 平方增长（具体为 $p(p+1)/2$）。这导致参数估计的[方差](@entry_id:200758)很高，尤其是在样本量 $n$ 不足时，模型容易过拟合训练数据。

我们可以通过一个天文学分类的例子来直观理解这一点 [@problem_id:1914063]。假设脉冲星（类别1）和[类星体](@entry_id:159221)（类别2）的特征[分布](@entry_id:182848)有不同的[协方差矩阵](@entry_id:139155)：$\Sigma_1 = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}$（在$x_1$方向上更分散），而 $\Sigma_2 = \begin{pmatrix} 1 & 0 \\ 0 & 9 \end{pmatrix}$（在$x_2$方向上更分散）。[LDA](@entry_id:138982)被迫计算一个共同的协方差矩阵，从而忽略了这种结构上的差异，最终可能产生一个简单的、次优的线性[决策边界](@entry_id:146073)。相比之下，QDA会利用这些不同的协[方差](@entry_id:200758)信息，生成一个更贴[合数](@entry_id:263553)据真实形态的二次边界，从而获得更好的分类效果。

那么在实践中如何选择？ [@problem_id:1914081]
- 当**训练样本量较小**时，或者特征维度 $p$ 很大时，为了避免高[方差](@entry_id:200758)带来的过拟合，**[LDA](@entry_id:138982)**通常是更安全的选择。
- 当**训练样本量非常大**时，QDA的高[方差](@entry_id:200758)问题会得到缓解（因为参数估计会更精确）。此时，如果数据表明各类别协[方差](@entry_id:200758)确实不同，那么**QDA**的低偏见优势将凸显出来，通常会带来更好的性能。
- 如果你有理由相信所有类别的协[方差](@entry_id:200758)是近似相等的，那么LDA是更合理的选择。

### 降维视角：费舍尔判别分析

除了作为一种[概率分类](@entry_id:637254)器，[LDA](@entry_id:138982)还可以从另一个完全不同的角度来理解——**[降维](@entry_id:142982)** (dimensionality reduction)。这个观点由统计学巨匠 Ronald Fisher 提出，其历史甚至早于概率解释。

在多类别（尤其是在两类）问题中，Fisher的目标是找到一个能将 $p$ 维数据投影到一维直线上（$y = \mathbf{w}^T \mathbf{x}$）的方向向量 $\mathbf{w}$，使得投影后不同类别的数据点尽可能地分开。

那么，如何衡量“尽可能分开”呢？Fisher提出了一个非常巧妙的准则 [@problem_id:1914092]：最大化**类间散度** (between-class scatter) 与**类内散度** (within-class scatter) 的比率。

- **类间散度**：衡量的是投影后，不同类别均值之间的距离。对于两类问题，这可以表示为 $(m_2 - m_1)^2$，其中 $m_k = \mathbf{w}^T \mu_k$ 是类别 $k$ 在投影方向上的均值。
- **类内散度**：衡量的是投影后，每个类别内部数据的离散程度。它可以表示为 $S_W = \sum_{n \in C_1} (y_n - m_1)^2 + \sum_{n \in C_2} (y_n - m_2)^2$。

Fisher的判别准则 $J(\mathbf{w})$ 就是最大化这两者的比率：
$$
J(\mathbf{w}) = \frac{(m_2 - m_1)^2}{S_W} = \frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_W \mathbf{w}}
$$
其中 $S_B = (\mu_2-\mu_1)(\mu_2-\mu_1)^T$ 是原始空间中的类间散布矩阵，$S_W$ 是原始空间中的类内散布矩阵。

这个准则的直观意义是：我们寻找一个投影方向，它既能使不同类别的中心在投影后相距甚远，又能使每个类别自身在投影后保持紧凑。仅仅最大化类间距离或仅仅最小化类内[方差](@entry_id:200758)都是不够的，它们的比率才是关键。

神奇的是，可以证明，使该准则最大化的最优方向 $\mathbf{w}$ 正比于 $\Sigma^{-1}(\mu_2 - \mu_1)$，这恰好与我们从[概率模型](@entry_id:265150)推导出的[LDA](@entry_id:138982)决策边界[法向量](@entry_id:264185)相吻合。这两种看似不同的观点——概率[生成模型](@entry_id:177561)和最佳投影降维——最终殊途同归，共同指向了同一个线性判别方法。

### 判别分析的局限性

尽管判别分析功能强大，但它的性能高度依赖其核心假设，即类条件密度为[高斯分布](@entry_id:154414)。当这个假设被严重违反时，尤其是当决策边界本质上不是线性或二次时，判别分析可能会完全失效。

一个经典的例子是同心圆问题 [@problem_id:1914040]。假设类别1的数据[均匀分布](@entry_id:194597)在一个圆盘内，而类别2的数据[均匀分布](@entry_id:194597)在环绕它的一个[圆环](@entry_id:163678)内。这是一个[非线性](@entry_id:637147)可分的问题。

如果我们尝试应用LDA，会发生什么？由于两个类别的[分布](@entry_id:182848)都是关于[原点对称](@entry_id:172995)的，它们的**[质心](@entry_id:265015)（[均值向量](@entry_id:266544)）将重合于原点**，即 $\mu_1 = \mu_2 = (0,0)$。
当我们计算LDA的决策规则时，它依赖于均值之差 $\mu_1 - \mu_2$。由于这个差值为[零向量](@entry_id:156189)，[LDA](@entry_id:138982)的[判别函数](@entry_id:637860) $\delta_k(\mathbf{x})$ 将不再依赖于 $\mathbf{x}$，而只取决于先验概率。这意味着分类器要么将所有点都分给一个类（如果先验不同），要么完全无法区分（如果先验相同），其表现等同于随机猜测。

这个例子生动地说明了[LDA](@entry_id:138982)的根本局限性：它是一种**[线性分类器](@entry_id:637554)**，本质上是在寻找一个超平面来分割数据。对于那些无法通过直[线或](@entry_id:170208)平面有效分离的类别，LDA将[无能](@entry_id:201612)为力。同样，QDA虽然更灵活，但它也仅限于二次边界，对于更复杂的[非线性](@entry_id:637147)结构，它也可能不是最佳选择。理解这些局限性对于在实践中选择合适的分类工具至关重要。