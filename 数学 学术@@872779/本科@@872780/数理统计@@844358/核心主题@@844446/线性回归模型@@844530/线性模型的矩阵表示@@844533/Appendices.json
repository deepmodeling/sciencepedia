{"hands_on_practices": [{"introduction": "从数据到模型的第一步是构建正规方程，这是求解线性模型参数的核心。本练习将指导您完成这一基本步骤，通过一个简单线性回归的例子，将观测数据转化为设计矩阵 $X$，并计算出关键矩阵 $X^T X$。这个过程不仅是机械的计算，更是理解模型如何将原始数据结构化的关键。[@problem_id:1933331]", "problem": "考虑一个用于分析预测变量 $x$ 和响应变量 $y$ 之间关系的简单线性回归模型。该模型由 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ 给出，其中 $i=1, 2, 3, 4$。进行了一项实验，得到了预测变量的四个观测值：$x_1 = 1$，$x_2 = 2$，$x_3 = 3$ 和 $x_4 = 4$。\n\n该模型可以写成矩阵形式 $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$，其中 $X$ 是设计矩阵。\n\n求矩阵乘积 $X^T X$。将答案表示为一个 $1 \\times 4$ 的行矩阵，按行主序（即，第一行的元素后跟第二行的元素）列出这个 $2 \\times 2$ 乘积矩阵的元素。", "solution": "带截距项的简单线性回归的设计矩阵由一列1和一列预测变量值组成。因此\n$$\nX=\\begin{pmatrix}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n1 & x_{3}\\\\\n1 & x_{4}\n\\end{pmatrix}.\n$$\n根据给定的值 $x_{1}=1$，$x_{2}=2$，$x_{3}=3$，$x_{4}=4$，该矩阵变为\n$$\nX=\\begin{pmatrix}\n1 & 1\\\\\n1 & 2\\\\\n1 & 3\\\\\n1 & 4\n\\end{pmatrix}.\n$$\n根据矩阵乘法的定义，$X^{T}X$ 的元素为\n$$\n(X^{T}X)_{11}=\\sum_{i=1}^{4}1\\cdot 1=\\sum_{i=1}^{4}1,\\quad\n(X^{T}X)_{12}=\\sum_{i=1}^{4}1\\cdot x_{i}=\\sum_{i=1}^{4}x_{i},\n$$\n$$\n(X^{T}X)_{21}=\\sum_{i=1}^{4}x_{i}\\cdot 1=\\sum_{i=1}^{4}x_{i},\\quad\n(X^{T}X)_{22}=\\sum_{i=1}^{4}x_{i}\\cdot x_{i}=\\sum_{i=1}^{4}x_{i}^{2}.\n$$\n计算这些和：\n$$\n\\sum_{i=1}^{4}1=4,\\quad \\sum_{i=1}^{4}x_{i}=1+2+3+4=10,\\quad \\sum_{i=1}^{4}x_{i}^{2}=1^{2}+2^{2}+3^{2}+4^{2}=30.\n$$\n因此\n$$\nX^{T}X=\\begin{pmatrix}\n4 & 10\\\\\n10 & 30\n\\end{pmatrix},\n$$\n按行主序将其表示为一个 $1\\times 4$ 的行矩阵，结果是\n$$\n\\begin{pmatrix}\n4 & 10 & 10 & 30\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 4 & 10 & 10 & 30 \\end{pmatrix}}$$", "id": "1933331"}, {"introduction": "在构建了正规方程之后，我们可以探索其蕴含的深刻性质。本练习将引导您从代数上证明一个普通最小二乘法 (OLS) 的基本性质：对于包含截距项的模型，残差之和恒为零。这个性质 $(\\mathbf{1}^T \\mathbf{e} = 0)$ 并非巧合，而是 OLS 最小化过程的直接结果，体现了拟合值与观测值之间的内在平衡。[@problem_id:1933375]", "problem": "在研究简单线性回归模型时，我们使用矩阵表示法来表达响应向量 $\\mathbf{y}$ 和预测向量 $\\mathbf{x}$ 之间的关系。考虑一个包含 $n$ 个观测值的数据集。模型由 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ 给出，其中：\n- $\\mathbf{y}$ 是观测响应的 $n \\times 1$ 向量。\n- $\\mathbf{X}$ 是 $n \\times 2$ 的设计矩阵。对于带截距项的简单线性回归模型，$\\mathbf{X}$ 被构造为 $[\\mathbf{1} \\quad \\mathbf{x}]$，其中 $\\mathbf{1}$ 是一个元素全为1的 $n \\times 1$ 列向量，$\\mathbf{x}$ 是预测变量观测值的 $n \\times 1$ 列向量。\n- $\\boldsymbol{\\beta} = [\\beta_0 \\quad \\beta_1]^T$ 是模型系数（截距和斜率）的 $2 \\times 1$ 向量。\n- $\\boldsymbol{\\epsilon}$ 是随机误差的 $n \\times 1$ 向量。\n\n系数使用普通最小二乘法 (OLS) 进行估计，得到估计量 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$。拟合值向量为 $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$，残差向量定义为 $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}$。\n\n根据所提供的定义，确定标量 $\\mathbf{1}^T \\mathbf{e}$ 的值。\n\nA. $0$\n\nB. $n$\n\nC. $\\mathbf{1}^T\\mathbf{y}$\n\nD. $\\det(\\mathbf{X}^T\\mathbf{X})$", "solution": "问题要求解标量积 $\\mathbf{1}^T \\mathbf{e}$ 的值。我们从普通最小二乘法 (OLS) 估计量 $\\hat{\\boldsymbol{\\beta}}$ 的定义开始。OLS 估计量是通过最小化残差平方和 $S(\\boldsymbol{\\beta}) = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ 推导出来的。\n\n为了找到最小值，我们将 $S(\\boldsymbol{\\beta})$ 对向量 $\\boldsymbol{\\beta}$ 求导，并令其等于零。\n$$S(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}$$\n由于 $\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}$ 是一个标量，它等于其转置，即 $(\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y})^T = \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta}$。因此，我们可以写成：\n$$S(\\boldsymbol{\\beta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}$$\n关于 $\\boldsymbol{\\beta}$ 的导数是：\n$$\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}$$\n将导数设为零以求得估计量 $\\hat{\\boldsymbol{\\beta}}$，可得：\n$$-2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}$$\n$$\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$$\n这组方程被称为正规方程组。我们可以将正规方程组重新整理为：\n$$\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}$$\n提取公因子 $\\mathbf{X}^T$ 可得：\n$$\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$$\n根据定义，残差向量为 $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}}$。由于拟合值向量为 $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$，我们可以将残差向量写为 $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$。\n将此代入重新整理后的正规方程组，我们得到：\n$$\\mathbf{X}^T\\mathbf{e} = \\mathbf{0}$$\n问题指明这是一个带截距项的简单线性回归。设计矩阵 $\\mathbf{X}$ 由 $\\mathbf{X} = [\\mathbf{1} \\quad \\mathbf{x}]$ 给出，其中 $\\mathbf{1}$ 是一个元素全为1的 $n \\times 1$ 向量，$\\mathbf{x}$ 是预测变量值的 $n \\times 1$ 向量。\n\n设计矩阵的转置是 $\\mathbf{X}^T = \\begin{pmatrix} \\mathbf{1}^T \\\\ \\mathbf{x}^T \\end{pmatrix}$。\n现在，我们可以明确地写出方程 $\\mathbf{X}^T\\mathbf{e} = \\mathbf{0}$：\n$$\\begin{pmatrix} \\mathbf{1}^T \\\\ \\mathbf{x}^T \\end{pmatrix} \\mathbf{e} = \\begin{pmatrix} \\mathbf{1}^T\\mathbf{e} \\\\ \\mathbf{x}^T\\mathbf{e} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n该矩阵方程代表了一个包含两个独立方程的方程组：\n1. $\\mathbf{1}^T\\mathbf{e} = 0$\n2. $\\mathbf{x}^T\\mathbf{e} = 0$\n\n我们被要求解的量是 $\\mathbf{1}^T\\mathbf{e}$。从方程组的第一个方程中，我们看到 $\\mathbf{1}^T\\mathbf{e} = 0$。\n表达式 $\\mathbf{1}^T\\mathbf{e}$ 是一个全1向量与残差向量的点积，这等价于残差向量各元素之和：$\\mathbf{1}^T\\mathbf{e} = \\sum_{i=1}^n e_i$。因此，我们证明了在带截距项的简单线性回归中，残差之和为零。\n\n将我们的结果与给定选项进行比较：\nA. $0$\nB. $n$\nC. $\\mathbf{1}^T\\mathbf{y}$\nD. $\\det(\\mathbf{X}^T\\mathbf{X})$\n\n我们推导出的值为 $0$。因此，选项A是正确答案。", "answer": "$$\\boxed{A}$$", "id": "1933375"}, {"introduction": "虽然正规方程 $\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$ 提供了参数估计的理论解，但直接计算矩阵的逆 $(\\mathbf{X}^T\\mathbf{X})^{-1}$ 在数值上可能不稳定，尤其是在处理复杂数据集时。本练习介绍了一种更稳健、更高效的计算方法：QR 分解。通过将设计矩阵 $X$ 分解为正交矩阵 $Q$ 和上三角矩阵 $R$ 的乘积，我们可以将求解过程转化为一个简单的回代问题，从而优雅地绕过矩阵求逆的难题。[@problem_id:1933337]", "problem": "一位材料科学家正在研究一种新开发的热电发电机性能。其电功率输出 $y$ 被建模为两个操作参数的线性函数：设备两端的温度梯度 $x_1$ 和用于其结构的新复合材料的热导率 $x_2$。\n\n该关系由多元线性回归模型描述：\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\n其中 $\\beta_0, \\beta_1, \\beta_2$ 是待估计的模型系数，而 $\\epsilon$ 是随机误差项。\n\n进行了四次实验，得到了响应向量 $y$ 和设计矩阵 $X$ 的以下数据：\n$$\ny = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\\\ 6 \\end{pmatrix}, \\quad\nX = \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 2 & 3 \\\\ 1 & 3 & 5 \\\\ 1 & 4 & 6 \\end{pmatrix}\n$$\n为了数值稳定性和效率，设计矩阵 $X$ 已被分解为乘积 $X=QR$，其中 $Q$ 是一个具有标准正交列的矩阵，$R$ 是一个上三角矩阵。这些矩阵如下所示：\n$$\nQ = \\begin{pmatrix}\n\\frac{1}{2} & -\\frac{3}{2\\sqrt{5}} & \\frac{1}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & -\\frac{1}{2\\sqrt{5}} & -\\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{1}{2\\sqrt{5}} & \\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{3}{2\\sqrt{5}} & -\\frac{1}{2\\sqrt{5}}\n\\end{pmatrix}, \\quad\nR = \\begin{pmatrix}\n2 & 5 & 8 \\\\\n0 & \\sqrt{5} & \\frac{7}{\\sqrt{5}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{5}}\n\\end{pmatrix}\n$$\n\n确定系数向量 $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)^T$ 的普通最小二乘 (OLS) 估计。将向量 $\\hat{\\beta}$ 的每个分量表示为精确的小数。", "solution": "我们使用 QR 分解法来求解普通最小二乘问题。当 $X=QR$ 时，其中 $Q$ 具有标准正交列（$Q^{T}Q=I$），$R$ 是上三角矩阵，OLS 估计 $\\hat{\\beta}$ 求解\n$$\n\\min_{\\beta}\\|y-X\\beta\\|_{2}=\\min_{\\beta}\\|y-QR\\beta\\|_{2}.\n$$\n左乘 $Q^{T}$ 并使用 $Q^{T}Q=I$ 得到三角系统\n$$\nR\\hat{\\beta}=Q^{T}y.\n$$\n因此，计算 $z=Q^{T}y$，然后通过回代法求解 $R\\hat{\\beta}=z$。\n\n给定\n$$\nQ=\\begin{pmatrix}\n\\frac{1}{2} & -\\frac{3}{2\\sqrt{5}} & \\frac{1}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & -\\frac{1}{2\\sqrt{5}} & -\\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{1}{2\\sqrt{5}} & \\frac{3}{2\\sqrt{5}} \\\\\n\\frac{1}{2} & \\frac{3}{2\\sqrt{5}} & -\\frac{1}{2\\sqrt{5}}\n\\end{pmatrix},\\quad\ny=\\begin{pmatrix}1\\\\1\\\\4\\\\6\\end{pmatrix},\n$$\n$z=Q^{T}y$ 的分量是\n$$\nz_{1}=q_{1}^{T}y=\\frac{1}{2}(1+1+4+6)=6,\n$$\n$$\nz_{2}=q_{2}^{T}y=\\frac{1}{2\\sqrt{5}}\\big((-3)\\cdot 1+(-1)\\cdot 1+1\\cdot 4+3\\cdot 6\\big)=\\frac{18}{2\\sqrt{5}}=\\frac{9}{\\sqrt{5}},\n$$\n$$\nz_{3}=q_{3}^{T}y=\\frac{1}{2\\sqrt{5}}\\big(1\\cdot 1+(-3)\\cdot 1+3\\cdot 4+(-1)\\cdot 6\\big)=\\frac{4}{2\\sqrt{5}}=\\frac{2}{\\sqrt{5}}.\n$$\n对于\n$$\nR=\\begin{pmatrix}\n2 & 5 & 8 \\\\\n0 & \\sqrt{5} & \\frac{7}{\\sqrt{5}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{5}}\n\\end{pmatrix},\n$$\n我们求解 $R\\hat{\\beta}=z$：\n- 第三个方程： $\\frac{1}{\\sqrt{5}}\\hat{\\beta}_{2}=\\frac{2}{\\sqrt{5}} \\implies \\hat{\\beta}_{2}=2$。\n- 第二个方程： $\\sqrt{5}\\hat{\\beta}_{1}+\\frac{7}{\\sqrt{5}}\\hat{\\beta}_{2}=\\frac{9}{\\sqrt{5}}$。两边同乘以 $\\sqrt{5}$ 得到 $5\\hat{\\beta}_{1}+7\\hat{\\beta}_{2}=9$。代入 $\\hat{\\beta}_{2}=2$ 得 $5\\hat{\\beta}_{1}+14=9 \\implies \\hat{\\beta}_{1}=-1$。\n- 第一个方程： $2\\hat{\\beta}_{0}+5\\hat{\\beta}_{1}+8\\hat{\\beta}_{2}=6$。代入 $\\hat{\\beta}_{1}=-1$ 和 $\\hat{\\beta}_{2}=2$ 得 $2\\hat{\\beta}_{0}-5+16=6 \\implies 2\\hat{\\beta}_{0}+11=6 \\implies \\hat{\\beta}_{0}=-2.5$。\n\n因此，\n$$\n\\hat{\\beta}=\\begin{pmatrix}-2.5 \\\\ -1 \\\\ 2\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} -2.5 \\\\ -1 \\\\ 2 \\end{pmatrix}}$$", "id": "1933337"}]}