{"hands_on_practices": [{"introduction": "要真正掌握最小二乘法，最好的方法莫过于从第一性原理出发进行推导。这个练习 [@problem_id:1948111] 挑战我们在一个略微非标准但很重要的情景下完成这项任务：即强制回归模型通过原点。通过解决这个问题，我们将巩固对如何构建最小化问题以及推导无偏性和方差等关键特性的理解。", "problem": "一位工程师正在对一种新的电阻材料进行表征。根据物理理论，该材料两端的电压 $V$ 应与通过它的电流 $I$ 成正比，遵循模型 $V = \\beta I$，其中 $\\beta$ 是未知的真实电阻。为了估计 $\\beta$，该工程师进行了一系列共 $n$ 次实验，将电流设置为已知的、非随机的值 $x_1, x_2, \\ldots, x_n$，并测量对应的电压 $Y_1, Y_2, \\ldots, Y_n$。\n\n该关系由强制通过原点的简单线性回归模型描述：\n$$Y_i = \\beta x_i + \\epsilon_i \\quad \\text{for } i=1, \\ldots, n$$\n此处，$Y_i$ 是为设定的电流 $x_i$ 测得的电压，$\\epsilon_i$ 代表随机测量误差。假设误差是独立的，且对于某个未知常数 $\\sigma^2 > 0$，有 $E[\\epsilon_i] = 0$ 和 $\\text{Var}(\\epsilon_i) = \\sigma^2$。还假设施加的电流 $x_i$ 中至少有一个不为零。\n\n该工程师使用最小二乘法来获得电阻的估计量，记为 $\\hat{\\beta}$。考虑以下关于此估计量 $\\hat{\\beta}$ 及其相应残差 $e_i = Y_i - \\hat{\\beta} x_i$ 的性质的陈述。\n\nA. 估计量的公式为 $\\hat{\\beta} = \\frac{1}{n} \\sum_{i=1}^n \\frac{Y_i}{x_i}$。\n\nB. 估计量 $\\hat{\\beta}$ 是 $\\beta$ 的无偏估计量。\n\nC. 估计量的方差由 $\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2 \\sum_{i=1}^n x_i^2}{(\\sum_{i=1}^n x_i)^2}$ 给出。\n\nD. 残差之和 $\\sum_{i=1}^n e_i$ 不保证为零。\n\nE. 估计量的方差由 $\\text{Var}(\\hat{\\beta}) = \\frac{\\sigma^2}{\\sum_{i=1}^n x_i^2}$ 给出。\n\n以上哪些陈述是正确的？", "solution": "我们首先推导通过原点的回归模型中斜率的普通最小二乘估计量。估计量 $\\hat{\\beta}$ 最小化残差平方和\n$$\nS(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta x_{i}\\right)^{2}.\n$$\n对 $\\beta$ 求导并将导数设为零，得到正规方程\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta}=-2\\sum_{i=1}^{n}x_{i}\\left(Y_{i}-\\beta x_{i}\\right)=0,\n$$\n化简为\n$$\n\\sum_{i=1}^{n}x_{i}Y_{i}-\\beta\\sum_{i=1}^{n}x_{i}^{2}=0.\n$$\n假设 $\\sum_{i=1}^{n}x_{i}^{2}>0$（由至少一个 $x_{i}\\neq 0$ 保证），最小二乘估计量为\n$$\n\\hat{\\beta}=\\frac{\\sum_{i=1}^{n}x_{i}Y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\n这直接表明陈述 A，即 $\\hat{\\beta}=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{Y_{i}}{x_{i}}$，在通常情况下是不正确的；此外，如果任何一个 $x_i=0$，该式将无定义，而正确的最小二乘估计量只要不是所有的 $x_i$ 都为零就仍然是良定义的。\n\n接下来，我们证明其无偏性。使用模型 $Y_{i}=\\beta x_{i}+\\epsilon_{i}$，其中 $E[\\epsilon_{i}]=0$，并将 $x_{i}$ 视为固定值，\n$$\nE[\\hat{\\beta}]=\\frac{\\sum_{i=1}^{n}x_{i}E[Y_{i}]}{\\sum_{i=1}^{n}x_{i}^{2}}=\\frac{\\sum_{i=1}^{n}x_{i}(\\beta x_{i})}{\\sum_{i=1}^{n}x_{i}^{2}}=\\beta,\n$$\n所以陈述 B 是正确的。\n\n对于方差，我们有\n$$\n\\hat{\\beta}=\\beta+\\frac{\\sum_{i=1}^{n}x_{i}\\epsilon_{i}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\n因为 $\\epsilon_{i}$ 是独立的且 $\\text{Var}(\\epsilon_{i})=\\sigma^{2}$，\n$$\n\\text{Var}\\!\\left(\\sum_{i=1}^{n}x_{i}\\epsilon_{i}\\right)=\\sum_{i=1}^{n}x_{i}^{2}\\,\\text{Var}(\\epsilon_{i})=\\sigma^{2}\\sum_{i=1}^{n}x_{i}^{2}.\n$$\n因此，\n$$\n\\text{Var}(\\hat{\\beta})=\\frac{\\text{Var}\\!\\left(\\sum_{i=1}^{n}x_{i}\\epsilon_{i}\\right)}{\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)^{2}}=\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}x_{i}^{2}}.\n$$\n因此陈述 E 是正确的，而给出不同表达式的陈述 C 是不正确的。\n\n最后，考虑残差 $e_{i}=Y_{i}-\\hat{\\beta}x_{i}$。正规方程意味着\n$$\n\\sum_{i=1}^{n}x_{i}e_{i}=\\sum_{i=1}^{n}x_{i}(Y_{i}-\\hat{\\beta}x_{i})=0.\n$$\n然而，在通过原点的模型中，没有约束强制 $\\sum_{i=1}^{n}e_{i}=0$。事实上，\n$$\n\\sum_{i=1}^{n}e_{i}=\\sum_{i=1}^{n}Y_{i}-\\hat{\\beta}\\sum_{i=1}^{n}x_{i},\n$$\n它通常不为零。因此，陈述 D 是正确的。\n\n总结结论：B、D 和 E 是正确的；A 和 C 是不正确的。", "answer": "$$\\boxed{BDE}$$", "id": "1948111"}, {"introduction": "在推导出估计量之后，一个实际的问题是，当我们改变变量的单位时，估计量会如何表现。这个练习 [@problem_id:1948136] 探讨了对预测变量进行重新缩放（例如，从米变为千米）所带来的影响，这是数据分析中的常见操作。理解这一性质对于正确解释回归系数及其统计显著性至关重要。", "problem": "考虑一个用于描述响应变量 $Y$ 和预测变量 $x$ 之间关系的简单线性回归模型。对于一组 $n$ 个观测值 $(x_i, Y_i)$，模型由下式给出\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i = 1, 2, \\ldots, n$$\n其中 $\\beta_0$ 和 $\\beta_1$ 分别是截距和斜率参数。误差项 $\\epsilon_i$ 假定为独立同分布的随机变量，其均值为 $E[\\epsilon_i] = 0$，方差为 $\\text{Var}(\\epsilon_i) = \\sigma^2$。原始模型的最小二乘估计量记为 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$。\n\n一位研究人员决定通过乘以一个非零常数 $c$ 来重缩放预测变量。定义一个新的预测变量 $x_i'$ 为 $x_i' = c x_i$。然后使用这个经过缩放的预测变量进行新的回归，模型由下式给出\n$$Y_i = \\beta_0' + \\beta_1' x_i' + \\epsilon_i$$\n这个新模型的最小二乘估计量记为 $\\hat{\\beta}_0'$ 和 $\\hat{\\beta}_1'$。\n\n新的估计量（$\\hat{\\beta}_0'$, $\\hat{\\beta}_1'$）与原始估计量（$\\hat{\\beta}_0$, $\\hat{\\beta}_1$）之间的关系是什么？新斜率估计量的方差 $\\text{Var}(\\hat{\\beta}_1')$ 与原始斜率估计量的方差 $\\text{Var}(\\hat{\\beta}_1)$ 之间的关系又是什么？\n\nA. $\\hat{\\beta}_0' = \\hat{\\beta}_0$, $\\hat{\\beta}_1' = \\frac{\\hat{\\beta}_1}{c}$, 且 $\\text{Var}(\\hat{\\beta}_1') = \\frac{\\text{Var}(\\hat{\\beta}_1)}{c^2}$\n\nB. $\\hat{\\beta}_0' = c\\hat{\\beta}_0$, $\\hat{\\beta}_1' = c\\hat{\\beta}_1$, 且 $\\text{Var}(\\hat{\\beta}_1') = c^2\\text{Var}(\\hat{\\beta}_1)$\n\nC. $\\hat{\\beta}_0' = \\hat{\\beta}_0$, $\\hat{\\beta}_1' = c\\hat{\\beta}_1$, 且 $\\text{Var}(\\hat{\\beta}_1') = c^2\\text{Var}(\\hat{\\beta}_1)$\n\nD. $\\hat{\\beta}_0' = \\frac{\\hat{\\beta}_0}{c}$, $\\hat{\\beta}_1' = \\frac{\\hat{\\beta}_1}{c}$, 且 $\\text{Var}(\\hat{\\beta}_1') = \\frac{\\text{Var}(\\hat{\\beta}_1)}{c^2}$\n\nE. $\\hat{\\beta}_0' = \\hat{\\beta}_0$, $\\hat{\\beta}_1' = \\frac{\\hat{\\beta}_1}{c^2}$, 且 $\\text{Var}(\\hat{\\beta}_1') = \\frac{\\text{Var}(\\hat{\\beta}_1)}{c^4}$", "solution": "我们从原始的简单线性回归模型开始\n$$\nY_i=\\beta_{0}+\\beta_{1}x_i+\\epsilon_i,\\quad i=1,2,\\ldots,n,\n$$\n其中 $E[\\epsilon_i]=0$ 且 $\\text{Var}(\\epsilon_i)=\\sigma^{2}$。普通最小二乘法 (OLS) 的斜率和截距估计量可以写作\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}},\\quad \\hat{\\beta}_{0}=\\bar{Y}-\\hat{\\beta}_{1}\\bar{x},\n$$\n其中\n$$\nS_{xy}=\\sum_{i=1}^{n}(x_i-\\bar{x})(Y_i-\\bar{Y}),\\quad S_{xx}=\\sum_{i=1}^{n}(x_i-\\bar{x})^{2},\n$$\n以及 $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i$，$\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i$。\n\n现在定义重缩放的预测变量 $x_i'=c\\,x_i$（其中 $c\\neq 0$），并拟合模型\n$$\nY_i=\\beta_{0}'+\\beta_{1}'x_i'+\\epsilon_i.\n$$\n对应的样本均值满足 $\\bar{x}'=c\\,\\bar{x}$。对于变换后的和，注意到\n$$\nx_i'-\\bar{x}'=c(x_i-\\bar{x}),\n$$\n所以\n$$\nS_{x'y}=\\sum_{i=1}^{n}(x_i'-\\bar{x}')(Y_i-\\bar{Y})=\\sum_{i=1}^{n}c(x_i-\\bar{x})(Y_i-\\bar{Y})=c\\,S_{xy},\n$$\n以及\n$$\nS_{x'x'}=\\sum_{i=1}^{n}(x_i'-\\bar{x}')^{2}=\\sum_{i=1}^{n}c^{2}(x_i-\\bar{x})^{2}=c^{2}S_{xx}.\n$$\n因此，新的 OLS 斜率为\n$$\n\\hat{\\beta}_{1}'=\\frac{S_{x'y}}{S_{x'x'}}=\\frac{c\\,S_{xy}}{c^{2}S_{xx}}=\\frac{1}{c}\\hat{\\beta}_{1}.\n$$\n新的截距为\n$$\n\\hat{\\beta}_{0}'=\\bar{Y}-\\hat{\\beta}_{1}'\\bar{x}'=\\bar{Y}-\\left(\\frac{1}{c}\\hat{\\beta}_{1}\\right)(c\\,\\bar{x})=\\bar{Y}-\\hat{\\beta}_{1}\\bar{x}=\\hat{\\beta}_{0}.\n$$\n\n对于同方差模型下斜率估计量的方差，标准公式给出\n$$\n\\text{Var}(\\hat{\\beta}_{1})=\\frac{\\sigma^{2}}{S_{xx}},\\quad \\text{以及}\\quad \\text{Var}(\\hat{\\beta}_{1}')=\\frac{\\sigma^{2}}{S_{x'x'}}=\\frac{\\sigma^{2}}{c^{2}S_{xx}}=\\frac{1}{c^{2}}\\text{Var}(\\hat{\\beta}_{1}).\n$$\n等价地，由于 $\\hat{\\beta}_{1}'=(1/c)\\hat{\\beta}_{1}$ 且 $c$ 是确定性的，所以 $\\text{Var}(\\hat{\\beta}_{1}')=(1/c^{2})\\text{Var}(\\hat{\\beta}_{1})$。\n\n因此，这些关系是 $\\hat{\\beta}_{0}'=\\hat{\\beta}_{0}$，$\\hat{\\beta}_{1}'=\\hat{\\beta}_{1}/c$，以及 $\\text{Var}(\\hat{\\beta}_{1}')=\\text{Var}(\\hat{\\beta}_{1})/c^{2}$，这对应于选项 A。", "answer": "$$\\boxed{A}$$", "id": "1948136"}, {"introduction": "从简单回归转向多元回归时，系数的解释变得更加微妙。这个练习 [@problem_id:1948175] 介绍了一个强大的概念，即 Frisch-Waugh-Lovell 定理。该定理为我们提供了关于多元回归系数真实含义的深刻见解：它衡量的是在剔除了其他变量的影响后，两个变量之间的“纯粹”关系。", "problem": "一位数据科学家正在对服务器的中央处理器（CPU）负载进行建模。该模型旨在基于两个预测变量来预测CPU负载（$Y$，以百分比计）：活跃用户会话数（$X_1$）和网络带宽利用率（$X_2$，单位为 Mbps）。多元线性回归模型由下式给出：\n$$\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\epsilon_i\n$$\n其中 $\\epsilon_i$ 是均值为零的独立同分布误差项，下标 $i$ 表示来自 $n$ 个观测值样本中的第 $i$ 个观测值。\n\n该科学家特别感兴趣的是，在考虑了网络带宽（$X_2$）的影响之后，量化活跃用户会话数（$X_1$）对CPU负载的独特贡献。为了研究这一点，他们进行了两次初步的普通最小二乘（OLS）回归。\n\n首先，他们将CPU负载（$Y$）对截距项和网络带宽（$X_2$）进行回归。设这次回归的残差向量为 $r_Y$。\n其次，他们将活跃用户会话数（$X_1$）对截距项和网络带宽（$X_2$）进行回归。设第二次回归的残差向量为 $r_{X_1}$。\n\n从这两组残差中，计算出以下汇总统计量：\n- 第二次回归的残差平方和：$\\sum_{i=1}^{n} (r_{X_1,i})^2 = 88.4$。\n- 两次回归的对应残差乘积之和：$\\sum_{i=1}^{n} r_{Y,i} \\, r_{X_1,i} = 153.7$。\n\n仅使用这些汇总统计量，确定完整多元回归模型中系数 $\\beta_1$ 的OLS估计值。将最终答案四舍五入到三位有效数字。", "solution": "我们的目标是估计在 $Y$ 对截距项、$X_{1}$ 和 $X_{2}$ 的多元回归中的 $\\hat{\\beta}_{1}$。根据 Frisch–Waugh–Lovell 定理，OLS估计值 $\\hat{\\beta}_{1}$ 等于将 $Y$ 对 $[1, X_{2}]$ 回归的残差对 $X_{1}$ 对 $[1, X_{2}]$ 回归的残差进行回归得到的斜率。记 $Z$ 为由截距项和 $X_{2}$ 的列组成的矩阵，并定义残差生成矩阵 $M_{Z} = I - Z(Z^{\\top}Z)^{-1}Z^{\\top}$。那么\n$$\nr_{Y} = M_{Z}Y, \\quad r_{X_{1}} = M_{Z}X_{1}.\n$$\n将 $r_{Y}$ 对 $r_{X_{1}}$ 进行无截距项回归得到的OLS系数使下式最小化：\n$$\nS(\\alpha) = \\|r_{Y} - \\alpha r_{X_{1}}\\|^{2} = (r_{Y} - \\alpha r_{X_{1}})^{\\top}(r_{Y} - \\alpha r_{X_{1}}).\n$$\n求导并令其等于零可得\n$$\n\\frac{dS}{d\\alpha} = -2 r_{X_{1}}^{\\top} r_{Y} + 2 \\alpha \\, r_{X_{1}}^{\\top} r_{X_{1}} = 0 \\;\\;\\Rightarrow\\;\\; \\hat{\\alpha} = \\frac{r_{X_{1}}^{\\top} r_{Y}}{r_{X_{1}}^{\\top} r_{X_{1}}}.\n$$\n根据 Frisch–Waugh–Lovell 定理，$\\hat{\\beta}_{1} = \\hat{\\alpha}$。使用所提供的汇总统计量，\n$$\nr_{X_{1}}^{\\top} r_{X_{1}} = \\sum_{i=1}^{n} (r_{X_{1},i})^{2} = 88.4, \\quad r_{X_{1}}^{\\top} r_{Y} = \\sum_{i=1}^{n} r_{X_{1},i} r_{Y,i} = 153.7,\n$$\n所以\n$$\n\\hat{\\beta}_{1} = \\frac{153.7}{88.4} \\approx 1.7386877828\\ldots\n$$\n四舍五入到三位有效数字，\n$$\n\\hat{\\beta}_{1} \\approx 1.74.\n$$", "answer": "$$\\boxed{1.74}$$", "id": "1948175"}]}