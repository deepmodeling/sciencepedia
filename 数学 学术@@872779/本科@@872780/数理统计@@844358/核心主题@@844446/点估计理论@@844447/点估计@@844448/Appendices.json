{"hands_on_practices": [{"introduction": "矩估计法 (Method of Moments) 是构建估计量最直观的方法之一。其基本思想是通过将样本矩与相应的总体矩相等来求解未知参数。本练习 ([@problem_id:1944335]) 提供了一个应用该方法的直接示例，让您通过一个具体的概率分布来推导参数的矩估计量，从而熟练掌握其核心步骤。", "problem": "在一次材料科学实验中，一种新开发的陶瓷复合材料的标准化断裂韧性 $X$ 被发现遵循一个特定的概率分布。单次测量 $X$ 的概率密度函数 (PDF) 由下式给出：\n$$f(x;\\theta) = (\\theta+1)x^{\\theta}$$\n其中 $x \\in (0,1)$，而 $\\theta > -1$ 是一个与材料微观结构相关的未知参数。\n\n为了估计该参数，从该复合材料的随机样本中收集了一组 $n$ 次的独立测量值 $X_1, X_2, \\dots, X_n$。通过令该分布的理论均值 $E[X]$ 与样本均值 $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ 相等，构造了 $\\theta$ 的一个估计量，记作 $\\hat{\\theta}$。\n\n根据此过程，推导用样本均值 $\\bar{X}$ 表示的估计量 $\\hat{\\theta}$ 的表达式。", "solution": "给定一个随机变量 $X$，其概率密度函数为 $f(x;\\theta)=(\\theta+1)x^{\\theta}$，其中 $x\\in(0,1)$ 且 $\\theta>-1$。矩估计法将理论均值 $E[X]$ 与样本均值 $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ 相等。\n\n首先，计算均值：\n$$\nE[X]=\\int_{0}^{1}x f(x;\\theta)\\,dx=\\int_{0}^{1}x(\\theta+1)x^{\\theta}\\,dx=(\\theta+1)\\int_{0}^{1}x^{\\theta+1}\\,dx.\n$$\n由于 $\\theta>-1$，该积分收敛且\n$$\n\\int_{0}^{1}x^{\\theta+1}\\,dx=\\left[\\frac{x^{\\theta+2}}{\\theta+2}\\right]_{0}^{1}=\\frac{1}{\\theta+2},\n$$\n因此\n$$\nE[X]=\\frac{\\theta+1}{\\theta+2}.\n$$\n\n令 $E[X]=\\bar{X}$ 并求解 $\\theta$：\n$$\n\\bar{X}=\\frac{\\theta+1}{\\theta+2}\\;\\Longrightarrow\\;\\bar{X}(\\theta+2)=\\theta+1,\n$$\n$$\n\\bar{X}\\theta+2\\bar{X}=\\theta+1,\n$$\n$$\n\\bar{X}\\theta-\\theta=1-2\\bar{X},\n$$\n$$\n\\theta(\\bar{X}-1)=1-2\\bar{X},\n$$\n$$\n\\theta=\\frac{1-2\\bar{X}}{\\bar{X}-1}=\\frac{2\\bar{X}-1}{1-\\bar{X}}.\n$$\n\n因此，通过将 $\\bar{X}$ 替换为样本均值，得到矩估计量：\n$$\n\\hat{\\theta}=\\frac{2\\bar{X}-1}{1-\\bar{X}}.\n$$", "answer": "$$\\boxed{\\frac{2\\bar{X}-1}{1-\\bar{X}}}$$", "id": "1944335"}, {"introduction": "最大似然估计法 (Maximum Likelihood Estimation) 是现代统计推断的基石，也是最常用和最强大的参数估计技术之一。其核心原理是寻找能使观测到的样本数据出现概率最大的参数值。本练习 ([@problem_id:1944340]) 将此方法应用于帕累托分布，这是一个在经济学等领域广泛用于描述收入或财富分布的重要模型。", "problem": "在发展经济学中，许多国家的高端收入分布通常使用帕累托分布进行建模。一位经济学家正在研究收入超过某一最低阈值的个人的收入分布。数据由一个随机变量 $X$ 建模，该变量遵循帕累托分布，其概率密度函数 (PDF) 如下：\n$$f(x; \\alpha, c) = \\alpha c^{\\alpha} x^{-(\\alpha+1)}$$\n对于 $x \\ge c$，以及对于 $x  c$ 时，$f(x; \\alpha, c) = 0$。\n\n在此模型中，$c$ 是一个已知的正常数，代表最低收入阈值，而 $\\alpha  0$ 是未知的帕累托指数，用于衡量收入不平等。从该总体中收集了一个包含 $n$ 个收入的随机样本，记为 $X_1, X_2, \\dots, X_n$。\n\n你的任务是求参数 $\\alpha$ 的最大似然估计 (MLE)。请用包含 $n$、$c$ 和样本观测值 $X_i$ 的符号表达式来表示你的答案。", "solution": "我们有一个来自帕累托分布的独立同分布样本 $X_1, \\dots, X_n$，其阈值 $c > 0$ 已知，形状参数 $\\alpha > 0$ 未知。其概率密度函数 (PDF) 为：\n$$f(x; \\alpha, c) = \\alpha c^{\\alpha} x^{-(\\alpha+1)} \\quad \\text{对于 } x \\ge c$$\n且对于 $x  c$，$f(x; \\alpha, c) = 0$。\n\n似然函数 $L(\\alpha)$ 是在给定参数 $\\alpha$ 的情况下，观测到样本数据 $X_1, \\dots, X_n$ 的联合概率。由于样本是独立同分布的，似然函数是各观测值 PDF 的乘积：\n$$L(\\alpha | \\mathbf{x}) = \\prod_{i=1}^n \\alpha c^{\\alpha} x_i^{-(\\alpha+1)} = \\alpha^n c^{n\\alpha} \\left(\\prod_{i=1}^n x_i\\right)^{-(\\alpha+1)}$$\n为了简化计算，我们最大化对数似然函数 $\\ell(\\alpha) = \\ln L(\\alpha)$:\n$$\\ell(\\alpha) = \\ln\\left(\\alpha^n c^{n\\alpha} \\left(\\prod_{i=1}^n x_i\\right)^{-(\\alpha+1)}\\right) = n\\ln\\alpha + n\\alpha\\ln c - (\\alpha+1)\\sum_{i=1}^n \\ln x_i$$\n对 $\\alpha$ 求导并令其等于 0 来寻找最大值：\n$$\\frac{d\\ell}{d\\alpha} = \\frac{n}{\\alpha} + n\\ln c - \\sum_{i=1}^n \\ln x_i = 0$$\n求解 $\\alpha$:\n$$\\frac{n}{\\alpha} = \\sum_{i=1}^n \\ln x_i - n\\ln c = \\sum_{i=1}^n (\\ln x_i - \\ln c)$$\n利用对数律 $\\ln a - \\ln b = \\ln(a/b)$:\n$$\\frac{n}{\\alpha} = \\sum_{i=1}^n \\ln\\left(\\frac{x_i}{c}\\right)$$\n因此，$\\alpha$ 的最大似然估计量 (MLE) 是：\n$$\\hat{\\alpha}_{MLE} = \\frac{n}{\\sum_{i=1}^n \\ln\\left(\\frac{X_i}{c}\\right)}$$", "answer": "$$\\boxed{\\frac{n}{\\sum_{i=1}^{n}\\ln\\!\\left(\\frac{X_{i}}{c}\\right)}}$$", "id": "1944340"}, {"introduction": "在构建了估计量之后，一个自然的问题便是：我们如何找到“最好”的估计量？一致最小方差无偏估计量 (UMVUE) 就是衡量估计量优良性的一个黄金标准，它指的是在所有无偏估计量中方差最小的那一个。本练习 ([@problem_id:1944380]) 引导您运用强大的 Lehmann–Scheffé 定理，通过寻找一个完备充分统计量来系统地推导出均匀分布参数的 UMVUE，深刻理解为什么某些估计量具有最优性。", "problem": "一种新型量子传感器被设计用于测量粒子的一维位置。由于其测量过程中固有的量子不确定性，当粒子的真实位置为 $\\theta$ 时，来自传感器的一次测量 $X$ 是一个在区间 $[\\theta - 1/2, \\theta + 1/2]$ 上服从均匀概率分布的随机变量。在一个实验中，对一个位置固定但未知的粒子进行了 $n$ 次独立测量，记为 $X_1, X_2, \\dots, X_n$。\n\n目标是基于这个样本找到 $\\theta$ 的最佳估计量。您的任务是推导参数 $\\theta$ 的一致最小方差无偏估计量 (UMVUE)。\n\n令样本的次序统计量表示为 $X_{(1)} \\le X_{(2)} \\le \\dots \\le X_{(n)}$，其中 $X_{(1)} = \\min(X_1, \\dots, X_n)$ 且 $X_{(n)} = \\max(X_1, \\dots, X_n)$。请用 $X_{(1)}$ 和 $X_{(n)}$ 表示您的最终答案。", "solution": "观测模型为 $X_{i} \\sim \\text{Uniform}\\!\\left[\\theta - \\frac{1}{2}, \\theta + \\frac{1}{2}\\right]$，且对于 $i=1,\\dots,n$，$X_i$ 相互独立。单个 $X_{i}$ 的密度为\n$$\nf(x_{i}\\,|\\,\\theta) = \\mathbf{1}\\!\\left\\{\\theta - \\frac{1}{2} \\le x_{i} \\le \\theta + \\frac{1}{2}\\right\\}。\n$$\n因此，联合密度为\n$$\nf(x_{1},\\dots,x_{n}\\,|\\,\\theta) = \\prod_{i=1}^{n} \\mathbf{1}\\!\\left\\{\\theta - \\frac{1}{2} \\le x_{i} \\le \\theta + \\frac{1}{2}\\right\\}\n= \\mathbf{1}\\!\\left\\{\\max_{1\\le i\\le n} x_{i} \\le \\theta + \\frac{1}{2},\\ \\min_{1\\le i\\le n} x_{i} \\ge \\theta - \\frac{1}{2}\\right\\}。\n$$\n记 $X_{(1)}=\\min_{i}X_{i}$ 和 $X_{(n)}=\\max_{i}X_{i}$，上式变为\n$$\nf(x_{1},\\dots,x_{n}\\,|\\,\\theta) = \\mathbf{1}\\!\\left\\{ \\theta \\in \\left[X_{(n)} - \\frac{1}{2},\\ X_{(1)} + \\frac{1}{2}\\right]\\right\\},\n$$\n它仅通过 $(X_{(1)},X_{(n)})$ 依赖于样本。根据因子分解定理，$(X_{(1)},X_{(n)})$ 是 $\\theta$ 的一个充分统计量。\n\n引入 $U_{i} = X_{i} - \\theta + \\frac{1}{2}$，于是 $U_{i} \\sim \\text{Uniform}[0,1]$ 独立同分布 (i.i.d.)，且 $X_{(1)} = \\theta - \\frac{1}{2} + U_{(1)}$，$X_{(n)} = \\theta - \\frac{1}{2} + U_{(n)}$。对于 $k=1,\\dots,n$，第 $k$ 个次序统计量 $U_{(k)}$ 服从 $\\text{Beta}(k,n+1-k)$ 分布，因此\n$$\n\\mathbb{E}[U_{(k)}] = \\frac{k}{n+1}。\n$$\n所以\n$$\n\\mathbb{E}\\!\\left[\\frac{X_{(1)} + X_{(n)}}{2}\\right]\n= \\mathbb{E}\\!\\left[\\theta - \\frac{1}{2} + \\frac{U_{(1)} + U_{(n)}}{2}\\right]\n= \\theta - \\frac{1}{2} + \\frac{1}{2}\\left(\\frac{1}{n+1} + \\frac{n}{n+1}\\right)\n= \\theta。\n$$\n因此，中程数 $\\frac{X_{(1)} + X_{(n)}}{2}$ 是 $\\theta$ 的一个无偏估计量。\n\n为了应用 Rao–Blackwellization，我们从无偏估计量 $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ 开始（因为 $\\mathbb{E}[\\bar{X}] = \\theta$）。给定 $X_{(1)}=a$ 和 $X_{(n)}=b$，剩下的 $n-2$ 个观测值在条件上独立同分布于 $\\text{Uniform}[a,b]$，所以它们的条件均值是 $\\frac{a+b}{2}$。因此\n$$\n\\mathbb{E}[\\bar{X}\\,|\\,X_{(1)}=a, X_{(n)}=b]\n= \\frac{1}{n}\\left(a + b + (n-2)\\cdot \\frac{a+b}{2}\\right)\n= \\frac{a+b}{2}。\n$$\n因此，$\\bar{X}$ 关于充分统计量 $(X_{(1)},X_{(n)})$ 的 Rao–Blackwell 改进恰好是中程数 $\\frac{X_{(1)} + X_{(n)}}{2}$，它是一个无偏估计量。\n\n最后，对于这个单参数均匀位置族，$(X_{(1)},X_{(n)})$ 不仅是充分的，而且是完备的。要证明这一点，一个标准方法是把 $(X_{(1)},X_{(n)})$ 表示为 $(\\theta - \\frac{1}{2} + U_{(1)}, \\theta - \\frac{1}{2} + U_{(n)})$，其中 $(U_{(1)},U_{(n)})$ 在 $\\{0 \\le u \\le v \\le 1\\}$ 上的联合密度为 $n(n-1)(v-u)^{n-2}$。如果对于所有的 $\\theta$，都有 $\\mathbb{E}_{\\theta}[g(X_{(1)},X_{(n)})]=0$，那么对于所有的 $t \\in \\mathbb{R}$，\n$$\n\\int_{0}^{1}\\int_{u}^{1} g(t+u, t+v)\\, n(n-1)(v-u)^{n-2}\\, dv\\, du = 0,\n$$\n根据其平移结构和在紧支撑集上的非退化核，这意味着 $g=0$ 几乎处处成立。因此 $(X_{(1)},X_{(n)})$ 是完备且充分的。根据 Lehmann–Scheffé 定理，经过 Rao–Blackwell 化的无偏估计量就是 UMVUE。\n\n所以，$\\theta$ 的 UMVUE 是\n$$\n\\frac{X_{(1)} + X_{(n)}}{2}。\n$$", "answer": "$$\\boxed{\\frac{X_{(1)}+X_{(n)}}{2}}$$", "id": "1944380"}]}