## 引言
条件期望是概率论中一个既深刻又实用的核心概念。它将我们日常生活中“根据新信息更新预测”的直觉，提炼成一个严谨的数学工具。当我们获得关于一个[不确定系统](@entry_id:177709)的部分信息时，我们对系统中某个随机量值的最佳猜测应该如何调整？[条件期望](@entry_id:159140)正是回答这一问题的数学语言。它不仅是理论研究的基石，更在[统计推断](@entry_id:172747)、[金融工程](@entry_id:136943)、信号处理等众多应用领域扮演着不可或缺的角色。

然而，从初等概率中基于特定事件的[条件期望](@entry_id:159140)，到现代概率论中基于[σ-代数](@entry_id:141463)的抽象定义，存在一个概念上的飞跃。本文旨在填补这一鸿沟，引领读者系统地掌握[条件期望](@entry_id:159140)的性质及其强大功能。我们将不再局限于零散的公式，而是构建一个完整的知识框架。

在接下来的内容中，我们将分三个章节逐步深入：第一章“原理与机制”将从[测度论](@entry_id:139744)出发，严格定义条件期望，并系统阐述其关键的代数、几何与分析性质，如[塔性质](@entry_id:273153)和正交投影；第二章“应用与跨学科联系”将展示这些理论如何在统计学、[随机过程](@entry_id:159502)、金融等领域解决实际问题，揭示其作为最优估计和动态[系统分析](@entry_id:263805)工具的威力；最后，在“动手实践”部分，您将通过解决一系列精心设计的问题，来巩固和深化对这些概念的理解。现在，让我们从其最根本的数学原理开始探索。

## 原理与机制

在理解了[条件期望](@entry_id:159140)作为给定信息下对[随机变量](@entry_id:195330)的最佳预测这一基本思想之后，我们现在转向对其数学原理和核心机制的深入探讨。本章将系统地阐述条件期望的定义、关键性质以及其在不同数学框架下的深刻诠释。我们将从最基本的情形出发，逐步建立一套强大的理论工具，这些工具在[随机过程](@entry_id:159502)、统计学和[金融数学](@entry_id:143286)等领域都至关重要。

### 定义与基本诠释

从[测度论](@entry_id:139744)的视角来看，[条件期望](@entry_id:159140)是一个严格定义的数学对象。给定一个概率空间 $(\Omega, \mathcal{F}, P)$，一个可积[随机变量](@entry_id:195330) $X$（即 $E[|X|] < \infty$），以及一个子 $\sigma$-代数 $\mathcal{G} \subseteq \mathcal{F}$，[随机变量](@entry_id:195330) $X$ 关于 $\mathcal{G}$ 的**条件期望**，记为 $E[X|\mathcal{G}]$，是一个[随机变量](@entry_id:195330) $Y$，它满足以下两个决定性条件：
1.  **[可测性](@entry_id:199191)**：$Y$ 是 $\mathcal{G}$-可测的。这意味着 $Y$ 的取值完全由 $\mathcal{G}$ 中包含的信息所确定。换言之，如果两个结果 $\omega_1, \omega_2$ 在 $\mathcal{G}$ 所描述的事件中无法区分，那么 $Y(\omega_1) = Y(\omega_2)$。
2.  **部分平均性质**：对于任何事件 $A \in \mathcal{G}$，$Y$ 在 $A$ 上的期望与 $X$ 在 $A$ 上的期望相等，即：
    $$
    \int_{A} Y \,dP = \int_{A} X \,dP
    $$
    这可以更直观地写成 $E[Y \mathbf{1}_{A}] = E[X \mathbf{1}_{A}]$，其中 $\mathbf{1}_{A}$ 是事件 $A$ 的[指示函数](@entry_id:186820)。

这两个条件唯一地（在[几乎必然](@entry_id:262518)相等的意义上）确定了[随机变量](@entry_id:195330) $E[X|\mathcal{G}]$。直观上，它是在给定信息 $\mathcal{G}$ 的情况下，对 $X$ 的最佳猜测或预测。

为了更好地理解这个抽象定义，我们可以考察信息集 $\mathcal{G}$ 的极端情况。假设我们没有任何有效信息，即 $\mathcal{G}$ 是**平凡 $\sigma$-代数** $\mathcal{G} = \{\emptyset, \Omega\}$。根据[可测性](@entry_id:199191)要求，$E[X|\mathcal{G}]$ 必须是一个常数 $c$（因为它在整个 $\Omega$ 上不能区分任何结果）。为了确定这个常数，我们使用部分平均性质，令 $A=\Omega$：
$$
E[c \cdot \mathbf{1}_{\Omega}] = E[X \cdot \mathbf{1}_{\Omega}] \implies E[c] = E[X] \implies c = E[X]
$$
因此，当信息为空时，对 $X$ 的最佳预测就是其无条件期望 $E[X]$ [@problem_id:1438516]。这符合我们的直觉：在没有任何特定信息的情况下，最好的猜测就是[总体平均值](@entry_id:175446)。

相反，如果信息是完备的，即 $\mathcal{G} = \mathcal{F}$，那么 $E[X|\mathcal{F}]$ 就等于 $X$ 本身，因为 $X$ 已经是 $\mathcal{F}$-可测的，并且对于任何 $A \in \mathcal{F}$，显然满足 $\int_A X dP = \int_A X dP$。

#### 基于离散划分的[条件期望](@entry_id:159140)

当 $\sigma$-代数 $\mathcal{G}$ 由[样本空间](@entry_id:275301) $\Omega$ 的一个有限或可数划分 $\mathcal{P} = \{A_1, A_2, \dots\}$ 生成时，[条件期望](@entry_id:159140)的形式变得非常直观。在这种情况下，任何 $\mathcal{G}$-可测的[随机变量](@entry_id:195330)必须在每个划分块 $A_k$ 上取常数值。因此，$E[X|\mathcal{G}]$ 是一个阶梯函数。对于任意 $\omega \in A_k$，其值 $y_k$ 可以通过定义确定：
$$
\int_{A_k} y_k \,dP = \int_{A_k} X \,dP \implies y_k P(A_k) = E[X \mathbf{1}_{A_k}]
$$
只要 $P(A_k) > 0$，我们就可以得到
$$
y_k = \frac{E[X \mathbf{1}_{A_k}]}{P(A_k)} = E[X | A_k]
$$
这正是 $X$ 在事件 $A_k$ 发生条件下的初等[条件期望](@entry_id:159140)。因此，$E[X|\mathcal{G}]$ 是一个[随机变量](@entry_id:195330)，当结果 $\omega$ 落在 $A_k$ 中时，它的值就是 $X$ 在 $A_k$ 上的平均值 [@problem_id:1438496]。

例如，考虑一个樣本空間 $\Omega = \{1, 2, 3, 4, 5, 6\}$，概率 $P(\{\omega\}) = \omega/21$。设[随机变量](@entry_id:195330) $X(\omega) = \omega^2$，信息 $\mathcal{G}$ 由划分 $\{A_1, A_2, A_3\}$ 生成，其中 $A_1 = \{1, 2\}$, $A_2 = \{3, 4, 5\}$, $A_3 = \{6\}$。$Y=E[X|\mathcal{G}]$ 在 $A_1, A_2, A_3$ 上分别取常数值 $y_1, y_2, y_3$。计算 $y_2$：
$$
y_2 = E[X | A_2] = \frac{\sum_{\omega \in A_2} X(\omega) P(\{\omega\})}{P(A_2)} = \frac{3^2 \cdot \frac{3}{21} + 4^2 \cdot \frac{4}{21} + 5^2 \cdot \frac{5}{21}}{\frac{3}{21} + \frac{4}{21} + \frac{5}{21}} = \frac{27+64+125}{3+4+5} = \frac{216}{12} = 18
$$
类似地，可以算出 $y_1=3$ 和 $y_3=36$。因此，$E[X|\mathcal{G}]$ 是一个取值为 $3, 18, 36$ 的[随机变量](@entry_id:195330)，具体取值取决于结果落在哪一个划分块中。

这个思想可以推广到由[离散随机变量](@entry_id:163471) $N$ 生成的 $\sigma$-代数 $\mathcal{G} = \sigma(N)$。此时，划分的原[子集](@entry_id:261956)是 $A_k = \{N=n_k\}$。[条件期望](@entry_id:159140) $E[X|\sigma(N)]$ 是一个关于 $N$ 的函数。当 $N$ 取值 $n_k$ 时，$E[X|\sigma(N)]$ 的值就是 $c_k = E[X|N=n_k]$。我们可以将其优雅地写成一个表达式：
$$
E[X|\sigma(N)] = \sum_{k=1}^{\infty} E[X|N=n_k] \cdot \mathbf{1}_{\{N=n_k\}}
$$
这个公式完美地连接了[测度论](@entry_id:139744)的抽象定义和基于特定事件的初等条件期望 [@problem_id:1438515]。

### 核心代数性质

[条件期望](@entry_id:159140)算子 $E[\cdot|\mathcal{G}]$ 拥有一系列与普通期望算子 $E[\cdot]$相似的代数性质，这些性质是进行计算和推导的基础。

#### 线性性

与普通期望一样，[条件期望](@entry_id:159140)也是一个**线性算子**。对于任意可积[随机变量](@entry_id:195330) $X$ 和 $Y$ 以及任意实数常量 $\alpha$ 和 $\beta$，我们有：
$$
E[\alpha X + \beta Y | \mathcal{G}] = \alpha E[X|\mathcal{G}] + \beta E[Y|\mathcal{G}] \quad \text{a.s.}
$$
这个性质极大地简化了涉及多个[随机变量](@entry_id:195330)组合的计算。例如，在金融模型中，一个投资组合 $W$ 的价值可能是由多个资产 $X$ 和 $Y$ [线性组合](@entry_id:154743)而成，$W = \alpha X + \beta Y$。如果已知在某种市场信息 $\mathcal{G}$（例如由市场波动率 $Z$ 生成的 $\sigma$-代数）下，各资产的预期价格变动为 $E[X|\mathcal{G}] = k_1 Z^2 + k_2$ 和 $E[Y|\mathcal{G}] = m_1 Z + m_2$，那么该投资组合的条件期望可以直接通过[线性组合](@entry_id:154743)得到：
$$
E[W|\mathcal{G}] = \alpha(k_1 Z^2 + k_2) + \beta(m_1 Z + m_2) = \alpha k_1 Z^2 + \beta m_1 Z + \alpha k_2 + \beta m_2
$$
这个结果本身也是 $\mathcal{G}$-可测的，因为它是一个关于 $Z$ 的函数 [@problem_id:1438526]。

#### 提取已知信息 (Taking Out What is Known)

一个至关重要的性质是，任何在给定信息 $\mathcal{G}$ 下已经“已知”的量（即 $\mathcal{G}$-可测的[随机变量](@entry_id:195330)），都可以从条件期望中“提取”出来。更形式化地，如果 $Z$ 是一个 $\mathcal{G}$-可测的[随机变量](@entry_id:195330)，并且 $X$ 和 $ZX$ 都是可积的，那么：
$$
E[ZX|\mathcal{G}] = Z \cdot E[X|\mathcal{G}] \quad \text{a.s.}
$$
这个性质的直观解释是，在以 $\mathcal{G}$ 为条件的环境下，$Z$ 的值是确定的，因此它可以被当作一个系数从期望中提出来。一个常见的应用场景是当信息集由某个[随机变量](@entry_id:195330) $Y$ 生成时，即 $\mathcal{G} = \sigma(Y)$。任何关于 $Y$ 的函数 $g(Y)$ 都是 $\sigma(Y)$-可测的。因此，我们可以得到：
$$
E[g(Y)X | \sigma(Y)] = g(Y) E[X|\sigma(Y)]
$$
例如，要计算 $E[Y^3 X | \sigma(Y)]$，我们可以立即将其简化为 $Y^3 E[X|\sigma(Y)]$，然后只需计算更简单的项 $E[X|\sigma(Y)]$ [@problem_id:1438494]。这个性质在[随机微积分](@entry_id:143864)和[鞅](@entry_id:267779)论中有基础性的作用。

其他基本性质，如**保号性**（若 $X \ge 0$ a.s.，则 $E[X|\mathcal{G}] \ge 0$ a.s.）和**[单调性](@entry_id:143760)**（若 $X \ge Y$ a.s.，则 $E[X|\mathcal{G}] \ge E[Y|\mathcal{G}]$ a.s.），也直接继承自普通期望的积分性质。

### [塔性质](@entry_id:273153)及其推论

条件期望的一个深刻性质是它在处理多层信息结构时的行为，这由所谓的**[塔性质](@entry_id:273153)**（或称**[迭代期望定律](@entry_id:188849)**）所描述。

#### [塔性质](@entry_id:273153) (Tower Property)

假设我们有两个信息集，由子 $\sigma$-代数 $\mathcal{H}$ 和 $\mathcal{G}$ 代表，并且 $\mathcal{H}$ 是比 $\mathcal{G}$ 更粗糙的信息集，即 $\mathcal{H} \subset \mathcal{G}$。[塔性质](@entry_id:273153)表明，对 $X$ 先按精细信息 $\mathcal{G}$ 取条件期望，然后再对结果按粗糙信息 $\mathcal{H}$ 取[条件期望](@entry_id:159140)，其效果等同于直接对 $X$ 按粗糙信息 $\mathcal{H}$ 取条件期望。数学上表示为：
$$
E[E[X|\mathcal{G}]|\mathcal{H}] = E[X|\mathcal{H}] \quad \text{a.s.}
$$
反之，$E[E[X|\mathcal{H}]|\mathcal{G}] = E[X|\mathcal{H}]$ 也成立，因为 $E[X|\mathcal{H}]$ 已经是 $\mathcal{H}$-可测的，因此也是 $\mathcal{G}$-可测的，所以按 $\mathcal{G}$ 再取条件期望不会改变它。

我们可以通过一个具体的例子来验证这个性质 [@problem_id:1438525]。考虑掷一个公平的六面骰子，$\Omega = \{1, ..., 6\}$，$X(\omega) = \omega^2$。设 $\mathcal{H}$ 由划分 $\{\{1,2\}, \{3,4,5,6\}\}$ 生成，$\mathcal{G}$ 由更精细的划分 $\{\{1,2\}, \{3,4\}, \{5,6\}\}$ 生成。显然 $\mathcal{H} \subset \mathcal{G}$。
第一步，计算 $Z = E[X|\mathcal{G}]$。$Z$ 在 $\mathcal{G}$ 的每个原[子集](@entry_id:261956)上取平均值：
*   在 $\{1,2\}$ 上，$Z = (1^2+2^2)/2 = 2.5$
*   在 $\{3,4\}$ 上，$Z = (3^2+4^2)/2 = 12.5$
*   在 $\{5,6\}$ 上，$Z = (5^2+6^2)/2 = 30.5$

第二步，计算 $Y = E[Z|\mathcal{H}]$。$Y$ 在 $\mathcal{H}$ 的每个原[子集](@entry_id:261956)上取 $Z$ 的（带权）平均值：
*   在 $\{1,2\}$ 上，$Y = (Z(1)+Z(2))/2 = (2.5+2.5)/2 = 2.5$
*   在 $\{3,4,5,6\}$ 上，$Y = (Z(3)+Z(4)+Z(5)+Z(6))/4 = (12.5+12.5+30.5+30.5)/4 = 86/4 = 21.5$

现在，我们直接计算 $E[X|\mathcal{H}]$：
*   在 $\{1,2\}$ 上，$E[X|\mathcal{H}] = (1^2+2^2)/2 = 2.5$
*   在 $\{3,4,5,6\}$ 上，$E[X|\mathcal{H}] = (3^2+4^2+5^2+6^2)/4 = (9+16+25+36)/4 = 86/4 = 21.5$

我们看到 $E[E[X|\mathcal{G}]|\mathcal{H}]$ 和 $E[X|\mathcal{H}]$ 的结果完全相同，验证了[塔性质](@entry_id:273153)。

#### [全期望定律](@entry_id:265946) (Law of Total Expectation)

[塔性质](@entry_id:273153)的一个极其重要的特例是**[全期望定律](@entry_id:265946)**。通过在[塔性质](@entry_id:273153)中令 $\mathcal{H}$为平凡 $\sigma$-代数 $\{\emptyset, \Omega\}$，我们得到 $E[X|\mathcal{H}] = E[X]$。于是公式变为：
$$
E[E[X|\mathcal{G}]] = E[X]
$$
这个定律表明，一个[随机变量](@entry_id:195330)的无[条件期望](@entry_id:159140)，等于其关于某个信息集 $\mathcal{G}$ 的条件期望的期望。这在处理多阶段随机实验时非常有用。我们可以先在一个中间阶段的条件下计算期望，然后再对该中间阶段的所有可能性求平均。

例如，一个[生态模型](@entry_id:186101)描述昆虫繁殖 [@problem_id:1438501]。一只雌虫产卵数 $N$ 服从均值为 $\lambda$ 的泊松分布，而每顆卵的孵化概率 $P$ 本身是一个[随机变量](@entry_id:195330)，服从 $[a, b]$ 上的[均匀分布](@entry_id:194597)。要求孵化出的蛋的总数 $X$ 的期望 $E[X]$。直接计算可能很复杂，但我们可以使用[全期望定律](@entry_id:265946)。首先，固定 $N$ 和 $P$ 的值，孵化出的蛋的数量服从二项分布 $B(N, P)$，其期望为 $NP$。所以：
$$
E[X | N, P] = NP
$$
然后，我们对所有可能的 $N$ 和 $P$ 求期望：
$$
E[X] = E[E[X|N, P]] = E[NP]
$$
由于 $N$ 和 $P$ [相互独立](@entry_id:273670)，$E[NP] = E[N]E[P]$。我们知道 $E[N]=\lambda$，$E[P] = (a+b)/2$。因此：
$$
E[X] = \lambda \frac{a+b}{2}
$$
若 $\lambda=12, a=0.5, b=0.8$，则 $E[X] = 12 \cdot (0.5+0.8)/2 = 7.8$。

### 解析与几何性质

除了代数性质，条件期望还具有深刻的几何和分析特性，尤其是在平方可积[随机变量](@entry_id:195330)的希尔伯特空间 $L^2(\Omega, \mathcal{F}, P)$ 中。

#### L2 空间中的正交投影

对于一个平方可积的[随机变量](@entry_id:195330) $X$（即 $E[X^2] < \infty$），条件期望 $E[X|\mathcal{G}]$ 在 $L^2$ 空间中扮演着**[正交投影](@entry_id:144168)**的角色。$L^2(\Omega, \mathcal{G}, P)$ 是由所有平方可积且 $\mathcal{G}$-可测的[随机变量](@entry_id:195330)构成的闭合[子空间](@entry_id:150286)。$E[X|\mathcal{G}]$ 正是 $X$ 在这个[子空间](@entry_id:150286)上的正交投影。

这个几何诠释的核心意义在于，$E[X|\mathcal{G}]$ 是在所有 $\mathcal{G}$-可测函数中，对 $X$ 的**最佳均方误差近似**。也就是说，对于任何 $\mathcal{G}$-可测的[随机变量](@entry_id:195330) $Z \in L^2(\Omega, \mathcal{G}, P)$，下面的不等式成立：
$$
E[(X - E[X|\mathcal{G}])^2] \le E[(X - Z)^2]
$$
等号成立当且仅当 $Z = E[X|\mathcal{G}]$ a.s.。误差项 $X - E[X|\mathcal{G}]$ 与[子空间](@entry_id:150286) $L^2(\Omega, \mathcal{G}, P)$ 中的任何元素 $Z$都是正交的，即 $E[(X - E[X|\mathcal{G}])Z] = 0$。

我们可以计算这个最小的[均方误差](@entry_id:175403) [@problem_id:1438507]。考虑连续抛掷三枚公平硬币，令 $X$ 为正面总数，$\mathcal{G}$ 为前两次抛掷结果的信息。令 $Y$ 为前两次的正面数，$Z$ 为第三次是否为正面的[指示变量](@entry_id:266428) ($1$ 或 $0$)。则 $X=Y+Z$。$Y$是 $\mathcal{G}$-可测的，$Z$ 与 $\mathcal{G}$ 独立。
$$
E[X|\mathcal{G}] = E[Y+Z|\mathcal{G}] = E[Y|\mathcal{G}] + E[Z|\mathcal{G}] = Y + E[Z] = Y + 0.5
$$
这里的 $Y + 0.5$ 就是给定前两次结果后对总正面数的最佳预测。[预测误差](@entry_id:753692)为：
$$
X - E[X|\mathcal{G}] = (Y+Z) - (Y+0.5) = Z - 0.5
$$
最小均方误差就是这个误差的平方的期望：
$$
E[(X - E[X|\mathcal{G}])^2] = E[(Z-0.5)^2] = \text{Var}(Z)
$$
由于 $Z$ 是一个参数为 $0.5$ 的伯努利[随机变量](@entry_id:195330)，其[方差](@entry_id:200758)为 $p(1-p) = 0.5(1-0.5) = 0.25$。这表明，知道了前两次的结果后，我们对总数的[预测误差](@entry_id:753692)的期望平方值是 $0.25$。

#### 条件 Jensen 不等式

Jensen 不等式可以推广到条件期望。如果 $\phi: \mathbb{R} \to \mathbb{R}$ 是一个凸函数，且 $X$ 和 $\phi(X)$ 都是可积的，那么：
$$
\phi(E[X|\mathcal{G}]) \le E[\phi(X)|\mathcal{G}] \quad \text{a.s.}
$$
这个不等式非常强大。一个直接的应用是定义**[条件方差](@entry_id:183803)**。取 $\phi(x) = x^2$，这是一个凸函数。我们得到：
$$
(E[X|\mathcal{G}])^2 \le E[X^2|\mathcal{G}]
$$
这使我们能够定义[条件方差](@entry_id:183803)为一个非负的[随机变量](@entry_id:195330)：
$$
\text{Var}(X|\mathcal{G}) := E[X^2|\mathcal{G}] - (E[X|\mathcal{G}])^2 \ge 0 \quad \text{a.s.}
$$
[条件方差](@entry_id:183803) $\text{Var}(X|\mathcal{G})$ 度量了在给定信息 $\mathcal{G}$ 之后，$X$ 剩余的不确定性或波动性。在某些特殊情况下，这个剩余波动性可能不依赖于 $\mathcal{G}$ 中的具体信息，从而成为一个常数 [@problem_id:1438498]。例如，对于 $\Omega = \{1,2,3,4\}$ 上的[均匀分布](@entry_id:194597)，$X(\omega)=\omega$，$\mathcal{G}$由划分 $\{\{1,2\}, \{3,4\}\}$生成。在原[子集](@entry_id:261956) $\{1,2\}$ 上，$E[X|\mathcal{G}] = 1.5$, $E[X^2|\mathcal{G}] = (1^2+2^2)/2 = 2.5$，所以 $\text{Var}(X|\mathcal{G}) = 2.5 - (1.5)^2 = 0.25$。在原[子集](@entry_id:261956) $\{3,4\}$ 上，$E[X|\mathcal{G}] = 3.5$, $E[X^2|\mathcal{G}] = (3^2+4^2)/2 = 12.5$，所以 $\text{Var}(X|\mathcal{G}) = 12.5 - (3.5)^2 = 0.25$。在这种情况下，[条件方差](@entry_id:183803)是一个常数 $0.25$。

### 与独立性的关系

[条件期望](@entry_id:159140)与[统计独立性](@entry_id:150300)之间有着密切但微妙的联系。理解这种联系对于避免概念混淆至关重要。

首先，**独立性蕴含着均值独立性**。如果[随机变量](@entry_id:195330) $X$ 与 $\sigma$-代数 $\mathcal{G}$ 相互独立，那么 $X$ 关于 $\mathcal{G}$ 的条件期望就是其无条件期望：
$$
E[X|\mathcal{G}] = E[X] \quad \text{a.s.}
$$
这个结论的证明直接源于定义。对于任何 $A \in \mathcal{G}$，由于 $X$ 和 $\mathbf{1}_A$ 是独立的，我们有 $E[X\mathbf{1}_A] = E[X]E[\mathbf{1}_A] = E[X]P(A)$。而 $E[E[X]\mathbf{1}_A] = E[X]P(A)$ 也成立。根据条件期望的唯一性，必然有 $E[X|\mathcal{G}] = E[X]$。直观上，如果 $\mathcal{G}$ 中的信息与 $X$ 无关，那么这些信息对于预测 $X$ 的平均值没有任何帮助，最佳预测仍然是全局平均值 $E[X]$。

然而，反向的结论并不成立。**均值独立性并不蕴含[统计独立性](@entry_id:150300)** [@problem_id:1438532]。换句话说，$E[X|\mathcal{G}] = E[X]$ 这一条件不足以保证 $X$ 和 $\mathcal{G}$ [相互独立](@entry_id:273670)。条件期望只捕捉了[随机变量](@entry_id:195330)的**均值**如何随信息变化，而完全的[统计独立性](@entry_id:150300)要求[随机变量](@entry_id:195330)的**整个[分布](@entry_id:182848)**都不受信息影响。

一个经典的反例可以说明这一点。设[随机变量](@entry_id:195330) $X$ 以等概率取值于 $\{-1, 1\}$，即 $P(X=1) = P(X=-1) = 0.5$。那么 $E[X]=0$。现在考虑 $\mathcal{G} = \sigma(X^2)$。由于 $X^2$ 恒等于 $1$，所以 $\mathcal{G}$ 是平凡 $\sigma$-代数 $\{\emptyset, \Omega\}$。因此，$E[X|\mathcal{G}] = E[X] = 0$。在这种情况下，$X$ 和 $\mathcal{G}$ 是独立的。

但我们可以构造一个更精妙的例子。设 $X$ 以概率 $p$ 取 $1$，概率 $p$ 取 $-1$，概率 $1-2p$ 取 $0$（其中 $0 \lt p \lt 0.5$）。那么 $E[X] = 1 \cdot p + (-1) \cdot p + 0 \cdot (1-2p) = 0$。现在令 $\mathcal{G} = \sigma(X^2)$。$\mathcal{G}$ 是由事件 $\{X^2=0\}(=\{X=0\})$ 和 $\{X^2=1\}(=\{X=1\} \cup \{X=-1\})$ 生成的。我们来计算 $E[X|\mathcal{G}]$：
*   当事件 $\{X^2=0\}$ 发生时，我们确定 $X=0$，所以 $E[X|X^2=0] = 0$。
*   当事件 $\{X^2=1\}$ 发生时，我们知道 $X$ 只能是 $1$ 或 $-1$。此时，$P(X=1|X^2=1) = \frac{P(X=1, X^2=1)}{P(X^2=1)} = \frac{P(X=1)}{P(X=1)+P(X=-1)} = \frac{p}{2p} = 0.5$。同样 $P(X=-1|X^2=1) = 0.5$。因此，$E[X|X^2=1] = 1 \cdot 0.5 + (-1) \cdot 0.5 = 0$。

在所有情况下，$E[X|\mathcal{G}]$ 都等于 $0$，所以 $E[X|\mathcal{G}] = E[X]$ 成立。然而，$X$ 和 $X^2$ 显然不是独立的。例如，$P(X=0|X^2=1) = 0$，但这不等于 $P(X=0) = 1-2p > 0$。这个例子清楚地表明，即便一个变量的条件均值不随信息变化，它的条件分布的其他方面（如[方差](@entry_id:200758)或特定取值的概率）仍可能依赖于该信息。