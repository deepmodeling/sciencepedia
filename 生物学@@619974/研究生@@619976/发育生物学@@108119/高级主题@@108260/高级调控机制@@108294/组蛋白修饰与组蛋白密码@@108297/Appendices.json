{"hands_on_practices": [{"introduction": "组蛋白修饰的一个直接后果是改变其物理化学性质，尤其是静电特性。赖氨酸乙酰化通过中和其正电荷来减弱组蛋白尾部与带负电的DNA主链之间的静电吸引力，这是染色质开放的基础。本练习将通过一个量化模型，训练您计算乙酰化如何改变核小体的净电荷，从而将抽象的“组蛋白密码”概念根植于基础的生物物理学原理中 [@problem_id:2821692]。", "problem": "组蛋白密码子假说强调，组蛋白尾部翻译后修饰（PTM）的模式，例如赖氨酸乙酰化，部分通过改变静电相互作用来调节染色质功能。考虑一个核小体，其组蛋白尾部总共含有 $N = 44$ 个赖氨酸残基，这些残基是乙酰化的潜在靶点。在生理pH条件下，由于其质子化的ε-氨基，每个未修饰的赖氨酸对尾部电荷的贡献约为 $+1$。乙酰化会中和此正电荷，使该位点的电荷贡献变为 $0$。已知在任何乙酰化发生之前，测得的尾部净电荷为 $Q_{0} = +36$（这是所有尾部残基电荷的代数和，不一定等于 $N$）。在一个稳态细胞群体中，假设有比例为 $f$ 的尾部赖氨酸被乙酰化，且乙酰化在 $N$ 个位点上均匀且独立地发生。\n\n仅从以下两点出发：(i) 净电荷是各位点电荷的代数和；(ii) 赖氨酸位点的乙酰化使其电荷贡献相对于未乙酰化状态改变了 $-1$。请推导期望净尾部电荷 $\\mathbb{E}[Q]$ 作为 $f$、$N$ 和 $Q_{0}$ 的函数表达式。然后，在 $f = 0.30$、$N = 44$ 和 $Q_{0} = +36$ 的条件下，计算该表达式的值。\n\n将最终答案表示为一个不带单位的实数，不要四舍五入。", "solution": "首先对问题陈述进行严格验证。\n\n步骤1：提取已知条件。\n- 组蛋白尾部上潜在的赖氨酸乙酰化位点数量为 $N = 44$。\n- 未修饰赖氨酸的电荷贡献为 $+1$。\n- 乙酰化赖氨酸的电荷贡献为 $0$。\n- 尾部的初始净电荷（乙酰化前）为 $Q_{0} = +36$。\n- 比例为 $f$ 的 $N$ 个赖氨酸位点被乙酰化。\n- 乙酰化在 $N$ 个位点上均匀且独立地发生。\n- 推导必须从两个原则出发：(i) 净电荷是各位点电荷的代数和；(ii) 赖氨酸位点的乙酰化使其电荷贡献改变 $-1$。\n- 任务是推导期望净尾部电荷 $\\mathbb{E}[Q]$ 作为 $f$、$N$ 和 $Q_{0}$ 的函数的表达式，然后在 $f = 0.30$、$N = 44$ 和 $Q_{0} = +36$ 的条件下求值。\n\n步骤2：使用提取的已知条件进行验证。\n根据有效性标准对问题进行评估。\n- **科学依据充分**：该问题基于分子生物学中组蛋白密码子假说的一个核心原则——诸如乙酰化之类的翻译后修饰会改变组蛋白尾部的静电特性。分配给未修饰（$+1$）和乙酰化（$0$）赖氨酸的电荷值在生物化学上是正确的。$Q_{0} \\neq N$ 这一事实也是现实的，因为它正确地暗示了除了指定的 $N$ 个赖氨酸之外，还有其他残基对总尾部电荷有贡献。该问题在科学上是合理的。\n- **适定性良好**：该问题是适定的。它指定了一个概率模型（均匀且独立的乙酰化），并要求计算系统属性（净电荷）的期望值。这是统计力学和概率论中的一个标准问题，保证了唯一且有意义的解。\n- **客观性**：该问题以精确、定量且无偏见的语言陈述。所有术语都得到了明确定义。\n\n该问题没有验证清单中列出的任何缺陷。它不是科学上不合理、不可形式化、不完整、自相矛盾、不切实际或不适定的。这是一个有效的科学问题。\n\n步骤3：结论与行动。\n问题有效。将推导解答。\n\n组蛋白尾部的净电荷 $Q$ 可以表示为初始电荷 $Q_{0}$ 与乙酰化导致的总电荷变化 $\\Delta Q$ 之和。\n$$Q = Q_{0} + \\Delta Q$$\n根据问题的明确指示，单个赖氨酸位点在乙酰化后电荷贡献的变化是从 $+1$ 到 $0$，即净变化为 $-1$。设 $K$ 为随机变量，表示在 $N$ 个可用位点中被乙酰化的赖氨酸残基总数。由于每次乙酰化事件使总电荷减少 $1$，总电荷变化为：\n$$\\Delta Q = K \\times (-1) = -K$$\n将此代入 $Q$ 的表达式中，得到：\n$$Q = Q_{0} - K$$\n我们需要求的是期望净尾部电荷 $\\mathbb{E}[Q]$。根据期望算子的线性性质：\n$$\\mathbb{E}[Q] = \\mathbb{E}[Q_{0} - K] = \\mathbb{E}[Q_{0}] - \\mathbb{E}[K]$$\n由于 $Q_{0}$ 是一个给定的常数值，其期望值就是它本身：$\\mathbb{E}[Q_{0}] = Q_{0}$。\n$$\\mathbb{E}[Q] = Q_{0} - \\mathbb{E}[K]$$\n下一步是确定 $\\mathbb{E}[K]$，即被乙酰化的赖氨酸的期望数量。问题陈述，在一个稳态群体中，乙酰化在 $N$ 个位点上均匀且独立地发生，其中有比例为 $f$ 的位点被乙酰化。这等价于说任何给定赖氨酸位点被乙酰化的概率为 $p = f$。\n\n我们定义 $N$ 个独立的伯努利随机变量 $X_{i}$（$i = 1, 2, \\dots, N$），其中如果第 $i$ 个赖氨酸被乙酰化，则 $X_{i} = 1$，否则 $X_{i} = 0$。成功（乙酰化）的概率为 $P(X_{i} = 1) = f$。\n被乙酰化的赖氨酸总数是这些随机变量之和：\n$$K = \\sum_{i=1}^{N} X_{i}$$\n$K$ 的期望值，根据期望的线性性质，为：\n$$\\mathbb{E}[K] = \\mathbb{E}\\left[\\sum_{i=1}^{N} X_{i}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[X_{i}]$$\n单个伯努利变量 $X_{i}$ 的期望值为：\n$$\\mathbb{E}[X_{i}] = 1 \\cdot P(X_{i} = 1) + 0 \\cdot P(X_{i} = 0) = 1 \\cdot f + 0 \\cdot (1-f) = f$$\n因此，被乙酰化的赖氨酸的期望总数为：\n$$\\mathbb{E}[K] = \\sum_{i=1}^{N} f = N \\cdot f$$\n将此结果代回 $\\mathbb{E}[Q]$ 的表达式，即可得到所需的通用公式：\n$$\\mathbb{E}[Q] = Q_{0} - Nf$$\n该表达式给出了期望净电荷作为初始电荷 $Q_{0}$、位点数 $N$ 和乙酰化比例 $f$ 的函数。\n\n最后，我们必须用给定值来计算该表达式：$Q_{0} = 36$，$N = 44$ 和 $f = 0.30$。\n$$\\mathbb{E}[Q] = 36 - (44)(0.30)$$\n计算过程很简单：\n$$(44)(0.30) = 13.2$$\n$$\\mathbb{E}[Q] = 36 - 13.2 = 22.8$$\n在指定条件下，组蛋白尾部的期望净电荷为 $22.8$。", "answer": "$$\\boxed{22.8}$$", "id": "2821692"}, {"introduction": "“组蛋白密码”的精髓在于修饰的“组合”而非单个标记。不同的效应蛋白复合物能够“读取”特定的修饰组合，从而执行激活或抑制等不同的调控功能。本练习将挑战您将这些复杂的生物学规则形式化为一个逻辑模型，通过构建一个真值表，来预测基于不同组蛋白修饰组合的细胞调控结果 [@problem_id:2642730]。这项实践旨在锻炼您对生物调控进行算法化思考的能力。", "problem": "一个多能干细胞中的基因座可以带有组蛋白H3尾部修饰的多种组合，这些组合由不同的效应复合物读取。请将以下广为接受的事实作为基础依据：(i) 组蛋白密码假说假定，组蛋白修饰的组合由模块化识别域解读，从而产生不同的调控输出；(ii) 组蛋白H3赖氨酸$27$位乙酰化 ($H_3K_{27}ac$) 和组蛋白H3赖氨酸$27$位三甲基化 ($H_3K_{27}me_3$) 不能同时占据同一个组蛋白尾上的同一个赖氨酸，但在一个调控窗口内（例如，一个跨越多个核小体的启动子或增强子区域），这两种标记可以存在于不同的组蛋白分子上，这种现象是二价性的基础；(iii) 一个包含植物同源结构域 (PHD) 指和溴结构域的共激活因子支架分别识别组蛋白H3赖氨酸$4$位三甲基化 ($H_3K_4me_3$) 和乙酰化赖氨酸，并且需要$H_3K_4me_3$和$H_3K_{27}ac$两者才能实现稳定的染色质结合；(iv) Polycomb抑制性复合物$1$ (PRC1) 的Chromobox (CBX) 染色质结构域识别$H_3K_{27}me_3$，而局部乙酰化会强烈拮抗PRC1的染色质压缩和占据，因为乙酰化会减少正电荷并促进开放的染色质状态；(v) 当两个大型复合物靶向重叠的DNA-组蛋白片段时，空间和拓扑限制会阻止它们同时稳定占据。\n\n将一个确定基因座内$H_3K_4me_3$、$H_3K_{27}ac$和$H_3K_{27}me_3$的存在与否分别建模为二元输入$x$、$y$和$z$，其中$x=1$表示存在$H_3K_4me_3$，$y=1$表示存在$H_3K_{27}ac$，$z=1$表示存在$H_3K_{27}me_3$（均在局部窗口内），$0$表示不存在。输出是二元变量$C$和$P$，分别表示PHD-溴结构域共激活因子复合物和PRC1的稳定招募（$1$）或不招募（$0$）。\n\n仅使用上述基本事实以及关于识别域-配体需求、乙酰化拮抗作用和空间竞争的第一性原理推理，构建针对所有$2^3 = 8$种输入组合的输出$C$和$P$的完整真值表。然后，选择一个选项，其对$C$和$P$的逻辑规范与此推理所推导的真值表完全匹配，并且为激活和抑制相关标记在基因座内共存的情况提供了一个机制上一致的冲突解决规则。\n\n哪个选项与基本事实最一致并能得出正确的真值表？\n\nA. $C = x \\land y \\land \\neg z$; $P = z \\land \\neg y$。冲突解决方法：窗口内的任何$H_3K_{27}me_3$都会阻止共激活因子招募；窗口内的任何$H_3K_{27}ac$都会阻止PRC1。\n\nB. $C = x \\land y$; $P = z \\land \\neg y$。冲突解决方法：共激活因子的招募仅取决于其两种对应标记的同时存在，并且对$H_3K_{27}me_3$不敏感；PRC1需要$H_3K_{27}me_3$，但受到$H_3K_{27}ac$的拮抗。\n\nC. $C = x \\land y$; $P = z$。冲突解决方法：如果所有三种标记都存在，共激活因子和PRC1可以共同占据。\n\nD. $C = x \\land y \\land \\neg z$; $P = z \\land \\neg (x \\lor y)$。冲突解决方法：任何激活标记（$H_3K_4me_3$或$H_3K_{27}ac$）都会阻断PRC1；任何$H_3K_{27}me_3$都会阻断共激活因子。", "solution": "该问题要求基于一组基本事实，为共激活因子复合物和Polycomb抑制性复合物$1$ (PRC$1$) 到染色质基因座的招募构建一个逻辑模型。基因座的状态由三个二元输入描述：$x$代表H3K4me3，$y$代表H3K27ac，$z$代表H3K27me3。输出是两个二元变量：$C$代表共激活因子招募，$P$代表PRC$1$招募。值$1$表示存在或招募，值$0$表示不存在或不招募。\n\n首先，我将把给定的事实形式化为控制输出$C$和$P$的逻辑规则。\n\n事实(iii)指出，共激活因子复合物“需要H3K4me3和H3K27ac两者才能实现稳定的染色质结合”。这直接转化为一个逻辑与运算。H3K4me3的存在（$x=1$）和H3K27ac的存在（$y=1$）是共激活因子招募的独立必要条件，也是联合充分条件，除非有其他指定的拮抗作用。\n因此，共激活因子招募的初步规则是：\n$$C = x \\land y$$\n\n事实(iv)为PRC$1$提供了两个条件。首先，PRC$1$的Chromobox (CBX) 染色质结构域“识别H3K27me3”。这意味着H3K27me3的存在（$z=1$）是PRC$1$招募的必要条件。\n其次，事实(iv)指出，PRC$1$的占据“受到局部乙酰化的强烈拮抗”。这意味着H3K27ac的存在（$y=1$）起到否决作用，阻止PRC$1$的稳定占据。\n综合这两点，PRC$1$的招募条件是当且仅当H3K27me3存在且H3K27ac不存在。\n因此，PRC$1$招募的规则是：\n$$P = z \\land \\neg y$$\n\n事实(v)指出，空间和拓扑限制“会阻止它们同时稳定占据”。这给系统施加了一个全局约束：我们不能同时拥有$C=1$和$P=1$。这可以表示为：\n$$\\neg (C \\land P) \\equiv \\text{True}$$\n让我们验证我们推导出的$C$和$P$的规则是否满足这个约束。我们代入$C$和$P$的表达式：\n$$(x \\land y) \\land (z \\land \\neg y)$$\n使用逻辑合取的交换律和结合律：\n$$x \\land z \\land (y \\land \\neg y)$$\n项$(y \\land \\neg y)$恒为假（无矛盾律）。因此，整个表达式恒为假。\n$$C \\land P \\equiv \\text{False}$$\n这意味着，根据推导出的规则，$C$和$P$是互斥的结果。它们的招募条件永远不能同时得到满足。这自动满足了事实(v)中的空间位阻约束，而无需引入额外的冲突解决规则。事实(iv)中描述的拮抗作用是关键的内置冲突解决机制。\n\n现在，我们将使用推导出的逻辑表达式$C = x \\land y$和$P = z \\land \\neg y$为所有$2^3 = 8$种输入组合$(x, y, z)$构建完整的真值表。\n\n| 输入 ($x, y, z$) | $C = x \\land y$ | $P = z \\land \\neg y$ |\n|---|---|---|\n| ($0, 0, 0$) | $0 \\land 0 = 0$ | $0 \\land \\neg 0 = 0$ |\n| ($0, 0, 1$) | $0 \\land 0 = 0$ | $1 \\land \\neg 0 = 1$ |\n| ($0, 1, 0$) | $0 \\land 1 = 0$ | $0 \\land \\neg 1 = 0$ |\n| ($0, 1, 1$) | $0 \\land 1 = 0$ | $1 \\land \\neg 1 = 0$ |\n| ($1, 0, 0$) | $1 \\land 0 = 0$ | $0 \\land \\neg 0 = 0$ |\n| ($1, 0, 1$) | $1 \\land 0 = 0$ | $1 \\land \\neg 0 = 1$ |\n| ($1, 1, 0$) | $1 \\land 1 = 1$ | $0 \\land \\neg 1 = 0$ |\n| ($1, 1, 1$) | $1 \\land 1 = 1$ | $1 \\land \\neg 1 = 0$ |\n\n关键的“冲突”案例是所有三种标记都存在的二价状态：$(x, y, z) = (1, 1, 1)$。在这种情况下，共激活因子的要求（$x=1, y=1$）得到满足，因此$C=1$。PRC$1$的主要配体存在（$z=1$），但其占据受到乙酰化（$y=1$）的拮抗，因此$P=0$。由于H3K27ac对PRC$1$的明确拮抗作用，冲突被解决，有利于共激活因子复合物。\n\n我们现在根据我们推导的逻辑和真值表评估给定的选项。\n\nA. $C = x \\land y \\land \\neg z$; $P = z \\land \\neg y$。\n$P$的表达式与我们的推导相符。然而，$C$的表达式引入了一个新条件$\\neg z$，意味着H3K27me3拮抗共激活因子复合物。这在基本事实中没有陈述。事实(iii)给出了共激活因子结合的要求，但没有提到任何被H3K27me3抑制的作用。该选项武断地增加了一条抑制规则。\n结论：**不正确**。\n\nB. $C = x \\land y$; $P = z \\land \\neg y$。\n两个逻辑表达式都与我们从基本事实推导出的结果完全匹配。所附的解释“共激活因子的招募仅取决于其两种对应标记的同时存在，并且对H3K27me3不敏感；PRC1需要H3K27me3，但受到H3K27ac的拮抗”完美地总结了推理过程。该模型是自洽的，并严格遵守了前提。\n结论：**正确**。\n\nC. $C = x \\land y$; $P = z$。\n$P$的表达式不正确。它忽略了事实(iv)中明确指出的PRC$1$占据“受到局部乙酰化的强烈拮抗”。这将导致输入为$(x, y, z) = (1, 1, 1)$时，状态为$(C, P) = (1, 1)$，这直接违反了事实(v)中的空间位阻约束。该选项声称复合物可以共同占据，这与事实(v)直接矛盾。\n结论：**不正确**。\n\nD. $C = x \\land y \\land \\neg z$; $P = z \\land \\neg (x \\lor y)$。\n两个表达式都与对前提的严格解读不一致。与选项A一样，$C$的表达式为H3K27me3增加了一个未经证实的抑制作用。$P$的表达式，等同于$P = z \\land \\neg x \\land \\neg y$，为H3K4me3 ($x$) 对PRC$1$招募增加了一个抑制作用。事实(iv)只指明了乙酰化($y$)的拮抗作用，而不是H3K4me3 ($x$)。提出“任何激活标记...都会阻断PRC1”是一种过度解读，没有得到给定事实的支持。\n结论：**不正确**。", "answer": "$$\\boxed{B}$$", "id": "2642730"}, {"introduction": "现代基因组学研究产生了海量的组蛋白修饰图谱数据，如何从中解读出有生物学意义的染色质状态是一个核心挑战。启动子、增强子和Polycomb抑制区域等功能元件通常具有特征性的组蛋白修饰模式，例如启动子区域富集$H_3K_4me_3$和$H_3K_{27}ac$。本练习让您扮演一名生物信息学家的角色，通过实现一个高斯朴素贝叶斯（Gaussian Naive Bayes）分类器，从ChIP-seq数据中自动注释染色质状态 [@problem_id:2821688]。这是一项将理论模型与真实世界数据分析相结合的关键技能训练。", "problem": "您将获得来自染色质免疫沉淀测序（ChIP-seq）的三种组蛋白修饰：$H_3K_4me_3$、$H_3K_{27}ac$ 和 $H_3K_{27}me_3$ 在基因组区域上的归一化信号强度。每个区域表示为 $\\mathbb{R}^3$ 中的一个实值向量，其分量分别对应 $H_3K_4me_3$、$H_3K_{27}ac$ 和 $H_3K_{27}me_3$。其生物学前提（普遍观察到且经过充分检验）是：启动子区域的特征是高 $H_3K_4me_3$、中到高 $H_3K_{27}ac$ 和低 $H_3K_{27}me_3$；增强子区域的特征是低 $H_3K_4me_3$、高 $H_3K_{27}ac$ 和低 $H_3K_{27}me_3$；而多梳抑制区域的特征是高 $H_3K_{27}me_3$、低 $H_3K_4me_3$ 和 $H_3K_{27}ac$。您的任务是使用基于贝叶斯定理的有原则的贝叶斯方法，将区域分类为启动子、增强子或多梳抑制状态，并将其形式化。\n\n从贝叶斯定理和给定类别下特征独立的假设（即，在给定染色质状态下每个组蛋白标记的条件独立性）出发，实现一个分类器，该分类器将每个组蛋白标记视为具有类别条件高斯分布的连续随机变量。使用以下基本基础：\n- 贝叶斯定理：$P(C \\mid \\mathbf{x}) \\propto P(C)\\,P(\\mathbf{x}\\mid C)$。\n- 朴素条件独立性：$P(\\mathbf{x}\\mid C) = \\prod_{j=1}^{3} P(x_j \\mid C)$。\n- 每个特征的高斯似然：$P(x_j \\mid C) = \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j})$，其中 $\\mu_{C,j}$和$\\sigma^2_{C,j}$是类别$C$的特征$j$的类别条件均值和方差。\n- 对每个具有 $n_C$ 个训练样本 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^{n_C}$ 的类别 $C$ 进行最大似然估计：$\\mu_{C,j} = \\frac{1}{n_C}\\sum_{i=1}^{n_C} x^{(i)}_j$ 和 $\\sigma^2_{C,j} = \\frac{1}{n_C}\\sum_{i=1}^{n_C} \\left(x^{(i)}_j - \\mu_{C,j}\\right)^2$。\n- 经验类别先验：$P(C) = \\frac{n_C}{N}$，其中$N$是所有类别的训练样本总数。\n- 数值稳定性要求：在对数域中计算，使用 $\\log P(\\mathbf{x}\\mid C) = \\sum_{j=1}^{3} \\left[-\\frac{1}{2}\\log\\left(2\\pi\\sigma^2_{C,j}\\right) - \\frac{(x_j - \\mu_{C,j})^2}{2\\sigma^2_{C,j}}\\right]$，并将方差正则化为 $\\sigma^2_{C,j} \\leftarrow \\sigma^2_{C,j} + \\epsilon$，其中 $\\epsilon = 10^{-6}$ 以避免零方差。如果后验概率出现完全相等的情况，则选择索引最小的类别。\n\n类别编码为整数：启动子 $\\rightarrow 0$，增强子 $\\rightarrow 1$，多梳抑制 $\\rightarrow 2$。\n\n实现这个高斯朴素贝叶斯（GNB）分类器，并评估分类准确率，准确率定义为$\\text{accuracy} = \\frac{\\text{正确分类的测试样本数}}{\\text{测试样本总数}}$，以小数表示。您的程序必须计算以下三个测试用例的准确率，每个用例都有指定的训练集和测试集。不要引入任何随机性。\n\n测试用例 $1$（类别条件结构良好分离）：\n- 训练集 $\\mathbf{X}_{\\text{train}}$（行为样本，列为 [$H_3K_4me_3$, $H_3K_{27}ac$, $H_3K_{27}me_3$]）：\n  - 启动子 ($y=0$): $[10.0, 4.0, 0.5]$, $[9.5, 3.8, 0.4]$, $[10.2, 4.1, 0.6]$, $[9.8, 4.2, 0.5]$, $[10.1, 3.9, 0.5]$\n  - 增强子 ($y=1$): $[1.0, 8.5, 0.5]$, $[0.9, 8.8, 0.6]$, $[1.2, 8.9, 0.4]$, $[1.1, 8.6, 0.5]$, $[0.8, 8.7, 0.5]$\n  - 多梳抑制 ($y=2$): $[0.5, 0.6, 9.5]$, $[0.4, 0.5, 9.8]$, $[0.6, 0.5, 9.7]$, $[0.5, 0.4, 9.6]$, $[0.6, 0.6, 9.9]$\n- 测试集 $\\mathbf{X}_{\\text{test}}$ 及标签 $\\mathbf{y}_{\\text{test}}$:\n  - $[9.9, 4.0, 0.5]\\rightarrow 0$, $[10.3, 4.1, 0.7]\\rightarrow 0$, $[1.0, 8.7, 0.5]\\rightarrow 1$, $[1.3, 8.4, 0.6]\\rightarrow 1$, $[0.5, 0.5, 9.6]\\rightarrow 2$, $[0.7, 0.7, 9.8]\\rightarrow 2$\n\n测试用例 $2$（启动子和增强子在 H3K27ac 上重叠；分离依赖于 H3K4me3）：\n- 训练集:\n  - 启动子 ($y=0$): $[5.5, 7.5, 1.0]$, $[5.8, 7.8, 1.2]$, $[5.2, 7.2, 0.8]$, $[5.6, 7.6, 1.1]$\n  - 增强子 ($y=1$): $[1.2, 7.7, 1.0]$, $[1.0, 7.5, 1.1]$, $[1.5, 7.9, 0.9]$, $[1.3, 7.6, 1.2]$\n  - 多梳抑制 ($y=2$): $[1.0, 1.5, 7.8]$, $[0.8, 1.2, 8.0]$, $[1.2, 1.3, 7.6]$, $[1.1, 1.4, 8.2]$\n- 测试集及标签:\n  - $[5.4, 7.4, 1.0]\\rightarrow 0$, $[1.4, 7.4, 1.1]\\rightarrow 1$, $[1.0, 1.3, 8.1]\\rightarrow 2$, $[3.2, 7.5, 1.0]\\rightarrow 1$\n\n测试用例 $3$（需要正则化的退化方差）：\n- 训练集:\n  - 启动子 ($y=0$): $[2.0, 4.0, 0.5]$, $[2.0, 4.0, 0.5]$, $[2.0, 4.0, 0.5]$\n  - 增强子 ($y=1$): $[1.8, 4.0, 0.4]$, $[1.8, 4.0, 0.4]$, $[1.8, 4.0, 0.4]$\n  - 多梳抑制 ($y=2$): $[0.2, 0.2, 6.0]$, $[0.2, 0.2, 6.0]$, $[0.2, 0.2, 6.0]$\n- 测试集及标签:\n  - $[1.9, 4.0, 0.45]\\rightarrow 0$, $[1.8, 4.0, 0.4]\\rightarrow 1$, $[0.2, 0.2, 6.0]\\rightarrow 2$\n\n您的程序必须：\n- 实现训练以估计 $\\mu_{C,j}$、$\\sigma^2_{C,j}$ 和 $P(C)$。\n- 通过最大化 $\\log P(C) + \\sum_{j=1}^{3} \\log \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j} + \\epsilon)$（其中 $\\epsilon = 10^{-6}$）来实现预测。\n- 以小数形式计算每个测试用例的准确率。\n\n最终输出格式：\n- 生成单行输出，包含测试用例 1、测试用例 2 和测试用例 3 的准确率，按此顺序排列，形式为用方括号括起来的逗号分隔列表，每个值四舍五入到恰好 $4$ 位小数，例如，该字符串由一个左方括号、三个用逗号分隔的四舍五入值 a、b、c 和一个右方括号组成：[a,b,c]。", "solution": "该问题要求制定并实现一个高斯朴素贝叶斯（GNB）分类器，用以将基因组区域划分为三种染色质状态之一：启动子（类别 0）、增强子（类别 1）或多梳抑制（类别 2）。分类基于一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^3$，该向量代表三种组蛋白修饰的信号强度：$H_3K_4me_3$ ($x_1$)、$H_3K_{27}ac$ ($x_2$) 和 $H_3K_{27}me_3$ ($x_3$) 。\n\n该问题具有充分的科学依据和严谨的数学表述，并为获得唯一解提供了所有必要信息。其生物学前提是可靠的，而指定的统计模型是解决此类分类任务的标准方法。为保证数值稳定性所作的规定对于稳健的实现至关重要。因此，该问题是有效的，我们将继续推导并实现解决方案。\n\n解决方案的核心是贝叶斯定理，它提供了一个规则，用于在给定观测数据 $\\mathbf{x}$ 的情况下，更新我们对类别 $C$ 的置信度：\n$$\nP(C \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid C) P(C)}{P(\\mathbf{x})}\n$$\n对于分类问题，我们寻求能使后验概率 $P(C \\mid \\mathbf{x})$ 最大化的类别 $\\hat{C}$。由于对于给定的数据点 $\\mathbf{x}$，$P(\\mathbf{x})$对所有类别都是常数，因此这等价于最大化类别条件似然 $P(\\mathbf{x} \\mid C)$ 和类别先验 $P(C)$ 的乘积：\n$$\n\\hat{C} = \\arg\\max_{C} P(\\mathbf{x} \\mid C) P(C)\n$$\n朴素贝叶斯分类器的“朴素”假设是，在给定类别 $C$ 的条件下，特征 $x_j$ 是条件独立的。这简化了似然项：\n$$\nP(\\mathbf{x} \\mid C) = \\prod_{j=1}^{3} P(x_j \\mid C)\n$$\n每个特征的类别条件分布 $P(x_j \\mid C)$ 被建模为高斯（正态）分布 $\\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j})$，其中 $\\mu_{C,j}$ 和 $\\sigma^2_{C,j}$ 分别是类别 $C$ 中特征 $j$ 的均值和方差。\n\n为了避免因概率过小而导致的数值下溢，并为计算方便，我们使用后验概率的对数形式。决策规则变为：\n$$\n\\hat{C} = \\arg\\max_{C} \\left[ \\log P(C) + \\sum_{j=1}^{3} \\log P(x_j \\mid C) \\right]\n$$\n这个表达式通常被称为判别函数 $g_C(\\mathbf{x})$。\n\n训练阶段包括从带标签的训练集中估计模型参数。\n设类别 $C$ 的训练集为 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^{n_C}$，其中 $n_C$ 是类别 $C$ 中的样本数。设 $N$ 为训练样本总数。\n参数通过最大似然估计（MLE）进行估计：\n\n1.  **类别先验**，$P(C)$：训练数据中每个类别的经验频率。\n    $$\n    P(C) = \\frac{n_C}{N}\n    $$\n\n2.  **类别条件均值**，$\\mu_{C,j}$：属于类别 $C$ 的所有训练样本中特征 $j$ 的样本均值。\n    $$\n    \\mu_{C,j} = \\frac{1}{n_C} \\sum_{i=1}^{n_C} x^{(i)}_j\n    $$\n\n3.  **类别条件方差**，$\\sigma^2_{C,j}$：属于类别 $C$ 的所有训练样本中特征 $j$ 的样本方差。这是方差的有偏估计量，与最大似然估计的公式一致。\n    $$\n    \\sigma^2_{C,j} = \\frac{1}{n_C} \\sum_{i=1}^{n_C} (x^{(i)}_j - \\mu_{C,j})^2\n    $$\n\n对于预测，我们使用这些估计出的参数来为新的数据点 $\\mathbf{x}$ 计算判别函数 $g_C(\\mathbf{x})$。高斯概率密度函数的对数公式为：\n$$\n\\log P(x_j \\mid C) = \\log \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j}) = -\\frac{1}{2} \\log(2\\pi\\sigma^2_{C,j}) - \\frac{(x_j - \\mu_{C,j})^2}{2\\sigma^2_{C,j}}\n$$\n一个关键的实现细节是处理零方差，这种情况可能在某个类别的所有训练样本对某个特征具有相同值时发生。这会导致除以零或对零取对数。为防止这种情况，将一个小的正则化常数 $\\epsilon = 10^{-6}$ 添加到每个估计的方差中：\n$$\n\\sigma^2_{C,j, \\text{reg}} = \\sigma^2_{C,j} + \\epsilon\n$$\n需要最大化的最终判别函数是：\n$$\ng_C(\\mathbf{x}) = \\log P(C) + \\sum_{j=1}^{3} \\left[ -\\frac{1}{2} \\log(2\\pi \\sigma^2_{C,j, \\text{reg}}) - \\frac{(x_j - \\mu_{C,j})^2}{2 \\sigma^2_{C,j, \\text{reg}}} \\right]\n$$\n对于给定的 $\\mathbf{x}$，预测类别 $\\hat{C}$ 是使 $g_C(\\mathbf{x})$ 值最高的类别。如果出现平局，则选择整数索引最小的类别。\n\n最后，使用准确率来评估分类器的性能，准确率定义为测试集中正确分类样本的比例：\n$$\n\\text{accuracy} = \\frac{\\sum_{i=1}^{m} I(\\hat{y}_i = y_i)}{m}\n$$\n其中 $m$ 是测试样本数， $y_i$ 是真实标签，$\\hat{y}_i$ 是第 $i$ 个测试样本的预测标签， $I(\\cdot)$ 是指示函数。\n\n实现将包含一个封装此逻辑的类。`fit` 方法将从训练数据中估计参数，而 `predict` 方法将使用这些参数对新的数据点进行分类。我们将通过在相应的训练集上训练模型，并在其测试集上评估准确率来处理每个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Gaussian Naive Bayes classifier and evaluates its accuracy\n    on three provided test cases related to chromatin state classification.\n    \"\"\"\n\n    class GaussianNaiveBayes:\n        \"\"\"\n        A Gaussian Naive Bayes classifier.\n\n        Parameters are estimated using Maximum Likelihood Estimation.\n        \"\"\"\n        def __init__(self, var_smoothing=1e-6):\n            self.var_smoothing = var_smoothing\n            self.class_priors_ = None\n            self.means_ = None\n            self.variances_ = None\n            self.classes_ = None\n\n        def fit(self, X, y):\n            \"\"\"\n            Train the classifier by estimating parameters from the data.\n\n            Args:\n                X (np.ndarray): Training data of shape (n_samples, n_features).\n                y (np.ndarray): Target values of shape (n_samples,).\n            \"\"\"\n            self.classes_ = np.unique(y)\n            n_samples, n_features = X.shape\n            n_classes = len(self.classes_)\n\n            self.means_ = np.zeros((n_classes, n_features))\n            self.variances_ = np.zeros((n_classes, n_features))\n            self.class_priors_ = np.zeros(n_classes)\n\n            for idx, c in enumerate(self.classes_):\n                X_c = X[y == c]\n                self.means_[idx, :] = X_c.mean(axis=0)\n                # The problem uses the MLE for variance (biased estimator, N in denominator)\n                # np.var calculates this by default (ddof=0)\n                self.variances_[idx, :] = X_c.var(axis=0)\n                self.class_priors_[idx] = X_c.shape[0] / float(n_samples)\n\n        def predict(self, X):\n            \"\"\"\n            Perform classification on an array of test vectors X.\n\n            Args:\n                X (np.ndarray): Test data of shape (n_samples, n_features).\n\n            Returns:\n                np.ndarray: Predicted class labels for each sample in X.\n            \"\"\"\n            # Add variance smoothing for numerical stability\n            vars_reg = self.variances_ + self.var_smoothing\n\n            log_priors = np.log(self.class_priors_)\n\n            # The log posterior is proportional to log_prior + log_likelihood\n            # log_likelihood for a Gaussian is sum over features of:\n            # -0.5 * log(2*pi*var) - 0.5 * ((x-mu)^2 / var)\n            \n            log_posteriors = []\n            for x_sample in X:\n                joint_log_likelihood = []\n                for i in range(len(self.classes_)):\n                    log_likelihood_class = -0.5 * np.sum(np.log(2. * np.pi * vars_reg[i, :]))\n                    log_likelihood_class -= 0.5 * np.sum(((x_sample - self.means_[i, :]) ** 2) / vars_reg[i, :])\n                    joint_log_likelihood.append(log_priors[i] + log_likelihood_class)\n                log_posteriors.append(joint_log_likelihood)\n\n            # The class with the highest log posterior is the prediction.\n            # np.argmax handles ties by returning the first index, which matches\n            # the problem's tie-breaking rule (smallest class index).\n            predictions = np.argmax(log_posteriors, axis=1)\n            return self.classes_[predictions]\n            \n    # Define test cases from the problem statement\n    test_cases = [\n        {\n            \"train_set\": {\n                0: np.array([[10.0, 4.0, 0.5], [9.5, 3.8, 0.4], [10.2, 4.1, 0.6], [9.8, 4.2, 0.5], [10.1, 3.9, 0.5]]),\n                1: np.array([[1.0, 8.5, 0.5], [0.9, 8.8, 0.6], [1.2, 8.9, 0.4], [1.1, 8.6, 0.5], [0.8, 8.7, 0.5]]),\n                2: np.array([[0.5, 0.6, 9.5], [0.4, 0.5, 9.8], [0.6, 0.5, 9.7], [0.5, 0.4, 9.6], [0.6, 0.6, 9.9]]),\n            },\n            \"test_set\": np.array([\n                [9.9, 4.0, 0.5], [10.3, 4.1, 0.7], \n                [1.0, 8.7, 0.5], [1.3, 8.4, 0.6], \n                [0.5, 0.5, 9.6], [0.7, 0.7, 9.8]\n            ]),\n            \"test_labels\": np.array([0, 0, 1, 1, 2, 2]),\n        },\n        {\n            \"train_set\": {\n                0: np.array([[5.5, 7.5, 1.0], [5.8, 7.8, 1.2], [5.2, 7.2, 0.8], [5.6, 7.6, 1.1]]),\n                1: np.array([[1.2, 7.7, 1.0], [1.0, 7.5, 1.1], [1.5, 7.9, 0.9], [1.3, 7.6, 1.2]]),\n                2: np.array([[1.0, 1.5, 7.8], [0.8, 1.2, 8.0], [1.2, 1.3, 7.6], [1.1, 1.4, 8.2]]),\n            },\n            \"test_set\": np.array([\n                [5.4, 7.4, 1.0], [1.4, 7.4, 1.1], \n                [1.0, 1.3, 8.1], [3.2, 7.5, 1.0]\n            ]),\n            \"test_labels\": np.array([0, 1, 2, 1]),\n        },\n        {\n            \"train_set\": {\n                0: np.array([[2.0, 4.0, 0.5], [2.0, 4.0, 0.5], [2.0, 4.0, 0.5]]),\n                1: np.array([[1.8, 4.0, 0.4], [1.8, 4.0, 0.4], [1.8, 4.0, 0.4]]),\n                2: np.array([[0.2, 0.2, 6.0], [0.2, 0.2, 6.0], [0.2, 0.2, 6.0]]),\n            },\n            \"test_set\": np.array([\n                [1.9, 4.0, 0.45], [1.8, 4.0, 0.4], [0.2, 0.2, 6.0]\n            ]),\n            \"test_labels\": np.array([0, 1, 2]),\n        },\n    ]\n\n    accuracies = []\n    \n    for case in test_cases:\n        # Prepare training data\n        X_train_list, y_train_list = [], []\n        for label, data in case[\"train_set\"].items():\n            X_train_list.append(data)\n            y_train_list.extend([label] * data.shape[0])\n        \n        X_train = np.vstack(X_train_list)\n        y_train = np.array(y_train_list)\n        \n        # Prepare test data\n        X_test = case[\"test_set\"]\n        y_test = case[\"test_labels\"]\n        \n        # Initialize and train the classifier\n        gnb = GaussianNaiveBayes(var_smoothing=1e-6)\n        gnb.fit(X_train, y_train)\n        \n        # Make predictions\n        y_pred = gnb.predict(X_test)\n        \n        # Calculate accuracy\n        accuracy = np.mean(y_pred == y_test)\n        accuracies.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{acc:.4f}' for acc in accuracies])}]\")\n\nsolve()\n```", "id": "2821688"}]}