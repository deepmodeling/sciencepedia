{"hands_on_practices": [{"introduction": "汉密尔顿法则（Hamilton's rule）是解释社会行为演化的基石，它将利他主义等行为的最终演化结果与个体间的遗传相关性（一个关键的近因变量）联系起来。理解这一法则不仅需要掌握 $rB > C$ 的概念，还需要思考如何在真实的生物世界中衡量其组成部分。本练习旨在通过一个关于合作繁殖鸟类的假想场景，让你将理论付诸实践，计算利他行为得以演化的亲缘关系阈值，并构思在野外种群中估算相关系数的实际方法学。", "problem": "在一种社会性单配制、合作繁殖的鸟类中，个体偶尔会发出响亮的警报叫声以吸引捕食者的注意。野外测量表明，每一次叫声都会给呼叫者带来大小为 $c = 0.1$ 的直接适合度成本（通过增加死亡风险和损失觅食时间），同时给附近的一个接收者带来大小为 $b = 0.2$ 的平均净适合度收益（通过提高存活率）。假设该行为处于弱选择压力下，不存在直接互惠，并且在选择的短时间尺度上，种群动态反馈可以忽略不计。利用广义适合度理论框架，确定当自然选择倾向于警报叫声行为在演化上增加时，行为者与接收者之间所需的最小遗传相关系数 $r$。请提供精确数值，无需四舍五入。\n\n另外，请你设计一种基于谱系的方法，用于评估该种群中自由生活的个体之间的配对相关系数 $r$。你可获取无创DNA样本，并能在多个高度多态性的短串联重复序列（STR）位点和一组单核苷酸多态性（SNP）标记上对个体进行基因分型。请概述一个科学合理的、分步骤的流程，该流程能得出一个基于孟德尔遗传和谱系结构的 $r$ 估计量，适用于一个可能存在不完全取样和亲子鉴定不确定性的野生种群。你的概述应明确说明将谱系转化为 $r$ 所使用的基本原理，以及你将如何处理谱系重建中的不确定性。在你的最终答案中，仅需报告 $r$ 的数值阈值。", "solution": "所给出的问题陈述具有科学依据，问题明确且客观。它包含两部分。第一部分是亲缘选择理论中汉密尔顿法则的一个标准应用。第二部分要求概述一种使用现代遗传技术估算野生种群遗传相关系数的方法。所有参数和条件都定义清晰，并与行为生态学和种群遗传学的既定原则相符。因此，该问题是有效的，并允许进行严谨的求解。\n\n问题的第一部分要求计算自然选择偏好一种利他行为时所需的最小遗传相关系数 $r$。这种性状的演化动态由汉密尔顿法则决定，这是广义适合度理论的核心原则。该法则指出，当行为者付出的适合度成本，小于接收者获得的、并根据两者之间的遗传相关系数进行折算的适合度收益时，控制社会行为的等位基因频率将会增加。\n\n汉密尔顿法则的经典形式由以下不等式给出：\n$$ rB > C $$\n在这里，$C$ 代表行为执行者（行为者）的适合度成本，$B$ 代表行为接受者（接收者）的适合度收益，$r$ 是行为者与接收者之间的遗传相关系数。系数 $r$ 定义为：在同一位点上，从行为者和接收者体内随机抽取的两个基因拷贝是同源相同的概率。\n\n问题提供了以下参数，其符号与经典公式略有不同，我们在此进行对应。呼叫者（行为者）的适合度成本为 $c = 0.1$。接收者的适合度收益为 $b = 0.2$。将这些值代入经典不等式，我们得到 $C=c$ 和 $B=b$。\n\n因此，警报叫声行为受自然选择偏好的条件是：\n$$ rb > c $$\n为了求得所需的最小相关系数，我们求解当广义适合度收益等于零时的 $r$，这正是该不等式的边界条件。\n$$ r > \\frac{c}{b} $$\n我们已知数值 $c = 0.1$ 和 $b = 0.2$。将这些值代入不等式，得到：\n$$ r > \\frac{0.1}{0.2} $$\n$$ r > 0.5 $$\n因此，在这些条件下，为了使警报叫声性状在演化上受到偏好，呼叫者与接收者之间的遗传相关系数必须大于 $0.5$。最小值是该条件的临界值。例如，在二倍体、远交种群中，全同胞的相关系数为 $r = 0.5$。在这种特定情况下，对于全同胞而言，该行为处于选择平衡点。为了使该性状的频率主动增加，$r$ 必须严格大于 $0.5$。这种情况可能发生在存在一定程度近交的系统中，或者在单倍二倍性系统中亲代与子代之间。问题要求的是选择*偏好*该行为增加所需的最小值，这指的是由不等式定义的阈值。\n\n问题的第二部分要求设计一种基于谱系的方法来估算配对相关系数 $r$。这是一个分子生态学中的方法论问题。一个科学合理的流程如下：\n\n1.  **野外取样与DNA提取**：从种群中尽可能多的个体收集无创遗传样本（例如，脱落的羽毛、粪便或从捕获鸟类获得的颊拭子），并跨越多年以捕获代际联系。从这些样本中提取高质量的DNA。\n\n2.  **多位点基因分型**：在大量高度多态性的微卫星（STR）位点和密集的单核苷酸多态性（SNP）标记组上，对所有个体进行基因分型。STR的高杂合度和大量的SNP为区分个体和建立亲子关系提供了强大的统计能力。\n\n3.  **亲子鉴定与谱系重建**：使用基因型数据重建社会和遗传谱系。这通常通过在专用软件（例如，CERVUS、COLONY）中实现的最大似然法或贝叶斯方法来完成。\n    *   对于每个子代，软件会将其基因型与所有潜在亲本（雄性为父本，雌性为母本）的基因型进行比较。\n    *   为每个候选亲本计算似然比，并根据统计置信度（例如，在 $95\\%$ 的置信水平下）分配亲本关系。社会性单配制的假设可以用作先验信息，但必须验证遗传亲子关系，因为婚外亲子关系在许多鸟类中很常见。\n    *   通过跨代连接这些亲子二元组，构建一个多代谱系。\n\n4.  **从谱系计算 `r`**：一旦建立了最可能的谱系，就可以根据孟德尔遗传的原理，使用路径分析法计算任意两个个体 $X$ 和 $Y$ 之间的配对相关系数 $r_{XY}$。该算法包括识别 $X$ 和 $Y$ 的所有共同祖先，并追踪他们之间的谱系路径。\n    *   通过一个共同祖先连接 $X$ 和 $Y$ 的单条路径的贡献为 $(\\frac{1}{2})^{n}$，其中 $n$ 是从 $X$ 上溯到共同祖先再下溯到 $Y$ 的路径中的步数（代际连接数）。\n    *   总相关系数是所有这些路径贡献的总和：\n    $$ r_{XY} = \\sum_{i} \\left( \\frac{1}{2} \\right)^{n_i} $$\n    *   如果任何共同祖先 $A$ 本身是近交的，其近交系数为 $F_A$，那么它对该路径的贡献将增加一个因子 $(1 + F_A)$。近交系数 $F_A$ 是通过识别连接 $A$ 的双亲并通过其共同祖先的路径来计算的。该计算对谱系中的所有个体递归执行。\n\n5.  **处理不确定性**：来自野生种群的真实世界谱系几乎总是不完整的（由于未取样个体、迁入等原因），并且可能包含错误（错误的亲本分配）。\n    *   **不完全取样**：谱系中的未取样奠基者被假设为彼此不相关 ($r=0$) 且非近交 ($F=0$)，这可能导致对相关系数的系统性低估。缓解这种情况的唯一方法是进行密集的、长期的取样。\n    *   **不确定的亲子关系**：当亲本分配是概率性的而非确定性的时候，可以考虑这种不确定性。可以不使用单一谱系，而是生成一个可能谱系的后验分布，其中每个谱系都按其似然性加权。然后，一对个体间的相关系数 $r$ 可以作为在所有可能的谱系配置上平均的期望值来报告。或者，也可以使用基于标记的估计量（直接从等位基因共享来估计相关系数）来验证或补充基于谱系的估计，尽管题目特别要求使用基于谱系的方法。\n\n以上概述了所要求的完整流程。然而，最终答案必须仅包含 $r$ 的数值阈值。", "answer": "$$\n\\boxed{0.5}\n$$", "id": "2778837"}, {"introduction": "最优觅食理论，特别是边际价值定理（Marginal Value Theorem, MVT），是预测动物如何做出觅食决策的强大框架，它完美地体现了终极目标（最大化长期能量收益率）如何塑造近因决策规则（何时离开一个资源斑块）。要真正掌握 MVT 的精髓，就需要超越对其图形化解释的理解，深入其数学根源。本练习将引导你从第一性原理出发，推导 MVT 的核心等式，并将其应用于一个具体的收益函数以求解最优停留时间，这对于培养定量分析和解决理论生态学问题的能力至关重要。", "problem": "行为生态学的一个核心问题是，自然选择如何塑造决策规则以最大化与适应度相关的指标。考虑一个觅食者遇到相同的资源斑块，其在斑块中停留时间 $t$ 的累积能量收益由一个收益函数 $G(t)$ 给出，该函数满足 $G(0)=0$，$G'(t)>0$ 以及对于所有 $t>0$ 都有 $G''(t)<0$。在斑块之间，觅食者需要固定的移动时间 $T \\ge 0$。假设在相关的生态时间尺度上，Darwinian适应度随长期平均能量摄入率单调增加，且环境是平稳和遍历的，因此时间平均值等于重复斑块循环的系综平均值。\n\n- 从第一性原理和长期平均速率（定义为重复斑块循环中总能量收益除以总时间）出发，推导使长期平均速率最大化的斑块停留时间 $t^{*}$ 所需的最优性必要条件。你的推导不能引用任何预先陈述的觅食规则。请明确说明为证明最优性所需的关于 $G(t)$ 的任何正则性条件。\n\n- 接着，对于一个特定形式的收益函数 $G(t) = a \\left(1 - \\exp(-b t)\\right)$（其中参数 $a>0$ 和 $b>0$）和固定的移动时间 $T \\ge 0$，计算最优停留时间 $t^{*}$ 关于 $a$, $b$, 和 $T$ 的显式闭合形式表达式。如果需要，你可以使用特殊函数，但必须清楚地指明得到生物学上可接受的解 $t^{*} \\ge 0$ 的是哪个分支。\n\n将最终的 $t^{*}$ 表示为单个解析表达式。不需要数值近似或四舍五入。时间单位应与 $t$ 和 $T$ 使用的单位相同。", "solution": "首先对问题进行严格验证。\n\n已知条件：\n$1$. 斑块中的能量收益函数为 $G(t)$，其中 $t$ 是在斑块中花费的时间。\n$2$. 收益函数的性质：$G(0)=0$；对于 $t > 0$，$G'(t) > 0$；对于 $t > 0$，$G''(t) < 0$。\n$3$. 斑块之间存在固定的移动时间 $T \\ge 0$。\n$4$. 目标是最大化长期平均能量摄入率。\n$5$. 适应度是该速率的单调递增函数。\n$6$. 环境是平稳和遍历的。\n$7. a.$ 第一个任务：推导斑块停留时间 $t^*$ 的最优性必要条件。\n$7. b.$ 第二个任务：对于 $G(t) = a(1 - \\exp(-bt))$，其中 $a > 0$ 且 $b > 0$，计算 $t^*$ 的闭合形式表达式。\n\n验证：\n该问题具有科学依据，是最优觅食理论中边际价值定理的经典表述，而该定理是行为生态学的一个基本概念。关于 $G(t)$ 的假设（单调递增且收益递减）是标准的，并代表了资源耗尽的现实情景。该问题是适定的，有明确定义的目标函数和指定的约束条件。语言客观且数学上精确。问题设定是自洽的，没有矛盾。$G(t)$ 的具体函数形式是一种在生物学建模中广泛使用的标准饱和函数。该问题并非微不足道，需要严格应用微积分和对特殊函数的理解。\n\n结论：问题有效。\n\n我们开始求解。\n\n第1部分：一般最优性条件的推导。\n\n设 $t$ 为在单个斑块中觅食所花费的时间，我们称之为斑块停留时间。从此斑块中获得的总能量收益为 $G(t)$。一个完整觅食周期（移动到斑块并在其中觅食）所经过的时间为 $T + t$。鉴于环境的平稳和遍历特性，长期平均能量摄入率（记为 $R(t)$）可以表示为单个周期的收益除以该周期的持续时间：\n$$ R(t) = \\frac{G(t)}{T + t} $$\n我们的目标是找到使该速率 $R(t)$ 在 $t \\ge 0$ 时最大化的斑块停留时间 $t = t^*$。为了找到极值，我们计算 $R(t)$ 对 $t$ 的导数并将其设为零。使用商法则求导：\n$$ \\frac{dR}{dt} = \\frac{G'(t)(T + t) - G(t)(1)}{(T + t)^2} $$\n令 $\\frac{dR}{dt} = 0$ 需要分子为零（对于 $T+t > 0$）：\n$$ G'(t)(T + t) - G(t) = 0 $$\n重新整理此方程，得到在 $t=t^*$ 时的最优性必要条件：\n$$ G'(t^*) = \\frac{G(t^*)}{T + t^*} $$\n该方程表明，最优策略是在瞬时收益率 $G'(t^*)$ 等于长期平均收益率 $R(t^*) = \\frac{G(t^*)}{T + t^*}$ 时离开斑块。这就是边际价值定理。\n\n为确保此条件对应一个最大值，我们必须检查在 $t=t^*$ 处的二阶导数 $\\frac{d^2R}{dt^2}$。最大值的充分条件是 $\\frac{d^2R}{dt^2}|_{t=t^*} < 0$。设 $N(t) = G'(t)(T+t) - G(t)$。一阶导数为 $R'(t) = N(t) / (T+t)^2$。二阶导数为：\n$$ R''(t) = \\frac{N'(t)(T+t)^2 - N(t) \\cdot 2(T+t)}{ (T+t)^4 } = \\frac{N'(t)(T+t) - 2N(t)}{ (T+t)^3 } $$\n在最优点 $t^*$ 处，$N(t^*) = 0$，所以表达式简化为：\n$$ R''(t^*) = \\frac{N'(t^*)}{(T+t^*)^2} $$\n我们需要 $N'(t^*) < 0$。我们来计算 $N'(t)$：\n$$ N'(t) = \\frac{d}{dt} \\left[ G'(t)(T+t) - G(t) \\right] = [G''(t)(T+t) + G'(t)(1)] - G'(t) = G''(t)(T+t) $$\n问题陈述中指出，对于 $t>0$ 时 $G''(t)<0$。由于 $T \\ge 0$ 且我们关心的是 $t^* > 0$ 的非平凡解，因此项 $T+t^*$ 严格为正。所以，$N'(t^*) = G''(t^*)(T+t^*) < 0$。这证实了 $R''(t^*) < 0$，因此条件 $G'(t^*) = \\frac{G(t^*)}{T + t^*}$ 产生一个局部最大值。$G(t)$ 的凹性确保这对于 $t>0$ 是唯一的全局最大值。所需的正则性条件恰好是给定的条件：$G''(t)<0$。\n\n第2部分：针对特定收益函数的求解。\n\n给定收益函数 $G(t) = a(1 - \\exp(-bt))$，其中参数 $a > 0$ 和 $b > 0$。其导数为 $G'(t) = ab\\exp(-bt)$。$G'(t)>0$ 和 $G''(t) = -ab^2\\exp(-bt)<0$ 的条件都满足。我们将它们代入最优性条件：\n$$ ab\\exp(-bt^*) = \\frac{a(1 - \\exp(-bt^*))}{T + t^*} $$\n参数 $a > 0$ 从两边消去：\n$$ b\\exp(-bt^*) (T + t^*) = 1 - \\exp(-bt^*) $$\n我们重新整理各项来求解 $t^*$：\n$$ bT\\exp(-bt^*) + bt^*\\exp(-bt^*) = 1 - \\exp(-bt^*) $$\n$$ (bT + 1 + bt^*)\\exp(-bt^*) = 1 $$\n$$ bT + 1 + bt^* = \\exp(bt^*) $$\n这是一个超越方程。为了求解它，我们使用 Lambert W 函数，该函数定义为方程 $z = W(z)\\exp(W(z))$ 的解 $W(z)$。我们必须将我们的方程整理成这种形式。设 $u = bt^*$。方程变为 $bT + 1 + u = \\exp(u)$。\n设 $X = -(bT + 1 + u)$。则 $u = -X - (bT+1)$。将此代入方程：\n$$ -X = \\exp(-X - (bT+1)) $$\n$$ -X = \\exp(-X) \\exp(-(bT+1)) $$\n$$ X \\exp(X) = -\\exp(-(bT+1)) $$\n这就是 $W(z)\\exp(W(z)) = z$ 的形式，其中 $z = -\\exp(-(bT+1))$。因此，$X$ 的解是 $X = W(-\\exp(-(bT+1)))$。\n将 $X = -(bT+1+u)$ 代回：\n$$ u = -X - (bT+1) = -W(-\\exp(-(bT+1))) - (bT+1) $$\n又因为 $u = bt^*$，所以 $t^*$ 的解是：\n$$ t^* = \\frac{1}{b} \\left( -W(-\\exp(-(bT+1))) - (bT+1) \\right) $$\nLambert W 函数的自变量 $z = -\\exp(-(bT+1))$ 位于区间 $[-\\exp(-1), 0)$ 内，因为 $b > 0$ 且 $T \\ge 0$。在此区间内，Lambert W 函数有两个实数分支：主分支 $W_0(z)$，其中 $W_0(z) \\in [-1, 0)$；以及下分支 $W_{-1}(z)$，其中 $W_{-1}(z) \\le -1$。\n\n我们必须选择能得出生物学上可接受的解的分支，即 $t^* \\ge 0$。\n对应于主分支 $W_0$ 的解是：\n$$ t_0^* = \\frac{1}{b} \\left( -W_0(-\\exp(-(bT+1))) - (bT+1) \\right) $$\n由于 $-1 \\le W_0(z) < 0$，我们有 $0 < -W_0(z) \\le 1$。由于 $bT+1 \\ge 1$，括号内的项 $(-W_0(z)) - (bT+1) \\le 1 - 1 = 0$，并且对于 $T>0$ 是严格为负的。因此，$t_0^* \\le 0$ (且当 $T>0$ 时 $t_0^* < 0$)。这个解没有物理意义。\n\n对应于下分支 $W_{-1}$ 的解是：\n$$ t_{-1}^* = \\frac{1}{b} \\left( -W_{-1}(-\\exp(-(bT+1))) - (bT+1) \\right) $$\n对于 $z \\in [-\\exp(-1), 0)$，我们有 $W_{-1}(z) \\le -1$。因此，$-W_{-1}(z) \\ge 1$。\n括号内的项是 $(-W_{-1}(z)) - (bT+1)$。对函数 $h(u) = \\exp(u) - u - (bT+1)$ 的分析表明，它在 $u=0$ 处有最小值 $h(0) = -bT \\le 0$，并且当 $u \\to \\pm\\infty$ 时趋向于 $+\\infty$。这意味着 $u$ 有两个实数根（当 $T>0$ 时），一个正根和一个负根。我们已经证明了 $W_0$ 分支对应于负根。因此，$W_{-1}$ 分支必须提供唯一的正根。从而，$t_{-1}^* \\ge 0$。这是正确的、生物学上可接受的解。\n\n最优停留时间 $t^*$ 的显式闭合形式表达式由 $W_{-1}$ 分支给出。", "answer": "$$ \\boxed{ \\frac{1}{b} \\left( -W_{-1}\\left(-\\exp\\left(-bT-1\\right)\\right) - bT - 1 \\right) } $$", "id": "2778870"}, {"introduction": "动态状态变量模型（Dynamic state variable models）是现代行为生态学研究的核心工具，它使我们能够在一个统一的框架内，对生物体面临的复杂决策进行建模，尤其适用于那些决策后果依赖于个体当前状态（近因）和未来不确定性的情景。通过构建一个关于鸟类迁徙时间的模型，你将亲手实践如何定义状态空间、决策规则和最终的适应性函数，并通过动态规划算法找出最优行为策略。这个练习将让你深刻体会到，动物的行为决策是在其生理状态（近因）和最大化长期繁殖成功（终极目标）之间的权衡与优化。", "problem": "一只候鸟必须每天决定是立即出发前往繁殖地，还是留在停歇地觅食。在任何决策时间点，其近因状态是二维的，包括脂肪储备和时间。设状态为 $(f,t)$，其中 $f \\in \\{0,1,\\dots,F_{\\max}\\}$ 表示离散的能量储备（以任意能量单位计），$t \\in \\{0,1,\\dots,T\\}$ 表示有限季节内的某一天。留在停歇地（行为 $\\text{wait}$）会导致因天气驱动的觅食而引起的能量储备的随机变化，并伴随一个每日存活概率；而出发（行为 $\\text{depart}$）则会立即产生一个成功迁徙的概率（取决于能量储备）和一个取决于到达时间的繁殖收益。该设定将行为生态学中近因（一天内的生理能量动态和存活）与究极原因（到达后的预期繁殖成功率）的区分形式化了。目标是计算出能最大化预期适应度的最优迁徙时间策略。\n\n您必须使用以下基本假设和定义，实现一个作为马尔可夫决策过程 (MDP) 的有限期动态状态变量模型。\n\n1. 状态、行为和时限\n   - 状态为 $(f,t)$，其中 $f \\in \\{0,1,\\dots,F_{\\max}\\}$ 且 $t \\in \\{0,1,\\dots,T\\}$。\n   - 行为 $a \\in \\{\\text{depart}, \\text{wait}\\}$。\n   - 时限在第 $T$ 天之后结束。如果鸟儿在 $t=T$ 时还未出发，则任何后续的等待行为产生的适应度均为零。\n\n2. 行为 $\\text{wait}$ 下的近因动态\n   - 在停歇地的每日存活概率是一个常数 $s \\in (0,1]$。\n   - 天气是随机的且每日独立，出现“好天气”的概率为 $p_G(t) \\in [0,1]$。在好天气日，净脂肪变化为 $g_G$；在坏天气日，净脂肪变化为 $g_B$。$g_G$ 和 $g_B$ 均为整数（能量单位/天），下一状态的脂肪计算为 $f'=\\min\\{\\max\\{f+\\Delta f,0\\},F_{\\max}\\}$，其中 $\\Delta f \\in \\{g_G,g_B\\}$。\n   - 等待的预期持续价值为\n     $$ W(f,t) \\;=\\; s \\left[ p_G(t)\\,V\\!\\left(\\min\\{\\max\\{f+g_G,0\\},F_{\\max}\\},\\,t+1\\right) \\;+\\; \\left(1-p_G(t)\\right) V\\!\\left(\\min\\{\\max\\{f+g_B,0\\},F_{\\max}\\},\\,t+1\\right) \\right]. $$\n\n3. 行为 $\\text{depart}$ 下的究极收益\n   - 成功迁徙的概率是脂肪的逻辑斯谛函数：\n     $$ q(f) \\;=\\; \\frac{1}{1+\\exp\\!\\left(-\\alpha\\,(f - f_{50})\\right)}, $$\n     其中 $\\alpha > 0$ 是斜率，$f_{50}$ 是使 $q(f)=1/2$ 时的脂肪量。\n   - 到达时的繁殖收益随时间递减：\n     $$ R(t) \\;=\\; \\max\\{0,\\, R_{\\max} - c_t \\, t\\}, $$\n     其中 $R_{\\max} \\ge 0$ 和 $c_t \\ge 0$ 是常数。\n   - 出发的预期价值为 $D(f,t)=q(f)\\,R(t)$。\n\n4. 目标和 Bellman 递归\n   - 设 $V(f,t)$ 为从状态 $(f,t)$ 出发可获得的最大预期适应度。Bellman 方程为\n     $$ V(f,t) \\;=\\; \\max\\{ D(f,t),\\, W(f,t) \\}, \\quad \\text{平局时倾向于 } \\text{depart}。$$\n   - 终止条件：对于所有 $f$，$V(f,T+1)=0$。\n\n5. 输出和单位\n   - 在此模型中，能量单位是任意的且无量纲。时间单位为天。概率必须以小数形式提供（例如，用 $0.6$ 而不是 $60\\%$）。程序必须报告预期适应度，为一个无量纲实数，四舍五入到六位小数。\n\n6. 要求的最终输出格式\n   - 您的程序应生成单行输出，其中包含用方括号括起来的、逗号分隔的结果列表（例如，“[result1,result2,result3]”）。\n   - 对于每个测试用例 $i$，按顺序报告两个值：指定初始状态下的最优行为，编码为整数（$0$ 代表等待，$1$ 代表出发），以及预期适应度 $V(f_0,t_0)$，四舍五入到六位小数。\n   - 对于 $N$ 个测试用例，最终输出必须是长度为 $2N$ 的单一列表，顺序为 $[a_1,V_1,a_2,V_2,\\dots,a_N,V_N]$。\n\n7. 测试套件\n   为以下 $5$ 个测试用例分别实现该模型。每个测试用例指定了参数和初始状态 $(f_0,t_0)$。所有数字均以小数形式给出，必须按原样使用。\n\n   - 案例 $1$ (理想路径，季节早期，中等脂肪)：\n     - $F_{\\max}=20$, $T=10$。\n     - $g_G=+3$, $g_B=-2$。\n     - $s=0.98$。\n     - $\\alpha=1.2$, $f_{50}=8$。\n     - $R_{\\max}=10$, $c_t=0.5$。\n     - $p_G(t)=0.6$ (对所有 $t$)。\n     - 初始状态 $(f_0,t_0)=(5,0)$。\n\n   - 案例 $2$ (边界：最后一天决策)：\n     - $F_{\\max}=20$, $T=5$。\n     - $g_G=+3$, $g_B=-2$。\n     - $s=0.98$。\n     - $\\alpha=1.2$, $f_{50}=8$。\n     - $R_{\\max}=10$, $c_t=0.5$。\n     - $p_G(t)=0.5$ (对所有 $t$)。\n     - 初始状态 $(f_0,t_0)=(7,5)$。\n\n   - 案例 $3$ (早期高脂肪；提早到达价值高)：\n     - $F_{\\max}=20$, $T=10$。\n     - $g_G=+3$, $g_B=-2$。\n     - $s=0.98$。\n     - $\\alpha=1.2$, $f_{50}=8$。\n     - $R_{\\max}=10$, $c_t=0.5$。\n     - $p_G(t)=0.5$ (对所有 $t$)。\n     - 初始状态 $(f_0,t_0)=(15,2)$。\n\n   - 案例 $4$ (边缘情况：危险的停歇地和恶劣天气下觅食效果差)：\n     - $F_{\\max}=20$, $T=8$。\n     - $g_G=+2$, $g_B=-3$。\n     - $s=0.85$。\n     - $\\alpha=1.0$, $f_{50}=7$。\n     - $R_{\\max}=10$, $c_t=0.7$。\n     - $p_G(t)=0.5$ (对所有 $t$)。\n     - 初始状态 $(f_0,t_0)=(3,0)$。\n\n   - 案例 $5$ (时变环境；后期觅食条件改善)：\n     - $F_{\\max}=20$, $T=12$。\n     - $g_G=+4$, $g_B=-2$。\n     - $s=0.97$。\n     - $\\alpha=1.3$, $f_{50}=9$。\n     - $R_{\\max}=12$, $c_t=0.4$。\n     - $p_G(t)=0.3 + (0.8-0.3)\\,t/T$ (对 $t \\in \\{0,\\dots,T\\}$)。\n     - 初始状态 $(f_0,t_0)=(6,3)$。\n\n您的程序必须通过动态规划为每个测试用例计算完整的 DP 策略，然后输出一行包含连接起来的结果 $[a_1,V_1,a_2,V_2,a_3,V_3,a_4,V_4,a_5,V_5]$，其中每个 $a_i \\in \\{0,1\\}$，每个 $V_i$ 四舍五入到六位小数。", "solution": "目标是形式化一个将近因机制与究极结果分开的迁徙时间问题，然后通过动态规划（DP）来求解。近因方面捕捉了能量储备和跨天存活的状态更新，而究极方面是成功到达后的预期繁殖成功率，这取决于出发日期和依赖于脂肪的成功概率。行为生态学中连接近因和究极原因的最优化原理是，自然选择倾向于偏好那些在生理和生态约束下能最大化预期繁殖成功率的决策规则。\n\n基本原理和建模原则：\n- 马尔可夫决策过程 (MDP) 框架代表了不确定性下的序贯决策，其中状态满足马尔可夫性质，决策者旨在最大化预期累积（或终点）回报。\n- 在这个问题中，状态是 $(f,t)$，行为是 $\\text{depart}$ 或 $\\text{wait}$，所有不确定性（觅食日是好是坏）都是随机的，并且在（时间条件独立的）各天之间是独立的。\n- 适应度收益是终点的，它取决于迁徙的成功（概率 $q(f)$，脂肪储备的近因函数）和通过 $R(t)$ 体现的到达日期（编码了繁殖收益季节性下降的究极适应度组成部分）。\n- 最优性原理导出了一个 Bellman 递归：今天的最优决策是最大化即时预期回报加上未来最优行动的预期价值之和。\n\nBellman 递归的逐步推导：\n1. 定义终止条件。在一个在第 $T$ 天之后结束的有限时限内，如果个体在时间 $T$ 之前没有出发然后等待，就没有时间获得额外的收益：对于所有 $f$，$V(f,T+1)=0$。\n2. 在 $(f,t)$ 出发的期望值。即时预期适应度是成功概率乘以到达时间的收益：\n   $$ D(f,t)= q(f)\\,R(t), \\quad q(f)=\\frac{1}{1+\\exp(-\\alpha(f-f_{50}))}, \\quad R(t)=\\max\\{0,R_{\\max}-c_t t\\}. $$\n   这捕捉了究极方面（繁殖收益 $R(t)$），并由一个近因机制（对脂肪的逻辑斯谛依赖 $q(f)$）进行加权。\n3. 等待一天的期望值。等待不会产生即时收益，但保留了稍后出发的选择权，代价是每日死亡率和随机觅食。存活部分 $s$（近因）乘以对下一天价值的期望：\n   $$ W(f,t)= s\\left[ p_G(t)\\,V\\!\\left(\\min\\{\\max\\{f+g_G,0\\},F_{\\max}\\},\\,t+1\\right) + (1-p_G(t))\\,V\\!\\left(\\min\\{\\max\\{f+g_B,0\\},F_{\\max}\\},\\,t+1\\right) \\right]. $$\n   裁剪操作 $\\min\\{\\max\\{\\cdot,0\\},F_{\\max}\\}$ 编码了生理约束，并确保状态保持在界限内。\n4. 最优性与平局决胜。价值函数满足\n   $$ V(f,t)=\\max\\{D(f,t),W(f,t)\\}, $$\n   平局时倾向于出发，这导出了一个明确定义的策略 $\\pi(f,t)\\in\\{0,1\\}$，编码为 $1$ 代表出发，$0$ 代表等待。\n\n算法设计：\n- 使用反向动态规划，时间 $t$ 从 $T$ 向下递减到 $0$。\n- 将 $V$ 表示为一个 $(F_{\\max}+1)\\times (T+2)$ 的数组，以包含在 $T+1$ 处的终止列。\n- 对于每个 $t$ 和每个 $f$，使用所提供的参数计算 $D(f,t)$ 和 $W(f,t)$，然后将 $V(f,t)$ 设置为它们的最大值，并在一个相同大小的策略数组中记录相应的行为。\n- 环境过程 $p_G(t)$ 要么是常数，要么是时变的（对于案例 5 是 $t$ 的线性函数），但由于天气在各天之间是独立的且不属于状态的一部分，因此只有其期望值对一步转移有影响。\n- 在填充 $V$ 和策略后，为每个测试用例读出最优行为 $\\pi(f_0,t_0)$ 和预期适应度 $V(f_0,t_0)$。\n\n正确性论证：\n- MDP 的马尔可夫性质成立，因为下一个状态的分布仅取决于 $(f,t)$ 和所选行为，而不取决于更早的历史。\n- Bellman 方程源于最优性原理，该原理对有限期 MDP 有效。\n- 由于有限的状态和行为空间以及在 $t=T+1$ 的终止条件，反向归纳算法在有限时间内精确收敛。\n- 计算出的策略通过构造能够最大化预期的终点适应度。\n\n计算考虑：\n- 每个测试用例的时间复杂度为 $O\\!\\left(T \\cdot (F_{\\max}+1)\\right)$，因为每个 $(f,t)$ 的内部计算是 $O(1)$。\n- 该模型处理边界情况：在 $t=T$ 时，等待产生 $W(f,T)=s\\,\\mathbb{E}[V(\\cdot,T+1)]=0$，因此如果 $D(f,T)>0$，则出发占优。如果脂肪很低，$q(f)$ 会降低成功出发的概率，DP 会将其与等待的好处和风险进行比较。\n\n测试覆盖理由：\n- 案例 1 探索了季节早期、中等脂肪和适度觅食随机性下的典型场景。\n- 案例 2 强制进行最后一天决策，此时等待无法产生未来收益，从而验证边界逻辑。\n- 案例 3 测试了早期高脂肪是否因高 $q(f)$ 和较早时间较高的 $R(t)$ 而导致提早出发。\n- 案例 4 检验了危险的停歇地和恶劣天气下觅食效果差的情况，这可能使得即使在低脂肪时也倾向于提早出发。\n- 案例 5 引入了一个时变的觅食环境，其中 $p_G(t)$ 随时间改善，测试模型是否能建议等待以利用未来更好的觅食条件，尽管 $R(t)$ 在下降。\n\n输出规范要求的实现细节：\n- 程序必须硬编码 5 个测试用例，为每个用例计算策略和价值函数，然后输出一个包含 10 个条目的单一列表：$[a_1,V_1,a_2,V_2,a_3,V_3,a_4,V_4,a_5,V_5]$，其中每个 $a_i\\in\\{0,1\\}$，每个 $V_i$ 打印时四舍五入到六位小数。不应打印任何额外文本。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logistic_success_prob(f_vals, alpha, f50):\n    # q(f) = 1 / (1 + exp(-alpha * (f - f50)))\n    return 1.0 / (1.0 + np.exp(-alpha * (f_vals - f50)))\n\ndef compute_policy_and_value(case):\n    Fmax = int(case['Fmax'])\n    T = int(case['T'])\n    gG = int(case['gG'])\n    gB = int(case['gB'])\n    s = float(case['s'])\n    alpha = float(case['alpha'])\n    f50 = float(case['f50'])\n    Rmax = float(case['Rmax'])\n    ct = float(case['ct'])\n    pG_type = case['pG_type']\n\n    # Precompute q(f) for all f\n    f_grid = np.arange(Fmax + 1, dtype=float)\n    q = logistic_success_prob(f_grid, alpha, f50)\n\n    # Precompute R(t) for all t\n    t_grid = np.arange(T + 1, dtype=float)\n    R = np.maximum(0.0, Rmax - ct * t_grid)\n\n    # Initialize value and policy arrays\n    V = np.zeros((Fmax + 1, T + 2), dtype=float)  # includes terminal column at T+1\n    policy = np.zeros((Fmax + 1, T + 1), dtype=int)  # actions for t=0..T\n\n    # Helper to get pG(t)\n    if pG_type == 'const':\n        p_const = float(case['pG_const'])\n        def pG(t):\n            return p_const\n    elif pG_type == 'linear':\n        p0 = float(case['p0'])\n        pT = float(case['pT'])\n        def pG(t):\n            # linear in t from p0 at t=0 to pT at t=T\n            return p0 + (pT - p0) * (t / T)\n    else:\n        raise ValueError(\"Unknown pG_type\")\n\n    # Backward DP\n    for t in range(T, -1, -1):\n        p = pG(t)\n        # For each f, compute depart and wait values\n        for f in range(Fmax + 1):\n            # Depart value\n            depart_val = q[f] * R[t]\n\n            # Wait value\n            fG = f + gG\n            if fG < 0:\n                fG = 0\n            elif fG > Fmax:\n                fG = Fmax\n\n            fB = f + gB\n            if fB < 0:\n                fB = 0\n            elif fB > Fmax:\n                fB = Fmax\n\n            wait_val = s * (p * V[fG, t + 1] + (1.0 - p) * V[fB, t + 1])\n\n            # Optimal choice; tie broken in favor of depart\n            if depart_val >= wait_val:\n                V[f, t] = depart_val\n                policy[f, t] = 1\n            else:\n                V[f, t] = wait_val\n                policy[f, t] = 0\n\n    # Initial state's action and value\n    f0 = int(case['init_f'])\n    t0 = int(case['init_t'])\n    a0 = int(policy[f0, t0])\n    v0 = float(V[f0, t0])\n    return a0, v0\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Case 1\n            'Fmax': 20,\n            'T': 10,\n            'gG': 3,\n            'gB': -2,\n            's': 0.98,\n            'alpha': 1.2,\n            'f50': 8.0,\n            'Rmax': 10.0,\n            'ct': 0.5,\n            'pG_type': 'const',\n            'pG_const': 0.6,\n            'init_f': 5,\n            'init_t': 0\n        },\n        {\n            # Case 2\n            'Fmax': 20,\n            'T': 5,\n            'gG': 3,\n            'gB': -2,\n            's': 0.98,\n            'alpha': 1.2,\n            'f50': 8.0,\n            'Rmax': 10.0,\n            'ct': 0.5,\n            'pG_type': 'const',\n            'pG_const': 0.5,\n            'init_f': 7,\n            'init_t': 5\n        },\n        {\n            # Case 3\n            'Fmax': 20,\n            'T': 10,\n            'gG': 3,\n            'gB': -2,\n            's': 0.98,\n            'alpha': 1.2,\n            'f50': 8.0,\n            'Rmax': 10.0,\n            'ct': 0.5,\n            'pG_type': 'const',\n            'pG_const': 0.5,\n            'init_f': 15,\n            'init_t': 2\n        },\n        {\n            # Case 4\n            'Fmax': 20,\n            'T': 8,\n            'gG': 2,\n            'gB': -3,\n            's': 0.85,\n            'alpha': 1.0,\n            'f50': 7.0,\n            'Rmax': 10.0,\n            'ct': 0.7,\n            'pG_type': 'const',\n            'pG_const': 0.5,\n            'init_f': 3,\n            'init_t': 0\n        },\n        {\n            # Case 5\n            'Fmax': 20,\n            'T': 12,\n            'gG': 4,\n            'gB': -2,\n            's': 0.97,\n            'alpha': 1.3,\n            'f50': 9.0,\n            'Rmax': 12.0,\n            'ct': 0.4,\n            'pG_type': 'linear',\n            'p0': 0.3,\n            'pT': 0.8,\n            'init_f': 6,\n            'init_t': 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        action, value = compute_policy_and_value(case)\n        results.append(action)\n        results.append(f\"{value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2778865"}]}