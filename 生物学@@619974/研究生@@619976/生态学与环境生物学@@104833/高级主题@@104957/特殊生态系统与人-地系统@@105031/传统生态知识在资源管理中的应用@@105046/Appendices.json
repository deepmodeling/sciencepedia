{"hands_on_practices": [{"introduction": "在将传统生态知识（TEK）整合到管理中之前，评估其可靠性是至关重要的第一步。本练习介绍了一个核心统计方法——科恩卡帕系数（Cohen's Kappa），用于量化知识持有者之间的一致性程度。通过这个实践，你将学会如何评估TEK数据的可靠性，为后续的管理应用奠定定量基础。[@problem_id:2540754]", "problem": "一项滨海合作管理项目旨在将传统生态知识 (TEK) 纳入特定物种的捕捞指南中。来自同一社区的两 ($2$) 位经验丰富的渔民独立地识别了一组 $N$ 张凭证照片 ($N = 80$) 中所描绘的物种，每张照片包含一个个体。该社区使用的局内分类学识别出 $K = 4$ 个类别，记为 $S_1, S_2, S_3, S_4$。两位信息提供者之间的 $4 \\times 4$ 识别列联表（行为信息提供者 A，列为信息提供者 B，类别顺序为 $S_1, S_2, S_3, S_4$）是\n$$\n\\mathbf{N} \\;=\\;\n\\begin{pmatrix}\n23 & 2 & 5 & 0 \\\\\n1 & 12 & 5 & 0 \\\\\n1 & 6 & 10 & 0 \\\\\n0 & 0 & 0 & 15\n\\end{pmatrix}.\n$$\n仅使用评分者间信度的基本定义，按以下步骤进行：\n- 将观测一致性定义为两位信息提供者给出相同类别的案例比例，并将期望一致性定义为在假设信息提供者根据其边际类别频率独立分配类别的情况下的期望比例。\n- 从这些定义中，推导出超出偶然性的一致性的表达式，即观测一致性减去期望一致性，再通过超出偶然性的空间进行归一化。\n- 计算给定数据的相应一致性统计量（科恩的 kappa 系数），将最终的量化答案表示为无单位的精确分数。\n- 根据您计算出的值的大小和 $\\mathbf{N}$ 中的模式，简要解释这对于使用这些信息提供者的识别结果进行管理决策的可靠性意味着什么。如果您熟悉文化共识分析 (CCA)，请用一句话说明它是否适用于此，并解释原因，但不要进行计算。\n\n将最终定量答案表示为精确分数。无单位。不四舍五入。", "solution": "所给问题要求基于两位信息提供者物种识别结果的列联表，计算和解释评分者间信度统计量，特别是科恩的 kappa 系数。首先必须评估问题陈述的有效性。\n\n问题验证：\n步骤1：提取已知条件。\n- 信息提供者数量：$2$。\n- 项目（照片）总数：$N = 80$。\n- 类别数量：$K = 4$，记为 $S_1, S_2, S_3, S_4$。\n- 识别结果的列联表 $\\mathbf{N}$：\n$$\n\\mathbf{N} \\;=\\;\n\\begin{pmatrix}\n23 & 2 & 5 & 0 \\\\\n1 & 12 & 5 & 0 \\\\\n1 & 6 & 10 & 0 \\\\\n0 & 0 & 0 & 15\n\\end{pmatrix}\n$$\n- 任务：定义观测一致性 ($P_o$)，定义期望一致性 ($P_e$)，推导科恩的 kappa 系数 ($\\kappa$) 的公式，计算 $\\kappa$，并提供解释。\n\n步骤2：使用提取的已知条件进行验证。\n该问题具有科学依据，提法得当，且客观。它将标准的统计方法（评分者间信度分析）应用于民族生态学中的一个现实场景。所提供的数据是完整且内部一致的。矩阵 $\\mathbf{N}$ 中各项的总和为 $23 + 2 + 5 + 0 + 1 + 12 + 5 + 0 + 1 + 6 + 10 + 0 + 0 + 0 + 0 + 15 = 80$，与给定的项目总数 $N$ 相匹配。没有科学或逻辑上的缺陷、歧义或缺失信息。因此，该问题是有效的。\n\n步骤3：结论与行动。\n问题有效。将提供解答。\n\n解答：\n设 $n_{ij}$ 为信息提供者 A 评定为类别 $S_i$ 且信息提供者 B 评定为类别 $S_j$ 的项目数。项目总数为 $N = \\sum_{i=1}^{K} \\sum_{j=1}^{K} n_{ij}$。此处，$K=4$ 且 $N=80$。\n\n首先，我们定义观测到的一致性比例 $P_o$。这是两位信息提供者对分类达成一致的项目所占的比例。一致性出现在列联表 $\\mathbf{N}$ 的主对角线上。\n$$\nP_o = \\frac{\\sum_{k=1}^{K} n_{kk}}{N}\n$$\n使用给定数据：\n$$\n\\sum_{k=1}^{4} n_{kk} = n_{11} + n_{22} + n_{33} + n_{44} = 23 + 12 + 10 + 15 = 60\n$$\n因此，观测到的一致性是：\n$$\nP_o = \\frac{60}{80} = \\frac{3}{4}\n$$\n接着，我们定义期望一致性 $P_e$。这是假设信息提供者的评分是独立的，偶然达成的一致性比例。对于每个类别 $S_k$，我们计算每位信息提供者分配该类别的边际频率。\n设 $n_{i\\cdot} = \\sum_{j=1}^{K} n_{ij}$ 为类别 $S_i$ 的行合计（信息提供者 A 评定为 $S_i$ 的总数）。\n设 $n_{\\cdot j} = \\sum_{i=1}^{K} n_{ij}$ 为类别 $S_j$ 的列合计（信息提供者 B 评定为 $S_j$ 的总数）。\n\n行合计为：\n$n_{1\\cdot} = 23 + 2 + 5 + 0 = 30$\n$n_{2\\cdot} = 1 + 12 + 5 + 0 = 18$\n$n_{3\\cdot} = 1 + 6 + 10 + 0 = 17$\n$n_{4\\cdot} = 0 + 0 + 0 + 15 = 15$\n\n列合计为：\n$n_{\\cdot 1} = 23 + 1 + 1 + 0 = 25$\n$n_{\\cdot 2} = 2 + 12 + 6 + 0 = 20$\n$n_{\\cdot 3} = 5 + 5 + 10 + 0 = 20$\n$n_{\\cdot 4} = 0 + 0 + 0 + 15 = 15$\n\n信息提供者 A 将一个项目分类为 $S_k$ 的概率是 $p_{A,k} = \\frac{n_{k\\cdot}}{N}$。信息提供者 B 将一个项目分类为 $S_k$ 的概率是 $p_{B,k} = \\frac{n_{\\cdot k}}{N}$。在独立性假设下，两者都将一个项目分类为 $S_k$ 的概率是 $p_{A,k} \\times p_{B,k}$。总的期望一致性是这些概率在所有类别上的总和：\n$$\nP_e = \\sum_{k=1}^{K} p_{A,k} \\cdot p_{B,k} = \\sum_{k=1}^{K} \\frac{n_{k\\cdot}}{N} \\frac{n_{\\cdot k}}{N} = \\frac{1}{N^2} \\sum_{k=1}^{K} n_{k\\cdot} n_{\\cdot k}\n$$\n使用计算出的边际总数：\n$$\n\\sum_{k=1}^{4} n_{k\\cdot} n_{\\cdot k} = (30 \\times 25) + (18 \\times 20) + (17 \\times 20) + (15 \\times 15)\n$$\n$$\n\\sum_{k=1}^{4} n_{k\\cdot} n_{\\cdot k} = 750 + 360 + 340 + 225 = 1675\n$$\n现在，我们计算 $P_e$：\n$$\nP_e = \\frac{1675}{N^2} = \\frac{1675}{80^2} = \\frac{1675}{6400}\n$$\n为简化此分数，我们注意到分子和分母都可被 $25$ 整除。\n$1675 \\div 25 = 67$。\n$6400 \\div 25 = 256$。\n所以，$P_e = \\frac{67}{256}$。\n\n科恩的 kappa 系数 $\\kappa$ 定义为超出偶然性的一致性，通过超出偶然性的最大可能一致性进行归一化。观测到的一致性是 $P_o$。偶然期望的一致性是 $P_e$。因此，超出偶然性达成的一致性是 $P_o - P_e$。最大可能的一致性是 $1$，所以超出偶然性的最大可能一致性是 $1 - P_e$。因此，该统计量是以下比率：\n$$\n\\kappa = \\frac{P_o - P_e}{1 - P_e}\n$$\n代入我们计算出的 $P_o$ 和 $P_e$ 值：\n$$\n\\kappa = \\frac{\\frac{3}{4} - \\frac{67}{256}}{1 - \\frac{67}{256}}\n$$\n我们为分子和分母中的项找到一个共同的分母。\n分子：\n$$\n\\frac{3}{4} - \\frac{67}{256} = \\frac{3 \\times 64}{256} - \\frac{67}{256} = \\frac{192 - 67}{256} = \\frac{125}{256}\n$$\n分母：\n$$\n1 - \\frac{67}{256} = \\frac{256}{256} - \\frac{67}{256} = \\frac{189}{256}\n$$\n现在，我们计算比率：\n$$\n\\kappa = \\frac{\\frac{125}{256}}{\\frac{189}{256}} = \\frac{125}{189}\n$$\n分子的质因数分解是 $125 = 5^3$。分母的质因数分解是 $189 = 3^3 \\times 7$。由于没有公因数，该分数为最简分数。\n\n解释：\n计算出的科恩 kappa 系数值为 $\\kappa = \\frac{125}{189} \\approx 0.661$。该值通常表示两位信息提供者之间存在显著的一致性。这种一致性远高于随机偶然性所预期的一致性。\n对矩阵 $\\mathbf{N}$ 的检查揭示了一致和不一致的模式。对于所有被识别为类别 $S_4$ 的项目，存在完全一致（即 $15$ 次一致， $0$ 次不一致），这表明该类别是独特的且能被可靠地识别。不一致主要集中在类别 $S_1$、$S_2$ 和 $S_3$ 之间。例如，当信息提供者 A 将一个样本识别为 $S_3$ 时，信息提供者 B 只有 $10$ 次同意，但在 $6$ 个样本中将其错误识别为 $S_2$，在 $1$ 个样本中错误识别为 $S_1$。这表明，虽然总体可靠性很高，但在子集 $\\{S_1, S_2, S_3\\}$ 内部存在特定的可辨性不足。出于管理目的，使用这些信息提供者的传统生态知识对于有关物种 $S_4$ 的决策将非常可靠，但对于需要区分 $S_1$、$S_2$ 和 $S_3$ 的决策则可能不可靠。\n\n关于文化共识分析 (CCA) 的适用性：文化共识分析不适用于此处，因为其基本模型要求至少有三名信息提供者才能统计上估计出共识答案和每个信息提供者的能力。", "answer": "$$\\boxed{\\frac{125}{189}}$$", "id": "2540754"}, {"introduction": "传统生态知识通常作为环境事件的预测指标发挥作用，指导社区做出决策。本练习将带你应用贝叶斯决策理论，将一个基于TEK的指标（鸟鸣频率）转化为一个正式的、以成本最小化为目标的管理行动规则。这让你能够理解如何在不确定性下，权衡不同类型错误（如“虚警”和“漏报”）的代价，从而制定最优策略。[@problem_id:2540709]", "problem": "一个雨养农林业社区利用传统生态知识（TEK）来预测次日降雨，并分配劳动力进行雨水收集。一个常用的TEK指标是一种哨兵鸟类的夜间鸣叫率，该指标被建模为一个连续随机变量 $X$（鸣叫次数/小时），在傍晚调查时测量。历史共时观测得出了以下季节性特征：\n- 当次日有雨时（事件 $R$），$X$ 近似服从均值为 $\\mu_{R}$、方差为 $\\sigma^{2}$ 的正态分布。\n- 当次日无雨时（事件 $\\bar{R}$），$X$ 近似服从均值为 $\\mu_{\\bar{R}}$、方差同为 $\\sigma^{2}$ 的正态分布。\n- 降雨的季节性基础率（先验概率）为 $\\mathbb{P}(R)=p$ 及 $\\mathbb{P}(\\bar{R})=1-p$。\n\n社区每晚必须决定是否动员劳动力进行雨水收集。考虑一个阈值策略：如果 $X \\ge \\tau$，则预测有雨并动员；否则不动员。其后果由一个成本结构表示：\n- 虚警（在事件 $\\bar{R}$ 发生时动员）：成本 $C_{F}$。\n- 漏报（在事件 $R$ 发生时未动员）：成本 $C_{M}$。\n- 正确决策不产生额外成本（基准）。\n\n假设分布和参数如下：\n- $\\mu_{R} = 30$ 鸣叫次数/小时, $\\mu_{\\bar{R}} = 15$ 鸣叫次数/小时。\n- $\\sigma = 6$ 鸣叫次数/小时。\n- $p = 0.3$。\n- $C_{F} = 1$ （任意但一致的成本单位）, $C_{M} = 4$。\n\n仅使用条件概率、贝叶斯定理、期望损失最小化和正态分布的性质，从第一性原理出发，推导在阈值规则 $X \\ge \\tau \\Rightarrow$动员下，使得期望错分成本最小化的唯一阈值 $\\tau^{\\star}$。然后，使用给定的参数数值计算 $\\tau^{\\star}$。以“鸣叫次数/小时”为单位表示最终阈值，并将答案四舍五入至四位有效数字。", "solution": "该问题是贝叶斯决策理论在确定最优分类阈值方面的经典应用。目标是找到观测变量 $X$ 的阈值 $\\tau^{\\star}$，以最小化总期望错分成本。\n\n问题陈述已经过验证，具有科学依据，提法得当且客观。它包含了所有必要信息，没有矛盾或逻辑缺陷。我们可以开始求解。\n\n设 $X$ 为表示鸟鸣率的随机变量。设 $R$ 表示次日有雨的事件，$\\bar{R}$ 表示次日无雨的事件。决策规则为：如果 $X \\ge \\tau$，则预测有雨；如果 $X < \\tau$，则预测无雨。\n\n存在两种类型的错误：\n1. 虚警：当我们预测有雨（$X \\ge \\tau$）但实际上无雨（事件 $\\bar{R}$）。此错误的成本为 $C_{F}$。\n2. 漏报：当我们预测无雨（$X < \\tau$）但实际上有雨（事件 $R$）。此错误的成本为 $C_{M}$。\n\n总期望成本，我们记为 $E[C(\\tau)]$，是这两种错误成本的总和，每种成本都按其发生概率进行加权。\n$$E[C(\\tau)] = C_{F} \\cdot \\mathbb{P}(X \\ge \\tau \\text{ and } \\bar{R}) + C_{M} \\cdot \\mathbb{P}(X < \\tau \\text{ and } R)$$\n根据条件概率的定义 $\\mathbb{P}(A \\text{ and } B) = \\mathbb{P}(A|B)\\mathbb{P}(B)$，我们可以将期望成本重写为：\n$$E[C(\\tau)] = C_{F} \\cdot \\mathbb{P}(X \\ge \\tau | \\bar{R})\\mathbb{P}(\\bar{R}) + C_{M} \\cdot \\mathbb{P}(X < \\tau | R)\\mathbb{P}(R)$$\n题目给出了先验概率 $\\mathbb{P}(R) = p$ 和 $\\mathbb{P}(\\bar{R}) = 1-p$。条件概率可以表示为相应概率密度函数（PDF）的积分。设 $f_{X|R}(x)$ 为给定 $R$ 时 $X$ 的概率密度函数， $f_{X|\\bar{R}}(x)$ 为给定 $\\bar{R}$ 时 $X$ 的概率密度函数。\n$$E[C(\\tau)] = C_{F}(1-p) \\int_{\\tau}^{\\infty} f_{X|\\bar{R}}(x) \\, dx + C_{M}p \\int_{-\\infty}^{\\tau} f_{X|R}(x) \\, dx$$\n为求得使 $E[C(\\tau)]$ 最小化的阈值 $\\tau^{\\star}$，我们必须对 $E[C(\\tau)]$ 关于 $\\tau$ 求导，并令导数等于零。根据微积分基本定理，特别是莱布尼茨积分法则，我们有：\n$$\\frac{d}{d\\tau} \\int_{\\tau}^{\\infty} f(x) \\, dx = -f(\\tau)$$\n$$\\frac{d}{d\\tau} \\int_{-\\infty}^{\\tau} f(x) \\, dx = f(\\tau)$$\n将此应用于我们的期望成本函数：\n$$\\frac{dE[C(\\tau)]}{d\\tau} = C_{F}(1-p) [-f_{X|\\bar{R}}(\\tau)] + C_{M}p [f_{X|R}(\\tau)]$$\n将导数设为零以求最优阈值 $\\tau^{\\star}$：\n$$C_{M}p f_{X|R}(\\tau^{\\star}) - C_{F}(1-p) f_{X|\\bar{R}}(\\tau^{\\star}) = 0$$\n$$C_{M}p f_{X|R}(\\tau^{\\star}) = C_{F}(1-p) f_{X|\\bar{R}}(\\tau^{\\star})$$\n这可以重排为关于似然比的条件：\n$$\\frac{f_{X|R}(\\tau^{\\star})}{f_{X|\\bar{R}}(\\tau^{\\star})} = \\frac{C_{F}(1-p)}{C_{M}p}$$\n题目指出条件分布是方差同为 $\\sigma^2$ 的正态分布：\n$$X|R \\sim \\mathcal{N}(\\mu_{R}, \\sigma^2) \\quad \\implies \\quad f_{X|R}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{R})^2}{2\\sigma^2}\\right)$$\n$$X|\\bar{R} \\sim \\mathcal{N}(\\mu_{\\bar{R}}, \\sigma^2) \\quad \\implies \\quad f_{X|\\bar{R}}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{\\bar{R}})^2}{2\\sigma^2}\\right)$$\n将这些代入似然比方程，指数前的因子被消去：\n$$\\frac{\\exp\\left(-\\frac{(\\tau^{\\star} - \\mu_{R})^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(\\tau^{\\star} - \\mu_{\\bar{R}})^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{(\\tau^{\\star} - \\mu_{\\bar{R}})^2 - (\\tau^{\\star} - \\mu_{R})^2}{2\\sigma^2}\\right) = \\frac{C_{F}(1-p)}{C_{M}p}$$\n我们对两边取自然对数以求解 $\\tau^{\\star}$：\n$$\\frac{(\\tau^{\\star} - \\mu_{\\bar{R}})^2 - (\\tau^{\\star} - \\mu_{R})^2}{2\\sigma^2} = \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\n分子是一个平方差，我们可以展开：\n$$(\\tau^{\\star 2} - 2\\tau^{\\star}\\mu_{\\bar{R}} + \\mu_{\\bar{R}}^2) - (\\tau^{\\star 2} - 2\\tau^{\\star}\\mu_{R} + \\mu_{R}^2) = 2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) - (\\mu_{R}^2 - \\mu_{\\bar{R}}^2)$$\n将 $(\\mu_{R}^2 - \\mu_{\\bar{R}}^2)$ 分解为 $(\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}})$ 得到：\n$$2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) - (\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}})$$\n将此代回方程：\n$$\\frac{2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) - (\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}})}{2\\sigma^2} = \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\n假设 $\\mu_{R} \\neq \\mu_{\\bar{R}}$，我们可以分离出 $\\tau^{\\star}$：\n$$2\\tau^{\\star}(\\mu_{R} - \\mu_{\\bar{R}}) = (\\mu_{R} - \\mu_{\\bar{R}})(\\mu_{R} + \\mu_{\\bar{R}}) + 2\\sigma^2 \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\n$$\\tau^{\\star} = \\frac{\\mu_{R} + \\mu_{\\bar{R}}}{2} + \\frac{\\sigma^2}{\\mu_{R} - \\mu_{\\bar{R}}} \\ln\\left(\\frac{C_{F}(1-p)}{C_{M}p}\\right)$$\n这是最优阈值的一般解析表达式。\n\n现在，我们代入给定的数值：\n$\\mu_{R} = 30$\n$\\mu_{\\bar{R}} = 15$\n$\\sigma = 6 \\implies \\sigma^2 = 36$\n$p = 0.3 \\implies 1-p = 0.7$\n$C_{F} = 1$\n$C_{M} = 4$\n\n表达式变为：\n$$\\tau^{\\star} = \\frac{30 + 15}{2} + \\frac{36}{30 - 15} \\ln\\left(\\frac{1 \\cdot (1-0.3)}{4 \\cdot 0.3}\\right)$$\n$$\\tau^{\\star} = \\frac{45}{2} + \\frac{36}{15} \\ln\\left(\\frac{0.7}{1.2}\\right)$$\n$$\\tau^{\\star} = 22.5 + 2.4 \\ln\\left(\\frac{7}{12}\\right)$$\n我们现在计算数值：\n$$\\ln\\left(\\frac{7}{12}\\right) \\approx -0.5389965$$\n$$\\tau^{\\star} \\approx 22.5 + 2.4 \\cdot (-0.5389965)$$\n$$\\tau^{\\star} \\approx 22.5 - 1.2935916$$\n$$\\tau^{\\star} \\approx 21.2064084$$\n题目要求答案四舍五入到四位有效数字。\n$$\\tau^{\\star} \\approx 21.21$$\n这个值在物理上是合理的，因为它位于两个均值 $\\mu_{\\bar{R}} = 15$ 和 $\\mu_{R} = 30$ 之间。因为漏报的期望成本 $C_M p = 4 \\cdot 0.3 = 1.2$ 大于虚警的期望成本 $C_F(1-p) = 1 \\cdot 0.7 = 0.7$，所以阈值比简单的中点 $22.5$ 要低，以减少发生代价更高的“漏报”错误的概率。", "answer": "$$\\boxed{21.21}$$", "id": "2540709"}, {"introduction": "在现实世界中，传统生态知识并非铁板一块，它在不同社区和个体之间存在差异。本练习将指导你构建一个分层贝叶斯模型，这是一种强大的统计工具，用于综合来自多个来源的知识，同时还能解释不同专业水平和群体间的真实异质性。通过实现一个吉布斯采样器（Gibbs sampler），你将掌握在尊重数据复杂性的前提下进行原则性信息整合的先进方法。[@problem_id:2540689]", "problem": "一个保护团队旨在综合多个社区的传统生态知识（TEK），以估计一个关于单一资源指标的、社区层面的潜在信念。每次观测都是一个数值化的TEK判断，并附有一个专业知识得分，该得分会修正观测的可靠性。考虑一个具有以下组成部分的分层概率模型。\n\n1. 对于每个由 $c \\in \\{1, \\dots, C\\}$ 索引的社区，以及每次由 $i \\in \\{1, \\dots, n_c\\}$ 索引的观测，TEK判断 $y_{c,i}$ 被建模为来自一个高斯分布的条件独立抽样。该分布具有特定于社区的均值和经专业知识加权的方差。观测模型为：\n$$\ny_{c,i} \\mid \\theta_c \\sim \\mathcal{N}\\!\\left(\\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right),\n$$\n其中 $e_{c,i} > 0$ 是第 $c$ 个社区中第 $i$ 位信息提供者的专业知识权重，$\\sigma^2$ 是已知的。\n\n2. 社区层面的潜在均值 $\\theta_c$ 通过一个均值为 $\\mu$、方差为 $\\tau^2$ 的总体分布进行汇集：\n$$\n\\theta_c \\mid \\mu, \\tau^2 \\sim \\mathcal{N}\\!\\left(\\mu, \\tau^2\\right).\n$$\n\n3. 为防止在社区确实存在异质性时，$\\theta_c$ 向 $\\mu$ 过度收缩，应对异质性方差 $\\tau^2$ 设置一个弱信息性的重尾先验。使用参数为 $(\\alpha_0, \\beta_0)$ 的逆伽马分布：\n$$\n\\tau^2 \\sim \\mathrm{InverseGamma}\\!\\left(\\alpha_0, \\beta_0\\right),\n$$\n其中较小的 $\\alpha_0$ 和 $\\beta_0$ 值会产生更重的尾部，从而减少过度收缩的压力。\n\n4. 对总体均值设置一个弥散高斯先验：\n$$\n\\mu \\sim \\mathcal{N}\\!\\left(m_0, s_0^2\\right),\n$$\n其中超参数 $m_0$ 和 $s_0^2$ 是已知的。\n\n您可以使用的基础理论：贝叶斯定理、高斯分布的性质（包括其与已知方差高斯似然的共轭性）、作为方差参数先验的逆伽马分布的定义和性质，以及全概率定律。除这些基础理论外，您不得引入未经检验或特设的公式。\n\n任务。从第一性原理出发，为该已知 $\\sigma^2$ 的分层模型推导出一个正确的吉布斯采样器所需的全条件分布。然后，实现一个数值稳定的吉布斯采样器，该采样器应：\n- 从数据中进行合理的初始化，\n- 使用 $N = 20000$ 次总迭代和 $B = 5000$ 次的预烧期（burn-in），\n- 使用固定的伪随机数生成器种子 $s = 42$ 以保证可复现性，\n- 对每个测试用例，返回后验均值 $\\mathbb{E}[\\mu \\mid \\mathrm{data}]$ 和 $\\mathbb{E}[\\tau \\mid \\mathrm{data}]$，其中 $\\tau = \\sqrt{\\tau^2}$。\n\n所有测试用例中使用的超参数：$m_0 = 0$, $s_0^2 = 10$, $\\alpha_0 = 0.5$, $\\beta_0 = 0.5$, $\\sigma^2 = 1$。\n\n测试套件。对于下方的每个测试用例，输入数据以社区的有序列表形式给出，每个社区提供一对长度相同的列表 $(\\mathbf{y}_c, \\mathbf{e}_c)$。所有数字都是没有物理单位的纯数。\n\n- 测试用例 1：\n  - 社区 1：$\\mathbf{y}_1 = \\left[\\, 1.20, 0.90, 1.10, 1.00 \\,\\right]$, $\\mathbf{e}_1 = \\left[\\, 1.00, 0.80, 1.20, 1.10 \\,\\right]$。\n  - 社区 2：$\\mathbf{y}_2 = \\left[\\, 0.20, 0.00, -0.10 \\,\\right]$, $\\mathbf{e}_2 = \\left[\\, 0.60, 0.50, 0.70 \\,\\right]$。\n  - 社区 3：$\\mathbf{y}_3 = \\left[\\, 1.80, 1.60, 1.90, 2.10, 1.70 \\,\\right]$, $\\mathbf{e}_3 = \\left[\\, 1.40, 1.30, 1.20, 1.50, 1.30 \\,\\right]$。\n\n- 测试用例 2：\n  - 社区 1：$\\mathbf{y}_1 = \\left[\\, 0.55, 0.52, 0.58, 0.49, 0.53, 0.51, 0.56, 0.54 \\,\\right]$, $\\mathbf{e}_1 = \\left[\\, 2.50, 2.20, 2.80, 2.00, 2.40, 2.30, 2.60, 2.50 \\,\\right]$。\n  - 社区 2：$\\mathbf{y}_2 = \\left[\\, 0.90 \\,\\right]$, $\\mathbf{e}_2 = \\left[\\, 0.40 \\,\\right]$。\n  - 社区 3：$\\mathbf{y}_3 = \\left[\\, 0.10, 0.20 \\,\\right]$, $\\mathbf{e}_3 = \\left[\\, 0.70, 0.80 \\,\\right]$。\n  - 社区 4：$\\mathbf{y}_4 = \\left[\\, 0.50, 0.60, 0.40 \\,\\right]$, $\\mathbf{e}_4 = \\left[\\, 1.00, 0.90, 1.10 \\,\\right]$。\n\n- 测试用例 3：\n  - 社区 1：$\\mathbf{y}_1 = \\left[\\, -1.50, -1.60, -1.40 \\,\\right]$, $\\mathbf{e}_1 = \\left[\\, 1.20, 1.10, 1.30 \\,\\right]$。\n  - 社区 2：$\\mathbf{y}_2 = \\left[\\, 1.50, 1.60, 1.40 \\,\\right]$, $\\mathbf{e}_2 = \\left[\\, 1.20, 1.10, 1.30 \\,\\right]$。\n\n要求的程序行为。您的程序必须完全按照规定构建分层模型，仅基于基础理论推导并实现一个正确的吉布斯采样器，并使用上述给定的超参数和采样设置在测试套件上运行它。您的程序不得读取任何输入。对于每个测试用例，计算两个浮点数：后验均值 $\\mathbb{E}[\\mu \\mid \\mathrm{data}]$ 和 $\\mathbb{E}[\\tau \\mid \\mathrm{data}]$。将所有测试用例的结果按顺序汇总到一个列表中，条目排序为 $\\left[\\mathbb{E}[\\mu]_1, \\mathbb{E}[\\tau]_1, \\mathbb{E}[\\mu]_2, \\mathbb{E}[\\tau]_2, \\mathbb{E}[\\mu]_3, \\mathbb{E}[\\tau]_3\\right]$。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，$\\left[\\, r_1, r_2, r_3, r_4, r_5, r_6 \\,\\right]$）。不应打印任何其他文本。", "solution": "该问题要求为一个三级分层贝叶斯模型推导并实现一个吉布斯采样器。该模型规定如下：\n1.  观测模型：$y_{c,i} \\mid \\theta_c \\sim \\mathcal{N}\\!\\left(\\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right)$\n2.  社区层面模型：$\\theta_c \\mid \\mu, \\tau^2 \\sim \\mathcal{N}\\!\\left(\\mu, \\tau^2\\right)$\n3.  先验分布：\n    -   $\\mu \\sim \\mathcal{N}\\!\\left(m_0, s_0^2\\right)$\n    -   $\\tau^2 \\sim \\mathrm{InverseGamma}\\!\\left(\\alpha_0, \\beta_0\\right)$\n\n方差参数 $\\sigma^2$ 是已知的。目标是推断参数 $\\mu$、$\\tau^2$ 以及社区层面均值集合 $\\boldsymbol{\\theta} = \\{\\theta_c\\}_{c=1}^C$。吉布斯采样器是一种迭代的马尔可夫链蒙特卡洛（MCMC）算法，它依次从每个参数（或参数块）的全条件分布中进行采样。一个参数的全条件分布是其在所有其他参数和数据条件下的分布。\n\n首先，我们根据贝叶斯定理写出所有未知参数的联合后验分布，它正比于似然与先验的乘积：\n$$\np(\\boldsymbol{\\theta}, \\mu, \\tau^2 \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\sigma^2) p(\\boldsymbol{\\theta} \\mid \\mu, \\tau^2) p(\\mu \\mid m_0, s_0^2) p(\\tau^2 \\mid \\alpha_0, \\beta_0)\n$$\n其中 $\\mathbf{y}$ 代表所有观测值 $\\{y_{c,i}\\}$。具体表达式为：\n$$\np(\\boldsymbol{\\theta}, \\mu, \\tau^2 \\mid \\mathbf{y}) \\propto \\left( \\prod_{c=1}^C \\prod_{i=1}^{n_c} \\mathcal{N}\\!\\left(y_{c,i} \\mid \\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right) \\right) \\left( \\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu \\mid m_0, s_0^2) \\mathrm{InverseGamma}(\\tau^2 \\mid \\alpha_0, \\beta_0)\n$$\n为了推导全条件分布，我们从联合后验中分离出与目标参数相关的项，并将所有其他量视为常数。得到的表达式是该条件分布的核。\n\n**1. $\\theta_c$ 的全条件分布**\n\n特定社区均值 $\\theta_c$ 的全条件分布取决于该社区的观测值 $\\mathbf{y}_c$ 以及更高层级的参数 $\\mu$ 和 $\\tau^2$。相关项为：\n$$\np(\\theta_c \\mid \\text{rest}) \\propto p(\\mathbf{y}_c \\mid \\theta_c) p(\\theta_c \\mid \\mu, \\tau^2) = \\left( \\prod_{i=1}^{n_c} \\mathcal{N}\\!\\left(y_{c,i} \\mid \\theta_c, \\frac{\\sigma^2}{e_{c,i}}\\right) \\right) \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2)\n$$\n这是高斯密度的乘积。由于共轭性，$\\theta_c$ 的后验也将是高斯分布。为了找到其参数，我们考察指数部分，它是一个关于 $\\theta_c$ 的二次函数：\n$$\n\\log p(\\theta_c \\mid \\text{rest}) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n_c} e_{c,i}(y_{c,i} - \\theta_c)^2 - \\frac{1}{2\\tau^2}(\\theta_c - \\mu)^2\n$$\n展开平方项并收集关于 $\\theta_c^2$ 和 $\\theta_c$ 的项：\n$$\n\\propto -\\frac{1}{2} \\left[ \\theta_c^2 \\left(\\frac{\\sum_i e_{c,i}}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2\\theta_c \\left(\\frac{\\sum_i e_{c,i} y_{c,i}}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) \\right]\n$$\n这是高斯对数密度的核。通过配方法，我们可以确定后验精度 $V_c^{-1}$ 和均值 $M_c$。后验精度是 $\\theta_c^2/2$ 的系数，而后验均值是 $\\theta_c$ 的系数除以后验精度。\n$$\nV_c^{-1} = \\frac{\\sum_{i=1}^{n_c} e_{c,i}}{\\sigma^2} + \\frac{1}{\\tau^2} \\implies V_c = \\left( \\frac{\\sum_{i=1}^{n_c} e_{c,i}}{\\sigma^2} + \\frac{1}{\\tau^2} \\right)^{-1}\n$$\n$$\nM_c = V_c \\left( \\frac{\\sum_{i=1}^{n_c} e_{c,i} y_{c,i}}{\\sigma^2} + \\frac{\\mu}{\\tau^2} \\right)\n$$\n因此，每个 $\\theta_c$ 的全条件分布为：\n$$\n\\theta_c \\mid \\mathbf{y}, \\mu, \\tau^2 \\sim \\mathcal{N}(M_c, V_c)\n$$\n\n**2. $\\mu$ 的全条件分布**\n\n总体均值 $\\mu$ 的全条件分布取决于社区均值 $\\boldsymbol{\\theta}$、社区间方差 $\\tau^2$ 及其先验超参数。给定 $\\boldsymbol{\\theta}$，数据 $\\mathbf{y}$ 与 $\\mu$ 条件独立。\n$$\np(\\mu \\mid \\text{rest}) \\propto p(\\boldsymbol{\\theta} \\mid \\mu, \\tau^2) p(\\mu) = \\left( \\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu \\mid m_0, s_0^2)\n$$\n这又是一个标准的高斯-高斯更新，其中 $\\theta_c$ 值扮演着数据点的角色。对数后验为：\n$$\n\\log p(\\mu \\mid \\text{rest}) \\propto -\\frac{1}{2\\tau^2} \\sum_{c=1}^C (\\theta_c - \\mu)^2 - \\frac{1}{2s_0^2}(\\mu - m_0)^2\n$$\n对 $\\mu$ 进行配方：\n$$\n\\propto -\\frac{1}{2} \\left[ \\mu^2 \\left(\\frac{C}{\\tau^2} + \\frac{1}{s_0^2}\\right) - 2\\mu \\left(\\frac{\\sum_c \\theta_c}{\\tau^2} + \\frac{m_0}{s_0^2}\\right) \\right]\n$$\n从这个二次形式中，我们确定后验精度 $V_\\mu^{-1}$ 和均值 $M_\\mu$：\n$$\nV_\\mu^{-1} = \\frac{C}{\\tau^2} + \\frac{1}{s_0^2} \\implies V_\\mu = \\left( \\frac{C}{\\tau^2} + \\frac{1}{s_0^2} \\right)^{-1}\n$$\n$$\nM_\\mu = V_\\mu \\left( \\frac{\\sum_{c=1}^C \\theta_c}{\\tau^2} + \\frac{m_0}{s_0^2} \\right)\n$$\n$\\mu$ 的全条件分布为：\n$$\n\\mu \\mid \\boldsymbol{\\theta}, \\tau^2, \\mathbf{y} \\sim \\mathcal{N}(M_\\mu, V_\\mu)\n$$\n\n**3. $\\tau^2$ 的全条件分布**\n\n社区间方差 $\\tau^2$ 的全条件分布取决于 $\\boldsymbol{\\theta}$、$\\mu$ 及其先验超参数。\n$$\np(\\tau^2 \\mid \\text{rest}) \\propto p(\\boldsymbol{\\theta} \\mid \\mu, \\tau^2) p(\\tau^2) = \\left( \\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\right) \\mathrm{InverseGamma}(\\tau^2 \\mid \\alpha_0, \\beta_0)\n$$\n高斯密度乘积贡献了包含 $\\tau^2$ 的项：\n$$\n\\prod_{c=1}^C \\mathcal{N}(\\theta_c \\mid \\mu, \\tau^2) \\propto (\\tau^2)^{-C/2} \\exp\\left(-\\frac{1}{2\\tau^2} \\sum_{c=1}^C (\\theta_c - \\mu)^2\\right)\n$$\n$\\tau^2$ 的先验密度正比于 $(\\tau^2)^{-(\\alpha_0+1)} \\exp(-\\beta_0/\\tau^2)$。将它们结合起来，得到 $\\tau^2$ 全条件分布的核：\n$$\np(\\tau^2 \\mid \\text{rest}) \\propto (\\tau^2)^{-C/2} \\exp\\left(-\\frac{\\sum_c (\\theta_c - \\mu)^2}{2\\tau^2}\\right) \\cdot (\\tau^2)^{-(\\alpha_0+1)} \\exp(-\\beta_0/\\tau^2)\n$$\n$$\np(\\tau^2 \\mid \\text{rest}) \\propto (\\tau^2)^{-(\\alpha_0 + C/2 + 1)} \\exp\\left(-\\frac{1}{\\tau^2} \\left[ \\beta_0 + \\frac{1}{2} \\sum_{c=1}^C (\\theta_c - \\mu)^2 \\right]\\right)\n$$\n这是逆伽马分布 $\\mathrm{InverseGamma}(\\alpha^*, \\beta^*)$ 的核。通过比较，我们找到后验的形状和尺度参数：\n$$\n\\alpha^* = \\alpha_0 + \\frac{C}{2}\n$$\n$$\n\\beta^* = \\beta_0 + \\frac{1}{2} \\sum_{c=1}^C (\\theta_c - \\mu)^2\n$$\n$\\tau^2$ 的全条件分布为：\n$$\n\\tau^2 \\mid \\boldsymbol{\\theta}, \\mu, \\mathbf{y} \\sim \\mathrm{InverseGamma}(\\alpha^*, \\beta^*)\n$$\n\n**吉布斯采样器算法**\n\n吉布斯采样器通过初始化参数，然后从它们的全条件分布中迭代采样来进行。\n\n1.  **初始化**：设置 $\\mu^{(0)}$、$(\\tau^2)^{(0)}$ 和 $\\boldsymbol{\\theta}^{(0)}$ 的初始值。合理的选择是将 $\\theta_c^{(0)}$ 初始化为社区 $c$ 中观测值的专业知识加权平均值，将 $\\mu^{(0)}$ 初始化为 $\\theta_c^{(0)}$ 的平均值，并将 $(\\tau^2)^{(0)}$ 初始化为一个常数，如 $1.0$。\n\n2.  **迭代**：对于 $t = 1, \\dots, N$：\n    a. 对每个社区 $c = 1, \\dots, C$，使用 $\\mu^{(t-1)}$ 和 $(\\tau^2)^{(t-1)}$ 计算 $M_c, V_c$，然后采样 $\\theta_c^{(t)} \\sim \\mathcal{N}(M_c, V_c)$。\n    b. 使用 $\\boldsymbol{\\theta}^{(t)}$ 和 $(\\tau^2)^{(t-1)}$ 计算 $M_\\mu, V_\\mu$，然后采样 $\\mu^{(t)} \\sim \\mathcal{N}(M_\\mu, V_\\mu)$。\n    c. 使用 $\\mu^{(t)}$ 和 $\\boldsymbol{\\theta}^{(t)}$ 计算 $\\alpha^*, \\beta^*$，然后采样 $(\\tau^2)^{(t)} \\sim \\mathrm{InverseGamma}(\\alpha^*, \\beta^*)$。\n\n3.  **后验推断**：经过 $B$ 次预烧期（burn-in）迭代后，余下的 $N-B$ 个样本用于近似后验分布。任何参数的后验均值通过其在预烧期后链中的样本的算术平均值来估计。对于 $\\tau = \\sqrt{\\tau^2}$，我们在每一步计算 $\\tau^{(t)} = \\sqrt{(\\tau^2)^{(t)}}$ 并对这些值求平均。", "answer": "```python\nimport numpy as np\n\ndef run_gibbs_sampler(communities_data, sigma_sq, m0, s0_sq, alpha0, beta0, n_iter, n_burn, seed):\n    \"\"\"\n    Implements a Gibbs sampler for the specified hierarchical model.\n\n    Args:\n        communities_data (list): List of tuples, each containing (y_c, e_c) numpy arrays.\n        sigma_sq (float): known observation variance component.\n        m0 (float): Prior mean for mu.\n        s0_sq (float): Prior variance for mu.\n        alpha0 (float): Prior shape parameter for tau_sq.\n        beta0 (float): Prior scale parameter for tau_sq.\n        n_iter (int): Total number of Gibbs iterations.\n        n_burn (int): Number of burn-in iterations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing the posterior means of mu and tau.\n    \"\"\"\n    C = len(communities_data)\n    rng = np.random.default_rng(seed)\n\n    # Pre-compute sums for efficiency\n    sum_e_y = np.array([np.sum(d[0] * d[1]) for d in communities_data])\n    sum_e = np.array([np.sum(d[1]) for d in communities_data])\n    \n    # Sensible initializations\n    thetas = sum_e_y / sum_e\n    mu = np.mean(thetas)\n    tau_sq = 1.0\n\n    # Storage for posterior samples (post-burn-in)\n    n_samples = n_iter - n_burn\n    mu_samples = np.zeros(n_samples)\n    tau_samples = np.zeros(n_samples)\n\n    # Gibbs sampling loop\n    for i in range(n_iter):\n        # 1. Sample thetas\n        for c in range(C):\n            # Posterior for theta_c is Gaussian\n            # Precision from likelihood: sum_e[c] / sigma_sq\n            # Precision from prior: 1 / tau_sq\n            post_var_c = 1.0 / (sum_e[c] / sigma_sq + 1.0 / tau_sq)\n            post_mean_c = post_var_c * (sum_e_y[c] / sigma_sq + mu / tau_sq)\n            \n            thetas[c] = rng.normal(post_mean_c, np.sqrt(post_var_c))\n\n        # 2. Sample mu\n        # Posterior for mu is Gaussian\n        # Precision from likelihood on thetas: C / tau_sq\n        # Precision from prior: 1 / s0_sq\n        post_var_mu = 1.0 / (C / tau_sq + 1.0 / s0_sq)\n        post_mean_mu = post_var_mu * (np.sum(thetas) / tau_sq + m0 / s0_sq)\n        \n        mu = rng.normal(post_mean_mu, np.sqrt(post_var_mu))\n\n        # 3. Sample tau_sq\n        # Posterior for tau_sq is Inverse-Gamma\n        post_alpha = alpha0 + C / 2.0\n        post_beta = beta0 + 0.5 * np.sum((thetas - mu)**2)\n        \n        # Sample from Inverse-Gamma by sampling from Gamma and inverting\n        # Gamma(shape=alpha, scale=1/beta) corresponds to InverseGamma(shape=alpha, scale=beta)\n        gamma_sample = rng.gamma(shape=post_alpha, scale=1.0/post_beta)\n        tau_sq = 1.0 / gamma_sample\n\n        # Store samples after burn-in\n        if i >= n_burn:\n            idx = i - n_burn\n            mu_samples[idx] = mu\n            tau_samples[idx] = np.sqrt(tau_sq)\n\n    # Compute posterior means\n    post_mean_mu = np.mean(mu_samples)\n    post_mean_tau = np.mean(tau_samples)\n\n    return post_mean_mu, post_mean_tau\n\ndef solve():\n    \"\"\"\n    Main function to run the Gibbs sampler on all test cases and print results.\n    \"\"\"\n    # Hyperparameters and sampler settings\n    SIGMA_SQ = 1.0\n    M0 = 0.0\n    S0_SQ = 10.0\n    ALPHA0 = 0.5\n    BETA0 = 0.5\n    N_ITER = 20000\n    N_BURN = 5000\n    SEED = 42\n\n    # Test suite\n    test_cases = [\n        # Test case 1\n        [\n            (np.array([1.20, 0.90, 1.10, 1.00]), np.array([1.00, 0.80, 1.20, 1.10])),\n            (np.array([0.20, 0.00, -0.10]), np.array([0.60, 0.50, 0.70])),\n            (np.array([1.80, 1.60, 1.90, 2.10, 1.70]), np.array([1.40, 1.30, 1.20, 1.50, 1.30])),\n        ],\n        # Test case 2\n        [\n            (np.array([0.55, 0.52, 0.58, 0.49, 0.53, 0.51, 0.56, 0.54]), np.array([2.50, 2.20, 2.80, 2.00, 2.40, 2.30, 2.60, 2.50])),\n            (np.array([0.90]), np.array([0.40])),\n            (np.array([0.10, 0.20]), np.array([0.70, 0.80])),\n            (np.array([0.50, 0.60, 0.40]), np.array([1.00, 0.90, 1.10])),\n        ],\n        # Test case 3\n        [\n            (np.array([-1.50, -1.60, -1.40]), np.array([1.20, 1.10, 1.30])),\n            (np.array([1.50, 1.60, 1.40]), np.array([1.20, 1.10, 1.30])),\n        ],\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        mean_mu, mean_tau = run_gibbs_sampler(case_data, SIGMA_SQ, M0, S0_SQ, ALPHA0, BETA0, N_ITER, N_BURN, SEED)\n        all_results.extend([mean_mu, mean_tau])\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in all_results)}]\")\n\nsolve()\n```", "id": "2540689"}]}