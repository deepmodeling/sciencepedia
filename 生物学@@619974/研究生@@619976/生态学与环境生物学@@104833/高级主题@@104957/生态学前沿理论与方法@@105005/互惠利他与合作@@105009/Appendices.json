{"hands_on_practices": [{"introduction": "在分析互惠利他主义时，一个基础的起点是量化不同策略在重复互动中的收益。本练习将通过计算一系列经典策略（如“永远合作”(ALLC)、“永远背叛”(ALLD)和“以牙还牙”(TFT)）之间的长期折扣收益，来实践这一核心技能。通过这些计算，我们将揭示合作策略在何种条件下能够获得优势，为理解更复杂的动态奠定基础。[@problem_id:2527624]", "problem": "在一个由集合 $\\{\\text{始终背叛 (ALLD)}, \\text{始终合作 (ALLC)}, \\text{一报还一报 (TFT)}, \\text{冷酷扳机 (GRIM)}\\}$ 中的两个纯策略之间进行无限重复的囚徒困境 (PD) 博弈。单次囚徒困境的支付由 $T$ (诱惑)、$R$ (奖励)、$P$ (惩罚) 和 $S$ (傻瓜) 表示，满足 $T>R>P>S$ 和 $2R>T+S$。时间是无限期的，具有公共贴现因子 $\\delta \\in [0,1)$，每期支付通过归一化贴现值进行汇总\n$$\nV \\;=\\; (1-\\delta)\\sum_{t=1}^{\\infty} \\delta^{t-1} u_t,\n$$\n其中 $u_t$ 是第 $t$ 期的阶段支付。策略定义如下：$\\text{ALLD}$ 在每一期都选择背叛；$\\text{ALLC}$ 在每一期都选择合作；$\\text{TFT}$ 在第 1 期合作，之后重复对手上一期的实际行动；$\\text{GRIM}$ 一直合作，直到对手背叛一次，之后它将永远背叛。\n\n任务：\n- 仅使用上述基本元素，推导 $\\{\\text{ALLD}, \\text{ALLC}, \\text{TFT}, \\text{GRIM}\\}$ 中每种无序配对的归一化贴现支付的闭式表达式，这些表达式是 $\\delta$、$T$、$R$、$P$ 和 $S$ 的函数。\n- 根据这些表达式，为每对无序策略确定哪一方获得了更大的归一化贴现支付（如果所有 $\\delta$ 的支付都相等，请明确说明）。\n- 最后，求解临界贴现因子 $\\delta^{\\star} \\in [0,1]$，在该值下，$\\text{ALLD}$ 对抗 $\\text{TFT}$ 的归一化贴现支付等于 $\\text{TFT}$ 对抗 $\\text{ALLD}$ 的归一化贴现支付。请提供 $\\delta^{\\star}$ 的精确值作为最终答案（无单位）。无需四舍五入。", "solution": "在尝试求解之前，首先对问题进行严格的验证。\n\n**步骤 1：提取已知条件**\n以下信息逐字引自问题陈述：\n- 博弈类型：无限重复囚徒困境 (PD)。\n- 纯策略集合：$\\{\\text{始终背叛 (ALLD)}, \\text{始终合作 (ALLC)}, \\text{一报还一报 (TFT)}, \\text{冷酷扳机 (GRIM)}\\}$。\n- 单次囚徒困境支付：$T$ (诱惑)，$R$ (奖励)，$P$ (惩罚) 和 $S$ (傻瓜)。\n- 支付排序约束：$T>R>P>S$ 和 $2R>T+S$。\n- 时间范围：无限期，公共贴现因子为 $\\delta \\in [0,1)$。\n- 归一化贴现值：$V = (1-\\delta)\\sum_{t=1}^{\\infty} \\delta^{t-1} u_t$。\n- 策略定义：\n    - $\\text{ALLD}$：在每一期都选择背叛。\n    - $\\text{ALLC}$：在每一期都选择合作。\n    - $\\text{TFT}$：在第 1 期合作，之后重复对手上一期的实际行动。\n    - $\\text{GRIM}$：一直合作，直到对手背叛一次，之后它将永远背叛。\n- 任务：\n    1. 为每种无序配对推导归一化贴现支付的闭式表达式。\n    2. 确定每对策略中哪一方获得更大的支付。\n    3. 求解临界贴现因子 $\\delta^{\\star} \\in [0,1]$，在该值下，$\\text{ALLD}$ 对抗 $\\text{TFT}$ 的支付等于 $\\text{TFT}$ 对抗 $\\text{ALLD}$ 的支付。\n\n**步骤 2：使用提取的已知条件进行验证**\n根据既定标准评估问题的有效性。\n- **科学依据**：该问题是博弈论和演化生物学中一个标准的、典型的模型，用于研究合作的出现。重复囚徒困境、指定的策略（特别是 $\\text{TFT}$ 和 $\\text{GRIM}$）以及包括条件 $T>R>P>S$ 和 $2R>T+S$ 在内的支付结构，在科学文献中都是基础且公认的。该问题在科学上是合理的。\n- **适定性**：该问题在数学上是明确的。所有变量、函数和目标都有清晰的定义。所提供的信息是完整、一致且足以推导出所有规定任务的唯一解。\n- **客观性**：语言正式、精确，没有主观论断或含糊不清之处。\n\n**步骤 3：结论与行动**\n该问题有效。它是一个适定的、有科学依据的问题，可以利用给定信息解决。现在开始推导解答。\n\n对于一个恒定的支付流，其中对所有 $t \\geq 1$ 都有 $u_t = u$，其归一化贴现值为 $V = (1-\\delta)\\sum_{t=1}^{\\infty} \\delta^{t-1} u = (1-\\delta) u (\\frac{1}{1-\\delta}) = u$。如果一个支付流是 $(u_1, u_2, u_2, \\dots)$，其值为 $V = (1-\\delta)[u_1 + \\delta u_2 + \\delta^2 u_2 + \\dots] = (1-\\delta)[u_1 + \\frac{\\delta u_2}{1-\\delta}] = (1-\\delta)u_1 + \\delta u_2$。这些公式将用于后续计算。设 $V(A, B)$ 表示策略 $A$ 在与策略 $B$ 对抗时获得的支付。\n\n**第 1 部分：支付推导**\n需要考虑 $\\binom{4}{2} + 4 = 10$ 种可能的配对。\n\n1.  **ALLC 对抗 ALLC**：双方在所有时期都合作。各自的支付流为 $(R, R, R, \\dots)$。因此，$V(\\text{ALLC, ALLC}) = R$。\n\n2.  **ALLD 对抗 ALLD**：双方在所有时期都背叛。各自的支付流为 $(P, P, P, \\dots)$。因此，$V(\\text{ALLD, ALLD}) = P$。\n\n3.  **TFT 对抗 TFT**：双方都从合作开始。在随后的每个时期，每一方都观察到对方的合作，并相应地进行合作。支付流为 $(R, R, R, \\dots)$。因此，$V(\\text{TFT, TFT}) = R$。\n\n4.  **GRIM 对抗 GRIM**：双方都从合作开始。由于从未发生背叛，双方都无限期地合作。支付流为 $(R, R, R, \\dots)$。因此，$V(\\text{GRIM, GRIM}) = R$。\n\n5.  **ALLC 对抗 ALLD**：ALLC 始终合作，ALLD 始终背叛。ALLC 的支付流是 $(S, S, S, \\dots)$，ALLD 的支付流是 $(T, T, T, \\dots)$。\n    $V(\\text{ALLC, ALLD}) = S$。\n    $V(\\text{ALLD, ALLC}) = T$。\n\n6.  **ALLC 对抗 TFT**：ALLC 始终合作。TFT 从合作开始。TFT 观察到 ALLC 的合作并继续合作。结果是所有时期都相互合作。\n    $V(\\text{ALLC, TFT}) = R$ 且 $V(\\text{TFT, ALLC}) = R$。\n\n7.  **ALLC 对抗 GRIM**：ALLC 始终合作。GRIM 从合作开始，并且从未观察到背叛，因此它继续合作。结果是所有时期都相互合作。\n    $V(\\text{ALLC, GRIM}) = R$ 且 $V(\\text{GRIM, ALLC}) = R$。\n\n8.  **ALLD 对抗 TFT**：ALLD 始终背叛。TFT 从合作开始。\n    - 第 $t=1$ 期：TFT 选择 C，ALLD 选择 D。TFT 的支付是 $S$，ALLD 的支付是 $T$。\n    - 第 $t \\geq 2$ 期：TFT 观察到来自 $t-1$ 期的背叛并选择 D。ALLD 继续选择 D。双方的支付都是 $P$。\n    TFT 的支付流是 $(S, P, P, \\dots)$。ALLD 的支付流是 $(T, P, P, \\dots)$。\n    $V(\\text{TFT, ALLD}) = (1-\\delta)S + \\delta P$。\n    $V(\\text{ALLD, TFT}) = (1-\\delta)T + \\delta P$。\n\n9.  **ALLD 对抗 GRIM**：ALLD 始终背叛。GRIM 从合作开始。\n    - 第 $t=1$ 期：GRIM 选择 C，ALLD 选择 D。GRIM 的支付是 $S$，ALLD 的支付是 $T$。\n    - 第 $t \\geq 2$ 期：GRIM 观察到背叛并转为永久背叛。ALLD 继续选择 D。双方的支付都是 $P$。\n    支付流与 ALLD 对抗 TFT 的情况相同。\n    $V(\\text{GRIM, ALLD}) = (1-\\delta)S + \\delta P$。\n    $V(\\text{ALLD, GRIM}) = (1-\\delta)T + \\delta P$。\n\n10. **TFT 对抗 GRIM**：两种策略都从合作开始。由于没有任何一方背叛，它们在所有后续时期都合作。\n    $V(\\text{TFT, GRIM}) = R$ 且 $V(\\text{GRIM, TFT}) = R$。\n\n**第 2 部分：无序配对的支付比较**\n\n-   **ALLC 对抗 ALLD**：$V(\\text{ALLD, ALLC}) = T$ 且 $V(\\text{ALLC, ALLD}) = S$。因为 $T > S$，$\\text{ALLD}$ 获得更大的支付。\n-   **ALLC 对抗 TFT**：$V(\\text{TFT, ALLC}) = R$ 且 $V(\\text{ALLC, TFT}) = R$。对所有 $\\delta$ 而言支付相等。\n-   **ALLC 对抗 GRIM**：$V(\\text{GRIM, ALLC}) = R$ 且 $V(\\text{ALLC, GRIM}) = R$。对所有 $\\delta$ 而言支付相等。\n-   **ALLD 对抗 TFT**：$V(\\text{ALLD, TFT}) = (1-\\delta)T + \\delta P$ 且 $V(\\text{TFT, ALLD}) = (1-\\delta)S + \\delta P$。差值为 $(1-\\delta)(T-S)$。鉴于 $T > S$ 且 $\\delta \\in [0, 1)$，我们有 $1-\\delta > 0$，所以差值严格为正。因此，对所有 $\\delta \\in [0,1)$，$\\text{ALLD}$ 获得更大的支付。\n-   **ALLD 对抗 GRIM**：支付与 ALLD 对抗 TFT 的情况相同，所以对所有 $\\delta \\in [0,1)$，$\\text{ALLD}$ 获得更大的支付。\n-   **TFT 对抗 GRIM**：$V(\\text{TFT, GRIM}) = R$ 且 $V(\\text{GRIM, TFT}) = R$。对所有 $\\delta$ 而言支付相等。\n\n**第 3 部分：临界贴现因子计算**\n\n最后的任务是找到 $\\delta^{\\star} \\in [0,1]$，使得 $\\text{ALLD}$ 对抗 $\\text{TFT}$ 的归一化贴现支付等于 $\\text{TFT}$ 对抗 $\\text{ALLD}$ 的归一化贴现支付。这是对第 1 部分结果的直接应用，需要解以下方程：\n$$\nV(\\text{ALLD, TFT}) = V(\\text{TFT, ALLD})\n$$\n代入推导出的表达式：\n$$\n(1-\\delta^{\\star})T + \\delta^{\\star}P = (1-\\delta^{\\star})S + \\delta^{\\star}P\n$$\n等式两边的 $\\delta^{\\star}P$ 项相互抵消：\n$$\n(1-\\delta^{\\star})T = (1-\\delta^{\\star})S\n$$\n该方程可重排为：\n$$\n(1-\\delta^{\\star})T - (1-\\delta^{\\star})S = 0\n$$\n$$\n(1-\\delta^{\\star})(T - S) = 0\n$$\n问题指明了严格不等式 $T > S$，这意味着 $(T-S)$ 项是一个非零正常数。要使两项之积为零，至少有一项必须为零。因此，我们必须有：\n$$\n1 - \\delta^{\\star} = 0\n$$\n求解 $\\delta^{\\star}$ 得到唯一解：\n$$\n\\delta^{\\star} = 1\n$$\n此值位于问题指定的搜索集合 $\\delta^{\\star} \\in [0,1]$ 中。", "answer": "$$\\boxed{1}$$", "id": "2527624"}, {"introduction": "理解了成对互动中的收益后，一个自然的进阶问题是：这些策略在群体层面上的演化结果如何？本练习将引入演化博弈论的核心工具——复制子动态方程，来模拟合作行为的频率变化。通过推导“以牙还牙”(TFT)和“永远背叛”(ALLD)策略的期望适应度并分析系统的稳定性，我们将揭示合作在群体中建立所需克服的关键障碍。[@problem_id:2527577]", "problem": "一个无限大、充分混合的种群由两种行为策略组成。这些策略在成对、无限重复的囚徒困境（PD）博弈中相互作用，博弈的几何持续概率为 $\\delta \\in (0,1)$。单次囚徒困境博弈具有标准的支付，满足 $T>R>P>S$，其中 $T$ 是诱惑支付，$R$ 是相互合作的奖励，$P$ 是相互背叛的惩罚，$S$ 是傻瓜支付。每次重复博弈产生一个阶段性支付序列 $\\{u_t\\}_{t=1}^{\\infty}$，单个个体在单次博弈中的适应度贡献是每个阶段的几何加权平均支付，\n$$(1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}u_t.$$\n这两种策略是：\n- 一报还一报 (TFT)：在第一回合合作；此后，如果对手在上一回合合作，则本回合合作，否则背叛。\n- 始终背叛 (ALLD)：在每一回合都背叛。\n\n设 $x \\in [0,1]$ 表示种群中 TFT 策略的频率。假设随机匹配，并且适应度等于每次博弈的期望加权平均支付。频率动态遵循双策略的连续时间复制子方程。\n\n任务：\n1. 仅从上面给出的重复互动支付和复制子方程的定义出发，推导 TFT 和 ALLD 的期望适应度，将其表示为 $x$、$\\delta$ 以及 PD 支付 $T,R,P,S$ 的函数。\n2. 推导关于 $x$ 的复制子方程，并确定在 $[0,1]$ 内的所有不动点。\n3. 根据 $\\delta$ 和支付 $T,R,P,S$ 分析边界不动点和任何内部不动点的局部稳定性，并从第一性原理出发给出明确的条件。\n4. 报告内部不动点频率 $x^{\\ast}$（如果存在）的封闭形式表达式，作为 $\\delta, T, R, P, S$ 的函数，作为你的最终答案。不要简化为数值。\n\n你的最终答案必须是单一的封闭形式表达式。无需四舍五入。", "solution": "首先对问题陈述进行验证。\n\n步骤1：提取已知条件\n- 种群：无限大、充分混合。\n- 策略：一报还一报 (TFT) 和始终背叛 (ALLD)。\n- TFT 的频率：$x \\in [0,1]$。\n- 互动：成对、无限重复的囚徒困境 (PD)。\n- 持续概率：$\\delta \\in (0,1)$。\n- 单次PD支付：$T>R>P>S$。\n- 单次博弈的适应度贡献：$W = (1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}u_t$，其中 $u_t$ 是第 $t$ 阶段的支付。\n- 动态：双策略的连续时间复制子方程。\n\n步骤2：使用提取的已知条件进行验证\n该问题是演化博弈论中一个标准的、基础的模型，具体涉及合作的演化。\n- **科学基础**：该问题基于复制子方程和重复博弈的成熟框架，这是理论生物学和经济学的核心内容。囚徒困境是研究合作的经典模型。在此框架内，所有前提在事实上都是合理的。\n- **良态问题**：该问题定义清晰，包含了所有必要的参数和方程。其结构在特定条件下允许存在唯一且稳定的解。\n- **客观性**：语言精确，没有任何主观或非科学的主张。任务是分析性的，需要严谨的推导。\n- 该问题是自洽、一致的，并且不包含指令中列出的任何使其无效的缺陷。\n\n步骤3：结论与行动\n问题有效。将提供完整的解答。\n\n解答过程遵循问题陈述中指定的四个任务进行。\n\n任务1：推导期望适应度\n首先，我们确定两种策略 TFT 和 ALLD 之间每种可能的成对博弈的贴现支付。适应度贡献是几何加权平均支付，计算公式为 $(1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}u_t$。\n\n- **TFT 对 TFT**：两个个体在第一回合都合作。由于对手合作了，双方在第二回合继续合作，并无限持续下去。每个参与者的支付序列是 $\\{R, R, R, \\dots\\}$。贴现支付为：\n$$W(\\text{TFT} | \\text{TFT}) = (1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}R = (1-\\delta)R \\left(\\frac{1}{1-\\delta}\\right) = R$$\n\n- **TFT 对 ALLD**：在第一回合 ($t=1$)，TFT 合作，ALLD 背叛。TFT 获得傻瓜支付 $S$，ALLD 获得诱惑支付 $T$。在之后的所有回合 ($t \\ge 2$)，TFT 观察到上一回合的背叛并做出背叛的回应。ALLD 继续背叛。因此，从第二回合开始，两个个体都背叛，在这些回合中各获得惩罚支付 $P$。\nTFT 的支付序列为 $\\{S, P, P, \\dots\\}$，其贴现支付为：\n$$W(\\text{TFT} | \\text{ALLD}) = (1-\\delta) \\left( S \\cdot \\delta^0 + \\sum_{t=2}^{\\infty} \\delta^{t-1}P \\right) = (1-\\delta) \\left( S + P \\frac{\\delta}{1-\\delta} \\right) = (1-\\delta)S + \\delta P$$\nALLD 的支付序列为 $\\{T, P, P, \\dots\\}$，其贴现支付为：\n$$W(\\text{ALLD} | \\text{TFT}) = (1-\\delta) \\left( T \\cdot \\delta^0 + \\sum_{t=2}^{\\infty} \\delta^{t-1}P \\right) = (1-\\delta) \\left( T + P \\frac{\\delta}{1-\\delta} \\right) = (1-\\delta)T + \\delta P$$\n\n- **ALLD 对 ALLD**：两个个体从第一回合开始在每一回合都背叛。每个人的支付序列都是 $\\{P, P, P, \\dots\\}$。贴现支付为：\n$$W(\\text{ALLD} | \\text{ALLD}) = (1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}P = (1-\\delta)P \\left(\\frac{1}{1-\\delta}\\right) = P$$\n\n现在，我们可以写出每种策略的期望适应度，即所有可能博弈的平均支付。设 $x$ 为 TFT 的频率。一个 TFT 个体遇到另一个 TFT 的概率为 $x$，遇到一个 ALLD 的概率为 $1-x$。\n\nTFT 的期望适应度 $W_{\\text{TFT}}(x)$ 为：\n$$W_{\\text{TFT}}(x) = x \\cdot W(\\text{TFT} | \\text{TFT}) + (1-x) \\cdot W(\\text{TFT} | \\text{ALLD})$$\n$$W_{\\text{TFT}}(x) = xR + (1-x)((1-\\delta)S + \\delta P)$$\n\nALLD 的期望适应度 $W_{\\text{ALLD}}(x)$ 为：\n$$W_{\\text{ALLD}}(x) = x \\cdot W(\\text{ALLD} | \\text{TFT}) + (1-x) \\cdot W(\\text{ALLD} | \\text{ALLD})$$\n$$W_{\\text{ALLD}}(x) = x((1-\\delta)T + \\delta P) + (1-x)P$$\n\n任务2：复制子方程与不动点\nTFT 策略频率 $x$ 的连续时间复制子方程由下式给出：\n$$\\dot{x} = \\frac{dx}{dt} = x (W_{\\text{TFT}}(x) - \\bar{W}(x))$$\n其中 $\\bar{W}(x) = x W_{\\text{TFT}}(x) + (1-x) W_{\\text{ALLD}}(x)$ 是种群的平均适应度。该方程可以简化为：\n$$\\dot{x} = x(1-x) (W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x))$$\n让我们计算适应度差异 $W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x)$：\n$$W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x) = [xR + (1-x)((1-\\delta)S + \\delta P)] - [x((1-\\delta)T + \\delta P) + (1-x)P]$$\n按 $x$ 的幂次重新整理各项：\n$$= x(R - ((1-\\delta)T + \\delta P)) - (1-x)(P - ((1-\\delta)S + \\delta P))$$\n$$= x(R - (1-\\delta)T - \\delta P) + (1-x)(-(P - (1-\\delta)S - \\delta P))$$\n$$= x(R - T + \\delta T - \\delta P) + (1-x)( (1-\\delta)S - (1-\\delta)P )$$\n$$= x(R - T + \\delta(T-P)) + (1-x)(S-P)(1-\\delta)$$\n将此表达式展开为 $x$ 的线性函数：\n$$W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x) = x[R - T + \\delta(T-P) - (S-P)(1-\\delta)] + (S-P)(1-\\delta)$$\n$$= x[R - T + \\delta T - \\delta P - S + P + \\delta S - \\delta P] + (S-P)(1-\\delta)$$\n$$= x[R - T - S + P + \\delta(T+S-2P)] + (S-P)(1-\\delta)$$\n复制子动态的不动点是使 $\\dot{x}=0$ 的 $x \\in [0,1]$ 的值。\n从 $\\dot{x} = x(1-x)(W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x)) = 0$ 中，我们确定三个可能的不动点：\n1. $x = 0$：ALLD 的单态种群。\n2. $x = 1$：TFT 的单态种群。\n3. 一个内部不动点 $x^*$，其中 $W_{\\text{TFT}}(x^*) - W_{\\text{ALLD}}(x^*) = 0$。\n将适应度差异的线性表达式设为零，得到：\n$$x^*[R - T - S + P + \\delta(T+S-2P)] + (S-P)(1-\\delta) = 0$$\n解出 $x^*$：\n$$x^* = -\\frac{(S-P)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}$$\n$$x^* = \\frac{(P-S)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}$$\n只有当 $x^* \\in (0,1)$ 时，这个内部不动点 $x^*$ 才具有生物学意义。\n\n任务3：局部稳定性分析\n设 $f(x) = \\dot{x} = x(1-x) (W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x))$。不动点 $x_{fp}$ 的局部稳定性由导数 $f'(x_{fp})$ 的符号决定。如果 $f'(x_{fp}) < 0$，该不动点是局部稳定的。如果 $f'(x_{fp}) > 0$，它是不稳定的。\n设 $g(x) = W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x)$。那么 $f(x) = x(1-x)g(x)$。\n$f'(x) = (1-2x)g(x) + x(1-x)g'(x)$。\n\n- $x=0$ 的稳定性：\n$f'(0) = (1-0)g(0) + 0 = g(0) = W_{\\text{TFT}}(0) - W_{\\text{ALLD}}(0)$。\n在 $x=0$ 时，$W_{\\text{TFT}}(0)=(1-\\delta)S+\\delta P$ 且 $W_{\\text{ALLD}}(0)=P$。\n$f'(0) = (1-\\delta)S+\\delta P - P = (1-\\delta)(S-P)$。\n已知 $P>S$ 且 $\\delta \\in (0,1)$，我们有 $S-P<0$ 且 $1-\\delta>0$。因此，$f'(0) < 0$。\n不动点 $x=0$ 总是局部稳定的。\n\n- $x=1$ 的稳定性：\n$f'(1) = (1-2)g(1) + 0 = -g(1) = -(W_{\\text{TFT}}(1) - W_{\\text{ALLD}}(1))$。\n在 $x=1$ 时，$W_{\\text{TFT}}(1)=R$ 且 $W_{\\text{ALLD}}(1)=(1-\\delta)T+\\delta P$。\n$f'(1) = -[R - ((1-\\delta)T+\\delta P)] = (1-\\delta)T+\\delta P - R$。\n不动点 $x=1$ 是局部稳定的，如果 $f'(1) < 0$，这要求：\n$$(1-\\delta)T+\\delta P - R < 0 \\implies R > (1-\\delta)T+\\delta P$$\n这是 TFT 成为对抗 ALLD 的演化稳定策略（ESS）的著名条件。如果不等式反向，则 $x=1$ 是不稳定的。\n\n- 内部不动点 $x^*$ 的稳定性：\n对于 $x^* \\in (0,1)$，必须有 $W_{\\text{TFT}}(x^*)-W_{\\text{ALLD}}(x^*) = g(x^*) = 0$。\n在 $x^*$ 处的导数是 $f'(x^*) = x^*(1-x^*) g'(x)$。\n斜率 $g'(x)$ 是 $g(x)$ 线性表达式中 $x$ 的系数：\n$$g'(x) = R - T - S + P + \\delta(T+S-2P)$$\n所以，$f'(x^*) = x^*(1-x^*)[R - T - S + P + \\delta(T+S-2P)]$。$f'(x^*)$ 的符号由方括号中项的符号决定，该项是 $x^*$ 表达式的分母。\n内部不动点 $x^*$ 存在于 $(0,1)$ 内，当且仅当 $g(0)$ 和 $g(1)$ 的符号相反。\n我们知道 $g(0) = (S-P)(1-\\delta) < 0$。\n因此，要存在一个内部不动点，我们必须有 $g(1) > 0$。\n$g(1) = R - ((1-\\delta)T+\\delta P) > 0$，这恰好是使 $x=1$ 稳定的条件。\n所以，一个内部不动点 $x^* \\in (0,1)$ 存在，当且仅当 $R > (1-\\delta)T+\\delta P$。\n在此条件下：\n- $x^*$ 的分子是 $(P-S)(1-\\delta) > 0$。\n- 分母是 $g'(x) = g(1)-g(0) = [R - ((1-\\delta)T+\\delta P)] - [(S-P)(1-\\delta)]$。因为 $g(1)>0$ 且 $-g(0)>0$，所以分母 $g'(x)$ 为正。\n因此 $x^* > 0$。条件 $g(1)>0$ 也等价于 $x^*<1$。\n由于分母 $g'(x)$ 为正，所以 $f'(x^*) = x^*(1-x^*)g'(x) > 0$。\n这意味着如果内部不动点 $x^*$ 存在，它总是不稳定的。这就产生了一个双稳态系统，其中如果初始频率 $x_0 > x^*$，种群会演化为全 TFT ($x=1$)；如果 $x_0 < x^*$，则演化为全 ALLD ($x=0$)。\n\n任务4：报告最终答案\n最终答案是内部不动点频率 $x^*$ 的封闭形式表达式，该不动点在 $R > (1-\\delta)T + \\delta P$ 的条件下存在。\n如任务2中所推导的，该表达式为：\n$$x^* = \\frac{(P-S)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}$$\n这就是所要求的最终表达式。", "answer": "$$\n\\boxed{\\frac{(P-S)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}}\n$$", "id": "2527577"}, {"introduction": "现实世界中的互动充满了不确定性与偶然的错误。本练习将引入“噪音”（即行动执行错误，以概率 $\\epsilon$ 发生）这一现实因素，探讨它如何影响策略的长期表现。我们将使用马尔可夫链这一强大工具，来比较经典的“以牙还牙”(TFT)与更为宽容的“赢定输移”(WSLS)策略在充满错误的环境中的适应性，从而理解最优策略是如何依赖于环境条件的。[@problem_id:2527576]", "problem": "考虑一个无限期重复的囚徒困境（PD），其形式为捐赠博弈，这是生态学中研究非亲缘个体间合作的典型模型。在每一轮中，每个博弈方选择合作（$C$）或背叛（$D$）。合作以个人成本 $c>0$ 为代价，为对方带来收益 $b>0$。因此，对于行博弈方而言，单次囚徒困境的收益如下：双方合作（$CC$）得到 $R=b-c$，己方合作而对方背叛（$CD$）得到 $S=-c$，己方背叛而对方合作（$DC$）得到 $T=b$，双方背叛（$DD$）得到 $P=0$。假设 $b>c>0$，从而满足囚徒困境的要求 $T>R>P>S$。\n\n博弈方采用两种仅依赖于上一轮实际结果的记忆一策略之一：\n- “以牙还牙” (Tit-for-Tat, TFT)：当且仅当对方在上一轮中合作时，本轮才合作。\n- “赢定输移” (Win-Stay, Lose-Shift, WSLS)：如果上一轮的收益是“赢”（$R$ 或 $T$），则重复上一轮的行动；如果上一轮的收益是“输”（$P$ 或 $S$），则改变行动。\n\n执行错误是独立发生的：在每一次行动中，博弈方原意图的行动有 $\\epsilon \\in (0,1)$ 的概率被翻转（$C \\leftrightarrow D$），此错误在博弈方之间和各轮次之间是独立的。对于任何固定的策略对，四个实际联合行动状态 $\\{CC, CD, DC, DD\\}$ 上的动态构成一个时间齐次的马尔可夫链。由于对于 $0<\\epsilon<1$ 该链是有限、不可约和非周期的，因此存在唯一的平稳分布。\n\n将一个策略对的每轮长期平均收益定义为在 $\\{CC, CD, DC, DD\\}$ 上的平稳分布下的期望单次收益。如果 WSLS 自身博弈的长期平均收益超过 TFT 自身博弈的长期平均收益，则称 WSLS “优于” TFT。\n\n仅从上述定义和马尔可夫链的全概率定律出发，符号化地推导在噪声水平为 $\\epsilon$ 时，TFT-vs-TFT 和 WSLS-vs-WSLS 的平稳分布，计算它们各自的长期平均收益，并确定唯一的临界错误概率 $\\epsilon^{\\ast}$，在该概率下 WSLS 自身博弈和 TFT 自身博弈的长期平均收益相等。你的最终答案必须是 $\\epsilon^{\\ast}$ 的封闭形式值。无需舍入，且答案无单位。", "solution": "问题要求推导临界错误概率 $\\epsilon^{\\ast}$，在该概率下，以牙还牙（TFT）策略自身博弈的长期平均收益 $V_{TFT}$ 等于赢定输移（WSLS）策略自身博弈的长期平均收益 $V_{WSLS}$。该博弈是一个重复囚徒困境，其收益为 $R=b-c$, $S=-c$, $T=b$, 和 $P=0$，其中 $b>c>0$。系统的状态是两个博弈方在上一轮的联合行动，来自集合 $\\{CC, CD, DC, DD\\}$。一个意图的行动以概率 $\\epsilon \\in (0,1)$ 被翻转。\n\n首先，我们分析 TFT-vs-TFT 的互动。一个 TFT 博弈方当且仅当其伙伴在上一轮合作时才合作。给定当前状态 $(A_1, A_2)$，下一轮的意图行动 $(I_1, I_2)$ 是：\n- 从 $CC$：两个博弈方都看到了合作，所以都意图合作。意图状态是 $(C,C)$。\n- 从 $CD$：博弈方1看到了背叛，博弈方2看到了合作。意图状态是 $(D,C)$。\n- 从 $DC$：博弈方1看到了合作，博弈方2看到了背叛。意图状态是 $(C,D)$。\n- 从 $DD$：两个博弈方都看到了背叛，所以都意图背叛。意图状态是 $(D,D)$。\n\n一个意图的合作（$I_C$）以概率 $1-\\epsilon$ 成为实际的合作（$C$），以概率 $\\epsilon$ 成为实际的背叛（$D$）。一个意图的背叛（$I_D$）以概率 $1-\\epsilon$ 成为实际的 $D$，以概率 $\\epsilon$ 成为实际的 $C$。因此，状态空间 $\\{CC, CD, DC, DD\\}$ 上的马尔可夫链的转移矩阵 $M_{TFT}$ 是：\n$$\nM_{TFT} =\n\\begin{pmatrix}\n(1-\\epsilon)^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & \\epsilon^2 \\\\\n\\epsilon(1-\\epsilon) & \\epsilon^2 & (1-\\epsilon)^2 & \\epsilon(1-\\epsilon) \\\\\n\\epsilon(1-\\epsilon) & (1-\\epsilon)^2 & \\epsilon^2 & \\epsilon(1-\\epsilon) \\\\\n\\epsilon^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & (1-\\epsilon)^2\n\\end{pmatrix}\n$$\n$M_{TFT}$ 中每一列的和都是 $1$，意味着该矩阵是双随机的。对于一个不可约、非周期的有限马尔可夫链，双随机转移矩阵意味着唯一的平稳分布是均匀分布。设 $\\pi_{TFT} = (p_{CC}, p_{CD}, p_{DC}, p_{DD})$ 是该分布。因此，$p_{CC} = p_{CD} = p_{DC} = p_{DD} = \\frac{1}{4}$。\nTFT 博弈方的长期平均收益是在此平稳分布下的期望收益：\n$$V_{TFT} = p_{CC}R + p_{CD}S + p_{DC}T + p_{DD}P$$\n$$V_{TFT} = \\frac{1}{4} (b-c) + \\frac{1}{4} (-c) + \\frac{1}{4} (b) + \\frac{1}{4} (0) = \\frac{1}{4}(2b - 2c) = \\frac{b-c}{2}$$\n\n接着，我们分析 WSLS-vs-WSLS 的互动。一个 WSLS 博弈方在“赢”（收益 $R$ 或 $T$）后重复其上一次的行动，在“输”（收益 $P$ 或 $S$）后转换其行动。意图的行动是：\n- 从 $CC$：两个博弈方都获得了 $R$（赢）。两者都重复他们的行动（$C$）。意图状态是 $(C,C)$。\n- 从 $CD$：博弈方1获得了 $S$（输），博弈方2获得了 $T$（赢）。博弈方1从 $C$ 转换为 $D$，博弈方2重复 $D$。意图状态是 $(D,D)$。\n- 从 $DC$：博弈方1获得了 $T$（赢），博弈方2获得了 $S$（输）。博弈方1重复 $D$，博弈方2从 $C$ 转换为 $D$。意图状态是 $(D,D)$。\n- 从 $DD$：两者都获得了 $P$（输）。两者都将他们的行动从 $D$ 转换为 $C$。意图状态是 $(C,C)$。\n\n转移矩阵 $M_{WSLS}$ 是基于这些意图的行动构建的：\n$$\nM_{WSLS} =\n\\begin{pmatrix}\n(1-\\epsilon)^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & \\epsilon^2 \\\\\n\\epsilon^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & (1-\\epsilon)^2 \\\\\n\\epsilon^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & (1-\\epsilon)^2 \\\\\n(1-\\epsilon)^2 & \\epsilon(1-\\epsilon) & \\epsilon(1-\\epsilon) & \\epsilon^2\n\\end{pmatrix}\n$$\n设平稳分布为 $\\pi_{WSLS} = (q_{CC}, q_{CD}, q_{DC}, q_{DD})$。由于对称性，$q_{CD}=q_{DC}$。平稳性方程 $\\pi_{WSLS} M_{WSLS} = \\pi_{WSLS}$ 对状态 $CD$ 产生：\n$$q_{CD} = q_{CC}\\epsilon(1-\\epsilon) + q_{CD}\\epsilon(1-\\epsilon) + q_{DC}\\epsilon(1-\\epsilon) + q_{DD}\\epsilon(1-\\epsilon)$$\n$$q_{CD} = (q_{CC} + q_{CD} + q_{DC} + q_{DD})\\epsilon(1-\\epsilon)$$\n由于概率之和为 $1$，我们得到 $q_{CD} = \\epsilon(1-\\epsilon)$。因此，$q_{DC} = \\epsilon(1-\\epsilon)$。\n归一化条件是 $q_{CC} + q_{CD} + q_{DC} + q_{DD} = 1$，这给出 $q_{CC} + q_{DD} = 1 - 2\\epsilon(1-\\epsilon)$。\n一个更简洁的方法是使用状态聚合。设 $X=q_{CC}+q_{DD}$ 和 $Y=q_{CD}+q_{DC}$。在平稳状态下，处于状态 $CC$ 的概率是：\n$$q_{CC} = (q_{CC}+q_{DD})(1-\\epsilon)^2 + (q_{CD}+q_{DC})\\epsilon^2 = X(1-\\epsilon)^2 + Y\\epsilon^2$$\n对于 $DD$：\n$$q_{DD} = (q_{CC}+q_{DD})\\epsilon^2 + (q_{CD}+q_{DC})(1-\\epsilon)^2 = X\\epsilon^2 + Y(1-\\epsilon)^2$$\n将这些相加得到 $X = q_{CC}+q_{DD} = (X+Y)((1-\\epsilon)^2+\\epsilon^2) = 1-2\\epsilon+2\\epsilon^2$。那么 $Y=1-X=2\\epsilon-2\\epsilon^2$。这证实了 $q_{CD}=q_{DC}=\\epsilon(1-\\epsilon)$。\n两方程相减得到：\n$$q_{CC}-q_{DD} = (X-Y)((1-\\epsilon)^2-\\epsilon^2) = (X-Y)(1-2\\epsilon)$$\n代入 $X=1-2\\epsilon+2\\epsilon^2$ 和 $Y=2\\epsilon-2\\epsilon^2$ 得到 $X-Y = 1-4\\epsilon+4\\epsilon^2 = (1-2\\epsilon)^2$。\n所以，$q_{CC}-q_{DD} = (1-2\\epsilon)^2(1-2\\epsilon) = (1-2\\epsilon)^3$。\n我们得到了一个关于 $q_{CC}$ 和 $q_{DD}$ 的二元一次方程组：\n$q_{CC}+q_{DD} = 1-2\\epsilon+2\\epsilon^2$\n$q_{CC}-q_{DD} = (1-2\\epsilon)^3 = 1-6\\epsilon+12\\epsilon^2-8\\epsilon^3$\n两式相加：$2q_{CC} = 2-8\\epsilon+14\\epsilon^2-8\\epsilon^3 \\implies q_{CC} = 1-4\\epsilon+7\\epsilon^2-4\\epsilon^3$。\nWSLS 的长期平均收益是：\n$$V_{WSLS} = q_{CC}R + q_{CD}S + q_{DC}T + q_{DD}P$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(-c) + \\epsilon(1-\\epsilon)(b) + q_{DD}(0)$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(b-c) = (q_{CC} + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1-4\\epsilon+7\\epsilon^2-4\\epsilon^3 + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n\n最后，我们通过令收益相等来找到临界错误概率 $\\epsilon^{\\ast}$：\n$$V_{TFT} = V_{WSLS}$$\n$$\\frac{b-c}{2} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n因为 $b>c$，$b-c>0$，所以我们可以把它约掉：\n$$\\frac{1}{2} = 1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n$$0 = \\frac{1}{2} - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n乘以 $-2$：\n$$0 = 8\\epsilon^3 - 12\\epsilon^2 + 6\\epsilon - 1$$\n这是立方展开式：\n$$0 = (2\\epsilon - 1)^3$$\n通过令底数为零，可以找到在区间 $(0,1)$ 内的唯一实数解：\n$$2\\epsilon^{\\ast} - 1 = 0 \\implies \\epsilon^{\\ast} = \\frac{1}{2}$$\n这就是在 TFT 和 WSLS 自身博弈的长期平均收益相同时的临界错误概率。", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2527576"}]}