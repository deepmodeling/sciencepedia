{"hands_on_practices": [{"introduction": "主成分分析（Principal Component Analysis, PCA）是探索几何形态测量数据中主要变异模式的核心工具。本练习将引导你亲手实践PCA的关键步骤，从协方差矩阵的特征分解到将主成分分数反向变换回原始坐标，从而直观地“看到”形态空间坐标轴所代表的具体形态变化。通过这个过程，你将不再把形态空间视为一个抽象概念，而是理解其与生物体实际形态的直接联系 [@problem_id:2577650]。", "problem": "给定一个二维普氏叠合对齐后的标点坐标矩阵，其中每个样本都已通过广义普氏叠合分析 (GPA) 进行对齐，使得列均值为零。此外，还给出了一个表示平均形状的独立向量。标点以坐标的形式按 $\\left[x_1,y_1,x_2,y_2,\\dots,x_p,y_p\\right]$ 的顺序堆叠，因此共有 $2p$ 栏。您的目标是重构几何形态计量学中主成分分析 (PCA) 的核心步骤，并实现原始标点空间与主成分 (PC) 形态空间之间的可逆转换。\n\n使用的基本原理：\n- 经验协方差矩阵的正交对角化。对于一个中心化的数据矩阵 $\\mathbf{Y}\\in\\mathbb{R}^{n\\times d}$（其中 $d=2p$），经验协方差为 $\\mathbf{S}=\\frac{1}{n-1}\\mathbf{Y}^\\top\\mathbf{Y}$。由于 $\\mathbf{S}$ 是对称半正定矩阵，它可以进行特征分解 $\\mathbf{S}=\\mathbf{V}\\,\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)\\,\\mathbf{V}^\\top$，其中 $\\mathbf{v}_k$ 是正交归一列，$\\lambda_k$ 是非负特征值。\n- 在正交归一基中的坐标。对于任何中心化向量 $\\mathbf{y}\\in\\mathbb{R}^d$，其在特征基中的坐标是该向量与基向量的内积。\n\n需要实现的任务（请根据这些原理推导，不要使用任何提供给您的简化公式）：\n1. 计算 $\\mathbf{S}$ 的特征分解，并将特征对排序，使得 $\\lambda_1\\ge \\lambda_2\\ge \\dots\\ge \\lambda_d\\ge 0$。对每个特征向量 $\\mathbf{v}_k$ 强制执行一个确定性的符号约定：找到绝对值最大的坐标索引 $j^\\star$，如果 $\\left(\\mathbf{v}_k\\right)_{j^\\star}<0$，则将 $\\mathbf{v}_k$ 乘以 $-1$。\n2. 将 $\\mathbf{Y}$ 的每一行在正交归一特征基 $\\mathbf{V}$ 中表示，从而形成所有样本的 PC 分数。\n3. 实现沿单一 PC 轴 $k$ 到原始标点空间的逆转换：给定平均形状向量 $\\mathbf{m}\\in\\mathbb{R}^{d}$、轴索引 $k$ 以及一个以标准差为单位的纯量 $t$（即 $\\sqrt{\\lambda_k}$ 的 $t$ 倍），构建形变后的形状\n$$\n\\mathbf{x}(t)\\;=\\;\\mathbf{m}\\;+\\;t\\,\\sqrt{\\lambda_k}\\,\\mathbf{v}_k.\n$$\n这种形变应可解释为在 PC 形态空间中沿轴 $k$ 移动 $t$ 个标准差，然后逆转换回原始标点坐标。\n\n本问题不涉及角度单位，也不涉及任何物理单位。所有输出均为无因次的实数。当指定舍入指令时，请四舍五入至小数点后 $6$ 位。\n\n测试套组与所需输出：\n采用列堆叠顺序 $\\left[x_1,y_1,x_2,y_2,\\dots\\right]$。\n\n- 测试案例 A（具有两种非零变异模式的理想路径）：\n  - 令 $n=4$，$p=3$，因此 $d=6$。\n  - 平均形状 $\\mathbf{m}_A=\\left[0,0,1,0,0,1\\right]$。\n  - 数据矩阵 $\\mathbf{Y}_A\\in\\mathbb{R}^{4\\times 6}$，其行为\n    - $\\left[0,0,-2,0,0,0\\right]$,\n    - $\\left[0,0,2,0,0,0\\right]$,\n    - $\\left[0,0,0,0,0,-1\\right]$,\n    - $\\left[0,0,0,0,0,1\\right]$。\n  - 使用第一个 PC 轴（按特征值降序排序后 $k=0$）。\n  - 对于 $t$ 值 $[0.0,\\,1.0,\\,-1.0,\\,2.5]$，逆转换至 $\\mathbf{x}(t)$ 并回报标点 2 的 $x$ 坐标（即向量中索引为 2 的坐标）。输出一个包含四个浮点数的列表，每个数四舍五入至小数点后 6 位。\n\n- 测试案例 B（对每个标点的欧几里得效应进行类边界检查）：\n  - 令 $n=2$，$p=2$，因此 $d=4$。\n  - 平均形状 $\\mathbf{m}_B=\\left[2,2,0,0\\right]$。\n  - 数据矩阵 $\\mathbf{Y}_B\\in\\mathbb{R}^{2\\times 4}$，其行为\n    - $\\left[\\frac{3}{\\sqrt{2}},\\frac{3}{\\sqrt{2}},0,0\\right]$,\n    - $\\left[-\\frac{3}{\\sqrt{2}},-\\frac{3}{\\sqrt{2}},0,0\\right]$。\n  - 使用第一个 PC 轴（$k=0$）。\n  - 计算 $\\mathbf{x}(+1)$ 和 $\\mathbf{x}(-1)$，并仅回传标点 1 的两个位置之间的欧几里得距离，即使用 $x_1,y_1$ 分量在 $\\mathbb{R}^2$ 中的范数。回传一个四舍五入至小数点后 6 位的单一浮点数。\n\n- 测试案例 C（在次要轴上的正向与反向转换的一致性）：\n  - 重用测试案例 A 的 $\\left(\\mathbf{Y}_A,\\mathbf{m}_A\\right)$。\n  - 使用第二个 PC 轴（排序后 $k=1$）。\n  - 设定 $t=-1.5$。构建 $\\mathbf{x}(t)$ 并计算中心化向量 $\\mathbf{y}(t)=\\mathbf{x}(t)-\\mathbf{m}_A$。将 $\\mathbf{y}(t)$ 投影到轴 $k$上（与 $\\mathbf{v}_k$ 做内积），以获得该轴上的纯量 PC 分数。回传此纯量，四舍五入至小数点后 6 位。\n\n最终输出格式：\n您的程序应产生单行输出，其中包含以逗号分隔并用方括号括起来的结果列表，顺序为 $\\left[\\text{A 的结果},\\text{B 的结果},\\text{C 的结果}\\right]$。A 的结果本身是一个包含四个浮点数的列表。例如，您的输出应类似于 $\\left[[a_1,a_2,a_3,a_4],b,c\\right]$，其中每个符号代表如上所述的数值。", "solution": "此问题定义明确，具备科学基础，并包含唯一解所需的所有信息。其中描述的几何形态计量学中的主成分分析 (PCA) 程序是标准且数学上正确的。我们将按照指定的任务来解决这个问题。\n\n分析的核心是经验协方差矩阵 $\\mathbf{S}$ 的特征分解。给定一个中心化的数据矩阵 $\\mathbf{Y} \\in \\mathbb{R}^{n \\times d}$，其中 $n$ 是样本数量，$d=2p$ 是标点坐标的数量，协方差矩阵定义为：\n$$\n\\mathbf{S} = \\frac{1}{n-1}\\mathbf{Y}^\\top\\mathbf{Y}\n$$\n$\\mathbf{S}$ 的特征值 $\\lambda_k$ 代表数据沿对应主成分 (PC) 轴的变异数，而特征向量 $\\mathbf{v}_k$ 定义了这些轴在原始 $d$ 维标点空间中的方向。这些特征向量构成一个正交归一基。具有中心化坐标 $\\mathbf{y}_i$ 的样本的 PC 分数，是其在这个新基底中的坐标，通过内积 $\\mathbf{y}_i \\cdot \\mathbf{v}_k$ 计算得出。\n\n可以通过从平均形状 $\\mathbf{m}$ 沿著特定 PC 轴 $k$ 移动来重构形状。沿轴 $k$ 移动 $t$ 个标准差的位移，对应于将向量 $t\\sqrt{\\lambda_k}\\mathbf{v}_k$ 加到平均形状向量上。沿轴 $k$ 的分数的标准差为 $\\sqrt{\\lambda_k}$。因此，重构的形状 $\\mathbf{x}(t)$ 由下式给出：\n$$\n\\mathbf{x}(t) = \\mathbf{m} + t\\sqrt{\\lambda_k}\\mathbf{v}_k\n$$\n我们将把这些原理应用于每个测试案例。\n\n**测试案例 A**\n\n已知：一个包含 $n=4$ 个样本、$p=3$ 个标点的样本，因此维度为 $d=2 \\times 3=6$。\n平均形状向量为 $\\mathbf{m}_A = [0, 0, 1, 0, 0, 1]^\\top$。\n中心化的数据矩阵为：\n$$\n\\mathbf{Y}_A = \\begin{pmatrix} 0 & 0 & -2 & 0 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & -1 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{pmatrix}\n$$\n首先，我们计算矩阵 $\\mathbf{Y}_A^\\top\\mathbf{Y}_A$：\n$$\n\\mathbf{Y}_A^\\top\\mathbf{Y}_A = \\begin{pmatrix} 0 & 0 & -2 & 0 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & -1 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{pmatrix}^\\top \\begin{pmatrix} 0 & 0 & -2 & 0 & 0 & 0 \\\\ 0 & 0 & 2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & -1 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 8 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 2 \\end{pmatrix}\n$$\n协方差矩阵 $\\mathbf{S}_A$ 为：\n$$\n\\mathbf{S}_A = \\frac{1}{4-1}\\mathbf{Y}_A^\\top\\mathbf{Y}_A = \\frac{1}{3}\\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 8 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 8/3 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 2/3 \\end{pmatrix}\n$$\n由于 $\\mathbf{S}_A$ 是一个对角矩阵，其特征值为对角线上的元素，其特征向量为标准基向量。按降序排序，非零的特征值及其对应的特征向量为：\n$\\lambda_0 = 8/3$ 及其特征向量 $\\mathbf{v}_0 = [0, 0, 1, 0, 0, 0]^\\top$。\n$\\lambda_1 = 2/3$ 及其特征向量 $\\mathbf{v}_1 = [0, 0, 0, 0, 0, 1]^\\top$。\n其余的特征值均为 $0$。\n根据符号约定，需要检查绝对值最大的元素。对于 $\\mathbf{v}_0$ 和 $\\mathbf{v}_1$，这个元素都是 $1$（一个正值），因此它们的符号不变。\n\n我们使用第一个 PC 轴（$k=0$）。逆转换为：\n$$\n\\mathbf{x}(t) = \\mathbf{m}_A + t\\sqrt{\\lambda_0}\\,\\mathbf{v}_0 = [0, 0, 1, 0, 0, 1]^\\top + t\\sqrt{8/3}\\,[0, 0, 1, 0, 0, 0]^\\top = [0, 0, 1 + t\\sqrt{8/3}, 0, 0, 1]^\\top\n$$\n我们需要找到标点 2 的 $x$ 坐标，即 $\\mathbf{x}(t)$ 中索引为 2 的元素。此坐标为 $1 + t\\sqrt{8/3}$。\n对于 $t \\in [0.0, 1.0, -1.0, 2.5]$：\n- $t=0.0$: $1 + 0.0\\sqrt{8/3} = 1.0$\n- $t=1.0$: $1 + 1.0\\sqrt{8/3} \\approx 1 + 1.632993 = 2.632993$\n- $t=-1.0$: $1 - 1.0\\sqrt{8/3} \\approx 1 - 1.632993 = -0.632993$\n- $t=2.5$: $1 + 2.5\\sqrt{8/3} \\approx 1 + 4.082483 = 5.082483$\n结果四舍五入至小数点后 6 位，为 $[1.000000, 2.632993, -0.632993, 5.082483]$。\n\n**测试案例 B**\n\n已知：$n=2$ 个样本，$p=2$ 个标点，$d=4$。\n平均形状：$\\mathbf{m}_B = [2, 2, 0, 0]^\\top$。\n数据矩阵：\n$$\n\\mathbf{Y}_B = \\begin{pmatrix} 3/\\sqrt{2} & 3/\\sqrt{2} & 0 & 0 \\\\ -3/\\sqrt{2} & -3/\\sqrt{2} & 0 & 0 \\end{pmatrix}\n$$\n协方差矩阵为 $\\mathbf{S}_B = \\frac{1}{2-1}\\mathbf{Y}_B^\\top\\mathbf{Y}_B = \\mathbf{Y}_B^\\top\\mathbf{Y}_B$：\n$$\n\\mathbf{S}_B = \\begin{pmatrix} 3/\\sqrt{2} & -3/\\sqrt{2} \\\\ 3/\\sqrt{2} & -3/\\sqrt{2} \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 3/\\sqrt{2} & 3/\\sqrt{2} & 0 & 0 \\\\ -3/\\sqrt{2} & -3/\\sqrt{2} & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 9 & 9 & 0 & 0 \\\\ 9 & 9 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\n左上角 $2\\times2$ 区块 $\\begin{pmatrix} 9 & 9 \\\\ 9 & 9 \\end{pmatrix}$ 的特征值 $\\lambda$ 可由特征方程式 $(9-\\lambda)^2 - 81 = 0$ 求得，解得 $\\lambda = 18$ 与 $\\lambda = 0$。\n最大特征值为 $\\lambda_0 = 18$。对应的特征向量 $\\mathbf{v}_0 = [c_1, c_2, 0, 0]^\\top$ 满足 $(9-18)c_1 + 9c_2 = 0$，即 $-9c_1 + 9c_2 = 0$，意味着 $c_1=c_2$。为了归一化， $c_1^2 + c_2^2 = 1 \\implies 2c_1^2 = 1 \\implies c_1 = 1/\\sqrt{2}$。因此，$\\mathbf{v}_0 = [1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top$。\n绝对值最大的元素是正数，因此无需改变符号。\n\n使用第一个 PC 轴（$k=0$）。逆转换为：\n$$\n\\mathbf{x}(t) = \\mathbf{m}_B + t\\sqrt{\\lambda_0}\\,\\mathbf{v}_0 = [2, 2, 0, 0]^\\top + t\\sqrt{18}\\,[1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top\n$$\n由于 $\\sqrt{18} = 3\\sqrt{2}$，此式可简化为：\n$$\n\\mathbf{x}(t) = [2, 2, 0, 0]^\\top + t(3\\sqrt{2})\\,[1/\\sqrt{2}, 1/\\sqrt{2}, 0, 0]^\\top = [2, 2, 0, 0]^\\top + t[3, 3, 0, 0]^\\top = [2+3t, 2+3t, 0, 0]^\\top\n$$\n我们计算 $\\mathbf{x}(+1)$ 和 $\\mathbf{x}(-1)$：\n$\\mathbf{x}(+1) = [2+3, 2+3, 0, 0]^\\top = [5, 5, 0, 0]^\\top$。\n$\\mathbf{x}(-1) = [2-3, 2-3, 0, 0]^\\top = [-1, -1, 0, 0]^\\top$。\n标点 1 的坐标是前两个元素。当 $t=+1$ 时，位置是 $(5, 5)$。当 $t=-1$ 时，位置是 $(-1, -1)$。\n这两个位置之间的欧几里得距离是：\n$$\nd = \\sqrt{(5 - (-1))^2 + (5 - (-1))^2} = \\sqrt{6^2 + 6^2} = \\sqrt{72} \\approx 8.48528137\n$$\n四舍五入至小数点后 6 位，得到 $8.485281$。\n\n**测试案例 C**\n\n我们重用案例 A 的数据和特征分解。我们使用第二个 PC 轴（$k=1$），其特征值为 $\\lambda_1 = 2/3$，特征向量为 $\\mathbf{v}_1 = [0, 0, 0, 0, 0, 1]^\\top$。给定 $t=-1.5$。\n首先，我们构建形状 $\\mathbf{x}(t)$：\n$$\n\\mathbf{x}(-1.5) = \\mathbf{m}_A + (-1.5)\\sqrt{\\lambda_1}\\,\\mathbf{v}_1\n$$\n接着，我们求中心化向量 $\\mathbf{y}(t) = \\mathbf{x}(t) - \\mathbf{m}_A$：\n$$\n\\mathbf{y}(-1.5) = (\\mathbf{m}_A - 1.5\\sqrt{\\lambda_1}\\mathbf{v}_1) - \\mathbf{m}_A = -1.5\\sqrt{\\lambda_1}\\mathbf{v}_1\n$$\n问题要求计算 PC 分数，即此向量在轴 $\\mathbf{v}_1$ 上的投影。这可通过计算内积 $\\mathbf{y}(-1.5) \\cdot \\mathbf{v}_1$ 来得到：\n$$\n\\text{Score} = (-1.5\\sqrt{\\lambda_1}\\mathbf{v}_1) \\cdot \\mathbf{v}_1 = -1.5\\sqrt{\\lambda_1}(\\mathbf{v}_1 \\cdot \\mathbf{v}_1)\n$$\n由于 $\\mathbf{v}_1$ 是单位向量，$\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = 1$。分数可简化为：\n$$\n\\text{Score} = -1.5\\sqrt{\\lambda_1} = -1.5\\sqrt{2/3} \\approx -1.5 \\times 0.81649658 \\approx -1.22474487\n$$\n这证明了一致性：一个通过沿轴 $k$ 移动 $t$ 个标准差而构建的形状，在该轴上的 PC 分数恰好是 $t\\sqrt{\\lambda_k}$。\n四舍五入至小数点后 6 位，得到 $-1.224745$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the geometric morphometrics problem by implementing PCA steps\n    and back-transformation for three test cases.\n    \"\"\"\n\n    def perform_pca_and_get_components(Y, n):\n        \"\"\"\n        Computes covariance matrix, its eigendecomposition, and sorts\n        and normalizes the components according to the problem statement.\n        \"\"\"\n        d = Y.shape[1]\n        \n        # Handle the case n=1 where variance is undefined, though not in test cases\n        if n <= 1:\n            return np.zeros(d), np.eye(d)\n\n        # 1. Compute the covariance matrix S\n        S = (1 / (n - 1)) * (Y.T @ Y)\n\n        # 2. Eigendecomposition of S. eigh is for symmetric matrices.\n        # It returns eigenvalues in ascending order.\n        eigvals, eigvecs = np.linalg.eigh(S)\n\n        # 3. Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigvals)[::-1]\n        sorted_eigvals = eigvals[sorted_indices]\n        sorted_eigvecs = eigvecs[:, sorted_indices]\n        \n        # 4. Enforce deterministic sign convention on eigenvectors\n        for k in range(d):\n            vk = sorted_eigvecs[:, k]\n            max_abs_idx = np.argmax(np.abs(vk))\n            if vk[max_abs_idx] < 0:\n                sorted_eigvecs[:, k] = -vk\n        \n        return sorted_eigvals, sorted_eigvecs\n\n    # --- Test Case A ---\n    n_A = 4\n    m_A = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0])\n    Y_A = np.array([\n        [0.0, 0.0, -2.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 2.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0, -1.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n    ])\n    \n    eigvals_A, eigvecs_A = perform_pca_and_get_components(Y_A, n_A)\n    \n    # First PC axis (k=0)\n    lambda0_A = eigvals_A[0]\n    v0_A = eigvecs_A[:, 0]\n    \n    t_values_A = [0.0, 1.0, -1.0, 2.5]\n    results_A = []\n    \n    for t in t_values_A:\n        # Back-transform to shape coordinates\n        x_t = m_A + t * np.sqrt(lambda0_A) * v0_A\n        # Get x-coordinate of landmark 2 (index 2)\n        coord = x_t[2]\n        results_A.append(round(coord, 6))\n\n    # --- Test Case B ---\n    n_B = 2\n    m_B = np.array([2.0, 2.0, 0.0, 0.0])\n    Y_B = np.array([\n        [3/np.sqrt(2), 3/np.sqrt(2), 0.0, 0.0],\n        [-3/np.sqrt(2), -3/np.sqrt(2), 0.0, 0.0]\n    ])\n\n    eigvals_B, eigvecs_B = perform_pca_and_get_components(Y_B, n_B)\n\n    # First PC axis (k=0)\n    lambda0_B = eigvals_B[0]\n    v0_B = eigvecs_B[:, 0]\n\n    # Compute shapes at t=+1 and t=-1\n    x_plus_1 = m_B + 1.0 * np.sqrt(lambda0_B) * v0_B\n    x_minus_1 = m_B + (-1.0) * np.sqrt(lambda0_B) * v0_B\n    \n    # Landmark 1 coordinates are at indices 0 and 1\n    lm1_plus_1 = x_plus_1[:2]\n    lm1_minus_1 = x_minus_1[:2]\n    \n    # Euclidean distance between the two landmark 1 positions\n    dist_B = np.linalg.norm(lm1_plus_1 - lm1_minus_1)\n    result_B = round(dist_B, 6)\n\n    # --- Test Case C ---\n    \n    # Reuse PCA results from Case A\n    # Second PC axis (k=1)\n    lambda1_A = eigvals_A[1]\n    v1_A = eigvecs_A[:, 1]\n    \n    t_C = -1.5\n    \n    # The centered vector y(t) represents the deviation from the mean\n    y_t = t_C * np.sqrt(lambda1_A) * v1_A\n    \n    # Project y(t) onto the PC axis v1 to get the score.\n    # Mathematically, this simplifies to t * sqrt(lambda_k).\n    score_C = np.dot(y_t, v1_A)\n    result_C = round(score_C, 6)\n\n    # --- Final Output Formatting ---\n    # The required format is [[a1, a2, a3, a4], b, c]\n    # To match the no-space format in the list for A:\n    str_A = f\"[{','.join(f'{x:.6f}' for x in results_A)}]\"\n    str_B = f'{result_B:.6f}'\n    str_C = f'{result_C:.6f}'\n\n    print(f\"[{str_A},{str_B},{str_C}]\")\n\nsolve()\n\n```", "id": "2577650"}, {"introduction": "在对形态变异进行生物学解释之前，我们必须首先评估并量化测量误差，这是确保研究结论可靠性的前提。本练习将介绍如何使用普氏方差分析（Procrustes ANOVA）这一强大工具，将总的形态变异分解为个体间（代表真实生物学差异）和个体内（代表测量误差）的方差组分。计算重复性（repeatability）是验证任何形态测量研究有效性的关键步骤，它告诉我们测量数据在多大程度上反映了真实的生物学信号 [@problem_id:2577647]。", "problem": "在一项旨在量化几何形态计量学测量误差的单一物种叶形研究中，您收集了 $n$ 个数字化标本，并由同一操作员为每个标本获取了 $k$ 个独立的重复地标构型。经过广义普氏分析(GPA)后，您拟合了一个单因素普氏方差分析(ANOVA)模型，其中包含随机效应“个体”以及代表数字化误差的个体内残差。采用形态坐标的标准单因素随机效应模型，\n$Y_{ij}=\\mu + A_i + E_{ij}$，\n其中 $A_i$ 和 $E_{ij}$ 相互独立，$A_i \\sim \\mathcal{N}(0,\\sigma^2_{\\text{indiv}})$ 且 $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2_{\\text{error}})$。假设设计是平衡的，并且形态坐标间的误差是各向同性的，因此通常的随机效应均方期望适用。\n\n个体和误差项的普氏ANOVA摘要（对所有形态坐标求和）如下：\n- 个体平方和 $SS_{\\text{indiv}}=0.120$，自由度 $df_{\\text{indiv}}=n-1=14$，\n- 误差平方和 $SS_{\\text{error}}=0.090$，自由度 $df_{\\text{error}}=n(k-1)=30$，\n其中 $n=15$ 且 $k=3$。\n\n根据单因素随机效应模型和普氏ANOVA的基本原理，选择正确说明了如何从ANOVA中估计方差分量 $\\sigma^2_{\\text{indiv}}$ 和 $\\sigma^2_{\\text{error}}$ 的选项，并给出相应的形态测量重复性（组内相关系数）的数值估计，该重复性定义为可归因于个体间差异的方差比例。\n\nA. 使用均方和标准的单因素随机效应矩量法进行估计：$\\hat{\\sigma}^2_{\\text{indiv}}=(MS_{\\text{indiv}}-MS_{\\text{error}})/k$，$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}$，因此 $\\hat{R}=\\hat{\\sigma}^2_{\\text{indiv}}/(\\hat{\\sigma}^2_{\\text{indiv}}+\\hat{\\sigma}^2_{\\text{error}})=13/34\\approx 0.382$。\n\nB. 直接将个体均方视为个体间方差，并将误差均方除以重复次数：$\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}$，$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}/k$，因此 $\\hat{R}\\approx 0.895$。\n\nC. 忽略重复次数，将均方之差作为个体间方差：$\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}-MS_{\\text{error}}$，$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}$，因此 $\\hat{R}\\approx 0.650$。\n\nD. 直接使用平方和而非均方，因为普氏ANOVA对坐标求和：$\\hat{\\sigma}^2_{\\text{indiv}}=(SS_{\\text{indiv}}-SS_{\\text{error}})/k$，$\\hat{\\sigma}^2_{\\text{error}}=SS_{\\text{error}}$，因此 $\\hat{R}=0.100$。", "solution": "我们从单因素随机效应模型 $Y_{ij}=\\mu + A_i + E_{ij}$ 开始，其中 $A_i \\sim \\mathcal{N}(0,\\sigma^2_{\\text{indiv}})$ 和 $E_{ij} \\sim \\mathcal{N}(0,\\sigma^2_{\\text{error}})$ 相互独立。在每个个体有 $k$ 次重复的平衡设计中，单因素随机效应方差分析(ANOVA)中均方的经典期望为\n$$\\mathbb{E}[MS_{\\text{error}}]=\\sigma^2_{\\text{error}},\\quad \\mathbb{E}[MS_{\\text{indiv}}]=\\sigma^2_{\\text{error}}+k\\,\\sigma^2_{\\text{indiv}}.$$\n这些期望是根据独立性和相等重复次数下的方差分解得出的。在普氏ANOVA的背景下，平方和是在广义普氏分析(GPA)之后对所有形态坐标进行汇总的。在各向同性误差的假设下，各坐标间的比例关系是恒定的，因此这些关系对每个坐标都适用，也同样适用于汇总的均方。因此，矩量法估计量为\n$$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}},\\quad \\hat{\\sigma}^2_{\\text{indiv}}=\\frac{MS_{\\text{indiv}}-MS_{\\text{error}}}{k}.$$\n\n根据给定的平方和与自由度计算均方：\n$$MS_{\\text{indiv}}=\\frac{SS_{\\text{indiv}}}{df_{\\text{indiv}}}=\\frac{0.120}{14}=\\frac{3}{350}\\approx 0.0085714,$$\n$$MS_{\\text{error}}=\\frac{SS_{\\text{error}}}{df_{\\text{error}}}=\\frac{0.090}{30}=\\frac{3}{1000}=0.003.$$\n\n则\n$$\\hat{\\sigma}^2_{\\text{indiv}}=\\frac{MS_{\\text{indiv}}-MS_{\\text{error}}}{k}=\\frac{\\frac{3}{350}-\\frac{3}{1000}}{3}=\\frac{\\frac{60}{7000}-\\frac{21}{7000}}{3}=\\frac{\\frac{39}{7000}}{3}=\\frac{13}{7000}\\approx 0.0018571,$$\n$$\\hat{\\sigma}^2_{\\text{error}}=MS_{\\text{error}}=\\frac{3}{1000}=0.003.$$\n\n重复性（组内相关系数）是可归因于个体间差异的总方差比例：\n$$\\hat{R}=\\frac{\\hat{\\sigma}^2_{\\text{indiv}}}{\\hat{\\sigma}^2_{\\text{indiv}}+\\hat{\\sigma}^2_{\\text{error}}}=\\frac{\\frac{13}{7000}}{\\frac{13}{7000}+\\frac{3}{1000}}=\\frac{\\frac{13}{7000}}{\\frac{34}{7000}}=\\frac{13}{34}\\approx 0.3823529.$$\n\n逐项分析：\n\n- 选项A：该选项使用了从具有 $k$ 次重复的单因素随机效应模型中的 $\\mathbb{E}[MS_{\\text{error}}]$ 和 $\\mathbb{E}[MS_{\\text{indiv}}]$ 推导出的正确的矩量法估计量。数值计算与推导相符：$MS_{\\text{indiv}}=\\frac{3}{350}$，$MS_{\\text{error}}=\\frac{3}{1000}$，得到 $\\hat{R}=\\frac{13}{34}\\approx 0.382$。结论 — 正确。\n\n- 选项B：该选项错误地设定 $\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}$ 并将误差均方除以 $k$。期望 $\\mathbb{E}[MS_{\\text{indiv}}]=\\sigma^2_{\\text{error}}+k\\,\\sigma^2_{\\text{indiv}}$ 表明，$MS_{\\text{indiv}}$ 因 $\\sigma^2_{\\text{error}}$ 而被夸大，因此不能直接估计 $\\sigma^2_{\\text{indiv}}$。将 $MS_{\\text{error}}$ 除以 $k$ 也是不合理的，因为 $\\mathbb{E}[MS_{\\text{error}}]=\\sigma^2_{\\text{error}}$。因此，所得的 $\\hat{R}\\approx 0.895$ 是基于一个有缺陷的程序。结论 — 错误。\n\n- 选项C：该选项设定了 $\\hat{\\sigma}^2_{\\text{indiv}}=MS_{\\text{indiv}}-MS_{\\text{error}}$，但省略了除以 $k$ 的步骤。从期望 $\\mathbb{E}[MS_{\\text{indiv}}]-\\mathbb{E}[MS_{\\text{error}}]=k\\,\\sigma^2_{\\text{indiv}}$ 可知，正确的估计量必须除以 $k$。因此，得出的 $\\hat{R}\\approx 0.650$ 高估了重复性。结论 — 错误。\n\n- 选项D：该选项使用平方和而非均方来估计方差分量。方差分量与均方的期望有关，而与平方和无关，并且必须考虑自由度。使用平方和会得到量纲不一致且有偏的估计；计算出的 $\\hat{R}=0.100$ 不符合恰当的基本原理。结论 — 错误。", "answer": "$$\\boxed{A}$$", "id": "2577647"}, {"introduction": "几何形态测量学的一个核心应用是检验驱动形态演化的生物学假设，其中最常见的就是异速生长（allometry），即形态随体型大小变化的规律。本高级练习将超越简单的线性关系，指导你如何使用多项式回归和样条回归等方法来建模复杂的非线性异速生长轨迹。通过实践赤池信息准则（Akaike Information Criterion, AIC）进行模型选择，并应用置换检验进行稳健的统计推断，你将掌握一套完整的、用于检验复杂生物学假设的分析流程 [@problem_id:2577673]。", "problem": "您将实现一个完整且可测试的程序，用于在形态测量学背景下，通过使用形态变异的投影和体型代理变量来检测和建模非线性异速生长。在几何形态测量学中，重心大小 (Centroid Size, CS) 是一种常用的体型度量，而异速生长通常被建模为形态变异随重心大小对数变化的函数。为了本任务的目的，我们考虑一个简化但科学上合理的设定：形态变异由少量正交形态得分 (例如, 普氏叠置后的地标点构型的前几个主成分 (Principal Components, PC)) 概括，预测变量为重心大小的对数。缩写 CS 代表重心大小 (Centroid Size)，PC 代表主成分 (Principal Component)，AIC 代表赤池信息准则 (Akaike Information Criterion)。\n\n从以下基本基础开始：\n- 经过广义普氏分析后的形态得分可以建模为多元响应 $Y \\in \\mathbb{R}^{n \\times p}$，其中 $n$ 是样本数量，$p$ 是形态轴的数量。\n- 预测变量为 $L = \\log(CS) \\in \\mathbb{R}^{n}$。\n- 多元多重回归通过 $Y = X B + E$ 将 $Y$ 与设计矩阵 $X \\in \\mathbb{R}^{n \\times k}$ 联系起来，其中 $B \\in \\mathbb{R}^{k \\times p}$ 是系数矩阵，$E$ 是残差。\n- 在经过充分检验的、残差为多元正态分布且样本间协方差恒定的假设下，可以使用基于似然的信息准则和非参数置换检验进行模型比较。\n- 可以使用位于内部节点 $k_{j}$ 处的截断线性 (铰链) 函数从 $L$ 构建回归样条基，其中 $(x)_{+} = \\max(0, x)$。\n\n您的任务是编写一个程序，该程序能够：\n1. 在指定的异速生长生成模型下，以 $L = \\log(CS)$ 作为输入，模拟多元形态数据 $Y$。其中，$L$ 在指定区间上均匀抽样，$Y$ 通过向 $L$ 的一个确定性函数添加高斯噪声而生成。该确定性均值函数可以包含沿形态空间中正交归一方向投影的 $L$ 的线性、多项式及样条项。\n2. 将四个备选多元模型拟合到作为 $L$ 的函数的 $Y$：\n   - 模型 1 (线性)：列为 $[1, L]$，\n   - 模型 2 (二次)：列为 $[1, L, L^{2}]$，\n   - 模型 3 (三次)：列为 $[1, L, L^{2}, L^{3}]$，\n   - 模型 4 (样条)：列为 $[1, L, (L - k_{1})_{+}, (L - k_{2})_{+}]$，其中 $k_{1}$ 和 $k_{2}$ 是 $L$ 的经验第 33.3 和 66.7 百分位数。\n3. 为每个拟合模型计算一个源自多元正态模型的、基于似然的信息准则，并根据以下规则选择模型：找出四个模型中的最小准则值，然后在与最小值差距 $\\Delta \\leq 2$ 的模型中，选择最简单的模型 (即 $X$ 中列数最少的模型)。\n4. 使用 Freedman–Lane 残差置换框架，对超出线性模型范围的额外非线性进行置换检验：对于选定的最佳非线性模型 (如果所选模型是模型 2、模型 3 或模型 4)，通过将响应和非线性设计相对于简化线性模型进行残差化后，计算由新增非线性项解释的多元平方和作为检验统计量；然后通过将简化模型的残差在样本间置换 $B$ 次并每次重新计算该统计量，来近似零分布。将 $P$ 值计算为蒙特卡洛尾部概率。如果所选模型是线性的，则报告 $P$ 值为 1.0。\n5. 为一组预定义的参数集测试套件生成最终输出。每个测试用例的输出必须是一个二元列表 $[m, p]$，其中 $m$ 是所选模型的标识符，为 $\\{1,2,3,4\\}$ 中的一个整数，$p$ 是置换检验的 $P$ 值，为一个四舍五入到六位小数的浮点数。最终的程序输出必须是包含这些用例结果列表的单行文本，即一个由方括号括起来的、由二元列表组成的类 JSON 列表。\n\n数学和算法要求：\n- 信息准则的推导必须始于具有恒定列协方差的 $E$ 的多元正态似然，以及该模型下的最大似然估计。\n- 置换检验必须专门评估非线性项在线性模型之外提供的额外拟合优度，其方法应使用以简化线性设计为条件的残差化 (Freedman–Lane)，并采用一个统计量，该统计量是残差化的 $Y$ 在残差化的非线性设计上的投影的弗罗贝尼乌斯范数的平方。\n- 不涉及角度。不涉及物理单位。不报告百分比；$P$ 值必须是一个十进制数。\n\n测试套件：\n为了可复现性，每个用例都为随机数生成器指定了一个种子。对于所有用例，设 $(x)_{+} = \\max(0, x)$，并从实现的 $L$ 值中，分别计算在分位数 $q = 1/3$ 和 $q = 2/3$ 处的经验分位数作为样条节点 $k_{1}$ 和 $k_{2}$。\n\n- 用例 1 (理想情况，线性异速生长)：\n  - 种子: $1729$,\n  - $n = 120$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.5, 3.0)$,\n  - 确定性均值：沿一个正交归一方向，线性效应振幅为 $0.6$，无高阶项，\n  - 噪声：独立的、零均值高斯分布，在 $p$ 个维度上标准差均为 $\\sigma = 0.05$。\n\n- 用例 2 (带曲率的非线性异速生长)：\n  - 种子: $20211$,\n  - $n = 140$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.3, 3.1)$,\n  - 确定性均值：沿一个正交归一方向，二次效应振幅为 $0.5$，线性效应可忽略不计，\n  - 噪声: $\\sigma = 0.05$。\n\n- 用例 3 (高阶非线性)：\n  - 种子: $314159$,\n  - $n = 160$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.3, 3.1)$,\n  - 确定性均值：沿一个正交归一方向，三次效应振幅为 $0.35$，\n  - 噪声: $\\sigma = 0.05$。\n\n- 用例 4 (由样条捕捉的分段异速生长)：\n  - 种子: $271828$,\n  - $n = 150$, $p = 3$,\n  - $L \\sim \\mathrm{Uniform}(1.4, 3.0)$,\n  - 确定性均值：具有样条状效应，包含两个铰链项 $(L - k_{1})_{+}$ 和 $(L - k_{2})_{+}$, 它们沿正交归一方向的振幅分别为 $0.8$ 和 $-0.6$，并带有一个振幅为 $0.1$ 的微小基线线性效应，\n  - 噪声: $\\sigma = 0.06$。\n\n实现细节：\n- 通过对一个随机高斯矩阵 $\\mathbb{R}^{p \\times p}$ 应用 QR 分解，并使用其 $p$ 个正交归一列来构建正交归一的效应方向。\n- 对于样条模型，使用如上定义的、在 $k_{1}$ 和 $k_{2}$ 处带有两个铰链的截断线性基。\n- 在所有用例中，置换检验均使用 $B = 499$ 次置换。\n- 为了在评估信息准则时保持数值稳定性，如果估计的残差协方差矩阵接近奇异，可以在计算其对数行列式之前添加一个小的岭项 $\\epsilon I$，其中 $\\epsilon = 10^{-9}$。\n\n您的程序应生成单行输出，其中包含一个类 JSON 格式的用例结果列表，每个用例结果是如上所述的二元列表 $[m, p]$ (例如，$[[1,0.842000],[2,0.010000],\\dots]$)。确保输出中的 $P$ 值四舍五入到六位小数。", "solution": "我们从几何形态测量学中建立形态与协变量 (如重心大小 (CS) 的对数) 之间函数关系时所使用的标准多元多重回归模型开始。设 $Y \\in \\mathbb{R}^{n \\times p}$ 为 $n$ 个样本的 $p$ 个形态得分矩阵，这可以看作是经过普氏叠置的形态坐标的主导主成分 (PC)。设 $L \\in \\mathbb{R}^{n}$ 为预测向量，其元素为 $L_{i} = \\log(CS_{i})$。一个通用的线性模型为\n$$\nY = X B + E,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times k}$ 是一个设计矩阵，$B \\in \\mathbb{R}^{k \\times p}$ 是系数矩阵，$E \\in \\mathbb{R}^{n \\times p}$ 是残差。我们假设 $E$ 的行是独立同分布的，服从均值为零、列协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$ 的多元正态分布。\n\n我们考虑四个对应不同设计矩阵的备选模型：\n- 模型 1: $X = [\\mathbf{1}, L]$，有 $k = 2$ 列。\n- 模型 2: $X = [\\mathbf{1}, L, L^{2}]$，有 $k = 3$ 列。\n- 模型 3: $X = [\\mathbf{1}, L, L^{2}, L^{3}]$，有 $k = 4$ 列。\n- 模型 4: $X = [\\mathbf{1}, L, (L - k_{1})_{+}, (L - k_{2})_{+}]$，一个有 $k = 4$ 列的截断线性样条基，其中 $k_{1}$ 和 $k_{2}$ 是位于经验分位数 $q = 1/3$ 和 $q = 2/3$ 处的内部节点，且 $(x)_{+} = \\max(0, x)$。\n\n对于每个模型，在多元正态残差假设下，$B$ 的最大似然估计为\n$$\n\\hat{B} = (X^{\\top} X)^{-1} X^{\\top} Y,\n$$\n残差矩阵为\n$$\n\\hat{E} = Y - X \\hat{B}.\n$$\n列协方差的最大似然估计为\n$$\n\\hat{\\Sigma} = \\frac{1}{n}\\, \\hat{E}^{\\top} \\hat{E}.\n$$\n在行协方差为单位矩阵、列协方差为 $\\Sigma$ 的矩阵正态模型下，具有设计矩阵 $X$ 的模型的最大化对数似然计算为\n$$\n\\hat{\\ell} = -\\frac{n}{2}\\left[ p \\log(2\\pi) + \\log\\left|\\hat{\\Sigma}\\right| + p \\right],\n$$\n这是将 $\\hat{\\Sigma}$ 代入似然函数后得到的结果。赤池信息准则 (Akaike Information Criterion (AIC)) 定义为\n$$\n\\mathrm{AIC} = 2 \\, \\#\\text{parameters} - 2 \\hat{\\ell}.\n$$\n在这种多元设定中，参数数量是回归参数和对称协方差矩阵中自由参数的总和，即 $k p + \\frac{p(p+1)}{2}$。由于在所有备选模型中 $p$ 是相同的，协方差的附加项在 AIC 差异中会抵消，但为求完整和正确，我们在计算中仍包含此项。在数值上，为稳定地计算 $\\log|\\hat{\\Sigma}|$，我们通过 $\\mathrm{slogdet}(\\hat{\\Sigma})$ 计算一个符号-对数行列式对，并在出现近奇异情况时，向 $\\hat{\\Sigma}$ 添加一个小的岭项 $\\epsilon I$ (其中 $\\epsilon = 10^{-9}$)。\n\n模型选择规则：为四个模型中的每一个计算 $\\mathrm{AIC}$，找到 $\\mathrm{AIC}_{\\min}$，然后在满足 $\\mathrm{AIC} \\leq \\mathrm{AIC}_{\\min} + 2$ 的模型中选择最简单的模型 (即 $X$ 中列数 $k$ 最少的模型)。\n\n超出线性范围的非线性置换检验：为检验非线性项在模型 1 (线性) 之外的额外贡献，我们使用 Freedman–Lane 残差置换框架。设 $X_{r} = [\\mathbf{1}, L]$ 为简化线性设计，其投影矩阵为\n$$\nP_{r} = X_{r} (X_{r}^{\\top} X_{r})^{-1} X_{r}^{\\top}, \\quad M_{r} = I - P_{r}.\n$$\n设所选的最佳非线性模型的设计矩阵为 $X_{f} = [X_{r}, Z]$，其中 $Z$ 包含新增的非线性列，例如多项式模型中的 $L^{2}$ 和 $L^{3}$，或样条模型中的铰链函数。对响应和新增的非线性设计进行残差化：\n$$\nY_{r} = M_{r} Y, \\quad Z_{r} = M_{r} Z.\n$$\n定义到 $Z_{r}$ 的列空间上的投影为\n$$\nP_{Z} = Z_{r} (Z_{r}^{\\top} Z_{r})^{-1} Z_{r}^{\\top}.\n$$\n一个用于衡量新增项所赋予的额外拟合度的自然多元检验统计量是残差化响应投影的弗罗贝尼乌斯范数的平方：\n$$\nT_{\\mathrm{obs}} = \\| P_{Z} Y_{r} \\|_{F}^{2} = \\mathrm{trace}\\left( Y_{r}^{\\top} P_{Z}^{\\vphantom{\\top}} Y_{r} \\right).\n$$\n为了生成一个保留了以简化模型为条件下的结构的零分布，我们在样本间对 $Y_{r}$ 的行进行置换。对于 $\\{1,\\dots,n\\}$ 的每个置换 $\\pi$，通过相应地置换 $Y_{r}$ 的行来构成 $Y_{r}^{(\\pi)}$，计算\n$$\nT^{(\\pi)} = \\| P_{Z} Y_{r}^{(\\pi)} \\|_{F}^{2},\n$$\n并对 $B$ 次置换重复此过程以构建经验零分布。蒙特卡洛单侧 $P$ 值为\n$$\np = \\frac{1 + \\#\\{ T^{(\\pi)} \\geq T_{\\mathrm{obs}} \\}}{1 + B}.\n$$\n如果选择的模型是线性的，则没有新增的非线性项需要检验，按照惯例我们报告 $p = 1.0$。\n\n测试套件的数据生成：对每个用例，我们在指定区间上均匀生成 $L$，通过对一个随机高斯矩阵应用 QR 分解来构建一个正交归一基 $V = [v_{1}, v_{2}, v_{3}] \\in \\mathbb{R}^{p \\times p}$，并将确定性均值函数定义为沿特定基向量的项的总和，例如 $v_{1} L$、$v_{2} L^{2}$、$v_{3} L^{3}$ 或乘以固定振幅的铰链函数 $(L - k_{j})_{+}$。观测到的 $Y$ 是确定性均值加上每个维度方差为 $\\sigma^{2}$ 的高斯噪声。样条节点 $k_{1}$ 和 $k_{2}$ 是根据每个用例实现的 $L$ 值，在分位数 $q = 1/3$ 和 $q = 2/3$ 处凭经验计算得出。测试套件中指定的种子用于初始化伪随机数生成器以保证可复现性。\n\n算法实现：\n- 对于每个用例，使用提供的种子、$n$、$p$、$L$ 范围、振幅和 $\\sigma$ 按上述方式模拟 $(Y, L)$。\n- 对于四个备选模型中的每一个，构建 $X$，计算 $\\hat{B}$、$\\hat{E}$、$\\hat{\\Sigma}$，并利用最大多元正态对数似然计算 $\\mathrm{AIC}$。\n- 根据相对于最小 $\\mathrm{AIC}$ 的 $\\Delta \\leq 2$ 简约性法则选择模型。\n- 如果所选模型是非线性的，则构造 $Z$ 并用 $B = 499$ 次置换计算 Freedman–Lane $P$ 值；否则设 $p = 1.0$。\n- 为每个用例报告一个对 $[m, p]$，其中 $m \\in \\{1,2,3,4\\}$ 是所选模型的标识符，$p$ 四舍五入到六位小数。\n- 打印一行包含这些用例结果列表的文本，并用方括号括起来。\n\n此程序通过用 $L = \\log(CS)$ 的多项式或样条项来增强线性模型，并使用基于似然的信息准则和一个直接评估非线性在线性效应之外的增量贡献的置换检验来论证模型选择的合理性，从而将一个有原则的非线性异速生长检验付诸实践。四个用例中的模拟设置覆盖了线性的“理想情况”、二次曲率、高阶三次行为以及分段样条状异速生长，同时通过固定的种子和明确定义的参数来确保可复现性。", "answer": "```python\nimport numpy as np\n\n# No other external libraries are used; SciPy is not required for this implementation.\n\ndef orthonormal_basis(p, rng):\n    \"\"\"Generate an orthonormal basis in R^p via QR decomposition.\"\"\"\n    A = rng.normal(size=(p, p))\n    Q, _ = np.linalg.qr(A)\n    # Ensure right-handed orientation for determinism if desired\n    return Q\n\ndef design_matrices(L, model_id, knots=None):\n    \"\"\"Construct design matrix X for given model_id.\n    model_id: 1 linear, 2 quadratic, 3 cubic, 4 spline (linear truncated at knots).\"\"\"\n    n = L.shape[0]\n    ones = np.ones((n, 1))\n    Lc = L.reshape(-1, 1)\n    if model_id == 1:\n        X = np.hstack([ones, Lc])\n    elif model_id == 2:\n        X = np.hstack([ones, Lc, Lc**2])\n    elif model_id == 3:\n        X = np.hstack([ones, Lc, Lc**2, Lc**3])\n    elif model_id == 4:\n        if knots is None or len(knots) != 2:\n            raise ValueError(\"Knots required for spline model.\")\n        k1, k2 = knots\n        h1 = np.maximum(0.0, Lc - k1)\n        h2 = np.maximum(0.0, Lc - k2)\n        X = np.hstack([ones, Lc, h1, h2])\n    else:\n        raise ValueError(\"Unknown model_id\")\n    return X\n\ndef multivariate_aic(Y, X, ridge=1e-9):\n    \"\"\"Compute AIC for multivariate multiple regression under MVN residuals.\"\"\"\n    n, p = Y.shape\n    # OLS coefficients\n    XtX = X.T @ X\n    try:\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n        XtX_inv = np.linalg.pinv(XtX)\n    B_hat = XtX_inv @ X.T @ Y\n    E = Y - X @ B_hat\n    # Residual covariance MLE\n    Sigma_hat = (E.T @ E) / n\n    # Stabilize if necessary\n    try:\n        sign, logdet = np.linalg.slogdet(Sigma_hat)\n        if sign <= 0:\n            raise np.linalg.LinAlgError\n    except np.linalg.LinAlgError:\n        Sigma_hat = Sigma_hat + ridge * np.eye(p)\n        sign, logdet = np.linalg.slogdet(Sigma_hat)\n        if sign <= 0:\n            # As a last resort, use small positive diagonal\n            Sigma_hat = ridge * np.eye(p)\n            sign, logdet = 1.0, p * np.log(ridge)\n    # Maximized log-likelihood\n    ll = - (n / 2.0) * (p * np.log(2.0 * np.pi) + logdet + p)\n    # Parameter count: regression + covariance (symmetric)\n    k = X.shape[1]\n    param_count = k * p + (p * (p + 1)) // 2\n    aic = 2.0 * param_count - 2.0 * ll\n    return aic, B_hat, E\n\ndef freedman_lane_pvalue(Y, L, best_model_id, B=499, knots=None, seed=0):\n    \"\"\"Permutation p-value for added nonlinearity beyond linear using Freedman-Lane.\"\"\"\n    n, p = Y.shape\n    # Reduced model (linear)\n    Xr = design_matrices(L, 1)\n    # Full model components\n    if best_model_id == 1:\n        return 1.0  # No nonlinearity to test\n    Xf = design_matrices(L, best_model_id, knots=knots)\n    # Identify added columns Z beyond reduced Xr\n    # For model 2: added L^2; model 3: added L^2, L^3; model 4: added hinges h1,h2\n    Z = Xf[:, Xr.shape[1]:]\n    # Residualize both Y and Z w.r.t. Xr\n    XtX = Xr.T @ Xr\n    try:\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n        XtX_inv = np.linalg.pinv(XtX)\n    Pr = Xr @ XtX_inv @ Xr.T\n    Mr = np.eye(n) - Pr\n    Yr = Mr @ Y\n    Zr = Mr @ Z\n    # If Zr is rank-deficient or zero, return p=1.0\n    ZtZ = Zr.T @ Zr\n    # Handle degeneracy\n    try:\n        ZtZ_inv = np.linalg.inv(ZtZ)\n    except np.linalg.LinAlgError:\n        # If singular, use pseudo-inverse; if near-zero, treat as no added effect\n        if np.allclose(ZtZ, 0.0):\n            return 1.0\n        ZtZ_inv = np.linalg.pinv(ZtZ)\n    Pz = Zr @ ZtZ_inv @ Zr.T\n    # Observed statistic\n    T_obs = np.linalg.norm(Pz @ Yr, ord='fro')**2\n    # Permutations\n    rng = np.random.default_rng(seed)\n    count_ge = 0\n    for _ in range(B):\n        perm = rng.permutation(n)\n        Yperm = Yr[perm, :]\n        T_perm = np.linalg.norm(Pz @ Yperm, ord='fro')**2\n        if T_perm >= T_obs - 1e-12:\n            count_ge += 1\n    pval = (1 + count_ge) / (1 + B)\n    return pval\n\ndef simulate_case(case_params):\n    \"\"\"Simulate Y and L for one case per provided parameters and amplitudes.\"\"\"\n    seed = case_params['seed']\n    n = case_params['n']\n    p = case_params['p']\n    Lmin, Lmax = case_params['L_range']\n    sigma = case_params['sigma']\n    truth = case_params['truth']  # dict with amplitudes and type info\n    rng = np.random.default_rng(seed)\n\n    # Generate L uniformly\n    L = rng.uniform(Lmin, Lmax, size=n)\n    # Orthonormal basis for effects\n    V = orthonormal_basis(p, rng)\n    v1, v2, v3 = V[:, 0], V[:, 1], V[:, 2]\n\n    # Compute spline knots for truth if needed\n    k1 = np.quantile(L, 1/3)\n    k2 = np.quantile(L, 2/3)\n\n    # Deterministic mean function\n    mean_Y = np.zeros((n, p))\n    # Optional intercept along v1 with small amplitude for realism\n    intercept_amp = truth.get('intercept_amp', 0.0)\n    if intercept_amp != 0.0:\n        mean_Y += intercept_amp * np.outer(np.ones(n), v1)\n\n    # Linear component\n    lin_amp = truth.get('lin_amp', 0.0)\n    if lin_amp != 0.0:\n        mean_Y += lin_amp * np.outer(L, v1)\n\n    # Quadratic component\n    quad_amp = truth.get('quad_amp', 0.0)\n    if quad_amp != 0.0:\n        mean_Y += quad_amp * np.outer(L**2, v2)\n\n    # Cubic component\n    cubic_amp = truth.get('cubic_amp', 0.0)\n    if cubic_amp != 0.0:\n        mean_Y += cubic_amp * np.outer(L**3, v3)\n\n    # Spline (hinge) components\n    h1 = np.maximum(0.0, L - k1)\n    h2 = np.maximum(0.0, L - k2)\n    h1_amp = truth.get('h1_amp', 0.0)\n    if h1_amp != 0.0:\n        mean_Y += h1_amp * np.outer(h1, v3)  # project on v3\n    if h2_amp != 0.0:\n        mean_Y += h2_amp * np.outer(h2, v2)  # project on v2\n\n    # Add Gaussian noise\n    Y = mean_Y + rng.normal(scale=sigma, size=(n, p))\n    return Y, L, (k1, k2)\n\ndef fit_and_select(Y, L, knots):\n    \"\"\"Fit four candidate models, compute AIC, and select model by delta <= 2 and parsimony.\"\"\"\n    aics = []\n    ks = []\n    for mid in [1, 2, 3, 4]:\n        X = design_matrices(L, mid, knots=knots)\n        aic, B_hat, E = multivariate_aic(Y, X)\n        aics.append(aic)\n        ks.append(X.shape[1])\n    aics = np.array(aics)\n    ks = np.array(ks)\n    min_aic = np.min(aics)\n    # Candidates within delta <= 2\n    within = np.where(aics <= min_aic + 2.0)[0]\n    # Select simplest (fewest columns)\n    min_k = np.min(ks[within])\n    simplest = within[ks[within] == min_k]\n    # If multiple, choose the one with the smallest AIC among them\n    if simplest.size > 1:\n        idx = simplest[np.argmin(aics[simplest])]\n    else:\n        idx = simplest[0]\n    selected_model_id = int(idx + 1)  # model IDs are 1-based\n    return selected_model_id, aics\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'seed': 1729,\n            'n': 120,\n            'p': 3,\n            'L_range': (1.5, 3.0),\n            'sigma': 0.05,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.6,\n                'quad_amp': 0.0,\n                'cubic_amp': 0.0,\n                'h1_amp': 0.0,\n                'h2_amp': 0.0,\n            },\n        },\n        {\n            'seed': 20211,\n            'n': 140,\n            'p': 3,\n            'L_range': (1.3, 3.1),\n            'sigma': 0.05,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.0,\n                'quad_amp': 0.5,\n                'cubic_amp': 0.0,\n                'h1_amp': 0.0,\n                'h2_amp': 0.0,\n            },\n        },\n        {\n            'seed': 314159,\n            'n': 160,\n            'p': 3,\n            'L_range': (1.3, 3.1),\n            'sigma': 0.05,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.0,\n                'quad_amp': 0.0,\n                'cubic_amp': 0.35,\n                'h1_amp': 0.0,\n                'h2_amp': 0.0,\n            },\n        },\n        {\n            'seed': 271828,\n            'n': 150,\n            'p': 3,\n            'L_range': (1.4, 3.0),\n            'sigma': 0.06,\n            'truth': {\n                'intercept_amp': 0.0,\n                'lin_amp': 0.1,\n                'quad_amp': 0.0,\n                'cubic_amp': 0.0,\n                'h1_amp': 0.8,\n                'h2_amp': -0.6,\n            },\n        },\n    ]\n\n    results = []\n    # Permutation settings\n    B_perms = 499\n    for i, case in enumerate(test_cases):\n        # Simulate data\n        Y, L, knots = simulate_case(case)\n        # Fit and select model by AIC with parsimony\n        selected_model_id, aics = fit_and_select(Y, L, knots)\n        # Permutation test for added nonlinearity beyond linear if applicable\n        # Use a distinct seed per case for permutations for reproducibility\n        perm_seed = case['seed'] + 12345\n        pval = freedman_lane_pvalue(Y, L, selected_model_id, B=B_perms, knots=knots, seed=perm_seed)\n        # Round p-value to six decimals\n        pval_rounded = round(float(pval), 6)\n        results.append([selected_model_id, pval_rounded])\n\n    # Final print statement in the exact required format.\n    # Print as a single-line JSON-like list of per-case results.\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2577673"}]}