{"hands_on_practices": [{"introduction": "在分析空间转录组数据之前，理解技术本身的物理限制至关重要。此练习将探讨空间捕获点（spot）的物理布局，包括其直径和间距，如何从根本上决定了实验的空间分辨率。通过这个练习，您将把几何计算与采样理论中的基本原理（如奈奎斯特-香农采样定理）联系起来，从而深入理解数据采集的局限性。[@problem_id:2753018]", "problem": "在一个空间转录组学 (ST) 实验中，捕获点被排列在蚀刻于载玻片上的六边形晶格上。每个捕获点是一个圆形的捕获寡核苷酸斑块，用于结合放置在其上的组织中的核糖核酸 (RNA)。对于载玻片上一个理想化的、均匀覆盖的区域，我们做如下假设：相邻捕获点的中心距为 $p = 100\\,\\mu\\mathrm{m}$，每个捕获点的直径为 $d = 55\\,\\mu\\mathrm{m}$，且该晶格为无限、理想周期性的六边形（三角形）晶格。将覆盖分数 $f$ 定义为直接位于捕获点上方的组织区域所占的比例（即圆形捕获点区域的并集与总面积之比），此定义基于组织完全覆盖阵列且每个捕获点的贡献恰好为其几何足迹的理想化假设。\n\n仅使用几何定义和晶格周期性，推导 $f$ 的表达式，并根据上面给出的 $p$ 和 $d$ 的值计算其数值。将覆盖分数以纯小数形式报告（不带百分号），并四舍五入到四位有效数字。\n\n然后，运用采样理论和空间平均的基本原理，就 $p$ 和 $d$ 的值如何限制解析大脑中薄层状结构（例如，厚度与 $p$ 和 $d$ 相当或更小的皮层）的能力，给出一个有理有据的论证。你的论证应基于 Nyquist-Shannon 采样定理以及由有限点直径产生的部分体积效应的定义。你的最终数值答案应仅为覆盖分数的值，格式如上所述。", "solution": "所提出的问题表述清晰，具有科学依据，并包含了进行严谨求解所需的充分信息。我们将首先处理覆盖分数的几何计算，然后就系统参数所施加的分辨率限制给出一个形式化的论证。\n\n问题要求计算覆盖分数 $f$，其定义为捕获点所占面积与总面积之比。对于一个无限、理想周期性的六边形晶格，这个全局比率等同于单个捕获点的面积与晶格晶胞面积之比。\n\n六边形晶格由基矢定义，并且可以由菱形晶胞铺满。每个晶胞包含一个晶格点。设相邻捕获点之间的中心距为 $p$。这对应于菱形晶胞的边长。该菱形的内角为 $\\frac{\\pi}{3}$ 弧度 ($60^\\circ$) 和 $\\frac{2\\pi}{3}$ 弧度 ($120^\\circ$)。这样一个晶胞的面积 $A_{\\text{cell}}$ 由菱形面积公式给出，它也可以看作是两个边长为 $p$ 的等边三角形的拼接。\n边长为 $p$ 的等边三角形的面积是 $\\frac{\\sqrt{3}}{4} p^2$。菱形晶胞由两个这样的三角形组成，所以其面积为：\n$$A_{\\text{cell}} = 2 \\times \\left(\\frac{\\sqrt{3}}{4} p^2\\right) = \\frac{\\sqrt{3}}{2} p^2$$\n\n每个捕获点是一个直径为 $d$ 的圆。捕获点的半径为 $r = \\frac{d}{2}$。因此，单个捕获点的面积 $A_{\\text{spot}}$ 为：\n$$A_{\\text{spot}} = \\pi r^2 = \\pi \\left(\\frac{d}{2}\\right)^2 = \\frac{\\pi d^2}{4}$$\n问题说明中心距为 $p = 100\\,\\mu\\mathrm{m}$，捕获点直径为 $d = 55\\,\\mu\\mathrm{m}$。由于中心距 $p$ 大于捕获点直径 $d$，所以捕获点之间不重叠。\n\n覆盖分数 $f$ 是 $A_{\\text{spot}}$ 与 $A_{\\text{cell}}$ 的比值。\n$$f = \\frac{A_{\\text{spot}}}{A_{\\text{cell}}} = \\frac{\\frac{\\pi d^2}{4}}{\\frac{\\sqrt{3} p^2}{2}}$$\n简化此表达式，我们得到：\n$$f = \\frac{\\pi d^2}{2\\sqrt{3} p^2}$$\n\n现在，我们代入给定的数值：$p = 100\\,\\mu\\mathrm{m}$ 和 $d = 55\\,\\mu\\mathrm{m}$。\n$$f = \\frac{\\pi (55)^2}{2\\sqrt{3} (100)^2} = \\frac{3025 \\pi}{20000 \\sqrt{3}}$$\n其数值计算结果为：\n$$f \\approx \\frac{3025 \\times 3.14159265}{2 \\times 1.7320508 \\times 10000} \\approx \\frac{9503.3188}{34641.016} \\approx 0.274333$$\n按要求四舍五入到四位有效数字，覆盖分数为 $f \\approx 0.2743$。\n\n接下来，我们必须就绘制大脑中薄层状结构的物理分辨率限制提供一个有理有据的论证。该技术的空间分辨率受到两个基本参数的限制：采样间距 $p$ 和捕获点直径 $d$。\n\n1.  **来自采样间距 ($p$) 和 Nyquist-Shannon 采样定理的限制**：\n    捕获点的排列构成了一个离散的空间采样网格。解析空间特征的能力由 Nyquist-Shannon 采样定理决定。在其一维形式中，该定理指出，要无损地重建一个信号，采样频率必须至少是信号中存在的最高频率的两倍。在空间背景下，波长为 $\\lambda$ 的周期性特征具有 $k = \\frac{1}{\\lambda}$ 的空间频率。采样间隔 $p$ 对可以被无歧义地解析的空间频率施加了一个上限。最高可解析频率，即奈奎斯特频率，是 $k_{\\text{Nyquist}} = \\frac{1}{2p}$。这对应于最小可解析波长，或特征尺寸，为 $\\lambda_{\\text{min}} = 2p$。\n    对于给定的间距 $p=100\\,\\mu\\mathrm{m}$，理论上的最小可解析特征尺寸为 $\\lambda_{\\text{min}} = 2 \\times 100\\,\\mu\\mathrm{m} = 200\\,\\mu\\mathrm{m}$。\n    任何厚度小于此 $\\lambda_{\\text{min}}$ 的生物结构，例如皮层，都会被欠采样。这种欠采样会导致混叠现象，即高空间频率信息（精细细节）被错误地表示为低空间频率信息（粗糙伪影）。因此，如果一个高基因表达的薄层恰好落在采样点之间，它可能会被漏掉；或者其空间轮廓会发生扭曲，可能表现为一个更宽、强度更低的条带，或与相邻层的信号合并。精细的解剖结构因此而丢失。\n\n2.  **来自捕获点直径 ($d$) 和部分体积效应的限制**：\n    捕获点的有限尺寸引入了另一种限制。每个捕获点并非测量一个无穷小点的基因表达，而是捕获并平均其直径为 $d = 55\\,\\mu\\mathrm{m}$ 的圆形区域内所有细胞的 RNA 含量。这个过程等效于一个空间低通滤波器。这种圆形孔径的传递函数在空间频率约为 $\\frac{1.22}{d}$ 处有其第一个零点。\n    这种平均化导致了“部分体积效应”。如果一个捕获点横跨两个不同组织区域或细胞层的边界，其测量结果将是两者分子特征的混合。厚度 $T < d$ 的薄层状结构无法被清晰地解析。例如，如果一个厚度为 $30\\,\\mu\\mathrm{m}$ 的皮层被一个直径为 $55\\,\\mu\\mathrm{m}$ 的捕获点覆盖，该层的独特基因表达特征将被同一捕获点覆盖的相邻层的贡献严重稀释。该层特异性特征的信噪比会严重下降，甚至可能降至无法检测的水平。捕获点直径 $d$ 相对于特征尺寸越大，模糊和细节损失就越严重。\n\n总而言之，采样间距 $p$ 决定了避免混叠伪影的极限（一个采样问题），而捕获点直径 $d$ 决定了空间平均和模糊的极限（一个测量孔径问题）。要精确绘制厚度为 $T$ 的层状结构，一个必要但不充分的条件是 $p \\ll T$ 和 $d \\ll T$。在 $p=100\\,\\mu\\mathrm{m}$ 和 $d=55\\,\\mu\\mathrm{m}$ 的条件下，任何厚度在几十微米量级的层（这在哺乳动物皮层中很常见）都将无法被良好地解析，其边界会变得模糊，其定量表达谱会与其相邻层混淆。使用这些参数的该技术，从根本上不适合解析如此精细的解剖细节。", "answer": "$$\\boxed{0.2743}$$", "id": "2753018"}, {"introduction": "空间转录组学数据与所有基于测序的技术一样，会受到技术偏差的影响，其中聚合酶链式反应（Polymerase Chain Reaction, PCR）扩增偏差是一个常见问题。本练习将指导您建立一个统计模型，以量化和检测基因特异性的扩增偏差。您将应用基于泊松（Poisson）分布的似然比检验，这是一个在生物信息学数据分析中用于假设检验的强大而标准的框架。[@problem_id:2752992]", "problem": "给定来自空间转录组学的配对计数数据：对于每个基因和每个空间点，您拥有测序读数（read）的数量和唯一分子标识符（UMI）分子的数量。聚合酶链式反应（PCR）的扩增偏倚表现为每个UMI的预期读数数量在不同基因间存在的特异性偏差。在无偏倚的假设下，每个基因在每个空间点的预期读数与UMI计数成正比，且所有基因共享一个单一的比例常数。您的任务是使用基于似然的方法来形式化一个用于检测扩增偏倚的统计检验，并将其实现。\n\n基本原理和假设：\n- 唯一分子标识符（UMI）计数近似于原始cDNA分子的数量，而读数计数则是在PCR扩增后对这些分子进行测序产生的。\n- 在无偏倚的原假设下，基因 $g$ 在空间点 $s$ 的读数来自于一个泊松计数过程，其暴露量等于UMI计数：如果 $U_{g,s}$ 是UMI计数，$R_{g,s}$ 是读数计数，那么\n  $$ R_{g,s} \\mid U_{g,s} \\sim \\mathrm{Poisson}\\left(\\lambda \\, U_{g,s}\\right), $$\n  其中 $\\lambda$ 是一个单一的、与基因无关的扩增率（每个UMI的预期读数）。\n- 在针对基因 $g$ 的备择假设下，一个基因特异性扩增率 $\\lambda_g$ 取代了 $\\lambda$：\n  $$ R_{g,s} \\mid U_{g,s} \\sim \\mathrm{Poisson}\\left(\\lambda_g \\, U_{g,s}\\right). $$\n- 在该模型下，假设各个空间点和基因之间是相互独立的。假设只要 $U_{g,s} = 0$，那么 $R_{g,s} = 0$。\n\n为每个基因设计一个似然比检验（LRT），该检验比较全局扩增率的原假设与基因特异性扩增率的备择假设。在每个假设下，使用扩增率的最大似然估计值。使用具有1个自由度的卡方分布为每个基因计算一个$p$值。然后，应用Benjamini–Hochberg程序，在每个数据集内部的所有基因上将假发现率（FDR）控制在 $\\alpha = 0.05$ 的水平。如果一个基因被经过FDR调整的检验所拒绝，则称该基因为“有偏倚的”。如果一个基因的 $\\sum_s U_{g,s} = 0$（无暴露量），则将其视为不可检验，并报告为无偏倚。\n\n您的程序必须实现此过程，并在以下三个数据集上运行。每个数据集都为读数和UMI提供了基因-空间点矩阵。数据集A和数据集B的索引为基因 $g \\in \\{1,2,3\\}$ 和空间点 $s \\in \\{1,2,3,4\\}$，数据集C的索引为空间点 $s \\in \\{1,2,3\\}$。所有数字都是计数，因此是整数。\n\n数据集A（均衡，预期无偏倚）：\n- 读数矩阵 $R^{(A)}$:\n  - 基因 $1$: $[\\,6,\\,3,\\,9,\\,12\\,]$\n  - 基因 $2$: $[\\,3,\\,0,\\,6,\\,6\\,]$\n  - 基因 $3$: $[\\,12,\\,9,\\,0,\\,9\\,]$\n- UMI矩阵 $U^{(A)}$:\n  - 基因 $1$: $[\\,2,\\,1,\\,3,\\,4\\,]$\n  - 基因 $2$: $[\\,1,\\,0,\\,2,\\,2\\,]$\n  - 基因 $3$: $[\\,4,\\,3,\\,0,\\,3\\,]$\n\n数据集B（一个基因有强偏差；UMI与数据集A相同）：\n- 读数矩阵 $R^{(B)}$:\n  - 基因 $1$: $[\\,6,\\,3,\\,9,\\,12\\,]$\n  - 基因 $2$: $[\\,3,\\,0,\\,6,\\,6\\,]$\n  - 基因 $3$: $[\\,24,\\,18,\\,0,\\,18\\,]$\n- UMI矩阵 $U^{(B)}$:\n  - 基因 $1$: $[\\,2,\\,1,\\,3,\\,4\\,]$\n  - 基因 $2$: $[\\,1,\\,0,\\,2,\\,2\\,]$\n  - 基因 $3$: $[\\,4,\\,3,\\,0,\\,3\\,]$\n\n数据集C（边界情况：零暴露量基因和零读数基因）：\n- 读数矩阵 $R^{(C)}$:\n  - 基因 $1$: $[\\,0,\\,0,\\,0\\,]$\n  - 基因 $2$: $[\\,0,\\,0,\\,0\\,]$\n  - 基因 $3$: $[\\,6,\\,6,\\,3\\,]$\n- UMI矩阵 $U^{(C)}$:\n  - 基因 $1$: $[\\,0,\\,0,\\,0\\,]$\n  - 基因 $2$: $[\\,1,\\,1,\\,2\\,]$\n  - 基因 $3$: $[\\,2,\\,2,\\,1\\,]$\n\n实现要求：\n- 对于每个数据集，使用所有 $U_{g,s} > 0$ 的基因和空间点，通过最大似然估计全局扩增率 $\\lambda$。对于每个基因，仅使用该基因的数据通过最大似然估计其特异性扩增率 $\\lambda_g$。为该基因构建LRT统计量，使用1个自由度的卡方分布将其转换为$p$值，然后对该数据集内的$p$值集合应用Benjamini–Hochberg程序（水平为 $\\alpha = 0.05$）。一个无暴露量（即 $\\sum_s U_{g,s} = 0$）的基因必须报告为无偏倚。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个元素按A、B、C的顺序对应一个数据集，其本身是每个基因（按基因1,2,3的顺序）的布尔值列表，指示在FDR控制后是否检测到扩增偏倚。例如，一个语法上有效的输出看起来像 $[[\\mathrm{False},\\mathrm{True},\\mathrm{False}],[\\dots],[\\dots]]$。\n\n角度单位不适用。不涉及物理单位。要求的输出仅为布尔值。", "solution": "该问题要求形式化并实现一个统计程序，用以在空间转录组学数据中检测基因特异性的PCR扩增偏倚。此问题被构建为针对每个基因的假设检验问题，并随之进行多重检验校正。\n\n分析的基础依赖于一个特定的概率模型。对于给定的基因 $g$ 和空间点 $s$，我们获得了唯一分子标识符（UMI）计数 $U_{g,s}$ 和测序读数计数 $R_{g,s}$。我们假设读数计数服从泊松分布，其中UMI计数作为暴露量。\n\n原假设 $H_0$ 提出不存在基因特异性的扩增偏倚。在此假设下，任何基因 $g$ 在任何空间点 $s$ 的读数计数 $R_{g,s}$ 均从一个泊松分布中抽样，其速率与UMI计数成正比，由一个单一的全局扩增率 $\\lambda$ 控制：\n$$H_0: R_{g,s} \\mid U_{g,s} \\sim \\mathrm{Poisson}\\left(\\lambda \\, U_{g,s}\\right) \\quad \\text{对所有 } g,s.$$\n\n备择假设 $H_A$ 允许存在基因特异性的扩增率。对于特定基因 $g$，其扩增由一个速率 $\\lambda_g$ 控制，该速率可能与其他基因的速率不同。\n$$H_A(\\text{对于基因 } g): R_{g,s} \\mid U_{g,s} \\sim \\mathrm{Poisson}\\left(\\lambda_g \\, U_{g,s}\\right).$$\n我们假设在所有基因和空间点上的计数是独立的。\n\n为了检验每个基因的这些假设，我们构建了一个似然比检验（LRT）。这需要在原假设和备择假设模型下，使用最大似然估计（MLE）的原理来估计未知的速率参数。\n\n对于一组观测值，忽略与速率参数无关的常数项，其对数似然由下式给出：\n$$ \\mathcal{L}(\\text{rate}) = \\sum_{i} \\left( R_i \\log(\\text{rate}) - \\text{rate} \\cdot U_i \\right), $$\n其中索引 $i$ 遍历相关的观测值集合。\n\n**速率的最大似然估计**\n\n在原假设（$H_0$）下，我们使用所有数据点 $(R_{g,s}, U_{g,s})$ 估计一个单一的全局速率 $\\hat{\\lambda}$。其对数似然为：\n$$ \\mathcal{L}_0(\\lambda) = \\sum_{g,s} \\left( R_{g,s} \\log(\\lambda) - \\lambda U_{g,s} \\right). $$\n关于 $\\lambda$ 最大化 $\\mathcal{L}_0(\\lambda)$ 可得出最大似然估计（MLE）：\n$$ \\frac{d\\mathcal{L}_0}{d\\lambda} = \\frac{1}{\\lambda} \\sum_{g,s} R_{g,s} - \\sum_{g,s} U_{g,s} = 0 \\implies \\hat{\\lambda} = \\frac{\\sum_{g,s} R_{g,s}}{\\sum_{g,s} U_{g,s}}. $$\n这是所有基因和空间点的总读数计数除以总UMI计数。\n\n对于与基因 $g$ 相关的备择假设，我们仅使用该基因的数据来估计一个基因特异性速率 $\\hat{\\lambda}_g$。来自基因 $g$ 的对数似然贡献为：\n$$ \\mathcal{L}_g(\\lambda_g) = \\sum_{s} \\left( R_{g,s} \\log(\\lambda_g) - \\lambda_g U_{g,s} \\right). $$\n关于 $\\lambda_g$ 最大化此式，可得到基因特异性速率的MLE：\n$$ \\hat{\\lambda}_g = \\frac{\\sum_{s} R_{g,s}}{\\sum_{s} U_{g,s}} = \\frac{R_g}{U_g}, $$\n其中 $R_g = \\sum_s R_{g,s}$ 和 $U_g = \\sum_s U_{g,s}$ 是基因 $g$ 的总计数。如果 $U_g=0$，则假设 $R_g=0$，按规定，该基因被视为不可检验。\n\n**似然比检验统计量**\n\n对于每个可检验的基因 $g$ （其中 $U_g > 0$），我们构建LRT统计量 $\\Lambda_g$。它比较了该基因数据在其自身速率 $\\hat{\\lambda}_g$ 下的最大化对数似然与在全局速率 $\\hat{\\lambda}$ 下的对数似然：\n$$ \\Lambda_g = 2 \\left( \\sup_{\\lambda_g} \\mathcal{L}_g(\\lambda_g) - \\mathcal{L}_g(\\hat{\\lambda}) \\right). $$\n代入MLE，我们得到：\n$$ \\Lambda_g = 2 \\left[ \\left( R_g \\log(\\hat{\\lambda}_g) - \\hat{\\lambda}_g U_g \\right) - \\left( R_g \\log(\\hat{\\lambda}) - \\hat{\\lambda} U_g \\right) \\right]. $$\n利用 $\\hat{\\lambda}_g = R_g/U_g$ 这一事实，上式可简化为：\n$$ \\Lambda_g = 2 \\left[ R_g \\log\\left(\\frac{R_g}{U_g}\\right) - R_g \\log(\\hat{\\lambda}) - R_g + \\hat{\\lambda} U_g \\right]. $$\n此公式适用于 $R_g > 0$ 且 $U_g > 0$ 的情况。在 $R_g=0$ 但 $U_g > 0$ 的边界情况下，我们有 $\\hat{\\lambda}_g=0$。在 $\\hat{\\lambda}_g=0$ 下的对数似然贡献为 $0$，而在 $\\hat{\\lambda}$ 下为 $-\\hat{\\lambda}U_g$。因此，统计量变为 $\\Lambda_g = 2(0 - (-\\hat{\\lambda}U_g)) = 2\\hat{\\lambda}U_g$。\n根据Wilks定理，在 $H_0$ 下，$\\Lambda_g$ 渐近服从自由度为1的卡方分布（$\\chi^2_1$），这对应于备择模型中估计的单个额外参数（$\\lambda_g$）。我们从 $\\Lambda_g$ 计算每个基因的 $p$ 值。\n\n**多重检验校正**\n\n由于对数据集中的每个基因都进行了一次假设检验，我们必须进行多重比较校正以控制假阳性的数量。我们采用Benjamini–Hochberg（BH）程序，将假发现率（FDR）控制在指定的水平 $\\alpha = 0.05$。\n\nBH程序如下：\n$1$. 对于 $m$ 个可检验的基因，收集其 $p$ 值：$\\{p_1, p_2, \\dots, p_m\\}$。\n$2$. 将这些 $p$ 值按升序排序：$p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$。\n$3$. 找到满足 $p_{(k)} \\le \\frac{k}{m} \\alpha$ 的最大整数 $k$。\n$4$. 如果存在这样的 $k$，则拒绝对应于 $p$ 值 $p_{(1)}, \\dots, p_{(k)}$ 的所有基因的原假设。这些基因被声明为“有偏倚的”。如果不存在这样的 $k$，则不拒绝任何假设。\n\n**算法总结**\n对于每个数据集：\n$1$. 计算全局MLE $\\hat{\\lambda}$。\n$2$. 对于每个基因 $g$：\n    a. 如果总UMI计数 $U_g=0$，将该基因归类为无偏倚，并将其排除在检验之外。\n    b. 否则，根据 $R_g > 0$ 或 $R_g=0$ 的情况，使用相应公式计算LRT统计量 $\\Lambda_g$。\n    c. 从 $\\chi^2_1$ 分布计算 $p$ 值 $p_g$。\n$3$. 收集所有可检验基因的 $p$ 值，并应用 $\\alpha = 0.05$ 的Benjamini-Hochberg程序。\n$4$. 为每个基因报告一个布尔值结果，指示其在FDR校正后是否被识别为有偏倚。\n至此，该程序的正式规范说明完成。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve_for_dataset(R, U, alpha=0.05):\n    \"\"\"\n    Implements the full statistical procedure for a single dataset.\n    \n    Args:\n        R (np.ndarray): Genes-by-spots matrix of read counts.\n        U (np.ndarray): Genes-by-spots matrix of UMI counts.\n        alpha (float): The False Discovery Rate level.\n\n    Returns:\n        list[bool]: A list of booleans indicating if each gene is biased.\n    \"\"\"\n    num_genes = R.shape[0]\n    \n    # Step 1: Calculate global rate lambda_hat\n    total_R = np.sum(R)\n    total_U = np.sum(U)\n    \n    # If there are no UMIs at all, no rates can be estimated.\n    if total_U == 0:\n        return [False] * num_genes\n    \n    lambda_hat = total_R / total_U\n    \n    p_values = []\n    testable_gene_indices = []\n    \n    # Step 2: Calculate p-value for each gene\n    for g in range(num_genes):\n        R_g = np.sum(R[g, :])\n        U_g = np.sum(U[g, :])\n        \n        # As per problem, if total UMI is 0, gene is non-testable and not biased.\n        if U_g == 0:\n            continue\n        \n        testable_gene_indices.append(g)\n        \n        # Calculate Likelihood Ratio Test statistic (Lambda_g)\n        lrt_stat = 0.0\n        if R_g == 0:\n            # Edge case: gene-specific rate is 0.\n            lrt_stat = 2 * lambda_hat * U_g\n        else:\n            lambda_g_hat = R_g / U_g\n            # Check for numerical stability if rates are nearly identical\n            if not np.isclose(lambda_g_hat, lambda_hat):\n                # Full formula: 2 * [R_g*log(lambda_g_hat) - R_g*log(lambda_hat) - R_g + lambda_hat*U_g]\n                term1 = R_g * np.log(lambda_g_hat)\n                term2 = R_g * np.log(lambda_hat)\n                term3 = R_g\n                term4 = lambda_hat * U_g\n                lrt_stat = 2 * (term1 - term2 - term3 + term4)\n\n        # Ensure non-negative statistic due to potential floating point errors\n        if lrt_stat < 0:\n            lrt_stat = 0.0\n\n        # p-value from chi-square survival function (1-CDF) with 1 degree of freedom\n        p_values.append(chi2.sf(lrt_stat, df=1))\n\n    # Step 3: Apply Benjamini-Hochberg procedure\n    results_bool = [False] * num_genes\n    if not p_values:\n        return results_bool\n        \n    num_tests = len(p_values)\n    \n    # Sort p-values and retain original indices mapping to testable_gene_indices\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = np.array(p_values)[sorted_indices]\n    \n    # Calculate BH-adjusted significance thresholds\n    k_ranks = np.arange(1, num_tests + 1)\n    bh_thresholds = (k_ranks / num_tests) * alpha\n    \n    # Find all p-values that are below their corresponding BH threshold\n    significant_mask = sorted_p_values <= bh_thresholds\n    \n    if np.any(significant_mask):\n        # Find the largest rank k that is significant\n        max_k_index = np.where(significant_mask)[0].max()\n        \n        # All hypotheses up to rank max_k are rejected\n        significant_test_indices = sorted_indices[:max_k_index + 1]\n        \n        # Map rejected test indices back to original gene indices and set to True\n        for idx in significant_test_indices:\n            original_gene_index = testable_gene_indices[idx]\n            results_bool[original_gene_index] = True\n            \n    return results_bool\n\ndef solve():\n    # Define the test cases from the problem statement using numpy arrays.\n    test_cases = [\n        (np.array([[6, 3, 9, 12], [3, 0, 6, 6], [12, 9, 0, 9]], dtype=float),\n         np.array([[2, 1, 3, 4], [1, 0, 2, 2], [4, 3, 0, 3]], dtype=float)),\n        (np.array([[6, 3, 9, 12], [3, 0, 6, 6], [24, 18, 0, 18]], dtype=float),\n         np.array([[2, 1, 3, 4], [1, 0, 2, 2], [4, 3, 0, 3]], dtype=float)),\n        (np.array([[0, 0, 0], [0, 0, 0], [6, 6, 3]], dtype=float),\n         np.array([[0, 0, 0], [1, 1, 2], [2, 2, 1]], dtype=float))\n    ]\n\n    results = []\n    for R, U in test_cases:\n        result = solve_for_dataset(R, U)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() on a list of booleans produces the correct [False, True, ...] format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2752992"}, {"introduction": "空间转录组学的最终目标之一是识别具有生物学意义的组织区域或“域”。这个高级练习将引导您构建一个强大的概率模型——马尔可夫随机场（Markov Random Field, MRF），它能同时整合基因表达谱和空间邻近信息来进行组织分割。通过解决这个问题，您将接触到计算生物学中的前沿方法，例如变分推断（variational inference）和期望最大化（Expectation-Maximization, EM）算法，学会如何从复杂的空间数据中提取高阶生物学见解。[@problem_id:2752997]", "problem": "您正在对通过空间转录组学分析的脑组织切片中的空间域进行建模。二维格点上的每个斑点包含一组固定基因的基因表达计数，并且每个斑点属于一个潜在的域标签。您将推导一个概率模型和一个推断算法，然后实现它以估计域参数并在固定的测试套件上评估性能。\n\n从以下基本依据开始：\n- 马尔可夫性质：图模型中一个节点的标签在给定其邻居的情况下，条件独立于所有其他非邻居标签。\n- 对于具有局部能量和成对能量的有限状态无向图模型，成对吉布斯分布是马尔可夫随机场（MRFs）的一种广泛使用且经过充分检验的构造。\n- 斑点的观测基因计数来自基因的某种组成，并且在给定该组成的情况下，可以使用多项分布对基因的测序计数进行建模，这是一个在给定总计数和类别概率的情况下对计数进行建模的经过充分检验的模型。\n- 期望最大化（EM）原则通过交替计算潜在变量的条件期望和最大化期望完整数据对数似然来最大化观测数据似然。\n\n任务A部分（推导）：\n1) 使用带有 Potts 先验的马尔可夫随机场 (MRF) 为二维格点上的潜在域标签推导一个概率模型。具体来说，假设每个斑点 $i$ 带有潜在标签 $z_i \\in \\{1,\\dots,K\\}$，空间上相邻的斑点对在一个边集为 $E$ 的无向格点图中相连。Potts 先验规定相邻标签倾向于相等，这由一个标量平滑参数 $\\beta \\ge 0$ 控制。使用边上标签相等性的指示函数和标量 $\\beta$ 写下关于 $z = (z_1,\\dots,z_n)$ 的未归一化先验密度。\n2) 对于总数为 $N_i = \\sum_{g=1}^G x_{ig}$ 的观测计数 $x_i \\in \\mathbb{N}_0^G$，假设一个多项发射：在 $z_i = k$ 的条件下，$x_i$ 的概率是总数为 $N_i$ 且类别特定的基因概率为 $\\phi_k = (\\phi_{k1},\\dots,\\phi_{kG})$ 的多项分布，其中满足 $\\sum_{g=1}^G \\phi_{kg} = 1$ 和 $\\phi_{kg} > 0$。写出完整数据似然和完整数据对数似然。\n3) 解释为什么对于此模型，精确后验边缘概率 $p(z_i = k \\mid x)$ 通常是难解的，并提出一个将后验分解为 $\\prod_{i=1}^n q_i(z_i)$ 的平均场变分近似，其中 $q_i(k)$ 是类别变分参数。陈述用于 $q_i(k)$ 的坐标上升变分更新的不动点方程，该方程应精确到某个比例常数，并结合 Potts 相互作用和多项发射。\n4) 对每个 $\\phi_k$ 添加一个集中度为 $\\alpha > 1$ 的对称狄利克雷先验，即 $p(\\phi_k) \\propto \\prod_{g=1}^G \\phi_{kg}^{\\alpha - 1}$。在给定变分责任 $q$ 的情况下，推导关于 $\\phi$ 的期望完整数据对数后验的最大化器，并将解表示为由 $\\alpha$ 确定的带伪计数的归一化计数。\n\n任务B部分（算法详述）：\n为此模型设计一个带有平均场变分 E-步的期望最大化 (EM) 算法：\n- E-步：对于固定的 $\\phi$，使用坐标上升平均场更新来更新 $q_i(k)$，直到收敛或达到固定的迭代次数。\n- M-步：对于固定的 $q$，使用您推导的狄利克雷正则化解来更新 $\\phi$。\n- 进行固定次数的外部迭代。\n在适当的地方使用数值稳定化。通过使用固定的伪随机种子，从集中度为 $1$ 的对称狄利克雷分布中独立抽取每一行来初始化 $\\phi$，以打破标签对称性。\n\n任务C部分（实现与评估）：\n在 Python 中实现该算法，以便在下面定义的测试套件上运行。格点是一个具有四邻域邻接（存在时包括上、下、左、右）的规则网格。索引从 $0$ 开始以行主序枚举。对于每个测试案例，您将获得网格形状、标签数量 $K$、基因数量 $G$、Potts 平滑度 $\\beta$、真实标签场、真实发射概率 $\\phi^{\\text{true}}$ 以及每个斑点的总计数 $N_i$。按如下方式从 $\\phi^{\\text{true}}$ 和 $N_i$ 确定性地构建观测计数：对于所有 $g \\in \\{1,\\dots,G\\}$，计算 $y_{ig} = \\lfloor N_i \\phi^{\\text{true}}_{z_i, g} \\rfloor$，然后计算余数 $r_i = N_i - \\sum_{g=1}^G y_{ig}$，并令 $d_{ig} = N_i \\phi^{\\text{true}}_{z_i, g} - y_{ig}$。将剩余的 $r_i$ 个计数通过将 $1$ 加到具有最大 $d_{ig}$ 值的 $r_i$ 个基因上来分配，平局时按基因索引递增顺序处理。结果是 $x_i$，满足 $\\sum_{g=1}^G x_{ig} = N_i$。\n\n对所有案例使用以下超参数和数值细节：\n- 狄利克雷先验集中度 $\\alpha = 1.1$。\n- 外部 EM 迭代次数 $T = 25$。\n- 每个 E-步的内部平均场迭代次数 $L = 50$，若提前达到，则更新中最大绝对变化量的停止容差为 $\\varepsilon = 10^{-6}$。\n- 初始化：使用伪随机种子 $13$，从集中度为 $1$ 的对称狄利克雷分布中抽取 $\\phi$ 的每一行。\n- 对 $\\phi$ 使用 $\\ell_1$ 归一化的行约束。\n- 对类别归一化使用自然对数（以 e 为底）和标准的 log-sum-exp 稳定化方法。\n\n测试套件：\n- 案例 1 (理想路径)：网格形状 $3 \\times 3$，$K = 2$，$G = 4$，$\\beta = 0.8$，行主序网格中的真实标签\n  $\\begin{bmatrix}\n  1 & 1 & 2\\\\\n  1 & 1 & 2\\\\\n  1 & 2 & 2\n  \\end{bmatrix}$\n  映射为从零开始的索引 $\\{0,1\\}$，即 $\\begin{bmatrix}0 & 0 & 1\\\\ 0 & 0 & 1\\\\ 0 & 1 & 1\\end{bmatrix}$，总数 $N = [40, 35, 45, 50, 30, 42, 25, 38, 60]$，以及\n  $\\phi^{\\text{true}} = \\begin{bmatrix}\n  0.55 & 0.25 & 0.15 & 0.05\\\\\n  0.05 & 0.20 & 0.35 & 0.40\n  \\end{bmatrix}$。\n- 案例 2 (多类别，更强空间耦合)：网格形状 $2 \\times 4$，$K = 3$，$G = 3$，$\\beta = 1.2$，行主序网格中的真实标签\n  $\\begin{bmatrix}\n  0 & 1 & 2 & 2\\\\\n  0 & 1 & 1 & 2\n  \\end{bmatrix}$，总数 $N = [30, 35, 40, 45, 28, 32, 36, 44]$，以及\n  $\\phi^{\\text{true}} = \\begin{bmatrix}\n  0.70 & 0.20 & 0.10\\\\\n  0.20 & 0.60 & 0.20\\\\\n  0.15 & 0.15 & 0.70\n  \\end{bmatrix}$。\n- 案例 3 (无空间耦合，近乎均匀的发射)：网格形状 $2 \\times 2$，$K = 2$，$G = 3$，$\\beta = 0.0$，行主序网格中的真实标签\n  $\\begin{bmatrix}\n  0 & 1\\\\\n  0 & 1\n  \\end{bmatrix}$，总数 $N = [25, 25, 25, 25]$，以及\n  $\\phi^{\\text{true}} = \\begin{bmatrix}\n  0.34 & 0.33 & 0.33\\\\\n  0.33 & 0.34 & 0.33\n  \\end{bmatrix}$。\n\n对于每个测试案例，使用指定的超参数和初始化协议运行上述 EM 算法。收敛后，形成硬分配 $\\hat{z}_i = \\arg\\max_k q_i(k)$。因为标签身份在排列下是任意的，所以计算从估计标签到真实标签的最佳排列映射，以最大化匹配数量。然后计算两个评估指标：\n- 标签准确率：在 $[0,1]$ 范围内的分数，表示其排列后的硬分配等于真实标签的斑点所占的比例。\n- 类别平均 $\\ell_1$ 距离：将相同的最优排列应用于估计的 $\\phi$ 的行后，估计的 $\\phi$ 与真实的 $\\phi^{\\text{true}}$ 之间的 $\\ell_1$ 距离，并在所有类别上取平均值。\n\n您的程序应生成单行输出，其中包含一个结果列表，每个测试案例一个结果，每个结果是一个双元素列表 $[\\text{accuracy}, \\text{phi\\_l1}]$。所有浮点数必须四舍五入到 $6$ 位小数。输出格式必须是严格的单个 Python 风格列表，例如 $[[0.95,0.12],[0.85,0.20],[1.0,0.0]]$。", "solution": "这是一个计算统计学问题，将分层贝叶斯模型应用于空间转录组学数据。该问题是良定的，科学上是合理的，并为唯一解提供了所有必要信息。我将按要求进行推导和算法详述。\n\n**A.1 部分：Potts 先验推导**\n模型定义在由 $n$ 个斑点组成的二维格点上。设潜在域标签集合为 $z = (z_1, \\dots, z_n)$，其中每个 $z_i \\in \\{1, \\dots, K\\}$。空间排列由一个无向图表示，其边集 $E$ 连接空间上相邻的斑点。Potts 模型是马尔可夫随机场 (MRF) 的一种，它基于局部相互作用为每个组态 $z$ 分配一个概率。标签 $z$ 上的先验由吉布斯分布给出。未归一化的先验密度与组态负能量的指数成正比，$p(z \\mid \\beta) \\propto \\exp(-H(z))$。对于 Potts 模型，能量函数定义为：\n$$H(z) = -\\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j)$$\n这里，$\\beta \\ge 0$ 是平滑度参数，控制着相互作用的强度。较大的 $\\beta$ 对具有不同标签的相邻斑点施加更强的惩罚，从而促进更大、更同质的域。项 $\\mathbb{I}(z_i = z_j)$ 是一个指示函数，如果 $z_i = z_j$ 则等于 $1$，否则等于 $0$。因此，未归一化的先验密度可以写为：\n$$p(z \\mid \\beta) \\propto \\exp\\left(\\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j)\\right)$$\n这个概率是未归一化的，因为它省略了配分函数 $Z(\\beta) = \\sum_z \\exp\\left(\\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j)\\right)$，该函数通常是难于计算的。\n\n**A.2 部分：完整数据似然推导**\n每个斑点 $i$ 的观测数据是基因计数 $x_i = (x_{i1}, \\dots, x_{iG})$，总计数为 $N_i = \\sum_{g=1}^G x_{ig}$。发射概率被建模为多项分布，以潜在标签 $z_i$ 为条件。如果斑点 $i$ 属于域 $k$（即 $z_i=k$），则计数 $x_i$ 从一个参数为 $N_i$ 和 $\\phi_k = (\\phi_{k1}, \\dots, \\phi_{kG})$ 的多项分布中抽取：\n$$p(x_i \\mid z_i = k, N_i, \\phi_k) = \\frac{N_i!}{\\prod_{g=1}^G x_{ig}!} \\prod_{g=1}^G \\phi_{kg}^{x_{ig}}$$\n完整数据包括观测计数 $x = (x_1, \\dots, x_n)$ 和潜在标签 $z = (z_1, \\dots, z_n)$。给定参数 $\\phi = \\{\\phi_1, \\dots, \\phi_K\\}$ 的完整数据似然是联合概率 $p(x, z \\mid \\phi, \\beta)$。假设在给定标签的情况下观测值条件独立，并结合标签上的 MRF 先验，则为：\n$$p(x, z \\mid \\phi, \\beta) = p(x \\mid z, \\phi) p(z \\mid \\beta) = \\left( \\prod_{i=1}^n p(x_i \\mid z_i, \\phi_{z_i}) \\right) p(z \\mid \\beta)$$\n完整数据对数似然 $\\mathcal{L}(\\phi, \\beta; x, z) = \\log p(x, z \\mid \\phi, \\beta)$ 是：\n$$\\mathcal{L} = \\sum_{i=1}^n \\log p(x_i \\mid z_i, \\phi_{z_i}) + \\log p(z \\mid \\beta)$$\n使用指示变量 $\\mathbb{I}(z_i=k)$ 并代入多项对数概率和 Potts 先验形式，我们得到：\n$$\\mathcal{L} = \\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{I}(z_i=k) \\left( \\log\\left(\\frac{N_i!}{\\prod_{g=1}^G x_{ig}!}\\right) + \\sum_{g=1}^G x_{ig} \\log \\phi_{kg} \\right) + \\beta \\sum_{(i,j) \\in E} \\mathbb{I}(z_i = z_j) - \\log Z(\\beta)$$\n\n**A.3 部分：变分推断的动机与推导**\n在此模型中，精确推断是难解的。潜在标签上的后验分布 $p(z \\mid x, \\phi, \\beta)$ 由贝叶斯法则给出：\n$$p(z \\mid x, \\phi, \\beta) = \\frac{p(x \\mid z, \\phi) p(z \\mid \\beta)}{p(x \\mid \\phi, \\beta)}$$\n难解性源于分母中的证据项，$p(x \\mid \\phi, \\beta) = \\sum_z p(x \\mid z, \\phi) p(z \\mid \\beta)$。这个求和遍及所有 $K^n$ 种可能的标签组态，对于任何非平凡的格点大小 $n$，这在计算上都是不可行的。因此，计算像 $p(z_i = k \\mid x)$ 这样的后验边缘概率也是难解的。\n\n为了克服这个问题，我们使用平均场变分推断。我们用一个更简单的、可分解的分布 $q(z)$ 来近似真实的后验 $p(z \\mid x)$：\n$$q(z) = \\prod_{i=1}^n q_i(z_i)$$\n其中每个 $q_i(z_i)$ 是斑点 $i$ 的 $K$ 个标签上的一个类别分布，参数为 $q_i(k) = q_i(z_i=k)$。每个因子 $q_j(z_j)$ 的最优解通过最小化 KL 散度 $D_{KL}(q(z) || p(z|x))$ 来找到，这等价于最大化证据下界 (ELBO)。这导出了不动点方程：\n$$\\log q_j(z_j) = \\mathbb{E}_{q_{\\sim j}}[\\log p(x, z \\mid \\phi, \\beta)] + \\text{const.}$$\n其中期望 $\\mathbb{E}_{q_{\\sim j}}$ 是对除 $z_j$ 之外的所有潜在变量取的。我们展开对数联合概率并分离出涉及 $z_j$ 的项：\n$$\\log q_j(z_j=k) \\propto \\mathbb{E}_{q_{\\sim j}}\\left[ \\log p(x_j \\mid z_j=k, \\phi_k) + \\sum_{m \\in \\mathcal{N}(j)} \\beta \\mathbb{I}(z_j=k, z_m) \\right]$$\n其中 $\\mathcal{N}(j)$ 是斑点 $j$ 的邻居集合。第一项与其他 $z_m$ 无关，第二项在平均场近似下的期望是：\n$$\\mathbb{E}_{q_m}[\\beta \\mathbb{I}(z_j=k, z_m)] = \\beta \\sum_{l=1}^K q_m(z_m=l) \\cdot \\mathbb{I}(k=l) = \\beta q_m(k)$$\n因此，变分参数 $q_j(k)$ 的不动点更新方程，在比例常数内，是：\n$$\\log q_j(k) \\propto \\log p(x_j \\mid z_j=k, \\phi_k) + \\beta \\sum_{m \\in \\mathcal{N}(j)} q_m(k)$$\n代入多项对数似然（省略在 $k$ 上为常数的项）：\n$$\\log q_j(k) \\propto \\sum_{g=1}^G x_{jg} \\log \\phi_{kg} + \\beta \\sum_{m \\in \\mathcal{N}(j)} q_m(k)$$\n然后通过对 $k \\in \\{1, \\dots, K\\}$ 进行指数化和归一化来获得变分参数 $q_j(k)$。\n\n**A.4 部分：带狄利克雷先验的 M-步推导**\n对每组发射概率 $\\phi_k$ 施加一个对称狄利克雷先验，其集中度参数为 $\\alpha > 1$：\n$$p(\\phi_k \\mid \\alpha) \\propto \\prod_{g=1}^G \\phi_{kg}^{\\alpha-1}$$\nEM 算法 M-步的目标是，在给定数据和变分分布 $q(z)$ 的条件下，最大化关于参数 $\\phi$ 的期望完整数据对数后验。此目标是：\n$$\\mathcal{Q}(\\phi) = \\mathbb{E}_q[\\log p(x, z, \\phi \\mid \\alpha, \\beta)] = \\mathbb{E}_q[\\log p(x,z \\mid \\phi,\\beta) + \\log p(\\phi \\mid \\alpha)]$$\n我们只考虑依赖于 $\\phi$ 的项：\n$$\\mathcal{Q}(\\phi) = \\mathbb{E}_q\\left[\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{I}(z_i=k) \\sum_{g=1}^G x_{ig} \\log \\phi_{kg} \\right] + \\sum_{k=1}^K \\sum_{g=1}^G (\\alpha - 1) \\log \\phi_{kg} + \\text{const.}$$\n使用性质 $\\mathbb{E}_q[\\mathbb{I}(z_i=k)] = q_i(k)$，我们得到：\n$$\\mathcal{Q}(\\phi) = \\sum_{i=1}^n \\sum_{k=1}^K q_i(k) \\sum_{g=1}^G x_{ig} \\log \\phi_{kg} + \\sum_{k=1}^K \\sum_{g=1}^G (\\alpha - 1) \\log \\phi_{kg}$$\n目标对于每个 $\\phi_k$ 是解耦的。对于给定的 $k$，我们最大化：\n$$J(\\phi_k) = \\sum_{g=1}^G \\left( \\left(\\sum_{i=1}^n q_i(k) x_{ig}\\right) + \\alpha - 1 \\right) \\log \\phi_{kg}$$\n受约束 $\\sum_{g=1}^G \\phi_{kg} = 1$。这是狄利克雷分布对数密度的核。$\\phi_k$ 的最大化器通过归一化 $\\log \\phi_{kg}$ 项的系数给出：\n$$\\phi_{kg} = \\frac{\\sum_{i=1}^n q_i(k) x_{ig} + \\alpha - 1}{\\sum_{g'=1}^G \\left( \\sum_{i=1}^n q_i(k) x_{ig'} + \\alpha - 1 \\right)}$$\n分母简化为分配给类别 $k$ 的总期望计数加上来自先验的总伪计数：\n$$\\sum_{g'=1}^G \\left( \\dots \\right) = \\sum_{i=1}^n q_i(k) \\left(\\sum_{g'=1}^G x_{ig'}\\right) + \\sum_{g'=1}^G (\\alpha-1) = \\sum_{i=1}^n q_i(k) N_i + G(\\alpha-1)$$\n项 $\\alpha-1 > 0$ 充当伪计数，正则化 $\\phi_k$ 的估计并防止概率为零。\n\n**B 部分：算法详述**\n该算法是一个平均场变分期望最大化 (VEM) 过程。它在更新标签的近似后验（E-步）和更新模型参数（M-步）之间交替进行。\n\n1.  **初始化：**\n    -   固定超参数：Potts 耦合 $\\beta$、狄利克雷集中度 $\\alpha$、EM 迭代次数 $T$、平均场迭代次数 $L$ 和容差 $\\varepsilon$。\n    -   通过使用固定的伪随机种子，从集中度为 $1.0$ 的对称狄利克雷分布中独立抽取每一行 $\\phi_k^{(0)}$ 来初始化发射参数 $\\phi^{(0)}$，以保证可复现性。\n\n2.  **迭代：** 对于 $t = 1, \\dots, T$：\n    -   **变分 E-步：** 更新变分分布 $q$ 以近似后验 $p(z \\mid x, \\phi^{(t-1)})$。\n        -   使用前一次 EM 迭代的结果初始化 $q^{(t,0)}$，或者在第一次迭代时使用均匀分布初始化。\n        -   对于 $l=1, \\dots, L$（内部坐标上升循环）：\n            -   对于每个斑点 $i = 1, \\dots, n$：\n                -   计算每个类别 $k = 1, \\dots, K$ 的未归一化对数后验：\n                    $$\\lambda_{ik} = \\sum_{g=1}^G x_{ig} \\log \\phi_{kg}^{(t-1)} + \\beta \\sum_{j \\in \\mathcal{N}(i)} q_j(k)^{(t, l-1)}$$\n                -   使用 log-sum-exp 技巧进行归一化以更新 $q_i(k)$，以保证数值稳定性：\n                    $$q_i(k)^{(t,l)} = \\frac{\\exp(\\lambda_{ik})}{\\sum_{k'=1}^K \\exp(\\lambda_{ik'})}$$\n            -   检查收敛：如果 $\\max_{i,k} |q_i(k)^{(t,l)} - q_i(k)^{(t,l-1)}| < \\varepsilon$，则中断内部循环。\n        -   将最终的 $q^{(t)}$ 设置为收敛后的值。\n\n    -   **M-步：** 在给定 $q^{(t)}$ 的情况下更新发射参数 $\\phi$，以最大化期望完整数据对数后验。\n        -   对于每个类别 $k = 1, \\dots, K$ 和基因 $g = 1, \\dots, G$：\n            -   使用推导出的正则化公式计算更新后的 $\\phi_{kg}^{(t)}$：\n                $$\\phi_{kg}^{(t)} = \\frac{\\sum_{i=1}^n q_i(k)^{(t)} x_{ig} + \\alpha - 1}{\\sum_{i=1}^n q_i(k)^{(t)} N_i + G(\\alpha-1)}$$\n\n3.  **终止：** 在 $T$ 次迭代后，算法返回最终估计的参数 $\\phi^{(T)}$ 和变分后验 $q^{(T)}$。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the variational EM algorithm on the test suite\n    and print the evaluation results.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"shape\": (3, 3), \"K\": 2, \"G\": 4, \"beta\": 0.8,\n            \"z_true\": np.array([0, 0, 1, 0, 0, 1, 0, 1, 1]),\n            \"N\": np.array([40, 35, 45, 50, 30, 42, 25, 38, 60]),\n            \"phi_true\": np.array([\n                [0.55, 0.25, 0.15, 0.05],\n                [0.05, 0.20, 0.35, 0.40]\n            ])\n        },\n        {\n            \"shape\": (2, 4), \"K\": 3, \"G\": 3, \"beta\": 1.2,\n            \"z_true\": np.array([0, 1, 2, 2, 0, 1, 1, 2]),\n            \"N\": np.array([30, 35, 40, 45, 28, 32, 36, 44]),\n            \"phi_true\": np.array([\n                [0.70, 0.20, 0.10],\n                [0.20, 0.60, 0.20],\n                [0.15, 0.15, 0.70]\n            ])\n        },\n        {\n            \"shape\": (2, 2), \"K\": 2, \"G\": 3, \"beta\": 0.0,\n            \"z_true\": np.array([0, 1, 0, 1]),\n            \"N\": np.array([25, 25, 25, 25]),\n            \"phi_true\": np.array([\n                [0.34, 0.33, 0.33],\n                [0.33, 0.34, 0.33]\n            ])\n        }\n    ]\n\n    hyperparams = {\n        \"alpha\": 1.1,\n        \"T\": 25,\n        \"L\": 50,\n        \"epsilon\": 1e-6,\n        \"seed\": 13\n    }\n\n    results = []\n    for case in test_cases:\n        adj_list = build_adjacency_list(case[\"shape\"])\n        x = generate_counts(case[\"z_true\"], case[\"N\"], case[\"phi_true\"])\n\n        q, phi = run_vem(x,\n                         case[\"N\"],\n                         adj_list,\n                         case[\"K\"],\n                         case[\"G\"],\n                         case[\"beta\"],\n                         hyperparams)\n        \n        accuracy, phi_l1 = evaluate(q, phi, case[\"z_true\"], case[\"phi_true\"], case[\"K\"])\n        results.append([round(accuracy, 6), round(phi_l1, 6)])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef build_adjacency_list(shape):\n    \"\"\"\n    Builds a 4-neighbor adjacency list for a grid.\n    \"\"\"\n    H, W = shape\n    n = H * W\n    adj = [[] for _ in range(n)]\n    for i in range(n):\n        r, c = divmod(i, W)\n        # Up\n        if r > 0: adj[i].append(i - W)\n        # Down\n        if r < H - 1: adj[i].append(i + W)\n        # Left\n        if c > 0: adj[i].append(i - 1)\n        # Right\n        if c < W - 1: adj[i].append(i + 1)\n    return adj\n\ndef generate_counts(z_true, N, phi_true):\n    \"\"\"\n    Deterministically generates gene counts based on ground truth.\n    \"\"\"\n    n, G = len(N), phi_true.shape[1]\n    x = np.zeros((n, G), dtype=int)\n    for i in range(n):\n        true_label = z_true[i]\n        expected_counts = N[i] * phi_true[true_label, :]\n        \n        y_i = np.floor(expected_counts).astype(int)\n        remainder_counts = N[i] - y_i.sum()\n        \n        residuals = expected_counts - y_i\n        \n        # Assign remainder counts to genes with largest residuals\n        remainder_indices = np.argsort(-residuals)[:remainder_counts]\n        \n        x[i, :] = y_i\n        x[i, remainder_indices] += 1\n    return x\n\ndef log_sum_exp(vec, axis=-1, keepdims=False):\n    \"\"\"\n    Computes log(sum(exp(vec))) in a numerically stable way.\n    \"\"\"\n    max_val = np.max(vec, axis=axis, keepdims=True)\n    return max_val + np.log(np.sum(np.exp(vec - max_val), axis=axis, keepdims=True))\n\ndef run_vem(x, N, adj, K, G, beta, hp):\n    \"\"\"\n    Runs the Variational EM algorithm.\n    \"\"\"\n    n = x.shape[0]\n    rng = np.random.default_rng(hp[\"seed\"])\n    phi = rng.dirichlet(np.ones(G), size=K)\n    \n    q = np.full((n, K), 1.0 / K)\n\n    log_phi = np.log(phi + 1e-100) # Add small const to avoid log(0)\n    \n    for t in range(hp[\"T\"]):\n        # E-step: Variational inference for q\n        for l in range(hp[\"L\"]):\n            q_old = q.copy()\n            \n            mrf_term = np.zeros((n, K))\n            for i in range(n):\n                if adj[i]:\n                    mrf_term[i, :] = beta * np.sum(q[adj[i], :], axis=0)\n\n            log_lik_term = x @ log_phi.T\n            log_q_unnorm = log_lik_term + mrf_term\n            \n            log_q = log_q_unnorm - log_sum_exp(log_q_unnorm, axis=1, keepdims=True)\n            q = np.exp(log_q)\n\n            if np.max(np.abs(q - q_old)) < hp[\"epsilon\"]:\n                break\n        \n        # M-step: Update phi\n        numerator = q.T @ x + hp[\"alpha\"] - 1.0\n        denominator = (q.T @ N) + G * (hp[\"alpha\"] - 1.0)\n        \n        phi = numerator / denominator[:, np.newaxis]\n        log_phi = np.log(phi + 1e-100)\n\n    return q, phi\n\ndef evaluate(q, phi_est, z_true, phi_true, K):\n    \"\"\"\n    Evaluates the model performance against ground truth.\n    \"\"\"\n    n = len(z_true)\n    z_hat = np.argmax(q, axis=1)\n\n    # Find optimal permutation of labels for matching\n    cost_matrix = np.zeros((K, K))\n    for i in range(n):\n        cost_matrix[z_hat[i], z_true[i]] += 1\n    \n    # linear_sum_assignment finds a minimum cost assignment. We want max matches.\n    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n    \n    # Accuracy\n    accuracy = cost_matrix[row_ind, col_ind].sum() / n\n\n    # Permute estimated phi to match true phi ordering\n    phi_permuted = np.zeros_like(phi_est)\n    phi_permuted[col_ind] = phi_est[row_ind]\n\n    # Mean L1 distance\n    l1_distances = np.sum(np.abs(phi_permuted - phi_true), axis=1)\n    mean_l1_dist = np.mean(l1_distances)\n    \n    return accuracy, mean_l1_dist\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2752997"}]}