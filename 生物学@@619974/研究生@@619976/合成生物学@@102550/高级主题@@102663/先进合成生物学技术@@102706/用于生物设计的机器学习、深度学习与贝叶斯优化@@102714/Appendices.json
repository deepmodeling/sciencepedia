{"hands_on_practices": [{"introduction": "生物设计空间极其广阔，而实验预算往往有限。该练习旨在量化模型引导的自适应采样相比于简单的随机筛选，在发现高性能序列方面的优势。通过从基本概率论出发推导，你将亲身体会到，即使是一个不完美的预测模型，也能如何显著提高在有限实验次数 $n$ 内发现目标序列的成功率，从而理解机器学习在加速生物设计中的核心价值 [@problem_id:2749115]。", "problem": "考虑一个由长度为 $L$、字母表大小为 $A$ 的生物序列组成的组合设计库，因此不同序列的总数为 $N = A^{L}$。每个序列都有一个标量性能 $Y$（例如，测得的活性），并假设水平为 $\\alpha \\in (0,1)$ 的顶部分位数的定义是：如果一个序列的性能 $Y$ 位于整个库的前 $\\alpha$ 部分，则该序列被称为“好的”。令 $G$ 表示“随机选择的序列是好的”这一事件，因此 $\\mathbb{P}(G) = \\alpha$。您有 $n$ 次实验的预算，且 $n \\ll N$。\n\n现考虑两种抽样策略：\n\n- 有放回的均匀随机抽样：每次实验从整个库中均匀随机地抽取一个序列，且各次抽取相互独立。\n- 通过贝叶斯优化（BO）进行自适应抽样：一个深度神经网络（DNN）代理分类器，它在历史数据上进行训练，并用于贝叶斯优化（BO）中以优先排序候选序列。该分类器为每个序列生成一个二元决策特征 $F \\in \\{0,1\\}$，用于在固定的工作点上标记序列是否被预测为“好的”。在独立的验证数据上，该工作点表现出灵敏度 $s = \\mathbb{P}(F=1 \\mid G)$ 和假阳性率 $t = \\mathbb{P}(F=1 \\mid \\overline{G})$，其中 $s \\in (0,1)$ 且 $t \\in (0,1)$。假设分类器在该工作点上经过良好校准，因此通过贝叶斯法则计算出的后验概率是可靠的。自适应策略在每次实验中从被标记的集合 $\\{F=1\\}$ 中均匀随机地选择序列。假设 $n \\ll N \\,\\mathbb{P}(F=1)$，因此从被标记集合中的抽样可以视为有放回的独立抽样。\n\n仅从分位数、伯努利试验、贝叶斯法则和全概率定律的定义出发，推导在两种策略下，$n$ 次实验内发现至少一个好序列的概率。然后，为以下比率提供一个单一的闭式表达式：\n$$\nR(\\alpha,s,t,n) \\;=\\; \\frac{\\text{自适应抽样下至少有一次发现的概率}}{\\text{均匀随机抽样下至少有一次发现的概率}}。\n$$\n将您的最终答案表示为关于 $\\alpha$、$s$、$t$ 和 $n$ 的简化解析表达式。无需进行数值近似，最终答案不涉及单位，因为它是一个无量纲比率。", "solution": "我们首先对问题陈述进行严格的验证。\n\n**步骤1：提取已知条件**\n- 库中序列总数：$N = A^{L}$，其中 $L$ 是序列长度， $A$ 是字母表大小。\n- 如果一个序列的性能 $Y$ 处于水平为 $\\alpha \\in (0,1)$ 的顶部分位数，则该序列是“好的”。\n- $G$ 是随机选择的序列为好的事件。\n- 一个序列为好的先验概率是 $\\mathbb{P}(G) = \\alpha$。\n- 实验预算（实验次数）为 $n$，条件为 $n \\ll N$。\n- **策略1（均匀随机抽样）**：序列通过有放回的方式均匀随机抽样。\n- **策略2（自适应抽样）**：\n    - 来自DNN分类器的二元决策特征 $F \\in \\{0, 1\\}$ 标记被预测为好的序列。\n    - 分类器的灵敏度：$s = \\mathbb{P}(F=1 \\mid G)$，其中 $s \\in (0,1)$。\n    - 分类器的假阳性率：$t = \\mathbb{P}(F=1 \\mid \\overline{G})$，其中 $t \\in (0,1)$。\n    - 抽样策略是从集合 $\\{F=1\\}$ 中均匀随机地选择序列。\n    - 鉴于假设 $n \\ll N \\mathbb{P}(F=1)$，抽样过程被视为有放回的独立抽样。\n- **目标**：\n    1. 推导在两种策略下，$n$ 次实验内发现至少一个好序列的概率。\n    2. 推导比率 $R(\\alpha, s, t, n)$，其定义为自适应抽样下至少有一次发现的概率除以均匀抽样下至少有一次发现的概率。\n\n**步骤2：使用提取的已知条件进行验证**\n对问题进行对照所需标准的验证。\n- **科学依据**：该问题在概率论、统计学和机器学习方面有坚实的基础。它模拟了合成生物学中的一个现实场景，即计算模型指导高通量筛选。分位数、伯努利试验、灵敏度（真阳性率）、假阳性率和贝叶斯推断等概念都是标准的，并且应用正确。\n- **适定性**：该问题是适定的。它提供了所有必要的参数（$\\alpha$, $s$, $t$, $n$）并陈述了得出唯一解析解所需的简化假设（$n \\ll N$，有放回抽样）。\n- **客观性**：该问题使用精确、客观且无歧义的数学语言进行陈述。\n\n问题陈述没有任何缺陷。它不是不科学、不可形式化、不完整、不切实际、不适定、琐碎或不可验证的。\n\n**步骤3：结论与行动**\n该问题被判定为**有效**。我们现在继续推导解答。\n\n核心任务是计算在两种不同的抽样策略下，一系列 $n$ 次伯努利试验中至少有一次成功的概率。两种策略下单次试验的成功概率不同。对于任何成功概率为 $p$ 的 $n$ 次独立试验，观察到零次成功的概率是 $(1-p)^n$。根据互补事件原理，观察到至少一次成功的概率是 $1 - (1-p)^n$。\n\n**策略1：均匀随机抽样**\n对于此策略，序列从整个库中均匀随机抽取。单次试验的成功概率，即抽到一个“好的”序列的概率，已由问题陈述直接给出。\n设 $p_{\\text{unif}}$ 为此概率。\n$$p_{\\text{unif}} = \\mathbb{P}(G) = \\alpha$$\n各次试验是独立的。因此，在 $n$ 次实验中发现至少一个好序列的概率是：\n$$P_1 = 1 - (1 - p_{\\text{unif}})^n = 1 - (1 - \\alpha)^n$$\n\n**策略2：通过贝叶斯优化进行自适应抽样**\n对于此策略，序列仅从被分类器标记的序列子集（即 $F=1$ 的序列）中均匀随机抽取。单次试验的成功概率 $p_{\\text{adapt}}$ 是在序列被分类器标记的条件下，该序列是好的的条件概率，即 $\\mathbb{P}(G \\mid F=1)$。\n\n我们应用贝叶斯法则来求这个后验概率：\n$$p_{\\text{adapt}} = \\mathbb{P}(G \\mid F=1) = \\frac{\\mathbb{P}(F=1 \\mid G) \\mathbb{P}(G)}{\\mathbb{P}(F=1)}$$\n分子中的项是已知的：$\\mathbb{P}(F=1 \\mid G) = s$ 和 $\\mathbb{P}(G) = \\alpha$。\n\n分母 $\\mathbb{P}(F=1)$ 是一个序列被标记的总概率。我们使用全概率定律，以序列是好的（$G$）还是不好的（$\\overline{G}$）为条件进行计算：\n$$\\mathbb{P}(F=1) = \\mathbb{P}(F=1 \\mid G)\\mathbb{P}(G) + \\mathbb{P}(F=1 \\mid \\overline{G})\\mathbb{P}(\\overline{G})$$\n我们已知各组成部分的概率：\n- $\\mathbb{P}(F=1 \\mid G) = s$ (灵敏度)\n- $\\mathbb{P}(G) = \\alpha$\n- $\\mathbb{P}(F=1 \\mid \\overline{G}) = t$ (假阳性率)\n- $\\mathbb{P}(\\overline{G}) = 1 - \\mathbb{P}(G) = 1 - \\alpha$\n\n将这些代入全概率定律，得到：\n$$\\mathbb{P}(F=1) = s\\alpha + t(1-\\alpha)$$\n这是整个库中被分类器标记的比例。\n\n现在，我们将此代回到 $p_{\\text{adapt}}$ 的表达式中：\n$$p_{\\text{adapt}} = \\frac{s\\alpha}{s\\alpha + t(1-\\alpha)}$$\n这是自适应策略下单次抽样的成功概率。抽样独立的假设使我们能够将其建模为一系列 $n$ 次伯努利试验。至少有一次发现的概率是：\n$$P_2 = 1 - (1 - p_{\\text{adapt}})^n = 1 - \\left(1 - \\frac{s\\alpha}{s\\alpha + t(1-\\alpha)}\\right)^n$$\n我们可以简化指数内的项：\n$$1 - \\frac{s\\alpha}{s\\alpha + t(1-\\alpha)} = \\frac{s\\alpha + t(1-\\alpha) - s\\alpha}{s\\alpha + t(1-\\alpha)} = \\frac{t(1-\\alpha)}{s\\alpha + t(1-\\alpha)}$$\n因此，自适应抽样下至少有一次发现的概率是：\n$$P_2 = 1 - \\left(\\frac{t(1-\\alpha)}{s\\alpha + t(1-\\alpha)}\\right)^n$$\n\n**概率之比**\n最后，我们计算问题陈述中定义的比率 $R(\\alpha, s, t, n)$：\n$$R(\\alpha, s, t, n) = \\frac{P_2}{P_1} = \\frac{\\text{自适应抽样下至少有一次发现的概率}}{\\text{均匀随机抽样下至少有一次发现的概率}}$$\n代入 $P_1$ 和 $P_2$ 的表达式：\n$$R(\\alpha, s, t, n) = \\frac{1 - \\left(\\frac{t(1-\\alpha)}{s\\alpha + t(1-\\alpha)}\\right)^n}{1 - (1 - \\alpha)^n}$$\n这就是该比率的最终、简化的闭式解析表达式。", "answer": "$$\\boxed{\\frac{1 - \\left(\\frac{t(1-\\alpha)}{s\\alpha + t(1-\\alpha)}\\right)^n}{1 - (1 - \\alpha)^n}}$$", "id": "2749115"}, {"introduction": "在了解了模型引导搜索的“为什么”之后，我们来深入探究模型本身是如何工作的。卷积神经网络 (CNNs) 是从序列数据（如DNA）中识别模式（如基序）的强大工具。本练习将通过一个具体的例子，让你亲手计算损失函数对于一个卷积核权重的梯度 [@problem_id:2749093]，这正是神经网络通过反向传播算法学习和更新自身参数以识别关键特征的基本步骤，从而揭开其学习过程的神秘面纱。", "problem": "在合成生物学的从头调控序列设计中，卷积神经网络常用于检测以 one-hot 向量编码的脱氧核糖核酸 (DNA) 序列中的短序列基序。考虑一个单滤波器一维卷积模型，该模型将单个输入的DNA序列映射到一个用于预测增强子活性的标量值。输入序列的长度为 $L=5$，其字母表为 $\\{A,C,G,T\\}$，并以 $(A,C,G,T)$ 的列顺序进行 one-hot 编码。滤波器的宽度为 $k=3$，其权重矩阵为 $w \\in \\mathbb{R}^{3 \\times 4}$，其中行由偏移量 $j \\in \\{0,1,2\\}$ 索引，列按 $(A,C,G,T)$ 的顺序由核苷酸索引。该模型使用互相关（深度学习中一维卷积的惯例）、步幅为 $1$、无填充且无偏置。对于位置 $t \\in \\{0,1,2\\}$，其预激活值为\n$$\nz_t \\;=\\; \\sum_{j=0}^{2} \\sum_{c \\in \\{A,C,G,T\\}} w_{j,c} \\, x_{t+j,c},\n$$\n其中 $x_{i,c} \\in \\{0,1\\}$ 是位置 $i$ 处核苷酸的 one-hot 编码。激活函数为修正线性单元 (ReLU)，定义为 $\\phi(u) = \\max(0,u)$。模型通过求和进行池化，以产生 $\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$。对于目标值为 $y$ 的单个训练样本，其损失为均方误差 (MSE) $L = \\tfrac{1}{2}\\,(\\hat{y} - y)^2$。\n\n对于下面指定的单个训练样本，计算梯度 $\\nabla_w L$，即所有的偏导数 $\\frac{\\partial L}{\\partial w_{j,c}}$。将最终答案表示为一个单行矩阵，按顺序 $(j=0 \\text{ 行 } [A,C,G,T],\\, j=1 \\text{ 行 } [A,C,G,T],\\, j=2 \\text{ 行 } [A,C,G,T])$ 列出其元素。无需四舍五入。\n\n输入规范：\n- 序列（长度 $L=5$）：位置 $0$ 到 $4$ 依次为 $\\big(A,\\,C,\\,G,\\,T,\\,A\\big)$。\n- one-hot 编码的列顺序：$(A,C,G,T)$。\n- 滤波器宽度 $k=3$，步幅 $1$，有效互相关，无偏置。\n- 权重\n$$\nw \\;=\\;\n\\begin{pmatrix}\n2 & -1 & 0 & 1\\\\\n-1 & 2 & 1 & -2\\\\\n0 & 1 & -1 & 2\n\\end{pmatrix},\n$$\n行 $j=0,1,2$ 和列 $(A,C,G,T)$。\n- 激活函数：修正线性单元 (ReLU)，$\\phi(u) = \\max(0,u)$。\n- 池化：求和，$\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$。\n- 损失：$L = \\tfrac{1}{2}\\,(\\hat{y} - y)^2$，目标值 $y=4$。\n\n按要求提供梯度。您的答案必须是一个由实数组成的单行矩阵。", "solution": "该问题要求计算损失函数 $L$ 相对于滤波器权重 $w$ 的梯度，记作 $\\nabla_w L$。这需要应用多元微积分中的链式法则，该过程在此背景下称为反向传播。我们必须计算 $j \\in \\{0,1,2\\}$ 和 $c \\in \\{A,C,G,T\\}$ 的每一个偏导数 $\\frac{\\partial L}{\\partial w_{j,c}}$。\n\n模型的结构如下：\n$x \\rightarrow z \\rightarrow \\phi(z) \\rightarrow \\hat{y} \\rightarrow L$。\n损失 $L = \\frac{1}{2}(\\hat{y} - y)^2$，其中 $\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$ 且 $z_t = \\sum_{j=0}^{2} \\sum_{c} w_{j,c} x_{t+j,c}$。\n\n我们首先从前向传播开始，计算所有必要的中间值。\n\n首先，我们将输入序列 $(A,C,G,T,A)$ 表示为一个 one-hot 编码矩阵 $x \\in \\mathbb{R}^{5 \\times 4}$，其中行对应位置 $0$ 到 $4$，列对应核苷酸 $(A,C,G,T)$。\n$$\nx =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0\n\\end{pmatrix}\n$$\n各行为 $x_0, x_1, x_2, x_3, x_4$。\n\n接下来，我们使用互相关运算计算 $t \\in \\{0,1,2\\}$ 的预激活值 $z_t$。\n对于 $t=0$，输入窗口为 $(x_0, x_1, x_2)$，对应核苷酸 $(A,C,G)$。\n$$\nz_0 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j)} = w_{0,A} + w_{1,C} + w_{2,G} = 2 + 2 + (-1) = 3\n$$\n对于 $t=1$，输入窗口为 $(x_1, x_2, x_3)$，对应核苷酸 $(C,G,T)$。\n$$\nz_1 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j+1)} = w_{0,C} + w_{1,G} + w_{2,T} = (-1) + 1 + 2 = 2\n$$\n对于 $t=2$，输入窗口为 $(x_2, x_3, x_4)$，对应核苷酸 $(G,T,A)$。\n$$\nz_2 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j+2)} = w_{0,G} + w_{1,T} + w_{2,A} = 0 + (-2) + 0 = -2\n$$\n预激活值的向量为 $z = (3, 2, -2)$。\n\n我们将 ReLU 激活函数 $\\phi(u) = \\max(0,u)$ 应用于每个 $z_t$。\n$$\n\\phi(z_0) = \\max(0,3) = 3\n$$\n$$\n\\phi(z_1) = \\max(0,2) = 2\n$$\n$$\n\\phi(z_2) = \\max(0,-2) = 0\n$$\n激活值的向量为 $(3, 2, 0)$。\n\n最终预测值 $\\hat{y}$ 通过对激活值求和得到。\n$$\n\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t) = 3 + 2 + 0 = 5\n$$\n\n现在，我们进行反向传播来计算梯度 $\\nabla_w L$。损失 $L$ 相对于权重 $w_{j,c}$ 的偏导数由链式法则给出：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_{j,c}}\n$$\n我们进一步分析 $\\frac{\\partial \\hat{y}}{\\partial w_{j,c}}$ 这一项。由于 $\\hat{y}$ 是 $\\phi(z_t)$ 的和，并且每个 $z_t$ 都依赖于权重 $w$，我们有：\n$$\n\\frac{\\partial \\hat{y}}{\\partial w_{j,c}} = \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial w_{j,c}} = \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_{j,c}}\n$$\n将这些结合起来，梯度元素的全表达式为：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = \\frac{\\partial L}{\\partial \\hat{y}} \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_{j,c}}\n$$\n我们计算该表达式的每个组成部分。\n损失 $L$ 相对于预测值 $\\hat{y}$ 的导数为：\n$$\n\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y = 5 - 4 = 1\n$$\nReLU 函数的导数是一个阶跃函数：当 $u > 0$ 时 $\\phi'(u) = 1$，当 $u \\le 0$ 时为 $0$。我们将其表示为 $\\mathbb{I}(u > 0)$。\n$$\n\\frac{\\partial \\phi(z_0)}{\\partial z_0} = \\mathbb{I}(3 > 0) = 1\n$$\n$$\n\\frac{\\partial \\phi(z_1)}{\\partial z_1} = \\mathbb{I}(2 > 0) = 1\n$$\n$$\n\\frac{\\partial \\phi(z_2)}{\\partial z_2} = \\mathbb{I}(-2 > 0) = 0\n$$\n预激活值 $z_t$ 相对于权重 $w_{j,c}$ 的导数为：\n$$\n\\frac{\\partial z_t}{\\partial w_{j,c}} = \\frac{\\partial}{\\partial w_{j,c}} \\left( \\sum_{j'=0}^{2} \\sum_{c'} w_{j',c'} \\, x_{t+j',c'} \\right) = x_{t+j,c}\n$$\n因为 $x_{t+j',c'}$ 相对于 $w_{j,c}$ 是常数，并且只有在索引匹配时导数才不为零。\n\n将这些分量代回梯度表达式中：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = (1) \\left( (1) \\cdot \\frac{\\partial z_0}{\\partial w_{j,c}} + (1) \\cdot \\frac{\\partial z_1}{\\partial w_{j,c}} + (0) \\cdot \\frac{\\partial z_2}{\\partial w_{j,c}} \\right) = \\frac{\\partial z_0}{\\partial w_{j,c}} + \\frac{\\partial z_1}{\\partial w_{j,c}}\n$$\n使用 $\\frac{\\partial z_t}{\\partial w_{j,c}} = x_{t+j,c}$，该式可简化为：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = x_{j,c} + x_{j+1,c}\n$$\n该公式表明，相对于权重 $w_{j,c}$ 的梯度是输入序列中位置 $j$ 和 $j+1$ 处核苷酸 $c$ 的 one-hot 编码值的和，因为只有前两个卷积窗口 ($t=0,1$) 产生了正的预激活值。\n\n现在我们计算梯度矩阵 $\\nabla_w L \\in \\mathbb{R}^{3 \\times 4}$。\n\n对于 $j=0$：\n$(\\nabla_w L)_{0,c} = x_{0,c} + x_{1,c}$。\n$x_0$ 对应于 $A \\implies (1,0,0,0)$。\n$x_1$ 对应于 $C \\implies (0,1,0,0)$。\n第一行权重的梯度为 $(1,0,0,0) + (0,1,0,0) = (1,1,0,0)$。\n\n对于 $j=1$：\n$(\\nabla_w L)_{1,c} = x_{1,c} + x_{2,c}$。\n$x_1$ 对应于 $C \\implies (0,1,0,0)$。\n$x_2$ 对应于 $G \\implies (0,0,1,0)$。\n第二行权重的梯度为 $(0,1,0,0) + (0,0,1,0) = (0,1,1,0)$。\n\n对于 $j=2$：\n$(\\nabla_w L)_{2,c} = x_{2,c} + x_{3,c}$。\n$x_2$ 对应于 $G \\implies (0,0,1,0)$。\n$x_3$ 对应于 $T \\implies (0,0,0,1)$。\n第三行权重的梯度为 $(0,0,1,0) + (0,0,0,1) = (0,0,1,1)$。\n\n组合成完整的梯度矩阵：\n$$\n\\nabla_w L =\n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n$$\n问题要求答案为一个单行矩阵，该矩阵通过连接 $\\nabla_w L$ 的各行形成。\n得到的向量是 $(1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n}\n$$", "id": "2749093"}, {"introduction": "我们已经知道模型可以学习并指导实验，但如何利用模型来智能地选择*下一个*最有价值的实验对象呢？贝叶斯优化为此提供了一个强大的理论框架。本练习将要求你推导其核心组件——期望提升 (Expected Improvement, EI) 采集函数 [@problem_id:2749128]。EI 公式精确地量化了在给定当前模型预测的均值 $\\mu(x)$ 和不确定性 $\\sigma^2(x)$ 的情况下，探索一个新设计点的潜在价值，完美地平衡了“利用”已知高性能区域与“探索”未知区域之间的权衡。", "problem": "在一个旨在优化合成启动子以最大化转录输出的闭环设计活动中，您使用带有高斯过程 (GP) 代理模型的贝叶斯优化。对于一个候选设计 $x$，未知真实目标 $f(x)$ 的 GP 后验预测分布是均值为 $\\mu(x)$、方差为 $\\sigma^{2}(x)$ 的高斯分布，记为 $f(x) \\sim \\mathcal{N}\\!\\left(\\mu(x), \\sigma^{2}(x)\\right)$。设当前观测到的最佳目标值为 $f_{\\text{best}}$，并假设目标是最大化 $f(x)$。\n\n将改进随机变量定义为 $I(x) = \\max\\!\\big(0, f(x) - f_{\\text{best}}\\big)$，并将期望改进 (Expected Improvement, EI) 采集函数定义为 $\\text{EI}(x) = \\mathbb{E}\\!\\left[I(x)\\right]$，其中期望是基于 GP 后验预测分布计算的。从这些定义和 $f(x)$ 的正态性出发，推导 $\\text{EI}(x)$ 的一个闭式表达式，该表达式应使用 $\\mu(x)$、$\\sigma(x)$、$f_{\\text{best}}$、标准正态累积分布函数 $\\Phi(\\cdot)$ 和标准正态概率密度函数 $\\phi(\\cdot)$ 来表示。假设 $\\sigma(x) > 0$。您的最终答案必须是单一的闭式解析表达式。不要在最终答案框中提供中间步骤。", "solution": "该问题陈述是贝叶斯优化领域的一个标准推导，它科学严谨、问题明确且客观。其中不包含逻辑或事实性错误。因此，我们开始进行求解。\n\n目标是推导出期望改进 (EI) 采集函数的闭式表达式。我们已知以下定义：\n在点 $x$ 处，目标函数 $f(x)$ 的后验预测分布为高斯分布：\n$$f(x) \\sim \\mathcal{N}(\\mu(x), \\sigma^{2}(x))$$\n改进随机变量 $I(x)$ 是相对于当前观测到的最佳值 $f_{\\text{best}}$ 定义的：\n$$I(x) = \\max(0, f(x) - f_{\\text{best}})$$\n期望改进是该随机变量的期望，该期望是针对 $f(x)$ 的后验预测分布计算的：\n$$\\text{EI}(x) = \\mathbb{E}[I(x)] = \\mathbb{E}[\\max(0, f(x) - f_{\\text{best}})]$$\n为了计算这个期望，我们必须对 $f(x)$ 的概率密度函数上的改进值进行积分。令 $p(y | x)$ 为高斯分布 $\\mathcal{N}(\\mu(x), \\sigma^{2}(x))$ 在 $y$ 处的概率密度函数 (PDF)。那么，期望由以下积分给出：\n$$\\text{EI}(x) = \\int_{-\\infty}^{\\infty} \\max(0, y - f_{\\text{best}}) \\, p(y | x) \\, dy$$\n项 $\\max(0, y - f_{\\text{best}})$ 仅在 $y > f_{\\text{best}}$ 时非零。因此，我们可以将积分下限改为 $f_{\\text{best}}$ 并去掉 $\\max$ 算子：\n$$\\text{EI}(x) = \\int_{f_{\\text{best}}}^{\\infty} (y - f_{\\text{best}}) \\, p(y | x) \\, dy$$\nPDF $p(y|x)$ 由下式给出：\n$$p(y|x) = \\frac{1}{\\sqrt{2\\pi}\\sigma(x)} \\exp\\left(-\\frac{(y - \\mu(x))^2}{2\\sigma^{2}(x)}\\right)$$\n为了简化积分，我们进行变量替换以标准化分布。定义一个新的随机变量 $Z$ 为：\n$$Z = \\frac{y - \\mu(x)}{\\sigma(x)}$$\n这个变量 $Z$ 服从标准正态分布 $Z \\sim \\mathcal{N}(0, 1)$。根据此定义，我们有 $y = \\mu(x) + \\sigma(x)Z$，其微分为 $dy = \\sigma(x) dZ$。$Z$ 的 PDF 是标准正态 PDF，记为 $\\phi(z)$。积分下限 $y = f_{\\text{best}}$ 变为：\n$$z_{lower} = \\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}$$\n上限 $y \\to \\infty$ 对应于 $z \\to \\infty$。将这些代入积分中，得到：\n$$\\text{EI}(x) = \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} ((\\mu(x) + \\sigma(x)z) - f_{\\text{best}}) \\, \\phi(z) \\, dz$$\n我们可以重新整理积分内的项，并将其拆分为两部分：\n$$\\text{EI}(x) = \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} (\\mu(x) - f_{\\text{best}}) \\phi(z) dz + \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} \\sigma(x)z \\phi(z) dz$$\n为了标记清晰，我们定义缩放后的改进项：\n$$Z_{imp} = \\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}$$\n注意 $\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)} = -Z_{imp}$。积分变为：\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\int_{-Z_{imp}}^{\\infty} \\phi(z) dz + \\sigma(x) \\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz$$\n我们分别计算每个积分。\n\n对于第一个积分，我们认识到它与标准正态分布的累积分布函数 (CDF) $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$ 相关。\n$$\\int_{-Z_{imp}}^{\\infty} \\phi(z) dz = 1 - \\int_{-\\infty}^{-Z_{imp}} \\phi(z) dz = 1 - \\Phi(-Z_{imp})$$\n利用标准正态 CDF 的性质 $1 - \\Phi(-z) = \\Phi(z)$，上式简化为：\n$$\\int_{-Z_{imp}}^{\\infty} \\phi(z) dz = \\Phi(Z_{imp})$$\n所以第一项是 $(\\mu(x) - f_{\\text{best}}) \\Phi(Z_{imp})$。\n\n对于第二个积分，我们有 $\\int z \\phi(z) dz$。被积函数为 $z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$。\n$$\\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz = \\int_{-Z_{imp}}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$$\n这个积分可以直接求解，因为 $-\\phi'(z) = z \\phi(z)$。或者，使用换元法，令 $u = -z^2/2$，则 $du = -z dz$。\n$$\\int z \\exp\\left(-\\frac{z^2}{2}\\right) dz = -\\int \\exp(u) du = -\\exp(u) = -\\exp\\left(-\\frac{z^2}{2}\\right)$$\n计算定积分：\n$$\\sigma(x) \\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz = \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\left[ -\\exp\\left(-\\frac{z^2}{2}\\right) \\right]_{-Z_{imp}}^{\\infty}$$\n$$= \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\left( 0 - \\left(-\\exp\\left(-\\frac{(-Z_{imp})^2}{2}\\right)\\right) \\right)$$\n$$= \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{Z_{imp}^2}{2}\\right)$$\n这恰好是 $\\sigma(x)\\phi(Z_{imp})$，因为标准正态 PDF 是一个偶函数，$\\phi(z) = \\phi(-z)$，且其定义为 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{z^2}{2})$。\n\n将两项合并，我们得到：\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\Phi(Z_{imp}) + \\sigma(x) \\phi(Z_{imp})$$\n将 $Z_{imp}$ 的定义代回：\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\Phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right)$$\n这就是期望改进的最终闭式表达式。它正确地依赖于 $\\mu(x)$、$\\sigma(x)$、$f_{\\text{best}}$ 以及标准正态 CDF $\\Phi$ 和 PDF $\\phi$。假设 $\\sigma(x) > 0$ 确保了分母不为零，从而验证了推导的有效性。", "answer": "$$\\boxed{(\\mu(x) - f_{\\text{best}}) \\Phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right)}$$", "id": "2749128"}]}