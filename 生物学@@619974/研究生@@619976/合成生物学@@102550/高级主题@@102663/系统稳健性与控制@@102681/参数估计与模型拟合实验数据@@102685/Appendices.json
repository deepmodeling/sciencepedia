{"hands_on_practices": [{"introduction": "将模型与实验数据拟合时，非线性最小二乘法是基础工具，但其容易陷入局部最优解的陷阱，导致参数估计结果不准确。本练习将引导你运用一种名为“同伦延拓”的实用策略，来规避这一常见问题。通过在一个简化的模型（即一个经典的基因激活模块）上逐步求解一系列相关问题，你将学会如何引导优化算法找到更优的参数解，这是参数估计中一项至关重要的实践技能 [@problem_id:2757696]。", "problem": "您的任务是实施一种同伦延拓策略，根据稳态数据来估计合成生物学中一个转录激活模块的参数，并在非线性最小二乘拟合过程中避免陷入劣质的局部最小值。请从以下适用于基因表达模块的基本基础开始：分子生物学的中心法则指出，脱氧核糖核酸（DNA）转录为核糖核酸（RNA），后者再翻译成蛋白质。在恒定的诱导剂输入下，单个基因产物的浓度（表示为 $P(t)$）可以通过一个质量平衡方程来建模，其中合成速率（取决于诱导剂输入）和一级损失速率（降解和稀释）决定了 $dP/dt$。稳态 $P_{\\mathrm{ss}}$ 由这些相反通量的平衡所定义。诱导剂与启动子之间的相互作用可以用希尔（Hill）激活函数来建模。所有符号必须按照下文所述的单位进行解释。\n\n您的模型假设和定义如下：\n- 动态质量平衡：$dP/dt = \\text{synthesis} - \\text{loss}$。在稳态下，$dP/dt = 0$。\n- 一级损失：$\\text{loss} = k_{\\mathrm{deg}} P$，其中 $k_{\\mathrm{deg}}$ 是一个正常数（单位为 $\\mathrm{min}^{-1}$）。\n- 合成速率取决于诱导剂浓度 $u$，通过一个Hill激活函数表示：$\\text{synthesis} = k_{\\mathrm{syn}} \\cdot f(u)$，其中 $f(u) = \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$。\n- 使用 $S = k_{\\mathrm{syn}}/k_{\\mathrm{deg}}$（单位为任意荧光单位）进行重新参数化，使得稳态蛋白质水平满足 $P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$。\n- 所有输入 $u$ 均为诱导剂浓度，单位是微摩尔（$\\mu\\mathrm{M}$）。输出 $P_{\\mathrm{ss}}$ 的单位是任意荧光单位（a.u.）。Hill系数 $n$ 是无量纲的。\n\n目标：对于下面列出的每个测试案例，通过最小化模型预测与所提供的“实验”稳态读数之间的残差平方和，来估计参数向量 $\\theta = [S, K_{\\mathrm{d}}, n]$。为避免陷入劣质局部最小值，请对输入振幅使用同伦延拓：定义一个尺度参数 $s \\in (0,1]$，将所有输入按 $s$ 进行缩放，即 $u \\mapsto s\\,u$，并在每个连续的 $s$ 值上，从前一个解初始化来求解最小二乘问题。从一个故意设定的较差初始猜测值开始，并沿尺度序列逐步推进，直到 $s = 1$。\n\n同伦过程要求：\n- 设尺度序列为 $s \\in \\{0.1, 0.3, 0.6, 1.0\\}$。\n- 在每个 $s$ 值上，使用上一步的估计值作为初始猜测，将 $\\theta$ 拟合到在缩放输入 $s\\,u$ 下测量的稳态数据。使用边界约束 $S \\in [10^{-6}, 5 \\cdot 10^{3}]$, $K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$ 和 $n \\in [0.5, 6.0]$。\n- 在 $s = 0.1$ 时，使用初始猜测值 $\\theta_{0} = [500.0, 500.0, 3.0]$。\n- 在每个 $s$ 值上，最小化未加权残差平方和 $\\sum_{i} \\left(P_{\\mathrm{ss}}(s\\,u_{i}; \\theta) - y_{i}(s)\\right)^{2}$，其中 $y_{i}(s)$ 是在尺度 $s$ 下提供的“实验”稳态读数。\n\n数据和单位：\n- 对所有案例使用同一组基础输入浓度：$u \\in \\{0, 10, 50, 200\\}$（单位为 $\\mu\\mathrm{M}$）。同伦尺度 $s$ 将这些浓度转换为 $s\\,u$。\n- 在本练习中，每个 $s$ 处的“实验”稳态读数 $y_{i}(s)$ 是由同一模型使用下面给出的真实参数无噪声生成的。这构成了一个完全确定性的测试，而拟合算法除了用于生成合成数据外，不得以任何方式使用真实参数。\n\n测试套件（三个具有不同机制的案例）：\n1. 理想路径案例：真实参数 $\\theta^{\\ast}_{1} = [1200.0, 30.0, 2.0]$，其中 $S$ 单位为 a.u.，$K_{\\mathrm{d}}$ 单位为 $\\mu\\mathrm{M}$。\n2. 高协同性案例：真实参数 $\\theta^{\\ast}_{2} = [800.0, 5.0, 4.0]$。\n3. 低动态范围案例：真实参数 $\\theta^{\\ast}_{3} = [300.0, 100.0, 1.5]$。\n\n需要实现的任务：\n- 对于每个案例 $k \\in \\{1,2,3\\}$，构建尺度列表 $[0.1, 0.3, 0.6, 1.0]$、基础输入列表 $[0, 10, 50, 200]$，并为每个尺度 $s$ 计算合成稳态读数 $y_{i}^{(k)}(s) = P_{\\mathrm{ss}}(s\\,u_{i}; \\theta^{\\ast}_{k})$。\n- 按照尺度升序实现同伦延拓拟合，从 $s=0.1$ 和初始猜测值 $\\theta_{0} = [500.0, 500.0, 3.0]$ 开始，并遵守指定的边界约束。在每个尺度上，仅对该尺度的数据进行拟合。\n- 返回每个案例在 $s=1.0$ 时的最终估计值。\n\n数值和单位要求：\n- 报告每个案例 $k \\in \\{1,2,3\\}$ 的最终估计值 $\\widehat{\\theta}_{k} = [\\widehat{S}_{k}, \\widehat{K}_{\\mathrm{d},k}, \\widehat{n}_{k}]$，其中 $\\widehat{S}_{k}$ 的单位为 a.u.，$\\widehat{K}_{\\mathrm{d},k}$ 的单位为 $\\mu\\mathrm{M}$，$\\widehat{n}_{k}$ 为无量纲。\n- 将报告的每个数字四舍五入到三位小数。不应打印任何其他单位。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个含三个内部列表的列表，每个内部列表对应一个测试案例，形式为 $[\\widehat{S}, \\widehat{K}_{\\mathrm{d}}, \\widehat{n}]$，其中的三个数字均四舍五入到三位小数。输出不得包含空格，且整行必须用方括号括起来。例如，包含三个案例的输出格式应如 `[[a,b,c],[d,e,f],[g,h,i]]`，其中每个符号都被替换为四舍五入到三位小数的十进制数。", "solution": "在尝试任何解决方案之前，需对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **模型**：稳态蛋白质水平\n$$ P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}} $$\n- **参数**：$\\theta = [S, K_{\\mathrm{d}}, n]$。\n  - $S$：最大合成/降解速率比（a.u.）。\n  - $K_{\\mathrm{d}}$：激活常数（$\\mu\\mathrm{M}$）。\n  - $n$：Hill系数（无量纲）。\n- **目标**：使用同伦延拓策略为三个测试案例估计 $\\theta$，以最小化残差平方和。\n- **同伦过程**：\n  - 尺度序列：$s \\in \\{0.1, 0.3, 0.6, 1.0\\}$。\n  - 在每个 $s$ 值上，使用上一步的估计作为初始猜测，将 $\\theta$ 拟合到缩放输入 $s \\cdot u$ 下的数据。\n  - $s=0.1$ 时的初始猜测：$\\theta_{0} = [500.0, 500.0, 3.0]$。\n  - 参数边界：$S \\in [10^{-6}, 5 \\cdot 10^{3}]$, $K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$, $n \\in [0.5, 6.0]$。\n  - 目标函数：非线性最小二乘法，最小化 $\\sum_{i} \\left(P_{\\mathrm{ss}}(s\\,u_{i}; \\theta) - y_{i}(s)\\right)^{2}$。\n- **数据**：\n  - 基础输入浓度：$u \\in \\{0, 10, 50, 200\\}$ ($\\mu\\mathrm{M}$)。\n  - “实验”数据 $y_{i}(s)$ 是使用真实参数从模型中无噪声生成的：$y_{i}(s) = P_{\\mathrm{ss}}(s\\,u_{i}; \\theta^{\\ast})$。\n- **测试案例（真实参数）**：\n  1. $\\theta^{\\ast}_{1} = [1200.0, 30.0, 2.0]$。\n  2. $\\theta^{\\ast}_{2} = [800.0, 5.0, 4.0]$。\n  3. $\\theta^{\\ast}_{3} = [300.0, 100.0, 1.5]$。\n- **输出规范**：\n  - 报告每个案例 $k \\in \\{1,2,3\\}$ 的最终估计值 $\\widehat{\\theta}_{k} = [\\widehat{S}_{k}, \\widehat{K}_{\\mathrm{d},k}, \\widehat{n}_{k}]$。\n  - 将每个数字四舍五入到三位小数。\n  - 最终输出为单行、一个列表的列表，无空格：`[[a,b,c],[d,e,f],[g,h,i]]`。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学性（关键）**：该问题基于质量作用动力学和Hill函数，这些是分子与系统生物学中描述基因调控网络的经典模型。这是一种标准的、科学上可靠的方法。\n- **适定性（关键）**：该问题是适定的。它详细说明了一个特定的数值程序（用于非线性最小二乘的同伦延拓），包含了所有必要组成部分：一个模型、一个目标函数、初始条件、参数边界以及清晰的步骤序列。使用无噪声的合成数据确保了“真实”解（用于生成数据的参数）的存在，可以据此衡量算法的性能。\n- **客观性（关键）**：该问题以精确、定量和客观的语言陈述，没有歧义或主观解释。\n- **不完整或矛盾的设置**：该问题是自洽的，并提供了所有必要信息。所提供的数据或约束中没有矛盾之处。\n- **不切实际或不可行**：参数值、浓度和模型行为对于合成基因线路是合理的。该任务是一个标准的计算机模拟（in-silico）实验。\n- **非形式化或不相关**：该问题是*合成生物学*领域内*参数估计和模型拟合实验数据*的一个典型例子。它直接相关且可在数学上形式化。\n\n### 步骤 3：结论与行动\n问题被判定为有效。它构成了一个系统生物学中定义明确的计算任务，基于已确立的科学原理。我们继续进行求解。\n\n---\n\n该问题要求为一个非线性生物模型估计参数。该模型将蛋白质的稳态浓度 $P_{\\mathrm{ss}}$ 描述为诱导剂浓度 $u$ 的函数。控制方程是一个重新参数化的Hill激活函数：\n$$ P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}} $$\n待确定的参数向量是 $\\theta = [S, K_{\\mathrm{d}}, n]$。$S$ 代表最大表达水平，$K_{\\mathrm{d}}$ 是激活系数（产生半最大响应时 $u$ 的浓度），$n$ 是Hill系数，它量化了转录响应的协同性或陡峭程度。\n\n参数估计被表述为一个非线性最小二乘优化问题。目标是找到参数向量 $\\theta$，以最小化模型预测与所提供的“实验”数据 $y_i$ 之间的残差平方和（SSR）。对于一组 $N$ 个输入-输出对 $(u_i, y_i)$，目标函数为：\n$$ \\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\left( P_{\\mathrm{ss}}(u_{i}; \\theta) - y_{i} \\right)^{2} $$\n直接最小化这样一个非凸函数可能很困难，因为优化算法可能会收敛到次优的局部最小值，特别是当从远离全局最小值的初始猜测开始时。为了规避这个问题，题目规定使用同伦延拓方法。这种技术将原始的难题变形为一系列更简单、相关的问题，引导优化器走向一个更好的解。\n\n同伦是通过引入一个延拓参数 $s$ 来构建的，该参数用于缩放输入浓度：$u \\mapsto s \\cdot u$，$s \\in (0, 1]$。首先针对一个小的 $s$ 值（例如 $s=0.1$）求解优化问题。在小的 $s$ 值下，缩放后的输入 $s \\cdot u_i$ 都处于低浓度区域，此时Hill函数近似为一个幂律（$P_{\\mathrm{ss}} \\approx S (u/K_{\\mathrm{d}})^n$），这使得优化景观更加平滑。从这个简化问题得到的解，将作为序列中下一个问题（使用稍大的 $s$ 值）的初始猜测。这个迭代过程一直持续到 $s=1$ 为止，此时求解的是原始的全范围问题，但得益于从延拓路径推导出的一个信息量高的初始猜测。\n\n对于由其真实参数向量 $\\theta^{\\ast}_{k}$ 定义的三个测试案例中的每一个，指定的算法按以下方式执行：\n1.  定义尺度参数的离散序列 $s_j \\in \\{0.1, 0.3, 0.6, 1.0\\}$。\n2.  用一个故意设定的较差猜测值来初始化参数估计：$\\hat{\\theta}^{(0)} = [500.0, 500.0, 3.0]$。\n3.  对 $j=1, 2, 3, 4$ 遍历尺度 $s_j$：\n    a. 对于当前测试案例 $k$ 和尺度 $s_j$，使用基础输入浓度 $u_i \\in \\{0, 10, 50, 200\\}$ 生成合成数据点 $y_{i,k}^{(j)} = P_{\\mathrm{ss}}(s_j u_i; \\theta^{\\ast}_{k})$。\n    b. 此优化步骤的残差向量定义为 $\\mathbf{r}(\\theta)$，其第 $i$ 个分量是 $r_i(\\theta) = P_{\\mathrm{ss}}(s_j u_i; \\theta) - y_{i,k}^{(j)}$。\n    c. 求解有界非线性最小二乘问题：\n    $$ \\hat{\\theta}^{(j)} = \\arg\\min_{\\theta \\in \\Theta} \\| \\mathbf{r}(\\theta) \\|_2^2 $$\n    优化从上一步的估计值 $\\hat{\\theta}^{(j-1)}$ 开始初始化。参数搜索空间 $\\Theta$ 受到指定边界的约束：$S \\in [10^{-6}, 5 \\cdot 10^{3}]$, $K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$, 以及 $n \\in [0.5, 6.0]$。\n4.  测试案例 $k$ 的最终估计参数向量是延拓过程最后一步的结果，即 $\\widehat{\\theta}_{k} = \\hat{\\theta}^{(4)}$，对应于 $s=1.0$ 时的解。\n\n该过程使用 `scipy.optimize.least_squares` 函数实现，该函数非常适合有界非线性最小二乘问题。由于数据是由被拟合的同一模型综合生成的，并且不含噪声，因此一个成功的优化过程应能在数值精度的限制内恢复真实参数 $\\theta^{\\ast}_{k}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Implements a homotopy continuation strategy to estimate parameters of a\n    transcriptional activation module from synthetic steady-state data.\n    \"\"\"\n\n    # Define the base input concentrations (in micromolar)\n    base_u = np.array([0., 10., 50., 200.])\n\n    # Define the test cases with their true parameters [S, Kd, n]\n    test_cases = [\n        [1200.0, 30.0, 2.0],  # Happy-path case\n        [800.0, 5.0, 4.0],   # High-cooperativity case\n        [300.0, 100.0, 1.5], # Low-dynamic-range case\n    ]\n\n    # Homotopy continuation parameters\n    scales = [0.1, 0.3, 0.6, 1.0]\n    initial_guess = np.array([500.0, 500.0, 3.0])\n    bounds = ([1e-6, 1e-6, 0.5], [5000.0, 10000.0, 6.0])\n\n    def p_ss_model(theta, u):\n        \"\"\"\n        Calculates the steady-state protein level using the Hill activation model.\n        P_ss(u; S, Kd, n) = S * (u^n) / (Kd^n + u^n)\n        \n        Args:\n            theta (array-like): Parameter vector [S, Kd, n].\n            u (array-like): Inducer concentrations.\n        \n        Returns:\n            numpy.ndarray: Predicted steady-state protein levels.\n        \"\"\"\n        S, Kd, n = theta\n        # Use np.power with a float exponent for robustness\n        n_float = float(n)\n        u_n = np.power(u, n_float)\n        Kd_n = np.power(Kd, n_float)\n        \n        denominator = Kd_n + u_n\n        \n        # Handle cases where denominator might be zero or near-zero, although\n        # bounds on Kd and u>=0 should prevent this.\n        # Create a zero array for the result\n        result = np.zeros_like(u)\n        # Find indices where denominator is non-zero\n        idx = denominator > 0\n        # Calculate fraction only for those indices\n        result[idx] = u_n[idx] / denominator[idx]\n        \n        return S * result\n\n    def residuals(theta, u, y_data):\n        \"\"\"\n        Calculates the residuals between the model prediction and experimental data.\n        \n        Args:\n            theta (array-like): Parameter vector [S, Kd, n].\n            u (array-like): Inducer concentrations.\n            y_data (array-like): Experimental data points.\n        \n        Returns:\n            numpy.ndarray: Vector of residuals.\n        \"\"\"\n        return p_ss_model(theta, u) - y_data\n\n    final_results = []\n\n    for true_params in test_cases:\n        # Reset the parameter estimate to the initial guess for each new test case\n        theta_estimate = np.copy(initial_guess)\n\n        # Homotopy continuation loop through the scales\n        for s in scales:\n            # 1. Scale inputs for the current continuation step\n            scaled_u = s * base_u\n            \n            # 2. Generate synthetic \"experimental\" data for this scale using true parameters\n            y_data = p_ss_model(true_params, scaled_u)\n            \n            # 3. Perform nonlinear least-squares fitting\n            # The initial guess (x0) is the result from the previous, smaller scale\n            result = least_squares(\n                fun=residuals,\n                x0=theta_estimate,\n                bounds=bounds,\n                args=(scaled_u, y_data)\n            )\n            \n            # 4. Update the parameter estimate for the next iteration\n            theta_estimate = result.x\n        \n        # Store the final estimate for s=1.0\n        final_results.append(theta_estimate.tolist())\n\n    # Format the results for final output\n    # Round numbers to three decimal places\n    rounded_results = [[round(val, 3) for val in res] for res in final_results]\n    \n    # Convert to string and remove spaces to match the required format\n    output_str = str(rounded_results).replace(\" \", \"\")\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "2757696"}, {"introduction": "在合成生物学中，我们常常处理的是细胞群体的平均数据，但这掩盖了细胞间固有的异质性。本练习将带你超越简单的曲线拟合，进入分层贝叶斯建模的世界，这是一种能够从群体数据中推断单个细胞参数分布的强大方法。通过构建一个考虑了细胞间变异性的模型并进行最大后验（MAP）估计，你将学会如何严谨地处理和解释源于细胞群体测量的实验数据 [@problem_id:2757765]。", "problem": "一个合成基因回路在一个诱导型启动子下表达一种荧光蛋白。由于外在变异性，单个细胞在蛋白质生产的动力学参数上存在差异。考虑一个分层模型，其中细胞 $i$ 的动力学参数表示为 $k_i$，并假设其独立地从参数为 $\\mu$ 和 $\\sigma$ 的对数正态分布中抽取，即 $\\log k_i$ 服从均值为 $\\mu$、标准差为 $\\sigma$ 的高斯分布。在固定的诱导条件 $c$ 下，每个细胞的预期荧光与 $k_i$ 成正比，比例常数 $s_c$ 是一个已知的、特定于条件的常数，由诱导剂水平和报告基因校准确定。对于每个条件 $c$，实验测量 $n_c$ 个细胞的荧光样本均值，产生一个观测值 $y_c$。此外，报告的样本均值上存在独立的测量噪声，其标准差为 $\\tau_c$。\n\n假设模型包含以下组成部分：\n- 每个细胞的动力学参数 $k_i$ 在细胞间独立，且 $\\log k_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$。\n- 在给定 $k_i$ 和条件 $c$ 的情况下，细胞 $i$ 的预期荧光为 $s_c k_i$。\n- 报告的样本均值 $y_c$ 被建模为从一个高斯分布中的单次抽取，该分布的中心是条件 $c$ 下的真实总体均值，其方差等于两个部分的和：$n_c$ 个细胞的细胞水平荧光平均值的抽样方差，以及一个特定于条件的测量方差 $\\tau_c^2$。\n- 先验：$\\mu \\sim \\mathcal{N}(m_0, s_0^2)$ 且 $\\sigma \\sim \\text{Half-Normal}(b_0)$，对于 $\\sigma>0$，其密度与 $\\exp\\{-\\sigma^2/(2 b_0^2)\\}$ 成正比。\n\n您的任务是执行贝叶斯参数估计，通过对细胞水平的变异性进行积分，并基于群体水平的观测数据计算超参数 $\\mu$ 和 $\\sigma$ 的最大后验 (MAP; maximum a posteriori) 估计。积分应利用对数正态分布的性质精确执行，以获得所有条件下观测样本均值 $y_c$ 的边际似然。然后，将其与给定的先验结合，构建后验密度 $\\pi(\\mu,\\sigma \\mid \\{(y_c,s_c,n_c,\\tau_c)\\}_c)$，并计算 MAP 估计 $(\\mu_{\\text{MAP}},\\sigma_{\\text{MAP}})$。\n\n您可以不经证明直接使用的基本原理：\n- 如果 $X$ 是一个随机变量，$Y = a X + b$，$a$ 和 $b$ 是常数，则 $\\mathbb{E}[Y] = a \\mathbb{E}[X] + b$ 且 $\\operatorname{Var}(Y) = a^2 \\operatorname{Var}(X)$。\n- 中心极限定理表明，独立同分布随机变量的样本均值在真实均值附近近似呈高斯波动，其方差等于总体方差除以样本量。\n- 正态分布和对数正态分布的定义。\n\n构建一个程序，对于下方的每个测试用例，通过在 $\\mu \\in \\mathbb{R}$ 和 $\\sigma > 0$ 上最大化对数后验来计算 MAP 估计 $(\\mu_{\\text{MAP}},\\sigma_{\\text{MAP}})$。您可以使用任何正确且数值稳定的方法。为保证数值稳定性，请强制实施 $\\sigma \\geq 10^{-6}$ 的下界。\n\n对所有测试用例使用相同的先验超参数：$m_0 = 0$，$s_0 = 1$，$b_0 = 1$。\n\n测试套件 (每个测试用例提供观测均值 $y$、比例常数 $s$、样本量 $n$ 和测量噪声标准差 $\\tau$ 的数组)：\n- 测试用例 1:\n  - $y = \\{0.791,\\;1.442,\\;2.934\\}$\n  - $s = \\{0.5,\\;1.0,\\;2.0\\}$\n  - $n = \\{1000,\\;1000,\\;1000\\}$\n  - $\\tau = \\{0.05,\\;0.05,\\;0.05\\}$\n- 测试用例 2:\n  - $y = \\{0.9381,\\;2.7593\\}$\n  - $s = \\{1.0,\\;3.0\\}$\n  - $n = \\{200,\\;200\\}$\n  - $\\tau = \\{0.02,\\;0.02\\}$\n- 测试用例 3:\n  - $y = \\{0.23,\\;1.15,\\;5.2\\}$\n  - $s = \\{0.2,\\;1.2,\\;5.0\\}$\n  - $n = \\{50,\\;50,\\;50\\}$\n  - $\\tau = \\{0.1,\\;0.1,\\;0.1\\}$\n- 测试用例 4:\n  - $y = \\{1.842\\}$\n  - $s = \\{1.7\\}$\n  - $n = \\{1000\\}$\n  - $\\tau = \\{0.02\\}$\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。结果必须是每个测试用例的四舍五入 MAP 估计值的顺序串联：对于测试用例 $j$，追加四舍五入到六位小数的 $\\mu_{\\text{MAP}}$ 和 $\\sigma_{\\text{MAP}}$。例如，对于两个测试用例，一个有效的输出格式为 $[\\mu_{\\text{MAP},1},\\sigma_{\\text{MAP},1},\\mu_{\\text{MAP},2},\\sigma_{\\text{MAP},2}]$，每个值都四舍五入到六位小数。", "solution": "问题陈述经过严格验证。\n\n### 步骤 1：提取已知条件\n\n- **每个细胞动力学参数的模型**：细胞 $i$ 的动力学参数，记为 $k_i$，独立地从对数正态分布中抽取。具体来说，$\\log k_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$。\n- **每个细胞荧光的模型**：在条件 $c$ 下，细胞 $i$ 的预期荧光是 $s_c k_i$，其中 $s_c$ 是一个已知的、特定于条件的比例常数。\n- **观测数据的模型**：$n_c$ 个细胞的报告荧光样本均值 $y_c$ 被建模为从高斯分布中的单次抽取。该分布的中心是条件 $c$ 下细胞水平荧光的真实总体均值。方差是两个分量的总和：$n_c$ 个细胞的细胞水平荧光平均值的抽样方差，以及一个特定于条件的测量方差 $\\tau_c^2$。\n- **先验**：$\\mu$ 的先验是 $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$。$\\sigma$ 的先验是 $\\sigma \\sim \\text{Half-Normal}(b_0)$（对于 $\\sigma > 0$），其密度与 $\\exp\\{-\\sigma^2/(2b_0^2)\\}$ 成正比。\n- **任务**：执行贝叶斯参数估计，以根据群体水平的观测数据 $\\{(y_c, s_c, n_c, \\tau_c)\\}_c$ 计算超参数 $(\\mu_{\\text{MAP}}, \\sigma_{\\text{MAP}})$ 的最大后验 (MAP) 估计。\n- **数值约束**：为保证数值稳定性，强制实施下界 $\\sigma \\geq 10^{-6}$。\n- **先验超参数**：对于所有测试用例，$m_0 = 0$，$s_0 = 1$，$b_0 = 1$。\n- **测试数据**：提供了四个测试用例，每个用例都包含 $\\{y_c\\}$、$\\{s_c\\}$、$\\{n_c\\}$ 和 $\\{\\tau_c\\}$ 的数组。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n根据指定标准对问题进行评估。\n\n- **科学性**：该问题描述了一个用于基因表达的分层贝叶斯模型，这是定量生物学和合成生物学中一种标准且广为接受的方法。对动力学参数使用对数正态分布反映了生物学量通常呈右偏分布的普遍观察。观测数据的模型同时包含了生物学（抽样）和技术（测量）噪声，是一种统计上合理的构造。该问题牢固地建立在生物学统计建模的既定原则之上。\n- **适定性**：任务是找到后验概率分布的最大值。$\\mu$ 的先验（正态）和 $\\sigma$ 的先验（半正态）都是正常先验 (proper priors)，似然函数源自高斯假设。这种结构产生了一个定义明确的优化目标函数。问题的结构旨在产生唯一的 MAP 估计。\n- **客观性**：问题使用精确、无歧义的数学语言表述。所有术语都有定义，数据是定量的。没有主观或基于意见的陈述。\n- **完整性与一致性**：问题是自洽的。它提供了所有必要组成部分：数据生成模型、先验分布、所有必需的参数和超参数，以及每个测试用例的具体数据。没有明显的矛盾。\n- **现实性**：该模型是一个真实生物实验的标准（尽管简化了）表示。所提供的数据值对于荧光测量是合理的。\n- **结构**：该问题要求先进行后验的多步推导，然后进行数值优化。这是一项实质性的、不简单的任务，能够正确评估对分层背景下贝叶斯推断的理解。\n\n### 步骤 3：结论与行动\n\n问题被判定为**有效**。它具有科学性、适定性和完整性。将构建一个解决方案。\n\n### 解法推导\n\n目标是找到参数 $\\mu$ 和 $\\sigma$ 的 MAP 估计，即最大化后验概率密度 $\\pi(\\mu, \\sigma \\mid \\text{data})$ 的值。根据贝叶斯定理，后验与似然和先验的乘积成正比：\n$$\n\\pi(\\mu, \\sigma \\mid \\{y_c\\}_c) \\propto p(\\{y_c\\}_c \\mid \\mu, \\sigma) \\, p(\\mu) \\, p(\\sigma)\n$$\n最大化后验等价于最大化其对数，即对数后验。\n\n**1. 边际似然的推导**\n\n首先，我们通过对未观测到的细胞水平参数 $k_i$ 进行边缘化，来推导观测数据 $\\{y_c\\}_c$ 的似然。\n\n每个细胞的动力学参数 $k_i$ 从对数正态分布中抽取，$k_i \\sim \\text{LogNormal}(\\mu, \\sigma^2)$，即 $\\log k_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$。$k_i$ 的均值和方差为：\n$$\n\\mathbb{E}[k_i] = \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(k_i) = (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)\n$$\n在条件 $c$ 下，单个细胞 $i$ 的预期荧光为 $F_{i,c} = s_c k_i$。其均值和方差为：\n$$\n\\mathbb{E}[F_{i,c}] = s_c \\mathbb{E}[k_i] = s_c \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(F_{i,c}) = s_c^2 \\operatorname{Var}(k_i) = s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)\n$$\n问题陈述指出，观测到的样本均值 $y_c$ 是从一个高斯分布中抽取的。此高斯分布的均值是荧光的真实总体均值，$\\mu_c(\\mu, \\sigma) = \\mathbb{E}[F_{i,c}]$。\n$$\n\\mu_c(\\mu, \\sigma) = s_c \\exp(\\mu + \\sigma^2/2)\n$$\n此高斯分布的方差 $\\sigma_c^2(\\mu, \\sigma)$ 是 $n_c$ 个细胞均值的抽样方差与测量方差 $\\tau_c^2$ 之和。\n$$\n\\sigma_c^2(\\mu, \\sigma) = \\frac{\\operatorname{Var}(F_{i,c})}{n_c} + \\tau_c^2 = \\frac{s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)}{n_c} + \\tau_c^2\n$$\n因此，单个观测值 $y_c$ 的模型是 $y_c \\sim \\mathcal{N}(\\mu_c(\\mu, \\sigma), \\sigma_c^2(\\mu, \\sigma))$。观测到 $y_c$ 的似然由高斯概率密度函数给出。假设不同条件 $c$ 下的观测是独立的，总对数似然为：\n$$\n\\log L(\\mu, \\sigma) = \\log p(\\{y_c\\}_c \\mid \\mu, \\sigma) = \\sum_c \\log p(y_c \\mid \\mu, \\sigma)\n$$\n$$\n\\log L(\\mu, \\sigma) = -\\frac{1}{2} \\sum_c \\left[ \\log(2\\pi \\sigma_c^2(\\mu, \\sigma)) + \\frac{(y_c - \\mu_c(\\mu, \\sigma))^2}{\\sigma_c^2(\\mu, \\sigma)} \\right]\n$$\n\n**2. 组合对数后验**\n\n$\\mu$ 和 $\\sigma$ 的对数先验（忽略归一化常数）为：\n$$\n\\log p(\\mu) = -\\frac{(\\mu - m_0)^2}{2s_0^2} + \\text{const}\n$$\n$$\n\\log p(\\sigma) = -\\frac{\\sigma^2}{2b_0^2} + \\text{const}, \\quad \\text{for } \\sigma > 0\n$$\n需要最大化的完整对数后验是 $\\log \\pi(\\mu, \\sigma) = \\log L(\\mu, \\sigma) + \\log p(\\mu) + \\log p(\\sigma)$。\n\n**3. 优化策略**\n\n我们寻求 $(\\mu_{\\text{MAP}}, \\sigma_{\\text{MAP}}) = \\arg\\max_{\\mu, \\sigma>0} \\log \\pi(\\mu, \\sigma)$。这等价于最小化负对数后验。为了提高数值稳定性，我们对模型进行重新参数化。令 $\\phi = \\mu + \\sigma^2/2$。则 $\\mu = \\phi - \\sigma^2/2$。$y_c$ 的均值和方差表达式变得更加稳定：\n$$\n\\mu_c(\\phi) = s_c \\exp(\\phi)\n$$\n$$\n\\exp(2\\mu + \\sigma^2) = \\exp(2(\\phi - \\sigma^2/2) + \\sigma^2) = \\exp(2\\phi)\n$$\n$$\n\\sigma_c^2(\\phi, \\sigma) = \\frac{s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\phi)}{n_c} + \\tau_c^2 = \\frac{(\\exp(\\sigma^2) - 1) \\mu_c(\\phi)^2}{n_c} + \\tau_c^2\n$$\n需要最小化的目标函数是以 $(\\phi, \\sigma)$ 表示的负对数后验，忽略常数：\n$$\nJ(\\phi, \\sigma) = \\frac{1}{2} \\sum_c \\left[ \\log(\\sigma_c^2(\\phi, \\sigma)) + \\frac{(y_c - \\mu_c(\\phi))^2}{\\sigma_c^2(\\phi, \\sigma)} \\right] + \\frac{(\\phi - \\sigma^2/2 - m_0)^2}{2s_0^2} + \\frac{\\sigma^2}{2b_0^2}\n$$\n该函数相对于 $\\phi \\in \\mathbb{R}$ 和 $\\sigma > 0$ 进行最小化。将施加约束 $\\sigma \\geq 10^{-6}$。优化使用 `scipy.optimize.minimize` 进行数值计算。一旦找到最优的 $(\\phi_{\\text{MAP}}, \\sigma_{\\text{MAP}})$，我们使用变换恢复 $\\mu_{\\text{MAP}}$：\n$$\n\\mu_{\\text{MAP}} = \\phi_{\\text{MAP}} - \\frac{\\sigma_{\\text{MAP}}^2}{2}\n$$\n以下程序为每个测试用例实现了此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the MAP estimation problem for all test cases.\n    \"\"\"\n    # Prior hyperparameters (fixed for all test cases)\n    m0 = 0.0\n    s0 = 1.0\n    b0 = 1.0\n    priors = (m0, s0, b0)\n\n    # Test suite as defined in the problem statement\n    test_cases = [\n        {\n            \"y\": np.array([0.791, 1.442, 2.934]),\n            \"s\": np.array([0.5, 1.0, 2.0]),\n            \"n\": np.array([1000, 1000, 1000]),\n            \"tau\": np.array([0.05, 0.05, 0.05]),\n        },\n        {\n            \"y\": np.array([0.9381, 2.7593]),\n            \"s\": np.array([1.0, 3.0]),\n            \"n\": np.array([200, 200]),\n            \"tau\": np.array([0.02, 0.02]),\n        },\n        {\n            \"y\": np.array([0.23, 1.15, 5.2]),\n            \"s\": np.array([0.2, 1.2, 5.0]),\n            \"n\": np.array([50, 50, 50]),\n            \"tau\": np.array([0.1, 0.1, 0.1]),\n        },\n        {\n            \"y\": np.array([1.842]),\n            \"s\": np.array([1.7]),\n            \"n\": np.array([1000]),\n            \"tau\": np.array([0.02]),\n        },\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Initial guess for optimization parameters [phi, sigma]\n        # A reasonable guess for phi = mu + sigma^2/2 is the log of the scaled observation\n        with np.errstate(divide='ignore'):\n            phi_guess = np.mean(np.log(case_data['y'] / case_data['s']))\n            if not np.isfinite(phi_guess):\n                phi_guess = 0.0\n        sigma_guess = 0.5\n        x0 = [phi_guess, sigma_guess]\n\n        # Define bounds for the optimization: phi is unbounded, sigma >= 1e-6\n        bounds = [(None, None), (1e-6, None)]\n\n        # Run the optimization to minimize the negative log-posterior\n        result = minimize(\n            negative_log_posterior,\n            x0,\n            args=(case_data, priors),\n            bounds=bounds,\n            method='L-BFGS-B'\n        )\n\n        phi_map, sigma_map = result.x\n        \n        # Transform back from phi to mu\n        mu_map = phi_map - 0.5 * sigma_map**2\n\n        results.extend([round(mu_map, 6), round(sigma_map, 6)])\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef negative_log_posterior(params, data, priors):\n    \"\"\"\n    Calculates the negative of the log-posterior probability density.\n    This is the objective function to be minimized.\n    Uses the numerically stable reparameterization (phi, sigma).\n    \n    Args:\n        params (list-like): A list or array [phi, sigma].\n        data (dict): A dictionary containing numpy arrays for y, s, n, tau.\n        priors (tuple): A tuple of prior hyperparameters (m0, s0, b0).\n        \n    Returns:\n        float: The value of the negative log-posterior.\n    \"\"\"\n    phi, sigma = params\n    y, s, n, tau = data[\"y\"], data[\"s\"], data[\"n\"], data[\"tau\"]\n    m0, s0, b0 = priors\n\n    # --- Log-prior term ---\n    # The original parameter mu is expressed in terms of phi and sigma\n    mu = phi - 0.5 * sigma**2\n    \n    # Calculate log-prior contribution (proportional to negative log-prior)\n    log_prior_cost = ( (mu - m0)**2 / (2 * s0**2) + \n                       sigma**2 / (2 * b0**2) )\n\n    # --- Log-likelihood term ---\n    try:\n        with np.errstate(over='raise'):\n            # Calculate mu_c = s_c * exp(phi)\n            exp_phi = np.exp(phi)\n            mu_c = s * exp_phi\n\n            # Use np.expm1 for precision: exp(x) - 1\n            exp_sigma2_minus_1 = np.expm1(sigma**2)\n\n            # Calculate sigma_c^2 using the reparameterized formula\n            sampling_variance = (exp_sigma2_minus_1 * mu_c**2) / n\n            sigma_c_sq = sampling_variance + tau**2\n            \n            # Guard against non-positive variance which can occur during optimization search\n            if np.any(sigma_c_sq = 0):\n                return np.inf\n\n            # Calculate log-likelihood contribution (proportional to negative log-likelihood)\n            log_likelihood_cost = 0.5 * np.sum(np.log(sigma_c_sq) + (y - mu_c)**2 / sigma_c_sq)\n\n    except (FloatingPointError, RuntimeWarning):\n        # Return a large value if numerical issues arise (e.g., overflow in exp)\n        return np.inf\n\n    # Total cost is the sum of prior and likelihood costs\n    total_cost = log_prior_cost + log_likelihood_cost\n    \n    return total_cost\n\n# Execute the main function\nsolve()\n\n```", "id": "2757765"}, {"introduction": "当我们的模型从稳态描述转向捕捉系统动态变化的常微分方程（ODEs）时，参数估计的计算复杂度会显著增加。基于梯度的优化方法是拟合动态模型的关键，而计算这些梯度本身就是一个挑战。本练习将向你介绍“伴随方法”（Adjoint Method），这是一种用于高效计算动态模型梯度的先进技术。通过为一个基因表达的动力学模型实现伴随方法，你将掌握一个在处理高维参数空间时能大幅提升计算效率的核心工具 [@problem_id:2757781]。", "problem": "考虑一个标准的二级基因表达模型，该模型捕获在诱导剂作用下的转录和翻译过程，这是在合成生物学中从时间序列数据估计参数时常遇到的场景。设信使RNA (mRNA) 浓度为 $m(t)$，蛋白质浓度为 $p(t)$。诱导剂浓度 $I$ 是恒定且已知的。其动力学由以下常微分方程组给出\n$$\n\\frac{d}{dt}\\begin{bmatrix} m \\\\ p \\end{bmatrix}\n=\n\\begin{bmatrix}\nk_{\\mathrm{tx}}\\,H(I;K,n) - \\delta_m\\, m \\\\\nk_{\\mathrm{tl}}\\, m - \\delta_p\\, p\n\\end{bmatrix},\n\\quad\nH(I;K,n) \\equiv \\frac{1}{1 + \\left(\\frac{I}{K}\\right)^n},\n$$\n其中未知参数向量为\n$$\n\\theta \\equiv \\begin{bmatrix} k_{\\mathrm{tx}}  K  n  \\delta_m  k_{\\mathrm{tl}}  \\delta_p \\end{bmatrix}^{\\top}.\n$$\n假设初始条件为 $x(0)=\\begin{bmatrix} m(0)  p(0) \\end{bmatrix}^{\\top} = \\begin{bmatrix} 0  0 \\end{bmatrix}^{\\top}$。仅在离散时间点 $0  t_1  \\cdots  t_M$ 对蛋白质 $p(t)$ 进行测量，产生数据 $y_i$，并带有已知的正权重 $w_i$。定义最小二乘目标函数\n$$\nJ(\\theta) \\equiv \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left(p(t_i;\\theta) - y_i\\right)^2.\n$$\n所有变量和参数均为无量纲。\n\n任务。实现一个程序，该程序使用连续时间伴随方法，针对在离散时间点进行测量的目标函数，计算梯度 $\\nabla_{\\theta}J$。从拉格朗日构造推导出的伴随方程开始。在两次测量时间之间，伴随变量 $\\lambda(t)\\in\\mathbb{R}^2$ 满足由正向系统所蕴含的齐次线性伴随动力学，其终端条件为 $\\lambda(t_M^+)=\\mathbf{0}$。在每个测量时间 $t_i$ 应用由离散损失项引起的跳跃条件。使用得到的伴随轨迹，将梯度计算为 $\\left(\\partial f/\\partial \\theta\\right)^{\\top}\\lambda$ 的时间积分。\n\n您的实现必须：\n- 计算正向状态轨迹，并提供密集输出，以便在任意时间 $t$ 都能获得 $x(t)$。\n- 从 $t_M$ 到 $0$ 进行伴随变量的反向时间积分，并在每个 $t_i$ 应用跳跃：\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\frac{\\partial}{\\partial x}\\left(\\frac{1}{2}w_i\\left(p(t_i;\\theta)-y_i\\right)^2\\right),\n$$\n其中关于 $x$ 的梯度是在将 $p$ 视为 $x$ 的第二个分量的情况下计算的。\n- 使用以下公式累积梯度：\n$$\n\\nabla_{\\theta}J(\\theta) = \\int_{0}^{t_M} \\left(\\frac{\\partial f}{\\partial \\theta}(t, x(t), \\theta)\\right)^{\\top}\\lambda(t)\\, dt,\n$$\n注意在此设置中，离散测量项不显式依赖于 $\\theta$。\n\n验证要求。对于下方的每个测试用例，还需计算 $\\nabla_{\\theta}J$ 的中心有限差分近似，其每个参数的步长为 $h_j = \\max\\{10^{-6}\\cdot|\\theta_j|,\\,10^{-8}\\}$，并报告各分量上的最大相对差异：\n$$\n\\varepsilon \\equiv \\max_{j\\in\\{1,\\dots,6\\}} \\frac{\\left|\\, \\left[\\nabla_{\\theta}J(\\theta)\\right]_j - \\left[\\nabla_{\\theta}J(\\theta)\\right]_j^{\\mathrm{FD}} \\,\\right|}{\\max\\left\\{1,\\left| \\left[\\nabla_{\\theta}J(\\theta)\\right]_j^{\\mathrm{FD}} \\right|\\right\\}}.\n$$\n\n测试套件。使用以下三个独立的测试用例。在每个用例中，通过在指定的真实参数 $\\theta^{\\mathrm{true}}$ 下模拟正向模型来生成合成数据，然后在测量时间的蛋白质值上添加指定标准差的独立高斯噪声。使用给定的随机种子初始化伪随机数生成器，以确保结果是确定性的。然后，在指定的 $\\theta^{\\mathrm{eval}}$ 处评估梯度，并计算该用例的 $\\varepsilon$。\n\n- 用例 A（顺利情况）：\n  - 诱导剂：$I = 100$。\n  - 测量时间：$\\{0.25,\\,0.5,\\,1.0,\\,2.0,\\,3.0,\\,4.0\\}$。\n  - 权重：全部等于 $1$。\n  - 初始条件：$m(0)=0$，$p(0)=0$。\n  - 真实参数：$\\theta^{\\mathrm{true}} = [15.0,\\,50.0,\\,2.0,\\,0.3,\\,5.0,\\,0.2]^{\\top}$。\n  - 评估参数：$\\theta^{\\mathrm{eval}} = [12.0,\\,60.0,\\,2.5,\\,0.25,\\,4.5,\\,0.25]^{\\top}$。\n  - 噪声标准差：$0.05$。\n  - 随机种子：$0$。\n\n- 用例 B（单个终端测量，权重较大）：\n  - 诱导剂：$I = 80$。\n  - 测量时间：$\\{6.0\\}$。\n  - 权重：$\\{10.0\\}$。\n  - 初始条件：$m(0)=0$，$p(0)=0$。\n  - 真实参数：$\\theta^{\\mathrm{true}} = [10.0,\\,40.0,\\,1.5,\\,0.4,\\,3.5,\\,0.1]^{\\top}$。\n  - 评估参数：$\\theta^{\\mathrm{eval}} = [9.0,\\,50.0,\\,1.2,\\,0.35,\\,3.2,\\,0.12]^{\\top}$。\n  - 噪声标准差：$0.0$。\n  - 随机种子：$1$。\n\n- 用例 C（慢动力学，长时程）：\n  - 诱导剂：$I = 120$。\n  - 测量时间：$\\{0.5,\\,1.0,\\,2.0,\\,4.0,\\,8.0,\\,12.0\\}$。\n  - 权重：全部等于 $1$。\n  - 初始条件：$m(0)=0$，$p(0)=0$。\n  - 真实参数：$\\theta^{\\mathrm{true}} = [8.0,\\,30.0,\\,3.0,\\,0.1,\\,2.0,\\,0.05]^{\\top}$。\n  - 评估参数：$\\theta^{\\mathrm{eval}} = [7.5,\\,35.0,\\,2.8,\\,0.08,\\,1.8,\\,0.06]^{\\top}$。\n  - 噪声标准差：$0.02$。\n  - 随机种子：$2$。\n\n数值指导：\n- 以绝对容差 $10^{-12}$ 和相对容差 $10^{-9}$ 对正向和伴随系统进行积分。\n- 对正向积分使用密集输出，以便在反向伴随积分过程中评估 $x(t)$。\n- 所有变量和参数均为无量纲。\n\n最终输出。您的程序应生成一行输出，其中包含用例 A、B 和 C 的三个 $\\varepsilon$ 值，形式为用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3]$）。每个 $r_i$ 必须是一个浮点数。", "solution": "所提出的问题是计算系统生物学领域中一个定义明确且具有科学依据的任务，具体涉及一种常见基因表达模型的参数敏感性分析。这是常微分方程（ODE）与离散时间目标函数的连续时间伴随方法的典型应用。该问题是完整的、明确的且在计算上是可行的。因此，我将提供一个完整的解决方案。\n\n问题的核心是计算最小二乘目标函数 $J(\\theta)$ 关于参数向量 $\\theta$ 的梯度 $\\nabla_{\\theta}J$。该模型描述了信使RNA（mRNA）浓度 $m(t)$ 和蛋白质浓度 $p(t)$ 的动力学。\n\n**1. 正向模型与目标函数**\n\n系统状态由向量 $x(t) = [m(t), p(t)]^{\\top}$ 给出。动力学由以下常微分方程组控制：\n$$\n\\frac{d x}{dt} = f(x, \\theta) =\n\\begin{bmatrix}\nk_{\\mathrm{tx}}\\,H(I;K,n) - \\delta_m\\, m(t) \\\\\nk_{\\mathrm{tl}}\\, m(t) - \\delta_p\\, p(t)\n\\end{bmatrix}\n$$\n初始条件为 $x(0) = [0, 0]^{\\top}$。调控项是一个抑制性希尔函数 $H(I;K,n) = \\frac{1}{1 + (I/K)^n}$，其中 $I$ 是恒定的诱导剂浓度。参数向量为 $\\theta = [k_{\\mathrm{tx}}, K, n, \\delta_m, k_{\\mathrm{tl}}, \\delta_p]^{\\top}$。\n\n目标函数衡量了在离散测量时间点 $0  t_1  \\dots  t_M$，模型的预测蛋白质浓度 $p(t_i; \\theta)$ 与实验数据 $y_i$ 之间的平方差：\n$$\nJ(\\theta) \\equiv \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left(p(t_i;\\theta) - y_i\\right)^2\n$$\n其中 $w_i$ 是正权重。\n\n**2. 连续时间伴随方法**\n\n梯度 $\\nabla_{\\theta}J$ 通过伴随方法求得，该方法避免了为每个参数求解敏感性方程这种计算成本高昂的途径。此方法涉及状态方程的正向积分和伴随方程的反向积分。\n\n梯度表示为在时间区间 $[0, t_M]$ 上的一个积分：\n$$\n\\nabla_{\\theta}J(\\theta) = \\int_{0}^{t_M} \\left(\\frac{\\partial f}{\\partial \\theta}(t, x(t), \\theta)\\right)^{\\top}\\lambda(t)\\, dt\n$$\n这里，$\\lambda(t) \\in \\mathbb{R}^2$ 是伴随状态向量，它是通过反向时间积分求解一个线性ODE系统的解。\n\n伴随状态 $\\lambda(t) = [\\lambda_m(t), \\lambda_p(t)]^\\top$ 的动力学由下式给出：\n$$\n\\frac{d\\lambda}{dt} = -A(t)^{\\top} \\lambda(t),\n$$\n其中 $A(t) = \\frac{\\partial f}{\\partial x}$ 是沿着正向轨迹 $x(t)$ 计算的正向系统的雅可比矩阵。对于此问题，雅可比矩阵是常数：\n$$\nA = \\frac{\\partial f}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial f_m}{\\partial m}  \\frac{\\partial f_m}{\\partial p} \\\\\n\\frac{\\partial f_p}{\\partial m}  \\frac{\\partial f_p}{\\partial p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-\\delta_m  0 \\\\\nk_{\\mathrm{tl}}  -\\delta_p\n\\end{bmatrix}\n$$\n因此，伴随系统为：\n$$\n\\frac{d\\lambda}{dt} = -A^{\\top} \\lambda = -\\begin{bmatrix} -\\delta_m  k_{\\mathrm{tl}} \\\\ 0  -\\delta_p \\end{bmatrix} \\lambda = \\begin{bmatrix} \\delta_m  -k_{\\mathrm{tl}} \\\\ 0  \\delta_p \\end{bmatrix} \\lambda\n$$\n\n这个反向时间问题的边界条件由一个在 $t_M$ 的终端条件和在每个测量时间 $t_i$ 的离散`跳跃`组成。终端条件是 $\\lambda(t_M^+) = \\mathbf{0}$。在每个 $t_i$，伴随状态经历一次不连续的跳跃：\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\nabla_x \\left(\\frac{1}{2}w_i\\left(p(t_i) - y_i\\right)^2\\right)\n$$\n其中 $\\lambda(t_i^-)$ 和 $\\lambda(t_i^+)$ 分别是跳跃时间 $t_i$ 前后的状态。代价项关于状态 $x=[m, p]^\\top$ 的梯度是：\n$$\n\\nabla_x \\left(\\frac{1}{2}w_i(p(t_i) - y_i)^2\\right) =\n\\begin{bmatrix} 0 \\\\ w_i (p(t_i) - y_i) \\end{bmatrix}\n$$\n\n**3. 用于梯度计算的增广系统**\n\n为了在对 $\\lambda(t)$ 进行反向积分的同时计算梯度积分，我们定义一个增广状态向量 $Z(t) = [\\lambda(t)^\\top, G(t)^\\top]^\\top$，其中 $G(t) = \\int_{t}^{t_M} (\\frac{\\partial f}{\\partial \\theta})^\\top\\lambda(\\tau)d\\tau$。我们所求的梯度即为 $G(0)$。$G(t)$ 的微分方程为 $\\frac{dG}{dt} = -(\\frac{\\partial f}{\\partial \\theta})^\\top\\lambda(t)$，终端条件为 $G(t_M)=\\mathbf{0}$。\n\n需要计算 $f$ 关于参数的偏导数：\n令 $H = \\frac{1}{1+(I/K)^n}$。\n$$\n\\frac{\\partial f}{\\partial k_{\\mathrm{tx}}} = \\begin{bmatrix} H \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial K} = \\begin{bmatrix} k_{\\mathrm{tx}} \\frac{n}{K}H(1-H) \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial n} = \\begin{bmatrix} -k_{\\mathrm{tx}} \\ln(I/K) H(1-H) \\\\ 0 \\end{bmatrix}\n$$\n$$\n\\frac{\\partial f}{\\partial \\delta_m} = \\begin{bmatrix} -m \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial k_{\\mathrm{tl}}} = \\begin{bmatrix} 0 \\\\ m \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial \\delta_p} = \\begin{bmatrix} 0 \\\\ -p \\end{bmatrix}\n$$\n用于反向积分的完整增广ODE系统为：\n$$\n\\frac{dZ}{dt} =\n\\begin{bmatrix}\n\\dot{\\lambda} \\\\\n\\dot{G}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\begin{bmatrix} \\delta_m  -k_{\\mathrm{tl}} \\\\ 0  \\delta_p \\end{bmatrix} \\lambda \\\\\n- \\left(\\frac{\\partial f(t, x(t), \\theta)}{\\partial \\theta}\\right)^{\\top} \\lambda\n\\end{bmatrix}\n$$\n\n**4. 数值实现策略**\n\n整体算法流程如下：\n1.  **生成合成数据**：对于每个测试用例，使用真实参数 $\\theta^{\\mathrm{true}}$ 模拟正向模型以获得 $p(t_i;\\theta^{\\mathrm{true}})$。按规定添加高斯噪声以生成数据点 $y_i$。\n2.  **正向传递**：使用评估参数 $\\theta^{\\mathrm{eval}}$ 从 $t=0$ 到 $t_M$ 求解正向ODE系统 $\\dot{x} = f(x, \\theta^{\\mathrm{eval}})$。这必须使用密集输出选项来提供一个插值函数，以获得在任何时间 $t \\in [0, t_M]$ 的 $x(t)$。\n3.  **反向传递**：\n    a. 初始化增广状态 $Z(t_M) = \\mathbf{0} \\in \\mathbb{R}^8$。\n    b. 按时间倒序处理测量时间，从 $t_M$ 到 $t_1$。\n    c. 对于每个测量时间 $t_i$（$i=M, \\dots, 1$）：\n        i. 对 $Z$ 的伴随分量应用跳跃：$\\lambda_p(t_i^-) = \\lambda_p(t_i^+) + w_i(p(t_i;\\theta^{\\mathrm{eval}}) - y_i)$。\n        ii. 对增广系统 $\\dot{Z}$ 从 $t_i$ 到 $t_{i-1}$（使用 $t_0=0$）进行反向时间积分。\n    d. 在 $t=0$ 时梯度分量 $G$ 的最终状态就是所求的梯度 $\\nabla_{\\theta}J$。\n4.  **有限差分验证**：\n    a. 对于每个参数 $\\theta_j$，通过将 $\\theta_j$ 扰动 $\\pm h_j$ 并重新评估目标函数 $J$，来计算梯度分量 $(\\nabla_{\\theta}J)_j^{\\mathrm{FD}}$ 的中心有限差分近似。这需要对6个参数中的每一个进行两次正向ODE求解。\n    b. 根据问题陈述中的规定，计算伴随梯度和有限差分近似之间的最大相对差异 $\\varepsilon$。\n5.  **报告结果**：收集每个测试用例的差异 $\\varepsilon$。\n\n这个过程虽然复杂，但它是伴随敏感性分析理论的直接实现，为动态模型中的梯度计算提供了一种强大而高效的方法。使用高精度数值积分器对于确保准确性至关重要，尤其是在验证步骤中。", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A\n        {\n            \"I\": 100.0,\n            \"times\": np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0]),\n            \"weights\": np.ones(6),\n            \"theta_true\": np.array([15.0, 50.0, 2.0, 0.3, 5.0, 0.2]),\n            \"theta_eval\": np.array([12.0, 60.0, 2.5, 0.25, 4.5, 0.25]),\n            \"noise_std\": 0.05,\n            \"seed\": 0,\n        },\n        # Case B\n        {\n            \"I\": 80.0,\n            \"times\": np.array([6.0]),\n            \"weights\": np.array([10.0]),\n            \"theta_true\": np.array([10.0, 40.0, 1.5, 0.4, 3.5, 0.1]),\n            \"theta_eval\": np.array([9.0, 50.0, 1.2, 0.35, 3.2, 0.12]),\n            \"noise_std\": 0.0,\n            \"seed\": 1,\n        },\n        # Case C\n        {\n            \"I\": 120.0,\n            \"times\": np.array([0.5, 1.0, 2.0, 4.0, 8.0, 12.0]),\n            \"weights\": np.ones(6),\n            \"theta_true\": np.array([8.0, 30.0, 3.0, 0.1, 2.0, 0.05]),\n            \"theta_eval\": np.array([7.5, 35.0, 2.8, 0.08, 1.8, 0.06]),\n            \"noise_std\": 0.02,\n            \"seed\": 2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        discrepancy = process_case(case)\n        results.append(discrepancy)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case: generates data, computes adjoint and finite\n    difference gradients, and returns the discrepancy.\n    \"\"\"\n    # Unpack case parameters\n    I = case_params[\"I\"]\n    t_meas = case_params[\"times\"]\n    weights = case_params[\"weights\"]\n    theta_true = case_params[\"theta_true\"]\n    theta_eval = case_params[\"theta_eval\"]\n    noise_std = case_params[\"noise_std\"]\n    seed = case_params[\"seed\"]\n\n    # Numerical settings\n    atol, rtol = 1e-12, 1e-9\n    x0 = np.array([0.0, 0.0])\n    t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n\n    # 1. Generate synthetic data\n    rng = np.random.default_rng(seed)\n\n    def forward_rhs(t, x, theta, I_val):\n        k_tx, K, n, delta_m, k_tl, delta_p = theta\n        m, p = x\n        hill = 1.0 / (1.0 + (I_val / K)**n) if K > 0 else 0.0\n        dm_dt = k_tx * hill - delta_m * m\n        dp_dt = k_tl * m - delta_p * p\n        return np.array([dm_dt, dp_dt])\n\n    sol_true = solve_ivp(\n        lambda t, x: forward_rhs(t, x, theta_true, I),\n        (0, t_max), x0, atol=atol, rtol=rtol, dense_output=True\n    )\n    p_true_at_t = sol_true.sol(t_meas)[1]\n    noise = rng.normal(0, noise_std, size=len(t_meas))\n    y_data = p_true_at_t + noise\n    \n    data_map = dict(zip(t_meas, y_data))\n    weights_map = dict(zip(t_meas, weights))\n\n    # 2. Compute gradient with adjoint method\n    adjoint_grad = compute_adjoint_gradient(theta_eval, I, t_meas, data_map, weights_map, atol, rtol)\n\n    # 3. Compute gradient with finite differences\n    fd_grad = compute_fd_gradient(theta_eval, I, t_meas, y_data, weights, atol, rtol)\n\n    # 4. Compare gradients\n    numerator = np.abs(adjoint_grad - fd_grad)\n    denominator = np.maximum(1.0, np.abs(fd_grad))\n    relative_discrepancy = numerator / denominator\n    \n    return np.max(relative_discrepancy)\n\ndef compute_adjoint_gradient(theta, I, t_meas, data_map, weights_map, atol, rtol):\n    \"\"\"Computes the gradient using the continuous-time adjoint method.\"\"\"\n    x0 = np.array([0.0, 0.0])\n    t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n    \n    def forward_rhs(t, x, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        m, p = x\n        hill = 1.0 / (1.0 + (I_loc / K_loc)**n_loc) if K_loc > 0 else 0.0\n        dm_dt = k_tx_loc * hill - delta_m_loc * m\n        dp_dt = k_tl_loc * m - delta_p_loc * p\n        return np.array([dm_dt, dp_dt])\n\n    # Forward pass\n    sol_fwd = solve_ivp(\n        lambda t, x: forward_rhs(t, x, theta, I), \n        (0, t_max), x0, atol=atol, rtol=rtol, dense_output=True\n    )\n\n    memoized_sol_fwd = sol_fwd.sol\n\n    def backward_rhs(t, z, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        lambda_m, lambda_p = z[0], z[1]\n        \n        m_t, p_t = memoized_sol_fwd(t)\n\n        # Adjoint dynamics (d(lambda)/dt = -A^T * lambda)\n        dlambda_m_dt = delta_m_loc * lambda_m - k_tl_loc * lambda_p\n        dlambda_p_dt = delta_p_loc * lambda_p\n        \n        # Gradient dynamics (dG/dt = - (df/dtheta)^T * lambda)\n        s = I_loc / K_loc if K_loc > 0 else np.inf\n        s_n = s**n_loc if s > 0 else (0.0 if n_loc > 0 else np.inf)\n        h = 1.0 / (1.0 + s_n)\n        \n        dG_ktx_dt = -h * lambda_m\n        dG_K_dt = -k_tx_loc * (n_loc / K_loc) * h * (1 - h) * lambda_m if K_loc > 0 else 0.0\n        dG_n_dt = k_tx_loc * np.log(s) * h * (1-h) * lambda_m if I_loc > 0 and K_loc > 0 else 0.0\n        dG_dm_dt = m_t * lambda_m\n        dG_ktl_dt = -m_t * lambda_p\n        dG_dp_dt = p_t * lambda_p\n\n        # Note: the ODE must return negative of this for backward integration\n        return -np.array([\n            dlambda_m_dt, dlambda_p_dt,\n            dG_ktx_dt, dG_K_dt, dG_n_dt, dG_dm_dt, dG_ktl_dt, dG_dp_dt\n        ])\n\n    # Backward pass\n    z_aug = np.zeros(8)\n    integration_points = sorted(list(set([0.0] + list(t_meas))), reverse=False)\n\n    for i in reversed(range(len(integration_points) -1)):\n        t_start = integration_points[i+1]\n        t_end = integration_points[i]\n\n        # Apply jump if t_start is a measurement time\n        if t_start in data_map:\n            p_at_t_start = memoized_sol_fwd(t_start)[1]\n            residual = p_at_t_start - data_map[t_start]\n            weight = weights_map[t_start]\n            z_aug[1] += weight * residual\n        \n        # Integrate backward\n        if t_start > t_end:\n            sol_bwd = solve_ivp(\n                lambda t, z: backward_rhs(t, z, theta, I),\n                (t_start, t_end), z_aug, atol=atol, rtol=rtol, method='LSODA'\n            )\n            z_aug = sol_bwd.y[:, -1]\n    \n    return z_aug[2:]\n\ndef compute_fd_gradient(theta, I, t_meas, y_data, weights, atol, rtol):\n    \"\"\"Computes the gradient using central finite differences.\"\"\"\n    grad_fd = np.zeros_like(theta)\n\n    def forward_rhs(t, x, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        m, p = x\n        hill = 1.0 / (1.0 + (I_loc / K_loc)**n_loc) if K_loc > 0 else 0.0\n        dm_dt = k_tx_loc * hill - delta_m_loc * m\n        dp_dt = k_tl_loc * m - delta_p_loc * p\n        return np.array([dm_dt, dp_dt])\n\n    def objective_J(theta_loc):\n        x0 = np.array([0.0, 0.0])\n        t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n        if t_max == 0:\n            return 0.0\n        \n        sol = solve_ivp(\n            lambda t, x: forward_rhs(t, x, theta_loc, I),\n            (0, t_max), x0, atol=atol, rtol=rtol, t_eval=t_meas, dense_output=True\n        )\n        p_model = sol.sol(t_meas)[1]\n\n        return 0.5 * np.sum(weights * (p_model - y_data)**2)\n\n    for j in range(len(theta)):\n        h_j = max(1e-6 * abs(theta[j]), 1e-8)\n        \n        theta_plus = np.copy(theta)\n        theta_plus[j] += h_j\n        \n        theta_minus = np.copy(theta)\n        theta_minus[j] -= h_j\n        \n        J_plus = objective_J(theta_plus)\n        J_minus = objective_J(theta_minus)\n        \n        grad_fd[j] = (J_plus - J_minus) / (2 * h_j)\n        \n    return grad_fd\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2757781"}]}