## 应用与跨学科连接

在我们之前的讨论中，我们已经领略了神经[微分方程](@article_id:327891)（Neural ODEs）的优雅机制，它们如何巧妙地在离散的数据点之间架起一座通往时间[连续流](@article_id:367779)动的桥梁。但是，一件精美的数学工具，就像一个放在盒子里的奇妙仪器，只有当我们拿出它来创造些什么时，其真正的价值才得以彰显。所以，现在，真正的乐趣开始了。我们到底能用神经[微分方程](@article_id:327891)来*做*什么呢？

本章将是一场进入现代科学家工作室的旅程。我们将看到，神经[微分方程](@article_id:327891)不仅仅是一个用来拟合曲线的工具，更是一种功能强大的新型科学仪器。我们将首先学习如何利用它们作为“动力学侦探”，从数据中发掘隐藏的模式。接着，我们将看到一个训练好的模型如何成为生物系统的“[数字孪生](@article_id:323264)”——一个*计算机中的（in silico）*世界，我们可以在其中进行那些在实验室里难以甚至无法完成的实验。最后，也是最激动人心的部分，我们将探索如何将这些模型与我们已有的科学知识相融合，创造出能够从数据中学习我们未知部分的混合理论。

### 数据驱动的动力学发现

想象一下，你面对着一个随时间演变的复杂系统，但你对其内在的运作法则知之甚少。这正是神经[微分方程](@article_id:327891)大展身手的舞台。

最直接的应用，便是从[时间序列数据](@article_id:326643)中学习动力学本身。假设你是一位追踪细菌菌落生长的生物学家。你在几个不同的时间点上进行了测量，但这些测量点之间发生了什么？明天又会怎样？传统的模型（如指数增长或逻辑斯蒂增长）可能因为形式过于简单而无法准确描述。而一个神经[微分方程](@article_id:327891)则能学习到细菌正在遵循的那个内在的“增长法则”——即描述种群变化率的函数 $f$，形式为 $\frac{dP}{dt} = f(P)$。得到这个函数后，你就拥有了一个连续的时间模型，可以精确地在任意两个测量点之间进行插值，或是在此基础上对未来进行预测 [@problem_id:1453829]。

这种方法的威力远不止于此。思考一下系统生物学中的一个宏大挑战：破解[基因调控网络](@article_id:311393)（Gene Regulatory Networks, GRNs）。细胞是一个由成千上万个[基因相互作用](@article_id:339419)构成的繁华都市。我们可以测量它们随时间变化的表达水平，但“谁调控谁”这张关系网却异常复杂。传统方法要求我们预先猜测这些相互作用的数学形式。而神经[微分方程](@article_id:327891)则让我们能够简单地假设：所有基因表达水平的变化率 $\frac{d\mathbf{x}}{dt}$，都是当前状态 $\mathbf{x}$ 的某个函数 $f(\mathbf{x})$，然后让神经网络直接从数据中发现这个函数 $f$。这种方法已经被用来模拟复杂的[生物振荡器](@article_id:308549)，例如控制我们日常节律的[生物钟](@article_id:327857) [@problem_id:1453845]。神经网络学习到了基因[反馈回路](@article_id:337231)之间错综复杂的舞蹈，而我们无需事先写下一个[米氏方程](@article_id:306915) [@problem_id:1453840]。

这种原理的普适性也使其超越了细胞的界限，延伸到[流行病学](@article_id:301850)等领域。经典的 SIR（易感-感染-抵抗）模型通常假设疾病的传播率是一个常数。但在现实中，人类行为、[公共卫生](@article_id:337559)干预和季节性因素都会导致这个速率随时间变化。通过用一个小型[神经网络](@article_id:305336)来学习一个依赖于时间的传播因子 $g(t)$，并以此取代原模型中的常数传播参数，我们就能从感染数据中构建出远比经典模型更真实、更具适应性的[流行病模型](@article_id:334747) [@problem_id:1453809]。

### 学习模型的力量：*计算机中的*实验

一旦我们通过数据训练得到了一个动力学模型，它便不再仅仅是一个用于预测的黑箱。它成为了我们研究对象的一个“[数字孪生](@article_id:323264)”，一个我们可以随心所欲进行实验的虚拟实验室。

我们可以用它来回答“如果……会怎样？”类型的问题。假设我们已经训练好一个描述细胞代谢网络的神经[微分方程](@article_id:327891)。这个模型体现了细胞的正常运作规则。那么，如果我们敲除某个基因会发生什么？在现实世界中，这是一项艰苦的实验。但在*计算机中*，这就像在 learned function $f$ 中找到与该基因对应的酶的部分，并将其设为零一样简单。然后，我们可以求解这个新的、被修改过的[微分方程](@article_id:327891)，以预测细胞在[基因敲除](@article_id:306232)后的新[稳态](@article_id:326048)，所有这一切都在计算机上完成。这使得我们能快速检验假设，并为真实世界的实验提供指导 [@problem_id:1453773]。

我们还能进行更深层次的[数学分析](@article_id:300111)。一个学习到的模型本身就是一套方程组，我们可以运用[动力系统理论](@article_id:324239)的所有工具来分析它。例如，许多生物系统（如基因开关）表现出双稳态特性。这些开关的行为由[分岔](@article_id:337668)（bifurcation）所支配——在[分岔点](@article_id:366550)，一个参数（如诱导物浓度）的微小变化会导致系统行为发生剧烈的、质的改变。通过分析学习到的[向量场](@article_id:322515) $f(y, p)$，我们可以从数学上求解出这些分岔点，从而精确预测在何种条件下开关会‘翻转’ [@problem_id:1453779]。

此外，我们还可以定量地探索我们的“数字孪生”以测量其特性。想象一个合成的基因线路，其中某个蛋白质的激活依赖于外部信号。一旦我们有了神经[微分方程](@article_id:327891)模型，我们就可以通过在一系列信号浓度下模拟系统来绘制其响应曲线，进行一次虚拟的“[滴定](@article_id:305793)实验”。我们甚至可以更进一步，分析性地计算灵敏度等系统属性，例如[稳态](@article_id:326048)蛋白质水平对信号无限小变化的响应程度，从而为我们提供关于线路设计的深刻定量见解 [@problem_id:1453835]。

### 两全其美：混合与物理知识启发的模型

这是该领域的前沿地带，也是最令人兴奋的地方。我们不必在机理模型和机器学习之间做出非此即彼的选择，而是可以将它们结合起来。

#### [混合模型](@article_id:330275)与[通用微分方程](@article_id:330428)

通常情况下，我们并非对系统一无所知。我们可能从物理或化学的[第一性原理](@article_id:382249)出发，已经知道了系统动力学的一部分，而另一部分仍然是个谜。[通用微分方程](@article_id:330428)（Universal Differential Equations, UDEs）就是一种优美的混合体：我们写下已知部分的方程，然后插入一个[神经网络](@article_id:305336)来表示未知的、缺失的部分。神经网络的任务就是从数据中学习对我们不完整理论的“修正”。

例如，在免疫学中，我们可以为 T 细胞的自然[死亡率](@article_id:375989)建立一个好的模型，但它们为响应感染而进行的复杂增殖和分化过程却难以确定。我们可以用一个神经网络来表示这些未知项，从而创建一个既尊重我们先验知识，又从数据中学习复杂生物学的[混合模型](@article_id:330275) [@problem_id:1453772]。同样，在一个生物反应器中，我们了解[流体动力学](@article_id:319275)的物理原理（如体积变化、[稀释效应](@article_id:366710)），但细胞对营养物质的代谢反应可能非常复杂。UDE 可以将已知的物理学与学习到的细胞[生长动力学](@article_id:368909)模型结合起来 [@problem_id:1453813]。

这种方法最强大的地方在于，我们有时可以*诠释*神经网络学到了什么。假设我们有一个关于某个信号通路的基本机理模型，但它与数据不太吻合。通过添加一个 UDE 项 $U(y)$，我们训练模型来发现这种差异。之后，我们可以分析学习到的函数 $U(y)$，并追問：“这个数学项可能代表什么样的物理机制？”也许它代表了一个我们之前没有考虑到的隐藏[反馈回路](@article_id:337231)。通过这种方式，UDE 不仅提升了预测的准确性，它还帮助我们发现新的科学 [@problem_id:1453841]。

#### 施加软硬约束

除了部分已知的方程外，我们常常还知道系统必须遵守的一些基本原则，如守恒定律、[化学计量关系](@article_id:304922)和[稀疏性](@article_id:297245)等结构特性。我们可以将这些原则“教给”我们的模型。

**硬约束**。这些是不可违背的规则。最优雅的施加方式是将其构建到模型的体系结构中。考虑一个代谢网络。质量必须守恒。一个消耗一个分子 A 生成一个分子 B 的反应，在模型中不能违反这一原则。化学计量约束神经[微分方程](@article_id:327891)（Stoichiometrically Constrained Neural ODEs, SC-Neural ODEs）巧妙地实现了这一点。[神经网络](@article_id:305336)学习反应的*速率*（通量）$\mathbf{v}$，而最终的动力学则通过将这些通量乘以已知的化学计量矩阵 $S$ 来构建。由此产生的模型 $\frac{d\mathbf{c}}{dt} = S \mathbf{v}$，无论[神经网络](@article_id:305336)学到什么，都能保证质量守恒。这是已知结构与学习功能的完美结合 [@problem_id:1453787]。

**软约束**。有些原则更像是强有力的指导方针，或者更难直接构建到架构中。我们可以通过在训练的损失函数中添加一个惩罚项来实施这些约束。如果在训练过程中模型违反了该原则，它就会受到“惩罚”，优化过程会推动它回到一个尊重我们知识的解。

-   例如，如果我们知道某种酶的总量（游离态和底物结合态之和）必须保持恒定，我们就可以在模拟过程中对这个总量的时间[导数](@article_id:318324)偏离零的情况施加惩罚。这会鼓励模型学习到能够自动遵守该守恒定律的动力学 [@problem_id:1453797]。

-   另一个强有力的例子是稀疏性。在一个大型[基因调控网络](@article_id:311393)中，从生物学角度看，不大可能每个基因都直接影响其他所有基因。网络是稀疏的。我们可以通过对学习到的调控相互作用的复杂性施加惩罚（具体来说，是[向量场](@article_id:322515)[雅可比矩阵](@article_id:303923)的 L1 范数，$\sum_{i,j} \left|\frac{\partial f_i}{\partial y_j}\right|$），来鼓励模型发现这种[稀疏性](@article_id:297245)。这会引导模型去寻找能够解释数据的最簡潔網絡，使其更易于解释，也可能更准确 [@problem_id:1453812]。

### 结论

回顾我们的旅程，我们已经看到神经[微分方程](@article_id:327891)是一种用途极其广泛的工具。当我们一无所知时，它们可以充当纯粹的数据驱动建模者；当我们有了模型后，它们又可以成为探索假设的数字实验室；而最强大的是，它们还能在科学发现的过程中充当合作者。通过让我们将已知与从数据中学习到的知识相融合，它们不仅取代了旧模型，更增强了我们创造新模型的能力。它们代表着我们迈向未来的重要一步，在那个未来，自动化的科学发现将帮助我们揭示自然世界中复杂而动态的美。