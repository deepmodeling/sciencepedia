{"hands_on_practices": [{"introduction": "To grasp self-organized criticality, we must first understand the fundamental dynamics of an avalanche. This practice problem uses the stochastic Manna model, where the distribution of particles during a topple is random, to explore the probabilistic nature of avalanche propagation. By calculating the probability of a small, finite avalanche in a hypothetical one-dimensional system, you will develop a concrete feel for how local instability rules lead to complex, emergent behavior. [@problem_id:891367]", "problem": "The 1D stochastic Manna model is defined on a lattice of sites $i=1, 2, \\dots, L$. Each site has an integer number of particles, denoted by a height $z_i$. A site $i$ becomes unstable and topples if its height $z_i$ reaches or exceeds a critical threshold, $z_c$. A toppling event at site $i$ consists of reducing its height by $z_c$ (i.e., $z_i \\to z_i - z_c$) and distributing the $z_c$ particles to its neighbors. In the stochastic Manna model, each of the $z_c$ particles is independently and randomly sent to one of the neighboring sites. An avalanche is a sequence of such topplings that continues until all sites are stable ($z_i  z_c$ for all $i$).\n\nConsider this model on a lattice with $L=5$ sites and open boundaries, where the critical threshold is $z_c=2$. The neighbors of a site $i$ are $i-1$ and $i+1$. Particles sent to sites $0$ or $L+1=6$ are removed from the system.\n\nAn avalanche is initiated by raising the height of site 3 to its critical value, $z_3=z_c$, starting from a configuration where site 3 has height $z_3=z_c-1=1$ and all other sites are empty ($z_i=0$ for $i \\ne 3$).\n\nCalculate the probability that this avalanche terminates after exactly two topplings.", "solution": "After the first toppling at site 3, the two particles are sent independently to sites 2 or 4 with probability $1/2$ each.  Let $k_2$ be the number sent to site 2 and $k_4=2-k_2$.  Then\n$$\nP(k_2=2)=\\Bigl(\\tfrac12\\Bigr)^2=\\tfrac14,\\quad\nP(k_2=1)=2\\Bigl(\\tfrac12\\Bigr)^2=\\tfrac12,\\quad\nP(k_2=0)=\\tfrac14.\n$$\nAn avalanche continues only if one site receives $2$ particles, so the probability that exactly one site (either 2 or 4) topples next is\n$$\nP(\\text{second topple occurs})=P(k_2=2)+P(k_2=0)=\\tfrac14+\\tfrac14=\\tfrac12.\n$$\nSuppose the second topple occurs at site $j\\in\\{2,4\\}$.  It then emits two particles independently to its neighbors ($j-1$ and $j+1$).  To terminate after this second toppling, neither neighbor may receive both particles.  The probability of a “split” $(1,1)$ distribution is\n$$\n2\\Bigl(\\tfrac12\\Bigr)^2=\\tfrac12.\n$$\nHence the probability to stop exactly after two topplings is\n$$\nP(\\text{exactly 2 topplings})\n=\\bigl(\\tfrac12\\bigr)\\times\\bigl(\\tfrac12\\bigr)\n=\\tfrac14.\n$$", "answer": "$$\\boxed{\\tfrac14}$$", "id": "891367"}, {"introduction": "Beyond stochastic models, the deterministic Abelian Sandpile Model (ASM) reveals a deep and elegant mathematical structure underlying self-organized criticality. This exercise introduces the concept of the sandpile group and its unique identity element, which represents a fundamental recurrent state of the system. By manually simulating the relaxation of an unstable configuration on a small lattice with mixed boundary conditions, you will gain direct experience with the model's dynamics and appreciate how a simple, deterministic rule generates a rich algebraic structure. [@problem_id:891389]", "problem": "The Abelian sandpile model (ASM) is a paradigm of self-organized criticality. Consider the ASM defined on a graph of sites. Each site $i$ holds an integer number of particles, called its height $z_i$. A site is unstable if its height $z_i$ is greater than or equal to its critical height $z_{c,i}$, which is defined as the number of sites it is connected to (its degree, $d_i$).\n\nWhen a site $i$ is unstable, it topples: its height decreases by $d_i$, and it distributes $d_i$ particles, one to each of its neighbors. The process continues until no sites are unstable. A key property of the ASM is that the final stable configuration is independent of the order in which unstable sites are toppled.\n\nThe set of recurrent (ergodic) configurations of the ASM forms a finite Abelian group, known as the sandpile group or the group of recurrents. The group operation is particle addition followed by relaxation. Like any group, it contains a unique identity element.\n\nConsider a one-dimensional chain of $L=5$ sites, indexed $i=1, 2, 3, 4, 5$. The boundary conditions are mixed:\n-   At $i=1$, the boundary is closed (reflecting). This means site 1 is only connected to site 2.\n-   At $i=5$, the boundary is open (dissipative). This means site 5 is connected to site 4 and to an external \"sink\" site that absorbs particles forever.\n\nFor this system, the identity configuration, $e = (z_1^{(e)}, \\dots, z_5^{(e)})$, can be constructed via the following algorithm:\n1.  Start with the configuration of zero height at all sites, $z_i=0$ for all $i$.\n2.  Add one particle to *every* site from $i=1$ to $i=5$.\n3.  Let the system evolve by toppling all unstable sites until it reaches a stable configuration.\n\nThis final stable configuration is the identity element of the sandpile group for this specific graph.\n\nYour task is to compute the total number of particles, $N_{identity} = \\sum_{i=1}^{5} z_i^{(e)}$, in the identity configuration of this model.", "solution": "We have a chain of 5 sites with degrees  \n$$d_1=1,\\quad d_2=d_3=d_4=d_5=2.$$  \nThe toppling rule at site $i$ is  \n$$z_i\\to z_i-d_i,\\quad z_j\\to z_j+1\\ \\text{for each neighbor }j\\in N(i).$$  \nAlgorithm: start with $z^{(0)}=(0,0,0,0,0)$, add one particle to each site to get $z^{(1)}=(1,1,1,1,1)$, then topple unstable sites ($z_i\\ge d_i$) until stability.\n\nWe label successive configurations $z^{(k)}=(z_1^{(k)},\\dots,z_5^{(k)})$:\n\n1. $z^{(1)}=(1,1,1,1,1)$; unstable $\\{1\\}$. Topple 1:\n   $$z^{(2)}=(1-1,\\;1+1,\\;1,\\;1,\\;1)=(0,2,1,1,1).$$\n\n2. $z^{(2)}=(0,2,1,1,1)$; unstable $\\{2\\}$. Topple 2:\n   $$z^{(3)}=(0+1,\\;2-2,\\;1+1,\\;1,\\;1)=(1,0,2,1,1).$$\n\n3. $z^{(3)}=(1,0,2,1,1)$; unstable $\\{1,3\\}$. Topple 1:\n   $$z^{(4)}=(1-1,\\;0+1,\\;2,\\;1,\\;1)=(0,1,2,1,1).$$\n\n4. $z^{(4)}=(0,1,2,1,1)$; unstable $\\{3\\}$. Topple 3:\n   $$z^{(5)}=(0,\\;1+1,\\;2-2,\\;1+1,\\;1)=(0,2,0,2,1).$$\n\n5. $z^{(5)}=(0,2,0,2,1)$; unstable $\\{2,4\\}$. Topple 2:\n   $$z^{(6)}=(0+1,\\;2-2,\\;0+1,\\;2,\\;1)=(1,0,1,2,1).$$\n\n6. $z^{(6)}=(1,0,1,2,1)$; unstable $\\{1,4\\}$. Topple 1:\n   $$z^{(7)}=(1-1,\\;0+1,\\;1,\\;2,\\;1)=(0,1,1,2,1).$$\n\n7. $z^{(7)}=(0,1,1,2,1)$; unstable $\\{4\\}$. Topple 4:\n   $$z^{(8)}=(0,1,\\;1+1,\\;2-2,\\;1+1)=(0,1,2,0,2).$$\n\n8. $z^{(8)}=(0,1,2,0,2)$; unstable $\\{3,5\\}$. Topple 3:\n   $$z^{(9)}=(0,\\;1+1,\\;2-2,\\;0+1,\\;2)=(0,2,0,1,2).$$\n\n9. $z^{(9)}=(0,2,0,1,2)$; unstable $\\{2,5\\}$. Topple 2:\n   $$z^{(10)}=(0+1,\\;2-2,\\;0+1,\\;1,\\;2)=(1,0,1,1,2).$$\n\n10. $z^{(10)}=(1,0,1,1,2)$; unstable $\\{1,5\\}$. Topple 1:\n    $$z^{(11)}=(1-1,\\;0+1,\\;1,\\;1,\\;2)=(0,1,1,1,2).$$\n\n11. $z^{(11)}=(0,1,1,1,2)$; unstable $\\{5\\}$. Topple 5 (one to sink, one to 4):\n    $$z^{(12)}=(0,1,1,\\;1+1,\\;2-2)=(0,1,1,2,0).$$\n\n12. $z^{(12)}=(0,1,1,2,0)$; unstable $\\{4\\}$. Topple 4:\n    $$z^{(13)}=(0,1,\\;1+1,\\;2-2,\\;0+1)=(0,1,2,0,1).$$\n\n13. $z^{(13)}=(0,1,2,0,1)$; unstable $\\{3\\}$. Topple 3:\n    $$z^{(14)}=(0,\\;1+1,\\;2-2,\\;0+1,\\;1)=(0,2,0,1,1).$$\n\n14. $z^{(14)}=(0,2,0,1,1)$; unstable $\\{2\\}$. Topple 2:\n    $$z^{(15)}=(0+1,\\;2-2,\\;0+1,\\;1,\\;1)=(1,0,1,1,1).$$\n\n15. $z^{(15)}=(1,0,1,1,1)$; unstable $\\{1\\}$. Topple 1:\n    $$z^{(16)}=(1-1,\\;0+1,\\;1,\\;1,\\;1)=(0,1,1,1,1).$$\n\nNow $z^{(16)}=(0,1,1,1,1)$ is stable ($z_1d_1$, $z_{2\\ldots5}d_{2\\ldots5}$).  \nThus the identity configuration is  \n$$(z_1^{(e)},\\dots,z_5^{(e)})=(0,1,1,1,1),$$  \nand  \n$$N_{\\rm identity}=\\sum_{i=1}^5 z_i^{(e)}=0+1+1+1+1=4.$$", "answer": "$$\\boxed{4}$$", "id": "891389"}, {"introduction": "The definitive signature of self-organized criticality is the emergence of power-law distributions, which describe events occurring across a wide range of magnitudes with no characteristic scale. This computational problem shifts our focus from the microscopic toppling rules to this macroscopic statistical outcome. You will implement the inverse transform sampling method to generate data following a power-law, a critical skill for simulating and analyzing not just sandpiles, but a vast array of complex systems that exhibit SOC. [@problem_id:2403921]", "problem": "Avalanche sizes in a self-organized criticality (SOC) sandpile can be modeled as a continuous truncated power-law random variable. Let the avalanche size be a dimensionless variable denoted by $s$, supported on the closed interval $[s_{\\min}, s_{\\max}]$ with $s_{\\min}  0$ and $s_{\\max}  s_{\\min}$. The probability density function is defined up to a normalization constant by $p(s) \\propto s^{-\\tau}$ for $s \\in [s_{\\min}, s_{\\max}]$, where $\\tau  0$ is a given exponent. Assume the distribution is strictly zero outside this interval.\n\nYour task is to write a complete, runnable program that, for several parameter sets, generates independent samples from this distribution using only independent draws from the continuous uniform distribution on $(0,1)$ as the base source of randomness. The program must compute, from first principles, the exact normalization constant and the exact mean $\\mathbb{E}[s]$ of the truncated distribution for each parameter set, and then estimate the relative error between the sample mean and the exact mean. Use a single deterministic pseudorandom number generator seeded once with the integer $314159$ for reproducibility across all test cases.\n\nFor each parameter set $(\\tau, s_{\\min}, s_{\\max}, N)$, where $N$ is the number of independent samples to draw, perform the following steps:\n1. Compute the exact normalization constant of the truncated power-law density on $[s_{\\min}, s_{\\max}]$.\n2. Compute the exact mean $\\mathbb{E}[s]$ of the truncated distribution on $[s_{\\min}, s_{\\max}]$.\n3. Generate $N$ independent samples from the specified truncated distribution using only independent uniform $(0,1)$ variates as input randomness, with the single fixed seed $314159$ set once at the start of the program.\n4. Compute the empirical sample mean $\\overline{s}_N$ and report the relative error defined as $|\\overline{s}_N - \\mathbb{E}[s]| / \\mathbb{E}[s]$.\n\nAll quantities are dimensionless. The test suite consists of the following four parameter sets:\n- Case A: $(\\tau, s_{\\min}, s_{\\max}, N) = (1.0, 1.0, 100.0, 100000)$.\n- Case B: $(\\tau, s_{\\min}, s_{\\max}, N) = (1.5, 1.0, 50.0, 120000)$.\n- Case C: $(\\tau, s_{\\min}, s_{\\max}, N) = (2.5, 0.5, 20.0, 80000)$.\n- Case D: $(\\tau, s_{\\min}, s_{\\max}, N) = (0.75, 1.0, 10.0, 100000)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order A, B, C, D. Each list entry must be the relative error for the corresponding case, rounded to $6$ decimal places, for example, $[x_A,x_B,x_C,x_D]$ where each $x_\\cdot$ is a floating-point number with $6$ digits after the decimal point.", "solution": "The probability density function (PDF) for the avalanche size $s$ is given by $p(s) = C s^{-\\tau}$ on the support $[s_{\\min}, s_{\\max}]$, and $p(s) = 0$ otherwise. Here, $\\tau  0$, $s_{\\min}  0$, and $s_{\\max}  s_{\\min}$ are given parameters, and $C$ is a normalization constant. The solution requires several steps: deriving the normalization constant $C$, finding the cumulative distribution function (CDF) and its inverse for sampling, deriving the exact expected value $\\mathbb{E}[s]$, and finally, implementing the numerical simulation to estimate the relative error.\n\nFirst, we determine the normalization constant $C$ by requiring the integral of the PDF over its support to be unity:\n$$\n\\int_{s_{\\min}}^{s_{\\max}} p(s) ds = \\int_{s_{\\min}}^{s_{\\max}} C s^{-\\tau} ds = 1\n$$\nThe evaluation of this integral depends on the value of $\\tau$.\n\nCase 1: $\\tau \\neq 1$.\nThe integral is $C \\left[ \\frac{s^{1-\\tau}}{1-\\tau} \\right]_{s_{\\min}}^{s_{\\max}} = C \\frac{s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}}{1-\\tau}$.\nSetting this to $1$ and solving for $C$ yields:\n$$\nC = \\frac{1-\\tau}{s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}}\n$$\n\nCase 2: $\\tau = 1$.\nThe PDF is $p(s) \\propto s^{-1}$, and the integral becomes $C \\int_{s_{\\min}}^{s_{\\max}} \\frac{1}{s} ds = C [\\ln s]_{s_{\\min}}^{s_{\\max}} = C (\\ln s_{\\max} - \\ln s_{\\min}) = C \\ln(s_{\\max}/s_{\\min})$.\nSetting this to $1$ gives the normalization constant:\n$$\nC = \\frac{1}{\\ln(s_{\\max}/s_{\\min})}\n$$\n\nNext, we derive the machinery for the inverse transform sampling method. This requires the cumulative distribution function (CDF), $F(s) = P(S \\le s)$, which is defined as:\n$$\nF(s) = \\int_{s_{\\min}}^{s} p(x) dx\n$$\nThe method generates samples by drawing a uniform random number $u \\in [0,1)$ and computing $s = F^{-1}(u)$.\n\nCase 1: $\\tau \\neq 1$.\n$$\nF(s) = \\int_{s_{\\min}}^{s} C x^{-\\tau} dx = C \\frac{s^{1-\\tau} - s_{\\min}^{1-\\tau}}{1-\\tau}\n$$\nSubstituting the expression for $C$ for this case, we find:\n$$\nF(s) = \\left( \\frac{1-\\tau}{s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}} \\right) \\left( \\frac{s^{1-\\tau} - s_{\\min}^{1-\\tau}}{1-\\tau} \\right) = \\frac{s^{1-\\tau} - s_{\\min}^{1-\\tau}}{s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}}\n$$\nSetting $F(s) = u$ and solving for $s$ yields the inverse CDF, $F^{-1}(u)$:\n$$\nu = \\frac{s^{1-\\tau} - s_{\\min}^{1-\\tau}}{s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}} \\implies s^{1-\\tau} = s_{\\min}^{1-\\tau} + u(s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau})\n$$\n$$\ns = F^{-1}(u) = \\left[ s_{\\min}^{1-\\tau} + u(s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}) \\right]^{\\frac{1}{1-\\tau}}\n$$\n\nCase 2: $\\tau = 1$.\n$$\nF(s) = \\int_{s_{\\min}}^{s} C x^{-1} dx = C (\\ln s - \\ln s_{\\min})\n$$\nSubstituting the corresponding $C$:\n$$\nF(s) = \\frac{\\ln s - \\ln s_{\\min}}{\\ln s_{\\max} - \\ln s_{\\min}} = \\frac{\\ln(s/s_{\\min})}{\\ln(s_{\\max}/s_{\\min})}\n$$\nSetting $F(s) = u$ and solving for $s$:\n$$\nu \\ln(s_{\\max}/s_{\\min}) = \\ln(s/s_{\\min}) \\implies s/s_{\\min} = \\exp(u \\ln(s_{\\max}/s_{\\min})) = \\left( \\frac{s_{\\max}}{s_{\\min}} \\right)^u\n$$\n$$\ns = F^{-1}(u) = s_{\\min} \\left( \\frac{s_{\\max}}{s_{\\min}} \\right)^u\n$$\n\nThen, we compute the exact mean (expected value) of the distribution, $\\mathbb{E}[s]$:\n$$\n\\mathbb{E}[s] = \\int_{s_{\\min}}^{s_{\\max}} s \\cdot p(s) ds = C \\int_{s_{\\min}}^{s_{\\max}} s^{1-\\tau} ds\n$$\nThe evaluation of this integral depends on the exponent $1-\\tau$, so we must consider cases for $\\tau$.\n\nCase 1: $\\tau \\neq 2$.\nThe integral is $\\int s^{1-\\tau} ds = \\frac{s^{2-\\tau}}{2-\\tau}$. Thus,\n$$\n\\int_{s_{\\min}}^{s_{\\max}} s^{1-\\tau} ds = \\frac{s_{\\max}^{2-\\tau} - s_{\\min}^{2-\\tau}}{2-\\tau}\n$$\nThe mean is $\\mathbb{E}[s] = C \\left( \\frac{s_{\\max}^{2-\\tau} - s_{\\min}^{2-\\tau}}{2-\\tau} \\right)$. The expression for $C$ depends on whether $\\tau=1$.\n- If $\\tau \\notin \\{1, 2\\}$, we use $C = \\frac{1-\\tau}{s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}}$:\n$$\n\\mathbb{E}[s] = \\left( \\frac{1-\\tau}{2-\\tau} \\right) \\left( \\frac{s_{\\max}^{2-\\tau} - s_{\\min}^{2-\\tau}}{s_{\\max}^{1-\\tau} - s_{\\min}^{1-\\tau}} \\right)\n$$\n- If $\\tau=1$, the integral becomes $\\int_{s_{\\min}}^{s_{\\max}} s^0 ds = s_{\\max} - s_{\\min}$. We use $C = 1/\\ln(s_{\\max}/s_{\\min})$:\n$$\n\\mathbb{E}[s] = \\frac{s_{\\max} - s_{\\min}}{\\ln(s_{\\max}/s_{\\min})}\n$$\n\nCase 2: $\\tau = 2$.\nThe integral becomes $\\int_{s_{\\min}}^{s_{\\max}} s^{-1} ds = \\ln(s_{\\max}/s_{\\min})$. The normalization constant for $\\tau=2$ (which is $\\neq 1$) is $C=\\frac{1-2}{s_{\\max}^{-1}-s_{\\min}^{-1}} = \\frac{-1}{1/s_{\\max}-1/s_{\\min}} = \\frac{s_{\\min}s_{\\max}}{s_{\\max}-s_{\\min}}$.\nTherefore, the mean is:\n$$\n\\mathbb{E}[s] = \\left( \\frac{s_{\\min}s_{\\max}}{s_{\\max}-s_{\\min}} \\right) \\ln\\left(\\frac{s_{\\max}}{s_{\\min}}\\right)\n$$\n\nFinally, for each parameter set $(\\tau, s_{\\min}, s_{\\max}, N)$, the algorithm is as follows:\n1.  Select the appropriate formula for $\\mathbb{E}[s]$ based on the value of $\\tau$ and compute it.\n2.  Initialize a pseudo-random number generator with the specified seed.\n3.  Generate $N$ uniform variates $u_i \\in [0,1)$.\n4.  For each $u_i$, compute a sample $s_i = F^{-1}(u_i)$ using the correct inverse CDF formula for the given $\\tau$.\n5.  Compute the sample mean $\\overline{s}_N = \\frac{1}{N}\\sum_{i=1}^{N} s_i$.\n6.  Compute the relative error $\\epsilon_{rel} = \\frac{|\\overline{s}_N - \\mathbb{E}[s]|}{\\mathbb{E}[s]}$.\n\nThis procedure is implemented for the four specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating the relative error for the mean of a\n    truncated power-law distribution using inverse transform sampling.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0, 100.0, 100000),  # Case A\n        (1.5, 1.0, 50.0, 120000),   # Case B\n        (2.5, 0.5, 20.0, 80000),    # Case C\n        (0.75, 1.0, 10.0, 100000),  # Case D\n    ]\n\n    # Initialize a single RNG with a fixed seed for reproducibility across all cases.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for case in test_cases:\n        tau, s_min, s_max, N = case\n\n        # 1. Compute the exact mean E[s]\n        exact_mean = 0.0\n        if tau == 1.0:\n            # Special case for tau = 1\n            if s_max > s_min:\n                exact_mean = (s_max - s_min) / np.log(s_max / s_min)\n        elif tau == 2.0:\n            # Special case for tau = 2\n            if s_max > s_min:\n                exact_mean = (s_min * s_max / (s_max - s_min)) * np.log(s_max / s_min)\n        else:\n            # General case for tau != 1 and tau != 2\n            term1 = (1.0 - tau) / (2.0 - tau)\n            # Use np.power for robust exponentiation\n            num = np.power(s_max, 2.0 - tau) - np.power(s_min, 2.0 - tau)\n            den = np.power(s_max, 1.0 - tau) - np.power(s_min, 1.0 - tau)\n            if den != 0:\n                exact_mean = term1 * (num / den)\n\n        # 2. Generate N uniform random variates\n        u = rng.random(size=int(N))\n        \n        # 3. Generate N samples from the distribution using inverse transform sampling\n        samples = np.zeros(int(N))\n        if tau == 1.0:\n            # Inverse CDF for tau = 1\n            samples = s_min * np.power(s_max / s_min, u)\n        else:\n            # Inverse CDF for tau != 1\n            one_minus_tau = 1.0 - tau\n            s_min_pow = np.power(s_min, one_minus_tau)\n            s_max_pow = np.power(s_max, one_minus_tau)\n            \n            inner_term = s_min_pow + u * (s_max_pow - s_min_pow)\n            samples = np.power(inner_term, 1.0 / one_minus_tau)\n            \n        # 4. Compute the sample mean and relative error\n        sample_mean = np.mean(samples)\n        \n        relative_error = 0.0\n        if exact_mean != 0:\n            relative_error = np.abs(sample_mean - exact_mean) / exact_mean\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{err:.6f}' for err in results])}]\")\n\nsolve()\n```", "id": "2403921"}]}