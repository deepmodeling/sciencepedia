{"hands_on_practices": [{"introduction": "To truly understand the correlation dimension, we must first master its foundational component: the correlation sum $C(r)$. This exercise provides a direct, hands-on opportunity to apply the governing formula to a small set of points, demystifying the calculation by breaking it down into manageable steps. By manually counting the pairs of vectors within a given radius [@problem_id:1665706], you will build a concrete intuition for how the geometric distribution of points in phase space translates into the value of $C(r)$.", "problem": "In the analysis of chaotic systems, the correlation dimension is a measure of the fractal dimension of a strange attractor. A key step in its calculation is computing the correlation sum, $C(r)$, for a given radius $r$. The correlation sum is defined as the fraction of pairs of state-space vectors whose distance is less than or equal to $r$.\n\nConsider a set of $N=5$ two-dimensional reconstructed state-space vectors, $\\{\\vec{x}_i\\}_{i=1}^5$, given by:\n$\\vec{x}_1 = (1.0, 2.0)$\n$\\vec{x}_2 = (3.0, 3.0)$\n$\\vec{x}_3 = (5.0, 1.0)$\n$\\vec{x}_4 = (1.5, 4.0)$\n$\\vec{x}_5 = (4.0, 1.5)$\n\nThe correlation sum is given by the formula:\n$$C(r) = \\frac{1}{\\binom{N}{2}} \\sum_{1 \\le i < j \\le N} \\Theta(r - d(\\vec{x}_i, \\vec{x}_j))$$\nwhere $\\binom{N}{2}$ is the total number of distinct pairs of vectors, and $\\Theta(u)$ is the Heaviside step function, defined as $\\Theta(u) = 1$ for $u \\ge 0$ and $\\Theta(u) = 0$ for $u < 0$. The distance $d(\\vec{x}_i, \\vec{x}_j)$ between two vectors $\\vec{x}_i = (x_{i1}, x_{i2})$ and $\\vec{x}_j = (x_{j1}, x_{j2})$ is calculated using the maximum norm (or Chebyshev distance):\n$$d(\\vec{x}_i, \\vec{x}_j) = \\max(|x_{i1} - x_{j1}|, |x_{i2} - x_{j2}|)$$\n\nCalculate the value of the correlation sum $C(r)$ for a radius of $r=2.1$. Express your answer as an exact fraction in simplest form.", "solution": "The correlation sum for radius $r$ is defined as\n$$\nC(r)=\\frac{1}{\\binom{N}{2}}\\sum_{1\\leq i< j\\leq N}\\Theta\\!\\big(r-d(\\vec{x}_{i},\\vec{x}_{j})\\big),\n$$\nwhere $d(\\vec{x}_{i},\\vec{x}_{j})=\\max\\!\\big(|x_{i1}-x_{j1}|,\\;|x_{i2}-x_{j2}|\\big)$ and $\\Theta(u)=1$ if $u\\geq 0$ and $0$ otherwise. With $N=5$, we have $\\binom{5}{2}=10$. For $r=2.1$, evaluate each of the $10$ distinct pairs:\n\n1. $(1,2)$: $d=\\max(|1-3|,|2-3|)=\\max(2,1)=2\\leq 2.1\\Rightarrow \\Theta=1$.\n2. $(1,3)$: $d=\\max(|1-5|,|2-1|)=\\max(4,1)=4>2.1\\Rightarrow \\Theta=0$.\n3. $(1,4)$: $d=\\max(|1-1.5|,|2-4|)=\\max(0.5,2)=2\\leq 2.1\\Rightarrow \\Theta=1$.\n4. $(1,5)$: $d=\\max(|1-4|,|2-1.5|)=\\max(3,0.5)=3>2.1\\Rightarrow \\Theta=0$.\n5. $(2,3)$: $d=\\max(|3-5|,|3-1|)=\\max(2,2)=2\\leq 2.1\\Rightarrow \\Theta=1$.\n6. $(2,4)$: $d=\\max(|3-1.5|,|3-4|)=\\max(1.5,1)=1.5\\leq 2.1\\Rightarrow \\Theta=1$.\n7. $(2,5)$: $d=\\max(|3-4|,|3-1.5|)=\\max(1,1.5)=1.5\\leq 2.1\\Rightarrow \\Theta=1$.\n8. $(3,4)$: $d=\\max(|5-1.5|,|1-4|)=\\max(3.5,3)=3.5>2.1\\Rightarrow \\Theta=0$.\n9. $(3,5)$: $d=\\max(|5-4|,|1-1.5|)=\\max(1,0.5)=1\\leq 2.1\\Rightarrow \\Theta=1$.\n10. $(4,5)$: $d=\\max(|1.5-4|,|4-1.5|)=\\max(2.5,2.5)=2.5>2.1\\Rightarrow \\Theta=0$.\n\nThere are $6$ pairs with $d\\leq r$. Therefore,\n$$\nC(2.1)=\\frac{6}{10}=\\frac{3}{5}.\n$$", "answer": "$$\\boxed{\\frac{3}{5}}$$", "id": "1665706"}, {"introduction": "Real-world data is never infinitely precise; it is always limited by the resolution of the measuring instrument. This practice explores the consequences of such data quantization on the correlation dimension analysis [@problem_id:1665729]. By considering how discretization affects the minimum possible distance between distinct points, you will discover a characteristic artifact on the log-log plot that is crucial to recognize for accurate dimension estimation.", "problem": "An undergraduate student is analyzing an experimental time series $\\{x_n\\}$ that is believed to originate from a chaotic system. The data was recorded using a digital instrument with a finite resolution. This quantization means that any measured value $x_n$ is an integer multiple of a fundamental resolution step $\\delta > 0$.\n\nTo characterize the system's dynamics, the student employs the method of time-delay embedding to reconstruct the attractor in an $m$-dimensional phase space, where $m > 1$. The reconstructed state vectors are of the form $\\vec{V}_i = (x_i, x_{i+\\tau}, \\dots, x_{i+(m-1)\\tau})$, where $\\tau$ is a chosen time delay.\n\nFollowing this, the student computes the correlation integral, $C(r)$, using the Grassberger-Procaccia algorithm. $C(r)$ is defined as the fraction of pairs of reconstructed vectors $(\\vec{V}_i, \\vec{V}_j)$ with $i \\neq j$ whose Euclidean distance is less than a given radius $r$:\n$$\nC(r) = \\frac{2}{N(N-1)} \\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} \\Theta(r - \\|\\vec{V}_i - \\vec{V}_j\\|)\n$$\nwhere $N$ is the total number of vectors and $\\Theta$ is the Heaviside step function. The correlation dimension, $D_2$, is then estimated from the slope of the linear scaling region in the log-log plot of $C(r)$ versus $r$.\n\nUpon generating the plot of $\\log(C(r))$ versus $\\log(r)$, the student notices a distinct behavior for very small radii, specifically in the range $r < \\delta$. Which of the following statements best describes the characteristic feature of the plot that the student should expect to see in this small-radius regime?\n\nA. The plot will be a line with a slope equal to the embedding dimension $m$.\n\nB. The plot will be a line with a slope equal to the true correlation dimension $D_2$.\n\nC. The plot will exhibit a plateau with a slope approximately equal to zero.\n\nD. The plot will be a line with a slope of approximately one, irrespective of the system's dynamics.\n\nE. The plot will show a slope that tends towards negative infinity.", "solution": "The problem asks us to determine the behavior of the correlation integral $C(r)$ for small radii $r$ when the time series data is quantized with a resolution $\\delta$.\n\nStep 1: Analyze the structure of the reconstructed vectors.\nThe time series data $\\{x_n\\}$ consists of values that are integer multiples of the resolution $\\delta$. That is, for every $n$, $x_n = k_n \\delta$ for some integer $k_n$. A reconstructed vector in the $m$-dimensional embedding space is given by $\\vec{V}_i = (x_i, x_{i+\\tau}, \\dots, x_{i+(m-1)\\tau})$. Substituting the quantized form of the data, we get:\n$$\n\\vec{V}_i = (k_i \\delta, k_{i+\\tau} \\delta, \\dots, k_{i+(m-1)\\tau} \\delta) = \\delta (k_i, k_{i+\\tau}, \\dots, k_{i+(m-1)\\tau})\n$$\nThis expression shows that all reconstructed state vectors $\\vec{V}_i$ do not lie on a continuous fractal manifold, but are restricted to the vertices of a hypercubic lattice in the $m$-dimensional space. The fundamental spacing of this lattice is $\\delta$.\n\nStep 2: Calculate the minimum distance between distinct reconstructed vectors.\nWe need to find the distance between two distinct vectors, $\\vec{V}_i$ and $\\vec{V}_j$, where $i \\neq j$. The squared Euclidean distance is:\n$$\n\\|\\vec{V}_i - \\vec{V}_j\\|^2 = \\sum_{k=0}^{m-1} (x_{i+k\\tau} - x_{j+k\\tau})^2\n$$\nSince each $x_n$ is an integer multiple of $\\delta$, the difference $(x_{i+k\\tau} - x_{j+k\\tau})$ must also be an integer multiple of $\\delta$. Let this difference be $n_k \\delta$ for some integers $n_k$.\n$$\n\\|\\vec{V}_i - \\vec{V}_j\\|^2 = \\sum_{k=0}^{m-1} (n_k \\delta)^2 = \\delta^2 \\sum_{k=0}^{m-1} n_k^2\n$$\nSince the vectors $\\vec{V}_i$ and $\\vec{V}_j$ are distinct, at least one of the component differences must be non-zero, meaning at least one integer $n_k$ is non-zero. The smallest possible non-zero value for the sum of squares of integers $\\sum n_k^2$ is 1 (this occurs when one $n_k = \\pm 1$ and all other $n_l=0$ for $l\\neq k$).\nTherefore, the minimum possible squared distance between any two distinct points is $\\delta^2 \\cdot 1 = \\delta^2$. The minimum possible distance is thus $\\sqrt{\\delta^2} = \\delta$.\n\nStep 3: Analyze the correlation integral $C(r)$ for $r < \\delta$.\nThe correlation integral $C(r)$ is proportional to the number of pairs of distinct vectors $(\\vec{V}_i, \\vec{V}_j)$ whose distance is less than $r$. From Step 2, we know that the minimum distance between any two distinct points is $\\delta$.\nIf we consider a radius $r < \\delta$, there are no pairs of *distinct* vectors $(\\vec{V}_i, \\vec{V}_j)$ that can satisfy the condition $\\|\\vec{V}_i - \\vec{V}_j\\| < r$. Consequently, the number of such pairs is zero.\nAccording to the definition provided, $C(r)$ counts pairs of distinct vectors. So, for any $r$ in the range $0 < r < \\delta$, $C(r) = 0$.\n\nStep 4: Analyze the log-log plot $\\log(C(r))$ versus $\\log(r)$.\nIf $C(r)=0$ for $r < \\delta$, then $\\log(C(r))$ would be $-\\infty$. In practice, numerical algorithms often have a finite number of points, and it's possible for some reconstructed vectors to be identical, $\\vec{V}_i = \\vec{V}_j$ for $i \\neq j$. If the definition were modified to include $i=j$ or if there are identical vectors, these pairs would have a distance of 0. In this slightly modified or more practical view, for any small $r > 0$, these zero-distance pairs are counted. Let the number of such pairs be $N_0$. Then for $0 < r < \\delta$, the correlation sum is constant, equal to $N_0$, because no new pairs are added until $r$ reaches $\\delta$.\nThus, for $0 < r < \\delta$, $C(r)$ is a constant (either zero or a small positive constant).\nThe correlation dimension is given by the slope of the plot of $y = \\log(C(r))$ versus $x = \\log(r)$.\n$$\nD_2 \\approx \\frac{d(\\log C(r))}{d(\\log r)}\n$$\nIf $C(r)$ is a constant, say $C_0$, for $r < \\delta$, then $\\log(C(r)) = \\log(C_0)$, which is also a constant. The plot of a constant value on the y-axis against a changing value on the x-axis is a horizontal line. The slope of a horizontal line is zero.\nTherefore, in the region $r < \\delta$, the plot of $\\log(C(r))$ versus $\\log(r)$ will be flat, exhibiting a plateau with a slope of approximately zero. This is a numerical artifact caused by the instrument's finite resolution and does not reflect the true geometry of the attractor. The actual scaling region from which $D_2$ can be estimated only begins for $r > \\delta$.\n\nStep 5: Evaluate the options.\nA. Incorrect. A slope of $m$ would imply the points fill the $m$-dimensional embedding space, which is not true at these scales due to the empty space between lattice points.\nB. Incorrect. The true dimension $D_2$ is only revealed at scales larger than the resolution $\\delta$ but smaller than the overall attractor size.\nC. Correct. As derived, for $r < \\delta$, $C(r)$ is constant, leading to a zero slope on the log-log plot, which appears as a plateau.\nD. Incorrect. A slope of 1 is not generally expected.\nE. Incorrect. A slope tending to negative infinity is not the standard behavior. A zero or small constant value for $C(r)$ would lead to a large negative value for $\\log(C(r))$, but the slope of the plot is what matters for the dimension, and this slope is zero.", "answer": "$$\\boxed{C}$$", "id": "1665729"}, {"introduction": "Beyond instrumental limitations, time series data can also be corrupted by sporadic, large errors or outliers. This exercise presents a thought experiment to quantify how such contamination can skew the results of the Grassberger-Procaccia algorithm [@problem_id:1665704]. You will analyze how a few outlier points can introduce a systematic bias, altering the apparent scaling behavior and leading to an inaccurate estimate of the correlation dimension.", "problem": "A researcher analyzes a set of state-space vectors generated from a time series of a chaotic system. The original, clean dataset consists of $N_A = 10000$ vectors, and for this set, the number of pairs of vectors separated by a distance less than or equal to $r$ is described by the power law $P_A(r) = K r^{D_T}$ within a certain \"scaling region.\" The true correlation dimension of the system is $D_T = 2.4$.\n\nUnfortunately, the dataset is contaminated by the addition of $N_B = 3$ identical outlier vectors. These three vectors are located at the exact same point in the state space, a position that is very far from all the original $N_A$ vectors. The total number of vectors in the corrupted dataset is thus $N = N_A + N_B = 10003$.\n\nThe correlation integral for the full corrupted dataset, denoted $C(r)$, is defined as the total number of distinct pairs of vectors with a separation distance less than or equal to $r$, all divided by the total number of distinct pairs available in the dataset. From this, an apparent correlation dimension is calculated using the formula $D_{app}(r) = \\frac{d(\\ln C(r))}{d(\\ln r)}$.\n\nAt a specific small radius $r_0$ that lies within the scaling region, it is determined that the number of pairs from the original clean dataset with separation less than or equal to $r_0$ is exactly $P_A(r_0) = 30$. Assume that this radius $r_0$ is small enough that the distance between any of the original vectors and any of the outlier vectors is much greater than $r_0$.\n\nCalculate the value of the apparent correlation dimension, $D_{app}(r_0)$, for the corrupted dataset at this radius. Round your final answer to four significant figures.", "solution": "The clean dataset has $N_{A}$ vectors and, in the scaling region, the number of pairs within radius $r$ is\n$$\nP_{A}(r)=K r^{D_{T}},\n$$\nwith $D_{T}=2.4$. The corrupted dataset adds $N_{B}=3$ identical outliers at the same location, all far from the original $N_{A}$ vectors. For any $r>0$ small enough that outliers are far from every original vector, there are:\n- No cross pairs between original and outlier vectors within $r$: $P_{AB}(r)=0$.\n- All outlier-outlier pairs within $r$ because their mutual distance is zero. With $N_{B}=3$, the number of distinct outlier pairs is\n$$\nP_{B}(r)=\\binom{3}{2}=3 \\quad \\text{for all } r>0.\n$$\n\nLet $N=N_{A}+N_{B}$ and $M=\\binom{N}{2}$ be the total number of distinct pairs in the corrupted dataset. The correlation integral is\n$$\nC(r)=\\frac{P_{A}(r)+P_{B}(r)+P_{AB}(r)}{M}=\\frac{K r^{D_{T}}+3}{M}.\n$$\nThe apparent correlation dimension is\n$$\nD_{\\text{app}}(r)=\\frac{d(\\ln C(r))}{d(\\ln r)}=\\frac{r}{C(r)}\\frac{dC(r)}{dr}.\n$$\nDifferentiating $C(r)$ gives\n$$\n\\frac{dC(r)}{dr}=\\frac{K D_{T} r^{D_{T}-1}}{M}.\n$$\nTherefore,\n$$\nD_{\\text{app}}(r)=\\frac{r}{\\frac{K r^{D_{T}}+3}{M}}\\cdot \\frac{K D_{T} r^{D_{T}-1}}{M}\n= D_{T}\\,\\frac{K r^{D_{T}}}{K r^{D_{T}}+3}.\n$$\n\nAt $r=r_{0}$, it is given that $P_{A}(r_{0})=K r_{0}^{D_{T}}=30$. Hence\n$$\nD_{\\text{app}}(r_{0})=D_{T}\\,\\frac{30}{30+3}=D_{T}\\,\\frac{10}{11}.\n$$\nWith $D_{T}=2.4$,\n$$\nD_{\\text{app}}(r_{0})=2.4 \\times \\frac{10}{11}=\\frac{24}{11}=2.181818\\ldots\n$$\nRounded to four significant figures, this is $2.182$.", "answer": "$$\\boxed{2.182}$$", "id": "1665704"}]}