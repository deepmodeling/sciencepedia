{"hands_on_practices": [{"introduction": "The Law of Large Numbers provides a crucial link between theoretical probability and real-world observation. This practice explores this connection in a classic scenario: repeating a trial until a success is achieved. By analyzing the average number of trials needed over many independent experiments, you can directly verify the core tenet of the Strong Law of Large Numbersâ€”that the sample mean almost surely converges to the theoretical expected value [@problem_id:1957093]. This exercise solidifies your understanding of how long-term averages emerge from random, independent events.", "problem": "Let $X_1, X_2, \\dots$ be a sequence of independent and identically distributed (i.i.d.) random variables. Each $X_i$ represents the number of trials needed to get the first success in a sequence of Bernoulli trials, where the probability of success on any given trial is a constant $p$, with $0 < p < 1$. This implies that each $X_i$ follows a Geometric distribution with a probability mass function given by $P(X_i = k) = (1-p)^{k-1}p$ for $k \\in \\{1, 2, 3, \\dots\\}$.\n\nConsider the sample mean of the first $n$ variables, defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. It is a known result in probability theory that as the number of observations $n$ tends to infinity, the sample mean $\\bar{X}_n$ converges almost surely to a specific constant value.\n\nYour task is to determine this constant value. Express your answer as a closed-form analytic expression in terms of the parameter $p$.", "solution": "We are given i.i.d. random variables $X_{1},X_{2},\\dots$ with the geometric distribution on $\\{1,2,\\dots\\}$ with parameter $p\\in(0,1)$, that is $P(X_{i}=k)=(1-p)^{k-1}p$ for $k\\geq 1$. We are to find the almost sure limit of the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ as $n\\to\\infty$.\n\nBy the Strong Law of Large Numbers, if $\\{X_{i}\\}$ are i.i.d. with $\\mathbb{E}[|X_{1}|]<\\infty$, then\n$$\n\\bar{X}_{n}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X_{1}].\n$$\nThus it suffices to compute $\\mathbb{E}[X_{1}]$ and verify it is finite.\n\nCompute the expectation using the series:\n$$\n\\mathbb{E}[X_{1}]=\\sum_{k=1}^{\\infty}k\\,P(X_{1}=k)=\\sum_{k=1}^{\\infty}k\\,(1-p)^{k-1}p.\n$$\nLet $r=1-p$. Since $0<p<1$, we have $0<r<1$. Then\n$$\n\\mathbb{E}[X_{1}]=p\\sum_{k=1}^{\\infty}k\\,r^{k-1}.\n$$\nUse the known identity for $|r|<1$:\n$$\n\\sum_{k=1}^{\\infty}k\\,r^{k-1}=\\frac{1}{(1-r)^{2}},\n$$\nwhich follows by differentiating the geometric series $\\sum_{k=0}^{\\infty}r^{k}=\\frac{1}{1-r}$ with respect to $r$. Therefore,\n$$\n\\mathbb{E}[X_{1}]=p\\cdot\\frac{1}{(1-r)^{2}}=p\\cdot\\frac{1}{(1-(1-p))^{2}}=p\\cdot\\frac{1}{p^{2}}=\\frac{1}{p}.\n$$\nSince $X_{1}\\geq 0$, we have $\\mathbb{E}[|X_{1}|]=\\mathbb{E}[X_{1}]=\\frac{1}{p}<\\infty$. Hence by the Strong Law of Large Numbers,\n$$\n\\bar{X}_{n}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X_{1}]=\\frac{1}{p}.\n$$\nTherefore, the almost sure limit of the sample mean is the constant $\\frac{1}{p}$.", "answer": "$$\\boxed{\\frac{1}{p}}$$", "id": "1957093"}, {"introduction": "Beyond abstract theory, the Law of Large Numbers is the foundation for powerful computational techniques like the Monte Carlo method. This exercise provides a hands-on look at one of the most famous examples: estimating the value of $\\pi$ by metaphorically throwing darts at a square board. By determining the long-term proportion of random points that land inside an inscribed circle, you will see how a seemingly random process can yield a remarkably accurate approximation of a fundamental mathematical constant [@problem_id:1406798]. This problem highlights how probability can be used to solve problems in geometry and numerical analysis.", "problem": "A computer simulation generates a sequence of independent random points, $P_1, P_2, P_3, \\dots$, in a two-dimensional Cartesian plane. Each point $P_i$ has coordinates $(X_i, Y_i)$, where $X_i$ and $Y_i$ are independent random variables drawn from a uniform distribution on the interval $[0, 1]$. We define a region $\\mathcal{C}$ to be the set of all points $(x, y)$ that satisfy the inequality $(x - 0.5)^2 + (y - 0.5)^2 \\le 0.25$.\n\nLet $S_n$ be the number of points among the first $n$ points ($P_1, \\dots, P_n$) that fall inside or on the boundary of the region $\\mathcal{C}$. Consider the random variable $A_n = 4 \\frac{S_n}{n}$.\n\nDetermine the value to which the sequence of random variables $A_n$ converges almost surely as $n \\to \\infty$. Express your answer as a single closed-form analytic expression.", "solution": "Let $I_{i}$ be the indicator of the event that $P_{i}$ falls in $\\mathcal{C}$, i.e., $I_{i}=1$ if $(X_{i},Y_{i})\\in\\mathcal{C}$ and $I_{i}=0$ otherwise. Then $S_{n}=\\sum_{i=1}^{n} I_{i}$ and $A_{n}=4\\frac{S_{n}}{n}=4\\left(\\frac{1}{n}\\sum_{i=1}^{n} I_{i}\\right)$.\n\nBecause $(X_{i},Y_{i})$ are i.i.d. uniformly distributed on $[0,1]^{2}$, the $I_{i}$ are i.i.d. Bernoulli with parameter $p=\\mathbb{P}\\big((X_{1},Y_{1})\\in\\mathcal{C}\\big)$. Since $\\mathcal{C}$ is the disk centered at $(\\tfrac{1}{2},\\tfrac{1}{2})$ of radius $r$ determined by $(x-\\tfrac{1}{2})^{2}+(y-\\tfrac{1}{2})^{2}\\le \\tfrac{1}{4}$, we have $r^{2}=\\tfrac{1}{4}$ and hence $r=\\tfrac{1}{2}$. This disk lies entirely within the unit square $[0,1]^{2}$, so\n$$\np=\\frac{\\text{area}(\\mathcal{C})}{\\text{area}([0,1]^{2})}=\\pi r^{2}=\\pi\\left(\\frac{1}{2}\\right)^{2}=\\frac{\\pi}{4}.\n$$\n\nBy the strong law of large numbers, since the $I_{i}$ are i.i.d. with finite mean, we have\n$$\n\\frac{S_{n}}{n}=\\frac{1}{n}\\sum_{i=1}^{n} I_{i}\\to \\mathbb{E}[I_{1}]=p=\\frac{\\pi}{4}\\quad\\text{almost surely}.\n$$\nMultiplying by $4$, it follows that\n$$\nA_{n}=4\\frac{S_{n}}{n}\\to 4\\cdot\\frac{\\pi}{4}=\\pi\\quad\\text{almost surely}.\n$$\nTherefore, the sequence $A_{n}$ converges almost surely to $\\pi$ as $n\\to\\infty$.", "answer": "$$\\boxed{\\pi}$$", "id": "1406798"}, {"introduction": "To truly master a scientific law, one must also understand its boundaries and limitations. This problem presents a fascinating counterexample to the Law of Large Numbers using the Cauchy distribution, a distribution with \"heavy tails\" and an undefined mean. You will investigate why, for this distribution, the sample mean fails to converge to a stable value, no matter how many samples are taken [@problem_id:1967315]. This exercise is critical for appreciating the necessary conditions, such as a finite expected value, that underpin the predictive power of the Law of Large Numbers.", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\ldots, X_n, \\ldots$ drawn from the standard Cauchy distribution. The probability density function (PDF) for this distribution is given by:\n$$f(x) = \\frac{1}{\\pi(1+x^2)}, \\quad \\text{for } x \\in (-\\infty, \\infty)$$\nLet $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean of the first $n$ variables. A notable and unusual property of the Cauchy distribution is that for any integer $n \\ge 1$, the sample mean $\\bar{X}_n$ also follows the exact same standard Cauchy distribution.\n\nIn the context of the Weak Law of Large Numbers (WLLN), which describes the convergence of the sample mean, it is instructive to examine distributions that may not satisfy the law's standard conditions. For the sequence of Cauchy random variables described, calculate the limiting value of the probability that the sample mean deviates from the origin by more than a constant $k$.\n\nSpecifically, find an expression for the limit $L = \\lim_{n \\to \\infty} P(|\\bar{X}_n| > k)$, where $k$ is a positive real constant. Express your answer as a function of $k$.", "solution": "We have a sequence of i.i.d. random variables $X_{1}, X_{2}, \\ldots$ with the standard Cauchy density $f(x) = \\frac{1}{\\pi(1+x^{2})}$. The standard Cauchy distribution is strictly stable with index $1$, so for any integer $n \\geq 1$, the sample mean $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$ has the same distribution as $X_{1}$, i.e., $\\bar{X}_{n} \\sim \\text{Cauchy}(0,1)$. Consequently, for any fixed $k > 0$ and any $n$,\n$$\nP(|\\bar{X}_{n}| > k) = P(|X| > k),\n$$\nwhere $X \\sim \\text{Cauchy}(0,1)$. Therefore, the limit $L = \\lim_{n \\to \\infty} P(|\\bar{X}_{n}| > k)$ equals this constant tail probability for the standard Cauchy distribution.\n\nWe compute $P(|X| > k)$ using the density. By symmetry,\n$$\nP(|X| > k) = 2 \\int_{k}^{\\infty} \\frac{1}{\\pi(1+x^{2})} \\, dx.\n$$\nUsing the antiderivative $\\int \\frac{1}{1+x^{2}} \\, dx = \\arctan(x)$ and the limit $\\lim_{x \\to \\infty} \\arctan(x) = \\frac{\\pi}{2}$, we obtain\n$$\nP(|X| > k) = \\frac{2}{\\pi} \\left[ \\arctan(x) \\right]_{x=k}^{x=\\infty} = \\frac{2}{\\pi} \\left( \\frac{\\pi}{2} - \\arctan(k) \\right) = 1 - \\frac{2}{\\pi} \\arctan(k).\n$$\nHence,\n$$\nL = \\lim_{n \\to \\infty} P(|\\bar{X}_{n}| > k) = 1 - \\frac{2}{\\pi} \\arctan(k).\n$$\nThis shows that the probability does not converge to zero as $n \\to \\infty$, illustrating the failure of the Weak Law of Large Numbers for the Cauchy distribution.", "answer": "$$\\boxed{1 - \\frac{2}{\\pi}\\arctan(k)}$$", "id": "1967315"}]}