{"hands_on_practices": [{"introduction": "To truly understand the Metropolis algorithm, there is no substitute for walking through the steps manually. This first exercise provides a guided, \"pencil-and-paper\" simulation for a single particle in a potential well [@problem_id:2005962]. By calculating the energy changes and applying the acceptance criteria for a pre-determined sequence of random numbers, you will build a concrete intuition for how the algorithm explores the configuration space and generates a trajectory consistent with a thermal distribution.", "problem": "A single classical particle moves in one dimension under the influence of a potential given by $V(x) = x^4 - 2x^2$. The particle is in thermal equilibrium with a heat bath at a temperature $T$. We wish to estimate its average potential energy, $\\langle V \\rangle$, by performing a small Monte Carlo simulation using the Metropolis algorithm.\n\nThe simulation proceeds in discrete steps. Starting from an initial position $x_0$, a new position $x_{new}$ is proposed at each step $i$ by taking a random step $\\delta_i$: $x_{new} = x_{old} + \\delta_i$. The change in potential energy is $\\Delta V = V(x_{new}) - V(x_{old})$. The proposed move is accepted with a probability $P_{acc}$ given by:\n$$P_{acc} = \\min\\left(1, \\exp\\left(-\\frac{\\Delta V}{k_B T}\\right)\\right)$$\nTo decide whether to accept the move, a random number $r_i$, drawn uniformly from the interval $[0, 1)$, is compared to $P_{acc}$. If $r_i < P_{acc}$, the move is accepted, and the particle's new position is $x_i = x_{new}$. If $r_i \\ge P_{acc}$, the move is rejected, and the particle remains at its old position, so $x_i = x_{old}$. Note that in either case, the configuration (either the new or the repeated old one) is added to the trajectory for calculating averages.\n\nFor this simulation, use the following parameters:\n- The thermal energy is $k_B T = 1.0$ in arbitrary units of energy. The potential $V(x)$ is also expressed in these same units.\n- The initial position of the particle is $x_0 = 1.0$.\n- You will simulate 4 steps (from $i=1$ to $i=4$).\n- The sequence of random step displacements is: $\\delta_1 = 0.40$, $\\delta_2 = -0.30$, $\\delta_3 = 0.10$, $\\delta_4 = -0.50$.\n- The sequence of random numbers for the acceptance check is: $r_1 = 0.50$, $r_2 = 0.20$, $r_3 = 0.90$, $r_4 = 0.10$.\n\nCalculate the average potential energy over the full trajectory of 5 states (the initial state at $i=0$ and the states after each of the 4 steps). Report your answer as a single real number, rounded to three significant figures.", "solution": "We use the Metropolis algorithm with potential $V(x)=x^{4}-2x^{2}$, thermal energy $k_{B}T=1$, initial position $x_{0}=1.0$, and the given sequences $\\delta_{i}$ and $r_{i}$. At each step, propose $x_{\\text{new}}=x_{\\text{old}}+\\delta_{i}$, compute $\\Delta V=V(x_{\\text{new}})-V(x_{\\text{old}})$, and accept with probability $P_{\\text{acc}}=\\min\\left(1,\\exp\\left(-\\frac{\\Delta V}{k_{B}T}\\right)\\right)=\\min\\left(1,\\exp(-\\Delta V)\\right)$ since $k_{B}T=1$.\n\nInitial state $i=0$: $x_{0}=1.0$, so\n$$\nV(x_{0})=1^{4}-2\\cdot 1^{2}=1-2=-1.\n$$\n\nStep $i=1$: $\\delta_{1}=0.40$. Propose $x_{\\text{new}}=1.0+0.40=1.40$.\n$$\nV(1.40)=1.40^{4}-2\\cdot 1.40^{2}=3.8416-3.92=-0.0784.\n$$\n$$\n\\Delta V=-0.0784-(-1)=0.9216,\\quad P_{\\text{acc}}=\\exp(-0.9216)\\approx 0.398.\n$$\nSince $r_{1}=0.50>0.398$, reject. Thus $x_{1}=1.0$ and $V(x_{1})=-1$.\n\nStep $i=2$: $\\delta_{2}=-0.30$. Propose $x_{\\text{new}}=1.0-0.30=0.70$.\n$$\nV(0.70)=0.70^{4}-2\\cdot 0.70^{2}=0.2401-0.98=-0.7399.\n$$\n$$\n\\Delta V=-0.7399-(-1)=0.2601,\\quad P_{\\text{acc}}=\\exp(-0.2601)\\approx 0.771.\n$$\nSince $r_{2}=0.20<0.771$, accept. Thus $x_{2}=0.70$ and $V(x_{2})=-0.7399$.\n\nStep $i=3$: $\\delta_{3}=0.10$. Propose $x_{\\text{new}}=0.70+0.10=0.80$.\n$$\nV(0.80)=0.80^{4}-2\\cdot 0.80^{2}=0.4096-1.28=-0.8704.\n$$\n$$\n\\Delta V=-0.8704-(-0.7399)=-0.1305.\n$$\nSince $\\Delta V<0$, $P_{\\text{acc}}=1$, so accept. Thus $x_{3}=0.80$ and $V(x_{3})=-0.8704$.\n\nStep $i=4$: $\\delta_{4}=-0.50$. Propose $x_{\\text{new}}=0.80-0.50=0.30$.\n$$\nV(0.30)=0.30^{4}-2\\cdot 0.30^{2}=0.0081-0.18=-0.1719.\n$$\n$$\n\\Delta V=-0.1719-(-0.8704)=0.6985,\\quad P_{\\text{acc}}=\\exp(-0.6985)\\approx 0.498.\n$$\nSince $r_{4}=0.10<0.498$, accept. Thus $x_{4}=0.30$ and $V(x_{4})=-0.1719$.\n\nThe trajectory potentials are\n$$\nV(x_{0})=-1,\\quad V(x_{1})=-1,\\quad V(x_{2})=-0.7399,\\quad V(x_{3})=-0.8704,\\quad V(x_{4})=-0.1719.\n$$\nSum:\n$$\nS=-1-1-0.7399-0.8704-0.1719=-3.7822.\n$$\nAverage over $5$ states:\n$$\n\\langle V\\rangle=\\frac{S}{5}=\\frac{-3.7822}{5}=-0.75644\\approx -0.756\\ \\text{(to three significant figures)}.\n$$", "answer": "$$\\boxed{-0.756}$$", "id": "2005962"}, {"introduction": "Having mastered the fundamental steps, we now apply the algorithm to one of its most important domains: discrete lattice models. This problem involves a small 2x2 grid of spins, a simplified version of the celebrated Ising model used to study magnetism [@problem_id:2005989]. You will simulate the system's evolution over two steps, observing how local spin flips affect the total energy and how the Metropolis criterion governs the transition toward thermal equilibrium.", "problem": "Consider a simplified model of a magnetic material represented by a 2x2 square lattice of four classical spins. The spins are located at positions $(i,j)$ where $i,j \\in \\{1, 2\\}$, and are denoted as $s_{ij}$. Each spin can only point in one of two directions, represented by the values $s_{ij} = +1$ or $s_{ij} = -1$. The total energy of the system is given by the ferromagnetic Ising Hamiltonian:\n$$H = -J \\sum_{\\langle (ij), (kl) \\rangle} s_{ij} s_{kl}$$\nwhere the sum runs over all unique pairs of nearest-neighboring spins on the lattice, and $J$ is a positive coupling constant. The system is initially in the ferromagnetic ground state, where all four spins are aligned: $s_{11} = s_{12} = s_{21} = s_{22} = +1$.\n\nThe system evolves through a Monte Carlo simulation in contact with a heat bath at a fixed temperature $T$. The simulation proceeds in discrete steps. At each step, a single spin is chosen and an attempt is made to flip it (i.e., change its sign). The change in energy, $\\Delta E$, resulting from the proposed flip is calculated.\n- If $\\Delta E \\le 0$, the flip is always accepted.\n- If $\\Delta E > 0$, the flip is accepted with a probability $P = \\exp(-\\Delta E / (k_B T))$, where $k_B$ is the Boltzmann constant.\n\nTo make the process deterministic for this problem, the probabilistic decision is handled as follows: for the $k$-th move that requires such a decision, the move is accepted if and only if the calculated probability $P$ is greater than a provided comparison value $R_k$.\n\nThe system is at a temperature such that the thermal energy is given by $k_B T = \\frac{4J}{\\ln 2}$. Two steps of the simulation are performed according to the following sequence:\n1.  An attempt is made to flip the spin $s_{11}$. The comparison value for this step, if needed, is $R_1 = 0.4$.\n2.  Starting from the configuration resulting from the first step, an attempt is made to flip the spin $s_{22}$. The comparison value for this step, if needed, is $R_2 = 0.4$.\n\nCalculate the final energy of the system, $H_{final}/J$, after these two steps. Your answer should be a single integer.", "solution": "We consider a 2x2 lattice with open boundary conditions so that the sum over unique nearest-neighbor pairs includes exactly the four bonds: $(11)-(12)$, $(11)-(21)$, $(12)-(22)$, and $(21)-(22)$. The Ising Hamiltonian is $H=-J\\sum_{\\langle (ij),(kl)\\rangle}s_{ij}s_{kl}$ with $J>0$. In the fully aligned initial state, each pair has $s_{ij}s_{kl}=+1$, hence the initial energy is\n$$\nH_{0}=-J\\,(1+1+1+1)=-4J.\n$$\n\nA single-spin flip at site $p$ changes only the bonds touching that site. The standard result for the energy change is\n$$\n\\Delta E=2J\\,s_{p}\\sum_{q\\in \\text{nn}(p)}s_{q},\n$$\nwhere the sum is over the nearest neighbors of $p$. The Metropolis acceptance probability for $\\Delta E>0$ is $P=\\exp\\!\\big(-\\Delta E/(k_{B}T)\\big)$, with the deterministic rule here: accept if and only if $P>R_{k}$.\n\nStep 1: Propose flipping $s_{11}$. Its neighbors are $s_{12}$ and $s_{21}$, both initially $+1$, and $s_{11}=+1$. Thus\n$$\n\\Delta E_{1}=2J\\,(+1)\\big[(+1)+(+1)\\big]=4J>0.\n$$\nWith $k_{B}T=\\frac{4J}{\\ln 2}$, the acceptance probability is\n$$\nP_{1}=\\exp\\!\\left(-\\frac{\\Delta E_{1}}{k_{B}T}\\right)=\\exp\\!\\left(-\\frac{4J}{4J/\\ln 2}\\right)=\\exp(-\\ln 2)=2^{-1}.\n$$\nSince $2^{-1}=0.5>R_{1}=0.4$, the flip is accepted. The new energy is\n$$\nH_{1}=H_{0}+\\Delta E_{1}=-4J+4J=0.\n$$\n\nStep 2: Starting from this configuration, propose flipping $s_{22}$. Its neighbors are $s_{12}=+1$ and $s_{21}=+1$, and $s_{22}=+1$, so\n$$\n\\Delta E_{2}=2J\\,(+1)\\big[(+1)+(+1)\\big]=4J>0.\n$$\nThe acceptance probability is the same as before,\n$$\nP_{2}=\\exp\\!\\left(-\\frac{4J}{4J/\\ln 2}\\right)=2^{-1}.\n$$\nSince $2^{-1}=0.5>R_{2}=0.4$, this flip is accepted. Therefore, the final energy is\n$$\nH_{\\text{final}}=H_{1}+\\Delta E_{2}=0+4J=4J,\n$$\nso\n$$\n\\frac{H_{\\text{final}}}{J}=4.\n$$", "answer": "$$\\boxed{4}$$", "id": "2005989"}, {"introduction": "The principles of the Metropolis algorithm form the basis of a broader class of methods known as Markov Chain Monte Carlo (MCMC), which are essential tools in fields from Bayesian statistics to machine learning. This final practice demonstrates this versatility by using a Metropolis-like MCMC method to estimate the value of $\\pi$ [@problem_id:2412845]. This exercise moves beyond physics to pure estimation and introduces you to crucial practical aspects of computational simulation, such as the \"burn-in\" period and the critical role of step size in sampling efficiency.", "problem": "You are to write a complete, runnable program that estimates the mathematical constant $\\pi$ by computing the area ratio of a unit-radius circle inscribed in a square of side length $2$ centered at the origin. The square is the set $\\mathcal{S} = \\{(x,y) \\in \\mathbb{R}^2 : -1 \\le x \\le 1,\\,-1 \\le y \\le 1\\}$, and the circle is $\\mathcal{C} = \\{(x,y) \\in \\mathbb{R}^2 : x^2 + y^2 \\le 1\\}$. The constant $\\pi$ satisfies $\\pi = 4 \\cdot \\text{Area}(\\mathcal{C}) / \\text{Area}(\\mathcal{S})$ with $\\text{Area}(\\mathcal{S}) = 4$ and $\\text{Area}(\\mathcal{C}) = \\pi$. Your program must generate a stationary sequence of points in $\\mathcal{S}$ with uniform stationary distribution using a reversible Markov chain based on symmetric Gaussian displacements of specified standard deviation. At each step, the proposed displacement is added to the current point; if the proposed point lies in $\\mathcal{S}$ it is accepted, otherwise the chain remains at the current point. Use the long-run average of the indicator function of $\\mathcal{C}$ to produce an estimator of $\\pi$.\n\nFor each independent case, the following parameters are specified: the number of samples $N$ to be used in the estimator after discarding an initial burn-in segment of length $N_{\\mathrm{burn}}$, the Gaussian displacement scale $s > 0$, the initial point $(x_0,y_0) \\in \\mathcal{S}$, and an integer random seed $\\sigma_{\\mathrm{seed}}$ used to initialize the pseudorandom number generator. The estimator must be computed as\n$$\n\\widehat{\\pi} = 4 \\cdot \\frac{1}{N} \\sum_{t=1}^{N} \\mathbf{1}\\{X_t^2 + Y_t^2 \\le 1\\},\n$$\nwhere $\\{(X_t,Y_t)\\}_{t=1}^{N}$ are the last $N$ states of the chain after discarding the first $N_{\\mathrm{burn}}$ transitions.\n\nYour program must implement exactly the following test suite, each element formatted as $(N, N_{\\mathrm{burn}}, s, x_0, y_0, \\sigma_{\\mathrm{seed}})$:\n- Case A (general case): $(N = 200000,\\, N_{\\mathrm{burn}} = 1000,\\, s = 0.5,\\, x_0 = 0.0,\\, y_0 = 0.0,\\, \\sigma_{\\mathrm{seed}} = 12345)$.\n- Case B (high correlation due to small steps): $(N = 60000,\\, N_{\\mathrm{burn}} = 1000,\\, s = 0.05,\\, x_0 = 0.9,\\, y_0 = -0.9,\\, \\sigma_{\\mathrm{seed}} = 2024)$.\n- Case C (frequent rejections due to large steps): $(N = 60000,\\, N_{\\mathrm{burn}} = 1000,\\, s = 3.0,\\, x_0 = 0.0,\\, y_0 = 0.0,\\, \\sigma_{\\mathrm{seed}} = 9876)$.\n- Case D (boundary start, short run): $(N = 10000,\\, N_{\\mathrm{burn}} = 1000,\\, s = 0.5,\\, x_0 = 1.0,\\, y_0 = 1.0,\\, \\sigma_{\\mathrm{seed}} = 4242)$.\n\nFor each case, produce the estimate $\\widehat{\\pi}$ as a real number rounded to six decimal places. There are no physical units involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\widehat{\\pi}_{\\text{A}}, \\widehat{\\pi}_{\\text{B}}, \\widehat{\\pi}_{\\text{C}}, \\widehat{\\pi}_{\\text{D}}]$.", "solution": "The problem posed is a valid exercise in computational physics, specifically in the application of Markov Chain Monte Carlo (MCMC) methods. It is scientifically sound, well-posed, and all necessary parameters are provided for a reproducible numerical experiment. We shall proceed with the solution.\n\nThe fundamental principle is to estimate the constant $\\pi$ by computing the ratio of the area of a circle to the area of a square that circumscribes it. We are given a square domain $\\mathcal{S} = \\{(x,y) \\in \\mathbb{R}^2 : -1 \\le x \\le 1,\\,-1 \\le y \\le 1\\}$ and an inscribed circle $\\mathcal{C} = \\{(x,y) \\in \\mathbb{R}^2 : x^2 + y^2 \\le 1\\}$. The area of the square is $\\text{Area}(\\mathcal{S}) = (2)^2 = 4$, and the area of the unit-radius circle is $\\text{Area}(\\mathcal{C}) = \\pi (1)^2 = \\pi$. The ratio of these areas is thus:\n$$\n\\frac{\\text{Area}(\\mathcal{C})}{\\text{Area}(\\mathcal{S})} = \\frac{\\pi}{4}\n$$\nThis implies that $\\pi = 4 \\cdot \\frac{\\text{Area}(\\mathcal{C})}{\\text{Area}(\\mathcal{S})}$.\n\nWe can estimate this area ratio using a Monte Carlo method. If we generate points $(X,Y)$ that are uniformly distributed within the square $\\mathcal{S}$, the probability that a given point also falls inside the circle $\\mathcal{C}$ is precisely this ratio of areas. Let $\\mathbf{1}_{\\mathcal{C}}(x,y)$ be the indicator function, which equals $1$ if $(x,y) \\in \\mathcal{C}$ and $0$ otherwise. By the Law of Large Numbers, for a sequence of $N$ independent and identically distributed points $\\{(X_t, Y_t)\\}_{t=1}^{N}$ from the uniform distribution on $\\mathcal{S}$, we have:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{t=1}^{N} \\mathbf{1}_{\\mathcal{C}}(X_t, Y_t) = E[\\mathbf{1}_{\\mathcal{C}}(X,Y)] = P((X,Y) \\in \\mathcal{C}) = \\frac{\\text{Area}(\\mathcal{C})}{\\text{Area}(\\mathcal{S})} = \\frac{\\pi}{4}\n$$\nThis gives rise to the estimator $\\widehat{\\pi} = 4 \\cdot \\frac{1}{N} \\sum_{t=1}^{N} \\mathbf{1}_{\\mathcal{C}}(X_t, Y_t)$.\n\nThe problem, however, specifies that the sequence of points must be generated not by independent sampling but by a particular Markov chain. This is a common and powerful technique when direct sampling from a distribution is difficult. Here, we must sample from the uniform distribution on $\\mathcal{S}$. The specified algorithm is an instance of the Metropolis-Hastings algorithm.\n\nLet the state of the chain at step $k$ be $Z_k = (X_k, Y_k) \\in \\mathcal{S}$. To generate the next state $Z_{k+1}$, we follow a two-step process:\n$1$. **Proposal:** Generate a candidate point $Z'_{k+1}$ from a proposal distribution $q(Z'|Z_k)$. The problem specifies a symmetric Gaussian displacement, so $Z'_{k+1} = Z_k + \\epsilon_k$, where $\\epsilon_k$ is a $2$-dimensional random variable drawn from a Gaussian distribution with mean $(0,0)$ and standard deviation $s$ for each component. The symmetry of the proposal means $q(Z'|Z) = q(Z|Z')$.\n\n$2$. **Acceptance/Rejection:** The candidate point $Z'_{k+1}$ is accepted with a probability $\\alpha(Z'_{k+1}|Z_k)$. For a symmetric proposal, this probability simplifies to:\n$$\n\\alpha(Z'_{k+1}|Z_k) = \\min\\left(1, \\frac{p(Z'_{k+1})}{p(Z_k)}\\right)\n$$\nwhere $p(Z)$ is the target probability density function. Our target is the uniform distribution on $\\mathcal{S}$, so $p(Z)$ is a constant for $Z \\in \\mathcal{S}$ and $0$ otherwise.\n- If $Z'_{k+1}$ is inside the square $\\mathcal{S}$, then $p(Z'_{k+1}) = p(Z_k) > 0$, so $\\frac{p(Z'_{k+1})}{p(Z_k)} = 1$, and the acceptance probability $\\alpha$ is $1$. The proposed point is always accepted.\n- If $Z'_{k+1}$ is outside the square $\\mathcal{S}$, then $p(Z'_{k+1}) = 0$, so $\\alpha = 0$. The proposed point is always rejected.\n\nIf the proposal is accepted, the next state is $Z_{k+1} = Z'_{k+1}$. If it is rejected, the chain remains at the current state, so $Z_{k+1} = Z_k$. This logic perfectly matches the problem description: \"if the proposed point lies in $\\mathcal{S}$ it is accepted, otherwise the chain remains at the current point\". This construction guarantees that the stationary distribution of the Markov chain is the uniform distribution on $\\mathcal{S}$.\n\nThe simulation proceeds as follows for each test case $(N, N_{\\mathrm{burn}}, s, x_0, y_0, \\sigma_{\\mathrm{seed}})$:\n$1$. Initialize the pseudorandom number generator with the seed $\\sigma_{\\mathrm{seed}}$.\n$2$. Set the initial state of the chain to $Z_0 = (x_0, y_0)$.\n$3$. Run the chain for $N_{\\mathrm{burn}}$ steps (the \"burn-in\" period). This is necessary to allow the chain to converge from its initial state $Z_0$ to its stationary distribution. These samples are discarded and not used in the estimation.\n$4$. Continue running the chain for another $N$ steps. Let this sequence of states be $\\{Z_t\\}_{t=1}^{N}$.\n$5$. For each state $Z_t = (X_t, Y_t)$ in this sequence, we evaluate the indicator function $\\mathbf{1}_{\\mathcal{C}}(Z_t) = \\mathbf{1}\\{X_t^2 + Y_t^2 \\le 1\\}$.\n$6$. The estimator for $\\pi$ is then computed according to the ergodic theorem for Markov chains, which is analogous to the Law of Large Numbers:\n$$\n\\widehat{\\pi} = 4 \\cdot \\frac{1}{N} \\sum_{t=1}^{N} \\mathbf{1}\\{X_t^2 + Y_t^2 \\le 1\\}\n$$\nThe provided code will implement this exact procedure for each of the specified test cases, with the final numerical results rounded to six decimal places as required. The parameter $s$ controls the step size of the proposals. A very small $s$ (Case B) will lead to high acceptance rates but slow exploration of the state space, resulting in high autocorrelation between samples. A very large $s$ (Case C) will cause most proposals to fall outside $\\mathcal{S}$, leading to a high rejection rate and the chain staying in one place for long periods. An optimal $s$ balances these two effects to ensure efficient sampling.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for estimating pi using a Markov Chain Monte Carlo method.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Format: (N, N_burn, s, x0, y0, sigma_seed)\n    test_cases = [\n        (200000, 1000, 0.5, 0.0, 0.0, 12345),   # Case A\n        (60000, 1000, 0.05, 0.9, -0.9, 2024),   # Case B\n        (60000, 1000, 3.0, 0.0, 0.0, 9876),    # Case C\n        (10000, 1000, 0.5, 1.0, 1.0, 4242)     # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        pi_estimate = run_mcmc_pi_estimation(*case)\n        results.append(pi_estimate)\n\n    # Format the results as strings rounded to six decimal places\n    # The problem asks for the number to be rounded, then converted to a string.\n    formatted_results = [str(round(res, 6)) for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_mcmc_pi_estimation(N, N_burn, s, x0, y0, sigma_seed):\n    \"\"\"\n    Estimates pi using a Markov chain on a square domain.\n\n    Args:\n        N (int): Number of samples for estimation after burn-in.\n        N_burn (int): Number of burn-in samples to discard.\n        s (float): Standard deviation of the Gaussian displacement.\n        x0 (float): Initial x-coordinate.\n        y0 (float): Initial y-coordinate.\n        sigma_seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        float: The estimate of pi.\n    \"\"\"\n    # Initialize the pseudorandom number generator for reproducibility.\n    rng = np.random.default_rng(seed=sigma_seed)\n\n    # Set the initial point of the Markov chain.\n    current_point = np.array([x0, y0])\n\n    points_in_circle_count = 0\n    total_iterations = N_burn + N\n\n    for i in range(total_iterations):\n        # 1. Propose a new point using a symmetric Gaussian displacement.\n        displacement = rng.normal(loc=0.0, scale=s, size=2)\n        proposed_point = current_point + displacement\n\n        # 2. Apply acceptance/rejection rule.\n        # Check if the proposed point is within the square S = [-1, 1] x [-1, 1].\n        if np.all(np.abs(proposed_point) <= 1.0):\n            # Accept the proposal.\n            current_point = proposed_point\n        # else: reject the proposal, the point remains the same.\n        \n        # After the burn-in period, start collecting samples for the estimation.\n        if i >= N_burn:\n            # Check if the current point is inside the unit circle C.\n            # The condition is x^2 + y^2 <= 1.\n            if np.sum(current_point**2) <= 1.0:\n                points_in_circle_count += 1\n    \n    # 3. Compute the estimator for pi.\n    # The ratio of points in the circle to total samples approximates Area(C)/Area(S) = pi/4.\n    if N == 0:\n        return 0.0\n    \n    pi_estimate = 4.0 * points_in_circle_count / N\n    \n    return pi_estimate\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2412845"}]}