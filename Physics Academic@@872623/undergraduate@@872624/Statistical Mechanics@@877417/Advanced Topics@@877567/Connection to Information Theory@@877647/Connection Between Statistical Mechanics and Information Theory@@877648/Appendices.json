{"hands_on_practices": [{"introduction": "Before we can explore the deep connections between thermodynamics and information, we must first learn how to quantify information itself. This foundational exercise [@problem_id:1956735] asks you to calculate the Shannon entropy, the fundamental measure of uncertainty, for a simple hypothetical system. By working through this problem, you will gain a concrete understanding of how information is measured in bits for a system with a discrete set of equally likely states.", "problem": "In the field of molecular computing, researchers are exploring the use of individual molecules to store information. Consider a hypothetical, specially synthesized molecule designed for this purpose. This molecule possesses exactly 10 distinct and stable quantum states, which can be used to encode data. After a standard preparation procedure, a measurement is performed on the molecule. It is found that the molecule has an equal probability of being in any one of these 10 available states. According to information theory, the uncertainty associated with the outcome of this measurement is quantified by the Shannon entropy. Calculate the Shannon entropy of this molecular system. Express your answer in bits, rounded to four significant figures.", "solution": "Shannon entropy for a discrete distribution $\\{p_{i}\\}$ measured in bits is defined as\n$$H=-\\sum_{i} p_{i}\\log_{2}(p_{i}).$$\nHere there are $10$ distinct states with equal probability, so $p_{i}=\\frac{1}{10}$ for $i=1,2,\\dots,10$. Substituting,\n$$H=-\\sum_{i=1}^{10} \\frac{1}{10}\\log_{2}\\!\\left(\\frac{1}{10}\\right)=-10\\cdot\\frac{1}{10}\\log_{2}\\!\\left(\\frac{1}{10}\\right)=-\\log_{2}\\!\\left(\\frac{1}{10}\\right)=\\log_{2}(10).$$\nUsing the change-of-base identity,\n$$\\log_{2}(10)=\\frac{\\ln(10)}{\\ln(2)}\\approx 3.32192809489\\ldots.$$\nRounding to four significant figures gives $3.322$ bits.", "answer": "$$\\boxed{3.322}$$", "id": "1956735"}, {"introduction": "Having defined information entropy, we can now investigate how it changes during a physical process. This practice [@problem_id:1956759] examines a particle in a box whose accessible volume increases, a direct analogue to the isothermal expansion of a gas. Your task is to calculate the change in this \"locational\" information entropy, revealing a logarithmic dependence on the expansion factor that directly parallels the behavior of thermodynamic entropy in a similar process.", "problem": "A theoretical model for a nanoscale memory device uses the position of a single particle to store information. The device consists of a one-dimensional array of discrete, equally-sized cells.\n\nInitially, the particle is confined within a region containing $N_1$ such cells. After a reconfiguration process, the barriers confining the particle are expanded, making a total of $N_2$ cells accessible to it. The expansion is characterized by a dimensionless factor $k > 1$, such that $N_2 = k N_1$.\n\nAssume that the particle has an equal probability of being in any of the available cells at any given time. The information entropy, $I$, of a system that can be in any of $\\Omega$ equally probable microstates is given by the formula $I = \\ln(\\Omega)$, where $\\ln$ denotes the natural logarithm.\n\nCalculate the change in the information entropy of the system, $\\Delta I = I_{final} - I_{initial}$, that occurs due to this expansion. Express your answer as a closed-form analytic expression in terms of the expansion factor $k$.", "solution": "The problem asks for the change in information entropy, $\\Delta I$, when the number of accessible cells for a particle changes from $N_1$ to $N_2$. We are given the formula for information entropy for a system with $\\Omega$ equally probable microstates as $I = \\ln(\\Omega)$.\n\nFirst, we determine the initial information entropy, $I_{initial}$. In the initial state, the particle is confined to a region with $N_1$ cells. Since the particle has an equal probability of being found in any of these cells, the number of possible microstates for the system is equal to the number of available cells.\nTherefore, the initial number of microstates is $\\Omega_{initial} = N_1$.\n\nUsing the given formula, the initial information entropy is:\n$$I_{initial} = \\ln(\\Omega_{initial}) = \\ln(N_1)$$\n\nNext, we determine the final information entropy, $I_{final}$. After the expansion, the particle can be found in any of the $N_2$ accessible cells. Again, assuming equal probability for each cell, the final number of microstates is equal to the new total number of cells.\nSo, the final number of microstates is $\\Omega_{final} = N_2$.\n\nThe problem states that $N_2 = k N_1$. Substituting this into our expression for the final number of microstates, we get:\n$$\\Omega_{final} = k N_1$$\n\nNow, we calculate the final information entropy using the given formula:\n$$I_{final} = \\ln(\\Omega_{final}) = \\ln(k N_1)$$\n\nFinally, we calculate the change in information entropy, $\\Delta I$, which is defined as the difference between the final and initial entropies:\n$$\\Delta I = I_{final} - I_{initial}$$\n\nSubstituting the expressions we found for $I_{final}$ and $I_{initial}$:\n$$\\Delta I = \\ln(k N_1) - \\ln(N_1)$$\n\nTo simplify this expression, we use the property of logarithms that states $\\ln(a) - \\ln(b) = \\ln\\left(\\frac{a}{b}\\right)$. Applying this property, we get:\n$$\\Delta I = \\ln\\left(\\frac{k N_1}{N_1}\\right)$$\n\nThe term $N_1$ cancels out in the numerator and the denominator:\n$$\\Delta I = \\ln(k)$$\n\nThus, the change in information entropy depends only on the dimensionless expansion factor $k$.", "answer": "$$\\boxed{\\ln(k)}$$", "id": "1956759"}, {"introduction": "The parallels we've seen so far suggest a profound link, but this final practice [@problem_id:1956715] moves from analogy to formal identity. Here, you will derive a rigorous mathematical relationship connecting a key thermodynamic quantity—the heat capacity at constant volume, $C_V$—to the statistical variance of the \"information content\" of the system's microstates. This powerful result demonstrates that the fluctuations in energy, which determine heat capacity, are directly equivalent to the fluctuations in information, cementing the deep unity between the two fields.", "problem": "Consider a physical system in thermal equilibrium with a large heat reservoir at a constant temperature $T$. The system can exist in a set of discrete microstates, indexed by $i$, each with a corresponding energy $E_i$. The probability $p_i$ of finding the system in microstate $i$ is given by the Boltzmann distribution:\n$$p_i = \\frac{1}{Z} \\exp\\left(-\\frac{E_i}{k_B T}\\right)$$\nwhere $k_B$ is the Boltzmann constant and $Z = \\sum_i \\exp(-E_i / (k_B T))$ is the canonical partition function.\n\nIn information theory, a measure of surprise or \"information content\" associated with observing the system in microstate $i$ is defined as $I_i = -\\ln(p_i)$.\n\nThe heat capacity of the system at constant volume, $C_V$, is defined as $C_V = \\left(\\frac{\\partial \\langle E \\rangle}{\\partial T}\\right)_V$, where $\\langle E \\rangle$ is the average energy of the system.\n\nIt can be shown that $C_V$ is directly proportional to the statistical variance of the information content, $\\text{Var}(I_i)$. That is, $C_V = \\alpha \\cdot \\text{Var}(I_i)$, where $\\text{Var}(I_i) = \\langle (I_i - \\langle I_i \\rangle)^2 \\rangle$ and $\\alpha$ is a constant of proportionality.\n\nDerive the expression for the constant $\\alpha$ in terms of fundamental constants. Your final answer should be a closed-form analytic expression.", "solution": "Let $\\beta \\equiv \\frac{1}{k_{B} T}$. The Boltzmann probability is $p_{i} = \\frac{1}{Z} \\exp(-\\beta E_{i})$, so the information content is\n$$\nI_{i} = -\\ln(p_{i}) = -\\left(-\\beta E_{i} - \\ln Z\\right) = \\beta E_{i} + \\ln Z.\n$$\nTaking the ensemble average gives\n$$\n\\langle I \\rangle = \\beta \\langle E \\rangle + \\ln Z.\n$$\nTherefore,\n$$\nI_{i} - \\langle I \\rangle = \\beta \\left(E_{i} - \\langle E \\rangle\\right),\n$$\nand hence the variance of $I_{i}$ is\n$$\n\\text{Var}(I_{i}) = \\langle (I_{i} - \\langle I \\rangle)^{2} \\rangle = \\beta^{2} \\langle (E_{i} - \\langle E \\rangle)^{2} \\rangle = \\beta^{2} \\,\\text{Var}(E).\n$$\n\nIn the canonical ensemble, the partition function is $Z = \\sum_{i} \\exp(-\\beta E_{i})$. Then\n$$\n\\frac{\\partial \\ln Z}{\\partial \\beta} = \\frac{1}{Z} \\sum_{i}(-E_{i}) \\exp(-\\beta E_{i}) = -\\langle E \\rangle,\n$$\nand differentiating once more,\n$$\n\\frac{\\partial^{2} \\ln Z}{\\partial \\beta^{2}} = -\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\langle E^{2} \\rangle - \\langle E \\rangle^{2} = \\text{Var}(E).\n$$\nRelating the $\\beta$-derivative to the temperature derivative via $T = \\frac{1}{k_{B} \\beta}$ gives\n$$\n\\frac{\\partial T}{\\partial \\beta} = -\\frac{1}{k_{B} \\beta^{2}} = -k_{B} T^{2},\n$$\nso\n$$\n\\text{Var}(E) = -\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = -\\left(\\frac{\\partial \\langle E \\rangle}{\\partial T}\\right) \\left(\\frac{\\partial T}{\\partial \\beta}\\right) = C_{V} \\, k_{B} T^{2}.\n$$\nSubstituting into the expression for $\\text{Var}(I_{i})$ yields\n$$\n\\text{Var}(I_{i}) = \\beta^{2} \\,\\text{Var}(E) = \\left(\\frac{1}{k_{B} T}\\right)^{2} \\left(k_{B} T^{2} C_{V}\\right) = \\frac{C_{V}}{k_{B}}.\n$$\nTherefore,\n$$\nC_{V} = k_{B} \\,\\text{Var}(I_{i}),\n$$\nso the constant of proportionality is\n$$\n\\alpha = k_{B}.\n$$", "answer": "$$\\boxed{k_{B}}$$", "id": "1956715"}]}