{"hands_on_practices": [{"introduction": "The core idea of entropy as missing information is most clearly seen when we can simply count possibilities. The fundamental link is given by the Boltzmann entropy $S = k_B \\ln W$, where $W$ is the number of accessible microstates; in information theory, this is often written as $S = \\ln W$, measuring entropy in 'nats'. This first exercise applies this principle to a tangible scenario from cryptography, allowing you to quantify uncertainty by counting the number of possible anagrams for a passphrase and then calculating the information gained when new clues become available. [@problem_id:1963591]", "problem": "In cryptography, the uncertainty associated with a secret message can be quantified by information entropy. Imagine a scenario where a passphrase is known to be a 20-character string formed by an anagram of the letters in \"STATISTICALMECHANICS\" (ignoring any spaces). All possible distinct anagrams are considered equally likely to be the correct passphrase.\n\nThe information entropy $S$ for a system with $W$ equally probable configurations (or microstates) is given by the formula $S = \\ln(W)$, where $\\ln$ is the natural logarithm. The unit of entropy in this context is the \"nat\".\n\nSuppose an intelligence analyst makes a breakthrough and discovers that the first three characters of the passphrase are \"SSS\". Calculate the information gain resulting from this discovery. The information gain is defined as the reduction in the information entropy of the set of possible passphrases.\n\nExpress your answer as a single closed-form analytic expression in terms of the natural logarithm.", "solution": "The multiset of letters in the string \"STATISTICALMECHANICS\" (ignoring spaces) has length 20 with frequencies:\n- $S,T,A,I,C$ each appearing $3$ times,\n- $L,M,E,H,N$ each appearing $1$ time.\n\nThus the total number of distinct anagrams is\n$$\nW_{0}=\\frac{20!}{(3!)^{5}}.\n$$\nFor equally likely configurations, the information entropy is $S=\\ln(W)$, so the initial entropy is\n$$\nS_{0}=\\ln\\!\\left(\\frac{20!}{(3!)^{5}}\\right).\n$$\n\nGiven the discovery that the first three characters are $SSS$, all $3$ occurrences of $S$ are fixed in the first three positions. The remaining $17$ positions must arrange the multiset with counts $T^{3},A^{3},I^{3},C^{3}$ and $L,M,E,H,N$ each once. The number of compatible anagrams is\n$$\nW_{1}=\\frac{17!}{(3!)^{4}},\n$$\nso the posterior entropy is\n$$\nS_{1}=\\ln\\!\\left(\\frac{17!}{(3!)^{4}}\\right).\n$$\n\nThe information gain is the reduction in entropy:\n$$\n\\Delta S=S_{0}-S_{1}\n=\\ln\\!\\left(\\frac{20!}{(3!)^{5}}\\right)-\\ln\\!\\left(\\frac{17!}{(3!)^{4}}\\right)\n=\\ln\\!\\left(\\frac{20!}{3!\\,17!}\\right)\n=\\ln\\!\\left(\\binom{20}{3}\\right).\n$$", "answer": "$$\\boxed{\\ln\\!\\left(\\binom{20}{3}\\right)}$$", "id": "1963591"}, {"introduction": "Having established the principle of counting states, we now apply it to a physical system in the quantum realm. Here, the 'microstates' are the distinct quantum configurations of particles distributed among discrete energy levels, and the number of these states is denoted by $\\Omega$. This problem asks you to find the entropy of a system of identical bosons using the formula $S = k_B \\ln \\Omega$, given a fixed total energy. You will need to determine the number of ways the particles can be arranged to meet this energy constraint, illustrating how fundamental principles like energy conservation and quantum statistics govern a system's entropy. [@problem_id:1963626]", "problem": "A simplified model for a trapped quantum system consists of four distinct, non-degenerate single-particle energy levels: $E_1 = \\epsilon$, $E_2 = 2\\epsilon$, $E_3 = 3\\epsilon$, and $E_4 = 4\\epsilon$, where $\\epsilon$ is a positive energy constant. Three identical, non-interacting bosons are placed in this system. The only information available about the state of the system is that its total energy is exactly $E_{total} = 6\\epsilon$. Based on this information, determine the statistical entropy of the system. Express your answer as an analytical expression in terms of the Boltzmann constant, $k_B$.", "solution": "For three identical, non-interacting bosons in non-degenerate single-particle levels, microstates are uniquely specified by occupation numbers. Let $n_{i}$ be the number of bosons in level $E_{i}=i\\epsilon$ for $i=1,2,3,4$. The constraints are\n$$\nn_{1}+n_{2}+n_{3}+n_{4}=3,\\qquad \\epsilon\\left(1\\cdot n_{1}+2\\cdot n_{2}+3\\cdot n_{3}+4\\cdot n_{4}\\right)=6\\epsilon.\n$$\nDividing the energy constraint by $\\epsilon$ and subtracting the particle number constraint gives\n$$\nn_{2}+2n_{3}+3n_{4}=3,\n$$\nwith $n_{i}\\in\\{0,1,2,\\dots\\}$. Enumerating solutions:\n\n- If $n_{4}=0$, then $n_{2}+2n_{3}=3$, which yields $(n_{2},n_{3})=(3,0)$ and $(1,1)$. The corresponding $(n_{1},n_{2},n_{3},n_{4})$ are $(0,3,0,0)$ and $(1,1,1,0)$.\n- If $n_{4}=1$, then $n_{2}+2n_{3}=0$, giving $(n_{2},n_{3})=(0,0)$ and hence $(n_{1},n_{2},n_{3},n_{4})=(2,0,0,1)$.\n\nNo other $n_{4}$ are allowed since $3n_{4}\\leq 3$. Therefore, the number of distinct bosonic Fock states (microstates) consistent with the total energy is\n$$\n\\Omega=3.\n$$\nIn the microcanonical ensemble, the statistical entropy is\n$$\nS=k_{B}\\ln\\Omega=k_{B}\\ln 3.\n$$", "answer": "$$\\boxed{k_B \\ln 3}$$", "id": "1963626"}, {"introduction": "Our previous examples dealt with situations where all accessible microstates were equally likely. However, many real-world processes are probabilistic, leading to outcomes with different probabilities, $p_i$. In such cases, we must use the more general Gibbs entropy formula, $S = -k_B \\sum_i p_i \\ln p_i$. This final exercise challenges you to calculate the entropy for an ensemble of biopolymer chains where the sequence of monomers is determined by probabilistic rules, making it a perfect application of this more powerful formula. [@problem_id:1963635]", "problem": "A biopolymer chain is constructed monomer by monomer from a solution containing two types of monomers, A and B. The assembly process follows a set of simple, local rules. If the last monomer added to the chain was type A, the next monomer must be of type B. If the last monomer added was type B, the next monomer is chosen to be type A or type B with equal probability.\n\nA specific chain of length 5 is synthesized in a reaction vessel. It is known that the first monomer in this chain is of type B. Given this starting condition and the probabilistic rules of assembly, a specific set of possible 5-monomer sequences can be generated, each with a certain probability.\n\nLet the Boltzmann constant be denoted by $k_B$. Calculate the statistical entropy (also known as the Gibbs entropy) associated with the ensemble of all possible 5-monomer sequences that can be formed under these conditions.\n\nExpress your answer as a single closed-form analytic expression in terms of $k_B$ and mathematical constants.", "solution": "We model the assembly as a Markov process on monomer types with the stated rules: from $A$ the next monomer is deterministically $B$, and from $B$ the next monomer is $A$ or $B$ with probability $\\frac{1}{2}$ each. The Gibbs entropy of the ensemble of possible sequences is $S=-k_{B}\\sum_{i} p_{i}\\ln p_{i}$, where $p_{i}$ are the sequence probabilities.\n\nStarting with $s_{1}=B$, we enumerate all length-$5$ sequences and their probabilities by multiplying factors of $\\frac{1}{2}$ for each step that follows a $B$ (since $B\\to A$ and $B\\to B$ each have probability $\\frac{1}{2}$) and a factor of $1$ for each step that follows an $A$ (since $A\\to B$ is deterministic):\n- $B A B A B$ with probability $\\frac{1}{4}$,\n- $B A B B A$ with probability $\\frac{1}{8}$,\n- $B A B B B$ with probability $\\frac{1}{8}$,\n- $B B A B A$ with probability $\\frac{1}{8}$,\n- $B B A B B$ with probability $\\frac{1}{8}$,\n- $B B B A B$ with probability $\\frac{1}{8}$,\n- $B B B B A$ with probability $\\frac{1}{16}$,\n- $B B B B B$ with probability $\\frac{1}{16}$.\n\nThese probabilities sum to $1$, as required. The Gibbs entropy is then\n$$\nS=-k_{B}\\left[\\frac{1}{4}\\ln\\!\\left(\\frac{1}{4}\\right)+5\\cdot\\frac{1}{8}\\ln\\!\\left(\\frac{1}{8}\\right)+2\\cdot\\frac{1}{16}\\ln\\!\\left(\\frac{1}{16}\\right)\\right].\n$$\nUsing $\\ln(1/4)=-\\ln 4$, $\\ln(1/8)=-\\ln 8$, and $\\ln(1/16)=-\\ln 16$, this becomes\n$$\nS=k_{B}\\left[\\frac{1}{4}\\ln 4+\\frac{5}{8}\\ln 8+\\frac{1}{8}\\ln 16\\right]\n= k_{B}\\left[\\frac{1}{4}\\cdot 2\\ln 2+\\frac{5}{8}\\cdot 3\\ln 2+\\frac{1}{8}\\cdot 4\\ln 2\\right]\n= k_{B}\\left[\\frac{1}{2}+\\frac{15}{8}+\\frac{1}{2}\\right]\\ln 2\n= \\frac{23}{8}\\,k_{B}\\ln 2.\n$$\n\nAs a consistency check, the total entropy can be found by summing the conditional entropy added at each step. An entropy of $k_B \\ln 2$ is generated only when a 'B' monomer is followed by a probabilistic choice. The total entropy is therefore $S = k_B \\ln 2 \\sum_{t=1}^4 P(s_t=\\text{'B'})$, where $s_t$ is the monomer at position $t$. Let $q_t = P(s_t=\\text{'B'})$. We are given $q_1=1$, and the recurrence relation is $q_t = P(s_t='B'|s_{t-1}='A')P(s_{t-1}='A') + P(s_t='B'|s_{t-1}='B')P(s_{t-1}='B') = 1 \\cdot (1-q_{t-1}) + \\frac{1}{2}q_{t-1} = 1-\\frac{1}{2}q_{t-1}$ for $t>1$. We obtain the probabilities for the first four positions: $q_1=1$, $q_2=\\frac{1}{2}$, $q_3=\\frac{3}{4}$, $q_4=\\frac{5}{8}$. Their sum is $1+\\frac{1}{2}+\\frac{3}{4}+\\frac{5}{8} = \\frac{23}{8}$. This yields $S = \\frac{23}{8}k_B \\ln 2$, confirming the result.", "answer": "$$\\boxed{\\frac{23}{8} k_B \\ln 2}$$", "id": "1963635"}]}