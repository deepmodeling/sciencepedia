{"hands_on_practices": [{"introduction": "This first practice provides a striking visual demonstration of the Law of Large Numbers in action. We will explore a Monte Carlo method, a computational technique that relies on repeated random sampling to obtain numerical results. By simulating random points in a square and observing the proportion that fall within an inscribed circle, you will see how the LLN allows us to approximate the deterministic constant $\\pi$ from a purely probabilistic experiment [@problem_id:1406798].", "problem": "A computer simulation generates a sequence of independent random points, $P_1, P_2, P_3, \\dots$, in a two-dimensional Cartesian plane. Each point $P_i$ has coordinates $(X_i, Y_i)$, where $X_i$ and $Y_i$ are independent random variables drawn from a uniform distribution on the interval $[0, 1]$. We define a region $\\mathcal{C}$ to be the set of all points $(x, y)$ that satisfy the inequality $(x - 0.5)^2 + (y - 0.5)^2 \\le 0.25$.\n\nLet $S_n$ be the number of points among the first $n$ points ($P_1, \\dots, P_n$) that fall inside or on the boundary of the region $\\mathcal{C}$. Consider the random variable $A_n = 4 \\frac{S_n}{n}$.\n\nDetermine the value to which the sequence of random variables $A_n$ converges almost surely as $n \\to \\infty$. Express your answer as a single closed-form analytic expression.", "solution": "Let $I_{i}$ be the indicator of the event that $P_{i}$ falls in $\\mathcal{C}$, i.e., $I_{i}=1$ if $(X_{i},Y_{i})\\in\\mathcal{C}$ and $I_{i}=0$ otherwise. Then $S_{n}=\\sum_{i=1}^{n} I_{i}$ and $A_{n}=4\\frac{S_{n}}{n}=4\\left(\\frac{1}{n}\\sum_{i=1}^{n} I_{i}\\right)$.\n\nBecause $(X_{i},Y_{i})$ are i.i.d. uniformly distributed on $[0,1]^{2}$, the $I_{i}$ are i.i.d. Bernoulli with parameter $p=\\mathbb{P}\\big((X_{1},Y_{1})\\in\\mathcal{C}\\big)$. Since $\\mathcal{C}$ is the disk centered at $(\\tfrac{1}{2},\\tfrac{1}{2})$ of radius $r$ determined by $(x-\\tfrac{1}{2})^{2}+(y-\\tfrac{1}{2})^{2}\\le \\tfrac{1}{4}$, we have $r^{2}=\\tfrac{1}{4}$ and hence $r=\\tfrac{1}{2}$. This disk lies entirely within the unit square $[0,1]^{2}$, so\n$$\np=\\frac{\\text{area}(\\mathcal{C})}{\\text{area}([0,1]^{2})}=\\pi r^{2}=\\pi\\left(\\frac{1}{2}\\right)^{2}=\\frac{\\pi}{4}.\n$$\n\nBy the strong law of large numbers, since the $I_{i}$ are i.i.d. with finite mean, we have\n$$\n\\frac{S_{n}}{n}=\\frac{1}{n}\\sum_{i=1}^{n} I_{i}\\to \\mathbb{E}[I_{1}]=p=\\frac{\\pi}{4}\\quad\\text{almost surely}.\n$$\nMultiplying by $4$, it follows that\n$$\nA_{n}=4\\frac{S_{n}}{n}\\to 4\\cdot\\frac{\\pi}{4}=\\pi\\quad\\text{almost surely}.\n$$\nTherefore, the sequence $A_{n}$ converges almost surely to $\\pi$ as $n\\to\\infty$.", "answer": "$$\\boxed{\\pi}$$", "id": "1406798"}, {"introduction": "Many phenomena in science and engineering involve waiting for a specific event to occur, from radioactive decay to component failure. This exercise models such a scenario using the Geometric distribution, which describes the number of trials needed for the first success. By applying the Strong Law of Large Numbers, you will determine the long-term average number of trials required, reinforcing the fundamental principle that the sample mean of repeated experiments converges to the theoretical expected value of the underlying process [@problem_id:1957093].", "problem": "Let $X_1, X_2, \\dots$ be a sequence of independent and identically distributed (i.i.d.) random variables. Each $X_i$ represents the number of trials needed to get the first success in a sequence of Bernoulli trials, where the probability of success on any given trial is a constant $p$, with $0 < p < 1$. This implies that each $X_i$ follows a Geometric distribution with a probability mass function given by $P(X_i = k) = (1-p)^{k-1}p$ for $k \\in \\{1, 2, 3, \\dots\\}$.\n\nConsider the sample mean of the first $n$ variables, defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. It is a known result in probability theory that as the number of observations $n$ tends to infinity, the sample mean $\\bar{X}_n$ converges almost surely to a specific constant value.\n\nYour task is to determine this constant value. Express your answer as a closed-form analytic expression in terms of the parameter $p$.", "solution": "We are given i.i.d. random variables $X_{1},X_{2},\\dots$ with the geometric distribution on $\\{1,2,\\dots\\}$ with parameter $p\\in(0,1)$, that is $P(X_{i}=k)=(1-p)^{k-1}p$ for $k\\geq 1$. We are to find the almost sure limit of the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ as $n\\to\\infty$.\n\nBy the Strong Law of Large Numbers, if $\\{X_{i}\\}$ are i.i.d. with $\\mathbb{E}[|X_{1}|]<\\infty$, then\n$$\n\\bar{X}_{n}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X_{1}].\n$$\nThus it suffices to compute $\\mathbb{E}[X_{1}]$ and verify it is finite.\n\nCompute the expectation using the series:\n$$\n\\mathbb{E}[X_{1}]=\\sum_{k=1}^{\\infty}k\\,P(X_{1}=k)=\\sum_{k=1}^{\\infty}k\\,(1-p)^{k-1}p.\n$$\nLet $r=1-p$. Since $0<p<1$, we have $0<r<1$. Then\n$$\n\\mathbb{E}[X_{1}]=p\\sum_{k=1}^{\\infty}k\\,r^{k-1}.\n$$\nUse the known identity for $|r|<1$:\n$$\n\\sum_{k=1}^{\\infty}k\\,r^{k-1}=\\frac{1}{(1-r)^{2}},\n$$\nwhich follows by differentiating the geometric series $\\sum_{k=0}^{\\infty}r^{k}=\\frac{1}{1-r}$ with respect to $r$. Therefore,\n$$\n\\mathbb{E}[X_{1}]=p\\cdot\\frac{1}{(1-r)^{2}}=p\\cdot\\frac{1}{(1-(1-p))^{2}}=p\\cdot\\frac{1}{p^{2}}=\\frac{1}{p}.\n$$\nSince $X_{1}\\geq 0$, we have $\\mathbb{E}[|X_{1}|]=\\mathbb{E}[X_{1}]=\\frac{1}{p}<\\infty$. Hence by the Strong Law of Large Numbers,\n$$\n\\bar{X}_{n}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X_{1}]=\\frac{1}{p}.\n$$\nTherefore, the almost sure limit of the sample mean is the constant $\\frac{1}{p}$.", "answer": "$$\\boxed{\\frac{1}{p}}$$", "id": "1957093"}, {"introduction": "The power of the Law of Large Numbers extends beyond simple averages. This final practice explores how we can analyze the long-term behavior of functions of random variables, a concept crucial in fields like signal processing where average power (related to the square of the signal) is a key metric. You will apply the LLN to a sequence of squared measurements and use the fundamental relationship between mean, variance, and the second moment to find the value to which the average power converges [@problem_id:1957103].", "problem": "In a digital signal processing system, a sequence of measurements $X_1, X_2, \\ldots, X_n, \\ldots$ are taken from a noisy source. These measurements are modeled as a sequence of Independent and Identically Distributed (i.i.d.) random variables. Each random variable $X_i$ has a finite expected value (mean) $E[X_i] = \\mu$ and a finite, non-zero variance $\\text{Var}(X_i) = \\sigma^2$.\n\nA key performance metric for this system is the long-term average signal power, which is mathematically represented by the sample mean of the squared measurements. Determine the value to which the quantity $S_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2$ converges with probability 1 as the number of measurements $n$ approaches infinity. Express your answer in terms of $\\mu$ and $\\sigma$.", "solution": "The problem asks for the almost sure limit of the quantity $S_n = \\frac{1}{n} \\sum_{i=1}^n X_i^2$ as $n \\to \\infty$. This is a direct application of the Strong Law of Large Numbers (SLLN).\n\nLet us define a new sequence of random variables $Y_i = X_i^2$. The expression for $S_n$ can then be rewritten as the sample mean of this new sequence:\n$$ S_n = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}_n $$\n\nThe Strong Law of Large Numbers states that for a sequence of Independent and Identically Distributed (i.i.d.) random variables, say $Y_1, Y_2, \\ldots$, with a finite expected value $E[Y_i] = \\nu$, their sample mean $\\bar{Y}_n$ converges almost surely (i.e., with probability 1) to this expected value. Mathematically,\n$$ \\bar{Y}_n \\xrightarrow{\\text{a.s.}} \\nu \\quad \\text{as } n \\to \\infty $$\n\nTo apply the SLLN to our problem, we must verify that the sequence $Y_i = X_i^2$ meets the necessary conditions:\n1.  **I.i.d. condition**: The problem states that the random variables $X_1, X_2, \\ldots$ are i.i.d. Since each $Y_i$ is derived from the corresponding $X_i$ using the same function (the square function, $g(x) = x^2$), the sequence of random variables $Y_1, Y_2, \\ldots$ is also i.i.d.\n2.  **Finite mean condition**: We need to calculate the expected value of $Y_i$ and check if it is finite.\n    $$ E[Y_i] = E[X_i^2] $$\n    We can find $E[X_i^2]$ using the definition of variance for any random variable $X$:\n    $$ \\text{Var}(X) = E[X^2] - (E[X])^2 $$\n    The problem provides the mean and variance for each $X_i$:\n    $$ E[X_i] = \\mu $$\n    $$ \\text{Var}(X_i) = \\sigma^2 $$\n    Substituting these values into the variance formula:\n    $$ \\sigma^2 = E[X_i^2] - \\mu^2 $$\n    Solving for $E[X_i^2]$, we get:\n    $$ E[X_i^2] = \\mu^2 + \\sigma^2 $$\n    Since both $\\mu$ and $\\sigma^2$ are given as finite constants, their sum $\\mu^2 + \\sigma^2$ is also a finite value. Thus, the expected value of $Y_i$ is finite, and is equal to $\\mu^2 + \\sigma^2$.\n\nSince both conditions of the SLLN are satisfied for the sequence $Y_i$, we can conclude that the sample mean $\\bar{Y}_n$ converges almost surely to the expected value $E[Y_i]$.\n$$ \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n Y_i \\xrightarrow{\\text{a.s.}} E[Y_i] $$\nSubstituting $Y_i = X_i^2$ and $E[Y_i] = \\mu^2 + \\sigma^2$:\n$$ \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\xrightarrow{\\text{a.s.}} \\mu^2 + \\sigma^2 $$\nThe phrase \"converges with probability 1\" is synonymous with \"converges almost surely\". Therefore, the long-term average signal power converges to $\\mu^2 + \\sigma^2$.", "answer": "$$\\boxed{\\mu^{2} + \\sigma^{2}}$$", "id": "1957103"}]}