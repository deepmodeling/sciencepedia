{"hands_on_practices": [{"introduction": "After building a statistical model, the first essential step is to evaluate its performance. This practice introduces the Root Mean Square Error of Calibration (RMSEC), a fundamental figure of merit that quantifies the average error between your model's predictions and the known values of the calibration set. By working through this calculation, you will gain a core skill for assessing how well your Partial Least Squares (PLS) model fits the data it was trained on [@problem_id:1459311].", "problem": "An analytical chemist is developing a rapid quality control method using spectroscopy to measure the concentration of a key active ingredient in a pharmaceutical formulation. A calibration model is built using Partial Least Squares (PLS) regression, a multivariate statistical method that relates spectral data to the concentration of the analyte. To assess the performance of this calibration model, five standard samples with accurately known concentrations are measured. The PLS model then predicts the concentration for each of these same five samples. The actual (reference) and PLS-predicted concentrations are provided below.\n\nActual Concentrations (mg/mL):\n1.25, 1.40, 1.52, 1.68, 1.81\n\nPredicted Concentrations (mg/mL):\n1.22, 1.43, 1.55, 1.65, 1.83\n\nCalculate the Root Mean Square Error of Calibration (RMSEC) for this model. Express your answer in mg/mL and round your final answer to three significant figures.", "solution": "The Root Mean Square Error of Calibration (RMSEC) is a metric used to evaluate the performance of a calibration model by quantifying the average difference between the known (actual) values and the values predicted by the model for the calibration set.\n\nThe formula for RMSEC is:\n$$\n\\text{RMSEC} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_{i, \\text{actual}} - y_{i, \\text{predicted}})^{2}}{n}}\n$$\nwhere $y_{i, \\text{actual}}$ is the actual concentration of the $i$-th sample, $y_{i, \\text{predicted}}$ is the concentration predicted by the model for the $i$-th sample, and $n$ is the total number of samples in the calibration set.\n\nIn this problem, we are given $n=5$ samples. Let's calculate the squared difference for each sample.\n\nStep 1: Calculate the difference ($y_{i, \\text{actual}} - y_{i, \\text{predicted}}$) for each sample.\n- Sample 1: $1.25 - 1.22 = 0.03$ mg/mL\n- Sample 2: $1.40 - 1.43 = -0.03$ mg/mL\n- Sample 3: $1.52 - 1.55 = -0.03$ mg/mL\n- Sample 4: $1.68 - 1.65 = 0.03$ mg/mL\n- Sample 5: $1.81 - 1.83 = -0.02$ mg/mL\n\nStep 2: Square each of these differences.\n- Sample 1: $(0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 2: $(-0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 3: $(-0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 4: $(0.03)^{2} = 0.0009$ $(\\text{mg/mL})^2$\n- Sample 5: $(-0.02)^{2} = 0.0004$ $(\\text{mg/mL})^2$\n\nStep 3: Sum the squared differences (Sum of Squared Errors, SSE).\n$$\n\\sum_{i=1}^{5} (y_{i, \\text{actual}} - y_{i, \\text{predicted}})^{2} = 0.0009 + 0.0009 + 0.0009 + 0.0009 + 0.0004 = 0.0040 \\ (\\text{mg/mL})^2\n$$\n\nStep 4: Divide the sum by the number of samples, $n=5$, to find the Mean Squared Error (MSE).\n$$\n\\text{MSE} = \\frac{0.0040}{5} = 0.0008 \\ (\\text{mg/mL})^2\n$$\n\nStep 5: Take the square root of the MSE to find the RMSEC.\n$$\n\\text{RMSEC} = \\sqrt{0.0008} \\approx 0.02828427... \\ \\text{mg/mL}\n$$\n\nStep 6: Round the final answer to three significant figures as requested.\nThe first significant figure is 2, the second is 8, and the third is 2. The next digit is 8, which is 5 or greater, so we round up the last significant figure.\n$$\n\\text{RMSEC} \\approx 0.0283 \\ \\text{mg/mL}\n$$", "answer": "$$\\boxed{0.0283}$$", "id": "1459311"}, {"introduction": "A high error value tells us a model is performing poorly, but it doesn't tell us why. This next exercise delves into model diagnostics, focusing on the crucial trade-off between model simplicity and complexity. You will learn to diagnose a model's failure as either underfitting or overfitting by comparing its performance on training data versus new, independent data, a foundational concept in developing a truly predictive and robust PLS model [@problem_id:1459317].", "problem": "An analytical chemist is developing a quantitative model using Partial Least Squares (PLS) regression to determine the concentration of an active pharmaceutical ingredient (API) in a liquid formulation. The model uses near-infrared (NIR) spectra as the predictor variables ($X$) and the known API concentration as the response variable ($Y$).\n\nThe chemist builds an initial PLS model using only one latent variable. To evaluate its performance, they calculate the Root Mean Square Error of Calibration (RMSEC) on the training dataset and the Root Mean Square Error of Prediction (RMSEP) on an independent validation dataset. The chemist observes that both the RMSEC and the RMSEP are unacceptably high, and their values are very close to each other.\n\nBased on this observation, which of the following statements provides the most likely diagnosis for the model's poor performance?\n\nA. The model is overfitting the training data, as it is capturing random noise instead of the systematic relationship between the spectra and concentration.\n\nB. The model is underfitting the data, as a single latent variable is insufficient to capture the complex, concentration-relevant variance within the spectral data.\n\nC. The concentrations in the validation set fall outside the range of concentrations used in the calibration set, making accurate prediction impossible.\n\nD. The NIR spectra are dominated by random instrumental noise that is much larger than the signal related to the API, so no meaningful model can be built.", "solution": "The goal of this problem is to diagnose the performance of a Partial Least Squares (PLS) regression model based on the error metrics from the calibration and validation datasets.\n\nFirst, let's understand the key concepts. PLS is a multivariate regression technique that models the relationship between a set of predictor variables ($X$, the spectra) and one or more response variables ($Y$, the concentration). It does so by constructing a set of orthogonal latent variables (LVs) that are linear combinations of the original predictor variables. These LVs are chosen to maximize the covariance between $X$ and $Y$.\n\nThe number of latent variables used in a PLS model determines its complexity.\n- A model with too few latent variables is a simple model. It may not be complex enough to capture all the systematic variation in the data that is relevant for prediction. This condition is known as **underfitting**. An underfit model performs poorly on the data it was trained on and also performs poorly on new, unseen data. A key characteristic of underfitting is that the error on the training set (e.g., Root Mean Square Error of Calibration, RMSEC) is high, and the error on an independent test set (e.g., Root Mean Square Error of Prediction, RMSEP) is also high and has a similar magnitude to the training error (RMSEC ≈ RMSEP).\n\n- A model with too many latent variables is an overly complex model. It begins to fit not only the systematic, meaningful variation in the training data but also the random noise. This condition is known as **overfitting**. An overfit model performs exceptionally well on the training data (very low RMSEC) but performs poorly on new, unseen data because the noise it learned in the training set does not exist in the new data. A key characteristic of overfitting is a very low RMSEC and a significantly higher RMSEP.\n\nIn the scenario described, the chemist builds a PLS model with only one latent variable. They find that both the RMSEC and the RMSEP are high and numerically close to each other. This pattern—high error on both training and validation sets—is the classic signature of an underfitting model. The model is too simple to adequately describe the relationship between the NIR spectra and the API concentration. A single latent variable is not sufficient to capture the necessary information.\n\nLet's evaluate the given options:\n\nA. The model is overfitting. This is incorrect. Overfitting would be characterized by a very low RMSEC and a high RMSEP, which contradicts the observation that RMSEC and RMSEP are both high and similar. Overfitting typically occurs when *too many* latent variables are used, not too few.\n\nB. The model is underfitting. This is correct. The observation of high and similar RMSEC and RMSEP is the definitive sign of an underfit model. A single latent variable is likely not enough to capture the spectral features correlated with the API concentration in a complex formulation.\n\nC. The validation set concentrations are outside the calibration range. While this would certainly lead to a high RMSEP (an extrapolation problem), it does not explain why the RMSEC on the calibration set is also unacceptably high. The high RMSEC indicates the model is failing to describe the training data itself, which points to a fundamental model deficiency (underfitting) rather than just an issue with the validation set.\n\nD. The data is dominated by noise, so no model is possible. While high noise can degrade model performance, the diagnostic information given (RMSEC ≈ RMSEP and both are high) points specifically to underfitting as the primary issue with *this particular model construction* (using only one LV). It's likely that a more complex model (with more LVs) could extract the signal from the noise and perform better. This option suggests the data is useless, which is a more extreme conclusion than what is warranted by the evidence. The most direct diagnosis is that the model's complexity is too low.\n\nTherefore, the most accurate diagnosis is that the model is underfitting the data.", "answer": "$$\\boxed{B}$$", "id": "1459317"}, {"introduction": "Beyond evaluating the overall model, it is critical to assess each new sample we analyze. This practice explores the powerful diagnostic tools of Hotelling's $T^2$ and Q-residuals, which allow us to scrutinize individual predictions for abnormalities. Mastering these concepts will enable you to distinguish between a valid but extreme sample and a sample containing unmodeled chemical or physical properties, ensuring your model is applied reliably in real-world scenarios [@problem_id:1459316].", "problem": "An analytical chemist is developing a multivariate calibration model using Partial Least Squares (PLS) regression. The goal is to predict the concentration of an Active Pharmaceutical Ingredient (API) in pharmaceutical tablets using data from a Near-Infrared (NIR) spectrometer. The model is built using a set of calibration standards with known API concentrations ranging from 1.0 mg/g to 5.0 mg/g.\n\nAfter building the PLS model, the chemist uses it to analyze two new, unknown production samples, labeled Sample X and Sample Y. The diagnostic statistics for these samples are calculated and compared against the 95% confidence limits established by the model:\n\n- **Sample X**: Has a Hotelling's $T^2$ value that is significantly *above* the confidence limit, but its Q-residual value is well *below* the confidence limit.\n- **Sample Y**: Has a Hotelling's $T^2$ value that is well *below* the confidence limit, but its Q-residual value is significantly *above* the confidence limit.\n\nBased on these diagnostic results, which of the following statements provides the most chemically and statistically sound interpretation of Sample X and Sample Y?\n\nA. Sample X is likely a valid sample with an API concentration that is outside the model's calibration range (e.g., significantly higher than 5.0 mg/g), while Sample Y likely contains an unmodeled chemical interferent or its spectrum was affected by a physical difference not present in the calibration set (e.g., different particle size).\n\nB. Sample Y is likely a valid sample with an API concentration that is outside the model's calibration range, while Sample X likely contains an unmodeled chemical interferent or was measured under faulty instrument conditions.\n\nC. The high $T^2$ value for Sample X indicates that it is an extreme sample that validates the model's predictive power at the limits, while the high Q-residual for Sample Y indicates that its API concentration is very close to the average of the calibration set.\n\nD. Both samples are considered outliers and must be discarded. The high $T^2$ for Sample X indicates excessive random noise in its measurement, and the high Q-residual for Sample Y indicates a systematic error in the instrument's wavelength alignment during its measurement.\n\nE. The low Q-residual for Sample X indicates its API concentration is zero, while the high Q-residual for Sample Y indicates its API concentration is exactly the average of the calibration set.", "solution": "We model the NIR spectra in the PLS latent space. For a calibration model with $A$ latent variables, the $X$-block (spectra) is modeled as\n$$\nX \\approx T P^{\\top}, \n$$\nwith scores matrix $T \\in \\mathbb{R}^{n \\times A}$ and loadings matrix $P \\in \\mathbb{R}^{p \\times A}$. For a new sample spectrum $x \\in \\mathbb{R}^{p}$, its score vector $t \\in \\mathbb{R}^{A}$ under the trained model can be computed as\n$$\nt = x W \\left(P^{\\top} W\\right)^{-1},\n$$\nThe reconstruction of $x$ from the model subspace is\n$$\n\\hat{x} = t P^{\\top},\n$$\nand the residual (orthogonal to the model subspace) is\n$$\ne = x - \\hat{x}.\n$$\nTwo standard diagnostics are then defined. The Hotelling statistic in the modeled subspace is\n$$\nT^{2} = t^{\\top} S_{T}^{-1} t,\n$$\nwhere $S_{T} \\in \\mathbb{R}^{A \\times A}$ is the covariance matrix of the calibration scores. A large $T^{2}$ indicates that $t$ is extreme relative to the calibration score distribution (high leverage) but still within the modeled subspace. The orthogonal residual measure (Q-residual or squared prediction error) is\n$$\nQ = e^{\\top} e = \\left\\|x - \\hat{x}\\right\\|^{2},\n$$\nwhich quantifies variance in $x$ not captured by the model subspace. Confidence limits $T^{2}_{\\alpha}$ and $Q_{\\alpha}$ at confidence level $\\alpha$ (here $\\alpha=0.95$) are determined from the calibration set.\n\nInterpretation proceeds from these definitions:\n1) Sample X has $T^{2} > T^{2}_{0.95}$ and $Q < Q_{0.95}$. The condition $Q < Q_{0.95}$ implies that $e^{\\top} e$ is small, so $x$ is well represented by the modeled subspace and does not contain significant unmodeled spectral structure. The condition $T^{2} > T^{2}_{0.95}$ implies that $t$ is an extreme point in the latent space spanned by the model. In PLS built to predict API concentration, the dominant latent variables capture covariance between spectra and API concentration. Therefore, being extreme in scores while remaining well modeled is most consistent with extrapolation along the modeled concentration direction, i.e., a valid spectrum whose API concentration lies near or beyond the edge of the calibration range (for example, greater than the upper bound of 5.0 mg/g).\n\n2) Sample Y has $T^{2} < T^{2}_{0.95}$ and $Q > Q_{0.95}$. The condition $T^{2} < T^{2}_{0.95}$ implies that, in the modeled subspace, $t$ is not extreme; it lies near the center of the calibration score distribution and thus is not indicative of an unusually high or low API concentration along the modeled directions. The condition $Q > Q_{0.95}$ implies substantial residual variance orthogonal to the model subspace, i.e., spectral features not captured by the calibration model. Chemically and physically, this points to an unmodeled interferent or a systematic difference in sample or measurement conditions not represented in the calibration set (for example, different particle size distribution, moisture, baseline or pathlength differences, or instrument differences).\n\nThese conclusions rule out alternatives:\n- Attributing high $T^{2}$ to random noise is incorrect because random noise primarily contributes to $Q$, not to leverage within the modeled subspace.\n- Interpreting high $Q$ as indicating concentration near the calibration mean is incorrect; $Q$ measures lack of fit, not position within the modeled concentration direction.\n- Low $Q$ does not imply zero concentration; it only indicates that the spectrum is well described by the model subspace.\n\nTherefore, the chemically and statistically sound interpretation is that Sample X is a valid but extreme sample along the modeled API concentration direction (likely outside the calibration range), and Sample Y likely exhibits unmodeled spectral variance due to an interferent or physical differences compared with the calibration set.\n\nHence, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1459316"}]}