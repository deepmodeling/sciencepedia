{"hands_on_practices": [{"introduction": "High-throughput screening campaigns, whether computational or experimental, must begin with a strategic decision: which materials should we investigate? For compositional spaces like ternary alloys, this often involves defining a discrete grid of points to sample. This practice [@problem_id:2479781] delves into the theoretical underpinnings of this decision, connecting the resolution of the sampling grid to the accuracy of any subsequent surrogate model built from the data. By working from first principles, you will quantify the relationship between grid density and interpolation error, providing a rigorous method for designing an efficient screening plan that meets a specified accuracy tolerance.", "problem": "In a high-throughput screening campaign for a ternary alloy system, you plan to discretize the composition simplex to construct a data-driven surrogate for a smooth property field. Let the ternary composition simplex be defined as $S = \\{(x_1,x_2,x_3) \\in \\mathbb{R}^3 : x_i \\ge 0,\\ \\sum_{i=1}^{3} x_i = 1\\}$, and parametrize $S$ by $(x_1,x_2) \\in \\mathbb{R}^2$ with $x_3 = 1 - x_1 - x_2$. Consider a barycentric grid of resolution $m \\in \\mathbb{N}$ consisting of all compositions with barycentric coordinates $\\left(\\frac{i}{m}, \\frac{j}{m}, \\frac{k}{m}\\right)$ where $i,j,k \\in \\mathbb{Z}_{\\ge 0}$ and $i+j+k = m$. The grid induces a standard piecewise-linear triangulation in the $(x_1,x_2)$-plane comprised of congruent right isosceles triangles whose legs have length $\\frac{1}{m}$.\n\nAssume the target property is a real-valued function $f:S \\to \\mathbb{R}$ that is twice continuously differentiable on $S$, and its Hessian matrix (with respect to $(x_1,x_2)$) has uniform operator norm bound $\\|\\nabla^2 f(x)\\|_2 \\le M$ for all $x \\in S$, where $M$ is a known constant. You will approximate $f$ by the piecewise-linear interpolant built from the values at the grid nodes.\n\nPerform the following, starting from first principles:\n\n1) Using a combinatorial argument that counts the number of nonnegative integer solutions to $i+j+k=m$, derive a closed-form expression for the number of unique grid compositions $N(m)$.\n\n2) Using Taylor’s theorem with the integral or Lagrange form of the remainder in multiple dimensions and the geometry of the right isosceles triangles in the mesh, obtain a sufficient bound for the worst-case pointwise interpolation error in terms of $M$, $m$, and the triangle edge lengths. From this, derive a sufficient condition on $m$ to guarantee that the uniform interpolation error is at most a prescribed tolerance $\\varepsilon > 0$.\n\n3) For $M = 500$ and $\\varepsilon = 3.0 \\times 10^{-3}$, compute the smallest integer resolution $m$ that satisfies your sufficient condition, and then compute $N(m)$.\n\nExpress your final answer as the exact integer value of $N(m)$ corresponding to the minimal $m$ you found. Do not include units and do not round; report the exact integer.", "solution": "The problem presents three tasks concerning the numerical approximation of a property function on a ternary composition simplex. We will address each task in sequence, beginning from fundamental principles.\n\nFirst, we must determine the number of grid points, $N(m)$, for a given resolution $m$. The grid points are defined by barycentric coordinates $(\\frac{i}{m}, \\frac{j}{m}, \\frac{k}{m})$ where $i, j, k$ are non-negative integers such that $i+j+k=m$. The number of such unique compositions, $N(m)$, is therefore equivalent to the number of non-negative integer solutions to the equation $i+j+k=m$. This is a classic combinatorial problem that can be solved using the \"stars and bars\" method. We are distributing $m$ identical items (stars) into $3$ distinct bins (indexed by $i, j, k$). This is equivalent to arranging $m$ stars and $3-1=2$ bars in a sequence. The total number of positions in the sequence is $m+2$. The number of distinct arrangements is the number of ways to choose the $2$ positions for the bars from the $m+2$ available positions. This is given by the binomial coefficient $\\binom{m+2}{2}$.\nTherefore, the number of grid points is:\n$$N(m) = \\binom{m+2}{2} = \\frac{(m+2)!}{2!(m+2-2)!} = \\frac{(m+2)(m+1)}{2}$$\n\nSecond, we must derive a sufficient condition on the resolution $m$ to ensure the uniform interpolation error does not exceed a tolerance $\\varepsilon$. The approximation is a piecewise-linear interpolant $L(x)$ of the function $f(x)$ over a triangulation of the domain. The error of this interpolation is determined by the smoothness of the function $f$ and the size of the triangles in the mesh.\nLet $x=(x_1, x_2)$ be the coordinates in the plane. The function is $f(x)$. The error is $E(x) = f(x) - L(x)$. According to the problem statement, $f$ is twice continuously differentiable ($C^2$), and its Hessian matrix $\\nabla^2 f(x)$ has a uniform operator norm bound $\\|\\nabla^2 f(x)\\|_2 \\le M$.\nFor a single triangle $T$ in the mesh, a standard result from the theory of finite element approximation, derivable from Taylor's theorem, provides an upper bound on the pointwise interpolation error. For a linear interpolant on a triangle $T$, this bound is given by:\n$$\\max_{x \\in T} |f(x) - L(x)| \\le C h_T^2 \\max_{y \\in T} \\|\\nabla^2 f(y)\\|_2$$\nwhere $h_T$ is the diameter of the triangle (the length of its longest side) and $C$ is a constant that depends on the geometry of the triangle. For a general convex domain, a widely used constant is $C = \\frac{1}{8}$, which originates from the one-dimensional case.\nThe problem states that the grid induces a triangulation by congruent right isosceles triangles whose legs have length $h = \\frac{1}{m}$. The diameter $h_T$ of such a triangle is the length of its hypotenuse:\n$$h_T = \\sqrt{h^2 + h^2} = \\sqrt{2h^2} = h\\sqrt{2} = \\frac{\\sqrt{2}}{m}$$\nSubstituting this into the error formula along with the given bound on the Hessian norm, we obtain a bound for the error within a single triangle:\n$$\\max_{x \\in T} |f(x) - L(x)| \\le \\frac{1}{8} h_T^2 M = \\frac{1}{8} \\left(\\frac{\\sqrt{2}}{m}\\right)^2 M = \\frac{1}{8} \\left(\\frac{2}{m^2}\\right) M = \\frac{M}{4m^2}$$\nSince all triangles in the mesh are congruent, this bound is uniform over the entire domain $S$. The uniform interpolation error is $\\sup_{x \\in S} |f(x) - L(x)|$. We can therefore state the sufficient condition that this error is at most $\\varepsilon$:\n$$\\frac{M}{4m^2} \\le \\varepsilon$$\nSolving for $m$, we find the required condition on the resolution:\n$$m^2 \\ge \\frac{M}{4\\varepsilon} \\implies m \\ge \\sqrt{\\frac{M}{4\\varepsilon}} = \\frac{1}{2}\\sqrt{\\frac{M}{\\varepsilon}}$$\nSince $m$ must be an integer, the smallest integer resolution that guarantees the desired accuracy is $m_{min} = \\left\\lceil \\frac{1}{2}\\sqrt{\\frac{M}{\\varepsilon}} \\right\\rceil$.\n\nThird, we compute the specific minimal integer resolution $m$ and the corresponding number of grid points $N(m)$ for the given values $M=500$ and $\\varepsilon = 3.0 \\times 10^{-3}$.\nUsing the condition derived above:\n$$m \\ge \\frac{1}{2}\\sqrt{\\frac{500}{3.0 \\times 10^{-3}}} = \\frac{1}{2}\\sqrt{\\frac{5 \\times 10^2}{3 \\times 10^{-3}}} = \\frac{1}{2}\\sqrt{\\frac{5}{3} \\times 10^5}$$\nTo evaluate this numerically:\n$$m \\ge \\frac{1}{2}\\sqrt{166666.66...} \\approx \\frac{1}{2}(408.248...) \\approx 204.124$$\nSince $m$ must be an integer, the smallest integer value for $m$ that satisfies this condition is $m = 205$.\n\nNow, we compute the number of unique grid compositions $N(m)$ for this resolution using the formula derived in the first part:\n$$N(m) = \\frac{(m+1)(m+2)}{2}$$\nSubstituting $m=205$:\n$$N(205) = \\frac{(205+1)(205+2)}{2} = \\frac{206 \\times 207}{2} = 103 \\times 207 = 21321$$\nThus, a minimum of $21321$ compositions must be screened to ensure the interpolation error is below the specified tolerance.", "answer": "$$\\boxed{21321}$$", "id": "2479781"}, {"introduction": "A predictive model is only as good as the evidence supporting its accuracy. In materials science, datasets are rich with correlations—different crystal structures (polymorphs) can exist for the same chemical composition, and different compositions can adopt the same crystal prototype. This practice [@problem_id:2479770] addresses the critical challenge of validating a model in the presence of such structured data. You will move beyond simplistic random splits and learn to design robust evaluation protocols using nested and group-aware cross-validation, a crucial skill for avoiding data leakage and obtaining an unbiased estimate of your model's true generalization performance.", "problem": "You are training a data-driven model in materials chemistry to predict density functional theory (DFT) formation energies for a high-throughput screening campaign. Your dataset consists of $N$ crystalline entries; many entries share the same reduced composition (for example, multiple polymorphs of the same $\\mathrm{A}_x\\mathrm{B}_y\\mathrm{O}_z$) and many share a crystal prototype (for example, perovskite, spinel, or Heusler), leading to strong intra-group correlations. You will tune model hyperparameters and estimate generalization performance. The training and evaluation protocol must be statistically principled: your risk estimates should be unbiased with respect to the data-generating distribution, and independence between training and test evaluations must be preserved to avoid leakage.\n\nFrom first principles in statistical learning, consider the following: cross-validation is used to estimate expected generalization error by approximating out-of-sample risk under an independent and identically distributed sampling assumption; selecting hyperparameters by minimizing an empirical estimate of risk introduces an optimism bias if the same data are used both for selection and assessment; and correlations induced by shared composition or prototype break the independence assumption if group members are split across training and test folds.\n\nWhich of the following statements correctly define $k$-fold and nested cross-validation for hyperparameter tuning, and correctly explain how to avoid leakage by grouping splits by composition or crystal prototype? Select all that apply.\n\nA. In $k$-fold cross-validation, the index set $\\{1,\\dots,N\\}$ is partitioned into $k$ disjoint folds of approximately equal size; for each fold, train on the union of the other $k-1$ folds and validate on the held-out fold, then aggregate validation losses. In nested cross-validation, an outer $K$-fold split provides $K$ outer test folds; for each outer split, an inner $J$-fold cross-validation on the outer-training portion is used to select hyperparameters, after which a model is refit on the entire outer-training portion with the selected hyperparameters and evaluated once on the untouched outer-test fold. To avoid leakage in materials datasets with repeated compositions or prototypes, use group-aware splitting with group labels equal to either reduced composition or prototype identifier (depending on the intended generalization target), enforce the same grouping in both inner and outer loops, and fit any preprocessing solely on training folds.\n\nB. In nested cross-validation, it is acceptable to compute unsupervised preprocessing (for example, scaling feature columns) using all samples including the outer-test folds, because no labels are used; this does not introduce leakage and can therefore be performed once prior to the outer loop.\n\nC. To prevent composition leakage, it suffices to enforce grouped splits by composition in the outer loop only; the inner loop used for hyperparameter tuning may use random (non-grouped) $J$-fold splits without biasing the final outer-loop risk estimate.\n\nD. If the goal is to avoid any leakage via either shared reduced composition or shared prototype, define groups as the connected components of a graph on the samples where an edge connects two samples if they share reduced composition or share prototype; then apply group-aware $k$-fold splitting using these component labels in both inner and outer loops. This guarantees that no reduced composition or prototype spans multiple folds and preserves independence assumptions more faithfully.\n\nE. Stratified random splits that match the histogram of the target property across folds are sufficient to prevent leakage due to repeated compositions or prototypes; additional grouping by composition or prototype is unnecessary if stratification is used.", "solution": "The problem statement poses a valid and highly relevant question in the field of materials informatics. It concerns the estimation of model generalization performance and hyperparameter tuning in the presence of structured correlations within the dataset, specifically, shared chemical compositions and crystal prototypes. The premise is scientifically sound, as such correlations are a common feature of materials datasets and violate the independent and identically distributed (i.i.d.) assumption underlying standard cross-validation. The goal of obtaining an unbiased risk estimate while avoiding data leakage is a critical and well-defined objective in statistical learning. The problem is well-posed, objective, and contains all necessary information for a rigorous analysis. We shall proceed to evaluate each statement.\n\nThe fundamental principles are as follows. First, a final estimate of a model's generalization risk must be performed on data that was in no way used during the training or model selection process. Second, if a model's hyperparameters are optimized to minimize a performance metric on a set of validation folds, a new, independent test set is required to obtain an unbiased estimate of performance for the tuned model. This is the motivation for nested cross-validation. Third, when data points are not independent (e.g., they fall into groups), any splitting of data into training and testing sets must respect these groups—all members of a given group must reside in the same set (either training or testing, but not split across them). This is known as group-aware splitting and is essential to prevent the model from gaining trivial information about the test set, which would lead to an artificially optimistic performance estimate.\n\nLet us now analyze each option.\n\n**A. In $k$-fold cross-validation, the index set $\\{1,\\dots,N\\}$ is partitioned into $k$ disjoint folds of approximately equal size; for each fold, train on the union of the other $k-1$ folds and validate on the held-out fold, then aggregate validation losses. In nested cross-validation, an outer $K$-fold split provides $K$ outer test folds; for each outer split, an inner $J$-fold cross-validation on the outer-training portion is used to select hyperparameters, after which a model is refit on the entire outer-training portion with the selected hyperparameters and evaluated once on the untouched outer-test fold. To avoid leakage in materials datasets with repeated compositions or prototypes, use group-aware splitting with group labels equal to either reduced composition or prototype identifier (depending on the intended generalization target), enforce the same grouping in both inner and outer loops, and fit any preprocessing solely on training folds.**\n\nThis statement is entirely correct. It provides a precise and comprehensive summary of the state-of-the-art, statistically principled methodology for this exact problem.\n1.  The definition of $k$-fold cross-validation is standard and correct.\n2.  The description of nested cross-validation is also perfect. It correctly separates the process into an outer loop for performance estimation and an inner loop for hyperparameter selection, with the crucial step of evaluating the final chosen model on an untouched outer-test fold. This procedure is designed specifically to avoid the optimism bias that arises from using the same data for both model selection and final evaluation.\n3.  The prescription for handling grouped data is also impeccable. It correctly identifies group-aware splitting as the necessary tool, correctly notes that the grouping strategy should align with the desired generalization task (e.g., to new compositions or new prototypes), and crucially mandates that the same grouping be used in both inner and outer loops to ensure the hyperparameters are optimized for the correct generalization task. Finally, it correctly states the rule for preventing leakage from preprocessing pipelines: they must be fitted only on the training data within each fold.\nVerdict: **Correct**.\n\n**B. In nested cross-validation, it is acceptable to compute unsupervised preprocessing (for example, scaling feature columns) using all samples including the outer-test folds, because no labels are used; this does not introduce leakage and can therefore be performed once prior to the outer loop.**\n\nThis statement is fundamentally incorrect. It describes a common but severe error that leads to data leakage. While it is true that labels are not used in unsupervised preprocessing such as feature scaling (e.g., standardization to zero mean and unit variance), the parameters of this transformation (the mean and standard deviation) are computed from the data. If computed on the entire dataset, including the test fold, information about the distribution of the test data (its mean and variance) is used to transform the training data. This violates the cardinal rule that the test set must remain completely isolated and unseen until the final evaluation. The correct procedure is to fit the preprocessor on the training portion of each fold and use the *fitted* preprocessor to transform both the training and test portions of that fold. Performing preprocessing on the full dataset before splitting leaks information and will result in an overly optimistic, and therefore biased, estimate of generalization error.\nVerdict: **Incorrect**.\n\n**C. To prevent composition leakage, it suffices to enforce grouped splits by composition in the outer loop only; the inner loop used for hyperparameter tuning may use random (non-grouped) $J$-fold splits without biasing the final outer-loop risk estimate.**\n\nThis statement is incorrect. The purpose of the inner cross-validation loop is to select hyperparameters that are optimal for the generalization task being evaluated by the outer loop. The outer loop, using group-aware splitting, is configured to estimate the model's performance on entirely new groups (compositions). If the inner loop uses random splits, it will evaluate hyperparameters based on their ability to perform well when parts of every group are seen during training—an interpolation task. This is a fundamentally different task from the extrapolation task being evaluated by the outer loop. Consequently, the inner loop will likely select hyperparameters that are suboptimal for the true task. While the final risk estimate on the outer folds might not be biased in the same way as violating test-set independence, the procedure is flawed because the model itself has not been properly tuned for the problem, leading to a poor model selection strategy. The validation scheme of the inner loop must mimic that of the outer loop.\nVerdict: **Incorrect**.\n\n**D. If the goal is to avoid any leakage via either shared reduced composition or shared prototype, define groups as the connected components of a graph on the samples where an edge connects two samples if they share reduced composition or share prototype; then apply group-aware $k$-fold splitting using these component labels in both inner and outer loops. This guarantees that no reduced composition or prototype spans multiple folds and preserves independence assumptions more faithfully.**\n\nThis statement describes a sophisticated and correct approach for handling multiple, overlapping sources of correlation. If a sample `S1` shares a composition with `S2`, and `S1` shares a prototype with `S3`, then all three samples (`S1`, `S2`, `S3`) are informationally linked. Simply grouping by composition would split `S1` and `S3`, while grouping by prototype would split `S1` and `S2`, both leading to potential leakage. By constructing a graph where nodes are samples and edges represent shared attributes, the connected components of this graph form \"super-groups\". All samples within a connected component are linked directly or indirectly. Using these components as the group labels for a group-aware split ensures that no two folds can contain samples that share either a composition or a prototype, even through an intermediary sample. This is the most rigorous method to enforce independence when multiple types of group structures are present and one wishes to generalize beyond all of them. The statement's conclusion that this guarantees that no composition or prototype spans multiple folds is accurate.\nVerdict: **Correct**.\n\n**E. Stratified random splits that match the histogram of the target property across folds are sufficient to prevent leakage due to repeated compositions or prototypes; additional grouping by composition or prototype is unnecessary if stratification is used.**\n\nThis statement is incorrect and conflates two distinct concepts. Stratification is a technique to ensure that the distribution of a certain variable—typically the target variable—is similar across all folds. This is useful for dealing with imbalanced datasets or regression problems with skewed target distributions. However, stratification operates on the level of sample distributions and does nothing to address the issue of group-based correlations. A stratified split is still fundamentally a random split (within strata) and will readily place different members of the same composition or prototype group into different folds (e.g., training and testing). This is precisely the data leakage scenario that group-aware splitting is designed to prevent. Stratification and group-aware splitting address orthogonal problems. One cannot substitute for the other.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AD}$$", "id": "2479770"}, {"introduction": "The goal of materials discovery is rarely to optimize a single property; more often, we seek a delicate balance between conflicting objectives, such as maximizing performance while minimizing cost and environmental impact. This leads to a set of optimal trade-off solutions known as a Pareto front. This exercise [@problem_id:2479712] introduces a cornerstone metric for evaluating such outcomes: the Hypervolume (HV) indicator. By deriving its formula from basic geometric principles and applying it to compare two sets of candidate materials, you will gain a powerful, quantitative tool for measuring the progress and success of any multi-objective screening campaign.", "problem": "In multi-objective High-Throughput Screening (HTS) for materials discovery, machine learning models guide the selection of candidate materials by balancing conflicting objectives. Consider the bi-objective minimization of two normalized, dimensionless objectives, $f_1$ and $f_2$, such as a predicted synthesis difficulty and a predicted environmental burden. For a finite set $S$ of model-suggested candidates, the global quality of $S$ can be assessed by the Hypervolume (HV) indicator (Hypervolume (HV) indicator), defined as the two-dimensional Lebesgue measure of the region in objective space that is (i) weakly dominated by at least one point in $S$ under minimization and (ii) bounded by a chosen reference point $r=(r_1,r_2)$ that is component-wise no better than all points in $S$.\n\nStarting exclusively from the core definitions of Pareto dominance under minimization and the Lebesgue measure of a set in $\\mathbb{R}^2$, do the following:\n\n1) Derive a closed-form expression for the Hypervolume (HV) indicator in the special case of two objectives and a finite non-dominated set $S=\\{(x_i,y_i)\\}_{i=1}^m$ whose points are such that when sorted by $x_i$ in strictly increasing order, the corresponding $y_i$ are strictly decreasing, with the reference point $r=(r_1,r_2)$ satisfying $x_i \\le r_1$ and $y_i \\le r_2$ for all $i$. Your derivation must begin from first principles: the definition of weak Pareto dominance and the area (Lebesgue measure) of unions of axis-aligned rectangles in $\\mathbb{R}^2$.\n\n2) Use your derived expression to compute the Hypervolume (HV) of two sets of non-dominated points that summarize two sequential model-guided HTS campaigns against the same reference point $r=(3.0,3.0)$:\n- Baseline set $S_0=\\{(1.0,2.0),\\,(1.6,1.2),\\,(2.3,0.95)\\}$.\n- Improved set $S_1=\\{(0.9,1.9),\\,(1.4,1.1),\\,(2.0,0.9)\\}$.\n\n3) Report the Hypervolume (HV) improvement, defined as $HV(S_1;r)-HV(S_0;r)$, as a single real number. Express your final answer as a dimensionless number and round your final numeric answer to four significant figures.", "solution": "**Part 1: Derivation of the Hypervolume (HV) Expression**\n\nThe problem asks for the Hypervolume of a set of points $S = \\{(x_i, y_i)\\}_{i=1}^m$ relative to a reference point $r = (r_1, r_2)$. The objectives are to be minimized.\n\nBy definition, a point $p'=(x', y')$ weakly dominates a point $p=(x, y)$ if $x' \\le x$ and $y' \\le y$. The region weakly dominated by a point $s_i = (x_i, y_i)$ is the set $[x_i, \\infty) \\times [y_i, \\infty)$.\n\nThe Hypervolume is the two-dimensional Lebesgue measure (area) of the region in objective space that is weakly dominated by at least one point in $S$ and bounded by the reference point $r$. The region, which we denote $\\mathcal{A}$, is the union of rectangles formed by each point in $S$ and the reference point $r$. Given that $x_i \\le r_1$ and $y_i \\le r_2$ for all $i$, this region is mathematically expressed as:\n$$ \\mathcal{A} = \\bigcup_{i=1}^m [x_i, r_1] \\times [y_i, r_2] $$\nThe Hypervolume is the measure of this set, $HV(S;r) = \\mu(\\mathcal{A})$.\n\nTo derive a closed-form expression for this measure, we exploit the given properties of the set $S$. The points are sorted such that $x_1 < x_2 < \\dots < x_m$. Since the set $S$ is non-dominated, this implies a strict ordering for the $y$-coordinates as well: $y_1 > y_2 > \\dots > y_m$.\n\nWe can decompose the total area $\\mathcal{A}$ into a union of disjoint rectangles, allowing for a simple summation of their areas. Let us define an additional coordinate $x_{m+1} = r_1$. We partition the integration domain along the x-axis using the points $x_1, x_2, \\dots, x_m, x_{m+1}$. This creates $m$ adjacent vertical strips.\n\nConsider the $i$-th strip, for $i \\in \\{1, 2, \\dots, m\\}$. This strip covers the x-interval $[x_i, x_{i+1}]$. For any $x$ in this interval, a point $(x,y)$ is in the dominated region $\\mathcal{A}$ if there exists some $s_j = (x_j, y_j) \\in S$ such that $x \\ge x_j$ and $y \\ge y_j$.\n\nFor $x \\in [x_i, x_{i+1}]$, the condition $x \\ge x_j$ is satisfied for all $j \\le i$, since $x_j \\le x_i \\le x$. It is not satisfied for any $j > i$, since $x < x_{i+1} \\le x_j$.\nThus, for a point $(x,y)$ in this vertical strip, the y-coordinate must satisfy $y \\ge y_j$ for at least one $j \\in \\{1, 2, \\dots, i\\}$. This is equivalent to satisfying $y \\ge \\min\\{y_1, y_2, \\dots, y_i\\}$.\nDue to the sorting property $y_1 > y_2 > \\dots > y_i$, the minimum is $y_i$. Therefore, the condition on $y$ simplifies to $y \\ge y_i$.\n\nThe portion of the dominated region within the vertical strip from $x_i$ to $x_{i+1}$ is the rectangle $[x_i, x_{i+1}] \\times [y_i, r_2]$.\nLet us define these disjoint rectangular regions as $C_i$:\n$$ C_i = [x_i, x_{i+1}] \\times [y_i, r_2], \\quad \\text{for } i=1, \\dots, m-1 $$\n$$ C_m = [x_m, r_1] \\times [y_m, r_2] $$\nThe total region is the disjoint union $\\mathcal{A} = \\bigcup_{i=1}^m C_i$. The total area is the sum of the areas of these rectangles:\n$$ HV(S;r) = \\mu(\\mathcal{A}) = \\sum_{i=1}^m \\mu(C_i) $$\nThe area of each rectangle $C_i$ is its width times its height: $\\mu(C_i) = (x_{i+1} - x_i)(r_2 - y_i)$ for $i<m$, and $\\mu(C_m) = (r_1 - x_m)(r_2 - y_m)$.\nUsing our definition $x_{m+1} = r_1$, we can write this as a single summation:\n$$ HV(S;r) = \\sum_{i=1}^{m} (x_{i+1} - x_i)(r_2 - y_i) $$\nThis is the required closed-form expression.\n\n**Part 2: Computation of HV for $S_0$ and $S_1$**\n\nThe reference point is $r=(r_1, r_2)=(3.0, 3.0)$.\n\nFor the baseline set $S_0=\\{(1.0,2.0),\\,(1.6,1.2),\\,(2.3,0.95)\\}$, we have $m=3$. The points are ordered correctly.\n- $s_1 = (x_1, y_1) = (1.0, 2.0)$\n- $s_2 = (x_2, y_2) = (1.6, 1.2)$\n- $s_3 = (x_3, y_3) = (2.3, 0.95)$\nWe define $x_4 = r_1 = 3.0$.\nApplying the derived formula:\n$$ HV(S_0;r) = (x_2 - x_1)(r_2 - y_1) + (x_3 - x_2)(r_2 - y_2) + (x_4 - x_3)(r_2 - y_3) $$\n$$ HV(S_0;r) = (1.6 - 1.0)(3.0 - 2.0) + (2.3 - 1.6)(3.0 - 1.2) + (3.0 - 2.3)(3.0 - 0.95) $$\n$$ HV(S_0;r) = (0.6)(1.0) + (0.7)(1.8) + (0.7)(2.05) $$\n$$ HV(S_0;r) = 0.6 + 1.26 + 1.435 = 3.295 $$\n\nFor the improved set $S_1=\\{(0.9,1.9),\\,(1.4,1.1),\\,(2.0,0.9)\\}$, we have $m=3$. The points are also ordered correctly.\n- $s'_1 = (x'_1, y'_1) = (0.9, 1.9)$\n- $s'_2 = (x'_2, y'_2) = (1.4, 1.1)$\n- $s'_3 = (x'_3, y'_3) = (2.0, 0.9)$\nWe define $x'_4 = r_1 = 3.0$.\nApplying the formula:\n$$ HV(S_1;r) = (x'_2 - x'_1)(r_2 - y'_1) + (x'_3 - x'_2)(r_2 - y'_2) + (x'_4 - x'_3)(r_2 - y'_3) $$\n$$ HV(S_1;r) = (1.4 - 0.9)(3.0 - 1.9) + (2.0 - 1.4)(3.0 - 1.1) + (3.0 - 2.0)(3.0 - 0.9) $$\n$$ HV(S_1;r) = (0.5)(1.1) + (0.6)(1.9) + (1.0)(2.1) $$\n$$ HV(S_1;r) = 0.55 + 1.14 + 2.1 = 3.79 $$\n\n**Part 3: Hypervolume Improvement**\n\nThe improvement is the difference between the two hypervolumes:\n$$ \\Delta HV = HV(S_1;r) - HV(S_0;r) $$\n$$ \\Delta HV = 3.79 - 3.295 = 0.495 $$\nThe problem requires the answer to be rounded to four significant figures. Thus, $0.495$ becomes $0.4950$.", "answer": "$$\n\\boxed{0.4950}\n$$", "id": "2479712"}]}