{"hands_on_practices": [{"introduction": "This problem explores the theoretical foundation of the method of initial rates. Before conducting any experiments, a critical question is to determine the minimum number of measurements required to uniquely identify all the unknown parameters in a proposed rate law. This exercise guides you through a formal analysis using linear algebra to connect experimental design to parameter identifiability, ensuring your experimental plan is efficient and sufficient [@problem_id:2642295].", "problem": "A bimolecular-to-termolecular crossover reaction $A + B + C \\to P$ is studied under the method of initial rates. Assume that, near $t=0$, the rate obeys a power-law form $r = k [A]^{\\alpha}[B]^{\\beta}[C]^{\\gamma}$ with unknown real exponents $\\alpha$, $\\beta$, $\\gamma$ and unknown positive rate constant $k$. You can set and hold initial concentrations $[A]_0$, $[B]_0$, $[C]_0$ at prescribed positive values, and you can measure the initial rate $r_0$ with negligible error. You may choose initial concentrations so that, during the initial-rate measurement window, $[A]$, $[B]$, and $[C]$ do not change appreciably due to reaction. Transport limitations and side reactions are absent.\n\nYour goal is to determine the minimal number of distinct initial-rate experiments required to uniquely estimate the four parameters $\\{\\alpha,\\beta,\\gamma,k\\}$ using only initial-rate data and choices of $([A]_0,[B]_0,[C]_0)$, and to justify that this number is indeed minimal by an identifiability argument that does not rely on any prior knowledge of $\\{\\alpha,\\beta,\\gamma,k\\}$. In your justification, you must start from basic principles of the initial-rate method and the definition of a power-law rate model and argue using linear independence or the rank of an appropriate sensitivity or design matrix (or an equivalent Jacobian-based argument). Construct explicitly a concrete experimental design that achieves identifiability and is consistent with isolation or pseudo-first-order reasoning (i.e., one reactant varied at a time while the others are held fixed and large), and explain why this design ensures a unique solution for $\\{\\alpha,\\beta,\\gamma,k\\}$.\n\nReport only the minimal number of experiments as your final answer. No units are required for the final answer.", "solution": "The problem requires the determination of the minimum number of initial-rate experiments needed to uniquely identify the four parameters $\\{\\alpha, \\beta, \\gamma, k\\}$ of the rate law $r = k [A]^{\\alpha}[B]^{\\beta}[C]^{\\gamma}$.\n\nLet us begin by formalizing the problem. For an experiment $i$, the initial rate $r_i$ is measured at a set of prescribed initial concentrations $[A]_i$, $[B]_i$, and $[C]_i$. The problem statement guarantees that these concentrations are effectively constant during the measurement period. The governing equation is:\n$$r_i = k [A]_i^{\\alpha} [B]_i^{\\beta} [C]_i^{\\gamma}$$\nThe parameters to be determined are the rate constant $k \\in \\mathbb{R}^+$ and the reaction orders $\\alpha, \\beta, \\gamma \\in \\mathbb{R}$. There are $4$ unknown parameters.\n\nThis model is nonlinear in its parameters. To facilitate analysis using linear algebra, we linearize the equation by taking the natural logarithm. Since $r_i > 0$, $k > 0$, and all concentrations are positive, this operation is well-defined:\n$$\\ln(r_i) = \\ln(k) + \\alpha \\ln([A]_i) + \\beta \\ln([B]_i) + \\gamma \\ln([C]_i)$$\nThis equation is linear with respect to a new set of parameters: $\\{\\ln(k), \\alpha, \\beta, \\gamma\\}$. Let us define a parameter vector $\\mathbf{p} \\in \\mathbb{R}^4$:\n$$\\mathbf{p} = \\begin{pmatrix} \\ln(k) \\\\ \\alpha \\\\ \\beta \\\\ \\gamma \\end{pmatrix}$$\nLet the vector of experimental inputs for experiment $i$ be represented by $\\mathbf{x}_i \\in \\mathbb{R}^4$:\n$$\\mathbf{x}_i = \\begin{pmatrix} 1 \\\\ \\ln([A]_i) \\\\ \\ln([B]_i) \\\\ \\ln([C]_i) \\end{pmatrix}$$\nAnd let the measured output be $y_i = \\ln(r_i)$. The model for one experiment is then a simple scalar product:\n$$y_i = \\mathbf{x}_i^T \\mathbf{p}$$\nTo determine the $4$ components of the parameter vector $\\mathbf{p}$, we must conduct a series of $N$ distinct experiments. This yields a system of $N$ linear equations:\n$$y_1 = \\mathbf{x}_1^T \\mathbf{p}$$\n$$y_2 = \\mathbf{x}_2^T \\mathbf{p}$$\n$$\\vdots$$\n$$y_N = \\mathbf{x}_N^T \\mathbf{p}$$\nThis system can be written in matrix form as $\\mathbf{y} = \\mathbf{Xp}$, where $\\mathbf{y} \\in \\mathbb{R}^N$ is the vector of measured log-rates and $\\mathbf{X}$ is the $N \\times 4$ design matrix, whose rows are the vectors $\\mathbf{x}_i^T$:\n$$\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}, \\quad \\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T \\end{pmatrix} = \\begin{pmatrix} 1 & \\ln([A]_1) & \\ln([B]_1) & \\ln([C]_1) \\\\ 1 & \\ln([A]_2) & \\ln([B]_2) & \\ln([C]_2) \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & \\ln([A]_N) & \\ln([B]_N) & \\ln([C]_N) \\end{pmatrix}$$\nFor this system of linear equations to have a unique solution for $\\mathbf{p}$, the design matrix $\\mathbf{X}$ must have full column rank. The number of columns is $4$, corresponding to the number of parameters to be identified. Thus, the condition for unique identifiability is:\n$$\\text{rank}(\\mathbf{X}) = 4$$\nThe rank of an $N \\times 4$ matrix cannot exceed $\\min(N, 4)$. For the rank to be $4$, it is a necessary condition that $N \\ge 4$. Therefore, a minimum of $4$ experiments is required.\n\nTo demonstrate that $N=4$ is not only necessary but also sufficient, we must construct an explicit experimental design with $4$ experiments for which the resulting $4 \\times 4$ design matrix $\\mathbf{X}$ is invertible (i.e., has rank $4$). The problem requires this design to be consistent with the isolation method, where one reactant concentration is varied while others are held constant.\n\nLet us construct such a design. We choose two distinct positive concentration values, $c_1$ and $c_2$, where $c_1 \\ne c_2$.\n\\begin{itemize}\n    \\item Experiment $1$: $([A]_1, [B]_1, [C]_1) = (c_1, c_1, c_1)$\n    \\item Experiment $2$: $([A]_2, [B]_2, [C]_2) = (c_2, c_1, c_1)$\n    \\item Experiment $3$: $([A]_3, [B]_3, [C]_3) = (c_1, c_2, c_1)$\n    \\item Experiment $4$: $([A]_4, [B]_4, [C]_4) = (c_1, c_1, c_2)$\n\\end{itemize}\nThis design constitutes a baseline experiment (Experiment $1$) and three subsequent experiments, each varying the concentration of a single species relative to the baseline.\n\nNow, we construct the corresponding $4 \\times 4$ design matrix $\\mathbf{X}$. Let $x_1 = \\ln(c_1)$ and $x_2 = \\ln(c_2)$. Since $c_1 \\ne c_2$, we have $x_1 \\ne x_2$.\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & \\ln(c_1) & \\ln(c_1) & \\ln(c_1) \\\\\n1 & \\ln(c_2) & \\ln(c_1) & \\ln(c_1) \\\\\n1 & \\ln(c_1) & \\ln(c_2) & \\ln(c_1) \\\\\n1 & \\ln(c_1) & \\ln(c_1) & \\ln(c_2)\n\\end{pmatrix} = \\begin{pmatrix}\n1 & x_1 & x_1 & x_1 \\\\\n1 & x_2 & x_1 & x_1 \\\\\n1 & x_1 & x_2 & x_1 \\\\\n1 & x_1 & x_1 & x_2\n\\end{pmatrix}\n$$\nTo verify that this matrix has rank $4$, we can compute its determinant. We perform elementary row operations by subtracting the first row from each of the other three rows:\n$$\n\\det(\\mathbf{X}) = \\det \\begin{pmatrix}\n1 & x_1 & x_1 & x_1 \\\\\n1-1 & x_2-x_1 & x_1-x_1 & x_1-x_1 \\\\\n1-1 & x_1-x_1 & x_2-x_1 & x_1-x_1 \\\\\n1-1 & x_1-x_1 & x_1-x_1 & x_2-x_1\n\\end{pmatrix} = \\det \\begin{pmatrix}\n1 & x_1 & x_1 & x_1 \\\\\n0 & x_2-x_1 & 0 & 0 \\\\\n0 & 0 & x_2-x_1 & 0 \\\\\n0 & 0 & 0 & x_2-x_1\n\\end{pmatrix}\n$$\nThe determinant of this upper triangular matrix is the product of its diagonal elements:\n$$\\det(\\mathbf{X}) = 1 \\cdot (x_2-x_1) \\cdot (x_2-x_1) \\cdot (x_2-x_1) = (x_2-x_1)^3$$\nSince we chose $c_1 \\ne c_2$, it follows that $x_1 = \\ln(c_1) \\ne \\ln(c_2) = x_2$.\nTherefore, $x_2 - x_1 \\ne 0$, which implies that $\\det(\\mathbf{X}) \\ne 0$.\nA non-zero determinant for the $4 \\times 4$ matrix $\\mathbf{X}$ confirms that its rank is $4$. This means the matrix is invertible, and a unique solution for the parameter vector $\\mathbf{p}$ exists: $\\mathbf{p} = \\mathbf{X}^{-1}\\mathbf{y}$.\n\nA unique solution for the transformed parameter vector $\\mathbf{p} = (\\ln(k), \\alpha, \\beta, \\gamma)^T$ directly provides unique values for the reaction orders $\\alpha, \\beta, \\gamma$ and for $\\ln(k)$. Since the exponential function is a bijection from $\\mathbb{R}$ to $\\mathbb{R}^+$, a unique value for $\\ln(k)$ implies a unique positive value for the rate constant $k = \\exp(\\ln(k))$.\n\nThe proposed experimental design guarantees identifiability for any unknown parameter values $\\{\\alpha, \\beta, \\gamma, k\\}$, as the invertibility of $\\mathbf{X}$ depends only on the choice of concentrations ($c_1 \\ne c_2$), which are under experimental control.\n\nWe have established that at least $4$ experiments are necessary and that $4$ experiments are sufficient. Therefore, the minimal number of distinct initial-rate experiments required is $4$.", "answer": "$$ \\boxed{4} $$", "id": "2642295"}, {"introduction": "Building on the principles of ideal experimental design, we now consider a more realistic scenario where experimental limitations impose constraints on the initial concentrations you can use. This practice demonstrates how such constraints can lead to collinearity in the statistical model, making it impossible to disentangle the effects of individual parameters. By analyzing the structure of the design matrix, you will learn to identify which parameter combinations remain measurable and how to redesign the experiment to restore full identifiability [@problem_id:2642183].", "problem": "A homogeneous, irreversible bimolecular reaction between species $A$ and $B$ is studied under isothermal, well-mixed conditions in a batch reactor. Over the initial transient where conversion is negligible, the initial rate $v_0$ is observed to follow a power-law dependence on the initial concentrations,\n$$\nv_0 = k [A]_0^{\\alpha} [B]_0^{\\beta},\n$$\nwhere $k$ is a temperature-dependent rate coefficient and $\\alpha,\\beta$ are real-valued reaction orders to be determined experimentally. To estimate $\\alpha$ and $\\beta$ from initial-rate data, the standard approach is to perform multiple runs with different initial concentrations $\\{([A]_0^{(i)},[B]_0^{(i)})\\}_{i=1}^n$ and fit the log-linear model obtained by taking natural logarithms:\n$$\n\\ln v_0^{(i)} = \\ln k + \\alpha \\ln [A]_0^{(i)} + \\beta \\ln [B]_0^{(i)} + \\varepsilon^{(i)},\n$$\nwhere $\\varepsilon^{(i)}$ encompasses experimental error. Consider that, due to a solubility constraint and the use of a coupled dosing manifold, the initial concentrations are restricted to lie on a curve\n$$\n[B]_0^{(i)} = C \\left([A]_0^{(i)}\\right)^2 \\quad \\text{for all runs } i=1,\\dots,n,\n$$\nwith a fixed, known proportionality constant $C>0$. You aim to determine which aspects of the rate law are experimentally identifiable from the initial-rate data collected under this constraint, and to propose a remedy grounded in experimental design that restores identifiability using the initial-rate method, the isolation method, or pseudo-first-order conditions.\n\nStarting from the definition of the initial-rate method and the log-linearization above, analyze the structure of the resulting linear regression design in the predictors $\\ln [A]_0$ and $\\ln [B]_0$ under the stated constraint. Explain, using linear algebra arguments about the design matrix, how collinearity arises and which parameter combinations become non-identifiable. Then, propose a concrete remedy that uses an orthogonal design in the space of $\\ln [A]_0$ and $\\ln [B]_0$ (for example, a two-level full factorial in coded logarithmic concentrations, or an isolation/pseudo-first-order scheme that achieves orthogonality upon centering) to restore identifiability of both $\\alpha$ and $\\beta$. Describe how such a design eliminates the collinearity in the design matrix.\n\nYour final answer must be the unique linear combination of $\\alpha$ and $\\beta$ that remains identifiable under the constrained design $[B]_0 = C [A]_0^2$, expressed as a closed-form analytic expression in $\\alpha$ and $\\beta$ only. Do not include any units in your final answer. If any numerical approximation were required, you would be told how many significant figures to use; here, no numerical approximation is required.", "solution": "The initial-rate method assumes that over a sufficiently short time window at the start of a batch experiment, the concentrations have not changed appreciably from their initial values. Under this approximation, the instantaneous rate is well represented by the initial rate,\n$$\nv_0 = k [A]_0^{\\alpha} [B]_0^{\\beta},\n$$\nwith $k$ constant for all runs at fixed temperature. Taking the natural logarithm yields a linear regression model in the predictors $\\ln [A]_0$ and $\\ln [B]_0$,\n$$\n\\ln v_0 = \\ln k + \\alpha \\ln [A]_0 + \\beta \\ln [B]_0 + \\varepsilon,\n$$\nwhere $\\varepsilon$ captures measurement noise and any residual model discrepancy.\n\nLet us assemble the model for $n$ experiments in matrix form. Define the response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ with entries $y_i = \\ln v_0^{(i)}$, the parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^{3}$ as\n$$\n\\boldsymbol{\\theta} = \\begin{pmatrix} \\ln k \\\\ \\alpha \\\\ \\beta \\end{pmatrix},\n$$\nand the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times 3}$ whose columns are the intercept (a column of ones), $\\mathbf{x}_A$ with entries $x_{A,i} = \\ln [A]_0^{(i)}$, and $\\mathbf{x}_B$ with entries $x_{B,i} = \\ln [B]_0^{(i)}$. Then\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}, \\quad \\mathbf{X} = \\begin{pmatrix} 1 & x_{A,1} & x_{B,1} \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_{A,n} & x_{B,n} \\end{pmatrix}.\n$$\n\nUnder the constraint $[B]_0^{(i)} = C \\left([A]_0^{(i)}\\right)^2$ for all $i$, we have\n$$\nx_{B,i} = \\ln [B]_0^{(i)} = \\ln C + 2 \\ln [A]_0^{(i)} = \\ln C + 2 x_{A,i}.\n$$\nTherefore, the third column of $\\mathbf{X}$ is an affine combination of the first and second columns. Specifically,\n$$\n\\mathbf{x}_B = (\\ln C)\\,\\mathbf{1} + 2\\,\\mathbf{x}_A,\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{n}$ denotes the intercept column. Consequently, the three columns of $\\mathbf{X}$ are linearly dependent: the column space of $\\mathbf{X}$ is at most two-dimensional. This implies that $\\mathbf{X}^{\\top}\\mathbf{X}$ is singular and ordinary least squares parameter estimates for $(\\ln k,\\alpha,\\beta)$ are not unique. This is the manifestation of collinearity leading to non-identifiability of the individual parameters $\\alpha$ and $\\beta$.\n\nTo determine what is identifiable, substitute the constraint into the model at the scalar level:\n$$\n\\ln v_0^{(i)} = \\ln k + \\alpha \\ln [A]_0^{(i)} + \\beta\\big(\\ln C + 2 \\ln [A]_0^{(i)}\\big) + \\varepsilon^{(i)}.\n$$\nCollecting terms yields\n$$\n\\ln v_0^{(i)} = \\big(\\ln k + \\beta \\ln C\\big) + \\big(\\alpha + 2\\beta\\big)\\,\\ln [A]_0^{(i)} + \\varepsilon^{(i)}.\n$$\nThis shows that, under the constraint, all the data can at most resolve two quantities: an effective intercept $\\ln k_{\\text{eff}} = \\ln k + \\beta \\ln C$ and a single effective slope\n$$\ns = \\alpha + 2\\beta.\n$$\nThe intercept $\\ln k_{\\text{eff}}$ conflates $\\ln k$ with $\\beta$, and thus $k$ cannot be separated from $\\beta$ without additional information about $C$ and one of the orders. More relevantly for the orders themselves, the only identifiable linear combination of $\\alpha$ and $\\beta$ from variation along this constrained curve is the slope $s = \\alpha + 2\\beta$. There is no information in the data to separate $\\alpha$ from $\\beta$ individually because $\\ln [B]_0$ does not vary independently of $\\ln [A]_0$; their regressors are affinely dependent.\n\nA remedy is to redesign the set of initial concentrations so that the regressors $\\ln [A]_0$ and $\\ln [B]_0$ vary independently and, ideally, orthogonally. Orthogonality in the regression sense can be enforced by centering the predictors and choosing the runs so that the sample cross-product vanishes:\n$$\n\\sum_{i=1}^{n} \\big(\\ln [A]_0^{(i)} - \\overline{\\ln [A]_0}\\big)\\big(\\ln [B]_0^{(i)} - \\overline{\\ln [B]_0}\\big) = 0,\n$$\nwhich makes the off-diagonal entry of the centered Gram matrix zero and yields a block-diagonal $\\mathbf{X}^{\\top}\\mathbf{X}$ in the slope subspace. A concrete construction is a two-level full factorial in log-space: pick two levels for $A$, $\\ln [A]_0 \\in \\{-a,+a\\}$, and two levels for $B$, $\\ln [B]_0 \\in \\{-b,+b\\}$, and perform all four combinations with equal replication. Then the centered predictors take values in $\\{-a,+a\\}$ and $\\{-b,+b\\}$ with zero means, and the sample cross-product is\n$$\n\\sum \\big(\\ln [A]_0\\big)\\big(\\ln [B]_0\\big) = (+a)(+b) + (+a)(-b) + (-a)(+b) + (-a)(-b) = 0,\n$$\nso the design is orthogonal in the slope space and both $\\alpha$ and $\\beta$ become identifiable. An alternative remedy grounded in the isolation or pseudo-first-order method is to execute two experiment blocks: in block 1, hold $[B]_0$ constant and vary $[A]_0$ (isolation of $A$), which identifies $\\alpha$ from the slope in $\\ln v_0$ versus $\\ln [A]_0$; in block 2, hold $[A]_0$ constant and vary $[B]_0$ (isolation of $B$), which identifies $\\beta$. Either approach breaks the affine dependence between $\\ln [A]_0$ and $\\ln [B]_0$ and removes the collinearity.\n\nIn summary, under the constrained design $[B]_0 = C [A]_0^2$, the unique identifiable linear combination of the reaction orders is the effective slope $s$ given by\n$$\ns = \\alpha + 2\\beta.\n$$\nThis is the only combination that can be estimated from initial-rate variation along the constraint curve without additional design changes.", "answer": "$$\\boxed{\\alpha + 2\\beta}$$", "id": "2642183"}, {"introduction": "Often in kinetics, the true form of the rate law is unknown, and several plausible models must be considered. This hands-on practice moves beyond fitting a single model to the more advanced task of model selection using the Akaike Information Criterion (AIC). You will learn how to implement a quantitative framework to compare candidate models, balancing their goodness-of-fit against their complexity to identify the most likely rate law and quantify the strength of evidence in its favor [@problem_id:2642276].", "problem": "You are given three candidate rate laws to model initial-rate data for a bimolecular reaction under conditions that include isolation and pseudo-first-order regimes. The goal is to compare these candidates using the Akaike Information Criterion (AIC) and report the preferred model and the strength of evidence in its favor.\n\nFundamental base:\n- Assume initial-rate measurements are independent with additive, identically distributed, zero-mean Gaussian errors of unknown variance. Under this assumption, the log-likelihood for residuals is proportional to the negative residual sum of squares (RSS), and maximum likelihood estimation (MLE) of parameters corresponds to minimizing RSS.\n- The Akaike Information Criterion (AIC) for a model with $p$ fitted parameters and maximized likelihood $\\hat{L}$ is $ \\mathrm{AIC} = 2p - 2 \\ln(\\hat{L}) $. For Gaussian errors with unknown variance estimated by $ \\widehat{\\sigma}^2 = \\mathrm{RSS}/N $ where $N$ is the number of data points, model rankings by AIC are equivalently obtained from $ \\mathrm{AIC} = N \\ln(\\mathrm{RSS}/N) + 2p + C $, where $C$ is a constant independent of the model and cancels in differences.\n\nCandidate rate laws (with $[\\cdot]$ denoting concentration):\n- Model $\\mathcal{M}_1$ (index $1$): $ r = k_1 [A] $. Parameters: $p=1$ ($k_1$).\n- Model $\\mathcal{M}_2$ (index $2$): $ r = k_2 [A][B] $. Parameters: $p=1$ ($k_2$).\n- Model $\\mathcal{M}_3$ (index $3$): $ r = \\dfrac{k_3 [A][B]}{1 + K [B]} $. Parameters: $p=2$ ($k_3, K$).\n\nAll rate constants and parameters are constrained to be nonnegative. Concentrations $[A]$ and $[B]$ are in $\\mathrm{mol}\\,\\mathrm{L}^{-1}$ and initial rates $r$ are in $\\mathrm{mol}\\,\\mathrm{L}^{-1}\\,\\mathrm{s}^{-1}$. The outputs requested are dimensionless.\n\nParameter estimation requirement:\n- For each model, fit parameters by minimizing $ \\mathrm{RSS} = \\sum_{i=1}^{N} \\left( r_i^{\\mathrm{obs}} - r_i^{\\mathrm{model}} \\right)^2 $ using nonlinear least squares under nonnegativity constraints on parameters.\n\nModel comparison requirement:\n- Compute $ \\mathrm{AIC} $ for each candidate using the Gaussian-error formulation above. Choose the best model as the one with the smallest AIC. In case of ties within absolute tolerance $10^{-12}$, choose the smallest model index.\n- Compute the evidence ratio favoring the best model over the next best (second-smallest AIC) as $ \\exp\\!\\left( \\dfrac{\\mathrm{AIC}_{\\mathrm{second}} - \\mathrm{AIC}_{\\mathrm{best}}}{2} \\right) $. This ratio is unitless and greater than or equal to $1$. If there is an AIC tie, this ratio equals $1$.\n\nTest suite:\n- Case $\\#1$ (general two-reactant variation, second-order baseline with small noise). Data points $( [A], [B], r )$:\n  - $(0.02, 0.03, 0.0003)$\n  - $(0.02, 0.06, 0.000599)$\n  - $(0.02, 0.12, 0.001198)$\n  - $(0.05, 0.03, 0.000751)$\n  - $(0.05, 0.06, 0.0015)$\n  - $(0.05, 0.12, 0.002999)$\n  - $(0.10, 0.03, 0.001502)$\n  - $(0.10, 0.06, 0.003001)$\n  - $(0.10, 0.12, 0.006)$\n- Case $\\#2$ (pseudo-first-order in $[A]$ with $[B]$ held constant and small noise). Data points $( [A], [B], r )$:\n  - $(0.02, 0.20, 0.000602)$\n  - $(0.05, 0.20, 0.001498)$\n  - $(0.10, 0.20, 0.003002)$\n  - $(0.20, 0.20, 0.005998)$\n- Case $\\#3$ (saturable dependence on $[B]$ under isolation of $[A]$, with small noise). Data points $( [A], [B], r )$:\n  - $(0.10, 0.01, 0.000924925926)$\n  - $(0.10, 0.02, 0.001723637931)$\n  - $(0.10, 0.05, 0.003571428571)$\n  - $(0.10, 0.10, 0.005556055556)$\n  - $(0.10, 0.20, 0.007693307692)$\n  - $(0.10, 0.50, 0.0100015)$\n\nYour task:\n- Implement a program that, for each of the three cases, fits all three models, computes AIC values, selects the best model as specified, and computes the evidence ratio favoring the best model over the second-best model.\n- Report, for each case, the pair $[m, e]$ where $m$ is the integer model index in $\\{1,2,3\\}$ and $e$ is the evidence ratio rounded to exactly six digits after the decimal point.\n\nFinal output format:\n- Your program should produce a single line of output containing the three results as a comma-separated list of lists with no spaces, for example, $[[m_1,e_1],[m_2,e_2],[m_3,e_3]]$. The evidence ratios must be printed with exactly six digits after the decimal point.", "solution": "The task is to perform model selection among three candidate rate laws for three distinct experimental datasets. The selection is based on the Akaike Information Criterion (AIC), a standard method for comparing statistical models. The procedure for each dataset is as follows:\n\n1.  **Parameter Estimation via Non-Linear Least Squares**: For each of the three models, $\\mathcal{M}_1$, $\\mathcal{M}_2$, and $\\mathcal{M}_3$, the parameters must be estimated by minimizing the Residual Sum of Squares (RSS). The objective function is:\n    $$ \\mathrm{RSS}(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left( r_i^{\\mathrm{obs}} - r^{\\mathrm{model}}([A]_i, [B]_i; \\boldsymbol{\\theta}) \\right)^2 $$\n    where $r_i^{\\mathrm{obs}}$ is the observed initial rate for the $i$-th data point, $r^{\\mathrm{model}}$ is the rate predicted by the model for given concentrations $[A]_i$ and $[B]_i$ and parameter set $\\boldsymbol{\\theta}$, and $N$ is the number of data points. The parameters $\\boldsymbol{\\theta}$ are constrained to be non-negative, consistent with their physical meaning as rate or equilibrium constants. This constrained optimization problem is solved using a numerical non-linear least squares algorithm. The candidate models are:\n    -   Model $\\mathcal{M}_1$: $r = k_1 [A]$, with parameter vector $\\boldsymbol{\\theta}_1 = (k_1)$ and $p_1=1$ parameter.\n    -   Model $\\mathcal{M}_2$: $r = k_2 [A] [B]$, with parameter vector $\\boldsymbol{\\theta}_2 = (k_2)$ and $p_2=1$ parameter.\n    -   Model $\\mathcal{M}_3$: $r = \\dfrac{k_3 [A][B]}{1 + K [B]}$, with parameter vector $\\boldsymbol{\\theta}_3 = (k_3, K)$ and $p_3=2$ parameters.\n\n2.  **AIC Calculation**: After determining the optimal parameters $\\hat{\\boldsymbol{\\theta}}$ and the corresponding minimum $\\mathrm{RSS}_{\\min}$ for each model, the AIC is calculated. For a model with $p$ parameters, the formula is:\n    $$ \\mathrm{AIC} = N \\ln\\left(\\frac{\\mathrm{RSS}_{\\min}}{N}\\right) + 2p $$\n    The AIC balances the goodness of fit (represented by $\\mathrm{RSS}_{\\min}$) with model complexity (represented by $p$). A lower AIC indicates a more preferable model. A perfect fit ($\\mathrm{RSS}_{\\min} = 0$) results in $\\mathrm{AIC} = -\\infty$, making it unequivocally the best model. However, due to experimental noise, $\\mathrm{RSS}_{\\min}$ is expected to be a small positive value.\n\n3.  **Model Selection**: For each case, the three models are ranked based on their AIC values. The model with the minimum AIC is selected as the best. In the event of a tie, defined as an absolute difference in AIC values smaller than $10^{-12}$, the model with the smaller index ($1 < 2 < 3$) is chosen. Let the chosen best model have an AIC of $\\mathrm{AIC}_{\\text{best}}$.\n\n4.  **Evidence Ratio Calculation**: The strength of evidence for the best model is quantified by the evidence ratio, $E$. This ratio compares the likelihood of the best model to that of the second-best model (the one with the next-lowest AIC, $\\mathrm{AIC}_{\\text{second}}$).\n    $$ E = \\exp\\left( \\frac{\\mathrm{AIC}_{\\text{second}} - \\mathrm{AIC}_{\\text{best}}}{2} \\right) $$\n    If the AIC of the best and second-best models are considered tied (within the $10^{-12}$ tolerance), the evidence ratio is $E=1$. Otherwise, $E > 1$, with larger values indicating stronger support for the best model.\n\nThis entire procedure is applied independently to each of the three test cases. The final output for each case is a pair $[m, e]$, where $m$ is the index of the best model and $e$ is the computed evidence ratio.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import least_squares\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for three test cases in chemical kinetics.\n    For each case, it fits three candidate rate laws, calculates their AIC,\n    selects the best model, and computes the evidence ratio.\n    \"\"\"\n\n    # Define the three candidate model functions.\n    # Each function takes parameters and concentrations as input and returns the predicted rate.\n    def model1(params, A, B):\n        k1, = params\n        return k1 * A\n\n    def model2(params, A, B):\n        k2, = params\n        return k2 * A * B\n\n    def model3(params, A, B):\n        k3, K = params\n        # Denominator is 1 + K*B. With K>=0 and B>=0, it is always >= 1, so no risk of division by zero.\n        return (k3 * A * B) / (1.0 + K * B)\n\n    def process_case(data_points):\n        \"\"\"\n        Processes a single test case: fits models, calculates AICs, and determines the best model and evidence ratio.\n        \"\"\"\n        data = np.array(data_points)\n        A = data[:, 0]\n        B = data[:, 1]\n        r_obs = data[:, 2]\n        N = len(r_obs)\n\n        models_spec = [\n            {'name': 'M1', 'func': model1, 'p': 1, 'p0': [1.0], 'bounds': ([0], [np.inf]), 'id': 1},\n            {'name': 'M2', 'func': model2, 'p': 1, 'p0': [1.0], 'bounds': ([0], [np.inf]), 'id': 2},\n            {'name': 'M3', 'func': model3, 'p': 2, 'p0': [1.0, 1.0], 'bounds': ([0, 0], [np.inf, np.inf]), 'id': 3}\n        ]\n\n        results = []\n        for spec in models_spec:\n            # Define the residual function for the least squares optimizer.\n            def residuals(params, A, B, r_obs):\n                r_pred = spec['func'](params, A, B)\n                return r_obs - r_pred\n\n            # Perform non-linear least squares fitting with non-negativity constraints.\n            fit_result = least_squares(\n                residuals,\n                spec['p0'],\n                bounds=spec['bounds'],\n                args=(A, B, r_obs)\n            )\n\n            # RSS is 2 * cost, as least_squares minimizes 0.5 * sum(residuals^2).\n            rss = 2 * fit_result.cost\n            p = spec['p']\n            \n            # Calculate AIC. Handle the case of a perfect fit (RSS=0).\n            if rss <= 1e-30:  # Use a small threshold to handle floating point near-zero\n                aic = -np.inf\n            else:\n                aic = N * math.log(rss / N) + 2 * p\n            \n            results.append({'model_index': spec['id'], 'aic': aic})\n\n        # Sort results primarily by AIC, secondarily by model index for tie-breaking.\n        # Python's default sort is stable. Sorting by index then by AIC achieves the desired outcome.\n        results.sort(key=lambda x: x['model_index'])\n        results.sort(key=lambda x: x['aic'])\n\n        best_model = results[0]\n        second_best_model = results[1]\n\n        m = best_model['model_index']\n        aic_best = best_model['aic']\n        aic_second = second_best_model['aic']\n\n        # Calculate evidence ratio, checking for ties.\n        if abs(aic_second - aic_best) < 1e-12:\n            e = 1.0\n        else:\n            e = math.exp((aic_second - aic_best) / 2.0)\n            \n        return [m, e]\n\n    # Test suite data.\n    test_cases = [\n        # Case #1\n        [\n            (0.02, 0.03, 0.0003),\n            (0.02, 0.06, 0.000599),\n            (0.02, 0.12, 0.001198),\n            (0.05, 0.03, 0.000751),\n            (0.05, 0.06, 0.0015),\n            (0.05, 0.12, 0.002999),\n            (0.10, 0.03, 0.001502),\n            (0.10, 0.06, 0.003001),\n            (0.10, 0.12, 0.006)\n        ],\n        # Case #2\n        [\n            (0.02, 0.20, 0.000602),\n            (0.05, 0.20, 0.001498),\n            (0.10, 0.20, 0.003002),\n            (0.20, 0.20, 0.005998)\n        ],\n        # Case #3\n        [\n            (0.10, 0.01, 0.000924925926),\n            (0.10, 0.02, 0.001723637931),\n            (0.10, 0.05, 0.003571428571),\n            (0.10, 0.10, 0.005556055556),\n            (0.10, 0.20, 0.007693307692),\n            (0.10, 0.50, 0.0100015)\n        ]\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        result_pair = process_case(case_data)\n        all_results.append(result_pair)\n\n    # Format the final output string exactly as specified.\n    result_strings = [f\"[{m},{e:.6f}]\" for m, e in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2642276"}]}