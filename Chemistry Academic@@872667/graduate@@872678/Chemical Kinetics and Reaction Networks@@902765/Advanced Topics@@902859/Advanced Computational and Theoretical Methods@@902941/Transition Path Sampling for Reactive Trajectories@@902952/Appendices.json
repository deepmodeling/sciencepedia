{"hands_on_practices": [{"introduction": "Transition Path Sampling methods deconstruct a rare event into a series of more probable steps, but the final goal remains the calculation of a macroscopic rate constant. This exercise provides hands-on practice with the final step of this process: synthesizing the rate constant $k_{AB}$ from the initial flux and a set of conditional crossing probabilities measured at intermediate interfaces. By working through this calculation and propagating the associated uncertainties, you will solidify your understanding of how the components of Transition Interface Sampling (TIS) come together to yield a quantitative prediction with rigorous error bounds. [@problem_id:2690077]", "problem": "A bistable chemical system has reactant basin $A$ and product basin $B$. You perform Transition Interface Sampling (TIS) across a series of non-intersecting interfaces $\\{\\lambda_0,\\lambda_1,\\lambda_2,\\lambda_B\\}$ that monotonically separate $A$ from $B$ along an order parameter $\\lambda$. The Transition Interface Sampling (TIS) framework defines the forward rate constant $k_{AB}$ as the flux of first crossings of the innermost interface $\\lambda_0$ by trajectories leaving $A$ times the probability that such a first-crossing trajectory ultimately reaches $B$ before returning to $A$.\n\nFrom independent simulations you obtain the following estimates with their one-standard-deviation uncertainties (assume all estimates are unbiased, Gaussian, and mutually independent):\n- First-crossing flux at $\\lambda_0$: $\\Phi_{A,\\lambda_0} = 2.50 \\times 10^{-3} \\ \\mathrm{s}^{-1}$ with standard uncertainty $0.20 \\times 10^{-3} \\ \\mathrm{s}^{-1}$.\n- Conditional interface-to-interface crossing probabilities: $P(\\lambda_{1}\\mid \\lambda_0) = 0.45 \\pm 0.03$, $P(\\lambda_{2}\\mid \\lambda_1) = 0.38 \\pm 0.02$, and $P(\\lambda_{B}\\mid \\lambda_2) = 0.22 \\pm 0.015$.\n\nStarting from the stated definition of the rate in Transition Interface Sampling (TIS) and fundamental probability rules, do the following:\n1. Derive an explicit expression for $k_{AB}$ in terms of $\\Phi_{A,\\lambda_0}$ and the conditional probabilities $P(\\lambda_{i+1}\\mid \\lambda_i)$.\n2. Compute the central estimate of $k_{AB}$ using the provided numerical values.\n3. Using the first-order delta method for uncertainty propagation from independent inputs, derive and compute the standard uncertainty of $k_{AB}$.\n\nExpress intermediate symbolic steps clearly. For grading, report only the central estimate of $k_{AB}$, rounded to four significant figures, in units of $\\mathrm{s}^{-1}$.", "solution": "The problem presented is a valid application of the principles of Transition Interface Sampling (TIS) for the calculation of a rate constant, and it is well-posed. We shall proceed with its solution.\n\nThe problem asks for three items: the derivation of the rate constant expression, its central value, and the associated uncertainty. We will address these in order.\n\nFirst, we derive the expression for the rate constant $k_{AB}$. The problem defines the rate constant as the product of the flux out of the reactant basin $A$ across the first interface $\\lambda_0$, and the probability that a trajectory making this first crossing will eventually reach the product basin $B$ before returning to $A$. This is expressed as:\n$$k_{AB} = \\Phi_{A,\\lambda_0} \\times P(B \\mid A \\to \\lambda_0)$$\nwhere $\\Phi_{A,\\lambda_0}$ is the first-crossing flux and $P(B \\mid A \\to \\lambda_0)$ is the conditional probability of reaching basin $B$ given a trajectory has just crossed $\\lambda_0$ from $A$.\n\nThe TIS methodology employs a series of non-intersecting, ordered interfaces that define a path from $A$ to $B$. In this problem, the path is defined by the sequence of interfaces $\\{\\lambda_0, \\lambda_1, \\lambda_2, \\lambda_B\\}$. For a trajectory to proceed from $\\lambda_0$ to $B$, it must successively cross $\\lambda_1$ and $\\lambda_2$ without returning to $A$. The total probability $P(B \\mid A \\to \\lambda_0)$ can be decomposed using the chain rule of probability. Let the event of a trajectory reaching interface $\\lambda_i$ before returning to $A$, having come from $\\lambda_{i-1}$, be denoted $E_i$. We are interested in the probability of reaching $B$ (event $E_B$) given we have crossed $\\lambda_0$ (event $E_0$).\n$$P(B \\mid A \\to \\lambda_0) = P(E_B \\mid E_0)$$\nSince the interfaces are ordered, the event of reaching $E_B$ presupposes the events of reaching $E_2$ and $E_1$. The chain rule gives:\n$$P(E_B \\mid E_0) = P(E_1 \\mid E_0) \\times P(E_2 \\mid E_1 \\cap E_0) \\times P(E_B \\mid E_2 \\cap E_1 \\cap E_0)$$\nA fundamental assumption in TIS is that the crossing process is Markovian with respect to the interfaces. This means the probability of crossing the next interface $\\lambda_{i+1}$ depends only on the fact that the trajectory has reached $\\lambda_i$, not on its prior history. Therefore, $P(E_2 \\mid E_1 \\cap E_0) = P(E_2 \\mid E_1)$ and $P(E_B \\mid E_2 \\cap E_1 \\cap E_0) = P(E_B \\mid E_2)$.\nThe probabilities provided in the problem statement are precisely these conditional probabilities: $P(\\lambda_{i+1} \\mid \\lambda_i)$. Thus, the total probability is the product:\n$$P(B \\mid A \\to \\lambda_0) = P(\\lambda_1 \\mid \\lambda_0) \\times P(\\lambda_2 \\mid \\lambda_1) \\times P(\\lambda_B \\mid \\lambda_2)$$\nSubstituting this into the rate expression, we arrive at the explicit formula for $k_{AB}$:\n$$k_{AB} = \\Phi_{A,\\lambda_0} \\times P(\\lambda_1 \\mid \\lambda_0) \\times P(\\lambda_2 \\mid \\lambda_1) \\times P(\\lambda_B \\mid \\lambda_2)$$\nThis completes the first part of the task.\n\nSecond, we compute the central estimate of $k_{AB}$ using the provided values:\n$\\Phi_{A,\\lambda_0} = 2.50 \\times 10^{-3} \\ \\mathrm{s}^{-1}$\n$P(\\lambda_1 \\mid \\lambda_0) = 0.45$\n$P(\\lambda_2 \\mid \\lambda_1) = 0.38$\n$P(\\lambda_B \\mid \\lambda_2) = 0.22$\n\nSubstituting these values into our derived expression:\n$$k_{AB} = (2.50 \\times 10^{-3}) \\times (0.45) \\times (0.38) \\times (0.22)$$\n$$k_{AB} = (2.50 \\times 10^{-3}) \\times (0.171) \\times (0.22)$$\n$$k_{AB} = (2.50 \\times 10^{-3}) \\times 0.03762$$\n$$k_{AB} = 0.09405 \\times 10^{-3} \\ \\mathrm{s}^{-1} = 9.405 \\times 10^{-5} \\ \\mathrm{s}^{-1}$$\nThe central estimate for the rate constant is $9.405 \\times 10^{-5} \\ \\mathrm{s}^{-1}$.\n\nThird, we derive and compute the standard uncertainty of $k_{AB}$, denoted $\\sigma_{k_{AB}}$. The problem specifies using the first-order delta method for independent inputs. Let the function for the rate constant be $k_{AB} = f(x_1, x_2, x_3, x_4)$, where $x_1 = \\Phi_{A,\\lambda_0}$, $x_2 = P(\\lambda_1 \\mid \\lambda_0)$, $x_3 = P(\\lambda_2 \\mid \\lambda_1)$, and $x_4 = P(\\lambda_B \\mid \\lambda_2)$. The variance, $\\sigma_{k_{AB}}^2$, is given by:\n$$\\sigma_{k_{AB}}^2 \\approx \\sum_{i=1}^{4} \\left( \\frac{\\partial f}{\\partial x_i} \\right)^2 \\sigma_{x_i}^2$$\nThe partial derivatives are:\n$\\frac{\\partial k_{AB}}{\\partial x_1} = x_2 x_3 x_4 = \\frac{k_{AB}}{x_1}$\n$\\frac{\\partial k_{AB}}{\\partial x_2} = x_1 x_3 x_4 = \\frac{k_{AB}}{x_2}$\n$\\frac{\\partial k_{AB}}{\\partial x_3} = x_1 x_2 x_4 = \\frac{k_{AB}}{x_3}$\n$\\frac{\\partial k_{AB}}{\\partial x_4} = x_1 x_2 x_3 = \\frac{k_{AB}}{x_4}$\n\nSubstituting these into the variance formula yields:\n$$\\sigma_{k_{AB}}^2 \\approx \\left(\\frac{k_{AB}}{x_1}\\right)^2 \\sigma_{x_1}^2 + \\left(\\frac{k_{AB}}{x_2}\\right)^2 \\sigma_{x_2}^2 + \\left(\\frac{k_{AB}}{x_3}\\right)^2 \\sigma_{x_3}^2 + \\left(\\frac{k_{AB}}{x_4}\\right)^2 \\sigma_{x_4}^2$$\nDividing by $k_{AB}^2$ gives the expression for the squared relative uncertainty:\n$$\\left(\\frac{\\sigma_{k_{AB}}}{k_{AB}}\\right)^2 \\approx \\left(\\frac{\\sigma_{x_1}}{x_1}\\right)^2 + \\left(\\frac{\\sigma_{x_2}}{x_2}\\right)^2 + \\left(\\frac{\\sigma_{x_3}}{x_3}\\right)^2 + \\left(\\frac{\\sigma_{x_4}}{x_4}\\right)^2$$\nNow we compute the numerical values for the relative uncertainties of the inputs:\n$\\frac{\\sigma_{x_1}}{x_1} = \\frac{0.20 \\times 10^{-3}}{2.50 \\times 10^{-3}} = 0.08$\n$\\frac{\\sigma_{x_2}}{x_2} = \\frac{0.03}{0.45} = \\frac{1}{15} \\approx 0.06667$\n$\\frac{\\sigma_{x_3}}{x_3} = \\frac{0.02}{0.38} = \\frac{1}{19} \\approx 0.05263$\n$\\frac{\\sigma_{x_4}}{x_4} = \\frac{0.015}{0.22} = \\frac{3}{44} \\approx 0.06818$\n\nSquaring and summing these values:\n$$\\left(\\frac{\\sigma_{k_{AB}}}{k_{AB}}\\right)^2 \\approx (0.08)^2 + \\left(\\frac{1}{15}\\right)^2 + \\left(\\frac{1}{19}\\right)^2 + \\left(\\frac{3}{44}\\right)^2$$\n$$\\left(\\frac{\\sigma_{k_{AB}}}{k_{AB}}\\right)^2 \\approx 0.0064 + 0.004444... + 0.002770... + 0.004648... \\approx 0.018263$$\nThe relative uncertainty is the square root of this sum:\n$$\\frac{\\sigma_{k_{AB}}}{k_{AB}} \\approx \\sqrt{0.018263} \\approx 0.1351$$\nFinally, the absolute uncertainty $\\sigma_{k_{AB}}$ is:\n$$\\sigma_{k_{AB}} \\approx 0.1351 \\times k_{AB} = 0.1351 \\times (9.405 \\times 10^{-5}) \\approx 1.27 \\times 10^{-5} \\ \\mathrm{s}^{-1}$$\nThe full result is $k_{AB} = (9.405 \\pm 1.27) \\times 10^{-5} \\ \\mathrm{s}^{-1}$. However, the problem instructions for the final answer specify only the central estimate, rounded to four significant figures. The calculated value $9.405 \\times 10^{-5}$ already has four significant figures.", "answer": "$$\\boxed{9.405 \\times 10^{-5}}$$", "id": "2690077"}, {"introduction": "A rigorous analysis of reactive events requires a precise definition of the reactant, product, and transition states, which is best achieved using the committor probability, $q(\\mathbf{x})$. This exercise explores the most direct way to compute this crucial quantity: launching a volley of short trial trajectories from a given configuration and counting the fraction that commit to the product state. Mastering this \"shooting\" method is fundamental, as it not only allows for the mapping of the transition state ensemble but also requires applying rigorous statistical tools to determine the necessary sample size for a given accuracy. [@problem_id:2690097]", "problem": "A stochastic molecular system evolves under underdamped Langevin dynamics at temperature $T$, with phase space point $x = (q,p)$, where $q$ are positions and $p$ are momenta. Let $A$ and $B$ be two disjoint metastable sets in phase space with smooth boundaries, representing reactant and product basins. For a given configuration $x^\\star$ with positions fixed at $q^\\star$ and momenta to be drawn from the canonical (Maxwell–Boltzmann) distribution at temperature $T$, the committor $q(x^\\star)$ is defined as the probability that, starting from $x^\\star$ with thermalized momenta, the process first hits $B$ before $A$. Formally, letting $X_t$ denote the stochastic process and $\\tau = \\inf\\{t \\ge 0 : X_t \\in A \\cup B\\}$, the committor is $q(x^\\star) = \\mathbb{P}_{x^\\star}\\!\\left(X_\\tau \\in B\\right)$.\n\nYou are asked to select the option that correctly proposes a shooting-based procedure to estimate $q(x^\\star)$ as an empirical success probability by launching short trial trajectories from $x^\\star$, and that also provides a valid, nonasymptotic sample size determination to guarantee an absolute error at most $\\varepsilon$ with confidence at least $1 - \\delta$.\n\nAssume throughout that:\n- The dynamics are the standard underdamped Langevin stochastic differential equations in the canonical ensemble at temperature $T$, so that initial momenta should be sampled from the Maxwell–Boltzmann distribution conditional on $q^\\star$.\n- Trials are independent when their random draws of initial momenta and stochastic forces are independent.\n- A trial trajectory is terminated immediately upon first hitting $A \\cup B$.\n\nWhich option is correct?\n\nA. From $x^\\star = (q^\\star,p)$, draw $p_i$ independently from the Maxwell–Boltzmann distribution at temperature $T$. For each trial $i \\in \\{1,\\dots,n\\}$, integrate the Langevin dynamics forward in time with independently drawn stochastic forces until the stopping time $\\tau_i = \\inf\\{t \\ge 0 : X_t^{(i)} \\in A \\cup B\\}$. Define $I_i = 1$ if $X_{\\tau_i}^{(i)} \\in B$ and $I_i = 0$ otherwise, and estimate $\\widehat{q}(x^\\star) = \\frac{1}{n}\\sum_{i=1}^n I_i$. To ensure $\\mathbb{P}(|\\widehat{q}(x^\\star) - q(x^\\star)| \\le \\varepsilon) \\ge 1 - \\delta$ for prescribed $\\varepsilon \\in (0,1)$ and $\\delta \\in (0,1)$, choose\n$$\nn \\;\\ge\\; \\frac{1}{2 \\varepsilon^2}\\,\\ln\\!\\left(\\frac{2}{\\delta}\\right).\n$$\n\nB. From $x^\\star = (q^\\star,p)$, draw a single thermal noise realization shared across all trials. For each trial $i \\in \\{1,\\dots,n\\}$, draw $p_i$ once from the Maxwell–Boltzmann distribution, integrate the dynamics forward for a fixed short time $\\Delta t$, and set $I_i = 1$ if $X_{\\Delta t}^{(i)} \\in B$ and $I_i = 0$ otherwise. Estimate $\\widehat{q}(x^\\star) = \\frac{1}{n}\\sum_{i=1}^n I_i$. Determine $n$ adaptively during sampling using the normal approximation with the Wald interval, increasing $n$ until\n$$\nn \\;\\ge\\; \\frac{z_{1-\\delta/2}^2\\, \\widehat{q}(x^\\star)\\left(1-\\widehat{q}(x^\\star)\\right)}{\\varepsilon^2},\n$$\nwhere $z_{1-\\delta/2}$ is the standard normal quantile.\n\nC. From $x^\\star = (q^\\star,p)$, draw $p_i$ uniformly on the constant kinetic energy surface corresponding to temperature $T$ (microcanonical shell). For each trial $i \\in \\{1,\\dots,n\\}$, integrate deterministic Hamiltonian dynamics both forward and backward until hitting $A$ or $B$ and set $I_i = 1$ if the forward integration reaches $B$ before $A$. Estimate $\\widehat{q}(x^\\star) = \\frac{1}{n}\\sum_{i=1}^n I_i$. To ensure $\\mathbb{P}(|\\widehat{q}(x^\\star) - q(x^\\star)| \\le \\varepsilon) \\ge 1 - \\delta$, use Chebyshev’s inequality and the bound $q(x^\\star)(1-q(x^\\star)) \\le \\frac{1}{4}$ to choose\n$$\nn \\;\\ge\\; \\frac{1}{4\\,\\varepsilon^2\\,\\delta}.\n$$\n\nD. From $x^\\star = (q^\\star,p)$, draw $p_i$ independently from the Maxwell–Boltzmann distribution. For each trial $i$, generate an antithetic pair by also launching from $-p_i$. Integrate forward with the same stochastic noise realization for both $p_i$ and $-p_i$ until hitting $A$ or $B$, and set $I_i = \\frac{1}{2}\\left(\\mathbb{1}\\{X_{\\tau_i}^{(i,+)} \\in B\\} + \\mathbb{1}\\{X_{\\tau_i}^{(i,-)} \\in B\\}\\right)$, claiming $I_i = \\frac{1}{2}$ by time-reversal symmetry. Estimate $\\widehat{q}(x^\\star) = \\frac{1}{n}\\sum_{i=1}^n I_i \\equiv \\frac{1}{2}$. No explicit sample size calculation is needed since the antithetic pairing is assumed to eliminate variance by symmetry.\n\nSelect all options that are correct. Provide no partial credit; an option is correct only if both the estimation procedure and the sample size determination meet the stated requirements under the given assumptions.", "solution": "The task is to identify the correct procedure for estimating the committor probability $q(x^\\star)$ and determining a sufficient sample size $n$ for a desired precision and confidence. The system evolves under underdamped Langevin dynamics, and the committor $q(x^\\star)$ is the probability of a trajectory, starting with configuration $q^\\star$ and Maxwell-Boltzmann distributed momenta, reaching state $B$ before state $A$.\n\nThis is a problem of estimating a population proportion, which is a parameter of a Bernoulli distribution. The proposed methods will be evaluated based on their correctness in modeling the physical process and in applying statistical principles for estimation.\n\nThe committor $q(x^\\star)$ is formally defined as $q(x^\\star) = \\mathbb{P}_{x^\\star}(X_{\\tau} \\in B)$, where the probability is taken over the ensemble of trajectories starting from the phase space point $x = (q,p)$ with $q=q^\\star$ and $p$ drawn from the Maxwell-Boltzmann distribution at temperature $T$, and also over the realization of the stochastic forces in the Langevin equation. Let $p = q(x^\\star)$. A standard Monte Carlo approach involves running $n$ independent trials. For each trial $i$, we sample an initial momentum $p_i$ and run the dynamics. Let $I_i$ be an indicator random variable such that $I_i = 1$ if the $i$-th trial ends in $B$, and $I_i = 0$ if it ends in $A$. The random variables $I_1, \\dots, I_n$ are independent and identically distributed Bernoulli variables with success probability $p = q(x^\\star)$. The Maximum Likelihood Estimator for $p$ is the sample mean, $\\widehat{p}_n = \\widehat{q}(x^\\star) = \\frac{1}{n}\\sum_{i=1}^n I_i$.\n\nThe problem of sample size determination is to find the minimum $n$ that guarantees $\\mathbb{P}(|\\widehat{p}_n - p| \\le \\varepsilon) \\ge 1 - \\delta$. This can be addressed using concentration inequalities. Hoeffding's inequality for i.i.d. random variables $I_i$ bounded in $[a_i, b_i]$ states:\n$$\n\\mathbb{P}\\left(\\left|\\frac{1}{n}\\sum_{i=1}^n I_i - \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n I_i\\right]\\right| \\ge \\varepsilon\\right) \\le 2 \\exp\\left(-\\frac{2n^2\\varepsilon^2}{\\sum_{i=1}^n (b_i - a_i)^2}\\right).\n$$\nFor our Bernoulli variables $I_i$, we have $a_i=0$ and $b_i=1$. The inequality simplifies to:\n$$\n\\mathbb{P}(|\\widehat{p}_n - p| \\ge \\varepsilon) \\le 2 e^{-2n\\varepsilon^2}.\n$$\nTo satisfy the confidence requirement, we set the right-hand side to be less than or equal to $\\delta$:\n$$\n2 e^{-2n\\varepsilon^2} \\le \\delta \\implies e^{-2n\\varepsilon^2} \\le \\frac{\\delta}{2} \\implies -2n\\varepsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right) \\implies 2n\\varepsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right).\n$$\nThis yields the sample size requirement:\n$$\nn \\ge \\frac{1}{2\\varepsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right).\n$$\nThis is a standard, non-asymptotic result providing a rigorous guarantee.\n\nNow we analyze each option.\n\n**Option A Evaluation**\n- **Estimation Procedure**: The procedure described is the canonical Monte Carlo method for estimating the committor. It correctly specifies:\n    1. Independent sampling of initial momenta $p_i$ from the Maxwell-Boltzmann distribution for each trial.\n    2. Integration of Langevin dynamics with independent stochastic force realizations for each trial, ensuring the trials are i.i.d.\n    3. Correct stopping condition: termination upon first hitting $A \\cup B$.\n    4. Correct definition of the Bernoulli outcome variable $I_i$.\n    5. Correct form of the estimator $\\widehat{q}(x^\\star)$ as the sample mean.\nThis procedure is entirely correct.\n- **Sample Size Determination**: The option proposes $n \\ge \\frac{1}{2 \\varepsilon^2}\\,\\ln\\!\\left(\\frac{2}{\\delta}\\right)$. As derived above, this is the correct sample size obtained from Hoeffding's inequality. It is a valid, non-asymptotic bound as required by the problem statement.\n- **Verdict**: Both the estimation procedure and the sample size determination are correct. Thus, this option is **Correct**.\n\n**Option B Evaluation**\n- **Estimation Procedure**: This procedure is flawed for several reasons:\n    1. \"draw a single thermal noise realization shared across all trials\": This violates the requirement that trials be independent. The stochastic paths would be correlated, and the variables $I_i$ would not be independent.\n    2. \"integrate the dynamics forward for a fixed short time $\\Delta t$\": This is an incorrect stopping condition. The committor is defined based on the *first-passage* event to either $A$ or $B$. Terminating at a fixed time $\\Delta t$ does not measure this; many trajectories may not have reached either $A$ or $B$, and the outcome would depend on the arbitrary choice of $\\Delta t$.\n- **Sample Size Determination**: The formula $n \\ge \\frac{z_{1-\\delta/2}^2\\, \\widehat{q}(x^\\star)(1-\\widehat{q}(x^\\star))}{\\varepsilon^2}$ is based on the normal approximation from the Central Limit Theorem (CLT) and is used to construct a Wald confidence interval. This is an *asymptotic* result, valid for large $n$. It does not provide the *non-asymptotic* guarantee requested in the problem statement. Furthermore, this method can perform poorly if $q(x^\\star)$ is close to $0$ or $1$, or if $n$ is small.\n- **Verdict**: The estimation procedure is fundamentally incorrect, and the sample size formula is asymptotic, not non-asymptotic. Thus, this option is **Incorrect**.\n\n**Option C Evaluation**\n- **Estimation Procedure**: This procedure is incorrect as it deviates from the specified physics:\n    1. \"draw $p_i$ uniformly on the constant kinetic energy surface\": This corresponds to the microcanonical ensemble, not the canonical ensemble at temperature $T$ where momenta follow a Maxwell-Boltzmann distribution as stated in the problem.\n    2. \"integrate deterministic Hamiltonian dynamics\": The problem specifies underdamped Langevin dynamics, which are stochastic due to friction and random forces. Hamiltonian dynamics are deterministic and energy-conserving, which is a different physical model.\n- **Sample Size Determination**: The formula $n \\ge \\frac{1}{4\\,\\varepsilon^2\\,\\delta}$ is derived from Chebyshev's inequality, using the worst-case variance $\\text{Var}(I_i) \\le 1/4$. While this is a valid non-asymptotic bound, it is significantly looser (less efficient) than the bound from Hoeffding's inequality. More importantly, it is paired with a completely wrong physical procedure.\n- **Verdict**: The estimation procedure is based on the wrong physical ensemble and dynamics. Thus, this option is **Incorrect**.\n\n**Option D Evaluation**\n- **Estimation Procedure**: This option proposes an antithetic variate sampling scheme.\n    1. The core claim is that for every trial, the outcome from momentum $p_i$ and $-p_i$ will be opposite, leading to the conclusion that the average indicator is always $1/2$. This claim stems from an assumption of time-reversal symmetry: $q(q^\\star, p) + q(q^\\star, -p) = 1$.\n    2. This symmetry holds for time-reversible deterministic dynamics (like Hamiltonian dynamics) under certain conditions on the starting point. However, underdamped Langevin dynamics are *not* time-reversible in this simple sense. The friction term $-\\gamma p dt$ is not invariant under the transformation $(t,p) \\to (-t,-p)$. Flipping the momentum at the start leads to a different drift and friction effect, and the stochastic noise breaks any simple deterministic symmetry. Therefore, there is no guarantee that $q(q^\\star, p) + q(q^\\star, -p) = 1$. It is entirely possible for trajectories starting with $p_i$ and $-p_i$ to both end in $A$ or both end in $B$.\n    3. The claim that $I_i = \\frac{1}{2}$ is false. Consequently, the estimator $\\widehat{q}(x^\\star) = \\frac{1}{2}$ is not a valid estimator but a baseless assertion.\n- **Sample Size Determination**: The option claims no calculation is needed because the variance is zero. This is a direct consequence of the false premise that $I_i$ is a constant. Since the premise is wrong, the variance is not zero, and this conclusion is also invalid.\n- **Verdict**: The method is based on a mistaken application of time-reversal symmetry to Langevin dynamics. The resulting estimator and variance claims are incorrect. Thus, this option is **Incorrect**.\n\nIn conclusion, only Option A correctly describes the standard, valid procedure for estimating the committor and providing a rigorous, non-asymptotic sample size guarantee.", "answer": "$$\\boxed{A}$$", "id": "2690097"}, {"introduction": "The efficiency of interface-based methods like FFS and TIS depends critically on the placement of the interfaces that connect the reactant and product states. This practice problem moves beyond simply using the method to optimizing its design, challenging you to devise an adaptive algorithm that maximizes statistical accuracy for a fixed computational budget. By analyzing how the variance of the final rate constant depends on the intermediate crossing probabilities, you will derive the principle that guides modern, efficient transition path sampling simulations. [@problem_id:2690120]", "problem": "A rare reactive event $A \\to B$ is studied by an interface-based transition path method such as Forward Flux Sampling (FFS) or Transition Interface Sampling (TIS). Let $\\lambda(\\mathbf{x})$ be a monotonic order parameter that increases from basin $A$ to $B$, and consider a sequence of interfaces $A \\equiv \\{\\lambda \\le \\lambda_{0}\\} \\prec \\{\\lambda=\\lambda_{1}\\} \\prec \\cdots \\prec \\{\\lambda=\\lambda_{M}\\} \\equiv \\{\\lambda \\ge \\lambda_{B}\\}$ with $\\lambda_{0}<\\lambda_{1}<\\cdots<\\lambda_{M}=\\lambda_{B}$. At each interface $\\lambda_{i}$, $N_{i}$ trial trajectories are fired and terminated when they either reach $\\lambda_{i+1}$ (success) or return to $A$ (failure), producing an estimator $\\hat{p}_{i}$ of the conditional probability $p_{i}\\equiv P(\\lambda_{i+1}\\mid \\lambda_{i})$. The overall crossing probability is $P_{A\\to B}=\\prod_{i=0}^{M-1}p_{i}$, and the rate is $k_{AB}=\\Phi_{A0}\\,P_{A\\to B}$, where $\\Phi_{A0}$ is the steady-state flux of trajectories leaving $A$ and crossing $\\lambda_{0}$. Assume a simple cost model where each trial trajectory cost is approximately constant across interfaces, and that, to leading order, interface estimators are statistically independent with binomial fluctuations at each interface.\n\nWhich option best describes a scientifically sound adaptive algorithm to place the interfaces so that $P(\\lambda_{i+1}\\mid \\lambda_{i})$ is approximately constant across $i$, and provides a correct first-principles justification for the expected reduction in estimator variance under a fixed total computational budget?\n\nA. Start from $\\lambda_{0}$ and choose a target success probability $p^{\\star}\\in(0,1)$ (e.g., $p^{\\star}\\approx 0.2$). For each $i$, use short pilot batches of $K$ trials from $\\lambda_{i}$ to bracket two candidate positions $\\lambda_{i+1}^{-}$ and $\\lambda_{i+1}^{+}$ such that the observed success fractions straddle $p^{\\star}$, then apply a bisection search in $\\lambda$ with fresh pilot trials until the estimated $\\hat{p}_{i}\\approx p^{\\star}$ within a tolerance. Repeat to construct $\\lambda_{i+1}$, $i\\leftarrow i+1$, until $\\lambda_{M}=\\lambda_{B}$. In production, allocate $N_{i}$ roughly equally across interfaces (or according to small adjustments if per-interface costs differ). Justification: with binomial variance $\\mathrm{Var}(\\hat{p}_{i})=p_{i}(1-p_{i})/N_{i}$ and independence, a delta-method expansion gives the relative variance $\\mathrm{Var}(\\hat{P}_{A\\to B})/P_{A\\to B}^{2}\\approx \\sum_{i}(1-p_{i})/(p_{i}N_{i})$. For fixed $\\prod_{i}p_{i}$ and fixed $\\{N_{i}\\}$, this sum is minimized when all $p_{i}$ are equal, so adaptively enforcing $p_{i}\\approx p^{\\star}$ reduces variance for a fixed total cost.\n\nB. Place interfaces uniformly in $\\lambda$: $\\lambda_{i}=\\lambda_{0}+i(\\lambda_{B}-\\lambda_{0})/M$. During sampling, adapt $N_{i}$ so that each interface achieves the same number of successful trials $N_{i} \\hat{p}_{i}$, which equalizes information content and therefore minimizes variance. Justification: equal successes imply balanced contributions to the product, which is optimal.\n\nC. First compute a potential of mean force $F(\\lambda)$ and then place interfaces so that the free energy increments $\\Delta F_{i}=F(\\lambda_{i+1})-F(\\lambda_{i})$ are equal. This partitions the barrier evenly, making $p_{i}$ implicitly uniform and minimizing variance because the barrier is sampled uniformly.\n\nD. Make interfaces very sparse so that each $p_{i}\\ll 1$ and then compensate by choosing very large $N_{i}$ at those interfaces. Because each $\\mathrm{Var}(\\hat{p}_{i})$ then scales as $p_{i}/N_{i}$, driving $N_{i}$ large reduces the variance more efficiently than adding more interfaces.\n\nE. Estimate the committor $q(\\mathbf{x})\\equiv P(B\\ \\text{before}\\ A\\mid \\mathbf{x})$ on the fly and place interfaces at committor quantiles $q_{i}$ so that $q_{i+1}-q_{i}$ is constant. Because the committor changes linearly between interfaces, the conditional probabilities $p_{i}$ are equal and the variance is minimized without further justification.", "solution": "The problem asks for an evaluation of different adaptive algorithms for placing interfaces in transition path sampling methods like Forward Flux Sampling (FFS), with the goal of minimizing the statistical error in the estimated rate constant for a fixed computational cost. The core idea is to understand how the placement of interfaces, which determines the conditional probabilities $p_i$, and the allocation of computational effort, $N_i$, affect the overall variance of the estimator for the total crossing probability, $P_{A\\to B}$.\n\nFirst, let us establish the fundamental principles. The problem states that the total crossing probability is given by the product of conditional probabilities:\n$$P_{A\\to B} = \\prod_{i=0}^{M-1} p_i$$\nwhere $p_i = P(\\lambda_{i+1} \\mid \\lambda_i)$ is the probability that a trajectory initiated at interface $i$ will reach interface $i+1$ before returning to state $A$. The estimator for this probability is $\\hat{P}_{A\\to B} = \\prod_{i=0}^{M-1} \\hat{p}_i$.\n\nThe key to analyzing the statistical error is to calculate the relative variance of this estimator, $\\mathrm{Var}(\\hat{P}_{A\\to B}) / P_{A\\to B}^2$. For small errors, this can be approximated by the variance of the logarithm of the estimator. This is a standard application of the delta method.\n$$ \\frac{\\mathrm{Var}(\\hat{P}_{A\\to B})}{P_{A\\to B}^2} \\approx \\mathrm{Var}(\\ln \\hat{P}_{A\\to B}) = \\mathrm{Var}\\left(\\sum_{i=0}^{M-1} \\ln \\hat{p}_i\\right) $$\nThe problem states that the estimators $\\hat{p}_i$ for different interfaces are statistically independent. Therefore, the variance of the sum is the sum of the variances:\n$$ \\mathrm{Var}\\left(\\sum_{i=0}^{M-1} \\ln \\hat{p}_i\\right) = \\sum_{i=0}^{M-1} \\mathrm{Var}(\\ln \\hat{p}_i) $$\nUsing the delta method again for each term, $\\mathrm{Var}(\\ln \\hat{p}_i) \\approx \\mathrm{Var}(\\hat{p}_i) / p_i^2$.\nThe problem assumes binomial fluctuations for the success/failure of the $N_i$ trials at each interface. The estimator $\\hat{p}_i$ is the fraction of successful trials, so its variance is given by the binomial variance:\n$$ \\mathrm{Var}(\\hat{p}_i) = \\frac{p_i(1 - p_i)}{N_i} $$\nCombining these results gives the expression for the relative variance of the total probability estimator:\n$$ \\mathcal{V} \\equiv \\frac{\\mathrm{Var}(\\hat{P}_{A\\to B})}{P_{A\\to B}^2} \\approx \\sum_{i=0}^{M-1} \\frac{\\mathrm{Var}(\\hat{p}_i)}{p_i^2} = \\sum_{i=0}^{M-1} \\frac{p_i(1 - p_i)/N_i}{p_i^2} = \\sum_{i=0}^{M-1} \\frac{1 - p_i}{p_i N_i} $$\nThe optimization problem is to minimize this variance $\\mathcal{V}$ subject to a fixed total computational cost. Assuming the cost per trial is constant, this is equivalent to fixing the total number of trials, $N_{tot} = \\sum_{i=0}^{M-1} N_i$. The choice of interface positions determines the set of probabilities $\\{p_i\\}$, constrained by $\\prod_{i=0}^{M-1} p_i = P_{A\\to B}$.\n\nThe most straightforward strategy, and the one implicitly suggested by some options, is to allocate the computational budget evenly, such that $N_i = N_{tot}/M$ for all $i$. In this case, the variance becomes:\n$$ \\mathcal{V} \\approx \\frac{M}{N_{tot}} \\sum_{i=0}^{M-1} \\frac{1 - p_i}{p_i} = \\frac{M}{N_{tot}} \\left( \\sum_{i=0}^{M-1} \\frac{1}{p_i} - M \\right) $$\nTo minimize this expression for a fixed $M$, we must minimize the sum $\\sum_{i=0}^{M-1} 1/p_i$, subject to the constraint that the product $\\prod_{i=0}^{M-1} p_i = P_{A\\to B}$ is constant. This is a classic optimization problem that can be solved using Lagrange multipliers. Let $f(\\{p_i\\}) = \\sum 1/p_i$ and the constraint be $g(\\{p_i\\}) = \\prod p_i - P_{A\\to B} = 0$. The condition $\\nabla f = \\mu \\nabla g$ leads to $-\\frac{1}{p_k^2} = \\mu \\frac{\\prod p_i}{p_k}$, which implies that $1/p_k$ is constant for all $k$. Thus, the minimum is achieved when all probabilities are equal: $p_i = p = (P_{A\\to B})^{1/M}$.\n\nTherefore, the optimal strategy for placing interfaces under the assumption of equal $N_i$ is to choose them such that all conditional probabilities $p_i$ are made equal.\n\nNow, we evaluate each option based on this derivation.\n\n**A. Start from $\\lambda_{0}$ and choose a target success probability $p^{\\star}\\in(0,1)$ (e.g., $p^{\\star}\\approx 0.2$). For each $i$, use short pilot batches of $K$ trials from $\\lambda_{i}$ to bracket two candidate positions $\\lambda_{i+1}^{-}$ and $\\lambda_{i+1}^{+}$ such that the observed success fractions straddle $p^{\\star}$, then apply a bisection search in $\\lambda$ with fresh pilot trials until the estimated $\\hat{p}_{i}\\approx p^{\\star}$ within a tolerance. Repeat to construct $\\lambda_{i+1}$, $i\\leftarrow i+1$, until $\\lambda_{M}=\\lambda_{B}$. In production, allocate $N_{i}$ roughly equally across interfaces (or according to small adjustments if per-interface costs differ). Justification: with binomial variance $\\mathrm{Var}(\\hat{p}_{i})=p_{i}(1-p_{i})/N_{i}$ and independence, a delta-method expansion gives the relative variance $\\mathrm{Var}(\\hat{P}_{A\\to B})/P_{A\\to B}^{2}\\approx \\sum_{i}(1-p_{i})/(p_{i}N_{i})$. For fixed $\\prod_{i}p_{i}$ and fixed $\\{N_{i}\\}$, this sum is minimized when all $p_{i}$ are equal, so adaptively enforcing $p_{i}\\approx p^{\\star}$ reduces variance for a fixed total cost.**\n\nThis option proposes an adaptive algorithm that directly aims to make all $p_i$ equal to a target value $p^\\star$. The described procedure using pilot runs and a bisection search is a practical and robust way to achieve this. The justification provided is precisely the one derived above: it correctly states the formula for the relative variance and asserts that for fixed $\\{N_i\\}$, the minimum is achieved when all $p_i$ are equal. This is a correct statement. Therefore, both the proposed algorithm and its justification are sound. **Correct**.\n\n**B. Place interfaces uniformly in $\\lambda$: $\\lambda_{i}=\\lambda_{0}+i(\\lambda_{B}-\\lambda_{0})/M$. During sampling, adapt $N_{i}$ so that each interface achieves the same number of successful trials $N_{i} \\hat{p}_{i}$, which equalizes information content and therefore minimizes variance. Justification: equal successes imply balanced contributions to the product, which is optimal.**\n\nThis option is flawed. Firstly, placing interfaces uniformly in $\\lambda$ is a naive, non-adaptive strategy that will typically result in highly non-uniform probabilities $p_i$. Secondly, the proposal to adapt $N_i$ such that $N_i p_i$ is constant is not optimal. The optimal allocation of $N_i$ for a given set of $\\{p_i\\}$ is $N_i \\propto \\sqrt{(1-p_i)/p_i}$, not $N_i \\propto 1/p_i$. The justification provided is vague (\"equalizes information content\") and incorrect. As shown in the thought process, this strategy actually maximizes a term related to the variance, making it suboptimal. **Incorrect**.\n\n**C. First compute a potential of mean force $F(\\lambda)$ and then place interfaces so that the free energy increments $\\Delta F_{i}=F(\\lambda_{i+1})-F(\\lambda_{i})$ are equal. This partitions the barrier evenly, making $p_{i}$ implicitly uniform and minimizing variance because the barrier is sampled uniformly.**\n\nThis strategy is inefficient and not generally correct. Computing the potential of mean force (PMF) across a high barrier is often more computationally expensive than the rare event calculation itself, defeating the purpose of FFS. More importantly, the assumption that equal free energy increments $\\Delta F_i$ lead to equal conditional probabilities $p_i$ is a strong oversimplification. The probability $p_i$ depends on the dynamics and the full shape of the free energy surface, specifically the competition between moving forward to $\\lambda_{i+1}$ and returning to state $A$. The justification is hand-waving and lacks rigor. **Incorrect**.\n\n**D. Make interfaces very sparse so that each $p_{i}\\ll 1$ and then compensate by choosing very large $N_{i}$ at those interfaces. Because each $\\mathrm{Var}(\\hat{p}_{i})$ then scales as $p_{i}/N_{i}$, driving $N_{i}$ large reduces the variance more efficiently than adding more interfaces.**\n\nThis approach is fundamentally misguided. The entire purpose of interface-based methods is to break down a single, very rare event (with probability $P_{A\\to B} \\ll 1$) into a series of more frequent events (with probabilities $p_i$ that are not excessively small). For a very small $p_i$, one must run an extremely large number of trials $N_i \\gg 1/p_i$ just to observe a few successes. This leads to a catastrophic loss of efficiency. The relative variance $\\mathcal{V} \\approx \\sum_i 1/(p_i N_i)$ demonstrates that small $p_i$ values are extremely costly in terms of variance. Adding more interfaces to raise the individual $p_i$ values is vastly more efficient than trying to overcome a small $p_i$ with a brute-force increase in $N_i$. **Incorrect**.\n\n**E. Estimate the committor $q(\\mathbf{x})\\equiv P(B\\ \\text{before}\\ A\\mid \\mathbf{x})$ on the fly and place interfaces at committor quantiles $q_{i}$ so that $q_{i+1}-q_{i}$ is constant. Because the committor changes linearly between interfaces, the conditional probabilities $p_{i}$ are equal and the variance is minimized without further justification.**\n\nThis option contains several theoretical and practical issues. While the committor $q(\\mathbf{x})$ is indeed the ideal reaction coordinate, it is generally unknown. Estimating it accurately is typically as difficult as the original rate calculation problem, making the suggestion circular. Furthermore, the claim that \"the committor changes linearly between interfaces\" is false; the committor is a complex, non-linear function of the system's coordinates. Finally, even if one could define interfaces as level sets of the true committor, the relationship between these interfaces and the FFS conditional probabilities $p_i$ is not as simple as implied, and it does not automatically guarantee that the $p_i$ are equal. The option provides no valid justification for variance minimization. **Incorrect**.\n\nIn summary, option A correctly describes a standard, practical adaptive algorithm for interface placement in FFS and provides a rigorous and correct justification for its effectiveness based on first-principles statistical analysis.", "answer": "$$\\boxed{A}$$", "id": "2690120"}]}