{"hands_on_practices": [{"introduction": "Understanding how gene expression is controlled begins at the promoter. This first practice grounds our study in the principles of equilibrium statistical mechanics to model the binding of regulatory proteins—activators and repressors—to DNA. By enumerating the possible microstates of a promoter and assigning them statistical weights based on protein concentrations and binding affinities, you will derive the probability of a gene being transcriptionally active. This exercise [@problem_id:2645934] is fundamental, as it provides the mathematical form of the 'input functions' that govern the production rates in more complex dynamic models of gene networks.", "problem": "Consider a promoter with a single operator that can be bound by either an activator species $A$ at concentration $a$ or a repressor species $R$ at concentration $r$. The binding sites for $A$ and $R$ overlap, so that simultaneous binding is sterically forbidden. The promoter is transcriptionally active only when $A$ is bound and $R$ is not bound. Assume dilute-solution thermodynamic equilibrium at temperature $T$, such that the law of mass action holds and the equilibrium dissociation constants $K_A$ and $K_R$ characterize the reactions $P + A \\rightleftharpoons P\\!:\\!A$ and $P + R \\rightleftharpoons P\\!:\\!R$, respectively. Assume that nonspecific background effects are negligible, the solution behaves ideally, and the reference concentration used to define $K_A$ and $K_R$ makes the ratios $a/K_A$ and $r/K_R$ dimensionless.\n\nStarting from fundamental equilibrium statistical thermodynamics and chemical kinetics (law of mass action), construct the thermodynamic model by enumerating the promoter microstates and their statistical weights, and derive the closed-form expression for the probability $p_{\\text{on}}(a,r)$ that the promoter is in the transcriptionally active state as a function of $a$, $r$, $K_A$, and $K_R$. Express your final answer as a single analytic expression. No numerical evaluation or rounding is required.", "solution": "The problem statement is a well-posed and scientifically sound exercise in equilibrium statistical mechanics applied to gene regulation. It is a standard model of competitive binding. I shall proceed with the derivation.\n\nThe system consists of a single promoter, which we denote as $P$, in a solution containing an activator molecule $A$ at concentration $a$ and a repressor molecule $R$ at concentration $r$. The system is at thermodynamic equilibrium at temperature $T$. According to the principles of statistical mechanics, the probability of finding the system in a specific microstate $i$ is given by the Boltzmann distribution, which can be expressed in terms of statistical weights. The probability $p_i$ of the system being in state $i$ is:\n$$p_i = \\frac{w_i}{Z}$$\nwhere $w_i$ is the statistical weight of state $i$, and $Z$ is the partition function, defined as the sum of all statistical weights of all accessible microstates:\n$$Z = \\sum_j w_j$$\n\nThe first step is to enumerate the accessible microstates of the promoter $P$. The problem states that the promoter has a single operator site that can be bound by either $A$ or $R$, but not both simultaneously due to steric hindrance. This defines the set of possible states:\n1.  The promoter is unbound. We denote this state as $P_{free}$.\n2.  The promoter is bound by an activator. We denote this state as $P\\!:\\!A$.\n3.  The promoter is bound by a repressor. We denote this state as $P\\!:\\!R$.\n\nA state where both $A$ and $R$ are simultaneously bound, $P\\!:\\!A\\!:\\!R$, is forbidden.\n\nThe second step is to determine the statistical weight for each of these three states. We can choose the unbound state, $P_{free}$, as our reference state and assign it a statistical weight of $1$.\n$$w_{free} = 1$$\nThe statistical weights of the other states are determined relative to this reference state using the law of mass action for the binding equilibria.\n\nFor the binding of the activator $A$ to the promoter $P$:\n$$P + A \\rightleftharpoons P\\!:\\!A$$\nThe equilibrium dissociation constant, $K_A$, is defined as:\n$$K_A = \\frac{[P_{free}][A]}{[P\\!:\\!A]}$$\nwhere $[...]$ denotes concentration at equilibrium. Substituting the given concentration $a$ for $[A]$, we can express the ratio of the concentration of the bound state to the unbound state:\n$$\\frac{[P\\!:\\!A]}{[P_{free}]} = \\frac{a}{K_A}$$\nThis ratio represents the statistical weight of the activator-bound state, $P\\!:\\!A$, relative to the free state. The problem correctly specifies that this ratio is dimensionless.\n$$w_{A} = \\frac{a}{K_A}$$\n\nSimilarly, for the binding of the repressor $R$ to the promoter $P$:\n$$P + R \\rightleftharpoons P\\!:\\!R$$\nThe equilibrium dissociation constant, $K_R$, is defined as:\n$$K_R = \\frac{[P_{free}][R]}{[P\\!:\\!R]}$$\nSubstituting the given concentration $r$ for $[R]$, we find the statistical weight of the repressor-bound state, $P\\!:\\!R$:\n$$\\frac{[P\\!:\\!R]}{[P_{free}]} = \\frac{r}{K_R}$$\nThus,\n$$w_{R} = \\frac{r}{K_R}$$\n\nThe third step is to construct the partition function, $Z$, by summing the statistical weights of all accessible states:\n$$Z = w_{free} + w_{A} + w_{R} = 1 + \\frac{a}{K_A} + \\frac{r}{K_R}$$\nThis partition function is the normalization factor for the probability distribution of the promoter states.\n\nThe final step is to calculate the probability of the event of interest. The problem defines the transcriptionally active state, or \"on\" state, as the one where the activator $A$ is bound to the promoter. This corresponds to the $P\\!:\\!A$ microstate. The probability of this state, $p_{\\text{on}}(a,r)$, is the ratio of its statistical weight, $w_{A}$, to the total partition function, $Z$.\n$$p_{\\text{on}}(a,r) = \\frac{w_{A}}{Z}$$\nSubstituting the expressions for $w_{A}$ and $Z$:\n$$p_{\\text{on}}(a,r) = \\frac{\\frac{a}{K_A}}{1 + \\frac{a}{K_A} + \\frac{r}{K_R}}$$\nThis expression gives the probability that the promoter is in the transcriptionally active state as a function of the activator concentration $a$, the repressor concentration $r$, and their respective dissociation constants $K_A$ and $K_R$. This is the final, closed-form analytical expression required.", "answer": "$$\\boxed{\\frac{\\frac{a}{K_A}}{1 + \\frac{a}{K_A} + \\frac{r}{K_R}}}$$", "id": "2645934"}, {"introduction": "Having established how to model the input-output function of a single promoter, we now explore how these components can be wired into a network to create emergent behaviors. The genetic toggle switch, composed of two mutually repressing genes, is a classic synthetic biology motif that can produce bistability, a cellular memory system. This practice [@problem_id:2645867] will guide you through a linear stability analysis to determine the precise conditions under which this network transitions from a single stable state to two, a phenomenon known as a pitchfork bifurcation. This analysis is a cornerstone technique for understanding how network architecture shapes cellular decision-making.", "problem": "Consider a symmetric two-gene toggle-switch in which each gene represses the other with Hill-type cooperativity. Let $x(t)$ and $y(t)$ denote the concentrations of the two gene products. The dynamics are modeled by the ordinary differential equations\n$$\n\\frac{dx}{dt} \\;=\\; \\frac{\\beta}{1 + \\left(\\frac{y}{K}\\right)^{n}} \\;-\\; \\gamma\\, x,\n\\qquad\n\\frac{dy}{dt} \\;=\\; \\frac{\\beta}{1 + \\left(\\frac{x}{K}\\right)^{n}} \\;-\\; \\gamma\\, y,\n$$\nwhere $n$ is the Hill coefficient (dimensionless), $\\beta$ is the maximal synthesis rate, $\\gamma$ is the first-order dilution/degradation rate, and $K$ is the dissociation constant. Assume parameter symmetry between the two genes.\n\n1) Compute the Jacobian matrix at a symmetric fixed point $\\left(x^{\\ast},y^{\\ast}\\right)$ satisfying $x^{\\ast} = y^{\\ast}$. Express the eigenvalues in terms of $\\gamma$ and the derivative of the repression input-output function evaluated at $x^{\\ast}=y^{\\ast}$.\n\n2) Using linear stability analysis of this Jacobian, derive the condition under which the symmetric fixed point loses stability via a pitchfork bifurcation as $n$ increases. Your derivation should start from first principles of chemical kinetics and the Hill repression form, without presupposing any specialized bifurcation formulas.\n\n3) Specialize your result to the following parameter set: $\\beta = 10$ nM/min, $\\gamma = 0.1$ min$^{-1}$, and $K = 50$ nM. Compute the critical Hill coefficient $n_{\\mathrm{c}}$ at which the pitchfork bifurcation occurs. Give your final answer as an exact number with no rounding and no units. If multiple candidate values arise, report the smallest value $n  1$ that satisfies the derived onset condition.\n\nExpress the final answer as a single number. No units should be included in the final answer box. No rounding is required. The analysis must remain scientifically consistent with the given reaction-network model and parameters.", "solution": "The problem describes a symmetric genetic toggle-switch modeled by a system of two ordinary differential equations:\n$$\n\\frac{dx}{dt} = \\frac{\\beta}{1 + \\left(\\frac{y}{K}\\right)^{n}} - \\gamma x\n$$\n$$\n\\frac{dy}{dt} = \\frac{\\beta}{1 + \\left(\\frac{x}{K}\\right)^{n}} - \\gamma y\n$$\nLet us denote the right-hand sides as $f(x, y)$ and $g(x, y)$. A symmetric fixed point $(x^*, y^*)$ is defined by the conditions $\\frac{dx}{dt} = 0$, $\\frac{dy}{dt} = 0$, and $x^* = y^*$. For such a point, the two dynamic equations become identical, yielding a single condition for the steady-state concentration $x^*$:\n$$\n\\gamma x^* = \\frac{\\beta}{1 + \\left(\\frac{x^*}{K}\\right)^{n}}\n$$\n\n1) To compute the Jacobian matrix at the symmetric fixed point, we first define the Hill repression function $H(z) = \\frac{\\beta}{1 + (z/K)^{n}}$. The system dynamics are then $\\frac{dx}{dt} = H(y) - \\gamma x$ and $\\frac{dy}{dt} = H(x) - \\gamma y$. The Jacobian matrix $J$ has elements $J_{ij} = \\frac{\\partial \\dot{x}_i}{\\partial x_j}$.\n$$\nJ(x, y) = \\begin{pmatrix} \\frac{\\partial}{\\partial x}(H(y) - \\gamma x)  \\frac{\\partial}{\\partial y}(H(y) - \\gamma x) \\\\ \\frac{\\partial}{\\partial x}(H(x) - \\gamma y)  \\frac{\\partial}{\\partial y}(H(x) - \\gamma y) \\end{pmatrix} = \\begin{pmatrix} -\\gamma  H'(y) \\\\ H'(x)  -\\gamma \\end{pmatrix}\n$$\nwhere $H'(z)$ is the derivative of $H(z)$ with respect to its argument $z$. At the symmetric fixed point $(x^*, x^*)$, the Jacobian becomes:\n$$\nJ^* = J(x^*, x^*) = \\begin{pmatrix} -\\gamma  H'(x^*) \\\\ H'(x^*)  -\\gamma \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $J^*$ are found by solving the characteristic equation $\\det(J^* - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} -\\gamma - \\lambda  H'(x^*) \\\\ H'(x^*)  -\\gamma - \\lambda \\end{pmatrix} = (-\\gamma - \\lambda)^2 - (H'(x^*))^2 = 0\n$$\nThis gives $(-\\gamma - \\lambda) = \\pm H'(x^*)$, which solves to $\\lambda = -\\gamma \\mp H'(x^*)$. The two eigenvalues are therefore:\n$$\n\\lambda_{1} = -\\gamma - H'(x^*) \\quad \\text{and} \\quad \\lambda_{2} = -\\gamma + H'(x^*)\n$$\nThis expression provides the eigenvalues in terms of the degradation rate $\\gamma$ and the derivative of the repression function $H'(x^*)$ evaluated at the fixed point, as requested.\n\n2) The stability of the symmetric fixed point is determined by the sign of the real parts of its eigenvalues. Since $H(z)$ is a monotonically decreasing function for $z  0$, its derivative $H'(x^*)$ is strictly negative. The second eigenvalue, $\\lambda_{2} = -\\gamma + H'(x^*)$, is the sum of two negative terms and is therefore always negative. This eigenvalue corresponds to perturbations along the symmetric manifold ($x=y$), which are always stable. The first eigenvalue, $\\lambda_{1} = -\\gamma - H'(x^*)$, governs the stability with respect to anti-symmetric perturbations that break the $x=y$ symmetry. The fixed point is stable as long as $\\lambda_1  0$. The fixed point loses stability when $\\lambda_1$ crosses zero from a negative value. The onset of instability, which corresponds to the bifurcation point, occurs precisely when $\\lambda_1 = 0$. The condition for the bifurcation is therefore:\n$$\n\\lambda_1 = -\\gamma - H'(x^*) = 0 \\implies H'(x^*) = -\\gamma\n$$\nDue to the inherent $x \\leftrightarrow y$ symmetry of the system, this bifurcation, where a single symmetric fixed point becomes unstable and gives rise to two new stable asymmetric fixed points, is a pitchfork bifurcation.\n\n3) To find the critical Hill coefficient $n_c$, we must solve the system of two equations that hold at the bifurcation point:\n(i) The fixed point condition: $\\gamma x^* = H(x^*) = \\frac{\\beta}{1 + (x^*/K)^{n_c}}$\n(ii) The bifurcation condition: $-\\gamma = H'(x^*) = -\\frac{\\beta n_c}{K} \\frac{(x^*/K)^{n_c-1}}{(1 + (x^*/K)^{n_c})^2}$\n\nFrom (ii), we get $\\gamma = \\frac{\\beta n_c}{K} \\frac{(x^*/K)^{n_c-1}}{(1 + (x^*/K)^{n_c})^2}$. We can substitute $\\beta$ from (i) into this expression: $\\beta = \\gamma x^* (1 + (x^*/K)^{n_c})$.\n$$\n\\gamma = \\frac{\\gamma x^* (1 + (x^*/K)^{n_c}) n_c}{K} \\frac{(x^*/K)^{n_c-1}}{(1 + (x^*/K)^{n_c})^2}\n$$\nAssuming $\\gamma \\neq 0$, we cancel $\\gamma$ and simplify:\n$$\n1 = \\frac{x^* n_c}{K} \\frac{(x^*/K)^{n_c-1}}{1+(x^*/K)^{n_c}} = n_c \\frac{(x^*/K)^{n_c}}{1+(x^*/K)^{n_c}}\n$$\nLetting $z_c = (x^*/K)^{n_c}$, this equation becomes $1 = n_c z_c / (1+z_c)$, which solves to $1+z_c = n_c z_c$, or $z_c = 1/(n_c-1)$. Thus, a general condition for the bifurcation is:\n$$\n\\left(\\frac{x^*}{K}\\right)^{n_c} = \\frac{1}{n_c-1}\n$$\nNow we use the specific parameter set: $\\beta = 10 \\, \\text{nM/min}$, $\\gamma = 0.1 \\, \\text{min}^{-1}$, and $K = 50 \\, \\text{nM}$.\nLet us evaluate the product $\\gamma K$:\n$\\gamma K = (0.1_ \\, \\text{min}^{-1}) \\cdot (50 \\, \\text{nM}) = 5 \\, \\text{nM/min}$.\nLet us evaluate $\\beta/2$:\n$\\beta/2 = (10 \\, \\text{nM/min}) / 2 = 5 \\, \\text{nM/min}$.\nWe observe that these parameters satisfy the special condition $\\gamma K = \\beta/2$.\nLet's examine the fixed point equation $\\gamma x^* = \\beta / (1+(x^*/K)^n)$ under this condition. If we evaluate the equation at the point $x^*=K$, we get $\\gamma K = \\beta / (1+(K/K)^n) = \\beta/2$. Since our parameters satisfy $\\gamma K = \\beta/2$, it means that for this particular system, the symmetric fixed point is located at $x^*=K$ for any value of $n$.\nThis observation greatly simplifies the problem. We can substitute $x^*=K$ into the general bifurcation condition we derived:\n$$\n\\left(\\frac{K}{K}\\right)^{n_c} = \\frac{1}{n_c-1}\n$$\n$$\n1^{n_c} = \\frac{1}{n_c-1} \\implies 1 = \\frac{1}{n_c-1}\n$$\nSolving for $n_c$ yields $n_c - 1 = 1$, so the critical Hill coefficient is:\n$$\nn_c = 2\n$$\nThis value satisfies the condition $n  1$ and represents the smallest such value for the bifurcation.", "answer": "$$\\boxed{2}$$", "id": "2645867"}, {"introduction": "A predictive model is only as good as its parameters, but how can we know if these parameters can even be determined from experimental data? This final practice bridges the gap between theoretical modeling and experimental reality by introducing the concept of parameter identifiability. You will use the Fisher Information Matrix ($FIM$) to analyze a model of negative autoregulation and discover how the choice of experimental measurements can leave some parameters practically unknowable, a property known as 'sloppiness'. This exercise [@problem_id:2645879] is crucial for any modeler, as it teaches a formal method to assess the testability of a model and to guide future experimental design.", "problem": "Consider a single-gene negative autoregulatory module with protein concentration $x(t)$ governed by the ordinary differential equation (ODE)\n$$\n\\frac{dx}{dt} \\;=\\; \\frac{\\alpha}{1 + \\left(\\frac{x}{K}\\right)^{n}} \\;-\\; \\beta\\, x,\n$$\nwhere $\\alpha0$ is the maximal synthesis rate, $K0$ is the dissociation constant of the repressor-promoter interaction, $n0$ is the Hill coefficient, and $\\beta0$ is the first-order degradation/dilution rate. You observe the system at times $t_i$ and record noisy measurements\n$$\ny_i \\;=\\; x(t_i) \\;+\\; \\varepsilon_i,\\quad i=1,\\dots,m,\n$$\nwhere the noise $\\varepsilon_i$ are independent and identically distributed (i.i.d.) Gaussian with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. Assume the experimental protocol ensures that each $t_i$ is sufficiently large for the system to have relaxed to its unique steady state $x^{\\ast}$ corresponding to the current parameters, so that $x(t_i)=x^{\\ast}$ for all $i$.\n\nYou are interested in the Fisher Information Matrix (FIM) for the parameter vector $\\boldsymbol{\\theta} = (\\alpha, K, n)$ under this steady-state measurement design. Work at a nominal operating point where the steady state satisfies $x^{\\ast}=K$ and the parameters satisfy $\\alpha = 2 \\beta K$ with arbitrary $n0$ and known $\\beta0$. Starting from the definition of the log-likelihood for i.i.d. Gaussian errors and fundamental properties of steady states of ODEs, derive the Fisher Information Matrix with respect to $\\boldsymbol{\\theta}$ at this operating point. Analyze the resulting parameter correlations and the presence of sloppiness in this design by characterizing the rank and eigenstructure of the FIM.\n\nAnswer specification:\n- Report, as your final answer, a single closed-form analytic expression for the largest eigenvalue of the Fisher Information Matrix at the specified operating point. Express your answer in terms of $m$, $\\sigma$, $\\beta$, and $n$ only.\n- No rounding is required. Do not include units in your final answer.", "solution": "First, we verify that the specified operating point is a valid, self-consistent steady state of the system. The parameter vector of interest is $\\boldsymbol{\\theta} = (\\alpha, K, n)$, with $\\beta$ considered known. At steady state, $\\frac{dx}{dt} = 0$, which implies:\n$$\n\\beta x^{\\ast} = \\frac{\\alpha}{1 + \\left(\\frac{x^{\\ast}}{K}\\right)^{n}}\n$$\nSubstituting the specified operating point conditions $x^{\\ast} = K$ and $\\alpha = 2 \\beta K$ into this equation yields:\n$$\n\\beta K = \\frac{2 \\beta K}{1 + \\left(\\frac{K}{K}\\right)^{n}} = \\frac{2 \\beta K}{1 + 1^n}\n$$\nSince $n  0$, $1^{n}=1$, and the equation becomes $\\beta K = \\frac{2 \\beta K}{2}$, which is a true identity. This confirms the operating point is a valid steady state.\n\nThe derivation of the Fisher Information Matrix (FIM) commences from the log-likelihood function for the observed data. For $m$ i.i.d. Gaussian measurements $y_i \\sim \\mathcal{N}(x^{\\ast}, \\sigma^2)$, the log-likelihood function $L(\\boldsymbol{\\theta})$ is:\n$$\nL(\\boldsymbol{\\theta}) = \\sum_{i=1}^{m} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x^{\\ast}(\\boldsymbol{\\theta}))^2}{2\\sigma^2}\\right) \\right) = -\\frac{m}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{m} (y_i - x^{\\ast}(\\boldsymbol{\\theta}))^2\n$$\nThe FIM, denoted by $F$, has elements $F_{jk}$ defined as $F_{jk} = -E\\left[\\frac{\\partial^2 L}{\\partial\\theta_j \\partial\\theta_k}\\right]$. For this model with additive Gaussian noise, the FIM simplifies to:\n$$\nF_{jk} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{m} \\frac{\\partial x^{\\ast}}{\\partial \\theta_j} \\frac{\\partial x^{\\ast}}{\\partial \\theta_k}\n$$\nSince the steady state $x^{\\ast}$ is the same for all $m$ measurements, this becomes:\n$$\nF_{jk} = \\frac{m}{\\sigma^2} \\frac{\\partial x^{\\ast}}{\\partial \\theta_j} \\frac{\\partial x^{\\ast}}{\\partial \\theta_k}\n$$\nThis shows that the FIM can be written as $F = \\frac{m}{\\sigma^2} J^T J$, where $J$ is the sensitivity row vector $J = \\left[ \\frac{\\partial x^{\\ast}}{\\partial \\alpha}, \\frac{\\partial x^{\\ast}}{\\partial K}, \\frac{\\partial x^{\\ast}}{\\partial n} \\right]$.\n\nThe sensitivities $\\frac{\\partial x^{\\ast}}{\\partial \\theta_j}$ are calculated using implicit differentiation of the steady-state equation, $f(x^{\\ast}, \\boldsymbol{\\theta}) = 0$, where:\n$$\nf(x, \\boldsymbol{\\theta}) = \\frac{\\alpha}{1 + (x/K)^n} - \\beta x = 0\n$$\nDifferentiating with respect to a parameter $\\theta_j$ gives:\n$$\n\\frac{\\partial f}{\\partial x^{\\ast}} \\frac{\\partial x^{\\ast}}{\\partial \\theta_j} + \\frac{\\partial f}{\\partial \\theta_j} = 0 \\quad \\implies \\quad \\frac{\\partial x^{\\ast}}{\\partial \\theta_j} = - \\frac{\\partial f / \\partial \\theta_j}{\\partial f / \\partial x^{\\ast}}\n$$\nWe compute the necessary partial derivatives of $f(x, \\boldsymbol{\\theta}) = \\alpha(1+(x/K)^n)^{-1} - \\beta x$:\n$$\n\\frac{\\partial f}{\\partial \\alpha} = \\frac{1}{1+(x/K)^n}\n$$\n$$\n\\frac{\\partial f}{\\partial K} = -\\alpha (1+(x/K)^n)^{-2} \\left[ n(x/K)^{n-1} \\left(-\\frac{x}{K^2}\\right) \\right] = \\frac{\\alpha n x^n K^{-n-1}}{(1+(x/K)^n)^2}\n$$\n$$\n\\frac{\\partial f}{\\partial n} = -\\alpha (1+(x/K)^n)^{-2} \\left[ (x/K)^n \\ln(x/K) \\right] = -\\frac{\\alpha (x/K)^n \\ln(x/K)}{(1+(x/K)^n)^2}\n$$\n$$\n\\frac{\\partial f}{\\partial x} = -\\alpha (1+(x/K)^n)^{-2} \\left[ n(x/K)^{n-1} \\left(\\frac{1}{K}\\right) \\right] - \\beta = -\\frac{\\alpha n x^{n-1} K^{-n}}{(1+(x/K)^n)^2} - \\beta\n$$\nNow, we evaluate these derivatives at the specified operating point: $x=x^{\\ast}=K$ and $\\alpha=2\\beta K$. At this point, $x/K = 1$.\n$$\n\\left.\\frac{\\partial f}{\\partial \\alpha}\\right|_{*} = \\frac{1}{1+1^n} = \\frac{1}{2}\n$$\n$$\n\\left.\\frac{\\partial f}{\\partial K}\\right|_{*} = \\frac{(2\\beta K) n K^n K^{-n-1}}{(1+1^n)^2} = \\frac{2\\beta n K^{-1}}{4} = \\frac{\\beta n}{2}\n$$\n$$\n\\left.\\frac{\\partial f}{\\partial n}\\right|_{*} = -\\frac{(2\\beta K) (1)^n \\ln(1)}{(1+1^n)^2} = 0, \\text{ since } \\ln(1)=0\n$$\n$$\n\\left.\\frac{\\partial f}{\\partial x}\\right|_{*} = -\\frac{(2\\beta K) n K^{n-1} K^{-n}}{(1+1^n)^2} - \\beta = -\\frac{2\\beta n K^{-1}}{4} - \\beta = -\\frac{\\beta n}{2} - \\beta = -\\beta\\left(1+\\frac{n}{2}\\right) = -\\frac{\\beta(n+2)}{2}\n$$\nUsing these results, we find the sensitivities at the operating point:\n$$\n\\frac{\\partial x^{\\ast}}{\\partial \\alpha} = - \\frac{1/2}{-\\beta(n+2)/2} = \\frac{1}{\\beta(n+2)}\n$$\n$$\n\\frac{\\partial x^{\\ast}}{\\partial K} = - \\frac{\\beta n/2}{-\\beta(n+2)/2} = \\frac{n}{n+2}\n$$\n$$\n\\frac{\\partial x^{\\ast}}{\\partial n} = - \\frac{0}{-\\beta(n+2)/2} = 0\n$$\nThe sensitivity vector is $J = \\left[ \\frac{1}{\\beta(n+2)}, \\frac{n}{n+2}, 0 \\right]$. The FIM is $F = \\frac{m}{\\sigma^2} J^T J$. This is a rank-one matrix:\n$$\nF = \\frac{m}{\\sigma^2}\n\\begin{pmatrix}\n\\frac{1}{\\beta(n+2)} \\\\\n\\frac{n}{n+2} \\\\\n0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{\\beta(n+2)}  \\frac{n}{n+2}  0\n\\end{pmatrix}\n= \\frac{m}{\\sigma^2}\n\\begin{pmatrix}\n\\frac{1}{\\beta^2(n+2)^2}  \\frac{n}{\\beta(n+2)^2}  0 \\\\\n\\frac{n}{\\beta(n+2)^2}  \\frac{n^2}{(n+2)^2}  0 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nThe matrix $F$ is singular. The zero row and column corresponding to the parameter $n$ indicate that $n$ is locally unidentifiable from steady-state measurements at this operating point, a clear demonstration of parametric sloppiness.\nA rank-one matrix of the form $c \\mathbf{v}\\mathbf{v}^T$ has exactly one non-zero eigenvalue, given by $\\lambda = c (\\mathbf{v}^T \\mathbf{v}) = c \\|\\mathbf{v}\\|^2$. All other eigenvalues are zero. The largest eigenvalue, $\\lambda_{\\text{max}}$, is this sole non-zero eigenvalue.\nIn our case, $c = m/\\sigma^2$ and the vector is the transpose of the Jacobian, $\\mathbf{v} = J^T$. Thus, the largest eigenvalue is:\n$$\n\\lambda_{\\text{max}} = \\frac{m}{\\sigma^2} \\|J\\|^2 = \\frac{m}{\\sigma^2} \\left( \\left(\\frac{1}{\\beta(n+2)}\\right)^2 + \\left(\\frac{n}{n+2}\\right)^2 + 0^2 \\right)\n$$\n$$\n\\lambda_{\\text{max}} = \\frac{m}{\\sigma^2} \\left( \\frac{1}{\\beta^2(n+2)^2} + \\frac{n^2}{(n+2)^2} \\right) = \\frac{m}{\\sigma^2} \\frac{1 + n^2\\beta^2}{\\beta^2(n+2)^2}\n$$\nThis is the required expression.", "answer": "$$\n\\boxed{\\frac{m(1 + n^{2}\\beta^{2})}{\\sigma^{2}\\beta^{2}(n+2)^{2}}}\n$$", "id": "2645879"}]}