{"hands_on_practices": [{"introduction": "Mastering the Metropolis algorithm begins with understanding the fundamental mechanics of a simulation. Before we can explore complex energy landscapes, we must ensure our simulation correctly handles the geometry imposed by periodic boundary conditions and interaction cutoffs. This foundational exercise challenges you to determine the maximum allowable displacement for a trial move, a critical parameter that guarantees the integrity of particle interaction calculations within the minimum image convention. [@problem_id:2788157]", "problem": "A homogeneous classical fluid of identical particles is simulated with the Metropolis Monte Carlo (MMC) algorithm in a three-dimensional cubic box of side length $L$ with Periodic Boundary Conditions (PBC). Pair interactions are finite-ranged, and the system employs a spherical cutoff of radius $r_{c}$ together with the Minimum Image Convention (MIC), which defines pair separations by wrapping each Cartesian component of the separation into the interval $\\left(-\\frac{L}{2}, \\frac{L}{2}\\right]$. The number density is $\\rho$, the total number of particles is $N$, and the box length satisfies the definition of number density $\\,\\rho = \\frac{N}{L^{3}}\\,$. The cutoff satisfies $\\,r_{c}  \\frac{L}{2}\\,$ so that the MIC is unambiguous.\n\nThe MMC trial move for a single, randomly selected particle consists of adding a random displacement vector $\\boldsymbol{\\Delta}$ with magnitude bounded by a tunable parameter $\\delta_{\\max}$, i.e., $|\\boldsymbol{\\Delta}| \\le \\delta_{\\max}$. To ensure that no pairwise interaction beyond the nearest periodic image can ever be required when evaluating the energy change of a single-particle trial move, you adopt the following translationally invariant protocol: before proposing the displacement for the selected particle, you shift all particle positions by a common vector (which is permissible under PBC) so that the selected particle is exactly at the geometric center of the primary simulation cell. You then apply the displacement $\\boldsymbol{\\Delta}$ and wrap coordinates back into the primary cell as needed.\n\nUnder this protocol and using only the foundational definitions above, determine the largest displacement magnitude $\\delta_{\\max}$ such that, after any single-particle trial move, the entire interaction sphere of radius $r_{c}$ centered on the trial position is guaranteed to lie within the region where the MIC selects the central image uniquely (so that, by construction, no interaction beyond the cutoff with a non-nearest image is ever needed). Express your answer as a closed-form analytic expression in terms of $N$, $\\rho$, and $r_{c}$. The final answer must be a single analytic expression. No rounding is required, and no numerical values are provided or needed.", "solution": "The problem statement has been subjected to rigorous validation and is deemed to be scientifically grounded, well-posed, objective, and internally consistent. It presents a solvable problem in the domain of theoretical chemistry and computational physics. We shall proceed with the derivation.\n\nThe system is a cubic simulation box of side length $L$ with Periodic Boundary Conditions (PBC). The coordinates are defined within the primary simulation cell, which we take to be the cube $C$ centered at the origin, with vertices at $(\\pm \\frac{L}{2}, \\pm \\frac{L}{2}, \\pm \\frac{L}{2})$. Thus, any position vector $\\mathbf{r} = (x, y, z)$ within this cell has components satisfying $-\\frac{L}{2}  x \\le \\frac{L}{2}$, and similarly for $y$ and $z$.\n\nThe problem describes a specific protocol for a Metropolis Monte Carlo (MMC) trial move. For a randomly selected particle, say particle $i$, all particle positions in the system are shifted by a common vector such that particle $i$ is relocated to the geometric center of the primary cell, which is the origin $\\mathbf{0} = (0, 0, 0)$. Then, a random displacement vector $\\boldsymbol{\\Delta}$ is added to the position of particle $i$, resulting in its new trial position $\\mathbf{r}_{i}^{\\text{new}} = \\boldsymbol{\\Delta}$. The magnitude of the displacement is bounded by $|\\boldsymbol{\\Delta}| \\le \\delta_{\\max}$.\n\nThe core requirement is to find the largest value of $\\delta_{\\max}$ which guarantees that \"the entire interaction sphere of radius $r_{c}$ centered on the trial position is guaranteed to lie within the region where the MIC selects the central image uniquely\". The \"region where the MIC selects the central image uniquely\" refers to the primary simulation cell $C$ itself. For any position vector $\\mathbf{p}$ inside this cell, the Minimum Image Convention (MIC) returns $\\mathbf{p}$ as its own nearest image vector. If any part of the interaction sphere were to lie outside this primary cell, one would need to consider distances to particles whose central images are near the opposite face of the box, which is what the protocol is designed to avoid.\n\nTherefore, the condition is that the sphere of radius $r_c$ centered at the new trial position $\\mathbf{r}_{i}^{\\text{new}} = \\boldsymbol{\\Delta}$ must be fully contained within the primary cell $C = [-\\frac{L}{2}, \\frac{L}{2}]^3$.\n\nLet the trial position be $\\mathbf{r}_{i}^{\\text{new}} = \\boldsymbol{\\Delta} = (\\Delta_x, \\Delta_y, \\Delta_z)$. A sphere $S$ of radius $r_c$ is centered at this point. For this sphere to be contained within the cube defined by $-\\frac{L}{2} \\le x,y,z \\le \\frac{L}{2}$, the maximum and minimum extent of the sphere in each Cartesian direction must fall within this range.\n\nConsider the $x$-component. The extent of the sphere along the $x$-axis is from $\\Delta_x - r_c$ to $\\Delta_x + r_c$. We must satisfy the following two inequalities:\n$$\n\\Delta_x + r_c \\le \\frac{L}{2}\n$$\n$$\n\\Delta_x - r_c \\ge -\\frac{L}{2}\n$$\nThese two inequalities can be combined into a single condition on the absolute value of the component $\\Delta_x$:\n$$\n|\\Delta_x| \\le \\frac{L}{2} - r_c\n$$\nThis condition must hold for all three Cartesian components, so we require $|\\Delta_\\alpha| \\le \\frac{L}{2} - r_c$ for $\\alpha \\in \\{x, y, z\\}$.\n\nThis constraint must be satisfied for *any* possible trial displacement vector $\\boldsymbol{\\Delta}$ such that $|\\boldsymbol{\\Delta}| \\le \\delta_{\\max}$. To guarantee this, we must consider the worst-case scenario, which is the displacement that produces the largest possible magnitude for one of its components, $|\\Delta_\\alpha|$. Given the constraint $|\\boldsymbol{\\Delta}| = \\sqrt{\\Delta_x^2 + \\Delta_y^2 + \\Delta_z^2} \\le \\delta_{\\max}$, the maximum value that any single component, for instance $|\\Delta_x|$, can attain is $\\delta_{\\max}$. This occurs when the displacement vector is aligned with that coordinate axis, e.g., $\\boldsymbol{\\Delta} = (\\delta_{\\max}, 0, 0)$.\n\nTo ensure our condition holds for all valid displacements, we must impose it on this worst-case magnitude. Substituting $|\\Delta_\\alpha| = \\delta_{\\max}$ into the derived inequality gives:\n$$\n\\delta_{\\max} \\le \\frac{L}{2} - r_c\n$$\nThe problem asks for the largest displacement magnitude $\\delta_{\\max}$ that satisfies this guarantee. This is the equality limit:\n$$\n\\delta_{\\max} = \\frac{L}{2} - r_c\n$$\nThe problem statement requires the final answer to be expressed in terms of the number of particles $N$, the number density $\\rho$, and the cutoff radius $r_c$. The number density is given by the definition $\\rho = \\frac{N}{L^3}$. We can solve for the box length $L$:\n$$\nL^3 = \\frac{N}{\\rho} \\implies L = \\left(\\frac{N}{\\rho}\\right)^{1/3}\n$$\nSubstituting this expression for $L$ into our equation for $\\delta_{\\max}$ yields the final result:\n$$\n\\delta_{\\max} = \\frac{1}{2}\\left(\\frac{N}{\\rho}\\right)^{1/3} - r_c\n$$\nThis expression is the largest possible value for the maximum displacement parameter $\\delta_{\\max}$ consistent with the stated protocol and geometric constraints. The initial given condition $r_c  L/2$ ensures that $\\delta_{\\max}$ is a positive, physically meaningful quantity.", "answer": "$$\n\\boxed{\\frac{1}{2}\\left(\\frac{N}{\\rho}\\right)^{1/3} - r_{c}}\n$$", "id": "2788157"}, {"introduction": "Once a simulation is correctly implemented, the focus shifts to efficiency: how can we explore the configuration space as quickly as possible? The size of the proposed trial moves is a crucial tuning parameter that dictates the algorithm's performance. This problem guides you through a classic theoretical derivation to find the optimal acceptance rate for random-walk Metropolis in high-dimensional systems, revealing the origin of a widely used rule of thumb in computational chemistry. [@problem_id:2788234]", "problem": "A common scenario in theoretical chemistry is the sampling of the configurational Boltzmann distribution for a many-degree-of-freedom system near a local minimum of the potential energy surface, where the potential can be approximated as harmonic in appropriately mass-weighted normal modes. Consider a system with $d$ independent normal modes with potential energy $U(\\mathbf{x}) = \\frac{1}{2} \\sum_{i=1}^{d} x_{i}^{2}$ in nondimensional units such that the target distribution for the coordinates $\\mathbf{x} \\in \\mathbb{R}^{d}$ at inverse temperature $\\beta = 1$ is $ \\pi(\\mathbf{x}) \\propto \\exp\\!\\left(-U(\\mathbf{x})\\right)$, that is, a $d$-dimensional standard normal distribution.\n\nWe perform local Metropolis random-walk proposals of the form $\\mathbf{y} = \\mathbf{x} + s\\,\\boldsymbol{\\eta}$ with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$ independent of $\\mathbf{x}$. The Metropolis acceptance probability is $a(\\mathbf{x},\\mathbf{y}) = \\min\\!\\left\\{1, \\exp\\!\\left[\\log \\pi(\\mathbf{y}) - \\log \\pi(\\mathbf{x})\\right]\\right\\}$, and the chain is assumed to be at stationarity so that $\\mathbf{x} \\sim \\pi$. To study the high-dimensional behavior, consider the scaling $s = \\ell / \\sqrt{d}$ with fixed $\\ell  0$ and let $d \\to \\infty$.\n\nStarting only from the definitions above, the properties of the standard normal distribution, and classical limit theorems of probability, carry out the following steps:\n1. Derive the $d \\to \\infty$ limit of the average acceptance probability $\\alpha(\\ell) = \\mathbb{E}[a(\\mathbf{x},\\mathbf{y})]$ as a function of $\\ell$, expressing it in terms of the standard normal cumulative distribution function. You may use the fact that for independent standard normal variables one has weak convergence of suitably scaled sums to a standard normal, and that empirical averages converge almost surely to their expectation.\n2. Define the asymptotic expected squared jump distance per proposal as $J(\\ell) = \\lim_{d \\to \\infty} \\mathbb{E}\\!\\left[\\|\\mathbf{y} - \\mathbf{x}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right]$ under the scaling above, and show that $J(\\ell)$ is proportional to $\\ell^{2} \\alpha(\\ell)$ in the limit.\n3. Maximize $J(\\ell)$ over $\\ell  0$, and determine the corresponding optimal acceptance probability $\\alpha^{\\star}$ in the $d \\to \\infty$ limit.\n\nReport as your final answer only the numerical value of the optimal acceptance probability $\\alpha^{\\star}$, rounded to three significant figures. The answer is dimensionless; do not include any units.", "solution": "We are given a target distribution that is a $d$-dimensional standard normal, $\\pi(\\mathbf{x}) \\propto \\exp\\!\\left(-\\|\\mathbf{x}\\|^{2}/2\\right)$, and a symmetric random-walk proposal $\\mathbf{y} = \\mathbf{x} + s\\,\\boldsymbol{\\eta}$ with $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$, $s = \\ell/\\sqrt{d}$. The Metropolis acceptance probability is\n$$\na(\\mathbf{x},\\mathbf{y}) = \\min\\!\\left\\{1, \\exp\\!\\left[\\log \\pi(\\mathbf{y}) - \\log \\pi(\\mathbf{x})\\right]\\right\\}.\n$$\nSince the proposal is symmetric, the Hastings ratio reduces to the ratio of target densities. We analyze the high-dimensional limit with $d \\to \\infty$ and fixed $\\ell  0$.\n\nStep 1: Derive the limiting average acceptance probability $\\alpha(\\ell)$.\n\nThe log-density difference is\n$$\n\\Delta \\equiv \\log \\pi(\\mathbf{y}) - \\log \\pi(\\mathbf{x}) \\;=\\; -\\frac{1}{2}\\big(\\|\\mathbf{y}\\|^{2} - \\|\\mathbf{x}\\|^{2}\\big).\n$$\nWith $\\mathbf{y} = \\mathbf{x} + s\\,\\boldsymbol{\\eta}$ and $s = \\ell/\\sqrt{d}$,\n$$\n\\|\\mathbf{y}\\|^{2} - \\|\\mathbf{x}\\|^{2} \\;=\\; \\|\\mathbf{x} + s\\,\\boldsymbol{\\eta}\\|^{2} - \\|\\mathbf{x}\\|^{2}\n\\;=\\; 2 s\\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} + s^{2} \\|\\boldsymbol{\\eta}\\|^{2}.\n$$\nThus\n$$\n\\Delta \\;=\\; - s\\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;-\\; \\frac{1}{2} s^{2} \\|\\boldsymbol{\\eta}\\|^{2}.\n$$\nUnder stationarity, $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$ and is independent of $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$. Consider the two terms separately under the scaling $s = \\ell/\\sqrt{d}$.\n\n- By independence and the Central Limit Theorem for triangular arrays or by direct variance computation, the projection\n$$\n\\frac{1}{\\sqrt{d}} \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;=\\; \\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} x_{i} \\eta_{i}\n$$\nconverges in distribution to a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$, because the summands $x_{i}\\eta_{i}$ are independent, mean zero, and have unit variance. Therefore,\n$$\ns\\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;=\\; \\frac{\\ell}{\\sqrt{d}} \\, \\mathbf{x} \\cdot \\boldsymbol{\\eta} \\;\\xrightarrow[d\\to\\infty]{\\mathcal{D}}\\; \\ell\\, Z.\n$$\n\n- By the Strong Law of Large Numbers,\n$$\n\\frac{1}{d}\\|\\boldsymbol{\\eta}\\|^{2} \\;=\\; \\frac{1}{d} \\sum_{i=1}^{d} \\eta_{i}^{2} \\;\\xrightarrow[d\\to\\infty]{\\text{a.s.}}\\; \\mathbb{E}[\\eta_{1}^{2}] \\;=\\; 1.\n$$\nHence\n$$\n\\frac{1}{2} s^{2} \\|\\boldsymbol{\\eta}\\|^{2} \\;=\\; \\frac{1}{2} \\frac{\\ell^{2}}{d} \\|\\boldsymbol{\\eta}\\|^{2} \\;\\xrightarrow[d\\to\\infty]{\\text{a.s.}}\\; \\frac{1}{2} \\ell^{2}.\n$$\n\nCombining these two convergences and invoking standard arguments for convergence of bounded continuous functionals yields the limiting distribution for $\\Delta$:\n$$\n\\Delta \\;\\xrightarrow[d\\to\\infty]{\\mathcal{D}}\\; - \\ell Z \\;-\\; \\frac{1}{2}\\ell^{2},\n$$\nwith $Z \\sim \\mathcal{N}(0,1)$ independent of everything else. Therefore, the average acceptance probability in the limit is\n$$\n\\alpha(\\ell) \\;=\\; \\mathbb{E}\\!\\left[\\min\\!\\left\\{1, \\exp(\\Delta)\\right\\}\\right] \\;=\\; \\mathbb{E}\\!\\left[\\min\\!\\left\\{1, \\exp\\!\\left(-\\ell Z - \\frac{1}{2}\\ell^{2}\\right)\\right\\}\\right].\n$$\nLet $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^{2}/2)$ denote the standard normal probability density function and $\\Phi$ its cumulative distribution function. The event $\\exp(-\\ell Z - \\ell^{2}/2) \\geq 1$ is equivalent to $-\\ell Z - \\ell^{2}/2 \\geq 0$, namely $Z \\leq -\\ell/2$. Hence\n$$\n\\alpha(\\ell) \\;=\\; \\mathbb{P}(Z \\leq -\\ell/2) \\;+\\; \\mathbb{E}\\!\\left[\\exp\\!\\left(-\\ell Z - \\frac{1}{2}\\ell^{2}\\right)\\, \\mathbf{1}_{\\{Z  -\\ell/2\\}}\\right].\n$$\nCompute the second term by completing the square:\n\\begin{align*}\n\\mathbb{E}\\!\\left[\\exp\\!\\left(-\\ell Z - \\frac{1}{2}\\ell^{2}\\right)\\, \\mathbf{1}_{\\{Z  -\\ell/2\\}}\\right]\n= \\int_{-\\ell/2}^{\\infty} \\exp\\!\\left(-\\ell z - \\frac{1}{2}\\ell^{2}\\right) \\phi(z)\\, \\mathrm{d}z \\\\\n= \\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2} - \\ell z - \\frac{1}{2}\\ell^{2}\\right) \\, \\mathrm{d}z \\\\\n= \\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(z+\\ell)^{2}}{2}\\right) \\, \\mathrm{d}z \\\\\n= \\int_{-\\ell/2+\\ell}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{u^{2}}{2}\\right) \\, \\mathrm{d}u \\\\\n= 1 - \\Phi(\\ell/2).\n\\end{align*}\nTherefore,\n$$\n\\alpha(\\ell) \\;=\\; \\Phi(-\\ell/2) \\;+\\; \\big(1 - \\Phi(\\ell/2)\\big) \\;=\\; 2 \\big(1 - \\Phi(\\ell/2)\\big) \\;=\\; 2 \\, \\Phi(-\\ell/2).\n$$\nThis is the desired limiting acceptance probability as a function of $\\ell$.\n\nStep 2: Define and simplify the asymptotic expected squared jump distance $J(\\ell)$.\n\nThe squared jump length is $\\|\\mathbf{y} - \\mathbf{x}\\|^{2} = s^{2} \\|\\boldsymbol{\\eta}\\|^{2}$. Thus\n$$\n\\mathbb{E}\\!\\left[\\|\\mathbf{y} - \\mathbf{x}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right]\n\\;=\\; s^{2} \\, \\mathbb{E}\\!\\left[\\|\\boldsymbol{\\eta}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right].\n$$\nUnder the scaling $s = \\ell/\\sqrt{d}$ and using that $\\|\\boldsymbol{\\eta}\\|^{2}/d \\to 1$ almost surely, while $a(\\mathbf{x},\\mathbf{y}) \\to \\min\\{1, \\exp(-\\ell Z - \\ell^{2}/2)\\}$ in distribution with expectation $\\alpha(\\ell)$, standard dominated convergence arguments (the acceptance is bounded by $1$ and the squared norm grows linearly in $d$ while $s^{2}$ scales as $1/d$) give\n$$\nJ(\\ell) \\;\\equiv\\; \\lim_{d \\to \\infty} \\mathbb{E}\\!\\left[\\|\\mathbf{y} - \\mathbf{x}\\|^{2} \\, a(\\mathbf{x},\\mathbf{y})\\right]\n\\;=\\; \\ell^{2} \\, \\alpha(\\ell).\n$$\nThus, up to a constant of proportionality, the asymptotic expected squared jump distance is maximized by maximizing $\\ell^{2} \\alpha(\\ell)$.\n\nStep 3: Maximize $J(\\ell)$ and compute the optimal acceptance probability $\\alpha^{\\star}$.\n\nWe need to maximize\n$$\nH(\\ell) \\;=\\; \\ell^{2} \\, \\alpha(\\ell) \\;=\\; 2 \\, \\ell^{2} \\, \\Phi(-\\ell/2), \\quad \\ell  0.\n$$\nDifferentiate $H(\\ell)$ with respect to $\\ell$, using that $\\frac{\\mathrm{d}}{\\mathrm{d}\\ell} \\Phi(-\\ell/2) = -\\frac{1}{2} \\phi(\\ell/2)$ and that the standard normal density is even:\n\\begin{align*}\nH'(\\ell)\n= 4 \\ell \\, \\Phi(-\\ell/2) \\;+\\; 2 \\ell^{2} \\left(-\\frac{1}{2}\\right) \\phi(\\ell/2) \\\\\n= 4 \\ell \\, \\Phi(-\\ell/2) \\;-\\; \\ell^{2} \\, \\phi(\\ell/2).\n\\end{align*}\nSetting $H'(\\ell) = 0$ and restricting to $\\ell  0$ yields the optimality condition\n$$\n4 \\, \\Phi(-\\ell/2) \\;=\\; \\ell \\, \\phi(\\ell/2).\n$$\nLet $t = \\ell/2  0$. Then the condition becomes\n$$\n2 \\, \\Phi(-t) \\;=\\; t \\, \\phi(t).\n$$\nThis scalar equation has a unique solution $t^{\\star} \\in (0,\\infty)$. Numerically solving yields $t^{\\star} \\approx 1.19$, equivalently $\\ell^{\\star} \\approx 2.38$. The corresponding optimal acceptance probability is\n$$\n\\alpha^{\\star} \\;=\\; \\alpha(\\ell^{\\star}) \\;=\\; 2 \\, \\Phi\\!\\left(-\\frac{\\ell^{\\star}}{2}\\right) \\;=\\; 2 \\, \\Phi(-t^{\\star}).\n$$\nSubstituting $t^{\\star} \\approx 1.19$ gives\n$$\n\\alpha^{\\star} \\;\\approx\\; 2 \\, \\Phi(-1.19).\n$$\nUsing the standard normal cumulative distribution function value $\\Phi(-1.19) \\approx 0.117$, we obtain\n$$\n\\alpha^{\\star} \\;\\approx\\; 0.234.\n$$\nThis value is the high-dimensional optimal acceptance probability for local random-walk Metropolis moves under the harmonic (Gaussian) approximation and the $\\ell/\\sqrt{d}$ scaling, often used as a heuristic target acceptance in high-dimensional molecular Monte Carlo simulations.\n\nRounding to three significant figures, the optimal acceptance probability is $0.234$.", "answer": "$$\\boxed{0.234}$$", "id": "2788234"}, {"introduction": "Often, the ultimate goal of a simulation is to compute thermodynamic properties like free energy, which may be inaccessible from a single standard simulation. This requires not only advanced sampling strategies but also powerful analysis techniques. This final practice delves into the Weighted Histogram Analysis Method (WHAM), a cornerstone of computational chemistry, challenging you to derive its governing equations from the first principles of statistical mechanics and maximum likelihood, providing a tool to reconstruct the complete free energy landscape from multiple biased simulations. [@problem_id:2788182]", "problem": "Consider a set of $K$ independent canonical simulations at the same inverse temperature $\\beta = 1/(k_B T)$ of a classical system whose potential energy function is $U$. In simulation $i \\in \\{1,\\dots,K\\}$, a bias potential $w_i(U)$ is added so that the stationary distribution of the Metropolis algorithm targets the biased Boltzmann weight proportional to $g(U)\\,\\exp\\!\\big(-\\beta\\,[U + w_i(U)]\\big)$, where $g(U)$ is the (unknown) density of states. Each simulation produces a histogram $H_i(U_m)$ over a discretization $\\{U_m\\}$ of $U$ with uniform bin width $\\Delta U$, and total counts $N_i = \\sum_m H_i(U_m)$. Let $Z_i$ be the (unknown) normalization constant of the biased ensemble $i$ and define $f_i \\equiv -\\beta^{-1}\\ln Z_i$. \n\nStarting only from the following foundational ingredients:\n- the Boltzmann distribution for equilibrium sampling and the definition of density of states $g(U)$,\n- the fact that the Metropolis algorithm with detailed balance samples from the target distribution,\n- and the multinomial (or independent Poisson) model for histogram counts around their mean,\n\nderive, by maximum likelihood and necessary normalization constraints, the self-consistent Weighted Histogram Analysis Method (WHAM) equations that optimally combine the $K$ biased histograms $\\{H_i(U_m)\\}$ to estimate $g(U_m)$ up to an overall multiplicative constant, together with the associated equations determining the $f_i$. Then outline a fixed-point iteration that solves these equations self-consistently.\n\nReport, as your final answer, the closed-form analytical expressions that define the two coupled fixed-point updates for $g(U_m)$ and $f_i$ in terms of $\\{H_i(U_m)\\}$, $\\{N_i\\}$, $\\{w_i(U_m)\\}$, $\\beta$, and $\\Delta U$. Your final expressions must be exact symbolic formulas (no numerical evaluation is required). Express your final answer as analytical expressions only, without any units.", "solution": "The problem as stated is scientifically sound, well-posed, and contains all necessary information for a rigorous derivation. It is a standard problem in computational statistical mechanics. I will proceed with the derivation.\n\nThe objective is to derive the self-consistent equations for the density of states, $g(U)$, and the free energies, $f_i$, from a set of $K$ biased simulations. The derivation proceeds from first principles using the maximum likelihood method.\n\nThe probability of observing a system in a microstate with energy $E$ in the canonical ensemble at inverse temperature $\\beta$ is proportional to $\\exp(-\\beta E)$. The density of states, $g(U)$, is the number of microstates per unit energy at energy $U$. Therefore, the probability of the system having an energy $U$ is given by the Boltzmann distribution, $P_{\\text{can}}(U) \\propto g(U) \\exp(-\\beta U)$.\n\nIn simulation $i$, a bias potential $w_i(U)$ is applied. As specified, the simulation samples a target distribution proportional to $g(U)\\exp(-\\beta[U + w_i(U)])$. The probability of observing an energy $U$ in simulation $i$ is thus:\n$$ P_i(U) = \\frac{g(U) \\exp(-\\beta[U + w_i(U)])}{Z_i} $$\nwhere $Z_i$ is the partition function for simulation $i$:\n$$ Z_i = \\int g(U) \\exp(-\\beta[U + w_i(U)]) \\, dU $$\nThe problem defines the free energies $f_i \\equiv -\\beta^{-1}\\ln Z_i$, so $Z_i = \\exp(-\\beta f_i)$.\n\nThe energy landscape is discretized into bins $\\{U_m\\}$ of width $\\Delta U$. We approximate the continuous functions as piecewise constant over these bins. The integral for the partition function becomes a sum:\n$$ Z_i \\approx \\Delta U \\sum_m g(U_m) \\exp(-\\beta[U_m + w_i(U_m)]) $$\nLet $g_m \\equiv g(U_m)\\Delta U$ be the dimensionless number of states in bin $m$. The partition function is $Z_i = \\sum_m g_m \\exp(-\\beta[U_m + w_i(U_m)])$. The probability of a sample from simulation $i$ falling into energy bin $m$ is:\n$$ p_{im} = \\frac{g_m \\exp(-\\beta[U_m + w_i(U_m)])}{Z_i} = g_m \\exp(-\\beta[U_m + w_i(U_m) - f_i]) $$\nNote that $\\Delta U$ has been absorbed into the definition of $g_m$.\n\nThe data from the simulations are the histograms $H_i(U_m)$, which are the counts of samples in bin $m$ from simulation $i$. The total number of counts in simulation $i$ is $N_i = \\sum_m H_i(U_m)$. We model the counts $H_i(U_m)$ as independent Poisson random variables. The expected number of counts, $\\langle H_i(U_m) \\rangle$, is proportional to the total number of samples $N_i$ and the probability $p_{im}$. For a large number of independent samples $N_i$, the mean count is $\\langle H_i(U_m) \\rangle = N_i p_{im}$.\n$$ \\langle H_i(U_m) \\rangle = N_i g_m \\exp(-\\beta[U_m + w_i(U_m) - f_i]) $$\nThe log-likelihood function for a set of independent Poisson variates $\\{H_i(U_m)\\}$ with means $\\{\\langle H_i(U_m) \\rangle\\}$ is, up to an additive constant that does not depend on the parameters $\\{g_m, f_i\\}$:\n$$ \\ln \\mathcal{L}(\\{g_m\\}, \\{f_i\\}) = \\sum_{i=1}^K \\sum_m \\left( H_i(U_m) \\ln \\langle H_i(U_m) \\rangle - \\langle H_i(U_m) \\rangle \\right) $$\nSubstituting the expression for $\\langle H_i(U_m) \\rangle$:\n$$ \\ln \\mathcal{L} = \\sum_{i,m} \\left( H_i(U_m) \\ln\\left(N_i g_m \\exp(-\\beta[U_m + w_i(U_m) - f_i])\\right) - N_i g_m \\exp(-\\beta[U_m + w_i(U_m) - f_i]) \\right) $$\nThe second term summates to a constant: $\\sum_{i,m} N_i p_{im} = \\sum_i N_i \\sum_m p_{im} = \\sum_i N_i = \\text{constant}$. Thus, maximizing the log-likelihood is equivalent to maximizing:\n$$ \\ln \\mathcal{L}' = \\sum_{i,m} H_i(U_m) \\left( \\ln g_m - \\beta U_m - \\beta w_i(U_m) + \\beta f_i \\right) $$\nThis must be maximized with respect to $\\{g_m\\}$ and $\\{f_i\\}$, subject to the normalization constraints that define the $\\{f_i\\}$:\n$$ \\exp(-\\beta f_i) = Z_i = \\sum_m g_m \\exp(-\\beta[U_m + w_i(U_m)]) $$\nWe use the method of Lagrange multipliers. The objective function to maximize is:\n$$ \\Lambda = \\sum_{i,m} H_i(U_m) \\left( \\ln g_m + \\beta f_i \\right) - \\sum_i \\lambda_i \\left( \\exp(-\\beta f_i) - \\sum_m g_m \\exp(-\\beta[U_m + w_i(U_m)]) \\right) $$\nwhere we have dropped terms not dependent on $\\{g_m, f_i\\}$.\nWe set the partial derivatives with respect to the parameters to zero.\nFirst, with respect to $g_n$ for a particular bin $n$:\n$$ \\frac{\\partial \\Lambda}{\\partial g_n} = \\frac{1}{g_n} \\sum_{i=1}^K H_i(U_n) + \\sum_{i=1}^K \\lambda_i \\exp(-\\beta[U_n + w_i(U_n)]) = 0 $$\nSecond, with respect to $f_j$ for a particular simulation $j$:\n$$ \\frac{\\partial \\Lambda}{\\partial f_j} = \\beta \\sum_m H_j(U_m) - \\lambda_j (-\\beta \\exp(-\\beta f_j)) = 0 $$\n$$ \\beta N_j + \\lambda_j \\beta \\exp(-\\beta f_j) = 0 \\implies \\lambda_j = -N_j \\exp(\\beta f_j) $$\nSubstitute this expression for $\\lambda_i$ back into the equation from the derivative with respect to $g_n$:\n$$ \\frac{\\sum_i H_i(U_n)}{g_n} - \\sum_{i=1}^K N_i \\exp(\\beta f_i) \\exp(-\\beta[U_n + w_i(U_n)]) = 0 $$\nRearranging for $g_n$ yields the first self-consistent equation:\n$$ g_n = \\frac{\\sum_{i=1}^K H_i(U_n)}{\\sum_{i=1}^K N_i \\exp(-\\beta[U_n + w_i(U_n) - f_i])} $$\nThis gives the dimensionless number of states $g_m = g(U_m)\\Delta U$. In terms of the density of states $g(U_m)$, this is:\n$$ g(U_m) = \\frac{1}{\\Delta U} \\frac{\\sum_{i=1}^K H_i(U_m)}{\\sum_{i=1}^K N_i \\exp(-\\beta[U_m + w_i(U_m) - f_i])} $$\nThe second self-consistent equation is simply the constraint equation, which relates the $f_i$ to the $g(U_m)$:\n$$ \\exp(-\\beta f_i) = \\sum_m g(U_m)\\Delta U \\exp(-\\beta[U_m + w_i(U_m)]) $$\nTaking the logarithm gives an explicit equation for $f_i$:\n$$ f_i = -\\frac{1}{\\beta} \\ln \\left( \\Delta U \\sum_m g(U_m) \\exp(-\\beta[U_m + w_i(U_m)]) \\right) $$\nThese two coupled equations are the WHAM equations. They are solved self-consistently via fixed-point iteration:\n$1$. Initialize the free energies $\\{f_i^{(0)}\\}$, e.g., $f_i^{(0)} = 0$ for all $i$.\n$2$. In iteration $k+1$, calculate a new estimate for the density of states $g^{(k+1)}(U_m)$ using the current free energies $f_i^{(k)}$:\n$$ g^{(k+1)}(U_m) = \\frac{1}{\\Delta U} \\frac{\\sum_{i=1}^K H_i(U_m)}{\\sum_{i=1}^K N_i \\exp(-\\beta[U_m + w_i(U_m) - f_i^{(k)}])} $$\n$3$. Calculate new free energies $f_i^{(k+1)}$ using the new density of states $g^{(k+1)}(U_m)$:\n$$ f_i^{(k+1)} = -\\frac{1}{\\beta} \\ln \\left( \\Delta U \\sum_m g^{(k+1)}(U_m) \\exp(-\\beta[U_m + w_i(U_m)]) \\right) $$\n$4$. Repeat steps $2$ and $3$ until the values of $\\{g(U_m)\\}$ and $\\{f_i\\}$ converge. Due to an inherent ambiguity, $g(U_m)$ is determined only up to a multiplicative constant, and the $f_i$ values up to a corresponding additive constant. This is typically resolved by setting one $f_j$ to a fixed value (e.g., $0$) during the iteration. The final expressions for the updates are given below.", "answer": "$$ \\boxed{\\begin{pmatrix} g(U_m) = \\frac{\\sum_{k=1}^{K} H_k(U_m)}{\\Delta U \\sum_{k=1}^{K} N_k \\exp(-\\beta [U_m + w_k(U_m) - f_k])}  f_i = -\\frac{1}{\\beta} \\ln\\left( \\Delta U \\sum_{m} g(U_m) \\exp(-\\beta [U_m + w_i(U_m)]) \\right) \\end{pmatrix}} $$", "id": "2788182"}]}