{"hands_on_practices": [{"introduction": "The variational principle is the cornerstone of Variational Monte Carlo. This exercise takes you back to the beginning, applying the principle to the exactly solvable hydrogen atom [@problem_id:2828337]. By analytically deriving the local energy and minimizing the variational energy for a simple exponential trial wavefunction, you will gain a concrete understanding of how the quality of a trial wavefunction is directly linked to the energy it produces, laying the groundwork for more complex systems.", "problem": "In Variational Monte Carlo (VMC) and Diffusion Monte Carlo (DMC) for electronic structure, the local energy is defined for a trial wavefunction $\\Psi_{T}(\\mathbf{R})$ by $E_{L}(\\mathbf{R}) \\equiv \\dfrac{\\hat{H}\\,\\Psi_{T}(\\mathbf{R})}{\\Psi_{T}(\\mathbf{R})}$, where $\\hat{H}$ is the system Hamiltonian. Consider a single-electron hydrogenic problem in Hartree atomic units where $\\hbar = m_{e} = e = 1$ and $4\\pi\\varepsilon_{0} = 1$. The nonrelativistic Hamiltonian for the hydrogen atom is $\\hat{H} = -\\dfrac{1}{2}\\nabla^{2} - \\dfrac{1}{r}$, where $r = |\\mathbf{r}|$.\n\nLet the isotropic trial wavefunction be $\\Psi_{T}(r) = \\exp(-\\alpha r)$, with a real parameter $\\alpha  0$. Using only fundamental definitions, do the following:\n\n1) Derive the local energy $E_{L}(r)$.\n\n2) Using the definition of the variational energy $E_{V}(\\alpha) = \\dfrac{\\int_{\\mathbb{R}^{3}} |\\Psi_{T}(\\mathbf{r})|^{2} E_{L}(\\mathbf{r})\\, d^{3}\\mathbf{r}}{\\int_{\\mathbb{R}^{3}} |\\Psi_{T}(\\mathbf{r})|^{2}\\, d^{3}\\mathbf{r}}$, evaluate $E_{V}(\\alpha)$ exactly as a function of $\\alpha$.\n\n3) Minimize $E_{V}(\\alpha)$ with respect to $\\alpha$ to obtain the optimal parameter $\\alpha^{\\star}$.\n\nWork in spherical coordinates and show all steps from first principles, including the Laplacian acting on a spherically symmetric function. You may perform any required integrals analytically. Express the final answer as the exact value of $\\alpha^{\\star}$ in $\\text{bohr}^{-1}$. Provide only $\\alpha^{\\star}$ as your final answer; do not include the local energy or the minimized energy in the final box. No rounding is required.", "solution": "The problem statement is scientifically grounded, self-contained, and quantitatively rigorous. It presents a standard exercise in quantum mechanics, specifically the application of the variational principle to the hydrogen atom. The problem is valid and will be solved as stated.\n\nThe task is to find the optimal parameter $\\alpha^{\\star}$ for a given trial wavefunction $\\Psi_{T}(r) = \\exp(-\\alpha r)$ for the hydrogen atom, whose Hamiltonian in Hartree atomic units is $\\hat{H} = -\\dfrac{1}{2}\\nabla^{2} - \\dfrac{1}{r}$. This will be done in three sequential steps: first, deriving the local energy $E_{L}(r)$; second, evaluating the variational energy $E_{V}(\\alpha)$; and third, minimizing $E_{V}(\\alpha)$ with respect to $\\alpha$.\n\nStep 1: Derivation of the local energy $E_{L}(r)$\n\nThe local energy is defined as $E_{L}(\\mathbf{R}) = \\dfrac{\\hat{H}\\,\\Psi_{T}(\\mathbf{R})}{\\Psi_{T}(\\mathbf{R})}$. Since the trial wavefunction $\\Psi_{T}(r)$ is spherically symmetric, the local energy will also depend only on the radial coordinate $r$. We must apply the Hamiltonian $\\hat{H}$ to $\\Psi_{T}(r)$.\n\nThe action of the potential energy operator is trivial:\n$$\n-\\frac{1}{r} \\Psi_{T}(r) = -\\frac{1}{r} \\exp(-\\alpha r)\n$$\n\nThe action of the kinetic energy operator, $-\\dfrac{1}{2}\\nabla^{2}$, requires computing the Laplacian of $\\Psi_{T}(r)$. In spherical coordinates $(r, \\theta, \\phi)$, the Laplacian operator is:\n$$\n\\nabla^{2} = \\frac{1}{r^{2}} \\frac{\\partial}{\\partial r} \\left( r^{2} \\frac{\\partial}{\\partial r} \\right) + \\frac{1}{r^{2}\\sin\\theta} \\frac{\\partial}{\\partial \\theta} \\left( \\sin\\theta \\frac{\\partial}{\\partial \\theta} \\right) + \\frac{1}{r^{2}\\sin^{2}\\theta} \\frac{\\partial^{2}}{\\partial \\phi^{2}}\n$$\nSince $\\Psi_{T}(r)$ depends only on $r$, the derivatives with respect to $\\theta$ and $\\phi$ are zero. The Laplacian simplifies to its radial part:\n$$\n\\nabla^{2}\\Psi_{T}(r) = \\frac{1}{r^{2}} \\frac{d}{dr} \\left( r^{2} \\frac{d\\Psi_{T}(r)}{dr} \\right)\n$$\nFirst, we compute the derivative of $\\Psi_{T}(r)$ with respect to $r$:\n$$\n\\frac{d\\Psi_{T}}{dr} = \\frac{d}{dr}\\exp(-\\alpha r) = -\\alpha \\exp(-\\alpha r)\n$$\nNext, we multiply by $r^2$ and differentiate again:\n$$\n\\frac{d}{dr} \\left( r^{2} \\frac{d\\Psi_{T}}{dr} \\right) = \\frac{d}{dr} \\left( -r^{2}\\alpha \\exp(-\\alpha r) \\right) = - \\alpha \\left( 2r\\exp(-\\alpha r) - \\alpha r^{2}\\exp(-\\alpha r) \\right)\n$$\nFinally, we divide by $r^2$:\n$$\n\\nabla^{2}\\Psi_{T}(r) = \\frac{1}{r^2} \\left[ -2\\alpha r \\exp(-\\alpha r) + \\alpha^{2} r^{2} \\exp(-\\alpha r) \\right] = \\left( -\\frac{2\\alpha}{r} + \\alpha^{2} \\right) \\exp(-\\alpha r)\n$$\nSo, the action of the kinetic energy operator is:\n$$\n-\\frac{1}{2}\\nabla^{2}\\Psi_{T}(r) = -\\frac{1}{2} \\left( \\alpha^{2} - \\frac{2\\alpha}{r} \\right) \\exp(-\\alpha r) = \\left( -\\frac{\\alpha^{2}}{2} + \\frac{\\alpha}{r} \\right) \\exp(-\\alpha r)\n$$\nNow, we combine the kinetic and potential energy terms to find $\\hat{H}\\Psi_{T}(r)$:\n$$\n\\hat{H}\\Psi_{T}(r) = \\left( -\\frac{1}{2}\\nabla^{2} - \\frac{1}{r} \\right) \\Psi_{T}(r) = \\left( -\\frac{\\alpha^{2}}{2} + \\frac{\\alpha}{r} \\right) \\exp(-\\alpha r) - \\frac{1}{r} \\exp(-\\alpha r) = \\left( -\\frac{\\alpha^{2}}{2} + \\frac{\\alpha - 1}{r} \\right) \\exp(-\\alpha r)\n$$\nThe local energy $E_{L}(r)$ is obtained by dividing $\\hat{H}\\Psi_{T}(r)$ by $\\Psi_{T}(r)$:\n$$\nE_{L}(r) = \\frac{\\left( -\\frac{\\alpha^{2}}{2} + \\frac{\\alpha - 1}{r} \\right) \\exp(-\\alpha r)}{\\exp(-\\alpha r)} = -\\frac{\\alpha^{2}}{2} + \\frac{\\alpha - 1}{r}\n$$\n\nStep 2: Evaluation of the variational energy $E_{V}(\\alpha)$\n\nThe variational energy is the expectation value of the local energy over the probability distribution $|\\Psi_{T}|^{2}$:\n$$\nE_{V}(\\alpha) = \\frac{\\int_{\\mathbb{R}^{3}} |\\Psi_{T}(\\mathbf{r})|^{2} E_{L}(\\mathbf{r})\\, d^{3}\\mathbf{r}}{\\int_{\\mathbb{R}^{3}} |\\Psi_{T}(\\mathbf{r})|^{2}\\, d^{3}\\mathbf{r}} = \\langle E_{L}(r) \\rangle\n$$\nSince $\\Psi_{T}(r)$ is real, $|\\Psi_{T}(r)|^{2} = \\Psi_{T}(r)^{2} = \\exp(-2\\alpha r)$. We can write:\n$$\nE_{V}(\\alpha) = \\left\\langle -\\frac{\\alpha^{2}}{2} + \\frac{\\alpha - 1}{r} \\right\\rangle = -\\frac{\\alpha^{2}}{2} + (\\alpha - 1) \\left\\langle \\frac{1}{r} \\right\\rangle\n$$\nThe expectation value $\\langle \\frac{1}{r} \\rangle$ is given by:\n$$\n\\left\\langle \\frac{1}{r} \\right\\rangle = \\frac{\\int_{\\mathbb{R}^{3}} \\frac{1}{r} |\\Psi_{T}(\\mathbf{r})|^{2} \\,d^{3}\\mathbf{r}}{\\int_{\\mathbb{R}^{3}} |\\Psi_{T}(\\mathbf{r})|^{2} \\,d^{3}\\mathbf{r}}\n$$\nWe evaluate the numerator and denominator integrals separately. The volume element in spherical coordinates is $d^{3}\\mathbf{r} = r^{2}\\sin\\theta\\, dr\\, d\\theta\\, d\\phi$. The integration over angular variables $(\\theta, \\phi)$ gives a factor of $4\\pi$.\n\nThe denominator integral is:\n$$\n\\int_{\\mathbb{R}^{3}} |\\Psi_{T}|^{2} \\,d^{3}\\mathbf{r} = 4\\pi \\int_{0}^{\\infty} \\exp(-2\\alpha r) r^{2}\\, dr\n$$\nThis is a standard integral of the form $\\int_{0}^{\\infty} x^{n} \\exp(-ax)\\, dx = \\frac{n!}{a^{n+1}}$. Here, $n=2$ and $a=2\\alpha$.\n$$\n\\int_{0}^{\\infty} r^{2} \\exp(-2\\alpha r)\\, dr = \\frac{2!}{(2\\alpha)^{3}} = \\frac{2}{8\\alpha^{3}} = \\frac{1}{4\\alpha^{3}}\n$$\nSo, the denominator is $4\\pi \\left( \\frac{1}{4\\alpha^{3}} \\right) = \\frac{\\pi}{\\alpha^{3}}$.\n\nThe numerator integral for $\\langle \\frac{1}{r} \\rangle$ is:\n$$\n\\int_{\\mathbb{R}^{3}} \\frac{1}{r} |\\Psi_{T}|^{2} \\,d^{3}\\mathbf{r} = 4\\pi \\int_{0}^{\\infty} \\frac{1}{r} \\exp(-2\\alpha r) r^{2}\\, dr = 4\\pi \\int_{0}^{\\infty} r \\exp(-2\\alpha r)\\, dr\n$$\nThis corresponds to the case $n=1$ and $a=2\\alpha$:\n$$\n\\int_{0}^{\\infty} r \\exp(-2\\alpha r)\\, dr = \\frac{1!}{(2\\alpha)^{2}} = \\frac{1}{4\\alpha^{2}}\n$$\nSo, the numerator is $4\\pi \\left( \\frac{1}{4\\alpha^{2}} \\right) = \\frac{\\pi}{\\alpha^{2}}$.\n\nNow we compute $\\langle \\frac{1}{r} \\rangle$:\n$$\n\\left\\langle \\frac{1}{r} \\right\\rangle = \\frac{\\pi/\\alpha^{2}}{\\pi/\\alpha^{3}} = \\alpha\n$$\nSubstituting this back into the expression for $E_{V}(\\alpha)$:\n$$\nE_{V}(\\alpha) = -\\frac{\\alpha^{2}}{2} + (\\alpha - 1)\\alpha = -\\frac{\\alpha^{2}}{2} + \\alpha^{2} - \\alpha = \\frac{\\alpha^{2}}{2} - \\alpha\n$$\n\nStep 3: Minimization of $E_{V}(\\alpha)$\n\nTo find the optimal parameter $\\alpha^{\\star}$ that minimizes the variational energy, we differentiate $E_{V}(\\alpha)$ with respect to $\\alpha$ and set the result to zero:\n$$\n\\frac{dE_{V}}{d\\alpha} = \\frac{d}{d\\alpha} \\left( \\frac{\\alpha^{2}}{2} - \\alpha \\right) = \\alpha - 1\n$$\nSetting the derivative to zero gives the condition for the extremum:\n$$\n\\alpha - 1 = 0 \\implies \\alpha^{\\star} = 1\n$$\nTo confirm this is a minimum, we check the second derivative:\n$$\n\\frac{d^{2}E_{V}}{d\\alpha^{2}} = \\frac{d}{d\\alpha}(\\alpha - 1) = 1\n$$\nSince the second derivative is positive ($1  0$), the extremum is a minimum. The optimal value of the parameter is $\\alpha^{\\star} = 1$. In Hartree atomic units, length is measured in units of the Bohr radius, $a_{0}$. The parameter $\\alpha$ has units of inverse length, so the units are correctly $\\text{bohr}^{-1}$. The optimal value is $1\\ \\text{bohr}^{-1}$.", "answer": "$$\n\\boxed{1}\n$$", "id": "2828337"}, {"introduction": "Quantum Monte Carlo simulations are fundamentally statistical, generating a sequence of correlated energy values rather than a single number. This practice addresses the crucial task of transforming this raw data into a reliable scientific result [@problem_id:2828332]. You will learn to distinguish between different measures of variance and, most importantly, to calculate a statistically sound error bar for your mean energy by properly accounting for the autocorrelation inherent in Markov chain sampling.", "problem": "A Variational Monte Carlo (VMC) simulation of a small closed-shell atom produces a stationary Markov chain of local energy values $\\{E_{i}\\}_{i=1}^{N}$ sampled at equal time intervals. The per-step sampling is correlated. Let the population variance of the local energy be defined as $\\sigma^{2} \\equiv \\mathrm{Var}_{\\pi}(E)$ with respect to the stationary distribution $\\pi$, and let the (conventional) sample variance be $s^{2} \\equiv \\frac{1}{N-1}\\sum_{i=1}^{N}(E_{i}-\\bar{E})^{2}$, where $\\bar{E} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} E_{i}$. The normalized autocorrelation function at lag $k$ is $\\rho_{k} \\equiv \\frac{\\mathrm{Cov}(E_{i},E_{i+k})}{\\sigma^{2}}$, and the integrated autocorrelation time is defined as $\\tau_{\\mathrm{int}} \\equiv \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_{k}$.\n\nStarting from these definitions and the basic properties of covariance for stationary sequences, explain conceptually the differences among population variance, sample variance, and the effective variance of the sample mean under correlated sampling. Then, by expressing $\\mathrm{Var}(\\bar{E})$ in terms of $\\sigma^{2}$ and $\\tau_{\\mathrm{int}}$, obtain the standard error (the square root of the effective variance) used as the error bar for $\\bar{E}$ in VMC.\n\nNow consider a concrete run with $N = 100000$ samples and the following estimates from the data analysis:\n- $\\bar{E} = -2.861$ (in Hartree),\n- $s^{2} = 0.16$ (in $\\mathrm{Ha}^{2}$),\n- $\\hat{\\tau}_{\\mathrm{int}} = 12.5$ (dimensionless, measured in units of the sampling interval using the definition above).\n\nTreat $s^{2}$ as a consistent estimator of $\\sigma^{2}$ for large $N$ and use $\\hat{\\tau}_{\\mathrm{int}}$ as the estimate of $\\tau_{\\mathrm{int}}$. Compute the error bar, i.e., the standard error of $\\bar{E}$, in Hartree. Express the final energy uncertainty in Hartree (Ha) and round your answer to four significant figures.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All definitions and data provided are standard in the context of statistical analysis of Monte Carlo simulations in computational physics and chemistry. The problem is therefore deemed valid and a solution will be provided.\n\nThe problem requires a conceptual explanation of three different variance-related quantities, a derivation of the standard error of the mean for correlated data, and a numerical calculation of this error for a specific case.\n\nFirst, we clarify the conceptual distinctions.\n$1$. The **population variance**, $\\sigma^{2} \\equiv \\mathrm{Var}_{\\pi}(E)$, is a theoretical property of the physical system and the chosen trial wavefunction. It represents the intrinsic spread of the local energy observable, $E_{L}(\\mathbf{R}) = H\\Psi_{T}(\\mathbf{R})/\\Psi_{T}(\\mathbf{R})$, over the exact probability distribution $\\pi(\\mathbf{R}) \\propto |\\Psi_{T}(\\mathbf{R})|^{2}$. If the trial wavefunction $\\Psi_{T}$ were the exact eigenfunction of the Hamiltonian $H$, the local energy would be constant everywhere and $\\sigma^{2}$ would be zero. Thus, $\\sigma^{2}$ is a measure of the quality of the trial wavefunction. It is a fixed, albeit often unknown, value.\n\n$2$. The **sample variance**, $s^{2} \\equiv \\frac{1}{N-1}\\sum_{i=1}^{N}(E_{i}-\\bar{E})^{2}$, is a statistical estimator calculated from the finite set of $N$ data points $\\{E_{i}\\}$ generated during the simulation. It is an *estimate* of the true population variance $\\sigma^{2}$. The distinction is fundamental: $\\sigma^{2}$ is a theoretical constant, while $s^{2}$ is a random variable whose value depends on the specific finite sample obtained. For a large number of samples $N$, as stipulated, $s^{2}$ is a consistent estimator for $\\sigma^{2}$, meaning $s^{2} \\to \\sigma^{2}$ as $N \\to \\infty$.\n\n$3$. The **effective variance of the sample mean**, $\\mathrm{Var}(\\bar{E})$, quantifies the uncertainty in the estimate of the mean energy, $\\bar{E}$, not the spread of individual data points. The sample mean $\\bar{E}$ is itself a random variable. If one were to repeat the entire simulation many times, one would obtain a distribution of $\\bar{E}$ values. $\\mathrm{Var}(\\bar{E})$ is the variance of this distribution. It measures the reliability of $\\bar{E}$ as an approximation to the true expectation value of the energy. Unlike for independent samples where $\\mathrm{Var}(\\bar{E}) = \\sigma^{2}/N$, for correlated samples from a Markov chain, this variance is amplified by the presence of serial correlation, which is captured by the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$.\n\nNext, we derive the expression for the standard error of $\\bar{E}$. The variance of the sample mean $\\bar{E} = \\frac{1}{N}\\sum_{i=1}^{N} E_{i}$ is given by:\n$$\n\\mathrm{Var}(\\bar{E}) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} E_{i}\\right) = \\frac{1}{N^{2}} \\mathrm{Var}\\left(\\sum_{i=1}^{N} E_{i}\\right)\n$$\nUsing the general formula for the variance of a sum of random variables, $\\mathrm{Var}(\\sum_{i} X_{i}) = \\sum_{i,j} \\mathrm{Cov}(X_{i}, X_{j})$, we have:\n$$\n\\mathrm{Var}(\\bar{E}) = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(E_{i}, E_{j})\n$$\nSince the sequence $\\{E_{i}\\}$ is stationary, the covariance depends only on the lag $k = |i-j|$: $\\mathrm{Cov}(E_{i}, E_{j}) = \\mathrm{Cov}(E_{l}, E_{l+|i-j|})$. Using the definition of the normalized autocorrelation function, $\\rho_{k} = \\mathrm{Cov}(E_{i}, E_{i+k})/\\sigma^{2}$, we have $\\mathrm{Cov}(E_{i}, E_{j}) = \\sigma^{2} \\rho_{|i-j|}$.\nThe double summation can be rewritten by grouping terms with the same lag $k$:\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(E_{i}, E_{j}) = \\sum_{k=-(N-1)}^{N-1} (N-|k|) \\mathrm{Cov}(E_{i}, E_{i+k})\n$$\nThe term $(N-|k|)$ is the number of pairs of samples with a lag of $k$. Since $\\rho_{k} = \\rho_{-k}$, the sum becomes:\n$$\n= N \\mathrm{Cov}(E_{i}, E_{i}) + \\sum_{k=1}^{N-1} 2(N-k) \\mathrm{Cov}(E_{i}, E_{i+k})\n$$\n$$\n= N\\sigma^{2} + 2\\sigma^{2} \\sum_{k=1}^{N-1} (N-k)\\rho_{k} = N\\sigma^{2}\\left(1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\rho_{k}\\right)\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\bar{E})$:\n$$\n\\mathrm{Var}(\\bar{E}) = \\frac{\\sigma^{2}}{N}\\left(1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\rho_{k}\\right)\n$$\nFor a large number of samples $N$, and for lags $k$ where $\\rho_{k}$ is non-negligible, the term $k/N$ is small. Thus, we can approximate $(1-k/N) \\approx 1$. Furthermore, the sum can be extended to infinity since $\\rho_{k}$ decays to zero for large $k$:\n$$\n\\mathrm{Var}(\\bar{E}) \\approx \\frac{\\sigma^{2}}{N}\\left(1 + 2\\sum_{k=1 }^{\\infty}\\rho_{k}\\right)\n$$\nUsing the provided definition for the integrated autocorrelation time, $\\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_{k}$, we can express the sum as $\\sum_{k=1}^{\\infty} \\rho_{k} = \\tau_{\\mathrm{int}} - \\frac{1}{2}$.\nSubstituting this into the approximate expression for the variance:\n$$\n\\mathrm{Var}(\\bar{E}) \\approx \\frac{\\sigma^{2}}{N}\\left(1 + 2\\left(\\tau_{\\mathrm{int}} - \\frac{1}{2}\\right)\\right) = \\frac{\\sigma^{2}}{N}(1 + 2\\tau_{\\mathrm{int}} - 1) = \\frac{2\\tau_{\\mathrm{int}}\\sigma^{2}}{N}\n$$\nThis expression is the effective variance of the sample mean under correlated sampling. The quantity $N_{\\mathrm{eff}} = N/(2\\tau_{\\mathrm{int}})$ is often interpreted as the effective number of independent samples.\nThe standard error of the mean, denoted $\\mathrm{SE}(\\bar{E})$, is the square root of this variance:\n$$\n\\mathrm{SE}(\\bar{E}) = \\sqrt{\\mathrm{Var}(\\bar{E})} \\approx \\sqrt{\\frac{2\\tau_{\\mathrm{int}}\\sigma^{2}}{N}}\n$$\nThis is the required formula for the error bar of $\\bar{E}$.\n\nFinally, we compute the numerical value of the error bar. We are given:\n- Number of samples, $N = 100000 = 10^{5}$\n- Estimate of population variance, $s^{2} = 0.16 \\ \\mathrm{Ha}^{2}$\n- Estimate of integrated autocorrelation time, $\\hat{\\tau}_{\\mathrm{int}} = 12.5$\n\nWe use the sample estimates $s^{2}$ and $\\hat{\\tau}_{\\mathrm{int}}$ in place of the true values $\\sigma^{2}$ and $\\tau_{\\mathrm{int}}$ in our derived formula:\n$$\n\\mathrm{SE}(\\bar{E}) \\approx \\sqrt{\\frac{2 \\hat{\\tau}_{\\mathrm{int}} s^{2}}{N}}\n$$\nSubstituting the given numerical values:\n$$\n\\mathrm{SE}(\\bar{E}) \\approx \\sqrt{\\frac{2 \\times 12.5 \\times 0.16}{100000}} = \\sqrt{\\frac{25 \\times 0.16}{10^{5}}} = \\sqrt{\\frac{4}{10^{5}}} = \\sqrt{4 \\times 10^{-5}}\n$$\n$$\n\\mathrm{SE}(\\bar{E}) = 2 \\times 10^{-2.5} = \\frac{2}{100\\sqrt{10}} \\approx \\frac{2}{100 \\times 3.162277...} \\approx \\frac{0.02}{3.162277...} \\approx 0.006324555... \\ \\mathrm{Ha}\n$$\nThe problem requires the result to be rounded to four significant figures. The first four significant figures are $6$, $3$, $2$, and $4$. The fifth significant digit is $5$, so we round the fourth digit up.\n$$\n\\mathrm{SE}(\\bar{E}) \\approx 0.006325 \\ \\mathrm{Ha}\n$$\nThis value represents the statistical uncertainty of the calculated mean energy $\\bar{E} = -2.861 \\ \\mathrm{Ha}$. The final energy should be reported as $-2.861(6) \\ \\mathrm{Ha}$ or $-2.8610 \\pm 0.0063 \\ \\mathrm{Ha}$, although the problem only asks for the error bar itself.", "answer": "$$\n\\boxed{0.006325}\n$$", "id": "2828332"}, {"introduction": "Applying QMC methods to extended systems, such as solids, requires simulating a finite periodic cell and then extrapolating the results to the infinite system, or thermodynamic limit. This introduces finite-size errors that must be carefully handled. This advanced practice guides you through the process of deriving the leading-order finite-size correction and using it to perform a weighted regression, a standard and essential technique for obtaining high-accuracy results for materials [@problem_id:2828289].", "problem": "You are given twist-averaged Diffusion Monte Carlo (DMC) total energies per particle for periodic, homogeneous electron systems at several finite sizes under periodic boundary conditions. Twist-averaging removes one-body shell effects, so the remaining finite-size bias originates from two-body correlations governed by the long-wavelength behavior of the Coulomb interaction and the static structure factor. Using only the following foundational bases, derive an asymptotic scaling form for the finite-size bias in the energy per particle and construct a statistically principled estimator for the thermodynamic limit:\n\n- Translational invariance and extensivity imply that the total energy of an $N$-particle periodic system is an extensive property, and the energy per particle $e(N)$ approaches a thermodynamic-limit value $e(\\infty)$ as $N \\to \\infty$.\n- The replacement of continuum wavevectors by a discrete reciprocal lattice in a finite periodic box leads to errors that are controlled by the long-wavelength part of correlation functions. When one-body shell effects are removed by twist-averaging, the leading residual error is due to two-body correlations at small wavevector magnitude.\n- The difference between a Riemann sum over reciprocal lattice vectors and the corresponding continuum integral for a sufficiently smooth, long-wavelength integrand scales with the inverse volume in three spatial dimensions. Express your reasoning in terms of the box length $L$, particle number $N$, and number density $n$, where $L^{3} = N/n$.\n\nTask A (Derivation): Starting from the bases above and without invoking any specialized or system-specific correction formulas, argue the leading-order dependence of the two-body finite-size bias in the energy per particle on $N$ for a three-dimensional periodic homogeneous system under twist-averaging. From this, motivate a minimal consistent regression model for $e(N)$ that can be used to estimate $e(\\infty)$ from data at several values of $N$.\n\nTask B (Estimation): Given noisy estimates $e_{i}$ of $e(N_{i})$ with reported independent standard errors $\\sigma_{i}$ for $i = 1, \\dots, M$, pose and solve a weighted least squares estimation problem for the regression model parameters, and define the following quantities:\n- The thermodynamic-limit energy per particle estimate $\\hat{e}(\\infty)$ with units in Hartree per particle.\n- For a specified target size $N_{\\mathrm{tgt}}$, the signed estimate of the remaining two-body finite-size bias at $N_{\\mathrm{tgt}}$, defined as $\\Delta_{\\mathrm{tgt}} \\equiv \\hat{e}(N_{\\mathrm{tgt}}) - \\hat{e}(\\infty)$, with units in Hartree per particle. This quantity represents the estimated residual two-body finite-size error at $N_{\\mathrm{tgt}}$ after twist-averaging.\n\nImplement your solution as a program that:\n- Uses weighted least squares with weights $w_{i} = 1/\\sigma_{i}^{2}$ to estimate the parameters of your minimal regression model.\n- Computes $\\hat{e}(\\infty)$ and $\\Delta_{\\mathrm{tgt}}$ for each test case below.\n- Reports all outputs in Hartree per particle, rounded to six decimal places.\n\nTest Suite:\nFor each case, you are given a list of particle numbers $N_{i}$, twist-averaged DMC energies per particle $e_{i}$ (in Hartree per particle), their one-standard-deviation uncertainties $\\sigma_{i}$ (in Hartree per particle), and a target $N_{\\mathrm{tgt}}$.\n\n- Case $1$:\n  - $N = [\\,32,\\,54,\\,128,\\,250\\,]$\n  - $e = [\\,-0.421500,\\,-0.462241,\\,-0.496500,\\,-0.508700\\,]$\n  - $\\sigma = [\\,0.000800,\\,0.001000,\\,0.000600,\\,0.000600\\,]$\n  - $N_{\\mathrm{tgt}} = 128$\n\n- Case $2$ (boundary case with the minimal number of sizes to permit a linear fit):\n  - $N = [\\,54,\\,108\\,]$\n  - $e = [\\,-0.281481,\\,-0.290741\\,]$\n  - $\\sigma = [\\,0.001500,\\,0.001000\\,]$\n  - $N_{\\mathrm{tgt}} = 108$\n\n- Case $3$ (mild curvature beyond leading order to test robustness of the leading-order fit):\n  - $N = [\\,64,\\,96,\\,216,\\,512\\,]$\n  - $e = [\\,-0.710156,\\,-0.723650,\\,-0.738346,\\,-0.745097\\,]$\n  - $\\sigma = [\\,0.000700,\\,0.000700,\\,0.000700,\\,0.000700\\,]$\n  - $N_{\\mathrm{tgt}} = 512$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, where each inner list corresponds to a test case and contains exactly two floats:\n- The first entry is $\\hat{e}(\\infty)$ in Hartree per particle, rounded to six decimal places.\n- The second entry is $\\Delta_{\\mathrm{tgt}}$ in Hartree per particle, rounded to six decimal places.\nThe overall format must be like\n[[$\\hat{e}(\\infty)_{1}$,$\\Delta_{\\mathrm{tgt},1}$],[$\\hat{e}(\\infty)_{2}$,$\\Delta_{\\mathrm{tgt},2}$],[$\\hat{e}(\\infty)_{3}$,$\\Delta_{\\mathrm{tgt},3}$]]\nwith no spaces. For example, an output might look like \"[[-0.521234,0.012345],[-0.300000,0.009259],[-0.750000,0.004883]]\" but with values determined by your computation.", "solution": "The problem statement has been rigorously evaluated and is deemed valid. It is scientifically grounded, well-posed, and contains all necessary information to proceed with a formal derivation and numerical solution. It presents a standard, non-trivial problem in computational physics.\n\nThe task is to derive the asymptotic scaling of the finite-size energy bias in twist-averaged quantum Monte Carlo calculations and to construct a corresponding estimator for the thermodynamic limit. This will be addressed in two parts as requested.\n\n**Task A: Derivation of Asymptotic Scaling and Regression Model**\n\nOur objective is to determine the leading-order functional dependence of the energy per particle, $e(N)$, on the number of particles, $N$, for a large, periodic, homogeneous system. The finite-size bias is defined as $\\Delta e(N) = e(N) - e(\\infty)$, where $e(\\infty)$ is the energy per particle in the thermodynamic limit ($N \\to \\infty$).\n\nWe are given three foundational bases:\n1.  Extensivity of energy and the existence of a thermodynamic limit $e(\\infty)$.\n2.  The primary source of residual finite-size error, after twist-averaging has removed one-body shell effects, is the two-body correlation energy. This error arises from the substitution of a discrete sum over reciprocal lattice vectors for a continuous integral.\n3.  For a sufficiently smooth function in three dimensions, the error in approximating a continuum integral by a discrete Riemann sum over reciprocal lattice points scales as the inverse of the system volume, $V$.\n\nLet us formalize the argument. The energy per particle, $e(N)$, can be decomposed into kinetic and potential contributions. The finite-size error is dominated by the long-range two-body Coulomb interaction, which is most sensitive to the discretization of reciprocal space at small wavevectors $\\mathbf{k}$.\n\nLet $I(N)$ be an intensive physical quantity whose computation involves a summation over the reciprocal lattice vectors $\\{\\mathbf{k}\\}$ in a volume $V$. An example is the potential energy per unit volume. Schematically,\n$$\nI(N) = \\frac{1}{V} \\sum_{\\mathbf{k}} f(\\mathbf{k})\n$$\nwhere $f(\\mathbf{k})$ represents the contribution from a mode $\\mathbf{k}$. In the thermodynamic limit, this sum becomes an integral:\n$$\nI(\\infty) = \\int \\frac{d^3k}{(2\\pi)^3} f(\\mathbf{k})\n$$\nThe governing basis states that the error of this approximation scales with the inverse volume:\n$$\nI(N) - I(\\infty) = \\frac{1}{V} \\sum_{\\mathbf{k}} f(\\mathbf{k}) - \\int \\frac{d^3k}{(2\\pi)^3} f(\\mathbf{k}) \\propto \\frac{1}{V}\n$$\nThe energy per particle, $e(N)$, is an intensive quantity. Its value is determined by such underlying sums. Therefore, the finite-size bias in $e(N)$ must be proportional to the bias in these underlying intensive quantities.\n$$\n\\Delta e(N) = e(N) - e(\\infty) \\propto [I(N) - I(\\infty)]\n$$\nCombining these proportionalities, we find the scaling of the energy bias per particle with volume:\n$$\n\\Delta e(N) \\propto \\frac{1}{V}\n$$\nFor a homogeneous system, the number density $n = N/V$ is held constant as the thermodynamic limit is approached. This provides a direct relationship between the volume $V$ and the particle number $N$: $V = N/n$. Substituting this into our scaling relation gives the dependence on $N$:\n$$\n\\Delta e(N) \\propto \\frac{n}{N}\n$$\nAs the density $n$ is a constant parameter for a given system, the leading-order dependence of the finite-size bias in the energy per particle is:\n$$\n\\Delta e(N) \\propto \\frac{1}{N}\n$$\nFrom this derivation, the energy per particle for a large but finite system size $N$ can be expressed as an asymptotic expansion:\n$$\ne(N) = e(\\infty) + \\frac{A}{N} + \\mathcal{O}\\left(\\frac{1}{N^{\\alpha}}\\right), \\quad \\alpha  1\n$$\nwhere $A$ is a constant that depends on the system properties. For the purpose of extrapolation from a finite set of data, we adopt the minimal consistent regression model by truncating this series at the leading order:\n$$\ne(N) \\approx e(\\infty) + \\frac{A}{N}\n$$\nThis model is linear in the variable $1/N$. The parameters to be estimated are the thermodynamic limit energy $e(\\infty)$ (the intercept) and the coefficient $A$ (the slope).\n\n**Task B: Weighted Least Squares Estimation**\n\nWe are given a set of $M$ data points $(N_i, e_i, \\sigma_i)$, where $e_i$ is a noisy measurement of the true energy $e(N_i)$ with standard error $\\sigma_i$. Our linear model is $e_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, where $x_i = 1/N_i$, $\\beta_0 = e(\\infty)$, $\\beta_1 = A$, and $\\epsilon_i$ represents the statistical noise.\n\nTo find the optimal estimates for the parameters $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$, we use the principle of maximum likelihood, which for independent Gaussian errors is equivalent to minimizing the weighted sum of squared residuals, $\\chi^2$:\n$$\n\\chi^2(\\boldsymbol{\\beta}) = \\sum_{i=1}^{M} w_i (e_i - (\\beta_0 + \\beta_1 x_i))^2\n$$\nThe appropriate weights are the inverse variances, $w_i = 1/\\sigma_i^2$. This gives more influence to the more precise measurements.\n\nThis minimization problem can be expressed in matrix algebra. We aim to solve for $\\hat{\\boldsymbol{\\beta}}$ that minimizes $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{W} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$, where:\n- $\\mathbf{y} = [e_1, e_2, \\dots, e_M]^T$ is the vector of observed energies.\n- $\\mathbf{X}$ is the $M \\times 2$ design matrix, whose rows are $[1, x_i]$:\n  $$\n  \\mathbf{X} = \\begin{pmatrix} 1  1/N_1 \\\\ 1  1/N_2 \\\\ \\vdots  \\vdots \\\\ 1  1/N_M \\end{pmatrix}\n  $$\n- $\\mathbf{W}$ is the $M \\times M$ diagonal matrix of weights:\n  $$\n  \\mathbf{W} = \\text{diag}(1/\\sigma_1^2, 1/\\sigma_2^2, \\dots, 1/\\sigma_M^2)\n  $$\nThe solution to this weighted linear least squares problem is given by the normal equations:\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nThe estimator for the parameter vector is thus:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nFrom the resulting parameter vector $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$, we extract the required physical quantities:\n\n1.  The estimated energy per particle in the thermodynamic limit, $\\hat{e}(\\infty)$, is the intercept of the regression:\n    $$\n    \\hat{e}(\\infty) = \\hat{\\beta}_0\n    $$\n\n2.  The estimated signed two-body finite-size bias at a specified target size $N_{\\mathrm{tgt}}$, denoted $\\Delta_{\\mathrm{tgt}}$, is the difference between the fitted energy at that size and the extrapolated thermodynamic limit energy:\n    $$\n    \\Delta_{\\mathrm{tgt}} = \\hat{e}(N_{\\mathrm{tgt}}) - \\hat{e}(\\infty) = \\left( \\hat{\\beta}_0 + \\frac{\\hat{\\beta}_1}{N_{\\mathrm{tgt}}} \\right) - \\hat{\\beta}_0 = \\frac{\\hat{\\beta}_1}{N_{\\mathrm{tgt}}}\n    $$\nThis procedure provides a complete and statistically principled method for analyzing the provided data. The implementation will proceed by numerically solving the normal equations for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    It orchestrates the data processing, calculation, and final output formatting.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"N\": [32, 54, 128, 250],\n            \"e\": [-0.421500, -0.462241, -0.496500, -0.508700],\n            \"sigma\": [0.000800, 0.001000, 0.000600, 0.000600],\n            \"N_tgt\": 128,\n        },\n        {\n            \"N\": [54, 108],\n            \"e\": [-0.281481, -0.290741],\n            \"sigma\": [0.001500, 0.001000],\n            \"N_tgt\": 108,\n        },\n        {\n            \"N\": [64, 96, 216, 512],\n            \"e\": [-0.710156, -0.723650, -0.738346, -0.745097],\n            \"sigma\": [0.000700, 0.000700, 0.000700, 0.000700],\n            \"N_tgt\": 512,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result_pair = estimate_thermodynamic_limit(\n            np.array(case[\"N\"], dtype=float),\n            np.array(case[\"e\"], dtype=float),\n            np.array(case[\"sigma\"], dtype=float),\n            float(case[\"N_tgt\"]),\n        )\n        results.append(result_pair)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of lists with no spaces.\n    print(f\"[{','.join([f'[{r[0]:.6f},{r[1]:.6f}]' for r in results])}]\")\n\ndef estimate_thermodynamic_limit(N, e, sigma, N_tgt):\n    \"\"\"\n    Performs weighted least squares to extrapolate to the thermodynamic limit.\n\n    Args:\n        N (np.ndarray): Array of particle numbers (system sizes).\n        e (np.ndarray): Array of corresponding energies per particle.\n        sigma (np.ndarray): Array of standard errors for the energies.\n        N_tgt (float): The target system size for bias calculation.\n\n    Returns:\n        tuple: A tuple containing:\n            - (float) The estimated energy in the thermodynamic limit, e_infinity.\n            - (float) The estimated finite-size bias at N_tgt.\n    \"\"\"\n    # The independent variable for the linear model is 1/N.\n    x = 1.0 / N\n\n    # The dependent variable is the energy per particle.\n    y = e\n\n    # The weights for the weighted least squares are the inverse variances.\n    weights = 1.0 / (sigma**2)\n\n    # Construct the design matrix X for the model y = beta_0 * 1 + beta_1 * x.\n    # The first column is for the intercept (beta_0), the second for the slope (beta_1).\n    X = np.vstack((np.ones_like(x), x)).T\n\n    # Construct the diagonal weight matrix W.\n    W = np.diag(weights)\n\n    # Set up the normal equations for weighted least squares:\n    # (X^T * W * X) * beta = X^T * W * y\n    A = X.T @ W @ X\n    b = X.T @ W @ y\n\n    # Solve the system of linear equations for the parameter vector beta.\n    # beta_hat[0] will be the estimate for beta_0 (e_infinity).\n    # beta_hat[1] will be the estimate for beta_1 (the slope A).\n    try:\n        beta_hat = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # This case should not be reached with the given valid inputs.\n        return (np.nan, np.nan)\n        \n    e_infinity_hat = beta_hat[0]\n    A_hat = beta_hat[1]\n\n    # The finite-size bias at the target size N_tgt is A / N_tgt.\n    delta_tgt = A_hat / N_tgt\n\n    return e_infinity_hat, delta_tgt\n\nsolve()\n```", "id": "2828289"}]}