{"hands_on_practices": [{"introduction": "Before diving into complex algorithms, it's crucial to grasp the statistical foundation of Monte Carlo methods. At its core, MC is a powerful technique for approximating integrals by random sampling, but how accurate is this approximation? This exercise [@problem_id:2653261] guides you through an analytical derivation to determine the number of samples required to achieve a specific level of precision, connecting the central limit theorem to the practical considerations of computational cost.", "problem": "In equilibrium statistical mechanics, configurational averages of observables can be expressed as expectations with respect to a target probability density function (PDF). Consider the observable $f(x)=x^2$ defined on the configuration space $\\Omega=[0,1]$, and suppose the target PDF $\\pi(x)$ is uniform on $\\Omega$, that is, $\\pi(x)=1$ for $x \\in [0,1]$. You wish to estimate the integral $I=\\int_{0}^{1} f(x)\\,dx$ using a plain Monte Carlo (MC) scheme based on independent sampling from $\\pi(x)$.\n\nTask: \n1. Construct the plain MC estimator for $I$ using independent and identically distributed samples $X_1, \\dots, X_N$ drawn from the uniform distribution on $[0,1]$, and give its standard error in terms of $N$.\n2. Using only first principles of expectation and variance under independence, determine the minimal integer sample size $N$ required so that the standard error of the estimator is strictly less than $10^{-3}$.\n\nProvide as your final answer only the minimal integer $N$ (dimensionless). No derivations should appear in the final answer. The final answer must be a single number. If rounding is needed, report the minimal integer satisfying the requirement (do not apply a significant-figures rule to this integer).", "solution": "The problem requires the determination of a minimal sample size for a Monte Carlo estimation based on a specified precision. We begin by formally defining the components of the problem.\n\nThe integral to be estimated is $I = \\int_{0}^{1} f(x) \\, dx$, with the observable function given as $f(x) = x^{2}$ over the configuration space $\\Omega = [0,1]$. The target probability density function is uniform on this interval, $\\pi(x) = 1$ for $x \\in [0,1]$. The integral $I$ is therefore equivalent to the expectation of $f(X)$, where $X$ is a random variable drawn from the uniform distribution $U(0,1)$.\n$$\nI = \\int_{0}^{1} x^{2} \\cdot \\pi(x) \\, dx = \\mathbb{E}_{\\pi}[f(X)] = \\mathbb{E}[X^{2}]\n$$\nThis establishes the first principle: the integral is an expectation.\n\nThe plain Monte Carlo estimator for $I$, which we denote as $\\hat{I}_{N}$, is constructed as the sample mean of the observable $f(x)$ evaluated over $N$ independent and identically distributed (i.i.d.) samples $X_{1}, \\dots, X_{N}$, drawn from the distribution $\\pi(x)$.\n$$\n\\hat{I}_{N} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_{i}) = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}^{2}\n$$\nThis is the estimator for the first part of the task.\n\nThe second part of the first task is to determine the standard error of this estimator, $\\text{SE}(\\hat{I}_{N})$. The standard error is the square root of the variance of the estimator, $\\text{SE}(\\hat{I}_{N}) = \\sqrt{\\text{Var}(\\hat{I}_{N})}$. Due to the i.i.d. nature of the samples, the variance of the sample mean is the population variance of the observable divided by the sample size $N$.\n$$\n\\text{Var}(\\hat{I}_{N}) = \\text{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}^{2}\\right) = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\text{Var}(X_{i}^{2}) = \\frac{N \\cdot \\text{Var}(X^{2})}{N^{2}} = \\frac{\\text{Var}(X^{2})}{N}\n$$\nHere, $\\text{Var}(X^{2})$ is the variance of the observable $f(X) = X^{2}$. The variance of a random variable $Y$ is given by $\\text{Var}(Y) = \\mathbb{E}[Y^{2}] - (\\mathbb{E}[Y])^{2}$. Applying this to our observable $X^{2}$:\n$$\n\\text{Var}(X^{2}) = \\mathbb{E}\\left[(X^{2})^{2}\\right] - \\left(\\mathbb{E}[X^{2}]\\right)^{2} = \\mathbb{E}[X^{4}] - \\left(\\mathbb{E}[X^{2}]\\right)^{2}\n$$\nTo proceed, we must calculate the necessary moments of the uniform distribution $U(0,1)$. The $k$-th moment of a random variable $X \\sim U(0,1)$ is:\n$$\n\\mathbb{E}[X^{k}] = \\int_{0}^{1} x^{k} \\pi(x) \\, dx = \\int_{0}^{1} x^{k} \\, dx = \\left[ \\frac{x^{k+1}}{k+1} \\right]_{0}^{1} = \\frac{1}{k+1}\n$$\nWe require the second and fourth moments:\n$$\n\\mathbb{E}[X^{2}] = \\frac{1}{2+1} = \\frac{1}{3}\n$$\n$$\n\\mathbb{E}[X^{4}] = \\frac{1}{4+1} = \\frac{1}{5}\n$$\nSubstituting these results back into the expression for the variance of the observable:\n$$\n\\text{Var}(X^{2}) = \\frac{1}{5} - \\left(\\frac{1}{3}\\right)^{2} = \\frac{1}{5} - \\frac{1}{9} = \\frac{9 - 5}{45} = \\frac{4}{45}\n$$\nWith this result, the standard error of the estimator $\\hat{I}_{N}$ is:\n$$\n\\text{SE}(\\hat{I}_{N}) = \\sqrt{\\frac{\\text{Var}(X^{2})}{N}} = \\sqrt{\\frac{4/45}{N}} = \\frac{2}{\\sqrt{45N}}\n$$\nThis completes the derivation for the first task.\n\nThe second task is to find the minimal integer sample size $N$ such that the standard error is strictly less than $10^{-3}$. We establish the inequality:\n$$\n\\text{SE}(\\hat{I}_{N})  10^{-3}\n$$\n$$\n\\frac{2}{\\sqrt{45N}}  10^{-3}\n$$\nWe now solve this inequality for $N$.\n$$\n\\sqrt{45N} > \\frac{2}{10^{-3}} = 2 \\times 10^{3} = 2000\n$$\nSquaring both sides yields:\n$$\n45N > (2000)^{2} = 4 \\times 10^{6}\n$$\n$$\nN > \\frac{4 \\times 10^{6}}{45}\n$$\nTo determine the limit on $N$, we evaluate the fraction:\n$$\n\\frac{4000000}{45} = \\frac{800000}{9} = 88888.888\\dots = 88888.\\overline{8}\n$$\nThe condition is thus $N > 88888.\\overline{8}$. Since the sample size $N$ must be an integer, the minimal integer value that satisfies this strict inequality is the smallest integer greater than $88888.\\overline{8}$.\n$$\nN_{\\min} = 88889\n$$\nThis is the minimal required sample size.", "answer": "$$\\boxed{88889}$$", "id": "2653261"}, {"introduction": "The Metropolis algorithm revolutionized computational statistical mechanics by providing a simple recipe for sampling from any probability distribution, most notably the Boltzmann distribution. The engine of this algorithm is its acceptance criterion, which guarantees that the simulation will eventually generate states according to the desired target distribution. This practice [@problem_id:2458844] challenges you to analyze a subtle but common implementation of this rule, reinforcing your understanding of the detailed balance condition that underpins the entire method.", "problem": "In canonical-ensemble Monte Carlo sampling of a molecular system at temperature $T$ with Boltzmann constant $k_B$, the target distribution over configurations $\\mathbf{x}$ is $\\pi(\\mathbf{x}) \\propto \\exp(-\\beta E(\\mathbf{x}))$ with $\\beta = 1/(k_B T)$. Consider a Metropolis-type sampler with a symmetric proposal kernel that generates a trial configuration $\\mathbf{x}'$ from the current configuration $\\mathbf{x}$ via a random displacement. The code computes the energy change $\\Delta E = E(\\mathbf{x}') - E(\\mathbf{x})$ and then sets an acceptance probability $P_{\\mathrm{acc}} = \\exp(-\\Delta E/(k_B T))$. It draws $u \\sim \\mathrm{Uniform}(0,1)$ and accepts the move if $u  P_{\\mathrm{acc}}$; otherwise the move is rejected. The code never explicitly takes $\\min(1, \\exp(-\\Delta E/(k_B T)))$.\n\nWhich statement best characterizes the consequences of this implementation for sampling from the canonical distribution under symmetric proposals?\n\nA. The sampling remains correct: the implemented rule yields an actual acceptance probability equal to $\\min(1, \\exp(-\\Delta E/(k_B T)))$, so detailed balance holds and the stationary distribution is Boltzmann.\n\nB. The sampling is biased: downhill moves are accepted with probability greater than $1$, which breaks probability axioms and leads to oversampling of low-energy states beyond the Boltzmann law.\n\nC. Detailed balance is violated for downhill moves, but the stationary distribution is still Boltzmann because low-energy states dominate in the long run.\n\nD. The implementation is only correct if all proposed moves satisfy $\\Delta E \\ge 0$, which is not the case for physical systems, so it is generally invalid.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Ensemble: Canonical ensemble at temperature $T$.\n- Boltzmann constant: $k_B$.\n- Target probability distribution over configurations $\\mathbf{x}$: $\\pi(\\mathbf{x}) \\propto \\exp(-\\beta E(\\mathbf{x}))$.\n- Inverse temperature: $\\beta = 1/(k_B T)$.\n- Sampler: Metropolis-type with a symmetric proposal kernel.\n- A trial configuration $\\mathbf{x}'$ is generated from the current configuration $\\mathbf{x}$.\n- Energy change: $\\Delta E = E(\\mathbf{x}') - E(\\mathbf{x})$.\n- A quantity is computed: $P_{\\mathrm{acc}} = \\exp(-\\Delta E/(k_B T))$.\n- A random number $u$ is drawn from a uniform distribution on the interval $(0,1)$, i.e., $u \\sim \\mathrm{Uniform}(0,1)$.\n- Acceptance criterion: The move is accepted if $u  P_{\\mathrm{acc}}$.\n- Implementation note: The code does not use an explicit $\\min$ function, i.e., it avoids computing $\\min(1, \\exp(-\\Delta E/(k_B T)))$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem describes the Metropolis algorithm, a cornerstone of statistical mechanics simulations for sampling the canonical ensemble. All concepts—Boltzmann distribution, detailed balance, proposal and acceptance steps—are standard and rigorously defined in computational physics and chemistry. The setup is scientifically sound.\n-   **Well-Posedness**: The problem describes a specific implementation of an algorithm and asks for an analysis of its behavior. The description is unambiguous and provides all necessary information to determine the consequences for the sampling process. A unique, logical conclusion can be derived.\n-   **Objectivity**: The problem is stated in precise, objective language without any subjective or speculative elements.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It presents a clear, self-contained, and scientifically sound question regarding a computational algorithm. A rigorous analysis can be performed. I shall proceed with the solution.\n\nThe core of the Metropolis algorithm is the detailed balance condition, which ensures that the Markov chain generated by the sampler has the target distribution $\\pi(\\mathbf{x})$ as its stationary distribution. For a symmetric proposal kernel, where the probability of proposing a move from $\\mathbf{x}$ to $\\mathbf{x}'$ is the same as from $\\mathbf{x}'$ to $\\mathbf{x}$, the detailed balance condition simplifies to:\n$$ \\pi(\\mathbf{x}) A(\\mathbf{x} \\to \\mathbf{x}') = \\pi(\\mathbf{x}') A(\\mathbf{x}' \\to \\mathbf{x}) $$\nwhere $A(\\mathbf{x} \\to \\mathbf{x}')$ is the probability of accepting a proposed move from $\\mathbf{x}$ to $\\mathbf{x}'$.\n\nThis condition is satisfied by the Metropolis choice for the acceptance probability:\n$$ A_{\\text{Metropolis}}(\\mathbf{x} \\to \\mathbf{x}') = \\min\\left(1, \\frac{\\pi(\\mathbf{x}')}{\\pi(\\mathbf{x})}\\right) $$\nGiven that $\\pi(\\mathbf{x}) \\propto \\exp(-\\beta E(\\mathbf{x}))$ and $\\beta = 1/(k_B T)$, the ratio of probabilities is:\n$$ \\frac{\\pi(\\mathbf{x}')}{\\pi(\\mathbf{x})} = \\frac{\\exp(-\\beta E(\\mathbf{x}'))}{\\exp(-\\beta E(\\mathbf{x}))} = \\exp(-\\beta(E(\\mathbf{x}') - E(\\mathbf{x}))) = \\exp(-\\beta \\Delta E) $$\nThus, the correct acceptance probability is:\n$$ A_{\\text{Metropolis}}(\\mathbf{x} \\to \\mathbf{x}') = \\min(1, \\exp(-\\beta \\Delta E)) $$\n\nNow we must analyze the described implementation. The code calculates a value $P_{\\mathrm{acc}} = \\exp(-\\beta \\Delta E)$ and accepts the move if a random number $u \\sim \\mathrm{Uniform}(0,1)$ satisfies the condition $u  P_{\\mathrm{acc}}$. The actual acceptance probability, let us call it $A_{\\text{impl}}$, is the probability that this condition is met:\n$$ A_{\\text{impl}} = \\mathrm{Prob}(u  P_{\\mathrm{acc}}) $$\nWe must analyze this probability for all possible values of $\\Delta E$.\n\n**Case 1: Uphill or iso-energetic move ($\\Delta E \\ge 0$)**\nIn this case, since $\\beta=1/(k_B T)  0$, the argument of the exponential is non-positive: $-\\beta \\Delta E \\le 0$.\nTherefore, the calculated value $P_{\\mathrm{acc}}$ is:\n$$ P_{\\mathrm{acc}} = \\exp(-\\beta \\Delta E) \\le 1 $$\nThe acceptance condition is $u  P_{\\mathrm{acc}}$. Since $u$ is drawn from a uniform distribution on $(0,1)$ and $P_{\\mathrm{acc}}$ is a value between $0$ and $1$ (inclusive), the probability of this event occurring is simply $P_{\\mathrm{acc}}$.\n$$ A_{\\text{impl}} = \\mathrm{Prob}(u  P_{\\mathrm{acc}}) = P_{\\mathrm{acc}} = \\exp(-\\beta \\Delta E) $$\nFor $\\Delta E \\ge 0$, the correct Metropolis probability is $\\min(1, \\exp(-\\beta \\Delta E)) = \\exp(-\\beta \\Delta E)$.\nThus, in this case, $A_{\\text{impl}} = A_{\\text{Metropolis}}$. The implementation is correct.\n\n**Case 2: Downhill move ($\\Delta E  0$)**\nIn this case, the argument of the exponential is strictly positive: $-\\beta \\Delta E  0$.\nTherefore, the calculated value $P_{\\mathrm{acc}}$ is:\n$$ P_{\\mathrm{acc}} = \\exp(-\\beta \\Delta E)  1 $$\nThe acceptance condition is $u  P_{\\mathrm{acc}}$. The random number $u$ is drawn from the interval $(0,1)$, which means $u$ is always less than $1$. Since $P_{\\mathrm{acc}}$ is a number greater than $1$, the inequality $u  P_{\\mathrm{acc}}$ is **always** satisfied.\nThe probability of an event that always occurs is $1$.\n$$ A_{\\text{impl}} = \\mathrm{Prob}(u  P_{\\mathrm{acc}}) = 1 $$\nFor $\\Delta E  0$, the correct Metropolis probability is $\\min(1, \\exp(-\\beta \\Delta E))$. Since $\\exp(-\\beta \\Delta E)  1$, this evaluates to $1$.\nThus, in this case as well, $A_{\\text{impl}} = A_{\\text{Metropolis}}$. The implementation is correct.\n\nIn both possible scenarios, the effective acceptance probability resulting from the implemented code is exactly equivalent to the standard Metropolis acceptance probability. The comparison of a random number from $\\mathrm{Uniform}(0,1)$ with the value $\\exp(-\\beta \\Delta E)$ implicitly and correctly performs the `min(1, ...)` operation. Therefore, the implementation is correct, detailed balance is satisfied, and the sampler will correctly generate configurations from the canonical (Boltzmann) distribution.\n\nNow, we evaluate the given options.\n\n**A. The sampling remains correct: the implemented rule yields an actual acceptance probability equal to $\\min(1, \\exp(-\\Delta E/(k_B T)))$, so detailed balance holds and the stationary distribution is Boltzmann.**\nThis statement is a precise summary of our derivation. The *actual* acceptance probability is indeed $\\min(1, \\exp(-\\beta \\Delta E))$, detailed balance holds, and the sampling is correct.\nVerdict: **Correct**.\n\n**B. The sampling is biased: downhill moves are accepted with probability greater than $1$, which breaks probability axioms and leads to oversampling of low-energy states beyond the Boltzmann law.**\nThis is a flawed interpretation. The quantity $P_{\\mathrm{acc}} = \\exp(-\\beta \\Delta E)$ can be greater than $1$ for downhill moves, but this quantity is not the acceptance probability itself. The acceptance probability is $\\mathrm{Prob}(u  P_{\\mathrm{acc}})$, which cannot exceed $1$. As shown, for downhill moves this probability is exactly $1$, which is a valid probability and is the correct one for the Metropolis algorithm. No probability axiom is broken.\nVerdict: **Incorrect**.\n\n**C. Detailed balance is violated for downhill moves, but the stationary distribution is still Boltzmann because low-energy states dominate in the long run.**\nOur analysis shows that detailed balance is satisfied for all moves, including downhill ones. The premise of this statement is false. If detailed balance were violated, there would be no guarantee that the stationary distribution is the desired Boltzmann distribution. The reasoning is faulty on multiple levels.\nVerdict: **Incorrect**.\n\n**D. The implementation is only correct if all proposed moves satisfy $\\Delta E \\ge 0$, which is not the case for physical systems, so it is generally invalid.**\nThis is false. Our analysis explicitly showed that the implementation is also correct for downhill moves ($\\Delta E  0$), where it correctly yields an acceptance probability of $1$. The method is valid for all possible energy changes.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2458844"}, {"introduction": "Real-world systems often possess symmetries that make non-Cartesian coordinate systems a more natural choice for simulations. When we propose Monte Carlo moves in these curvilinear coordinates, the standard Metropolis rule is insufficient; we must invoke the more general Metropolis-Hastings framework. This hands-on problem [@problem_id:2458841] requires you to build a simulation for a particle on a sphere, demonstrating how to correctly incorporate the Jacobian of the coordinate transformation into the acceptance probability to ensure uniform sampling of the surface.", "problem": "A single particle is constrained to move on the surface of a sphere of radius $R$ in three-dimensional Euclidean space. The surface is parametrized by spherical coordinates $(\\theta, \\phi)$, where $\\theta \\in [0, \\pi]$ is the polar angle and $\\phi \\in [0, 2\\pi)$ is the azimuthal angle. The equilibrium (target) distribution of the particle is uniform with respect to the surface-area measure on the sphere. A proposal in angle space is constructed by adding independent zero-mean increments $\\Delta \\theta$ and $\\Delta \\phi$ to $(\\theta, \\phi)$; the perturbed angles are then mapped back to the canonical ranges by reflecting $\\theta$ at the boundaries $0$ and $\\pi$ and by wrapping $\\phi$ modulo $2\\pi$. The proposal increments $\\Delta \\theta$ and $\\Delta \\phi$ have Gaussian distributions with prescribed standard deviations. The resulting Markov chain must leave the uniform surface-area distribution invariant. The change-of-variables between $(\\theta, \\phi)$ and surface area induces a Jacobian that affects the proposal density on the surface when the kernel is specified in the angular coordinates.\n\nYour tasks are:\n\n- Using only first principles, determine the Jacobian determinant $J(R, \\theta)$ of the transformation from angular coordinates $(\\theta, \\phi)$ to the surface-area element on the sphere of radius $R$. State your result in terms of $R$ and $\\theta$.\n\n- For symmetric angular perturbations (identical forward and reverse distributions in angle space), determine from first principles the multiplicative factor in the acceptance ratio that arises purely from the proposal-density transformation between $(\\theta, \\phi)$ and the surface-area measure, expressed in terms of $\\theta$ and $\\theta'$ for a move $(\\theta, \\phi) \\to (\\theta', \\phi')$.\n\n- Implement a Monte Carlo (MC) simulation of the particle on the sphere using the above angular proposals. Construct two variants:\n  1. A variant that uses the correct proposal-density transformation implied by your Jacobian in its acceptance decision so that the chain is invariant for the uniform surface-area distribution.\n  2. A variant that incorrectly ignores this transformation in its acceptance decision.\n\n- For each variant, estimate the expectation of the observable $f(\\theta, \\phi) = \\cos^2(\\theta)$ under the chain’s stationary distribution, using a fixed seed for reproducibility.\n\nAngles must be in radians. All numerical answers must be expressed as real numbers without units.\n\nTest suite and required outputs:\n\n1. Evaluate the Jacobian determinant $J(R, \\theta)$ at the following parameter pairs:\n   - $(R, \\theta) = (1, \\pi/6)$,\n   - $(R, \\theta) = (2, \\pi/2)$,\n   - $(R, \\theta) = (3, \\pi)$.\n\n2. For symmetric angular perturbations, evaluate the proposal-density Jacobian ratio factor for the following $(\\theta, \\theta')$ pairs:\n   - $(\\theta, \\theta') = (\\pi/12, \\pi/3)$,\n   - $(\\theta, \\theta') = (\\pi/3, 5\\pi/12)$.\n\n3. Run a simulation for $N = 200,000$ total steps with burn-in $B = 5,000$, radius $R = 1$, starting angles $(\\theta_0, \\phi_0) = (1.234, 2.345)$, and Gaussian proposal standard deviations $\\sigma_\\theta = 0.3$ and $\\sigma_\\phi = 0.6$. Use a fixed seed equal to $123$. Produce two estimates for $\\mathbb{E}[\\cos^2(\\theta)]$:\n   - One using the acceptance decision that correctly accounts for the proposal-density transformation due to your Jacobian.\n   - One using an acceptance decision that ignores the proposal-density transformation.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as follows:\n  - The three Jacobian values from item $1$ in the order given.\n  - The two proposal-density Jacobian ratio factors from item $2$ in the order given.\n  - The two Monte Carlo estimates from item $3$ in the order given (first the correct-variant estimate, then the incorrect-variant estimate).\nFor example, the output must have the form [J1, J2, J3, F1, F2, E_correct, E_wrong].", "solution": "The problem presented is a well-posed and scientifically grounded exercise in computational statistical mechanics, specifically concerning the application of the Metropolis-Hastings algorithm to a non-Cartesian coordinate system. A rigorous validation of the problem statement finds no inconsistencies, ambiguities, or violations of scientific principles. I will therefore proceed with a complete solution derived from first principles.\n\nThe solution is presented in three parts, corresponding to the tasks outlined in the problem statement.\n\nFirst, we determine the Jacobian determinant of the transformation from spherical angular coordinates to the surface-area element on a sphere. A point on the surface of a sphere of radius $R$ is parametrized in Cartesian coordinates $(x, y, z)$ using spherical coordinates $(\\theta, \\phi)$ as:\n$$\n\\vec{r}(\\theta, \\phi) = (R \\sin\\theta \\cos\\phi, R \\sin\\theta \\sin\\phi, R \\cos\\theta)\n$$\nwhere $\\theta \\in [0, \\pi]$ is the polar angle and $\\phi \\in [0, 2\\pi)$ is the azimuthal angle. An infinitesimal surface-area element, $dA$, is given by the magnitude of the cross product of the tangent vectors $\\frac{\\partial\\vec{r}}{\\partial\\theta}$ and $\\frac{\\partial\\vec{r}}{\\partial\\phi}$.\n\nThe partial derivatives are:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\theta} = (R \\cos\\theta \\cos\\phi, R \\cos\\theta \\sin\\phi, -R \\sin\\theta)\n$$\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\phi} = (-R \\sin\\theta \\sin\\phi, R \\sin\\theta \\cos\\phi, 0)\n$$\nThe cross product is:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\theta} \\times \\frac{\\partial\\vec{r}}{\\partial\\phi} = (R^2 \\sin^2\\theta \\cos\\phi, R^2 \\sin^2\\theta \\sin\\phi, R^2 \\sin\\theta \\cos\\theta)\n$$\nThe magnitude of this vector, which represents the Jacobian of the transformation, is:\n$$\nJ(R, \\theta) = \\left\\| \\frac{\\partial\\vec{r}}{\\partial\\theta} \\times \\frac{\\partial\\vec{r}}{\\partial\\phi} \\right\\| = \\sqrt{(R^2 \\sin^2\\theta \\cos\\phi)^2 + (R^2 \\sin^2\\theta \\sin\\phi)^2 + (R^2 \\sin\\theta \\cos\\theta)^2}\n$$\n$$\nJ(R, \\theta) = \\sqrt{R^4 \\sin^4\\theta (\\cos^2\\phi + \\sin^2\\phi) + R^4 \\sin^2\\theta \\cos^2\\theta} = \\sqrt{R^4 \\sin^2\\theta (\\sin^2\\theta + \\cos^2\\theta)} = \\sqrt{R^4 \\sin^2\\theta}\n$$\nSince $\\theta \\in [0, \\pi]$, $\\sin\\theta \\ge 0$. Therefore, the Jacobian determinant is:\n$$\nJ(R, \\theta) = R^2 \\sin\\theta\n$$\nThis quantity relates the differential area element $dA$ on the sphere to the differential increments in the angular coordinates: $dA = J(R, \\theta) d\\theta d\\phi = R^2 \\sin\\theta d\\theta d\\phi$.\n\nSecond, we determine the factor in the Metropolis-Hastings acceptance ratio that arises from the coordinate transformation. The acceptance probability $\\alpha$ for a move from a state $s$ to a proposed state $s'$ is given by:\n$$\n\\alpha(s \\to s') = \\min\\left(1, \\frac{\\pi(s')}{\\pi(s)} \\frac{g(s' \\to s)}{g(s \\to s')}\\right)\n$$\nHere, $\\pi(s)$ is the target probability density and $g(s \\to s')$ is the proposal probability density from $s$ to $s'$. The states $s$ and $s'$ are points on the sphere. The target distribution is uniform with respect to the surface-area measure, which implies $\\pi(s) = \\text{constant}$ for all points $s$ on the sphere. Consequently, the ratio of target densities $\\frac{\\pi(s')}{\\pi(s)} = 1$.\n\nThe proposal is made in angular coordinates $(\\theta, \\phi)$, not directly on the surface. Let $g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))$ be the proposal density in the angular coordinate space. The corresponding proposal density on the sphere's surface, $g(s \\to s')$, must be defined with respect to the surface-area measure $dA$. The probability conservation requires $g(s \\to s') dA' = g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi')) d\\theta' d\\phi'$. Using $dA' = J(\\theta') d\\theta' d\\phi'$, we find:\n$$\ng(s \\to s') = \\frac{g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))}{J(\\theta')}\n$$\nSimilarly, for the reverse move:\n$$\ng(s' \\to s) = \\frac{g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi))}{J(\\theta)}\n$$\nThe ratio of proposal densities in the acceptance probability is therefore:\n$$\n\\frac{g(s' \\to s)}{g(s \\to s')} = \\frac{g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi))}{g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))} \\times \\frac{J(\\theta')}{J(\\theta)}\n$$\nThe problem states that the angular perturbations are symmetric, meaning the forward and reverse proposals in angle space have identical distributions. This implies $g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi)) = g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))$. The ratio thus simplifies to the ratio of the Jacobians:\n$$\n\\frac{g(s' \\to s)}{g(s \\to s')} = \\frac{J(\\theta')}{J(\\theta)} = \\frac{R^2 \\sin\\theta'}{R^2 \\sin\\theta} = \\frac{\\sin\\theta'}{\\sin\\theta}\n$$\nThis is the required multiplicative factor. The correct acceptance probability is $\\alpha = \\min\\left(1, \\frac{\\sin\\theta'}{\\sin\\theta}\\right)$.\n\nThird, we design the Monte Carlo simulation. The goal is to estimate the expectation value of the observable $f(\\theta, \\phi) = \\cos^2(\\theta)$ for the uniform distribution on the sphere. The theoretical expectation is:\n$$\n\\mathbb{E}[\\cos^2\\theta] = \\frac{\\int_0^{2\\pi} \\int_0^\\pi \\cos^2\\theta \\sin\\theta \\,d\\theta d\\phi}{\\int_0^{2\\pi} \\int_0^\\pi \\sin\\theta \\,d\\theta d\\phi} = \\frac{2\\pi \\int_0^\\pi \\cos^2\\theta \\sin\\theta \\,d\\theta}{4\\pi} = \\frac{1}{2}\\left[-\\frac{\\cos^3\\theta}{3}\\right]_0^\\pi = \\frac{1}{2}\\left(-\\frac{(-1)^3}{3} - \\left(-\\frac{1^3}{3}\\right)\\right) = \\frac{1}{3}\n$$\nThe simulation is implemented as follows:\n1.  Initialize the state $(\\theta_k, \\phi_k)$ at $k=0$ to $(\\theta_0, \\phi_0)$.\n2.  Iterate for $k = 0, \\dots, N-1$:\n    a. Propose a new state $(\\theta_p, \\phi_p)$ by drawing independent increments $\\Delta\\theta$ and $\\Delta\\phi$ from Gaussian distributions $\\mathcal{N}(0, \\sigma_\\theta^2)$ and $\\mathcal{N}(0, \\sigma_\\phi^2)$, respectively.\n    b. Apply boundary conditions. The new polar angle $\\theta_p$ is obtained by reflecting $\\theta_k + \\Delta\\theta$ at the boundaries $0$ and $\\pi$. This is achieved by the transformation $\\theta_p = \\text{mod}(\\theta_k+\\Delta\\theta, 2\\pi)$ followed by $\\theta_p = 2\\pi - \\theta_p$ if $\\theta_p  \\pi$. The new azimuthal angle $\\phi_p$ is obtained by wrapping $\\phi_k + \\Delta\\phi$ modulo $2\\pi$.\n    c. Calculate the acceptance probability $\\alpha$.\n       - **Correct variant**: $\\alpha = \\min\\left(1, \\frac{\\sin\\theta_p}{\\sin\\theta_k}\\right)$.\n       - **Incorrect variant**: The Jacobian factor is ignored. The acceptance probability becomes $\\alpha = \\min(1, 1) = 1$, meaning all moves are accepted.\n    d. Draw a random number $u \\sim U(0,1)$. If $u  \\alpha$, set $(\\theta_{k+1}, \\phi_{k+1}) = (\\theta_p, \\phi_p)$. Otherwise, $(\\theta_{k+1}, \\phi_{k+1}) = (\\theta_k, \\phi_k)$.\n3.  After a burn-in period of $B$ steps, the expectation $\\mathbb{E}[\\cos^2\\theta]$ is estimated by averaging $\\cos^2(\\theta_k)$ over the remaining $N-B$ steps.\n\nThe incorrect variant samples a probability density that is uniform in $(\\theta, \\phi)$ space, i.e., $p(\\theta, \\phi) \\propto 1$. The expected value under this incorrect distribution is $\\mathbb{E}_{incorrect}[\\cos^2\\theta] = \\frac{1}{2\\pi^2} \\int_0^{2\\pi} d\\phi \\int_0^\\pi \\cos^2\\theta \\,d\\theta = \\frac{1}{\\pi} \\int_0^\\pi \\frac{1+\\cos(2\\theta)}{2}d\\theta = \\frac{1}{2\\pi}[\\theta + \\frac{\\sin(2\\theta)}{2}]_0^\\pi = \\frac{1}{2}$. The simulation results should conform to these theoretical predictions of $1/3$ and $1/2$. The implementation will follow this design precisely.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_mc_simulation(params, correct_jacobian, seed):\n    \"\"\"\n    Runs a Monte Carlo simulation of a particle on a sphere.\n\n    Args:\n        params (tuple): A tuple containing simulation parameters:\n                        (N, B, R, theta0, phi0, sigma_theta, sigma_phi).\n        correct_jacobian (bool): If True, use the correct acceptance criterion.\n                                 If False, use the incorrect one.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The estimated expectation value of cos^2(theta).\n    \"\"\"\n    N, B, R, theta0, phi0, sigma_theta, sigma_phi = params\n    \n    # Initialize a new random number generator for each independent run\n    rng = np.random.default_rng(seed)\n    \n    theta = theta0\n    phi = phi0\n    \n    observable_sum = 0.0\n    samples_collected = 0\n    \n    for step in range(N):\n        # Propose a move in angular coordinates\n        d_theta = rng.normal(0.0, sigma_theta)\n        d_phi = rng.normal(0.0, sigma_phi)\n        \n        theta_prop = theta + d_theta\n        phi_prop = phi + d_phi\n        \n        # Apply boundary conditions\n        # For theta: reflection at 0 and pi\n        # This maps the real line to [0, pi] via folding\n        theta_p = np.mod(theta_prop, 2.0 * np.pi)\n        if theta_p > np.pi:\n            theta_p = 2.0 * np.pi - theta_p\n            \n        # For phi: wrapping modulo 2*pi\n        phi_p = np.mod(phi_prop, 2.0 * np.pi)\n        \n        # Calculate acceptance probability\n        if correct_jacobian:\n            # The target distribution is uniform on the sphere, so pi(s')/pi(s) = 1.\n            # The acceptance probability is determined by the Jacobian factor.\n            sin_theta_k = np.sin(theta)\n            sin_theta_p = np.sin(theta_p)\n            \n            # To avoid division by zero if theta is at a pole (0 or pi).\n            if sin_theta_k  1e-12:\n                # If moving from a pole, the volume element is increasing from zero,\n                # so the move should always be accepted unless the proposed\n                # point is also a pole, in which case the ratio is 1.\n                acceptance_ratio = 1.0 if sin_theta_p  1e-12 else np.inf\n            else:\n                acceptance_ratio = sin_theta_p / sin_theta_k\n            \n            alpha = min(1.0, acceptance_ratio)\n        else:\n            # Incorrect variant: ignore the Jacobian factor.\n            # Since the target density is uniform, the acceptance probability is 1.\n            alpha = 1.0\n            \n        # Accept or reject the move\n        if rng.uniform(0.0, 1.0)  alpha:\n            theta = theta_p\n            phi = phi_p\n            \n        # Collect samples after the burn-in period\n        if step >= B:\n            observable_sum += np.cos(theta)**2\n            samples_collected += 1\n            \n    if samples_collected == 0:\n        return np.nan\n        \n    return observable_sum / samples_collected\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, calculate all required values,\n    and print the final output in the specified format.\n    \"\"\"\n    # ====== Task 1: Evaluate Jacobian determinant J(R, theta) ======\n    # J(R, theta) = R^2 * sin(theta)\n    \n    # Test case 1: (R, theta) = (1, pi/6)\n    R1, theta1 = 1.0, np.pi/6.0\n    J1 = R1**2 * np.sin(theta1)\n    \n    # Test case 2: (R, theta) = (2, pi/2)\n    R2, theta2 = 2.0, np.pi/2.0\n    J2 = R2**2 * np.sin(theta2)\n    \n    # Test case 3: (R, theta) = (3, pi)\n    R3, theta3 = 3.0, np.pi\n    J3 = R3**2 * np.sin(theta3)\n    \n    # ====== Task 2: Evaluate proposal-density Jacobian ratio factor ======\n    # Factor = sin(theta') / sin(theta)\n    \n    # Test case 1: (theta, theta') = (pi/12, pi/3)\n    theta_a1, theta_a2 = np.pi/12.0, np.pi/3.0\n    F1 = np.sin(theta_a2) / np.sin(theta_a1)\n\n    # Test case 2: (theta, theta') = (pi/3, 5*pi/12)\n    theta_b1, theta_b2 = np.pi/3.0, 5.0*np.pi/12.0\n    F2 = np.sin(theta_b2) / np.sin(theta_b1)\n    \n    # ====== Task 3: Run Monte Carlo simulations ======\n    sim_params = (\n        200000,  # N: total steps\n        5000,    # B: burn-in steps\n        1.0,     # R: radius\n        1.234,   # theta0\n        2.345,   # phi0\n        0.3,     # sigma_theta\n        0.6      # sigma_phi\n    )\n    seed = 123\n    \n    # Run simulation with correct Jacobian factor\n    E_correct = run_mc_simulation(sim_params, correct_jacobian=True, seed=seed)\n    \n    # Run simulation with incorrect (ignored) Jacobian factor\n    E_wrong = run_mc_simulation(sim_params, correct_jacobian=False, seed=seed)\n\n    # Collate results\n    results = [J1, J2, J3, F1, F2, E_correct, E_wrong]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2458841"}]}