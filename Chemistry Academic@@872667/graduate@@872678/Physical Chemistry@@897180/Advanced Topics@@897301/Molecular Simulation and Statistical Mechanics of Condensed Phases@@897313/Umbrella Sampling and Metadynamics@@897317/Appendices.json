{"hands_on_practices": [{"introduction": "Before applying complex algorithms, it's crucial to grasp the definition of a potential of mean force (PMF) from first principles. This exercise guides you through calculating a PMF, $F(s)$, for a simple, analytically solvable system using the method of thermodynamic integration. By working through the connection between a constrained ensemble average and the mean force, you will build a solid theoretical foundation for understanding how methods like umbrella sampling function in practice [@problem_id:2685152].", "problem": "Consider a two-dimensional classical system with Cartesian coordinates $x$ and $y$ at temperature $T$, inverse temperature $\\beta = 1/(k_{B}T)$, and potential energy\n$$\nU(x,y) \\;=\\; \\tfrac{1}{2}\\,k_{x}\\,x^{2} \\;+\\; \\tfrac{1}{2}\\,k_{y}\\,\\bigl(y - \\alpha\\,x\\bigr)^{2},\n$$\nwhere $k_{x} > 0$, $k_{y} > 0$, and $\\alpha$ are constants. Let the collective variable be the Cartesian coordinate $s(x,y) = x$. The potential of mean force $F(s)$ along $s$ is defined up to an additive constant via a constrained (conditional) equilibrium ensemble at fixed $s$. The thermodynamic integration identity relates free energy differences to constrained mean generalized forces as\n$$\nF(b) - F(a) \\;=\\; \\int_{a}^{b} \\mathrm{d}s \\,\\bigl\\langle \\tfrac{\\partial U}{\\partial s} \\bigr\\rangle_{s},\n$$\nwhere for a Cartesian collective variable $s=x$ one should interpret $\\tfrac{\\partial U}{\\partial s}$ as $\\tfrac{\\partial U}{\\partial x}$ evaluated on the hypersurface $x=s$.\n\nStarting from the canonical ensemble and the definition of conditional expectation, do the following:\n\n1. Define the constrained ensemble average $\\langle A \\rangle_{s}$ of an observable $A(x,y)$ at fixed $s$ as a ratio of configurational integrals that explicitly involves the Dirac delta function enforcing the holonomic constraint $s(x,y)=s$. Carefully account for the appropriate surface measure that emerges from restricting the full configuration space integral to the hypersurface $s(x,y)=s$. Specialize your definition to the present Cartesian case and simplify it as far as possible.\n\n2. Using your definition, compute the constrained average $\\bigl\\langle \\tfrac{\\partial U}{\\partial s} \\bigr\\rangle_{s}$ for this system in closed form, keeping all parameters symbolic.\n\n3. Use thermodynamic integration to obtain $F(s)$, choosing the reference such that $F(0)=0$ so that $F(s)$ is uniquely defined. Express your final answer as a closed-form analytic expression in terms of $k_{x}$ and $s$. Do not include units in your final expression.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It presents a standard theoretical problem in statistical mechanics concerning the calculation of a potential of mean force for a system with a quadratic potential energy function. All necessary parameters and conditions are provided, and the tasks are logically structured. The problem is valid, and we shall proceed with its solution.\n\nThe solution is organized into three parts as requested by the problem statement.\n\n1. Definition of the constrained ensemble average.\n\nThe canonical ensemble average of an observable $A(q)$, where $q$ represents the set of all coordinates, is given by\n$$\n\\langle A \\rangle = \\frac{\\int A(q) \\exp(-\\beta U(q)) \\, dq}{\\int \\exp(-\\beta U(q)) \\, dq},\n$$\nwhere $U(q)$ is the potential energy, $\\beta = 1/(k_{B}T)$ is the inverse temperature, and the integral is over the entire configuration space.\n\nTo define a constrained ensemble average $\\langle A \\rangle_{s}$ at a fixed value $s$ of a collective variable $s(q)$, we must restrict the configuration space to the hypersurface defined by the holonomic constraint $s(q)=s$. This is achieved formally by introducing the Dirac delta function, $\\delta(s(q)-s)$. The probability density of finding the system at a particular configuration $q$ in the constrained ensemble is proportional to $\\exp(-\\beta U(q)) \\delta(s(q)-s)$.\n\nThus, the constrained ensemble average of an observable $A(q)$ is defined as the ratio of integrals:\n$$\n\\langle A \\rangle_{s} = \\frac{\\int A(q) \\exp(-\\beta U(q)) \\delta(s(q)-s) \\, dq}{\\int \\exp(-\\beta U(q)) \\delta(s(q)-s) \\, dq}.\n$$\nIn the present case, the configuration is specified by the Cartesian coordinates $q=(x,y)$, so $dq=dx\\,dy$. The collective variable is $s(x,y)=x$. The constraint is therefore $x=s$, and the delta function is $\\delta(x-s)$.\n\nSubstituting these into the general definition, we obtain:\n$$\n\\langle A \\rangle_{s} = \\frac{\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} A(x,y) \\exp[-\\beta U(x,y)] \\delta(x-s) \\, dx \\, dy}{\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\exp[-\\beta U(x,y)] \\delta(x-s) \\, dx \\, dy}.\n$$\nWe can now use the sifting property of the Dirac delta function, $\\int f(x) \\delta(x-s) \\, dx = f(s)$, to perform the integration over $x$. This yields:\n$$\n\\langle A \\rangle_{s} = \\frac{\\int_{-\\infty}^{\\infty} A(s,y) \\exp[-\\beta U(s,y)] \\, dy}{\\int_{-\\infty}^{\\infty} \\exp[-\\beta U(s,y)] \\, dy}.\n$$\nThis is the simplified definition for the constrained average in this Cartesian case. The surface measure element $d\\Sigma$ on the hypersurface $x=s$ is simply $dy$. A general surface element includes a Jacobian factor, $d\\Sigma = |\\nabla s|^{-1} dq_s$, where $dq_s$ is the volume element of the surface. Here, $s(x,y)=x$, so $\\nabla s = (1,0)$ and $|\\nabla s| = 1$. The Jacobian factor is unity, which is consistent with our direct simplification.\n\n2. Computation of the constrained average generalized force.\n\nThe generalized force along the collective variable $s=x$ is $\\frac{\\partial U}{\\partial s}$, which is interpreted as $\\frac{\\partial U}{\\partial x}$ evaluated on the hypersurface $x=s$. First, we compute the partial derivative of the potential energy $U(x,y) = \\tfrac{1}{2}\\,k_{x}\\,x^{2} + \\tfrac{1}{2}\\,k_{y}\\,(y - \\alpha\\,x)^{2}$:\n$$\n\\frac{\\partial U}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ \\tfrac{1}{2}\\,k_{x}\\,x^{2} + \\tfrac{1}{2}\\,k_{y}\\,(y - \\alpha\\,x)^{2} \\right] = k_{x}\\,x + k_{y}(y-\\alpha x)(-\\alpha) = k_{x}\\,x - \\alpha k_{y}(y - \\alpha x).\n$$\nWe need to compute the constrained average $\\bigl\\langle \\tfrac{\\partial U}{\\partial s} \\bigr\\rangle_{s}$. The observable to be averaged is the expression for $\\frac{\\partial U}{\\partial x}$ with $x$ replaced by $s$:\n$$\nA(s,y) = \\left. \\frac{\\partial U}{\\partial x} \\right|_{x=s} = k_{x}s - \\alpha k_{y}(y - \\alpha s).\n$$\nUsing the definition from part 1, the average is:\n$$\n\\bigl\\langle \\tfrac{\\partial U}{\\partial s} \\bigr\\rangle_{s} = \\frac{\\int_{-\\infty}^{\\infty} [k_{x}s - \\alpha k_{y}(y - \\alpha s)] \\exp[-\\beta U(s,y)] \\, dy}{\\int_{-\\infty}^{\\infty} \\exp[-\\beta U(s,y)] \\, dy},\n$$\nwhere $U(s,y) = \\tfrac{1}{2}\\,k_{x}\\,s^{2} + \\tfrac{1}{2}\\,k_{y}\\,(y - \\alpha s)^{2}$.\nThe term $\\exp[-\\beta (\\tfrac{1}{2} k_x s^2)]$ is a constant with respect to the integration variable $y$ and will cancel from the numerator and denominator. The expression simplifies to:\n$$\n\\bigl\\langle \\tfrac{\\partial U}{\\partial s} \\bigr\\rangle_{s} = \\frac{\\int_{-\\infty}^{\\infty} [k_{x}s - \\alpha k_{y}(y - \\alpha s)] \\exp[-\\beta \\tfrac{1}{2} k_{y}(y-\\alpha s)^2] \\, dy}{\\int_{-\\infty}^{\\infty} \\exp[-\\beta \\tfrac{1}{2} k_{y}(y-\\alpha s)^2] \\, dy}.\n$$\nWe can separate this into two terms:\n$$\n\\bigl\\langle \\tfrac{\\partial U}{\\partial s} \\bigr\\rangle_{s} = \\frac{\\int_{-\\infty}^{\\infty} k_{x}s \\exp[\\dots] \\, dy}{\\int_{-\\infty}^{\\infty} \\exp[\\dots] \\, dy} - \\frac{\\int_{-\\infty}^{\\infty} \\alpha k_{y}(y - \\alpha s) \\exp[\\dots] \\, dy}{\\int_{-\\infty}^{\\infty} \\exp[\\dots] \\, dy}.\n$$\nFor the first term, $k_x s$ is a constant, so it factors out of the integral, which then cancels with the denominator, leaving $k_x s$.\nFor the second term, we consider the integral $\\int_{-\\infty}^{\\infty} (y - \\alpha s) \\exp[-\\beta \\tfrac{1}{2} k_{y}(y-\\alpha s)^2] \\, dy$. Let $z = y - \\alpha s$, so $dy = dz$. The integral becomes $\\int_{-\\infty}^{\\infty} z \\exp(-\\frac{\\beta k_y}{2} z^2) \\, dz$. The integrand $f(z) = z \\exp(-C z^2)$ (with $C = \\frac{\\beta k_y}{2} > 0$) is an odd function of $z$. The integral of an odd function over a symmetric interval $(-\\infty, \\infty)$ is zero. Therefore, the second term vanishes.\nCombining the results, the constrained average force is:\n$$\n\\bigl\\langle \\tfrac{\\partial U}{\\partial s} \\bigr\\rangle_{s} = k_{x}s.\n$$\n\n3. Thermodynamic integration to obtain the potential of mean force $F(s)$.\n\nThe potential of mean force $F(s)$ is obtained by integrating the mean force along the collective variable, starting from a reference point. The thermodynamic integration identity is given as:\n$$\nF(b) - F(a) = \\int_{a}^{b} \\bigl\\langle \\tfrac{\\partial U}{\\partial s'} \\bigr\\rangle_{s'} \\, ds'.\n$$\nWe set $a=0$ and $b=s$, and use the result from part 2, $\\bigl\\langle \\tfrac{\\partial U}{\\partial s'} \\bigr\\rangle_{s'} = k_{x}s'$.\n$$\nF(s) - F(0) = \\int_{0}^{s} k_{x}s' \\, ds'.\n$$\nPerforming this elementary integration:\n$$\n\\int_{0}^{s} k_{x}s' \\, ds' = k_{x} \\left[ \\frac{(s')^{2}}{2} \\right]_{0}^{s} = k_{x} \\left( \\frac{s^{2}}{2} - \\frac{0^{2}}{2} \\right) = \\frac{1}{2}k_{x}s^{2}.\n$$\nSo, we have $F(s) - F(0) = \\frac{1}{2}k_{x}s^{2}$. The problem specifies the reference value $F(0)=0$, which uniquely determines the potential of mean force.\nSubstituting $F(0)=0$ yields the final expression:\n$$\nF(s) = \\frac{1}{2}k_{x}s^{2}.\n$$\nThis result is independent of the parameters $\\alpha$ and $k_y$. This is because the contribution to the free energy from integrating over the $y$ degree of freedom, for any fixed $x=s$, is a constant independent of $s$. This constant is absorbed into the reference value of the potential of mean force. The $s$-dependence of the PMF arises solely from the part of the potential energy that cannot be averaged out by integrating over the other degrees of freedom. In this case, that is the term $\\frac{1}{2}k_x x^2$.", "answer": "$$\\boxed{\\frac{1}{2} k_{x} s^{2}}$$", "id": "2685152"}, {"introduction": "Well-Tempered Metadynamics is a powerful tool for exploring complex free energy landscapes, but its accuracy is intrinsically linked to its parameters. This problem challenges you to analyze a key source of systematic error: the finite width of the deposited Gaussian hills. By deriving and applying a correction for this effect, you will develop a deeper appreciation for the practical limitations of the method and learn how to critically assess simulation results [@problem_id:2685048].", "problem": "A one-dimensional collective variable $s$ is used to describe a slow transformation in a molecular system at temperature $T$. The potential of mean force (free energy) along $s$ is defined by the canonical marginal $P_{0}(s)$ via $F(s) = -\\beta^{-1} \\ln P_{0}(s) + C$, where $\\beta = (k_{B} T)^{-1}$ and $k_{B}$ is the Boltzmann constant. A Well-Tempered Metadynamics (WTMetaD) simulation with bias factor $\\gamma  1$ is performed, depositing Gaussian hills of width $\\sigma$ in $s$, and is assumed converged to a stationary bias $V_{\\infty}(s)$.\n\nStarting from the canonical definition and the known property that in WTMetaD the long-time stationary distribution along $s$ satisfies $P_{V}(s) \\propto \\left[P_{0}(s)\\right]^{1/\\gamma}$, perform the following:\n\n1. Derive, in the limit $\\sigma \\to 0$, the relation between $F(s)$ and $V_{\\infty}(s)$ and write an explicit reconstruction formula $F_{\\mathrm{rec}}(s)$ in terms of $V_{\\infty}(s)$, $\\gamma$, and $\\beta$, up to an additive constant.\n\n2. Treat the finite hill width $\\sigma$ as inducing a Gaussian convolution of $\\exp(-\\beta F)$ and, in one dimension, carry out a small-$\\sigma$ expansion to the leading non-trivial order to obtain the $s$-dependent systematic deviation $\\Delta(s)$ of the naive reconstruction (that ignores $\\sigma$) from the true $F(s)$. Your derivation must start from the definition\n$$\nF_{\\sigma}(s) \\equiv -\\beta^{-1} \\ln \\int_{-\\infty}^{\\infty} \\mathrm{d}s' \\, \\exp\\left[-\\beta F(s')\\right] \\, \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left[-\\frac{(s-s')^{2}}{2\\sigma^{2}}\\right] + C',\n$$\nand proceed by a controlled Taylor expansion in $\\sigma$ that is valid for sufficiently smooth $F(s)$.\n\n3. Consider the specific double-well free energy\n$$\nF(s) = \\Delta U \\left(\\frac{s^{4}}{4 a^{4}} - \\frac{s^{2}}{2 a^{2}}\\right) + C'',\n$$\nwith wells at $s=\\pm a$ and a barrier at $s=0$. Using your result for $\\Delta(s)$, compute the signed systematic error in the barrier height obtained by the naive WTMetaD reconstruction (which ignores finite $\\sigma$), defined as\n$$\n\\delta \\Delta F^{\\ddagger} \\equiv \\big[F_{\\mathrm{naive}}(0) - F_{\\mathrm{true}}(0)\\big] - \\big[F_{\\mathrm{naive}}(a) - F_{\\mathrm{true}}(a)\\big].\n$$\nUse the parameters $\\Delta U = 12\\,\\mathrm{kJ\\,mol^{-1}}$, $a = 0.25\\,\\mathrm{nm}$, $\\sigma = 0.05\\,\\mathrm{nm}$, $T = 300\\,\\mathrm{K}$, and $\\gamma = 10$.\n\nExpress your final answer for $\\delta \\Delta F^{\\ddagger}$ in $\\mathrm{kJ\\,mol^{-1}}$ and round your answer to three significant figures.", "solution": "The problem posed is valid, scientifically grounded, and well-posed within the framework of statistical mechanics and the theory of enhanced sampling simulations. We shall proceed with its solution in three parts.\n\nThe fundamental relationship between the potential of mean force (PMF), $F(s)$, and the canonical probability distribution along the collective variable $s$, $P_{0}(s)$, is given by $F(s) = -\\beta^{-1} \\ln P_{0}(s) + C$, where $\\beta = (k_{B} T)^{-1}$ and $C$ is an arbitrary constant. This is equivalent to $P_{0}(s) \\propto \\exp(-\\beta F(s))$.\nIn a converged Well-Tempered Metadynamics (WTMetaD) simulation, the system samples a modified probability distribution $P_{V}(s)$ due to the presence of the stationary bias potential $V_{\\infty}(s)$. This distribution is given by $P_{V}(s) \\propto \\exp(-\\beta[F(s) + V_{\\infty}(s)])$. The problem states the key property of WTMetaD: $P_{V}(s) \\propto [P_{0}(s)]^{1/\\gamma}$, where $\\gamma$ is the bias factor.\n\nPart 1: Derivation of the reconstruction formula for $F(s)$ in the ideal limit ($\\sigma \\to 0$).\n\nWe equate the two expressions for the biased distribution $P_{V}(s)$:\n$$\n\\exp(-\\beta[F(s) + V_{\\infty}(s)]) \\propto \\left[\\exp(-\\beta F(s))\\right]^{1/\\gamma} = \\exp\\left(-\\frac{\\beta}{\\gamma} F(s)\\right)\n$$\nTaking the natural logarithm of both sides yields a linear relation between the potentials, up to an additive constant:\n$$\n-\\beta[F(s) + V_{\\infty}(s)] = -\\frac{\\beta}{\\gamma} F(s) + \\mathrm{const.}\n$$\nWe rearrange this equation to solve for the PMF, $F(s)$, in terms of the converged bias potential, $V_{\\infty}(s)$.\n$$\n\\beta V_{\\infty}(s) = -\\frac{\\beta}{\\gamma} F(s) + \\beta F(s) + \\mathrm{const'} = \\beta F(s) \\left(1 - \\frac{1}{\\gamma}\\right) + \\mathrm{const'}\n$$\n$$\nV_{\\infty}(s) = \\left(\\frac{\\gamma - 1}{\\gamma}\\right) F(s) + \\mathrm{const''}\n$$\nThis shows that the converged bias potential is, up to a scaling factor and a constant, equal to the negative of the true PMF, as expected, since the bias is built up to counteract the free energy landscape. The reconstruction formula for the free energy, $F_{\\mathrm{rec}}(s)$, is obtained by inverting this relationship:\n$$\nF_{\\mathrm{rec}}(s) = \\frac{\\gamma}{\\gamma - 1} V_{\\infty}(s) + \\mathrm{const'''}\n$$\nThis is the desired reconstruction formula, valid in the limit of infinitely narrow Gaussian hills ($\\sigma \\to 0$).\n\nPart 2: Derivation of the systematic deviation $\\Delta(s)$ for finite $\\sigma$.\n\nFor a finite Gaussian width $\\sigma$, the deposited bias potential does not perfectly cancel the true PMF. Instead, it cancels a \"smoothed\" or convoluted version of the PMF. The effective PMF that is reconstructed, which we call the naive reconstruction $F_{\\mathrm{naive}}(s)$, is an estimator for this convoluted PMF, $F_{\\sigma}(s)$. The problem defines $F_{\\sigma}(s)$ as:\n$$\nF_{\\sigma}(s) = -\\beta^{-1} \\ln \\int_{-\\infty}^{\\infty} \\mathrm{d}s' \\, \\exp\\left[-\\beta F(s')\\right] \\, \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left[-\\frac{(s-s')^{2}}{2\\sigma^{2}}\\right] + C'\n$$\nThe integral represents the convolution of $g(s) = \\exp[-\\beta F(s)]$ with a Gaussian kernel $K_{\\sigma}(s)$. We analyze this integral by performing a Taylor expansion of $F(s')$ around $s' = s$. Let $u = s' - s$.\n$$\nF(s') = F(s) + F'(s) u + \\frac{1}{2} F''(s) u^{2} + O(u^{3})\n$$\nSubstituting this into the integral:\n$$\nI(s) = \\int_{-\\infty}^{\\infty} \\mathrm{d}u \\, \\exp\\left[-\\beta \\left(F(s) + F'(s) u + \\frac{1}{2} F''(s) u^{2} + \\dots\\right)\\right] \\, \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left[-\\frac{u^{2}}{2\\sigma^{2}}\\right]\n$$\nFactor out the term independent of $u$:\n$$\nI(s) = \\exp[-\\beta F(s)] \\int_{-\\infty}^{\\infty} \\mathrm{d}u \\, \\exp\\left[-\\beta \\left(F'(s) u + \\frac{1}{2} F''(s) u^{2} + \\dots\\right)\\right] \\frac{\\exp\\left[-u^{2}/(2\\sigma^{2})\\right]}{\\sqrt{2\\pi}\\,\\sigma}\n$$\nFor small $\\sigma$, the Gaussian kernel is sharply peaked at $u=0$, so we can expand the other exponential term for small $u$:\n$$\n\\exp\\left[-\\beta \\left(F'(s) u + \\frac{1}{2} F''(s) u^{2} + \\dots\\right)\\right] \\approx 1 - \\beta F'(s) u + \\left( \\frac{1}{2} \\beta^{2} [F'(s)]^{2} - \\frac{1}{2} \\beta F''(s) \\right) u^{2} + \\dots\n$$\nThe integral becomes the expectation value of this expansion over the Gaussian distribution with mean $0$ and variance $\\sigma^{2}$. The moments of this distribution are $\\langle u \\rangle = 0$, $\\langle u^{2} \\rangle = \\sigma^{2}$, $\\langle u^{3} \\rangle = 0$, etc.\n$$\nI(s) \\approx \\exp[-\\beta F(s)] \\left( 1 - \\beta F'(s) \\langle u \\rangle + \\left( \\frac{1}{2} \\beta^{2} [F'(s)]^{2} - \\frac{1}{2} \\beta F''(s) \\right) \\langle u^{2} \\rangle \\right)\n$$\n$$\nI(s) \\approx \\exp[-\\beta F(s)] \\left( 1 + \\frac{\\sigma^{2}}{2} \\left( \\beta^{2} [F'(s)]^{2} - \\beta F''(s) \\right) \\right)\n$$\nNow we substitute this back into the expression for $F_{\\sigma}(s)$:\n$$\nF_{\\sigma}(s) \\approx -\\beta^{-1} \\ln \\left[ \\exp[-\\beta F(s)] \\left( 1 + \\frac{\\sigma^{2}}{2} \\left( \\beta^{2} [F'(s)]^{2} - \\beta F''(s) \\right) \\right) \\right] + C'\n$$\nUsing $\\ln(ab) = \\ln(a) + \\ln(b)$ and the approximation $\\ln(1+x) \\approx x$ for small $x$:\n$$\nF_{\\sigma}(s) \\approx -\\beta^{-1} \\left[ -\\beta F(s) + \\ln \\left( 1 + \\frac{\\sigma^{2}}{2} \\left( \\beta^{2} [F'(s)]^{2} - \\beta F''(s) \\right) \\right) \\right] + C'\n$$\n$$\nF_{\\sigma}(s) \\approx F(s) - \\beta^{-1} \\left[ \\frac{\\sigma^{2}}{2} \\left( \\beta^{2} [F'(s)]^{2} - \\beta F''(s) \\right) \\right] + C''\n$$\n$$\nF_{\\sigma}(s) \\approx F(s) + \\frac{\\sigma^{2}}{2} \\left( F''(s) - \\beta [F'(s)]^{2} \\right) + C''\n$$\nThe systematic deviation is $\\Delta(s) = F_{\\mathrm{naive}}(s) - F_{\\mathrm{true}}(s)$. Since $F_{\\mathrm{naive}}(s) \\approx F_{\\sigma}(s)$ and $F_{\\mathrm{true}}(s) = F(s)$, we have:\n$$\n\\Delta(s) = \\frac{\\sigma^{2}}{2} \\left( F''(s) - \\beta [F'(s)]^{2} \\right)\n$$\nThis is the leading non-trivial order ($\\sigma^2$) correction.\n\nPart 3: Calculation of the signed systematic error in the barrier height.\n\nThe error in the barrier height is defined as $\\delta \\Delta F^{\\ddagger} \\equiv [F_{\\mathrm{naive}}(0) - F_{\\mathrm{true}}(0)] - [F_{\\mathrm{naive}}(a) - F_{\\mathrm{true}}(a)]$. This is equal to $\\Delta(0) - \\Delta(a)$.\nThe given PMF is $F(s) = \\Delta U (\\frac{s^{4}}{4 a^{4}} - \\frac{s^{2}}{2 a^{2}}) + C''$. We compute its first and second derivatives:\n$$\nF'(s) = \\Delta U \\left( \\frac{s^{3}}{a^{4}} - \\frac{s}{a^{2}} \\right)\n$$\n$$\nF''(s) = \\Delta U \\left( \\frac{3s^{2}}{a^{4}} - \\frac{1}{a^{2}} \\right)\n$$\nWe evaluate these derivatives at the barrier top ($s=0$) and in the well ($s=a$):\nAt $s=0$:\n$F'(0) = 0$\n$F''(0) = \\Delta U (0 - \\frac{1}{a^{2}}) = -\\frac{\\Delta U}{a^{2}}$\nAt $s=a$:\n$F'(a) = \\Delta U (\\frac{a^{3}}{a^{4}} - \\frac{a}{a^{2}}) = 0$\n$F''(a) = \\Delta U (\\frac{3a^{2}}{a^{4}} - \\frac{1}{a^{2}}) = \\Delta U (\\frac{3}{a^{2}} - \\frac{1}{a^{2}}) = \\frac{2\\Delta U}{a^{2}}$\nNow we compute $\\Delta(0)$ and $\\Delta(a)$ using the formula from Part 2:\n$$\n\\Delta(0) = \\frac{\\sigma^{2}}{2} \\left( F''(0) - \\beta [F'(0)]^{2} \\right) = \\frac{\\sigma^{2}}{2} \\left( -\\frac{\\Delta U}{a^{2}} - 0 \\right) = -\\frac{\\sigma^{2} \\Delta U}{2 a^{2}}\n$$\n$$\n\\Delta(a) = \\frac{\\sigma^{2}}{2} \\left( F''(a) - \\beta [F'(a)]^{2} \\right) = \\frac{\\sigma^{2}}{2} \\left( \\frac{2\\Delta U}{a^{2}} - 0 \\right) = \\frac{\\sigma^{2} \\Delta U}{a^{2}}\n$$\nNote that since we are evaluating the error at stationary points of the PMF, the term involving $\\beta$ vanishes.\nThe systematic error in the barrier height is:\n$$\n\\delta \\Delta F^{\\ddagger} = \\Delta(0) - \\Delta(a) = -\\frac{\\sigma^{2} \\Delta U}{2 a^{2}} - \\frac{\\sigma^{2} \\Delta U}{a^{2}} = -\\frac{3 \\sigma^{2} \\Delta U}{2 a^{2}}\n$$\nNow, we substitute the given numerical values: $\\Delta U = 12\\,\\mathrm{kJ\\,mol^{-1}}$, $a = 0.25\\,\\mathrm{nm}$, and $\\sigma = 0.05\\,\\mathrm{nm}$.\n$$\n\\delta \\Delta F^{\\ddagger} = -\\frac{3 \\times (0.05\\,\\mathrm{nm})^{2} \\times (12\\,\\mathrm{kJ\\,mol^{-1}})}{2 \\times (0.25\\,\\mathrm{nm})^{2}}\n$$\n$$\n\\delta \\Delta F^{\\ddagger} = -\\frac{3 \\times 0.0025}{2 \\times 0.0625} \\times 12\\,\\mathrm{kJ\\,mol^{-1}} = -\\frac{3}{2} \\left(\\frac{0.05}{0.25}\\right)^{2} \\times 12\\,\\mathrm{kJ\\,mol^{-1}}\n$$\n$$\n\\delta \\Delta F^{\\ddagger} = -\\frac{3}{2} \\left(\\frac{1}{5}\\right)^{2} \\times 12\\,\\mathrm{kJ\\,mol^{-1}} = -\\frac{3}{2} \\times \\frac{1}{25} \\times 12\\,\\mathrm{kJ\\,mol^{-1}}\n$$\n$$\n\\delta \\Delta F^{\\ddagger} = -\\frac{36}{50}\\,\\mathrm{kJ\\,mol^{-1}} = -0.72\\,\\mathrm{kJ\\,mol^{-1}}\n$$\nRounding to three significant figures, the final result is $-0.720\\,\\mathrm{kJ\\,mol^{-1}}$. The negative sign indicates that the finite width of the Gaussian hills leads to a systematic underestimation of the free energy barrier height.", "answer": "$$\n\\boxed{-0.720}\n$$", "id": "2685048"}, {"introduction": "A calculated free energy profile is incomplete without an estimate of its statistical uncertainty. This exercise focuses on the correct application of the bootstrap method, a robust technique for generating confidence intervals from the correlated time-series data produced by umbrella sampling simulations. Mastering this protocol is essential for reporting statistically sound results and making meaningful physical interpretations of your findings [@problem_id:2685134].", "problem": "A molecular system is studied along a scalar collective variable $s(x)$ at temperature $T$, where the potential of mean force (PMF) is defined by the canonical ensemble as $F(s) = -k_{\\mathrm{B}} T \\ln p(s) + C$, with $p(s)$ the unbiased marginal density of $s$ and $C$ an arbitrary constant. To enhance sampling, umbrella sampling is performed in $K$ overlapping windows with harmonic biases $U_k(s) = \\tfrac{1}{2} k_k (s - s_k^\\star)^2$. In window $k$, a time series $\\{s_{k,t}\\}_{t=1}^{N_k}$ is collected under the total potential $U(x) + U_k(s(x))$, producing correlated samples with integrated autocorrelation time $\\tau_k$ for the observable $s$. The unbiased $F(s)$ is estimated by either the Weighted Histogram Analysis Method (WHAM) or the Multistate Bennett Acceptance Ratio (MBAR), which combine information from all windows to reconstruct $p(s)$ from the biased data.\n\nYou are asked to obtain pointwise $95\\%$ confidence intervals for $F(s)$ that account for finite-sample uncertainty and time correlations. From first principles, recall that (i) the canonical distribution is $p(x) \\propto \\exp[-\\beta U(x)]$ with $\\beta = 1/(k_{\\mathrm{B}} T)$, (ii) biasing modifies sampling weights by $\\exp[-\\beta U_k(s)]$, and (iii) the bootstrap approximates the sampling distribution of an estimator by resampling data in a way that respects the data-generating process, including correlation structure and stratification by distinct sampling conditions.\n\nWhich of the following protocols correctly constructs statistically sound bootstrap confidence intervals for $F(s)$ using WHAM or MBAR from the umbrella sampling data described above?\n\nA. For each window $k$, estimate $\\tau_k$ from $\\{s_{k,t}\\}_{t=1}^{N_k}$ and choose a block length $L_k \\ge 2 \\tau_k$ (in units of sampling intervals). Partition the series into non-overlapping blocks of length $L_k$ (discarding a short remainder if needed), then resample these blocks with replacement until $N_k$ samples are reconstructed for window $k$. Repeat this independently for all $k$ to form a bootstrap replicate dataset, apply the same WHAM binning or MBAR analysis to obtain $\\hat{F}^{(b)}(s)$, and iterate for $b = 1, \\dots, B$. Construct pointwise $95\\%$ intervals from the empirical quantiles of $\\{\\hat{F}^{(b)}(s)\\}_{b=1}^B$. Ensure the resampling is stratified by window so that each bootstrap replicate retains the original $\\{N_k\\}$ and window-specific bias functions.\n\nB. Pool all $\\{s_{k,t}\\}$ across windows into a single set of size $\\sum_{k=1}^K N_k$, draw independent bootstrap samples of that size ignoring window labels and biases, apply WHAM with a single effective bias chosen as the average bias over windows, and take the sample standard deviation across bootstrap realizations as the $95\\%$ interval half-width.\n\nC. Perform a “cluster” bootstrap by resampling the $K$ windows with replacement, keeping the full time series of any selected window and discarding any unselected window, then apply WHAM or MBAR to each bootstrap selection. Use the variability across these selections to form $95\\%$ confidence intervals.\n\nD. Within each window, treat all $\\{s_{k,t}\\}$ as independent ($L_k = 1$), resample individual points with replacement to size $N_k$, reconstruct bootstrap datasets by concatenating the per-window resamples, run WHAM or MBAR to obtain $\\hat{F}^{(b)}(s)$, and compute pointwise $95\\%$ confidence intervals from the empirical quantiles.\n\nE. Avoid resampling entirely by linearizing the WHAM fixed-point equations around the solution and using first-order (delta method) error propagation with the Fisher information to obtain asymptotic Gaussian error bars for $F(s)$; report these as the $95\\%$ intervals.\n\nSelect the option that best reflects a correct bootstrap-based protocol for this task.", "solution": "We begin from the canonical definition $p(x) \\propto \\exp[-\\beta U(x)]$ with $\\beta = 1/(k_{\\mathrm{B}} T)$. The potential of mean force is $F(s) = -k_{\\mathrm{B}} T \\ln p(s) + C$, where $p(s) = \\int \\mathrm{d}x\\, \\delta(s - s(x))\\, p(x)$. In umbrella sampling, configurations in window $k$ are generated under $U_k^{\\mathrm{tot}}(x) = U(x) + U_k(s(x))$, so the observed marginal density in that window is $p_k^{\\mathrm{obs}}(s) \\propto p(s)\\, \\exp[-\\beta U_k(s)]$. WHAM and Multistate Bennett Acceptance Ratio (MBAR) are likelihood-based estimators that combine data from all windows by properly reweighting the biased samples to reconstruct $p(s)$ (and hence $F(s)$). \n\nUncertainty quantification should reflect the finite-sample variability of the estimator under the actual data-generating process. A bootstrap emulates repeated experiments by resampling the collected data in a way that preserves two key features: (i) stratification by window $k$ (each window has a distinct sampling distribution determined by $U_k$ and its own sample size $N_k$), and (ii) time correlations within each window due to molecular dynamics, quantified by the integrated autocorrelation time $\\tau_k$ for $s$ (or for any slow collective variable relevant to $F(s)$ estimation). Because naive resampling of individual points would overcount correlated information, a block bootstrap is required, with block lengths at least on the order of $2 \\tau_k$ to approximate independent blocks. Each bootstrap replicate should then be processed through the same estimator (WHAM or MBAR), and the distribution of the resulting $\\hat{F}^{(b)}(s)$ across $b = 1, \\dots, B$ replicates can be used to form pointwise $95\\%$ intervals via empirical percentiles (or via the bootstrap standard error and normal approximation if justified).\n\nWe assess each option:\n\nA. This protocol explicitly:\n- Estimates $\\tau_k$ per window and chooses a block length $L_k \\ge 2 \\tau_k$, which is a standard rule of thumb ensuring blocks are approximately independent, preserving the correlation structure within windows.\n- Performs a stratified block bootstrap: resampling blocks within each window to reconstruct $N_k$ samples, thereby maintaining the window-specific sampling distribution and sample size.\n- Runs the identical WHAM binning or MBAR analysis on each bootstrap dataset to propagate uncertainty through the nonlinear estimator without linearization.\n- Constructs pointwise $95\\%$ confidence intervals from the empirical quantiles $\\{2.5\\%, 97.5\\%\\}$ of $\\hat{F}^{(b)}(s)$ for each $s$.\nThis matches the fundamental requirements: preserve the biased data-generating process per window and the correlation structure, and propagate uncertainty by reapplying the full estimator. This is statistically sound. Verdict: Correct.\n\nB. Pooling all samples and ignoring window labels destroys the stratification and obliterates the known differences in sampling distributions induced by $U_k(s)$. Assigning a single “average bias” is not justified by first principles: the observed $p_k^{\\mathrm{obs}}(s)$ differ by $\\exp[-\\beta U_k(s)]$, and WHAM/MBAR rely on those differences. This resampling scheme does not approximate the original experiment and thus yields invalid uncertainty. Verdict: Incorrect.\n\nC. A pure “cluster” bootstrap over windows resamples entire windows and discards others, capturing between-window variability but not within-window sampling variability at the correct scale. With fixed $K$, this can severely distort the effective sample sizes and bias coverage; it also fails to represent the correlation structure within each window unless supplemented by within-window block resampling. As stated, it is not an appropriate bootstrap for this task. Verdict: Incorrect.\n\nD. Resampling individual points with $L_k = 1$ treats correlated data as independent. The integrated autocorrelation time $\\tau_k$ implies an effective sample size $N_k^{\\mathrm{eff}} \\approx N_k / (2 \\tau_k)$, so naive independent resampling will drastically underestimate variance and produce anticonservative confidence intervals. Verdict: Incorrect.\n\nE. This describes an analytic, asymptotic variance calculation via linearization and the Fisher information, which may be valid under large-sample regularity but is not a bootstrap procedure and requires nontrivial derivations and assumptions. The question asks for a bootstrap-based protocol; furthermore, analytic linearization can be inaccurate for strongly nonlinear estimators or in finite samples. Verdict: Incorrect.\n\nTherefore, the only correct bootstrap protocol among the options is A.", "answer": "$$\\boxed{A}$$", "id": "2685134"}]}