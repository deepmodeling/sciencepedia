{"hands_on_practices": [{"introduction": "This practice focuses on a core diagnostic skill in REMD: predicting the exchange acceptance rate from simulation data. Efficient REMD simulations depend on a reasonable probability of swapping between adjacent temperature replicas, which requires sufficient overlap in their potential energy distributions. This exercise [@problem_id:2666608] provides a quantitative framework for this concept, asking you to derive and apply the formula for the expected acceptance probability assuming the energy distributions can be approximated as Gaussianâ€”a common scenario for complex systems.", "problem": "In Replica Exchange Molecular Dynamics (REMD), two replicas at inverse temperatures $\\beta_1$ and $\\beta_2$ attempt a swap of temperatures with Metropolis acceptance probability given by $\\min\\!\\left(1,\\exp\\!\\left((\\beta_1-\\beta_2)\\,(E_1-E_2)\\right)\\right)$, where $E_1$ and $E_2$ are the instantaneous potential energies sampled independently from the canonical ensembles at temperatures $T_1$ and $T_2$. Consider two adjacent temperatures $T_1=300\\ \\mathrm{K}$ and $T_2=330\\ \\mathrm{K}$. Suppose the measured energy statistics at these temperatures (energies expressed per mole) are approximately Gaussian with means $\\mu_1=-500.0\\ \\mathrm{kJ\\,mol^{-1}}$ and $\\mu_2=-440.0\\ \\mathrm{kJ\\,mol^{-1}}$, and variances $\\sigma_1^2=1496.6033\\ \\mathrm{kJ^2\\,mol^{-2}}$ and $\\sigma_2^2=1810.8900\\ \\mathrm{kJ^2\\,mol^{-2}}$. Assume the two energy samples are independent and that the Gaussian approximation for the canonical energy distribution is valid.\n\nStarting only from the canonical ensemble definition, the Metropolis acceptance criterion for exchanges, and the independence and Gaussian assumptions stated above, derive an analytic expression for the expected swap acceptance probability under these conditions and evaluate it numerically. Use the gas constant $R=0.008314462618\\ \\mathrm{kJ\\,mol^{-1}\\,K^{-1}}$ so that $\\beta_i = 1/(R T_i)$ is consistent with the per-mole energy units. Round your final expected acceptance probability to four significant figures. Express the final answer as a pure number (no units).", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary data and well-defined assumptions are provided. It represents a standard calculation in the analysis of Replica Exchange Molecular Dynamics simulations. Therefore, the problem is valid, and a solution will be provided.\n\nThe objective is to compute the expected value of the Metropolis acceptance probability, $\\langle P_{\\text{acc}} \\rangle$, for a swap between two replicas at inverse temperatures $\\beta_1$ and $\\beta_2$. The acceptance probability is given by\n$$\nP_{\\text{acc}}(E_1, E_2) = \\min\\!\\left(1, \\exp\\!\\left((\\beta_1-\\beta_2)(E_1-E_2)\\right)\\right)\n$$\nwhere $E_1$ and $E_2$ are the potential energies of the replicas. These are random variables drawn independently from their respective canonical ensembles. The problem states that these energy distributions are approximated as Gaussian: $E_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $E_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)$.\n\nLet us define the difference in inverse temperature $\\Delta\\beta = \\beta_1 - \\beta_2$ and the difference in energy $\\Delta E = E_1 - E_2$. The acceptance probability simplifies to $P_{\\text{acc}}(\\Delta E) = \\min(1, \\exp(\\Delta\\beta \\Delta E))$. Since $E_1$ and $E_2$ are independent Gaussian random variables, their difference, $\\Delta E$, is also a Gaussian random variable. Its distribution is $\\Delta E \\sim \\mathcal{N}(\\mu_{\\Delta}, \\sigma_{\\Delta}^2)$, with parameters:\nMean: $\\mu_{\\Delta} = \\mathbb{E}[E_1 - E_2] = \\mathbb{E}[E_1] - \\mathbb{E}[E_2] = \\mu_1 - \\mu_2$.\nVariance: $\\sigma_{\\Delta}^2 = \\text{Var}(E_1 - E_2) = \\text{Var}(E_1) + \\text{Var}(-E_2) = \\text{Var}(E_1) + (-1)^2\\text{Var}(E_2) = \\sigma_1^2 + \\sigma_2^2$.\n\nThe expected acceptance probability is found by integrating over the probability distribution of $\\Delta E$, which we denote by $p_{\\Delta}(x)$:\n$$\n\\langle P_{\\text{acc}} \\rangle = \\mathbb{E}[P_{\\text{acc}}(\\Delta E)] = \\int_{-\\infty}^{\\infty} \\min(1, \\exp(\\Delta\\beta x)) p_{\\Delta}(x) dx\n$$\nwhere $p_{\\Delta}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\Delta}^2}} \\exp\\left(-\\frac{(x - \\mu_{\\Delta})^2}{2\\sigma_{\\Delta}^2}\\right)$.\n\nGiven $T_1 = 300\\ \\mathrm{K}$ and $T_2 = 330\\ \\mathrm{K}$, we have $T_1  T_2$, which implies $\\beta_1 = 1/(RT_1) > 1/(RT_2) = \\beta_2$. Thus, $\\Delta\\beta > 0$. This allows us to split the integral based on the sign of the argument of the exponential, $x = \\Delta E$:\n- If $x > 0$, then $\\Delta\\beta x > 0$, so $\\exp(\\Delta\\beta x) > 1$, and $\\min(1, \\exp(\\Delta\\beta x)) = 1$.\n- If $x \\le 0$, then $\\Delta\\beta x \\le 0$, so $\\exp(\\Delta\\beta x) \\le 1$, and $\\min(1, \\exp(\\Delta\\beta x)) = \\exp(\\Delta\\beta x)$.\n\nThe expectation integral is therefore split into two parts:\n$$\n\\langle P_{\\text{acc}} \\rangle = \\int_{0}^{\\infty} 1 \\cdot p_{\\Delta}(x) dx + \\int_{-\\infty}^{0} \\exp(\\Delta\\beta x) p_{\\Delta}(x) dx\n$$\nThe first integral is the probability that $\\Delta E > 0$, which can be expressed using the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^z \\exp(-t^2/2) dt$.\n$$\n\\int_{0}^{\\infty} p_{\\Delta}(x) dx = P(\\Delta E > 0) = 1 - P(\\Delta E \\le 0) = 1 - \\Phi\\left(\\frac{0 - \\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right) = 1 - \\Phi\\left(-\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right) = \\Phi\\left(\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right)\n$$\nThe second integral requires us to combine the exponential terms in the integrand:\n$$\n\\int_{-\\infty}^{0} \\exp(\\Delta\\beta x) \\frac{1}{\\sqrt{2\\pi\\sigma_{\\Delta}^2}} \\exp\\left(-\\frac{(x - \\mu_{\\Delta})^2}{2\\sigma_{\\Delta}^2}\\right) dx\n$$\nThe argument of the total exponential is $\\Delta\\beta x - \\frac{(x - \\mu_{\\Delta})^2}{2\\sigma_{\\Delta}^2}$. We complete the square with respect to $x$:\n$$\n-\\frac{1}{2\\sigma_{\\Delta}^2} [x^2 - 2x\\mu_{\\Delta} + \\mu_{\\Delta}^2 - 2\\sigma_{\\Delta}^2\\Delta\\beta x] = -\\frac{1}{2\\sigma_{\\Delta}^2} [x^2 - 2x(\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta) + \\mu_{\\Delta}^2]\n$$\nLet $\\mu' = \\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta$. The term in the brackets is rewritten as:\n$$\n[x - \\mu']^2 - (\\mu')^2 + \\mu_{\\Delta}^2 = [x - \\mu']^2 - (\\mu_{\\Delta}^2 + 2\\mu_{\\Delta}\\sigma_{\\Delta}^2\\Delta\\beta + (\\sigma_{\\Delta}^2\\Delta\\beta)^2) + \\mu_{\\Delta}^2 = [x - \\mu']^2 - 2\\mu_{\\Delta}\\sigma_{\\Delta}^2\\Delta\\beta - \\sigma_{\\Delta}^4(\\Delta\\beta)^2\n$$\nThe exponent becomes:\n$$\n-\\frac{(x - \\mu')^2}{2\\sigma_{\\Delta}^2} + \\frac{2\\mu_{\\Delta}\\sigma_{\\Delta}^2\\Delta\\beta + \\sigma_{\\Delta}^4(\\Delta\\beta)^2}{2\\sigma_{\\Delta}^2} = -\\frac{(x - \\mu')^2}{2\\sigma_{\\Delta}^2} + \\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}\n$$\nThe second integral is now:\n$$\n\\int_{-\\infty}^{0} \\exp\\left(\\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_{\\Delta}^2}} \\exp\\left(-\\frac{(x - \\mu')^2}{2\\sigma_{\\Delta}^2}\\right) dx\n$$\nThe constant exponential factor can be moved outside the integral. The remaining integral is the probability $P(X' \\le 0)$ for a Gaussian variable $X' \\sim \\mathcal{N}(\\mu', \\sigma_{\\Delta}^2)$.\n$$\nP(X' \\le 0) = \\Phi\\left(\\frac{0 - \\mu'}{\\sigma_{\\Delta}}\\right) = \\Phi\\left(-\\frac{\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta}{\\sigma_{\\Delta}}\\right)\n$$\nCombining all parts, the final analytic expression is:\n$$\n\\langle P_{\\text{acc}} \\rangle = \\Phi\\left(\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}}\\right) + \\exp\\left(\\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}\\right) \\Phi\\left(-\\frac{\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta}{\\sigma_{\\Delta}}\\right)\n$$\nNow, we substitute the given numerical values.\n$R = 0.008314462618\\ \\mathrm{kJ\\,mol^{-1}\\,K^{-1}}$.\nTemperatures: $T_1 = 300\\ \\mathrm{K}$, $T_2 = 330\\ \\mathrm{K}$.\n$\\Delta\\beta = \\frac{1}{R}\\left(\\frac{1}{T_1} - \\frac{1}{T_2}\\right) = \\frac{1}{0.008314462618}\\left(\\frac{1}{300} - \\frac{1}{330}\\right) = \\frac{1}{0.008314462618}\\left(\\frac{30}{99000}\\right) \\approx 0.036446164\\ (\\mathrm{kJ\\,mol^{-1}})^{-1}$.\n\nEnergy statistics:\n$\\mu_1 = -500.0\\ \\mathrm{kJ\\,mol^{-1}}$, $\\mu_2 = -440.0\\ \\mathrm{kJ\\,mol^{-1}}$.\n$\\sigma_1^2 = 1496.6033\\ \\mathrm{kJ^2\\,mol^{-2}}$, $\\sigma_2^2 = 1810.8900\\ \\mathrm{kJ^2\\,mol^{-2}}$.\n\nParameters for the $\\Delta E$ distribution:\n$\\mu_{\\Delta} = \\mu_1 - \\mu_2 = -500.0 - (-440.0) = -60.0\\ \\mathrm{kJ\\,mol^{-1}}$.\n$\\sigma_{\\Delta}^2 = \\sigma_1^2 + \\sigma_2^2 = 1496.6033 + 1810.8900 = 3307.4933\\ \\mathrm{kJ^2\\,mol^{-2}}$.\n$\\sigma_{\\Delta} = \\sqrt{3307.4933} \\approx 57.5108085\\ \\mathrm{kJ\\,mol^{-1}}$.\n\nWe calculate the arguments for the functions in the analytic expression:\nArgument of the first $\\Phi$: $\\frac{\\mu_{\\Delta}}{\\sigma_{\\Delta}} = \\frac{-60.0}{57.5108085} \\approx -1.043282$.\n$\\Phi(-1.043282) \\approx 0.14841$.\n\nExponent of the exponential term: $\\mu_{\\Delta}\\Delta\\beta + \\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2}$\nTerm 1: $\\mu_{\\Delta}\\Delta\\beta = -60.0 \\times 0.036446164 \\approx -2.186770$.\nTerm 2: $\\frac{\\sigma_{\\Delta}^2(\\Delta\\beta)^2}{2} = \\frac{3307.4933 \\times (0.036446164)^2}{2} \\approx \\frac{4.393433}{2} = 2.196717$.\nExponent = $-2.186770 + 2.196717 = 0.009947$.\n$\\exp(0.009947) \\approx 1.009996$.\n\nArgument of the second $\\Phi$: $-\\frac{\\mu_{\\Delta} + \\sigma_{\\Delta}^2\\Delta\\beta}{\\sigma_{\\Delta}}$\n$\\sigma_{\\Delta}^2\\Delta\\beta = 3307.4933 \\times 0.036446164 \\approx 120.5483$.\nArgument $= -\\frac{-60.0 + 120.5483}{57.5108085} = -\\frac{60.5483}{57.5108085} \\approx -1.052816$.\n$\\Phi(-1.052816) \\approx 0.14620$.\n\nFinally, we compute $\\langle P_{\\text{acc}} \\rangle$:\n$\\langle P_{\\text{acc}} \\rangle \\approx 0.14841 + (1.009996) \\times (0.14620) \\approx 0.14841 + 0.147661 \\approx 0.296071$.\n\nRounding to four significant figures, the result is $0.2961$.", "answer": "$$\n\\boxed{0.2961}\n$$", "id": "2666608"}, {"introduction": "Moving from analysis to design, this practice addresses the crucial task of setting up an efficient REMD temperature ladder before a full-scale simulation. An optimal ladder ensures a uniform and reasonable exchange probability across all replica pairs, facilitating a rapid random walk in temperature space. This problem [@problem_id:2666613] connects the macroscopic property of heat capacity, $C_V$, to the microscopic details of energy fluctuations and exchange acceptance, allowing you to estimate the required temperature spacing $\\Delta T$ for a desired outcome.", "problem": "A Replica Exchange Molecular Dynamics (REMD) ladder is constructed for a system whose constant-volume heat capacity is $C_V = 1000\\,k_{\\mathrm{B}}$ near a reference temperature $T = 300\\ \\mathrm{K}$. Under the Gaussian model of canonical energy fluctuations with constant heat capacity, the expected exchange acceptance probability between adjacent temperatures $T$ and $T+\\Delta T$ can be obtained from first principles by averaging the Metropolis acceptance criterion over the joint canonical energy distribution of two replicas. Using only fundamental relations of the canonical ensemble, the Metropolis criterion, and the Gaussian approximation justified by large heat capacity, derive the acceptance probability as a function of the temperature ratio and use it to estimate the temperature increment $\\Delta T$ required to achieve an expected acceptance of $0.3$ at $T=300\\ \\mathrm{K}$ for this system. \n\nExpress your final answer for $\\Delta T$ in $\\mathrm{K}$ and round your result to four significant figures.", "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. It requires the derivation of the Replica Exchange Molecular Dynamics (REMD) acceptance probability under a specific set of standard approximations and a subsequent numerical calculation. We will proceed with the solution.\n\nThe fundamental principle of replica exchange is to perform a Monte Carlo move on the extended ensemble of system replicas, where the move consists of swapping the temperatures of two replicas. Let us consider two replicas, $1$ and $2$, with fixed configurations having potential energies $E_1$ and $E_2$. Replica $1$ is at temperature $T_1$ (inverse temperature $\\beta_1 = 1/(k_B T_1)$) and replica $2$ is at temperature $T_2$ (inverse temperature $\\beta_2 = 1/(k_B T_2)$). The total probability of this state in the extended canonical ensemble is proportional to $\\exp(-\\beta_1 E_1 - \\beta_2 E_2)$.\n\nA swap is proposed, where replica $1$ is assigned temperature $T_2$ and replica $2$ is assigned temperature $T_1$. The energies of the configurations remain $E_1$ and $E_2$. The new state has a probability proportional to $\\exp(-\\beta_2 E_1 - \\beta_1 E_2)$. The acceptance probability for this swap is given by the Metropolis criterion:\n$$\np_{\\text{swap}}(E_1, E_2) = \\min\\left(1, \\frac{\\exp(-\\beta_2 E_1 - \\beta_1 E_2)}{\\exp(-\\beta_1 E_1 - \\beta_2 E_2)}\\right)\n$$\nSimplifying the ratio gives:\n$$\n\\frac{\\exp(-\\beta_2 E_1 - \\beta_1 E_2)}{\\exp(-\\beta_1 E_1 - \\beta_2 E_2)} = \\exp\\left((\\beta_1 - \\beta_2)E_1 - (\\beta_1 - \\beta_2)E_2\\right) = \\exp\\left((\\beta_1 - \\beta_2)(E_1 - E_2)\\right)\n$$\nLet $\\Delta\\beta = \\beta_1 - \\beta_2$ and $\\Delta E = E_1 - E_2$. Then the acceptance probability is:\n$$\np_{\\text{swap}}(\\Delta E) = \\min(1, \\exp(\\Delta\\beta \\cdot \\Delta E))\n$$\nIn a simulation, the energies $E_1$ and $E_2$ are not fixed but are stochastic variables drawn from the canonical probability distributions at their respective temperatures, $P(E_1; T_1)$ and $P(E_2; T_2)$. The problem states that due to the large heat capacity, these distributions can be approximated as Gaussian. From statistical mechanics, the variance of energy fluctuations in the canonical ensemble is related to the heat capacity $C_V$ by $\\sigma_E^2 = \\langle E^2 \\rangle - \\langle E \\rangle^2 = k_B T^2 C_V$.\nThe energy distributions for the two replicas are thus:\n$$\nE_1 \\sim \\mathcal{N}(\\langle E_1 \\rangle, \\sigma_{E_1}^2) \\quad \\text{with} \\quad \\sigma_{E_1}^2 = k_B T_1^2 C_V\n$$\n$$\nE_2 \\sim \\mathcal{N}(\\langle E_2 \\rangle, \\sigma_{E_2}^2) \\quad \\text{with} \\quad \\sigma_{E_2}^2 = k_B T_2^2 C_V\n$$\nThe problem specifies that $C_V$ is constant. The change in average energy is related to $C_V$ by $d\\langle E \\rangle = C_V dT$. Integrating this from $T_1$ to $T_2$ gives $\\langle E_2 \\rangle - \\langle E_1 \\rangle = C_V (T_2-T_1)$.\n\nSince the two replicas are independent, the energy difference $\\Delta E = E_1 - E_2$ is also a Gaussian random variable. Its mean and variance are:\n$$\n\\langle \\Delta E \\rangle = \\langle E_1 \\rangle - \\langle E_2 \\rangle = -C_V(T_2-T_1)\n$$\n$$\n\\sigma_{\\Delta E}^2 = \\text{Var}(E_1) + \\text{Var}(E_2) = k_B T_1^2 C_V + k_B T_2^2 C_V = k_B C_V (T_1^2 + T_2^2)\n$$\nThe expected acceptance probability, $\\langle p \\rangle$, is obtained by averaging $p_{\\text{swap}}(\\Delta E)$ over the probability distribution of $\\Delta E$. Let $X = \\Delta\\beta \\cdot \\Delta E$. Since $X$ is a linear transformation of the Gaussian variable $\\Delta E$, $X$ is also Gaussian. Its mean $\\langle X \\rangle$ and variance $\\sigma_X^2$ are:\n$$\n\\langle X \\rangle = \\Delta\\beta \\langle \\Delta E \\rangle = \\left(\\frac{1}{k_B T_1} - \\frac{1}{k_B T_2}\\right) (-C_V(T_2-T_1)) = -\\frac{C_V}{k_B} \\frac{T_2-T_1}{T_1 T_2} (T_2-T_1) = -\\frac{C_V}{k_B} \\frac{(T_2-T_1)^2}{T_1 T_2}\n$$\n$$\n\\sigma_X^2 = (\\Delta\\beta)^2 \\sigma_{\\Delta E}^2 = \\left(\\frac{T_2-T_1}{k_B T_1 T_2}\\right)^2 k_B C_V (T_1^2 + T_2^2) = \\frac{C_V}{k_B} \\frac{(T_2-T_1)^2 (T_1^2+T_2^2)}{(T_1 T_2)^2}\n$$\nThe expected acceptance probability is $\\langle p \\rangle = \\int_{-\\infty}^\\infty \\min(1, \\exp(x)) P(x) dx$, where $P(x)$ is the Gaussian distribution for $X$. While the exact evaluation of this integral is complex (see Practice Problem 1), a widely used and accurate approximation is given by $\\langle p \\rangle \\approx \\text{erfc}(|\\langle X \\rangle|/(\\sigma_X\\sqrt{2}))$. Since $\\langle X \\rangle$ is negative in our case (with $T_2 > T_1$), this is equivalent to $2\\Phi(\\langle X \\rangle/\\sigma_X)$, where $\\Phi(z)$ is the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$. We proceed with this established approximation.\n\nLet us compute the ratio $\\langle X \\rangle / \\sigma_X$:\n$$\n\\frac{\\langle X \\rangle}{\\sigma_X} = \\frac{-\\frac{C_V}{k_B} \\frac{(T_2-T_1)^2}{T_1 T_2}}{\\sqrt{\\frac{C_V}{k_B} \\frac{(T_2-T_1)^2 (T_1^2+T_2^2)}{(T_1 T_2)^2}}} = \\frac{-\\sqrt{\\frac{C_V}{k_B}} (T_2-T_1)}{\\sqrt{T_1^2+T_2^2}}\n$$\nLet $T_1 = T$ and $T_2 = T+\\Delta T$. The expected acceptance probability is:\n$$\n\\langle p \\rangle \\approx 2\\Phi\\left(-\\sqrt{\\frac{C_V}{k_B}} \\frac{\\Delta T}{\\sqrt{T^2+(T+\\Delta T)^2}}\\right)\n$$\nTo express this as a function of the temperature ratio $r = T_2/T_1 = (T+\\Delta T)/T$, we note that $\\Delta T = T(r-1)$. Substituting this yields:\n$$\n\\frac{\\Delta T}{\\sqrt{T^2+(T+\\Delta T)^2}} = \\frac{T(r-1)}{\\sqrt{T^2 + (rT)^2}} = \\frac{T(r-1)}{T\\sqrt{1+r^2}} = \\frac{r-1}{\\sqrt{1+r^2}}\n$$\nThus, the acceptance probability as a function of the temperature ratio is:\n$$\n\\langle p(r) \\rangle \\approx 2\\Phi\\left(-\\sqrt{\\frac{C_V}{k_B}} \\frac{r-1}{\\sqrt{1+r^2}}\\right)\n$$\nThis completes the first part of the problem. Now, we must estimate $\\Delta T$ for the given parameters: $\\langle p \\rangle = 0.3$, $C_V = 1000\\,k_B$ (so $C_V/k_B = 1000$), and $T = 300\\ \\mathrm{K}$.\nThe equation to solve is:\n$$\n0.3 = 2\\Phi\\left(-\\sqrt{1000} \\frac{\\Delta T}{\\sqrt{300^2+(300+\\Delta T)^2}}\\right)\n$$\nFirst, we find the argument of $\\Phi$. Let $z$ be this argument.\n$$\n\\Phi(z) = \\frac{0.3}{2} = 0.15\n$$\nUsing the inverse CDF of the standard normal distribution, we find the value of $z$:\n$$\nz = \\Phi^{-1}(0.15) \\approx -1.036433\n$$\nThus, we have the equation:\n$$\n-1.036433 = -\\sqrt{1000} \\frac{\\Delta T}{\\sqrt{300^2+(300+\\Delta T)^2}}\n$$\nLet $\\zeta = 1.036433$. The equation simplifies to:\n$$\n\\zeta = \\sqrt{1000} \\frac{\\Delta T}{\\sqrt{300^2+(300+\\Delta T)^2}}\n$$\nSquaring both sides gives:\n$$\n\\zeta^2 = 1000 \\frac{(\\Delta T)^2}{300^2+(300+\\Delta T)^2} \\implies \\frac{\\zeta^2}{1000} = \\frac{(\\Delta T)^2}{90000 + 90000 + 600\\Delta T + (\\Delta T)^2}\n$$\nLet $A = \\zeta^2/1000 \\approx (1.036433)^2/1000 \\approx 0.00107419$.\n$$\nA \\left(180000 + 600\\Delta T + (\\Delta T)^2\\right) = (\\Delta T)^2\n$$\nRearranging this into a standard quadratic form $a(\\Delta T)^2 + b(\\Delta T) + c = 0$:\n$$\n(1-A)(\\Delta T)^2 - (600A)\\Delta T - 180000A = 0\n$$\nSubstituting the value of $A$:\n$$\n(1 - 0.00107419)(\\Delta T)^2 - (600 \\times 0.00107419)\\Delta T - (180000 \\times 0.00107419) = 0\n$$\n$$\n0.9989258 (\\Delta T)^2 - 0.644514 \\Delta T - 193.3542 = 0\n$$\nWe solve for $\\Delta T$ using the quadratic formula, taking the positive root as $\\Delta T$ must be positive:\n$$\n\\Delta T = \\frac{-b + \\sqrt{b^2-4ac}}{2a} = \\frac{0.644514 + \\sqrt{(-0.644514)^2 - 4(0.9989258)(-193.3542)}}{2(0.9989258)}\n$$\n$$\n\\Delta T = \\frac{0.644514 + \\sqrt{0.415398 + 772.633}}{1.9978516} = \\frac{0.644514 + \\sqrt{773.0484}}{1.9978516}\n$$\n$$\n\\Delta T = \\frac{0.644514 + 27.803748}{1.9978516} = \\frac{28.448262}{1.9978516} \\approx 14.2394\\ \\mathrm{K}\n$$\nRounding the result to four significant figures gives $\\Delta T = 14.24\\ \\mathrm{K}$.", "answer": "$$\\boxed{14.24}$$", "id": "2666613"}, {"introduction": "The final practice elevates our focus to the fundamental principle of verification in computational science. Before trusting any simulation results, one must have confidence that the implemented algorithm is correct. This exercise [@problem_id:2666530] guides you through the process of verifying an REMD exchange kernel by comparing its numerical output against a rigorous, analytically derived solution for a model system where the potential energy follows a known Gamma distribution.", "problem": "Design a complete, runnable program that verifies a Replica Exchange Molecular Dynamics (REMD) acceptance kernel against an analytic benchmark for a system with a known density of states. Use a mathematically tractable model system: a classical quadratic Hamiltonian with $f$ quadratic configurational degrees of freedom, whose canonical potential-energy distribution at inverse temperature $\\beta$ is gamma with shape $k = f/2$ and scale $1/\\beta$. Specifically, the canonical density for the potential energy $U \\ge 0$ is\n$$\np_{\\beta}(U) = \\frac{\\beta^{k}}{\\Gamma(k)} U^{k - 1} e^{-\\beta U},\n$$\nwhere $k \\in \\{1,2,3,\\dots\\}$ is fixed for the system.\n\nConsider two replicas at inverse temperatures $\\beta_1$ and $\\beta_2$ with $\\beta_1  \\beta_2  0$. The Metropolis exchange acceptance factor for swapping their temperatures (with fixed configurations) is\n$$\nA(U_1, U_2) = \\min\\left(1, \\exp\\left[(\\beta_1 - \\beta_2)(U_1 - U_2)\\right]\\right),\n$$\nwhere $U_1 \\sim p_{\\beta_1}(\\cdot)$ and $U_2 \\sim p_{\\beta_2}(\\cdot)$ are independent random energies drawn from the above canonical densities. The ensemble-average acceptance probability is the double integral\n$$\n\\mathcal{A}(k, \\beta_1, \\beta_2) = \\int_0^\\infty \\int_0^\\infty p_{\\beta_1}(U_1)\\, p_{\\beta_2}(U_2)\\, \\min\\left(1, \\exp\\left[(\\beta_1 - \\beta_2)(U_1 - U_2)\\right]\\right)\\, dU_1\\, dU_2.\n$$\n\nTask: From first principles, derive a closed-form analytic expression for $\\mathcal{A}(k, \\beta_1, \\beta_2)$ as a function of $k$, $\\beta_1$, and $\\beta_2$. Then implement a program that:\n- Computes the analytic value $\\mathcal{A}(k, \\beta_1, \\beta_2)$.\n- Independently estimates the same quantity by Monte Carlo, by sampling $U_1$ and $U_2$ as independent gamma random variables with shape $k$ and scales $1/\\beta_1$ and $1/\\beta_2$ respectively, and averaging $A(U_1, U_2)$ over $n$ independent draws.\n- Uses the sample standard error to decide whether the Monte Carlo estimate agrees with the analytic value within statistical error, reporting a boolean for each test case defined below.\n\nYou must treat energies and temperatures as dimensionless (no physical units). Angles are not involved. Percentages must not be used; all proportions must be expressed as decimals.\n\nTest Suite:\nFor each test case, the parameters are $(k, \\beta_1, \\beta_2, n, \\text{seed})$:\n- Case $1$: $(k=\\;1,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;1.0,\\; n=\\;100000,\\; \\text{seed}=\\;42)$\n- Case $2$: $(k=\\;1,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;0.5,\\; n=\\;100000,\\; \\text{seed}=\\;43)$\n- Case $3$: $(k=\\;3,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;0.8,\\; n=\\;100000,\\; \\text{seed}=\\;44)$\n- Case $4$: $(k=\\;5,\\; \\beta_1=\\;3.0,\\; \\beta_2=\\;2.4,\\; n=\\;100000,\\; \\text{seed}=\\;45)$\n- Case $5$: $(k=\\;10,\\; \\beta_1=\\;2.0,\\; \\beta_2=\\;1.6,\\; n=\\;100000,\\; \\text{seed}=\\;46)$\n- Case $6$: $(k=\\;10,\\; \\beta_1=\\;1.0,\\; \\beta_2=\\;0.7,\\; n=\\;100000,\\; \\text{seed}=\\;47)$\n\nDecision Rule:\nFor each case, compute the Monte Carlo estimate $\\widehat{\\mathcal{A}}$ and its sample standard error $\\mathrm{SE} = s/\\sqrt{n}$, where $s$ is the unbiased sample standard deviation of the $n$ independent values of $A(U_1, U_2)$. Let $\\mathcal{A}_{\\mathrm{an}}$ denote the analytic value. Report $\\text{True}$ if\n$$\n\\left|\\widehat{\\mathcal{A}} - \\mathcal{A}_{\\mathrm{an}}\\right| \\le 4\\, \\mathrm{SE},\n$$\nand $\\text{False}$ otherwise.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (e.g., \"[$\\text{result}_1,\\text{result}_2,\\dots$]\"). Each $\\text{result}_i$ must be a boolean value $\\text{True}$ or $\\text{False}$ corresponding to the $i$-th test case, in the same order as listed above. The program must be fully self-contained and not require any user input or external files. Use the specified runtime environment.", "solution": "The problem is valid. It is scientifically grounded in the principles of statistical mechanics, specifically the canonical ensemble and the Metropolis-Hastings algorithm as applied to Replica Exchange Molecular Dynamics (REMD). The model system, a classical quadratic Hamiltonian, is a standard textbook example whose potential energy follows a Gamma distribution, a well-established result. The problem is well-posed, providing all necessary parameters and a clear objective: to derive a closed-form expression for the average exchange acceptance probability and verify it against a Monte Carlo simulation. The language is precise and objective. All conditions for a valid problem are met.\n\nThe objective is to derive an analytic expression for the ensemble-average acceptance probability $\\mathcal{A}(k, \\beta_1, \\beta_2)$ for an REMD temperature swap and to implement a program that verifies this expression via Monte Carlo simulation.\n\nThe average acceptance probability is given by the integral:\n$$\n\\mathcal{A}(k, \\beta_1, \\beta_2) = \\int_0^\\infty \\int_0^\\infty p_{\\beta_1}(U_1)\\, p_{\\beta_2}(U_2)\\, \\min\\left(1, e^{(\\beta_1 - \\beta_2)(U_1 - U_2)}\\right)\\, dU_1\\, dU_2\n$$\nwhere $U_1$ and $U_2$ are independent random potential energies drawn from the canonical probability densities $p_{\\beta_1}(\\cdot)$ and $p_{\\beta_2}(\\cdot)$ respectively. The density $p_{\\beta}(U)$ is given as a Gamma distribution with shape $k$ and scale $1/\\beta$:\n$$\np_{\\beta}(U) = \\frac{\\beta^{k}}{\\Gamma(k)} U^{k - 1} e^{-\\beta U}\n$$\nGiven the condition $\\beta_1 > \\beta_2$, the exponent's sign is determined by the sign of $U_1 - U_2$. We can split the integral into two domains:\n1.  If $U_1 > U_2$, then $(\\beta_1 - \\beta_2)(U_1 - U_2) > 0$, so $\\min(\\cdot) = 1$.\n2.  If $U_1 \\le U_2$, then $(\\beta_1 - \\beta_2)(U_1 - U_2) \\le 0$, so $\\min(\\cdot) = e^{(\\beta_1 - \\beta_2)(U_1 - U_2)}$.\n\nThis allows us to write $\\mathcal{A}$ as the sum of two integrals, $I_1$ and $I_2$:\n$$\n\\mathcal{A}(k, \\beta_1, \\beta_2) = \\underbrace{\\iint_{U_1 \\le U_2} p_{\\beta_1}(U_1) p_{\\beta_2}(U_2) e^{(\\beta_1 - \\beta_2)(U_1 - U_2)} dU_1 dU_2}_{I_1} + \\underbrace{\\iint_{U_1  U_2} p_{\\beta_1}(U_1) p_{\\beta_2}(U_2) dU_1 dU_2}_{I_2}\n$$\nLet's analyze the integrand of $I_1$. We can substitute the explicit forms of the densities and the exponential term:\n$$\np_{\\beta_1}(U_1) p_{\\beta_2}(U_2) e^{(\\beta_1 - \\beta_2)(U_1 - U_2)} = \\left(\\frac{\\beta_1^k}{\\Gamma(k)} U_1^{k-1} e^{-\\beta_1 U_1}\\right) \\left(\\frac{\\beta_2^k}{\\Gamma(k)} U_2^{k-1} e^{-\\beta_2 U_2}\\right) e^{\\beta_1 U_1 - \\beta_2 U_1 - \\beta_1 U_2 + \\beta_2 U_2}\n$$\nCombining the exponential terms yields:\n$$\ne^{-\\beta_1 U_1 - \\beta_2 U_2 + \\beta_1 U_1 - \\beta_2 U_1 - \\beta_1 U_2 + \\beta_2 U_2} = e^{-\\beta_2 U_1 - \\beta_1 U_2}\n$$\nThe integrand of $I_1$ thus simplifies to:\n$$\n\\frac{\\beta_1^k \\beta_2^k}{\\Gamma(k)^2} U_1^{k-1} U_2^{k-1} e^{-\\beta_2 U_1 - \\beta_1 U_2} = \\left(\\frac{\\beta_2^k}{\\Gamma(k)} U_1^{k-1} e^{-\\beta_2 U_1}\\right) \\left(\\frac{\\beta_1^k}{\\Gamma(k)} U_2^{k-1} e^{-\\beta_1 U_2}\\right) = p_{\\beta_2}(U_1) p_{\\beta_1}(U_2)\n$$\nThis remarkable identity shows a swap in the temperature indices of the probability densities. The integral $I_1$ can now be interpreted probabilistically. Let $X \\sim p_{\\beta_1}(\\cdot)$ and $Y \\sim p_{\\beta_2}(\\cdot)$ be two independent random variables. The expression for $\\mathcal{A}$ can be written in terms of the probability of events involving these variables.\n$$\nI_1 = \\iint_{U_1 \\le U_2} p_{\\beta_2}(U_1) p_{\\beta_1}(U_2) dU_1 dU_2 = \\mathbb{P}(Y \\le X)\n$$\nThe integral $I_2$ is, by definition:\n$$\nI_2 = \\iint_{U_1  U_2} p_{\\beta_1}(U_1) p_{\\beta_2}(U_2) dU_1 dU_2 = \\mathbb{P}(X  Y)\n$$\nTherefore, the average acceptance probability is the sum of these two probabilities:\n$$\n\\mathcal{A} = \\mathbb{P}(Y \\le X) + \\mathbb{P}(X  Y)\n$$\nSince $X$ and $Y$ are continuous random variables, the probability of them being equal is zero, i.e., $\\mathbb{P}(X=Y)=0$. Thus, $\\mathbb{P}(Y \\le X)$ is the same as $\\mathbb{P}(Y  X)$, which is equivalent to $\\mathbb{P}(X > Y)$.\nThis simplifies the expression for $\\mathcal{A}$ to:\n$$\n\\mathcal{A} = \\mathbb{P}(X > Y) + \\mathbb{P}(X > Y) = 2\\, \\mathbb{P}(X > Y)\n$$\nTo obtain a closed-form expression, we must calculate $\\mathbb{P}(X > Y)$. Let $X \\sim \\text{Gamma}(k, \\theta_1)$ and $Y \\sim \\text{Gamma}(k, \\theta_2)$, where the scale parameters are $\\theta_1 = 1/\\beta_1$ and $\\theta_2 = 1/\\beta_2$.\nLet us define two new variables, $X' = X/\\theta_1$ and $Y' = Y/\\theta_2$. Both $X'$ and $Y'$ are independent random variables following the standard Gamma distribution, $\\text{Gamma}(k, 1)$.\nThe condition $X > Y$ is equivalent to $X' \\theta_1 > Y' \\theta_2$, or $\\frac{X'}{Y'} > \\frac{\\theta_2}{\\theta_1}$.\nNow, consider the variable $W = \\frac{X'}{X' + Y'}$. It is a standard result that if $X' \\sim \\text{Gamma}(k, 1)$ and $Y' \\sim \\text{Gamma}(k, 1)$, then $W$ follows a Beta distribution, $W \\sim \\text{Beta}(k, k)$.\nWe can express the ratio $X'/Y'$ in terms of $W$:\n$$\n\\frac{1}{W} = 1 + \\frac{Y'}{X'} \\implies \\frac{Y'}{X'} = \\frac{1-W}{W} \\implies \\frac{X'}{Y'} = \\frac{W}{1-W}\n$$\nThe inequality becomes $\\frac{W}{1-W} > \\frac{\\theta_2}{\\theta_1}$. Since $W \\in (0, 1)$, we can multiply by $1-W > 0$:\n$$\nW\\theta_1 > (1-W)\\theta_2 \\implies W(\\theta_1 + \\theta_2) > \\theta_2 \\implies W > \\frac{\\theta_2}{\\theta_1 + \\theta_2}\n$$\nSo, $\\mathbb{P}(X > Y) = \\mathbb{P}(W > \\frac{\\theta_2}{\\theta_1 + \\theta_2})$. The probability is the integral of the Beta PDF from this value to $1$. This is given by the complement of the cumulative distribution function (CDF), which is related to the regularized incomplete beta function, $I_x(a, b)$.\n$$\n\\mathbb{P}(X > Y) = 1 - I_{\\frac{\\theta_2}{\\theta_1 + \\theta_2}}(k, k)\n$$\nThe Beta distribution $\\text{Beta}(k, k)$ is symmetric around $1/2$. A property of the regularized incomplete beta function for symmetric parameters is $1 - I_x(k,k) = I_{1-x}(k,k)$.\nLet $x = \\frac{\\theta_2}{\\theta_1 + \\theta_2}$. Then $1-x = \\frac{\\theta_1}{\\theta_1 + \\theta_2}$.\nSubstituting the scale parameters $\\theta_1 = 1/\\beta_1$ and $\\theta_2 = 1/\\beta_2$:\n$$\n1-x = \\frac{1/\\beta_1}{1/\\beta_1 + 1/\\beta_2} = \\frac{1/\\beta_1}{(\\beta_1+\\beta_2)/(\\beta_1\\beta_2)} = \\frac{\\beta_2}{\\beta_1+\\beta_2}\n$$\nSo, $\\mathbb{P}(X>Y) = I_{\\frac{\\beta_2}{\\beta_1+\\beta_2}}(k, k)$.\nFinally, the analytic expression for the average acceptance probability is:\n$$\n\\mathcal{A}_{\\mathrm{an}}(k, \\beta_1, \\beta_2) = 2 \\cdot I_{\\frac{\\beta_2}{\\beta_1+\\beta_2}}(k, k)\n$$\nThis expression will be computed using `scipy.special.betainc`.\n\nThe numerical verification will be performed via a Monte Carlo simulation. For each test case $(k, \\beta_1, \\beta_2, n, \\text{seed})$:\n1.  Generate $n$ independent samples for $U_1$ from $\\text{Gamma}(k, 1/\\beta_1)$.\n2.  Generate $n$ independent samples for $U_2$ from $\\text{Gamma}(k, 1/\\beta_2)$.\n3.  For each pair $(U_{1,i}, U_{2,i})$, calculate the Metropolis acceptance factor $A_i = \\min(1, \\exp[(\\beta_1-\\beta_2)(U_{1,i}-U_{2,i})])$.\n4.  Compute the Monte Carlo estimate $\\widehat{\\mathcal{A}} = \\frac{1}{n} \\sum_{i=1}^n A_i$.\n5.  Compute the unbiased sample standard deviation $s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (A_i - \\widehat{\\mathcal{A}})^2}$.\n6.  Compute the standard error of the mean $\\mathrm{SE} = s/\\sqrt{n}$.\n7.  The verification succeeds if the absolute difference between the analytic value and the Monte Carlo estimate is within $4$ standard errors: $|\\widehat{\\mathcal{A}} - \\mathcal{A}_{\\mathrm{an}}| \\le 4\\cdot\\mathrm{SE}$.\nA sanity check for $\\beta_1=\\beta_2$ gives $(\\beta_1-\\beta_2)=0$, so $A(U_1, U_2)=1$ and $\\mathcal{A}_{\\mathrm{an}}=1$. The analytic formula yields $x = \\beta_1/(2\\beta_1) = 1/2$. For a symmetric Beta distribution, $I_{1/2}(k,k)=1/2$, so $\\mathcal{A}_{\\mathrm{an}} = 2 \\cdot (1/2) = 1$, which is consistent.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import betainc\n\ndef solve():\n    \"\"\"\n    Solves the REMD acceptance kernel verification problem.\n    Derives the analytic solution for the average acceptance probability and\n    verifies it against a Monte Carlo simulation for several test cases.\n    \"\"\"\n    \n    # Test Suite: (k, beta1, beta2, n_samples, seed)\n    test_cases = [\n        (1, 1.0, 1.0, 100000, 42),\n        (1, 1.0, 0.5, 100000, 43),\n        (3, 1.0, 0.8, 100000, 44),\n        (5, 3.0, 2.4, 100000, 45),\n        (10, 2.0, 1.6, 100000, 46),\n        (10, 1.0, 0.7, 100000, 47),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        k, beta1, beta2, n, seed = case\n        \n        # --- Analytic Calculation ---\n        # The average acceptance probability is 2 * I_x(k, k), where\n        # I_x is the regularized incomplete beta function and x = beta2 / (beta1 + beta2).\n        if beta1 + beta2 == 0:\n            # Avoid division by zero, although problem constraints ensure beta > 0.\n            # If beta1=beta2=0, x is undefined. If they are non-zero, this won't happen.\n            x_beta = 0.5 \n        else:\n            x_beta = beta2 / (beta1 + beta2)\n\n        # For the case beta1 == beta2, x_beta is 0.5 and I_0.5(k,k) is 0.5, so A_an = 1.\n        analytic_A = 2.0 * betainc(k, k, x_beta)\n        \n        # --- Monte Carlo Estimation ---\n        rng = np.random.default_rng(seed)\n        \n        # The potential energy U for a canonical ensemble at inverse temperature beta\n        # follows a Gamma distribution with shape k and scale 1/beta.\n        scale1 = 1.0 / beta1\n        scale2 = 1.0 / beta2\n        \n        # Generate n independent energy samples for each replica.\n        U1_samples = rng.gamma(shape=k, scale=scale1, size=n)\n        U2_samples = rng.gamma(shape=k, scale=scale2, size=n)\n        \n        # Calculate the Metropolis acceptance factor for each pair of samples.\n        delta_beta = beta1 - beta2\n        delta_U = U1_samples - U2_samples\n        \n        # A = min(1, exp(delta_beta * delta_U))\n        acceptance_factors = np.minimum(1.0, np.exp(delta_beta * delta_U))\n        \n        # Compute the MC estimate (sample mean) and standard error.\n        mc_A = np.mean(acceptance_factors)\n        # Use ddof=1 for the unbiased sample standard deviation.\n        s = np.std(acceptance_factors, ddof=1)\n        se = s / np.sqrt(n)\n\n        # --- Decision Rule ---\n        # The Monte Carlo estimate agrees if it is within 4 standard errors\n        # of the analytic value.\n        is_consistent = np.abs(mc_A - analytic_A) = 4.0 * se\n        results.append(is_consistent)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2666530"}]}