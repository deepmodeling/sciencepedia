{"hands_on_practices": [{"introduction": "To truly grasp the molecular origin of entropy, we begin with the simplest possible quantum system: a two-level model. This exercise serves as a foundational practice in statistical mechanics, demonstrating how to derive thermodynamic properties directly from the energy spectrum of a single molecule using the Gibbs formulation of entropy, $S = -k_{\\mathrm{B}} \\sum_i p_i \\ln p_i$. By calculating the entropy as a function of temperature and examining its behavior at extreme limits, you will see precisely how the Third Law of Thermodynamics emerges from the quantization of energy levels [@problem_id:2960081].", "problem": "A single, nondegenerate, two-level molecular system has energy eigenvalues $0$ and $\\epsilon0$. The system is in canonical equilibrium with a heat bath at absolute temperature $T$. Use only the following foundational definitions to derive the temperature dependence of the Gibbs entropy and its limiting behavior:\n- Canonical (Boltzmann) probabilities: for a microstate $i$ with energy $E_i$, the equilibrium probability is $p_i \\propto \\exp(-E_i/(k_{\\mathrm{B}} T))$, with normalization $\\sum_i p_i=1$.\n- Gibbs entropy: $S=-k_{\\mathrm{B}} \\sum_i p_i \\ln p_i$.\n\nStarting from these definitions, compute the exact analytic expression for the entropy $S(T)$ of this two-level system as a function of $T$. Then determine its limiting forms as $T \\to 0^+$ and as $T \\to \\infty$, and interpret these limits in light of the third law of thermodynamics. Express your final analytic expression for $S(T)$ in terms of $k_{\\mathrm{B}}$, $\\epsilon$, and $T$. You do not need to substitute numerical values. Do not include any units in your final boxed answer; it should be a single closed-form expression. No rounding is required.", "solution": "The problem statement is first subjected to a critical validation.\n\nStep 1: Extract Givens\n- System: A single, nondegenerate, two-level molecular system.\n- Energy eigenvalues: $E_1 = 0$ and $E_2 = \\epsilon$, where $\\epsilon  0$.\n- Equilibrium condition: The system is in canonical equilibrium with a heat bath at absolute temperature $T$.\n- Foundational definitions:\n    1. Canonical probability of a microstate $i$ with energy $E_i$: $p_i \\propto \\exp(-E_i/(k_{\\mathrm{B}} T))$, with the normalization condition $\\sum_i p_i=1$.\n    2. Gibbs entropy: $S = -k_{\\mathrm{B}} \\sum_i p_i \\ln p_i$.\n- Task: Derive the analytical expression for the entropy $S(T)$, determine its limits as $T \\to 0^+$ and $T \\to \\infty$, and interpret these limits in the context of the third law of thermodynamics.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it is a standard and fundamental problem in statistical mechanics used to illustrate the concepts of partition function, probability distribution, and entropy. The system described is a valid physical model (e.g., a spin-1/2 particle in a magnetic field). The definitions provided for canonical probability and Gibbs entropy are correct and foundational. The problem is well-posed, self-contained, and free from contradictions or ambiguities. It provides all necessary information to derive a unique and meaningful solution for the entropy $S(T)$. The language is objective and precise. Therefore, the problem is deemed valid.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe solution proceeds by first calculating the canonical partition function, then the probabilities of the two energy states, and finally substituting these into the Gibbs entropy formula.\n\nThe canonical partition function, $Z$, is the normalization constant for the probabilities and is defined as the sum over all states of the Boltzmann factor:\n$$Z = \\sum_{i=1}^{2} \\exp\\left(-\\frac{E_i}{k_{\\mathrm{B}} T}\\right)$$\nFor the given two-level system with energy eigenvalues $E_1=0$ and $E_2=\\epsilon$, the partition function is:\n$$Z = \\exp\\left(-\\frac{0}{k_{\\mathrm{B}} T}\\right) + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right) = 1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)$$\nThe probability $p_i$ of the system being in a microstate $i$ is given by:\n$$p_i = \\frac{\\exp\\left(-\\frac{E_i}{k_{\\mathrm{B}} T}\\right)}{Z}$$\nThus, the probabilities for the ground state ($p_1$) and the excited state ($p_2$) are:\n$$p_1 = \\frac{\\exp\\left(-\\frac{0}{k_{\\mathrm{B}} T}\\right)}{Z} = \\frac{1}{1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}$$\n$$p_2 = \\frac{\\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}{Z} = \\frac{\\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}{1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}$$\nNote that $p_1 + p_2 = 1$, satisfying the normalization condition.\n\nNow, we use the Gibbs formula for entropy, $S = -k_{\\mathrm{B}} \\sum_i p_i \\ln p_i$:\n$$S = -k_{\\mathrm{B}} \\left( p_1 \\ln p_1 + p_2 \\ln p_2 \\right)$$\nSubstituting the expressions for $p_1$ and $p_2$:\n$$\\ln p_1 = \\ln\\left(\\frac{1}{Z}\\right) = -\\ln Z$$\n$$\\ln p_2 = \\ln\\left(\\frac{\\exp(-\\epsilon/(k_{\\mathrm{B}} T))}{Z}\\right) = \\ln\\left(\\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)\\right) - \\ln Z = -\\frac{\\epsilon}{k_{\\mathrm{B}} T} - \\ln Z$$\nSubstituting these into the entropy formula:\n$$S = -k_{\\mathrm{B}} \\left[ p_1(-\\ln Z) + p_2\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T} - \\ln Z\\right) \\right]$$\n$$S = -k_{\\mathrm{B}} \\left[ -(p_1 + p_2)\\ln Z - p_2 \\frac{\\epsilon}{k_{\\mathrm{B}} T} \\right]$$\nSince $p_1 + p_2 = 1$:\n$$S = -k_{\\mathrm{B}} \\left[ -\\ln Z - p_2 \\frac{\\epsilon}{k_{\\mathrm{B}} T} \\right] = k_{\\mathrm{B}} \\ln Z + k_{\\mathrm{B}} p_2 \\frac{\\epsilon}{k_{\\mathrm{B}} T} = k_{\\mathrm{B}} \\ln Z + \\frac{\\epsilon}{T} p_2$$\nSubstituting the expressions for $Z$ and $p_2$ yields the final analytical expression for $S(T)$:\n$$S(T) = k_{\\mathrm{B}} \\ln\\left(1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)\\right) + \\frac{\\epsilon}{T} \\frac{\\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}{1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}$$\nThis expression can be written in an equivalent form by manipulating the second term:\n$$\\frac{\\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}{1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)} = \\frac{1}{\\exp\\left(\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right) + 1}$$\nSo, an alternative form is:\n$$S(T) = k_{\\mathrm{B}} \\ln\\left(1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)\\right) + \\frac{\\epsilon}{T} \\frac{1}{1 + \\exp\\left(\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}$$\nThis is the required analytical expression for the entropy as a function of temperature.\n\nNext, we analyze the limiting behavior.\n\nLimit as $T \\to 0^+$:\nIn this limit, the argument of the exponential, $\\frac{\\epsilon}{k_{\\mathrm{B}} T}$, approaches $+\\infty$.\nThe probabilities become:\n$$p_1 = \\frac{1}{1 + \\exp(-\\infty)} \\to \\frac{1}{1+0} = 1$$\n$$p_2 = \\frac{\\exp(-\\infty)}{1 + \\exp(-\\infty)} \\to \\frac{0}{1+0} = 0$$\nThe system occupies the ground state with certainty. The entropy is $S = -k_{\\mathrm{B}}(p_1 \\ln p_1 + p_2 \\ln p_2)$.\nThe first term $p_1 \\ln p_1 \\to 1 \\ln 1 = 0$.\nThe second term is of the form $0 \\cdot \\ln(0)$, which is an indeterminate form. We evaluate its limit:\n$$\\lim_{p_2 \\to 0^+} p_2 \\ln p_2 = \\lim_{p_2 \\to 0^+} \\frac{\\ln p_2}{1/p_2}$$\nUsing L'HÃ´pital's rule:\n$$\\lim_{p_2 \\to 0^+} \\frac{1/p_2}{-1/p_2^2} = \\lim_{p_2 \\to 0^+} (-p_2) = 0$$\nTherefore, in the limit $T \\to 0^+$, the entropy is:\n$$S(T \\to 0^+) = -k_{\\mathrm{B}}(0 + 0) = 0$$\nInterpretation: As the temperature approaches absolute zero, the system settles into its unique, nondegenerate ground state. There is only one accessible microstate ($\\Omega=1$), corresponding to a state of perfect order. According to the Boltzmann entropy definition $S=k_{\\mathrm{B}} \\ln \\Omega$, the entropy is $k_{\\mathrm{B}} \\ln(1)=0$. This result is a direct manifestation of the third law of thermodynamics, which states that the entropy of a perfect crystalline substance is zero at absolute zero temperature.\n\nLimit as $T \\to \\infty$:\nIn this limit, the argument $\\frac{\\epsilon}{k_{\\mathrm{B}} T}$ approaches $0$.\nThe probabilities become:\n$$p_1 = \\frac{1}{1 + \\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}$$\n$$p_2 = \\frac{\\exp(0)}{1 + \\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}$$\nAt infinite temperature, the thermal energy $k_{\\mathrm{B}} T$ is infinitely larger than the energy gap $\\epsilon$, making both states equally accessible.\nThe entropy is:\n$$S(T \\to \\infty) = -k_{\\mathrm{B}} \\left( \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) \\right) = -k_{\\mathrm{B}} \\ln\\left(\\frac{1}{2}\\right) = -k_{\\mathrm{B}}(-\\ln 2)$$\n$$S(T \\to \\infty) = k_{\\mathrm{B}} \\ln 2$$\nInterpretation: At infinite temperature, the system is maximally disordered, with both microstates ($\\Omega=2$) being equally probable. The entropy reaches its maximum possible value, which for a two-state system is $k_{\\mathrm{B}} \\ln 2$. This reflects the principle that high temperatures promote statistical disorder.", "answer": "$$\n\\boxed{k_{\\mathrm{B}} \\ln\\left(1 + \\exp\\left(-\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)\\right) + \\frac{\\epsilon}{T}\\frac{1}{1 + \\exp\\left(\\frac{\\epsilon}{k_{\\mathrm{B}} T}\\right)}}\n$$", "id": "2960081"}, {"introduction": "Entropy is not only about the distribution of energy among quantum states but also about the arrangement of particles in space. This practice explores this concept of configurational entropy by analyzing a crystalline solid with point defects, or vacancies. By applying the Boltzmann definition of entropy, $S = k_{\\mathrm{B}} \\ln W$, you will first quantify the entropy associated with this structural disorder. Subsequently, you will use thermodynamic principles to determine the equilibrium number of defects as a function of temperature, providing a concrete example of how nature balances energy costs against entropic gains [@problem_id:2960101].", "problem": "A crystalline solid contains $N$ equivalent lattice sites and $n$ identical, noninteracting vacancies with $0 \\le n \\le N$. Assume that all distinct arrangements of $n$ vacancies among the $N$ sites have the same energy when $n$ is fixed, and neglect vibrational, electronic, and any other nonconfigurational contributions. Use only the Boltzmann definition of entropy, $S = k_{\\mathrm{B}} \\ln W$, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $W$ is the number of microstates, together with standard combinatorial counting, to obtain an exact closed-form expression for the configurational entropy per Boltzmann constant, $S_{\\mathrm{conf}}/k_{\\mathrm{B}}$, as a function of $N$ and $n$.\n\nNext, let the internal energy cost for creating a vacancy be a constant $\\varepsilon_{\\mathrm{v}}$ per vacancy, independent of $n$ and $T$, and consider the Helmholtz free energy $F = U - T S$ at temperature $T$. In the dilute limit $n \\ll N$, determine the leading-order low-temperature equilibrium scaling of $n(T)$ by minimizing $F$ with respect to $n$ using a controlled large-$N$ asymptotic analysis valid for $n/N \\ll 1$. State the implication of this scaling for the configurational entropy as $T \\to 0$ for an equilibrium, perfect crystal with a unique ground state. You do not need to provide a numerical value.\n\nExpress your final answer as the exact closed-form expression for $S_{\\mathrm{conf}}/k_{\\mathrm{B}}$ in terms of $N$ and $n$. No numerical evaluation is required, and no units should be reported since you will present $S_{\\mathrm{conf}}/k_{\\mathrm{B}}$.", "solution": "The problem presented is a standard, well-posed problem in statistical mechanics and is therefore valid. We shall proceed with its solution in two parts as prescribed.\n\nFirst, we determine the configurational entropy, $S_{\\mathrm{conf}}$. The system consists of $N$ distinguishable lattice sites, among which $n$ identical, indistinguishable vacancies are distributed. The number of distinct arrangements, or microstates ($W$), is given by the number of ways to choose $n$ sites out of $N$. This is a standard combinatorial problem, and the solution is the binomial coefficient:\n$$\nW = \\binom{N}{n} = \\frac{N!}{n!(N-n)!}\n$$\nThe problem states that we must use the Boltzmann definition of entropy, $S = k_{\\mathrm{B}} \\ln W$, where $k_{\\mathrm{B}}$ is the Boltzmann constant.Substituting the expression for $W$, we obtain the configurational entropy:\n$$\nS_{\\mathrm{conf}} = k_{\\mathrm{B}} \\ln\\left(\\frac{N!}{n!(N-n)!}\\right)\n$$\nThe problem specifically asks for the configurational entropy per Boltzmann constant, $S_{\\mathrm{conf}}/k_{\\mathrm{B}}$. This is simply:\n$$\n\\frac{S_{\\mathrm{conf}}}{k_{\\mathrm{B}}} = \\ln\\left(\\frac{N!}{n!(N-n)!}\\right)\n$$\nThis is the exact, closed-form expression for the configurational entropy.\n\nSecond, we analyze the system at thermal equilibrium to find the temperature-dependent vacancy concentration, $n(T)$, and its implications for the third law of thermodynamics. Equilibrium is found by minimizing the Helmholtz free energy, $F = U - TS$, with respect to the number of vacancies, $n$.\n\nThe internal energy, $U$, is the total energy cost for creating $n$ vacancies. With a constant energy cost of $\\varepsilon_{\\mathrm{v}}$ per vacancy, the internal energy relative to the perfect crystal ($n=0$) is:\n$$\nU(n) = n\\varepsilon_{\\mathrm{v}}\n$$\nThe entropy term is $TS = TS_{\\mathrm{conf}}$. The full expression for the Helmholtz free energy is:\n$$\nF(n, T) = n\\varepsilon_{\\mathrm{v}} - T k_{\\mathrm{B}} \\ln\\left(\\frac{N!}{n!(N-n)!}\\right)\n$$\nTo minimize $F$ with respect to $n$, we must treat $n$ as a continuous variable, which is a valid approximation for a macroscopic system where $N$ and $n$ are large. This requires an analytical form for the logarithm of the factorials. We employ Stirling's approximation, $\\ln(x!) \\approx x\\ln x - x$ for large $x$.\n$$\n\\ln(W) = \\ln(N!) - \\ln(n!) - \\ln((N-n)!) \\approx (N\\ln N - N) - (n\\ln n - n) - ((N-n)\\ln(N-n) - (N-n))\n$$\nSimplifying this expression yields:\n$$\n\\ln(W) \\approx N\\ln N - n\\ln n - (N-n)\\ln(N-n)\n$$\nThus, the approximate Helmholtz free energy is:\n$$\nF(n, T) \\approx n\\varepsilon_{\\mathrm{v}} - k_{\\mathrm{B}}T [N\\ln N - n\\ln n - (N-n)\\ln(N-n)]\n$$\nWe find the equilibrium value of $n$ by setting the partial derivative of $F$ with respect to $n$ to zero, $\\frac{\\partial F}{\\partial n} = 0$.\n$$\n\\frac{\\partial F}{\\partial n} = \\varepsilon_{\\mathrm{v}} - k_{\\mathrm{B}}T \\frac{\\partial}{\\partial n}[N\\ln N - n\\ln n - (N-n)\\ln(N-n)] = 0\n$$\nThe derivative of the entropy term is:\n$$\n\\frac{\\partial}{\\partial n}[- n\\ln n - (N-n)\\ln(N-n)] = -(\\ln n + 1) - (-\\ln(N-n) -1)(-1) = -\\ln n + \\ln(N-n) = \\ln\\left(\\frac{N-n}{n}\\right)\n$$\nSubstituting this back into the minimization condition gives:\n$$\n\\varepsilon_{\\mathrm{v}} - k_{\\mathrm{B}}T \\ln\\left(\\frac{N-n}{n}\\right) = 0\n$$\nSolving for the fractional vacancy concentration $n/N$:\n$$\n\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T} = \\ln\\left(\\frac{N-n}{n}\\right) \\implies \\exp\\left(\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right) = \\frac{N-n}{n} = \\frac{N}{n} - 1\n$$\n$$\n\\frac{n}{N} = \\frac{1}{1 + \\exp\\left(\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right)}\n$$\nThe problem requires the leading-order low-temperature scaling of $n(T)$ in the dilute limit ($n \\ll N$). As temperature $T \\to 0$, the term $\\varepsilon_{\\mathrm{v}}/(k_{\\mathrm{B}}T) \\to \\infty$, since $\\varepsilon_{\\mathrm{v}}  0$. Consequently, $\\exp(\\varepsilon_{\\mathrm{v}}/(k_{\\mathrm{B}}T)) \\gg 1$. In this limit, the expression for the vacancy fraction simplifies:\n$$\n\\frac{n(T)}{N} \\approx \\frac{1}{\\exp\\left(\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right)} = \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right)\n$$\nSo, the leading-order scaling is $n(T) \\approx N \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right)$. This shows that the number of equilibrium vacancies decreases exponentially as temperature approaches absolute zero.\n\nFinally, we examine the implication for the configurational entropy as $T \\to 0$. We must evaluate $S_{\\mathrm{conf}}$ using the equilibrium value $n(T)$ in the low-temperature limit. For small $n$ ($n \\ll N$), the entropy expression $\\ln(W) = \\ln\\binom{N}{n}$ can be approximated.\nFor $n \\ll N$, $\\ln(W) \\approx n\\ln N - (n\\ln n - n) = n\\ln(N/n)+n$.\n$$\nS_{\\mathrm{conf}}(n) \\approx k_{\\mathrm{B}} [n\\ln(N/n) + n]\n$$\nSubstituting the low-temperature scaling of $n(T)$:\n$$\nS_{\\mathrm{conf}}(T) \\approx k_{\\mathrm{B}} \\left[ N \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right) \\ln\\left(\\frac{N}{N \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right)}\\right) + N \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right) \\right]\n$$\n$$\nS_{\\mathrm{conf}}(T) \\approx k_{\\mathrm{B}} N \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right) \\left[ \\ln\\left(\\exp\\left(\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right)\\right) + 1 \\right] = k_{\\mathrm{B}} N \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T}\\right) \\left[ \\frac{\\varepsilon_{\\mathrm{v}}}{k_{\\mathrm{B}}T} + 1 \\right]\n$$\nTo find the limit as $T \\to 0$, let $x = 1/T$. As $T \\to 0^{+}$, $x \\to \\infty$.\n$$\n\\lim_{T\\to 0} S_{\\mathrm{conf}}(T) = \\lim_{x\\to\\infty} k_{\\mathrm{B}} N \\exp\\left(-\\frac{\\varepsilon_{\\mathrm{v}}x}{k_{\\mathrm{B}}}\\right) \\left[ \\frac{\\varepsilon_{\\mathrm{v}}x}{k_{\\mathrm{B}}} + 1 \\right]\n$$\nThe exponential term $\\exp(-ax)$ goes to zero much faster than the linear term $x$ goes to infinity. Therefore, the limit is zero.\n$$\n\\lim_{T\\to 0} S_{\\mathrm{conf}}(T) = 0\n$$\nThis result demonstrates that as the system in equilibrium is cooled to absolute zero, the number of thermally generated vacancies vanishes, and the system approaches the unique, perfectly ordered ground state ($n=0$) which has zero configurational entropy ($S_{\\mathrm{conf}}(n=0)=k_{\\mathrm{B}}\\ln(1)=0$). This is in complete agreement with the third law of thermodynamics, which states that the entropy of a perfect crystal with a unique ground state approaches zero as the temperature approaches absolute zero.", "answer": "$$\n\\boxed{\\ln\\left(\\frac{N!}{n!(N-n)!}\\right)}\n$$", "id": "2960101"}, {"introduction": "The theoretical framework of statistical mechanics provides a microscopic definition of entropy, but how is this quantity measured in the laboratory? This final practice bridges the gap between theory and experiment by demonstrating the determination of absolute entropy from calorimetric data. Starting from experimental heat capacity measurements and applying the thermodynamic relation $S(T) = \\int_{0}^{T} \\frac{C_p(T')}{T'} dT'$, you will calculate the absolute molar entropy of a substance at standard temperature. This exercise underscores the profound practical importance of the Third Law of Thermodynamics, which provides the crucial zero-point reference for such calculations [@problem_id:2960056].", "problem": "A crystalline solid is measured to be a perfect, defect-free crystal with a unique ground state at $0\\,\\mathrm{K}$, and it undergoes no phase transitions between $0\\,\\mathrm{K}$ and $298\\,\\mathrm{K}$. Its molar constant-pressure heat capacity ($C_p$) has been measured from $T = 1\\,\\mathrm{K}$ to $T = 298\\,\\mathrm{K}$ and is well represented by the following empirical fits that are consistent with the measured data:\n- For $1\\,\\mathrm{K} \\leq T \\leq 40\\,\\mathrm{K}$: $C_{p}(T) = \\beta\\,T^{3}$, with $\\beta = \\left(1.200 \\pm 0.006\\right)\\times 10^{-4}\\ \\mathrm{J\\,mol^{-1}\\,K^{-4}}$.\n- For $40\\,\\mathrm{K}  T \\leq 298\\,\\mathrm{K}$: $C_{p}(T) = 3R\\left[1 - \\alpha \\left(\\dfrac{40\\,\\mathrm{K}}{T}\\right)\\right]$, where $R$ is the molar gas constant and $\\alpha$ is chosen so that the two expressions are continuous at $T = 40\\,\\mathrm{K}$, that is, $C_{p}(40\\,\\mathrm{K})$ matches from both sides.\n\nBelow the lowest measured temperature $T = 1\\,\\mathrm{K}$, extrapolate using the same low-temperature Debye $T^{3}$ form, $C_{p}(T) = \\beta\\,T^{3}$, and take the stated uncertainty in $\\beta$ to represent the uncertainty of this extrapolation. Use $R = 8.314\\,462\\,618\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$.\n\nUsing only fundamental thermodynamic relations and the Third Law of Thermodynamics, compute the absolute molar entropy at $T = 298\\,\\mathrm{K}$. Treat the measured region ($T \\geq 1\\,\\mathrm{K}$) as exact and assess the uncertainty due solely to the extrapolation below $1\\,\\mathrm{K}$. Express the final entropy in $\\mathrm{J\\,mol^{-1}\\,K^{-1}}$ and round your answer to four significant figures.", "solution": "The calculation of absolute molar entropy is based on the Third Law of Thermodynamics, which posits that the entropy of a perfect crystal at absolute zero ($T=0\\,\\mathrm{K}$) is zero. The problem statement provides the necessary condition: a perfect, defect-free crystal with a unique ground state. The absolute molar entropy at a temperature $T$, denoted $S_m(T)$, is determined by integrating the molar heat capacity at constant pressure, $C_{p,m}(T)$, divided by temperature, from $0\\,\\mathrm{K}$ to $T$.\n$$S_m(T) = S_m(0\\,\\mathrm{K}) + \\int_{0}^{T} \\frac{C_{p,m}(T')}{T'} dT' = \\int_{0}^{T} \\frac{C_{p,m}(T')}{T'} dT'$$\nTo compute the entropy at $T = 298\\,\\mathrm{K}$, the integral is partitioned according to the given empirical forms for the heat capacity, which we denote as $C_p$ for simplicity.\n$$S_m(298\\,\\mathrm{K}) = \\int_{0\\,\\mathrm{K}}^{1\\,\\mathrm{K}} \\frac{C_p(T)}{T} dT + \\int_{1\\,\\mathrm{K}}^{40\\,\\mathrm{K}} \\frac{C_p(T)}{T} dT + \\int_{40\\,\\mathrm{K}}^{298\\,\\mathrm{K}} \\frac{C_p(T)}{T} dT$$\nAs the form for $C_p(T)$ is the same for the first two intervals ($0\\,\\mathrm{K}$ to $1\\,\\mathrm{K}$ and $1\\,\\mathrm{K}$ to $40\\,\\mathrm{K}$), we can combine these integrals:\n$$S_m(298\\,\\mathrm{K}) = \\int_{0\\,\\mathrm{K}}^{40\\,\\mathrm{K}} \\frac{C_p(T)}{T} dT + \\int_{40\\,\\mathrm{K}}^{298\\,\\mathrm{K}} \\frac{C_p(T)}{T} dT$$\nFirst, the constant $\\alpha$ must be determined from the condition that $C_p(T)$ is continuous at $T = 40\\,\\mathrm{K}$.\nFor $T \\leq 40\\,\\mathrm{K}$, $C_p(T) = \\beta T^3$. At $T = 40\\,\\mathrm{K}$:\n$$C_p(40\\,\\mathrm{K}) = \\beta (40\\,\\mathrm{K})^3 = (1.200 \\times 10^{-4}\\ \\mathrm{J\\,mol^{-1}\\,K^{-4}})(40^3\\ \\mathrm{K}^3) = 7.68\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$$\nFor $T  40\\,\\mathrm{K}$, $C_p(T) = 3R\\left[1 - \\alpha \\left(\\frac{40\\,\\mathrm{K}}{T}\\right)\\right]$. At $T = 40\\,\\mathrm{K}$:\n$$C_p(40\\,\\mathrm{K}) = 3R\\left[1 - \\alpha \\left(\\frac{40\\,\\mathrm{K}}{40\\,\\mathrm{K}}\\right)\\right] = 3R(1 - \\alpha)$$\nEquating these gives:\n$$7.68\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}} = 3R(1 - \\alpha)$$\nUsing the value $R = 8.314\\,462\\,618\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$, we find $3R \\approx 24.943388\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$. Solving for $\\alpha$:\n$$1 - \\alpha = \\frac{7.68}{3 \\times 8.314\\,462\\,618} \\implies \\alpha \\approx 1 - 0.307900 = 0.692100$$\nWe proceed with the integration. The entropy contribution from $0\\,\\mathrm{K}$ to $40\\,\\mathrm{K}$ is:\n$$\\Delta S_{0 \\to 40} = \\int_{0}^{40} \\frac{\\beta T^3}{T} dT = \\int_{0}^{40} \\beta T^2 dT = \\left[ \\frac{\\beta T^3}{3} \\right]_{0}^{40} = \\frac{\\beta (40)^3}{3}$$\nThis evaluates to:\n$$\\Delta S_{0 \\to 40} = \\frac{7.68\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}}{3} = 2.56\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$$\nThe entropy contribution from $40\\,\\mathrm{K}$ to $298\\,\\mathrm{K}$ is:\n$$\\Delta S_{40 \\to 298} = \\int_{40}^{298} \\frac{3R}{T}\\left[1 - \\alpha \\left(\\frac{40}{T}\\right)\\right] dT = \\int_{40}^{298} \\left( \\frac{3R}{T} - \\frac{120 R \\alpha}{T^2} \\right) dT$$\n$$\\Delta S_{40 \\to 298} = \\left[ 3R \\ln(T) + \\frac{120 R \\alpha}{T} \\right]_{40}^{298}$$\n$$= \\left( 3R \\ln(298) + \\frac{120 R \\alpha}{298} \\right) - \\left( 3R \\ln(40) + \\frac{120 R \\alpha}{40} \\right)$$\n$$= 3R \\ln\\left(\\frac{298}{40}\\right) + 120 R \\alpha \\left(\\frac{1}{298} - \\frac{1}{40}\\right)$$\nSubstituting numerical values:\n$$3R \\ln\\left(\\frac{298}{40}\\right) = (24.943388...) \\times \\ln(7.45) \\approx 50.093339\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$$\nThe term $120 R \\alpha$ can be expressed as $40(3R\\alpha) = 40(3R - C_p(40\\,\\mathrm{K})) = 40(24.943388... - 7.68) \\approx 690.535514$.\nThe second part of the expression is then:\n$$(690.535514...) \\times \\left(\\frac{1}{298} - \\frac{1}{40}\\right) \\approx -14.946924\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$$\nSo, the entropy change is:\n$$\\Delta S_{40 \\to 298} \\approx 50.093339 - 14.946924 = 35.146415\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$$\nThe total molar entropy at $298\\,\\mathrm{K}$ is the sum of the contributions:\n$$S_m(298\\,\\mathrm{K}) = \\Delta S_{0 \\to 40} + \\Delta S_{40 \\to 298} \\approx 2.56 + 35.146415 = 37.706415\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$$\nThe problem requires an assessment of the uncertainty arising from the extrapolation of $C_p(T)$ below $T=1\\,\\mathrm{K}$. This uncertainty propagates from the value of $\\beta = (1.200 \\pm 0.006) \\times 10^{-4}\\ \\mathrm{J\\,mol^{-1}\\,K^{-4}}$. The entropy contribution from this extrapolation region ($0\\,\\mathrm{K}$ to $1\\,\\mathrm{K}$) is $\\Delta S_{0 \\to 1} = \\int_{0}^{1} (\\beta T^3/T) dT = \\beta/3$. The uncertainty in the total entropy, $\\delta S_m$, is therefore:\n$$\\delta S_m = \\delta \\left(\\frac{\\beta}{3}\\right) = \\frac{\\delta\\beta}{3} = \\frac{0.006 \\times 10^{-4}}{3} = 2 \\times 10^{-6}\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$$\nThis uncertainty affects the sixth decimal place of the final calculated entropy. Since the result must be rounded to four significant figures, this uncertainty is negligible and does not influence the final reported value.\nRounding the result $S_m(298\\,\\mathrm{K}) = 37.706415\\ \\mathrm{J\\,mol^{-1}\\,K^{-1}}$ to four significant figures gives $37.71$.", "answer": "$$\\boxed{37.71}$$", "id": "2960056"}]}