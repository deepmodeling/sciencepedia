{"hands_on_practices": [{"introduction": "理解一种新的统计方法，最好的起点往往是最简单的特例。这个练习将岭回归分解到其核心，让你专注于一个仅含单个系数的无截距模型。通过这个练习，你将亲手推导岭回归估计量，直观地理解惩罚项 $\\lambda \\beta^2$ 如何改变标准的最小二乘目标函数，并最终得到一个“收缩”后的解。", "problem": "在机器学习的背景下，我们的任务是拟合一个不含截距项的简单线性模型 $y = \\beta x$ 到一个包含 $n$ 个数据点 $(x_i, y_i)$ 的数据集上。为防止在小数据集上发生过拟合，我们采用岭回归。系数 $\\beta$ 的岭估计值是通过最小化惩罚平方误差和（也称为目标函数 $L(\\beta)$）得到的值：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n其中 $\\lambda > 0$ 是控制收缩量的正则化参数。\n\n您的任务分为两部分。首先，通过最小化目标函数 $L(\\beta)$，推导岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 关于数据点 $(x_i, y_i)$ 和参数 $\\lambda$ 的通用闭式表达式。\n\n其次，将此推导出的表达式应用于一个包含两个点的特定数据集：$(x_1, y_1) = (1, 3)$ 和 $(x_2, y_2) = (2, 5)$。使用正则化参数 $\\lambda = 1$ 计算岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 的数值。\n\n以精确分数形式提供最终的数值。", "solution": "我们最小化无截距项线性模型 $y=\\beta x$ 的惩罚平方误差和，其目标函数为\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\n展开平方项并合并 $\\beta$ 的同类项：\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导并将导数设为零（一阶最优性条件）：\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\n求解 $\\beta$：\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n二阶导数为\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\n因此该解是唯一的最小化子。\n\n将此应用于数据点 $(x_{1},y_{1})=(1,3)$，$(x_{2},y_{2})=(2,5)$ 且 $\\lambda=1$ 的情况：\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "掌握了单系数的情况后，下一步自然是将其推广到包含多个预测变量的更一般模型中。本练习介绍了岭回归估计量的矩阵形式，这是在实践中处理复杂模型的标准且高效的方法。通过直接应用这一关键公式 [@problem_id:1951893]，你将练习处理岭回归问题中的核心矩阵运算，并计算出系数向量。", "problem": "在机器学习领域，岭回归是用于正则化线性回归模型的一种常用技术。这对于防止过拟合并处理预测变量之间的多重共线性特别有用。岭回归的系数向量估计量 $\\hat{\\beta}_{\\lambda}$ 由以下公式给出：\n$$ \\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y $$\n此处，$X$ 是设计矩阵，$y$ 是观测结果向量，$I$ 是适当维度的单位矩阵，$\\lambda$ 是一个非负的正则化参数。\n\n假设对于一个具有两个预测变量的特定数据集，已预先计算出以下量：\n$$ X^T X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} \\quad \\text{and} \\quad X^T y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\n使用正则化参数 $\\lambda = 5$，确定岭回归系数向量 $\\hat{\\beta}_5$。", "solution": "岭回归估计量定义为\n$$\n\\hat{\\beta}_{\\lambda} = (X^{T}X + \\lambda I)^{-1} X^{T} y.\n$$\n根据给定的数据，\n$$\nX^{T}X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix}, \\quad X^{T} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\n计算正则化矩阵：\n$$\nX^{T}X + \\lambda I = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} + 5 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 15  5 \\\\ 5  15 \\end{pmatrix}.\n$$\n对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵由以下公式给出\n$$\n\\left(\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}.\n$$\n应用此公式，\n$$\n\\det(X^{T}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\n所以\n$$\n(X^{T}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix}.\n$$\n那么\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$", "id": "1951893"}, {"introduction": "想要真正掌握一个概念，从不同角度审视它会大有裨益。这个练习揭示了对岭回归一个出人意料且非常巧妙的理解方式 [@problem_id:1951855]。它将向你展示，岭回归在数学上等价于在一个特殊构造的“增广”数据集上进行普通的最小二乘（OLS）回归。这一深刻的见解将新方法与你所熟悉的线性回归基础联系起来，帮助你更直观地理解正则化惩罚项是如何发挥作用的。", "problem": "在线性回归的背景下，普通最小二乘法 (Ordinary Least Squares, OLS) 是估计线性模型系数的标准方法。对于一个由 $y = X\\beta + \\epsilon$ 描述的模型，其中 $y$ 是一个 $n \\times 1$ 的观测响应向量，$X$ 是一个 $n \\times p$ 的满秩预测变量矩阵（设计矩阵），$\\beta$ 是一个 $p \\times 1$ 的未知系数向量，$\\epsilon$ 是一个误差向量，最小化残差平方和的 OLS 估计量由以下公式给出：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^T X)^{-1} X^T y\n$$\n\n现在，考虑一个我们构建“增广”数据集的场景。令 $\\lambda$ 为一个正常数标量。我们如下构建一个新的增广设计矩阵 $X_{\\text{aug}}$ 和一个新的响应向量 $y_{\\text{aug}}$：\n$$\nX_{\\text{aug}} = \\begin{pmatrix} X \\\\ \\sqrt{\\lambda}I \\end{pmatrix} \\quad \\text{和} \\quad y_{\\text{aug}} = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}\n$$\n这里，$I$ 是 $p \\times p$ 的单位矩阵，$0$ 是一个 $p \\times 1$ 的零列向量。矩阵 $X_{\\text{aug}}$ 的维度是 $(n+p) \\times p$，向量 $y_{\\text{aug}}$ 的维度是 $(n+p) \\times 1$。\n\n你的任务是将标准的 OLS 公式应用于这个增广系统，以找到相应的系数估计量，我们称之为 $\\hat{\\beta}_{\\text{aug}}$。请将你关于 $\\hat{\\beta}_{\\text{aug}}$ 的最终答案表示为包含 $X$、$y$ 和 $\\lambda$ 的单个符号表达式。", "solution": "我们从应用于增广系统的标准 OLS 估计量开始。对于任何设计矩阵 $Z$ 和响应向量 $w$，OLS 估计量由下式给出\n$$\n\\hat{\\beta}=(Z^{T}Z)^{-1}Z^{T}w\n$$\n前提是 $Z^{T}Z$ 是可逆的。\n\n在这里，我们设 $Z=X_{\\text{aug}}$ 和 $w=y_{\\text{aug}}$，其中\n$$\nX_{\\text{aug}}=\\begin{pmatrix}X\\\\ \\sqrt{\\lambda}I\\end{pmatrix}, \\quad y_{\\text{aug}}=\\begin{pmatrix}y\\\\ 0\\end{pmatrix}.\n$$\n我们计算所需的两个部分：\n\n1) 增广设计的格拉姆矩阵：\n使用分块矩阵乘法恒等式 $\\begin{pmatrix}A\\\\ B\\end{pmatrix}^{T}\\begin{pmatrix}A\\\\ B\\end{pmatrix}=A^{T}A+B^{T}B$，我们得到\n$$\nX_{\\text{aug}}^{T}X_{\\text{aug}}=X^{T}X+(\\sqrt{\\lambda}I)^{T}(\\sqrt{\\lambda}I)=X^{T}X+\\lambda I,\n$$\n因为 $I^{T}=I$ 且标量与矩阵乘法可交换。\n\n2) 与增广响应的交叉项：\n使用 $\\begin{pmatrix}A\\\\ B\\end{pmatrix}^{T}\\begin{pmatrix}c\\\\ d\\end{pmatrix}=A^{T}c+B^{T}d$，我们有\n$$\nX_{\\text{aug}}^{T}y_{\\text{aug}}=X^{T}y+(\\sqrt{\\lambda}I)^{T}0=X^{T}y.\n$$\n\n因此，增广系统的 OLS 估计量是\n$$\n\\hat{\\beta}_{\\text{aug}}=\\left(X_{\\text{aug}}^{T}X_{\\text{aug}}\\right)^{-1}X_{\\text{aug}}^{T}y_{\\text{aug}}=\\left(X^{T}X+\\lambda I\\right)^{-1}X^{T}y.\n$$\n当 $\\lambda>0$ 时，该表达式是良定义的，因为在给定的假设下，$X^{T}X+\\lambda I$ 是可逆的。", "answer": "$$\\boxed{(X^{T}X+\\lambda I)^{-1}X^{T}y}$$", "id": "1951855"}]}