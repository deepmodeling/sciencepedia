{"hands_on_practices": [{"introduction": "为了真正掌握岭回归的机制，最有效的方法就是从一个基础的计算问题入手。这个练习将问题简化到极致，旨在通过一个仅包含两个数据点的简单线性模型，亲手计算岭回归系数。通过这个过程，你将清晰地看到正则化参数 $\\lambda$ 如何直接调整模型的最小化目标函数，并最终影响系数的估计值，从而直观地理解岭回归的“收缩”效应 [@problem_id:1950377]。", "problem": "一位数据科学家正在分析一个非常小的数据集，其中仅包含两个观测值 $(x_1, y_1) = (1, 2)$ 和 $(x_2, y_2) = (3, 4)$。他们选择使用一个穿过原点的简单线性模型来建模变量之间的关系，该模型由方程 $y_i = \\beta x_i + \\epsilon_i$ 描述，其中 $\\beta$ 是要估计的单一模型参数，而 $\\epsilon_i$ 代表第 $i$ 个观测值的误差项。\n\n为了防止过拟合，即使是对于这个极小的数据集，该科学家也决定使用岭回归。目标是找到能使惩罚平方残差和最小化的 $\\beta$ 值，该惩罚平方残差和由下列表达式给出：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n给定正则化参数（或惩罚项）为 $\\lambda = 1$，计算岭回归估计值，记为 $\\hat{\\beta}_{\\text{Ridge}}$。请将答案表示为精确分数。", "solution": "我们最小化惩罚平方和\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导并令其为零，得到一阶条件\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}(y_{i}-\\beta x_{i})+2\\lambda \\beta=0.\n$$\n整理得，\n$$\n\\sum_{i=1}^{n}x_{i}y_{i}-\\beta \\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta=0\n\\quad\\Longrightarrow\\quad\n\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=\\sum_{i=1}^{n}x_{i}y_{i}.\n$$\n因此，岭估计量（无截距项）为\n$$\n\\hat{\\beta}_{\\text{Ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n对于给定的数据 $(x_{1},y_{1})=(1,2)$ 和 $(x_{2},y_{2})=(3,4)$ 以及 $\\lambda=1$，\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 2+3\\cdot 4=14,\\qquad \\sum_{i=1}^{2}x_{i}^{2}=1^{2}+3^{2}=10,\n$$\n所以\n$$\n\\hat{\\beta}_{\\text{Ridge}}=\\frac{14}{10+1}=\\frac{14}{11}.\n$$", "answer": "$$\\boxed{\\frac{14}{11}}$$", "id": "1950377"}, {"introduction": "在了解了岭回归之后，我们转向另一种强大的正则化方法：LASSO。LASSO与岭回归的关键区别在于其惩罚项使用了 $L_1$ 范数，这一改变赋予了LASSO独特的变量选择能力。本练习将引导你计算LASSO估计值，并探索一个核心概念：找到使系数收缩至零的最小正则化参数 $\\lambda_{crit}$，这个过程直观地展示了LASSO如何实现特征选择 [@problem_id:1950382]。", "problem": "一位数据科学家正在研究单个预测变量 $x$ 和响应变量 $y$ 之间的关系。所提出的模型是简单线性回归：$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。为了防止过拟合并进行特征选择，该数据科学家决定使用最小绝对收缩和选择算子 (LASSO) 方法进行参数估计。\n\n截距 $\\beta_0$ 和斜率 $\\beta_1$ 的估计值是通过最小化以下目标函数得到的：\n$$Q(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 + \\lambda |\\beta_1|$$\n其中 $\\lambda \\geq 0$ 是正则化参数。请注意，截距项 $\\beta_0$ 没有被惩罚。\n\n已收集到一个包含 $n=4$ 个观测值的小数据集：\n预测变量 $x$：$(-2, -1, 1, 2)$\n响应变量 $y$：$(-1, 0, 2, 3)$\n\n你的任务是：\n1. 对于正则化参数值为 $\\lambda = 12$ 时，斜率系数的 LASSO 估计值 $\\hat{\\beta}_1$ 是多少？\n2. 能使斜率估计值 $\\hat{\\beta}_1$ 恰好为零的正则化参数的最小值是多少？我们将其表示为 $\\lambda_{crit}$。\n\n请以有序对 $(\\hat{\\beta}_1, \\lambda_{crit})$ 的形式给出你的精确数值答案。", "solution": "我们最小化 LASSO 目标函数\n$$Q(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)^{2}+\\lambda|\\beta_{1}|,$$\n其中 $\\beta_{0}$ 未被惩罚。对于任何固定的 $\\beta_{1}$，最小化截距是通过将平方误差部分关于 $\\beta_{0}$ 的导数设为零来获得的，这会得到\n$$\\hat{\\beta}_{0}(\\beta_{1})=\\bar{y}-\\beta_{1}\\bar{x}.$$\n代入后，得到以中心化变量 $x_{i}^{c}=x_{i}-\\bar{x}$ 和 $y_{i}^{c}=y_{i}-\\bar{y}$ 表示的剖析目标函数：\n$$Q(\\beta_{1})=\\sum_{i=1}^{n}\\left(y_{i}^{c}-\\beta_{1}x_{i}^{c}\\right)^{2}+\\lambda|\\beta_{1}|.$$\n定义\n$$S_{xx}=\\sum_{i=1}^{n}\\left(x_{i}^{c}\\right)^{2},\\qquad S_{xy}=\\sum_{i=1}^{n}x_{i}^{c}y_{i}^{c}.$$\n对于 $\\beta_{1}\\neq 0$，导数条件是\n$$\\frac{dQ}{d\\beta_{1}}=-2S_{xy}+2\\beta_{1}S_{xx}+\\lambda\\,\\mathrm{sign}(\\beta_{1})=0,$$\n如果符号一致，这会给出\n$$\\beta_{1}=\\frac{S_{xy}-\\frac{\\lambda}{2}\\mathrm{sign}(\\beta_{1})}{S_{xx}}.$$\n在 $\\beta_{1}=0$ 处的次梯度条件是\n$$0\\in -2S_{xy}+[-\\lambda,\\lambda]\\quad\\Longleftrightarrow\\quad |2S_{xy}|\\leq \\lambda.$$\n等价地，解是软阈值形式\n$$\\hat{\\beta}_{1}=\\frac{\\mathrm{sign}(S_{xy})\\max\\left(|S_{xy}|-\\frac{\\lambda}{2},\\,0\\right)}{S_{xx}}.$$\n\n对于给定的数据，计算均值 $\\bar{x}$ 和 $\\bar{y}$：\n$$\\bar{x}=\\frac{-2-1+1+2}{4}=0,\\qquad \\bar{y}=\\frac{-1+0+2+3}{4}=1.$$\n因此，中心化变量是 $x^{c}=(-2,-1,1,2)$ 和 $y^{c}=(-2,-1,1,2)$。因此，\n$$S_{xx}=\\sum_{i=1}^{4}(x_{i}^{c})^{2}=4+1+1+4=10,\\qquad S_{xy}=\\sum_{i=1}^{4}x_{i}^{c}y_{i}^{c}=\\sum_{i=1}^{4}(x_{i}^{c})^{2}=10.$$\n\n1) 对于 $\\lambda=12$，应用软阈值公式：\n$$\\hat{\\beta}_{1}=\\frac{\\max\\left(10-\\frac{12}{2},\\,0\\right)}{10}=\\frac{10-6}{10}=\\frac{2}{5}.$$\n\n2) 使 $\\hat{\\beta}_{1}=0$ 的最小 $\\lambda$ 是满足 $|2S_{xy}|\\leq \\lambda$ 的阈值，即，\n$$\\lambda_{\\text{crit}}=2|S_{xy}|=2\\cdot 10=20.$$\n\n因此，有序对是 $\\left(\\frac{2}{5},\\,20\\right)$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{5} & 20 \\end{pmatrix}}$$", "id": "1950382"}, {"introduction": "正则化方法的一个主要应用场景是处理多重共线性问题，即预测变量之间存在高度相关性。在这种情况下，传统的普通最小二乘法（OLS）估计会变得极不稳定。这个练习模拟了一个存在完全共线性的极端情况，并要求你使用岭回归进行建模，通过推导你会发现岭回归如何巧妙地处理这个问题，将系数的影响力在相关的预测变量之间进行分配 [@problem_id:1950412]。", "problem": "一位数据科学家正在使用一个数据集来构建一个预测模型。在进行初步数据探索后，他们发现两个预测变量 $x_1$ 和 $x_2$ 之间存在完全线性关系。具体来说，对于样本量为 $n$ 的样本中的每个数据点 $i$，都存在 $x_{i1} = x_{i2}$。同时假设所有变量（预测变量和响应变量 $y$）都已中心化，即它们的均值为零。\n\n该数据科学家决定拟合一个不含截距项的线性模型：\n$$y_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$$\n为了处理 $x_1$ 和 $x_2$ 之间的完全共线性问题，他们采用了岭回归。岭回归系数的估计值 $(\\hat{\\beta}_1, \\hat{\\beta}_2)$ 是使以下目标函数最小化的 $(\\beta_1, \\beta_2)$ 的值：\n$$L(\\beta_1, \\beta_2) = \\sum_{i=1}^{n} (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^{2} + \\lambda (\\beta_1^{2} + \\beta_2^{2})$$\n此处，$\\lambda$ 是一个严格为正的正则化参数（$\\lambda > 0$）。\n\n根据数据，计算出以下汇总统计量：\n- $\\sum_{i=1}^{n} y_i x_{i1} = A$\n- $\\sum_{i=1}^{n} x_{i1}^{2} = B$\n\n请用 $A$、$B$ 和 $\\lambda$ 来表示估计系数 $\\hat{\\beta}_1$。", "solution": "我们最小化岭回归目标函数\n$$L(\\beta_{1},\\beta_{2})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{1}x_{i1}-\\beta_{2}x_{i2}\\right)^{2}+\\lambda\\left(\\beta_{1}^{2}+\\beta_{2}^{2}\\right),$$\n其中对于所有 $i$ 都有 $x_{i1}=x_{i2}$，并且汇总统计量为 $\\sum_{i=1}^{n}y_{i}x_{i1}=A$ 和 $\\sum_{i=1}^{n}x_{i1}^{2}=B$。同时定义 $C=\\sum_{i=1}^{n}x_{i1}x_{i2}$。因为 $x_{i1}=x_{i2}$ 逐点成立，所以我们有 $C=B$，并且类似地有 $\\sum_{i=1}^{n}y_{i}x_{i2}=A$。\n\n令偏导数为零。对于 $\\beta_{1}$：\n$$\\frac{\\partial L}{\\partial \\beta_{1}}=2\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{1}x_{i1}-\\beta_{2}x_{i2}\\right)(-x_{i1})+2\\lambda\\beta_{1}=0,$$\n展开得\n$$-2\\sum_{i=1}^{n}x_{i1}y_{i}+2\\beta_{1}\\sum_{i=1}^{n}x_{i1}^{2}+2\\beta_{2}\\sum_{i=1}^{n}x_{i1}x_{i2}+2\\lambda\\beta_{1}=0.$$\n两边除以 $2$ 并代入 $A$、$B$ 和 $C$，得到\n$$(B+\\lambda)\\beta_{1}+C\\beta_{2}=A.$$\n根据对称性，关于 $\\beta_{2}$ 的导数得出\n$$C\\beta_{1}+(B+\\lambda)\\beta_{2}=A.$$\n\n使用 $C=B$，方程组变为\n$$(B+\\lambda)\\beta_{1}+B\\beta_{2}=A,\\qquad B\\beta_{1}+(B+\\lambda)\\beta_{2}=A.$$\n将两个方程相减，得到\n$$\\lambda(\\beta_{1}-\\beta_{2})=0.$$\n由于 $\\lambda>0$，可得 $\\beta_{1}=\\beta_{2}$。将 $\\beta_{2}=\\beta_{1}$ 代入任意一个方程，得到\n$$(2B+\\lambda)\\beta_{1}=A,$$\n所以\n$$\\hat{\\beta}_{1}=\\frac{A}{2B+\\lambda}.$$", "answer": "$$\\boxed{\\frac{A}{2B+\\lambda}}$$", "id": "1950412"}]}