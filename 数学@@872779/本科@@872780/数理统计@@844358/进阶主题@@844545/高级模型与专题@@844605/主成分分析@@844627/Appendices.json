{"hands_on_practices": [{"introduction": "第一个实践是一项基础练习。通过处理一个简单的 $2 \\times 2$ 协方差矩阵，你将亲手计算特征值，并理解它们如何代表每个主成分所捕获的方差。这个练习将巩固“解释方差”的概念，并阐明主成分分析旨在最大化方差捕获的目标。[@problem_id:1946278]", "problem": "一架自主环境监测无人机使用一对相同的传感器来测量大气压力。设两个传感器的读数在减去其长期平均值进行中心化后，由随机变量 $X_1$ 和 $X_2$ 表示。\n\n这些读数的联合行为由一个二元随机向量 $(X_1, X_2)$ 描述，其协方差矩阵为 $\\Sigma$。由于这些传感器类型相同，且受到类似的环境波动影响，它们具有相同的方差，$\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma^2$，其中 $\\sigma > 0$ 为某个常数。它们的读数也是相关的，相关系数为 $\\rho$，满足 $0  \\rho  1$。因此，协方差矩阵由下式给出：\n$$\n\\Sigma = \\begin{pmatrix} \\sigma^2  \\rho\\sigma^2 \\\\ \\rho\\sigma^2  \\sigma^2 \\end{pmatrix}\n$$\n为了减少数据冗余并识别主要的变异轴，工程团队应用了主成分分析 (PCA)。PCA 将原始的相关变量 $(X_1, X_2)$ 转换为一组新的不相关变量，称为主成分。第一主成分被定义为能够捕捉最大可能方差的 $X_1$ 和 $X_2$ 的线性组合。\n\n确定数据中总方差由第一主成分解释的比例。请用含 $\\rho$ 的符号表达式表示你的答案。", "solution": "我们给定一个中心化的二元随机向量，其协方差矩阵为\n$$\n\\Sigma=\\begin{pmatrix}\\sigma^{2}  \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2}  \\sigma^{2}\\end{pmatrix},\n$$\n其中 $0  \\rho  1$ 且 $\\sigma > 0$。在 PCA 中，主成分的方差是协方差矩阵的特征值。由第一主成分解释的总方差比例等于其特征值除以总方差，总方差即为 $\\Sigma$ 的迹。\n\n首先，通过求解特征方程来计算 $\\Sigma$ 的特征值\n$$\n\\det(\\Sigma-\\lambda I)=0.\n$$\n我们有\n$$\n\\det\\begin{pmatrix}\\sigma^{2}-\\lambda  \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2}  \\sigma^{2}-\\lambda\\end{pmatrix}\n=(\\sigma^{2}-\\lambda)^{2}-(\\rho\\sigma^{2})^{2}=0.\n$$\n因此，\n$$\n(\\sigma^{2}-\\lambda)^{2}=\\rho^{2}\\sigma^{4}\n\\quad\\Longrightarrow\\quad\n\\sigma^{2}-\\lambda=\\pm\\rho\\sigma^{2}\n\\quad\\Longrightarrow\\quad\n\\lambda=\\sigma^{2}(1\\pm\\rho).\n$$\n由于 $0  \\rho  1$，最大的特征值是\n$$\n\\lambda_{1}=\\sigma^{2}(1+\\rho).\n$$\n总方差等于 $\\Sigma$ 的迹，\n$$\n\\operatorname{tr}(\\Sigma)=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2},\n$$\n这也等于特征值之和 $\\sigma^{2}(1+\\rho)+\\sigma^{2}(1-\\rho)=2\\sigma^{2}$。因此，由第一主成分解释的总方差比例为\n$$\n\\frac{\\lambda_{1}}{\\operatorname{tr}(\\Sigma)}=\\frac{\\sigma^{2}(1+\\rho)}{2\\sigma^{2}}=\\frac{1+\\rho}{2}.\n$$", "answer": "$$\\boxed{\\frac{1+\\rho}{2}}$$", "id": "1946278"}, {"introduction": "真实世界的数据集通常包含具有不同单位和尺度的变量。这个动手编码练习阐释了一个在应用主成分分析时必须避免的关键陷阱：未对数据进行标准化。通过模拟，你将定量地观察到未经缩放的数据如何产生误导性的主成分，这些主成分往往被尺度最大的变量所主导，而非由其内在结构的重要性决定。[@problem_id:2421735]", "problem": "本题要求您运用主成分分析（PCA）的基本原理，演示当变量以不同单位度量时，若不进行标准化，会如何扭曲主方向的估计和解释方差。您将在一个纯数学框架下进行操作，使用一个模拟典型金融变量（如价格和交易量）的合成数据生成过程。您将实现完整的分析流程，并报告定量诊断指标，以比较对原始数据执行PCA与对标准化数据执行PCA的结果。\n\n基本原理：\n- PCA旨在寻找使样本方差最大化的标准正交方向。给定一个中心化的数据矩阵 $X \\in \\mathbb{R}^{T \\times n}$，样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^\\top X$。主成分是 $\\Sigma$ 的特征向量，按其对应的特征值从大到小排序。\n- 标准化将每个变量 $x_j$ 转换为 $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$，其中 $\\bar{x}_j$ 是样本均值，$\\hat{\\sigma}_j$ 是样本标准差，从而使每个标准化后的变量都具有单位样本方差。对标准化数据进行PCA等同于对样本相关系数矩阵进行PCA。\n- 对变量应用对角缩放 $D = \\operatorname{diag}(s_1,\\dots,s_n)$，即 $X \\mapsto X D$，会将协方差矩阵的元素乘以 $s_i s_j$，因此除非所有 $s_j$ 都相等，否则会改变特征向量。\n\n数据生成过程：\n- 对于每个测试用例 $k$，固定样本数量 $T_k \\in \\mathbb{N}$、变量数量 $n_k \\in \\mathbb{N}$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、异质性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n- 生成单一共同因子 $f_t \\sim \\mathcal{N}(0,1)$（对于 $t = 1,\\dots,T_k$），以及异质性噪声 $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$，所有因子和噪声在 $t$ 和 $j$ 上相互独立。\n- 构造原始观测值 $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$（对于 $t=1,\\dots,T_k$ 和 $j=1,\\dots,n_k$）。\n- 在计算任何协方差之前，通过减去样本均值来中心化 $X$ 的每一列。\n\n每个测试用例的计算任务：\n- 根据中心化的原始数据 $X$ 计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$，并获得第一主成分的特征向量 $v_{\\text{raw}}$（单位范数）及其特征值 $\\lambda_{\\text{raw}}$。\n- 将 $X$ 的每一列标准化为单位样本方差以得到 $Z$，计算 $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$（即样本相关系数矩阵），并获得第一主成分的特征向量 $v_{\\text{std}}$（单位范数）及其特征值 $\\lambda_{\\text{std}}$。\n- 计算角度 $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$；以弧度报告 $\\theta$。\n- 计算解释方差份额的差异 $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$，该值必须以小数形式报告（而非百分比）。\n\n随机性与可复现性：\n- 在整个实验中使用固定的伪随机数生成器种子 $314159$，以确保结果可复现。\n\n测试套件：\n- 共有 $3$ 个测试用例。对于每个测试用例 $k$，使用以下参数 $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$：\n  - 用例 $1$（单位相似，两个变量）：\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$.\n  - 用例 $2$（单位不匹配，两个变量：一个因尺度而占主导地位）：\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$.\n  - 用例 $3$（单位不匹配，三个变量：一个巨大尺度，一个中等尺度，一个微小尺度）：\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$.\n\n每个测试用例的所需输出：\n- 一个包含两个浮点数的列表 $[\\theta, \\Delta]$，其中 $\\theta$ 是以弧度为单位的角度，$\\Delta$ 是解释方差份额的绝对差。两个值都必须精确到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由各用例列表组成的逗号分隔列表，并用方括号括起来，例如 $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$，每个浮点数都精确到 $6$ 位小数，角度以弧度为单位。", "solution": "所提出的问题是计算统计学中一个有效且定义明确的练习，专门用于演示主成分分析（PCA）对变量尺度的敏感性。该问题在科学上是合理的，基于线性代数和统计学的基础原理，并且所有参数和步骤都已明确指定，足以得出一个唯一的、可验证的解。我们将继续进行分析。\n\n核心论点是，PCA作为一种方差最大化技术，不具备尺度不变性。当变量以迥异的单位度量时（例如，以美元计的股价与以百万股计的交易量），方差最大的变量将在机制上主导第一个主成分。这通常是所选单位造成的人为结果，而不是真实潜在重要性的指标。标准化是标准的补救措施，它将所有变量转换到同一尺度（单位方差），从而使分析集中于数据的相关性结构，而非任意的度量尺度。\n\n我们首先将数据生成和分析流程形式化。\n\n**1. 数据生成过程**\n\n对于每个测试用例 $k$，我们给定一个样本大小 $T_k$、变量数量 $n_k$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、异质性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n\n数据由单因子模型生成。对于每个时间点 $t=1, \\dots, T_k$，从标准正态分布中抽取一个共同的潜在因子 $f_t \\sim \\mathcal{N}(0, 1)$。对于每个变量 $j=1, \\dots, n_k$，从 $\\mathcal{N}(0, (u^{(k)}_j)^2)$ 中抽取一个异质性噪声项 $e_{t,j}$。所有的 $f_t$ 和 $e_{t,j}$ 相互独立。\n\n时刻 $t$ 变量 $j$ 的观测值构造如下：\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\n这构成了一个数据矩阵 $X \\in \\mathbb{R}^{T_k \\times n_k}$，其列代表不同的变量。尺度因子 $s^{(k)}_j$ 代表变量 $j$ 的任意度量单位。\n\n**2. 对原始数据进行PCA（基于协方差的PCA）**\n\nPCA的第一步是通过减去列向的样本均值来中心化数据。设 $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ 为第 $j$ 个变量的样本均值。中心化后的数据矩阵记为 $X_c$，其元素为 $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$。\n\n然后计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$：\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\n主成分是 $\\Sigma_{\\text{raw}}$ 的特征向量。我们对该矩阵进行特征分解：\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\n其中 $V$ 是标准正交特征向量组成的矩阵，$\\Lambda$ 是相应特征值组成的对角矩阵。特征值按降序排列，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$。第一主成分是与最大特征值 $\\lambda_1$ 相关联的特征向量 $v_1$。在本问题中，我们将此特征向量记为 $v_{\\text{raw}}$，特征值记为 $\\lambda_{\\text{raw}}$。\n\n**3. 对标准化数据进行PCA（基于相关系数的PCA）**\n\n为消除任意尺度的影响，我们对数据进行标准化。对于原始数据矩阵 $X$ 的每一列 $j$，我们计算其样本标准差 $\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$。\n\n标准化数据矩阵 $Z$ 的构造元素为：\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\n根据构造， $Z$ 的每一列都具有样本均值 $0$ 和样本方差 $1$。\n\n然后对这个标准化数据 $Z$ 执行PCA。相关矩阵是 $Z$ 的样本协方差矩阵，我们记为 $\\Sigma_{\\text{std}}$：\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\n由于 $Z$ 的每一列都具有单位方差，$\\Sigma_{\\text{std}}$ 的对角元素均为 $1$，而非对角元素 $(i, j)$ 是原始变量 $x_i$ 和 $x_j$ 之间的样本相关系数。因此，$\\Sigma_{\\text{std}}$ 是 $X$ 的样本相关系数矩阵。\n\n我们对 $\\Sigma_{\\text{std}}$ 进行特征分解，以找到其最大特征值 $\\lambda_{\\text{std}}$ 和相应的特征向量 $v_{\\text{std}}$。\n\n**4. 诊断指标**\n\n为量化因未标准化而造成的扭曲，我们计算两个指标：\n\n- **主成分之间的角度**：主成分方向 $v_{\\text{raw}}$ 和 $v_{\\text{std}}$ 是 $\\mathbb{R}^{n_k}$中的单位向量。它们之间的角度衡量了最大方差方向的偏移程度。由于特征向量仅定义到符号为止（即，如果 $v$ 是一个特征向量，那么 $-v$ 也是），我们计算它们所张成的直线之间的锐角：\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  $\\theta=0$ 的值表示完全对齐，而一个大角度（接近 $\\pi/2$）表示严重错位。\n\n- **解释方差份额的差异**：由第一主成分解释的总方差比例由其特征值除以所有特征值的总和给出。特征值的总和等于矩阵的迹，$\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$，代表数据中的总方差。我们计算解释方差份额的绝对差：\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  请注意，对于标准化数据，$\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$，即变量的数量。一个大的 $\\Delta$ 值表明两种方法对第一主成分重要性的评估截然不同。\n\n将使用指定的参数和固定的随机种子对每个测试用例执行此程序，以保证可复现性。预计结果将显示，用例1（尺度相似）的扭曲最小，而用例2和3（尺度迥异）的扭曲显著，从而验证了在实践中进行标准化的必要性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421735"}, {"introduction": "在金融和基因组学等许多现代领域，我们经常会遇到变量多于观测值 ($p \\gt n$) 的数据集。这个问题探讨了主成分分析在这种高维场景下面临的数学限制。通过确定样本协方差矩阵可能拥有的最大非零特征值数量，你将揭示数据协方差结构固有的维度上限，这对于正确解释分析结果至关重要。[@problem_id:2421774]", "problem": "一位金融分析师正准备对一组资产收益率面板数据进行主成分分析 (PCA)。令 $X \\in \\mathbb{R}^{n \\times p}$ 表示数据矩阵，其中 $p$ 列中的每一列包含了一项资产在 $n$ 个连续时期内的收益率时间序列，每一行对应一个时间段。假设 $n \\geq 2$ 且 $n  p$。该分析师使用标准无偏估计量构建了这 $p$ 项资产收益率的样本协方差矩阵，该估计量是根据对 $n$ 个观测值进行均值中心化的变量计算得出的。\n\n用 $n$ 和 $p$ 表示，该样本协方差矩阵的非零特征值的最大可能数量是多少？请提供一个单一的闭式解析表达式作为您的答案。不要包含单位，也不要对答案进行四舍五入。", "solution": "用户提交了一个问题以供验证和解答。\n\n**第一步：提取已知条件**\n- 数据矩阵：$X \\in \\mathbb{R}^{n \\times p}$\n- 观测值/时间段数量：$n$\n- 资产/变量数量：$p$\n- 约束条件 1：$n \\geq 2$\n- 约束条件 2：$n  p$\n- 数据处理：数据矩阵的各列已对 $n$ 个观测值进行了均值中心化。\n- 待分析矩阵：$p$ 项资产收益率的样本协方差矩阵 $S$，使用标准无偏估计量计算。\n- 问题：此样本协方差矩阵 $S$ 的非零特征值的最大可能数量是多少？答案必须用 $n$ 和 $p$ 表示。\n\n**第二步：使用提取的已知条件进行验证**\n根据所需标准对问题进行评估。\n\n- **科学依据：** 该问题基于应用于计算金融的线性代数和统计学的基本概念，具体涉及主成分分析 (PCA) 和样本协方差矩阵的性质。所描述的情景，即资产数量 $p$ 超过观测值数量 $n$，是金融计量经济学中一个常见且重要的案例，通常被称为高维或“胖矩阵”问题。这些概念是标准的，并且在事实上是正确的。\n- **适定性：** 该问题提供了所有必要信息（$n, p$、约束条件 $n \\geq 2$ 和 $n  p$，以及协方差矩阵的构建方法），以确定一个唯一的、有意义的答案。\n- **客观性：** 问题陈述使用精确、客观的数学语言进行表述，没有歧义或主观性陈述。\n\n**结论：** 该问题有效。这是一个在多元统计和线性代数中定义明确的问题。\n\n**第三步：进行求解**\n问题要求样本协方差矩阵 $S$ 的非零特征值的最大可能数量。任何矩阵的非零特征值的数量等于其秩。因此，问题转化为求 $S$ 的秩的最大可能值。\n\n令均值中心化后的数据矩阵为 $\\tilde{X} \\in \\mathbb{R}^{n \\times p}$。根据均值中心化的定义，$\\tilde{X}$ 的每一列之和为零。样本协方差矩阵的标准无偏估计量由下式给出：\n$$\nS = \\frac{1}{n-1} \\tilde{X}^T \\tilde{X}\n$$\n这里，$S$ 是一个 $p \\times p$ 矩阵。\n\n矩阵的秩不受乘以一个非零标量的影响。由于问题指定 $n \\geq 2$，标量因子 $\\frac{1}{n-1}$ 是有定义的且非零。因此，我们有：\n$$\n\\text{rank}(S) = \\text{rank}\\left(\\frac{1}{n-1} \\tilde{X}^T \\tilde{X}\\right) = \\text{rank}(\\tilde{X}^T \\tilde{X})\n$$\n\n线性代数的一个基本定理指出，对于任何矩阵 $A$，$\\text{rank}(A^T A) = \\text{rank}(A)$。应用此定理，我们发现协方差矩阵的秩等于均值中心化数据矩阵的秩：\n$$\n\\text{rank}(S) = \\text{rank}(\\tilde{X})\n$$\n\n现在，问题简化为求 $n \\times p$ 均值中心化矩阵 $\\tilde{X}$ 的最大可能秩。任何矩阵的秩不能超过其维度的最小值。\n$$\n\\text{rank}(\\tilde{X}) \\leq \\min(n, p)\n$$\n考虑到约束条件 $n  p$，此不等式变为：\n$$\n\\text{rank}(\\tilde{X}) \\leq n\n$$\n\n但是，我们必须考虑均值中心化所施加的约束。$\\tilde{X}$ 的 $p$ 个列向量中的每一个，记为 $\\tilde{x}_j \\in \\mathbb{R}^n$（其中 $j=1, \\dots, p$），其元素之和为零。这可以表示为与一个全1向量的内积。令 $\\mathbf{1} \\in \\mathbb{R}^n$ 是一个每个元素都为 1 的列向量。均值中心化的约束是：\n$$\n\\mathbf{1}^T \\tilde{x}_j = 0 \\quad \\text{for all } j \\in \\{1, 2, \\dots, p\\}\n$$\n这个约束意味着 $\\tilde{X}$ 的所有列向量都必须位于 $\\mathbb{R}^n$ 中与向量 $\\mathbf{1}$ 正交的子空间内。令这个子空间为 $W$。\n$$\nW = \\{ v \\in \\mathbb{R}^n \\mid \\mathbf{1}^T v = 0 \\}\n$$\n$\\mathbb{R}^n$ 的维度是 $n$。由单个非零向量 $\\mathbf{1}$ 张成的子空间的维度是 $1$。子空间 $W$ 是由 $\\mathbf{1}$ 张成的子空间的正交补空间。因此，$W$ 的维度是：\n$$\n\\dim(W) = \\dim(\\mathbb{R}^n) - \\dim(\\text{span}(\\{\\mathbf{1}\\})) = n - 1\n$$\n由于 $\\tilde{X}$ 的所有列都属于子空间 $W$，因此 $\\tilde{X}$ 的列空间必须是 $W$ 的一个子空间。矩阵的秩是其列空间的维度。因此，$\\tilde{X}$ 的秩受 $W$ 的维度限制：\n$$\n\\text{rank}(\\tilde{X}) = \\dim(\\text{colspace}(\\tilde{X})) \\leq \\dim(W) = n - 1\n$$\n这就为 $\\tilde{X}$ 的秩建立了一个上界 $n-1$，从而也为 $S$ 的非零特征值的数量建立了上界。\n\n为了确定这是否是*最大可能*的秩，我们必须证明秩 $n-1$ 是可以达到的。如果我们可以构造一个数据矩阵 $X$，使其均值中心化版本 $\\tilde{X}$ 具有 $n-1$ 个线性无关的列，那么秩 $n-1$ 就可以达到。由于问题指定了 $p > n$ 和 $n \\geq 2$，我们有 $p > n-1$。这意味着我们的列数 ($p$) 多于子空间 $W$ 的维度 ($n-1$)，这使得 $\\tilde{X}$ 的列有可能张成 $W$。确实可以构造这样一个矩阵 $\\tilde{X}$，使其前 $n-1$ 列构成 $W$ 的一个基。例如，可以选择 $W$ 中的 $n-1$ 个线性无关向量作为 $\\tilde{X}$ 的前 $n-1$ 列，并将其余的 $p - (n-1)$ 列设置为零向量。这样的矩阵 $\\tilde{X}$ 的秩为 $n-1$，并且满足均值中心化的性质。\n\n因此，均值中心化数据矩阵 $\\tilde{X}$ 的最大可能秩是 $n-1$。\n$$\n\\max(\\text{rank}(\\tilde{X})) = n-1\n$$\n由于 $S$ 的非零特征值的数量等于 $\\tilde{X}$ 的秩，因此非零特征值的最大可能数量是 $n-1$。", "answer": "$$\n\\boxed{n-1}\n$$", "id": "2421774"}]}