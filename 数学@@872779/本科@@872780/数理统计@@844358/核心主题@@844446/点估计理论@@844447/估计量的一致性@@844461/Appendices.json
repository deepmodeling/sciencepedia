{"hands_on_practices": [{"introduction": "本练习旨在通过检验对样本均值（一个已知的一致估计量）的几种简单修改，来帮助你建立关于一致性核心定义的直观理解。通过分析这些修改如何影响估计量在大样本下的行为，你将学会运用像Slutsky定理这样的工具来严谨地判断一个估计量是否会依概率收敛于真实参数。这项实践将加深你对“依概率收敛”这一概念的理解。[@problem_id:1909315]", "problem": "设 $X_1, X_2, \\dots, X_n$ 是一个来自具有有限均值 $\\mu$ 和有限正方差 $\\sigma^2  0$ 的概率分布的独立同分布（i.i.d.）随机样本。设 $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ 是样本均值。\n\n如果一个参数的估计量在样本量 $n$ 趋于无穷大时，依概率收敛于该参数的真值，则称该估计量是相合的。\n\n考虑以下四个关于总体均值 $\\mu$ 的估计量。\n\nA. $\\hat{\\mu}_A = \\bar{X}_n + \\frac{1}{\\sqrt{n}}$\n\nB. $\\hat{\\mu}_B = \\frac{n}{n+2} \\bar{X}_n$\n\nC. $\\hat{\\mu}_C = \\frac{1}{2}(\\bar{X}_n + X_1)$\n\nD. $\\hat{\\mu}_D = \\bar{X}_n + \\frac{\\cos(n\\pi)}{n}$\n\n以上哪些估计量是 $\\mu$ 的相合估计量？选择所有适用的选项。", "solution": "根据弱大数定律，由于 $X_{i}$ 是独立同分布的，具有有限均值 $\\mu$ 和有限正方差 $\\sigma^{2}0$，所以 $\\bar{X}_{n} \\xrightarrow{p} \\mu$。我们利用这个事实和斯卢茨基定理来分析每个估计量。\n\nA. $\\hat{\\mu}_{A}=\\bar{X}_{n}+\\frac{1}{\\sqrt{n}}$。写成\n$$\n\\hat{\\mu}_{A}-\\mu=(\\bar{X}_{n}-\\mu)+\\frac{1}{\\sqrt{n}}.\n$$\n我们有 $\\bar{X}_{n}-\\mu \\xrightarrow{p} 0$ 且 $\\frac{1}{\\sqrt{n}} \\to 0$ 是确定性收敛。根据斯卢茨基定理（一个依概率收敛于 $0$ 的序列与一个确定性收敛于 $0$ 的序列之和），我们有 $\\hat{\\mu}_{A} \\xrightarrow{p} \\mu$。因此 $\\hat{\\mu}_{A}$ 是相合的。\n\nB. $\\hat{\\mu}_{B}=\\frac{n}{n+2}\\bar{X}_{n}$。注意 $\\frac{n}{n+2} \\to 1$ 是确定性收敛。那么\n$$\n\\hat{\\mu}_{B}=\\frac{n}{n+2}\\bar{X}_{n} \\xrightarrow{p} 1 \\cdot \\mu=\\mu\n$$\n根据斯卢茨基定理（一个依概率收敛的序列与一个确定性收敛于一个常数的序列之积）。更具体地说，\n$$\n\\hat{\\mu}_{B}-\\mu=\\frac{n}{n+2}(\\bar{X}_{n}-\\mu)+\\left(\\frac{n}{n+2}-1\\right)\\mu,\n$$\n其中第一项依概率收敛于 $0$，第二项确定性地收敛于 $0$。因此 $\\hat{\\mu}_{B}$ 是相合的。\n\nC. $\\hat{\\mu}_{C}=\\frac{1}{2}(\\bar{X}_{n}+X_{1})$。考虑对于任何 $\\epsilon0$，\n$$\n\\left|\\hat{\\mu}_{C}-\\mu\\right|=\\left|\\frac{1}{2}(\\bar{X}_{n}-\\mu)+\\frac{1}{2}(X_{1}-\\mu)\\right|\\ge \\frac{1}{2}|X_{1}-\\mu|-\\frac{1}{2}|\\bar{X}_{n}-\\mu|.\n$$\n因此，在事件 $\\{|X_{1}-\\mu|3\\epsilon,\\ |\\bar{X}_{n}-\\mu|\\epsilon\\}$ 上，我们有 $\\left|\\hat{\\mu}_{C}-\\mu\\right|\\epsilon$。所以，\n$$\n\\mathbb{P}\\left(\\left|\\hat{\\mu}_{C}-\\mu\\right|\\epsilon\\right)\\ge \\mathbb{P}\\left(|X_{1}-\\mu|3\\epsilon,\\ |\\bar{X}_{n}-\\mu|\\epsilon\\right)\\ge \\mathbb{P}\\left(|X_{1}-\\mu|3\\epsilon\\right)-\\mathbb{P}\\left(|\\bar{X}_{n}-\\mu|\\ge \\epsilon\\right).\n$$\n根据弱大数定律，$\\mathbb{P}\\left(|\\bar{X}_{n}-\\mu|\\ge \\epsilon\\right)\\to 0$。由于 $\\sigma^{2}0$，$X_{1}$ 的分布是非退化的，所以存在某个 $\\epsilon0$ 使得 $\\mathbb{P}\\left(|X_{1}-\\mu|3\\epsilon\\right)0$。对于这样的 $\\epsilon$，上述下界与 $0$ 有界相离，所以 $\\mathbb{P}\\left(\\left|\\hat{\\mu}_{C}-\\mu\\right|\\epsilon\\right)\\not\\to 0$。因此 $\\hat{\\mu}_{C}$ 不是相合的。\n\nD. $\\hat{\\mu}_{D}=\\bar{X}_{n}+\\frac{\\cos(n\\pi)}{n}$。注意 $\\cos(n\\pi)=(-1)^{n}$，所以 $\\frac{\\cos(n\\pi)}{n}\\to 0$ 是确定性收敛。那么\n$$\n\\hat{\\mu}_{D}-\\mu=(\\bar{X}_{n}-\\mu)+\\frac{\\cos(n\\pi)}{n},\n$$\n其中第一项依概率收敛于 $0$，第二项确定性地收敛于 $0$。根据斯卢茨基定理，$\\hat{\\mu}_{D} \\xrightarrow{p} \\mu$。因此 $\\hat{\\mu}_{D}$ 是相合的。\n\n所以，相合的估计量是 A、B 和 D。", "answer": "$$\\boxed{ABD}$$", "id": "1909315"}, {"introduction": "一个常见的误解是认为无偏估计量必然是一致的。这个练习提供了一个重要的反例，旨在澄清无偏性与一致性之间的关系。你将分析一个为指数分布均值构建的无偏估计量，并发现尽管它在任何样本量下期望都正确，但其方差并不随样本量增加而趋于零，因此它并非一致估计量。这个例子强调了，要保证一致性，估计量的变异性必须随着数据的增多而消失。[@problem_id:1909328]", "problem": "设 $X_1, X_2, \\ldots, X_n$ 是从指数分布中抽取的样本容量为 $n$ 的随机样本，其概率密度函数为 $f(x; \\theta) = \\frac{1}{\\theta} \\exp(-\\frac{x}{\\theta})$，其中 $x  0$ 且 $\\theta  0$。参数 $\\theta$ 表示该分布的均值。\n\n考虑参数 $\\theta$ 的一个估计量 $T_n$，定义为 $T_n = n X_{(1)}$，其中 $X_{(1)} = \\min(X_1, X_2, \\ldots, X_n)$ 是样本中的最小观测值。\n\n当样本容量 $n$ 趋于无穷大时，下列哪个陈述准确地描述了估计量 $T_n$ 的性质？\n\nA. $T_n$ 是 $\\theta$ 的相合估计量。\n\nB. $T_n$ 不是 $\\theta$ 的相合估计量，因为对于任何有限的 $n$，它都是一个有偏估计量。\n\nC. $T_n$ 不是 $\\theta$ 的相合估计量，因为当 $n \\to \\infty$ 时，其方差不趋于零。\n\nD. $T_n$ 是 $\\theta$ 的相合估计量，因为它是无偏估计量，并且当 $n \\to \\infty$ 时，其方差趋于零。\n\nE. $T_n$ 不是 $\\theta$ 的相合估计量，且它依概率收敛于 0。", "solution": "设 $X_{1},\\ldots,X_{n}$ 独立同分布于概率密度函数为 $f(x;\\theta)=(1/\\theta)\\exp(-x/\\theta)$（$x0$）的分布。其生存函数为 $P(X_{i}x)=\\exp(-x/\\theta)$，其中 $x\\geq 0$。\n\n对于最小值 $X_{(1)}=\\min(X_{1},\\ldots,X_{n})$，由独立性可得\n$$\nP(X_{(1)}x)=P(X_{1}x,\\ldots,X_{n}x)=\\left[P(X_{i}x)\\right]^{n}=\\exp\\!\\left(-\\frac{n x}{\\theta}\\right),\\quad x\\geq 0.\n$$\n因此 $X_{(1)}$ 服从速率为 $n/\\theta$ 的指数分布，所以\n$$\n\\mathbb{E}[X_{(1)}]=\\frac{\\theta}{n},\\qquad \\operatorname{Var}(X_{(1)})=\\frac{\\theta^{2}}{n^{2}}.\n$$\n定义 $T_{n}=nX_{(1)}$。则\n$$\n\\mathbb{E}[T_{n}]=n\\,\\mathbb{E}[X_{(1)}]=n\\cdot\\frac{\\theta}{n}=\\theta,\n$$\n所以对于任意 $n$，$T_{n}$ 都是无偏的。其方差为\n$$\n\\operatorname{Var}(T_{n})=n^{2}\\operatorname{Var}(X_{(1)})=n^{2}\\cdot\\frac{\\theta^{2}}{n^{2}}=\\theta^{2},\n$$\n当 $n\\to\\infty$ 时，该值不趋于 $0$。\n\n等价地， $T_{n}$ 的分布与 $n$ 无关：对于 $t\\geq 0$，\n$$\nP(T_{n}t)=P(nX_{(1)}t)=P\\!\\left(X_{(1)}\\frac{t}{n}\\right)=\\exp\\!\\left(-\\frac{n}{\\theta}\\cdot\\frac{t}{n}\\right)=\\exp\\!\\left(-\\frac{t}{\\theta}\\right),\n$$\n所以 $T_{n}$ 服从速率为 $1/\\theta$ 的指数分布，对于所有 $n$，其均值为 $\\theta$，方差为 $\\theta^{2}$。因此，$T_{n}$ 不依概率收敛于 $\\theta$（其方差不消失），并且不是相合的。它也不依概率收敛于 $0$，因为它的分布是非退化的，并且不随 $n$ 变化。\n\n因此，正确的陈述是，$T_{n}$ 不是一个相合估计量，因为当 $n\\to\\infty$ 时，其方差不趋于零。", "answer": "$$\\boxed{C}$$", "id": "1909328"}, {"introduction": "在实际应用中，许多估计量是样本矩的函数，例如最大似然估计量。本练习将引导你运用统计学中的两个基石性定理——大数定律和连续映射定理——来证明一个实际估计量的一致性。通过分析指数分布中失效率参数$\\lambda$的估计量$\\hat{\\lambda}_n = 1/\\bar{X}_n$，你将掌握一种证明复杂估计量一致性的强大通用方法。[@problem_id:1909316]", "problem": "一位材料科学家正在研究一种新型光纤电缆的寿命。该光缆一段的故障时间 $X$ 服从指数分布，其故障率 $\\lambda  0$ 为常数。其寿命的概率密度函数 (PDF) 为 $f(x; \\lambda) = \\lambda \\exp(-\\lambda x)$，其中 $x \\ge 0$。为了估计故障率，该科学家测试了 $n$ 个独立电缆段的随机样本，测量了它们的寿命 $X_1, X_2, \\ldots, X_n$。\n\n基于样本平均寿命 $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$，提出了一个 $\\lambda$ 的估计量。该估计量为 $\\hat{\\lambda}_n = \\frac{1}{\\bar{X}_n}$。\n\n科学家希望证明这是一个好的估计量，即随着样本量 $n$ 的增加，它会变得更加准确。这个性质被称为相合性。如果一个估计量 $\\hat{\\theta}_n$ 在 $n \\to \\infty$ 时依概率收敛于参数 $\\theta$，那么它被称为 $\\theta$ 的一个相合估计量。\n\n下列哪个陈述提供了正确的推理来证明 $\\hat{\\lambda}_n = 1/\\bar{X}_n$ 是 $\\lambda$ 的一个相合估计量？\n\nA. 该估计量是相合的，因为中心极限定理指出，当 $n$ 很大时，$\\hat{\\lambda}_n$ 的抽样分布近似为均值为 $\\lambda$ 的正态分布。\n\nB. 该估计量是相合的，因为大数定律指出，样本均值 $\\bar{X}_n$ 依概率收敛于真实的故障率 $\\lambda$，因此 $\\hat{\\lambda}_n = 1/\\bar{X}_n$ 必定依概率收敛于 $1/\\lambda$。\n\nC. 该估计量是相合的，因为可以证明它是 $\\lambda$ 的一个无偏估计量，即对于任何样本量 $n$，其期望值 $E[\\hat{\\lambda}_n]$ 都精确地等于 $\\lambda$。\n\nD. 该估计量是相合的，因为可以证明其方差 $\\text{Var}(\\hat{\\lambda}_n)$ 随着样本量 $n$ 趋近于无穷大而趋近于零。虽然这个事实是正确的，但它本身并不是相合性的一个完整证明。\n\nE. 该估计量是相合的，因为大数定律确保了样本均值 $\\bar{X}_n$ 依概率收敛于真实的平均寿命，即 $1/\\lambda$。由于函数 $g(y) = 1/y$ 在 $y \\neq 0$ 时是连续的，根据连续映射定理，可以保证 $\\hat{\\lambda}_n = g(\\bar{X}_n)$ 依概率收敛于 $g(1/\\lambda) = \\lambda$。", "solution": "要确定估计量 $\\hat{\\lambda}_n = 1/\\bar{X}_n$ 相合性的正确理由，我们必须遵循相合性的形式化定义，并应用概率论中的相关定理。\n\n如果一个估计量 $\\hat{\\theta}_n$ 随着样本量 $n$ 趋近于无穷大而依概率收敛于参数 $\\theta$，那么它就是 $\\theta$ 的一个相合估计量。我们记作 $\\hat{\\theta}_n \\xrightarrow{p} \\theta$。我们需要证明 $\\hat{\\lambda}_n \\xrightarrow{p} \\lambda$。\n\n首先，我们来确定随机样本 $X_1, X_2, \\ldots, X_n$ 的性质。这些变量是独立同分布的（i.i.d.），服从率为 $\\lambda$ 的指数分布。一个率为 $\\lambda$ 的指数分布随机变量的期望值（或均值）是 $E[X] = 1/\\lambda$。方差是 $\\text{Var}(X) = 1/\\lambda^2$。由于率 $\\lambda  0$，均值 $1/\\lambda$ 是有限的。\n\n证明相合性的核心论证通常涉及大数定律。弱大数定律 (WLLN) 指出，对于一列具有有限均值 $\\mu$ 的独立同分布随机变量，样本均值 $\\bar{X}_n$ 依概率收敛于 $\\mu$。\n在我们的例子中，$\\mu = E[X] = 1/\\lambda$。因此，根据弱大数定律：\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow{p} E[X] = \\frac{1}{\\lambda} \\quad \\text{当 } n \\to \\infty $$\n\n我们的估计量不是 $\\bar{X}_n$，而是它的一个函数：$\\hat{\\lambda}_n = 1/\\bar{X}_n$。为了处理随机变量函数的收敛问题，我们使用连续映射定理 (CMT)。连续映射定理指出，如果一个随机变量序列 $Y_n$ 依概率收敛于一个常数 $c$（即 $Y_n \\xrightarrow{p} c$），并且函数 $g$ 在点 $c$ 处是连续的，那么变换后的随机变量序列 $g(Y_n)$ 依概率收敛于 $g(c)$（即 $g(Y_n) \\xrightarrow{p} g(c)$）。\n\n在这个问题中，我们的随机变量序列是 $Y_n = \\bar{X}_n$，它依概率收敛于常数 $c = 1/\\lambda$。我们的函数是 $g(y) = 1/y$。这个函数在所有 $y \\neq 0$ 的点上都是连续的。由于 $\\lambda  0$，常数 $c = 1/\\lambda$ 也大于0，因此函数 $g(y)$ 在 $c=1/\\lambda$ 处是连续的。\n\n应用连续映射定理：\n$$ \\hat{\\lambda}_n = g(\\bar{X}_n) \\xrightarrow{p} g\\left(\\frac{1}{\\lambda}\\right) $$\n现在，我们计算 $g(1/\\lambda)$：\n$$ g\\left(\\frac{1}{\\lambda}\\right) = \\frac{1}{(1/\\lambda)} = \\lambda $$\n因此，我们证明了 $\\hat{\\lambda}_n \\xrightarrow{p} \\lambda$，这就是相合性的定义。\n\n这一推理过程与选项E完全匹配。\n\n让我们评估一下为什么其他选项不正确：\n- **A：** 中心极限定理描述的是依*分布*收敛（收敛于正态分布），而不是依*概率*收敛（收敛于一个常数）。相合性是由依概率收敛定义的。这个推理是有缺陷的。\n- **B：** 这个陈述错误地声称 $\\bar{X}_n$ 收敛于 $\\lambda$。弱大数定律指出 $\\bar{X}_n$ 收敛于分布的均值，即 $E[X] = 1/\\lambda$，而不是 $\\lambda$。这导致了 $\\hat{\\lambda}_n$ 收敛于 $1/\\lambda$ 的矛盾结论。\n- **C：** 这个陈述声称该估计量是无偏的。无偏性（$E[\\hat{\\theta}_n] = \\theta$）与相合性是不同的性质。虽然一些无偏估计量是相合的，但这并非普遍规律，一个性质不能自动推导出另一个。此外，对于指数分布，$E[\\hat{\\lambda}_n] = E[1/\\bar{X}_n] \\neq \\lambda$（根据 Jensen 不等式，由于 $1/y$ 是一个凸函数，$E[1/\\bar{X}_n]  1/E[\\bar{X}_n] = \\lambda$），所以这个陈述的前提是错误的。\n- **D：** 这个陈述提到 $\\text{Var}(\\hat{\\lambda}_n) \\to 0$。相合性的一个充分条件是估计量的偏差和方差都随着 $n \\to \\infty$ 而趋于零。仅说明方差趋于零是不完整的。还必须证明偏差 $B(\\hat{\\lambda}_n) = E[\\hat{\\lambda}_n] - \\lambda$ 也趋于零。虽然对于这个估计量来说，这两个条件确实都成立，但声称方差趋于零*本身就是一个完整的证明*是错误的。选项E中的推理更直接、更根本。\n\n因此，在所有选项中，唯一正确且完整的理由是E。", "answer": "$$\\boxed{E}$$", "id": "1909316"}]}