{"hands_on_practices": [{"introduction": "最大似然估计的核心思想是：选择一个参数值，使得我们观察到的数据出现的概率（或“似然”）最大化。在最简单的情况下，当参数的可能取值是有限的、离散的几个点时，我们可以直接计算每个参数值下观测数据出现的似然度，然后选出那个使似然度最大的参数作为我们的估计。这个练习 [@problem_id:1933649] 完美地诠释了这一基本原理，通过直接比较三个候选参数的似然值，帮助你建立对最大似然估计最直观的理解，而无需借助复杂的微积分。", "problem": "一位材料科学家正在测试一种新型非易失性存储单元在一年期内的数据保留可靠性。将一个比特存入并在一年后读出的过程被建模为一系列独立的伯努利试验。一次“成功”被定义为该比特被正确读出且未损坏。任何给定试验的成功概率由参数 $\\theta$ 表示。\n\n根据该特定单元设计的量子隧穿数据退化模型，首席工程师假设 $\\theta$ 的真实值被限制在一个离散的可能性集合中：$\\{0.1, 0.5, 0.9\\}$。\n\n为了检验这一假设并估计该参数，对10个相同的存储单元进行了一项实验。一年后，发现其中8个单元正确地保留了它们的数据（即观察到8次成功）。\n\n给定这一实验结果，参数 $\\theta$ 的最大似然估计 (MLE) 是什么？", "solution": "我们将结果建模为二项式似然。对于单个参数值 $\\theta$，在 $n=10$ 次试验和 $k=8$ 次成功的情况下，似然函数为\n$$\nL(\\theta)\\propto \\theta^{k}(1-\\theta)^{n-k}=\\theta^{8}(1-\\theta)^{2},\n$$\n其中，比例关系忽略了所有 $\\theta$ 共有的二项式系数 $\\binom{10}{8}$。\n\n候选集合为 $\\{0.1,0.5,0.9\\}$。计算未归一化的似然值：\n- 对于 $\\theta=0.1$：\n$$\nL(0.1)\\propto (0.1)^{8}(0.9)^{2}=\\left(\\frac{1}{10}\\right)^{8}\\left(\\frac{9}{10}\\right)^{2}=\\frac{9^{2}}{10^{10}}.\n$$\n- 对于 $\\theta=0.5$：\n$$\nL(0.5)\\propto (0.5)^{8}(0.5)^{2}=\\left(\\frac{1}{2}\\right)^{10}=\\frac{1}{2^{10}}.\n$$\n- 对于 $\\theta=0.9$：\n$$\nL(0.9)\\propto (0.9)^{8}(0.1)^{2}=\\left(\\frac{9}{10}\\right)^{8}\\left(\\frac{1}{10}\\right)^{2}=\\frac{9^{8}}{10^{10}}.\n$$\n\n比较 $L(0.9)$ 和 $L(0.1)$：\n$$\n\\frac{L(0.9)}{L(0.1)}=\\frac{9^{8}/10^{10}}{9^{2}/10^{10}}=9^{6}1,\n$$\n所以 $L(0.9)L(0.1)$。\n\n比较 $L(0.9)$ 和 $L(0.5)$：\n$$\n\\frac{L(0.9)}{L(0.5)}=\\frac{9^{8}/10^{10}}{1/2^{10}}=\\frac{9^{8}2^{10}}{10^{10}}=\\frac{9^{8}}{5^{10}}=\\left(\\frac{9^{4}}{5^{5}}\\right)^{2}.\n$$\n因为 $9^{4}=6561$ 且 $5^{5}=3125$，我们有 $9^{4}5^{5}$，因此 $\\left(\\frac{9^{4}}{5^{5}}\\right)^{2}1$，所以 $L(0.9)L(0.5)$。\n\n因此，$L(0.9)$ 超过了所有其他候选值的似然值，最大似然估计为 $\\hat{\\theta}=0.9$。", "answer": "$$\\boxed{0.9}$$", "id": "1933649"}, {"introduction": "当参数的取值范围是连续的（例如，所有正实数）时，我们无法像上一个练习那样逐一检验。在这种更常见的情形下，微积分成为了我们寻找最大值的强大工具。通常，为了简化计算，我们会对似然函数取对数，形成“对数似然函数”，然后通过求导并令其为零来找到最大值点。这个练习 [@problem_id:1933625] 演示了求解最大似然估计最标准、最通用的方法，它将引导你完成构建似然函数、取对数、求导和求解这一整套“标准操作”，这是统计推断中的一项基本技能。", "problem": "在通信理论中，窄带高斯噪声信号的包络通常使用瑞利分布进行建模。一位工程师正在研究一组信号幅度的独立测量值，共 $n$ 个，记为 $x_1, x_2, \\dots, x_n$。这些测量值被假定为来自瑞利分布的一个随机样本，其概率密度函数 (PDF) 如下所示：\n$$f(x; \\sigma) = \\frac{x}{\\sigma^2} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$$\n其中 $x \\ge 0$ 且 $\\sigma  0$。参数 $\\sigma$ 是一个与噪声功率相关的未知尺度参数。\n\n请根据样本数据 $x_1, x_2, \\dots, x_n$，确定参数 $\\sigma$ 的最大似然估计量 (MLE)，记为 $\\hat{\\sigma}$。", "solution": "目标是求出瑞利分布参数 $\\sigma$ 的最大似然估计量 (MLE)。设随机样本为 $X_1, X_2, \\dots, X_n$，其观测值为 $x_1, x_2, \\dots, x_n$。\n\n首先，我们构造似然函数 $L(\\sigma)$，它是样本的联合概率密度函数。由于观测值是独立同分布的，似然函数是各单个概率密度函数的乘积：\n$$L(\\sigma | x_1, \\dots, x_n) = \\prod_{i=1}^{n} f(x_i; \\sigma) = \\prod_{i=1}^{n} \\frac{x_i}{\\sigma^2} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right)$$\n\n为简化最大化过程，我们转而处理似然函数的自然对数，即对数似然函数 $\\ell(\\sigma) = \\ln(L(\\sigma))$。取对数可将乘积转化为求和：\n$$\\ell(\\sigma) = \\ln\\left( \\prod_{i=1}^{n} \\frac{x_i}{\\sigma^2} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right) \\right)$$\n利用对数性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(a/b) = \\ln(a) - \\ln(b)$，我们得到：\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\ln\\left( \\frac{x_i}{\\sigma^2} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right) \\right)$$\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\left[ \\ln(x_i) - \\ln(\\sigma^2) + \\ln\\left(\\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right)\\right) \\right]$$\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\left[ \\ln(x_i) - 2\\ln(\\sigma) - \\frac{x_i^2}{2\\sigma^2} \\right]$$\n我们可以将求和项分开：\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\ln(x_i) - \\sum_{i=1}^{n} 2\\ln(\\sigma) - \\sum_{i=1}^{n} \\frac{x_i^2}{2\\sigma^2}$$\n第二项与索引 $i$ 无关，因此变为 $2n\\ln(\\sigma)$。第三项可写为：\n$$\\ell(\\sigma) = \\sum_{i=1}^{n} \\ln(x_i) - 2n\\ln(\\sigma) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} x_i^2$$\n\n为了找到使 $\\ell(\\sigma)$ 最大化的 $\\sigma$ 值，我们对 $\\ell(\\sigma)$ 关于 $\\sigma$ 求导，并令其等于零。\n$$\\frac{d\\ell}{d\\sigma} = \\frac{d}{d\\sigma} \\left( \\sum_{i=1}^{n} \\ln(x_i) - 2n\\ln(\\sigma) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} x_i^2 \\right)$$\n第一项相对于 $\\sigma$ 是常数，因此其导数为零。对于第二项，$\\frac{d}{d\\sigma}(-2n\\ln(\\sigma)) = -\\frac{2n}{\\sigma}$。对于第三项，我们对 $\\sigma^{-2}$ 使用幂法则：\n$$\\frac{d}{d\\sigma} \\left(-\\frac{1}{2}\\sigma^{-2} \\sum_{i=1}^{n} x_i^2\\right) = -\\frac{1}{2} (-2\\sigma^{-3}) \\sum_{i=1}^{n} x_i^2 = \\frac{1}{\\sigma^3}\\sum_{i=1}^{n} x_i^2$$\n将这些项合并，我们得到完整的导数：\n$$\\frac{d\\ell}{d\\sigma} = -\\frac{2n}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^{n} x_i^2$$\n\n现在，我们将导数设为零，求解 $\\sigma$。解即为最大似然估计量，我们记为 $\\hat{\\sigma}$。\n$$-\\frac{2n}{\\hat{\\sigma}} + \\frac{1}{\\hat{\\sigma}^3} \\sum_{i=1}^{n} x_i^2 = 0$$\n$$\\frac{1}{\\hat{\\sigma}^3} \\sum_{i=1}^{n} x_i^2 = \\frac{2n}{\\hat{\\sigma}}$$\n假设 $\\hat{\\sigma} \\ne 0$，我们可以在等式两边同乘以 $\\hat{\\sigma}^3$：\n$$\\sum_{i=1}^{n} x_i^2 = 2n\\hat{\\sigma}^2$$\n求解 $\\hat{\\sigma}^2$：\n$$\\hat{\\sigma}^2 = \\frac{1}{2n} \\sum_{i=1}^{n} x_i^2$$\n由于参数 $\\sigma$ 必须为正，我们取正平方根：\n$$\\hat{\\sigma} = \\sqrt{\\frac{1}{2n} \\sum_{i=1}^{n} x_i^2}$$\n为了确认这是一个最大值，我们可以检查二阶导数，此时二阶导数应为负值。但对于这个标准问题，我们可以确信这就是最大似然估计量。MLE $\\hat{\\sigma}$ 的表达式即为最终答案。", "answer": "$$\\boxed{\\sqrt{\\frac{1}{2n} \\sum_{i=1}^{n} x_i^2}}$$", "id": "1933625"}, {"introduction": "找到一个估计量只是统计分析的第一步，更重要的是理解它的性质，例如它是否存在系统性的偏差（bias）。一个理想的估计量应该在多次重复实验的平均下接近真实的参数值；如果它系统地偏高或偏低，我们就称它是有偏的。这个练习 [@problem_id:2402432] 探讨了一个在实际数据分析中非常重要的警示性案例：数据筛选过程可能会无意中引入偏差。它要求你精确地计算出由于排除了观测值为零的样本而导致估计量产生的偏差，这能加深你对估计量性质以及数据处理过程如何影响统计推断结果的理解。", "problem": "在一项用于量化单个基因座上稀有等位基因的靶向扩增子测序分析中，每个样本产生恰好 $n$ 个独立读数。每个读数报告等位基因为突变型的概率为 $p$，为野生型的概率为 $1-p$，且各个读数之间相互独立。令 $X$ 表示样本中突变读数的数量，因此 $X \\sim \\mathrm{Binomial}(n,p)$。作为质量控制筛选的一部分，生物信息学流程会丢弃任何 $X=0$（即未观察到突变读数）的样本，只保留 $X0$ 的样本。然后，分析师对保留的样本使用标准二项似然（忽略筛选过程）拟合一个二项模型，并报告从单个保留样本中得到的 $p$ 的最大似然估计量（MLE）$\\hat{p}=X/n$。\n\n将整个过程（数据生成和筛选）视为数据生成机制，并视 $p \\in (0,1)$ 和 $n \\in \\{1,2,3,\\dots\\}$ 为固定值。仅使用第一性原理和定义，推导在该真实数据生成机制下，此报告估计量的偏差的解析表达式，即：\n$$\n\\mathrm{Bias}(\\hat{p}) \\equiv \\mathbb{E}[\\hat{p}]-p,\n$$\n其中，期望是相对于 $X$ 在给定 $X0$ 条件下的条件分布计算的。请以 $p$ 和 $n$ 的封闭形式表达式给出最终答案。请勿进行近似计算。您的最终答案必须是单一表达式，不含单位。", "solution": "该问题要求推导比例参数 $p$ 的一个特定估计量的偏差。问题陈述经过严格验证，被认为是科学上合理、问题定义清晰、客观且自洽的。因此，有必要提供一个解。\n\n突变等位基因概率 $p$ 的估计量为 $\\hat{p} = \\frac{X}{n}$，其中 $X$ 是总共 $n$ 个读数中的突变读数数量。数据生成过程包含一个筛选步骤，会丢弃任何 $X=0$ 的样本。因此，分析仅在 $X0$ 的样本上进行。\n\n估计量 $\\hat{p}$ 的偏差定义为：\n$$\n\\mathrm{Bias}(\\hat{p}) \\equiv \\mathbb{E}[\\hat{p}] - p\n$$\n期望 $\\mathbb{E}[\\cdot]$ 必须是关于实际观测到的数据的分布来计算。由于筛选步骤的存在，这个分布是 $X$ 在给定 $X  0$ 条件下的条件分布。\n\n首先，我们表示估计量 $\\hat{p}$ 的条件期望：\n$$\n\\mathbb{E}[\\hat{p} | X0] = \\mathbb{E}\\left[\\frac{X}{n} \\Big| X0\\right]\n$$\n根据期望的线性性质，我们可以写出：\n$$\n\\mathbb{E}[\\hat{p} | X0] = \\frac{1}{n} \\mathbb{E}[X | X0]\n$$\n我们的主要任务是计算条件期望 $\\mathbb{E}[X | X0]$。随机变量 $X$ 服从二项分布，$X \\sim \\mathrm{Binomial}(n,p)$，其概率质量函数（PMF）为：\n$$\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots, n\\}\n$$\n对于离散随机变量，条件期望的定义是：\n$$\n\\mathbb{E}[X | X0] = \\sum_{k} k \\cdot P(X=k | X0)\n$$\n求和是对给定条件下 $X$ 的所有可能值进行的。由于 $X0$， $k$ 的可能值为 $\\{1, 2, \\dots, n\\}$。\n根据条件概率的定义，对于任何 $k  0$：\n$$\nP(X=k | X0) = \\frac{P(X=k \\text{ and } X0)}{P(X0)}\n$$\n由于对于 $k \\ge 1$ 的事件 $\\{X=k\\}$ 是事件 $\\{X0\\}$ 的一个子集，它们的交集就是 $\\{X=k\\}$。因此：\n$$\nP(X=k | X0) = \\frac{P(X=k)}{P(X0)}\n$$\n条件事件的概率 $P(X0)$ 是观察到零个突变读数概率的补集：\n$$\nP(X0) = 1 - P(X=0)\n$$\n对于二项分布，零次成功的概率是：\n$$\nP(X=0) = \\binom{n}{0} p^0 (1-p)^{n-0} = (1-p)^n\n$$\n因此，条件事件的概率为：\n$$\nP(X0) = 1 - (1-p)^n\n$$\n现在我们可以写出条件期望的表达式：\n$$\n\\mathbb{E}[X | X0] = \\sum_{k=1}^{n} k \\cdot \\frac{P(X=k)}{P(X0)} = \\frac{1}{1 - (1-p)^n} \\sum_{k=1}^{n} k \\cdot P(X=k)\n$$\n求和项 $\\sum_{k=1}^{n} k \\cdot P(X=k)$ 几乎是 $X$ 的无条件期望 $\\mathbb{E}[X] = \\sum_{k=0}^{n} k \\cdot P(X=k)$ 的定义。在无条件求和中，当 $k=0$ 时的项是 $0 \\cdot P(X=0) = 0$，所以它对求和没有贡献。因此，从 $k=1$ 到 $n$ 的求和与从 $k=0$ 到 $n$ 的求和是相同的：\n$$\n\\sum_{k=1}^{n} k \\cdot P(X=k) = \\sum_{k=0}^{n} k \\cdot P(X=k) = \\mathbb{E}[X]\n$$\n众所周知，二项随机变量 $X \\sim \\mathrm{Binomial}(n,p)$ 的无条件期望是 $\\mathbb{E}[X] = np$。\n将此结果代入我们的条件期望表达式中：\n$$\n\\mathbb{E}[X | X0] = \\frac{np}{1 - (1-p)^n}\n$$\n现在我们可以计算我们的估计量 $\\hat{p}$ 的条件期望：\n$$\n\\mathbb{E}[\\hat{p} | X0] = \\frac{1}{n} \\mathbb{E}[X | X0] = \\frac{1}{n} \\left(\\frac{np}{1 - (1-p)^n}\\right) = \\frac{p}{1 - (1-p)^n}\n$$\n最后，我们可以通过将此结果代入偏差的定义来计算偏差：\n$$\n\\mathrm{Bias}(\\hat{p}) = \\mathbb{E}[\\hat{p} | X0] - p = \\frac{p}{1 - (1-p)^n} - p\n$$\n为了简化这个表达式，我们找到一个公分母：\n$$\n\\mathrm{Bias}(\\hat{p}) = \\frac{p - p \\left(1 - (1-p)^n\\right)}{1 - (1-p)^n} = \\frac{p - p + p(1-p)^n}{1 - (1-p)^n}\n$$\n这可以简化为偏差的最终解析表达式：\n$$\n\\mathrm{Bias}(\\hat{p}) = \\frac{p(1-p)^n}{1 - (1-p)^n}\n$$\n该表达式是在指定的数据生成和筛选机制下，估计量 $\\hat{p}$ 的偏差的封闭形式表示，用参数 $p$ 和 $n$ 表示。由于 $p \\in (0,1)$，分子和分母都为正，这表明存在正偏差，这与筛选掉最低可能结果（$X=0$）的预期相符。", "answer": "$$\n\\boxed{\\frac{p(1-p)^n}{1 - (1-p)^n}}\n$$", "id": "2402432"}]}