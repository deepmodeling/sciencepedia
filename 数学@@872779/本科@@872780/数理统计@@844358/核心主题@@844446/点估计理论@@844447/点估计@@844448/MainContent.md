## 引言
在统计科学的核心，我们面临一个永恒的挑战：如何利用有限的样本数据，来推断整个群体的未知特征？当我们试图为某个群体参数——例如，一种新药的平均疗效、一个电子元件的平均寿命，或某一物种的真实数量——给出一个单一的“最佳猜测值”时，我们就进入了点估计的领域。点估计是[统计推断](@entry_id:172747)的基石，它将抽象的数据转化为具体、可操作的数值结论。但是，我们如何定义一个估计的“好坏”？又有哪些系统性的方法可以帮助我们从数据中找到这个最佳猜测值？

本文旨在系统地回答这些问题，为读者构建一个关于点估计的完整知识框架。我们将分三个章节展开：
首先，在“原理与机制”中，我们将深入探讨评估估计量优良性的四大核心标准：无偏性、效率、[均方误差](@entry_id:175403)和相合性。同时，我们将详细介绍两种最重要、最经典的估计量构建方法——[矩估计法](@entry_id:270941)（MoM）和极大似然估计法（MLE），并进一步探索寻找理论上“最优”估计量的深刻思想，如充分统计量和[一致最小方差无偏估计量](@entry_id:166888)（[UMVUE](@entry_id:169429)）。

接着，在“应用与跨学科联系”中，我们将理论联系实际，展示点估计如何在工程学、生命科学、[公共卫生](@entry_id:273864)和物理学等多元领域中解决具体问题。您将看到这些原理如何被应用于处理[删失数据](@entry_id:173222)、修正测量误差以及在复杂的贝叶斯框架下进行推断。

最后，“动手实践”部分提供了一系列精心挑选的练习，旨在通过解决具体问题来巩固您对核心概念的理解，从推导矩估计量到应用[Lehmann–Scheffé定理](@entry_id:176171)寻找[UMVUE](@entry_id:169429)。

通过这段学习旅程，您将不仅掌握点估计的数学工具，更将理解其背后的统计思想，从而能够自信地在未来的学术研究和实际工作中应用这些强大的方法。

## 原理与机制

在统计推断领域，我们常常面对一项核心任务：利用从群体中抽取的样本数据，来猜测该群体某个未知的数值特征。这个未知的数值特征被称为**参数**（parameter），而我们用来猜测它的、基于样本数据计算出的函数，则被称为**估计量**（estimator）。点估计（point estimation）的目标就是为未知参数提供一个最佳的单一猜测值。本章将深入探讨点估计的基本原理、评估估计量优良性的标准，以及构建估计量的常用方法。

### 评估估计量的标准

一个合理的估计量应该“接近”它所估计的真实参数。但是，“接近”的含义是什么？统计学家通过一系列数学性质来精确刻画估计量的优劣。

#### 无偏性 (Unbiasedness)

最直观的一个优良性质是，一个好的估计量在平均意义上应该等于它所要估计的参数。换言之，如果我们能够进行无数次[重复抽样](@entry_id:274194)，每次都计算出一个估计值，那么这些估计值的平均数应该恰好是真实的参数值。满足此性质的估计量被称为**[无偏估计量](@entry_id:756290)**。

形式上，如果一个参数 $\theta$ 的估计量 $\hat{\theta}$ 的[期望值](@entry_id:153208)等于 $\theta$，即 $E[\hat{\theta}] = \theta$，那么 $\hat{\theta}$ 就是一个[无偏估计量](@entry_id:756290)。反之，**偏差**（bias）被定义为 $B(\hat{\theta}) = E[\hat{\theta}] - \theta$。

一个经典的[无偏估计量](@entry_id:756290)是样本均值 $\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i$，它被用来估计[总体均值](@entry_id:175446) $\mu$。由于[期望的线性](@entry_id:273513)性质，$E[\bar{X}] = E[\frac{1}{n}\sum X_i] = \frac{1}{n}\sum E[X_i] = \frac{1}{n} \sum \mu = \mu$，所以样本均值是[总体均值](@entry_id:175446)的[无偏估计量](@entry_id:756290)。

然而，并非所有直观的估计量都具有无偏性。考虑一个在[半导体制造](@entry_id:159349)中遇到的场景：为了监控硅晶圆层的厚度，工程师们收集了一组随机样本 $\{X_1, \dots, X_n\}$，其总体[方差](@entry_id:200758)为未知的 $\sigma^2$。一个很自然的[方差估计](@entry_id:268607)量是样本点与其均值 $\bar{X}$ 的离差平方和的平均值，即 $\hat{\sigma}^2_{\text{intuitive}} = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$。通过计算其期望可以发现，$E[\hat{\sigma}^2_{\text{intuitive}}] = \frac{n-1}{n}\sigma^2$ ([@problem_id:1944322])。这个结果表明，该直观估计量系统性地低估了真实的[方差](@entry_id:200758) $\sigma^2$，因此它是一个**有偏估计量**，其偏差为 $E[\hat{\sigma}^2_{\text{intuitive}}] - \sigma^2 = -\frac{1}{n}\sigma^2$。为了纠正这个偏差，我们引入了**样本[方差](@entry_id:200758)**（sample variance），其定义为 $S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2$。不难证明，$E[S^2] = \sigma^2$，因此 $S^2$ 是总体[方差](@entry_id:200758) $\sigma^2$ 的[无偏估计量](@entry_id:756290)。分母从 $n$ 变为 $n-1$ 的修正，被称为**[贝塞尔校正](@entry_id:169538)**（Bessel's correction）。

在某些情况下，有偏估计量可以通过一个简单的修正因子变为无偏的。例如，假设我们有一系列来自[均匀分布](@entry_id:194597) $U(0, \theta)$ 的独立测量值 $X_1, \dots, X_n$，其中 $\theta$ 是未知的上限。一个自然的估计量是样本中的最大值 $X_{(n)} = \max\{X_1, \dots, X_n\}$。通过计算可以得到其[期望值](@entry_id:153208)为 $E[X_{(n)}] = \frac{n}{n+1}\theta$ ([@problem_id:1944385])。显然，$X_{(n)}$ 是一个有偏估计量。然而，我们可以构造一个新的估计量 $\hat{\theta}_{\text{corrected}} = \frac{n+1}{n} X_{(n)}$，它的期望为 $E[\hat{\theta}_{\text{corrected}}] = \frac{n+1}{n} E[X_{(n)}] = \frac{n+1}{n} \cdot \frac{n}{n+1}\theta = \theta$。因此，通过乘以一个修正因子 $\frac{n+1}{n}$，我们得到了一个 $\theta$ 的[无偏估计量](@entry_id:756290)。

#### 均方误差 (Mean Squared Error)

无偏性固然是一个理想的性质，但它并非评估估计量的唯一标准。一个[无偏估计量](@entry_id:756290)可能围绕真实参数值剧烈波动，导致任何单次估计的结果都可能离真实值很远。我们需要一个同时考虑偏差和波动性的综合指标，这就是**[均方误差](@entry_id:175403)**（Mean Squared Error, MSE）。

参数 $\theta$ 的估计量 $\hat{\theta}$ 的均方误差定义为：
$$ \mathrm{MSE}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] $$
它度量了估计值与真实参数值之间平方误差的平均大小。一个非常重要的恒等式将 MSE 分解为[方差](@entry_id:200758)和偏差的平方和：
$$ \mathrm{MSE}(\hat{\theta}) = \mathrm{Var}(\hat{\theta}) + [B(\hat{\theta})]^2 $$
其中 $\mathrm{Var}(\hat{\theta})$ 是[估计量的方差](@entry_id:167223)，$B(\hat{\theta})$ 是其偏差。这个分解揭示了估计误差的两个来源：由估计量自身波动性引起的[方差](@entry_id:200758)，以及由其系统性偏离真实值引起的偏差。对于[无偏估计量](@entry_id:756290)，$B(\hat{\theta}) = 0$，其 MSE 就等于它的[方差](@entry_id:200758)。

以天文学家估计遥远天体亮度 $\mu$ 为例 ([@problem_id:1944368])，使用样本均值 $\bar{X}$ 作为估计量。我们已知 $\bar{X}$ 是无偏的，其[方差](@entry_id:200758)为 $\mathrm{Var}(\bar{X}) = \frac{\sigma^2}{n}$，其中 $\sigma^2$ 是单次测量的[方差](@entry_id:200758)。因此，$\bar{X}$ 的[均方误差](@entry_id:175403)为：
$$ \mathrm{MSE}(\bar{X}) = \mathrm{Var}(\bar{X}) + 0^2 = \frac{\sigma^2}{n} $$
这个结果表明，随着样本量 $n$ 的增加，$\bar{X}$ 的[均方误差](@entry_id:175403)趋于零，这意味着它越来越精确。

#### 效率 (Efficiency)

在所有[无偏估计量](@entry_id:756290)中，我们自然偏爱[方差](@entry_id:200758)最小的那个，因为它最稳定。**效率**（efficiency）这个概念就是用来比较不同无偏[估计量的[方](@entry_id:167223)差](@entry_id:200758)。一个无偏[估计量的[方](@entry_id:167223)差](@entry_id:200758)越小，我们就说它越有效。

那么，一个无偏[估计量的[方](@entry_id:167223)差](@entry_id:200758)可以小到什么程度呢？**克拉美-罗下界**（Cramér-Rao Lower Bound, CRLB）给出了这个问题的答案。它为任何无偏[估计量的[方](@entry_id:167223)差](@entry_id:200758)设定了一个理论上的最小值。这个下界与一个叫做**费雪信息**（Fisher Information）的量有关，记作 $I(\theta)$。费雪信息 $I_n(\theta)$ 度量了来自 $n$ 个样本的观测数据中包含的关于参数 $\theta$ 的信息量。CRLB 表明，对于任何 $\theta$ 的[无偏估计量](@entry_id:756290) $\hat{\theta}$，其[方差](@entry_id:200758)满足：
$$ \mathrm{Var}(\hat{\theta}) \ge \frac{1}{I_n(\theta)} $$
如果一个无偏[估计量的[方](@entry_id:167223)差](@entry_id:200758)恰好达到了克拉美-罗下界，我们就称它为**[有效估计量](@entry_id:271983)**（efficient estimator）。

考虑一个[量子计算](@entry_id:142712)实验，其中每次操作的成功概率为 $p$ ([@problem_id:1944324])。这可以被建模为来自[伯努利分布](@entry_id:266933)的一系列观测。对于 $n$ 次独立试验，其参数 $p$ 的任何无偏[估计量的[方](@entry_id:167223)差](@entry_id:200758)的 CRLB 可以被计算为 $\frac{p(1-p)}{n}$。我们知道，样本均值 $\hat{p} = \bar{X}$ 是 $p$ 的一个[无偏估计量](@entry_id:756290)，其[方差](@entry_id:200758)恰好是 $\mathrm{Var}(\hat{p}) = \frac{p(1-p)}{n}$。由于它的[方差](@entry_id:200758)达到了理论下限，因此样本均值是[伯努利分布](@entry_id:266933)成功概率 $p$ 的一个[有效估计量](@entry_id:271983)。

#### 相合性 (Consistency)

无偏性、MSE 和效率都是在固定样本量 $n$ 的情况下的性质。然而，我们同样关心当收集到越来越多数据时，估计量的表现会如何。一个理想的估计量应该随着样本量的增加而越来越接近真实的参数值。这个[渐近性质](@entry_id:177569)被称为**相合性**（consistency），或一致性。

形式上，如果一个估计量 $\hat{\theta}_n$（下标 $n$ 表示它基于大小为 $n$ 的样本）随着 $n \to \infty$ [依概率收敛](@entry_id:145927)于 $\theta$，那么它就是相合的。这意味着对于任意小的正数 $\epsilon$，$\hat{\theta}_n$ 与 $\theta$ 的差距大于 $\epsilon$ 的概率会随着 $n$ 的增大而趋向于 0：
$$ \lim_{n \to \infty} P(|\hat{\theta}_n - \theta| \ge \epsilon) = 0 $$
证明一个估计量是否相合，一个有力的工具是**[切比雪夫不等式](@entry_id:269182)**。该不等式指出，对于任何期望为 $\mu$、[方差](@entry_id:200758)为 $\sigma^2$ 的[随机变量](@entry_id:195330) $Y$，有 $P(|Y - \mu| \ge \epsilon) \le \frac{\sigma^2}{\epsilon^2}$。

我们可以利用这个不等式来证明样本均值 $\bar{X}$ 是[总体均值](@entry_id:175446) $\mu$ 的[相合估计量](@entry_id:266642) ([@problem_id:1944351])。我们已经知道 $E[\bar{X}] = \mu$ 且 $\mathrm{Var}(\bar{X}) = \frac{\sigma^2}{n}$。将 $\bar{X}$ 代入[切比雪夫不等式](@entry_id:269182)，我们得到：
$$ P(|\bar{X} - \mu| \ge \epsilon) \le \frac{\mathrm{Var}(\bar{X})}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} $$
当 $n \to \infty$ 时，上式的右侧趋向于 0。因此，$\lim_{n \to \infty} P(|\bar{X} - \mu| \ge \epsilon) = 0$，这正是相合性的定义。这个结论，即样本均值[依概率收敛](@entry_id:145927)于[总体均值](@entry_id:175446)，也被称为**[弱大数定律](@entry_id:159016)**。

### 构建估计量的方法

我们已经讨论了如何评价一个估计量，但我们如何从零开始找到一个估计量呢？以下是两种最基本和最重要的方法。

#### [矩估计法](@entry_id:270941) (Method of Moments)

**[矩估计法](@entry_id:270941)**（Method of Moments, MoM）是一种构造估计量的直观方法，其思想非常朴素：用样本矩来估计[总体矩](@entry_id:170482)。具体来说，该方法通过建立一个[方程组](@entry_id:193238)，使总体的理论矩等于对应的样本矩，然后求解该[方程组](@entry_id:193238)得到参数的估计。

总体的 $k$ 阶[原点矩](@entry_id:165197)定义为 $\mu'_k = E[X^k]$，而样本的 $k$ 阶[原点矩](@entry_id:165197)为 $m_k = \frac{1}{n}\sum_{i=1}^n X_i^k$。[矩估计法](@entry_id:270941)的步骤是：
1.  计算前 $k$ 个[总体矩](@entry_id:170482)（$k$ 是待估参数的个数），这些矩通常是待估参数的函数。
2.  计算前 $k$ 个样本矩。
3.  令 $\mu'_j = m_j$ 对 $j=1, \dots, k$ 成立，得到一个包含 $k$ 个方程的[方程组](@entry_id:193238)。
4.  求解这个[方程组](@entry_id:193238)，解出的参数即为矩估计量。

例如，在微芯片的质量控制中，首次出现故障所需的天数 $K$ 服从几何分布，其[概率质量函数](@entry_id:265484)为 $P(K=k) = (1-p)^{k-1}p$，其中 $p$ 是未知的每日故障概率 ([@problem_id:1944369])。我们需要估计参数 $p$。该[分布](@entry_id:182848)的理论均值（一阶矩）为 $E[K] = \frac{1}{p}$。样本均值（一阶样本矩）为 $\bar{K} = \frac{1}{n}\sum K_i$。根据[矩估计法](@entry_id:270941)，我们令理论均值等于样本均值：
$$ \frac{1}{p} = \bar{K} $$
求解 $p$，我们得到矩估计量 $\hat{p}_{MoM} = \frac{1}{\bar{K}}$。这个结果非常直观：如果平均需要 $\bar{K}$ 天才能观察到一次故障，那么每天发生故障的概率的估计值就是其倒数。

#### 极大[似然](@entry_id:167119)估计法 (Maximum Likelihood Estimation)

**极大似然估计法**（Maximum Likelihood Estimation, MLE）是[统计推断](@entry_id:172747)中应用最广泛的[参数估计](@entry_id:139349)方法。它的核心思想是：寻找一个参数值，使得我们已经观测到的这组样本数据出现的概率最大。

给定一组独立的观测数据 $\mathbf{x} = (x_1, \dots, x_n)$ 和一个由参数 $\theta$ 决定的[概率模型](@entry_id:265150)，**似然函数**（likelihood function）$L(\theta | \mathbf{x})$ 被定义为在参数值为 $\theta$ 时，观测到数据 $\mathbf{x}$ 的[联合概率](@entry_id:266356)（对于[离散分布](@entry_id:193344)）或[联合概率](@entry_id:266356)密度（对于[连续分布](@entry_id:264735)）。
$$ L(\theta | \mathbf{x}) = \prod_{i=1}^n f(x_i; \theta) $$
极大[似然](@entry_id:167119)估计量 $\hat{\theta}_{MLE}$ 就是使这个[似然函数](@entry_id:141927) $L(\theta | \mathbf{x})$ 达到最大值的 $\theta$ 值。

在实践中，直接最大化 $L(\theta)$ 常常很困难，因为连乘运算在求导时会变得复杂。由于对数函数是单调递增的，最大化 $L(\theta)$ 等价于最大化其对数，即**[对数似然函数](@entry_id:168593)**（log-likelihood function） $\ell(\theta) = \ln L(\theta)$。[对数似然](@entry_id:273783)将乘积转化为加和，大大简化了求导过程。因此，MLE 的标准步骤是：
1.  写出似然函数 $L(\theta)$。
2.  计算[对数似然函数](@entry_id:168593) $\ell(\theta) = \ln L(\theta)$。
3.  对 $\ell(\theta)$ 关于 $\theta$ 求导，并令导数等于 0。
4.  解出 $\theta$，得到的解即为极大似然估计量 $\hat{\theta}_{MLE}$。

让我们看两个例子。首先，回到[量子比特](@entry_id:137928)制备的实验 ([@problem_id:1944372])，每次试验的结果 $X_i$ 服从参数为 $p$ 的[伯努利分布](@entry_id:266933)。其[对数似然函数](@entry_id:168593)为：
$$ \ell(p) = \ln \left( p^{\sum x_i} (1-p)^{n-\sum x_i} \right) = \left(\sum x_i\right) \ln p + \left(n - \sum x_i\right) \ln(1-p) $$
对其求导并设为0，解得 $\hat{p}_{MLE} = \frac{1}{n}\sum x_i = \bar{x}$。这说明，对于[伯努利试验](@entry_id:268355)，成功概率的最佳估计就是观测到的成功频率，这与我们的直觉完全一致。

其次，考虑一个[连续分布](@entry_id:264735)的例子。假设电子元件的寿命服从参数为 $\lambda$ 的指数分布，其概率密度函数为 $f(x; \lambda) = \lambda \exp(-\lambda x)$ ([@problem_id:1944346])。对于一组观测寿命 $x_1, \dots, x_n$，[对数似然函数](@entry_id:168593)为：
$$ \ell(\lambda) = \ln \left( \prod_{i=1}^n \lambda \exp(-\lambda x_i) \right) = n \ln \lambda - \lambda \sum_{i=1}^n x_i $$
同样，求导并令其为0，我们得到 $\frac{n}{\lambda} - \sum x_i = 0$，解出 $\hat{\lambda}_{MLE} = \frac{n}{\sum x_i} = \frac{1}{\bar{x}}$。这个结果也相当直观：[失效率](@entry_id:266388)（rate）的估计值是[平均寿命](@entry_id:195236)的倒数。

极大似然估计量拥有许多优良的性质，例如在温和的条件下它们是相合的、渐近无偏的、[渐近有效](@entry_id:167883)的，这使其成为理论和应用中的首选方法。

### 寻找[最优估计量](@entry_id:176428)

有了构建估计量的方法后，我们自然会问：是否存在一个“最好”的估计量？在[无偏估计量](@entry_id:756290)的框架下，这个“最好”通常指具有一致最小[方差](@entry_id:200758)。

#### [数据压缩](@entry_id:137700)：充分统计量

在处理原始数据时，我们常常希望能够对数据进行“压缩”或“汇总”，同时不损失任何关于未知参数的信息。满足这一性质的统计量（样本的函数）被称为**充分统计量**（sufficient statistic）。换言之，一旦我们知道了充分统计量的值，原始样本数据对于估计参数就不再提供任何额外信息。

**[奈曼-费雪因子分解定理](@entry_id:167279)**（Neyman-Fisher Factorization Theorem）是识别充分统计量的主要工具。该定理指出，一个统计量 $T(\mathbf{X})$ 是参数 $\theta$ 的充分统计量，当且仅当样本的联合概率（或密度）函数可以分解为两部分乘积的形式：
$$ f(\mathbf{x}; \theta) = g(T(\mathbf{x}), \theta) \cdot h(\mathbf{x}) $$
其中，$g$ 函数依赖于数据仅通过 $T(\mathbf{x})$，而 $h$ 函数完全不依赖于参数 $\theta$。

例如，在[高能天体物理学](@entry_id:159925)中，探测器在单位时间内记录到的粒子数 $X_i$ 被建模为[泊松分布](@entry_id:147769) $\text{Poisson}(\lambda)$ ([@problem_id:1944361])。对于一个随机样本 $X_1, \dots, X_n$，其[联合概率质量函数](@entry_id:184238)为：
$$ f(\mathbf{x}; \lambda) = \prod_{i=1}^n \frac{e^{-\lambda}\lambda^{x_i}}{x_i!} = e^{-n\lambda} \lambda^{\sum x_i} \left( \frac{1}{\prod x_i!} \right) $$
我们可以令 $T(\mathbf{x}) = \sum x_i$，$g(T, \lambda) = e^{-n\lambda} \lambda^T$，以及 $h(\mathbf{x}) = 1 / (\prod x_i!)$。由于[联合概率函数](@entry_id:272740)成功地分解为此形式，根据[因子分解定理](@entry_id:749213)，$T = \sum X_i$（即观测到的总事件数）是 $\lambda$ 的一个充分统计量。任何基于 $T$ 的[一对一函数](@entry_id:141802)，如样本均值 $\bar{X} = T/n$，也同样是充分统计量。

#### [一致最小方差无偏估计量](@entry_id:166888) ([UMVUE](@entry_id:169429))

[统计估计](@entry_id:270031)的“圣杯”之一是找到**[一致最小方差无偏估计量](@entry_id:166888)**（Uniformly Minimum-Variance Unbiased Estimator, [UMVUE](@entry_id:169429)）。这是一个在所有可能的参数值 $\theta$ 下，[方差](@entry_id:200758)都小于或等于任何其他[无偏估计量](@entry_id:756290)的估计量。

**拉奥-[布莱克威尔定理](@entry_id:269898)**（Rao-Blackwell Theorem）将充分统计量与[方差](@entry_id:200758)减小联系起来。它指出，如果你有一个任意的[无偏估计量](@entry_id:756290)，并对它关于一个充分统计量取[条件期望](@entry_id:159140)，那么你将得到一个新的[无偏估计量](@entry_id:756290)，其[方差](@entry_id:200758)不会比原来的大。这暗示了最优的[无偏估计量](@entry_id:756290)必然是充分统计量的函数。

**[雷曼-谢费定理](@entry_id:176171)**（Lehmann-Scheffé Theorem）则更进一步。它需要一个更强的概念——**完备性**（completeness）。一个统计量 $T$ 被称为完备的，如果唯一期望恒为零的 $T$ 的函数是零函数本身。[雷曼-谢费定理](@entry_id:176171)表明：如果 $T$ 是一个**完备的充分统计量**，那么任何一个作为 $T$ 的函数的[无偏估计量](@entry_id:756290)，就是唯一（[几乎必然](@entry_id:262518)）的 [UMVUE](@entry_id:169429)。

这个定理非常强大。要寻找某个参数 $\tau(\theta)$ 的 [UMVUE](@entry_id:169429)，我们只需要：
1.  找到一个完备的充分统计量 $T$。
2.  找到一个 $T$ 的函数，记为 $\delta(T)$。
3.  验证 $\delta(T)$ 是 $\tau(\theta)$ 的[无偏估计量](@entry_id:756290)，即 $E[\delta(T)] = \tau(\theta)$。
如果能完成这三步，那么 $\delta(T)$ 就是所求的 [UMVUE](@entry_id:169429)。

让我们看一个精妙的应用。在研究宇宙射线的例子中，假设我们想估计在单个时间间隔内没有探测到任何事件的概率，即 $\tau(\lambda) = P(X=0) = e^{-\lambda}$ ([@problem_id:1944343])。我们已知 $T = \sum X_i$ 是[泊松分布](@entry_id:147769)参数 $\lambda$ 的一个充分统计量，并且可以证明它也是完备的。现在的挑战是找到一个 $T$ 的函数 $\delta(T)$，使其期望等于 $e^{-\lambda}$。
我们知道 $T$ 服从[泊松分布](@entry_id:147769) $\text{Poisson}(n\lambda)$。利用[概率生成函数](@entry_id:190573)的一个性质，对于任意常数 $a$，我们有 $E[a^T] = \exp(n\lambda(a-1))$。为了让结果等于 $e^{-\lambda}$，我们需要 $n\lambda(a-1) = -\lambda$，解得 $a = 1 - \frac{1}{n} = \frac{n-1}{n}$。
因此，我们找到了一个函数 $\delta(T) = (\frac{n-1}{n})^T$，它的期望是：
$$ E\left[ \left(\frac{n-1}{n}\right)^T \right] = \exp\left(n\lambda\left(\frac{n-1}{n} - 1\right)\right) = \exp(-\lambda) $$
这个函数 $\delta(T) = (\frac{n-1}{n})^{\sum X_i}$ 是一个关于完备充分统计量 $T$ 的函数，并且是 $e^{-\lambda}$ 的[无偏估计量](@entry_id:756290)。根据[雷曼-谢费定理](@entry_id:176171)，它就是 $e^{-\lambda}$ 的 [UMVUE](@entry_id:169429)。这个例子完美地展示了如何结合理论工具来构造具有最优性质的估计量。