{"hands_on_practices": [{"introduction": "本练习将展示斯卢茨基定理最直接的应用。我们将处理两个随机序列，一个依分布收敛，另一个依概率收敛到一个非零常数。通过分析它们的比率，你可以掌握斯卢茨基定理的核心思想，即如何确定这类组合序列的极限分布。[@problem_id:1955680]", "problem": "在一个高级信号处理应用中，一位工程师分析了两个独立的测量序列。第一个序列由随机变量 $\\{A_n\\}_{n=1}^{\\infty}$ 表示，它捕获了一个已经中心化的含噪信号。理论模型预测，当测量次数 $n$ 很大时，$A_n$ 的分布趋近于一个均值为0、方差为 $\\sigma^2$ 的正态分布。形式上表示为 $A_n \\xrightarrow{d} N(0, \\sigma^2)$，其中 $\\sigma > 0$。\n\n第二个序列 $\\{B_n\\}_{n=1}^{\\infty}$ 是一个系统参数的自适应估计。已知该估计量是一致的，依概率收敛于一个非零正常数 $c$。形式上表示为 $B_n \\xrightarrow{p} c$，其中 $c > 0$。\n\n工程师对校准后信号的统计性质感兴趣，该信号定义为比值 $Z_n = \\frac{A_n}{B_n}$。确定当 $n \\to \\infty$ 时序列 $\\{Z_n\\}$ 的极限分布。\n\n从以下选项中选择对极限分布的正确描述。\n\nA. 均值为0、方差为 $\\sigma^2 c^2$ 的正态分布。\n\nB. 自由度为 $n-1$ 的学生t分布。\n\nC. 均值为0、方差为 $\\frac{\\sigma^2}{c^2}$ 的正态分布。\n\nD. 卡方分布。\n\nE. 分布退化为常数值0。\n\nF. 均值为0、方差为 $\\sigma^2$ 的正态分布。", "solution": "我们已知 $A_{n} \\xrightarrow{d} N(0,\\sigma^{2})$（其中 $\\sigma > 0$）和 $B_{n} \\xrightarrow{p} c$（其中 $c > 0$）。定义 $Z_{n}=\\frac{A_{n}}{B_{n}}$。\n\n由于函数 $g(y) = 1/y$ 在点 $c \\neq 0$ 处是连续的，并且我们已知 $B_n \\xrightarrow{p} c$，根据连续映射定理 (Continuous Mapping Theorem)，我们可以得出 $g(B_n) \\xrightarrow{p} g(c)$，即 $\\frac{1}{B_n} \\xrightarrow{p} \\frac{1}{c}$。\n\n现在，我们可以将 $Z_n$ 写成 $A_n$ 和 $\\frac{1}{B_n}$ 的乘积：$Z_n = A_n \\cdot \\frac{1}{B_n}$。\n\n我们有一个序列 $A_n$ 依分布收敛于一个正态随机变量 $X \\sim N(0, \\sigma^2)$，另一个序列 $\\frac{1}{B_n}$ 依概率收敛于一个常数 $\\frac{1}{c}$。\n\n根据斯卢茨基定理的乘法法则，它们的乘积依分布收敛于 $X \\cdot \\frac{1}{c}$。\n\n极限随机变量 $X/c$ 是一个正态分布的随机变量。它的均值为 $E[X/c] = E[X]/c = 0/c = 0$。它的方差为 $\\text{Var}(X/c) = \\text{Var}(X)/c^2 = \\sigma^2/c^2$。\n\n因此，$\\{Z_n\\}$ 的极限分布是均值为0、方差为 $\\frac{\\sigma^2}{c^2}$ 的正态分布，即 $N(0, \\sigma^2/c^2)$。\n\n这与选项 C 相匹配。", "answer": "$$\\boxed{C}$$", "id": "1955680"}, {"introduction": "在更实际的场景中，我们常常需要结合多个极限定理。这个练习就是一个很好的例子，它要求你将中心极限定理（产生一个依分布收敛的序列）和强大数定律（产生一个依概率收敛的序列）下的结果结合起来。通过解决这个问题，你将学会如何运用斯卢茨基定理作为桥梁，推导出一个复合统计量的极限分布。[@problem_id:840102]", "problem": "设 $X_1, X_2, \\dots, X_n$ 为一组独立同分布 (i.i.d.) 的随机变量，其总体均值为有限值 $\\mu$，总体方差为有限的非零值 $\\sigma^2$。\n设 $Y_1, Y_2, \\dots, Y_n$ 为第二组独立同分布的随机变量，与第一组序列独立，服从成功概率为 $p$ 的伯努利分布，其中 $p \\in (0, 1)$。\n\n定义第一组序列的样本均值为 $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$，第二组序列的样本成功比例为 $\\hat{p}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$。\n\n考虑由下式定义的随机变量序列 $W_n$：\n$$W_n = \\hat{p}_n \\left( \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma} \\right)$$\n使用适当的极限定理，推导当 $n \\to \\infty$ 时 $W_n$ 的极限分布的概率密度函数 (PDF)。将您的答案表示为变量 $w$ 和参数 $p$ 的函数。", "solution": "1. 根据关于 $\\{X_i\\}$ 的中心极限定理，\n$$Z_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu)}{\\sigma}\\;\\xrightarrow{d}\\;Z\\sim N(0,1).$$\n2. 根据关于 $\\{Y_i\\}$ 的大数定律，\n$$\\hat p_n=\\frac1n\\sum_{i=1}^nY_i\\;\\xrightarrow{p}\\;p.$$\n3. 由于 $\\{X_i\\}$ 和 $\\{Y_i\\}$ 是独立的，因此 $Z_n$ 和 $\\hat p_n$ 也是独立的。根据 Slutsky 定理，\n$$W_n=\\hat p_n\\,Z_n\\;\\xrightarrow{d}\\;p\\,Z.$$\n4. 因此，极限 $pZ$ 服从均值为 $0$、方差为 $p^2$ 的正态分布，所以其概率密度函数 (PDF) 为\n$$f(w)=\\frac1{\\sqrt{2\\pi\\,p^2}}\\exp\\!\\biggl(-\\frac{w^2}{2p^2}\\biggr)\n=\\frac1{p\\sqrt{2\\pi}}\\exp\\!\\biggl(-\\frac{w^2}{2p^2}\\biggr).$$", "answer": "$$\\boxed{\\frac{1}{p\\sqrt{2\\pi}}\\exp\\!\\bigl(-\\tfrac{w^2}{2p^2}\\bigr)}$$", "id": "840102"}, {"introduction": "斯卢茨基定理在理论统计学中是一个强大的分析工具。本练习探讨了当一个关键假设被违反时，一个常用的统计检验量（t-统计量）的渐近行为。通过推导其极限分布的方差，你将深刻理解斯卢茨基定理如何用于评估统计方法的稳健性，并揭示不当使用统计检验可能带来的后果。[@problem_id:840045]", "problem": "考虑两个独立样本：$X_1, X_2, \\dots, X_{n_1}$ 是来自均值为 $\\mu_1$、方差为 $\\sigma_1^2$ 的总体的独立同分布（i.i.d.）随机变量，$Y_1, Y_2, \\dots, Y_{n_2}$ 是来自均值为 $\\mu_2$、方差为 $\\sigma_2^2$ 的总体的独立同分布（i.i.d.）随机变量。假设总体方差不相等，即 $\\sigma_1^2 \\neq \\sigma_2^2$。\n\n一位研究者错误地假设方差相等，并使用合并方差 t-统计量来检验原假设 $H_0: \\mu_1 = \\mu_2$。该统计量定义为：\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\n其中 $\\bar{X}_{n_1}$ 和 $\\bar{Y}_{n_2}$ 是样本均值，$S_p^2$ 是合并样本方差估计量：\n$$\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}\n$$\n其中 $S_1^2$ 和 $S_2^2$ 是无偏样本方差。\n\n我们感兴趣的是当两个样本量都很大时该统计量的渐近行为。假设 $n_1 \\to \\infty$ 且 $n_2 \\to \\infty$，并且它们的相对比例保持不变。具体来说，令 $N = n_1 + n_2$，并假设当 $N \\to \\infty$ 时，比率 $\\frac{n_1}{N}$ 收敛到一个常数 $\\lambda \\in (0, 1)$。\n\n在原假设 $H_0: \\mu_1 = \\mu_2$ 下，统计量 $T_{n_1, n_2}$ 依分布收敛于一个正态分布，$T_{n_1, n_2} \\xrightarrow{d} N(0, V)$。\n\n推导渐近方差 $V$ 关于 $\\sigma_1^2$、$\\sigma_2^2$ 和 $\\lambda$ 的表达式。", "solution": "问题要求计算被错误使用的合并方差 t-统计量的渐近方差。该统计量由下式给出：\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\n我们将使用 Slutsky 定理来求其极限分布。该定理指出，如果 $A_n \\xrightarrow{d} A$ 且 $B_n \\xrightarrow{p} b$（一个常数），那么 $A_n / B_n \\xrightarrow{d} A/b$。\n\n**步骤1：分析分子**\n\n我们首先在适当缩放后，确定分子 $\\bar{X}_{n_1} - \\bar{Y}_{n_2}$ 的极限分布。在原假设 $H_0: \\mu_1 = \\mu_2$ 下，$\\bar{X}_{n_1} - \\bar{Y}_{n_2}$ 的均值为 $E[\\bar{X}_{n_1} - \\bar{Y}_{n_2}] = \\mu_1 - \\mu_2 = 0$。方差为 $\\text{Var}(\\bar{X}_{n_1} - \\bar{Y}_{n_2}) = \\text{Var}(\\bar{X}_{n_1}) + \\text{Var}(\\bar{Y}_{n_2}) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$。\n\n根据中心极限定理（CLT），标准化后的均值差依分布收敛于一个标准正态分布：\n$$\nZ_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\xrightarrow{d} N(0, 1)\n$$\n\n**步骤2：重写统计量并分析分母**\n\n我们可以用 $Z_{n_1, n_2}$ 来重写统计量 $T_{n_1, n_2}$：\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\cdot \\frac{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} = Z_{n_1, n_2} \\cdot \\sqrt{\\frac{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\n我们定义 $W_{n_1, n_2}^2$ 如下：\n$$\nW_{n_1, n_2}^2 = \\frac{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n根据 Slutsky 定理，如果 $W_{n_1, n_2}$ 依概率收敛于一个常数 $W$，那么 $T_{n_1, n_2} \\xrightarrow{d} N(0, 1) / W \\sim N(0, 1/W^2)$。因此，我们的目标是求出 $W_{n_1, n_2}^2$ 的概率极限。\n\n**步骤3：求合并方差估计量 $S_p^2$ 的概率极限**\n\n合并方差估计量为：\n$$\nS_p^2 = \\frac{n_1-1}{n_1+n_2-2} S_1^2 + \\frac{n_2-1}{n_1+n_2-2} S_2^2\n$$\n根据大数定律（LLN），样本方差是总体方差的相合估计量：$S_1^2 \\xrightarrow{p} \\sigma_1^2$ 且 $S_2^2 \\xrightarrow{p} \\sigma_2^2$。\n\n现在，我们来分析当 $n_1, n_2 \\to \\infty$ 时这些系数的变化。令 $N = n_1 + n_2$。已知 $\\frac{n_1}{N} \\to \\lambda$，这意味着 $\\frac{n_2}{N} = \\frac{N-n_1}{N} = 1 - \\frac{n_1}{N} \\to 1-\\lambda$。\n$$\n\\lim_{N\\to\\infty} \\frac{n_1-1}{n_1+n_2-2} = \\lim_{N\\to\\infty} \\frac{\\frac{n_1}{N} - \\frac{1}{N}}{1 - \\frac{2}{N}} = \\frac{\\lambda - 0}{1-0} = \\lambda\n$$\n$$\n\\lim_{N\\to\\infty} \\frac{n_2-1}{n_1+n_2-2} = \\lim_{N\\to\\infty} \\frac{\\frac{n_2}{N} - \\frac{1}{N}}{1-\\frac{2}{N}} = \\frac{1-\\lambda - 0}{1-0} = 1-\\lambda\n$$\n根据连续映射定理（Slutsky 定理的一个推论），$S_p^2$ 的概率极限是：\n$S_p^2 \\xrightarrow{p} \\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2$\n\n**步骤4：求方差和之比的极限**\n\n我们来分析比率 $\\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}$。我们可以将分子和分母同乘以 $N = n_1+n_2$：\n$$\n\\frac{N\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}{N\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)} = \\frac{\\frac{N}{n_1} + \\frac{N}{n_2}}{\\frac{N}{n_1}\\sigma_1^2 + \\frac{N}{n_2}\\sigma_2^2}\n$$\n当 $N \\to \\infty$ 时，我们有 $\\frac{N}{n_1} \\to \\frac{1}{\\lambda}$ 和 $\\frac{N}{n_2} \\to \\frac{1}{1-\\lambda}$。因此，该比率的概率极限为：\n$$\n\\lim_{N \\to \\infty} \\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} = \\frac{\\frac{1}{\\lambda} + \\frac{1}{1-\\lambda}}{\\frac{\\sigma_1^2}{\\lambda} + \\frac{\\sigma_2^2}{1-\\lambda}} = \\frac{\\frac{1-\\lambda+\\lambda}{\\lambda(1-\\lambda)}}{\\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda(1-\\lambda)}} = \\frac{1}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n\n**步骤5：结合结果求 $W_{n_1, n_2}^2$ 的极限**\n\n现在我们可以求出 $W_{n_1, n_2}^2$ 的概率极限：\n$$\nW_{n_1, n_2}^2 = S_p^2 \\cdot \\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n使用我们在步骤3和步骤4中推导出的极限：\n$$\nW^2 = \\text{plim}_{N \\to \\infty} W_{n_1, n_2}^2 = (\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2) \\cdot \\frac{1}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n$$\nW^2 = \\frac{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n\n**步骤6：最后应用 Slutsky 定理**\n\n我们有 $T_{n_1, n_2} = Z_{n_1, n_2} / W_{n_1, n_2}$。因为 $Z_{n_1, n_2} \\xrightarrow{d} Z \\sim N(0, 1)$ 且 $W_{n_1, n_2} \\xrightarrow{p} W$，根据 Slutsky 定理：\n$$\nT_{n_1, n_2} \\xrightarrow{d} \\frac{Z}{W}\n$$\n该极限分布是均值为0，方差为 $1/W^2$ 的正态分布，即 $N\\left(0, \\frac{1}{W^2}\\right)$。\n\n因此，渐近方差 $V$ 为 $1/W^2$。\n$$\nV = \\frac{1}{W^2} = \\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}\n$$\n该表达式给出了错误应用的 t-统计量的极限正态分布的方差。", "answer": "$$\n\\boxed{\\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}}\n$$", "id": "840045"}]}