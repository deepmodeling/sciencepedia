## 引言
[经典信息论](@entry_id:142021)，由 Claude Shannon 在20世纪中叶奠基，彻底改变了我们对通信和数据存储的理解。它提供了一套严谨的数学框架，用以量化信息、度量不确定性，并揭示了在物理限制下进行可靠通信和高效压缩的根本极限。尽管诞生于[通信工程](@entry_id:272129)，其深刻的原理已远远超出了原有范畴，渗透到计算机科学、物理学、经济学乃至生物学等众多领域。

然而，对于初学者而言，信息论的各个分支——从[信源编码](@entry_id:755072)到[信道编码](@entry_id:268406)，再到[率失真理论](@entry_id:138593)和[网络信息论](@entry_id:276799)——往往看似孤立。本文旨在解决这一知识鸿沟，系统性地梳理[经典信息论](@entry_id:142021)的核心脉络，将抽象的数学定义与具体的应用场景紧密结合。

通过本文，读者将踏上一段从基础到前沿的旅程。在“原理与机制”一章中，我们将建立信息论的基石，探索熵、[数据压缩](@entry_id:137700)和[信道容量](@entry_id:143699)的数学本质。接着，在“应用与跨学科联系”一章中，我们将见证这些理论如何在先进通信网络、信息安全、控制系统和[统计学习](@entry_id:269475)等领域大放异彩。最后，“动手实践”部分将通过具体的计算问题，帮助您将理论知识转化为解决实际问题的能力。这三章环环相扣，旨在为您构建一个全面而深入的[经典信息论](@entry_id:142021)知识体系。

## 原理与机制

本章深入探讨[经典信息论](@entry_id:142021)的核心原理与机制。我们将从量化信息的基本概念——熵——出发，探索[无损数据压缩](@entry_id:266417)的理论极限。随后，我们将转向有噪[信道编码](@entry_id:268406)问题，定义信道容量并阐述其在可靠通信中的核心作用。最后，我们将这些基本思想扩展到更高级的主题，包括[有损压缩](@entry_id:267247)、[多用户通信](@entry_id:262688)和通用编码，为理解现代通信和数据存储系统的理论基础提供一个坚实的框架。

### 信息与不确定性的量化

信息论的基石是对“信息”这一概念的数学量化。Claude Shannon在其开创性的工作中提出，一个事件所包含的[信息量](@entry_id:272315)与其不确定性或“意外性”直接相关。一个非常可能发生的事件，其发生并不会带来太多信息；相反，一个极不可能发生的事件一旦发生，则会提供大量信息。

#### 熵：不确定性的度量

[香农熵](@entry_id:144587) (Shannon Entropy) 是度量一个[随机变量](@entry_id:195330)不确定性的核心工具。对于一个[离散随机变量](@entry_id:163471) $X$，其取值于字母表 $\mathcal{X}$，[概率质量函数](@entry_id:265484)为 $P(x)$，其[香农熵](@entry_id:144587) $H(X)$ 定义为：
$$ H(X) = -\sum_{x \in \mathcal{X}} P(x) \log_2 P(x) $$
熵的单位是“比特”(bit)，直观上，它表示了编码该[随机变量](@entry_id:195330)的样本所需的最小平均比特数。

例如，考虑一个重复进行独立伯努利试验的过程，每次试验成功的概率为 $p$。我们感兴趣的[随机变量](@entry_id:195330) $X$ 是获得第一次成功所需的试验次数。这是一个几何分布，其[概率质量函数](@entry_id:265484)为 $P(X=k) = (1-p)^{k-1}p$，其中 $k = 1, 2, 3, \dots$。尽管这个变量的取值空间是无限的，我们仍然可以计算其熵。通过展开对数项并利用无穷级数的求和技巧，可以推导出该[几何分布](@entry_id:154371)的熵为 [@problem_id:53401]：
$$ H(X) = - \log_2 p - \frac{1-p}{p} \log_2 (1-p) $$
这个结果量化了等待一个概率为 $p$ 的事件首次发生所需的不确定性。当 $p$ 接近1时，成功几乎是瞬时发生的，不确定性趋于0，熵也趋于0。当 $p$ 很小时，等待时间的不确定性非常大，熵也相应地增大。

#### [联合熵](@entry_id:262683)与[条件熵](@entry_id:136761)

当处理多个[随机变量](@entry_id:195330)时，我们可以扩展熵的概念。两个[随机变量](@entry_id:195330) $(X, Y)$ 的**[联合熵](@entry_id:262683) (Joint Entropy)** $H(X, Y)$ 度量了这对变量的总体不确定性。给定 $X$ 的值后，$Y$ 的剩余不确定性则由**[条件熵](@entry_id:136761) (Conditional Entropy)** $H(Y|X)$ 来量化。这些量之间通过熵的**[链式法则](@entry_id:190743) (chain rule)** 紧密联系：
$$ H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y) $$
这个法则表明，一对变量的总不确定性等于其中一个变量的不确定性，加上在已知该变量的条件下另一个变量的剩余不确定性。

举一个具体的例子，假设一个均匀随机比特 $X \sim \text{Bernoulli}(1/2)$ 通过一个[交叉概率](@entry_id:276540)为 $p$ 的[二进制对称信道](@entry_id:266630) (Binary Symmetric Channel, BSC)，得到输出 $Y$。这个过程可以表示为 $Y = X \oplus Z$，其中 $Z \sim \text{Bernoulli}(p)$ 是独立于 $X$ 的噪声。根据香农的[信源编码定理](@entry_id:138686)，[无损压缩](@entry_id:271202)这对联合信源 $(X, Y)$ 所需的最小平均[码率](@entry_id:176461)等于其[联合熵](@entry_id:262683) $H(X, Y)$。利用链式法则，我们可以轻松计算这个值 [@problem_id:53403]：
$$ R = H(X, Y) = H(X) + H(Y|X) $$
由于 $X$ 是均匀的，其熵 $H(X) = 1$ 比特。给定 $X$，$Y$ 的不确定性完全来自于噪声 $Z$，因此 $H(Y|X) = H(Z) = H_2(p)$，其中 $H_2(p) = -p\log_2 p - (1-p)\log_2(1-p)$ 是[二进制熵函数](@entry_id:269003)。因此，最小压缩码率为：
$$ R = 1 + H_2(p) $$

#### [互信息](@entry_id:138718)与[数据处理不等式](@entry_id:142686)

两个[随机变量](@entry_id:195330)之间的**互信息 (Mutual Information)** $I(X;Y)$ 度量了它们共享的[信息量](@entry_id:272315)，或者说，知道一个变量能够消除另一个变量的多少不确定性。它的定义为：
$$ I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) $$
互信息是一个非负且对称的量。

信息论中的一个基本原理是**[数据处理不等式](@entry_id:142686) (Data Processing Inequality)**。它指出，对于任何形成[马尔可夫链](@entry_id:150828) $X \to Y \to Z$ 的[随机变量](@entry_id:195330)，信息在处理过程中只会丢失或保持不变，而绝不会增加。数学上，这意味着：
$$ I(X;Z) \le I(X;Y) $$
直观地说，对数据 $Y$ 进行的任何处理（无论多么复杂）得到 $Z$，都无法提取出比 $Y$ 本身所含的关于 $X$ 的信息更多的信息。我们可以通过一个具体的例子来验证这个不等式 [@problem_id:53429]。假设一个均匀三元信源 $X$ 经过第一个信道产生二元输出 $Y$，随后 $Y$ 再经过一个[交叉概率](@entry_id:276540)为 $f$ 的BSC产生最终输出 $Z$。通过直接计算 $I(X;Y)$ 和 $I(X;Z)$，我们可以发现它们的差值 $D = I(X;Y) - I(X;Z)$ 总是非负的，这个差值精确地量化了在第二级处理中丢失的关于 $X$ 的信息。

### 无损[信源编码](@entry_id:755072)：压缩的极限

[信源编码](@entry_id:755072)，或数据压缩，旨在以更少的比特来表示信息，同时要求能够完美地重构原始数据。香农的理论为[无损压缩](@entry_id:271202)的性能极限提供了精确的答案。

#### 香农第一定理与[典型集](@entry_id:274737)

**香农的无损[信源编码定理](@entry_id:138686)** (或称香农第一定理) 指出，对于一个熵为 $H(X)$ 的独立同分布 (i.i.d.) 信源，存在一种编码方案，可以用平均 $L$ 比特/符号来表示该信源的输出序列，其中 $L$ 可以任意接近 $H(X)$。反之，任何[无损压缩](@entry_id:271202)方案的[平均码长](@entry_id:263420)都不可能小于 $H(X)$。因此，$H(X)$ 是[无损压缩](@entry_id:271202)的根本极限。

这个深刻结果的背后是**渐近均分特性 (Asymptotic Equipartition Property, AEP)**。AEP表明，对于一个由 i.i.d. 信源产生的长序列 $x^n = (x_1, \dots, x_n)$，其经验熵 $-\frac{1}{n}\log_2 P(x^n)$ 几乎总是接近于真实熵 $H(X)$。所有这些概率“表现正常”的序列构成了所谓的**[典型集](@entry_id:274737) (typical set)** $A_{\epsilon}^{(n)}$。AEP的关键结论是：
1.  一个随机序列属于[典型集](@entry_id:274737)的概率随着 $n$ 的增长趋向于1。
2.  [典型集](@entry_id:274737)的大小约为 $|A_{\epsilon}^{(n)}| \approx 2^{n H(X)}$。

这意味着，尽管总共有 $|\mathcal{X}|^n$ 种可能的序列，但几乎所有的概率都集中在一个仅包含约 $2^{n H(X)}$ 个序列的小集合中。因此，我们只需要为这个[典型集](@entry_id:274737)中的序列分配唯一的码字，就可以用约 $n H(X)$ 比特来表示整个序列，即每个符号平均使用 $H(X)$ 比特。

这一思想可以扩展到更复杂的场景，例如复合信源。假设我们不知道数据来自两个伯努利信源 $S_1$ 和 $S_2$ 中的哪一个。我们可以定义一个**复合[典型集](@entry_id:274737)**，即两个信源各自[典型集](@entry_id:274737)的并集。只要这两个[典型集](@entry_id:274737)不怎么重叠，复合[典型集](@entry_id:274737)的大小就约等于两个[典型集](@entry_id:274737)大小之和，即 $|A_{\epsilon}^{(n)}(S_1)| + |A_{\epsilon}^{(n)}(S_2)| \approx 2^{n H(S_1)} + 2^{n H(S_2)}$ [@problem_id:53523]。这为处理不确定信源的编码问题提供了理论基础。

#### [前缀码](@entry_id:261012)与[霍夫曼编码](@entry_id:262902)

为了实现香农极限，我们需要设计具体的编码方案。**[前缀码](@entry_id:261012) (prefix code)** 是一类非常实用的编码，其中没有任何码字是其他码字的前缀。这个特性保证了码流可以被即时、无歧义地解码。

什么样的码长组合 $\{l_1, l_2, \dots, l_N\}$ 可以构成一个使用 $D$ 元字母表的[前缀码](@entry_id:261012)？**[克拉夫特不等式](@entry_id:274650) (Kraft's inequality)** 给出了答案：这样的[前缀码](@entry_id:261012)存在的充要条件是：
$$ \sum_{i=1}^{N} D^{-l_i} \le 1 $$
这个不等式为[码字长度](@entry_id:274532)的分配设定了严格的“预算”。例如，如果我们有 $N_A$ 个符号需要长度为 $L$ 的码字，以及 $N_B$ 个符号需要长度为 $L+1$ 的码字，那么我们必须选择一个足够大的字母表大小 $D$，以满足 $N_A D^{-L} + N_B D^{-(L+1)} \le 1$ [@problem_id:53425]。

在所有满足[克拉夫特不等式](@entry_id:274650)的[前缀码](@entry_id:261012)中，哪一个的平均长度 $L = \sum p_i l_i$ 最小？**[霍夫曼编码](@entry_id:262902) (Huffman coding)** 算法提供了一个构造性的答案。它是一个简单的[贪心算法](@entry_id:260925)：
1.  将所有信源符号视为叶节点，并赋予其相应的概率。
2.  反复选择当前概率最小的两个节点，将它们合并成一个新的父节点，该父节点的概率是两个子节点概率之和。
3.  重复此过程，直到所有节点合并成一个根节点。
4.  从根节点到每个叶节点的路径就定义了该符号的码字。

[霍夫曼编码](@entry_id:262902)所产生的码长分配给高概率符号较短的码字，给低概率符号较长的码字，最终得到的[平均码长](@entry_id:263420) $L_H$ 是所有[前缀码](@entry_id:261012)中最小的，并且满足 $H(X) \le L_H  H(X) + 1$。对于一个具体的[概率分布](@entry_id:146404)，例如 $\{0.4, 0.2, 0.2, 0.1, 0.1\}$，我们可以通过执行霍夫曼算法来构建最优码树，并计算出其[期望码长](@entry_id:261607) [@problem_id:53428]。

### 有噪[信道编码](@entry_id:268406)：通信的极限

通信的中心问题是如何在有噪声干扰的信道上可靠地传输信息。香农的第二个重要定理，即有噪[信道编码定理](@entry_id:140864)，为这个问题提供了惊人而深刻的解答。

#### [信道容量](@entry_id:143699)与香农第二定理

一个**[离散无记忆信道](@entry_id:275407) (DMC)** 由一个输入字母表 $\mathcal{X}$、一个输出字母表 $\mathcal{Y}$ 以及一组[条件概率](@entry_id:151013) $P(y|x)$ 定义。信道的**容量 (Capacity)** $C$ 是该信道能够可靠传输信息的最大速率。它被定义为在所有可能的输入[分布](@entry_id:182848) $p(x)$ 上，输入 $X$ 和输出 $Y$ 之间互信息 $I(X;Y)$ 的最大值：
$$ C = \max_{p(x)} I(X;Y) $$
由于 $I(X;Y) = H(Y) - H(Y|X)$，且 $H(Y|X)$ 对于给定的信道是固定的（只取决于输入[分布](@entry_id:182848)），最大化[互信息](@entry_id:138718)等价于最大化输出熵 $H(Y)$。对于具有对称性的信道，例如**三元[对称信道](@entry_id:274947) (Ternary Symmetric Channel, TSC)**，输入符号 $i$ 以概率 $1-2p$ 正确传输，并以等概率 $p$ 错误地传输到其他两个符号之一。由于其对称性，选择均匀输入[分布](@entry_id:182848) $p(x)=1/3$ 能够使输出[分布](@entry_id:182848)也均匀，从而最大化输出熵 $H(Y)$。通过这个方法，我们可以计算出TSC的容量为 $C = \log_2 3 + (1-2p)\log_2(1-2p) + 2p\log_2 p$ [@problem_id:53400]。

**香农的有噪[信道编码定理](@entry_id:140864)** (或称香non第二定理) 指出：
1.  对于任何信息速率 $R  C$，都存在一种编码和解码方案，使得传输错误率可以任意小。
2.  对于任何信息速率 $R > C$，传输错误率不可能任意小，并且随着码长的增加会趋近于1。

这个定理的意义在于，只要传输速率不超过一个特定的阈值——[信道容量](@entry_id:143699)，我们就能实现近乎完美的通信，无论信道噪声有多大（只要 $C>0$）。

#### 逆定理、[Fano不等式](@entry_id:138517)与强逆定理

[信道编码定理](@entry_id:140864)的第二部分，即逆定理，说明了容量是一个硬性限制。证明这个逆定理的一个关键工具是**[Fano不等式](@entry_id:138517) (Fano's inequality)**。它将解码错误概率 $P_e = \Pr(X \neq \hat{X})$ 与已知信道输出 $Y$ 后对输入 $X$ 的剩余不确定性 $H(X|Y)$ 联系起来。对于一个有 $k$ 个可能消息的系统，该不等式为：
$$ H(X|Y) \le H_2(P_e) + P_e \log_2(k-1) $$
这个不等式表明，如果[条件熵](@entry_id:136761) $H(X|Y)$ 很大（即接收到的信号 $Y$ 对 $X$ 的信息很少），那么任何解码器的[错误概率](@entry_id:267618) $P_e$ 都不可能很小。给定一个信道和其导致的[条件熵](@entry_id:136761) $H(X|Y)$，我们可以利用[Fano不等式](@entry_id:138517)来确定任何解码器所能达到的最小[错误概率](@entry_id:267618)的下界 [@problem_id:53434]。

**强逆定理 (Strong Converse)** 进一步加强了逆定理的结论。它不仅说明当 $R > C$ 时错误率无法趋于0，还精确地描述了成功解码的概率如何随着码块长度 $n$ 的增加而指数级地衰减。对于BSC，当速率 $R$ 超过容量 $C$ 时，最大成功概率表现为 $P_{\text{succ}}^{\text{max}}(n, R) \asymp 2^{-n E_{sc}(R,p)}$，其中 $E_{sc}(R,p)$ 是**强逆指数**。这个指数可以通过两个[伯努利分布](@entry_id:266933)之间的Kullback-Leibler散度来计算，它量化了以超容速率通信的指数级惩罚 [@problem_id:53444]。

### 高级信道模型与概念

现实世界的信道往往比简单的DMC模型更复杂。信息论也发展了相应的工具来分析这些更贴近实际的场景。

#### 具有记忆的信道

许多信道（如无线[衰落信道](@entry_id:269154)）的噪声特性是随时间变化的，并且具有记忆性，即当前的状态依赖于过去的状态。**吉尔伯特-艾略特 (Gilbert-Elliott) 信道**是这类信道的一个经典模型。它假设信道在“好”(G) 和“坏”(B) 两种状态之间切换，每种状态对应一个不同[交叉概率](@entry_id:276540)的BSC。状态之间的转移遵循马尔可夫链。分析这类信道的容量通常更为复杂，但对于某些特殊情况，例如状态确定性地交替变化（$G \to B \to G \to \dots$），[信道容量](@entry_id:143699)可以简化为在两种状态下的容量的[算术平均值](@entry_id:165355) [@problem_id:53399]。

#### 连续信道与“注水”原理

当信道输入和输出是连续变量时，例如模拟信号，我们需要使用[微分熵](@entry_id:264893)来代替离散熵。一个最重要的连续信道模型是**[加性高斯白噪声](@entry_id:269320) ([AWGN](@entry_id:269320)) 信道**。对于带宽受限的[AWGN信道](@entry_id:269115)，其容量由著名的香农-哈特利定理给出。

当噪声不是“白色”的（即其功率在不同频率上不均匀）时，情况会变得更有趣。假设我们有一个总功率预算 $P$，需要将其分配到不同的频率上以最大化总传输速率。**[注水](@entry_id:270313) (water-filling)** 原理为这个问题提供了一个优美的几何解释 [@problem_id:53407]。我们可以想象一个容器，其底部形状是[噪声功率谱密度](@entry_id:274939) $S_N(f)$ 的倒数 $1/S_N(f)$。向这个容器中“注入”总量为 $P$ 的“水”（[信号功率](@entry_id:273924)），水面会达到一个恒定的高度。在每个频率 $f$ 上，水深 $S_X(f)$ 就是最优的信号功率分配。这个策略直观地告诉我们：应该在噪声较小的频率（容器底部较高）分配更多的功率，而在噪声非常大的频率（容器底部非常低）则不分配功率。对于其他类型的信道，例如**泊松 (Poisson) 信道**，它常用于模拟[光通信](@entry_id:200237)中的[光子计数](@entry_id:186176)，其互信息的计算也需要类似的积分方法，有时甚至涉及高等数学函数，如伽玛函数 [@problem_id:53378]。

#### 有限块长编码与[I-MMSE关系](@entry_id:273569)

香农定理是关于块长 $n \to \infty$ 时的渐近结果。在实际应用中，块长总是有限的。在有限块长下，我们无法同时达到速率 $C$ 和错误率 $0$。**[正态近似](@entry_id:261668) (Normal Approximation)** 提供了对有限块长下最大[可达速率](@entry_id:273343) $R^*(n, \epsilon)$ 的一个精确近似。它表明，速率会从香农容量 $C$ 下降，下降的幅度与信道的一个新特性——**信道散布 (channel dispersion)** $V$ 有关，并且与块长的平方根成反比：
$$ R^*(n, \epsilon) \approx C - \sqrt{\frac{V}{n}} \Phi^{-1}(1-\epsilon) $$
其中 $\Phi^{-1}$ 是标准正态分布的[逆累积分布函数](@entry_id:266870)。这个公式精确地刻画了在有限资源（块长 $n$）下，速率、可靠性（$\epsilon$）和延迟之间的权衡。对于**[二进制删除信道](@entry_id:267278) (BEC)**，其容量和散布都可以被精确计算，从而可以得到其在有限块长下的性能近似 [@problem_id:53438]。

信息论与[估计理论](@entry_id:268624)之间也存在深刻的联系，其中最著名的就是**[I-MMSE关系](@entry_id:273569)**。该关系指出，对于 $Y = \sqrt{\rho} X + Z$ 形式的[AWGN信道](@entry_id:269115)（其中 $\rho$ 是[信噪比](@entry_id:185071)），互信息 $I(X;Y)$ 对[信噪比](@entry_id:185071) $\rho$ 的导数，恰好等于从 $Y$ 估计 $X$ 的**[最小均方误差 (MMSE)](@entry_id:264377)** 的一半。这个美妙的公式 $\frac{d}{d\rho} I = \frac{1}{2} \text{mmse}(\rho)$ 连接了两个看似无关的量，为[互信息](@entry_id:138718)的计算和MMSE的分析提供了强大的新工具 [@problem_id:53420]。

### [率失真理论](@entry_id:138593)：[有损压缩](@entry_id:267247)的极限

对于许多现实世界的数据，如图像和音频，我们通常不需要完美地重构。我们可以容忍一定的失真，以换取更高的压缩率。**[率失真理论](@entry_id:138593) (Rate-Distortion Theory)** 就是研究这种速率和失真之间权衡的框架。

给定一个信源 $X$ 和一个[失真度量](@entry_id:276563) $d(x, \hat{x})$（例如[均方误差](@entry_id:175403)），**[率失真函数](@entry_id:263716) (rate-distortion function)** $R(D)$ 定义了在平均失真不超过 $D$ 的条件下，压缩该信源所需的最小信息速率。
$$ R(D) = \min_{p(\hat{x}|x) \text{ s.t. } E[d(X,\hat{X})] \le D} I(X; \hat{X}) $$
对于一个[方差](@entry_id:200758)为 $\sigma^2$ 的高斯信源和[均方误差失真](@entry_id:261750)，[率失真函数](@entry_id:263716)有一个简洁而著名的形式 [@problem_id:53554]：
$$ R(D) = \frac{1}{2} \log_2\left(\frac{\sigma^2}{D}\right) \quad (\text{for } 0  D \le \sigma^2) $$
这个公式表明，我们愿意容忍的失真 $D$ 越大，所需的[码率](@entry_id:176461) $R(D)$ 就越低。当 $D \to 0$ 时，$R(D) \to \infty$，说明[完美重构](@entry_id:194472)一个连续信源需要无限的[码率](@entry_id:176461)。

对于具有记忆的高斯信源（例如高斯马尔可夫信源），其[率失真函数](@entry_id:263716)可以通过一个类似于信道容量中“[注水](@entry_id:270313)”的**逆注水 (reverse water-filling)** 过程来确定 [@problem_id:53369]。其几何图像是，我们有一个由信源[功率谱密度](@entry_id:141002) $S_X(f)$ 构成的“山丘”，用一个水平面去切割它。水平面以下的部分代表被“淹没”并被噪声替代的[频谱](@entry_id:265125)分量，其面积对应于总失真 $D$。水平面以上的部分则是需要被精确编码的，其积分形式给出了所需的[码率](@entry_id:176461) $R(D)$。

### 多用户信息论

[经典信息论](@entry_id:142021)主要关注点对点的通信链路。**多用户信息论 (Multi-user Information Theory)** 将其扩展到包含多个发送者或接收者的网络场景。

#### 多址信道 (MAC)

**多址信道 (Multiple-Access Channel, MAC)** 模型化了多个发送者向一个共同的接收者发送信息的场景，例如多个手机用户与一个基站通信。其性能由一个**[容量区](@entry_id:271060)域 (capacity region)** 来描述，这是一个所有[可达速率](@entry_id:273343)对 $(R_1, R_2)$ 的集合。对于一个无噪的[二进制加法信道](@entry_id:265650) $Y = X_1 + X_2$，其[容量区](@entry_id:271060)域由不等式 $R_1 \le 1$, $R_2 \le 1$ 和 $R_1 + R_2 \le 1.5$ 所界定 [@problem_id:53397]。这个区域的形状表明了用户之间的速率权衡：一个用户提高速率可能会限制另一个用户的速率。

如果发送方拥有**信道状态信息 (CSI)**，[容量区](@entry_id:271060)域可能会显著扩大。例如，在一个受状态 $S$ 影响的[二进制加法信道](@entry_id:265650) $Y = X_1 \oplus X_2 \oplus S$ 中，如果发送者1事先知道状态序列 $S^n$，他就可以通过编码 $X_1$ 来预先“抵消”状态 $S$ 的影响。通过选择 $X_1 = U_1 \oplus S$，信道对于辅助变量 $U_1$ 和输入 $X_2$ 来说，等效于一个无状态的信道 $Y = U_1 \oplus X_2$。这使得总速率（和速率）可以达到1比特/信道使用，显著高于没有CSI的情况 [@problem_id:53421]。

#### [广播信道](@entry_id:266614) (BC)

**[广播信道](@entry_id:266614) (Broadcast Channel, BC)** 模型化了一个发送者向多个接收者发送信息的场景，例如电台或电视台。由于不同接收者处的信道条件可能不同，发送者需要设计一种能同时服务多个用户的编码策略。对于**物理退化[广播信道](@entry_id:266614)**（即一个接收者的信号是另一个接收者信号的进一步退化版本，形成马尔可夫链 $X \to Y_1 \to Y_2$），**[叠加编码](@entry_id:275923) (superposition coding)** 是一种实现[容量区](@entry_id:271060)域[边界点](@entry_id:176493)的[最优策略](@entry_id:138495) [@problem_id:53433]。其思想是将信息分成两部分：一部分是“公共信息”，两个用户都需要解码；另一部分是“私有信息”，只有信道条件更好的用户才能解码。发送的信号是这两部分信息的叠加。通过调整分配给公共和私有信息的功率/[速率比](@entry_id:164491)例，就可以在[容量区](@entry_id:271060)域的边界上移动，实现不同的速率分配。

### [通用信源编码](@entry_id:267905)

到目前为止，我们大多假设信源的统计特性（如[概率分布](@entry_id:146404) $p(x)$）是已知的。但在许多实际应用中，这个假设并不成立。**[通用信源编码](@entry_id:267905) (Universal Source Coding)** 的目标是设计一种对某一类信源（例如所有伯努利信源）都表现良好的单一编码方案。

通用编码的性能通常通过其**冗余度 (redundancy)** 来衡量，即相比于知道真实信源参数的理想编码，它多用了多少比特。**极小化极大冗余度 (minimax redundancy)** $R_n^*$ 是在所有可能的信源参数中最坏情况下的最小可能冗余度。对于一类信源，**归一化[最大似然](@entry_id:146147) (Normalized Maximum Likelihood, NML)** 概率分配可以实现这种极小化极大冗余度。其思想是，对于一个给定的序列 $x^n$，我们首先找到使其概率最大的那个参数 $\hat{\theta}(x^n)$（即[最大似然估计](@entry_id:142509)），然后用这个最大化的概率 $P(x^n|\hat{\theta}(x^n))$，除以一个在所有可能序列上的归一化常数 $C_n$，来作为该序列的编码概率。极小化极大冗余度就是这个归一化常数的对数 $R_n^* = \log_2(C_n)$。通过直接枚举所有长度为 $n=3$ 的二[进制](@entry_id:634389)序列并计算它们的NML概率，我们可以精确地计算出这种场景下的冗余度 [@problem_id:53495]。