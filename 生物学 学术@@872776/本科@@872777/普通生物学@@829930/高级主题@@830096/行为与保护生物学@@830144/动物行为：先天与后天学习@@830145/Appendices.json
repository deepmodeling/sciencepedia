{"hands_on_practices": [{"introduction": "科学家们如何才能厘清像玩耍这样复杂行为中交织的遗传与环境因素？本练习通过一个经典的实验设计——隔离实验与交叉抚养——来探究狼群玩耍打斗行为的发展。通过分析在不同社会环境中长大的幼狼的行为结果，你将学会区分那些与生俱来的、固化的动作模式，以及那些需要通过社会学习来完善才能正确施展的精细调节。[@problem_id:2278625]", "problem": "一位动物行为学家正在研究灰狼 (*Canis lupus*) 复杂社会行为的发展。该研究人员专注于“玩耍性打斗”，这是一种在幼狼中观察到的行为，包括追逐、摔跤和轻咬等动作，这些对于发展社会联系和练习成年技能至关重要。为了调查这种行为的起源，三组幼狼从3周龄到6月龄在不同条件下被抚养。\n\n*   **第1组（对照组）：** 幼狼与它们的母亲和同窝出生的兄弟姐妹一起被抚养。观察到它们进行频繁且长时间的玩耍性打斗。这种行为协调良好，并包含清晰的开始和结束玩耍的信号。至关重要的是，它们表现出极好的“咬合抑制”，即它们会轻咬它们的兄弟姐妹而不会施加伤害性力量。\n\n*   **第2组（社会隔离组）：** 一只幼狼被隔离抚养，可以接触其母亲进行哺乳，但与其他任何幼狼没有身体或视觉接触。当这只幼狼在6月龄时被介绍给一只社交正常的幼狼时，它表现出可识别但零散的玩耍行为元素，例如笨拙的猛扑和不协调的摔跤尝试。它无法识别社交线索，最值得注意的是，它缺乏咬合抑制，经常造成疼痛并将互动升级为真正的打斗。\n\n*   **第3组（交叉抚养组）：** 一只幼狼从3周龄起在一窝比格犬 (*Canis familiaris*) 幼犬中被抚养，比格犬是一种以不同、身体接触较少的玩耍风格而闻名的犬种。在6月龄时，这只幼狼表现出一种混合的玩耍风格。它尝试进行狼式的摔跤，但也融入了比格犬式的吠叫和鞠躬。它的咬合抑制存在但对于与其他狼互动来说校准得很差；它要么太温柔，要么在过度兴奋时又太用力，这反映了它从比格犬玩伴那里收到的反馈。\n\n根据所有三组的结果，以下哪个陈述为狼的玩耍性打斗行为的发展提供了最准确和最全面的解释？\n\nA. 玩耍性打斗是一种纯粹的先天行为，其在第2组和第3组中的不当表现是由于非自然抚养条件造成的压力所致。\n\nB. 玩耍性打斗完全是一种后天学习的行为，通过模仿同窝出生的兄弟姐妹获得，没有遗传或本能基础。\n\nC. 玩耍性打斗的基本运动模式是先天的，但其正确的顺序、社会整合和强度调节（如咬合抑制）需要通过社会学习来完善。\n\nD. 咬合抑制是一种简单的、非习得性的反射，在所有犬科动物出生时就存在，但如果不使用就会丧失，正如在社会隔离组中所见。\n\nE. 这只幼狼的行为是亲代印记的经典例子，即它在一个关键时期内，从与之一起被抚养的物种那里不可逆地学习了一整套玩耍行为。", "solution": "我们使用关于先天行为模式、剥夺效应、社会学习和交叉抚养结果的标准动物行为学推理，分析每个组的观察结果，以对比关于行为发展的相互竞争的假说。\n\n1) 识别先天成分的证据：\n- 社会隔离组的幼狼（第2组）尽管没有接触过同种的同伴，但仍表现出可识别的玩耍元素（例如，猛扑、摔跤尝试）。这表明核心运动模式并非纯粹通过模仿获得；相反，它们在没有正常社交经验的情况下也能出现。因此，该行为不可能完全是后天学习的。这直接与选项B相矛盾。\n\n2) 识别后天学习/校准成分的证据：\n- 同一只社会隔离组的幼狼表现出糟糕的顺序性和社会整合能力（无法识别线索），并且至关重要的是缺乏咬合抑制。这表明对先天模式的适当调节和社交应用需要依赖经验的完善。\n- 交叉抚养组的幼狼（第3组）表现出一种混合风格，将狼典型的摔跤与比格犬典型的吠叫和鞠躬相结合，并表现出存在但对狼来说校准不当的咬合抑制（有时太温柔，有时太用力）。这种模式与通过社交伙伴的反馈进行学习是一致的：强度和时机是根据玩伴的反应进行调整的。因此，正确的表现并非纯粹是先天的。这与选项A相矛盾。\n\n3) 评估关于咬合抑制的“反射”解释：\n- 如果咬合抑制是一种出生时就存在的简单的、非习得性的反射（选项D），那么社会隔离的幼狼在后来的互动中，无论有无社会反馈，都应该仍然表现出这种反射。然而，隔离的幼狼缺乏咬合抑制，而交叉抚养的幼狼的抑制虽然存在但对狼来说校准不当，这与反馈历史相符，而不是一个固定的反射。这证伪了选项D。\n\n4) 评估印记解释：\n- 经典的亲代印记假定在一个关键时期内，对物种特异性模板进行不可逆的习得。交叉抚养的狼保留了狼的运动倾向，同时融入了比格犬特有的元素；其结果是一种混合体，而不是完全的替代，并且没有被描述为不可逆的。此外，社会隔离组的缺陷和对照组的正常发展表明这是一个持续的社会校准过程，而不是一次性印记一个完整的行为库。这与选项E相矛盾。\n\n5) 综合最准确的解释：\n- 综合证据支持，基本的运动模式是先天的，而正确的顺序、社交信号和强度调节（包括咬合抑制）需要社会学习和反馈，这与选项C完全一致。\n\n因此，最准确和最全面的解释是，基本的运动模式是先天的，但其协调的、符合社会规范的表达和强度调节是通过社会经验和学习来完善的。", "answer": "$$\\boxed{C}$$", "id": "2278625"}, {"introduction": "本能反应和后天习得的任务并非总能和谐共存；有时，它们会争夺行为的控制权。此问题设定了一个场景：一条暹罗斗鱼的先天攻击性反应被触发，有可能压倒它先前学会的走迷宫技能。你将应用一个简单的概率模型来量化这种“行为捕获”的强度，亲身体验如何为先天与后天学习系统之间的动态互动建立模型。[@problem_id:1728989]", "problem": "一位行为生物学家正在研究暹罗斗鱼 (*Betta splendens*) 的习得行为与先天反应之间的相互作用。一条鱼首先被训练穿越一个T形迷宫。起点位于'T'字的底部，食物奖励被持续地放置在右侧臂中，而左侧臂则为空。经过长时间的训练后，这条鱼表现出稳定的习得行为，以 $P_{train} = 0.925$ 的概率选择正确的（右侧）臂。选择错误臂的微小概率被归因于随机的探索行为或记忆失误。\n\n在实验的第二阶段，引入了一个干扰刺激。一面镜子被放置在迷宫的错误（左侧）臂中。看到自己倒影的景象作为一个强烈的信号刺激，可靠地触发了一种高度固定的先天攻击性反应，这是一种固定行为模式（FAP）。在这个测试阶段，观察到鱼的表现发生了变化；鱼选择正确（右侧）臂的总概率下降到 $P_{test} = 0.550$。\n\n这位生物学家提出了一个“行为捕获”模型来解释这一变化。该模型假设，在迷宫的决策点，先天刺激有一定的概率 $P_{cap}$ “捕获”鱼的行为，并覆盖其习得的决策过程。\n- 如果先天刺激*没有*捕获鱼的行为（概率为 $1-P_{cap}$），鱼会依赖其训练，并以概率 $P_{train}$ 选择正确的臂。\n- 如果先天刺激*确实*捕获了鱼的行为（概率为 $P_{cap}$），鱼会被FAP所驱使，朝刺激物移动，导致它必然选择错误的（左侧）臂。\n\n根据此模型和提供的实验数据，计算捕获概率 $P_{cap}$ 的数值。将您的最终答案四舍五入到三位有效数字。", "solution": "设 $P_{cap}$ 表示在决策点先天刺激捕获行为的概率。根据应用于测试阶段选择正确（右侧）臂情况的全概率定律，\n$$\nP_{test} = P(\\text{correct}) = P(\\text{not captured})P(\\text{correct} \\mid \\text{not captured}) + P(\\text{captured})P(\\text{correct} \\mid \\text{captured}).\n$$\n根据模型假设，$P(\\text{not captured}) = 1 - P_{cap}$，$P(\\text{correct} \\mid \\text{not captured}) = P_{train}$，以及 $P(\\text{correct} \\mid \\text{captured}) = 0$。因此，\n$$\nP_{test} = (1 - P_{cap})P_{train}.\n$$\n解出 $P_{cap}$：\n$$\nP_{cap} = 1 - \\frac{P_{test}}{P_{train}}.\n$$\n代入给定值 $P_{test} = 0.550$ 和 $P_{train} = 0.925$。为简化计算，将其转换为精确分数：\n$$\n0.550 = \\frac{11}{20}, \\quad 0.925 = \\frac{37}{40},\n$$\n所以\n$$\n\\frac{P_{test}}{P_{train}} = \\frac{\\frac{11}{20}}{\\frac{37}{40}} = \\frac{11}{20} \\cdot \\frac{40}{37} = \\frac{22}{37}.\n$$\n因此，\n$$\nP_{cap} = 1 - \\frac{22}{37} = \\frac{15}{37} \\approx 0.405405\\ldots\n$$\n四舍五入到三位有效数字，得到 $0.405$。", "answer": "$$\\boxed{0.405}$$", "id": "1728989"}, {"introduction": "对于动物而言，决定是固守已知的先天偏好，还是学习探索不确定的新选项，是一项攸关生死的计算。本练习让你扮演一位行为生态学家，为蜂鳥的觅食选择建立模型，并将其与生存和适应度直接联系起来。通过使用一个关于能量储备和适应度的数学模型，你将计算出蜂鳥应从“利用”其先天偏好转向“探索”潜在更佳学习选项的关键阈值，从而阐明最优决策的原则。[@problem_id:2278657]", "problem": "蜂鸟的生存依赖于通过做出最优的觅食决策来有效管理其能量储备。考虑一只蜂鸟所处的环境中有两种花：红花和蓝花。蜂鸟天生偏爱红花，这是一种已知的、可靠的食物来源。蓝花是环境中新出现的物种，其奖励具有不确定性。\n\n蜂鸟当前的能量储备为 $E$。它必须将其能量储备维持在临界饥饿阈值 $E_{crit}$ 之上。拜访任何一朵花（无论是红花还是蓝花）都会产生固定的能量消耗 $C$。\n\n- 红花提供确定且恒定的花蜜奖励 $R_{red}$。\n- 蓝花是一场赌博：它有 $p$ 的概率提供大量的花蜜奖励 $R_{blue}$，有 $(1-p)$ 的概率不提供任何奖励。\n\n蜂鸟的行为目标是选择能使其长期适应度最大化的行动（拜访红花或蓝花）。对于该物种，生物学家已确定其适应度 $V(E)$ 是能量储备 $E$ 的函数，具体如下：\n- 当 $E \\leq E_{crit}$ 时，$V(E) = 0$\n- 当 $E > E_{crit}$ 时，$V(E) = (E - E_{crit})^2$\n\n“利用”已知的红花还是“探索”不确定的蓝花的决策取决于蜂鸟当前的能量储备 $E$。存在一个阈值能量储备 $E^*$，它标志着这两种策略之间的转换点。\n\n给定以下参数：\n- 饥饿阈值, $E_{crit} = 100$ 焦耳\n- 觅食消耗, $C = 10$ 焦耳\n- 红花奖励, $R_{red} = 25$ 焦耳\n- 蓝花奖励, $R_{blue} = 80$ 焦耳\n- 蓝花奖励概率, $p = 0.25$\n\n确定阈值能量储备 $E^*$ 的值，单位为焦耳。高于此阈值时，蜂鸟应利用确定的红花。低于此阈值时，它应探索不确定的蓝花。答案以焦耳 (J) 为单位，并四舍五入到四位有效数字。", "solution": "蜂鸟在每次决策开始时拥有能量储备 $E$。拜访一朵花的消耗为 $C$，因此拜访后的能量为 $E' = E - C + \\text{reward}$。适应度函数为\n$$\nV(E) = \\begin{cases}\n0,  E \\leq E_{crit},\\\\\n(E - E_{crit})^{2},  E > E_{crit}.\n\\end{cases}\n$$\n对于红花（确定性奖励 $R_{red}$），拜访后的能量为 $E'_{red} = E - C + R_{red}$，所以\n$$\nV_{red}(E) = V(E - C + R_{red}) = \\max\\{0,\\,(E - C + R_{red} - E_{crit})^{2}\\}.\n$$\n对于蓝花（概率性奖励：以概率 $p$ 获得 $R_{blue}$，以概率 $1-p$ 获得零），拜访后的能量分别为 $E'_{succ} = E - C + R_{blue}$ 和 $E'_{fail} = E - C$，所以\n$$\nV_{blue}(E) = p\\,V(E - C + R_{blue}) + (1-p)\\,V(E - C) \\\\\n= p\\,\\max\\{0,\\,(E - C + R_{blue} - E_{crit})^{2}\\} + (1-p)\\,\\max\\{0,\\,(E - C - E_{crit})^{2}\\}.\n$$\n\n定义 $y = E - E_{crit} - C$。则\n$$\nV_{red}(E) = \\max\\{0,\\,(y + R_{red})^{2}\\}, \\quad\nV_{blue}(E) = p\\,\\max\\{0,\\,(y + R_{blue})^{2}\\} + (1-p)\\,\\max\\{0,\\,y^{2}\\}.\n$$\n当 $y > 0$（等价于 $E > E_{crit} + C$）时，所有三个最大值函数中的参数都为正，因此期望适应度为\n$$\nV_{red}(E) = (y + R_{red})^{2}, \\quad V_{blue}(E) = p\\,(y + R_{blue})^{2} + (1-p)\\,y^{2}.\n$$\n阈值 $E^{*}$ 是蜂鸟对两种选择无差异的点，即\n$$\n(y + R_{red})^{2} = p\\,(y + R_{blue})^{2} + (1-p)\\,y^{2}.\n$$\n展开并化简：\n$$\ny^{2} + 2R_{red}y + R_{red}^{2} = p\\,(y^{2} + 2R_{blue}y + R_{blue}^{2}) + (1-p)\\,y^{2}\n= y^{2} + 2pR_{blue}y + pR_{blue}^{2}.\n$$\n消去 $y^{2}$ 并解出 $y$：\n$$\n2R_{red}y + R_{red}^{2} = 2pR_{blue}y + pR_{blue}^{2}\n\\;\\;\\Rightarrow\\;\\; 2(R_{red} - pR_{blue})y = pR_{blue}^{2} - R_{red}^{2}\n$$\n$$\n\\Rightarrow\\;\\; y^{*} = \\frac{pR_{blue}^{2} - R_{red}^{2}}{2\\,(R_{red} - pR_{blue})}.\n$$\n那么\n$$\nE^{*} = y^{*} + E_{crit} + C = E_{crit} + C + \\frac{pR_{blue}^{2} - R_{red}^{2}}{2\\,(R_{red} - pR_{blue})}.\n$$\n\n代入给定值 $E_{crit} = 100$, $C = 10$, $R_{red} = 25$, $R_{blue} = 80$, $p = 0.25$：\n$$\ny^{*} = \\frac{0.25 \\times 80^{2} - 25^{2}}{2\\,(25 - 0.25 \\times 80)}\n= \\frac{0.25 \\times 6400 - 625}{2\\,(25 - 20)}\n= \\frac{1600 - 625}{2 \\times 5}\n= \\frac{975}{10}\n= 97.5.\n$$\n因此\n$$\nE^{*} = 100 + 10 + 97.5 = 207.5.\n$$\n这个 $E^{*}$ 满足 $E^{*} > E_{crit} + C$，这验证了期望值中所有项均为正的代数步骤的有效性。当 $E  E^{*}$ 时，$V_{blue}(E) > V_{red}(E)$；当 $E > E^{*}$ 时，$V_{red}(E) > V_{blue}(E)$，这导致了指定的策略转换。", "answer": "$$\\boxed{207.5}$$", "id": "2278657"}]}