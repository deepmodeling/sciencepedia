{"hands_on_practices": [{"introduction": "组蛋白密码的一个基本原则是，翻译后修饰（PTMs）改变了染色质的物理化学性质。赖氨酸乙酰化通过中和正电荷，直接减弱了组蛋白尾部与带负电的DNA骨架之间的静电吸引力。这项练习 [@problem_id:2821692] 提供了一个从第一性原理进行计算的机会，量化评估大范围乙酰化对核小体净电荷的累积效应，从而将分子事件与染色质的宏观物理性质联系起来。", "problem": "组蛋白密码假说强调，组蛋白尾部上的翻译后修饰（PTM）模式，例如赖氨酸乙酰化，部分通过改变静电相互作用来调节染色质功能。考虑一个核小体，其组蛋白尾部总共包含 $N = 44$ 个赖氨酸残基，这些残基是乙酰化的潜在靶点。在生理 pH 值下，由于其质子化的 ε-氨基，每个未修饰的赖氨酸对尾部电荷的贡献约为 $+1$。乙酰化会中和这个正电荷，使该位点的贡献变为 $0$。已知在任何乙酰化发生之前，测得的尾部净电荷为 $Q_{0} = +36$（这是对所有尾部残基的代数和，不一定等于 $N$）。在一个稳态细胞群体中，假设有比例为 $f$ 的尾部赖氨酸被乙酰化，且乙酰化在 $N$ 个位点上是均匀且独立发生的。\n\n仅从 (i) 净电荷是各位点电荷的代数和这一定义，以及 (ii) 赖氨酸位点的乙酰化使其贡献相对于未乙酰化状态改变 $-1$ 这两点出发，推导期望净尾部电荷 $\\mathbb{E}[Q]$ 作为 $f$、$N$ 和 $Q_{0}$ 的函数表达式。然后，在 $f = 0.30$、$N = 44$ 和 $Q_{0} = +36$ 的条件下计算该表达式的值。\n\n将您的最终答案表示为一个无单位的实数，且不要四舍五入。", "solution": "首先对问题陈述进行严格验证。\n\n步骤1：提取已知条件。\n- 组蛋白尾部上潜在的赖氨酸乙酰化位点数量为 $N = 44$。\n- 一个未修饰的赖氨酸的电荷贡献为 $+1$。\n- 一个乙酰化的赖氨酸的电荷贡献为 $0$。\n- 尾部的初始净电荷（乙酰化前）为 $Q_{0} = +36$。\n- 在 $N$ 个赖氨酸位点中，有比例为 $f$ 的位点被乙酰化。\n- 乙酰化在 $N$ 个位点上均匀且独立地发生。\n- 推导必须从两个原则出发：(i) 净电荷是各位点电荷的代数和，以及 (ii) 赖氨酸位点的乙酰化使其电荷贡献改变 $-1$。\n- 任务是推导期望净尾部电荷 $\\mathbb{E}[Q]$ 作为 $f$、$N$ 和 $Q_{0}$ 的函数表达式，然后针对 $f = 0.30$、$N = 44$ 和 $Q_{0} = +36$ 进行求值。\n\n步骤2：使用提取的已知条件进行验证。\n根据有效性标准对问题进行评估。\n- **科学依据**：该问题基于分子生物学中组蛋白密码假说的一个核心宗旨——翻译后修饰（如乙酰化）会改变组蛋白尾部的静电性质。为未修饰 ($+1$) 和乙酰化 ($0$) 的赖氨酸指定的电荷值在生物化学上是正确的。$Q_{0} \\neq N$ 这一事实也是现实的，因为它正确地暗示了除了 $N$ 个指定的赖氨酸之外，还有其他残基对总尾部电荷有贡献。该问题在科学上是合理的。\n- **适定性**：该问题是适定的。它指定了一个概率模型（均匀且独立的乙酰化），并要求计算系统属性（净电荷）的期望值。这是统计力学和概率论中的一个标准问题，保证了唯一且有意义的解。\n- **客观性**：该问题以精确、定量且无偏见的语言陈述。所有术语都有明确的定义。\n\n该问题没有验证清单中列出的任何缺陷。它不是科学上不合理、不可形式化、不完整、矛盾、不现实或不适定的。这是一个有效的科学问题。\n\n步骤3：结论与行动。\n问题有效。将推导解答。\n\n组蛋白尾部的净电荷 $Q$ 可以表示为初始电荷 $Q_{0}$ 与乙酰化导致的总电荷变化量 $\\Delta Q$ 之和。\n$$Q = Q_{0} + \\Delta Q$$\n根据问题的明确指示，单个赖氨酸位点在乙酰化后，其电荷贡献从 $+1$ 变为 $0$，净变化为 $-1$。设 $K$ 为一个随机变量，表示在 $N$ 个可用位点中被乙酰化的赖氨酸残基总数。由于每个乙酰化事件使总电荷减少 $1$，因此总电荷变化量为：\n$$\\Delta Q = K \\times (-1) = -K$$\n将此代入 $Q$ 的表达式中，得到：\n$$Q = Q_{0} - K$$\n我们需要求的是期望净尾部电荷 $\\mathbb{E}[Q]$。根据期望算子的线性性质：\n$$\\mathbb{E}[Q] = \\mathbb{E}[Q_{0} - K] = \\mathbb{E}[Q_{0}] - \\mathbb{E}[K]$$\n由于 $Q_{0}$ 是一个给定的常数值，其期望值就是它本身：$\\mathbb{E}[Q_{0}] = Q_{0}$。\n$$\\mathbb{E}[Q] = Q_{0} - \\mathbb{E}[K]$$\n下一步是确定 $\\mathbb{E}[K]$，即期望的乙酰化赖氨酸数量。问题陈述，在稳态细胞群体中，乙酰化在 $N$ 个位点中的每一个上都均匀且独立地发生，且有比例为 $f$ 的位点被乙酰化。这等同于陈述任何一个给定的赖氨酸位点被乙酰化的概率为 $p = f$。\n\n我们定义 $N$ 个独立的伯努利随机变量 $X_{i}$（对于 $i = 1, 2, \\dots, N$），其中如果第 $i$ 个赖氨酸被乙酰化，则 $X_{i} = 1$，如果未被乙酰化，则 $X_{i} = 0$。成功（乙酰化）的概率为 $P(X_{i} = 1) = f$。\n乙酰化的赖氨酸总数是这些随机变量的和：\n$$K = \\sum_{i=1}^{N} X_{i}$$\n根据期望的线性性质，$K$ 的期望值为：\n$$\\mathbb{E}[K] = \\mathbb{E}\\left[\\sum_{i=1}^{N} X_{i}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[X_{i}]$$\n单个伯努利变量 $X_{i}$ 的期望值为：\n$$\\mathbb{E}[X_{i}] = 1 \\cdot P(X_{i} = 1) + 0 \\cdot P(X_{i} = 0) = 1 \\cdot f + 0 \\cdot (1-f) = f$$\n因此，期望的乙酰化赖氨酸总数为：\n$$\\mathbb{E}[K] = \\sum_{i=1}^{N} f = N \\cdot f$$\n将此结果代回 $\\mathbb{E}[Q]$ 的表达式中，得到所需的通用公式：\n$$\\mathbb{E}[Q] = Q_{0} - Nf$$\n该表达式给出了期望净电荷作为初始电荷 $Q_{0}$、位点数 $N$ 和乙酰化比例 $f$ 的函数。\n\n最后，我们必须用给定的值计算这个表达式：$Q_{0} = 36$，$N = 44$，以及 $f = 0.30$。\n$$\\mathbb{E}[Q] = 36 - (44)(0.30)$$\n计算很简单：\n$$(44)(0.30) = 13.2$$\n$$\\mathbb{E}[Q] = 36 - 13.2 = 22.8$$\n在指定条件下，组蛋白尾部的期望净电荷是 $22.8$。", "answer": "$$\\boxed{22.8}$$", "id": "2821692"}, {"introduction": "组蛋白修饰的生物学功能不仅取决于它们如何改变染色质的物理性质，还取决于它们如何被特定的蛋白质“阅读器”结构域识别。这项练习 [@problem_id:2821678] 探讨了染色质结构域（chromodomain）通过芳香笼识别H3K9me3的分子机制，特别是阳离子–$\\pi$相互作用的能量贡献。通过将微观的相互作用能与宏观的结合亲和力（$K_d$）联系起来，本练习阐明了如何利用生物物理学原理来预测突变对分子识别事件的影响。", "problem": "一个着色域通过一个预组织的芳香笼识别组蛋白H3赖氨酸$9$三甲基化（H3K9me3），该芳香笼通过阳离子–$\\pi$相互作用和疏水去溶剂化与三甲基化赖氨酸的季铵基团结合。考虑一个典型的着色域，其野生型对H3K9me3的平衡解离常数在温度$T = 298\\,\\mathrm{K}$时为$K_d^{\\mathrm{WT}} = 1.0\\,\\mu\\mathrm{M}$。该芳香笼包含$3$个芳香族残基；将单个酪氨酸（$\\mathrm{Tyr}$）替换为丙氨酸（$\\mathrm{Ala}$）会移除一个阳离子–$\\pi$接触，但结合口袋在其他方面结构保持完整。\n\n假设以下基本事实和常数成立：\n- 标准结合自由能与平衡解离常数的关系为$\\Delta G = R T \\ln K_d$，其中$R = 1.987 \\times 10^{-3}\\,\\mathrm{kcal\\,mol^{-1}\\,K^{-1}}$。\n- 在水溶液中，三甲基赖氨酸的季铵基团保持一个完整的正电荷，在该野生型背景下，此阳离子与单个芳香环之间的吸引性阳离子–$\\pi$相互作用对结合的贡献约为$\\Delta G_{\\mathrm{c}\\mbox{--}\\pi} \\approx -2.0\\,\\mathrm{kcal\\,mol^{-1}}$。\n- 所有其他能量贡献（氢键、一般疏水效应和构象预组织）不受$\\text{Tyr}\\to\\text{Ala}$突变的影响，并且剩余的$2$个芳香族残基继续与配体结合。\n\n仅使用这些原则，预测芳香笼中一个残基的突变（$\\text{Tyr}\\to\\text{Ala}$）对H3K9me3结合亲和力的影响，并根据阳离子–$\\pi$相互作用的能量贡献证明你的选择。选择唯一的最佳选项。\n\nA. $K_d$增加到约$30\\,\\mu\\mathrm{M}$，因为移除一个阳离子–$\\pi$接触使结合不稳定约$2.0\\,\\mathrm{kcal\\,mol^{-1}}$，导致亲和力降低$\\exp(\\Delta\\Delta G/RT)$倍。\n\nB. $K_d$降低到约$0.03\\,\\mu\\mathrm{M}$，因为移除一个酪氨酸减少了三甲基赖氨酸的去溶剂化罚能，尽管失去了阳离子–$\\pi$接触，但结合反而增强了。\n\nC. 结合被有效消除（$K_d \\gtrsim 1\\,\\mathrm{mM}$），因为芳香笼需要所有$3$个芳香族残基才能对H3K9me3进行任何可检测的识别。\n\nD. $K_d$适度增加到约$3\\,\\mu\\mathrm{M}$，因为侧链柔性增加带来的熵增益部分补偿了失去一个阳离子–$\\pi$相互作用的损失，将总体罚能限制在远低于$1.0\\,\\mathrm{kcal\\,mol^{-1}}$的水平。", "solution": "在尝试任何解答之前，必须首先对问题陈述进行严格的验证。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n以下数据和条件是逐字或直接从问题陈述中提取的：\n- **系统：** 一个着色域蛋白与组蛋白H3赖氨酸$9$三甲基化（H3K9me3）配体结合。\n- **识别机制：** 着色域内的芳香笼通过阳离子–$\\pi$相互作用和疏水去溶剂化与三甲基化赖氨酸的季铵基团结合。\n- **野生型结合亲和力：** 野生型蛋白的平衡解离常数为$K_d^{\\mathrm{WT}} = 1.0\\,\\mu\\mathrm{M}$。\n- **温度：** 实验在$T = 298\\,\\mathrm{K}$下进行。\n- **野生型结构：** 芳香笼包含$3$个芳香族残基。\n- **突变：** 引入了单个酪氨酸到丙氨酸（$\\text{Tyr}\\to\\text{Ala}$）的替换。\n- **突变的后果：** 该突变移除了一个阳离子–$\\pi$接触。陈述说明结合口袋“在其他方面结构保持完整”。\n- **热力学关系：** 标准结合自由能由$\\Delta G = R T \\ln K_d$给出。\n- **气体常数：** $R = 1.987 \\times 10^{-3}\\,\\mathrm{kcal\\,mol^{-1}\\,K^{-1}}$。\n- **阳离子–π相互作用的能量贡献：** 单个阳离子–$\\pi$相互作用的贡献被给出为$\\Delta G_{\\mathrm{c}\\mbox{--}\\pi} \\approx -2.0\\,\\mathrm{kcal\\,mol^{-1}}$。\n- **核心假设：** 假设所有其他能量贡献不受突变影响，并且剩余的$2$个芳香族残基继续发挥作用。\n\n**步骤2：使用提取的已知条件进行验证**\n\n根据所需标准对问题进行评估：\n- **有科学依据：** 该问题牢固地植根于生物物理化学和分子生物学的既定原则。着色域与H3K9me3之间的相互作用、芳香笼的作用以及阳离子–$\\pi$相互作用的能量贡献都是有充分文献记载的科学事实。所提供的$K_d$、$T$、$R$和$\\Delta G_{\\mathrm{c}\\mbox{--}\\pi}$值在此背景下是物理上现实且标准的。\n- **提法恰当：** 该问题提供了所有必要的信息和一组清晰的简化假设，从而可以计算出唯一且有意义的解。它要求根据所提供的模型进行特定的定量预测。\n- **客观性：** 语言是技术性的、精确的，并且没有主观或模糊的术语。\n\n**步骤3：结论与行动**\n\n问题陈述在科学上是合理的、提法是恰当的、并且是客观的。它不包含内部矛盾、事实错误或模糊之处。因此，该问题是**有效的**。我将继续推导解答。\n\n### 解答推导\n\n解答是从结合吉布斯自由能的变化（$\\Delta\\Delta G$）与平衡解离常数（$K_d$）变化之间的基本热力学关系推导出来的。\n\n标准结合吉布斯自由能$\\Delta G$与平衡解离常数$K_d$通过以下方程相关联：\n$$ \\Delta G = R T \\ln K_d $$\n对于野生型（WT）和突变型（MUT）蛋白，我们有：\n$$ \\Delta G^{\\mathrm{WT}} = R T \\ln K_d^{\\mathrm{WT}} $$\n$$ \\Delta G^{\\mathrm{MUT}} = R T \\ln K_d^{\\mathrm{MUT}} $$\n\n由突变引起的结合自由能变化$\\Delta\\Delta G$是突变型和野生型自由能之间的差值：\n$$ \\Delta\\Delta G = \\Delta G^{\\mathrm{MUT}} - \\Delta G^{\\mathrm{WT}} = R T \\ln K_d^{\\mathrm{MUT}} - R T \\ln K_d^{\\mathrm{WT}} $$\n$$ \\Delta\\Delta G = R T \\ln\\left(\\frac{K_d^{\\mathrm{MUT}}}{K_d^{\\mathrm{WT}}}\\right) $$\n\n问题陈述指出，$\\text{Tyr}\\to\\text{Ala}$突变移除了一个阳离子–$\\pi$相互作用，该相互作用对结合能的贡献为$\\Delta G_{\\mathrm{c}\\mbox{--}\\pi} \\approx -2.0\\,\\mathrm{kcal\\,mol^{-1}}$。移除一个有利的相互作用会使复合物不稳定，这意味着突变体的结合自由能变得更小的负值（即增加）。因此，自由能的变化是正的：\n$$ \\Delta\\Delta G = -(\\Delta G_{\\mathrm{c}\\mbox{--}\\pi}) = -(-2.0\\,\\mathrm{kcal\\,mol^{-1}}) = +2.0\\,\\mathrm{kcal\\,mol^{-1}} $$\n\n现在，我们可以求解新的解离常数$K_d^{\\mathrm{MUT}}$：\n$$ \\frac{K_d^{\\mathrm{MUT}}}{K_d^{\\mathrm{WT}}} = \\exp\\left(\\frac{\\Delta\\Delta G}{R T}\\right) $$\n$$ K_d^{\\mathrm{MUT}} = K_d^{\\mathrm{WT}} \\exp\\left(\\frac{\\Delta\\Delta G}{R T}\\right) $$\n\n我们代入给定值：\n- $\\Delta\\Delta G = 2.0\\,\\mathrm{kcal\\,mol^{-1}}$\n- $R = 1.987 \\times 10^{-3}\\,\\mathrm{kcal\\,mol^{-1}\\,K^{-1}}$\n- $T = 298\\,\\mathrm{K}$\n- $K_d^{\\mathrm{WT}} = 1.0\\,\\mu\\mathrm{M}$\n\n首先，计算$R T$的值：\n$$ R T = (1.987 \\times 10^{-3}\\,\\mathrm{kcal\\,mol^{-1}\\,K^{-1}}) \\times (298\\,\\mathrm{K}) \\approx 0.5921\\,\\mathrm{kcal\\,mol^{-1}} $$\n\n接下来，计算指数部分：\n$$ \\frac{\\Delta\\Delta G}{R T} = \\frac{2.0\\,\\mathrm{kcal\\,mol^{-1}}}{0.5921\\,\\mathrm{kcal\\,mol^{-1}}} \\approx 3.377 $$\n\n最后，计算$K_d^{\\mathrm{MUT}}$：\n$$ K_d^{\\mathrm{MUT}} = (1.0\\,\\mu\\mathrm{M}) \\times \\exp(3.377) \\approx 1.0\\,\\mu\\mathrm{M} \\times 29.28 $$\n$$ K_d^{\\mathrm{MUT}} \\approx 29.3\\,\\mu\\mathrm{M} $$\n\n这个计算值约为$30\\,\\mu\\mathrm{M}$。$K_d$的增加意味着结合变弱，这是移除一个有利相互作用后的预期结果。\n\n### 逐项分析选项\n\n**A. $K_d$增加到约$30\\,\\mu\\mathrm{M}$，因为移除一个阳离子–$\\pi$接触使结合不稳定约$2.0\\,\\mathrm{kcal\\,mol^{-1}}$，导致亲和力降低$\\exp(\\Delta\\Delta G/RT)$倍。**\n- 计算出的$K_d^{\\mathrm{MUT}}$值为$\\approx 29.3\\,\\mu\\mathrm{M}$，可以很好地近似为$30\\,\\mu\\mathrm{M}$。\n- 其推理过程完全正确。失去阳离子–$\\pi$相互作用代表了$+2.0\\,\\mathrm{kcal\\,mol^{-1}}$的去稳定化（正的$\\Delta\\Delta G$）。\n- 解离常数的变化被正确地确定为乘以一个因子$\\exp(\\Delta\\Delta G/RT)$。\n- **结论：正确**\n\n**B. $K_d$降低到约$0.03\\,\\mu\\mathrm{M}$，因为移除一个酪氨酸减少了三甲基赖氨酸的去溶剂化罚能，尽管失去了阳离子–$\\pi$接触，但结合反而增强了。**\n- $K_d$的降低意味着更强的结合。这与移除一个吸引力的物理现实相矛盾。$K_d$为$0.03\\,\\mu\\mathrm{M}$将对应于$\\Delta\\Delta G$为$-2.0\\,\\mathrm{kcal\\,mol^{-1}}$。\n- 该推理援引了去溶剂化罚能的变化。问题明确禁止了这种推理思路，因为它声明“所有其他能量贡献...保持不变”。此选项违反了给定的假设。\n- **结论：不正确**\n\n**C. 结合被有效消除（$K_d \\gtrsim 1\\,\\mathrm{mM}$），因为芳香笼需要所有$3$个芳香族残基才能对H3K9me3进行任何可检测的识别。**\n- $K_d$为$1\\,\\mathrm{mM}$（$1000\\,\\mu\\mathrm{M}$）代表了比野生型增加了$1000$倍。这将需要$\\Delta\\Delta G = R T \\ln(1000) \\approx 0.592 \\times 6.908 \\approx 4.1\\,\\mathrm{kcal\\,mol^{-1}}$。这超过了问题中指定失去单个接触的罚能的两倍。\n- 该推理假定了极端的正协同性，即如果移除一个组分，笼子就失去功能。这与问题的“另外两个芳香族残基‘继续与配体结合’”以及能量罚能仅限于失去单个接触的前提相矛盾。\n- **结论：不正确**\n\n**D. $K_d$适度增加到约$3\\,\\mu\\mathrm{M}$，因为侧链柔性增加带来的熵增益部分补偿了失去一个阳离子–$\\pi$相互作用的损失，将总体罚能限制在远低于$1.0\\,\\mathrm{kcal\\,mol^{-1}}$。**\n- $K_d$增加到$3\\,\\mu\\mathrm{M}$对应于$\\Delta\\Delta G = R T \\ln(3) \\approx 0.592 \\times 1.0986 \\approx 0.65\\,\\mathrm{kcal\\,mol^{-1}}$。\n- 该推理引入了熵补偿效应。与选项B一样，这违反了“所有其他能量贡献...保持不变”的明确指示。问题要求仅基于失去$+2.0\\,\\mathrm{kcal\\,mol^{-1}}$相互作用进行计算。\n- **结论：不正确**", "answer": "$$\\boxed{A}$$", "id": "2821678"}, {"introduction": "在全基因组尺度上，单个组蛋白修饰往往不足以定义一个功能元件；相反，是多种修饰的组合模式（即“染色质状态”）与特定的生物学功能相关联。这项练习 [@problem_id:2821688] 引导你将这一概念形式化，通过构建一个高斯朴素贝叶斯分类器来区分启动子、增强子和Polycomb抑制区域。这项实践将基础的统计学习方法应用于真实的染色质免疫沉淀测序（ChIP-seq）数据，是现代计算表观基因组学中的一项核心技能。", "problem": "给定来自染色质免疫沉淀测序（ChIP-seq）的三种组蛋白修饰（H3K4me3、H3K27ac 和 H3K27me3）在基因组区域上的归一化信号强度。每个区域表示为一个实值向量 $\\mathbb{R}^3$，其分量分别对应于 H3K4me3、H3K27ac 和 H3K27me3。生物学前提（通常被观察到且经过充分检验）是：启动子区域的特征是高 H3K4me3、中到高 H3K27ac 和低 H3K27me3；增强子区域的特征是低 H3K4me3、高 H3K27ac 和低 H3K27me3；而Polycomb抑制区域的特征是高 H3K27me3、低 H3K4me3 和 H3K27ac。您的任务是，基于贝叶斯定理，使用一种有原则的贝叶斯方法，将这些区域形式化地分类为启动子、增强子或Polycomb抑制状态。\n\n从贝叶斯定理和给定类别下特征独立的假设（即，给定染色质状态，每个组蛋白标记都是条件独立的）出发，实现一个分类器，该分类器将每个组蛋白标记视为具有类条件高斯分布的连续随机变量。使用以下基本原理：\n- 贝叶斯定理：$P(C \\mid \\mathbf{x}) \\propto P(C)\\,P(\\mathbf{x}\\mid C)$。\n- 朴素条件独立性：$P(\\mathbf{x}\\mid C) = \\prod_{j=1}^{3} P(x_j \\mid C)$。\n- 每个特征的高斯似然：$P(x_j \\mid C) = \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j})$，其中 $\\mu_{C,j}$ 和 $\\sigma^2_{C,j}$ 是类别 $C$ 中特征 $j$ 的类条件均值和方差。\n- 对于每个类别 $C$ 的最大似然估计，使用 $n_C$ 个训练样本 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^{n_C}$：$\\mu_{C,j} = \\frac{1}{n_C}\\sum_{i=1}^{n_C} x^{(i)}_j$ 和 $\\sigma^2_{C,j} = \\frac{1}{n_C}\\sum_{i=1}^{n_C} \\left(x^{(i)}_j - \\mu_{C,j}\\right)^2$。\n- 经验类先验：$P(C) = \\frac{n_C}{N}$，其中 $N$ 是所有类别中的训练样本总数。\n- 数值稳定性要求：在对数域中进行计算，使用 $\\log P(\\mathbf{x}\\mid C) = \\sum_{j=1}^{3} \\left[-\\frac{1}{2}\\log\\left(2\\pi\\sigma^2_{C,j}\\right) - \\frac{(x_j - \\mu_{C,j})^2}{2\\sigma^2_{C,j}}\\right]$，并将方差正则化为 $\\sigma^2_{C,j} \\leftarrow \\sigma^2_{C,j} + \\epsilon$，其中 $\\epsilon = 10^{-6}$，以避免零方差。如果后验概率出现完全相等的情况，选择索引最小的类别。\n\n类别编码为整数：启动子 $\\rightarrow 0$，增强子 $\\rightarrow 1$，Polycomb抑制 $\\rightarrow 2$。\n\n实现这个高斯朴素贝叶斯（GNB）分类器，并评估其分类准确率，准确率定义为 $\\text{accuracy} = \\frac{\\text{正确分类的测试样本数}}{\\text{测试样本总数}}$，以小数形式表示。您的程序必须计算以下三个测试用例的准确率，每个测试用例都有指定的训练集和测试集。请勿引入任何随机性。\n\n测试用例 1（类条件结构分离良好）：\n- 训练集 $\\mathbf{X}_{\\text{train}}$（行为样本，列为 [H3K4me3, H3K27ac, H3K27me3]）：\n  - 启动子 ($y=0$): $[10.0, 4.0, 0.5]$, $[9.5, 3.8, 0.4]$, $[10.2, 4.1, 0.6]$, $[9.8, 4.2, 0.5]$, $[10.1, 3.9, 0.5]$\n  - 增强子 ($y=1$): $[1.0, 8.5, 0.5]$, $[0.9, 8.8, 0.6]$, $[1.2, 8.9, 0.4]$, $[1.1, 8.6, 0.5]$, $[0.8, 8.7, 0.5]$\n  - Polycomb抑制 ($y=2$): $[0.5, 0.6, 9.5]$, $[0.4, 0.5, 9.8]$, $[0.6, 0.5, 9.7]$, $[0.5, 0.4, 9.6]$, $[0.6, 0.6, 9.9]$\n- 测试集 $\\mathbf{X}_{\\text{test}}$ 和标签 $\\mathbf{y}_{\\text{test}}$:\n  - $[9.9, 4.0, 0.5]\\rightarrow 0$, $[10.3, 4.1, 0.7]\\rightarrow 0$, $[1.0, 8.7, 0.5]\\rightarrow 1$, $[1.3, 8.4, 0.6]\\rightarrow 1$, $[0.5, 0.5, 9.6]\\rightarrow 2$, $[0.7, 0.7, 9.8]\\rightarrow 2$\n\n测试用例 2（启动子和增强子在 H3K27ac 上重叠；分离依赖于 H3K4me3）：\n- 训练集：\n  - 启动子 ($y=0$): $[5.5, 7.5, 1.0]$, $[5.8, 7.8, 1.2]$, $[5.2, 7.2, 0.8]$, $[5.6, 7.6, 1.1]$\n  - 增强子 ($y=1$): $[1.2, 7.7, 1.0]$, $[1.0, 7.5, 1.1]$, $[1.5, 7.9, 0.9]$, $[1.3, 7.6, 1.2]$\n  - Polycomb抑制 ($y=2$): $[1.0, 1.5, 7.8]$, $[0.8, 1.2, 8.0]$, $[1.2, 1.3, 7.6]$, $[1.1, 1.4, 8.2]$\n- 测试集和标签：\n  - $[5.4, 7.4, 1.0]\\rightarrow 0$, $[1.4, 7.4, 1.1]\\rightarrow 1$, $[1.0, 1.3, 8.1]\\rightarrow 2$, $[3.2, 7.5, 1.0]\\rightarrow 1$\n\n测试用例 3（需要正则化的退化方差）：\n- 训练集：\n  - 启动子 ($y=0$): $[2.0, 4.0, 0.5]$, $[2.0, 4.0, 0.5]$, $[2.0, 4.0, 0.5]$\n  - 增强子 ($y=1$): $[1.8, 4.0, 0.4]$, $[1.8, 4.0, 0.4]$, $[1.8, 4.0, 0.4]$\n  - Polycomb抑制 ($y=2$): $[0.2, 0.2, 6.0]$, $[0.2, 0.2, 6.0]$, $[0.2, 0.2, 6.0]$\n- 测试集和标签：\n  - $[1.9, 4.0, 0.45]\\rightarrow 0$, $[1.8, 4.0, 0.4]\\rightarrow 1$, $[0.2, 0.2, 6.0]\\rightarrow 2$\n\n您的程序必须：\n- 实现训练以估计 $\\mu_{C,j}$、$\\sigma^2_{C,j}$ 和 $P(C)$。\n- 通过最大化 $\\log P(C) + \\sum_{j=1}^{3} \\log \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j} + \\epsilon)$ 实现预测，其中 $\\epsilon = 10^{-6}$。\n- 以小数形式计算每个测试用例的准确率。\n\n最终输出格式：\n- 生成单行输出，包含测试用例 1、测试用例 2 和测试用例 3 的准确率，按此顺序排列，形式为逗号分隔的列表，并用方括号括起，每个值精确到小数点后 $4$ 位，例如字符串：`[a,b,c]`。", "solution": "该问题要求设计并实现一个高斯朴素贝叶斯（GNB）分类器，用于将基因组区域分类为三种染色质状态之一：启动子（类别 0）、增强子（类别 1）或Polycomb抑制（类别 2）。分类基于一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^3$，该向量表示三种组蛋白修饰的信号强度：H3K4me3 ($x_1$)、H3K27ac ($x_2$) 和 H3K27me3 ($x_3$)。\n\n该问题在科学上基础扎实，在数学上定义明确，并为唯一解提供了所有必要信息。其生物学前提合理，指定的统计模型是此类分类任务的标准方法。为保证实现的鲁棒性，关于数值稳定性的规定至关重要。因此，该问题是有效的，我们将继续推导和实现解决方案。\n\n解决方案的核心是贝叶斯定理，它提供了一个在给定观测数据 $\\mathbf{x}$ 的情况下更新我们对类别 $C$ 的信念的规则：\n$$\nP(C \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid C) P(C)}{P(\\mathbf{x})}\n$$\n对于分类问题，我们寻求使后验概率 $P(C \\mid \\mathbf{x})$ 最大化的类别 $\\hat{C}$。由于对于给定的数据点 $\\mathbf{x}$，$P(\\mathbf{x})$ 对所有类别都是常数，因此这等价于最大化类条件似然 $P(\\mathbf{x} \\mid C)$ 和类先验 $P(C)$ 的乘积：\n$$\n\\hat{C} = \\arg\\max_{C} P(\\mathbf{x} \\mid C) P(C)\n$$\n朴素贝叶斯分类器的“朴素”假设是，在给定类别 $C$ 的条件下，特征 $x_j$ 是条件独立的。这简化了似然项：\n$$\nP(\\mathbf{x} \\mid C) = \\prod_{j=1}^{3} P(x_j \\mid C)\n$$\n每个特征的类条件分布 $P(x_j \\mid C)$ 被建模为高斯（正态）分布 $\\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j})$，其中 $\\mu_{C,j}$ 和 $\\sigma^2_{C,j}$ 分别是类别 $C$ 中特征 $j$ 的均值和方差。\n\n为了避免小概率导致的数值下溢并为了计算方便，我们使用后验概率的对数。决策规则变为：\n$$\n\\hat{C} = \\arg\\max_{C} \\left[ \\log P(C) + \\sum_{j=1}^{3} \\log P(x_j \\mid C) \\right]\n$$\n这个表达式通常被称为判别函数 $g_C(\\mathbf{x})$。\n\n训练阶段包括从带标签的训练集中估计模型参数。\n设类别 $C$ 的训练集为 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^{n_C}$，其中 $n_C$ 是类别 $C$ 中的样本数。设 $N$ 为训练样本总数。\n参数使用最大似然估计（MLE）进行估计：\n\n1.  **类先验**，$P(C)$：训练数据中每个类别的经验频率。\n    $$\n    P(C) = \\frac{n_C}{N}\n    $$\n\n2.  **类条件均值**，$\\mu_{C,j}$：属于类别 $C$ 的所有训练样本中特征 $j$ 的样本均值。\n    $$\n    \\mu_{C,j} = \\frac{1}{n_C} \\sum_{i=1}^{n_C} x^{(i)}_j\n    $$\n\n3.  **类条件方差**，$\\sigma^2_{C,j}$：属于类别 $C$ 的所有训练样本中特征 $j$ 的样本方差。这是方差的有偏估计，与 MLE 的公式一致。\n    $$\n    \\sigma^2_{C,j} = \\frac{1}{n_C} \\sum_{i=1}^{n_C} (x^{(i)}_j - \\mu_{C,j})^2\n    $$\n\n对于预测，我们使用这些估计出的参数来为一个新的数据点 $\\mathbf{x}$ 计算判别函数 $g_C(\\mathbf{x})$。高斯概率密度函数的对数公式为：\n$$\n\\log P(x_j \\mid C) = \\log \\mathcal{N}(x_j \\mid \\mu_{C,j}, \\sigma^2_{C,j}) = -\\frac{1}{2} \\log(2\\pi\\sigma^2_{C,j}) - \\frac{(x_j - \\mu_{C,j})^2}{2\\sigma^2_{C,j}}\n$$\n一个关键的实现细节是处理零方差的情况，当一个类别中的所有训练样本在某个特征上具有相同的值时，就会发生这种情况。这将导致除以零或对零取对数。为防止这种情况，将一个小的正则化常数 $\\epsilon = 10^{-6}$ 添加到每个估计的方差中：\n$$\n\\sigma^2_{C,j, \\text{reg}} = \\sigma^2_{C,j} + \\epsilon\n$$\n需要最大化的最终判别函数是：\n$$\ng_C(\\mathbf{x}) = \\log P(C) + \\sum_{j=1}^{3} \\left[ -\\frac{1}{2} \\log(2\\pi \\sigma^2_{C,j, \\text{reg}}) - \\frac{(x_j - \\mu_{C,j})^2}{2 \\sigma^2_{C,j, \\text{reg}}} \\right]\n$$\n对于给定的 $\\mathbf{x}$，预测的类别 $\\hat{C}$ 是使 $g_C(\\mathbf{x})$ 值最高的类别。如果出现平局，选择整数索引最小的类别。\n\n最后，使用准确率来评估分类器的性能，准确率定义为测试集中被正确分类的样本所占的比例：\n$$\n\\text{accuracy} = \\frac{\\sum_{i=1}^{m} I(\\hat{y}_i = y_i)}{m}\n$$\n其中 $m$ 是测试样本的数量， $y_i$ 是真实标签，$\\hat{y}_i$ 是第 $i$ 个测试样本的预测标签， $I(\\cdot)$ 是指示函数。\n\n实现将包含一个封装此逻辑的类。一个 `fit` 方法将从训练数据中估计参数，一个 `predict` 方法将使用这些参数对新数据点进行分类。我们将通过在每个测试用例对应的训练集上训练模型，并在其测试集上评估准确率来处理每个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Gaussian Naive Bayes classifier and evaluates its accuracy\n    on three provided test cases related to chromatin state classification.\n    \"\"\"\n\n    class GaussianNaiveBayes:\n        \"\"\"\n        A Gaussian Naive Bayes classifier.\n\n        Parameters are estimated using Maximum Likelihood Estimation.\n        \"\"\"\n        def __init__(self, var_smoothing=1e-6):\n            self.var_smoothing = var_smoothing\n            self.class_priors_ = None\n            self.means_ = None\n            self.variances_ = None\n            self.classes_ = None\n\n        def fit(self, X, y):\n            \"\"\"\n            Train the classifier by estimating parameters from the data.\n\n            Args:\n                X (np.ndarray): Training data of shape (n_samples, n_features).\n                y (np.ndarray): Target values of shape (n_samples,).\n            \"\"\"\n            self.classes_ = np.unique(y)\n            n_samples, n_features = X.shape\n            n_classes = len(self.classes_)\n\n            self.means_ = np.zeros((n_classes, n_features))\n            self.variances_ = np.zeros((n_classes, n_features))\n            self.class_priors_ = np.zeros(n_classes)\n\n            for idx, c in enumerate(self.classes_):\n                X_c = X[y == c]\n                self.means_[idx, :] = X_c.mean(axis=0)\n                # Using ddof=0 for MLE variance, as specified by the problem (1/n_C)\n                self.variances_[idx, :] = X_c.var(axis=0, ddof=0)\n                self.class_priors_[idx] = X_c.shape[0] / float(n_samples)\n\n        def predict(self, X):\n            \"\"\"\n            Perform classification on an array of test vectors X.\n\n            Args:\n                X (np.ndarray): Test data of shape (n_samples, n_features).\n\n            Returns:\n                np.ndarray: Predicted class labels for each sample in X.\n            \"\"\"\n            # Add variance smoothing for numerical stability\n            vars_reg = self.variances_ + self.var_smoothing\n\n            log_priors = np.log(self.class_priors_)\n\n            # The log posterior is proportional to log_prior + log_likelihood\n            # log_likelihood for a Gaussian is sum over features of:\n            # -0.5 * log(2*pi*var) - 0.5 * ((x-mu)^2 / var)\n            \n            log_posteriors = []\n            for x_sample in X:\n                joint_log_likelihood = []\n                for i in range(len(self.classes_)):\n                    log_likelihood_class = -0.5 * np.sum(np.log(2. * np.pi * vars_reg[i, :]))\n                    log_likelihood_class -= 0.5 * np.sum(((x_sample - self.means_[i, :]) ** 2) / vars_reg[i, :])\n                    joint_log_likelihood.append(log_priors[i] + log_likelihood_class)\n                log_posteriors.append(joint_log_likelihood)\n\n            # The class with the highest log posterior is the prediction.\n            # np.argmax handles ties by returning the first index, which matches\n            # the problem's tie-breaking rule (smallest class index).\n            predictions = np.argmax(log_posteriors, axis=1)\n            return self.classes_[predictions]\n            \n    # Define test cases from the problem statement\n    test_cases = [\n        {\n            \"train_set\": {\n                0: np.array([[10.0, 4.0, 0.5], [9.5, 3.8, 0.4], [10.2, 4.1, 0.6], [9.8, 4.2, 0.5], [10.1, 3.9, 0.5]]),\n                1: np.array([[1.0, 8.5, 0.5], [0.9, 8.8, 0.6], [1.2, 8.9, 0.4], [1.1, 8.6, 0.5], [0.8, 8.7, 0.5]]),\n                2: np.array([[0.5, 0.6, 9.5], [0.4, 0.5, 9.8], [0.6, 0.5, 9.7], [0.5, 0.4, 9.6], [0.6, 0.6, 9.9]]),\n            },\n            \"test_set\": np.array([\n                [9.9, 4.0, 0.5], [10.3, 4.1, 0.7], \n                [1.0, 8.7, 0.5], [1.3, 8.4, 0.6], \n                [0.5, 0.5, 9.6], [0.7, 0.7, 9.8]\n            ]),\n            \"test_labels\": np.array([0, 0, 1, 1, 2, 2]),\n        },\n        {\n            \"train_set\": {\n                0: np.array([[5.5, 7.5, 1.0], [5.8, 7.8, 1.2], [5.2, 7.2, 0.8], [5.6, 7.6, 1.1]]),\n                1: np.array([[1.2, 7.7, 1.0], [1.0, 7.5, 1.1], [1.5, 7.9, 0.9], [1.3, 7.6, 1.2]]),\n                2: np.array([[1.0, 1.5, 7.8], [0.8, 1.2, 8.0], [1.2, 1.3, 7.6], [1.1, 1.4, 8.2]]),\n            },\n            \"test_set\": np.array([\n                [5.4, 7.4, 1.0], [1.4, 7.4, 1.1], \n                [1.0, 1.3, 8.1], [3.2, 7.5, 1.0]\n            ]),\n            \"test_labels\": np.array([0, 1, 2, 1]),\n        },\n        {\n            \"train_set\": {\n                0: np.array([[2.0, 4.0, 0.5], [2.0, 4.0, 0.5], [2.0, 4.0, 0.5]]),\n                1: np.array([[1.8, 4.0, 0.4], [1.8, 4.0, 0.4], [1.8, 4.0, 0.4]]),\n                2: np.array([[0.2, 0.2, 6.0], [0.2, 0.2, 6.0], [0.2, 0.2, 6.0]]),\n            },\n            \"test_set\": np.array([\n                [1.9, 4.0, 0.45], [1.8, 4.0, 0.4], [0.2, 0.2, 6.0]\n            ]),\n            \"test_labels\": np.array([0, 1, 2]),\n        },\n    ]\n\n    accuracies = []\n    \n    for case in test_cases:\n        # Prepare training data\n        X_train_list, y_train_list = [], []\n        for label, data in case[\"train_set\"].items():\n            X_train_list.append(data)\n            y_train_list.extend([label] * data.shape[0])\n        \n        X_train = np.vstack(X_train_list)\n        y_train = np.array(y_train_list)\n        \n        # Prepare test data\n        X_test = case[\"test_set\"]\n        y_test = case[\"test_labels\"]\n        \n        # Initialize and train the classifier\n        gnb = GaussianNaiveBayes(var_smoothing=1e-6)\n        gnb.fit(X_train, y_train)\n        \n        # Make predictions\n        y_pred = gnb.predict(X_test)\n        \n        # Calculate accuracy\n        accuracy = np.mean(y_pred == y_test)\n        accuracies.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{acc:.4f}' for acc in accuracies])}]\")\n\nsolve()\n```", "id": "2821688"}]}