{"hands_on_practices": [{"introduction": "在合成生物学中，将希尔（Hill）函数等非线性模型拟合到剂量反应数据是一项基本任务。然而，由于成本函数的非凸性，标准的优化算法可能会陷入局部最优解，导致参数估计不准确。本练习将介绍一种强大的数值技术——同伦延拓法（homotopy continuation），通过解决一系列从简到难的关联问题，逐步引导优化器走向更好的解，从而有效避免局部最优陷阱，提高参数估计的鲁棒性。[@problem_id:2757696]", "problem": "您的任务是实现一种同伦延拓策略，从稳态数据中估计合成生物学中一个转录激活模块的参数，以在非线性最小二乘拟合过程中避免陷入劣质的局部最小值。请从适用于基因表达模块的以下基本原理开始：分子生物学的中心法则指出，脱氧核糖核酸（DNA）被转录为核糖核酸（RNA），后者再被翻译成蛋白质。在恒定的诱导物输入下，单个基因产物的浓度（表示为 $P(t)$）可以通过一个质量平衡方程来建模，其中合成速率（取决于诱导物输入）和一阶损失速率（降解和稀释）决定了 $dP/dt$。稳态 $P_{\\mathrm{ss}}$ 由这两个相反通量的平衡所定义。诱导物与启动子之间的相互作用可以用希尔（Hill）激活函数来建模。所有符号必须按照下文所述的单位进行解释。\n\n您的模型假设和定义如下：\n- 动态质量平衡：$dP/dt = \\text{合成} - \\text{损失}$。在稳态下，$dP/dt = 0$。\n- 一阶损失：$\\text{损失} = k_{\\mathrm{deg}} P$，其中 $k_{\\mathrm{deg}}$ 是一个正常数（单位为 $\\mathrm{min}^{-1}$）。\n- 合成取决于诱导物浓度 $u$，通过一个希尔（Hill）激活函数表示：$\\text{合成} = k_{\\mathrm{syn}} \\cdot f(u)$，其中 $f(u) = \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$。\n- 使用 $S = k_{\\mathrm{syn}}/k_{\\mathrm{deg}}$（单位为任意荧光单位）进行重新参数化，使得稳态蛋白质水平满足：\n$$P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$$\n- 所有输入 $u$ 均为诱导物浓度，单位为微摩尔（$\\mu\\mathrm{M}$）。输出 $P_{\\mathrm{ss}}$ 的单位为任意荧光单位（a.u.）。希尔系数 $n$ 是无量纲的。\n\n目标：对于下面列出的每个测试用例，通过最小化模型预测与所提供的“实验”稳态读数之间的残差平方和，来估计参数向量 $\\theta = [S, K_{\\mathrm{d}}, n]$。为避免陷入劣质局部最小值，请使用输入幅度的同伦延拓：定义一个尺度参数 $s \\in (0,1]$，将所有输入按 $s$ 进行缩放，即 $u \\mapsto s\\,u$，并在每个连续的 $s$ 值上，从前一步的解开始初始化，求解最小二乘问题。从一个刻意设置的较差初始猜测值开始，并沿着尺度序列逐步推进，直到 $s = 1$。\n\n同伦过程要求：\n- 设尺度序列为 $s \\in \\{\\,0.1, 0.3, 0.6, 1.0\\,\\}$（无单位）。\n- 在每个 $s$ 值处，使用前一步的估计值作为初始猜测值，将 $\\theta$ 拟合到在缩放输入 $s\\,u$ 下测量的稳态数据。使用边界约束 $S \\in [10^{-6}, 5 \\cdot 10^{3}]$，$K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$ 和 $n \\in [0.5, 6.0]$。\n- 在 $s = 0.1$ 时，使用初始猜测值 $\\theta_{0} = [500.0, 500.0, 3.0]$。\n- 在每个 $s$ 值处，最小化未加权的残差平方和 $\\sum_{i} \\left(P_{\\mathrm{ss}}(s\\,u_{i}; \\theta) - y_{i}(s)\\right)^{2}$，其中 $y_{i}(s)$ 是在尺度 $s$ 下提供的“实验”稳态读数。\n\n数据和单位：\n- 对所有用例使用相同的基础输入浓度集：$u \\in \\{\\,0, 10, 50, 200\\,\\}$（单位为 $\\mu\\mathrm{M}$）。同伦尺度 $s$ 将这些值转换为 $s\\,u$。\n- 在本练习中，每个尺度 $s$ 下的“实验”稳态读数 $y_{i}(s)$ 是由同一模型使用下面给出的真实参数无噪声地生成的。这产生了一个完全确定性的测试，而拟合算法除了生成合成数据外，不得以任何方式使用真实参数。\n\n测试套件（三个具有不同机制的用例）：\n1. 理想路径用例：真实参数 $\\theta^{\\ast}_{1} = [1200.0, 30.0, 2.0]$，其中 $S$ 的单位为 a.u.，$K_{\\mathrm{d}}$ 的单位为 $\\mu\\mathrm{M}$。\n2. 高协同性用例：真实参数 $\\theta^{\\ast}_{2} = [800.0, 5.0, 4.0]$。\n3. 低动态范围用例：真实参数 $\\theta^{\\ast}_{3} = [300.0, 100.0, 1.5]$。\n\n需要实现的任务：\n- 对于每个用例 $k \\in \\{1,2,3\\}$，构建尺度列表 $[0.1, 0.3, 0.6, 1.0]$、基础输入列表 $[0, 10, 50, 200]$，并为每个尺度 $s$ 计算合成稳态读数 $y_{i}^{(k)}(s) = P_{\\mathrm{ss}}(s\\,u_{i}; \\theta^{\\ast}_{k})$。\n- 按照升序实现跨尺度的同伦延拓拟合，从 $s=0.1$ 和初始猜测值 $\\theta_{0} = [500.0, 500.0, 3.0]$ 开始，并按规定进行边界约束。在每个尺度下，仅对该尺度的数据进行拟合。\n- 返回每个用例在 $s=1.0$ 时的最终估计值。\n\n数值和单位要求：\n- 报告 $k \\in \\{1,2,3\\}$ 的最终估计值 $\\widehat{\\theta}_{k} = [\\widehat{S}_{k}, \\widehat{K}_{\\mathrm{d},k}, \\widehat{n}_{k}]$，其中 $\\widehat{S}_{k}$ 的单位是 a.u.，$\\widehat{K}_{\\mathrm{d},k}$ 的单位是 $\\mu\\mathrm{M}$，$\\widehat{n}_{k}$ 无量纲。\n- 将报告的每个数字四舍五入到三位小数。不应打印任何其他单位。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个含三个内部列表的列表，每个内部列表对应一个测试用例，形式为 $[\\widehat{S}, \\widehat{K}_{\\mathrm{d}}, \\widehat{n}]$，其中三个数字均四舍五入到三位小数。输出不得包含任何空格，且整行必须用方括号括起来。例如，包含三个用例的输出格式应为 $[[a,b,c],[d,e,f],[g,h,i]]$，其中每个符号都被替换为一个四舍五入到三位小数的数字。", "solution": "在尝试任何解决方案之前，对问题进行验证。\n\n### 步骤 1：提取已知信息\n- **模型**：稳态蛋白质水平 $P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}}$。\n- **参数**：$\\theta = [S, K_{\\mathrm{d}}, n]$。\n  - $S$：最大合成/降解速率比（a.u.）。\n  - $K_{\\mathrm{d}}$：激活常数（$\\mu\\mathrm{M}$）。\n  - $n$：希尔系数（无量纲）。\n- **目标**：使用同伦延拓策略，通过最小化残差平方和，为三个测试用例估计 $\\theta$。\n- **同伦过程**：\n  - 尺度序列：$s \\in \\{0.1, 0.3, 0.6, 1.0\\}$。\n  - 在每个 $s$ 值处，使用前一步的估计值作为初始猜测值，将 $\\theta$ 拟合到缩放输入 $s \\cdot u$ 处的数据。\n  - 在 $s=0.1$ 时的初始猜测值：$\\theta_{0} = [500.0, 500.0, 3.0]$。\n  - 参数边界：$S \\in [10^{-6}, 5 \\cdot 10^{3}]$，$K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$，$n \\in [0.5, 6.0]$。\n  - 目标函数：对 $\\sum_{i} \\left(P_{\\mathrm{ss}}(s\\,u_{i}; \\theta) - y_{i}(s)\\right)^{2}$ 进行非线性最小二乘最小化。\n- **数据**：\n  - 基础输入浓度：$u \\in \\{0, 10, 50, 200\\}$ ($\\mu\\mathrm{M}$)。\n  - “实验”数据 $y_{i}(s)$ 由模型使用真实参数无噪声地生成：$y_{i}(s) = P_{\\mathrm{ss}}(s\\,u_{i}; \\theta^{\\ast})$。\n- **测试用例（真实参数）**：\n  1. $\\theta^{\\ast}_{1} = [1200.0, 30.0, 2.0]$。\n  2. $\\theta^{\\ast}_{2} = [800.0, 5.0, 4.0]$。\n  3. $\\theta^{\\ast}_{3} = [300.0, 100.0, 1.5]$。\n- **输出规范**：\n  - 报告 $k \\in \\{1,2,3\\}$ 的最终估计值 $\\widehat{\\theta}_{k} = [\\widehat{S}_{k}, \\widehat{K}_{\\mathrm{d},k}, \\widehat{n}_{k}]$。\n  - 将每个数字四舍五入到三位小数。\n  - 最终输出为单行、由列表组成的列表，无空格：`[[a,b,c],[d,e,f],[g,h,i]]`。\n\n### 步骤 2：使用提取的已知信息进行验证\n- **科学依据（关键）**：该问题基于质量作用动力学和希尔函数，这些是分子与系统生物学中描述基因调控网络的经典模型。这是一种标准的、科学合理的方法。\n- **适定性**：该问题是适定的。它详细描述了一个特定的数值程序（用于非线性最小二乘的同伦延拓），包含了所有必要组成部分：模型、目标函数、初始条件、参数边界和清晰的步骤序列。使用无噪声的合成数据确保了“真实”解（用于生成数据的参数）的存在，可以据此衡量算法的性能。\n- **客观性（关键）**：该问题以精确、定量和客观的语言陈述，没有模糊性或主观解释。\n- **不完整或矛盾的设置**：该问题是自洽的，并提供了所有必要信息。所提供的数据或约束中没有矛盾之处。\n- **不切实际或不可行**：参数值、浓度和模型行为对于合成基因线路是合理的。该任务是一个标准的计算机模拟（计算）实验。\n- **无法形式化或不相关**：该问题是*合成生物学*领域中*参数估计和模型对实验数据拟合*的一个典型例子。它直接相关且可以进行数学形式化。\n\n### 步骤 3：结论与行动\n该问题被确定为有效。它构成了系统生物学中一个定义明确的计算任务，并基于已建立的科学原理。我们继续进行求解。\n\n---\n\n该问题要求为一个非线性生物模型估计参数。该模型描述了蛋白质的稳态浓度 $P_{\\mathrm{ss}}$ 作为诱导物浓度 $u$ 的函数。控制方程是一个重新参数化的希尔激活函数：\n$$ P_{\\mathrm{ss}}(u; S, K_{\\mathrm{d}}, n) = S \\cdot \\dfrac{u^{n}}{K_{\\mathrm{d}}^{n} + u^{n}} $$\n待确定的参数向量为 $\\theta = [S, K_{\\mathrm{d}}, n]$。$S$ 代表最大表达水平，$K_{\\mathrm{d}}$ 是激活系数（产生半最大响应的 $u$ 的浓度），$n$ 是希尔系数，量化了转录响应的协同性或陡峭程度。\n\n参数估计被表述为一个非线性最小二乘优化问题。目标是找到参数向量 $\\theta$，使其最小化模型预测值与所提供的“实验”数据 $y_i$ 之间的残差平方和（SSR）。对于一组 $N$ 个输入-输出对 $(u_i, y_i)$，目标函数为：\n$$ \\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\left( P_{\\mathrm{ss}}(u_{i}; \\theta) - y_{i} \\right)^{2} $$\n直接最小化这样一个非凸函数可能很困难，因为优化算法可能会收敛到次优的局部最小值，特别是当从远离全局最小值的初始猜测开始时。为规避此问题，该问题规定使用同伦延拓方法。该技术将原始的困难问题变形为一系列更简单、相关的问题，引导优化器走向更好的解。\n\n同伦是通过引入一个延拓参数 $s$ 来构建的，该参数缩放输入浓度：$u \\mapsto s \\cdot u$，其中 $s \\in (0, 1]$。首先对一个小的 $s$（例如 $s=0.1$）求解优化问题。在小的 $s$ 值下，缩放后的输入 $s \\cdot u_i$ 都处于低浓度区域，此时希尔函数近似为一个幂律（$P_{\\mathrm{ss}} \\approx S (u/K_{\\mathrm{d}})^n$），使得优化景观更为平滑。这个简化问题的解随后作为序列中下一个问题的初始猜测，而下一个问题使用一个稍大的 $s$ 值。这个迭代过程一直持续到 $s=1$，此时原始的全范围问题被求解，但得益于从延拓路径中获得的高度信息化的初始猜测。\n\n指定的算法对三个测试用例（由其真实参数向量 $\\theta^{\\ast}_{k}$ 定义）中的每一个执行如下：\n1.  定义尺度参数的离散序列 $s_j \\in \\{0.1, 0.3, 0.6, 1.0\\}$。\n2.  用一个刻意设置的较差猜测值初始化参数估计：$\\hat{\\theta}^{(0)} = [500.0, 500.0, 3.0]$。\n3.  遍历尺度 $s_j$（$j=1, 2, 3, 4$）：\n    a. 对于当前测试用例 $k$ 和尺度 $s_j$，使用基础输入浓度 $u_i \\in \\{0, 10, 50, 200\\}$ 生成合成数据点 $y_{i,k}^{(j)} = P_{\\mathrm{ss}}(s_j u_i; \\theta^{\\ast}_{k})$。\n    b. 此优化步骤的残差向量定义为 $\\mathbf{r}(\\theta)$，其第 $i$ 个分量为 $r_i(\\theta) = P_{\\mathrm{ss}}(s_j u_i; \\theta) - y_{i,k}^{(j)}$。\n    c. 求解有界非线性最小二乘问题：\n    $$ \\hat{\\theta}^{(j)} = \\arg\\min_{\\theta \\in \\Theta} \\| \\mathbf{r}(\\theta) \\|_2^2 $$\n    优化使用上一步的估计值 $\\hat{\\theta}^{(j-1)}$ 进行初始化。参数搜索空间 $\\Theta$ 受指定的边界约束：$S \\in [10^{-6}, 5 \\cdot 10^{3}]$, $K_{\\mathrm{d}} \\in [10^{-6}, 10^{4}]$ 和 $n \\in [0.5, 6.0]$。\n4.  测试用例 $k$ 的最终估计参数向量是延拓最后一步的结果，$\\widehat{\\theta}_{k} = \\hat{\\theta}^{(4)}$，对应于 $s=1.0$ 时的解。\n\n该过程使用 `scipy.optimize.least_squares` 函数实现，该函数非常适合有界非线性最小二乘问题。由于数据是从正在拟合的同一模型中综合生成且无噪声，一个成功的优化过程应能在数值精度的限制内恢复真实参数 $\\theta^{\\ast}_{k}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef solve():\n    \"\"\"\n    Implements a homotopy continuation strategy to estimate parameters of a\n    transcriptional activation module from synthetic steady-state data.\n    \"\"\"\n\n    # Define the base input concentrations (in micromolar)\n    base_u = np.array([0., 10., 50., 200.])\n\n    # Define the test cases with their true parameters [S, Kd, n]\n    test_cases = [\n        [1200.0, 30.0, 2.0],  # Happy-path case\n        [800.0, 5.0, 4.0],   # High-cooperativity case\n        [300.0, 100.0, 1.5], # Low-dynamic-range case\n    ]\n\n    # Homotopy continuation parameters\n    scales = [0.1, 0.3, 0.6, 1.0]\n    initial_guess = np.array([500.0, 500.0, 3.0])\n    bounds = ([1e-6, 1e-6, 0.5], [5000.0, 10000.0, 6.0])\n\n    def p_ss_model(theta, u):\n        \"\"\"\n        Calculates the steady-state protein level using the Hill activation model.\n        P_ss(u; S, Kd, n) = S * (u^n) / (Kd^n + u^n)\n        \n        Args:\n            theta (array-like): Parameter vector [S, Kd, n].\n            u (array-like): Inducer concentrations.\n        \n        Returns:\n            numpy.ndarray: Predicted steady-state protein levels.\n        \"\"\"\n        S, Kd, n = theta\n        u_n = np.power(u, n)\n        Kd_n = np.power(Kd, n)\n        \n        # Denominator is guaranteed to be non-zero due to bounds on Kd and u>=0.\n        denominator = Kd_n + u_n\n        \n        # Handle u=0 case to avoid 0/0 if Kd was 0 (not possible here, but good practice)\n        # np.divide is robust and allows specifying output for division by zero.\n        # Here, numerator is S * u_n.\n        # safe division: if denominator is zero, result is 0 (as u would be 0).\n        # We can also handle this explicitly for clarity.\n        # If u=0, u_n=0, numerator is 0, so fraction is 0.\n        # If u>0, denominator > 0.\n        # The direct calculation is safe given the constraints.\n        return S * u_n / denominator\n\n    def residuals(theta, u, y_data):\n        \"\"\"\n        Calculates the residuals between the model prediction and experimental data.\n        \n        Args:\n            theta (array-like): Parameter vector [S, Kd, n].\n            u (array-like): Inducer concentrations.\n            y_data (array-like): Experimental data points.\n        \n        Returns:\n            numpy.ndarray: Vector of residuals.\n        \"\"\"\n        return p_ss_model(theta, u) - y_data\n\n    final_results = []\n\n    for true_params in test_cases:\n        # Reset the parameter estimate to the initial guess for each new test case\n        theta_estimate = np.copy(initial_guess)\n\n        # Homotopy continuation loop through the scales\n        for s in scales:\n            # 1. Scale inputs for the current continuation step\n            scaled_u = s * base_u\n            \n            # 2. Generate synthetic \"experimental\" data for this scale using true parameters\n            y_data = p_ss_model(true_params, scaled_u)\n            \n            # 3. Perform nonlinear least-squares fitting\n            # The initial guess (x0) is the result from the previous, smaller scale\n            result = least_squares(\n                fun=residuals,\n                x0=theta_estimate,\n                bounds=bounds,\n                args=(scaled_u, y_data)\n            )\n            \n            # 4. Update the parameter estimate for the next iteration\n            theta_estimate = result.x\n        \n        # Store the final estimate for s=1.0\n        final_results.append(theta_estimate.tolist())\n\n    # Format the results for final output\n    # Round numbers to three decimal places\n    rounded_results = [[round(val, 3) for val in res] for res in final_results]\n    \n    # Convert to string and remove spaces to match the required format\n    output_str = str(rounded_results).replace(\" \", \"\")\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "2757696"}, {"introduction": "细胞间的异质性是生物系统中普遍存在的现象，简单地用单一参数值来拟合群体平均数据会忽略这一关键的生物学特性。为了更精确地描述系统，我们需要从群体数据中推断出参数的分布。本练习将引导您构建一个分层贝叶斯模型（hierarchical Bayesian model），通过这种方法，您可以将细胞特异性参数视为从一个更高层次的分布中抽样得到，进而估计该分布的超参数（hyperparameters），从而实现对细胞群体异质性的量化表征。[@problem_id:2757765]", "problem": "一个合成基因回路在诱导型启动子的调控下表达一种荧光蛋白。由于外在变异性，单个细胞在蛋白质生产的动力学参数上存在差异。考虑一个分层模型，其中细胞 $i$ 的动力学参数记为 $k_i$，并假设其独立地从对数正态分布中抽取，该分布的参数为 $\\mu$ 和 $\\sigma$，即 $\\log k_i$ 服从均值为 $\\mu$、标准差为 $\\sigma$ 的高斯分布。在固定的诱导条件 $c$ 下，每个细胞的期望荧光强度与 $k_i$ 成正比，比例常数 $s_c$ 是已知的、特定于条件的，由诱导剂水平和报告基因校准确定。对于每个条件 $c$，实验测量了 $n_c$ 个细胞的荧光样本均值，产生一个观测值 $y_c$。此外，报告的样本均值上存在独立的测量噪声，其标准差为 $\\tau_c$。\n\n假设模型包含以下组成部分：\n- 每个细胞的动力学参数 $k_i$ 在细胞间独立，且 $\\log k_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$。\n- 在给定 $k_i$ 和条件 $c$ 的情况下，细胞 $i$ 的期望荧光为 $s_c k_i$。\n- 报告的样本均值 $y_c$ 被建模为从一个高斯分布中的单次抽取，该分布的中心是条件 $c$ 下的真实总体均值，其方差等于两个部分的和：在 $n_c$ 个细胞上平均的细胞水平荧光的抽样方差，以及特定于条件的测量方差 $\\tau_c^2$。\n- 先验分布：$\\mu \\sim \\mathcal{N}(m_0, s_0^2)$ 和 $\\sigma \\sim \\text{Half-Normal}(b_0)$，对于 $\\sigma>0$ 其密度正比于 $\\exp\\{-\\sigma^2/(2 b_0^2)\\}$。\n\n您的任务是通过对细胞水平的变异性进行积分，并根据群体水平的观测数据计算超参数 $\\mu$ 和 $\\sigma$ 的最大后验（MAP；maximum a posteriori）估计，来进行贝叶斯参数估计。应利用对数正态分布的性质精确地进行积分，以获得观测样本均值 $y_c$ 在不同条件下的边际似然。然后与给定的先验分布结合，构建后验密度 $\\pi(\\mu,\\sigma \\mid \\{(y_c,s_c,n_c,\\tau_c)\\}_c)$，并计算 MAP 估计 $(\\mu_{\\text{MAP}},\\sigma_{\\text{MAP}})$。\n\n您可以不经证明地假设以下基本原理：\n- 如果 $X$ 是一个随机变量，且 $Y = a X + b$（其中 $a$ 和 $b$ 为常数），则 $\\mathbb{E}[Y] = a \\mathbb{E}[X] + b$ 且 $\\operatorname{Var}(Y) = a^2 \\operatorname{Var}(X)$。\n- 中心极限定理表明，独立同分布随机变量的样本均值在真实均值附近近似呈高斯波动，其方差等于总体方差除以样本量。\n- 正态分布和对数正态分布的定义。\n\n构建一个程序，对于下面的每个测试用例，通过在 $\\mu \\in \\mathbb{R}$ 和 $\\sigma > 0$ 上最大化对数后验来计算 MAP 估计 $(\\mu_{\\text{MAP}},\\sigma_{\\text{MAP}})$。您可以使用任何正确且数值稳定的方法。为保证数值稳定性，请强制实施下界 $\\sigma \\geq 10^{-6}$。\n\n对所有测试用例使用相同的先验超参数：$m_0 = 0, s_0 = 1, b_0 = 1$。\n\n测试集（每个测试用例提供观测均值 $y$、比例常数 $s$、样本量 $n$ 和测量噪声标准差 $\\tau$ 的数组）：\n- 测试用例 1：\n  - $y = \\{0.791,\\;1.442,\\;2.934\\}$\n  - $s = \\{0.5,\\;1.0,\\;2.0\\}$\n  - $n = \\{1000,\\;1000,\\;1000\\}$\n  - $\\tau = \\{0.05,\\;0.05,\\;0.05\\}$\n- 测试用例 2：\n  - $y = \\{0.9381,\\;2.7593\\}$\n  - $s = \\{1.0,\\;3.0\\}$\n  - $n = \\{200,\\;200\\}$\n  - $\\tau = \\{0.02,\\;0.02\\}$\n- 测试用例 3：\n  - $y = \\{0.23,\\;1.15,\\;5.2\\}$\n  - $s = \\{0.2,\\;1.2,\\;5.0\\}$\n  - $n = \\{50,\\;50,\\;50\\}$\n  - $\\tau = \\{0.1,\\;0.1,\\;0.1\\}$\n- 测试用例 4：\n  - $y = \\{1.842\\}$\n  - $s = \\{1.7\\}$\n  - $n = \\{1000\\}$\n  - $\\tau = \\{0.02\\}$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。结果必须是按顺序连接的每个测试用例的四舍五入后的 MAP 估计值：对于测试用例 $j$，附加四舍五入到六位小数的 $\\mu_{\\text{MAP}}$ 和 $\\sigma_{\\text{MAP}}$。例如，对于两个测试用例，一个有效的输出格式将是 $[\\mu_{\\text{MAP},1},\\sigma_{\\text{MAP},1},\\mu_{\\text{MAP},2},\\sigma_{\\text{MAP},2}]$，其中每个值都四舍五入到六位小数。", "solution": "问题陈述经过了严格的验证。\n\n### 步骤 1：提取给定信息\n\n- **每个细胞的动力学参数模型**：细胞 $i$ 的动力学参数，记为 $k_i$，独立地从对数正态分布中抽取。具体来说，$\\log k_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$。\n- **每个细胞的荧光模型**：在条件 $c$ 下，细胞 $i$ 的期望荧光为 $s_c k_i$，其中 $s_c$ 是一个已知的、特定于条件的比例常数。\n- **观测数据模型**：报告的 $n_c$ 个细胞的荧光样本均值 $y_c$ 被建模为从一个高斯分布中的单次抽取。该分布的中心是条件 $c$ 下细胞水平荧光的真实总体均值。其方差是两个部分的和：在 $n_c$ 个细胞上平均的细胞水平荧光的抽样方差，以及特定于条件的测量方差 $\\tau_c^2$。\n- **先验分布**：$\\mu$ 的先验分布是 $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$。$\\sigma$ 的先验分布是 $\\sigma \\sim \\text{Half-Normal}(b_0)$（对于 $\\sigma > 0$），其密度正比于 $\\exp\\{-\\sigma^2/(2b_0^2)\\}$。\n- **任务**：进行贝叶斯参数估计，以根据群体水平的观测数据 $\\{(y_c, s_c, n_c, \\tau_c)\\}_c$ 计算超参数 $(\\mu_{\\text{MAP}}, \\sigma_{\\text{MAP}})$ 的最大后验 (MAP) 估计。\n- **数值约束**：为保证数值稳定性，强制实施下界 $\\sigma \\geq 10^{-6}$。\n- **先验超参数**：对所有测试用例， $m_0 = 0$，$s_0 = 1$，$b_0 = 1$。\n- **测试数据**：提供了四个测试用例，每个用例都包含 $\\{y_c\\}$、$\\{s_c\\}$、$\\{n_c\\}$ 和 $\\{\\tau_c\\}$ 的数组。\n\n### 步骤 2：使用提取的给定信息进行验证\n\n根据指定标准对问题进行评估。\n\n- **科学依据**：该问题描述了一个用于基因表达的分层贝叶斯模型，这是定量生物学和合成生物学中一种标准且广为接受的方法。对动力学参数使用对数正态分布反映了生物学量通常呈右偏分布的常见观察结果。观测数据的模型包含了生物学（抽样）和技术（测量）噪声，这是一个统计上合理的结构。该问题牢固地建立在生物学统计建模的既定原则之上。\n- **适定性**：任务是找到后验概率分布的最大值。$\\mu$ 的先验分布（正态）和 $\\sigma$ 的先验分布（半正态）都是正常的，并且似然函数是根据高斯假设推导出来的。这种结构导出了一个定义明确的优化目标函数。该问题的结构旨在产生一个唯一的 MAP 估计。\n- **客观性**：该问题以精确、无歧义的数学语言表述。所有术语都有定义，数据是定量的。没有主观或基于意见的陈述。\n- **完整性和一致性**：该问题是自洽的。它提供了所有必要的组成部分：数据生成模型、先验分布、所有必需的参数和超参数，以及每个测试用例的具体数据。没有明显的矛盾之处。\n- **现实性**：该模型是一个真实生物实验的标准（尽管简化了）表示。所提供的数据值对于荧光测量是合理的。\n- **结构**：该问题需要多步推导后验分布，然后进行数值优化。这是一项实质性的、不平凡的任务，能够正确评估对分层背景下贝叶斯推断的理解。\n\n### 步骤 3：结论与行动\n\n该问题被判定为**有效**。它在科学上是合理的，适定的，并且是完整的。将构建一个解决方案。\n\n### 解题推导\n\n目标是找到参数 $\\mu$ 和 $\\sigma$ 的 MAP 估计，即最大化后验概率密度 $\\pi(\\mu, \\sigma \\mid \\text{data})$ 的值。根据贝叶斯定理，后验概率正比于似然与先验的乘积：\n$$\n\\pi(\\mu, \\sigma \\mid \\{y_c\\}_c) \\propto p(\\{y_c\\}_c \\mid \\mu, \\sigma) \\, p(\\mu) \\, p(\\sigma)\n$$\n最大化后验等价于最大化其对数，即对数后验。\n\n**1. 边际似然的推导**\n\n首先，我们通过对未观测到的细胞水平参数 $k_i$ 进行边缘化，来推导观测数据 $\\{y_c\\}_c$ 的似然。\n\n每个细胞的动力学参数 $k_i$ 从对数正态分布中抽取，$k_i \\sim \\text{LogNormal}(\\mu, \\sigma^2)$，即 $\\log k_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$。$k_i$ 的均值和方差为：\n$$\n\\mathbb{E}[k_i] = \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(k_i) = (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)\n$$\n在条件 $c$ 下，单个细胞 $i$ 的期望荧光为 $F_{i,c} = s_c k_i$。其均值和方差为：\n$$\n\\mathbb{E}[F_{i,c}] = s_c \\mathbb{E}[k_i] = s_c \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(F_{i,c}) = s_c^2 \\operatorname{Var}(k_i) = s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)\n$$\n问题陈述，观测到的样本均值 $y_c$ 是从一个高斯分布中抽取的。这个高斯分布的均值是荧光的真实总体均值，$\\mu_c(\\mu, \\sigma) = \\mathbb{E}[F_{i,c}]$。\n$$\n\\mu_c(\\mu, \\sigma) = s_c \\exp(\\mu + \\sigma^2/2)\n$$\n这个高斯分布的方差 $\\sigma_c^2(\\mu, \\sigma)$ 是 $n_c$ 个细胞均值的抽样方差与测量方差 $\\tau_c^2$ 的和。\n$$\n\\sigma_c^2(\\mu, \\sigma) = \\frac{\\operatorname{Var}(F_{i,c})}{n_c} + \\tau_c^2 = \\frac{s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\mu + \\sigma^2)}{n_c} + \\tau_c^2\n$$\n因此，单个观测值 $y_c$ 的模型是 $y_c \\sim \\mathcal{N}(\\mu_c(\\mu, \\sigma), \\sigma_c^2(\\mu, \\sigma))$。观测到 $y_c$ 的似然由高斯概率密度函数给出。假设不同条件 $c$ 的观测是独立的，总对数似然为：\n$$\n\\log L(\\mu, \\sigma) = \\log p(\\{y_c\\}_c \\mid \\mu, \\sigma) = \\sum_c \\log p(y_c \\mid \\mu, \\sigma)\n$$\n$$\n\\log L(\\mu, \\sigma) = -\\frac{1}{2} \\sum_c \\left[ \\log(2\\pi \\sigma_c^2(\\mu, \\sigma)) + \\frac{(y_c - \\mu_c(\\mu, \\sigma))^2}{\\sigma_c^2(\\mu, \\sigma)} \\right]\n$$\n\n**2. 构建对数后验**\n\n$\\mu$ 和 $\\sigma$ 的对数先验（忽略归一化常数）为：\n$$\n\\log p(\\mu) = -\\frac{(\\mu - m_0)^2}{2s_0^2} + \\text{const}\n$$\n$$\n\\log p(\\sigma) = -\\frac{\\sigma^2}{2b_0^2} + \\text{const}, \\quad \\text{for } \\sigma > 0\n$$\n需要最大化的完整对数后验是 $\\log \\pi(\\mu, \\sigma) = \\log L(\\mu, \\sigma) + \\log p(\\mu) + \\log p(\\sigma)$。\n\n**3. 优化策略**\n\n我们寻求 $(\\mu_{\\text{MAP}}, \\sigma_{\\text{MAP}}) = \\arg\\max_{\\mu, \\sigma>0} \\log \\pi(\\mu, \\sigma)$。这等价于最小化负对数后验。为了提高数值稳定性，我们对模型进行重新参数化。令 $\\phi = \\mu + \\sigma^2/2$。那么 $\\mu = \\phi - \\sigma^2/2$。$y_c$ 的均值和方差表达式变得更稳定：\n$$\n\\mu_c(\\phi) = s_c \\exp(\\phi)\n$$\n$$\n\\exp(2\\mu + \\sigma^2) = \\exp(2(\\phi - \\sigma^2/2) + \\sigma^2) = \\exp(2\\phi)\n$$\n$$\n\\sigma_c^2(\\phi, \\sigma) = \\frac{s_c^2 (\\exp(\\sigma^2) - 1) \\exp(2\\phi)}{n_c} + \\tau_c^2 = \\frac{(\\exp(\\sigma^2) - 1) \\mu_c(\\phi)^2}{n_c} + \\tau_c^2\n$$\n需要最小化的目标函数是用 $(\\phi, \\sigma)$ 表示的负对数后验，忽略常数：\n$$\nJ(\\phi, \\sigma) = \\frac{1}{2} \\sum_c \\left[ \\log(\\sigma_c^2(\\phi, \\sigma)) + \\frac{(y_c - \\mu_c(\\phi))^2}{\\sigma_c^2(\\phi, \\sigma)} \\right] + \\frac{(\\phi - \\sigma^2/2 - m_0)^2}{2s_0^2} + \\frac{\\sigma^2}{2b_0^2}\n$$\n该函数相对于 $\\phi \\in \\mathbb{R}$ 和 $\\sigma > 0$ 进行最小化。将施加约束 $\\sigma \\geq 10^{-6}$。优化过程使用 `scipy.optimize.minimize` 进行数值计算。一旦找到最优的 $(\\phi_{\\text{MAP}}, \\sigma_{\\text{MAP}})$，我们使用变换恢复 $\\mu_{\\text{MAP}}$：\n$$\n\\mu_{\\text{MAP}} = \\phi_{\\text{MAP}} - \\frac{\\sigma_{\\text{MAP}}^2}{2}\n$$\n以下程序为每个测试用例实现了此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the MAP estimation problem for all test cases.\n    \"\"\"\n    # Prior hyperparameters (fixed for all test cases)\n    m0 = 0.0\n    s0 = 1.0\n    b0 = 1.0\n    priors = (m0, s0, b0)\n\n    # Test suite as defined in the problem statement\n    test_cases = [\n        {\n            \"y\": np.array([0.791, 1.442, 2.934]),\n            \"s\": np.array([0.5, 1.0, 2.0]),\n            \"n\": np.array([1000, 1000, 1000]),\n            \"tau\": np.array([0.05, 0.05, 0.05]),\n        },\n        {\n            \"y\": np.array([0.9381, 2.7593]),\n            \"s\": np.array([1.0, 3.0]),\n            \"n\": np.array([200, 200]),\n            \"tau\": np.array([0.02, 0.02]),\n        },\n        {\n            \"y\": np.array([0.23, 1.15, 5.2]),\n            \"s\": np.array([0.2, 1.2, 5.0]),\n            \"n\": np.array([50, 50, 50]),\n            \"tau\": np.array([0.1, 0.1, 0.1]),\n        },\n        {\n            \"y\": np.array([1.842]),\n            \"s\": np.array([1.7]),\n            \"n\": np.array([1000]),\n            \"tau\": np.array([0.02]),\n        },\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Initial guess for optimization parameters [phi, sigma]\n        # A reasonable guess for phi = mu + sigma^2/2 is the log of the scaled observation\n        phi_guess = np.mean(np.log(case_data['y'] / case_data['s']))\n        sigma_guess = 0.5\n        x0 = [phi_guess, sigma_guess]\n\n        # Define bounds for the optimization: phi is unbounded, sigma > 1e-6\n        bounds = [(None, None), (1e-6, None)]\n\n        # Run the optimization to minimize the negative log-posterior\n        result = minimize(\n            negative_log_posterior,\n            x0,\n            args=(case_data, priors),\n            bounds=bounds,\n            method='L-BFGS-B'\n        )\n\n        phi_map, sigma_map = result.x\n        \n        # Transform back from phi to mu\n        mu_map = phi_map - 0.5 * sigma_map**2\n\n        results.extend([round(mu_map, 6), round(sigma_map, 6)])\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef negative_log_posterior(params, data, priors):\n    \"\"\"\n    Calculates the negative of the log-posterior probability density.\n    This is the objective function to be minimized.\n    Uses the numerically stable reparameterization (phi, sigma).\n    \n    Args:\n        params (list-like): A list or array [phi, sigma].\n        data (dict): A dictionary containing numpy arrays for y, s, n, tau.\n        priors (tuple): A tuple of prior hyperparameters (m0, s0, b0).\n        \n    Returns:\n        float: The value of the negative log-posterior.\n    \"\"\"\n    phi, sigma = params\n    y, s, n, tau = data[\"y\"], data[\"s\"], data[\"n\"], data[\"tau\"]\n    m0, s0, b0 = priors\n\n    # --- Log-prior term ---\n    # The original parameter mu is expressed in terms of phi and sigma\n    mu = phi - 0.5 * sigma**2\n    \n    # Calculate log-prior contribution (proportional to negative log-prior)\n    log_prior_cost = ( (mu - m0)**2 / (2 * s0**2) + \n                       sigma**2 / (2 * b0**2) )\n\n    # --- Log-likelihood term ---\n    try:\n        # Calculate mu_c = s_c * exp(phi)\n        exp_phi = np.exp(phi)\n        mu_c = s * exp_phi\n\n        # Use np.expm1 for precision: exp(x) - 1\n        exp_sigma2_minus_1 = np.expm1(sigma**2)\n\n        # Calculate sigma_c^2 using the reparameterized formula\n        sampling_variance = (exp_sigma2_minus_1 * mu_c**2) / n\n        sigma_c_sq = sampling_variance + tau**2\n        \n        # Guard against non-positive variance which can occur during optimization search\n        if np.any(sigma_c_sq <= 0):\n            return np.inf\n\n        # Calculate log-likelihood contribution (proportional to negative log-likelihood)\n        log_likelihood_cost = 0.5 * np.sum(np.log(sigma_c_sq) + (y - mu_c)**2 / sigma_c_sq)\n\n    except (OverflowError, RuntimeWarning):\n        # Return a large value if numerical issues arise (e.g., overflow in exp)\n        return np.inf\n\n    # Total cost is the sum of prior and likelihood costs\n    total_cost = log_prior_cost + log_likelihood_cost\n    \n    return total_cost\n\n# Execute the main function\nsolve()\n\n```", "id": "2757765"}, {"introduction": "将常微分方程（ODE）等动态模型与时间序列数据进行拟合，对于理解合成生物回路的动态行为至关重要。基于梯度的优化方法非常高效，但这需要计算目标函数相对于模型参数的梯度，当参数数量众多时，传统的有限差分法计算成本极高。本练习将指导您实现连续时间伴随方法（continuous-time adjoint method），这是一种计算效率极高的梯度计算技术，其计算成本与参数数量无关，是大规模动态模型参数估计的前沿方法。[@problem_id:2757781]", "problem": "考虑一个标准的双阶段基因表达模型，该模型捕获了在诱导剂作用下的转录和翻译过程。在合成生物学中，当从时间序列数据估计参数时，这是一种常见情景。设信使RNA (mRNA) 浓度为 $m(t)$，蛋白质浓度为 $p(t)$。诱导剂浓度 $I$ 是恒定且已知的。其动力学由以下常微分方程组给出：\n$$\n\\frac{d}{dt}\\begin{bmatrix} m \\\\ p \\end{bmatrix}\n=\n\\begin{bmatrix}\nk_{\\mathrm{tx}}\\,H(I;K,n) - \\delta_m\\, m \\\\\nk_{\\mathrm{tl}}\\, m - \\delta_p\\, p\n\\end{bmatrix},\n\\quad\nH(I;K,n) \\equiv \\frac{1}{1 + \\left(\\frac{I}{K}\\right)^n},\n$$\n其中未知参数向量为：\n$$\n\\theta \\equiv \\begin{bmatrix} k_{\\mathrm{tx}} & K & n & \\delta_m & k_{\\mathrm{tl}} & \\delta_p \\end{bmatrix}^{\\top}.\n$$\n假设初始条件为 $x(0)=\\begin{bmatrix} m(0) & p(0) \\end{bmatrix}^{\\top} = \\begin{bmatrix} 0 & 0 \\end{bmatrix}^{\\top}$。仅在离散时间点 $0, t_1, \\dots, t_M$ 对蛋白质 $p(t)$ 进行测量，产生数据 $y_i$ 和已知的正权重 $w_i$。定义最小二乘目标函数：\n$$\nJ(\\theta) \\equiv \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left(p(t_i;\\theta) - y_i\\right)^2.\n$$\n所有变量和参数均为无量纲。\n\n任务。实现一个程序，该程序使用连续时间伴随方法，针对在离散时间点进行测量的目标函数计算梯度 $\\nabla_{\\theta}J$。计算应从基于拉格朗日构造推导出的伴随方程开始。在测量时间点之间，伴随变量 $\\lambda(t)\\in\\mathbb{R}^2$ 满足由前向系统所蕴含的齐次线性伴随动力学，其终端条件为 $\\lambda(t_M^+)=\\mathbf{0}$。在每个测量时间点 $t_i$，应用由离散损失项引起的跳跃条件。使用得到的伴随轨迹，通过对 $\\left(\\partial f/\\partial \\theta\\right)^{\\top}\\lambda$ 进行时间积分来评估梯度。\n\n您的实现必须：\n- 计算前向状态轨迹，并使用密集输出，以便在任意时间 $t$ 都能获得 $x(t)$。\n- 从 $t_M$ 到 $0$ 对伴随方程进行时间上的后向积分，并在每个 $t_i$ 应用跳跃条件：\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\frac{\\partial}{\\partial x}\\left(\\frac{1}{2}w_i\\left(p(t_i;\\theta)-y_i\\right)^2\\right),\n$$\n其中，相对于 $x$ 的梯度是将 $p$ 视为 $x$ 的第二个分量来计算的。\n- 使用以下公式累积梯度：\n$$\n\\nabla_{\\theta}J(\\theta) = \\int_{0}^{t_M} \\left(\\frac{\\partial f}{\\partial \\theta}(t, x(t), \\theta)\\right)^{\\top}\\lambda(t)\\, dt,\n$$\n请注意，在此设置中，离散测量项不显式依赖于 $\\theta$。\n\n验证要求。对于下方的每个测试用例，还需使用每个参数的步长 $h_j = \\max\\{10^{-6}\\cdot|\\theta_j|,\\,10^{-8}\\}$ 计算 $\\nabla_{\\theta}J$ 的中心有限差分近似，并报告各分量上的最大相对差异：\n$$\n\\varepsilon \\equiv \\max_{j\\in\\{1,\\dots,6\\}} \\frac{\\left|\\, \\left[\\nabla_{\\theta}J(\\theta)\\right]_j - \\left[\\nabla_{\\theta}J(\\theta)\\right]_j^{\\mathrm{FD}} \\,\\right|}{\\max\\left\\{1,\\left| \\left[\\nabla_{\\theta}J(\\theta)\\right]_j^{\\mathrm{FD}} \\right|\\right\\}}.\n$$\n\n测试套件。使用以下三个独立的测试用例。在每个用例中，通过在指定的真实参数 $\\theta^{\\mathrm{true}}$ 下模拟前向模型来生成合成数据，然后在测量时间的蛋白质值上添加指定标准差的独立高斯噪声。使用给定的随机种子初始化伪随机数生成器，以确保结果是确定性的。然后，在指定的 $\\theta^{\\mathrm{eval}}$ 处评估梯度，并计算该用例的 $\\varepsilon$。\n\n- 用例 A (理想路径):\n  - 诱导剂: $I = 100$.\n  - 测量时间: $\\{0.25,\\,0.5,\\,1.0,\\,2.0,\\,3.0,\\,4.0\\}$.\n  - 权重: 全部等于 $1$.\n  - 初始条件: $m(0)=0$, $p(0)=0$.\n  - 真实参数: $\\theta^{\\mathrm{true}} = [15.0,\\,50.0,\\,2.0,\\,0.3,\\,5.0,\\,0.2]^{\\top}$.\n  - 评估参数: $\\theta^{\\mathrm{eval}} = [12.0,\\,60.0,\\,2.5,\\,0.25,\\,4.5,\\,0.25]^{\\top}$.\n  - 噪声标准差: $0.05$.\n  - 随机种子: $0$.\n\n- 用例 B (单一终端测量，权重较大):\n  - 诱导剂: $I = 80$.\n  - 测量时间: $\\{6.0\\}$.\n  - 权重: $\\{10.0\\}$.\n  - 初始条件: $m(0)=0$, $p(0)=0$.\n  - 真实参数: $\\theta^{\\mathrm{true}} = [10.0,\\,40.0,\\,1.5,\\,0.4,\\,3.5,\\,0.1]^{\\top}$.\n  - 评估参数: $\\theta^{\\mathrm{eval}} = [9.0,\\,50.0,\\,1.2,\\,0.35,\\,3.2,\\,0.12]^{\\top}$.\n  - 噪声标准差: $0.0$.\n  - 随机种子: $1$.\n\n- 用例 C (慢动态，长时程):\n  - 诱导剂: $I = 120$.\n  - 测量时间: $\\{0.5,\\,1.0,\\,2.0,\\,4.0,\\,8.0,\\,12.0\\}$.\n  - 权重: 全部等于 $1$.\n  - 初始条件: $m(0)=0$, $p(0)=0$.\n  - 真实参数: $\\theta^{\\mathrm{true}} = [8.0,\\,30.0,\\,3.0,\\,0.1,\\,2.0,\\,0.05]^{\\top}$.\n  - 评估参数: $\\theta^{\\mathrm{eval}} = [7.5,\\,35.0,\\,2.8,\\,0.08,\\,1.8,\\,0.06]^{\\top}$.\n  - 噪声标准差: $0.02$.\n  - 随机种子: $2$.\n\n数值指导：\n- 使用 $10^{-12}$ 的绝对容差和 $10^{-9}$ 的相对容差来积分前向和伴随系统。\n- 对前向积分使用密集输出，以便在后向伴随积分期间评估 $x(t)$。\n- 所有变量和参数均为无量纲。\n\n最终输出。您的程序应生成一行输出，其中包含用例 A、B 和 C 的三个 $\\varepsilon$ 值，格式为一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,r_3]$）。每个 $r_i$ 都必须是一个浮点数。", "solution": "所提出的问题是计算系统生物学领域中一个适定且有科学依据的任务，具体涉及一种常见基因表达模型的参数敏感性分析。这是连续时间伴随方法在处理带有离散时间目标函数的常微分方程 (ODE) 时的典型应用。该问题是完整的、无歧义的，并且在计算上是可行的。因此，我将提供一个完整的解决方案。\n\n问题的核心是计算最小二乘目标函数 $J(\\theta)$ 关于参数向量 $\\theta$ 的梯度 $\\nabla_{\\theta}J$。该模型描述了信使RNA (mRNA) 浓度 $m(t)$ 和蛋白质浓度 $p(t)$ 的动力学。\n\n**1. 前向模型与目标函数**\n\n系统的状态由向量 $x(t) = [m(t), p(t)]^{\\top}$ 给出。其动力学由以下ODE系统控制：\n$$\n\\frac{d x}{dt} = f(x, \\theta) =\n\\begin{bmatrix}\nk_{\\mathrm{tx}}\\,H(I;K,n) - \\delta_m\\, m(t) \\\\\nk_{\\mathrm{tl}}\\, m(t) - \\delta_p\\, p(t)\n\\end{bmatrix}\n$$\n初始条件为 $x(0) = [0, 0]^{\\top}$。调控项是一个抑制性Hill函数 $H(I;K,n) = \\frac{1}{1 + (I/K)^n}$，其中 $I$ 是恒定的诱导剂浓度。参数向量为 $\\theta = [k_{\\mathrm{tx}}, K, n, \\delta_m, k_{\\mathrm{tl}}, \\delta_p]^{\\top}$。\n\n目标函数衡量在离散测量时间点 $0, t_1, \\dots, t_M$ 上，模型预测的蛋白质浓度 $p(t_i; \\theta)$ 与实验数据 $y_i$ 之间的平方差：\n$$\nJ(\\theta) \\equiv \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left(p(t_i;\\theta) - y_i\\right)^2\n$$\n其中 $w_i$ 是正权重。\n\n**2. 连续时间伴随方法**\n\n梯度 $\\nabla_{\\theta}J$ 是使用伴随方法求得的，该方法避免了为每个参数求解敏感性方程这一计算成本高昂的途径。此方法涉及状态方程的前向积分和伴随方程的后向积分。\n\n梯度表示为在时间范围 $[0, t_M]$ 上的一个积分：\n$$\n\\nabla_{\\theta}J(\\theta) = \\int_{0}^{t_M} \\left(\\frac{\\partial f}{\\partial \\theta}(t, x(t), \\theta)\\right)^{\\top}\\lambda(t)\\, dt\n$$\n这里，$\\lambda(t) \\in \\mathbb{R}^2$ 是伴随状态向量，它是一个进行时间后向积分的线性ODE系统的解。\n\n伴随状态 $\\lambda(t) = [\\lambda_m(t), \\lambda_p(t)]^\\top$ 的动力学由以下方程给出：\n$$\n\\frac{d\\lambda}{dt} = -A(t)^{\\top} \\lambda(t),\n$$\n其中 $A(t) = \\frac{\\partial f}{\\partial x}$ 是沿前向轨迹 $x(t)$ 求值的前向系统的雅可比矩阵。对于此问题，该雅可比矩阵是恒定的：\n$$\nA = \\frac{\\partial f}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial f_m}{\\partial m} & \\frac{\\partial f_m}{\\partial p} \\\\\n\\frac{\\partial f_p}{\\partial m} & \\frac{\\partial f_p}{\\partial p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-\\delta_m & 0 \\\\\nk_{\\mathrm{tl}} & -\\delta_p\n\\end{bmatrix}\n$$\n因此，伴随系统为：\n$$\n\\frac{d\\lambda}{dt} = -A^{\\top} \\lambda = -\\begin{bmatrix} -\\delta_m & k_{\\mathrm{tl}} \\\\ 0 & -\\delta_p \\end{bmatrix} \\lambda = \\begin{bmatrix} \\delta_m & -k_{\\mathrm{tl}} \\\\ 0 & \\delta_p \\end{bmatrix} \\lambda\n$$\n\n这个时间后向问题的边界条件由一个在 $t_M$ 的终端条件和在每个测量时间点 $t_i$ 的离散`跳跃`组成。终端条件是 $\\lambda(t_M^+) = \\mathbf{0}$。在每个 $t_i$，伴随状态经历一个不连续的跳跃：\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\nabla_x \\left(\\frac{1}{2}w_i\\left(p(t_i) - y_i\\right)^2\\right)\n$$\n其中 $\\lambda(t_i^-)$ 和 $\\lambda(t_i^+)$ 分别是跳跃时间点 $t_i$ 前后的状态。成本项关于状态 $x=[m, p]^\\top$ 的梯度是：\n$$\n\\nabla_x \\left(\\frac{1}{2}w_i(p(t_i) - y_i)^2\\right) =\n\\begin{bmatrix} 0 \\\\ w_i (p(t_i) - y_i) \\end{bmatrix}\n$$\n\n**3. 用于梯度计算的增广系统**\n\n为了在对 $\\lambda(t)$ 进行后向积分的同时计算梯度积分，我们定义一个增广状态向量 $Z(t) = [\\lambda(t)^\\top, G(t)^\\top]^\\top$，其中 $G(t) = \\int_{t}^{t_M} (\\frac{\\partial f}{\\partial \\theta})^\\top\\lambda(\\tau)d\\tau$。我们所求的梯度是 $G(0)$。$G(t)$ 的微分方程为 $\\frac{dG}{dt} = -(\\frac{\\partial f}{\\partial \\theta})^\\top\\lambda(t)$，终端条件为 $G(t_M)=\\mathbf{0}$。\n\n需要计算 $f$ 关于各参数的偏导数：\n令 $H = \\frac{1}{1+(I/K)^n}$。\n$$\n\\frac{\\partial f}{\\partial k_{\\mathrm{tx}}} = \\begin{bmatrix} H \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial K} = \\begin{bmatrix} k_{\\mathrm{tx}} \\frac{n}{K}H(1-H) \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial n} = \\begin{bmatrix} -k_{\\mathrm{tx}} \\ln(I/K) H(1-H) \\\\ 0 \\end{bmatrix}\n$$\n$$\n\\frac{\\partial f}{\\partial \\delta_m} = \\begin{bmatrix} -m \\\\ 0 \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial k_{\\mathrm{tl}}} = \\begin{bmatrix} 0 \\\\ m \\end{bmatrix}, \\quad\n\\frac{\\partial f}{\\partial \\delta_p} = \\begin{bmatrix} 0 \\\\ -p \\end{bmatrix}\n$$\n用于后向积分的完整增广ODE系统为：\n$$\n\\frac{dZ}{dt} =\n\\begin{bmatrix}\n\\dot{\\lambda} \\\\\n\\dot{G}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\begin{bmatrix} \\delta_m & -k_{\\mathrm{tl}} \\\\ 0 & \\delta_p \\end{bmatrix} \\lambda \\\\\n- \\left(\\frac{\\partial f(t, x(t), \\theta)}{\\partial \\theta}\\right)^\\top \\lambda\n\\end{bmatrix}\n$$\n\n**4. 数值实现策略**\n\n总体算法流程如下：\n1.  **生成合成数据**：对每个测试用例，使用真实参数 $\\theta^{\\mathrm{true}}$ 模拟前向模型以获得 $p(t_i;\\theta^{\\mathrm{true}})$。按规定添加高斯噪声以生成数据点 $y_i$。\n2.  **前向过程**：使用评估参数 $\\theta^{\\mathrm{eval}}$，从 $t=0$ 到 $t_M$ 求解前向ODE系统 $\\dot{x} = f(x, \\theta^{\\mathrm{eval}})$。此过程必须使用密集输出选项，以提供在任意时间 $t \\in [0, t_M]$ 的 $x(t)$ 的插值函数。\n3.  **后向过程**：\n    a. 初始化增广状态 $Z(t_M) = \\mathbf{0} \\in \\mathbb{R}^8$。\n    b. 按时间倒序处理测量时间点，从 $t_M$ 到 $t_1$。\n    c. 对于每个测量时间点 $t_i$ ($i=M, \\dots, 1$)：\n        i. 对 $Z$ 的伴随分量应用跳跃：$\\lambda_p(t_i^-) = \\lambda_p(t_i^+) + w_i(p(t_i;\\theta^{\\mathrm{eval}}) - y_i)$。\n        ii. 将增广系统 $\\dot{Z}$ 从 $t_i$ 到 $t_{i-1}$（使用 $t_0=0$）进行时间上的后向积分。\n    d. 在 $t=0$ 时梯度分量 $G$ 的最终状态即为所求的梯度 $\\nabla_{\\theta}J$。\n4.  **有限差分验证**：\n    a. 对于每个参数 $\\theta_j$，通过将 $\\theta_j$ 扰动 $\\pm h_j$ 并重新评估目标函数 $J$ 来计算梯度分量 $(\\nabla_{\\theta}J)_j^{\\mathrm{FD}}$ 的中心有限差分近似。这需要对 6 个参数中的每一个进行两次前向ODE求解。\n    b. 按照问题陈述中的规定，计算伴随梯度和有限差分近似之间的最大相对差异 $\\varepsilon$。\n5.  **报告结果**：收集每个测试用例的差异 $\\varepsilon$。\n\n这个过程虽然复杂，却是伴随敏感性分析理论的直接实现，并为动态模型中的梯度计算提供了一种强大而高效的方法。使用高精度的数值积分器对于保证准确性至关重要，尤其是在验证步骤中。", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A\n        {\n            \"I\": 100.0,\n            \"times\": np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0]),\n            \"weights\": np.ones(6),\n            \"theta_true\": np.array([15.0, 50.0, 2.0, 0.3, 5.0, 0.2]),\n            \"theta_eval\": np.array([12.0, 60.0, 2.5, 0.25, 4.5, 0.25]),\n            \"noise_std\": 0.05,\n            \"seed\": 0,\n        },\n        # Case B\n        {\n            \"I\": 80.0,\n            \"times\": np.array([6.0]),\n            \"weights\": np.array([10.0]),\n            \"theta_true\": np.array([10.0, 40.0, 1.5, 0.4, 3.5, 0.1]),\n            \"theta_eval\": np.array([9.0, 50.0, 1.2, 0.35, 3.2, 0.12]),\n            \"noise_std\": 0.0,\n            \"seed\": 1,\n        },\n        # Case C\n        {\n            \"I\": 120.0,\n            \"times\": np.array([0.5, 1.0, 2.0, 4.0, 8.0, 12.0]),\n            \"weights\": np.ones(6),\n            \"theta_true\": np.array([8.0, 30.0, 3.0, 0.1, 2.0, 0.05]),\n            \"theta_eval\": np.array([7.5, 35.0, 2.8, 0.08, 1.8, 0.06]),\n            \"noise_std\": 0.02,\n            \"seed\": 2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        discrepancy = process_case(case)\n        results.append(discrepancy)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case: generates data, computes adjoint and finite\n    difference gradients, and returns the discrepancy.\n    \"\"\"\n    # Unpack case parameters\n    I = case_params[\"I\"]\n    t_meas = case_params[\"times\"]\n    weights = case_params[\"weights\"]\n    theta_true = case_params[\"theta_true\"]\n    theta_eval = case_params[\"theta_eval\"]\n    noise_std = case_params[\"noise_std\"]\n    seed = case_params[\"seed\"]\n\n    # Numerical settings\n    atol, rtol = 1e-12, 1e-9\n    x0 = np.array([0.0, 0.0])\n    t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n\n    # 1. Generate synthetic data\n    rng = np.random.default_rng(seed)\n\n    def forward_rhs(t, x, theta, I_val):\n        k_tx, K, n, delta_m, k_tl, delta_p = theta\n        m, p = x\n        hill = 1.0 / (1.0 + (I_val / K)**n)\n        dm_dt = k_tx * hill - delta_m * m\n        dp_dt = k_tl * m - delta_p * p\n        return np.array([dm_dt, dp_dt])\n\n    sol_true = solve_ivp(\n        lambda t, x: forward_rhs(t, x, theta_true, I),\n        (0, t_max), x0, atol=atol, rtol=rtol, dense_output=True\n    )\n    p_true_at_t = sol_true.sol(t_meas)[1]\n    noise = rng.normal(0, noise_std, size=len(t_meas))\n    y_data = p_true_at_t + noise\n    \n    data_map = dict(zip(t_meas, y_data))\n    weights_map = dict(zip(t_meas, weights))\n\n    # 2. Compute gradient with adjoint method\n    adjoint_grad = compute_adjoint_gradient(theta_eval, I, t_meas, data_map, weights_map, atol, rtol)\n\n    # 3. Compute gradient with finite differences\n    fd_grad = compute_fd_gradient(theta_eval, I, t_meas, y_data, weights, atol, rtol)\n\n    # 4. Compare gradients\n    numerator = np.abs(adjoint_grad - fd_grad)\n    denominator = np.maximum(1.0, np.abs(fd_grad))\n    relative_discrepancy = numerator / denominator\n    \n    return np.max(relative_discrepancy)\n\ndef compute_adjoint_gradient(theta, I, t_meas, data_map, weights_map, atol, rtol):\n    \"\"\"Computes the gradient using the continuous-time adjoint method.\"\"\"\n    k_tx, K, n, delta_m, k_tl, delta_p = theta\n    x0 = np.array([0.0, 0.0])\n    t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n\n    def forward_rhs(t, x, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        m, p = x\n        hill = 1.0 / (1.0 + (I_loc / K_loc)**n_loc)\n        dm_dt = k_tx_loc * hill - delta_m_loc * m\n        dp_dt = k_tl_loc * m - delta_p_loc * p\n        return np.array([dm_dt, dp_dt])\n    \n    # Forward pass\n    sol_fwd = solve_ivp(\n        lambda t, x: forward_rhs(t, x, theta, I), \n        (0, t_max), x0, atol=atol, rtol=rtol, dense_output=True\n    )\n\n    memoized_sol_fwd = sol_fwd.sol\n\n    def backward_rhs(t, z, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        lambda_m, lambda_p = z[0], z[1]\n        \n        m_t, p_t = memoized_sol_fwd(t)\n\n        # Adjoint dynamics\n        dlambda_m_dt = delta_m_loc * lambda_m - k_tl_loc * lambda_p\n        dlambda_p_dt = delta_p_loc * lambda_p\n        \n        # Gradient dynamics (dG/dt = - (df/dtheta)^T * lambda)\n        s = I_loc / K_loc\n        s_n = s**n_loc\n        h = 1.0 / (1.0 + s_n)\n        \n        # Note: some derivatives (df/dtheta) are time-dependent via m, p\n        dG_ktx_dt = -h * lambda_m\n        if K_loc > 0:\n            dG_K_dt = -k_tx_loc * (n_loc / K_loc) * h * (1 - h) * lambda_m\n        else:\n            dG_K_dt = 0.0\n        if I_loc > 0 and K_loc > 0:\n            dG_n_dt = k_tx_loc * np.log(s) * h * (1 - h) * lambda_m\n        else:\n            dG_n_dt = 0.0\n        dG_dm_dt = m_t * lambda_m\n        dG_ktl_dt = -m_t * lambda_p\n        dG_dp_dt = p_t * lambda_p\n\n        return np.array([\n            dlambda_m_dt, dlambda_p_dt,\n            dG_ktx_dt, dG_K_dt, dG_n_dt, dG_dm_dt, dG_ktl_dt, dG_dp_dt\n        ])\n\n    # Backward pass\n    z_aug = np.zeros(8)\n    sorted_times = sorted(t_meas, reverse=True)\n    integration_points = sorted_times + ([0.0] if t_max > 0 else [])\n    \n    unique_points = sorted(list(set(integration_points)), reverse=True)\n\n    for i in range(len(unique_points) - 1):\n        t_start = unique_points[i]\n        t_end = unique_points[i+1]\n\n        # Apply jump if t_start is a measurement time\n        if t_start in data_map:\n            p_at_t_start = memoized_sol_fwd(t_start)[1]\n            residual = p_at_t_start - data_map[t_start]\n            weight = weights_map[t_start]\n            z_aug[1] += weight * residual\n        \n        # Integrate backward\n        sol_bwd = solve_ivp(\n            lambda t, z: backward_rhs(t, z, theta, I),\n            (t_start, t_end), z_aug, atol=atol, rtol=rtol, method='LSODA'\n        )\n        z_aug = sol_bwd.y[:, -1]\n    \n    # Sign flip for gradient because of G(t) definition\n    return -z_aug[2:]\n\ndef compute_fd_gradient(theta, I, t_meas, y_data, weights, atol, rtol):\n    \"\"\"Computes the gradient using central finite differences.\"\"\"\n    grad_fd = np.zeros_like(theta)\n\n    def forward_rhs(t, x, theta_loc, I_loc):\n        k_tx_loc, K_loc, n_loc, delta_m_loc, k_tl_loc, delta_p_loc = theta_loc\n        m, p = x\n        hill = 1.0 / (1.0 + (I_loc / K_loc)**n_loc)\n        dm_dt = k_tx_loc * hill - delta_m_loc * m\n        dp_dt = k_tl_loc * m - delta_p_loc * p\n        return np.array([dm_dt, dp_dt])\n\n    def objective_J(theta_loc):\n        x0 = np.array([0.0, 0.0])\n        t_max = t_meas[-1] if len(t_meas) > 0 else 0.0\n        if t_max == 0:\n            return 0.0\n        \n        sol = solve_ivp(\n            lambda t, x: forward_rhs(t, x, theta_loc, I),\n            (0, t_max), x0, atol=atol, rtol=rtol, t_eval=t_meas\n        )\n        p_model = sol.y[1]\n        \n        if p_model.shape != y_data.shape: # Handle single measurement case with t_eval\n             return 0.5 * np.sum(weights * (np.array([p_model[0]]) - y_data)**2)\n\n        return 0.5 * np.sum(weights * (p_model - y_data)**2)\n\n    for j in range(len(theta)):\n        h_j = max(1e-6 * abs(theta[j]), 1e-8)\n        \n        theta_plus = np.copy(theta)\n        theta_plus[j] += h_j\n        \n        theta_minus = np.copy(theta)\n        theta_minus[j] -= h_j\n        \n        J_plus = objective_J(theta_plus)\n        J_minus = objective_J(theta_minus)\n        \n        grad_fd[j] = (J_plus - J_minus) / (2 * h_j)\n        \n    return grad_fd\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2757781"}]}