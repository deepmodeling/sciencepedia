{"hands_on_practices": [{"introduction": "要理解神经网络如何“学习”，就需要超越高层次的概念，深入研究其参数更新的机制。这项练习将指导您手动计算一个应用于DNA序列的简单卷积神经网络（CNN）的权重梯度，从而带您了解反向传播的基本过程 [@problem_id:2749093]。完成此练习将使您对模型如何调整其内部筛选器以识别生物序列中的预测性模式有一个具体的理解。", "problem": "在合成生物学的从头调控序列设计中，卷积神经网络常用于检测编码为 one-hot 向量的脱氧核糖核酸（DNA）序列中的短序列基序。考虑一个单滤波器一维卷积模型，该模型将单个输入的 DNA 序列映射为增强子活性的标量预测值。输入序列的长度为 $L=5$，其字母表为 $\\{A,C,G,T\\}$，并以 $(A,C,G,T)$ 的列顺序进行 one-hot 编码。滤波器宽度为 $k=3$，权重矩阵为 $w \\in \\mathbb{R}^{3 \\times 4}$，其行由偏移量 $j \\in \\{0,1,2\\}$ 索引，列由核苷酸 $(A,C,G,T)$ 按此顺序索引。该模型使用互相关（深度学习中一维卷积的惯例）、步幅为 $1$、无填充且无偏置。对于位置 $t \\in \\{0,1,2\\}$，预激活值为\n$$\nz_t \\;=\\; \\sum_{j=0}^{2} \\sum_{c \\in \\{A,C,G,T\\}} w_{j,c} \\, x_{t+j,c},\n$$\n其中 $x_{i,c} \\in \\{0,1\\}$ 是位置 $i$ 处核苷酸的 one-hot 编码。激活函数为修正线性单元 (ReLU)，定义为 $\\phi(u) = \\max(0,u)$。模型通过求和进行池化，以产生 $\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$。对于目标值为 $y$ 的单个训练样本，其损失为均方误差 (MSE) $L = \\tfrac{1}{2}\\,(\\hat{y} - y)^2$。\n\n对于下面指定的单个训练样本，计算梯度 $\\nabla_w L$，即所有偏导数 $\\frac{\\partial L}{\\partial w_{j,c}}$。将你的最终答案表示为一个单行矩阵，按 $(j=0 \\text{ 行 } [A,C,G,T],\\, j=1 \\text{ 行 } [A,C,G,T],\\, j=2 \\text{ 行 } [A,C,G,T])$ 的顺序列出其元素。无需四舍五入。\n\n输入规范：\n- 序列（长度 $L=5$）：位置 $0$ 到 $4$ 为 $\\big(A,\\,C,\\,G,\\,T,\\,A\\big)$。\n- one-hot 编码的列顺序： $(A,C,G,T)$。\n- 滤波器宽度 $k=3$，步幅 $1$，有效互相关，无偏置。\n- 权重\n$$\nw \\;=\\;\n\\begin{pmatrix}\n2  -1  0  1\\\\\n-1  2  1  -2\\\\\n0  1  -1  2\n\\end{pmatrix},\n$$\n行 $j=0,1,2$，列 $(A,C,G,T)$。\n- 激活函数：修正线性单元 (ReLU), $\\phi(u) = \\max(0,u)$。\n- 池化：求和，$\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$。\n- 损失：$L = \\tfrac{1}{2}\\,(\\hat{y} - y)^2$，目标值 $y=4$。\n\n按规定提供梯度。您的答案必须是一个由实数组成的单行矩阵。", "solution": "问题要求计算损失函数 $L$ 关于滤波器权重 $w$ 的梯度，记为 $\\nabla_w L$。这需要应用多变量微积分的链式法则，这个过程在此背景下称为反向传播。我们必须计算每个偏导数 $\\frac{\\partial L}{\\partial w_{j,c}}$，其中 $j \\in \\{0,1,2\\}$ 且 $c \\in \\{A,C,G,T\\}$。\n\n模型的结构如下：\n$x \\rightarrow z \\rightarrow \\phi(z) \\rightarrow \\hat{y} \\rightarrow L$。\n损失 $L = \\frac{1}{2}(\\hat{y} - y)^2$，其中 $\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t)$ 且 $z_t = \\sum_{j=0}^{2} \\sum_{c} w_{j,c} x_{t+j,c}$。\n\n我们从前向传播开始，计算所有必要的中间值。\n\n首先，我们将输入序列 $(A,C,G,T,A)$ 表示为一个 one-hot 编码矩阵 $x \\in \\mathbb{R}^{5 \\times 4}$，其中行对应位置 $0$ 到 $4$，列对应核苷酸 $(A,C,G,T)$。\n$$\nx =\n\\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n1  0  0  0\n\\end{pmatrix}\n$$\n这些行是 $x_0, x_1, x_2, x_3, x_4$。\n\n接下来，我们使用互相关操作计算 $t \\in \\{0,1,2\\}$ 的预激活值 $z_t$。\n对于 $t=0$，输入窗口是 $(x_0, x_1, x_2)$，对应于核苷酸 $(A,C,G)$。\n$$\nz_0 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j)} = w_{0,A} + w_{1,C} + w_{2,G} = 2 + 2 + (-1) = 3\n$$\n对于 $t=1$，输入窗口是 $(x_1, x_2, x_3)$，对应于核苷酸 $(C,G,T)$。\n$$\nz_1 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j+1)} = w_{0,C} + w_{1,G} + w_{2,T} = (-1) + 1 + 2 = 2\n$$\n对于 $t=2$，输入窗口是 $(x_2, x_3, x_4)$，对应于核苷酸 $(G,T,A)$。\n$$\nz_2 = \\sum_{j=0}^2 w_{j, \\text{nuc}(j+2)} = w_{0,G} + w_{1,T} + w_{2,A} = 0 + (-2) + 0 = -2\n$$\n预激活值的向量是 $z = (3, 2, -2)$。\n\n我们将 ReLU 激活函数 $\\phi(u) = \\max(0,u)$ 应用于每个 $z_t$。\n$$\n\\phi(z_0) = \\max(0,3) = 3\n$$\n$$\n\\phi(z_1) = \\max(0,2) = 2\n$$\n$$\n\\phi(z_2) = \\max(0,-2) = 0\n$$\n激活值的向量是 $(3, 2, 0)$。\n\n最终预测值 $\\hat{y}$ 是通过对激活值求和得到的。\n$$\n\\hat{y} = \\sum_{t=0}^{2} \\phi(z_t) = 3 + 2 + 0 = 5\n$$\n\n现在，我们进行反向传播来计算梯度 $\\nabla_w L$。损失 $L$ 关于权重 $w_{j,c}$ 的偏导数由链式法则给出：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_{j,c}}\n$$\n我们进一步分析 $\\frac{\\partial \\hat{y}}{\\partial w_{j,c}}$ 这一项。由于 $\\hat{y}$ 是 $\\phi(z_t)$ 的和，并且每个 $z_t$ 都依赖于权重 $w$，我们有：\n$$\n\\frac{\\partial \\hat{y}}{\\partial w_{j,c}} = \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial w_{j,c}} = \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_{j,c}}\n$$\n结合这些，梯度元素的完整表达式是：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = \\frac{\\partial L}{\\partial \\hat{y}} \\sum_{t=0}^{2} \\frac{\\partial \\phi(z_t)}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_{j,c}}\n$$\n我们计算这个表达式的每个组成部分。\n损失 $L$ 关于预测值 $\\hat{y}$ 的导数是：\n$$\n\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y = 5 - 4 = 1\n$$\nReLU 函数的导数是一个阶跃函数：如果 $u  0$，则 $\\phi'(u) = 1$；如果 $u \\le 0$，则为 $0$。我们将其表示为 $\\mathbb{I}(u  0)$。\n$$\n\\frac{\\partial \\phi(z_0)}{\\partial z_0} = \\mathbb{I}(3  0) = 1\n$$\n$$\n\\frac{\\partial \\phi(z_1)}{\\partial z_1} = \\mathbb{I}(2  0) = 1\n$$\n$$\n\\frac{\\partial \\phi(z_2)}{\\partial z_2} = \\mathbb{I}(-2  0) = 0\n$$\n预激活值 $z_t$ 关于权重 $w_{j,c}$ 的导数是：\n$$\n\\frac{\\partial z_t}{\\partial w_{j,c}} = \\frac{\\partial}{\\partial w_{j,c}} \\left( \\sum_{j'=0}^{2} \\sum_{c'} w_{j',c'} \\, x_{t+j',c'} \\right) = x_{t+j,c}\n$$\n因为 $x_{t+j',c'}$ 相对于 $w_{j,c}$ 是常数，且仅当索引匹配时导数才非零。\n\n将这些分量代回梯度表达式中：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = (1) \\left( (1) \\cdot \\frac{\\partial z_0}{\\partial w_{j,c}} + (1) \\cdot \\frac{\\partial z_1}{\\partial w_{j,c}} + (0) \\cdot \\frac{\\partial z_2}{\\partial w_{j,c}} \\right) = \\frac{\\partial z_0}{\\partial w_{j,c}} + \\frac{\\partial z_1}{\\partial w_{j,c}}\n$$\n使用 $\\frac{\\partial z_t}{\\partial w_{j,c}} = x_{t+j,c}$，上式简化为：\n$$\n\\frac{\\partial L}{\\partial w_{j,c}} = x_{0+j,c} + x_{1+j,c}\n$$\n该公式表明，关于权重 $w_{j,c}$ 的梯度，是输入序列在位置 $j$ 和 $j+1$ 上，对应核苷酸 $c$ 的 one-hot 编码值之和。这是因为只有前两个卷积窗口（$t=0,1$）产生了正的预激活值。\n\n我们现在计算梯度矩阵 $\\nabla_w L \\in \\mathbb{R}^{3 \\times 4}$。\n\n对于 $j=0$：\n$(\\nabla_w L)_{0,c} = x_{0,c} + x_{1,c}$。\n$x_0$ 对应于 $A \\implies (1,0,0,0)$。\n$x_1$ 对应于 $C \\implies (0,1,0,0)$。\n第一行权重的梯度是 $(1,0,0,0) + (0,1,0,0) = (1,1,0,0)$。\n\n对于 $j=1$：\n$(\\nabla_w L)_{1,c} = x_{1,c} + x_{2,c}$。\n$x_1$ 对应于 $C \\implies (0,1,0,0)$。\n$x_2$ 对应于 $G \\implies (0,0,1,0)$。\n第二行权重的梯度是 $(0,1,0,0) + (0,0,1,0) = (0,1,1,0)$。\n\n对于 $j=2$：\n$(\\nabla_w L)_{2,c} = x_{2,c} + x_{3,c}$。\n$x_2$ 对应于 $G \\implies (0,0,1,0)$。\n$x_3$ 对应于 $T \\implies (0,0,0,1)$。\n第三行权重的梯度是 $(0,0,1,0) + (0,0,0,1) = (0,0,1,1)$。\n\n组装完整的梯度矩阵：\n$$\n\\nabla_w L =\n\\begin{pmatrix}\n1  1  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  1\n\\end{pmatrix}\n$$\n问题要求答案为单行矩阵，该矩阵通过串联 $\\nabla_w L$ 的行形成。\n得到的向量是 $(1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  0  0  0  1  1  0  0  0  1  1\n\\end{pmatrix}\n}\n$$", "id": "2749093"}, {"introduction": "生成模型，特别是变分自编码器（VAEs），为将庞大、高维的生物序列空间映射到一个紧凑、有意义的潜在表示提供了一种强大的范式。本练习要求您推导证据下界（ELBO），这是训练VAE的核心目标函数 [@problem_id:2749056]。通过这个推导过程，您将理解在精确重构输入数据和正则化潜在空间以确保其结构良好从而生成新设计之间，VAE是如何进行关键权衡的。", "problem": "您正在使用变分自编码器 (VAE) 对长度为 $L$、字母表大小为 $K$ 的脱氧核糖核酸序列进行建模。每个序列表示为一个 one-hot 矩阵 $S \\in \\{0,1\\}^{L \\times K}$，其行为 $\\{s_{1},\\dots,s_{L}\\}$，其中 $s_{\\ell} \\in \\{0,1\\}^{K}$ 且对于所有 $\\ell \\in \\{1,\\dots,L\\}$ 都有 $\\sum_{k=1}^{K} s_{\\ell k} = 1$。潜变量为 $z \\in \\mathbb{R}^{d}$，其先验为 $p(z) = \\mathcal{N}(0, I_{d})$。摊销变分后验是一个对角高斯分布 $q_{\\phi}(z \\mid S) = \\mathcal{N}(\\mu_{\\phi}(S), \\operatorname{diag}(\\sigma_{\\phi}^{2}(S)))$，其中 $\\mu_{\\phi}(S) \\in \\mathbb{R}^{d}$ 且 $\\sigma_{\\phi}(S) \\in \\mathbb{R}_{0}^{d}$。解码器指定了给定 $z$ 时各位置上条件独立的分类分布：对于每个位置 $\\ell \\in \\{1,\\dots,L\\}$，解码器计算 logits $a_{\\ell}(z) = W_{\\ell} z + b_{\\ell} \\in \\mathbb{R}^{K}$，其中 $W_{\\ell} \\in \\mathbb{R}^{K \\times d}$ 且 $b_{\\ell} \\in \\mathbb{R}^{K}$，并定义 $\\pi_{\\ell}(z) = \\operatorname{softmax}(a_{\\ell}(z)) \\in \\Delta^{K-1}$。似然为\n$$\np_{\\theta}(S \\mid z) = \\prod_{\\ell=1}^{L} \\operatorname{Cat}(s_{\\ell} \\mid \\pi_{\\ell}(z)).\n$$\n\n请仅使用边缘似然、Kullback–Leibler 散度和 Jensen 不等式的基本定义，推导单个观测序列 $S$ 的证据下界 (ELBO) 的显式表达式。该表达式需用关于 $q_{\\phi}(z \\mid S)$ 的期望和对角高斯分布 $q_{\\phi}(z \\mid S)$ 与标准正态先验 $p(z)$ 之间的闭式 Kullback–Leibler 散度来表示。您的表达式必须完全以 $W_{\\ell}$、$b_{\\ell}$、$\\mu_{\\phi}(S)$ 和 $\\sigma_{\\phi}(S)$ 显式表示。\n\n然后，考虑到合成生物学中用于序列的实用解码器通常包含自回归依赖，即网络将先前生成的符号作为输入，请讨论如何使用带有温度 $\\tau$ 的松弛分类分布（例如 Gumbel–Softmax，也称为 Concrete 分布）构建一个可微代理，以便在训练期间通过离散采样实现低方差梯度估计。请在较高层面上解释该松弛是如何被重参数化的，以及在极限 $\\tau \\to 0$ 和 $\\tau \\to \\infty$ 的情况下会发生什么。\n\n最后，请提供您推导出的 ELBO，它是一个包含 $S$、$\\{W_{\\ell}, b_{\\ell}\\}_{\\ell=1}^{L}$、$\\mu_{\\phi}(S)$ 和 $\\sigma_{\\phi}(S)$ 的单一闭式解析表达式，其中 Kullback–Leibler 项需以闭式形式写出。最终答案必须是单个不含任何等号的解析表达式。不需要进行数值评估，也不涉及任何单位。", "solution": "所提出的问题是有效的、有科学依据的且定义明确。它要求为指定的离散序列变分自编码器 (VAE) 模型推导证据下界 (ELBO)，讨论使用 Gumbel-Softmax 松弛将解码器扩展为自回归模型，并给出 ELBO 的最终解析表达式。我将开始解答。\n\n首先，我们推导证据下界 (ELBO)。目标是最大化观测数据点 $S$ 的边缘对数似然，即 $\\ln p(S)$。我们可以将其写为：\n$$\n\\ln p(S) = \\ln \\int p(S, z) dz\n$$\n其中 $z$ 是潜变量。由于该积分通常是难解的，我们引入一个近似后验分布 $q_{\\phi}(z \\mid S)$ 并重写表达式：\n$$\n\\ln p(S) = \\ln \\int p_{\\theta}(S \\mid z) p(z) \\frac{q_{\\phi}(z \\mid S)}{q_{\\phi}(z \\mid S)} dz = \\ln \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} \\left[ \\frac{p_{\\theta}(S \\mid z) p(z)}{q_{\\phi}(z \\mid S)} \\right]\n$$\n通过应用 Jensen 不等式，即对于凹函数 $f$（如对数函数），有 $f(\\mathbb{E}[X]) \\ge \\mathbb{E}[f(X)]$，我们得到边缘对数似然的一个下界：\n$$\n\\ln p(S) \\ge \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} \\left[ \\ln \\left( \\frac{p_{\\theta}(S \\mid z) p(z)}{q_{\\phi}(z \\mid S)} \\right) \\right]\n$$\n这个下界就是 ELBO，记为 $\\mathcal{L}(\\theta, \\phi; S)$。我们可以重新排列期望内的各项：\n$$\n\\mathcal{L}(\\theta, \\phi; S) = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} [\\ln p_{\\theta}(S \\mid z) + \\ln p(z) - \\ln q_{\\phi}(z \\mid S)]\n$$\n$$\n\\mathcal{L}(\\theta, \\phi; S) = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} [\\ln p_{\\theta}(S \\mid z)] - \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} \\left[ \\ln \\frac{q_{\\phi}(z \\mid S)}{p(z)} \\right]\n$$\n根据定义，第二项是近似后验与先验之间的 Kullback-Leibler (KL) 散度，即 $D_{KL}(q_{\\phi}(z \\mid S) \\| p(z))$。因此，ELBO 由两项组成：一个重构项和一个正则化项。\n$$\n\\mathcal{L}(\\theta, \\phi; S) = \\underbrace{\\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} [\\ln p_{\\theta}(S \\mid z)]}_{\\text{重构项}} - \\underbrace{D_{KL}(q_{\\phi}(z \\mid S) \\| p(z))}_{\\text{KL 散度项}}\n$$\n现在我们将根据问题陈述，为这两项找出显式表达式。\n\n重构项涉及似然 $p_{\\theta}(S \\mid z)$。根据模型设定：\n$$\np_{\\theta}(S \\mid z) = \\prod_{\\ell=1}^{L} \\operatorname{Cat}(s_{\\ell} \\mid \\pi_{\\ell}(z))\n$$\n给定 $z$ 时，单个观测 $S$ 的对数似然为：\n$$\n\\ln p_{\\theta}(S \\mid z) = \\sum_{\\ell=1}^{L} \\ln \\operatorname{Cat}(s_{\\ell} \\mid \\pi_{\\ell}(z))\n$$\n由于 $s_{\\ell}$ 是一个 one-hot 向量，分类分布的对数概率质量函数为 $\\ln \\prod_{k=1}^{K} (\\pi_{\\ell k}(z))^{s_{\\ell k}} = \\sum_{k=1}^{K} s_{\\ell k} \\ln(\\pi_{\\ell k}(z))$。因此：\n$$\n\\ln p_{\\theta}(S \\mid z) = \\sum_{\\ell=1}^{L} \\sum_{k=1}^{K} s_{\\ell k} \\ln(\\pi_{\\ell k}(z))\n$$\n项 $\\pi_{\\ell}(z)$ 是 logits $a_{\\ell}(z) = W_{\\ell} z + b_{\\ell}$ 的 softmax。因此，第 $k$ 个分量是 $\\pi_{\\ell k}(z) = \\frac{\\exp((W_{\\ell} z + b_{\\ell})_k)}{\\sum_{j=1}^{K} \\exp((W_{\\ell} z + b_{\\ell})_j)}$。\n其对数为 $\\ln(\\pi_{\\ell k}(z)) = (W_{\\ell} z + b_{\\ell})_k - \\ln \\sum_{j=1}^{K} \\exp((W_{\\ell} z + b_{\\ell})_j)$。\n重构项是这个量在 $q_{\\phi}(z \\mid S)$ 上的期望：\n$$\n\\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} [\\ln p_{\\theta}(S \\mid z)] = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid S)} \\left[ \\sum_{\\ell=1}^{L} \\sum_{k=1}^{K} s_{\\ell k} \\left( (W_{\\ell} z + b_{\\ell})_k - \\ln \\sum_{j=1}^{K} \\exp((W_{\\ell} z + b_{\\ell})_j) \\right) \\right]\n$$\n这个期望通常是难解的，在实践中通过从 $q_{\\phi}(z \\mid S)$ 中抽取 $z$ 的样本，使用蒙特卡洛采样来估计。\n\n第二项是变分后验与先验之间的 KL 散度。后验是 $q_{\\phi}(z \\mid S) = \\mathcal{N}(\\mu_{\\phi}(S), \\operatorname{diag}(\\sigma_{\\phi}^{2}(S)))$，先验是 $p(z) = \\mathcal{N}(0, I_d)$。两个多元正态分布 $\\mathcal{N}_1 = \\mathcal{N}(\\mu_1, \\Sigma_1)$ 和 $\\mathcal{N}_2 = \\mathcal{N}(\\mu_2, \\Sigma_2)$ 之间的 KL 散度有闭式解：\n$$\nD_{KL}(\\mathcal{N}_1 \\| \\mathcal{N}_2) = \\frac{1}{2} \\left( \\operatorname{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2-\\mu_1)^T \\Sigma_2^{-1} (\\mu_2-\\mu_1) - d + \\ln \\frac{\\det \\Sigma_2}{\\det \\Sigma_1} \\right)\n$$\n代入我们的分布，有 $\\mu_1 = \\mu_{\\phi}(S)$，$\\Sigma_1 = \\operatorname{diag}(\\sigma_{\\phi}^{2}(S))$，$\\mu_2 = 0$，$\\Sigma_2 = I_d$，维度为 $d$。这得到 $\\Sigma_2^{-1} = I_d$ 和 $\\det \\Sigma_2 = 1$。各项变为：\n$\\operatorname{tr}(I_d \\cdot \\operatorname{diag}(\\sigma_{\\phi}^{2}(S))) = \\sum_{j=1}^{d} \\sigma_{\\phi, j}(S)^2$\n$(\\mu_2-\\mu_1)^T \\Sigma_2^{-1} (\\mu_2-\\mu_1) = (-\\mu_{\\phi}(S))^T I_d (-\\mu_{\\phi}(S)) = \\sum_{j=1}^{d} \\mu_{\\phi, j}(S)^2$\n$\\ln \\frac{\\det \\Sigma_2}{\\det \\Sigma_1} = \\ln(1) - \\ln(\\prod_{j=1}^{d} \\sigma_{\\phi, j}(S)^2) = - \\sum_{j=1}^{d} \\ln(\\sigma_{\\phi, j}(S)^2)$\n将这些组合起来得到 KL 散度：\n$$\nD_{KL}(q_{\\phi}(z \\mid S) \\| p(z)) = \\frac{1}{2} \\left(\\sum_{j=1}^{d} \\sigma_{\\phi, j}(S)^2 + \\sum_{j=1}^{d} \\mu_{\\phi, j}(S)^2 - d - \\sum_{j=1}^{d} \\ln(\\sigma_{\\phi, j}(S)^2) \\right)\n$$\n$$\n= \\frac{1}{2} \\sum_{j=1}^{d} \\left( \\mu_{\\phi, j}(S)^2 + \\sigma_{\\phi, j}(S)^2 - \\ln(\\sigma_{\\phi, j}(S)^2) - 1 \\right)\n$$\n\n现在来看问题的第二部分。如果解码器是自回归的，序列的概率被分解为 $p_{\\theta}(S \\mid z) = \\prod_{\\ell=1}^L p_{\\theta}(s_{\\ell} \\mid s_{1:\\ell-1}, z)$。要训练这样的模型或评估似然，必须逐个元素地生成序列。在每一步 $\\ell$，从分类分布 $\\operatorname{Cat}(\\cdot \\mid s_{1:\\ell-1}, z)$ 中采样一个符号 $s_{\\ell}$。然后将这个采样到的符号作为模型的输入，以预测下一个符号 $s_{\\ell+1}$ 的分布。根本问题在于，从分类分布中采样是一个离散的、不可微的操作。使用 $\\operatorname{argmax}$ 来获得 one-hot 样本会中断从损失函数到网络参数的梯度流，因为 $\\operatorname{argmax}$ 函数的梯度几乎处处为零。这妨碍了通过反向传播进行端到端的训练。\n\n为了克服这个问题，可以为离散采样过程采用一个可微的代理，例如 Gumbel-Softmax（或 Concrete）分布。该方法使用了重参数化技巧。我们不是直接从分类分布中采样，而是将样本表示为分布参数和独立随机噪声的一个确定性的、可微的函数。其过程如下：\n设第 $\\ell$ 步分类分布的 logits 为 $a = (a_1, \\dots, a_K)$。\n1. 从 Gumbel(0,1) 分布中抽取 $K$ 个独立样本 $g_1, \\dots, g_K$。一个样本 $g$ 可以通过 $g = -\\ln(-\\ln(u))$ 生成，其中 $u \\sim \\operatorname{Uniform}(0,1)$。\n2. 将此噪声加到 logits 上：$a'_k = a_k + g_k$。\n3. 对这些受扰动的 logits 不使用不可微的 $\\operatorname{argmax}$，而是应用一个带有温度参数 $\\tau  0$ 的 $\\operatorname{softmax}$ 函数。得到的样本 $y = (y_1, \\dots, y_K)$ 由下式给出：\n$$\ny_k = \\frac{\\exp((a_k + g_k)/\\tau)}{\\sum_{j=1}^K \\exp((a_j + g_j)/\\tau)}\n$$\n向量 $y$ 现在是 Gumbel-Softmax 分布的一个样本。它是一个“软”的或松弛的 one-hot 向量，位于单纯形 $\\Delta^{K-1}$ 中。关键的是，$y$ 是 logits $a$ 的一个可微函数，因此梯度可以通过这个采样步骤进行反向传播。随机的 Gumbel 噪声在梯度路径之外。\n\n温度参数 $\\tau$ 控制 Gumbel-Softmax 分布逼近分类分布的程度：\n- 当 $\\tau \\to 0$（低温）时，softmax 函数变得更“硬”，并趋近于 $\\operatorname{argmax}$。样本向量 $y$ 趋近于一个离散的 one-hot 向量。这使得模型能够生成类似离散的序列，但在训练期间可能导致梯度的高方差。\n- 当 $\\tau \\to \\infty$（高温）时，softmax 的输出变得平坦，样本向量 $y$ 趋近于一个均匀分布 $(1/K, \\dots, 1/K)$。这导致了低方差的梯度，但对离散分类样本的近似效果很差。\n在训练期间，一个常见的策略是将温度从一个较高的值退火到一个较低的值，这使得初始学习稳定，并在后期实现更准确的离散采样。\n\n结合显式的重构项和 KL 散度项，下面提供了 ELBO 的最终表达式。", "answer": "$$\n\\boxed{\\mathbb{E}_{z \\sim \\mathcal{N}(\\mu_{\\phi}(S), \\operatorname{diag}(\\sigma_{\\phi}^{2}(S)))} \\left[ \\sum_{\\ell=1}^{L} \\sum_{k=1}^{K} s_{\\ell k} \\left( (W_{\\ell} z + b_{\\ell})_k - \\ln \\sum_{j=1}^{K} \\exp((W_{\\ell} z + b_{\\ell})_j) \\right) \\right] - \\frac{1}{2} \\sum_{j=1}^{d} \\left( \\mu_{\\phi, j}(S)^{2} + \\sigma_{\\phi, j}(S)^{2} - \\ln(\\sigma_{\\phi, j}(S)^{2}) - 1 \\right)}\n$$", "id": "2749056"}, {"introduction": "贝叶斯优化在复杂设计空间中高效导航的能力，源于其由采集函数控制的智能决策过程。本练习侧重于推导期望提升（EI）的闭合形式表达式，这是最经典和最有效的采集函数之一 [@problem_id:2749128]。通过完成这一推导，您将从第一性原理层面理解，代理模型的预测及其相关的不确定性是如何在数学上结合起来，以量化一个新实验的潜在价值。", "problem": "在一个旨在优化合成启动子以最大化转录输出的闭环设计活动中，您使用了带有高斯过程 (GP) 代理模型的贝叶斯优化。在一个候选设计 $x$ 处，对于未知的真实目标函数 $f(x)$，GP 的后验预测分布是均值为 $\\mu(x)$、方差为 $\\sigma^{2}(x)$ 的高斯分布，记为 $f(x) \\sim \\mathcal{N}\\!\\left(\\mu(x), \\sigma^{2}(x)\\right)$。令当前观测到的最佳目标值为 $f_{\\text{best}}$，并假设目标是最大化 $f(x)$。\n\n定义改进随机变量为 $I(x) = \\max\\!\\big(0, f(x) - f_{\\text{best}}\\big)$，期望改进 (EI) 采集函数为 $\\text{EI}(x) = \\mathbb{E}\\!\\left[I(x)\\right]$，其中期望是在 GP 后验预测分布下计算的。从这些定义以及 $f(x)$ 的正态性出发，推导出一个用 $\\mu(x)$、$\\sigma(x)$、$f_{\\text{best}}$、标准正态累积分布函数 $\\Phi(\\cdot)$ 和标准正态概率密度函数 $\\phi(\\cdot)$ 表示的 $\\text{EI}(x)$ 的闭式表达式。假设 $\\sigma(x)  0$。您的最终答案必须是单一的闭式解析表达式。不要在最终答案框中提供中间步骤。", "solution": "该问题陈述是贝叶斯优化领域的一个标准推导，它具有科学依据、提法恰当且客观，不含任何逻辑或事实错误。因此，我们着手进行解答。\n\n目标是推导期望改进 (EI) 采集函数的闭式表达式。我们有以下定义：\n在点 $x$ 处，目标函数 $f(x)$ 的后验预测分布是高斯分布：\n$$f(x) \\sim \\mathcal{N}(\\mu(x), \\sigma^{2}(x))$$\n改进随机变量 $I(x)$ 是相对于当前观测到的最佳值 $f_{\\text{best}}$ 定义的：\n$$I(x) = \\max(0, f(x) - f_{\\text{best}})$$\n期望改进是该随机变量的期望，该期望是关于 $f(x)$ 的后验预测分布计算的：\n$$\\text{EI}(x) = \\mathbb{E}[I(x)] = \\mathbb{E}[\\max(0, f(x) - f_{\\text{best}})]$$\n为了计算这个期望，我们必须对 $f(x)$ 的概率密度上的改进进行积分。令 $p(y | x)$ 为高斯分布 $\\mathcal{N}(\\mu(x), \\sigma^{2}(x))$ 在 $y$ 处的概率密度函数 (PDF)。那么期望由以下积分给出：\n$$\\text{EI}(x) = \\int_{-\\infty}^{\\infty} \\max(0, y - f_{\\text{best}}) \\, p(y | x) \\, dy$$\n$\\max(0, y - f_{\\text{best}})$ 项仅在 $y  f_{\\text{best}}$ 时非零。因此，我们可以将积分下限更改为 $f_{\\text{best}}$ 并去掉 $\\max$ 算子：\n$$\\text{EI}(x) = \\int_{f_{\\text{best}}}^{\\infty} (y - f_{\\text{best}}) \\, p(y | x) \\, dy$$\nPDF $p(y|x)$ 由下式给出：\n$$p(y|x) = \\frac{1}{\\sqrt{2\\pi}\\sigma(x)} \\exp\\left(-\\frac{(y - \\mu(x))^2}{2\\sigma^{2}(x)}\\right)$$\n为了简化积分，我们进行变量替换以使分布标准化。令新的随机变量 $Z$ 定义为：\n$$Z = \\frac{y - \\mu(x)}{\\sigma(x)}$$\n这个变量 $Z$ 服从标准正态分布，$Z \\sim \\mathcal{N}(0, 1)$。根据这个定义，我们有 $y = \\mu(x) + \\sigma(x)Z$，其微分为 $dy = \\sigma(x) dZ$。$Z$ 的 PDF 是标准正态 PDF，记为 $\\phi(z)$。积分下限 $y = f_{\\text{best}}$ 变为：\n$$z_{lower} = \\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}$$\n上限 $y \\to \\infty$ 对应于 $z \\to \\infty$。将这些代入积分中得到：\n$$\\text{EI}(x) = \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} ((\\mu(x) + \\sigma(x)z) - f_{\\text{best}}) \\, \\phi(z) \\, dz$$\n我们可以重新整理积分内的项，并将其分成两部分：\n$$\\text{EI}(x) = \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} (\\mu(x) - f_{\\text{best}}) \\phi(z) dz + \\int_{\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)}}^{\\infty} \\sigma(x)z \\phi(z) dz$$\n为了记法清晰，我们定义缩放后的改进项：\n$$Z_{imp} = \\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}$$\n注意 $\\frac{f_{\\text{best}} - \\mu(x)}{\\sigma(x)} = -Z_{imp}$。积分变为：\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\int_{-Z_{imp}}^{\\infty} \\phi(z) dz + \\sigma(x) \\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz$$\n我们分别计算每个积分。\n\n对于第一个积分，我们认识到它与标准正态分布的累积分布函数 (CDF) 相关，即 $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) dt$。\n$$\\int_{-Z_{imp}}^{\\infty} \\phi(z) dz = 1 - \\int_{-\\infty}^{-Z_{imp}} \\phi(z) dz = 1 - \\Phi(-Z_{imp})$$\n利用标准正态 CDF 的性质 $1 - \\Phi(-z) = \\Phi(z)$，上式简化为：\n$$\\int_{-Z_{imp}}^{\\infty} \\phi(z) dz = \\Phi(Z_{imp})$$\n所以第一项是 $(\\mu(x) - f_{\\text{best}}) \\Phi(Z_{imp})$。\n\n对于第二个积分，我们有 $\\int z \\phi(z) dz$。被积函数是 $z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$。\n$$\\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz = \\int_{-Z_{imp}}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$$\n这个积分可以直接求解，因为 $-\\phi'(z) = z \\phi(z)$。或者，使用换元法，令 $u = -z^2/2$，则 $du = -z dz$。\n$$\\int z \\exp\\left(-\\frac{z^2}{2}\\right) dz = -\\int \\exp(u) du = -\\exp(u) = -\\exp\\left(-\\frac{z^2}{2}\\right)$$\n计算定积分：\n$$\\sigma(x) \\int_{-Z_{imp}}^{\\infty} z \\phi(z) dz = \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\left[ -\\exp\\left(-\\frac{z^2}{2}\\right) \\right]_{-Z_{imp}}^{\\infty}$$\n$$= \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\left( 0 - \\left(-\\exp\\left(-\\frac{(-Z_{imp})^2}{2}\\right)\\right) \\right)$$\n$$= \\frac{\\sigma(x)}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{Z_{imp}^2}{2}\\right)$$\n这恰好是 $\\sigma(x)\\phi(Z_{imp})$，因为标准正态 PDF 是一个偶函数，$\\phi(z) = \\phi(-z)$，并且其定义为 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{z^2}{2})$。\n\n将这两项合并，我们得到：\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\Phi(Z_{imp}) + \\sigma(x) \\phi(Z_{imp})$$\n将 $Z_{imp}$ 的定义代回：\n$$\\text{EI}(x) = (\\mu(x) - f_{\\text{best}}) \\Phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right)$$\n这就是期望改进的最终闭式表达式。它正确地依赖于 $\\mu(x)$、$\\sigma(x)$、$f_{\\text{best}}$ 以及标准正态 CDF $\\Phi$ 和 PDF $\\phi$。假设 $\\sigma(x)  0$ 确保了分母不为零，从而验证了推导的有效性。", "answer": "$$\\boxed{(\\mu(x) - f_{\\text{best}}) \\Phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{\\mu(x) - f_{\\text{best}}}{\\sigma(x)}\\right)}$$", "id": "2749128"}]}