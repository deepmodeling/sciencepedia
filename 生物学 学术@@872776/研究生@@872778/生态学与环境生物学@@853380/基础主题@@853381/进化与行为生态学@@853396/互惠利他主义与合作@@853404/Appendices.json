{"hands_on_practices": [{"introduction": "任何关于合作演化的分析都始于对个体策略间直接互动的理解。重复囚徒困境 (repeated Prisoner's Dilemma) 为此提供了一个经典的框架。通过计算不同策略组合（如“以牙还牙”策略和“永远背叛”策略）在无限重复博弈中的贴现收益，我们可以揭示决定一个策略成功与否的基本“交战规则”[@problem_id:2527624]。", "problem": "一个无限重复的囚徒困境（PD）在来自集合 $\\{\\text{始终背叛 (ALLD)}, \\text{始终合作 (ALLC)}, \\text{一报还一报 (TFT)}, \\text{冷酷触发 (GRIM)}\\}$ 的两个纯策略之间进行。单次囚徒困境的支付由 $T$ (诱惑)、$R$ (奖励)、$P$ (惩罚) 和 $S$ (傻瓜) 表示，满足 $TRPS$ 和 $2RT+S$。时间是无限期的，共同贴现因子为 $\\delta \\in [0,1)$，每期支付通过归一化贴现值进行汇总\n$$\nV \\;=\\; (1-\\delta)\\sum_{t=1}^{\\infty} \\delta^{t-1} u_t,\n$$\n其中 $u_t$ 是第 $t$ 期的阶段支付。策略定义如下：$\\text{ALLD}$ 在每期都背叛；$\\text{ALLC}$ 在每期都合作；$\\text{TFT}$ 在第1期合作，此后重复对手上一期的实际行动；$\\text{GRIM}$ 一直合作，直到对手背叛一次，之后它就永远背叛。\n\n任务：\n- 仅使用上述原始要素，推导 $\\{\\text{ALLD}, \\text{ALLC}, \\text{TFT}, \\text{GRIM}\\}$ 中每对无序配对的归一化贴现支付的闭式表达式，作为 $\\delta$、$T$、$R$、$P$ 和 $S$ 的函数。\n- 根据这些表达式，为每对无序策略确定哪一方获得更大的归一化贴现支付（作为 $\\delta$ 的函数），如果所有 $\\delta$ 的支付都相等，请明确说明。\n- 最后，求解临界贴现因子 $\\delta^{\\star} \\in [0,1]$，在该值下，$\\text{ALLD}$ 对抗 $\\text{TFT}$ 的归一化贴现支付等于 $\\text{TFT}$ 对抗 $\\text{ALLD}$ 的归一化贴现支付。以 $\\delta^{\\star}$ 的精确值形式提供最终答案（无单位）。无需四舍五入。", "solution": "在尝试求解之前，首先对问题进行严格的验证。\n\n**步骤1：提取已知条件**\n问题陈述中逐字提供了以下信息：\n- 博弈类型：无限重复囚徒困境（PD）。\n- 纯策略集合：$\\{\\text{始终背叛 (ALLD)}, \\text{始终合作 (ALLC)}, \\text{一报还一报 (TFT)}, \\text{冷酷触发 (GRIM)}\\}$。\n- 单次PD支付：$T$ (诱惑)、$R$ (奖励)、$P$ (惩罚) 和 $S$ (傻瓜)。\n- 支付排序约束：$TRPS$ 和 $2RT+S$。\n- 时间范围：无限，共同贴现因子为 $\\delta \\in [0,1)$。\n- 归一化贴现值：$V = (1-\\delta)\\sum_{t=1}^{\\infty} \\delta^{t-1} u_t$。\n- 策略定义：\n    - $\\text{ALLD}$：在每期都背叛。\n    - $\\text{ALLC}$：在每期都合作。\n    - $\\text{TFT}$：在第1期合作，此后重复对手上一期的实际行动。\n    - $\\text{GRIM}$：一直合作，直到对手背叛一次，之后它就永远背叛。\n- 任务：\n    1. 推导每对无序配对的归一化贴现支付的闭式表达式。\n    2. 确定每对策略中哪一方获得更大的支付。\n    3. 求解 $\\text{ALLD}$ 对抗 $\\text{TFT}$ 的支付等于 $\\text{TFT}$ 对抗 $\\text{ALLD}$ 的支付时的临界贴现因子 $\\delta^{\\star} \\in [0,1]$。\n\n**步骤2：使用提取的已知条件进行验证**\n根据既定标准评估问题的有效性。\n- **科学基础**：该问题是博弈论和演化生物学中一个标准的、典型的模型，用于研究合作的出现。重复囚徒困境、所指定的策略（特别是 $\\text{TFT}$ 和 $\\text{GRIM}$）以及包括条件 $TRPS$ 和 $2RT+S$ 在内的支付结构，在科学文献中都是基础且公认的。该问题具有科学合理性。\n- **适定性**：该问题在数学上是明确无误的。所有变量、函数和目标都得到了清晰的定义。所提供的信息是完整、一致且充分的，足以推导出所有规定任务的唯一解。\n- **客观性**：语言正式、精确，没有主观论断或含糊之处。\n\n**步骤3：结论与行动**\n问题有效。这是一个适定的、有科学依据的问题，可以利用给定信息解决。现在开始推导解。\n\n对于一个恒定的支付流，即对所有 $t \\geq 1$ 都有 $u_t = u$，其归一化贴现值为 $V = (1-\\delta)\\sum_{t=1}^{\\infty} \\delta^{t-1} u = (1-\\delta) u (\\frac{1}{1-\\delta}) = u$。如果一个支付流是 $(u_1, u_2, u_2, \\dots)$，其值为 $V = (1-\\delta)[u_1 + \\delta u_2 + \\delta^2 u_2 + \\dots] = (1-\\delta)[u_1 + \\frac{\\delta u_2}{1-\\delta}] = (1-\\delta)u_1 + \\delta u_2$。这些公式将用于后续计算。令 $V(A, B)$ 表示策略 $A$ 在与策略 $B$ 对抗时获得的支付。\n\n**第一部分：支付推导**\n需要考虑的有 $\\binom{4}{2} + 4 = 10$ 种可能的配对。\n\n1.  **ALLC 对 ALLC**：双方在所有时期都合作。每方的支付流为 $(R, R, R, \\dots)$。因此，$V(\\text{ALLC, ALLC}) = R$。\n\n2.  **ALLD 对 ALLD**：双方在所有时期都背叛。每方的支付流为 $(P, P, P, \\dots)$。因此，$V(\\text{ALLD, ALLD}) = P$。\n\n3.  **TFT 对 TFT**：双方都从合作开始。在随后的每个时期，每一方都观察到对方的合作并继而合作。支付流为 $(R, R, R, \\dots)$。因此，$V(\\text{TFT, TFT}) = R$。\n\n4.  **GRIM 对 GRIM**：双方都从合作开始。由于从未发生背叛，双方都无限期地合作。支付流为 $(R, R, R, \\dots)$。因此，$V(\\text{GRIM, GRIM}) = R$。\n\n5.  **ALLC 对 ALLD**：ALLC 始终合作，ALLD 始终背叛。ALLC 的支付流为 $(S, S, S, \\dots)$，ALLD 的支付流为 $(T, T, T, \\dots)$。\n    $V(\\text{ALLC, ALLD}) = S$。\n    $V(\\text{ALLD, ALLC}) = T$。\n\n6.  **ALLC 对 TFT**：ALLC 总是合作。TFT 以合作开始。TFT 观察到 ALLC 的合作并继续合作。结果是所有时期都相互合作。\n    $V(\\text{ALLC, TFT}) = R$ 且 $V(\\text{TFT, ALLC}) = R$。\n\n7.  **ALLC 对 GRIM**：ALLC 总是合作。GRIM 以合作开始，且从未观察到背叛，因此它继续合作。结果是所有时期都相互合作。\n    $V(\\text{ALLC, GRIM}) = R$ 且 $V(\\text{GRIM, ALLC}) = R$。\n\n8.  **ALLD 对 TFT**：ALLD 总是背叛。TFT 以合作开始。\n    - 时期 $t=1$：TFT 选择合作，ALLD 选择背叛。TFT 的支付为 $S$，ALLD 的支付为 $T$。\n    - 时期 $t \\geq 2$：TFT 观察到 $t-1$ 期的背叛并选择背叛。ALLD 继续选择背叛。双方支付均为 $P$。\n    TFT 的支付流为 $(S, P, P, \\dots)$。ALLD 的支付流为 $(T, P, P, \\dots)$。\n    $V(\\text{TFT, ALLD}) = (1-\\delta)S + \\delta P$。\n    $V(\\text{ALLD, TFT}) = (1-\\delta)T + \\delta P$。\n\n9.  **ALLD 对 GRIM**：ALLD 总是背叛。GRIM 以合作开始。\n    - 时期 $t=1$：GRIM 选择合作，ALLD 选择背叛。GRIM 的支付为 $S$，ALLD 的支付为 $T$。\n    - 时期 $t \\geq 2$：GRIM 观察到背叛并转为永久背叛。ALLD 继续选择背叛。双方支付均为 $P$。\n    支付流与 ALLD 对 TFT 的配对相同。\n    $V(\\text{GRIM, ALLD}) = (1-\\delta)S + \\delta P$。\n    $V(\\text{ALLD, GRIM}) = (1-\\delta)T + \\delta P$。\n\n10. **TFT 对 GRIM**：两种策略都以合作开始。由于没有玩家曾背叛，他们在所有后续时期都合作。\n    $V(\\text{TFT, GRIM}) = R$ 且 $V(\\text{GRIM, TFT}) = R$。\n\n**第二部分：无序配对的支付比较**\n\n-   **ALLC 对 ALLD**：$V(\\text{ALLD, ALLC}) = T$ 且 $V(\\text{ALLC, ALLD}) = S$。因为 $T > S$，所以 $\\text{ALLD}$ 获得更大的支付。\n-   **ALLC 对 TFT**：$V(\\text{TFT, ALLC}) = R$ 且 $V(\\text{ALLC, TFT}) = R$。对于所有 $\\delta$，支付相等。\n-   **ALLC 对 GRIM**：$V(\\text{GRIM, ALLC}) = R$ 且 $V(\\text{ALLC, GRIM}) = R$。对于所有 $\\delta$，支付相等。\n-   **ALLD 对 TFT**：$V(\\text{ALLD, TFT}) = (1-\\delta)T + \\delta P$ 且 $V(\\text{TFT, ALLD}) = (1-\\delta)S + \\delta P$。差值为 $(1-\\delta)(T-S)$。鉴于 $T > S$ 且 $\\delta \\in [0, 1)$，我们有 $1-\\delta > 0$，所以差值严格为正。因此，对于所有 $\\delta \\in [0,1)$，$\\text{ALLD}$ 获得更大的支付。\n-   **ALLD 对 GRIM**：支付与 ALLD 对 TFT 的情况相同，因此对于所有 $\\delta \\in [0,1)$，$\\text{ALLD}$ 获得更大的支付。\n-   **TFT 对 GRIM**：$V(\\text{TFT, GRIM}) = R$ 且 $V(\\text{GRIM, TFT}) = R$。对于所有 $\\delta$，支付相等。\n\n**第三部分：临界贴现因子计算**\n\n最后的任务是找到 $\\delta^{\\star} \\in [0,1]$，使得 $\\text{ALLD}$ 对抗 $\\text{TFT}$ 的归一化贴现支付等于 $\\text{TFT}$ 对抗 $\\text{ALLD}$ 的归一化贴现支付。这是第一部分结果的直接应用，需要解以下方程：\n$$\nV(\\text{ALLD, TFT}) = V(\\text{TFT, ALLD})\n$$\n代入推导出的表达式：\n$$\n(1-\\delta^{\\star})T + \\delta^{\\star}P = (1-\\delta^{\\star})S + \\delta^{\\star}P\n$$\n等式两边的 $\\delta^{\\star}P$ 项相消：\n$$\n(1-\\delta^{\\star})T = (1-\\delta^{\\star})S\n$$\n该方程可重排为：\n$$\n(1-\\delta^{\\star})T - (1-\\delta^{\\star})S = 0\n$$\n$$\n(1-\\delta^{\\star})(T - S) = 0\n$$\n问题指定了严格不等式 $T > S$，这意味着 $(T-S)$ 项是一个非零正常数。要使两项的乘积为零，至少有一项必须为零。因此，我们必须有：\n$$\n1 - \\delta^{\\star} = 0\n$$\n解出 $\\delta^{\\star}$ 得到唯一解：\n$$\n\\delta^{\\star} = 1\n$$\n该值在问题指定的搜索集合 $\\delta^{\\star} \\in [0,1]$ 内。", "answer": "$$\\boxed{1}$$", "id": "2527624"}, {"introduction": "个体间的收益差异是自然选择的驱动力。复制子动态方程 (replicator equation) 是一个强有力的工具，它将个体层面的博弈收益与策略在群体中频率的演变联系起来。这个练习将让你运用该方程，模拟一个由“以牙还牙”(TFT) 和“永远背叛”(ALLD) 策略组成的种群的演化轨迹，从而理解合作策略如何在自私的对手面前立足和扩张 [@problem_id:2527577]。", "problem": "一个无限大、充分混合的种群由两种行为策略组成，它们在成对、无限重复的囚徒困境 (PD) 博弈中相互作用，博弈的几何持续概率为 $\\delta \\in (0,1)$。单次囚徒困境博弈具有满足 $TRPS$ 的标准收益，其中 $T$ 是诱惑收益， $R$ 是相互合作的奖励， $P$ 是相互背叛的惩罚， $S$ 是受骗者收益。每次重复博弈产生一个阶段收益序列 $\\{u_t\\}_{t=1}^{\\infty}$，个体在单次博弈中的适应度贡献是每阶段的几何加权平均收益，即\n$$(1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}u_t.$$\n这两种策略是：\n- 一报还一报 (TFT)：在第一回合合作；此后，如果对手在前一回合合作，则本回合合作，否则背叛。\n- 始终背叛 (ALLD)：在每一回合都背叛。\n\n设 $x \\in [0,1]$ 表示种群中 TFT 策略的频率。假设随机匹配，且适应度等于每次博弈的期望加权平均收益。频率动态遵循双策略的连续时间复制子动态方程。\n\n任务：\n1. 仅从上述重复互动收益和复制子动态方程的定义出发，推导 TFT 和 ALLD 的期望适应度，将其表示为 $x$、$\\delta$ 以及 PD 收益 $T,R,P,S$ 的函数。\n2. 推导关于 $x$ 的复制子动态方程，并确定在 $[0,1]$ 内的所有不动点。\n3. 根据 $\\delta$ 和收益 $T,R,P,S$ 分析边界不动点和任何内部不动点的局部稳定性，并从第一性原理推导出明确的条件。\n4. 作为最终答案，报告内部不动点频率 $x^{\\ast}$（如果存在）的闭式表达式，该表达式是 $\\delta$、$T$、$R$、$P$ 和 $S$ 的函数。不要简化为数值。\n\n你的最终答案必须是单个闭式表达式。无需进行四舍五入。", "solution": "首先对问题陈述进行验证。\n\n步骤 1：提取已知条件\n- 种群：无限大、充分混合。\n- 策略：一报还一报 (TFT) 和始终背叛 (ALLD)。\n- TFT 的频率：$x \\in [0,1]$。\n- 互动：成对、无限重复的囚徒困境 (PD)。\n- 持续概率：$\\delta \\in (0,1)$。\n- 单次 PD 收益：$TRPS$。\n- 单次博弈的适应度贡献：$W = (1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}u_t$，其中 $u_t$ 是阶段 $t$ 的收益。\n- 动态：双策略的连续时间复制子动态方程。\n\n步骤 2：使用提取的已知条件进行验证\n该问题是演化博弈论中一个标准的、基础的模型，具体涉及合作的演化。\n- **科学依据**：该问题基于复制子动态方程和重复博弈的成熟框架，这是理论生物学和经济学的核心内容。囚徒困境是研究合作的典范模型。在此框架内，所有前提在事实上都是合理的。\n- **提法明确**：问题定义清晰，包含了所有必要的参数和方程。其结构设计得可以在特定条件下得到唯一且稳定的解。\n- **客观性**：语言精确，没有任何主观或非科学的主张。任务是分析性的，需要严谨的推导。\n- 问题是自洽、一致的，并且不包含说明中列出的任何使其无效的缺陷。\n\n步骤 3：结论与行动\n问题有效。将提供完整解答。\n\n解答过程遵循问题陈述中指定的四个任务。\n\n任务 1：期望适应度的推导\n首先，我们确定两种策略 TFT 和 ALLD 之间每种可能的成对博弈的折现收益。适应度贡献是几何加权平均收益，计算公式为 $(1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}u_t$。\n\n- **TFT vs. TFT**：两个个体在第一回合都合作。由于对手合作了，双方在第二回合继续合作，如此无限持续下去。每个参与者的收益序列是 $\\{R, R, R, \\dots\\}$。折现收益为：\n$$W(\\text{TFT} | \\text{TFT}) = (1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}R = (1-\\delta)R \\left(\\frac{1}{1-\\delta}\\right) = R$$\n\n- **TFT vs. ALLD**：在第一回合 ($t=1$)，TFT 合作而 ALLD 背叛。TFT 得到受骗者收益 $S$，而 ALLD 得到诱惑收益 $T$。在之后的所有回合 ($t \\ge 2$)，TFT 观察到对手在 $t-1$ 回合的背叛，并以背叛作为回应。ALLD 继续背叛。因此，从第二回合开始，两个个体都选择背叛，在这些回合中均获得惩罚收益 $P$。\nTFT 的收益序列是 $\\{S, P, P, \\dots\\}$，其折现收益为：\n$$W(\\text{TFT} | \\text{ALLD}) = (1-\\delta) \\left( S \\cdot \\delta^0 + \\sum_{t=2}^{\\infty} \\delta^{t-1}P \\right) = (1-\\delta) \\left( S + P \\frac{\\delta}{1-\\delta} \\right) = (1-\\delta)S + \\delta P$$\nALLD 的收益序列是 $\\{T, P, P, \\dots\\}$，其折现收益为：\n$$W(\\text{ALLD} | \\text{TFT}) = (1-\\delta) \\left( T \\cdot \\delta^0 + \\sum_{t=2}^{\\infty} \\delta^{t-1}P \\right) = (1-\\delta) \\left( T + P \\frac{\\delta}{1-\\delta} \\right) = (1-\\delta)T + \\delta P$$\n\n- **ALLD vs. ALLD**：两个个体从第一回合开始，在每一回合都背叛。每个参与者的收益序列是 $\\{P, P, P, \\dots\\}$。折现收益为：\n$$W(\\text{ALLD} | \\text{ALLD}) = (1-\\delta)\\sum_{t=1}^{\\infty}\\delta^{t-1}P = (1-\\delta)P \\left(\\frac{1}{1-\\delta}\\right) = P$$\n\n现在，我们可以写出每种策略的期望适应度，即在所有可能博弈中的平均收益。设 $x$ 为 TFT 的频率。一个 TFT 个体遇到另一个 TFT 的概率为 $x$，遇到一个 ALLD 的概率为 $1-x$。\n\nTFT 的期望适应度 $W_{\\text{TFT}}(x)$ 是：\n$$W_{\\text{TFT}}(x) = x \\cdot W(\\text{TFT} | \\text{TFT}) + (1-x) \\cdot W(\\text{TFT} | \\text{ALLD})$$\n$$W_{\\text{TFT}}(x) = xR + (1-x)((1-\\delta)S + \\delta P)$$\n\nALLD 的期望适应度 $W_{\\text{ALLD}}(x)$ 是：\n$$W_{\\text{ALLD}}(x) = x \\cdot W(\\text{ALLD} | \\text{TFT}) + (1-x) \\cdot W(\\text{ALLD} | \\text{ALLD})$$\n$$W_{\\text{ALLD}}(x) = x((1-\\delta)T + \\delta P) + (1-x)P$$\n\n任务 2：复制子动态方程和不动点\nTFT 策略频率 $x$ 的连续时间复制子动态方程由下式给出：\n$$\\dot{x} = \\frac{dx}{dt} = x (W_{\\text{TFT}}(x) - \\bar{W}(x))$$\n其中 $\\bar{W}(x) = x W_{\\text{TFT}}(x) + (1-x) W_{\\text{ALLD}}(x)$ 是种群的平均适应度。该方程可以简化为：\n$$\\dot{x} = x(1-x) (W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x))$$\n我们来计算适应度差异 $W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x)$：\n$$W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x) = [xR + (1-x)((1-\\delta)S + \\delta P)] - [x((1-\\delta)T + \\delta P) + (1-x)P]$$\n按 $x$ 的幂次重新整理各项：\n$$= x(R - ((1-\\delta)T + \\delta P)) - (1-x)(P - ((1-\\delta)S + \\delta P))$$\n$$= x(R - (1-\\delta)T - \\delta P) + (1-x)(-(P - (1-\\delta)S - \\delta P))$$\n$$= x(R - T + \\delta T - \\delta P) + (1-x)( (1-\\delta)S - (1-\\delta)P )$$\n$$= x(R - T + \\delta(T-P)) + (1-x)(S-P)(1-\\delta)$$\n将此表达式展开为关于 $x$ 的线性函数：\n$$W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x) = x[R - T + \\delta(T-P) - (S-P)(1-\\delta)] + (S-P)(1-\\delta)$$\n$$= x[R - T + \\delta T - \\delta P - S + P + \\delta S - \\delta P] + (S-P)(1-\\delta)$$\n$$= x[R - T - S + P + \\delta(T+S-2P)] + (S-P)(1-\\delta)$$\n复制子动态的不动点是使 $\\dot{x}=0$ 的 $x \\in [0,1]$ 的值。\n从 $\\dot{x} = x(1-x)(W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x)) = 0$ 中，我们确定了三个可能的不动点：\n1. $x = 0$：ALLD 的单态种群。\n2. $x = 1$：TFT 的单态种群。\n3. 一个内部不动点 $x^*$，满足 $W_{\\text{TFT}}(x^*) - W_{\\text{ALLD}}(x^*) = 0$。\n令适应度差异的线性表达式为零，得到：\n$$x^*[R - T - S + P + \\delta(T+S-2P)] + (S-P)(1-\\delta) = 0$$\n求解 $x^*$：\n$$x^* = -\\frac{(S-P)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}$$\n$$x^* = \\frac{(P-S)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}$$\n只有当 $x^* \\in (0,1)$ 时，这个内部不动点才具有生物学意义。\n\n任务 3：局部稳定性分析\n设 $f(x) = \\dot{x} = x(1-x) (W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x))$。不动点 $x_{fp}$ 的局部稳定性由导数 $f'(x_{fp})$ 的符号决定。如果 $f'(x_{fp})  0$，该不动点是局部稳定的。如果 $f'(x_{fp}) > 0$，则它是不稳定的。\n设 $g(x) = W_{\\text{TFT}}(x) - W_{\\text{ALLD}}(x)$。那么 $f(x) = x(1-x)g(x)$。\n$f'(x) = (1-2x)g(x) + x(1-x)g'(x)$。\n\n- $x=0$ 的稳定性：\n$f'(0) = (1-0)g(0) + 0 = g(0) = W_{\\text{TFT}}(0) - W_{\\text{ALLD}}(0)$。\n在 $x=0$ 处，$W_{\\text{TFT}}(0)=(1-\\delta)S+\\delta P$ 且 $W_{\\text{ALLD}}(0)=P$。\n$f'(0) = (1-\\delta)S+\\delta P - P = (1-\\delta)(S-P)$。\n已知 $P>S$ 且 $\\delta \\in (0,1)$，我们有 $S-P0$ 且 $1-\\delta>0$。因此，$f'(0)  0$。\n不动点 $x=0$ 总是局部稳定的。\n\n- $x=1$ 的稳定性：\n$f'(1) = (1-2)g(1) + 0 = -g(1) = -(W_{\\text{TFT}}(1) - W_{\\text{ALLD}}(1))$。\n在 $x=1$ 处，$W_{\\text{TFT}}(1)=R$ 且 $W_{\\text{ALLD}}(1)=(1-\\delta)T+\\delta P$。\n$f'(1) = -[R - ((1-\\delta)T+\\delta P)] = (1-\\delta)T+\\delta P - R$。\n如果 $f'(1)  0$，不动点 $x=1$ 是局部稳定的，这要求：\n$$(1-\\delta)T+\\delta P - R  0 \\implies R > (1-\\delta)T+\\delta P$$\n这是 TFT 成为对抗 ALLD 的演化稳定策略 (ESS) 的著名条件。如果该不等式反向，$x=1$ 是不稳定的。\n\n- 内部不动点 $x^*$ 的稳定性：\n对于 $x^* \\in (0,1)$，必须有 $W_{\\text{TFT}}(x^*)-W_{\\text{ALLD}}(x^*) = g(x^*) = 0$。\n在 $x^*$ 处的导数是 $f'(x^*) = x^*(1-x^*) g'(x)$。\n斜率 $g'(x)$ 是 $g(x)$ 线性表达式中 $x$ 的系数：\n$$g'(x) = R - T - S + P + \\delta(T+S-2P)$$\n所以，$f'(x^*) = x^*(1-x^*)[R - T - S + P + \\delta(T+S-2P)]$。$f'(x^*)$ 的符号由方括号中项的符号决定，该项是 $x^*$ 表达式的分母。\n内部不动点 $x^* \\in (0,1)$ 存在，当且仅当 $g(0)$ 和 $g(1)$ 的符号相反。\n我们知道 $g(0) = (S-P)(1-\\delta)  0$。因此，要存在一个内部不动点，我们必须有 $g(1) > 0$。\n$g(1) = R - ((1-\\delta)T+\\delta P) > 0$ 这一条件，与 $x=1$ 不稳定的条件是相同的。\n在此条件下：\n- $x^*$ 的分子是 $(P-S)(1-\\delta) > 0$。\n- 分母是 $g'(x) = g(1)-g(0) = [R - ((1-\\delta)T+\\delta P)] - [(S-P)(1-\\delta)]$。由于 $g(1)>0$ 且 $-g(0)>0$，分母 $g'(x)$ 是正的。\n因此 $x^* > 0$。条件 $g(1)>0$ 也等价于 $x^*1$。\n由于分母 $g'(x)$ 是正的，$f'(x^*) = x^*(1-x^*)g'(x) > 0$。\n这意味着如果内部不动点 $x^*$ 存在，它总是不稳定的。这就产生了一个双稳态系统，如果初始频率 $x_0 > x^*$，种群会演化为全 TFT ($x=1$)；如果 $x_0  x^*$，种群会演化为全 ALLD ($x=0$)。\n\n任务 4：报告最终答案\n最终答案是内部不动点频率 $x^*$ 的闭式表达式，该不动点存在的条件是 $R > (1-\\delta)T + \\delta P$。如任务 2 中推导的表达式为：\n$$x^* = \\frac{(P-S)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}$$\n这就是所要求的最终表达式。", "answer": "$$\n\\boxed{\\frac{(P-S)(1-\\delta)}{R - T - S + P + \\delta(T+S-2P)}}\n$$", "id": "2527577"}, {"introduction": "真实世界中的行为充满了偶然的错误，即“噪声”。这个练习引入了执行错误 ($\\epsilon$) 的概念，这是一个至关重要的现实复杂性，它可能会颠覆理想模型中的结论。通过分析“以牙还牙”(TFT) 和更复杂的“赢定输移”(Win-Stay, Lose-Shift, WSLS) 策略在噪声环境下的表现，你将发现简单互惠的脆弱性以及纠错能力对于维持合作的重要性 [@problem_id:2527576]。", "problem": "考虑一个无限期重复的捐赠博弈形式的囚徒困境（PD），这是生态学中研究无亲缘关系个体间合作的经典模型。在每一回合，每个参与者选择合作（$C$）或背叛（$D$）。合作会给对手带来 $b>0$ 的收益，同时自身承担 $c>0$ 的成本。因此，对于行参与者而言，单次囚徒困境的收益如下：相互合作（$CC$）的收益为 $R=b-c$，己方合作而对手背叛（$CD$）的收益为 $S=-c$，己方背叛而对手合作（$DC$）的收益为 $T=b$，相互背叛（$DD$）的收益为 $P=0$。假设 $b>c>0$，从而满足囚徒困境的要求 $T>R>P>S$。\n\n参与者采用两种仅依赖于上一回合实际结果的一步记忆策略之一：\n- 以牙还牙 (TFT)：当且仅当对手在上一回合合作时，本回合才选择合作。\n- 赢定输移 (WSLS)：如果上一回合的收益是“赢”（$R$ 或 $T$），则重复上一回合的行动；如果上一回合的收益是“输”（$P$ 或 $S$），则改变行动。\n\n执行时会独立地出现错误：在每次行动中，参与者的意图行动有 $\\epsilon \\in (0,1)$ 的概率被翻转（$C \\leftrightarrow D$），此过程在参与者之间和回合之间都是独立的。对于任何固定的策略对，四个已实现的联合行动状态 $\\{CC, CD, DC, DD\\}$ 上的动态过程构成一个时齐马尔可夫链。因为当 $0  \\epsilon  1$ 时，该链是有限、不可约且非周期的，所以存在唯一的平稳分布。\n\n定义一个策略对的每回合长期平均收益为在 $\\{CC, CD, DC, DD\\}$ 的平稳分布下单次博弈的期望收益。如果 WSLS 策略自我博弈的长期平均收益超过 TFT 策略自我博弈的长期平均收益，我们称 WSLS “优于” TFT。\n\n仅从上述定义和马尔可夫链的全概率公式出发，符号推导在噪声水平 $\\epsilon$ 下 TFT 对 TFT 以及 WSLS 对 WSLS 博弈的平稳分布，计算它们各自的长期平均收益，并确定唯一的临界错误概率 $\\epsilon^{\\ast}$，在该概率下，WSLS 自我博弈和 TFT 自我博弈的长期平均收益相等。你的最终答案必须是 $\\epsilon^{\\ast}$ 的闭式解。无需四舍五入，且答案无单位。", "solution": "本题要求推导临界错误概率 $\\epsilon^{\\ast}$，在该概率下，以牙还牙（TFT）策略自我博弈的长期平均收益 $V_{TFT}$ 等于赢定输移（WSLS）策略自我博弈的长期平均收益 $V_{WSLS}$。该博弈是一个重复囚徒困境，其收益为 $R=b-c$，$S=-c$，$T=b$，$P=0$，其中 $b>c>0$。系统的状态是两个参与者在上一回合的联合行动，状态集为 $\\{CC, CD, DC, DD\\}$。意图行动以概率 $\\epsilon \\in (0,1)$ 被翻转。\n\n首先，我们分析 TFT 对 TFT 的互动。TFT 参与者当且仅当其对手在上一回合合作时才合作。给定当前状态 $(A_1, A_2)$，下一回合的意图行动 $(I_1, I_2)$ 如下：\n- 从 $CC$ 状态：两个参与者都观察到合作，因此都意图合作。意图状态为 $(C,C)$。\n- 从 $CD$ 状态：参与者1观察到背叛，参与者2观察到合作。意图状态为 $(D,C)$。\n- 从 $DC$ 状态：参与者1观察到合作，参与者2观察到背叛。意图状态为 $(C,D)$。\n- 从 $DD$ 状态：两个参与者都观察到背叛，因此都意图背叛。意图状态为 $(D,D)$。\n\n意图合作（$I_C$）以 $1-\\epsilon$ 的概率成为实际合作（$C$），以 $\\epsilon$ 的概率成为实际背叛（$D$）。意图背叛（$I_D$）以 $1-\\epsilon$ 的概率成为实际背叛（$D$），以 $\\epsilon$ 的概率成为实际合作（$C$）。因此，状态集 $\\{CC, CD, DC, DD\\}$ 上的马尔可夫链的转移矩阵 $M_{TFT}$ 为：\n$$\nM_{TFT} =\n\\begin{pmatrix}\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2 \\\\\n\\epsilon(1-\\epsilon)  \\epsilon^2  (1-\\epsilon)^2  \\epsilon(1-\\epsilon) \\\\\n\\epsilon(1-\\epsilon)  (1-\\epsilon)^2  \\epsilon^2  \\epsilon(1-\\epsilon) \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2\n\\end{pmatrix}\n$$\n$M_{TFT}$ 中每一列的和都为1，这意味着该矩阵是双随机矩阵。对于一个不可约、非周期的有限马尔可夫链，双随机转移矩阵意味着唯一的平稳分布是均匀分布。设 $\\pi_{TFT} = (p_{CC}, p_{CD}, p_{DC}, p_{DD})$ 为该分布。因此，$p_{CC} = p_{CD} = p_{DC} = p_{DD} = \\frac{1}{4}$。\nTFT 参与者的长期平均收益是在此平稳分布下的期望收益：\n$$V_{TFT} = p_{CC}R + p_{CD}S + p_{DC}T + p_{DD}P$$\n$$V_{TFT} = \\frac{1}{4} (b-c) + \\frac{1}{4} (-c) + \\frac{1}{4} (b) + \\frac{1}{4} (0) = \\frac{1}{4}(2b - 2c) = \\frac{b-c}{2}$$\n\n接下来，我们分析 WSLS 对 WSLS 的互动。WSLS 参与者在获得“赢”的收益（$R$ 或 $T$）后重复其上一步的行动，在获得“输”的收益（$P$ 或 $S$）后改变其行动。意图行动如下：\n- 从 $CC$ 状态：两个参与者都获得了收益 $R$（赢）。两者都重复其行动（$C$）。意图状态为 $(C,C)$。\n- 从 $CD$ 状态：参与者1获得了收益 $S$（输），参与者2获得了收益 $T$（赢）。参与者1从 $C$ 切换到 $D$，参与者2重复 $D$。意图状态为 $(D,D)$。\n- 从 $DC$ 状态：参与者1获得了收益 $T$（赢），参与者2获得了收益 $S$（输）。参与者1重复 $D$，参与者2从 $C$ 切换到 $D$。意图状态为 $(D,D)$。\n- 从 $DD$ 状态：两者都获得了收益 $P$（输）。两者都将其行动从 $D$ 切换到 $C$。意图状态为 $(C,C)$。\n\n基于这些意图行动，构建转移矩阵 $M_{WSLS}$：\n$$\nM_{WSLS} =\n\\begin{pmatrix}\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2 \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2 \\\\\n\\epsilon^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  (1-\\epsilon)^2 \\\\\n(1-\\epsilon)^2  \\epsilon(1-\\epsilon)  \\epsilon(1-\\epsilon)  \\epsilon^2\n\\end{pmatrix}\n$$\n设平稳分布为 $\\pi_{WSLS} = (q_{CC}, q_{CD}, q_{DC}, q_{DD})$。由于对称性，$q_{CD}=q_{DC}$。平稳性方程 $\\pi_{WSLS} M_{WSLS} = \\pi_{WSLS}$ 对状态 $CD$ 给出：\n$$q_{CD} = q_{CC}\\epsilon(1-\\epsilon) + q_{CD}\\epsilon(1-\\epsilon) + q_{DC}\\epsilon(1-\\epsilon) + q_{DD}\\epsilon(1-\\epsilon)$$\n$$q_{CD} = (q_{CC} + q_{CD} + q_{DC} + q_{DD})\\epsilon(1-\\epsilon)$$\n由于概率之和为1，我们得到 $q_{CD} = \\epsilon(1-\\epsilon)$。因此，$q_{DC} = \\epsilon(1-\\epsilon)$。\n归一化条件为 $q_{CC} + q_{CD} + q_{DC} + q_{DD} = 1$，这给出 $q_{CC} + q_{DD} = 1 - 2\\epsilon(1-\\epsilon)$。\n现在考虑对称状态集 $S_{sym}=\\{CC, DD\\}$ 和非对称状态集 $S_{asym}=\\{CD, DC\\}$ 之间的流动。设 $X=q_{CC}+q_{DD}$ 和 $Y=q_{CD}+q_{DC}=2\\epsilon(1-\\epsilon)$。\n从 $S_{sym}$ 转移到 $S_{asym}$ 的总概率是 $P(S_{asym}|S_{sym}) = \\epsilon(1-\\epsilon) + \\epsilon(1-\\epsilon) = 2\\epsilon(1-\\epsilon)$。\n从 $S_{asym}$ 转移到 $S_{asym}$ 的总概率是 $P(S_{asym}|S_{asym}) = \\epsilon(1-\\epsilon) + \\epsilon(1-\\epsilon) = 2\\epsilon(1-\\epsilon)$。\n使用关于 $q_{CC}$ 的平稳性方程。\n$$q_{CC} = q_{CC}(1-\\epsilon)^2 + q_{CD}\\epsilon^2 + q_{DC}\\epsilon^2 + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC} = q_{CC}(1-\\epsilon)^2 + 2\\epsilon(1-\\epsilon)\\epsilon^2 + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC}(1 - (1-\\epsilon)^2) = 2\\epsilon^3(1-\\epsilon) + q_{DD}(1-\\epsilon)^2$$\n$$q_{CC}\\epsilon(2-\\epsilon) = 2\\epsilon^3(1-\\epsilon) + (1 - 2\\epsilon(1-\\epsilon) - q_{CC})(1-\\epsilon)^2$$\n一个更简洁的方法是使用状态聚合。设 $X=q_{CC}+q_{DD}$ 和 $Y=q_{CD}+q_{DC}$。在平稳状态下，处于状态 $CC$ 的概率是：\n$$q_{CC} = (q_{CC}+q_{DD})(1-\\epsilon)^2 + (q_{CD}+q_{DC})\\epsilon^2 = X(1-\\epsilon)^2 + Y\\epsilon^2$$\n对于 $DD$：\n$$q_{DD} = (q_{CC}+q_{DD})\\epsilon^2 + (q_{CD}+q_{DC})(1-\\epsilon)^2 = X\\epsilon^2 + Y(1-\\epsilon)^2$$\n将这两者相加得到 $X = q_{CC}+q_{DD} = (X+Y)((1-\\epsilon)^2+\\epsilon^2) = 1-2\\epsilon+2\\epsilon^2$。那么 $Y=1-X=2\\epsilon-2\\epsilon^2$。这证实了 $q_{CD}=q_{DC}=\\epsilon(1-\\epsilon)$。\n将这两个方程相减得到：\n$$q_{CC}-q_{DD} = (X-Y)((1-\\epsilon)^2-\\epsilon^2) = (X-Y)(1-2\\epsilon)$$\n代入 $X=1-2\\epsilon+2\\epsilon^2$ 和 $Y=2\\epsilon-2\\epsilon^2$ 得到 $X-Y = 1-4\\epsilon+4\\epsilon^2 = (1-2\\epsilon)^2$。\n所以，$q_{CC}-q_{DD} = (1-2\\epsilon)^2(1-2\\epsilon) = (1-2\\epsilon)^3$。\n我们得到了关于 $q_{CC}$ 和 $q_{DD}$ 的一个二元线性方程组：\n$q_{CC}+q_{DD} = 1-2\\epsilon+2\\epsilon^2$\n$q_{CC}-q_{DD} = (1-2\\epsilon)^3 = 1-6\\epsilon+12\\epsilon^2-8\\epsilon^3$\n将它们相加：$2q_{CC} = 2-8\\epsilon+14\\epsilon^2-8\\epsilon^3 \\implies q_{CC} = 1-4\\epsilon+7\\epsilon^2-4\\epsilon^3$。\nWSLS 的长期平均收益是：\n$$V_{WSLS} = q_{CC}R + q_{CD}S + q_{DC}T + q_{DD}P$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(-c) + \\epsilon(1-\\epsilon)(b) + q_{DD}(0)$$\n$$V_{WSLS} = q_{CC}(b-c) + \\epsilon(1-\\epsilon)(b-c) = (q_{CC} + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1-4\\epsilon+7\\epsilon^2-4\\epsilon^3 + \\epsilon-\\epsilon^2)(b-c)$$\n$$V_{WSLS} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n\n最后，我们通过令收益相等来找到临界错误概率 $\\epsilon^{\\ast}$：\n$$V_{TFT} = V_{WSLS}$$\n$$\\frac{b-c}{2} = (1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3)(b-c)$$\n由于 $b>c$，所以 $b-c>0$，我们可以用它来除：\n$$\\frac{1}{2} = 1 - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n$$0 = \\frac{1}{2} - 3\\epsilon + 6\\epsilon^2 - 4\\epsilon^3$$\n乘以 $-2$：\n$$0 = 8\\epsilon^3 - 12\\epsilon^2 + 6\\epsilon - 1$$\n这是一个立方展开式：\n$$0 = (2\\epsilon - 1)^3$$\n在区间 $(0,1)$ 内的唯一实数解可以通过令底数为零得到：\n$$2\\epsilon^{\\ast} - 1 = 0 \\implies \\epsilon^{\\ast} = \\frac{1}{2}$$\n这就是使得 TFT 和 WSLS 自我博弈的长期平均收益相等的临界错误概率。", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2527576"}]}