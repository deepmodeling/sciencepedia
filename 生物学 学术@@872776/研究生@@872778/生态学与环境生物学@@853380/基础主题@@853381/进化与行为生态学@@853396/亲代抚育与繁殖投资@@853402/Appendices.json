{"hands_on_practices": [{"introduction": "第一个练习将我们带到亲代投资理论的核心：亲代与其后代在关怀量上固有的冲突。我们将运用内含适应性原则，从亲代和后代各自的角度对其投资的成本和收益进行建模。这项练习将磨练你应用微分学来寻找和比较演化最优解的技能，从而精确地揭示亲代与后代“分歧”的原因和程度 [@problem_id:2517971]。", "problem": "考虑一个二倍体物种中的单次亲代抚育事件，其中亲代将非负的投资量 $I \\ge 0$ 分配给一个目标子代。设子代从投资中获得的适应性收益为 $B(I) = b_0 \\ln(1 + a I)$，其中 $b_0 > 0$ 且 $a > 0$，亲代的适应性成本为 $C(I) = c I$，其中 $c > 0$。假设在一个经典的亲代-子代冲突框架中，亲代最大化其广义适合度 $W_P(I) = r B(I) - C(I)$，而子代偏好的抚育水平对应于在它能够单方面决定 $I$ 的情况下能够最大化 $W_O(I) = B(I) - C(I)$ 的水平，其中 $r$ 是二倍体物种的亲代-子代亲缘关系。取 $r = \\tfrac{1}{2}$。假设 $I$ 被约束为非负，并且如果无约束最优解为负，则有约束最优解为 $I = 0$。\n\n从广义适合度和边际优化（一阶和二阶条件）的定义出发，推导内部最优解及其为正的参数条件。当内部最优解为正时，以闭式解形式，用 $a$、$b_0$ 和 $c$ 表示亲代偏好的投资 $I_P$ 和子代偏好的投资 $I_O$。将你的最终答案表示为有序对 $(I_P, I_O)$ 的精确解析表达式。无需数值舍入，也无需为 $I$ 添加单位。", "solution": "首先对问题陈述进行严格的验证过程。\n\n第 1 步：提取给定信息。\n提供的信息如下：\n- 投资水平：$I \\ge 0$。\n- 子代适应性收益函数：$B(I) = b_0 \\ln(1 + a I)$，参数 $b_0 > 0$ 且 $a > 0$。\n- 亲代适应性成本函数：$C(I) = c I$，参数 $c > 0$。\n- 亲代的广义适合度函数：$W_P(I) = r B(I) - C(I)$。\n- 子代偏好的抚育水平适应性函数：$W_O(I) = B(I) - C(I)$。\n- 二倍体物种的亲代-子代亲缘关系：$r = \\frac{1}{2}$。\n- 约束条件：如果无约束最优解为负，则有约束最优解为 $I = 0$。\n- 目标：推导亲代偏好的投资 $I_P$ 和子代偏好的投资 $I_O$ 作为闭式内部最优解，并以有序对 $(I_P, I_O)$ 的形式呈现答案。\n\n第 2 步：使用提取的给定信息进行验证。\n该问题是行为生态学中的一个标准练习，具体而言，是将汉密尔顿法则和最优化理论应用于亲代-子代冲突模型。\n- **科学依据：** 该模型基于演化生物学的既定原则，即广义适合度理论。使用对数收益函数表示投资的收益递减，这是一个常见且现实的假设。线性成本函数是一个标准的简化。二倍体亲代-子代亲缘关系值 $r = \\frac{1}{2}$ 是正确的。该问题在科学上是合理的。\n- **良态问题：** 问题提供了两个定义良好、连续且二阶可导的目标函数 $W_P(I)$ 和 $W_O(I)$，需要在定义域 $I \\ge 0$ 上进行最大化。该结构保证了每种情况下都存在唯一的最大值。\n- **目标：** 问题以精确的数学语言陈述，没有歧义或主观论断。\n\n第 3 步：结论与行动。\n该问题被视为有效。它是自洽的、一致的，并且基于既定的科学和数学原理。将推导其解。\n\n所用方法是使用微分学的优化方法。我们寻求找到使亲代的广义适合度 $W_P(I)$ 和子代的适应性 $W_O(I)$ 分别最大化的 $I$ 值。这些值分别表示为 $I_P$ 和 $I_O$。\n\n首先，我们确定亲代偏好的投资 $I_P$。\n亲代的广义适合度函数为：\n$$W_P(I) = r B(I) - C(I) = \\frac{1}{2} b_0 \\ln(1 + a I) - c I$$\n为了找到最大值，我们计算其关于 $I$ 的一阶导数并令其为零。这是内部最优解的一阶条件。\n$$\\frac{dW_P}{dI} = \\frac{d}{dI} \\left( \\frac{1}{2} b_0 \\ln(1 + a I) - c I \\right) = \\frac{1}{2} b_0 \\frac{a}{1 + a I} - c$$\n将导数设为零可得：\n$$\\frac{a b_0}{2(1 + a I)} - c = 0$$\n$$\\frac{a b_0}{2c} = 1 + a I$$\n$$a I = \\frac{a b_0}{2c} - 1$$\n$$I_P = \\frac{1}{a} \\left( \\frac{a b_0}{2c} - 1 \\right) = \\frac{b_0}{2c} - \\frac{1}{a}$$\n这是内部最优解的候选值。为确认其为最大值，我们检查二阶条件。二阶导数必须为负。\n$$\\frac{d^2W_P}{dI^2} = \\frac{d}{dI} \\left( \\frac{a b_0}{2} (1 + a I)^{-1} - c \\right) = -\\frac{a b_0}{2} (1 + a I)^{-2} (a) = -\\frac{a^2 b_0}{2(1 + a I)^2}$$\n鉴于 $a > 0$ 且 $b_0 > 0$，分子 $a^2 b_0$ 为正。对于 $I \\ge 0$，分母 $2(1+aI)^2$ 也为正。因此，$\\frac{d^2W_P}{dI^2} < 0$，这证实了该临界点是一个局部最大值。\n问题要求一个内部最优解，这意味着 $I_P > 0$。其条件是：\n$$\\frac{b_0}{2c} - \\frac{1}{a} > 0 \\implies \\frac{b_0}{2c} > \\frac{1}{a} \\implies a b_0 > 2c$$\n如果 $a b_0 \\le 2c$，则在 $I=0$ 处的边际收益为非正，最优投资是角点解 $I_P=0$。问题要求的是内部最优解，即在它为正的条件下上面推导出的表达式。\n\n接下来，我们确定子代偏好的投资 $I_O$。\n子代的适应性函数为：\n$$W_O(I) = B(I) - C(I) = b_0 \\ln(1 + a I) - c I$$\n注意，成本 $C(I)$ 是对亲代的成本，但它通过影响亲代未来的繁殖成功来影响子代的广义适合度，这涉及到目标子代有亲缘关系的兄弟姐妹。函数 $W_O(I)$ 代表子代的演化利益，它并不按照与亲代的亲缘关系度来折算对亲代的成本，也不是按照与自身的亲缘关系度（1）来折算。问题陈述中定义了 $W_O(I) = B(I) - C(I)$，这意味着子代认为对亲代适应性的成本与对其自身适应性的收益同等重要。这代表了冲突的极端情况。\n我们求其关于 $I$ 的一阶导数：\n$$\\frac{dW_O}{dI} = \\frac{d}{dI} \\left( b_0 \\ln(1 + a I) - c I \\right) = b_0 \\frac{a}{1 + a I} - c$$\n为满足一阶条件，将导数设为零：\n$$\\frac{a b_0}{1 + a I} - c = 0$$\n$$\\frac{a b_0}{c} = 1 + a I$$\n$$a I = \\frac{a b_0}{c} - 1$$\n$$I_O = \\frac{1}{a} \\left( \\frac{a b_0}{c} - 1 \\right) = \\frac{b_0}{c} - \\frac{1}{a}$$\n我们检查二阶条件：\n$$\\frac{d^2W_O}{dI^2} = \\frac{d}{dI} \\left( a b_0 (1 + a I)^{-1} - c \\right) = -a b_0 (1 + a I)^{-2} (a) = -\\frac{a^2 b_0}{(1 + a I)^2}$$\n由于 $a > 0$ 且 $b_0 > 0$，我们有 $\\frac{d^2W_O}{dI^2} < 0$，确认是一个局部最大值。\n内部最优解 $I_O > 0$ 的条件是：\n$$\\frac{b_0}{c} - \\frac{1}{a} > 0 \\implies \\frac{b_0}{c} > \\frac{1}{a} \\implies a b_0 > c$$\n问题要求正内部最优解的闭式表达式。因此，当各自的条件（$a b_0 > 2c$ 和 $a b_0 > c$）满足时，$I_P$ 和 $I_O$ 的表达式是有效的。$I_P > 0$ 的条件比 $I_O > 0$ 的条件更严格。当亲代存在内部最优解时，子代也必然存在内部最优解。\n\n最终答案是有序对 $(I_P, I_O)$。\n$$I_P = \\frac{b_0}{2c} - \\frac{1}{a}$$\n$$I_O = \\frac{b_0}{c} - \\frac{1}{a}$$", "answer": "$$\n\\boxed{\n\\left( \\frac{b_0}{2c} - \\frac{1}{a}, \\frac{b_0}{c} - \\frac{1}{a} \\right)\n}\n$$", "id": "2517971"}, {"introduction": "现实世界中的育儿行为通常涉及在多个后代之间分配有限的资源。这项练习将我们的分析扩展到一个巢穴，其中由于混合亲缘关系，亲代与每个雏鸟的遗传相关度可能不同。你将使用拉格朗日乘数法来确定固定关怀预算的最优分配方案，这有力地证明了演化逻辑如何决定一个家庭内部的投资模式 [@problem_id:2517949]。", "problem": "一只具有领地意识的雄鸟为其三只雏鸟提供亲代抚育，抚育时间可以在三者之间分配。设 $I_i \\ge 0$ 表示分配给雏鸟 $i \\in \\{1,2,3\\}$ 的抚育时间，总抚育预算为 $I$，因此 $\\sum_{i=1}^{3} I_i = I$。雄鸟从投资于雏鸟 $i$ 的抚育中获得的广义适合度回报是遗传相关系数 $r_i$ 与收益递减的效益函数 $B_i(I_i)$ 的乘积。假设基本量遵循以下定义：\n- 广义适合度是遗传相关度乘以受体适合度效应后对所有受体的总和。\n- 父鸟与雏鸟的遗传相关度等于父鸟对该雏鸟的亲权概率乘以二分之一。\n- 由于收益递减，亲代抚育的回报相对于投入是凹函数。\n\n假设混合亲权产生的亲权概率为 $p_1 = 1$、$p_2 = 0.6$ 和 $p_3 = 0.2$。因此，预期的相关度为 $r_i = \\tfrac{1}{2} p_i$，即 $r_1 = 0.5$、$r_2 = 0.3$ 和 $r_3 = 0.1$。设效益函数为 $B_i(I_i) = s_i \\sqrt{I_i}$，效率乘数为 $s_1 = 1.0$、$s_2 = 0.8$ 和 $s_3 = 1.2$。总可用抚育时间为 $I = 120$ 小时。\n\n从上述核心定义出发，使用拉格朗日乘数法，从第一性原理推导最优分配 $I_i^{\\ast}$，以在约束条件 $\\sum_{i=1}^{3} I_i = I$ 和 $I_i \\ge 0$ 下最大化雄鸟的广义适合度 $\\sum_{i=1}^{3} r_i B_i(I_i)$。然后计算给定参数下的 $(I_1^{\\ast}, I_2^{\\ast}, I_3^{\\ast})$ 的数值。最后，分析混合亲权相对于所有雏鸟均为完全亲权（即对所有 $i$ 都有 $p_i = 1$）的情况，如何影响 $I_i^{\\ast}$。\n\n以小时为单位，报告最终的数值分配向量 $(I_1^{\\ast}, I_2^{\\ast}, I_3^{\\ast})$，并四舍五入到四位有效数字。", "solution": "所提出的问题是一个定义明确的约束优化问题，其基础是行为生态学的标准原理。该问题在科学上是合理的，并且内部一致。因此，我们将着手解决它。\n\n目标是最大化雄鸟的总广义适合度 $F$，即从三只雏鸟中获得的适合度回报之和。雏鸟 $i$ 的适合度回报是遗传相关度 $r_i$ 和效益函数 $B_i(I_i)$ 的乘积。总广义适合度为：\n$$ F(I_1, I_2, I_3) = \\sum_{i=1}^{3} r_i B_i(I_i) $$\n抚育时间的投入 $I_i$ 必须为非负值，$I_i \\ge 0$，并受到总预算约束：\n$$ \\sum_{i=1}^{3} I_i = I $$\n给定函数的具体形式和参数：\n- 亲权概率：$p_1 = 1$, $p_2 = 0.6$, $p_3 = 0.2$。\n- 相关系数：$r_i = \\frac{1}{2} p_i$，所以 $r_1 = 0.5$, $r_2 = 0.3$, $r_3 = 0.1$。\n- 效益函数：$B_i(I_i) = s_i \\sqrt{I_i}$。\n- 效率乘数：$s_1 = 1.0$, $s_2 = 0.8$, $s_3 = 1.2$。\n- 总预算：$I = 120$。\n\n目标函数变为：\n$$ F(I_1, I_2, I_3) = r_1 s_1 \\sqrt{I_1} + r_2 s_2 \\sqrt{I_2} + r_3 s_3 \\sqrt{I_3} $$\n这是一个在等式约束和非负约束下最大化函数的问题。我们使用拉格朗日乘数法。拉格朗日函数 $\\mathcal{L}$ 构造如下：\n$$ \\mathcal{L}(I_1, I_2, I_3, \\lambda) = \\left( \\sum_{i=1}^{3} r_i s_i \\sqrt{I_i} \\right) - \\lambda \\left( \\left( \\sum_{i=1}^{3} I_i \\right) - I \\right) $$\n还必须考虑非负约束 $I_i \\ge 0$。如果可能出现边界解（即某个 $i$ 的 $I_i=0$），则需要完整的 Karush-Kuhn-Tucker (KKT) 条件。然而，对雏鸟 $i$ 的边际效益为 $\\frac{\\partial (r_i B_i)}{\\partial I_i} = \\frac{r_i s_i}{2\\sqrt{I_i}}$。当 $I_i \\to 0^+$ 时，对于任何 $r_i s_i > 0$ 的雏鸟，该边际效益都趋于无穷大。在这个问题中，所有的乘积 $r_i s_i$ 都是正的：\n- $r_1 s_1 = (0.5)(1.0) = 0.5$\n- $r_2 s_2 = (0.3)(0.8) = 0.24$\n- $r_3 s_3 = (0.1)(1.2) = 0.12$\n由于零投入时的边际回报是无穷大，任何最优分配都必须满足对所有 $i$ 都有 $I_i^{\\ast} > 0$。因此，非负约束不是紧的，我们可以通过将拉格朗日函数对每个 $I_i$ 的偏导数设为零来找到最优解。\n\n最大值的一阶条件是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial I_i} = \\frac{r_i s_i}{2\\sqrt{I_i}} - \\lambda = 0 \\quad \\text{for } i \\in \\{1, 2, 3\\} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = I - \\sum_{i=1}^{3} I_i = 0 $$\n从第一个条件可知，在最优点，投资于每只雏鸟的边际广义适合度回报必须等于一个常数 $\\lambda$：\n$$ \\frac{r_i s_i}{2\\sqrt{I_i}} = \\lambda $$\n求解 $I_i$ 可得其关于拉格朗日乘数 $\\lambda$ 的表达式：\n$$ \\sqrt{I_i} = \\frac{r_i s_i}{2\\lambda} \\implies I_i = \\left(\\frac{r_i s_i}{2\\lambda}\\right)^2 = \\frac{(r_i s_i)^2}{4\\lambda^2} $$\n现在，我们将这个 $I_i$ 的表达式代入预算约束中以求得 $\\lambda$：\n$$ \\sum_{i=1}^{3} I_i = \\sum_{i=1}^{3} \\frac{(r_i s_i)^2}{4\\lambda^2} = I $$\n$$ \\frac{1}{4\\lambda^2} \\sum_{i=1}^{3} (r_i s_i)^2 = I $$\n$$ 4\\lambda^2 = \\frac{1}{I} \\sum_{j=1}^{3} (r_j s_j)^2 $$\n将此结果代回 $I_i$ 的表达式中，我们推导出每只雏鸟的最优分配 $I_i^{\\ast}$：\n$$ I_i^{\\ast} = \\frac{(r_i s_i)^2}{ \\frac{1}{I} \\sum_{j=1}^{3} (r_j s_j)^2 } = I \\cdot \\frac{(r_i s_i)^2}{\\sum_{j=1}^{3} (r_j s_j)^2} $$\n这是一般解。它表明对雏鸟 $i$ 的最优投资与总预算 $I$ 成正比，并与它的有效质量 $(r_i s_i)^2$ 相对于所有雏鸟的平方和的比值成正比。\n\n我们现在计算特定混合亲权情况下的数值。首先，计算有效质量的平方：\n- $(r_1 s_1)^2 = (0.5)^2 = 0.25$\n- $(r_2 s_2)^2 = (0.24)^2 = 0.0576$\n- $(r_3 s_3)^2 = (0.12)^2 = 0.0144$\n总和为：\n$$ \\sum_{j=1}^{3} (r_j s_j)^2 = 0.25 + 0.0576 + 0.0144 = 0.322 $$\n总预算为 $I=120$ 小时，最优分配为：\n$$ I_1^{\\ast} = 120 \\cdot \\frac{0.25}{0.322} \\approx 93.1677 \\text{ 小时} $$\n$$ I_2^{\\ast} = 120 \\cdot \\frac{0.0576}{0.322} \\approx 21.4658 \\text{ 小时} $$\n$$ I_3^{\\ast} = 120 \\cdot \\frac{0.0144}{0.322} \\approx 5.3665 \\text{ 小时} $$\n最后，我们通过将此分配与完全亲权的假设情况（即对所有 $i$，$p_i = 1$ 从而 $r_i = 0.5$）进行比较，来分析混合亲权的影响。\n在完全亲权情景下，有效质量为：\n- $r_1 s_1 = (0.5)(1.0) = 0.5$\n- $r_2 s_2 = (0.5)(0.8) = 0.4$\n- $r_3 s_3 = (0.5)(1.2) = 0.6$\n有效质量的平方为：\n- $(r_1 s_1)^2 = 0.25$\n- $(r_2 s_2)^2 = 0.16$\n- $(r_3 s_3)^2 = 0.36$\n总和为 $\\sum_{j=1}^{3} (r_j s_j)^2 = 0.25 + 0.16 + 0.36 = 0.77$。\n在完全亲权下的最优分配将是：\n$$ I_{1, \\text{full}}^{\\ast} = 120 \\cdot \\frac{0.25}{0.77} \\approx 38.96 \\text{ 小时} $$\n$$ I_{2, \\text{full}}^{\\ast} = 120 \\cdot \\frac{0.16}{0.77} \\approx 24.94 \\text{ 小时} $$\n$$ I_{3, \\text{full}}^{\\ast} = 120 \\cdot \\frac{0.36}{0.77} \\approx 56.10 \\text{ 小时} $$\n比较分配结果：\n- 混合亲权：$(I_1^{\\ast}, I_2^{\\ast}, I_3^{\\ast}) \\approx (93.17, 21.47, 5.367)$\n- 完全亲权：$(I_{1, \\text{full}}^{\\ast}, I_{2, \\text{full}}^{\\ast}, I_{3, \\text{full}}^{\\ast}) \\approx (38.96, 24.94, 56.10)$\n分析结果很明确。雏鸟2和3的亲权不确定性导致了亲代抚育的急剧重新分配。对确定亲权的雏鸟的投资（$I_1^{\\ast}$）增加了一倍以上，从大约39小时增加到93小时。相反，对不确定亲权的雏鸟的投资则急剧下降。对雏鸟3（亲权最低）的投资减少了十倍，从56小时降至略多于5小时。对雏鸟2（中等亲权）的投资也减少了。雄鸟将其有限的资源集中在能提供最高广义适合度回报的后代上，而这取决于遗传相关度（$r_i$）和转换效率（$s_i$）的综合效应。降低的亲权在雄鸟的适合度计算中急剧贬低了雏鸟的价值，导致投资理性地转向其自身的遗传后代。\n\n按要求将混合亲权情况下的结果四舍五入到四位有效数字：\n$$ I_1^{\\ast} = 93.17 $$\n$$ I_2^{\\ast} = 21.47 $$\n$$ I_3^{\\ast} = 5.366 $$\n总和为 $93.17 + 21.47 + 5.366 = 120.006$，在四舍五入精度范围内，这与总预算 $I=120$ 一致。", "answer": "$$\n\\boxed{(93.17, 21.47, 5.366)}\n$$", "id": "2517949"}, {"introduction": "亲代抚育决策并非孤立做出，而是在整个繁殖季节中一系列决策的一部分。这项高级练习引入了时间维度，将喂养行为建模为一个动态过程，其中亲代必须在当前后代的需求与其自身生存及未来繁殖机会之间进行权衡。通过实施动态规划算法，你将解决一个依赖于状态的优化问题——这是现代生活史理论的基石——从而规划出随时间推移的最优策略 [@problem_id:2517947]。", "problem": "考虑一个基于能量的亲代抚育问题，此问题发生在一个有限的繁殖季节内，时间为离散周期，以 $t \\in \\{0,1,\\dots,T-1\\}$ 索引。亲代的能量储存状态为 $S_t$（单位：千焦），每个周期通过觅食机会获得外源能量输入 $I_t$（单位：千焦），并分配 $C_t$（单位：千焦）用于哺育后代。储存状态根据物质平衡演化\n$$\nS_{t+1} = S_t + I_t - C_t,\n$$\n附带可行性约束 $S_t \\ge 0$ 和 $C_t \\ge 0$ 对所有 $t$ 成立。初始储存量 $S_0$ 为给定值。目标是最大化每期哺育所带来的繁殖收益的折现总和，再加上季末储备的期末生存/繁殖价值。具体来说，假设每期繁殖收益为\n$$\nf(C_t) = \\alpha \\,\\log(1 + C_t),\n$$\n其中 $\\log(\\cdot)$ 表示自然对数，$\\alpha > 0$ 代表边际效益的规模，以及期末价值\n$$\ng(S_T) = \\gamma \\,\\sqrt{S_T},\n$$\n其中 $\\gamma > 0$。亲代选择一个可行的序列 $\\{C_t\\}_{t=0}^{T-1}$ 来最大化\n$$\n\\max_{\\{C_t\\}} \\quad \\sum_{t=0}^{T-1} \\beta^t f(C_t) + \\beta^T g(S_T),\n$$\n受上述储存动态和非负性约束的限制，其中 $0 < \\beta \\le 1$ 是跨期折现因子。其生态学解释是，$f(C_t)$ 捕捉了对当前哺育后代的边际报酬递减现象，而 $g(S_T)$ 则捕捉了季末储备对未来生存或剩余繁殖价值的贡献，这两者在生活史理论中都是标准的假设。\n\n您的任务是从第一性原理推导出动态规划递推式，并实现一个数值算法来计算近似最优的哺育策略 $\\pi_t(S)$。该策略将时间 $t$ 的任何储存量 $S$ 映射到一个行动 $C_t$，并使用离散化的状态-行动空间和持续价值的线性插值法。假设以下模型和数值条件成立：\n- 最优性原理适用，因此存在贝尔曼方程，且价值函数在有限终期内有良好定义。\n- 函数 $f(\\cdot)$ 和 $g(\\cdot)$ 在其定义域上是凹函数且递增，在时间 $t$ 给定状态 $S_t$ 时的可行行动集为 $A_t(S_t) = \\{C_t \\in \\mathbb{R}_{\\ge 0} \\,:\\, C_t \\le S_t + I_t\\}$。\n- 您必须在储存状态的均匀网格上实现逆向归纳法。设状态网格为 $\\mathcal{S} = \\{0, \\Delta, 2\\Delta, \\dots, S_{\\max}\\}$，其中 $S_{\\max} = S_0 + \\sum_{t=0}^{T-1} I_t$，$\\Delta > 0$ 是指定的网格步长。对于每个状态 $S \\in \\mathcal{S}$ 和时间 $t$，在均匀行动网格 $\\{0, \\Delta, 2\\Delta, \\dots, \\lfloor (S+I_t)/\\Delta \\rfloor \\Delta\\}$ 上枚举可行行动，并对非网格上的 $S_{t+1}$ 值使用关于 $S$ 的线性插值来评估贝尔曼算子。\n- 若有多个行动达到最大值（在数值容许误差内），则确定性地选择其中最小的 $C_t$ 来解决平局。\n\n通过逆向归纳法计算出策略后，使用该策略和实现的 $I_t$ 路径，从给定的初始条件 $S_0$ 开始模拟最优哺育序列。在每个周期中，选择与当前 $S_t$ 最接近的状态网格节点相关的策略行动，然后根据储存平衡精确地更新 $S_{t+1}$。\n\n所有哺育值 $C_t$ 以千焦 (kJ) 为单位回答，并将每个值四舍五入至小数点后三位。不涉及角度。请勿在打印输出中包含单位字符串。\n\n测试用例：\n为以下五个案例提供结果，每个案例由一个元组 $(T,\\beta,\\alpha,\\gamma,S_0,\\{I_t\\}_{t=0}^{T-1},\\Delta)$ 指定。\n\n- 案例 A (理想情景)：$(3,\\, 0.95,\\, 2.0,\\, 1.0,\\, 5.0,\\, \\{2.0,\\, 1.0,\\, 0.5\\},\\, 0.5)$。\n- 案例 B (高期末价值，鼓励储蓄)：$(3,\\, 0.97,\\, 1.5,\\, 4.0,\\, 3.0,\\, \\{1.5,\\, 0.5,\\, 0.5\\},\\, 0.5)$。\n- 案例 C (无收入，仅消耗储备)：$(3,\\, 0.90,\\, 2.5,\\, 0.5,\\, 3.0,\\, \\{0.0,\\, 0.0,\\, 0.0\\},\\, 0.5)$。\n- 案例 D (偏重当前，早期有大量输入)：$(3,\\, 0.50,\\, 2.0,\\, 1.0,\\, 4.0,\\, \\{3.0,\\, 0.0,\\, 0.0\\},\\, 0.5)$。\n- 案例 E (单周期终期边界)：$(1,\\, 0.95,\\, 2.0,\\, 2.0,\\, 2.0,\\, \\{1.0\\},\\, 0.5)$。\n\n最终输出格式要求：\n您的程序应产生单行输出，其中包含一个逗号分隔的列表，用方括号括起来，列表中的每个元素是对应测试案例计算出的最优哺育序列 $[C_0,\\dots,C_{T-1}]$，每个数字四舍五入到小数点后三位，例如：$[[c_{0}^{(A)},c_{1}^{(A)},c_{2}^{(A)}],[c_{0}^{(B)},\\dots],\\dots]$。", "solution": "所提出的问题是一个有限终期的离散时间最优控制问题，完全属于动态规划的范畴。其目标是确定一个最优哺育策略 $\\{C_t\\}_{t=0}^{T-1}$，以在代表能量储存动态的线性状态空间模型的约束下，最大化回报的折现总和。此问题定义良好，并在科学上植根于生活史理论的原理。解决方案是通过应用最优性原理推导出来的，这使我们能够构建贝尔曼方程，并使用逆向归纳法进行数值求解。\n\n设 $V_t(S)$ 为从时间 $t$ 到终期 $T$ 的目标函数的最优值，该值是在系统处于状态 $S_t = S$ 的条件下得到的。价值函数定义在未来收益的折现总和上，并归一化到周期 $t$ 的开始。此问题的贝尔曼方程为：\n$$\nV_t(S) = \\max_{C_t \\in A_t(S)} \\left\\{ f(C_t) + \\beta V_{t+1}(S_{t+1}) \\right\\}\n$$\n对于时间周期 $t \\in \\{0, 1, \\dots, T-1\\}$，其中 $S_{t+1} = S + I_t - C_t$。可行行动集为 $A_t(S) = \\{C_t \\in \\mathbb{R}_{\\ge 0} \\,|\\, C_t \\le S + I_t\\}$。时间 $T$ 的终端条件由终端价值函数给出：\n$$\nV_T(S) = g(S) = \\gamma \\sqrt{S}\n$$\n每期回报函数为 $f(C_t) = \\alpha \\log(1 + C_t)$。$f(\\cdot)$ 和 $g(\\cdot)$ 的凹性保证了在每个时间步 $t$，价值函数 $V_t(S)$ 对 $S$ 也是凹函数，这确保了贝尔曼方程内的最大化问题是良好行为的。\n\n此问题的解析解通常难以求得。因此，我们将实现指定的基于离散状态空间上的逆向归纳法的数值算法。\n\n**1. 离散化与初始化：**\n首先，我们对连续的状态空间进行离散化。状态变量 $S_t$ 的范围可以从 $0$到最大可能值 $S_{\\max} = S_0 + \\sum_{t=0}^{T-1} I_t$。我们构建一个均匀状态网格 $\\mathcal{S} = \\{0, \\Delta, 2\\Delta, \\dots, N_S\\Delta\\}$，其中 $N_S\\Delta$ 是大于或等于 $S_{\\max}$ 的最小网格点。我们将价值函数 $V_t(S)$ 存储为一个数组，其值对应于每个网格点 $S \\in \\mathcal{S}$。我们还初始化一个大小为 $T \\times |\\mathcal{S}|$ 的策略矩阵 $\\Pi$，用于存储每个状态-时间对 $(S, t)$ 的最优行动 $\\pi_t(S)$。\n\n**2. 逆向归纳法：**\n算法的核心是时间上的逆向循环，从 $t=T-1$ 向下到 $t=0$。\n\n- **终端步骤 ($t=T$)：** 我们首先为所有状态 $S \\in \\mathcal{S}$ 计算终端价值函数 $V_T(S)$。这由 $V_T(S_i) = g(S_i) = \\gamma\\sqrt{S_i}$ 直接给出，其中 $S_i$ 是我们状态网格上的点。这个值数组代表了最终决策周期的持续价值。\n\n- **递推步骤 ($t = T-1, \\dots, 0$)：** 对于每个时间步 $t$，我们为网格上的每个状态 $S$ 计算价值函数 $V_t(S)$ 和最优策略 $\\pi_t(S)$。\n    - 对于给定的状态 $S_i \\in \\mathcal{S}$，我们枚举所有可行的行动。问题指定了一个行动网格 $\\{0, \\Delta, 2\\Delta, \\dots, \\lfloor (S_i+I_t)/\\Delta \\rfloor \\Delta\\}$。\n    - 对于此网格上的每个候选行动 $C_k$，我们计算贝尔曼方程的右侧，我们将其表示为 Q-值：$Q_t(S_i, C_k) = f(C_k) + \\beta V_{t+1}(S_i + I_t - C_k)$。\n    - 下一个状态 $S_{t+1} = S_i + I_t - C_k$ 通常不会落在网格点上。因此，我们必须使用对先前为时间 $t+1$ 计算的价值函数数组进行线性插值来评估 $V_{t+1}(S_{t+1})$。给定 $S_{t+1}$，我们找到两个相邻的网格点 $S_{low}$ 和 $S_{high}$，并将 $V_{t+1}(S_{t+1})$ 计算为 $V_{t+1}(S_{low})$ 和 $V_{t+1}(S_{high})$ 的加权平均值。\n    - 在评估完所有可行行动 $C_k$ 的 Q-值后，我们找到最大值，$V_t(S_i) = \\max_{k} Q_t(S_i, C_k)$。\n    - 最优行动 $\\pi_t(S_i)$ 是产生此最大值的行动 $C_k$。为了处理平局，我们遵循问题规范，选择达到最大 Q-值的最小行动 $C_k$。\n    - 我们存储计算出的 $V_t(S_i)$ 和 $\\pi_t(S_i)$，然后处理网格上的下一个状态。在处理完时间 $t$ 的所有状态后，我们移至时间 $t-1$，使用新计算的 $V_t$ 数组作为持续价值函数。\n\n**3. 正向模拟：**\n一旦逆向归纳完成，我们就得到了一个完整的策略图 $\\Pi = \\{\\pi_t(S)\\}_{t=0, S\\in\\mathcal{S}}^{T-1}$。然后我们从给定的初始状态 $S_0$ 开始，对系统的轨迹进行时间上的正向模拟。\n\n- 对于每个时间周期 $t = 0, 1, \\dots, T-1$：\n    - 当前状态 $S_t$ 可能不在网格上。我们找到与 $S_t$ 最接近的网格点 $S_{grid} \\in \\mathcal{S}$。\n    - 我们从计算出的策略中检索最优行动：$C_t = \\pi_t(S_{grid})$。\n    - 这个 $C_t$ 被记录为最优哺育序列的一部分。\n    - 使用精确的动态方程更新下一周期的状态：$S_{t+1} = S_t + I_t - C_t$。此过程重复直到 $t=T-1$。\n\n此过程为给定的初始条件和外源输入产生近似最优的哺育行动序列 $\\{C_0, C_1, \\dots, C_{T-1}\\}$。对提供的五个测试用例中的每一个都重复整个过程。", "answer": "```python\nimport numpy as np\n\ndef solve_case(T, beta, alpha, gamma, S0, I_t, Delta):\n    \"\"\"\n    Solves a single instance of the parental provisioning dynamic programming problem.\n    \"\"\"\n\n    # 1. Initialization\n    S_max = S0 + sum(I_t)\n    # Ensure the state grid covers S_max and its points are multiples of Delta\n    # using a small tolerance for floating point comparisons.\n    S_max_grid = np.ceil(S_max / Delta) * Delta\n    state_grid = np.arange(0, S_max_grid + 1e-9, Delta)\n    num_states = len(state_grid)\n\n    policy = np.zeros((T, num_states))\n\n    # Define utility functions\n    f = lambda C: alpha * np.log(1 + C)\n    # Ensure S is non-negative for sqrt\n    g = lambda S: gamma * np.sqrt(np.maximum(0, S))\n\n    # 2. Backward Induction\n    # At time T, the value is just the terminal value function g(S_T)\n    V_next = g(state_grid)\n\n    # Loop backwards from t = T-1 down to 0\n    for t in range(T - 1, -1, -1):\n        V_current = np.zeros(num_states)\n        \n        # Iterate over each possible state S in the state grid\n        for i, S in enumerate(state_grid):\n            resources = S + I_t[t]\n            if resources  0: resources = 0\n\n            # Define the discrete action grid for the current state S\n            max_C_val = np.floor(resources / Delta) * Delta\n            if max_C_val  0:\n                action_grid = np.array([0.0])\n            else:\n                num_actions = int(round(max_C_val / Delta)) + 1\n                action_grid = np.linspace(0, max_C_val, num_actions)\n\n            # Vectorized calculation of Q-values\n            S_next = S + I_t[t] - action_grid\n            # Enforce non-negativity of next state\n            S_next = np.maximum(0, S_next)\n\n            # Interpolate the value function at the next states\n            V_next_interp = np.interp(S_next, state_grid, V_next)\n\n            # Calculate Q-values for all actions\n            q_values = f(action_grid) + beta * V_next_interp\n            \n            # Find the best action according to the tie-breaking rule\n            # (smallest C among those achieving the maximal value)\n            if len(q_values) > 0:\n                max_q = np.max(q_values)\n                # Find the first index corresponding to the max value\n                best_action_idx = np.where(np.isclose(q_values, max_q))[0][0]\n                best_action = action_grid[best_action_idx]\n                \n                policy[t, i] = best_action\n                V_current[i] = max_q\n            else:\n                # No feasible action other than C=0 (if resources=0)\n                policy[t, i] = 0.0\n                # Value is determined by C=0 and subsequent optimal path\n                S_next_zero_C = S + I_t[t]\n                V_next_interp_zero_C = np.interp(S_next_zero_C, state_grid, V_next)\n                V_current[i] = f(0.0) + beta * V_next_interp_zero_C\n\n        # Move to the next step of the recursion\n        V_next = V_current\n\n    # 3. Forward Simulation\n    optimal_C = []\n    S_current = S0\n    for t in range(T):\n        # Find the nearest state grid index for the current state\n        s_idx = int(np.round(S_current / Delta))\n        # Clip the index to be within the valid range of the policy table\n        s_idx = np.clip(s_idx, 0, num_states - 1)\n        \n        # Get the optimal action from the computed policy\n        C_t = policy[t, s_idx]\n        optimal_C.append(C_t)\n        \n        # Update the state using the exact dynamics\n        S_current = S_current + I_t[t] - C_t\n        # Enforce non-negativity of state\n        S_current = np.maximum(0, S_current)\n\n    return optimal_C\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # (T, beta, alpha, gamma, S0, {I_t}, Delta)\n        (3, 0.95, 2.0, 1.0, 5.0, [2.0, 1.0, 0.5], 0.5), # Case A\n        (3, 0.97, 1.5, 4.0, 3.0, [1.5, 0.5, 0.5], 0.5), # Case B\n        (3, 0.90, 2.5, 0.5, 3.0, [0.0, 0.0, 0.0], 0.5), # Case C\n        (3, 0.50, 2.0, 1.0, 4.0, [3.0, 0.0, 0.0], 0.5), # Case D\n        (1, 0.95, 2.0, 2.0, 2.0, [1.0], 0.5),             # Case E\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_case(*case)\n        all_results.append(result)\n\n    # Format the output string as per requirements\n    formatted_cases = []\n    for case_result in all_results:\n        # Format each number to exactly three decimal places\n        formatted_nums = [f\"{num:.3f}\" for num in case_result]\n        # Join numbers into a string representation of a list\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    # Join all case strings into the final output format\n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2517947"}]}