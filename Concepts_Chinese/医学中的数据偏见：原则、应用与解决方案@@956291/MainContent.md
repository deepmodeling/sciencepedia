## 引言
大数据时代为医学带来了新的愿景，海量数据集有望揭开疾病的秘密，实现高度个性化的医疗服务。然而，这种数据驱动的范式建立在一个脆弱的假设之上：即数据本身是人类健康的准确、公正的反映。现实远比这复杂。医学数据往往是不完整、不具代表性的，并且受到其本应帮助克服的社会和系统性不平等现象的塑造。数据的前景与其充满偏见的现实之间的差距，对建立一个公正有效的医疗保健系统构成了重大威胁。本文直面这一挑战。第一章“原则与机制”将解构数据偏见的[基本类](@entry_id:158335)型，从有缺陷的样本选择到危险的代理变量使用。随后的“应用与跨学科联系”一章将探讨这些理论上的偏见如何在现实世界的临床算法、药物开发和科学记录中体现，从而证明在医学领域采用更具批判性和严谨性的数据处理方法是何等迫切。

## 原则与机制

想象一下，有一座巨大的图书馆，为地球上的每个人都藏有一本书。你被告知，每本书都包含了那个人健康的完整故事。对医学科学家来说，这座图书馆就是天堂。我们相信，有了足够的数据，我们就能揭开疾病的奥秘，在疾病来袭前进行预测，并以惊人的精度量身定制治疗方案。这就是数据驱动医学的梦想。

但如果这座图书馆存在一种微妙、几乎看不见的缺陷呢？如果一些人的书是用隐形墨水写的，整章整章地缺失呢？如果有些书根本不是关于本人，而是关于他们的影子呢？又如果图书馆只收集住在主干道附近的人的书呢？这就是医学数据的现实。它并非世界的完美镜像，而是一系列扭曲的映像，被设计用来收集它的系统本身所塑造。理解这些扭曲——我们称之为**数据偏见**——不仅仅是一项技术工作；它是建立一个公正有效的医学体系的根本前提。

### 选择的阴影：谁属于数据之中？

让我们从最基本的问题开始：我们到底在看谁？我们分析的数据永远是世界的一个样本，而选择该样本的方式可能会深刻地误导我们。

思考一个经典的医学侦探故事：一项病例-对照研究。我们想知道大量饮酒是否与[胰腺炎](@entry_id:167546)有关。我们在一家地区医院找到了所有[胰腺炎](@entry_id:167546)患者（“病例”）。现在我们需要一个比较组（“[对照组](@entry_id:188599)”d）。一个简单的选择似乎是同一家医院里因不同原因入院的其他患者。这很方便，而且他们的数据格式也相同。

但这里有一个陷阱。人们 왜住院？原因多种多样，其中许多与大量饮酒等行为有关。普通住院患者中大量饮酒的流行率可能是 $0.25$，而在更广泛的社区中只有 $0. sobbing$。通过选择医院[对照组](@entry_id:188599)，我们不是将胰腺炎病例与他们来自的健康人群进行比较，而是与另一群本身就更可能是重度饮酒者的病人进行比较。这种形式的**选择偏见**使得酒精与[胰腺炎](@entry_id:167546)之间的联系看起来比实际要弱，因为“对照”组本身就偏向于我们正在研究的暴露因素 [@problem_id:4638763]。我们实际上是从一个充满镜子的殿堂中得出了结论。

同样的选择阴影也笼罩着人工智能的世界。一个AI模型的优劣取决于其训练数据。想象一个算法，旨在通过特定的实验室测试 $Z$ 来预测一种严重的并发症。问题是，该模型仅在实际接受了该测试的患者数据上进行训练。谁会接受测试呢？是那些症状 $X$ 让临床医生担忧的患者。但决定是否进行测试还受到其他因素的影响，比如医院有多忙，用变量 $H$ 表示。

在这里，一个奇怪的统计幽灵出现了。疾病 $D$ 同时导致症状 $X$ 和异常的实验室值 $Z$。在一般人群中，$X$ 和 $Z$ 仅通过疾病联系起来。但在我们的数据集中，我们只关注那些接受了测试的人。决定进行测试的事件 $S$ 是一个**对撞因子**（collider）——一个由两个独立父原因造成的事件：症状 ($X \to S$) 和医院繁忙程度 ($H \to S$)。通过只选择 $S=1$（即进行了测试）的患者，我们在 $X$ 和 $H$ 之间制造了一种虚假的联系。而且由于 $H$ 也影响实验室测量值 $Z$（例如，压力大的实验室会产生更多错误），我们现在人为地将症状 $X$与实验室值 $Z$ 联系起来，即使在考虑了疾病之后也是如此。模型的基本假设——即特征是独立的——被打破了，不是因为自然界的缺陷，而是因为我们观察方式的缺陷 [@problem_id:4588323]。对共同效应进行条件限制，为偏见潜入创造了后门。

### 不完美的镜头：当缺失即是证据

即使对于那些进入我们数据集的人来说，我们拥有的信息也 rarely是完整的。数据会丢失。但在医学领域，数据 rarely是“[完全随机缺失](@entry_id:170286)”的。它的缺失本身往往就是一个信号。

想一想一位医生决定是否要开具乳酸测试以检查败血症。如果病人看起来病得很重，他们更有可能开具这项测试。如果病人的乳酸值在记录中缺失，这通常不是随机的计算机故障。这往往是临床医生*不*担心的证据。这是一个典型的数据**[非随机缺失](@entry_id:163489)（MNAR）**案例：一个值缺失的概率取决于该值本身 [@problem_id:4849724]。如果我们试图通过平均我们已有的值来“填补”缺失的乳酸值，我们将灾难性地高估整个人群的平均乳酸水平，因为我们现有的数据不成比例地来自最病的患者。数据中的沉默胜于雄辩。

我们提问的方式也会改变我们得到的答案。在一项疫苗安全试验中，我们可以使用**主动报告**：我们每周发出调查问卷，积极询问参与者：“你头痛吗？”这种方法具有很高的**敏感性**；它能捕捉到许多人们可能忘记或忽略的真实头痛。但它的**特异性**较低；人们可能仅仅因为被提示就将日常的普通头痛归因于疫苗。另一方面，我们可以使用**非主动报告**：一条热线，人们如果遇到问题可以打电话报告。这种方法对于像头痛这样的轻微事件敏感性非常低（大量漏报），但特异性很高。至关重要的是，它有一个隐藏的优点：一个罕见但严重的事件，如[过敏性休克](@entry_id:196321)，本身就提供了报告的动力。虽然主动报告数据为我们计算发病率提供了稳定的分母，但非主动报告数据尽管有其缺陷，有时却能使一个罕见、严重的[危险信号](@entry_id:195376)从常见症状的背景噪音中更清晰地突显出来 [@problem_id:4989412]。没有哪个镜头是完美的；每个镜头都揭示了现实的不同切片。

最后，我们数据的时间线可能会对我们对因果关系的感知产生戏弄。一份电子健康记录包含了一系列丰富的事件：第0天开具了测试，第2天采集了标本，第3天发布了结果。但这项服务的保险索赔可能直到第10天才开具账单。如果我们研究该测试后发生不良事件的风险，并使用账单日期作为我们的“时间零点”，我们就创造了一个7天的**永生时间**窗口。根据定义，我们研究中的任何患者都必须从第3天存活到第10天才能产生账单。我们排除了任何在该[窗口期](@entry_id:196836)间发生不良事件的人，这使得测试看起来比实际更安全。我们在数据集的时间结构中内置了一种偏见 [@problem_id:5054635]。

### 代理变量的欺骗：测量错误的东西

也许最阴险的偏见形式出现在我们完全测量了错误的东西时。我们常常无法直接测量我们真正关心的概念，比如“健康需求”，所以我们使用一个**代理变量**——我们*能够*测量并且相信是一个良好替代品的东西。但如果这个代理变量有缺陷呢？

想象一个卫生系统想要找到那些患有未控制糖尿病且最需要预防性外展计划的患者。“需求”是一个复杂的概念。于是，该系统使用一个代理变量：“未来高额医疗保健成本”。逻辑似乎很合理——病情更重的人成本更高。一个AI模型被训练来预测高成本患者。

问题在于，产生高额医疗保健成本需要两样东西：生病，以及有途径获得护理。一个来自低收入社区、没有交通工具且保险不稳定的患者可能有迫切的健康需求（$N=1$），但无法获得服务，因此产生低成本。另一个健康需求完全相同但拥有优质保险和便利就医途径的患者则产生高成本（$C=1$）。一个被训练来预测成本的模型会学到，与低就医途径相关的特征是*低成本*的预测因子。因此，它会系统性地未能选中那些可能从外展计划中受益最多的人。在这种情况下，对于高就医途径的患者，该算法对真实需求的敏感度可能是$0.90$，但对于低就医途径的患者则只有$0.50$ [@problem_id:4524553]。

这是一个**建构效度错误**。该算法被完美地优化以完成其被告知的任务——预测成本。但预测成本是错误的目标。通过将地图（$C$）误认为领土（$N$），该系统构建了一个惩罚弱势群体的 inequity 引擎。

### 算法的回音室

当我们把这些各种形式的有偏见的数据——经过选择的、不完整的、基于有缺陷的代理变量的——喂给一个[机器学习算法](@entry_id:751585)时，机器不会纠正它们。它会学习它们。它以令人 chilling 的效率，放大了它所看到的模式，将历史的产物变成了未来的预测。

让我们回到一个再入院预测模型。一个算法被构建出来，用于标记那些最近出院且有高风险再次入院的患者，以便他们能接到主动的随访电话。该模型在两个人口群体（A组和B组）之间进行了公平性审计。数据显示，模型的**真正例率**——即有风险的患者被正确标记的比例——对于A组是$0.70$，但对于B组只有$0.50$。同时，**假正例率**——即没有风险的患者被错误标记的比例——对于A组是$0.15$，对于B组是$0.10$ [@problem_id:4367362]。

这就是**[算法偏见](@entry_id:637996)**：一种系统性的错误模式，它在不同群体间造成了利益和负担的不公平分配 [@problem_id:4849723]。在这里，一个来自B组、真正需要随访电话的患者被识别出来的可能性远低于一个来自A组的类似患者。这个算法在人类意义上并非“种族主义”或“性别歧视”；它没有意图。它仅仅是其所賴以训练的数据中內嵌的偏见的一个数学回声——这些偏见源于疾病流行率、护理模式或记录质量的差异。

### 从代码到良知：对公平的追求

认识到这些偏见是第一步。下一步是问它们对于处于系统中心的人类意味着什么。当一个看似客观的算法为一个来自记录不足群体的患者产生低风险评分时，它可能导致**认知不公**。一位信任该工具的临床医生可能会下意识地降低患者对自己症状陈述的可信度——这是一种**证言不公**。随着时间的推移，理解这一群体健康状况所需的概念和特征在我们集体医学知识中持续缺失——这是一种**诠释学不公** [@problem_id:4888862]。

那么，我们前进的道路是什么？不是放弃数据，而是以谦逊和严谨的态度对待它。伦理上的 imperative 是要量化这些不确定性。我们必须执行**定量偏见分析**，以模拟在关于我们数据缺陷的不同合理假设下，我们的结果可能会如何变化，从而将一个单一的、误导性的数字转变为一个反映我们真实知识状态的可能性范围 [@problem_id:4504888]。

我们必须审计我们的算法的公平性，而不仅仅是准确性。这包括测量特定群体的错误率，并评估模型在不同人群中的校准情况。当我们发现偏见时，我们必须采取行动。这可能涉及技术修复，如为不同群体重新[校准模型](@entry_id:180554)，或使用先进方法调整已知偏见。但更重要的是，它需要社会技术变革：改善记录实践，改变有缺陷的激励结构，以及——最关键的是——直接与患者和社区接触，以定义什么是“公平”和“有益”的结果 [@problem_id:4367362]。

机器中的幽灵是我们自身社会的反映。要建立一个真正智能和公正的医疗保健系统，我们必须超越我们算法的优雅数学，直面其数据所诞生的那个混乱、充满偏见且深刻人性化的现实。

