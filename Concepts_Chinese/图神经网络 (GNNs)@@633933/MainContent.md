## 引言
在一个由连接定义的世界里——从社交网络到生物通路——我们需要一种能够描述并从这种错综复杂的关系网络中学习的语言。传统的机器学习模型专为有序的像素网格或单词序列而设计，当面对图数据不规则、非结构化的特性时便会捉襟见肘。挑战在于构建一个能够从“关系与实体同等重要”的数据中学习的模型。这正是[图神经网络](@entry_id:136853)（GNNs）应运而生以填补的空白。

本文对[图神经网络](@entry_id:136853)进行了全面概述，不仅解释了它们的功能，更阐明了它们的“思考”方式。本文的结构旨在引导您从基本概念走向其开创性的应用。

- **第一章：原理与机制** 将揭开 GNNs 核心引擎的神秘面纱：优雅的[消息传递算法](@entry_id:262248)。我们将探讨其组成部分，通过将其与 Weisfeiler-Lehman 测试相关联来深入研究其理论能力和局限性，并对主流的空域和[谱域](@entry_id:755169)方法进行比较。

- **第二章：应用与跨学科联系** 将展示 GNNs 在各个领域的变革性影响。我们将看到 GNNs 如何彻底改变药物发现、解析生物网络、建模物理系统，甚至在医学中与隐私保护技术结合应用。

读完本文，您将理解使 GNNs 成为一个从我们所居住的互联世界中学习的强大工具的基本原理。

## 原理与机制

科学的核心在于寻找描述自然的恰当语言。为了描述行星的舞蹈，我们发明了微积分。为了描述亚原子世界，我们发明了量子力学。那么，要描述定义我们现代世界的复杂连接网络——从社交网络、生物通路到知识本身的结构——什么才是恰当的语言呢？答案是图的语言。为了从这种语言中学习，我们开发了一种卓越的工具：**[图神经网络](@entry_id:136853)（GNN）**。

但是，图不同于我们所习惯的有序数据。图像是整齐的像素网格，句子是清晰的单词序列。然而，图是一个狂野不羁的对象。它没有起点或终点，没有顶部或底部。每个点，或称**节点 (node)**，可以有不同数量的连接，或称**边 (edges)**。并且，如果你打乱节点的顺序，图在根本上保持不变。我们究竟如何才能构建一个尊重这种美丽而又混沌的对称性的[机器学习模型](@entry_id:262335)呢？

### 低语的网络：[消息传递](@entry_id:751915)

GNN 背后的核心洞见简单而优雅，令人叹为观止：让节点彼此交谈。想象你置身于一个巨大而拥挤的舞厅。你不知道派对的整体氛围，但你可以与你身边的邻居交谈。你问他们感觉如何，并将他们的回答与你自己的感受结合起来。现在，你对房间里你所在小角落的氛围有了更清晰的感知。如果你重复这个过程——询问你刚刚更新了状态的邻居他们更新后的感受——你的感知将慢慢地向外扩散。几轮过后，你的个人感受将是你自身状态与整个局部区域集体情绪的复杂融合。

这正是 GNN 所做的事情。这个过程被称为**[消息传递](@entry_id:751915) (message passing)**，是所有 GNN 的基本引擎。它可以分解为三个简单的步骤，在网络的每一个“层”中重复进行 [@problem_id:4329695]。

假设我们图中每个节点 $v$ 都始于一个特征向量，我们称之为它的初始“状态” $h_v^{(0)}$。

1.  **构建消息：** 在每一轮中，每个节点 $u$ 查看其当前状态 $h_u$，并准备一条要发送给其邻居的“消息”。在最简单的 GNN 中，这条消息就是节点自身的状态，可能经过一个可学习的线性映射（矩阵乘法）进行变换。

2.  **邻里会议：聚合 (Aggregation)：** 现在，节点 $v$ 从其所有邻居那里接收消息。它需要将这些消息合并成一个单一、连贯的摘要。但这里有一个关键点：邻居们形成一个无序集合。模型的输出不能依赖于是先听到 Alice 的消息再听到 Bob 的，还是先听到 Bob 的再听到 Alice 的。这个属性被称为**[置换不变性](@entry_id:753356) (permutation invariance)**。因此，聚合函数必须是一个[对称函数](@entry_id:177113)，比如**求和 (sum)**、**均值 (mean)** 或**最大值 (max)** 运算。

    你可能认为聚合器的选择是个小细节，但它对网络的[表达能力](@entry_id:149863)有着深远的影响。让我们来做一个简单的思想实验。假设一个节点有两个邻居，它们的特征值都是 $\pi$。邻居特征的多重集是 $\{\pi, \pi\}$。现在，想象另一个节点只有一个邻居，其特征值也是 $\pi$。它的邻居多重集是 $\{\pi\}$。这显然是两种不同的邻域结构。`MEAN` 聚合器对第一个节点会计算 $\frac{\pi+\pi}{2} = \pi$，对第二个节点会计算 $\frac{\pi}{1} = \pi$。它无法区分这两种情况！然而，`SUM` 聚合器对第一个节点会计算 $2\pi$，对第二个节点会计算 $\pi$。它*能够*区分 [@problem_id:4298447]。这揭示了一个微妙的事实：简单的 `SUM` 或 `MEAN` 聚合器并非同等强大。更复杂的 GNN 利用这一洞见来构建更具[表达能力](@entry_id:149863)的聚合方案 [@problem_id:4311909]。

3.  **更新：整合新知识：** 最后，节点 $v$ 将从其邻居处获得的单一聚合消息与它*自己*上一步的状态 $h_v$ 相结合。这种组合通常会通过一个[非线性激活函数](@entry_id:635291)，成为该节点的新状态 $h_v^{(1)}$。

这个三步舞——消息、聚合、更新——构成了 GNN 的一个层。在第 $k$ 层，这个过程的通用公式大致如下：

$$
h_v^{(k+1)} = \text{UPDATE}^{(k)} \left( h_v^{(k)}, \text{AGGREGATE}^{(k)} \left( \{ \text{MESSAGE}^{(k)}(h_u^{(k)}) \mid u \in \mathcal{N}(v) \} \right) \right)
$$

其中 $\mathcal{N}(v)$ 是节点 $v$ 的邻居集合。通过堆叠这些层，我们让消息得以传播得越来越远。经过一层后，一个节点了解其直接邻居。经过两层后，它接收到来自其邻居的邻居的消息，因此其状态现在反映了其 2 跳邻域的信息。经过 $K$ 层后，一个节点的特征向量变成了一个丰富的、学习到的嵌入，捕获了其整个 $K$ 跳邻域的信息 [@problem_id:4579984]。最终的节点嵌入既编码了节点的初始特征，也编码了其在网络中复杂的拓扑角色，可用于任何下游任务，从预测[蛋白质相互作用](@entry_id:271521)到分类社交媒体用户。

### 感知的极限：GNN 有多“聪明”？

那么，这种[消息传递](@entry_id:751915)机制很强大。但它到底有多强大？它的根本局限是什么？要回答这个问题，我们转向图论中的一个经典问题：**[图同构问题](@entry_id:261854) (graph isomorphism problem)**。你能判断两个图是否在结构上完全相同，只是在空间上重新排列了吗？

有一个简单而优美的算法可以解决这个问题，叫做 **Weisfeiler-Lehman (WL) 测试**。想象一下，你给图中每个节点赋予相同的初始颜色。然后，在每一轮中，你根据每个节点当前的颜色及其邻居颜色的*多重集*，为该节点分配一个新的颜色。你重复这个过程，直到颜色不再改变。如果在最后，一个图中的最终颜色集合与另一个图中的不同，你就知道这两个图不是同构的。

现在，仔细观察 WL 测试的更新规则。它是一个关于节点标签及其邻居标签多重集的[单射](@entry_id:183792)（一对一）函数。这听起来很熟悉，不是吗？这正是 GNN [消息传递](@entry_id:751915)方案的一个抽象、理想化的版本！已有证明表明，最强大的[消息传递](@entry_id:751915) GNN，实际上其能力恰好等同于一维 WL 测试，仅此而已 [@problem_id:4311909]。它们本质上是这种着色算法的一个可学习的、连续的版本。

这种等价性也揭示了它们的局限性。任何两个一维 WL 测试无法区分的图，标准 GNN 也无法区分。考虑以下两个简单的“共病网络”，其中节点是疾病，边连接着经常同时出现的疾病 [@problem_id:5199242]：

-   **图 1 ($G_1$)：** 由六种疾病组成的环状图，每种疾病与其他两种相连。
-   **图 2 ($G_2$)：** 两个独立的疾病三角形，其中三种疾病两两相互连接。


_图 1：一个简单的 GNN 无法区分的两个图。两者都是 2-正则的，意味着每个节点都有两个邻居。一个基于局部邻域聚合的 GNN 在每个节点处看到的结构都是相同的。_

