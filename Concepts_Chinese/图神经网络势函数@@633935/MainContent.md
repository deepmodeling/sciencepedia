## 引言
模拟原子间复杂的相互作用是现代科学的基石，但这带来了一个重大挑战：植根于量子力学的最精确方法对于大体系来说计算速度太慢，而更快的经典方法又缺乏精度。图神经网络（GNN）[势函数](@entry_id:176105)作为一种革命性的解决方案应运而生，它将物理学的预测能力与人工智能的学习能力相结合，弥合了这一关键差距。这些模型从[量子数](@entry_id:145558)据中学习，以接近量子力学的精度预测[原子间作用力](@entry_id:158182)，而计算成本仅为量子力学的一小部分。本文将深入探讨GNN[势函数](@entry_id:176105)的世界，全面概述其设计和影响。

以下章节将引导您了解这一前沿领域。首先，在“原理与机制”一章中，我们将探讨GNN[势函数](@entry_id:176105)的理论基础，重点关注它们如何巧妙地将物理学的[基本对称性](@entry_id:161256)（如[不变性](@entry_id:140168)和[等变性](@entry_id:636671)）融入其架构中。随后，“应用与跨学科联系”一章将展示这些势函数在实践中的变革性力量，阐述它们如何被用于模拟物质运动、预测宏观材料性质，甚至设计全新的材料，从而重塑了从[材料科学](@entry_id:152226)到[药物发现](@entry_id:261243)等多个学科。

## 原理与机制

要模拟构成我们世界的原子之间错综复杂的舞蹈，我们必须首先理解它们所遵循的旋律——物理定律。从本质上讲，[图神经网络](@entry_id:136853)（GNN）势函数不仅仅是一项巧妙的计算机科学技术，它更是一个经过精心雕琢、旨在体现自然界基本对称性的数学对象。让我们踏上征程，去理解这些原理以及将其付诸实践的精妙机制。

### 宇宙的游戏规则

想象一下，您正尝试构建一个模型来预测分子的能量。一种简单甚至有些天真的方法可能是，获取所有原子的[笛卡尔坐标](@entry_id:167698)，将它们展平成一个长列表，然后将此列表输入一个标准的[神经网](@entry_id:276355)络。让我们在一个熟悉的、高度对称的分子——苯（$\text{C}_6\text{H}_6$）上测试这个想法。

您使用碳和氢原子按常规顺序索引的苯分子的能量来训练您的网络。网络学习后给出了一个不错的答案。现在，您给它输入*完全相同*的分子，但这次您只是改变了标签——您之前称为碳-1的原子现在是碳-2，碳-2是碳-3，依此类推，进行[循环移位](@entry_id:177315)。对您来说，这还是同一个分子。但对于这个天真的[神经网](@entry_id:276355)络来说，它接收到的输入向量完全不同。它精心学习到的权重本来期望在前三个位置接收碳-1的坐标，现在却接收到了碳-2的坐标。结果呢？它几乎肯定会预测出一个不同（因此是错误）的能量。这是一次灾难性的失败。

这个思想实验揭示了一个深刻的真理：物理定律对我们的标记方式毫不在意。这就是**[置换不变性](@entry_id:753356)**：交换两个相同的粒子不会改变系统的能量。同样，物理定律也不关心您在空间中的位置（**平移不变性**）或您面向的方向（**[旋转不变性](@entry_id:137644)**）。任何旨在表示物理现实的[计算模型](@entry_id:152639)，都不能仅仅通过包含这些对称性的样本进行*训练*，而必须将这些对称性融入其结构本身 [@problem_id:2457453]。

### 构建[不变性](@entry_id:140168)的艺术：初次尝试

我们如何才能构建一个尊重这些规则的模型呢？第一个突破来自一个被称为“电子物质短视性原理”的物理洞见 [@problem_id:2837978]。该原理指出，单个原子的能量贡献主要由其紧邻的局域环境决定，而不是由材料中远离它的原子决定。这启发了一种强大而优雅的分解方法：我们可以将系统的总能量 $E$ 写成单个原子能量贡献 $E_i$ 的总和：

$$E = \sum_{i} E_i$$

这个看似简单的公式是神来之笔。求和运算是可交换的——你加总各项的顺序无关紧要。因此，如果我们使用一个对所有相同种类的原子都相同的函数（例如，一个函数用于所有碳原子，另一个函数用于所有氧原子）来计算原子能量 $E_i$，那么交换两个相同的原子只会重新[排列](@entry_id:136432)总和中两个相同的项，而总能量保持不变。[置换不变性](@entry_id:753356)问题就这么解决了！[@problem_id:2648619] 为了处理具有多种不同化学物종的系统，我们可以为每种物种使用一个单独的模型，或者更灵活地为每个元素分配一个称为**物种嵌入**的独特、可学习的向量，并将其输入到一个单一的共享模型中 [@problem_id:3422822]。

处理完[置换对称性](@entry_id:185825)后，我们还剩下平移和[旋转对称](@entry_id:137077)性。一个原子的能量贡献 $E_i$ 必须只依赖于其局域环境的几何结构，而不依赖于其在空间中的绝对位置或整个系统的朝向。解决方案是使用本身对[旋转和平移](@entry_id:175994)不变的量来描述原子邻域：原子间的距离和原子三元组之间的夹角。

这催生了第一代现代[机器学习势函数](@entry_id:138428)，例如[Behler-Parrinello神经网络](@entry_id:194343)（BPNN）。其策略是首先将局域原[子环](@entry_id:154194)境转换为一个固定大小、人工设计的[特征向量](@entry_id:151813)，称为**描述符**或“指纹”，该向量在设计上就是不变的。然后将此描述符输入一个标准的[神经网](@entry_id:276355)络。可以将其想象为拍摄原子邻域的三维照片，并系统地测量一组不变属性（如所有邻居距离和所有键角的列表）来生成一个[特征向量](@entry_id:151813)。更复杂的描述符，如原子位置光滑重叠（SOAP），会在中心原子周围创建一个平滑的邻居密度[分布](@entry_id:182848)，然后使用从量子力学借鉴的工具（如球谐函数）来分析此密度，从而生成一个旋转不变的功率谱——这是一种内容丰富且稳健的环境指纹 [@problem_id:2837978]。

### 让机器来学习：图的兴起

基于描述符的方法功能强大，但有一个局限：必须由人来预定义特征。我们必须决定哪些几何属性是重要的。如果模型能够直接从数据中*学习*最相关的特征呢？

这就是从人工设计特征到学习表示的飞跃，也是图神经网络[势函数](@entry_id:176105)背后的核心思想。我们可以将任何原[子集](@entry_id:261956)合视为一个图，其中原子是**节点**，任何两个距离小于某个截断距离 $r_c$ 的原子之间存在一条**边**。我们不再计算固定的描述符，而是为每个原子分配一个初始[特征向量](@entry_id:151813)（例如，其物种嵌入），然后通过一个称为**消息传递**的过程来优化这些特征。

想象一下，原子是房间里的人，每个人都有一条信息。在第一轮[消息传递](@entry_id:751915)中，每个人都与他们的直接邻居分享信息。在下一轮中，他们再次与邻居交谈，但这次他们的消息包含了上一轮听到的内容。经过 $L$ 轮（或层）这个过程后，每个人所持有的信息（即原子的[特征向量](@entry_id:151813)）已经受到了最远 $L$ 个连接之外的人的影响。网络的“感受野”——影响一个原子最终特征的空间区域——随着层数 $L$ 和图的[截断半径](@entry_id:136708) $r_c$ 的增加而增长 [@problem_id:3449485]。这个过程自然地构建了一个丰富的、多体的原[子环](@entry_id:154194)境描述，远远超出了简单的原子对和原子三元组。

### 更深的对称性：从[不变性](@entry_id:140168)到[等变性](@entry_id:636671)

至此，我们来到了现代GNN[势函数](@entry_id:176105)中最优美和精妙的概念。基于描述符的方法在第一步就强制实现[不变性](@entry_id:140168)——通过将所有丰富的[三维几何](@entry_id:176328)信息压缩成一个旋转不变的[特征向量](@entry_id:151813)。但这丢弃了大量信息。如果我们需要模型来推断[方向性](@entry_id:266095)物理量，如力或[电场](@entry_id:194326)，该怎么办？

如果你旋转一个晶体，其上原子的受力也必须随之旋转。一个内部机制纯粹是“不变的”模型无法实现这一点。它会在晶体任何朝向下都预测出相同的力矢量，这在物理上是荒謬的。模型的输出必须随其输入*[协变](@entry_id:634097)*。这个性质被称为**[等变性](@entry_id:636671)**。对于像力这样的矢量，[等变性](@entry_id:636671)意味着，如果你用一个旋转矩阵 $R$ 旋转输入坐标，输出的力矢量也会被同一个矩阵 $R$ 变换 [@problem_id:3455847]。

等变GNN就是为此而设计的。它们的内部[特征向量](@entry_id:151813)不局限于标量、不变的特征，而可以是几何对象本身——即携带方向信息并具有明确变换性质的向量和张量。

这个魔术是如何实现的呢？网络并非从头学习[三维旋转](@entry_id:148533)定律，那样的效率会极低。相反，它利用群论的数学将这些定律內建其中。特征和消息被表示为**球张量**，这些对象由角动量量子数 $\ell$ 分类（其中 $\ell=0$ 是标量，$\ell=1$ 是矢量，等等）。当网络组合特征时——例如，当一个原子聚合来自其邻居的消息时——它不被允许以任何任意方式混合它们。它必须遵守严格的[角动量耦合](@entry_id:145967)规则，这与支配原子中电子的规则完全相同。这些规则是通过称为**[Clebsch-Gordan系数](@entry_id:142551)**的数学对象来实现的 [@problem_id:3449486]。通过硬编码这些几何规则，网络保证了如果输入系统被旋转，所有的中间[特征向量](@entry_id:151813)都会以物理上一致的方式旋转。网络的任务不是学习旋转规则，而是学习物理上允许的相互作用的*强度*。

最后，为了计算总能量——这是一个标量并且*必须*是不变的——网络会对其等变特征进行最终的组合，这种组合被专门设计用来产生一个[不变量](@entry_id:148850)（一个类型为 $\ell=0$ 的球张量）。

### 从势函数到盒子里的宇宙

我们现在拥有一个精巧的机器——一个等变GNN，它能接收原子构型并预测其能量，同时完美地遵守物理学的基本对称性。但单个能量值仅仅是个起点。

这个框架的真正威力在于，从输入坐标到最终能量，整个GNN是[可微函数](@entry_id:144590)的复合。这意味着我们可以使用现代人工智能的主力工具——**[自动微分](@entry_id:144512)**（也称为[反向传播](@entry_id:199535)），来解析地计算能量相对于每个原子位置的导数。根据物理学的一个基本定义，[势能](@entry_id:748988)的负梯度就是**力** [@problem_id:3422849]。

这是一个深刻的结果。我们得到的施加在所有原子上的力，不是一个单独的预测，而是能量的精确梯度。这保证了[力场](@entry_id:147325)是**保守的**，这对于运行稳定且具有物理意义的分子动力学模拟至关重要。一个[保守力场](@entry_id:164320)确保能量随时间守恒，防止了系统自发升温或降温等病态现象 [@problem_id:3422849]。与那些试图直接学习力但常常不能保证[能量守恒](@entry_id:140514)的模型相比，这是一个主要优势。

在实践中，开发GNN势函数是一项权衡工作。我们希望模型不仅在能量上准确，在力甚至材料的应力张量（其对拉伸或压缩的响应）上也准确。这通常通过在一个包含所有这些物理量误差的组合损失函数上训练模型来实现。调整[损失函数](@entry_id:634569)中每个分量权重的过程是一个精细的[优化问题](@entry_id:266749)，人们通常在**[帕累托前沿](@entry_id:634123)**上寻找解——在这一点上，你无法在不使另一种误差（例如能量）恶化的情况下改善一种误差（例如力）[@problem_id:3455782]。有时，对于那些设计上并非完美对称的架构，甚至可以在损失函数中添加一个“对抗性”惩罚项，以明确惩罚任何检测到的对物理对称性的违反，从而将模型推回到一个物理上正确的描述上来 [@problem_id:3455774]。

通过这一系列的原理——从对对称性的基本要求，到[等变性](@entry_id:636671)的优雅机制，再到训练的实际操作——[图神经网络](@entry_id:136853)[势函数](@entry_id:176105)成为物理学与机器学习的强大而优美的结合体，使我们能够以前所未有的精度和速度模拟物质世界。

