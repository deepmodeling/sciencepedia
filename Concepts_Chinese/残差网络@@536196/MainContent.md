## 引言
长期以来，人们一直[期望](@article_id:311378)深度神经网络能通过学习跨越多个层次的层级特征来解决复杂问题。然而，一个根本性的障碍在历史上一直阻碍着这一目标的实现：随着网络深度的增加，训练变得异常困难，这在很大程度上是由于[梯度消失问题](@article_id:304528)扼杀了早期层的学习。我们如何才能构建数百甚至数千层深的网络而让信号不丢失呢？本文深入探讨了[残差网络](@article_id:641635)（[ResNet](@article_id:638916)s），这一革命性的架构为这个深奥的挑战提供了一个惊人地简单的解决方案。我们将首先探讨 [ResNet](@article_id:638916)s 的核心“原理与机制”，剖析简单的“跳跃连接”如何创建梯度高速公路、促进隐式网络集成，以及如何通过[微分方程](@article_id:327891)的视角将其视为一个[连续变换](@article_id:305274)。随后，在“应用与跨学科联系”部分，我们将看到这一核心原则如何远远超出一个简单的训练技巧，影响着从对抗性鲁棒性和持续学习到与量子力学和生物系统之间惊人相似的方方面面。

## 原理与机制

乍一看，[残差网络](@article_id:641635)背后的架构创新似乎简单得近乎侮辱人。几个世纪以来，工匠、工程师和数学家都是通过按顺序堆叠更简单的组件来构建复杂事物的。传统的[深度神经网络](@article_id:640465)正是如此：它接收一个输入，用第一层对其进[行变换](@article_id:310184)；再接收该输出，用第二层进行变换，以此类推。每个层都被赋予学习一个完整变换的任务，比如从输入表示 $x$ 到输出表示 $H(x)$。[ResNet](@article_id:638916) 的革命性思想是增加一根微小的[连接线](@article_id:375787)，即“跳跃连接”，它将原始输入 $x$ 与该层变换的输出相加。

因此，网络不再直接学习 $H(x)$，而是学习一个*[残差](@article_id:348682)*函数 $F(x)$，该块的输出变为 $y = x + F(x, \theta)$。网络被问到的问题不再是“你应该产生什么样的全新表示？”而是“你应该对当前表示做出什么微小的改变，即[残差](@article_id:348682)？”这似乎只是一个微不足道的[变量替换](@article_id:301827)。究竟为什么这个简单的加法操作能够解锁训练数百甚至数千层深的网络的能力——这一壮举在之前被认为是不可想象的？答案，正如在物理学和数学中经常出现的那样，在于理解信息的流动——在这里，是梯度的流动。

### 梯度高速公路

想象一下，你站在一队长龙的一端，向第一个人耳语一条信息，他再耳语给第二个人，依此类推。每经过一次转述，信息都会被轻微扭曲、模糊或改变。当信息传到队伍的另一端时，可能已经面目全非。这本质上就是**[梯度消失问题](@article_id:304528)**。在训练过程中，“误差”信号（梯度）必须从最后一层[反向传播](@article_id:302452)到初始层，以告诉它们如何调整参数。在一个非常深的传统网络中，这个信号逐层向后传递。每一层的[雅可比矩阵](@article_id:303923)都会乘以梯度向量。如果这些矩阵的奇异值平均小于 1，梯度信号就会随着每一步呈指数级缩小。经过一百层后，一个活跃的误差信号可能会变成微弱的耳语，网络的早期层将什么也学不到。

[残差连接](@article_id:639040)为这个梯度构建了一条高速公路。让我们看看其中的数学原理，它 ternyata 惊人地优雅。一个标准[残差块](@article_id:641387)的雅可比矩阵是：
$$
J_{\text{res}} = \frac{\partial}{\partial x} (x + F(x)) = I + F'(x)
$$
其中 $I$ 是单位矩阵，$F'(x)$ 是[残差](@article_id:348682)函数的雅可比矩阵。

当梯度通过该层向后传递时，它会乘以 $J_{\text{res}}^T = (I + F'(x))^T$。那个 $I$ 项就是高速公路。它意味着，梯度至少可以完全不变地通过该层。$F'(x)$ 项增加了一个新的、变换过的梯度版本，但原始信号始终被保留下来。

为了更清晰地看到这一点，我们可以考虑一个玩具模型：一个深度*线性*[残差网络](@article_id:641635)，其中每个[残差](@article_id:348682)函数只是一个[矩阵乘法](@article_id:316443)，$F_l(x_l) = W_l x_l$ [@problem_id:3169686]。在一个普通线性网络中，每层的[雅可比矩阵](@article_id:303923)就是 $W_l$，其[特征值](@article_id:315305)，我们称之为 $\lambda_i$，决定了信号是缩小还是增长。如果[特征值](@article_id:315305)的模 $|\lambda_i|$ 小于 1，信号就会消失。而在线性 [ResNet](@article_id:638916) 中，雅可比矩阵是 $I+W_l$。它的[特征值](@article_id:315305)不是 $\lambda_i$，而是 $1+\lambda_i$！[@problem_id:3187046]

这就是全部的诀窍。在训练开始时，权重矩阵 $W_l$ 通常被初始化为非常小的值，因此它们的[特征值](@article_id:315305) $\lambda_i$ 接近于 0。对于普通网络，这意味着梯度信号会立即消失。但对于 [ResNet](@article_id:638916)，放大因子是 $1+\lambda_i$，非常接近 1。梯度几乎完美地流经数百层。这是一个极其简单而强大的机制。

然而，这并非免费的午餐。如果在训练过程中，网络学到的[残差](@article_id:348682)函数 $F'(x)$ 使得范数 $\|I + F'(x)\|_2$ 持续大于 1，梯度现在就可能呈指数级增长，导致**[梯度爆炸问题](@article_id:641874)** [@problem_id:3185064]。恒等连接创造了一个不易出现[梯度消失](@article_id:642027)的格局，但爆炸的风险依然存在。这促使了进一步的改进，比如缩减[残差](@article_id:348682)分支的尺度，以明确控制这个放大因子。理论与实践之间的这种持续对话——发现问题、提出解决方案、找到其局限性并加以完善——是科学进步的命脉。

### 网络的议会

梯度的故事很有说服力，但这只是看待 [ResNet](@article_id:638916) 的一种方式。另一个同样优美的视角是将单个 [ResNet](@article_id:638916) 不看作一个单一的庞然大物，而是看作一个由许多不同网络组成的庞大*集成*。

如果我们展开递归关系 $x_{l+1} = x_l + F_l(x_l)$，我们会发现最终输出 $x_L$ 是一个宏大的总和：
$$
x_L = x_0 + \sum_{l=0}^{L-1} F_l(x_l)
$$
请注意，每个函数 $F_l$ 的输入是 $x_l$，它依赖于所有之前的[残差块](@article_id:641387)。现在，思考一下信号从输入 $x_0$ 到输出 $x_L$ 可能采取的路径。在 $L$ 个块中的每一个，信号既可以通过[残差](@article_id:348682)函数 $F_l$，也可以通过恒等连接“跳过”它。[组合分析](@article_id:329264)表明，从输入到第 $k$ 层，信号可能采取的路径有 $2^k$ 条之多 [@problem_id:3108062]。

这意味着单个 [ResNet](@article_id:638916) 的行为就像指数级数量的不同网络路径交织在一起。这些路径中的大多数都相对较短，只通过少数几个[残差块](@article_id:641387)，其余时间则走恒等跳跃连接。这种结构提供了令人难以置信的鲁棒性。如果你在[前向传播](@article_id:372045)过程中随机“丢弃”一些[残差块](@article_id:641387)（通过将 $F_l$ 设为零），网络的性能会平稳下降，而不是灾难性地崩溃。这就像一个大型议会中的一些成员缺席投票；最终的决定不太可能发生剧烈变化 [@problem_id:3169730]。这与传统的深度[网络形成](@article_id:305967)鲜明对比，在传统网络中，每一层都是单一链条上的关键环节。

这种集成观点甚至可以更进一步。[ResNet](@article_id:638916) 的学习方式让人联想到一种强大的经典机器学习技术，即**[梯度提升](@article_id:641131)（gradient boosting）**。在提升法中，通过迭代添加“[弱学习器](@article_id:638920)”来构建一个强预测器，其中每个新学习器都被训练来纠正当前集成的错误（即“[残差](@article_id:348682)”）。在某些简化的假设下，训练 [ResNet](@article_id:638916) 的行为与此类似：每个块 $F_l$ 学会贡献一个与损失函数负梯度方向一致的修正项——换句话说，它学着去修复剩余的错误 [@problem_id:3169973]。

### 作为[连续变换](@article_id:305274)的学习

也许对 [ResNet](@article_id:638916)s 最深刻的解读来自一个完全不同的领域：常微分方程（ODE）的[数值分析](@article_id:303075)。让我们稍微改写一下[残差块](@article_id:641387)的更新规则：
$$
x_{k+1} = x_k + h \cdot f(x_k, \theta_k)
$$
在这里，我们将层的变换捆绑到一个函数 $f$ 中，并引入了一个小步长 $h$。如果我们将深度 $k$ 看作时间的替代物 $t$，任何物理学家或工程师都能立即认出这个方程。它就是**前向欧拉法**，是求解 ODE 的最简单[数值方法](@article_id:300571)：
$$
\frac{dx}{dt} = f(x(t), \theta(t))
$$
从这个角度看，[ResNet](@article_id:638916) 不再是一系列离散的层。相反，它是特征在“时间”即深度的维度上进行单一、[连续变换](@article_id:305274)的离散化 [@problem_id:3202086]。输入 $x_0$ 是 $t=0$ 时的初始状态，网络的任务是积分系统的动力学，以找到在某个稍后时间 $T$ 的最终状态 $x(T)$。

这个观点极具启发性。它解释了为什么[残差](@article_id:348682)函数 $F_l$（或 $h \cdot f$）应该是“小”变换：在[数值积分](@article_id:302993)中，稳定性要求采取小步长。它还为设计[网络架构](@article_id:332683)开辟了一个全新的工具箱。如果前向欧拉法对于某些“刚性”问题不稳定，为什么不使用更稳定、更复杂的 ODE 求解器，如隐式方法或[龙格-库塔法](@article_id:304681)，作为新型网络块的蓝图呢？这已经催生了“神经 ODE”及相关模型的整个子领域，所有这些都源于这个优美的类比。

### 学习的是什么，为什么更容易？

那么，哪种观点是正确的？[ResNet](@article_id:638916) 是关于梯度、集成还是 ODE？美妙的真相是，它们都是。这些是同一基本原则的不同方面。跳跃连接不仅仅是为[梯度消失问题](@article_id:304528)提供了一个技术修复；它从根本上重构了学习问题本身。

[通用近似定理](@article_id:307394)告诉我们，即使是一个普通的浅层神经网络，原则上也可以近似任何[连续函数](@article_id:297812)。问题从来不在于*[表示能力](@article_id:641052)*。[ResNet](@article_id:638916) 的通用性并不比其他网络更强 [@problem_id:3194207]。它们的魔力在于使学习这些函数对于非常深层的架构变得切实可行。

它们通过改变问题来实现这一点。我们不再要求网络学习一个可能非常复杂的函数 $f(x)$，而是要求它学习[残差](@article_id:348682)函数 $r(x) = f(x) - x$。如果我们试图建模的函数，比如说，是对输入图像的精炼，它很可能“接近”[恒等函数](@article_id:312550)。在这种情况下，[残差](@article_id:348682) $r(x)$ 将是一个简单得多、幅度小得多的函数，网络从随机初始化开始近似它要容易得多。

这就是[残差块](@article_id:641387)的统一精髓。它学会做出修正。我们可以将这种修正看作一个即时更新，一个通过其与假想目标的对齐度来衡量的误差信号 [@problem_id:3169972]。我们可以将其看作一个[弱学习器](@article_id:638920)对强大集成的贡献。或者我们可以将其看作一个动力系统连续演化中的一小步。在所有情况下，核心思想都是相同的：通过专注于学习*变化*而不是学习整个状态，[残差网络](@article_id:641635)将训练超深度网络这一不可能的任务变成了现实，从而永久地改变了人工智能的面貌。

