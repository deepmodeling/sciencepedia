## 引言
人工智能 (AI) 有望彻底改变医学，成为协助临床医生诊断黑色素瘤等疾病的强大工具。然而，这个技术“神谕”并非天生客观；它从反映现实世界偏见的数据中学习。这就产生了一个重大挑战：人工智能系统可能无意中延续甚至加剧健康不平等，为特定人群提供可靠性较低的结果。本文旨在解决开发人工智能模型与以公平、合乎伦理的方式部署模型之间的关键知识差距。

您将踏上一段旅程，探索在皮肤病学领域构建公平人工智能的核心挑战与解决方案。在第一章**原则与机制**中，我们将剖析偏见如何源于有偏的数据和有缺陷的[系统设计](@entry_id:755777)，探索超越简单统计的严格公平性定义，并审视必须管理人机关系的问责框架。随后，在**应用与跨学科联系**一章中，我们将理论与实践相结合，展示这些原则如何融入监管审批、临床实施和患者数据的伦理管理中，揭示真正的公平性是一项宏大的跨学科事业。

## 原则与机制

想象一个神谕。一种新型的医学神谕，它不是由大理石和神话构成，而是由硅和数据构成。我们向它展示一张患者皮肤上可疑痣的照片，它会低声回给我们一个数字——这个痣是危险黑色素瘤的概率。我们可能期望这样一台机器，一个**人工智能 (AI)** 的奇迹，是最终的客观观察者，不受可能影响人类判断的偏见所干扰。但如果这个神谕有缺陷呢？如果它的宣告对某些人系统性地不如对另一些人可靠呢？这不是一个假设性的谜题；这是医学领域人工智能公平性的核心挑战，理解它就是一场深入探究技术、数据和人类价值观如何交织的旅程。

### 有缺陷神谕的剖析

让我们以一个看似简单的问题开始我们的旅程。一家医院部署了一款新的人工智能皮肤病学工具。对于肤色较浅的患者（菲茨帕特里克皮肤类型 II–III），该人工智能的**敏感性**——即其正确识别出真正黑色素瘤的能力——为 $0.92$。对于肤色较深的患者（菲茨帕特里克皮肤类型 V–VI），其敏感性仅为 $0.75$。这种差异对患者来说到底意味着什么？

我们不要迷失在百分比中，让我们谈谈具体的人。想象两组各 $10,000$ 名患者，一组肤色较浅，另一组肤色较深。在两组中，我们假设黑色素瘤的患病率相同，约为 $1\%$，即 $100$ 人患有此病。

现在，让我们引入我们的人工智能神谕。在肤色较浅的组中，敏感性为 $0.92$，人工智能将正确标记出 $100 \times 0.92 = 92$ 例黑色素瘤。这意味着将有 $8$ 例被漏诊——这是一个不完美测试中悲剧性但不可避免的结果。

但肤色较深的组呢？敏感性为 $0.75$，人工智能将只标记出 $100 \times 0.75 = 75$ 例黑色素瘤。在这里，有 $25$ 例被漏诊。

这种差异现在变得异常清晰。在一组 $10,000$ 次筛查中，与肤色较浅的患者相比，该人工智能系统预计将为肤色较深的患者多漏诊 $17$ 例黑色素瘤（$25 - 8 = 17$）[@problem_id:4849731]。这个数字 $17$ 不仅仅是一个统计产物，它是一个深层伦理失败的量化体现。它代表了对医学界两条最神圣原则的违背：**不伤害原则**（“首先，不造成伤害”），因为该系统正在为一个特定群[体制](@entry_id:273290)造可预见的伤害风险；以及**公正原则**，该原则要求新技术的惠益和风险应被公平分配。事实证明，这个神谕正在系统性地辜负它本应服务的一部分人群。

### 机器中的幽灵：偏见从何而来？

一台机器，一个由代码构成的逻辑结构，怎么会有偏见？它不怀有偏见，也不心存怨恨。答案是，人工智能是一台学习机器，而它的老师是数据。一个人工智能模型就像一个只用一套书学习的学生。如果那些书不完整或有偏颇，那么学生的知识也会如此。偏见并非存在于算法的灵魂中；它是从它所赖以学习的数据中萦绕不散的幽灵。

这通常被称为“垃圾进，垃圾出”问题，但现实更为微妙。在许多现实世界的场景中，数据并非垃圾——它只是我们世界的一个有偏颇的反映。例如，这些皮肤病学模型的训练数据集通常包含绝大多数来自肤色较浅患者的图像。我们的一个假设情景就反映了这一现实，其训练集由 $80\%$ 的浅肤色图像和仅 $20\%$ 的深肤色图像构成 [@problem_id:4867509]。人工智能对一个群体的经验——可供学习的例子——远多于另一个群体。

但问题比纯粹的数字更深，这种现象被称为**代表性偏见**。偏见可以通过其他更隐蔽的方式悄然潜入：

*   **[标签噪声](@entry_id:636605)：** 数据中的“基准真相”标签（例如，“黑色素瘤”或“良性”）通常由人类专家提供。如果这些专家本身在诊断深色皮肤上的罕见病症或非典型表现时信心不足，那么这种不确定性就可能作为“噪声”被固化到数据中，而人工智能会勤奋地学习这些噪声 [@problem_id:4867509]。

*   **[域漂移](@entry_id:637840)：** 人工智能可能使用一种名为皮肤镜的特殊设备拍摄的原始、高分辨率图像进行训练，但随后被部署在一家诊所，在那里它接收的是来自患者手机的模糊图像。训练世界与现实世界之间的这种不匹配，即**[域漂移](@entry_id:637840)**，可能导致其性能出现不可预测的下降 [@problem_id:4867509]。

人工智能并“不知道”它在行不公之事。它只是一个强大的[模式匹配](@entry_id:137990)引擎，忠实地再现了其被赋予数据中存在的模式——以及偏见。

### 超越代码：当“公平”的算法创造出不公的世界

让我们做一个思想实验。假设我们成功了。通过巨大的努力，我们构建了一个在所有肤色上都具有完全相同准确性的人工智能。我们实现公平了吗？

考虑一个国家卫生服务机构，它通过一个智能手机应用程序来部署这个“完美”的人工智能，用于黑色素瘤筛查 [@problem_id:4400728]。这个应用程序本身是公平的。但对应用程序的访问却并非如此。在收入较高、城市化的人群中，也许有 $92\%$ 的人拥有兼容的智能手机。而在收入较低、农村地区的人群中，这个数字可能只有 $35\%$。

让我们追踪其后果。即使两个群体在拥有应用程序后都以相同的频率使用它，筛查项目的整体**覆盖率**也大相径庭。在城市群体中，大约有 $74\%$ 的人口接受了筛查。而在农村群体中，只有 $28\%$ 的人接受了筛查。结果呢？该项目最终可能在已经处于优势的群体中检测出 $66\%$ 的黑色素瘤病例，但在弱势群体中仅检测出 $25\%$ 的病例。

这是一个至关重要的教训。数学模型的公平性只是整个谜题的一块拼图。我们必须考虑整个**社会技术系统**——技术本身、使用它的人、访问它所需的基础设施，以及它被部署于其中的经济和社会背景。一个嵌入在不公平系统中的“公平”算法，可能会加剧而非减少健康不平等。真正的公平要求我们超越代码，审视它所触及的世界。

### 探寻真正的公平：更深层次的审视

这给我们带来了一个更深刻的问题：我们所说的公平性究竟*意味着*什么？正如科学家和伦理学家所发现的，公平性没有单一的定义，而是有几个深刻的概念帮助我们更清晰地思考。其中有两个概念尤其优美且富有启发性。

第一个是**个体公平性**。该原则指出，“相似的个体应被相似地对待”。关键当然在于“相似”这个词。对于皮肤病学人工智能而言，相似性不应由数据点之间某种通用的数学距离来定义，它必须是一种*具有临床意义*的相似性。对于黑色素瘤，这意味着要考察公认的ABCDEs标准：不对称性 (Asymmetry)、边界不规则 (Border irregularity)、颜色多样性 (Color variegation)、直径 (Diameter) 和演变 (Evolution)。个体公平性原则要求，对于皮肤科专家会判定为形态相似的两个皮损，无论它们属于谁，人工智能都应给出相似的风险评分 [@problem_id:4426600]。

第二个，甚至更强大的思想是**[反事实公平性](@entry_id:636788)**。它提出了一个因果问题：“如果该患者属于另一个群体，而在所有其他因果相关因素保持不变的情况下，他/她是否会得到相同的诊断？” 再次思考我们的皮肤病变人工智能。我们希望它的判断基于病变本身的生物学特性，而不是周围皮肤的色调。我们可以想象一个假设的，或“反事实的”世界，在那里我们可以将完全相同的病变放置在不同色调的皮肤上。如果人工智能的预测保持不变，那么[反事实公平性](@entry_id:636788)就成立 [@problem_id:4426600]。虽然我们无法在现实中做到这一点，但我们可以用复杂的因果模型和数据生成技术来模拟它。这触及了非歧视的核心：确保像种族或肤色这样的受保护属性不是导致不同结果的*原因*。这些是正确评估我们系统所需的严谨原则，从而超越简单的统计数据，进入因果推理的层面 [@problem_id:4500034]。

### 人在环路中：权威、信任与问责

所以我们有了这个强大、复杂且可能存在缺陷的神谕。作为临床医生和患者，我们该如何负责任地与它互动？答案在于明确所有相关方的角色——机器、医生和患者。

首先，我们必须区分**可信度**和**权威**。人工智能可以有可信度。如果它经过了严格的测试并显示出高性能（例如，**受试者工作特征曲线下面积 ([AUROC](@entry_id:636693))** 为 $0.95$），它的输出就是一条可信的证据 [@problem_id:4861466]。但它不能拥有权威。**认知权威**——即被相信和信任的合理权利——来源于能力、为患者最佳利益行事的信托责任，以及至关重要的**问责制**的结合。算法不负责任；临床医生负责任。人工智能是一个才华横溢但可能犯错的证人；临床医生必须是明智且负责的法官，权衡所有证据以得出结论。

这就引出了医患关系的基石：**知情同意**。为了让患者信任这个过程，这种信任必须是合理的。它不能建立在营销之上，也不能通过隐藏人工智能的缺陷来建立。它必须建立在透明度之上。这意味着要披露“理性患者”会认为对其决策至关重要的信息 [@problem_id:4867509]。这不仅仅是人工智能的总体准确性，还包括：
*   **亚组性能：** 针对像他们这样的患者，敏感性可能是 $0.78$，而不是 $0.92$。
*   **数据集偏见：** 人工智能的训练数据集对其所属的人口群体代表性不足。
*   **[不确定性的来源](@entry_id:164809)：** [图像质量](@entry_id:176544)差可能会导致性能下降。

这种披露水平并非一成不变；它必须适应人工智能在决策中的角色 [@problem_id:4442201]。如果人工智能仅仅是**辅助性**的，那么关于其角色和错误的基本信息可能就足够了。如果它变为**确认性**的，用于检查医生的工作，那么患者需要了解解决[分歧](@entry_id:193119)的协议。而如果人工智能变得**自主**，能自行做出决策，那么一类全新的信息就变得至关重要：患者**质疑**该决策的权利、保护系统免受攻击的安全措施，以及保持模型更新和安全的政策。

最终，所有这一切都建立在**问责制**的基石之上。当人工智能被整合到医疗服务中时，临床医生和机构的专业职责不会消失，反而会扩大。医院不能简单地购买供应商的“符合HIPAA标准”的产品，然后将其伦理义务[外包](@entry_id:262441)。它对其患者保留着一种**不可委托的信托责任**。这意味着它必须在其自己的患者群体中独立验证该工具，持续监测偏见，确保有意义的人工监督，并为患者的福祉承担最终责任 [@problem_id:4880669]。当伤害发生时，责任链是复杂的，并且可以被分担。供应商可能因未提供充分警告而承担责任，医院可能因疏忽地压制这些警告而承担责任，而医生则可能因过度依赖工具的输出而未行使独立的临床判断而承担责任 [@problem_id:4436682]。

因此，在皮肤病学中构建和部署公平的人工智能，不仅仅是一个技术问题。它是在伦理、科学和治理方面的一个深刻挑战。它要求我们量化不公，理解其在我们数据和系统中的根源，用因果和临床的严谨性来定义公平，并建立值得我们患者信任的透明度和问责制体系。人工智能的神谕已经降临，但它的智慧完全取决于我们用以构建、测试和治理它的人类智慧。

