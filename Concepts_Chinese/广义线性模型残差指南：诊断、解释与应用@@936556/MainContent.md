## 引言
虽然[广义线性模型](@entry_id:171019)（GLM）为模拟各种数据类型提供了灵活的框架，但只有当我们能够自信地评估其与现实的拟合程度时，其威力才能得以实现。进行此评估的主要工具是残差——即观测数据与模型预测之间的差异。然而，在GLM的世界里，解释残差远非易事。与标准[线性回归](@entry_id:142318)不同，GLM的原始残差会表现出一些模式，例如非恒定方差，这些是数据本身性质（如泊松分布或[二项分布](@entry_id:141181)）的体现，而非模型失效的迹象。这一根本性挑战推动了更复杂诊断工具的发展。

本文将引领您探索GLM残差的领域，从基本原理到高级应用。接下来的章节将探讨这些重要诊断工具的理论和实际应用。**原理与机制**部分将揭示各种残差的奥秘，从为控制方差和[偏度](@entry_id:178163)而设计的经典Pearson残差和偏差（deviance）残差，到提供稳健、直观诊断的现代基于模拟的方法。随后的**应用与跨学科联系**部分将展示这些统计工具在神经科学、生态学和公共卫生等领域如何成为侦探的放大镜，不仅用于检查模型，还用于构建更强大的理论和做出新发现。

## 原理与机制

在我们用广义线性模型（GLM）对世界进行建模的征程中，我们已经构建了一台强大的机器。我们现在可以将预测变量与那些不那么“规矩”的结果联系起来，比如感染计数或病人康复的[二元结果](@entry_id:173636)。但对于任何强大的机器，一个关键问题随之而来：我们如何知道它是否正常工作？我们如何诊断问题？在统计学中，我们主要的诊断工具是**残差**——即剩余部分、误差，也就是我们观测到的值与模型预测值之间的差异。探索GLM残差的世界不仅仅是一项技术练习；它讲述了一个美丽的故事，关于统计学家如何与数据的复杂性搏斗，发明出越来越巧妙的方法来追问：“我们的模型说的是实话吗？”

### 原始残差的麻烦

最简单的想法是计算**原始残差**，$e_i = y_i - \hat{\mu}_i$，其中$y_i$是我们的观测数据点，$\hat{\mu}_i$是我们拟合模型预测的均值。在熟悉的线性回归世界里，我们绘制这些残差，并期望看到一团无定形、在零附近随机散布且离散程度恒定的点云。这告诉我们，模型的假设很可能得到了满足。

那么，让我们在GLM中尝试一下。假设我们用泊松模型对一家医院每周的感染计数进行建模。我们绘制原始残差对拟合值的散点图。我们看到的不是随机的点云，而是一个明显的漏斗形状：随着预测计数值的增大，残差的离散程度也随之变宽。[@problem_id:4894661] 我们失败了吗？我们的模型错了吗？

完全没有！我们只是重新发现了一个关于数据的基本事实。对于泊松分布，方差等于均值。这不是我们做出的假设，而是这个过程的本质。因此，在均值计数较高的地方，随机波动（以及因此产生的残差）较大是*意料之中*的。类似地，对于逻辑斯蒂回归的二[元数据](@entry_id:275500)，绘制原始残差会得到两条奇怪的曲线，这是结果只能是0或1的直接后果。这里的教训是深刻的：在GLM中，原始残差反映的是响应变量的内在性质，而不一定是模型的缺陷。直接观察它们具有误导性。我们需要一个更好的透镜。

### 控制方差：Pearson残差

如果问题在于方差不恒定，那么解决方法异常简单：让我们把它除掉！我们可以用每个原始残差的标准差估计值来对其进行缩放。这就产生了**Pearson残差**，以伟大的统计学家 Karl Pearson 的名字命名。其定义为：

$$
r_{Pi} = \frac{y_i - \hat{\mu}_i}{\sqrt{V(\hat{\mu}_i)}}
$$

这里，$V(\hat{\mu}_i)$ 是我们所选分布的**方差函数**，在拟合均值处求值。对于我们的泊松模型，$V(\mu) = \mu$，所以我们除以 $\sqrt{\hat{\mu}_i}$。对于有 $m_i$ 次试验的[二项模型](@entry_id:275034)，$V(\mu) = \mu(1 - \mu/m_i)$，依此类推。[@problem_id:4949137]

通过这样做，我们创造了一个新的量，如果我们的模型关于均值-方差关系的假设是正确的，那么这个量的方差*应该*是恒定的。Pearson残差对拟合值的散点图现在应该看起来像我们所期望的随机、无定形的点云。如果确实如此，我们就更有信心认为我们选择了正确的分布族（例如，泊松分布）。如果不是——如果仍然存在某种模式——那就告诉我们，我们的假设中存在更深层次的问题。[@problem_id:4894661]

此外，Pearson残差的平方和 $\sum r_{Pi}^2$ 为我们提供了一个有价值的统计量。对于像泊松分布这样方差在理论上与均值锁定的模型，这个和应该接近于数据点数量减去我们估计的参数数量。如果它大得多，则表明存在**过度离散**——我们的数据比模型所允许的变异性更大。例如，发现[离散度](@entry_id:168823)估计值为 $\hat{\phi}_{P}=1.6$ 意味着真实方差大约是均值的1.6倍，这是做出正确推断的关键洞见。[@problem_id:4894661]

### 更深入的审视：偏差（Deviance）与似然的语言

Pearson残差源于稳定方差的愿望。但还有另一种更根本的思考[模型误差](@entry_id:175815)的方式，它根植于**似然**的概念。我们可以问：我们的模型对每个数据点有多“惊讶”？在现代统计学中，表达“惊讶”的语言是[对数似然](@entry_id:273783)。一个“完美”模型（即直接穿过数据点 $y_i$ 的模型）的[对数似然](@entry_id:273783)与我们拟合模型的[对数似然](@entry_id:273783)之间的巨大差异，表明该模型对那个数据点的拟合很差。

这个想法催生了**偏差（deviance）残差**。对于每个观测值，我们计算它对模型总偏差的贡献——这是一个基于似然的整体模型失拟度量。[偏差残差](@entry_id:635876)就是这个贡献值的带符号平方根：

$$
r_{Di} = \operatorname{sign}(y_i - \hat{\mu}_i) \sqrt{2 \left[ \ell(y_i; y_i) - \ell(y_i; \hat{\mu}_i) \right]}
$$

其中 $\ell(y; \mu)$ 是[对数似然函数](@entry_id:168593)。[@problem_id:4949137] 这个残差度量了在似然尺度上的不一致程度，而 `sign` 函数则告诉我们误差的方向，就像原始残差一样。对于许多GLM，[偏差残差](@entry_id:635876)的分布比Pearson残差更接近对称的正态分布，这使它们在诊断图中发现异常值和检查整体[模型拟合](@entry_id:265652)时特别有用。[@problem_id:4894175]

### 追求正态性：从偏度到Anscombe的技巧

我们现在有了两种复杂的工具，Pearson残差和[偏差残差](@entry_id:635876)，它们都考虑了均值-方差关系。我们可能希望它们的行为能像线性回归中那些良好的、服从正态分布的残差一样。但通常情况下，并非如此。一张比较残差[分位数](@entry_id:178417)与正态分布分位数的[Q-Q图](@entry_id:174944)，可能仍然显示出一条弯曲的、不理想的模式。

原因在于我们还没有摆脱数据本身的基本性质。一个均值较低的泊松分布天生就是[右偏](@entry_id:180351)的。一个概率接近0或1的二项分布也同样高度偏斜。我们的残差，无论多么巧妙，都继承了这种[偏度](@entry_id:178163)。[@problem_id:4894175] 这种非正态性的根源深藏于[指数族](@entry_id:263444)的数学结构之中，与[累积量](@entry_id:152982)函数 $b(\theta)$ 的三阶和四阶导数有关，对于大多数分布来说，这些导数是非零的。[@problem_id:4840935]

我们能做得更好吗？我们能否找到一种数学变换，不仅能稳定方差，还能使分布更加对称？这一追求催生了巧妙的**Anscombe残差**。Francis Anscombe发现，通过对数据应用一个精心选择的变换 $A(\cdot)$，可以显著减少残差的[偏度](@entry_id:178163)。其一般原理是统计理论中的一颗瑰宝：该变换应满足 $A'(\mu) \propto V(\mu)^{-1/3}$。[@problem_id:4914509]

对于泊松数据，这导致了一个与 $y^{2/3}$ 相关的变换。对于二项数据，这导致了一个与反正弦[平方根函数](@entry_id:184630)相关的变换。[@problem_id:3871126] [@problem_id:4914509] 虽然公式看起来可能令人生畏，但其思想是直观的：我们正在扭曲我们测量结果的尺度，以使最终的误差尽可能对称和接近正态。在计数较低或概率接近边界的情况下，Anscombe残差通常比它们的Pearson或偏差对应物提供更清晰的[模型拟合](@entry_id:265652)图像。

### 最后的润色：标准化与渐近的魔力

我们的机器里还潜藏着最后一个微妙的幽灵。从数据中拟合模型和估计参数这一行为本身，会影响残差的性质。事实证明，给定点的残差方差取决于其**杠杆值**，杠杆值是衡量该点在决定[模型拟合](@entry_id:265652)中的影响力的指标。一个[高杠杆点](@entry_id:167038)（例如，具有非常异常预测变量值的点）会把回归线拉向自己，导致其残差方差变小。

为了将所有残差置于一个真正平等的地位，我们必须进行最后一次标准化，即除以其真实标准差的估计值，该估计值包含杠杆项 $h_{ii}$：

$$
r_i^{\text{std}} = \frac{r_i}{\sqrt{\phi(1-h_{ii})}}
$$

这些被称为**[标准化残差](@entry_id:634169)**。在这里，我们见证了统计理论的一个小奇迹。经过所有这些工作——为方差函数进行缩放，或许为偏度进行变换，现在又为[杠杆值](@entry_id:172567)进行调整——对于大样本而言，这些[标准化残差](@entry_id:634169)的表现就如同它们是从一个标准正态分布 $N(0,1)$ 中抽取的一样！[@problem_id:4980521] 这个深刻的结果根植于中心极限定理和[最大似然估计](@entry_id:142509)的性质，它最终证明了我们使用熟悉的工具（如正态[Q-Q图](@entry_id:174944)）来诊断我们复杂模型的合理性。这些残差并非完全独立——它们通过共享的估计参数而略有相关——但对于大多数实际应用而言，这套机制使我们能够最终应用我们关于随机正态误差的直觉。[@problem_id:4958315]

### 一场现代革命：模拟来救场

我们所遵循的路径是一条数学复杂性不断增加的道路。但是，如果我们能用一种更直观、基于计算的方法达到同样的目标呢？这就是现代基于模拟的残差背后的思想，例如在**DHARMa**包中实现的那些。

其逻辑异常简单而强大。[@problem_id:4982818] 对于我们的每一个实际观测值 $y_i$，我们向我们拟合的模型提出一个问题：“给定这个观测值的协变量，你期望看到什么样的结果？” 然后，我们让模型从这个预测的分布中模拟，比如说，1000个新的数据点。我们正在根据我们的模型创造一个充满可能结果的完整世界。

现在，我们只需看看我们的*实际*观测值 $y_i$ 在这个模拟世界中的位置。我们计算它的秩次。如果模型是正确的，那么真实数据应该看起来就像一个典型的模拟数据点。它的秩次应该是随机的。如果我们将这个秩次缩放到0和1之间，我们就得到了一个在正确模型下必须服从均匀分布Uniform(0,1)的残差。

这种方法优雅地回避了我们早期努力中遇到的所有离散性和偏度问题。通过将观测值与其模拟同伴的群体进行比较，我们在构造上就创造了一个完全平滑、均匀的残差。这些均匀残差可以很容易地被转换成完美的标准正态残差用于绘图。这种方法证明了现代计算能力如何能够为经典的统计挑战提供直观而稳健的解决方案。[@problem_id:4982818] [@problem_id:4840935]

### 作为发现工具的残差：偏[残差图](@entry_id:169585)

最后，至关重要的是要记住，残差不仅仅用于检查假设；它们是发现的工具。假设我们的模型包含几个预测变量，我们想在考虑了所有其他变量之后，分离并可视化其中一个变量（比如 $X_1$）的影响。这就是**偏[残差图](@entry_id:169585)**的工作。

在GLM中，这需要小心处理非线性连接函数。我们不能简单地在原始响应 $y_i$ 的尺度上工作。相反，我们必须在线性预测变量 $\eta$ 的尺度上工作。关键是使用**工作残差**，这是GLM拟合算法中自然产生的一个量。[@problem_id:4949137] 通过将我们感兴趣的预测变量的拟合线性效应 $\hat{\beta}_1 X_{i1}$ 添加到这个工作残差中，我们创造了一个偏残差，它在线性化尺度上分离了 $X_1$ 与结果之间的关系。将这个偏残差对 $X_1$ 绘图，可以揭示这种关系是否真的是线性的，或者是否存在我们的模型所遗漏的更复杂的曲线关系。它将残差从一个单纯的误差项转变为指向一个更好、更真实模型的路标。[@problem_id:4937017]

