## 引言
在模拟我们周围世界的探索中，科学传统上遵循两条截然不同的路径：一条是数据驱动的机器学习方法，它擅长在海量数据集中发现模式；另一条是原理驱动的物理模拟方法，它利用了数百年来建立的自然法则。这两种方法虽然强大，但各有其局限性。纯粹的数据驱动模型通常是“黑箱”，需要大量数据，并且无法保证物理上的一致性。相反，传统的[物理模拟](@entry_id:144318)器可能很死板、计算成本高昂，并且难以处理[不适定问题](@entry_id:182873)或知识不完备的情况。物理信息机器学习（PIML）作为一种革命性的[范式](@entry_id:161181)应运而生，它统一了这两个领域，创造出既受数据启发又受自然基本法则约束的模型。

本文探讨了人工智能与第一性原理科学的这种强大融合。它解决了数据显示的内容与物理定律要求之间的关键知识鸿沟，为科学探究和工程设计提供了一类新工具。首先，在“原理与机制”一章中，我们将深入探讨 PIML 的核心机制，探索如何通过基于残差的损失函数和[自动微分](@entry_id:144512)等概念，将物理定律转化为[神经网](@entry_id:276355)络可以理解的语言。随后，“应用与跨学科联系”一章将展示这种方法的卓越通用性，说明它如何解决复杂的[正问题](@entry_id:749532)和反问题，加速从[流体动力学](@entry_id:136788)到高能物理等领域的发现，甚至与传统计算方法协同工作。

## 原理与机制

要真正领会物理信息机器学习的独创性，我们必须超越表面，探索其运作的内在机制。其核心是一个极其简单却又深刻的想法：如果我们能教机器不仅模仿数据，还能理解支配物理世界的规则，那会怎样？想象一下，一个[神经网](@entry_id:276355)络不仅仅是一只学舌的鹦鹉，而是一位见习物理学家，学习从第一性原理进行推理。本章将带您踏上一段旅程，探索实现这一切的核心原理，从导数的语言到嵌入[基本对称性](@entry_id:161256)的艺术。

### 学习游戏规则：将物理定律作为[损失函数](@entry_id:634569)

传统的[神经网](@entry_id:276355)络学习方式，很像我们通过观看数千个视频来学习投篮。我们看到一个轨迹（数据），然后调整我们的投篮动作，直到与之匹配。网络被赋予一组输入-输出对——比如，时间点和下落苹果的相应位置 $(t_i, y_i)$——然后它会调整其内部参数，即“权重” $\omega$，以最小化其预测 $y_\omega(t_i)$ 与实际数据 $y_i$ 之间的误差。这个误差由一个**[损失函数](@entry_id:634569)**来量化，通常是平[方差](@entry_id:200758)之和：$\mathcal{L}_{\text{data}} = \sum (y_\omega(t_i) - y_i)^2$。网络对于“为什么”是盲目的；它只知道“是什么”。

物理信息神经网络（PINN）则向前迈出了一大步。它要求：“你不仅必须匹配观测到的数据，你的预测还必须在所有时间和所有地点都遵守物理定律。”我们如何将像[牛顿万有引力定律](@entry_id:170220)这样的物理法则传达给网络呢？我们将该法则转化为一种可以添加到损失函数中的数学形式。

对于我们下落的苹果，其支配定律是一个[微分方程](@entry_id:264184)：位置对时间的[二阶导数](@entry_id:144508)是常数，$y''(t) = -g$。我们可以重新[排列](@entry_id:136432)这个方程，形成一个**残差**，即一个在定律被遵守时应等于零的表达式：$\mathcal{R}(t) = y''(t) + g$。现在，我们可以创建一个新的损失项，即**物理损失**，通过要求这个残差在任何地方都为零。在实践中，我们在整个域中采样的大量点（称为**[配置点](@entry_id:169000)**）上强制执行此条件：

$$
\mathcal{L}_{\text{phys}} = \frac{1}{N_{\text{coll}}} \sum_{j=1}^{N_{\text{coll}}} \left( y''_\omega(t_j) + g \right)^2
$$

PINN 的总[损失函数](@entry_id:634569)变成了数据损失和物理损失的加权和：

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \mathcal{L}_{\text{phys}}
$$

其中 $\lambda$ 是一个权重，用于平衡我们对数据的信任与对物理定律的坚持。通过最小化这个组合损失，网络被迫进行微妙的权衡。它必须找到一个函数，不仅能穿过观测到的数据点，而且在其他任何地方都符合潜在的物理定律。这一个思想是 PINN 的基石。它允许我们通过惩罚[法拉第定律](@entry_id:149836)和安培定律的残差来嵌入无源[麦克斯韦方程组](@entry_id:150940)，或者通过惩罚应力[张量的散度](@entry_id:191736)来模拟固体中力的复杂平衡 [@problem_id:3327836] [@problem_id:2668902]。

### 变革的语言：[自动微分](@entry_id:144512)

一个关键问题立刻出现。我们的[神经网](@entry_id:276355)络 $y_\omega(t)$ 是一个复杂的、深度嵌套的函数。我们究竟如何计算它的导数，如 $y''_\omega(t)$，来评估物理残差呢？

有人可能会尝试使用**有限差分**来近似导数，例如，$y'(t) \approx \frac{y(t+h) - y(t-h)}{2h}$。但这会引入**截断误差**；它是一种近似。我们正试图教网络*精确的*物理定律，所以使用近似导数感觉像是对我们核心原则的背叛。

解决方案是[现代机器学习](@entry_id:637169)核心中一种优美而强大的技术：**[自动微分](@entry_id:144512)（AD）**。AD 不是[符号微分](@entry_id:177213)（可能导致表达式冗长笨重），也不是[数值微分](@entry_id:144452)（不精确）。AD 是一种计算由计算机程序表示的函数的*精确*导数的方法。

把[神经网](@entry_id:276355)络想象成一个由简单的乐高积木——加法、乘法和简单的[非线性](@entry_id:637147)**激活函数**如 $\tanh(z)$ 或 $\sin(z)$——搭建起来的复杂结构。我们知道这些基本模块中每一个的导数。微积分的链式法则精确地告诉我们如何组合较小函数的导数以获得它们复合函数的导数。AD 只是[链式法则](@entry_id:190743)在程序上的应用，一步一步地贯穿网络的整个[计算图](@entry_id:636350)。它解开了复杂的嵌套函数，并通过一丝不苟地应用链式法则，提供网络输出相对于其输入的解析导数，其精度可达机器精度的极限 [@problem_id:3513273] [@problem_id:3337936]。

这就是神奇的配方。AD 允许我们计算像 $\nabla \times \mathbf{E}_\theta$ 或 $u_{\theta,xx}$ 这样的项，而无需引入任何[离散化误差](@entry_id:748522)，从而使 PINN 能够为其当前的近似解评估物理定律的真实残差。虽然 AD 在算法上是精确的，但它并不能免于[有限精度算术](@entry_id:142321)的挑战。对于更高阶的导数或非常深的网络，舍入误差的累积或激活函数的问题仍然可能带来数值挑战 [@problem_id:3337936]。

### [归纳偏置](@entry_id:137419)的力量：硬约束与软约束

上面描述的惩罚方法是一种“软”约束。它鼓励但不强制网络遵守物理定律。一种不同且通常更强大的方法是“通过构造”来强制执行物理定律——即构建一个模型，使其在设计上就满足某些法则。这就是**[归纳偏置](@entry_id:137419)**的概念：将我们的先验知识直接构建到学习机器的结构中。

想象一下我们正在研究一个我们已知呈指数增长的[生物种群](@entry_id:200266)，遵循定律 $g'(x) = \alpha g(x)$。解的形式必须是 $g(x) = C e^{\alpha x}$。
- 一种通用方法可能会使用多项式，比如 $h(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$，来拟[合数](@entry_id:263553)据。这个模型有4个自由参数，并且没有内置关于指数增长的知识。
- “软”PINN 会使用一个通用的[神经网](@entry_id:276355)络，并在其[损失函数](@entry_id:634569)中增加一个惩罚项 $\lambda \int (h'(x) - \alpha h(x))^2 dx$。
- “硬”约[束方法](@entry_id:636307)会将模型构建为 $h(x) = C e^{\alpha x}$，其中[神经网](@entry_id:276355)络的唯一工作就是学习单个常数 $C$。

如果我们的物理知识是正确的，硬约束模型要简单得多（它的“容量”要低得多）。它需要的数据少得多才能训练，并且更有可能正确地泛化和外推，因为它在一个更小、物理上合理的[函数空间](@entry_id:143478)中搜索解 [@problem_id:3130045]。

这种“通过构造”的理念在施加各种物理真理方面具有令人难以置信的通用性：

*   **正定性和守恒性：** 像浓度或概率这样的物理量不能是负数。标准[神经网](@entry_id:276355)络的输出可以是任何值。为了强制正定性，我们可以将网络的原始输出 $N(t)$ 通过一个只返回非负值的函数，例如指数映射 $c(t) = \exp(N(t))$，或 softplus 函数 $c(t) = \ln(1 + \exp(N(t)))$。类似地，为了强制一组概率 $\{p_i\}$ 的和为1，我们可以使用 softmax 函数。这些重参数化保证了无论底层网络学到什么，这些约束都会被满足 [@problem_id:3337944]。

*   **对称性与不变性：** [连续介质力学](@entry_id:155125)中的一个基本原理是**材料框架无关性**：材料的储存能 $W$ 不应因物体刚性旋转而改变。这意味着对于任何旋转 $\mathbf{Q}$ 和[形变梯度](@entry_id:163749) $\mathbf{F}$，都有 $W(\mathbf{Q}\mathbf{F}) = W(\mathbf{F})$。一个以 $\mathbf{F}$ 的九个分量作为输入的朴素[神经网](@entry_id:276355)络几乎肯定不会满足这一点。我们可以通过添加一个衡量违规程度的惩罚项来“软”强制它 [@problem_id:3440130]。或者，我们可以“硬”强制它，方法是构造网络，使其只接受本身对旋转不变的输入，例如[右柯西-格林张量](@entry_id:174156) $\mathbf{C} = \mathbf{F}^\mathsf{T}\mathbf{F}$ 的[不变量](@entry_id:148850)。这将对称性硬编码到模型的架构中 [@problem_id:3440130]。

### 当逐点计算毫无意义时：弱形式

标准的 PINN 方法，即在离散点上惩罚残差，被称为**强形式**强制。当我们的[偏微分方程](@entry_id:141332)的解是光滑的时候，它工作得非常好。但大自然并不总是那么友好。许多物理现象涉及突变，如[超音速流](@entry_id:262511)中的激波或材料裂纹尖端的[奇异点](@entry_id:199525)。

在激波或[奇异点](@entry_id:199525)处，解的导数可能变为无穷大。标准的 PINN 作为一个无限光滑的函数，无法指望表示这样的特征。试图在这样的点上评估 PDE 残差是无意义的——真实的残差是无穷大！一个在这种问题上训练的朴素 PINN 通常会惨败。它可能会学到一个完全错误的、碰巧损失很低的光滑解，因为均匀的[配置点](@entry_id:169000)错过了那个微小而有问题的区域。或者，如果一些点确实落在了[奇异点](@entry_id:199525)附近，它们巨大的残差值会主导损失函数，导致训练不稳定和在其他所有地方性能不佳 [@problem_id:2411081] [@problem_id:2668902]。

补救措施来自数学和物理学中一个经典而深刻的思想：**[弱形式](@entry_id:142897)**。我们不再要求 PDE 在每一点上都成立，而是要求它在“平均”意义上成立。我们将 PDE 残差乘以一组光滑的“测试函数” $\phi$，并要求该乘积在域上的积分为零：

$$
\int_\Omega \left(\mathcal{N}[u] - f \right) \phi \, dV = 0 \quad \text{for all suitable } \phi
$$

真正的魔力发生在我们应用**[分部积分](@entry_id:136350)**时。这项技术允许我们将导数从解 $u$（可能不光滑）转移到测试函数 $\phi$（我们选择其为光滑的）上。对于像[泊松方程](@entry_id:143763)这样的二阶 PDE，这意味着[弱形式](@entry_id:142897)只包含 $u$ 的一阶导数。这降低了对我们解的正则性要求，使得该形式适用于有扭结、拐角甚至某些[奇异点](@entry_id:199525)的问题 [@problem_id:2668902] [@problem_id:3513303]。

[弱形式](@entry_id:142897) PINN（通常称为变分 PINN 或 vPINN）在这些积分残差上构建其损失函数。这种方法有几个优点：
1.  **处理低正则性问题：** 通过降低所需导数的阶数，它可以解决强形式失败的、带有激波和[奇异点](@entry_id:199525)的问题 [@problem_id:2411081] [@problem_id:2668902]。
2.  **对噪声的鲁棒性：** 积分是一种平滑操作。它能平均掉局部误差，因此与强形式的逐点求值相比，它对数据或源项中的高频噪声不那么敏感 [@problem_id:3513303]。
3.  **自然处理边界条件：** 分部积分自然会产生边界项，为包含通量条件（[诺伊曼边界条件](@entry_id:142124)）提供了一种优雅的方式。

其代价是计算成本。对于强形式工作良好的光滑问题，计算[弱形式](@entry_id:142897)所需的许多积分可能比简单的逐点配置更昂贵 [@problem_id:2668902]。

### 从求解器到科学家：利用残差进行发现

到目前为止，我们一直将 PINN 视为求解已知方程的工具。但也许它们最激动人心的应用是帮助我们*发现*这些方程。考虑一个“灰箱”建模场景，我们对物理学有部分理解。例如，我们可能知道一个系统受扩散控制，但我们不确定[平流](@entry_id:270026)（输运）是否也起作用。

我们可以基于我们目前最好的猜测——纯扩散方程 $u_t = \nu u_{xx}$——来构建一个 PINN。我们用可用的 $u(x,t)$ 数据来训练这个网络。训练后，我们可以检查模型的失败之处：**残差** $r(x,t) = u_t - \nu u_{xx}$。如果我们纯粹的[扩散模型](@entry_id:142185)是完美的，这个残差将不过是随机噪声。

然而，如果真实物理中存在一个缺失的平流项 $-c u_x$，那么我们的残差就不会是随机的。它会与我们遗漏的[平流](@entry_id:270026)项高度相关：$r(x,t) \approx -c u_x$。我们可以统计地检验这个假设。通过对计算出的残差与候选物理项（如 $u_x$）进行[回归分析](@entry_id:165476)，我们可以检测到缺失物理的特征。强相关性就是一个确凿的证据，精确地告诉我们模型所缺乏的东西 [@problem_id:3410549]。

这将 PINN 从一个单纯的方程求解器转变为一个科学发现引擎。模型的误差不再仅仅是误差；它们是指导我们走向更完整物理理论的线索。这种机器学习与[科学方法](@entry_id:143231)的融合——数据驱动的推断与有原则的物理推理的结合——代表了这个新兴领域核心的美丽而统一的愿景。

