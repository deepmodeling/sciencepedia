## 引言
随着人工智能成为金融、医学和法律领域关键决策的仲裁者，一个深刻的挑战也随之出现：一个算法即使满足所有统计上的公平性基准，却仍可能固化根深蒂固的社会不公。通过从历史数据中学习，人工智能系统有可能成为一个充满偏见的世界的镜子，将歧视误认为命运。这提出了一个关键问题：我们如何才能构建不仅预测准确，而且在伦理上健全的模型？答案在于超越简单的相关性，并采用严谨的因果语言。传统的[公平性指标](@entry_id:634499)仅限于观察数据中的模式，但因果推断提供了一个强大的框架来提问“如果……会怎样？”——从而理解这些模式*为什么*存在，并设计能够主动对抗不公平机制的系统。本文将引领读者探索这一变革性领域。我们将首先探讨因果公平性的基本“原理与机制”，剖析反事实和路径特定干预等概念。随后，“应用与跨学科联系”部分将展示这些理论如何被用于审计、重新设计和革新现实世界的系统，为由人工智能驱动的更公平的未来铺平道路。

## 原理与机制

想象一下你是一名法官。一名被告站在你面前，你必须根据一个预测其再犯风险的算法来决定他的命运。该算法基于海量历史数据训练而成，准确率惊人。你拿到一份报告，报告显示，对于任何给定的风险评分，实际再犯的人口比例是相同的，无论其种族如何。此外，该算法的错误率——包括[假阳性](@entry_id:635878)和假阴性——在所有种族群体中都完全平衡。按照标准的统计公平性衡量标准，这个算法是完美的。

然而，你有一种挥之不去的感觉。如果产生这些数据的社会本身就是不公正的呢？如果某个群体受到更严密的监控，导致因轻微违法行为而被捕的次数更多，从而抬高了他们的风险评分呢？该算法在其统计上的完美性中，只会学习并固化这种不公，将歧视的历史误认为犯罪的倾向。它准确地预测了在一个有偏见的世界里*将要*发生什么，但它没有告诉你一个公正的世界里*应该*发生什么。

要打破这个循环，我们必须超越“是什么？”——这个关于相关性的问题——并开始提问“如果……会怎样？”——这个关于因果关系的问题。这正是因果推断的核心，其在公平性上的应用正在改变我们对算法公正含义的理解。

### 超越表象：相关性的欺骗性魅力

几十年来，[算法公平性](@entry_id:143652)一直由统计性或“观察性”标准主导。这些指标，如[人口均等](@entry_id:635293)（demographic parity，即相等的预测率）或[均等化赔率](@entry_id:637744)（equalized odds，即相等的错误率），检查观察数据中的模式。它们对数据进行切分和剖析，比较不同群体间的比率，但它们从不探究这些模式*为什么*存在。这正是它们的根本局限性。

考虑一个鲜明而假设性的临床场景。一个人工智能模型被构建用于预测死亡率，其预测完全准确——它只是复制了真实结果。假设在历史数据中，一个弱势群体 ($A=1$) 比一个特权群体 ($A=0$) 更难获得一种救命疗法，而这种机会的缺乏是该群体死亡率较高的唯一原因。由于人工智能的预测与真实结果完全相同，它将满足像[均等化赔率](@entry_id:637744)这样强大的观察性公平性标准。毕竟，在给定真实结果（例如，患者存活）的条件下，无论群体成员身份如何，预测都是确定的。然而，这个系统是极度不公平的，而模型通过“准确”地预测，将这种社会不公“洗白”成看似客观的风险预测。然而，一次因果审计会揭示，为弱势群体改善治疗机会的干预措施可以拯救生命，从而暴露出观察性指标所忽略的伤害。[@problem_id:4408342]

通过一个简单的思想实验，这种脱节可以变得更加生动。想象一个算法的决策 $\hat{Y}$ 基于一个代理变量 $Z$，而 $Z$ 只是患者的群体身份 $A$ 与一个随机抛硬币结果 $N$ 的混合。具体来说，决策是这两者的[异或](@entry_id:172120)：$\hat{Y} = A \oplus N$。如果硬币是公平的，算法标记某人的概率恰好是 $50\%$，无论他们属于哪个群体。该系统实现了完美的统计均等。但从因果角度来看，它却是最大程度不公平的。对于任何给定的个体（由其特定的抛硬币结果 $N$ 定义），将其群体身份 $A$ 从 $0$ 变为 $1$ *总是*会翻转算法的决策。统计公平性没有看到任何偏见，而因果公平性看到的只有偏见。[@problem_id:5192808]

这些例子揭示了一个深刻的真理：一个模型可以满足我们直观的统计公平性检查，同时参与并固化一个根本上不公正的因果机制。要看清这种不公，我们需要戴上一副新眼镜——因果关系的眼镜。

### 本可能的世界：反事实的力量

因果语言是围绕着讲述世界如何运作的故事构建的。在**结构因果模型 (Structural Causal Models, SCMs)** 的框架中，我们写下一组“结构方程”，每个方程描述我们世界中的一个变量如何获得其值。可以把它想象成一个食谱。最终的菜肴（一个结果，如健康诊断 $Y$）取决于一系列食材（其直接原因，或称“父节点”）。例如，患者的血压 ($M$) 可能由其年龄 ($Z$)、医疗保健的可及性 ($H$) 以及一些固有的生物随机性所引起。我们可以将这个故事写成一个方程：$M := f(Z, H, U_M)$，其中 $U_M$ 代表所有未测量的、随机的因素——“命运的骰子”——这些因素影响着血压。

这组方程，连同外生“骰子” ($U$) 的分布，定义了一个完整的因果宇宙。它不仅告诉我们现实世界中发生了什么，还允许我们推断在一个不同的、反事实的世界里*本可能发生*什么。这是通过听起来很神奇的**_do_-算子**实现的。当我们写下 $do(A=a)$ 时，我们不仅仅是在观察一个来自群体 $a$ 的患者；我们是在想象一个世界，在这个世界里，通过一次强大的干预，我们将该个体的属性设置为 $a$，切断了所有先前指向 $A$ 的因果箭头，然后观察下游会发生什么。[@problem_id:4408229]

这引出了因果公平性最严格的定义：**[反事实公平性](@entry_id:636788)**。一个决策是反事实公平的，如果对于任何个体，在其他一切条件相同的情况下，如果其受保护属性不同，结果也会相同。[@problem_id:4408229]“其他一切条件相同”这句话在这里有精确的含义：它意味着保持构成个体独特背景和体质的所有外生变量 $U$ 的具体实现不变。在我们的基于故事的SCM中，这意味着我们问：对于一个由一组特定的“骰子” $U=u$ 定义的人，如果我们干预并将 $A \leftarrow 0$ 与 $A \leftarrow 1$ 进行对比，决策 $\hat{Y}$ 是否会改变？如果对于每个人（每个可能的 $u$），答案都是否定的，那么这个系统就是反事实公平的。

这是一个深刻而严苛的标准。它坚持认为，受保护属性——我们必须记住，它是一个**受保护属性**，并非因为任何固有的统计特性，而是因为我们作为一个社会，由于其在历史上的边缘化经历，做出了一个规范性决定，以保护它免受不当影响[@problem_id:4420300]——对最终决策的因果影响应为零。

### 解开命运的纠葛：公平与不公平的因果路径

完全的[反事实公平性](@entry_id:636788)总是我们想要的吗？考虑一下受保护属性（如性别）与心脏病风险之间的复杂关系。性别可以通过生物路径（例如，激素水平、染色体相关的遗传因素）影响风险，也可以通过社会路径（例如，历史上被排除在临床试验之外、诊断中的性别偏见）影响风险。一个纯粹的反事实公平模型，由于要求性别对预测的因果效应为零，将被迫忽略真正的生物学差异，这可能会损害患者。

这揭示了真正的伦理问题通常不在于受保护属性*是否*影响预测，而在于*如何*影响。因果图使我们能够明确区分这些。我们可以将从受保护属性 $A$ 到结果 $Y$ 的某些因果路径标记为“公平”或“可接受的”（例如，生物路径），而将其他路径标记为“不公平”或“不可接受的”（例如，那些由社会经济地位或医疗保健可及性介导的路径）[@problem_id:4862561] [@problem_id:4595801]。

这引出了一个更细致、更实用的概念：**路径特定公平性**。目标不再是消除 $A$ 的总因果效应，而是选择性地消除那些沿着不公平路径传播的效应。想象一个因果图，其中种族 ($A$) 通过两条路径影响健康结果 ($Y$)：一条是通过社会经济变量 ($X$) 和医疗保健可及性 ($H$) 的不公平路径，另一条是通过临床中介变量 ($M$) 如血压的路径。我们的目标是构建一个对 $M \to Y$ 捕捉到的临床风险敏感的预测器，但该预测器所依据的 $M$ 值已经从流经 $X$ 和 $H$ 的 $A$ 的不公平影响中被“清除”了[@problem_id:5189823]。我们想知道，在一个没有社会经济差异的世界里，这位患者的血压*本应是*多少，并基于那个反事实的值进行预测。

这种强大的重构将辩论从简单的变量包含/排除，转向了对不公机制的更深层次的参与。挑战变成了“因果手术”：如何设计一个算法，使其“看待”世界的方式不是其现状，而是其应然的状态，通过计算手段剪断那些不公正的因果线索。这可以通过各种先进方法来尝试，例如在反事实调整过的数据上进行训练，或使用对抗性技术来惩罚模型从不允许的路径中学习信息。[@problem_id:4595801]

### 视野的局限：识别性的严峻挑战

这种关于公平的因果愿景是美好且引人注目的。但有一个巨大的难题：要执行这种因果手术，我们必须首先拥有正确的“患者地图”。也就是说，我们必须知道真正的结构因果模型。而在现实世界中，我们几乎永远无法做到这一点。我们只有观察性数据，而这些数据往往是潜在因果现实的一个苍白、扭曲的影子。

这就是**可识别性**（identifiability）问题。从一个观察到的相关性数据集中，通常不可能唯一地确定生成它的[因果结构](@entry_id:159914)。例如，一个特征 $X$ 和一个受保护属性 $A$ 之间的观察到的相关性，可能是由于一条直接的因果路径 $A \to X$（一个潜在的偏见来源）。或者，它可能是由于一个未观察到的潜在混杂因素，比如基线疾病严重程度 $U$，它同时影响两者（$A \leftarrow U \to X$）。这两个故事对于[反事实公平性](@entry_id:636788)有不同的含义，但它们可以在我们看到的数据中产生完全相同的模式。[@problem_id:4562330] 如果无法区分它们，我们就无法确定地证明一个系统是反事实公平的。

事实上，要从观察性数据中可靠地发现真实的因果图并量化反事实，需要做出一长串强有力的、通常是“英雄般”的假设[@problem_id:4426592]：
- **因果充分性：** 我们已经测量了模型中所有变量对的所有[共同原因](@entry_id:266381)（即没有重要的未测量混杂因素）。
- **忠实性：** 我们在数据中发现的所有[条件独立性](@entry_id:262650)都源于[因果结构](@entry_id:159914)，而不是由于参数的奇迹般抵消。
- **一致性：** 我们观察到的接受了某项治疗的个体的结果，与我们干预给予他们该治疗时会看到的[潜在结果](@entry_id:753644)是相同的。
- **无选择偏倚或测量误差：** 我们拥有的数据是目标群体的公正代表，并且测量是完美的。

在像电子健康记录 (EHR) 这样混乱的真实世界数据背景下，这一整套假设几乎肯定是不成立的。EHR数据充满了未测量的社会因素、有偏见的测试和记录实践，以及不断变化的临床方案[@problem_id:4426592] [@problem_id:4562380]。

这并不意味着对公平的因果追求是徒劳的。它意味着我们必须谦虚和诚实。与其声称找到了唯一的“公平”答案，一个更负责任的方法是探索各种可能性。我们可以使用因果发现算法来找到所有与我们的数据和一组更合理、更弱的假设相一致的因果图。通过在所有这些可能性上评估我们的[公平性指标](@entry_id:634499)，我们可以计算出不公平性的*边界*，从而诚实地报告我们的不确定性[@problem_id:4426592]。

因此，进入因果公平性的旅程，也是一次深入理解正义本身的旅程。它迫使我们直面造成不平等的机制，精确地阐述我们的伦理承诺，并对我们能从数据中了解到的知识的局限性保持理智上的诚实。它教导我们，构建一个公平的人工智能不仅仅是一个纯粹的技术挑战；它是一项深刻的科学和道德事业。

