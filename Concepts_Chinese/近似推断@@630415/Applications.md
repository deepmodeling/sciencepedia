## 应用与跨学科联系

在遍历了近似推断的基本原理之后，我们可能会感到一种满足感，就像一个刚刚掌握了地图和指南针的登山者。但地图并非疆域。真正的冒险始于我们踏入纷繁复杂的现实问题世界。这些优雅的数学机器究竟在何处发挥作用？事实证明，无处不在。

近似推断，特别是我们已经探讨过的[变分方法](@entry_id:163656)，其美妙之处不仅在于当精确解遥不可及时它能给出一个答案。更在于它提供了一个*在不确定性下进行推理的原则性框架*。它将我们的模型从吐出单一“最佳”答案的僵化、确定性机器，转变为能够提供一整片可能性景观的灵活、好奇的系统，这片景观被信念的浓淡所着色。本章将带领我们游览这片景观，从机器学习的基石到科学发现的前沿，甚至深入我们心智的结构。

### 一套更精致的机器学习工具包

让我们从现代数据科学的本土领域——机器学习开始。许多经典算法，从线性回归到分类，都为模型参数提供单一的[点估计](@entry_id:174544)。近似推断让我们能够升级这些工具，将它们转变为能够对不确定性进行推理的全贝叶斯模型。

想象一个简单的[贝叶斯线性回归](@entry_id:634286)问题。我们有一些数据，并相信它是由一条直线加上一些噪声生成的。我们对于那条线的斜率和截距可能是什么，也有一些先验信念。[贝叶斯法则](@entry_id:275170)告诉我们如何将先验信念与来自数据的证据相结合，以获得所有可能直线的后验分布。对于这个特定情况，精确的后验是一个表现良好的高斯分布，我们可以直接计算出来。这提供了一个完美的实验室，让我们看看我们的近似方法表现如何。当我们应用平均场[变分推断](@entry_id:634275)时，我们有意地做了一个简化的假设：我们对斜率的不确定性与对截距的不确定性是独立的。这个近似忽略了真实后验可能具有的相关性。如果我们数据中的特征是独立的（正交的），这个假设是无害的，我们的近似是完美的。但如果特征是相关的，我们简单的因子化近似将错过真实后验特有的倾斜特性，这是为计算简便性付出的实在代价。

这个教训是无价的，但在许多现实场景中，选择并非在精确答案和近似答案之间——而是在近似答案和毫无答案之间。考虑贝叶斯逻辑回归，这是分类模型的基石。一旦我们引入逻辑 sigmoid 函数将我们的[线性模型](@entry_id:178302)与概率输出联系起来，线性高斯情况下的便捷数学和谐就被打破了。后验分布变成一个复杂的、非高斯的对象，没有简单的解析形式。在这里，近似推断变得至关重要。我们无法直接解决问题，所以我们用一个更易于处理的二次曲线来界定这个困难的 sigmoid 函数，这是一个巧妙的技巧，使得变分更新变得 tractable。这是一个反复出现的主题：当面对一块难啃的数学骨头时，我们围绕它构建一个更简单、可解的脚手架。

当我们构建更结构化的模型时，这种方法的力量才真正闪耀。在[多任务学习](@entry_id:634517)中，我们可能想一次性解决几个相关的问题，比如预测学生在数学、物理和化学中的表现。与其建立三个独立的模型，我们可以建立一个[分层贝叶斯模型](@entry_id:169496)，其中每个特定任务的参数集都从一个共享的全局[分布](@entry_id:182848)中抽取。这使得任务之间可以相互借鉴统计强度。[变分推断](@entry_id:634275)提供了拟合这样一个模型的引擎，既能推断每个任务的具体细节，也能推断将它们统一起来的全局模式。我们再次看到了平均场假设的权衡：通过强制我们的近似是因子化的，我们切断了共享父节点自然在任务间引起的后验相关性。我们的近似正确地学会了将任务收缩到一个共同的均值，但它未能捕捉到这样一个微妙的事实：鉴于数据，物理任务中的一个意外结果应该直接更新我们对化学任务的信念。

### 解锁黑箱：深度学习中的近似推断

从经典模型到深度神经网络的飞跃是巨大的。这些“黑箱”以其复杂性而臭名昭著，很长一段时间里，深度学习的世界和原则性[贝叶斯推断](@entry_id:146958)的世界似乎相距甚远。近似推断提供了连接它们的桥梁。

一个关键思想是*摊销推断*。我们不再为每一个我们想做推断的数据点进行迭代优化，而是可以训练一个单独的[神经网](@entry_id:276355)络——一个“编码器”或“识别模型”——来为我们做推断。这个网络学习一个从观测（比如，一个失真的信号）直接到潜在原因（原始的、干净的信号）的近似后验分布参数的映射。因此，推断的成本在整个训练过程中被“摊销”了。在一个线性反演问题的背景下，这个摊销编码器惊人地学会了近似前向系统的正则化[伪逆](@entry_id:140762)——这是现代深度学习与经典线性代数之间一个深刻而美丽的联系。

也许最令人惊讶的联系是[深度学习](@entry_id:142022)中一个常见技巧——dropout——与贝叶斯推断之间被发现的联系。Dropout 最初作为一种[防止过拟合](@entry_id:635166)的实用方法被引入，它在训练期间随机将一部分神经元激活设置为零。这是一种效果奇佳的[启发式方法](@entry_id:637904)。多年后，有人证明 dropout 实际上是近似[变分推断](@entry_id:634275)的一种形式。用 dropout 和标准[权重衰减](@entry_id:635934)训练一个[神经网](@entry_id:276355)络在数学上等价于优化一个深度贝叶斯模型的[证据下界](@entry_id:634110)。每一次使用不同随机 dropout 掩码的[前向传播](@entry_id:193086)，都像是在网络权重的近似后验分布中抽取一个样本。

这一洞见不仅仅是理论上的奇闻；它是一个实际的意外之喜。这意味着我们可以拿一个标准的、现成的[神经网](@entry_id:276355)络，在测试时保持 dropout 开启，并对同一个输入进行多次预测。这些预测中的变化为我们提供了一个模型*认知不确定性*的度量——即它对自己权重的不确定性。这种“蒙特卡洛 dropout”技术提供了一种强大、计算成本低廉的[不确定性估计](@entry_id:191096)方法。对于使用[神经网](@entry_id:276355)络模拟物理世界的科学家来说，这是一个游戏规则的改变者。例如，在[计算材料科学](@entry_id:145245)中，[机器学习势函数](@entry_id:138428)被训练来预测原子构型的[势能](@entry_id:748988)。模拟原子运动所需的力是这个能量的梯度。力的[点估计](@entry_id:174544)很有用，但如果模型不确定，基于它的模拟可能会迅速失控。通过使用 MC dropout，我们可以得到一个力的[分布](@entry_id:182848)，从而能够评估我们模拟的可靠性，并检测模型何时在它不理解的区域进行外推。

### 科学发现的新视角

有了这些强大的工具，我们可以将注意力从构建更好的预测模型转向一个更宏伟的目标：科学发现。近似推断提供了一个框架，用于构建复杂系统的[生成模型](@entry_id:177561)，然后反演它们以揭示产生我们所观察到数据背后的隐藏结构。

这一点在现代生物学中表现得最为明显。来自[单细胞基因组学](@entry_id:274871)的数据量和复杂性是惊人的。我们可以为每个细胞测量数千个基因表达水平（转录组学）和表面蛋白标记（蛋白质组学）。一个中心任务是从这些数据中识别细胞类型。概率[混合模型](@entry_id:266571)提供了一个自然的框架：我们假设每个细胞属于 $K$ 个潜在类型之一，每种类型在每个数据模态中都有一个特征性的统计签名。[变分推断](@entry_id:634275)允许我们拟合这个模型，计算每个细胞属于每种类型的后验概率。当处理实验数据的现实情况时，该框架的优雅之处得以彰显：如果某个细胞缺少某个模态的数据，其相应的似然项就会从推断更新中简单地去掉。后验是利用任何可用的证据形成的，优雅地降级而不是崩溃。

我们可以把这个推得更远。不仅仅是聚类，我们可以尝试找到驱动所有数据类型变化的潜在连续变化轴。像[多组学](@entry_id:148370)[因子分析](@entry_id:165399)（MOFA+）这样的模型假设，庞大的、多模态的数据矩阵是由少数共享的潜在因子生成的。这些因子可能代表[生物过程](@entry_id:164026)，如[细胞分化](@entry_id:273644)或对药物的反应。[变分推断](@entry_id:634275)成为发现的引擎，从数据中提取这些因子，并量化每个数据类型（RNA、蛋白质、[染色质可及性](@entry_id:163510)）的[方差](@entry_id:200758)有多少是由每个因子解释的。这使得生物学家能够从一片数据点的海洋转向一个对系统基本驱动力的可理解、可解释的总结。

这种反演[生成模型](@entry_id:177561)以寻找潜在原因的主题在科学界反复出现。在[计算地球物理学](@entry_id:747618)中，科学家测量地球表面的[重力异常](@entry_id:750038)来推断地下的密度结构。这是一个经典的线性反演问题。贝叶斯公式允许我们结合先验知识（例如，密度变化通常是平滑的），并得到关于地下结构的全[后验分布](@entry_id:145605)，而不仅仅是单一的重建。在这里，[变分推断](@entry_id:634275)同样可以成为首选工具。甚至可以采用更具表达力的变分族，如[归一化流](@entry_id:272573)，它通过一系列可逆映射变换一个简单的基础[分布](@entry_id:182848)来构建复杂的[分布](@entry_id:182848)。对于一个线性高斯问题，一个简单的仿射流就足以表示*精确的*高斯后验，展示了一个近似变得精确的美丽案例。更重要的是，这个框架允许关键的自我批判：我们可以进行后验预测检查，看看我们推断出的模型是否能生成真实的数据，帮助我们诊断和理解我们模型和推断的局限性。

### 终极应用？一种心智理论

我们以所有应用中最雄心勃勃、最深刻的一个来结束：一种关于大脑本身的理论。[自由能原理](@entry_id:172146)是[计算神经科学](@entry_id:274500)中一个极具影响力的理论，它提出大脑本质上是一个推断引擎。它认为大脑构建了一个世界的内部生成模型，然后在其整个存在期间试图最小化该模型的预测与传入的感觉证据之间的差异。

这个最小化过程的数学表述恰恰是[变分推断](@entry_id:634275)。大脑被认为要最小化的量——变分自由能——与我们用来训练算法的目标函数是同一个。这个大胆的假设将大脑的解剖学和生理学重塑为一个[变分推断](@entry_id:634275)算法的物理实现。在这种观点下，大脑皮层的层次结构反映了大脑[生成模型](@entry_id:177561)的层次结构。源自皮层深层的下行信号不仅仅是任意的信息；它们是模型的预测。源自表层的上行信号是预测误差——预测与来自较低层次或感官的证据之间的差异。[兴奋与抑制](@entry_id:176062)的复杂舞蹈、不同皮层层的不同作用，以及神经调质对神经增益的调节，都被赋予了一个功能性目的：它们是计算和加权[预测误差](@entry_id:753692)以不断更新我们对世界信念的生物基质。

尽管这仍然是一个充满激烈研究和辩论的话题，但这一视角让我们得以一窥知识终极统一的可能性。帮助我们把一封邮件分类为垃圾邮件、发现一种新的细胞类型或窥探地球地壳之下的数学原理，可能恰恰就是支配我们自身感知和思维的原理。从一个处理难解积分的简单工具，近似推断绽放成为一个关于智能系统（无论是人工的还是生物的）潜在的宏[大统一理论](@entry_id:150304)。它证明了一个好想法的力量，不仅仅是解决问题，更是改变我们看待世界的方式。