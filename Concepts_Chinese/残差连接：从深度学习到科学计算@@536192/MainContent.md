## 引言
在追求更强大人工智能的过程中，出现了一个矛盾的挑战：让[神经网络](@article_id:305336)更深通常会使其性能变得更差。这种退化问题表明，我们训练这些复杂模型的方式存在一个根本性的障碍。增加层数本应只会增强网络的潜力，为何会导致更差的结果？答案不在于一个更复杂的解决方案，而在于一个优雅而简单的架构改变：[残差连接](@article_id:639040)。本文旨在揭开这一关键概念的神秘面纱，填补多年来困扰研究人员的知识鸿沟。第一章“原理与机制”将剖析[残差连接](@article_id:639040)的工作方式，探讨其对[梯度流](@article_id:640260)和优化景观的影响。随后的“应用与跨学科联系”将揭示其对各种人工智能架构的变革性影响，并揭示其在数值分析和生物学等领域中令人惊讶的概念相似性，展示这一思想的普适力量。

## 原理与机制

为什么在生活中的许多方面，添加更多的好东西有时反而会使情况变得更糟？一撮盐能提升菜肴的风味，但一把盐却会毁了它。一个专家团队可以解决一个问题，但一个数百人的委员会可能导致僵局。在[深度神经网络](@article_id:640465)的世界里，也观察到了一个类似且起初令人深感困惑的现象。当研究人员构建越来越深的网络时，他们碰壁了。超过某个点后，增加更多的层不仅无法提升性能，反而会主动地使其变得更差。

这是一个奇怪的悖论。原则上，一个更深的网络至少应该和一个更浅的网络一样好。毕竟，额外的层可以简单地学习成为**[恒等映射](@article_id:638487)**——即让它们的输入原封不动地通过——从而使网络的功能与其较浅的对应部分相同。它们难以做到这一点的事实，指向了我们训练它们的方式存在根本性困难。网络迷失了方向。当解决方案到来时，它是一种深受物理学家和数学家欣赏的类型：一个看似简单的视角转变，却开启了一系列深远的益处。这就是[残差连接](@article_id:639040)的故事。

### 重构问题：学习[残差](@article_id:348682)

与其让一堆层学习某个我们[期望](@article_id:311378)的复杂变换（我们称之为 $H(x)$），不如让它学习该变换与输入本身之间的*差异*，即**[残差](@article_id:348682)**？一个**[残差块](@article_id:641387)**的架构正是这样做的。其输出 $y$ 不仅仅是某个复杂函数 $F(x)$ 的结果，而是输入 $x$ 与该函数输出的和：

$$
y = x + F(x)
$$

从输入到输出的直接连接，即 $x$ 项，被称为**跳跃连接**或**恒等快捷方式**。现在，网络层只负责学习[残差](@article_id:348682)函数 $F(x)$。

为什么这如此强大？想象一下，对于一组层来说，[恒等变换](@article_id:328378)是最佳选择；也就是说，我们希望 $y=x$。在传统网络中，这些层必须学习去逼近[恒等函数](@article_id:312550)，这对于一堆非线性函数来说是一个出乎意料的非平凡任务。然而，在一个[残差块](@article_id:641387)中，网络只需要学习 $F(x) = 0$。对于像[随机梯度下降](@article_id:299582)这样的[优化算法](@article_id:308254)来说，将权重推向零是一个自然得多也容易得多的任务。

这种重构改变了整个学习问题。一个[残差网络](@article_id:641635)不再是逼近一个函数 $f$，而是逼近*[残差](@article_id:348682)* $f(x) - x$ [@problem_id:3194207]。这看似只是一个代数技巧，但它从根本上改变了网络在训练期间的行为。

### 梯度高速公路：一条学习的直接路径

这种结构的首个也是最著名的好处是它对**[反向传播](@article_id:302452)**的影响。在训练过程中，[神经网络](@article_id:305336)通过将梯度——即误差信号——从输出向后传递到输入来进行学习。在非常深的网络中，这个信号会经过一长串的乘法。如果链条中的每个环节都倾向于缩小信号，梯度就会指数级地缩小，到达早期层时几乎变为零。这就是臭名昭著的**[梯度消失问题](@article_id:304528)**，就像试图通过一百个人的队伍传递一个秘密；最终的消息变得模糊不清、微弱不堪。

跳跃连接构建了一条“梯度高速公路”，绕过了这条长链。让我们看一下其背后的数学原理，它出奇地简单。如果损失函数是 $L$，那么损失相对于[残差块](@article_id:641387)输入的梯度 $\frac{dL}{dx}$ 与输出处的梯度 $\frac{dL}{dy}$ 通过链式法则相关。仔细的推导揭示了一个优美的结构 [@problem_id:3181571]：

$$
\frac{dL}{dx} = \frac{dL}{dy} \frac{dy}{dx} = \frac{dL}{dy} \left( 1 + \frac{dF(x)}{dx} \right)
$$

看看那个+1！总梯度由两部分组成：通过恒等路径回流的梯度（$\frac{dL}{dy} \cdot 1$）和通过[残差](@article_id:348682)函数各层回流的梯度（$\frac{dL}{dy} \cdot \frac{dF(x)}{dx}$）。即使通过[残差](@article_id:348682)分支的梯度变得非常小，这个+1项确保了来自输出的梯度可以直接且无障碍地回流到输入。这不再是一场传话游戏；就好像队伍中的最后一个人可以直接向第一个人喊出原始信息。这保证了即使在非常深的网络中，最早的层也能接收到有意义的学习信号。

### 一个代数视角：锚定变换

通过线性代数的角度思考，我们可以对此有更深的理解。暂时想象一个简化的网络，其中每一层都只是与一个权重矩阵 $W$ 相乘。一个包含 $L$ 层的深层堆叠对应于变换 $W^L$。如果 $W$ 的[特征值](@article_id:315305) $\lambda$ 的[绝对值](@article_id:308102)小于1，那么当它与自身相乘时，其影响会消失。$W^L$ 的[特征值](@article_id:315305)是 $\lambda^L$，它们会以指数速度迅速趋向于零。这是[梯度消失问题](@article_id:304528)的代数核心。

现在考虑一个[残差块](@article_id:641387)，它对应于与 $(I + W)$ 相乘，其中 $I$ 是单位矩阵。如果 $W$ 的[特征向量](@article_id:312227)是 $v$，对应的[特征值](@article_id:315305)为 $\lambda$，那么一个简单的计算表明 $(I+W)v = Iv + Wv = v + \lambda v = (1+\lambda)v$。[特征值](@article_id:315305)被加上了1！一个包含 $L$ 个这样的块的堆叠对应于 $(I+W)^L$，其[特征值](@article_id:315305)为 $(1+\lambda)^L$。

差异是巨大的。假设 $W$ 是行为良好的，其[特征值](@article_id:315305)有界，比如 $|\lambda|  0.1$。在普通网络中，经过50层后，信号强度可能会衰减 $(0.1)^{50} = 10^{-50}$——完全湮灭。在[残差网络](@article_id:641635)中，衰减由 $(1+\lambda)^L$ 控制。即使在 $\lambda = -0.1$ 的最坏情况下，这个因子是 $(0.9)^{50} \approx 0.005$。信号被衰减了，但其强度要强上好几个数量级。恒等连接将变换**锚定**在了1附近，防止其消失 [@problem_id:3120969]。

当然，天下没有免费的午餐。这种加法结构也意味着，如果[残差](@article_id:348682)分支[雅可比矩阵的特征值](@article_id:327715)为正，梯度可能会增长。端到端的放大可以被 $(1+\alpha)^L$ 所限制，其中 $\alpha$ 是[残差](@article_id:348682)函数[雅可比矩阵](@article_id:303923)范数的界。如果 $\alpha$ 没有保持很小，这可能导致[梯度爆炸](@article_id:640121) [@problem_id:3198587]。这就是为什么 [ResNet](@article_id:638916) 几乎总是与[批量归一化](@article_id:639282)等技术一起使用，这有助于正则化[残差](@article_id:348682)分支，使其贡献保持良好行为。

### 平滑景观：让优化变得容易

[残差连接](@article_id:639040)的好处甚至比[梯度流](@article_id:640260)更深。它们从根本上改变了**[损失景观](@article_id:639867)**——即优化器必须导航以找到最小值的那个高维[曲面](@article_id:331153)。一个充满许多不良局部最小值和扁平[鞍点](@article_id:303016)的“崎岖”景观很容易困住优化器。

让我们用一个玩具问题来探讨这一点。假设我们想让一个微小的两层网络 $\hat{y} = w_2 w_1 x$ 学习一个简单的线性函数 $y = \alpha x$。关于权重 $(w_1, w_2)$ 的损失[曲面](@article_id:331153)是 $L = (w_1 w_2 - \alpha)^2$。[全局最小值](@article_id:345300)位于[双曲线](@article_id:353265) $w_1 w_2 = \alpha$ 上。但是点 $(w_1, w_2) = (0,0)$ 呢？在该点，梯度为零，但这是一个危险的[鞍点](@article_id:303016)，一个可能使优化器停滞的平坦区域。

现在，让我们添加一个跳跃连接：$\hat{y} = x + w_2 w_1 x$。为了学习相同的函数 $y = \alpha x$，网络现在必须学习一个乘积 $w_1 w_2 = \alpha - 1$。损失[曲面](@article_id:331153)现在是 $L = (w_1 w_2 - (\alpha - 1))^2$。考虑一个特殊但关键的情况，即我们希望网络学习[恒等函数](@article_id:312550)，也就是 $\alpha = 1$。损失函数变为 $L = (w_1 w_2)^2$。突然之间，位于 $(0,0)$ 的那个危险的[鞍点](@article_id:303016)被转变成了一个优美、宽阔的全局最小值！跳跃连接抚平了优化景观中的一个关键缺陷，使得“什么都不做”的解变得轻而易举 [@problem_id:3156480]。通过使恒等成为阻力最小的路径，我们使景观对我们的优化器友好得多。

### 路径的集成

看待[残差网络](@article_id:641635)的最后一种优美方式是，不把它看作一个单一的、极深的网络，而是看作一个由许多不同深度的网络构成的**隐式集成**。展开 [ResNet](@article_id:638916) 的[递归定义](@article_id:330317)可以发现，最终的输出是初始输入与所有[残差块](@article_id:641387)输出的总和：

$$
x_L = x_0 + \sum_{l=0}^{L-1} F_l(x_l)
$$

数据在网络中有多种通路。它可以穿越所有 $L$ 个[残差块](@article_id:641387)的“主干线”，也可以在任何一层通过跳跃连接走“出口匝道”。例如，[U-Net](@article_id:640191) 架构使用长程跳跃连接，创建了从早期编码器层到晚期解码器层的极短路径，从而创建了与总网络深度 $L$ 无关的常数长度 $O(1)$ 的梯度路径 [@problem_id:3194503]。这意味着一个 [ResNet](@article_id:638916) 实际上包含了指数数量级的不同长度路径的集合 [@problem_id:3151194] [@problem_id:3193894]。

这种结构让人联想到像 boosting 这样的[集成方法](@article_id:639884)，其中最终的预测是由一系列“[弱学习器](@article_id:638920)”的总和做出的。每个[残差块](@article_id:641387) $F_l$ 可以被看作是一个[弱学习器](@article_id:638920)，它不试图解决整个问题，而只是试图对传递给它的表示做一个小的修正。使用[梯度下降](@article_id:306363)进行训练会鼓励每个块 $F_l$ 逼近当前表示的“[残差](@article_id:348682)误差”，从而将最终输出推向目标 [@problem_id:3169973]。因此，一个 [ResNet](@article_id:638916) 的行为不像一个单一、脆弱的深层模型。它的行为像一个由许多协作者组成的强大委员会，共同努力提炼信号，而跳跃连接确保了它们的声音都能被听到。

最终，将输入加回到输出——学习[残差](@article_id:348682)——这个简单的想法，通过重构问题、创建梯度高速公路、平滑[损失景观](@article_id:639867)和实现隐式集成，解决了网络退化之谜。这是一个惊人的例子，说明了视角的转变如何能将一个看似棘手的问题转化为一个可管理的问题，并在此过程中揭示出层层相扣的数学之美。

