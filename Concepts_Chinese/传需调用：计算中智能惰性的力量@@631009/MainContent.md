## 引言
在编程世界中，一个简单的决定——*何时*计算一个值——对效率、正确性和表达能力有着深远的影响。大多数常见语言都遵循“急切”原则，立即对函数参数求值，这可能导致不必要的计算，并且无法处理某些问题，如处理无限数据。本文探讨了一种更为优雅的替代方案：传需调用，一种体现了“非绝对必要不工作”原则的“惰性”求值策略。我们将分两大部分来解析这个强大的概念。首先，在“原理与机制”中，我们将深入探讨惰性的机制，探索传需调用如何避免不必要的工作，通过[记忆化](@entry_id:634518)共享结果，并实现对无限数据结构的使用。随后，“应用与跨学科联系”将揭示这个看似简单的思想如何在[编译器优化](@entry_id:747548)、[算法设计](@entry_id:634229)、大规模数据系统乃至区块链技术等领域找到强大的应用。让我们从探索计算拖延的艺术开始吧。

## 原理与机制

想象你是一位正在准备盛大宴会的厨师。你有两个选择。你可以做一个“急切”的厨師，在开始烹饪之前，为菜单上每道可能的菜肴 meticulous 准备好每一种食材。或者，你可以做一个“惰性”的厨师，只在你当前步骤需要时，才去切洋葱或取香料。急切的厨师预先做了大量工作，如果某道菜最终没有被点单，其中一些工作可能就浪费了。惰性厨师则在每一刻都只做最少量的必要工作。在计算世界里，这种在急切与惰性之间的简单选择带来了深刻而美妙的结果。这就是**传需调用**（call-by-need）的故事，它是编程语言求值策略中那位聪明而惰性的厨师。

### 拖延的艺术：[严格求值](@entry_id:755525)与[惰性求值](@entry_id:751191)

任何编程语言的核心都是一种求值策略——一条规定*何时*以及*如何*计算表达式值的规则。最常见的策略是**[传值调用](@entry_id:753240)**（call-by-value），见于 C++、Java 和 Python 等语言中。它就是那位急切的厨师。在函数开始工作之前，[传值调用](@entry_id:753240)坚持要求其所有参数都必须被完全求值，得到它们的最终值。

让我们通过一个思想实验来看看这意味着什么。假设我们有一个特殊的函数，称之为 `const42`，它接受一个参数 `x` 但直接忽略它，总是返回数字 42。在 λ 演算的形式语言中，我们写作 $(\lambda x. 42)$。现在，假设我们给这个函数传入一个真正可怕的参数：一个永不停止的计算。我们称这个计算[黑洞](@entry_id:158571)为 $\Omega$。一个典型的例子是一个无限调用自身的函数，写作 $(\lambda x. x x)(\lambda x. x x)$。

当我们要求计算机对 `const42(Ω)` 求值时，会发生什么？

一个[传值调用](@entry_id:753240)求值器，由于其无法救药的急切性，会说：“等等！在我运行 `const42` 之前，我*必须*先算出它的参数 $\Omega$ 的值。” 然后它一头扎进那个[黑洞](@entry_id:158571)，开始无限计算。它永远不会返回。整个程序都被困住了，尽管该函数甚至不需要这个参数来产生它的答案。

这正是[惰性求值](@entry_id:751191)，特别是**传需调用**（call-by-need），提供了一条更优雅路径的地方。传需调用求值器是终极的拖延者。它会说：“除非我绝对必须，否则我不会去计算那个参数。” 当它看到 `const42(Ω)` 时，它不去求值 $\Omega$。相反，它直接开始执行 `const42` 函数。该函数的指令很简单：“返回 42。” 由于参数 `x` 从未被提及，求值器永远不需要查看 `x` 绑定的是什么。那个[黑洞](@entry_id:158571) $\Omega$ 从未被触及。求值器愉快地返回 42，程序终止。

这个简单的例子揭示了一个深刻的真理：通过[延迟计算](@entry_id:755964)，[惰性求值](@entry_id:751191)可以避免不必要的工作。而有时，那些“不必要的工作”可能是无限量的工作 [@problem_id:3649660]。它将传递参数的行为与計算參數的行为分离开来。

### 惰性的天才之处：共享工作

延迟工作是个好的开始，但这还不是全部。一个真正“聪明”的惰性策略不应只是惰性，还应避免重复自己。这就是精巧的传需调用与其更简单但更“笨”的近亲——**[传名调用](@entry_id:753236)**（call-by-name）的区别所在。

[传名调用](@entry_id:753236)是惰性的，但却是健忘的。每次需要一个参数的值时，它都从头重新计算。让我们想象一个函数 `add_three_times(x)`，它计算 $x + x + x$，我们用一个中等耗费的参数来调用它，比如 `E = (1+1) + (1+1)`。
一个[传名调用](@entry_id:753236)求值器会有效地将程序重写为 `((1+1) + (1+1)) + ((1+1) + (1+1)) + ((1+1) + (1+1))`。可怜的机器不得不将 `E` 的计算执行三次 [@problem_id:3649720]。

传需调用既惰性又*聰明*。它采用了一种名为 **thunk** 的优美机制。thunk 就像一张值的“期票”。当你向函数传递参数时，你传递的不是最终值，也不是原始表达式。你传递的是一个 thunk——一个整洁的包，其中包含了配方（待计算的表达式）和一个指示其是否已被计算的标志。

函数第一次需要 `x` 的值时，它会“强制”（force）这个 thunk。这会触发计算。值被计算出来。但神奇之处在于：thunk 随后会更新自身，用计算出的值替换掉配方。期票被兑换成了真金白银。这个过程被称为**[记忆化](@entry_id:634518)**（memoization）。下一次函数需要 `x` 时，它发现值已经在那儿了。无需重新计算。它是即时可用的。对于我们的例子 `add_three_times(E)`，表达式 `E` 只被计算一次 [@problem_id:3649720]。`x` 的所有三次使用都共享那一次计算的结果 [@problem_id:3675809]。

这种差异不仅仅是一个小优化。它可能是一个程序在几秒内完成与运行到宇宙末日之间的区别。考虑一个函数 `G(x)` 计算 $x+x$。如果我们嵌套这个函数，创建一个像 $H_k = G(G(...G(1)...))$ 这样的项，[传名调用](@entry_id:753236)策略将表现出指数级的计算增长，其成本与 $2^k$ 成正比。相比之下，传需调用通过在每一层共享结果，将成本降低到线性级别，仅与 $k$ 成正比 [@problem_id:3649661]。这是一个巨大的收益，通过“计算一次，处处共享”这一简单而优雅的原则实现。

### 创造无限：惰性[数据结构](@entry_id:262134)

当我们考虑一个看似不可能的任务时，传需调用的真正力量和优雅就显现出来了：在一台有限的机器上表示一个无限的[数据结构](@entry_id:262134)。你怎么可能存储所有[斐波那契数](@entry_id:267966)或所有素数的列表呢？

使用严格的[传值调用](@entry_id:753240)求值，你做不到。你必须在开始使用之前就计算出所有这些数，这是一个永远不会结束的无限任务 [@problem_id:3649646]。

但有了[惰性求值](@entry_id:751191)，无限变得可控。一个惰性列表，或称**流**（stream），不是一个装满预计算值的巨大容器。它是一个单一的数据单元，包含两样东西：一个值（列表的“头”）和一个 thunk（对“尾”或列表其余部分的承诺）。为了定义[斐波那契数](@entry_id:267966)的无限流 $F$，我们可以使用一个奇妙的自引用定义：
`F = 0 : 1 : zipWith(+) F tail(F)`
这可以翻译为：“斐波那契流以 0 开始，接着是 1，然后是一个通过将 $F$ 与其自身的尾部逐元素相加而创建的流。”

对一个[严格求值](@entry_id:755525)器来说，这是一个毫无意义的悖论。要定义 $F$，你需要 $F$！但[惰性求值](@entry_id:751191)器看不出任何问题。`zipWith(+)` 部分只是一个 thunk，一个未来计算的配方。它不会立即执行。

现在，如果你请求 $F$ 的前五个元素，`take(5, F)`，求值过程会优美地展开：
1.  第一个元素 `0` 是现成的。
2.  为了得到第二个元素，第一个 thunk 被强制求值，产生 `1`。
3.  为了得到第三个元素，`zipWith` thunk 被首次强制求值。它需要计算 $F_0 + F_1$。但等等——多亏了共享，$F$ 结构就是我们开始时的那个！$F_0=0$ 和 $F_1=1$ 的值已经实现。它们被即时检索，求和计算完成，第三个元素就产生了。
4.  这个过程继续下去，每一步只强制求值多一个 thunk，仅在需要时精确地实体化流的一个新元素 [@problem_id:3649646]。在某种意义上，我们创造了一个无限的结构，只在需要时探索它。

### 一种新的思维方式：成本与后果

生活在一个惰性的世界里，需要我们对程序性能的思考方式进行微妙的转变。[时间复杂度](@entry_id:145062)不再仅仅是总输入大小（比如 $n$）的函数。它变成了实际被*需要*（demanded）的输入量（我们称之为 $m$）的函数 [@problem_id:3226986]。如果你有一个包含十亿个数字的列表，但只请求前十个，那么所做的工作量与 10 成正比，而不是十亿。这使得一种奇妙的组合式编程风格成为可能，你可以将大型、甚至可能是無限的處理管道粘合在一起，並確信不會做任何不必要的工作。

然而，天下没有免费的午餐。惰性有时会导致一个令人惊讶的问题，称为**空间泄漏**（space leak）。当程序忙于“不计算”某些东西时，它必须记住它所做出的所有承诺。这些未求值的 thunk 会占用内存。如果一个程序构建了一个非常长的延迟操作链——例如，通过一个惰性的 `foldl` 函数对列表求和，该函数创建了一个像 `(...((0 + h(1)) + h(2)) + ... + h(m))` 这样的结构——它可能会消耗大量内存来持有这些嵌套的 thunk，等待一个最终命令来对整个事物求值 [@problem_id:3226986] [@problem_id:3251977]。这意味着，虽然惰性编程者从思考控制流中解放出来，但他们必须更加关注[数据流](@entry_id:748201)和未求值表达式的内存占用。

### 纯洁性契约：惰性与混乱的世界

到目前为止，我们一直生活在纯函数的纯净、数学领域中——这些函数，就像 `2+2` 一样，对于相同的输入总是给出相同的输出，并且除了返回值之外，对世界没有其他可观察的影响。当我们的优雅惰性模型与充满副作用（如打印到屏幕、写入文件或更改变量值）的混乱现实世界碰撞时，会发生什么？

答案是：事情变得复杂了。考虑表达式 `print("A") + print("B")`。`+` 运算符是严格的；它需要两个操作数的数值才能继续。在惰性环境中，求值器有两个 thunk 需要强制求值：一个用于 `print("A")`，另一个用于 `print("B")`。它应该先强制哪一个？如果顺序没有指定，输出可能是 "AB" 或 "BA" [@problem_id:3649634]。这种不确定性对于编写可靠的软件来说是一场噩梦。

更糟糕的是，当延迟的计算依赖于一个可以改变的值时，[记忆化](@entry_id:634518)本身变得“不健全”（unsound）。如果我们[记忆化](@entry_id:634518)像 `s + 1` 这样表达式的结果，其中 `s` 是一个可变变量，然后程序的其他部分改变了 `s`，我们[记忆化](@entry_id:634518)的值现在就过时了 [@problem_id:3661462]。这种聪明的共享优化破坏了我们程序的正确性。

这导致了拥抱惰性的语言的一个基本原则：它们必须强制执行一个**纯洁性契约**（purity pact）。传需调用的惊人好处——对未使用参数的终止保证、避免重复计算以及启用无限数据结构——只有在被延迟的计算是**纯粹**（pure）的情况下才是安全和可预测的。

像 Haskell 这样的现实世界惰性函数式语言通过在代码的纯粹部分和不纯部分之间建立一堵墙来解决这个问题。副作用不是被禁止的，而是被隔離起來。像 I/O 这样的不纯操作被封装到特殊的数据类型中，通常称为 **monads**，它们充当与现实世界进行一系列交互的配方。在这个 monadic 世界内部，操作的顺序被严格执行，确保如果我们希望的话，“A”总是在“B”之前打印 [@problem_id:3649634]。在此之外，在广阔的纯代码世界中，传需调用至高无上，发挥其效率和表达优雅的全部力量 [@problem_id:3661462]。这种分离让程序员可以享受两全其美：一种有原则的方式来推理副作用，和一个用于纯计算的强大惰性引擎。

