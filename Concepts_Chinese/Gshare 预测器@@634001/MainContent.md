## 引言
在追求更快的处理器的过程中，几乎没有什么挑战能比预测未来更为根本。现代 CPU 依赖一种名为“[推测执行](@entry_id:755202)”的技术，即猜测程序将要执行的路径以便提前工作。这些猜测的准确性由一个称为“分支预测器”的组件管理，对性能至关重要。然而，早期的预测器难以理解程序流中复杂且相互关联的特性，造成了严重的性能瓶颈。本文旨在揭开解决此问题中最优雅且影响深远的方案之一：gshare 预测器。首先，在“原理与机制”部分，我们将剖析 gshare 是如何从更简单的设计演变而来，利用一个巧妙的[异或](@entry_id:172120)（XOR）操作解决了棘手的[混叠](@entry_id:146322)问题，并显著提高了预测准确性。随后，“应用与跨学科联系”部分将拓宽我们的视野，揭示这一核心硬件技术如何与编译器、[操作系统](@entry_id:752937)相互作用，甚至对计算机安全产生深远的影响。

## 原理与机制

想象一下你正在尝试预测天气。你可以简单地看看昨天的天气。如果昨天下了雨，或许今天也会下雨。这就是**局部预测**的本质：一个系统的未来是基于其自身的过去行为来预测的。早期的分支预测器就是这样工作的。程序中的每条分支指令都有其自己的私有历史，处理器会根据该特定分支过去的行为来猜测其下一步的动作——**“跳转”（Taken）**或**“不跳转”（Not-Taken）**。这是一个合理的开始，但它忽略了一个关键因素：上下文。

### 对关联性的追求：从局部到全局

自然界和计算机程序中充满了关联性。一个城市的天气并非独立于邻近城市的天气。同样，一条分支指令的行为也常常与另一条分支指令的行为紧密相连。

考虑一个简单的程序，其中有两个分支 A 和 B 在一个循环中相继执行[@problem_id:3619816]。假设分支 A 的结果是随机的，就像抛硬币一样。但分支 B 是确定性的：它在当前循环迭代中的结果总是与分支 A 在*上一次*迭代中的结果完全相同。

对于分支 B 来说，一个局部预测器是毫无用处的。它勤奋地记录 B 自身的过去结果，但这段历史只不过是 A 过去抛硬币结果的记录，滞后了一步。这段历史对于预测 A *接下来*会做什么毫无帮助，而这正是决定 B 命运的关键。B 的局部预测器就像试图只通过观看片尾字幕来预测电影情节一样。它看的是正确的电影，但却是错误的部分。A（随机）和 B（依赖于 A）的准确率都将停滞在区区 50%。

解决方法是拓宽我们的视野。与其让每个分支生活在自己孤立的世界里，不如让处理器维护一个所有近期分支结果的共享历史？这就是**全局历史预测器**背后的思想。它使用一个称为**全局历史寄存器（GHR）**的硬件移位寄存器，来记录最近执行的几个分支的结果，无论它们在程序中的位置如何。这个 GHR 代表了程序最近所采取的[控制流](@entry_id:273851)“路径”。

现在，让我们回到 A-B 循环的例子。当处理器即将预测分支 B 时，最近一个分支 A 的结果正好就存放在 GHR 中。如果我们使用至少包含两个分支的历史，GHR 中会包含类似 `(A 的结果, 上一个 B 的结果, ...)` 的内容。关键信息——分支 A 的结果——不再被隐藏！预测器现在可以学习一个简单而强大的规则：“每当历史显示倒数第二个分支为‘跳转’时，就预测 B 将会‘跳转’。”有了这个全局上下文，对随机分支 A 的预测仍然是 50/50 的猜测，但[对相关](@entry_id:203353)分支 B 的预测则变得近乎完美。总体准确率从 50% 跃升至可观的 75% [@problem_id:3619816]。这就是全局历史的力量：它捕捉的是交响乐，而不仅仅是单个音符。

### 混叠的危害：一桩身份错认的案例

这种全局方法似乎取得了巨大的成功。我们将 GHR 输入一个大型查找表，即**模式历史表（PHT）**，其中每个条目都是一个小型计数器，用于学习针对该特定历史模式预测“跳转”或“不跳转”。但这个简单的方案隐藏了一个微妙而危险的缺陷：**[混叠](@entry_id:146322)（aliasing）**。

当两个完全不相关的分支，位于代码的不同部分，恰好在执行前都经历了完全相同的分支结果序列时，会发生什么？在一个简单的全局预测器中，GHR 是索引 PHT 的*唯一*依据，这两个分支将映射到表中的*同一个条目*。它们被“[混叠](@entry_id:146322)”在了一起。

如果两个分支在该历史下倾向于有相同的结果，这没问题——这是一种“建设性”干扰。但如果它们的行为相反呢？这就会导致**破坏性干扰**，一种预测器永远处于困惑状态的情况。

让我们构造一个病态案例来看看这场灾难是如何展开的 [@problem_id:3619731]。想象一个程序有两个关键分支 A 和 B。分支 A，位于以 $01010101_2$ 结尾的地址，总是“跳转”。分支 B，位于以 $10101010_2$ 结尾的地址，总是“不跳转”。现在，假设程序的结构使得在 A 和 B 执行之前，GHR 都被全“1”填满（例如，在一个包含 8 次“跳转”分支的循环之后）。

一个简单的全局预测器看到历史 $11111111_2$ 并查找 PHT 中的对应条目。
当分支 A 执行时，它使用这个条目，并且由于是“跳转”，它会训练计数器预测“跳转”。
稍后，分支 B 执行。它之前也出现了同样的历史 $11111111_2$，所以它使用*相同的 PHT 条目*。但 B 是“不跳转”的，所以它训练计数器预测“不跳转”。

这两个分支现在为了一个计数器而交战，不断地将其拉向相反的方向。预测器永远无法正确学习这两种行为。这就像两个同名的人共享一本**日记**；最终写出的故事将是一团乱麻。

### Gshare 的解决方案：一点 XOR 的魔力

混叠问题困扰着早期的全局预测器。Scott McFarling 在一篇堪称计算机体系结构里程碑的论文中提出的解决方案，既优雅又强大。其洞见在于，即使两个分支共享相同的历史，它们仍然是不同的实体。区分它们最明显的特征是它们在程序代码中的位置：它们的**[程序计数器](@entry_id:753801)（PC）**地址。

**gshare** 预测器通过一个简单的硬件操作——[按位异或](@entry_id:269594)（XOR），将这两条信息——全局历史和局部地址——结合起来。PHT 的索引不再仅仅是 GHR。取而代之的是：

$$
\text{index} = \text{GHR} \oplus \text{PC}
$$

让我们回到那个混叠灾难的例子 [@problem_id:3619731]。两个分支的 GHR 都是 $11111111_2$。
- 对于分支 A，其 PC 以 $01010101_2$ 结尾，索引变为：$\text{index}_A = 11111111_2 \oplus 01010101_2 = 10101010_2$
- 对于分支 B，其 PC 以 $10101010_2$ 结尾，索引变为：$\text{index}_B = 11111111_2 \oplus 10101010_2 = 01010101_2$

突然之间，它们映射到了完全不同的条目！XOR 操作在硬件中实现起来微不足道，却成功地利用了每个分支独一无二的 PC，将其引导到 PHT 的不同部分。破坏性[混叠](@entry_id:146322)问题得以解决。每个分支都获得了自己专属的、以全局上下文为条件的预测。这是一种美妙的综合：全局信息（GHR）与局部信息（PC）相结合，创造出一种强大且个性化的预测。这个简单的技巧非常有效，以至于多年来它成为现代[处理器设计](@entry_id:753772)的基石。我们可以在其他场景中看到同样的原理解决[混叠](@entry_id:146322)问题，例如，当分支因处理奇偶数据结构而区[分时](@entry_id:274419)，PC 的低位比特提供了必要的区分度 [@problem_id:3619709]。

### 预测的艺术：现实世界中的工程学

gshare 的核心思想非常出色，但在现实世界中构建一个预测器需要在一系列有趣的工程权衡中进行抉择。

首先，**多长的历史才足够？** 有了足够长的历史寄存器和相应巨大的 PHT，gshare 预测器可以学习到惊人复杂的模式。例如，它可以学会预测一个其结果取决于过去十几个分支结果的奇偶性（XOR 总和）的分支 [@problem_id:3619730]。然而，GHR 每增加一位，为避免[混叠](@entry_id:146322)所需的 PHT 大小就要翻倍。硬件成本呈指数级增长。这引出了一个经典的成本效益分析。通过测量一个工作负载的性能，我们可以看到将历史长度从（比如说）8 位增加到 12 位，可能会将错误预测率从 11% 降低到 8.5%。对于一个有 14 个周期的错误预测惩罚的处理器，我们可以将其直接转化为平均[每指令周期数](@entry_id:748135)（[CPI](@entry_id:748135)）的减少，甚至可以计算出我们为 GHR 和 PHT 投入的每一比特硅片的“投资回报” [@problem_id:3619808]。

如果我们想要长历史但又无法承受一个巨大的 PHT 怎么办？架构师们已经开发出巧妙的压缩方案。例如，一个 12 位的 GHR 可以通过将其组成块进行 XOR 运算，“折叠”成一个 8 位的值，从而可以使用一个更小的 $2^8$ 项 PHT，而不是一个 $2^{12}$ 项的 PHT [@problem_id:3619796]。这并非免费的午餐；折叠历史会增加混叠的几率。利用基本概率，我们可以将其建模为一个“球盒问题”，来估计增加的冲突率以及由此导致的预测准确性下降。这是一个在硬件预算和性能之间不断的平衡过程。

即使是“简单”的 PC 异或操作也并非那么简单。**我们应该使用 PC 的哪些位？** 使用最低位的比特似乎是显而易见的选择，但它们通常是“随机性”最差的。由于指令对齐，最低的几位总是零。此外，编译器通常以非常规整的、2 的幂次方的步长来布局代码。使用这些低熵比特可能会无意中重新引入系统性的[混叠](@entry_id:146322)模式。通常，PC 的**中阶比特**——那些标识不同函数或内存页内大代码块的比特——具有更高的熵，能更好地将预测分散到整个 PHT 中，从而避免这些结构[性冲突](@entry_id:152298) [@problem_id:3619764]。这显示了硬件架构与生成代码的软件开发工具之间深度的相互作用。

### 历史的局限：我们看不到什么

尽管 gshare 预测器功能强大，但它并非最终的解决方案。它的视野受限于 GHR 中记录的内容。如果一个分支的行为取决于近期分支结果历史中不存在的信息——比如说，一个循环计数器的奇偶性——gshare 将难以达到完美的预测效果 [@problem_id:3619761]。

更根本的是，gshare 依赖于**结果历史**。它知道最近的分支是“跳转”还是“不跳转”，但它不知道它们是*哪个*分支。这种对所经路径的“盲目”可能是一个关键缺陷。

考虑最后一个富有启发性的例子 [@problem_id:3619806]。一个分支 R 可以通过两条不同的路径到达：一条经过分支 P，另一条经过分支 Q。
- 如果我们经过 P，R 总是“跳转”。
- 如果我们经过 Q，R 总是“不跳转”。

这里的关键是：P 和 Q 本身都总是“跳转”。所以，无论选择哪条路径，在预测 R 之前写入 GHR 的最后一个结果都是“跳转”。两种情况下 GHR 是完全相同的。gshare 预测器无法区分 P 路径和 Q 路径。它看到相同的历史，因此在两种场景下都使用相同的 PHT 条目。它最终会学会预测多数情况（例如，如果 P 路径更频繁），并对少数情况持续预测错误。它最多只能达到 60% 的准确率，错误预测率为 40%。

为了解决这个问题，我们不仅需要知道结果，还需要知道产生该结果的分支的身份。我们需要**路径历史**。一个哪怕只增加一位路径历史来区分 P 和 Q 的增强型预测器，都可以完美地学习到这种关联，并实现 0% 的错误预测率。这种基于结果的历史的局限性，推动了分支预测领域的下一波创新浪潮，催生了更加复杂的设计，这些设计在一个宏大的预测器锦标赛中结合了全局历史、局部历史和路径历史。看来，探索的旅程永无止境。

