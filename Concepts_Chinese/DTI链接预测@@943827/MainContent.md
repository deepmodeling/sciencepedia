## 引言
新药发现的过程，好比在一间巨大、昏暗的房间里，从数十亿把潜在的钥匙（化合物）中为数千把独特的锁（蛋白质）寻找那一把唯一的钥匙。这个搜索空间的浩瀚使得传统的试错法极其缓慢和昂贵。药物-靶点相互作用（DTI）预测作为一种强大的计算范式应运而生，以应对这一挑战。它利用机器学习构建一个“万能锁匠”，能够预测任何给定的药物和蛋白质是否会相互作用。这种方法有望加速新药的发现，增进我们对其副作用的理解，并最终实现个性化医疗。

本文深入探讨DTI[链接预测](@entry_id:262538)的核心原理和变革性应用。在接下来的章节中，您将对这一前沿领域获得全面的理解。第一章“原理与机制”将解析基础概念，解释我们如何为机器表示生物实体，区分记忆型模型与学习型模型，以及实现稳健可信预测所需的统计技术。第二章“应用与跨学科联系”将探讨这些模型在现实世界中的应用——从产生新药假说、确保患者安全，到指导实验科学和革新监管审批流程。

## 原理与机制

想象一下，人体是一个拥有无数锁的图书馆——这些锁就是我们的蛋白质。每把锁都执行特定的功能。疾病可能是一把卡住打不开的锁，或者是一把根本关不上的锁。药物是我们设计用来与这些锁相互作用的钥匙。然而，传统的“锁与钥匙”模型是一个极大的简化。[药物发现](@entry_id:261243)的真正挑战在于，我们身处一个巨大、昏暗的房间，里面有数十亿把潜在的钥匙（小分子）和数千种不同的锁（蛋白质靶点）。大多数钥匙从未在大多数锁上测试过。我们的任务是构建一个类似万能锁匠的系统，它能审视任何一把钥匙和任何一把锁，并有一定信心地预测它们是否会相互作用。这就是药物-靶点相互作用（DTI）预测的核心。

### 蓝图：从生物学到可能性网络

为了让机器了解这个广阔的世界，我们首先需要一种它能理解的语言。我们将生物学问题转化为数学问题。我们可以将所有药物看作一个点集，所有蛋白质看作另一个点集。每当我们知道某个特定药物与某个特定蛋白质相互作用时，我们就在它们之间画一条线，即一条**边**（edge）。我们最终得到的是一个巨大的网络，或称为**图**（graph）。因为这些线只连接药物和蛋白质，从不连接药物与药物或蛋白质与蛋白质，所以这是一种特殊的网络，称为**[二分图](@entry_id:262451)**（bipartite graph）。

因此，我们的问题不再是关于化学或生物学，而是关于在这个图中找到缺失的连线。这是网络科学中的一个经典问题，称为**[链接预测](@entry_id:262538)**（link prediction）[@problem_id:4570200]。我们希望构建一个模型，该模型能接收任何药物-蛋白质对——即使是那些尚无连线的——并为其分配一个相互作用的概率。这可以从几个方面来构建：我们可能希望预测相互作用的*强度*，这是一个连续值，如[结合亲和力](@entry_id:261722)，这是一个**回归**（regression）问题[@problem_id:1426722]。更常见的是，我们希望预测相互作用的*存在*，一个简单的“是/否”概率，这是一个**[二元分类](@entry_id:142257)**（binary classification）问题。

### 表征的艺术：教会机器什么是药物

机器无法从“Aspirin”或“Cyclin-dependent kinase 2”这样的名称中学习。它需要从功能的角度理解这些实体*是*什么。我们必须用有意义的特征来表示它们。对于一个药物分子，这可以是一个其化学亚结构的列表，或者更强大的是，它的整个分[子图](@entry_id:273342)，其中原子为节点，[化学键](@entry_id:145092)为边。对于一个蛋白质，我们可以使用其主要的[氨基酸序列](@entry_id:163755)。

但我们可以做得更好。蛋白质的功能由其折叠成的复杂三维形状决定，而这个形状本身又由其氨基酸序列决定。这种关系是深刻而微妙的。我们如何从一个简单的字母串中捕捉到这种形成结构和功能的潜力？

这就是现代人工智能中最绝妙的思想之一发挥作用的地方：**[蛋白质语言模型](@entry_id:188811)（PLMs）**。想象一个大型神经网络，类似于驱动ChatGPT的那些网络，它通过阅读生命之树中数百万个[蛋白质序列](@entry_id:184994)，学会了“生命语言”。它在一个简单的游戏上进行训练：我们取一个序列，隐藏几个氨基酸，然后要求模型根据其余部分的上下文预测缺失的氨基酸。为了精通这个游戏，模型不能仅仅记忆局部模式。它必须学习[蛋白质结构](@entry_id:140548)的深层语法规则，包括序列中相距遥远但在折叠后的蛋白质中聚集在一起形成关键功能位点的氨基酸之间的[长程依赖](@entry_id:181727)关系。

通过这项任务的训练，模型学会为任何给定的蛋白质创建一个丰富的[数值表示](@entry_id:138287)——一个**嵌入**（embedding）。这个嵌入是一个密集的数字向量，它隐含地编码了关于蛋白质可能结构和功能的重要信息。这是一个基础性的洞见：模型在从未被明确教导生物学机制的情况下，仅仅通过学习进化杰作的统计模式就了解了它们[@problem_id:5173701]。这是一个反复出现的主题：在正确的学习框架下，复杂的生物学特性可以从简单的、自监督的目标中涌现出来。

### 归纳的飞跃：泛化至新发现

在这里，我们遇到了一个关键的哲学和实践上的区别：一个靠记忆的学生和一个真正理解的学生之间的差异。有些模型是**直推式**的（transductive）。它们学习关于它们训练数据中实体的特定事实。一个经典的例子是[矩阵分解](@entry_id:139760)，它为训练数据中的每个药物和每个靶点学习一个唯一的嵌入向量。如果你给它一个训练集中没有的全新药物，它就会束手无策。它没有该药物的嵌入，不经完全重新训练就无法做出预测。它只能从特定的训练节点“直推”到其他训练节点[@problem_id:4375852]。

这对于真正的发现是无用的。我们需要一个能学习通用原理的模型。我们需要一个**归纳式**（inductive）模型。归纳式模型学习一个*函数*，该函数可以接收任何药物的内在特征（如其分子图）和任何蛋白质的内在特征（如其序列），并动态生成一个嵌入。这正是[图神经网络](@entry_id:136853)（GNNs）和[蛋白质语言模型](@entry_id:188811)的设计目的。因为函数本身具有共享参数，所以它可以应用于新的、未见过的药物和靶点——即所谓的**[冷启动问题](@entry_id:636180)**。这就像是背诵一本短语手册和真正学习一门语言的区别。归纳式模型可以为零已知相互作用的实体生成预测，为假设新化合物和未表征蛋白质的功能铺平了道路[@problem_id:4570167]。

### 数字媒人：为[完美配对](@entry_id:187756)打分

假设我们的归纳式编码器已经完成了它们的工作。现在我们有一个药物的向量（嵌入）$\mathbf{z}_d$和一个靶点的向量（嵌入）$\mathbf{z}_t$。我们如何将它们结合起来以产生一个相互作用的概率？这就是**解码器**（decoder）的作用。

最简单的方法是测量这些向量的几何对齐程度。例如，我们可以说，如果向量在[嵌入空间](@entry_id:637157)中彼此接近，那么相互作用的可能性就很大，从而得出一个基于它们距离的分数，比如 $\exp(-\|\mathbf{z}_d - \mathbf{z}_t\|^2)$。另外，我们可以使用一个更灵活的[双线性形式](@entry_id:746794)，$\mathbf{z}_d^\top W \mathbf{z}_t$。在这里，矩阵 $W$ 是在训练过程中学习的，它充当一个“兼容性度量”，捕捉了超越简单相似性的复杂、多方面的相互作用性质[@problem_id:4570125]。

为了构建一个真正稳健的[概率模型](@entry_id:265150)，我们可以将这些思想组合成一个单一、优雅的公式。进入我们概率函数的最终分数，或**logit**，应该考虑三件事：

1.  **特异性亲和力**：该药物与该靶点之间独特的兼容性，由[双线性](@entry_id:146819)项 $\mathbf{z}_d^\top W \mathbf{z}_t$ 捕捉。
2.  **普遍滥交性**：有些药物就是比其他药物更“粘”，而有些蛋白质是常见的靶点。我们可以通过为每个药物和靶点学习一个简单的偏置项 $b_d$ 和 $c_t$ 来解释这一点。
3.  **基础比率**：相互作用是罕见的。我们可以用一个全局偏移量 $\log(\frac{\pi}{1-\pi})$ 来编码这一先验知识，其中 $\pi$ 是相互作用的总体普遍率。

将这些放在一起，我们得到相互作用的logit：$\mathbf{z}_d^\top W \mathbf{z}_t + b_d + c_t + \log(\frac{\pi}{1-\pi})$。然后，我们将这个分数通过一个logistic sigmoid函数 $\sigma(z) = 1 / (1 + \exp(-z))$，将其压缩到一个介于0和1之间的有效概率。这为预测相互作用提供了一个有原则且强大的框架[@problem_id:4570200]。

### 穿越迷宫：从稀缺中学习的实践

DTI数据的现实世界带来了一个艰巨的挑战：巨大的**[类别不平衡](@entry_id:636658)**。已知的非相互作用对的数量远远超过已知的相互作用对。如果我们天真地训练一个模型，它会很快学会一个虽然准确率高但很平庸的策略：“总是预测无相互作用”。

为了克服这个问题，我们需要更智能的训练程序。首先，我们必须使用一个不平等对待所有样本的[损失函数](@entry_id:136784)。**[二元交叉熵](@entry_id:636868)（BCE）**损失是一个起点，但它可以被改进。**Focal Loss**是一个巧妙的修改，它动态地降低了那些容易被正确分类的样本（比如数百万个真阴性样本）的贡献权重。这迫使模型将其精力集中在困难的案例上，即那些真正需要学习的边界样本上[@problem_id:4570205]。

其次，我们甚至无法计算所有可能的药物-靶点对的损失。这个数字是天文数字。相反，对于训练批次中的每个已知的正向相互作用，我们必须智能地采样一些非相互作用作为负样本。这被称为**[负采样](@entry_id:634675)**（negative sampling）。简单地随机均匀采样非相互作用对是一种方法，但它通常会选择那些模型可以轻易分类为非相互作用的样本。一个更有效的策略是**流行度偏置采样**（popularity-biased sampling），它倾向于采样涉及那些已经被充分研究的药物和靶点（即在图中度数较高的节点）的非相互作用对。这些“困难负样本”提供了更有信息量的训练信号，并导致一个更平衡、更有效的学习过程[@problem_id:4570150]。

### 超越预测：[置信度](@entry_id:267904)、信任与机制洞察

一门成熟的预测科学不仅给出答案，还会报告其自身的[置信度](@entry_id:267904)。对于一个可能指导数十亿美元研究项目并最终影响人类健康的DTI模型预测来说，这一点是不可或缺的。这引导我们走向了**[不确定性量化](@entry_id:138597)**这一关键概念。

我们的模型可能会经历两种“不知道”[@problem_id:4570188]：

1.  **认知不确定性**（模型不确定性）：这是模型由于训练数据有限而产生的“自我怀疑”。在化学或蛋白质空间中模型见过的样本很少的区域，这种不确定性很高。我们可以通过使用[蒙特卡洛](@entry_id:144354) dropout 等技术来估计它，即我们用不同的随机“[子网](@entry_id:156282)络”多次运行推理。预测中的高方差表明[认知不确定性](@entry_id:149866)高。好消息是，这种不确定性可以通过在那些稀疏区域收集更多数据来减少。

2.  **[偶然不确定性](@entry_id:154011)**（数据不确定性）：这是[生物过程](@entry_id:164026)及其测量中固有的、不可减少的随机性或噪声。一些相互作用本质上是随机的或难以精确测量。一个复杂的模型可以学会从数据本身预测这种不确定性。与认知不确定性不同，这种不确定性无法通过收集更多同类型数据来减少。

区分这两者告诉我们如何行动。高认知不确定性意味着“我们需要更多数据”或“这个预测不可信”。高[偶然不确定性](@entry_id:154011)意味着“我们已经达到了这个系统可预测性的基本极限”。

最后，这引出了终极问题：我们*为什么*应该信任这些模型？一个能正确预测相互作用的模型是真的在学习生物学，还是仅仅是一个利用数据中偏见的聪明[模式匹配](@entry_id:137990)器？这是**事后可解释性**（post-hoc explainability）和**[机制可解释性](@entry_id:637046)**（mechanistic interpretability）之间的区别。一个事后解释，比如一个显示模型“关注”了哪些原子的[显著性图](@entry_id:635441)，虽然令人安心但很肤浅。它不能证明模型的推理在生物学上是合理的。

另一方面，**[机制可解释性](@entry_id:637046)**是圣杯。这是我们寻找证据的地方，证明模型的内部计算直接对应于真实的、物理的因果机制——例如，证明一个[子网](@entry_id:156282)络的激活与实验测量的结合速率如 $k_{\text{on}}$ 相关。当我们有这样的证据时，我们对模型预测的信任会大大增加。这表明模型不仅仅是在插值，而是学到了一些关于世界的、可泛化的、真实的东西。这种机制基础使我们能够负责任地管理与将药物推进到首次人体临床试验等决策相关的巨大**归纳风险**（inductive risk），将我们的预测从计算好奇心的范畴推向拯救生命的科学领域[@problem_id:4439818]。

