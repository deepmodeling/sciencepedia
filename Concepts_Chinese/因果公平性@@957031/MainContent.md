## 引言
当人工智能系统做出改变人生的决策时，例如批准贷款或推荐医疗方案，我们如何确保它们是真正公平的？标准的统计度量通常关注群体平均值，却无法回答个体的根本问题：“如果仅仅改变我的种族或性别，对*我*的决策会有所不同吗？”正是在群体层面平等与个体层面正义之间的这种差距中，因果公平性这一深刻概念应运而生。它提供了一种范式转变，从询问聚合结果转向审视决策过程本身。

本文将对这个强大的框架进行全面探索。第一章**“原理与机制”**将解析其核心理论，介绍“如果…会怎样”的思想实验以及用于严格回答该问题的结构因果模型 (Structural Causal Models, SCMs)。我们将探讨因果关系如何为构建个体层面公平的系统提供蓝图，并将此方法与传统的统计[公平性指标](@entry_id:634499)进行对比。随后，**“应用与跨学科联系”**一章将展示这些原则如何应用于高风险领域。我们将遍览医学、遗传学和政策领域，看看因果公平性如何提供一种通用语言来诊断偏见、设计更公正的解决方案，并在现实世界中建立负责任的系统。

## 原理与机制

### 我们真正想问的问题：“如果…会怎样？”

想象一下，一家银行使用人工智能 (AI) 系统来批准或拒绝贷款。一个来自历史上被边缘化群体的人申请了贷款，尽管有良好的财务状况，但仍被拒绝。他们可能会问：“为什么是我？”。一份显示不同人口群体之间总体批准率相似的统计报告并不能回答这个人的问题。萦绕在他们脑海中的问题更个人化，也更根本：“如果我在所有其他方面都完全相同——我的财务纪律、我的就业历史、我的抱负——只是碰巧属于另一个群体，我会被批准吗？”

这就是“如果…会怎样”的问题。它不是关于比较不同的人；它是关于将一个人与平行世界中自己的一个假设版本进行比较。这便是**因果公平性**的核心。它将焦点从群体统计数据转移到个体正义上，关注决策的*过程*，而不仅仅是其聚合结果。它试图将非歧视的核心道德原则形式化：决策不应*仅仅*因为某个受保护的属性（如种族、性别或民族）而改变。[@problem_id:4426572] [@problem_id:4372296]

### 一个充满因果的世界：结构因果模型

要严格回答一个“如果…会怎样”的问题，我们需要的不仅仅是数据。数据告诉我们*已经*发生了什么；它揭示了相关性。要理解*本可以*发生什么，我们需要一个世界模型，一种拥有其内部逻辑和物理定律的“玩具宇宙”。在因果科学中，这被称为**结构因果模型 (Structural Causal Model, SCM)**。

可以将 SCM 看作是现实一角如何运作的蓝图。它包含三个主要要素：

1.  **变量 (Variables)：** 这些是我们宇宙中可测量的方面，如一个人的 `Race`（种族）、`Income`（收入）、`Education`（教育）以及最终的 `Loan Decision`（贷款决策）。

2.  **结构方程 (Structural Equations)：** 这些是我们玩具宇宙的“物理定律”。它们是关于因果关系的简单陈述。例如，一个方程可能陈述为 `Income = function(Education, Zip Code)`，意味着我们相信收入是由一个人的教育水平和居住地决定的。这些方程共同构成一个因果图，一张描绘影响如何从一个变量流向另一个变量的地图。

3.  **外生变量 ($U$)：** 这也许是最神奇也是最重要的部分。这些变量代表了所有不被我们玩具宇宙*内部*任何其他事物所引起的事物。你可以将它们视为现实中基本的“背景噪音”，或者更贴切地说，是个体独特的本质。对于一个人来说，他们的外生变量集合（通常用 $U$ 表示）可能代表了他们与生俱来的天赋、家庭的支持系统、遗传倾向——所有那些未被测量、构成他们之所以为他们的背景因素。它们是我们所见世界中所有变化的最终、未被观察到的源头。[@problem_id:4205267] [@problem_id:4372296]

### 我们玩具宇宙中的公平性实验

有了我们的 SCM，即我们的玩具宇宙，我们终于可以以一种有原则的方式进行“如果…会怎样”的实验。这个过程被称为**溯因-行动-预测 (abduction-action-prediction)** 程序，它是一套优美的[科学推理](@entry_id:754574)方法，使我们能够计算反事实。[@problem_id:4372296]

1.  **溯因 (Abduction - 了解个体)：** 我们从数据中选取一个真实的人。假设是 Maria，她的贷款申请被拒绝了。我们反向运用 SCM 的“物理定律”来推断她外生变量的具体值，即她独特的本质 $U$。我们正在以计算的方式捕捉关于 Maria 的一切，那些超越她可观察属性、使她成为*她*自己的东西。

2.  **行动 (Action - ‘如果…会怎样’)：** 现在我们执行 Judea Pearl 所说的“外科手术式干预”。我们进入我们的玩具宇宙，只改变关于 Maria 的一件事。我们使用 **do-operator** 将她的 `Race` 属性设置为一个不同的值。至关重要的是，我们保持她的本质，即她的 $U$，绝对不变。就她所有未测量的背景而言，她仍然是*同一个人*。我们在一个平行世界中创造了一个反事实的 Maria。

3.  **预测 (Prediction - 观察结果)：** 我们让 SCM 中的物理定律从这个被改变的状态开始向前运行。新的 `Race` 值在系统中传播，可能会影响下游的其他变量，然后我们观察这个反事实 Maria 的最终 `Loan Decision`。

这个程序导出了一个优美简洁而又深刻的公平性定义。如果对于任何个体（对于任何本质 $U$），无论我们将其受保护的属性设置为何值，其结果都完全相同，那么这个决策就是**反事实公平**的。决策不应该改变。对于一个预测 $\hat{Y}$ 和一个受保护属性 $A$，这可以写成：
$$ \hat{Y}_{A \leftarrow a}(U) = \hat{Y}_{A \leftarrow a'}(U) $$
这个等式必须对属性的任意两个值 $a$ 和 $a'$ 以及每个个体 $U$ 都成立。对 Maria 来说，这意味着对她真实自己和反事实自己的贷款决策必须是相同的。这个决策在最深层的因果意义上对她的种族是“盲目”的。[@problem_id:4205267] [@problem_id:4420254]

### 公平性的蓝图：因果路径

那么，我们如何构建一个满足这个优雅条件的系统呢？SCM 为我们提供了直接的蓝图。规则非常清晰：为了让预测 $\hat{Y}$ 相对于属性 $A$ 是反事实公平的，在模型的因果图中必须**不存在从 $A$到 $\hat{Y}$ 的因果路径**。

这意味着算法不能使用任何受到受保护属性影响或“污染”的信息。这个条件比初看起来要强得多。仅仅不告诉算法一个人的种族是不够的。这就是“通过无知实现公平”的谬误。想象一下，算法使用一个人的邮政编码来做决策。如果由于历史上的种族隔离，邮政编码受到种族的严重影响，那么就存在一条**因果路径**：`Race` $\rightarrow$ `Zip Code` $\rightarrow$ `Loan Decision`。算法间接地使用了关于种族的信息，它将不会是反事实公平的。[@problem_id:4420254]

一个优美的临床例子完美地说明了这一原则。想象一下构建一个 AI 来预测患者患肾损伤的风险。让受保护属性为患者的保险类别 ($A$)。一个公平的模型可以使用生理测量值，如血压 ($W$)，我们可能假设这不受保险类型的影响。然而，它不能使用“医疗过程”变量 ($V$)，例如患者接受抗生素的速度，如果该变量本身受到保险状况的影响。因果路径 $A \rightarrow V \rightarrow \hat{Y}$ 会违反[反事实公平性](@entry_id:636788)。一个真正公平的模型必须仅由那些在因果图中不属于受保护属性后代的变量来构建。[@problem_id:4849762]

### 公平性的宇宙：因果与统计

至关重要的是要理解，[反事实公平性](@entry_id:636788)只是众多公平性定义星系中的一颗星。许多其他流行的定义是**统计性**的，而[非因果性](@entry_id:194897)的。它们关注数据中的模式，而不是玩具宇宙的机制。

*   **人口统计均等 (Demographic Parity)** 问：所有种族群体的贷款批准率总体上是否相同？它关注的是群体平均值。用概率的语言来说，它要求预测 $\hat{Y}$ 独立于属性 $A$，记为 $\hat{Y} \perp A$。[@problem_id:4407148]

*   **[均等化赔率](@entry_id:637744) (Equalized Odds)** 问一个更精细的问题：在那些会偿还贷款的人（“合格”群体）中，所有种族的批准率是否相同？在那些会违约的人中，拒绝率是否相同？它关注的是在群体之间均等化错误率。这记为 $\hat{Y} \perp A \mid Y$，其中 $Y$ 是真实结果。[@problem_id:4407148]

区别是深刻的。统计公平[性比](@entry_id:172643)较的是*不同的人群*（例如，所有男性申请人群体与所有女性申请人群体）。因果公平性则是为*同一个个体*在不同假设情景下进行比较。统计概念关乎群体间结果的平等；因果公平性关乎个体决策过程的完整性和公正性。[@problem_id:4426572]

### 柏拉图洞穴中的阴影：现实世界中的挑战

结构因果模型的世界是优美纯净的。而现实世界并非如此。应用这些优雅的原则充满了挑战，就像试图从洞穴墙壁上闪烁的阴影来重建三维物体一样。

*   **可识别性问题 (The Identifiability Problem)：** 最大的挑战在于我们永远无法看到真实的 SCM 或个体的“本质” $U$。我们所拥有的只是观测数据——那些阴影。要测试我们的模型是否是反事实公平的，我们必须对世界的[因果结构](@entry_id:159914)做出强有力且通常无法检验的假设。这被称为**可识别性**问题。没有随机对照试验（这通常是不可能或不道德的），我们永远无法百分之百确定我们的因果模型是正确的。[@problem_id:4390060]

*   **世界的战争（标准冲突）：** 当实现一种公平性迫使你违反另一种时，会发生什么？这不仅仅是一个理论难题；这是一个常见且令人烦恼的困境。如果一种疾病在某个群体中确实更普遍，一个完美的诊断模型自然会对每个群体有不同的阳性预测率，从而违反人口统计均等。为了“修正”这一点并强制实现统计均等，你可能需要为每个群体使用不同的决策阈值。但是，一个规定“如果患者来自 A 组，使用阈值 X；如果来自 B 组，使用阈值 Y”的规则，是[反事实公平性](@entry_id:636788)的典型违例，因为决策规则本身现在明确依赖于受保护属性。[@problem_id:5174953]

*   **扭曲的镜头（偏见数据）：** 我们的数据，即我们正在分析的阴影本身，常常是扭曲的。
    *   **测量偏见 (Measurement Bias)：** 像[脉搏血氧仪](@entry_id:202030)这样的医疗设备在肤色较深的人身上可能不太准确。观测到的特征 ($X^{\text{obs}}$) 是真实生理状态 ($X^{\text{true}}$) 的一个有偏见的反映。这种偏见可以从 `Race` 到测量值之间创造一个虚假的因果路径，使得一个公平的模型看起来不公平，或者掩盖了真正的不公平。[@problem_id:4426626]
    *   **标签偏见 (Label Bias)：** 与白人患者相比，医生可能不太可能记录黑人患者的疼痛，即使他们的真实疼痛程度相同。我们数据集中的“基准真相”标签 ($Y^{\text{obs}}$) 不是真实的结果 ($Y^{\text{true}}$)，而是有偏见的人类行为的反映。一个基于这些数据训练的 AI 将勤奋地学习复制这种记录偏见，而不是预测实际的医疗需求。[@problem_id:4426626]

*   **可移植性问题 (The Portability Problem)：** 想象你已经解决了所有这些问题，并为波士顿的一家医院建立了一个反事实公平的模型。你能把它部署到密西西比州农村的一家医院吗？不一定。底层的因果关系——患者人口统计、当地环境因素，甚至医生做决策的方式——都可能不同。这意味着“当地的物理定律” ($f^{(s)}$) 和“背景本质”的分布 ($U^{(s)}$) 是特定于地点的。公平性保证不是一个通用印章；它是依赖于上下文的，并且可能无法从一个环境**迁移**到另一个环境。[@problem_id:5185231]

这段从一个直观问题到一个强大数学框架，再到现实世界棘手挑战的旅程，揭示了因果公平性的深刻之美和实用价值。它没有给我们简单的答案，但它给了我们正确的问题去问，以及一种严谨的语言来开始回答它们。

