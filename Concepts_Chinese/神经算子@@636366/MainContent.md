## 引言
从全球气候模式到[湍流](@entry_id:151300)，理解和预测复杂物理系统的探索长期以来一直是数值模拟的领域。然而，这些传统方法虽然强大，但计算成本往往高得令人望而却步，成为科学发现和工程设计的主要瓶颈。尽管[深度学习](@entry_id:142022)已经改变了许多领域，但传统[神经网](@entry_id:276355)络在这一领域存在根本性局限；它们被设计用于映射固定大小的向量，难以独立于测量网格来学习底层的物理定律。

本文介绍神经算子，这是一类革命性的模型，它通过学习算子——即整个[函数空间](@entry_id:143478)之间的映射——来直接应对这一挑战。我们将首先探讨赋予这些模型强大能力的核心**原理与机制**，包括离散化不变性的概念以及[傅里叶神经算子](@entry_id:189138)（FNOs）和[深度算子网络](@entry_id:748262)（[DeepONet](@entry_id:748262)s）的具体架构。随后，我们将综述其变革性的**应用与跨学科联系**，展示它们如何加速物理学、工程学及其他领域的研究。首先，我们必须理解神经算子所代表的思维方式的根本转变——从近似函数转向学习支配这些函数的算子。

## 原理与机制

要真正理解神经算子所代表的革命，我们必须首先回到我们所熟悉的传统[神经网](@entry_id:276355)络世界。几十年来，这些网络因其学习复杂关系的能力而备受赞誉，扮演着“[通用函数逼近器](@entry_id:637737)”的角色。你给它们一个固定大小的数字列表作为输入——比如一张照片的像素值——它们就会生成另一个固定大小的数字列表作为输出——或许是照片中包含一只猫的概率。它们将 $\mathbb{R}^n$ 中的向量映射到 $\mathbb{R}^m$ 中的向量。但如果我们要解决的问题并非关于固定大小的数字列表呢？

如果我们想预测天气呢？输入不是一个固定大小的向量，而是在全球范围内定义的温度、压力和风的[连续函数](@entry_id:137361)。我们期望的输出是另一组函数，用于预测这些值在明天的情况。如果我们想设计一个新的飞机机翼呢？输入是机翼的形状（一个函数），输出是其周围的气流（另一个函数）。科学和工程领域充满了这类问题，我们需要学习的不是向量之间的映射，而是函数之间的映射。这些映射在数学上被称为**算子**。

一个在 $32 \times 32$ 网格上离散化的机翼气流模拟数据上训练的传统[神经网](@entry_id:276355)络，如果你让它预测一个更精细的 $128 \times 128$ 网格上的气流，它将完全不知所措。这个网络的架构本身就与输入维度绑定。更微妙的是，即使我们能重新训练它，那些确保其预测稳定可靠的属性也可能随着分辨率的增加而退化 [@problem_id:3407177]。我们不是在学习物理，而只是在学习特定相机的像素到像素的映射。

因此，我们的目标是构建一种新型的学习机器：一种能够学习算子本身，即学习底层的物理定律，而不依赖于我们选择如何测量或离散化它的方式。这个属性是[科学机器学习](@entry_id:145555)的“圣杯”：**离散化不变性** [@problem_id:3407193]。一个真正具有离散化不变性的模型，一旦训练完成，就可以利用粗糙天气模拟的数据生成高分辨率的预报，或者利用飞机机翼上任意位置传感器的读数绘制出完整的压[力场](@entry_id:147325)。它学习的是连续的物理现实，而不是离散的投影投射在我们的传感器或网格上。

### 通用蓝图：利用积分核学习

我们该如何构建这样一台机器呢？让我们从物理学中寻求灵感。自然界中的许多基本算子，从[引力](@entry_id:175476)到电磁学，都可以表示为[积分变换](@entry_id:186209)的形式。算子 $u$ 在点 $x$ 处的输出由输入函数 $a$ 在整个定义域上的积分给出，并通过一个称为**[核函数](@entry_id:145324)**的函数 $K(x, y)$进行加权：

$$
u(x) = \int_{\Omega} K(x, y) a(y) dy
$$

核函数 $K(x, y)$ 编码了相互作用：它指定了点 $y$ 处的输入如何[影响点](@entry_id:170700) $x$ 处的输出。这为我们提供了一个通用的蓝图。神经算子本质上是这种[积分算子](@entry_id:262332)的一个复杂、可学习的版本。这种可适应任何几何形状的通用架构包括三个步骤 [@problem_id:3386866]：

1.  **升维**：我们首先取输入函数 $a(x)$，并在每个点将其提升到一个更高维的通道空间，创建一个表示 $v_0(x)$。这就像给网络一个更丰富的“草稿纸”来进行计算。

2.  **迭代更新**：然后我们应用一系列层。每一层通过组合一个可学习的[积分算子](@entry_id:262332)、一个简单的逐点线性变换和一个[非线性](@entry_id:637147)函数 $\sigma$，将函数 $v_{\ell-1}(x)$ 更新为 $v_{\ell}(x)$：

    $$
    v_{\ell}(x) = \sigma \left( W_{\ell} v_{\ell-1}(x) + \int_{\Omega} K_{\theta}(x, y, \dots) v_{\ell-1}(y) dy \right)
    $$

3.  **投影**：在最后一层之后，我们将丰富的、高维的表示 $v_L(x)$ 逐点投影回最终的输出函数 $u(x)$。

这个优雅的蓝图具有非凡的通用性。对于定义在任意网格或点云上的函数，我们可以用加权和来近似积分，其中权重考虑了点的局部密度。这催生了**[图神经算子](@entry_id:750017)**，它通过学习一个依赖于点的属性而非其在网格上任意索引的核函数 $K_{\theta}$，即使在复杂的非结构化几何体上也能学习底层的连续物理 [@problem_id:3386866]。其核心原理保持不变：学习一个连续积分算子的[相互作用核](@entry_id:193790)。

### [傅里叶神经算子](@entry_id:189138)：[频域](@entry_id:160070)中的交响乐

现在，让我们考虑一个特殊但极其重要的案例：在规则网格上的问题，比如图像或矩形[域上的模](@entry_id:150832)拟。在这里，我们可以运用数学和物理学中最强大的“魔术”之一：[傅里叶变换](@entry_id:142120)。著名的**卷积定理**告诉我们，实空间中复杂的全局操作——卷积——在[频域](@entry_id:160070)中变成了简单的局部乘法。许多物理过程，如[扩散](@entry_id:141445)（热流），都是由卷积描述的。

这就是**[傅里叶神经算子](@entry_id:189138)（FNO）**的核心思想 [@problem_id:3426959]。FNO 不试图学习一个复杂的、与空间相关的[核函数](@entry_id:145324) $K(x, y)$，而是通过[参数化](@entry_id:272587)其[傅里叶变换](@entry_id:142120)来学习一个更简单的、平移不变的核。单个 FNO 层的工序如同一首优美的变换交响曲 [@problem_id:3407036]：

1.  **进入[频域](@entry_id:160070)**：首先，表示在网格上的输入函数 $v_{\ell-1}(x)$ 通过[快速傅里叶变换](@entry_id:143432)（FFT）被转换为其频率分量 $\mathcal{F}(v_{\ell-1})(k)$。

2.  **滤波与混合**：在这个[谱域](@entry_id:755169)中，FNO 执行其关键操作。它截断高频部分，只保留截止频率 $K$ 内的模式 $|k| \leq K$。这起到低通滤波器的作用，不仅使模型更高效，还提供了一种[隐式正则化](@entry_id:187599)，使学习过程更稳定，尤其是在数据有噪声时 [@problem_id:3407262]。在这些保留的频率上，它应用一个可学习的复数值矩阵 $R_{\theta}(k)$，该矩阵对模式进行乘法运算，并混合函数不同通道间的信息。这个乘法就是该层学习到的“物理”。为确保实值输入函数产生实值输出，学习到的权重必须遵循优美的[共轭对称性](@entry_id:144131)，$R_{\theta}(-k) = \overline{R_{\theta}(k)}$ [@problem_id:3407262]。

3.  **返回现实世界**：然后，经过滤波的模式通过逆 FFT 变换回空间域。整个过程有效地对输入函数应用了全局卷积。

4.  **局部精化**：全局卷积的结果与一个简单的逐点[线性映射](@entry_id:185132)的结果（以及一个[跳跃连接](@entry_id:637548)）相结合，最后通过一个[非线性激活函数](@entry_id:635291) $\sigma$。

这种方法真正的天才之处在于它对离散化[不变性](@entry_id:140168)问题的解决方案。学习到的参数 $\theta$ 在连续的[频域](@entry_id:160070)中定义了函数 $R_{\theta}(k)$，而不是作为一组与特定网格索引绑定的权重。因此，如果我们想在一个具有不同分辨率的新网格上评估算子，我们只需在新网格上应用 FFT，并在新网格对应的频率位置[上采样](@entry_id:275608)*完全相同*的学习函数 $R_{\theta}(k)$ [@problem_id:3407193]。这赋予了 FNO 卓越的零样本超分辨率能力。

此外，这种[频域](@entry_id:160070)方法非常高效。在一个有 $N$ 个点的网格上进行直接的全局卷积，其计算成本会随 $N$ 呈二次方增长。而通过使用 FFT，FNO 以仅为 $O(N \log N)$ 的成本实现了相同的目标 [@problem_id:3407231]。这一计算上的飞跃使得学习[大规模系统](@entry_id:166848)中复杂的、[长程相互作用](@entry_id:140725)成为可能，而这对于像 CNNs 这样局限于小型局部核的标准架构来说通常是难以处理的任务。

### [深度算子网络](@entry_id:748262)：分支与主干的二重奏

FNO 很强大，但它依赖于特定的基——[傅里叶基](@entry_id:201167)，这对于周期性域最为自然。如果我们想要一种不同类型的灵活性呢？让我们回到我们的[积分算子](@entry_id:262332)蓝图，并从一个不同的视角来看待它。如果我们能用[分离表示](@entry_id:634176)来近似[核函数](@entry_id:145324) $K(x,y)$ 呢？

$$
K(x, y) \approx \sum_{k=1}^{p} \tau_k(x) \beta_k(y)
$$

如果我们将这个代入我们的积分中，会发生奇妙的事情：

$$
u(x) = \int K(x, y) a(y) dy \approx \sum_{k=1}^{p} \tau_k(x) \left( \int \beta_k(y) a(y) dy \right)
$$

仔细观察这个表达式。括号中的项，我们可以称之为系数 $c_k$，仅依赖于输入函数 $a(y)$。另一项 $\tau_k(x)$ 仅依赖于我们想要评估解的输出坐标 $x$。输出是[基函数](@entry_id:170178) $\tau_k(x)$ 的[线性组合](@entry_id:154743)，其中系数 $c_k$ 由输入函数 $a(y)$ 决定。

这就是**[深度算子网络](@entry_id:748262)（[DeepONet](@entry_id:748262)）**背后的精妙思想 [@problem_id:3426959]。从架构上讲，它是两个不同[神经网](@entry_id:276355)络之间的二重奏：

*   一个**分支网络（Branch Net）**，充当算子的“耳朵”。它接收输入函数 $a$（通常在一组固定的“传感器”位置采样）并计算系数向量 $[c_1, c_2, \dots, c_p]$。
*   一个**主干网络（Trunk Net）**，充当“手”。它接收单个坐标 $x$作为输入，并生成一个[基函数](@entry_id:170178)值的向量 $[\tau_1(x), \tau_2(x), \dots, \tau_p(x)]$。

最终输出就是这两个网络输出的[点积](@entry_id:149019)。分支网络倾听整个输入函数，决定*表达什么*，而主干网络学习一组合适的[基函数](@entry_id:170178)，以便在*需要的地方*表达它。

[DeepONet](@entry_id:748262) 实现离散化[不变性](@entry_id:140168)的方法不同但同样强大。因为主干网络接收连续坐标 $x$ 作为输入，我们可以在域内的任何点请求解，从而使其具有天然的输出分辨率无关性。输入不变性是通过在物理空间中固定分支网络的传感器位置来实现的，这与任何特定的网格无关 [@problem_id:3407193]。

### 力量与前景：从理论到现实

这些架构不仅仅是巧妙的工程技巧；它们植根于深厚的数学原理。正如标准[神经网](@entry_id:276355)络是函数的通用逼近器一样，神经算子已被证明是**[连续算子](@entry_id:143297)的通用逼近器** [@problem_id:3426998]。全局[积分变换](@entry_id:186209)（如 FNO 的谱卷积）和局部逐点[非线性](@entry_id:637147)的结合，其能力足以近似任何从一个[函数空间](@entry_id:143478)到另一个[函数空间](@entry_id:143478)的连续映射。理论证实，通过堆叠这些层，我们可以从简单、高效的组件构建出任意复杂的、非平移不变的核。

当然，现实世界是复杂的。它有复杂的几何形状和边界。FNO 在其最纯粹的形式中假设一个周期性的世界。但其原理是可适应的。通过将[算子学习](@entry_id:752958)与数值分析的经典技术相结合，可以克服这些挑战。我们可以不强行将[非周期性](@entry_id:275873)问题放入周期性盒子中，而是通过使用**[提升函数](@entry_id:175709)**来处理边界条件来变换问题本身，或者我们可以使用更合适的[光谱](@entry_id:185632)基（如**[切比雪夫多项式](@entry_id:145074)**）来构建算子，这对于有界域是自然的 [@problem_id:3407244]。[DeepONet](@entry_id:748262)s 由于其设计，可以通过巧妙的架构选择来整合边界条件，例如将其输出乘以一个在边界处消失的掩码 [@problem_id:3427044]。

也许最令人兴奋的前景在于预测未来——学习演化系统的动力学。我们可以训练一个神经算子来近似系统（如[流体流动](@entry_id:201019)或天气）在单个短时间步长 $\Delta t$ 内的演化。然后，为了预测长期的未来，我们只需反复应用学习到的算子进行“[前推](@entry_id:158718)”（rollout）。人们可能会担心模型在每一步犯下的小错误会累积，并迅速导致灾难性的失败。但在这里，物理学伸出了援手。

一项卓越的分析表明，如果底层的物理系统是耗散的（如热流，能量会衰减），那么学习模型的长期误差**可证明是有界的**，无论我们走了多少步。系统的自然稳定性会不断纠正模型微小的不准确性。即使对于[能量守恒](@entry_id:140514)系统（如理想的波传播），误差增长也最多是随时间线性增长——与人们可能天真预期的爆炸性[指数增长](@entry_id:141869)相去甚远 [@problem_id:3427040]。当一个学习到的算子真正捕捉到系统的底层物理时，它的预测就继承了系统自身的稳定性，为以传统模拟器一小部分成本进行可靠的、长时程预测打开了大门。这是物理学与机器学习的美妙统一，一个刚刚开始展开的合作关系。

