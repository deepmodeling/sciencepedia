## 引言
在一个日益依赖人工智能进行从医疗诊断到资源分配等关键决策的世界里，由数据驱动的客观精准性前景广阔。然而，这种乐观情绪被一个关键挑战所冲淡：算法，尽管只是代码，却可能产生极其不公平和带有偏见的结果，从而系统性地伤害弱势群体。这提出了一个根本性问题：我们如何确保我们的技术创造物能够服务于正义和公平？本文旨在通过全面概述算法公平性来填补这一知识空白。第一部分“原则与机制”将剖析不公平的构成，探讨偏见如何在数据和模型设计中产生，并介绍用于衡量公平性的各种相互竞争的数学定义。随后的“应用与跨学科联系”部分将把这些理论与现实世界相结合，通过考察高风险的医疗案例研究，来说明有偏见的人工智能所带来的实际危害，并概述构建和维护可信赖系统所需的严格实践。我们的探索始于那些在算法时代用以定义、衡量和争论公平性的基本原则。

## 原则与机制

想象一位技术精湛的医生。当他们做出诊断时，他们会运用多年的训练、海量的知识以及敏锐的直觉。但即使是最好的医生也会犯错。现在，如果我们构建一个人工智能来辅助他们呢？这个人工智能，和医生一样，也会犯错。关键问题，也是算法公平性核心所在的问题，不是人工智能*是否*会犯错，而是它会犯*哪种*错误，以及为谁犯错。这些错误是否存在一种模式？如果存在，这种模式是否系统性地、不公平地伤害了某个特定人群？

我们的旅程就从这里开始：在一个日益由算法引导的世界里，去理解公平性的原则与机制。

### 不公平的剖析

乍一看，人们可能会认为算法，作为一段代码，是客观性的缩影。它没有个人偏见，也不会有糟糕的日子。然而，这些系统能够并且确实会产生带有严重偏见的结果。当我们意识到算法并非一个脱离实体的头脑，而是我们赋予它的世界的产物时，这个悖论就迎刃而解了。这种偏见的来源可以大致分为两类：我们提供的数据中的缺陷，以及我们构建的引擎中的缺陷。

#### 世界的扭曲映像：数据偏见

算法完全通过被投喂的数据来了解世界。如果数据是现实的一面扭曲的镜子，算法学到的也将是一个扭曲的观点。这种“数据偏见”并非单一问题，而是在收集我们世界信息的混乱过程中可能出现的各种不同问题的集合 [@problem_id:4406676]。

其中最隐蔽的一种形式是**测量偏见**。想象一下，我们试图预测患者真实、潜在的临床状态，我们称之为 $z$。但我们无法直接测量 $z$。取而代之的是，我们测量一组特征 $x$，比如来自医疗设备的读数。如果这个设备本身就有缺陷呢？例如，如果一个[脉搏血氧仪](@entry_id:202030)在肤色较深的人身上准确性较低怎么办？真实状态 $z$ 和测量特征 $x$ 之间的关系就变得与群体相关。算法永远看不到真实的现实 $z$；它只能看到带有偏见的测量值 $x$。信息被不可挽回地丢失了，而且这种丢失对不同群体来说是不同的。无论下游的算法如何巧妙调整，都无法奇迹般地恢复这些丢失的信息。测量的瞬间就已经注定了结果 [@problem_id:4406676]。

然后是**标签偏见**。一个学习检测疾病的算法需要被标记为“有病”或“无病”的样本。但这些标签是谁提供的呢？是人类专家。而人类有其自身的偏见。假设在一个临床数据集中，医生历史上更有可能对某一人口群体的某种状况做出误诊。训练标签，记为 $\tilde{y}$，就成了真实结果 $y$ 的一个充满噪声、带有偏见的版本。算法被训练来预测这些带噪声的标签 $\tilde{y}$，可能只是学会了复制那些创建数据的医生们的历史诊断偏见 [@problem_id:4406676]。

最后，**抽样偏见**发生于训练数据集不能代表模型将要部署的人群。例如，如果一个代表性不足的群体不太可能同意他们的数据用于研究，那么他们在训练数据中就会……嗯，代表性不足 [@problem_id:4434056]。算法将花费大部分时间学习多数群体的模式，而较少关注少数群体。部署时，它对它见得最少的那个群体的表现可能会很差。

#### 引擎的内部缺陷：[算法偏见](@entry_id:637996)

真正令人惊讶的是，即使我们能奇迹般地获得完美、无偏见的数据，算法自身的设计也可能造成不公平的结果。这就是**[算法偏见](@entry_id:637996)** [@problem_id:4406676]。

大多数学习算法的工作方式是，试图从一个预定义的模型族中找到最佳模型，数学家称之为**假设类别** $\mathcal{H}$。假设特征与疾病之间的真实关系对于 A 组是一条复杂的曲线，而对于 B 组是一条简单的直线。如果我们强迫我们的算法只学习线性模型，那么它对 A 组的误差自然会高得多。这不是数据问题，而是设计选择。我们选择的工具根本不适合用于其中一个群体 [@problem_id:4406676]。

更根本的是，算法几乎总是被设计来优化一个单一的、全局性的指标，比如总体准确率。“尽可能多地做出正确的预测！”这似乎是一个合理的目标。但这个看似无害的目标却包含一个隐藏的陷阱：多数人的暴政。

让我们考虑一个极端的假设情景。一个人工智能被设计用来诊断一种严重疾病。在我们的验证数据中，有两个交叉群体：群体 $G_1$ 很大，有 $1,800$ 名患病者；而群体 $G_2$ 小得多，只有 $200$ 名患病者。测试人工智能后，我们发现总体性能非常出色：其**灵敏度**——即它正确识别出的患病者比例——高达 $91\%$。这是一项胜利！真的是吗？

魔鬼藏在细节中，这种实践被称为**子群组分析**。当我们分别审视每个群体时，一幅可怕的画面浮现出来。对于大群体 $G_1$，灵敏度高达 $0.95$。但对于小群体 $G_2$，灵敏度却是灾难性的 $0.55$。这个人工智能漏掉了少数群体中将近一半的患病者。高总体得分是一个统计上的幻觉，一个被多数群体的良好表现所主导的加权平均值。该系统看似良好的性能掩盖了对 $G_2$ 群体患者的深重伤害和严重不公 [@problem_id:4850164]。这不仅仅是一个统计上的奇特现象，更是我们不伤害（non-maleficence）和正义（justice）伦理责任的失败。

### 指标议会：定义公平性

如果像总体准确率这样的单一指标都如此具有误导性，那么我们该如何衡量公平性呢？这个问题没有唯一的答案。取而代之的是，我们有一个“指标议会”，每个指标都代表一种不同的哲学和数学上的公平概念。它们常常相互冲突，在它们之间做出选择要求我们明确我们的伦理目标。

我们可以做出的一个基本区分是*程序*公平性与*实质*公平性 [@problem_id:4420267]。**程序公平性**倡导平等的流程：对每个人应用相同的规则。在人工智能的语境下，这可能意味着对所有群体使用相同的决策阈值。这迎合了我们对形式平等的感知，并使系统可预测和透明，从而支持患者的自主权。而**实质公平性**则关注平等的结果：它力求确保人工智能的结果和影响是公平分配的，即使这意味着对不同群体应用不同的规则（如不同的阈值）。这与[分配正义](@entry_id:185929)以及我们为所有人最大化利益和最小化伤害的责任相一致。

让我们来见识一下我们这个议会中的主要候选者，以一个数字病理学人工智能为例，该人工智能将玻片标记为“可疑”（$\hat{Y}=1$）或“不可疑”（$\hat{Y}=0$）以供病理学家审查 [@problem_id:4366384]。

**人口统计均等（Demographic Parity）：** 该指标主张最简单的平等形式：算法应以相同的比率为所有群体标记可疑玻片，无论它们实际上是否为恶性。
$$P(\hat{Y}=1 \mid G=A) = P(\hat{Y}=1 \mid G=B)$$
其吸[引力](@entry_id:189550)在于其简单性，以及它与确保“平等享有”某种资源（在此例中是病理学家的注意力）等目标的契合。但它的巨大弱点是完全忽略了真实情况 $Y$。根据这个定义，一个完美的公平系统可以通过随机标记每个群体的 $30\%$ 来实现，这是一个在医学上毫无用处的程序。

**[机会均等](@entry_id:637428)（Equal Opportunity）与[均等化赔率](@entry_id:637744)（Equalized Odds）：** 这两个强大的指标将真实情况重新带回了讨论。它们主张公平性应该通过算法在临床情况相似的人群中的表现来评判。

- **[机会均等](@entry_id:637428)**关注利益。它要求对于所有真正患有该疾病（$Y=1$）的患者，被正确识别的机会在各群体间是相同的。这意味着**真阳性率（TPR）**，或称灵敏度，必须相等。
$$P(\hat{Y}=1 \mid Y=1, G=A) = P(\hat{Y}=1 \mid Y=1, G=B)$$
这确保了人工智能的益处——获得正确及时的诊断——是公平分配的 [@problem_id:4849777]。在我们的病理学例子中，两个群体的 TPR 都是 $0.8$，因此该系统满足[机会均等](@entry_id:637428) [@problem_id:4366384]。

- **[均等化赔率](@entry_id:637744)**更进一步，还考虑了负担。它增加了第二个条件：对于所有*没有*患该疾病（$Y=0$）的患者，被错误标记的机会在各群体间也必须相同。这意味着**假阳性率（FPR）**也必须相等。
$$P(\hat{Y}=1 \mid Y=0, G=A) = P(\hat{Y}=1 \mid Y=0, G=B)$$
这确保了人工智能的负担——一次不必要的、可能带来压力的随访检查——也是公平分配的 [@problem_id:4849777]。我们的病理学人工智能未能通过这个测试，因为它对 B 组的 FPR（$0.20$）高于 A 组（$0.15$） [@problem_id:4366384]。

**预测均等（Predictive Parity）：** 这个指标关注的是预测的意义。它要求来自人工智能的“可疑”标记无论在哪个群体中都具有相同的分量。也就是说，在给定可疑标记的情况下，实际患有癌症的概率对每个人都是相同的。这意味着**阳性预测值（PPV）**必须相等。
$$P(Y=1 \mid \hat{Y}=1, G=A) = P(Y=1 \mid \hat{Y}=1, G=B)$$
这对于使用该系统的临床医生和患者的信任至关重要。如果一个阳性结果对 B 组意味着 $63\%$ 的癌症几率，而对 A 组仅意味着 $57\%$，那么这个人工智能输出的意义本身就是不稳定的 [@problem_id:4366384]。

### 无法逃避的权衡

拥有一个指标议会是一回事，让他们达成一致则是另一回事。我们现在面临着算法公平性领域中最深刻、最美妙的发现之一：你不可能拥有一切。这些理想的属性往往是相互排斥的。

考虑一下[均等化赔率](@entry_id:637744)和人口统计均等之间的紧张关系。想象一个满足[均等化赔率](@entry_id:637744)的算法，对于两个群体，其 TPR 为 $0.8$，FPR 为 $0.2$。如果疾病在 A 组（$30\%$）中比在 B 组（$10\%$）中常见得多，那么 A 组的总体阳性标记率（$0.38$）将远高于 B 组（$0.26$）。为了使这两个比率相等——即满足人口统计均等——我们将不得不调整其中一个群体的决策阈值，这必然会改变其 TPR 或 FPR，从而打破[均等化赔率](@entry_id:637744) [@problem_id:4745875]。

这引出了一个基本的不可能性定理。假设我们有一个人工智能，它能产生一个完全**校准**的风险评分 $S$。一个经过校准的评分是“真实的”评分：$S=0.7$ 的评分意味着患者有 $70\%$ 的真实概率患有该疾病 [@problem_id:4438896]。校准是可信赖人工智能的基石，对于向医生和患者传达风险至关重要。该定理指出，对于任何不完美的分类器，它在数学上不可能同时满足以下所有三个属性 [@problem_id:4418563]：
1.  校准
2.  [均等化赔率](@entry_id:637744)
3.  各群体间疾病的基础率不相等（例如，$\pi_A \ne \pi_B$）

如果疾病在 A 组比在 B 组更常见，那么来自 B 组的患者需要表现出更强的疾病迹象，才能达到与 A 组患者相同的“真实”风险水平。这意味着他们的评分分布*必须*是不同的，这违反了[均等化赔率](@entry_id:637744)的核心条件。我们被迫做出选择。

这种不可能性并非绝望的理由，而是对清晰度的呼唤。它迫使我们作为科学家、医生和社会来决定，在特定情况下我们最看重什么。[公平性指标](@entry_id:634499)的选择不是一个纯粹的技术决策，而是一个伦理决策。随着情境的变化，我们选择的指标也可能随之改变 [@problem_id:4438896]：

-   对于一个资源有限的**低风险筛查项目**，其目标是为初步评估提供平等的机会，**人口统计均等**可能是最公正的选择，即使它效率不高。
-   对于一个**高风险的诊断决策**，其中错误分类的危害是严重的，**[均等化赔率](@entry_id:637744)**是一个有说服力的选择，因为它力求公平地分配这些危害。
-   当目标是为医生和患者之间的**共同决策提供信息**时，**校准**的真实性对于尊重患者自主权至关重要。

因此，一项可辩护的政策不是盲目地强制执行单一的[公平性指标](@entry_id:634499)，而是根据任务的需要确定优先事项，对权衡取舍保持透明，并在系统部署后持续监控其对所有群体的性能 [@problem_id:4418563]。我们还必须认识到，这些群体层面的统计数据虽然至关重要，但并不能捕捉到公平性的全貌。一个真正公正的系统还必须考虑到个体，致力于实现一个相似的个体被相似对待的世界——这是一个简单的想法，却代表了该领域最具挑战性的前沿之一 [@problem_id:4434056]。追求算法公平性不是要找到一个神奇的公式，而是要参与一个持续、深思熟虑且基于伦理的科学过程。

