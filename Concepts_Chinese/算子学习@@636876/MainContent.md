## 引言
几十年来，机器学习在学习将有限数据点映射到预测（例如识别图像）的函数方面表现出色。然而，自然界的基本定律并非由这类映射描述，而是由算子——将整个函数转换为其他函数的规则——来描述，正如在控制物理和工程领域的[偏微分方程](@entry_id:141332) (PDE) 中所见。传统的[神经网](@entry_id:276355)络在特定的数据网格上训练，当分辨率改变时就会失效，这从根本上限制了它们捕捉这些底层物理定律的能力。这一差距凸显了对一种新方法的需求，这种新方法能够学习一个系统永恒的、连续的规则，而与我们选择测量它的方式无关。

本文介绍了算子学习，这是一个旨在学习算子本身的革命性[范式](@entry_id:161181)。通过这样做，这些模型可以独立于数据的离散化方式进行操作，使其能够跨越不同分辨率进行泛化，并实现零样本超分辨率等应用。在接下来的章节中，我们将深入探讨这个强大的概念。第一章 **“原理与机制”** 将揭示使在无穷维中学习成为可能的数学理论，并探讨两种基石模型的架构：[深度算子网络](@entry_id:748262) ([DeepONet](@entry_id:748262)) 和[傅里叶神经算子 (FNO)](@entry_id:749541)。随后，**“应用与跨学科联系”** 一章将展示这些学习到的算子如何被用于构建数字孪生、驱动科学发现、设计未来技术，并与动力系统理论中的深刻概念建立联系。

## 原理与机制

### 一种新的学习方式：从函数到算子

几十年来，机器学习的奇迹在于其学习函数的能力。我们向[神经网](@entry_id:276355)络展示一百万张猫的图片，它就能学习一个函数，将新图片的像素向量映射到一个表示其为猫的概率的单一数字。这本质上是学习一个从高维空间到低维空间的映射，比如从 $\mathbb{R}^n$ 到 $\mathbb{R}^m$。这一[范式](@entry_id:161181)取得了惊人的成功。但是，当我们将目光从识别猫转向破译宇宙时，我们便会碰壁。

自然法则不是以有限向量之间的映射形式写成的。它们是用微积分的语言——以[微分方程](@entry_id:264184)的形式——写成的。它们描述的是*函数*之间的关系。想象一根吉他弦。它的初始形状是一个函数 $u_0(x)$，描述了其长度上每个点 $x$ 的位移。控制其[振动](@entry_id:267781)的物理定律是一个**算子**——一种抽象的机器——它将这个初始函数 $u_0(x)$ 作为输入，并产生一个新函数 $u(x, t)$，该函数描述了琴弦在未来任何时间 $t$ 的形状。[偏微分方程](@entry_id:141332) (PDE) 的解正是如此：一个将某些输入函数（[初始条件](@entry_id:152863)、边界条件或[源项](@entry_id:269111)）映射到输出解函数的算子 [@problem_id:3337943]。

传统方法的问题就在于此。如果我们通过将[振动弦](@entry_id:138456)离散化为100个点来模拟它，并训练一个[神经网](@entry_id:276355)络来预测其运动，那么这个网络学到的是一个针对100维向量的映射。如果我们想用1000个点进行更精确的模拟，我们的网络就毫无用处了。它是在特定的离散化方案下训练的，对底层的连续实体毫无概念。这种对网格的依赖性是一个根本的限制 [@problem_id:3407177] [@problem_id:3583435]。

因此，算子学习代表了一次宏大的雄心转变。我们的目标不是为单个网格学习一个一次性的近似，而是学习算子本身——那永恒的、连续的法则。我们希望构建一个像大自然一样，将整个函数作为输入并返回另一个函数作为输出的[神经网](@entry_id:276355)络。这样一个学习到的算子将是**离散不变的**。你可以用低分辨率的模拟来训练它，然后用它来预测在更精细的网格上，甚至在你希望的任何连续空间点上的结果。这就好比是记住 $123 \times 456$ 的答案与学习[乘法算法](@entry_id:636220)本身之间的区别。

### 无穷维的秘密：打破维度诅咒

乍一看，这个雄心似乎有些鲁莽。函数是无穷维的对象。一个有限的计算机，从有限数量的样本中学习，怎么可能学会它们之间的映射呢？这难道不会成为“[维度灾难](@entry_id:143920)”的终极受害者吗？（维度灾难指所需数据量随维度呈指数级增长）。

摆脱这个悖论的出路在于一个关于控制物理世界的算子的美丽秘密：虽然它们在无穷维空间中运作，但它们的本质行为通常出人意料地是低维的。

物理学中的许多算子，特别是那些涉及[扩散](@entry_id:141445)、平滑或积分的算子，在数学上被称为**紧算子**。一个紧算子有一个显著的特性，可以通过与数据分析的类比来理解。在分析复杂数据集时，我们经常使用主成分分析 (PCA) 来找到最重要的变化方向。我们仅用少数几个主成分就可以捕捉到数据集的大部分结构。

一个[紧算子](@entry_id:139189)具有类似的结构，这通过**[奇异值分解 (SVD)](@entry_id:172448)** 得以揭示。SVD 将[算子分解](@entry_id:154443)为沿着“主”输入和输出函数（称为[奇异函数](@entry_id:159883)）的一系列简单作用。每个作用都由一个称为奇异值的数字加权。对于大量的物理算子，这些奇异值会**迅速衰减** [@problem_id:3407216]。第一个奇异值可能很大，第二个较小，第三个更小，依此类推，迅速趋近于零。这意味着算子的行为由其前几个奇异分量主导。它在技术上可能是无穷维的，但它却是*近似低秩*的。

这就是解开这个谜题的关键。我们不需要学习算子在每一种可以想象的输入函数上的行为。我们只需要学习它在少数几个起决定性作用的“主”函数上的作用。问题的[有效维度](@entry_id:146824)不是无穷大，而是将算子近似到我们期望的精度 $\varepsilon$ 所需的少数奇异分量个数 $r$ [@problem_id:3407216]。这个有效秩 $r$ 取决于奇异值衰减的速度，而不是我们恰好所处的空间的维度。[统计学习理论](@entry_id:274291)证实了这一直觉，表明学习这样一个算子所需的样本数量与这个有效秩 $r$ 成正比，而不是与我们网格的环境维度成正比 [@problem_id:3407216]。[维度灾难](@entry_id:143920)被打破了。

### 两种主要策略：[DeepONet](@entry_id:748262) 和 FNO

知道了算子学习在原则上是可行的，我们实际上如何构建一个[神经网](@entry_id:276355)络来做到这一点呢？两种卓越的策略应运而生，每一种都体现了深刻的数学思想。

#### [DeepONet](@entry_id:748262)：通用构建器

第一个架构是**[深度算子网络](@entry_id:748262) ([DeepONet](@entry_id:748262))**，它基于逼近理论中的一个经典思想：任何合理的函数都可以表示为一些[基函数](@entry_id:170178)的加权和。例如，声波可以表示为[傅里叶级数](@entry_id:139455)中的正弦和余弦之和。[DeepONet](@entry_id:748262) 学会动态地发现这种表示。

它通过一个由**分支网络** (branch network) 和**主干网络** (trunk network) 组成的巧妙双重架构来实现这一点 [@problem_id:3513285]。

-   **分支网络**是“传感器”。它观察*输入函数*（例如，通过在几个固定位置上对其进行采样），其任务是计算[基展开](@entry_id:746689)的*系数*。它回答了这样一个问题：“对于这个特定的输入函数，我需要多少每种[基函数](@entry_id:170178)？”

-   **主干网络**是“模式生成器”。它接受输出域中的一个*坐标*（比如空间中的一个点 $x$），其任务是在该点上产生*[基函数](@entry_id:170178)*的值。它回答了这样一个问题：“我的基模式在这里看起来是什么样的？”

最终的预测就是两个网络输出的[点积](@entry_id:149019)：$G(u)(y) \approx \sum_{k=1}^{p} b_k(u) t_k(y)$，其中系数 $b_k$ 来自处理输入 $u$ 的分支网络，[基函数](@entry_id:170178)的值 $t_k$ 来自处理输出坐标 $y$ 的主干网络。这种优雅的结构是数学家所称的可分离近似的直接实现。

这种设计的强大之处在于其巨大的灵活性。如果问题依赖于其他参数——比如变化的[扩散](@entry_id:141445)系数，甚至变化的域几何形状——我们可以简单地将这些信息输入到相应的网络中。定义整个问题实例的全局参数进入分支网络；描述查询点周围空间的局部特征进入主干网络 [@problem_id:3407225]。

#### [傅里叶神经算子](@entry_id:189138)：波的主宰

第二种策略是**[傅里叶神经算子 (FNO)](@entry_id:749541)**，其灵感来自一个不同但同样深刻的原理：卷积定理。许多物理过程，如热的传播，都是由卷积描述的。卷积是一种运算，其中某一点的输出是其周围输入的加权平均，权重由一个“核”函数定义。

虽然物理空间中的卷积计算成本可能很高，但卷积定理告诉我们，在**傅里叶空间**中，这个复杂的操作变成了简单的逐元素乘法。一个 FNO 层巧妙地利用了这一点 [@problem_id:3337935]：

1.  **变换**：它接受一个输入函数（在网格上表示）并使用[快速傅里叶变换 (FFT)](@entry_id:146372) 计算其[傅里叶变换](@entry_id:142120)。这将函数转换为其组成频率或“模式”。

2.  **滤波**：在傅里叶空间中，它将低频模式的一个[子集](@entry_id:261956)与一组学习到的权重相乘。这是 FNO 的核心，它在这里学习卷积核的谱特征。[高频模式](@entry_id:750297)通常被丢弃，这具有正则化效果，类似于假设映射是平滑的。

3.  **[逆变](@entry_id:192290)换**：它应用逆 FFT 将滤波后的模式变换回物理空间，从而得到学习到的全局卷积的结果。

这个序列——FFT、学习到的线性变换、逆 FFT——被称为**谱卷积**。在这些谱卷积层之间，应用一个简单的、逐点的[非线性激活函数](@entry_id:635291)。这种[非线性](@entry_id:637147)至关重要；它允许 FNO 构建对高度复杂、非线性算子的近似，远远超出了简单的卷积 [@problem_id:3337935]。

FNO 的天才之处在于，学习到的权重位于傅里叶空间，独立于网格分辨率。这意味着我们可以在一个粗糙的 $64 \times 64$ 网格上训练模型，并且因为核是在连续的傅里叶域中定义的，所以我们可以在测试时将其应用于一个精细的 $1024 \times 1024$ 网格上，而无需任何重新训练。这种特性，有时被称为零样本超分辨率，是在离散不变的方式下学习算子的直接结果 [@problem_id:3513285]。

### 教授物理定律

从数据中学习是强大的，但如果数据有噪声或不完整怎么办？我们如何确保我们学习到的算子尊重我们已知为真的基本物理定律？这就是我们从纯数据驱动学习转向[物理信息](@entry_id:152556)算子学习的地方。

一个微妙但深刻的考虑是算子和数据之间的一致性检查，称为**[皮卡条件](@entry_id:753438) (Picard condition)**。从本质上讲，它指出，对于一个稳定的学习过程，训练数据对于被学习的算子必须“有意义”。一个平滑事物（如热扩散）的算子具有迅速衰减的奇异值。如果我们试图用平滑输入映射到噪声输出的数据来训练它，学习到的算子将被迫放大高频分量，变得不稳定且对泛化无用。一个成功的学习算子必须使其结构，特别是其奇异值衰减，与训练数据的统计特性相匹配 [@problem_id:3419555]。

更直接地，我们可以将物理约束直接融入训练目标中。想象一下学习[不可压缩流体](@entry_id:181066)（如水）的算子。一个基本法则是[速度场](@entry_id:271461) $u$ 必须是无散度的：$\nabla \cdot u = 0$。我们可以通过在其损失函数中增加一个惩罚项来将这一点教给我们的[神经算子](@entry_id:752448)。在衡量与训练数据误差的常规项旁边，我们增加一个项，比如 $\rho \|\nabla \cdot u_{\text{pred}}\|^2$，该项惩罚网络任何时候产生可压缩的[速度场](@entry_id:271461)。通过最小化这个组合损失，网络学会找到一个不仅拟合数据而且遵守物理定律的解 [@problem_id:3426978]。这种方法不同于[物理信息神经网络](@entry_id:145229) (PINN)，后者通常学习*单个* PDE 实例的解；在这里，我们是以物理信息的方式学习一族问题的整个*算子* [@problem_id:3337943]。

这些原则甚至指导我们克服实际挑战。最纯粹形式的 FNO 是为周期性域（如甜甜圈的表面）设计的。为了将其应用于现实世界的[非周期性](@entry_id:275873)问题，我们可以使用巧妙的策略：我们可以通过将其重新表述为具有齐次（零）边界条件的问题来“提升”问题，或者我们可以完全用更适合有界域的基（如[切比雪夫多项式](@entry_id:145074)）替换[傅里叶基](@entry_id:201167)，从而将[傅里叶神经算子](@entry_id:189138)变成切比雪夫[神经算子](@entry_id:752448) [@problem_id:3407244]。在每种情况下，核心思想保持不变：通过理解物理和数学的深层原理，我们可以设计出不仅功能强大，而且优雅且忠实于自然世界结构的学​​习架构。

