## 引言
随着人工智能日益融入医疗等关键领域，确保这些系统值得我们信赖的需求变得前所未有的紧迫。单凭高性能是不足够的；我们必须要求人工智能是安全、透明、公平和可问责的。然而，许多强大的人工智能系统以“黑箱”方式运行，这为信任建立了根本障碍，并引发了关于责任和偏见的复杂问题。本文通过提供一个构建可信人工智能的全面框架来应对这一挑战。第一章“原则与机制”确立了可信人工智能的基本支柱，解构了可解释性、问责制、安全性和公平性等概念。随后，“应用与跨学科联系”展示了这些抽象原则如何应用于解决医疗领域中复杂、真实的难题，将人工智能从一个神秘的工具转变为可靠的人类伙伴。

## 原则与机制

想象一下，有人请你信任一座新桥。你不仅想知道大多数汽车都能成功过桥，你还想看到设计蓝图，了解材料经过了测试，明白承重限制，并确定有明确的检查和维护程序。你想知道如果桥梁倒塌，谁应承担责任。构建一个值得我们信赖的人工智能系统，尤其是在医疗等高风险领域，并无不同。这不仅仅要求在平均表现上令人印象深刻，更要求对安全性、透明度、问责制和公平性有深刻的、根本性的承诺。这并非是给一个神秘的“黑箱”添加一些令人安心的功能，而是一种设计哲学，一门严谨的工程学科，以及一种人与机器之间的新型伙伴关系。

### 从黑箱到玻璃箱：对[可解释性](@entry_id:637759)的追求

当今许多最强大的人工智能系统都以“黑箱”方式运行。我们给它们输入数据，它们产生一个答案，但从输入到输出的复杂计算网络如同一个迷宫，即使对其创造者来说也是不透明的。这种不透明性是信任的根本障碍。如果一个人工智能无法解释其推理过程，医生如何能信任它的建议？如果我们不知道人工智能为什么会犯错，我们如何能修复它？对于一个我们无法理解的决定，我们又如何能让任何人承担责任？

通往可信人工智能的旅程始于拆解这些黑箱，或者至少在上面安装窗户。这就是**可解释性**的范畴，但至关重要的是要理解，“一个解释”并非单一事物。我们需要什么样的解释完全取决于谁在问，以及为什么问。

考虑一个旨在帮助医生选择正确抗生素的人工智能。该系统旨在平衡个体患者的需求与[抗生素耐药性](@entry_id:147479)的公共卫生危机 [@problem_id:4436711]。患者和医生在共同对话中可能会问：“为什么人工智能推荐抗生素A，而不是我通常服用的抗生素B？”他们需要一个**对比性解释**，即一个能够阐明具体权衡的解释。例如：“系统选择抗生素A，是因为尽管预测其对您个人的疗效稍差 ($E(a,x)$)，但它导致群体层面耐药性 ($R(a)$) 的风险要低得多，这是医院政策优先考虑的权衡。”这种解释阐明了系统中嵌入的价值观，使其可见并可供讨论。

医生可能会有不同的问题，源于临床好奇心和提前规划的愿望：“我的患者病情需要发生什么变化，人工智能才会推荐抗生素B？”这需要一个**反事实解释**。答案可能是：“如果患者测得的肾功能 $c_{\mathrm{cr}}$ 降至特定阈值以下，系统会将其推荐切换为抗生素B。”这揭示了模型对特定临床数据的敏感性，突出了需要密切关注的参数，并将人工智能从一个黑箱神谕转变为一个互动的思维工具。

最后，负责验证人工智能的监管者和科学家会有一个更深层次的问题：“人工智能的内部逻辑是否与已建立的医学科学相符？”他们需要一个**机制性解释**，例如，该解释需表明模型的药物疗效计算是基于药代动力学的现实世界原则，并且其耐药性风险模型与已知的进化动力学相符。

这种分层解释方法 [@problem_id:4411879] 是真正透明度的核心。它并非要求彻底公开源代码或专有数据，因为这可能损害知识产权和患者隐私。它的核心在于向正确的受众提供恰当深度的洞察，从而实现有意义的理解和安全、有效的使用。

### 责任链：算法时代的问责制

如果人工智能卷入医疗差错，谁应受责备？是人工智能吗？是听从其建议的医生？是购买它的医院？还是构建它的开发者？这个问责问题不是一个哲学上的清谈游戏；它是任何可信系统的关键支柱。

让我们通过一个困难但现实的场景来探讨这个问题：在一个姑息治疗病房，一个人工智能工具建议为一名患有难治性疼痛的患者实施适度的姑息镇静。主治临床医生审查了这一建议，与患者讨论后，以符合指南的方式执行了该决定。之后，家属要求知道谁应为此负责 [@problem_id:4423636]。

这里最深刻的洞见是，人工智能本身永远无法承担责任。人工智能是一个工具——尽管是一个极其复杂的工具，但终究只是一个工具。它没有道德主体性，没有意图，也没有能力“承担责任”。因此，问责完全属于人类范畴，分布于系统中的各个参与者之间。为了理清这一点，我们必须精确用语：

- **可应答性 (Answerability)** 是提供理由和解释的义务。人工智能的*开发者*对其工具的技术设计和安全保证负有应答责任。*临床医生*对患者及其家属负有应答责任，需解释其临床判断和最终决定背后的理由。

- **问责 (Accountability)** 是一个更广泛的、基于角色的义务，要求治理系统并对结果负责。*临床医生*对临床决策保留主要问责责任，因为他们是必须行使独立判断的执业专业人员。*机构*（医院）对人工智能系统的负责任采购、部署和监控负有问责责任。

- **法律责任 (Liability)** 是一个法律概念，指如果违反注意义务并造成损害，将面临制裁的风险。法律责任只会附属于人类参与者之一——临床医生、机构或开发者——前提是能够证明存在疏忽或缺陷。仅仅有人工智能的推荐并不自动产生或转移法律责任。

这个框架表明，人工智能并没有消除责任，而是折射了责任。为了管理这一点，我们必须构建使这条责任链清晰可追溯的系统。一个真正可问责的系统包括一个强大的**审计追踪**，它不仅记录人工智能的最终建议，还记录其使用的关键输入特征、其[置信度](@entry_id:267904)、临床医生是否推翻了建议，以及至关重要的，临床医生自己对其最终决定的理由 [@problem_id:4862075]。问责不是一个抽象的理想；它是一个必须从一开始就设计到系统中的工程特性。

### 为安全而设计：从预防错误到构建韧性

俗话说，“人非圣贤，孰能无过”。但在工程学中，尤其是在人工智能领域，我们必须补充一句：“算法亦会出错”。算法和人一样，不可避免地会遇到它们未经训练处理的情况或犯下错误。一个可信的系统不是一个永不失败的系统，而是一个其失败能被理解、被限制并被安全管理的系统。最稳健的安全方法不是简单地期望最好的结果，而是主动地为韧性而工程设计。

这门学科在航空和土木工程等领域早已实践，它提供了一个强大的[控制层级](@entry_id:199483)，我们可以直接应用于人工智能。让我们考虑一个实际例子：一个用于家庭的AI驱动的血压袖带和智能手机应用 [@problem_id:4429047]。一个关键风险是用户可能错误地佩戴袖带（例如，在手臂上过低的位置），导致读数错误地偏低，从而使人工智能错过了高血压危象，可能导致中风等伤害。我们如何控制这种风险？

1.  **本质安全设计：** 这是最强大的安全形式。不要只是警告用户问题所在，而是通过设计将问题消除。我们可以重新设计袖带，加入触觉提示，使其能够直观地正确佩戴。更好的是，配套应用可以利用手机摄像头分析用户的手臂位置，并在袖带正确放置前拒绝进行测量。这可以从根本上防止错误发生。

2.  **防护措施：** 如果无法消除危害，就建立一个防护盾。应用的软件可以分析血压信号本身的质量。如果信号嘈杂或具有袖带错位的特征，软件联锁可以阻止人工智能发出令人安心的“一切正常”消息，而是提示用户重新测量。即使初始错误发生，这个防护层也能控制住损害。

3.  **安全信息：** 这是最后一道防线。它包括清晰的说明、屏幕上的警告和弹出提醒，告诉用户将袖带保持在心脏水平。虽然这是必要的，但这是最弱的方法，因为它依赖于用户总是看到、记住并遵守指示。

这种系统化、分层级的方法是工程安全的精髓。它使我们从被动地“打地鼠”式修复错误，转向主动的[风险管理](@entry_id:141282)文化，正如ISO 14971和IEC 62304等正式标准所规定的那样 [@problem_id:4425866]。信任并非建立在对人工智能完美的信念之上，而是建立在一个严谨而系统的安全流程的证据之上。

### 公平性议题：超越平均准确率

在创建可信人工智能的过程中，最微妙和深刻的挑战或许在于**公平性**这一概念。一个人工智能对于总人口的平均准确率可能很高，但却可能系统性地、危险地对特定的、通常是弱势的子群体产生偏见。一个对某一人口群体表现出色但在另一群体上失败的诊断工具，不仅仅是一个技术缺陷；它是一个制造不平等的引擎。

迈向公平人工智能的第一步是认识到“公平性”并非单一、简单的数学属性。它是一个充满争议的伦理概念，不同的正义哲学导致我们人工智能系统的设计也不同。想象一个旨在帮助在大规模伤亡事件中对患者进行分诊的人工智能，此时一种维持生命的资源稀缺 [@problem_id:4421123]。它应该如何排定优先级？

- 一个**平等主义**框架，寻求减少不公正的不平等，可能会要求当临床因素相同时，人工智能必须确保获取资源的机会不受患者结构性劣势的影响。它甚至可能使用抽签来决定临床相似患者的先后，确保每个人都有平等的机会。

- 一个**优先主义**框架会给予最差境遇者所获得的益处额外的权重。一个以此原则设计的人工智能可能会给来自有显著社会剥夺背景的患者的优先级分数一个“提升”，其原则是，给予他们的益处在伦理上更有价值。

- 一个**充足主义**框架旨在确保尽可能多的人达到一个“足够好”的结果。使用这种逻辑的人工智能可能会优先考虑那些低于生存关键阈值但该资源有很高机会将他们提升到阈值之上的患者。

这里没有唯一的“正确”答案。选择将哪种正义原则嵌入人工智能是一个社会和伦理的决定，而非纯粹的技术决定 [@problem_id:4443481]。但一旦选定了原则，我们就可以将其编码到算法本身。考虑一个[联邦学习](@entry_id:637118)系统，其中一个人工智能在一个诊所网络中进行训练，其中一些诊所规模大、资源充足，而另一些则是服务少数族裔的小型诊所 [@problem_id:4400748]。一个简单的平均会让大诊所主导最终的模型。但我们可以设计一个“公平性正则化的聚合器”。通过给予那些训练信号更稳定、更可靠的诊所（在此情景下是服务少数族裔的诊所）的更新更大的权重，我们可以在数学上放大它们的声音。这确保了最终模型对它们所服务的群体表现公平。这就是设计伦理，将抽象的正义原则转化为具体的代码行。

### 系统中的人：从环路到伙伴关系

最后，通往可信人工智能的道路将我们带回起点：它旨在服务的人类。尽管人工智能系统功能强大，但它们从根本上是有限的。在电子健康记录上训练的模型可能能够以很高的准确性预测临床结果，但它们对构成人类生活的丰富关系背景仍然视而不见 [@problem_id:4410369]。患者的价值观、他们的家庭支持系统、他们的恐惧和希望、他们对自己病情的理解——这些因素在数据中往往是不可见的，但对于良好的护理却至关重要。

这种根本性的盲点揭示了简单的“人在环路中”模型的不足，在该模型中，临床医生仅仅是签署人工智能的建议。我们需要更深层次的整合，一种真正的伙伴关系。这就是**参与式治理**背后的理念。那些受人工智能决策影响最大的人——患者、家属和社区成员——必须作为合作伙伴被纳入人工智能的整个生命周期。他们是唯一能提供缺失背景的人。他们是唯一能告诉我们，当人工智能经过优化的、数据驱动的目标开始偏离真正的人类价值观时。通过创建提升患者叙事并提供易于理解的解释的机制，我们可以对抗**认知不公**——即一个系统的逻辑可能会忽略或贬低一个人关于自己经历的证词的风险 [@problem_id:4862075]。

因此，构建可信人工智能并非是追求构建一个完美的、自主的智能体。这是一个将技术编织到人类关系和社会价值观结构中的过程。它要求我们的系统不仅是可解释、可问责和安全的，而且是公正的，并深深尊重它们所服务的人。通往可信人工智能的旅程，归根结底，就是让我们的技术变得更全面、更美好地人性化的旅程。

