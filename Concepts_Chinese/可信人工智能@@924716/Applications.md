## 应用与跨学科联系

在我们之前的讨论中，我们探讨了可信人工智能的基础原则——安全性、问责制、公平性和透明度这些抽象的支柱。这些原则就像物理定律，为描述一个系统应如何行为提供了一套普适语法。但正如物理学真正的激动人心之处在于看到这些定律如何在星系的旋转或量子粒子的奇异舞蹈中显现，可信人工智能的真正意义也只有当我们在它应对现实世界中那些复杂、高风险且极具人性的问题时才能揭示。

现在，我们将踏上那段旅程。我们将从抽象走向具体，探索这些原则如何在复杂的医疗生态系统中得到应用。在这里，人工智能不仅仅是一串代码，而是临床医生手中的一种新仪器——一种有潜力看到前所未见之物，但也要求使用者具备新的智慧和责任水平的仪器。

### 诊断助手：增强之眼，人类之判断

人工智能在医学中最直接的承诺之一是成为一个不知疲倦的诊断助手，一个能够扫描数千张图像或数据点，标记出可能逃过人眼的微妙模式的伙伴。想象一个人工智能，旨在帮助眼部肿瘤学家对眼底的色素性病变进行分诊，寻找罕见但致命的葡萄膜黑色素瘤 [@problem_id:4732277]。人们可能梦想一个永不犯错的完美人工智能。但现实更为微妙，也远为有趣。

即使是一个非常精确的人工智能——一个能正确识别绝大多数癌性和良性病变的人工智能——也难免会犯错。由于该疾病罕见，一个简单的统计学事实便显现出来：人工智能发出的大多数警报最终都会是[假阳性](@entry_id:635878)。如果临床医生不加质疑地对每一个人工智能警报采取行动，他们将让许多健康的患者承受不必要的焦虑和侵入性的后续检查。反之，过度依赖人工智能的“一切正常”信号，可能导致在少数模型遗漏的案例（假阴性）中，灾难性地未能诊断出真正的癌症。

在这里，我们看到了可信人工智能在实践中的第一个优美原则：解决方案不是一个完美的算法，而是一个完美的*伙伴关系*。人工智能不是神谕；它是一个强大但会犯错的初级伙伴。它的角色是进行初步的、详尽的筛选。人类专家的角色——永远无法被自动化取代——是提供最终的判断，用他们全部的经验和背景理解来审查人工智能的*所有*发现，无论是阳性还是阴性。人工智[能标](@entry_id:196201)记可能性；人类确定现实。真正的安全源于这个无缝的、人在环路中的系统，其中机器的优势与心智的长处被编织在一起。

这种人机团队的概念不仅仅是一个哲学理想；它必须被精心设计到临床工作流程中。考虑一个针对心力衰竭的远程患者监护项目，其中人工智能筛选来自可穿戴设备的数据，一个护士团队对警报进行分诊，而一名医生负最终责任 [@problem_id:4955123]。谁做什么？谁负责对警报采取行动？如果遗漏了什么，谁应被*问责*？答案不能靠运气。它需要一个深思熟虑的编排，一个精确的角色映射，例如一个责任-问责-咨询-知情（RACI）矩阵。这种社会技术设计确保每个任务都有一个明确的负责人，并且人工智能的角色是支持，而非取代，那些承担最终注意义务的执业专业人员。信任不仅仅是编码到人工智能中；它被设计到团队的结构本身。

### 公平性的挑战：看透数据之影

人工智能从我们给它的数据中学习世界。但数据并非现实本身；它只是现实投下的影子，和任何影子一样，它可能被扭曲。一个天真地信任这些影子的人工智能将对世界形成扭曲的看法，其方式往往会固化甚至放大现有的人类偏见。这就是公平性的挑战。

想象一个人工智能系统，旨在为患者分配稀缺的护理管理资源 [@problem_id:4421550]。该模型基于历史医疗保健使用数据进行训练，这似乎是一个合乎逻辑的需求代表。它很快发现一个模式：经历住房和食品无保障的患者历史上的医疗成本很低。一个天真的人工智能，为了优化成本预测，会得出结论，认为这个群体健康且风险低，从而拒绝向他们提供他们迫切需要的资源。数据的影子是一个谎言。现实是，这些人有很高的*需求*，但面临巨大的*获取障碍*，这就是为什么他们的使用率很低。

一个可信的人工智能必须足够聪明，能够识别其数据何时具有误导性。这里的优雅解决方案不是丢弃数据，而是从根本上重构问题。我们不再要求人工智能预测“成本”，而是要求它预测“未满足的需求”或“可避免的伤害”。这需要一种更深层次的思维模式，一种整合世界知识——在这种情况下是健康的社会决定因素——来纠正数据内在偏见的模式。

同样，如果不加制约，人工智能驱动的个性化逻辑在其他领域也可能导致严重的不公平结果。例如，用于健康保险的人工智能可能成为一个完美的歧视引擎。它可能学会以如此精确的方式计算个人的健康风险，以至于给那些最病弱的人分配高得令人难以承受的保费，从而完全瓦解了作为保险概念基石的风险共担原则。在这种背景下，可信度要求我们将社会价值观直接强加于算法。我们可以内置明确的公平性约束，例如设置上限和下限，限制个人保费偏离社区平均水平的程度 [@problem_id:4403238]。这是一个有意识的决定，将团结的伦理原则置于纯粹、无约束的优化之上。这是一个强有力的例子，说明我们如何利用人工智能的架构来强制执行公平性，并建立一个更公正的世界。

### 维护自主权：算法中的患者之声

医学中或许最神圣的原则是尊重患者的自主权——他们决定自己道路的权利。一个可信的人工智能必须被设计为一种赋权的工具，而非控制的工具，它应能放大患者的声音并尊重他们的价值观。

思考一下姑息治疗这个困难且情感丰富的领域 [@problem_id:4423597]。一位88岁的晚期痴呆症患者，患有多种疾病，发展为危及生命的败血症。一个基于数百万病例训练并体现了“拯救败血症运动”最新证据的人工智能，推荐了一套积极的治疗方案：使用升压药、转入ICU等等。从纯粹的统计学角度看，这是最大化生存率的“正确”行动。但是这位患者，当他尚有决定能力时，已经通过“不进行心肺复苏”（DNR）指令和其他明确的治疗限制表明了他的意愿。他明确的目标是舒适，而不是不惜任何代价地生存。

这里蕴含着一个深刻的教训。一个可信的人工智能不是那个知道最多的，而是那个知道自己位置的。它的设计必须在人类价值观设定的硬性约束内运行。人工智能的推荐算法必须服从于患者记录在案的意愿，过滤掉任何会违反其指令的行动。这里的美妙之处在于系统的谦逊——它能够认识到数学上的最优路径并不总是人性上的正确路径。

这一原则远远超出了临终关怀的范畴。在为残障人士设计人工智能时，我们可以借鉴“能力方法”的强大思想，该方法主张一个公正社会的目标是扩展人们真正能够*成为*和*做到*的事情 [@problem_id:4416897]。在这种观点下，一个可信的人工智能不仅仅是满足一份可访问性功能清单的工具。它是一个真正增强个人能动性和社会参与度的工具——增强他们以自己的方式沟通、导航环境、做出知情决定和控制自己隐私的能力。它成为他们蓬勃发展的伙伴。

对这种自主权承诺的最终考验出现在处理最弱势群体时。想象一个人工智能，它筛选青少年的公开社交媒体帖子，以预测即将来临的自残风险 [@problem_id:4434259]。其拯救生命的潜力是巨大的。然而，冷酷地审视统计数据会揭示一个 sobering 的真相：由于真正的危机很少见，绝大多数警报都将是假警报。由算法触发的自动干预可能会对大量年轻人造成巨大的伤害、创伤和污名。在这种脆弱的背景下，信任不能单独寄托于算法。它必须由一个多层次的、以人为中心的保障措施编织而成：一个需要父母许可和青少年本人同意的明确选择加入流程；使用先进的隐私保护技术；以及最关键的，在进行任何接触之前，必须有一名人类临床医生作为富有同情心、深思熟虑的守门人，这是不可协商的要求。

### 搭建支架：新前沿，新规则

可信人工智能不是单个算法的属性；它是其运行所在的整个社会技术系统的一个涌现属性。在我们开发这些强大的新工具的同时，我们必须同步构建能够支持它们的制度和法律支架。

我们正处于革命性应用的边缘，例如*计算机模拟临床试验*，其中新药可以在招募任何人类受试者之前，在大量的“数字孪生”队列上进行测试 [@problem_id:4426232]。为了使这成为一种可信的证据形式，我们必须为这些虚拟试验注入其现实世界对应物的所有科学严谨性：一个预先设定的方案、具有临床意义的终点，以及通过为每个数字孪生模拟反事实结果来创建的恰当[对照组](@entry_id:188599)。

建立信任还意味着我们的职业准则和机构必须与时俱进 [@problem-id:4843273]。几个世纪以来指导医学的永恒伦理原则仍然是我们的北极星，但我们需要新的地图来导航人工智能和大数据的新领域。这意味着要为模型治理、确保[数据溯源](@entry_id:175012)、为人工智能的输出提供有意义的解释以及合乎道德地共享数据制定稳健的标准。

最后，我们到达了科幻小说与临床现实交汇的前沿。当一个数字孪生，一个关于我的计算模型，在我无法再为自己发声时，它能代表我说话吗？[@problem_id:4405923]。这个问题挑战了我们关于自我、意愿甚至生命本身的法律定义的边界。明智的前进道路不是授予这些人工智能构造物法律人格，而是 painstaking 地构建一个适合21世纪的新法律工具：“数字预立医嘱”。这将是一个框架，一个人在具有完全行为能力和法律形式的情况下，可以指定其自己经过验证和审计的数字模型作为表达其意愿的方式。这样一个与医疗设备同等严肃监管的系统，代表了可信设计的顶峰——一个旨在即使面对我们最先进的技术也能尊重人类自主权的密码学、法律、伦理和计算机科学的融合。

通往可信人工智能的旅程不仅仅是技术之旅。它是一次内省之旅，是定义我们的价值观并将其嵌入我们机器逻辑的旅程。这是一种认识，即最终目标不是构建一个更智能的AI，而是在我们用它构建世界的过程中变得更明智。