## 引言
我们如何教会机器识别它从未见过的事物？这个问题对于构建真正智能和自适应的系统至关重要，它超越了简单的[模式匹配](@article_id:298439)，迈向了一种更类似人类的类比推理形式。[零样本分类](@article_id:641658)为此提供了一个优雅的答案，它构建了一个框架，让模型能够通过丰富、描述性的信息而非带标签的样本来识别新的类别。这种泛化到未知事物的能力是人工智能领域的一大关键飞跃，打破了传统上对庞大、特定任务数据集的依赖。

然而，这种能力似乎近乎魔术。模型是如何在抽象描述与图像或声音等具体数据之间架起桥梁的？本文将通过剖析其核心组成部分来揭开这一过程的神秘面纱。我们将首先探讨其基础的“原理与机制”，深入研究共享语义空间的概念、从基于属性的系统到现代语言提示模型的演变，以及学习过程本身的细微之处。然后，在“应用与跨学科联系”部分，我们将见证这项技术在不同科学技术领域的卓越影响力。通过这次探索，您不仅将全面了解[零样本分类](@article_id:641658)的工作原理，还将理解为何它代表了机器学习领域的一次[范式](@article_id:329204)转变——使我们能够构建更灵活、知识更渊博、适应性更强的 AI 系统。

## 原理与机制

想象你是一位毕生研究马匹的探险家。你了解关于马的一切：它们的形态、声音、动作。一天，你收到一位在非洲的同事发来的电报，描述了一种新动物：“它的构造和马一模一样，”电报上写道，“但身上布满了黑白条纹。”即便你从未见过这种动物，你很可能也能从一群动物中认出“斑马”。你刚刚完成了一次[零样本学习](@article_id:639506)。你将已有的视觉知识（“马”）与一条新的描述性信息（“有条纹”）结合起来，识别出了一种你从未见过的东西。

这种简单的类比推理行为，正是**[零样本分类](@article_id:641658)**背后优美而核心的思想。其核心在于教会机器做同样的事情：不是通过一组带标签的样本来识别新概念，而是通过描述。为实现这一点，我们需要三个关键要素：一个理解事物*看起来*像什么的模​​型（图像模型），一个理解事物*是*什么的模型（语言模型），以及一个它们可以相遇并共享信息的共同基础。

### 搭建桥梁：语义[嵌入空间](@article_id:641450)

这个“共同基础”是我们必须欣赏的第一个奇迹。在机器学习中，我们称之为**[共享嵌入空间](@article_id:638675)**。可以把它想象成一个巨大的、多维的图书馆，每个概念都有一个特定的位置，由一个[坐标向量](@article_id:313731)表示。在这个图书馆里，猫的图像被放在紧邻书面文字“猫”的架子上。狗的图像则靠近“狗”这个词。但更重要的是，相关的概念被聚集在一起。“猫”的区域靠近“狗”的区域，而这两个区域都属于更大的“哺乳动物”区，与大厅对面的“交通工具”区截然不同。

我们如何定义这些位置呢？最早也是最直观的方法之一是通过**属性** [@problem_id:3125728]。我们可以通过一组特性来描述一种动物：它有毛皮吗？它有翅膀吗？它会游泳吗？它有条纹吗？一只“知更鸟”可能由坐标 `(有翅膀=1, 有毛皮=0, 会飞=1)` 表示。而一头“狮子”则是 `(有翅膀=0, 有毛皮=1, 会飞=0)`。

机器可以在一组已知的动物上进行训练——比如知更鸟和狮子。它学习一种映射关系，一种内部的 GPS，能将属性空间转换到视觉[特征空间](@article_id:642306)。它学习到 `(有翅天=1, ...)` 在图像中*看起来*像什么。现在，我们引入一个未见过的类别：“蝙蝠”。我们提供它的属性向量：`(有翅膀=1, 有毛皮=1, 会飞=1)`。尽[管模型](@article_id:300746)从未见过蝙蝠，但它可以使用其学到的 GPS 来预测具有这些属性的生物应该是什么样子。当一张新图像到来时，模型提取其视觉特征，并检查哪组属性提供了最接近的匹配。它找到了“蝙蝠”的属性，并做出了正确的分类。

当然，这个过程依赖于一个关键假设：已知类别的多样性必须足以让模型学习到有意义的映射关系。如果我们只在绿色物体上训练模型，它不可能学会“颜色：红色”这个属性是什么样子。这是统计学中一个深刻的思想，称为**[可识别性](@article_id:373082)**：你只能学习那些你有信息去区分的东西 [@problem_id:3125728]。

### 现代方法：从语言本身学习

为每一种可以想象的物体手动创建属性列表是乏味且常常是不可能的。 “民主”或“微积分”的属性是什么？[零样本学习](@article_id:639506)的现代革命来自一个绝妙的认识：我们已经有了一个用于描述事物的通用系统——自然语言。我们可以不使用结构化的属性，而是直接利用由强大的**[预训练](@article_id:638349)语言模型**捕获的单词和句子的含义 [@problem_id:3121759]。

在这种[范式](@article_id:329204)中，像“狗”这样的类别在我们语义图书馆中的“位置”，就是“狗”这个词的[向量表示](@article_id:345740)。分类过程就变成了一个匹配游戏。给定一张金毛寻回犬的图像，图像模型生成一个向量 $\mathbf{x}$。然后，我们将这个向量与“狗”、“猫”、“汽车”等词的文本向量进行比较。预测的类别是其文本向量 $\mathbf{w}_y$ 与图像向量具有最高**[余弦相似度](@article_id:639253)**的那个。从几何上看，这仅仅意味着我们正在寻找与图像向量指向最相似方向的文本向量 [@problem_id:3178397]。分类规则简单到只需找到使分数 $s_y(\mathbf{x}) = \mathbf{x}^\top \mathbf{w}_y$ 最大化的类别 $y$。

然而，这种优雅的方法有一个微妙的缺陷。语言是模棱两可的。考虑“bass”这个词。它指的是鱼还是乐器？如果我们只使用“bass”的[嵌入](@article_id:311541)，模型可能会感到困惑。一张男人拿着鱼的图像可能与“bass”和“trout”（鳟鱼）的文本[嵌入](@article_id:311541)同样相似，从而导致错误 [@problem_id:3125805]。

解决方案既简单又强大：提供上下文。我们不只使用“狗”这个词，而是使用像“一张狗的照片”这样的**提示**。这个短语帮助语言模型锁定预期的含义。对于我们的“bass”问题，使用“一种淡水鱼”的定义作为文本描述，可以立即解决歧义，并让模型正确地将鲈鱼与鳟鱼区分开来。

这种使用提示的思想在[情感分析](@article_id:642014)中以一种非常直观的形式体现出来 [@problem_id:3102497]。假设我们想在没有任何训练样本的情况下，将一篇电影评论分类为正面或负面。我们可以给语言模型一个提示，比如：“评论：‘这部电影绝对是一种享受。’ 这部电影很 [MASK]。” 然后我们问模型：哪个词最有可能填入 `[MASK]`？如果它以高概率预测“棒极了”、“优秀”或“有趣”，那么这篇评论很可能是正面的。如果它预测“糟糕”、“差劲”或“可怕”，那么它很可能是负面的。我们用来检查的词（“棒极了”、“差劲”等）被称为**标签词**（verbalizers），选择这些词是设计一个好的[零样本分类](@article_id:641658)器的关键部分。

### 精妙的学习艺术

这就引出了一个关键点：我们“提问的方式”至关重要。零样本模型的性能可能对提示的确切措辞高度敏感 [@problem_id:3125757]。 “一张狗的照片”可能比“一张狗的图像”效果更好，而后者又可能比单独使用“狗”效果更好。这种“提示敏感性”似乎像一门玄学，但有一些原则性的方法可以管理它。一种是**集成**：我们不依赖单一提示，而是尝试几种不同的措辞，并对它们的预测结果取平均值。这往往会产生一个更稳定、更准确的结果，就像征求一个委员会的意见通常比只问一个专家要好。

更好的是，如果我们有少数几个带标签的样本——也许只有五个或十个——我们可以做一些更聪明的事情。我们可以在这几个样本上测试一整套候选提示，并选择表现最好的那个。这种技术是**提示调整**的一种简单形式，它在[零样本学习](@article_id:639506)和**[少样本学习](@article_id:640408)**之间架起了一座桥梁 [@problem_id:3178397]。

这揭示了一个优美的学习层次结构，一个我们如何利用数据的谱系 [@problem_id:3195216]：

1.  **[零样本学习](@article_id:639506)**：你的任务*没有*任何带标签的样本。你完全依赖于[预训练](@article_id:638349)模型的知识，并通过精心设计的提示来引导它。

2.  **少样本上下文学习 (ICL)**：你有*少量*样本（比如 1 到 10 个）。你不需要重新训练模型。相反，你将这些样本直接打包到提示中。例如：“正面评论就像‘这太棒了！’。负面评论就像‘真是浪费时间。’。现在请分类这篇评论：‘这是一部杰作。’” 模型*即时*将这些样本用作类比。这是一种快速且出奇有效的适应方式。

3.  **少样本微调 (FT)**：你有更多一些的样本（也许 25 到 100 个）。现在，对模型进行一次小规模的“手术”是值得的，即基于这些样本稍微更新其参数。这比 ICL 计算成本更高，但通常会带来更好的性能，因为它进行了更持久的适应。

这里存在一种权衡。ICL 在样本极少时通常提供一个更好的起点，因为它不会冒着破坏模型庞大的[预训练](@article_id:638349)知识的风险。然而，其性能很快就会达到平台期。微调开始时风险更大，但有更高的上限；只要有足够的样本，它几乎总会超越 ICL。理解这些[学习曲线](@article_id:640568)使我们能够根据所拥有的数据量选择正确的策略 [@problem_id:3195216]。

### 智慧与警示：迁移的局限性

尽管[零样本学习](@article_id:639506)功能强大，但它并非万能灵药。它的成功取决于一个假设：新任务与模型已有的知识相关。如果你只用动物图像来训练模型，然后让它去分类星系类型，底层的“视觉语法”差异如此之大，以至于试图迁移知识实际上可能会损害性能。这种现象被称为**负迁移** [@problem_id:3125802]。一个明智的实践者会首先检查新任务的数据与模型训练数据之间的一致性。如果它们差异太大（通过它们平均[特征向量](@article_id:312227)之间的低[余弦相似度](@article_id:639253)来判断），那么坚持零样本方法并避免任何可能导致模型误入歧途的适应会更安全。

这种利用稳定、外部知识的能力也使得这些模型可能对不断变化的世界更具鲁棒性。想象一个在 2010 年训练的标准图像分类器。随着时间的推移，相机质量、摄影风格和图像内容会发生漂移——这被称为**域漂移**。分类器的性能会下降，因为它记住的模式不再完全有效。而一个零样本模型（在 [@problem_id:3160900] 中称为[范式](@article_id:329204) A）可能更具弹性。虽然“猫”的视觉特征可能会漂移，但“猫”这个词的语义含义保持稳定。通过将其决策锚定在这个稳定的语义空间中，模型有更好的机会优雅地适应。

最后，一个真正智能的系统应该知道它不知道什么。我们可以通过观察模型的输出概率来衡量其不确定性。如果它以 99% 的概率预测“猫”，那么它非常自信。如果它的[概率分布](@article_id:306824)很分散——20%“猫”，18%“狗”，22%“狐狸”——那么它很困惑。这个[概率分布](@article_id:306824)的**[香农熵](@article_id:303050)**是这种困惑程度的一个正式度量 [@problem_id:3174144]。高熵意味着高不确定性。这不仅仅是一个诊断工具；它可以成为解决方案的一部分。我们可以设计这样的系统：当检测到高不确定性时，触发一个特殊的推理过程——例如，通过给未见类别的概率一个额外的“提升”，促使模型重新考虑它否则可能会忽略的新奇可能性。

从简单的类比到语义空间的数学，从提示的艺术到信息论，[零样本分类](@article_id:641658)证明了知识迁移的力量。这是迈向机器不仅能识别模式，而且开始以一种可识别的、奇妙的、类似人类的方式理解世界的重要一步。

