## 引言
我们如何确定利用数据进行猜测的“最佳”方法？在统计学中，如同在任何严谨的学科中一样，我们对于何为优越策略的直觉往往可能具有误导性。估计过程需要一套正式的规则来比较不同的猜测策略（即“估计量”），并识别出那些存在明显缺陷的策略。这就引出了[不可容许估计量](@article_id:355828)这一关键概念——一种已被证明存在普遍更优替代方案的策略。根据定义，使用[不可容许估计量](@article_id:355828)是一种次优选择。

本文将深入探讨不可容许性这个引人入胜且常常违反直觉的世界。接下来的章节将首先阐述理解这一原则所需的基础概念。在“原理与机制”一章中，我们将定义统计博弈的规则——损失函数、风险和优控，并探讨一些简单的错误（如浪费信息或忽略物理约束）如何产生不可容许的估计量。然后，我们将逐步引出著名的、令人费解的结果，即斯坦悖论。在此之后，“应用与跨学科联系”一章将展示这些理论思想如何产生深远的实际影响，打破常见的统计假设，并揭示其与人工智能和机器人学等不同领域之间的惊人联系。

## 原理与机制

好了，我们给自己设定了一个任务：找到用数据猜测未知量的“最佳”方法。但“最佳”究竟意味着什么？在物理学和生活中，要说某样东西是最好的，你首先必须就游戏规则达成一致。你试[图优化](@article_id:325649)什么？是想造出最快的车，还是最省油的，抑或是最安全的？你不可能拥有一切。统计学的世界也是如此。我们需要为好的猜测（或好的“估计量”）制定规则。

### 游戏规则：风险与优控

让我们想象一下，我们正在尝试估计某个真实的、未知的值，我们称之为 $\theta$。这可以是任何东西——一个电子的质量、七月的平均温度，或者一枚硬币正面朝上的概率。我们收集一些数据，称之为 $X$，然后我们使用一个规则、一个配方，从这些数据中得出一个猜测。这个规则就是我们的**估计量**，我们称之为 $\delta(X)$。

我们如何评价我们的猜测？一个自然的方法是看它与真相相差多远。这个差值 $\delta(X) - \theta$ 就是我们的误差。为了确保正负误差不会相互抵消，我们通常将这个差值平方。这就得到了一个**[损失函数](@article_id:638865)**，最常用的是**[平方误差损失](@article_id:357257)**，$L(\theta, \delta) = (\delta(X) - \theta)^2$。这是一种惩罚：错误越大，惩罚就越大得多。

当然，我们的数据 $X$ 是随机的。如果我们再做一次实验，我们会得到不同的数据和不同的猜测。所以，我们不能根据单一结果来评判我们的估计量。相反，我们看它在所有*可能*得到的数据上的平均表现。这个平均损失被称为估计量的**风险**。对于一个给定的[真值](@article_id:640841) $\theta$，风险是 $R(\theta, \delta) = E[(\delta(X) - \theta)^2]$。这是在真实值为 $\theta$ 的情况下，我们的猜测策略将招致的预期或平均惩罚。

现在我们可以比较两个不同的估计量，比如 $\delta_1$ 和 $\delta_2$。什么时候一个明确比另一个更好？想象一下比较两辆车。如果车1在*所有*条件下——城市、高速、山区——的燃油经济性都更好（或相等），并且在至少一种条件下严格更好，你就会说车1完胜车2。我们对估计量使用完全相同的逻辑。

我们说一个估计量 $\delta_1$ **优控** (dominates) 另一个估计量 $\delta_2$，如果它的风险从不更高，并且对于至少一个可能的真参数值 $\theta$ 严格更低。数学上，这意味着对于所有的 $\theta$，有 $R(\theta, \delta_1) \le R(\theta, \delta_2)$，并且存在至少一个 $\theta_0$ 使得 $R(\theta_0, \delta_1) \lt R(\theta_0, \delta_2)$ [@problem_id:1956822]。

这就引出了一个关键定义。一个被另一个估计量优控的估计量被称为**不可容许的** (inadmissible)。如果你正在使用一个不可容许的估计量，那你就犯了一个错误。因为存在另一个估计量，按照我们的规则，它在所有情况下都更好。你应该换掉它！一个*不*是不可容许的估计量被称为**可容许的** (admissible)。一个可容许的估计量不一定是“最好的”——可能存在另一个估计量，它在某些 $\theta$ 值上更好，但在另一些值上更差。但它至少是一个站得住脚的选择；没有其他策略能保证在所有方面都比它好。

### 直观案例：浪费信息之罪

有了这些规则，我们能立即发现一些糟糕的策略。在估计中最明显的罪过就是故意丢弃有用的信息。

假设你有一组物理学家对一个[物理常数](@article_id:338291) $\mu$ 进行 $n$ 次独立测量（$X_1, X_2, \ldots, X_n$）。每次测量都来自一个[正态分布](@article_id:297928) $N(\mu, 1)$。一位懒惰的分析师建议只用第一次测量值 $\delta_1 = X_1$ 作为估计。另一位更勤奋的分析师则建议使用所有测量的平均值 $\delta_2 = \bar{X} = \frac{1}{n} \sum X_i$。谁是对的？

让我们看看风险。只使用第一次测量的风险就是它的方差，即 $R(\mu, \delta_1) = 1$。使用样本均值的风险也是它的方差，众所周知是 $R(\mu, \delta_2) = \frac{1}{n}$。如果你进行了不止一次测量（$n > 1$），那么很明显 $\frac{1}{n}  1$。对于*每一个可能的 $\mu$ 值*，$\delta_2$ 的风险都严格小于 $\delta_1$ 的风险。因此，懒惰分析师的估计量 $\delta_1 = X_1$ 被[样本均值](@article_id:323186)所优控。它是**不可容许的** [@problem_id:1894907] [@problem_id:1894887]。教训很简单：不要丢弃完好的数据！

你可能会说：“好吧，但如果有些数据噪声很大，不是很好呢？” 让我们把情景细化一下。假设我们有两个独立的仪器。第一个给出的测量值 $X \sim N(\mu, 1)$，第二个不太精确的仪器给出的测量值 $Y \sim N(\mu, 9)$。一位分析师建议只使用更精确的测量值，将估计设为 $\delta_1 = X$，完全忽略 $Y$。这似乎很合理；为什么要让有噪声的数据污染我们干净的测量值呢？

这就是直觉可能误导我们的地方。让我们考虑一个组合估计量，一个形如 $\delta_w = wX + (1-w)Y$ 的加权平均。一点微积分知识表明，当 $w = \frac{9}{10}$ 时，这个估计量的风险最小。得到的估计量是 $\delta_2 = \frac{9}{10}X + \frac{1}{10}Y$。注意，我们给予更精确的测量值更大的权重，这很合理。它的风险是多少？我们最初的估计量 $\delta_1 = X$ 的风险是 $1$。我们新的、改进的估计量 $\delta_2$ 的风险结果是 $\frac{9}{10}$，它对所有 $\mu$ 都严格小于 $1$。再一次，那个忽略了信息（即使是有噪声的信息）的估计量是**不可容许的** [@problem_id:1894880]。现在的教训更微妙了：*所有*信息都是宝贵的，挑战在于如何明智地组合它们，而不是丢弃它们。

### 更微妙的缺陷与惊人的优点

当我们把不可容许性的概念推向极致时，它变得真正有趣起来。我们已经看到，忽略信息是坏事。但是否还有其他更微妙的方式让一个估计量存在缺陷？

考虑一个完全忽略数据的均值 $\theta$ 的估计量。例如，无论我们的测量值 $X$ 是什么，我们总是猜测 $\theta$ 恰好是 5。我们称这个估计量为 $\delta_5(X) = 5$。这似乎愚蠢得可笑。这肯定应该是不可容许的，对吧？

让我们按规则来。这个估计量的风险是 $R(\theta, \delta_5) = E[(\theta - 5)^2] = (\theta - 5)^2$。对于任何其他估计量 $\delta'$ 要优控它，我们必须对所有 $\theta$ 有 $R(\theta, \delta') \le (\theta - 5)^2$，并且在某处有严格不等式。但看看在 $\theta=5$ 时会发生什么。我们这个“愚蠢”估计量的风险是 $R(5, \delta_5) = (5-5)^2 = 0$。由于风险不能为负，任何潜在的优控者 $\delta'$ 在 $\theta=5$ 时的风险也必须是零。这迫使 $\delta'(X)$ 等于 5（几乎总是），这意味着它就是同一个估计量！所以，没有其他估计量可以在不牺牲其他地方表现的情况下严格做得更好。与所有直觉相反，常数估计量 $\delta_5(X) = 5$ 是**可容许的** [@problem_id:1924876]。这是智力严谨性方面一个极好的教训。“可容许”不意味着“好”或“明智”。它有一个非常具体的、技术性的含义：不存在一个单一的替代方案在所有情况下都比它更好。

让我们换个角度。如果我们的估计量能给出物理上不可能的答案呢？假设我们正在估计一个参数 $\theta$，我们知道它必须是非负的，比如一个长度或等待时间（$\theta \ge 0$）。我们从 $N(\theta, 1)$ 分布中取一个测量值 $X$。标准的估计量就是 $\delta(X) = X$。但如果我们观察到 $X = -2.5$ 怎么办？我们的估计是 $-2.5$，尽管我们知道真值不可能是负数。这感觉不对。

它的确是错的！考虑一个替代估计量 $\delta_+(X) = \max(0, X)$。当 $X$ 是正数时，这个估计量的行为与 $X$ 完全一样，但如果 $X$ 是负数，它会明智地将估计修正为 0，即最接近的可[能值](@article_id:367130)。事实证明，$\delta_+$ 的风险总是小于或等于 $\delta(X)=X$ 的风险，并且对于许多 $\theta$ 值来说严格更小。因此，由于它可能产生不可能的值，当知道 $\theta \ge 0$ 时，标准估计量 $\delta(X)=X$ 是**不可容许的** [@problem_id:1894895]。这个原则相当普遍：如果你的参数有约束，你的估计量就应该尊重它们。

损失函数的选择——即游戏规则本身——也能揭示不可容许性。[平方误差损失](@article_id:357257)对高估和低估的惩罚是对称的。但如果低估远比高估危险得多呢？想象一下估计火箭发射所需的燃料。“Linex 损失”函数就模拟了这种不对称性。在这样的损失下，即使对于一个简单的[正态均值](@article_id:357504)问题，标准估计量 $\delta(X)=X$ 也变得不可容许。它被一个有偏的估计量所优控，该估计量系统地将猜测向一侧移动，以提供一个安全边际 [@problem_id:1894881]。

### 伟大的启示：斯坦悖论

到目前为止，我们的旅程虽然富有启发性，但也许并非惊天动地。浪费信息是坏事。忽略物理约束是坏事。这些感觉像是任何一位明智的科学家都已经知道的教训。但不可容许性的故事有一个转折，它给统计学界带来了冲击波，这个结果如此反直觉，以至于被称为悖论。

背景是统计学中最标准的问题：估计[正态分布](@article_id:297928)的均值。正如我们所见，对于单个参数（$p=1$），标准估计量 $\delta(X)=X$ 是可容许的（只要没有约束）。对于两个参数（$p=2$），它也是可容许的。它是[最大似然估计量](@article_id:323018)（MLE），它是无偏的，它具备你想要的一切。自然的、显而易见的、看似无懈可击的策略是：如果你有几个不相关的量要估计，你就用各自的数据单独估计每一个。

假设我们正在估计三个不相关的量的均值，比如说，东京大米的平均价格（$\theta_1$）、某棒球运动员一个赛季打出的本垒打数（$\theta_2$），以及亚马逊地区某种蝴蝶的平均丰度（$\theta_3$）。我们的数据是一个向量 $\mathbf{X} = (X_1, X_2, X_3)$。标准估计量就是 $\delta_0(\mathbf{X}) = \mathbf{X}$。

1956年，Charles Stein 证明了这是**不可容许的**。只要你同时估计三个或更多的均值（$p \ge 3$），就存在一个更好的方法 [@problem_id:1956807]。这就是**斯坦悖论** (Stein's Paradox) 的精髓。

James 和 Stein 后来给出了一个明确优控标准估计量的估计量：
$$ \delta_{JS}(\mathbf{X}) = \left(1 - \frac{p-2}{\|\mathbf{X}\|^2}\right)\mathbf{X} $$
让我们花点时间来体会一下这有多么怪异。这个公式取了我们原始估计的向量 $\mathbf{X}$，并将其向原点（零向量）“压缩”。压缩的程度取决于所有三个问题的组合数据。对大米价格的估计竟然被关于本垒打和蝴蝶的数据所调整！这感觉就像疯了。就好像你可以通过同时测量房间的温度来改善你对光速的测量一样。

然而，它确实有效。James-Stein 估计量的总平方误差风险*总是*小于单独估计每个均值的风险 [@problem_id:1894890]。通过汇集不相关问题的信息，我们可以改进对所有这些问题的估计（就总平均风险而言）。这不是一个理论上的奇闻。这种现象甚至在非常实际的场景中也会出现。例如，在估计 Uniform$(0, \theta)$ 分布的参数 $\theta$ 时，最自然的估计量，即观测值的最大值 $X_{(n)}$，是不可容许的。一个更好的估计量是通过将其乘以一个因子 $\frac{n+2}{n+1}$ 得到的，这个因子稍微增大了估计值，以纠正样本最大值几乎总是低于真实最大值 $\theta$ 的事实 [@problem_id:1924842]。在这两种情况下，我们都是引入一点偏差来换取方差的大幅减少，从而降低了总风险。

这可能会让你产生最后一个问题。我们常学到，标准估计量 $\mathbf{X}$ 是“极小化极大”(minimax)的，意味着它最小化了最坏情况下的风险。它怎么能被 James-Stein 估计量优控呢？这不会产生矛盾吗？美妙的解答是，极小化极大的桂冠并非独享。标准估计量的风险是恒定的：$R(\theta, \delta_0) = p$。James-Stein 估计量的风险总是低于 $p$，但随着真实均值离零点越来越远，它会逐渐逼近 $p$。因此，两个估计量的*最大*风险都是同一个值 $p$。两者都是极小化极大的！只是其中一个在整个参数空间上都优于另一个 [@problem_id:1956787]。

[不可容许估计量](@article_id:355828)的发现，以及最终的斯坦悖论，是一个关于知识本质的有力故事。它告诉我们，我们关于信息的直觉可能是有缺陷的，“不相关”的问题有时可以用数学的语言相互对话，即使在最行为良好和“已解决”的问题中，也可能在表面之下隐藏着令人惊讶、美丽且极其实用的宝藏。