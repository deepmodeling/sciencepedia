## 引言
虽然由点和线组成的[简单图](@entry_id:274882)为理解连接提供了一个强大的框架，但现实世界远比这复杂和多彩。从[生物网络](@entry_id:267733)到社交互动，许多系统都由通过多种关系类型连接起来的不同实体组成。像简单图那样将所有节点和边视为同质的，会丢弃关键的上下文信息，导致对现实产生扁平化、不完整的看法。这造成了知识鸿沟，限制了我们有效建模和推理这些系统复杂动态的能力。

本文将探讨异构图，这是一种能够包容这种复杂性的更丰富的结构。我们首先将在**原理与机制**章节深入探讨其基本概念，解释这些图如何保留意义，并引入诸如元路径之类的强大分析工具。我们还将探讨机器如何通过[表示学习](@entry_id:634436)从这种丰富的结构中学习，并将浅层方法与[图神经网络](@entry_id:136853)的归纳能力进行对比。随后，**应用与跨学科联系**章节将展示该方法在医学、生物学、材料科学和[环境监测](@entry_id:196500)等领域的变革性影响，展示异构图如何为描述和理解我们的世界提供一种新的语言。

## 原理与机制

我们大多数人对图都有一个简单直观的印象：一些由线连接的点。这是一个强大的想法。这些点可以是人，线是友谊。或者点是城市，线是高速公路。在这个简单的世界里，所有的点都是平等的，所有的线都意味着同样的事情——存在一个连接。但如果我们告诉您，这个黑白素描只是故事的开始呢？现实世界充满了色彩、多样性和意义，为了捕捉这一切，我们的图也必须学会看到色彩。这就是**异构图**的世界。

### 从平面地图到生动世界

想象一下，试图通过一张地图来理解人类健康的复杂舞蹈，而地图上每个实体——基因、蛋白质、疾病、药物——都只是一个黑点。一条线可能连接一种药物和一种疾病，或者一个基因和一种蛋白质，但这条线本身并不能告诉你它们是*如何*相关的。药物是*治疗*疾病还是*引发*疾病？基因是*编码*蛋白质还是被蛋白质*调控*？简单图将所有节点和边视为同质的，这迫使我们丢弃了这些至关重要的上下文。这就像尝试根据一个只列出食材却省略了所有动词的食谱来烹饪：“面粉、鸡蛋、糖、加热。”你有了原料，却失去了过程。

**异构图**让这些动词回归了。它是一个我们明确承认存在不同*类型*的节点和不同*类型*的关系的图 [@problem_id:2956863]。现在，我们的节点被恰当地标记了：这是一个**基因**节点，那是一个**蛋白质**节点，这边我们还有**药物**和**疾病**节点。这些线也获得了自己的标签。从药物到蛋白质的边可能是 `targets` 类型，而从基因到疾病的边可能是 `associated_with` 类型。

突然间，我们的平面地图变成了一个生动的三维世界。我们不仅添加了标签；我们还给了图一个*模式 (schema)*，即一套支配世界如何运作的规则。`treats` 关系只能存在于药物和疾病之间。`regulates` 关系只能存在于两个基因之间。天真地将这种[结构扁平化](@entry_id:755550)——假装一个 `Protein-Protein Interaction` 与一个 `Drug-Target` 互动是相同的——肯定会迷失方向。这会引入偏见并导致荒谬的结论，例如试图基于共享连接来寻找“基因-药物”混合体的社群，这项任务从根本上误解了生物学 [@problem_id:4329205]。通过保留异构性，我们确保我们的分析尊重我们所建模系统的基本事实。

### 连接的语法：元路径

一旦你有了一个拥有不同种类对象和关系的世界，你就可以开始讲述故事了。在[简单图](@entry_id:274882)中，路径只是一连串的跳跃：A 到 B 再到 C。在异构图中，一条路径就变成了一个叙事，一个具有特定语义的事件链。我们称之为**元路径 (meta-path)**。

元路径是在*类型*层面上定义的路径 [@problem_id:5199541]。考虑以下元路径：

`Drug` $\xrightarrow{\text{targets}}$ `Gene` $\xrightarrow{\text{associated_with}}$ `Disease`

这不仅仅是一次随机游走；它是一个关于生物学机制的潜在故事。它描述了一种药物可能通过靶向一个特定基因来发挥作用，而这个基因又与某种特定疾病相关。这种复合关系——通过一个基因将药物与疾病联系起来——远比仅仅知道药物和疾病以某种方式“相连”更有见地。元路径为我们提供了一个全新的、更高层次的视角来审视网络。

在数学上，这种讲故事的方式出人意料地优雅。如果每种关系类型都由一个[邻接矩阵](@entry_id:151010)（一个告诉我们一种类型的哪些节点与另一种类型的哪些节点相连的表）表示，那么一条元路径就对应于[矩阵乘法](@entry_id:156035)。`Drug` $\to$ `Gene` $\to$ `Disease` 的复合关系可以通过将 `Drug-Gene` 关系的邻接矩阵与 `Gene-Disease` 关系的[邻接矩阵](@entry_id:151010)相乘得到。结果是一个新的矩阵，它直接告诉我们哪些药物*通过这个特定的叙事*与哪些疾病相连。

这个想法使我们能够以一种更细致的方式来定义相似性 [@problem_id:4549330]。两个基因相似吗？也许不直接相似。但如果它们都与同一组疾病相关联呢？我们可以使用元路径 `Gene` $\to$ `Disease` $\to$ `Gene` 来找出答案。连接两个基因的此类路径的数量成为它们功能相似性的一个度量。例如，如果基因 $g_1$ 与疾病 $d_1$ 和 $d_2$ 相关联，而基因 $g_2$ 只与疾病 $d_1$ 相关联，那么它们共同关联一个疾病。通过这条元路径，它们之间的路径计数将为 $1$。当然，我们必须小心。如果一个基因与数百种疾病相关联，它的连接就不那么具体了。一种有原则的相似性度量方法，如 **PathSim**，通过每个节点的“中心度”（hubbiness）来对这些路径计数进行归一化，从而为我们提供一个更有意义的分数，衡量两个节点在给定故事情节下的相似程度 [@problem_id:4549330]。通过基于这些有意义的、受类型约束的叙事来定义相似性，我们可以发现共享深层功能角色的节点社群（例如，[基因簇](@entry_id:268425)），而不会受到混合不同节点类型所带来的噪声干扰。

### 教会机器看懂色彩

那么，我们有了这个极其丰富多彩的图。我们可以使用元路径来定义复杂的关系。但我们如何让计算机理解这一切呢？它如何从这种结构中学习以做出预测，比如识别一种药物可能治疗哪些新疾病？这就是**[图表示学习](@entry_id:634527)**的领域。目标是将每个节点转换成一个数字向量——即**嵌入 (embedding)**——这种方式能够捕捉其在网络中的角色。

#### 机械学习：浅层嵌入

一种方法是让机器基本上记住关系。这些通常被称为**浅层嵌入**方法。模型通过尝试使一个[评分函数](@entry_id:175243)生效来为每一个节点和每一种关系类型学习一个嵌入。例如，它可能会学习一些向量，使得对于一个像 `(Drug_A, treats, Disease_X)` 这样的真实事实，`Drug_A` 的嵌入加上 `treats` 的嵌入会非常接近 `Disease_X` 的嵌入。

这是一种强大的技术，但它有一个根本的局限性：它是**直推式 (transductive)** 的 [@problem_id:4549791]。模型只学习它训练时用到的特定节点。如果一种模型从未见过的新药出现，模型就不知道该如何处理它。它没有记住它的嵌入。这就像通过背诵常用语手册来学习一门语言；你能处理手册中的情景，但你无法构造一个新句子。这些模型学习了关于世界的一组固定事实，但没有学习到底层规则。

#### 学习规则：[图神经网络](@entry_id:136853)

这就引出了一个更深刻的方法：**[图神经网络 (GNNs)](@entry_id:750014)**。GNN不是去记忆事实，而是学习*系统的规则*。它学习一个*函数*，这个函数可以通过查看节点的特征（例如，药物的化学结构）及其局部邻域来为*任何*节点（甚至是它从未见过的节点）计算嵌入。这种泛化能力使GNN成为**归纳式 (inductive)** 的。

GNN如何在异构图上工作？想象每个节点都是一个在派对上的人，试图弄清楚自己的身份（即它的新嵌入）。它通过倾听邻居的意见来做到这一点。但这是一个非常复杂的派对——这个过程被称为**[消息传递](@entry_id:751915) (message passing)** [@problem_id:4349456]。

1.  **类型化对话**：一个节点不会同等对待所有邻居。它通过一个不同于听取“疾病”邻居的过滤器来听取它的“基因”邻居。来自邻居的消息会通过一个学习到的权重矩阵进行转换，该矩阵特定于它们之间连接的*关系类型*。沿着 `causes` 边传来的消息与沿着 `treats` 边传来的消息的解读方式截然不同 [@problem_id:4298401]。

2.  **加权意见**：并非所有邻居都同等重要。通过一种称为**注意力 (attention)** 的机制，GNN可以学会在形成新意见时，对某些邻居给予比其他邻居更多的关注。这个重要性权重甚至可以取决于所考虑的具体关系。

3.  **自我反思**：节点不仅仅听取他人的意见。它还考虑自己先前的状态，并用一个单独的学习矩阵对其进行转换。一个节点的新身份是它从外部世界听到的信息和它自己先前信念的结合。

综上所述，单个类型为 $t$ 的节点 $v$ 的更新看起来像这样 [@problem_id:4549791]：

$$
\mathbf{h}_v^{(l+1)} \;=\; \sigma_{t}\! \left( \mathbf{W}^{\mathrm{self}}_{t}\,\mathbf{h}_v^{(l)} \;+\; \sum_{r \in \mathcal{R}} \sum_{u \in \mathcal{N}_r(v)} \alpha_{r,t}(u,v)\,\mathbf{W}_{r \rightarrow t}\,\mathbf{h}_u^{(l)} \right)
$$

这个方程虽然看起来令人生畏，但它只是对那场复杂对话的一个优美的数学总结。$\mathbf{h}_v^{(l)}$ 是节点在上一步的状态。第一项 $\mathbf{W}^{\mathrm{self}}_{t}\,\mathbf{h}_v^{(l)}$ 是自我反思。第二项是针对所有关系类型 $r$ 和每种关系下的所有邻居 $u$ 的总和。项 $\mathbf{W}_{r \rightarrow t}\,\mathbf{h}_u^{(l)}$ 是来自邻居 $u$ 的消息，根据关系 $r$ 和目标类型 $t$ 进行转换。$\alpha_{r,t}(u,v)$ 项是注意力权重。最后，$\sigma_{t}$ 是一个用于修饰最终结果的函数。

诸如**关系[图卷积网络](@entry_id:194500) (R-GCN)** 之类的架构实现了这种特定于关系的转换思想，而**异构[图注意力网络](@entry_id:634951) (HAN)** 则更进一步。HAN 首先沿着不同的元路径（我们的“故事情节”）计算节点嵌入，然后使用另一层注意力来学习哪些故事情节对于手头的任务最重要 [@problem_id:5199542]。

通过学习信息在不同实体和关系类型之间流动和转换的过程，这些模型构建了对系统的深刻、功能性的理解。它们超越了连接的静态图像，转向了交互的动态模拟，为洞察支配我们世界的复杂系统打开了一扇强大的新窗户。

