## 应用与跨学科联系

模拟[星系碰撞](@entry_id:158614)与会写诗的人工智能有什么共同之处？它们又与设计新型[太阳能电池](@entry_id:138078)材料的计算机模型，或揭示疾病[遗传网络](@entry_id:203784)的生物学家有什么共同点？这些天差地别的科学前沿领域似乎不可能建立在同一个计算基石之上。然而，事实确实如此。这个共同元素并非某种奇异的新数学，而是一种你可能在初级代数课上学过的运算：矩阵乘法。然而，我们故事的主角并非乘法本身，而是通用[矩阵乘法](@entry_id:156035)（GEMM）的*范式*——一种组织计算的深刻原则，它已成为现代科学技术中默默无闻的引擎。

在理解了 GEMM 如何实现其非凡效率的原理之后，我们现在可以踏上一段旅程，去看看这单一思想如何像一根统一的线索，贯穿于广阔且看似毫无关联的科学领域。我们将发现，最成功、最强大的计算方法，无论是通过设计还是演化，通常都是那些“说 GEMM 语言”的方法。

### [科学模拟](@entry_id:637243)的核心：从星辰到材料

几乎所有物理模拟的核心，从星辰之舞到原子振动，都归结于求解庞大的[线性方程组](@entry_id:140416)。一个典型的例子是 LU 分解，这是一种用于分解大矩阵 $A$ 以求解 $A\vec{x} = \vec{b}$ 这[类方程](@entry_id:144428)组的方法。一个朴素的实现通过一系列小的秩-1 操作来更新矩阵。这就像一个在巨大厨房（主内存）里工作的大厨，他有一个小而快的工作台（缓存），但为了食谱中每个微小的步骤，他都要为每一种配料跑到储藏室去。处理器大部分时间都在等待数据。

革命随着*分块*而来。算法被重构，不再进行微小的更新，而是将其大部分工作作为大子矩阵的乘法来执行——即一次 GEMM。这就像我们的大厨将几道菜所需的一整盘配料都带到工作台上，在再次去储藏室之前，尽可能多地完成工作。这种策略极大地提高了*[算术强度](@entry_id:746514)*，即执行的计算量与移动的数据量之比。通过多次复用已加载到高速缓存中的数据，算法变得*计算受限*而非*内存受限*，从而释放了处理器的全部威力 [@problem_id:3507962]。

这并非一次性的技巧。利用分块来发挥 GEMM 效率的同样原则，对于[数值线性代数](@entry_id:144418)的其他主力算法也至关重要，例如用于解决最小二乘问题的 QR 分解 [@problem_id:3549735]，甚至在量子力学计算中用于寻找特征值的复杂迭代方法中也是如此 [@problem_id:3577279]。

GEMM 范式在模拟需要针对许多不同场景求解同一系统时也大放异彩，这在天体物理学和[流体动力](@entry_id:750449)学中使用的时间步进代码中很常见。我们不是逐一求解每个场景，而是可以将它们捆绑成一个“批次”。例如，在求解 $AX=B$ 时，如果 $B$ 是一个包含代表不同右端项的许多列的矩阵，一个分块三角求解器（一种被称为 TRSM 的 GEMM 变体）可以一次性处理所有列。它在所有列上复用 $A$ 的分解，分摊了成本，实现了巨大的加速 [@problem_id:3507966]。

这种普适性贯穿各个科学学科。在[计算材料科学](@entry_id:145245)中，研究人员使用[密度泛函理论](@entry_id:139027)（DFT）来预测新材料的性质。其中一个关键且昂贵的步骤涉及应用所谓的非局域赝势算子。一个朴素的实现涉及一系列独立的点积，其[算术强度](@entry_id:746514)非常低。自然而然，高性能的方法是将整个操作重构为一个大型的矩阵-矩阵乘积，将跨多个电子态的计算进行批处理。我们甚至可以使用一个名为*roofline 模型*的性能指南来直观地解释为什么这样做如此有效：基于 GEMM 的方法具有如此高的[算术强度](@entry_id:746514)，以至于其性能“触及屋顶”，仅受处理器峰值计算速度的限制，而不是慢得多的内存访问速度 [@problem_id:3470126]。即使在凝聚态物理的先进[张量网络方法](@entry_id:165192)中，如[密度矩阵重整化群](@entry_id:137826)（DMRG），性能优化的核心主题也是不懈地融合操作和重构算法，以最大化利用计算受限的 GEMM 内核，而不是像[奇异值分解](@entry_id:138057)（SVD）那样内存受限的对应物 [@problem_id:2980998]。

### 人工智能革命：[深度学习](@entry_id:142022)的语言

虽然几十年来 GEMM 一直是[科学模拟](@entry_id:637243)中默默无闻的主力，但它在正在进行的人工智能革命中找到了一个璀璨的新舞台。[深度学习](@entry_id:142022)的爆发与 GPU 等专用硬件的兴起密不可分，而深度学习算法的成功在很大程度上归功于它们能够用这种硬件的母语——GEMM 的语言——来表达。

考虑一个处理文本序列的简单[循环神经网络](@entry_id:171248)（RNN）。在每个时间步，输入向量都与一个权重矩阵相乘。如果我们同时处理一个包含许多句子的批次，最有效的方法是将所有时间步和所有句子的全部输入堆叠成一个巨大的矩阵，并与权重矩阵执行一次大规模的 GEMM。这种将批处理直接映射到 GEMM 的基本技术是每个深度学习框架的基石 [@problem_id:3148061]。

GEMM 范式的影响在 Transformer 架构中表现得最为明显，后者是像 ChatGPT 这样的模型背后的引擎。看似复杂的“[多头注意力](@entry_id:634192)”机制，从计算的角度来看，是一个并行设计的杰作。整个问题被优雅地划分为针对每个“头”的更小的、独立的子问题，然后作为*批量 GEMM* 同时执行。这使得 GPU 能够以极高的效率[并行处理](@entry_id:753134)所有的头 [@problem_id:3148000]。

但故事不止于此。为什么著名的“[缩放点积注意力](@entry_id:636814)”是那样定义的？答案不仅在于其数学特性，还在于其计算结构。其核心计算，即通过 $QK^T$ 形成注意力分数，是一个纯粹、优美的 GEMM。而像早期的“[加性注意力](@entry_id:637004)”这样的替代方案，则涉及一系列破坏这种清晰结构的操作，需要广播加法和非线性函数，这些对 GPU 效率而言是毒药。[加性注意力](@entry_id:637004)无法被折叠成一个单一的大型 GEMM，并且受到内存受限步骤和巨大中间结果可能导致的“内存爆炸”的困扰。点积注意力的胜利是一个惊人的例子，说明了算法的设计是由其与 GEMM 范式的深度兼容性所决定的。最成功的模型是那个能最流利地说硬件语言的模型 [@problem_id:5228191]。

这个原则也教导我们什么*不*该做。像 MobileNet 这样的架构使用一种称为[深度可分离卷积](@entry_id:636028)的操作，它巧妙地减少了所需的算术运算总数。这不应该更快吗？不一定。它通过将一个大的卷积分解成许多微小的、独立的卷积来实现这种减少。这样做，它就失去了被表述为一个大型、高效 GEMM 的能力。在并行处理器上，这可能比一个原始计算量更大但具有更好、GEMM 友好结构的算法要慢得多。这教给我们一个深刻的教训：在现代计算机上，计算的*组织*方式通常比计算的原始*数量*更重要 [@problem_id:3120083]。

### 超越矩阵：驾驭生物学和数据科学中的张量

世界并非总能用二维矩阵整齐地描述。从医学成像到系统生物学，当我们可能拥有关于患者、基因和药物的数据时，我们会遇到称为张量的多维数组。在这里，GEMM 范式同样提供了驾驭其复杂性的钥匙。

在系统生物医学中，[非负矩阵分解](@entry_id:635553)（NMF）和[典范多项分解](@entry_id:189762)（CP）[张量分解](@entry_id:173366)等方法被用来在[高维数据](@entry_id:138874)集中寻找隐藏的模式。计算挑战是巨大的。CP 分解中的一个关键步骤，称为[矩阵化](@entry_id:751739)张量与 Khatri–Rao 积的乘积（MTTKRP），朴素地实现需要构建一个巨大到可能无法装入任何[计算机内存](@entry_id:170089)的中间矩阵。这似乎是一条死胡同。

然而，GEMM 范式指明了前进的道路。聪明的算法不是构建那个庞大的矩阵，而是逐块计算最终结果。该操作被重构为一系列更小、可管理的 GEMM，其结果再被加在一起。这种“GEMM 求和”策略完全避免了灾难性的内存瓶颈。这与我们之前看到的核心原则相同——组织工作以最大化数据复用并利用高效的计算内核——但现在应用于一个更复杂、更高维度的情境中 [@problem_id:4360137]。

### 一种普适模式

我们的旅程带领我们从星系的核心到细胞的核心，从物理学的基础到人工智能的前沿。在每一个转折点，进步都是通过发现将计算组织成大型、[稠密矩阵](@entry_id:174457)-[矩阵乘法](@entry_id:156035)形式的巧妙方法而解锁的。GEMM 不仅仅是一个算法；它是一种计算主题，一块罗塞塔石碑，将我们最具挑战性的科学问题翻译成我们强大的计算机器最能理解的语言。它的美在于这种意想不到的统一性——一个简单的代数运算，当通过计算效率的视角来看待时，成为贯穿整个科学领域发现过程的普适指导原则。