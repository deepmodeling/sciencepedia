## 应用与跨学科联系

现在我们已经拆解了 Vision [Transformer](@article_id:334261)，审视了它的齿轮和弹簧——图像块、[嵌入](@article_id:311541)、[注意力机制](@article_id:640724)——我们来到了旅程中最激动人心的部分。我们能用这台奇妙的机器*做*什么？就像科学中的任何伟大思想一样，其真正的力量不仅在于其优雅，还在于其实用性。我们发现的这些原理不仅仅是抽象的好奇心；它们是开启新能力、以新方式解决老问题，甚至提出我们以前没想过要问的问题的钥匙。

Vision [Transformer](@article_id:334261) 的应用故事，是一个单一而强大的思想——全局上下文——从其[计算机视觉](@article_id:298749)的源头向外扩散，触及[气候科学](@article_id:321461)和[计算物理学](@article_id:306469)等不同领域的故事。让我们追溯这些涟漪，看看它们能走多远。

### 重新定义[计算机视觉](@article_id:298749)：洞见全局

Vision [Transformer](@article_id:334261) 的第一个也是最自然的归宿，当然是计算机视觉。但它不仅仅是复制其前辈——[卷积神经网络 (CNN)](@article_id:303143)——所能做到的事情。它从根本上改变了机器*如何*看待世界。

CNN 是逐块、局部地建立对图像的理解。它就像一个只能与邻居交谈的侦探。要从城镇的另一头获取信息，消息必须通过人传人、一个街区一个街区地传递。这在很多情况下行之有效，但它有一个关键的弱点。如果关键线索位于场景的两端，而它们之间的路径被遮挡了怎么办？

想象一张猫躲在栅栏后面的照片。我们人类毫无困难；我们的思维毫不费力地越过栅栏柱，将可见的部分——这里一只耳朵，那里一条尾巴，中间一块毛皮——连接成一个连贯的整体。标准的 CNN 对此感到困难。局部的信息链被[遮挡](@article_id:370461)的栅栏打断了。但 Vision Transformer 的行为更像我们。它的[自注意力机制](@article_id:642355)允许任何图像块直接与任何其他图像块通信，无论它们相距多远。看到猫耳朵的图像块可以与看到它尾巴的图像块直接“对话”。这种从遥远、不相连的区域合成信息的能力不仅仅是一个微小的改进；它是一种超能力。它使模型能够“看穿”[遮挡](@article_id:370461)，并基于对所有可用证据的整体理解来识别物体，这项任务完美地突显了全局注意力相对于[局部感受野](@article_id:638691)的架构优势 [@problem_id:3199235]。这个想法的一个简化、近乎有趣的例子可以在一个谜题中看到，其中机器必须计算两个半边被放置在图像中相距很远位置的物体；ViT 通过注意力将它们配对来解决问题，而一个仅有局部视野的模型则会惨败 [@problem_id:3199150]。

这种全局视角不仅用于识别图像中的内容，也用于精细地理解其结构。在像[语义分割](@article_id:642249)这样的任务中，目标是为每个像素分配一个类别标签（例如，“这是道路”，“这是天空”，“这是一辆车”），ViT 可以被调整以产生这些密集的预测。这里出现了一个有趣的假设：也许注意力的“锐度”——一个图像块是将其注意力集中在少数几个关键的其他图像块上，还是广泛地分散注意力——与模型绘制物体之间精确边界的能力相关。一个学会在分析边界区域时急剧集中注意力的模型，可能会产生更清晰、更准确的分割图 [@problem_id:3199195]。

此外，真实世界并不像基准数据集那样整洁。例如，医学扫描图像有各种形状和大小。许多旧架构对输入尺寸的严格要求在实践中是个令人头疼的问题。在这里，ViT 基于图像块的特性再次提供了一种天生的灵活性。无论图像是正方形还是矩形，都可以被分割成图像块。真正的挑战来自于告诉模型每个图像块在*哪里*。虽然早期的 ViT 使用学习到的绝对[位置编码](@article_id:639065)，在面对新的图像维度时必须尴尬地调整大小或进行插值，但使用*相对*[位置编码](@article_id:639065)的新方法——只关心两个图像块之间的偏移量，而不是它们的绝对坐标——是处理真实世界数据可变几何形状的一种更自然、更稳健的解决方案 [@problem_id:3199220]。

### 时间维度：运动中的 ViT

所以，ViT 可以理解一幅静态图像。但我们的世界不是静止的；它在流动和变化。ViT 能学会感知时间吗？

答案是响亮的“是”，而且方法异常简单。想象一个短视频片段。你可以把它看作是一堆图像或帧。现在，如果我们把整个视频看作一个在时间上展开的巨大“图像”呢？我们可以把*每一*帧都切成图像块，然后把所有这些图像块排成一个非常长的序列：第一帧的图像块，然后是第二帧的图像块，依此类推。

当我们将这个[时空](@article_id:370647)词元序列输入到 Transformer 中时，[自注意力机制](@article_id:642355)现在可以在空间和时间上施展其魔力。一个显示在第 5 帧左上角有一个球的图像块，现在可以注意到第 4 帧中球*曾经*在的位置的图像块。通过这样做，模型可以学习运动、轨迹和时间依赖性。注意力权重讲述了一个故事：大部分注意力指向来自其他帧的词元（“时间注意力”），表明模型正在跟踪随时间发生的变化。我们甚至可以设计一些指标，显示一个图像块中的运动量与其对自身过去位置的关注程度之间存在直接关联，这为我们提供了一个量化的方法来理解模型如何“看到”运动 [@problem_id:3199225]。

### 科学新视角：作为模拟器的 Transformer

故事在这里发生了真正深刻的转变。ViT，这个为看猫狗图片而生的模型，正在作为基础科学发现的工具重获新生。关键的洞见是，“图像”不一定是一张照片。它可以是任何[排列](@article_id:296886)在网格上的数据。

考虑[气候科学](@article_id:321461)。科学家们研究来自气象站和卫星的数据，这些数据[排列](@article_id:296886)在经纬度网格上。该领域一个长期存在的谜题是“遥相关 (teleconnections)”现象——世界遥远地区天气模式之间的因果联系，例如太平洋的厄尔尼诺事件如何影响北美的降雨。这些本质上是长程[空间相关性](@article_id:382131)。而什么架构是专门为寻找长程相关性而设计的呢？Transformer。通过将气候数据网格视为一幅图像，可以训练 ViT 来寻找这些模式。它的注意力图谱成为这些遥相关可能存在位置的假设。我们甚至可以为[注意力机制](@article_id:640724)配备一个“距离偏置”，以鼓励或不鼓励局部与长程注意力，然后测量模型学会观察的平均距离。通过这种方式，[Transformer](@article_id:334261) 成为一个用于发现全球现象的数据驱动的发现引擎 [@problem_id:3199147]。

旅程继续深入，进入了物理学和数学的领域。许多自然法则都以[偏微分方程](@article_id:301773) (PDE) 的形式表达，它们描述了像热量、压力或[磁场](@article_id:313708)这样的量如何随空间和时间变化。为了在计算机上求解这些方程，科学家使用数值方法，如[有限差分法](@article_id:307573)，它在离散网格上近似连续系统。在这种方法中，一个点在下一个时间步的值是根据其当前时间步直接邻居的值来计算的——这是一种称为“模板 (stencil)”的固定计算模式。

但是，如果我们将在时间 $t$ 的物理数据网格看作一幅图像，而在时间 $t+1$ 的网格看作要预测的目标图像呢？ViT 能否*学习*这个时间演化规则？在一项非凡的应用中，研究人员正在这样做。每个网格单元都是一个词元。[自注意力机制](@article_id:642355)通过观察所有其他单元，学习一个更新每个单元值的规则。本质上，Transformer 学会了近似底层 PDE 的行为。它学到的注意力权重形成了一个数据驱动的、动态的“模板”，这可能比经典方法的固定、手工制作的模板要灵活得多。通过将 Transformer 学到的更新规则与传统求解器的基准真相进行比较，我们可以探索一种全新的“神经 PDE 求解器”[范式](@article_id:329204) [@problem_id:3199194]。

### 高效学习的艺术

拥有所有这些惊人能力的同时，也有一个问题。最初的 Vision Transformer 是一个庞然大物，需要巨大的数据集从零开始学习。这会限制其仅被最大的科技公司使用。但科学界是聪明的。

出现的其中一个最强大的想法是**[知识蒸馏](@article_id:642059) (knowledge distillation)**。想象一个智慧、经验丰富的“教师”模型——可能是一个已经在数百万张图片上训练过的大型、笨重的 CNN。我们想训练一个更小、更高效的 ViT “学生”。我们不是只给学生看原始教科书（基准真相标签），而是让它从老师细致入微的解释中学习。老师的输出不仅仅是一个硬性的“这是一只猫”，而是一个软性的[概率分布](@article_id:306824)：“这是 95% 的猫，4% 的狗，和 1% 的汽车。”通过训练学生模仿这个更丰富、更软的[目标分布](@article_id:638818)，我们可以更有效地传递老师的“知识”。这通常通过将标准的[交叉熵损失](@article_id:301965) $\mathcal{L}_{\text{CE}}$（与基准真相标签计算）和衡量学生与教师分布差异的 Kullback-Leibler (KL) 散度项结合起来实现。最终的损失是一个加权和：$\mathcal{L} = \lambda \, \mathrm{KL}(p_{\text{student}} \| p_{\text{teacher}}) + (1-\lambda) \, \mathcal{L}_{\text{CE}}$。这项技术，是像 Data-Efficient Image Transformer (DeiT) 这样模型的核心，使得在小得多的数据集上训练高性能 ViT 成为可能 [@problem_id:3199218]。

最后，由在巨大数据集上[预训练](@article_id:638349)的 ViT 学到的特征非常强大和通用。这引出了**[迁移学习](@article_id:357432) (transfer learning)** 的概念。对于许多新任务，你不需要从零开始训练 ViT，甚至不需要微调整个网络。你通常可以冻结[预训练](@article_id:638349)模型的绝大部分——将其视为一个固定的、通用的[特征提取器](@article_id:641630)——只在其输出之上训练一个简单的[线性分类器](@article_id:641846)。这种“线性探查 (linear probing)”方法效果如此之好，证明了 ViT 学到的表示的质量和可迁移性。当这种简单方法与完全微调之间的差距很小时，它告诉我们[预训练](@article_id:638349)的特征已经几乎完美地适用于新任务 [@problem_id:3199207]。这种模块化和可重用性是 Vision Transformer 在实践中取得广泛成功的关键。

从看穿栅栏到模拟物理定律，Vision [Transformer](@article_id:334261) 已经证明自己远不止是又一个图像分类器。它证明了一个优美而统一的思想——通用、感知上下文的注意力——的力量，而它的应用之旅才刚刚开始。