## 引言
什么是信息？虽然我们每天都在使用这个词，但 Claude Shannon 对其作出的严谨定义改变了科学和技术，将“信息”从一个模糊的概念转变为一个可量化的、衡量不确定性和惊奇程度的指标。本文旨在弥合信息的直观概念与[香农熵](@article_id:303050)这一强大数学框架之间的鸿沟，后者满足了对不确定性进行形式化度量和比较的需求。在接下来的章节中，我们将首先探讨其核心的“原理与机制”，揭示熵的计算方法，并阐明其与[热力学](@article_id:359663)物理定律之间深刻而惊人的联系。随后，在“应用与跨学科联系”部分，我们将见证这一个简单的思想如何提供一个统一的视角，用以分析从生命的多样性、计算的物理成本，到宇宙自身结构等万事万物。

## 原理与机制

### 什么是“信息”？惊奇程度的度量

什么是“信息”？这个词非常普遍，以至于我们很少停下来思考它。如果一个朋友告诉你：“今天早上太阳升起来了”，你基本上什么也没学到。这是一个必然的结果。但如果同一个朋友说：“我刚中了彩票”，你的世界会瞬间为之震动。你接收到了大量的信息。

这个简单的对比掌握了严格定义信息的关键，这一概念由杰出的数学家和工程师 Claude Shannon 开创。他意识到，**信息是惊奇程度的度量**。一个极不可能发生的事件非常令人惊讶，得知它发生便提供了大量信息。而一个确定的事件则不提供任何信息。

要建立一门信息科学，我们需要将这种直觉转化为数学。我们需要一个函数，它随着事件概率 $p$ 的减小而增大。一个简单的选择可能是 $\frac{1}{p}$，但 Shannon 发现了一个更好的选择：$\log(\frac{1}{p})$，也就是 $-\log(p)$。

为什么用对数？因为它有一个神奇的性质：它使得独立事件的信息具有可加性。想象一下，分别抛掷两枚均匀的硬币。第一枚硬币正面朝上*且*第二枚硬币反面朝上的概率是 $\frac{1}{2} \times \frac{1}{2} = \frac{1}{4}$。我们从观察这个联合结果中获得的信息，应该是从每次单独抛掷中获得的信息之和。对数自动为我们做到了这一点：$\log_2(4) = \log_2(2) + \log_2(2)$。这种可加性不仅方便，更是我们思考如何组合知识的基础。

当然，我们通常不关心单个特定结果的信息内容，而是关心事件发生*前*某个情况的*平均不确定性*。为了求得这个平均值，我们只需取每个可能结果 $i$ 的信息 $-\log_2(p_i)$，并按其发生的概率 $p_i$ 进行加权。将所有可能性相加，就得到了著名的**[香农熵](@article_id:303050)**公式：

$$
H = -\sum_{i} p_i \log_2(p_i)
$$

结果 $H$ 是一个量化系统固有总不确定性的数值。选择以 2 为底的对数（$\log_2$）意味着我们用一种称为**比特**（bits）的单位来衡量这种不确定性。一个“比特”是你在等待一次均匀硬币抛掷结果时所面临的不确定性大小。正如我们将看到的，这个简单的公式功能强大，足以描述从[量子计算](@article_id:303150)机的状态到[热力学](@article_id:359663)的基本定律等一切事物 [@problem_id:1867963]。

### 抛硬币：一比特的不确定性

让我们用一个能想到的最简单的非平凡例子来探讨这个想法：一个只有两种结果的单一事件。这可以是一次抛硬币，一个消费者在两个品牌之间做出选择，或者[计算机内存](@article_id:349293)中一个比特位是 0 还是 1。用概率论的语言来说，这是一个伯努利试验（Bernoulli trial）[@problem_id:687]。

假设“成功”（例如，正面朝上）的概率是 $p$，那么“失败”（反面朝上）的概率就是 $1-p$。将此代入我们的新公式，熵为：

$$
H(p) = -[p \log_2(p) + (1-p) \log_2(1-p)]
$$

让我们来研究一下这个函数。如果硬币是双面的（两面都是正面），那么 $p=1$。你完全确定结果会是正面。没有惊奇，没有不确定性。我们的公式也证实了这一点：$H(1) = -[1 \log_2(1) + 0 \log_2(0)] = 0$。（我们定义 $0 \log 0 = 0$，因为零概率事件对平均不确定性没有贡献）。对于 $p=0$ 的情况也是如此。

现在，假设硬币有很大的偏向性，比如 $p=0.99$。你几乎可以肯定它会是正面。不确定性非常低，$H(0.99)$ 的值是一个接近于零的小数。只有当那罕见的 $1\%$ 事件发生时，你才会真正感到惊讶。

那么，什么时候你*最*不确定呢？什么时候你预测结果的能力处于绝对最低点？你的直觉会告诉你答案：当硬币完全均匀，即正面和反面出现的可能性相等时。也就是说，当 $p = \frac{1}{2}$ 时。在这一点上，你没有任何理性依据去偏好任何一种结果。简单的微积分计算可以证实，函数 $H(p)$ 在此时达到其绝对最大值 [@problem_id:1899954]。让我们来计算一下这个值：

$$
H\left(\frac{1}{2}\right) = -\left[\frac{1}{2} \log_2\left(\frac{1}{2}\right) + \frac{1}{2} \log_2\left(\frac{1}{2}\right)\right] = -\left[\frac{1}{2}(-1) + \frac{1}{2}(-1)\right] = 1
$$

一比特。我们找到了信息的[基本单位](@article_id:309297)。它是一个完全均衡的二元选择中所固有的不确定性。

### 最大无知原理

这个结果远不止是关于抛硬币的数学趣闻。它是一个深刻而普适的概念的例证，通常被称为**[最大熵原理](@article_id:313038)**。熵最大的[概率分布](@article_id:306824)是那个最不偏不倚、最均匀的分布，是那个包含最少隐藏偏见或假设的分布。它最能代表一种最大无知的状态——或者，从更积极的角度来说，是最大程度的开放心态。

如果一个系统可能有 $N$ 种可能的结果，而不仅仅是两种呢？这个原理依然成立。当没有任何理由相信某一个结果比其他结果更可能发生时，不确定性达到最大。这种情况发生在概率被均匀地分布在所有可能性上时：对于所有的 $i$，$p_i = \frac{1}{N}$。

让我们将这个[均匀分布](@article_id:325445)代入香农公式：

$$
H_{\text{max}} = -\sum_{i=1}^{N} \frac{1}{N} \log_2\left(\frac{1}{N}\right) = -N \cdot \left(\frac{1}{N} \log_2\left(\frac{1}{N}\right)\right) = -\log_2\left(\frac{1}{N}\right) = \log_2(N)
$$

这个更简单的表达式 $H_0 = \log_2(N)$ 被称为**[哈特利熵](@article_id:326312)（Hartley entropy）**。我们现在不把它看作一个与之竞争的定义，而是更普适的[香农熵](@article_id:303050)的一个特例——它是一个具有 $N$ 个状态的系统可能达到的*最大*熵 [@problem_id:1629247]。任何偏离这种完美均匀性的情况，例如一个环境传感器发出“正常”信号的频率高于错误信号，都必然包含更多的信息（或更少的不确定性），因此其熵将小于这个最大可能值 $\log_2(N)$ [@problem_id:1963606]。

值得注意的是，这个原理可以无缝地从离散选择扩展到连续可能性。如果一个粒子被限制在一个从点 $a$ 延伸到点 $b$ 的盒子中，什么样的[概率分布](@article_id:306824)能反映其位置的最大不确定性？答案是[均匀分布](@article_id:325445)，即一条平坦的线，表示粒子在任何地方被发现的可能性都相等。利用[变分法](@article_id:300897)的工具，可以证明这正是使[香农熵](@article_id:303050)泛函的连续版本最大化的分布 [@problem_id:2051942]。这个原理是普适的：最大熵和最大无偏性是同一回事。

### 通往物理学的桥梁：熵即缺失的信息

此时，你可能会认为这只是一个非常优雅的[通信理论](@article_id:336278)数学框架。但现在，我们的故事将发生戏剧性的转折。如果你学过物理学，表达式 $-\sum p_i \ln p_i$ 应该会让你不寒而栗。除了一个常数之外，它与[统计力](@article_id:373880)学中的**[吉布斯熵](@article_id:314565)（Gibbs entropy）**公式完全相同，而后者是[热力学](@article_id:359663)的基石之一：

$$
S = -k_B \sum_{i} p_i \ln(p_i)
$$

在这里，$p_i$ 是一个物理系统处于特定微观状态（其所有原子及其动量的特定[排列](@article_id:296886)）的概率，而 $k_B$ 是一个被称为玻尔兹曼常数（Boltzmann constant）的自然[基本常数](@article_id:309193)。

这仅仅是巧合吗？是大自然在和我们开数学玩笑吗？完全不是。这是整个科学领域中最深刻、最美妙的统一之一。香农的[信息熵](@article_id:336376)和吉布斯的[热力学熵](@article_id:316293)，实际上是在度量*完全相同的基本量*。

它们的关系是一个简单的正比关系：$S = (k_B \ln 2) H$ [@problem_id:1967976]。这些常数仅仅是转换因子。$\ln(2)$ 这一项只是将对数的底从 2（用于比特）转换为自然底数 $e$（用于物理学家偏爱的单位“奈特”（nats））。[玻尔兹曼常数](@article_id:302824) $k_B$ 才是真正深刻的部分。它是抽象信息与物理现实之间的转换因子。它精确地告诉我们，关于一个系统状态的一比特缺失信息，对应于多少[焦耳](@article_id:308101)每[开尔文](@article_id:297450)的能量。

我们可以在一个具体的物理过程中看到这种非凡的联系：两种不同气体的混合。想象一个被隔板分开的盒子，左边是氦气，右边是氖气。最初，你的知识是完备的。如果你能从左边取出一个粒子，你确定它是氦。关于粒子身份的[信息熵](@article_id:336376)为零。

现在，你移开隔板。原子们开始混合，随机运动，直到它们[均匀分布](@article_id:325445)。系统变得更加无序，任何物理学家都会告诉你，它的[热力学熵](@article_id:316293)增加了一个量，称为**混合熵**。但从信息的角度思考一下。如果你现在从盒子中取出一个原子，你不再确定它的身份。它可能是氦，也可能是氖。你的不确定性——你的[香农熵](@article_id:303050)——增加了。

这就是惊人的结论：计算出的[热力学熵](@article_id:316293)增量（$\Delta S_{\text{mix}}$）与你的香农[信息熵](@article_id:336376)增量（$\Delta H$）*完全*成正比。而比例常数是什么呢？它正是玻尔兹曼常数 $k_B$ [@problem_id:1632179]。这不是一个比喻。[热力学熵](@article_id:316293)*就是*缺失的信息。它是我们对世界精确微观状态无知的度量。

### 作为物理量的信息

如果[信息熵](@article_id:336376)是物理上真实存在的，它应该像其他物理量一样行事。在[热力学](@article_id:359663)中，我们将性质分为**[强度性质](@article_id:307936)**（intensive，与系统大小无关，如温度或密度）或**[广延性质](@article_id:305834)**（extensive，随系统大小而变化，如体积或质量）两类。熵属于哪一类呢？

让我们将一个“系统”建模为一条由 $N$ 个符号组成的消息，其中每个符号都独立地从一个固定的字母表（如字母 A-Z）中选择。设与单个符号选择相关的熵为 $H_1$，这是一个由每个字母的概率决定的常数。

由于每次选择都是独立的，整条消息的总不确定性就是各部分不确定性的总和。因此，长度为 $N$ 的消息的总熵为 $H_N = H_1 + H_1 + \dots + H_1 = N \times H_1$ [@problem_id:1971017]。

总熵与系统的大小 $N$ 成直接的线性关系。根据定义，这使得香农熵成为一个**[广延性质](@article_id:305834)**，就像它的[热力学](@article_id:359663)“表亲”一样。两个相同、独立的系统组合在一起的熵是单个系统熵的两倍。

这幅图景至此完整了。香农的优雅抽象概念，源于在嘈杂[信道](@article_id:330097)中发送消息的实际问题，结果却是一个具有最深刻物理根源的概念。它遵循着与支配时间方向、[发动机效率](@article_id:307095)和生命化学的熵相同的尺度缩放和可加性规则。它揭示了自然界中惊人的统一性，表明硬币抛掷的不确定性与星辰不可逆的混合，其核心都是同一基本原理的体现：信息、概率以及宇宙向着更大不确定性状态发展的无情趋势。