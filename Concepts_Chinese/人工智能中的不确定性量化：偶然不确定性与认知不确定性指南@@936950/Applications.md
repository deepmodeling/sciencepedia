## 应用与跨学科联系

在我们迄今为止的旅程中，我们已经探索了不确定性的原理，学会了区分两种基本的“不知”。第一种是**[偶然不确定性](@entry_id:154011)**，即世界固有的随机性——骰子的滚动、分子的不可预测的[抖动](@entry_id:262829)。即使有完美的模型，这种模糊性依然存在。第二种是**[认知不确定性](@entry_id:149866)**，即我们自身的无知——源于有限数据或不完美模型的不确定性。这是一种“我不确定，因为我以前没见过这个”的疑虑，原则上，我们可以通过学习更多来减少它。

这种区分远不止是学术上的好奇心。它是解锁在现实世界中安全、合乎道德且有效地使用人工智能的关键。知道你*为什么*不确定，就是知道下一步该做什么。现在，我们将看到这个深刻的思想如何在从重症监护室的繁忙走廊到模拟恒星核心的广阔应用场景中焕发生机。

### 医学领域的高风险：当人工智能必须说“我不确定”时

没有任何地方比医学领域更能体现正确决策的沉重负担。当我们构建一个辅助医生的人工智能时，我们不是在构建一个简单的计算器，而是在构建一个合作伙伴。一个值得信赖的合作伙伴必须诚实地面对其知识的局限。

想象一下，在重症监护室里，一个临床人工智能的任务是估计患者发展成败血症（一种危及生命的疾病）的概率 [@problem_id:4442178]。医生必须决定是否使用强效抗生素。这个决策的错误成本很高：假阴性（漏掉一个真正的败血症病例）可能是致命的，而[假阳性](@entry_id:635878)（治疗一个健康的患者）则会加剧抗生素耐药性并可能产生副作用。最优决策取决于对这些风险的仔细权衡，而这种权衡又取决于人工智能的预测概率 [@problem_id:4442774]。

但如果人工智能说有 30% 的败血症几率，这*意味着*什么？这时，我们那两种不确定性就变得至关重要。高的[偶然不确定性](@entry_id:154011)意味着患者的状况确实模棱两可；他们的生命体征正处于危险边缘。结果本身就很难预测。而高的认知不确定性则意味着完全不同的事情：人工智能遇到了一个它不熟悉的患者档案，可能来自其训练数据中代表性不足的人群。模型实际上是在说：“这超出了我的能力范围。”

对这两种情况的反应应该完全不同。对于高的[偶然不确定性](@entry_id:154011)，医生可能会采取“观察等待”的策略。对于高的[认知不确定性](@entry_id:149866)，正确的做法是不信任人工智能的预测，并依赖人类的专业知识。此时，人工智能最有价值的贡献不是它的预测，而是它对自己无能的声明。这种*安全移交*原则是负责任人工智能的基石。在数学上，我们可以利用[全方差定律](@entry_id:184705)来解开这些不确定性。预测的总方差可以优雅地分为两部分：一部分反映了结果的平均固有随机性，另一部分则反映了由于我们对[模型参数不确定性](@entry_id:752081)而导致的预测[分歧](@entry_id:193119)或方差 [@problem_id:4442178] [@problem_id:5225980]。

[不确定性的来源](@entry_id:164809)本身也是多种多样的，不仅仅关乎最终的预测模型。不确定性可能从一开始就潜入，源于数据本身。考虑从 MRI 扫描中测量肿瘤体积 [@problem_id:4525766]。不同的放射科医生或不同的人工智能分割算法可能会画出略有不同的边界，导致测量体积的可变性。这是一种测量误差。如果治疗决策取决于体积是否超过某个阈值，这种“分割可变性”可能纯粹由于偶然性导致错误的决策。对于一个真实肿瘤体积略低于阈值的患者来说，一个小的测量误差可能就是接受标准剂量和高剂量之间的区别。同样，用于心率或体温等生命体征的嘈杂传感器会将误差引入人工智能的输入中 [@problem_id:5201653]。一个负责任的人工智能系统必须考虑到这种“输入模糊性”，将不确定性从输入一直传播到最终预测。这使得它能够将一个预测标记为不太可靠，不是因为模型不好，而是因为原始输入数据就很模糊。

这引出了[不确定性量化](@entry_id:138597)最关键的应用之一：争取公平和消除偏见。人工智能模型从数据中学习，如果数据反映了历史偏见，人工智能也会学会它们。一个主要在某个人群上训练的模型，在另一个人群上可能表现不佳。我们如何检测到这一点？高的[认知不确定性](@entry_id:149866)是一个强大的“偏见检测器”[@problem_id:5225980]。如果一个人工智能模型持续对来自特定群体的患者显示出更高的[认知不确定性](@entry_id:149866)，这是一个巨大的警示信号，表明模型正在一个不熟悉的领域运行，很可能是由于在训练数据中代表性不足。

更糟糕的是，一个看似合理、规避风险的策略可能会放大这种不公平。假设一家医院决定，只有在对人工智能的高风险预测非常有信心时，才进行一种有风险但能挽救生命的治疗。这是通过要求[风险估计](@entry_id:754371)的*下限*高于决策阈值来实现的 [@problem_id:4849755]。对于一个模型具有高不确定性的患者群体，这个下限会被拉低，意味着他们接受治疗的可能性更小，即使他们的平均风险评分与来自一个代表性良好群体的人相同。不平等的不确定性变成了不平等的医疗机会。伦理上的要求是双重的：短期内，我们可以考虑调整决策阈值以确保公平的结果；长期内，我们必须通过收集更多数据来解决根本原因，以减少弱势群体的[认知不确定性](@entry_id:149866) [@problem_id:4849755]。

为了将这些安全原则付诸实践，工程师们开发了强大的框架。一种被称为**保形预测 (conformal prediction)** 的方法提供了一个非常直接的保证。它不只是给出一个概率，而是创建一个预测集，该预测集保证以指定的频率（例如，90% 的时间）包含真实结果，而不管底层数据分布如何 [@problem_id:4404437]。对医生来说，这转化为一个具体的承诺：“对于这个新患者，我给你一个风险范围。我保证我用来生成这个范围的方法在 10 次中有 9 次是正确的。” 这为做出高风险决策提供了一个严谨的安全网 [@problem_id:4405942]。

### 超越医学：预测我们世界的未来

基本原理的美妙之处在于其普适性。那些帮助我们构建更安全医疗人工智能的相同思想，正被用于应对科学和工程领域一些最大的挑战。

在[气候科学](@entry_id:161057)中，研究人员构建复杂的模型来预测降雨等现象 [@problem_id:4040927]。这些模型与所有模型一样，都是不确定的。通过分解机器学习气候模型的预测方差，科学家可以区分[认知不确定性](@entry_id:149866)（不同模型结构或训练运行之间的[分歧](@entry_id:193119)）和[偶然不确定性](@entry_id:154011)（天气的内在混乱和未解决的物理问题）。这告诉他们应该把精力集中在哪里。如果认知不确定性高，他们需要更好的模型和更多的数据。如果[偶然不确定性](@entry_id:154011)高，这告诉我们可预测性的基本极限，从而为我们如何设计有弹性的基础设施提供信息。

让我们从地球大气层转向人造恒星的核心。在核聚变研究中，科学家在称为[托卡马克](@entry_id:182005) (tokamaks) 的设备中使用强大的磁场来约束比太阳还热的等离子体。一个主要挑战是防止“破裂”(disruptions)，即等离子体变得不稳定并可能损坏机器。现在，人工智能模型被用来预测这些破裂，并能提前宝贵的几秒钟发出警报 [@problem_id:4003846]。它们是如何做到的？通过使用模型的*集成*。当集成中的不同模型开始在距离破裂的时间上产生分歧时，这标志着高的认知不确定性——一个警告，表明系统正在进入一个知之甚少的状态。模型*之间*预测的方差是认知不确定性的直接估计，而每个模型自身随机预测（例如，来自 MC dropout）的平均方差*内部*则估计了偶然部分。其数学原理与气候科学的例子完全相同，这是一个统一概念在起作用的优美例证。

同样利用模型[分歧](@entry_id:193119)作为指导的思想也出现在新材料的探索中 [@problem_id:3822074]。从量子力学的第一性原理模拟原子间的相互作用非常精确，但计算成本高得惊人。科学家现在使用[机器学习势](@entry_id:183033) (machine learning potentials) 以一小部分成本来近似这些力。但要训练这些模型，你需要来自昂贵模拟的数据。你应该在哪里收集数据？答案是一种称为**[主动学习](@entry_id:157812) (active learning)** 的策略，由不确定性引导。一个机器学习模型集成在一个初始数据集上进行训练。然后，它们都被要求预测一个新的、未见过的原子构型的力。引起模型之间*最大分歧*的构型，就是模型最不确定的地方。这是运行昂贵的[量子模拟](@entry_id:145469)最有信息量的构型。然后，这个新的、信息量高的数据点被添加到[训练集](@entry_id:636396)中，模型被重新训练。不确定性成为引导科学发现的指南针，使整个过程的效率呈指数级增长。

### 知其所不知的智慧

在所有这些领域中，一个单一而强大的主题浮现出来。[量化不确定性](@entry_id:272064)不是承认失败，而是一种更复杂、更诚实、最终也更有用的知识形式。仅仅给出一个预测，就像一个黑箱神谕。给出一个预测，并附上对其[置信度](@entry_id:267904)的衡量——以及缺乏信心的原因——才是成为一个真正的协作者。

通过教我们的机器区分世界的随机性和它们自身的无知，我们正在向它们灌输一丝科学的智慧。我们正在构建的工具不仅能给我们答案，还能与我们展开对话，探讨什么是已知的，什么是未知的，以及我们应该做什么来发现更多。这就是我们与人工智能合作的未来——不是盲目信任，而是透明、协作的发现。