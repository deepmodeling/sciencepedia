## 引言
当人工智能给出一个答案时，我们如何才能信任它？除了简单的预测，理解人工智能的[置信度](@entry_id:267904)——或其缺失——对于构建安全可靠的系统至关重要。然而，单一的置信度分数可能会产生误导，因为它未能区分不同来源的疑虑。本文旨在通过探索不确定性量化这一基本概念来弥补这一关键空白。它将人工智能的不确定性解构为两种截然不同的类型：世界固有的随机性和模型自身的无知。在接下来的章节中，我们将首先深入探讨定义并从数学上分离这些不确定性的“原理与机制”。随后，“应用与跨学科联系”部分将展示这种强大的区分如何应用于从医学到气候科学等高风险领域，从而实现与智能系统之间新层次的信任与协作。

## 原理与机制

想象你正在向一位神谕——一个智慧、无所不知的人工智能——请教一个关键问题。你不仅想要一个答案，还想知道这位神谕有多自信。当它犹豫时，是因为未来真的模糊不清、难以预测？还是因为你的问题太过奇怪，以至于这位神谕在其广博但有限的经验中，从未遇到过类似的情况？

这两种基本的“不知”之间的区别，是[人工智能不确定性量化](@entry_id:634027)的基石。它体现了两种人工智能的区别：一种能意识到世界固有的模糊性，另一种能意识到自身的无知。我们将这两个概念称为**[偶然不确定性](@entry_id:154011) (aleatoric uncertainty)** 和**认知不确定性 (epistemic uncertainty)**。理解它们不仅仅是一项学术活动，更是构建安全、可靠和可信赖的人工智能系统的关键。

### 世界的固有模糊性：[偶然不确定性](@entry_id:154011)

**Aleatoric** 一词源自拉丁语 *alea*，意为“骰子”——即一对骰子中的一个。它描述了我们在机遇游戏中遇到的那种不确定性。即使我们拥有一个完美的掷骰子物理模型，也无法确切预测单次投掷的结果。我们最多只能陈述概率。这是一种系统本身固有的随机性，是世界不可简化的噪声。

在人工智能领域，[偶然不确定性](@entry_id:154011)源于数据本身。它可能来自嘈杂的传感器、生物过程中的随机波动，或根本上缺失的信息。设想一个医疗人工智能，其任务是从胸部 X 光片中检测气胸（肺塌陷）。如果图像因患者在扫描过程中移动而变得模糊怎么办？[@problem_id:4418697] 即使是世界上最有经验的放射科医生看了也可能会说：“这很难说。” 数据中根本不存在做出明确判断所需的信息。

一个设计良好的人工智能不应假装知晓不可知之事，而应表达出这种模糊性。在这种假设情况下，一个贝叶斯人工智能可能会生成一系列紧密聚集在概率 $0.5$ 附近的预测。例如，如果我们向模型询问八次，可能会得到如下预测：
$$[0.49, 0.51, 0.50, 0.48, 0.52, 0.51, 0.49, 0.50]$$
注意其一致性。模型所有内部的“推理路径”都汇集于同一个结论：这是一个对半开的情况。模型*确信*情况是*不确定*的。这是高**[偶然不确定性](@entry_id:154011)**的标志。

这种类型的不确定性无法通过简单地收集更多同[类数](@entry_id:156164)据来减少。给人工智能喂入数千张更模糊的 X 光片，并不能帮助它解读第一张。减少[偶然不确定性](@entry_id:154011)的唯一方法是提高数据本身的质量——例如，通过使用更好的成像技术或开发运动校正软件 [@problem_id:4418697]。

### 模型的自我坦承的无知：[认知不确定性](@entry_id:149866)

**Epistemic** 一词源自希腊语 *episteme*，意为“知识”。**[认知不确定性](@entry_id:149866)**是由于缺乏知识而产生的不确定性。这是模型在坦承：“我不知道，因为我没学过这个。” 当模型被迫对与训练数据截然不同的数据进行预测时，就会出现这种情况。这是关于模型本身的不确定性。

让我们回到那个专门用成人 X 光片训练的气胸检测器。当我们给它看一张儿童患者的图像时会发生什么？[@problem_id:4418697] 儿童的解剖结构不同，而且图像可能是用模型从未见过的便携式扫描仪拍摄的。此时，人工智能正在“分布外”运行。面对这种不熟悉的输入，模型的内部推理可能会出现[分歧](@entry_id:193119)。如果我们再次要求八次预测，可能会看到一个完全不同的模式：
$$[0.12, 0.86, 0.18, 0.90, 0.08, 0.91, 0.20, 0.88]$$
看这深刻的分歧！模型的一些部分在大喊“无疾病”（概率接近 $0$），而另一些部分则在尖叫“有疾病！”（概率接近 $1$）。模型陷入了深深的矛盾。这些预测中的巨大方差正是高**[认知不确定性](@entry_id:149866)**的明显标志。人工智能在发出信号，表明其预测不可靠，因为它正在远超其经验范围进行推断。当用于糖尿病视网膜病变筛查的模型看到一张不寻常的图像时，也会出现类似的[置信度](@entry_id:267904)危机，导致一组双峰预测，如 $[0.05, 0.85, 0.90, \dots]$ [@problem_id:5210036]。

一个关于认知不确定性的优美而纯粹的例子来自模拟器 (emulators) 的世界 [@problem_id:3888326]。想象一个复杂、计算成本高昂的气候模拟器，它是完全确定性的：给予相同的输入，它总会产生完全相同的输出。这里没有[偶然不确定性](@entry_id:154011)。现在，我们通过在一些样本输入上运行这个模拟器，来训练一个快速的人工智能模型（一个“模拟器”）来近似它。模拟器对于模拟器在*新的*、未经测试的输入上的输出的不确定性，纯粹是认知性的。它的不确定性直接衡量了其无知程度，这种无知通常在远离其训练区域的输入空间中最高。

与[偶然不确定性](@entry_id:154011)不同，[认知不确定性](@entry_id:149866)*可以*通过更多数据来减少。如果我们用一个包含儿童 X 光片的多样化数据集来训练我们的人工智能，它对新儿童病例的认知不确定性就会降低。因此，高的认知不确定性是一个至关重要的信号：它告诉我们模型在哪里薄弱，需要更多训练，或者何时应该放弃决策，将问题交给人类专家 [@problem_id:5210036]。

### 统一原则：[全方差定律](@entry_id:184705)

这种将不确定性优雅地划分为两类，并不仅仅是一种方便的叙述，而是概率论基本定理——**[全方差定律](@entry_id:184705)**——的直接结果。该定理出奇地简单而深刻，为我们整个讨论提供了数学支柱。

对于任何预测量 $Y$，[全方差定律](@entry_id:184705)指出，总不确定性可以被完美地分解：
$$ \text{总方差} = \text{偶然不确定性} + \text{认知不确定性} $$

更正式地，如果我们将“模型”（由其参数 $\theta$ 表示）视为不确定性的一个来源，[全方差定律](@entry_id:184705)给了我们这个优美的分解 [@problem_id:4422525]：
$$ \mathrm{Var}(Y) = \mathbb{E}_{\theta}[\mathrm{Var}(Y \mid \theta)] + \mathrm{Var}_{\theta}(\mathbb{E}[Y \mid \theta]) $$

让我们来解析一下这个公式。

1.  **偶然分量: $\mathbb{E}_{\theta}[\mathrm{Var}(Y \mid \theta)]$**
    这一项代表*平均[固有噪声](@entry_id:261197)*。内部部分 $\mathrm{Var}(Y \mid \theta)$ 是在假设我们完全了解模型参数 $\theta$ 的情况下，结果的随机性。这是世界不可简化的模糊性。外部的期望 $\mathbb{E}_{\theta}[\cdot]$ 只是在我们对模型的不确定性上对这种[固有噪声](@entry_id:261197)进行平均。在实践中，这通过观察模型每个“内部声音”的平均预测不确定性来估计 [@problem_id:3807440] [@problem_id:3317115]。

2.  **认知分量: $\mathrm{Var}_{\theta}(\mathbb{E}[Y \mid \theta])$**
    这一项代表*模型共识预测的方差*。内部部分 $\mathbb{E}[Y \mid \theta]$ 是由具有参数 $\theta$ 的单一版本模型做出的预测。外部的方差 $\mathrm{Var}_{\theta}(\cdot)$ 衡量了当我们考虑所有可能的模型时，这些预测彼此之间的分歧程度。这正是我们在儿童 X 光片例子中看到的那种“分歧”。

这种分解不是一个近似或一种便利；它是一个数学恒等式。它向我们保证，通过要求我们的人工智能将其知识表示为可能答案的分布，而不仅仅是单一答案，我们可以清晰地解开它对世界随机性的了解与它对自己局限性的了解。随着我们为人工智能提供越来越多的数据，我们会看到这个原则在起作用：随着模型无知的消退，认知项向零收缩，而偶然项则收敛到被建模过程的真实、不可移除的噪声水平 [@problem_id:5174319]。这是学习的数学标志。

