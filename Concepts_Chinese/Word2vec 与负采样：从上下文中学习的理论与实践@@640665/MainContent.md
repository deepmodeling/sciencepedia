## 引言
在自然语言处理领域，将词语表示为几何空间中有意义的向量，而非孤立的符号，是一次革命性的飞跃。这些被称为“[词嵌入](@entry_id:633879)”的表示方法捕捉了复杂的语义关系，使我们能够推断出“king”之于“queen”犹如“man”之于“woman”。word2vec 模型是创建这些嵌入的里程碑，但其最初的公式面临着严重的计算瓶瓶颈，使其在处理大型词汇表时速度缓慢且不切实际。本文旨在解决这一挑战，重点关注[负采样](@entry_id:634675)——一种高效的训练方法，它使 word2vec 成为一种实用且具有变革性的工具。在接下来的章节中，我们将首先深入探讨[负采样](@entry_id:634675)的“原理与机制”，探索其数学基础和背后优雅的直觉。然后，我们将遍历其“应用与跨学科联系”，发现这个强大的思想如何远远超越语言的范畴，为基因组学、网络科学等领域中的复杂关系建模。

## 原理与机制

想象你是一位雕塑家，但你的媒介不是石头，而是广阔的高维空间。你的工具不是锤子和凿子，而是数据和算法。你的目标是雕刻出意义，将一团无形的点云——每个点代表一个词——[排列](@entry_id:136432)成一个美丽的结构，其中距离意味着相关性。像“king”和“queen”这样的词应该很接近，而从“king”到“queen”的向量应与从“man”到“woman”的向量惊人地相似。这就是[词嵌入](@entry_id:633879)的承诺，而[负采样](@entry_id:634675)是实现这种艺术的最优雅和强大的工具之一。

### 意义的物理学：推拉的游戏

其核心在于，训练过程是一场由简单的物理直觉主导的舞蹈：吸引和排斥。当我们在一个真实句子中观察到两个词一起出现时——比如“fox”出现在“quick”附近——我们希望将它们的[向量表示](@entry_id:166424)拉得更近。相反，如果我们取一个词如“fox”并将其与一个*没有*出现在其上下文中的词（如“asleep”）配对，我们则希望将它们的向量推开。这就是驱动学习的基本“推-拉”动态。

在数学上，这个游戏被形式化为一个目标函数。对于一个真实的、观察到的中心词 $w$ 和上下文词 $c$ 的词对，我们希望最大化它们嵌入的相似性。在 word2vec 中，嵌入就是向量，衡量相似性的一个自然方法是**[点积](@entry_id:149019)**。更大的[点积](@entry_id:149019)意味着向量更对齐。我们将这个[点积](@entry_id:149019)通过一个**逻辑 S 型函数** (logistic sigmoid function)，$\sigma(x) = 1/(1 + \exp(-x))$，它将任何值压缩到 0 和 1 之间的一个概率。所以，对于一个“正”词对 $(w, c)$，我们希望使概率 $\sigma(v_w^\top u_c)$ 尽可能接近 1，其中 $v_w$ 是中心词的“输入”嵌入，$u_c$ 是上下文词的“输出”嵌入。

但仅有吸引是不够的。如果我们只把向量拉到一起，它们最终会坍缩成一个点。我们需要排斥。这就是**[负采样](@entry_id:634675)**发挥作用的地方。对于每个正词对，我们通过从词汇表中随机抽取词 $n_i$ 来虚构几个“负”或“噪声”词对 $(w, n_i)$。对于这些假的词对，我们希望它们的相似性尽可能低。也就是说，我们希望概率 $\sigma(v_w^\top u_{n_i})$ 接近 0，或者等价地，我们希望 $\sigma(-v_w^\top u_{n_i})$ 接近 1。

一个训练实例的总目标结合了这些愿望。我们想要最大化：
$$
J = \ln\big(\sigma(v_{w}^{\top} u_{c})\big) + \sum_{i=1}^{k} \ln\big(\sigma(-v_{w}^{\top} u_{n_{i}})\big)
$$
在这里，$k$ 是我们选择的负样本数量。当我们使用梯度上升来最大化这个目标时，梯度自然地分解为两种力 [@problem_id:3200018]。相对于中心词向量 $v_w$ 的梯度是：
$$
\frac{\partial J}{\partial v_{w}} = \underbrace{\big(1 - \sigma(v_{w}^{\top} u_{c})\big) u_{c}}_{\text{拉向正向上下文}} - \underbrace{\sum_{i=1}^{k} \sigma(v_{w}^{\top} u_{n_{i}}) u_{n_{i}}}_{\text{推离负样本}}
$$

看这个方程多么优美！第一项通过加上正向上下文向量 $u_c$ 的一部分来更新 $v_w$。这个“拉力”的大小由 $(1 - \sigma(v_{w}^{\top} u_{c}))$ 控制。如果模型已经确信这是一个好的词对（即 $\sigma(v_{w}^{\top} u_{c})$ 接近 1），这一项就会变得非常小——学习已经完成。如果模型错了（[点积](@entry_id:149019)很低），拉力就很强。第二项则相反。它减去每个负样本向量 $u_{n_i}$ 的一部分。这个“推力”的大小由 $\sigma(v_{w}^{\top} u_{n_{i}})$ 控制。如果模型错误地认为一个负样本是一个好的上下文词（[点积](@entry_id:149019)很高），推力就很强，以纠正错误。如果它已经确信这个负样本不匹配，推力就很弱。

### 舞蹈中的一步：观察向量学习

让我们把这变得更具体些。想象一个微型词汇表 $\{a, b, c\}$，我们从随机放置的向量开始。假设我们正在训练词对 $(b \to a)$，意味着 $b$ 是中心词，$a$ 是上下文词。然后我们抽取两个负样本，恰好是 $b$ 和 $c$。遵循详细计算中的过程 [@problem_id:3200045]，我们应用梯度更新规则。

中心词的向量 $v_b$ 同时感受到三种力：
1.  一股朝向正向上下文向量 $u_a$ 的“拉力”。
2.  一股远离第一个负样本向量 $u_b$ 的“推力”。
3.  一股远离第二个负样本向量 $u_c$ 的“推力”。

同时，上下文词的向量也会被更新。正向上下文向量 $u_a$ 被拉向 $v_b$。负向上下文向量 $u_b$ 和 $u_c$ 被推离 $v_b$。这是一场多体舞蹈，每个参与者都根据其与中心词的互动来调整自己的位置。仅经过一步[随机梯度下降](@entry_id:139134) (SGD) 舞蹈，向量就发生了轻微的移动。[点积](@entry_id:149019) $v_b^\top u_a$ 会增加，而 $v_b^\top u_b$ 和 $v_b^\top u_c$ 会减少。将这个过程在一个大型语料库的词对上重复数百万次，一个宏伟的结构就会从混乱中浮现。

### [负采样](@entry_id:634675)的天才之处：从百万问题到寥寥数问

有人可能会问，为什么要费心去采样负样本呢？为什么不直接做“正确”的事情？最初的 word2vec 模型提出了一个 **softmax** 输出层。这种方法将真实词对 $(w,c)$ 与词汇表中*所有其他可能的上下文词*进行比较。它试图最大化这个概率：
$$
P(c|w) = \frac{\exp(v_w^\top u_c)}{\sum_{c' \in \mathcal{V}} \exp(v_w^\top u_{c'})}
$$
问题出在分母上。要计算它，你必须计算 $v_w$ 和词汇表 $\mathcal{V}$ 中*每一个词*的向量的[点积](@entry_id:149019)。如果你的词汇表有 50,000 个词，那就是 50,000 次[点积](@entry_id:149019)和一个巨大的求和，而且是每一步训练都要计算！这在计算上是致命的。

[负采样](@entry_id:634675)是对这个计算陷阱的巧妙逃脱。我们不再要求模型从 50,000 个词中挑出正确的词，而是重构了这个任务。我们给模型一个真实词对和少数几个虚假词对（比如，$k=5$ 或 $k=15$），然后问它 $k+1$ 个简单的“是/非”问题：“这个词对合理吗？”这被称为**噪声对比估计** (Noise-Contrastive Estimation, NCE)，而[负采样](@entry_id:634675)是它的一个简化版本。计算成本从与词汇表大小成正比 $O(|\mathcal{V}|)$，急剧下降到只与负样本数量成正比 $O(k)$ [@problem_id:3199987]。对于一个典型的词汇表和 $k \approx 15$ 的情况，这意味着训练运行时间可能从几个月缩短到几个小时。正是这种效率使 word2vec 成为一场实践性的革命。

### 秘密身份：[Word2Vec](@entry_id:634267) *真正*在做什么

所以，[负采样](@entry_id:634675)是一个高效的技巧。但它仅仅是个技巧吗？还是有更深层次的东西在发生？在该领域最美的理论成果之一中，研究人员证明了这个简单高效的训练过程正在隐式地做一件意义深远的事情：它在分解一个词共现[统计矩](@entry_id:268545)阵。

让我们定义一个叫做**点[互信息](@entry_id:138718) (PMI)** 的量。对于一个词-上下文对 $(w,c)$，它被定义为：
$$
\text{PMI}(w,c) = \log \frac{P(w,c)}{P(w)P(c)}
$$
$P(w,c)$ 是观察到 $w$ 和 $c$ 一起出现的概率，而 $P(w)$ 和 $P(c)$ 是它们各自的概率。PMI 告诉我们这两个词共现的概率比它们独立出现时的概率高（或低）多少。高 PMI 意味着强关联。

惊人的发现是，带有[负采样](@entry_id:634675)的 word2vec skip-gram 模型 (SGNS) 在训练至最优时，其得到的嵌入的[点积](@entry_id:149019)近似于 PMI，但有一个常数偏移：
$$
v_w^\top u_c \approx \text{PMI}(w,c) - \log k
$$
这个结果在一个关键假设下成立：负样本是根据它们在语料库中的频率来抽取的 [@problem_id:3200029]。突然之间，[神经网](@entry_id:276355)络的黑箱变得透明。该算法在从未明确构建巨大的[共现矩阵](@entry_id:635239)或计算 PMI 的情况下，学到了能够编码这种基本统计关系的向量。它对平移后的 PMI 矩阵执行了隐式的低秩分解。这统一了神经嵌入的新世界与基于计数矩阵和 SVD 的经典[分布](@entry_id:182848)语义学世界，表明它们是同一枚硬币的两面。[@problem_id:3182845] 中的编程练习可以让人实际观察到这一点，通过改变超参数 $k$ 和负[采样[分](@entry_id:269683)布](@entry_id:182848)，观察这个隐式矩阵的属性（特别是奇异值）如何变化。

### 完善技艺：应对混乱世界的实用智慧

核心原理是优美的，但真实世界的文本是混乱的。为了获得高质量的嵌入，还需要一些更巧妙的技巧。

#### 驯服常见词：二次采样启发式

像“the”、“a”和“in”这样的词非常频繁，但几乎不携带特定的语义权重。它们几乎与所有词一同出现，产生了大量无[信息量](@entry_id:272315)的训练样本。在“quick”和“fox”上训练是有用的；在“quick”和“the”上训练则用处不大。解决方法是在创建训练对之前应用**二次采样启发式** (subsampling heuristic) [@problem_id:3200047]。我们以一个随频率增加而增加的概率丢弃高频词的出现。一个频率为 $f(w)$ 的词 $w$ 被保留的概率是：
$$
P_{\text{keep}}(w) = \min\left(1, \sqrt{\frac{t}{f(w)}} + \frac{t}{f(w)}\right)
$$
其中 $t$ 是一个阈值。这个简单的公式有两个深远的影响。首先，通过剔除数百万无用的例子，它极大地加快了训练速度。其次，更重要的是，它提高了稀有、更有意义的词的嵌入质量。通过减少来自常见词的压倒性信号，我们让来自内容词的更微妙的信号得以显现。这导致高频词的嵌入范数较小，而稀有词的嵌入范数相对较大，从而更准确地反映它们的信息含量。

#### 明智地选择你的敌人：Unigram [分布](@entry_id:182848)

我们究竟应该如何选择我们的“敌人”——负样本？一个简单的方法是从词汇表中均匀地挑选。但这并不理想。这意味着我们挑选稀有词“axolotl”和常见词“cat”的概率是一样的。一个更好的策略是根据词的实际出现频率来采样。然而，原始[频率分布](@entry_id:176998)过于倾斜。单词“the”几乎总会被选为负样本。

word2vec 的创造者们找到了一个神奇的甜蜜点。他们建议从提升到 0.75 次幂的 unigram [分布](@entry_id:182848)中进行采样：
$$
P_{\text{neg}}(w) \propto f(w)^{0.75}
$$
这个“平滑”指数使[分布](@entry_id:182848)稍微变平。相对于原始[频率分布](@entry_id:176998)，它降低了挑选超高频词的概率，并增加了挑选稀有词的概率。这个简单的启发式方法 ternyata 有一个清晰的数学解释。它修改了 SGNS 正在隐式分解的矩阵，从而有效地改变了模型正在学习的统计量 [@problem_id:3200081]。

#### 不断扩展的上下文视野

“上下文”本身的概念是一个可调参数。**窗口大小**决定了我们从中心词向外看多远来寻找上下文词，它具有显著的影响。一个小窗口（例如，大小为 1 或 2）倾向于捕捉句法关系和功能相似性（例如，“dog”和“cat”都是可以作为宠物的名词）。一个更大的窗口（例如，大小为 5 或 10）则倾向于捕捉更广泛的主题或语义关系（例如，“doctor”和“fever”）。这种选择系统地改变了我们提供给模型的共现统计数据，因此，也改变了它所学习的“意义”的本质 [@problem_id:3200014]。

[负采样](@entry_id:634675)的框架不是一个僵化的教条，而是一块灵活的画布。我们可以为特定目标设计自定义的采样器。例如，我们可以使用另一个模型来提出“困难负例”——那些在语义上合理但在给定上下文中不正确的词，而不是基于频率进行采样，从而迫使我们的[主模](@entry_id:263463)型学习更精细的区别 [@problem_id:3156761]。或者我们可以混合不同的距离概念，如[嵌入空间](@entry_id:637157)中的语义相似性和句法相似性（即拼写），来创建一个既理解意义又理解形态学的采样器 [@problem_id:3156724]。正是这种适应性使[负采样](@entry_id:634675)在不断探索教机器理解人类语言细微差别的征程中，始终是一个至关重要且不断发展的组成部分。

