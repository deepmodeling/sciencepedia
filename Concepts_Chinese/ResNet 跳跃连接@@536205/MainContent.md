## 引言
在构建更强大人工智能的竞赛中，研究人员面临一个令人沮丧的悖论：让[神经网络](@article_id:305336)变得更深，通常会使其性能变得更差。这个由学习信号在多层传播中消失所引起的“退化问题”，为我们能够构建的模型的复杂性设置了硬性上限。我们如何才能建造[深度学习](@article_id:302462)的摩天大楼，而又不会让它们在自身重量下崩塌呢？

本文将探讨一个极其简单的解决方案，它解锁了前所未有的网络深度：**[ResNet](@article_id:638916) 跳跃连接**。我们将揭示这一架构上的神来之笔——仅仅是将输入与输出相加的简单行为——是如何从根本上改变了人工智能领域的格局。

我们的旅程将从第一章**原理与机制**开始，我们将在这里解构跳跃连接。我们将探讨它如何创建“梯度高速公路”来解决[梯度消失问题](@article_id:304528)，并重新定义了网络实际学习的内容。随后，在**应用与跨学科联系**中，我们将看到这一原理的实际应用，从构建最先进的[计算机视觉](@article_id:298749)模型，到其在[数值分析](@article_id:303075)乃至生物学等领域出人意料的体现。准备好去发现一个简单的加号是如何成为[现代机器学习](@article_id:641462)中最深刻的思想之一。

## 原理与机制

要真正领会[残差连接](@article_id:639040)的巧妙之处，我们必须首先回顾它旨在解决的问题。想象你是一位建筑师，你的任务是建造一座宏伟高耸的摩天大楼。逻辑上，建筑越高，其能力就越令人印象深刻。但如果你发现，超过某个高度——比如30层——后，增加更多的楼层反而会神秘地导致整个结构比20层的大楼*更不*稳定、功能更差呢？这正是[深度神经网络架构](@article_id:640922)师们面临的令人困惑的困境。当他们堆叠越来越多的层以期创造更强大的模型时，他们碰壁了。性能会达到饱和，然后矛盾地开始下降。这不仅仅是过拟合的问题；这些模型在根本上变得更难训练了。摩天大楼在自身重量下正在崩塌。

罪魁祸首是一种被称为**[梯度消失](@article_id:642027)**的现象。在训练过程中，网络通过将误差信号（即梯度）从最终输出层反向传播到输入层来进行学习。这个梯度是指导网络中每个参数应如何自我调整以提升性能的说明书。在一个非常深的网络中，这个信号必须穿过一长串的计算。在每一层，它都会乘以该层变换的局部梯度。如果这些乘法因子持续小于1，信号就会呈指数级缩小，就像一个沿着长队传递的耳语。当它到达早期层时，耳语已经消失得无影无踪。我们摩天大楼的最初几十层没有收到如何对齐的指令，导致整个结构失效。

### 梯度高速公路

如何解决这种[信号衰减](@article_id:326681)问题呢？在[残差网络](@article_id:641635)（[ResNet](@article_id:638916)s）中提出的解决方案，其简洁性令人惊叹。它没有强迫信号沿着一条漫长、曲折的乡间小路——即一系列变换——前进，而是建造了一条平行的、无障碍的高速公路。这就是**跳跃连接**。

在传统网络中，一个层的输出 $x_{k+1}$ 是其输入 $x_k$ 的某个变换 $g$，写作 $x_{k+1} = g(x_k)$。在[残差网络](@article_id:641635)中，我们将原始输入加回去：$x_{k+1} = x_k + g(x_k)$。$g(x_k)$ 这一项被称为**[残差块](@article_id:641387)**，而直接传递 $x_k$ 的连接就是跳跃连接。

为了理解为什么这种方法如此有效，让我们做一个思想实验。想象一个玩具网络，其中每一层的变换只是将输入乘以一个标量 $a$。在一个普通网络中，经过 $L$ 层后，输出与输入之间相差一个因子 $a^L$。当我们进行[反向传播](@article_id:302452)时，链式法则告诉我们梯度信号也乘以这个因子。如果 $|a|  1$（这很常见），对于大的 $L$，梯度 $|a|^L$ 会消失为零。现在，让我们引入跳跃连接：新的变换是 $x + ax = (1+a)x$。经过 $L$ 层后，梯度被缩放 $|1+a|^L$ 倍。即使 $a$ 很小（比如 $0.5$），指数的底数现在是 $1.5$，而不是 $0.5$。对于一个仅有 $L=20$ 层的网络，[残差网络](@article_id:641635)中的梯度可以比普通网络大惊人的 $3^{20}$ 倍，即近35亿倍！[@problem_id:3113800] 添加 $x$ 这个简单的行为创造了一条‘梯度高速公路’，使误差信号几乎可以原封不动地流回到最早的层。

我们可以将这个优美的思想形式化。流回某层输入 $x$ 的梯度由两部分组成：直接沿单位‘高速公路’下来的信号，以及通过变换块走‘旁路’的信号。对于一个块 $y = x + \alpha h(x)$，其中 $h(x)$ 是变换，$\alpha$ 是一个控制其强度的可学习参数，损失 $L$ 对输入 $x$ 的梯度是：
$$
\nabla_x L = (y-t) + \alpha J_h(x)^T (y-t)
$$
在这里，$(y-t)$ 是来自输出的[误差信号](@article_id:335291)，而 $J_h(x)$ 是变换的雅可比矩阵（[偏导数](@article_id:306700)矩阵）。这个方程优雅地剖析了梯度流。第一项 $(y-t)$ 是沿着单位路径传播的、原始未受影响的误差信号。第二项是经过变换 $h(x)$ 的信号，并受到网络自身学习到的参数 $\alpha$ 的调节。网络不仅拥有一条高速公路，它还能确切地学习应该将多少流量发送到这条风景优美的路线上。[@problem_id:3108047]

### 学习有待学习的部分

这种架构上的改变对网络*学习什么内容*产生了深远的影响。网络现在被要求学习一个远为简单的任务：学习**[残差](@article_id:348682)**，即对恒等映射的*修正*，而不是强迫每一层从头开始学习一个完整、复杂的变换。

想象你是一个照片编辑人工智能。你的任务是将一张模糊的照片（$x$）变成一张清晰的照片（$f(x)$）。一个普通的网络必须学习从模糊像素到清晰像素的整个、极其复杂的映射。而一个[残差网络](@article_id:641635)，则试图学习[残差](@article_id:348682)函数 $r(x) = f(x) - x$。也就是说，它学习预测清晰照片和模糊照片之间的*差异*——一个“锐化滤镜”。最终的清晰照片就是原始模糊照片加上学习到的锐化滤镜：$f(x) = x + r(x)$。这通常是一个容易得多的问题。如果输入已经相当好，[残差](@article_id:348682)可以非常小，网络只需要学习一个微小的修饰。

这不仅仅是一个类比。一个巧妙设计的实验可以证明这一原理的有效性。想象一个分类任务，其中数据点已经非常接近正确的决策边界。一个[恒等映射](@article_id:638487)（$y=x$）会表现得相当不错，但并非完美。一个[残差块](@article_id:641387) $y = x + F(x)$ 则可以完全专注于学习所需的小修正 $F(x)$，以将数据点推到边界的正确一侧，从而显著提高准确性。恒等部分完成了繁重的工作，而[残差块](@article_id:641387)提供了关键的最终调整。[@problem_id:3169949]

这种视角的转变在数学上是强大的。著名的**通用逼近定理**告诉我们，神经网络可以逼近任何[连续函数](@article_id:297812)。[残差](@article_id:348682)架构重新诠释了这个定理：让一个 [ResNet](@article_id:638916) 逼近[目标函数](@article_id:330966) $f(x)$ 的任务，在数学上等同于让一个*普通*网络逼近[残差](@article_id:348682)函数 $r(x) = f(x) - x$。[@problem_id:3194207] 如果我们想学习的函数已经接近恒等映射（这在深层网络中很常见，其中一层的输出与其输入相似），那么[残差](@article_id:348682)函数就接近于零，这对于一个以小[权重初始化](@article_id:641245)的网络来说是一个非常容易学习的函数。这一洞见也有助于解释为什么非常深但窄的 [ResNet](@article_id:638916) 能够如此有效，因为它们可以通过组合许多微小、简单的[残差](@article_id:348682)修正来表示复杂的函数。[@problem_gdid:3194207]

我们甚至可以在一个简化的线性环境中观察这个过程的展开。如果我们将一个[残差网络](@article_id:641635)的所有变换[权重初始化](@article_id:641245)为零，它的行为就像一个纯粹的[恒等函数](@article_id:312550)。随着训练的开始，每个块开始学习整体目标[残差](@article_id:348682)的一小部分，以一种稳定、逐步的方式，合作构建出最终正确的变换。[@problem_id:3169676]

### 统一视角：从常微分方程到[集成方法](@article_id:639884)

一个最初为解决梯度问题而设计的巧妙工程技巧，最终演变成某种更为深刻的东西。[残差网络](@article_id:641635)的结构 $x_{k+1} = x_k + F(x_k)$ 是多个科学和数学领域交汇的十字路口。

其中一个最美的联系是与**数值分析**领域的联系。[ResNet](@article_id:638916) 的更新规则在形式上与**前向欧拉法**（Forward Euler method）完全相同，后者是求解[常微分方程](@article_id:307440)（ODE）[数值解](@article_id:306259)的最简单方法之一。一个 ODE 描述了一个系统随时间的演化，$x'(t) = f(x(t))$。[前向欧拉法](@article_id:301680)用离散的步骤来近似这个连续的演化：$x(t+h) \approx x(t) + h f(x(t))$。这简直是完美的匹配！一个[残差网络](@article_id:641635)可以被解释为一个[连续变换](@article_id:305274)的离散化。网络的“深度”就是我们在“时间”上所走的步数。这种观点不仅仅是哲学上的好奇；它开启了一个充满新的架构可能性的宝库。如果一个 [ResNet](@article_id:638916) 是一个前向欧拉求解器，我们能否基于更强大、更稳定的 ODE 求解器，比如后向欧拉法（Backward Euler method），来构建网络？答案是肯定的，这导致了具有卓越稳定性的新型隐式定义网络。[@problem_id:3208219]

另一个强大的视角来自**[统计学习](@article_id:333177)**。如果我们展开一个深的 [ResNet](@article_id:638916)，最终的输出是输入加上所有[残差块](@article_id:641387)的总和：$x_L = x_0 + \sum_{l=0}^{L-1} F_l(x_l)$。这看起来像一个**集成**（ensemble），其中许多独立模型（$F_l$）的预测被组合在一起。在典型的训练条件下，这种结构的行为与一种著名的[集成方法](@article_id:639884)——**[梯度提升](@article_id:641131)**（Gradient Boosting）——惊人地相似。在提升法（boosting）中，会训练一系列“[弱学习器](@article_id:638920)”，每个新的学习器都专注于修正前一个学习器留下的剩余误差（伪[残差](@article_id:348682)）。在 [ResNet](@article_id:638916) 中，每个[残差块](@article_id:641387) $F_l$ 被训练来产生一个输出，当加到整体上时，能将最终的预测推向更接近目标的方向。它学会修正当前表示的“误差”。因此，一个深的 [ResNet](@article_id:638916) 不是一个单一、庞大的实体；它是许多较弱模型的一个隐式的、深度交织的集成，这有助于解释其强大的性能。[@problem_id:3169973]

因此，跳跃连接的优雅之处在于其纯粹性。其他架构，如 Highway Networks，提出了更复杂的门控连接，例如 $y = T(x) \odot F(x) + (1-T(x)) \odot x$，其中一个可学习的门控 $T(x)$ 决定如何混合变换和恒等映射。虽然这提供了更大的灵活性，但也引入了一个风险：网络可能会学会在恒等路径上“关闭门控”（通过设置 $T(x) \approx 1$），从而有效地关闭梯度高速公路，重新陷入[梯度消失](@article_id:642027)的陷阱。[ResNet](@article_id:638916) 简单、[非门](@article_id:348662)控、加法式的跳跃连接证明了简约的力量。它确保了高速公路始终开放，为信息在任何深度下的前向和后向传播提供了一个鲁棒的骨干。[@problem_id:3170021]

