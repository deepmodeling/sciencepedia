## 应用与跨学科联系

在之前的讨论中，我们为一个革命性的思想奠定了基础：一个通常被认为是“黑箱”的[神经网络](@article_id:305336)，可以被转化为物理定律的透明容器。我们不仅仅是在训练这些网络模仿数据，我们是在教它们游戏规则，即宇宙的语法。现在，让我们踏上一段旅程，看看这个强大的新哲学将我们带向何方。我们会发现，它开辟了全新的科学研究方式，以曾经只存在于科幻小说中的方式，连接了不同学科，并沟通了理论、模拟与实验。

我们将反复看到一个核心主题，这个主题深刻而简单：让机器学习我们*不知道*的，同时尊重我们*知道*的。想象一下，你对一个复杂系统有一个相当不错但不完美的物理模型——也许是对一个分子的密度泛函理论（DFT）计算。它捕捉了大部分物理，但并不完美。我们不必让神经网络从零开始学习全部极其复杂的物理过程，而是可以要求它做一些更微妙、更聪明的事情：学习*修正量*。我们将真实的高精度能量 $E^{\mathrm{CC}}$ 建模为我们简单模型的能量 $E^{\mathrm{DFT}}$ 加上一个学习到的[残差](@article_id:348682) $\Delta_{\theta}$。网络的工作就是学习这个 $\Delta_{\theta}$。因为这个[残差](@article_id:348682)通常比总能量是一个“更简单”或“更平滑”的函数，所以网络可以用少得多的数据和更高的稳定性来学习它。这个被称为 $\Delta$-学习的策略是现代[科学机器学习](@article_id:305979)的基石，因为它将网络的力量集中在问题的困难部分，同时利用了数代人类科学知识的成果 ([@problem_id:2903824])。

### 新型显微镜：学习不可见之物的动力学

科学中最基本的任务之一就是观察事物的变化并推断出支配其运动的规律。几个世纪以来，这意味着靠手工提出一个方程。但如果系统如此复杂，以至于我们甚至无法猜测其控制定律的形式呢？

想象一位生物学家正在观察细胞中两种相互作用的蛋白质浓度。数据显示了一场复杂的舞蹈，但其分子编排——激活和抑制的精确规则——却是一个谜。在这里，神经[微分方程](@article_id:327891)（Neural Ordinary Differential Equation, Neural ODE）就像一种革命性的新型显微镜。我们不需要预设相互作用的数学形式。我们只需陈述系统状态的变化率是当前状态的某个函数，然后我们让一个神经网络*成为*那个未知函数。通过在观测到的时间序列数据上进行训练，网络直接学习了动力学的[向量场](@article_id:322515)，仅从其行为中就发现了系统演化的隐藏规则 ([@problem_id:1453811])。

当我们想要创建一个更通用的模型时，这个想法变得更加强大。假设我们正在模拟一个癌细胞系对一系列不同连续输注速率的药物治疗的反应，而不仅仅是一种。我们可以构建一个统一的神经[微分方程](@article_id:327891)，学习以药物动力学为条件的细胞反应。我们通过一个非常优雅的技巧——[状态增广](@article_id:301312)（state augmentation）来实现这一点。我们只需将药物浓度及其控制参数（如输注速率）添加到定义系统状态的变量列表中。已知的物理学——药物的[药代动力学](@article_id:296934)——被硬编码到常微分方程中，而[神经网络](@article_id:305336)则学习未知的生物反应。结果是一个单一的模型，可以预测整个系列的各种实验条件下的结果，成为该特定系统的一个统一理论 ([@problem_id:1453803])。

这种发现未知函数的能力将我们引向了最强大的应用之一：求解反问题。在典型的“正”问题中，我们知道原因（例如，[质量分布](@article_id:318855)），然后计算结果（[引力场](@article_id:348648)）。但在科学研究中，我们常常面临相反的情况：我们可以测量结果，但原因却是隐藏的。想象一下通过测量地表地球[引力场](@article_id:348648)的细微变化来绘制地下水源的分布图。我们看到了一个物理方程的解，但方程本身的一部分——[源项](@article_id:332813)——才是我们希望找到的。

[物理信息神经网络](@article_id:305653)（PINNs）为此提供了一个绝佳的框架。我们建立两个神经网络：一个用来表示未知的解场，另一个用来表示未知的源项。然后我们同时训练它们，由一个包含两部分的[损失函数](@article_id:638865)引导。一部分迫使解在数据点处与我们的实验测量值相匹配。另一部分则迫使网络在所有其他地方都遵守控制物理定律（如泊松方程 $\nabla^2 u = f$）。网络同时发现了未知的源 $f(x)$ 和与已知物理及稀疏数据都一致的[全局解](@article_id:360384) $u(x,y)$。这同一个原理可以应用于从物理学到[环境科学](@article_id:367136)的各个学科，例如，我们可以通过测量净二氧化[碳通量](@article_id:373068)并强制执行[生物地球化学](@article_id:312603)[质量平衡](@article_id:361086)定律，来推断森林中隐藏的光合作用速率（总[初级生产力](@article_id:311694)，GPP）([@problem_id:2126332] [@problem_id:1861479])。

### 数字炼金术士的熔炉：从原子层面模拟物质

几十年来，[材料科学](@article_id:312640)的圣杯一直是在计算机上设计出具有所需性质的新材料，然后再踏入实验室。这需要模拟原子和分子的相互作用，而这受制于极其复杂的量子力学定律。虽然我们可以为小系统求解这些方程，但对于模拟真实材料所需的成千上万甚至数百万个原子来说，[计算成本](@article_id:308397)是天文数字。正是在这里，神经网络正在引发一场名副其实的革命。

核心思想是创建一个[神经网络势能面](@article_id:369075)（Neural Network Potential Energy Surface, NNPES）。我们进行有限次数的高度精确、昂贵的[量子计算](@article_id:303150)，以生成一个关于各种原子[排列](@article_id:296886)下能量和力的行为的参考数据集。然后，我们训练一个神经网络来学习原子位置和能量之间这种复杂的关系。一旦训练完成，网络预测这些能量和力的速度可以比原始的量子方法快数百万倍，从而使得以前无法实现的大规模分子动力学模拟成为可能。

但是我们如何确保网络学到正确的东西呢？材料的性质，如其刚度或其变形方式（其[弹性常数](@article_id:306627)），不仅取决于能量，还取决于能量的[导数](@article_id:318324)。力是能量对位置的一阶[导数](@article_id:318324)。控制材料弹性响应的[应力张量](@article_id:309392)与能量对应变（模拟盒的形变）的[导数](@article_id:318324)有关。要为晶体固体构建一个高保真度的 NNPES，仅仅拟合能量是不够的。一个稳健的训练过程必须包含关于力*和*应力的数据，并且[损失函数](@article_id:638865)中的每一项都要经过仔细的[归一化](@article_id:310343)和加权。通过直接在小应变下的构型上对[应力张量](@article_id:309392)进行训练，我们明确地教给网络关于材料的弹性响应，确保它学习到一个能够准确预测这些至关重要的力学性质的[势能面](@article_id:307856) ([@problem_id:2908447])。

### 超越模拟：理论、计算与实验的共生关系

将物理原理融入[神经网络](@article_id:305336)不仅仅是创造了更快的模拟器；它正在理论、计算和真实世界实验之间建立一种新的共生关系。

在光谱的一端，这些方法加速了我们最先进的计算工具。[离散化](@article_id:305437)[偏微分方程](@article_id:301773)产生的大规模线性代数问题是[计算物理学](@article_id:306469)的主力，其速度通常取决于是否有一个好的“预条件子（preconditioner）”——一个为迭代求解器简化问题的辅助算子。在传统[数值方法](@article_id:300571)与机器学习的惊人结合中，我们现在可以训练一个[神经网络](@article_id:305336)来充当一个高度专业化的[预条件子](@article_id:297988)。网络学习特定类别物理问题的共同结构，并生成一个定制的预条件子，从而显著加速求解过程。至关重要的是，这并非一个黑箱替代品；网络被训练来生成一个算子，该算子尊重其所辅助的经典[算法](@article_id:331821)的严格数学要求（例如，作为一个[对称正定矩阵](@article_id:297167)）([@problem_id:2382409])。

这种[嵌入](@article_id:311541)物理结构的哲学同样完美地适用于[多尺度建模](@article_id:315375)。要预测复合材料的宏观行为，必须理解其复杂的内部[微观结构](@article_id:309020)。我们可以训练一个神经网络作为这种微观尺度响应的快速而准确的“[代理模型](@article_id:305860)（surrogate）”。关键在于将力学的基本原理直接构建到网络的架构中。通过设计网络输出一个应变能的[标量势](@article_id:339870)，我们通过[自动微分](@article_id:304940)的数学原理，自动保证了最终的[应力-应变关系](@article_id:337788)是物理上一致的（[超弹性](@article_id:319760)），并且材料的[刚度张量](@article_id:355554)具有所有正确的对称性。这是一个深刻的例子，说明如何将物理定律用作*[归纳偏置](@article_id:297870)*——一个约束模型学习物理上有意义的解的指导原则 ([@problem_id:2904240])。

也许最激动人心的前沿是闭合模型与实时实验之间的循环。想象一下观察[催化剂](@article_id:298981)表面上发生的[化学反应](@article_id:307389)。我们可以测量材料不断变化的[X射线吸收](@article_id:363653)谱，这是存在的不同化学物种信号的复杂混合。我们如何理清这些数据以找到底层的[反应速率](@article_id:303093)？一个 PINN 恰好能做到这一点。我们构建一个模型，将反应动力学的已知[常微分方程](@article_id:307440)与一个描述物种[表面覆盖度](@article_id:380916)如何产生我们测量的总光谱的“测量模型”结合起来。整个系统——近似[物种浓度](@article_id:375861)的[神经网络](@article_id:305336)和未知的速率常数本身——被训练来使预测的光谱与实验数据相匹配，同时遵守化学动力学定律。这使我们能够以一种有原则的方式，直接从实时的、复杂的实验数据中推断出隐藏的物理参数 ([@problem_id:77144])。

### 圣杯：预测未知

我们终于来到了任何科学理论的终极考验：其[外推](@article_id:354951)能力。它能预测从未见过的事物吗？大多数标准的机器学习模型都是出色的[内插](@article_id:339740)器，但当被要求在其训练数据域之外进行预测时，它们往往会惨败。这正是物理信息模型展现其真正力量的地方。

考虑一下预测[相变](@article_id:297531)的挑战。我们有一个物理系统的[时间序列数据](@article_id:326643)，比如一个序参量（如磁化强度），这些数据是在一个[临界点](@article_id:305080)以下的温度 $T  T_c$ 下获得的。我们想要预测当系统被加热到[临界点](@article_id:305080)*以上*，进入一个我们从未向模型展示过的新物相时，会发生什么。一个标准的、在这些数据上训练的黑箱[时间序列预测](@article_id:302744)器会学习到低温相的行为，并且在[相变](@article_id:297531)点会完全失效。它学会了数据，但没有学会定律。

现在，考虑一种不同的方法。我们借鉴我们的理论知识——在这里是[朗道相变理论](@article_id:315756)——并构建一个神经[微分方程](@article_id:327891)，其架构本身就体现了已知的物理学。我们将动力学约束为一个[势能函数](@article_id:345549)的梯度流。我们内置系统的已知对称性（例如，能量必须是磁化强度的[偶函数](@article_id:343017)）。模型的任务不再是学习一个任意的函数，而是学习朗道势中的少数几个系数如何随温度变化。通过在 $T  T_c$ 时观察系统，模型可以学习到这种函数依赖关系并进行*外推*。它可以正确预测一个系数将在 $T_c$ 时改变符号，导致动力学的本质发生变化——能量景观将从双阱势翻转为单阱势。该模型之所以成功，是因为它不仅仅是在拟合一条曲线，而是在学习底层的物理定律。这就是圣杯：一个能够进行真正科学发现的机器学习模型，通过学习和泛化物理学的基本原理来预测新现象 ([@problem_id:2410517])。

这就是将物理学与机器学习相结合的前景。这是一段从模仿到理解、从[内插](@article_id:339740)到预测、从作为数据的学生到成为我们世界法则发现者的旅程。