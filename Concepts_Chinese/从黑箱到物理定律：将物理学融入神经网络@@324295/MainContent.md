## 引言
数十年来，[神经网络](@article_id:305336)一直被视为功能强大但又神秘莫测的“黑箱”，能够从数据中学习复杂的模式，但缺乏传统科学模型的严谨性和[可解释性](@article_id:642051)。然而，一场革命性的[范式](@article_id:329204)转变正在发生，它将机器学习的数据驱动灵活性与物理定律的原则性基础融为一体。这种融合有望将[神经网络](@article_id:305336)从单纯的[模式识别](@article_id:300461)器转变为用于科学探究和发现的精密工具。该方法所要解决的核心问题是，标准模型未能遵循自然界的基本定律，这限制了它们在训练数据之外进行泛化和预测现象的能力。

本文描绘了这一激动人心的旅程。我们将在 **“原理与机制”** 一章中首先深入探讨基本思想，探索[神经网络](@article_id:305336)与[统计力](@article_id:373880)学之间深刻而悠久的历史渊源。我们将审视将物理知识灌输给模型的两种主要哲学：是教会模型规则，还是构建一个物理上无法打破规则的网络。随后，**“应用与跨学科联系”** 一章将展示这些基于物理原理的网络如何在科学和工程领域得到应用，从原子尺度上模拟新材料到发现生物系统的隐藏规则。要真正领会这场革命，我们必须首先揭开这些“黑箱”，发现其中已然涌动的惊人物理学。

## 原理与机制

[神经网络](@article_id:305336)通常被视为复杂的“黑箱”，通过调整数百万个参数从数据中学习。然而，为了将其可靠地应用于科学领域，有必要深入理解其内部工作机制。当我们审视其底层结构时，会发现一个惊人的事实：其核心原理与物理学中早已成熟的概念有着深刻的联系。

### 机器之魂：作为蓝图的物理学

让我们暂时回到过去，回到“深度学习”还不是一个家喻户晓的词汇的时代。在20世纪80年代初，一位名叫 John Hopfield 的物理学家观察了一个由简单的类[神经元](@article_id:324093)单元组成的网络，并看到了它与物理学家们非常熟悉的另一个系统的惊人相似之处：微小磁体的集合，就像[伊辛模型](@article_id:299514)（Ising model）一样 [@problem_id:2425734]。

想象一下一组微观磁体，每个磁体可以指向向上 ($+1$) 或向下 ($-1$)。每个磁体都感受到其邻居的影响。有些磁体对“倾向于”指向同一方向，而另一些则倾向于相反。我们可以为整个系统写下一个总“能量”，其中满足这些偏好的[排列](@article_id:296886)具有较低的能量。如果任其自然演化，特别是在低温下，整个系统会四处[振荡](@article_id:331484)，并最终稳定在一个最小能量状态——一个稳定的构型。

Hopfield 意识到，一个[神经元](@article_id:324093)网络可以用完全相同的方式来描述。一个[神经元](@article_id:324093)要么处于激活状态 ($+1$)，要么处于非激活状态 ($-1$)。[神经元](@article_id:324093)之间的“突触权重”就像磁体之间的相互作用强度。我们可以为整个网络写下一个“能量函数”。当我们向网络呈现一个输入时，它的状态开始演化，每个[神经元](@article_id:324093)根据其邻居的输入更新自己的状态。这个网络，就像磁体一样，将稳定在一个“[吸引子](@article_id:338770)状态”——其能量景观中的一个最小值 [@problem_id:1437735]。

这有什么意义呢？这些稳定状态可以被解释为记忆！你可以通过设置权重来“编程”网络，使得特定的模式（如一张脸的图像）成为低能状态。然后，如果你向网络展示那张脸的带噪声或不完整的版本，它会自然地演化到“正确”的、完整的记忆，就像一个物理系统滚下[山坡](@article_id:379674)到达谷底一样。

这种深刻的联系，将感知机的[权重和偏置](@article_id:639384)与[伊辛模型](@article_id:299514)的耦合及外场进行了形式上的映射，向我们表明其核心思想并非黑魔法，而是[统计力](@article_id:373880)学 [@problem_id:2425734]。简单感知机的确定性硬阈值就像零温度下的伊辛模型，系统会选择唯一的最低能量状态。而一个更“柔和”的、给出概率的决策，则像是处于有限温度下的同一系统，此时系统有一定几率处于能量稍高的状态——这一概念被从玻尔兹曼统计中自然产生的 logistic sigmoid 函数完美地捕捉了 [@problem_id:2425734]。这不仅仅是一个类比，这是一个深刻的数学等同。它告诉我们，从一开始，神经网络的逻辑就与物理学的逻辑交织在一起。

### 教机器物理学：两种哲学

好吧，这是一个美丽的历史联系。但我们如何利用这些网络来解决*新*的物理问题呢？我们如何让它们理解，比如说，行星的运动或流体的流动？事实证明，主要有两种思想流派，两种将物理知识灌输给机器的哲学。

#### 哲学一：勤奋的学生

第一种方法是把[神经网络](@article_id:305336)当作一个勤奋的学生。我们给它一组数据——一个系统行为的测量值。但我们不只是让它记住答案。我们还给了它教科书：以[微分方程](@article_id:327891)形式表达的、支配该系统的物理定律。

这便是**[物理信息神经网络](@article_id:305653)（Physics-Informed Neural Networks, PINNs）**背后的核心思想。我们设计一个损失函数——我们给网络的“分数”——它包含两个部分。第一部分，$\mathcal{L}_{data}$，衡量网络预测与我们已有的实验数据的匹配程度。第二部分，$\mathcal{L}_{physics}$，则更为巧妙。它衡量网络的预测在多大程度上遵守了物理定律。我们让网络生成一个函数，比如说，振子随时间变化的位置 $x(t)$。然后，利用一个叫做**[自动微分](@article_id:304940)**的奇妙工具，我们可以毫不费力地计算出网络输出的[导数](@article_id:318324) $\dot{x}(t)$ 和 $\ddot{x}(t)$，并将它们直接代入控制方程。例如，对于一个[阻尼谐振子](@article_id:340538)，其定律是 $\ddot{x}(t) + p_1 \dot{x}(t) + p_2 x(t) = 0$。物理损失就是这个表达式的平方，我们称之为**[残差](@article_id:348682)（residual）**。如果网络完美地遵守了该定律，[残差](@article_id:348682)就为零 [@problem_id:1595359]。

总损失是一个加权和：$\mathcal{L} = \mathcal{L}_{data} + \lambda \mathcal{L}_{physics}$。通过最小化这个组合损失，网络被迫去寻找一个既能拟合我们的观测数据，又能处处（甚至在我们没有数据的地方）都尊重潜在物理原理的解！它不仅在学习发生了*什么*，还在根据规则学习*为什么*会发生。同样的原理可以从简单的常微分方程（ODE）漂亮地扩展到复杂的[偏微分方程](@article_id:301773)（PDE），例如描述反应[扩散过程](@article_id:349878)的方程 [@problem_id:29925]。网络必须同时满足数据*和*方程。这是将物理学家的理论知识与真实世界测量相结合的强大方式。

#### 哲学二：天生的运动员

第二种哲学在许多方面更为深刻。我们不是教网络规则，而是构建一个物理上无法打破规则的网络。我们将物理学直接构建到它的架构中。这是一个作为“天生运动员”的网络——它不需要被教导如何优雅地奔跑；它生来就善于此道。在机器学习中，这个概念被称为**[归纳偏置](@article_id:297870)（inductive bias）**。

想想自然界的[基本对称性](@article_id:321660)。如果我们将实验移到另一个实验室（[平移不变性](@article_id:374761)），或者从另一个角度观察它（[旋转不变性](@article_id:298095)），物理定律不会改变。对物理学家来说，这不是一个建议，而是一个基石原理。那么，为什么我们的神经网络必须从数据中*学习*这一点呢？这会非常低效，而且我们永远无法确定它是否完美地学会了。

更好的方法是设计一个能自动尊重这些对称性的架构。这引出了一个至关重要的区别。

### 用正确的砖块构建：[对称性与守恒](@article_id:315270)定律

如果我们要将物理学构建到网络中，我们需要精确地定义我们的意思。这要求我们暂时像数学家一样思考，使用群和对称性的语言。

#### 对称性的语言：不变性与[等变性](@article_id:640964)

假设我们的网络预测一个分子的总能量。如果我们在空间中旋转这个分子，能量不应该改变。能量在旋转下是**不变的（invariant）**。那么，作用在每个原子上的力呢？如果我们旋转分子，力矢量应该随之旋转。它们不是保持不变，而是以一种可预测的方式变换。力在旋转下是**等变的（equivariant）**。

这个区别至关重要 [@problem_id:2784668]。如果一个函数 $f$ 将一个来自群 $G$（如旋转群）的变换 $g$ 应用于其输入 $X$ 时，其输出会根据相应的表示 $D(g)$ 进[行变换](@article_id:310184)：$f(g \cdot X) = D(g) f(X)$，那么这个函数就是 $G$-等变的。不变性只是 $D(g)$ 为[恒等变换](@article_id:328378)——输出没有任何变化的特殊情况。

现代用于物理学和化学的[神经网络](@article_id:305336)越来越多地将[等变性](@article_id:640964)作为其核心。像 Behler-Parrinello 网络这样的架构通过首先使用“[对称函数](@article_id:356066)”来描述原子的局域环境来实现这一点——这些手工制作的数学描述符在构造上就对邻近原子的旋转和[置换](@article_id:296886)具有[不变性](@article_id:300612) [@problem_id:2648619]。更新的基于图的架构，如[消息传递](@article_id:340415)[神经网络](@article_id:305336)，在以尊重系统几何和对称性的方式传播信息的同时，自动学习这些表示。这种内置的对称性是一种强大的[归纳偏置](@article_id:297870)；它极大地减少了所需的数据量，因为网络不用浪费时间去学习我们已知为宇宙真理的东西。

#### 不可违背的定律：能够守恒的架构

对称性是一个强大的指导，但我们可以更进一步。一些物理定律不仅关乎对称性，还关乎守恒。能量是守恒的。动量是守恒的。我们能否构建一个*被迫*遵守这些定律的网络？

答案是肯定的，而且这是物理学与机器学习最优雅的融合之一。

考虑一下哈密顿力学的美妙框架。一个系统的状态由其位置 $q$ 和动量 $p$ 描述。总能量，即哈密顿量 $H(q,p)$，决定了一切。时间演化由[哈密顿方程](@article_id:316621)给出：$\dot{q} = \partial H / \partial p$ 和 $\dot{p} = -\partial H / \partial q$。这个结构的一个神奇结果是能量 $H$ 会自动守恒。

那么，如果我们设计一个**[哈密顿神经网络](@article_id:301139)（Hamiltonian Neural Network, HNN）**呢？我们不让网络直接学习时间[导数](@article_id:318324) $\dot{q}$ 和 $\dot{p}$，而是让它学习一个单一的标量函数：哈密顿量 $H_{\theta}(q,p)$。然后，我们通过哈密顿方程，利用这个网络的输出来定义动力学。凭借其本身的结构——方程的“辛”性质——这个模型在数学上保证了它所学习到的能量 $H_{\theta}$ 是精确守恒的 [@problem_id:2410539]。它不是[损失函数](@article_id:638865)中的一个惩罚项，也不是网络试图去做的事情，而是架构本身不可违背的法则。

这个原理可以扩展到其他守恒定律。我们可以构建一个模型，其中力被定义为一个学习到的势能函数的梯度，$\mathbf{F} = -\nabla V_{\theta}(q)$。这是一个**保守**[力场](@article_id:307740)，总能量 $E = T + V_{\theta}$ 将会守恒。或者，对于一个相互作用的粒子系统，我们可以设计一个网络来学习成对力 $F_{ij}$，但我们施加条件 $F_{ij} = -F_{ji}$（牛顿第三定律）。如果我们然后对[总动量](@article_id:352180)的变化率求和，所有这些[内力](@article_id:346879)都会成对抵消，[总动量](@article_id:352180)将完美守恒 [@problem_id:2410539]。

这是一种[范式](@article_id:329204)转变。我们正在从统计模仿转向结构体现。网络不仅仅是在模仿一个物理系统，它*本身就是*一个物理系统，遵守着同样深刻的结构定律。

### 魔鬼在细节中：物理学家的实践考量

当然，宇宙是微妙的，即使有了这些宏大的哲学，我们也必须小心。在实践中应用这些思想需要物理学家对细节的关注。

#### 扭结的诅咒：为何平滑性至关重要

让我们回到试图求解微分方程的物理信息神经网络（PINN）。物理学的控制方程通常涉及二阶[导数](@article_id:318324)——想想 $F=ma$ 中的加速度，或者众多[场论](@article_id:315652)中的[拉普拉斯算子](@article_id:334415) $\nabla^2$。要让一个网络预测具有良好定义的二阶[导数](@article_id:318324)，网络本身必须是一个“平滑”函数。

这对我们选择神经网络最基本的构件之一——**[激活函数](@article_id:302225)**——产生了令人惊讶的后果。一个流行的选择是整流线性单元（Rectified Linear Unit, ReLU），它就是 $f(x) = \max(0,x)$。它计算成本低廉，在许多任务中效果奇佳。但对物理学家来说，它隐藏着一个致命的陷阱。一个由 ReLU 构成的网络是一个连续的、分段*线性*函数。它的一阶[导数](@article_id:318324)是一系列[阶跃函数](@article_id:362824)，而它的二阶[导数](@article_id:318324)[几乎处处](@article_id:307050)为零，只在“扭结”处存在间断点或无穷大。

如果你试图将 ReLU 网络用于弹性力学问题，而这涉及位移的二阶[导数](@article_id:318324)，网络几乎处处都会报告二阶[导数](@article_id:318324)为零 [@problem_id:2668888]。它无法表示曲率！这就像试图用直的乐高积木搭建一个完美的圆。同样，如果你想计算分子的[振动频率](@article_id:330258)，你需要 Hessian 矩阵——势能的二阶[导数](@article_id:318324)矩阵。一个基于 ReLU 的势能会给出一个不明确且无用的 Hessian 矩阵 [@problem_id:2908452]。

解决方案是什么？我们必须明智地选择我们的构建模块。我们需要平滑的激活函数，如[双曲正切函数](@article_id:638603)（$\tanh$）或[高斯误差线性单元](@article_id:642324)（[GELU](@article_id:642324)），它们是无限可微的。这确保了我们需要的物理量——如应力、曲率或[振动](@article_id:331484)模式——是良好定义的。这是一个绝佳的例子，说明物理定律的具体性质如何决定了机器学习设计中最基本的选择。

#### 远与近：跨越尺度

当我们模拟具有跨尺度相互作用的系统时，另一个深刻的挑战出现了。想象一下溶液中的两个离子。在短距离上，它们的相互作用是一个复杂的量子力学问题，难以描述。但在长距离上，它简化为优雅而熟悉的库仑定律，能量以 $1/r$ 的形式衰减。

大多数用于[分子模拟](@article_id:362031)的[神经网络架构](@article_id:641816)本质上是**局域的**；它们在一个小的[截断半径](@article_id:297161)（比如 5 埃）内，根据原子的直接邻域来确定其能量贡献 [@problem_id:2648619]。这在计算上是高效的，但这意味着网络完全看不到超出该截断范围的任何东西。它永远无法学习一个 $1/r$ 的力，因为它甚至看不到远处的物体！

强迫一个局域网络去学习长程物理学是徒劳的。一个漂亮的解决方案是采用混合方法。我们不要求机器去做我们已经能做得更好的事情。我们让[神经网络](@article_id:305336)做它*最擅长*的事情：模拟复杂的、混乱的、[短程相互作用](@article_id:306102)。而对于长程部分，我们只需从第一性原理*加上*物理上正确的解析项，如[库仑定律](@article_id:299808)或[范德华相互作用](@article_id:347680) [@problem_id:2796824]。我们使用一个平滑的阻尼函数在短程的学习势和长程的解析势之间进行过渡，以避免重复计算。

这不是机器学习的失败。这是它最大的胜利：不是作为物理理论的替代品，而是作为一个强大、灵活的组件，被集成到一个更大、更完整的物理模型中。它是为正确的工作使用正确工具的终[极体](@article_id:337878)现，是数据驱动的发现与原则性物理定律的综合。从在网络中看到物理学，到用物理学构建网络，这一旅程引领我们走向了一种全新的、更强大的科学研究方式。