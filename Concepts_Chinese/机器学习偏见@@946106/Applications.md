## 应用与跨学科联系

我们常常求助于计算机以获得客观性。毕竟，机器没有感情或偏见。我们给它输入数据，它寻找模式，然后做出决定。还有什么比这更公平的呢？这个简单而美好的想法引发了一场革命，我们现在期望算法能帮助我们做出那些困扰了人[类数](@entry_id:156164)千年的选择：谁能获得贷款？谁能得到救命的器官？谁最有可能面临即将来临的危险？人们最大的希望是，机器的冰冷逻辑能将我们从人类判断的易错性和偏见中解放出来。

但正如科学中所有美好的想法一样，现实远比这更有趣、更复杂。当我们用数学的显微镜审视这些算法时，我们发现一个奇怪而令人不安的真相：我们的机器，以其自身沉默而合乎逻辑的方式，也可能带有偏见。这并非因为它们有不可告人的动机，而是因为它们是忠实的学习者。它们从我们给定的数据中学习，而这些数据是我们世界的一面镜子——一个充满了历史不平等、社会分层和系统性差异的世界。

这段深入[算法偏见](@entry_id:637996)的旅程不仅仅是调试代码的技术练习。它是一次探索，将统计学与社会正义、计算机科学与宪法、机器学习与医学伦理的本质联系起来。它揭示了，要构建一台真正“公平”的机器，我们必须首先就“公平”的*含义*进行深入的对话——这个问题将我们带到远超二进制的世界。

### 法律与金融领域的偏见：[显性与隐性](@entry_id:272032)

也许思考公平性最直接的方式来自我们的法律传统。当我们指责一个系统不公平时，我们通常指两种情况之一。一种是*差别对待*（disparate treatment），这是直截了当的恶行：因为某人的种族、性别或其他受保护身份而明确地区别对待。如果一个算法有一行代码写着：“如果申请人来自A组，则降低分数”，那就是差别对待。

但还有一种更微妙、更普遍的偏见形式，称为*差别影响*（disparate impact）。当一个看似中立的规则——它从未提及任何受保护群体——却对该群体产生了不成比例的负面影响，并且这种影响无法用某种合法的必要性来证明时，就发生了这种情况。例如，一家医院可能使用一种分诊工具，该工具排除了种族这一特征，但包含了患者的邮政编码来估计其社会需求。如果由于历史上的种族隔离，邮政编码与种族高度相关，那么这个表面中立的算法可能会系统性地给予来自特定种族群体的患者较低的优先级，即使他们的医疗需求与其他人相同 [@problem_id:4489362]。该算法在*意图*上并非“种族主义”，但其*影响*是歧视性的。

这引出了一个关键问题：算法与它们旨在协助或替代的人类相比，其偏见是天生更多还是更少？有趣的答案是：视情况而定，但至少我们可以测量它！想象我们有一位人类信贷员和一个机器学习模型，两者都在判断谁可能拖欠贷款。我们可以观察他们一年，并计算他们的错误。我们对两种错误感兴趣。“假阳性率”($FPR$) 是指那些*本不会*违约但被拒绝贷款的人的比例。“假阴性率”($FNR$) 是指那些*会*违约但被批准贷款的人的比例。

通过比较人类与机器在不同人口群体之间这些错误率的差异，我们可以构建一个“偏见指数”。我们可能会发现，人类在某一方面偏见更大，而机器在另一方面偏见更大。机器并非自动更公平；它只是反映了其训练所用的历史贷款数据中的偏见。但与人类头脑中那些不透明且往往未经审视的偏见不同，机器的偏见可以被审计、量化，并可能被纠正 [@problem_id:2438791]。这是研究算法公平性的第一个伟大前景：它为我们提供了使偏见可见化和可量化的工具。

### 医疗偏见的高风险：当代码可能造成伤害时

在任何领域，这场讨论的风险都没有医学领域高，因为在这里，决策可能关乎生死。

#### 你的皮肤

考虑诊断皮肤癌的挑战。皮肤科医生的眼睛经过训练，能够看到颜色和纹理的细微变化。一个在数十万张图像上训练的人工智能也可以学会这样做。但如果训练图像绝大多数来自浅肤色个体呢？当这个人工智能随后用于肤色较深的患者时，它可能会灾难性地失败。在浅色皮肤上如此明显的恶性黑色素瘤的红色调，在深色皮肤上表现不同。如果算法从未被教过如何识别这一点，它可能会将致命的癌症视为“常规”病例而忽略 [@problem_id:4507443]。

这个失败不仅仅是一个孤立的错误。它是一个具有多种、复合性偏见来源的系统性问题。存在*数据不平衡*（或代表性偏见），即[训练集](@entry_id:636396)中肤色较深的图像太少这个简单事实。但也存在*测量偏见*。也许肤色较深者的照片是在较差的光线下拍摄的，或者人类专家提供的标签不那么确定，导致该群体的训练数据中[标签噪声](@entry_id:636605)更高。最后，还有*部署偏见*：该算法可能被用于一个社区的移动诊所，而该社区的皮肤病患病率与收集训练数据的医院不同。所有这些因素都确保了模型对整个一个人群的表现更差 [@problem_id:4440162]。

要真正理解这个问题的深度，我们必须超越统计学，审视物理学。想想你手腕上的智能手表，它使用一道绿光照射你的皮肤来测量心率。这项称为光电容积描记法（PPG）的技术，通过测量血液脉冲流经毛细血管时反射光的微小变化来工作。问题在于，赋予皮肤颜色的色素——黑色素，非常善于吸收绿光。在肤色较深的人身上，更多的绿光被黑色素吸收，反射回传感器的光就更少。这导致信号更弱、噪声更大。一个主要在浅肤色个体数据上训练的心率算法，可能难以在这种噪声中找到信号，导致对深肤色用户的读数不那么准确。

这不是一个单靠巧妙的软件补丁就能解决的问题。偏见源于硬件及其与人体生物学的相互作用，受比尔-朗伯吸收定律 $I = I_{0}\exp\{-\mu_{a}(\lambda,G)L\}$ 的支配。[吸收系数](@entry_id:156541) $\mu_a$ 同时取决于光的波长 $\lambda$ 和特定群体的皮肤特性 $G$。最有效的解决方案是改变物理原理：使用不同波长的光，例如近红外光（$\lambda \approx 940\,\text{nm}$），它较少被黑色素吸收，能更深地穿透皮肤以获得更清晰的信号 [@problem_id:4822376]。在这里，我们看到了问题的美妙统一性：健康公平的社会问题与光[吸收的量子力学](@entry_id:169728)密不可分。

#### 机器中的幽灵：医学影像与基因组学中的偏见

这种隐藏的环境偏见原则延伸到医学的其他领域。在医学影像中，一个被训练用于检测供应商A的机器CT扫描图像中肿瘤的AI模型，在处理供应商B的机器扫描图像时可能表现不佳，因为每台机器都有其独特的图像噪声和伪影“指纹”。如果一家医院的训练数据主要来自供应商A，算法可能会无意中学会将供应商B扫描图像的微妙特性与“无肿瘤”的预测联系起来。这是一个有趣的区分：*数据偏见*来自供应商B扫描图像的代表性不足，但*[算法偏见](@entry_id:637996)*来自学习过程本身——[经验风险最小化](@entry_id:633880)——它发现通过牺牲少数群体的表现来最小化平均误差更容易 [@problem_id:4530626]。

当我们审视我们自己的基因密码时，问题变得更加深刻。多基因风险评分（PRS）是强大的工具，通过分析我们DNA中数千个微小变异来预测我们患心脏病或糖尿病等疾病的风险。然而，用于开发这些评分的绝大多数基因数据来自欧洲血统的人。将一个在欧洲人身上开发的PRS应用于非洲或亚洲血统的人，不仅不准确，而且在科学上是无效的。特定[遗传标记](@entry_id:202466)与疾病之间的[统计关联](@entry_id:172897)依赖于连锁不平衡（LD）的复杂模式——即基因成块遗传的方式——这在不同祖先群体之间差异显著。PRS公式 $S=\sum w_i x_i$ 中的权重 $w_i$ 不是普适常数；它们是特定于群体的估计值。使用错误的权重集就像在一个新的国家使用错误的地图；你肯定会迷路 [@problem_id:5139455]。

其悲剧性后果是，这些不同层次的偏见可能复合成为一连串的失败。想象一个旨在寻找罕见病病因的基因组诊断流程 [@problem_id:4345688]。对于一个来自代表性不足的祖源群体的患者，第一步——检测其DNA中的[结构变异](@entry_id:173359)——可能灵敏度较低。第二步——将变异与策划的数据库进行比对——可能会找到较少的匹配项，因为针对其人群的研究较少。而第三步——使用贝叶斯模型计算变异致病性的概率——可能因为参考数据稀疏而从一个不公平的低[先验概率](@entry_id:275634)开始。每一步都引入一个微小、看似独立的偏见。但加在一起，它们可能导致诊断率——找到答案的机会——从一个合理的几率骤降至几乎为零，让一个家庭得不到另一个家庭本可获得的诊断。

### 预测的伦理学：分配护理与管理风险

这把我们带到了问题最困难的部分。如果我们能测量偏见，我们能消除它吗？我们当然可以尝试。这催生了计算机科学中一个有趣的子领域，试图用数学方式定义“公平”。但是，分配稀缺的公共卫生资源，比如热浪期间的紧急家庭访视 [@problem_id:4862491] 或对有自杀风险的人进行干预 [@problem_id:4752721]，最“公平”的方式是什么？

假设我们有两个群体，A和B。我们可以要求*[人口均等](@entry_id:635293)*（demographic parity），即算法应该在每个群体中标记相同比例的人。或者我们可以要求*预测均等*（predictive parity），即在被标记的人中，实际属于高风险的比例在各群体间应该相同。或者我们可以要求*[机会均等](@entry_id:637428)*（equal opportunity），即在所有真正高风险的人中，算法应该在每个群体中成功识别出相同的比例。这最后一个等同于拥有相等的[真阳性率](@entry_id:637442)（$TPR$）。一个更强的版本，*[均等化赔率](@entry_id:637744)*（equalized odds），要求相等的TPR和相等的[假阳性率](@entry_id:636147)（$FPR$）。

关键在于：公平性研究中的一个著名结果表明，除非在微不足道的情况下，如果不同群体之间病症的基础比率不同，那么在数学上就不可能同时满足所有这些公平性定义。

你被迫做出选择。而你的选择是一个伦理选择。考虑一个自杀风险预测模型 [@problem_id:4752721]。如果你的模型对一个少数群体的TPR较低，你就在未能识别和治疗该群体中急需帮助的人。这是[机会均等](@entry_id:637428)的失败，一种*干预不足*的伤害。同时，如果你的模型对同一群体的FPR较高，你就在让更多*并非*处于风险中的人接受不必要的、可能带有强制性的干预，比如精神病强制住院。这是一种*过度干预*的伤害。一个模型可以，并且常常会，同时对同一群体造成这两种类型的伤害，即使它满足了像预测均等这样的不同指标。没有简单的技术修复方法。选择一个[公平性指标](@entry_id:634499)意味着选择你更愿意容忍哪些伤害。

### 结论：超越代码

那么，这段旅程将我们引向何方？我们从一个客观机器的简单理想开始，最终深入到法律、伦理、物理和遗传学的丛林中。我们了解到，[算法偏见](@entry_id:637996)不是一个需要被清除的漏洞，而是一个从有偏见的世界中学习的系统的基本特征。

因此，解决方案不能是纯粹技术性的，它必须是社会性的。它需要一种新的法律和监管思维，一种能够理解差别对待和差别影响之间差异的思维 [@problem_id:4489362]，并让系统为其歧视性效果负责，而不仅仅是其意图。它为临床医生和平台创造了一种责任，即不能盲目相信算法的输出，而是要验证和监控其性能，特别是对于他们所服务的最脆弱的人群 [@problem_id:4507443]。

最重要的是，它呼吁权力的转移。长期以来，这些系统都是在与它们所影响的社区相去甚远的孤立环境中构建的。前进的道路，尤其是在像原住民健康这样的背景下，需要深入的社区合作、数据主权和共同治理 [@problem_id:4986447]。

归根结底，[算法偏见](@entry_id:637996)的研究是一门深刻的人文学科。它教导我们，要构建更好的机器，我们必须首先更仔细地审视我们自己——我们的历史，我们的社会，以及我们将我们的价值观和缺陷编码到我们创造的数据中的微妙方式。理解一台机器的决定如何可以追溯到[统计学习](@entry_id:269475)的各个层面，再到分子的[光吸收](@entry_id:136597)，再到一个社区的历史，就是以一种全新的、统一的、令人谦卑的清晰度来看待世界。而伴随这种清晰度而来的，是使用我们的工具不是为了复制过去的不公，而是为了建立一个更公平的未来的深远责任。