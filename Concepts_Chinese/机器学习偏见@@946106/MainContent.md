## 引言
我们常常求助于计算机以获得客观性。我们期望算法能帮助我们做出那些困扰了人[类数](@entry_id:156164)千年的选择：谁能获得贷款？谁能得到救命的器官？人们最大的希望是，机器的冰冷逻辑能将我们从人类判断的易错性中解放出来。然而，现实远比这复杂得多。我们的机器，以其自身沉默而合乎逻辑的方式，也可能带有偏见。这并非因为它们有不可告人的动机，而是因为它们是忠实的学习者，基于反映我们世界的数据进行训练——而我们的世界充满了历史遗留的不平等和系统性的差异。理解这一现象需要一段既是技术性又是哲学性的旅程。

要构建真正公平的系统，我们必须首先剖析偏见是如何产生的，然后审视其在整个社会中的深远影响。本文将引导您穿越这一复杂的领域。在第一部分“原理与机制”中，我们将解构偏见这一概念，探索其不同形式，并追溯其从数据收集到模型部署的生命周期。我们将揭示衡量公平性的挑战以及偏见可能造成的伤害类型。随后，在“应用与跨学科联系”中，我们将探讨其在法律、金融和医学等高风险领域的现实后果，揭示[算法偏见](@entry_id:637996)的研究如何将计算机科学与社会正义、伦理学乃至物理学联系起来。

## 原理与机制

在理解世界的过程中，我们构建模型。物理学家构建一个苹果下落的模型；经济学家构建一个市场模型；[机器学习算法](@entry_id:751585)则构建一个图像中的像素与“猫”这个词之间关系的模型。所有模型都是简化的。它们是地图，而非疆域。正如地图可能有误，模型也可能存在偏见。但我们所说的“偏见”究竟是什么意思？在算法的世界里，这个词带有两种不同但相关的含义，理解其差异是我们的第一步。

### 两种偏见的故事

想象你有一个浴室体重秤。如果它总是显示你比实际重五磅，统计学家会说这个秤有**偏见**（bias）。这是一种系统性的、可预测的误差。即使你称一百次体重，这些测量值的平均值仍会与真实值相差五磅。这与*[随机误差](@entry_id:144890)*不同——随机误差是你可能会看到的由于秤的弹簧有点[抖动](@entry_id:262829)而产生的小波动。这些波动可能平均后为零，但系统性偏见不会 [@problem_id:5225896]。这是偏见在统计学上的经典含义：估计量的平均值与其试图估计的真实值之间的持续差距 [@problem_id:4849723]。

然而，当我们在公平性的语境下讨论**[算法偏见](@entry_id:637996)**时，我们谈论的是具有更深层次社会和伦理分量的事情。我们谈论的是系统性地对可识别的人群造成不公劣势的误差。这不仅仅是秤不准的问题，而是秤对女性系统性地多显示十磅，而对男性只多显示一磅。这种偏见关乎[模型误差](@entry_id:175815)的不公平分布及其在现实世界中的伤害。一个算法可能对大多数人来说准确率很高，但对某个特定的人口群体却系统性地、危险地出错，这违反了正义这一基本伦理原则 [@problem_id:4396488] [@problem_id:4849723]。我们的焦点将是这第二种含义——作为不公来源的偏见。

### 偏见的生命周期：从数据到部署的旅程

偏见并非程序员恶意植入代码中的骇人漏洞。更多时候，它是一种在算法生命周期的每个阶段（从实验室构思到世界范围内部署）悄然渗入的毒药。让我们跟随这个生命周期，看看陷阱在哪里。我们可以将每个阶段看作一次转换，就像一系列透镜，每一个都有可能扭曲现实的真实图景 [@problem_id:5225894]。

#### 测量偏见：世界不等于数据

首先，我们必须收集数据。但我们测量世界的工具并非完美无缺；它们自身也带有偏见。一个经典而悲惨的例子来自医学。**[脉搏血氧仪](@entry_id:202030)**是一种夹在手指上测量血氧水平的设备，研究表明它对肤色较深的患者准确性较低。与白人患者相比，它会系统性地高估黑人患者的血氧饱和度。这不是一个软件问题，而是一个硬件问题，根源在于设备发出的光与不同程度的皮肤色素沉着之间的相互作用 [@problem_id:4408271]。当来自这种设备的数据被输入到人工智能模型中时，模型学习到的不是患者真实的血氧水平，而是已经被系统性误差污染的测量值。模型继承了仪器的偏见。

#### 抽样与选择偏见：数据不等于世界

接下来，我们选择一个数据集进行训练。我们的训练数据几乎永远不可能是一个完美的、能代表整个群体的快照。更多时候，它是一个便利样本。想象一下，我们仅使用入住重症监护室（ICU）的患者数据来开发一个预测败血症风险的模型。如果由于现有的分诊实践或结构性不平等，来自某个[边缘化](@entry_id:264637)群体的患者在病情严重程度相同的情况下不太可能被ICU收治，那么我们的训练数据就会系统性地倾斜。它将低估该群体中的重症病例 [@problem_id:4408271]。从这个倾斜的样本中学习的算法可能会得出结论，认为这个群体风险较低，却没有意识到它看到的是一幅不完整的图景。模型成了我们机构中早已存在的选择偏见的回声。

#### 标签偏见：标签不等于真相

机器学习通常需要“基准真相”标签。对于一个癌症检测模型，标签是“存在癌症”或“不存在癌症”。但谁来提供这些标签呢？通常，它们是真相的代理指标，而非真相本身。考虑一个旨在识别需要紧急干预的败血症患者的模型。作为“真实败血症”的代理指标，开发者可能会使用“医生施用了抗生素”这个标签 [@problem_id:4408271]。这似乎很合理。但如果医生们，无论出于有意识还是无意识的原因，对于一个群体的患者比另一个群体更快地施用抗生素，即使他们的症状相似呢？如果发生这种情况，训练标签就成了医生行为的记录，其中包含了所有潜在的偏见，而不是纯粹的疾病记录。人工智能学会复制这种模式，将历史和社会偏见直接融入其逻辑中。它学到的不是败血症*是什么*，而是*谁会得到*败血症的治疗。

#### 算法与评估偏见：模型不等于世界

即使我们得到一个完美测量、完美代表、完美标注的数据集——这在数据科学中如同神话中的独角兽——偏见仍然可能从算法本身产生。大多数算法旨在最小化总体误差。这听起来很高尚，但可能导致“多数暴政”。如果一个少数群体只占数据的$1\%$，模型可以通过对多数群体完全准确、对少数群体完全错误来达到$99\%$的准确率。从算法的角度来看，这是一个巨大的成功。但从公平性的角度来看，这是一场灾难 [@problem_id:4408271]。此外，模型架构的选择本身也可能引入偏见。如果对于一个群体来说，因素之间的真实关系是复杂的，而对于另一个群体来说是简单的，而我们选择了一个简单的模型（比如线性模型），它可能对后一个群体表现良好，但对前一个群体则惨败。这是一种源于模型自身局限性的偏见 [@problem_id:4406676]。

#### 部署偏见：部署环境不等于实验室

最后，一个在实验室原始条件下验证过的模型被部署到混乱、嘈杂的现实世界中。新的偏见可能因此产生。想象一个败血症警报系统，仅在一家特别繁忙的医院的夜班部署。临床医生已经不堪重负，可能会出现“警报疲劳”，开始忽略人工智能的建议。如果发生这种情况，模型的理论性能就变得毫无意义。它在现实世界中的影响为零，甚至更糟，如果临床医生的注意力分配不公，它可能会扩[大性](@entry_id:268856)能差距 [@problem_id:4408271]。

### “人在回路”中：一把双刃剑

一个普遍的希望是，人类专家——医生、法官、招聘经理——可以作为最后一道关卡，纠正算法的错误。但当这个人也带有偏见时会发生什么呢？考虑一个用于败血症的决策支持系统，它给出一个患者的风险评分。模型本身存在**[算法偏见](@entry_id:637996)**：由于历史上测试不足导致数据稀疏，它系统性地给予G2组患者比病情严重程度相同的G1组患者更低的风险评分。现在，我们在回路中加入一位临床医生。这位临床医生，或许由于自己的[内隐偏见](@entry_id:637999)或有缺陷的[启发式方法](@entry_id:637904)，对G2组采用了更高的决策阈值，需要评分达到$0.45$才采取行动，而对G1组仅为$0.30$。

结果是一场灾难。算法的偏见降低了分数，而临床医生的偏见提高了门槛。这两种偏见没有相互抵消，反而叠加在一起，极大地放大了不平等。在这个社会技术工作流中，G1组的[真阳性率](@entry_id:637442)可能达到可观的$0.85$，而G2组则骤降至$0.60$ [@problem_id:4849720]。在这个人机结合的系统中，G2组中百分之四十的败血症患者被漏诊了。“人在回路”中，远非保障，反而成了伤害的放大器。

### 衡量公平性：一项不可能完成的平衡之举？

如果我们要修正偏见，就必须首先衡量它。但这引出了一个棘手的问题：一个“公平”的结果是什么样的？事实证明，公平性有许多相互竞争的定义，而且它们往往是相互排斥的。

让我们想象一个用于在数字病理学中标记可疑玻片的AI工具。我们审计它在两个人口群体A和B上的表现 [@problem_id:4366384]。我们可能要求**[机会均等](@entry_id:637428)**（Equal Opportunity），即两个群体的真阳性率（TPR）应该相同。在我们的例子中，模型标记了A组$80\%$的恶性玻片和B组$80\%$的恶性玻片。这个条件满足了！这看起来很公平：如果你患有癌症，你的玻片被标记的机会与你所在的群体无关。

但让我们看得更深一些。我们也可以要求**[均等化赔率](@entry_id:637744)**（Equalized Odds），这要求TPR*和*[假阳性率](@entry_id:636147)（FPR）都相等。我们发现，模型对A组的误报率为$15\%$，但对B组为$20\%$。[均等化赔率](@entry_id:637744)被违反了。B组的患者要经受更多不必要的后续检查。

或者我们可以要求**预测均等**（Predictive Parity），这要求阳性预测值（PPV）相同。这意味着一个“可疑”的标记对于两个群体应该具有相同的权重。我们计算PPV，发现A组约为$57\%$，而B组约为$63\%$。预测均等也被违反了。对于B组患者的标记比对于A组患者的标记更有可能是真正的[癌症诊断](@entry_id:197439)。

症结就在于此。在数学上，一个模型通常不可能同时满足所有这些公平性标准，特别是当病症在不同群体中的基础患病率不同时 [@problem_id:4396488]。我们被迫做出选择。哪个更重要：确保对病患的检测率相等，最小化对健康者的误报，还是确保标记的预测意义一致？没有纯粹的技术答案。选择一个[公平性指标](@entry_id:634499)是一项具有深远权衡的伦理决策。

### 伤害的两个方面：分配与代表

归根结底，我们关心[算法偏见](@entry_id:637996)是因为它会造成伤害。这些伤害主要以两种方式表现：分配性和代表性。

**分配性伤害**关乎资源和机会的分配。当一个基于历史成本数据训练的算法，给一位年长的低收入患者Rivera女士分配了低风险评分时，它剥夺了她迫切需要的重症监护协调服务。它基于她过去开销这个有缺陷的代理指标，而非她当下的需求，将资源从她身边分配走 [@problem_id:4862115]。当一个分诊模型给一位跨性别患者分配了比临床上相似的顺性别患者更低的紧急性评分时，它将宝贵的时间和诊断资源从他们身边分配走 [@problem_id:4889180]。这是被剥夺有形之物的伤害。

**代表性伤害**更为微妙，但其破坏性不减。它关乎算法如何描述和描绘人。当一个系统因为一位怀孕的黑人患者Johnson女士因交通问题错过预约而将她标记为“不依从”时，它抹去了她的现实，并用一个污名化的标签取而代之。这个标签，经过一位临床医生称她“不遵从医嘱”的笔记放大后，损害了治疗关系并固化了有害的刻板印象 [@problem_id:4862115]。当一个电子健康记录系统反复错误地称呼一位跨性别患者的性别时，它侵犯了他们的尊严并动摇了他们的身份认同 [@problem_id:4889180]。这是被错误看待、身份被否认，或被迫成为一个有辱人格的讽刺画的伤害。

因此，[算法偏见](@entry_id:637996)远不止是一个技术故障。它是一面镜子，反映了我们的社会结构、历史不公以及我们自身的[内隐偏见](@entry_id:637999)。它提出了一个深刻的挑战，迫使我们不仅要面对我们编写的代码，还要面对代码所处的这个世界。理解其原理和机制，是朝着构建一种服务于全人类，而不仅仅是其中特权部分的技术迈出的第一步，也是至关重要的一步。

