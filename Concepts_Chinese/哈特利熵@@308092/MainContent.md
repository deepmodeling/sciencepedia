## 引言
我们如何为一个像“信息”这样的抽象概念赋予一个数值？这个基本问题曾困扰着通信领域的先驱们，他们需要一种方法来度量信号的内容、[信道](@article_id:330097)的容量，乃至知识本身的性质。在数字时代的复杂[算法](@article_id:331821)出现之前，人们需要一个简单而优雅的解决方案，来量化从众多可能性中选定一个结果时所消除的不确定性。这一探索催生了信息论的诞生，其最初、最基本的思想之一，便是通过简单地计算选择的数量来度量信息。

本文探讨的正是这一基础概念：[哈特利熵](@article_id:326312)。它弥合了“选择越多意味着信息越多”这一直观感受与构建稳健理论所需的数学形式主义之间的鸿沟。我们将揭示一个简单而深刻的见解：对数尺度是以一种可加且普适的方式度量信息的关键。

本文的结构旨在从基础开始逐步建立这种理解。在“原理与机制”部分，我们将深入探讨 Hartley 公式背后的逻辑，探索比特和奈特等不同的[信息单位](@article_id:326136)，并了解这一思想如何成为更普适的香农熵的一个特例。之后，“应用与跨学科联系”部分将带领我们穿越工程学、生物学和物理学，揭示这一个概念如何提供一种统一的语言，来描述从电报信号到 DNA 的信息内容乃至宇宙本身的一切事物。

## 原理与机制

### 如何度量一个选择？

单条数据中包含多少信息？它是在构成这个字母“A”的黑白像素中，还是在你大脑中单个[神经元](@article_id:324093)的放电中？在构建宏大理论之前，我们必须就我们正在度量什么达成共识。信息论的先驱们正面临着这个问题。他们不仅在思考计算机数据，还在思考电话信号、语言，乃至知识本身的性质。

让我们来做一个小小的思想实验，其灵感来源于 20 世纪 20 年代这些思想初见端倪时所进行的心理学研究。想象一下，你身处一个房间，面前有一块嵌有 16 个相同按钮的面板。你被告知，其中只有一个按钮是“正确”的，且所有按钮被选中的可能性均等。当你最终被告知哪个是正确按钮时，你获得了多少“信息”？

这比被告知一次硬币投掷结果的信息是更多还是更少？你的直觉可能会告诉你，信息多得多。一次硬币投掷只有两种可能性。而在这里，你有 16 种。你所获得的信息对应于被消除的不确定性。你关于 16 种可能性的不确定性，被简化为一种确定性。

这正是问题的核心。[信息量](@article_id:333051)是可能性数量的一种度量。但这种关系不是线性的。考虑两个这样的面板，每个都有 16 个按钮。组合可能性的总数不是 $16 + 16 = 32$，而是 $16 \times 16 = 256$。我们感觉信息应该是相加的，而不是相乘的。如果一次选择给了你 $I$ 量的信息，那么两次独立的选择应该给你 $2I$ 量的信息。什么数学函数能将乘法变为加法？是对数！

这个简单而深刻的见解是开启整个领域的钥匙。如果等可能性的数量是 $N$，我们称之为 $H_0$ 的信息内容，与 $N$ 的对数成正比。

$$H_0 = \log(N)$$

对于我们那块有 16 个按钮的面板，找到正确按钮所消除的[信息量](@article_id:333051)是 $\log(16)$ [@problem_id:1629825]。这个单一而优雅的思想由 Ralph Hartley 于 1928 年首次正式提出，而这个度量 $H_0$ 现在被称为 **[哈特利熵](@article_id:326312)**。它是对系统信息容量最简单、最基本的度量。

### 通用标尺：计算可能性

Hartley 思想的美妙之处在于其普适性。它不关心你是在按钮、安全码还是[量子态](@article_id:306563)之间进行选择。唯一重要的是一个问题：“有多少种选择？”整个游戏归结为计数。

想象一下，你正在设计一个生成 8 字符令牌的安全系统。规则可能很复杂：也许前三个字符是字母的特定[排列](@article_id:296886)，后五个是数字 [@problem_id:1666595]。要计算该系统的[哈特利熵](@article_id:326312)，你不需要了解任何电子学或密码学的知识，你只需要做一个细心的记账员。你计算选择字母的方式数量，计算选择数字的方式数量，然后将它们相乘以获得可能的令牌总数 $N$。

一旦你有了那个总数 $N$，任何单个令牌所包含的最大信息——其[哈特利熵](@article_id:326312)——就只是 $\log(N)$。就是这样。困难的工作在于计数，而不在于信息论。对数只是我们用来衡量结果的通用标尺。

但这引出了一个关键点。当我们测量长度时，我们必须指定单位——英寸、米或光年。当我们写下 $\log(N)$ 时，我们忽略了一个重要的东西：对数的底。改变底数就像更换我们的标尺。它不会改变信息的基本现实，但会改变我们用来描述它的数字。

### 信息的“货币”：比特、奈特和哈特利

对数底的选择无非是单位的选择。多年来，三种主要的信息“货币”已成为标准，每种都有其自身的历史和便利的应用领域。

1.  **比特（bit，底为2）：** 最著名的单位比特，源于使用以 2 为底的对数。信息以硬币投掷的次数为单位进行度量。它回答的问题是：“你需要进行多少次公平的硬币投掷，才能在 $N$ 种可能性中指定一个唯一的结果？”对于我们的 16 个按钮，由于 $16 = 2^4$，我们发现信息内容恰好是 $\log_{2}(16) = 4$ 比特 [@problem_id:1629825]。这并非巧合。你可以将按钮标记为 0000、0001、0010、...、1111，四个二元问题（第一个数字是 1 吗？第二个是……？）就能完美地识别任何按钮。这是计算机的自然语言。

2.  **哈特利（hartley，底为10）：** 这个单位也称为“班”（ban）或“迪特”（dit），是为了纪念 Hartley 本人而命名的，它使用以 10 为底的对数。这是为围绕我们十个手指和十进制系统构建的系统所做的自然选择。它回答的问题是：“你需要掷多少次一个 10 面骰子？”一个仅使用数字 0-9 且有 $10^6$ 种可能性的密码系统将包含 $\log_{10}(10^6) = 6$ 哈特利的信息。

3.  **奈特（nat，底为e）：** 这个单位使用自然对数（底为 $e \approx 2.718$），这个数字因其在微积分中的优雅性质而深受数学家喜爱。“奈特”从纯数学的角度看是最“自然”的单位，常用于理论物理和高等统计学。

由于这些只是同一基础数量的不同单位，我们必须能够在它们之间进行转换，就像我们在英寸和厘米之间转换一样。其工具是对数的标准换底公式：$\log_{b}(x) = \frac{\log_{a}(x)}{\log_{a}(b)}$。

所以，如果一位科学家测量一个[生物电路](@article_id:336127)的熵为 $1.75$ 比特，我们可以通过计算 $1.75 \times \log_{10}(2) \approx 0.5268$ 哈特利，将其表示为哈特利单位 [@problem_id:1666569]。反之，一个测量为 $2.5$ 哈特利的信号等效于 $2.5 \times \log_{2}(10) \approx 8.30$ 比特 [@problem_id:1666613]。一个 $\ln(42)$ 奈特的值就是 $\log_{10}(42)$ 哈特利 [@problem_id:1666597]。

这使我们能够进行有意义的比较。哪个系统具有更大的不确定性：一个测量为 $15$ 哈特利的量子过程，还是一个测量为 $45$ 比特的数据流？我们只需将它们转换为通用货币即可。将物理学家的测量值转换为比特，我们得到 $15 \text{ hartleys} \times \log_2(10) \frac{\text{bits}}{\text{hartley}} \approx 49.8$ 比特。事实上，这个量子系统的不确定性略高于该数据流 [@problem_id:1666612]。我们甚至可以合并来自独立测量的信息——例如，来自一个以奈特为单位测量的等离子体探测器和一个以比特为单位测量的磁力计——方法是将两者都转换为单一单位（如哈特利），然后简单地将它们相加 [@problem_id:1666590]。

### 当可能性不再均等：概率的引入

Hartley 的优雅公式 $H_0 = \log(N)$ 建立在一个强大但通常未言明的假设之上：每个结果都是等可能的。它假设硬币是公平的，骰子没有被做手脚，每个按钮成为正确选项的机会都相同。但当情况并非如此时会发生什么呢？

想象一个简化的电报系统，只有四个符号：A、B、C 和 D [@problem_id:1629789]。字母表大小为 $N=4$。[哈特利熵](@article_id:326312)，我们基于基数的度量，是 $\log_{2}(4) = 2$ 比特。这告诉我们每个符号*可能*发送的*最大*[信息量](@article_id:333051)。

但假设我们分析传输数据后发现，‘A’的使用频率为一半（$p_1=0.5$），‘B’为四分之一（$p_2=0.25$），‘C’和‘D’各为八分之一（$p_3=p_4=0.125$）。当你收到一个‘A’时，你真的会感到惊讶吗？不会很惊讶。它是最可能出现的结果。但如果你收到一个‘D’，那就更令人惊讶，[信息量](@article_id:333051)也更大。

简单的[哈特利熵](@article_id:326312)忽略了这种细微差别。它为系统赋予了每个符号 2 比特的固定信息量。这就是 Claude Shannon 的天才之处。他通过引入概率来完善 Hartley 的思想。Shannon 提出，单个事件 $i$ 的[信息量](@article_id:333051)或“惊奇度”与其概率 $p_i$ 成反比：

$$I(i) = -\log(p_i)$$

一个非常可能的事件（大的 $p_i$）携带很少的信息。一个极其罕见的事件（微小的 $p_i$）携带巨大的[信息量](@article_id:333051)。**香农熵**，我们用 $H$ 表示，则是每个事件的*平均*[信息量](@article_id:333051)，通过将每个事件的[信息量](@article_id:333051)按其发生概率加权计算得出：

$$H = -\sum_{i=1}^{N} p_i \log(p_i)$$

对于我们的电报例子，香农熵是 $H = -(0.5\log_2(0.5) + 0.25\log_2(0.25) + 2 \times 0.125\log_2(0.125)) = 1.75$ 比特。

注意发生了什么。真实的平均信息量，即香农熵，是 $1.75$ 比特。[哈特利熵](@article_id:326312)是 $2$ 比特。这种简单的、基于基数的方法高估了 $0.25$ 比特的信息内容 [@problem_id:1629789]。这不是一个错误，而是一个深刻的见解。不均匀的概率给系统引入了一定程度的可预测性，这*减少*了其平均不确定性。

我们现在可以正确地看待[哈特利熵](@article_id:326312)了：**[哈特利熵](@article_id:326312)是香农熵在所有概率均等时的特例。**它代表了一个具有 $N$ 个状态的系统所能拥有的绝对[最大熵](@article_id:317054)。任何偏离[均匀概率分布](@article_id:325112)的情况都会导致[香农熵](@article_id:303050)*小于*[哈特利熵](@article_id:326312)。最大可能熵（$H_0$）与实际熵（$H$）之间的这种差异是系统结构和可预测性的一种度量。在数据压缩中，这个差距被称为**冗余**——它是在一个编码中没有承载新信息的“浪费”的容量 [@problem_id:1666594]。

这个单一的框架，从计算选择开始，并用概率对其进行完善，其功能惊人地强大。它让生态学家能够根据物种种群量化栖息地的生物多样性 [@problem_id:2472839]，也让生物学家能够度量储存在蛋白质[分子构型](@article_id:298301)中的信息 [@problem_id:1666569]。从选择一个按钮这个简单的动作，我们抵达了一种描述世界结构和不确定性的通用语言。