## 引言
人工智能（AI）在医学领域的迅速崛起有望彻底改变诊断和治疗，但这种力量伴随着一个关键挑战：信任。我们如何能确定一个临床 AI 系统——一个做出事关生死建议的复杂“黑箱”——是安全、有效且公平的？许多关于 AI 性能的声明都缺乏透明度，导致其难以验证和复现，在潜力和可靠实践之间造成了巨大的鸿沟。本文旨在通过探索透明和可复现 AI 开发的框架来应对这一挑战。第一章“原则与机制”揭示了可复现 AI“配方”的核心组成部分，从数据处理到模型训练，并解释了像 TRIPOD-AI 这样的报告指南如何帮助避免数据泄露和选择偏倚等常见陷阱。第二章“应用与跨学科联系”则追溯了一个 AI 模型从可复现实验到真实世界临床部署的实践历程，考察了如何通过决策科学对其进行评估，其与临床医生的互动，以及其在伦理和法规环境中的应对。

## 原则与机制

想象一下，你找到一本蒙尘的旧食谱，上面记载着号称史上最华丽蛋糕的配方。但说明却很模糊：“加入一些面粉，一点糖，烤到看起来对劲为止。” 你试着照做，但你的蛋糕却成了一场灾难。这个配方真的好用过吗？还是你对“一点糖”的理解与原作者不同？没有一套精确、完整的说明，根本无从知晓。你无法复现结果，因此也无法验证其声明。

科学，其核心就是一本理解宇宙的食谱。每一篇科学论文都是一个配方，一份关于世界某部分如何运作的详细声明。其价值不在于作者的权威，而在于任何其他有好奇心的人都能够遵循同样的配方，并理想地得到相同的结果。这就是**[可复现性](@entry_id:151299)**原则，它是可信知识的基石。

当“配方”是一个临床人工智能（AI）系统时——一个可能决定谁能获得救生干预的复杂软件——对一份完整、明确配方的需求便事关生死。像 TRIPOD-AI 这样的报告指南，正是我们为撰写这些高风险“配方”而制定标准的尝试。它们的目的不是规定*如何*做科学研究，而是确保所做之事被足够清晰和详细地报告出来，从而使研究声明能够被审视、复现和信任 [@problem_id:5223340]。

### 数字配方的剖析

一个完整的 AI 模型“配方”是什么样的？它远不止是最终的代码。它是其创造过程的完整故事，这个故事包含几个必须完整叙述的关键章节。遗漏任何一章都会使配方不完整，结果也无法复现 [@problem_id:5223321]。

*   **原料（数据）：** 模型的优劣取决于训练它的数据。配方必须以一丝不苟的态度描述原料。数据从何而来？哪些患者被纳入，哪些被排除？我们试图预测的结局的确切定义是什么，又是如何测量的？这些信息定义了模型所知的“世界”，对于理解它可能在何处有效、何处失效至关重要 [@problem_id:5223369]。

*   **蓝图（模型架构）：** 我们的蛋糕是一个简单的磅蛋糕，还是一个复杂的多层法式蛋糕？配方必须指明模型的架构——例如，它是逻辑回归模型，还是具有特定层数和连接的[深度神经网络](@entry_id:636170)。这定义了可能的函数族，即**假设类别** $\mathcal{H}$，我们的最终模型将从中选出。

*   **烘焙说明（训练过程）：** 这是配方的动态部分。我们从一个随机配置的模型开始，通过向其展示数据来“训练”它。为了使其可复现，我们必须报告：
    *   **目标函数** $J(\theta)$：我们优化的目标是什么？是希望最小化误差，还是别的什么？这是对一个“好”蛋糕的定义。
    *   **优化器**：使用了哪种算法来找到最佳模型参数 $\theta$？（例如，[随机梯度下降](@entry_id:139134)）。
    *   **超参数** $\lambda$：这些是训练过程中的“旋钮和刻度盘”——比如学习率 $\alpha_t$、[批量大小](@entry_id:174288)和正则化强度。最终模型对这些设置极其敏感，必须精确报告 [@problem_id:5223323]。

*   **偶然性元素（随机种子）：** 许多训练算法涉及随机步骤，比如打乱数据或初始化模型参数。要获得*完全*相同的结果，你需要控制这种随机性。**随机种子** $s$ 是一个初始化[随机数生成器](@entry_id:754049)的数字。报告种子就像告诉下一位烘焙师洗牌的确切顺序。

*   **厨房环境（软件和硬件）：** 在高海拔地区烘焙蛋糕与在海平面不同。同样，在不同的软件版本或硬件上运行代码可能导致微小的数值差异，这些差异会累积成一个完全不同的最终模型。一个真正可复现的配方必须指明所用的确切软件库版本 $v$（例如，Python、PyTorch），有时甚至要指明硬件（如特定的 GPU）[@problem_id:5223321]。报告一个“容器”规范，如 [Docker](@entry_id:262723)file，是打包整个厨房环境的一种现代方式。

### 偷尝面糊的弥天大罪

现在，假设你正在开发一个新的蛋糕配方。你混合了面糊，烤了一小勺，尝了尝。太淡了。你又加了些糖，再次混合，再烤一勺。好多了。你重复了几次。最后，你把你最满意的面糊拿去烤成一个完整的蛋糕，并根据你最后一次尝面糊的味道，宣称它是一件杰作。这听起来对吗？当然不对。你一直在针对那批特定的面糊优化配方。你根本不知道最终的蛋糕对一个新的人来说是否好吃。

这是机器学习中最常见、也最危险的错误之一——**数据泄露**——的一个有力类比。为了诚实地评估一个模型在*新的*、未见过的数据上的表现，用来评估其最终性能的数据必须与用于构建和调整它的数据完全、彻底地分开。

这就是为什么我们要严格地划分数据 [@problem_id:5223320]：
*   **训练集 ($D_{\text{train}}$):** 这是主要的原料批次。它用于学习基本的模型参数 $\theta$。
*   **[验证集](@entry_id:636445) ($D_{\text{val}}$):** 这是你品尝的一小勺面糊。你用模型在此数据集上的表现来调整你的超参数 $\lambda$——决定加多少“糖”。
*   **测试集 ($D_{\text{test}}$):** 这是最终成品蛋糕，用选定的配方烘焙而成。它被锁在保险库里，只在生成最终报告的性能指标时被触碰*一次*。模型在开发过程中从未见过这些数据。**内部[测试集](@entry_id:637546)**可能来自与训练数据相同的源人群，而真正的**外部测试集**则来自不同的时间段或不同的医院，为模型的泛化能力提供了更强的考验。

但这种罪过还有一个更微妙的版本。如果你有 10 种不同的配方变体（即 10 种超参数 $\lambda$ 的选择），并在你的测试集上评估了所有这些变体，然[后选择](@entry_id:154665)最好的一个并报告其分数呢？你仍然“作弊”了。你使用了测试集来选择你的最佳模型。你报告的性能将是乐观偏倚的，因为你挑选了在那个特定测试集上碰巧表现最好的配方。这被称为**选择偏倚**。

正确、无偏倚的程序是**[嵌套交叉验证](@entry_id:176273)** [@problem_id:4558941]。它听起来复杂，但理念很简单。想象一场烘焙比赛。
1.  **外层循环（比赛本身）：** 评委（外层循环）给你一部分原料，并保留一部分秘密原料用于最终品尝测试。
2.  **内层循环（你的厨房）：** 用你得到的原料，你举办自己的迷你比赛。你将*这些*原料分成你自己的训练集和[验证集](@entry_id:636445)，以选择你的最佳超参数 $\hat{\lambda}$（你的最佳配方）。
3.  **最终烘焙对决：** 一旦你选定了你的唯一最佳配方，你就用评委给你的所有原料烘焙一个最终的蛋糕。然后他们用他们秘密保留的部分来评估它。

评委给出的最终分数是对你*整个配方选择过程*性能的[无偏估计](@entry_id:756289)。报告指南要求这种程度的严谨性，因为微妙的偏倚可能导致对模型性能的极度夸大。

### 并非所有问题都生而平等

到目前为止，我们一直在谈论一个通用的“配方”。但在医学领域，我们提出的问题类型千差万别，每种问题都需要一种不同的配方和不同的报告标准来确保其完整性 [@problem_id:5223377]。

1.  **预测：“这会发生吗？”** 一个预测患者败血症风险的模型正在回答一个预测性问题。其目标是准确估计一个条件概率，$P(Y \mid X)$——在给定一组预测因子 $X$ 的情况下，结局 $Y$ 发生的概率。关键关注点是**区分度**（它能否区分高风险和低风险患者？）和**校准度**（它预测的概率是否准确？）。**TRIPOD** 指南就是为此设计的，确保模型的开发和验证得到透明的报告。

2.  **因果推断：“这个干预措施有效吗？”** 一项评估 AI 分诊工具*影响*的研究正在提出一个因果问题。其目标是估计如果我们使用该工具与不使用该工具会发生什么——一个类似于 $E[Y(1) - Y(0)]$ 的量，即在两个潜在世界中结局的平[均差](@entry_id:138238)异。这里最大的威胁是**混杂**。金标准是随机对照试验，而**CONSORT-AI** 指南确保试验的所有关键要素——随机化、盲法以及复杂的 AI 干预本身——都得到妥善记录。

3.  **[诊断准确性](@entry_id:185860)：“这个测试检测疾病的效果如何？”** 一项关于 AI 解读胸部 X 光片的研究正在评估其作为诊断测试的准确性。在这里，我们关心的是它的内在属性：**敏感性**，$P(\text{测试为阳性} \mid \text{疾病存在})$，和**特异性**，$P(\text{测试为阴性} \mid \text{疾病不存在})$。主要威胁是患者选择方式中的偏倚（**谱系偏倚**）或确认真实疾病状态方式中的偏倚（**验证偏倚**）。**STARD-AI** 指南关注这些问题，要求清晰报告患者人群、待测检验（AI）和[参考标准](@entry_id:754189)（“金标准”）。

混淆这些问题会导致有缺陷的科学。你不能仅仅通过展示一个 AI 工具具有高预测准确性来证明它能改善结局（这是一个因果声明）。这些是根本不同的科学追求，有着不同的估计量和偏倚，因此需要不同的报告结构 [@problem_id:5223368]。

### 防范我们自己：偏倚、机遇和公平

即使有最好的技术程序，科学仍然是一项人类事业，而我们是欺骗自己的大师。从某种意义上说，报告指南是防范我们自身固有偏倚的一种方式。

最诱人的偏倚之一是**选择性报告**，或“采樱桃”。想象一项研究在 8 种不同的健康结局上测试一个 AI。即使这个 AI 毫无用处，如果你在标准的 $\alpha = 0.05$ [显著性水平](@entry_id:170793)上测试每个结局，仅凭纯粹的机遇获得至少一个“统计显著”的阳性结果的概率可能会出奇地高——在这种情况下，大约是 34%！($1 - (0.95)^8 \approx 0.337$) [@problem_id:5223346]。一项未注册的研究允许研究人员强调这一个幸运的发现，并掩盖 7 个无效结果，从而制造一幅完全误导性的图景。

解药是**预注册**。在研究开始之前，研究人员必须在像 ClinicalTrials.gov 这样的注册平台上公开声明他们的主要终点和完整的分析计划。这就像与科学界签订了一份有时间戳的合同。它使他们承诺报告其主要问题的结果，无论这些结果是令人兴奋还是无效。它防止了在比赛开始后移动球门。

最后，我们必须防范一种更[隐蔽](@entry_id:196364)的错误形式：构建一个不公平的模型。一个模型可能总体性能出色，但对特定人群效果不佳，这可能是由于训练数据中的偏倚或疾病在不同人口群体中表现方式的差异所致。对于临床 AI 而言，这不仅仅是一个统计问题，更是一场伦理危机。

因此，现代报告指南推动我们评估**[算法公平性](@entry_id:143652)** [@problem_id:5223341]。这意味着预先指定重要的亚组（例如，基于种族、性别或年龄），并评估模型在每个组内的性能——其区分度、校准度和错误率。目标是透明地报告该模型对其所有预期服务对象是否都值得信赖。该模型对男性和女性的校准度是否都很好？它在不同种族群体中的[真阳性率](@entry_id:637442)和[假阳性率](@entry_id:636147)是否相似（一个被称为**[均等化赔率](@entry_id:637744)**的概念）？通过要求进行这些分析，我们从“它有效吗？”转向了更关键的问题：“它对谁有效，又可能对谁造成伤害？”

归根结底，透明报告的原则和机制并非官僚主义的勾选框框。它们是科学怀疑主义和谦逊精神的实践体现。它们是我们用来建立信任基础的工具，确保我们部署在医学领域的强大 AI 系统不仅是工程奇迹，而且是人类福祉的安全、有效和公平的仆人。

