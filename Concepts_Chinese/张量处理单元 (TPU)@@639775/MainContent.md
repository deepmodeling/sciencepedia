## 引言
机器学习的快速发展对计算能力产生了巨大的需求，将 CPU 等通用处理器推向了极限。这一计算瓶颈催生了为加速人工智能独特工作负载而设计的专用硬件的发展。张量处理单元 (TPU) 正是这一新[范式](@entry_id:161181)的典型代表。然而，要理解其影响力，我们必须超越简单的基准测试，深入探究使其如此高效的根本性设计哲学。本文旨在填补“知道 TPU 很快”与“理解 TPU *为何*很快”之间的知识鸿沟，探讨其与传统[处理器设计](@entry_id:753772)的根本性背离。

接下来的章节将引导您进入 TPU 复杂精妙的世界。首先，在“原理与机制”一章中，我们将剖析该机器的核心，审视[脉动阵列](@entry_id:755785)、[数据流](@entry_id:748201)[范式](@entry_id:161181)和专用[数值格式](@entry_id:752822)，这些共同提供了前所未有的性能和效率。然后，在“应用与跨学科联系”一章中，我们将看到这些原理在实践中的应用，探索 TPU 如何解决机器学习中的实际问题，以及其计算模式如何应用于信号处理和[科学计算](@entry_id:143987)等不同领域，从而阐明硬件、算法和系统级设计之间的深层相互作用。

## 原理与机制

要真正理解张量处理单元，我们必须超越“更快”或“更强大”的表面描述。我们必须追问*为什么*它更快，以及它的设计如何体现了一种根本不同的计算哲学。就像物理学家拆解时钟以观察齿轮如何啮合一样，我们现在将探讨赋予 TPU 非凡能力的核心原理和机制。我们会发现，其强大之处并非源于增加更多复杂性（这在通用处理器中很常见），而是源于一种激进而优雅的简化，这种简化完美地契合了机器学习的世界。

### 机器的心脏：[脉动阵列](@entry_id:755785)

[现代机器学习](@entry_id:637169)的核心在于一项无休止、近乎单调的任务：矩阵乘法。训练和运行[神经网](@entry_id:276355)络涉及一遍又一遍地乘以巨大的数字矩阵。传统的处理器，就像一个技艺高超的工匠，可以执行许多复杂的任务，但对于这种重[复性](@entry_id:162752)的大规模劳动可能效率不高。TPU 的解决方案不是创造一个更熟练的工匠，而是为数字构建一个巨大、完美同步的工厂流水线。这条流水线被称为**[脉动阵列](@entry_id:755785)**。

想象一个由简单处理单元或**处理单元 (PE)** 组成的巨大网格。假设是一个 $128 \times 128$ 的网格，总共给我们带来了 $16,384$ 个 PE。对于一次矩阵乘法，我们可以将[神经网](@entry_id:276355)络层的权重预加载到这个网格中，每个 PE 保存一个权重值。然后，我们让输入数据（激活值）流过这个网格。当数据“脉冲式地”通过阵列时——很像血液流经[循环系统](@entry_id:151123)，这也赋予了该阵列“脉动”的名称——每个 PE 执行一个单一、简单的操作：它将其存储的权重与传入的激活值相乘，并将结果与从其邻居传下来的运行总和相加。[矩阵乘法](@entry_id:156035)的最终结果从阵列的另一侧出现。

每个 PE 都很简单，但成千上万个 PE 完美协同工作的集体力量是巨大的。一个典型的[数字信号处理器 (DSP)](@entry_id:748428) 可能使用少数几个强大的核心，每个核心在每个时钟周期内能够执行数个操作。但这种方法根本无法扩展到 TPU 的水平。一个由 90 个高性能 DSP 核心组成的假设系统，其持续吞吐量可能达到约 $0.65$ 万亿次乘加 (MAC) 运算每秒。而单个 TPU，凭借其 $16,384$ 个 PE，可以达到近 $10$ 万亿次 MAC 运算每秒，同时[功耗](@entry_id:264815)预算相近 [@problem_id:3634505]。这是从利用单个指令流内的并行性（**时间并行**）到利用由简单计算元件组成的广阔物理空间中的并行性（**空间并行**）的根本性转变 [@problem_id:3634537]。

### 让数据引领节奏：数据流优于控制流

传统的处理器，如 CPU 或 DSP，遵循“控制流”[范式](@entry_id:161181)运行。它不断地从内存中获取指令，解码它们，然后执行它们。这些指令中有很大一部分用于控制——`if-then` 语句、循环、函数调用——这些在代码中表现为分支。每当处理器遇到一个分支，它必须预测程序将走哪条路径，以保持其长执行流水线充满。如果猜错了，整个流水线必须被清空并重新填充，浪费了宝贵的周期和能量。即使有复杂的分支预测器，这种开销也相当可观。对于一个控制密集型任务，DSP 可能会将高达 $30\%$ 的时间用于因这些错误预测而导致的[停顿](@entry_id:186882)，从而大幅降低其性能 [@problem_id:3634472]。

TPU 采取了不同的方法。它是一台**数据流**机器。对于机器学习的核心计算，数据的“舞蹈”是高度可预测的。矩阵乘法总是一系列相同的乘法和加法。TPU 的控制逻辑不是不断地获取指令来问“我下一步该做什么？”，而是在事先配置好的。数据流入[脉动阵列](@entry_id:755785)，而阵列根据其自身的设计，执行正确的操作序列。没有需要预测的分支。

这消除了整个分支预测机制及其相关的惩罚。TPU 的性能变得极其确定和高效。存在的少量控制开销是用于管理整体流程，而不是每秒做出数百万个微小的决策。通过牺牲运行具有复杂控制流的任意代码的灵活性，TPU 在其设计针对的任务上实现了近乎完美的效率。

### 赢得对抗[内存墙](@entry_id:636725)的战争

现代计算最大的挑战之一是“[内存墙](@entry_id:636725)”。处理器速度极快，但从主内存 (D[RAM](@entry_id:173159)) 获取数据的速度相对较慢且能耗高。一个渴望数据的处理器，无论其多么强大，都是无用的。因此，性能的关键在于通过最小化从主内存的数据移动来保持处理器的供给。

这里的关键指标是**[算术强度](@entry_id:746514)**，定义为执行的算术运算次数与从内存传输的数据字节数之比。目标是最大化这个比率。这是通过**数据重用**实现的：一旦一块数据被取入高速的片上内存，它应该在被丢弃之前被尽可能多次地使用。

这正是专用加速器的设计优势所在，而通用处理器则可能在此 falter。考虑一个在 DSP 上实现的[数字滤波器](@entry_id:181052)。如果其片上内存不足以容纳滤波器所需的所有历史数据，它将被迫为它计算的每一个输出都从主内存重新获取旧数据。这会彻底摧毁其[算术强度](@entry_id:746514) [@problem_id:3634573]。

相比之下，TPU 从头开始就是为数据重用而构建的。[脉动阵列](@entry_id:755785)的结构本身就是这一点的证明。例如，在**权重固定** (weight-stationary) [数据流](@entry_id:748201)中，一块滤波器权重被加载到 PE 中并保持不动，而整个图像流经阵列。每个权重在它操作的每一个像素上都被重用，从而最大化其重用率。这是**算法-架构协同设计**的完美范例，其中算法（例如，卷积的结构方式）和硬件被协同设计以最大化效率 [@problem_id:3634483]。为了保持[脉动阵列](@entry_id:755785)的持续供给，TPU 采用了复杂的技术，例如将下一个数据块的预取与当前[数据块](@entry_id:748187)的计算重叠，从而有效地隐藏了内存访问的延迟 [@problem_id:3634485]。

### 选对工具：专用数值格式与[能效](@entry_id:272127)

[神经网](@entry_id:276355)络识别一只猫所需的[数值精度](@entry_id:173145)，与物理学家模拟亚原子粒子所需的[数值精度](@entry_id:173145)相同吗？答案是响亮的“不”。[神经网](@entry_id:276355)络对噪声和精度降低具有显著的弹性。这一观察为另一种强大的特化形式打开了大门：[数值格式](@entry_id:752822)的选择。

虽然 DSP 通常依赖于刚性的[定点运算](@entry_id:170136)，但 TPU 采用了一种名为 **[bfloat16](@entry_id:746775)**（脑浮点数）的格式。一个标准的 32 位浮点数有 8 位用于指数（决定其范围）和 23 位用于[尾数](@entry_id:176652)（决定其精度）。Bfloat16 是一个聪明的折衷方案：它保留了 32 位浮点数的 8 位指数，但将尾数削减到只有 7 位。结果是一个 16 位的数字，它具有与 32 位数字相同的巨大动态范围，但精度较低。对于深度学习而言，其中数值的大小可能变化剧烈，但高精度并非至关重要，这是一个完美的权衡。它提供了足够的范围以防止数值溢出或下溢，而没有完整 32 位精度的存储和计算成本 [@problem_id:3634529]。

这种特化对能效产生了深远的影响。CMOS 电路的动态功耗由优美而简单的关系 $P_{dyn} = \alpha C V^2$ 决定，其中 $\alpha$ 是开关活动， $C$ 是电容， $V$ 是电源电压。因此，执行一次操作的能量与 $V^2$ 成正比。更简单的算术单元，如用于 [bfloat16](@entry_id:746775) 的单元，可以被设计为在更低的电压下运行。即使电压从例如 $1.0\,\text{V}$ 适度降低到 $0.8\,\text{V}$，每次操作的能耗也能减少近 $36\%$，因为节能效果与电压的平方成比例。这种平方优势是 TPU 不仅更快，而且[能效](@entry_id:272127)远高于其通用对应物的关键原因 [@problem_id:3634564]。

### 从单芯片到超级计算机

TPU 的设计哲学——为大规模、重[复性](@entry_id:162752)工作负载进行特化——意味着一种权衡。使用 TPU 存在一次性的“[预热](@entry_id:159073)”成本，主要用于即时 (JIT) 编译，其中高级的[神经网](@entry_id:276355)络图被转换为配置[脉动阵列](@entry_id:755785)的低级指令。对于一个小任务，这个固定的启动成本可能会主导总执行时间，使得传统的 CPU 或 DSP 更快。然而，随着数据量的增长，这个初始成本被摊销，TPU 令人难以置信的[吞吐量](@entry_id:271802)很快就会胜出 [@problem_id:3634499]。TPU 是为马拉松而生，而非短跑。

此外，该架构被设计为可以扩展到单芯片之外。世界上最大的[神经网](@entry_id:276355)络体量过于庞大，无法装入单个设备。**TPU Pod** 通过定制的、超高带宽、低延迟的互连将成百上千个 TPU 芯片连接在一起。这不是标准的[以太](@entry_id:275233)网网络；它是一种与芯片本身协同设计的专用[网络结构](@entry_id:265673)。这允许一个巨大的模型被划分到多个芯片上（**模型并行**），激活值从一个芯片无缝地流到下一个芯片，就好像它们都是一个巨大的、[分布](@entry_id:182848)式[脉动阵列](@entry_id:755785)的一部分。对于这样的系统要能工作，互连的通信延迟和带宽与芯片本身的计算能力同样至关重要 [@problem_id:3634563]。

本质上，使单个 TPU 核心高效的原则在整个数据中心的规模上得到了镜像，创造了一个用于机器学习的、内聚的、专用的超级计算机。TPU 的美妙之处在于它对几个核心思想的一贯应用：拥抱问题的本质，为该任务构建最简单的硬件，然后将其大规模扩展。

