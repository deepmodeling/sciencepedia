## 引言
在深度学习这个错綜复杂的世界里，优化器是驱动训练过程的引擎，它在广阔复杂的“景观”中导航，以寻找能让模型学会知识的解决方案。在这些引擎中，像 Adam 这样的自适应方法因其速度和效率而备受青睐，已成为主力工具。然而，在它们的应用中长期存在一个微妙但重大的误解：将 L2 正则化与[权重衰减](@article_id:640230)混为一谈。虽然对于较简单的优化器而言，两者在数学上是等价的，但这种等价性在自适应方法中不复存在，导致了次优的[正则化](@article_id:300216)效果，并阻碍了模型的泛化能力。本文将剖析这一关键区别，以揭示 [AdamW](@article_id:343374) 这一修正了此缺陷的优化器的精妙与强大之处。我们将首先探讨区分真实[权重衰减](@article_id:640230)与其有缺陷实现的**原理与机制**，理解为何将其与优化步骤[解耦](@article_id:641586)至关重要。随后，我们将探索其深远的**应用与跨学科联系**，考察 [AdamW](@article_id:343374) 如何与其他先进的训练技术相互作用，以在真实世界场景中提高稳定性、泛化能力和性能。

## 原理与机制

要真正领略 [AdamW](@article_id:343374) 的精妙之处，我们必须从一个看似简单的概念——正则化——开始。在机器学习的世界里，我们的模型常常像个过于热切的学生，能够完美地记住训练数据，却无法掌握在新问题上表现良好所需的基本概念。这种现象被称为**过拟合**。为了解决这个问题，我们引入一种惩罚形式，即**[正则化](@article_id:300216)**，以抑制模型变得过于复杂。

### 两种[正则化方法](@article_id:310977)的故事

最常见且有效的[正则化](@article_id:300216)形式之一是 **$L_2$ 惩罚项**。其思想非常简单：我们在主要的目标函数，即**损失** $L(\mathbf{w})$，上增加一项，用于惩罚较大的模型参数（或权重）$\mathbf{w}$。新的[目标函数](@article_id:330966) $J(\mathbf{w})$ 变为：

$$
J(\mathbf{w}) = L(\mathbf{w}) + \frac{\lambda}{2}\|\mathbf{w}\|_2^2
$$

在这里，$\|\mathbf{w}\|_2^2$ 是整个参数矢量大小的平方，而 $\lambda$ 是一个控制惩罚强度的小数值。当我们的优化器试图最小化这个新目标时，它必须在最小化原始损失和保持权重较小之间取得平衡。

当我们计算梯度来更新权重时，$L_2$ 惩罚项会增加一个简单的项：

$$
\nabla J(\mathbf{w}) = \nabla L(\mathbf{w}) + \lambda \mathbf{w}
$$

现在，让我们考虑最简单的优化器——**[随机梯度下降](@article_id:299582) (SGD)**。SGD 的更新规则是在梯度的反方向上迈出一小步：

$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla J(\mathbf{w}_t) = \mathbf{w}_t - \eta (\nabla L(\mathbf{w}_t) + \lambda \mathbf{w}_t)
$$

如果我们重新[排列](@article_id:296886)这个方程，会发生奇妙的事情：

$$
\mathbf{w}_{t+1} = (1 - \eta \lambda) \mathbf{w}_t - \eta \nabla L(\mathbf{w}_t)
$$

看！这个更新可以被看作两个独立的步骤：首先，将当前权重按因子 $(1 - \eta \lambda)$ 进行收缩；然后，对原始损失执行正常的梯度步骤。这种直接的收缩被称为**[权重衰减](@article_id:640230)**。在很长一段时间里，由于在 SGD 中存在这种精确的代数等价性，术语“$L_2$ [正则化](@article_id:300216)”和“[权重衰减](@article_id:640230)”被互换使用。它们似乎只是描述同一事物的两种不同方式 [@problem_id:3141373] [@problem_id:3100029]。

### 自适应方法的复杂性

然而，[深度学习](@article_id:302462)的世界早已超越了原生 SGD。我们现在拥有更强大的工具，其中最主要的是**自适应优化器**，如 **Adam ([自适应矩估计](@article_id:343985))**。

想象你正在管理一个庞大的工人团队，每个工人都负责模型中的一个参数。有些任务很简单，指令清晰（梯度大且一致），而另一些任务则很精细，需要小心调整（梯度小且有噪声）。像 Adam 这样的自适应优化器就像一个老练的管理者。它不是给每个人下达相同大小的指令（单一的学习率 $\eta$），而是为每个工人提供个性化的指令大小。处理有噪声、困难任务的工人会得到更小、更谨慎的指令，而那些路径清晰的工人则可以迈出更大、更自信的步伐。

在数学上，Adam 通过为每个参数维护一个过去梯度平方的[移动平均](@article_id:382390)值（我们称之为 $v_t$）来实现这一点。然后，每个参数 $w_i$ 的更新量会根据其自身的 $v_{t,i}$ 的平方根进行反向缩放。[更新过程](@article_id:337268)大致如下，其中 $\mathbf{D}_t$ 是一个包含这些逐参数[学习率](@article_id:300654)的对角矩阵：

$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \mathbf{D}_t \nabla J(\mathbf{w}_t)
$$

那么，当我们使用 Adam 结合“标准” $L_2$ 正则化时会发生什么呢？我们给它输入梯度 $\nabla L(\mathbf{w}_t) + \lambda \mathbf{w}_t$。更新变为：

$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \mathbf{D}_t (\nabla L(\mathbf{w}_t) + \lambda \mathbf{w}_t)
$$

让我们将[自适应学习率](@article_id:352843)矩阵 $\mathbf{D}_t$ 分配进去：

$$
\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \mathbf{D}_t \nabla L(\mathbf{w}_t) - \eta \lambda \mathbf{D}_t \mathbf{w}_t
$$

突然之间，我们美丽的等价性被打破了。[权重衰减](@article_id:640230)项 $\lambda \mathbf{w}_t$ 现在也被[自适应学习率](@article_id:352843)矩阵 $\mathbf{D}_t$ 所乘。收缩过程不再是简单、统一的衰减，而是与优化器的自适应机制**耦合**在了一起 [@problem_id:3141373] [@problem_id:3100029]。

### 一个意想不到的后果

为什么这种耦合是个问题？让我们回想一下管理者的比喻。Adam 对那些有 large、frequent updates 历史的参数给予较小的学习率。这对于在[损失景观](@article_id:639867)中导航通常是件好事。但由于[权重衰减](@article_id:640230)现在是耦合的，这意味着这些非常“活跃”的参数所受到的正则化收缩也*更少*。这就像告诉最富生产力的员工，全公司范围内的预算削减（[权重衰减](@article_id:640230)）对他们不那么适用。这是一种奇怪且常常适得其反的相互作用。

我们可以更具体地说明这一点。可以证明，标准 $L_2$ 正则化的 Adam 的**有效收缩系数**与 $\frac{\alpha \lambda}{\sqrt{v_t} + \epsilon}$ 成正比，其中 $\alpha$ 是基础学习率，而 $v_t$ 是梯度平方的移动平均值 [@problem_id:3096924]。对于一个变化很大的参数，$v_t$ 会很大，其有效的正则化强度就会很小。[正则化](@article_id:300216)在参数间的分布变得不均匀，而这绝非初衷 [@problem_id:3141378]。

为了以最鲜明的形式看待这一点，考虑一个数据梯度为零（$g^{\text{data}} = 0$）但值不为零的参数。它应该只做一件事：衰减至零。在 [AdamW](@article_id:343374) 中，正如我们将看到的，它确实如此，按因子 $(1 - \eta \lambda)$ 收缩。但在标准的 Adam 中，总梯度仅为衰减项 $\lambda \theta_0$。然后，自适应机制会用其自身的大小来[归一化](@article_id:310343)这个梯度！结果是一个奇异的收缩因子，它取决于权重自身的值，导致较大的[权重衰减](@article_id:640230)的效率反而*低于*较小的权重 [@problemid:3161372]。那个旨在使优化更稳定的机制，无意中破坏了[权重衰减](@article_id:640230)简单、 čisté 的行为。

### [AdamW](@article_id:343374) 的优雅[解耦](@article_id:641586)

这一有缺陷的相互作用的发现，催生了一个极其简单的解决方案，由 Ilya Loshchilov 和 Frank Hutter 提出：**[解耦](@article_id:641586)的[权重衰减](@article_id:640230)**。由此产生的优化器现在以 **[AdamW](@article_id:343374)** 闻名。

其思想正是我们*一直以为*自己在做的事情。我们让 Adam 的自适应机制只处理*数据损失*的梯度，然后像在简单的 SGD 案例中看到的那样，单独应用[权重衰减](@article_id:640230)步骤。

[AdamW](@article_id:343374) 的更新规则是：

1.  仅根据数据损失梯度 $\nabla L(\mathbf{w}_t)$ 计算自适应步骤。我们称之为 $\Delta \mathbf{w}_{\text{Adam}}$。
2.  将此步骤和单独的[权重衰减](@article_id:640230)步骤应用于权重。

$$
\mathbf{w}_{t+1} = \mathbf.w}_t - \Delta \mathbf{w}_{\text{Adam}} - \eta \lambda \mathbf{w}_t
$$

我们可以用那个熟悉的形式重写它：

$$
\mathbf{w}_{t+1} = (1 - \eta \lambda) \mathbf{w}_t - \Delta \mathbf{w}_{\text{Adam}}
$$

就是这样。[权重衰减](@article_id:640230)再次成为一个简单、干净、统一的收缩，按因子 $(1 - \eta \lambda)$ 应用于所有参数，无论它们的梯度历史如何 [@problem_id:3181573] [@problem_id:3100029]。它与[自适应学习率](@article_id:352843)“[解耦](@article_id:641586)”了。这个小小的改变恢复了[权重衰减](@article_id:640230)的初衷，并修复了那个有缺陷的相互作用。两种方法之间的差异不仅仅是理论上的；从第一步开始，它们就会导致不同的更新路径 [@problem_id:2152239] [@problem_id:3169473]。

### 涟漪效应

这个看似微小的修正带来了深远的实际影响。

首先，它使[超参数调整](@article_id:304085)更加稳健。在标准 Adam 中，有效[正则化](@article_id:300216)强度与[学习率](@article_id:300654)和梯度历史相关联。改变学习率 $\eta$ 或动量参数 $\beta_1, \beta_2$ 会间接改变有效的 $\lambda$。在 [AdamW](@article_id:343374) 中，$\eta$ 和 $\lambda$ 更加独立，使得为两者找到好的值变得更容易。

其次，它揭示了训练调度中的一个新的微妙之处。[AdamW](@article_id:343374) 中每一步的收缩由乘积 $\eta_t \lambda$ 决定。如果你使用像**[学习率调度](@article_id:642137)**这样的常用技术，其中学习率 $\eta_t$ 在训练过程中减小，那么你的有效[权重衰减](@article_id:640230)也会减小。为了保持恒定的正则化压力，可能需要调度 $\lambda$ 使其随着 $\eta_t$ 的减小而增加 [@problem_id:3176533]。

最终，[AdamW](@article_id:343374)之所以成为训练像 transformers 这样的大型模型的默认选择，是因为这个修正确实有效。通过将[正则化](@article_id:300216)恢复到其“真实”形式，[AdamW](@article_id:343374) 常常能找到“更平坦”的解——即它们位于[损失景观](@article_id:639867)中更宽、更稳健的山谷中。这些更平坦的解往往能更好地**泛化**，在它们从未见过的数据上表现得更准确 [@problem_id:3135436]。这是一个完美的例子，说明了对我们的工具有了深刻的、基于第一性原理的理解后，如何能通过一个简单的改变带来强大而实际的影响。

