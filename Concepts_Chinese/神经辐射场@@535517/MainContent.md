## 引言
我们如何能教会机器仅从一组二维照片中感知并重建我们的三维世界？传统[计算机图形学](@article_id:308496)依赖于网格和体素等显式几何图元，而一种重新定义场景表示方法的革命性技术已经出现。[神经辐射场](@article_id:641556)（Neural Radiance Fields, NeRFs）通过将场景建模为[连续函数](@article_id:297812)，提供了一种新的[范式](@article_id:329204)，弥合了数字图像与真正的三维理解之间的鸿沟。本文旨在揭开这项强大技术的神秘面纱，从其核心概念讲起，一直延伸到其不断扩展的应用领域。

本次探索分为两大章节。首先，在“原理与机制”一章中，我们将剖析 NeRF 优雅的架构，从其隐式函数表示、[位置编码](@article_id:639065)的巧妙运用，到基于物理的体渲染艺术。我们将揭示这些模型如何学会从光影中“雕塑”出场景。随后，在“应用与跨学科联系”一章中，我们将揭示 NeRF 如何超越简单的图像合成。我们将看到其可微性如何将其转变为强大的科学工具，用于逆向渲染、动态场景捕捉，甚至[材料科学](@article_id:312640)中的微观分析等任务，展示其作为描述现实的通用语言的潜力。

## 原理与机制

### 教会网络看见三维：隐式表示思想

想象你有一个神奇的函数，我们称之为 $f_{\theta}$。这个函数本质上是一个带有参数 $\theta$ 的[神经网络](@article_id:305336)，它接受任何三维坐标 $\mathbf{x} = (x, y, z)$ 作为输入。然后，它会告诉你空间中该确[切点](@article_id:351997)上存在什么。具体来说，它输出两样东西：体积**密度**（volumetric **density**）$\sigma$（你可以将其视为该点的“[不透明度](@article_id:320846)”），以及一个颜色 $\mathbf{c}$。为了使场景看起来逼真，颜色应该依赖于观察方向，所以我们的函数实际上是 $f_{\theta}(\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)$，其中 $\mathbf{d}$ 是表示观察方向的[单位向量](@article_id:345230)。

这就是隐式[表示的核](@article_id:380858)心。没有三角形，没有体素，只有一个函数。这种方法的直接优点在于其连续性。因为神经网络是一个[连续函数](@article_id:297812)，我们可以在空间的*任何*点进行查询，而不仅仅是在预定义的网格上。这原则上给了我们无限的分辨率。如果你想放大一个微小的细节，你只需在越来越精细的坐标上查询该函数。该性质被称为**利普希茨连续**（**Lipschitz continuous**），它确保了当我们放大时，渲染出的图像会平滑无缝地变化，而不会出现放大低分辨率图像时看到的像素化瑕疵[@problem_id:3136687]。场景不再是离散的数据集合，而是一个连续、可微的颜色和[不透明度](@article_id:320846)场。

### 清晰度的秘密：[位置编码](@article_id:639065)

但是，一个标准的神经网络如何可能学习到真实世界场景中错综复杂的高频细节，比如木头的纹理或树上的叶子？事实证明，一个简单的网络出奇地“懒惰”。它具有很强的**谱偏见**（**spectral bias**），这意味着它学习平滑、低频的函数远比学习清晰、细节丰富的函数要容易得多。如果你让一个简单的网络学习一个复杂的场景，你很可能会得到一个模糊、[过度平滑](@article_id:638645)的结果。

解决方案是一个名为**[位置编码](@article_id:639065)**（**positional encoding**）的巧妙技巧。在将三维坐标 $\mathbf{x}$ 输入网络之前，我们首先使用一组具有[几何级数](@article_id:318894)增长频率的正弦和余弦函数将其映射到一个更高维的向量：
$$
\gamma(\mathbf{x}) = \left(\dots, \sin(2^k \pi \mathbf{x}), \cos(2^k \pi \mathbf{x}), \dots\right)
$$
这就像给了网络一套特制的尺子。它现在不仅看到坐标“5.3”，还能看到该坐标在许多不同频率的波上的投影。这使得网络能够轻松学习带有精细细节的函数。

然而，这里有一个陷阱，即**混叠**（**aliasing**）现象可以很好地说明这一点。如果场景包含的细节频率高于我们[位置编码](@article_id:639065)中的最高频率，网络就会感到困惑。它会“看到”一个冒名顶替者——一个恰好在网络训练的特[定点](@article_id:304105)上与高频信号匹配的低频模式[@problem_id:3136712]。这类似于电影中的马[车轮效应](@article_id:297428)，即快速旋转的车轮看起来像是在缓慢旋转甚至倒转。

对谱偏见的这种理解催生了“课程学习”（curriculum learning）策略。我们可以不一次性呈现所有频率，而是通过从低频[位置编码](@article_id:639065)开始训练网络来学习场景的粗略形状，然后逐渐引入更高频率来填充精细细节[@problem_id:3136713]。这就像一位艺术家在细致地添加皮肤纹理和眼中神采之前，先勾勒出肖像的大致轮廓。

### 用光绘画：体渲染的艺术

那么，我们有了一个可以告诉我们空间中任意点的颜色和密度的函数。如何将其转换成一幅二维图片呢？我们使用一种受物理学启发的称作**体渲染**（**volume rendering**）的技术。

对于我们[期望](@article_id:311378)图像中的每个像素，我们从相机向场景中投射一条虚拟光线。然后我们沿着这条光线“行进”，在沿途的许多点上对我们的[神经网络](@article_id:305336) $f_{\theta}$ 进行采样。在光线上的每个点 $t_i$，网络都给我们一个密度 $\sigma_i$ 和一个颜色 $\mathbf{c}_i$。

像素的最终颜色是其光线上所有点的颜色累积而成。但并非所有点都贡献均等。一个点的贡献取决于两件事：
1.  它发出多少光，这是其颜色 $\mathbf{c}_i$ 和密度 $\sigma_i$ 的乘积。
2.  这些光有多少真正到达了相机。

第二个因素至关重要。来自场景深处某点的光线可能会被其前方的物体阻挡。我们用一个称为**透射率**（**transmittance**）的值 $T_i$ 来量化这一点，它是光线从相机传播到点 $t_i$ 而不被吸收的概率。透射率在相机处为 $1$，并随着光线穿过场景中更密集的部分而减小。具体来说，$T_{i+1} = T_i \times \exp(-\sigma_i \delta_i)$，其中 $\delta_i = t_{i+1} - t_i$ 是相邻样本之间的距离。

该光线的最终颜色 $C$ 是所有采样点的加权和：
$$
C = \sum_i T_i \cdot (1 - \exp(-\sigma_i \delta_i)) \cdot \mathbf{c}_i
$$
项 $(1 - \exp(-\sigma_i \delta_i))$ 代表了在 $t_i$ 处小段的不透明度。这个公式完美地捕捉了光传输的物理过程：每个点贡献出它的颜色，并根据其从相机处可见的程度进行衰减。随着网络学会在何处放置密度，场景从一片“发光的雾”中浮现出来。

### 雕塑家的梯度：NeRF如何学习一个场景

NeRF最神奇的部分在于它如何学习。训练过程在概念上很简单：对于一组训练图像，我们使用上述方法渲染每个像素的颜色。然后我们计算误差——渲染颜色与照片中实际颜色之间的差异。神奇之处在于我们如何利用这个误差来更新网络的权重。我们使用反向传播，就像在任何其他[深度学习](@article_id:302462)模型中一样。

让我们思考一下梯度，这个告诉网络如何改变的信号。最终像素颜色 $C$ 相对于光线上某一点 $u$ 处密度 $\sigma$ 的[导数](@article_id:318324)是什么？答案揭示了学习过程中美妙的推拉动态[@problem_id:3136798]。梯度有两个相互竞争的部分：

$$
\frac{\partial C}{\partial \sigma}(u) = \underbrace{T(u)c(u)}_{\text{Local Emission}} - \underbrace{\int_{u}^{\infty} T(t)\sigma(t)c(t)\,dt}_{\text{Occlusion of Background}}
$$

第一项，$T(u)c(u)$，是正的。它告诉网络：“如果这个点 $u$ 从相机处可见（$T(u)>0$），那么增加它的密度将把更多它的颜色 $c(u)$ 添加到最终图像中。”这是“发出更多光”的信号。

第二项是负的。它代表了点 $u$ *之后*整个场景贡献的总光量。这一项告诉网络：“小心！增加点 $u$ 的密度会投下阴影，阻挡来自其后方所有物体的光线。”这是“阻挡更多光”的信号。

学习过程是在这两个信号引导下的一场宏大的平衡之舞。对于每张训练图像中的每条光线上的每个点，网络都会问：“为了让我的渲染图像更像真实的图像，空间中的这个点应该变得更不透明、更具辐射性，还是更透明？”通过调整密度场以解决来自不同视角的数百万条光线中的这种冲突，网络从最初均匀的雾中“雕塑”出三维场景。

### 从点到可信世界：平滑的力量

一个 NeRF 在有限的图像集上进行训练，却能从全新的相机位置生成逼真的照片级视图。它是如何实现如此好的泛化能力的？答案在于神经网络表示的内在平滑性。

一个[正则化](@article_id:300216)的 NeRF 与一种名为**[核密度估计](@article_id:346997)（KDE）**的经典统计方法之间存在着深刻而美妙的联系[@problem_id:3136776]。在 KDE 中，你通过在每个数据点上放置一个“核”（一个平滑的凸起，如高斯函数）并将它们相加，从而从一组离散的数据[点估计](@article_id:353588)出一个连续的概率密度。核的宽度，或其“带宽”，控制了最终估计的平滑度。

训练一个 NeRF，特别是使用惩罚大梯度的正则化时，类似于执行[核密度估计](@article_id:346997)。网络不仅仅学习一组不相连的点；它学习一个最能拟合观测数据的平滑、连续的场。NeRF 训练中的正则化强度扮演着与 KDE 中核带宽类似的角色。更强的[正则化](@article_id:300216)会鼓励一个更平滑的场，这有助于网络忽略噪声，并创建一个可信的、连续的表面来填补它所见的视图之间的空白。这就是为什么 NeRF 不仅仅是记忆；它们构建了一个连贯的世界模型。

### 并非所有 NeRF 生而平等

最初的 NeRF 架构基于一个大型的多层感知机（MLP），它开创了先河但速度很慢。较新的架构已经实现了令人难以置信的加速。其中一个关键创新是使用**哈希编码的特征网格**。

在纯 MLP 方法和基于网格的方法之间存在一个有趣的权衡[@problem_id:3136690]。哈希网格就像一个非常详细、局部化的记忆库。它擅长在训练数据密集的区域表示极其精细的细节。另一方面，纯 MLP 是一个更全局性的函数，当训练数据稀疏时，可能更擅长创建平滑、可信的插值。架构的选择取决于任务的具体需求，需要在速度、内存和不同数据密度下的泛化能力之间进行权衡。

### 现实的配方：捕捉完美的数据集

NeRF 的好坏取决于训练它的图像。该模型的原理为我们如何捕捉理想的数据集提供了直接而实用的指导。

首先，**相机基线**（camera baseline）——即相机位置之间的距離——至关重要。如果相机之间太近，视差效应太小，场景会显得扁平，使模型难以推断[三维几何](@article_id:355311)。如果它们相距太远，图像看起来差异太大，网络难以找到对应关系。重建误差通常遵循一条可预测的曲线，随着基线从零开始增加而改善，但最终会受到光照和[模型容量](@article_id:638671)等其他因素的限制而达到一个平台期[@problem_id:3136704]。

其次，相机镜头本身的物理特性很重要。任何真实的镜头都会引入一定程度的**离焦模糊**（**defocus blur**）。为了让 NeRF 学习到场景所能提供的最精细细节，来自相机的光学模糊必须小于模型能够表示的最小特征（这由[位置编码](@article_id:639065)中的最高频率设定）[@problemid:946350]。这在一个物理相机设置——控制[景深](@article_id:349268)的光圈或f值——与NeRF代码中的一个超参数之间建立了切实的联系。要捕捉一个具有大量精细细节的场景，你不仅需要一个高频的[位置编码](@article_id:639065)，还需要将相机设置为具有大景深，确保所有物体都清晰对焦。这种光学、信号处理和[深度学习](@article_id:302462)的美妙交集，概括了 NeRF 的精神：一个深深植根于我们如何看待世界的物理原理的模型。

