## 引言
在人工智能的许多突破性进展的核心，存在着一种与物理学基本原理——对称性——深刻而常被忽略的联系。虽然[神经网络](@article_id:305336)学习识别物体的过程看似纯粹由数据驱动的计算，但其效率和成功深深植根于这一概念。然而，标准模型仅能捕捉我们世界中存在的一小部分对称性，这导致了一个关键的知识鸿沟：我们如何构建能够将对称性理解为普适规则而非特例的模型？这种局限性使得它们在面对旋转等简单变换时变得数据饥渴且脆弱。

本文深入探讨了群论与[深度学习](@article_id:302462)的强大结合，这种结合为将对称性直接[嵌入](@article_id:311541)模型架构提供了一种形式化语言。通过将对称性视为一种需要强制执行的世界基本属性，而非一个有待学习的特征，我们可以创建出更鲁棒、更高效、更具洞察力的模型。在接下来的章节中，您将对这一[范式](@article_id:329204)获得全面的理解。我们将在“原理与机制”中探讨[等变性](@article_id:640964)和[群卷积](@article_id:639745)的理论基础，然后在“应用与跨学科联系”中见证它们在一系列领域产生的深远影响，从革新[计算机视觉](@article_id:298749)到开启人工智能与自然科学之间的新对话。

## 原理与机制

如果您曾惊叹于计算机在照片中识别出一只猫的能力，那么您已经见证了一个深刻的物理原理在起作用，尽管它被伪装在硅和代码之中。您可能会认为，这种魔力在于某个极其复杂的程序，它记住了所有可能的猫的样子。但事实既更简单，也远为优美。其中的秘诀在于**对称性**。

### 卷积的奥秘：一个关于对称性的故事

让我们玩个游戏。我给您看一张猫在左上角的图片，您学会了识别它。现在，我给您看一张新图片，里面是完全相同的猫，但它被移到了右下角。您需要重新学习这只“新”猫是什么吗？当然不需要！您的大脑明白一个关于世界的基本真理：一个物体的身份不会因为其位置的改变而改变。猫之所以为猫的法则是**在平移下保持不变的**。

一个标准的**[卷积神经网络](@article_id:357845)（CNN）**——现代[计算机视觉](@article_id:298749)的主力军——正是建立在这一思想之上。其核心操作，即卷积，涉及将一个小型滤波器——一个微小的“模式检测器”——在整个图像上滑动。这个滤波器可能会学会对胡须、耳朵或眼睛的模式做出响应。关键在于，它在每个位置都使用*完全相同的滤波器*。这被称为**[权重共享](@article_id:638181)**。网络不必学习一个“左上角胡须检测器”和一个单独的“右下角胡须检测器”。它学习的是一个单一的、通用的“胡须检测器”。

这种特性，即输入图像的平移会导致滤波器输出相应平移的特性，被称为**[等变性](@article_id:640964)**。如果 $T$ 是平移图像的操作，而 $\Phi$ 是卷积层，那么[平移等变性](@article_id:640635)意味着 $T(\Phi(\text{image})) = \Phi(T(\text{image}))$。先应用滤波器再平移结果，与先平移图像再应用滤波器是完全相同的。这个简单而强大的思想，正是CNN在处理图像方面如此出色和高效的原因。它们的架构中融入了一条物理原理——[平移对称性](@article_id:350762)。

### 从一种对称性到所有对称性：群的语言

但世界上的对称性远不止平移。一个物体可以被旋转、反射或缩放。一个标准的CNN，尽管在处理平移方面非常巧妙，却对其他这些对称性视而不见。它会把一只旋转过的猫当作一个需要从头开始学习的全新对象。这是极其低效的，就像一个物理学家坚持认为澳大利亚的引力定律不同，因为那里的一切都是“上下颠倒”的。

为了构建更智能的模型，我们需要一种语言来谈论*所有*可能的对称性。这种语言就是**群论**。一个**群**只是一个具有一些良好性质的变换数学集合，例如能够撤销每个变换（逆元）以及组合它们。所有平移的集合构成一个群。所有旋转的集合，或一个正方形的所有旋转和反射（**[二面体群](@article_id:306236)** $D_4$）也同样构成群。

一旦我们有了这种语言，我们就可以推广卷积的概念。对于任何对称群 $G$，我们可以定义一个**[群卷积](@article_id:639745)**。如果我们有一个输入信号 $f$ 和一个滤波器 $\psi$，它们的[群卷积](@article_id:639745)会创建一个存在于群本身上的新函数，定义如下：

$$
(f * \psi)(g) = \sum_{h \in G} f(h) \psi(h^{-1} g)
$$

不要被这些符号吓到。其直觉与之前相同：我们正在测量输入信号与一个经过我们[对称群](@article_id:306504)中每个元素 $g$ 变换后的滤波器的相似度 [@problem_id:3126226]。我们不只是将滤波器向左和向右滑动；我们现在是通过所有可能的旋转、反射或我们定义的任何其他对称性来“滑动”它。如果输入是网格上的图像，且群包含旋转（如六边形网格上的 $60^\circ$ 旋转的 $C_6$ 群），则[群卷积](@article_id:639745)产生的输出不仅会告诉我们*存在什么*特征，还会告诉我们该特征处于*什么朝向* [@problem_id:3133441]。

### 用对称性构建：[权重共享](@article_id:638181)的力量

[群卷积](@article_id:639745)这个想法不仅仅是一个抽象的好奇心；它是一个构建效率高得多的学习机器的蓝图。还记得标准CNN如何在所有空间位置上共享其滤波器权重吗？一个**群[卷积[神经网](@article_id:357845)络](@article_id:305336)（[G-CNN](@article_id:642289)）**将这一思想推向其逻辑结论：它在群中的所有变换上共享权重。

想象一下，我们想检测一个可以出现在 $n$ 个不同旋转角度的条形图案。一个标准的CNN，由于缺乏对旋转的内置理解，将被迫学习 $n$ 个独立的滤波器，每个方向一个。相比之下，一个[G-CNN](@article_id:642289)只需要学习*一个*基础滤波器。群结构本身提供了免费旋转该单一滤波器以获得所有其他滤波器的“指令” [@problem_id:3126226]。

这样做的实际效果是巨大的。假设我们的条形图案是对称的，在 $n$ 次旋转下只有 $n/h$ 种不同的外观（其中 $h$ 是该图案[稳定子群](@article_id:297667)的大小，即那些使其看起来不变的旋转集合）。为了学会检测这个图案，一个标准CNN需要与不同外观数量成正比的训练样本，即 $n/h$。而[G-CNN](@article_id:642289)通过只学习一个滤波器，需要的样本数量仅与 $1$ 成正比。所需数据量的比率达到了惊人的 $n/h$ [@problem_id:3133438]。通过将世界的对称性硬编码到我们的模型中，我们给了它一个巨大的领先优势。它不需要浪费时间和数据去学习我们作为物理学家和世界观察者早已知晓为真的规则。

### 一个充满对称性的宇宙

[等变性](@article_id:640964)原理的应用远不止于二维网格上的旋转图像。对称性是宇宙的一个基本组织原则，我们随处可见。

*   **分子的物理学：** 支配分子相互作用的物理定律在三维空间的平移和旋转（**欧几里得群** $E(3)$）下是不变的。因此，一个预测分子能量或力的机器学习模型也应该尊重这些对称性。为此，我们可以设计**E(3)[等变神经网络](@article_id:297888)**。这些网络不只操作数字；它们操作具有几何意义的特征——**标量**（如能量，不随旋转而改变）、**向量**（如力，必须随分子一起旋转）以及高阶**[张量](@article_id:321604)**。实现这一目标所需的数学涉及来自物理学的美妙工具，如**[球谐函数](@article_id:357279)**和**克莱布施-戈登系数 (Clebsch-Gordan coefficients)**，以确保当我们组合特征时，它们的旋转属性得到正确保持 [@problem_id:2648604]。一个显著的推论是，如果你构建一个模型来预测总能量（一个[旋转不变量](@article_id:349651)），那么作为能量负梯度的每个原子上的力，将自动保证是正确的等变向量！

*   **图的结构：** 一个社交网络或分子图的“对称性”是什么？它是任何保持连接网络不变的节点[置换](@article_id:296886)。这组对称性构成了图的**[自同构群](@article_id:304728)**。一个[图神经网络](@article_id:297304)（GNN）本质上是一个等变模型。标准的“[消息传递](@article_id:340415)”机制，即每个节点使用一套共享规则（可学习的权重）从其邻居那里聚合信息，是[图对称性](@article_id:336074)上的一种[群卷积](@article_id:639745)形式。它确保给定节点的输出取决于其局部邻域结构，而不是我们分配给它的任意标签。如果我们重新标记节点，每个节点的输出特征将以完全相同的方式被[置换](@article_id:296886) [@problem_id:3133466]。与CNN一样，即使我们堆叠层并应用逐点非线性[激活函数](@article_id:302225)，这种[等变性](@article_id:640964)也得以保持。

*   **传感器的[可交换性](@article_id:327021)：** 想象一下你正在构建一个系统，它融合来自 $k$ 个相同、可互换的传感器的数据。如果这些传感器是真正相同的，那么我们将哪个数据流标记为“传感器1”，哪个标记为“传感器2”应该无关紧要。其底层物理学在传感器的任何[置换](@article_id:296886)（**对称群** $S_k$）下都是对称的。我们可以通过设计层来强制实现这种对称性，其中处理每个传感器通道的权重是捆绑在一起的。这是另一种形式的[权重共享](@article_id:638181)，可以在数据稀缺时极大地提高[样本效率](@article_id:641792)。但这也揭示了一个关键的教训：你必须选择正确的对称性。如果一个传感器是摄像头，另一个是雷达，它们就*不是*可互换的。强迫模型将它们视为相同，将是强制执行一种错误的对称性，从而削弱其学习正确函数的能力 [@problem_id:3133490]。艺术在于正确识别手头问题的真实对称性。

### 更精细的要点：超越基础

构建真正等变的模型需要对对称性的一些微妙之处有更深的理解。

*   **使输出与任务相匹配：** [等变性](@article_id:640964)不仅仅是输入数据的属性；它必须与任务本身保持一致。考虑根据物体的**手性**（它们的“利手性”，就像我们的左手和右手）对物体进行分类。旋转不会改变物体的手性，但反射会——它将左手变成右手。因此，为此任务设计的网络不能是完全不变的。它的输出必须在旋转下*不变*，但在反射下必须*反转*。这可以通过设计网络的最后一层来实现，使其输出的不仅仅是一个数字，而是一个其在[群作用](@article_id:332514)下的变换规则与标签的变换规则相匹配的特征。例如，它可以输出一个单一值 $y$，该值被设计为在输入被反射时翻转其符号（$y \to -y$）[@problem_id:3133472]。输出的表示必须与问题解决方案的表示相匹配。

*   **连接连续与离散世界：** 物理定律通常是连续的——一个物体可以被旋转任何角度。然而，我们的计算机生活在离散的像素网格上。这产生了一种有趣的[张力](@article_id:357470)。我们如何在一个离散格子上最好地逼近一个连续的对称性？事实证明，并非所有格子都是生而平等的。一个**六边形[晶格](@article_id:300090)**在旋转方面比方形[晶格](@article_id:300090)“更对称”。其离散旋转群 $C_6$（$60^\circ$ 的倍数旋转）比方形[晶格](@article_id:300090)的 $C_4$ 群（$90^\circ$ 的倍数）为连续[旋转群](@article_id:383013) $SO(2)$ 提供了更精细的采样。这意味着对任意旋转的近似更准确，并且更容易保持[等变性](@article_id:640964) [@problem_id:3133441]。

当我们考虑更复杂的连续群，如包含缩放的**相似群** $SIM(2)$ 时，挑战会成倍增加。缩放维度是非紧的（一个物体可以任意大），这使得不进行[离散化](@article_id:305437)就无法处理。对尺度进行朴素的[离散化](@article_id:305437)可能导致错误并破坏[等变性](@article_id:640964)。一个更优雅的解决方案是对尺度进行对数[重参数化](@article_id:355381)（$u = \ln s$）。这将尺度的[乘法群](@article_id:316383)转换为一个[加法群](@article_id:312215)，从而可以更自然地应用标准的卷积机制 [@problem_id:3133453]。

从CNN的简单优雅到E(3)[等变网络](@article_id:304312)的复杂机制，其原理保持不变。通过理解问题中固有的对称性并将其[嵌入](@article_id:311541)到我们模型的架构中，我们超越了单纯的[模式匹配](@article_id:298439)。我们开始构建对世界有更深刻、更鲁棒、更高效理解的模型，反映了支配宇宙本身的相同原理。

