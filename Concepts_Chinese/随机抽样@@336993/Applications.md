## 应用与[交叉](@article_id:315017)学科联系

我们花了一些时间学习[随机抽样](@article_id:354218)的形式化数学——伯努利试验、[二项分布](@article_id:301623)和概率的世界。这可能感觉有些抽象，就像一个用硬币和骰子玩的游戏。但现在，我们将看到真正有趣的部分从何开始。我们会发现，这个简单的想法，即“抽签”的行为，是所有现代科学中最强大、最普遍、最深刻的概念之一。它是我们诚实地观察世界的基石，是生物进化的引擎，是我们最先进的解读生命之书技术背后的原理，甚至是检验我们最智能机器的基准。

让我们踏上一段旅程，看看这个单一的想法如何将看似毫无关联的领域——从我们脚下的泥土到我们大脑错综复杂的布线，再到分子的微观舞蹈——编织成一条统一的线索。

### 观察者的困境：如何看世界而不自欺欺人

想象一下，你想知道某个巨大而复杂事物的平均特性——一个受污染场地的铅浓度、一个星系中的恒星数量，或者人脑中的[神经元](@article_id:324093)数量。你不可能测量每一个部分。你唯一的选择是进行抽样。从这个样本中做出有效推论的整个科学都建立在[随机抽样](@article_id:354218)的原则之上。它是我们保持学术诚信的主要工具。

但“[随机抽样](@article_id:354218)”并不像听起来那么简单。假设你是一位环境科学家，任务是评估一个受污染的场地。你可以从不同地点取土，将它们混合成“复合”样本进行分析。或者，你可以将场地划分为不同区域，并从每个区域中抽取独立的随机样本，这是一种“随机分层”方法。哪种更好？答案在于你结果的方差。不同的[抽样策略](@article_id:367605)，即使两者都是“随机的”，也可能产生截然不同的精确度。通过使用统计检验，科学家可以确定一个更费力的策略是否在方差上提供了统计上显著的减少，从而证明额外的成本和努力是值得的 [@problem_id:1432674]。我们选择*如何*抽样决定了我们对结论的信心。

当我们从一个平坦的场地转向大脑这个三维、密集堆积的宇宙时，这个挑战变得异常复杂。神经科学的一个核心信条，即[神经元学说](@article_id:314530)，指出[神经元](@article_id:324093)是离散的、独立的细胞。你如何才能证明这一点？你需要数清它们。但是，当你只能观察二维切片时，你如何计算三维体积中的物体数量呢？

如果你只是计算你在一个切片中看到的每一个细胞轮廓，你将大错特错。较大的细胞比小细胞更有可能被切到，从而使你的计数产生偏差。一个恰好位于切片边缘的细胞可能在你这个切片和下一个切片中都被计数。组织本身在制备时会收缩和变形。一种天真的方法注定会失败。

解决方案是在三维空间中对随机抽样进行优美而严谨的应用，称为体视学（stereology）。使用像“光学分割器”（optical fractionator）这样的方案，神经科学家采用系统均匀随机抽样来选择位置，然后使用一个巧妙的三维计数探针（一个“[光学切片](@article_id:323972)器”，optical disector），该探针设有“保护区”和包含/排除线。这种方法被巧妙地设计成不受[细胞大小](@article_id:299527)和组织收缩偏差的影响。它能够对[神经元](@article_id:324093)的总数进行无偏估计 [@problem_id:2764730]。

最深刻的是，这种方法还提供了一种计算[抽样误差](@article_id:361980)或“误差系数”（Coefficient of Error, $CE$）的方法。这不仅仅是一个技术细节；它是进行科学发现的关键。如果你看到一簇[神经元](@article_id:324093)，它是大脑回路中一个真实的“模块”，还是仅仅由你抽样的随机性造成的幻象？你无法回答这个问题，除非你能够将观察到的变异与仅由你的抽样过程所预期的变异进行比较。只有当生物信号远强于[测量噪声](@article_id:338931)时，你才能声称发现了真实的东西。[随机抽样](@article_id:354218)，如果做得正确，不仅给你一个估计值；它还精确地告诉你这个估计值有多好。

### 宇宙彩票：当自然本身掷骰子时

到目前为止，我们已经将抽样讨论为科学家用来观察世界的工具。但在科学发现最优雅的转折之一中，我们发现自然本身也使用[随机抽样](@article_id:354218)作为变革的基本机制。这个过程被称为**遗传漂变（genetic drift）**。

思考一下[抗生素耐药性](@article_id:307894)的演变。在一个庞大的细[菌群](@article_id:349482)体中，少数耐药突变体可能以非常低的频率存在，比如1%。现在，想象一下这种感染被传播给一个新的宿主。并非所有细菌都能完成这次旅程。一个严重的“[瓶颈效应](@article_id:304134)”发生了，其中只有原始群体的一个微小的、随机的样本——也许只有几百个细胞——存活下来以建立新的感染。耐药菌株会在这其中吗？

这是一个经典的二项抽样问题。每个传播的$N_b$个细胞都是一次独立的试验，具有$p$的概率是耐药的。*至少一个*耐药细胞通过的概率是$1 - (1-p)^{N_b}$。当$p = 0.01$且瓶颈大小为$N_b = 100$个细胞时，完全失去耐药菌株的概率是$(0.99)^{100}$，大约是$0.37$。这意味着，新感染中含有耐药变异的几率高达63%，尽管它在原始群体中很罕见 [@problem_id:2776103]。纯粹靠抽签的运气，一个罕见的性状频率可以增加，或者相反，被完全消除。

同样的原理在我们自己的身体内部运作，对遗传性疾病产生巨大影响。我们的细胞含有线粒体，这是一种拥有自己DNA（mtDNA）的微型能量工厂。一个人可以同时拥有健康和突变的mtDNA，这种状态称为“异质性”（heteroplasmy）。在卵细胞形成（[卵子发生](@article_id:312559)，oogenesis）期间，发生了一个严重的[瓶颈效应](@article_id:304134)，只有一小部分有效数量的mtDNA分子（$N_e$）从母亲的大量mtDNA池中被抽样出来，以填充卵细胞。

这意味着，一个拥有低水平、无害的突变[mtDNA](@article_id:327628)的母亲，可能纯粹由于偶然，产生一个具有非常高、致病水平突变[mtDNA](@article_id:327628)的卵子。后代中异质性的分布可以被精确地建模为一个二项抽样过程 [@problem_id:2658788]。在这个线粒体瓶颈期间发生的[随机抽样](@article_id:354218)是[线粒体疾病](@article_id:332930)具有如此复杂和不可预测[遗传模式](@article_id:369397)的主要原因。在进化和发育中，自然都在掷骰子，而随机抽样的数学让我们能够理解其后果。

### 解码蓝图：抽样生命物质

在过去的几十年里，生物学被那些能让我们以惊人规模读取遗传密码的技术所革命。这些技术的核心都是复杂的抽样机器。

想象一下，你是一位[微生物生态学](@article_id:323869)家，手头有一个来自深海的样本。你想知道那里生活着哪些微生物。你不能在实验室里培养它们。相反，你提取所有的DNA，将其切碎，然后对片段进行测序。这被称为宏基因组学（metagenomics）。一个关键问题出现了：你需要进行多少测序才能确信检测到一个可能以例如0.1%丰度存在的稀有物种？

这再次是一个关于[随机抽样](@article_id:354218)的问题。每个测序“读段”都是一次试验。如果该物种的相对丰度为$p=0.001$，那么在一次读段中错过它的概率是$1-p$。在$n$次读段中都错过它的概率是$(1-p)^n$。如果我们希望至少检测到它一次的概率为95%，我们必须解这个不等式$1 - (1-p)^n \ge 0.95$。这个植根于[第一性原理](@article_id:382249)的简单计算，让科学家能够确定达到其实验目标所需的[测序深度](@article_id:357491)，从而界定了他们观察能力的极限 [@problem_id:2499647]。

抽样的行为甚至有更微妙的后果。在[RNA测序](@article_id:357091)（RNA-seq）中，我们通过计算每个基因存在的[RNA转录](@article_id:361745)本数量来测量基因的活性。该技术的工作原理是捕获这些RNA分子，将它们分解成片段，然后[随机抽样](@article_id:354218)这些片段进行测序。这里有一个为粗心者设下的陷阱。一个较长的基因，仅仅因为其长度，为片段化和抽样过程提供了一个更大的“目标”。即使两个基因以完全相同的拷贝数存在，较长的基因平均也会产生更多的测序读段。

如果我们不考虑这一点，我们将系统性地、错误地得出结论，认为较长的基因总是更活跃。这个过程的数学模型表明，来自一个基因的预期读段数与其真实的分子丰度和其[有效长度](@article_id:363629)成正比 [@problem_id:2848905]。这个基本见解，源于将实验视为一个[随机抽样](@article_id:354218)过程的观点，是所有现代RNA-seq分析都包含一个校正基因长度的[归一化](@article_id:310343)步骤的原因。

这种逻辑的美妙之处在于其普适性。我们可以想象一个假设的外星生命形式，它使用不同的化学物质，比如[肽核酸](@article_id:376581)（Peptide Nucleic Acid, PNA）。即使在这个虚构的世界里，如果我们想用类似的技术研究其生物学，同样的原则也适用。我们仍然需要找到一个普遍保守的基因作为[系统发育](@article_id:298241)“标记”，我们的“鸟枪法”测序覆盖度仍然受[随机抽样](@article_id:354218)数学的支配 [@problem_id:2405467]。这个逻辑超越了特定的分子基质。

随着我们变得越来越复杂，我们的模型也随之演进。测序实验计数数据的最简单模型是泊松分布（Poisson distribution），它源于随机、独立的事件。[泊松分布](@article_id:308183)的一个关键特性是其均值等于其方差。然而，当我们查看真实数据时，我们经常发现方差远大于均值——这种现象被称为“过度离散”（overdispersion）。为什么？因为我们的简单模型做出了一个隐藏的假设：在我们抽样的每个细胞中，潜在的[转录](@article_id:361745)速率是相同的。在一个复杂的、发育中的组织中，这绝非事实。不同细胞有不同的表达水平。额外的方差来自这种真实的生物异质性。

解决方案是建立一个更好的模型。我们可以想象每个位置的计数是一个泊松[随机变量](@article_id:324024)，但其[平均速率](@article_id:307515)本身是从另一个分布（如伽马分布，Gamma distribution）中抽取的[随机变量](@article_id:324024)。这个结合了两层随机性的[层次模型](@article_id:338645)，产生了能够处理[过度离散](@article_id:327455)的负二项分布（Negative Binomial distribution）。例如，如果我们观察到一个基因的[样本均值](@article_id:323186)为$4.2$，样本方差为$4.4$，泊松模型似乎没问题。但如果另一个基因具有相同的均值$4.2$，但方差为$18.5$，这清楚地表明简单的泊松模型是错误的，需要使用更复杂的、考虑了潜在生物学变异的负[二项模型](@article_id:338727) [@problem_id:2673451]。从泊松到负二项的旅程是一个完美的例子，说明我们如何通过构建更真实的嵌套[随机过程](@article_id:333307)模型来完善我们的理解，而这一切都始于抽样这个基本行为。

### 智能抽样者：超越盲目偶然

均匀随机抽样总是我们能做的最好的吗？不总是。我们旅程的最后一站将我们带到前沿，在那里我们学习如何*智能地*抽样。

考虑在一个复杂的网络或图上定义的信号进行抽样的问题。信号可以分解为基本模式，即图拉普拉斯算子（graph Laplacian）的[特征向量](@article_id:312227)（eigenvectors）。假设我们想重建一个仅由前几个模式构成的信号。我们应该把传感器放在哪里？如果这些模式是“非相干的”——均匀地分布在整个网络上——那么随机放置传感器效果非常好 [@problem_id:2903961]。这就是[压缩感知](@article_id:376711)（compressed sensing）魔力发生的范畴。

但如果模式是“相干的”——高度局部化并集中在网络的一小部分呢？在这种情况下，随机抽样是一个糟糕的策略。你很可能会完全错过重要的点。在这里，一种*确定性*的策略，根据对这些模式的了解仔细选择传感器位置，则要优越得多。这教给我们一个深刻的教训：[随机抽样](@article_id:354218)的有效性取决于抽样者与被抽样事物结构之间的相互作用。当我们无知时，[随机抽样](@article_id:354218)是我们最好、最诚实的选择。当我们拥有知识时，我们可以利用它来做得更好。

这一思想在合成生物学（synthetic biology）领域达到了顶峰，科学家们正试图从一个组合上极其庞大的可能性库中设计新的蛋白质或生物回路。测试每一个是不可能的。你的预算是，比如说，几千次实验。你应该从哪里寻找？

一种方法是均匀随机抽样：只需从库中随机挑选序列，希望自己能走运。找到至少一个“好”序列的概率由我们熟悉的公式给出，$1 - (1-\alpha)^n$，其中$\alpha$是好序列的（微小）比例。

但我们可以更聪明。我们可以使用机器学习——例如，一个深度神经网络——作为“向导”。我们首先用一些初始数据训练模型，以学习序列空间的“地图”。然后，模型预测库中哪些未探索的区域最有可能包含高性能的序列。我们的自适应[抽样策略](@article_id:367605)就是优先从这些有希望的区域进行抽样。使用[贝叶斯法则](@article_id:338863)（Bayes' rule），我们可以精确地计算出单次抽样成功的新概率。这个概率不再是$\alpha$，而是一个依赖于我们机器学习模型灵敏度和特异性的高得多的值。这种“智能”[抽样策略](@article_id:367605)可以显著提高效率，在相同的实验预算下，极大地增加发现的概率 [@problem_id:2749115]。这种方法就像一种复杂的拒绝抽样（rejection sampling） [@problem_id:832400]，我们不是从[均匀分布](@article_id:325445)中提出样本然后拒绝大部分，而是构建一个已经塑造成我们[期望](@article_id:311378)的目标样子的[提议分布](@article_id:305240)，从而导致更高的[接受率](@article_id:640975)。

### 结论

我们的旅程结束了。我们从简单的抽签行为开始，发现它是一个具有非凡深度和广度的概念。它是逻辑学家进行诚实观察的工具，在[土壤科学](@article_id:367893)和定量[神经解剖学](@article_id:311052)等不同领域保护我们免受偏差的影响。它是[遗传漂变](@article_id:306018)的自然引擎，通过机会的数学塑造着进化的进程和疾病的遗传。它是我们最先进分子技术核心的无形过程，迫使我们巧妙地校正其偏差并为其效应建模。最后，它作为我们衡量最先进、最智能、最具适应性的搜索策略的基准。理解[随机抽样](@article_id:354218)，就是理解测量的本质，理解偶然性在宇宙中的作用，以及在我们已知和我们试图发现的事物之间持续而迷人的对话。