## 引言
每一项计算任务都是在数学的无限精度与计算机的有限语言之间进行翻译的行为。这种翻译并不完美，会产生一种微妙的差异，即数值误差——一个萦绕在所有计算中的“机器中的幽灵”。虽然这些误差通常微乎其微，但它们并非总是无害的；它们会累积、传播，在某些情况下，甚至会使复杂的模拟或分析结果完全失效。因此，对于任何依赖计算机解决科学、工程及其他领域问题的人来说，理解这个数字幽灵的本质至关重要。

本文对数值误差进行了全面的探讨。首先，“原理与机制”一章将解构误差的起源，审视[浮点数](@article_id:352415)的机制、[舍入误差与截断误差](@article_id:640303)的关键区别，以及[灾难性抵消](@article_id:297894)和[算法不稳定性](@article_id:342590)等危险现象。在这一基础理解之上，“应用与跨学科联系”一章将深入现实世界，揭示这些抽象概念如何在不同领域中显现——从在物理模拟中产生幽灵力、导致音频滤波器不稳定，到在金融模型中制造混乱，甚至影响[优化算法](@article_id:308254)的结果。

## 原理与机制

每当我们要求计算机进行算术运算时，我们都是在要求它进行一次小小的翻译。我们说的是实数的语言——一种无限精度的语言，其中 $\pi$ 有无穷无尽的小数位，任意两个数之间的空间都可以无限分割。然而，计算机说的是另一种语言：有限的、二进制的、浮点数的语言。这是一种绝妙但有限的方言。在这种翻译中，总会有所损失。这种损失，即一个数的柏拉图式理想与其在机器内部的影子之间的微妙差异，正是所有**数值误差**的起源。它是机器中的幽灵，我们的任务是了解它的习性，这样它就不会困扰我们的计算。

### 数字幽灵及其度量方法

计算机通常以一种称为**[浮点数](@article_id:352415)**的格式存储数字，这本质上是二进制的[科学记数法](@article_id:300524)。一个数由一个符号、一个称为**[尾数](@article_id:355616)**的[小数部分](@article_id:338724)和一个指数来表示。例如，在常见的 [IEEE 754](@article_id:299356) [双精度](@article_id:641220)标准中，[尾数](@article_id:355616)包含大约 52 个二进制位，或者说大约 15 到 17 位的十进制精度。这个位数是有限的。你的计算器可能会说 $\pi \approx 3.141592653589793$，但它就到此为止了。无限序列的其余部分都消失了，被截断了。

可用于[尾数](@article_id:355616)的比特数是我们能获得的精度的根本预算。在设计硬件时，比如需要执行快速傅里叶变换（FFT）的[数字信号处理](@article_id:327367)器，工程师必须做出一个关键选择：多少比特才足够？使用更多的比特会使计算更精确，但也会在功耗和硅片成本上更加昂贵。正如一项分析所示，输出的质量，以信噪比来衡量，直接取决于[尾数](@article_id:355616)的比特数 [@problem_id:1717749]。这是第一条原则：精度是有限的资源。

因此，当一次计算完成时，我们有一个真实的、理想的值，我们称之为 $p$，还有一个从机器得到的计算值 $p^*$。我们如何衡量这个差异呢？有两种常用的标尺。

第一种是**绝对误差**，$E_a = |p - p^*|$。这是直接的差值。如果到月球的真实距离是 384,400 公里，而你的程序计算出 384,401 公里，那么绝对误差就是 1 公里。

第二种是**相对误差**，$E_r = \frac{|p - p^*|}{|p|}$。它衡量的是误差占真实值的比例。在月球的例子中，相对误差是 $1/384400 \approx 2.6 \times 10^{-6}$，或者大约 0.00026%。

现在，你可能会认为小的相对误差总是好的，而大的相对误差总是坏的。但事情要微妙得多。考虑一位工程师正在为一个微小的、对称的微加热器建模。在理想世界中，热流将是完全平衡的，净残余功率将恰好为零。实际上，由于微小的瑕疵，真实的残余功率是一个极小的值 $p = 1 \times 10^{-9}$ 瓦特。[计算机模拟](@article_id:306827)在处理其有限精度时，可能会计算出 $p^* = 3 \times 10^{-7}$ 瓦。

让我们看看误差。[绝对误差](@article_id:299802)是 $|10^{-9} - 3 \times 10^{-7}| \approx 2.99 \times 10^{-7}$ 瓦。这是一个非常小的功率，远不足以影响设备的性能。从物理学的角度来看，这个结果非常出色。但[相对误差](@article_id:307953)呢？它是 $(2.99 \times 10^{-7}) / (1 \times 10^{-9}) = 299$。这是一个 29,900% 的误差！按照这个标准，这是一次灾难性的失败。那么发生了什么呢？[相对误差](@article_id:307953)之所以爆炸，是因为我们试图测量的真实值本身就极其接近于零。用一个接近零的数做除法，可以使任何微小、无足轻重的绝对误差看起来像一场灾难。这教会了我们一个至关重要的教训：选择正确的误差度量标准是一门艺术。你必须问这个数字在现实世界中意味着什么 [@problem_id:2370359]。

### 不精确性的两个方面：[截断误差与舍入误差](@article_id:343437)

数值误差并非都来自同一源头。它们主要有两个父系：截断和舍入。

**截断误差**是近似的误差。这是我们作为数学家和科学家有意做出的选择。我们经常用一个更简单的、有限的过程来代替一个无限复杂的过程。当我们用泰勒级数的前几项来近似一个函数时，我们就在截断这个级数。当我们用[有限差分](@article_id:347142)，如 $\frac{f(x+h) - f(x)}{h}$，来近似[导数](@article_id:318324) $f'(x)$ 时，我们就在截断 $h \to 0$ 的[极限过程](@article_id:339451)。这不是计算机的错；这是[算法](@article_id:331821)的一个特性。

另一方面，**舍入误差**是计算机的错。它是在计算的每一步引入的误差，因为机器只能存储有限位数的数字。任何乘法或加法之后，结果都必须被舍入以重新适应浮点格式。这是每一步的一个微小推挤。

这两种类型的误差常常处于一种奇妙的[张力](@article_id:357470)状态，一场拉锯战，这正是数值分析的核心所在。要看清这一点，没有比计算[导数](@article_id:318324)的任务更好的地方了 [@problem_id:2173571]。

假设我们使用更对称的[中心差分公式](@article_id:299899)：
$$
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}
$$
这里的[截断误差](@article_id:301392)比更简单的公式有所改进；从[泰勒级数](@article_id:307569)可以证明，它与 $h^2$ 成正比。所以，为了使我们的数学近似更好，我们应该让步长 $h$ 尽可能小。

但现在，[舍入误差](@article_id:352329)这个怪物苏醒了。当 $h$ 变得非常小时，$x+h$ 和 $x-h$ 会变得非常接近。这意味着 $f(x+h)$ 和 $f(x-h)$ 也可能非常接近。我们正在减去两个几乎相等的数——一个我们稍后将探讨的危险游戏。这个减法放大了它们微小舍入误差的重要性。更糟糕的是，我们然后用一个非常小的数 $2h$ 来除这个充满噪声的结果。用一个小数做除法会放[大分子](@article_id:310961)中的任何误差。所以，[舍入误差](@article_id:352329)的贡献实际上随着 $h$ 变小而*变大*，其行为类似于 $\frac{1}{h}$。

总误差 $E(h)$ 是这两个相互对抗效应的总和：
$$
E(h) \approx C_1 h^2 + \frac{C_2}{h}
$$
其中 $C_1$ 与函数的三阶[导数](@article_id:318324)有关，而 $C_2$ 取决于函数的大小和机器的精度。看看这个优美的表达式！它讲述了一个完整的故事。如果你选择的 $h$ 太大，你的数学公式就太粗糙了。如果你选择的 $h$ 太小，你就会被计算机的[舍入噪声](@article_id:380884)淹没。一定存在一个最佳点，一个完美的折衷。通过使用微积分来最小化这个总误差函数，我们可以找到最佳步长 $h_{opt}$。这个最佳的 $h$ [@problem_id:2173571] 不是零；它是一个有限值，完美地平衡了我们[算法](@article_id:331821)的误差和我们机器的误差。

### 灾难性抵消：消失的数字的艺术

让我们更仔细地看看那个危险的游戏：减去两个几乎相等的数。这种现象，称为**灾难性抵消**，是良好精度突然丧失的最常见方式之一。

想象一下你的计算器有 8 位精度。你想计算 $1.2345678 - 1.2345670$。确切的答案是 $0.0000008$。但看看发生了什么。我们开始时有两个数，每个数都有 8 位[有效数字](@article_id:304519)。我们的结果只有*一位*[有效数字](@article_id:304519)。原始数字的前七位相互抵消了，结果主要由原始数字中曾经最不重要、最不确定的部分主导。我们拿了两个精确的信息片段，通过相减，制造出了垃圾。

这正是在我们的[导数](@article_id:318324)公式的分子 $f(x+h) - f(x-h)$ 中，当 $h \to 0$ 时发生的情况。一个极好的实际例子来自设计一个[光学滤波](@article_id:345051)器，我们需要找到两个函数 $f(x) = \cosh(x)$ 和 $g(x) = 1 + \frac{x^2}{2} + \epsilon$ 的交点 [@problem_id:2158301]。这等同于找到 $h(x) = f(x) - g(x)$ 的根。使用 $\cosh(x)$ 的[泰勒级数](@article_id:307569)，即 $1 + \frac{x^2}{2} + \frac{x^4}{24} + \dots$，我们看到对于小的 $x$，函数近似为 $h(x) \approx \frac{x^4}{24} - \epsilon$。然而，计算机不使用泰勒级数；它计算 $\cosh(x)$ 和 $1 + \frac{x^2}{2} + \epsilon$ 然后相减。在根所在的小 $x$ 值处，这两个量几乎相同。减法消灭了前面的数字，在真根周围产生了一片数值迷雾，一个“不确定区域”，其中计算噪声比函数的实际值还要大。

我们如何对抗这种情况？有时，一个聪明的计划改变就足够了。考虑对[交错调和级数](@article_id:301407)求和的任务，$S_N = \sum_{k=1}^{N} \frac{(-1)^k}{k} = -1 + \frac{1}{2} - \frac{1}{3} + \dots$ [@problem_id:2393710]。我们可以从前往后求和（从 $k=1$ 到 $N$）或从后往前求和（从 $k=N$ 到 1）。这有关系吗？在纯数学的世界里，没有。在浮点运算的世界里，关系重大。

当我们从前往后求和时，我们从 $-1$ 开始，然后加 $0.5$ 得到 $-0.5$，再减去 $0.333\dots$ 得到 $-0.833\dots$。运行总和很快接近其最终值，约 $-\ln(2) \approx -0.693$。在许多项之后，我们是在向一个大得多的运行总和中添加非常小的数（如 $\frac{1}{N}$）。这是一种变相的抵消；小数被加到一个大数上，其最低有效位在舍入过程中丢失了。但如果我们从后往前求和，我们首先将最小的项加在一起：$\frac{(-1)^N}{N} + \frac{(-1)^{N-1}}{N-1} + \dots$。运行总和增长非常缓慢，所以我们总是在加[数量级](@article_id:332848)相当的数。这最大限度地减少了精度的损失。这就像称量一堆金粉和一个大金条：如果你先称量金粉堆，然后再加大金条，你会得到更准确的总重量。仅仅改变运算顺序这个简单的动作，就可以显著提高结果的准确性。

### 多米诺效应：[误差传播](@article_id:306993)与[不稳定算法](@article_id:343101)

误差很少是一个单一、孤立的事件。它更像是长链中倒下的第一张多米诺骨牌。**[误差传播](@article_id:306993)**是研究[算法](@article_id:331821)一步中引入的误差如何影响所有后续步骤的学科。

考虑求解一个描述系统如何随时间变化的[微分方程](@article_id:327891) [@problem_id:2152535]。[数值方法](@article_id:300571)通过采取小的时间步长来解决这个问题。在每一步，由于截断，[算法](@article_id:331821)都会产生一个小的**[局部误差](@article_id:640138)**。但是*下一步*的系统状态是基于当前步骤的（略有错误的）状态计算的。第一步的误差被带入第二步的计算中，第二步又增加了自己的[局部误差](@article_id:640138)。如此继续，误差不断累积。每一步一个量级为，比如说，$O(h^{s+1})$ 的小局部误差，在整个过程中累积，可能会产生一个大得多的量级为 $O(h^s)$ 的**[全局误差](@article_id:308288)**。最终的误差是沿途所有微小失误的总和。

这种级联效应在一些寻找[矩阵特征值](@article_id:316772)的方法中也得到了优美的体现 [@problem_id:2165905]。一种称为**[降阶法](@article_id:347095)**的技术，其工作方式是先找到最大的[特征值](@article_id:315305) $\lambda_1$，然后构造一个新矩阵，该矩阵除了 $\lambda_1$ 被替换为零外，与原矩阵具有所有相同的[特征值](@article_id:315305)。然后对新矩阵重复此过程以找到下一个[特征值](@article_id:315305) $\lambda_2$。这看起来很优雅，但它有一个隐藏的缺陷。$\lambda_1$ 的计算值会有一些微小的数值误差。这个误差被“融入”到降阶矩阵的构造中。因此，当我们寻找 $\lambda_2$ 时，我们使用的不是理想的矩阵，而是一个略受扰动的矩阵。因此，我们计算出的 $\lambda_2$ 的误差将来自[数值方法](@article_id:300571)本身*和*从 $\lambda_1$ 传播过来的误差。这种情况会一直持续下去，所有先前阶段的误差都会累积。结果是，前几个[特征值](@article_id:315305)被精确地找到，但准确性随着每一步的进行而下降，最后找到的[特征值](@article_id:315305)通常是最不准确的。

有时，[算法](@article_id:331821)本身的结构就会使其成为误差的放大器。一个经典的例子是将一组向量转换为标准正交基的经典格拉姆-施密特（CGS）过程 [@problem_id:2169893]。一个关键步骤是取一个向量 $v_2$，通过减去它在第一个[基向量](@article_id:378298) $q_1$ 上的投影，使其与 $q_1$ 正交：$u_2 = v_2 - (v_2 \cdot q_1)q_1$。这个 $u_2$ 应该与 $q_1$ 完全正交。但如果计算机在执行这个减法时，产生了一个微小的[舍入误差](@article_id:352329)，并留下了一点 $q_1$ 的残留呢？一个假设模型显示，如果初始向量几乎平行，即使是微不足道的误差项也可能导致正交性的灾难性损失。一个数量级为 $10^{-4}$ 的误差可能导致最终的“正交”向量的[点积](@article_id:309438)接近 $0.5$，而不是所要求的 $0$。该[算法](@article_id:331821)是**数值不稳定**的；它会把微小、不可避免的[误差放大](@article_id:303004)成灾难性的最终结果。

### 问题不在[算法](@article_id:331821)，而在问题本身：[病态性](@article_id:299122)

到目前为止，我们一直在指责我们的工具——计算机的有限精度和我们[算法](@article_id:331821)的不稳定性。但有时，问题不在于工具，而在于*任务本身*。这把我们带到了最后一个，也是最微妙的概念：**[病态性](@article_id:299122)（conditioning）**。

如果输入数据的微小变化只导致输出的微小变化，那么这个问题就是**良态的（well-conditioned）**。如果输入的微小、无足轻重的扰动可能导致输出的巨大变化，那么这个问题就是**病态的（ill-conditioned）**。一个[病态问题](@article_id:297518)就像一座纸牌屋；最轻微的风都可能让它倒塌。

考虑[范德蒙矩阵](@article_id:308161)，它出现在诸如将[多项式拟合](@article_id:357735)到一组数据点的问题中 [@problem_id:2395209]。如果我们试图通过在彼此非常接近的点上采样来确定这样一个多项式的性质，我们的直觉告诉我们这不是一个好主意；我们从每个样本中没有得到太多新信息。相关范德蒙[矩阵的[行列](@article_id:308617)式](@article_id:303413)将这种直觉形式化了。当数据点聚集在一起时，矩阵变得近乎奇异，其[行列式](@article_id:303413)对点的确切位置变得极其敏感。它是严重病态的。

在这种情况下，即使我们有一个完全稳定的[算法](@article_id:331821)和一台高精度的计算机，我们也无法信任我们的答案。为什么？因为输入数据本身——也许来自物理测量——总有一些微小的不确定性。对于一个[病态问题](@article_id:297518)，这个微小的输入不确定性被问题自身的性质放大，变成了输出的巨大不确定性。这不是[算法](@article_id:331821)的错。问题本身就是一个雷区。

这给了我们谜题的最后一块。要对一个数值结果有信心，我们需要两样东西。我们需要一个**稳定的[算法](@article_id:331821)**，它不会放大它自己产生的误差。并且我们需要解决的是一个**良态的问题**，它对输入的不确定性不过于敏感。一个不稳定的[算法](@article_id:331821)就像一个摇晃的梯子。一个病态的问题就像试图把那个梯子放在流沙上。要得到一个正确的答案，你必须两者都避免。理解这些原则是成为数值计算大师的第一步，学会与数字幽灵共事，而不是被它所困扰。