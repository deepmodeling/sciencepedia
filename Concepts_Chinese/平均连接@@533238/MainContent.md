## 引言
在浩瀚的数据世界中，结构很少是显而易见的。寻找有意义的群体——相似股票的集群、相关基因的族群或观点相近的选民群体——是数据科学中的一个根本挑战。[凝聚式层次聚类](@article_id:639966)提供了一种直观的解决方案：从单个数据点开始，迭代地合并最接近的点，直到形成一个包含所有点的单一群体。但整个过程的成功取决于一个关键问题：我们如何定义“最接近”？

本文深入探讨了对该问题最流行和最稳健的答案之一：**平均连接**。与那些容易被离群点误导的简单方法不同，平均连接采用一种民主的方法，考虑了潜在簇所有成员之间的集体距离。它弥合了“仅仅知道什么是[聚类](@article_id:330431)”与“理解特定[算法](@article_id:331821)的设计选择（其规则、效率和权衡）如何决定其发现的结构”之间的知识鸿沟。

首先，在“原理与机制”一章中，我们将剖析平均连接的数学引擎，探索其优雅的更新公式、概率论解释以及其在面对充满噪声的高维数据时的内在优缺点。之后，“应用与跨学科联系”一章将展示这种强大的方法如何应用于解决现实世界的问题，从在生物信息学中揭示生命的蓝图到描绘我们城市的社会结构。

## 原理与机制

想象一下，你正在一个谁也不认识谁的派对上。作为一位顶级的社交组织者，你的任务是逐渐将人们分组成朋友小组，直到每个人都属于一个大的友好圈子。你会怎么做？你可能会先将两个看起来最相似的人配对，也许他们正在谈论同一个冷门爱好。然后你会寻找下一对最合得来的人，或者可能找到第三个人，他能完美地融入第一对。这种迭代的、逐步合并的过程是[凝聚式层次聚类](@article_id:639966)的核心。

当然，关键问题是你所说的“合得来”或“接近”是什么意思。你选择的规则将极大地改变最终形成的群体。**平均连接**法对这个问题有一个特别民主和直观的回答。

### 平均法则

在决定合并哪两个簇时，平均连接不仅仅看每个组里最接近的两个人。这就像根据两个大家族中最外向的两个孩子之间的友谊来判断整个家族的关系；这是一种很容易被欺骗的策略。相反，平均连接考虑*所有人*。两个簇（比如 $\mathcal{A}$ 和 $\mathcal{C}$）之间的距离，被定义为簇 $\mathcal{A}$ 中的每个人与簇 $\mathcal{C}$ 中的每个人之间所有个体距离的*全部*平均值。

在数学上，如果你有两个大小分别为 $|\mathcal{A}|$ 和 $|\mathcal{C}|$ 的簇 $\mathcal{A}$ 和 $\mathcal{C}$，它们之间的距离是：

$$
d(\mathcal{A},\mathcal{C}) = \frac{1}{|\mathcal{A}| |\mathcal{C}|} \sum_{a \in \mathcal{A}} \sum_{c \in \mathcal{C}} d(a,c)
$$

这就是该[算法](@article_id:331821)的基本原则：合并是基于所有成员的集体平均意见。没有任何一个点对，无论远近，可以单独决定合并 [@problem_id:3097662]。

### 优雅的更新：[算法](@article_id:331821)的引擎

每次合并后都从头计算这个总平均值将是一场计算噩梦。幸运的是，数学提供了一个绝妙的捷径。假设我们刚刚合并了两个簇 $\mathcal{A}$ 和 $\mathcal{B}$，形成了一个新的、更大的簇 $\mathcal{A} \cup \mathcal{B}$。我们如何计算它与某个其他簇 $\mathcal{C}$ 的距离呢？

事实证明，我们不需要回到原始数据。新的距离仅仅是旧距离的**[加权平均](@article_id:304268)**，其中权重是组成簇的相对大小 [@problem_id:3097662]：

$$
d(\mathcal{A} \cup \mathcal{B}, \mathcal{C}) = \frac{|\mathcal{A}|}{|\mathcal{A}| + |\mathcal{B}|} d(\mathcal{A},\mathcal{C}) + \frac{|\mathcal{B}|}{|\mathcal{A}| + |\mathcal{B}|} d(\mathcal{B},\mathcal{C})
$$

这就是平均连接的 Lance-Williams 更新公式，也是使该[算法](@article_id:331821)高效的引擎。它是一台奇妙的数学机器。关于新簇与外界关系的信息已经包含在其父簇的关系中，只待组合。

但这个优雅的规则带来了一个有趣的后果。在这个平均值中，较大簇的“声音”更大。让我们想象一个场景，簇 $\mathcal{C}$ 正在决定是与一个小的、邻近的簇 $\mathcal{A}$ 合并，还是与一个巨大的、更远的簇 $\mathcal{B}$ 合并。簇 $\mathcal{B}$ 的巨大规模可以使其距离 $d(\mathcal{B},\mathcal{C})$ 在更新公式中占据如此大的权重，以至于它可以“拉低”合并后的距离，即使它的个体成员平均而言更远。这种大小加权效应意味着该[算法](@article_id:331821)不仅关心邻近性；它还关心所涉及簇的质量或大小 [@problem_id:3097585]。

### 概率论视角

还有另一种，也许更优美的方式来思考平均连接规则。让我们不从枯燥的[算术平均值](@article_id:344700)出发，而是从物理学家的角度来看待它。簇 $\mathcal{A}$ 和 $\mathcal{C}$ 之间的平均连接距离，精确地说是从 $\mathcal{A}$ 中随机抽取一个点和从 $\mathcal{C}$ 中随机抽取一个点时，你会发现的**[期望](@article_id:311378)距离** [@problem_id:3140669] [@problem_id:3140579]。

在每一步，[算法](@article_id:331821)只是合并那些随机抽取的成员平均而言最接近的两个群体。这种概率论的框架感觉更自然，将[算法](@article_id:331821)与大数定律联系起来。随着我们的簇变大，这种基于样本的平均距离将收敛于点所来源的底层分布之间的真实[期望](@article_id:311378)距离 [@problem_id:3140669]。

这个视角还让我们能够与另一种常识性方法建立起优美的联系：**[质心](@article_id:298800)连接**，即簇之间的距离就是它们几何中心（[质心](@article_id:298800)）之间的距离。利用一个强大的数学工具**Jensen's Inequality**，我们可以证明平均连接距离*总是大于或等于*[质心](@article_id:298800)连接距离 [@problem_id:3140669]。

$$
\underbrace{\mathbb{E}[\|X - Y\|]}_{\text{Average Linkage}} \ge \underbrace{\|\mathbb{E}[X] - \mathbb{E}[Y]\|}_{\text{Centroid Linkage}}
$$

这不仅仅是一个数学上的奇趣现象；它显示了“距离的平均值”和“平均值的距离”之间存在着根本的[张力](@article_id:357470)。平均连接考虑了簇的完整分布，而[质心](@article_id:298800)连接则将它们简化为单个点。

### 在充满噪声的世界中生存：鲁棒性及其局限

真实世界是混乱的。数据包含噪声、离群点和奇怪的结构。我们这个民主的规则表现如何呢？

最简单的[连接方法](@article_id:640851)——**单一连接**（基于*单个*最接近的点对进行合并）——一个著名的失败模式是**链式效应**。它可能被少数几个“桥接”点欺骗，将两个遥远且不同的簇连接在一起，就像一条横跨峡谷的链条。平均连接对这个问题具有更强的鲁棒性。因为它对所有点对进行平均，少数几个桥接点不足以欺骗它；许多相距遥远の点的声音压倒了少数相近的点的声音 [@problem_id:3097573]。

然而，平均连接并非无懈可击。回想一下我们的概率模型。如果一个簇有一小部分极端的离群点“尾巴”会怎样？假设一个簇中 $1\%$ 的点异常地远离其他所有点。虽然选中这些离群点的概率很低，但它们巨大的距离会不成比例地夸大[期望](@article_id:311378)距离，可能主导合并决策 [@problem_id:3140669]。所以，虽然平均连接是鲁棒的，但它并非不受非常强的离群点的影响。

再来看一个更微妙的鲁棒性测试。如果你在数据集中添加一个点的精确副本会发生什么？[算法](@article_id:331821)要做的第一件事就是以零距离将这个点与其分身合并。但聚类过程的其余部分呢？对于平均连接，以及单一连接和完全连接，层次结构的其余部分保持完全相同！这些方法依赖于成对距离的集合，添加一个重复点并不会改变所有*其他*点之间的距离。然而，像[质心](@article_id:298800)连接和 War[d'](@article_id:368251)s linkage 这类在其更新规则中明确依赖于簇大小和[质心](@article_id:298800)的方法，其计算将会在整个层次结构中被改变。这种“对重复点的鲁棒性”是一个微妙但深刻的属性，它将平均连接区分开来 [@problem_id:3114193]。

### 局部贪婪与全局最优

平均连接[算法](@article_id:331821)是**贪心**的。在每一步，它都采取当下看起来最好的行动——合并两个最接近的簇。但这一系列局部最优的移动是否会导向最佳的最终分组？

不一定。想象一个全局目标，比如创建的最终簇中所有内总距离尽可能小。事实证明，平均连接做出的贪心选择并不总能为这个全局目标带来最好的结果。一个现在看来最好的合并，未来可能会导致一个次优的配置 [@problem_id:3140579]。这是一个深刻的教训，其适用范围远超聚类：最好的局部策略并不总是最好的全局策略。

这凸显了没有哪一种是“最佳”[聚类算法](@article_id:307138)。方法的选择是你对所珍视价值的选择。例如，在生物信息学中，科学家对肿瘤基因表达数据进行[聚类](@article_id:330431)时，如果他们寻找的是紧凑的、球形的簇，这些簇代表由协调的基因程序驱动的不同生物亚型，他们可能会选择 **War[d'](@article_id:368251)s method**。但如果他们怀疑重要的生物学故事是一个连续的梯度——比如免疫细胞浸润水平的变化——他们可能会选择**平均连接**，因为它更擅长发现这类拉长的、连续的结构 [@problem_id:2379267]。正确的工具取决于你试图发现的结构的性质。

### 最后的转折：高维空间的奇特性

我们对几何的直觉是建立在我们生活的三维空间之上的。但许多现实世界的数据集，如基因表达数据，可能拥有数千个维度。在这些高维空间中，几何学变得异常奇特。

最显著的影响之一是**测度集中**现象。随着维度数（$p$）的急剧增加，随机点之间的距离变得不那么随机，而更具可预测性。它们都倾向于集中在一个特定的值附近。

让我们考虑使用基于向量之间角度的相异性，$d(x,y) = 1 - \cos(\theta_{xy})$。对于高维球面上的点，大小为 $m$ 和 $n$ 的两个簇之间的平均连接距离的方差由一个极其简单且令人惊讶的公式给出 [@problem_id:3129057]：

$$
\mathrm{Var}[D(\mathcal{A}, \mathcal{B})] = \frac{1}{mnp}
$$

看分母中的 $p$！随着维度 $p$ 的增长，我们的连接距离的方差向零收缩。任意两个簇之间的“平均距离”几乎变成了一个常数。这是**维度灾难**的一种表现。如果所有的簇间距离都几乎相同，[算法](@article_id:331821)如何可能做出关于哪一对是“最接近”的有意义选择？在这个奇特的高维世界里，我们对邻近性的直观概念开始瓦解，这对[聚类](@article_id:330431)提出了深刻而有趣的挑战，也揭示了需要新思想的前沿领域。

