## 引言
对自然界的复杂现象（从[流体动力学](@article_id:319275)到量子力学）进行建模，传统上依赖于将空间和[时间离散化](@article_id:348605)的数值方法。这些方法虽然强大，但在处理复杂几何形状、高维问题以及其潜在物理定律不完全已知的情景时可能会遇到困难。本文介绍了一种处于机器学习与计算科学[交叉](@article_id:315017)领域的革命性方法：物理信息神经网络（PINN）。PINN 通过将物理学的基本定律直接[嵌入](@article_id:311541)到学习过程中，解决了传统求解器和纯数据驱动模型的局限性。在接下来的章节中，您将了解驱动这些网络的核心机制，并探索它们的变革性应用。第一章“原理与机制”将揭示[神经网络](@article_id:305336)如何被教会“说”物理学的语言。随后的“应用与跨学科联系”将展示这一强大框架如何被用于解决以前难以处理的问题，并推动不同领域的科学发现。

## 原理与机制

想象一下，你有一块神奇的黏土，可以塑造成任何形状。但它不仅仅是普通的黏土，而是“智能黏土”。你可以告诉它：“我希望你形成一个描述机翼上空气流动的形状，并且这个形状必须遵守 Navier-Stokes 方程。”然后，这块黏土会扭动和变形，直到稳定成正确的形式，完美地满足你设定的物理定律。这就是物理信息神经网络的精髓。神经网络就是我们那块神奇的、无限柔韧的黏土，而物理定律就是我们给它的指令。

但这魔法究竟是如何运作的呢？当然，这不是魔法，而是微积分、优化和计算机科学的美妙交响。让我们来层层揭开它的面纱。

### 一个可微的宇宙

每个现代神经网络的核心都是一个简单的思想：它是一个函数。你从一端输入数字，从另一端得到数字。对于一个图像分类器，你输入一张猫的图片的像素值，然后会得到标签“猫”的高概率。我们可以将其写为 $\text{output} = f_{\theta}(\text{input})$，其中 $\theta$ 代表网络中所有在训练期间进行调整的内部旋钮和拨盘——即其“权重”和“偏置”。

PINN 的突破在于，它不把网络看作一个分类器，而是看作一个逼近物理场的[连续函数](@article_id:297812)。例如，我们可以说，空间中任意点 $\boldsymbol{x}$ 和时间 $t$ 的温度 $T$ 可由一个神经网络近似：$T(\boldsymbol{x}, t) \approx T_{\theta}(\boldsymbol{x}, t)$。我们网络的输入现在是坐标 $(\boldsymbol{x}, t)$，输出则是物理场的值，即温度。

仅此一点并不新鲜；几十年来，科学家们一直将网络用作通用的[函数逼近](@article_id:301770)器。革命性的一步在于接下来的内容。物理定律以[偏微分方程](@article_id:301773)（PDEs）的形式表达，其中涉及[导数](@article_id:318324)——即变化率。例如，[热方程](@article_id:304863)涉及像 $\frac{\partial T}{\partial t}$ 和 $\nabla^2 T$ 这样的[导数](@article_id:318324)。如果我们的近似 $T_{\theta}$ 是一个神经网络，我们到底该如何计算它的[导数](@article_id:318324)呢？

答案是驱动每个 PINN 的引擎：**[自动微分](@article_id:304940)（AD）**。与你在微积分课上可能学到的数值近似（如有限差分）不同，AD 是一种计算技术，它能计算出由网络所表示函数的*精确*[导数](@article_id:318324)，直至计算机精度的极限。它通过在网络的每个简单操作上细致地应用[链式法则](@article_id:307837)来做到这一点。AD 是我们的魔杖；有了它，我们可以用我们的神经网络 $T_{\theta}(\boldsymbol{x}, t)$ 瞬间得到 $\frac{\partial T_{\theta}}{\partial t}$、$\frac{\partial T_{\theta}}{\partial x}$、$\frac{\partial^2 T_{\theta}}{\partial x^2}$ 等表达式。我们现在有了一个可以直接代入物理方程的[函数逼近](@article_id:301770)器。 [@problem_id:2668954]

### [物理信息](@article_id:312969)[损失函数](@article_id:638865)：现实的记分卡

现在我们有了物理场的“可微”近似，我们如何强制它遵守物理定律呢？我们创建一个记分卡，一个告诉网络它做得怎么样的数学函数。在机器学习中，这个记分卡被称为**[损失函数](@article_id:638865)**。训练的目标是调整网络的参数 $\theta$，使损失尽可能小。

让我们继续以热方程为例，将所有项移到一边后，可以写成一个**[残差](@article_id:348682)** $R$：
$$
R = \rho c_p \frac{\partial T}{\partial t} - \nabla \cdot (k \nabla T) - q = 0
$$
对于一个完美的解，这个[残差](@article_id:348682)在任何地方都为零。对于我们的网络近似 $T_{\theta}$，[残差](@article_id:348682)一开始很可能不为零：
$$
R_{\theta}(\boldsymbol{x}, t) = \rho c_p \frac{\partial T_{\theta}}{\partial t} - \nabla \cdot (k \nabla T_{\theta}) - q
$$
我们现在可以在大量随机选择的空间和时间点（称为**配置点**）上评估这个[残差](@article_id:348682)。一个好的网络应该在所有这些点上都使[残差](@article_id:348682)很小。因此，我们[损失函数](@article_id:638865)的一个核心部分是所有这些点上[残差](@article_id:348682)平方的均值：
$$
\mathcal{L}_{PDE}(\theta) = \frac{1}{N_{pde}} \sum_{i=1}^{N_{pde}} |R_{\theta}(\boldsymbol{x}_i, t_i)|^2
$$
这个简单的思想将 PINN 构建为对一种称为**[配置法](@article_id:299333)**的经典数值技术的现代演绎。但有一个关键区别。在经典方法中，基函数（如多项式或正弦函数）是固定的。而在 PINN 中，“[基函数](@article_id:307485)”——由网络隐藏层创建的特征——是在训练过程中*自适应*的。这就好比网络不仅在寻找[基函数](@article_id:307485)的最佳组合，更是在为手头的问题发明最佳的基函数。 [@problem_id:3214158]

当然，[偏微分方程](@article_id:301773)只是故事的一部分。一个物理问题还由其边界条件（例如，一堵墙上的温度固定在 $100^\circ\text{C}$）和[初始条件](@article_id:313275)（$t=0$ 时各处的温度）定义。我们向损失函数中添加项来惩罚任何对这些条件的偏离。如果我们有一些真实的传感器测量数据，我们还可以添加一个数据失配项。总损失函数变成了一个加权和，它根据网络的总体物理一致性对其进行评分：
$$
\mathcal{L}(\theta) = w_{PDE} \mathcal{L}_{PDE} + w_{BC} \mathcal{L}_{BC} + w_{IC} \mathcal{L}_{IC} + w_{data} \mathcal{L}_{data}
$$
训练 PINN 现在是一个优化问题：找到使这个总损失最小化的参数 $\theta$，从而找到一个同时满足控制[偏微分方程](@article_id:301773)、边界和初始条件以及任何可用实验数据的函数。 [@problem_id:2502969] [@problem_id:2668921]

### 驾驭边界：硬约束与软约束

施加边界条件是如此关键，值得我们更深入地探讨。上面描述的“软”惩罚方法很简单，但有一个缺点：它是一种协商。通过调整权重 $w_{BC}$，我们告诉优化器满足边界条件相对于满足[偏微分方程](@article_id:301773)的重要性。如果我们将 $w_{BC}$ 设置得太高，优化器可能会过分关注边界，导致一个病态的、难以训练的过程。 [@problem_id:2656059]

有一种更优雅、更严谨的方法：**硬施加**。我们可以设计网络的输出，使其*通过构造*就满足边界条件。想象一下，我们需要解决一个长度为 $L$ 的杆上的问题，其中位移 $u(x)$ 必须满足 $u(0)=A$ 和 $u(L)=B$。我们可以取一个[神经网络](@article_id:305336)的原始输出 $\hat{u}_{\theta}(x)$，并对其进行变换：
$$
u_{\theta}(x) = \underbrace{A\left(1 - \frac{x}{L}\right) + B\left(\frac{x}{L}\right)}_{\text{A line that fits the boundaries}} + \underbrace{x(L-x)}_{\text{Vanishes at boundaries}} \hat{u}_{\theta}(x)
$$
仔细看这个构造。第一部分只是一条通过 $(0, A)$ 和 $(L, B)$ 的[直线方程](@article_id:346093)。第二部分包含一个项 $x(L-x)$，它保证在 $x=0$ 和 $x=L$ 处为零。这意味着无论网络学习到什么样的函数 $\hat{u}_{\theta}(x)$，总表达式 $u_{\theta}(x)$ 都将*永远*完美地满足边界条件！优化过程现在从担忧边界的束缚中解放出来，可以将其全部精力集中在满足内部区域的[偏微分方程](@article_id:301773)上。这是一个将数学结构直接融入[网络架构](@article_id:332683)的绝佳例子。 [@problem_id:2656059] [@problem_id:2126300]

### 真正的力量：揭示未知

这就是 PINN 从一个聪明的 PDE 求解器转变为一个名副其实的科学发现工具的地方。到目前为止，我们都假设完全了解 PDE。但如果我们不了解呢？如果我们有一些来自新型材料的温度传感器数据，但我们不知道其[热导率](@article_id:307691) $k$ 怎么办？

我们可以将 $k$ 视为另一个可训练参数，就像网络权重 $\theta$ 一样。我们用一个随机猜测来初始化 $k$，然后让优化器找到 $k$ 的值*以及*温度场 $T_{\theta}(x,t)$，它们共同在与热方程的一般形式保持一致的同时，最好地解释传感器数据。损失函数中的物理定律充当了一个极其强大的正则化器，用一个物理上合理的解来填补我们稀疏测量数据之间的巨大空白。 [@problem_id:2668921]

这就提出了一个深刻的问题：这在什么时候是可能的？我们总能发现隐藏的参数吗？这就是**可辨识性**的问题。考虑在一个温度不再变化的[稳态](@article_id:326048)实验中，试图同时找出热导率 $k$ 和一个均匀的内部热源 $q$。控制方程是 $k \nabla^2 T + q = 0$。注意到任何解 $(T, k, q)$ 都可以被 $(T, \alpha k, \alpha q)$ 替换（对于任意常数 $\alpha$），方程仍然成立。物理学本身存在模糊性！我们只能确定比率 $q/k$。然而，如果我们进行一个温度随时间变化的*瞬态*实验，完整的方程 $\rho c_p \frac{\partial T}{\partial t} = k \nabla^2 T + q$ 就起作用了。参数 $k$ 和 $\rho c_p$ 以不同的方式控制着[时间演化](@article_id:314355)，使得优化器能够从[时间序列数据](@article_id:326643)中解开它们的各自的值。这揭示了一个深刻的真理：实验的设计决定了什么是可能被知道的。 [@problem_id:2502969]

### 现实的考验：前沿的挑战

到目前为止，这幅图景似乎好得令人难以置信，而在科学中，“好得令人难以置信”通常意味着我们还没有提出棘手的问题。PINN 尽管功能强大，但也有其独特的失效模式。

其中最引人入胜的一个是**[谱偏差](@article_id:306060)**。[神经网络](@article_id:305336)在使用标准的基于梯度的方法进行训练时，基本上是“懒惰的”。它们发现学习平滑的低频函数比学习快速[振荡](@article_id:331484)的高频函数要容易得多。如果你让一个 PINN 求解一个看起来像一个简单、平缓小山的解，它会很乐意地做到。但如果你让它求解一个高频波，比如对于大波数 $k$ 的 Helmholtz 方程 $u'' + k^2 u = 0$ 的解，网络通常会惨败。它会放弃并返回平凡解 $u(x)=0$，这也是方程的一个完美解，并且损失为零。网络选择了阻力最小的路径，而零频率的平直线解是所有路径中最容易的。 [@problem_id:2411070]

我们如何才能引导这个懒惰的网络学习这些更难的高频模式呢？我们可以给它一个更好的词汇库！我们可以不给网络输入原始坐标 $x$，而是给它一组**傅里叶特征**：$[\sin(\omega_1 x), \cos(\omega_1 x), \sin(\omega_2 x), \cos(\omega_2 x), \dots]$。这预处理了输入，实质上是给了网络一组高频的构建模块。在这里，物理学可以再次为我们的选择提供信息。如果我们要解决一个我们知道具有特征厚度为 $\ell$ 的尖锐**[边界层](@article_id:299864)**的问题，我们应该选择量级为 $1/\ell$ 的频率，以便为网络提供解决该特定特征所需的工具。这种美妙的协同作用——利用对问题的物理分析来设计机器学习架构——正是这个领域如此激动人心的原因。 [@problem_id:2668903] [@problem_id:2411070]

当问题涉及[激波](@article_id:302844)或[奇点](@article_id:298215)时，会出现一个更严峻的挑战，例如[超音速射流](@article_id:344506)产生的[激波](@article_id:302844)，或材料中[裂纹尖端](@article_id:362136)的应力。在这些点上，解是不连续的，其[导数](@article_id:318324)是无限的或未定义的。逐点 PDE [残差](@article_id:348682)的概念本身就失效了。一个标准的 PINN，试图评估一个无限的[残差](@article_id:348682)，将会完全迷失方向。 [@problem_id:2411081]

前进的道路是再次借鉴经典工程分析中的一个思想：如果你无法处理点，那就处理平均值。我们可以不要求 PDE 在离散点上成立（**[强形式](@article_id:346022)**），而是要求 PDE 的一个积分或平均版本在小体积内成立。这被称为**[弱形式](@article_id:303333)**。这种表述方式对[不连续性](@article_id:304538)和[奇点](@article_id:298215)具有天然的鲁棒性，因为积分过程会使它们平滑化。基于这些[弱形式](@article_id:303333)构建 PINN，使它们能够解决固[体力](@article_id:353281)学和[流体动力学](@article_id:319275)中一整类新的挑战性问题，从而推动了可能性的边界。 [@problem_id:2668902]

从[可微函数](@article_id:305017)的核心思想到处理边界的实际问题，再到处理[激波](@article_id:302844)和波的前沿挑战，PINN 的故事是一个构建桥梁的故事。它是一座连接机器学习的数据驱动世界与物理科学的原理驱动世界之间的桥梁，创造了一个比两者单独存在时更强大、更具洞察力的工具。

