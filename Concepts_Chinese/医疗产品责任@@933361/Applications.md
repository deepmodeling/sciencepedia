## 应用与跨学科联系

在探寻了责任的基本原则之后，我们现在来到了探索中最激动人心的部分：见证这些抽象理念在现实世界中焕发生机。正如物理定律不仅限于实验室，而是支配着行星的运动和河流的流淌，产品责任原则也不仅限于法律书籍；它们塑造着拯救我们生命、诊断我们疾病、并日益辅助我们做出最关键决策的技术本身。正是在这里，法律不再是一套静态的规则，而成为一种动态的力量，与医学、工程学、伦理学乃至数据科学相互作用。

我们将看到这些原则如何在联邦监管与个人正义之间创造出一场微妙且时而充满争议的博弈。然后，我们将冒险进入人工智能这个勇敢的新世界，探问沿用百年的法律原则如何可能理解一台能够学习的机器。最后，我们将看到，制造商的责任远不止于工厂车间，它包含了一种在不断演变的威胁世界中保持警惕的义务。

### 联邦与州的博弈：谁来设定标准？

想象一台巨大而精密的机器，旨在确保国内每一件医疗器械都安全有效。这台机器就是美国食品药品监督管理局（FDA）。现在，想象一个被其中一件器械伤害的人。他们的正义之路通常会通向州法院，在那里他们可以提起人身伤害诉讼。一个根本性的问题立即出现：一个州的陪审团，在适用本州法律时，能否在联邦政府的专家机构已经批准某器械的情况下，宣布该器械不安全？这就是**联邦优先适用**的战场。

正如最高法院所阐明的，答案完全取决于FDA最初审查该器械的严格程度。对于风险最高的器械，如植入式除颤器，FDA会进行艰苦的**上市前批准（PMA）**过程，仔细审查器械设计、制造和标签的每一个方面。由于这种联邦监督如此彻底，法律规定州法院不能施加自己“不同于或附加于”的要求。陪审团不能简单地判定一个经联邦批准的设计是不安全的。

然而，这并未给制造商一张免死金牌。如果制造商未能达到FDA对其施加的标准，会发生什么？假设PMA规定环氧树脂必须固化至少60分钟，但为了加快速度，某一特定批次只固化了30分钟，导致器械失灵[@problem_id:4483397]。在这种情况下，州级诉讼并非在创造一个*新*标准；它是在执行*现有的联邦*标准。这就是关键的“平行诉求”例外：患者可以根据州法提起诉讼，主张其伤害是由于制造商违反联邦法规造成的。这种巧妙的法律机制允许州级诉讼充当FDA强大的执法伙伴，在实地监督联邦规则的遵守情况[@problem_id:4483416]。

对于绝大多数通过一种不那么严格的程序——即**510(k)许可**——上市的医疗器械而言，情况则完全不同。在这里，制造商仅需证明其新器械与市场上已有的器械“实质上等同”。FDA并未像对PMA器械那样批准该器械的设计为安全有效。因此，法院裁定这些诉求通常*不*被优先适用。例如，一名被经510(k)许可的骨科螺钉所伤的患者，完全可以在州法院主张其设计存在缺陷，无论其是否获得FDA许可[@problem_id:4483417]。这种双轨制是法律逻辑的一个绝佳范例，它根据联邦监督的深度来调整州法院的权力。

### 机器中的幽灵：人工智能的责任

我们现在转向这些原则正受到最严峻考验的前沿领域：人工智能。当产品不是一个实体物品，而是一个算法时，我们如何谈论“缺陷”？

让我们从一个基本的区分开始。想象医院里一个AI分诊系统，在数千次决策中，持续地将某类危重病人误分类的比例约为$2\%$。这不是一个随机的小故障。一个随机的、一次性的错误——比如某台机器因[内存泄漏](@entry_id:635048)导致的软件崩溃——可以被视为**制造缺陷**。它是一个异常，偏离了预期的设计。但一个在每台运行该软件的机器上都出现的、可预测的稳定错误率，则暗示了更深层次的问题：**设计缺陷**。缺陷在于蓝图本身——即算法、数学模型或其训练所用的数据[@problem_id:4494856]。

这就把我们带到了我们这个时代最深刻的跨学科挑战之一：[算法偏见](@entry_id:637996)。考虑一个使用光基传感器（光电容积描记法，即PPG）来检测不规则心跳的可穿戴设备。这些传感器的物理原理众所周知：黑色素，即深色皮肤中的色素，对光的吸收方式与浅色皮肤不同。如果一家公司主要使用来自浅色皮肤个体的数据来设计其设备并训练其AI，那么该设备对深色皮肤用户的准确性会降低，这是完全可以预见的。现在，假设该公司的工程师们自己知道这一点，甚至已经开发出一种可行的、低成本的替代设计，即采用双波长传感器，对所有人都同样有效，但公司为了更快地推向市场而选择不使用它。如果一个深色皮肤的用户因此错过了危险的[心律失常](@entry_id:178381)诊断，他们就有了强有力的诉讼理由。他们可以主张存在**设计缺陷**，因为存在一个合理的、更安全、更公平的替代设计。他们也可以主张存在**未能警示**的缺陷，因为公司未能披露这一关键限制，甚至可能在营销该设备时宣称其适用于“多样化的用户”[@problem_id:5014165]。

一个AI的“设计”不仅仅是它的代码；它是创造它的整个过程，这包括它学习所用的数据。如果一家公司开发了一个黑色素瘤检测AI，使用的是一个第三方数据集，其中只有$10\%$的图像来自深色皮肤类型，而该群体占目标患者群体的$40\%$，那么问题就在酝酿之中。如果公司自己的内部测试随后证实，该AI对这个代表性不足的群体有更高的假阴性率，那么漏诊癌症的风险就不再只是一种可能性；它是一个可预见、可预测的后果。制造商有**审核**其数据的**义务**，并采取措施减轻此类偏见。仅仅依赖供应商对“多样性”的模糊保证是远远不够的。法律不允许制造商对一个不成比例地影响整个群体的可预见伤害视而不见[@problem_id:4400521]。

### 错综复杂的网络：一个共同责任的体系

当AI卷入医疗差错时，“谁应负责？”的问题变得异常复杂。责任方很少是单一的个人或实体。想象一下，一家医院使用AI帮助放射科医生对CT扫描上的肺癌风险进行分层。

首先是**开发者**。他们负责设计安全的产品，并提供清晰的说明和警示。

其次是**医院**或提供者。他们负责系统的实施方式。假设AI输出一个恶性肿瘤的概率，医院可以设定一个阈值$t$，高于该阈值的病例被标记为“高风险”。开发者自己的数据可能显示，为了减少放射科医生的工作量而设置一个高阈值，会大大增加漏诊癌症的数量。例如，一个假设的计算可能显示，选择$t=0.70$的阈值所导致的预期患者伤害，几乎是选择较低阈值$t=0.35$的四倍。如果医院仍然选择了高风险阈值，那么它可以说违反了其提供安全医疗系统的义务[@problem_id:4405387]。

第三是**临床医生**。AI是一个咨询工具。如果一位放射科医生看到了明确的癌症临床体征——比如一个大的、有毛刺的结节——但因为AI评分略低于医院的阈值而予以忽略，那么他们很可能违反了诊疗标准。他们未能运用自己的独立判断，成了“自动化偏见”的受害者。在这种情况下，责任并非一个整齐的小包裹；它是一张网，开发者、医院和临床医生都可能在其中分担一部分责任。

这就引出了另一个经典的法律原则：**有学识的中间人**。几十年来，处方药制造商一直主张，只要他们充分警示了开药的医生，他们就可以免于承担责任，因为医生充当了通向患者的“有学识的中间人”。AI供应商能提出同样的论点吗？或许可以。但这种辩护是脆弱的。如果供应商关于AI局限性的详细警示只提供给了医院的IT部门，而从未到达实际使用该系统的医生手中呢？如果供应商还运营一个网站，鼓励患者“问问你的医生”是否使用他们特定的AI工具呢？通过直接向患者营销，公司削弱了其仅依赖医生判断的主张。在AI时代，这一古老的原则正被迫做出调整，其提供的保护远非绝对[@problem_id:4494882]。

### 销售之外的义务：警惕的制造商

最后，制造商的法律义务并非在销售那一刻就凝固了。它们贯穿产品的整个生命周期。这一点在商法和网络安全领域或许最为清晰。

在许多地方，复杂的软件被视为《统一商法典》（UCC）下的“商品”，这意味着存在一个产品“适销”的默示担保——即适合其通常用途。一个用于急诊室的AI辅助决策支持系统，其通常用途是在时间敏感的危机中可靠地运行。如果该系统在群体伤亡事件的峰值负载下有相当大的崩溃概率，那么可以说它不适合其用途，制造商可能要因违反这一默示担保而承担责任。这提供了一条问责途径，其根源不在于侵权法，而在于商业交易中做出明示或默示的承诺[@problem_id:4400477]。

在[网络安全](@entry_id:262820)的背景下，这种持续的义务更为严峻。想象一个支持AI的输液泵存在一个漏洞，允许远程攻击者操纵向护士显示的推荐剂量。潜在的灾难性伤害是显而易见的。制造商的注意义务不允许他们等待一个可能需要数周才能完成的、经过充分测试的软件补丁。法律要求立即采取与风险相称的行动。这意味着部署“补偿性控制措施”——即可以立即采取以减轻危险的行动。这可能包括在几小时内通过服务器端标志禁用易受攻击的AI功能，推送配置更新以要求进行二次验证步骤，以及向所有医院发布紧急安全通告。修补的义务不仅仅是最终解决问题的义务；它是在此期间使用一切可用工具保护患者的义务[@problem_id:4400540]。

从高风险的优先适用宪法冲突，到肤色偏见的微妙物理学；从AI辅助诊断中的共同责任，到网络安全的紧急要求，产品责任原则如同一条共同的线索贯穿其中。它们不仅关乎在悲剧发生后追究责任，更关乎创建一个激励体系，鼓励人们对那些掌握着我们健康的技术——无论是在其硬件中还是在其代码中——抱有远见、警惕、公平和深刻的责任感。