## 引言
世界上许多最有价值的信息并不能整齐地放入电子表格中。从救命药物的[分子结构](@article_id:300554)到连接我们的社交网络，这些数据的决定性特征是其关系网络。传统的机器学习模型难以理解这种结构，因为它们常常忽略连接关系，并且可能被任意的数据排序所迷惑。这造成了巨大的知识鸿沟，使我们无法有效建模科学和社会中一些最复杂、最重要的系统。

[图神经网络](@article_id:297304)（GNNs）已成为解决此问题的强大方案。它们是一类专门设计用于直接在图结构数据上操作的[神经网络](@article_id:305336)，使用节点和边的原生语言。本文旨在介绍这项变革性技术。首先，在“原理与机制”部分，我们将探讨 GNNs 背后的基本概念，揭示它们如何通过一种称为[消息传递](@article_id:340415)的过程从连接中学习，并讨论其固有的优势和局限性。随后，“应用与跨学科联系”部分将通过一系列真实世界的案例研究，展示 GNNs 如何在从[计算生物学](@article_id:307404)、物理学到经济学等领域引发革命。

## 原理与机制

### 世界不是一张电子表格

在我们教机器认识世界的过程中，我们通常首先将信息组织成整齐的行和列，就像电子表格一样。这对于很多事情——客户记录、天气测量、历史股价——都非常有效。但当数据的真正本质不在于单个项目，而在于它们错综复杂的关系网络时，情况又会如何呢？

想象一下，你是一名计算生物学家，试图预测一种新药分子是否会与目标蛋白结合。你拥有蛋白质“结合口袋”——一个由原子组成的复杂腔体——的精确三维结构。一种天真的方法可能是简单地将所有原子及其 $(x, y, z)$ 坐标列在一个长长的扁平向量中，然后输入一个标准[神经网络](@article_id:305336)，即多层感知机（MLP）。但你应该先列出哪个原子呢？你选择的顺序完全是任意的，由数据文件决定，而不是由物理学决定。如果你重新[排列](@article_id:296886)列表中的原子顺序，你将得到一个完全不同的输入向量，而可怜的 MLP 也会给你一个不同的答案，尽管分子本身在物理上是完全相同的。

这就是问题的核心。分子的物理现实与我们如何[选择标记](@article_id:383421)其原子无关。其性质是**[置换](@article_id:296886)不变的**（permutation invariant）。一个未能尊重这种基本对称性的模型是在不牢固的基础上学习；它是在记忆一个任意的排序，而不是理解真实的结构。[@problem_id:1426741] 分子、社交网络、互联网、路线图以及大脑本身的神经线路都不是电子表格。它们的决定性特征是它们的结构，它们的关系网络。要理解它们，我们需要一种能说“图”语言的语言。

### GNN 解决方案：低语的交响乐

[图神经网络](@article_id:297304)（GNNs）就是为说这种语言而设计的。图是一种非常简单的抽象：一组由**边**（[化学键](@article_id:305517)、友谊、道路）连接的**节点**（原子、人、城市）。GNN 不会将这种丰富的结构扁平化，而是拥抱它。

大多数 GNNs 的核心机制是一个称为**[消息传递](@article_id:340415)**（message passing）的过程。你可以把它想象成在网络上进行的一场精心编排的传话游戏。在每一轮中，每个节点做两件事：
1.  它从其直接邻居那里收集“消息”。消息本质上是邻居当前的状态或[特征向量](@article_id:312227)。
2.  它通过将所有收到的消息与自己之前的状态相结合来更新自己的状态。

经过一轮这样的过程，每个节点不仅拥有关于自身的信息，还拥有关于其局部邻域（其 1-hop 邻居）的信息。两轮过后，来自其邻居的邻居的信息也已到达，因此每个节点现在都有一个延伸到两跳之外的视角。通过堆叠多个[消息传递](@article_id:340415)层，一个节点可以建立一个表示，该表示融合了来自图上不断扩大的“[感受野](@article_id:640466)”的信息。网络学习*如何*转换和组合这些消息，以产生对最终任务有用的特征，例如预测节点的函数或整个图的属性。

这个局部的、迭代的过程非常强大。至关重要的是，GNN 为聚合邻域信息所学习的函数对所有节点都是*相同*的。它不会为 5 号原子学习一个规则，又为 72 号原子学习另一个不同的规则。它学习的是一套单一的、通用的参数化函数，可以应用于任何邻域中的任何节点。这就是 GNNs 具有卓越**归纳**（inductive）能力的原因。

想象一下，你已经训练了一个 GNN，用于在经过充分研究的*大肠杆菌*（*E. coli*）上预测蛋白质功能。该模型学习了局部氨基酸[排列](@article_id:296886)如何影响功能的一般原理。因为它学到的是一个*规则*而不是记住一个*特定的图*，你现在可以将这个训练好的模型直接应用于一个新发现的生物体的蛋白质网络。GNN 可以为这个完全未见过的图生成有意义的预测，因为它学到的生物化学基本规则是普适的。[@problem_id:1436659] 它没有记住一张地图，而是学会了如何阅读地图。

### 群体的智慧：作为受控[扩散](@article_id:327616)的[消息传递](@article_id:340415)

这个“[消息传递](@article_id:340415)”过程到底是什么？其核心是一种学习到的**扩散**（diffusion）。想象一下将一滴墨水滴入一杯水中会发生什么。墨水颗粒会散开，与邻近的颗粒平均浓度，直到颜色均匀。GNN 中的基本[消息传递](@article_id:340415)操作类似于这个[扩散过程](@article_id:349878)的一步。它对相邻节点的[特征向量](@article_id:312227)进行平均或*平滑*，使它们变得更加相似。[@problem_id:2752979]

如果网络中相连的节点倾向于相似（这种特性称为**[同质性](@article_id:640797)**（homophily），或“物以类聚”），那么这种平滑就非常有用。例如，在社交网络中，你朋友的兴趣很可能是你自己兴趣的良好预测指标。通过将你的特征与你朋友的特征进行平均，GNN 可以清理嘈杂的信号并强化社区的共同特征。

“聚合”（AGGREGATE）步骤是不同 GNN 架构或“风格”的区分之处。一个节点应该如何组合来自其邻居的消息？
*   **[图卷积网络](@article_id:373416)（GCN）**通常使用归一化平均。它简单、稳定且有效，就像对你的邻居进行民意调查一样。
*   **[图同构](@article_id:303507)网络（GIN）**使用简单的求和。这看似微不足道，但可能比平均更强大，因为它允许模型区分一个邻居的节点和十个邻居的节点——而平均操作可能会抹掉这些信息。
*   **[图注意力网络](@article_id:639247)（GAT）**采用了一种更复杂的方法。它认识到并非所有邻居都生而平等。通过使用**注意力机制**（attention mechanism），节点可以动态地为每个邻居的消息（以及它自己的消息）分配一个“重要性分数”。

### 并非所有低语都等价：聚合的风格

为什么我们需要像注意力这样复杂的东西？考虑一个网络，其中连接的节点通常是*不同的*，这种特性称为**异质性**（heterophily）。在分子中，一个带正电的离子可能与带负电的离子成键。在引文网络中，一个领域的奠基性论文可能会被许多不同新兴领域的论文引用。

在这些低[同质性](@article_id:640797)的场景中，简单地平均邻居的特征会适得其反——它会模糊掉定义结构本身的差异。这就是 GAT 发挥作用的地方。通过学习注意力权重，一个节点可以学会更多地关注某些信息丰富的邻居的消息，并有效地忽略其他邻居。例如，在一个异质性图中，一个节点可能会学会对与自己最*不相似*的邻居分配高注意力，因为这种不相似性是理解其在网络中角色的关键。这种选择性地过滤邻域信息的能力使得基于注意力的模型在多样的图结构中特别鲁棒。[@problem_id:3106182] [@problem_id:2752979]

### 闲言碎语的危险：过平滑与深度限制

如果几层[消息传递](@article_id:340415)是好的，那么很多层会更好吗？不一定。与[扩散](@article_id:327616)的类比揭示了深度 GNN 的一个关键陷阱：**过平滑**（over-smoothing）。

当我们堆叠越来越多的层时，迭代平均过程会继续。来自越来越远地方的信息被混合到每个节点的表示中。经过多层之后，图中一个连通区域内的每个节点都收到了来自其他所有节点的消息。它们的[特征向量](@article_id:312227)在与所有其他向量反复平均后，会收敛到一个单一的、全局的平均值。它们变得无法区分。[@problem_id:2395461]

想象一个房间里坐满了人，每个人都在和邻居窃窃私语。最初，你听到的是局部的八卦。但如果这个过程持续足够长的时间，每个谣言都会传遍每个人，最后唯一能谈论的就只剩下所有谈话的乏味平均值。所有局部的、具体的、有趣的信息都丢失了。

这种表示崩溃是一种**[欠拟合](@article_id:639200)**（underfitting）。模型变得过于简单，无法区分节点，其预测能力急剧下降，不仅在新数据上如此，甚至在它训练过的数据上也是如此。这与我们更熟悉的**过拟合**（overfitting）问题有根本不同，[过拟合](@article_id:299541)是指模型可能变得过于复杂并记住了训练数据，例如通过学习将一个唯一的特征（如节点的任意 ID）与其标签关联起来。[@problem_id:3135731] 过平滑使模型变得太笨，而不是太聪明。节点[感受野](@article_id:640466)和其特征独特性之间的这种权衡，对许多 GNN 架构的深度施加了实际限制。

### 见树不见林：GNN 的盲点

[消息传递](@article_id:340415)的局部性也对 GNN 的[表达能力](@article_id:310282)施加了更根本的限制。一个基于[消息传递](@article_id:340415)的 GNN 根据其邻居特征的*多重集*来确定节点的新特征。它的“世界观”纯粹是局部的。这意味着，如果两个图的结构在局部上无法区分，一个简单的 GNN 将无法分辨它们。

典型的例子是一个 6 节点环（$C_6$）与两个独立的 3 节点三角形（$C_3 \cup C_3$）的对比。[@problem_id:3126471] 在这两个图中，每个节点都具有完全相同的局部结构：它与另外两个节点相连。如果我们用相同的[特征向量](@article_id:312227)初始化所有节点，那么在[消息传递](@article_id:340415)的每一步，每个节点都将执行相同的计算。最终所有节点将具有相同的[特征向量](@article_id:312227)，GNN 将无法区分单个环和两个独立的三角形。

这个局限性在形式上与一个经典的[图论算法](@article_id:327137)有关，即**一维 Weisfeiler-Lehman（1-WL）测试**。本质上，依赖于简单[消息传递](@article_id:340415)的 GNN 在区分[非同构图](@article_id:337723)方面的能力不比这个测试更强。它们擅长捕捉局部邻域模式，但可能对更全局的结构属性（如环的长度）视而不见。

### 给网络一个指南针：用[位置编码](@article_id:639065)打破对称性

我们如何帮助我们的 GNN 看出 $C_6$ 和两个 $C_3$ 之间的区别？我们需要打破对称性。我们需要给节点一些关于它们在图的全局结构中的“位置”或“角色”的感觉。

实现这一目标最优雅的方法之一是使用源自图自身结构的**[位置编码](@article_id:639065)**（positional encodings）。一个强大的工具是**[图拉普拉斯矩阵](@article_id:338883)**（Graph Laplacian），这是一个捕捉节点连接方式的矩阵。该拉普拉斯矩阵的[特征向量](@article_id:312227)代表了图的基本“[振动](@article_id:331484)模式”。通过提取与一个节点相对应的前几个[特征向量](@article_id:312227)的条目，我们可以创建一个独特的[特征向量](@article_id:312227)，作为一种[坐标系](@article_id:316753)，以反映其全局位置的方式[嵌入](@article_id:311541)该节点。[@problem_id:3189951]

当我们将这些[位置编码](@article_id:639065)作为初始特征输入 GNN 时，先前无法区分的节点（如 $C_6$ 环中的所有节点）现在就有了独特的标识符。GNN 现在可以利用这些初始差异来学习更强大的、具有结构感知能力的表示。这其中也存在一些微妙之处——例如，当[特征值](@article_id:315305)具有[多重性](@article_id:296920)时，相应的[特征向量](@article_id:312227)不是唯一的，可以任意旋转，从而产生歧义。尽管如此，用结构信息丰富节点特征的想法是 GNN 研究的一个关键前沿，它使 GNN 能够克服其固有的局部性，以更高的清晰度感知图的全局架构。

