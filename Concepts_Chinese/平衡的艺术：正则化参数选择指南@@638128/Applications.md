## 应用与跨学科联系

我们已经花了一些时间学习正则化的形式化机制，这是一套用于驯服[不适定问题](@entry_id:182873)的优美数学思想。但科学不仅仅是欣赏这些机制；它是关于用它们来理解世界。现在，我们将领略其诗意之美。我们将穿越一片看似不相关的领域——从窥探人体内部，到绘制地球深处，再到预测金融市场——并欣喜地发现，同样的原理无处不在。正则化不仅仅是一种数学技巧；它是一种在不确定性下进行推理的基本语言，在所有科学和工程领域通用。

每个应用中的挑战都是选择正则化参数，即我们写成 $\alpha$ 的那个小旋钮，它调整我们假设的“强度”。我们如何设置这个旋钮？事实证明，答案取决于另一个问题：我们对自己的问题了解多少？这引出了三种宏大策略，每种策略都针对不同的知识状态量身定制。

### 偏差原则：相信你的噪声

想象你是一位艺术品修复师，正在修复一张褪色的照片。你知道拍摄这张照片的相机有一定的颗粒感，一个特有的噪声水平。当你努力锐化图像时，你应该做到什么程度？何时停止？试图去除*每一个瑕疵*是愚蠢的，因为你不可避免地会开始移除场景的真实细节，将它们误认为是噪声。一个更明智的策略是清理图像，直到剩下的“污垢”——即你修复后的图像与原始褪色图像之间的差异——在统计上看起来与相机固有的颗粒感相同。当残差与噪声匹配时，你就停下来。

这就是 Morozov 偏差原则的精髓。当我们对测量噪声水平（通常用其[方差](@entry_id:200758) $\sigma^2$ 表示）有可靠的估计时，这是首选方法。我们选择[正则化参数](@entry_id:162917) $\alpha$，使得我们的解恰好拟合数据，留下的残差大小与已知的噪声水平一致。例如，在医学[图像去模糊](@entry_id:136607)中，传感器噪声可以被仔细校准，该原则使我们能够创建更清晰的图像，而不会捏造虚假细节 [@problem_id:3200560]。

同样的想法也出现在更复杂的系统中。在现代[天气预报](@entry_id:270166)中，数据同化将大气的物理模型与数百万个真实世界的观测数据相结合。用于寻找最佳大气状态的成本函数是一种[正则化最小二乘法](@entry_id:754212)，它在模型预测和带噪数据之间进行平衡。调整观测值的相对权重至关重要，而一种强有力的方法是确保分析结果与观测值之间的最终不匹配与已知的仪器[统计误差](@entry_id:755391)相符，这种方法正是广义偏差原则的实际应用 [@problem_id:3361694]。最后，在一个美妙的转折中，这个原则甚至可以用来理解其他领域的专门技术。在[计算电磁学](@entry_id:265339)中，一种旨在解决[电磁散射](@entry_id:182193)计算问题的工程“修复”方法，称为[组合场积分方程 (CFIE)](@entry_id:747496)，可以被重新解释为一种[Tikhonov正则化](@entry_id:140094)。在这里，选择CFIE的混合参数可以由偏差原则指导，从而将一个特定领域的解决方案与一个普适的统计思想联系起来 [@problem_id:3338383]。

### [L曲线](@entry_id:167657)：寻找最佳点

如果你不知道噪声水平怎么办？如果你正在探索一个新系统，其中的[测量误差](@entry_id:270998)是个谜怎么办？偏差原则就[无能](@entry_id:201612)为力了。我们需要一个不同的向导。让我们回到艺术品修复师那里。在不知道相机颗粒度的情况下，她必须做出判断。她可以创建一系列修复版本，从非常模糊（强正则化）到非常嘈杂（弱正则化）。她注意到一种权衡：当她使图像更清晰时（减少对解的惩罚），图像开始变得不像原始数据（[数据失配](@entry_id:748209)度增加）。她可以绘制出这种权衡关系：一个轴是解的“平滑度”，另一个轴是它与数据的不一致程度。她通常会发现一条形如字母“L”的曲线。在L的垂直部分，使解更平滑并不会对[数据拟合](@entry_id:149007)造成太大影响。在水平部分，让[数据拟合](@entry_id:149007)得稍好一点就需要使解急剧变得不平滑且更嘈杂。“最佳点”，即最优[平衡点](@entry_id:272705)，就位于这个[L曲线](@entry_id:167657)的拐角处。这种几何方法使我们能够在没有任何关于噪声的先验知识的情况下，选择一个合理的 $\alpha$。

[L曲线法](@entry_id:751079)是许多领域的主力。当化学家使用[光谱学](@entry_id:141940)来识别分子时，重叠的[光谱](@entry_id:185632)峰会使信号变得一团糟。对这个信号进行[反卷积](@entry_id:141233)是一个[不适定问题](@entry_id:182873)，[L曲线](@entry_id:167657)提供了一种稳健、可视化的方法来选择正则化参数，以获得最清晰、最可信的[光谱](@entry_id:185632) [@problem_id:3711446]。同样的想法可以从一个更深刻的角度来看：[多目标优化](@entry_id:637420)。解决一个正则化问题等同于在一个双目标问题的帕累托前沿上找到一个点，你同时试图最小化[数据失配](@entry_id:748209)度和正则化惩罚项。[L曲线](@entry_id:167657)不过是这个[帕累托前沿](@entry_id:634123)的可视化，其拐角代表了在这两个相互竞争的目标之间的一个折衷点 [@problem_id:3154126]。

### 交叉验证：让数据做主

还有第三种，也许更强大的理念。与其依赖关于噪声的假设或几何直觉，为什么不让数据本身告诉我们哪个模型是最好的呢？科学模型的最终检验是其预测新事物的能力。这就是交叉验证的基础。

这个想法非常简单。我们拿我们的数据集，假装其中一小部分缺失了。然后我们用剩余的数据来为一系列不同的 $\alpha$ 值构建模型。对于每个 $\alpha$，我们检查得到的模型预测我们隐藏的“缺失”数据的效果如何。我们重复这个过程，每次隐藏不同部分的数据，然后对结果取平均。在未见过的数据上给出最佳预测性能的 $\alpha$ 值就是我们的获胜者。这就像给我们的模型进行一系列模拟考试，看看哪种“学习方法”（哪个 $\alpha$）能让它为最终测试做好最佳准备。

在预测准确性是最终目标且[噪声模型](@entry_id:752540)不可靠的领域，这种方法是不可或缺的。在金融领域，人们可能会建立一个线性模型来从各种因素预测股票回报，那里的噪声是出了名的混乱且随时间变化。目标不是找到“真实”的参数，而是找到在未来数据上表现良好的稳定参数。交叉验证是这项工作的完美工具，它直接优化样本外性能 [@problem_id:3200560]。这项技术在像地球物理层析成像这样的大规模科学问题中达到了其复杂性的顶峰。当根据地震走时数据绘制地球地下结构图时，我们可以使用一种称为“[留一法交叉验证](@entry_id:637718)”([LOOCV](@entry_id:637718)) 的形式，我们系统地排除每一个数据点，并评估用所有其他数据点建立的模型对它的预测效果如何。虽然这听起来在计算上是噩梦般的，但一个使用“影响矩阵”的优美数学捷径使其变得可行，提供了一种高度稳健、数据驱动的方法，不仅可以选择正则化参数，甚至可以选择模型离散化的参数 [@problem_id:3585099]。

### 模型的灵魂：选择正确的简洁性

到目前为止，我们一直专注于选择正则化的*量*，即 $\alpha$。但有一个更深层次的问题：我们在寻找*哪种*简洁性？这被编码在正则化算子中，即惩罚项 $\alpha \|L x\|_2^2$ 中的矩阵 $L$。$L$ 的选择是我们模型灵魂的表达；它是我们对解的物理直觉在数学中的体现。

在某些情况下，最简单的假设就是解的参数不应过大。在金融模型中，或者在设置核[磁共振](@entry_id:143712)谱仪中数十个匀[场线](@entry_id:172226)圈的电流以使[磁场](@entry_id:153296)均匀化时，我们想要稳定、不极端的数值。在这里，选择是 $L=I$，即单位矩阵，它惩罚解向量本身的平方大小。这是经典的Tikhonov或“岭”回归，是高维模型中对抗过拟合的有力武器 [@problem_id:3726320] [@problem_id:3200560]。

然而，在许多物理问题中，我们期望空间上的平滑性。一张去模糊的医学图像不应看起来像一堆随机像素；相邻像素应具有相似的值。我们脚下的地质层在很大程度上是连续的。在这些情况下，我们选择 $L$ 为离散导数算子，如梯度 ($\nabla$) 或拉普拉斯算子 ($\Delta$)。这会惩罚剧烈变化或高曲率，迫使解变得平滑，这正是成像和许多地球物理问题所需要的 [@problem_id:3200560] [@problem_id:2589999]。

但如果世界不是那么简单呢？如果我们期望我们的解*大部分*是简单的，但包含一些关键的、剧烈的变化呢？想象一种用于太阳能电池的新材料，由几个不同的层组成。材料特性，如[电荷](@entry_id:275494)[载流子复合](@entry_id:195598)率，可能在每层内部是恒定的，但在界面处会突然跳变。标准的[平滑度惩罚](@entry_id:754985)会模糊掉这些重要的边界。我们需要一种不同的简洁性：[稀疏性](@entry_id:136793)。不是解本身的[稀疏性](@entry_id:136793)，而是其*导数*的稀疏性。函数之所以简单，是因为它的梯度[几乎处处](@entry_id:146631)为零。为了促进这一点，我们从 $\ell_2$（平方）惩罚切换到 $\ell_1$（[绝对值](@entry_id:147688)）惩罚。这产生了总变差正则化，一种能够神奇地保留锐利边缘同时平滑掉其间噪声的方法。这是重建先进半导体器件分层结构的关键 [@problem_id:2850652]。

用 $\ell_1$ 范数促进稀疏性的思想是现代数据科学中最具变革性的概念之一。如果我们将它应用于解向量本身（一种称为LASSO的方法），而不是其导数，它会做一些了不起的事情：它迫使解的许多分量恰好为零。它起到了自动特征选择工具的作用。想象一下，试图找出数百个[编译器优化](@entry_id:747548)标志中哪些真正影响程序的运行时间。通过将运行时间变化建模为标志的线性组合并应用LASSO正则化，我们可以发现那少数几个真正起作用的标志，而所有无关标志的系数都被驱动为零 [@problem_id:3154709]。

从寻找导致某种疾病的最重要基因，到寻找经济中最具影响力的因素，原理是相同的：在一个极其复杂的世界里，我们寻求优雅、稀疏的解释。而正则化，以其多种形式，是让我们找到它的工具。它是连接寻找清晰图像、稳定投资、真实物理定律和更快计算机程序的统一线索。它是有原则的妥协艺术，无处不在。