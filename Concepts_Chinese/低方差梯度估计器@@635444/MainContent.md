## 引言
现代科学和人工智能越来越依赖于优化复杂[随机过程](@entry_id:159502)的结果。从训练[强化学习](@entry_id:141144)智能体到设计新材料，我们常常需要调整系统参数以提高*期望*性能。这需要计算期望的梯度，而这项任务的复杂性在于，我们优化的参数本身也决定了我们进行平均所依据的[概率分布](@entry_id:146404)。这个“如何对期望求导”的基本挑战，阻碍了标准优化工具的直接应用。本文通过探讨估计这些难以捉摸的梯度的基础技术，来揭开这个问题的神秘面纱。它将引导读者了解解决此问题的两条主要路径：优雅的[重参数化技巧](@entry_id:636986)和通用的[分数函数](@entry_id:164520)估计器。我们将深入探讨它们的核心原理，审视区分二者的关键问题——[方差](@entry_id:200758)，并探索驯服[方差](@entry_id:200758)的艺术。通过理解这一工具集，我们可以在众多复杂系统中实现稳定而高效的优化。

## 原理与机制

想象一下，你正在尝试完善一种新型面包的配方。面包的最终口感取决于许多参数——酵母的用量、[发酵](@entry_id:144068)时间、烤箱温度——但其中也存在纯粹的偶然因素。每一条面包都略有不同。你的目标是调整参数，使*平均*而言的面包尽可能美味。你如何知道应该朝哪个方向调整旋钮呢？如果某一条面包做得非常完美，是因为你的新设置，还是仅仅是运气好？

这正是现代科学和工程领域中大量问题的核心挑战，从训练人工智能到设计新材料，再到模拟粒子物理学。我们通常希望优化一个[随机过程](@entry_id:159502)的*期望*结果。在数学上，我们有一个函数 $f(x)$，它告诉我们一个结果 $x$ 有多好；而获得该结果的概率取决于我们的参数 $\theta$，记为 $p_\theta(x)$。我们的目标是找到期望结果的梯度 $\nabla_\theta \mathbb{E}_{x \sim p_\theta(x)}[f(x)]$，以便使用[基于梯度的优化](@entry_id:169228)方法来找到 $\theta$ 的最佳设置。

困难在于那个位于期望 $\mathbb{E}$ 之外的小小符号 $\nabla_\theta$。参数 $\theta$ 不仅影响平均值内部的数值，它们还改变了我们进行平均所依据的结果[分布](@entry_id:182848)本身。这就像在试图测量一片景观的平均高度，而地面本身却在你的脚下倾斜和扭曲。你不能简单地将[梯度算子](@entry_id:275922)移入期望内部。这样做就好比只测量了山丘的坡度，却忽略了整个地势的倾斜。

幸运的是，解决这个问题的方法不止一种。科学家和数学家们已经发现了两种极其优美而强大的策略来应对这一挑战。它们之间的选择取决于一个简单的问题：我们系统的哪一部分可以被视为平滑可微的？是生成结果的过程，还是结果本身的概率？

### [重参数化技巧](@entry_id:636986)：塑造随机性

第一条路径或许更为优雅，它是一种巧妙的视角转换，被称为**[重参数化技巧](@entry_id:636986)**。想象一位雕塑家想要创作一尊雕像。一种方法是从一块神奇的大理石开始，其形状取决于某个参数 $\theta$。要弄清楚如何改进雕像，他们必须理解这块复杂且依赖于参数的大理石。但如果他们总是从一块标准的、简单的大理石开始——一块完全独立于他们参数的大理石——然后使用一个由 $\theta$ 控制设置的、确定性的、可微的工具来塑造它呢？

现在，因果链变得清晰无比。参数 $\theta$ 控制工具，工具塑造大理石，大理石的形状决定了其品质。要观察 $\theta$ 的微小变化如何影响最终的雕像，我们只需沿着这条可微的路径进行追溯即可。

这就是重[参数化](@entry_id:272587)的精髓。我们将随机结果 $x$ 不再视为来自复杂[分布](@entry_id:182848) $p_\theta(x)$ 的样本，而是重写为一个确定性且可微的函数 $g_\theta(\epsilon)$，该函数依赖于我们的参数 $\theta$ 和一个从简单的、固定的[分布](@entry_id:182848) $p(\epsilon)$ 中抽取的“标准”噪声变量 $\epsilon$。我们那个困难的梯度问题 $\nabla_\theta \mathbb{E}_{x \sim p_\theta(x)}[f(x)]$，就转变成了更易于处理的形式：
$$ \nabla_\theta \mathbb{E}_{\epsilon \sim p(\epsilon)}[f(g_\theta(\epsilon))] $$
由于现在的期望是针对一个不依赖于 $\theta$ 的固定“景观” $p(\epsilon)$ 计算的，我们终于可以把梯度移到内部了！
$$ \mathbb{E}_{\epsilon \sim p(\epsilon)}[\nabla_\theta f(g_\theta(\epsilon))] $$
期望内部的梯度 $\nabla_\theta f(g_\theta(\epsilon))$ 现在可以用标准的微积分[链式法则](@entry_id:190743)计算，这正是[反向传播算法](@entry_id:198231)在[神经网](@entry_id:276355)络中所做的事情。这种估计器通常被称为**路径导数**，因为它遵循从参数到最终结果的确定性路径。

一个典型的成功应用是在训练[变分自编码器](@entry_id:177996)（VAEs）中，这是一种用于学习复杂数据（如[生物序列](@entry_id:174368)）表示的[深度学习模型](@entry_id:635298)[@problem_id:2439762]。VAE 可能会尝试学习基因表达向量的压缩表示，即[潜变量](@entry_id:143771) $z$。这个潜变量通常被建模为从一个高斯分布中采样，其均值 $\mu_\phi$ 和[标准差](@entry_id:153618) $\sigma_\phi$ 由一个[神经网](@entry_id:276355)络输出。我们不从这个复杂且依赖参数的[分布](@entry_id:182848) $\mathcal{N}(z; \mu_\phi, \sigma_\phi^2)$ 中采样 $z$，而是对其进行重[参数化](@entry_id:272587)。我们采样一个简单的、无参数的噪声变量 $\epsilon \sim \mathcal{N}(0, 1)$，然后将 $z$ 计算为一个确定性函数：
$$ z = \mu_\phi + \sigma_\phi \cdot \epsilon $$
现在，[损失函数](@entry_id:634569)相对于网络参数 $\phi$ 的梯度就可以通过这个方程，经由反向传播顺畅地流动。这是一个绝妙的技巧，将一条阻塞的路径变成了梯度的高速公路。

这种方法具有惊人的通用性。它适用于**位置-尺度族**中的任何[分布](@entry_id:182848)，甚至可以通过更高级的技术，如隐式重[参数化](@entry_id:272587)，扩展到像 Gamma 或学生 $t$ [分布](@entry_id:182848)这样不属于简单位置-尺度族的[分布](@entry_id:182848)[@problem_id:3357989] [@problem_id:3511433]。关键在于我们必须能够将随机结果表示为参数的[可微函数](@entry_id:144590)。当这成为可能时，得到的[梯度估计](@entry_id:164549)器不仅是无偏的，而且通常[方差](@entry_id:200758)很低，从而使学习过程高效而稳定。

### [分数函数](@entry_id:164520)：为结果加权

但如果重[参数化](@entry_id:272587)不可能呢？这种情况经常发生。最常见的情况是当[随机变量](@entry_id:195330)是**离散**的时候。你无法对一次抛硬币的结果相对于硬币的重量[分布](@entry_id:182848)求导。另一种情况是我们正在评估的函数 $f(x)$ 本身是不可微的，比如一个[阶跃函数](@entry_id:159192)[@problem_id:3157956]。在这些情况下，从参数到结果的路径是断裂的，路径导数会失效，通常导致估计器的信号为零而[方差](@entry_id:200758)为无穷大。

这时，我们必须走第二条路。如果我们不能对*结果*求导，或许我们可以处理结果的*概率*。这就引出了**[分数函数](@entry_id:164520)估计器**，在[强化学习](@entry_id:141144)中也以 **REINFORCE** 之名而广为人知。该方法建立在一个简单的微积分恒等式上，通常称为对数-导数技巧：$\nabla_\theta p_\theta(x) = p_\theta(x) \nabla_\theta \log p_\theta(x)$。

让我们用一个类比来说明。想象你是一位管理竞选活动的政治策略师。你的参数 $\theta$ 是你的竞选策略。结果 $f(x)$ 是你从特定选民群体 $x$ 中获得的票数。你无法直接改变选民的偏好（函数 $f(x)$ 对他们来说是固定的）。但你的策略 $\theta$ 可以改变某个选民群体前来投票的概率 $p_\theta(x)$。[分数函数](@entry_id:164520)梯度确切地告诉你如何更新你的策略：
$$ \nabla_\theta \mathbb{E}_{x \sim p_\theta(x)}[f(x)] = \mathbb{E}_{x \sim p_\theta(x)} [f(x) \nabla_\theta \log p_\theta(x)] $$
$\nabla_\theta \log p_\theta(x)$ 这一项被称为**分数**。它指向[参数空间](@entry_id:178581)中能使结果 $x$ 更有可能发生的方向。该公式告诉我们，通[过采样](@entry_id:270705)一个结果 $x$，评估其质量 $f(x)$，然后将我们的参数 $\theta$ 朝着分数的方向微调，微调的幅度由结果的好坏程度来决定。简而言之，如果我们得到了一个高回报的结果，我们就“强化”那些使该结果更可能出现的参数。

这种方法具有极高的通用性。无论 $x$ 是离散的还是连续的，也无论 $f(x)$ 是一个奇异的、不可微的函数，只要我们能计算出我们行动的对数概率及其导数，我们就能估计梯度。这使其成为训练带有离散[潜变量](@entry_id:143771)的模型 [@problem_id:3107989] 和在强化学习中进行[策略优化](@entry_id:635350) [@problem_id:3157956] 不可或缺的工具。

### 无法避免的[方差](@entry_id:200758)问题

所以我们有两条主要路径：重[参数化](@entry_id:272587)，高效但适用范围有限；以及[分数函数](@entry_id:164520)，通用但有一个主要缺陷：**高[方差](@entry_id:200758)**。

[分数函数](@entry_id:164520)估计器的高[方差](@entry_id:200758)很容易从直观上理解。梯度的单一样本是奖励 $f(x)$ 和分数 $\nabla_\theta \log p_\theta(x)$ 的乘积。这两个量都可能剧烈波动。你可能仅仅因为运气好，就从一个实际上非常不可能的行动中获得了非常高的奖励。估计器会看到这个高奖励，并告诉你大幅增加该行动的概率，这可能将你引[向错](@entry_id:161223)误的方向。这就像一个赌徒在老虎机上中了大奖，便错误地断定这台机器“正热”，应该把所有的钱都投进去。这种充满噪声的信号使得学习过程缓慢且不稳定。

实证比较显示了这种巨大的[方差](@entry_id:200758)差异。在模拟中，对于同一个问题，[分数函数](@entry_id:164520)估计器的[方差](@entry_id:200758)可能比路径导数估计器的[方差](@entry_id:200758)高出几个[数量级](@entry_id:264888)[@problem_id:3511433]。驯服这种[方差](@entry_id:200758)是[随机优化](@entry_id:178938)领域的一大追求。

### 驯服野兽：[方差缩减](@entry_id:145496)的艺术

如果说[分数函数](@entry_id:164520)估计器是一头野兽，那么[方差缩减技术](@entry_id:141433)就是我们用来驯服它的工具。许多这类技术的核心思想是一个优美的统计学概念：**[控制变量](@entry_id:137239)**。

最简单的形式是**基线**。事实证明，你可以从你的奖励 $f(x)$ 中减去任何不依赖于具体结果 $x$ 的值 $b$，而不会改变梯度的[期望值](@entry_id:153208)：
$$ \nabla_\theta \mathbb{E}[f(x)] = \mathbb{E}[(f(x) - b) \nabla_\theta \log p_\theta(x)] $$
这是因为你增加的项 $\mathbb{E}[-b \nabla_\theta \log p_\theta(x)]$ 等于 $-b \cdot \mathbb{E}[\nabla_\theta \log p_\theta(x)]$，而分数的一个基本性质是其期望为零。这背后的直觉非常有力：你只需要从奖励中*出乎意料*的部分进行学习。任何你本就期望得到的奖励（基线 $b$）不提供任何新信息。通过选择一个接近平均奖励的基线 $b$，$(f(x)-b)$ 这一项的平均值会变得小得多，从而极大地降低了[梯度估计](@entry_id:164549)的[方差](@entry_id:200758) [@problem_id:3121685] [@problem_id:3357989]。

更高级的技术则更进一步。在某些情况下，我们可以用其部分平均的、解析的期望来替换充满噪声的、采样的奖励 $f(x)$，这种技术被称为 Rao-Blackwellization [@problem_id:3157956]。其他方法，如 SVRG 和 SAGA，则利用过去梯度或整个数据集的梯度作为强大的[控制变量](@entry_id:137239)，以便在优化一个大型有限数据集时降低[方差](@entry_id:200758) [@problem_id:3150923]。

有时，降低[方差](@entry_id:200758)的最佳方法是放弃完全无偏的梯度。这就引出了一个引人入胜的**偏差-方差权衡**。对于[离散变量](@entry_id:263628)，我们可以使用**连续松弛**来代替无偏但高[方差](@entry_id:200758)的[分数函数](@entry_id:164520)估计器。像 **[Gumbel-Softmax](@entry_id:637826)** 估计器这样的方法为离散选择创建了一个“软”的、可微的近似，从而允许计算低[方差](@entry_id:200758)的路径导数[@problem_id:3121685]。这个梯度是*有偏*的——它指向的是松弛问题的最优解，而非原始问题的最优解——但这个偏差通常可以通过一个“温度”参数来控制。类似地，像**直通估计器（STE）**这样的[启发式方法](@entry_id:637904)在反向传播时直接忽略了不[可微性](@entry_id:140863)，提供了一个有偏但有用的低[方差](@entry_id:200758)信号，在实践中效果出奇地好 [@problem_id:3107976]。

这种在[偏差和方差](@entry_id:170697)之间进行可控权衡的思想是一个反复出现的主题。在[强化学习](@entry_id:141144)中，像广义优势估计（GAE）和 TD($\lambda$) 这样的方法提供了一个参数 $\lambda$，它明确地在高偏差、低[方差](@entry_id:200758)的估计器和低偏差、高[方差](@entry_id:200758)的估计器之间进行插值，让实践者可以为他们的问题选择谱系上的正确点 [@problem_id:3158027] [@problem_id:2738648]。

### 宏大统一的视角

从一个单一的障碍——如何对期望求导——开始，最终发展成为一个丰富且相互关联的估计器体系。没有哪一种是“最佳”方法，而是一个适用于不同情况、有原则的工具包。

-   如果你的[随机变量](@entry_id:195330)可以表示为参数和噪声的[可微函数](@entry_id:144590)，那么**路径导数**（[重参数化技巧](@entry_id:636986)）是首选方法：它无偏且[方差](@entry_id:200758)极低。

-   如果你的变量是离散的，或者你的目标函数是不可微的，你有以下选择：
    -   为了获得**无偏**估计，你必须使用**[分数函数](@entry_id:164520)估计器**。但切勿使用其原始形式；务必用基[线或](@entry_id:170208)其他[控制变量](@entry_id:137239)来驯服其高[方差](@entry_id:200758)。
    -   如果你能容忍一定的**偏差**以换取更低的[方差](@entry_id:200758)和稳定性，那么像 [Gumbel-Softmax](@entry_id:637826) 这样的连续松弛或像 STE 这样的启发式方法是强大的实用工具。
    -   并且，永远不要忘记最简单的解决方案：如果你的[离散空间](@entry_id:155685)足够小，你可以**精确地对所有可能性求和**！这完全消除了采样的需要，并给出了一个精确的、零[方差](@entry_id:200758)的梯度 [@problem_id:3357989]。

这个框架的真正力量体现在那些包含不同类型随机性的复杂模型中。在单个[计算图](@entry_id:636350)中，你可能会发现自己对一个连续变量使用路径导数，而对一个离散选择节点使用带有基线的[分数函数](@entry_id:164520)估计器 [@problem_id:3107989]。理解这些原理使我们能够组合这些解决方案，为每个任务应用正确的工具，并成功地驾驭我们试图建模和优化的这个复杂、随机而美丽的世界。

