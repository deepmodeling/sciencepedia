## 应用与跨学科联系

在我们穿越了神经网络的各种零部件——[神经元](@article_id:324093)、权重、沿[山坡](@article_id:379674)滚下的梯度——之后，人们可能会想坐下来欣赏这台错综复杂的机器。但这就像研究了鸟的解剖结构，却从未看过它飞翔。这些思想真正的灵魂，其固有的美丽与力量，只有在我们将它们付诸实践时才能显现。我们能用这些会思考的机器*做*什么？事实证明，它们的应用不仅数量众多，而且具有变革性，将看似不相关的人类探究领域用一根共同的线索编织在一起。我们学到的原理不仅仅是计算机科学的技巧；它们是一个看待世界的新视角，从生命密码到物理定律。

### 看见世界：从图像到基因组

我们自己的大脑是[视觉处理](@article_id:310479)的大师。我们能毫不费力地在人群中认出朋友，从沙发后探出头的猫，或句子中一个错位的逗号。我们已经知道，[卷积神经网络](@article_id:357845)（CNNs）的灵感正是来源于此过程，它采用一种局部[特征检测](@article_id:329562)器的层次结构，从简单的部分构建出一幅完整的图景。

但“局部性”究竟给我们带来了什么好处？想象一下，你正在训练一个 CNN 来识别一个特定物体。在一个完美、干净的实验室里，这很容易。但真实世界是混乱的。一个物体可能被部分遮挡、弄脏或在光线不佳的情况下看到。一个试图一次性匹配*整个*图像的幼稚模型将是极其脆弱的。然而，CNN 是不同的。它的卷积滤波器就像一个个小侦探，每个只负责场景的一小块。一个可能寻找水平边缘，另一个寻找一块红色，还有一个寻找毛茸茸的纹理。最终的决定是基于所有这些局部观察的投票。如果物体的一小部分被隐藏，只有分配到那个特定区域的少数侦探会感到困惑。其他的侦探，看到它们熟悉的线索，仍然可以做出自信的识别。这种对局部“损伤”的固有鲁棒性是网络[局部感受野](@article_id:638691)及其响应池化的直接结果 [@problem_id:3126215]。

这种“扫描之眼”的想法是如此强大，以至于我们可以将其应用于远超常规图像的领域。考虑一下基因组，那本用 A、C、G、T 四个字母写成的生命之书。我们可以将 DNA 序列视为一维“图像”，并使用 CNN 来扫描它，寻找可能预示基因开始或蛋白质结合位点的有意义的模式，或称“基序”。

现在，科学变得有趣了。生物学家发现生命的字母表不仅仅是四个字母。胞嘧啶（C）可以被化学修饰，产生第五个字母，[5-甲基胞嘧啶](@article_id:372017)（$5\mathrm{m}C$），它在调控哪些基因被开启或关闭方面起着关键作用。我们那个被训练来阅读四字母表的 CNN 会如何适应？突然间，我们被迫像工程师一样思考。增加第五个字母意味着我们的独热输入向量需要五个通道而不是四个。与此输入直接接触的第一个卷积层必须增长；它的核现在需要一套新的权重来学习这个新字母的重要性。这个第一层中可学习参数的数量增加了，直接反映了输入字母表复杂性的增加。

此外，我们必须重新考虑基本的对称性。DNA 的一个关键特性是反向互补性：一条链上的序列 `AGTC` 对应于另一条链上的 `GACT`。在许多情况下，一个稳健的生物学模型应该将这两者视为等同。对于四个字母，这种对称性对应于我们输入通道的一个简单[置换](@article_id:296886)（$A \leftrightarrow T$, $C \leftrightarrow G$）。但是对于我们新的五字母表，问题出现了：普通的胞嘧啶（$C$）和甲基化的胞嘧啶（$5\mathrm{m}C$）都与鸟嘌呤（$G$）互补。这种映射不再是一对一的[置换](@article_id:296886)。我们简单的对称性技巧被打破了！这表明，将神经网络应用于科学发现并非“即插即用”那么简单。它需要对工具和应用领域的原理进行深入、周到的思考 [@problem_id:2382323]。

### 理解序列：从语言到生命密码

世界上许多最有趣的现象并非像照片一样静止不动。它们在时间中展开：一句说出的话、一段音乐、股票的波动价格，或是氨基酸序列折叠成蛋白质的过程。要理解这些，你需要的不仅仅是一个快照；你需要记忆。这就是[循环神经网络](@article_id:350409)（RNNs）的领域。

RNN 每次处理序列中的一个元素，维持一个内部的“[隐藏状态](@article_id:638657)”，作为它到目前为止所见一切的总结。当你读一个故事时，你对当前句子的理解会受到前面段落的影响。RNN 做的也是同样的事情。然而，对于像离线语音识别这样的任务，我们可以做得更好。在[转录](@article_id:361745)录音时，人类听者可能会停下来，根据*后面*的内[容重](@article_id:338804)新考虑一个词。双向 RNN 通过使用两个并行的网络来模仿这一点：一个从过去到未来读取序列，另一个从未来到过去读取。在每个时间点，模型的决策都基于完整的上下文。这种“向前看”的能力对于解决歧义非常有价值。一个可能是“I”或“eye”的声音，当未来的上下文揭示出“scream”或“lash”这样的词时，就很容易被消除歧义 [@problem_id:3103017]。

同样的原理使我们能够探究序列内部变化的后果。让我们回到基因组。一个“[移码突变](@article_id:299296)”——单个[核苷酸](@article_id:339332)的插入或删除——就像一个毁灭性的打字错误，会使余下的遗传句子变得混乱。我们可以用 RNN 来模拟这个过程。我们处理原始的、健康的 DNA 序列，并跟踪网络隐藏状态的演变。然后，我们对突变的序列做同样的事情。在突变点之前，两个隐藏状态的轨迹是相同的。但在单个字母改变之后，它们立刻开始分歧。通过测量这些内部表示在每个后续位置之间的距离，我们得到了序列“意义”如何逐渐被破坏的量化图像。一个微小的、局部的变化级联成对最终功能的巨大、全局的影响，我们可以直接在网络的行为中看到这一点 [@problem_id:2425716]。这将 RNN 从一个简单的预测器转变为一个理解动态过程的工具。

### 建模我们的世界：从分子到生态系统

世界并不总是一个整齐的网格或一条简单的线。通常，它的结构是一个复杂的关系网：一个分子是原子通过[化学键](@article_id:305517)连接而成的图；一个社会是人通过关系连接而成的网络；一个生态系统是物种通过捕食和[共生](@article_id:302919)连接而成的网。[图神经网络](@article_id:297304)（GNNs）就是为学习这类关系数据而设计的。GNN 通过在连接的节点之间传递“消息”来工作，允许每个节点根据其邻居的状态更新自己的状态。经过几轮[消息传递](@article_id:340415)后，每个节点的表示都反映了其局部邻域的结构。

考虑预测一个分子是否有毒的挑战。毒性通常是由一种特定的原子局部[排列](@article_id:296886)，即“毒性基团”引起的。GNN 天然适合这个问题，因为它可以学习识别这些局部[子图](@article_id:337037) [@problem_id:2395462]。这就引出了一个关于[迁移学习](@article_id:357432)的迷人问题：如果我们用一个庞大的小型、简单分子库来训练一个 GNN，它能否被用来在一个巨大的蛋白质中找到有毒的肽段？GNN 的原理告诉我们，这是可行的，*如果*毒性是一个局部属性，并且 GNN 有足够的层数来“看到”整个毒性基团。

当我们融合来自完全不同世界的信息时，神经网络的真正灵活性才得以展现。想象一下预测一种疾病媒介（如蚊子）的地理传播的任务 [@problem_id:2373359]。你需要什么信息？你可能想看看卫星图像，以判断地形是否合适（是否有积水？）。这是 CNN 的工作。你还需要气候数据（天气是否温暖潮湿？）。这是一个简单的数字向量。至关重要的是，你还想知道人类的流动性（人们是否从当前受感染的地区旅行过来？）。这可以用一个流矩阵来表示。神经网络提供了一个统一的框架来融合这些异构信息源。它可以有一个用于图像的卷积分支，一个用于气候数据的简单线性分支，以及另一个用于处理流动性暴露的分支。网络学会了如何权衡来自每个来源的证据——CNN 的“眼睛”、气候输入的“温度计”和流动性数据的“社会学家”——以做出一个单一、综合的预测。

### 从学习模式到发现定律

到目前为止，我们主要将[神经网络](@article_id:305336)视为从数据中学习模式的识别器。但其应用的一个深刻转变是利用它们来求解科学的基本方程本身。这就是物理知识通知的神经网络（PINNs）的世界。

传统的[数值方法](@article_id:300571)可能会通过将[空间离散化](@article_id:351289)成一个精细的网格来求解微分方程。PINN 采取了一种不同的、相当大胆的方法。[神经网络](@article_id:305336)本身成为候选解。它的输入是一个坐标（如 $x$ 和 $t$），它的输出是函数在该坐标处的值。我们如何训练它？我们不是在数据点上训练它。相反，我们是在物理定律本身上训练它。[损失函数](@article_id:638865)是[微分方程](@article_id:327891)的[残差](@article_id:348682)。网络的[导数](@article_id:318324)使用[自动微分](@article_id:304940)精确计算，优化过程迫使网络找到一个使[残差](@article_id:348682)在任何地方都尽可能接近于零的函数。本质上，网络因违反物理定律而受到惩罚 [@problem_id:3214094]。这将网络重新定义为现代版经典[数值分析](@article_id:303075)[配置法](@article_id:299333)中的[函数逼近](@article_id:301770)器。我们甚至可以通过构造来内置边界条件，创建一个通过定义就满足它们的架构，从而让优化器专注于满足内部的定律 [@problem_id:3214094]。

这种“物理知识通知”的理念甚至可以延伸得更远。许多科学模型是混合的：它们包含源自第一性原理的部分，以及其他根据实验拟合的经验性、手动[参数化](@article_id:336283)的函数。我们现在可以用灵活的、数据训练的[神经网络](@article_id:305336)来取代这些脆弱的、手动调整的组件。例如，在计算化学中，[半经验方法](@article_id:355786)使用简化的积分和[参数化](@article_id:336283)函数来近似量子力学的昂贵计算。通过用[神经网络](@article_id:305336)取代这些参数，我们可以创建一个“数据驱动”的方法，该方法从高精度的参考计算中学习，同时保留原始物理模型的有效结构 [@problem_-id:2459241]。

然而，这种物理学和机器学习的融合带来了一个关键的警告：我们绝不能因噎废食。物理定律承载着关于世界的深刻真理，尤其是对称性。例如，各向同性材料——一种无论如何定向其行为都相同的材料——的[本构定律](@article_id:357811)必须尊重旋转对称性。如果我们训练一个“幼稚”的、基于坐标的网络来从仅在一个方向上进行的实验中学习这个定律，当在旋转后的样本上进行测试时，它将惨败。它没有各向同性的概念。但是，如果我们设计我们的[网络架构](@article_id:332683)以明确地强制执行这种对称性——无论是通过只接受[标量不变量](@article_id:372725)作为输入，还是通过使用等变结构——模型将变得数据效率更高、更稳健。一个单一的训练样本提供了关于一整族旋转状态的信息。这种对物理原理的编码，或称*归纳偏见*，可以说是机器学习在科学应用中最重要的概念 [@problem_id:2629354]。最成功的模型不是巨大的、无结构的黑匣子，而是精心制作的、体现了我们对世界现有知识的架构，无论这些知识是来自物理学 [@problem_id:2459241]、力学 [@problem_id:2629354]，还是甚至来自高效逼近的数学 [@problem_id:2432667]。

因此，宏大的图景是一种协同作用。[神经网络](@article_id:305336)并非能让[科学方法](@article_id:303666)过时的万能灵药。它们是一种极其新颖的工具，一种通用的、可微的黏土，可以被数据塑造，并且最强大的是，可以被我们试图理解的科学原理本身所塑造。前方的旅程不仅是构建人工智能，更是构建一种新型的科学仪器——一种会学习的仪器。