## 引言
对计算机完美、确定性逻辑的信念是我们信任计算的基石。然而，在科学模拟这一复杂领域，这种理想常常受到挑战，揭示出两次获得完全相同的结果并非理所当然，而是一个难以实现的目标。本文直面不可复现性问题，这是验证和发展计算研究的关键障碍。它探讨了我们理论模型与计算机、算法和复杂工作流实际运作方式之间令人惊讶的差距。在接下来的章节中，您将对这一挑战有深入的了解。首先，我们将剖析不可复现性的“原理与机制”，从随机性的幻象到[并行计算](@entry_id:139241)的混沌。随后，在“应用与跨学科联系”中，我们将遍览不同的科学领域，看看研究人员如何构建可复现的系统，将计算的艺术转变为可验证的科学。

## 原理与机制

一个普遍且令人安心的信念是，计算机是具有完美逻辑的机器。给它相同的程序和相同的数据，它每次都会输出完全相同的答案。毕竟，数字计算器不会感到疲倦或突然情绪变化；$2+2$ 总是等于 $4$。这种确定性的理想是我们建立对计算信任的基石。然而，在[科学模拟](@entry_id:637243)的世界里——我们探索从宇宙到细胞内部运作的宏大数字实验室——这种令人安心的信念是众多被打破的信念中的第一个。对可复现性的追求是一场进入我们理想化模型与计算机实际工作方式的混乱现实之间那些迷人且常常令人惊讶的差距的旅程。这段旅程揭示了，“两次获得相同答案”并非理所当然的小事，而是一个需要努力实现的深远目标。

### 确定性的幻象与随机性的幽灵

我们许多最强大的模拟技术，如**[蒙特卡洛方法](@entry_id:136978)**，都建立在概率的基础之上。为了计算一群分子的[平均能量](@entry_id:145892)，我们不会计算每个粒子的状态；相反，我们在可能性的空间中进行巧妙的[随机游走](@entry_id:142620)，对各种构型进行抽样并计算其属性的平均值。但是，一台完全逻辑化的机器从哪里获得其“随机性”呢？

它没有。它拥有的是**[伪随机性](@entry_id:264938)**。**[伪随机数生成器](@entry_id:145648)（PRNG）**并非像放射性衰变那样的真正随机源。它是一个完全确定性的算法，一份“食谱”。给定一个起始数字，称为**种子**，它会生成一长串*看起来*随机的数字。这些数字通过了随机性的统计检验，但它们就像$\pi$的数字一样是预先确定的。可以把 PRNG 想象成一副巨大且完美洗好的牌。种子告诉你从哪里切牌开始发牌。

这就引出了计算科学中的一个经典难题。想象一下，两名学生 Chloe 和 David 得到了完全相同的模拟代码和输入文件来模拟一个粒子系统 [@problem_id:1994827]。他们在相同的计算机上运行代码。然而，他们得到的平均能量却有不同的答案。奇怪的是，每当 Chloe 重新运行她的模拟时，她都能得到与她之前完全相同的结果，精确到每一个比特。David 对他的结果也发现了同样的情况。他们的结果各自是可复现的，但相互之间却不可复现。这是怎么回事？

罪魁祸首是种子。如果程序没有明确设置 PRNG 的种子，它通常会从系统时钟之类的东西进行初始化。Chloe 和 David 在略微不同的时间开始了他们的模拟，所以他们的 PRNG 被赋予了不同的种子。他们从同一副巨大的牌堆的不同位置开始发牌。因为 PRNG 一旦被播种就是确定性的，所以他们各自的模拟都遵循了一条独特但可重复的路径。这是[随机模拟](@entry_id:168869)中可复现性的第一原则：**要确保可重复的结果，你必须明确控制你的[随机数生成器](@entry_id:754049)的种子。**

当然，并非所有的 PRNG 都是生而平等的。对于科学工作，我们需要的不仅仅是任何序列；我们需要高质量的序列。这与**[密码学安全性](@entry_id:260978)**关系不大，后者关注的是使序列对攻击者不可预测 [@problem_id:3529409]。一个科学用的 PRNG 可以是完全可预测的——它的配方是公开的——但它必须具有出色的统计特性。其中最重要的两个是它的**周期**和**[均匀分布](@entry_id:194597)性**。周期是序列在重复之前的长度。现代的生成器，如[梅森旋转算法](@entry_id:145337)（[Mersenne Twister](@entry_id:145337)），其周期大到天文数字（$2^{19937}-1$），以至于你可以用它运行宇宙生命周期那么长的大型模拟，也绝不会看到一个数字出现两次 [@problem_id:3522944]。[均匀分布](@entry_id:194597)性意味着数字均匀地填满可能性的空间，不仅在一维上，而且在多维上。一个差的生成器产生的随机点，在三维空间中观察时，可能会落在少数几个平面上——如果你正在模拟三维空间中的粒子路径，这将是一场灾难 [@problem_id:2678062]。

### 现代工作流的纠缠之网

不可复现性的来源并不总是深埋在算法内部。有时，它们是我们用来简化生活的工具的副产品。以计算笔记本为例，这是数据分析和[科学计算](@entry_id:143987)中无处不在的工具。它是一个极好的交互式环境，科学家可以在其中编写一些代码，看到结果，再写一些，以一种流畅、[非线性](@entry_id:637147)的方式探索他们的数据。

这里面就有一个陷阱。分析师可能花了一天时间调试代码，不按顺序运行单元格，在笔记本底部的单元格中重新定义一个变量，然后又跳回顶部重新运行一个绘图。一天结束时，他们有一个看起来很精美的笔记本，似乎从上到下逻辑清晰。但是最终的结果——他们即将发表的那个——依赖于一个隐藏的状态，一个特定的、未被记录的单元格执行历史。当一个同事（或者几个月后的原作者）试图通过从上到下运行笔记本复现结果时，他们可能会得到不同的答案，或者代码可能完全崩溃 [@problem_id:1463247]。页面上的线性脚本不再讲述结果是如何产生的真实故事。[可复现性](@entry_id:151299)要求最终归档的工作流是自包含的，并且能从一个干净的起点产生所声称的结果。

在现代机器学习中，复杂性成倍增加。训练一个深度学习模型，比如根据[蛋白质序列](@entry_id:184994)预测其功能，是一项充满随机性的工作 [@problem_id:1463226]。[神经网](@entry_id:276355)络的初始权重是随机设置的。在每次训练（或“轮次”）之前，数据都会被随机打乱。数据集本身也被随机分割成[训练集](@entry_id:636396)和验证集。甚至图形处理单元（GPU）上的一些高性能算法也可能有内部的、[非确定性](@entry_id:273591)的选择以最大化速度。这是一个多头的随机性怪兽。为了实现按位[可复现性](@entry_id:151299)，必须踏上一场细致的征途，驯服每一个来源：为 Python 环境、为 `NumPy` 库、为深度学习框架本身（在 CPU 和 GPU 上）设置随机种子，并明确命令框架使用确定性算法，即使它们稍慢一些。

### 并行中的混沌

为了解决最重大的科学问题，我们需要更强的计算能力。我们转向高性能计算（HPC），将我们的模拟任务分割到数千个并行工作的处理器核心上。这种计算能力的飞跃引入了一类全新的[可复现性](@entry_id:151299)挑战。

想象一下我们的模拟需要数十亿个随机数。我们有 1000 个处理器线程准备就绪。我们如何为它们提供随机数？天真的方法是灾难性的。如果所有线程都从一个由锁保护的共享 PRNG 中抽取数字，我们会造成一个巨大的瓶颈，并且模拟的任何给定部分接收到的数字序列都将取决于线程间获取锁的不可预测的竞争 [@problem_id:2678062]。结果将不再是可复现的。另一个坏主意是用相邻的整数（$1, 2, 3, \dots$）为每个线程播种。对于许多生成器来说，这会产生高度相关的随机数流，从而毒害了模拟的[统计独立性](@entry_id:150300) [@problem_id:3529409]。

优雅的解决方案是将随机数流与逻辑上的**工作单元**而非物理上的**线程**绑定。如果你正在模拟一百万个[粒子碰撞](@entry_id:160531)事件，那么第 54,321 号事件应该获得其自己独特的、独立的、预先确定的随机数序列，无论哪个线程在何时处理它 [@problem_id:3536190]。这可以通过复杂的技术来实现。一种是**跳跃法**（jump-ahead），即将主 PRNG 序列分割成巨大的、不重叠的块，并为每个事件分配自己的块 [@problem_id:2678062] [@problem_id:3536190]。另一种更灵活的方法是使用**[基于计数器的生成器](@entry_id:747948)**。在这种方法中，随机数不是通过更新状态生成的，而是通过将一个复杂的、类似哈希的函数应用于主密钥、事件 ID 和数字本身计数器的组合来生成。这使得为任何事件生成第 N 个随机数成为一个即时可访问且完全可复现的操作，非常适合大规模并行。

但是，即使完美地驯服了随机性，并行计算中仍潜伏着一个更微妙的魔鬼。假设我们有一个完全确定性的模拟，在最后我们想通过对数百万个网格点的能量求和来计算总能量。这个求和任务被分配到我们数千个处理器上。每个处理器对其局部值求和，然后它们将各自的[部分和](@entry_id:162077)合并起来。在这里，我们面临一个[计算机算术](@entry_id:165857)的基本事实：[浮点数](@entry_id:173316)加法**不满足结合律**。也就是说，由于每一步的[舍入误差](@entry_id:162651)，$(a + b) + c$ 可能与 $a + (b + c)$ 在比特位上不完全相同。在两次不同的运行中，部分和以非确定性的顺序组合，可能导致最终答案出现微小但真实的差异 [@problem_id:3614187]。

### 关于“相同”的新哲学

这一发现迫使我们提出一个更深层次的问题。如果我们的[并行模拟](@entry_id:753144)一次运行得到的能量是 $1.00000000000000 \times 10^{12}$，下一次是 $1.00000000000500 \times 10^{12}$，这些结果真的“不同”吗？物理原理没有变，算法没有变，变化的只是[浮点运算](@entry_id:749454)中不可避免的噪声。

这导向了一种更成熟的[可复现性](@entry_id:151299)哲学。与其要求严格的**按位确定性**——这在高性能并行环境中可能不切实际或不可能——我们可以追求**统计上一致的可复现性**。这意味着我们接受会有微小的变化，但我们要求这些变化落在一个狭窄的、有数学依据的容差范围内。两次运行的结果在统计上应该是不可区分的。我们可以通过为每种配置模拟一个轨迹**系综**，然后使用统计检验，如**[柯尔莫哥洛夫-斯米尔诺夫检验](@entry_id:751068)**，来形式化地提出问题：“这两组最终能量的系综是否可能来自同一个 underlying [分布](@entry_id:182848)？”[@problem_id:3109390]。如果 p 值很高，我们就可以自信地说，在所有实际意义上，结果是可复现的。

### 当“地图”本身存在缺陷时

到目前为止，我们都将不[可复现性](@entry_id:151299)归咎于我们的计算机和算法的怪癖。但如果问题出在我们试图模拟的模型本身的数学原理中呢？

通常，我们写下的描述物理定律的[微分方程](@entry_id:264184)是行为良好的。**[皮卡-林德洛夫定理](@entry_id:136826)**给了我们一个条件——描述系统演化的函数必须是“利普希茨连续”的——这保证了从任何给定的起点，都只有一个可能的未来路径。解是唯一的。

然而，一些物理上看似合理的模型，例如描述材料中缺陷动力学的模型，可能具有像 $y'(t) = \sqrt{|y|}$ 这样的[速率定律](@entry_id:276849) [@problem_id:3472104]。在 $y=0$ 附近，这个函数不是[利普希茨连续的](@entry_id:267396)；它的斜率变为无穷大。令人震惊的后果是，[解的唯一性](@entry_id:143619)保证消失了。对于初始条件 $y(0)=0$，不是一个，而是有*无限多个*有效的数学解。一个解是 $y$ 永远保持为零。另一个解是它立即脱离零。并且对于任何任意的“等待时间” $T$，都存在一个解，其中 $y$ 在时间 $T$ 之前保持为零，然后*才*平滑地开始增长。

当我们试图模拟这样一个系统时，计算机面临着一个不可能的选择。在零附近的任何一点微小的[数值舍入](@entry_id:173227)误差，都可能将模拟推向这无限多个有效路径中的任何一条。从几乎相同的数据开始的两次运行，可能会产生截然不同的轨迹，一个保持休眠，另一个在任意时间点活跃起来。在这种情况下，[可复现性](@entry_id:151299)的失败不是计算上的瑕疵；它是来自数学本身的警告，揭示了模型预测能力中的根本性模糊。

因此，通往可复现科学的道路是一条丰富而富有启发性的道路。它迫使我们超越将计算机视为完美神谕的天真看法。它要求我们理解[伪随机性](@entry_id:264938)的本质、我们自己工作流中的隐藏状态、并行计算的微妙机制，甚至我们模型的基本数学属性。实现[可复现性](@entry_id:151299)就是实现对整个科学过程更深层次的理解，从纸笔理论到超级计算机输出的最后一个比特。

