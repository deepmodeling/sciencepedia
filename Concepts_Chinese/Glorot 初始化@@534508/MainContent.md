## 引言
训练深度神经网络是一项精妙的平衡艺术。一个关键却又常被忽视的方面是我们如何设置初始权重——这个过程被称为[权重初始化](@article_id:641245)。若没有一种有原则的方法，信号在[多层网络](@article_id:325439)中传播时，要么会衰减至无（[梯度消失问题](@article_id:304528)），要么会[失控增长](@article_id:320576)（[梯度爆炸问题](@article_id:641874)），从而导致学习停滞。挑战在于找到一种初始化策略，能让信息在网络的深处顺畅地前向和后向流动。

本文深入探讨了 Glorot（或 Xavier）初始化所提供的优雅解决方案。我们将探索其基本原理，然后看这个核心思想如何远远超越简单网络，指导当今最复杂的人工智能模型的设计。在“原理与机制”部分，我们将揭示维持信号平衡背后的数学推理，并推导出著名的 Glorot 公式。随后，“应用与跨学科联系”部分将带领我们参观现代架构的“动物园”——从 [ResNet](@article_id:638916)s 到 [Transformer](@article_id:334261)s——看看这个基本原理在实际中是如何被调整和应用的。

## 原理与机制

想象你站在一列很长的人群的一端，需要将一个秘密信息传递给另一端。队伍中的每个人代表深度神经网络中的一层。信息就是信号——来自你数据中的信息——需要从输入端传到输出端。如果每个人都对下一个人耳语这条信息，它会很快衰减成难以辨认的低语。这就是**信号消失问题**。现在，想象如果每个人都大声喊出这条信息。当信息传到队尾时，它将变成一种失真、震耳欲聋的咆哮，其原始含义已荡然无存。这就是**信号爆炸问题**。为了成功传递信息，每个人都必须以他们听到的大致相同的音量重复它。

在神经网络中，这个“音量”就是信号的**方差**。训练深度网络的核心挑战是确保信号及其对应的误差梯度在网络多层传播时，其方差保持稳定。如果方差减小，梯度就会消失，网络停止学习。如果方差[失控增长](@article_id:320576)，计算会变得不稳定，学习就会崩溃。[权重初始化](@article_id:641245)的艺术与科学，特别是 **Glorot 初始化**，就在于将每个[神经元](@article_id:324093)的初始“说话音量”设置得恰到好处，从而使信息能够在网络深处传播而不会衰减或爆炸。

### “金发姑娘”原则：保持方差“恰到好处”

让我们来看单层中的一个[神经元](@article_id:324093)。它接收来自前一层的输入，比如说来自 $n_{\text{in}}$ 个[神经元](@article_id:324093)。它自身的输出（在被[激活函数](@article_id:302225)进行非线性“挤压”之前）是这些输入的加权和。我们称之为预激活值 $z$：

$$
z = \sum_{i=1}^{n_{\text{in}}} w_i x_i
$$

在这里，$x_i$ 是前一层的输出，$w_i$ 是我们[神经元](@article_id:324093)连接的权重。现在，让我们来思考方差。我们会做一些合理的初始假设：权重 $w_i$ 和输入 $x_i$ 相互独立，并且它们的均值都为零。[独立随机变量之和](@article_id:339783)的方差等于它们各自方差之和。这给了我们一个优美而简单的关系：

$$
\text{Var}(z) = \sum_{i=1}^{n_{\text{in}}} \text{Var}(w_i x_i)
$$

对于均值为零的[独立变量](@article_id:330821)，其乘积的方差等于它们方差的乘积，即 $\text{Var}(wx) = \text{Var}(w)\text{Var}(x)$。由于所有权重都从同一个分布中初始化，它们具有相同的方差 $\text{Var}(w)$。同样，来自前一层的输入共享一个共同的方差 $\text{Var}(x)$。我们的方程极大地简化了：

$$
\text{Var}(z) = \sum_{i=1}^{n_{\text{in}}} \text{Var}(w) \text{Var}(x) = n_{\text{in}} \text{Var}(w) \text{Var}(x)
$$

这个小小的方程是一切的关键。我们的“金发姑娘”目标是保持输出的方差 $\text{Var}(z)$ 与输入的方差 $\text{Var}(x)$ 相同。观察这个方程，你可以清楚地看到如何实现这一点。我们只需要选择权重的方差 $\text{Var}(w)$，使得其他因子可以被抵消：

$$
n_{\text{in}} \text{Var}(w) = 1 \quad \implies \quad \text{Var}(w) = \frac{1}{n_{\text{in}}}
$$

这就是基本原理。为了在信号前向流动时保持其“音量”恒定，连接到[神经元](@article_id:324093)的权重的方差应与其接收的输入数量（其**[扇入](@article_id:344674) (fan-in)**）成反比。

### 双信号记：伟大的折衷

但[神经网络](@article_id:305336)并非单行道。在训练过程中，信号前向流动产生预测后，一个[误差信号](@article_id:335291)（梯度）必须反向流动。这个梯度告诉每个权重如何调整自身以改进预测。这个反向流动的梯度只是另一种信号，它同样容易消失或爆炸。

让我们看一下在同一层中[反向传播](@article_id:302452)的梯度。前一层的一个输入[神经元](@article_id:324093)连接到当前层的所有 $n_{\text{out}}$ 个[神经元](@article_id:324093)（其**[扇出](@article_id:352314) (fan-out)**）。它接收到的梯度是来自下一层梯度的加权和。一个非常相似的推导表明，为了在梯度反向流动时保持其方差稳定，我们需要：

$$
n_{\text{out}} \text{Var}(w) = 1 \quad \implies \quad \text{Var}(w) = \frac{1}{n_{\text{out}}}
$$

现在我们面临一个两难的境地。为了保持前向信号，我们需要 $\text{Var}(w) = 1/n_{\text{in}}$。为了保持反向梯度，我们需要 $\text{Var}(w) = 1/n_{\text{out}}$。除非输入和输出的数量相同（$n_{\text{in}} = n_{\text{out}}$），否则我们无法完美地同时满足这两个条件。

Xavier Glorot 和 Yoshua Bengio 提出了什么方案呢？一个优美而务实的折衷。如果你不能同时满足两个约束条件，那就找到一个中间地带。他们建议通过平均[扇入](@article_id:344674)和[扇出](@article_id:352314)来实现一个务实的折衷，从而得出了那个著名的理想权重方差公式 [@problem_id:3200129]：

$$
\text{Var}(w) = \frac{2}{n_{\text{in}} + n_{\text{out}}}
$$

这就是 **Glorot（或 Xavier）初始化**。它是一种折衷方案，旨在将前向和后向信号都保持在一个稳定的范围内，防止任一者系统性地衰减或增长。同样的逻辑也延伸到简单的[全连接层](@article_id:638644)之外。对于卷积层，[扇入](@article_id:344674)是输入通道[数乘](@article_id:316379)以核大小，$n_{\text{in}} = c_{\text{in}} \times k^2$，[扇出](@article_id:352314)也类似地为 $n_{\text{out}} = c_{\text{out}} \times k^2$。原理保持不变。这个方差的可接受窗口非常窄，特别是在深度网络中。随着层数 $L$ 的增加，与这个理想值的偏差容忍度按 $\beta^{-1/L}$ 的比例缩小，这意味着深度网络要求极其精确的初始化 [@problem_id:3200186]。

### 信使的特性：[激活函数](@article_id:302225)改变一切

到目前为止，我们的分析隐藏了一个假设：激活函数不改变信号的方差。对于像[双曲正切函数](@article_id:638603) $\tanh$ 这样的函数，这大致是正确的，因为它在零附近的小输入下表现为线性（$\tanh(z) \approx z$）。在 Glorot 发表论文时，$\tanh$ 是首选的激活函数，因此他的初始化方法效果非常好 [@problem_id:3199598]。

然而，深度学习世界被**[修正线性单元](@article_id:641014) (ReLU)** 彻底改变，其定义为 $\phi(z) = \max(0, z)$。与 $\tanh$ 不同，ReLU 是高度非线性和不对称的。对于任何均值为零、对称分布的预激活值 $z$，ReLU 会简单地将所有负值设为零。它“杀死”了恰好一半的信号！

这对​​方差产生了巨大影响。事实证明，对于一个均值为零的高斯输入，ReLU 输出的方差减半：$\text{Var}(\text{ReLU}(z)) \approx \frac{1}{2} \text{Var}(z)$。让我们把它代回到我们的[前向传播](@article_id:372045)方程中：

$$
\text{Var}(z_{\text{layer }\ell}) = n_{\text{in}} \text{Var}(w) \text{Var}(\text{ReLU}(z_{\text{layer }\ell-1})) \approx n_{\text{in}} \text{Var}(w) \left( \frac{1}{2} \text{Var}(z_{\text{layer }\ell-1}) \right)
$$

如果我们在这里使用标准的 Glorot 初始化，其中 $n_{\text{in}} \text{Var}(w) \approx 1$，那么 $\text{Var}(z_{\ell}) \approx \frac{1}{2} \text{Var}(z_{\ell-1})$。信号方差在每一层都减半！在一个深度网络中，信号会以指数级速度消失 [@problem_id:3134487]。

反向传播过程也同样如此。ReLU 的[导数](@article_id:318324)对于正输入是 $1$，对于负输入是 $0$。这意味着其[导数](@article_id:318324)平方的[期望值](@article_id:313620)也是 $\frac{1}{2}$。在 ReLU 中使用 Glorot 初始化也会导致[梯度消失](@article_id:642027) [@problem_id:3125165]。

Kaiming He 等人提出的解决方案是修改初始化方法，以考虑这个 $\frac{1}{2}$ 的因子。为了保持前向信号的方差，我们现在需要：

$$
n_{\text{in}} \text{Var}(w) \frac{1}{2} = 1 \quad \implies \quad \text{Var}(w) = \frac{2}{n_{\text{in}}}
$$

这就是 **He 初始化**。它不是一个新原理；它仍然是那个“金发姑娘”原则，但为适应 ReLU 激活函数的特性而做了正确的调整。这个关键的洞察揭示了一个更深层次的真理：初始化方案和激活函数是不可分割的一对。

### 普适性方法及其局限

这个原理非常通用。我们可以为任何激活函数推导出自定义的初始化方法。例如，对于 [Leaky ReLU](@article_id:638296)，$\phi(z) = \max(z, 0) + \alpha \min(z, 0)$，方差被一个因子 $\frac{1+\alpha^2}{2}$ 缩放。因此，相应的“正确”初始化方差是 [@problem_id:3200172]：

$$
\text{Var}(w) = \frac{2}{n_{\text{in}}(1 + \alpha^2)}
$$

请注意，对于标准的 ReLU（$\alpha=0$），这简化为 He 初始化，$\text{Var}(w) = 2/n_{\text{in}}$。而对于线性激活（$\alpha=1$），它简化为 Glorot 的[前向传播](@article_id:372045)条件，$\text{Var}(w) = 1/n_{\text{in}}$。这种统一性正是物理学般思维方式的美妙之处：一个单一、简单的原理可以解释一系列现象。

然而，这幅基于匹配平均“音量”（二阶矩）的优雅图景也有其局限性。如果我们的权重分布虽然具有正确的方差，但却有“重尾”呢？这意味着它产生极大权重值（高[峰度](@article_id:333664)）的概率高于正常水平。这样的权重会产生同样倾向于极端值的预激活值 $z$。对于像 $\tanh$ 这样的饱和[激活函数](@article_id:302225)，这会将许多[神经元](@article_id:324093)推入其梯度几乎为零的平坦区域。即使信号的*方差*平均保持不变，*梯度信号*也会被压缩，导致[梯度消失](@article_id:642027)。在这种情况下，仅仅匹配方差是不够的；权重分布的高阶属性变得至关重要 [@problem_id:3200101]。

### 指挥家的权杖：[随机矩阵](@article_id:333324)的视角

我们可以从单个权重和[神经元](@article_id:324093)中抽身出来，通过**随机矩阵理论**的视角看到一个更宏大的画面。将一层的整个权重矩阵 $W$ 视为一个单一实体。Glorot 初始化所做的不仅仅是控制每个权重的方差；它还驯服了整个矩阵的行为。

矩阵的一个关键属性是其奇异值集合，它描述了矩阵在不同方向上“拉伸”空间的程度。一个爆炸的信号对应于一个具有非常大奇异值的矩阵。随机矩阵理论告诉我们，对于一个其元素根据 Glorot 初始化抽取的大型矩阵，其最大奇异值并不会随着层的大小的增加而增长。相反，它会集中在一个小的常数附近（对于方形层，这个值大约是 2）[@problem_id:3200123]。

这意味着 Glorot 初始化就像交响乐队中指挥家的权杖。它确保了由该层执行的整个[线性变换](@article_id:376365)是行为良好的，防止高维信号空间中的任何方向被灾难性地放大。它强制实施了一种全局稳定性，这比仅仅考虑单个[神经元](@article_id:324093)输出的方差更为强大和全面。这种深层的联系表明，微观层面上的一个简单统计要求，如何在宏观层面上产生深刻而稳定的结构，从而使信息能够在现代神经网络的浩瀚深处顺畅流动。

