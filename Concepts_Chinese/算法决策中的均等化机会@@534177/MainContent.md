## 引言
随着[算法](@article_id:331821)在金融、医疗等领域越来越多地做出关键决策，确保其公平性已成为一个至关重要的问题。那种直觉上认为应同等对待每个人的想法往往会失败，并矛盾地造成不公平的结果。本文通过深入探讨**[均等化机会](@article_id:639009)**来应对这一挑战。这是一个强有力的公平性准则，它将焦点从同等对待转向为符合条件的个体提供公平的结果。在接下来的章节中，我们将首先剖析[均等化机会](@article_id:639009)的核心原则和数学机制，探究其定义、实现方式及其所带来的权衡。随后，我们将深入探讨这一概念的实际应用，审视其在医学、公共卫生和资源分配等复杂伦理领域中的影响，揭示抽象理论如何转化为现实世界中的正义。

## 原理与机制

在我们理解[算法](@article_id:331821)如何做出公平决策的征程中，我们必须首先亲手接触其内部机制。机器是如何决策的？一旦做出决策，我们如何检查其决策是否公平？更重要的是，如果不公平，我们又该如何修正？这不仅仅是一个哲学问题，更是一个工程学、概率学以及通过数学视角看待世界的问题。

### 什么是“公平的机会”？

想象一个用于批准贷款的[算法](@article_id:331821)。在这种情况下，“机会”就是获得贷款。我们希望这个机会能够被公平地分配。但“公平”意味着什么呢？

一个很诱人的答案是同等对待每个人。对每一个人都应用相同的规则、相同的标准。但正如我们将看到的，这个看似崇高的目标可能会导致极其不公平的结果。

相反，让我们考虑一个不同的想法，这个想法后来被称为**[均等化机会](@article_id:639009)**。其原则是：在所有*真正符合*贷款资格的人（即“[真阳性](@article_id:641419)”个体）中，无论他们属于哪个群体，被批准的概率都应该是相同的。无论你属于A组还是B组，只要你有能力偿还贷款，你就应该有同样的机会获得贷款。

在数学上，这意味着我们希望**[真阳性率](@article_id:641734)（TPR）**在所有群体中都相等。一个群体的TPR指的是该群体中符合条件的个体被[算法](@article_id:331821)正确识别出来的比例。

$$
\mathrm{TPR}_{\text{group}} = \mathbb{P}(\text{Prediction is Positive} \mid \text{Individual is Qualified, from the group})
$$

这个定义异常简洁，但其后果却既深远又常常有违直觉。它将焦点从同等对待每个人，转移到了确保那些同样符合条件的个体能够获得公平的*结果*。

### 双重阈值的寓言

大多数简单的分类器分两步工作。首先，它们为每个个体计算一个**分数**，这个数字代表他们符合资格的可能性有多大。分数越高越好。其次，它们应用一个**阈值**。任何分数高于阈值的人都会被批准；其他人则被拒绝。

现在，让我们来做一个思想实验。假设我们有一个相当不错的评分模型。对于真正符合条件的人（“阳性”），分数往往较高。对于不符合条件的人（“阴性”），分数往往较低。让我们将这些分数建模为[钟形曲线](@article_id:311235)，即高斯分布。

考虑两个群体，A和B。一个常见的情况是，即使对于同样符合条件的个体，A组的平均分也可能高于B组。也许我们的模型所使用的特征对B组的预测性稍差，或者历史数据存在偏见。例如，假设对于符合条件的个体，A组的分数中心在 $\mu_{A,+} = 1.5$，而B组的中心在 $\mu_{B,+} = 1.0$。对于不符合条件的个体，我们假设两个组的分数中心都在 $0$ [@problem_id:3167078]。

如果我们对所有人都应用一个单一的、“公平”的阈值，比如说 $\tau = 0.5$，会发生什么？对于A组，其符合条件的个体中，分数高于 $0.5$ 的比例将远大于B组，这仅仅是因为他们的整个分布向右移动了。单一阈值导致A组的TPR高于B组。我们未能实现机会均等。

这导出了一个惊人的结论。为了实现均等的结果（相等的TPR），我们可能需要采取*不平等*的对待。我们需要为每个群体设置不同的阈值。我们可以精确计算出这些阈值 $\tau_A$ 和 $\tau_B$ 应该是多少。为了达到一个目标TPR，比如说 $80\%$，我们需要为每个群体找到一个分数，该分数能划分出各自“符合条件”分布中前 $80\%$ 的个体。如果群体 $g$ 中符合条件的个体的分数服从分布 $\mathcal{N}(\mu_{g,1}, \sigma_{g,1}^2)$，那么要达到目标TPR为 $t$ 所需的阈值 $\tau_g$ 可以通过一个非常简洁的公式给出 [@problem_id:3105509]：

$$
\tau_g = \mu_{g,1} + \sigma_{g,1} \Phi^{-1}(1 - t)
$$

这里，$\Phi^{-1}$ 是标准正态[累积分布函数](@article_id:303570)的[反函数](@article_id:639581)——一种在[钟形曲线](@article_id:311235)上找到对应于特定面积的点的方法。在我们的例子中，为了得到 $\mathrm{TPR}_A = \mathrm{TPR}_B = 0.80$，我们需要设置 $\tau_A \approx 0.66$ 和 $\tau_B \approx 0.16$ [@problem_id:3167078]。我们必须对B组更宽容，才能给予他们与A组同样的机会。

但这个解决方案是有代价的。虽然我们均等化了[真阳性率](@article_id:641734)，但**[假阳性率](@article_id:640443)（FPR）**——即*不符合条件*的人被错误批准的比例——又如何了呢？由于B组的阈值低得多，其“不符合条件”分布中也会有更大一部分落在阈值之上。在我们的例子中，实现相等的TPR会导致B组的FPR显著高于A组 [@problem_id:3167078]。

这是一个根本性的权衡。通过强制执行[均等化机会](@article_id:639009)（相等的TPR），我们可能会在另一个指标上造成差异。一个更严格的公平性定义，称为**[均等化赔率](@article_id:642036)**，要求TPR和FPR*都*在各群体间相等。我们简单的阈值调整技巧无法同时满足这两个要求，除非分类器本身已经完全公平。看来，天下没有免费的午餐。

同样至关重要的是要理解，调整阈值就像在一条充满可能性的曲线上选择操作点。它并不能改变分类器本身的根本质量。模型对一个群体的整体判别能力，通常用**[ROC曲线下面积](@article_id:640986)（AUC）**来衡量，是保持不变的。AUC告诉我们，从同一群体中随机抽取一个符合条件的个体，其得分高于一个随机抽取的不符合条件的个体的概率。选择一个阈值只是在这条曲线上选取一个点；它并不会改变曲线下的面积 [@problem_id:3167078]。

### 可能性的艺术：现实世界中的公平性

高斯分布的世界是优雅的，但现实世界的数据是凌乱且有限的。这种阈值调整方法在真实数据集上是如何运作的呢？

想象一下，我们有一个数据集，里面包含了来自不同群体的个体，每个人都有我们分类器给出的一个分数。对于每个群体 $g$，我们有一定数量的符合条件的个体，我们称之为 $P_g$。如果我们想让这个群体的TPR达到，比如说 $0.5$，我们需要恰好批准这 $P_g$ 人中的一半。

[算法](@article_id:331821)非常直接 [@problem_id:3099474]：
1.  分离出群体 $g$ 中所有符合条件的个体。
2.  根据他们的分数按降序排序。
3.  要达到 $k/P_g$ 的TPR，我们只需批准这个列表上排名前 $k$ 的个体。
4.  新的、针对特定群体的阈值（或者更准确地说，是对分数的偏置调整）被设定为一个恰好落在第 $k$ 个人和第 $(k+1)$ 个人分数之间的值。

这个过程，被称为**后处理**，是强制实现机会均等的一种强大而直接的方法。它不需要重新训练模型；它只是对输出进行简单的调整。然而，它也凸显了一个实际的约束：在一个有限的数据集上，对于一个拥有 $P_g$ 个阳性样本的群体，唯一可实现的TPR是分数集合 $\{0, 1/P_g, 2/P_g, \dots, 1\}$。为了在各群体间均等化TPR，我们必须选择一个对*所有*群体都可实现的目标比率。

### 问题的核心：分数、概率与决策

到目前为止，我们一直把“分数”当作某种神奇的数字。但理想情况下，一个分数*是*什么？一个表现良好的分数，来自一个所谓的**校准**模型，它其实就是给定个体特征下，该个体符合条件的概率。也就是说，$\eta(x) = \mathbb{P}(Y=1 \mid X=x)$。

将分数视为概率使一切都变得更加清晰 [@problem_id:3169423]。为了在保持低[假阳性率](@article_id:640443)的同时实现高[真阳性率](@article_id:641734)，我们直觉上应该优先批准那些符合[条件概率](@article_id:311430)最高的个体。这正是阈值法所做的事情。

当我们设定一个阈值 $\tau$ 时，我们实际上是在说：“我们将批准任何其符合[条件概率](@article_id:311430) $\eta(x)$ 至少为 $\tau$ 的人。”我们批准的[真阳性](@article_id:641419)个体的总“质量”是我们批准的所有个体的概率之和。为了达到一个目标TPR，我们只需从概率最高的个体开始不断接纳，直到累积的概率质量达到我们的目标。如果目标恰好落在一群分数相同的个体中间，我们可以使用随机化的方法，只批准其中恰当的比例，以精确达到我们的目标 [@problem_id:3169423]。这种概率论的视角为那些更启发式的分数[排序方法](@article_id:359794)提供了深刻而坚实的基础。

### 从源头干预

事后调整阈值就像贴创可贴。它很有效，但并不能解决导致差异的根本原因。我们能否在流程的早期进行干预？答案是肯定的。机器学习流程有三个主要阶段：我们输入的数据（**预处理**）、学习[算法](@article_id:331821)本身（**过程内处理**），以及最终输出的决策（**后处理**）。我们已经讨论了后处理；现在让我们看看另外两个。

**[预处理](@article_id:301646)：修饰数据**
差异的产生往往是因为我们数据中的特征在不同群体间的分布不同。例如，对于一个群体，某个特征的范围可能从1到10，而对于另一个群体，它的范围可能从100到1000。一个常见的[预处理](@article_id:301646)步骤是将特征[标准化](@article_id:310343)，使其均值为0，标准差为1。但如果我们在整个数据集上这样做，可能会抹去重要的群体特定信息。

一个更精细的方法是*在每个群体内部分别*进行[标准化](@article_id:310343) [@problem_id:3121549]。这确保了从模型的角度看，每个群体的特征都处于一个“公平的竞争平台”上。这种重新缩放输入的简单行为可以显著改变模型产生的分数，从而改变TPR，使系统更接近或更远离公平状态。这表明，公平性不仅仅是事后的考虑；它根植于数据的结构之中。

**过程内处理：教[算法](@article_id:331821)学会公平**
与其事后修正结果，为什么不从一开始就教模型变得公平呢？这就是**过程内处理**技术背后的思想。在训练期间，[算法](@article_id:331821)试图最小化其预测误差。我们可以通过增加一个对不公平的*惩罚*来修改这个目标。

使用一种称为**[拉格朗日松弛](@article_id:639905)**的数学工具，我们可以创建一个新的[目标函数](@article_id:330966) [@problem_id:3141519] [@problem_id:3105421]：
$$
\text{新目标} = \text{预测误差} + \lambda \times (\text{公平性违规度})
$$
在这里，$\lambda$ 是一个我们可以调节的旋钮。更高的 $\lambda$ 会告诉[算法](@article_id:331821)更重地优先考虑公平性，即使会牺牲一些准确性。例如，“公平性违规度”可以是两个群体TPR之间的平方差。然后[算法](@article_id:331821)会学习一组参数，以平衡这些相互竞争的目标。这种方法通常比后处理[能带](@article_id:306995)来更好的[整体解](@article_id:345303)决方案，因为模型从一开始就学习了既有预测性又公平的特征。

**优化器看不见的手**
更深入地看，偏见的来源有时隐藏在最意想不到的地方。在训练期间用于更新模型参数的[算法](@article_id:331821)——即**优化器**——本身也可能对公平性产生影响。

像**[RMSprop](@article_id:639076)**这样的优化器非常聪明：它们会为每个特征调整[学习率](@article_id:300654)。如果一个特征的梯度（指示如何改变其权重的信号）非常嘈杂且方差很大，[RMSprop](@article_id:639076)会降低其学习率以避免不稳定的跳跃。然而，如果与少数群体相关的特征仅仅因为数据较少而更嘈杂，[RMSprop](@article_id:639076)就会系统性地减慢该群体的学习速度 [@problem_id:3170927]。这可能导致模型需要更长的时间来纠正其对少数群体的错误，从而延长TPR上的差异。这是一个微妙但强有力的例子，说明一个看似中立的技术选择如何无意中编码了偏见。解决方案是设计能够感知群体的优化器，通过归一化来消除这种方差，确保所有群体都能以相当的速度学习。

### 哈哈镜：当数据欺骗我们时

我们整个框架的建立都基于一个关键假设：我们用于训练和评估的数据是现实世界的忠实再现。但如果不是呢？如果我们的数据是一个扭曲的映像，就像哈哈镜一样呢？

不同的数据收集方式，即**抽样框**，可以给我们呈现出截然不同的公平性图景 [@problem_id:3159192]。如果我们从总体中进行简单的随机抽样，我们对公平性指标的估计平均来说是准确的。但在许多领域，尤其是医学领域，研究人员使用**病例对照抽样**。对于每个群体，他们刻意抽取相同数量的患病者（病例，$Y=1$）和非患病者（对照，$Y=0$）。

这种平衡抽样对于训练一个准确的模型非常有效，但它可能会对某些公平性指标造成严重破坏。像[人口统计学](@article_id:380325)均等（Demographic Parity）这样的指标，它衡量每个群体的总体批准率，将会被完全扭曲。在病例对照样本中，每个群体中符合条件的个体基准率被人为地强制设为50%，这在真实人口中几乎从不成立。我们在这个样本上测量的公平性是一种幻象。

有趣的是，有些指标对这种抽样方式具有鲁棒性。[均等化机会](@article_id:639009)和[均等化赔率](@article_id:642036)是根据真实结果 $Y$ 来定义的。由于病例对照抽样保持了*病例集合内部*和*对照集合内部*数据的完整性，我们对TPR和FPR的估计仍然是无偏的 [@problem_id:3159192]。

这给我们上了一堂至关重要的课：你必须了解你的数据是如何收集的。同样的数据集，根据你使用的指标以及数据的抽样方式，既可以支持也可以否定一项公平性声明。此外，我们拥有的数据类型限制了我们甚至可以衡量的公平性类型。如果我们有大量的未标记数据但标签非常少——这是**[半监督学习](@article_id:640715)**中的常见情况——我们就无法计算TPR，因为我们不知道谁是真正符合条件的。然而，我们*仍然*可以衡量和强制执行[人口统计学](@article_id:380325)均等，因为它只依赖于预测率，而不依赖于真实结果 [@problem_id:3162645]。

追求机会均等并非易事。它是一场在定义、权衡以及数据和[算法](@article_id:331821)的现实之间展开的舞蹈。它要求我们超越“相同”这种简单化的观念，去接触[现代机器学习](@article_id:641462)中复杂且相互关联的机制，从我们收集的数据到执行决策的最后一行代码。

