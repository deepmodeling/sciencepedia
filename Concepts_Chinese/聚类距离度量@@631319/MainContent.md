## 引言
“距离”的概念看似简单——即两点之间的空间。然而，在数据分析和机器学习领域，这个看似基本的概念变成了一个深刻而关键的选择。聚类，这项将相似项分组的基本任务，完全取决于我们如何定义“相似”，而这一定义被编码在[距离度量](@entry_id:636073)中。挑战在于，没有单一、普遍“正确”的标尺；度量的选择可以极大地改变从数据中浮现的模式，从而导致截然不同的结论。本文深入探讨了[距离度量](@entry_id:636073)在[聚类](@entry_id:266727)中的关键作用，揭示它们并非仅仅是一个技术细节，而是科学探究的有力透镜。

首先，在“原理与机制”部分，我们将游历一个由各种距离构成的宇宙，从我们熟悉的[欧几里得距离](@entry_id:143990)和[曼哈顿距离](@entry_id:141126)，到更复杂的度量，如余[弦距离](@entry_id:170189)、[马氏距离](@entry_id:269828)和[测地距离](@entry_id:159682)。我们将探讨每种度量如何创造其独特的几何形状，以及[特征缩放](@entry_id:271716)和数据相关性等因素如何影响它们的行为。然后，在“应用与跨学科联系”部分，我们将看到这些原理在一系列令人惊叹的学科中付诸实践。从解读[癌症遗传学](@entry_id:139559)、组织分子结构，到重建[粒子碰撞](@entry_id:160531)、绘制生态群落图谱，我们将发现科学家们如何精心制作和选择[距离度量](@entry_id:636073)，以揭示世界隐藏的结构。

## 原理与机制

[聚类](@entry_id:266727)就是分组，而分组就是决定什么是“相似的”，什么是“不同的”。这个看似简单的想法是我们整个事业的基石，其核心是一个强大而单一的概念：**距离**。但距离*是*什么？我们自以为知道。它是你能用尺子画出的直线长度，是光束会走的熟悉路径。这是 Euclid 的世界，而他的标尺——**[欧几里得距离](@entry_id:143990)**——已经统治了两千年。对于一个空间中具有坐标的两个点 $x$ 和 $y$，比如 $(x_1, x_2, \dots, x_d)$ 和 $(y_1, y_2, \dots, y_d)$，这个距离是：

$$
d_2(x,y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \dots + (x_d-y_d)^2}
$$

这是 $L_2$ 范数，其中的“2”指的是差值的平方。这是飞鸟的距离。但如果你不是一只鸟呢？如果你是曼哈顿的一名出租车司机呢？

### 标尺的暴政与网格的自由

想象一下，你需要从纽约市的一个点到另一个点。你不能飞越建筑物。你必须沿着街道和大道组成的网格行进。你走过的距离不是“乌鸦飞行”的直线距离，而是你东西向和南北向走过的街区总和。这就是**[曼哈顿距离](@entry_id:141126)**，也称为 $L_1$ 范数：

$$
d_1(x,y) = |x_1-y_1| + |x_2-y_2| + \dots + |x_d-y_d|
$$

突然间，我们那个舒适、单一的“距离”概念有了一个竞争者。而且还有更多。我们可以想象一整个距离家族，即**闵可夫斯基 ($L_p$) 距离**，通过改变指数来定义：

$$
d_p(x,y) = \left( |x_1-y_1|^p + |x_2-y_2|^p + \dots + |x_d-y_d|^p \right)^{1/p}
$$

当我们改变 $p$ 时会发生什么？让我们考虑另一个著名的例子：如果 $p$ 变得无限大呢？这听起来可能很奇怪，但它引出了一个非常简单而有用的想法。在极限情况下，总和中最大的一项 $|x_i - y_i|^p$ 变得如此之大，以至于所有其他项都可以忽略不计。结果就是**[切比雪夫距离](@entry_id:174938)**，或称 $L_\infty$ 范数：

$$
d_\infty(x,y) = \max_{i} |x_i-y_i|
$$

这是国际象棋棋盘上国王的距离，他可以向八个方向中的任意一个方向移动一格。移动的步数由行或列上所需的最大变化量决定。

现在，这个选择对聚类重要吗？非常重要。想象一下我们有两个[聚类](@entry_id:266727)中心，一个数据点位于它们之间的某个位置。它属于哪个[聚类](@entry_id:266727)？答案完全取决于我们使用哪把标尺！正如一个简单的分配任务 [@problem_id:3109650] 所探讨的，在一个度量下与两个中心等距的点，在另一个度量下可能更接近其中一个。为什么？因为每个度量都为与中心等距的点定义了不同的“形状”。对于 $L_2$ 范数，这个形状是一个完美的圆形（或在更高维度上是球面）。对于 $L_1$ 范数，它是一个菱形。对于 $L_\infty$ 范数，它是一个正方形。[聚类](@entry_id:266727)之间的[决策边界](@entry_id:146073)是在这些从中心向外生长的形状相遇的地方形成的。将度量从欧几里得距离改为[曼哈顿距离](@entry_id:141126)，实际上改变了我们数据的地图，重新划分了群组之间的边界。没有单一的“真实”距离；度量的选择是数据科学家做出的第一个，或许也是最根本的决定。

### [绝对空间](@entry_id:192472)的幻觉：为何单位与特征至关重要

到目前为止，我们一直在思考物理空间中的点。但在数据分析中，维度不是米或英寸；它们是“特征”——价格、年龄、体重、像素亮度。而这正是欧几里得标尺可能变成暴君的地方。

欧几里得公式 $d_2(x,y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2}$ 含蓄地假设，沿第一个轴的“1”个单位变化等同于沿第二个轴的“1”个单位变化。但如果轴1是“以米为单位的身高”，轴2是“以美元为单位的年收入”呢？1米的差异是巨大的，而1美元的差异则微不足道。距离计算将完全由[数值范围](@entry_id:752817)最大的特征主导。我们在不知不觉中，让单位的选择决定了我们对相似性的概念。

这个问题甚至更深。如果我们对一个特征进行变换会怎样？考虑一组数据点，其中一个特征 $x_1$ 的取值范围从小到大。如果我们用它的对数 $\ln(x_1)$ 来替换 $x_1$，我们就压缩了大的值，扩展了小的值。我们没有改变点沿该轴的顺序——这种变换是单调的——但我们从根本上扭曲了空间。正如一项计算实验所示，当使用标准的 k-means 算法时，这个简单的改变可以完全重排聚类的分配结果 [@problem_id:3109574]。曾经相距很远的两点现在可能变得很近，反之亦然。原始空间中的“自然”[聚类](@entry_id:266727)可能被破坏，而在变换后的空间中可能出现新的、不同的聚类。

这个教训是深刻的：数据的几何结构不是给定的；它是被创造的。它是由你选择的特征以及你如何缩放它们来创造的。看似无害的欧几里得距离对这种选择极为敏感。在你开始聚类之前，你已经做出了关键的几何假设。

### 重要的不是位置，而是方向：作为角度的距离

我们对位置的执着有时会误导我们。想象一下你在分析文档，你的特征是每个词的计数。一份文档可能是某个主题的简短摘要，而另一份是关于同一主题的详细长章节。它们的词计数在量级上会有巨大差异。像欧几里得距离或[曼哈顿距离](@entry_id:141126)这样的位置距离会判定它们相距很远。但它们不应该被认为是相似的吗？它们说的都是同一件事！

这时我们就需要一种新的距离，一种忽略量级而只关注比例或方向的距离。这就是**余[弦距离](@entry_id:170189)**。想象一下我们的[特征向量](@entry_id:151813)是从原点出发的箭头。余[弦距离](@entry_id:170189)不关心箭头的长度；它只关心它们之间的夹角。它的定义是：

$$
d_{\cos}(x,y) = 1 - \frac{x \cdot y}{\|x\|_2 \|y\|_2}
$$

其中 $x \cdot y$ 是[点积](@entry_id:149019)。如果向量指向完全相同的方向，夹角为 $0$，其夹角的余弦值为 $1$，距离为 $0$。如果它们是正交的（完全不相关），夹角为 $90^\circ$，其夹角的余弦值为 $0$，距离为 $1$。这个度量是文本分析和[推荐系统](@entry_id:172804)的主力。正如比较不同度量的聚类结果 [@problem_id:3135257] 所展示的，使用余[弦距离](@entry_id:170189)可以得到与位置度量完全不同、且通常更有意义的数据分组。中心点（medoids）——每个聚类的代表性数据点——的选择，对你是按位置还是按角度定义相似性高度敏感。

### 让数据锻造自己的标尺

我们看到欧几里得距离会受到[特征缩放](@entry_id:271716)的偏见影响。我们可以通过[标准化](@entry_id:637219)我们的特征（例如，将它们缩放为零均值和单位[方差](@entry_id:200758)）来尝试解决这个问题。但如果特征是相关的呢？想象两个高度相关的特征，比如一个人的身高和体重。数据点将形成一个细长的、倾斜的云——一个椭圆。欧几里得距离，以其球形的邻近概念，对这种结构是盲目的。它认为沿着椭圆短轴的一步与沿着长轴的一步相同，这与数据的自然变异不符。

我们需要一把能理解数据形状的标尺。这就是**[马氏距离](@entry_id:269828)**。它自动考虑了特征之间的相关性。公式初看起来有点吓人：

$$
d_M(x,y) = \sqrt{(x-y)^\top \Sigma^{-1} (x-y)}
$$

这里，$\Sigma$ 是数据的协方差矩阵，它捕捉了每个特征的[方差](@entry_id:200758)以及每对特征之间的协[方差](@entry_id:200758)。其[逆矩阵](@entry_id:140380) $\Sigma^{-1}$ 起到了变换的作用。直观地说，它所做的是“去拉伸”和“去旋转”数据。它将倾斜的椭圆形数据云变换成一个漂亮的球形云。在这个变换后的空间里，[马氏距离](@entry_id:269828)就是我们熟悉的欧几里得距离。这是一种从数据自身结构中学习到的距离。

当应用于[聚类](@entry_id:266727)各向异性（椭球形）数据时，结果可能非常显著。正如一项[层次聚类](@entry_id:268536)的比较所示，欧几里得距离可能无法分开重叠、拉伸的聚类，而[马氏距离](@entry_id:269828)通过首先“白化”空间，可以很好地将它们分开 [@problem_id:3128989]。这种方法的力量取决于对[协方差矩阵](@entry_id:139155) $\Sigma$ 的良好估计，根据问题和可用数据的数量，可以使用不同的估计策略。

### 世界不是平的：在[流形](@entry_id:153038)上寻找路径

我们目前讨论的所有距离，甚至包括[马氏距离](@entry_id:269828)，都在一个“平坦”的（欧几里得）空间中运作。它们假设你可以沿着一条直线从任何一点到达任何其他点。但如果你的数据并不存在于一个平坦的空间中呢？如果它位于一个[曲面](@entry_id:267450)上，一个**[流形](@entry_id:153038)**上呢？

一个经典的例子是“瑞士卷”数据集 [@problem_id:3109630]。想象一张纸，上面有数据点，然后像糕点一样卷起来。在平坦的纸上相距很远的两点，在三维空间中可能最终变得非常接近，它们之间只有空气。三维空间中的[欧几里得距离](@entry_id:143990)是一条“捷径”，它作弊了。它不尊[重数](@entry_id:136466)据的内在几何结构。真正的距离是一个人必须沿着纸的表面行走的路径——即**[测地距离](@entry_id:159682)**。

当我们只知道点在更高维空间中的[坐标时](@entry_id:263720)，我们如何发现这个[测地距离](@entry_id:159682)呢？一个巧妙的想法，也是像 Isomap 这样的[流形学习](@entry_id:156668)算法的基础，就是去近似它。我们假设对于彼此非常接近的点，欧几里得距离是[测地距离](@entry_id:159682)的一个良好近似。我们可以通过将每个点连接到其 $k$ 个最近的邻居来构建一个图。图中每条边的权重是连接点之间的[欧几里得距离](@entry_id:143990)。现在，任何两点之间（无论远近）的[测地距离](@entry_id:159682)，都可以通过在这个图上找到它们之间的[最短路径](@entry_id:157568)来近似。

当我们使用这些近似的[测地距离](@entry_id:159682)进行聚类时，我们尊重了数据真实的、潜在的结构。对于瑞士卷数据集，使用[欧几里得距离](@entry_id:143990)的[聚类](@entry_id:266727)无法区分卷的内部和外部，而使用[测地距离](@entry_id:159682)的聚类则能完美成功 [@problem_id:3109630]。这揭示了一个基本原则：对于具有复杂、[非线性](@entry_id:637147)结构的数据，我们必须找到一个度量 *在* 该结构 *内部* 测量邻近性的距离，而不是穿越它。

### 定制的罗盘：将知识编码入距离

也许最强大的想法是，我们可以从头开始设计一个[距离度量](@entry_id:636073)，以编码关于我们问题领域的特定专家知识。一个很好的例子是**[推土机距离](@entry_id:147338) (EMD)**。

想象一下我们根据颜色直方图对图像进行[聚类](@entry_id:266727)。直方图只是每个颜色区间的计数的向量。假设我们的区间是红色、橙色、绿色和蓝色，[排列](@entry_id:136432)在一个圆上。现在考虑两张图片：一张是纯红色的，另一张是纯橙色的。第三张图片是纯绿色的。直观上，红色和橙色的图片非常相似，而红色和绿色的图片非常不同。但简单的 $L_1$ 距离会认为这两对的差异是相同的，因为它不知道橙色在色轮上紧挨着红色，而绿色在对面。

EMD 解决了这个问题。它把距离看作是将一个[直方图](@entry_id:178776)转换成另一个[直方图](@entry_id:178776)的最小“成本”，就好像它们是一堆堆的泥土。关键要素是我们定义的**[成本矩阵](@entry_id:634848)**。这个矩阵告诉我们从一个区间“移动”一个单位质量（一个像素）到另一个区间的成本。我们可以通过设置从“红色”移动到“橙色”的成本很小，而从“红色”移动到“绿色”的成本很大来编码我们的知识 [@problem_id:3109582]。最终的 EMD 将正确地报告红色和橙色图像之间的距离很小，而红色和绿色图像之间的距离很大。现在的[聚类](@entry_id:266727)将尊重颜色的感知几何。

这个原则延伸到许多领域。在分子生物学中，**[均方根偏差 (RMSD)](@entry_id:170106)** 被用来比较[蛋白质结构](@entry_id:140548)。但由于蛋白质的随机旋转和平移，一个简单的计算是无意义的。因此，距离只在*经过*最佳刚体对齐*之后*才被定义。然而，对齐这个行为本身可能很棘手。全局对齐可能会意外地“减去”重要的内禀运动，比如两个结构域之间的铰链式运动，从而人为地使不同的功能状态看起来比它们实际上更相似 [@problem_id:3401814]。距离的定义不仅仅是一个公式；它是一个过程，这个过程的每一步都是一个选择，嵌入了关于什么重要、什么不重要的假设。

### 距离的大千世界

旅程并未在此结束。“数据点”的概念可以被推广，超越一个简单的数字向量。一个数据点可以是一个图、一个时间序列，或者在像脑成像这样的高级应用中，是一个矩阵。例如，一个**[对称正定](@entry_id:145886) (SPD) 矩阵**可以表示不同大脑区域之间的[功能连接](@entry_id:196282)性。

就像瑞士卷一样，这些矩阵的空间不是平的；它有自己的弯曲几何。一个简单的、类似欧几里得的[矩阵距离](@entry_id:193702)（[弗罗贝尼乌斯范数](@entry_id:143384)）忽略了这种曲率，可能会给出误导性的结果。解决方案是再次定义一个**黎曼[测地距离](@entry_id:159682)**——两个矩阵*在所有 SPD 矩阵组成的弯曲[流形](@entry_id:153038)上*的最短有效路径 [@problem_id:3109557]。一旦我们有了这个恰当定义的距离，我们就可以使用像 k-medoids 这样的标准[聚类算法](@entry_id:146720)来找到有意义的大脑连接模式群组。

这引导我们走向一个最终的、统一的视角。任何[构象系综](@entry_id:194778)，无论是分子的还是其他复杂对象的，都可以被看作是某个高维构型空间上的概率测度 $\mu$。当我们选择一组特征时，我们正在定义一个从这个空间到低维特征空间的映射 $\phi$。这个映射通过[伪度量](@entry_id:151770) $d_{\phi}(x,y) = \|\phi(x) - \phi(y)\|$ 诱导出一个几何结构。然后，聚类就变成了对这个诱导出的**度量-[测度空间](@entry_id:191702)**的探索 [@problem_id:3401800]。

从简单的标尺到城市网格，从[特征缩放](@entry_id:271716)到抽象角度，从数据驱动的统计度量到弯曲[流形](@entry_id:153038)上的路径，“距离”的概念逐渐展开。它不是一个关于世界的僵硬、预设的事实。它是一个灵活、有创意、且强大的工具——一个我们为了提出一个特定问题而设计和构建的透镜：“相似意味着什么？”聚类的艺术和科学就在于为任务选择或创造合适的透镜。

