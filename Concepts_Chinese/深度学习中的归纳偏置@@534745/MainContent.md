## 引言
在机器学习中，试图在没有任何先验信念的情况下从有限的数据集中学习，就像试图解决一个有无限可能解的谜题。为了找到一个适用于新情况的有意义的答案，[算法](@article_id:331821)需要一套关于问题本质的指导性假设或“直觉”。这个基本概念被称为[归纳偏置](@article_id:297870)。[归纳偏置](@article_id:297870)远非缺陷，而是学习的先决条件，它使模型能够在广阔的可能性空间中导航并有效泛化。“没有免费的午餐”定理将这一点形式化，指出[算法](@article_id:331821)的成功取决于其固有偏置与手头问题的结构匹配得有多好。

在本文中，我们将深入探讨[归纳偏置](@article_id:297870)的关键作用。第一章**“原理与机制”**将剖析偏置是如何构建到深度学习模型的结构之中的，从模型架构到训练过程。我们将探讨诸如使用卷积层或特定[正则化方法](@article_id:310977)等选择如何为模型预设关于世界的假设。随后的**“应用与跨学科联系”**一章将展示这些理论上的偏置如何成为解决从基因组学到物理学等领域复杂现实问题的关键，将抽象的假设转化为切实的科学发现。

## 原理与机制

想象你是一名抵达犯罪现场的侦探。你看到一些零散的线索：一个脚印、一扇破窗、一件丢失的物品。如果没有一些关于世界如何运作的先验信念——人不能穿墙而过，窗户是从外面打破的，盗窃等动机是存在的——你将束手无策，无法从稀疏的证据中形成一个连贯的理论。学习[算法](@article_id:331821)也面临类似的困境。给定一个有限的数据集，有无限多的可能解释或函数能够完美拟合这些样本。为了有希望泛化到新的、未见的情况，[算法](@article_id:331821)必须带有一些先入为主的观念，一些关于它试图解决的问题本质的“直觉”。在机器学习中，我们称这些直觉为**[归纳偏置](@article_id:297870)**。

[归纳偏置](@article_id:297870)不是缺陷，而是学习的先决条件。它是模型用来在浩瀚的可能性海洋中航行的一系列假设，以找到一个不仅能解释已见数据，还能预测未见数据的解。机器学习中的“没有免费的午餐”定理对此进行了形式化：没有任何单一[算法](@article_id:331821)对所有可能的问题都是最优的。一个[算法](@article_id:331821)的成功取决于其[归纳偏置](@article_id:297870)是否与手头问题的底层结构相符。因此，[深度学习](@article_id:302462)的艺术与科学在很大程度上就是设计和选择具有*正确*[归纳偏置](@article_id:297870)的架构和训练过程。

### 架构的偏置：以特定方式看待世界

我们将偏置注入深度学习模型最基本的方式是通过其架构本身——我们选择的层的类型以及我们如何连接它们。这就像给了我们的侦探一个特定的镜头来观察世界。

#### 图像世界：卷积、局部性与[平稳性](@article_id:304207)

让我们从最著名的例子开始：处理图像。一张图片只是一个像素网格。一种简单的方法可能是使用**全连接网络**，其中一层中的每个[神经元](@article_id:324093)都与下一层中的每个[神经元](@article_id:324093)相连。这种网络是“不可知”的；它对输入的结构不做任何假设。它对待图像的方式与对待一堆打乱顺序的像素值的方式相同。但这种不可知性正是它的致命弱点。对于一张中等大小的图像，参数（连接）的数量变得天文数字般庞大。正如一个思想实验所示，一个将小的 $H \times W \times c$ [特征图](@article_id:642011)映射回其自身的连接层需要惊人的 $(H \cdot W \cdot c)^2$ 个权重 [@problem_id:3126227]。网络过于灵活，拥有太多的“自由度”，在数据有限的情况下，它几乎肯定会[过拟合](@article_id:299541)——它会记住训练图像，而不是学习通用的视觉概念。

于是，**[卷积神经网络 (CNN)](@article_id:303143)** 登场了。CNN 体现了两个关于视觉世界的强大、符合常识的假设。

1.  **局部性**：一个像素的含义由其直接邻居决定。要识别一只眼睛，你看的是构成眼睛的像素，而不是一个来自图像角落的像素和另一个来自对角另一侧的像素。卷积层通过使用小的滤波器或核（例如，$3 \times 3$ 或 $5 \times 5$），只观察输入的小的局部区域来实现这一点。这也被称为[稀疏连接](@article_id:639409)。

2.  **[平稳性](@article_id:304207)**（或[平移等变性](@article_id:640635)）：一个物体的性质不会因为它移动了就改变。一只猫无论在图像的左上角还是右下角，它仍然是一只猫。CNN 通过在图像的每一个位置使用相同的滤波器——同一组权重——来强制实现这一点。这被称为**[权重共享](@article_id:638181)**。一个训练用于检测水平边缘的滤波器可以在整个图像中重复使用。

这两个偏置，局部性和[平稳性](@article_id:304207)，非常有效。它们极大地减少了参数数量。我们不再需要在每个可能的位置都为猫设置一个独特的检测器，而只需要一个。卷积层的参数数量骤降至仅 $k^2 c c'$，与图像的空间维度 $H$ 和 $W$ 无关 [@problem_id:3126227]。这种效率不仅仅是为了节省内存；一个参数更少、约束更强的模型更不容易过拟合，也更有可能学习到真正的底层模式。这是一个精心选择的偏置如何带来更好泛化能力的典型例子。

这个原理不仅限于图像。在[基因组学](@article_id:298572)中，科学家寻找“基序”——作为蛋白质结合位点的短而特定的 DNA 序列。一个基序无论出现在长启动子区域的哪个位置都有其意义。一维 CNN 是完成这项任务的完美工具。其源于[权重共享](@article_id:638181)的[平移等变性](@article_id:640635)是理想的[归纳偏置](@article_id:297870)。单个滤波器可以学习识别该基序，通过在整个 DNA 序列上滑动它，可以在任何出现的地方找到该基序 [@problem_id:2373385]。

#### 分解问题：高级卷积

基本卷积层的成功启发了进一步的问题。我们能否引入更强、更专门的偏置？

考虑**[深度可分离卷积](@article_id:640324) (DSC)**。这种架构做出了一个大胆的假设：单个特征通道*内部*的[空间模式](@article_id:360081)（如检测纹理）可以与*跨*通道混合信息的过程（如将纹理检测器的输出与颜色检测器的输出结合）分离开来。DSC 首先对每个输入通道独立应用一个单独的[空间滤波](@article_id:324234)器（“深度”步骤），然后使用简单的 $1 \times 1$ 卷积来混合结果（“逐点”步骤）。

想象一个合成数据集，其中每个通道编码一种完全不同的模式：通道 1 有垂直条纹，通道 2 有棋盘格，通道 3 有同心圆。分类标签取决于这三种模式是否同时存在。一个标准卷积将不得不学习复杂的 $3 \times 3 \times 3$ 滤波器，同时处理[空间模式](@article_id:360081)和跨通道交互。然而，DSC 非常适合这种情况。它的深度步骤可以学习三个专门的[空间滤波](@article_id:324234)器——一个用于条纹，一个用于棋盘格，一个用于[圆环](@article_id:343088)——而逐点步骤可以学习如何组合它们的响应 [@problem_id:3115156]。因为它的偏置与问题的结构[完美匹配](@article_id:337611)，DSC 的参数效率要高得多，并且能更快地从有限数据中学习。这个思想延伸到**[空洞卷积](@article_id:640660)**，它假设信号中的相关信息可能是周期性的或间隔开的，并将这种“跳跃”模式直接构建到滤波器的结构中 [@problem_id:3116390]。

#### 忽略“位置”，关注“内容”：[全局平均池化](@article_id:638314)

在一系列卷积层提取了一组丰富的特征——眼睛、皮毛、轮子、文本的检测器——之后，我们应该如何使用它们来进行最终分类？一种方法是将最终的特征图展平为一个巨大的向量，并将其输入到一个[全连接层](@article_id:638644)。但这重新引入了对绝对位置的依赖和大量的参数。

一个更优雅的解决方案是**[全局平均池化](@article_id:638314) (GAP)**。对于每个[特征图](@article_id:642011)，GAP 只是计算所有空间位置上的平均激活值，将整个 $H \times W$ 的图压缩成一个单一的数字 [@problem_id:3130696]。然后，得到的通道平[均值向量](@article_id:330248)被送入最终的分类器。GAP 的[归纳偏置](@article_id:297870)是一种强形式的[平移不变性](@article_id:374761)：它假设对于分类而言，重要的是特征的*存在*和整体强度，而不是其精确位置。一张猫的图片就是一张猫的图片，无论猫是充满了整个画面还是蜷缩在一个角落。这极大地减少了参数数量——从展平-全[连接方法](@article_id:640851)的 $H \cdot W \cdot C \cdot K$ 个减少到 GAP-全[连接方法](@article_id:640851)的仅 $C \cdot K$ 个——并作为一种强大的正则化器，迫使模型将每个特征图直接与一个类别概念关联起来。

当你将 DSC 与 GAP 结合使用时，你会得到一个近似于“词袋”模型的[归纳偏置](@article_id:297870)：网络首先学习检测一个局部模式的字典，然后简单地检查它们是否出现在图像的任何地方 [@problem_id:3129824]。

#### 层次结构的力量：为何深通常优于宽

到目前为止，我们讨论了单个层的偏置。但是网络的整体形状呢？在给定的参数预算下，是构建一个浅而非常宽的网络更好，还是一个深而窄的网络更好？

[深度学习](@article_id:302462)的理论和实践都表现出对深度的强烈偏好。深度架构的[归纳偏置](@article_id:297870)是世界是**[组合性](@article_id:642096)**和**层次性**的。简单的特征组合形成更复杂的特征，后者又组合形成更复杂的特征。在视觉中，像素构成边缘，边缘构成纹理和形状，形状构成物体，物体构成场景。

想象一个具有这种结构的[目标函数](@article_id:330966)，比如 $f(\mathbf{x}) = h(g(x_1, x_2), g(x_3, x_4))$，其中函数 $g$ 被重用。一个深度网络可以自然地反映这种结构。第一层可以学习函数 $g$。第二层可以学习函数 $h$，将第一层的输出作为其输入。它自然地实现了 $g$ 的“[特征重用](@article_id:638929)”。相比之下，一个浅层网络必须从头开始学习整个展开的函数。为了表示相同的组合函数，它可能需要比其深度对应物多指数级的[神经元](@article_id:324093) [@problem_id:3098859]。深度提供了一个强大的架构先验，与许多现实世界问题的[组合性](@article_id:642096)质相符。

### 简单性偏置：显式正则化

除了在网络的连接方式中编码假设，我们还可以通过训练目标更明确地施加偏置。这就是**[正则化](@article_id:300216)**的作用。这是一种告诉模型的方式：“在所有拟合数据的假设中，我更喜欢最简单的那一个。”这是[奥卡姆剃刀](@article_id:307589)的机器学习化身。但是“简单”意味着什么呢？

#### 发现潜在结构：低秩偏置

考虑一个电影[推荐系统](@article_id:351916)。数据是一个巨大的、稀疏的用户对电影的[评分矩阵](@article_id:351579)。我们的目标是填补缺失的条目。所有可能的[评分矩阵](@article_id:351579)的空间是巨大的（对于 $m$ 个用户和 $n$ 部电影，有 $m \times n$ 个参数）。试图直接从少数几个评分中学习这是没有希望的。

但我们可以引入一个强大的[归纳偏置](@article_id:297870)：假设用户的品味不是任意的，而是由少数几个潜在因素决定的，比如说 $r$ 个（例如，对喜剧、科幻、某个特定导演的偏好）。同样，每部电影也可以通过它在这些相同的 $r$ 个因素上的体现程度来描述。这个假设在数学上等同于说真实、完整的[评分矩阵](@article_id:351579)是**低秩**的。通过将我们的[假设空间](@article_id:639835)限制为秩最多为 $r$ 的矩阵，我们将有效参数数量从 $mn$ 大幅减少到大约 $r(m+n)$ [@problem_id:3130009]。这种对低维潜在结构的偏置是使[矩阵补全](@article_id:351174)成为可能的基础，也是现代[推荐系统](@article_id:351916)的基石。

#### 对平滑性的偏置

另一个常见的简单性概念是平滑性。我们通常认为，输入的小变化不应导致输出的剧烈波动。

-   **[流形](@article_id:313450)平滑性**：想象数据点并不充满整个空间，而是位于一个较低维度的[曲面](@article_id:331153)或**[流形](@article_id:313450)**上，就像“瑞士卷”的[表皮](@article_id:344241)一样。如果两个点位于瑞士卷的不同层上，它们之间的[欧几里得距离](@article_id:304420)可能很小，但沿着[流形](@article_id:313450)表面的“真实”距离却很大。[流形正则化](@article_id:642117)引入了一种偏置，鼓励函数沿着[流形](@article_id:313450)的[测地线](@article_id:327811)路径平滑，这可以通过使用未标记数据来发现 [@problem_id:3129968]。这可以防止模型在空旷空间中走“捷径”，并尊[重数](@article_id:296920)据的内在几何结构。

-   **利普希茨平滑性**：我们也可以强制实施一种更全局形式的平滑性。一个函数的**[利普希茨常数](@article_id:307002)**限制了对于给定的输入变化，其输出能变化多少。通过在训练目标中增加一个惩罚项来约束网络权重矩阵的**[谱范数](@article_id:303526)**，我们可以直接控制这个[利普希茨常数](@article_id:307002)的上界 [@problem_id:3130043]。这会诱导一种对更[平滑函数](@article_id:362303)的偏置，这有一个非常实际的好处：它使模型对被称为[对抗性攻击](@article_id:639797)的小型恶意扰动更具鲁棒性。

### [算法](@article_id:331821)偏置及其他

最后，[归纳偏置](@article_id:297870)不仅可以来自于我们构建的结构或施加的约束，还可以来自于学习过程本身。

-   **Mixup 的线性偏置**：一个有趣的例子是 **mixup**，这是一种[数据增强](@article_id:329733)技术。我们不仅在真实样本上训练模型，还在通过对成对的真实样本及其标签进行[线性组合](@article_id:315155)而创建的“虚构”样本上进行训练。这里的[归纳偏置](@article_id:297870)是偏好在训练数据点之间表现为线性的函数。这起到了[正则化](@article_id:300216)器的作用。这种偏置太少（没有 mixup）可能导致锯齿状、[过拟合](@article_id:299541)的[决策边界](@article_id:306494)。这种偏置太多可能会“磨平”模型的灵活性，以至于它会[欠拟合](@article_id:639200)，无法捕捉到真正弯曲的决策边界。最佳点，即适量的 mixup，通常通过在偏置和方差之间找到正确的平衡来获得最佳的泛化能力 [@problem_id:3135774]。

-   **零的隐藏偏置**：即使是看似微不足道的架构选择也可能隐藏着深刻的偏置。考虑一个由 ReLU 激活函数构成但移除了所有偏置项（`Wx + b` 中的 `+ b`）的网络。一个简单的归纳证明表明，这样的网络有一个惊人的特性：对于零输入，输出也必须是零。也就是说，$f(0)=0$ 是一个硬约束 [@problem_id:3098905]。这是一个强大的、通常是不希望出现的[归纳偏置](@article_id:297870)，它迫使函数通过原点。这是一个绝佳的例子，说明这些假设可以被多么深刻地根植于模型中，也恰恰解释了为什么我们几乎总是包含偏置项或使用像[批量归一化](@article_id:639282)这样的技术来提供等效的偏移量。

归根结底，学习是一段发现之旅，但它不是一段没有地图的旅程。[归纳偏置](@article_id:297870)就是那张地图。它提供了引导学习[算法](@article_id:331821)穿越无限可能性空间的假设、结构和约束。一个选择不当的偏置会使[算法](@article_id:331821)误入歧途，但一个真正反映问题本质的偏置，却是从数据中解锁非凡洞见的钥匙。

