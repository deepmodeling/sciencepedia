## 引言
学习任何新技能，无论是演奏乐器还是在复杂环境中导航，都涉及一个基本循环：我们行动，观察结果，然后根据反馈调整未来的行动。在人工智能的世界里，这个过程被形式化为[强化学习](@article_id:301586)（RL），而其中最强大和直观的框架之一就是 Actor-Critic 方法。这种方法将学习智能体分为两个不同的组件：一个决定做什么的“行动者”（Actor），和一个评估行动结果好坏的“评论者”（Critic）。

虽然存在更简单的[强化学习](@article_id:301586)方法，但它们常常面临一个严峻的挑战：学习过程可能极其缓慢且不稳定，因为反馈信号充满噪声且不频繁。Actor-Critic 架构通过引入一个“裁判”来提供更细致、更一致的学习信号，从而直接解决了这一知识鸿沟。智能体不再仅仅知道一个结果是好是坏，而是能够了解这个结果比预期好多少或差多少，从而将每一次经历都转化为宝贵的教训。

本文将引导您深入了解这个优雅的[范式](@article_id:329204)。在“原理与机制”一章中，我们将剖析行动者与评论者之间的对话，探索使其合作成为可能的核心概念，如[优势函数](@article_id:639591)和时间[差分](@article_id:301764)误差。随后，在“应用与跨学科联系”一章中，我们将看到这一基本思想如何超越计算机科学，出现在机器人学、金融学等不同领域，甚至为我们大脑的学习方式提供了一个令人信服的模型。

## 原理与机制

想象一下学习一项新技能，比如射箭。你的头脑中有两个部分协同工作。一部分，即“行动者”，决定如何持弓、拉弦的力度以及何时释放。它负责射出箭。另一部分，即“评论者”，观察箭的落点。它不射箭，但它会评判结果。“正中靶心！漂亮的一箭！”或者“那一箭偏了，不太好。”

这种简单的伙伴关系正是 Actor-Critic 方法的核心。它是一个优美而强大的学习框架，是两个不同但合作的过程之间的一场发现之旅。用[强化学习](@article_id:301586)的语言来说，我们有：

- **行动者（Actor）**：这是智能体的**策略（policy）**，通常表示为 $\pi_{\theta}(a|s)$。它是一个由 $\theta$ 参数化的函数，观察世界的当前状态（$s$）并决定采取哪个动作（$a$）。其目标是制定一个能够带来最高累积奖励的策略。

- **评论者（Critic）**：这是智能体的**价值函数（value function）**，通常写作 $V_{w}(s)$ 或 $Q_{w}(s,a)$，并由 $w$ [参数化](@article_id:336283)。它不选择动作，其工作是评估动作。它学习预测从一个给定状态或一个给定的状态-动作对中可以获得的[期望](@article_id:311378)未来奖励。它为行动者提供了改进所必需的关键反馈。

让我们踏上一段旅程，去理解这场优雅的对话是如何运作的，从它的基本原理到使其成为现代人工智能最成功的[范式](@article_id:329204)之一的精妙改进。

### 天真抱负的问题：为什么行动者需要评论者

行动者的抱负很简单：最大化其总奖励。用机器学习的术语来说，它希望对性能目标 $J(\theta)$ 执行**梯度上升**。它想找到调整其参数 $\theta$ 的方向，以增加[期望](@article_id:311378)奖励。最基本的[策略梯度方法](@article_id:639023)，通常称为 REINFORCE，有一个简单的规则：如果一个动作之后是高奖励，就增加其发生的概率。更新规则大致如下：

$\theta \leftarrow \theta + (\text{学习率}) \times (\text{累积奖励}) \times (\text{对数策略的梯度})$

这看起来很合理。强化好的结果。但让我们思考一个思想实验 [@problem_id:3163464]。想象一个游戏，你有五个杠杆可以拉。其中四个什么也不做（$R=0$）。其中一个，“特殊”杠杆，会给你一个 $R=1$ 的奖励，但只有在你拉动它的时候有 5% 的几率发生（$p^*=0.05$）。大多数时候，即使是特殊杠杆也只给出 $R=0$ 的奖励。

一个使用朴素 REINFORCE 规则的行动者将面临一段艰难的时期。它会一次又一次地拉动杠杆并得到 0 的奖励。当奖励为 0 时，更新量为零，学习不会发生。行动者就像在盲目飞行。然后，纯粹靠运气，它拉动了特殊杠杆并得到 $R=1$ 的奖励。突然间，它获得一个大的梯度更新，鼓励它去拉那个杠杆。但这个事件非常罕见，导致学习信号极其“尖峰”且不频繁。梯度的**方差**巨大；这是一个漫长、无聊的平台期后跟随着突然、剧烈波动的疯狂过程。学习过程极其缓慢且不稳定。

这就是评论者登场的时刻。评论者的工作是学习处于某种情况下的*[期望](@article_id:311378)*价值。行动者不再对原始奖励做出反应，而是对结果是否*出乎意料地好*或*出乎意料地坏*做出反应。拉动一个次优杠杆的原始奖励是 0，[期望](@article_id:311378)奖励也是 0，所以没有意外发生。但是，如果评论者已经学到这个游戏中的平均奖励非常低（比如说 0.01），那么得到 0 的奖励就不仅仅是一个中性事件；它是一个*比平均情况稍差*的事件。反之，得到罕见的 1 的奖励则是一个*远好于平均情况*的事件。

这种“惊喜”的概念被形式化为**[优势函数](@article_id:639591)（advantage function）** $A^{\pi}(s,a)$。它被定义为在状态 $s$ 下采取特定动作 $a$ 的价值，减去仅仅处于状态 $s$ 的平均价值：

$$ A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s) $$

这里，$Q^{\pi}(s,a)$ 是动作价值（在状态 $s$ 采取动作 $a$ 后的[期望](@article_id:311378)回报），而 $V^{\pi}(s)$ 是状态价值（从状态 $s$ 出发，对策略可能采取的所有动作进行平均后的[期望](@article_id:311378)回报）。根据定义，如果你根据策略对所有动作的优势进行平均，结果为零：$\mathbb{E}_{a \sim \pi(\cdot|s)}[A^{\pi}(s,a)] = 0$ [@problem_id:2738651]。

通过使用优势作为其学习信号，行动者现在可以从每一个动作中学习。一个导致负优势的动作，即使奖励为 0，也提供了一个有用的梯度来抑制该动作。学习信号变得密集且稳定得多。评论者将行动者的天真抱负转化为一种专注、智能的搜索，极大地降低了更新的方差 [@problem_id:3163464]。

### 对话的媒介：时间[差分](@article_id:301764)误差

我们已经确定，行动者应该以[优势函数](@article_id:639591)的形式听取评论者的判断。但是评论者最初是如何形成这个判断的，以及这个判断是如何传达的呢？这里蕴含着[强化学习](@article_id:301586)中最优雅的联系之一。

评论者通过观察世界中的[状态转换](@article_id:346822)来学习其价值函数，例如 $V_w(s)$。它观察智能体从状态 $S_t$ 开始，采取动作 $A_t$，获得奖励 $R_{t+1}$，并到达新状态 $S_{t+1}$。评论者对起始状态价值的估计 $V_w(S_t)$，理应接近它刚刚获得的奖励加上它到达状态的（折扣后）价值。这为我们提供了一个更新目标：$R_{t+1} + \gamma V_w(S_{t+1})$，其中 $\gamma$ 是一个[折扣因子](@article_id:306551)，它使即时奖励优先于远期奖励。

然后，评论者计算其“错误”，即**时间差分（TD）误差** $\delta_t$。这是目标值与其原始预测之间的差异：

$$ \delta_t = R_{t+1} + \gamma V_{w_{t}}(S_{t+1}) - V_{w_{t}}(S_t) $$

这个 TD 误差是 Actor-Critic 对话的“通货”。它的字面意思是“在单个时间步后，我的预测中所发现的差异”。评论者使用这个[误差信号](@article_id:335291)来更新自己的参数 $w$，微调其预测 $V_w(S_t)$ 以使其更接近目标 [@problem_id:2738643]。

但请仔细观察 TD 误差。它的形式是“（我得到的）-（我[期望](@article_id:311378)的）”。这恰恰是[优势函数](@article_id:639591)的一个单步估计！因此，行动者可以使用这同一个信号 $\delta_t$ 来更新自己的策略。完整的学习过程变成了一支优美耦合的舞蹈：

1.  行动者在状态 $S_t$ 下采取动作 $A_t$。
2.  环境产生奖励 $R_{t+1}$ 和新状态 $S_{t+1}$。
3.  评论者计算 TD 误差：$\delta_t = R_{t+1} + \gamma V_{w_{t}}(S_{t+1}) - V_{w_{t}}(S_t)$。
4.  **评论者更新**其参数 $w$ 以减小此误差：$w_{t+1} = w_{t} + \alpha_{t} \delta_{t} \nabla_{w} V_{w_{t}}(S_t)$。
5.  **行动者更新**其参数 $\theta$，使用相同的[误差信号](@article_id:335291)：$\theta_{t+1} = \theta_{t} + \beta_{t} \delta_{t} \nabla_{\theta} \log \pi_{\theta_{t}}(A_t | S_t)$。

$\nabla_{\theta} \log \pi_{\theta_{t}}(A_t | S_t)$ 这一项是在参数空间中使动作 $A_t$ 更有可能发生的方向。行动者沿此方向推动其参数 $\theta$，步长由 TD 误差 $\delta_t$ 决定。如果 $\delta_t$ 为正（一个惊喜的好结果），行动者会强化该动作。如果 $\delta_t$ 为负（一个意外的坏结果），它会抑制该动作。对于一个简单的策略，这种更新可以有非常直观的形式。例如，如果策略从一个均值由状态决定的高斯分布中选择动作，当结果好时，更新会将平均动作推向刚采取的动作；当结果差时，则将其推离 [@problem_id:29961]。

### 评论者智慧的光谱：偏差-方差权衡

单步 TD 误差是对[优势函数](@article_id:639591)的一个方便且低方差的估计，但它不是唯一的估计。这是一个**有偏**的估计，因为它依赖于评论者自身对下一状态的、可能有缺陷的价值估计 $V_w(S_{t+1})$。这被称为**[自举](@article_id:299286)（bootstrapping）**。

在光谱的另一端，我们可以让评论者等到整个回合结束后，计算完整的、观测到的**蒙特卡洛回报** $G_t$（所有未来折扣奖励的总和）。使用 $G_t - V_w(S_t)$ 作为学习信号将是**无偏**的，因为它基于实际的、完整的结果。然而，这个信号具有非常高的**方差**，因为它是许多随机事件的总和。

这揭示了评论者设计中一个根本性的**偏差-方差权衡** [@problem_id:2738648]。
-   **低 $\lambda$（例如 TD(0)）**：重度[自举](@article_id:299286)，高偏差，低方差。评论者是短视的，相信其下一步的猜测。这可能导致初期学习更快，但如果其自身的价值估计存在系统性错误，可能会收敛到次优解。
-   **高 $\lambda$（例如蒙特卡洛）**：无自举，零偏差，高方差。评论者是耐心的，等待最终的真相。这在平均意义上保证是正确的，但充满噪声的信号会使学习非常缓慢。

TD($\lambda$) [算法](@article_id:331821)中的参数 $\lambda$ 允许我们在这个光谱中导航，将短期的自举估计与长期的蒙特卡洛回报相结合。通过调整 $\lambda$，我们可以为给定问题选择合适的[平衡点](@article_id:323137)，创造一个“恰到好处”的优势估计器。

### 交战规则：确保稳定且诚实的对话

为了让 Actor-Critic 伙伴关系成功，对话必须既稳定又诚实。两个深刻的理论原则确保了这一点。

#### 双时间尺度规则：不要互相打断

评论者试图评估一个策略，但行动者在不断地改变那个策略。评论者在追逐一个移动的目标。如果行动者和评论者以相同的速率学习，它们可能会变得不稳定。行动者可能会基于评论者不成熟的评估而改变策略，导致评论者做出新的、不正确的评估，这反过来又给行动者一个错误的梯度，如此循环。它们可能会失控。

解决方案是**双时间尺度[随机近似](@article_id:334352)** [@problem_id:2738670]。评论者必须在比行动者**更快的时间尺度**上学习。我们通过仔细选择它们的学习率来实现这一点，评论者的学习率为 $\alpha_t$，行动者的[学习率](@article_id:300654)为 $\beta_t$，使得 $\beta_t$ 比 $\alpha_t$ 更快地趋于零（即 $\lim_{t \to \infty} \beta_{t}/\alpha_{t} = 0$）[@problem_id:2738643]。

这确保了从行动者缓慢移动的角度来看，评论者似乎已经收敛，并正在对当前策略提供稳定的评估。评论者在行动者做出任何重大举动之前先理清了自己的思路。这种分离对于保证[算法](@article_id:331821)的收敛至关重要 [@problem_id:2738654]。

#### 兼容性规则：说同一种语言

即使有一个稳定的评论者，也存在另一个危险：如果评论者有系统性偏差怎么办？一个[函数逼近](@article_id:301770)器，比如[神经网络](@article_id:305336)，可能无法完美地表示真实的[价值函数](@article_id:305176)。如果它的近似误差误导了行动者，整个过程就会失败。行动者的[梯度估计](@article_id:343928)将是有偏的。

令人惊讶的是，有一种方法可以设计评论者，使其即使不完全准确也能保持“诚实”。**兼容函数近似定理**为此提供了蓝图 [@problem_id:3190862]。该定理指出，如果我们以一种特殊的方式为我们的评论者选择特征——具体来说，使其特征与行动者[策略梯度](@article_id:639838)的结构 $\nabla_{\theta} \log \pi_{\theta}(a | s)$ 对齐——那么即使使用近似的评论者，得到的[策略梯度](@article_id:639838)估计也将是**无偏**的 [@problem_id:2738654]。

本质上，通过让评论者“说与行动者梯度相同的语言”，我们确保了评论者价值近似中的任何误差都与行动者想要前进的方向“正交”。这些误差不会系统性地将行动者推向错误的方向。数值实验证实了这一优美的理论：具有兼容特征的 Actor-Critic 系统计算出的是精确的[策略梯度](@article_id:639838)，而具有不兼容特征的系统则不然 [@problem_id:3190800]。

### 深度学习时代的对话

当行动者和评论者被实例化为大型[神经网络](@article_id:305336)时，新的挑战和巧妙的解决方案应运而生，为解决复杂、高维问题而完善了这场对话。

-   **连续动作与确定性策略**：对于像机器人控制这样的任务，我们通常想要一个特定的、确定性的动作，而不是一个[概率分布](@article_id:306824)。在这种情况下，行动者变成一个确定性策略 $\mu_\phi(s)$。[策略梯度定理](@article_id:639305)发生了变化，行动者的更新现在需要评论者*关于动作的梯度*，即 $\nabla_a Q(s,a)$ [@problem_id:3190862]。评论者的角色扩展了：它不仅必须估计价值，还必须在动作空间上提供一个平滑、可微的景观，告诉行动者应该向哪个方向“推动”其输出动作以获得更多奖励。这是像 DDPG 等[算法](@article_id:331821)背后的核心思想 [@problem_id:2738632]。

-   **稳定化目标**：[神经网络](@article_id:305336)功能强大，但在离策略设置下（所谓的“死亡三元组”）从其自身的自举估计中学习时，是出了名的不稳定。评论者的学习目标 $y = r + \gamma Q_{\theta}(s', \mu_{\phi}(s'))$ 每次评论者参数 $\theta$ 更新时都会改变。为了解决这个问题，我们引入了**[目标网络](@article_id:639321)**。我们创建行动者和评论者的副本，称之为 $\mu_{\phi'}$ 和 $Q_{\theta'}$，并且只非常缓慢地更新它们。然后，学习目标使用这些稳定的[目标网络](@article_id:639321)来计算：$y = r + \gamma Q_{\theta'}(s', \mu_{\phi'}(s'))$。这为评论者在多次更新中提供了一个固定的目标，从而极大地提高了稳定性 [@problem_id:2738632]。

-   **共享参数与梯度干扰**：为了提高效率，行动者和评论者网络通常共享它们的初始层，从而为状态创建一个共同的特征表示。然而，这可能导致**梯度干扰**。一个改善评论者损失的共享参数更新可能会恶化行动者的损失，反之亦然。这两个任务的梯度可能指向相反的方向。现代技术可以通过分析两个梯度向量的对齐情况，并将一个投影到与另一个正交的方向来缓解这个问题，确保一个任务的更新不会直接抵消另一个任务的更新 [@problem_id:3113617]。

从简单的伙伴关系到在[深度神经网络](@article_id:640465)上运行的复杂、稳定的对话，Actor-Critic 框架体现了一个基本的学习原则：行动与反思之间的平衡，尝试新事物与评估后果之间的平衡。正是这种优雅而强大的相互作用，推动了现代人工智能中一些最令人印象深刻的成就。

