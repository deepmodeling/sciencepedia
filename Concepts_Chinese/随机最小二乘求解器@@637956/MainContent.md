## 引言
在一个由大数据定义的时代，求解庞大的[线性方程组](@entry_id:148943)是机器学习、统计学和[科学计算](@entry_id:143987)等领域面临的一个基本挑战。经典的最小二乘法虽然理论上很完善，但在处理包含数百万甚至数十亿条目的矩阵时，由于高昂的计算成本，在实践中常常会失效。这种计算瓶颈在我们能够收集的数据和我们能从中获得的洞见之间造成了巨大的鸿沟。本文探讨了一种革命性的解决方案：随机最小二乘求解器。我们将首先深入探讨其核心的**原理与机制**，解释随机“勾画”如何能够在不丢失基本信息的情况下，将庞大的问题压缩到可管理的规模，这得益于诸如[约翰逊-林登施特劳斯引理](@entry_id:750946)等几何保证。我们将检验不同勾画策略之间的权衡，例如数据无关的投影和数据相关的[杠杆分数采样](@entry_id:751254)。随后，在**应用与跨学科联系**一章中，我们将展示这些方法的变革性影响，从克服超级计算机中的通信瓶颈到在机器学习、[天气预报](@entry_id:270166)和不确定性量化领域开辟新前沿。准备好去发现，拥抱随机性如何为我们提供一把强大而实用的钥匙，以解开隐藏在海量数据集中的秘密。

## 原理与机制

想象一下，你面临着一项极其艰巨的任务：求解一个[线性方程组](@entry_id:148943) $A\mathbf{x} = \mathbf{b}$，其中矩阵 $A$ 代表着数百万甚至数十亿的观测数据。这在机器学习、天气预报到计算生物学等领域已是家常便饭。教科书中寻找“最佳”近似解的方法，即著名的**最小二乘法**，告诉我们应计算解 $\mathbf{x}_{\text{LS}} = (A^{\top}A)^{-1}A^{\top}\mathbf{b}$。但是，当 $A$ 巨大时，仅仅是构建矩阵 $A^{\top}A$ 就可能是一个极其缓慢、消耗大量内存的噩梦。这种直接方法的计算成本大致与行[数乘](@entry_id:155971)以列数的平方成正比，记为 $\Theta(mn^2)$ [@problem_id:3215894]。我们究竟如何才能在如此巨大的干草堆中找到一根针般的解呢？

答案源于线性代数与概率论的美妙结合：不要去看整个干草堆。相反，我们对问题进行一次巧妙的、压缩的快照——即“勾画”——然后求解这个快照。这就是随机最小二乘求解器的核心所在。

### 一幅更简单的图景：勾画的艺术

其核心思想简单得惊人。我们不去求解庞大的原始系统，而是求解一个规模小得多的勾画版本。为此，我们创建一个特殊的“勾画”矩阵 $S$，其行数远少于 $A$ 的行数。然后，我们用 $S$ 左乘原始系统，得到一个新的压缩系统：$(SA)\mathbf{x} = S\mathbf{b}$。

可以这样想：$A$ 是一幅逼真的、十亿像素的风景图像，而求解最小二乘问题就像试图找到最能拟合这幅图像的3D模型。这个过程太慢了。勾画矩阵 $S$ 就像一台相机，对这片风景拍摄了一张低分辨率的照片。新问题 $\min_{\mathbf{x}} \|SA\mathbf{x} - S\mathbf{b}\|_2^2$ 就是要找到一个最能拟合这张模糊、低分辨率照片的3D模型。由于 $SA$ 和 $S\mathbf{b}$ 中的数据量大大减少，这项任务要快得多。

但这立刻引出了一个关键问题：我们如何能确定这个“模糊照片”问题的解与“高分辨率照片”问题的解有任何关系？毕竟，我们扔掉了大量的信息。奇迹般地，如果我们不是刻意地，而是*随机地*选择我们的“相机” $S$，那么问题的基本几何结构将以极高的概率得以保留。

### 几何保证：当勾画保留了真相

要理解其工作原理，我们必须转向最小二乘的几何学。问题 $\min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2$ 是要在 $A$ 的列空间——即所有形如 $A\mathbf{x}$ 的向量构成的[子空间](@entry_id:150286)——中，找到离目标向量 $\mathbf{b}$ 最近的向量。我们试图使其最小化的向量是残差 $\mathbf{r}(\mathbf{x}) = A\mathbf{x} - \mathbf{b}$。

为了使我们的勾画忠于原问题，它必须保留所有这些可能的残差向量的长度。如果它将所有这些向量大致按相同的比例缩放，那么在原始问题中最短的向量在勾画问题中仍将是最短的。对于所有可能的 $\mathbf{x}$，所有这些[残差向量](@entry_id:165091)都存在于一个由 $A$ 的列和向量 $\mathbf{b}$ 本身张成的特定[子空间](@entry_id:150286)中。我们称之**为问题[子空间](@entry_id:150286)**，$\mathcal{U} = \mathrm{span}\{\mathrm{col}(A), \mathbf{b}\}$ [@problem_id:3186049]。

奇迹就发生在这里。我们需要勾画矩阵 $S$ 在这整个[子空间](@entry_id:150286)上起到近似等距的作用。也就是说，对于问题[子空间](@entry_id:150286) $\mathcal{U}$ 中的任何向量 $\mathbf{v}$，我们需要其勾画后的长度 $\|S\mathbf{v}\|_2$ 与其原始长度 $\|\mathbf{v}\|_2$ 非常接近。更精确地说，我们需要所谓的**[子空间嵌入](@entry_id:755615)**：
$$
(1 - \varepsilon) \|\mathbf{v}\|_2^2 \le \|S\mathbf{v}\|_2^2 \le (1 + \varepsilon) \|\mathbf{v}\|_2^2
$$
对于某个小的失真因子 $\varepsilon$。如果此属性成立，那么对于所有的 $\mathbf{x}$，我们的勾画[目标函数](@entry_id:267263) $\|SA\mathbf{x} - S\mathbf{b}\|_2^2$ 就是真实[目标函数](@entry_id:267263) $\|A\mathbf{x} - \mathbf{b}\|_2^2$ 的高保真近似。

**约翰逊-林登施特劳斯 (JL) 引理**的深刻见解在于，一个其条目从简单随机[分布](@entry_id:182848)（如高斯[钟形曲线](@entry_id:150817)）中选取的矩阵 $S$ 将以高概率满足此属性。勾画所需的行数 $s$ 仅取决于我们想要嵌入的[子空间](@entry_id:150286)的维度（此处最多为 $d+1$，其中 $d$ 是 $A$ 的列数）和期望的精度 $\varepsilon$。一个里程碑式的结果表明，勾画大小 $s$ 需要大致在 $s = \Omega(d/\varepsilon^2)$ 的量级 [@problem_id:3416480]。这个结果最不可思议的特点是它*不*依赖于什么：原始的行数 $n$。无论我们是从一千个方程还是一亿个方程开始，我们都可以将问题压缩到一个仅取决于我们所求解的变量数量的规模。这正是这些方法能够提供巨[大加速](@entry_id:198882)的理论基础 [@problem_id:3215894]。

### 随机性的代价：并非所有行都生而平等

虽然简单地将我们的数据投影到一个随机[子空间](@entry_id:150286)上效果出奇地好，但有时我们只要多加思考，就能做得更好。在我们的系统 $A\mathbf{x} = \mathbf{b}$ 中，每一行——每一个方程，每一个数据点——都同等重要吗？

考虑一个极具启发性的优美例子 [@problem_id:3570159]。想象两个矩阵 $A_1$ 和 $A_2$。它们在能量上是相同的——它们具有相同的奇异值——但在结构上却截然不同。在 $A_1$ 中，所有的“作用”都集中在前两行；其余行全为零。在 $A_2$ 中，作用均匀地[分布](@entry_id:182848)在其所有行中。现在，如果我们使用一个简单地从行中均匀随机抽样的勾画方法，会发生什么？对于 $A_2$，任何小的行样本都能给出整体的一个不错的描绘。但对于 $A_1$，如果我们的随机样本恰好错过了前两行，我们得到的勾画将是纯零——完全无用！使用均匀采样的成功概率低得危险。

这告诉我们，有些行比其他行更具“影响力”。这种影响力在数学上由一个称为**统计杠杆分数**的量来捕捉。具有高杠杆分数的行在结构上是独特的，并且对最终的[最小二乘解](@entry_id:152054)有不成比例的影响力。矩阵 $A_1$ 的病态之处在于有两行的杠杆分数极高，其余的则为零。

这一观察引出了一种更复杂的勾画策略：**[杠杆分数采样](@entry_id:751254)**。我们不再均匀地采样行，而是根据它们的杠杆分数成比例地进行采样。这是一种[数据依赖](@entry_id:748197)的，或称*自适应*的勾画。我们预先花费一些计算力来计算这些分数，作为回报，我们得到了一个对 $A_1$ 中那种病态结构更为稳健的勾画 [@problem_id:3570188]。

这揭示了随机求解器世界中的一个[基本权](@entry_id:200855)衡 [@problem_id:3570163]：
- **无关勾画**（如[随机投影](@entry_id:274693)）：这些是“数据盲的”。它们简单且应用起来极快。你只需生成一个[随机矩阵](@entry_id:269622) $S$ 并进行乘法运算。这通常就足够了。
- **自适应勾画**（如[杠杆分数采样](@entry_id:751254)）：这些是“数据感知的”。它们需要一个可能很昂贵的[预处理](@entry_id:141204)步骤来分析 $A$ 的结构并计算重要性分数。但对于具有高度非均匀结构（高“相干性”）的数据，这些额外的工作会带来丰厚的回报，产生一个更小或更准确的勾画。

虽然我们的讨论集中在采样行（方程）上，但同样的原则也适用于采样矩阵 $A$ 的列（特征）。这在机器学习中是一种强大的[特征选择](@entry_id:177971)技术，同样，可以为列定义杠杆分数来指导智能[采样策略](@entry_id:188482) [@problem_id:3239997]。

### 勾画的风险：当好的勾画变坏时

随机性是强大的，但并非万无一失。对于任何勾画方法，总有很小但非零的概率会抽到一个不幸的样本，从而产生对问题的糟糕表征。

考虑一种情况，我们的勾画恰好完全“忽视”了向量 $\mathbf{b}$ 中位于 $A$ [列空间](@entry_id:156444)之外的部分。在这种情况下，勾画后的问题可能有一个零误差的完美解，$\min \|SA\mathbf{x} - S\mathbf{b}\|_2 = 0$。然而，这个解可能与原始问题非常不匹配，产生一个非常大的真实残差 $\|A\mathbf{x} - \mathbf{b}\|_2$。这是**对勾画的[过拟合](@entry_id:139093)**的典型案例 [@problem_id:3570209]。

我们如何防范这种情况？一个被恰当构建的随机勾画的优雅特性之一是它是**期望无偏**的。这意味着，平均而言，勾画残差的平方长度等于真实残差的平方长度：$\mathbb{E}[\|S(A\mathbf{x}-\mathbf{b})\|_2^2] = \|A\mathbf{x}-\mathbf{b}\|_2^2$ [@problem_id:3570209]。这让我们相信该方法不会系统性地误导我们。然而，对于任何单次试验，一个更实用的保障措施是**验证**：我们可以抽取第二个独立的勾画 $T$，并用它来检查用 $S$ 找到的解的质量。如果解在两个独立的随机视角下看起来都很好，那么它几乎可以肯定是一个好解 [@problem_id:3570209]。

最后，还有一个更根本的弱点需要考虑。整个最小二乘框架是建立在最小化误差*平方*和之上的。这使得它对**异常值**极其敏感。一个严重不正确的数据点会产生巨大的残差，而平方过程会放大其影响，将整个解拉向这个坏点。由于标准的随机求解器旨在近似真实的[最小二乘解](@entry_id:152054)，它自然也继承了同样的脆弱性 [@problem_id:3570156]。

这里的解决方案不是为[最小二乘问题](@entry_id:164198)找到一个更好的勾画，而是改变问题本身。我们可以转而寻求最小化误差*[绝对值](@entry_id:147688)*之和，即 $\min_{\mathbf{x}} \|A\mathbf{x}-\mathbf{b}\|_1$。这种 $\ell_1$-回归问题对异常值要稳健得多。美妙的是，勾画的世界对此也有答案。通过设计一种不同类型的勾画——一种保留 $\ell_1$ 几何而非 $\ell_2$ 几何的勾画——我们可以为[稳健回归](@entry_id:139206)创建随机求解器，从而在存在[数据损坏](@entry_id:269966)的情况下，开辟一个快速、可靠的数据分析新前沿 [@problem_id:3570156]。探索之旅仍在继续，揭示了用随机性思维的非凡力量和灵活性。

