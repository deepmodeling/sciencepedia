## 应用与跨学科联系

现在我们已经熟悉了这些随机求解器优雅的数学机制，你可能会倾向于将它们视为一种巧妙但或许小众的技巧，一种专家的奇思妙想。但事实远非如此。实际上，这些思想不仅有用，而且已经成为我们在庞大数据集和极其复杂模型的现代世界中航行的不可或缺的工具。它们的力量不仅在于加速旧的计算，更在于使全新类型的探究成为可能。

我们即将踏上一段穿越应用领域的旅程，去看看这个优美的思想——即在一个小的[随机投影](@entry_id:274693)中捕捉问题本质——如何在科学与工程这支庞大的交响乐团中产生共鸣。从我们超级计算机的核心，到我们天气的预报，再到人工智能的内部运作，随机求解器正在悄然改变游戏规则。

### 克服数据与距离的暴政

让我们从现代计算时代最直接、也许是最残酷的挑战开始：我们数据的庞大规模。我们建造的计算机能够以惊人的速度进行计算。但有一个陷阱，一个像光速一样根本的物理限制。移动数据需要时间。在现代计算机中，处理器可以在一眨眼的功夫完成一次计算，但从主内存——更不用说从硬盘或网络上的另一台计算机——获取它需要的数字，相比之下则像一个永恒。这就是“通信瓶颈”，对于许多大规模问题来说，它才是真正的敌人。

考虑一个经典的超定[最小二乘问题](@entry_id:164198)，这种问题在将模型拟合到堆积如山的实验数据时会出现。如果我们的数据矩阵 $A$ 如此巨大，以至于无法放入计算机的高速内存（其缓存）中，传统的算法将被迫不断地在慢速内存之间来回搬运矩阵块。处理器将大部分时间花在等待数据到达上。对于一个有 $m$ 行和 $n$ 列的矩阵，这种通信的总成本可以被证明是像 $\Omega(mn^2/\sqrt{M})$ 这样扩展的，其中 $M$ 是高速内存的大小。对于大的 $n$ 来说，这个成本是惩罚性的。

在这里，勾画提供了一个惊人简单的出路。我们不试图解决那个完整而笨重的问题，而是对巨型矩阵 $A$ 执行一两次快速的流式处理，构建我们的小勾画 $SA$。这只需要移动数据一次，其通信成本为最优的 $\Theta(mn)$。一旦我们有了这个问题的压缩版本，它就小到可以轻松地放入高速内存中，我们可以用极快的速度解决它，而不会产生更多的通信成本。我们将滚动巨石上山的西西弗斯式的任务，换成了携带一颗卵石的简单动作。这种通过勾画来避免通信的原则，现在已是高性能计算的基石之一 [@problem_id:3537901]。

同样的想法以壮丽而优雅的方式延伸到超级计算机的世界，在那个世界里，一个问题被[分布](@entry_id:182848)在成千上万个独立的节点上。想象每个节点都持有一部分我们巨大的数据矩阵。为了找到一个[全局解](@entry_id:180992)，这些节点必须相互通信。经典方法会引发一场通信风暴，有可能堵塞网络。同样，勾画提供了一条通往宁静和谐的路径。每个节点独立地计算其*本地*数据的一个小勾画。然后，所有节点需要做的就是执行一个集体的“all-reduce”操作——一种高度优化的网络协同计算——来汇总它们的小勾画。被通信的不是数PB的原始数据，而是数KB的压缩信息。一旦全局勾画被组装起来，一个节点就可以解决它，或者这个小问题可以被并行解决。这种策略，针对像 MPI 这样的真实世界通信协议进行了详细分析，使得在当今最大的机器上解决大陆规模的[最小二乘问题](@entry_id:164198)成为可能 [@problem_id:3270700]。

### 打磨统计学与机器学习的工具

此时，保持一种健康的怀疑态度是应该的。毕竟，我们正在扔掉大部分数据！通过这种看似鲁莽的程序得到的答案能被信任吗？这仅仅是一种得到错误答案的快捷方式吗？答案是“是的，它们可以被信任”，和“不，这是一种得到一个可证明是好答案的快捷方式”，而其背后的推理将我们的[数值算法](@entry_id:752770)与统计学和机器学习的核心联系起来。

考虑线性回归，这是在数据中寻找关系的基础工具。当我们使用勾画来解决回归问题时，我们正在创造一种新的[统计估计量](@entry_id:170698)。一个自然的问题出现了：这个估计量是有偏的吗？它是否系统性地将我们的答案拉离我们试图找到的真实参数？对于标准的勾画-求解程序，非凡的答案是，该估计量仍然是*无偏的*。随机性并不会系统性地破坏我们对真理的探索；它只是增加了一点统计噪声，其[方差](@entry_id:200758)我们也可以进行分析和理解 [@problem_id:3146922]。

这是一个深刻的启示。它将勾画从一个纯粹的数值技巧提升为一个合法的统计程序。而且我们可以更加精确。我们可以对我们引入的误差给出一个严格的数学界限。以高概率，我们从勾画问题中得到的解的残差——衡量其误差的指标——将仅比真实最优解的残差略大。具体来说，误差被一个像 $\sqrt{(1+\varepsilon)/(1-\varepsilon)}$ 这样的因子放大，其中 $\varepsilon$ 是我们勾画的小失真参数。我们也可以对解向量本身的误差进行界定。这个误差取决于 $\varepsilon$、真实残差的大小以及问题的条件，由矩阵 $A$ 的最小[奇异值](@entry_id:152907)所概括 [@problem_id:3225865]。这意味着我们对权衡有了一个定量的把握：我们接受一个小的、可控的、可证明的误差量，以换取计算可行性的巨大增益。

这里可以与数据科学中另一个著名的思想做一个美妙的类比：*[压缩感知](@entry_id:197903)*。你可能听说过这个别样的魔法，即可以从惊人少量的测量中重建高分辨率图像。用于最小二乘的勾画-求解是其近亲。在这两种情况下，一个巧妙设计的[随机投影](@entry_id:274693)都充当了一个“感知”算子，将高维对象的基本信息捕捉到一小组测量中。对于压缩感知，对象是稀疏信号；对于最小二乘，关键对象是矩阵 $A$ 的低维[列空间](@entry_id:156444)。保证信息得以保留的数学工具是无关[子空间嵌入](@entry_id:755615)（OSE）属性。计算上高效的勾画矩阵的存在，例如那些为稀疏输入矩阵设计的矩阵，意味着我们可以在仅与数据中非零条目数量成正比的时间内执行这种“感知”操作，使得该方法在巨大规模上变得实用 [@problem_id:3570187]。

### 科学问题的通用溶剂

当我们看到勾画框架如何毫不费力地适应现实世界科学问题中更复杂的结构时，它的美感就更加深刻了。它不是一个僵硬的配方，而是一个模块化的原则，一种计算瓶颈的通用溶剂。

例如，如果我们的测量并非都具有相同的质量怎么办？在许多实验中，已知某些数据点比其他数据点更可靠。这在**[广义最小二乘法 (GLS)](@entry_id:172315)** 问题中得到体现，其中使用[协方差矩阵](@entry_id:139155)来对数据进行加权。似乎这种复杂性会破坏我们简单的勾画程序。但解决方案非常优雅：我们首先进行变量替换，一个“[预白化](@entry_id:185911)”步骤，将[问题转换](@entry_id:274273)回一个所有误差都被同等对待的标准问题。然后，我们只需将我们信赖的勾画-求解机制应用于这个转换后的问题。原则保持不变；我们只是将其应用于被恰当表述的问题。这种简单的适配使得勾画适用于计量经济学、信号处理和[大地测量学](@entry_id:272545)等广泛领域的问题 [@problem_id:3570206]。

或者，考虑带有硬性物理约束的问题。想象一下为[飞机机翼设计](@entry_id:273620)一个最佳形状。解决方案不仅必须拟合某些[空气动力学](@entry_id:193011)数据（一个最小二乘目标），还必须遵守关于重量、[结构完整性](@entry_id:165319)和材料限制的严格约束。这是一个**约束最小二乘**问题。天真地应用勾画可能会违反这些神圣的约束。正确且远为优美的做法是，认识到这些约束定义了所有可能有效解的一个[子空间](@entry_id:150286)。[优化问题](@entry_id:266749)于是变成了在这个*[子空间](@entry_id:150286)内*找到最佳点。[零空间](@entry_id:171336)方法提供了一种[参数化](@entry_id:272587)这个可行集的方式。然后，我们可以将我们的勾画技术仅应用于问题中我们仍有活动自由度的部分，从而保证最终解在构造上将精确满足约束。这是确定性约束执行与随机[降维](@entry_id:142982)的完美结合，使得解决复杂的、真实世界的设计和工程问题成为可能 [@problem_id:3570186]。

### 在计算与发现的前沿

一个伟大的科学思想的真正力量，不仅在于它能更快地解决昨天的问题，还在于它能开启明天的问题。随机勾画正在做到这一点，推动着一些最激动人心的科学领域的前沿。

在建模中，最深刻的问题或许不是“答案是什么？”，而是“我们对答案有多确定？” 这就是**不确定性量化 (UQ)** 的领域。例如，在天气预报中，我们得到的是一个单一的“最可能”的预报。但大气是一个混沌系统；我们真正渴望的是一个可能性的*范围*，一个未来天气的[概率分布](@entry_id:146404)。在数据同化的贝叶斯语言中，这意味着我们想要从大气初始状态的后验概率[分布](@entry_id:182848)中进行采样。这个[分布](@entry_id:182848)的形状由一个巨大无比的矩阵的逆——[成本函数](@entry_id:138681)的Hessian矩阵——所控制。这个矩阵是如此之大，以至于永远无法被构建出来，更不用说求逆了。几十年来，对这种不确定性的完整刻画一直被认为在计算上是不可能的。

随机方法带来了一个惊人的突破。通过使用类似于随机SVD的随机算法，我们可以“探测”这个巨大的隐式Hessian矩阵，以找到其主要的[方差](@entry_id:200758)方向。这些方向对应于系统中最显著的不确定性模式。该算法构建了那个无法计算的协方差矩阵的一个低秩近似，捕捉了其最重要的部分。有了这个紧凑的表示，我们突然可以做不可能的事：从后验分布中抽取样本。这使得预报员能够生成一个*集成预报*，其中每一个都是天气的一种可能演变，从而为我们提供了预报不确定性的真实画面。这不仅仅是一种加速；它是现代科学预测的一项赋能技术 [@problem_id:3423542]。

最后，我们来到了当前技术革命的引擎：**人工智能**。训练当今[深度学习模型](@entry_id:635298)背后的魔法是一种称为反向模式[自动微分 (AD)](@entry_id:746586) 的技术，它能高效地计算[损失函数](@entry_id:634569)相对于数百万或数十亿模型参数的梯度。这个梯度计算等价于一个向量-雅可比积。为这个[反向传播](@entry_id:199535)（“tape”）存储中间值所需的内存可能成为一个主要瓶颈，限制了我们能训练的模型的大小。

在这里，勾画也提供了一条革命性的前进道路。通过在运行AD*之前*，将模型的[前向传播](@entry_id:193086)与一个勾画矩阵复合，可以有效地计算一个勾画后的雅可比矩阵。现在的反向传播只需为那个小的、勾画后的输出传播梯度，从而极大地减少了内存占用。这可以直接集成到深度学习框架的软件架构中。这是一种使AD过程本身更具[可扩展性](@entry_id:636611)的方法，允许我们训练更大的模型，或探索依赖于Hessian信息的更强大的[二阶优化](@entry_id:175310)方法。从一个非常真实的意义上说，构建我们AI的软件本身正在学习使用随机性来克服其自身的局限性 [@problem_id:3416440]。

从超级计算机的硅心，到我们星球旋转的大气层，再到学习机器的人工神经元，随机勾画的原则揭示了一种深刻而美丽的统一性。它教会了我们一个深刻的教训：在一个被数据淹没的世界里，理解的秘诀不是去观察一切，而是懂得如何恰到好处地观察。通过拥抱随机性，我们可以在压倒性的复杂性中找到隐藏的简单而优雅的结构。