## 引言
科学与工程领域的许多关键挑战——从锐化空间望远镜的图像到绘制地球内部结构图——本质上都是反问题。我们试图从间接且带噪声的测量中揭示潜在的真实情况。然而，这些问题通常是不适定的，这意味着传统方法在面对噪声时难以产生稳定且有意义的解。一种开创性的[范式](@entry_id:161181)——展开优化——应运而生，它通过创建一种强大的混合体，结合了经典的、[基于模型的优化](@entry_id:635801)算法与现代的、数据驱动的深度学习，从而填补了这一空白。本文将探讨这种融合。第一章“原理与机制”将揭示迭代算法如何能被重新构想为[深度神经网络](@entry_id:636170)，从而实现对最优参数和先验的学习。接下来的“应用与跨学科联系”一章将展示该技术如何革新从医学成像、[可微物理](@entry_id:634068)到[蛋白质结构预测](@entry_id:144312)等领域，为科学发现开辟新路径。

## 原理与机制

想象一下，你是一位天体物理学家，手握一幅来自遥远望远镜的模糊图像；或是一位地球物理学家，试图通过[地震波](@entry_id:164985)绘制地球核心的地图；又或是一位医生，正在解读一幅医学扫描图。在所有这些情况下，你都面临着一个相似的挑战：你拥有间接且带噪声的测量值（$y$），并且你想要重建真实、潜在的现实（$x$）。你的测量设备的物理原理为你提供了一个“正向模型”，即一个算子 $A$，它描述了真实情况 $x$ 如何产生你所观测到的数据 $y$：$y = A x + \text{noise}$。从 $y$ 回溯到 $x$ 的任务，就是我们所说的**[反问题](@entry_id:143129)**。

而这其中蕴含着一个深刻的困难。这些问题通常是**不适定的** (ill-posed) [@problem_id:3396223]。测量中微小的噪声扰动都可能导致重建图像发生灾难性的、完全错误的变化。这就像只通过观察很久之后才到达岸边的涟漪，来猜测投入池塘的石子的确切形状。信息已经被冲淡和搅乱了。在数学上，当你尝试对算子 $A$ 求逆时，它的性质会极大地放大噪声。因此，一种简单地“撤销”$A$ 的尝试会得到一个被放大噪声淹没的解。

我们如何找到一个有意义的答案？我们需要更聪明一些。我们需要将数据告诉我们的信息与我们已经*知道*的关于世界的信息结合起来。

### 两个世界的故事：[迭代算法](@entry_id:160288)与[神经网](@entry_id:276355)络

驯服[不适定性](@entry_id:635673)的经典方法是**正则化**。我们不是仅仅试图寻找一个能完美拟合数据的 $x$（这也意味着拟合了噪声），而是寻找一个能在两者之间取得平衡的 $x$。我们定义一个需要最小化的目标函数，它包含两个相互竞争的部分 [@problem_id:3396223]：

$$
\min_{x} \underbrace{\frac{1}{2}\|A x - y\|_2^2}_{\text{Data Fidelity}} + \underbrace{\lambda R(x)}_{\text{Regularization (Prior)}}
$$

第一部分，**数据保真项**，促使我们的解与测量值 $y$ 保持一致。第二部分，**正则化项**，融入了我们关于解应具有何种形态的先验信念。对于“好的”解，$R(x)$ 的值很小；而对于“差的”解，$R(x)$ 的值很大。例如，如果我们期望图像是稀疏的（大部分是黑色，只有少数明亮的物体），我们可能会为 $R(x)$ 选择 $L_1$ 范数，它会对存在大量非零像素值进行惩罚 [@problem_id:3147012]。超参数 $\lambda$ 是一个用于调节权衡的旋钮：大的 $\lambda$ 意味着我们更信任先验信念，而小的 $\lambda$ 意味着我们更信任数据。

解决这个[优化问题](@entry_id:266749)很少能通过一次性计算完成。相反，我们使用**[迭代算法](@entry_id:160288)**，从一个初始猜测开始，逐步对其进行修正，从而逐渐走向[目标函数](@entry_id:267263)的最小值。

最简单也最基础的算法之一是**[梯度下降法](@entry_id:637322)**。如果我们的[目标函数](@entry_id:267263)是一个平滑、起伏的[曲面](@entry_id:267450)，梯度 $\nabla \ell(x)$ 会指向最陡峭的上升方向。因此，要找到一个谷底，我们只需朝相反方向迈出一小步：$x_{k+1} = x_k - \eta \nabla \ell(x_k)$。这里，$\eta$ 是我们的步长，或称学习率。

这个简单的想法与现代深度学习的基石之一——[残差网络](@entry_id:634620)（**[ResNets](@entry_id:634620)**）——有着惊人而美妙的联系。一个基本的[残差块](@entry_id:637094)具有 $x_{k+1} = x_k + g(x_k)$ 的形式，其中 $g(x_k)$ 是一个[神经网](@entry_id:276355)络层。如果我们设置 $g(x_k) = -\eta \nabla \ell(x_k)$，那么这个 [ResNet](@entry_id:635402) 块*就是*一步[梯度下降](@entry_id:145942)！ [@problem_id:3169678] 这是我们发现迭代优化与深度学习两个世界相距不远的第一个线索。

但是，如果我们的正则化项（如 $L_1$ 范数）有尖锐的角点且不平滑，该怎么办呢？我们无法在所有点上计算它的梯度。解决方案是一种优雅的两步法，称为**[近端梯度法](@entry_id:634891)**（也称为 ISTA 或前向-后向[分裂法](@entry_id:755245)）[@problem_id:3396290]。
1.  **前向步骤 (Forward Step):** 对平滑的数据保真部分执行一次常规的[梯度下降](@entry_id:145942)步骤：$z^k = x^k - \alpha \nabla g(x^k)$。
2.  **后向步骤 (Backward Step):** 应用一个“清理”操作，即**[近端算子](@entry_id:635396)**，来处理非平滑的正则化项：$x^{k+1} = \mathrm{prox}_{\alpha \lambda R}(z^k)$。

[近端算子](@entry_id:635396)是数学直觉的奇迹 [@problem_id:3583439]。对于一个给定的点 $z$，$\mathrm{prox}_{\gamma R}(z)$ 会找到一个新的点 $u$，这个点是在保持与 $z$ 接近和使正则化项 $R(u)$ 变小之间达成的完美折衷。对于 $L_1$ 范数，这个算子最终表现为一个简单而著名的函数，称为**[软阈值](@entry_id:635249)**函数，它将数值向零收缩，并将较小的值精确地设置为零，从而促进[稀疏性](@entry_id:136793) [@problem_id:3147012] [@problem_id:3171976]。

### 桥梁：将[算法展开](@entry_id:746359)为网络

这就是核心的、变革性的思想。让我们看一次[近端梯度算法](@entry_id:193462)的迭代：

$$
x^{k+1} = \mathrm{prox}_{\alpha_k \lambda R}\big(x^k - \alpha_k \nabla g(x^k)\big)
$$

这不过是一个接收输入 $x^k$ 并产生输出 $x^{k+1}$ 的数学函数。在[深度学习](@entry_id:142022)的世界里，一个将状态从一个映射到下一个的函数，就是一个**层**。通过“展开”[迭代算法](@entry_id:160288)，我们可以将整个包含 $K$ 次迭代的序列重新诠释为一个拥有 $K$ 层的深度神经网络 [@problem_id:3583439]。

这个展开网络中的每一层都继承自优化算法，具有特定且可解释的结构：
1.  一个执行梯度步骤的**[数据一致性](@entry_id:748190)模块**：$z^k = x^k - \alpha_k A^\top(Ax^k - y)$。这部分是我们根据问题物理知识硬编码的，这些知识嵌入在算子 $A$ 及其转置 $A^\top$ 中。
2.  一个应用[近端算子](@entry_id:635396)的**正则化模块**：$x^{k+1} = \mathrm{prox}_{\alpha_k \lambda R}(z^k)$。这部分强制施加我们对解的先验。

那么，这种视角转变有什么好处呢？在经典算法中，参数——步长 $\alpha_k$、正则化强度 $\lambda$——都是由人类专家精心手动调整的。这是一项费力且针对特定问题的技艺。在展开网络中，我们可以让这些参数变为**可学习的**。我们可以将步长序列 $\{\alpha_k\}$ 视为可训练的权重，并使用一个包含“真实”解的数据集来为重建过程的每个阶段学习最优的步长。

我们甚至可以更进一步。为什么我们必须局限于像 $L_1$ 范数这样手动设计的正则化项呢？真实世界要复杂得多。我们可以用一个灵活、强大的**可学习的近端模块** $\mathrm{prox}_{\theta_k}$ 来代替固定的[近端算子](@entry_id:635396)，这个模块本身就是一个小型的[神经网](@entry_id:276355)络 [@problem_id:3583439]。然后我们从数据中训练这个网络的参数 $\theta_k$。展开网络不再仅仅是解决一个预定义的[优化问题](@entry_id:266749)；它在*学习如何在每次迭代中以最佳方式对解进行正则化*，从数据本身中发现复杂的先验。这种融合就是展开优化的魔力所在：它将基于物理的模型的刚性、可解释结构与[深度学习](@entry_id:142022)的灵活、数据驱动能力相结合。

### 学习迭代的艺术与科学

一旦我们将[迭代算法](@entry_id:160288)视为网络，一个充满可能性的全新世界便豁然开朗。我们不必局限于展开简单的梯度下降法。

更强大的经典算法也可以进行[深度学习](@entry_id:142022)改造。例如，使用**动量**的方法，如 **Nesterov 加速梯度法**，就可以被展开。这些算法就像一个滚下山坡的球，它会记住自己的速度，这有助于它快速通过平坦区域并更快地收敛。通过展开这个过程，我们可以为特定类别的问题学习到最优的动量调度方案 [@problem_id:3396294]。即使是像**[交替方向乘子法](@entry_id:163024) ([ADMM](@entry_id:163024))** 这样复杂的方案——它将一个大[问题分解](@entry_id:272624)成多个更小、更容易解决的部分——也可以被映射到一个[网络架构](@entry_id:268981)上 [@problem_id:3396227]。

网络的**深度**，对应于迭代次数 $K$，成为一个关键的设计选择。它体现了基本的**偏差-方差权衡** [@problem_id:3396226]。
-   一个**浅层网络**（少量迭代）可能无法非常接近目标的真实最小值。它具有较高的**优化偏差**。然而，通过提前停止，它可以防止测量中的噪声被过度放大，从而使其[方差](@entry_id:200758)较低。
-   一个**深层网络**（大量迭代）偏差较低，能非常接近最优解。但每一层都可能放大输入噪声，导致最终输出具有高[方差](@entry_id:200758)。

最优深度并非普适的；它取决于信号和噪声。对于噪声非常小的问题，我们可以使用更深的网络来获得更精细的解。这是经典概念**[早停](@entry_id:633908)**作为一种正则化形式在[深度学习](@entry_id:142022)中的类比。

这个框架对于**非凸**问题同样强大——这些问题如同有许多山丘和山谷的地形，简单的下降方法很容易陷入一个不好的局部最小值。一种巧妙的策略是**连续**法或[同伦](@entry_id:139266)法。我们设计的展开网络在每一层使用不同的正则化强度 $\lambda_\ell$。我们从一个非常大的 $\lambda_1$ 开始，这使得优化[曲面](@entry_id:267450)变得更加平滑，更像[凸函数](@entry_id:143075)，从而引导初始步骤走向一个好的区域。然后，在后续的层中，我们逐渐减小正则化强度，即 $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_L$，让网络在越来越复杂的地形上对解进行精炼 [@problem_id:3396283]。这种学习到的、类似[退火](@entry_id:159359)的调度方案在寻找高质量解方面可以非常有效。

### 深入底层：对解求导

也许最深刻的洞见出现在我们不仅想学习算法*内部*的参数（如步长），还想学习*定义*问题本身的参数（如整体正则化强度 $\lambda$）的时候。要做到这一点，我们需要计算某个最终性能指标（验证损失）相对于 $\lambda$ 的导数。这被称为计算**[超梯度](@entry_id:750478)**。

有两种同样优美的方式来思考这个问题。

首先，我们可以将“展开并学习”的理念推向其逻辑终点。整个包含 $T$ 步的优化过程就是一个巨大而深入的[计算图](@entry_id:636350)。我们可以将[深度学习](@entry_id:142022)的主力军——**反向传播**——应用于这个图。通过在末端输入一个“1”，我们可以计算出最开始 $\lambda$ 的一个微小变化是如何通过所有 $T$ 次迭代层层传递，最终影响最终输出的 [@problem_id:3100024]。

或者，我们可以运用一些数学上的优雅技巧。最终解 $x^\star$ 不仅仅是一个过程的结果；它是一个满足特定条件的状态：[目标函数](@entry_id:267263)的梯度为零，即 $\nabla J(x^\star, \lambda) = 0$。这是将 $x^\star$ 定义为 $\lambda$ 的函数的一种隐式方式。**[隐函数定理](@entry_id:147247)**是高等微积分的基石，它为我们提供了一个计算导数 $dx^\star/d\lambda$ 的直接公式，而无需知道我们是*如何*找到 $x^\star$ 的。这种方法绕过了展开的需要，并且可[能效](@entry_id:272127)率高得多，尤其是在迭代次数 $T$ 非常大的情况下。它需要求解一个相关的[线性系统](@entry_id:147850)，称为伴随系统 [@problem_id:3396255]。

这两种观点——通过展开路径进行显式[微分](@entry_id:158718)和对最终条件进行隐式[微分](@entry_id:158718)——是同一枚硬币的两面。它们揭示了优化数学中深刻的统一性，表明无论我们将解看作一段旅程的终点，还是一个具有特定属性的目的地，我们都可以对其进行推理、[微分](@entry_id:158718)，并最终学会更好地找到它。这就是展开优化的核心：基于原则的模型推理与强大的数据驱动学习的完美结合。

