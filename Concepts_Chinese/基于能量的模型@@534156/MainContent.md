## 引言
在人工智能广阔的领域中，[基于能量的模型](@article_id:640714)（EBMs）代表了一种极为优雅和通用的框架，其灵感直接来源于[统计物理学](@article_id:303380)的原理。在其核心，EBMs 提供了一种简单而强大的方式来捕捉数据的复杂性，即为每个可能的数据配置分配一个称为“能量”的标量值。这种方法避开了归一化概率模型的限制，为从世界中学习开辟了一条独特而灵活的途径。然而，这种灵活性也带来了其自身的重大挑战：处理非[归一化](@article_id:310343)分布的困难。本文将对基于能量的框架进行全面探讨，阐明这些模型的训练方式及其有效的原因。

首先，在“原理与机制”部分，我们将深入探讨 EBMs 的基本概念，解释能量如何定义概率，并剖析难以处理的[配分函数](@article_id:371907)这一核心问题。我们将揭示训练这些模型背后巧妙的对比逻辑，并考察像[受限玻尔兹曼机](@article_id:640921)这样的特定架构如何利用结构来提高效率。随后，“应用与跨学科联系”部分将展示 EBMs 在实践中惊人的广度。我们将从它们在[异常检测](@article_id:638336)和创造性合成中的应用，到它们在[推荐系统](@article_id:351916)中的隐藏角色，最终揭示 EBMs 如何作为一个宏大的统一理论，连接了当今许多最先进的人工智能模型，包括 Transformers、[扩散模型](@article_id:302625)和[对比学习](@article_id:639980)。

## 原理与机制

### 将能量作为概率的语言

[基于能量的模型](@article_id:640714)（EBM）的核心思想源于[统计物理学](@article_id:303380)，优雅而深刻。想象一个连绵起伏、深谷纵横的景观。如果你将一百万个弹珠撒在这片景观上并用力摇晃，你[期望](@article_id:311378)在哪里找到它们？绝大多数弹珠会停留在山谷中，即[引力势能](@article_id:332740)最低的点。很少有（甚至没有）弹珠会岌岌可危地停在山顶上。

EBMs 将这一直觉精确地应用于数据世界。对于任何可能的数据片段——无论是一张图片、一个句子，还是一笔金融交易——模型都会为其分配一个称为**能量**的标量值，记为 $E(x)$。规则很简单：合情理的、真实的数据点被赋予低能量，而无意义或不可能的数据点被赋予高能量。一个由带有参数 $\theta$ 的[神经网络](@article_id:305336)[参数化](@article_id:336283)的 EBM，会学习塑造一个[能量景观](@article_id:308140)，其中“山谷”对应于它所训练的数据类型。

特定数据点 $x$ 的观测概率随后通过优美的**吉布斯分布**由其能量定义：

$$
p_{\theta}(x) = \frac{\exp(-E_{\theta}(x))}{Z(\theta)}
$$

负号至关重要：它意味着低能量对应于高的 $\exp(-E_{\theta}(x))$ 值，从而对应高概率。数据点是弹珠，模型学习一个能量函数 $E_{\theta}(x)$ 作为景观。

### 难以言喻的[归一化](@article_id:310343)项：配分函数

如果故事到此为止，EBMs 将会非常简单。但这里有一个陷阱，一个隐藏在分母中的巨人：$Z(\theta)$。这个项被称为**配分函数**，它是 $\exp(-E_{\theta}(x))$ 在*所有可能存在的 x 的配置*上的总和（或积分）。

$$
Z(\theta) = \int \exp(-E_{\theta}(x)) dx
$$

为什么这是个问题？想象一下我们的数据 $x$ 是一张仅有 $100 \times 100$ 像素的小型灰度图像。每个像素可以有 256 个值中的一个。可能图像的总数是 $256^{10000}$，这是一个天文数字，使得宇宙中的原子数量看起来都像是零钱。计算[配分函数](@article_id:371907)需要为每一张这样的图像评估能量并将它们相加——这项任务不仅困难，而且根本上是不可能的。配分函数的这种难以处理性是使用 EBMs 的核心挑战 [@problem_id:3166256]。这意味着我们无法直接计算任何给定 $x$ 的概率 $p_{\theta}(x)$。

那么，我们是否束手无策了？我们如何可能学习一个我们甚至无法计算其概率的模型呢？

### 通过[对比学习](@article_id:639980)：梯度的两面性

其中的诀窍在于，我们不直接关注概率本身，而是关注当我们调整模型参数 $\theta$ 时概率如何变化。我们通过最小化一个[损失函数](@article_id:638865)来训练模型，通常是我们观测到的数据的**[负对数似然](@article_id:642093)**。对于单个数据点 $x_{\text{data}}$，损失为 $\mathcal{L}(\theta) = -\ln p_{\theta}(x_{\text{data}})$。

让我们看看当我们试图计算这个损失的梯度以更新参数时会发生什么。一点微积分知识揭示了一个非常直观的结构 [@problem_id:3153991] [@problem_id:3121406]：

$$
\nabla_{\theta} \mathcal{L}(\theta) = \nabla_{\theta} E_{\theta}(x_{\text{data}}) - \mathbb{E}_{x \sim p_{\theta}}[\nabla_{\theta} E_{\theta}(x)]
$$

这个方程讲述了一个简单的故事。梯度是两种相反力量之间的拉锯战：

1.  **正相（The Positive Phase）：** 第一项 $\nabla_{\theta} E_{\theta}(x_{\text{data}})$ 告诉我们如何改变 $\theta$ 以*降低*我们实际看到的数据的能量。我们沿着这个梯度方向，在数据点所在的位置下压[能量景观](@article_id:308140)，从而加深山谷。这部分很容易计算；我们只需将数据点通过网络并进行[反向传播](@article_id:302452)。

2.  **负相（The Negative Phase）：** 第二项 $\mathbb{E}_{x \sim p_{\theta}}[\nabla_{\theta} E_{\theta}(x)]$ 是从模型*自身*分布中抽取的样本的能量梯度的平均值。这一项告诉我们要*增加*模型当前认为可能的数据点的能量。它的作用是提升其他所有地方的能量基底，防止模型简单地为所有东西都赋予低能量（这会形成一个无用的平坦景观）。

因此，学习是一个对比的过程：让你见过的事物更可能出现（降低它们的能量），让你*想象*的事物更不可能出现（提高它们的能量）。那个邪恶的[配分函数](@article_id:371907) $Z(\theta)$ 从最终的梯度表达式中消失了，但它的幽灵仍然存在于负相中。为了计算那个[期望](@article_id:311378)，我们需要能够从我们自己的模型中抽取样本，$x \sim p_{\theta}(x)$，这本身就是一个难题，因为 $p_{\theta}(x)$ 包含了 $Z(\theta)$！我们似乎又回到了起点。

### 一个有代价的巧妙捷径：对比散度

这就是一个名为**对比散度（Contrastive Divergence, CD）**的极其务实的捷径发挥作用的地方。负相的困难之处在于从模型分布 $p_{\theta}(x)$ 中生成真实的样本。这通常需要运行一个漫长的模拟过程，比如**[马尔可夫链](@article_id:311246)蒙特卡洛（MCMC）**，直到它达到平衡。

CD 的洞见在于提出：如果我们不等待平衡呢？如果我们从一个数据点 $x_{\text{data}}$ 开始我们的 MCMC 采样器，并只运行几步（$k$ 步）呢？得到的样本，我们称之为 $x_k$，不会是来自 $p_{\theta}(x)$ 的完美样本，但它会从原始数据点漂移到模型当前认为可能的区域。然后我们使用这个“负”样本 $x_k$ 来近似负相梯度。

这是一种近似，和许多捷径一样，它是有代价的。对于少量的步数，CD 提供了对真实梯度的**有偏**估计 [@problem_id:3121406]。在某些情况下，这个有偏梯度甚至可能指向与真实梯度相反的方向，从而误导模型 [@problem_id:3112328]。例如，使用 $k=0$ 是无用的，因为“负”样本就是原始数据点，导致正相和负相完全抵消，更新量为零。随着步数 $k$ 的增加，偏差会减小，在 $k \to \infty$ 的极限情况下，CD 梯度会变成真实的、无偏的最大似然梯度 [@problem_id:3112328]。在实践中，即使是单步（CD-1）也常常出人意料地有效，特别是对于具有正确结构的模型。

### 结构决定一切：[受限玻尔兹曼机](@article_id:640921)

为什么像**[受限玻尔兹曼机](@article_id:640921)（RBM）**这样的特定 EBMs，在当前深度学习复兴之前很久就已经如此流行和实用？答案在于它们的内部结构，这种结构使得训练中的 MCMC 采样步骤效率大大提高 [@problem_id:3170414]。

一个 RBM 有一层“可见”单元（用于存放数据，如像素）和一层“隐藏”单元（用于学习特征）。关键的限制是连接只存在于可见层和隐藏层*之间*，而*不在*层内。这种[二分图](@article_id:339387)结构意味着，给定可见单元的状态，所有隐藏单元彼此条件独立。对称地，给定隐藏单元，所有可见单元都是独立的。

这种[条件独立性](@article_id:326358)是一种计算上的超能力。在 MCMC 采样期间，我们不必一次更新一个单元，而是可以一次性同时更新所有隐藏单元，然后一次性同时更新所有可见单元。这种“块[吉布斯采样](@article_id:299600)”使得采样器能够比在所有单元都相互依赖的全连接模型中所需的缓慢、逐一采样的方式更快速、更高效地探索能量景观。这种结构优势使得 RBMs 在 21 世纪中期成为[预训练](@article_id:638349)深度网络的主力。

### 从生成到判断：EBMs 在分类和[对比学习](@article_id:639980)中的应用

虽然 EBMs 是强大的[生成模型](@article_id:356498)，但它们的框架远比这更通用。它们可以直接用于分类，通过对[条件概率](@article_id:311430) $p(y|x)$ 进行建模，其中 $y$ 是类别标签 [@problem_id:3110716]。在这里，模型为每个可能的输入-标签对学习一个能量 $E(x, y)$。给定 $x$ 时标签 $y$ 的概率则为：

$$
p(y|x) = \frac{\exp(-E(x,y))}{\sum_{y'} \exp(-E(x,y'))}
$$

这正是几乎所有现代分类器中使用的熟悉的 **softmax** 函数！使用标准的[交叉熵损失](@article_id:301965)进行训练，相当于降低正确配对 $(x, y_{\text{true}})$ 的能量，并提升所有错误配对的能量。

这个视角提供了一座强大而统一的桥梁，通向现代人工智能的另一个基石：**[对比学习](@article_id:639980)**。在像 SimCLR 或 InfoNCE 这样的方法中，目标是学习[嵌入](@article_id:311541)，使得一个“锚点”数据点 $z$ 与一个“正”样本 $z^{+}$（例如，同一图像的增强版本）比与许多“负”样本 $\{z^{-}\}$ 更相似。流行的 InfoNCE [损失函数](@article_id:638865)在数学上与条件 EBM 的[交叉熵损失](@article_id:301965)完全相同，其中能量被定义为[嵌入](@article_id:311541)之间的*负相似度*，$E(z, z_j) = -\text{sim}(z, z_j)$ [@problem_id:3173250]。从能量的角度思考揭示了，[对比学习](@article_id:639980)本质上是在训练一个 EBM，使其为相似的配对赋予低能量，为不相似的配对赋予高能量。

### 雕塑虚空：正则化的艺术

训练一个 EBM 不仅仅是拟合你拥有的数据；它还关乎为整个空间，包括不存在数据的广阔“虚空”，定义一个合理的能量值。这就是[正则化](@article_id:300216)的艺术所在。

一个关键的担忧是采样器的稳定性。如果[能量景观](@article_id:308140)有“洞”或通往无穷远的下坡，MCMC 采样器可能会“失控”到无穷大，生成坐标值巨大的无意义样本。为了防止这种情况，我们必须确保能量在远离数据点的地方增长。一种常见的技术是在能量函数中添加一个[正则化](@article_id:300216)项，保证当输入的范数 $\|x\|$ 趋于无穷大时，$E(x) \to \infty$。一个简单而有效的[正则化](@article_id:300216)器是范数本身的光滑版本，如 $\phi(x) = \sqrt{\|x\|^2 + \epsilon^2}$，它像一个限制性的碗，防止采样器逃逸 [@problem_id:3122297]。

一个更微妙的问题与**[得分函数](@article_id:323040)**（score function）有关，它是对数概率相对于*数据*的梯度，$s_{\theta}(x) = \nabla_x \ln p_{\theta}(x) = -\nabla_x E_{\theta}(x)$。这个得分是一个[矢量场](@article_id:322515)，指向[概率密度](@article_id:304297)上的“上坡”方向，引导我们的采样器走向能量景观的山谷。如果能量函数在远离数据的地方饱和或变得平坦，得分将接近于零，采样器就会失去引导，漫无目的地游荡 [@problem_id:3194491]。

有趣的是，对网络*参数* $\theta$ 进行简单的 L2 [正则化](@article_id:300216)可以帮助稳定这种行为 [@problem_id:3141362]。通过鼓励较小的权重，L2 [正则化](@article_id:300216)倾向于产生一个“更平滑”的[能量景观](@article_id:308140)。这可以防止[得分函数](@article_id:323040)变化过于剧烈，从而降低采样器采取过大、爆炸性步骤的风险。然而，这里存在一个权衡：过多的正则化会使景观过于平坦，削弱得分并减慢采样器的探索速度。掌握 EBMs 真正是一门艺术，需要将这个高维[能量景观](@article_id:308140)雕塑得恰到好处。

