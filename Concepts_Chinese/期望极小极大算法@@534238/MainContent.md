## 引言
在任何策略性活动中，从简单的棋盘游戏到复杂的金融交易，目标都是做出尽可能最佳的决策。在一个像国际象棋那样所有变量都已知的完美信息世界里，极小极大（minimax）等[算法](@article_id:331821)可以通过假设对手是一个完美无瑕的对抗者来找到最优走法。但当世界并非如此井然有序时会发生什么？当我们面临掷骰子、新手对手的不可预测行为，或隐藏在战争迷雾中的信息时，我们该如何理性行动？这个差距——在不确定性下做出最优选择的挑战——正是传统逻辑的不足之处。

本文探讨了**[期望](@article_id:311378)极小极大**（expectiminimax）[算法](@article_id:331821)，它是极小极大[算法](@article_id:331821)的一个强大扩展，为在随机和不确定环境中进行决策提供了一个正式框架。它是一种核心逻辑，允许人工智能体“按平均值行事”并进行经过计算的博弈。我们将首先深入探讨该[算法](@article_id:331821)的原理和机制，理解它如何整合概率来推断从掷骰子到对手心理等一切事物。随后，我们将遍览其多样化的应用，发现这个单一而优雅的思想如何统一了游戏、机器人学、金融及其他领域的决策过程，将不确定性从障碍转变为策略中可计算的一部分。

## 原理与机制

想象一下你正在下一盘国际象棋。所有棋子都清晰可见，规则固定，唯一未知的是你对手的下一步棋。如果你是一台超级智能机器，你会如何决定你的最佳策略？你会想：“如果我走这一步，我的对手，作为一个天才，会走*那*一步来让我陷入最不利的境地。而如果我走*另一*步，他们会用*他们*最好的棋来反击……”你会在脑海中推演每一种可能性，假设你的对手是一个完美的敌人，并选择那条即使在最坏情况下也能为你带来最佳可能结果的路径。

这种优美而递归的逻辑正是**极小极大[算法](@article_id:331821)**的核心。我们可以将博弈想象成一棵巨大的可能性之树。在每一层，你，即**最大化玩家**（MAX player），选择通往最高分数的那个分支。在下一层，你的对手，即**最小化玩家**（MIN player），选择通往最低分数的那个分支。通过从游戏终局向后推演，你可以找到任何局面的保证值[@problem_id:3205681]。这是一个完美信息的世界，一个纯粹逻辑的钟表宇宙。

### 从完美逻辑到计算博弈

但是，当我们离开棋盘，进入一场西洋双陆棋（Backgammon）游戏时，会发生什么？突然之间，我们的钟表宇宙被掷骰子的行为打乱了。你最完美的计划可能因一个随机事件而毁于一旦，也可能因此得救。纯粹确定性的极小极大[算法](@article_id:331821)之舞已无法再指引我们。我们需要一个新的原则。

这就是我们在博弈树中引入第三个角色的地方：**自然**（Nature），由一个**机会节点**（chance node）代表。自然并非敌人，它是中立的。它既不试图帮助你，也不试图阻碍你。它只是遵循概率法则行事。当我们到达一个机会节点时——比如说，在我们决定移动一个棋子并等待掷骰结果时——我们不能再简单地挑选最好或最坏的结果。相反，我们必须更加精细。我们计算我们处境的**[期望值](@article_id:313620)**（expected value）。如果掷出双六（概率为 $\frac{1}{36}$）会导向一个价值为 $+50$ 的状态，而所有其他掷骰结果都导向一个价值为 $-1$ 的状态，那么我们当前位置的价值既不是 $+50$ 也不是 $-1$。它的价值是[加权平均](@article_id:304268)值：$(\frac{1}{36} \times 50) + (\frac{35}{36} \times -1)$，大约是 $0.42$。

这个简单但深刻的修改为我们带来了**[期望](@article_id:311378)极小极大**（expectiminimax）[算法](@article_id:331821)。对于最大化（MAX）和最小化（MIN）节点，规则与之前相同，但在机会（CHANCE）节点，我们计算按各事件概率加权的平均结果[@problem_id:3205681]。

可以这样想：想象游戏中有一个第三方玩家，一个“平均”（MEAN）玩家，他既不试图赢也不试图输，只是随机选择一步棋。从你作为决策者的角度来看，这个玩家的回合在功能上等同于一个每个结果都等可能的机会节点[@problem_id:3204250]。核心思想是相同的：当面对不确定性时，一个理性的智能体会按平均值行事。

### 机会的多种面貌

这个框架真正的美妙之处在于认识到“机会”并不仅仅是掷骰子。它是*所有*不确定性的数学体现。

考虑一个游戏，其规则本身可能会改变。假设你正在一个迷宫中导航，在你走第一步之后，有 $50\%$ 的概率地震会封锁一条路径，有 $50\%$ 的概率会打开另一条路径[@problem_id:3204218]。要决定你的第一步，你不能只为一种版本的迷宫做计划。你必须通过对所有可能的未来世界的结果进行平均来评估你的选项。这里的“机会节点”代表了你对世界物理状态的不确定性。

更强大的是，不确定性可以关乎你对手的思维。极小极大[算法](@article_id:331821)做出了一个非常强的假设：你的对手是一个完美无瑕、理性的最小化者。但如果他们不是呢？如果你的对手是一个新手，只在部[分时](@article_id:338112)间里能做出最优决策呢？

假设你相信你的对手有概率 $p$ 是一个特级大师（一个完美的最小化者），有概率 $1-p$ 是一个会随机选择走法的业余爱好者[@problem_id:3204261]。你如何从他们的回合评估游戏的价值？你使用**全[期望](@article_id:311378)定律**（Law of Total Expectation）。该局面的[期望值](@article_id:313620)是一个混合体：

$V(\text{state}) = p \times (\text{Value if opponent is a Grandmaster}) + (1-p) \times (\text{Value if opponent is an Amateur})$

“如果对手是大师时的价值”是我们熟悉的后续局面的最小值。而“如果对手是业余爱好者时的价值”是后续局面的平均值。因此，对手节点的价值变为：

$$V(n) = p \left( \min_{i} V(c_i) \right) + (1-p) \left( \frac{1}{k} \sum_{i=1}^{k} V(c_i) \right)$$

突然之间，我们的[算法](@article_id:331821)可以推断对手的心理和技能水平。它仍然是[期望](@article_id:311378)极小极大[算法](@article_id:331821)，但现在的“机会”不在于掷骰子的结果，而在于人类思维的易错性。

### 揭开战争迷雾

最大的飞跃发生在我们进入具有**不完美信息**（imperfect information）或“战争迷雾”的游戏时。想一想像扑克这样的纸牌游戏，你不知道对手的底牌；或者像策略视频游戏，你看不到他们的基地。游戏本身的状态就是不确定的。

在这里，标准的极小极大[算法](@article_id:331821)及其著名的优化方法——**alpha-beta剪枝**，完全失效。[Alpha-beta剪枝](@article_id:639115)之所以有效，是因为它可以做出确定性的陈述，例如：“我已找到一条路径，保证我至少能得到 $\alpha$ 的分数。我正在探索的这条新路径对我的对手来说已经比某个值 $\beta$ 更差了，而且既然他是一个完美的最小化者，他绝不会让这条路径变得比 $\beta$ 更好。如果 $\beta \le \alpha$，就没有必要再探索下去了。”当涉及不确定性时，这种逻辑就崩溃了。例如，如果对手的行动取决于一张隐藏的牌，他们的目标可能不再是最小化你的分数，而是最大化他们自己的分数，这两者不再是完全对立的[@problem_id:3252749]。或者，如果一个行动导向一个机会结果，一个糟糕的样本并不能保证平均值也会很差。

解决方案是提升我们的思维层次。我们不再对单一、已知的游戏状态进行推理。我们必须对一个**[信念状态](@article_id:374005)**（belief state）进行推理：一个关于所有与我们观测一致的可能真实游戏状态的[概率分布](@article_id:306824)[@problem_id:3204333]。当你在玩扑克时，你不会想：“我的对手有黑桃A。”你会想：“他们有 $15\%$ 的概率是同花，有 $40\%$ 的概率是在虚张声势”，等等。

你如何做出决定？你选择那个在你的整个[信念状态](@article_id:374005)上能最大化你的*[期望效用](@article_id:307899)*的行动。关键在于：你必须在建模你的对手从*他们*的角度，基于*他们*的观察和*他们*的[信念状态](@article_id:374005)，也在做完全相同的事情的同时，来完成这一过程。

这种宏大、对信念的递归推理是[期望](@article_id:311378)极小极大原则的终极体现。正是这种逻辑支撑了那些已经征服了像扑克这类游戏的人工智能。将掷骰子的结果平均化的简单想法，已经发展成为一个在任何被不确定性迷雾笼罩的竞争环境中采取理性行动的完整理论。

### 务实的才华：风险、回报与现实

这个强大的理论也带来了实际的挑战和引人入胜的微妙之处。例如，最大化平均结果总是正确的目标吗？也许不是。许多人宁愿选择一个保证到手的90万美元奖金，也不愿选择一个有50%机会获得200万美元的选项（后者的[期望值](@article_id:313620)更高，为100万美元）。我们可以是**[风险规避](@article_id:297857)**（risk-averse）的。我们可以将这一点直接构建到我们的效用函数中。一个风险敏感的智能体可能不是最大化[期望值](@article_id:313620) $\mathbb{E}[V]$，而是最大化 $U = \mathbb{E}[V] - \lambda \mathrm{Var}[V]$，其中 $\lambda$ 是一个惩罚方差（即风险）的参数[@problem_id:3252700]。[期望](@article_id:311378)极小极大[算法](@article_id:331821)足够灵活，可以处理这种情况；它只是最大化我们提供的任何“效用”定义。

最后的挑战是计算。对于现实问题，博弈树大得惊人。正如我们所见，使极小极大[算法](@article_id:331821)易于处理的alpha-beta剪枝对机会节点不起作用。这是否意味着我们注定要探索每一个随机结果？

不。解决方案和问题本身一样优雅。我们不能以*确定性*进行剪枝，但我们可以以*置信度*进行剪枝。想象我们正在探索一个机会节点。我们可以从其结果中抽样一部分——比如说，从十亿种可能性中抽样1000个。根据这个样本，我们可以使用像[集中不等式](@article_id:337061)这样的统计工具来计算真实[期望值](@article_id:313620)的**置信区间**。例如，我们可能会得出结论：“有 $99.9\%$ 的置信度，这个分支的真实平均值在 $0.6$ 和 $0.7$ 之间。”现在，如果我们已经找到了另一个保证我们价值为 $0.8$ 的行动，我们就可以剪掉这个机会分支！我们正在承担一个微小但可量化的风险（在本例中为 $0.1\%$）来犯错，以换取节省大量的计算[@problem_id:3252754]。

当然，这些微小的风险会累积。我们做出的每一次概率性剪枝都会削弱我们的整体确定性。复杂的[算法](@article_id:331821)必须管理一个“风险预算”，仔细追踪在数千次此类决策中累积的[错误概率](@article_id:331321)[@problem_id:3252751]。这就是高风险、实用人工智能的世界：[博弈论](@article_id:301173)、概率论和计算机科学的美妙融合，全部建立在[期望](@article_id:311378)极小极大这个基础性、统一性的原则之上。

