## 应用与跨学科联系

在我们遍历了[人工神经网络](@article_id:301014)的原理和机制之后，你可能会留有一种抽象之美的感觉，仿佛看到一组齿轮和弹簧在一个数学黑箱中滴答作响。但是，这套机器究竟是*用来做什么*的呢？它在何处落地生根？说一个[神经网络](@article_id:305336)是“[通用函数逼近器](@article_id:642029)”是一回事，而亲眼看到它在现实世界中的意义则完全是另一回事。事实是，这个单一而强大的思想——一个简单的分层结构可以学会逼近输入和输出之间的几乎任何关系——已经在几乎所有科学和工程领域点燃了一场革命。

在本章中，我们将探索这片壮丽的图景。我们将看到[人工神经网络](@article_id:301014)不仅是分析数据的工具，更正在成为科学发现的伙伴、复杂物理系统中的组件，甚至是一种描述自然的新语言。我们将从熟悉走向奇幻，我希望通过这个过程向你展示，[神经网络](@article_id:305336)的应用不仅仅是一系列巧妙的技巧，更是一个深刻而统一的原理在起作用的证明。

### 模式识别与预测：从世界的数据流中学习

让我们从最直观的应用开始：在海量数据中寻找模式。我们的世界充满了信息，从股票价格的闪烁到细胞中蛋白质的复杂舞蹈。通常，支配这些系统的规则过于复杂、充满噪声，或者我们根本不知道，以至于无法写下一套简洁的方程。这正是神经网络大放异彩的地方。我们不需要告诉它规则；我们只需要给它看例子。

考虑一下实时追踪经济的挑战。经济学家传统上依赖月度或季度报告，这就像在比赛结束后很久才看到一张快照。但是，每一次信用卡刷卡，每一次在线购物，都是经济活动的微小、高频信号。可以训练一个[人工神经网络](@article_id:301014)处理庞大的交易数据集，其中每笔交易都由金额、商家和地点等特征描述，从而将它们分类为“食品杂货”、“旅行”或“娱乐”等类别。通过处理数以百万计的这些交易，网络学会了区分不同类别的微妙、非线性关系。汇总这些分类可以构建实时的零售指数，为我们提供经济的实时脉搏，这在以前是无法想象的任务 [@problem_id:2387310]。同样，[人工神经网络](@article_id:301014)如今在保险行业中对于预测自然灾害的财务成本已不可或缺。通过从连接风速和洪水深度等气象特征与财产损失的历史数据中学习，这些模型可以为一个投保财产组合预测预期的损坏率，从而在风暴来临时提供关键、快速的[风险评估](@article_id:323237) [@problem_id:2387311]。这甚至延伸到了现代金融领域，其中使用[人工神经网络](@article_id:301014)进行[时间序列预测](@article_id:302744)可以用来预测股票投资组合未来的[碳足迹](@article_id:321127)，这是对环境负责的投资者的一个关键工具 [@problem_id:2414326]。

这种破译复杂模式的能力在生命科学中可能更为深远，因为这里的“规则”是用进化这一杂乱的语言书写的。以糖基化过程为例，即糖分子附着到蛋白质上——这是影响从蛋白质折叠到免疫反应等一切的关键步骤。N-联[糖基化](@article_id:342951)遵循一个相对清晰的规则，一个特定的氨基酸序列或“序列基元”（sequon）（$\mathrm{Asn}$-$X$-$\mathrm{Ser}/\mathrm{Thr}$）。但是O-联[糖基化](@article_id:342951)没有这样简单的规则；它依赖于一个复杂的、与上下文相关的模式，这种模式一直未能被简单描述。这对于[人工神经网络](@article_id:301014)来说是一个完美的问题。通过在已知[糖基化](@article_id:342951)位点的大量蛋白质文库上进行训练，网络可以学习到有利于O-联修饰的微妙序列偏好。它学会了细胞指令手册中人眼可能忽略的“细则”，创造出强大的预测工具，这些工具现已成为现代生物化学的基石 [@problem_id:2580209]。同样的原理也适用于免疫学，用于预测病毒的哪些片段（称为肽）会与我们细胞上的[MHC分子](@article_id:361224)结合，从而呈递给免疫系统。结合规则是模糊的，并且依赖于多个氨基酸位置之间的相互作用。虽然像[位置权重矩阵](@article_id:310744)（PWM）这样的简单模型可以捕捉到主要的“锚定”偏好，但它们假设每个位置的贡献是独立的。而[人工神经网络](@article_id:301014)凭借其更大的容量，可以学习到*位置间的依赖关系*——即一个位置的氨基酸如何弥补另一个位置的不良匹配——从而为肽和[MHC分子](@article_id:361224)之间复杂的“握手”提供更丰富、更准确的图景 [@problem_id:2507812]。

### 代理与加速器：当物理定律计算过于缓慢时

现在，让我们从规则未知的系统转向规则完全已知但计算起来极其困难的系统。想象一下模拟空气流过机翼的情景。其控制定律是著名的[纳维-斯托克斯方程](@article_id:321891)。我们知道这些方程，但在超级计算机上高保真地求解它们，一次模拟可能需要数天或数周。如果你需要运行数千次这样的模拟来设计一架新飞机呢？

在这里，[神经网络](@article_id:305336)扮演了一个新角色：*代理模型*。我们不是每次都从头开始求解方程，而是可以先针对一系列输入条件（如流速和[攻角](@article_id:330712)）运行几十或几百次高保真模拟。然后，我们训练一个[人工神经网络](@article_id:301014)来学习从这些输入条件到感兴趣的输出（如阻力和[升力系数](@article_id:335811)）的映射。一旦训练完成，网络可以在几分之一秒内提供答案。它不求解纳维-斯托克斯方程；它模仿或充当昂贵求解器的代理 [@problem_id:2438962]。

我们可以将这个想法推向一个更根本的层面。在像用于[流体动力学](@article_id:319275)的格子玻尔兹曼方法（LBM）中，流体不是被建模为[连续体](@article_id:320471)，而是网格上粒[子群](@article_id:306585)的集合。模拟分两步进行：“迁移”，即粒子跳到相邻的网格点；和“碰撞”，即单个点上的粒[子群](@article_id:306585)相互作用并重新分布。编码了流体所有复杂物理过程的碰撞步骤，其计算可能非常密集。一个引人入胜的想法是用一个训练好的小型神经网络来取代解析的[碰撞算子](@article_id:368587)。网络将碰撞前的粒[子群](@article_id:306585)作为输入，并直接输出碰撞后的状态。然而，要使其工作，网络不能是一个幼稚的黑箱。它必须被设计或训练来尊重碰撞过程中内置的基本物理定律：质量和动量的精确守恒。此外，它必须尊重底层格子的对称性以产生各向同性的（与方向无关的）流体行为，并且它必须以能产生正的物理粘度的速率来弛豫非[守恒量](@article_id:321879)。如果满足这些条件，由[人工神经网络](@article_id:301014)驱动的LBM可以正确地再现宏观的[纳维-斯托克斯方程](@article_id:321891)，并有可能捕捉到超出简单碰撞模型能力的复杂物理过程 [@problem_id:2407028]。

### 混合模型：将网络编织进物理学的结构中

代理模型是一个强大的工具，但它仍然在核心[物理模拟](@article_id:304746)之外运行。这段旅程的下一步是将网络带入内部，创建*混合模型*，其中[人工神经网络](@article_id:301014)成为物理定律本身的一个组成部分。

这是计算力学的一个前沿领域。想象一下运行一个[有限元方法](@article_id:297335)（FEM）模拟，来预测一种复杂新型合金在应力下的变形。这个模拟的核心，在材料内部的每一个积分点上，都是一个*本构律*——一个关联[应力与应变](@article_id:297825)的方程。对于新材料，这些定律可能极其复杂，难以从第一性原理推导。混合方法就是用一个神经网络来取代这个解析方程。有限元求解器照常进行，但每当它需要知道某一点上给定应变的应力时，它就查询[人工神经网络](@article_id:301014)。网络变成了一种“数字材料”，一个从实验数据中学到的可编程本构律 [@problem_id:2656045]。

但在这里我们面临一个新的挑战：物理合理性。一个幼稚的网络可能学习了数据，但违反了像[能量守恒](@article_id:300957)这样的基本物理原则。这催生了*[物理信息](@article_id:312969)*[网络架构](@article_id:332683)的发展。例如，在为一个[超弹性材料](@article_id:369306)建模时，我们知道应力必须可以从一个标量应变能势 $\psi$ 推导出来，即 $\boldsymbol{\sigma} = \partial \psi / \partial \boldsymbol{\epsilon}$。与其直接学习[应力-应变关系](@article_id:337788)，我们可以设计网络来表示势能 $\psi$，然后通过[自动微分](@article_id:304940)计算网络输出对其输入的[导数](@article_id:318324)来得到应力。这通过构造*保证*了最终的模型是[超弹性](@article_id:319760)的并且[能量守恒](@article_id:300957)。我们还可以更进一步。通过精心选择网络的输入为在旋转下不变的量（[标架无关性](@article_id:376074)），并将[能量分解](@article_id:372528)为体积（改变尺寸）和等容（改变形状）部分，我们可以将自然界的基本对称性直接融入[网络架构](@article_id:332683)中。网络不再是一个黑箱；它是一个被约束以遵守物理定律的灵活数学结构 [@problem_id:2898899]。

将[人工神经网络](@article_id:301014)作为一个更大系统内部可学习组件的想法也彻底改变了控制理论。考虑为机器人手臂或无人机设计一个控制器的问题，其确切的摩擦力和[空气动力学](@article_id:323955)特性是未知的。在像[自适应反步法](@article_id:354036)这样的方法中，控制器拥有一个它试图控制的系统的模型。我们可以将一个[人工神经网络](@article_id:301014)置于此模型内部，以表示描述系统漂移动力学的未知函数 $f(x)$。随着系统的运行，网络的权重会实时更新，使控制器能够即时适应和学习未知的物理特性，即使在存在显著不确定性的情况下也能确保稳定和鲁棒的性能 [@problem_id:2693965]。

### 一种新形式的发现：作为物理[拟设](@article_id:363651)的网络

我们现在来到了最深刻、最令人脑洞大开的应用。到目前为止，网络一直被用来学习一个*关于*物理系统的函数。如果网络*就是*这个物理系统呢？如果我们研究的对象本身——一个量子系统的[波函数](@article_id:307855)——可以由一个[神经网络](@article_id:305336)来表示呢？

这就是在量子物理学中使用[人工神经网络](@article_id:301014)作为变分[拟设](@article_id:363651)（variational [ansatz](@article_id:363651)）的核心思想。根据量子力学的[变分原理](@article_id:324104)，一个系统的真实[基态](@article_id:312876)是使其[能量期望值](@article_id:353094)最小化的那个状态。挑战在于，一个多体系统的[波函数](@article_id:307855)是一个具有天文数字般复杂度的对象。对于一个只有 $N$ 个自旋的系统，[波函数](@article_id:307855)是一个包含 $2^N$ 个复数的列表——这个数字即使对于几十个粒子也很快变得无法存储。

革命性的提议是，不用一个巨大的列表来表示这个[波函数](@article_id:307855)，而是用一个紧凑高效的[神经网络](@article_id:305336)。自旋的状态 $s=(s_1, \dots, s_N)$ 作为输入送入网络，而输出 $\Psi_{\theta}(s)$ 是该构型的[波函数](@article_id:307855)振幅。网络的参数 $\theta$ 成为变分参数。现在，寻找[基态](@article_id:312876)变成了寻找一组能够最小化变分能量 $E(\theta) = \langle \Psi_{\theta} | \hat{H} | \Psi_{\theta} \rangle / \langle \Psi_{\theta} | \Psi_{\theta} \rangle$ 的[权重和偏置](@article_id:639384)。这是一个优化问题，可以用我们训练任何其他网络时使用的基于梯度的方法来解决。网络凭借其表示复杂高维函数的能力，为近似求解多体系统的薛定谔方程提供了一种强大的新方法，而这个问题近一个世纪以来一直是物理学的核心挑战 [@problem_id:2410566]。

在这里，[人工神经网络](@article_id:301014)完成了它的转变。它始于一个不起眼的模式分类器。它成为一个计算加速器，然后是[嵌入](@article_id:311541)物理定律中的一个组件。现在，它已成为物理理论本身的候选者——一种对[量子态](@article_id:306563)的压缩的、可学习的描述。这段旅程揭示了[人工神经网络](@article_id:301014)的真正力量：它们提供了一种灵活、强大且统一的语言来描述复杂关系，无论这些关系存在于经济数据中、蛋白质的核心，还是量子现实的基本结构中。