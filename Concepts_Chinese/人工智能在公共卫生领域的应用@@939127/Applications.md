## 应用与跨学科联系

现在我们已经探讨了人工智能在公共卫生领域运作的基本原理，你可能会想：我们能用它*做*什么？事实证明，答案惊人地广泛。这个领域的真正魅力不仅在于算法的巧妙，更在于它们如何成为一面透镜、一个工具和一个伙伴，参与到人类最古老的追求之一：为所有人建立一个更健康的社会。这段旅程将带我们从新生儿血液的微观分析到[全球治理](@entry_id:202679)的宏大舞台，揭示代码、伦理、法律和哲学之间的深刻联系。

我们的出发点是一个简单而有力的想法。任何公共卫生干预的目标，从一种新疫苗到一则公共服务公告，都是为了改变概率——降低生病的几率，提高康复的几率。例如，如果一个为阿片类药物使用障碍提供当日治疗的项目，将患者开始治疗的概率从 $0.30$ 提高到 $0.55$，这就代表了一项巨大的胜利。这个 $0.25$ 的绝对增幅意味着每四个符合条件的人中就多出一人开始治疗——这是一个切实、能拯救生命的影响。人工智能，就其本质而言，是一个发现和放大这类变化的强大引擎 [@problem_id:4553969]。

### 数字临床医生的助手：增强我们的感知

让我们从健康通常开始的地方开始：单个患者。医学的很大一部分是关于模式识别的，而这正是人工智能可以作为一个不知疲倦、极其敏锐的助手，将我们的感知力提升到人眼或简单规则无法企及的程度的地方。

想象一下一个州[新生儿筛查](@entry_id:275895)项目的工作。每天，成千上万的干血斑样本送达，用于检测罕见但具有毁灭性的[遗传病](@entry_id:273195)。对于像[中链酰基辅酶A脱氢酶缺乏症](@entry_id:173095)（MCAD）这样的疾病，实验室会分析来自质谱仪的复杂图谱。多年来，标准方法是使用固定的阈值——如果物质A除以物质B的水平高于某个数值，就标记出来进行随访。这种方法有效，但它是一个粗糙的工具。它能捕捉到大多数患病婴儿，但也会标记出许多健康的婴儿，导致他们的父母在等待确诊测试期间经历数天或数周的恐惧。

现在，让我们引入一个[机器学习分类器](@entry_id:636616)。人工智能不再使用简单的规则，而是在成千上万个过去的例子上进行训练，学习那些真正指示疾病的微妙、复杂的分子信号交响曲。结果是显著的。人工智能的敏感性可能稍低一些，也许会漏掉百分之一的患病婴儿，而旧规则本可以捕捉到。但它真正的力量在于特异性的显著提高——即正确识别健康个体的能力。通过将特异性从（比如说）$0.99$ 提高到 $0.996$，[假阳性](@entry_id:635878)的数量可以减少一半以上。在一个筛查50万新生儿的州，这意味着将受惊吓的家庭数量从5000个减少到2000个。阳性预测值——即阳性检测结果为真阳性的几率——增加了一倍多。人工智能不仅找到了患病婴儿，它还防止了巨大的附带伤害。这就是使用复杂工具来解读复杂数据，将信息洪流转化为精确、可操作智慧的魔力 [@problem_id:5066636]。

这种优化资源的原则远远超出了实验室的范围。思考一下与麻风病等疾病的斗争。公共卫生项目资源有限，无法对已确诊患者的所有密切接触者进行随访。你先检查谁？医生可能会根据几个因素凭直觉判断。但人工智能可以系统地权衡数千名接触者的数十个变量：指示病例的细菌载量、睡眠距离、接触者的年龄、疫苗接种状况等等。然后，它可以为每个人生成一个风险评分。通过对接触者进行排序并专注于前k个风险最高的个体，该项目可以用其有限的随访名额最大化地发现新病例。这并非要取代临床医生，而是要赋予他们一种超能力——一张地图，引导他们的努力投向最能产生效益的地方。并且，为了确保这不是一个神秘的“黑箱”，像SHAP（Shapley Additive exPlanations）这样的方法可以解释模型*为什么*给某个人分配了高风险，从而培养信任和问责制 [@problem_id:4670682]。

### 建筑师与瞭望塔：人工智能在更广阔的健康世界中

当我们从个体放大到社区，人工智能的角色从临床医生的助手转变为社会建筑师和瞭望塔。在这里，技术挑战与伦理、法律和社会正义深度交织，迫使我们直面公共卫生中最棘手的一些问题。

#### 地图与领土：人工智能神谕的伦理

一个人工智能模型可以被训练来预测社区层面的阿片类药物滥用热点，使用的数据包括救护车呼叫记录和去识别化的健康记录。这似乎是一个巨大的好处，让卫生官员能够有针对性地开展预防工作和分配资源。但地图不是领土，标签可能成为一种烙印。当一张面向公众的地图宣布某个社区为“高风险”时，会发生什么？

即使所有数据在个体层面都已按照HIPAA等法律要求完美地去识别化，但社区本身并不是匿名的。这样的标签可能导致污名化、歧视性的服务分配（一种[新形式](@entry_id:199611)的红线政策）以及财产价值下降。居民们并没有同意让他们的社区被画像。这揭示了我们传统法规中的一个深刻空白：保护人类研究受试者的通用规则（Common Rule）关注的是个体伤害，但群体层面的伤害呢？行善（do good）和公正（be fair）的伦理原则迫使我们超越单纯的法律合规。在没有进行深入的社区咨询以减轻这些可预见的伤害的情况下发布这样的模型，将是一种伦理失当，表明技术上的去识别化不能替代社会责任 [@problem_id:4427460]。

#### 救生艇与账本：危机中的人工智能

没有什么地方比在公共卫生紧急情况下的风险更高。当资源稀缺、时间紧迫时，人工智能似乎可以成为一个客观的仲裁者。但它的部署迫使我们直面我们最深层的价值观。

想象一种新型病毒爆发，只有10000份救命的预防药物可用。一个人工智能模型可以预测哪些人口普查区域风险最高。纯粹的功利主义方法是将药物送到模型预测能拯救最多生命的地方。但如果风险最高的区域也恰好是历史上被边缘化、遭受忽视和歧视的社区的家园呢？又如果人工智能模型本身对这些社区的准确性较低，因为它们在训练数据中代表性不足呢？

这是一个被技术强化的经典救生艇伦理问题。一种真正公正的、植根于生物伦理四项原则的方法，需要的不仅仅是运行算法。**公正**要求我们审计模型的偏见，并考虑进行调整以避免延续历史错误。**不伤害**（do no harm）要求我们以非污名化的方式沟通风险。**尊重自主**意味着与社区互动，对策略保持透明，并确保参与是自愿的。基于风险分配资源可以是公平的，但前提是必须伴随着这些深刻的公平保障措施 [@problem_id:4435479]。

危机也考验着个人权利与集体利益之间的平衡。假设一个人工智能监控系统可以通过分析你的智能吸入器数据和手机位置来检测疫情爆发。卫生部门根据紧急命令，要求获取你的可识别数据以进行快速接触者追踪。你以隐私权为由拒绝。应该怎么办？这不是一个权利压倒另一个权利的简单案例。美国和欧盟的公共卫生法都包含相关规定（如HIPAA的公共卫生例外条款和州警权），允许在紧急情况下进行此类披露。但这种权力并非绝对。它受到**必要性**、**相称性**和使用**最少限制手段**原则的约束。一个精心设计的治理框架不会是一个全有或全无的选择。它会使用去识别化的数据进行[一般性](@entry_id:161765)监控，但会建立明确、预定义的阈值。只有当一个人的暴露概率超过一个关键风险阈值时，系统才会升级到使用可识别数据，确保对自主权的侵犯在法律上是正当的，并且在伦理上与所要预防的伤害相称 [@problem_id:4429807]。

但如果人工智能工具本身未经证实呢？在紧急情况的混乱中，一家医院可能会倾向于部署一种新的人工智能分诊工具来帮助分配呼吸机。开发者可能会指出联邦的紧急使用授权（EUA）或州一级的危机护理标准（CSC）为其提供了法律掩护。这是一个关键的法律区别。FDA的EUA是允许在收益被认为大于风险时使用未经批准产品的临时许可；它不是对安全性的全面验证，也不是免于责任的护盾。CSC是州一级的协议，重新定义了在极端稀缺情况下什么是“合理”护理；它们调整了护理标准，但没有消除它。这两种机制都不能提供全面的豁免权。它们是精心构建的护栏，而不是免罪金牌，旨在在快速创新与基本护理责任之间的危险地带航行 [@problem_id:4494804]。

### 游戏规则：从实验室到全球的治理

鉴于这些工具的力量和危险，建立明确的游戏规则至关重要。这种治理在从单一医院到国际社会的各个层面上运作。

任何机构的一个基本问题是对其人工智能活动进行分类。当一家医院部署人工智能脓毒症警报时，它是在进行旨在产生普适性知识的**研究**吗？如果是，它需要机构审查委员会（IRB）的严格监督。这是州卫生部门为控制疫情而授权的**公共卫生实践**吗？在这种情况下，可能不需要IRB审查。或者，它仅仅是一个旨在改善医院自身护理的内部**质量改进**项目？这通常也超出了IRB的管辖范围。根据意图、设计和传播计划做出正确的区分，是应用正确伦理和监管规则的关键第一步 [@problem_id:4427513]。

扩大规模来看，想象一个人工智能系统发现了一种新型抗生素和一种针对大流行病的新诊断算法。谁拥有这项发现？如果一家公司为其申请专利，他们可能会设定一个低收入国家无法承受的价格，这违反了全球正义原则。在这里，人工智能的世界与国际法和经济学相交。存在一个丰富的机制工具箱来平衡创新激励与获取途径。**强制许可**，在TRIPS协定下是允许的，允许政府在卫生紧急情况下授权仿制药生产。**专利池**是一种自愿协议，多个专利持有者将其技术以合理条款共同许可，从而扫清法律障碍。一项**FRAND（公平、合理和非歧视性）**许可承诺可以确保一个诊断算法如果成为技术标准的一部分，对所有人都是可及的。而在流程的最开始，一个**数据共享平台**可以为训练这些模型所需的庞大数据集提供一个受治理的、合乎伦理的基础设施 [@problem_id:4427989]。

这把我们带到了最深层的问题。当我们委托人工智能分配资源或指导政策时，我们常常指示它要“公平”。但公平意味着什么？是纯粹的**功利主义**，最大化挽救的总生命年数，即使这意味着某些群体被抛下？是一种**道义论**方法，即个人的绝对权利——比如不被置于有风险、未经证实的技技术下的权利——不能为了任何社会收益而被侵犯？是一种**罗尔斯式（Rawlsian）**的正义观，即我们的首要任务必须始终是改善处境最不利者的状况，即使以牺牲一些总体效率为代价？还是**能力方法**，它认为目标不仅仅是分配医疗保健，而是要确保每个人都有实现良好健康的实质性自由和真实机会，解决阻碍他们前进的结构性障碍？

这些不是有可编程答案的技术问题。它们是深刻的人类问题。伦理框架的选择——它决定了我们如何处理人工智能性能中的历史劣势和种族不平等等问题——是关于我们想要建立什么样的社会的选择。最终，公共卫生领域的人工智能是一面镜子。它反映了我们的独创性、我们的同情心，以及最终，我们的价值观 [@problem_id:4423955]。从代码到治愈的旅程不仅仅是一次技术之旅；它是一次伦理和社会的奥德赛，挑战我们不仅要更聪明，还要更明智。