## 引言
我们如何衡量两首歌、两张图片或两种思想之间的“距离”？虽然标准的[欧几里得距离](@article_id:304420)是衡量物理空间的可靠工具，但它往往无法捕捉复杂数据中微妙的相似性概念。这种“一刀切”的标尺所存在的不足，给[数据分析](@article_id:309490)带来了巨大挑战，导致分类效果不佳和模式误读。本文旨在通过介绍**[度量学习](@article_id:641198)（metric learning）**来弥补这一差距，这是一个直接从数据中学习距离函数的强大框架。首先，我们将探讨其核心的**原理与机制**，从标准度量的局限性，到[马氏距离](@article_id:333529)（Mahalanobis distance）的优美几何学，再到用于学习度量的优化技术。随后，在**应用与跨学科联系**一章中，我们将带领您进行一次巡礼，揭示这一理念如何在深度学习、免疫学和[经典统计学](@article_id:311101)等不同领域中统一概念并解决问题。

## 原理与机制

想象一下，你正在整理自己的音乐库。是什么让两首歌“相似”？仅仅是每分钟节拍数（BPM）吗？是调性吗？还是“能量”水平？我们几何课上学到的标准尺子——[欧几里得距离](@article_id:304420)——衡量的是两点之间的直线距离。它简单、可靠，在测量足球场上的距离时效果绝佳。但对于比较歌曲、图片甚至人来说，这种“一刀切”的方法可能会出奇地笨拙。

### 标准尺度的束缚

让我们回到音乐库的例子。假设我们仅用两个特征来表示每首歌曲：其节奏（以 BPM 为单位）和“[频谱](@article_id:340514)通量”（一个描述声音音色变化速度的数值）。一首快节奏的朋克歌曲可能是（180 BPM, 0.8 flux），而一首慢节奏的氛围音乐则是（60 BPM, 0.1 flux）。现在，假设我们有一首种子歌曲，其特征为（120 BPM, 0.25 flux）。下面两个候选项中，哪一个更“不相似”？
*   候选项 A: (130 BPM, 0.25 flux) — [相差](@article_id:318112) 10 BPM。
*   候选项 B: (120 BPM, 0.35 flux) — [相差](@article_id:318112) 0.1 flux。

原始数值不具可比性！10 BPM 的差异很常见，但 0.1 的[频谱](@article_id:340514)通量差异可能就非常巨大。[欧几里得距离](@article_id:304420)公式 $\sqrt{(\Delta \text{BPM})^2 + (\Delta \text{flux})^2}$ 将完全被数值范围较大的特征所主导。

我们可以通过[归一化](@article_id:310343)特征来解决这个问题，即缩放它们，使其具有相似的取值范围。但即便如此，仍潜藏着一个更深层次的问题。如果我们的耳朵对节奏的微小变化比对[频谱](@article_id:340514)通量的微小变化更敏感呢？或者，通常情况下，一首在节奏和[频谱](@article_id:340514)通量上都有中等程度差异的歌曲，听起来比一首在某个特征上完全相同但在另一个特征上差异巨大的歌曲更相似，那该怎么办？

假设[归一化](@article_id:310343)后，我们有两个与种子歌曲的差异向量：$\boldsymbol{d}_1 = (10, 0)$ 和 $\boldsymbol{d}_2 = (5, 5)$。使用标准的欧几里得（$L_2$）距离，我们发现 $\boldsymbol{d}_1$ “更远”（$\sqrt{10^2+0^2}=10$），而 $\boldsymbol{d}_2$ 则“更近”（$\sqrt{5^2+5^2} \approx 7.07$）。这与“在单个特征上的巨大偏差比在多个特征上的中等偏差更糟糕”这一认知相符。相比之下，曼哈顿（$L_1$）距离会认为它们同样遥远（$|10|+|0| = 10$ 和 $|5|+|5|=10$）[@problem_id:3285944]。这个简单的思想实验揭示了一个深刻的真理：**衡量距离的正确方法取决于具体情境**。我们抽象的“歌曲空间”的几何结构不一定是欧几里得的。我们需要为这项任务学习正确的尺子。

### 锻造新标尺：[马氏距离](@article_id:333529)

我们如何才能创建一把为数据量身定制、具有数据感知能力的尺子？答案在于对欧几里得距离进行推广。两个向量 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 之间的平方欧几里得距离是 $(\boldsymbol{x}-\boldsymbol{y})^\top I (\boldsymbol{x}-\boldsymbol{y})$，其中 $I$ 是[单位矩阵](@article_id:317130)。如果我们用一个自定义的矩阵（称之为 $M$）来替换单位矩阵，会怎么样？

这就引出了**[马氏距离](@article_id:333529) (Mahalanobis distance)**：
$$
d_M(\boldsymbol{x}, \boldsymbol{y})^2 = (\boldsymbol{x} - \boldsymbol{y})^\top M (\boldsymbol{x} - \boldsymbol{y})
$$

这个矩阵 $M$ 就是我们新尺子的配方。为了成为一个有效的距离配方，$M$ 必须是**对称且正半定 (PSD)** 的。这是一个数学上的保证，确保最终的平方距离总是非负的。如果 $M$ 是严格正定的（其所有[特征值](@article_id:315305)都为正），那么 $d_M(\boldsymbol{x}, \boldsymbol{y})=0$ 仅当 $\boldsymbol{x}=\boldsymbol{y}$ 时成立，这给了我们一个真正的**度量 (metric)**。如果 $M$ 是半定但非正定的（某些[特征值](@article_id:315305)为零），它定义了一个**[伪度量](@article_id:312184) (pseudometric)**，即两个不同的点之间的距离可以为零。这不是一个缺陷，而是一个特性！这意味着该度量已经学到，沿某些方向的差异是完全无关紧要的，从而有效地执行了[降维](@article_id:303417) [@problem_id:3129992]。

[马氏距离](@article_id:333529)有一个优美的几何解释。由于 $M$ 是正半定的，我们可以将其分解为 $M = A^\top A$。距离公式于是变为：
$$
d_M(\boldsymbol{x}, \boldsymbol{y})^2 = (\boldsymbol{x} - \boldsymbol{y})^\top A^\top A (\boldsymbol{x} - \boldsymbol{y}) = \|A(\boldsymbol{x} - \boldsymbol{y})\|_2^2
$$
这个极为简洁的方程告诉我们，原始空间中的[马氏距离](@article_id:333529)，不过是新空间中的标准[欧几里得距离](@article_id:304420)，而这个新空间里，我们所有的数据点都经过了矩阵 $A$ 的变换。学习度量 $M$ 就等同于学习一个[线性变换](@article_id:376365) $A$——一副新的“眼镜”——它通过拉伸、压缩和旋转空间，使数据的内在结构更加清晰可见。

想象二维空间中的两类点。使用标准的[欧几里得度量](@article_id:307612)，分隔它们的边界可能是一条简单的直线。但如果我们学习一个对角矩阵 $M = \mathrm{diag}(\alpha, \beta)$，我们实际上是在重新加权坐标轴。如果 $\alpha > \beta$，我们等于是在说第一个特征更重要。从某点出发的[等距](@article_id:311298)“圆”会变成椭圆，并沿着第一个轴被压缩。因此，像 k-近邻 (k-Nearest Neighbors) 这样的分类器的[决策边界](@article_id:306494)会扭曲和移动，以适应这种新的几何结构，从而有望更好地分离类别 [@problem_id:3121545]。

### 从经验中学习：如何找到正确的度量

所以，矩阵 $M$ 是秘密武器。但我们如何找到它呢？我们从数据中学习它。这就是**[度量学习](@article_id:641198)**中的“学习”二字。

最直观的学习方式是从样例中学习。假设我们得到一些被标记为“相似”和“不相似”的数据对。我们可以将此转化为对度量 $M$ 的一组约束条件：
*   对于一个**相似**对 $(\boldsymbol{x}_i, \boldsymbol{x}_j)$，我们希望它们的距离很小：$d_M(\boldsymbol{x}_i, \boldsymbol{x}_j)^2 \le \alpha$。
*   对于一个**不相似**对 $(\boldsymbol{x}_i, \boldsymbol{x}_k)$，我们希望它们的距离很大：$d_M(\boldsymbol{x}_i, \boldsymbol{x}_k)^2 \ge \beta$。

一个更强大的表述方式使用**三元组 (triplets)** 数据点：一个“锚点”$\boldsymbol{x}_i$，一个“正例”$\boldsymbol{x}_j$（与锚点同类），以及一个“负例”$\boldsymbol{x}_k$（与锚点不同类）。约束条件是，锚点到正例的距离应该比到负例的距离更近，并且至少要有一个裕度（比如 1）：
$$
d_M(\boldsymbol{x}_i, \boldsymbol{x}_j)^2 + 1 \le d_M(\boldsymbol{x}_i, \boldsymbol{x}_k)^2
$$

学习问题现在成了一个优化任务：找到一个满足尽可能多约束条件的 PSD 矩阵 $M$。每个约束条件，如 $(\boldsymbol{x}_i - \boldsymbol{x}_j)^\top M (\boldsymbol{x}_i - \boldsymbol{x}_j) \le \alpha$，都是关于 $M$ 元素的一个[线性不等式](@article_id:353347)。这意味着该问题通常可以被构建成一种优美的[凸优化](@article_id:297892)问题，称为**[半定规划 (SDP)](@article_id:332315)** [@problem_id:3168746]。

但我们必须小心。我们不希望得到一个过于复杂的度量，以至于它只是记住了训练数据，而无法泛化到新的、未见过的数据上。我们需要引入一种**[归纳偏置](@article_id:297870) (inductive bias)**，即对“更简单”度量的偏好。这通过**[正则化](@article_id:300216) (regularization)** 来实现。一种常见的方法是在我们的优化目标中加入一个[惩罚复杂度](@article_id:641455)的项，例如，尝试最小化 $M$ 的迹 $\operatorname{tr}(M)$。由于迹是[特征值](@article_id:315305)的总和，而对于 PSD 矩阵，[特征值](@article_id:315305)是非负的，因此最小化迹会鼓励许多[特征值](@article_id:315305)接近于零。这是寻找[低秩矩阵](@article_id:639672)的凸代理方法，它对应于一种“更简单”的度量，能执行一种强形式的[降维](@article_id:303417) [@problem_id:3168746]。一种更强的偏置是将 $M$ 限制为对角矩阵，这将问题简化为仅学习每个特征的最[优权](@article_id:373998)重 [@problem_id:3129992]。找到这个最优 $M$ 的过程可以通过投影[次梯度下降](@article_id:641779)法等[算法](@article_id:331821)完成，该[算法](@article_id:331821)会迭代地优化 $M$，并将其投影回 PSD 矩阵集合中，以确保它始终是一个有效的度量配方 [@problem_id:3192833]。

### 实践中的[度量学习](@article_id:641198)：从高维到深度球面

在现实世界中，数据往往是混乱且高维的。这正是[度量学习](@article_id:641198)大放异彩的地方。

现代数据科学面临的最大挑战之一是**维度灾难 (curse of dimensionality)**。当我们拥有的特征远多于数据点时（$p \gg n$），试图用标准[样本协方差矩阵](@article_id:343363) $S$ 来估计数据的“形状”简直是灾难。矩阵 $S$ 会变得奇异——它有零[特征值](@article_id:315305)——这意味着它在某些方向上已经坍缩了。用它的逆矩阵来定义[马氏距离](@article_id:333529)将涉及除以零。解决方案是[正则化](@article_id:300216)。通过使用正则化估计 $S_\lambda = S + \lambda I$，我们实际上是在用一个很小的量 $\lambda$“膨胀”那些坍缩的维度。这使得矩阵可逆，距离定义良好，从而在数据告诉我们的信息（在 $S$ 中）和鲁棒的默认值（欧几里得的 $I$）之间架起了一座桥梁 [@problem_id:3181589]。

在**[深度学习](@article_id:302462)**的世界里，模型学习将图像或文本等复杂对象映射为称为**[嵌入](@article_id:311541) (embeddings)** 的数值向量。深度[度量学习](@article_id:641198)中一种强大的技术是将这些[嵌入](@article_id:311541)向量约束在单位球面上（即它们的 $L_2$ 范数为 1）。这带来了一个深刻而优雅的结果。两个向量 $\boldsymbol{w}$ 和 $\boldsymbol{x}$ 之间的[点积](@article_id:309438) $\boldsymbol{w}^\top \boldsymbol{x}$ 等于 $\|\boldsymbol{w}\|_2 \|\boldsymbol{x}\|_2 \cos\theta$。当范数固定为 1 时，这简化为 $\boldsymbol{w}^\top \boldsymbol{x} = \cos\theta$。此外，平方欧几里得距离变为 $\|\boldsymbol{w}-\boldsymbol{x}\|_2^2 = 2(1 - \boldsymbol{w}^\top \boldsymbol{x})$。突然间，最大化[点积](@article_id:309438)（或[余弦相似度](@article_id:639253)）与最小化欧几里得距离变得完[全等](@article_id:323993)价！这种简单的[归一化](@article_id:310343)技巧消除了[向量大小](@article_id:351230)的干扰影响，迫使模型完全专注于学习[嵌入](@article_id:311541)向量之间最优的相对*角度* [@problem_id:3198364]。

如果我们根本没有任何标签呢？我们还能学到一个有用的度量吗？这就是**无监督[度量学习](@article_id:641198)**的领域。其指导原则是：一个好的度量是能揭示数据内在结构的度量。例如，我们可以构建一个相似性图，其中边的权重取决于点之间的距离。然后，我们会寻找一个能使这个图看起来“最具[聚类](@article_id:330431)性”的度量。图的[聚类](@article_id:330431)结构的清晰度可以通过其谱来衡量——具体来说，是其**[图拉普拉斯算子](@article_id:338883) (graph Laplacian)** 的第二小[特征值](@article_id:315305)（$\lambda_2$）。最小化这个[特征值](@article_id:315305)对应于找到一个能在数据中创造出最清晰的[聚类](@article_id:330431)间“瓶颈”的度量 [@problem_id:3117855]。

### 另一条路：三角不等式的神圣性

马氏框架之所以强大，是因为通过其设计，它产生的任何距离都自动满足度量的基本属性，包括著名的**三角不等式 (triangle inequality)**：从 A 到 C 的距离永远不会大于从 A 到 B 的距离加上从 B 到 C 的距离。

但如果我们采取一种完全不同的方法呢？与其学习一个矩阵 $M$ 来*计算*距离，不如训练一个模型来直接预测距离值 $\hat{y}_{ij}$？这似乎更简单，但它有一个主要的陷阱：模型的预测值不保证满足三角不等式。一个模型可能会预测纽约到芝加哥的距离是 800 英里，芝加哥到洛杉矶的距离是 2000 英里，但纽约到洛杉矶的距离是 3500 英里——这在物理上是不可能的。为了使这样的模型有用，我们需要在我们的学习目标中明确添加一个惩罚项，对数据中所有点三元组违[反三角不等式](@article_id:306523)的情况进行惩罚 [@problem_id:3170643]。

这种对比凸显了马氏方法的优雅之处。通过将我们的搜索范围限制在正半定矩阵的世界里，我们将几何定律直接构建到我们的尺子中。我们不仅学习了距离，还学习了一个连贯、自洽的空间，这些距离就存在于这个空间之中。这是[度量学习](@article_id:641198)的核心原则和持久的力量：让数据本身教会我们它所占据的空间的本质。

