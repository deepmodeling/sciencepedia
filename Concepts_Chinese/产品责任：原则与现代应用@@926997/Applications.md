## 应用与跨学科联系

产品责任的原则诞生于蒸汽机和流水线的时代，在我们这个充满智能算法和合成生物学的现代世界里，它们似乎是古老的遗物。然而，事实恰恰相反。那些基础理念——产品必须对其预期用途足够安全，其风险必须被清晰传达，其创造者必须对其造成的伤害承担责任——比以往任何时候都更为重要。最引人入胜的是，观察这些古老而坚固的原则如何与那些对于最初构想它们的律师和法官来说纯属科幻小说的技术所带来的挑战进行博弈，并最终驾驭它们。这不是一个旧法律在新压力下崩溃的故事，而是其非凡灵活性和持久智慧的体现。

### 机器中的幽灵：软件、AI与医疗设备

最初的巨大挑战之一是机器中的幽灵本身：软件。一串由0和1组成的、无形的指令集，能否成为一个“有缺陷”的“产品”？法律已经响亮地回答：“是”。当软件被集成到一个设备中，并有能力造成物理伤害时，它就不再仅仅是信息；它是一个关键部件，其后果与任何齿轮或杠杆一样真实。

但界限在哪里？监管机构和法院不得不变得出人意料地精通技术。想象一个临床决策支持（CDS）系统，它分析患者数据并向医生建议药物剂量。如果该软件是一个“玻璃盒”——向医生展示其使用的底层规则和数据点，允许医生对其逻辑进行真正的、独立的审查——它可能被视为一个纯粹的信息工具。但如果它是一个“黑箱”，只呈现最终建议而不揭示其推理过程，它就要求临床医生依赖其输出。在那个点上，它不再是一个单纯的工具，而成为一个受监管的医疗设备，随之而来的是所有的法律责任 [@problem_id:4505206]。

一旦我们同意软件可以是一个有缺陷的产品，我们就必须问：算法中的缺陷*看起来*是什么样的？答案与软件本身一样多种多样和复杂。

有时，缺陷是一个简单的、经典的错误。考虑一个为国际使用而设计的AI，它必须判断患者的实验室结果是否足够新近以具有相关性。如果其代码未能正确地将不同时区的时间戳标准化——例如，将一个UTC时间与一个本地时间进行比较而未考虑时差——它可能会错误地丢弃一个完全有效的新近实验室结果。这个小小的编码错误可能导致一系列的失败，使AI错失一个关键的诊断 [@problem_id:4400513]。在这种失败之后，“可解释性AI”的新工具可以像算法的飞行数据记录器一样工作。一种称为反事实解释的技术可以数字方式回溯事件，提问：“要改变结果，输入的最小变化是什么？”当分析显示，*唯一*需要改变的是软件正确读取时间戳时，它就建立了一个清晰的“若无，则不”的因果关系，直接指向软件数据摄取管道中的设计缺陷 [@problem_id:4400513]。

其他缺陷不在于AI的深层逻辑，而在于表面，在计算机与人之间的关键界面上。想象一个出色的AI剂量[辅助系统](@entry_id:142219)，其用户界面使用几乎无法区分的蓝色和绿色阴影来代表标准剂量与十倍过量。如果一个匆忙的临床医生在繁忙的医院里点击了错误的颜色，这仅仅是“人为错误”吗？产品责任法说不。一个使可预见的错误易于发生且后果灾难性的设计，本身就是一种缺陷设计。对人为因素的可预见性是安全产品设计的基石，无论界面是一个物理旋钮还是一组屏幕上的像素 [@problem_id:4494865]。

也许算法缺陷最深刻、最具挑战性的形式源于偏见。考虑一个可穿戴设备，它使用基于光的传感器来检测危险的[心律失常](@entry_id:178381)。该技术在公司的内部测试中表现出色。然而，[光吸收](@entry_id:136597)的物理原理意味着该传感器在肤色较深的人身上效果显著较差。公司的工程师知道这一点，甚至开发了一种可行的替代设计，采用双波长传感器，以中等成本解决了这个问题。但是，为了优先考虑快速上市，公司发布了最初的有偏见的版本，并附有一个模糊的营销声明，称其“适用于不同用户”。当一个肤色较深的人因漏诊[心律失常](@entry_id:178381)而中风时，随之而来的诉讼不是关于制造中的错误；而是关于设计中的一个根本[性选择](@entry_id:138426)。这里的风险-效用平衡是鲜明的：对整个人群的伤害风险与更快的上市时间所带来的效用相权衡。由于未能为其*所有*可预见的用户设计一个合理安全的产品，尤其是在已知存在成本效益高的替代方案的情况下，该公司创造了一个根植于健康不平等的有设计缺陷的产品 [@problem_id:5014165]。此外，由于未能警告用户这一已知限制，它也构成了警示缺陷，双重违反了其对公众的责任。

当然，AI及其制造商并非唯一的参与者。医院不仅仅是技术的被动消费者。它对患者负有直接责任，以确保其系统是安全的。如果医院仓促实施一个新的AI系统，缩减安全验证，并提供不足的培训，当该系统导致患者伤害时，它可能因公司过失而被判有责 [@problem_id:4494865]。这种责任延伸到技术的管理方式。如果一名医院雇佣的数据科学家单方面更改了脓毒症AI的警报阈值，违反了制造商明确的、关乎安全的关键警告——例如，为了减少烦人的假警报而降低系统敏感度——医院本身就要为后果负责。制造商提供了无缺陷的产品和充分的警告，因此受到保护，责任转移到通过其雇员实际上滥用了该产品的机构 [@problem_id:4400470]。

这个“人在环路中”是最终的保障。即使有完美的AI，临床医生的专业判断仍然是护理标准。想象一个脓毒症AI发出了高风险警报。一个临床医生可能会盲目遵循建议，而不记录其推理过程或告知患者风险。另一个可能会进行独立评估，记录他们对AI推理的认同，并在继续操作前获得知情同意。第三个可能会根据患者特定的合并症，合理地决定推翻AI的建议，并记录他们采取更保守方法的理由。如果出现不良后果，责任将不仅仅取决于医生是否遵循了AI。它将取决于*过程*。将自己的判断力让渡给机器的临床医生会面临风险，而那些将AI作为工具来辅助自己有充分记录的专业判断的临床医生，无论结果如何，都在实践良好的医学，并且在法律上是可辩护的 [@problem_id:4499401]。

面对这个责任网络，供应商能否简单地洗手不干？他们常常试图这样做。供应商可能会向医院出售一个AI系统，并在最终用户许可协议（EULA）中声明医院承担所有责任。但侵权法保护人们免受伤害的责任通常比合同法更强大。虽然这样的合同可能*在两家企业之间*可以执行——例如，要求医院赔偿供应商——但它不能剥夺受害患者起诉危险缺陷产品制造商的权利 [@problem_id:4400484]。

### 重写生命本身：[生物技术](@entry_id:141065)时代的责任

法律的适应性旅程从硅的世界走向碳的世界——走向生物技术。一个基于[CRISPR](@entry_id:143814)技术、出售给诊所的基因编辑工具包是什么？是产品？是服务？还是生物制品？这些试剂是在商业中销售的合成制造商品，因此产品责任法适用。但这些不是普通产品。它们是“不可避免不安全的”。即使完美制造，危险的脱靶编辑风险也是该技术固有的。对于这类高风险、高回报的医疗产品，法律制定了一条特殊规则（通常称为“评论k”）。它保护制造商免受*设计缺陷*的索赔，理由是产品的巨大治疗价值超过了其固有风险。然而，这并非豁免权。如果产品受到污染（制造缺陷）或其风险警告不充分，制造商仍可能被追究严格责任 [@problem_id:4485718]。

然而，当同样的技术不是用于治疗疾病，而是用于“增强”——例如，为健康的青少年增强耐力时，法律的计算就发生了根本性的转变。在这里，风险-效用平衡被颠覆了。脱靶编辑和载体介导的伤害风险仍然巨大，但效用不再是拯救生命，而是提供一种选择性的生活方式益处。在这种背景下，“不可避免不安全”的辩护失去了其道德和法律效力。一个为增强而设计的产品，并未追求一种如此至关重要的社会利益，以至于我们必须容忍其固有的危险。营销模式也很重要。将如此复杂的干预措施直接出售给消费者，绕过专家医生的监督，剥夺了“知情中介”原则的保护，并将全部警告责任完全置于制造商身上 [@problem_id:4863259]。

### 一种通用的安全语言

这些挑战并非一国独有。在世界各地，法律体系正面临同样的问题。例如，在欧盟，一部全面的新《AI法案》对高风险AI系统的供应商施加了严格的文档、风险管理和监督要求。然而，即使完全遵守这一尖端法规，也无法为严格的产品责任创造一个“安全港”。其原因十分深刻：监管批准证明了制造商遵循了一个特定的流程。而欧洲的《产品责任指令》下的严格责任，则以一个不同的、更全面的标准来评判产品：一个人有权期望的安全水平。如果一个产品，尽管获得了监管许可，但未能满足这种社会对安全的期望并造成了伤害，其制造商仍然可能被追究责任 [@problem_id:4400466]。

这揭示了法律中一种美妙的统一性。无论是在美国还是在欧盟，无论处理的是[机械压力](@entry_id:263227)机、软件算法还是[基因编辑](@entry_id:147682)工具，法律最终都回归到一组基础的、以人为中心的问题：产品在设计时是否为所有可预见的用户考虑了合理的安全？其风险是否以清晰和诚实的方式进行了传达？它是否达到了我们作为一个社会，对我们允许进入我们生活的强大技术所要求的安全基本期望？产品责任的原则为回答这些问题提供了持久的框架，证明了自己是法律最重要、适应性最强的创造之一。