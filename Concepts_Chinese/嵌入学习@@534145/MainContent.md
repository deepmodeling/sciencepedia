## 引言
我们如何教会机器理解诸如一个词、一件消费品，乃至一个生物分子等抽象概念背后的含义？用数字来表示我们世界中错综复杂的关系网是人工智能领域的一个根本挑战。传统方法常常力不从心，产生的表示不仅笨重、稀疏，而且难以被[模型解释](@article_id:642158)。[嵌入学习](@article_id:641946)提供了一种强大而优雅的解决方案，它提供了一个框架，能将复杂的关系转化为几何这一通用语言。它使我们能够创建密集的低维地图，其中点的邻近度和方向反映了它们所代表的概念之间的语义联系。

本文将对[嵌入学习](@article_id:641946)进行全面探索。我们首先将深入探讨“原理与机制”，追溯从奠基性的[分布假说](@article_id:638229)到[矩阵分解](@article_id:307986)的数学魔力，再到现代[对比学习](@article_id:639980)动态的学术历程。我们将揭示关于上下文的简单思想如何能转化为强大的几何表示。随后，“应用与跨学科联系”部分将展示[嵌入](@article_id:311541)非凡的通用性。我们将看到这一个思想如何革新了计算金融、[基因组学](@article_id:298572)、[推荐系统](@article_id:351916)和人工智能公平性等截然不同的领域，为分析和与复杂系统互动提供了一个统一的视角。

## 原理与机制

### “观其伴，知其义”

[嵌入学习](@article_id:641946)的核心是一个优美、简单而深刻的思想，由语言学家 John Rupert Firth 在1957年提出：“观其伴，知其义（You shall know a word by the company it keeps）。” 这就是**[分布假说](@article_id:638229)**，是我们整个事业的哲学基石。它认为，一个词的意义并非孤立的属性，而是由它出现的上下文所定义。

想一想“bank”这个词。如果我告诉你它出现在一个包含“money”、“loan”和“interest rates”的句子中，你立刻会想到金融机构。但如果我说它的邻居是“river”、“shore”和“slippery”，你则会想象到水道的岸边。上下文定义了意义。因此，我们的第一个挑战，就是如何用数学方式捕捉这种“伙伴”关系的概念。我们如何教会机器看到“bank”所交往的不同朋友圈？[@problem_id:3182897]

最直接的方法是计数。我们可以系统地通读大量文本，统计词对在彼此附近出现的频率。这就引出了**[共现矩阵](@article_id:639535)**的概念。

### 从词到数：[共现矩阵](@article_id:639535)

想象一张巨大的电子表格。每一行代表我们词汇表中的一个词，每一列也是。位于词A的行和词B的列[交叉](@article_id:315017)处的单元格中的数字，是A和B在给定上下文中一同出现的次数的计数。这个矩阵，我们称之为 $X$，是[分布假说](@article_id:638229)的原始数值体现。

当然，我们必须精确定义“一同出现”的含义。我们通常定义一个**上下文窗口**，比如目标词左右各五个词的小范围。如果一个词落在这个窗口内，它就被认为是邻居。此外，我们可能会认为更近的邻居更重要。紧邻目标词的词可能比五个词远的词告诉我们更多信息。我们可以通过用距离的倒数 $1/\Delta$ 来加权共现计数来编码这种直觉，其中 $\Delta$ 是词之间的距离。距离为1的词贡献计数1，距离为2的词贡献 $1/2$，依此类推。[@problem_id:3130247]

即便是这简单的一步也揭示了一个根本性的选择。我们是否允许上下文窗口跨越句子边界？如果允许，我们可能会意外地将一个句子的最后一个词与下一个句子的第一个词联系起来，产生无意义的共现。对于像“bank”这样的多义词，允许窗口跨越句子可能会将金融语境与河岸语境混合在一起，模糊了我们希望捕捉的区别。将窗口限制在句子内部通常会产生更清晰、更精确的意义信号。[@problem_id:3130247]

完成所有这些工作后，我们得到了[共现矩阵](@article_id:639535) $X$。代表“bank”的那一行是一长串数字，表示它与语言中其他所有词的共现情况。这个行向量*是*“bank”的一种表示，但不是一个很好的表示。对于一个有10万个词的词汇表，这个向量有10万个维度。它巨大、大部分由[零填充](@article_id:642217)（**稀疏**），而且笨重。这就像试图通过列出一个人见过的每一个人来表示这个人一样。这在技术上提供了信息，但并非一个有用的总结。这类似于使用[独热编码](@article_id:349211)来表示一个高基数类别变量——比如一支股票IPO的150个不同承销商。你会为每一个承销商得到一个巨大而稀疏的向量，这对于许多模型来说都难以优雅地处理。[@problem_id:2386917] 我们需要更好的东西。我们需要提炼其精髓。

### 寻找精髓：低秩思维的魔力

目标是将每个词的巨大、稀疏的向量压缩成一个短得多的**密集向量**——即**[嵌入](@article_id:311541)**。我们希望从一个10万维的向量变成，比如说，一个300维的向量，而不丢失关于意义的基本信息。这怎么可能呢？

关键的洞见在于，[共现矩阵](@article_id:639535)是高度冗余的。词与词之间的关系是结构化的。如果“cat”经常与“purr”和“meow”一起出现，而“dog”经常与“bark”和“fetch”一起出现，那么这里就存在一个潜在的“宠物般的叫声和行为”的概念。成千上万个单独的共现统计数据只是一些少数潜在语义主题的表象。发现这种潜在主题的数学工具是**矩阵分解**。

其中最优雅和成功的方法之一，**GloVe (Global Vectors)**，提出我们应该分解的对象不是原始[共现矩阵](@article_id:639535) $X$，而是它的对数 $\log(X)$。该模型试图为每个词找到两个[嵌入](@article_id:311541)向量，一个词向量 $w_i$ 和一个上下文向量 $\tilde{w}_j$，使得它们的[点积](@article_id:309438)近似于共现计数的对数：$w_i^\top \tilde{w}_j \approx \log(X_{ij})$。

但为什么要取对数？为什么 GloVe 的目标函数是一个奇特的加权平方误差和，$\sum_{i,j} f(X_{ij}) (w_i^\top \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2$？事实证明，这不是一个随意的选择。通过一段优美的推理，可以证明这个目标函数正是从一个有原则的统计模型出发所得到的结果。如果你假设“真实”的意义关系由[点积](@article_id:309438)给出，并且观察到的对数计数被方差与计数本身成反比的[高斯噪声](@article_id:324465)所破坏（这对计数数据来说是一个非常合理的假设），那么寻找最可能[嵌入](@article_id:311541)的过程——[最大似然估计](@article_id:302949)——会直接引导你到这个加权最小二乘目标，其中权重函数自然地呈现为 $f(X_{ij}) = X_{ij}$。[@problem_id:3130217] 这是一个绝佳的例子，说明一个看似临时的工程选择背后，其实有着深刻、有原则的根源。

### 意义的几何学

通过分解，我们将巨大的矩阵提炼成一组密集的低维向量。现在，有趣的部分开始了。我们已将词语映射到一个几何空间中。在这个空间里，向量之间的关系——它们的距离和角度——应该对应于意义之间的关系。“King”应该靠近“Queen”，“walking”应该靠近“ran”，并且从“king”到“queen”的向量应该与从“man”到“woman”的向量惊人地相似。

为了使这种几何关系变得清晰，一个标准做法是对[嵌入](@article_id:311541)向量进行**L2[归一化](@article_id:310343)**。这意味着我们缩放每个向量，使其长度为1。从几何上看，我们将所有词[向量投影](@article_id:307461)到一个高维球体（一个**超球面**）的表面上。这个简单的操作带来了深远的影响。在这个球面上，最大化[点积](@article_id:309438) $w^\top x$（它与向量之间夹角的余弦，即**[余弦相似度](@article_id:639253)**成正比）变得等同于最小化[欧氏距离](@article_id:304420)的平方 $\|w - x\|^2_2$。它们之间精确的关系非常简单：$\|w - x\|_2^2 = 2(1 - w^\top x)$。[@problem_id:3198364]

为什么要这样做？因为它移除了一个干扰性的自由度。在不进行[归一化](@article_id:310343)的情况下，模型可以通过简单地使向量 $w$ 和 $x$ 变得很长来增大[点积](@article_id:309438) $w^\top x$，而实际上并不需要让它们指向相同的方向。通过强制所有向量具有相同的长度，我们迫使模型只关注它们之间的**角度**。现在的学习纯粹是关于相对方向，而这正是真正语义精髓所在之处。[@problem_id:3198364]

### 我们看到的是幻象吗？随机性的零假设

所以，我们训练了模型，发现“cat”和“feline”之间的[余弦相似度](@article_id:639253)是0.85。这个数字算大吗？我们怎么知道我们看到的不仅仅是噪声中的模式？我们需要一个基准——一个零假设。完全随机选择的两个向量之间的[余弦相似度](@article_id:639253)会是多少？

让我们在高维超球面上随机选取两个点，并找出它们之间的夹角。有人可能会猜测答案是“任何值都有可能”。但在这里，高维空间揭示了其最令人惊讶和有用的特性之一。从[第一性原理](@article_id:382249)出发，仅使用对称性和[期望的线性性质](@article_id:337208)，就可以证明在一个 $d$ 维空间中，两个独立的随机[单位向量](@article_id:345230)之间期望的[余弦相似度](@article_id:639253)恰好为0。[@problem_id:3114469]

更重要的是，这种相似度的方差是 $1/d$。这意味着，随着维度 $d$ 变大，[余弦相似度](@article_id:639253)的分布会难以置信地集中在0附近。在一个300维的空间中，两个随机向量的[余弦相似度](@article_id:639253)为0.85是天文数字般不可能的。它们几乎总是接近**正交**（成90度角）。这所谓的“维度灾难”对我们来说反而成了一种福音。它保证了我们在学习到的[嵌入](@article_id:311541)中发现的任何强相似性或不相似性都是模型发现的真实信号，而不是随机偶然的结果。空间的广阔性确保了结构不是偶然产生的。

### [对比学习](@article_id:639980)：一个现代视角

虽然像 GloVe 这样基于计数的方法很强大，但现代主流方法是直接从神经网络中学习[嵌入](@article_id:311541)。主导的[范式](@article_id:329204)是**[对比学习](@article_id:639980)**。其思想非常直观：通过学习区分相似事物和不相似事物来学习好的表示。

对于一个给定的“锚点”数据点（比如一张猫的图片），我们通过[数据增强](@article_id:329733)（例如，裁剪或变色）来创建一个“正”样本。批次中的所有其他数据点都被视为“负”样本。模型，即一个编码器网络，被训练来产生[嵌入](@article_id:311541)，使得锚点的[嵌入](@article_id:311541)被拉近正样本的[嵌入](@article_id:311541)，同时被推离所有负样本的[嵌入](@article_id:311541)。

这听起来与计算共现次数非常不同。但在这里，一个优美的统一原则再次出现。最流行的对比损失函数 **InfoNCE**，可以被证明在代数上与分类中使用的标准 softmax [交叉熵损失](@article_id:301965)是等价的。其中的“诀窍”在于将问题构建为一个大规模的分类任务，其中*数据集中的每一个实例都是其自己独特的类别*。模型的任务是，对于一个给定的锚点，预测在成千上万个“类别键”（每个实例一个）中，哪一个是它的正匹配。[@problem_id:3173290] 这揭示了学习区分个体是一种极其强大的方式，可以学习定义它们的通用语义特征。

### 简单的危险：表示坍塌

模型要满足将正样本对拉近的目标，最简单的方法是什么？最懒惰——也最聪明——的解决方案是将每一个输入都映射到完全相同的点！如果所有的[嵌入](@article_id:311541)都相同，正样本对之间的距离为零，这堪称完美。模型轻而易举地解决了对齐任务，但这个表示却完全无用。这种失败模式被称为**表示坍塌**。

从统计上看，坍塌意味着一批[嵌入](@article_id:311541)的协方差矩阵失去了秩；一个或多个维度上的方差缩小到零。[@problem_id:3113838] 我们如何对抗这种情况？深度学习中最有效和无处不在的工具之一，**[批量归一化](@article_id:639282)（Batch Normalization, BN）**，前来救场。BN 对一批[嵌入](@article_id:311541)进行操作，对每个维度，它减去均值并除以[标准差](@article_id:314030)。这个看似简单的清理步骤是对抗坍塌的强大解药。通过强制每个维度的均值为0、方差为1，它明确地防止了任何维度方差的消失。如果模型试图使某个维度坍塌，BN 会简单地将其“拉伸”回来，放大任何残余信号，并迫使模型找到一个更有意义的解决方案。[@problem_id:3108508]

一个更复杂的观点将训练过程视为一种权衡。我们既想要好的**对齐性**（正样本对之间的距离小），又想要好的**均匀性**（[嵌入](@article_id:311541)应该[均匀分布](@article_id:325445)在超球面上，而不是聚集在一起）。坍塌是完美对齐但均匀性为零的状态。通过在训练期间同时监控这两个指标，我们可以实施一个更智能的[早停](@article_id:638204)规则：当看到对齐性持续改善但均匀性开始下降时，我们停止训练，这表明模型开始过度优化对齐任务，而牺牲了整体表示质量。[@problem_id:3119066]

从简单的上下文概念到高维球体的复杂几何，再到训练的微妙动态，[嵌入学习](@article_id:641946)是一段发现和利用数据中隐藏结构的旅程。

