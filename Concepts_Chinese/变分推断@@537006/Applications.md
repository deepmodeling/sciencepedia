## 应用与跨学科联系

“我无法创造的，我就不理解。” [Richard Feynman](@article_id:316284) 曾把这句话写在他的黑板上。但在现实世界中，我们希望理解的系统——大脑、市场、宇宙本身——远比我们能从[第一性原理](@article_id:382249)上“创造”其所有细节要复杂得多。我们对这些系统建立的模型，丰富而错综复杂，往往变得和它们所代表的现实一样难以理解。从模型到具体预测的路径可能是一片充满不可能积分的丛林。那么我们该怎么做？我们进行近似。但不是任何近似都可以。我们需要一种有原则的方法，一种技艺。[变分推断](@article_id:638571)就是这种技艺。它不仅仅是一种计算上的捷径；它是一场深刻的对话，关乎我们想知道什么和我们能实际计算什么。它是为我们的模型建立一个更简单、可解的模型的艺术，在本节中，我们将看到这门艺术如何成为跨越科学领域的通用发现语言。

### 核心工具集的实际应用：VI 在[经典统计学](@article_id:311101)中的表现

让我们从一个可以看得一清二楚的地方开始我们的旅程：线性回归的世界。假设我们试图找到一组输入和一个输出之间的关系，但我们认为我们的参数——定义这种关系的权重——本身是不确定的。[贝叶斯线性回归](@article_id:638582)为这些权重的[后验分布](@article_id:306029)提供了精确而优美的答案。这是检验[变分推断](@article_id:638571)作用的完美实验室，因为我们可以将其近似直接与真实情况进行比较 ([@problem_id:3161610])。

当我们应用最简单形式的 VI，即[平均场近似](@article_id:304551)时，我们做出了一个大胆的假设：我们对每个权重的不确定性与对所有其他权重的不确定性是独立的。这就像通过给出每个人的坐标来描述一个拥挤房间里人们的位置，却完全忽略了他们可能手拉着手或挤成一团的事实。结果会怎样？如果底层的“真实”不确定性确实是独立的（在我们的回归中，当输入特征是正交的时会发生这种情况），那么[平均场近似](@article_id:304551)根本不是近似——它是精确的！但如果特征是相关的——即一个变量能告诉你关于另一个变量的一些信息——那么真实的后验不确定性就是相互关联的。权重们“手拉着手”。[平均场近似](@article_id:304551)，就其本质而言，错过了这些相关性。它给了我们正确的平均位置，但对集体结构的描绘却是有缺陷的。这就是平均场 VI 的基本权衡：我们以忽略依赖性为代价，获得了巨大的计算简便性。理解这一交易是在应用 VI 中获得智慧的第一步。

### 解构不确定性：[现代机器学习](@article_id:641462)视角

“不确定性”这个概念可以被进一步精确化。在任何建模工作中，我们都面临两种不确定性。第一种是*[偶然不确定性](@article_id:314423)*（aleatoric uncertainty），源自拉丁语 *alea*，意为“骰子”。它是世界固有的随机性，是我们测量中不可减少的噪声。这是我们无法预测的骰子掷出的结果。第二种是*[认知不确定性](@article_id:310285)*（epistemic uncertainty），源自希腊语 *episteme*，意为“知识”。它是我们自身的无知，是我们模型参数的不确定性，因为我们只看到了有限的数据。这是我们希望通过学习更多来减少的不确定性。

[变分推断](@article_id:638571)提供了一个强大的镜头来区分这两者 ([@problem_id:3197125])。[偶然不确定性](@article_id:314423)通常是我们模型[似然函数](@article_id:302368)的一个固定参数，比如噪声方差 $\sigma^2$。VI 不会改变这一点。但[认知不确定性](@article_id:310285)被我们的变分后验分布 $q(\mathbf{w})$ 的扩展范围所捕捉。一个宽的 $q(\mathbf{w})$ 意味着我们对模型参数非常不确定。一个窄的 $q(\mathbf{w})$ 意味着我们很有信心。在这里，我们变分族的选择变得至关重要。如果当真实后验具有丰富的相关性时，我们使用一个限制性的[平均场近似](@article_id:304551)，我们的近似在某些方向上可能会比应有的更窄。它可能无法捕捉到我们无知的全部范围。这可能导致过度自信，这在科学和工程中是一个危险的罪过。因此，VI 不仅仅是近似一个[后验分布](@article_id:306029)；它迫使我们明确我们*如何*近似我们自身的不确定性。

### 超越高斯：建模结构与稀疏性

世界并不总是高斯分布的。我们常常需要建模更奇特的结构，比如稀疏性或离散序列。VI 的灵活性在这里大放异彩。

考虑*[稀疏表示](@article_id:370569)*的问题。你听到一个复杂的声音，一个音乐和弦。你能否仅用一个庞大音符库中的几个音符来描述它？这就是稀疏性的本质，是信号处理和[压缩感知](@article_id:376711)中的一个指导原则。贝叶斯方法使用“尖峰-厚板”（spike-and-slab）先验来对此进行建模：对于绝大多数未使用的音符，有一个位于零点的“尖峰”，而对于少数活跃的音符，则有一个“厚板”（一个连续分布）([@problem_id:2865249])。这种先验对于精确推断来说是出了名的困难。但通过 VI，我们可以构建一个分解的近似，试图弄清楚每个音符处于“尖峰”与“厚板”状态的概率。VI 将一个棘手的组合问题变成了一个平滑的优化问题。

或者想想一系列事件，其潜在原因被隐藏起来，就像仅通过观察某人是否带伞来推断天气模式（晴天、雨天）。这些由隐马尔可夫模型（HMMs）描述，这是语音识别和生物信息学的主力工具 ([@problem_id:765236])。在给定观测值的情况下推断最可能的[隐藏状态](@article_id:638657)序列是一个核心挑战。[变分推断](@article_id:638571)提供了一种方法，它假设在给定来自邻居的平均信息的情况下，时间 $t$ 的状态可以独立于其他时间的状态进行推断。同样，我们为可解性牺牲了真实的依赖关系，使我们能够解开否则会纠缠不清的复杂时间序列。

### 科学前沿：VI 作为通用推断引擎

有了这个工具包，我们现在可以 venturing 到科学发现的前沿，在那里 VI 不仅仅是机器学习的工具，而是科学本身的引擎。

在**神经科学和[基因组学](@article_id:298572)**中，研究人员使用[空间转录组学](@article_id:333797)来创建大脑组织切片上的基因表达图谱 ([@problem_id:2752945])。他们在数千个位置观察数千种不同基因分子的计数。基本问题是：每个位置不同细胞类型（[神经元](@article_id:324093)、胶质细胞等）的混合比例是多少？该模型是一幅美丽的织锦：基因计数服从[泊松分布](@article_id:308183)，其速率是已知的细胞类型“特征”和未知比例的混合，并带有一个空间先验，即相邻位置应具有相似的组成。其复杂性是惊人的。精确推断是不可能的。[变分推断](@article_id:638571)提供了一条切实可行的前进道路，让科学家能够[解卷积](@article_id:300181)数据，并生成令人惊叹的大脑细胞结构图。

在**进化生物学**中，我们试图重建过去。从不同地点采样的个体[基因序列](@article_id:370112)出发，结构化[溯祖模型](@article_id:380888)旨在推断其祖先在各大洲之间的迁徙历史 ([@problem_id:2753743])。我们推断的对象不仅仅是一组参数，而是在系统发育树上的整个*路径*集合。所有可能历史的空间是无限广阔的。通过假设一个在树的分支上分解的变分分布，VI 驯服了这个无限维问题。它让我们能够提出诸如“这个谱系的祖先在十万年前在非洲的概率是多少？”之类的问题。这是一台时间机器，由原则性近似驱动。

在**物理学和[材料科学](@article_id:312640)**中，我们模拟原子的舞蹈来预测材料属性 ([@problem_id:102380])。这些模拟速度极慢。一种现代方法是训练一个机器学习模型，通常是[图神经网络](@article_id:297304)，直接从量子力学计算中学习[原子间势](@article_id:356603)能函数。通过使这个模型贝叶斯化并使用 VI 进行训练，我们得到的不仅仅是一个能量预测。我们得到了模型*不确定性*的度量。这彻底改变了游戏规则。如果模型告诉我们它对某个特定的原子构型不确定，它就在告诉我们我们的知识在何处薄弱。然后我们可以将我们昂贵的[量子计算](@article_id:303150)导向那个特定的构型，这个过程称为[主动学习](@article_id:318217)。VI 使得模拟和机器学习之间形成了一个闭环，相互教学，加速了新药物和新材料的发现。

### 统一框架：深层联系

也许一个伟大的物理学原理最美妙的方面是其统一看似毫不相干现象的能力。[变分推断](@article_id:638571)，在其数学抽象中，实现了类似的统一。

考虑**[多任务学习](@article_id:638813)**，我们希望同时解决几个相关问题，例如，预测不同学校学生的表现 ([@problem_id:3155081])。[分层贝叶斯模型](@article_id:348718)允许学校通过共享先验来相互“借用统计强度”。这种共享在后验中引起了相关性：了解一所学校的情况会告诉你关于另一所学校的一些信息。然而，平均场变分近似，根据其定义，打破了这些后验依赖关系。这揭示了一个深刻的真理：你的变分近似的结构决定了你的模型中信息流的结构。问题中介绍的“后验耦合指数”是一种绝妙的方式，可以量化从丰富的[分层模型](@article_id:338645)到简单的分解近似的转换中究竟损失了什么。

这一主题在**[主题建模](@article_id:639001)**中得以延续，我们使用像[潜在狄利克雷分配](@article_id:639566)（LDA）这样的模型来发现大量文本文档中的抽象主题 ([@problem_-id:3098020])。VI 是 LDA 的标准推断引擎。在这里，我们也必须小心。ELBO，即我们在 VI 中的优化目标，只是真实[模型证据](@article_id:641149)的一个*下界*。将其作为经典[模型选择标准](@article_id:307870)（如 AIC）的插件可能会产生误导，因为当我们改变模型复杂性（如主题数量）时，下界与真实值之间的差距可能会改变。这是一个至关重要的提醒：我们的变分世界是真实后验世界的一个影子，而影子有时会扭曲现实。

最后也是最令人叹为观止的联系是与**强化学习（RL）**的联系，即做出最优决策的科学。一个智能体——一个学习走路的机器人或一个学习玩游戏的程序——试图找到一个策略，一种选择行动的策略，以最大化其未来的总回报。一个近期的、深刻的见解将整个过程重构为[变分推断](@article_id:638571) ([@problem_id:3157986])。在这种观点下，我们定义了一个关于整个轨迹的“目标”分布，其中“好”的轨迹（那些有高回报的）被赋予高概率。然后，智能体的策略被视为一个试图匹配这个[目标分布](@article_id:638818)的变分分布。在 RL 中最大化熵[正则化](@article_id:300216)回报在数学上等同于最大化 ELBO！这个“控制即推断”框架统一了决策制定和[贝叶斯推断](@article_id:307374)的领域。它表明，学会在世界中智能地行动是推断通往理想未来路径的一种形式。

### 结论

我们的巡览结束了。我们看到的[变分推断](@article_id:638571)不是一个枯燥的[算法](@article_id:331821)，而是一个动态且多才多艺的原则。我们在[线性回归](@article_id:302758)的简单背景下看到了它的核心权衡。我们看到它将不确定性剖析为“我们无法知道的”和“我们还不知道的”。我们看到它适应于建模[稀疏性](@article_id:297245)、序列、计数，甚至是我们的祖先穿越[时空](@article_id:370647)的路径。最后，我们看到它在学习、决策和发现之间建立了令人惊讶和美丽的联系。

[变分推断](@article_id:638571)是物理学家寻找一个足够好的可解模型的诀窍，并将其提升为统计推理的一般原则。它是一种语言，让我们能够谈论复杂的系统，量化我们的无知，并在现代科学广阔、棘手的领域中导航。它没有给我们“真理”——那个精确后验的柏拉图式理想——但它给了我们一些可以说更有价值的东西：一种可行的、有原则的持续学习方式。而最终，这正是科学的全部意义所在。