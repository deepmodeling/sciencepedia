## 引言
在[药物发现](@entry_id:261243)这个高风险领域，预测模型是前景广阔的强大工具，有望加速新药的寻找过程。然而，它们的真正价值取决于一个关键问题：我们能相信它们的预测吗？一个在已知数据上表现出色，但在面对新分子时却失效的模型不仅毫无用处，甚至可能产生误导。这在一个以创新为基础的领域引发了一个根本性问题：我们如何正确衡量模型对未知事物的泛化能力？最常见的验证技术——简单的数据随机拆分，往往会带来一种危险的虚假信心。

本文剖析了化学领域中[模型验证](@entry_id:141140)这一关键问题，揭示了随机拆分背后公平的假象，并引入了一种更严谨、科学上更诚实的替代方法。以下各节将引导您了解其核心概念。在“原理与机制”中，我们将探讨化学家将分子视为结构家族的观点，并阐明为何这一视角使得基于骨架的数据拆分至关重要。然后，我们会将这一实用策略与机器学习的理论基础联系起来，展示它如何模拟真实的发现挑战。在“应用与跨学科联系”中，我们将拓宽视野，揭示防止[信息泄露](@entry_id:155485)这一原则如何成为可信赖人工智能的基石，不仅在药物优化领域，在各种复杂的科学问题中亦是如此。

## 原理与机制

要真正理解一个预测模型为什么有效——或者更重要的，为什么它可能会失败——我们必须超越其算法的表象，深入探究其学习数据的本质。在药物发现中，这意味着要像化学家一样，而不仅仅是像计算机一样去理解分子的结构。

### 化学家的乐高积木：骨架与系列

想象一下，你拿到一大箱乐高作品，并被要求预测它们的属性，比如它们从斜坡上滚下的速度。乍一看，这是一堆令人眼花缭乱的颜色和形状。但很快，你会注意到一个模式。许多作品都构建在一个共同的底盘上。可能有一系列赛车，都使用相同的扁平底座，但装饰着不同颜色的积木和扰流板。另一个系列可能是卡车，构建在一个更厚实、更坚固的框架上。

分子也是如此。它们不仅仅是原子的随机组合。药物化学家早就认识到，许多药物可以被分解为一个核心结构骨架（即**骨架**），以及一组外围的装饰物（即侧链）。这就是 **Bemis-Murcko 骨架**框架背后优美而简单的思想 [@problem_id:4563973]。骨架由分子的环系和连接它们的原子连接体组成——它就是底盘。侧链则是附着在这个核心上的装饰物。

这个视角极其强大，因为它将浩瀚的化学世界组织成结构相关的分子家族，通常称为**同类物系列 (congeneric series)**。一个系列中的分子共享一个共同的骨架，因此通常也共享一个共同的生物作用机制，而侧链则调节它们的效力、选择性或代谢特性。理解这种家族结构是为任何预测模型设计公平测试的关键。

### 公平的假象：为什么随机拆分会失败

假设我们建立了一个机器学习模型来预测分子的生物活性。为了测试它的效果，我们需要预留一部分数据作为测试集。最显而易见的方法是**随机拆分**，这种方法直观上感觉很公平。我们将整个分子集合像洗牌一样打乱，然后将一部分分到训练堆，其余的分到测试堆。每个分子都有同等的机会被分到任一集合中。

不幸的是，这种表面的公平只是一种幻觉，而且是一种危险的幻觉。

因为我们的数据集包含了构建在共同骨架上的分子家族，随机打乱不可避免地会将“亲戚”分子分散到训练集和[测试集](@entry_id:637546)中。我们可能用一个带有苯环骨架和甲基侧链的分子来训练模型，然后用另一个带有完全相同的苯环骨架但侧链略有不同的乙基侧链的分子来测试它。模型要取得成功并不需要学习深刻的生物学原理；它只需要识别出“这个骨架通常是活性的”，这种现象被称为**类似物偏见 (analogue bias)** 或 **同类物系列泄露 (congeneric series leakage)** [@problem_id:4602692]。

让我们通过一个思想实验来具体说明。想象一个模型，它在处理真正新型的分子时完全是个“傻瓜”，表现不比随机抛硬币好。对于这些分子，它将活性分子排在非活性分子之前的正确概率是 $0.5$。然而，对于那些与它在训练中见过的分子非常相似的类似物，假设它是个“天才”，能以 $1.0$ 的概率完美地对它们进行排序。

现在，假设我们使用随机拆分，它创建了一个[测试集](@entry_id:637546)，其中30%的分子是训练分子的近亲类似物。模型的整体性能，通过一个称为曲线下面积（AUC）的常用指标来衡量，将是它在这两组分子上表现的加权平均值：

$ \mathrm{AUC} = (0.30 \times \text{Performance on Analogues}) + (0.70 \times \text{Performance on New Stuff}) $
$ \mathrm{AUC} = (0.30 \times 1.0) + (0.70 \times 0.5) = 0.30 + 0.35 = 0.65 $

$0.65$ 的分数表明模型具有一定的预测能力。但这是一种假象。当在现实世界中面对一个真正新颖的化学家族时，它的性能将崩溃到 $0.5$ 的基线水平。随机拆分没有测试泛化能力，而是奖励了记忆能力，给了我们一种危险的、被夸大的信心 [@problem_id:3869887]。

### 更严格的考试：骨架拆分的逻辑

如果我们想测试一个学生对物理学的真正理解，我们不会给他们一份与家庭作业题目完全相同、只是数字改变了的考卷。我们会给他们需要应用相同基本原理的新问题。

同样的逻辑也适用于我们的模型。诚实评估模型对新化学家族泛化能力的唯一方法是设计一个只包含新化学家族的“考卷”。这就是**骨架拆分**的核心原则。

这个过程简单但意义深远。首先，我们为数据集中的每个分子确定其 Bemis-Murcko 骨架。然后，我们将所有共享相同骨架的分子归为一个“家族”。最后，当我们创建[训练集](@entry_id:636396)和[测试集](@entry_id:637546)时，我们将整个家族分配到其中一个集合中。出现在训练集中的骨架禁止出现在[测试集](@entry_id:637546)中，反之亦然 [@problem_id:4563973] [@problem_id:4375829]。

这个简单的规则改变了一切。模型不能再仅仅通过识别熟悉的骨架来取得成功。为了在测试集上表现良好，它被迫学习连接结构特征与生物活性的更深层次、更具迁移性的规则——也就是[药物化学](@entry_id:178806)家努力揭示的“[结构-活性关系](@entry_id:178339)”。这是一场更难，但也更诚实的考试。

### 物理学家的视角：将泛化视为弥合鸿沟

骨架拆分这一实用策略，完美地体现了理论机器学习中的一个深刻思想：**分布外 (Out-of-Distribution, OOD) 泛化**。我们已经合成和测试过的分子的“分布”，与我们明天可能发现的分子的“分布”是不同的。这之间存在**[协变量偏移](@entry_id:636196) (covariate shift)**——已知世界与未知世界之间的鸿沟。预测模型的最终目标就是弥合这个鸿沟。

因此，我们的验证策略应该旨在模拟这个现实世界中的鸿沟。我们甚至可以量化它。通过使用[分子指纹](@entry_id:172531)（一种分子的条形码），我们可以计算一个衡量结构重叠度的**Tanimoto 相似性**分数。让我们想象一下，我们当前数据与未来真正新颖的分子之间的平均相似性非常低，比如说 $0.06$。

现在考虑我们的拆分策略 [@problem_id:5173710]：
- **随机拆分**创建的[测试集](@entry_id:637546)与[训练集](@entry_id:636396)非常相似，其平均 Tanimoto 相似性可能为 $0.42$。这是一个微小的鸿沟，是对真实挑战的拙劣模拟。
- **骨架拆分**创建了一个包含全新骨架的测试集，导致平均相似性大大降低，可能为 $0.08$。

显然，通过创建一个 $1 - 0.08 = 0.92$ 的测试“鸿沟”，骨架拆分在模拟真实世界部署中 $1 - 0.06 = 0.94$ 的“鸿沟”方面，比随机拆分做得好得多。因此，在骨架拆分的测试集上计算出的风险，是对模型未来表现的一个偏差更小、更现实的估计。化学家的实践智慧与计算机科学家的形式理论在此殊途同归。

### 终极测试：模拟时间的流逝

骨架拆分是严谨验证的最终定论吗？不完全是。现实世界甚至更为复杂。想象一下我们测量的生物活性 $Y_t$，它不仅取决于分子的真实属性 $f^\star(X_t)$，还取决于其骨架家族特有的偏倚 $b_{s(t)}$，以及我们实验装置随时间产生的系统性漂移 $d(t)$。我们测量值的一个简单模型可能如下所示：

$ Y_t = f^\star(X_t) + b_{s(t)} + d(t) + \varepsilon_t $

其中 $\varepsilon_t$ 是随机噪声 [@problem_id:3860381]。

骨架拆分通过迫使模型跨骨架进行泛化，出色地解决了骨架特异性偏倚 $b_{s(t)}$ 的问题。但因为它通常会打乱所有时间点的数据，所以它无法捕捉到时间 $d(t)$ 无情前进的脚步。[训练集](@entry_id:636396)和测试集都会有相似的平均时间漂移，因此模型从未被测试其在外推到实验基线已发生变化的未来的能力。

对真实世界部署最忠实的模拟通常是**时间拆分**（或称时序拆分）。在这种方法中，我们将在某个特定日期之前收集的所有数据用于训练，并将该日期*之后*收集的所有数据用于测试。这种方法自然地捕捉了随时间变化的所有来源：新骨架的引入、研究重点的转移以及实验分析中的系统性漂移。它通常被认为是评估前瞻性性能的黄金标准，因为它提出了最直接、最诚实的问题：“基于我们昨天所知的一切，我们能多好地预测明天？”

### 量化飞跃：衡量新颖性

这引出了最后一个实际问题。一个新的测试集到底有多“新”？我们可以而且应该衡量这一点。一个直观的测试分子**新颖性分数**是测量它与训练集中最亲近“亲戚”的相异度。我们可以为[测试集](@entry_id:637546)中的每个分子计算这个分数，并检查分数的分布，以量化我们的模型被要求完成的“飞跃”有多大 [@problem_id:3835232]。

我们可以更直接一些。既然骨架是新颖性的关键元素，为什么不直接衡量骨架本身的新颖性呢？通过直接在骨架上计算指纹，我们可以计算出[测试集](@entry_id:637546)中的核心框架与[训练集](@entry_id:636396)中的核心框架有多大不同 [@problem_id:3835232]。这为我们的验证策略在模拟发现真正新药的挑战方面的表现提供了一份直接、量化的成绩单。

归根结底，从化学家的乐高积木到物理学家对[分布偏移](@entry_id:638064)的看法，其原理是相通的：最有价值的测试不是最简单的，而是最现实的。通过迎接结构新颖性和时间流逝带来的挑战，我们可以构建出不仅在回顾时准确，而且对未来真正有用的模型。

