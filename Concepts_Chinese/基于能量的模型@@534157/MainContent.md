## 引言
在[统计物理学](@article_id:303380)和机器学习的交汇处，存在着一个用于理解数据的强大而优雅的框架：[基于能量的模型](@article_id:640714)（Energy-Based Model, EBM）。EBM并不试图直接构建一个复杂的、[归一化](@article_id:310343)的[概率分布](@article_id:306824)，而是采用一个更简单的思想：为每一种可能的数据构型关联一个标量的“能量”。合理的、结构良好的数据被赋予低能量，而混乱或无意义的数据则被赋予高能量。这种方法提供了巨大的灵活性，但也引入了一个核心挑战，即一个难以处理的归一化常数，这在历史上使其训练和应用变得复杂。本文对这一引人入胜的模型类别进行了全面概述。第一章 **“原理与机制”** 将解析其基础理论，从[玻尔兹曼分布](@article_id:303203)和棘手的配分函数，到像对比散度这样巧妙的训练解决方案。在这一理论基础之后，**“应用与跨学科联系”** 章节将揭示EBM视角的深远影响，展示它如何统一现代AI的核心概念，并催生科学发现中的前沿应用。

## 原理与机制

### 能量视角下的世界

[基于能量的模型](@article_id:640714)（EBM）的核心思想深刻地简洁而优雅，它直接借鉴自物理学的核心思想。其原理是：我们可以将系统的任何构型——无论是晶体中原子的[排列](@article_id:296886)、图像中的像素，还是句子中的单词——与一个称为**能量**的标量值关联起来。那些合理的、结构化的或“可能”的构型被赋予低能量。那些无意义的、混乱的或“不可能”的构型被赋予高能量。就像球会在山谷中找到最低点一样，EBM在其[能量景观](@article_id:308140)的“谷底”找到最可能的数据。

这种关系通过优美的**玻尔兹曼分布**来形式化：

$$
p(x) \propto \exp\left(-\frac{E(x)}{T}\right)
$$

其中，$p(x)$ 是构型 $x$ 的概率，$E(x)$ 是其能量，$T$ 是一个“温度”参数，控制系统在多大程度上可以探索更高能量的状态。在机器学习中，我们通常将温度吸收到能量函数中或将其设置为1，从而将表达式简化为：

$$
p_{\theta}(x) \propto \exp(-E_{\theta}(x))
$$

在这里，能量函数 $E_{\theta}(x)$ 通常是一个带有参数 $\theta$ 的神经网络。这种方法的美妙之处在于其自由度。要构建一个[生成模型](@article_id:356498)，我们一开始不需要担心复杂的约束或架构选择；我们只需要设计一个能量函数——一台告诉我们任何给定数据点 $x$ 有多“昂贵”的机器。低成本意味着高概率，高成本意味着低概率。

### 灵活性的代价：难以处理的配分函数

自然满足于比例关系，但对于一个合格的[概率分布](@article_id:306824)，我们需要概率之和为一。为了实现这一点，我们必须对分布进行归一化。这使我们直面EBM发展史中的核心难题：**[配分函数](@article_id:371907)**，记为 $Z(\theta)$。

$$
p_{\theta}(x) = \frac{\exp(-E_{\theta}(x))}{Z(\theta)}
$$

其中

$$
Z(\theta) = \sum_{x'} \exp(-E_{\theta}(x')) \quad \text{或} \quad Z(\theta) = \int_{x'} \exp(-E_{\theta}(x')) dx'
$$

这个[归一化常数](@article_id:323851) $Z(\theta)$ 是[玻尔兹曼因子](@article_id:301496) $\exp(-E_{\theta}(x'))$ 在系统可能取到的*每一个可能构型*上的总和（或积分）。

对于一个非常简单的系统，我们可以手动计算它。想象一个微小的材料模型，只有三个原子位点排成一线，由两个'A'原子和一个'B'原子占据。只有三种可能的[排列](@article_id:296886)方式：B-A-A、A-B-A和A-A-B。我们可以根据原子相互作用计算这三种状态各自的能量（$E_1, E_2, E_3$）。配分函数就是它们玻尔兹曼因子的简单加总 [@problem_id:65955]：

$$
Z = \exp(-E_1/k_B T) + \exp(-E_2/k_B T) + \exp(-E_3/k_B T)
$$

这微不足道。但现在，考虑一张小的、28x28像素的灰度图像，就像手写数字的MNIST数据集中的那些一样。784个像素中的每一个都可以取256个值之一。可能的图像总数是 $256^{784}$，这个数字如此之大，以至于可观测宇宙中的原子数量与之相比也显得微不足道。要计算 $Z(\theta)$，我们需要为这些图像中的每一个计算能量，并将它们的玻尔兹曼因子相加。这不仅仅是困难，而是计算上不可能实现的。

配分函数的这种难处理性是EBM的巨大挑战。我们可以轻易计算两个点 $x_1$ 和 $x_2$ 的概率*比率* $p(x_1)/p(x_2)$，因为 $Z(\theta)$ 项会消掉。但计算绝对概率 $p(x_1)$ 却是遥不可及的。这对我们如何训练这些模型产生了深远的影响。虽然直接计算是不可能的，但存在一些先进的统计方法，如**退火[重要性采样](@article_id:306126)（AIS）**和确定性近似，如**[拉普拉斯近似](@article_id:641152)**，可以在特定条件下估计 $Z(\theta)$，但它们计算成本高昂，通常只用于模型评估而非训练 [@problem_id:3166256]。

### 学习：宇宙间的推与拉

如果我们甚至无法计算一个数据点的概率，我们到底该如何训练模型呢？答案在于观察似然函数的*梯度*。当我们想要增加观测到的数据点 $x_{\text{data}}$ 的概率时，我们需要调整参数 $\theta$。最大化[对数似然](@article_id:337478)的数学推导揭示了[损失函数](@article_id:638865) $\mathcal{L}(\theta) = -\log p_{\theta}(x_{\text{data}})$ 梯度的一个优美而直观的结构：

$$
\nabla_{\theta} \mathcal{L}(\theta) = \nabla_{\theta} E_{\theta}(x_{\text{data}}) - \mathbb{E}_{x \sim p_{\theta}}[\nabla_{\theta} E_{\theta}(x)]
$$

让我们来解析一下。这个方程告诉我们，梯度是两种力量之间的一场竞争 [@problem_id:3153991]：

1.  **正相**：第一项 $\nabla_{\theta} E_{\theta}(x_{\text{data}})$ 将参数 $\theta$ 向着*降低*观测数据点能量的方向推动。这就像找到一张真实的猫的照片，然后告诉模型：“就是这个！让长得像这样的东西能量更低。”

2.  **负相**：第二项 $\mathbb{E}_{x \sim p_{\theta}}[\nabla_{\theta} E_{\theta}(x)]$ 是一个[期望](@article_id:311378)。它涉及*从模型自身的当前分布* $p_{\theta}(x)$ 中生成样本，并将其能量*推高*。这一项表示：“无论你当前认为世界是什么样子，都让它的能量更高些。”

这是一个**对比**学习过程。模型通过对比它看到的真实数据和它自己生成的“幻想”数据来进行学习。它压低真实事物的能量，同时推高自己创造物的能量。这可以防止能量景观坍缩到单个点，并迫使模型正确地分配其概率质量。

但请仔细看负相。它要求对 $p_{\theta}(x)$ 求[期望](@article_id:311378)。为此，我们需要从 $p_{\theta}(x)$ 中抽取样本。但我们无法直接从中采样，正是因为我们不知道那个难以处理的 $Z(\theta)$！这个难题再次出现，这次它破坏了我们的训练[算法](@article_id:331821)。

### 近似之术：对比散度

在很长一段时间里，这个障碍使得训练EBM看起来不切实际。由Geoffrey Hinton提出的解决方案是一个巧妙而实用的技巧，称为**对比散度（Contrastive Divergence, CD）**。其思想是使用马尔可夫链蒙特卡洛（MCMC）方法（如[吉布斯采样](@article_id:299600)）来获取负相所需的样本。一个长期运行的MCMC链，如果运行足够多步，保证能从真实模型分布 $p_{\theta}(x)$ 中产生样本。但“足够多步”可能长得令人望而却步。

CD的技巧在于根本不让马尔可夫链长时间运行。实际上，对于CD-$k$，我们只运行它 $k$ 步（通常，$k=1$）。而且至关重要的是，我们不是从一个随机点开始链，而是从一个*真实数据点*来初始化它 [@problem_id:3121406]。

仅经过 $k$ 步后得到的样本肯定不是来自真实模型分布 $p_{\theta}(x)$。它来自一个偏向于原始数据点的分布。这意味着我们使用CD计算的梯度是真实[对数似然](@article_id:337478)梯度的**有偏估计量**。我们并不是完美地笔直攀登[最大似然](@article_id:306568)的山峰；我们正在偏离轨道。

然而，经验表明，这种有偏的、“快速而粗略”的梯度通常已经足够好。从真实数据点出发的幻想粒子仍然处于空间中的一个合理区域，提供了有用的对比。随着 $k$ 的增加，偏差减小，在极限 $k \to \infty$ 时，偏差完全消失 [@problem_id:3121406]。CD体现了机器学习中一个强有力的教训：有时，一个快速、廉价且有偏的近似远比一个理论上完美但计算上爆炸的方法更有用。

### 寻求可处理性：结构决定一切

目前为止的故事可能将EBM描绘成一个永远在与难以处理的总和与有偏的近似作斗争的领域。但事实并非如此。通过巧妙地选择我们建模的对象，或通过对我们的能量函数施加结构，我们可以使这种难处理性完全消失。

#### [条件概率](@article_id:311430)的解决之道

如果我们的目标不是对数据的完整分布 $p(x)$ 进行建模，而是执行分类——即，为给定输入 $x$ 建模标签 $y$ 的[条件概率](@article_id:311430) $p(y|x)$，情况会怎样？我们可以定义一个关于对 $(x,y)$ 的联合[基于能量的模型](@article_id:640714)，$p(x,y) \propto \exp(-E(x,y))$。根据定义，[条件概率](@article_id:311430)为 $p(y|x) = p(x,y)/p(x)$。当我们用基于能量的公式写出这个表达式时，奇妙的事情发生了：

$$
p_{\theta}(y \mid x) = \frac{\frac{\exp(-E_{\theta}(x,y))}{Z_{\theta}}}{\sum_{y'=1}^{K} \frac{\exp(-E_{\theta}(x,y'))}{Z_{\theta}}} = \frac{\exp(-E_{\theta}(x,y))}{\sum_{y'=1}^{K} \exp(-E_{\theta}(x,y'))}
$$

那个全局的、难以处理的[配分函数](@article_id:371907) $Z_{\theta}$ 同时出现在分子和分母中，它**被消去了**！[@problem_id:3134113]。我们剩下的是一个新的、局部的[配分函数](@article_id:371907)，它只是对 $K$ 个可[能标](@article_id:375070)签求和。如果 $K$ 很小（例如，图像识别任务中的10个类别），这个求和是完全可处理的。这个表达式无非就是将熟悉的**softmax函数**应用于每个类别的负能量。

这揭示了一个深刻的统一性：许多标准分类模型都隐含地是条件EBM。通过最大化条件[对数似然](@article_id:337478)来训练它们是直接的，因为[配分函数](@article_id:371907)的问题已经被回避了 [@problem_id:3110716]。这个框架还揭示了一个微妙的[不变性](@article_id:300612)：如果我们通过添加任何只依赖于输入的项 $h(x)$ 来修改我们的能量函数，它对最终的分类概率没有影响。这是因为 $h(x)$ 为每个类别的能量增加了相同的常数，而这个共同的偏移量被softmax归一化抵消了 [@problem_id:3134113]。

#### 架构上的解决方案：[受限玻尔兹曼机](@article_id:640921)

实现可处理性的另一种方法是对能量函数施加结构性约束。一个经典的例子是**[受限玻尔兹曼机](@article_id:640921)（Restricted Boltzmann Machine, RBM）**。RBM有一层“可见”单元（代表数据）和一层“隐藏”单元（代表潜在特征）。关键的限制是其底层图是**二分的**：连接只存在于可见层和隐藏层*之间*，而*不在*层内部 [@problem_id:3170414]。

这个简单的架构规则带来了一个强大的后果。因为没有隐藏-隐藏连接，给定可见层，所有隐藏单元彼此之间都变得条件独立。对称地，给定隐藏层，所有可见单元都是条件独立的。这意味着我们可以在MCMC过程中执行**块[吉布斯采样](@article_id:299600)**，而不是一次采样一个单元：给定可见单元，同时采样所有 $H$ 个隐藏单元，然后给定隐藏单元，同时采样所有 $D$ 个可见单元。这极大地提高了效率，并导致MCMC链的混合速度更快，使得对比散度中的近似更有效，训练更稳定。RBM是一个美丽的证明，展示了智能的架构设计如何能将一个计算上困难的问题转化为一个可管理的问题。

### 从物理学到神经网络：[激活函数](@article_id:302225)的起源

让我们放大观察EBM中最简单的可能组件：一个可以处于 $+1$ 或 $-1$ 两种状态之一的二元单元 $g$。假设它的能量由一个输入场 $a(x)$ 决定，使得 $E(g|x) = -g a(x)$。这是物理学中著名的**伊辛模型**的最简单形式。这个单元的平均值或[期望值](@article_id:313620) $\mathbb{E}[g|x]$ 是多少？让我们从[第一性原理计算](@article_id:377535)它。

配分函数是两种状态的和：
$$
Z(x) = \exp(-E(+1|x)) + \exp(-E(-1|x)) = \exp(a(x)) + \exp(-a(x))
$$
概率是：
$$
p(g=+1|x) = \frac{\exp(a(x))}{Z(x)}, \quad p(g=-1|x) = \frac{\exp(-a(x))}{Z(x)}
$$
[期望](@article_id:311378)是加权和：
$$
\mathbb{E}[g|x] = (+1) \cdot p(g=+1|x) + (-1) \cdot p(g=-1|x) = \frac{\exp(a(x)) - \exp(-a(x))}{\exp(a(x)) + \exp(-a(x))}
$$
这个表达式是**[双曲正切函数](@article_id:638603)**（$\tanh$）的定义！

$$
\mathbb{E}[g|x] = \tanh(a(x))
$$

如果我们选择的二元单元是 $\{0, 1\}$，类似的推导会表明其[期望](@article_id:311378)是 logistic **sigmoid函数** $\sigma(2a(x))$ [@problem_id:3174558]。这是一个非凡的结果。[深度学习](@article_id:302462)中普遍使用的、通常被当作随意选择的[激活函数](@article_id:302225)，竟然从最简单的二元开关的基本[统计力](@article_id:373880)学中自然地涌现出来。“温度”参数 $\beta$（我们设为1）控制着tanh曲线的陡峭程度。在高温下（$\beta \to 0$），曲线变平为零，代表纯粹的随机性。在零温下（$\beta \to \infty$），曲线锐化成一个确定性的阶跃函数。这为我们[神经网络](@article_id:305336)构建块的行为提供了深刻的物理直觉。

### 全局视角：关于梯度的警示

最后，值得注意的是，能量函数形式的选择不仅仅是方便与否的问题；它对学习有着深远的实际影响。考虑一个会饱和的能量函数，比如 $E(x) = a \tanh(b \|x\|^2)$。对于范数 $\|x\|$ 非常大的数据点 $x$，能量函数变得平坦。

后果是什么？能量的梯度 $\nabla_x E(x)$ 在这些区域变得极小 [@problem_id:3194491]。这意味着，如果在我们的MCMC模拟中，一个“幻想”粒子游荡到远离数据的高能量、低概率区域，模型几乎不会产生梯度信号来推高其能量。学习信号恰恰在我们最需要它来塑造[能量景观](@article_id:308140)的地方变得异常微弱。这个“[梯度消失](@article_id:642027)”问题会使训练变得缓慢而困难，这表明设计一个行为良好的能量景观是一门微妙的艺术，需要在[表达能力](@article_id:310282)和可训练性之间取得平衡。探索[基于能量的模型](@article_id:640714)的旅程不仅仅是关于一个单一的方程，而是关于在一个充满计算、近似和设计的丰富而复杂的景观中航行。

