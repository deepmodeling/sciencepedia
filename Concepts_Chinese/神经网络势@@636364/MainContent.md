## 引言
预测原子的运动和相互作用是现代化学与[材料科学](@entry_id:152226)的基石。这场原子之舞受一个复杂的高维景观所支配，这个景观被称为[势能面](@entry_id:147441)（PES），其真实形状由量子力学定律决定。几十年来，一个根本性的权衡一直困扰着计算科学家：虽然像[密度泛函理论](@entry_id:139027)（DFT）这样的量子方法可以高保真地描绘这个景观，但对于大体系或长时间尺度来说，它们的计算成本过于高昂。这一差距限制了我们以所需的精度模拟[蛋白质折叠](@entry_id:136349)或[晶体生长](@entry_id:136770)等复杂现象的能力。

本文介绍[神经网](@entry_id:276355)络势（NNP），这是一种革命性的方法，它通过将机器学习的预测能力与物理学的严谨性相结合来弥合这一差距。通过从一组精心挑选的高精度[量子计算](@entry_id:142712)结果中学习，NNP 能够创建真实[势能面](@entry_id:147441)的快速可靠的代理模型。本文探讨了使这些模型成为科学领域变革性工具的核心概念。

首先，在“原理与机制”部分，我们将深入探讨 NNP 是如何构建的。我们将看到对称性和局域性等基本物理定律如何被融入其架构之中，以确保它们产生具有物理意义的结果。接下来，“应用与跨学科联系”部分将展示 NNP 惊人的应用范围。我们将从它们对材料设计和化学的影响，到它们通过[主动学习](@entry_id:157812)实现智能模拟的作用，甚至它们在支配[原子核](@entry_id:167902)的基本力方面的应用，进行一次探索之旅。

## 原理与机制

要理解[神经网](@entry_id:276355)络势（NNP）的魔力，我们必须首先回到化学和[材料科学](@entry_id:152226)一切展开的基础舞台：原子的世界。想象一下，原子不是简单的台球，而是在一个广阔、复杂且无形的景观上舞动的舞者。这个景观就是**[势能面](@entry_id:147441)（PES）**，一个优雅得令人惊叹且意义深远的概念。对于任何给定的[原子核](@entry_id:167902)排布，PES 告诉我们系统的[势能](@entry_id:748988)。驱动原子之舞的力量——将原子拉到一起形成[化学键](@entry_id:138216)、将它们推开、将它们弯曲成分子——无非是这个景观的斜坡和山谷。一个原子，就像在表面上滚动的弹珠，总是会被推向能量更低的山下。作用在它上面的力就是[势能](@entry_id:748988)的负梯度 [@problem_id:3422849]。

这个景观从何而来？它是由量子力学定律描绘的。**Born-Oppenheimer 近似**为我们提供了一个优美的简化：由于[原子核](@entry_id:167902)比电子[重数](@entry_id:136466)千倍，它们的运动非常缓慢，而电子则在它们周围飞速运动，几乎瞬间就能适应任何新的[原子核](@entry_id:167902)排布。对于任何固定的[原子核](@entry_id:167902)位置 $\mathbf{R}$，我们可以求解电子的薛定谔方程，以找到它们的最低可能能量，即它们的[基态](@entry_id:150928)。这个[基态能量](@entry_id:263704)，加上带正电的[原子核](@entry_id:167902)之间的经典排斥力，*就是* [势能面](@entry_id:147441)在该点的值，即 $V(\mathbf{R})$ [@problem_id:2784636]。这就是“真实”的景观，是支配[分子结构](@entry_id:140109)、[化学反应](@entry_id:146973)和材料性质的客观现实。

计算科学的巨大挑战一直是描绘这个景观。虽然像[密度泛函理论](@entry_id:139027)（DFT）这样的量子力学方法可以以极高的精度计算[势能面](@entry_id:147441)上的一个点，但它们的速度慢得令人难以忍受。对几百个原子进行单点计算可能需要数小时或数天。模拟[蛋白质折叠](@entry_id:136349)或晶体生长，这些过程涉及数百万个原子和数十亿个微小的时间步长，根本是遥不可及的。正是在这里，[神经网](@entry_id:276355)络势的精妙之处登场了。这个想法简单而强大：如果我们无法承担在模拟过程中即时计算整个景观的成本，那么就让我们学习一张它的地图。我们使用缓慢但精确的量子方法来绘制一些关键位置——几千个有[代表性](@entry_id:204613)的原子构型——然后训练一个高度灵活的函数，即[神经网](@entry_id:276355)络，在这些点之间进行插值，从而创建一个代理模型 $V_{\text{NNP}}(\mathbf{R})$，来模仿真实的[势能面](@entry_id:147441) $V_{\text{BO}}(\mathbf{R})$。

### 构建具备物理意识的网络

[神经网](@entry_id:276355)络是一个强大的[函数逼近](@entry_id:141329)器，但一个“天真”的[神经网](@entry_id:276355)络对物理学一无所知。为了构建一个具有物理意义的势，我们必须将自然界的基本定律融入其架构之中。

#### 对称性定律

首先，一个势必须遵守空间的基本对称性。一个水分子的能量不应该因为我们简单地将它从实验室的一边移到另一边（**[平移不变性](@entry_id:195885)**）或旋转它（**[旋转不变性](@entry_id:137644)**）而改变。此外，水分子中的两个氢原子是相同的。如果我们交换它们的标签，能量必须保持不变（**[置换不变性](@entry_id:753356)**）。一个未能通过此测试的模型是根本性错误的；它会不符合物理地将两个相同的原子区分开来，导致荒谬的结果，比如对完全相同的物理构型预测出不同的能量 [@problem_id:2908410]。

#### 局域性的智慧

第二个关键的见解是**局域性**。作用在一个原子上的力，以及其能量贡献，主要由其近邻原子决定。你茶杯里一个水分子中的原子，并不会在意月球上另一个分子中的原子。这使我们能够做出一个深刻的简化：我们可以将系统的总[能量表示](@entry_id:202173)为各个原子能量贡献的总和 [@problem_id:3422822]：
$$
E_{\text{total}} = \sum_i E_i
$$
在这里，$E_i$ 是分配给原子 $i$ 的能量，它仅取决于其周围一个小的、有限邻域内的原子排布，这个邻域通常由一个**[截断半径](@entry_id:136708)** $r_c$ 定义。这种分解自然地确保了总能量随系统大小线性增长，这一性质被称为**[广延性](@entry_id:144932)**。

#### 描述符：网络的眼睛

那么，网络如何以一种尊重所有对称性的方式“看到”一个原子的局域环境呢？我们不能简单地将邻近原子的原始笛卡尔坐标输入给它，因为这些坐标会随着旋转而改变。取而代之的是，我们计算环境的一个数学“指纹”，称为**描述符**或**[对称函数](@entry_id:177113)**。这个描述符是一个数字向量，它唯一地表征了邻域的几何结构，并且其构造方式使其自动对邻近原子的平移、旋转和[置换](@entry_id:136432)保持不变。

一个著名的例子是由 Behler and Parrinello 开发的方案 [@problem_id:2784613]。它使用两种类型的函数来构建指纹。**径向函数**就像一组球形探针，计算原子 $i$ 在不同距离处有多少个邻居。**角向函数**通过测量以原子 $i$ 为中心的三原子组之间的角度[分布](@entry_id:182848)来捕捉三维结构。通过组合一组丰富的径向和角向函数，我们可以为每个原子环境创建一个唯一的、不变的描述符向量 $\mathbf{D}_i$。

#### 化学身份

当然，即使在相似的几何环境中，碳原子的行为也与硅原子不同。必须告知网络原子的化学身份。一个极其简单而有效的解决方案是为每种化学元素分配一个唯一的、可学习的数字向量，称为**物种嵌入**。这个嵌入向量与几何描述符一起被输入到[神经网](@entry_id:276355)络中。然后，网络不仅学习几何结构如何影响能量的一般规则，还学习每种元素的特定化学特性 [@problem_id:3422822]。

整个流程堪称一件艺术品：我们从原始原子位置开始。对于每个原子，我们计算一个以对称不变的方式编码其局域几何结构的描述符向量。这个描述符，连同一个物种嵌入，被输入到一个[神经网](@entry_id:276355)络中，该网络输出原子能量 $E_i$。总能量就是这些原子能量的总和。

### 揭开“黑箱”的神秘面纱

“[神经网](@entry_id:276355)络”这个词听起来可能很神秘，但其核心只是一个高度灵活的数学函数。考虑一个只有两个原子的简单系统，一个二聚体，相距为 $r$。NNP 的机制可以归结为一系列明确定义的步骤。距离 $r$ 被转换成一个描述符值。这个值通过一个由简单的[非线性](@entry_id:637147)函数（如[双曲正切函数](@entry_id:634307) $\tanh$）组成的网络。网络中的每个神经元会组合其输入，加上一个偏置，并应用这个激活函数。通过对这些神经元的输出求和，网络可以为相互作用势 $V(r)$ 构建出一条非常复杂且平滑的曲线，从而能够精确地模拟二聚体的[结合能](@entry_id:143405)随其间距变化的函数关系 [@problem_id:90970]。

当我们考虑更复杂的系统时，这种方法的真正威力就显现出来了。由于原子能量 $E_i$ 是*整个*局域邻域描述符 $\mathbf{D}_i$ 的[非线性](@entry_id:637147)函数，该模型自然地捕捉了**[多体相互作用](@entry_id:751663)**。一个原子的能量不仅仅是成对吸引和排斥的总和；它复杂地依赖于其邻居中的三原子组、四原子组以及更大原子团簇的角度和相对位置。正是这种对[高阶相互作用](@entry_id:263120)的隐式包含，使得 NNP 能够捕捉[化学键合](@entry_id:138216)中微妙的量子力学效应，远远超过了简单经典势的能力。这些多体项的范围自然受到[截断半径](@entry_id:136708)的限制，这为势所能描述的相互作用的“体序”施加了一个有限的界限 [@problem_id:3431662]。然而，通过使用基于物理的[长程相互作用](@entry_id:140725)（如带有环境依赖[电荷](@entry_id:275494)的[静电相互作用](@entry_id:166363)）来增强这些局域模型，可以创建实际上具有无限范围多体特征的势 [@problem_id:3431662]。

### 从景观到运动：[保守力](@entry_id:170586)的重要性

一旦我们有了学习到的景观 $V_{\text{NNP}}(\mathbf{R})$，我们如何让原子运动起来？我们需要力。正如我们所见，力是势的负梯度，$\mathbf{F}_i = -\nabla_{\mathbf{R}_i} V$。[神经网](@entry_id:276355)络最优雅的一个方面是它们被设计成可[微分](@entry_id:158718)的。使用一种称为**[自动微分](@entry_id:144512)**的算法（即用于训练网络的“[反向传播](@entry_id:199535)”算法），我们可以计算能量相对于每个原子坐标的解析梯度，从而得到与学习到的势完全对应的力 [@problem_id:3422849]。

这是一个关键点。因为力直接从一个单一、行为良好的[标量势](@entry_id:276177)导出，所以得到的[力场](@entry_id:147325)保证是**保守的**。在物理学上，这意味着模型不能自发地创造或毁灭能量。如果你模拟一个球在这个景观上滚动，它的总能量（动能加[势能](@entry_id:748988)）将是守恒的，这是[微正则系综](@entry_id:141513)动力学的基本定律。这种能量-力的一致性对于运行稳定且具有物理意义的[分子动力学模拟](@entry_id:160737)至关重要 [@problem_id:2459317]。在实践中为了保持这一特性，即使是像截断这样的细节也必须小心处理。一个尖锐、不连续的截断就像[能量景观](@entry_id:147726)中的悬崖，会引起一个不符合物理的力的突变，从而破坏[能量守恒](@entry_id:140514)。因此，总是使用平滑的切换函数来确保势及其导数（力）在[截断半径](@entry_id:136708)处平缓地趋于零 [@problem_id:2459317]。

### 处在边缘：不确定性的前沿

NNP 是一种插值方案。它在预测与训练期间见过的构型相似的能量方面表现非常出色。但是，当系统在模拟过程中漫游到[构型空间](@entry_id:149531)中一个全新的、未探索过的区域时，会发生什么呢？这就是**外推**问题，也是所有[机器学习模型](@entry_id:262335)的阿喀琉斯之踵。NNP 在这些区域的预测可能会非常不准确，导致不符合物理的力，并引发灾难性的模拟崩溃。

正是在这里，**不确定性**的概念变得至关重要。模型预测的误差可以分解为两种类型 [@problem_id:2648582]。
1.  **[认知不确定性](@entry_id:149866)**：这是由于*知识缺乏*导致的不确定性。在训练数据中代表性不足的景观区域，这种不确定性很高。这是外推误差的来源。幸运的是，因为它代表了模型*不知道*的东西，所以它是可以减少的。
2.  **[偶然不确定性](@entry_id:154011)**：这是由于*内在随机性*导致的不确定性。如果参考的[量子计算](@entry_id:142712)本身存在统计噪声（如在[量子蒙特卡洛方法](@entry_id:753887)中），或者如果我们正在构建一个粗粒度模型，其中被舍弃的细粒度细节的行为引入了有效的随机性，就可能产生这种不确定性。这种不确定性是数据或所选描述层次的内在特征，不能通过简单地添加更多同[类数](@entry_id:156164)据来减少。

处理认知不确定性是 NNP 发展的一个主要前沿领域。一个强大的策略是使用一个由多个在相同数据上独立训练的 NNP 组成的集成模型。在模型有信心的区域，集成模型中的所有[神经网](@entry_id:276355)络都会给出相似的预测。在未探索的区域，它们的预测会大相径庭。这种[分歧](@entry_id:193119)可以作为[模型不确定性](@entry_id:265539)的定量度量。这为一种被称为**主动学习**的优美策略打开了大门。当一个由 NNP 引导的模拟检测到它正在进入一个高不确定性区域时，它可以自动暂停，调用昂贵但精确的量子力学代码来获取一个新的参考数据点，将这个新信息添加到[训练集](@entry_id:636396)中，并即时重新训练 NNP。这使得势能够在它需要的地方精确地学习和改进，在探索的过程中构建一张越来越完整的量子景观地图 [@problem_id:2459317]。正是这种[物理模拟](@entry_id:144318)与机器学习的无缝融合，使[神经网](@entry_id:276355)络势成为现代科学中最令人兴奋和最具变革性的工具之一。

