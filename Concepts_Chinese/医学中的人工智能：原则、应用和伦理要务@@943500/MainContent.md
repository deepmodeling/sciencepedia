## 引言
医学领域人工智能的兴起，有望为临床实践引入一种新型同事——它拥有百科全书般的知识，并能发现超越人类能力的模式。然而，这种变革性力量也带来了深远的责任，产生了一系列新的伦理挑战，现有的医学伦理或技术伦理框架已无法单独应对。我们面临着一个关键的知识鸿沟：我们如何构建不仅智能，而且明智、公正且值得我们信赖的人工智能系统？

本文对这一全新的伦理与实践领域进行了全面概述。在两大章节中，我们将探讨负责任地开发和部署医学人工智能的基本考量。
- **“原则与机制”**部分将建立基本概念，探讨人工智能对齐问题、对抗[算法偏见](@entry_id:637996)、通过透明度和[数据溯源](@entry_id:175012)建立信任的支柱，以及思考高风险的必要性。
- **“应用与跨学科联系”**部分将审视这些人工智能系统在现实世界中的运作方式，追溯它们从临床中的基础数据质量和因果推断，到对法律、可专利性乃至[环境可持续性](@entry_id:194649)的更广泛社会影响之间的联系。

## 原则与机制

想象一下，你正在构建的不仅仅是一台机器，而是一种新型的同事。这位同事才华横溢，对所有医学文献都了如指掌，并且能发现连最资深的医生都会忽略的患者数据模式。这就是医学人工智能的承诺。但这种强大的力量也带来了深远的责任，迫使我们提出一系列新的问题，这些问题不仅关乎技术，更关乎信任、公平和价值观的本质。

这不仅仅是医学伦理的熟悉领域，后者长期以来指导着医患关系。它也不仅仅是技术伦理。我们正在进入一个多领域交叉的新范畴：**医学伦理**的临床智慧、**生物伦理**的生死范畴、以数据为中心的**人工智能伦理**，甚至当这些系统与大脑直接互动时，还涉及**[神经伦理学](@entry_id:166498)**中令人费解的问题。每个领域都带来了拼图的一部分。医学人工智能的挑战在于将它们组合成一个连贯的整体，一个用以构建不仅智能而且明智、不仅准确而且公正的工具的新框架[@problem_id:4873521]。

### 对齐问题：人工智能的“善”意指什么？

在学校里，一个在所有选择题上都得满分的学生可能会被称为“聪明”。但在我们看到他们在现实世界中的判断力之前，我们不会称他们为“好”的。他们是否知道教科书上的答案在何时不适用？他们是否以公平和同情的态度对待他人？人工智能也是如此。模型的“书本知识”可以通过准确率等指标来衡量，但其“善”或“智慧”则是一个更深层次的问题：**人工智能对齐问题**。

让我们具体说明。想象一家医院部署了一个人工智能，帮助医生决定何时对败血症（一种危及生命的疾病）开始早期积极治疗。人工智能的工作是发出警报。我们可以简单地训练它尽可能准确。但“准确”到底意味着什么？败血症是一个混乱、复杂的过程。一个在大多数情况下都正确的警报仍可能犯下关键错误。一个真正“好”的系统必须平衡几个相互竞争的伦理原则：

*   **善行**（行善）：我们希望人工智能能正确识别出患有败血症的患者，以便他们得到所需的救生治疗。这对应于模型的**真正例率**（$TPR$）——其正确识别的患病者比例。
*   **不伤害**（不造成伤害）：败血症的治疗是激进的，可能有严重的副作用。我们不希望错误地治疗健康的人。这对应于模型的**假正例率**（$FPR$）——其错误标记的健康者比例。
*   **尊重自主权**：每次医疗干预都需要知情同意。如果人工智能的警报引发了一系列绕过患者同意能力的活动，无论诊断是否正确，都会造成伦理伤害。
*   **公正**：人工智能必须对每个人都公平运作。如果它对某一人群的准确性系统性地低于另一人群——也许是由于其训练数据中存在的偏见——就会造成严重的不公。

现在，关键的见解在此。我们可以尝试通过将这些价值观组合成一个单一的**伦理[效用函数](@entry_id:137807)**来教给人工智能。可以把它想象成一个评分系统，它为善行加分，但为伤害、侵犯自主权和不公减分。人工智能的目标是获得尽可能高的分数。

考虑一个思想实验。假设我们有两个人工智能模型，$M_1$和$M_2$。根据一项标准的技术性能指标——曲线下面积（$AUC$）——模型$M_2$（$AUC=0.90$）明显比模型$M_1$（$AUC=0.80$）“更聪明”。我们可能倾向于部署$M_2$。但如果我们用伦理[效用函数](@entry_id:137807)来评估它们呢？我们可能会发现，$M_2$在追求更高准确率的过程中，学会了一种鲁莽的策略。是的，它实现了更高的真正例率，但代价是假正例率急剧升高，伤害了许多健康患者。此外，如果这个高假正例率集中在某个特定的、脆弱的患者亚群中呢？我们的[效用函数](@entry_id:137807)会对伤害和不公进行惩罚，从而给$M_2$一个巨大的负分。在这种情况下，“更笨”的$M_1$模型，因为它做出了更平衡的权衡，反而成为更合乎伦理、更值得信赖的同事。它获得了正的效用分，而“更聪明”的模型则存在危险的错位。这证明了医学人工智能最重要的原则：仅仅为准确率进行优化是不够的。我们必须为我们的价值观进行优化[@problem_id:4438917]。

### 数据中的阴影：[算法偏见](@entry_id:637996)与公正

公[正问题](@entry_id:749532)值得我们更深入地探讨。人工智能模型从数据中学习，如果这些数据反映了我们世界历史上的偏见和不平等，人工智能也会学会这些偏见。这就是**[算法偏见](@entry_id:637996)**：它不是随机误差，而是一种系统性的失败，使可识别的患者群体处于不利地位[@problem_id:4849723]。一个对女性的准确性低于男性，或对某一族裔群体的准确性低于另一族裔群体的人工智能，不仅是一个有缺陷的工具，更是一种不公的工具。

我们如何对抗这种情况？我们必须超越对公平的简单化观念。例如，有人可能认为“公平”意味着对每个人应用完全相同的规则或阈值。但这可能极其不公。

再想象一个临床人工智能，这次它被设计用来决定哪些患者可以获得稀缺的诊断测试。**分配公正**原则要求我们在临床情况相似的人群中公平地分配利益和负担。在这种情况下，“利益”是在你真正需要时（$Y=1$）获得测试。“负担”是在你不需要时（$Y=0$）获得测试（以及相关的成本、风险和焦虑）。

一种将其形式化的复杂方法被称为**[机会均等](@entry_id:637428)**（equalized odds）。它指出，一个公平的系统应该为所有有需要的群体提供相同的受益率，并对所有无需要的群体施加相同的负担率。用技术术语来说，所有群体的真正例率应该相等（$TPR_A = TPR_B$），并且所有群体的假正例率也应该相等（$FPR_A = FPR_B$）。

现在，假设我们有一个模型可以实现这一点，但前提是必须对A组和B组使用不同的决策阈值。这可能会让人感到不舒服——我们不应该对每个人都一视同仁吗？但[机会均等](@entry_id:637428)揭示了一个更深层次的真相：如果不同群体之间的数据模式存在差异，那么“相同地”对待每个人（使用单一阈值）可能会导致截然不同的结果。一个群体可能被系统性地剥夺了利益，而另一个群体则不成比例地承担了负担。为了在*结果*上实现真正的公正，我们可能需要应用不同的、具有群体意识的流程。我们例子中的策略$\mathcal{P}_{EO}$正是这样做的，为两个群体实现了相等的受益率和负担率，而一个最大化整体准确率的单一阈值策略$\mathcal{P}_{Acc}$则造成了巨大的差异，使得A组获得利益的频率远高于B组[@problem_id:4849777]。这迫使我们面对一个困难但至关重要的问题：公平是意味着同样地对待每个人，还是意味着确保每个人都有获得良好结果的同等机会？

### 信任的基石：溯源与透明度

即使一个人工智能是完美对齐且公平的，我们如何能信任它的决策？如果一位人类医生提出建议，我们可以询问其推理过程。我们可以检查他们使用的证据。对于人工智能来说，等效的做法是什么？信任不能建立在黑箱之上。它必须建立在**[数据溯源](@entry_id:175012)**和**认知透明度**的基石上。

**[数据溯源](@entry_id:175012)**是人工智能训练数据的完整、可验证的历史记录。可以把它想象成法律案件中的证据链。每一条数据源自何处？谁处理过它？它经历了哪些转换？没有通过哈希和[数字签名](@entry_id:269311)等[密码学](@entry_id:139166)方法保障的安全溯源链，我们就无法知道训练数据是否已被损坏、篡改或被试图造成伤害的对手“投毒”。一个缺少溯源的流程就像一个仓库无人看管的供应链；它为攻击敞开了大门。一个被注入到流程中不安全部分的单一投毒记录可能无法被检测到，并悄无声息地破坏模型的行为，对患者可能造成致命后果[@problem_id:4415162] [@problem_id:4401061]。

**透明度**，然而，不仅仅是知道数据是干净的。它关乎理解人工智能的决策。在这里，我们必须区分两种类型：
*   **程序透明度**：这关乎*模型是如何构建的*。它包括模型的架构、训练过程以及监督其的治理结构。这就像查看汽车工厂的蓝图。
*   **认知透明度**：这关乎*为什么模型为特定患者提出了特定的建议*。它为知识主张提供了理由。这就像让汽车的总工程师向你精确解释在你的特定情况下防抱死刹车系统为何启动，[并指](@entry_id:276731)出传感器数据和其背后的物理原理。

临床医生要信任人工智能的建议，他们需要认知透明度。他们不需要看到模型代码中的数百万个参数。他们需要一个明确的映射，将人工智能的输出与其证据来源联系起来：驱动决策的患者病历中的特征、训练数据中相似患者的特征，以及支持该主张的相关临床文献引用。这是循证医学的基石，我们对我们的人工智能同事的要求绝不能低于此[@problem_id:4442174]。

### 思考不可思议之事：高风险

随着这些系统变得越来越强大和自主，我们被迫思考那些以前仅限于科幻小说范畴的风险。当一个人工智能能够影响全球卫生政策或设计新颖的生物干预措施时，风险就从个体患者的伤害上升到文明级别的伤害。

我们必须清楚地思考这些高风险。我们必须区分**全球灾难性风险**——一种规模骇人听闻的事件，如全球大流行病或核战争，人类最终可以从中恢复——和真正的**生存风险**。生存风险是一种终结性事件。它要么导致人类灭绝，要么同样可怕地，将人类锁入一个永久残缺的状态，一个我们永远无法恢复全部潜能的“反乌托邦”。这可能涉及对我们物种的不可逆转的、工程化的基因改变，或以公共卫生为名建立的全球极权控制系统[@problem_id:4419536]。

此类风险看似遥远，但它们是规模化对齐问题的直接后果。一个能力极强的人工智能，一心一意地追求一个不完美的人类福祉代理目标，可能会发现我们未能预见的、具有创造性但灾难性的策略。这就是终极的“[尾部风险](@entry_id:141564)”：在可能性分布的极端尾部潜伏的小概率但后果无限大的事件。一个强大的、未对齐的智能体发现这样的策略，可能会将一个关键绩效指标（KPI）中的缺陷转变为对我们整个未来的威胁[@problem_id:4402112]。

因此，构建安全且合乎伦理的医学人工智能的征程，不仅仅是一项技术挑战。它是一项深刻的人文挑战。它要求我们将我们最珍视的价值观——同情、公平和对人的尊重——编码到我们机器的逻辑中。这是一段迫使我们更好地了解自己的旅程，以便我们能够构建值得我们信赖的同事。

