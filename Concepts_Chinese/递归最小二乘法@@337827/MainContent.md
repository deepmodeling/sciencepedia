## 引言
在一个系统不断变化的世界里，实时学习和适应的能力不仅是一种优势，更是一种必需。传统方法通常需要离线处理大量数据，但工程和科学领域的许多实际问题要求参数估计能够与持续不断的新[信息流](@article_id:331691)同步进行。本文通过探讨递归最小二乘 (RLS) [算法](@article_id:331821)——一种强大的在线系统辨识方法——来应对这一挑战。它通过提供一种在每次新测量时更新系统模型的方法，填补了静态、批量处理方法留下的空白。读者将首先探索 RLS 的“原理与机制”，揭示其精妙的“预测-检验-校正”循环、协方差矩阵的作用以及“遗忘”旧数据的艺术。随后，“应用与跨学科联系”一章将展示该[算法](@article_id:331821)的广泛用途，从构建自整定控制器到提升我们对宇宙的观测清晰度，揭示 RLS 作为[自适应学习](@article_id:300382)基本原则的地位。

## 原理与机制

想象你是一位正在绘制未知土地地图的探险家。你有一张地图，但它并不完整。你知道地标之间的大致关系——比如说，某地点的海拔 $y$ 是其坐标的简单线性函数，如 $y_t = \theta_1 + \theta_2 x_{1,t} + \theta_3 x_{2,t}$。向量 $\boldsymbol{\theta}$ 包含了这片土地的“真实”参数，即定义其形状的秘密数字。你的任务就是发现它们。每当你在一个新地点 $(\boldsymbol{\phi}_t, y_t)$ 进行一次测量，其中 $\boldsymbol{\phi}_t$ 是坐标和特征的向量，如 $[1, x_{1,t}, x_{2,t}]^T$，你就会得到一条新线索。

一种天真的方法是等到收集了一千次测量数据后，再坐下来一次性找出最能拟合所有数据的参数——这是一个“批量”处理过程。但如果你需要实时导航呢？或者，如果这片土地本身就像沙漠中的沙丘一样，在你的脚下悄然变化呢？你需要一种方法，在你迈出的每一步都能更新你的地图，一种能够在实践中学习的方法。这就是[递归估计](@article_id:349160)的世界，而递归最小二乘 (RLS) [算法](@article_id:331821)是其中最优雅、最强大的成员之一。

### [算法](@article_id:331821)核心：通过预测进行校正

RLS [算法](@article_id:331821)的核心是一个优美的自校正循环，它在每个时间点展开。它遵循一个简单的哲学：预测、检验和校正。让我们看看它是如何工作的。

假设在时刻 $t-1$，我们对系统参数的最佳猜测是 $\boldsymbol{\theta}_{t-1}$。此时，一组新数据到来：一组输入 $\boldsymbol{\phi}_t$ 和实际测量的输出 $y_t$。

1.  **预测**：使用我们当前对世界的认知，即 $\boldsymbol{\theta}_{t-1}$，我们预测输出*应该*是什么。我们的预测值为 $\hat{y}_t = \boldsymbol{\phi}_t^{\mathsf{T}} \boldsymbol{\theta}_{t-1}$。

2.  **检验**：我们将预测与现实进行比较。差值即为**预测误差**，或称“意外”：
    $$
    \epsilon_t = y_t - \hat{y}_t = y_t - \boldsymbol{\phi}_t^{\mathsf{T}} \boldsymbol{\theta}_{t-1}
    $$
    如果这个误差为零，说明我们当前的认知对于这个新数据点是完美的，我们无需做任何改变。但如果误差不为零，则意味着我们的认知是错误的，需要更新。 [@problem_id:1588640]

3.  **校正**：我们通过加上一个与刚刚发现的误差成正比的校正项来更新我们的参数估计。
    $$
    \boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} + \boldsymbol{k}_t \epsilon_t
    $$
    这是核心的[更新方程](@article_id:328509)。我们的新估计 $\boldsymbol{\theta}_t$ 是旧估计加上一个校正量。但这个神秘的 $\boldsymbol{k}_t$ 是什么呢？它被称为**增益向量**，是 RLS [算法](@article_id:331821)智能的绝对核心。这个向量决定了总误差 $\epsilon_t$ 中有多少“责任”应分配给 $\boldsymbol{\theta}_{t-1}$ 中的每个参数。它是一个复杂的权重因子，决定了如何根据我们最新的“意外”来调整我们的认知。

为了让这个过程更具体，想象一个简单的双参数系统，在时刻 $n-1$，我们的估计是 $\boldsymbol{w}(n-1) = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$。一个新的数据点到来，其输入为 $\boldsymbol{u}(n) = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$，真实输出为 $d(n) = 2$。我们的预测将是 $\boldsymbol{u}(n)^{\mathsf{T}}\boldsymbol{w}(n-1) = 1(1) + 1(-1) = 0$。误差高达 $2 - 0 = 2$。RLS [算法](@article_id:331821)计算出一个增益向量，假设结果为 $\boldsymbol{k}(n) = \begin{pmatrix} 0.5 \\ 0.25 \end{pmatrix}$。那么更新就是 $\boldsymbol{w}(n) = \begin{pmatrix} 1 \\ -1 \end{pmatrix} + \begin{pmatrix} 0.5 \\ 0.25 \end{pmatrix} \times 2 = \begin{pmatrix} 2 \\ -0.5 \end{pmatrix}$。我们的估计值已经显著地向能够解释新数据的方向移动 [@problem_id:2850229]。但[算法](@article_id:331821)是如何知道要进行这样特定的校正的呢？答案在于另一个更基本的组成部分。

### 不确定性的日记：[协方差矩阵](@article_id:299603) $P$

增益向量 $\boldsymbol{k}_t$ 并非魔法；它是由一个称为**[逆协方差矩阵](@article_id:298898)**的矩阵 $\boldsymbol{P}_t$ 计算得出的。如果说增益向量是[算法](@article_id:331821)的智能，那么 $\boldsymbol{P}$ 矩阵就是它的记忆和[置信度](@article_id:361655)。

可以将 $\boldsymbol{P}_t$ 想象成一本“不确定性日记”。对于一个 $M$ 参数系统，它是一个 $M \times M$ 矩阵。$\boldsymbol{P}_t$ 的对角线元素告诉我们[算法](@article_id:331821)对其每个相应参数估计的不确定性程度。值大表示不确定性高（“我完全不知道这个参数是什么”），而值小则表示不确定性低（“我很确定我已经搞定了这个参数”）。

当我们启动 RLS [算法](@article_id:331821)时，我们一无所知。因此，我们用一个非常大的不确定性矩阵来初始化它，通常是 $\boldsymbol{P}_0 = \alpha \boldsymbol{I}$，其中 $\boldsymbol{I}$ 是单位矩阵，$\alpha$ 是一个非常大的数（例如，$10^6$）。这就像告诉[算法](@article_id:331821)：“你对 $\boldsymbol{\theta}_0 = \boldsymbol{0}$ 的初始猜测很可能是错的。准备好根据你看到的前几条数据彻底改变你的想法！” [@problem_id:1588640]。

增益向量 $\boldsymbol{k}_t$ 与上一步的不确定性矩阵 $\boldsymbol{P}_{t-1}$ 直接成正比：
$$
\boldsymbol{k}_t = \frac{\boldsymbol{P}_{t-1}\boldsymbol{\phi}_t}{\lambda + \boldsymbol{\phi}_t^{\mathsf{T}} \boldsymbol{P}_{t-1} \boldsymbol{\phi}_t}
$$
此处，$\lambda$ 是我们稍后将讨论的“[遗忘因子](@article_id:354656)”。看看这个关系！如果我们的不确定性 $\boldsymbol{P}_{t-1}$ 很大，我们的增益 $\boldsymbol{k}_t$ 也会很大。这意味着我们对参数进行大幅修正。如果我们的不确定性很小，增益就会很小，我们只做微小的调整。这是非常直观的：你越不确定，就越应该愿意从新证据中学习。

每当一个新数据点为我们提供信息，我们对世界的不确定性就会降低。因此，在更新参数之后，我们还必须更新我们的不确定性日记。RLS [算法](@article_id:331821)对此有一条优美的规则：
$$
\boldsymbol{P}_t = \frac{1}{\lambda}(\boldsymbol{I} - \boldsymbol{k}_t \boldsymbol{\phi}_t^{\mathsf{T}})\boldsymbol{P}_{t-1}
$$
这个方程表明，新的不确定性 $\boldsymbol{P}_t$ 是旧不确定性 $\boldsymbol{P}_{t-1}$ 的一个“缩小”版本。我们学到了一些东西，所以我们变得更加自信。这种对 $\boldsymbol{P}$ 的递归更新是一个计算上的奇迹。它源于一个[矩阵代数](@article_id:314236)技巧（Sherman-Morrison-Woodbury 公式），使我们能够在不执行耗时的完全求逆（需要 $\mathcal{O}(M^3)$ 次操作）的情况下，找到更新后[矩阵的逆](@article_id:300823)。相反，整个 RLS 循环——计算增益、更新参数和更新不确定性——每步只需 $\mathcal{O}(M^2)$ 次操作 [@problem_id:2891039] [@problem_id:2408064]。

### 遗忘的艺术：跟踪变化的世界

到目前为止，我们一直假设我们未知的地貌是静态的。但如果不是呢？如果参数 $\boldsymbol{\theta}$ 随着时间缓慢漂移怎么办？这就是**[遗忘因子](@article_id:354656)** $\lambda$ 发挥作用的地方。

在我们的方程中，$\lambda$ 是一个介于 0 和 1 之间的数。如果我们设置 $\lambda = 1$，我们就处于“完全记忆”模式。[算法](@article_id:331821)会最小化其所见过的所有数据的平方误差之和。随着更多数据的输入，$\boldsymbol{P}$ 矩阵会向零收缩，增益 $\boldsymbol{k}_t$ 会消失，[算法](@article_id:331821)最终会“进入休眠”。它对其积累的知识变得如此自信，以至于不再理会新数据。这对于[静态系统](@article_id:336055)来说是极好的，因为估计会完美收敛。但如果[系统发生](@article_id:298241)变化，休眠的估计器将不会注意到 [@problem_id:2408064]。

为了跟踪一个变化的世界，我们必须设置 $\lambda < 1$。通过这样做，我们是在告诉[算法](@article_id:331821)去最小化一个*指数加权*的平方误差和。这会给予近期数据更多的权重，而给予久远的数据指数级的较少权重。$\boldsymbol{P}_t$ 更新中的 $1/\lambda$ 项现在充当一个“膨胀”因子，防止不确定性矩阵收缩到零。这使得增益保持活跃，确保估计器始终保持响应并准备好适应变化。

选择 $\lambda$ 是一门艺术，一个经典的工程权衡。
*   **小的 $\lambda$**（例如 0.90）会创建一个记忆短暂的估计器。它会迅速忘记过去，使其能够快速跟踪系统参数的变化。然而，这种灵活性是有代价的：估计器对随机测量噪声变得非常敏感，导致参数估计值“跳跃”或具有高方差。
*   **大的 $\lambda$**（例如 0.999）会创建一个记忆长久的估计器。它对大量数据进行平均，使其估计值非常平滑，对噪声不敏感（低方差）。缺点是它对系统中的真实变化响应迟缓，导致其跟踪存在滞后或偏差。

这就是经典的**[偏差-方差权衡](@article_id:299270)**在起作用。对于一个参数缓慢漂移的系统，你必须寻求一个平衡：选择一个足够小以跟上漂移，但又足够大以不被每个噪声干扰的 $\lambda$ [@problem_id:1608448] [@problem_id:2891111]。

### 游戏规则：每个估计器都需要什么

即使是最复杂的[算法](@article_id:331821)也遵循“垃圾进，垃圾出”的规则。为了让 RLS 成功辨识真实参数，它接收的数据必须满足一些关键条件。

首先，输入信号必须是**[持续激励](@article_id:327541)**的。这是一个花哨的术语，其思想很简单：为了辨识 $M$ 个未知参数，输入信号必须足够丰富，以在至少 $M$ 个独立方向上“摇动”系统。如果输入过于简单——例如，用一个单一的[正弦波](@article_id:338691)来辨识一个具有两个以上参数的滤波器——数据将不包含足够的信息来区分所有参数。这就像试图仅从一个角度观察物体的影子来判断其三维形状一样。没有足够的视角，问题的某些维度仍然是不可观测的，任何[算法](@article_id:331821)都无法解决它看不见的问题 [@problem_id:2891027]。

其次，标准的 RLS 公式假设影响系统的任何噪声或干扰都是**白噪声**——即随机且在时间上不相关。如果这个假设被违反了会怎样？想象一个[温度控制](@article_id:356381)器，其主要干扰是来自空调的缓慢、周期性的气流。这就是**[有色噪声](@article_id:329140)**。在许多常见设置中，这意味着输入信号与噪声变得相关。这种对核心假设的违反会导致**有偏估计**。[算法](@article_id:331821)可能会平滑地收敛到一组参数，但它们将是错误的参数 [@problem_id:1608430]。

最后，即使有[遗忘因子](@article_id:354656)，如果系统在长时间内没有激励（例如，恒定输入），估计器仍然可能“进入[休眠](@article_id:352064)”。一个防止这种情况的实用技巧叫做**[协方差膨胀](@article_id:639900)**，即在每一步都向 $\boldsymbol{P}$ 矩阵中加入一个小的[正定矩阵](@article_id:311286) $\boldsymbol{Q}$：$\boldsymbol{P}_k \leftarrow \boldsymbol{P}_k + \boldsymbol{Q}$。这就像注入了一点强制的不确定性，确保估计器永远不会完全自满，并始终准备好学习 [@problem_id:1608437]。

### RLS 的优势：曲率的力量

鉴于 RLS 的[计算成本](@article_id:308397)为 $\mathcal{O}(M^2)$，高于更简单的[最小均方 (LMS)](@article_id:373058) [算法](@article_id:331821)的 $\mathcal{O}(M)$ 成本，我们为什么还要使用 RLS？答案是其惊人的收敛速度，尤其是在处理“有色”输入信号时。

想象一下我们试图最小化的成本函数是一片地貌。对于许多现实世界的信号，这片地貌不是一个简单的圆形碗；而是一个长而陡峭的狭窄山谷。这种形状是由于输入信号的自[相关矩阵](@article_id:326339) $\boldsymbol{R}$ 具有较大的**[特征值](@article_id:315305)扩展**所导致的。
*   **LMS [算法](@article_id:331821)**是一种简单的[梯度下降法](@article_id:302299)。它就像一个只能看到脚下坡度的徒步者。在一个狭窄的山谷中，最陡下降方向主要指向附近的峡谷壁，而不是沿着谷底向下。因此，LMS [算法](@article_id:331821)会缓慢而低效地曲折前进，以之字形向最小值移动。其[收敛速度](@article_id:641166)极其缓慢，受限于山谷最窄的维度。
*   **RLS [算法](@article_id:331821)**则是一种二阶方法。通过维护 $\boldsymbol{P}$ 矩阵（该矩阵是这个自[相关矩阵](@article_id:326339) $\boldsymbol{R}$ 的逆的估计），它学习了山谷的*曲率*。它有效地对梯度进行“[预处理](@article_id:301646)”，将长而窄的山谷转换为一个完美的圆形碗。在这个变换后的空间中，梯度直接指向最小值。

因此，RLS 的[收敛速度](@article_id:641166)极快，通常在参数数量 ($M$) 数量级的迭代次数内完成，并且其[收敛速度](@article_id:641166)在很大程度上与输入信号的[特征值](@article_id:315305)扩展无关。它每一步都付出更高的计算代价，但用少得多的步数到达目的地。它利用对地貌形状的了解来找到通往谷底的最直接路径，展示了不仅适应误差，而且适应其接收信息结构本身的强大力量 [@problem_id:2891055] [@problem_id:2891111]。