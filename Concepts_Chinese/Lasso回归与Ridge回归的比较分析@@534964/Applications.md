## 应用与跨学科联系

在遍历了 $\ell_1$ 和 $\ell_2$ 范数的数学景观之后，我们看到了那种优雅的几何学，它使一个成为“选择者”，另一个成为“收缩者”。我们领略了 LASSO 菱形般的尖锐角落与 Ridge 光滑、民主的球面之间的鲜明差异。但是，这些方法乃至所有伟大科学的真正魔力，不仅在于其原理的抽象之美，更在于这些原理如何在现实世界中显现。我们为什么要关心这些几何形状？它们为我们*做*了什么？

在本章中，我们将离开纯粹数学的原始世界，进入科学与工程领域那些混乱、复杂但引人入胜的领域。我们将看到，这个看似简单的选择——在求[绝对值](@entry_id:147688)之和与求平方和之间——如何产生深远的影响，塑造了我们寻找致病基因、构建高效人工智能、理解社会现象，甚至定义“相似性”这一概念本身的方式。LASSO 和 Ridge 的原理不仅仅是教科书上的练习题；它们是强大的发现工具，为我们施加信念、管理复杂性以及从噪声中提取信号提供了一个框架。

### 驯服相关线索的艺术

大自然在呈现信息时很少是吝啬的。更多的时候，它是冗余的。一个基因的表达常常与另一个相关；一只股票的价格与其竞争对手相关；一个城市的温度是附近城市温度的良好预测指标。这种现象，统计学家称之为多重共线性，提出了一个根本性的挑战：如果两个预测变量高度相关，我们如何为它们分配功劳或责任？

想象一个简单的生物系统，其中两个活性紧密相关的基因，都对某个性状有同等的贡献。Ridge 回归，凭借其二次惩罚项，表现得像一个明智而公平的委员会。它看到两个基因都参与其中，并认识到它们的相关性，于是它分散了风险。它将*两个*基因的系数都向零收缩，在它们之间分配预测能力。它优雅地承认了它们共同的角色。[@problem_id:3169449]

另一方面，LASSO 是一个无情的极简主义者。它的唯一目标是稀疏性。面对两个高度相关的基因，它发现，就其 $\ell_1$ 惩罚而言，将所有信心押在一个基因上，并完全忽略另一个基因，将其系数驱动到恰好为零，是更“划算”的。虽然这产生了一个更稀疏、更简单的模型，但选择保留哪个基因可能是任意且不稳定的。如果现实情况是两个基因都确实参与其中，[LASSO](@entry_id:751223) 的强制选择会引入偏差，并可能导致比 Ridge 更细致入微的方法更差的预测性能。[@problem_id:3169449]

这不仅仅是一个玩具问题。在计算生物学中，当我们试图预测两种蛋白质是否相互作用时，我们会整合数十种证据来源——基因共表达、共同的进化历史、实验室实验结果等等。许多这类证据都是相关的；例如，在一个功能模块中协同工作的蛋白质可能既共表达又具有高度的功能相似性。一个假设这些线索是独立的朴[素模型](@entry_id:155161)会“重复计算”证据并错误地计算概率。一个更复杂的模型，如[逻辑斯谛回归](@entry_id:136386)，仍然必须应对这种相关性。在这里，Ridge 正则化成为一个必不可少的工具，它通过将相关证据来源的权重一起收缩来稳定模型，防止任何单一来源获得不当的影响力。[@problem_id:3341713]

在一个美妙的转折中，Ridge 的这种“分组”行为可以被用来让 LASSO 变得更好。在一种名为自适应 [LASSO](@entry_id:751223) 的方法中，我们首先运行 Ridge 回归以获得所有系数的初始、稳定的估计值。对于相关的预测变量，Ridge 会为它们分配相似的、非零的权重。然后我们使用这些初始权重来指导后续的 LASSO 步骤，告诉它对初始权重小的变量施加更重的惩罚。通过使用 Ridge 作为“侦察兵”，我们可以引导 LASSO 的[稀疏性](@entry_id:136793)搜索，帮助它区分真正不相关的预测变量和那些属于相关、重要群体的变量。[@problem_id:3095581]

### 从发现基因到雕塑人工智能

这些方法的力量远远超出了简单的线性模型。考虑一位社会学家研究一个[二元结果](@entry_id:173636)——比如一个人是否投票——使用收入和教育水平等预测变量。如果其中一个预测变量是[分类变量](@entry_id:637195)，比如一个人的居住州？我们可以用一系列“虚拟”变量来表示，每个州一个。一个关键问题是，居住在基准参考州以外的*任何*州是否有影响。我们可以使用一个有针对性的 Ridge 惩罚，它*只*应用于州[虚拟变量](@entry_id:138900)的系数群组。随着我们增加惩罚，模型会同时将所有州的估计效应向零收缩。在[逻辑斯谛回归](@entry_id:136386)的语言中，这意味着每个州与基准州相比的[优势比](@entry_id:173151)都被推向 1——这个值表示“无效应”。这为检验一整组相关变量的集体重要性提供了一种有纪律的方法。[@problem_id:3164725]

这种结构化正则化的思想在现代人工智能的核心找到了其最引人注目的应用。当今的[大型语言模型](@entry_id:751149)和视觉 Transformer 是庞然大物，拥有数十亿个参数。它们计算和内存成本的很大一部分在于[注意力机制](@entry_id:636429)，这些机制决定了输入序列的不同部分如何相互关联。如果用于计算这些关系的许多维度是冗余的呢？

我们可以将此构建为一个正则化问题。通过对这些注意力投影的权重应用 $\ell_1$ 惩罚，我们可以迫使模型仅使用可用维度的一个稀疏[子集](@entry_id:261956)来执行其任务。就像雕塑家凿掉多余的大理石以显露其中的雕像一样，[LASSO](@entry_id:751223) 充当了模型剪枝的自动化工具。它识别并消除了最不重要的计算路径，将它们的权重驱动到恰好为零。结果可能是一个更小、更快、更节省内存的模型，通常预测准确性几乎没有损失。这使得在资源有限的设备上部署强大的人工智能成为可能，比如智能手机或嵌入式传感器。[@problem_id:3140982]

### 宏大统一：作为普适原则的稀疏性

LASSO-Ridge [二分法](@entry_id:140816)的概念优雅性甚至延伸到更抽象的领域。到目前为止，我们一直考虑对系数的*向量*施加惩罚。但是，如果我们正则化的对象是一个*矩阵*呢？

在[计算机视觉](@entry_id:138301)和[推荐系统](@entry_id:172804)等领域，一个核心任务是“[度量学习](@entry_id:636905)”。目标不仅仅是分类一个对象，而是学习一种由矩阵 $M$ [参数化](@entry_id:272587)的距离或相似性度量，使得相似的项目彼此靠近，不相似的项目彼此远离。我们可以对这个矩阵 $M$ 进行正则化以鼓励理想的属性。

应用 Frobenius 范数惩罚（$\|M\|_F^2$，即矩阵元素的平方和）类似于 Ridge 回归。它收缩矩阵的所有元素，促进一个行为良好、稳定的度量。但是，与 LASSO 等价的矩阵形式是什么？它就是[核范数](@entry_id:195543)，$\|M\|_*$，定义为矩阵奇异值的和。正如 $\ell_1$ 范数促进向量元素的[稀疏性](@entry_id:136793)一样，核范数促进其*奇异值*的[稀疏性](@entry_id:136793)。一个几乎没有非零奇异值的矩阵是一个“低秩”矩阵。学习一个低秩度量等同于发现相似性的最重要方面可以在数据的低维投影中被捕获。原理是相同的：$\ell_1$ 风格的惩罚（核范数）执行了激进的特征选择，但是作用于一组更抽象的特征——度量本身的主成分。向量[稀疏性](@entry_id:136793)与矩阵低秩结构之间的这种并行是一个深刻的统一，揭示了一个单一的深层原理在看似不同的情境中发挥作用。[@problem_id:3146447]

这种惩罚的选择不仅仅是一个数学上的好奇心；它反映了我们对世界的假设以及我们对不同类型错误的容忍度。在工程或金融应用中，我们可能更关心最小化单一最坏情况的预测误差，而不是最小化平均误差。这对应于不是用标准的[均方误差](@entry_id:175403)（误差向量上的 $L_2$ 范数）来评估我们模型的性能，而是用最大绝对误差（$L_{\infty}$ 范数）。一个复杂的[模型选择](@entry_id:155601)流程可以调整像 $\lambda$ 这样的正则化超参数，以明确地优化这些量身定制的、现实世界的风险配置。[@problem_id:3201751]

最终，LASSO 和 Ridge 之间的区别是一个关于维度的故事。在一个低维世界中，$\ell_1$ 和 $\ell_2$ 范数之间的几何差异是微妙的。然而，随着维度数 $n$ 的增长，它们之间的差距急剧扩大。那个“最密集”的向量——所有分量都相等——是 $\ell_1$ 和 $\ell_2$ 范数差异最大的向量，其比率按 $\sqrt{n}$ 的比例缩放。[@problem_id:3544607] 正是在现代数据的广阔、高维空间中——从人类基因组到[神经网](@entry_id:276355)络的[参数空间](@entry_id:178581)——[LASSO](@entry_id:751223) 的极简主义哲学和 Ridge 的谨慎对冲，不仅变得不同，而且成为我们在复杂世界中寻找结构和意义的 quest 中必不可少且互补的工具。