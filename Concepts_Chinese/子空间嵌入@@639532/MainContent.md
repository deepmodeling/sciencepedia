## 引言
在一个由数据定义的时代，我们常常面临分析庞大数据集的挑战，这些数据集存在于难以想象的高维空间中。这种“维度灾难”会使即使是简单的计算任务，如拟合模型或发现关系，也变得异常缓慢和昂贵。然而，如果我们可以找到一种方法，将这个复杂的现实投影到一个简单的、低维的“影子”世界，而又不失其基本的几何信息，那会怎样呢？本文探讨的正是这样一种工具：[子空间](@entry_id:150286)嵌入。它旨在弥合这些嵌入的理论优雅性与其实际、革命性影响之间的知识鸿沟。首先，在“原理与机制”部分，我们将揭示[随机投影](@entry_id:274693)如何像一个“神奇罗盘”一样，在特定的感兴趣[子空间](@entry_id:150286)内保持向量长度和角度。然后，在“应用与跨学科联系”部分，我们将探讨这种强大的技术如何被用于驾驭海量数据集、加速机器学习算法，甚至在基本统计概念之间建立起令人惊讶的联系。

## 原理与机制

### 维度灾难与神奇罗盘

想象一下，你正试图描述一个极其复杂的东西——比如一张高分辨率的照片。一张百万像素的图片可以被看作是百万维空间中的一个单点。一年中金融市场的数据，或一个复杂蛋白质的构型，同样存在于这些难以想象的巨大空间中。直接在如此高的维度中工作通常被称为**维度灾难**。像测量距离或寻找最近邻居这样的简单任务，在计算上会变得极其恐怖。

但这里有一个绝妙的想法。如果我们实际关心的数据，尽管存在于这个巨大的空间里，却只占据一个非常小的、平坦的区域，那该怎么办？想象一下宇宙中所有猫的图片。它们只是所有可能的百万像素图像中一个极小极小的[子集](@entry_id:261956)。如果这个“有意义”的数据[子集](@entry_id:261956)位于或非常接近一个低维平面，也就是数学家所说的**[子空间](@entry_id:150286)**，那会怎样？假设我们的数据生活在一个$n$维空间中（$n$非常巨大，比如一百万），但所有有趣的变化仅沿着$k$个基本方向发生，而$k$可能只有几百。

这就引出了一个极好的问题：我们能否发明一种神奇的罗盘？一个能将我们原始、笨重的$n$维世界映射并投影到一个更易于管理的$m$维“影子”世界中的映射，其中$m$仅比$k$稍大一点？并且，这个映射能否在这样做的时候完美地保持我们那个特殊的$k$维[子空间](@entry_id:150286)的几何结构——所有的长度、角度和形状？如果我们有这样的工具，我们就可以研究这个简单的影子，并从中了解到关于复杂原始世界所需的一切信息。

这个神奇的工具是存在的，它被称为**[子空间](@entry_id:150286)嵌入**。

### “看起来相同”意味着什么？[子空间](@entry_id:150286)嵌入性质

当物理学家或数学家说他们想要“保持几何结构”时，他们实际上是说他们想要保持距离的概念。如果你能保持每个向量的长度，你就能自动保持其他一切——角度、面积和体积。这是一个被称为**[极化恒等式](@entry_id:271819)**的数学瑰宝所带来的美妙结果，它将定义角度的[内积](@entry_id:158127)与长度联系起来。

因此，我们对一个保持几何结构的映射（我们称之为$S$）的要求可以归结为：对于任何位于我们特殊$k$维[子空间](@entry_id:150286)$U$中的向量$x$，其影子的长度$\|Sx\|_2$必须与其原始长度$\|x\|_2$几乎完全相同。我们可以用数学语言精确地写出这个要求。我们说$S$是[子空间](@entry_id:150286)$U$的一个**$(1\pm\varepsilon)$[子空间](@entry_id:150286)嵌入**，如果对于$U$中的每一个向量$x$，以下不等式都成立：

$$ (1-\varepsilon)\|x\|_2^2 \le \|Sx\|_2^2 \le (1+\varepsilon)\|x\|_2^2 $$

在这里，微小的数字$\varepsilon$（epsilon）是我们允许的“失真度”。$\varepsilon=0.01$的值意味着影子世界中的所有长度平方的精度都在1%以内。值得注意的是，我们可以将$\varepsilon$选得任意小。[@problem_id:3416518]

这个定义虽然直观，但似乎难以检验。难道我们必须测试[子空间](@entry_id:150286)中的每一个向量吗？幸运的是，不必。线性代数的魔力使我们能够以一种更优雅的方式重新表述这个条件。如果我们为[子空间](@entry_id:150286)$U$取任意一组[标准正交基](@entry_id:147779)向量，并将它们堆叠成一个矩阵$Q \in \mathbb{R}^{n \times k}$，那么[子空间](@entry_id:150286)嵌入性质就完全等价于关于一个$k \times k$矩阵的以下条件：

$$ \|Q^\top S^\top S Q - I_k\|_2 \le \varepsilon $$

这告诉我们，我们的嵌入作用在[子空间](@entry_id:150286)上时，其行为几乎与单位变换完全一样。它几乎什么都没改变。这个简洁的条件是整个理论建立的基石。[@problem_id:3569848]

### 如何构建神奇罗盘：随机性的力量

这一切似乎好得令人难以置信。我们怎么可能构造一个单一的矩阵$S$，能够对*任何*$k$维[子空间](@entry_id:150286)，无论它在广阔的$n$维空间中如何朝向，都能忠实地进行投影？试图确定性地构建这样一个矩阵是徒劳的。

答案，也是现代计算机科学和数学最深刻的洞见之一，简单得惊人：**不要试图耍小聪明，要随机**。

让我们通过简单地用从标准正态（高斯）[分布](@entry_id:182848)中抽取的随机数填充来构建我们的映射矩阵$S \in \mathbb{R}^{m \times n}$，然后将整个[矩阵缩放](@entry_id:751763)$1/\sqrt{m}$。当我们将这个[随机矩阵](@entry_id:269622)应用于我们的[子空间](@entry_id:150286)时，会发生什么？

一个[随机投影](@entry_id:274693)没有偏好的方向。它对我们的[子空间](@entry_id:150286)$U$指向何方是“无视”的。当它投影一个来自$U$的向量$x$时，投影结果意外地落在零点或其长度发生剧烈变化的可能性极小。事实上，由于一个被称为**[测度集中](@entry_id:265372)现象**的美妙现象，我们可以保证，以极高的概率，[子空间](@entry_id:150286)中*所有*向量的长度都同时被保持。这就是著名的**Johnson-Lindenstrauss (JL)引理**及其对[子空间](@entry_id:150286)扩展的核心。

现在是关键部分，也是让整个事业如此强大的部分。我们的影子维度$m$需要多大？有人可能会担心$m$必须依赖于巨大的环境维度$n$。但事实并非如此。所需的维度$m$仅取决于我们[子空间](@entry_id:150286)的内在维度$k$和我们期望的精度$\varepsilon$。一个典型的结果表明，对于一个[高斯随机矩阵](@entry_id:749758)，选择

$$ m \ge C \varepsilon^{-2} \left( k + \log(1/\delta) \right) $$

就足以保证以至少$1- \delta$的成功概率获得一个$(1 \pm \varepsilon)$[子空间](@entry_id:150286)嵌入。[@problem_id:3569848] [@problem_id:3570742] 这就是奇迹所在。我们可以将一个存在于一千万维空间中的一百维[子空间](@entry_id:150286)，几乎确定地压缩到一个仅有几千维的空间中，同时几乎完美地保持其几何结构。环境维度$n$对成本几乎没有影响。

### 超越高斯矩阵的罗盘工具箱

我们的随机高斯罗盘效果很好，但它有一个实际的缺点：矩阵$S$是稠密的（充满了非零数字）。将其应用于一个向量或矩阵的计算成本很高，大约需要$O(mn)$次操作。对于非常大的$n$，这可能仍然太慢。我们能构建一个*更快*的罗盘吗？

是的！这催生了一整套“结构化”[随机投影](@entry_id:274693)的工具箱，它们的应用速度要快得多。

#### [子采样随机哈达玛变换 (SRHT)](@entry_id:755609)

SRHT是一种快速嵌入的秘诀，读起来就像一个魔术。要将其应用于矩阵$A$，你需遵循三个步骤：
1. **[随机化](@entry_id:198186)：** 将$A$的每一列乘以一个随机的$\pm 1$符号。这是[对角矩阵](@entry_id:637782)$D$的工作。这个简单的步骤对于打破数据中任何可能迷惑下一步的预先存在的结构至关重要。[@problem_id:3416505]
2. **混合：** 应用**[快速沃尔什-哈达玛变换](@entry_id:194514)**（矩阵$H$）。这是一种[正交变换](@entry_id:155650)，很像[傅里叶变换](@entry_id:142120)，可以以令人难以置信的速度计算——[时间复杂度](@entry_id:145062)为$O(n \log n)$而不是$O(n^2)$。它彻底地混合了所有坐标上的信息。
3. **采样：** 简单地保留行的一个小的、随机选择的[子集](@entry_id:261956)（矩阵$R$）。

应用这种变换的总成本主要由混合步骤决定，将成本从$O(mn)$降低到$O(n \log n)$。这是一个巨大的计算节省。[@problem_id:3416505]

#### CountSketch

对于**稀疏**（大部分是零）的数据，我们可以做得更好。**CountSketch**算子可能是所有想法中最简单的。想象一下你有$m$个桶。对于你的矩阵$A$的$n$列中的每一列，你随机选择$m$个桶中的一个，抛硬币得到一个$\pm 1$的符号，然后将该带符号的列加到所选的桶中。就是这样。得到的矩阵$SA$就是每个桶中各列的总和。对于一个有$\text{nnz}(A)$个非零项的稀疏矩阵$A$，计算$SA$的成本仅为惊人的$O(\text{nnz}(A))$。[@problem_id:3570706]

当然，没有免费的午餐。这些更快的、结构化的嵌入通常需要一个稍大的影子维度$m$来达到相同的精度$\varepsilon$。例如，SRHT所需的$m$可能需要额外的$\log k$或$\log n$因子，而对于CountSketch，它通常与$k^2$而不是$k$成比例。但在实践中，计算速度的提升是如此巨大，以至于这是一个极好的权衡。[@problem_id:3570706] [@problem_id:3416505]

### 将罗盘付诸实践：解决实际问题

这远非仅仅是数学上的抽象。[子空间](@entry_id:150286)嵌入已经彻底改变了我们解决大规模计算问题的方式。

#### 加速[最小二乘回归](@entry_id:262382)

科学和工程中最常见的任务之一是为一个超定[方程组](@entry_id:193238)找到“最佳拟合”解，这个问题被称为**[最小二乘法](@entry_id:137100)**。我们想找到向量$x$来最小化误差$\|Ax-b\|_2^2$。如果$A$是一个巨大的$n \times p$矩阵（例如，有$n$个数据点，如数十亿，用于一个有$p$个参数的模型），解决这个问题可能会非常慢。

关键的洞见是，在这个问题中我们关心的所有向量——$A$的列向量、向量$b$以及残差$Ax-b$——都存在于一个维度最多为$p+1$的小[子空间](@entry_id:150286)中。那么，我们该怎么做呢？我们用一个[子空间](@entry_id:150286)嵌入$S$来处理整个问题：

$$ \min_{x} \|S(Ax-b)\|_2^2 $$

我们将一个在$\mathbb{R}^n$中的巨大[问题转换](@entry_id:274273)成了在$\mathbb{R}^m$中的一个微小、易于处理的问题。因为$S$保持了相关[子空间](@entry_id:150286)的几何结构，所以小问题的解被证明是原始大问题解的一个极好的近似。[@problem_id:3186049] 这个简单的想法使我们能够以比经典方法快几个[数量级](@entry_id:264888)的速度找到巨大回归问题的近似解。

毫无疑问，嵌入性质是*至关重要*的。如果你使用一个不是有效[子空间](@entry_id:150286)嵌入的“草图”——例如，一个意外地消除了$A$中关键信息部分的投影——那么草图问题的解可能完全是无稽之谈，与真实答案毫无关系。这个性质不仅仅是一个理论上的精妙之处；它是该方法之所以有效的原因。[@problem_id:3570153]

#### 发现数据集之间的关系

想象一下，你有两个不同的数据集，也许是来自系统不同部分的传感器读数。你可以将每个数据集中的基本信息表示为一个[子空间](@entry_id:150286)，比如$\mathcal{R}(A)$和$\mathcal{S}$。一个基本的问题是：这两个[子空间](@entry_id:150286)是如何相关的？它们是重合的，正交的，还是介于两者之间？它们之间的**主夹角**给出了一个精确的答案。

从原始[高维数据](@entry_id:138874)计算这些角度是昂贵的。但同样，嵌入提供了一条捷径。我们可以构造一个[随机投影](@entry_id:274693)$S$，它是两个[子空间](@entry_id:150286)*并集*$\text{span}(\mathcalR(A) \cup \mathcal{S})$的一个[子空间](@entry_id:150286)嵌入。因为这个更大空间的几何结构被保留了，所以草图化[子空间](@entry_id:150286)$S\mathcal{R}(A)$和$S\mathcal{S}$之间的角度几乎与原始角度相同。通过检查低维的影子，我们可以理解原始复杂对象之间的关系。[@problem_id:3571055]

### 拓展边界：稳健性与其他几何

到目前为止，我们的罗盘都是为了保持标准的欧几里得（$\ell_2$）距离而设计的。但是，如果我们的数据被少数但非常大的**异常值**所污染，会发生什么？想象一个传感器瞬间失灵，给出了一个极其错误的读数。

$\ell_2$范数对这类异常值 notoriously 敏感，因为它对误差进行平方。一个大的误差就可以主导整个和，完全破坏[最小二乘拟合](@entry_id:751226)。一个$\ell_2$[子空间](@entry_id:150286)嵌入，由于其忠实性，只会在低维空间中重现这种脆弱的几何结构。草图问题将和原始问题一样不稳健。[@problem_id:3570156]

解决方案要求我们更加精细。我们必须首先改变我们对“距离”的概念，转而使用一种内在稳健的度量，比如$\ell_1$范数（[绝对值](@entry_id:147688)之和）。这引出了**[最小绝对偏差](@entry_id:175855)回归**，它对异常值的敏感度要低得多。然后，我们需要一种新型的罗盘：一个**$\ell_1$[子空间](@entry_id:150286)嵌入**。这是一种专门为保持$\ell_1$几何结构而设计的草图。通过将基于$\ell_1$的目标函数与保持$\ell_1$的草图相结合，我们可以设计出不仅快速、可扩展，而且对恶意损坏也具有稳健性的算法。这阐明了一个深刻而普遍的原则：选择你的草图以匹配你关心的几何结构。[@problem_id:3570156]

这种保持几何结构的思想是现代数据分析中一个强大而统一的主题。虽然我们专注于保持单个[子空间](@entry_id:150286)的几何结构，但在**压缩感知**领域中一个相关的思想是**[限制等距性质 (RIP)](@entry_id:273173)**。RIP保证的不是单个[子空间](@entry_id:150286)的范数保持，而是所有**稀疏向量**集合的范数保持——这是一个由许多、许多[子空间](@entry_id:150286)的并集构成的更复杂的[非线性](@entry_id:637147)对象。这些相关但不同的思想构成了一个强大的工具包，用以驯服维度灾难，使我们能够从巨大而复杂的数据中发现简单的真理。[@problem_id:3416493]

