## 应用与跨学科联系

在掌握了在认知者能力方面对他人造成伤害的原则之后，我们现在可以开始一段旅程，去看看这些理念在实践中的应用。在抽象层面定义一种不公是一回事，而目睹其后果铭刻在我们最关键机构的结构中则完全是另一回事。你可能会惊讶地发现，授予或收回信誉这一微妙行为并非无足轻重的社交失礼，而是一股强大的力量，它塑造着诊所里事关生死的决策，被固化到支配我们未来的算法中，甚至可以改写我们对过去的理解。通过观察这一个强大理念——认知不公——如何在如此多不同领域中显现，我们开始领会到知识、权力与正义交织方式中一种优美而又令人不安的统一性。

### 诊室：证言事关生死之地

也许没有任何地方比医院更能即刻体现“被相信”的重要性了。当一个人报告症状时，他们是在为自己的内心世界提供证言，一个只有他们自己才能独特触及的世界。恰当地接收这份证言是医疗诊断和护理的基石。然而，正是在这些高风险的会面中，证言不公常常以其最赤裸的形式出现。

设想一位原住民患者来到急诊室，描述自己有急性胸痛。临床医生可能受到有害刻板印象的影响，认为某些群体倾向于“寻求药物”或“焦虑”，因而驳回了患者的证言。心脏病发作这种危及生命的可能性被搁置一旁，取而代之的是心理学解释，而一项可能挽救生命的心电图检查也被拒绝了 [@problem_id:4986411]。这不只是一次简单的误诊，而是一种根植于偏见的倾听失败。患者所受的伤害不仅在于得到了糟糕的医疗护理，更在于被当作自身痛苦的不可靠叙述者。当一名有物质使用障碍史的患者描述严重的戒断症状时，他们被贴上“寻求药物”的标签，而不是获得挽救生命的治疗 [@problem_id:4848719]，我们能看到同样悲剧的模式；或者当一名被诊断为精神病的患者报告药物带来了使其衰弱的副作用时，其证言却被斥为“操纵行为”或其疾病的症状 [@problem_id:4747502]。

这就是证言不公：因说话者的身份而被赋予信誉赤字。但有时问题不在于临床医生不相信患者，而在于整个医疗系统缺乏理解其话语所需的概念。这就是它的姊妹概念，*诠释学不公*。想象一个诊所，其框架完全建立在针对成瘾的“唯禁欲”治疗模式上。当患者试图讨论他们对伤害减免的需求——比如清洁针具或过量用药预防咨询——工作人员可能没有共同的语言将这些需求界定为合法的健康目标。患者的请求被误解为“不依从”，因为系统存在概念上的盲点 [@problem_id:4848719]。同样，当像DSM这样的精神评估系统及其相应的电子健康记录（EHR）模板中没有为具有文化或精神意义的体验设置类别时，患者试图用这些术语描述其现实的努力可能会变得无法理解、被病理化或被忽视 [@problem_id:4747502]。

对于那些生活在[多重边](@entry_id:273920)缘化身份交汇点的人来说，这些不公会加剧。对于一位有轻度认知障碍的年长移民女性，关于年龄、认知能力和语言的偏见可能会汇合，造成灾难性后果。当她报告说儿子拿了她的钱时，护士可能会将她的证言当作“神志不清”而打折扣。结构性地缺乏合格的医疗翻译或适用于不同文化的虐待老人筛查工具，意味着她没有获得让自己的经历被理解的资源。证言不公和诠释学不公同时发生，使她处于弱势，并可能压制了向成人保护服务机构提交法定报告的机会 [@problem_id:4859767]。这些不仅是伦理上的失败，还可能产生深远的法律后果，因为一个被此类不公玷污的能力评估可能被裁定为非法，违反了那些预设患者具有行为能力并要求采取一切实际步骤支持其决策的法规 [@problem_id:4473083]。

### 机器中的数字幽灵：人工智能时代的认知不公

人们可能希望，不受人类偏见影响的计算机能提供一条更客观的前进道路。然而，现实是，人工智能常常成为我们试图摆脱的那些不公的强大放大器。原因很简单：AI模型从数据中学习，而临床数据在很大程度上是过去人类决策的化石记录。

让我们回到急诊室。一家医院部署了一款AI工具，以帮助优先决定哪些患者能获得止痛药。该工具使用多年的电子健康记录（EHR）数据进行训练。这些数据包含什么？它包含患者自我报告的疼痛评分，比如0到10分制。但它也包含临床医生的决定：他们是否将疼痛记录为“严重”，以及他们是否真的开了镇痛药。

现在，如果临床医生在历史上一直对来自某个被刻板印象化的群体（$G=1$）的患者施加信誉赤字，那么他们为这些患者开镇痛药的可能性就会系统性地低于为[对照组](@entry_id:188599)（$G=0$）患者开药的可能性，*即使他们报告的疼痛程度完全相同，临床状况也相似*。我们可以通过比较两组患者获得镇痛药的概率来观察这一点，同时保持疼痛报告 $R$ 和其他临床因素 $S$ 不变。如果 $P(A=1 \mid R, S, G=1)  P(A=1 \mid R, S, G=0)$，我们就在数据中发现了证言不公的统计学特征 [@problem_id:4415701]。

当一个AI用这些数据进行训练时，它会将这种不公的模式学成一条规则。它学会了，对于给定的疼痛报告水平，“正确”的输出是对来自群体 $G=1$ 的人推荐治疗的可能性更小。AI本身没有偏见，但它完美地复制了源于偏见的行为。人类的偏见变成了机器中的数字幽灵。

更糟糕的是，这会造成一个有害的反馈循环。临床医生看到AI的建议（其本身已带偏见）并受其影响，使他们更不愿意治疗来自被刻板印象化群体的患者。这个新的、带有偏见的决策被记录在EHR中，并成为下一代AI的训练数据，使其偏见更深 [@problem_id:4421141]。系统变成了一个自我强化的不平等引擎。天真的技术修复手段，比如简单地从模型输入中移除患者的群体身份，是会失败的，因为AI可以轻易地从其他数据点中学到该身份的代理变量。不公并非存在于单个变量中，而是交织在所有变量之间的关系里。

### 重写过去，建设未来：超越诊室

认知不公的长长阴影远远超出了诊室的墙壁和算法的代码。它塑造着我们对过去的理解，并为建设一个更公正的未来指明了方向。

19世纪护士的笔记本与21世纪的AI有何共同之处？两者都是知识的档案库，且都可能被同样的认知力量所扭曲。当一位历史学家研究医学领域中的女性历史时，他们受制于证据记录。如果在一个世纪里，档案管理员和男性医生系统性地贬低女性从业者的证言——只总结她们的笔记，却逐字引用男性的，将她们的临床观察归档为“辅助性”材料——那么历史记录本身就是歪曲的。证言不公实际上篡改了过去，削弱了女性认知者被感知的贡献。同样，如果那个时代的医学词汇中缺乏产后抑郁症的概念，那么女性描述其痛苦的信件可能会被归档于“家庭琐事”之下。这种诠释学上的空白使得一整类经验对未来的历史学家变得不可见，在知识本应存在的地方制造了档案的沉默 [@problem_id:4773298]。

那么该怎么办呢？如果这些不公如此深地嵌入我们的系统和实践中，我们该如何反击？事实证明，答案不仅仅是告诉个人要“更好地倾听”。答案是重新设计那些决定谁能发言、谁被相信、以及谁能参与创造我们用以理解世界的概念的系统。

一种强有力的方法来自研究领域，称为社区参与式研究（Community-Based Participatory Research, CBPR）。想象一个团队试图为一个多元化的儿科诊所设计一个关于健康社会决定因素（如食物或住房不安全）的筛查工具。一项[试点研究](@entry_id:172791)表明，该工具对讲西班牙语的看护者效果不佳。CBPR并非让“专家”远程修复问题，而是将看护者作为平等的合作伙伴请到谈判桌前。通过组建一个有报酬且共同治理的看护者顾问委员会，团队可以共同创建一个使用具有本地意义语言的工具，从而解决诠释学不公。通过授予看护者定义和衡量自身经历的认知权威，这一过程直接对抗了证言不公。其结果不仅是一个更公正的过程，也是一个更具科学有效性的工具 [@problem_id:5206084]。

这个原则可以推广到医院政策层面。要真正重塑专业知识的伦理，我们必须使其民主化。想象一个临床政策委员会，不再仅仅由临床医生组成，而是一个“共同生产”委员会，其中来自边缘化社区的患者代表拥有平等的投票权。想象一个系统，其中决策背后的推理是公开的，社区有正式的渠道，可以用自己的生活经验作为证据来挑战政策 [@problem_id:4866459]。这不仅仅是咨询，这是权力的根本性再分配。这是承认患者不仅是护理的对象，更是自身领域的专家。

从对单个患者证言的悄然漠视，到我们[体制](@entry_id:273290)的结构性偏见，我们看到了一个共同的线索。正义不仅在于我们如何分配资源，还在于我们如何分配信誉和理解。要建设一个更健康、更公平的世界，我们必须成为更好的认知系统架构师，确保每个声音不仅有权发声，更有权被真正听到。