## 应用与跨学科联系

我们已经看到，[神经网络](@article_id:305336)本质上是一个具有巨大灵活性的数学函数，其特性由其可调旋钮的集合——[权重和偏置](@article_id:639384)——来定义。学习过程就是不断调整这些旋钮，直到函数能完成我们[期望](@article_id:311378)的任务。这是一个简单而深刻的想法。但具体来说，我们能让这些函数*做*什么呢？答案原来是惊人地广泛。通过调整这些简单的数值参数，我们解锁了一个正在重塑整个科学和工程学科的工具箱。让我们通过一些应用来一探究竟，感受一下这些[权重和偏置](@article_id:639384)中蕴含的真正力量。

### 数字工匠：驾驭复杂函数

现实世界中的许多现象都是杂乱的。它们受制于复杂的、非线性的关系，无法用简单、优雅的方程来描述。想想机器人关节内部的摩擦。我们可以写出一个简单的[线性模型](@article_id:357202)，但真实的行为——刚开始移动时的“粘滞性”（[静摩擦](@article_id:379964)）与运动起来后的平滑阻力之间的差异——是出了名的难以完美捕捉。

在这里，神经网络可以扮演“数字工匠”的角色，直接从数据中学习系统的“感觉”。通过向一个简单的网络输入关节的速度并测量产生的摩擦力，我们可以训练它来逼近这种复杂的关系。网络的[权重和偏置](@article_id:639384)不断调整，直到其输出能够在所有速度下忠实地模仿真实的摩擦力 ([@problem_id:1595336])。最终的权重集合并不像方程那样代表一个关于摩擦的物理理论；相反，它本身就是对*行为*的数值编码，是对一个复杂函数的实践性掌握。

同样的原则也适用于无数其他问题。考虑一个用于关键机器的[预测性维护](@article_id:347079)系统，比如装配线上的一个机器人执行器 ([@problem_id:1595339])。通过监测电机电流和温度的传感器，可以训练一个[神经网络](@article_id:305336)来预测即将发生故障的概率。网络学习到传感器读数之间微妙的、非线性的相关性，这些相关性是故障的前兆——这些模式对于人类操作员或简单的基于阈值的警报系统可能是不可见的。学习到的[权重和偏置](@article_id:639384)体现了“故障函数”，这是防止代价高昂的停机时间的重要知识。

### 新的伙伴关系：物理学与机器学习的结合

虽然[神经网络](@article_id:305336)本身就是强大的[函数逼近](@article_id:301770)器，但当它们与既有的科学知识结合使用时，或许才最具革命性。我们不必抛弃几个世纪以来积累的物理学知识；我们可以对其进行增强。

这就引出了**灰箱建模**这一优美的概念。想象我们有一个直流电机。我们有一个来自物理学的非常好的“白箱”模型来描述其行为：一组关联电流、电压和转动的线性方程。然而，这个模型并不完美。它忽略了诸如齿槽转矩和复杂摩擦等非线性效应。我们可以用一个[神经网络](@article_id:305336)作为“黑箱”来为*整个*电机建模，但这将是一种浪费——我们会迫使它重新发现我们已知的线性物理规律。灰箱方法是一种综合：我们使用我们信赖的物理模型来处理大部分动态，并附加一个小型神经网络，其唯一的工作就是学习我们的模型所忽略的那些杂乱的、非线性的部分 ([@problem_id:1595291])。网络的权重被训练来仅预测物理模型的*误差*。这种协同作用是强大的：物理学提供了一个坚实的基础，而网络则提供了高保真模拟所需的灵活、数据驱动的修正。

我们可以将这种合作关系推得更远，用网络来求解物理学的基本方程本身。这就是**物理信息神经网络（[PINNs](@article_id:305653)）**的领域。一个 PINN 可以被优雅地理解为对一种称为[配置法](@article_id:299333)（collocation method）的经典数值技术的现代改造 ([@problem_id:3214094])。在传统方法中，人们可能会通过组合一些固定的“基函数”（如正弦和余弦函数）来逼近[微分方程](@article_id:327891)的解。相比之下，神经网络提供了一个由其架构定义的、几乎无限灵活的试验函数族。网络的输出 $u_{\theta}(x, t)$ 是空间和时间坐标的函数，其形状由参数 $\theta$ 决定。PINN 的魔力在于其[损失函数](@article_id:638865)：我们不仅要求网络[匹配数](@article_id:337870)据，还要求其输出*满足[微分方程](@article_id:327891)本身*。利用[自动微分](@article_id:304940)，我们可以计算 $u_{\theta}$ 对其输入（$x$ 和 $t$）的[导数](@article_id:318324)，并将它们直接代入[偏微分方程](@article_id:301773)（PDE）。然后，训练过程调整[权重和偏置](@article_id:639384) $\theta$，以最小化方程的“[残差](@article_id:348682)”，从而有效地迫使网络发现一个遵守物理定律的函数。

此外，一个训练好的 PINN 中编码的知识是可迁移的。如果我们花费大量的计算资源训练一个网络来求解具有特定粘度 $\nu_1$ 的[流体流动](@article_id:379727)的[伯格斯方程](@article_id:323487)（Burgers' equation），得到的权重 $\theta_1$ 代表了对解的结构的深刻理解。如果之后我们需要解决具有稍有不同粘度 $\nu_2$ 的同一问题，我们就不需要从头开始。将 $\theta_1$ 作为新训练过程的初始猜测——这是一种[迁移学习](@article_id:357432)的形式——会给优化器一个巨大的领先优势。这就像请一位水流专家来猜测蜂蜜的流动；他们的直觉已经接近正确答案，收敛速度会显著加快 ([@problem_id:2126311])。

### 从像素到蛋白质：学习感知与结构

世界不仅仅是由[连续函数](@article_id:297812)构成的；它也充满了高维度的、结构化的数据。在这里，[权重和偏置](@article_id:639384)同样提供了学习的手段。

最著名的例子是[计算机视觉](@article_id:298749)。**[卷积神经网络](@article_id:357845)（CNN）**是一种专门为处理像图像这样的网格状数据而设计的架构。对于像引导一个循线机器人这样的任务，CNN 输入相机图像并输出一个转向指令 ([@problem_id:1595341])。它的力量来自于其卷积滤波器层，这些滤波器本质上是小的权重矩阵。通过训练，这些滤波器学会成为[特征检测](@article_id:329562)器。最初的几层可能学会检测简单的边缘和角落。后面的层则将这些组合起来，以检测更复杂的形状，比如机器人应该遵循的线条。一个现代 CNN 中庞大的[权重和偏置](@article_id:639384)数量，是其[表达能力](@article_id:310282)的一种度量——即它学习大量视觉模式层次结构的能力。

这就引出了现代人工智能中最实用的技术之一：**[迁移学习](@article_id:357432)**。从头开始训练一个大型 CNN 需要巨大的数据集和计算能力。但我们并非总是需要这样做。一个在数百万张互联网照片上训练过的网络，已经在其权重中学会了丰富的视觉特征词汇。对于一个专门的科学任务，比如在[电子显微镜](@article_id:322064)图像中分类不同的[细胞器](@article_id:314982)，我们可以借用这个[预训练](@article_id:638349)好的网络 ([@problem_id:1423370])。我们“冻结”其绝大部分的权重——保持其强大的[特征提取](@article_id:343777)能力不变——然后简单地替换并重新训练最后几层。这使我们能够用少得多的数据和计算资源，将一个强大的模型应用于新领域，使[深度学习](@article_id:302462)成为专门科学领域的可行工具。

学习的原则并不仅限于网格结构。**[图神经网络](@article_id:297304)（GNNs）**将这些思想扩展到任意的网络结构，例如社交网络、分子图，或系统生物学中研究的[蛋白质-蛋白质相互作用](@article_id:335218)（PPI）网络。在 GNN 中，权重被训练来定义一个规则，该规则规定了每个节点（例如，一个蛋白质）应如何通过聚合图中邻居的信息来更新其[特征向量](@article_id:312227)。通过堆叠这些层，网络学会了在整个[生物网络](@article_id:331436)中传递信息。这使其能够做出依赖于所有组件复杂相互作用的预测，例如根据蛋白质活性模式对细胞表型进行分类 ([@problem_id:1436674])。此外，我们可以将先验的生物学知识构建到架构本身中。一个分层的 GNN 可能首先学习已知蛋白质复合物（小的、[紧密连接](@article_id:349689)的子图）的表示，然后学习这些复合物如何相互作用。这样的设计通常会产生一个参数更少、更高效的模型，这证明了一个强大的原则：由领域知识指导的良好架构选择，可以带来更好、更高效的学习。

### 更广阔的图景：复杂性、鲁棒性与未来

随着这些工具融入科学领域，我们也必须理解它们的更广泛属性。其中最关键的一个是计算成本。神经网络正在取代复杂科学模拟的某些部分，例如在分子动力学中，它们可以比传统的[量子化学](@article_id:300637)方法快得多地逼近分子的势能。它们成功的一个关键原因是计算梯度的效率。每个原子上的力就是势能对其位置的负梯度。使用[反向模式自动微分](@article_id:638822)（与[反向传播](@article_id:302452)是相同的[算法](@article_id:331821)），计算*所有* $3N$ 个原子坐标上的力的成本，仅比计算单个能量值的成本高出一个很小的常数倍。总计算成本与权重数量 $W$ 呈线性关系，即 $O(W)$，而不是与原子数量成正比，这使得它对于大型系统来说效率极高 ([@problem_id:2372991])。

最后，使我们能够找到正确权重的优化能力本身，也可能被反过来利用，以揭示模型的弱点。在**对抗性样本**现象中，一个优化算法可以被用来寻找对输入图像的一个微小的、人类无法察觉的扰动 $\mathbf{\delta}$，这个扰动会导致一个训练有素的网络完全错误地分类该图像。这被表述为一个优化问题，其中网络的权重 $\mathbf{W}$ 和原始图像 $\mathbf{x}_{\text{orig}}$ 是*固定参数*，而扰动 $\mathbf{\delta}$ 是我们要求解的*[决策变量](@article_id:346156)* ([@problem_id:2165346])。这揭示了网络学到的高维函数可能是脆弱和反直觉的，突显了[人工智能安全](@article_id:640281)和鲁棒性研究的一个关键前沿。

从电机的细微行为到物理学的基本定律，再到错综复杂的生命之网，神经网络中不起眼的[权重和偏置](@article_id:639384)为编码功能性知识提供了一种统一的语言。它们是我们可以塑造解决方案的黏土，是数据可以描绘其模式的画布，也是在现代科学版图上连接理论与观察的桥梁。探索之旅才刚刚开始。