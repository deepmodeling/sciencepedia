## 应用与跨学科联系

在我们之前的讨论中，我们揭示了基于势能的[奖励塑造](@article_id:638250)这一优雅原则。这是一种卓越的想法：一种给予人工智能体有益“提示”以加速其学习的方法，就像一位好老师引导学生一样。正如我们所见，真正的魔力在于，这些提示的构造方式非常特殊，它们永远不会改变智能体所寻求的最终“正确答案”。它们使旅程变短，但目的地保持不变。

这是一套优美的理论。但理论的价值在于它能让我们多大程度上理解世界。那么，这个巧妙的想法究竟出现在哪里？我们可以在哪里使用它？答案是美妙的：它无处不在。从可移动机器人的有形世界到科学发现的抽象领域，甚至[经典计算](@article_id:297419)机[算法](@article_id:331821)隐藏的数学原理中，这个单一、优雅的原则都提供了一个强大的工具。现在让我们来巡览这些应用，并在此过程中体会这个思想深邃的统一性。

### 运动的世界：从机器人到原子

也许最直观的起点是会移动的物体。想象一个简单的机器人，任务是把一个方塊推到网格上的指定目标位置 [@problem_id:3145250]。它如何学习？我们可以给它一个巨大的奖励，一份奖品，但只有在它最终成功时才给。这是一种“稀疏”奖励，它使得学习极其困难。这就像让一个人闭着眼睛在广阔的海滩上寻找一粒特定的沙子，并且只在最后告诉他“你找到了”。他们会永无止境地漫无目的徘徊！

[奖励塑造](@article_id:638250)为这个迷失的机器人提供了一张地图。我们可以定义一个“势函数” $\Phi(s)$，它代表一个状态 $s$ 的前景如何。对于机器人来说，一个自然的选择是与方块离目标的距离有关。假设我们让势能在方塊更接近目的地时更高。通过添加塑造奖励 $F(s, a, s') = \gamma \Phi(s') - \Phi(s)$，我们每次在它采取一个*增加*势能的步骤时——也就是每次它把方块移近目标时——都给它一个小奖励。它现在有了一个向导，一种“冷”或“热”的感觉，帮助它有效地导航巨大的可能性空间。它不再靠盲目的运气学习，而是通过遵循我们有益[势能的梯度](@article_id:352233)来学习。

同样的想法可以扩展到极其复杂的问题上。考虑操作[原子力显微镜](@article_id:342830)（AFM）的精细任务 [@problem_id:2777676]。AFM 使用一个微小的悬臂来“感觉”材料的表面，一个原子一个原子地进行。目标是尽快创建高分辨率图像，但有一个问题：如果你推得太用力，你就会损坏你试图观察的样品。这是在速度与安全之间进行的一场高风险的平衡博弈。

在这里，我们同样可以为强化学习智能体构建这个问题。“外在”奖励是针对快速准确的扫描。但我们如何引导它保持轻柔？我们可以使用[奖励塑造](@article_id:638250)。悬臂在偏转时会储存[弹性势能](@article_id:343666)，由 $\frac{1}{2}kd^2$ 给出。我们可以将我们的塑造势能 $\Phi$ 定义为这个储存能量的负值。塑造项 $\gamma \Phi(s') - \Phi(s)$ 随后会奖励那些导致悬臂储存能量*减少*的动作——也就是那些使悬臂松弛并减少样品上作用力的动作。智能体学会了在表面上“摸索”前进，由一个直接植根于物理学的原则引导，而从未妥协其创建良好图像的主要目标。

然而，有时温和的指导是不够的。对于安全关键系统，如工业机器人或自动驾驶汽车，我们需要的不仅仅是“提示”；我们需要硬性的保证。[奖励塑造](@article_id:638250)鼓励智能体安全行事，但学习中的智能体，就其本质而言，会进行探索。而探索有时可能导致危险的动作。这就是[奖励塑造](@article_id:638250)与经典控制理论思想相结合的地方 [@problem id:2738649]。在这些系统中，可以实现一个“安全滤波器”。这个滤波器了解系统的物理原理，并且可以为任何给定状态计算出一组可证明安全的动作。如果学习智能体提出了一个超出这个安[全集](@article_id:327907)的动作，滤波器就会介入并将其投影回最近的安全动作。

这创造了一种美妙的协同作用：RL 智能体在一个精心设计的塑造奖励的引导下，可以自由地创造性探索并找到高效的策略。与此同时，安全滤波器扮演着一个警惕的监督者角色，确保这种创造性探索永远不会导致灾难。智能体学会了既聪明又明智。

### 抽象景观：从分子到市场

[奖励塑造](@article_id:638250)的力量并不局限于物理空间。“状态”可以是任何东西：分子的当前构型、科学理论中已知事实的集合，或[金融市场](@article_id:303273)的状况。

考虑设计新药或合成DNA序列的挑战 [@problem_id:2749103]。可能的序列数量是天文数字。我们可以把一次构建一个组件的序列看作是在一个巨大的抽象景观中旅行。最终的奖励，即“奖品”，只有在最后，当我们有一个可以测试其生物功能的完整序列时才会给出。这又是一个稀疏奖励问题。为了引导搜索，我们可以在*部分*序列上定义一个[势函数](@article_id:332364) $\Phi(s)$，代表该前缀导致成功最终产品的可能性估计。通过使用基于势能的形式 $\tilde{r}_t = r_t^{\mathrm{base}} + \gamma \Phi(s_t) - \Phi(s_{t-1})$，我们可以提供引导构建过程的中间奖励，而不会意外地偏向于让智能体创造一个次优的最终序列。任何其他形式的中间奖励——例如直接奖励合理的 prefixes 或添加特设的 penalties——都有改变目标并引导智能体走上歧途的风险。基于势能的结构是提供提示同时保证最终目标完整性的*唯一*方法。

这种使用先验知识的概念延伸到了科学过程本身 [@problem_id:3186213]。想象一个 RL 智能体，其动作是提出和检验科学假设。它的目标是找到一个能够解释实验数据的理论。我们可以给它一个塑造奖励，用于奖励它提出的符合基本物理定律（如[能量守恒](@article_id:300957)）的假设。这里的[势函数](@article_id:332364) $\Phi$ 代表一个假设的“物理合理性”。天真地惩罚任何违反这些定律的行为可能会扼杀创造力，因为一些伟大的理论需要暂时挑战既定规范。但微妙的基于势能的公式鼓励尊重已知定律，而不禁止探索激进的新思想，完美地反映了人类科学进步中的微妙平衡。

同样的原则甚至适用于看起来截然不同的金融和经济世界 [@problem_id:2426686]。交易智能体的首要目标是最大化利润。然而，它也需要探索，去了解它从未见过的不同“市场机制”。我们可以给它一个“内在奖励”以激发好奇心——一个访问陌生状态的奖励。但我们如何能确定这种好奇心不会变成一种分心，让智能体为了新奇而寻求新奇，而不是为了利润？答案再次是基于势能的[奖励塑造](@article_id:638250)。如果好奇心奖励的结构是 $r_{\mathrm{int}}(s,a,s') = \gamma \Phi(s') - \Phi(s)$，其中 $\Phi(s)$ 是衡量一个状态新奇程度的指标，那么智能体就会被鼓励去探索，而永远不会忽视其最终的财务目标。

### 惊人的统一性：与经典[算法](@article_id:331821)的深刻联系

我们已经看到了[奖励塑造](@article_id:638250)在[机器人学](@article_id:311041)、纳米科学、生物学和金融领域的应用。它感觉像一个非常 modern 的想法，诞生于最近机器学习的爆炸式发展。但最美妙的启示是，其数学核心要古老得多，并且位于一个完全不同的领域：经典[算法](@article_id:331821)理论。

在20世纪70年代，计算机科学家们关心一个基本问题：在一个网络或图中找到所有点对之间的[最短路径](@article_id:317973) [@problem_id:3242553]。Dijkstra 的著名[算法](@article_id:331821)可以非常有效地做到这一点，但它有一个严格的要求：网络中所有边的“成本”（或权重）必须是非负的。如果某些边有负成本怎么办？

Donald B. Johnson 找到了一个绝妙的解决方案。他的[算法](@article_id:331821)首先对所有边的成本进行巧妙的“重加权”，使其变为非负，同时不改变哪条路径是最短的。他为网络中的每个节点 $v$ 分配了一个“势” $h(v)$。从节点 $u$ 到节点 $v$ 的一条边的新权重被定义为：
$$ w'(u,v) = w(u,v) + h(u) - h(v) $$
如果你追踪任何从起始节点到结束节点的路径的总成本，你会发现新的总成本仅仅是旧的总成本加上一个只依赖于起始和结束节点的常数。这就是为什么[最短路径](@article_id:317973)仍然是[最短路径](@article_id:317973)的原因。

这个公式看起来熟悉吗？要看清它与[奖励塑造](@article_id:638250)的联系，必须先将其从成本语言转换为奖励语言。在[强化学习](@article_id:301586)中，我们通常考虑最大化奖励，而在[最短路径问题](@article_id:336872)中，我们最小化成本。让我们将奖励定义为成本的负值，$r(u,v) = -w(u,v)$。塑造后的奖励将是 $r'(u,v) = -w'(u,v)$。将此代入 Johnson 的重加权方程得到：
$$ -r'(u,v) = -r(u,v) + h(u) - h(v) $$
$$ r'(u,v) = r(u,v) + h(v) - h(u) $$
项 $h(v) - h(u)$在数学上与基于势能的塑造项 $F(s, a, s') = \gamma \Phi(s') - \Phi(s)$ 是相同的，对应于未来价值不随时间衰减的特殊情况（无折扣问题，其中 $\gamma=1$）。

这是一个深刻的发现。现代强化学习中的“[势函数](@article_id:332364)” $\Phi$ 和半个世纪前发展的经典图[算法](@article_id:331821)中的“势” $h$ 在数学上是完全相同的概念。一个用于引导智能体的原则和一个用于解决计算机科学基本问题的技巧，是同一枚美丽硬币的两面。这是科学和数学思想统一性的一个惊人例子，提醒我们一个好主意，无论我们在哪里发现它，永远都是一个好主意。