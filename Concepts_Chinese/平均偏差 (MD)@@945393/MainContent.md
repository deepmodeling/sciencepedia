## 引言
在分析一组数据时，我们通常从“平均值”开始寻找其中心，但这只是故事的一半。要真正理解一个数据集，我们还必须衡量其离散程度或分散性。虽然标准差是完成此任务最著名的工具，但其平方差的方法使其对异常值高度敏感，并且可能不总是最直观的度量。这就提出了一个基本问题：是否存在一种更直接、更稳健的方法来量化“与中心的平均距离”？

本文深入探讨了平均偏差（MD），或称平均[绝对偏差](@entry_id:265592)，它是对这个问题的一个优雅而有力的回答。通过简单地对与中心值的绝对距离进行平均，MD 提供了一种直接且具有弹性的[离散度量](@entry_id:154658)。在接下来的章节中，您将发现这个重要统计工具的核心概念。第一章“原理与机制”将阐释 MD 的数学基础，将其与标准差直接比较，并揭示其与[中位数](@entry_id:264877)和[拉普拉斯分布](@entry_id:266437)之间深刻而自然的联系。随后，“应用与跨学科联系”一章将展示 MD 非凡的多功能性，探索其在混乱的现实世界数据中作为一种稳健的度量、作为混沌和复杂系统的描述符、作为科学模型的验证器，甚至在医学科学语言中作为一个警示故事的来源。

## 原理与机制

### 一个简单的想法：平均距离有多远？

我们如何描述一群人？我们可能会从“平均”的人开始。但这只是故事的一半。这群人是五年级学生，大家身高都差不多吗？还是说这是城市街道，混杂着蹒跚学步的幼儿、篮球运动员以及介于两者之间的各色人等？两种情况下的“平均身高”可能相同，但群体的特征却截然不同。这种差异的本质是*[离散度](@entry_id:168823)*，或*分散性*。

在科学和统计学中，当我们观察数据或概率分布时，我们面临同样的问题。我们通常从寻找一个中心点开始，这像是我们数字的[重心](@entry_id:273519)。让我们将这个中心称为**平均数**，或[期望值](@entry_id:150961)，并用希腊字母 $\mu$ 标记它。

一个自然的下一步是问：“平均而言，任何给定点离这个中心有多远？” 对于任意单点 $x$，其距离或偏差就是 $x - \mu$。那么，为什么不直接对所有这些偏差求平均呢？在这里，我们遇到了第一个优雅的小障碍。根据平均数作为重心的定义，正偏差和负偏差会完美地相互抵消。所有偏差的平均值 $E[X - \mu]$ 总是令人沮丧地为零。平均偏差为零并不能告诉我们关于[离散度](@entry_id:168823)的任何信息。

为了解决这个问题，我们必须去掉负号。关于如何做到这一点，主要有两种思路，它们引向了两个不同但相关的统计学世界。

第一种，也是迄今为止更著名的方法，是将偏差平方。负数平方后变为正数。然后我们可以对这些平方偏差 $(X-\mu)^2$ 进行平均，得到一个称为**方差**的量，记为 $\sigma^2$。方差的平方根 $\sigma$ 就是著名的**标准差**。这条路径在数学上很方便，并引出了美丽的**正态分布**理论。

但还有一种更简单、更直接的方法。为什么不直接取偏差的**绝对值**呢？绝对值 $|X - \mu|$ 衡量的是与中心的原始距离，忽略了方向。如果我们对*这个*量进行平均，我们就得到了所谓的**平均[绝对偏差](@entry_id:265592)**（或简称平均偏差），我们将其表示为 MD。
$$
\text{MD} = E[|X - \mu|]
$$
这个量正是我们直觉最初所要求的：与平均数的平均距离。这是一种简单、稳健且非常直接地讨论[离散度](@entry_id:168823)的方式。让我们来探索它的特性。

### 分布巡礼

计算平均偏差就像衡量不同“个性”的尺度。每个概率分布都有其特有的[离散度](@entry_id:168823)，而 MD 为我们提供了一个描述它的数字。

让我们从最简单的随机事件开始：一次硬币投掷。这就是**伯努利分布**，其中我们有概率 $p$ 得到“成功”（我们称之为 $X=1$），有概率 $1-p$ 得到“失败”（$X=0$）。该分布的平均数就是 $\mu=p$。那么平均偏差是多少呢？我们将每个结果的[绝对偏差](@entry_id:265592)按其概率加权求和 [@problem_id:713]：
$$
\text{MD} = |1-p| \cdot p + |0-p| \cdot (1-p)
$$
由于 $p$ 是一个概率，其值在 0 和 1 之间，所以 $|1-p|$ 就是 $1-p$，而 $|-p|$ 就是 $p$。表达式可以漂亮地简化为：
$$
\text{MD} = (1-p)p + p(1-p) = 2p(1-p)
$$
这个结果令人愉快。项 $p(1-p)$ 是伯努利分布的方差。平均偏差恰好是方差的两倍！请注意，如果 $p=0$ 或 $p=1$（当结果确定时，没有[离散度](@entry_id:168823)），此函数为零；当 $p=0.5$（一枚公平的硬币）时，它达到最大值，而这恰好是我们对结果不确定性最大的时候。

现在，让我们进入连续世界。想象一个[随机数生成器](@entry_id:754049)，它以同等可能性从 $-L$ 到 $L$ 中选取一个数。这就是**均匀分布**。平均数显然是 $\mu=0$，正好在中间。平均偏差是与 0 的平均距离 [@problem_id:6661]。计算涉及一个简单的积分，我们可以将其看作是所有可能距离的总和并求平均。结果是 MD = $\frac{L}{2}$。这在直觉上很有道理：点均匀地分布在 $-L$ 到 $L$ 之间，因此它们到中心的平均距离是最大距离的一半。如果我们将区间移至从 $0$ 到 $L$，平均数会移动到中心 $\mu = L/2$。此时平均偏差变为 $L/4$ [@problem_id:3214]。

生活很少是均匀的。让我们考虑**[指数分布](@entry_id:273894)**，它通常描述随机事件的等待时间，比如放射性原子的衰变。它本质上是不对称的；短的等待时间很常见，而长的等待时间则很罕见。其平均数为 $\mu$。计算 MD 需要更多工作，需要将积分分成两部分：平均数以下的区域和以上的区域 [@problem_id:7498]。答案是一个引人注目的公式：
$$
\text{MD} = 2\mu e^{-1} \approx 0.736\mu
$$
平均偏差是平均数本身的一个固定分数。

最后，我们必须拜访所有分布之王：**正态（或高斯）分布**。它是我们熟悉的[钟形曲线](@entry_id:150817)，无处不在，从人类身高到测量误差。对于一个平均数为 $\mu$、标准差为 $\sigma$ 的正态分布，其平均偏差与标准差成正比 [@problem_id:13225]：
$$
\text{MD} = \sigma\sqrt{\frac{2}{\pi}} \approx 0.798\sigma
$$
对于任何钟形曲线，无论其宽窄，一个点到中心的平均距离总是标准差的约 80%。这种固定的关系非常有用，当我们知道数据服从正态分布时，它使我们能够在这两种[离散度量](@entry_id:154658)之间轻松切换。

### 两种度量的故事：MD vs. SD

我们现在有两种[离散度量](@entry_id:154658)：平均偏差（MD）和标准差（SD 或 $\sigma$）。它们显然是相关的，但关系如何？一个比另一个更好吗？

“更好”这个问题具有误导性。它们是不同的工具，各自对数据的不同特征敏感。它们的基本关系被一个简单、普适的不等式所捕获：对于任何分布，平均偏差总是小于或等于标准差 [@problem_id:1313460]。
$$
\text{MD} \le \sigma
$$
这不是巧合；这是一个数学上的必然，源于一个深刻的结果，即柯西-[施瓦茨不等式](@entry_id:202153)（Cauchy-Schwarz inequality）。直观地，你可以这样想：通过对偏差进行平方，标准差对远离平均数（异常值）的点施加了重得多的惩罚。一个距离平均数 10 个单位的异常值对 MD 的总和贡献为 $10$，但对方差的总和贡献为 $10^2 = 100$。这种对大偏差的额外权重倾向于将 $\sigma$ 的最[终值](@entry_id:141018)拉高，使其大于权重更“民主”的 MD。

那么它们何时可以相等呢？等式成立的条件异常严格 [@problem_id:1934685]。MD 只有在每个数据点与平均数的*[绝对偏差](@entry_id:265592)完全相同*时才能等于 $\sigma$。这只可能在两种情况下发生：要么所有点都相同（一个无聊的情况，此时 MD 和 $\sigma$ 都为零），要么数据完全聚集在两个值上，比如 $\mu-c$ 和 $\mu+c$，且每个值上的点数相等。例如，数据集 $\{10, 20\}$ 或 $\{5, 5, 15, 15\}$ 就满足这个条件。对于任何更复杂或“自然”的分布，标准差将总是严格大于平均偏差。

### 真正的中心

我们一直在从平均数 $\mu$ 计算偏差。当我们的目标是标准差时，这是很自然的，因为平均数正是使平均*平方*误差 $E[(X-c)^2]$ 最小化的值 $c$。

但我们现在处于绝对值的世界。所以我们必须问一个不同的问题：哪个点 $c$ 能使*平均绝对*误差 $E[|X-c|]$ 最小化？答案不是平均数，而是**中位数**。

[中位数](@entry_id:264877)是能将分布完美地一分为二的值——50% 的值在它之下，50% 在它之上。为什么这使其成为[绝对偏差](@entry_id:265592)的最优中心呢？可以这样想：想象你和几个朋友在一条数轴上的不同位置。你想选择一个集合点，使每个人的总步行距离最小。如果你站在一个不是中位数的点上，那么你一边的朋友会比另一边多。通过向人多的一边移动一步，你靠近的人数会多于你远离的人数。所以你的总行进距离会减少。你只有在到达[中位数](@entry_id:264877)时才会停止取得进展，因为此时你两边的朋友数量相等。

这个深刻的结果可以通过对函数 $L(t) = E[|X-t|]$ 求导并找到其导数为零的点来正式证明 [@problem_id:566013]。最小值恰好出现在[累积分布函数](@entry_id:143135)为 $0.5$ 的地方，这正是[中位数](@entry_id:264877)的定义。

这个性质使得平均[绝对偏差](@entry_id:265592)（当从中位数计算时）成为一种**稳健**的统计量。平均数可以被一个极端的异常值拖来拖去，但[中位数](@entry_id:264877)却不受影响。考虑一个登录失败的数据集：$\{5, 7, 9, 11, 18\}$ [@problem_id:1934702]。中位数是 9。平均数被异常值 18 拉高到 10。如果我们分别从两者计算平均[绝对偏差](@entry_id:265592)，我们会发现从[中位数](@entry_id:264877)计算的偏差（3.4）确实小于从平均数计算的偏差（3.6）。如果我们使用的[距离度量](@entry_id:636073)是绝对值，那么中位数就是真正的“中心”。

### 平均偏差的自然归宿

我们说过，正态分布是方差和标准差的自然归宿。这背后有来[自信息](@entry_id:262050)论的深刻原因。**最大熵原理**告诉我们，如果我们对一个随机变量所知的只是某些平均值（如其平均数和方差），那么与这些知识相匹配的最无偏、“最随机”的分布就是熵最高的那个分布。对于固定的平均数和方差，该分布就是正态分布。

这就提出了一个优美的问题：如果我们不知道方差，而只知道平均[绝对偏差](@entry_id:265592)，那会怎样？对于一个我们约束了其与中心平均距离 $E[|X-\mu|] = \alpha$ 的系统，其“最真实”的分布是什么？

当我们用这个新约束来转动[最大熵](@entry_id:156648)的“曲柄”时，正态分布并未出现。取而代之的是一个不同而优美的函数：**[拉普拉斯分布](@entry_id:266437)** [@problem_id:1613677]。其[概率密度函数](@entry_id:140610)如下：
$$
p(x) = \frac{1}{2\alpha} \exp\left(-\frac{|x-\mu|}{\alpha}\right)
$$
这种对应关系令人惊叹。正态分布的概率密度函数（PDF）的指数中包含 $(x-\mu)^2$，将其与方差联系在一起。[拉普拉斯分布](@entry_id:266437)的 PDF 的指数中则包含 $|x-\mu|$，将其直接与平均[绝对偏差](@entry_id:265592)联系在一起。

为了完美地闭合这个循环，如果我们取一个平均数为 $\mu$、[尺度参数](@entry_id:268705)为 $b$ 的拉普拉斯分布，并计算其关于平均数的平均[绝对偏差](@entry_id:265592)，答案就是 $b$ [@problem_id:1400057]。定义该分布[离散度](@entry_id:168823)的参数*就是*其平均[绝对偏差](@entry_id:265592)。拉普拉斯分布之于平均偏差，正如正态分布之于标准差。它是它的自然归宿，是其最基本的形式，是当随机性被“平均距离”这个简单直观的概念所约束时呈现的形状。

