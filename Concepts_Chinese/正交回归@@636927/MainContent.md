## 引言
当面对离散的数据点时，我们直觉上会寻找一条能够捕捉其潜在趋势的唯一“最佳拟合”直线。完成这项任务最常用的工具是[普通最小二乘法](@entry_id:137121)（OLS）回归，其原理是最小化每个数据点到直线的[垂直距离](@entry_id:176279)。然而，这个无处不在的方法依赖于一个关键但往往存在缺陷的假设：所有[测量误差](@entry_id:270998)仅存在于垂直（y）变量中，而水平（x）变量是完全已知的。当这个假设不成立时会发生什么呢？在无数真实世界的场景中，从比较两个不完美的温度计到追踪一颗小行星的位置，两个变量都存在不确定性，而 OLS 会提供一个有偏且不尽人意的答案。

本文将介绍一种更具原则性且在几何上更稳健的替代方法：正交回归。它通过对称地处理 x 和 y，解决了“变量误差”问题，从而为隐藏在含噪数据中的真实关系提供了更稳健的估计。在接下来的章节中，您将发现这项强大技术的基础概念。“原理与机制”一章将剖析正交回归的工作原理，揭示其与主成分分析（PCA）和[奇异值分解](@entry_id:138057)（SVD）等强大的统计学和线性代数概念之间的深刻联系。随后，“应用与跨学科联系”一章将带领读者穿梭于物理学、工程学到生物学等不同科学领域，展示该方法不仅仅是一个统计学上的奇特方法，而是实现精确测量和科学发现的关键工具。

## 原理与机制

当我们观察图上的一簇散点时，我们的大脑非常善于发现趋势。我们可以毫不费力地想象一条线穿过数据，这条线以某种方式代表了两个变量之间关系的“最佳”总结。但“最佳”到底意味着什么？我们如何指导一台缺乏我们直觉的机器来找到这条线？

在入门科学和统计学课程中，教得最多的方法是**[普通最小二乘法](@entry_id:137121)（OLS）**。其思想简单而优雅：对于任何给定的直线，你测量每个数据点到该直线的垂直距离。你将所有这些距离平方（使其为正，并对较大的误差施加更重的惩罚），然后将它们相加。那条使这个垂直[误差平方和](@entry_id:149299)尽可能小的直线就是“最佳”直线。这种方法是数据分析的主力，原因很充分：它计算简单且在统计上很强大。然而，它在一个隐藏的、且常常未经声明的假设下运行——一种无声的暴政。

### [垂直线](@entry_id:174147)的“暴政”

OLS 假设所有[测量误差](@entry_id:270998)都存在于垂直（$y$）变量中。它将水平（$x$）变量视为完全已知的、不可动摇的真理来源。对于某些实验来说，这是一个合理的近似。如果我们正在测试一种新肥料，我们可能会非常精确地控制施用量（$x$），而最终的作物产量（$y$）则受到各种生物和环境噪声的影响。在这种情况下，将所有偏离直线的误差归咎于 $y$ 的误差是合理的。

但如果我们正在比较两个不同的温度计，看看它们的读数如何关联呢？两种仪器都有其自身的缺陷；没有一个是完美的标准。或者，如果我们是一位追踪小行星的天文学家呢？我们在望远镜图像的 $x$ 和 $y$ 坐标上对其位置的测量都具有内在的不确定性。

在这些情况下，OLS 变成了一个有偏见的法官。它武断地将所有离散的责任都归咎于我们放在垂直轴上的变量。如果你交换坐标轴，对 $y$ 进行 $x$ 的回归，你会得到一条*不同*的[最佳拟合直线](@entry_id:172910)！这是非常不尽人意的。量之间的真实潜在关系不应该依赖于我们决定将哪个量称为“$y$”。当应用于两个变量都有误差的数据时，OLS 方法会系统性地低估斜率的大小，这种现象被称为**[衰减偏误](@entry_id:746571)**或[回归稀释](@entry_id:746571) [@problem_id:2408090]。我们需要一种更民主的方法，一种能公平对待两个变量的方法。

### 一种更“民主”的方法：最小化真实距离

如果我们想公平对待 $x$ 和 $y$，我们就不应该偏爱垂直方向。相反，让我们将误差定义为每个数据点到直线的*最短可能距离*。这当然就是**[垂直距离](@entry_id:176279)**，也称为**正交距离**。这个简单、直观的想法是**正交回归**的基础，在线性代数领域，它更广为人知的名字是**[总体最小二乘法](@entry_id:170210)（TLS）**。

我们的目标是找到一条直线（我们可以用隐式形式 $ax + by + c = 0$ 书写）的参数，该直线能最小化每个数据点 $(x_i, y_i)$ 到该直线的正交距离的平方和。一个点到直线的正交距离 $d_i$ 由以下公式给出：

$$
d_i = \frac{|a x_i + b y_i + c|}{\sqrt{a^2 + b^2}}
$$

要找到最佳直线，我们需要最小化这些距离的平方和 $E = \sum_i d_i^2$。这个表达式看起来有点复杂，但我们可以简化它。参数 $(a,b,c)$ 并非唯一；我们可以将它们全部乘以一个常数，得到的是同一条直线。让我们利用这种自由度施加一个约束：我们将要求[直线的法向量](@entry_id:178923) $\vec{n} = (a, b)$ 是一个单位向量。也就是说，$a^2 + b^2 = 1$。有了这个约束，我们[距离公式](@entry_id:164913)中的分母就变成了 1，我们的问题也大大简化了。我们现在寻求最小化：

$$
E(a,b,c) = \sum_{i=1}^N (a x_i + b y_i + c)^2, \quad \text{subject to } a^2 + b^2 = 1
$$

这是 TLS 的标准[优化问题](@entry_id:266749) [@problem_id:2409671]。在我们尝试求解斜率之前，我们可以问一个更简单的问题：这条线位于哪里？如果我们固定 $(a,b)$ 并询问哪个 $c$ 值可以最小化误差，我们可以使用微积分。将 $E$ 对 $c$ 求导并令其为零，会揭示一个优美的结果：该直线必须穿过数据点的**[质心](@entry_id:265015)** $(\bar{x}, \bar{y})$，其中 $\bar{x}$ 和 $\bar{y}$ 分别是 $x$ 和 $y$ 坐标的简单平均值 [@problem_id:2154103] [@problem_id:2422238]。这非常直观；[最佳拟合线](@entry_id:148330)必须锚定在数据云的[质心](@entry_id:265015)上。

### 散点的秘密：主成分分析

知道直线穿过质心极大地简化了我们的问题。我们可以想象将整个[坐标系](@entry_id:156346)平移，使[质心](@entry_id:265015)位于新的原点。现在，我们只需要确定直线的方向，或者说斜率。

在这里，我们偶然发现了科学中那些深刻统一的时刻之一。让我们暂时从拟合直线的问题中抽身，只看我们的（现在已中心化的）数据点云。这个点云有一个形状，通常是一个类似椭圆的斑点。这个斑点最重要或最具特征性的方向是什么？当然是数据[分布](@entry_id:182848)最分散的方向——即[方差](@entry_id:200758)最大的方向。

找到这些特征方向，是一种强大的统计技术——**主成分分析（PCA）**——的目标。PCA 找到一组有序的正交轴（主成分），这些轴与数据中[方差](@entry_id:200758)递减的方向对齐。根据定义，第一主成分（PC1）是指向数据投影[方差](@entry_id:200758)最大化方向的单位向量 $\mathbf{v}$。也就是说，它最大化了 $\sum (\mathbf{x}_i \cdot \mathbf{v})^2$。

这和我们拟合直线的问题有什么关系呢？一切都有关系。对于任何中心化的数据点 $\mathbf{x}_i$ 和任何穿过原点、方向为 $\mathbf{v}$ 的直线，我们可以构成一个直角三角形。斜边是向量 $\mathbf{x}_i$ 本身。两条直角边分别是 $\mathbf{x}_i$ 在直线上的投影，以及从 $\mathbf{x}_i$ 到直线的垂直向量。根据[毕达哥拉斯定理](@entry_id:264352)：

$$
(\text{distance from origin})^2 = (\text{projected distance along line})^2 + (\text{perpendicular distance to line})^2
$$

如果我们将所有数据点的这个关系加总，到原点的距离平方和就是数据的总[方差](@entry_id:200758)，这是一个固定量。这意味着*最大化*投影距离的平方和（PCA的目标）在数学上等同于*最小化*垂直距离的平方和（TLS的目标）[@problem_id:1946294]。

这是一个惊人的等价性。找到最近直线的几何问题和找到最大[方差](@entry_id:200758)方向的统计问题是同一个问题。TLS 的[最佳拟合线](@entry_id:148330)就是由数据的第一主成分所定义的直线。

这一洞见为我们提供了一种直接计算解的方法。主成分是数据**[协方差矩阵](@entry_id:139155)**（或散布矩阵 $S$）的**[特征向量](@entry_id:151813)**。沿每个主成分的[方差](@entry_id:200758)大小由相应的**[特征值](@entry_id:154894)**给出。因此，TLS 直线的方向由[协方差矩阵](@entry_id:139155)与其**最大[特征值](@entry_id:154894)**相关联的[特征向量](@entry_id:151813)给出 [@problem_id:2142970] [@problem_id:3173554]。

### 硬币的另一面：最小[奇异值](@entry_id:152907)

还有另一种同样优美的方式来看待这个问题，它来自数值线性代数的世界。假设我们正在尝试求解一个超定[方程组](@entry_id:193238) $A\mathbf{x} \approx \mathbf{b}$。OLS 假设所有误差都在 $\mathbf{b}$ 中，并试图最小化 $\|A\mathbf{x} - \mathbf{b}\|_2$。其解由著名的 Moore-Penrose [伪逆](@entry_id:140762)给出，$\mathbf{x}_{\text{LS}} = A^+\mathbf{b}$，它通过将 $\mathbf{b}$ 投影到由 $A$ 的列定义的固定[子空间](@entry_id:150286)上起作用。

然而，TLS 承认 $A$ 中也可能存在误差。它完全重新定义了问题：要使[方程组](@entry_id:193238)完全相容，对 $A$ 和 $\mathbf{b}$ 进行的最小可能扰动（我们称之为 $E$ 和 $f$）是什么？也就是说，我们希望求解 $(A+E)\mathbf{x} = \mathbf{b}+f$，同时最小化扰动的总大小，用 Frobenius 范数 $\|[E\; f]\|_F$ 来衡量 [@problem_id:3592284]。

这可以重写为 $[A+E \mid \mathbf{b}+f] \begin{pmatrix} \mathbf{x} \\ -1 \end{pmatrix} = \mathbf{0}$。这意味着我们正在寻找[增广矩阵](@entry_id:150523) $C = [A \mid \mathbf{b}]$ 的“最接近”的[秩亏](@entry_id:754065)（即列线性相关）版本。

著名的 Eckart-Young-Mirsky 定理告诉我们如何使用**[奇异值分解](@entry_id:138057)（SVD）**来找到这个最接近的矩阵。SVD 将任何矩阵 $C$ 分解为一个旋转、一个缩放和另一个旋转的乘积。缩放因子是奇异值 $\sigma_i$。使 $C$ [秩亏](@entry_id:754065)的最小扰动的大小等于最小奇异值 $\sigma_{\min}$。解的关键在于与这个最小奇异值相关联的[右奇异向量](@entry_id:754365) $\mathbf{v}_{\min}$。该[向量张成](@entry_id:152883)了被扰动[矩阵的零空间](@entry_id:152429)，而 TLS 解 $\mathbf{x}_{\text{TLS}}$ 可以直接从其分量中提取出来 [@problem_id:2203385]。

乍一看，这似乎与 PCA 方法大相径庭。一种方法使用协方差矩阵 $S=Z^T Z$ 的*最大*[特征值](@entry_id:154894)来找到直线的方向。另一种方法使用中心化数据矩阵 $Z$ 的*最小*奇异值来找到解。但它们是紧密相关的。$S$ 的[特征值](@entry_id:154894)是 $Z$ 的[奇异值](@entry_id:152907)的平方。与最大[特征值](@entry_id:154894)对应的 $S$ 的[特征向量](@entry_id:151813)给出了*最大*[方差](@entry_id:200758)的方向——即*直线*的方向。而与*最小*[特征值](@entry_id:154894)对应的[特征向量](@entry_id:151813)则给出了*最小*[方差](@entry_id:200758)的方向——即与*直线垂直*的方向 [@problem_id:2154103]。因此，关注零空间和最小误差的 SVD 方法自然地找到了法向量，而关注[方差](@entry_id:200758)的 PCA 方法则自然地找到了[方向向量](@entry_id:169562)。它们是对同一底层结构的两个完美互补的视角。

### 回归的宇宙

那么，每当我们怀疑两个变量都有误差时，是否就应该总是使用 TLS 呢？不一定。这个框架的美妙之处在于它可以被推广。标准的 TLS 通过最小化简单的[欧几里得距离](@entry_id:143990)，隐含地假设 x 和 y 中的[测量误差](@entry_id:270998)是相等且不相关的。从统计学的角度来看，仅在误差是[独立同分布](@entry_id:169067)的高斯分布（即 $\sigma_x^2 = \sigma_y^2$）的假设下，它才是**[最大似然估计](@entry_id:142509)** [@problem_id:2408090]。

如果误差不相等怎么办？如果我们知道 $y$ 的[测量噪声](@entry_id:275238)远大于 $x$ 的[测量噪声](@entry_id:275238)怎么办？在这种情况下，最小化简单的[垂直距离](@entry_id:176279)就不再合适了。$y$ 方向的偏差比 $x$ 方向的偏差“代价更小”。我们需要在一个扭曲的[坐标系](@entry_id:156346)中测量距离，一个能解释不同误差尺度的[坐标系](@entry_id:156346)。

这就引出了更通用的**正交距离回归（ODR）**，这是一个可以处理每个变量已知、不同的[误差方差](@entry_id:636041)，甚至它们之间相关性的框架。在误差独立但[方差](@entry_id:200758)不同（$\sigma_x^2$ 和 $\sigma_y^2$）的常见情况下，目标变成了最小化加权距离的平方和 [@problem_id:2952316]：

$$
E = \sum_i \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{\sigma_y^2 + \beta_1^2 \sigma_x^2}
$$

这个非凡的公式提供了一幅统一的图景。注意在极限情况下会发生什么。如果我们认为 x 中没有误差（即 $\sigma_x^2 \to 0$），该公式会优雅地简化为[加权最小二乘法](@entry_id:177517)的目标函数，即最小化 $\sum (y_i - \beta_0 - \beta_1 x_i)^2 / \sigma_y^2$ [@problem_id:2408090]。如果我们假设误差相等（$\sigma_x^2 = \sigma_y^2 = \sigma^2$），分母就变成 $\sigma^2(1+\beta_1^2)$，最小化这个就等价于 TLS 问题。

我们从简单但有缺陷的[普通最小二乘法](@entry_id:137121)到广义正交距离回归的旅程，揭示了一个深刻而统一的结构。方法的选择不仅仅是一个技术细节，它宣告了我们对误差性质的信念。通过超越[垂直线](@entry_id:174147)的暴政，我们拥抱了一种更诚实、几何上更稳健的方式来寻找隐藏在我们测量散点中的真理。这条路带领我们穿越了几何学、统计学和线性代数的[交叉](@entry_id:147634)世界，揭示了最佳拟合不仅仅是画一条线，更是理解我们数据中变异和不确定性本身的结构。

