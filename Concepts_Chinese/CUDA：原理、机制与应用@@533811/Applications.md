## 应用与跨学科联系

我们已经花了一些时间来理解现代图形处理单元（GPU）的架构，以及赋予其活力的计算统一设备架构（CUDA）的原理。从本质上讲，我们已经认识了一支名副其实的、由数千名简单勤奋的工人组成的军队，他们随时准备同步执行指令。现在自然要问的问题是：“我们到底能用这支军队*做*什么？”事实证明，答案惊人地广泛。通过学习并行思考——将大[问题分解](@article_id:336320)成无数个更小的、独立的部分——我们可以利用这种计算能力来攻克几乎所有科学、工程乃至经济学领域的问题。这段旅程不仅关乎原始的计算能力，更关乎发现隐藏在问题结构本身之中的内在并行性。

### 网格化的世界：模拟物理系统

许多自然法则都以[微分方程](@article_id:327891)的形式表达，描述了某个量如何在空间和时间上逐点变化。想象一下金属板上的温度分布、流体中的压力，或是电场的强度。我们通常用离散的网格来近似连续的世界，而正是在这里，GPU的架构找到了其最自然和直接的应用。

考虑[求解泊松方程](@article_id:307908)，这是[计算物理学](@article_id:306469)的一个基石，它支配着从[静电学](@article_id:300932)到[引力场](@article_id:348648)的各种现象。一种常见的数值方法是[Jacobi方法](@article_id:334645)，这是一个迭代过程，其中每个网格点的新值被计算为其直接邻居旧值的简单平均值 [@problem_id:2433927]。想象一下在GPU内存中布置好的网格。我们可以为每个点分配一个线程——我们的一个工人。在一个单一的、[同步](@article_id:339180)的步骤中，每个线程从上一次迭代中读取其邻居的值，执行一个简单的计算，并将其新值写入一个独立的内存位置。这个过程不断重复，更新的波潮席卷整个网格，直到解收敛。从物理问题到计算模型的映射是如此优美而直接；[算法](@article_id:331821)的结构反映了机器的架构。

如果网格上的“点”不是静止的，而是代表移动的独立实体呢？想象一下模拟一个百万颗恒星的球状星团的动力学、一群相互作用的粒子，或者一个复杂[化学反应](@article_id:307389)的组分。通常，每个实体在一个小时间步内的演化可以用一个与其他实体无关的[常微分方程](@article_id:307440)（ODE）来描述。这导致了所谓的“易于并行”（embarrassingly parallel）问题 [@problem_id:3213404]。我们可以分配一个GPU线程，使用像四阶[Runge-Kutta](@article_id:300895)[算法](@article_id:331821)这样的标准数值方法来为一个粒子求解ODE。所有一百万个粒子可以同时在时间上向前推进。没有复杂的通信，没有错综的依赖关系；这仅仅是将相同的计算配方应用于一个庞大的数据集，而我们的并行军队对于这类任务是极其高效的。

### [算法](@article_id:331821)的心跳：加速核心计算

除了直接模拟，许多复杂的科学和数据驱动问题都依赖于少数几个基础[算法](@article_id:331821)。加速这些核心计算构建模块会产生连锁反应，从而加快整个研究领域的发展。

以求解大型线性方程组为例，这项任务构成了工程、统计和机器学习领域无数应用的支柱。乍一看，像[LU分解](@article_id:305193)（一种复杂版的[高斯消元法](@article_id:302182)）这样的[算法](@article_id:331821)，由于步骤之间的数据依赖性，似乎是无可救药的串行[算法](@article_id:331821)。然而，即使在这里，也能找到并行性。虽然我们不能一次执行所有步骤，但我们可以并行化每一步*内部*的操作，例如同时更新矩阵的整行 [@problem_id:2409837]。在GPU上解决这个问题也教会了我们一个关于性能的关键教训：重要的不仅是计算速度，还有内存访问的效率。组织数据以使线程能够通过单次“合并”事务读取相邻的内存位置，对于保证计算工人的数据供给至关重要。

这种发现并利用隐藏并行性的原则也延伸到其他领域。考虑从十亿个未排序的数字列表中找到[中位数](@article_id:328584)（或任意第$k$小的元素）的任务。完全排序的代价会高得令人望而却步。一个类似于经典Quickselect[算法](@article_id:331821)的并行[选择算法](@article_id:641530)提供了一种快得多的方法。其并行实现中的一个关键原语是“前缀和”（prefix sum），或称扫描（scan）。想象一排人，每个人都持有一个数字。扫描操作就像第一个人告诉第二个人自己的数字，第二个人将其与自己的数字相加后告诉第三个人这个和，以此类推，直到最后一个人知道所有数字的总和。在GPU上，这个看似串行的过程可以在[对数时间](@article_id:641071)内执行，为根据某些标准快速组织和划分数据提供了一个强大的工具 [@problem_id:3257912]。这个原语允许我们并行地计算有多少元素小于一个选定的主元，并在一步之内将它们全部移动到正确的块中。

并行化[算法](@article_id:331821)所需的巧思在生物信息学中或许能得到最好的体现，例如计算两条长DNA序列之间的[编辑距离](@article_id:313123)（或[Levenshtein距离](@article_id:313123)）的问题 [@problem_id:3231026]。经典解法使用一个动态规划表，其中每个单元格的值都依赖于其上方、左方和左上方的邻居。这似乎暗示了一种逐行串行的计算方式。然而，换个角度看，我们会发现沿任何给定*反向对角线*（行和列索引之和为常数）的所有单元格仅依赖于先前反向对角线上的单元格。这使得计算可以像“波”一样进行，其中一个波前中的所有单元格都[并行计算](@article_id:299689)。这是一个绝佳的例子，展示了[算法](@article_id:331821)的独创性如何重构问题以适应并行执行模型。

### 从华尔街到人工智能：探索新前沿

借助这些强大的[算法](@article_id:331821)工具，[GPU计算](@article_id:353950)的范围已经远远超出了其在图形学和科学模拟中的起源，扩展到了金融、人工智能和[网络科学](@article_id:300371)等领域。

在[计算金融学](@article_id:306278)中，为期权等衍生证券定价是一项核心任务。一种流行的方法是多期二项式模型，它构建了一个庞大的未来资产价格可能路径的树。期权的价值是通过从树的末端分枝向后推算得到的，在每个节点根据其子节点计算[期望值](@article_id:313620)。这种逐层向后归纳是波前计算的另一个例子，非常适合并行执行 [@problem_id:2412816]。GPU可以同时评估一个给定时间步上的所有节点，从而能够快速为复杂的金融工具估值。

在人工智能和[网络科学](@article_id:300371)领域，我们经常处理代表社交网络、交通系统或互联网结构的庞大图。一个基本操作是找到从一个源节点到所有其他节点的[最短路径](@article_id:317973)，这个任务可以通过[广度优先搜索](@article_id:317036)（BFS）来解决。并行的BFS以扩展层或“前沿”的方式探索图。当前前沿中的所有节点被并行访问，以发现下一个未访问邻居的前沿 [@problem_id:2398485]。尽管[图遍历](@article_id:330967)的不规则内存访问模式给GPU带来了独特的挑战，但逐层[同步](@article_id:339180)的BFS方法为克服这种复杂性提供了一个稳健的框架。

模拟大量相互作用的智能体的能力也在神经经济学等意想不到的学科中打开了大门 [@problem_id:2417856]。通过对数百万个共同促成决策的、带有噪声的单个[神经元](@article_id:324093)群体进行建模，研究人员可以探索微观的神经行为如何产生宏观的经济选择，如[风险规避](@article_id:297857)。如果没有GPU提供的大规模并行性，这样的大规模模拟将是难以处理的。

### 性能的艺术：驾驭机器

在了解了我们能计算*什么*之后，我们必须谈谈让它运行得*快*的艺术。这需要对软件和硬件之间的相互作用有更深的理解。

考虑一个经典的GPU应用：用于[计算机图形学](@article_id:308496)的[光线追踪](@article_id:351632)。这项任务是“易于并行”的，因为每个像素的颜色都可以独立计算。但是，如果有些像素很容易渲染（例如，指向简单的蓝天），而另一些则极其复杂（例如，涉及多次反射和[折射](@article_id:323002)），该怎么办？一种朴素的静态分解方法，即我们将固定的像素块分配给线程组，是低效的。一些组会很快完成并处于空闲状态，而另一些则被困难的像素拖累。这种负载不均衡是性能的杀手。一个更优雅的解决方案是动态工作分配 [@problem_id:2422656]。我们将工作分解成一个由许多小图块组成的大队列。每当一组线程完成其当前的图块时，它就从队列中抓取下一个。这个自我平衡的系统确保所有处理器都保持繁忙，从而最大化效率并显著减少总渲染时间。

最后，让我们看看芯片本身。通常，高性能计算的最终瓶颈不是计算速度，而是从内存中获取数据所需的时间。“Roofline模型”提供了一种简单直观的方式来理解这种权衡 [@problem_id:3209810]。一个计算任务要么是“计算受限”（受限于处理器的峰值FLOPs），要么是“内存受限”（受限于内存带宽）。决定性因素是[算法](@article_id:331821)的*算术强度*——即执行的计算量与移动的每字节数据之比。为了最大限度地利用硬件，我们需要具有高算术强度的[算法](@article_id:331821)。

现代GPU通过集成用于关键操作的专用硬件，将这一点又向[前推](@article_id:319122)进了一步。例如，[张量](@article_id:321604)核心（Tensor Cores）是为加速矩阵乘法（深度学习的基本操作）而设计的超专用电路。当一个[算法](@article_id:331821)可以被设计为使用这些核心时，它可以达到远超通用CUDA核心的性能水平。这凸显了计算领域的一个深刻趋势：性能是通过[算法](@article_id:331821)、软件和专用硬件的协同设计来实现的，所有这些部分协同工作。

从模拟宇宙到为[金融衍生品定价](@article_id:360913)，从揭示DNA的秘密到驱动人工智能，其原理始终如一。CUDA和[GPU计算](@article_id:353950)的力量不仅在于其数千个核心的蛮力，更在于一个优雅的思想：通过将大量复杂问题分解为众多简单的并行任务，便可以将其攻克。真正的美在于发现这种潜在的统一性。