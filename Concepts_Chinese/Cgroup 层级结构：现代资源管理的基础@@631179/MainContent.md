## 引言
在任何多任务计算机系统中，从个人笔记本电脑到大型云服务器，都存在一个根本性挑战：如何公平、高效地分配 CPU 时间、内存和 I/O 带宽等共享资源。简单地平等对待每个进程往往会导致看似矛盾的不公平结果，即一个拥有许[多线程](@entry_id:752340)的应用程序可能会以牺牲另一个应用程序为代价，独占整个系统。这正是 Linux 控制组（control groups），或称 [cgroups](@entry_id:747258)，旨在解决的核心问题。Cgroups 不在单个进程的微观层面管理资源，而是提供了一个强大的框架，在应用程序和服务的宏观层面进行管理。本文深入探讨了 cgroup 层级结构这一优雅的模型。第一章“原则与机制”剖析了该系统的基本规则，探讨了层级结构如何形成，如何使用权重和上限分配资源，以及不同资源控制器如何协同工作。随后的“应用与跨学科联系”一章揭示了这些基本原则如何催生了从工作负载保护、容器化到创建[自驱动](@entry_id:197229)数据中心等一切事物，展示了为何 [cgroups](@entry_id:747258) 是现代计算不可或缺的支柱。

## 原则与机制

要真正理解任何物理系统，无论是星系还是计算机，我们必须首先掌握支配其行为的原则。游戏的基本规则是什么？对于 Linux 控制组而言，这些规则出人意料地简单而优雅，但它们结合在一起，创造了一个功能强大且精妙入微的系统。让我们踏上探索这些原则的旅程，不把它们当作一串枯燥的功能列表，而是作为一系列对管理共享资源这一根本问题的逻辑解答。

### 个体的暴政：群组的必要性

想象一下，你是计算机内核中的一个调度器。你的工作是要做到公平。但对谁公平呢？你看到一大群活跃的任务，都在请求 CPU 时间。最简单的公平形式可能是给予每个任务同等份额的关注。现在，考虑两个正在运行的程序。第一个是复杂的科学模拟，作为单个强大的执行线程运行。第二个是 Web 服务器，为一千个并发连接中的每一个都派生一个新线程。

如果你，作为调度器，对每个单独的线程都公平，那么 Web 服务器将获得比[科学模拟](@entry_id:637243)多一千倍的 CPU 时间。从*程序*的角度来看，这绝不公平。程序的架构——无论是使用多对一还是一对一的[线程模型](@entry_id:755945)——无意中决定了它的资源分配 [@problem_id:3689541]。这就是“个体的暴政”：通过关注最小部分的公平性，我们可能对其构成的更庞大的应用程序造成了严重的不公。

这正是[控制组](@entry_id:747837)被发明出来要解决的根本问题。解决方案异常简单：如果将单个线程作为记账单位是问题所在，那么我们就需要一种方法来定义一个新的单位。我们需要一种方法，在应用程序的所有线程周围画一个虚拟的“围栏”，并告诉内核：“将这个围栏里的一切都视为一个单一实体。”这个围栏就是一个**[控制组](@entry_id:747837)**（**control group**），或称 **cgroup**。

### 划分围栏：Cgroup 层级结构

一旦我们有了划分围栏的能力，下一个合乎逻辑的步骤就是在围栏内再划分围栏。一个现代应用程序并非铁板一块；它可能有一个处理用户请求的关键前台组件，以及一个执行批量分析的后台组件。我们可能希望给予整个应用程序一定的资源预算，但又想在*该预算内*优先考虑前台组件。

这就引出了 [cgroups](@entry_id:747258) 最根本的原则：它们形成一个**层级结构**。你可以把它想象成一家公司的组织结构图。整个公司有一个总预算。这个预算被分配给不同的事业部。每个事业部再将其预算分配给下属的部门，以此类推。

在 cgroup 的世界里，这种树形结构有一个至关重要的规则：只有树的“叶子节点”才能包含实际的进程（即工作者）。“内部节点”（即管理者）的存在只是为了向其子节点分配资源 [@problem_id:3628557]。一个 cgroup 要么包含进程，要么拥有子 cgroup，但不能两者兼有。这条“无内部进程”规则保持了模型的简洁和明确。管理者的工作是管理，而不是亲自干活。

这种层级结构的美妙之处在于，资源分配变成了一个简单的乘法法则。如果一个父 cgroup $\mathcal{P}$ 被分配了系统 I/O 带宽的一半，而它的子 cgroup $\mathcal{C}_1$ 被配置为接收其父节点份额的五分之一，那么 $\mathcal{C}_1$ 最终占系统总带宽的份额就是 $\frac{1}{2} \times \frac{1}{5} = \frac{1}{10}$ [@problem_id:3628646]。这种可预测的、级联式的分配正是层级控制的精髓。

### 分配的两种工具：权重和上限

那么，我们有了一个用于分配资源的层级结构。但是我们如何决定每个分支能获得多少呢？Cgroup 工具包提供了两种主要工具：我们称之为**权重**（weights）的相对份额，和我们称之为**上限**（caps）或**配额**（quotas）的绝对限制。

**权重**用于按比例划分。想象两个同级的 cgroup，$A$ 和 $B$，在竞争 CPU 时间。如果 $A$ 的 `cpu.weight` 是 $300$，$B$ 的 `cpu.weight` 是 $100$，这并不意味着 $A$ 能在绝对意义上获得 $300$ 单位的任何东西。它仅仅意味着当两者都在请求 CPU 时，$A$ 将获得 $B$ 所获 CPU 时间的三倍。它们以 $3:1$ 的比例共享可用资源 [@problem_id:3673679]。这是一个非常灵活的系统。如果第三个 cgroup $C$（权重为 $400$）开始竞争，比例会自动调整。现在权重的总“池子”是 $300+100+400=800$，而 $A$ 将获得 $\frac{300}{800}$ 的可用 CPU。

另一方面，**上限**是硬性的、不可移动的天花板。一个 CPU 上限，通常表示为在一个周期 $p$ 内的配额 $q$，规定一个 cgroup 在任何 $p$ 微秒的窗口内使用的 CPU 时间不能超过 $q$ 微秒，没有例外。例如，设置 $q=50,000$ 和 $p=100,000$ 意味着该 cgroup 最多可以使用一个 CPU 核心的 $50\%$，无论其他进程在做什么 [@problem_id:3628565]。即使系统空闲，并且该 cgroup 的比例份额本应让它获得更多资源，上限依然有效。它是一个不可侵犯的上限。

当这两种工具相互作用时，系统的真正巧妙之处就显现出来了。考虑一个父 cgroup $P$，它被限制在每 $100$ 毫秒周期内最多使用 $60$ 毫秒的 CPU 时间。它的子 cgroup $H_1$ 和 $H_2$ 的权重分别为 $2$ 和 $1$。表面上看，它们会以 $2:1$ 的比例分享父 cgroup 的 $60$ 毫秒预算，使得 $H_1$ 得到 $40$ 毫秒，$H_2$ 得到 $20$ 毫秒。但如果 $H_1$ 自身还有一个更严格的、仅为 $25$ 毫秒的上限呢？层级结构会强制执行最严格的规则。$H_1$ 得到它的 $25$ 毫秒，然后被节流。那么 $H_1$ 有权使用但未用完的 $15$ 毫秒会怎样呢？它不会消失。因为调度器是**工作保守型（work-conserving）**的，这部分剩余时间会被重新分配给其他竞争的兄弟 cgroup。$H_2$ 会愉快地接手这部分空闲资源，在获得自己原有的 $20$ 毫秒之外，再加上额外的 $15$ 毫秒，总共获得 $35$ 毫秒 [@problem_id:3673679]。

### 控制的交响乐：CPU、内存和 I/O 协同工作

Cgroups 的真正威力不仅在于控制单一资源，更在于协调整合所有资源。现代系统是 CPU、内存和 I/O 复杂相互作用的整体，而 [cgroups](@entry_id:747258) 提供了一个统一的框架来管理它们。从旧版的 [cgroups](@entry_id:747258) v1 到现代的 v2 的迁移完美地展示了这种统一的愿景 [@problem_id:3628557]。

对于 CPU，我们将用于在竞争下公平共享的 `cpu.weight` 与用于硬性限制的 `cpu.max` 结合起来。对于内存，我们有 `memory.max`，这是一个硬性限制，一旦突破，就会触发臭名昭著的内存不足（OOM）杀手。但我们还有一个更精妙的工具：`memory.high`。这不是一堵硬墙，而是一个“压力区”。一旦一个 cgroup 的内存使用量超过这个阈值，内核就会开始施加温和的压力，更积极地尝试从该组中回收内存。这使得应用程序在内存压力下能够优雅地降级，即减速运行而不是突然崩溃。

也许最重大的进步是在 I/O 控制方面。在旧系统中，对 I/O 进行节流是出了名的困难，特别是对于首先写入内存中页面缓存的“缓冲”写操作。内核可能会在稍后将这些写操作刷新到磁盘，而 I/O 成本会被归咎于一个通用的[内核线程](@entry_id:751009)，而不是发起它的进程。现代统一的 `io` 控制器通过将 I/O 与最初弄脏内存的 cgroup 相关联来解决这个问题，确保 I/O 限制是健壮且可预测的。

然而，这些控制可能会产生微妙的、贯穿多个层面的影响。一个处于从磁盘读取数据紧密循环中的进程可能看起来是 I/O 密集型的，但每个读操作也需要少量 CPU 时间来提交请求和处理其完成。如果该进程所在的 cgroup 有一个非常严格的 CPU 配额，它就可能变成 CPU 密集型！它可能拥有充足的 I/O 带宽，但无法获得足够的 CPU 时间来足够快地提交请求以利用这些带宽。这可能导致一个看似矛盾的情况：严重的 CPU 节流实际上增加了 I/O 操作的端到端延迟，即使磁盘本身大部分是空闲的 [@problem_id:3651825]。

### 不仅仅是预算：作为策略执行者的 Cgroups

除了简单地划分资源，[cgroups](@entry_id:747258) 还允许管理员制定复杂的操作策略，特别是在处理故障和确保安全方面。

这一点在内存不足（OOM）杀手问题上表现得最为淋漓尽致。当[系统内存](@entry_id:188091)耗尽时，它必须做出一个残酷的选择：应该杀死哪个进程来释放资源？没有 [cgroups](@entry_id:747258)，这是一场混乱的混战。有了 [cgroups](@entry_id:747258)，这就变成了一项可管理的策略。我们可以将最关键的服务放在一个 cgroup 中，并给予它们强有力的保护，比如通过 `memory.min` 保证来保护其部分内存不被回收，并通过一个非常负的 `oom_score_adj` 来告诉 OOM 杀手：“去别处看看。”反之，我们可以将一个低优先级的批量分析作业放在另一个 cgroup 中。我们甚至可以在这个 cgroup 上设置一个特殊标志 `memory.oom.group`。这个标志建立了一个强有力的策略：如果 OOM 杀手决定对该组内的任何一个进程下手，它会同时杀死组内的*所有*进程。这使得系统能够通过一次干净利落的行动回收一大块可预测的内存，而不是逐个杀死进程——那可能不足以解决内存短缺问题 [@problem_id:3628571]。

这种策略执行也延伸到了安全领域。一种常见的攻击或 bug 是“fork 炸弹”，即一个进程不受控制地创建新进程，迅速耗尽系统资源。Cgroup 的 **PID 控制器**提供了一种直接的防御。通过在 cgroup 上设置 `pids.max`，你可以对该组及其所有后代中可以存在的任务总数强制施加一个硬性限制。至关重要的是要理解，这是一种资源控制机制，完全独立于其他内核特性，如**[PID](@entry_id:174286) 命名空间**，后者仅改变进程对进程树的*视图*。一个在 PID 命名空间中的进程可能认为自己是“[PID](@entry_id:174286) 1”，但 cgroup 控制器在更深的内核层面操作，仍然会正确地将其计入该组的总任务限制 [@problem_id:3628624]。然而，并非所有资源都由 [cgroups](@entry_id:747258) 管理。例如，对打开文件描述符数量的经典限制仍然是一个按进程设置的 `RLIMIT_NOFILE`，这展示了内核资源管理的层次性 [@problem_id:3685852]。

### 感知压力：我们如何衡量竞争

有了所有这些控制措施，我们如何知道一个服务是否健康？仅仅看 CPU 使用率可能会产生误导。一个使用了其 CPU 配额 90% 的服务可能运行得很好，也可能正极度渴望更多资源。我们真正想知道的是：这个 cgroup 中的任务有多频繁地因等待所需资源而停滞？

这正是**压力失速信息（Pressure Stall Information，PSI）**所测量的。对于每种主要资源（CPU、内存、I/O），PSI 提供的指标报告了在墙上时钟时间（wall-clock time）中，cgroup 中至少有一个任务因等待该资源而停滞的百分比。

PSI 的层级性质为理解应用程序健康状况提供了深刻的洞见。想象一个父 cgroup 有两个子 cgroup，每个代表一个[微服务](@entry_id:751978)。子 cgroup 1 在 CPU 上停滞的时间占 20%（$p_1 = 0.2$），子 cgroup 2 停滞的时间占 40%（$p_2 = 0.4$）。那么代表整个应用程序的父 cgroup 的压力是多少？如果父 cgroup 的*至少一个*子 cgroup 处于停滞状态，那么父 cgroup 就被认为处于压力之下。假设它们的停滞是独立事件，那么父 cgroup *不*处于停滞状态的概率是两个子 cgroup 都能自由运行的概率：$(1-p_1)(1-p_2)$。因此，父 cgroup 的压力是 $p_{\text{parent}} = 1 - (1-p_1)(1-p_2)$。

代入数字，我们得到 $p_{\text{parent}} = 1 - (1-0.2)(1-0.4) = 1 - (0.8)(0.6) = 1 - 0.48 = 0.52$。父 cgroup 的压力（52%）比任何单个子 cgroup 的压力都高 [@problem_id:3628639]。这不是错误；这是一个更全面的真相。它告诉我们，在超过一半的时间里，我们应用程序的*某个部分*正遭受资源竞争。这种聚合视图对于理解复杂、多进程服务的整体健康状况和性能至关重要，从而完成了从简单的资源划分到复杂、可观测且有弹性的系统管理的演进。

