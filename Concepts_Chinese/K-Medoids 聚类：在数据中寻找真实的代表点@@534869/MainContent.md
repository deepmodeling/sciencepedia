## 引言
聚类，即对相似对象进行分组的任务，是现代数据分析的基石，而 K-means [算法](@article_id:331821)是其中最流行的方法之一。K-means 的运作基于一个直观的原则：寻找一个簇的“[质心](@article_id:298800)”，即 centroid。然而，这种优雅的背后隐藏着一个关键的局限性：它对算术平均值的依赖使其对离群点非常敏感，并将其应用限制在那些算术平均值有意义的数据上。当我们的数据不那么“规矩”，或者当“距离”并非一条直线时，会发生什么呢？正是在这一空白领域，K-medoids [聚类](@article_id:330431)作为一种鲁棒且灵活的替代方案应运而生。通过将聚类中心限制为真实的数据点——即“中心点”（medoids），它提供了一种更具韧性且通常更易于解释的解决方案。

本文将深入探讨 K-medoids 的世界，探索其理论基础和实践能力。在第一部分“原理与机制”中，我们将解析[中心点](@article_id:641113)（medoid）的核心概念，将其与[质心](@article_id:298800)（centroid）进行对比，并审视用于寻找这些代表点的[算法](@article_id:331821)策略。随后，在“应用与跨学科联系”部分，我们将展示 K-medoids 非凡的通用性，揭示这一简单思想如何被应用于从[药物发现](@article_id:324955)、生态学到量子物理和金融学等不同领域。

## 原理与机制

想象一下，你面前有一张地图，上面[散布](@article_id:327616)着一些点，或许是城市里所有咖啡馆的位置。如果你想建一个新的中央仓库来为它们供货，你会建在哪里？你的第一直觉，一个非常简单而强大的想法，很可能是找到所有咖啡馆位置的“[质心](@article_id:298800)”——即地理上的平均位置。这个平均位置就是数学家所称的**[质心](@article_id:298800)（centroid）**。几十年来，这个想法一直是著名[聚类算法](@article_id:307138)之一——K-means 的基石。它通过首先猜测聚类中心的位置，然后将每个点分配给最近的中心，最后更新中心为其所在组内所有点的新平均值来对数据进行分组。这个过程不断重复，直到不再有任何变化。它优雅、直观，而且通常效果很好。

但正如科学中许多美丽的想法一样，它的优雅建立在一个隐藏的假设之上，一根默默支撑着整个结构的支柱。要看清这一点，我们必须问一个更深层次的问题：当我们选择一个中心时，“最佳”到底意味着什么？

### 两种几何学的故事：当均值失效时

[算术平均值](@article_id:344700)，即[质心](@article_id:298800)，有一个特殊、近乎神奇的性质：它是唯一一个能使到一个集合中所有其他点的*平方[欧几里得距离](@article_id:304420)*之和最小化的点。“欧几里得距离”就是我们熟悉的直线距离，就像鸟儿飞行的距离一样。想一想：K-means [算法](@article_id:331821)试图最小化各点到其[聚类](@article_id:330431)中心的平方距离之和，而它更新中心的方式——取平均值——正是实现这一目标的完美策略。[算法](@article_id:331821)与其目标完美和谐。

但如果我们的世界不是欧几里得的呢？想象一下，你是在曼哈顿开出租车的司机。你不能开车穿过建筑物，你被限制在街道网格中。两点之间的距离不是一条直线，而是你必须行驶的水平和垂直街区数之和。这被称为**[曼哈顿距离](@article_id:340687)**，或 $L_1$ 距离。如果我们使用这种新的几何学，平均值还是最佳的中心吗？

让我们做一个简单的实验。假设一条线上有三个点，位置分别是 $0$、$0$ 和 $10$。平均值（[质心](@article_id:298800)）在 $\frac{0+0+10}{3} \approx 3.33$。从这个提议的中心到这三个点的总[曼哈顿距离](@article_id:340687)是 $|0 - 3.33| + |0 - 3.33| + |10 - 3.33| \approx 3.33 + 3.33 + 6.67 = 13.33$。

但如果我们选择一个不同的中心呢？在[曼哈顿距离](@article_id:340687)的世界里，使绝对距离之和最小化的点不是均值，而是**[中位数](@article_id:328584)**。我们集合 $\{0, 0, 10\}$ 的中位数是 $0$。让我们计算到中位数的总距离：$|0-0| + |0-0| + |10-0| = 10$。这明显优于我们用均值得到的 $13.33$！在[曼哈顿距离](@article_id:340687)的世界里使用均值作为我们的中心，实际上可能导致一个*更差*的结果 [@problem_id:3109562]。

这揭示了一个深刻的真理：[质心](@article_id:298800)并非一个普适的中心。它的王者地位仅限于平方[欧几里得距离](@article_id:304420)的王国。如果我们希望探索其他距离概念——在[数据科学](@article_id:300658)中，我们经常这样做——我们就需要一种新的代表点。

### 中心点（Medoid）：来自数据点中的代表

这就是**中心点（medoid）**概念的由来，它是一次[范式](@article_id:329204)转换。我们不再像均值那样构造一个人工的中心（它甚至可能不对应任何真实的数据点），而是制定一个简单但强大的新规则：中心必须是真实的数据点之一。[中心点](@article_id:641113)是一个现有的数据点，它对其所在的簇来说最具中心性，即它与该簇中所有其他点的平均相异性最小。

这个想法极具解放性。它将我们从任何特定几何学的束缚中解放出来。要找到一个中心点，我们不再需要坐标，也不需要“平均”点的能力。我们所需要的只是一个成对相异性表——一个矩阵，告诉我们数据集中任意两点之间的“成本”。然后，最著名的[算法](@article_id:331821)**围绕中心点划分（PAM）**，通过简单地查询这些成本来工作。

突然之间，我们可能性的世界爆炸了。我们可以对患者的基因表达数据进行[聚类](@article_id:330431)，不是根据它们的几何位置，而是根据它们表达谱之间的**相关性**。你不能将两个相关性值平均来得到一个“中心”，但你肯定可以找到那个其表达谱最能代表整个群体的患者。这在[生物信息学](@article_id:307177)等领域是一个巨大的优势，因为在这些领域中，[中心点](@article_id:641113)不仅仅是一个数学抽象，而是一个真实的、可观察的、**可解释的范例** [@problem_id:2379227]。我们可以使用**汉明距离**对二[元数据](@article_id:339193)（如调查问卷答案）进行[聚类](@article_id:330431)，汉明距离只是简单地计算两个向量在多少个位置上不同 [@problem_id:3109544]。[中心点](@article_id:641113)方法用同样优雅的机制处理所有这些情况。

### 鲁棒性的优点

这种“来自数据点中的代表”的哲学带来了另一个关键的好处：**鲁棒性**。[算术平均值](@article_id:344700)对离群点非常敏感，这是众所周知的。想象一下我们那群咖啡馆，然后在一个偏远的岛屿上新开了一家孤零零的咖啡馆。[质心](@article_id:298800)，即[质量中心](@article_id:298800)，将被显著地拖向那个岛屿，最终可能落在大海中央，既不能很好地代表主要的咖啡馆集群，也不能很好地代表那个离群点。

而[中心点](@article_id:641113)则要坚韧得多。由于它必须是一个真实的数据点，那个偏远岛屿上的咖啡馆极不可能被选为主要城市集群的代表。它离其他所有点都太远了。相反，K-medoids 更可能正确地用一个真正中心的咖啡馆来识别主要集群，并将那个离群点隔离成一个单独的簇。对于现实世界的数据来说，这是一个至关重要的属性，因为这些数据几乎总是杂乱的，并混杂着错误或异常的测量值 [@problem_id:3109628]。

我们甚至可以量化这种效应。想象一个点集 $\{0, 1, 2, 8, 9, 20\}$。如果我们想找到两个中心点，一个非常好的选择似乎是像 $\{1, 9\}$ 这样的一对，它将数据分成两个自然的组。现在，如果我们开始在位置 $20$ 添加重复的点会怎样？每个在 $20$ 的新点都会为该位置增加“投票权”。随着足够多的重复点，点 $20$ 最终将获得如此大的影响力，以至于它自己成为一个最优的[中心点](@article_id:641113)，将聚类结构拉向它。仔细计算表明，只需在 $20$ 处增加*一个*额外的点，就足以使其成为最优中心点对的一部分，从而改变[聚类](@article_id:330431)结果 [@problem_id:3109559]。[中心点](@article_id:641113)的位置是一个民主的结果，由所有其他点的“拉力”决定。

### 寻找最优[中心点](@article_id:641113)：一个崇高但困难的问题

所以，我们有一个明确的目标：找到一个包含 $k$ 个中心点的集合，使得所有点到其最近[中心点](@article_id:641113)的总相异性最小。这听起来足够简单。但我们到底如何找到它们呢？

在这里，我们遇到了一个有趣的挑战。找到绝对最佳的[中心点](@article_id:641113)集合属于计算机科学中一类被称为 **NP-难** 的问题。这是一种形式化的说法，意指它极其困难。要*保证*你找到了最佳解决方案，唯一的办法本质上是尝试所有可能的 $k$ 个点作为中心点的组合。对于一个小型数据集，这是可行的。对于一个有 5 个点和 2 个中心点的问题，只有 $\binom{5}{2}=10$ 种组合需要检查 [@problem_id:3153890]。但对于一个有 100 个点和 5 个中心点的数据集，组合数量接近 7500 万！这种暴力方法很快就变得不可能。

这就是为什么我们区分 **K-medoids 问题**（目标）和用于近似解决它的**[算法](@article_id:331821)**。最著名的是**围绕中心点划分（PAM）**[算法](@article_id:331821)。它使用一种巧妙的[局部搜索](@article_id:640744)[启发式方法](@article_id:642196)。它从一个[中心点](@article_id:641113)的初始猜测开始，然后通过考虑所有可能的交换来系统地尝试改进解决方案：将一个当前的[中心点](@article_id:641113)与一个非中心点进行交换。如果一次交换降低了总成本，就进行交换，并重复这个过程。这个过程持续到没有任何一次交换可以改进解决方案为止。[算法](@article_id:331821)已经攀登到了解空间中的一个“山峰”——一个局部最优解。

当然，这个山峰可能不是整个山脉中最高的山峰。为了找到一个更好的，甚至可能是全局最优的解决方案，我们可以求助于更复杂的方法，如**[模拟退火](@article_id:305364)**。这种[算法](@article_id:331821)受到金属冷却过程的启发，它也探索解空间，但有一个转折：它偶尔可以接受一个使解决方案*变差*的移动。这种能够“下山”一步的能力使其能够[逃离局部最优](@article_id:641935)解的陷阱，探索解空间的其他部分，从而增加找到真正[全局最优解](@article_id:354754)的机会 [@problem_id:3193485]。

### 相异性的景观

最后，值得反思的是相异性度量本身所起的深远作用。改变我们测量距离的方式，不仅仅是改变公式中的一个数字；它从根本上重塑了我们[算法](@article_id:331821)正在探索的“成本景观”。在欧几里得距离下最优的一组中心点，在[曼哈顿距离](@article_id:340687)下可能远非最优 [@problem_id:3109575]。问题的“几何结构”决定了[成本函数](@article_id:299129)的山谷和山峰位于何处。

任何单个点的分配都是一个微妙的平衡。一个点被分配给[中心点](@article_id:641113) A 而不是中心点 B，是因为它与 A 的相异性更低。但对于那些位于“边界”附近、与两个中心点的相异性几乎相等的点呢？它们的归属是脆弱的。一个微小的、有针对性的扰动——一种“对抗性扰动”——对相异性值就足以打破平衡，将该点的分配从一个簇翻转到另一个簇 [@problem_id:3109606]。

这段从简单的[质心](@article_id:298800)到鲁棒且灵活的[中心点](@article_id:641113)的旅程，揭示了现代[数据分析](@article_id:309490)中的一个关键主题：没有唯一的“最佳”方法。正确的工具取决于你的数据性质，以及最重要的是，你对“相似”或“不同”的定义。K-medoids 框架的美妙之处在于它坦诚而明确地拥抱了这种选择。它邀请我们不要寻求一个唯一的、普适的答案，而是深思熟虑地定义我们问题的几何结构，然后探索由此浮现的丰富结构。

