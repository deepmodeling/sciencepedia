## 应用与跨学科联系

我们已经探讨了[掩码语言建模](@article_id:641899)（MLM）的机制，这个聪明的捉迷藏游戏让模型学会从周围的上下文中预测缺失的词语。乍一看，这似乎只是一个精巧但狭窄的技巧，一个用于语言的数字客厅游戏。但如果仅作此观，便是只见树木，不见森林。掩码建模的真正力量不在于任务本身，而在于它所体现的*原则*：从无标签的[序列数据](@article_id:640675)中学习深度的、上下文相关的结构。而事实证明，序列无处不在。

MLM 超越其初始目的的旅程，完美地诠释了科学中的一个共同主题——一个特定的解决方案常常能解锁一个具有惊人广泛影响的通用原则。最初作为[预训练](@article_id:638349)语言模型的一种方法，如今已发展成为一种多功能工具，揭示了生物学、软件工程和医学等迥异领域所使用的“语言”之间隐藏的统一性。

### 为[自然语言处理](@article_id:333975)注入超强动力

即使在其原生的语言领域，MLM 的应用也远不止填空那么简单。它所培养的深度上下文理解能力，为众多高级功能奠定了坚实的基础。

最令人惊讶的应用之一是使用这些模型进行**文本生成**。基于 MLM 的模型，如 BERT，是“编码器”；它们被设计用来理解和表示文本，而不是像它们的“解码器”表亲（如 GPT）那样用来写作。但只要稍加巧思，我们就可以通过迭代优化的过程引导它们生成文本。想象一下，给模型一个带有几个掩码[空位](@article_id:308249)的句子。它对每个[空位](@article_id:308249)做出最佳猜测。然后我们取这个部分生成的句子，重新掩盖其中一个位置，让它在拥有更多上下文的情况下再次预测。通过重复这个过程，模型可以迭代地将一段文本“润色”成一个连贯的整体，这表明为理解而学的丰富世界模型可以被重新用于创造 [@problem_id:3102484]。

也许最具变革性的应用是**基于提示的或“零样本”学习**。如果我们不必花费数月时间收集数据和训练一个新模型来执行像[情感分析](@article_id:642014)这样的特定任务，而是能简单地*要求*[预训练](@article_id:638349)模型来完成，那会怎样？我们可以！通过将我们的任务构建成一个填空问题，我们能利用模型已有的知识。例如，要对“这部电影是一部杰作”进行情感分类，我们可以追加一个提示：“The film was a masterpiece. The review is [MASK].”然后我们检查模型倾向于用哪些词来填补这个空白。如果它为“positive”或“excellent”赋予了高概率，而为“negative”或“terrible”赋予了低概率，我们就得到了答案。这项技术使得单个模型能够执行大量它从未被明确训练过的任务，尽管其性能可能对提示的精确措辞和标签词（或称“verbalizers”）的选择很敏感 [@problem_id:3102497]。

此外，我们可以用外部知识来增强这些模型，将它们从闭卷考生转变为开卷专家。在一个称为**检索增强[掩码语言建模](@article_id:641899)**的框架中，当模型需要预测一个被掩盖的词元时，它首先会查询一个巨大的数据库（如维基百科）以获取相关文档。然后，它会同时关注局部句子上下文和这些检索到的段落，以做出更明智的 prediction。这将模型的预测建立在外部事实上，减少了其“幻觉”倾向，并使其在知识密集型任务中更加可靠。当然，挑战在于确保模型能注意到正确的段落，尤其是在存在干扰性的“难分负样本”——即那些乍一看似乎合理但不相关的文档——的情况下 [@problem_id:3102477]。

最后，MLM 原理是**多语言模型**的基石。通过在一个包含一百多种语言的大规模文本语料库上[预训练](@article_id:638349)单个 [Transformer](@article_id:334261)，模型学会了一个共享的表示空间。在某种程度上，语言的底层“语法”变得通用。为了进一步增强这一点，MLM可以与其他目标相结合，例如对比损失，它明确鼓励翻译句（例如，英语的“the cat sat on the mat”和法语的“le chat s'est assis sur le tapis”）的表示在[嵌入空间](@article_id:641450)中彼此接近。这种联合训练创造了强大的模型，能够执行跨语言任务，例如通过利用高资源语言的知识，来翻译或分类一个只有很少标签样本的语言中的文本 [@problem_id:3164805]。

### 解码生命与机器的语言

当我们跳出人类语言的范畴时，掩码原理的真正普适性变得惊人地清晰。事实证明，支配生物学和计算机代码的序列也有一种“语法”，可以用同样的方式来学习。

在**[计算生物学](@article_id:307404)**中的类比是直接而深刻的。蛋白质是氨基酸的序列，基因是[核苷酸](@article_id:339332)的序列。这些不是随机的字符串；它们是生命的语言，由数十亿年的进化塑造而成。通过应用相同的自监督逻辑，我们可以创建“蛋白质语言模型”。我们取一个蛋白质序列，掩盖几个氨基酸，然后训练一个模型从蛋白质其余部分的上下文中预测原始的氨基酸。这个任务，有时被称为掩码氨基酸建模（MAAM），迫使模型学习支配蛋白质折叠和功能的复杂生化和结构规则，而从未被明确教导过这些。由此产生的[嵌入](@article_id:311541)非常强大，它们构成了预测蛋白质结构和功能的革命性工具的基础 [@problem_id:1426773]。

同样的原理也适用于基因组学。像 **DNA-BERT** 这样的模型可以使用 MLM 目标在整个基因组上进行[预训练](@article_id:638349)。通过学习预测被掩盖的[核苷酸](@article_id:339332)，它隐式地捕捉了基因组的复杂语法，包括像[转录因子结合](@article_id:333886)位点这样的局部模式和调控基因表达的远距离依赖关系。当这个[预训练](@article_id:638349)模型随后在一个小的、有标签的数据集上针对特定任务（如识别[启动子区域](@article_id:346203)）进行微调时，它的表现远远超过从零开始训练的模型。[预训练](@article_id:638349)已经完成了学习 DNA 基本语言的繁重工作，使模型能够用少量数据快速 specialization。这是[迁移学习](@article_id:357432)的经典例子，其中来自通用领域的知识为特定领域提供了巨大的推动力 [@problemål_id:2429075]。

软件代码是另一个完美的候选对象。它是一种具有严格语法和逻辑结构的[形式语言](@article_id:328817)。通过在来自开源代码库的数十亿行代码上[预训练](@article_id:638349)一个 [Transformer](@article_id:334261)，我们可以创建能够理解编程语言的模型。我们甚至可以调整 MLM 目标，使其更关注代码中语义上更重要的部分，例如在预测缺失的类型注解时给予更多权重。这训练模型理解函数、变量及其类型之间的关系 [@problem_id:3164788]。值得注意的是，从代码中学到的抽象推理结构——如逻辑、层次结构和关系——有时可以正向迁移到自然语言任务中，这表明这些模型正在学习比单纯的[统计相关性](@article_id:331255)更深层次的东西。类似地，我们可以通过要求模型填补数学表达式中缺失的运算符或括号来测试其对形式语法的掌握程度，从而探究其学习[运算符优先级](@article_id:347931)等规则的能力 [@problem_id:3164749]。

### 从语言到行动：现实世界系统

MLM 的实际应用已经被部署在关键的现实世界系统中，通常是通过将日志和事件流视为一种语言形式来处理。

在**临床信息学**中，患者的医疗历程可以被看作是记录在其电子健康记录（EHR）中的一系列事件。每一次“就诊”都可以被标记化为一组诊断代码、程序和药物。一个类似 BERT 的模型可以在这些序列上进行训练，以理解疾病和治疗的典型进展。通过掩盖某些事件，模型学会预测，例如，基于患者病史的可能未来诊断。这类模型需要仔细的设计选择，例如如何最好地将一次复杂的就诊表示为单个词元，以及如何编码就诊之间至关重要的时间信息 [@problem_id:3102533]。

在**网络安全和 IT 运维**中，系统日志是记录服务器、网络和应用程序行为的持续文本流。这个流本质上是一种描述系统状态的语言。我们可以使用 MLM 目标在一个庞大的正常运行日志语料库上训练一个模型。模型学会了“正常”模式是什么样子。然后我们可以使用这个模型进行**[异常检测](@article_id:638336)**。通过计算一个新的、传入的日志序列的“伪[对数似然](@article_id:337478)”——本质上是模型认为该序列的可能性，通过概念上逐一掩盖和预测每个词元来评估——我们可以得出一个强大的异常分数。一个在模型下极不可能出现的序列会被标记为潜在的威胁或系统故障，从而提供一个能够适应每个系统日志特定“方言”的预警系统 [@problem_id:3164779]。

从诗歌到蛋白质，从软件到情感，从掩码上下文中学习的简单原理已被证明是一个惊人有效且具有统一性的概念。它告诉我们，在信息时代，许多复杂系统可以通过学习它们的语言来被理解。