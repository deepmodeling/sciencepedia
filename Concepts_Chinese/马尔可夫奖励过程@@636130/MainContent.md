## 引言
马尔可夫奖励过程 (MRP) 提供了一个简单而强大的数学框架，用于为一个在充满状态、转移和奖励的世界中导航的智能体建模。它是现代[强化学习](@entry_id:141144)的基石，提供了回答一个基本问题所需的基本工具：“处于一个特定情况有多好？”本文旨在解决量化这种“优良性”以及从经验中学习它的挑战，特别是当世界的规则未知或其规模巨大时。

本文的探索旨在从头开始构建您的理解。在“原理与机制”部分，我们将剖析 MRP 的核心组成部分，包括至关重要的[贝尔曼方程](@entry_id:138644)，并检验[蒙特卡洛](@entry_id:144354)和[时序差分学习](@entry_id:177975)等基础学习算法，这些算法使智能体能够从其环境中学习。随后，“应用与跨学科联系”部分将连接理论与实践，揭示这些原理如何应用于解决现实世界的强化学习问题，以及它们如何出人意料地统一了来自统计学和[编译器设计](@entry_id:271989)等不同领域的概念。

## 原理与机制

想象一个智能体——一个生物、一个机器人，甚至一个软件——在一个世界中导航。这个世界由不同的情况，即**状态**组成。当我们的智能体从一个状态移动到另一个状态时，它会收集奖励，这些奖励可以是正的（如找到食物）或负的（如撞到墙）。它的旅程并不完全由自己选择；从任何给定的状态，都有一定的概率在下一刻到达任何其他状态。这种简单而强大的世界模型就是我们所说的**马尔可夫奖励过程 (MRP)**。“马尔可夫”部分是一个美妙的简化：要预测未来，你只需要知道当前状态。智能体如何到达这里的全部历史都无关紧要。

我们的目标是从智能体的角度理解这个世界。我们可以提出的最基本的问题是：“处于一个特定状态有多好？”这个看似简单的问题将我们带入一个由深刻而优雅的思想构成的兔子洞，这些思想构成了现代强化学习的基石。

### 状态的价值：一个递归的梦想

我们如何量化一个状态“有多好”？我们可以将其**价值函数**（记为 $V(s)$）定义为智能体从该状态 $s$ 出发预期能够累积的总奖励。但这里有一个问题。如果智能体永远存在，总奖励可能是无限的。为了让事情变得可控，并反映某种“不耐烦”，我们引入一个**[折扣](@entry_id:139170)因子** $\gamma$，一个介于 0 和 1 之间的数字。未来一步收到的奖励只相当于今天收到该奖励的 $\gamma$ 倍。两步之遥的奖励价值为 $\gamma^2$，以此类推。如果 $\gamma$ 接近 1，智能体是“有远见的”；如果接近 0，则是“短视的”。

有了折扣因子，从时间 $t$ 开始的总[折扣](@entry_id:139170)回报 $G_t$ 为：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

状态 $s$ 的价值则是这个回报的[期望值](@entry_id:153208)，前提是我们处于状态 $s$：$V(s) = \mathbb{E}[G_t | S_t = s]$。

现在，奇妙之处来了。我们可以用一种绝妙的递归方式来表达这个价值。总回报 $G_t$ 可以分为两部分：我们立即获得的即时奖励 $R_{t+1}$，以及从下一个状态开始的剩余回报，即 $\gamma G_{t+1}$。

$$
G_t = R_{t+1} + \gamma G_{t+1}
$$

对两边取期望，我们就得到了问题的核心——**[贝尔曼方程](@entry_id:138644)**：

$$
V(s) = \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s]
$$

换句话说，今天处于状态 $s$ 的价值是您将获得的平均即时奖励，加上您明天将进入的状态的折扣平均价值。这个方程在整个[状态空间](@entry_id:177074)中建立了一个[自洽性](@entry_id:160889)条件。每个状态的价值都通过其邻居状态的价值来定义。对于一个已知的 MRP，价值函数是这个线性方程组的唯一解。

例如，在一个简单的双状态世界中，您有概率 $p$ 停留在同一状态，[贝尔曼方程](@entry_id:138644)将价值 $v_A$ 和 $v_B$ 以一种相互依赖的美妙舞蹈联系在一起，这可以通过简单的代数求解 [@problem_id:870814]。

### 全景图：从平均值到不确定性

价值函数告诉我们的是*平均*结果。但平均值可能具有欺骗性。每次都获得+100的回报与50/50的机会获得+200或0的平均值相同，但体验却大相径庭。为了捕捉这一点，我们需要理解回报的*[方差](@entry_id:200758)*。

[贝尔曼方程](@entry_id:138644)的逻辑可以被优美地扩展。[方差](@entry_id:200758)由 $\text{Var}(G_t | S_t=s) = \mathbb{E}[G_t^2 | S_t=s] - (V(s))^2$ 给出。我们已经知道如何找到 $V(s)$。为了找到[方差](@entry_id:200758)，我们只需要二阶矩 $M(s) = \mathbb{E}[G_t^2 | S_t=s]$。我们能为 $M(s)$ 找到一个类似贝尔曼的方程吗？当然可以！我们再次从回报的[递归定义](@entry_id:266613) $G_t = R_{t+1} + \gamma G_{t+1}$ 开始，这次我们对其进行平方：

$$
G_t^2 = R_{t+1}^2 + 2\gamma R_{t+1}G_{t+1} + \gamma^2 G_{t+1}^2
$$

取期望，我们得到了一个二阶矩的[贝尔曼方程](@entry_id:138644)，它将 $M(s)$ 与其后继状态的价值 $V(s')$ 和二阶矩 $M(s')$ 联系起来 [@problem_id:870814]。通过求解这个新的[方程组](@entry_id:193238)，我们可以计算回报的[方差](@entry_id:200758)，从而对与每个状态相关的风险和回报有更丰富的理解。这展示了一个强大的原则：MRP的递归结构不仅让我们能够计算期望，还能计算回报的[高阶矩](@entry_id:266936)，从而解锁了对智能体未来的更深层次的统计视角。

### 在黑暗中学习：当规则未知时

到目前为止，我们一直假设自己是上帝，以上帝视角俯瞰世界，完全了解转移概率和奖励。但如果我们是智能体，被投入一个没有说明书的世界中呢？我们不知道游戏规则。我们必须仅仅通过生活——通过体验[状态和](@entry_id:193625)奖励的轨迹——来学习它们。这是从*规划*到*学习*的转变。我们的目标仍然是估计[价值函数](@entry_id:144750)，但我们不能再直接求解[贝尔曼方程](@entry_id:138644)。相反，我们必须利用我们的经验来引导我们的估计值趋向正确的值。

#### 两个学习者的故事：耐心与急躁

想象两种学习状态价值的策略。

第一种策略是**[蒙特卡洛](@entry_id:144354) (MC) 估计**。它是耐心的缩影。为了估计状态 $s$ 的价值，MC 智能体从 $s$ 开始，经历一个完整的“生命”（一个回合）。在最后，它回顾实际收到的总[折扣](@entry_id:139170)奖励，并说：“嗯，这就是我这次运行的估计值。”在许多这样的生命之后，它将所有观察到的回报取平均，得到对 $V(s)$ 的最终估计。这种方法非常简单且**无偏**——平均而言，其估计是正确的。然而，它存在高**[方差](@entry_id:200758)**的问题。一次异常幸运或不幸的回合会极大地扭曲观察到的回报，而且你必须等到一个回合的最后才能学到任何东西 [@problem_id:3190865]。

第二种策略是**时序差分 (TD) 学习**。TD 智能体既不耐烦又聪明。它只走一步。从状态 $S_t$，它收到一个奖励 $R_{t+1}$ 并到达状态 $S_{t+1}$。然后，它通过结合这个真实的、观察到的奖励和它对下一个状态价值的*当前猜测* $V(S_{t+1})$，为其对 $V(S_t)$ 的估计形成一个“目标”。这个目标是 $R_{t+1} + \gamma V(S_{t+1})$。然后，智能体将其对 $V(S_t)$ 的估计朝这个新目标稍微调整一点。这被称为自举 (bootstrapping)——用一个猜测来更新另一个猜测。

这种自举引入了**偏差**；如果对 $V(S_{t+1})$ 的初始猜测是错误的，目标也会是错误的。然而，TD 学习显著降低了[方差](@entry_id:200758)。它从每一步中学习，并且其更新只依赖于即时的随机奖励和状态转移，而不是构成整个回合的漫长随机事件链。这种偏差-方差权衡是强化学习中的一个核心主题 [@problem_id:3190865]。TD 方法通常更有效，因为它们较低的[方差](@entry_id:200758)使其能够更快地学习，即使它们的估计暂时是有偏的。

#### 弥合差距：学习的[光谱](@entry_id:185632)

我们有两个极端：耐心的、无偏的 MC，和急躁的、有偏的 TD(0)（只看一步之遥）。有中间地带吗？有，而且它是一个被称为**TD($\lambda$)** 的美妙统一。

我们不仅可以向前看一步，还可以向前看两步然后从那里进行自举，或者三步，或者任意步数。TD($\lambda$) 方法巧妙地平均了所有这些不同的前瞻视野。参数 $\lambda$（从 0 到 1）控制这个平均过程。
- 如果 $\lambda=0$，我们得到纯粹的单步 TD(0)。
- 如果 $\lambda=1$，我们实际上得到[蒙特卡洛方法](@entry_id:136978)，即我们一直看到回合的结束。
- 对于介于两者之间的 $\lambda$ 值，我们得到了两者的复杂融合。

这创造了一个平滑的学习算法谱，使我们能够根据需要调整偏差-方差权衡 [@problem_id:3292372]。通过选择一个中间的 $\lambda$，我们通常可以实现比单独使用 MC 或 TD(0) 更低的总误差（均方误差），从而兼得两者的优点。

### 当世界变得太大：近似的艺术

到目前为止，我们的方法都假设我们可以为每个状态 $s$ 存储一个不同的值 $V(s)$。这对于一个有20个状态的迷宫来说是可行的，但是对于国际象棋，其状态数比宇宙中的原子还多，又该怎么办呢？或者一个状态是一组连续关节角度的机器人呢？一张价值表不再可行。

我们必须求助于**[函数近似](@entry_id:141329)**的艺术。我们不再为每个状态学习一个值，而是学习一个*近似*该值的函数。通常，这是一个描述状态的某些特征 $\phi(s)$ 的线性函数：

$$
V(s) \approx v_{\theta}(s) = \sum_{i} \theta_i \phi_i(s) = \Phi(s)^{\top}\theta
$$

我们的任务不再是找到值 $V(s)$，而是找到最好的参数集 $\theta$ 以使我们的近似尽可能准确。我们的指路明灯——[贝尔曼方程](@entry_id:138644)——再也无法对所有状态都完美满足。真实值甚至可能不在我们选择的函数类别之内。

解决方案是找到参数 $\theta$，使我们的近似 $\Phi\theta$ 尽可能接近[贝尔曼方程](@entry_id:138644)所说的*应该*在的位置，即 $T^{\pi}(\Phi\theta)$。这可以被构建为一个投影问题。我们将经贝尔曼更新后的[价值函数](@entry_id:144750)投影回我们能表示的函数空间上。这导出了最优参数的解，通常表示为一个简洁的矩阵方程，称为**投影[贝尔曼方程](@entry_id:138644)**，可以求解以找到最佳拟合的 $\theta$ [@problem_id:2738624]。

### 收敛的稳定嗡鸣

当我们将 TD 学习与[函数近似](@entry_id:141329)相结合时，我们得到了一个强大且可扩展的算法。参数向量 $\theta$ 在每一步都会被 TD 误差推动而更新。但这些更新真的会导向任何合理的结果吗？这个过程是随机的，受到奖励和转移随机性的冲击。

在这里，另一个优美的理论出现了。可以证明，我们随机算法的嘈杂、离散时间的[更新过程](@entry_id:273573)，会伴随一个平滑、连续时间的**[常微分方程](@entry_id:147024) (ODE)** [@problem_id:2738656]。想象一个在碗里滚动的弹珠，不断被随机的轻敲所推动。虽然它的路径是不规则的，但它的平均运动是朝碗底滚去。碗的形状由 ODE 描述，而随机的轻敲就是我们的随机更新。

通过分析这个底层的 ODE，我们可以研究我们学习算法的收敛性。如果该 ODE 有一个单一的、全局稳定的[平衡点](@entry_id:272705)，我们就可以相信我们的参数向量 $\theta$ 最终会收敛到一个固定的解，无论其旅途中的随机噪声如何。这个 ODE 的稳定性取决于描述 MRP 的[基本矩阵](@entry_id:275638)和我们选择的特征，它保证了我们的智能体确实在学习一些有意义的东西，而不仅仅是在参数的海洋中漫无目的地徘徊 [@problem_id:2738656]。这种混乱的[随机过程](@entry_id:159502)和确定性[微分方程](@entry_id:264184)之间的深刻联系，为这些强大的学习方法提供了坚实的理论保证。

