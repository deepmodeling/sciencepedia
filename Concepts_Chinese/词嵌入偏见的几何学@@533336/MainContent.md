## 引言
在人工智能领域，[词嵌入](@article_id:638175)代表了一次巨大的飞跃，它将词语从孤立的符号转变为丰富几何空间中的点。这项创新使计算机能够掌握语义关系，实现诸如著名的“国王 - 男性 + 女性 ≈ 女王”之类的类比推理。然而，这种强大的能力也伴随着一个隐藏的弱点。让模型学习意义的过程，也迫使它们学习偏见。人类语言中充满社会偏见和刻板印象的统计模式，不仅被模型复刻，还常常被放大并固化到模型的基础结构中。本文旨在解决一个关键问题：抽象的偏见是如何具体化为几何结构的？其深远影响又是什么？

为了回答这个问题，我们将首先探讨[词嵌入](@article_id:638175)的“原理与机制”，深入研究[分布假说](@article_id:638229)如何创建一幅语言地图，以及这幅地图又如何不可避免地继承了其源数据的缺陷。我们将揭示社会刻板印象如何演变为几何方向，以及词频等统计特性如何产生其自身的偏见形式。随后，“应用与跨学科联系”一章将展示这些带偏见的[嵌入](@article_id:311541)在现实世界中的影响，追溯它们在医学、金融和电子商务等领域的作用，并揭示偏见的几何学如何在现代人工智能系统中造成可预测的脆弱性。

## 原理与机制

想象一下，语言是一座广阔而杂乱的城市，每个词都是一个地点。有些地方，比如“国王”和“女王”，同在皇家区；另一些地方，比如“走”和“跑”，则是运动区的近邻。计算机怎么可能绘制出这样一幅地图呢？在很长一段时间里，这是不可能的。词语仅仅是任意的符号，就像没有地[图连接](@article_id:330798)的街道名称一样。

[词嵌入](@article_id:638175)改变了一切。它们就是地图。在这幅地图上，每个词都不是二维平面上的一个点，而是高维空间中的一个点——一个向量。这不仅仅是一个巧妙的归档系统，更是一个几何宇宙，词语之间的关系在这里具有了意义。“猫”和“狗”之间的距离很小；从“法国”到“巴黎”的方向与从“意大利”到“罗马”的方向惊人地相似。这就引出了看似神奇的著名“向量算术”：

$$
v_{\text{king}} - v_{\text{man}} + v_{\text{woman}} \approx v_{\text{queen}}
$$

从“男性”移动到“国王”定义了一个代表“王权”的向量。如果我们将这同一个王权向量加到“女性”上，我们就会稳稳地落在“女王”的邻近区域。这就是[词嵌入](@article_id:638175)的美妙与力量所在。但计算机究竟是如何学会绘制这样一幅地图的呢？如果地图的绘制者——数据本身——存在缺陷，又会发生什么？

### 地图绘制者的秘密：观其伴，知其义

这个秘密源于语言学家J.R. Firth提出的一个简单而深刻的观点，即**[分布假说](@article_id:638229)**。它指出：“观其伴，知其义（You shall know a word by the company it keeps）。”一个词的意义并非其固有属性，而是由其周围出现的词语所定义。计算机无法抽象地理解“正义”，但它可以阅读数十亿个句子，并注意到“正义”经常与“法庭”、“法律”、“公平”和“真相”等词语一同出现。它也注意到“正义”很少与“煎饼”或“[粒子加速器](@article_id:309257)”搭配。

像Word2Vec这样的模型本质上是这些共现关系的不懈记账员。它们在浩如烟海的文本上滑动一个小窗口，并学着将共享相似上下文的词语放置在[嵌入空间](@article_id:641450)中的相近位置。那些出现在“政府”和“首都”语境中的词语（如“巴黎”和“罗马”）将被推向空间中的相似区域。

这个简单的原则效果惊人，但它有一个关键的弱点。模型没有常识，只知道它所见过的东西。如果一个短语是习语，其意义具有非[组合性](@article_id:642096)，模型就很容易被迷惑。以短语“spill the beans”（意为“泄露秘密”）为例。模型见过“spill”（洒）与“coffee”（咖啡）和“water”（水）搭配，也见过“beans”（豆子）与“eat”（吃）和“grow”（生长）搭配。基于这些纯粹的分布事实，它可能会得出结论，认为“spill the beans”是一个字面上的物理动作。模型对局部词语上下文的依赖可能导致其只见树木，不见森林，无法领会整个短语的意义并非其各部分之和 [@problem_id:3182857]。这是我们得到的第一个线索：这些模型并非在“思考”，而是在为其所接收的文本中的统计模式创建一个几何镜像。

### 机器中的幽灵：数据的偏见演变为几何的缺陷

如果地图反映了其所描绘的疆域，而这疆域就是我们书写的文本，那么地图将继承我们语言中所有的怪癖、刻板印象和偏见。这就是[词嵌入](@article_id:638175)中偏见的根源。如果我们的历史文本更频繁地将“医生”一词与男性代词关联，将“护士”与女性代词关联，模型就会勤勉地学习这种模式。由此产生的几何结构将编码这种关联。

我们实际上可以将其可视化。想象一下，在[嵌入空间](@article_id:641450)中确定一个“性别方向”，即一个向量 $b$，它从一组与女性相关的词（如“她”、“女人”）的平均位置指向其男性对应词（“他”、“男人”）的平均位置 [@problem_id:3123006]。这个向量并非偶然，而是模型从数据中提炼出的一个切实的意义维度。

当我们取一个像“医生”这样本应“中立”的词，并测量它与这个性别方向的对齐程度时，会发生什么？我们可以用一个简单的[点积](@article_id:309438)来计算，$v_{\text{doctor}}^{\top} b$。如果这个值为正，则“医生”的向量偏向该轴的“男性”一侧；如果为负，则偏向“女性”一侧。在许多标准的、现成的[嵌入](@article_id:311541)模型中，我们发现像“程序员”、“工程师”和“医生”等词的向量带有男性化倾向，而“家庭主妇”、“接待员”和“护士”则带有女性化倾向。社会刻板印象已经被固化到了意义的几何结构之中。我们集体偏见的幽灵，如今在机器中挥之不去。

但模型仅仅是一面被动的镜子，忠实地反映文本的统计数据吗？还是它可能让情况变得更糟？这就引出了一个更微妙、更令人不安的问题：偏见放大。假设原始文本数据显示，男性代词与“程序员”共现的倾向性很小。我们可以测量这一点。然后，我们可以在最终的[嵌入空间](@article_id:641450)中测量“他”和“程序员”的几何接近度。如果几何关联性比原始文本统计数据所显示的*更强*，那么模型就**放大**了偏见。它将一个微小、不易察觉的模式转变为一个更显著的几何特征。这种情况确实会发生。一种量化此现象的巧妙方法是比较模型几何邻域的差异与原始共现数据的差异，这个概念被称为**公平性放大**（Fairness Amplification）[@problem_id:3130235]。模型不仅仅是一面镜子；有时，它是一面哈哈镜，会夸大它所反映的世界的缺陷。

### 频率的暴政：一种更微妙的偏见

并非所有偏见都像社会偏见那样显而易见。有些是学习过程本身的统计产物。其中最重要的之一是**频率偏见**。

想一个非常常见的词，比如“is”或“go”。它出现在数百万种不同的上下文中。在训练期间，每当它出现时，其向量都会得到一次微小的调整。由于它如此常见，它会不断地被各种不同的邻居词调整。这往往会使其向量的长度（即**范数**）增加。相比之下，像“古生物学家”这样的罕见词被更新的频率要低得多。

那么，我们如何衡量两个词的“相似性”呢？我们主要有两个选择。我们可以使用**[点积](@article_id:309438)**，$v_w^{\top} u_c$。这个指标对向量之间的夹角和它们的长度都很敏感。一个范数非常大的向量，即使其夹角不是完美匹配，也可能获得很高的[点积](@article_id:309438)得分。或者，我们可以使用**[余弦相似度](@article_id:639253)**，即[点积](@article_id:309438)除以范数的乘积：$\frac{v_w^{\top} u_c}{\|v_w\| \|u_c\|}$。这个指标完全忽略长度，只考虑向量之间的夹角。

陷阱就在于此。如果高频词具有较大的范数，那么在相似性任务中使用[点积](@article_id:309438)会偏向于选择高频词作为答案，仅仅因为它们的向量更长 [@problem_id:3200061]。模型可能会偏爱一个常见但不太精确的词，而不是一个罕见但[完美匹配](@article_id:337611)的词。

有趣的是，这些模型的设计者意识到了这种“频率的暴政”。他们内置了一种巧妙的防御机制：**子采样**。在训练过程中，[算法](@article_id:331821)会随机丢弃一部分非常高频词的出现。这看起来很浪费，但却是一个绝妙的技巧。它有意地使训练过程产生偏向，实际上是在告诉模型：“‘the’这个词我已经见过一百万次了，别再那么关注它了，多听听那些罕见词。”这有助于防止高频词的范数[失控增长](@article_id:320576)，并给那些更罕见、语义更具体的词语一个机会来发展出更好的表示 [@problem_id:3200047] [@problem_id:3200030]。

### 偏见的架构：各司其职

即便是模型架构的“螺丝钉”也能在区分信号和噪声方面发挥作用。考虑一个简单的[线性模型](@article_id:357202)，它建立在[词嵌入](@article_id:638175)之上进行预测：$z = \mathbf{w}^{\top}\mathbf{e} + b$。这里，$\mathbf{e}$ 是[词嵌入](@article_id:638175)，$\mathbf{w}$ 是一个权重向量，而 $b$ 是一个简单的标量偏置项。人们很容易将 $b$ 忽略为只是一个次要的调整参数。

但它扮演着一个优美而深刻的角色。如果我们对[嵌入](@article_id:311541)进行[预处理](@article_id:301646)，使其均值为零（一种称为均值中心化的常用技术），就会发生一件奇妙的事情。偏置项 $b$ 会学会捕捉我们试图预测的事件的整体、与上下文无关的基准率。例如，如果我们要预测一个句子是否表达积极情绪，并且数据中70%的句子是积极的，那么偏置项 $b$ 会自行调整，以产生一个0.7的基线预测。这样一来，权重向量 $\mathbf{w}$ 就被解放出来，只需专注于学习 $\mathbf{e}$ 中的特定词语特征如何导致情绪*偏离*该基线。架构本身提供了一种自然的方式来解开全局频率偏见（由 $b$ 捕捉）与特定语义信号（由 $\mathbf{w}^{\top}\mathbf{e}$ 捕捉） [@problem_id:3199859]。

### 手术刀式的解决方案？去偏见的希望与风险

如果偏见被编码为几何方向，我们难道不能通过一些几何手术来移除它吗？这是许多去偏见[算法](@article_id:331821)的核心思想。

让我们回到“性别方向”向量 $b$。对于任何词向量，比如 $v_{\text{doctor}}$，我们可以将其分解为两部分：一个沿着性别方向的分量，以及一个与之垂直的分量。去偏见的过程在概念上很简单：只需切掉向量在偏见方向上的投影部分。新的、“去偏见”的向量 $v'_{\text{doctor}}$ 就是剩下的部分：

$$
v'_{\text{doctor}} = v_{\text{doctor}} - (v_{\text{doctor}}^{\top} b) b
$$

这个过程被称为**[零空间](@article_id:350496)投影**，它优雅且高效。经过这次手术后，新的“医生”向量在性别轴上的投影为零。性别化的关联被手术般地移除了 [@problem_id:3123006]。在某些情况下，我们可以使用主成分分析（PCA）等技术自动识别出这个主导的偏见方向，该技术能够找到数据中变化的主要轴线 [@problem_id:3200094]。

但手术从来没有无风险的。语言是一个错综复杂的关联网络。编码“医生是男性”的几何关系，可能也与有用的、无偏见的语义信息交织在一起。当我们切除偏见时，我们是否也损害了地图解决有用类比的能力？答案往往是肯定的。我们经常面临一个权衡：减少偏见可能会以在其他语义任务上性能轻微下降为代价。这揭示了一个深刻的真理：“修复”偏见不是一个简单的技术问题。它是一种复杂的平衡行为，迫使我们决定我们希望模型保留哪些方面的意义，以及我们愿意为创造一个更公平的世界表征付出什么样的代价。

