## 引言
在对世界进行建模的探索中，我们处在两种强大[范式](@entry_id:161181)之间：机器学习的经验性力量，它从海量数据中学习；以及物理定律的理性优雅，它描述了基本原理。当应用于科学问题时，纯数据驱动的“黑箱”模型通常在插值方面取得成功，但在面对新场景时可能会惨败，其预测结果不仅不准确，而且在物理上是荒谬的。这种差距的产生是因为这类模型只学习模式而不理解其背后的原因，缺乏支配自然现象的“物理直觉”。

本文介绍了[物理信息](@entry_id:152556)模型（PIMs），这是一种通过将机器学习与物理原理相融合来弥合这一鸿沟的革命性方法。它解决了科学模型不仅需要感知数据，还需要遵守定律的关键需求。通过阅读本文，您将对这一新兴领域获得全面的理解。第一章“原理与机制”深入探讨了这些模型的工作方式，探索了[物理信息神经网络](@entry_id:145229)（PINNs）中复合[损失函数](@entry_id:634569)的巧妙运用，以及通过架构性[归纳偏置](@entry_id:137419)实现的物理学更深层次整合。随后，“应用与跨学科联系”一章将展示这些模型在不同领域的变革性影响，从求解宇宙的基本方程到揭示生物系统的隐藏动态。

## 原理与机制

### 学习的两种灵魂：数据与定律

想象一下，你想教一台计算机预测一个被抛出的球的轨迹。一种方法——传统的机器学习方法——是向它展示数百万个球在飞行中的视频。在看过足够多的例子后，这台计算机（通常是一个[深度神经网络](@entry_id:636170)）在预测类似条件下抛出的球的路径方面变得非常出色。它学会了从其庞大的经验库中进行插值。这就是从**数据**中学习。

但是，如果给它看一个它从未见过的情境呢？一个重得多的球，一次在月球上的投掷，一个被一阵风卷入的球？纯数据驱动的模型，通常被称为**黑箱**模型，可能会惨败。它可能会预测球会摆动、突然改变方向，甚至向上飞。为什么？因为它从未学过球为什么会以抛物线飞行；它只学会了识别这种模式。它没有重力、动量或空气阻力的概念。它缺乏“物理直觉”。

这是在科学和工程领域应用机器学习时面临的一个深刻挑战。当我们使用神经网络分析科学数据时，我们不希望它仅仅是一个灵活的[曲线拟合](@entry_id:144139)器。我们要求它的预测尊重宇宙的基本定律。例如，在分析材料的[光谱](@entry_id:185632)数据时，一个无约束的模型可能会预测出带有负吸收或违反量子力学规则的[谱线](@entry_id:193408)强度的[光谱](@entry_id:185632)——这些结果不仅是错误的，而且在物理上是荒谬的[@problem_id:2501468]。模型学习了数据，但没有理解物理。

[物理信息](@entry_id:152556)模型诞生于学习的这两种灵魂的结合：**数据**的经验灵魂和物理**定律**的理性灵魂。它们不仅被展示了发生了什么；它们还被教导了*为什么*会发生。这种结合不仅仅是防止了令人尴尬的、不符合物理的预测。它开启了一种更深刻、更稳健、更强大的世界建模方式。

### 教机器物理学：[损失函数](@entry_id:634569)的艺术

那么，我们如何教一个[神经网](@entry_id:276355)络牛顿定律或麦克斯韦方程呢？我们不能让它去读教科书。秘密在于[神经网](@entry_id:276355)络学习的核心：通过最小化一个**[损失函数](@entry_id:634569)**。[损失函数](@entry_id:634569)是衡量[模型误差](@entry_id:175815)的指标，即它对自己表现的“不满意度”。整个训练过程就是寻找使这个损失尽可能小的模型参数。

**[物理信息神经网络](@entry_id:145229)（[PINNs](@entry_id:145229)）**的天才之处在于设计一种特殊的复合[损失函数](@entry_id:634569)，它如同对物理现实的核对清单[@problem_id:3583475]。这个损失函数有几个组成部分：

1.  **数据损失**：这是传统的部分。它衡量网络预测与我们拥有的实际实验数据的匹配程度。如果网络预测温度为50°C，而传感器读数为52°C，这一项就会增加一个惩罚。

2.  **物理损失**：这是革命性的部分。我们将网络的输出——例如，一个描述温度场 $u(x,t)$ 的函数——直接代入控制物理定律，例如[热方程](@entry_id:144435) $\frac{\partial u}{\partial t} - \alpha \nabla^2 u = 0$。如果定律被遵守，这个方程的结果在任何地方都应该是零。任何非零结果都被称为**[偏微分方程](@entry_id:141332)残差**。物理损失就是这个残差的量值，在整个问题域上取平均。通过迫使网络最小化这个损失，我们实际上是在迫使它发现一个满足物理定律的解。

3.  **边界/初始损失**：物理问题不仅由方程定义；它们还受到边界和初始条件的约束。一个合格的[PINN损失函数](@entry_id:137288)应包括惩罚任何偏离这些条件的项。

使这一切成为可能的神奇技术是一种叫做**[自动微分](@entry_id:144512)（AD）**的方法。它与让网络首先从数据中学习的数学机制是相同的。通过AD，我们可以自动高效地计算形成[偏微分方程](@entry_id:141332)残差所需的导数，无论方程或网络多么复杂。这使得PINN框架具有惊人的灵活性，能够处理广泛的物理现象。网络在其对最小损失的不懈追求中，同时学会了拟合我们拥有的[稀疏数据](@entry_id:636194)点，并在它们之间广阔的空白区域编织出一个物理上一致的解。

### 不只是惩罚：将物理学编织进模型的结构中

在损失函数中增加一个物理惩罚项，就像老师批改学生的作业。这很有效，但如果我们能设计一个天生就倾向于用物理语言思考的学生呢？这种更深层次的整合是通过所谓的**[归纳偏置](@entry_id:137419)**实现的：将物理原理直接构建到模型的架构中。

考虑这样一个问题：预测半径为 $R$ 的球形针尖压入软材料深度为 $\delta$ 时的力 $F$ [@problem_id:2777675]。[接触力学](@entry_id:177379)定律告诉我们，对于弹性材料，力遵循一个特定的标度律：$F \propto R^{1/2} \delta^{3/2}$。这是关于相互作用基本几何形状的陈述。一个“物理小白”模型将不得不从头开始学习这种关系，需要大量关于不同半径和压入深度的数据。

一种更复杂的方法是将这个定律构建到模型本身中。我们可以设计网络，使其关于这种缩放是**等变的**。这意味着，如果我们告诉模型压入深度加倍，它*自动*知道力必须增加 $2^{3/2}$ 倍，而无需在该特定示例上进行训练。这类似于[物理学中的对称性](@entry_id:144576)概念。通过编码这些基本原理——从标度律和因果关系到像**[被动性](@entry_id:171773)**（材料不能无中生有地创造能量）这样的[热力学约束](@entry_id:755911)——我们创建了一个模型，它不仅学习一个解，而且学习了底层物理学的*语法*。

这使得模型具有非凡的泛化能力。它们可以在其训练数据之外的条件下做出准确的预测，因为它们受到支配系统的持久物理定律的指导。这个想法的最终体现是在**[算子学习](@entry_id:752958)**中，其目标不再是学习从一组数字到另一组数字的映射，而是学习物理*算子*本身——例如，将材料属性映射到最终温度场的数学算子[@problem_id:3513285]。像[傅里叶神经算子](@entry_id:189138)这样的架构是特别优美的例子，因为它们在傅里叶空间中执行卷积的内部结构对于许多物理系统来说是一种天然的[归纳偏置](@entry_id:137419)。

### 知识的果实：更优的预测与诚实的不确定性

这种物理学的深度整合从根本上改变了科学建模中的权衡。经典的数值方法，如[有限差分法](@entry_id:147158)，依赖于将空间和时间划分成越来越精细的网格来减少[离散化误差](@entry_id:748522)。纯数据驱动的模型需要海量数据集来减少[估计误差](@entry_id:263890)。[PINNs](@entry_id:145229)开辟了第三条道路[@problem_id:3109322]。它们通常可以用[稀疏数据](@entry_id:636194)实现高精度，因为物理损失在整个域上提供了一个密集、连续的信息源，在没有数据的地方引导解。

此外，一个真正优秀的科学模型不仅仅给出一个答案；它还告诉我们它对这个答案有多自信。这就是**[不确定性量化](@entry_id:138597)**的领域。在任何预测中，都有两种不确定性[@problem_id:3197079][@problem_id:3513334]：

*   **[偶然不确定性](@entry_id:154011)**：这是世界固有的随机性或噪声。它是测量误差或系统内在随机性的不可约减的“迷雾”。更多的数据不会让它消失。
*   **认知不确定性**：这是由于缺乏知识而产生的不确定性。它源于数据量有限或模型不完美。这是我们可以通过更多学习来减少的不确定性。

物理定律是减少[认知不确定性](@entry_id:149866)的强大工具。我们融入模型的每一条物理知识，都像获得了一大批高质量、无噪声的数据。通过约束可能解的空间，物理学使模型对其预测更加确定。例如，在一个简单的模型中，知道当输入为零时输出必须为零，就消除了模型参数中一整个维度的不确定性，从而在各处都得到更精确的预测[@problem_id:3197079]。当然，一个真正诚实的模型还必须承认我们的物理定律本身可能是近似的，这是一种被称为**模型形式差异**的不确定性来源[@problem_id:3513334]。

这个新[范式](@entry_id:161181)也为我们提供了一种更结构化的方式来思考我们模型的总误差[@problem_id:3513306]。最终误差是三种来源的复合体：**近似误差**（我们的模型架构能否表示真实解？）、**[估计误差](@entry_id:263890)**（我们是否有足够的数据和物理点来在我们架构内找到最佳可能解？），以及**优化误差**（我们的训练算法是否成功找到了那个最佳解？）。物理信息约束主要攻击的是[估计误差](@entry_id:263890)，它提供了关键的指导，减少了模型对观测数据的单独依赖。

### 搭建桥梁：复杂系统的新[范式](@entry_id:161181)

PINNs的原理不仅仅是学术上的好奇心；它们构成了一个灵活而强大的[范式](@entry_id:161181)，用于解决以前难以处理的真实复杂问题。一个极好的例子来自对经典数值方法中一个旧思想的改造：**区域分解**[@problem_id:3351998]。

想象一下，试图模拟水流通过复杂地质结构，其中多孔沙土区域紧挨着坚硬的花岗岩。物理特性从一个区域到另一个区域急剧变化。一个单一、庞大的[神经网](@entry_id:276355)络将难以同时学习这些截然不同的行为——这是一个“刚度”问题。

区域分解方法优雅而简单：不要使用一个巨大的网络，而是使用几个较小的、专门化的网络。为“沙土”子域训练一个PINN，为“花岗岩”子域训练另一个。每个网络可以专注于学习其自身区域的特定物理学。但我们如何确保最终的解是一致的呢？物理学再次提供了答案。在沙土和花岗岩之间的界面上，基本的[守恒定律](@entry_id:269268)规定水压必须是连续的，并且水的总通量必须守恒。这些物理[界面条件](@entry_id:750725)成为额外的损失项，将这些专门的网络“缝合”在一起，形成一个单一、全局一致且高度准确的解。这就像建造一座大型复杂的桥梁，不是一次性浇筑完成，而是设计不同的部分，然后使用精确的蓝图——物理定律——将它们完美地连接起来。

从防止不符合物理的预测，到通过深层对称性实现零样本泛化，再从量化不确定性到处理多尺度系统，物理定律与机器学习的融合代表了一次真正的[范式](@entry_id:161181)转变。它正在创造新一代的科学模型，这些模型不仅更准确、更高效，而且更稳健、更具[可解释性](@entry_id:637759)，并最终更符合我们对世界的基本理解。

