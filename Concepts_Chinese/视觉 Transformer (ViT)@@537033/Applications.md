## 应用与跨学科联系

在详细了解了视觉 Transformer 复杂的内部机制之后，我们可能会问自己一个简单的问题：这一切到底有什么用？我们已经看到了那些齿轮和传动装置——图像块、[嵌入](@article_id:311541)、[注意力头](@article_id:641479)——但现在我们必须看看这个引擎运转起来的样子。一个科学思想的真正美妙之处不仅在于其内在的优雅，还在于它能解释的世界有多广阔，以及它让我们能够构建怎样的新世界。事实证明，视觉 Transformer 远不止是另一个识别猫狗的工具。它代表了我们思考数据、关系乃至科学发现过程本身方式的根本转变。

### 超越旧有局限：全局视角的威力

几十年来，[计算机视觉](@article_id:298749)领域的王者一直是[卷积神经网络 (CNN)](@article_id:303143)。它们的成功建立在一个简单、强大且受生物学启发的理念之上：局部[特征检测](@article_id:329562)。CNN 通过一系列小窗口观察图像，首先识别边缘和纹理等简单模式，然后将它们组合成眼睛或轮子等更复杂的形状。这种分层的、从局部到全局的过程非常有效，但它有一个根本性的局限——一种“局部的束缚”。CNN 通过其直接的邻域来理解世界。

为了理解这意味着什么，想象一个简单的游戏。我给你看一张包含几对圆点的图像。每一对中的两个圆点共享同一种独特的颜色，但它们在画布上相距很远。任务是数出有多少对圆点。对我们来说，这轻而易举。但对于一个经典的 CNN 来说，这几乎是不可能的。CNN 的小窗口滑过图像，在这里看到一个彩色的点，在那里看到另一个，但它没有内在的机制将左上角的红点与其右下角的伙伴联系起来。它很可能会把每个点都算作一个独立的对象，从而彻底失败。

然而，视觉 Transformer 却觉得这个游戏易如反掌。正如我们所学，ViT 首先将图像切成一组图像块，或称词元。然后，通过[自注意力](@article_id:640256)的魔力，*每个词元都可以直接与所有其他词元通信*。左上角的图像块词元不限于与其邻居交谈；它可以立即查询右下角的词元并问道：“你和我颜色一样吗？”这种全局感受野使 ViT 能够毫不费力地发现这些远程依赖关系，正确地配对远处的两半，从而解决这个难题 ([@problem_id:3199150])。这个简单的思想实验揭示了 ViT 的核心超能力：它能同时看到森林*和*树木。

### 超越简单视觉：分割与交互

这种全局视角开启了远超[简单图](@article_id:338575)像分类的能力。考虑一下**[语义分割](@article_id:642249)**任务，其目标不是给整个图像分配一个标签，而是为*每一个像素*分配一个标签。我们不仅想知道图像中有一辆“汽车”，还想确切地知道哪些像素属于汽车，哪些属于道路，哪些属于天空。

ViT 天然适合这项任务。因为它已经以图像块的方式思考，我们可以让一个简单的解码器来解释每个图像块词元丰富的、包含上下文信息的[嵌入](@article_id:311541)，并为其分配一个类别标签。[自注意力机制](@article_id:642355)确保了汽车引擎盖上一个图像块的标签，会受到汽车轮子上一个图像块的信息影响，即使它们位于图像的两端。我们甚至可以通过可视化其注意力图来窥探模型的“思维”。我们常常发现，注意力的模式——哪些图像块在与哪些“交谈”——形成了一种原始的分割，为我们提供了模型在做出最终决定之前是如何解析场景的线索 ([@problem_id:3136246])。

但如果我们能让这个过程实现交互呢？如果模型不仅能被动分类，还能响应我们的引导呢？这就是**可提示的视觉模型**背后的革命性思想。想象一下，在一个杂乱的场景中指向一个物体，模型就能立即分割出那个物体。这是通过用*[交叉注意力](@article_id:638740)*扩展 ViT 架构来实现的。

在这种设置中，我们引入了不来自图像的新的“提示词元”。提示词元可以代表你点击的一个点、你画的一个[边界框](@article_id:639578)，甚至是一个文本查询，如“那条狗”。然后，图像块词元执行[交叉注意力](@article_id:638740)：它们不再相互查询，而是查询提示词元。这使得模型能够将信息从你的提示“路由”到图像的相关部分。包含你点击像素的图像块会大喊：“用户对我很感兴趣！”，然后通过[注意力机制](@article_id:640724)，属于同一物体的其他图像块会“听到”并成为最终分割的一部分。这将 ViT 从一个静态分析器转变为一个动态的协作工具，这个概念在一个简化模型中得到了精彩的展示 ([@problem_id:3199142])。

### 词元的宇宙：从图像到科学

这里我们来到了 ViT 设计最深远的结果。[自注意力机制](@article_id:642355)的核心是一种用于在*一组词元*中寻找关系的[算法](@article_id:331821)。它实际上并不关心这些词元是否来自图像。任何可以由数值[向量表示](@article_id:345740)的东西都可以成为一个词元。这一认识打破了领域之间的界限，将 Transformer 转变为一个通用的科学探究引擎。

*   **动态图像 (视频分析):** 最自然的扩展是从静态图像到视频。视频只是一系[列图像](@article_id:311207)。我们可以通过将每一帧切成图像块，然后将所有图像块序列串联起来，一帧接一帧，从而对其进行词元化。将 ViT 应用于这个长长的[时空](@article_id:370647)词元序列，现在不仅可以询问空间关系，还可以询问时间关系。它可以学习将一帧中的一个图像块与下一帧中的一个图像块关联起来，从而理解运动和变化 ([@problem-LEC_ID:3199225])。模型会动态分配其注意力，有时专注于单帧内的[空间模式](@article_id:360081)，有时则专注于跨帧的时间变化，具体取决于内容。

*   **第三维度 (体数据):** 为什么要止步于二维？许多科学数据集本质上是三维的，例如医学中的 MRI 和 CT 扫描。我们可以通过将三维体数据词元化为一个由小立方体或“体素”组成的网格来调整 ViT。然而，这带来了一个艰巨的工程挑战。[自注意力](@article_id:640256)的成本随着词元数量 $N$ 的平方 ($N^2$) 增长。对于高分辨率的三维体数据，$N$ 会变得极其巨大，存储 $N \times N$ 注意力矩阵所需的内存甚至可能超过超级计算机的处理能力。

    这正是科学创造力与工程智慧相遇的地方。我们可以使用**轴向注意力**，而不是一次性计算所有词元对之间的注意力。这个想法是沿每个轴顺序执行注意力：首先，所有词元关注 x 轴上的其他词元；然后，它们关注 y 轴；最后，关注 z 轴。这种分解将计算成本从与 $N^2$ 成正比，显著降低到大致与 $N \cdot (n_x + n_y + n_z)$ 成正比，其中 $n_x, n_y, n_z$ 是沿每个维度的词元数量。这个巧妙的技巧使得将 Transformer 的强大能力应用于高维科学数据成为可能 ([@problem_id:3199168])。

*   **我们的星球如拼图 ([气候科学](@article_id:321461)):** 让我们把视野放得更远。我们可以将整个地球表面表示为一个词元网格，每个词元的[嵌入](@article_id:311541)包含温度、压力和湿度等信息。[气候科学](@article_id:321461)家早就知道**遥相关**的存在——即远程相关的天气模式，例如，太平洋海面温度异常（厄尔尼诺现象）会影响数千公里外的北美天气。通常依赖局部相互作用的传统模型很难捕捉这些现象。而视觉 [Transformer](@article_id:334261) 凭借其全局[注意力机制](@article_id:640724)，天然适合发现和建模这类远程依赖关系 ([@problem_id:3199147])。通过分析 ViT 的注意力图，我们有可能在地球复杂的气候系统中发现新的、以前未知的联系。

*   **向抽象思维的飞跃 (关系推理):** 最后的飞跃是进入纯粹的抽象领域。如果词元代表的不是物理位置，而是概念呢？想象一组物体，每个物体都有 `shape` 和 `color` 等属性。任务是找出“另类”。ViT 可以通过将这些抽象属性[嵌入](@article_id:311541)到词元中来解决这个问题。通过[自注意力](@article_id:640256)，它可以根据相关属性（例如 `shape`）比较所有物体，而忽略无关的干扰项（例如 `color`）。从其同伴那里获得最少注意力的物体，根据定义，就是那个另类 ([@problem_id:3199180])。这表明 Transformer 架构不仅仅是一台感知机器；它是一个通用的**关系推理引擎**。

### 构建更好的 Transformer：训练与信任

这种令人难以置信的能力和通用性并非没有代价。构建、训练和部署这些大型模型带来了它们自身的严峻挑战，这也 thúc đẩy了进一步的创新。

*   **向教师学习 ([知识蒸馏](@article_id:642059)):** ViT 是出了名的数据饥渴。从头开始训练一个 ViT 需要庞大的数据集。一个巧妙的解决方案是**[知识蒸馏](@article_id:642059)**，这个过程类似于学徒制。我们可以用一个更小、数据效率更高的 ViT “学生”模型，不仅在原始数据上训练它，还让它模仿一个更大的、[预训练](@article_id:638349)好的“教师”模型（甚至可以是一个强大的 CNN）的输出分布。教师提供的“软标签”比简单的对/错答案信息更丰富，引导学生更好地理解数据的细微差别 ([@problem_id:3199218])。

*   **站在巨人肩膀上 ([迁移学习](@article_id:357432)):** 大多数从业者不会从头开始训练 ViT。相反，他们会使用由大型研究实验室[预训练](@article_id:638349)的模型，并将其调整到自己的特定任务上。这被称为**[迁移学习](@article_id:357432)**。但如何最好地调整它呢？一种方法是进行**线性探查**，即冻结整个[预训练](@article_id:638349)的 ViT，只在其输出[嵌入](@article_id:311541)之上训练一个新的、简单的分类器。另一种方法是进行**完全微调**，允许 ViT 的所有权重都被更新，尽管幅度很小。最佳策略取决于[预训练](@article_id:638349)[嵌入](@article_id:311541)的质量。如果模型已经学习到一个表示空间，其中不同类别已经被很好地分开了，那么一个简单的线性探查就能产生奇效。如果空间更纠缠，就需要更广泛的微调来调整表示本身 ([@problem_id:3199207])。

*   **一句提醒 (对抗性脆弱性):** 最后，我们必须以一种健康的科学怀疑态度来对待这些模型。它们的复杂性可能隐藏着微妙的脆弱性。最令人震惊的发现之一是它们对**[对抗性攻击](@article_id:639797)**的脆弱性。人们可以对输入图像进行微小的、通常人类无法察觉的改动，从而导致模型做出完全错误的预测。一个高度简化的 ViT 的思想实验揭示了这种情况是如何发生的：一个精心制作的扰动可以操纵注意力分数，有效地“劫持”模型的焦点，并引导它得出错误的结论 ([@problem_id:3199208])。虽然这是一个活跃的研究领域，有许多提议的防御措施，但它作为一个至关重要的提醒，即我们强大的工具并非万无一失。了解它们的失败模式与庆祝它们的成功同等重要。

从一个简单的数点游戏到模拟地球的气候，视觉 [Transformer](@article_id:334261) 带我们踏上了一段非凡的旅程。它向我们展示，通过重新思考一个核心假设——视觉的局部性——我们可以创造出一个其应用仅受限于我们的想象力以及我们将世界表示为一组等待被发现关系的词元的能力的架构。