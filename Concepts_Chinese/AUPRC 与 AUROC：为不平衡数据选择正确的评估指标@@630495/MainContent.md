## 引言
在评估分类模型时，原始准确率可能具有危险的误导性，尤其是在处理[类别不平衡](@entry_id:636658)的数据时（即一个类别的样本数量远超另一个类别）。一个简单地预测多数类别的模型可以获得很高的准确率，但对于识别罕见的、关键的事件却完全无用。这凸显了简单评估方法中的一个根本缺陷：我们需要能够评估模型有效区分不同类别能力的指标，而不仅仅是做出单一正确猜测的能力。解决方案在于评估模型预测的整个排序输出，这催生了更精细的工具。

本文探讨了用于此目的的两种最强大的指标：[受试者工作特征曲线下面积](@entry_id:636693) ([AUROC](@entry_id:636693)) 和[精确率-召回率曲线](@entry_id:637864)下面积 (AUPRC)。您不仅将了解这些指标是什么，更重要的是，它们在揭示模型性能方面讲述了哪些不同的故事。接下来的章节将引导您做出这一关键选择：

*   **原理与机制：** 我们将剖析 [AUROC](@entry_id:636693) 和 AUPRC 的机制，解释它们是如何构建的，以及为什么它们看似微小的差异会导致截然不同的结论，尤其是在面对[类别不平衡](@entry_id:636658)时。
*   **应用与跨学科联系：** 我们将考察从药物发现到金融领域的真实世界场景，展示选择合适的指标对于做出有意义的科学和商业决策是何等关键。

## 原理与机制

想象一下，您是一家大型[公共卫生](@entry_id:273864)诊所的医生。针对一种罕见但严重的疾病，一种可能挽救生命的新筛查测试被开发出来。在人群中，每 1000 人中只有 50 人患有此病。您的任务是评估这项新测试的效果如何。

您对 1000 人进行了一项试验并得到了结果。该测试共正确分类了 910 人。这意味着 91% 的**准确率**。听起来很了不起，不是吗？但请等一下。让我们考虑一个“懒惰的医生”，他根本不做任何测试，只是宣布每个人都健康。由于 1000 人中有 950 人确实是健康的，这位懒惰医生的“测试”准确率高达 95%！这是一个令人困惑的结果。一个完全不寻找患病患者的方法，其准确率似乎比我们精密的新测试还要*高*。

这个简单的故事揭示了在不[平衡问题](@entry_id:636409)中使用原始准确率的一个深层缺陷，即一个类别（健康人群）的数量远超另一个类别（患病人群）[@problem_id:3147839]。准确率分数完全由多数类别主导；它几乎注意不到我们是否找到了少数关键的正例。我们需要一种更精细的方式来衡量性能，一种关注分类器区分两个群体能力的指标。

### 超越单一猜测：评判排序

一个好的诊断工具很少给出简单的“是”或“否”。相反，它提供一个分数——一种[置信度](@entry_id:267904)或风险的度量。高分可能意味着“高风险”，低分则意味着“低风险”。这使得我们，作为科学家或医生，能够将每个人从最可能到最不可能患病进行排序。于是，根本问题就变成了：这个*排序*的效果如何？患病的人是否总是比健康的人获得更高的分数？

这种视角的转变——从评估单个是/否决策转变为评估整个排序列表——是解锁更强大指标的关键。其中最重要的两个是[受试者工作特征曲线下面积](@entry_id:636693)（**[AUROC](@entry_id:636693)**）和[精确率-召回率曲线](@entry_id:637864)下面积（**AUPRC**）。这些指标的一个优美特性是，它们关注的是分数的*顺序*，而不是其确切值。你可以对所有分数取对数，或者将它们转换为简单的排名；只要相对顺序得以保留，[AUROC](@entry_id:636693) 和 AUPRC 就不会改变 [@problem_id:3331692]。它们衡量的是分类器分离类别的某种根本能力。

### ROC 曲线的视角：两个比例的故事

让我们构建第一个工具，即[受试者工作特征](@entry_id:634523)（ROC）曲线。它是一个讲述权衡取舍故事的图表。我们绘制两个简单的量：

1.  **[真阳性率](@entry_id:637442) (TPR)**：在纵轴上，我们问：“在所有*实际患病*的人中，我们的测试标记了多大比例？” 这也被称为**召回率 (Recall)** 或灵敏度 (Sensitivity)。TPR 为 1.0 意味着我们找到了每一个病人。

2.  **[假阳性率](@entry_id:636147) (FPR)**：在[横轴](@entry_id:177453)上，我们问：“在所有*完全健康*的人中，我们的测试错误地标记了多大比例为病人？”

我们通过在患者的排序列表上滑动决策阈值来生成这条曲线。如果我们将阈值设置得非常高，我们只会标记得分最高的个体。我们将捕捉到很少的人，因此我们的 TPR 和 FPR 都将接近于零。随着我们降低阈值，我们开始标记更多的人。我们的 TPR 增加了，因为我们捕捉到了更多的病人，但我们的 FPR 也增加了，因为我们不可避免地会犯更多的错误并标记一些健康人。曲线描绘了从 $(0,0)$ 到 $(1,1)$ 的路径。

**ROC [曲线下面积](@entry_id:169174) ([AUROC](@entry_id:636693))** 是一个从 0 到 1 的单一数字，它总结了整个权衡过程。它有一个非常直观的解释：[AUROC](@entry_id:636693) 是模型将一个随机选择的正例（病人）的分数排在一个随机选择的负例（健康人）之前的概率 [@problem_id:3297889]。[AUROC](@entry_id:636693) 为 1.0 意味着完美分离——每个病人的排名都高于每个健康人。[AUROC](@entry_id:636693) 为 0.5 意味着排序不比随机猜测好。

现在，请注意 TPR 和 FPR 定义中一些微妙但关键的地方。TPR 是*正例*的一个分数。FPR 是*负例*的一个分数。每个都通过其自身群体的规模进行归一化。因此，ROC 曲线的形状——以及 [AUROC](@entry_id:636693)——是众所周知的**与类别流行率无关**。如果你的测试具有某种区分病患与健康者的内在能力，无论疾病是罕见的（千分之一）还是常见的（二分之一），其 [AUROC](@entry_id:636693) 都将是相同的 [@problem_id:3147829]。这种稳定性似乎是一个理想的属性，但正如我们将看到的，它隐藏着一个危险的秘密。

### 不平衡的隐藏灾难

让我们回到医生的困境，但使其对于[全基因组](@entry_id:195052)筛选或欺诈检测等场景更加现实。想象一下，你正在人类基因组中搜索一个特定的 DNA 结合基序。可能有几千个真正的结合位点（正例）散布在三十亿个碱基对（负例）中。这个问题是极其不平衡的 [@problem_id:3297889]。

假设你开发了一个 [AUROC](@entry_id:636693) 高达 0.95 的模型。你对此感觉很棒！你决定为模型设置一个阈值，对应于仅为 0.01 的极小[假阳性率](@entry_id:636147)。这意味着你只错误地标记了 1% 的负例序列。听起来很棒，对吧？

但是让我们看看绝对数字。比如说，如果你有 100 万个负例序列，1% 的 FPR 意味着你有 $0.01 \times 1,000,000 = 10,000$ 个[假阳性](@entry_id:197064)！现在，假设在同一阈值下，你的[真阳性率](@entry_id:637442)是 0.9，而你需要找到 100 个真正的结合位点。你成功找到了其中的 $0.9 \times 100 = 90$ 个。

你最终交给生物学家同事进行昂贵实验验证的候选位点列表包含 90 个真位点和 10,000 个假警报。这个列表中任何一个给定站点是真实的的几率被称为**[精确率](@entry_id:190064) (Precision)**。在这里，[精确率](@entry_id:190064)是 $\frac{TP}{TP + FP} = \frac{90}{90 + 10,000} \approx 0.0089$。不到 1%！

这就是隐藏的灾难。[AUROC](@entry_id:636693) 高达 0.95，但模型的实际效用却糟透了。ROC 曲线最大的优点——其[对流](@entry_id:141806)行率的[不变性](@entry_id:140168)——在这种情况下变成了其最大的弱点。通过将[假阳性](@entry_id:197064)表示为相对于庞大负例库的一个*比率*，它掩盖了一个事实：一个微小的比率可以转化为一个数量上极其庞大的假警报。

### [精确率-召回率曲线](@entry_id:637864)：一个诚实的仲裁者

这就是我们的第二个主角——**[精确率](@entry_id:190064)-召回率（PR）曲线**——登场的地方。它绘制的不是 TPR vs. FPR，而是：

1.  **[精确率](@entry_id:190064) (Precision)**：在纵轴上，它问：“在我们的测试*标记为阳性*的所有实例中，有多大比例*实际上*是阳性？”

2.  **召回率 (Recall)**：在横轴上，它问和以前同样的问题：“在所有*实际上是阳性*的实例中，我们标记了多大比例？”（召回率与 TPR 相同）。

PR 曲线讲述了一个不同的，并且通常更具相关性的故事。在高召回率（当我们试图找到几乎所有病人时）时，我们通常不得不大幅降低标准，以至于标记了许多健康人，因此[精确率](@entry_id:190064)会很低。在低召回率（当我们只标记最明显、得分最高的病例时），我们希望我们的[精确率](@entry_id:190064)非常高。

关键的区别在于[精确率](@entry_id:190064)的定义：$\frac{TP}{TP + FP}$。它不是通过负例总数来归一化的。它直接将[真阳性](@entry_id:637126)的数量与[假阳性](@entry_id:197064)的数量进行对比。它极其诚实地反映了你预测中的[信噪比](@entry_id:185071)。

**PR 曲线下面积 (AUPRC)** 总结了这种权衡。与 [AUROC](@entry_id:636693) 对随机分类器有固定的 0.5 基线不同，AUPRC 的基线就是正类的流行率 $\pi$ [@problem_id:3331692]。如果你的数据中只有 1% 是正例，一个随机[模型平均](@entry_id:635177)会有 1% 的[精确率](@entry_id:190064)，给出的基线 AUPRC 为 0.01。这使得 AUPRC 值立即可解释：在一个基线为 0.01 的问题中，0.3 的 AUPRC 告诉你，你的模型比随机猜测提供了 30 倍的富集。

这种[对流](@entry_id:141806)行率的依赖不是一个缺陷，而是其最重要的特性。通过在其基线中反映正类的稀有性，PR 曲线为不平衡任务的性能提供了一个[信息量](@entry_id:272315)远为丰富的图景 [@problem_id:3331731]。一个在 ROC 曲线上看起来很棒的模型，如果它产生了太多的[假阳性](@entry_id:197064)，那么在 PR 曲线上会看起来逊色得多，因为它的[精确率](@entry_id:190064)会骤降。

### 当优化一个指标损害另一个时

你可能会认为，一个在 [AUROC](@entry_id:636693) 上表现更好的模型，在 AUPRC 上也总会更好。但世界比这更微妙、更有趣。有可能在“提升”一个模型的 [AUROC](@entry_id:636693) 的同时，*摧毁*它的 AUPRC。

考虑两个模型的故事 [@problem_id:3182575]。模型 A 非常保守。它学会以极高的[置信度](@entry_id:267904)识别一小部分正例，并且关键的是，在这一群体上几乎不犯错。它对前几个预测实现了完美的[精确率](@entry_id:190064)，为 PR 曲线提供了一个很好的开端，从而获得了一个不错的 AUPRC。

现在，一位数据科学家“调整”了该模型，创建了模型 B。这个新模型学习了一个更宽泛的规则，成功地识别了更多的[真阳性](@entry_id:637126)，将它们推到了排序列表的顶部。这改善了正例和负例的整体分离，因此它的 **[AUROC](@entry_id:636693) 增加了**。然而，这个新的、更宽泛的规则并不完美。它也错误地给一小部分负例打了高分——比如说，其中的 2%。

在一个平衡的世界里，这无关紧要。但在我们这个罕见事件的世界里，负例与正例的比例是 999 比 1，那 2% 的负例在排序列表的顶端造成了大量的假警报。模型 A 所享有的完美[精确率](@entry_id:190064)消失了。模型 B 的 PR 曲线现在从一个低得多的点开始。尽管它在整体排序上更好（更高的 [AUROC](@entry_id:636693)），但它在最置信预测上的性能已经被污染了。它的 **AUPRC 骤降**。

这鲜明地说明了 [AUROC](@entry_id:636693) 和 AUPRC 衡量的是不同的东西。[AUROC](@entry_id:636693) 衡量的是全局的、成对的区分能力。AUPRC 由于其对[精确率](@entry_id:190064)的关注，更加侧重于排序列表顶端的性能——而这正是在大海捞针时最重要的事情。

### 选择你的衡量标准

那么，你应该使用哪个指标呢？答案，就像科学中所有好的答案一样，是：这取决于你的问题。

如果你的类别是平衡的，或者你同样关心在整个 TPR 和 FPR 范围内的性能，[AUROC](@entry_id:636693) 是一个稳健且稳定的指标。

但是，如果你正在处理一个存在显著[类别不平衡](@entry_id:636658)的问题，并且目标是高[置信度](@entry_id:267904)地识别罕见的正例，那么 AUPRC 就是你诚实的仲裁者。它直接回答了这个实际问题：“当我的模型将某样东西标记为重要时，它正确的可能性有多大？” 在从[医学诊断](@entry_id:169766)、药物发现到欺诈检测和天文学等领域，假警报的代价很高，搜索空间巨大，PR 曲线提供了更有意义，并最终更有用的成功衡量标准。

