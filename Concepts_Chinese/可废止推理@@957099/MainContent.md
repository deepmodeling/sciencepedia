## 引言
在形式数学和计算机科学的世界里，我们常常在[经典逻辑](@entry_id:264911)令人满意的确定性中进行操作，其中结论一旦被证明，便永远为真。然而，我们的日常生活很少提供这种可预测性。我们不断地制定计划、形成信念，并根据不完整且可能变化的信息采取行动——一趟被取消的火车、一个意料之外的诊断、或一个法律上的例外。这种僵化的单调逻辑与人类认知的流动现实之间的鸿沟，催生了对一种不同推理方式的需求。本文探讨的便是可废止推理，这是一种常识的形式化框架，它允许我们做出合理的假设，优雅地修正我们的信念，并在充满不确定性的世界中航行。这种逻辑将修正视为智能的基本特征，而非错误。

本文的探讨分为两个主要部分。首先，在“原则与机制”部分，我们将深入探讨将可废止推理与其经典对应物区分开来的核心概念，审视默认规则、例外以及关于未知事物的[推理机](@entry_id:154913)制。随后，在“应用与跨学科联系”部分，我们将见证这种强大逻辑的实际作用，发现它对医学、人工智能和科学发现哲学等不同领域产生的深远影响。

## 原则与机制

如果你曾上过逻辑学、数学或计算机科学的课程，那么你一定接触过一种非常优美且强大的思维方式。那是一个充满确定性的世界。如果我们知道“所有人都终有一死”且“Socrates 是人”，我们就能以不可动摇的信心得出结论：“Socrates 终有一死”。这就是**[经典逻辑](@entry_id:264911)**的世界。其最显著的特征是**单调性**（monotonic）。这是一个比较专业的说法，意思是：一旦你知道了某件事，你就永远知道了它。向你的事实库中添加新信息只会增加你可以证明的事项列表；它永远不会迫使你收回任何已有的结论。如果你后来得知“Socrates 有胡子”，你关于他终有一死的结论仍然不受影响。

这种推理方式是数学和大部分计算机编程的基石。它严谨、可靠，非常适合构建要求绝对可预测性的系统。然而，如果你停下来想一想，就会发现这并不是我们人类在日常生活中导航的方式。

想象一下你正在计划一次旅行。你查了火车时刻表，上面写着去伦敦的火车上午 10:00 出发。你形成了一个信念：“我的火车上午 10:00 开。”你可能会把它写在日历上，设好闹钟，并围绕它安排你早上的活动。但接着，你听到一则关于铁路罢工的新闻报道。突然间，你的信念岌岌可危。你查询了交通部门的网站，网站确认所有火车都已取消。你必须放弃或*撤销*你最初的结论。你添加了新的信息（“发生了罢工”），而这不仅仅是增加了你所知的内容——它使你原以为是真的事情变得无效了。

简而言之，这就是**可废止推理**（defeasible reasoning）的世界。它是常识的逻辑，是医学的逻辑，是法律的逻辑，也是生活本身的逻辑。它是一种利用不完整信息进行推理的方式，允许我们做出合理的假设，并在出现新的、更好的信息时优雅地修正我们的信念。它是一种将不确定性视为世界基本特征而非缺陷的逻辑。

### “鸟会飞”原则：默认与例外

在人工智能领域，用于探索可废止推理的最早也是最著名的例子之一，是一个简单的陈述：“鸟会飞”。[@problem_id:3037582] 这个陈述是真的吗？嗯，基本上是真的。作为一个普遍的[经验法则](@entry_id:262201)，或者逻辑学家所说的**默认规则**（default rule），它运作得非常好。如果有人告诉你他们看到一种叫做“Zzzyx”的新鸟，你会很自然地假设它会飞。你正在应用默认规则。

但接着，有人告诉你：“啊，但是企鹅是一种鸟。”如果你的知识库只包含“鸟会飞”和“企鹅是一种鸟”，一个经典的、单调的系统会迫使你得出企鹅会飞的结论，而这是错误的。为了得到正确的结果，你需要添加另一条信息：“企鹅不会飞”。更重要的是，你需要一个能够理解这个新事实是一个*例外*，并且应该凌驾于一般规则之上的推理系统。

这就是**非单调推理**（non-monotonic reasoning）的精髓。与它的经典表亲不同，非[单调系统](@entry_id:752160)是一种在增加新前提时，其结论集合可能会缩小的系统。

`Premises_1 = {Bird(Tweety)}`
`Default_Rule = For any X, if Bird(X), then conclude Flies(X).`
`Conclusion_1 = {Flies(Tweety)}`

现在，让我们添加一个新事实。

`Premises_2 = {Bird(Tweety), Penguin(Tweety), For any X, if Penguin(X) then not Flies(X)}`
`Conclusion_2 = {not Flies(Tweety)}`

通过添加更多信息，我们失去了一个结论。这是可废止系统的决定性特征。它们处理现实世界中的“通常”、“典型地”和“除非”。一个临床决策支持系统可能会基于这样的默认规则运作：“除非最近的检测结果显示异常，否则假设患者的钾水平是正常的。”[@problem_id:4606510] 医生可以根据这个合理的假设采取行动。但如果一份新的化验结果异步传来，显示钾水平严重偏高，系统必须立即撤销其“正常”的结论并发出警报。在面对新数据时优雅地修正信念的能力，不仅仅是一个理论上的奇思妙想；它是构建能够在真实、不断变化的世界中运作的智能系统的先决条件。

### 知道你所不知的艺术

这引出了一个更深层次、更深刻的问题：系统如何知道何时应用默认规则？它如何知道“没有相反的证据”？这要求系统能够对其*不知道*什么进行推理。

大多数简单的数据库在**封闭世界假设**（Closed-World Assumption, CWA）下运行。如果你在员工数据库中查询一个名叫 John Doe 的人而没有得到结果，你可以安全地得出结论，他不在那里工作。数据库被假定为一份完整的记录。

但是，许多最重要的知识系统，从语义网到高级人工智能，都必须在一个远为谦逊和现实的前提下运行：**开放世界假设**（Open-World Assumption, OWA）。[@problem_id:4846310] OWA 指出，一个事实的缺失并不意味着其为假；它仅仅意味着它是未知的。

考虑一个病人的电子健康记录。如果记录中没有条目说明 $hasAllergy(\text{Patient\_2}, \text{Penicillin})$，我们能得出病人对[青霉素](@entry_id:171464)不过敏的结论吗？在 OWA 下，答案是坚决的“不能”。这个信息可能只是缺失了。病人可能从未被问及，或者数据尚未被录入。得出病人没有过敏的结论将是一个危险的跳跃。[模型论](@entry_id:150447)为我们提供了一种优美的看待方式：我们可以构建一个与已知事实相符的可能世界（一个“模型”），在这个世界里病人 2 对花生过敏；同时也可以构建另一个同样可能的、他没有任何过敏的世界。由于“病人 2 没有任何过敏”这一陈述并非在*所有*可能的模型中都为真，因此它不是一个逻辑上必然的结论。[@problem_id:4846310]

这种谦逊对安全性有着巨大的影响。想象一个控制化工厂的[数字孪生](@entry_id:171650)。[@problem_id:4244962] 如果系统在开始一个化学反应前需要知道一个关键阀门是否关闭，它不能仅仅检查是否缺少表明 $Open(\text{valve}_1)$ 的传感器读数就假定是安全的。传感器可能坏了。证据的缺失并非缺失的证据。一个安全的系统必须要求肯定的确认；它必须等到能够证明 $Closed(\text{valve}_1)$ 为真时才能继续。

这种区别催生了两种截然不同的规则设计模式：
1.  **失败即否定（默认允许）：** 这种模式使用类似 $RecommendTherapy(p) \leftarrow Eligible(p), \neg Contraindicated(p)$ 的规则，其中 `$\neg$` 意为“无法被证明”。这是一个经典的可废止规则。它是乐观的：它推荐治疗，除非有明确的禁忌症证据。这很强大，但如果数据不完整，可能会不安全。[@problem_id:4606545]
2.  **明确证据（默认拒绝）：** 这种模式要求对安全性有肯定的证明，使用类似 $RecommendTherapy(p) \leftarrow Eligible(p), SafetyVerified(p)$ 的规则。这是一个单调模式。它保守且在高风险环境中安全得多，因为它绝不会基于缺失的信息做出推荐。

在这些模式之间做出选择不仅仅是一个技术细节；它是任何智能系统设计中的核心伦理和安全决策。

### 当规则冲突时：优先级和可能世界

现实世界是混乱的，我们的责任和信念常常发生冲突。一个使用可废止推理的系统必须有办法处理这些冲突。

有时，相互冲突的默认规则只是创造了多种可能的未来。考虑一个关于用药依从性的默认理论。[@problem_id:4606567] 我们可能有两条默认规则：
*   $d_1$: “通常情况下，病人是用药依从的。” $(Patient(p) : Adherent(p)) / Adherent(p)$
*   $d_2$: “通常情况下，错过续药的病人是不依从的。” $(MissedRefills(p) : \neg Adherent(p)) / \neg Adherent(p)$

当我们有一个错过了续药的病人时会发生什么？这两条默认规则都被激活，并且指向相反的方向。这种冲突产生了两个自洽的、合理的“世界”，或者逻辑学家称之为**扩展**（extensions）。
*   **扩展 1：** 我们应用 $d_1$。病人是依从的（也许他们错过续药有正当理由）。在这个世界里，$d_2$ 被阻止了，因为它的结论（$\neg Adherent(p)$）会造成矛盾。
*   **扩展 2：** 我们应用 $d_2$。病人是不依从的。在这个世界里，$d_1$ 被阻止了。

系统现在可以用两种方式进行推理。**轻信推理**（Credulous reasoning）接受一个结论，只要它在至少一个扩展中为真（例如，“病人*可能*是不依从的”）。**怀疑推理**（Skeptical reasoning）只接受一个结论，当且仅当它在所有扩展中都为真（例如，“我们无法确定其依从性状态”）。

然而，在其他情况下，*必须*做出决定。考虑一个为医生提供伦理义务建议的人工智能。[@problem_id:4412718] 医生有维护病人隐私的一般义务。但法律也要求报告某些[传染病](@entry_id:182324)以保护公众健康。如果一个患有此类疾病的病人要求保密，规则就会发生冲突：
*   $r_2$: $\top \Rightarrow O(\neg \text{disclose})$ (有不披露的义务)。
*   $r_1$: $\text{is\_reportable}(disease) \Rightarrow O(\text{report})$ (有报告的义务，这意味着披露)。

在这里，我们不能简单地考虑两个可能的世界。医生必须采取行动。可废止逻辑通过引入规则的**优先级排序**（priority ordering）来解决这个问题。我们可以明确规定，报告的法律义务比一般的保密义务有更高的优先级：$r_1 \succ r_2$。更强规则的结论击败了较弱规则的结论。冲突得以解决，一个单一、连贯的义务集合浮现出来。这种有序规则的机制是模拟法律和伦理中复杂的责任层级的一种强大方式。

### 合理辩论的语法

可废止推理最美妙之处或许在于，这种形式化的机制恰恰反映了复杂人类论证的结构。它不仅仅是用来制造机器人的工具；它还是一个用来理解我们自身的透镜。

哲学家 Stephen Toulmin 发展了一种论证模型，完美地捕捉了这种可废止结构。让我们再次想象那个临床情景：一位具有决策能力的老年患者因慢性头痛拒绝进行 CT 扫描，理由是费用和辐射担忧。医生注意到没有“危险信号”症状，同意了患者的决定。[@problem_id:4851500] 对此决定的一个决疑的，或称基于案例的伦理学论证如下所示：

*   **数据（事实）：** 患者有行为能力，拒绝检查，且在此特定案例中检查的预期益处很低。
*   **主张（结论）：** 因此，临床医生今天不应该开具 CT 扫描。
*   **保证（默认规则）：** 在有行为能力的患者拒绝一项低效益、非紧急的干预措施时，临床医生应尊重其拒绝。
*   **限定（不确定性）：** 这*很可能*导向该结论。
*   **反驳（例外条款）：** ……*除非*出现新的危险症状（例如，神经功能缺损）或患者的决策能力受损。

这是可废"止"推理最实际、最人性化的形式。结论不是绝对的；它是暂定的，并明确附带其自身的否决条件。它反映了一种细致入微的理解，即在尊重患者自主权等原则与提供护理的责任之间取得平衡，并承认今天的正确答案如果事实发生变化，明天可能就是错误的。

因此，可废止推理并非逻辑的“次等”或“不完美”版本。它是一个更强大、更灵活的框架，一种用于表达实用主义、谦逊和智能所需适应性的形式化语言。它是一种让我们做出最佳猜测、采取行动，但始终准备好学习、修正和重新看待世界的逻辑。它本身就是发现的逻辑。

