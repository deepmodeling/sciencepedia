## 引言
在复杂的多层系统中，一个信号或一条信息如何在传输过程中幸存下来，既不至于消失于无形，也不会爆炸成混乱？这个基本问题是训练深度神经网络的核心，在深度神经网络中，信号和梯度必须在数百个层中传播。如果没有一种机制来控制它们的量级，这些网络就无法学习。本文通过介绍方差保持原理来应对这一挑战，这是一个确保[信息流](@article_id:331691)稳定性的统计学概念。我们将首先探讨其核心的“原理与机制”，深入研究像 Xavier 初始化和 He 初始化这类稳定网络初始化方案背后的统计学逻辑。在深入探讨之后，本文将在“应用与跨学科联系”部分扩展视野，揭示保持、管理或解释方差这一相同的基本思想，如何在统计学、生物学和物理学等不同领域中作为一个统一的概念出现，从而展示其深远的科学意义。

## 原理与机制

想象一下，你正在玩一个很长的“传话游戏”。你向第一个人耳语一条信息，他再传给第二个人，如此在一个百人长队中传递下去。你认为最后一个人会听到什么？很可能，那将是完全的胡言乱语，甚至可能是沉默。原始信息——即信号——已经被扭曲和丢失了。深度神经网络的核心，就是一个非常长的传话游戏，但传递的不是文字，而是数字。如果我们不小心，输入网络的初始信号要么会消失于无形（**信号消失**问题），要么会增长为一堆混乱、无意义的数字（**信号爆炸**问题）。为了训练这些宏伟的结构，让它们[学会学习](@article_id:642349)，我们必须首先解决一个物理学中的基本问题：如何在一个复杂的动态系统中保持[信息流](@article_id:331691)？这就是**方差保持**原理。

### 一个简单的数字链

让我们将[神经网络](@article_id:305336)简化到其最纯粹的本质。它是一系列层的序列，每一层都执行一个简单的操作：从前一层获取一组数字，将它们乘以一组“权重”，然后相加。我们称从一个层输出的数字为“激活值”。对于第 $l$ 层的单个[神经元](@article_id:324093)，其预激活值（我们称之为 $h^{(l)}$）是前一层激活值 $x^{(l-1)}$ 的加权和：

$$
h^{(l)} = \sum_{j=1}^{n_{l-1}} w_{j} x_{j}^{(l-1)}
$$

在这里，$w_j$ 是权重——我们最终将在学习过程中调整的旋钮——而 $n_{l-1}$ 是前一层[神经元](@article_id:324093)的数量，通常称为**[扇入](@article_id:344674)**（fan-in）。

现在，让我们思考一下这个信号的“能量”或“大小”。一个很好的衡量标准是统计学中一个叫做**方差**的概念。方差告诉我们一组数字的离散程度。方差为零的信号只是一个常数——它不携带任何信息。而一个具有稳定、健康方差的信号则是一个充满活力的信号。

我们的预激活值的方差 $\operatorname{Var}[h^{(l)}]$ 是多少？如果我们为简化起见，假设我们的权重和先前的激活值都是独立的，并且平均值（均值）为零，那么统计学定律会给我们一个非常简单的结果。[独立变量之和](@article_id:357343)的方差就是它们各自方差的和。这导出了一个递推关系：

$$
\operatorname{Var}[h^{(l)}] = n_{l-1} \operatorname{Var}[w] \operatorname{Var}[x^{(l-1)}]
$$

这个方程是一切的关键。它告诉我们信号的方差在通过一层时如何变化。新的方差是旧方差乘以一个因子 $n_{l-1} \operatorname{Var}[w]$。

为了让我们的信号在网络中传输时能够幸存下来，其方差必须在层与层之间大致保持恒定。我们希望 $\operatorname{Var}[h^{(l)}] \approx \operatorname{Var}[x^{(l-1)}]$。如果我们的“激活函数”只是[恒等函数](@article_id:312550)（即 $x^{(l-1)} = h^{(l-1)}$），那么保持稳定的条件就很简单：乘法因子必须为 1。

$$
n_{l-1} \operatorname{Var}[w] = 1 \quad \implies \quad \operatorname{Var}[w] = \frac{1}{n_{l-1}}
$$

这是我们的第一个重要洞见[@problem_id:3123395]。为了在简单的线性网络中保持信号稳定，我们应该随机初始化我们的权重，但其方差需要与输入数量成反比。这确保了对 $n_{l-1}$ 个项求和的放大效应，与使用较小权重的衰减效应完美平衡。

### 非线性的复杂性

当然，[神经网络](@article_id:305336)不仅仅是线性链。它们的力量来自于**[激活函数](@article_id:302225)**——应用于预激活值的非线性变换。如今最流行的是[修正线性单元](@article_id:641014)，即 **ReLU**，定义为 $\phi(z) = \max\{0, z\}$。

ReLU 对我们的信号做了什么？它的作用相当剧烈：它将任何负数变为零。它就像一个只允许正信号通过的守门人。这对我们的方差有何影响？直观地说，如果你砍掉一半的数字，你就在减小整体的离散程度，所以方差必然会减小。

我们可以精确地计算这个效应。如果我们假设预激活值 $h$ 是一个良好、对称、零均值的变量（比如高斯分布，由于中心极限定理，这是一个合理的近似），那么结果是输出的二阶矩恰好是输入的一半：$\mathbb{E}[(\phi(h))^2] = \frac{1}{2}\mathbb{E}[h^2]$ [@problem_id:3166688]。在初始化的实际应用中，这意味着 ReLU 函数实际上将信号的能量减半了。

我们的方差传播规则必须更新：

$$
\operatorname{Var}[h^{(l)}] \approx n_{l-1} \operatorname{Var}[w] \left( \frac{1}{2} \operatorname{Var}[h^{(l-1)}] \right)
$$

为了现在保持方差稳定，我们必须满足一个新的条件：

$$
\frac{1}{2} n_{l-1} \operatorname{Var}[w] = 1 \quad \implies \quad \operatorname{Var}[w] = \frac{2}{n_{l-1}}
$$

这就是著名的 **He 初始化**，以其发明者 Kaiming He 的名字命名。这是对我们之前规则的一个微妙但至关重要的修改。对于 ReLU 网络，我们需要初始权重稍大一些，以抵消激活函数的抑制效应。

这个理论上的精妙之处在现实世界中重要吗？绝对重要。实验以惊人的清晰度证实了这一原则[@problem_id:3199598]。如果你构建一个带有 ReLU 激活的深度网络，但使用旧的 $\operatorname{Var}[w] = 1/n_{l-1}$ 规则（即 **Xavier** 或 **Glorot 初始化**，它适用于像[双曲正切函数](@article_id:638603) $\tanh$ 这样的对称激活函数）进行初始化，信号会迅速消失。相反，如果你对一个 $\tanh$ 网络使用 He 初始化，信号将会爆炸。你必须将初始化方法与激活函数相匹配。方差保持原则精确地告诉你如何做。搞错这个的后果并不微妙——一个深度网络可能完全无法学习，其对数据的重构要么是纯粹的噪声，要么坍塌为零，而一个正确初始化的网络则能很好地训练[@problem_id:3134401]。

### 学习的双向通道

到目前为止，我们只讨论了**[前向传播](@article_id:372045)**，即信号从输入到输出的过程。但是神经网络中的学习——即**[反向传播](@article_id:302452)**过程——需要第二次旅程。这是信息从最终误差（或损失）反向流经各层的过程。这个反向信号，即**梯度**，告诉每个权重应该如何调整自己以改善网络的性能。

这个反向流动的梯度也只是一连串的数字，它同样容易受到消失和爆炸问题的影响。如果[梯度消失](@article_id:642027)，早期层的权重得不到更新信号，网络就会停止学习。如果[梯度爆炸](@article_id:640121)，学习过程将变得极度不稳定。

我们可以将完全相同的方差保持原则应用于这个[反向传播](@article_id:302452)过程[@problem_id:3194483]。计算过程惊人地对称。第 $l$ 层的梯度取决于它*前面*一层，即第 $l+1$ 层的权重和梯度。涉及的连接数不是[扇入](@article_id:344674)（$n_{l-1}$），而是**[扇出](@article_id:352314)**（$n_l$），即下一层中[神经元](@article_id:324093)的数量。此外，数学推导表明，[激活函数](@article_id:302225)的相关属性不是 $\phi(z)$ 本身，而是它的[导数](@article_id:318324) $\phi'(z)$。

这导出了一个保持梯度方差的条件：

$$
n_{l} \operatorname{Var}[w] \mathbb{E}[(\phi'(z))^2] = 1
$$

我们现在有两个条件，一个用于[前向传播](@article_id:372045)，一个用于[反向传播](@article_id:302452)。对于 ReLU 网络，当输入为正时 $\phi'(z)$ 为 1，为负时为 0。所以，$\mathbb{E}[(\phi'(z))^2]$ 也约等于 $\frac{1}{2}$。[反向传播](@article_id:302452)的条件变为 $\frac{1}{2} n_l \operatorname{Var}[w] = 1$，或 $\operatorname{Var}[w] = 2/n_l$。

注意这里的矛盾：[前向传播](@article_id:372045)希望方差与 $1/\text{fan-in}$ 成比例，而[反向传播](@article_id:302452)则希望它与 $1/\text{fan-out}$ 成比例。这就是为什么最初的 Xavier/Glorot 方案提出了一个折衷方案：$\operatorname{Var}[w] = 2 / (n_{in} + n_{out})$。对于 ReLU 网络，He 初始化方案通常只使用[扇入](@article_id:344674)来实现，即 $\operatorname{Var}[w] = 2/n_{in}$，因为它在实践中效果很好。关键的结论是，一个真正稳定的网络必须是一个行为良好的双向通道，允许信息在两个方向上[自由流](@article_id:319910)动。无论层类型如何，无论是标准的卷积层还是用于[上采样](@article_id:339301)的更奇特的[转置卷积](@article_id:640813)层，同样的逻辑都适用[@problem_id:3134464]。

### 一个具有普适力量的原则

方差保持的美妙之处在于，它不是一个僵化的公式，而是一个灵活、强大的原则。一旦你掌握了它，你就可以将其应用于任何情况。

-   **新的[激活函数](@article_id:302225)：** 如果你发明了一种新的激活函数，比如有上限的 ReLU6 ($\min(\max(0,z), 6)$) [@problem_id:3134445] 或用于表示隐式神经场的正弦函数 ($\sin(\omega z)$) [@problem_id:3200130] 呢？你不需要猜测如何初始化你的网络。你可以简单地应用这个原则：计算你的新函数及其[导数](@article_id:318324)如何影响方差，并推导出正确的权重方差来补偿。对于 SIREN 网络，这导出了一个独特的初始化规则，其中权重方差还必须依赖于频率 $\omega$。

-   **复杂架构：** 像 Highway Network 中那些显式地将变换后的信号与原始信号的保留副本混合的更复杂的层又如何呢[@problem_id:3134436]？同样，这个原则是你的指南。你可以分析通过每条路径的方差传播，并设置你的初始化参数，以确保组合输出的方差保持稳定。

-   **隐藏的假设：** 这个原则也迫使我们意识到我们的假设。最简单的推导假设我们的激活值保持在零附近。但对于将所有负数变为零的 ReLU，激活值的*均值*将偏离零。我们原则的更深层次应用涉及到不仅要保持方差，还要保持均值，这可以通过仔细初始化层的偏置来抵消这种漂移来实现[@problem_id:3200152]。

这段从简单的线性链到复杂的非线性深度网络的旅程，揭示了一个统一的思想。正确的初始化不仅仅是一种技巧；它是一种将网络设置为**动态等距**（dynamical isometry）状态的方法，这是一个花哨的术语，用来描述一个系统在信号传播时完美保持其范数。通过在最开始时仔细选择我们随机权重的方差，我们确保了网络内部的通信渠道是开放、清晰和稳定的。这使得信号能够向前传播，梯度能够向后传播，穿过成百上千个层，从而使这些卓越的系统能够从世界中学习。

