## 应用与跨学科联系

在经历了使用小样本进行推理的基础原则之旅后，我们可能感觉自己像是在穿越一条险峻的山路。我们身后是熟悉而开阔的大数平原及其令人慰藉的确定性。在这里，在[稀疏数据](@entry_id:636194)的高地，空气稀薄，每一步都需要小心翼翼。但正是从这些高处，我们获得了最令人叹为观止的景色。在这里，科学变得个人化，我们面对独特的、罕见的和全新的事物。

我们现在的任务不仅仅是列出这片崎岖地形中的前哨和定居点，而是要看到连接它们的隐藏路径。我们将看到，正是那些在小样本的熔炉中锻造出的相同的智力工具——相同的思维习惯——让我们能够窥视单个患者的细胞，解码基因组的语言，甚至稳定我们最先进人工智能的学习过程。这不是一堆孤立的技巧；这是一种统一的推断哲学，证明了人类智慧在面对不确定性时的非凡力量。

### 治愈过程中的高风险

没有什么地方比医学领域更能直接、更个人化地体现小样本的挑战。当我们从公共卫生转向病床边，从数百万人口转向单个个体时，[大数定律](@entry_id:140915)的声音渐行渐远。

想象一下[个性化医疗](@entry_id:152668)的前沿：为一种超罕见疾病开发疗法，其治疗本身就是用患者自己的细胞定制的。在这个自体基因疗法的世界里，生产的“[批量大小](@entry_id:174288)”通常只有一个——一个批次只为一个病人[@problem_id:5038027]。我们如何才能保证质量、纯度和一致性？测试一百个产品来表征第一百零一个产品的经典方法根本行不通。天真地使用少数几次开发运行中极小范围的数据来设定质量标准，在统计上是鲁莽的，这无异于声称在测量了三个人之后就知道人类身高的全部范围。

在这里，我们被迫变得更聪明。我们不能依赖我们没有的大量数据。相反，我们必须*[借力](@entry_id:167067)*。这是贝叶斯视角的核心。我们可以从一个制造*平台*——即从类似疗法中获得的经验——中获取知识，并将其与我们新疗法中涓涓细流的数据进行数学上的融合。这不是作弊；这是一种正式、严谨地利用我们所知一切的方法。此外，我们不能仅仅描述我们所看到的，还必须使用像*容忍区间*这样的工具，来对*未来*批次可能落在何处做出概率性陈述。这是从描述性统计到预测性、有原则的推断的深刻转变。

即使在结果罕见的更传统的临床试验中，这一挑战也会出现。考虑一项在重症监护室中预防感染新方案的初步研究。在一个接受新疗法的小组中，你可能会观察到一个绝佳的结果：零感染。临床上，这是一次胜利。统计上，这却是个头疼的问题[@problem_id:4803492]。最直接的[统计模型](@entry_id:755400)，如逻辑回归，试图计算感染的对数几率。但是零计数事件的几率为零，而零的对数是负无穷大！数学在这里失效了，产生了一个荒谬的完美、无限有效的估计。小样本把我们引向了一个悖论的深渊。优雅的解决方案是“[连续性校正](@entry_id:263775)”，即在我们的数据表格的每个单元格中加入一个微小的分数计数（如0.5）。这不仅仅是一个数学技巧；它是一种*正则化*，一种温和的推动，将我们的估计值从无穷大的荒谬悬崖边拉回，给我们一个有限的、更合理的答案，这个答案承认了我们仍然存在的不确定性。

当我们评估预测模型的临床效用时，这些微妙之处依然存在。像决策曲线分析这样的工具有助于我们权衡模型预测的益处与预测错误的危害。但是我们使用的度量标准本身，即“净收益”，可能会有一个奇怪的、偏斜的[抽样分布](@entry_id:269683)，尤其是当我们考虑罕见结果或极端决策阈值时[@problem_tissot:4553176]。我们的老朋友[钟形曲线](@entry_id:150817)，离我们而去了。在这些情况下，我们转向[自助法](@entry_id:139281)——一个强大的计算思想。我们让数据通过成千上万次的重抽样来告诉我们其自身的不确定性。像偏差校正和加速（BCa）自助法这样的高级版本，是为这些偏斜和有界的分布量身定做的，它们提供的[置信区间](@entry_id:138194)远比那些假设世界整洁、对称的方法要诚实得多。

当我们的数据不仅仅是一个简单的列表时，对结构的这种尊重同样至关重要。多中心试验中的患者不是罐子里可以互换的弹珠；他们是聚集在医院内的。一家医院的患者可能彼此之间比另一家医院的患者更相似。如果我们忽略这种结构而使用简单的[自助法](@entry_id:139281)，我们就是在自欺欺人，会低估真实的不确定性。解决方案是*整群自助法*，我们首先重抽样整群（医院），然后取其中所有的患者[@problem_id:4802805]。这个教训既优美又深刻：要理解世界，我们必须尊重它的结构，即使在我们的统计程序中也是如此。

### 基因组革命的精打细算

现在让我们将焦点从患者缩小到其内部的分子机器。在基因组学和[蛋白质组学](@entry_id:155660)中，我们面临一个奇特的数据问题反转。我们可能只有少数几个生物学重复——比如说，三个处理组样本对三个[对照组](@entry_id:188599)样本——但对于每个样本，我们同时测量两万个基因或蛋白质的活性。

如果我们试图孤立地分析每个基因，我们立刻就会碰壁。每组只有三个数据点，我们对该基因方差的估计是极其不可靠的[@problem_id:3884500]。这就像试图通过查看股票在三个随机日子的价格来衡量其波动性。我们得到的方差估计如此不稳定，以至于任何后续的统计检验都建立在沙子之上。

在这里，“[借力](@entry_id:167067)”的原则再次拯救了我们，但方式新颖而壮观。我们不再孤立地看每个基因，而是将所有20,000个基因放在一起看[@problem_id:2938428]。*[经验贝叶斯](@entry_id:171034)*方法的关键洞见是，这些基因的方差虽然不同，但可能遵循某个共同的潜在分布。我们可以利用所有20,000个基因的整体来学习这个分布的形状。然后，我们可以利用这个全局知识来精炼或“收缩”我们对每个单独基因的摇摆不定的[方差估计](@entry_id:268607)。基因A的估计值被来自基因B、C、D以及所有其他基因的信息所稳定。这是一个极其强大的思想，使我们即使在重复次数非常少的情况下也能进行稳健的推断。

有时，问题不在于估计的稳定性，而在于统计检验的逻辑本身。考虑[基因集富集分析](@entry_id:168908)（GSEA），这是一种询问一整*套*相关基因（例如，参与炎症的基因）是否表现异常的技术。经典的检验方法是通过置换样本标签（打乱谁在“处理组”和“[对照组](@entry_id:188599)”）来创建一个零分布。但如果你每组只有三个样本呢？打乱标签的唯一方式总共只有区区$\binom{6}{3} = 20$种。这意味着你能期望达到的最小p值是$1/(20+1) \approx 0.048$，而且检验统计量的分布是粗糙和离散的。这个检验变得[无能](@entry_id:201612)为力[@problem_id:4345974]。

然而，当一条路被堵死时，人类的创造力会找到其他出路。统计学家已经开发出了一整套创造性的替代方案。一些方法利用数据的几何结构，通过在高维空间中*旋转*数据来生成平滑的零分布。另一些方法则完全改变了问题。他们不再问这个基因集是否与结果相关（一种“自足式”检验），而是问这个基因集是否比一个同样大小的随机基因集*更*相关（一种“竞争式”检验）。这是通过置换基因标签而不是样本标签来完成的。由于有数千个基因，置换的数量实际上是无限的，小样本问题也就消失了。

### 机器中的幽灵

这可能看起来与少数患者或基因的世界相去甚远，但同样的小样本基本挑战也困扰着现代人工智能的核心。我们认为人工智能和“大数据”是同义词，但这是一种误解。

考虑训练一个[深度神经网络](@entry_id:636170)在三维医学扫描中分割肿瘤的任务[@problem_id:4535994]。这些需要大量数据的模型是以“小批量”（mini-batches）方式进行训练的。但单个3D扫描是巨大的，用于训练的强大GPU内存有限。通常，研究人员一次只能将一两个扫描放入内存。训练的[批量大小](@entry_id:174288)再次变得非常小。

这些网络的一个关键组成部分叫做*[批量归一化](@entry_id:634986)*（Batch Normalization）。在每一层，它试图通过计算当前小批量内神经元激活值的均值和方差来稳定学习过程。幽灵就在这里：网络试图从大小为$B=1$或$B=2$的样本中计算均值和方差的稳定估计！就像那个只有两个重复样本的生物学家一样，这些估计值极其嘈杂，从一个小批量到另一个小批量剧烈波动。这种统计上的不稳定性给训练过程注入了噪声，阻碍了网络的学习能力。

人工智能研究人员设计的解决方案，与[经典统计学](@entry_id:150683)的解决方案惊人地相似。一种方法，同步[批量归一化](@entry_id:634986)，就像是汇集多个实验室的数据；它在多个GPU之间进行协调，以创建一个更大的有效[批量大小](@entry_id:174288)。其他方法，如[实例归一化](@entry_id:638027)、[层归一化](@entry_id:636412)或[组归一化](@entry_id:634207)，则类似于问一个不同的问题[@problem_id:4322727]。它们不再试图跨批量（批量太小）估计统计量，而是在单个图像内部——跨其像素或跨其特征通道——计算统计量。它们使归一化过程独立于[批量大小](@entry_id:174288)，从而完全回避了这个问题。

从病人的床边到基因组测序仪，再到人工智能的硅心脏，一条统一的线索浮现出来。世界并不总是给予我们拥有丰富数据的奢侈。但在与稀缺作斗争的过程中，我们被迫进行更深入的思考。我们学会了诚实地面对我们的不确定性，尊重我们数据的复杂结构，从每一个可用的信息源中[借力](@entry_id:167067)，并在旧问题变得棘手时有创造力地提出新问题。用稀少数据进行科学研究的艺术证明了一个事实：即使通过最小的钥匙孔，一个训练有素、富有想象力的头脑仍然可以瞥见宇宙美丽、潜在的秩序。