## 引言
在为一组数据点拟合一条直线时，我们如何定义“最佳”？这个基本问题在统计分析领域引发了一场至关重要的辩论。几乎所有入门课程教授的最常见答案是最小化[误差平方和](@article_id:309718)——这是[普通最小二乘法](@article_id:297572) (OLS) 的基础。然而，这种方法可能具有误导性，因为它对误差平方的过度关注使其对异常值极为敏感，单个异[常点](@article_id:344000)就可能极大地扭曲结果。这种脆弱性凸显了一个重大缺口：当面对现实世界中常见的混乱、不完美的数据时，我们如何稳健地建模关系？

本文将探讨一种强大而优雅的替代方法：[最小绝对偏差](@article_id:354854) (LAD) 回归。通过选择最小化绝对误差之和而非其[平方和](@article_id:321453)，LAD 提供了一种具有弹性的[数据分析](@article_id:309490)方法，不易受极端值的影响。在接下来的章节中，您将深入了解这一重要的统计工具。“原理与机制”一章将解析 LAD 的数学和几何基础，揭示它如何与中位数的概念相关联，以及为何这[能带](@article_id:306995)来其备受赞誉的稳健性。随后的“应用与跨学科联系”一章将展示 LAD 的实际应用，考察其在金融、生物学等不同领域中的使用，并探讨每位数据科学家都必须面对的稳健性与[统计效率](@article_id:344168)之间的关键权衡。

## 原理与机制

想象一下，您正试图在一堆散乱的数据点中画出“最佳”的直线。但“最佳”到底意味着什么？这个看似简单的问题将我们带入了一场深刻的统计学和哲学辩论的核心。答案完全取决于您如何决定衡量“错误程度”。大自然给了我们数据，但我们必须选择度量的尺子。

### 两种衡量“错误”的方式

假设我们有几个数据点，并提出一条候选直线来代表它们。对于每个点，从该点到我们直线的[垂直距离](@article_id:355265)就是**[残差](@article_id:348682)** (residual)，即误差。它表示我们的预测偏差了多少。一条好的直线应该具有较小的[残差](@article_id:348682)，但我们如何将所有这些单个误差组合成一个衡量拟合好坏的单一分数？

主要有两种思想流派，它们之间的差异虽然微妙却影响深远。

最著名的方法，也就是您很可能在学校里学过的，是**[普通最小二乘法](@article_id:297572) (OLS)**。其理念很简单：对每个[残差](@article_id:348682)进行平方，然后将所有这些平方后的[残差](@article_id:348682)相加。所谓“最佳”直线，就是使这个[平方和](@article_id:321453)尽可能小的那条。我们将这个总平方误差称为 $S_2$。

$$
S_2 = \sum_{i=1}^{n} (\text{residual}_i)^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

第二种方法称为**[最小绝对偏差](@article_id:354854) (LAD)**。它不对[残差](@article_id:348682)进行平方，而是取其[绝对值](@article_id:308102)。“最佳”直线在这里是使这些[绝对值](@article_id:308102)之和最小化的那条。我们将这个总绝对误差称为 $S_1$。

$$
S_1 = \sum_{i=1}^{n} |\text{residual}_i| = \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

为何会有这种差异？考虑一个巨大的误差。假设我们的一个点是一个离群的[异常值](@article_id:351978)，远离其他点。在 OLS 的世界里，它的[残差](@article_id:348682)可能是 5。当我们对其平方时，它对总误[差分](@article_id:301764)数的贡献是 $5^2 = 25$。但在 LAD 的世界里，它只贡献了 $|5| = 5$。OLS 通过对误差进行平方，对异常值产生了近乎歇斯底里的反应。一个遥远的离群点就能抓住回归线，并将其急剧地拉向自己。而 LAD 则更为“淡定”。它承认这个误差，但不会给予其不成比例的影响 [@problem_id:1935135]。这是揭示 LAD 特殊能力——**稳健性**——的第一个线索。

### 误差的形态：欧几里得步长 vs. 曼哈顿街区

要真正理解其中的差异，让我们从回归中退后一步，思考一下距离。如果你身处一个街道呈网格状的城市，你如何测量从 A 点到 B 点的距离？你可以画一条“乌鸦飞行”的直线，穿过建筑物。这就是**[欧几里得距离](@article_id:304420)**，或称 **L2 范数**。它的计算方式就像平方和一样，只是最后加了平方根：$\sqrt{\sum x_i^2}$。这正是 OLS 所基于的那种距离。

但要实际步行或驾车，你必须沿着网格一个街区一个街区地走。总距离是你走过的水平街区和垂直街区之和。这就是**[曼哈顿距离](@article_id:340687)**，或称 **L1 范数**：$\sum |x_i|$。这就是 LAD 的世界。

让我们在一个更科学的背景下想象一下。假设我们正在研究一种药物如何影响细胞中五种不同化学物质（代谢物）的水平 [@problem_id:1477170]。这些变化在一个五维空间中形成一个向量。该向量的 L1 范数 $\sum |\Delta c_i|$ 告诉我们*代谢活动的总量*——所有单个增加和减少的总和。这就像一本记录所有交易的会计账本。而 L2 范数 $\sqrt{\sum (\Delta c_i)^2}$ 则给出了细胞代谢状态的直线位移。因为它对变化进行了平方，所以它主要由变化最大的那一两种代谢物所主导。

因此，OLS 寻找的是在欧几里得意义上最接近的解，而 LAD 寻找的是在曼哈顿街区行走意义上最接近的解。这种几何上的差异是它们所有不同性质的根源。

### 稳健性的力量：驯服异常值

LAD 最受赞誉的特性是其对异常值的稳健性。因为它不对大误差进行平方，所以它不容易受到它们的影响。这带来了一个极好且直观的结果：OLS 回归估计的是数据的*条件均值*（给定 $x$ 时 $y$ 的平均值），而 LAD 回归估计的是*条件[中位数](@article_id:328584)*。

如您所知，均值对极端值很敏感。如果一屋子人的平均收入是 50,000 美元，这时 Bill Gates 走了进来，平均收入就会飙升。然而，[中位数](@article_id:328584)收入——也就是收入正好处于中间位置的人的收入——几乎不会变动。[中位数](@article_id:328584)是稳健的。

通过最小化绝对偏差之和，LAD 在数学上找到了在每个点都穿过数据中位数的那条线。这就是为什么它是一类更广泛的稳健估计量——**M-估计量**——的成员，具体来说是使用函数 $\rho(u) = |u|$ 来衡量[残差](@article_id:348682) $u$ 的“成本”的那一种 [@problem_id:1932003]。

这不仅仅是一个理论上的奇特之处，它具有深远的实际意义。如果你认为数据中的误差不遵循完美的、表现良好的正态（高斯）分布，而是具有“重尾”——意味着极端[异常值](@article_id:351978)比[钟形曲线](@article_id:311235)所预示的更可能出现——那么 LAD 不仅仅是一种替代方案，它可能是一个极其优越的工具。例如，如果你的误差遵循**拉普拉斯 (Laplace) 分布**（看起来像两个背靠背的[指数分布](@article_id:337589)），LAD 估计量实际上比 OLS 估计量的效率高一倍 [@problem_id:1951481]。在这种情况下，使用 OLS 就像扔掉了你一半的数据！

### 处理拐点：如何找到 LAD 解

那么，我们究竟如何找到这条神奇的[中位数](@article_id:328584)线呢？对于 OLS，数学过程非常优美。我们想要最小化的函数——[误差平方和](@article_id:309718)——是一个光滑的、碗状的[曲面](@article_id:331153)。我们可以使用微积分，找到斜率为零的点，然后一个单一、唯一的解就出现了。

LAD 的[目标函数](@article_id:330966) $\sum |y_i - (a x_i + b)|$ 就没那么友好了。由于有[绝对值](@article_id:308102)符号，它的[曲面](@article_id:331153)是由平坦的平面在尖锐的“扭结”或“折痕”处连接而成。在这些扭结处，[导数](@article_id:318324)没有定义。基础形式的微积分在这里失效了。那么，我们如何找到这个尖锐、晶体状碗的底部呢？

有两种主要策略，都非常巧妙。

1.  **将其转化为[线性规划](@article_id:298637)问题：** 这是主要的工作方法。一个带有非线性[绝对值](@article_id:308102)的问题似乎不可能用*线性*方法解决，但一个巧妙的技巧使其成为可能。对于每个[绝对值](@article_id:308102) $|E|$，我们引入一个新的[辅助变量](@article_id:329712)，比如 $e$，并在我们的[目标函数](@article_id:330966)中用 $e$ 替换 $|E|$。然后我们添加两个简单的线性约束：$e \ge E$ 和 $e \ge -E$。由于我们是在最小化所有 $e$ 的总和，优化过程会自然地将每个 $e$ 向下推，直到它碰到 $E$ 或 $-E$，从而使 $e$ 精确地等于 $|E|$。通过这种转换，整个 LAD 问题就变成了一个**线性规划 (Linear Programming, LP)** 问题，可以用[单纯形法](@article_id:300777)等标准[算法](@article_id:331821)高效求解 [@problem_id:2443956]。

2.  **[迭代重加权最小二乘法](@article_id:354277) (IRLS)：** 这种方法可能更直观。它主张：让我们通过求解一系列*加权* OLS 问题来逼近 LAD 解。我们从对直线的一个猜测开始。我们计算[残差](@article_id:348682)。现在，在下一步中，我们将求解一个新的 OLS 问题，但我们会给每个数据点一个权重。来自我们上一次猜测的[残差](@article_id:348682)大的点会得到一个*小*权重，而[残差](@article_id:348682)小的点会得到一个*大*权重。一个常见的加权方案是使用绝对[残差](@article_id:348682)的倒数，$w_i = 1/|r_i|$ [@problem_id:1031888]。我们求解这个加权 OLS 问题，得到一条新的直线，然后重复此过程。在每一步中，潜在[异常值](@article_id:351978)的影响被系统地减小。直线会优雅地学会忽略噪声点，更仔细地倾听表现良好的大多数点。

在更深的层次上，优化的挑战来自于不可微的“扭结”。处理这类点的工具是**次梯度 (subgradient)**。一个扭结处没有单一的[导数](@article_id:318324)（单一的切线），而是有一整套可能的切线。[次梯度](@article_id:303148)就是所有这些可能斜率的集合。优化算法可以使用次梯度来选择一个仍然是“下坡”的方向，并最终找到最小值 [@problem_id:2207137]。

### 不同的工具需要不同的工具箱

因为 LAD 建立在与 OLS 不同的基础上，我们不能使用 OLS 自带的标准统计工具箱。

一个典型的例子是用于检验模型显著性的 F 检验。在 OLS 中，方差分析 (ANOVA) F 检验依赖于一个名为 Cochran 定理的优美数学理论，该理论之所以有效，是因为它假设误差服从[正态分布](@article_id:297928)。这个假设使我们能够将总平方变异分解为服从[卡方分布](@article_id:323073)的几个部分。F 统计量就是这些部分的比率。如果你试图用 LAD 拟合的[残差](@article_id:348682)来计算这个比率，其底层的[分布理论](@article_id:339298)会完全失效。得到的数值不服从 F 分布，将其与 F 分布表进行比较是毫无意义的 [@problem_id:1895444]。在 LAD 的世界里，你需要不同的假设检验方法，比如自助法 (bootstrapping) 或基于秩统计量的检验。

即使是衡量“[拟合优度](@article_id:355030)”，也需要一个新的视角。OLS 中著名的 $R^2$ 系数，是将你的模型（预测条件均值）的性能与一个仅预测总体[样本均值](@article_id:323186) $\bar{y}$ 的基线模型进行比较。这在均值和平方的世界里是完全合理的。

对于 LAD，其自然的类似物，我们可以称之为 $R^1$，应该将我们的模型（预测条件中位数）与一个仅预测总体样本*中位数* $\tilde{y}$ 的基线模型进行比较 [@problem_id:1904822]。

$$
R^1 = 1 - \frac{\sum |y_i - \hat{y}_{i, \text{LAD}}|}{\sum |y_i - \tilde{y}|}
$$

这个公式告诉我们，与每次都只猜测中位数相比，我们的 LAD 模型在预测数据方面表现得有多好。就像 $R^2$ 一样，如果你的模型特别糟糕，它也可能是负数！例如，当数据聚集在远离原点的地方时，如果你强迫模型穿过原点，就可能发生这种情况。在这种情况下，位于中位数的简单水平线可能比你设定错误的回归线提供更好的拟合（在 L1 意义上）。

通过选择用[绝对值](@article_id:308102)而非平方来衡量误差，我们踏入了一个平行的统计宇宙。在这个宇宙中，我们不易受到异常值暴政的影响，我们说的是[中位数](@article_id:328584)而非均值的语言，并且需要一套独特而优雅的工具来进行导航和发现。它提醒我们，在科学中，你选择如何看待世界，从根本上决定了你将看到什么。