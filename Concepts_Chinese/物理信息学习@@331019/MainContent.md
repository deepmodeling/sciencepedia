## 引言
在一个由数据主导的时代，机器学习模型已展现出发现模式和进行预测的卓越能力。然而，当应用于科学和工程问题时，这些纯粹由数据驱动的“黑箱”方法常常暴露出一个致命缺陷：它们缺乏对其旨在描述的物理现实的任何依据。这可能导致预测结果荒谬，违反了基本的守恒定律或产生物理上不可能的结果。因此，核心挑战不仅在于拟合数据，更在于以一种尊重不可磨灭的自然法则的方式来做到这一点。

本文介绍**[物理信息学习](@article_id:297248)**，这是一种变革性的[范式](@article_id:329204)，它弥合了数据驱动模型与[第一性原理](@article_id:382249)物理学之间的鸿沟。它探讨了我们如何能赋予[神经网络](@article_id:305336)对物理世界的“常识性”理解。我们将深入探讨实现这一目标的核心概念，从**原理与机制**一章开始，该章节解构了物理定律如何被转化为可训练的[损失函数](@article_id:638865)并编码到模型架构中。随后，在**应用与跨学科联系**一章中，我们将见证这些原理的实际应用，它们如何解决复杂方程、设计新颖材料，并在迥然不同的科学领域之间建立起一种通用语言。

## 原理与机制

那么，如何教一台机器物理学呢？你可能会想象，如果给它看足够多的例子——足够多的[流体流动](@article_id:379727)模拟，或者足够多的粒子加速器数据——它最终会自己弄清楚其背后的规律。在某种程度上，这是对的。[现代机器学习](@article_id:641462)模型，特别是[深度神经网络](@article_id:640465)，在数据中发现模式方面表现得惊人地出色。它们是**[插值](@article_id:339740)**的大师；如果你问一个恰好落在它们见过的数据范围内的问题，它们通常能给出一个非常准确的答案。

但当你走出那个舒适区时会发生什么呢？当你要求模型在一个更高的温度、不同的压力或一个它从未遇到过的新几何形状下进行预测时，情况又会怎样？这就是**[外推](@article_id:354951)**的领域，也正是在这里，一个纯粹由数据驱动的“黑箱”模型常常会暴露出其惊人、有时甚至是危险的无知。

### [黑箱模型](@article_id:641571)的不合理之无效性

想象一下，我们用一个来自换热器高保真模拟的庞大数据集训练了一个神经网络。该模型学会了将流速和入口温度等输入条件映射到出口温度。在其训练域内，它表现出色。但如果我们给它一个远超该范围的输入，它没有任何“常识”来引导自己。它只是一个极其复杂的[函数逼近](@article_id:301770)器，它可能会预测出一个极高的出口温度，以至于意味着能量凭空产生，这公然违反了[热力学第一定律](@article_id:306905)[@problem_id:2434477]。模型并不知道它在模拟一个换热器；它只知道它所见过的数据中的模式。它学会了相关性，但没有学会因果关系。

问题不仅限于违反守恒定律。考虑一个来自[材料科学](@article_id:312640)的不同场景，其中一个模型被训练来分析穆斯堡尔谱（Mössbauer spectroscopy）的复杂光谱数据，以识别不同的铁化合物[@problem_id:2501468]。一个纯粹由数据驱动的模型可能会产生一个包含*负*强度[谱线](@article_id:372357)或各组分分数之和不等于100%的拟合结果。对于物理学家来说，这些结果是荒谬的——就像从收银员那里收到负数的找零一样。吸收强度不可能是负数，而且所有的铁原子都必须被计算在内。[黑箱模型](@article_id:641571)对支配这些光谱的量子力学和守恒定律一无所知，因为它只是在优化对数据的统计拟合，而没有任何现实基础，所以它会产生物理上毫无意义的垃圾信息。

这正是**[物理信息学习](@article_id:297248)**旨在解决的根本问题。目标是超越纯粹的[模式匹配](@article_id:298439)，为我们的模型注入支配我们所研究系统的物理原理。我们不仅希望模型看到数据，还希望它能理解其背后的*故事*——即自然法则本身。

### [物理信息](@article_id:312969)损失函数：理解的秘诀

我们如何做到这一点？核心思想出人意料地优雅。我们改变了模型“良好性能”的定义。对于标准的机器学习模型，“好”的预测是指与训练数据接近的预测。而对于物理信息模型，一个好的预测不仅要做到这一点，*还*必须遵守物理定律。我们通过一个专门设计的目标函数——**[物理信息](@article_id:312969)损失函数**——来强制执行这一新规则。

让我们从一个简单的方案开始。假设我们想求解一个[偏微分方程](@article_id:301773)（PDE），它不过是物理定律的数学表述。例如，[双调和方程](@article_id:345035) $\nabla^4 u = f$ 描述了薄弹性[板的弯曲](@article_id:364005)。我们可以用一个[神经网络](@article_id:305336)来表示解 $u(x, y)$，我们称之为 $\hat{u}(x, y; \theta)$，其中 $\theta$ 代表网络中所有可训练的[权重和偏置](@article_id:639384)。

现在，对于任何给定的输入坐标 $(x, y)$，网络会给我们一个 $\hat{u}$ 的值。[现代机器学习](@article_id:641462)框架的魔力在于一种名为**[自动微分](@article_id:304940)**的技术，它让我们能够精确地（达到[机器精度](@article_id:350567)）计算网络输出相对于其输入的[导数](@article_id:318324)。我们可以计算 $\frac{\partial \hat{u}}{\partial x}$、$\frac{\partial^2 \hat{u}}{\partial x^2}$，甚至可以计算[双调和方程](@article_id:345035)所需的四阶[导数](@article_id:318324)，例如 $\frac{\partial^4 \hat{u}}{\partial x^4}$ [@problem_id:2126362]。

一旦我们获得了这些[导数](@article_id:318324)，我们就可以将它们直接代入[偏微分方程](@article_id:301773)，看看我们网络的解满足方程的程度如何。我们将**物理[残差](@article_id:348682)** $R$ 定义为方程两边的差值：
$$
R(x, y; \theta) = \nabla^4 \hat{u}(x, y; \theta) - f(x, y)
$$
如果我们的网络 $\hat{u}$ 是完美解，这个[残差](@article_id:348682)在任何地方都将精确为零。对于一个不完美的网络，它将是非零的。我们[损失函数](@article_id:638865)的核心就是该[残差](@article_id:348682)平方在大量随机采样点（称为**配置点**）上的均值。通过最小化这个损失，我们迫使网络一步步地去寻找一个能使物理定律成立的函数 $\hat{u}$。

这只是我们方案中的第一个要素。一个物理问题的完整解还需要尊重**边界条件**——即系统边缘发生的情况。对于我们的弹性板，也许它的一侧是固定的，这意味着位移 $u$ 在那里必须为零。我们在损失函数中添加另一项，用以惩罚网络违反这些边界条件的行为。如果我们碰巧在几个点上有 $u$ 的实验测量值，我们可以添加第三项，用以惩罚网络预测值与这些数据点之间的差异。

最终的损失函数变成一个加权和，即一个**复合损失**，它讲述了完整的故事[@problem_id:2898825]：
$$
L(\theta) = \lambda_{res} L_{residual} + \lambda_{bc} L_{boundary} + \lambda_{data} L_{data}
$$
在这里，$L_{residual}$ 是对在内部区域违反PDE的惩罚，$L_{boundary}$ 是对违反边界条件的惩罚，而 $L_{data}$ 是对与任何已知测量值不匹配的惩罚。$\lambda$ 值是我们选择的权重，用以平衡每个部分的重要性。现在，训练网络意味着找到能够最小化这个单一、全面的“错误度”度量的参数 $\theta$。这是一个美妙的综合，我们利用物理定律创造一个“地形”，而优化算法的工作就是在这个地形中找到最低点。

这种方法巧妙地弥合了百年历史的[数值分析](@article_id:303075)学科与[现代机器学习](@article_id:641462)之间的差距。事实上，我们可以将物理信息网络的总误差看作由两部分组成：我们离散化中固有的误差（例如使用有限数量的配置点），以及[神经网络](@article_id:305336)未能完美表示真实解和其背后物理规律所带来的误差[@problem_id:2380142]。这是一种演进，而非一场革命。

### 超越[损失函数](@article_id:638865)：将物理学融入模型结构

惩罚模型的错误行为是教导它的一种方式。一种更深刻的方式是从一开始就让错误行为变得不可能。我们可以超越损失函数，将物理原理直接融入我们[神经网络](@article_id:305336)的架构之中。这些内置的原理被称为**[归纳偏置](@article_id:297870)**。

一个很好的例子是边界条件的强制执行[@problem_id:2411060]。我们刚才描述的方法，即在损失中添加惩罚项，被称为**软约束**。它鼓励模型满足条件，但并不保证。一种更优雅的方法是**硬约束**。假设我们需要解 $u(x)$ 在 $x=0$ 和 $x=1$ 处为零。我们可以不让我们的网络 $N_\theta(x)$ 直接学习这个，而是通过一个*拟设*（ansatz）——一种特定的数学形式——来定义我们的解：
$$
u_\theta(x) = x(1-x) N_\theta(x)
$$
通过构造，无论[神经网络](@article_id:305336) $N_\theta(x)$ 输出什么，这个函数都*保证*在 $x=0$ 和 $x=1$ 处为零！物理不再是一个建议；它已成为模型设计中硬编码的事实。网络的工作现在变得更简单了：它只需要学习解中未被边界固定的那部分。

我们可以通过编码基本的对称性和标度律，将这个想法带到一个更深的层次。物理学不仅仅是关于方程；它是关于[不变性原理](@article_id:378160)的。考虑预测[原子力显微镜](@article_id:342830)针尖压入软材料时所产生力的问题[@problem_id:2777675]。经典力学精确地告诉我们力 $F$ 应该如何随针尖半径 $R$ 和压痕深度 $\delta$ 变化；它必须遵循类似于 $F \propto R^{1/2} \delta^{3/2}$ 的关系。一个[黑箱模型](@article_id:641571)将不得不从头开始发现这种复杂关系，并且很可能无法泛化到它从未见过的针尖半径。

但是，一个[物理信息](@article_id:312969)模型可以被构建成自动遵守这个标度律。我们可以将[网络架构](@article_id:332683)设计成**等变的（equivariant）**，这意味着如果我们以物理上正确的方式缩放输入 $R$ 和 $\delta$，输出 $F$ 也保证会正确地缩放。此外，我们可以内置[热力学](@article_id:359663)约束，如**[被动性](@article_id:323267)（passivity）**，确保模型永远不会预测材料会自发产生能量。通过[嵌入](@article_id:311541)这些深刻的物理原理，我们极大地缩小了模型必须搜索的可能函数的空间。我们给了它一个巨大的领先优势，使其能够从稀疏数据中学习，并且最重要的是，**泛化**到其训练舒适区之外并做出准确的预测。

### 崎岖的训练之路与[混沌边缘](@article_id:337019)

这一切听起来很美妙，但它并非魔法。构建一个[物理信息](@article_id:312969)的[损失函数](@article_id:638865)会产生一个优化问题，而这些问题可能异常难以解决。涉及巨大尺度差异现象的物理系统——比如由[伯格斯方程](@article_id:323487)（Burgers' equation）控制的流体中薄而急剧的[激波](@article_id:302844)——会产生“刚性”的损失地貌[@problem_id:2411076]。想象一个有着极长、极深、极窄峡谷的地形。标准的[优化算法](@article_id:308254)可能会卡住，在峡谷的两壁之间来回反弹，而不是沿着底部前进。成功训练这些模型需要对问题物理学与[数值优化](@article_id:298509)数学之间相互作用的深刻理解，通常需要使用[混合策略](@article_id:305685)，即像 Adam 这样的鲁棒[一阶方法](@article_id:353162)开辟初始路径，而像 [L-BFGS](@article_id:346550) 这样的更精确的二阶方法则精细调整最终解。

此外，我们能预测的内容存在根本性的限制。考虑一个混沌系统，比如著名的洛伦兹吸引子（Lorenz attractor），它是“蝴蝶效应”的典型代表[@problem_id:2411011]。在这样的系统中，初始条件中任何微小的误差都会随着时间呈指数级放大。我们可以构建一个完美学习了洛伦兹方程的 PINN。但是，即使在其训练区间结束时存在一个微小且不可避免的近似误差，这个误差也将作为[外推](@article_id:354951)的新“初始条件”。这个误差将呈指数级增长，模型的预测轨迹将迅速偏离真实轨迹。

这是否意味着[物理信息学习](@article_id:297248)在这里失败了？完全不是！这只意味着我们必须谦虚，并清楚我们试图预测的是什么。我们不应要求模型给出所有时间的*精确*轨迹（这是一项不可能的任务），而是可以要求模型学习系统的*统计特性*。例如，我们可以在[损失函数](@article_id:638865)中添加一个物理约束，强制执行已知的[相空间体积](@article_id:315608)收缩率，这是洛伦兹系统流的一个全局属性[@problem_id:2411011]。这有助于确保模型的长期行为保持在正确的“吸引子”上，产生与真实系统在统计上无法区分的轨迹，即使它们不是点对点相同的。这是一种深刻的视角转变：建模的目标不总是完美的预测，而往往是完美的理解。

当我们要将模型调整到新场景时，同样的原则也适用，即结合现有知识与新数据——例如，将一个为简单矩形内的传热问题训练的模型，应用于具有不同[材料属性](@article_id:307141)的复杂L形物体[@problem_id:2502958]。这种“[分布偏移](@article_id:642356)”是一个重大挑战。一种有原则的方法既不会丢弃旧模型，也不会盲目相信它。相反，它将旧知识作为起点，然后用少量新数据对模型进行微调，同时始终由一个[物理信息](@article_id:312969)损失函数引导，该损失函数强制执行适用于*新*问题的正确控制方程。

所以，原则很简单：利用你所知道的。不要要求机器成为一个纯粹的经验主义者，从零开始发现一切。物理定律是我们拥有的对世界最强大、最简洁的描述。通过教导我们的计算模型去阅读和尊重这些定律，我们不仅使它们更准确，我们还使它们成为科学发现事业中的伙伴。