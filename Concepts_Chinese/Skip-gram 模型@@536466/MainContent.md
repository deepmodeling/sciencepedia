## 引言
机器如何能学习像“国王”(king)这样一个词的意义——不仅仅是作为一串字母，而是作为一个与“王后”(queen)和“王室”(royalty)相关但又与“卷心菜”(cabbage)截然不同的概念？这个问题是现代人工智能的核心，其答案在于分布式假设：一个词的意义由其相伴的词所定义。Skip-gram 模型是一个强大的计算框架，它将这一原则付诸实践，将海量文本转化为一种几何化的意义地图。本文旨在探讨如何将这一优雅的语言学理论转化为实用且可扩展的[算法](@article_id:331821)。我们将首先探索 Skip-gram 的核心原理和机制，从它如何处理文本、通过梯度下降进行学习，到[负采样](@article_id:638971)的计算魔法。随后，在应用部分，我们将看到这个革命性的思想如何超越语言，为生物信息学和[网络科学](@article_id:300371)等不同领域的问题提供一个统一的视角。

## 原理与机制

现代语言理解的核心是一个优美简洁却又意涵深远的想法，最早由语言学家 J. R. Firth 提出：**“观其伴而知其义。”** 这就是**分布式假设**，它表明意义相似的词倾向于出现在相似的上下文中。如果我告诉你，我看到一只毛茸茸的、四条腿的 `____` 在追一个球，你大概能猜到哪些词可以填入这个空格——`dog` (狗)、`puppy` (小狗)，甚至可能是 `cat` (猫)——但几乎不可能是 `sandwich` (三明治) 或 `galaxy` (星系)。词的意义并非内在于词本身，而在于它与其他所有词构成的关系网络中。

**Skip-gram 模型**是这一原则的绝妙计算体现。它将这个哲学思想重构成一个简单的游戏：“给定一个词，你能预测它的邻居吗？”如果我们能构建一台擅长这个游戏的机器，那么这台机器必然已经学到了关于词义的深层知识。

### 原始材料：从文本到训练对

在学习任何东西之前，我们需要处理数据。想象一下，一篇浩瀚的文本，比如整个维基百科，就像一条由词语（或称**词元 (token)**）组成的巨大连续长带。Skip-gram 过程的第一步，就是将这条长带转化为适用于我们游戏的结构化数据集。我们通过在文本上滑动一个“窗口”来实现这一点。

让我们具体说明。假设我们的文本是一个包含 1200 个词元的数组。我们选择一个**中心词**，比如说，文本中的第 110 个词。然后，我们围绕它定义一个**上下文窗口**，比如包括其左边的 9 个词和右边的 9 个词。这些窗口内的词就是它的“伴侣”。对于这一个实例，我们游戏的目标就是根据中心词预测这 18 个上下文词。然后，我们将窗口向右滑动一步，选择第 111 个词作为新的中心词，并重复此过程。

这种机械的滑动操作会生成一个庞大的 `(中心词, 上下文词)` 词对列表。对于像“cat”这样的中心词，我们可能会生成 `(cat, the)`、`(cat, sat)`、`(cat, on)` 和 `(cat, mat)` 这样的词对。正如一个简化的模型过程 [@problem_id:3208041] 所探讨的，通过系统地定义我们如何选择中心词（例如，每隔 20 个词）和上下文词（例如，仅选择与中心词距离为奇数位置的词），我们可以精确地量化生成的训练样本数量。这个数据生成步骤虽然简单，却是构建一切的基础。它将非结构化文本转化为了学习的原始材料。

现在的游戏规则很清晰了：对于数据集中每一个 `(word, context)` 词对，我们希望模型能赋予其一个高概率 $p(\text{context} | \text{word})$。这就是 Skip-gram 的精髓。值得注意的是，这是一种设计选择。我们本可以反向进行游戏：“给定一组上下文词，预测中间的词。”这种替代方案被称为连续[词袋模型](@article_id:640022) (Continuous Bag-of-Words, CBOW)，或者在更现代的形式中称为[掩码语言建模](@article_id:641899) (Masked Language Modeling)，它导向一个不同的目标函数 $p(\text{word} | \text{context})$。正如人们可能预料的那样，提出这两个不同的问题会导致学习到略有不同的关系类型 [@problem_id:3182958]。目前，我们还是坚持 Skip-gram 的问题：一个词与哪些词为伴？

### 意义的几何学与梯度的舞蹈

那么，我们如何构建一台能玩这个游戏的机器呢？我们首先为词汇表中的每一个词赋予一个独一无二的向量，即它的**[嵌入](@article_id:311541) (embedding)**。你可以把这想象成在某个高维“意义空间”中为每个词赋予一个坐标。起初，我们随机地散布这些向量。词“king”（国王）可能与“cabbage”（卷心菜）的距离和它与“queen”（王后）的距离一样近。训练的目标就是移动这些向量，使它们形成一个有意义的几何结构——一张地图，其中“king”靠近“queen”，“Paris”（巴黎）靠近“France”（法国），而这两者都远离“cabbage”。

学习过程是一场由微积分引导的美妙舞蹈，这个过程被称为**梯度下降 (gradient descent)**。想象一下，词“bank”的[嵌入](@article_id:311541)向量在一个二维空间中从原点 $(0,0)$ 开始。现在，模型观察到了一个来自文本的训练样本：`(bank, money)`。模型的任务是调整“bank”的向量，使“money”在未来成为一个更可能的上下文词。它通过给“bank”向量一个微小的“推动”，使其朝向更接近“money”向量的方向移动。

但如果下一个训练样本是 `(bank, river)` 呢？这时，模型会再次给“bank”向量一个推动，但这次是朝向使“river”更可能出现的方向。经过数百万个这样的样本，词“bank”的[嵌入](@article_id:311541)向量被它出现过的所有上下文拉扯和推动。最终，“bank”向量的位置将是所有这些推动的[加权平均](@article_id:304268)。它将停留在一个既合理地靠近空间的“money”（钱）区域，又合理地靠近“river”（河）区域的位置，从而捕捉到它的多种含义，即**多义性 (polysemy)**。

一个极其清晰的思想实验 [@problem_id:3162578] 完美地阐释了这一点。如果我们设计一个玩具世界，其中一个词义（例如，river bank，河岸）只与 $x$ 轴相关，而另一个词义（例如，money bank，银行）只与 $y$ 轴相关，那么梯度——这个告诉我们向量该朝哪个方向推动的数学对象——会完美地分解。一个“river”上下文产生的梯度向量只包含 $x$ 分量，将[嵌入](@article_id:311541)向量水平拉动。一个“money”上下文产生的梯度向量只包含 $y$ 分量，将其垂直拉动。总梯度是这些单独拉力的总和，将[嵌入](@article_id:311541)向量移动到一个反映其不同用法频率和性质的位置。学习就是上下文之间的一场物理“拔河比赛”。

我们甚至可以用物理类比来形式化这个过程 [@problem_id:3156687]。学习目标可以被看作一种[势能函数](@article_id:345549)。每个正确的 `(word, context)` 词对都会产生一种**引力**，就像一根弹簧将它们的[嵌入](@article_id:311541)向量拉到一起。训练的目标就是将[嵌入](@article_id:311541)向量移动到一个低能量配置，一个稳定状态，在这个状态下，一同出现的词在意义空间中紧密地依偎在一起。

### [负采样](@article_id:638971)的魔力

到目前为止的图景虽然优雅，但存在一个令人望而生畏的计算难题。为了计算概率 $p(\text{context} | \text{word})$，我们需要使用一个名为 **softmax** 的函数。
$$
p(\text{context} | \text{word}) = \frac{\exp(\mathbf{v}_{\text{context}} \cdot \mathbf{v}_{\text{word}})}{\sum_{\text{all words } w'}\exp(\mathbf{v}_{w'} \cdot \mathbf{v}_{\text{word}})}
$$
问题出在分母上。为了计算它，我们必须对词汇表中的每一个词进行求和——这可能涉及数百万个词——而且是针对每一个训练样本！这在计算上是不可行的。

这时，一个名为**[负采样](@article_id:638971) (Negative Sampling)** 的天才创举应运而生。我们不再执行从数百万个选项中预测正确上下文词的复杂任务，而是把游戏变得简单得多：“这里有两个词。你能告诉我它们是文本中真实的上下文词对，还是我刚编造的假词对吗？”

对于文本中每个真实的 `(word, context)` 词对（一个“正”样本），我们生成几个假的词对，比如 `(word, cabbage)` 或 `(word, tectonic)`（“负”样本），方法是从词汇表中随机挑选词语。模型的新任务是学会为正样本对输出高分，为负样本对输出低分。

这重构了我们的物理类比 [@problem_id:3156687]。正样本对仍然产生引力，将[嵌入](@article_id:311541)向量拉到一起。但现在，负样本产生了**斥力**，将词的[嵌入](@article_id:311541)向量推离那些随机、不相关的词的[嵌入](@article_id:311541)向量。学习过程变成了一场引力与斥力的美妙平衡，以更高的效率雕塑着[嵌入空间](@article_id:641450)。

### [负采样](@article_id:638971)*真正*学到了什么

乍一看，[负采样](@article_id:638971)似乎是一个聪明但有点缺乏原则的计算技巧。但故事在这里发生了转折，揭示了一个惊人而优雅的真相。事实证明，这个简化的“真假辨别”游戏根本不是一个取巧的办法。正如 Levy 和 Goldberg 的里程碑式工作所示，并在 [@problem_id:3182845] 中详细探讨的，带有[负采样](@article_id:638971)的 Skip-gram (SGNS) 目标函数会隐式地使学到的[嵌入](@article_id:311541)向量具有一个非常特殊的性质：它们的[点积](@article_id:309438) $\mathbf{v}_{\text{word}} \cdot \mathbf{v}_{\text{context}}$ 近似于这两个词之间的**点互信息 (PMI)**，再加上一个常数偏移。

PMI 是一个源于信息论的概念。它衡量两个词共同出现的频率比它们在统计上独立的情况下预期的要高多少。高 PMI 意味着一种强烈的、有意义的关联。因此，SGNS 不仅仅是在学习哪些词会共现；它是在学习近似一种强大的[统计关联](@article_id:352009)度量。

更重要的是，[负采样](@article_id:638971)的参数直接控制着学习的内容。负样本的数量 $k$，以及选择负样本的噪声分布（通常是提升到 $\alpha = 0.75$ 次方的 unigram 分布），系统性地改变了 PMI 方程中的偏移量 [@problem_id:3182845]。这意味着这些不仅仅是随意的超参数；它们是让我们能够微调模型所学习的语义空间本质的杠杆。

### 选择“敌人”的艺术

事实上，选择负样本是一门艺术。选择像“and”或“the”这样完全随机的词作为负样本，提供的信息量并不大。模型可以轻易学会区分 `(cat, sat)` 和 `(cat, the)`。[信息量](@article_id:333051)最丰富的负样本是“难负例”——那些貌似合理但实际不正确的词。对于短语“the cat sat on the `___`”，词“rug”（小地毯）比“galaxy”（星系）是一个更难（因此也更有教育意义）的负例。

我们可以设计精巧的采样器来精确地教模型我们想让它学的东西。一种方法是倾向于选择那些在语义上已经与目标词很接近的负样本，因为这迫使模型学习更细微的差别 [@problem_id:3156761]。另一种创造性的方法是构建一个能够平衡不同类型相似性的采样器 [@problem_id:3156724]。我们可以混合**语义距离**（基于[嵌入](@article_id:311541)相似度）和**句法距离**（基于拼写，如 Levenshtein 距离）。通过调整参数 $\lambda$，我们可以告诉模型在多大程度上关注拼写与意义，引导它学习对两者都敏感的表示。

### 文本的局限：接地问题

分布式假设以及由此衍生的 Skip-gram 模型，是人工智能领域最成功的思想之一。然而，它有一个根本的局限性。它从文本中学习，且仅从文本中学习。当文本不完整、有偏见或充满隐喻时，会发生什么？

思考一个引人入胜的思想实验 [@problem_id:3182902]。想象我们创建一个特殊的语料库，其中“lion”（狮子）这个词*只*出现在比喻性的语境中，比如“Richard the Lionheart”（狮心王理查）或“he fought like a lion”（他像狮子一样战斗）。一个用这个文本训练的模型会学到，狮子与勇敢、国王和战斗相关联。它无从知晓狮子也是一种生活在非洲的大型食肉猫科动物。这种意义是未接地的，与物理世界脱节。

这就是所谓的**接地问题 (grounding problem)**。解决方案在于认识到语言并非一个自给自足的系统。要真正理解意义，我们必须将词语与它们所描述的世界联系起来。[表示学习](@article_id:638732)的下一个前沿是构建**多模态模型 (multimodal models)**，这些模型不仅从文本中学习，还从图像、声音和像百科全书这样的结构化知识库中学习 [@problem_id:3182902]。通过训练一个模型，将“lion”这个词与其文本上下文*以及*狮子的图片关联起来，我们可以将其意义根植于一个更丰富、更稳健的现实中。这场始于预测一个词的伴侣的简单游戏的旅程，最终将我们引向统一语言与所有其他形式人类知识的宏大挑战。

