## 应用与跨学科联系

我们花了一些时间探讨信息论的基本原理，如熵和互信息。现在，你可能会问：“这一切都非常优雅，但它到底有何用处？”这正是你能提出的最好的问题！这些思想的美妙之处不仅在于其数学上的工整，更在于它们在广阔的科学学科领域中描述、甚至指导学习和发现过程的非凡力量。让我们踏上一段旅程，看看这些原理在实践中的应用，从构建预测[算法](@article_id:331821)的务实任务，到破译生命语言的宏大挑战。

### 提出正确问题的艺术

想象一下，你正在编写一个简单的计算机程序，以判断贷款申请人是否可能违约。你有一份过往申请人的名单及其特征——收入、年龄、债务等等——并且你知道他们最终是否偿还了贷款。构建模型的一个简单方法是构造一个决策树，即一系列关于申请人的“是/否”问题。“他们的收入是否大于$50,000？”如果是，向左走；如果否，向右走。“他们的年龄是否小于30岁？”等等。但在每一步中，要问的*最佳*问题是什么？

信息论在这里给了我们一个异常清晰的答案。最好的问题是那个能为我们提供关于最终结果的最多*信息*的问题。用形式化的语言来说，在树的每个分支点，我们选择能最大化**信息增益**的划分 $S$——这个量恰好是划分与类别标签 $Y$（违约或不违约）之间的互信息 $I(Y; S)$。最大化信息增益等同于选择那个能最大程度减少我们对结果的不确定性（即熵）的划分 [@problem_id:2386919]。这是一种贪心策略，但也是一种极其直观的策略：在每个阶段，你都在寻找最具揭示性的线索。你不仅仅是在划分数据，你是在主动地减少无知。

### 从数据中提炼精华：信息瓶颈

提取最相关信息的想法引导我们走向一个更深层、更普遍的原则，即**信息瓶颈**。想一想一个好的理论，甚至一本书的好摘要，它们完成了什么。它们抛弃了大量不相关的细节，只保留了基本信息，即帮助你理解主题的核心概念。信息瓶颈原理将这种权衡形式化。它指出，对于某些复杂数据 $X$，一个理想的压缩表示 $T$ 应该尽可能简单（通过最小化互信息 $I(T; X)$），同时对于我们想要预测的某个相关变量 $Y$ 尽可能提供信息（通过最大化互信息 $I(T; Y)$）[@problem_id:1631188]。我们正在将来自 $X$ 的信息强行通过一个狭窄的“瓶颈” $T$，以使只有与 $Y$ 相关的信息能够通过。

这不仅仅是一个理论上的奇思妙想，它是现代机器学习大部分领域的指导原则。

考虑构建一个机器学习模型来预测化学或材料科学模拟中分子的能量。原始输入 $X$ 是所有原子的三维坐标集——这是一个巨大的数据量。一种常见的方法是首先为每个原子的局部环境计算一组“描述符”或“对称函数”$\mathbf{G}_i$。这个描述符向量就是我们的压缩表示 $T$。然后神经网络从这些描述符中预测能量。如果我们的描述符函数设计得不好——如果两个物理上不同但本应具有不同能量的原子环境被映射到同一个描述符向量——那么预测能量所必需的关键信息就将无法挽回地丢失了。这个描述符成了一个过度的瓶颈，无论后续的神经网络多么强大，它都永远无法恢复这些丢失的信息 [@problem_id:2456300]。这凸显了表示的关键作用：它必须保留正确类型的信息。

也许信息瓶颈原理最引人注目的现代例子来自计算生物学领域。研究人员现在使用包含来自生命之树各处数十亿个蛋白质序列的庞大数据库来训练巨大的“蛋白质语言模型”。通过一个与最小化交叉熵相关的自监督目标，模型学习预测序列中缺失的氨基酸。为了做好这一点，它必须学习蛋白质“语言”的统计规则——这些规则是由数十亿年的进化写就的。模型学习将一个巨大的蛋白质序列 $X$ 压缩成一组称为嵌入向量的数值向量，即我们的 $T$。因为蛋白质中的结构和功能约束在氨基酸之间产生了统计依赖性，结果表明，那些最能预测序列的嵌入向量恰恰是捕获了关于蛋白质结构和功能 $Y$ 信息的向量 [@problem_id:2749082]。模型在从未被告知蛋白质的结构或功能是什么的情况下，将它们作为压缩序列信息的最高效方式而发现。这是将学习视为有效压缩的一个惊人例证。

### 探索发现的指南针：主动学习

到目前为止，我们已经用信息论来理解模型如何从给定的数据中学习。但是，如果我们能用它来决定首先要收集什么数据呢？在许多科学和工程领域，获得一个标记数据点——进行一次湿实验、执行一次大规模量子化学计算——是极其昂贵和耗时的。我们无法承担测量一切的代价。我们必须明智地选择我们的实验。

在这里，信息论再次为我们提供了指南针。**主动学习**的原则建议我们应该执行那个我们预期会给我们带来最大可能信息增益，或者说导致我们模型不确定性最大程度减少的实验。

想象一下，你正在试图绘制一个分子的势能面以理解一个化学反应，或者正在建模一个酶的特异性如何随着你突变其氨基酸序列而改变。你可能会从几次测量开始，并拟合一个初步模型，也许是一个高斯过程，它不仅提供预测，还提供了其在整个可能性空间中的不确定性估计 [@problem_id:2648580] [@problem_id:2713847]。接下来你在哪里测量？你应该查询你的模型最不确定的点！这便是**不确定性采样**的精髓。通过在高度无知的区域进行测量，你将学到最多。

我们可以将这一点表达得非常精确。对于一种简单但常见的贝叶斯模型，从一次新实验中预期的信息增益 $I$（以奈特为单位）可以精确计算。它具有以下形式：
$$
I = \frac{1}{2}\ln\left(1 + \frac{\text{模型不确定性}}{\text{测量噪声}}\right)
$$
这个优雅的公式 [@problem_id:2760122] 告诉我们一些深刻的道理。你期望学到的东西数量取决于你模型当前的不确定性（信号）与你的测量设备有多嘈杂（噪声）的比率。如果你的模型已经非常确定，或者你的测量非常嘈杂，你就不期望学到太多。但是，如果你模型在一个可以进行干净测量的区域内不确定，那么发现的潜力就很大。这个单一的方程概括了科学探究的经济学。

这让我们回到了我们的蛋白质语言模型。最终目标通常是蛋白质*设计*——创造一种新的酶或治疗药物。仅凭少数昂贵的实验室测量，我们就可以使用**贝叶斯优化**。我们在蛋白质语言模型学到的丰富嵌入空间之上拟合一个代理模型（如高斯过程）。然后，我们使用一个“采集函数”，这只是一个花哨的名字，其策略是平衡利用已知的高活性区域和*探索*不确定区域以最大化信息增益。这种表示学习和主动学习的强大结合，正使得新型生物分子的设计效率大大提高 [@problem_id:2749082]。

### 探索自然语言

最后，信息论和机器学习不仅为我们提供了一套构建模型的工具，还为我们提供了一套分析和理解复杂系统的工具——将我们的模型用作虚拟显微镜。

让我们回到生物学。基因组常被称为“生命之书”，但它是用什么语言写成的？我们可以用一种擅长学习序列的模型——循环神经网络（RNN），在一个基因组的非蛋白质编码部分（基因间DNA）上进行训练。模型学习统计模式，并为任何给定位置的下一个“字母”（A、C、G或T）输出一个概率。然后我们可以使用交叉熵 $H$ 在一个预留的测试集上衡量其性能。**困惑度**，定义为 $2^H$，代表模型对下一个字母的有效选择数；较低的困惑度意味着序列更可预测，结构性更强。

当这个在基因间DNA上训练的模型被要求读取蛋白质编码DNA时，它的困惑度显著上升。它对在那里发现的模式感到更加“惊讶”。这以一种定量的方式告诉我们，编码和非编码DNA代表了基因组语言的不同统计“方言”。此外，我们可以将模型的性能与一个完全随机的DNA序列的理论熵进行比较，后者恰好是每碱基 $2$ 比特。模型实现了低于 $2$ 的交叉熵（例如 $1.85$ 比特/碱基）这一事实，明确地证明了它捕捉到了真实的、非随机的生物结构 [@problem_id:2425710]。

这种视角甚至可以延伸到基础物理学。在[量子化学](@article_id:300637)中，一个核心挑战是近似“[交换相关泛函](@article_id:302482)”，这是[密度泛函理论](@article_id:299475)中的一个关键组成部分。[现代机器学习](@article_id:641462)方法旨在从数据中学习这个泛函。一个关键的见解是，底层的物理是非局域的；空间中某一点的电子发生的情况取决于其他所有地方的电子密度。因此，一个成功的机器学习模型*必须*使用能够承载这种非局域信息的输入来构建，例如在整个空间上对密度进行积分，或者使用非局域的[量子力学轨道](@article_id:352971)本身 [@problem_id:2464269]。信息的结构必须与它试图描述的现实的结构相匹配。

从[决策树](@article_id:299696)的简单逻辑到[蛋白质进化](@article_id:344728)和量子力学的广阔、复杂的图景，信息论提供了一条统一的线索。它教导我们，学习是一个压缩的过程，一个不确定性的减少，一个对知识的引导式搜索。它是我们的[算法](@article_id:331821)和我们自己的科学思维都必须用来交易的货币。