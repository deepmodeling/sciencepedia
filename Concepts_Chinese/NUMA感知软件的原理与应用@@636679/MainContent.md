## 引言
在现代多插槽计算机的架构中，并非所有内存都是生而平等的。处理器与内存条之间的物理距离造成了访问速度的层级差异，这一概念被称为[非统一内存访问](@entry_id:752608)（NUMA）。虽然这种设计实现了大规模的可扩展性，但它也带来了一个隐藏的挑战：不了解这种拓扑结构的应用会因频繁、缓慢的“远程”内存访问而遭受严重的性能下降。本文旨在揭示NUMA感知软件设计的原理，以填补这一知识空白。

本次探索将为您提供一个审视系统性能的新视角。首先，“原理与机制”一章将剖析[NUMA系统](@entry_id:752769)的基本物理原理，解释诸如延迟叠加等概念，并介绍[数据放置](@entry_id:748212)和[处理器亲和性](@entry_id:753769)的核心策略。接着，“应用与跨学科关联”一章将展示这些原理在现实世界中如何被巧妙应用，从[操作系统](@entry_id:752937)和虚拟化[虚拟机](@entry_id:756518)管理程序（hypervisor）的最深层内核，到[高性能计算](@entry_id:169980)和[科学计算](@entry_id:143987)的严苛前沿领域。

## 原理与机制

要理解如何构建NUMA感知的软件，需要超越具体的软件实现，去探究支配信息流动的基本原理。这些原理关乎的不是代码本身，而是计算机架构内部空间和时间的物理约束。

### 距离的制约

想象一座现代化的大学图书馆，这是一个信息[分布](@entry_id:182848)的奇迹。它不再是单一、庞大的建筑，而是由几座分散在校园各处的专业图书馆组成——一座用于科学，一座用于人文学科，一座用于法律。对于物理楼里的学生来说，从本地的科学图书馆取书，只需穿过大厅，非常快捷。这便是**本地内存访问**。但要从远在校园另一端的法学院图书馆取一本法律文献呢？那就需要走很长一段路了，这是一次**远程内存访问**。这个简单的类比抓住了**[非统一内存访问](@entry_id:752608)（NUMA）**架构的精髓。

在计算机的世界里，“阅览室”是CPU插槽，每个插槽都有自己直接连接的高速内存——它的本地图书馆。“横穿校园的步行”则是跨越连接各个插槽的高速互联总线的一段旅程。虽然速度极快，但这段旅程并非瞬时完成。一次本地内存访问可能耗时，比如说，$t_L = 80$ 纳秒，而一次远程访问则可能需要 $t_R = 140$ 纳秒。在旧的**统一内存访问（UMA）**世界里，只有一个中央图书馆；对每个人来说，无论坐在哪里，访问时间 $t_U$ 都是相同的。

一次长途的校园穿行可能只是个小麻烦。但如果你在参加一场寻宝游戏，每条线索都藏在不同的书里，而每本书又可能在任何一个图书馆呢？一个在内存中“追逐指针”（即下一个数据片段的地址只有在读取当前数据后才能知晓）的程序，正是在进行这样一场寻宝游戏。如果它的数据是随机散布的，它可能执行一次本地访问，然后一次远程访问，接着又是一次远程访问。由于每一步都依赖于上一步，延迟会串行累加。这就是**延迟叠加**的危险所在。

我们可以非常清晰地看到这种效应。如果一个程序执行 $L$ 次这样的依赖性内存访问，它在UMA机器上的总时间就是 $L \cdot t_U$。在NUMA机器上，如果任何给定访问是远程的概率为 $p$，那么单次访问的*期望*时间是 $t_L(1-p) + t_R p$。总的期望时间则变为 $L \cdot (t_L(1-p) + t_R p)$。性能损失，我们可以称之为“延迟叠加[放大因子](@entry_id:144315)”，是两者之比，简化后得到一个极具启发性的表达式：

$$
A = \frac{t_L(1-p) + t_R p}{t_U}
$$

这个小公式揭示了我们的敌人：远程访问概率 $p$。从某种意义上说，所有NUMA感知软件都是在努力将这个值尽可能地降到接近零 [@problem_id:3687002]。

### 第一原理：将数据置于工作单元附近

如果问题在于到数据的距离，那么最直接的解决方案就是将数据移近。这属于**NUMA感知的[内存分配](@entry_id:634722)**领域。[操作系统](@entry_id:752937)作为系统的总图书管理员，在这里扮演着核心角色。

大多数现代[操作系统](@entry_id:752937)采用一种既简单又美妙有效的[启发式](@entry_id:261307)策略：**首次接触（first-touch）策略**。当一个程序请求一个新的内存页时，[操作系统](@entry_id:752937)不会立即为其分配物理位置，而是会等待。只有当程序首次*写入*该页时——即“首次接触”——[操作系统](@entry_id:752937)才会迅速为其寻找一个物理家园。它会把页放在哪里呢？就放在执行此次接触的CPU的本地内存库中。就像图书管理员把一本新书放在首次请求它的阅览室的书架上一样。

对于一个在处理数据前会先完成所有数据初始化的程序来说，这个策略简直是神奇。通过确保初始化工作由之后将进行处理的同一线程完成，我们可以保证其绝大部分数据都驻留在本地内存中。这个策略将远程访问概率 $p$ 推向零，从而有效地“抚平”了延迟叠加，恢复了性能 [@problem_id:3687002]。

当然，世界很少如此简单。如果一个程序混合了多种行为模式该怎么办？设想一个需要巨大内存带宽的大型流式处理任务，与一个对延迟敏感的计算[任务并行](@entry_id:168523)工作，同时两者都查询一个共享表 [@problem_id:3687071]。一个复杂的NUMA策略会为每种情况使用不同的策略：
- **对于流式处理任务**，通过首次接触实现本地放置至关重要。其性能受限于“图书馆门口的宽度”（内存带宽），而通往远程图书馆的门口要窄得多（互联带宽有限）。
- **对于延迟敏感的任务**，本地放置是最小化其[平均内存访问时间](@entry_id:746603)的关键。
- **对于共享表**，**交错（interleaving）**策略通常是最佳选择。[操作系统](@entry_id:752937)将表的页条带化地[分布](@entry_id:182848)在所有内存节点上——一页在这里，下一页在那里。这提供了一个公平的折中方案，让所有线程都能混合获得快速的本地访问和较慢的远程访问，并平衡了整个系统的[内存控制器](@entry_id:167560)负载。

### 第二原理：将工作单元置于数据附近

有时，数据是不可移动的或天生就是[分布](@entry_id:182848)式的。在这些情况下，我们反转策略：如果不能把图书馆搬到学生面前，我们就把学生送到图书馆去。这就是**[处理器亲和性](@entry_id:753769)**的原理。[操作系统调度](@entry_id:753016)器可以把一个线程“钉”在（pin）一个特定的CPU或一组CPU上，确保它总是在其数据附近运行。

但[操作系统](@entry_id:752937)如何智能地做出这个选择呢？它可以进行一个快速而巧妙的计算。它观察一个线程的阅读清单——即其内存访问模式，我们可以将其建模为一个[概率向量](@entry_id:200434) $\mathbf{p} = (p_1, p_2, \dots, p_N)$，其中 $p_j$ 是访问内存节点 $j$ 的比例。它也知道校园地图——即系统的访问[成本矩阵](@entry_id:634848) $D_{ij}$，表示在插槽 $i$ 上运行时访问节点 $j$ 上内存所需的时间。为了给线程找到最佳的归属，它只需计算如果线程在每个插槽 $i$ 上运行时的期望“总步行时间”：

$$
C(i) = \sum_{j=1}^{N} p_j D_{ij}
$$

自然，[操作系统](@entry_id:752937)倾向于将[线程调度](@entry_id:755948)到能最小化此期望成本的插槽 $i^{\star}$ 上，即 $i^{\star} = \arg\min_{i} C(i)$。这被称为**软亲和性**——一个强烈的偏好，但并非不可打破的规则 [@problem_id:3672843]。

有时，这还不够。如果一个线程即使在其最优插槽上仍然表现不佳（即远程内存访问率很高），这可能意味着该线程的工作负载根本上就是非本地的。在这种情况下，调度器可能会升级到**硬亲和性**，或称之为绑定。它将线程锁定在其已知的最佳位置。这看似严厉，但它防止了调度器的另一个目标——[负载均衡](@entry_id:264055)——通过将线程移动到更不理想的插槽而使情况变得更糟。

### 动态世界的现实：迁移的成本

世界并非静止。一个线程的行为可能会改变，或者系统负载可能会变得不均衡。这就需要将线程在插槽之间移动，这个过程称为**迁移**。但迁移不是没有代价的。

当一个线程运行时，它的数据会“[预热](@entry_id:159073)”本地缓存——即CPU的个人专属、超高速便签本。将一个[线程迁移](@entry_id:755946)到另一个插槽，就像一个工匠搬到一个新的工作室，那里没有一件他熟悉的工具摆放好。新的缓存是“冷”的。在重新从主内存中取回其[工作集](@entry_id:756753)时，线程会遭遇一连串的初始缓存未命中。更糟糕的是，如果它的数据页仍然在原始节点上，那么每一次未命中都会变成一次代价高昂的远程访问。这就是**冷缓存迁移成本** [@problem_t_id:3661545]。

一个智能的调度器遵循一个简单的权衡法则：只有当平衡负载带来的预期性能增益大于迁移本身造成的性能损失时，它才应该迁移线程。这种计算必须是任何NUMA感知[负载均衡](@entry_id:264055)器的核心。

这导致了迁移策略中一个微妙但重要的区别：**推送迁移（push migration）与拉取迁移（pull migration）**。推送迁移是一种主动策略，一个繁忙的节点将任务“推”给一个较不繁忙的节点以缓解过载。拉取迁移是被动的；一个空闲的核心从一个繁忙的队列中“拉”取一个任务。虽然听起来相似，但它们的典型实现在NUMA方面有非常不同的影响。由空闲状态触发的拉取迁移策略，几乎总会先尝试在自己的插槽上寻找工作，然后才会从远程节点窃取。它对亲和性有天然的尊重。而由过载驱动的推送策略，为了立即缓解负载，可能更愿意支付NUMA的代价。这种差异是如此深刻，以至于它甚至可以改变内部OS操作的性能，比如处理一个次要页错误（minor page fault）——针对已在内存中的数据的错误——因为[操作系统](@entry_id:752937)自身的[数据结构](@entry_id:262134)也受制于同样的局部性法则 [@problem_id:3674396] [@problem_id:3668867]。

### 精通游戏：主动与协同设计策略

最先进的NUMA感知软件不仅仅是对系统状态做出反应，它还会预测系统状态。

考虑一个[硬件预取](@entry_id:750156)器，这是一个绝妙的小电路，它能检测到您正在顺序读取内存，并自动为您取回下一个缓存行。它就像一个图书管理员的助手，看到你在读第1、2、3页……就为你拿来了第4页。然而，许多这种简单的硬件助手在页边界处会戛然而止。如果你数据的下一页恰好位于远程节点上，预取器就会放弃，你的程序将为那完整的140纳秒而停顿。

一个真正精湛的NUMA驱动程序可以做得更好。它使用**[软件预取](@entry_id:755013)**。它知道循环的速度和[内存延迟](@entry_id:751862)。如果它看到自己正在接近一个远程页边界，它会计算所需的提前时间。例如，如果远程延迟是140纳秒，而每次循环迭代需要1.6纳秒，它需要提前大约 $140 / 1.6 \approx 88$ 次迭代发出预取指令。由于一个页可能只包含64次迭代的数据，这意味着它在处理*前一个*页时就必须展望到*下一个*页。此外，它必须手法轻柔。它不会试图一次性预取整个远程页而压垮CPU有限的请求缓冲区（未命中状态处理寄存器，Miss Status Handling Registers, 或 MSHRs），而是只预取最初的几行。这足以在新页上“启动”[硬件预取](@entry_id:750156)器，然后[硬件预取](@entry_id:750156)器就会愉快地接管工作。这是硬件和软件之间一曲美妙的二重奏 [@problem_id:3687045]。

这种协作精神是未来。终极的NUMA感知系统是硬件和软件协同设计的系统。想象一下，将NUMA“提示”直接嵌入硬件的页表项中。在TLB未命中时，当硬件已经在获取[页表项](@entry_id:753081)时，它还可以读取几个额外的比特位。这些比特位可能编码了最近访问该页的节点的ID，以及一个微小的**饱和计数器**，用于跟踪这种远程访问模式的“热度”。一个在远程接触时递增、在本地接触时递减的计数器，提供了一个鲁棒的信号，过滤掉了瞬时噪声。在关键路径上不增加任何额外的内存访问，硬件就能为[操作系统](@entry_id:752937)提供丰富、低延迟的数据，以完美地指导其迁移决策。这不是科幻小说；这正是那种将NUMA的挑战从负债转变为优势的优雅、有原则的设计 [@problem_id:3651022]。

从简单的放置规则到复杂的预测算法，NUMA感知软件的原理证明了一个理念：理解一个系统的物理现实是释放其全部潜能的关键。

