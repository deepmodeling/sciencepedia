## 引言
我们如何从过去中学习，以便为未来做出更好的决策？尤其是当过去并非由我们现在想要考虑的选择所塑造时。这个问题是强化学习的核心，并由**[离策略评估](@article_id:361333)**这一概念进行形式化探讨。无论是金融交易策略、医疗方案还是气候干预措施，我们都需要评估未经检验的新策略，而[离策略评估](@article_id:361333)解决了这一关键需求——仅使用在不同现有策略下收集的历史数据，从而避免了真实世界实验的成本、风险和时间。本文将引导您了解这项强大但充满风险的技术。首先，在“原理与机制”一章中，我们将剖析[重要性采样](@article_id:306126)的核心思想、高方差的危险，以及可能导致学习灾难性失败的臭名昭著的“死亡三元组”。随后，“应用与跨学科联系”一章将揭示这些概念如何为医学、金融等不同领域的“情景假设”推理提供一种形式化语言，从而改变我们从历史中学习的方式。

## 原理与机制

### 他人经验的诱惑

假设您想了解一种未经测试的新药可能对患者产生何种影响。您不会想直接给药然后观察结果吧？那可能极其危险。相反，您可能会查看那些服用过其他类似药物的患者的历史数据。您拥有丰富的经验，但这些经验并不完全“对口”。您如何利用这些现有数据来预测新情景下会发生什么呢？

这就是**[离策略评估](@article_id:361333)**的核心问题。在[强化学习](@article_id:301586)中，我们通常拥有大量由某个旧策略——我们称之为**行为策略（behavior policy）**，$b$——下行动的智能体所收集的数据。我们希望评估一个新的、精妙的**目标策略（target policy）**，$\pi$，而无需在真实世界中运行它。为什么？因为运行它可能成本高昂、耗时，或者像我们的药物例子一样，充满风险。

主要动机是**[样本效率](@article_id:641792)**。相比之下，一个**在策略（on-policy）**[算法](@article_id:331821)有点像一个数据“势利眼”。它坚持只从其*当前*策略产生的经验中学习。每次进行微小的策略更新后，它会丢弃所有旧数据，然后去收集一批新数据。这样做安全又稳定，但极其浪费。而一个**离策略（off-policy）**[算法](@article_id:331821)则是一个数据“囤积者”。它保留一个巨大的“[经验回放](@article_id:639135)[缓冲区](@article_id:297694)”，其中装满了过去的经验，并反复重用这些经验进行学习。想象一下像深度确定性[策略梯度](@article_id:639838)（DDPG）这样的[算法](@article_id:331821)，它可以通过与世界的一次交互进行多次学习更新。这使其能够在稳定的环境中学习得更快，例如具有稳定动态的金融市场[@problem_id:2426683]。

但这种能力也伴随着一个陷阱。如果世界突然改变——比如，一次市场崩溃引入了新的“[范式](@article_id:329204)”（regime）——离策略智能体的[经验回放](@article_id:639135)缓冲区里充满了不再反映现实的“过时”数据。基于这些旧数据进行训练可能会带来灾难性的后果。而总是在使用新数据的在策略智能体，在这种情况下会适应得快得多。数据重用与适应变化之间的这种博弈是强化学习中的一个基本矛盾[@problem_id:2426683]。那么，我们究竟是如何实现这种从他人经验中学习的“魔术”呢？

### 重加权的魔力：[重要性采样](@article_id:306126)

让我们回到数据的类比。假设您想估计一个国家所有成年人的平均身高，但您唯一的数据来自一个在篮球大会上进行的调查。您的样本显然偏向于非常高的人。如果您只是简单地计算数据的平均值，您会得到一个高得离谱的数字。

您该如何纠正这个问题？很简单：应用一个“修正因子”。您知道非常高的人在篮球大会上比在总人口中常见得多。所以，对于样本中的每一个高个子，您在最终的平均值计算中给予他们一个较小的权重。而对于您碰巧找到的稀有的矮个子，您则给予他们一个大得多的权重。这种重加权技术被称为**[重要性采样](@article_id:306126)**。

在[离策略评估](@article_id:361333)中，我们做的完全一样。我们有一条由行为策略 $b$ 收集的状态、动作和奖励的轨迹，$(s_0, a_0, r_1, s_1, \dots)$。但我们想知道目标策略 $\pi$ 的[期望](@article_id:311378)回报。行为策略 $b$ 所采取的动作可能与目标策略 $\pi$ 会采取的动作大相径庭。为了纠正这一点，我们将每个奖励按该动作在两个策略下发生的概率之比进行加权。这就是**重要性比率**：

$$
\rho_t = \frac{\pi(a_t \mid s_t)}{b(a_t \mid s_t)}
$$

如果目标策略比行为策略更有可能采取某个动作，那么该动作结果的权重 $\rho_t \gt 1$。如果可能性更小，则权重 $\rho_t \lt 1$。

要估计从初始状态开始的总价值，我们不能只将这个权重应用于单个奖励。在轨迹开始时采取的动作会影响所有后续的状态和奖励。因此，在时间 $t$ 获得的奖励的权重必须考虑到截至该时间点的所有动作。在**逐决策[重要性采样](@article_id:306126)（PDIS）**中，估计的价值是折扣奖励的总和，其中每个奖励 $r_{t+1}$ 都由截至时间 $t$ 的重要性比率的累积乘积加权：

$$
\hat{V}_{\text{PDIS}}^\pi(s_0) = \sum_{t=0}^{T-1} \gamma^t \left( \prod_{k=0}^{t} \rho_k \right) r_{t+1}
$$

这个公式让我们能够利用一个策略生成的轨迹，魔术般地计算出另一个策略的价值的无偏估计[@problem_id:2738642]。这是使[离策略评估](@article_id:361333)成为可能的基础机制。

### 重加权的风险：方差爆炸

这种重加权听起来好得有些不真实。在很多方面，确实如此。[重要性采样](@article_id:306126)有其黑暗面：它可能遭受灾难性的**高方差**。

让我们回到我们的类比。您正试图估计成年人的平均身高，但您的行为策略是从NBA全明星赛中抽样，而您的目标策略是从幼儿园班级中抽样。行为策略（抽样NBA球员）碰巧抽到一个五岁孩子的概率几乎为零。但是，如果奇迹般地发生了，那个样本的重要性比率将是*巨大*的（在幼儿园班级中见到一个五岁孩子的概率除以在全明星赛上见到一个的近零概率）。您对幼儿园儿童平均身高的整个估计将被这一个被极度高估的样本所主导。最终的估计会变得极不可靠——其方差将是巨大的。

这正是[强化学习](@article_id:301586)中当目标策略 $\pi$ 与行为策略 $b$ 非常不同时出现的问题。一条轨迹的总权重是许多比率的乘积：$W = \prod_{t=0}^{H-1} \rho_t$。如果在任何时刻，行为策略采取了一个在目标策略下非常不可能发生的动作，那么比率 $\rho_t$ 会接近于零。相反，如果它采取了一个在目标策略下非常*可能*发生但在行为策略下不太可能的动作，$\rho_t$ 可能会非常大。将这些比率在一个长的时间范围 $H$ 内相乘意味着最终的权重 $W$ 可能会爆炸或消失。该[估计量的方差](@article_id:346512)会随着时间范围的长度**呈指数级**增长[@problem_id:2738653]。这使得标准的[重要性采样](@article_id:306126)估计器在长时间任务中几乎无法使用。

一个常见的解决方法是**加权[重要性采样](@article_id:306126)**（也称为自标准化[重要性采样](@article_id:306126)）。我们不是简单地计算加权回报的平均值 $\sum_i W_i G_i$，而是将其除以权重的总和：$\frac{\sum_i W_i G_i}{\sum_i W_i}$。这有效地对权重进行了[归一化](@article_id:310343)，防止单个轨迹完全“绑架”整个估计。它显著降低了方差，但这是有代价的：它在我们的估计中引入了少量的**偏差**。这是在统计学和机器学习中普遍存在的**偏差-方差权衡**的一个绝佳例子[@problem_id:2738653]。

### 死亡三元组：灾难的配方

到目前为止，我们已经看到[离策略评估](@article_id:361333)是一种微妙的平衡。但情况可能会变得更糟，更糟得多。真正的危险出现在我们将[离策略学习](@article_id:638972)与现代[强化学习](@article_id:301586)中另外两个主力工具结合使用时。这种组合是如此臭名昭著地不稳定，以至于被称为**死亡三元组**：

1.  **[离策略学习](@article_id:638972)**：正如我们一直在讨论的，从一个不同策略生成的数据中学习。
2.  **函数近似**：我们无法在表格中为每个状态储存一个值；状态通常太多了。因此我们近似价值函数，例如使用线性函数 $v(s) \approx \phi(s)^{\top}w$ 或深度神经网络。这就像试图只用一把直尺来捕捉一条复杂的波浪线。它是一种近似，其[表达能力](@article_id:310282)从根本上是有限的。
3.  **[自举](@article_id:299286)法（Bootstrapping）**：我们不等待一整个回合结束时的全部回报，而是根据我们对*下一个*状态价值的*当前估计*来更新当前状态的价值估计。这是时间[差分](@article_id:301764)（TD）学习的核心思想。实际上，我们是在“通过自己的鞋带把自己拉起来”。

当这三种成分混合在一起时，结果可能是灾难性的。价值估计可能会发散，螺旋式地趋向于无穷大。

让我们来看一个简单而令人不寒而栗的例子。想象一个只有两个状态的环境，$s_1$ 和 $s_2$。在我们的目标策略 $\pi$ 下，我们总是从 $s_1$ 移动到 $s_2$，而 $s_2$ 是一个[吸收态](@article_id:321440)。所有奖励都为零。显而易见，两个状态的真实价值都为零。现在，假设我们尝试使用一个简单的线性函数近似器 $v(s) = w \cdot \phi(s)$ 和带有离策略数据的TD学习来学习这个价值。我们的数学表明，对于某些特征 $\phi$ 和离策略数据分布，我们的权重参数 $w$ 的[期望值](@article_id:313620)并不会收敛到零。相反，它遵循一个类似 $w_{k+1} = (1 + c) w_k$ 的规则，其中 $c$ 是某个小的正常数。它会永远呈指数级增长！[@problem_id:2738617] 我们的价值估计爆炸了。我们甚至可以模拟这个过程，观察到参数向量不仅在增长，而且完美地与系统中的“不稳定”方向对齐[@problem_id:2738616]。

问题出在哪里？我们可以把它看作一场完美风暴。
- 离策略数据将我们的价值更新推向一个与真实[价值函数](@article_id:305176)结构不符的方向。
- 函数近似将一个状态上的这个“坏”更新强制“溢出”并影响其他状态，因为单个参数 $w$ 将它们联系在一起。
- 自举法将新近被破坏的价值估计用作下一次更新的目标，从而创建了一个恶性反馈循环。误差没有缩小；它在不断累积。

从更深层的数学角度看，在策略TD学习是稳定的，因为其底层的更新算子是一个**收缩**——在适当选择的范数下，它保证能缩小误差，就像将一个数反复乘以0.99总会使其更接近零一样。但当我们转向离策略时，这个被称为**投影贝尔曼算子**的算子不再保证是收缩的。它可能变成一个**扩张**，其“[谱半径](@article_id:299432)”大于1[@problem_id:2738666][@problem_id:2703351]。在这种情况下，误差在每一步都被放大，导致我们观察到的发散。

### 驯服野兽：实用解决方案与权衡

死亡三元组听起来很可怕，但幸运的是，故事并没有就此结束。研究人员已经开发了一系列技术，以使[离策略学习](@article_id:638972)变得稳定和有效。

一个直接的方法是打破这个三元组。例如，在我们那个发散的例子中，如果我们用**[蒙特卡洛方法](@article_id:297429)**（等待完整的、零奖励的回报）来代替自举法，不稳定性就会消失[@problem_id:2738617]。然而，这又带回了我们最初想通过使用TD学习来逃避的高方差问题。我们再次面临权衡。

另一条路径是完全使用不同的[算法](@article_id:331821)。标准TD学习是一种迭代的、[随机近似](@article_id:334352)的方法。另一种选择是**最小二乘时间差分（LSTD）**，这是一种批处理方法。它接收一大块数据，构建一个代表所需[不动点](@article_id:304105)的大型线性方程组，并直接求解。LSTD在**数据效率**上高得多——它能从每个样本中榨取更多信息。在离策略场景中，它也可能更稳定，因为它没有那种迭代反馈循环。然而，这种强大能力带来了高昂的计算代价。LSTD所需内存和计算量分别与特征数量成二次和三次关系（$O(d^2)$ 和 $O(d^3)$），这使得它对于今天使用的大型神经网络来说是不可行的。此外，如果特征是病态的，非正则化的LSTD本身也可能不稳定。在这种情况下，更简单、成本更低的TD[算法](@article_id:331821)实际上可能表现更好[@problem_id:2738615]。

[离策略学习](@article_id:638972)的现代前沿涉及更复杂的方法，例如通过额外的梯度修正项来修改TD更新、约束目标策略使其与行为策略保持“接近”，或者开发全新的[目标函数](@article_id:330966)。目标始终如一：在小心翼翼地驾驭方差和不稳定性的险恶环境的同时，获得[离策略学习](@article_id:638972)带来的惊人数据效率优势。这段旅程是[统计估计](@article_id:333732)、线性代数和优化之间持续而美妙的相互作用，一切都是为了从丰富的经验织锦中学习。