## 引言
一个智能体如何通过试错来学习做出最优决策，尤其是在其选择本质上是随机的情况下？这个问题是强化学习的核心。尽管[深度学习](@article_id:302462)在梯度下降的推动下蓬勃发展，但当面临随机动作时，这个强大的优化工具却遇到了障碍；我们无法简单地对掷骰子的结果进行[微分](@article_id:319122)。本文将深入探讨一种最优雅的解决方案——REINFORCE [算法](@article_id:331821)，以应对这一根本性挑战。

本文的结构旨在构建一个从基础理论到实际影响的完整理解。首先，“原理与机制”部分将揭示 REINFORCE 背后的数学技巧，即[得分函数](@article_id:323040)估计器（score function estimator）。该部分还将直面该[算法](@article_id:331821)的关键弱点——高方差，并介绍用于控制方差的强大技术，如基线和[演员-评论家](@article_id:638510)框架。随后，“应用与跨学科联系”部分将展示该[算法](@article_id:331821)的多功能性， showcasing 其如何被用于控制物理系统、驱动[生成式人工智能](@article_id:336039)，甚至进行自动化科学发现。读完本文，您不仅将掌握 REINFORCE 的工作原理，还将理解为何它至今仍是现代人工智能的基石之一。

让我们从揭示驱动[策略梯度方法](@article_id:639023)的引擎开始。

## 原理与机制

既然我们已经对强化学习的目标——通过试错学习——有了初步了解，现在让我们揭开帷幕，看看驱动它的引擎。智能体究竟是如何学会做出更优决策的？从图像分类器到语言模型，大多数现代学习系统都依赖于一个极其简单而强大的思想：梯度下降。我们定义一个“损失”或“奖励”函数来衡量我们的表现，然后计算它的梯度——即最陡上升或下降的方向。接着，我们沿着这个方向迈出一小步，微调我们的参数，使其变得更好一些。周而复始。

问题在于，当智能体的行动是随机的时，这幅优雅的图景似乎就失效了。如果一个机器人尝试抓取一个杯子，而它的动作是从一个[概率分布](@article_id:306824)中选择的，我们该如何计算梯度呢？从策略参数到最终奖励的路径被一个随机采样步骤打断了。这就像试图对掷骰子的结果进行微分。作为深度学习核心的[自动微分](@article_id:304940)框架会在此卡住；它无法追踪梯度穿过一个[随机数生成器](@article_id:302131) [@problem_id:2154631]。对于一个部分源于运气的结果，我们究竟如何将“功劳”或“过错”归于策略的参数？这正是[策略梯度方法](@article_id:639023)旨在解决的核心困境。

### 对数技巧：[得分函数](@article_id:323040)

解决方案是一套精妙的数学技巧，这个原理非常核心，以至于被多次独立发现，并有多个名称：**[得分函数](@article_id:323040)估计器**（score function estimator）、**[对数导数](@article_id:348468)技巧**（log-derivative trick），或者在强化学习中被称为 **REINFORCE**。

其核心思想是：我们不去尝试对随机选择的*结果*进行微分，而是对选择本身的*概率*进行微分。我们希望找到一个梯度，当我们沿着它走一步时，能让“好”的动作变得更可能发生，而“坏”的动作变得更不可能发生。

假设我们的策略是 $\pi_{\theta}(a|s)$，即在状态 $s$ 下采取动作 $a$ 的概率，由参数 $\theta$ 决定。$\nabla_{\theta} \log \pi_{\theta}(a|s)$ 这个量被称为**得分**（score）。可以把它想象成一个向量，指向参数空间中能最大程度增加选择动作 $a$ 概率的方向。那么，我们应该如何利用这个得分呢？我们应该用结果的好坏来缩放它！如果我们获得了很高的奖励 $R$，我们就希望朝那个方向前进一大步。如果我们获得的奖励很小或为负，我们就只前进一小步，甚至朝相反方向移动。

这就得到了著名的 REINFORCE 更新法则。[期望](@article_id:311378)奖励 $J(\theta)$ 的梯度是得分与奖励乘积的[期望值](@article_id:313620)：

$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \left( \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \right) R(\tau) \right]
$$

在这里，$\tau$ 代表一条完整的轨迹（状态和动作的序列），$R(\tau)$ 是该轨迹的总奖励 [@problem_id:66109]。在实践中，我们通常可以简化这一点。在时间点 $t$ 做出的决策不会影响过去已经发生的奖励。因此，我们只用*未来*奖励的总和来缩放时间点 $t$ 的得分，这个量被称为**未来回报**（return-to-go），即 $G_t = \sum_{t'=t}^T r_{t'}$。这就导出了[策略梯度](@article_id:639838)估计器更常见的形式：

$$
\nabla_{\theta} J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) G_t \right]
$$

这个单一而优雅的公式具有惊人的通用性。无论动作空间是离散的（如选择“向左”或“向右”）还是连续的，它都适用。奖励的计算方式也无关紧要。只要我们能计算动作的对数概率，我们就能估计这个梯度。我们只需运行策略，收集一条轨迹 $(s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$，计算每一步的得分和回报，然后对多条轨迹的结果进行平均，就能得到真实梯度的估计值 [@problem_id:2738661]。

### 强大力量的代价：方差噩梦

然而，这种通用性是有巨大代价的：**高方差**。想象一下，我们的智能体处在一个情境中，一个动作产生 1000 的奖励，另一个动作产生 999 的奖励。两者都是极好的选择。但如果智能体碰巧尝试了奖励为 999 的动作，REINFORCE 会很乐意地增加它的概率。现在，想象另一种情境，一个动作产生 1 的奖励，另一个产生 -1 的奖励。如果智能体尝试了奖励为 1 的动作，REINFORCE 也会增加它的概率。但是，第二种情况下的学习信号（差异为 2）比第一种情况下的学习信号（差异为 1）要清晰得多，尽管绝对奖励要小得多！

该[算法](@article_id:331821)依赖于运气的抽样。单一一 trajetória 可能会给出一个极具误导性的[梯度估计](@article_id:343928)。一个优秀的策略可能仅仅因为运气不好，采样到一条低奖励的轨迹，[算法](@article_id:331821)就会错误地“惩罚”它。反之，一个糟糕的策略可能运气好而被奖励。这意味着[梯度估计](@article_id:343928)值可能在不同批次的 episode 之间剧烈波动，导致学习缓慢且不稳定。

这不仅仅是一个直观上的问题，它在数学上是可以证明的。[统计估计理论](@article_id:352774)为任何无偏估计器的性能设定了一个硬性下限，即**[克拉默-拉奥下界](@article_id:314824)（Cramér-Rao Lower Bound, CRLB）**。事实证明，REINFORCE 估计器的方差通常远大于这个理论最小值 [@problem_id:3169909]。我们为了[算法](@article_id:331821)优雅的通用性，付出了沉重的方差税。

### 用基线驯服猛兽

那么，我们该如何解决这个问题呢？关键的洞见在于，真正重要的不是奖励的[绝对值](@article_id:308102)，而是该奖励比*预期*是更好还是更差。这引出了[策略梯度](@article_id:639838)中最重要的概念之一：**基线**（baseline）。

我们不再用原始回报 $G_t$ 来缩放得分，而是用回报*减去*一个基线 $b(s_t)$ 来缩放：

$$
\nabla_{\theta} J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) (G_t - b(s_t)) \right]
$$

只要基线 $b(s_t)$ 不依赖于动作 $a_t$，这个新的估计器就仍然是完全无偏的。我们根本没有改变梯度的平均值！为什么呢？因为减去部分的[期望](@article_id:311378)是 $\mathbb{E}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \cdot b(s_t)]$，而[得分函数](@article_id:323040)本身的[期望](@article_id:311378)是零。你不能通过加上或减去一个平均值为零的东西来改变平均值。

但是，虽然[期望值](@article_id:313620)不变，方差却可以被显著降低。目标是选择一个基线 $b(s_t)$，使其成为从状态 $s_t$ 出发的[期望](@article_id:311378)回报的良好猜测。如果我们这样做，$(G_t - b(s_t))$ 这一项（被称为**优势**，advantage）在我们做得比平均水平好时为正，比平均水平差时为负。学习信号变得以零为中心，从而更加稳定。

最佳的常数基线是什么？可以证明，最小化方差的常数基线恰好是奖励本身的[期望值](@article_id:313620) [@problem_id:3285872]。这在直觉上完全说得通：与平均值进行比较是判断单个结果最自然的方式。在更复杂的环境中，一个好的基线是*价值函数* $V(s)$，即从状态 $s$ 开始的[期望](@article_id:311378)回报。

### 从基线到评论家

我们可以将这个想法更进一步。为什么要满足于像平均奖励这样简单的基线呢？我们可以使用一个复杂的[函数逼近](@article_id:301770)器，比如另一个[神经网络](@article_id:305336)，来*学习*这个基线。这就引出了一类名为**[演员-评论家](@article_id:638510)（Actor-Critic）**的[算法](@article_id:331821)。

- **演员（Actor）**是我们的策略 $\pi_{\theta}(a|s)$，它决定要做什么。
- **评论家（Critic）**是一个学习到的价值函数 $Q_w(s,a)$ 或 $V_w(s)$，它估计这些动作或状态有多好。

评论家的工作是为演员提供一个更好、更具情境性的基线。演员执行一个动作。评论家评估该动作，告诉演员这个动作是比预期好还是差。然后，演员利用这个反馈来更新其策略。

一种特别优雅的形式是使用所谓的**兼容[函数逼近](@article_id:301770)器（compatible function approximator）**作为[控制变量](@article_id:297690)（control variate），这是一种来自统计学的强大[方差缩减技术](@article_id:301874) [@problem_id:3163434]。在这里，评论家被专门设计成给定得分的情况下，回报的最佳[线性预测](@article_id:359973)器。令人惊讶的是，可以证明，使用演员学习的同一批数据来拟合这个评论家，*不会给演员的[梯度估计](@article_id:343928)带来任何偏差*。这是一个了不起的结果，它让我们能够构建强大且样本高效的[算法](@article_id:331821)，其中两个组件——演员和评论家——共同学习，互相帮助。

### 随机梯度的两个世界：更广阔的视角

必须理解，REINFORCE 并非唯一的选择。它属于“[得分函数](@article_id:323040)”估计器家族。还有另一个完全不同的“路径[导数](@article_id:318324)”或“[重参数化](@article_id:355381)”估计器家族。关键区别在于随机性的性质 [@problem_id:2154631]。

- **[得分函数](@article_id:323040)（REINFORCE）：** 这是一种通用工具。即使随机选择是一个“黑箱”，比如从集合 {A, B, C} 中选择一个离散类别，它也同样有效。我们只需要能够评估我们所做选择的概率。这就是为什么它对于离散动作问题 [@problem_id:3107989] 或当 underlying 机制不可微时不可或缺。然而，这种通用性是以高方差为代价的。

- **路径[导数](@article_id:318324)（[重参数化技巧](@article_id:641279)）：** 这是一种 specialized 但效率高得多的工具。它仅适用于我们可以将随机动作表示为策略参数和某个独立噪声的*可微、确定性函数*的情况。例如，如果我们处理连续动作的策略是一个高斯分布，我们可以将动作 $a$写成 $a = \mu_{\theta}(s) + \sigma_{\theta}(s) \cdot \epsilon$，其中 $\epsilon$ 是从[标准正态分布](@article_id:323676) $\mathcal{N}(0,1)$ 中抽取的样本 [@problem_id:3191611]。现在从 $\theta$ 到奖励的路径是完全可微的了！我们可以直接通过均值 $\mu_{\theta}$ 和[标准差](@article_id:314030) $\sigma_{\theta}$ 进行反向传播，从而得到方差低得多的梯度。

许多现代模型都是混合体，在可能的情况下使用[重参数化技巧](@article_id:641279)（对于连续变量），而对于必须做出的任何离散随机选择，则退回到 REINFORCE [@problem_id:3107989]。其他方法，如 [Gumbel-Softmax](@article_id:642118) 技巧，为[离散变量](@article_id:327335)提供了不同的权衡：它们引入少量偏差以换取方差的大幅降低，这有时可以加速学习 [@problem_id:3121685]。

最终，当我们计算[策略梯度](@article_id:639838)时，我们在做什么？我们正在问一个因果问题：“如果我对我的策略参数 $\theta$ 做一个微小的改变，它对我的未来[期望](@article_id:311378)奖励会产生什么因果效应？”[策略梯度](@article_id:639838)本质上是一个因果效应估计器。这个视角也阐明了为什么这些方法可能会失败。如果存在一个未被观察到的**[混淆变量](@article_id:351736)**（confounder）——一个既影响我们采取的行动又影响我们获得的奖励的隐藏变量——那么我们测量的相关性就不再是真实的因果效应。在这种情况下，简单的 REINFORCE 估计器会变得有偏，需要借助[因果推断](@article_id:306490)中更先进的技术来厘清我们策略的真实影响 [@problem_id:3158026]。这段旅程，从一个简单的对数技巧到一个用于因果推理的工具，揭示了统计学、优化以及对因果关系的科学探索之间深刻而美丽的统一性。

