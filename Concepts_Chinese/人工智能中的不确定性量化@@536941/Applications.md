## 应用与跨学科联系

在遍历了不确定性的原理和机制之后，我们现在到达了探索中最激动人心的部分：在现实世界中看到这些思想的实际应用。你可能会倾向于将不确定性视为一种麻烦，一种遮蔽我们希望从强大[算法](@article_id:331821)中获得清晰、明确答案的迷雾。但这并非正确的图景。事实上，理解和[量化不确定性](@article_id:335761)正是将[算法](@article_id:331821)从一个纯粹的计算器提升为一个在发现和决策中值得信赖的伙伴的关键。这是一个新的人工智能时代的黎明——它不仅提供答案，还理解自身知识的局限。这些应用不仅仅是小众的技术修复；它们横跨了人类努力的整个光谱，从我们看待世界的方式到我们构建世界的方式，从揭开生命的秘密到应对我们时代最深刻的伦理问题。

### 用诚实的眼睛看世界：计算机视觉中的不确定性

让我们从我们醒着时每时每刻都在做的事情开始：看。当你看着一张照片时，你的大脑毫不费力地将一只猫与背景区分开来。但如果猫的边缘模糊，其皮毛融入了一块毛茸茸的地毯呢？如果光线昏暗，那个形状可能是一只猫，也可能是一只毛茸茸的小狗呢？你的大脑不会就此放弃；它会变得*不那么确定*。一个真正智能的[计算机视觉](@article_id:298749)系统必须做到同样的事情。

考虑[语义分割](@article_id:642249)任务，即要求人工智[能标](@article_id:375070)记图像中的每一个像素——这个像素是“道路”，那个是“天空”，这个是“猫”。一个标准的人工智能可能会生成一张色彩斑斓的地图，但它以一种绝对权威的姿态呈现这张地图。然而，一个具备[不确定性量化](@article_id:299045)能力的人工智能，可以为我们提供两种诚实。首先，它可以识别**[偶然不确定性](@article_id:314423)**，即数据本身固有的模糊性。这是模型在告诉我们：“这只猫皮毛的边缘确实是模糊的；无论我变得多聪明，都无法在这里画出一条完美的线。”其次，也许更重要的是，它可以识别**认知不确定性**，这反映了模型自身知识的缺乏。这是模型在坦白：“我没有见过足够多这个特定品种的猫从这个奇怪角度的例子，所以我对我的猜测不是很有信心。”

像[蒙特卡洛丢弃](@article_id:640595)这样的技术使我们能够感知这种[认知不确定性](@article_id:310285)。本质上，我们多次“问”模型同一个问题，每一次都用略微不同的内部配置。在模型答案变化剧烈的地方，其[认知不确定性](@article_id:310285)就高。通过这种方式分解总不确定性，我们可以创建不仅标记世界，还突出显示模型困惑之处的地图。这些不确定性的“热点”通常与模型出错的区域高度相关，为我们改进模型或让人类专家审查其工作提供了宝贵的指南 [@problem_id:3136291]。人工智能不再只是一个画图者；它是一位谨慎的制图师，不仅绘制领土，还绘制出自己无知的“恶龙”之地。

### 懂得说“我不知道”的智慧

一旦模型知道自己不确定，它应该做什么？这个问题将我们从感知领域带到行动和高风险决策的世界。想象一个诊断医疗扫描的人工智能系统，或一辆识别行人的[自动驾驶](@article_id:334498)汽车。在这些场景中，错误的答案可能是灾难性的。一个自信但错误的“一切正常”远比一个诚实的“我不确定”危险得多。

这就是[不确定性量化](@article_id:299045)成为安全机制的地方。我们可以赋予我们的人工智能拒绝回答的能力，而不是强迫它猜测。关键思想是建立一个非固定的、而是*自适应*的决策阈值。当模型对某个情况的总体不确定性较低时，它可以相当自信地做出决策。但当其不确定性很高时——也许是因为输入有噪声、不寻常或完全新颖——系统做出决策的内部标准应该相应提高。它必须要求更高程度的置信度才能做出一个答案。如果达不到那个高标准，系统会明智地选择说“我不知道”，并将问题上报给人类操作员 [@problem_id:3102053]。

这个简单原则的意义是深远的。它是一个脆弱、过度自信的自动机和一个可靠、值得信赖的助手之间的区别。这个原则也尖锐地揭示了紧迫的伦理困境。考虑一个用于[体外受精](@article_id:323833)（IVF）诊所的人工智能，它为胚胎分配一个“成功分数”。如果这个分数以一个单一、明确的数字呈现，它就危险地隐藏了潜在的不确定性。它给准父母施加压力，侵犯了他们根据全面信息做出深刻个人选择的自主权。它可能导致他们仅仅因为一个专有[算法](@article_id:331821)（及其所有隐藏的不确定性和偏见）给一个胚胎分配了稍低的分数，就丢弃一个本有合理成功机会的胚胎，从而造成伤害。在这里，理解不确定性不是一个技术细节，而是一种伦理要求 [@problem_id:1685386]。它要求我们构建能够传达自身易错性的系统，从而促进人与机器之间更诚实、更人道的协作。

### 科学发现的新指南针：科学与工程中的[不确定性量化](@article_id:299045)

懂得自己不知道什么的力量，远远超出了避免错误。事实上，它可以成为探索和科学发现的指导原则。科学是一场进入未知的旅程，而[认知不确定性](@article_id:310285)就是一张标示出“未知”所在之处的地图。

在[材料科学](@article_id:312640)中，研究人员正在使用人工智能从数百万种假设的化学成分中筛选，以寻找具有理想性质的新材料，如更好的电池或更强的合金。可以训练一个人工智能模型来预测材料的性质，例如其形成能。但它最有价值的输出不仅仅是预测，而是附加于其上的不确定性。当模型报告某个特定成分具有高*认知*不确定性时，它实际上是在说：“化学空间的这个区域远离我训练数据中的任何东西；我在这里的预测是一个大胆的猜测。”对于科学家来说，这不是模型的失败。这是一个指向新颖和未探索前沿的路标。这是对下一次实验的建议，一种引导昂贵且耗时的物理合成与测试过程走向最具[信息价值](@article_id:364848)可能性的方法 [@problem_id:2479717]。

同样的原则对工程安全至关重要。想象一个数据驱动的模型，用于预测桥梁构件或飞机机翼内部的应力。这样的模型是基于来自模拟或实验的数据进行训练的。所有训练输入的集合在可能应变的高维空间中形成一个“舒适区”。一个有原则地定义这个区域的方法是将其定义为训练数据的凸包。如果一个新的应变状态落在这个[凸包](@article_id:326572)内，模型是在*[内插](@article_id:339740)*——在其理解的区域内操作。如果新状态在外部，它是在*[外推](@article_id:354951)*——冒险进入未知领域。一个负责任的工程系统必须能够区分这两种状态。在外推时，模型的预测不仅会变得不那么准确（高认知不确定性），而且它们还可能变得物理上无意义，预测出违反基本力学定律的行为。通过将[不确定性估计](@article_id:370131)与物理约束（如预测的材料切线的[正定性](@article_id:357428)）的检查相结合，我们可以构建出当它们超出其能力范围时会向我们发出警告的系统，从而防止灾难性故障 [@problem-id:2656058]。

更深入地看，[不确定性量化](@article_id:299045)不仅将人工智能与工程实践联系起来，还与测量的基本物理学联系起来。在[纳米力学](@article_id:364574)等领域，科学家使用像[原子力显微镜](@article_id:342830)（AFM）这样的工具在原子尺度上“感觉”表面，测量中的噪声不仅仅是一种麻烦；它是一个包含关于物理世界信息的丰富信号。测得的总噪声是不同效应的混合体：悬臂因室温而产生的热[抖动](@article_id:326537)（热噪声）、光本身量子离散性（[散粒噪声](@article_id:300471)），以及电子设备中神秘、无处不在的低频漂移（$1/f$ 噪声）。一个复杂的贝叶斯模型不仅仅是试图平均掉这些噪声。相反，它构建了一个明确考虑每种物理来源的[似然函数](@article_id:302368)，每种来源都有其自身的尺度行为和特征。通过这样做，模型同时学习了针尖-样品相互作用的性质和测量设备的性质，将噪声从敌人变成了提供信息的盟友 [@problem_id:2777650]。

### 解开生命密码

也许没有任何地方的未知领域比生物学更广阔、更复杂。在这里，[不确定性量化](@article_id:299045)正在成为驾驭这种复杂性的不可或缺的工具，从设计新药到理解生命的基本构建模块。

考虑为像[流感](@article_id:369446)这样快速变异的病毒制造[疫苗](@article_id:306070)的挑战。我们不能仅仅针对今天看到的毒株设计[疫苗](@article_id:306070)；我们必须预测未来的毒株。利用像[变分自编码器](@article_id:356911)（VAEs）这样的[生成模型](@article_id:356498)，科学家可以学习病毒抗原空间的低维“地图”。每个已知的病毒株都是这张地图上的一个点。一个 VAE 可以用来在这张地图上生成一个新的点——一个新[疫苗](@article_id:306070)抗原的候选者。但哪个点是*最佳*点呢？答案在于不确定性。通过将一个提议的[疫苗](@article_id:306070)建模为这个抗原空间中的一个概率“云”而非单个点，我们可以计算它在当前流行和潜在未来毒株群体中的预期覆盖范围。目标是设计一种[疫苗](@article_id:306070)，其不确定性云与尽可能多的目标毒株重叠，从而提供广泛而鲁棒的保护 [@problem_id:2439818]。

当我们考虑到一个活细胞是一个极其复杂的系统时，复杂性成倍增加，它由其基因组（DNA蓝图）、[转录组](@article_id:337720)（活跃的RNA信息）、[蛋白质组](@article_id:310724)（起作用的蛋白质机器）等描述。这些不同的“组学”层就像是窥视同一个繁忙工厂的不同窗口。我们如何将这些不同的视图融合成一个连贯的整体，特别是当对于一个给定的细胞，一个或多个窗口可能缺失时？在这里，概率模型再次发挥作用。使用“专家乘积”框架，我们可以构建一个VAE，从多种模态中学习一个共享的潜在表示。每种模态的[编码器](@article_id:352366)都像一个“专家”，就细胞的潜在状态提供其意见。模型学会权衡这些意见，将它们组合成一个单一、更鲁棒的[后验分布](@article_id:306029)。关键是，如果一个模态缺失，它的专家就保持沉默，其他专家则填补空白。这使得进行整体分析成为可能，并且对单细胞生物学中普遍存在的混乱、不完整的数据具有鲁棒性 [@problem_id:2439755]。

### 一个知识与责任的框架

当我们放大视野时，我们看到[不确定性量化](@article_id:299045)的原则不仅为单个模型提供了框架，也为在最大尺度上综合知识和管理风险提供了框架。在计算金融领域，多个复杂模型竞相[预测市场](@article_id:298654)动向，我们面临着预测组合的问题。我们可以使用统计学中称为**copula**的工具来学习模型预测之间的*[依赖结构](@article_id:325125)*。一个 copula 可以告诉我们：这些模型是否倾向于同时出错？它们是否表现出“尾部依赖”，即在极端市场崩盘期间它们会一起失灵？知道这一点远比知道任何单个模型的准确性更重要。它使我们能够构建一个能够意识到我们模型集合中系统性风险的融合预测，从而实现更鲁棒的[风险管理](@article_id:301723) [@problem_id:2396039]。

这把我们带到了最后一个，也许是最重要的应用：人工智能本身的治理。随着人工智能模型变得足够强大，能够生成新颖的[生物序列](@article_id:353418)或其他潜在危险的输出，管理其风险成为一个至关重要的社会问题。一种成熟的[人工智能安全](@article_id:640281)方法依赖于我们一直在讨论的概念。我们可以区分**[模型风险](@article_id:297355)**（模型因内部缺陷而犯错）、**能力控制**（限制模型被允许做什么，比如将其置于沙箱中与互联网隔离）和**对齐**（塑造模型的内在目标以匹配人类价值观）。这些都是不确定性管理的形式。审计模型的缺陷是为了减少关于其行为的[认知不确定性](@article_id:310285)。能力控制是硬性的护栏，无论模型的不确定意图如何都能起作用。而对齐则是减少模型目标与我们自身目标偏离的不确定性的巨大挑战。围绕这些原则建立一个治理框架是UQ的终极应用：确保我们的创造物在变得更智能的同时，也在智慧和责任感上共同成长 [@problem_id:2766853]。

从图像的像素到社会的结构，不确定性的线索贯穿一切。拥抱它，量化它，并据此行动，就是迈向一个人工智能不仅更强大，而且在根本上更有洞察力、更安全、更符合人类福祉的未来。