## 引言
医疗算法在革新医疗保健领域展现出巨大潜力，但它们也隐藏着危险。这些强大的工具在一个不平等的世界中产生的数据上进行训练，可能会无意中学习、固化甚至放大历史性和系统性的偏见，导致极其不公平的健康结果。这带来了一个严峻的挑战：我们如何确保我们为治愈而构建的人工智能不会同时造成伤害？理解这种偏见的根本原因和机制，是构建公正可信的医疗人工智能的第一步。

本文对这一社会技术问题进行了全面探讨。在第一章“原理与机制”中，我们将深入研究偏见的根本来源，追溯其从社会到硅基的路径，并解构标签偏见、错误校准以及“色盲”模型失败等概念。这一旅程将在“应用与跨学科联系”中继续，我们将考察有偏见的算法在诊断学、遗传学和法律领域的现实世界后果，并探讨研究者用于发现和减轻这些伤害的工具箱。

## 原理与机制

想象你是一位研究粒子运动的物理学家。你有精妙的理论来描述它的路径。但如果你用来测量其位置的尺子是弯曲的呢？如果你所研究的粒子运动的空间本身就不是平坦的，而是被无形的力量所扭曲呢？那么你的预测，无论你的方程多么优雅，都将是系统性错误的。医疗算法也是如此。我们试图理解的“偏见”不仅仅是算法代码中的一个缺陷；它是一系列扭曲的测量与医学实践所在的不平等、弯曲的社会空间之间复杂的相互作用。为了真正把握这一挑战，我们必须成为这个社会技术世界的物理学家，追溯偏见从社会到硅基的起源和机制。

### “偏见”的双重含义

在科学中，“偏见”（bias）一词具有精确的技术含义。对于统计学家来说，一个估计量的**[统计偏差](@entry_id:275818)**（statistical bias）指的是该估计量的[期望值](@entry_id:150961)与被估计参数真值之间的差异 [@problem_id:4849723]。可以把它想象成步枪制造过程的一个属性：如果某个工厂生产的步枪平均来说总是稍微偏左，那么这个制造过程就存在偏差。这是一个关于某个程序长期平均表现的概念。

然而，当我们在医疗保健领域谈论**[算法偏见](@entry_id:637996)**（algorithmic bias）时，我们通常关注的是另一回事，一个更直接的问题：一个*单一的、已部署的*模型对不同人群的影响。这并非关于学习算法在所有可能的训练数据集上的平均属性；而是关于我们*最终确定*的模型所犯下的系统性、可重复的错误，以及这些错误是否不成比例地伤害或惠及可识别的患者群体 [@problem-id:4849723] [@problem-id:5225896]。

一个算法在技术意义上可能表现出很小的[统计偏差](@entry_id:275818)——其参数可能是对训练数据中模式的极佳估计——但仍然产生极其不公平的结果。当数据本身反映了一个不平等的世界时，这种情况就会发生。步枪可能制造得非常完美，但如果它总是瞄准错误的目标，结果就是系统性的伤害。伦理意义上的[算法偏见](@entry_id:637996)，关乎的就是这种有害的影响。它是一种结果上的系统性差异，通过捕捉临床伤害的指标来衡量，例如模型对某个群体的关键诊断漏诊率与另一群体相比有多高。

### 偏见之河：追溯从社会到硅基的来源

[算法偏见](@entry_id:637996)很少是由某一行恶意[代码注入](@entry_id:747437)的。相反，它更像是河流中不断累积的沉积物，从复杂的社会景观中流出，穿过数据收集的渠道，最终沉淀在我们模型的“水库”中。要理解它，我们必须追溯这条流。我们可以识别出几个偏见可能渗入的关键阶段 [@problem_id:4408271]。

1.  **数据生成（世界本身）：** 先已存在的社会不平等意味着健康和疾病的分布并非均等。
2.  **抽样（我们看到谁）：** 选择哪些患者被纳入数据集的过程通常不是中立的。
3.  **测量与标签（我们记录什么）：** 我们使用的工具和选择的定义可能会系统性地扭曲我们对现实的看法。
4.  **建模（算法学到什么）：** 算法的目标函数和学习过程可能创造或放大差异。

让我们依次探讨这些来源中的每一个。

### 我们所测量的世界：社会性偏见与代表性偏见

我们用来“喂养”算法的数据并非现实的完美镜子；它是一面哈哈镜，被历史、社会结构以及观察行为本身所扭曲。

#### 社会性偏见与数据生成偏见

最根本的偏见来源是世界本身。如果由于环境因素或系统性忽视，某种疾病在某个社区更为普遍，那么反映这一现实的数据集将显示社区归属与疾病之间的相关性。在此数据上训练的算法将学习到这种相关性。

一个强有力且危险的例子是在医学中使用**社会性种族**（social race）。将种族视为生物学或遗传学范畴是一个常见的错误。[群体遗传学](@entry_id:146344)告诉我们一个不同的故事。人类绝大多数的遗传变异存在于群体*内部*，而非群体之间。[固定指数](@entry_id:170530)（Fixation Index, $F_{ST}$）是衡量[群体分化](@entry_id:188346)程度的指标，在人类大陆群体间的数值相对较低（约 $0.05$ 到 $0.15$），这证实了不存在与社会性种族类别相符的、清晰的遗传[分界线](@entry_id:175112)。遗传变异是渐变的（clinal），随地理位置逐渐变化，反映了迁徙和混合 [@problem_id:5028504]。**遗传祖源**（Genetic ancestry）是对这一复杂遗产的连续、量化的度量。

相比之下，**社会性种族**（Social race）是一个社会政治构念。它是社会赋予的类别，塑造了个体的生活经历，包括他们遭受歧视、贫困和环境毒素的暴露程度。将两者混为一谈——使用粗糙的社会类别作为精细生物学的代理——是一个深远的科学错误，可能导致错误计算患者的风险或某种遗传变异的频率。从伦理上讲，这更加危险：它可能导致将由种族主义引起的健康差异错误地归因于天生的生物学差异，这种做法“抹去”了健康的真正社会决定因素，并固化了有害的意识形态 [@problem_id:5028504]。

#### 代表性偏见与抽样偏见

即使我们能完美地记录现实，我们记录的又是*谁*的现实呢？医学数据集通常是方便样本（samples of convenience），而非精心构建的、能代表总人口的调查。

考虑一个在一家专业三级医院的数据上训练的模型 [@problem_id:5226004]。谁会去这样的医院？病情严重到需要转诊的患者、有保险负担得起的患者，以及住得足够近可以到达的患者。这个选择过程就像一个过滤器。这导致了**选择偏见**（selection bias）：训练数据分布 $P_{train}(X,Y)$ 与目标人群分布 $P_{target}(X,Y)$ 不相同 [@problem_id:5226004]。

这会造成微妙但强大的扭曲。例如，想象一下高血压（$X$）和一种罕见的[遗传标记](@entry_id:202466)（$Y$）都可能触发转诊（$S=1$）。在普通人群中，$X$ 和 $Y$ 可能是不相关的。但*在被转诊的患者群体中*，它们变得相关了。如果你看到一个被转诊的患者没有高血压，那么他有那个[遗传标记](@entry_id:202466)的可能性就变大了，因为*总得有某个原因*让他被转诊。这种现象，即以一个共同效应为条件，导致其原因之间产生[虚假相关](@entry_id:755254)性，被称为**对撞偏见**（collider bias）或 Berkson 悖论。这是许多真实世界医疗数据集中隐藏的陷阱。

#### 测量偏见与标签偏见

我们对数据集中患者的看法，因我们使用的工具和定义而进一步扭曲。
- **测量偏见（Measurement Bias）：** 我们的仪器可能并非普遍准确。一个典型且悲剧性的例子是[脉搏血氧仪](@entry_id:202030)，它可能系统性地高估肤色较深患者的血氧水平。用这些数据训练的模型可能会学到，对于黑人患者来说，较低的血氧读数没有白人患者那么危险，这并非因为任何生物学差异，而是因为仪器在说谎 [@problem_id:4408271]。
- **标签偏见（Label Bias）：** 通常，我们想要预测的“基准真相”（ground truth）是无法直接观察的，所以我们使用一个代理（proxy）。为了预测败血症，我们可能使用“广谱抗生素的施用”作为标签 $Y$。但这不是对疾病的直接测量；它是对*医生决策*的测量。如果医生由于自身的[内隐偏见](@entry_id:637999)，对某个群体的患者有更高的治疗门槛，那么标签本身就是有偏见的。模型将学习到医生的偏见行为，而不是潜在的病理 [@problem_-id:4408271]。
- **[缺失数据](@entry_id:271026)（Missing Data）：** 医疗数据是出了名的不完整。数据可能是**[完全随机缺失](@entry_id:170286)（MCAR）**，就像书页被随机撕掉一样。它也可能是**[随机缺失](@entry_id:168632)（MAR）**，即数据缺失的事实可以完全由其他观察到的变量来解释——例如，如果男性患者没有进行妊娠测试。但最[隐蔽](@entry_id:196364)的类型是**[非随机缺失](@entry_id:163489)（MNAR）**。当缺失的原因取决于未观察到的值本身时，就会发生这种情况 [@problem_id:4849724]。一个关键的实验室值，如血清乳酸，并不是随机测定的；它正是在临床医生怀疑患者病情非常严重时才会测定。如果我们只分析观察到的乳酸值，我们看到的是一个因病情更重而被预选的人群，这会使我们对症状和乳酸水平之间关系的看法产生严重偏斜。

### 算法的盲点：建模与评估偏见

最后，当我们拿着有偏见的数据来训练算法时，新的偏见可能会出现。大多数标准的[机器学习算法](@entry_id:751585)都旨在最小化单一的、总体的错误指标，比如整体准确率。为了追求这个目标，算法可能会学到，通过在多数群体上表现得非常准确，即使在少数群体上表现不佳，也能实现很高的整体性能。来自少数群体的少量错误根本不会对整体准确率数字产生太大影响。这是一种**[算法偏见](@entry_id:637996)**或**建模偏见**（modeling bias），是优化过程本身内置的一种“多数暴政” [@problem_id:4408271]。

一个常见、善意但危险天真的修复建议是，简单地从数据集中移除敏感属性（例如，种族）。这种方法通常被称为“通过无知实现公平”（fairness through unawareness）。其逻辑是，如果算法从未“看到”种族，它就不可能具有种族主义。这是对偏见如何运作的深刻误解。

问题在于其他变量充当了代理（proxies）。由于系统性不平等，邮政编码、收入水平，甚至某些临床指标都可能与种族高度相关。通过移除种族，我们并没有消除其影响；我们只是让模型对偏见的来源视而不见。歧视的因果路径，从种族经由代理变量流向最终预测，依然完好无损 [@problem_id:4513548]。一个**反事实公平**（counterfactually fair）的模型，即使我们能假设性地改变一个人的种族而保持所有其他内在属性不变，也应产生相同的预测。一个“色盲”模型无法通过这个测试，因为改变种族会改变代理变量，进而改变预测。通过让模型“失明”，我们不仅未能消除偏见，还失去了检测和纠正它的能力。真正的公平往往需要*意识*，而非无知。

### 从有缺陷的预测到现实世界的伤害：不平等的机制

那么，我们的模型是有偏见的。这个抽象的属性是如何造成具体伤害的呢？其机制通常很微妙，但后果却是毁灭性的。

#### 让偏见可见：[公平性指标](@entry_id:634499)

要理解伤害，我们必须首先学会看到偏见。我们可以使用不同的统计“镜头”，即所谓的**公平性标准**（fairness criteria），来审计模型在不同群体间的表现。考虑一个对真实世界模型的审计，该模型旨在标记患者以便进行后续电话随访，以防止再入院 [@problem_id:4367362]。

让我们检查两组数据：
- **A组：** 在200名真正需要电话随访的患者中，模型正确标记了140人。**[真阳性率](@entry_id:637442)（TPR）**或灵敏度为 $\frac{140}{200} = 0.70$。
- **B组：** 在100名真正需要电话随访的患者中，模型仅正确标记了50人。TPR为 $\frac{50}{100} = 0.50$。

这种差异违反了**[机会均等](@entry_id:637428)**（Equal Opportunity），这是一个公平性标准，它要求模型对那些实际上属于阳性类别的个体（无论其群体归属）表现应同样好。在这里，一个需要帮助的B组患者被算法漏掉的可能性（$50\%$的漏掉几率）远高于A组患者（$30\%$的漏掉几率）。这是对一种有益干预措施分配的直接、可量化的不平等。我们也可以看到，患者被标记的总体比率不同：A组为$26\%$，而B组为$14\%$，这违反了另一个称为**人口统计均等**（Demographic Parity）的标准。

#### 错误校准的灾难

也许最直接的伤害机制是**错误校准**（miscalibration）。一个风险模型如果其预测的含义与所说的一致，就被认为是良好校准的。如果它预测有20%的风险，那么在所有被赋予该分数的患者中，应该有20%的人实际经历了该事件。

现在，想象一个败血症预测模型对A组是完美校准的，但对B组却严重错误校准 [@problem_id:4849714]。对于B组，该模型“过度自信”：其高风险预测系统性地过高，而低风险预测系统性地过低。假设医院的政策是向任何预测风险大于等于$20\%$的人分配一种稀缺的干预措施。

会发生什么？
- 一位来自B组的患者得分是$19\%$。他们被拒绝了干预。但由于模型对该群体在低值时预测偏低，他们真实的风险可能是$23\%$。他们被不公正地拒绝了治疗。这就是**治疗不足**（undertreatment）。
- 另一位来自B组的患者得分是$21\%$。他们接受了干预。但由于模型在高值时预测偏高，他们真实的风险可能只有$17\%$。他们基于一个虚高的分数获得了稀缺资源，这可能使他们暴露于副作用之中，并剥夺了真正有更大需求的人。这就是**过度治疗**（overtreatment）。

这就是算法伤害的阴险之处。一个看似客观的单一政策——治疗所有得分超过$20\%$的人——当应用于一个错误校准的模型时，系统性地产生了不公正的结果。政策是统一的，但其*影响*是不平等的。揭示这些隐藏的机制，是构建不仅强大而且公正的人工智能的第一步，也是至关重要的一步。

