## 引言
在探索数据中有意义模式的过程中，聚类是一项基础性任务。一种常见的方法，以 k-means [算法](@article_id:331821)为代表，是通过一个抽象的“平均值”或[质心](@article_id:298800)来表示数据群组。然而，这种统计学上的构造可能具有误导性，尤其是在存在离群点或可解释性至关重要的情况下。如果，一个群组的代表必须是该群组的实际成员，情况会怎样呢？这个简单的理念转变弥补了聚类中的一个关键空白，并直接引导我们走向一种更鲁棒、更直观的方法。

本文深入探讨了“围绕[中心点](@article_id:641113)划分”（Partitioning Around Medoids, PAM）[算法](@article_id:331821)，这是一种建立在上述原则之上的强大聚类技术。通过选择实际数据点作为簇中心，即“中心点”（medoids），PAM 提供了更强的鲁棒性和清晰度。我们将探讨这一基本思想如何塑造了[算法](@article_id:331821)的机制，并释放其卓越的灵活性。第一章“原理与机制”将解析基于中心点的[聚类](@article_id:330431)的核心理念、[相异性矩阵](@article_id:641021)的通用能力，以及在应对计算挑战的同时寻找最优中心点的策略。第二章“应用与跨学科联系”将通过探索 PAM 在客户分析、基因组学、网络科学和[异常检测](@article_id:638336)等不同领域的应用，展示其多功能性——仅仅通过改变距离的定义即可实现。

## 原理与机制

要真正欣赏聚类的艺术，我们必须超越单纯的分组行为，提出一个更根本的问题：什么才是一个群体的优秀代表？想象一下，你正试图描述一个班级的学生。一种方法是创建一个“平均”学生——一个具有平均身高、平均体重和平均成绩的统计构造。这正是流行的 $k$-means [算法](@article_id:331821)所采用的路径，它使用几何平均值，即**[质心](@article_id:298800)**（centroid），作为其簇中心。但如果你的数据不那么简单呢？如果一个学生是身高七英尺的篮球神童呢？他极端的身高会扭曲平均值，创造出一个实际上与任何人都不相似的“代表”。

### 人民的代表：中心点与[质心](@article_id:298800)

这正是“围绕中心点划分”（PAM）[算法](@article_id:331821)提供更直观、更鲁棒理念的地方。PAM 并不创建抽象的平均值，而是坚持代表必须是群体中一个真实存在的成员。它旨在寻找一个**中心点**（medoid）——一个在其簇内位置最中心化的实际数据点。可以把它想象成选举班长；班长是学生之一，而不是计算机生成的合成图像。

这种区别不仅仅是理念上的，它具有深远的实际意义。考虑对生物样本的基因表达谱进行[聚类](@article_id:330431)。由于[实验误差](@article_id:303589)，某些样本可能具有极端异常的值，这些离群点会极大地将[质心](@article_id:298800)从群体的真实中心拉开。而[中心点](@article_id:641113)本身是一个实际样本，它自身是离群点的可能性要小得多，因此对离群点的存在更具鲁棒性。此外，中心点具有内在的可解释性。生物学家可以观察这个[中心点](@article_id:641113)样本——一个真实的病人，一个真实的组织——并理解其完整的概况，从而为它所代表的簇提供一个具体而全面的图景。而[质心](@article_id:298800)作为一个人工的平均值，则无法提供这种直接的生物学洞见 [@problem_id:2379227]。

### 通用翻译器：相异性的力量

基于[中心点](@article_id:641113)的方法的优雅之处远不止于此。由于 $k$-means [算法](@article_id:331821)建立在计算算术平均值的基础上，它从根本上与[欧几里得空间](@article_id:298501)——我们熟悉的直线距离世界——绑定在一起。它需要原始坐标数据来完成其工作。

另一方面，PAM 则是抽象化的大师。它根本不需要知道数据点的坐标。它只需要一个**[相异性矩阵](@article_id:641021)**，这仅仅是一个列出每个对象与其他所有对象差异程度的表格。[算法](@article_id:331821)的目标变成最小化每个点到其最近[中心点](@article_id:641113)的相异性之和：
$$
\sum_{i=1}^n d(x_i, m_{c(i)})
$$
其中 $d(\cdot, \cdot)$ 是我们选择的相异性度量，而 $m_{c(i)}$ 是点 $x_i$ 所属簇的中心点。

这种灵活性是一种超能力。我们可以为 PAM 提供一个基于任何合理度量构建的[相异性矩阵](@article_id:641021)。对于基因谱，我们可能会使用[基于相关的距离](@article_id:351383)，这种距离更关心表达模式的形状而非其绝对水平 [@problem_id:2379227]。对于比较二进制字符串（0 和 1 的序列），我们可以使用**汉明距离**（Hamming distance），它只计算符号不同的位置数量。有趣的是，对于二进制向量，汉明距离、[曼哈顿距离](@article_id:340687)和[欧几里得距离](@article_id:304420)的平方结果完全相同！然而，只有 PAM 能够在其核心机制保持不变的情况下互换使用它们，因为它的更新步骤不依赖于在某个抽象空间中计算“平均值” [@problem_id:3109544]。PAM 能够理解任何相异性度量的语言，这使其成为一个真正通用的划分工具。

### 改进游戏：交换以获得更好的世界

那么，PAM 是如何找到这些理想的中心点呢？从 $n$ 个点中选择 $k$ 个[中心点](@article_id:641113)的方式有 $\binom{n}{k}$ 种，这个数字增长得非常快。对于除极小数据集之外的所有情况，暴力检查都是不可行的 [@problem_id:3135230]。因此，PAM 采用了一种巧妙的迭代改进游戏。

该[算法](@article_id:331821)的经典形式分两个阶段工作：一个用于做出初始猜测的 BUILD 阶段和一个用于优化的 SWAP 阶段。SWAP 阶段是该机制的核心。它是一种爬山法，或者更准确地说，是下谷法。我们从一组中心点开始，计算总“成本”——每个点到其最近[中心点](@article_id:641113)的距离之和。然后，我们系统地考虑每一种可能的交换：如果我们用一个非[中心点](@article_id:641113)替换掉一个当前的[中心点](@article_id:641113)会怎样？

对于 $k(n-k)$ 种可能的交换中的每一种，我们计算总成本的潜在变化。在评估了所有交换之后，我们执行[能带](@article_id:306995)来最大成本降低的那一次交换。我们迭代地重复这个过程，总是做出局部最优的移动。当没有任何单次交换可以进一步降低成本时，游戏结束 [@problem_id:3135245]。[算法](@article_id:331821)已经收敛到一个解。虽然这在计算上是密集的，但巧妙的实现可以利用数学技巧，如[三角不等式](@article_id:304181)，来证明某些交换不可[能带](@article_id:306995)来改进，从而可以修剪搜索空间，跳过许多耗时的距离计算 [@problem_id:3135247]。

### 山谷陷阱：局部最小值与智能启动

这种迭代交换的游戏简单而强大，但它有一个致命弱点：它是一种**[局部搜索](@article_id:640744)**方法。想象一个登山者在浓雾中试图找到广阔山脉中的最低点。他们可能会下降到一个小山谷，发现从那里出发的每一步都是上坡路，于是宣称自己找到了底部。但真正的最低点——全局最小值——可能在一个更深的山谷中，位于山脊的另一侧。

PAM 可能会陷入同样的陷阱。它可能收敛到一个**局部最小值**，即一组优于其所有直接邻居（通过单次交换可达到的那些）的[中心点](@article_id:641113)，但并非[全局最优解](@article_id:354754)。最终结果严重依赖于对[中心点](@article_id:641113)的初始猜测。一个天真的开始，比如简单地选择列表中的前 $k$ 个点，可能会直接将 PAM 引入一个无法逃脱的差的局部最小值 [@problem_id:3135253]。

为了避免这些陷阱，我们需要一个更智能的开始。一种流行的方法，受 $k$-means++ [算法](@article_id:331821)的启发，是选择彼此相距较远的初始中心点。这种“最远优先”策略确保初始猜测在数据中分布均匀，使 PAM 有更好的机会在成本地貌中找到一个更深、更有意义的山谷 [@problem_id:3135253]。其他高级优化方法，如**[模拟退火](@article_id:305364)**（Simulated Annealing），也可以使用。它们引入了随机元素，偶尔允许“上坡”移动，以跳出局部山谷，更广泛地探索地貌 [@problem_id:3193485]。

### 驯服巨兽：使用 CLARA 聚类大型数据集

SWAP 步骤的二次复杂度使得经典的 PAM [算法](@article_id:331821)在处理大型数据集时慢得令人望而却步。试图在数百万个点中找到最佳[中心点](@article_id:641113)，就像试图通过检查每一次交换来为数百万人在一个巨大的宴会厅中安排最佳座位一样。这根本不可行。

解决方案，正如在统计学中常见的那样，是拥抱抽样的力量。**CLARA (Clustering Large Applications)** [算法](@article_id:331821)正是这样做的。CLARA 不是处理整个数据集，而是抽取一个小的随机点样本，并对该样本运行完整的 PAM [算法](@article_id:331821)。由于样本很小，PAM 能很快完成并返回该样本的最佳中心点。然后，CLARA 评估这组[中心点](@article_id:641113)对于*整个*数据集的效果有多好。它在几个不同的随机样本上重复这个过程，并最终报告在所有运行中找到的最佳中心点集。

这种方法效果出奇地好，并且有优美的数学理论可以解释其原因。使用基本概率论可以证明，即使是中等大小的样本，也有非常高的概率包含至少一个“真实”的最优[中心点](@article_id:641113)。通过抽样，我们用找到一个非常好解的高概率换取了在完整数据集上找到最佳局部最小值的保证，而时间仅为原来的零头 [@problem_id:3135252]。

### 秩序的绿洲：一维的优雅

在经历了启发式方法、局部最小值和抽样的复杂性之后，发现 k-medoids 问题在其所有 NP-hard 的荣耀中突然变得完全可解的情况令人耳目一新。这种情况发生于当我们的数据仅位于一维——即一条直线上的点时。

当数据在一条直线上排序时，最优[聚类](@article_id:330431)具有一个优美的特性：其簇必须形成连续的段。你永远不会遇到最优解将点 A 和 C 分配给一个簇，而将位于它们之间的点 B 分配给另一个簇的情况。这个“邻接原则”改变了一切。问题不再是选择任意 $k$ 个点，而是找到最佳的 $k$ 个位置来“切割”这条线以形成分段。

这个结构化问题是应用一种称为**[动态规划](@article_id:301549)**的优雅技术的完美候选。我们可以逐步构建最优解，通过使用我们预先计算出的如何最好地[聚类](@article_id:330431)更少数量的点的知识，来计算将前 $j$ 个点最优地聚类成 $k$ 个段的成本。这种方法有条不紊地以高效的方式构建出精确的、全局最优的解决方案，完全绕过了局部最小值的陷阱 [@problem_id:3135297]。这个特例是一个绝佳的提醒：在复杂、困难的问题中，常常隐藏着等待被发现的美丽、有序的简单绿洲。

