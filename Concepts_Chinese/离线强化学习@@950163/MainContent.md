## 引言
离线[强化学习](@entry_id:141144)（RL）提供了一个强大的范式：我们如何仅使用一个固定的、预先存在的数据集来学习做出最优的序列决策？这种能力对于医学和工程等领域具有变革性意义，因为在这些领域，历史数据非常丰富，但实时实验成本高昂、风险大或不符合伦理。然而，从静态的过去中学习充满了挑战，其中一个基本挑战被称为[分布偏移](@entry_id:638064)——即生成数据的行为与我们希望学习的、可能更优的新行为之间存在不匹配。这种差距可能导致标准算法惨败，做出危险的、过于乐观且无依据的决策。

本文旨在为探索这一复杂领域提供指南。在两个章节中，我们将剖析离线[强化学习](@entry_id:141144)的核心问题，并探讨已经出现的优雅解决方案。首先，在“原理与机制”一章中，我们将深入探讨传统方法失败的原因，审视妄想性乐观和高方差估计器的陷阱，并介绍支撑成功算法的现代保守主义哲学。随后，在“应用与跨学科联系”一章中，我们将见证这些原理如何应用于解决高风险问题，从为脓毒症患者设计个性化治疗方案，到发现新药物，甚至为我们自己的大脑如何从过去学习提供一个模型。

## 原理与机制

想象一下，你想学习如何成为一名世界级的赛车手。但有一个难题：你不能亲自上车。你唯一的资源是一个庞大的数据库，其中包含了成千上万名普通司机的行车记录仪录像——这些人只是在上下班通勤、处理杂务，并且通常遵守限速。你拥有海量的驾驶数据，但没有一条数据是为了赢得比赛而生成的。这就是**离线强化学习**（RL）的根本挑战。我们拥有一个固定的、由“行为策略”（普通司机）收集的过往经验数据集，我们必须从中学习一个新的“目标策略”（赛车手），以优化一个目标，并且不能与真实世界进行任何进一步的交互。

这项任务极具挑战性。数据既是宝库，也是充满隐藏偏见和缺失信息的雷区。要想驾驭它，我们必须理解从固定的过去中学习的核心原则。

### 妄想性乐观：为什么标准算法会失败

在传统的在线[强化学习](@entry_id:141144)中，智能体通过试错来学习，就像婴儿学走路一样。它尝试一个动作，观察结果，然后调整其策略。这种方法的一个基石是**[Q学习](@entry_id:144980)**，其中智能体学习一个函数 $Q(s, a)$，该函数预测在状态 $s$ 下采取动作 $a$ 的总未来奖励。为了做出最佳决策，智能体只需寻找具有最高[Q值](@entry_id:265045)的动作：$\max_{a'} Q(s', a')$。当你可以探索自己行为的后果时，这种方法效果非常好。

但是，当我们将此逻辑应用于我们的离线数据集时，可怕的事情发生了。我们的算法在研究谨慎司机的录像时，可能会注意到它所处的一个状态——比如，正在接近一个平缓的弯道。然后它会问：“在这里我能采取的最好的动作是什么？”它的Q函数，作为一个像神经网络一样的灵活学习器，只在数据集中的温和动作上进行过训练。当它考虑一个激进的、分布外的动作——比如以每小时150英里的速度过这个弯——它没有任何数据来支持其预测。当[函数逼近](@entry_id:141329)器被要求对外推到未知领域时，它们可能会产生任意离谱的答案。而且由于$\max$算子的存在，算法会贪婪地抓住任何其[函数逼近](@entry_id:141329)器产生了虚假高估值的动作。

这就产生了一个危险的反馈循环。算法使用来自一个未观察到的动作的极高Q值来更新其当前的估计，这反过来又夸大了通往该动作的各个状态的价值。智能体变成了**妄想性乐观主义者**。它学会了偏爱那些没有任何现实世界证据的动作，仅仅因为它自己的内部模型幻想出了一个美妙的结果。这不仅仅是一个小错误；它是一种被称为**外推误差**的系统性失败模式，源于生成数据的谨慎行为策略与我们试图学习的激进目标策略之间的**[分布偏移](@entry_id:638064)**。

在高风险应用中，其后果可能是灾难性的。一个旨在协助医生治疗脓毒症的人工智能可能会从历史数据中学习，但随后由于这种妄想性乐观，推荐一种临床医生从未给药过的危险药物组合[@problem_id:5221425]。同样，一个用于设计新药的人工智能可能会提出化学上不稳定且无用的分子，仅仅基于纯粹的外推就确信它们具有非凡的特性[@problem_id:3861923]。

### 脆弱的修复：[重要性采样](@entry_id:145704)的风险

如果问题是[分布偏移](@entry_id:638064)，是否有直接的统计修复方法？最经典的工具是**[重要性采样](@entry_id:145704)（IS）**。其思想很直观：如果我们的目标策略采取某个特定动作的频率远高于行为策略，那么我们应该在计算中给该数据点更大的权重。一条轨迹的重要性权重是该轨迹在目标策略下发生与在行为策略下发生的概率之比：

$$
\rho(\tau) = \frac{P_{\pi}(\tau)}{P_{\mu}(\tau)} = \prod_{t=0}^{T-1} \frac{\pi(a_t|s_t)}{\mu(a_t|s_t)}
$$

通过用这些比率对数据集中的回报进行重新加权，我们可以得到新策略性能的无偏估计[@problem_id:3242021]。

不幸的是，这个优雅的解决方案在实践中常常无法使用。权重是整个轨迹上比率的乘积。如果两个策略在每一步上都有哪怕是微小但持续的分歧，最终的权重可能会爆炸性增长或趋近于零。这会导致估计器具有极高的方差；整个估计可能由单一条具有巨大权重的幸运轨迹所主导[@problem_id:3242021]。

这个问题与**[正定性](@entry_id:149643)**假设密切相关，该假设指出，目标策略可能采取的任何动作，在行为策略下被采取的概率必须为非零。如果我们的赛车策略想要以100英里/小时的速度行驶，但历史数据中完全没有这种情况，那么重要性权重就是无限大。在实践中，我们面临的是“近似违规”，即行为策略的概率 $\mu(a|s)$ 非常小但不为零。这仍然会导致巨大的权重。

我们可以通过计算**[有效样本量](@entry_id:271661)（ESS）**来诊断这个问题，这是一个[启发式](@entry_id:261307)指标，告诉我们加权后的数据集实际上相当于多少个“好”样本[@problem_id:4855038]。一个包含数千条患者轨迹的数据集，其ESS小于20的情况并不少见，这意味着我们的估计的可靠性仅相当于基于少数几个患者的估计。这使得朴素的[重要性采样](@entry_id:145704)成为解决复杂现实世界问题的脆弱工具。

### 悲观主义的兴起：离线学习的新哲学

由于朴素的乐观主义（[Q学习](@entry_id:144980)）和朴素的重加权（IS）都会失败，现代离线强化学习采纳了一种新的哲学：**保守主义**，或称结构化悲观。其指导原则很简单：*当有疑问时，保持谨慎。如果你没有某个动作的数据，不要假设它是好的。* 这种悲观主义可以通过两种主要方式实现。

#### 策略一：约束策略

最直接的保守方法就是强制我们的新策略保持与原始行为策略“接近”。我们可以通过在优化问题中增加一个约束来形式化这一点：最大化期望奖励，但条件是新策略 $\pi$ 与行为策略 $\mu_0$ 的偏离不能太大。衡量两个概率分布之间“距离”的一个自然方法是**KL散度（Kullback-Leibler divergence）**。我们的目标就变成了找到一个策略 $\pi$，在最大化回报的同时，将平均KL散度保持在某个安全阈值 $\varepsilon$ 以下[@problem_id:4855021]：

$$
\max_{\pi} \ \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^{t} r_t\right] \quad \text{subject to} \quad \mathbb{E}_{s \sim d^{\mu_0}} \left[ D_{\mathrm{KL}}\left(\pi(\cdot \mid s) \, \| \, \mu_0(\cdot \mid s)\right) \right] \ \le \ \varepsilon
$$

这种方法有效地在已知数据周围建立了一道“围栏”，防止智能体闯入动作空间中危险、未探索的区域。它提供了一个在最优性和安全性之间进行权衡的杠杆，这在医学等应用中至关重要。通过解决这个问题的惩罚版本，我们甚至可以获得相对于基线策略的安全[策略改进](@entry_id:139587)保证[@problem_id:3113572]。

#### 策略二：约束价值函数

一种更巧妙且通常更强大的方法是将悲观主义直接融入[价值函数](@entry_id:144750)本身。这就是**保守[Q学习](@entry_id:144980)（CQL）**背后的思想。CQL不仅仅是学习[Q值](@entry_id:265045)的估计，而是学习一个*保守估计*——真实[Q值](@entry_id:265045)的下界。它通过一个特殊的正则化器来修改标准的[Q学习](@entry_id:144980)目标，该正则化器会[主动抑制](@entry_id:191436)分布外动作的价值。

CQL的目标函数包含一个惩罚项，对于每个状态，该惩罚项旨在最小化数据中见过的动作的价值与数据中*未*见过的动作的价值之间的差距[@problem_id:3946166] [@problem_id:4115634]。示意性地，该惩罚项如下所示：

$$
\text{Penalty} = \underbrace{\log \sum_{a} \exp(Q(s, a))}_{\text{Soft maximum over ALL actions}} - \underbrace{\mathbb{E}_{a \sim \mu(\cdot|s)} [Q(s, a)]}_{\text{Average value of DATA actions}}
$$

最小化这个惩罚项会迫使分布外动作的Q值下降到与分布内动作的Q值相当的水平。它告诉算法：“你不能相信某个幻想中的动作会比我们有实际数据的动作好得多。” 这种对悲观主义的优雅数学表达在一系列具有挑战性的领域中被证明非常有效，从设计最优的[电池充电](@entry_id:269533)协议到管理电力微网[@problem_id:3946166] [@problem_id:4115634]。

### 数据中的幽灵：混杂与因果

即使有了这些复杂的算法，观测数据集中仍然潜藏着一个更深层次的挑战，尤其是在医学或经济学等领域。这就是**混杂**问题。

想象一个数据集，其中一种效果强但风险高的药物更频繁地被给予病情更重的患者。如果我们只看数据，我们可能会发现接受这种药物的患者预后更差。一个朴素的算法会学习到这种药物是有害的。但真的是药物*导致*了伤害，还是它的使用仅仅是患者本已病重的一个*标记*？患者潜在的病情严重程度就是一个**混杂因素**：一个同时影响行动（治疗选择）和结果的变量。

为了学习一个真正有效的策略，我们需要将这种相关性与因果关系[解耦](@entry_id:160890)。我们需要估计*干预*动态——如果我们*强制*一个患者服药，无论其状况如何，会发生什么？这是一个因果推断问题。未能解释混杂偏见会同时影响基于模型的方法（它们会学习一个有缺陷的世界模型）和无模型的方法（它们会学习一个有缺陷的价值函数）[@problem_id:4855013]。这揭示了现代离线[强化学习](@entry_id:141144)不仅仅是一个算法挑战；它与因果关系原则深度交织在一起。

### 科学家的准则：关于严格评估

最后，在应对了这些理论挑战并构建了一个保守的智能体之后，我们如何知道它是否真的有效？唯一的方法是通过严格、诚实的评估。这需要一种在任何科学探索中都至关重要的统计纪律。

最关键的规则是严格的数据分离。完整的数据集必须被划分——在独立单元的层面上，比如一个患者的就诊事件——为训练集和[测试集](@entry_id:637546)。测试集必须被保存在一个“保险箱”里，在整个学习过程中完全不被触碰。所有的策略训练、[超参数调整](@entry_id:143653)以及所有干扰成分（如行为策略）的估计都必须只使用训练数据来完成[@problem_id:4855030]。

只有当所有组件都最终确定后，我们才能打开测试集的“保险箱”，并进行一次性的、最终的评估。任何其他程序——比如将单个患者的数据分割到[训练集](@entry_id:636396)和[测试集](@entry_id:637546)中，或者根据[测试集](@entry_id:637546)性能来[调整参数](@entry_id:756220)——都会污染评估并导致过于乐观的偏倚结果。这相当于让一个学生在参加期末考试前先学习考卷。遵守这一准则是区分一厢情愿和可靠科学的关键，它将离线[强化学习](@entry_id:141144)从一个理论上的好奇心转变为一个值得信赖的、用于现实世界决策的工具。

