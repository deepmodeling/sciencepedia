## 引言
在创建智能系统的过程中，机器学习从业者通常从一个简单的目标开始：最大化准确率。这个直观的指标计算正确预测的数量，似乎是衡量成功的最终标准。然而，当模型从受控的数据集进入复杂、高风险的现实世界时，这种对准确率的关注暴露了一个关键缺陷：并非所有错误都是生而平等的。误诊一种危重疾病比造成一次微小的不便带来远为沉重的代价，但标准指标却对它们同等对待。统计性能与现实世界影响之间的这种鸿沟，使得对一种更精细方法的需​​求变得不容否认。

本文介绍**[成本敏感学习](@article_id:638483)**，这是一个强大的框架，直接解决了非对称后果的问题。它提供了一种形式化语言，用于构建不仅能识别模式，还能理解其错误相关成本并据此行动的模型。首先，在“原理与机制”一章中，我们将探讨最小化预期成本的核心思想，推导出最优决策规则，并检验实现它的两种主要策略：在训练后调整决策阈值和将成本直接[嵌入学习](@article_id:641946)[算法](@article_id:331821)中。随后，“应用与跨学科联系”一章将展示这一视角的深远影响，说明它如何为金融、医学、数据科学，乃至智能、自适应代理的基本设计中的问题提供一个统一的视角。

## 原理与机制

在我们构建智能系统的征途中，我们常常从一个简单、近乎孩童般的是非观念开始。一个分类要么是正确的，要么是错误的。目标似乎应该是最大化正确答案的数量。这就是**准确率**的世界，一个如此直观以至于感觉上是唯一可能重要的指标。但当我们从教科书问题的整洁世界 venturing 到人类事务的混乱现实——从医学到金融到工程——我们很快发现一个深刻的真理：并非所有错误都是生而平等的。

### 并非所有错误都是生而平等的

想象一个机器学习模型，旨在从医学图像中检测一种罕见但侵袭性强的癌症。它每天分析数千张扫描图。可能发生两种错误。

**假阳性**是指模型对健康患者发出了警报。这会导致焦虑、进一步的检查，或许还有不必要的活检。它有真实的成本——情感痛苦、时间和金钱。

**假阴性**是指模型对实际患有该疾病的患者给出了“一切正常”的信号。癌症未被发现，其宝贵的早期治疗窗口被错过。这里的成本是灾难性的，不是用金钱来衡量，而是用生活质量，甚至生命本身来衡量。

显而易见，这两种错误并不等同。假阴性的成本，我们称之为 $C_{FN}$，远大于假阳性的成本 $C_{FP}$。一个通过正确识别所有健康患者但错过了少数患病者而达到 99% 准确率的模型，在实践中是 spectacularly 失败的，无论其准确率分数看起来多么令人印象深刻 [@problem_id:3105717]。这就是**[成本敏感学习](@article_id:638483)**的精髓：识别并根据**非对称成本**的现实采取行动。世界不是一个“正确”与“不正确”的 0-1 游戏；它是一个充满各种后果的图景，我们的目标是构建能够理性地驾驭这个图景的系统。

### 行动的理性指南：最小化预期成本

那么，如果仅仅最大化准确率是错误的目标，那什么才是正确的目标呢？答案来自决策理论核心的一个优美而强大的思想：**最小化预期成本**。

让我们像一个理性代理一样思考。对于任何给定的情况——一张特定的医学图像、一笔金融交易、一组来自风力涡轮机的传感器读数 [@problem_id:3142093]——我们的模型为我们提供了一些信息。具体来说，一个表现良好的分类器会给我们某个事件为真的概率。假设我们的模型查看一张图像 $x$ 并计算出患癌症的概率 $p(Y=1|x)$。这意味着没有癌症的概率是 $1 - p(Y=1|x)$。

我们有两个选择：预测“癌症”（$\hat{Y}=1$）或预测“无癌症”（$\hat{Y}=0$）。每个选择的预期成本是多少？

如果我们预测“癌症”，我们可能是对的（[真阳性](@article_id:641419)，成本=0），也可能是错的（假阳性，成本=$C_{FP}$）。预期成本是每个结果的成本乘以其概率的总和：
$$
\text{预期成本}(\hat{Y}=1) = (0 \times p(Y=1|x)) + (C_{FP} \times p(Y=0|x)) = C_{FP} (1 - p(Y=1|x))
$$

如果我们预测“无癌症”，我们可能是对的（真阴性，成本=0），也可能是错的（假阴性，成本=$C_{FN}$）。预期成本是：
$$
\text{预期成本}(\hat{Y}=0) = (C_{FN} \times p(Y=1|x)) + (0 \times p(Y=0|x)) = C_{FN} p(Y=1|x)
$$

理性的做法是选择预期成本*较低*的行动。我们应该当且仅当以下条件成立时预测“癌症”：
$$
C_{FP} (1 - p(Y=1|x)) \le C_{FN} p(Y=1|x)
$$
通过一点代数运算，我们可以分离出概率 $p(Y=1|x)$。规则变成：如果……则预测“癌症”
$$
p(Y=1|x) \ge \frac{C_{FP}}{C_{FN} + C_{FP}}
$$
这个单一、优雅的不等式是[成本敏感分类](@article_id:639556)的北极星。它精确地告诉我们在面对不确定性和不平等的后果时如何做出最佳决策 [@problem_id:3142093] [@problem_id:3178441]。它为构建不仅能模仿模式，还能做出理性的、成本感知的选择的机器提供了蓝图。现在，让我们探讨实现这一原则的机制。

### 机制一：在决策时调整标准

使用我们的指导原则最直接的方法是，拿一个标准的、训练有素的分类器，然后简单地改变它的决策规则。大多数分类器默认使用 0.5 的概率阈值。如果它们“超过 50% 确定”，它们就预测为正类。我们的公式给出了一个新的、更智能的阈值。

我们称之为**贝叶斯最优阈值**，$t^{\star}$：
$$
t^{\star} = \frac{C_{FP}}{C_{FN} + C_{FP}}
$$
我们的新决策规则是：如果 $p(Y=1|x) \ge t^{\star}$，则预测为类别 1。

让我们看看这个公式。如果假阴性的成本（$C_{FN}$）远高于假阳性的成本（$C_{FP}$），分母会变得非常大，阈值 $t^{\star}$ 会变得非常小。这完全符合直觉！如果漏掉一个癌症病例是灾难性的，我们应该降低发出警报的门槛。我们不再需要 50% 的把握；也许仅仅 5% 或 1% 的把握就足以值得再看一眼。我们变得更加敏感，以更多假警报为代价捕获更多[真阳性](@article_id:641419)——这是我们明确选择为最优的权衡 [@problem_id:3105717]。

这种技术，被称为**阈值移动**，因其简单而强大。我们可以拿一个现成的模型，并将其适应于特定的成本情景，而无需重新训练。如果涡轮机维护的经济成本发生变化，我们只需重新计算 $t^{\star}$ 并部署新规则即可 [@problem_id:3142093]。

### 一个关键前提：概率的语言

阈值移动有一个至关重要的微妙之处。规则 $p(Y=1|x) \ge t^{\star}$ 只有在数字 $p(Y=1|x)$ 是一个*真实概率*时才有意义。模型的输出不能只是某个任意的分数；它的大小必须是有意义的。0.7 的输出必须真正意味着事件发生的概率为 70%。具有此属性的模型被称为是**已校准的**。

许多模型，特别是像深度神经网络这样的复杂模型，产生的得分（通常称为 **logits**）并未经过校准。这些得分可能很适合排序——正确地为更可能的候选项分配更高的分数——但它们的[绝对值](@article_id:308102)没有意义。对这些得分应用 `[argmax](@article_id:638906)` 来找到最可能的类别是可以的，但将它们与一个特定的基于成本的阈值 $t^{\star}$ 进行比较则是无稽之谈。这就像试图通过看一个刻度扭曲、非线性的温度计来决定天气是否热到可以去游泳一样 [@problem_id:3170662]。

因此，在使用阈值移动之前，我们通常必须执行一个称为**校准**的后处理步骤。像 Platt 缩放或保序回归这样的技术可以学习一个从模型的原始分数到校准概率的映射。对于[神经网络](@article_id:305336)，一种流行的方法是**温度缩放**，它通过在最终概率计算前将 logits 除以一个温度值 $T$ 来调整模型的“[置信度](@article_id:361655)”。最优温度可以通过在验证集上最小化经验成本来找到，确保最终的概率不仅排名良好，而且对于我们的成本敏感决策规则在数值上也是有意义的 [@problem_id:3180170]。

### 机制二：在训练期间改变规则

阈值移动是一个优雅的解决方案，但它有一个局限性。它只能利用模型已经学到的信息。如果标准的训练过程，痴迷于整体准确率，导致模型忽略了稀有但关键类别的微妙模式，该怎么办？如果模型从一开始就没学会区分患病患者，那么再多的阈值移动也无济于事。模型对患病和健康样本的后验概率可能完全混杂在一起。

解决方案是更早地介入：在训练过程本身。我们可以修改学习[算法](@article_id:331821)，使其从一开始就具有成本意识。

#### 改变评分方案：加权损失

一种方法是改变损失函数，也就是模型优化的“评分方案”。在标准训练中，每个错误受到的惩罚是相等的。在**成本敏感训练**中，我们可以使用**加权[损失函数](@article_id:638865)**。对于我们的医学例子，我们会告诉模型：犯一个假阴性错误对你的分数造成的损害是犯一个[假阳性](@article_id:375902)错误的 100 倍。

在数学上，当我们使用加权[交叉熵损失](@article_id:301965)时，我们是在告诉模型最小化一个风险，其中错误分类一个类别为 $y$ 的样本的损失乘以一个权重 $w_y$。一个优美的理论结果表明，这等同于在一个*新的、想象的数据集*上训练一个[标准模型](@article_id:297875)，其中类别已根据这些权重重新采样 [@problem_id:3110756]。通过提高稀有、高成本类别的权重，我们实际上是迫使模型在一个更平衡的数据集上进行训练，在这个数据集中，它再也不能通过简单地忽略少数类来获得好成绩。

#### 重塑大脑：修改[算法](@article_id:331821)

一个更深刻的方法是修改学习[算法](@article_id:331821)本身的内部机制。决策树提供了一个极其清晰的例证。一棵树是通过一系列分裂来构建的。在每一步，[算法](@article_id:331821)都会搜索一个能使产生的数据组更“纯净”的分裂。纯度的标准度量是像**Gini 不纯度**或**熵**之类的东西。

但我们为什么要使用 Gini 不纯度呢？在一个成本敏感的世界里，“最好”的分裂是导致**总预期成本降低**最大的那个。我们可以用我们自己的基于成本的计算来取代 Gini 不纯度计算。对于树中的任何节点，我们可以计算出如果停在那里并做出最优的基于成本的预测所能达到的最小成本。该节点的“不纯度”*就是*这个最小成本。然后，[算法](@article_id:331821)贪婪地选择那些能尽可能多地降低这个成本的分裂 [@problem_id:3113027]。这棵树不再仅仅是划分数据；它正在积极地构建一个决策过程，以最小化现实世界的成本。

### 选择你的策略：通往成本敏感思维的两条路径

我们有两种强大的技术系列：在训练后调整决策阈值，以及在训练期间融入成本。我们应该选择哪一种？

答案取决于问题。正如我们所见，阈值法简单而灵活。如果成本改变，你只需重新计算 $t^\star$。然而，它只有在底层模型从一开始就擅长区分不同类别时才有效。

训练时的方法更强大。它们可以从根本上改变所学模型的结构。考虑这样一个场景：一个未加权的决策树决定某个特定的分裂不够“纯净”，不值得进行，于是它停止了。然而，同一个树的成本加权版本可能会意识到，尽管这个分裂只分开了几个数据点，但这些点都来自高成本的少数类。成本的降低是巨大的，所以它进行了分裂。加权树发现了数据中一个未加权树所忽视的结构。在这种情况下，无论对未加权树进行多少阈值调整，都无法恢复那些丢失的信息 [@problem_id:3112943]。

### 常用指标的隐藏成本

这次进入[成本敏感学习](@article_id:638483)的旅程揭示了最后一个、统一的见解。我们开始时批评准确率是幼稚的。但像**F1 分数**这样更复杂的指标又如何呢？F1 分数是[精确率和召回率](@article_id:638215)的调和平均值。许多从业者将最大化 F1 分数视为不平衡问题的一个良好默认目标。

这真的更好吗？不一定。事实证明，最大化 F1 分数等同于最小化一个预期成本，但带有一个特定的、*隐性*的成本比率。通过选择优化 F1 分数，你实际上是在告诉你的模型，假阴性的成本比[假阳性](@article_id:375902)更重要，但其重要程度仅由你数据中的类别平衡决定的特定比率所决定。在一个理想化的情景中，可以证明最大化 F1 分数等同于假设成本比率 $C_{FN}/C_{FP}$ 是[黄金比例](@article_id:299545)，$\phi \approx 1.618$ [@problem_id:3105755]。

成本是无法逃避的。无论你是明确指定它们，还是通过选择像准确率、精确率或 F1 分数这样的指标来隐性地指定它们，你总是在声明你关心什么样的后果。[成本敏感学习](@article_id:638483)的原则是使这一选择变得审慎、理性和透明，将我们的模型从机械学习者转变为能够推理其行为后果的代理。

