## 引言
在大数据时代，从生物学到[材料科学](@article_id:312640)的各个领域都充斥着极其复杂的信息。我们如何才能教会机器理解蛋白质的微妙语言、DNA 的语法或分子的反应潜力？答案不在于编写明确的规则，而在于让机器从数据本身中学习自己的表示方法。本文深入探讨了推动这场革命的核心概念：**学习[嵌入](@article_id:311541)**。我们探索了这种强大的方法如何将抽象数据转化为有意义的几何图谱，在这些图谱中，复杂的关系变成了简单的距离和角度。这种方法弥合了原始[高维数据](@article_id:299322)与可操作的科学洞见之间的根本差距。在接下来的章节中，您将发现创建这些“意义图谱”的基本原理，并见证它们在不同科学学科中的变革性影响。我们将首先探讨支配[嵌入学习](@article_id:641946)方式的原理和机制，从意义的几何学到[预训练](@article_id:638349)的力量。随后，我们将遍览其众多应用和跨学科联系，了解这些学习到的表示如何被用于解读生命蓝图、绘制[复杂网络](@article_id:325406)，并为医学带来新的清晰度。

## 原理与机制

一台由硅和[逻辑门](@article_id:302575)构成的机器，如何能理解分子的微妙舞蹈、蛋白质的复杂语言，或是编码在 DNA 中的宏大进化图景？当然，它无法像人类一样“理解”。但它可以做一些非常强大的事情：学会以一种能将复杂关系简单化的方式来表示世界。这种能力的关键在于一个被称为**学习[嵌入](@article_id:311541)**的概念。[嵌入](@article_id:311541)不过是一串数字——一个向量——但它是一串特殊的数字，充当坐标，将一个概念置于一个丰富的高维“意义图谱”中。在模型学会了这张图谱后，从[药物发现](@article_id:324955)到设计新酶的一切都变成了几何问题。

### 意义的几何学

想象一下，试着解释猫、狗和汽车之间的关系。你可以写几段文字来描述它们的特征。猫和狗都是动物，有四条腿，通常是宠物。汽车是机器。现在，如果你能把这些概念放在一张地图上呢？你很可能会把猫和狗放在非常靠近的位置，而把汽车放在很远的地方。

这就是[嵌入](@article_id:311541)的核心思想。我们将每一个感兴趣的对象——无论是一个词、一个分子，还是一个完整的蛋白质——表示为一个多维空间中的点。对象的“意义”由其位置捕获，而它与其他对象的关系则由该空间的几何结构捕获。邻近意味着相似。

这不仅仅是一种哲学上的奇想，它具有深远的实际意义。假设一个生物学家团队拥有一种靶向特定蛋白质（我们称之为蛋白质Y）的药物。他们发现了一种与某种疾病相关、未经研究的新蛋白质X。同一种药物会起作用吗？如果我们已经为这些蛋白质学到了好的[嵌入](@article_id:311541)，我们可以简单地将它们表示为向量 $v_X$ 和 $v_Y$，并测量它们之间的“夹角”。一个小的夹角（即高的**[余弦相似度](@article_id:639253)**）表明这两种蛋白质在功能上是相似的，该药物可能对两者都有效[@problem_id:1426742]。 “蛋白质相似性”这个抽象概念被转化为了一个简单的几何计算。因此，其魔力不在于计算本身，而在于计算机最初是如何学会绘制这张图谱的。

### 学习自然语言

这些神奇的图谱是如何创建的？我们不是手动编程蛋白质或基因的“意义”。那将是一项不可能完成的任务。相反，我们让机器从海量数据中学习，遵循语言学中一个简单而深刻的原则，即**[分布假说](@article_id:638229)**：一个词的意义由其上下文决定（you shall know a word by the company it keeps）。

想一想“queen”（女王）这个词。你知道它的意思，因为你在“the king and queen”（国王和女王）、“Queen Elizabeth”（伊丽莎白女王）和“the queen ruled her kingdom”（女王统治她的王国）等语境中见过它。周围的词语提供了它的意义。同样的原则也适用于自然的构件。一个氨基酸的定义取决于在数百万个[蛋白质序列](@article_id:364232)中它倾向于出现在哪些其他氨基酸旁边。一个基因的功能则由其在数千个实验中与其他哪些基因共表达所暗示。

机器学习模型，特别是[神经网络](@article_id:305336)，可以被训练来玩一个基于此原则的“游戏”。我们可以取一个句子（或一个[蛋白质序列](@article_id:364232)），隐藏其中一个词（或氨基酸），然后要求模型根据上下文预测这个缺失的部分。这是**连续词袋（CBOW）**模型背后的核心思想。或者，我们可以给模型一个词，让它预测可能出现在其邻近位置的词。这就是**Skip-Gram**模型 [@problem_id:2373389]。

为了在这个游戏中表现出色，模型必须为每个词开发一个内部表示——这个表示*就是*学习到的[嵌入](@article_id:311541)。出现在相似上下文中的词需要有相似的内部表示，才能做出持续良好的预测。因此，作为学习预测上下文的一个美妙副产品，模型会自动将概念组织到一个有意义的几何空间中。它学习自然语言不是通过背诵字典，而是通过观察其“词汇”如何被使用。

### 从原始数据到有意义的词元

在我们[嵌入](@article_id:311541)任何东西之前，我们必须首先确定我们语言的基本“词汇”或**词元（tokens）**是什么。这个过程被称为**词元化（tokenization）**，是一个至关重要且常常微妙的步骤。词元化器的任务是将原始数据流分解为一系列离散的单元，每个单元都将获得自己的[嵌入](@article_id:311541)向量 [@problem_id:1426767]。

对于一个由SMILES字符串（如`CCO`，乙醇）表示的分子，一个简单的词元化器可能只是将其分割成字符：`C`、`C`、`O`。然而，一个更智能的词元化器会认识到`C`和`O`是原子单元，并且可能还会将更复杂的化学符号（如代表氯的`Cl`）作为单个词元来处理。选择什么构成一个“词元”是为问题施加结构的第一步。

这个选择绝非小事，它可以极大地改变模型能够学习的内容。考虑一个编码蛋白质的基因。DNA序列以三个[核苷酸](@article_id:339332)为一组的**[密码子](@article_id:337745)**被读取，每个[密码子](@article_id:337745)对应一个氨基酸。这个编码存在冗余；例如，六个不同的[密码子](@article_id:337745)都编码氨基酸亮氨酸（Leucine）。

如果我们选择在氨基酸水平上进行词元化，我们就会丢失这些信息。所有六个亮氨酸[密码子](@article_id:337745)都被映射到同一个“亮氨酸”词元。模型将无法辨别使用了哪个具体的[密码子](@article_id:337745)。然而，如果我们在[密码子](@article_id:337745)水平上进行词元化，模型就能区分所有64种可能的[密码子](@article_id:337745)。这使得它能够学习与**[密码子使用偏好](@article_id:304192)**相关的模式——这是一种微妙的生物学现象，即生物体倾向于使用某些[同义密码子](@article_id:354624)而非其他，这会影响蛋白质生产的速度和效率[@problem_id:2749071]。词元化的选择定义了我们“地图”的分辨率。

在架构上，[嵌入](@article_id:311541)层本身出奇地简单：它只是一个大型查找表，一个大小为 $V \times d$ 的矩阵 $E$，其中 $V$ 是我们词汇表中唯一词元的数量，$d$ 是我们[嵌入空间](@article_id:641450)的维度。当我们需要第 $i$ 个词元的[嵌入](@article_id:311541)时，我们只需取出矩阵的第 $i$ 行。如果我们发现一种新的氨基酸并需要将其添加到模型中，我们不需要重建所有东西；我们只需在[嵌入](@article_id:311541)表中添加新的一行，以容纳我们新词元的向量 [@problem_id:2387795]。

### 对称性与[不变性](@article_id:300612)：将物理学[嵌入](@article_id:311541)机器

一个真正强大的表示不仅要捕捉统计模式，还必须尊重宇宙的基本定律——自然的对称性。一个理解这些对称性的模型数据效率更高，也更鲁棒，因为它不需要浪费精力从头重新学习物理学的基本规则。

考虑在三维空间中对一个分子进行建模。它的总势能是一个标量属性，取决于其原子的相对位置。如果你将这个分子平移到不同位置或旋转它，它的能量不会改变。这个属性被称为**$SE(3)$ 不变性**，代表平移和旋转的[特殊欧几里得群](@article_id:299831)。任何从三维坐标预测能量的模型都*必须*内置这种不变性。如果你给它一个分子，然后给它同一个分子但方向不同，它必须返回完全相同的能量。

然而，模型*不应*对镜像反射具有[不变性](@article_id:300612)。生命的构件是手性的——蛋白质由L-氨基酸构成，DNA的螺旋是右旋的。一个分子和它的镜像（它的对映异构体）可以有截然不同的生物学特性。一个好的模型必须能够区分它们[@problem_id:2749074]。

所需的对称性完全取决于数据和问题。
-   **对于三维[原子结构](@article_id:297641)**，模型必须对整个系统的旋转和平移具有不变性，但对反射则不然。它还必须对原子具有**[置换](@article_id:296886)不变性**——能量不取决于你将哪个原子标记为“1号”。这是[图神经网络](@article_id:297304)的领域。
-   **对于一维蛋白质序列**，模型*绝不能*具有[置换](@article_id:296886)[不变性](@article_id:300612)。“丙氨酸-甘氨酸”序列与“甘氨酸-丙氨酸”是不同的分子。顺序决定一切。这是像 Transformers 和[循环神经网络](@article_id:350409)这样对位置敏感的模型的领域。

我们甚至可以将这些对称性直接注入学习过程。DNA 是一个双螺旋结构。一条链上的序列，比如`GA[TTA](@article_id:642311)CA`，在另一条链上有一个与之对应的**反向互补**序列`TGTAATC`。从生物学角度来看，这两个序列代表了同一段基因组信息。在为短 DNA 片段（[k-mer](@article_id:345405)s）学习[嵌入](@article_id:311541)时，我们可以通过强制一个 [k-mer](@article_id:345405) 的[嵌入](@article_id:311541)向量与其反向互补序列的[嵌入](@article_id:311541)向量相同，来强制执行这一物理现实。这个简单的技巧，称为**[参数绑定](@article_id:638451)**，能将有效词汇量减半，并使模型立即意识到 DNA 结构的一个基本属性 [@problem_id:2479909]。

### [预训练](@article_id:638349)的力量：站在巨人的肩膀上

[嵌入](@article_id:311541)、从上下文中学习以及尊重对称性等概念，最终汇聚成现代人工智能中最具变革性的思想之一：**[迁移学习](@article_id:357432)**。

想象一下，在一个几乎包含所有已知[蛋白质序列](@article_id:364232)的庞大数据集上训练一个大型模型——这些序列数以百万计，从[生命之树](@article_id:300140)的各个角落收集而来。通过在这个巨大的数据集上玩上下文预测游戏，模型学习到一个结构精深的[嵌入空间](@article_id:641450)，一种通用的“蛋白质语言”[@problem_id:2749082]。这个[预训练](@article_id:638349)模型没有被明确地教导关于蛋白质折叠或[酶动力学](@article_id:306191)的知识，但为了在预测任务中表现出色，它已经含蓄地学会了将这些原理编码到其表示中。它的[嵌入](@article_id:311541)捕捉了共进化、结构接触和功能角色的信号。

现在，假设你是一位科学家，面临一个新的具体问题：预测一个酶家族的某个属性，但你只有几百个标记好的例子。从头开始训练一个复杂的模型将是无望的；它只会记住这个小数据集，而无法泛化。但有了[迁移学习](@article_id:357432)，你就不必从零开始。你使用那个强大的[预训练](@article_id:638349)模型作为[特征提取器](@article_id:641630)。你将你的酶序列输入其中，得到丰富的[预训练](@article_id:638349)[嵌入](@article_id:311541)。然后，你基于这些[嵌入](@article_id:311541)训练一个非常简单的模型 [@problem_id:1426776]。

这个过程非常有效，因为[嵌入](@article_id:311541)提供了巨大的领先优势。从贝叶斯角度来看，[预训练](@article_id:638349)充当了一个信息量极大的**先验**，即使在特定数据非常少的情况下，也能引导模型走向物理上和生物学上合理的解决方案 [@problem_id:2749082]。

这个[范式](@article_id:329204)也是可扩展的。当我们遇到全新的事物，比如需要为一个包含原始模型从未见过的元素（例如氧）的[系统建模](@article_id:376040)时，会发生什么？我们不必扔掉我们辛苦学到的知识。我们可以使用一些巧妙的技术，比如为氧添加一个新的**元素[嵌入](@article_id:311541)**，并在一小部分新例子上仅对模型的一小部分（使用**适配器**）进行微调。或者我们可以使用**[多任务学习](@article_id:638813)**来明确地建模新元素与旧元素之间的关系。这使得模型可以跨元素“借用”统计强度，基于氧与氮和碳的化学相似性来对氧进行推理 [@problem_id:2784623]。

为了使这种适应更加高效，我们可以使用**[主动学习](@article_id:318217)**。我们不是随机收集一些含氧分子的例子，而是询问模型在哪些地方最不确定。然后，我们只对那些信息量极高的点进行昂贵的实验室实验或[量子计算](@article_id:303150)，并将结果反馈给模型，以填补其知识中最大的漏洞 [@problem_id:2784623]。

最终，这些学到的意义图谱不仅用于分析，也用于创造。通过在这个丰富的[嵌入空间](@article_id:641450)之上构建一个统计模型，例如[高斯过程](@article_id:323592)，我们可以在可能存在的分子或蛋白质的广阔图景中导航。利用**[贝叶斯优化](@article_id:323401)**，我们可以智能地问模型：“根据你所知，我应该在实验室合成什么新序列，才能最有希望提高这种酶的活性？”[@problem_id:2749082]。我们不再仅仅是阅读自然之书；我们正在学习它的语言，以便开始书写我们自己的新篇章。