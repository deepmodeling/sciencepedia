## 引言
科学正处在一场[范式](@article_id:329204)转变的边缘，这场变革由机器学习与宇宙基本定律之间的强大新联盟所驱动。虽然标准的“黑箱”人工智能模型在模式识别方面表现出色，但它们通常对支配其所处理数据背后的物理原理缺乏内在理解。这种知识鸿沟可能导致物理上不合理的预测，以及对海量数据集的无尽需求。[科学机器学习](@article_id:305979)（Scientific Machine Learning, SciML）通过系统地将科学领域知识融入人工智能[算法](@article_id:331821)的核心，直接应对了这一挑战。本文将探索这一变革性领域，详细阐述我们如何构建更智能、更鲁棒、更具洞察力的模型。

本次探索之旅主要分为两个部分。在第一章“原理与机制”中，我们将深入研究 SciML 的基础技术，探讨如何将物理对称性编码到模型架构中，利用[可微分编程](@article_id:343210)掌握微积分的语言，以及通过信息损失函数将物理定律本身作为“教师”。随后，“应用与跨学科联系”一章将展示这些原理的实际应用，揭示 SciML 如何革新[材料科学](@article_id:312640)等领域，并为理解[统计力](@article_id:373880)学与机器学习本身之间的深层联系提供新的视角。

## 原理与机制

我们已经看到，科学正处于一场革命的转折点上，其动力源于物理原理与机器学习之间的新型伙伴关系。但这种伙伴关系具体是怎样的呢？我们如何将一个本质上是模式发现机器的机器学习模型，转变为能够理解宇宙深邃而优雅规则的东西？

这并非简单地将更多数据投入“黑箱”并[期望](@article_id:311378)得到最佳结果。那种方法有其局限性。一个用一百万张猫的图片训练出来的神经网络会学会猫长什么样，但它不会知道，如果一只猫被扔下，会因为重力而下落。它不理解物理。[科学机器学习](@article_id:305979)的挑战——也是其魅力所在——在于打开那个黑箱，并为其注入数百年科学发现的智慧。一个标准的机器学习模型或许能根据数据中的模式区分出好材料和坏材料，但选择一个能反映预期底层物理规律的模型架构，例如为复杂[材料属性](@article_id:307141)选择一个非[线性模型](@article_id:357202)，本身就[能带](@article_id:306995)来远为优越的结果 [@problem_id:1312273]。

在本章中，我们将踏上一段旅程，去理解那些能更进一步的核心原理。我们将看到如何将自然界的[基本对称性](@article_id:321660)编码到我们的模型中，教会它们微积分的语言，并在学习过程中将物理定律本身作为指导性的“教师”。

### 对称性的语言：在机器中构建[不变性](@article_id:300612)

整个物理学中最深刻的思想之一是[对称性与守恒](@article_id:315270)定律之间的联系。当我们说一个系统具有**对称性**时，我们指的是即使我们改变观察视角，它的某些属性仍然保持不变。例如，今天的物理定律和昨天一样（[时间平移对称性](@article_id:324805)，导致[能量守恒](@article_id:300957)），在纽约和在东京也一样（空间[平移对称性](@article_id:350762)，导致动量守恒）。

一个标准的机器学习模型对这些对称性一无所知。如果你用一个地点的数据训练它，它没有*先验*理由相信，如果你把整个实验向左移动一英里，它的预测仍然成立。[科学机器学习](@article_id:305979)通过将这些对称性明确地构建到模型的架构本身来纠正这一点。

#### 平移不变性：你身在何处不应有影响

想象一下，对一个盒子中一群原子的总能量进行建模。这个能量取决于原子们*相互之间*的[排列](@article_id:296886)方式，但它肯定不应该取决于盒子是在你的实验室里还是在月球上。总能量必须对整个系统的平移保持不变。

我们如何保证这一点？通过设计我们的模型，使其只看到原子间的*相对*位置 $\mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i$，而不是它们的绝对位置 $\mathbf{r}_i$ 和 $\mathbf{r}_j$。如果模型的输入本身就是平移不变的，那么它的输出也必然是平移不变的。

这个简单的设计选择带来了一个优美而深刻的推论。如果一个模型的能量预测是平移不变的，那么可以证明，它为系统中所有原子预测的力的总和必须恰好为零，即 $\sum_{k=1}^{N} \mathbf{F}_k = \mathbf{0}$ [@problem_id:91075]。这正是牛顿第三定律的体现，直接导致了孤立系统的总[动量守恒](@article_id:321373)。通过教会模型一个空间的基本对称性，我们免费得到了一个基本的物理定律！

#### [置换](@article_id:296886)[不变性](@article_id:300612)：同卵双胞胎是可互换的

在处理全同粒子时，另一个至关重要的对称性出现了。如果你有两个[氦原子](@article_id:310662)，原子1和原子2，它们之间没有任何物理上的区别。你计算的任何属性，比如包含它们的系统的能量，如果在你魔法般地交换它们的标签后，结果必须保持不变。这就是**[置换](@article_id:296886)[不变性](@article_id:300612)**。

一个朴素的机器学习模型可能会将一系列相邻原子作为输入：（邻居1，邻居2，...）。如果你重新[排列](@article_id:296886)这个列表，输入就变了，模型的输出也可能随之改变，这在物理上是错误的。那么，我们如何编码这一点呢？

一个非常简单的策略是使用一个不依赖于顺序的描述符。例如，假设我们想要描述一个中心原子周围的环境。我们可以计算到所有邻居的距离的倒数，把它们放进一个列表，然后按降序*排序*这个列表 [@problem_id:91132]。这个排好序的列表现在就是我们模型的输入。无论你最初如何打乱邻居的顺序，排序后的列表总是相同的。现在，模型正确地理解了相同的邻居是可互换的。实践中使用的更复杂的描述符，比如 Behler-Parrinello 网络中的描述符，就是建立在这一基本原则之上的 [@problem_id:91080]。这个思想可以被进一步推广，利用[对比学习](@article_id:639980)等先进技术来教导模型识别出一个[晶体缺陷](@article_id:330719)，即使它被移动到[晶格](@article_id:300090)中一个不同但[晶体学](@article_id:301099)等效的位置，仍然是同一个对象 [@problem_id:38541]。

通过坚持让我们的模型尊重这些[基本对称性](@article_id:321660)，我们不仅仅是在增加约束；我们是在为它们提供关于世界如何运作的强大先验知识，使它们更准确、数据高效且物理上更合理。

### [导数](@article_id:318324)的力量：[可微分编程](@article_id:343210)

微积分是物理科学的母语。速度是位置的[导数](@article_id:318324)；力是势能的负[导数](@article_id:318324)；电场是电势的[导数](@article_id:318324)。要构建真正理解物理的模型，它们必须能够“说”微积分。

这正是现代人工智能的一项关键赋能技术的用武之地：**[可微分编程](@article_id:343210)**。[深度学习](@article_id:302462)中的“学习”是通过一种名为[反向传播](@article_id:302452)的[算法](@article_id:331821)实现的，这是一种计算[损失函数](@article_id:638865)相对于数百万个模型参数的[导数](@article_id:318324)的巧妙方法。驱动这一切的引擎被称为**[自动微分](@article_id:304940) (AD)**。

AD 是一项非凡的技术，它将程序计算的任何复杂[函数分解](@article_id:376689)为一系列基本运算（如加法、乘法、`sin`、`exp`）。然后，它细致地、一步步地应用[链式法则](@article_id:307837)，来计算精确的[导数](@article_id:318324) [@problem_id:2154666]。它不是[符号微分](@article_id:356163)（像你手算的那样），也不是数值近似（像有限差分）。它是对*代码本身*的[导数](@article_id:318324)的精确、[算法](@article_id:331821)化的计算。

这对科学有着惊人的启示。如果我们能构建一个机器学习模型来预测一个物理量，比如一个原子系统的势能 $E$，我们就可以使用 AD 来计算该能量对其任何输入的[导数](@article_id:318324)。例如，作用在一个原子上的力是能量对其位置的负梯度：$\mathbf{F}_k = -\nabla_{\mathbf{r}_k} E$。

有了可[微分](@article_id:319122)的[机器学习势](@article_id:362354)，我们就不需要训练一个单独的模型来学习力！我们训练一个模型来学习标量能量 $E$，然后我们通过对训练好的模型进行[微分](@article_id:319122)，就能够*解析地*获得矢量力 $\mathbf{F}_k$ [@problem_id:91000]。这不是一个近似；它是我们所构建模型的一个直接推论。这保证了力和能量是自洽的，这个性质在[分子模拟](@article_id:362031)的背景下被称为**[能量守恒](@article_id:300957)**。

同样的原理也用于训练模型本身。模型的参数（[权重和偏置](@article_id:639384)）是通过计算一个权重的微小变化如何影响最终误差来调整的——这是一个使用 AD 计算的[导数](@article_id:318324) [@problem_id:91003]。这个统一的框架，其中物理定律（如 $F = -\nabla E$）和学习过程本身都通过[导数](@article_id:318324)来表达，是 SciML 如此强大的核心所在。

### 物理作为终极教师：[信息损失](@article_id:335658)函数

有时候，我们想要强制执行的物理原理过于复杂，无法硬编码到模型的架构中。那该怎么办？我们可以将物理作为一种监督形式，一个在训练过程中指导模型的“教师”。这就是**物理信息神经网络 ([PINNs](@article_id:305653))** 背后的核心思想。

标准机器学习模型的训练是通过最小化一个**损失函数**来驱动的，该函数通常衡量模型预测与真实数据之间的差异（例如，[均方误差](@article_id:354422)）。在物理信息方法中，我们对这个损失函数进行了扩充。我们增加了额外的项，这些项会对模型违反已知物理定律的行为进行惩罚。

让我们来看一个来自[材料科学](@article_id:312640)的具体例子。假设我们正在训练一个神经网络 $E_{NN}(V; \mathbf{w})$，来预测一个晶体的[内聚能](@article_id:299771) $E$ 作为其体积 $V$ 的函数。我们有一些来自昂贵的量子力学计算的数据点。标准的[损失函数](@article_id:638865)将是：

$L_{data} = \frac{1}{N} \sum_{i=1}^{N} (E_{NN}(V_i; \mathbf{w}) - E_{true,i})^2$

但我们还知道关于这条能量曲线的一些基本物理知识。在平衡体积 $V_0$ 处，晶体是稳定的，这意味着压强 $P = -dE/dV$ 必须为零。我们还知道它的刚度，即体模量 $B_0 = V_0 \left. \frac{d^2E}{dV^2} \right|_{V=V_0}$。

我们可以把这些定律教给我们的模型！我们在[损失函数](@article_id:638865)中增加惩罚项：

$L_{physics} = \lambda_d \left( \left. \frac{dE_{NN}}{dV} \right|_{V_0} \right)^2 + \lambda_b \left( V_0 \left. \frac{d^2E_{NN}}{dV^2} \right|_{V_0} - B_0 \right)^2$

现在的总损失是 $L_{total} = L_{data} + L_{physics}$ [@problem_id:90090]。在训练期间，模型试图最小化这个总损失。它被迫找到一组参数 $\mathbf{w}$，这组参数不仅要拟合我们拥有的数据点，*而且*要满足在平衡体积处压强为零和具有正确体模量的物理约束。模型学会了像一个真实的物理系统一样行事，即使在没有直接数据的区域也是如此。这是一种注入领域知识、提高泛化能力并使模型更鲁棒的极其强大的方式，尤其是在数据稀缺的情况下。

### 拥抱复杂性：SciML 的前沿

对称性、可微分性和[物理信息](@article_id:312969)损失的原则构成了 SciML 的基石。但该领域正在迅速扩展，以应对更复杂、更实际的科学挑战。

#### 融合廉价与昂贵的知识

在许多科学领域，我们可以获得不同层次的信息。我们可能有快速、低保真度的模拟（如经典力学）和缓慢、高保真度的模拟（如量子力学）。我们如何将它们结合起来？**多保真度建模**提供了一个统计解决方案。我们可以建立一个模型，其中高保真度预测 $f_H$ 被视为对低保真度预测 $f_L$ 缩放版本的修正，形式类似于 $f_{H}(\mathbf{x})=\rho f_{L}(\mathbf{x})+\delta(\mathbf{x})$。通过使用高斯过程等工具对基础函数和修正项进行建模，我们可以学习两种保真度之间的关系，并以仅进行高保真度模拟成本的一小部分做出准确的预测 [@problem_id:2837960]。

#### 知道你所不知道的

模型的单次预测可能会产生误导。一个负责任的科学家——以及一个负责任的模型——也必须报告其不确定性。我们对这个预测有多大的信心？一个估算这一点的有效方法是，不只训练一个模型，而是训练一整个**委员会**（或集成）的模型。每个模型都以略微不同的方式进行训练（例如，在数据的不同子集上）。当我们向委员会请求预测时，我们可以将其输出的平均值视为最佳猜测。更重要的是，我们可以观察其预测的*方差*或分布。如果委员会中的所有模型都高度一致，方差就会很低，我们就可以充满信心。如果它们存在[分歧](@article_id:372077)，方差就会很高，这表明模型不确定，我们应该保持谨慎 [@problem_id:91126]。

#### 适应新世界

最后，当我们为一个特定的物理系统——比如一个简单矩形中的热流——训练了一个模型，然后想把它应用到一个新的、更复杂的系统，比如一个具有不同边界条件的 L 形物体时，会发生什么？这是一个**域自适应**问题。一个朴素的模型很可能会失败，因为输入（几何形状）和底层的物理（边界条件）都发生了变化。这被称为**[分布漂移](@article_id:370424)**。

解决方案不是从头开始。我们可以使用**[迁移学习](@article_id:357432)**，即取在简单系统上训练好的模型，并在来自新的复杂系统的少量数据上对其进行微调。此外，我们可以使用我们的[物理信息](@article_id:312969)[损失函数](@article_id:638865)，这次编码的是*新*系统的[偏微分方程](@article_id:301773)和边界条件，来指导模型的[适应过程](@article_id:377717) [@problem_id:2502958]。这种组合使得模型能够将其“知识”推广到新问题上，使它们在现实世界的科学探索中变得更加灵活和有用。

通过将这些原则——从对称性的优雅到[不确定性量化](@article_id:299045)的实用主义——交织在一起，[科学机器学习](@article_id:305979)正在打造一种新的发现语言，它将人工智能原始的[模式匹配](@article_id:298439)能力与物理定律永恒而严谨的逻辑结合在一起。