## 引言
对称性是宇宙的一项基本原则；无论你是在纽约还是在东京，或者你把头倒过来，物理定律都不会改变。将这一深刻思想直接[嵌入](@article_id:311541)人工智能的架构中，正在彻底改变我们模拟物理世界的方式。传统的机器学习模型在处理几何任务时常常举步维艰，需要大量数据才能掌握[旋转不变性](@article_id:298095)等基本原理。这种暴力学习效率低下，并且在面对新的方向时容易失败。本文将探讨一种更优雅的解决方案：设计能够内在地理解对称性规则的更智能的模型。

本文将分两部分探索[等变神经网络](@article_id:297888)的世界。首先，在**原理与机制**一章中，我们将揭示其核心概念，从[卷积神经网络](@article_id:357845)中简单的平移对称性概念，一直到物理学所需的完整三维旋转和平移对称性。我们将揭示连接不变能量和等变力的“黄金法则”，并审视构建这些强大模型的主要架构蓝图。然后，在**应用与跨学科联系**一章中，我们将看到这些原理的实际应用，展示它们如何推动[量子化学](@article_id:300637)、[材料科学](@article_id:312640)和[药物发现](@article_id:324955)领域的突破。我们的旅程始于对称性与一种更高效、更稳健的学习方式之间的根本联系。

## 原理与机制

想象一下，世界一片漆黑，你把钥匙弄丢了。你知道它们掉在了一堵长而直的墙边。你会怎么找？你大概不会在房间中央胡乱摸索。一个更好的策略是找到那堵墙，然后沿着墙边用手滑动。你的搜索策略利用了问题的一个对称性：即你知道钥匙在一条线上。这个“知识”是一个约束，但却是一个非常有用的约束。它将一个棘手的三维搜索问题，简化成了一个简单的一维问题。

在物理学和数据科学中，“对称性”同样是一个强大的指引。它不仅仅是漂亮的图案，更是关于当你做某件事时，什么东西保持不变的正式陈述。如果在一个系统上执行某个操作后，该系统的属性或支配它的定律保持不变，那么这个操作就是该系统的一个对称性。对于一个试图理解世界的机器学习模型来说，将这些对称性直接构建到其架构中，就像赋予它一种超能力。这正是**[等变神经网络](@article_id:297888)**背后的核心思想。

### 从平移图案到旋转分子

让我们从一个简单具体的例子开始。假设我们想构建一个模型来扫描一条长长的DNA序列，并找到一个特定的模式，称为结合基序，蛋白质可以附着于此[@problem_id:2373385]。一个关键的生物学事实是，这个基序可以出现在序列的*任何*位置，并且仍然具有功能。位置会移动，但模式本身是相同的。

我们的模型应如何反映这一点？我们可以尝试教一个朴[素模型](@article_id:315572)在位置1处基序的样子，然后在位置2处，再在位置3处，依此类推。这样做效率极低。这就像在黑暗中通过记住墙上每一个点的感觉来寻找钥匙。一个远为优雅的解决方案是设计一个“基序检测器”，并让它沿着序列滑动。这正是**[卷积神经网络](@article_id:357845)（CNN）**所做的事情。

CNN的魔力在于一种称为**[平移等变性](@article_id:640635)**的属性。“[等变性](@article_id:640964)”是一个简单但深刻的概念。如果一个函数，当你对其输入应用一个变换时，其输出也会以一种相应且可预测的方式变换，那么这个函数就是等变的。对于我们的CNN来说，这意味着如果我们将输入的DNA序列移动$\tau$个位置，网络产生的特征图——本质上是“检测信号”的图谱——也会精确地移动相同的量$\tau$。函数的输出与输入的变换*协同变化*。

这是通过一种名为**[权重共享](@article_id:638181)**的精妙机制实现的。同一个小滤波器（基序检测器）被应用于每一个位置。这极大地减少了模型需要学习的参数数量，可能从数百万减少到几百个，从而使其数据效率大大提高[@problem_id:2373385]。模型学习的是一个独立于其位置的概念——那个基序。

如果我们最终的目标只是问：“这个基序是否存在，是或否？”，我们不关心它在*哪里*。我们需要**[不变性](@article_id:300612)**。我们可以通过对*等变*的特征图应用一个全局池化操作来轻松实现这一点，例如取所有位置上激活值的最大值。**[等变性](@article_id:640964)$\to$[不变性](@article_id:300612)**这个序列是一个反复出现的主题。等变层保留了“哪里”的信息，而当不再需要时，一个最终的不变层会将其丢弃[@problem_id:2373385]。

现在，让我们把这个想法从一维的线提升到我们生活的三维世界。物理学的基本定律不关心你的视角。无论你是在纽约的实验室里做实验，还是在一个被搬到东京并旋转朝北的实验室里，结果都将是相同的。物理定律在[平移和旋转](@article_id:348766)下是不变的。这组变换构成了**欧几里得群**，记作$E(3)$。

因此，任何旨在学习这些物理定律的模型都必须尊重这种对称性。对于一个以物理系统（如分子的原子坐标$X$）为输入，并预测一个属性$f(X)$为输出的函数$f$而言，$E(3)$[等变性](@article_id:640964)意味着，如果我们对系统应用一个旋转$Q$和平移$t$，其输出必须相应地变换[@problem_id:2784668]：
$$
f(g \cdot X) = \rho(g) f(X)
$$
这里，$g = (Q, t)$代表[刚体运动](@article_id:329499)。$g \cdot X$项表示对输入坐标进行变换（例如，旋转和平移原子），而$\rho(g) f(X)$则表示对输出预测应用相应的变换。

### 黄金法则：不变的能量，等变的力

在分子和材料的世界里，有两个量至关重要：能量和力。
-   一个[孤立系统](@article_id:319605)的**势能**$E$是一个**标量**。它有大小，但没有方向。如果你在空间中旋转一个分子，它的能量不会改变。这意味着能量必须在旋转下是**不变的**。不变性是[等变性](@article_id:640964)的一个特例，其中输出变换$\rho(g)$只是[恒等变换](@article_id:328378)——它什么都不做。
-   作用在每个原子$i$上的**力**$\mathbf{F}_i$是**向量**。它们既有大小又有方向。如果你旋转整个分子，力向量必须随之旋转完全相同的量。这是一个经典的**[等变性](@article_id:640964)**案例。

让我们把这个概念具体化。考虑一个简单的双原子系统[@problem_id:2837945]。如果我们把系统旋转$90^\circ$，原子会移动到新的位置。可能取决于它们之间距离的能量保持不变。然而，可能从一个原子指向另一个原子的力向量，也必须旋转$90^\circ$，以保持它们相对于分子新构象的方向。

至此，我们得出了该领域中最优美、最统一的原则之一。在经典力学中，力是势能相对于位置的负梯度：$\mathbf{F}_i = -\nabla_{\mathbf{r}_i} E$。事实证明，如果你构建一个模型来预测一个对旋转完全**不变**的标量能量$E$，那么你通过对其求梯度计算出的力，在数学上被**保证**是**等变的**向量[@problem_id:2837945] [@problem_id:2648604] [@problem_id:2784682] [@problem_id:2908456]。

这条“黄金法则”提供了一个清晰的架构蓝图：要构建一个能预测物理上正确的力的模型，重点应放在构建一个能预测物理上正确的、不变的能量的模型上。力的[等变性](@article_id:640964)将自然而然地得到满足。

### 如何内置对称性：两种方法

那么，我们如何设计一个能输出$E(3)$不变能量的[神经网络](@article_id:305336)呢？主要有两种方法。

#### 方法一：[不变性](@article_id:300612)优先法

要使一个函数对某些变换保持不变，最简单的方法是确保它只看到那些已经对这些变换不变的输入。对于一个原子系统，哪些属性在全局旋转和平移下是天然不变的？原子$i$和原子$j$之间的距离、原子$i-j-k$形成的夹角，等等。

这引出了一个直接的策略：
1.  从输入的[笛卡尔坐标](@article_id:323143)$\{\mathbf{r}_i\}$中，计算一组不变特征（距离、角度等）。
2.  将这些标量特征输入到一个标准的神经网络中（如多层感知机）。
3.  该网络的输出就是标量能量$E$。

由于网络的输入是不变的，其输出也必然是不变的。像SchNet这类模型以及使用不变核的高斯过程方法都采用了这种策略，它很稳健，并保证了能量[曲面](@article_id:331153)的对称性[@problem_id:2784682] [@problem_id:2908456]。

#### 方法二：全程等变法

第一种方法很强大，但有一个缺点：在最开始就把所有几何信息转换成标量，我们可能会丢掉有用的方向信息。一种更现代、更强大的方法是在整个网络中将特征保持为向量和其他几何对象，确保每一个操作都尊重$E(3)$对称性。

其核心思想始于一个简单的问题：一个线性层$\mathbf{v}' = W \mathbf{v}$要满足旋转[等变性](@article_id:640964)需要什么条件？条件是权重矩阵$W$必须与[旋转矩阵](@article_id:300745)$D(g)$交换，即$W D(g) = D(g) W$。在一个简单的二维旋转例子中[@problem_id:90164]可以发现，这个[交换规则](@article_id:363688)严格限制了$W$的结构。它不能是一个任意的矩阵；其元素以一种特定的模式被联系在一起。对称性强制了可学习参数的结构。

现代的[等变网络](@article_id:304312)，如NequIP，将这一原则提升到了一种高级艺术[@problem_id:2760132]。
1.  **类型化特征：** 特征不仅仅是数字；它们根据在旋转下的变换方式进行分类。一个不改变的标量是“0型”($l=0$)特征。一个随之旋转的向量是“1型”($l=1$)特征。更高阶的[张量](@article_id:321604)（如[四极矩](@article_id:318122)）是2型($l=2$)，以此类推。这些在形式上被称为**[不可约表示](@article_id:298633)**或**球[张量](@article_id:321604)**。
2.  **等变“乘法”：** 你不能简单地将一个向量特征与另一个向量特征相乘并[期望](@article_id:311378)得到一个有意义的结果。你必须使用正确的数学工具来组合它们：**[张量积](@article_id:301137)**，然后使用**[Clebsch-Gordan系数](@article_id:302991)**——一种直接从量子力学中借鉴的数学工具——将其分解回类型化特征的和[@problem_id:2903794] [@problem_id:2648604]。这个过程是组合几何对象的、具备旋转感知能力的方式。
3.  **等变[消息传递](@article_id:340415)：** 网络在原子之间传递“消息”。这些消息是通过将一个原子上的特征与几何信息（如指向邻居的向量，使用称为**球谐函数**的函数进行编码）通过等变张量积组合而成的。在每一层，特征都会更新，但它们始终保持其精确的变换属性。
4.  **最终不变输出：** 为了得到最终的标量能量，所有更高类型的特征($l>0$)被“收缩”成标量（例如，通过向量与其自身的[点积](@article_id:309438)得到其模长的平方，一个不变标量）。然后将这些每个原子的标量相加，得到总的系统能量[@problem_id:2760132]。

这种方法更为复杂，但它允许网络在其计算的每一步中推理和传播丰富的方向信息，从而在性能和数据效率上取得了显著的提升。

### 回报：为何对称性是学习的超能力

为什么要经历所有这些代数上的麻烦？因为将正确的**[归纳偏置](@article_id:297870)**——一组关于问题的假设——[嵌入](@article_id:311541)到模型中，会使学习变得极为有效。

考虑一个用于材料应力-应变行为的模型[@problem_id:2629354]。假设我们只用材料沿x轴拉伸的数据来训练它。一个朴素的网络学会了这种特定情况，但如果你用一个沿对角线拉伸的材料来测试它，它就完全不知道该怎么办。它从未见过剪切应变，无法泛化。

相比之下，一个[等变网络](@article_id:304312)，其结构中硬编码了旋转定律。对于这个网络来说，学习x轴方向的拉伸，为其提供了关于*任何*拉伸方向下必须发生什么的深刻洞见。它明白对角线拉伸只是x轴拉伸的一个旋转版本。通过这种方式，一个数据点实际上为其所有可能旋转的整个*轨道*提供了信息。

这有两个巨大的好处[@problem_id:2629354] [@problem_id:2373385]：
1.  **惊人的[样本效率](@article_id:641792)：** 模型需要少得多的数据来学习潜在的物理定律，因为每个样本都教授了更广泛的一课。
2.  **卓越的泛化能力：** 模型可以对与训练中看到的非常不同的输入（如新的方向）做出准确的预测。它不是通过记忆数据点来泛化，而是通过学习支配它们的、基本的、对称的原则。

从识别我们DNA中的模式到预测新材料的行为和发现新药，[等变性](@article_id:640964)原则是一条统一的线索。通过教会我们的模型世界的基本对称性，我们不仅约束了它们，还赋予了它们一种更深刻、更稳健、更高效的理解方式。