## 引言
几个世纪以来，科学发现遵循着一条清晰的路径：将物理定律表述为[偏微分方程](@entry_id:141332)（PDE），然后不辞辛劳地去求解它。但是，当方程未知或过于复杂以至于传统方法难以应对时，会发生什么呢？这一挑战催生了一场[范式](@entry_id:161181)转变，引导我们去问：我们能否教会机器直接从数据中学习物理定律？这就是深度学习求解 PDE 的前沿领域。传统的[神经网](@entry_id:276355)络擅长学习固定大小输入和输出之间的函数，但物理定律则不同。它们是算子——一种转换整个函数的规则，且与我们如何测量或离散化无关。本文旨在解决标准函数学习与模拟物理世界所需的[算子学习](@entry_id:752958)之间的关键差距。

首先，在“原理与机制”一章中，我们将探讨这一新方法背后的核心概念，从强制执行物理定律的[物理信息神经网络](@entry_id:145229)（PINN）到学习通用解映射的[神经算子](@entry_id:752448)。我们还将剖析训练这些模型的独特挑战和偏见。随后，“应用与跨学科联系”一章将展示这些工具如何解决现实世界的问题，并揭示人工智能架构与物理学基本原理之间深刻的类比。这段旅程始于理解从学习函数到学习支配我们宇宙的算子本身的根本转变。

## 原理与机制

物理世界是建立在方程之上的。从抛出小球的优美弧线到星系间复杂的舞蹈，我们发现大自然说着数学的语言，特别是[微分方程](@entry_id:264184)的语言。几个世纪以来，我们的方法都是先发现方程，即基本定律，然后不懈地努力去求解它。但如果方程极其复杂，或者对我们来说是隐藏的呢？如果我们只能观察系统的行为——输入和相应的输出——而永远看不到规则手册呢？正是这个巨大的挑战，引领科学家们走向了一个新的前沿：不仅教机器解决已知方程，还要让它们学习物理定律本身。这就是[深度学习](@entry_id:142022)求解 PDE 的世界。

### 从函数到算子：一种新的学习方式

首先，让我们谈谈我们所说的“学习”是什么意思。今天我们称之为人工智能的大部分内容都是关于学习**函数**。[神经网](@entry_id:276355)络学习一个函数 $f$，它将一个固定大小的数字向量映射到另一个，例如，将一张 $1024 \times 1024$ 像素的图像映射到一个标签：“猫”或“不是猫”。这非常强大，但也很脆弱。如果你给网络一张不同分辨率的图像，比如 $512 \times 512$，它就根本无法工作。游戏规则已经改变了。

物理定律并非如此。描述河水流动的纳维-斯托克斯方程并不关心你是每毫米还是每米测量一次水的速度。定律就是定律，与你的测量网格无关。其支配原则不是一个简单的函数，而是一个**算子**：一个以整个*函数*为输入并产生另一个*函数*为输出的数学机器 $\mathcal{G}$。例如，一个算子可以将描述金属棒初始温度[分布](@entry_id:182848)的函数 $u(x, 0)$ 映射到描述其一分钟后温度[分布](@entry_id:182848)的函数 $u(x, 1)$。

这就是我们主题核心的[范式](@entry_id:161181)转变。我们不想学习有限维向量之间的映射，如 $f: \mathbb{R}^n \to \mathbb{R}^m$。这样的映射与特定的离散化、特定的网格大小 $n$ 绑定。如果你训练一个标准的[神经网](@entry_id:276355)络来预测 $64 \times 64$ 网格上的流体流动，那么当你尝试将其应用于 $128 \times 128$ 的网格时，该模型的稳定性和准确性都无法得到保证。随着分辨率的改变，支配其行为的数学常数可能会改变，而且往往是变得更糟 [@problem_id:3407177]。

相反，我们寻求学习算子 $\mathcal{G}$ 本身，一个在无穷维函数空间之间的映射，比如说从 $\mathcal{U}$ 到 $\mathcal{V}$ [@problem_id:3513285]。一个学习算子的网络，原则上是**[离散化不变的](@entry_id:748519)**。你可以用来自一个网格的数据来训练它，然后，因为它已经学习了底层的连续物理定律，你可以要求它在任何其他网格上给出解。这就是创建一个从数据中学习的、真正通用的“[物理模拟](@entry_id:144318)器”的希望所在。

### 融合物理学的两种理念

那么，我们如何让[神经网](@entry_id:276355)络这个臭名昭著的“黑箱”来学习和遵守物理定律呢？两种主要的理念已经出现，每一种都有其优雅的直觉。

#### 原位求解器：物理信息神经网络（PINN）

想象一下，你希望一个网络能够发现加热板上的温度[分布](@entry_id:182848) $u(x, t)$。你没有给它答案。相反，你给了它控制物理过程的热方程和边界条件。这个[神经网](@entry_id:276355)络，我们称之为 $u_{\theta}(x, t)$，将坐标 $(x, t)$ 作为输入，并提出一个温度值。

我们如何知道它的提议是否足够好呢？我们检查它是否满足物理定律。我们可以使用[自动微分](@entry_id:144512)的魔力（这也是用来训练网络的工具）来计算网络输出的导数，如 $\frac{\partial u_{\theta}}{\partial t}$、$\frac{\partial^2 u_{\theta}}{\partial x^2}$ 等。然后，我们把这些导数直接代入热方程。如果网络的提议是正确的，那么方程的残差——即方程未被满足的程度——应该在任何地方都为零。

这就是**[物理信息神经网络](@entry_id:145229)（PINN）**的精髓。它的“损失函数”，即它试图最小化的量，是 PDE 在大量随机采样的点上的[残差平方和](@entry_id:174395)，再加上边界上的任何不匹配 [@problem_id:3337943]。网络不是在输入-输出数据对上训练的。它是通过强迫其遵守控制方程而*原位*训练的。PINN 是一种针对特定实例的求解器：它学习解决一个特定问题（一组边界条件、一种几何形状）的解。要解决一个新问题，你必须从头开始重新训练它。

#### 通用法则书：[神经算子](@entry_id:752448)

第二种理念更为宏大。与其解决一个问题，我们是否可以学习解决一整族问题的*通用方法*？这就是**[神经算子](@entry_id:752448)**的目标。

在这里，我们确实使用了一个输入-输出对的数据集。我们可能会为我们的金属棒生成 1000 个不同的初始温度[分布](@entry_id:182848)，并使用一个传统（但缓慢）的求解器计算出一分钟后相应的温度[分布](@entry_id:182848)。然后我们训练一个[神经算子](@entry_id:752448)来学习从*任何*初始[分布](@entry_id:182848)到最终[分布](@entry_id:182848)的映射。训练后，该算子可以接收一个全新的、前所未见的第 1001 个[初始条件](@entry_id:152863)，并几乎即时地预测结果，无需任何进一步的训练或优化 [@problem_id:3337943]。它将学习的成本分摊到了整个问题族上。

两种优美的架构主导着这个领域。**[深度算子网络](@entry_id:748262)（[DeepONet](@entry_id:748262)）**采用“[分而治之](@entry_id:273215)”的策略。它使用一个网络（“分支”）来处理输入函数（例如，在几个传感器位置采样），并使用另一个网络（“主干”）来处理输出坐标。然后将两者结合起来产生最[终值](@entry_id:141018)。这就像学习一组[基函数](@entry_id:170178)（从主干网络）以及该基的正确系数（从分支网络）来构建解 [@problem_id:3513285]。

**[傅里叶神经算子](@entry_id:189138)（FNO）**则有另一个同样优美的想法，其灵感来自[波动力学](@entry_id:166256)。许多物理过程，特别是涉及波或[扩散](@entry_id:141445)的过程，可以在[频域](@entry_id:160070)（或傅里叶空间）中被描述为一个滤波过程。FNO 通过将输入函数转换到其频率分量，在这个[频域](@entry_id:160070)中应用一个学习到的滤波器，然后再转换回来，从而进行学习。由于实空间中的卷积在傅里叶空间中只是乘法，因此这种方式非常高效。至关重要的是，学习到的滤波器与输入网格无关，这赋予了 FNO 其卓越的离散化不变性 [@problem_id:3513285]。

### 机器中的幽灵：训练偏见与动力学

这些网络很强大，但它们不是魔法棒。训练它们的过程——通常是通过[梯度下降](@entry_id:145942)缓慢调整其数百万个参数——本身就是一个具有其独特行为的动力学系统。理解这些行为就像天文学家理解其望远镜的怪癖一样；这对于正确解读至关重要。

其中一个最深刻且最具挑战性的特性是**谱偏见**。当使用梯度下降进行训练时，[神经网](@entry_id:276355)络有一种压倒性的倾向，即学习低频函数的速度远快于高频函数 [@problem_id:3352051]。这就像一位画家，可以迅速勾勒出日落的柔和色彩，却需要花费极长时间来渲染破[碎波](@entry_id:268639)浪的精细、复杂的细节。对于一个试图解决带有[冲击波](@entry_id:199561)或尖锐[边界层](@entry_id:139416)——这些特征富含高频信息——问题的 PINN 来说，这是一场灾难。网络会很乐意拟合解中平滑、缓慢变化的部分，而在尖锐特征应该出现的地方产生一个模糊、平滑的混乱结果。至关重要的是，不要将此与 PDE 中的**刚性**混淆，刚性是系统具有多个、广泛分离的时间或空间尺度的内在物理属性。谱偏见是学习器的属性，而不是问题的属性。

训练过程本身也可能变得不稳定，这与经典数值方法的类比惊人地相似。标准的[梯度下降](@entry_id:145942)更新可以看作是描述沿[损失景观](@entry_id:635571)向下移动的[常微分方程](@entry_id:147024)（ODE）的“显式欧拉”[时间步进方案](@entry_id:755998)。就像 PDE 的欧拉方案如果时间步长过大（违反著名的 Courant–Friedrichs–Lewy 或 CFL 条件）可能会爆炸一样，如果[学习率](@entry_id:140210) $\eta$ 相对于[损失景观](@entry_id:635571)的属性（特别是其 Hessian 矩阵的最大[特征值](@entry_id:154894) $\lambda_{\max}(H)$）过大，[梯度下降](@entry_id:145942)也会发散。稳定性条件 $\eta < 2/\lambda_{\max}(H)$ 是 CFL 条件的完美类比 [@problem_id:2378443]。此外，在非常深的网络中臭名昭著的“梯度消失和爆炸”问题是这种动力学系统观点的直接后果：当你反[复乘](@entry_id:168088)以[谱范数](@entry_id:143091)小于或大于一的矩阵时，就会发生这种情况。

### 驯服野兽：将物理学构建到学习器中

如果我们了解这些偏见和失败模式，我们能设计出更好的网络吗？这就是该领域成为物理学、艺术和工程学的美妙结合之处。我们不必与网络的本性作斗争，而是可以改变其本性，使其更符合我们想要建模的物理学。

你的问题是否涉及波，比如亥姆霍兹方程？那么，一个偏爱低频、非[振荡](@entry_id:267781)函数的标准网络就是一个糟糕的选择。解决方案是将[振荡](@entry_id:267781)直接构建到网络的神经元中。通过使用正弦激活函数，如 $f(z) = \sin(sz)$，我们创建了一个具有表示[振荡](@entry_id:267781)函数的天然“[归纳偏置](@entry_id:137419)”的网络。一个简单的分析表明，当这个网络自身的内部频率与物理波数匹配时，它可以完美地满足[波动方程](@entry_id:139839)，这是标准网络难以实现的壮举 [@problem_id:3431024]。

你的网络是否忽略了尖锐特征和精细细节？也许你的“评分标准”——[损失函数](@entry_id:634569)——太宽松了。标准的 $L^2$ 损失只衡量函数*值*的误差。我们可以切换到 $H^1$ 损失，它同时惩罚*值*和*梯度*的误差 [@problem_id:3407197]。这就像告诉一个学生：“不仅你的答案必须正确，你的推理过程（斜率）也必须正确。”在[频域](@entry_id:160070)中，这对应于更重地加权高频误差，迫使优化器关注它们。这起到了一种正则化的作用，使得学习到的模型更平滑，对训练数据中的噪声也更具鲁棒性。

我们甚至可以引导学习过程从易到难。就像人类学生从简单问题开始一样，我们可以先在粗糙的点网格上训练网络，让它捕捉解的大尺度、低频的“要点”。然后，我们使用这个训练好的网络作为“热启动”，在更精细的网格上进行训练。这种多分辨率方案是经典[数值分析](@entry_id:142637)中高效的完全[多重网格](@entry_id:172017)（FMG）方法的直接类比。通过在每个级别上只求解到该级别分辨率所需的精度，我们可以实现最优的计算复杂度，以大约在最精细网格上进行单次训练的成本获得高分辨率解 [@problem_id:3396913]。

### 摆脱维度灾难

这给我们带来了最终的回报。我们为什么要费这么大劲？圣杯是解决高维问题。金融、量子力学和[分子动力学](@entry_id:147283)中的许多问题都涉及数十、数百甚至数千维度的 PDE。

对于传统的基于网格的求解器来说，这是一堵无法逾越的墙。如果你需要 100 个网格点来解析一个维度，那么三维就需要 $100^3 = 100$ 万个点，而 100 维则需要一个不可能的 $100^{100}$ 个点。这种指数级的扩展就是臭名昭著的**维度灾难**。

PINN 和其他[无网格方法](@entry_id:177458)摆脱了这一诅咒。它们不依赖于网格，而是依赖于**[蒙特卡洛采样](@entry_id:752171)**——在随机选择的点上评估 PDE 残差。蒙特卡洛方法的魔力在于其[收敛速度](@entry_id:636873)，对于 $M$ 个样本，其收敛速度大约为 $1/\sqrt{M}$，与维度 $d$ 无关 [@problem_id:2969616]。这就是这些方法甚至可以开始考虑解决 100 维 PDE 的原因。

当然，天下没有免费的午餐。收敛速度前面的“常数”可能对 $d$ 有某种多项式依赖。更重要的是，摆脱[维度灾难](@entry_id:143920)的能力依赖于一个关键假设：我们正在寻找的高维函数（PDE 的解）具有特殊的结构。它们不仅仅是任意的、复杂的函数。它们可能是复合的，或者近似低秩的，或者拥有对称性。正是这种隐藏的简单性，使得[深度神经网络](@entry_id:636170)——本身就是一个[复合函数](@entry_id:147347)——特别适合高效地发现和表示 [@problem_id:2969616] [@problem_id:1453806]。万能逼近定理告诉我们网络*可以*表示该函数，但真正的突破在于希望它能*紧凑地*做到这一点，而不需要其大小随维度呈[指数增长](@entry_id:141869)。

我们已经看到，用[深度学习](@entry_id:142022)来学习物理学，并非将数据塞进一个黑箱那么简单。它是物理原理与计算结构之间深刻的对话，是一场发现正确架构、正确训练策略和正确[归纳偏置](@entry_id:137419)的旅程，旨在创造出不仅能模仿世界，更能学习其底层规则的模型。

