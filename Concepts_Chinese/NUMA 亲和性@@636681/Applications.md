## 应用与跨学科联系

想象一位大师级工匠在一个巨大的工作台前。为了高效工作，他们会将最常用的工具和材料放在触手可及的范围内。为了拿一件常用工具而穿越整个工作室会慢得令人发疯。现代多处理器计算机就像这个巨大的工作室，但有几十个工匠——处理器核心——同时工作。每个处理器都有“近”的内存（本地内存）和“远”的内存（远程内存，属于另一个处理器）。这就是[非统一内存访问](@entry_id:752608)（NUMA）的世界。

让工作者靠近其工作内容的这个简单而优美的想法，在计算机设计中成了一个深刻的挑战。实现高性能的全部艺术，往往可以看作是在两个相互竞争的愿望之间进行微妙的舞蹈：既要将每个处理器及其数据保持在一起，放在它們的本地“工作台”上（我们称之为*局部性*），又要确保总工作量在所有处理器之间[均匀分布](@entry_id:194597)，以免有处理器闲置（我们称之为*负载均衡*）。在本章中，我们将探讨这种基本张力如何在一系列引人入胜的应用中展现出来，从[操作系统](@entry_id:752937)的核心，到最宏大的[科学模拟](@entry_id:637243)，再到云的虚拟化世界。

### 机器之心：[操作系统](@entry_id:752937)的角色

[操作系统](@entry_id:752937)是这场复杂舞蹈的总编舞。它做出关于程序在哪里运行以及它们的数据存放在哪里的关键决策。

它在[内存布局](@entry_id:635809)上采用的一种聪明而常见的策略叫做“首次接触”。当程序需要一个新的内存页时，系统不会立即决定其物理位置，而是会等待。物理内存被分配在*首次*写入该页的处理器核心所在的 NUMA 节点上。这巧妙地将数据的位置与其初始用户联系在一起。这意味着，如果一个程序线程停留在其节点上，它后续对该数据的所有访问都将是快速和本地的。只有当线程本身迁移到另一个节点时，才会发生远程访问。在一个极其简单的关系中，预期的远程内存访问比例不过是一个线程离开其分配内存的节点的概率 [@problem_id:3683607]。这个原则是如此基础，以至于[操作系统](@entry_id:752937)常常在其核心[内存分配](@entry_id:634722)器（如用于内核对象的 slab 分配器）中构建每节点缓存来利用它。

但是计算机是一个繁忙的地方。当我们有关键的、对延迟敏感的应用程序，它们绝不能被打扰，同时又有嘈杂的“内务管理”任务（如系统日志记录、监控或托管运行时的[垃圾回收](@entry_id:637325)）在运行时，会发生什么？在这里，我们可以使用[处理器亲和性](@entry_id:753769)作为*隔离*的工具。我们可以指定几个核心作为“内务核心”，并使用软亲和性温和地引导这些非关键任务到那里运行。通过分析资源需求——例如，将日志线程与处理存储中断的核心以及垃圾回收器与其本地内存协同定位——我们可以为主应用程序创造一个稳定、可预测的环境，保护它免受后台琐事的[抖动](@entry_id:200248)和干扰 [@problem_id:3672772]。对于确定性至关重要的任务，例如垃圾回收器中的“stop-the-world”暂停，我们可以更进一步。通过使用*硬亲和性*将回收器线程锁定到正确 NUMA 节点上的一组特定核心，我们可以防止调度器迁移它们，并消除来自远程内存访问和中断抢占的延迟，从而最小化最坏情况下的暂停时间 [@problem_id:3672835]。

当涉及高速网络时，这场舞蹈变得更加复杂。现代网卡是数据的贪婪生产者和消费者，使用直接内存访问（DMA）每秒移动数GB的数据。假设网卡物理上连接到节点 A，但处理网络数据的应用程序运行在节点 B 上。冲突就产生了。如果驱动程序的内存缓冲区放在节点 A 上以靠近网卡，节点 B 上的 CPU 就必须不断执行缓慢的远程读取。如果缓冲区放在节点 B 上以靠近 CPU，网卡就必须跨越互连执行缓慢的远程 DMA 写入。仔细分析表明，通常存在一个“最不坏”的选择。在许多情况下，CPU 对数据的渴求是主导因素，最小化其访问延迟是关键。最佳策略通常是将所有内存缓冲区放在与设备相同的节点上，消除所有远程 DMA 流量，即使这意味着其他节点上的应用程序线程必须支付远程访问的代价 [@problem_id:3648063]。

### 坚实土地上的虚拟世界：虚拟化中的 NUMA

当我们增加另一层抽象——虚拟机（VM）时，会发生什么？VM 内部的客户机[操作系统](@entry_id:752937)常常被误导，以为自己生活在一个简单的、扁平的 UMA 世界里。但是管理物理硬件的 hypervisor 知道底层 NUMA 拓扑的真相。这可能导致令人惊讶的性能谜团。

想象一个 VM 运行平稳，其所有 vCPU 和内存都愉快地驻留在单个 NUMA 节点上。hypervisor 为了给另一个任务回收内存，可能会使用一种称为“[内存气球](@entry_id:751846)”的技术，从 VM 中取走一些物理页面。当 VM 稍后需要这些内存时，hypervisor 可能被迫在*远程* NUMA 节点上分配它。突然之间，VM 的一部分内存访问变得缓慢。客户机[操作系统](@entry_id:752937)不知道为什么其性能下降了；从它的角度看，什么都没变。但 NUMA 的物理现实已经“泄露”过了抽象层，应用程序的[吞吐量](@entry_id:271802)与平均内存访问延迟的增加成正比地下降 [@problem_id:3663629]。

对于要求最苛刻的 I/O 工作负载，我们可能会给 VM 对物理设备（如高速网卡）的直接、“直通”控制。这承诺了接近原生的性能，但也意味着 VM 现在直接暴露于物理布局的现实中。一个常见且灾难性的错误配置是将直通设备放在一个插槽上，而将 VM 的 vCPU 和内存固定到另一个插槽上。这就造成了性能不佳的“完美风暴”。每一次操作都变成了缓慢的、跨插槽的旅程：设备的 DMA到客户机内存是远程的，设备到 vCPU 的中断是远程的，甚至 CPU 对设备控制寄存器的命令也是远程的。理解并修复这种 NUMA 不对齐——通过确保设备、vCPU 和内存都协同定位在同一个物理插槽上——对于在云中实现高性能 I/O 至关重要 [@problem_id:3648949]。

### 终极追求：高性能[科学计算](@entry_id:143987)

在高性能计算（HPC）领域，对 NUMA 亲和性的掌握比任何地方都更为关键。在这里，科学家们将硬件推向其绝对极限，以模拟从[星系碰撞](@entry_id:158614)到[蛋白质折叠](@entry_id:136349)的一切。

即使对于[科学计算](@entry_id:143987)的基本构建块如矩阵乘法，其好处也是清晰且可量化的。一个简单的 NUMA 感知调度策略，确保处理器计算存储在其本地内存中的矩阵块，可以比随机分配工作的 NUMA 非感知策略产生显著的加速。性能增益直接来自于避免了大部分内存访问时跨插槽链路的带宽限制 [@problem_id:3663647]。

然而，现实世界中的[性能调优](@entry_id:753343)通常就像当侦探一样。一个应用程序可能很慢，但简单地归咎于 NUMA 并收紧亲和性并非总是答案。必须首先诊断真正的瓶颈。考虑一个遭受高[尾延迟](@entry_id:755801)的[微服务](@entry_id:751978)。通过检查性能计数器，我们可能会发现 CPU 利用率已接近 $100\%$，任务正在排队等待核心，而缓存未命中等指标实际上很低。在这种情况下，问题不是[内存局部性](@entry_id:751865)差——而是 CPU 饱和！解决方案不是增加亲和性的“粘性”，这可能会使问题恶化，而是扩展应用程序允许运行的核心集（最好在同一个 NUMA 节点上）以缓解压力 [@problem_id:3672826]。

更多时候，我们面临着局部性和负载均衡之间的经典权衡。例如，在一个复杂的[流体动力学模拟](@entry_id:142279)中，网格的某些区域（例如包含障碍物的区域）可能需要比其他区域多得多的计算。如果我们使用简单的 `static` 调度策略，即每个线程获得一个固定的、连续的工作块，我们可能会实现完美的 NUMA 局部性，但会遭受可怕的负载不均，因为一些线程会提前完成并闲置，而其他线程仍在艰难地处理困难部分。一个 `dynamic` 调度，即线程从中央队列中获取小的工作项，将完美地平衡负载，但会破坏局部性，因为线程会不断地访问来自机器各处的数据。最佳点通常是 `guided` 调度，它从大块开始以保持局部性，然后逐渐转向更小的块以平衡末尾的剩余工作。最佳选择总是根据特定的工作负载和硬件进行 carefully 妥协 [@problem_id:3329284]。

最后，对于最大规模的模拟，我们必须将所有这些原则综合成一个宏大的分层策略。为了优化模拟声波或[宇宙学流体动力学](@entry_id:747918)的代码，专家们采用了一种多层次的方法：

1.  **节点间分区：** 使用 MPI 将全局问题域分解成大的、大致立方的块。立方体形状最小化了表面积与体积之比，从而最小化了不同节点上的 MPI 进程之间所需的通信量。
2.  **节点内布局：** 在单个计算节点内，MPI 进程被固定到特定的 NUMA 插槽。这创建了孤立的“计算岛屿”。
3.  **[内存局部性](@entry_id:751865)：** 使用“首次接触”初始化策略。每个 MPI 进程，利用其本地的 [OpenMP](@entry_id:178590) 线程团队，写入其域的部分，确保物理[内存分配](@entry_id:634722)在其自己的 NUMA 插槽上。
4.  **[线程级并行](@entry_id:755943)：** 在每个 MPI 进程内部，[OpenMP](@entry_id:178590) 线程以“紧凑”亲和性固定，因此它们都在单个插槽的核心上运行。这使它们能够有效地共享插槽的大末级缓存（LLC）并从快速的本地内存访问其所有主要数据。
5.  **缓存优化：** 最内层的循环被分块或“blocked”，以使每个线程的[工作集](@entry_id:756753)能够舒适地放入更小、更快的[缓存层次结构](@entry_id:747056)中。

这种从整个集群的规模到单个核心缓存的整体策略，是 NUMA 感知编程的巅峰。正是通过在每个层面上精心编排处理器和内存的这场复杂舞蹈，我们才得以释放现代超级计算机的全部力量，以解决我们这个时代的重大科学挑战 [@problem_id:3516586] [@problem_id:3586201]。