## 引言
信息仅仅是一个抽象概念，还是可以像物理物质一样被测量和对待？这个基本问题是现代科学的核心，它连接了从计算机工程到理论物理的各个领域。几个世纪以来，“信息”一直是一个直观但难以捉摸的概念。缺乏一个严谨的、定量的框架限制了我们理解通信、计算乃至生命本身基本约束的能力。本文直面这一挑战，清晰地阐述了信息是如何被量化的。

第一章“原理与机制”将探讨那些为我们带来“比特”这一概念的突破性思想。我们将深入研究 Claude Shannon 如何用熵将不确定性转化为数字，以及 [R. A. Fisher](@article_id:346210) 如何量化从实验中获得的知识。至关重要的是，我们将揭示一个深刻的发现：[信息是物理的](@article_id:339966)，受[热力学定律](@article_id:321145)的支配。

接下来，“应用与跨学科联系”一章将揭示这一新视角的巨大威力。我们将看到这些原理如何解释生命遗传密码的效率，解决[热力学](@article_id:359663)中长期存在的悖论，并为理解科学探究的本质提供一种新的语言。读完本文，读者将明白，量化信息不仅仅是一项数学活动，更是通往一个更深刻、更统一的宇宙观的大门。

## 原理与机制

如何为一个像“信息”这样抽象的概念赋予一个数字？是否有可能测量一条消息的内容、我们不确定性的程度，或者我们从一次实验中学到的东西？对这些问题的探索引发了20世纪最伟大的科学革命之一，揭示了信息不仅仅是一个概念，而是一个受自然法则支配的物理量。这段量化的旅程揭示了[通信工程](@article_id:335826)、统计学和[热力学](@article_id:359663)等不同领域之间惊人的一致性。

### 从消息到比特：初步理解

想象一下，你正在设计一个早期的自动电报系统。你的机器可以传输一组固定的符号，比如说150个不同的符号[@problem_id:1629820]。传输一个符号传达了多少“信息”？

先驱工程师 Ralph Hartley 有一个极其简单的见解。你拥有的选择越多，指定其中一个所需的信息就越多。但这种关系并非简单的线性关系。如果你做出两个独立的选择——一个来自包含 $S_1$ 个符号的集合，另一个来自包含 $S_2$ 个符号的集合——那么可能的组合消息总数为 $S_1 \times S_2$。一个合理的信息度量应该是可加的；总信息量应为每次选择所含信息量的总和。什么数学函数能将乘法转化为加法？答案是对数。

Hartley 提出，信息内容 $H$ 与可能结果数 $S$ 的对数成正比。
$$H = \log(S)$$
对数的底数只是设定了我们的度量单位。如果我们选择以2为底，我们就定义了信息最基本的单位：**比特**（bit）。一个比特是决定两个等可能选项所需的信息量，因为 $\log_{2}(2) = 1$。它是一个明确的“是”或“否”问题的答案——是信息的基本原子。

### 不确定性与意外性：Shannon的天才之作

Hartley 的想法是一个强有力的开端，但它假设所有结果都是等可能的。在现实世界中，情况很少如此。在英文文本中，字母'E'远比'Z'常见。一次公平的硬币投掷是不确定性的来源，但一枚两面都是正面的硬币则不然。当然，从一个事件中获得的信息量应该取决于其可能性。

这就是现代信息论之父 Claude Shannon 做出革命性贡献的地方。他关注的是“意外性”这一要素。一个很可能发生的事件在发生时并不会让人感到意外，因此传达的新信息很少。相反，一个罕见且出乎意料的事件则非常令人意外，并携带大量信息。通过将概率为 $p$ 的结果的**信息内容**（或称“意外度”）定义为以下公式，可以完美地捕捉到这种直觉：
$$I = -\log_{2}(p)$$
注意它的工作原理：如果 $p$ 接近1（几乎确定），$I$ 就接近0。如果 $p$ 非常小（罕见事件），$I$ 就会变得非常大。

然后，Shannon 将我们现在所说的**香农熵**定义为从一个[随机过程](@article_id:333307)中可以预期的*平均意外度*。对于一组具有各自概率 $p_i$ 的结果，熵 $H$ 是其信息内容的加权平均值：
$$H = -\sum_{i} p_i \log_{2}(p_i)$$
想象一个微小的纳米机械开关，它可以处于“开”或“关”的状态[@problem_id:1604159]。如果处于“开”状态的概率为 $p=0.5$，那么该系统是最大程度上不可预测的。其熵为 $H = -0.5\log_2(0.5) - 0.5\log_2(0.5) = 1$ 比特。测量其状态后，我们获得了1比特的信息。但是，如果开关是有偏的，比如说 $p=0.99$，那么它几乎总是处于“开”的状态。不确定性很低，熵也很低。因此，香农熵是我们观察一个系统*之前*对其不确定性的精确度量。

### 信息的物理性：从比特到[焦耳](@article_id:308101)

“比特”仅仅是一个方便的数学抽象，还是它具有物理现实性？由 Rolf Landauer 发现的答案是整个科学领域最深刻的答案之一：[信息是物理的](@article_id:339966)。

考虑一个纳米尺度的设备，它可以存在于八个等概率的[量子态](@article_id:306563)之一[@problem_id:1868009]。为了知道它处于哪个状态，我们进行一次测量。我们获得的信息是该系统的香农熵，即 $H = \log_2(8) = 3$ 比特。这些信息现在被存储起来，也许是在计算机的内存中。

现在，如果我们想为下一个周期将该内存重置为默认状态，会发生什么？我们必须擦除这3比特的信息。**[朗道尔原理](@article_id:307021)**指出，这种擦除行为不是没有代价的。它是一个[热力学](@article_id:359663)[不可逆过程](@article_id:303743)，至少必须向环境中耗散一定量的热量。在温度为 $T$ 时，擦除一比特信息所需的最小能量成本是 $k_B T \ln(2)$，其中 $k_B$ 是基本的[玻尔兹曼常数](@article_id:302824)。

要从我们的设备中擦除这3比特的信息，我们必须付出至少 $Q_{\text{min}} = 3 \times k_B T \ln(2)$ 焦耳的[热力学](@article_id:359663)代价[@problem_id:1868009]。突然之间，抽象的比特与能量、温度和宇宙的[基本常数](@article_id:309193)密不可分地联系在了一起。一个源于[通信理论](@article_id:336278)的概念，竟然受制于[热力学定律](@article_id:321145)。信息不只存在于你的头脑中，它存在于世界之中。

### 一个新视角：费雪信息

[香农熵](@article_id:303050)量化了随机结果的不确定性。但是另一种信息呢——即实验为我们提供的关于世界本身的信息？假设你正在尝试测量一个未知的物理常数，比如一个特殊制备的[量子比特](@article_id:298377)在测量时得到结果“1”的概率 $p$ [@problem_id:1918234]。每进行一次测量，你对 $p$ 的真实值就多了解一点。我们如何量化这种“学习”过程？

这就是**费雪信息**的目的，它以杰出的统计学家 [R. A. Fisher](@article_id:346210) 的名字命名。它不测量单个结果的熵。相反，它衡量的是一个可观测的[随机变量](@article_id:324024)携带了多少关于定义其[概率分布](@article_id:306824)的*未知参数*的信息。可以把它看作是衡量你实验“灵敏度”的指标。如果未知参数的微小变化导致你观察到的结果发生巨大且易于检测的变化，那么你的实验就非常灵敏，并提供了大量的费雪信息。

对于[量子比特](@article_id:298377)的测量，关于参数 $p$ 的费雪信息被发现为：
$$I(p) = \frac{1}{p(1-p)}$$
这个简单的公式非常直观[@problem_id:1918234]。当 $p$ 接近0或1时，[信息量](@article_id:333051)最低。为什么？如果你已经知道结果几乎总是“0”（$p \approx 0$），那么再观察到一个“0”并不会让你学到太多新东西。但是当 $p=0.5$ 时，也就是在不确定性最大的中间点，信息量是最高的。在这里，每一次观察在帮助你区分真实概率是0.50还是0.51等方面都具有最大的价值。

### 用[费雪信息](@article_id:305210)构建知识

[费雪信息](@article_id:305210)是统计推断的“货币”，其行为方式非常合理。

首先，对于**独立观测**，它是**可加的**。如果你进行两次独立的实验来测量同一个参数——比如，在两个不同的培养皿上计数细菌菌落[@problem_id:1941230]——你获得的总费雪信息就是每次实验信息量的总和。这正是“更多数据带来更准确知识”这一科学原理的数学体现。

这个原理适用于各种物理过程，从估计[不稳定粒子](@article_id:309082)的衰变率[@problem_id:1653754]到在测量噪声中精确定位信号的真实值[@problem_id:1615043]。事实上，对于一个被噪声干扰的测量，[费雪信息](@article_id:305210)通常与噪声量成反比。对于一个特征噪声标度为 $b$ 的过程，费雪信息通常与 $1/b^2$ 成正比[@problem_id:1615043]。噪声越少，信息越多。

此外，费雪信息出色地捕捉了高效[数据分析](@article_id:309490)的思想。通常，一个庞大的数据集可以被压缩成一个单一的数字——比如一个总和或平均值——而不会丢失任何我们关心的参数信息。这样的数字被称为**[充分统计量](@article_id:323047)**。令人惊讶的事实是，这个简单统计量中包含的[费雪信息](@article_id:305210)与整个原始数据集中包含的[费雪信息](@article_id:305210)是相同的[@problem_id:1615022]。这就是为什么科学家们常常可以充满信心地使用汇总数据；他们并没有丢弃任何关键信息。

### 伟大的统一

我们的旅程沿着两条平行的道路前进：Shannon 将信息视为不确定性，而 Fisher 将信息视为知识。我们通过[朗道尔原理](@article_id:307021)看到了[香农熵](@article_id:303050)与[热力学](@article_id:359663)的深刻联系。在一个最终而美妙的转折中，我们发现费雪信息与物理世界也有其深刻的联系。

让我们看一个简单的[两能级量子系统](@article_id:369845)——比如一个有[基态](@article_id:312876)和[激发态](@article_id:325164)的原子——与其周围环境处于[热平衡](@article_id:318390)状态[@problem_id:1631481]。它的行为由[玻尔兹曼分布](@article_id:303203)描述，该分布取决于[逆温](@article_id:300532)度 $\beta = 1/(k_B T)$。我们可以问：对系统能量的测量能为我们提供多少关于其温度的费雪信息？

结果是惊人的。关于[逆温](@article_id:300532)度的[费雪信息](@article_id:305210) $I(\beta)$，恰好是系统能量的方差 $\text{Var}(E)$。
$$I(\beta) = \text{Var}(E)$$
这是一个惊人的统一。一个能量剧烈波动的系统，也正是一个单次能量测量对其温度信息最丰富的系统。这个量，即能量的方差，也与系统的**[热容](@article_id:340019)**成正比，而[热容](@article_id:340019)是[热力学](@article_id:359663)中的一个核心概念。

所以，这就是宏伟的图景。量化信息的两种主要方法，分别诞生于通信和统计学这两个截然不同的领域，却被发现都编织在统计物理的基本结构之中。[香农熵](@article_id:303050)支配着擦除记忆的[热力学](@article_id:359663)成本。费雪信息与系统的自然[热涨落](@article_id:304074)和[热容](@article_id:340019)紧密相连。信息的研究不仅仅是一个抽象的数学游戏；它是描述物理现实的一种基本语言，在最意想不到的地方揭示了其内在的美和统一性。